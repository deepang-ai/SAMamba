Namespace(seed=15, model='AEtransGAT', dataset='phish_hack/Times', num_heads=1, num_layers=2, dim_hidden=64, dropout=0.1, epochs=1000, lr=0.0003, weight_decay=0.0001, batch_size=64, abs_pe=None, abs_pe_dim=20, num_class=2, outdir='./outdir/phish_hack/Times/seed15/edge_attr/khopgnn_gat_1_0.1_0.0003_0.0001_2_1_64_BN', warmup=10, layer_norm=False, use_edge_attr=True, edge_dim=128, gnn_type='gat', k_hop=1, global_pool='mean', se='khopgnn', ie='mamba', aggr='add', not_extract_node_feature=False, k_ford=3, early_stop=1, early_stop_mindelta=-0.0, patience=2, training_times=2, Lambda=0.01, order='sd', use_cuda=True, batch_norm=True, save_logs=True)
Data(edge_index=[2, 248], edge_attr=[248, 2], x=[104, 14887], y=[1, 1], num_nodes=104)
Data(edge_index=[2, 248], edge_attr=[248, 2], x=[104, 14887], y=[1, 1], num_nodes=104)
Data(edge_index=[2, 218], edge_attr=[218, 2], x=[94, 14887], y=[1, 1], num_nodes=104)
========================training times:0========================
========================k_idx:0========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: AEtransGAT
AEtransGAT(
  (gae): GAE(
    (encoder): transGAT(
      (encoder): EdgeFeatureGAT()
      (classifier): Sequential(
        (0): Linear(in_features=64, out_features=64, bias=True)
        (1): ReLU(inplace=True)
        (2): Linear(in_features=64, out_features=2, bias=True)
      )
    )
    (decoder): InnerProductDecoder()
  )
)
Total number of parameters: 1914242
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x71f234d65cc0>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 2.4051;  Loss pred: 2.4051; Loss self: 0.0000; time: 7.49s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6933 score: 0.5000 time: 2.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6933 score: 0.5000 time: 2.09s
Epoch 2/1000, LR 0.000029
Train loss: 2.2973;  Loss pred: 2.2973; Loss self: 0.0000; time: 7.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6933 score: 0.5000 time: 2.32s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6933 score: 0.5000 time: 2.26s
     INFO: Early stopping counter 1 of 2
Epoch 3/1000, LR 0.000059
Train loss: 2.1142;  Loss pred: 2.1142; Loss self: 0.0000; time: 7.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6932 score: 0.5000 time: 2.56s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6933 score: 0.5000 time: 2.38s
Epoch 4/1000, LR 0.000089
Train loss: 1.8739;  Loss pred: 1.8739; Loss self: 0.0000; time: 6.45s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6931 score: 0.5000 time: 2.01s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6931 score: 0.5000 time: 1.92s
Epoch 5/1000, LR 0.000119
Train loss: 1.6280;  Loss pred: 1.6280; Loss self: 0.0000; time: 6.40s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6927 score: 0.5000 time: 2.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6927 score: 0.5000 time: 1.94s
Epoch 6/1000, LR 0.000149
Train loss: 1.4132;  Loss pred: 1.4132; Loss self: 0.0000; time: 6.59s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6920 score: 0.5000 time: 2.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6920 score: 0.5000 time: 1.94s
Epoch 7/1000, LR 0.000179
Train loss: 1.2510;  Loss pred: 1.2510; Loss self: 0.0000; time: 6.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6910 score: 0.5000 time: 2.03s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6910 score: 0.5000 time: 1.94s
Epoch 8/1000, LR 0.000209
Train loss: 1.1408;  Loss pred: 1.1408; Loss self: 0.0000; time: 6.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6896 score: 0.5000 time: 2.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6896 score: 0.5000 time: 1.93s
Epoch 9/1000, LR 0.000239
Train loss: 1.0705;  Loss pred: 1.0705; Loss self: 0.0000; time: 6.11s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6874 score: 0.5000 time: 1.90s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6874 score: 0.5000 time: 1.82s
Epoch 10/1000, LR 0.000269
Train loss: 1.0313;  Loss pred: 1.0313; Loss self: 0.0000; time: 6.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6841 score: 0.5000 time: 2.00s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6840 score: 0.5000 time: 1.92s
Epoch 11/1000, LR 0.000299
Train loss: 1.0066;  Loss pred: 1.0066; Loss self: 0.0000; time: 6.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6789 score: 0.5000 time: 2.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6788 score: 0.5000 time: 1.97s
Epoch 12/1000, LR 0.000299
Train loss: 0.9907;  Loss pred: 0.9907; Loss self: 0.0000; time: 6.59s
Val loss: 0.6710 score: 0.5083 time: 2.13s
Test loss: 0.6708 score: 0.5118 time: 1.97s
Epoch 13/1000, LR 0.000299
Train loss: 0.9783;  Loss pred: 0.9783; Loss self: 0.0000; time: 6.45s
Val loss: 0.6594 score: 0.5793 time: 2.05s
Test loss: 0.6592 score: 0.5822 time: 1.94s
Epoch 14/1000, LR 0.000299
Train loss: 0.9658;  Loss pred: 0.9658; Loss self: 0.0000; time: 6.34s
Val loss: 0.6433 score: 0.7166 time: 2.06s
Test loss: 0.6431 score: 0.7053 time: 1.93s
Epoch 15/1000, LR 0.000299
Train loss: 0.9531;  Loss pred: 0.9531; Loss self: 0.0000; time: 6.55s
Val loss: 0.6213 score: 0.7976 time: 2.06s
Test loss: 0.6211 score: 0.7864 time: 2.03s
Epoch 16/1000, LR 0.000299
Train loss: 0.9372;  Loss pred: 0.9372; Loss self: 0.0000; time: 6.64s
Val loss: 0.5918 score: 0.8337 time: 2.18s
Test loss: 0.5918 score: 0.8201 time: 2.11s
Epoch 17/1000, LR 0.000299
Train loss: 0.9188;  Loss pred: 0.9188; Loss self: 0.0000; time: 6.89s
Val loss: 0.5561 score: 0.8698 time: 2.41s
Test loss: 0.5563 score: 0.8497 time: 2.50s
Epoch 18/1000, LR 0.000299
Train loss: 0.8957;  Loss pred: 0.8957; Loss self: 0.0000; time: 6.58s
Val loss: 0.5133 score: 0.8846 time: 2.18s
Test loss: 0.5140 score: 0.8710 time: 1.94s
Epoch 19/1000, LR 0.000299
Train loss: 0.8700;  Loss pred: 0.8700; Loss self: 0.0000; time: 6.40s
Val loss: 0.4653 score: 0.8935 time: 2.09s
Test loss: 0.4665 score: 0.8905 time: 2.02s
Epoch 20/1000, LR 0.000299
Train loss: 0.8419;  Loss pred: 0.8419; Loss self: 0.0000; time: 7.10s
Val loss: 0.4174 score: 0.9030 time: 2.13s
Test loss: 0.4190 score: 0.8935 time: 2.05s
Epoch 21/1000, LR 0.000299
Train loss: 0.8118;  Loss pred: 0.8118; Loss self: 0.0000; time: 6.49s
Val loss: 0.3728 score: 0.9095 time: 2.30s
Test loss: 0.3747 score: 0.9000 time: 2.18s
Epoch 22/1000, LR 0.000299
Train loss: 0.7854;  Loss pred: 0.7854; Loss self: 0.0000; time: 6.35s
Val loss: 0.3334 score: 0.9148 time: 2.01s
Test loss: 0.3354 score: 0.9095 time: 1.91s
Epoch 23/1000, LR 0.000299
Train loss: 0.7614;  Loss pred: 0.7614; Loss self: 0.0000; time: 6.84s
Val loss: 0.3009 score: 0.9178 time: 2.10s
Test loss: 0.3028 score: 0.9136 time: 1.95s
Epoch 24/1000, LR 0.000299
Train loss: 0.7410;  Loss pred: 0.7410; Loss self: 0.0000; time: 6.55s
Val loss: 0.2746 score: 0.9189 time: 1.98s
Test loss: 0.2764 score: 0.9183 time: 1.88s
Epoch 25/1000, LR 0.000299
Train loss: 0.7233;  Loss pred: 0.7233; Loss self: 0.0000; time: 6.39s
Val loss: 0.2534 score: 0.9207 time: 1.99s
Test loss: 0.2549 score: 0.9213 time: 1.91s
Epoch 26/1000, LR 0.000299
Train loss: 0.7092;  Loss pred: 0.7092; Loss self: 0.0000; time: 6.30s
Val loss: 0.2369 score: 0.9231 time: 1.98s
Test loss: 0.2381 score: 0.9225 time: 1.89s
Epoch 27/1000, LR 0.000299
Train loss: 0.6990;  Loss pred: 0.6990; Loss self: 0.0000; time: 6.31s
Val loss: 0.2228 score: 0.9266 time: 1.96s
Test loss: 0.2236 score: 0.9260 time: 1.90s
Epoch 28/1000, LR 0.000299
Train loss: 0.6873;  Loss pred: 0.6873; Loss self: 0.0000; time: 6.47s
Val loss: 0.2115 score: 0.9296 time: 2.20s
Test loss: 0.2121 score: 0.9302 time: 2.14s
Epoch 29/1000, LR 0.000299
Train loss: 0.6784;  Loss pred: 0.6784; Loss self: 0.0000; time: 6.69s
Val loss: 0.2015 score: 0.9302 time: 2.05s
Test loss: 0.2019 score: 0.9337 time: 1.96s
Epoch 30/1000, LR 0.000299
Train loss: 0.6724;  Loss pred: 0.6724; Loss self: 0.0000; time: 6.38s
Val loss: 0.1927 score: 0.9337 time: 2.14s
Test loss: 0.1930 score: 0.9349 time: 1.93s
Epoch 31/1000, LR 0.000299
Train loss: 0.6659;  Loss pred: 0.6659; Loss self: 0.0000; time: 6.37s
Val loss: 0.1857 score: 0.9343 time: 1.97s
Test loss: 0.1861 score: 0.9373 time: 1.90s
Epoch 32/1000, LR 0.000299
Train loss: 0.6609;  Loss pred: 0.6609; Loss self: 0.0000; time: 6.24s
Val loss: 0.1788 score: 0.9367 time: 1.99s
Test loss: 0.1791 score: 0.9391 time: 1.91s
Epoch 33/1000, LR 0.000299
Train loss: 0.6552;  Loss pred: 0.6552; Loss self: 0.0000; time: 6.15s
Val loss: 0.1734 score: 0.9432 time: 2.08s
Test loss: 0.1739 score: 0.9414 time: 1.94s
Epoch 34/1000, LR 0.000298
Train loss: 0.6521;  Loss pred: 0.6521; Loss self: 0.0000; time: 6.31s
Val loss: 0.1677 score: 0.9450 time: 2.15s
Test loss: 0.1682 score: 0.9420 time: 2.05s
Epoch 35/1000, LR 0.000298
Train loss: 0.6478;  Loss pred: 0.6478; Loss self: 0.0000; time: 6.36s
Val loss: 0.1631 score: 0.9462 time: 2.00s
Test loss: 0.1636 score: 0.9432 time: 1.98s
Epoch 36/1000, LR 0.000298
Train loss: 0.6437;  Loss pred: 0.6437; Loss self: 0.0000; time: 6.91s
Val loss: 0.1585 score: 0.9479 time: 2.12s
Test loss: 0.1588 score: 0.9444 time: 2.05s
Epoch 37/1000, LR 0.000298
Train loss: 0.6403;  Loss pred: 0.6403; Loss self: 0.0000; time: 6.43s
Val loss: 0.1548 score: 0.9491 time: 2.23s
Test loss: 0.1549 score: 0.9444 time: 2.05s
Epoch 38/1000, LR 0.000298
Train loss: 0.6367;  Loss pred: 0.6367; Loss self: 0.0000; time: 6.18s
Val loss: 0.1514 score: 0.9521 time: 1.98s
Test loss: 0.1513 score: 0.9462 time: 1.90s
Epoch 39/1000, LR 0.000298
Train loss: 0.6341;  Loss pred: 0.6341; Loss self: 0.0000; time: 6.17s
Val loss: 0.1486 score: 0.9533 time: 2.05s
Test loss: 0.1483 score: 0.9485 time: 2.04s
Epoch 40/1000, LR 0.000298
Train loss: 0.6313;  Loss pred: 0.6313; Loss self: 0.0000; time: 6.49s
Val loss: 0.1458 score: 0.9544 time: 2.15s
Test loss: 0.1451 score: 0.9491 time: 1.97s
Epoch 41/1000, LR 0.000298
Train loss: 0.6283;  Loss pred: 0.6283; Loss self: 0.0000; time: 6.43s
Val loss: 0.1435 score: 0.9538 time: 2.03s
Test loss: 0.1425 score: 0.9497 time: 1.93s
Epoch 42/1000, LR 0.000298
Train loss: 0.6278;  Loss pred: 0.6278; Loss self: 0.0000; time: 6.31s
Val loss: 0.1410 score: 0.9562 time: 1.99s
Test loss: 0.1397 score: 0.9509 time: 1.92s
Epoch 43/1000, LR 0.000298
Train loss: 0.6246;  Loss pred: 0.6246; Loss self: 0.0000; time: 6.50s
Val loss: 0.1395 score: 0.9550 time: 2.03s
Test loss: 0.1379 score: 0.9509 time: 1.97s
Epoch 44/1000, LR 0.000298
Train loss: 0.6235;  Loss pred: 0.6235; Loss self: 0.0000; time: 6.40s
Val loss: 0.1375 score: 0.9556 time: 2.27s
Test loss: 0.1356 score: 0.9509 time: 2.10s
Epoch 45/1000, LR 0.000298
Train loss: 0.6208;  Loss pred: 0.6208; Loss self: 0.0000; time: 6.25s
Val loss: 0.1357 score: 0.9574 time: 1.95s
Test loss: 0.1335 score: 0.9515 time: 1.87s
Epoch 46/1000, LR 0.000298
Train loss: 0.6203;  Loss pred: 0.6203; Loss self: 0.0000; time: 6.31s
Val loss: 0.1338 score: 0.9586 time: 1.97s
Test loss: 0.1312 score: 0.9515 time: 1.87s
Epoch 47/1000, LR 0.000298
Train loss: 0.6184;  Loss pred: 0.6184; Loss self: 0.0000; time: 6.20s
Val loss: 0.1323 score: 0.9586 time: 2.09s
Test loss: 0.1294 score: 0.9515 time: 2.01s
Epoch 48/1000, LR 0.000298
Train loss: 0.6155;  Loss pred: 0.6155; Loss self: 0.0000; time: 6.30s
Val loss: 0.1312 score: 0.9574 time: 1.99s
Test loss: 0.1281 score: 0.9515 time: 2.03s
Epoch 49/1000, LR 0.000298
Train loss: 0.6146;  Loss pred: 0.6146; Loss self: 0.0000; time: 6.45s
Val loss: 0.1295 score: 0.9580 time: 2.28s
Test loss: 0.1260 score: 0.9521 time: 2.13s
Epoch 50/1000, LR 0.000298
Train loss: 0.6133;  Loss pred: 0.6133; Loss self: 0.0000; time: 6.65s
Val loss: 0.1279 score: 0.9586 time: 2.11s
Test loss: 0.1242 score: 0.9521 time: 1.97s
Epoch 51/1000, LR 0.000298
Train loss: 0.6113;  Loss pred: 0.6113; Loss self: 0.0000; time: 6.16s
Val loss: 0.1268 score: 0.9580 time: 2.03s
Test loss: 0.1228 score: 0.9521 time: 2.08s
Epoch 52/1000, LR 0.000298
Train loss: 0.6103;  Loss pred: 0.6103; Loss self: 0.0000; time: 6.44s
Val loss: 0.1256 score: 0.9580 time: 1.96s
Test loss: 0.1214 score: 0.9521 time: 1.88s
Epoch 53/1000, LR 0.000298
Train loss: 0.6078;  Loss pred: 0.6078; Loss self: 0.0000; time: 6.07s
Val loss: 0.1249 score: 0.9580 time: 1.98s
Test loss: 0.1205 score: 0.9521 time: 1.89s
Epoch 54/1000, LR 0.000297
Train loss: 0.6068;  Loss pred: 0.6068; Loss self: 0.0000; time: 6.01s
Val loss: 0.1234 score: 0.9574 time: 1.87s
Test loss: 0.1188 score: 0.9527 time: 1.82s
Epoch 55/1000, LR 0.000297
Train loss: 0.6054;  Loss pred: 0.6054; Loss self: 0.0000; time: 6.05s
Val loss: 0.1227 score: 0.9580 time: 1.97s
Test loss: 0.1178 score: 0.9527 time: 1.87s
Epoch 56/1000, LR 0.000297
Train loss: 0.6036;  Loss pred: 0.6036; Loss self: 0.0000; time: 6.10s
Val loss: 0.1216 score: 0.9586 time: 2.12s
Test loss: 0.1165 score: 0.9527 time: 1.94s
Epoch 57/1000, LR 0.000297
Train loss: 0.6024;  Loss pred: 0.6024; Loss self: 0.0000; time: 6.33s
Val loss: 0.1206 score: 0.9598 time: 2.16s
Test loss: 0.1153 score: 0.9521 time: 2.07s
Epoch 58/1000, LR 0.000297
Train loss: 0.6008;  Loss pred: 0.6008; Loss self: 0.0000; time: 6.60s
Val loss: 0.1197 score: 0.9604 time: 2.05s
Test loss: 0.1142 score: 0.9544 time: 1.92s
Epoch 59/1000, LR 0.000297
Train loss: 0.5994;  Loss pred: 0.5994; Loss self: 0.0000; time: 6.54s
Val loss: 0.1187 score: 0.9633 time: 1.97s
Test loss: 0.1130 score: 0.9598 time: 1.90s
Epoch 60/1000, LR 0.000297
Train loss: 0.5976;  Loss pred: 0.5976; Loss self: 0.0000; time: 6.38s
Val loss: 0.1182 score: 0.9633 time: 2.23s
Test loss: 0.1124 score: 0.9609 time: 2.19s
Epoch 61/1000, LR 0.000297
Train loss: 0.5951;  Loss pred: 0.5951; Loss self: 0.0000; time: 6.51s
Val loss: 0.1174 score: 0.9633 time: 2.08s
Test loss: 0.1113 score: 0.9609 time: 1.96s
Epoch 62/1000, LR 0.000297
Train loss: 0.5949;  Loss pred: 0.5949; Loss self: 0.0000; time: 6.12s
Val loss: 0.1164 score: 0.9633 time: 1.95s
Test loss: 0.1102 score: 0.9615 time: 1.88s
Epoch 63/1000, LR 0.000297
Train loss: 0.5935;  Loss pred: 0.5935; Loss self: 0.0000; time: 6.34s
Val loss: 0.1158 score: 0.9633 time: 2.04s
Test loss: 0.1094 score: 0.9615 time: 1.95s
Epoch 64/1000, LR 0.000297
Train loss: 0.5913;  Loss pred: 0.5913; Loss self: 0.0000; time: 6.20s
Val loss: 0.1154 score: 0.9633 time: 2.11s
Test loss: 0.1088 score: 0.9615 time: 2.02s
Epoch 65/1000, LR 0.000297
Train loss: 0.5903;  Loss pred: 0.5903; Loss self: 0.0000; time: 6.44s
Val loss: 0.1151 score: 0.9633 time: 2.24s
Test loss: 0.1083 score: 0.9604 time: 2.17s
Epoch 66/1000, LR 0.000297
Train loss: 0.5893;  Loss pred: 0.5893; Loss self: 0.0000; time: 6.37s
Val loss: 0.1140 score: 0.9627 time: 2.06s
Test loss: 0.1070 score: 0.9615 time: 1.91s
Epoch 67/1000, LR 0.000297
Train loss: 0.5884;  Loss pred: 0.5884; Loss self: 0.0000; time: 6.16s
Val loss: 0.1137 score: 0.9627 time: 1.99s
Test loss: 0.1065 score: 0.9615 time: 1.91s
Epoch 68/1000, LR 0.000296
Train loss: 0.5853;  Loss pred: 0.5853; Loss self: 0.0000; time: 6.25s
Val loss: 0.1129 score: 0.9633 time: 2.01s
Test loss: 0.1055 score: 0.9615 time: 1.90s
Epoch 69/1000, LR 0.000296
Train loss: 0.5842;  Loss pred: 0.5842; Loss self: 0.0000; time: 6.56s
Val loss: 0.1126 score: 0.9627 time: 2.14s
Test loss: 0.1050 score: 0.9609 time: 1.92s
Epoch 70/1000, LR 0.000296
Train loss: 0.5831;  Loss pred: 0.5831; Loss self: 0.0000; time: 6.19s
Val loss: 0.1128 score: 0.9621 time: 2.17s
Test loss: 0.1052 score: 0.9609 time: 2.00s
     INFO: Early stopping counter 1 of 2
Epoch 71/1000, LR 0.000296
Train loss: 0.5818;  Loss pred: 0.5818; Loss self: 0.0000; time: 6.12s
Val loss: 0.1123 score: 0.9633 time: 2.10s
Test loss: 0.1045 score: 0.9609 time: 2.03s
Epoch 72/1000, LR 0.000296
Train loss: 0.5800;  Loss pred: 0.5800; Loss self: 0.0000; time: 6.53s
Val loss: 0.1116 score: 0.9633 time: 2.11s
Test loss: 0.1035 score: 0.9621 time: 2.02s
Epoch 73/1000, LR 0.000296
Train loss: 0.5788;  Loss pred: 0.5788; Loss self: 0.0000; time: 6.55s
Val loss: 0.1110 score: 0.9639 time: 2.10s
Test loss: 0.1027 score: 0.9627 time: 2.03s
Epoch 74/1000, LR 0.000296
Train loss: 0.5776;  Loss pred: 0.5776; Loss self: 0.0000; time: 6.38s
Val loss: 0.1111 score: 0.9639 time: 1.95s
Test loss: 0.1027 score: 0.9621 time: 1.97s
     INFO: Early stopping counter 1 of 2
Epoch 75/1000, LR 0.000296
Train loss: 0.5762;  Loss pred: 0.5762; Loss self: 0.0000; time: 6.17s
Val loss: 0.1107 score: 0.9639 time: 2.03s
Test loss: 0.1021 score: 0.9627 time: 1.93s
Epoch 76/1000, LR 0.000296
Train loss: 0.5740;  Loss pred: 0.5740; Loss self: 0.0000; time: 6.29s
Val loss: 0.1107 score: 0.9639 time: 2.10s
Test loss: 0.1020 score: 0.9621 time: 1.94s
     INFO: Early stopping counter 1 of 2
Epoch 77/1000, LR 0.000296
Train loss: 0.5726;  Loss pred: 0.5726; Loss self: 0.0000; time: 6.30s
Val loss: 0.1106 score: 0.9639 time: 2.07s
Test loss: 0.1018 score: 0.9621 time: 2.00s
Epoch 78/1000, LR 0.000296
Train loss: 0.5705;  Loss pred: 0.5705; Loss self: 0.0000; time: 6.40s
Val loss: 0.1099 score: 0.9645 time: 2.02s
Test loss: 0.1007 score: 0.9627 time: 1.87s
Epoch 79/1000, LR 0.000295
Train loss: 0.5694;  Loss pred: 0.5694; Loss self: 0.0000; time: 6.58s
Val loss: 0.1103 score: 0.9639 time: 2.24s
Test loss: 0.1012 score: 0.9621 time: 2.07s
     INFO: Early stopping counter 1 of 2
Epoch 80/1000, LR 0.000295
Train loss: 0.5679;  Loss pred: 0.5679; Loss self: 0.0000; time: 6.37s
Val loss: 0.1102 score: 0.9645 time: 2.25s
Test loss: 0.1010 score: 0.9621 time: 2.25s
     INFO: Early stopping counter 2 of 2
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 077,   Train_Loss: 0.5705,   Val_Loss: 0.1099,   Val_Precision: 0.9769,   Val_Recall: 0.9515,   Val_accuracy: 0.9640,   Val_Score: 0.9645,   Val_Loss: 0.1099,   Test_Precision: 0.9815,   Test_Recall: 0.9432,   Test_accuracy: 0.9620,   Test_Score: 0.9627,   Test_loss: 0.1007


[2.0911515819607303, 2.262356181978248, 2.382134042913094, 1.9260399850318208, 1.9420033859787509, 1.949627339025028, 1.9503265239764005, 1.93740049097687, 1.8235668370034546, 1.9274736819788814, 1.9744480550289154, 1.9711262419587001, 1.9495742550352588, 1.936083855922334, 2.0390239630360156, 2.1134655410423875, 2.50531380798202, 1.9463682409841567, 2.02270834101364, 2.05961313797161, 2.1820704720448703, 1.9185140179470181, 1.9570099740521982, 1.8893659439636394, 1.9139319129753858, 1.89530236099381, 1.9002253259532154, 2.1445616639684886, 1.9601782710524276, 1.939203055924736, 1.9083201590692624, 1.9112083970103413, 1.9451326000271365, 2.058885319973342, 1.9855987490154803, 2.0574006799142808, 2.056829118053429, 1.9068163329502568, 2.048990988987498, 1.9785629370016977, 1.9352274000411853, 1.9224203949561343, 1.9756166510051116, 2.1002364720916376, 1.875777932931669, 1.8787151109427214, 2.0119841770501807, 2.0336251279804856, 2.1386081079253927, 1.9790542129194364, 2.0871985820122063, 1.8826108030043542, 1.8963146050227806, 1.8284112400142476, 1.8726877379231155, 1.9428857309976593, 2.0710763239767402, 1.9271774640074, 1.9061481968965381, 2.190437792916782, 1.9602605249965563, 1.8881705680396408, 1.954546806984581, 2.0275450199842453, 2.1756654910277575, 1.911388255073689, 1.9186397739686072, 1.9016780820675194, 1.9264403861016035, 2.006570341065526, 2.036801574053243, 2.0271045939298347, 2.032473846920766, 1.9734895959263667, 1.9391338069690391, 1.9446903700008988, 2.0045766240218654, 1.8703945969464257, 2.072724673897028, 2.2512077030260116]
[0.0012373677999767635, 0.001338672297028549, 0.0014095467709544936, 0.0011396686301963436, 0.0011491144295732254, 0.0011536256443935077, 0.0011540393632996453, 0.0011463908230632367, 0.0010790336313629908, 0.0011405169715851368, 0.0011683124585969915, 0.0011663468887329587, 0.0011535942337486738, 0.0011456117490664698, 0.001206523055050897, 0.0012505713260605844, 0.0014824342059065207, 0.001151697184014294, 0.0011968688408364734, 0.001218705998799769, 0.001291165959789864, 0.001135215395234922, 0.0011579940674865078, 0.0011179680141796683, 0.0011325040905179797, 0.0011214806869785858, 0.0011243936839959855, 0.0012689713987979222, 0.0011598687994393063, 0.0011474574295412638, 0.0011291835260764866, 0.0011308925426096693, 0.0011509660355190157, 0.0012182753372623326, 0.0011749105023760237, 0.0012173968520202845, 0.0012170586497357568, 0.0011282936881362466, 0.0012124207035428983, 0.0011707473000010045, 0.0011451049704385712, 0.0011375268609207896, 0.0011690039355059833, 0.0012427434746104364, 0.0011099277709654847, 0.0011116657461199535, 0.0011905231816864974, 0.0012033284780949619, 0.0012654485845712384, 0.0011710379958103174, 0.0012350287467527848, 0.0011139708893516888, 0.0011220796479424738, 0.001081900142020265, 0.001108099253208944, 0.0011496365272175499, 0.0012254889490986628, 0.0011403416946789348, 0.0011278983413588984, 0.0012961170372288652, 0.0011599174704121635, 0.0011172606911477164, 0.0011565365721802254, 0.0011997307810557665, 0.0012873760301939394, 0.0011309989674992242, 0.001135289807082016, 0.0011252533029985322, 0.0011399055539062742, 0.0011873197284411397, 0.0012052080319841674, 0.001199470173922979, 0.001202647246698678, 0.001167745323033353, 0.0011474164538278339, 0.0011507043609472775, 0.0011861400142141217, 0.0011067423650570566, 0.0012264643040810817, 0.0013320755639207168]
[808.1671432041297, 747.0088103113062, 709.4479024082589, 877.4480348974058, 870.2353519060712, 866.8323254254001, 866.5215691956869, 872.3028655514966, 926.7551732718852, 876.7953699190984, 855.9354072119412, 857.3778604462461, 866.8559279725591, 872.8960756686329, 828.8279248487426, 799.6345183685706, 674.5661939097606, 868.2837935875242, 835.5134379646104, 820.5424450071145, 774.493776278573, 880.8900973308766, 863.5622824653644, 894.4799737707793, 882.9990181692186, 891.6783067340459, 889.3682117157556, 788.0398257575271, 862.1664799358439, 871.4920259828592, 885.5956333995115, 884.257312098266, 868.8353688465367, 820.8325075735066, 851.1286587171517, 821.4248281819426, 821.6530897809373, 886.2940655564896, 824.7962090038804, 854.1552903851599, 873.2823852969598, 879.1001200538982, 855.4291133050525, 804.6712941409494, 900.9595274205433, 899.5509697859268, 839.9668443107491, 831.0282838008958, 790.2336074276949, 853.9432568180974, 809.6977520800733, 897.6895263232436, 891.2023329481755, 924.2997215368414, 902.446235844037, 869.8401419275419, 816.0008303098056, 876.9301382789057, 886.6047260918873, 771.5352636194245, 862.1303028090967, 895.0462572640416, 864.6505644995443, 833.5203328866808, 776.7738225243738, 884.1741051374443, 880.832359950676, 888.6887933012398, 877.2656616797416, 842.2331205705844, 829.7322731526044, 833.7014306319987, 831.4990141498647, 856.3511069369001, 871.5231480810252, 869.0329453316616, 843.0707909829272, 903.5526528782029, 815.3518994988133, 750.7081633242229]
Elapsed: 1.9943453808911726~0.11675620426736516
Time per graph: 0.0011800860241959602~6.908651140080779e-05
Speed: 850.0542738709125~45.60771177919699
Total Time: 2.2517
best val loss: 0.10987664677158615 test_score: 0.9627

Testing...
Test loss: 0.1007 score: 0.9627 time: 2.12s
test Score 0.9627
Epoch Time List: [11.757770439027809, 11.711095330072567, 12.210727476980537, 10.387575979111716, 10.368071971926838, 10.569913334096782, 10.213528512045741, 10.248894623946398, 9.83224269701168, 10.173475824994966, 10.300324118928984, 10.688114683958702, 10.441479033092037, 10.329199391999282, 10.638811875949614, 10.927575593115762, 11.80326841888018, 10.698172340053134, 10.509322443162091, 11.284602698870003, 10.971020247088745, 10.272307842154987, 10.88901650311891, 10.415233571082354, 10.290869105025195, 10.170538150938228, 10.170974940992892, 10.811731514986604, 10.69054230605252, 10.457778733922169, 10.246139632887207, 10.13686288392637, 10.173006555996835, 10.51213109085802, 10.33895348990336, 11.08825956005603, 10.709281761897728, 10.062379723996855, 10.26445868902374, 10.606364047038369, 10.393126470036805, 10.217317195842043, 10.494951745029539, 10.760982494917698, 10.072795080021024, 10.158271224005148, 10.294752364978194, 10.319614356034435, 10.86182383808773, 10.726872592931613, 10.274099923088215, 10.270078278030269, 9.938920169021003, 9.708534673904069, 9.888081807992421, 10.154748100205325, 10.553793914034031, 10.575228411122225, 10.411127954022959, 10.801363590057008, 10.544778609997593, 9.95128027012106, 10.33606983604841, 10.32859040016774, 10.854004182037897, 10.336923647089861, 10.069440645980649, 10.156840395997278, 10.614505616831593, 10.36516901885625, 10.251510736881755, 10.663879850995727, 10.67920849705115, 10.305540462955832, 10.13791953411419, 10.321850977023132, 10.366543298936449, 10.281779804150574, 10.883393621072173, 10.871895757038146]
Total Epoch List: [80]
Total Time List: [2.251737711019814]
========================k_idx:1========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: AEtransGAT
AEtransGAT(
  (gae): GAE(
    (encoder): transGAT(
      (encoder): EdgeFeatureGAT()
      (classifier): Sequential(
        (0): Linear(in_features=64, out_features=64, bias=True)
        (1): ReLU(inplace=True)
        (2): Linear(in_features=64, out_features=2, bias=True)
      )
    )
    (decoder): InnerProductDecoder()
  )
)
Total number of parameters: 1914242
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x71f20acacc10>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 2.7622;  Loss pred: 2.7622; Loss self: 0.0000; time: 6.67s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6991 score: 0.5000 time: 2.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6991 score: 0.5000 time: 2.36s
Epoch 2/1000, LR 0.000029
Train loss: 2.6257;  Loss pred: 2.6257; Loss self: 0.0000; time: 6.55s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6987 score: 0.5000 time: 2.64s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6988 score: 0.5000 time: 2.59s
Epoch 3/1000, LR 0.000059
Train loss: 2.3854;  Loss pred: 2.3854; Loss self: 0.0000; time: 6.43s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6983 score: 0.5000 time: 2.00s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6984 score: 0.5000 time: 2.03s
Epoch 4/1000, LR 0.000089
Train loss: 2.0672;  Loss pred: 2.0672; Loss self: 0.0000; time: 6.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6978 score: 0.5000 time: 2.00s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6978 score: 0.5000 time: 2.10s
Epoch 5/1000, LR 0.000119
Train loss: 1.7447;  Loss pred: 1.7447; Loss self: 0.0000; time: 6.33s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6972 score: 0.5000 time: 1.94s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6973 score: 0.5000 time: 2.11s
Epoch 6/1000, LR 0.000149
Train loss: 1.4731;  Loss pred: 1.4731; Loss self: 0.0000; time: 6.68s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6966 score: 0.5000 time: 2.34s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6967 score: 0.5000 time: 2.41s
Epoch 7/1000, LR 0.000179
Train loss: 1.2723;  Loss pred: 1.2723; Loss self: 0.0000; time: 6.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6959 score: 0.5000 time: 2.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6960 score: 0.5000 time: 2.09s
Epoch 8/1000, LR 0.000209
Train loss: 1.1459;  Loss pred: 1.1459; Loss self: 0.0000; time: 6.29s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6949 score: 0.5000 time: 2.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6950 score: 0.5000 time: 2.09s
Epoch 9/1000, LR 0.000239
Train loss: 1.0743;  Loss pred: 1.0743; Loss self: 0.0000; time: 6.30s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6940 score: 0.5000 time: 2.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6941 score: 0.5000 time: 2.16s
Epoch 10/1000, LR 0.000269
Train loss: 1.0344;  Loss pred: 1.0344; Loss self: 0.0000; time: 6.68s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6925 score: 0.5000 time: 2.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6927 score: 0.5000 time: 2.23s
Epoch 11/1000, LR 0.000299
Train loss: 1.0104;  Loss pred: 1.0104; Loss self: 0.0000; time: 6.52s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6906 score: 0.5000 time: 2.47s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6909 score: 0.5000 time: 2.48s
Epoch 12/1000, LR 0.000299
Train loss: 0.9956;  Loss pred: 0.9956; Loss self: 0.0000; time: 6.40s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6875 score: 0.5000 time: 2.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6881 score: 0.5000 time: 2.09s
Epoch 13/1000, LR 0.000299
Train loss: 0.9870;  Loss pred: 0.9870; Loss self: 0.0000; time: 6.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6833 score: 0.5000 time: 2.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6840 score: 0.5000 time: 2.13s
Epoch 14/1000, LR 0.000299
Train loss: 0.9791;  Loss pred: 0.9791; Loss self: 0.0000; time: 6.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6764 score: 0.5000 time: 2.09s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6775 score: 0.5000 time: 2.09s
Epoch 15/1000, LR 0.000299
Train loss: 0.9722;  Loss pred: 0.9722; Loss self: 0.0000; time: 6.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6665 score: 0.5000 time: 2.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6681 score: 0.5000 time: 2.06s
Epoch 16/1000, LR 0.000299
Train loss: 0.9657;  Loss pred: 0.9657; Loss self: 0.0000; time: 6.03s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6537 score: 0.5000 time: 2.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6559 score: 0.5000 time: 2.10s
Epoch 17/1000, LR 0.000299
Train loss: 0.9552;  Loss pred: 0.9552; Loss self: 0.0000; time: 6.37s
Val loss: 0.6373 score: 0.5290 time: 2.19s
Test loss: 0.6402 score: 0.5225 time: 2.12s
Epoch 18/1000, LR 0.000299
Train loss: 0.9460;  Loss pred: 0.9460; Loss self: 0.0000; time: 6.28s
Val loss: 0.6168 score: 0.6669 time: 2.10s
Test loss: 0.6204 score: 0.6408 time: 2.14s
Epoch 19/1000, LR 0.000299
Train loss: 0.9330;  Loss pred: 0.9330; Loss self: 0.0000; time: 6.27s
Val loss: 0.5924 score: 0.7675 time: 2.15s
Test loss: 0.5970 score: 0.7586 time: 2.15s
Epoch 20/1000, LR 0.000299
Train loss: 0.9186;  Loss pred: 0.9186; Loss self: 0.0000; time: 6.17s
Val loss: 0.5638 score: 0.8314 time: 2.05s
Test loss: 0.5693 score: 0.8172 time: 2.07s
Epoch 21/1000, LR 0.000299
Train loss: 0.9018;  Loss pred: 0.9018; Loss self: 0.0000; time: 6.00s
Val loss: 0.5318 score: 0.8633 time: 2.03s
Test loss: 0.5383 score: 0.8598 time: 2.10s
Epoch 22/1000, LR 0.000299
Train loss: 0.8831;  Loss pred: 0.8831; Loss self: 0.0000; time: 6.10s
Val loss: 0.4970 score: 0.8870 time: 2.07s
Test loss: 0.5044 score: 0.8817 time: 2.11s
Epoch 23/1000, LR 0.000299
Train loss: 0.8625;  Loss pred: 0.8625; Loss self: 0.0000; time: 6.51s
Val loss: 0.4587 score: 0.9036 time: 2.15s
Test loss: 0.4670 score: 0.8982 time: 2.15s
Epoch 24/1000, LR 0.000299
Train loss: 0.8408;  Loss pred: 0.8408; Loss self: 0.0000; time: 6.20s
Val loss: 0.4185 score: 0.9071 time: 2.11s
Test loss: 0.4275 score: 0.9018 time: 2.13s
Epoch 25/1000, LR 0.000299
Train loss: 0.8179;  Loss pred: 0.8179; Loss self: 0.0000; time: 6.85s
Val loss: 0.3787 score: 0.9154 time: 2.19s
Test loss: 0.3881 score: 0.9118 time: 2.24s
Epoch 26/1000, LR 0.000299
Train loss: 0.7935;  Loss pred: 0.7935; Loss self: 0.0000; time: 6.54s
Val loss: 0.3408 score: 0.9207 time: 2.37s
Test loss: 0.3504 score: 0.9148 time: 2.50s
Epoch 27/1000, LR 0.000299
Train loss: 0.7733;  Loss pred: 0.7733; Loss self: 0.0000; time: 6.28s
Val loss: 0.3082 score: 0.9231 time: 2.19s
Test loss: 0.3174 score: 0.9213 time: 2.07s
Epoch 28/1000, LR 0.000299
Train loss: 0.7517;  Loss pred: 0.7517; Loss self: 0.0000; time: 6.19s
Val loss: 0.2811 score: 0.9284 time: 2.13s
Test loss: 0.2899 score: 0.9266 time: 2.27s
Epoch 29/1000, LR 0.000299
Train loss: 0.7336;  Loss pred: 0.7336; Loss self: 0.0000; time: 6.14s
Val loss: 0.2589 score: 0.9302 time: 2.06s
Test loss: 0.2671 score: 0.9308 time: 2.07s
Epoch 30/1000, LR 0.000299
Train loss: 0.7190;  Loss pred: 0.7190; Loss self: 0.0000; time: 6.24s
Val loss: 0.2405 score: 0.9361 time: 2.00s
Test loss: 0.2480 score: 0.9331 time: 2.02s
Epoch 31/1000, LR 0.000299
Train loss: 0.7055;  Loss pred: 0.7055; Loss self: 0.0000; time: 5.94s
Val loss: 0.2254 score: 0.9391 time: 2.03s
Test loss: 0.2322 score: 0.9343 time: 2.03s
Epoch 32/1000, LR 0.000299
Train loss: 0.6939;  Loss pred: 0.6939; Loss self: 0.0000; time: 6.64s
Val loss: 0.2129 score: 0.9408 time: 2.18s
Test loss: 0.2189 score: 0.9379 time: 2.21s
Epoch 33/1000, LR 0.000299
Train loss: 0.6850;  Loss pred: 0.6850; Loss self: 0.0000; time: 6.50s
Val loss: 0.2018 score: 0.9420 time: 2.40s
Test loss: 0.2070 score: 0.9420 time: 2.34s
Epoch 34/1000, LR 0.000298
Train loss: 0.6758;  Loss pred: 0.6758; Loss self: 0.0000; time: 6.53s
Val loss: 0.1926 score: 0.9320 time: 2.07s
Test loss: 0.1969 score: 0.9266 time: 2.10s
Epoch 35/1000, LR 0.000298
Train loss: 0.6671;  Loss pred: 0.6671; Loss self: 0.0000; time: 6.51s
Val loss: 0.1849 score: 0.9302 time: 2.21s
Test loss: 0.1884 score: 0.9284 time: 2.17s
Epoch 36/1000, LR 0.000298
Train loss: 0.6616;  Loss pred: 0.6616; Loss self: 0.0000; time: 6.36s
Val loss: 0.1781 score: 0.9302 time: 2.29s
Test loss: 0.1808 score: 0.9302 time: 2.30s
Epoch 37/1000, LR 0.000298
Train loss: 0.6546;  Loss pred: 0.6546; Loss self: 0.0000; time: 6.45s
Val loss: 0.1725 score: 0.9320 time: 2.24s
Test loss: 0.1744 score: 0.9337 time: 2.21s
Epoch 38/1000, LR 0.000298
Train loss: 0.6498;  Loss pred: 0.6498; Loss self: 0.0000; time: 6.05s
Val loss: 0.1674 score: 0.9337 time: 2.13s
Test loss: 0.1683 score: 0.9349 time: 2.09s
Epoch 39/1000, LR 0.000298
Train loss: 0.6442;  Loss pred: 0.6442; Loss self: 0.0000; time: 6.16s
Val loss: 0.1629 score: 0.9343 time: 2.05s
Test loss: 0.1630 score: 0.9373 time: 2.38s
Epoch 40/1000, LR 0.000298
Train loss: 0.6412;  Loss pred: 0.6412; Loss self: 0.0000; time: 6.01s
Val loss: 0.1589 score: 0.9343 time: 1.92s
Test loss: 0.1581 score: 0.9373 time: 1.95s
Epoch 41/1000, LR 0.000298
Train loss: 0.6373;  Loss pred: 0.6373; Loss self: 0.0000; time: 6.41s
Val loss: 0.1551 score: 0.9355 time: 2.09s
Test loss: 0.1536 score: 0.9373 time: 2.05s
Epoch 42/1000, LR 0.000298
Train loss: 0.6342;  Loss pred: 0.6342; Loss self: 0.0000; time: 6.20s
Val loss: 0.1518 score: 0.9379 time: 2.21s
Test loss: 0.1494 score: 0.9385 time: 2.28s
Epoch 43/1000, LR 0.000298
Train loss: 0.6303;  Loss pred: 0.6303; Loss self: 0.0000; time: 6.49s
Val loss: 0.1485 score: 0.9396 time: 2.24s
Test loss: 0.1453 score: 0.9396 time: 2.19s
Epoch 44/1000, LR 0.000298
Train loss: 0.6277;  Loss pred: 0.6277; Loss self: 0.0000; time: 6.22s
Val loss: 0.1455 score: 0.9414 time: 2.11s
Test loss: 0.1415 score: 0.9396 time: 2.16s
Epoch 45/1000, LR 0.000298
Train loss: 0.6242;  Loss pred: 0.6242; Loss self: 0.0000; time: 6.42s
Val loss: 0.1427 score: 0.9550 time: 2.20s
Test loss: 0.1381 score: 0.9592 time: 2.36s
Epoch 46/1000, LR 0.000298
Train loss: 0.6221;  Loss pred: 0.6221; Loss self: 0.0000; time: 6.61s
Val loss: 0.1402 score: 0.9550 time: 2.27s
Test loss: 0.1348 score: 0.9592 time: 2.29s
Epoch 47/1000, LR 0.000298
Train loss: 0.6191;  Loss pred: 0.6191; Loss self: 0.0000; time: 6.58s
Val loss: 0.1379 score: 0.9562 time: 1.99s
Test loss: 0.1319 score: 0.9604 time: 2.03s
Epoch 48/1000, LR 0.000298
Train loss: 0.6175;  Loss pred: 0.6175; Loss self: 0.0000; time: 6.68s
Val loss: 0.1357 score: 0.9568 time: 2.35s
Test loss: 0.1291 score: 0.9609 time: 2.51s
Epoch 49/1000, LR 0.000298
Train loss: 0.6149;  Loss pred: 0.6149; Loss self: 0.0000; time: 6.85s
Val loss: 0.1337 score: 0.9598 time: 2.32s
Test loss: 0.1264 score: 0.9645 time: 2.28s
Epoch 50/1000, LR 0.000298
Train loss: 0.6117;  Loss pred: 0.6117; Loss self: 0.0000; time: 6.17s
Val loss: 0.1319 score: 0.9609 time: 2.05s
Test loss: 0.1241 score: 0.9669 time: 2.22s
Epoch 51/1000, LR 0.000298
Train loss: 0.6101;  Loss pred: 0.6101; Loss self: 0.0000; time: 6.19s
Val loss: 0.1303 score: 0.9615 time: 2.05s
Test loss: 0.1219 score: 0.9669 time: 2.20s
Epoch 52/1000, LR 0.000298
Train loss: 0.6088;  Loss pred: 0.6088; Loss self: 0.0000; time: 6.33s
Val loss: 0.1288 score: 0.9609 time: 2.10s
Test loss: 0.1199 score: 0.9680 time: 2.14s
Epoch 53/1000, LR 0.000298
Train loss: 0.6061;  Loss pred: 0.6061; Loss self: 0.0000; time: 6.64s
Val loss: 0.1274 score: 0.9609 time: 2.14s
Test loss: 0.1179 score: 0.9686 time: 2.17s
Epoch 54/1000, LR 0.000297
Train loss: 0.6050;  Loss pred: 0.6050; Loss self: 0.0000; time: 6.34s
Val loss: 0.1260 score: 0.9627 time: 2.15s
Test loss: 0.1159 score: 0.9692 time: 2.34s
Epoch 55/1000, LR 0.000297
Train loss: 0.6024;  Loss pred: 0.6024; Loss self: 0.0000; time: 6.58s
Val loss: 0.1249 score: 0.9609 time: 2.19s
Test loss: 0.1143 score: 0.9686 time: 2.23s
Epoch 56/1000, LR 0.000297
Train loss: 0.6017;  Loss pred: 0.6017; Loss self: 0.0000; time: 6.48s
Val loss: 0.1237 score: 0.9621 time: 2.19s
Test loss: 0.1126 score: 0.9686 time: 2.17s
Epoch 57/1000, LR 0.000297
Train loss: 0.6001;  Loss pred: 0.6001; Loss self: 0.0000; time: 6.36s
Val loss: 0.1227 score: 0.9609 time: 2.11s
Test loss: 0.1112 score: 0.9680 time: 2.10s
Epoch 58/1000, LR 0.000297
Train loss: 0.5990;  Loss pred: 0.5990; Loss self: 0.0000; time: 6.14s
Val loss: 0.1215 score: 0.9621 time: 2.06s
Test loss: 0.1095 score: 0.9686 time: 2.06s
Epoch 59/1000, LR 0.000297
Train loss: 0.5964;  Loss pred: 0.5964; Loss self: 0.0000; time: 6.01s
Val loss: 0.1204 score: 0.9621 time: 2.03s
Test loss: 0.1080 score: 0.9692 time: 2.06s
Epoch 60/1000, LR 0.000297
Train loss: 0.5942;  Loss pred: 0.5942; Loss self: 0.0000; time: 6.15s
Val loss: 0.1196 score: 0.9621 time: 2.07s
Test loss: 0.1068 score: 0.9686 time: 2.08s
Epoch 61/1000, LR 0.000297
Train loss: 0.5937;  Loss pred: 0.5937; Loss self: 0.0000; time: 6.07s
Val loss: 0.1185 score: 0.9621 time: 2.07s
Test loss: 0.1052 score: 0.9686 time: 2.11s
Epoch 62/1000, LR 0.000297
Train loss: 0.5912;  Loss pred: 0.5912; Loss self: 0.0000; time: 6.27s
Val loss: 0.1180 score: 0.9621 time: 2.19s
Test loss: 0.1043 score: 0.9675 time: 2.05s
Epoch 63/1000, LR 0.000297
Train loss: 0.5914;  Loss pred: 0.5914; Loss self: 0.0000; time: 6.33s
Val loss: 0.1168 score: 0.9621 time: 2.15s
Test loss: 0.1026 score: 0.9686 time: 2.13s
Epoch 64/1000, LR 0.000297
Train loss: 0.5883;  Loss pred: 0.5883; Loss self: 0.0000; time: 6.26s
Val loss: 0.1161 score: 0.9621 time: 2.07s
Test loss: 0.1017 score: 0.9680 time: 2.06s
Epoch 65/1000, LR 0.000297
Train loss: 0.5879;  Loss pred: 0.5879; Loss self: 0.0000; time: 6.10s
Val loss: 0.1153 score: 0.9621 time: 2.02s
Test loss: 0.1006 score: 0.9675 time: 2.07s
Epoch 66/1000, LR 0.000297
Train loss: 0.5872;  Loss pred: 0.5872; Loss self: 0.0000; time: 6.06s
Val loss: 0.1144 score: 0.9627 time: 2.07s
Test loss: 0.0994 score: 0.9675 time: 2.08s
Epoch 67/1000, LR 0.000297
Train loss: 0.5845;  Loss pred: 0.5845; Loss self: 0.0000; time: 5.99s
Val loss: 0.1136 score: 0.9627 time: 2.02s
Test loss: 0.0983 score: 0.9680 time: 2.18s
Epoch 68/1000, LR 0.000296
Train loss: 0.5834;  Loss pred: 0.5834; Loss self: 0.0000; time: 6.30s
Val loss: 0.1125 score: 0.9627 time: 2.06s
Test loss: 0.0970 score: 0.9680 time: 2.08s
Epoch 69/1000, LR 0.000296
Train loss: 0.5814;  Loss pred: 0.5814; Loss self: 0.0000; time: 6.37s
Val loss: 0.1121 score: 0.9633 time: 2.22s
Test loss: 0.0963 score: 0.9675 time: 2.07s
Epoch 70/1000, LR 0.000296
Train loss: 0.5812;  Loss pred: 0.5812; Loss self: 0.0000; time: 6.30s
Val loss: 0.1112 score: 0.9627 time: 2.07s
Test loss: 0.0952 score: 0.9669 time: 2.08s
Epoch 71/1000, LR 0.000296
Train loss: 0.5787;  Loss pred: 0.5787; Loss self: 0.0000; time: 6.39s
Val loss: 0.1108 score: 0.9639 time: 2.10s
Test loss: 0.0946 score: 0.9663 time: 2.18s
Epoch 72/1000, LR 0.000296
Train loss: 0.5772;  Loss pred: 0.5772; Loss self: 0.0000; time: 6.25s
Val loss: 0.1093 score: 0.9627 time: 2.30s
Test loss: 0.0931 score: 0.9675 time: 2.25s
Epoch 73/1000, LR 0.000296
Train loss: 0.5741;  Loss pred: 0.5741; Loss self: 0.0000; time: 6.43s
Val loss: 0.1091 score: 0.9639 time: 2.21s
Test loss: 0.0928 score: 0.9669 time: 2.26s
Epoch 74/1000, LR 0.000296
Train loss: 0.5747;  Loss pred: 0.5747; Loss self: 0.0000; time: 6.32s
Val loss: 0.1088 score: 0.9633 time: 2.06s
Test loss: 0.0923 score: 0.9663 time: 2.32s
Epoch 75/1000, LR 0.000296
Train loss: 0.5730;  Loss pred: 0.5730; Loss self: 0.0000; time: 6.69s
Val loss: 0.1074 score: 0.9645 time: 2.46s
Test loss: 0.0908 score: 0.9686 time: 2.65s
Epoch 76/1000, LR 0.000296
Train loss: 0.5705;  Loss pred: 0.5705; Loss self: 0.0000; time: 6.51s
Val loss: 0.1072 score: 0.9722 time: 2.27s
Test loss: 0.0905 score: 0.9704 time: 2.13s
Epoch 77/1000, LR 0.000296
Train loss: 0.5696;  Loss pred: 0.5696; Loss self: 0.0000; time: 6.32s
Val loss: 0.1060 score: 0.9722 time: 2.11s
Test loss: 0.0892 score: 0.9710 time: 2.10s
Epoch 78/1000, LR 0.000296
Train loss: 0.5664;  Loss pred: 0.5664; Loss self: 0.0000; time: 6.05s
Val loss: 0.1057 score: 0.9728 time: 2.08s
Test loss: 0.0888 score: 0.9716 time: 2.17s
Epoch 79/1000, LR 0.000295
Train loss: 0.5649;  Loss pred: 0.5649; Loss self: 0.0000; time: 6.39s
Val loss: 0.1048 score: 0.9728 time: 2.14s
Test loss: 0.0877 score: 0.9716 time: 2.11s
Epoch 80/1000, LR 0.000295
Train loss: 0.5631;  Loss pred: 0.5631; Loss self: 0.0000; time: 6.05s
Val loss: 0.1043 score: 0.9728 time: 2.08s
Test loss: 0.0870 score: 0.9710 time: 2.08s
Epoch 81/1000, LR 0.000295
Train loss: 0.5621;  Loss pred: 0.5621; Loss self: 0.0000; time: 10.13s
Val loss: 0.1037 score: 0.9722 time: 2.36s
Test loss: 0.0862 score: 0.9710 time: 6.18s
Epoch 82/1000, LR 0.000295
Train loss: 0.5606;  Loss pred: 0.5606; Loss self: 0.0000; time: 13.58s
Val loss: 0.1033 score: 0.9722 time: 2.44s
Test loss: 0.0857 score: 0.9704 time: 2.51s
Epoch 83/1000, LR 0.000295
Train loss: 0.5583;  Loss pred: 0.5583; Loss self: 0.0000; time: 7.20s
Val loss: 0.1026 score: 0.9722 time: 2.54s
Test loss: 0.0849 score: 0.9704 time: 2.27s
Epoch 84/1000, LR 0.000295
Train loss: 0.5565;  Loss pred: 0.5565; Loss self: 0.0000; time: 6.34s
Val loss: 0.1026 score: 0.9722 time: 2.21s
Test loss: 0.0846 score: 0.9698 time: 2.24s
     INFO: Early stopping counter 1 of 2
Epoch 85/1000, LR 0.000295
Train loss: 0.5550;  Loss pred: 0.5550; Loss self: 0.0000; time: 6.91s
Val loss: 0.1016 score: 0.9722 time: 2.14s
Test loss: 0.0836 score: 0.9716 time: 2.12s
Epoch 86/1000, LR 0.000295
Train loss: 0.5526;  Loss pred: 0.5526; Loss self: 0.0000; time: 6.46s
Val loss: 0.1015 score: 0.9722 time: 2.01s
Test loss: 0.0832 score: 0.9716 time: 2.06s
Epoch 87/1000, LR 0.000295
Train loss: 0.5513;  Loss pred: 0.5513; Loss self: 0.0000; time: 6.52s
Val loss: 0.1013 score: 0.9722 time: 2.00s
Test loss: 0.0827 score: 0.9716 time: 1.92s
Epoch 88/1000, LR 0.000294
Train loss: 0.5498;  Loss pred: 0.5498; Loss self: 0.0000; time: 6.76s
Val loss: 0.1012 score: 0.9722 time: 2.40s
Test loss: 0.0822 score: 0.9722 time: 2.69s
Epoch 89/1000, LR 0.000294
Train loss: 0.5481;  Loss pred: 0.5481; Loss self: 0.0000; time: 6.97s
Val loss: 0.1013 score: 0.9722 time: 2.43s
Test loss: 0.0819 score: 0.9728 time: 2.38s
     INFO: Early stopping counter 1 of 2
Epoch 90/1000, LR 0.000294
Train loss: 0.5467;  Loss pred: 0.5467; Loss self: 0.0000; time: 6.36s
Val loss: 0.1015 score: 0.9722 time: 2.13s
Test loss: 0.0817 score: 0.9728 time: 2.16s
     INFO: Early stopping counter 2 of 2
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 087,   Train_Loss: 0.5498,   Val_Loss: 0.1012,   Val_Precision: 0.9807,   Val_Recall: 0.9633,   Val_accuracy: 0.9719,   Val_Score: 0.9722,   Val_Loss: 0.1012,   Test_Precision: 0.9819,   Test_Recall: 0.9621,   Test_accuracy: 0.9719,   Test_Score: 0.9722,   Test_loss: 0.0822


[2.0911515819607303, 2.262356181978248, 2.382134042913094, 1.9260399850318208, 1.9420033859787509, 1.949627339025028, 1.9503265239764005, 1.93740049097687, 1.8235668370034546, 1.9274736819788814, 1.9744480550289154, 1.9711262419587001, 1.9495742550352588, 1.936083855922334, 2.0390239630360156, 2.1134655410423875, 2.50531380798202, 1.9463682409841567, 2.02270834101364, 2.05961313797161, 2.1820704720448703, 1.9185140179470181, 1.9570099740521982, 1.8893659439636394, 1.9139319129753858, 1.89530236099381, 1.9002253259532154, 2.1445616639684886, 1.9601782710524276, 1.939203055924736, 1.9083201590692624, 1.9112083970103413, 1.9451326000271365, 2.058885319973342, 1.9855987490154803, 2.0574006799142808, 2.056829118053429, 1.9068163329502568, 2.048990988987498, 1.9785629370016977, 1.9352274000411853, 1.9224203949561343, 1.9756166510051116, 2.1002364720916376, 1.875777932931669, 1.8787151109427214, 2.0119841770501807, 2.0336251279804856, 2.1386081079253927, 1.9790542129194364, 2.0871985820122063, 1.8826108030043542, 1.8963146050227806, 1.8284112400142476, 1.8726877379231155, 1.9428857309976593, 2.0710763239767402, 1.9271774640074, 1.9061481968965381, 2.190437792916782, 1.9602605249965563, 1.8881705680396408, 1.954546806984581, 2.0275450199842453, 2.1756654910277575, 1.911388255073689, 1.9186397739686072, 1.9016780820675194, 1.9264403861016035, 2.006570341065526, 2.036801574053243, 2.0271045939298347, 2.032473846920766, 1.9734895959263667, 1.9391338069690391, 1.9446903700008988, 2.0045766240218654, 1.8703945969464257, 2.072724673897028, 2.2512077030260116, 2.3608328549889848, 2.5921790780266747, 2.039955380023457, 2.1023125830106437, 2.1198749219765887, 2.411651220987551, 2.100224480032921, 2.095808390993625, 2.1663294540485367, 2.2338018869049847, 2.4880670530255884, 2.0965340438997373, 2.134245907072909, 2.1002865799237043, 2.0641269319457933, 2.1043300569290295, 2.127098176977597, 2.1437043419573456, 2.158655039034784, 2.0736177229555324, 2.109577593044378, 2.1191818369552493, 2.1547510960372165, 2.135493017034605, 2.2452705230098218, 2.5010956540936604, 2.0775624880334362, 2.27227595099248, 2.0735319149680436, 2.0217101329471916, 2.039211638038978, 2.218156235991046, 2.3433312460547313, 2.1091049319365993, 2.1745592179941013, 2.3085187129909173, 2.2144775479100645, 2.0904955760343, 2.386603496968746, 1.95053765992634, 2.051344914943911, 2.288486283039674, 2.1904787240782753, 2.167036383994855, 2.3661639260826632, 2.299549176939763, 2.0352839659899473, 2.514203995000571, 2.2850577490171418, 2.22817907796707, 2.2088530510663986, 2.1402118840487674, 2.1757841209182516, 2.3475338369607925, 2.2326322130393237, 2.176075865048915, 2.1015322250314057, 2.063199218013324, 2.0635625610593706, 2.0878370130667463, 2.1110089449211955, 2.0556739929597825, 2.1389594450592995, 2.0638868029927835, 2.072864286019467, 2.086816471070051, 2.183490099036135, 2.089778908994049, 2.0790310960728675, 2.089877755031921, 2.1845617110375315, 2.250804358976893, 2.262258558999747, 2.3242153220344335, 2.659125708974898, 2.136636149021797, 2.1015458679758012, 2.178781416034326, 2.1191239670151845, 2.0810445020906627, 6.184110682923347, 2.5106263540219516, 2.27371508104261, 2.2431689728982747, 2.1291905550751835, 2.0627385400002822, 1.929378708009608, 2.698116194922477, 2.3831955429632217, 2.1657139339949936]
[0.0012373677999767635, 0.001338672297028549, 0.0014095467709544936, 0.0011396686301963436, 0.0011491144295732254, 0.0011536256443935077, 0.0011540393632996453, 0.0011463908230632367, 0.0010790336313629908, 0.0011405169715851368, 0.0011683124585969915, 0.0011663468887329587, 0.0011535942337486738, 0.0011456117490664698, 0.001206523055050897, 0.0012505713260605844, 0.0014824342059065207, 0.001151697184014294, 0.0011968688408364734, 0.001218705998799769, 0.001291165959789864, 0.001135215395234922, 0.0011579940674865078, 0.0011179680141796683, 0.0011325040905179797, 0.0011214806869785858, 0.0011243936839959855, 0.0012689713987979222, 0.0011598687994393063, 0.0011474574295412638, 0.0011291835260764866, 0.0011308925426096693, 0.0011509660355190157, 0.0012182753372623326, 0.0011749105023760237, 0.0012173968520202845, 0.0012170586497357568, 0.0011282936881362466, 0.0012124207035428983, 0.0011707473000010045, 0.0011451049704385712, 0.0011375268609207896, 0.0011690039355059833, 0.0012427434746104364, 0.0011099277709654847, 0.0011116657461199535, 0.0011905231816864974, 0.0012033284780949619, 0.0012654485845712384, 0.0011710379958103174, 0.0012350287467527848, 0.0011139708893516888, 0.0011220796479424738, 0.001081900142020265, 0.001108099253208944, 0.0011496365272175499, 0.0012254889490986628, 0.0011403416946789348, 0.0011278983413588984, 0.0012961170372288652, 0.0011599174704121635, 0.0011172606911477164, 0.0011565365721802254, 0.0011997307810557665, 0.0012873760301939394, 0.0011309989674992242, 0.001135289807082016, 0.0011252533029985322, 0.0011399055539062742, 0.0011873197284411397, 0.0012052080319841674, 0.001199470173922979, 0.001202647246698678, 0.001167745323033353, 0.0011474164538278339, 0.0011507043609472775, 0.0011861400142141217, 0.0011067423650570566, 0.0012264643040810817, 0.0013320755639207168, 0.0013969425177449615, 0.001533833773980281, 0.0012070741893629923, 0.0012439719426098484, 0.0012543638591577448, 0.0014270125567973674, 0.0012427363787177047, 0.0012401233082802515, 0.001281851747957714, 0.0013217762644408193, 0.001472229025458928, 0.0012405526886980695, 0.0012628674006348575, 0.0012427731242152097, 0.0012213768828081618, 0.0012451657141591892, 0.001258637974542957, 0.0012684641076670684, 0.0012773106739850792, 0.0012269927354766465, 0.0012482707651150166, 0.0012539537496776623, 0.0012750006485427315, 0.001263605335523435, 0.001328562439650782, 0.0014799382568601541, 0.0012293269159961162, 0.001344541982835787, 0.0012269419615195524, 0.001196278185175853, 0.0012066341053485077, 0.0013125184828349384, 0.00138658653612706, 0.0012479910839861535, 0.0012867214307657404, 0.0013659874041366375, 0.001310341744325482, 0.0012369796307895265, 0.0014121914183247018, 0.0011541642958144023, 0.001213813559138409, 0.001354133895289748, 0.0012961412568510504, 0.0012822700497010977, 0.0014000969976820492, 0.0013606799863548895, 0.001204310039047306, 0.001487694671597971, 0.00135210517693322, 0.0013184491585603965, 0.0013070136396842594, 0.0012663975645258979, 0.001287446225395415, 0.0013890732763081612, 0.0013210841497274105, 0.0012876188550585298, 0.0012435101923262755, 0.0012208279396528543, 0.0012210429355380891, 0.0012354065166075422, 0.0012491177188882813, 0.001216375143763185, 0.0012656564763664493, 0.001221234794670286, 0.0012265469148044182, 0.0012348026456035805, 0.0012920059757610266, 0.0012365555674521, 0.0012301959148360163, 0.0012366140562319059, 0.0012926400657026814, 0.0013318368988028953, 0.001338614531952513, 0.0013752753384819133, 0.0015734471650739042, 0.0012642817449833117, 0.0012435182650744386, 0.0012892197728013764, 0.0012539195071095766, 0.0012313872793435873, 0.0036592370904871873, 0.001485577724273344, 0.0013453935390784675, 0.0013273189188747188, 0.0012598760680918245, 0.0012205553491125931, 0.001141644205922845, 0.0015965184585340101, 0.0014101748774930306, 0.0012814875349082802]
[808.1671432041297, 747.0088103113062, 709.4479024082589, 877.4480348974058, 870.2353519060712, 866.8323254254001, 866.5215691956869, 872.3028655514966, 926.7551732718852, 876.7953699190984, 855.9354072119412, 857.3778604462461, 866.8559279725591, 872.8960756686329, 828.8279248487426, 799.6345183685706, 674.5661939097606, 868.2837935875242, 835.5134379646104, 820.5424450071145, 774.493776278573, 880.8900973308766, 863.5622824653644, 894.4799737707793, 882.9990181692186, 891.6783067340459, 889.3682117157556, 788.0398257575271, 862.1664799358439, 871.4920259828592, 885.5956333995115, 884.257312098266, 868.8353688465367, 820.8325075735066, 851.1286587171517, 821.4248281819426, 821.6530897809373, 886.2940655564896, 824.7962090038804, 854.1552903851599, 873.2823852969598, 879.1001200538982, 855.4291133050525, 804.6712941409494, 900.9595274205433, 899.5509697859268, 839.9668443107491, 831.0282838008958, 790.2336074276949, 853.9432568180974, 809.6977520800733, 897.6895263232436, 891.2023329481755, 924.2997215368414, 902.446235844037, 869.8401419275419, 816.0008303098056, 876.9301382789057, 886.6047260918873, 771.5352636194245, 862.1303028090967, 895.0462572640416, 864.6505644995443, 833.5203328866808, 776.7738225243738, 884.1741051374443, 880.832359950676, 888.6887933012398, 877.2656616797416, 842.2331205705844, 829.7322731526044, 833.7014306319987, 831.4990141498647, 856.3511069369001, 871.5231480810252, 869.0329453316616, 843.0707909829272, 903.5526528782029, 815.3518994988133, 750.7081633242229, 715.8490684457563, 651.9611296633608, 828.449492841636, 803.8766516727088, 797.216846371403, 700.7646815976811, 804.675888728575, 806.3714255856993, 780.1214154392121, 756.5576920259288, 679.2421441957896, 806.0923240990886, 791.8487716899564, 804.6520966016892, 818.7480982125869, 803.1059549975324, 794.5096367866424, 788.3549829716335, 782.8948903089504, 815.0007502787153, 801.108243456987, 797.4775786244566, 784.3133265406218, 791.386338667018, 752.6932646559344, 675.7038649177202, 813.4532702309752, 743.747694579897, 815.0344770681022, 835.9259680498144, 828.7516452314877, 761.8940327911249, 721.1955214805035, 801.287775875725, 777.1689940727033, 732.0711720852528, 763.1596904628609, 808.4207493067047, 708.1193009842185, 866.4277725680114, 823.849752271528, 738.4794099596976, 771.5208467551447, 779.8669244696966, 714.2362291009583, 734.9266616898577, 830.3509624407601, 672.1809381261509, 739.5874352527456, 758.4668650339855, 765.1029565701962, 789.6414427916013, 776.7314706234574, 719.9044262501191, 756.9540518719703, 776.6273350777736, 804.1751536666274, 819.1162468679679, 818.9720204714341, 809.4501579496484, 800.5650587440255, 822.114793390327, 790.1038067382092, 818.8433578573104, 815.2969836946329, 809.8460134989366, 773.9902281883587, 808.6979884458259, 812.878654481062, 808.6597390353996, 773.6105560494121, 750.8426901964026, 747.0410458949617, 727.1271228522443, 635.5472380625062, 790.962935253961, 804.1699330730287, 775.6629405606122, 797.4993564819091, 812.092196155435, 273.2810078362156, 673.1387955410682, 743.2769453352317, 753.3984378432466, 793.7288637560788, 819.2991827261679, 875.9296414872554, 626.3629428489301, 709.1319069431814, 780.3431346459198]
Elapsed: 2.1216421127202922~0.3537445464037221
Time per graph: 0.001255409534154019~0.00020931629964717282
Speed: 807.2797724629227~72.88048348576176
Total Time: 2.1665
best val loss: 0.10120681686734659 test_score: 0.9722

Testing...
Test loss: 0.0888 score: 0.9716 time: 2.21s
test Score 0.9716
Epoch Time List: [11.757770439027809, 11.711095330072567, 12.210727476980537, 10.387575979111716, 10.368071971926838, 10.569913334096782, 10.213528512045741, 10.248894623946398, 9.83224269701168, 10.173475824994966, 10.300324118928984, 10.688114683958702, 10.441479033092037, 10.329199391999282, 10.638811875949614, 10.927575593115762, 11.80326841888018, 10.698172340053134, 10.509322443162091, 11.284602698870003, 10.971020247088745, 10.272307842154987, 10.88901650311891, 10.415233571082354, 10.290869105025195, 10.170538150938228, 10.170974940992892, 10.811731514986604, 10.69054230605252, 10.457778733922169, 10.246139632887207, 10.13686288392637, 10.173006555996835, 10.51213109085802, 10.33895348990336, 11.08825956005603, 10.709281761897728, 10.062379723996855, 10.26445868902374, 10.606364047038369, 10.393126470036805, 10.217317195842043, 10.494951745029539, 10.760982494917698, 10.072795080021024, 10.158271224005148, 10.294752364978194, 10.319614356034435, 10.86182383808773, 10.726872592931613, 10.274099923088215, 10.270078278030269, 9.938920169021003, 9.708534673904069, 9.888081807992421, 10.154748100205325, 10.553793914034031, 10.575228411122225, 10.411127954022959, 10.801363590057008, 10.544778609997593, 9.95128027012106, 10.33606983604841, 10.32859040016774, 10.854004182037897, 10.336923647089861, 10.069440645980649, 10.156840395997278, 10.614505616831593, 10.36516901885625, 10.251510736881755, 10.663879850995727, 10.67920849705115, 10.305540462955832, 10.13791953411419, 10.321850977023132, 10.366543298936449, 10.281779804150574, 10.883393621072173, 10.871895757038146, 11.159621149068698, 11.768102963105775, 10.468135770061053, 10.210085088969208, 10.388633430120535, 11.424266007030383, 10.38969604310114, 10.448470543022268, 10.58899628103245, 11.153777759987861, 11.479574620956555, 10.626004740828648, 10.370117145008408, 10.249337845947593, 10.280492400052026, 10.28284671495203, 10.678846781956963, 10.517936731921509, 10.565854687942192, 10.285638552042656, 10.14027101173997, 10.285092466045171, 10.807485310127959, 10.443138403934427, 11.280672270921059, 11.410175924072973, 10.54336289910134, 10.587150653125718, 10.2704623718746, 10.259005957981572, 10.003304800018668, 11.040192384971306, 11.238014195114374, 10.709363699075766, 10.88815009908285, 10.949029844021425, 10.900586295989342, 10.270315385889262, 10.589451324078254, 9.87423796497751, 10.54964703891892, 10.692069862037897, 10.913703690981492, 10.489957044948824, 10.97670861298684, 11.174484526971355, 10.601866494049318, 11.540642005973496, 11.455686367000453, 10.443437940906733, 10.438230913016014, 10.564564119908027, 10.950150762917474, 10.832828163052909, 11.000452794949524, 10.836017852998339, 10.564970375970006, 10.257674994994886, 10.093915323959664, 10.30348234414123, 10.24430834397208, 10.515986540936865, 10.608830705168657, 10.392332971096039, 10.182452252018265, 10.204258930985816, 10.190677701029927, 10.439980254974216, 10.662758325808682, 10.454025353072211, 10.667746288934723, 10.798598081921227, 10.900806355057284, 10.700893622939475, 11.808671324979514, 10.91097596695181, 10.530768954893574, 10.299275352037512, 10.64939506095834, 10.213694156962447, 18.67144667787943, 18.522611040971242, 12.010071475873701, 10.791328394901939, 11.178725607926026, 10.532828470109962, 10.44335814495571, 11.85221385222394, 11.777629079995677, 10.654383537941612]
Total Epoch List: [80, 90]
Total Time List: [2.251737711019814, 2.166522498941049]
========================k_idx:2========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: AEtransGAT
AEtransGAT(
  (gae): GAE(
    (encoder): transGAT(
      (encoder): EdgeFeatureGAT()
      (classifier): Sequential(
        (0): Linear(in_features=64, out_features=64, bias=True)
        (1): ReLU(inplace=True)
        (2): Linear(in_features=64, out_features=2, bias=True)
      )
    )
    (decoder): InnerProductDecoder()
  )
)
Total number of parameters: 1914242
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x71f20acafd90>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 2.9054;  Loss pred: 2.9054; Loss self: 0.0000; time: 7.48s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7003 score: 0.5000 time: 2.53s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7003 score: 0.5000 time: 2.46s
Epoch 2/1000, LR 0.000029
Train loss: 2.7631;  Loss pred: 2.7631; Loss self: 0.0000; time: 21.10s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6994 score: 0.5000 time: 2.49s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6994 score: 0.5000 time: 2.41s
Epoch 3/1000, LR 0.000059
Train loss: 2.4815;  Loss pred: 2.4815; Loss self: 0.0000; time: 6.75s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6982 score: 0.5000 time: 6.36s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6982 score: 0.5000 time: 10.11s
Epoch 4/1000, LR 0.000089
Train loss: 2.1507;  Loss pred: 2.1507; Loss self: 0.0000; time: 17.95s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6970 score: 0.5000 time: 2.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6969 score: 0.5000 time: 2.13s
Epoch 5/1000, LR 0.000119
Train loss: 1.8143;  Loss pred: 1.8143; Loss self: 0.0000; time: 19.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6963 score: 0.5000 time: 2.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6963 score: 0.5000 time: 11.98s
Epoch 6/1000, LR 0.000149
Train loss: 1.5167;  Loss pred: 1.5167; Loss self: 0.0000; time: 6.52s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6954 score: 0.5000 time: 2.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6954 score: 0.5000 time: 2.12s
Epoch 7/1000, LR 0.000179
Train loss: 1.2941;  Loss pred: 1.2941; Loss self: 0.0000; time: 14.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6944 score: 0.5000 time: 2.49s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6943 score: 0.5000 time: 15.79s
Epoch 8/1000, LR 0.000209
Train loss: 1.1550;  Loss pred: 1.1550; Loss self: 0.0000; time: 25.66s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6932 score: 0.5000 time: 2.55s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6930 score: 0.5000 time: 12.70s
Epoch 9/1000, LR 0.000239
Train loss: 1.0747;  Loss pred: 1.0747; Loss self: 0.0000; time: 26.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6922 score: 0.5000 time: 3.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6919 score: 0.5000 time: 14.04s
Epoch 10/1000, LR 0.000269
Train loss: 1.0309;  Loss pred: 1.0309; Loss self: 0.0000; time: 36.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6903 score: 0.5000 time: 2.61s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6900 score: 0.5000 time: 11.27s
Epoch 11/1000, LR 0.000299
Train loss: 1.0079;  Loss pred: 1.0079; Loss self: 0.0000; time: 17.53s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6881 score: 0.5000 time: 2.47s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6876 score: 0.5000 time: 2.48s
Epoch 12/1000, LR 0.000299
Train loss: 0.9926;  Loss pred: 0.9926; Loss self: 0.0000; time: 7.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6837 score: 0.5000 time: 15.79s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6830 score: 0.5000 time: 2.14s
Epoch 13/1000, LR 0.000299
Train loss: 0.9857;  Loss pred: 0.9857; Loss self: 0.0000; time: 25.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6776 score: 0.5000 time: 2.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6766 score: 0.5000 time: 2.25s
Epoch 14/1000, LR 0.000299
Train loss: 0.9760;  Loss pred: 0.9760; Loss self: 0.0000; time: 17.78s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6678 score: 0.5000 time: 14.90s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6664 score: 0.5000 time: 2.10s
Epoch 15/1000, LR 0.000299
Train loss: 0.9681;  Loss pred: 0.9681; Loss self: 0.0000; time: 27.54s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6526 score: 0.5000 time: 2.56s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6506 score: 0.5000 time: 11.50s
Epoch 16/1000, LR 0.000299
Train loss: 0.9586;  Loss pred: 0.9586; Loss self: 0.0000; time: 7.70s
Val loss: 0.6336 score: 0.5006 time: 7.04s
Test loss: 0.6310 score: 0.5018 time: 10.93s
Epoch 17/1000, LR 0.000299
Train loss: 0.9465;  Loss pred: 0.9465; Loss self: 0.0000; time: 11.89s
Val loss: 0.6105 score: 0.5438 time: 18.27s
Test loss: 0.6070 score: 0.5450 time: 2.66s
Epoch 18/1000, LR 0.000299
Train loss: 0.9340;  Loss pred: 0.9340; Loss self: 0.0000; time: 18.83s
Val loss: 0.5832 score: 0.7053 time: 16.29s
Test loss: 0.5790 score: 0.7118 time: 7.90s
Epoch 19/1000, LR 0.000299
Train loss: 0.9164;  Loss pred: 0.9164; Loss self: 0.0000; time: 7.31s
Val loss: 0.5533 score: 0.7811 time: 2.24s
Test loss: 0.5483 score: 0.7911 time: 2.44s
Epoch 20/1000, LR 0.000299
Train loss: 0.8999;  Loss pred: 0.8999; Loss self: 0.0000; time: 31.74s
Val loss: 0.5204 score: 0.8444 time: 5.55s
Test loss: 0.5145 score: 0.8515 time: 6.93s
Epoch 21/1000, LR 0.000299
Train loss: 0.8790;  Loss pred: 0.8790; Loss self: 0.0000; time: 6.68s
Val loss: 0.4846 score: 0.8740 time: 2.12s
Test loss: 0.4779 score: 0.8817 time: 2.17s
Epoch 22/1000, LR 0.000299
Train loss: 0.8606;  Loss pred: 0.8606; Loss self: 0.0000; time: 47.47s
Val loss: 0.4452 score: 0.8911 time: 2.65s
Test loss: 0.4377 score: 0.9024 time: 13.55s
Epoch 23/1000, LR 0.000299
Train loss: 0.8377;  Loss pred: 0.8377; Loss self: 0.0000; time: 15.26s
Val loss: 0.4064 score: 0.9059 time: 7.18s
Test loss: 0.3982 score: 0.9166 time: 2.72s
Epoch 24/1000, LR 0.000299
Train loss: 0.8156;  Loss pred: 0.8156; Loss self: 0.0000; time: 26.21s
Val loss: 0.3678 score: 0.9166 time: 2.55s
Test loss: 0.3592 score: 0.9225 time: 2.44s
Epoch 25/1000, LR 0.000299
Train loss: 0.7925;  Loss pred: 0.7925; Loss self: 0.0000; time: 18.43s
Val loss: 0.3325 score: 0.9201 time: 19.42s
Test loss: 0.3237 score: 0.9278 time: 18.37s
Epoch 26/1000, LR 0.000299
Train loss: 0.7704;  Loss pred: 0.7704; Loss self: 0.0000; time: 14.56s
Val loss: 0.3015 score: 0.9278 time: 2.68s
Test loss: 0.2926 score: 0.9314 time: 2.69s
Epoch 27/1000, LR 0.000299
Train loss: 0.7506;  Loss pred: 0.7506; Loss self: 0.0000; time: 20.72s
Val loss: 0.2750 score: 0.9314 time: 2.40s
Test loss: 0.2663 score: 0.9343 time: 14.63s
Epoch 28/1000, LR 0.000299
Train loss: 0.7333;  Loss pred: 0.7333; Loss self: 0.0000; time: 7.53s
Val loss: 0.2526 score: 0.9355 time: 2.53s
Test loss: 0.2444 score: 0.9408 time: 9.28s
Epoch 29/1000, LR 0.000299
Train loss: 0.7185;  Loss pred: 0.7185; Loss self: 0.0000; time: 27.34s
Val loss: 0.2341 score: 0.9195 time: 12.30s
Test loss: 0.2264 score: 0.9325 time: 15.46s
Epoch 30/1000, LR 0.000299
Train loss: 0.7058;  Loss pred: 0.7058; Loss self: 0.0000; time: 15.59s
Val loss: 0.2189 score: 0.9207 time: 2.21s
Test loss: 0.2119 score: 0.9343 time: 18.13s
Epoch 31/1000, LR 0.000299
Train loss: 0.6947;  Loss pred: 0.6947; Loss self: 0.0000; time: 16.51s
Val loss: 0.2055 score: 0.9237 time: 16.49s
Test loss: 0.1993 score: 0.9349 time: 3.75s
Epoch 32/1000, LR 0.000299
Train loss: 0.6847;  Loss pred: 0.6847; Loss self: 0.0000; time: 27.33s
Val loss: 0.1943 score: 0.9254 time: 15.83s
Test loss: 0.1889 score: 0.9367 time: 9.34s
Epoch 33/1000, LR 0.000299
Train loss: 0.6778;  Loss pred: 0.6778; Loss self: 0.0000; time: 23.48s
Val loss: 0.1845 score: 0.9290 time: 2.60s
Test loss: 0.1800 score: 0.9355 time: 13.02s
Epoch 34/1000, LR 0.000298
Train loss: 0.6702;  Loss pred: 0.6702; Loss self: 0.0000; time: 14.85s
Val loss: 0.1765 score: 0.9290 time: 2.19s
Test loss: 0.1729 score: 0.9379 time: 10.50s
Epoch 35/1000, LR 0.000298
Train loss: 0.6633;  Loss pred: 0.6633; Loss self: 0.0000; time: 16.03s
Val loss: 0.1694 score: 0.9302 time: 2.16s
Test loss: 0.1666 score: 0.9396 time: 2.19s
Epoch 36/1000, LR 0.000298
Train loss: 0.6588;  Loss pred: 0.6588; Loss self: 0.0000; time: 19.07s
Val loss: 0.1630 score: 0.9331 time: 8.80s
Test loss: 0.1612 score: 0.9426 time: 13.96s
Epoch 37/1000, LR 0.000298
Train loss: 0.6521;  Loss pred: 0.6521; Loss self: 0.0000; time: 25.88s
Val loss: 0.1574 score: 0.9355 time: 2.53s
Test loss: 0.1563 score: 0.9438 time: 2.59s
Epoch 38/1000, LR 0.000298
Train loss: 0.6494;  Loss pred: 0.6494; Loss self: 0.0000; time: 19.03s
Val loss: 0.1522 score: 0.9373 time: 2.59s
Test loss: 0.1519 score: 0.9438 time: 18.54s
Epoch 39/1000, LR 0.000298
Train loss: 0.6443;  Loss pred: 0.6443; Loss self: 0.0000; time: 7.56s
Val loss: 0.1474 score: 0.9396 time: 2.22s
Test loss: 0.1480 score: 0.9444 time: 7.73s
Epoch 40/1000, LR 0.000298
Train loss: 0.6407;  Loss pred: 0.6407; Loss self: 0.0000; time: 29.22s
Val loss: 0.1430 score: 0.9456 time: 5.19s
Test loss: 0.1446 score: 0.9509 time: 15.78s
Epoch 41/1000, LR 0.000298
Train loss: 0.6389;  Loss pred: 0.6389; Loss self: 0.0000; time: 15.08s
Val loss: 0.1390 score: 0.9456 time: 2.21s
Test loss: 0.1414 score: 0.9509 time: 2.25s
Epoch 42/1000, LR 0.000298
Train loss: 0.6341;  Loss pred: 0.6341; Loss self: 0.0000; time: 32.78s
Val loss: 0.1353 score: 0.9663 time: 10.60s
Test loss: 0.1387 score: 0.9615 time: 2.76s
Epoch 43/1000, LR 0.000298
Train loss: 0.6320;  Loss pred: 0.6320; Loss self: 0.0000; time: 7.51s
Val loss: 0.1319 score: 0.9675 time: 9.31s
Test loss: 0.1361 score: 0.9621 time: 2.53s
Epoch 44/1000, LR 0.000298
Train loss: 0.6295;  Loss pred: 0.6295; Loss self: 0.0000; time: 26.50s
Val loss: 0.1287 score: 0.9675 time: 2.26s
Test loss: 0.1338 score: 0.9639 time: 2.33s
Epoch 45/1000, LR 0.000298
Train loss: 0.6257;  Loss pred: 0.6257; Loss self: 0.0000; time: 16.01s
Val loss: 0.1258 score: 0.9686 time: 9.56s
Test loss: 0.1318 score: 0.9633 time: 2.12s
Epoch 46/1000, LR 0.000298
Train loss: 0.6244;  Loss pred: 0.6244; Loss self: 0.0000; time: 13.22s
Val loss: 0.1228 score: 0.9686 time: 9.25s
Test loss: 0.1297 score: 0.9633 time: 2.13s
Epoch 47/1000, LR 0.000298
Train loss: 0.6207;  Loss pred: 0.6207; Loss self: 0.0000; time: 6.96s
Val loss: 0.1203 score: 0.9686 time: 2.26s
Test loss: 0.1280 score: 0.9627 time: 19.68s
Epoch 48/1000, LR 0.000298
Train loss: 0.6198;  Loss pred: 0.6198; Loss self: 0.0000; time: 7.43s
Val loss: 0.1179 score: 0.9704 time: 2.55s
Test loss: 0.1264 score: 0.9621 time: 7.75s
Epoch 49/1000, LR 0.000298
Train loss: 0.6179;  Loss pred: 0.6179; Loss self: 0.0000; time: 28.47s
Val loss: 0.1157 score: 0.9698 time: 13.97s
Test loss: 0.1251 score: 0.9621 time: 2.68s
Epoch 50/1000, LR 0.000298
Train loss: 0.6160;  Loss pred: 0.6160; Loss self: 0.0000; time: 38.41s
Val loss: 0.1137 score: 0.9698 time: 4.00s
Test loss: 0.1237 score: 0.9627 time: 13.39s
Epoch 51/1000, LR 0.000298
Train loss: 0.6130;  Loss pred: 0.6130; Loss self: 0.0000; time: 20.56s
Val loss: 0.1116 score: 0.9698 time: 13.22s
Test loss: 0.1226 score: 0.9633 time: 11.91s
Epoch 52/1000, LR 0.000298
Train loss: 0.6116;  Loss pred: 0.6116; Loss self: 0.0000; time: 20.89s
Val loss: 0.1099 score: 0.9692 time: 14.93s
Test loss: 0.1215 score: 0.9633 time: 11.80s
Epoch 53/1000, LR 0.000298
Train loss: 0.6091;  Loss pred: 0.6091; Loss self: 0.0000; time: 19.87s
Val loss: 0.1081 score: 0.9692 time: 19.38s
Test loss: 0.1206 score: 0.9627 time: 12.05s
Epoch 54/1000, LR 0.000297
Train loss: 0.6070;  Loss pred: 0.6070; Loss self: 0.0000; time: 15.77s
Val loss: 0.1065 score: 0.9692 time: 9.74s
Test loss: 0.1199 score: 0.9627 time: 2.71s
Epoch 55/1000, LR 0.000297
Train loss: 0.6049;  Loss pred: 0.6049; Loss self: 0.0000; time: 20.60s
Val loss: 0.1049 score: 0.9692 time: 2.66s
Test loss: 0.1191 score: 0.9627 time: 10.66s
Epoch 56/1000, LR 0.000297
Train loss: 0.6026;  Loss pred: 0.6026; Loss self: 0.0000; time: 7.55s
Val loss: 0.1034 score: 0.9686 time: 9.90s
Test loss: 0.1184 score: 0.9621 time: 2.78s
Epoch 57/1000, LR 0.000297
Train loss: 0.6017;  Loss pred: 0.6017; Loss self: 0.0000; time: 25.28s
Val loss: 0.1020 score: 0.9680 time: 7.16s
Test loss: 0.1178 score: 0.9621 time: 2.19s
Epoch 58/1000, LR 0.000297
Train loss: 0.5987;  Loss pred: 0.5987; Loss self: 0.0000; time: 33.91s
Val loss: 0.1008 score: 0.9680 time: 2.74s
Test loss: 0.1174 score: 0.9627 time: 13.48s
Epoch 59/1000, LR 0.000297
Train loss: 0.5987;  Loss pred: 0.5987; Loss self: 0.0000; time: 31.33s
Val loss: 0.0996 score: 0.9680 time: 2.69s
Test loss: 0.1169 score: 0.9627 time: 12.99s
Epoch 60/1000, LR 0.000297
Train loss: 0.5958;  Loss pred: 0.5958; Loss self: 0.0000; time: 24.24s
Val loss: 0.0983 score: 0.9680 time: 15.44s
Test loss: 0.1165 score: 0.9627 time: 2.24s
Epoch 61/1000, LR 0.000297
Train loss: 0.5949;  Loss pred: 0.5949; Loss self: 0.0000; time: 14.54s
Val loss: 0.0972 score: 0.9698 time: 2.23s
Test loss: 0.1162 score: 0.9633 time: 10.59s
Epoch 62/1000, LR 0.000297
Train loss: 0.5931;  Loss pred: 0.5931; Loss self: 0.0000; time: 28.81s
Val loss: 0.0961 score: 0.9698 time: 16.98s
Test loss: 0.1160 score: 0.9639 time: 11.50s
Epoch 63/1000, LR 0.000297
Train loss: 0.5912;  Loss pred: 0.5912; Loss self: 0.0000; time: 28.98s
Val loss: 0.0952 score: 0.9686 time: 2.36s
Test loss: 0.1159 score: 0.9639 time: 5.83s
Epoch 64/1000, LR 0.000297
Train loss: 0.5899;  Loss pred: 0.5899; Loss self: 0.0000; time: 22.40s
Val loss: 0.0942 score: 0.9704 time: 18.33s
Test loss: 0.1155 score: 0.9657 time: 2.66s
Epoch 65/1000, LR 0.000297
Train loss: 0.5875;  Loss pred: 0.5875; Loss self: 0.0000; time: 41.43s
Val loss: 0.0933 score: 0.9698 time: 8.98s
Test loss: 0.1155 score: 0.9633 time: 4.06s
Epoch 66/1000, LR 0.000297
Train loss: 0.5864;  Loss pred: 0.5864; Loss self: 0.0000; time: 29.84s
Val loss: 0.0925 score: 0.9692 time: 4.01s
Test loss: 0.1154 score: 0.9627 time: 11.99s
Epoch 67/1000, LR 0.000297
Train loss: 0.5849;  Loss pred: 0.5849; Loss self: 0.0000; time: 25.97s
Val loss: 0.0918 score: 0.9692 time: 12.52s
Test loss: 0.1154 score: 0.9627 time: 4.46s
Epoch 68/1000, LR 0.000296
Train loss: 0.5842;  Loss pred: 0.5842; Loss self: 0.0000; time: 17.71s
Val loss: 0.0911 score: 0.9716 time: 14.15s
Test loss: 0.1152 score: 0.9639 time: 13.45s
Epoch 69/1000, LR 0.000296
Train loss: 0.5816;  Loss pred: 0.5816; Loss self: 0.0000; time: 24.69s
Val loss: 0.0905 score: 0.9692 time: 2.50s
Test loss: 0.1156 score: 0.9633 time: 17.66s
Epoch 70/1000, LR 0.000296
Train loss: 0.5816;  Loss pred: 0.5816; Loss self: 0.0000; time: 31.54s
Val loss: 0.0899 score: 0.9698 time: 9.96s
Test loss: 0.1154 score: 0.9633 time: 11.57s
Epoch 71/1000, LR 0.000296
Train loss: 0.5792;  Loss pred: 0.5792; Loss self: 0.0000; time: 28.52s
Val loss: 0.0891 score: 0.9710 time: 3.88s
Test loss: 0.1151 score: 0.9633 time: 8.48s
Epoch 72/1000, LR 0.000296
Train loss: 0.5773;  Loss pred: 0.5773; Loss self: 0.0000; time: 30.75s
Val loss: 0.0888 score: 0.9692 time: 13.62s
Test loss: 0.1157 score: 0.9627 time: 11.30s
Epoch 73/1000, LR 0.000296
Train loss: 0.5774;  Loss pred: 0.5774; Loss self: 0.0000; time: 23.09s
Val loss: 0.0884 score: 0.9704 time: 14.43s
Test loss: 0.1157 score: 0.9627 time: 15.34s
Epoch 74/1000, LR 0.000296
Train loss: 0.5761;  Loss pred: 0.5761; Loss self: 0.0000; time: 28.29s
Val loss: 0.0877 score: 0.9704 time: 15.43s
Test loss: 0.1156 score: 0.9627 time: 15.91s
Epoch 75/1000, LR 0.000296
Train loss: 0.5746;  Loss pred: 0.5746; Loss self: 0.0000; time: 32.72s
Val loss: 0.0874 score: 0.9704 time: 6.94s
Test loss: 0.1159 score: 0.9627 time: 2.63s
Epoch 76/1000, LR 0.000296
Train loss: 0.5734;  Loss pred: 0.5734; Loss self: 0.0000; time: 25.85s
Val loss: 0.0869 score: 0.9704 time: 19.58s
Test loss: 0.1157 score: 0.9627 time: 15.21s
Epoch 77/1000, LR 0.000296
Train loss: 0.5711;  Loss pred: 0.5711; Loss self: 0.0000; time: 33.33s
Val loss: 0.0867 score: 0.9698 time: 9.69s
Test loss: 0.1162 score: 0.9621 time: 9.61s
Epoch 78/1000, LR 0.000296
Train loss: 0.5699;  Loss pred: 0.5699; Loss self: 0.0000; time: 31.56s
Val loss: 0.0860 score: 0.9704 time: 12.06s
Test loss: 0.1159 score: 0.9627 time: 15.35s
Epoch 79/1000, LR 0.000295
Train loss: 0.5679;  Loss pred: 0.5679; Loss self: 0.0000; time: 29.77s
Val loss: 0.0858 score: 0.9704 time: 23.07s
Test loss: 0.1164 score: 0.9627 time: 2.75s
Epoch 80/1000, LR 0.000295
Train loss: 0.5674;  Loss pred: 0.5674; Loss self: 0.0000; time: 27.68s
Val loss: 0.0855 score: 0.9704 time: 4.25s
Test loss: 0.1168 score: 0.9621 time: 23.15s
Epoch 81/1000, LR 0.000295
Train loss: 0.5663;  Loss pred: 0.5663; Loss self: 0.0000; time: 31.58s
Val loss: 0.0850 score: 0.9704 time: 8.19s
Test loss: 0.1172 score: 0.9621 time: 2.68s
Epoch 82/1000, LR 0.000295
Train loss: 0.5657;  Loss pred: 0.5657; Loss self: 0.0000; time: 40.70s
Val loss: 0.0849 score: 0.9692 time: 8.67s
Test loss: 0.1177 score: 0.9621 time: 5.56s
Epoch 83/1000, LR 0.000295
Train loss: 0.5655;  Loss pred: 0.5655; Loss self: 0.0000; time: 37.86s
Val loss: 0.0843 score: 0.9704 time: 12.80s
Test loss: 0.1176 score: 0.9627 time: 2.75s
Epoch 84/1000, LR 0.000295
Train loss: 0.5630;  Loss pred: 0.5630; Loss self: 0.0000; time: 28.53s
Val loss: 0.0843 score: 0.9686 time: 12.27s
Test loss: 0.1182 score: 0.9621 time: 11.23s
Epoch 85/1000, LR 0.000295
Train loss: 0.5625;  Loss pred: 0.5625; Loss self: 0.0000; time: 33.52s
Val loss: 0.0837 score: 0.9698 time: 2.80s
Test loss: 0.1179 score: 0.9627 time: 14.41s
Epoch 86/1000, LR 0.000295
Train loss: 0.5607;  Loss pred: 0.5607; Loss self: 0.0000; time: 39.27s
Val loss: 0.0836 score: 0.9698 time: 11.72s
Test loss: 0.1183 score: 0.9627 time: 14.20s
Epoch 87/1000, LR 0.000295
Train loss: 0.5593;  Loss pred: 0.5593; Loss self: 0.0000; time: 21.13s
Val loss: 0.0835 score: 0.9686 time: 2.72s
Test loss: 0.1189 score: 0.9621 time: 10.75s
Epoch 88/1000, LR 0.000294
Train loss: 0.5587;  Loss pred: 0.5587; Loss self: 0.0000; time: 39.23s
Val loss: 0.0831 score: 0.9698 time: 2.82s
Test loss: 0.1190 score: 0.9627 time: 2.66s
Epoch 89/1000, LR 0.000294
Train loss: 0.5574;  Loss pred: 0.5574; Loss self: 0.0000; time: 6.81s
Val loss: 0.0828 score: 0.9686 time: 2.27s
Test loss: 0.1196 score: 0.9621 time: 2.18s
Epoch 90/1000, LR 0.000294
Train loss: 0.5568;  Loss pred: 0.5568; Loss self: 0.0000; time: 27.53s
Val loss: 0.0826 score: 0.9680 time: 15.73s
Test loss: 0.1197 score: 0.9621 time: 13.40s
Epoch 91/1000, LR 0.000294
Train loss: 0.5550;  Loss pred: 0.5550; Loss self: 0.0000; time: 36.24s
Val loss: 0.0822 score: 0.9692 time: 18.90s
Test loss: 0.1193 score: 0.9627 time: 15.67s
Epoch 92/1000, LR 0.000294
Train loss: 0.5529;  Loss pred: 0.5529; Loss self: 0.0000; time: 40.94s
Val loss: 0.0823 score: 0.9675 time: 8.51s
Test loss: 0.1208 score: 0.9621 time: 12.37s
     INFO: Early stopping counter 1 of 2
Epoch 93/1000, LR 0.000294
Train loss: 0.5535;  Loss pred: 0.5535; Loss self: 0.0000; time: 47.22s
Val loss: 0.0816 score: 0.9686 time: 18.81s
Test loss: 0.1200 score: 0.9627 time: 19.69s
Epoch 94/1000, LR 0.000294
Train loss: 0.5513;  Loss pred: 0.5513; Loss self: 0.0000; time: 39.50s
Val loss: 0.0815 score: 0.9686 time: 13.01s
Test loss: 0.1209 score: 0.9621 time: 19.76s
Epoch 95/1000, LR 0.000294
Train loss: 0.5513;  Loss pred: 0.5513; Loss self: 0.0000; time: 24.53s
Val loss: 0.0821 score: 0.9675 time: 13.02s
Test loss: 0.1228 score: 0.9621 time: 23.10s
     INFO: Early stopping counter 1 of 2
Epoch 96/1000, LR 0.000293
Train loss: 0.5499;  Loss pred: 0.5499; Loss self: 0.0000; time: 36.12s
Val loss: 0.0814 score: 0.9675 time: 27.55s
Test loss: 0.1221 score: 0.9615 time: 2.75s
Epoch 97/1000, LR 0.000293
Train loss: 0.5494;  Loss pred: 0.5494; Loss self: 0.0000; time: 40.70s
Val loss: 0.0813 score: 0.9675 time: 16.22s
Test loss: 0.1228 score: 0.9621 time: 17.93s
Epoch 98/1000, LR 0.000293
Train loss: 0.5477;  Loss pred: 0.5477; Loss self: 0.0000; time: 48.91s
Val loss: 0.0811 score: 0.9675 time: 10.28s
Test loss: 0.1231 score: 0.9615 time: 12.87s
Epoch 99/1000, LR 0.000293
Train loss: 0.5465;  Loss pred: 0.5465; Loss self: 0.0000; time: 43.43s
Val loss: 0.0807 score: 0.9675 time: 14.51s
Test loss: 0.1232 score: 0.9615 time: 8.99s
Epoch 100/1000, LR 0.000293
Train loss: 0.5439;  Loss pred: 0.5439; Loss self: 0.0000; time: 40.90s
Val loss: 0.0805 score: 0.9675 time: 2.63s
Test loss: 0.1236 score: 0.9621 time: 20.16s
Epoch 101/1000, LR 0.000293
Train loss: 0.5436;  Loss pred: 0.5436; Loss self: 0.0000; time: 20.51s
Val loss: 0.0799 score: 0.9686 time: 12.82s
Test loss: 0.1235 score: 0.9627 time: 10.87s
Epoch 102/1000, LR 0.000293
Train loss: 0.5423;  Loss pred: 0.5423; Loss self: 0.0000; time: 24.50s
Val loss: 0.0803 score: 0.9746 time: 14.36s
Test loss: 0.1253 score: 0.9675 time: 18.90s
     INFO: Early stopping counter 1 of 2
Epoch 103/1000, LR 0.000293
Train loss: 0.5419;  Loss pred: 0.5419; Loss self: 0.0000; time: 30.10s
Val loss: 0.0811 score: 0.9746 time: 7.57s
Test loss: 0.1271 score: 0.9669 time: 14.39s
     INFO: Early stopping counter 2 of 2
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 100,   Train_Loss: 0.5436,   Val_Loss: 0.0799,   Val_Precision: 0.9703,   Val_Recall: 0.9669,   Val_accuracy: 0.9686,   Val_Score: 0.9686,   Val_Loss: 0.0799,   Test_Precision: 0.9700,   Test_Recall: 0.9550,   Test_accuracy: 0.9624,   Test_Score: 0.9627,   Test_loss: 0.1235


[2.0911515819607303, 2.262356181978248, 2.382134042913094, 1.9260399850318208, 1.9420033859787509, 1.949627339025028, 1.9503265239764005, 1.93740049097687, 1.8235668370034546, 1.9274736819788814, 1.9744480550289154, 1.9711262419587001, 1.9495742550352588, 1.936083855922334, 2.0390239630360156, 2.1134655410423875, 2.50531380798202, 1.9463682409841567, 2.02270834101364, 2.05961313797161, 2.1820704720448703, 1.9185140179470181, 1.9570099740521982, 1.8893659439636394, 1.9139319129753858, 1.89530236099381, 1.9002253259532154, 2.1445616639684886, 1.9601782710524276, 1.939203055924736, 1.9083201590692624, 1.9112083970103413, 1.9451326000271365, 2.058885319973342, 1.9855987490154803, 2.0574006799142808, 2.056829118053429, 1.9068163329502568, 2.048990988987498, 1.9785629370016977, 1.9352274000411853, 1.9224203949561343, 1.9756166510051116, 2.1002364720916376, 1.875777932931669, 1.8787151109427214, 2.0119841770501807, 2.0336251279804856, 2.1386081079253927, 1.9790542129194364, 2.0871985820122063, 1.8826108030043542, 1.8963146050227806, 1.8284112400142476, 1.8726877379231155, 1.9428857309976593, 2.0710763239767402, 1.9271774640074, 1.9061481968965381, 2.190437792916782, 1.9602605249965563, 1.8881705680396408, 1.954546806984581, 2.0275450199842453, 2.1756654910277575, 1.911388255073689, 1.9186397739686072, 1.9016780820675194, 1.9264403861016035, 2.006570341065526, 2.036801574053243, 2.0271045939298347, 2.032473846920766, 1.9734895959263667, 1.9391338069690391, 1.9446903700008988, 2.0045766240218654, 1.8703945969464257, 2.072724673897028, 2.2512077030260116, 2.3608328549889848, 2.5921790780266747, 2.039955380023457, 2.1023125830106437, 2.1198749219765887, 2.411651220987551, 2.100224480032921, 2.095808390993625, 2.1663294540485367, 2.2338018869049847, 2.4880670530255884, 2.0965340438997373, 2.134245907072909, 2.1002865799237043, 2.0641269319457933, 2.1043300569290295, 2.127098176977597, 2.1437043419573456, 2.158655039034784, 2.0736177229555324, 2.109577593044378, 2.1191818369552493, 2.1547510960372165, 2.135493017034605, 2.2452705230098218, 2.5010956540936604, 2.0775624880334362, 2.27227595099248, 2.0735319149680436, 2.0217101329471916, 2.039211638038978, 2.218156235991046, 2.3433312460547313, 2.1091049319365993, 2.1745592179941013, 2.3085187129909173, 2.2144775479100645, 2.0904955760343, 2.386603496968746, 1.95053765992634, 2.051344914943911, 2.288486283039674, 2.1904787240782753, 2.167036383994855, 2.3661639260826632, 2.299549176939763, 2.0352839659899473, 2.514203995000571, 2.2850577490171418, 2.22817907796707, 2.2088530510663986, 2.1402118840487674, 2.1757841209182516, 2.3475338369607925, 2.2326322130393237, 2.176075865048915, 2.1015322250314057, 2.063199218013324, 2.0635625610593706, 2.0878370130667463, 2.1110089449211955, 2.0556739929597825, 2.1389594450592995, 2.0638868029927835, 2.072864286019467, 2.086816471070051, 2.183490099036135, 2.089778908994049, 2.0790310960728675, 2.089877755031921, 2.1845617110375315, 2.250804358976893, 2.262258558999747, 2.3242153220344335, 2.659125708974898, 2.136636149021797, 2.1015458679758012, 2.178781416034326, 2.1191239670151845, 2.0810445020906627, 6.184110682923347, 2.5106263540219516, 2.27371508104261, 2.2431689728982747, 2.1291905550751835, 2.0627385400002822, 1.929378708009608, 2.698116194922477, 2.3831955429632217, 2.1657139339949936, 2.468534377985634, 2.4170604520477355, 10.118665490997955, 2.1333133169682696, 11.989639563020319, 2.122140194987878, 15.794439426041208, 12.706672235042788, 14.048380319029093, 11.274006464052945, 2.4873928129673004, 2.1477196080377325, 2.257246339926496, 2.10530757100787, 11.506976629025303, 10.938309269957244, 2.6682128929533064, 7.906241480028257, 2.4464490939863026, 6.935353640001267, 2.175248980987817, 13.556610079016536, 2.727797614992596, 2.442756221978925, 18.372465698979795, 2.6984692639671266, 14.632014239090495, 9.283019047928974, 15.462074603070505, 18.1345440329751, 3.7567637180909514, 9.348739702021703, 13.028022332931869, 10.505879986914806, 2.19128550903406, 13.963445040979423, 2.590875594993122, 18.547851552953944, 7.732142906985246, 15.781331005971879, 2.2504851950798184, 2.7693898000288755, 2.536957019008696, 2.3332306359661743, 2.122507828986272, 2.138439115951769, 19.681079232948832, 7.760924794944003, 2.6846528030000627, 13.39034250297118, 11.918018359923735, 11.807151832035743, 12.057241552975029, 2.7166670080041513, 10.66621550603304, 2.78163195902016, 2.198466846952215, 13.487913858960383, 12.997631700942293, 2.244815375073813, 10.600046753999777, 11.503877968993038, 5.833379589021206, 2.670416367938742, 4.063613238977268, 11.9992084458936, 4.469574221991934, 13.453605848015286, 17.663516396074556, 11.579458851949312, 8.48204044206068, 11.303044702974148, 15.34781915997155, 15.920881109894253, 2.6326031340286136, 15.219575881958008, 9.61483674601186, 15.360213003004901, 2.755663650925271, 23.154318292974494, 2.682421964011155, 5.570806170930155, 2.7520947919692844, 11.235234895022586, 14.4171109769959, 14.204284446081147, 10.75295441201888, 2.6637859649490565, 2.1847124570049345, 13.409289274015464, 15.671987827983685, 12.373836443992332, 19.69807006290648, 19.764806585968472, 23.105382689042017, 2.7561679639620706, 17.934373795054853, 12.873703285004012, 8.993208093103021, 20.16687182500027, 10.879044280969538, 18.907023346982896, 14.39343544805888]
[0.0012373677999767635, 0.001338672297028549, 0.0014095467709544936, 0.0011396686301963436, 0.0011491144295732254, 0.0011536256443935077, 0.0011540393632996453, 0.0011463908230632367, 0.0010790336313629908, 0.0011405169715851368, 0.0011683124585969915, 0.0011663468887329587, 0.0011535942337486738, 0.0011456117490664698, 0.001206523055050897, 0.0012505713260605844, 0.0014824342059065207, 0.001151697184014294, 0.0011968688408364734, 0.001218705998799769, 0.001291165959789864, 0.001135215395234922, 0.0011579940674865078, 0.0011179680141796683, 0.0011325040905179797, 0.0011214806869785858, 0.0011243936839959855, 0.0012689713987979222, 0.0011598687994393063, 0.0011474574295412638, 0.0011291835260764866, 0.0011308925426096693, 0.0011509660355190157, 0.0012182753372623326, 0.0011749105023760237, 0.0012173968520202845, 0.0012170586497357568, 0.0011282936881362466, 0.0012124207035428983, 0.0011707473000010045, 0.0011451049704385712, 0.0011375268609207896, 0.0011690039355059833, 0.0012427434746104364, 0.0011099277709654847, 0.0011116657461199535, 0.0011905231816864974, 0.0012033284780949619, 0.0012654485845712384, 0.0011710379958103174, 0.0012350287467527848, 0.0011139708893516888, 0.0011220796479424738, 0.001081900142020265, 0.001108099253208944, 0.0011496365272175499, 0.0012254889490986628, 0.0011403416946789348, 0.0011278983413588984, 0.0012961170372288652, 0.0011599174704121635, 0.0011172606911477164, 0.0011565365721802254, 0.0011997307810557665, 0.0012873760301939394, 0.0011309989674992242, 0.001135289807082016, 0.0011252533029985322, 0.0011399055539062742, 0.0011873197284411397, 0.0012052080319841674, 0.001199470173922979, 0.001202647246698678, 0.001167745323033353, 0.0011474164538278339, 0.0011507043609472775, 0.0011861400142141217, 0.0011067423650570566, 0.0012264643040810817, 0.0013320755639207168, 0.0013969425177449615, 0.001533833773980281, 0.0012070741893629923, 0.0012439719426098484, 0.0012543638591577448, 0.0014270125567973674, 0.0012427363787177047, 0.0012401233082802515, 0.001281851747957714, 0.0013217762644408193, 0.001472229025458928, 0.0012405526886980695, 0.0012628674006348575, 0.0012427731242152097, 0.0012213768828081618, 0.0012451657141591892, 0.001258637974542957, 0.0012684641076670684, 0.0012773106739850792, 0.0012269927354766465, 0.0012482707651150166, 0.0012539537496776623, 0.0012750006485427315, 0.001263605335523435, 0.001328562439650782, 0.0014799382568601541, 0.0012293269159961162, 0.001344541982835787, 0.0012269419615195524, 0.001196278185175853, 0.0012066341053485077, 0.0013125184828349384, 0.00138658653612706, 0.0012479910839861535, 0.0012867214307657404, 0.0013659874041366375, 0.001310341744325482, 0.0012369796307895265, 0.0014121914183247018, 0.0011541642958144023, 0.001213813559138409, 0.001354133895289748, 0.0012961412568510504, 0.0012822700497010977, 0.0014000969976820492, 0.0013606799863548895, 0.001204310039047306, 0.001487694671597971, 0.00135210517693322, 0.0013184491585603965, 0.0013070136396842594, 0.0012663975645258979, 0.001287446225395415, 0.0013890732763081612, 0.0013210841497274105, 0.0012876188550585298, 0.0012435101923262755, 0.0012208279396528543, 0.0012210429355380891, 0.0012354065166075422, 0.0012491177188882813, 0.001216375143763185, 0.0012656564763664493, 0.001221234794670286, 0.0012265469148044182, 0.0012348026456035805, 0.0012920059757610266, 0.0012365555674521, 0.0012301959148360163, 0.0012366140562319059, 0.0012926400657026814, 0.0013318368988028953, 0.001338614531952513, 0.0013752753384819133, 0.0015734471650739042, 0.0012642817449833117, 0.0012435182650744386, 0.0012892197728013764, 0.0012539195071095766, 0.0012313872793435873, 0.0036592370904871873, 0.001485577724273344, 0.0013453935390784675, 0.0013273189188747188, 0.0012598760680918245, 0.0012205553491125931, 0.001141644205922845, 0.0015965184585340101, 0.0014101748774930306, 0.0012814875349082802, 0.0014606712295772981, 0.0014302132852353465, 0.005987376030176305, 0.0012623155721705737, 0.00709446127989368, 0.001255704257389277, 0.009345822145586513, 0.007518740967480939, 0.008312651076348576, 0.006671009742043163, 0.0014718300668445565, 0.0012708400047560548, 0.0013356487218499978, 0.0012457441248567278, 0.00680886191066586, 0.006472372349087127, 0.0015788241970137909, 0.004678249396466424, 0.0014476030141930786, 0.004103759550296608, 0.0012871295745490042, 0.008021662768648838, 0.001614081428989702, 0.0014454178828277665, 0.01087128147868627, 0.0015967273751284774, 0.008657996591177807, 0.005492910679247913, 0.009149156569864205, 0.010730499427795918, 0.0022229371112964208, 0.005531798640249529, 0.0077088889543975555, 0.0062164970336774, 0.001296618644398852, 0.008262393515372439, 0.001533062482244451, 0.010975060090505293, 0.004575232489340382, 0.00933806568400703, 0.001331648044425928, 0.0016386921893661985, 0.0015011579994134295, 0.001380609843766967, 0.0012559217922995694, 0.0012653485893205734, 0.011645609013579191, 0.004592263192274558, 0.0015885519544379069, 0.007923279587556911, 0.007052081869777358, 0.006986480373985647, 0.007134462457381674, 0.0016074952710083735, 0.0063113701219130415, 0.0016459360704261302, 0.0013008679567764584, 0.007981014117728037, 0.007690906331918516, 0.0013282931213454514, 0.006272217014201052, 0.006807028384019549, 0.0034517038988291157, 0.0015801280283661197, 0.0024045048751344784, 0.007100123340765444, 0.00264471847455144, 0.007960713519535673, 0.010451784849748258, 0.006851750799970007, 0.005018958841456023, 0.0066881921319373655, 0.009081549798799734, 0.0094206397099966, 0.0015577533337447417, 0.009005666202342017, 0.005689252512433054, 0.00908888343373071, 0.0016305702076480895, 0.013700780055014494, 0.0015872319313675475, 0.003296335012384707, 0.0016284584567865588, 0.006648067985220465, 0.00853083489763071, 0.00840490203910127, 0.006362694918354367, 0.0015762047129876074, 0.0012927292644999613, 0.007934490694683707, 0.009273365578688571, 0.007321796712421498, 0.011655662759116262, 0.011695151826016848, 0.013671824076356223, 0.0016308686177290358, 0.010612055500032457, 0.007617575908286398, 0.005321424907161551, 0.01193306025147945, 0.006437304308265999, 0.01118758777927982, 0.00851682570891058]
[808.1671432041297, 747.0088103113062, 709.4479024082589, 877.4480348974058, 870.2353519060712, 866.8323254254001, 866.5215691956869, 872.3028655514966, 926.7551732718852, 876.7953699190984, 855.9354072119412, 857.3778604462461, 866.8559279725591, 872.8960756686329, 828.8279248487426, 799.6345183685706, 674.5661939097606, 868.2837935875242, 835.5134379646104, 820.5424450071145, 774.493776278573, 880.8900973308766, 863.5622824653644, 894.4799737707793, 882.9990181692186, 891.6783067340459, 889.3682117157556, 788.0398257575271, 862.1664799358439, 871.4920259828592, 885.5956333995115, 884.257312098266, 868.8353688465367, 820.8325075735066, 851.1286587171517, 821.4248281819426, 821.6530897809373, 886.2940655564896, 824.7962090038804, 854.1552903851599, 873.2823852969598, 879.1001200538982, 855.4291133050525, 804.6712941409494, 900.9595274205433, 899.5509697859268, 839.9668443107491, 831.0282838008958, 790.2336074276949, 853.9432568180974, 809.6977520800733, 897.6895263232436, 891.2023329481755, 924.2997215368414, 902.446235844037, 869.8401419275419, 816.0008303098056, 876.9301382789057, 886.6047260918873, 771.5352636194245, 862.1303028090967, 895.0462572640416, 864.6505644995443, 833.5203328866808, 776.7738225243738, 884.1741051374443, 880.832359950676, 888.6887933012398, 877.2656616797416, 842.2331205705844, 829.7322731526044, 833.7014306319987, 831.4990141498647, 856.3511069369001, 871.5231480810252, 869.0329453316616, 843.0707909829272, 903.5526528782029, 815.3518994988133, 750.7081633242229, 715.8490684457563, 651.9611296633608, 828.449492841636, 803.8766516727088, 797.216846371403, 700.7646815976811, 804.675888728575, 806.3714255856993, 780.1214154392121, 756.5576920259288, 679.2421441957896, 806.0923240990886, 791.8487716899564, 804.6520966016892, 818.7480982125869, 803.1059549975324, 794.5096367866424, 788.3549829716335, 782.8948903089504, 815.0007502787153, 801.108243456987, 797.4775786244566, 784.3133265406218, 791.386338667018, 752.6932646559344, 675.7038649177202, 813.4532702309752, 743.747694579897, 815.0344770681022, 835.9259680498144, 828.7516452314877, 761.8940327911249, 721.1955214805035, 801.287775875725, 777.1689940727033, 732.0711720852528, 763.1596904628609, 808.4207493067047, 708.1193009842185, 866.4277725680114, 823.849752271528, 738.4794099596976, 771.5208467551447, 779.8669244696966, 714.2362291009583, 734.9266616898577, 830.3509624407601, 672.1809381261509, 739.5874352527456, 758.4668650339855, 765.1029565701962, 789.6414427916013, 776.7314706234574, 719.9044262501191, 756.9540518719703, 776.6273350777736, 804.1751536666274, 819.1162468679679, 818.9720204714341, 809.4501579496484, 800.5650587440255, 822.114793390327, 790.1038067382092, 818.8433578573104, 815.2969836946329, 809.8460134989366, 773.9902281883587, 808.6979884458259, 812.878654481062, 808.6597390353996, 773.6105560494121, 750.8426901964026, 747.0410458949617, 727.1271228522443, 635.5472380625062, 790.962935253961, 804.1699330730287, 775.6629405606122, 797.4993564819091, 812.092196155435, 273.2810078362156, 673.1387955410682, 743.2769453352317, 753.3984378432466, 793.7288637560788, 819.2991827261679, 875.9296414872554, 626.3629428489301, 709.1319069431814, 780.3431346459198, 684.6167568381481, 699.196413796035, 167.01807184984068, 792.1949329045212, 140.95502964179775, 796.3658593298796, 106.99968225612356, 133.00099103361418, 120.2985655015922, 149.9023444228587, 679.4262615818762, 786.8811150558295, 748.6998517206732, 802.7330653596374, 146.86742265011026, 154.50285398692205, 633.3827426077034, 213.75517105935398, 690.7971247610444, 243.67899428408833, 776.9225568065972, 124.66243331847758, 619.5474292929122, 691.8414472938678, 91.98547585770396, 626.280989213664, 115.50016097476461, 182.05284199831908, 109.29969253054792, 93.19230728530998, 449.8552815184223, 180.7730297202018, 129.72037941077716, 160.86229826582013, 771.2367891051169, 121.03030412912057, 652.2891347102625, 91.115674242651, 218.56812792133576, 107.08855921978191, 750.9491747356553, 610.2427328873598, 666.1523972764661, 724.3175937899439, 796.2279228940033, 790.2960563119987, 85.86927474844508, 217.75755398389913, 629.5041199038654, 126.21036389659237, 141.80209737575993, 143.13358751046204, 140.16472943457003, 622.085811408145, 158.4442016049741, 607.557011458593, 768.7175280095244, 125.29736011601881, 130.02368730585584, 752.8458771111323, 159.43325904315492, 146.90698254581102, 289.71198842960405, 632.8601113632658, 415.8860355581822, 140.84262371309634, 378.11207870418264, 125.61688064091112, 95.67743829170824, 145.94809840491826, 199.24451098106542, 149.51723579004457, 110.11336414541991, 106.14990391139382, 641.9501588201147, 111.0412020090129, 175.7700150968237, 110.02451591455062, 613.2823936740419, 72.98854488463957, 630.027647653489, 303.367223368646, 614.0776854530895, 150.41964104806576, 117.22182084167781, 118.97818622368254, 157.16610851721265, 634.4353571336278, 773.5571766349767, 126.03203387333018, 107.83571417675296, 136.5784983217972, 85.79520707373491, 85.5055167197929, 73.14312957913054, 613.1701776152192, 94.23245100790713, 131.27535741550014, 187.91959248625386, 83.80080037524498, 155.34452809942857, 89.38477353018571, 117.41463711694458]
Elapsed: 4.855841103336525~5.116482554084977
Time per graph: 0.0028732787593707247~0.003027504469872768
Speed: 629.4432936633884~287.1851750960936
Total Time: 14.3943
best val loss: 0.0799417969809305 test_score: 0.9627

Testing...
Test loss: 0.1253 score: 0.9675 time: 18.86s
test Score 0.9675
Epoch Time List: [11.757770439027809, 11.711095330072567, 12.210727476980537, 10.387575979111716, 10.368071971926838, 10.569913334096782, 10.213528512045741, 10.248894623946398, 9.83224269701168, 10.173475824994966, 10.300324118928984, 10.688114683958702, 10.441479033092037, 10.329199391999282, 10.638811875949614, 10.927575593115762, 11.80326841888018, 10.698172340053134, 10.509322443162091, 11.284602698870003, 10.971020247088745, 10.272307842154987, 10.88901650311891, 10.415233571082354, 10.290869105025195, 10.170538150938228, 10.170974940992892, 10.811731514986604, 10.69054230605252, 10.457778733922169, 10.246139632887207, 10.13686288392637, 10.173006555996835, 10.51213109085802, 10.33895348990336, 11.08825956005603, 10.709281761897728, 10.062379723996855, 10.26445868902374, 10.606364047038369, 10.393126470036805, 10.217317195842043, 10.494951745029539, 10.760982494917698, 10.072795080021024, 10.158271224005148, 10.294752364978194, 10.319614356034435, 10.86182383808773, 10.726872592931613, 10.274099923088215, 10.270078278030269, 9.938920169021003, 9.708534673904069, 9.888081807992421, 10.154748100205325, 10.553793914034031, 10.575228411122225, 10.411127954022959, 10.801363590057008, 10.544778609997593, 9.95128027012106, 10.33606983604841, 10.32859040016774, 10.854004182037897, 10.336923647089861, 10.069440645980649, 10.156840395997278, 10.614505616831593, 10.36516901885625, 10.251510736881755, 10.663879850995727, 10.67920849705115, 10.305540462955832, 10.13791953411419, 10.321850977023132, 10.366543298936449, 10.281779804150574, 10.883393621072173, 10.871895757038146, 11.159621149068698, 11.768102963105775, 10.468135770061053, 10.210085088969208, 10.388633430120535, 11.424266007030383, 10.38969604310114, 10.448470543022268, 10.58899628103245, 11.153777759987861, 11.479574620956555, 10.626004740828648, 10.370117145008408, 10.249337845947593, 10.280492400052026, 10.28284671495203, 10.678846781956963, 10.517936731921509, 10.565854687942192, 10.285638552042656, 10.14027101173997, 10.285092466045171, 10.807485310127959, 10.443138403934427, 11.280672270921059, 11.410175924072973, 10.54336289910134, 10.587150653125718, 10.2704623718746, 10.259005957981572, 10.003304800018668, 11.040192384971306, 11.238014195114374, 10.709363699075766, 10.88815009908285, 10.949029844021425, 10.900586295989342, 10.270315385889262, 10.589451324078254, 9.87423796497751, 10.54964703891892, 10.692069862037897, 10.913703690981492, 10.489957044948824, 10.97670861298684, 11.174484526971355, 10.601866494049318, 11.540642005973496, 11.455686367000453, 10.443437940906733, 10.438230913016014, 10.564564119908027, 10.950150762917474, 10.832828163052909, 11.000452794949524, 10.836017852998339, 10.564970375970006, 10.257674994994886, 10.093915323959664, 10.30348234414123, 10.24430834397208, 10.515986540936865, 10.608830705168657, 10.392332971096039, 10.182452252018265, 10.204258930985816, 10.190677701029927, 10.439980254974216, 10.662758325808682, 10.454025353072211, 10.667746288934723, 10.798598081921227, 10.900806355057284, 10.700893622939475, 11.808671324979514, 10.91097596695181, 10.530768954893574, 10.299275352037512, 10.64939506095834, 10.213694156962447, 18.67144667787943, 18.522611040971242, 12.010071475873701, 10.791328394901939, 11.178725607926026, 10.532828470109962, 10.44335814495571, 11.85221385222394, 11.777629079995677, 10.654383537941612, 12.470221673836932, 25.994377936935052, 23.220544838928618, 22.26553298602812, 33.31948801595718, 10.761881473241374, 32.395658112945966, 40.91568433190696, 43.21868312894367, 50.10089783009607, 22.476811942877248, 25.118369884905405, 29.52497907995712, 34.77636870415881, 41.59601930889767, 25.665462580043823, 32.81920246302616, 43.019009519019164, 11.994157169945538, 44.224550436018035, 10.966047304915264, 63.66602425696328, 25.15783414908219, 31.199095222982578, 56.211070843972266, 19.930932898889296, 37.74428057379555, 19.34306952287443, 55.087372194975615, 35.93127800093498, 36.75209454400465, 52.50757568597328, 39.10544090298936, 27.54416645213496, 20.380725124035962, 41.83724648610223, 30.99953870999161, 40.15626073512249, 17.51168488699477, 50.19027689401992, 19.532850142917596, 46.139564515906386, 19.352392256027088, 31.07870399695821, 27.681287848972715, 24.600958275957964, 28.89059449499473, 17.729305932996795, 45.12212203396484, 55.79500469996128, 45.695907485089265, 47.6275091819698, 51.2972026738571, 28.225664378958754, 33.923715423909016, 20.228095354046673, 34.630711404955946, 50.13367618212942, 47.012393102864735, 41.91314226097893, 27.359666125033982, 57.28796678094659, 37.16706379095558, 43.39017738203984, 54.46460326202214, 45.84695471415762, 42.95913571608253, 45.30360052897595, 44.851423591026105, 53.07564504002221, 40.88087009487208, 55.67104381497484, 52.861003373982385, 59.63767869991716, 42.28741416009143, 60.64768623514101, 52.634976080968045, 58.97743296704721, 55.59166788798757, 55.08456466102507, 42.45132838690188, 54.93513238511514, 53.40917213214561, 52.03284217300825, 50.7279557059519, 65.1906429399969, 34.59711097506806, 44.70554396498483, 11.262615312007256, 56.66395537392236, 70.80623566301074, 61.823212741990574, 85.72693463694304, 72.2700033109868, 60.65555955201853, 66.427207774017, 74.85330827406142, 72.05382707202807, 66.92602696083486, 63.697572685894556, 44.208878824836574, 57.757982832961716, 52.05307539587375]
Total Epoch List: [80, 90, 103]
Total Time List: [2.251737711019814, 2.166522498941049, 14.394346780958585]
========================training times:1========================
========================k_idx:0========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: AEtransGAT
AEtransGAT(
  (gae): GAE(
    (encoder): transGAT(
      (encoder): EdgeFeatureGAT()
      (classifier): Sequential(
        (0): Linear(in_features=64, out_features=64, bias=True)
        (1): ReLU(inplace=True)
        (2): Linear(in_features=64, out_features=2, bias=True)
      )
    )
    (decoder): InnerProductDecoder()
  )
)
Total number of parameters: 1914242
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x71f20acae050>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 2.2849;  Loss pred: 2.2849; Loss self: 0.0000; time: 33.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6944 score: 0.5000 time: 2.65s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6944 score: 0.5000 time: 13.94s
Epoch 2/1000, LR 0.000029
Train loss: 2.1878;  Loss pred: 2.1878; Loss self: 0.0000; time: 39.93s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6944 score: 0.5000 time: 10.41s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6944 score: 0.5000 time: 14.47s
Epoch 3/1000, LR 0.000059
Train loss: 1.9854;  Loss pred: 1.9854; Loss self: 0.0000; time: 46.80s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6943 score: 0.5000 time: 18.48s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6942 score: 0.5000 time: 18.92s
Epoch 4/1000, LR 0.000089
Train loss: 1.7467;  Loss pred: 1.7467; Loss self: 0.0000; time: 31.66s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6940 score: 0.5000 time: 11.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6940 score: 0.5000 time: 8.40s
Epoch 5/1000, LR 0.000119
Train loss: 1.5051;  Loss pred: 1.5051; Loss self: 0.0000; time: 32.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6937 score: 0.5000 time: 12.71s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6937 score: 0.5000 time: 11.75s
Epoch 6/1000, LR 0.000149
Train loss: 1.3093;  Loss pred: 1.3093; Loss self: 0.0000; time: 14.50s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6931 score: 0.5000 time: 15.44s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6931 score: 0.5000 time: 12.98s
Epoch 7/1000, LR 0.000179
Train loss: 1.1702;  Loss pred: 1.1702; Loss self: 0.0000; time: 23.71s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6926 score: 0.5000 time: 16.82s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6926 score: 0.5000 time: 11.54s
Epoch 8/1000, LR 0.000209
Train loss: 1.0844;  Loss pred: 1.0844; Loss self: 0.0000; time: 37.34s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6917 score: 0.5000 time: 9.36s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6916 score: 0.5000 time: 4.78s
Epoch 9/1000, LR 0.000239
Train loss: 1.0354;  Loss pred: 1.0354; Loss self: 0.0000; time: 30.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6906 score: 0.5000 time: 15.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6906 score: 0.5000 time: 14.49s
Epoch 10/1000, LR 0.000269
Train loss: 1.0076;  Loss pred: 1.0076; Loss self: 0.0000; time: 48.03s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6886 score: 0.5000 time: 14.03s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6884 score: 0.5000 time: 3.20s
Epoch 11/1000, LR 0.000299
Train loss: 0.9925;  Loss pred: 0.9925; Loss self: 0.0000; time: 39.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6856 score: 0.5000 time: 16.38s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6855 score: 0.5000 time: 5.11s
Epoch 12/1000, LR 0.000299
Train loss: 0.9820;  Loss pred: 0.9820; Loss self: 0.0000; time: 19.79s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6815 score: 0.5000 time: 2.78s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6813 score: 0.5000 time: 2.40s
Epoch 13/1000, LR 0.000299
Train loss: 0.9752;  Loss pred: 0.9752; Loss self: 0.0000; time: 6.80s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6756 score: 0.5000 time: 16.67s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6753 score: 0.5000 time: 2.59s
Epoch 14/1000, LR 0.000299
Train loss: 0.9685;  Loss pred: 0.9685; Loss self: 0.0000; time: 26.31s
Val loss: 0.6672 score: 0.5012 time: 2.60s
Test loss: 0.6668 score: 0.5036 time: 16.53s
Epoch 15/1000, LR 0.000299
Train loss: 0.9614;  Loss pred: 0.9614; Loss self: 0.0000; time: 18.01s
Val loss: 0.6550 score: 0.5556 time: 2.63s
Test loss: 0.6547 score: 0.5592 time: 13.78s
Epoch 16/1000, LR 0.000299
Train loss: 0.9527;  Loss pred: 0.9527; Loss self: 0.0000; time: 27.62s
Val loss: 0.6386 score: 0.6947 time: 14.67s
Test loss: 0.6383 score: 0.6929 time: 2.59s
Epoch 17/1000, LR 0.000299
Train loss: 0.9423;  Loss pred: 0.9423; Loss self: 0.0000; time: 27.19s
Val loss: 0.6176 score: 0.8000 time: 14.20s
Test loss: 0.6173 score: 0.7864 time: 21.67s
Epoch 18/1000, LR 0.000299
Train loss: 0.9283;  Loss pred: 0.9283; Loss self: 0.0000; time: 19.85s
Val loss: 0.5900 score: 0.8444 time: 15.00s
Test loss: 0.5897 score: 0.8302 time: 12.31s
Epoch 19/1000, LR 0.000299
Train loss: 0.9107;  Loss pred: 0.9107; Loss self: 0.0000; time: 23.07s
Val loss: 0.5552 score: 0.8645 time: 16.88s
Test loss: 0.5552 score: 0.8473 time: 12.62s
Epoch 20/1000, LR 0.000299
Train loss: 0.8915;  Loss pred: 0.8915; Loss self: 0.0000; time: 19.10s
Val loss: 0.5166 score: 0.8893 time: 12.93s
Test loss: 0.5168 score: 0.8728 time: 11.78s
Epoch 21/1000, LR 0.000299
Train loss: 0.8668;  Loss pred: 0.8668; Loss self: 0.0000; time: 19.81s
Val loss: 0.4758 score: 0.8935 time: 14.08s
Test loss: 0.4764 score: 0.8882 time: 19.33s
Epoch 22/1000, LR 0.000299
Train loss: 0.8438;  Loss pred: 0.8438; Loss self: 0.0000; time: 28.19s
Val loss: 0.4329 score: 0.9024 time: 2.93s
Test loss: 0.4337 score: 0.8935 time: 16.64s
Epoch 23/1000, LR 0.000299
Train loss: 0.8176;  Loss pred: 0.8176; Loss self: 0.0000; time: 19.81s
Val loss: 0.3911 score: 0.9083 time: 3.08s
Test loss: 0.3924 score: 0.8976 time: 18.41s
Epoch 24/1000, LR 0.000299
Train loss: 0.7947;  Loss pred: 0.7947; Loss self: 0.0000; time: 21.33s
Val loss: 0.3524 score: 0.9136 time: 10.49s
Test loss: 0.3538 score: 0.9065 time: 9.64s
Epoch 25/1000, LR 0.000299
Train loss: 0.7705;  Loss pred: 0.7705; Loss self: 0.0000; time: 20.54s
Val loss: 0.3194 score: 0.9160 time: 19.41s
Test loss: 0.3208 score: 0.9124 time: 2.31s
Epoch 26/1000, LR 0.000299
Train loss: 0.7496;  Loss pred: 0.7496; Loss self: 0.0000; time: 21.12s
Val loss: 0.2920 score: 0.9183 time: 2.41s
Test loss: 0.2933 score: 0.9148 time: 8.57s
Epoch 27/1000, LR 0.000299
Train loss: 0.7328;  Loss pred: 0.7328; Loss self: 0.0000; time: 30.30s
Val loss: 0.2697 score: 0.9195 time: 21.04s
Test loss: 0.2708 score: 0.9189 time: 2.75s
Epoch 28/1000, LR 0.000299
Train loss: 0.7183;  Loss pred: 0.7183; Loss self: 0.0000; time: 31.38s
Val loss: 0.2513 score: 0.9225 time: 2.98s
Test loss: 0.2522 score: 0.9219 time: 7.61s
Epoch 29/1000, LR 0.000299
Train loss: 0.7050;  Loss pred: 0.7050; Loss self: 0.0000; time: 49.76s
Val loss: 0.2360 score: 0.9237 time: 22.10s
Test loss: 0.2366 score: 0.9249 time: 2.82s
Epoch 30/1000, LR 0.000299
Train loss: 0.6930;  Loss pred: 0.6930; Loss self: 0.0000; time: 36.50s
Val loss: 0.2234 score: 0.9272 time: 3.26s
Test loss: 0.2237 score: 0.9290 time: 16.03s
Epoch 31/1000, LR 0.000299
Train loss: 0.6872;  Loss pred: 0.6872; Loss self: 0.0000; time: 34.00s
Val loss: 0.2120 score: 0.9302 time: 21.25s
Test loss: 0.2120 score: 0.9331 time: 2.53s
Epoch 32/1000, LR 0.000299
Train loss: 0.6773;  Loss pred: 0.6773; Loss self: 0.0000; time: 36.96s
Val loss: 0.2024 score: 0.9343 time: 2.85s
Test loss: 0.2022 score: 0.9343 time: 50.74s
Epoch 33/1000, LR 0.000299
Train loss: 0.6713;  Loss pred: 0.6713; Loss self: 0.0000; time: 224.81s
Val loss: 0.1942 score: 0.9343 time: 2.78s
Test loss: 0.1939 score: 0.9355 time: 13.18s
Epoch 34/1000, LR 0.000298
Train loss: 0.6650;  Loss pred: 0.6650; Loss self: 0.0000; time: 114.58s
Val loss: 0.1872 score: 0.9367 time: 2.85s
Test loss: 0.1868 score: 0.9391 time: 64.49s
Epoch 35/1000, LR 0.000298
Train loss: 0.6599;  Loss pred: 0.6599; Loss self: 0.0000; time: 63.45s
Val loss: 0.1806 score: 0.9391 time: 2.71s
Test loss: 0.1800 score: 0.9402 time: 2.79s
Epoch 36/1000, LR 0.000298
Train loss: 0.6552;  Loss pred: 0.6552; Loss self: 0.0000; time: 30.54s
Val loss: 0.1749 score: 0.9396 time: 10.87s
Test loss: 0.1742 score: 0.9408 time: 6.19s
Epoch 37/1000, LR 0.000298
Train loss: 0.6515;  Loss pred: 0.6515; Loss self: 0.0000; time: 38.95s
Val loss: 0.1697 score: 0.9426 time: 2.79s
Test loss: 0.1688 score: 0.9426 time: 16.20s
Epoch 38/1000, LR 0.000298
Train loss: 0.6478;  Loss pred: 0.6478; Loss self: 0.0000; time: 26.34s
Val loss: 0.1647 score: 0.9420 time: 4.29s
Test loss: 0.1638 score: 0.9450 time: 25.96s
Epoch 39/1000, LR 0.000298
Train loss: 0.6431;  Loss pred: 0.6431; Loss self: 0.0000; time: 20.79s
Val loss: 0.1608 score: 0.9467 time: 19.00s
Test loss: 0.1598 score: 0.9456 time: 18.49s
Epoch 40/1000, LR 0.000298
Train loss: 0.6401;  Loss pred: 0.6401; Loss self: 0.0000; time: 20.93s
Val loss: 0.1569 score: 0.9497 time: 17.68s
Test loss: 0.1558 score: 0.9462 time: 19.95s
Epoch 41/1000, LR 0.000298
Train loss: 0.6382;  Loss pred: 0.6382; Loss self: 0.0000; time: 37.44s
Val loss: 0.1531 score: 0.9521 time: 21.51s
Test loss: 0.1518 score: 0.9473 time: 2.72s
Epoch 42/1000, LR 0.000298
Train loss: 0.6342;  Loss pred: 0.6342; Loss self: 0.0000; time: 39.15s
Val loss: 0.1503 score: 0.9521 time: 2.89s
Test loss: 0.1490 score: 0.9473 time: 17.22s
Epoch 43/1000, LR 0.000298
Train loss: 0.6323;  Loss pred: 0.6323; Loss self: 0.0000; time: 15.38s
Val loss: 0.1469 score: 0.9550 time: 18.29s
Test loss: 0.1453 score: 0.9479 time: 14.27s
Epoch 44/1000, LR 0.000298
Train loss: 0.6305;  Loss pred: 0.6305; Loss self: 0.0000; time: 16.39s
Val loss: 0.1441 score: 0.9544 time: 14.30s
Test loss: 0.1423 score: 0.9503 time: 2.61s
Epoch 45/1000, LR 0.000298
Train loss: 0.6267;  Loss pred: 0.6267; Loss self: 0.0000; time: 34.62s
Val loss: 0.1421 score: 0.9556 time: 2.85s
Test loss: 0.1401 score: 0.9497 time: 12.52s
Epoch 46/1000, LR 0.000298
Train loss: 0.6263;  Loss pred: 0.6263; Loss self: 0.0000; time: 35.00s
Val loss: 0.1392 score: 0.9562 time: 2.83s
Test loss: 0.1370 score: 0.9515 time: 19.48s
Epoch 47/1000, LR 0.000298
Train loss: 0.6226;  Loss pred: 0.6226; Loss self: 0.0000; time: 18.13s
Val loss: 0.1380 score: 0.9568 time: 13.05s
Test loss: 0.1356 score: 0.9509 time: 8.87s
Epoch 48/1000, LR 0.000298
Train loss: 0.6221;  Loss pred: 0.6221; Loss self: 0.0000; time: 27.84s
Val loss: 0.1356 score: 0.9586 time: 2.36s
Test loss: 0.1328 score: 0.9509 time: 19.55s
Epoch 49/1000, LR 0.000298
Train loss: 0.6188;  Loss pred: 0.6188; Loss self: 0.0000; time: 30.03s
Val loss: 0.1339 score: 0.9592 time: 2.74s
Test loss: 0.1308 score: 0.9509 time: 12.63s
Epoch 50/1000, LR 0.000298
Train loss: 0.6170;  Loss pred: 0.6170; Loss self: 0.0000; time: 32.12s
Val loss: 0.1322 score: 0.9592 time: 22.04s
Test loss: 0.1288 score: 0.9515 time: 2.64s
Epoch 51/1000, LR 0.000298
Train loss: 0.6160;  Loss pred: 0.6160; Loss self: 0.0000; time: 32.51s
Val loss: 0.1305 score: 0.9592 time: 13.09s
Test loss: 0.1268 score: 0.9521 time: 3.24s
Epoch 52/1000, LR 0.000298
Train loss: 0.6146;  Loss pred: 0.6146; Loss self: 0.0000; time: 29.79s
Val loss: 0.1292 score: 0.9592 time: 3.11s
Test loss: 0.1252 score: 0.9527 time: 19.72s
Epoch 53/1000, LR 0.000298
Train loss: 0.6116;  Loss pred: 0.6116; Loss self: 0.0000; time: 22.32s
Val loss: 0.1283 score: 0.9586 time: 13.51s
Test loss: 0.1240 score: 0.9527 time: 3.28s
Epoch 54/1000, LR 0.000297
Train loss: 0.6112;  Loss pred: 0.6112; Loss self: 0.0000; time: 272.23s
Val loss: 0.1269 score: 0.9598 time: 40.61s
Test loss: 0.1223 score: 0.9527 time: 2.69s
Epoch 55/1000, LR 0.000297
Train loss: 0.6090;  Loss pred: 0.6090; Loss self: 0.0000; time: 95.92s
Val loss: 0.1256 score: 0.9604 time: 73.51s
Test loss: 0.1208 score: 0.9533 time: 31.34s
Epoch 56/1000, LR 0.000297
Train loss: 0.6073;  Loss pred: 0.6073; Loss self: 0.0000; time: 102.71s
Val loss: 0.1243 score: 0.9604 time: 14.10s
Test loss: 0.1192 score: 0.9533 time: 11.87s
Epoch 57/1000, LR 0.000297
Train loss: 0.6049;  Loss pred: 0.6049; Loss self: 0.0000; time: 25.94s
Val loss: 0.1238 score: 0.9615 time: 8.94s
Test loss: 0.1183 score: 0.9533 time: 11.67s
Epoch 58/1000, LR 0.000297
Train loss: 0.6045;  Loss pred: 0.6045; Loss self: 0.0000; time: 26.43s
Val loss: 0.1222 score: 0.9609 time: 14.42s
Test loss: 0.1164 score: 0.9544 time: 16.35s
Epoch 59/1000, LR 0.000297
Train loss: 0.6026;  Loss pred: 0.6026; Loss self: 0.0000; time: 30.21s
Val loss: 0.1213 score: 0.9615 time: 9.36s
Test loss: 0.1153 score: 0.9544 time: 3.53s
Epoch 60/1000, LR 0.000297
Train loss: 0.6008;  Loss pred: 0.6008; Loss self: 0.0000; time: 31.02s
Val loss: 0.1208 score: 0.9615 time: 5.17s
Test loss: 0.1146 score: 0.9538 time: 4.79s
Epoch 61/1000, LR 0.000297
Train loss: 0.5993;  Loss pred: 0.5993; Loss self: 0.0000; time: 28.93s
Val loss: 0.1199 score: 0.9615 time: 11.20s
Test loss: 0.1135 score: 0.9562 time: 9.02s
Epoch 62/1000, LR 0.000297
Train loss: 0.5985;  Loss pred: 0.5985; Loss self: 0.0000; time: 38.32s
Val loss: 0.1188 score: 0.9645 time: 3.49s
Test loss: 0.1122 score: 0.9627 time: 16.39s
Epoch 63/1000, LR 0.000297
Train loss: 0.5964;  Loss pred: 0.5964; Loss self: 0.0000; time: 25.59s
Val loss: 0.1181 score: 0.9645 time: 11.14s
Test loss: 0.1113 score: 0.9633 time: 11.04s
Epoch 64/1000, LR 0.000297
Train loss: 0.5955;  Loss pred: 0.5955; Loss self: 0.0000; time: 34.07s
Val loss: 0.1172 score: 0.9657 time: 14.95s
Test loss: 0.1101 score: 0.9639 time: 6.82s
Epoch 65/1000, LR 0.000297
Train loss: 0.5936;  Loss pred: 0.5936; Loss self: 0.0000; time: 26.69s
Val loss: 0.1166 score: 0.9657 time: 18.13s
Test loss: 0.1093 score: 0.9645 time: 3.64s
Epoch 66/1000, LR 0.000297
Train loss: 0.5925;  Loss pred: 0.5925; Loss self: 0.0000; time: 36.07s
Val loss: 0.1159 score: 0.9657 time: 14.23s
Test loss: 0.1083 score: 0.9633 time: 3.36s
Epoch 67/1000, LR 0.000297
Train loss: 0.5904;  Loss pred: 0.5904; Loss self: 0.0000; time: 35.99s
Val loss: 0.1154 score: 0.9657 time: 16.48s
Test loss: 0.1077 score: 0.9627 time: 13.43s
Epoch 68/1000, LR 0.000296
Train loss: 0.5881;  Loss pred: 0.5881; Loss self: 0.0000; time: 52.31s
Val loss: 0.1152 score: 0.9657 time: 3.52s
Test loss: 0.1072 score: 0.9627 time: 15.45s
Epoch 69/1000, LR 0.000296
Train loss: 0.5876;  Loss pred: 0.5876; Loss self: 0.0000; time: 8.97s
Val loss: 0.1140 score: 0.9651 time: 2.98s
Test loss: 0.1058 score: 0.9633 time: 16.23s
Epoch 70/1000, LR 0.000296
Train loss: 0.5858;  Loss pred: 0.5858; Loss self: 0.0000; time: 26.89s
Val loss: 0.1141 score: 0.9651 time: 18.50s
Test loss: 0.1058 score: 0.9621 time: 7.17s
     INFO: Early stopping counter 1 of 2
Epoch 71/1000, LR 0.000296
Train loss: 0.5850;  Loss pred: 0.5850; Loss self: 0.0000; time: 25.17s
Val loss: 0.1136 score: 0.9651 time: 14.13s
Test loss: 0.1051 score: 0.9615 time: 12.55s
Epoch 72/1000, LR 0.000296
Train loss: 0.5827;  Loss pred: 0.5827; Loss self: 0.0000; time: 26.23s
Val loss: 0.1132 score: 0.9651 time: 13.57s
Test loss: 0.1044 score: 0.9615 time: 3.64s
Epoch 73/1000, LR 0.000296
Train loss: 0.5811;  Loss pred: 0.5811; Loss self: 0.0000; time: 37.65s
Val loss: 0.1129 score: 0.9651 time: 14.49s
Test loss: 0.1039 score: 0.9621 time: 14.57s
Epoch 74/1000, LR 0.000296
Train loss: 0.5800;  Loss pred: 0.5800; Loss self: 0.0000; time: 16.21s
Val loss: 0.1124 score: 0.9657 time: 14.94s
Test loss: 0.1032 score: 0.9615 time: 3.65s
Epoch 75/1000, LR 0.000296
Train loss: 0.5786;  Loss pred: 0.5786; Loss self: 0.0000; time: 32.91s
Val loss: 0.1120 score: 0.9663 time: 23.29s
Test loss: 0.1026 score: 0.9627 time: 3.34s
Epoch 76/1000, LR 0.000296
Train loss: 0.5763;  Loss pred: 0.5763; Loss self: 0.0000; time: 36.39s
Val loss: 0.1115 score: 0.9663 time: 3.41s
Test loss: 0.1017 score: 0.9627 time: 17.97s
Epoch 77/1000, LR 0.000296
Train loss: 0.5753;  Loss pred: 0.5753; Loss self: 0.0000; time: 46.45s
Val loss: 0.1113 score: 0.9669 time: 12.54s
Test loss: 0.1013 score: 0.9639 time: 3.35s
Epoch 78/1000, LR 0.000296
Train loss: 0.5730;  Loss pred: 0.5730; Loss self: 0.0000; time: 44.84s
Val loss: 0.1110 score: 0.9669 time: 4.05s
Test loss: 0.1008 score: 0.9633 time: 16.21s
Epoch 79/1000, LR 0.000295
Train loss: 0.5710;  Loss pred: 0.5710; Loss self: 0.0000; time: 31.72s
Val loss: 0.1112 score: 0.9669 time: 20.32s
Test loss: 0.1008 score: 0.9633 time: 4.17s
     INFO: Early stopping counter 1 of 2
Epoch 80/1000, LR 0.000295
Train loss: 0.5696;  Loss pred: 0.5696; Loss self: 0.0000; time: 36.50s
Val loss: 0.1110 score: 0.9669 time: 11.69s
Test loss: 0.1005 score: 0.9633 time: 13.29s
Epoch 81/1000, LR 0.000295
Train loss: 0.5687;  Loss pred: 0.5687; Loss self: 0.0000; time: 24.22s
Val loss: 0.1110 score: 0.9669 time: 8.54s
Test loss: 0.1004 score: 0.9633 time: 15.18s
     INFO: Early stopping counter 1 of 2
Epoch 82/1000, LR 0.000295
Train loss: 0.5672;  Loss pred: 0.5672; Loss self: 0.0000; time: 34.07s
Val loss: 0.1107 score: 0.9663 time: 3.19s
Test loss: 0.1000 score: 0.9633 time: 14.55s
Epoch 83/1000, LR 0.000295
Train loss: 0.5655;  Loss pred: 0.5655; Loss self: 0.0000; time: 33.64s
Val loss: 0.1107 score: 0.9657 time: 12.34s
Test loss: 0.0999 score: 0.9639 time: 3.82s
     INFO: Early stopping counter 1 of 2
Epoch 84/1000, LR 0.000295
Train loss: 0.5632;  Loss pred: 0.5632; Loss self: 0.0000; time: 41.12s
Val loss: 0.1108 score: 0.9651 time: 8.07s
Test loss: 0.0999 score: 0.9639 time: 12.23s
     INFO: Early stopping counter 2 of 2
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 081,   Train_Loss: 0.5672,   Val_Loss: 0.1107,   Val_Precision: 0.9782,   Val_Recall: 0.9538,   Val_accuracy: 0.9658,   Val_Score: 0.9663,   Val_Loss: 0.1107,   Test_Precision: 0.9815,   Test_Recall: 0.9444,   Test_accuracy: 0.9626,   Test_Score: 0.9633,   Test_loss: 0.1000


[13.949843397014774, 14.47833548404742, 18.9286899860017, 8.408394561964087, 11.76012842496857, 12.981042103958316, 11.544976261095144, 4.789094323059544, 14.499288861989044, 3.2061719859484583, 5.117124191951007, 2.4087865030160174, 2.591102499049157, 16.538114755996503, 13.790494148037396, 2.598531006020494, 21.675999352009967, 12.314670338993892, 12.621489405049942, 11.789681688998826, 19.33057677594479, 16.649264479987323, 18.415471957996488, 9.64673663501162, 2.313939257990569, 8.578555540996604, 2.754034681012854, 7.6123750150436535, 2.8218725440092385, 16.03345513099339, 2.538399558980018, 50.747085006092675, 13.18628157209605, 64.49679519503843, 2.7966133230365813, 6.192231667926535, 16.20289676601533, 25.96750794001855, 18.491944027948193, 19.95253770891577, 2.7219541399972513, 17.22892918996513, 14.274844339000992, 2.617070795968175, 12.529470354085788, 19.48262694803998, 8.876522541977465, 19.550846505910158, 12.637942244997248, 2.647476025042124, 3.2481846211012453, 19.727041669073515, 3.2856190679594874, 2.6992552200099453, 31.34693169395905, 11.87210593407508, 11.675102123990655, 16.356103308033198, 3.5374078870518133, 4.803426983067766, 9.030407494981773, 16.394424356985837, 11.048905068077147, 6.828486741986126, 3.647561792982742, 3.3693839700426906, 13.436971636954695, 15.459368840092793, 16.23427910602186, 7.180336119025014, 12.55350855703, 3.643562998971902, 14.57499261002522, 3.6587844169698656, 3.3490627000574023, 17.971484843990766, 3.3558372430270538, 16.216027852962725, 4.177285106037743, 13.298826853046194, 15.181342590949498, 14.557791846920736, 3.834234243957326, 12.239592362078838]
[0.008254345205334187, 0.008567062416596107, 0.011200408275740651, 0.004975381397611886, 0.0069586558727624676, 0.007681090002342199, 0.006831346900056298, 0.0028337836231121564, 0.008579460865082274, 0.0018971431869517505, 0.0030278841372491164, 0.0014253174574059275, 0.0015331967449995012, 0.009785866719524559, 0.008160055708897867, 0.00153759231125473, 0.012826035119532524, 0.007286787182836623, 0.007468336926065054, 0.006976143011241909, 0.011438211110026504, 0.009851635786974747, 0.010896728969228692, 0.005708128186397409, 0.0013691948272133545, 0.005076068367453611, 0.001629606320125949, 0.004504363914227014, 0.0016697470674610878, 0.009487251556800821, 0.0015020115733609574, 0.030027860950350694, 0.007802533474613047, 0.03816378413907599, 0.0016548007828618824, 0.0036640424070571212, 0.009587512879299011, 0.015365389313620444, 0.010941978714762244, 0.011806235330719391, 0.0016106237514776636, 0.0101946326567841, 0.008446653455030173, 0.0015485626011646006, 0.0074138877834827146, 0.01152818162605916, 0.00525238020235353, 0.011568548228349206, 0.0074780723343178986, 0.001566553860971671, 0.001922002734379435, 0.011672805721345275, 0.0019441532946505842, 0.0015971924378757074, 0.01854848029228346, 0.0070249147538905795, 0.0069083444520654765, 0.009678167637889467, 0.00209314076156912, 0.0028422644870223467, 0.0053434363875631795, 0.009700842814784519, 0.006537813649749791, 0.0040405246994000745, 0.002158320587563753, 0.0019937183254690478, 0.007950870791097452, 0.009147555526682127, 0.009606082311255537, 0.004248719597056222, 0.0074281115722071005, 0.0021559544372614804, 0.008624255982263443, 0.0021649611934732933, 0.0019816939053594095, 0.010634014700586251, 0.0019857025106668957, 0.009595282753232381, 0.002471766334933576, 0.00786912831541195, 0.008983042953224555, 0.00861407801592943, 0.0022687776591463465, 0.007242362344425348]
[121.14831341846138, 116.72612517247461, 89.28245965514796, 200.98961669149347, 143.70591365412886, 130.1898558271117, 146.38401689010388, 352.8850939232144, 116.55744058113497, 527.1083420997641, 330.26362789050336, 701.598085959037, 652.2320134460796, 102.18818921831632, 122.54818296271954, 650.367456106726, 77.96641679836966, 137.2346927265024, 133.8986189160702, 143.34568520004834, 87.42625838785405, 101.50598556659403, 91.77065914219793, 175.18877771228426, 730.3562503484212, 197.00286276909344, 613.6451409458892, 222.00692906749927, 598.8931015285668, 105.40460469641096, 665.7738313975588, 33.30240544451173, 128.16350013155096, 26.202852326064214, 604.3023488728098, 272.9226053917804, 104.30233707004051, 65.08133178985341, 91.39114835334686, 84.70100518816838, 620.8774700376497, 98.09083207471355, 118.39008257222598, 645.7601386265866, 134.8819983798358, 86.74394908382824, 190.38987306210464, 86.44126992092738, 133.7243015704546, 638.3438354170215, 520.2906229594279, 85.66920617648654, 514.3627319674534, 626.0986317528631, 53.91277259603958, 142.35048182558714, 144.75248113909797, 103.32534395096215, 477.7509560562718, 351.83214108537595, 187.145486063518, 103.0838267450283, 152.95633273950398, 247.4926091030893, 463.32319941810397, 501.57536660287116, 125.77238723583518, 109.31882261694301, 104.10071115342105, 235.3650263700298, 134.62371832722377, 463.83169454648225, 115.95203134700428, 461.90204379399466, 504.61879975284836, 94.03786134928609, 503.60010859036043, 104.21787723380304, 404.5689860999232, 127.07887836083023, 111.32085254485395, 116.08903450267897, 440.76597632588704, 138.0764938901087]
Elapsed: 11.941450654889369~9.662842624430471
Time per graph: 0.007065947133070632~0.005717658357651165
Speed: 258.5806110504339~206.7410874499338
Total Time: 12.2402
best val loss: 0.11069383429881384 test_score: 0.9633

Testing...
Test loss: 0.1013 score: 0.9639 time: 16.41s
test Score 0.9639
Epoch Time List: [49.75316980993375, 64.81354755198117, 84.20819640601985, 51.30455477593932, 56.61773758998606, 42.90884113300126, 52.05909938714467, 51.48066081199795, 59.90493578789756, 65.26616330095567, 60.702709062956274, 24.973904476035386, 26.054543146979995, 45.44348083809018, 34.42676243092865, 44.88741596415639, 63.060382734867744, 47.16527542890981, 52.56737354688812, 43.811824036878534, 53.21504718903452, 47.763190194033086, 41.306771852076054, 41.461726585170254, 42.26118015591055, 32.10907772695646, 54.08549654006492, 41.970322001958266, 74.67898021207657, 55.79275807202794, 57.785349357058294, 90.54559806804173, 240.77022502210457, 181.92516272782814, 68.95055509719532, 47.59745459491387, 57.931134973070584, 56.586931089055724, 58.27532891603187, 58.55789281707257, 61.66861688694917, 59.26813076506369, 47.935911902925, 33.29774593003094, 49.99971009010915, 57.312754777027294, 40.04704944090918, 49.747248028055765, 45.396703021018766, 56.80500474211294, 48.839708331855945, 52.62375906901434, 39.10578359302599, 315.5368702709675, 200.76877739990596, 128.67129278404173, 46.55711358611006, 57.19428976310883, 43.10187075880822, 40.98619308101479, 49.16031530709006, 58.20000787090976, 47.76827681006398, 55.83408093790058, 48.46285150991753, 53.661158973001875, 65.90901638299692, 71.28452692192513, 28.177924149786122, 52.56777732493356, 51.84965166996699, 43.44090598297771, 66.70895859389566, 34.80249892605934, 59.541050752974115, 57.76347738306504, 62.33999379002489, 65.09439359093085, 56.21240401803516, 61.48117225407623, 47.931114753941074, 51.812166279996745, 49.81023962900508, 61.42616542603355]
Total Epoch List: [84]
Total Time List: [12.24022517807316]
========================k_idx:1========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: AEtransGAT
AEtransGAT(
  (gae): GAE(
    (encoder): transGAT(
      (encoder): EdgeFeatureGAT()
      (classifier): Sequential(
        (0): Linear(in_features=64, out_features=64, bias=True)
        (1): ReLU(inplace=True)
        (2): Linear(in_features=64, out_features=2, bias=True)
      )
    )
    (decoder): InnerProductDecoder()
  )
)
Total number of parameters: 1914242
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x71f20acade40>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 2.0958;  Loss pred: 2.0958; Loss self: 0.0000; time: 41.31s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6928 score: 0.5000 time: 14.53s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6930 score: 0.5000 time: 17.91s
Epoch 2/1000, LR 0.000029
Train loss: 2.0117;  Loss pred: 2.0117; Loss self: 0.0000; time: 34.67s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6927 score: 0.5000 time: 13.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6929 score: 0.5000 time: 10.53s
Epoch 3/1000, LR 0.000059
Train loss: 1.8677;  Loss pred: 1.8677; Loss self: 0.0000; time: 33.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6924 score: 0.5000 time: 3.54s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6926 score: 0.5000 time: 3.39s
Epoch 4/1000, LR 0.000089
Train loss: 1.6837;  Loss pred: 1.6837; Loss self: 0.0000; time: 9.31s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6917 score: 0.5000 time: 3.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6920 score: 0.5000 time: 3.26s
Epoch 5/1000, LR 0.000119
Train loss: 1.4922;  Loss pred: 1.4922; Loss self: 0.0000; time: 9.72s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6907 score: 0.5000 time: 3.64s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6910 score: 0.5000 time: 3.02s
Epoch 6/1000, LR 0.000149
Train loss: 1.3254;  Loss pred: 1.3254; Loss self: 0.0000; time: 8.82s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6895 score: 0.5000 time: 3.71s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6898 score: 0.5000 time: 3.39s
Epoch 7/1000, LR 0.000179
Train loss: 1.1972;  Loss pred: 1.1972; Loss self: 0.0000; time: 17.56s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6879 score: 0.5000 time: 3.02s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6883 score: 0.5000 time: 3.00s
Epoch 8/1000, LR 0.000209
Train loss: 1.1101;  Loss pred: 1.1101; Loss self: 0.0000; time: 25.95s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6851 score: 0.5000 time: 7.89s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6858 score: 0.5000 time: 2.88s
Epoch 9/1000, LR 0.000239
Train loss: 1.0560;  Loss pred: 1.0560; Loss self: 0.0000; time: 21.60s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6808 score: 0.5000 time: 22.44s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6817 score: 0.5000 time: 12.72s
Epoch 10/1000, LR 0.000269
Train loss: 1.0225;  Loss pred: 1.0225; Loss self: 0.0000; time: 23.03s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6738 score: 0.5000 time: 12.54s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6750 score: 0.5000 time: 21.26s
Epoch 11/1000, LR 0.000299
Train loss: 0.9996;  Loss pred: 0.9996; Loss self: 0.0000; time: 31.68s
Val loss: 0.6636 score: 0.5036 time: 22.35s
Test loss: 0.6652 score: 0.5012 time: 17.46s
Epoch 12/1000, LR 0.000299
Train loss: 0.9819;  Loss pred: 0.9819; Loss self: 0.0000; time: 23.35s
Val loss: 0.6500 score: 0.5651 time: 17.68s
Test loss: 0.6522 score: 0.5497 time: 17.00s
Epoch 13/1000, LR 0.000299
Train loss: 0.9663;  Loss pred: 0.9663; Loss self: 0.0000; time: 41.08s
Val loss: 0.6323 score: 0.7189 time: 16.04s
Test loss: 0.6353 score: 0.7024 time: 3.45s
Epoch 14/1000, LR 0.000299
Train loss: 0.9513;  Loss pred: 0.9513; Loss self: 0.0000; time: 30.61s
Val loss: 0.6105 score: 0.8160 time: 16.71s
Test loss: 0.6145 score: 0.7994 time: 11.97s
Epoch 15/1000, LR 0.000299
Train loss: 0.9351;  Loss pred: 0.9351; Loss self: 0.0000; time: 34.30s
Val loss: 0.5839 score: 0.8592 time: 3.37s
Test loss: 0.5889 score: 0.8521 time: 12.06s
Epoch 16/1000, LR 0.000299
Train loss: 0.9159;  Loss pred: 0.9159; Loss self: 0.0000; time: 29.13s
Val loss: 0.5529 score: 0.8746 time: 15.63s
Test loss: 0.5589 score: 0.8704 time: 12.96s
Epoch 17/1000, LR 0.000299
Train loss: 0.8972;  Loss pred: 0.8972; Loss self: 0.0000; time: 34.66s
Val loss: 0.5180 score: 0.8953 time: 10.31s
Test loss: 0.5248 score: 0.8893 time: 3.82s
Epoch 18/1000, LR 0.000299
Train loss: 0.8748;  Loss pred: 0.8748; Loss self: 0.0000; time: 32.45s
Val loss: 0.4816 score: 0.9053 time: 11.92s
Test loss: 0.4892 score: 0.8959 time: 13.79s
Epoch 19/1000, LR 0.000299
Train loss: 0.8531;  Loss pred: 0.8531; Loss self: 0.0000; time: 20.44s
Val loss: 0.4443 score: 0.9089 time: 12.28s
Test loss: 0.4524 score: 0.9036 time: 14.93s
Epoch 20/1000, LR 0.000299
Train loss: 0.8307;  Loss pred: 0.8307; Loss self: 0.0000; time: 24.81s
Val loss: 0.4064 score: 0.9166 time: 16.45s
Test loss: 0.4148 score: 0.9118 time: 3.67s
Epoch 21/1000, LR 0.000299
Train loss: 0.8073;  Loss pred: 0.8073; Loss self: 0.0000; time: 35.04s
Val loss: 0.3701 score: 0.9207 time: 3.12s
Test loss: 0.3788 score: 0.9166 time: 2.92s
Epoch 22/1000, LR 0.000299
Train loss: 0.7863;  Loss pred: 0.7863; Loss self: 0.0000; time: 34.44s
Val loss: 0.3357 score: 0.9249 time: 3.26s
Test loss: 0.3444 score: 0.9231 time: 18.97s
Epoch 23/1000, LR 0.000299
Train loss: 0.7643;  Loss pred: 0.7643; Loss self: 0.0000; time: 35.21s
Val loss: 0.3043 score: 0.9290 time: 15.77s
Test loss: 0.3129 score: 0.9302 time: 18.12s
Epoch 24/1000, LR 0.000299
Train loss: 0.7448;  Loss pred: 0.7448; Loss self: 0.0000; time: 35.89s
Val loss: 0.2773 score: 0.9349 time: 15.07s
Test loss: 0.2856 score: 0.9320 time: 5.94s
Epoch 25/1000, LR 0.000299
Train loss: 0.7274;  Loss pred: 0.7274; Loss self: 0.0000; time: 19.91s
Val loss: 0.2547 score: 0.9278 time: 13.81s
Test loss: 0.2626 score: 0.9183 time: 2.90s
Epoch 26/1000, LR 0.000299
Train loss: 0.7115;  Loss pred: 0.7115; Loss self: 0.0000; time: 31.81s
Val loss: 0.2348 score: 0.9272 time: 2.86s
Test loss: 0.2423 score: 0.9219 time: 12.49s
Epoch 27/1000, LR 0.000299
Train loss: 0.6977;  Loss pred: 0.6977; Loss self: 0.0000; time: 24.04s
Val loss: 0.2181 score: 0.9272 time: 10.72s
Test loss: 0.2250 score: 0.9254 time: 14.42s
Epoch 28/1000, LR 0.000299
Train loss: 0.6871;  Loss pred: 0.6871; Loss self: 0.0000; time: 7.76s
Val loss: 0.2045 score: 0.9284 time: 11.95s
Test loss: 0.2106 score: 0.9266 time: 2.86s
Epoch 29/1000, LR 0.000299
Train loss: 0.6761;  Loss pred: 0.6761; Loss self: 0.0000; time: 7.94s
Val loss: 0.1934 score: 0.9302 time: 3.05s
Test loss: 0.1988 score: 0.9290 time: 2.59s
Epoch 30/1000, LR 0.000299
Train loss: 0.6672;  Loss pred: 0.6672; Loss self: 0.0000; time: 19.09s
Val loss: 0.1843 score: 0.9320 time: 15.48s
Test loss: 0.1889 score: 0.9308 time: 4.36s
Epoch 31/1000, LR 0.000299
Train loss: 0.6599;  Loss pred: 0.6599; Loss self: 0.0000; time: 28.76s
Val loss: 0.1767 score: 0.9331 time: 12.25s
Test loss: 0.1804 score: 0.9320 time: 19.41s
Epoch 32/1000, LR 0.000299
Train loss: 0.6529;  Loss pred: 0.6529; Loss self: 0.0000; time: 7.39s
Val loss: 0.1702 score: 0.9337 time: 2.62s
Test loss: 0.1730 score: 0.9331 time: 19.55s
Epoch 33/1000, LR 0.000299
Train loss: 0.6472;  Loss pred: 0.6472; Loss self: 0.0000; time: 7.81s
Val loss: 0.1644 score: 0.9349 time: 2.84s
Test loss: 0.1664 score: 0.9355 time: 2.75s
Epoch 34/1000, LR 0.000298
Train loss: 0.6420;  Loss pred: 0.6420; Loss self: 0.0000; time: 7.34s
Val loss: 0.1592 score: 0.9367 time: 2.42s
Test loss: 0.1604 score: 0.9355 time: 2.46s
Epoch 35/1000, LR 0.000298
Train loss: 0.6386;  Loss pred: 0.6386; Loss self: 0.0000; time: 7.42s
Val loss: 0.1545 score: 0.9402 time: 2.93s
Test loss: 0.1549 score: 0.9373 time: 2.82s
Epoch 36/1000, LR 0.000298
Train loss: 0.6339;  Loss pred: 0.6339; Loss self: 0.0000; time: 7.19s
Val loss: 0.1503 score: 0.9414 time: 2.28s
Test loss: 0.1499 score: 0.9379 time: 2.21s
Epoch 37/1000, LR 0.000298
Train loss: 0.6305;  Loss pred: 0.6305; Loss self: 0.0000; time: 7.02s
Val loss: 0.1464 score: 0.9426 time: 2.31s
Test loss: 0.1452 score: 0.9402 time: 2.38s
Epoch 38/1000, LR 0.000298
Train loss: 0.6266;  Loss pred: 0.6266; Loss self: 0.0000; time: 7.28s
Val loss: 0.1428 score: 0.9432 time: 2.83s
Test loss: 0.1409 score: 0.9426 time: 12.63s
Epoch 39/1000, LR 0.000298
Train loss: 0.6236;  Loss pred: 0.6236; Loss self: 0.0000; time: 7.11s
Val loss: 0.1393 score: 0.9580 time: 2.31s
Test loss: 0.1369 score: 0.9627 time: 2.25s
Epoch 40/1000, LR 0.000298
Train loss: 0.6200;  Loss pred: 0.6200; Loss self: 0.0000; time: 6.91s
Val loss: 0.1362 score: 0.9598 time: 15.08s
Test loss: 0.1332 score: 0.9651 time: 2.76s
Epoch 41/1000, LR 0.000298
Train loss: 0.6178;  Loss pred: 0.6178; Loss self: 0.0000; time: 7.70s
Val loss: 0.1335 score: 0.9604 time: 2.65s
Test loss: 0.1299 score: 0.9663 time: 2.35s
Epoch 42/1000, LR 0.000298
Train loss: 0.6156;  Loss pred: 0.6156; Loss self: 0.0000; time: 7.10s
Val loss: 0.1310 score: 0.9615 time: 11.23s
Test loss: 0.1269 score: 0.9657 time: 2.72s
Epoch 43/1000, LR 0.000298
Train loss: 0.6129;  Loss pred: 0.6129; Loss self: 0.0000; time: 7.62s
Val loss: 0.1288 score: 0.9609 time: 13.28s
Test loss: 0.1243 score: 0.9663 time: 2.37s
Epoch 44/1000, LR 0.000298
Train loss: 0.6104;  Loss pred: 0.6104; Loss self: 0.0000; time: 6.82s
Val loss: 0.1268 score: 0.9609 time: 2.28s
Test loss: 0.1217 score: 0.9675 time: 2.25s
Epoch 45/1000, LR 0.000298
Train loss: 0.6081;  Loss pred: 0.6081; Loss self: 0.0000; time: 6.35s
Val loss: 0.1250 score: 0.9609 time: 2.21s
Test loss: 0.1197 score: 0.9663 time: 7.86s
Epoch 46/1000, LR 0.000298
Train loss: 0.6058;  Loss pred: 0.6058; Loss self: 0.0000; time: 7.16s
Val loss: 0.1233 score: 0.9609 time: 2.39s
Test loss: 0.1177 score: 0.9663 time: 8.42s
Epoch 47/1000, LR 0.000298
Train loss: 0.6042;  Loss pred: 0.6042; Loss self: 0.0000; time: 22.92s
Val loss: 0.1217 score: 0.9615 time: 2.42s
Test loss: 0.1155 score: 0.9675 time: 12.96s
Epoch 48/1000, LR 0.000298
Train loss: 0.6012;  Loss pred: 0.6012; Loss self: 0.0000; time: 25.21s
Val loss: 0.1202 score: 0.9615 time: 14.17s
Test loss: 0.1137 score: 0.9680 time: 4.31s
Epoch 49/1000, LR 0.000298
Train loss: 0.5998;  Loss pred: 0.5998; Loss self: 0.0000; time: 32.57s
Val loss: 0.1189 score: 0.9615 time: 13.86s
Test loss: 0.1121 score: 0.9680 time: 15.73s
Epoch 50/1000, LR 0.000298
Train loss: 0.5980;  Loss pred: 0.5980; Loss self: 0.0000; time: 7.62s
Val loss: 0.1176 score: 0.9615 time: 2.75s
Test loss: 0.1106 score: 0.9692 time: 2.46s
Epoch 51/1000, LR 0.000298
Train loss: 0.5960;  Loss pred: 0.5960; Loss self: 0.0000; time: 28.81s
Val loss: 0.1164 score: 0.9615 time: 2.75s
Test loss: 0.1091 score: 0.9692 time: 10.90s
Epoch 52/1000, LR 0.000298
Train loss: 0.5949;  Loss pred: 0.5949; Loss self: 0.0000; time: 20.54s
Val loss: 0.1152 score: 0.9621 time: 17.17s
Test loss: 0.1077 score: 0.9692 time: 3.11s
Epoch 53/1000, LR 0.000298
Train loss: 0.5933;  Loss pred: 0.5933; Loss self: 0.0000; time: 7.68s
Val loss: 0.1141 score: 0.9621 time: 2.76s
Test loss: 0.1064 score: 0.9686 time: 2.22s
Epoch 54/1000, LR 0.000297
Train loss: 0.5906;  Loss pred: 0.5906; Loss self: 0.0000; time: 13.09s
Val loss: 0.1130 score: 0.9633 time: 2.42s
Test loss: 0.1050 score: 0.9692 time: 2.42s
Epoch 55/1000, LR 0.000297
Train loss: 0.5890;  Loss pred: 0.5890; Loss self: 0.0000; time: 25.64s
Val loss: 0.1121 score: 0.9627 time: 2.54s
Test loss: 0.1039 score: 0.9680 time: 2.88s
Epoch 56/1000, LR 0.000297
Train loss: 0.5884;  Loss pred: 0.5884; Loss self: 0.0000; time: 7.12s
Val loss: 0.1112 score: 0.9633 time: 2.21s
Test loss: 0.1027 score: 0.9680 time: 2.26s
Epoch 57/1000, LR 0.000297
Train loss: 0.5864;  Loss pred: 0.5864; Loss self: 0.0000; time: 7.05s
Val loss: 0.1103 score: 0.9633 time: 7.75s
Test loss: 0.1015 score: 0.9680 time: 2.44s
Epoch 58/1000, LR 0.000297
Train loss: 0.5852;  Loss pred: 0.5852; Loss self: 0.0000; time: 6.95s
Val loss: 0.1097 score: 0.9633 time: 2.29s
Test loss: 0.1008 score: 0.9686 time: 2.30s
Epoch 59/1000, LR 0.000297
Train loss: 0.5827;  Loss pred: 0.5827; Loss self: 0.0000; time: 7.47s
Val loss: 0.1089 score: 0.9633 time: 2.65s
Test loss: 0.0998 score: 0.9680 time: 7.46s
Epoch 60/1000, LR 0.000297
Train loss: 0.5821;  Loss pred: 0.5821; Loss self: 0.0000; time: 21.73s
Val loss: 0.1081 score: 0.9639 time: 2.57s
Test loss: 0.0988 score: 0.9680 time: 2.51s
Epoch 61/1000, LR 0.000297
Train loss: 0.5801;  Loss pred: 0.5801; Loss self: 0.0000; time: 6.73s
Val loss: 0.1075 score: 0.9663 time: 2.31s
Test loss: 0.0980 score: 0.9704 time: 2.33s
Epoch 62/1000, LR 0.000297
Train loss: 0.5782;  Loss pred: 0.5782; Loss self: 0.0000; time: 18.19s
Val loss: 0.1070 score: 0.9728 time: 17.98s
Test loss: 0.0973 score: 0.9722 time: 2.60s
Epoch 63/1000, LR 0.000297
Train loss: 0.5773;  Loss pred: 0.5773; Loss self: 0.0000; time: 24.09s
Val loss: 0.1063 score: 0.9728 time: 2.70s
Test loss: 0.0965 score: 0.9728 time: 18.16s
Epoch 64/1000, LR 0.000297
Train loss: 0.5763;  Loss pred: 0.5763; Loss self: 0.0000; time: 8.23s
Val loss: 0.1058 score: 0.9728 time: 2.93s
Test loss: 0.0959 score: 0.9722 time: 2.70s
Epoch 65/1000, LR 0.000297
Train loss: 0.5741;  Loss pred: 0.5741; Loss self: 0.0000; time: 13.41s
Val loss: 0.1053 score: 0.9722 time: 12.20s
Test loss: 0.0952 score: 0.9722 time: 2.74s
Epoch 66/1000, LR 0.000297
Train loss: 0.5730;  Loss pred: 0.5730; Loss self: 0.0000; time: 32.55s
Val loss: 0.1048 score: 0.9722 time: 2.34s
Test loss: 0.0944 score: 0.9728 time: 2.34s
Epoch 67/1000, LR 0.000297
Train loss: 0.5714;  Loss pred: 0.5714; Loss self: 0.0000; time: 6.74s
Val loss: 0.1043 score: 0.9722 time: 12.21s
Test loss: 0.0938 score: 0.9722 time: 2.42s
Epoch 68/1000, LR 0.000296
Train loss: 0.5696;  Loss pred: 0.5696; Loss self: 0.0000; time: 7.54s
Val loss: 0.1043 score: 0.9722 time: 2.77s
Test loss: 0.0936 score: 0.9722 time: 2.90s
Epoch 69/1000, LR 0.000296
Train loss: 0.5678;  Loss pred: 0.5678; Loss self: 0.0000; time: 16.98s
Val loss: 0.1038 score: 0.9722 time: 2.50s
Test loss: 0.0930 score: 0.9722 time: 2.33s
Epoch 70/1000, LR 0.000296
Train loss: 0.5667;  Loss pred: 0.5667; Loss self: 0.0000; time: 18.77s
Val loss: 0.1036 score: 0.9722 time: 2.24s
Test loss: 0.0926 score: 0.9722 time: 2.36s
Epoch 71/1000, LR 0.000296
Train loss: 0.5660;  Loss pred: 0.5660; Loss self: 0.0000; time: 22.42s
Val loss: 0.1032 score: 0.9722 time: 5.89s
Test loss: 0.0920 score: 0.9716 time: 2.46s
Epoch 72/1000, LR 0.000296
Train loss: 0.5635;  Loss pred: 0.5635; Loss self: 0.0000; time: 20.00s
Val loss: 0.1029 score: 0.9722 time: 17.40s
Test loss: 0.0914 score: 0.9716 time: 15.31s
Epoch 73/1000, LR 0.000296
Train loss: 0.5623;  Loss pred: 0.5623; Loss self: 0.0000; time: 26.49s
Val loss: 0.1027 score: 0.9722 time: 2.87s
Test loss: 0.0909 score: 0.9722 time: 3.00s
Epoch 74/1000, LR 0.000296
Train loss: 0.5612;  Loss pred: 0.5612; Loss self: 0.0000; time: 28.61s
Val loss: 0.1029 score: 0.9734 time: 2.93s
Test loss: 0.0907 score: 0.9728 time: 2.83s
     INFO: Early stopping counter 1 of 2
Epoch 75/1000, LR 0.000296
Train loss: 0.5604;  Loss pred: 0.5604; Loss self: 0.0000; time: 18.27s
Val loss: 0.1029 score: 0.9734 time: 2.79s
Test loss: 0.0904 score: 0.9722 time: 2.90s
     INFO: Early stopping counter 2 of 2
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 072,   Train_Loss: 0.5623,   Val_Loss: 0.1027,   Val_Precision: 0.9796,   Val_Recall: 0.9645,   Val_accuracy: 0.9720,   Val_Score: 0.9722,   Val_Loss: 0.1027,   Test_Precision: 0.9807,   Test_Recall: 0.9633,   Test_accuracy: 0.9719,   Test_Score: 0.9722,   Test_loss: 0.0909


[13.949843397014774, 14.47833548404742, 18.9286899860017, 8.408394561964087, 11.76012842496857, 12.981042103958316, 11.544976261095144, 4.789094323059544, 14.499288861989044, 3.2061719859484583, 5.117124191951007, 2.4087865030160174, 2.591102499049157, 16.538114755996503, 13.790494148037396, 2.598531006020494, 21.675999352009967, 12.314670338993892, 12.621489405049942, 11.789681688998826, 19.33057677594479, 16.649264479987323, 18.415471957996488, 9.64673663501162, 2.313939257990569, 8.578555540996604, 2.754034681012854, 7.6123750150436535, 2.8218725440092385, 16.03345513099339, 2.538399558980018, 50.747085006092675, 13.18628157209605, 64.49679519503843, 2.7966133230365813, 6.192231667926535, 16.20289676601533, 25.96750794001855, 18.491944027948193, 19.95253770891577, 2.7219541399972513, 17.22892918996513, 14.274844339000992, 2.617070795968175, 12.529470354085788, 19.48262694803998, 8.876522541977465, 19.550846505910158, 12.637942244997248, 2.647476025042124, 3.2481846211012453, 19.727041669073515, 3.2856190679594874, 2.6992552200099453, 31.34693169395905, 11.87210593407508, 11.675102123990655, 16.356103308033198, 3.5374078870518133, 4.803426983067766, 9.030407494981773, 16.394424356985837, 11.048905068077147, 6.828486741986126, 3.647561792982742, 3.3693839700426906, 13.436971636954695, 15.459368840092793, 16.23427910602186, 7.180336119025014, 12.55350855703, 3.643562998971902, 14.57499261002522, 3.6587844169698656, 3.3490627000574023, 17.971484843990766, 3.3558372430270538, 16.216027852962725, 4.177285106037743, 13.298826853046194, 15.181342590949498, 14.557791846920736, 3.834234243957326, 12.239592362078838, 17.914717758074403, 10.539524195017293, 3.3991684439824894, 3.270428111893125, 3.0300268470309675, 3.3999420009786263, 3.006582076079212, 2.885914435959421, 12.724731388036162, 21.269877488026395, 17.468348826048896, 17.010218312963843, 3.4530882019316778, 11.980341764981858, 12.06332257494796, 12.970216606045142, 3.830298498040065, 13.792302969959565, 14.939243905013427, 3.677748381975107, 2.9286323790438473, 18.978699723025784, 18.129325886024162, 5.946254774928093, 2.9008628870360553, 12.498337837052532, 14.424702041083947, 2.8616281900322065, 2.5989549149526283, 4.368408133974299, 19.413995387032628, 19.55421432608273, 2.753650404047221, 2.4623658480122685, 2.8216328799026087, 2.2200687349541113, 2.381402433034964, 12.634909518063068, 2.2567367650335655, 2.7639630310004577, 2.3600319400429726, 2.7279274839675054, 2.3786603540647775, 2.2570264670066535, 7.861256356933154, 8.420809356030077, 12.962200752925128, 4.319941140944138, 15.737672685994767, 2.4682809849036857, 10.906645264010876, 3.1185864279977977, 2.2213882809737697, 2.425495268078521, 2.881136607960798, 2.269879827974364, 2.4438059370731935, 2.3093225000193343, 7.471243696054444, 2.5144112691050395, 2.3371659050462767, 2.6083552670897916, 18.16810656595044, 2.706149765988812, 2.750096053001471, 2.342378670000471, 2.4249972590478137, 2.9060724501032382, 2.3388471379876137, 2.360466195968911, 2.4652509540319443, 15.313827776000835, 3.010111824958585, 2.836689864983782, 2.901052698958665]
[0.008254345205334187, 0.008567062416596107, 0.011200408275740651, 0.004975381397611886, 0.0069586558727624676, 0.007681090002342199, 0.006831346900056298, 0.0028337836231121564, 0.008579460865082274, 0.0018971431869517505, 0.0030278841372491164, 0.0014253174574059275, 0.0015331967449995012, 0.009785866719524559, 0.008160055708897867, 0.00153759231125473, 0.012826035119532524, 0.007286787182836623, 0.007468336926065054, 0.006976143011241909, 0.011438211110026504, 0.009851635786974747, 0.010896728969228692, 0.005708128186397409, 0.0013691948272133545, 0.005076068367453611, 0.001629606320125949, 0.004504363914227014, 0.0016697470674610878, 0.009487251556800821, 0.0015020115733609574, 0.030027860950350694, 0.007802533474613047, 0.03816378413907599, 0.0016548007828618824, 0.0036640424070571212, 0.009587512879299011, 0.015365389313620444, 0.010941978714762244, 0.011806235330719391, 0.0016106237514776636, 0.0101946326567841, 0.008446653455030173, 0.0015485626011646006, 0.0074138877834827146, 0.01152818162605916, 0.00525238020235353, 0.011568548228349206, 0.0074780723343178986, 0.001566553860971671, 0.001922002734379435, 0.011672805721345275, 0.0019441532946505842, 0.0015971924378757074, 0.01854848029228346, 0.0070249147538905795, 0.0069083444520654765, 0.009678167637889467, 0.00209314076156912, 0.0028422644870223467, 0.0053434363875631795, 0.009700842814784519, 0.006537813649749791, 0.0040405246994000745, 0.002158320587563753, 0.0019937183254690478, 0.007950870791097452, 0.009147555526682127, 0.009606082311255537, 0.004248719597056222, 0.0074281115722071005, 0.0021559544372614804, 0.008624255982263443, 0.0021649611934732933, 0.0019816939053594095, 0.010634014700586251, 0.0019857025106668957, 0.009595282753232381, 0.002471766334933576, 0.00786912831541195, 0.008983042953224555, 0.00861407801592943, 0.0022687776591463465, 0.007242362344425348, 0.010600424708919766, 0.0062364048491226586, 0.002011342274545852, 0.0019351645632503698, 0.0017929152941011642, 0.0020118000005790687, 0.0017790426485675811, 0.0017076416780824977, 0.007529426856826131, 0.012585726324275974, 0.010336301080502305, 0.010065217936664997, 0.0020432474567643063, 0.0070889596242496205, 0.007138060695235479, 0.007674684382275232, 0.002266448815408323, 0.008161126017727553, 0.008839789292907354, 0.0021761824745414834, 0.0017329185674815665, 0.01122999983610993, 0.010727411766878202, 0.0035184939496615934, 0.00171648691540595, 0.0073954661757707285, 0.008535326651528962, 0.0016932711183622524, 0.0015378431449423837, 0.002584856884008461, 0.01148757123493055, 0.011570541021350727, 0.001629378937306048, 0.001457021211841579, 0.001669605254380242, 0.0013136501390261014, 0.001409113865701162, 0.007476277821339094, 0.0013353471982447134, 0.0016354810834322234, 0.0013964686035757234, 0.0016141582745369855, 0.0014074913337661405, 0.0013355186195305642, 0.004651630980433819, 0.004982727429603596, 0.007669941273920194, 0.002556178189907774, 0.009312232358576785, 0.0014605212928424176, 0.00645363625089401, 0.0018453174130164483, 0.0013144309354874376, 0.001435204300638178, 0.001704814560923549, 0.0013431241585647126, 0.0014460390160196412, 0.0013664630177629197, 0.004420854258020381, 0.0014878173189970649, 0.0013829384053528264, 0.0015434054834850839, 0.010750358914763574, 0.0016012720508809537, 0.0016272757710067876, 0.0013860228816570835, 0.0014349096207383514, 0.0017195694971025078, 0.0013839332177441501, 0.0013967255597449179, 0.0014587283751668308, 0.009061436553846648, 0.0017811312573719439, 0.0016785147130081552, 0.0017165992301530562]
[121.14831341846138, 116.72612517247461, 89.28245965514796, 200.98961669149347, 143.70591365412886, 130.1898558271117, 146.38401689010388, 352.8850939232144, 116.55744058113497, 527.1083420997641, 330.26362789050336, 701.598085959037, 652.2320134460796, 102.18818921831632, 122.54818296271954, 650.367456106726, 77.96641679836966, 137.2346927265024, 133.8986189160702, 143.34568520004834, 87.42625838785405, 101.50598556659403, 91.77065914219793, 175.18877771228426, 730.3562503484212, 197.00286276909344, 613.6451409458892, 222.00692906749927, 598.8931015285668, 105.40460469641096, 665.7738313975588, 33.30240544451173, 128.16350013155096, 26.202852326064214, 604.3023488728098, 272.9226053917804, 104.30233707004051, 65.08133178985341, 91.39114835334686, 84.70100518816838, 620.8774700376497, 98.09083207471355, 118.39008257222598, 645.7601386265866, 134.8819983798358, 86.74394908382824, 190.38987306210464, 86.44126992092738, 133.7243015704546, 638.3438354170215, 520.2906229594279, 85.66920617648654, 514.3627319674534, 626.0986317528631, 53.91277259603958, 142.35048182558714, 144.75248113909797, 103.32534395096215, 477.7509560562718, 351.83214108537595, 187.145486063518, 103.0838267450283, 152.95633273950398, 247.4926091030893, 463.32319941810397, 501.57536660287116, 125.77238723583518, 109.31882261694301, 104.10071115342105, 235.3650263700298, 134.62371832722377, 463.83169454648225, 115.95203134700428, 461.90204379399466, 504.61879975284836, 94.03786134928609, 503.60010859036043, 104.21787723380304, 404.5689860999232, 127.07887836083023, 111.32085254485395, 116.08903450267897, 440.76597632588704, 138.0764938901087, 94.3358428986856, 160.3487945688261, 497.1804215798098, 516.751918152307, 557.7508336785796, 497.0673027697404, 562.1000715217045, 585.602947524035, 132.81223378820744, 79.45508858484793, 96.7464078505155, 99.35204645269107, 489.4169801555037, 141.06442313188543, 140.09407354402032, 130.29851785299616, 441.2188765091703, 122.53211111160464, 113.12486834978688, 459.5202891755195, 577.0611607291451, 89.04719631290749, 93.21913074014603, 284.21251089437834, 582.5852740412528, 135.21798034534092, 117.16013233317398, 590.5728794141421, 650.2613763235689, 386.86861395949023, 87.05060273831205, 86.42638215056097, 613.7307762510796, 686.3318062034703, 598.9439703645401, 761.237692055031, 709.6658576291911, 133.75639909284266, 748.8689093851244, 611.4408843552004, 716.0920033858642, 619.517934377808, 710.4839482913317, 748.772787871352, 214.97836010773543, 200.69329782294668, 130.37909473965095, 391.2090338412909, 107.38563660076377, 684.6870394157921, 154.95140431279683, 541.9121897112269, 760.7855026853613, 696.7649132289667, 586.57406085168, 744.53280705532, 691.5442729564755, 731.8163660492854, 226.20062585998733, 672.125527261706, 723.0980035910362, 647.9178742723863, 93.02015011114561, 624.5034998580293, 614.5239902277319, 721.4888103466465, 696.9080042026877, 581.5409040954787, 722.5782192221876, 715.9602636488078, 685.5285857352522, 110.35777760596828, 561.4409358440536, 595.7648105495882, 582.5471562811067]
Elapsed: 9.54799959046031~8.511607989173697
Time per graph: 0.005649703899680657~0.005036454431463726
Speed: 347.1056019168253~246.41971710091056
Total Time: 2.9020
best val loss: 0.10273151087117266 test_score: 0.9722

Testing...
Test loss: 0.0907 score: 0.9728 time: 17.45s
test Score 0.9728
Epoch Time List: [49.75316980993375, 64.81354755198117, 84.20819640601985, 51.30455477593932, 56.61773758998606, 42.90884113300126, 52.05909938714467, 51.48066081199795, 59.90493578789756, 65.26616330095567, 60.702709062956274, 24.973904476035386, 26.054543146979995, 45.44348083809018, 34.42676243092865, 44.88741596415639, 63.060382734867744, 47.16527542890981, 52.56737354688812, 43.811824036878534, 53.21504718903452, 47.763190194033086, 41.306771852076054, 41.461726585170254, 42.26118015591055, 32.10907772695646, 54.08549654006492, 41.970322001958266, 74.67898021207657, 55.79275807202794, 57.785349357058294, 90.54559806804173, 240.77022502210457, 181.92516272782814, 68.95055509719532, 47.59745459491387, 57.931134973070584, 56.586931089055724, 58.27532891603187, 58.55789281707257, 61.66861688694917, 59.26813076506369, 47.935911902925, 33.29774593003094, 49.99971009010915, 57.312754777027294, 40.04704944090918, 49.747248028055765, 45.396703021018766, 56.80500474211294, 48.839708331855945, 52.62375906901434, 39.10578359302599, 315.5368702709675, 200.76877739990596, 128.67129278404173, 46.55711358611006, 57.19428976310883, 43.10187075880822, 40.98619308101479, 49.16031530709006, 58.20000787090976, 47.76827681006398, 55.83408093790058, 48.46285150991753, 53.661158973001875, 65.90901638299692, 71.28452692192513, 28.177924149786122, 52.56777732493356, 51.84965166996699, 43.44090598297771, 66.70895859389566, 34.80249892605934, 59.541050752974115, 57.76347738306504, 62.33999379002489, 65.09439359093085, 56.21240401803516, 61.48117225407623, 47.931114753941074, 51.812166279996745, 49.81023962900508, 61.42616542603355, 73.74921284907032, 58.40802762610838, 39.9797585849883, 15.729828767944127, 16.388030236936174, 15.921022239956073, 23.580858001019806, 36.71686734689865, 56.75242519704625, 56.83349146193359, 71.49273051903583, 58.03360327496193, 60.56944897992071, 59.29553306882735, 49.72632414102554, 57.72937625902705, 48.79740523616783, 58.15609396004584, 47.650476517970674, 44.93411104893312, 41.07924120500684, 56.678565060021356, 69.10623509401921, 56.89051163208205, 36.62051323009655, 47.16691578819882, 49.176712945103645, 22.56209147593472, 13.585272015072405, 38.92986307491083, 60.418740603956394, 29.556862765923142, 13.404320425004698, 12.217558917123824, 13.165333824115805, 11.680059839156456, 11.71099121007137, 22.743990181945264, 11.67691551009193, 24.746155958855525, 12.702155711012892, 21.060779726016335, 23.274502039072104, 11.355355138075538, 16.415255825966597, 17.962513591046445, 38.29647621407639, 43.69093591510318, 62.165907096117735, 12.833666298887692, 42.464952962007374, 40.82389893301297, 12.66034689208027, 17.932229760917835, 31.050827604951337, 11.59602293802891, 17.235486444085836, 11.550406055874191, 17.57877736107912, 26.81012605386786, 11.367065652040765, 38.77765258192085, 44.957582680974156, 13.864885542076081, 28.35378794290591, 37.21963107690681, 21.365188918076456, 13.200451218057424, 21.815353287849575, 23.36540397303179, 30.774895332055166, 52.70271303795744, 32.36758208996616, 34.37526571715716, 23.954674200969748]
Total Epoch List: [84, 75]
Total Time List: [12.24022517807316, 2.9020142449298874]
========================k_idx:2========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: AEtransGAT
AEtransGAT(
  (gae): GAE(
    (encoder): transGAT(
      (encoder): EdgeFeatureGAT()
      (classifier): Sequential(
        (0): Linear(in_features=64, out_features=64, bias=True)
        (1): ReLU(inplace=True)
        (2): Linear(in_features=64, out_features=2, bias=True)
      )
    )
    (decoder): InnerProductDecoder()
  )
)
Total number of parameters: 1914242
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x71f20acaffd0>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 2.9574;  Loss pred: 2.9574; Loss self: 0.0000; time: 22.49s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6941 score: 0.5000 time: 2.92s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6941 score: 0.5000 time: 14.41s
Epoch 2/1000, LR 0.000029
Train loss: 2.8165;  Loss pred: 2.8165; Loss self: 0.0000; time: 20.83s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6936 score: 0.5000 time: 11.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6935 score: 0.5000 time: 2.94s
Epoch 3/1000, LR 0.000059
Train loss: 2.5679;  Loss pred: 2.5679; Loss self: 0.0000; time: 18.55s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6929 score: 0.5000 time: 2.76s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6928 score: 0.5000 time: 8.45s
Epoch 4/1000, LR 0.000089
Train loss: 2.2537;  Loss pred: 2.2537; Loss self: 0.0000; time: 19.30s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6923 score: 0.5000 time: 2.82s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6922 score: 0.5000 time: 2.85s
Epoch 5/1000, LR 0.000119
Train loss: 1.9082;  Loss pred: 1.9082; Loss self: 0.0000; time: 19.84s
Val loss: 0.6918 score: 0.5183 time: 2.80s
Test loss: 0.6917 score: 0.5231 time: 2.83s
Epoch 6/1000, LR 0.000149
Train loss: 1.6051;  Loss pred: 1.6051; Loss self: 0.0000; time: 18.90s
Val loss: 0.6913 score: 0.6899 time: 12.15s
Test loss: 0.6912 score: 0.6976 time: 9.69s
Epoch 7/1000, LR 0.000179
Train loss: 1.3619;  Loss pred: 1.3619; Loss self: 0.0000; time: 8.32s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6907 score: 0.5000 time: 2.85s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6905 score: 0.5000 time: 2.91s
Epoch 8/1000, LR 0.000209
Train loss: 1.1982;  Loss pred: 1.1982; Loss self: 0.0000; time: 16.46s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6897 score: 0.5000 time: 10.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6895 score: 0.5000 time: 2.79s
Epoch 9/1000, LR 0.000239
Train loss: 1.0979;  Loss pred: 1.0979; Loss self: 0.0000; time: 33.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6883 score: 0.5000 time: 2.31s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6881 score: 0.5000 time: 6.37s
Epoch 10/1000, LR 0.000269
Train loss: 1.0433;  Loss pred: 1.0433; Loss self: 0.0000; time: 19.47s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6865 score: 0.5000 time: 16.82s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6862 score: 0.5000 time: 11.68s
Epoch 11/1000, LR 0.000299
Train loss: 1.0136;  Loss pred: 1.0136; Loss self: 0.0000; time: 21.79s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6836 score: 0.5000 time: 19.88s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6832 score: 0.5000 time: 2.90s
Epoch 12/1000, LR 0.000299
Train loss: 0.9966;  Loss pred: 0.9966; Loss self: 0.0000; time: 30.66s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6792 score: 0.5000 time: 14.99s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6787 score: 0.5000 time: 2.78s
Epoch 13/1000, LR 0.000299
Train loss: 0.9841;  Loss pred: 0.9841; Loss self: 0.0000; time: 30.66s
Val loss: 0.6719 score: 0.5118 time: 7.48s
Test loss: 0.6711 score: 0.5077 time: 14.36s
Epoch 14/1000, LR 0.000299
Train loss: 0.9754;  Loss pred: 0.9754; Loss self: 0.0000; time: 23.63s
Val loss: 0.6615 score: 0.5787 time: 9.13s
Test loss: 0.6603 score: 0.5953 time: 3.38s
Epoch 15/1000, LR 0.000299
Train loss: 0.9658;  Loss pred: 0.9658; Loss self: 0.0000; time: 18.57s
Val loss: 0.6461 score: 0.7385 time: 4.07s
Test loss: 0.6445 score: 0.7426 time: 11.04s
Epoch 16/1000, LR 0.000299
Train loss: 0.9543;  Loss pred: 0.9543; Loss self: 0.0000; time: 19.77s
Val loss: 0.6247 score: 0.8296 time: 19.19s
Test loss: 0.6223 score: 0.8361 time: 2.80s
Epoch 17/1000, LR 0.000299
Train loss: 0.9400;  Loss pred: 0.9400; Loss self: 0.0000; time: 24.94s
Val loss: 0.5974 score: 0.8686 time: 7.84s
Test loss: 0.5940 score: 0.8805 time: 10.15s
Epoch 18/1000, LR 0.000299
Train loss: 0.9234;  Loss pred: 0.9234; Loss self: 0.0000; time: 20.93s
Val loss: 0.5645 score: 0.8905 time: 2.63s
Test loss: 0.5601 score: 0.9030 time: 2.62s
Epoch 19/1000, LR 0.000299
Train loss: 0.9039;  Loss pred: 0.9039; Loss self: 0.0000; time: 7.04s
Val loss: 0.5258 score: 0.8953 time: 2.18s
Test loss: 0.5203 score: 0.9065 time: 2.22s
Epoch 20/1000, LR 0.000299
Train loss: 0.8815;  Loss pred: 0.8815; Loss self: 0.0000; time: 14.69s
Val loss: 0.4837 score: 0.9036 time: 2.85s
Test loss: 0.4769 score: 0.9136 time: 2.99s
Epoch 21/1000, LR 0.000299
Train loss: 0.8573;  Loss pred: 0.8573; Loss self: 0.0000; time: 28.46s
Val loss: 0.4406 score: 0.9118 time: 2.76s
Test loss: 0.4329 score: 0.9189 time: 2.79s
Epoch 22/1000, LR 0.000299
Train loss: 0.8327;  Loss pred: 0.8327; Loss self: 0.0000; time: 17.88s
Val loss: 0.3994 score: 0.9154 time: 2.97s
Test loss: 0.3907 score: 0.9249 time: 11.44s
Epoch 23/1000, LR 0.000299
Train loss: 0.8105;  Loss pred: 0.8105; Loss self: 0.0000; time: 16.89s
Val loss: 0.3616 score: 0.9219 time: 9.50s
Test loss: 0.3523 score: 0.9278 time: 16.13s
Epoch 24/1000, LR 0.000299
Train loss: 0.7875;  Loss pred: 0.7875; Loss self: 0.0000; time: 22.25s
Val loss: 0.3287 score: 0.9243 time: 2.25s
Test loss: 0.3191 score: 0.9278 time: 26.07s
Epoch 25/1000, LR 0.000299
Train loss: 0.7681;  Loss pred: 0.7681; Loss self: 0.0000; time: 19.55s
Val loss: 0.3011 score: 0.9290 time: 10.61s
Test loss: 0.2912 score: 0.9314 time: 3.94s
Epoch 26/1000, LR 0.000299
Train loss: 0.7503;  Loss pred: 0.7503; Loss self: 0.0000; time: 28.13s
Val loss: 0.2775 score: 0.9325 time: 17.23s
Test loss: 0.2678 score: 0.9355 time: 2.88s
Epoch 27/1000, LR 0.000299
Train loss: 0.7334;  Loss pred: 0.7334; Loss self: 0.0000; time: 14.20s
Val loss: 0.2574 score: 0.9166 time: 15.18s
Test loss: 0.2479 score: 0.9284 time: 17.37s
Epoch 28/1000, LR 0.000299
Train loss: 0.7197;  Loss pred: 0.7197; Loss self: 0.0000; time: 25.51s
Val loss: 0.2403 score: 0.9124 time: 2.84s
Test loss: 0.2312 score: 0.9308 time: 20.38s
Epoch 29/1000, LR 0.000299
Train loss: 0.7085;  Loss pred: 0.7085; Loss self: 0.0000; time: 28.34s
Val loss: 0.2257 score: 0.9172 time: 2.96s
Test loss: 0.2172 score: 0.9320 time: 17.02s
Epoch 30/1000, LR 0.000299
Train loss: 0.6993;  Loss pred: 0.6993; Loss self: 0.0000; time: 25.65s
Val loss: 0.2129 score: 0.9201 time: 10.79s
Test loss: 0.2050 score: 0.9337 time: 2.85s
Epoch 31/1000, LR 0.000299
Train loss: 0.6892;  Loss pred: 0.6892; Loss self: 0.0000; time: 45.51s
Val loss: 0.2016 score: 0.9219 time: 2.97s
Test loss: 0.1944 score: 0.9343 time: 10.78s
Epoch 32/1000, LR 0.000299
Train loss: 0.6816;  Loss pred: 0.6816; Loss self: 0.0000; time: 23.75s
Val loss: 0.1916 score: 0.9231 time: 13.91s
Test loss: 0.1850 score: 0.9343 time: 15.24s
Epoch 33/1000, LR 0.000299
Train loss: 0.6744;  Loss pred: 0.6744; Loss self: 0.0000; time: 32.34s
Val loss: 0.1829 score: 0.9254 time: 11.98s
Test loss: 0.1769 score: 0.9367 time: 4.15s
Epoch 34/1000, LR 0.000298
Train loss: 0.6672;  Loss pred: 0.6672; Loss self: 0.0000; time: 11.15s
Val loss: 0.1751 score: 0.9314 time: 18.88s
Test loss: 0.1697 score: 0.9391 time: 3.22s
Epoch 35/1000, LR 0.000298
Train loss: 0.6619;  Loss pred: 0.6619; Loss self: 0.0000; time: 39.27s
Val loss: 0.1680 score: 0.9331 time: 11.14s
Test loss: 0.1633 score: 0.9420 time: 8.70s
Epoch 36/1000, LR 0.000298
Train loss: 0.6559;  Loss pred: 0.6559; Loss self: 0.0000; time: 15.89s
Val loss: 0.1616 score: 0.9337 time: 14.78s
Test loss: 0.1576 score: 0.9438 time: 8.75s
Epoch 37/1000, LR 0.000298
Train loss: 0.6509;  Loss pred: 0.6509; Loss self: 0.0000; time: 22.59s
Val loss: 0.1561 score: 0.9355 time: 13.41s
Test loss: 0.1529 score: 0.9438 time: 11.54s
Epoch 38/1000, LR 0.000298
Train loss: 0.6480;  Loss pred: 0.6480; Loss self: 0.0000; time: 32.92s
Val loss: 0.1509 score: 0.9373 time: 2.71s
Test loss: 0.1487 score: 0.9462 time: 15.53s
Epoch 39/1000, LR 0.000298
Train loss: 0.6441;  Loss pred: 0.6441; Loss self: 0.0000; time: 27.31s
Val loss: 0.1465 score: 0.9396 time: 11.75s
Test loss: 0.1451 score: 0.9479 time: 2.65s
Epoch 40/1000, LR 0.000298
Train loss: 0.6403;  Loss pred: 0.6403; Loss self: 0.0000; time: 26.03s
Val loss: 0.1424 score: 0.9444 time: 15.55s
Test loss: 0.1420 score: 0.9515 time: 12.04s
Epoch 41/1000, LR 0.000298
Train loss: 0.6370;  Loss pred: 0.6370; Loss self: 0.0000; time: 26.99s
Val loss: 0.1385 score: 0.9621 time: 8.65s
Test loss: 0.1390 score: 0.9592 time: 2.76s
Epoch 42/1000, LR 0.000298
Train loss: 0.6345;  Loss pred: 0.6345; Loss self: 0.0000; time: 32.01s
Val loss: 0.1351 score: 0.9633 time: 9.98s
Test loss: 0.1365 score: 0.9609 time: 2.91s
Epoch 43/1000, LR 0.000298
Train loss: 0.6319;  Loss pred: 0.6319; Loss self: 0.0000; time: 35.18s
Val loss: 0.1316 score: 0.9651 time: 2.78s
Test loss: 0.1339 score: 0.9615 time: 16.56s
Epoch 44/1000, LR 0.000298
Train loss: 0.6277;  Loss pred: 0.6277; Loss self: 0.0000; time: 39.28s
Val loss: 0.1285 score: 0.9651 time: 2.82s
Test loss: 0.1318 score: 0.9621 time: 10.84s
Epoch 45/1000, LR 0.000298
Train loss: 0.6246;  Loss pred: 0.6246; Loss self: 0.0000; time: 23.56s
Val loss: 0.1256 score: 0.9675 time: 13.65s
Test loss: 0.1298 score: 0.9621 time: 3.44s
Epoch 46/1000, LR 0.000298
Train loss: 0.6224;  Loss pred: 0.6224; Loss self: 0.0000; time: 47.45s
Val loss: 0.1229 score: 0.9675 time: 12.57s
Test loss: 0.1279 score: 0.9621 time: 13.26s
Epoch 47/1000, LR 0.000298
Train loss: 0.6199;  Loss pred: 0.6199; Loss self: 0.0000; time: 22.97s
Val loss: 0.1203 score: 0.9680 time: 14.23s
Test loss: 0.1262 score: 0.9621 time: 2.95s
Epoch 48/1000, LR 0.000298
Train loss: 0.6187;  Loss pred: 0.6187; Loss self: 0.0000; time: 32.39s
Val loss: 0.1180 score: 0.9686 time: 9.25s
Test loss: 0.1247 score: 0.9621 time: 14.14s
Epoch 49/1000, LR 0.000298
Train loss: 0.6151;  Loss pred: 0.6151; Loss self: 0.0000; time: 40.96s
Val loss: 0.1157 score: 0.9686 time: 4.75s
Test loss: 0.1233 score: 0.9627 time: 9.78s
Epoch 50/1000, LR 0.000298
Train loss: 0.6148;  Loss pred: 0.6148; Loss self: 0.0000; time: 33.67s
Val loss: 0.1135 score: 0.9698 time: 11.36s
Test loss: 0.1220 score: 0.9633 time: 4.73s
Epoch 51/1000, LR 0.000298
Train loss: 0.6115;  Loss pred: 0.6115; Loss self: 0.0000; time: 40.58s
Val loss: 0.1115 score: 0.9698 time: 2.79s
Test loss: 0.1208 score: 0.9627 time: 12.75s
Epoch 52/1000, LR 0.000298
Train loss: 0.6099;  Loss pred: 0.6099; Loss self: 0.0000; time: 30.07s
Val loss: 0.1095 score: 0.9698 time: 7.53s
Test loss: 0.1197 score: 0.9633 time: 7.25s
Epoch 53/1000, LR 0.000298
Train loss: 0.6072;  Loss pred: 0.6072; Loss self: 0.0000; time: 26.91s
Val loss: 0.1076 score: 0.9698 time: 14.00s
Test loss: 0.1187 score: 0.9627 time: 3.24s
Epoch 54/1000, LR 0.000297
Train loss: 0.6068;  Loss pred: 0.6068; Loss self: 0.0000; time: 35.30s
Val loss: 0.1059 score: 0.9692 time: 2.72s
Test loss: 0.1178 score: 0.9633 time: 14.10s
Epoch 55/1000, LR 0.000297
Train loss: 0.6039;  Loss pred: 0.6039; Loss self: 0.0000; time: 30.45s
Val loss: 0.1041 score: 0.9692 time: 12.94s
Test loss: 0.1169 score: 0.9633 time: 12.86s
Epoch 56/1000, LR 0.000297
Train loss: 0.6031;  Loss pred: 0.6031; Loss self: 0.0000; time: 19.87s
Val loss: 0.1026 score: 0.9704 time: 5.55s
Test loss: 0.1161 score: 0.9633 time: 9.91s
Epoch 57/1000, LR 0.000297
Train loss: 0.6006;  Loss pred: 0.6006; Loss self: 0.0000; time: 40.10s
Val loss: 0.1011 score: 0.9686 time: 5.73s
Test loss: 0.1155 score: 0.9621 time: 15.12s
Epoch 58/1000, LR 0.000297
Train loss: 0.5994;  Loss pred: 0.5994; Loss self: 0.0000; time: 26.65s
Val loss: 0.0995 score: 0.9692 time: 14.47s
Test loss: 0.1146 score: 0.9621 time: 2.43s
Epoch 59/1000, LR 0.000297
Train loss: 0.5983;  Loss pred: 0.5983; Loss self: 0.0000; time: 38.71s
Val loss: 0.0981 score: 0.9698 time: 2.96s
Test loss: 0.1139 score: 0.9627 time: 12.26s
Epoch 60/1000, LR 0.000297
Train loss: 0.5962;  Loss pred: 0.5962; Loss self: 0.0000; time: 18.68s
Val loss: 0.0968 score: 0.9692 time: 11.69s
Test loss: 0.1135 score: 0.9627 time: 11.53s
Epoch 61/1000, LR 0.000297
Train loss: 0.5949;  Loss pred: 0.5949; Loss self: 0.0000; time: 21.68s
Val loss: 0.0955 score: 0.9692 time: 13.76s
Test loss: 0.1129 score: 0.9621 time: 14.90s
Epoch 62/1000, LR 0.000297
Train loss: 0.5931;  Loss pred: 0.5931; Loss self: 0.0000; time: 22.04s
Val loss: 0.0943 score: 0.9692 time: 5.24s
Test loss: 0.1125 score: 0.9621 time: 8.59s
Epoch 63/1000, LR 0.000297
Train loss: 0.5910;  Loss pred: 0.5910; Loss self: 0.0000; time: 25.66s
Val loss: 0.0933 score: 0.9686 time: 7.86s
Test loss: 0.1123 score: 0.9615 time: 4.29s
Epoch 64/1000, LR 0.000297
Train loss: 0.5907;  Loss pred: 0.5907; Loss self: 0.0000; time: 16.59s
Val loss: 0.0920 score: 0.9698 time: 15.12s
Test loss: 0.1116 score: 0.9621 time: 2.36s
Epoch 65/1000, LR 0.000297
Train loss: 0.5884;  Loss pred: 0.5884; Loss self: 0.0000; time: 18.93s
Val loss: 0.0910 score: 0.9686 time: 14.63s
Test loss: 0.1114 score: 0.9615 time: 18.11s
Epoch 66/1000, LR 0.000297
Train loss: 0.5865;  Loss pred: 0.5865; Loss self: 0.0000; time: 16.71s
Val loss: 0.0900 score: 0.9686 time: 11.03s
Test loss: 0.1111 score: 0.9609 time: 2.50s
Epoch 67/1000, LR 0.000297
Train loss: 0.5856;  Loss pred: 0.5856; Loss self: 0.0000; time: 39.86s
Val loss: 0.0889 score: 0.9686 time: 2.37s
Test loss: 0.1105 score: 0.9621 time: 11.23s
Epoch 68/1000, LR 0.000296
Train loss: 0.5829;  Loss pred: 0.5829; Loss self: 0.0000; time: 32.87s
Val loss: 0.0880 score: 0.9686 time: 6.12s
Test loss: 0.1104 score: 0.9633 time: 8.38s
Epoch 69/1000, LR 0.000296
Train loss: 0.5829;  Loss pred: 0.5829; Loss self: 0.0000; time: 28.25s
Val loss: 0.0870 score: 0.9698 time: 14.29s
Test loss: 0.1100 score: 0.9627 time: 9.72s
Epoch 70/1000, LR 0.000296
Train loss: 0.5799;  Loss pred: 0.5799; Loss self: 0.0000; time: 22.30s
Val loss: 0.0864 score: 0.9686 time: 16.25s
Test loss: 0.1101 score: 0.9633 time: 2.26s
Epoch 71/1000, LR 0.000296
Train loss: 0.5803;  Loss pred: 0.5803; Loss self: 0.0000; time: 20.63s
Val loss: 0.0855 score: 0.9692 time: 2.61s
Test loss: 0.1096 score: 0.9627 time: 16.90s
Epoch 72/1000, LR 0.000296
Train loss: 0.5764;  Loss pred: 0.5764; Loss self: 0.0000; time: 21.74s
Val loss: 0.0847 score: 0.9686 time: 2.74s
Test loss: 0.1096 score: 0.9627 time: 9.36s
Epoch 73/1000, LR 0.000296
Train loss: 0.5767;  Loss pred: 0.5767; Loss self: 0.0000; time: 8.36s
Val loss: 0.0841 score: 0.9686 time: 2.75s
Test loss: 0.1096 score: 0.9627 time: 15.97s
Epoch 74/1000, LR 0.000296
Train loss: 0.5753;  Loss pred: 0.5753; Loss self: 0.0000; time: 22.56s
Val loss: 0.0832 score: 0.9698 time: 16.14s
Test loss: 0.1091 score: 0.9627 time: 2.62s
Epoch 75/1000, LR 0.000296
Train loss: 0.5727;  Loss pred: 0.5727; Loss self: 0.0000; time: 19.59s
Val loss: 0.0827 score: 0.9698 time: 10.49s
Test loss: 0.1093 score: 0.9627 time: 9.60s
Epoch 76/1000, LR 0.000296
Train loss: 0.5712;  Loss pred: 0.5712; Loss self: 0.0000; time: 8.24s
Val loss: 0.0821 score: 0.9698 time: 19.74s
Test loss: 0.1093 score: 0.9621 time: 2.78s
Epoch 77/1000, LR 0.000296
Train loss: 0.5717;  Loss pred: 0.5717; Loss self: 0.0000; time: 39.71s
Val loss: 0.0812 score: 0.9704 time: 2.87s
Test loss: 0.1089 score: 0.9627 time: 9.04s
Epoch 78/1000, LR 0.000296
Train loss: 0.5691;  Loss pred: 0.5691; Loss self: 0.0000; time: 8.19s
Val loss: 0.0807 score: 0.9698 time: 2.87s
Test loss: 0.1089 score: 0.9627 time: 12.56s
Epoch 79/1000, LR 0.000295
Train loss: 0.5671;  Loss pred: 0.5671; Loss self: 0.0000; time: 30.25s
Val loss: 0.0803 score: 0.9716 time: 12.34s
Test loss: 0.1092 score: 0.9627 time: 2.95s
Epoch 80/1000, LR 0.000295
Train loss: 0.5667;  Loss pred: 0.5667; Loss self: 0.0000; time: 27.23s
Val loss: 0.0796 score: 0.9757 time: 5.84s
Test loss: 0.1090 score: 0.9657 time: 2.56s
Epoch 81/1000, LR 0.000295
Train loss: 0.5649;  Loss pred: 0.5649; Loss self: 0.0000; time: 16.24s
Val loss: 0.0791 score: 0.9775 time: 22.07s
Test loss: 0.1092 score: 0.9692 time: 12.86s
Epoch 82/1000, LR 0.000295
Train loss: 0.5639;  Loss pred: 0.5639; Loss self: 0.0000; time: 26.52s
Val loss: 0.0784 score: 0.9781 time: 2.61s
Test loss: 0.1090 score: 0.9698 time: 2.69s
Epoch 83/1000, LR 0.000295
Train loss: 0.5613;  Loss pred: 0.5613; Loss self: 0.0000; time: 13.88s
Val loss: 0.0782 score: 0.9775 time: 18.23s
Test loss: 0.1095 score: 0.9686 time: 6.70s
Epoch 84/1000, LR 0.000295
Train loss: 0.5608;  Loss pred: 0.5608; Loss self: 0.0000; time: 25.57s
Val loss: 0.0776 score: 0.9775 time: 2.73s
Test loss: 0.1095 score: 0.9686 time: 12.97s
Epoch 85/1000, LR 0.000295
Train loss: 0.5598;  Loss pred: 0.5598; Loss self: 0.0000; time: 20.91s
Val loss: 0.0773 score: 0.9769 time: 2.67s
Test loss: 0.1098 score: 0.9686 time: 11.99s
Epoch 86/1000, LR 0.000295
Train loss: 0.5571;  Loss pred: 0.5571; Loss self: 0.0000; time: 21.83s
Val loss: 0.0768 score: 0.9775 time: 2.34s
Test loss: 0.1100 score: 0.9692 time: 12.27s
Epoch 87/1000, LR 0.000295
Train loss: 0.5572;  Loss pred: 0.5572; Loss self: 0.0000; time: 14.67s
Val loss: 0.0768 score: 0.9775 time: 2.27s
Test loss: 0.1106 score: 0.9698 time: 15.23s
Epoch 88/1000, LR 0.000294
Train loss: 0.5566;  Loss pred: 0.5566; Loss self: 0.0000; time: 25.06s
Val loss: 0.0759 score: 0.9775 time: 2.64s
Test loss: 0.1101 score: 0.9686 time: 2.78s
Epoch 89/1000, LR 0.000294
Train loss: 0.5534;  Loss pred: 0.5534; Loss self: 0.0000; time: 43.72s
Val loss: 0.0755 score: 0.9775 time: 2.90s
Test loss: 0.1102 score: 0.9686 time: 11.88s
Epoch 90/1000, LR 0.000294
Train loss: 0.5520;  Loss pred: 0.5520; Loss self: 0.0000; time: 18.00s
Val loss: 0.0754 score: 0.9775 time: 2.34s
Test loss: 0.1107 score: 0.9686 time: 2.21s
Epoch 91/1000, LR 0.000294
Train loss: 0.5512;  Loss pred: 0.5512; Loss self: 0.0000; time: 19.39s
Val loss: 0.0751 score: 0.9775 time: 3.17s
Test loss: 0.1111 score: 0.9686 time: 10.27s
Epoch 92/1000, LR 0.000294
Train loss: 0.5514;  Loss pred: 0.5514; Loss self: 0.0000; time: 21.31s
Val loss: 0.0747 score: 0.9775 time: 2.86s
Test loss: 0.1112 score: 0.9686 time: 11.07s
Epoch 93/1000, LR 0.000294
Train loss: 0.5489;  Loss pred: 0.5489; Loss self: 0.0000; time: 28.25s
Val loss: 0.0744 score: 0.9775 time: 2.94s
Test loss: 0.1116 score: 0.9686 time: 13.32s
Epoch 94/1000, LR 0.000294
Train loss: 0.5488;  Loss pred: 0.5488; Loss self: 0.0000; time: 23.03s
Val loss: 0.0745 score: 0.9775 time: 3.14s
Test loss: 0.1123 score: 0.9680 time: 2.99s
     INFO: Early stopping counter 1 of 2
Epoch 95/1000, LR 0.000294
Train loss: 0.5464;  Loss pred: 0.5464; Loss self: 0.0000; time: 35.30s
Val loss: 0.0743 score: 0.9775 time: 2.70s
Test loss: 0.1127 score: 0.9680 time: 12.95s
Epoch 96/1000, LR 0.000293
Train loss: 0.5452;  Loss pred: 0.5452; Loss self: 0.0000; time: 14.12s
Val loss: 0.0739 score: 0.9775 time: 2.25s
Test loss: 0.1131 score: 0.9680 time: 14.66s
Epoch 97/1000, LR 0.000293
Train loss: 0.5438;  Loss pred: 0.5438; Loss self: 0.0000; time: 16.01s
Val loss: 0.0736 score: 0.9775 time: 2.78s
Test loss: 0.1135 score: 0.9680 time: 11.07s
Epoch 98/1000, LR 0.000293
Train loss: 0.5431;  Loss pred: 0.5431; Loss self: 0.0000; time: 24.59s
Val loss: 0.0738 score: 0.9775 time: 13.00s
Test loss: 0.1143 score: 0.9680 time: 2.71s
     INFO: Early stopping counter 1 of 2
Epoch 99/1000, LR 0.000293
Train loss: 0.5425;  Loss pred: 0.5425; Loss self: 0.0000; time: 22.70s
Val loss: 0.0737 score: 0.9769 time: 8.84s
Test loss: 0.1148 score: 0.9680 time: 2.33s
     INFO: Early stopping counter 2 of 2
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 096,   Train_Loss: 0.5438,   Val_Loss: 0.0736,   Val_Precision: 0.9867,   Val_Recall: 0.9680,   Val_accuracy: 0.9773,   Val_Score: 0.9775,   Val_Loss: 0.0736,   Test_Precision: 0.9806,   Test_Recall: 0.9550,   Test_accuracy: 0.9676,   Test_Score: 0.9680,   Test_loss: 0.1135


[13.949843397014774, 14.47833548404742, 18.9286899860017, 8.408394561964087, 11.76012842496857, 12.981042103958316, 11.544976261095144, 4.789094323059544, 14.499288861989044, 3.2061719859484583, 5.117124191951007, 2.4087865030160174, 2.591102499049157, 16.538114755996503, 13.790494148037396, 2.598531006020494, 21.675999352009967, 12.314670338993892, 12.621489405049942, 11.789681688998826, 19.33057677594479, 16.649264479987323, 18.415471957996488, 9.64673663501162, 2.313939257990569, 8.578555540996604, 2.754034681012854, 7.6123750150436535, 2.8218725440092385, 16.03345513099339, 2.538399558980018, 50.747085006092675, 13.18628157209605, 64.49679519503843, 2.7966133230365813, 6.192231667926535, 16.20289676601533, 25.96750794001855, 18.491944027948193, 19.95253770891577, 2.7219541399972513, 17.22892918996513, 14.274844339000992, 2.617070795968175, 12.529470354085788, 19.48262694803998, 8.876522541977465, 19.550846505910158, 12.637942244997248, 2.647476025042124, 3.2481846211012453, 19.727041669073515, 3.2856190679594874, 2.6992552200099453, 31.34693169395905, 11.87210593407508, 11.675102123990655, 16.356103308033198, 3.5374078870518133, 4.803426983067766, 9.030407494981773, 16.394424356985837, 11.048905068077147, 6.828486741986126, 3.647561792982742, 3.3693839700426906, 13.436971636954695, 15.459368840092793, 16.23427910602186, 7.180336119025014, 12.55350855703, 3.643562998971902, 14.57499261002522, 3.6587844169698656, 3.3490627000574023, 17.971484843990766, 3.3558372430270538, 16.216027852962725, 4.177285106037743, 13.298826853046194, 15.181342590949498, 14.557791846920736, 3.834234243957326, 12.239592362078838, 17.914717758074403, 10.539524195017293, 3.3991684439824894, 3.270428111893125, 3.0300268470309675, 3.3999420009786263, 3.006582076079212, 2.885914435959421, 12.724731388036162, 21.269877488026395, 17.468348826048896, 17.010218312963843, 3.4530882019316778, 11.980341764981858, 12.06332257494796, 12.970216606045142, 3.830298498040065, 13.792302969959565, 14.939243905013427, 3.677748381975107, 2.9286323790438473, 18.978699723025784, 18.129325886024162, 5.946254774928093, 2.9008628870360553, 12.498337837052532, 14.424702041083947, 2.8616281900322065, 2.5989549149526283, 4.368408133974299, 19.413995387032628, 19.55421432608273, 2.753650404047221, 2.4623658480122685, 2.8216328799026087, 2.2200687349541113, 2.381402433034964, 12.634909518063068, 2.2567367650335655, 2.7639630310004577, 2.3600319400429726, 2.7279274839675054, 2.3786603540647775, 2.2570264670066535, 7.861256356933154, 8.420809356030077, 12.962200752925128, 4.319941140944138, 15.737672685994767, 2.4682809849036857, 10.906645264010876, 3.1185864279977977, 2.2213882809737697, 2.425495268078521, 2.881136607960798, 2.269879827974364, 2.4438059370731935, 2.3093225000193343, 7.471243696054444, 2.5144112691050395, 2.3371659050462767, 2.6083552670897916, 18.16810656595044, 2.706149765988812, 2.750096053001471, 2.342378670000471, 2.4249972590478137, 2.9060724501032382, 2.3388471379876137, 2.360466195968911, 2.4652509540319443, 15.313827776000835, 3.010111824958585, 2.836689864983782, 2.901052698958665, 14.417181742028333, 2.941024543950334, 8.452456397004426, 2.8581359339877963, 2.8354856440564618, 9.695459411013871, 2.915039106970653, 2.792477209935896, 6.377253261976875, 11.68803104502149, 2.9072612719610333, 2.7814881129888818, 14.366428298992105, 3.385917015024461, 11.045347405946814, 2.8050883900141343, 10.153547725989483, 2.6255429580342025, 2.2237256549997255, 2.993394654011354, 2.7920003589242697, 11.449859034037217, 16.135288427001797, 26.07975716306828, 3.9565937099978328, 2.8849133589537814, 17.372163172927685, 20.385722509934567, 17.02820335398428, 2.8574644359759986, 10.788407466956414, 15.250082704937086, 4.154264569049701, 3.220497112022713, 8.70216307300143, 8.759702499955893, 11.549429484992288, 15.536670295987278, 2.655409490922466, 12.047735853935592, 2.7721786450129002, 2.9166358929360285, 16.5665377760306, 10.846601977944374, 3.440386595088057, 13.260995452990755, 2.9518291710410267, 14.147313938941807, 9.784421029035002, 4.73093284398783, 12.758167266962118, 7.251854575006291, 3.242513664998114, 14.110123354010284, 12.862769935047254, 9.91978810809087, 15.1293721239781, 2.435463772038929, 12.268369593075477, 11.53668703592848, 14.908915859065019, 8.597082567983307, 4.29223528096918, 2.3607445600209758, 18.115465987008065, 2.5048763919621706, 11.232830986031331, 8.388432026957162, 9.726210123975761, 2.2701184740290046, 16.90375919395592, 9.366742918035015, 15.970219425973482, 2.6234500149730593, 9.608579448075034, 2.7866723779588938, 9.049472872982733, 12.56378815905191, 2.9504487729864195, 2.5604591770097613, 12.870256631984375, 2.699629631009884, 6.704060862073675, 12.973830025992356, 12.001011566026136, 12.276392404921353, 15.234981509973295, 2.7882109800120816, 11.88501194899436, 2.216993251000531, 10.27635060204193, 11.074107788037509, 13.329082787036896, 2.9900940550724044, 12.957186714978889, 14.660957184038125, 11.072594019933604, 2.7121243929723278, 2.332486456958577]
[0.008254345205334187, 0.008567062416596107, 0.011200408275740651, 0.004975381397611886, 0.0069586558727624676, 0.007681090002342199, 0.006831346900056298, 0.0028337836231121564, 0.008579460865082274, 0.0018971431869517505, 0.0030278841372491164, 0.0014253174574059275, 0.0015331967449995012, 0.009785866719524559, 0.008160055708897867, 0.00153759231125473, 0.012826035119532524, 0.007286787182836623, 0.007468336926065054, 0.006976143011241909, 0.011438211110026504, 0.009851635786974747, 0.010896728969228692, 0.005708128186397409, 0.0013691948272133545, 0.005076068367453611, 0.001629606320125949, 0.004504363914227014, 0.0016697470674610878, 0.009487251556800821, 0.0015020115733609574, 0.030027860950350694, 0.007802533474613047, 0.03816378413907599, 0.0016548007828618824, 0.0036640424070571212, 0.009587512879299011, 0.015365389313620444, 0.010941978714762244, 0.011806235330719391, 0.0016106237514776636, 0.0101946326567841, 0.008446653455030173, 0.0015485626011646006, 0.0074138877834827146, 0.01152818162605916, 0.00525238020235353, 0.011568548228349206, 0.0074780723343178986, 0.001566553860971671, 0.001922002734379435, 0.011672805721345275, 0.0019441532946505842, 0.0015971924378757074, 0.01854848029228346, 0.0070249147538905795, 0.0069083444520654765, 0.009678167637889467, 0.00209314076156912, 0.0028422644870223467, 0.0053434363875631795, 0.009700842814784519, 0.006537813649749791, 0.0040405246994000745, 0.002158320587563753, 0.0019937183254690478, 0.007950870791097452, 0.009147555526682127, 0.009606082311255537, 0.004248719597056222, 0.0074281115722071005, 0.0021559544372614804, 0.008624255982263443, 0.0021649611934732933, 0.0019816939053594095, 0.010634014700586251, 0.0019857025106668957, 0.009595282753232381, 0.002471766334933576, 0.00786912831541195, 0.008983042953224555, 0.00861407801592943, 0.0022687776591463465, 0.007242362344425348, 0.010600424708919766, 0.0062364048491226586, 0.002011342274545852, 0.0019351645632503698, 0.0017929152941011642, 0.0020118000005790687, 0.0017790426485675811, 0.0017076416780824977, 0.007529426856826131, 0.012585726324275974, 0.010336301080502305, 0.010065217936664997, 0.0020432474567643063, 0.0070889596242496205, 0.007138060695235479, 0.007674684382275232, 0.002266448815408323, 0.008161126017727553, 0.008839789292907354, 0.0021761824745414834, 0.0017329185674815665, 0.01122999983610993, 0.010727411766878202, 0.0035184939496615934, 0.00171648691540595, 0.0073954661757707285, 0.008535326651528962, 0.0016932711183622524, 0.0015378431449423837, 0.002584856884008461, 0.01148757123493055, 0.011570541021350727, 0.001629378937306048, 0.001457021211841579, 0.001669605254380242, 0.0013136501390261014, 0.001409113865701162, 0.007476277821339094, 0.0013353471982447134, 0.0016354810834322234, 0.0013964686035757234, 0.0016141582745369855, 0.0014074913337661405, 0.0013355186195305642, 0.004651630980433819, 0.004982727429603596, 0.007669941273920194, 0.002556178189907774, 0.009312232358576785, 0.0014605212928424176, 0.00645363625089401, 0.0018453174130164483, 0.0013144309354874376, 0.001435204300638178, 0.001704814560923549, 0.0013431241585647126, 0.0014460390160196412, 0.0013664630177629197, 0.004420854258020381, 0.0014878173189970649, 0.0013829384053528264, 0.0015434054834850839, 0.010750358914763574, 0.0016012720508809537, 0.0016272757710067876, 0.0013860228816570835, 0.0014349096207383514, 0.0017195694971025078, 0.0013839332177441501, 0.0013967255597449179, 0.0014587283751668308, 0.009061436553846648, 0.0017811312573719439, 0.0016785147130081552, 0.0017165992301530562, 0.008530876770430967, 0.0017402512094380675, 0.005001453489351731, 0.0016912046946673351, 0.001677802156246427, 0.005736958231369155, 0.0017248752112252383, 0.0016523533786602935, 0.003773522640223003, 0.006915994701196147, 0.001720272941988777, 0.001645850954431291, 0.008500845147332606, 0.002003501192322166, 0.006535708524228884, 0.001659815615392979, 0.0060080164059109365, 0.0015535757148131375, 0.0013158139970412578, 0.0017712394402434046, 0.0016520712182983843, 0.006775064517181785, 0.009547507944971478, 0.015431808972229752, 0.0023411797100578893, 0.0017070493248247227, 0.010279386492856618, 0.012062557698186134, 0.010075859972771764, 0.0016908073585656797, 0.00638367305737066, 0.009023717576885849, 0.0024581447154140242, 0.0019056195929128478, 0.005149208918935757, 0.0051832559171336645, 0.006833981943782419, 0.009193296033128568, 0.001571248219480749, 0.007128837783393841, 0.0016403423934987576, 0.0017258200549917328, 0.00980268507457432, 0.006418107679257026, 0.0020357317130698565, 0.007846742871592163, 0.001746644479905933, 0.008371191679847222, 0.005789598242032546, 0.0027993685467383607, 0.007549211400569301, 0.004291038210062895, 0.0019186471390521386, 0.008349185416574133, 0.007611106470442162, 0.005869697105379213, 0.008952291197620178, 0.001441102823691674, 0.0072593902917606375, 0.006826442033093775, 0.008821843703588768, 0.005087031105315566, 0.0025397841899225918, 0.0013968902722017607, 0.010719210643200038, 0.0014821753798592726, 0.006646645553864693, 0.004963569246720214, 0.005755153919512285, 0.0013432653692479317, 0.010002224375121847, 0.00554245143078995, 0.009449833979865966, 0.0015523372869663073, 0.005685549969275168, 0.001648918566839582, 0.005354717676321143, 0.007434194176953793, 0.0017458276763233252, 0.001515064601780924, 0.007615536468629808, 0.0015974139828460853, 0.00396689991838679, 0.007676822500587193, 0.00710119027575511, 0.007264137517704943, 0.00901478195856408, 0.0016498289822556695, 0.007032551449109089, 0.0013118303260358172, 0.006080680829610609, 0.006552726501797342, 0.007887031234933074, 0.001769286423119766, 0.0076669743875614725, 0.008675122594105399, 0.006551830781025801, 0.0016048073331197206, 0.0013801695011589213]
[121.14831341846138, 116.72612517247461, 89.28245965514796, 200.98961669149347, 143.70591365412886, 130.1898558271117, 146.38401689010388, 352.8850939232144, 116.55744058113497, 527.1083420997641, 330.26362789050336, 701.598085959037, 652.2320134460796, 102.18818921831632, 122.54818296271954, 650.367456106726, 77.96641679836966, 137.2346927265024, 133.8986189160702, 143.34568520004834, 87.42625838785405, 101.50598556659403, 91.77065914219793, 175.18877771228426, 730.3562503484212, 197.00286276909344, 613.6451409458892, 222.00692906749927, 598.8931015285668, 105.40460469641096, 665.7738313975588, 33.30240544451173, 128.16350013155096, 26.202852326064214, 604.3023488728098, 272.9226053917804, 104.30233707004051, 65.08133178985341, 91.39114835334686, 84.70100518816838, 620.8774700376497, 98.09083207471355, 118.39008257222598, 645.7601386265866, 134.8819983798358, 86.74394908382824, 190.38987306210464, 86.44126992092738, 133.7243015704546, 638.3438354170215, 520.2906229594279, 85.66920617648654, 514.3627319674534, 626.0986317528631, 53.91277259603958, 142.35048182558714, 144.75248113909797, 103.32534395096215, 477.7509560562718, 351.83214108537595, 187.145486063518, 103.0838267450283, 152.95633273950398, 247.4926091030893, 463.32319941810397, 501.57536660287116, 125.77238723583518, 109.31882261694301, 104.10071115342105, 235.3650263700298, 134.62371832722377, 463.83169454648225, 115.95203134700428, 461.90204379399466, 504.61879975284836, 94.03786134928609, 503.60010859036043, 104.21787723380304, 404.5689860999232, 127.07887836083023, 111.32085254485395, 116.08903450267897, 440.76597632588704, 138.0764938901087, 94.3358428986856, 160.3487945688261, 497.1804215798098, 516.751918152307, 557.7508336785796, 497.0673027697404, 562.1000715217045, 585.602947524035, 132.81223378820744, 79.45508858484793, 96.7464078505155, 99.35204645269107, 489.4169801555037, 141.06442313188543, 140.09407354402032, 130.29851785299616, 441.2188765091703, 122.53211111160464, 113.12486834978688, 459.5202891755195, 577.0611607291451, 89.04719631290749, 93.21913074014603, 284.21251089437834, 582.5852740412528, 135.21798034534092, 117.16013233317398, 590.5728794141421, 650.2613763235689, 386.86861395949023, 87.05060273831205, 86.42638215056097, 613.7307762510796, 686.3318062034703, 598.9439703645401, 761.237692055031, 709.6658576291911, 133.75639909284266, 748.8689093851244, 611.4408843552004, 716.0920033858642, 619.517934377808, 710.4839482913317, 748.772787871352, 214.97836010773543, 200.69329782294668, 130.37909473965095, 391.2090338412909, 107.38563660076377, 684.6870394157921, 154.95140431279683, 541.9121897112269, 760.7855026853613, 696.7649132289667, 586.57406085168, 744.53280705532, 691.5442729564755, 731.8163660492854, 226.20062585998733, 672.125527261706, 723.0980035910362, 647.9178742723863, 93.02015011114561, 624.5034998580293, 614.5239902277319, 721.4888103466465, 696.9080042026877, 581.5409040954787, 722.5782192221876, 715.9602636488078, 685.5285857352522, 110.35777760596828, 561.4409358440536, 595.7648105495882, 582.5471562811067, 117.22124547222613, 574.6296825289396, 199.94187732206944, 591.2944797002842, 596.0178298001455, 174.30839822610054, 579.7520849578825, 605.1974189751026, 265.00437266249, 144.5923606371541, 581.3031034737533, 607.5884315694558, 117.635362445554, 499.12623153517865, 153.00559936123912, 602.4765586768139, 166.4442858405244, 643.6763850420248, 759.9858355729626, 564.5764075028611, 605.3007817846916, 147.60007044419535, 104.73937343269606, 64.80121687609962, 427.1350873681002, 585.8061541969084, 97.2820703545803, 82.90115786558033, 99.24711168102016, 591.4334326343984, 156.64962647881677, 110.81907112889799, 406.8108739609215, 524.7637061032958, 194.20458865488814, 192.928926525588, 146.32757420581183, 108.77491559027828, 636.4366798331013, 140.2753198185312, 609.6288213749427, 579.4346850400868, 102.01286610683297, 155.8091652516123, 491.2238649031081, 127.4414131270103, 572.5263563961545, 119.4573052732022, 172.72355666063135, 357.2234178186841, 132.4641670419493, 233.04383485910344, 521.2005791195279, 119.77216340349567, 131.3869414234992, 170.3665422673279, 111.70324757374222, 693.9130113133075, 137.75261555161097, 146.4891952721666, 113.35498945568432, 196.5783144032823, 393.7342408728351, 715.8758421474384, 93.29045144143814, 674.6839905645624, 150.4518319648548, 201.46792565869242, 173.7572989333262, 744.4545380931548, 99.97776119552587, 180.42557747005333, 105.82196492876203, 644.189898932515, 175.88448002462755, 606.4580872035791, 186.7512090174343, 134.51357015936276, 572.794218789095, 660.0378616360799, 131.3105129388108, 626.0117982805666, 252.08601693351207, 130.26222762393047, 140.82146248273347, 137.66259209199876, 110.92891703830905, 606.1234290070391, 142.19590247387154, 762.2937053314445, 164.45526874727238, 152.60823105095426, 126.79041964114721, 565.1996120767769, 130.4295474916874, 115.2721462033843, 152.62909458773186, 623.1277607985598, 724.5486870709033]
Elapsed: 9.204524647007252~7.492059529556379
Time per graph: 0.005446464288169972~0.0044331713192641295
Speed: 339.0024865486718~240.30874494159502
Total Time: 2.3334
best val loss: 0.0736437164731985 test_score: 0.9680

Testing...
Test loss: 0.1090 score: 0.9698 time: 2.33s
test Score 0.9698
Epoch Time List: [49.75316980993375, 64.81354755198117, 84.20819640601985, 51.30455477593932, 56.61773758998606, 42.90884113300126, 52.05909938714467, 51.48066081199795, 59.90493578789756, 65.26616330095567, 60.702709062956274, 24.973904476035386, 26.054543146979995, 45.44348083809018, 34.42676243092865, 44.88741596415639, 63.060382734867744, 47.16527542890981, 52.56737354688812, 43.811824036878534, 53.21504718903452, 47.763190194033086, 41.306771852076054, 41.461726585170254, 42.26118015591055, 32.10907772695646, 54.08549654006492, 41.970322001958266, 74.67898021207657, 55.79275807202794, 57.785349357058294, 90.54559806804173, 240.77022502210457, 181.92516272782814, 68.95055509719532, 47.59745459491387, 57.931134973070584, 56.586931089055724, 58.27532891603187, 58.55789281707257, 61.66861688694917, 59.26813076506369, 47.935911902925, 33.29774593003094, 49.99971009010915, 57.312754777027294, 40.04704944090918, 49.747248028055765, 45.396703021018766, 56.80500474211294, 48.839708331855945, 52.62375906901434, 39.10578359302599, 315.5368702709675, 200.76877739990596, 128.67129278404173, 46.55711358611006, 57.19428976310883, 43.10187075880822, 40.98619308101479, 49.16031530709006, 58.20000787090976, 47.76827681006398, 55.83408093790058, 48.46285150991753, 53.661158973001875, 65.90901638299692, 71.28452692192513, 28.177924149786122, 52.56777732493356, 51.84965166996699, 43.44090598297771, 66.70895859389566, 34.80249892605934, 59.541050752974115, 57.76347738306504, 62.33999379002489, 65.09439359093085, 56.21240401803516, 61.48117225407623, 47.931114753941074, 51.812166279996745, 49.81023962900508, 61.42616542603355, 73.74921284907032, 58.40802762610838, 39.9797585849883, 15.729828767944127, 16.388030236936174, 15.921022239956073, 23.580858001019806, 36.71686734689865, 56.75242519704625, 56.83349146193359, 71.49273051903583, 58.03360327496193, 60.56944897992071, 59.29553306882735, 49.72632414102554, 57.72937625902705, 48.79740523616783, 58.15609396004584, 47.650476517970674, 44.93411104893312, 41.07924120500684, 56.678565060021356, 69.10623509401921, 56.89051163208205, 36.62051323009655, 47.16691578819882, 49.176712945103645, 22.56209147593472, 13.585272015072405, 38.92986307491083, 60.418740603956394, 29.556862765923142, 13.404320425004698, 12.217558917123824, 13.165333824115805, 11.680059839156456, 11.71099121007137, 22.743990181945264, 11.67691551009193, 24.746155958855525, 12.702155711012892, 21.060779726016335, 23.274502039072104, 11.355355138075538, 16.415255825966597, 17.962513591046445, 38.29647621407639, 43.69093591510318, 62.165907096117735, 12.833666298887692, 42.464952962007374, 40.82389893301297, 12.66034689208027, 17.932229760917835, 31.050827604951337, 11.59602293802891, 17.235486444085836, 11.550406055874191, 17.57877736107912, 26.81012605386786, 11.367065652040765, 38.77765258192085, 44.957582680974156, 13.864885542076081, 28.35378794290591, 37.21963107690681, 21.365188918076456, 13.200451218057424, 21.815353287849575, 23.36540397303179, 30.774895332055166, 52.70271303795744, 32.36758208996616, 34.37526571715716, 23.954674200969748, 39.815027864067815, 34.81239018996712, 29.756394917028956, 24.97814504604321, 25.466845585033298, 40.73914177704137, 14.08249856904149, 29.423279676004313, 41.72725166508462, 47.96578435692936, 44.57091286301147, 48.422887271037325, 52.507461837958544, 36.13651512900833, 33.68134845304303, 41.74973771010991, 42.92588617815636, 26.171627731062472, 11.442796613904648, 20.53017031098716, 34.00978302990552, 32.29142361099366, 42.519580012070946, 50.58196782995947, 34.10235163511243, 48.246594002936035, 46.744060026016086, 48.72932616202161, 48.32557889900636, 39.28951763687655, 59.266554726753384, 52.908632420003414, 48.47252580814529, 33.237629567156546, 59.10195338586345, 39.42644432315137, 47.53789387503639, 51.15798378398176, 41.708472554106265, 53.612837482010946, 38.407562338979915, 44.89220120490063, 54.518546438077465, 52.9467857349664, 40.64679802500177, 73.27487441990525, 40.14493273408152, 55.779446191038005, 55.491906648967415, 49.7624646788463, 56.12293230101932, 44.84644843393471, 44.14910569495987, 52.1315398189472, 56.25282858603168, 35.3298473599134, 60.956419083988294, 43.54563763202168, 53.93601385317743, 41.901757701067254, 50.34555417892989, 35.869194306083955, 37.814526504138485, 34.063161251018755, 51.66609420499299, 30.236066646059044, 53.4532924919622, 47.378589916974306, 52.2596722060116, 40.80917556316126, 40.134522012085654, 33.846996185951866, 27.076160209137015, 41.31804654793814, 39.680578295956366, 30.758563606999815, 51.62304590002168, 23.614473741850816, 45.534712944878265, 35.62690176896285, 51.175536733004265, 31.81939012708608, 38.80306720105, 41.26399371505249, 35.56813343404792, 36.43745450198185, 32.161750653991476, 30.48422515182756, 58.50680530606769, 22.5448902300559, 32.83191937592346, 35.23815129417926, 44.518220960046165, 29.163427519029938, 50.95208909397479, 31.028591830050573, 29.858399144024588, 40.29555747413542, 33.86595440807287]
Total Epoch List: [84, 75, 99]
Total Time List: [12.24022517807316, 2.9020142449298874, 2.3333677899790928]
T-times Epoch Time: 34.168807364837605 ~ 11.627362428940298
T-times Total Epoch: 88.5 ~ 2.5
T-times Total Time: 6.048035700650265 ~ 0.2228332963228845
T-times Inference Elapsed: 7.030182875171889 ~ 2.1743417718353637
T-times Time Per Graph: 0.004159871523770348 ~ 0.0012865927643996237
T-times Speed: 484.22289010603015 ~ 145.2204035573583
T-times cross validation test micro f1 score:0.9664151496477733 ~ 0.0009861932938856621
T-times cross validation test precision:0.9793657854082679 ~ 0.0015780938602689698
T-times cross validation test recall:0.9538461538461538 ~ 0.00039447731755420934
T-times cross validation test f1_score:0.9664151496477733 ~ 0.0009754325662529872
