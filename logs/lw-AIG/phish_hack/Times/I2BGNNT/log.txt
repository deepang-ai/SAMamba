Namespace(seed=15, model='I2BGNNT', dataset='phish_hack/Times', num_heads=1, num_layers=2, dim_hidden=64, dropout=0.1, epochs=1000, lr=0.0003, weight_decay=0.0001, batch_size=64, abs_pe=None, abs_pe_dim=20, num_class=2, outdir='./outdir/phish_hack/Times/seed15/edge_attr/khopgnn_gat_1_0.1_0.0003_0.0001_2_1_64_BN', warmup=10, layer_norm=False, use_edge_attr=True, edge_dim=128, gnn_type='gat', k_hop=1, global_pool='mean', se='khopgnn', ie='mamba', aggr='add', not_extract_node_feature=False, k_ford=3, early_stop=1, early_stop_mindelta=-0.0, patience=20, training_times=2, Lambda=0.01, order='sd', use_cuda=True, batch_norm=True, save_logs=True)
Data(edge_index=[2, 248], edge_attr=[248, 2], x=[104, 14887], y=[1, 1], num_nodes=104)
Data(edge_index=[2, 248], edge_attr=[248, 2], x=[104, 14887], y=[1, 1], num_nodes=104)
Data(edge_index=[2, 218], edge_attr=[218, 2], x=[94, 14887], y=[1, 1], num_nodes=104)
========================training times:0========================
========================k_idx:0========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: I2BGNNT
I2BGNN(
  (gcs): ModuleList(
    (0): GCNConv(14887, 64)
    (1): GCNConv(64, 64)
  )
  (bns): ModuleList(
    (0-1): 2 x BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  )
  (drops): ModuleList(
    (0-1): 2 x Dropout(p=0.2, inplace=False)
  )
  (lin1): Linear(in_features=64, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=64, bias=True)
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=64, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=64, out_features=2, bias=True)
  )
)
Total number of parameters: 969858
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7e3645153220>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.6925;  Loss pred: 0.6925; Loss self: 0.0000; time: 6.82s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6934 score: 0.5000 time: 2.57s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6934 score: 0.5000 time: 2.40s
Epoch 2/1000, LR 0.000029
Train loss: 0.6903;  Loss pred: 0.6903; Loss self: 0.0000; time: 6.62s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6931 score: 0.5000 time: 2.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6930 score: 0.5000 time: 2.02s
Epoch 3/1000, LR 0.000059
Train loss: 0.6870;  Loss pred: 0.6870; Loss self: 0.0000; time: 6.42s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6889 score: 0.5000 time: 2.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6888 score: 0.5000 time: 2.06s
Epoch 4/1000, LR 0.000089
Train loss: 0.6810;  Loss pred: 0.6810; Loss self: 0.0000; time: 7.13s
Val loss: 0.6720 score: 0.6941 time: 2.33s
Test loss: 0.6716 score: 0.6746 time: 2.34s
Epoch 5/1000, LR 0.000119
Train loss: 0.6700;  Loss pred: 0.6700; Loss self: 0.0000; time: 6.92s
Val loss: 0.6482 score: 0.7254 time: 2.40s
Test loss: 0.6474 score: 0.7065 time: 2.29s
Epoch 6/1000, LR 0.000149
Train loss: 0.6495;  Loss pred: 0.6495; Loss self: 0.0000; time: 6.57s
Val loss: 0.6193 score: 0.6550 time: 2.15s
Test loss: 0.6185 score: 0.6396 time: 2.07s
Epoch 7/1000, LR 0.000179
Train loss: 0.6077;  Loss pred: 0.6077; Loss self: 0.0000; time: 6.61s
Val loss: 0.5803 score: 0.6272 time: 2.10s
Test loss: 0.5808 score: 0.6154 time: 2.02s
Epoch 8/1000, LR 0.000209
Train loss: 0.5354;  Loss pred: 0.5354; Loss self: 0.0000; time: 6.20s
Val loss: 0.5566 score: 0.5822 time: 1.96s
Test loss: 0.5613 score: 0.5740 time: 1.94s
Epoch 9/1000, LR 0.000239
Train loss: 0.4026;  Loss pred: 0.4026; Loss self: 0.0000; time: 6.53s
Val loss: 0.5562 score: 0.6012 time: 2.26s
Test loss: 0.5677 score: 0.6030 time: 2.32s
Epoch 10/1000, LR 0.000269
Train loss: 0.2155;  Loss pred: 0.2155; Loss self: 0.0000; time: 6.60s
Val loss: 0.6547 score: 0.6503 time: 2.11s
Test loss: 0.6777 score: 0.6515 time: 2.05s
     INFO: Early stopping counter 1 of 20
Epoch 11/1000, LR 0.000299
Train loss: 0.0634;  Loss pred: 0.0634; Loss self: 0.0000; time: 6.37s
Val loss: 0.4753 score: 0.7811 time: 2.08s
Test loss: 0.5053 score: 0.7716 time: 2.12s
Epoch 12/1000, LR 0.000299
Train loss: 0.0178;  Loss pred: 0.0178; Loss self: 0.0000; time: 6.47s
Val loss: 0.6285 score: 0.7249 time: 2.06s
Test loss: 0.6653 score: 0.7260 time: 1.98s
     INFO: Early stopping counter 1 of 20
Epoch 13/1000, LR 0.000299
Train loss: 0.0094;  Loss pred: 0.0094; Loss self: 0.0000; time: 6.15s
Val loss: 0.5286 score: 0.7675 time: 2.09s
Test loss: 0.5651 score: 0.7615 time: 2.27s
     INFO: Early stopping counter 2 of 20
Epoch 14/1000, LR 0.000299
Train loss: 0.0055;  Loss pred: 0.0055; Loss self: 0.0000; time: 6.40s
Val loss: 0.6998 score: 0.7254 time: 2.07s
Test loss: 0.7465 score: 0.7272 time: 1.98s
     INFO: Early stopping counter 3 of 20
Epoch 15/1000, LR 0.000299
Train loss: 0.0045;  Loss pred: 0.0045; Loss self: 0.0000; time: 10.26s
Val loss: 0.5656 score: 0.7763 time: 1.95s
Test loss: 0.6093 score: 0.7698 time: 1.89s
     INFO: Early stopping counter 4 of 20
Epoch 16/1000, LR 0.000299
Train loss: 0.0031;  Loss pred: 0.0031; Loss self: 0.0000; time: 6.18s
Val loss: 0.6616 score: 0.7562 time: 2.09s
Test loss: 0.7122 score: 0.7503 time: 1.99s
     INFO: Early stopping counter 5 of 20
Epoch 17/1000, LR 0.000299
Train loss: 0.0025;  Loss pred: 0.0025; Loss self: 0.0000; time: 6.16s
Val loss: 0.6908 score: 0.7538 time: 2.06s
Test loss: 0.7439 score: 0.7497 time: 1.97s
     INFO: Early stopping counter 6 of 20
Epoch 18/1000, LR 0.000299
Train loss: 0.0022;  Loss pred: 0.0022; Loss self: 0.0000; time: 6.22s
Val loss: 0.6177 score: 0.7751 time: 2.11s
Test loss: 0.6685 score: 0.7669 time: 2.06s
     INFO: Early stopping counter 7 of 20
Epoch 19/1000, LR 0.000299
Train loss: 0.0018;  Loss pred: 0.0018; Loss self: 0.0000; time: 6.23s
Val loss: 0.6435 score: 0.7751 time: 2.26s
Test loss: 0.6972 score: 0.7645 time: 2.15s
     INFO: Early stopping counter 8 of 20
Epoch 20/1000, LR 0.000299
Train loss: 0.0015;  Loss pred: 0.0015; Loss self: 0.0000; time: 6.42s
Val loss: 0.6509 score: 0.7757 time: 2.16s
Test loss: 0.7052 score: 0.7663 time: 2.25s
     INFO: Early stopping counter 9 of 20
Epoch 21/1000, LR 0.000299
Train loss: 0.0013;  Loss pred: 0.0013; Loss self: 0.0000; time: 6.51s
Val loss: 0.6207 score: 0.7864 time: 2.10s
Test loss: 0.6737 score: 0.7763 time: 1.98s
     INFO: Early stopping counter 10 of 20
Epoch 22/1000, LR 0.000299
Train loss: 0.0012;  Loss pred: 0.0012; Loss self: 0.0000; time: 6.09s
Val loss: 0.7269 score: 0.7604 time: 2.10s
Test loss: 0.7857 score: 0.7562 time: 1.92s
     INFO: Early stopping counter 11 of 20
Epoch 23/1000, LR 0.000299
Train loss: 0.0010;  Loss pred: 0.0010; Loss self: 0.0000; time: 6.19s
Val loss: 0.6948 score: 0.7769 time: 1.93s
Test loss: 0.7531 score: 0.7698 time: 1.87s
     INFO: Early stopping counter 12 of 20
Epoch 24/1000, LR 0.000299
Train loss: 0.0010;  Loss pred: 0.0010; Loss self: 0.0000; time: 6.16s
Val loss: 0.6362 score: 0.7935 time: 2.18s
Test loss: 0.6915 score: 0.7876 time: 2.19s
     INFO: Early stopping counter 13 of 20
Epoch 25/1000, LR 0.000299
Train loss: 0.0008;  Loss pred: 0.0008; Loss self: 0.0000; time: 6.60s
Val loss: 0.7012 score: 0.7757 time: 2.23s
Test loss: 0.7597 score: 0.7663 time: 3.49s
     INFO: Early stopping counter 14 of 20
Epoch 26/1000, LR 0.000299
Train loss: 0.0007;  Loss pred: 0.0007; Loss self: 0.0000; time: 8.75s
Val loss: 0.7575 score: 0.7580 time: 2.35s
Test loss: 0.8178 score: 0.7538 time: 2.19s
     INFO: Early stopping counter 15 of 20
Epoch 27/1000, LR 0.000299
Train loss: 0.0006;  Loss pred: 0.0006; Loss self: 0.0000; time: 6.98s
Val loss: 0.6695 score: 0.7828 time: 2.36s
Test loss: 0.7266 score: 0.7728 time: 2.68s
     INFO: Early stopping counter 16 of 20
Epoch 28/1000, LR 0.000299
Train loss: 0.0005;  Loss pred: 0.0005; Loss self: 0.0000; time: 6.75s
Val loss: 0.7093 score: 0.7746 time: 2.19s
Test loss: 0.7686 score: 0.7657 time: 2.44s
     INFO: Early stopping counter 17 of 20
Epoch 29/1000, LR 0.000299
Train loss: 0.0005;  Loss pred: 0.0005; Loss self: 0.0000; time: 6.69s
Val loss: 0.7133 score: 0.7751 time: 2.16s
Test loss: 0.7733 score: 0.7663 time: 2.06s
     INFO: Early stopping counter 18 of 20
Epoch 30/1000, LR 0.000299
Train loss: 0.0005;  Loss pred: 0.0005; Loss self: 0.0000; time: 7.79s
Val loss: 0.7142 score: 0.7769 time: 2.44s
Test loss: 0.7744 score: 0.7680 time: 1.96s
     INFO: Early stopping counter 19 of 20
Epoch 31/1000, LR 0.000299
Train loss: 0.0005;  Loss pred: 0.0005; Loss self: 0.0000; time: 6.26s
Val loss: 0.7274 score: 0.7746 time: 2.09s
Test loss: 0.7881 score: 0.7663 time: 1.89s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 010,   Train_Loss: 0.0634,   Val_Loss: 0.4753,   Val_Precision: 0.9857,   Val_Recall: 0.5704,   Val_accuracy: 0.7226,   Val_Score: 0.7811,   Val_Loss: 0.4753,   Test_Precision: 0.9914,   Test_Recall: 0.5479,   Test_accuracy: 0.7058,   Test_Score: 0.7716,   Test_loss: 0.5053


[2.4095491940388456, 2.027767034014687, 2.065021723974496, 2.3424211490200832, 2.2917687100125477, 2.0730484529631212, 2.026286744978279, 1.9422559960512444, 2.3286584289744496, 2.053686457918957, 2.1250873879762366, 1.9866675219964236, 2.2730366210453212, 1.9878991779405624, 1.8953465439844877, 1.9914596549933776, 1.9802538480143994, 2.0674888910725713, 2.152672305004671, 2.2591636129654944, 1.9829172029858455, 1.929170661023818, 1.8773399509955198, 2.1950947149889544, 3.4955117360223085, 2.1950646810000762, 2.683234693016857, 2.4510414879769087, 2.064569848938845, 1.969017751980573, 1.8920665349578485]
[0.0014257687538691394, 0.0011998621503045484, 0.0012219063455470391, 0.0013860480171716469, 0.0013560761597707382, 0.0012266558893272907, 0.0011989862396321177, 0.001149263902988902, 0.0013779043958428697, 0.00121519908752601, 0.0012574481585658204, 0.0011755429124239193, 0.00134499208345877, 0.0011762717029234096, 0.0011215068307600519, 0.0011783784940789218, 0.0011717478390617748, 0.0012233662077352492, 0.0012737705946773202, 0.0013367832029381624, 0.001173323788749021, 0.0011415211011975253, 0.0011108520420091833, 0.0012988726124194996, 0.0020683501396581705, 0.0012988548408284476, 0.001587712836104649, 0.0014503204070869281, 0.0012216389638691392, 0.0011650992615269663, 0.0011195659970164784]
[701.3760101603282, 833.4290732866108, 818.393327479044, 721.4757263897596, 737.4217095366256, 815.2245537649594, 834.0379288312998, 870.1221689807625, 725.7397559779871, 822.9104269950302, 795.261413512703, 850.6707746959596, 743.4987999545757, 850.1437189338837, 891.657520554105, 848.6237698878313, 853.4259391514694, 817.4167258152775, 785.0707216658008, 748.0644563771187, 852.2796602173931, 876.0241041106808, 900.2098949121192, 769.89844149322, 483.4771351456319, 769.908975634391, 629.8368176284545, 689.5028126981756, 818.5724502702741, 858.2959692974237, 893.2032615003415]
Elapsed: 2.1617602813170262~0.30592143644162434
Time per graph: 0.0012791480954538617~0.00018101860144474814
Speed: 793.7152917696528~86.44887062964867
Total Time: 1.8925
best val loss: 0.4753042431625389 test_score: 0.7716

Testing...
Test loss: 0.6915 score: 0.7876 time: 1.87s
test Score 0.7876
Epoch Time List: [11.790671681170352, 10.907358843949623, 10.592530591995455, 11.80493331712205, 11.60099192801863, 10.789073272026144, 10.736601696000434, 10.099645330104977, 11.108201753930189, 10.750599425984547, 10.572006760980003, 10.50924979604315, 10.511360240983777, 10.449827456963249, 14.09864011395257, 10.258792735869065, 10.191800610045902, 10.387205867911689, 10.637225125916302, 10.8424864950357, 10.595081763109192, 10.107480062055402, 9.98678844794631, 10.52395715401508, 12.32137307699304, 13.29106897301972, 12.015290773939341, 11.39050128182862, 10.90966669213958, 12.190161732956767, 10.237523401970975]
Total Epoch List: [31]
Total Time List: [1.8924651920096949]
========================k_idx:1========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: I2BGNNT
I2BGNN(
  (gcs): ModuleList(
    (0): GCNConv(14887, 64)
    (1): GCNConv(64, 64)
  )
  (bns): ModuleList(
    (0-1): 2 x BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  )
  (drops): ModuleList(
    (0-1): 2 x Dropout(p=0.2, inplace=False)
  )
  (lin1): Linear(in_features=64, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=64, bias=True)
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=64, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=64, out_features=2, bias=True)
  )
)
Total number of parameters: 969858
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7e3645434820>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.6959;  Loss pred: 0.6959; Loss self: 0.0000; time: 6.30s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6948 score: 0.5000 time: 2.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6948 score: 0.5000 time: 2.45s
Epoch 2/1000, LR 0.000029
Train loss: 0.6948;  Loss pred: 0.6948; Loss self: 0.0000; time: 6.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6950 score: 0.5000 time: 2.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6951 score: 0.5000 time: 2.10s
     INFO: Early stopping counter 1 of 20
Epoch 3/1000, LR 0.000059
Train loss: 0.6927;  Loss pred: 0.6927; Loss self: 0.0000; time: 5.92s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6939 score: 0.5000 time: 2.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6939 score: 0.5000 time: 2.05s
Epoch 4/1000, LR 0.000089
Train loss: 0.6893;  Loss pred: 0.6893; Loss self: 0.0000; time: 5.93s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6843 score: 0.5000 time: 2.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6845 score: 0.5000 time: 2.08s
Epoch 5/1000, LR 0.000119
Train loss: 0.6805;  Loss pred: 0.6805; Loss self: 0.0000; time: 5.97s
Val loss: 0.6672 score: 0.5006 time: 2.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6680 score: 0.5000 time: 2.18s
Epoch 6/1000, LR 0.000149
Train loss: 0.6616;  Loss pred: 0.6616; Loss self: 0.0000; time: 6.09s
Val loss: 0.6375 score: 0.5201 time: 2.08s
Test loss: 0.6405 score: 0.5118 time: 2.11s
Epoch 7/1000, LR 0.000179
Train loss: 0.6209;  Loss pred: 0.6209; Loss self: 0.0000; time: 5.96s
Val loss: 0.5891 score: 0.6633 time: 2.06s
Test loss: 0.5959 score: 0.6385 time: 2.03s
Epoch 8/1000, LR 0.000209
Train loss: 0.5437;  Loss pred: 0.5437; Loss self: 0.0000; time: 5.83s
Val loss: 0.5196 score: 0.7893 time: 2.04s
Test loss: 0.5317 score: 0.7669 time: 2.04s
Epoch 9/1000, LR 0.000239
Train loss: 0.4010;  Loss pred: 0.4010; Loss self: 0.0000; time: 5.80s
Val loss: 0.4328 score: 0.8213 time: 2.06s
Test loss: 0.4507 score: 0.7994 time: 2.07s
Epoch 10/1000, LR 0.000269
Train loss: 0.1955;  Loss pred: 0.1955; Loss self: 0.0000; time: 5.77s
Val loss: 0.3926 score: 0.8047 time: 2.05s
Test loss: 0.4188 score: 0.7811 time: 2.09s
Epoch 11/1000, LR 0.000299
Train loss: 0.0467;  Loss pred: 0.0467; Loss self: 0.0000; time: 6.16s
Val loss: 0.3065 score: 0.8686 time: 2.12s
Test loss: 0.3272 score: 0.8485 time: 2.15s
Epoch 12/1000, LR 0.000299
Train loss: 0.0139;  Loss pred: 0.0139; Loss self: 0.0000; time: 6.29s
Val loss: 0.3710 score: 0.8408 time: 2.23s
Test loss: 0.3956 score: 0.8237 time: 2.24s
     INFO: Early stopping counter 1 of 20
Epoch 13/1000, LR 0.000299
Train loss: 0.0074;  Loss pred: 0.0074; Loss self: 0.0000; time: 5.87s
Val loss: 0.4137 score: 0.8296 time: 2.07s
Test loss: 0.4414 score: 0.8124 time: 2.08s
     INFO: Early stopping counter 2 of 20
Epoch 14/1000, LR 0.000299
Train loss: 0.0049;  Loss pred: 0.0049; Loss self: 0.0000; time: 5.88s
Val loss: 0.3889 score: 0.8479 time: 2.05s
Test loss: 0.4101 score: 0.8314 time: 2.08s
     INFO: Early stopping counter 3 of 20
Epoch 15/1000, LR 0.000299
Train loss: 0.0035;  Loss pred: 0.0035; Loss self: 0.0000; time: 5.93s
Val loss: 0.4606 score: 0.8260 time: 5.73s
Test loss: 0.4869 score: 0.8053 time: 2.79s
     INFO: Early stopping counter 4 of 20
Epoch 16/1000, LR 0.000299
Train loss: 0.0025;  Loss pred: 0.0025; Loss self: 0.0000; time: 7.64s
Val loss: 0.4711 score: 0.8266 time: 2.84s
Test loss: 0.4976 score: 0.8083 time: 2.80s
     INFO: Early stopping counter 5 of 20
Epoch 17/1000, LR 0.000299
Train loss: 0.0023;  Loss pred: 0.0023; Loss self: 0.0000; time: 10.34s
Val loss: 0.4811 score: 0.8278 time: 2.41s
Test loss: 0.5084 score: 0.8083 time: 2.38s
     INFO: Early stopping counter 6 of 20
Epoch 18/1000, LR 0.000299
Train loss: 0.0016;  Loss pred: 0.0016; Loss self: 0.0000; time: 6.74s
Val loss: 0.4698 score: 0.8343 time: 2.36s
Test loss: 0.4947 score: 0.8148 time: 2.81s
     INFO: Early stopping counter 7 of 20
Epoch 19/1000, LR 0.000299
Train loss: 0.0013;  Loss pred: 0.0013; Loss self: 0.0000; time: 7.70s
Val loss: 0.4901 score: 0.8325 time: 2.38s
Test loss: 0.5156 score: 0.8142 time: 2.31s
     INFO: Early stopping counter 8 of 20
Epoch 20/1000, LR 0.000299
Train loss: 0.0010;  Loss pred: 0.0010; Loss self: 0.0000; time: 6.69s
Val loss: 0.5418 score: 0.8207 time: 2.86s
Test loss: 0.5717 score: 0.8000 time: 2.65s
     INFO: Early stopping counter 9 of 20
Epoch 21/1000, LR 0.000299
Train loss: 0.0011;  Loss pred: 0.0011; Loss self: 0.0000; time: 8.33s
Val loss: 0.5071 score: 0.8337 time: 2.23s
Test loss: 0.5316 score: 0.8136 time: 2.20s
     INFO: Early stopping counter 10 of 20
Epoch 22/1000, LR 0.000299
Train loss: 0.0008;  Loss pred: 0.0008; Loss self: 0.0000; time: 6.30s
Val loss: 0.5649 score: 0.8213 time: 2.16s
Test loss: 0.5956 score: 0.8018 time: 2.23s
     INFO: Early stopping counter 11 of 20
Epoch 23/1000, LR 0.000299
Train loss: 0.0007;  Loss pred: 0.0007; Loss self: 0.0000; time: 6.25s
Val loss: 0.5532 score: 0.8272 time: 2.35s
Test loss: 0.5810 score: 0.8077 time: 2.32s
     INFO: Early stopping counter 12 of 20
Epoch 24/1000, LR 0.000299
Train loss: 0.0007;  Loss pred: 0.0007; Loss self: 0.0000; time: 6.13s
Val loss: 0.5934 score: 0.8225 time: 2.24s
Test loss: 0.6239 score: 0.8018 time: 2.11s
     INFO: Early stopping counter 13 of 20
Epoch 25/1000, LR 0.000299
Train loss: 0.0006;  Loss pred: 0.0006; Loss self: 0.0000; time: 5.90s
Val loss: 0.5408 score: 0.8361 time: 2.09s
Test loss: 0.5645 score: 0.8172 time: 2.12s
     INFO: Early stopping counter 14 of 20
Epoch 26/1000, LR 0.000299
Train loss: 0.0005;  Loss pred: 0.0005; Loss self: 0.0000; time: 5.96s
Val loss: 0.5821 score: 0.8260 time: 2.08s
Test loss: 0.6098 score: 0.8077 time: 2.12s
     INFO: Early stopping counter 15 of 20
Epoch 27/1000, LR 0.000299
Train loss: 0.0004;  Loss pred: 0.0004; Loss self: 0.0000; time: 6.13s
Val loss: 0.5871 score: 0.8260 time: 2.12s
Test loss: 0.6147 score: 0.8065 time: 2.26s
     INFO: Early stopping counter 16 of 20
Epoch 28/1000, LR 0.000299
Train loss: 0.0004;  Loss pred: 0.0004; Loss self: 0.0000; time: 6.24s
Val loss: 0.5730 score: 0.8314 time: 2.13s
Test loss: 0.5980 score: 0.8118 time: 2.13s
     INFO: Early stopping counter 17 of 20
Epoch 29/1000, LR 0.000299
Train loss: 0.0004;  Loss pred: 0.0004; Loss self: 0.0000; time: 5.94s
Val loss: 0.5842 score: 0.8296 time: 2.12s
Test loss: 0.6095 score: 0.8089 time: 2.13s
     INFO: Early stopping counter 18 of 20
Epoch 30/1000, LR 0.000299
Train loss: 0.0003;  Loss pred: 0.0003; Loss self: 0.0000; time: 5.94s
Val loss: 0.5975 score: 0.8290 time: 2.12s
Test loss: 0.6234 score: 0.8083 time: 2.08s
     INFO: Early stopping counter 19 of 20
Epoch 31/1000, LR 0.000299
Train loss: 0.0003;  Loss pred: 0.0003; Loss self: 0.0000; time: 5.94s
Val loss: 0.6204 score: 0.8254 time: 2.10s
Test loss: 0.6490 score: 0.8030 time: 2.12s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 010,   Train_Loss: 0.0467,   Val_Loss: 0.3065,   Val_Precision: 0.9952,   Val_Recall: 0.7408,   Val_accuracy: 0.8494,   Val_Score: 0.8686,   Val_Loss: 0.3065,   Test_Precision: 0.9820,   Test_Recall: 0.7101,   Test_accuracy: 0.8242,   Test_Score: 0.8485,   Test_loss: 0.3272


[2.4095491940388456, 2.027767034014687, 2.065021723974496, 2.3424211490200832, 2.2917687100125477, 2.0730484529631212, 2.026286744978279, 1.9422559960512444, 2.3286584289744496, 2.053686457918957, 2.1250873879762366, 1.9866675219964236, 2.2730366210453212, 1.9878991779405624, 1.8953465439844877, 1.9914596549933776, 1.9802538480143994, 2.0674888910725713, 2.152672305004671, 2.2591636129654944, 1.9829172029858455, 1.929170661023818, 1.8773399509955198, 2.1950947149889544, 3.4955117360223085, 2.1950646810000762, 2.683234693016857, 2.4510414879769087, 2.064569848938845, 1.969017751980573, 1.8920665349578485, 2.4591254480183125, 2.1053321650251746, 2.0557578469160944, 2.0813779460731894, 2.1821899010101333, 2.119481544010341, 2.035496974014677, 2.0410604750504717, 2.073365402990021, 2.092568786931224, 2.154107589973137, 2.2420547689544037, 2.0905618300894275, 2.0803155130706728, 2.7953705249819905, 2.80577417393215, 2.3852881899802014, 2.8181204290594906, 2.314088712912053, 2.65876299503725, 2.20105577504728, 2.238556309021078, 2.3222123610321432, 2.119520177016966, 2.127837108913809, 2.130009803106077, 2.2640218869782984, 2.1332332199672237, 2.137862449977547, 2.0858564280206338, 2.120841305004433]
[0.0014257687538691394, 0.0011998621503045484, 0.0012219063455470391, 0.0013860480171716469, 0.0013560761597707382, 0.0012266558893272907, 0.0011989862396321177, 0.001149263902988902, 0.0013779043958428697, 0.00121519908752601, 0.0012574481585658204, 0.0011755429124239193, 0.00134499208345877, 0.0011762717029234096, 0.0011215068307600519, 0.0011783784940789218, 0.0011717478390617748, 0.0012233662077352492, 0.0012737705946773202, 0.0013367832029381624, 0.001173323788749021, 0.0011415211011975253, 0.0011108520420091833, 0.0012988726124194996, 0.0020683501396581705, 0.0012988548408284476, 0.001587712836104649, 0.0014503204070869281, 0.0012216389638691392, 0.0011650992615269663, 0.0011195659970164784, 0.0014551038153954512, 0.001245758677529689, 0.0012164247614888132, 0.00123158458347526, 0.0012912366278166468, 0.0012541310911303794, 0.0012044360792986255, 0.0012077280917458413, 0.0012268434337219059, 0.001238206382799541, 0.0012746198757237496, 0.001326659626600239, 0.0012370188343724423, 0.0012309559248938893, 0.0016540653993976275, 0.0016602214046935798, 0.0014114131301657996, 0.0016675268811002903, 0.0013692832620781378, 0.00157323254144216, 0.0013023998668918815, 0.0013245895319651349, 0.0013740901544568895, 0.001254153950897613, 0.001259075212375035, 0.0012603608302402822, 0.001339657921288934, 0.0012622681774953986, 0.0012650073668506194, 0.0012342345727932745, 0.0012549356834345756]
[701.3760101603282, 833.4290732866108, 818.393327479044, 721.4757263897596, 737.4217095366256, 815.2245537649594, 834.0379288312998, 870.1221689807625, 725.7397559779871, 822.9104269950302, 795.261413512703, 850.6707746959596, 743.4987999545757, 850.1437189338837, 891.657520554105, 848.6237698878313, 853.4259391514694, 817.4167258152775, 785.0707216658008, 748.0644563771187, 852.2796602173931, 876.0241041106808, 900.2098949121192, 769.89844149322, 483.4771351456319, 769.908975634391, 629.8368176284545, 689.5028126981756, 818.5724502702741, 858.2959692974237, 893.2032615003415, 687.2361885246186, 802.723688012334, 822.0812594903729, 811.9620961625069, 774.4513890462516, 797.3648106424627, 830.2640689594137, 828.0009439495954, 815.0999324879417, 807.6198070785545, 784.5476279209784, 753.7728441790662, 808.39512884807, 812.3767713991888, 604.571016577807, 602.3293020876127, 708.509775506006, 599.6904825547198, 730.3090804471801, 635.6339407290128, 767.8133462854652, 754.9508552407324, 727.7542865411557, 797.3502768813095, 794.2337281929863, 793.4235783964774, 746.4592147806385, 792.2246776308716, 790.5092303846533, 810.2187558535463, 796.8535863632039]
Elapsed: 2.2013834961765117~0.27239474177220885
Time per graph: 0.0013025937847198292~0.00016118032057527151
Speed: 777.32106025829~79.18369940370059
Total Time: 2.1214
best val loss: 0.3064653103168194 test_score: 0.8485

Testing...
Test loss: 0.3272 score: 0.8485 time: 2.07s
test Score 0.8485
Epoch Time List: [11.790671681170352, 10.907358843949623, 10.592530591995455, 11.80493331712205, 11.60099192801863, 10.789073272026144, 10.736601696000434, 10.099645330104977, 11.108201753930189, 10.750599425984547, 10.572006760980003, 10.50924979604315, 10.511360240983777, 10.449827456963249, 14.09864011395257, 10.258792735869065, 10.191800610045902, 10.387205867911689, 10.637225125916302, 10.8424864950357, 10.595081763109192, 10.107480062055402, 9.98678844794631, 10.52395715401508, 12.32137307699304, 13.29106897301972, 12.015290773939341, 11.39050128182862, 10.90966669213958, 12.190161732956767, 10.237523401970975, 11.006591119104996, 10.41995948087424, 10.035285145044327, 10.22265453694854, 10.22904157009907, 10.278158882865682, 10.049115550937131, 9.902666807873175, 9.925769771100022, 9.900576573098078, 10.422036159201525, 10.760369924944825, 10.025798697024584, 10.006072252057493, 14.450106390053406, 13.279808441060595, 15.128187246038578, 11.911566992872395, 12.380393478902988, 12.200751139083877, 12.757169116986915, 10.697330272989348, 10.919224217883311, 10.489024708978832, 10.109250352019444, 10.159498081193306, 10.512068181065843, 10.489352784119546, 10.187858409131877, 10.138341133017093, 10.155730176949874]
Total Epoch List: [31, 31]
Total Time List: [1.8924651920096949, 2.1213791409973055]
========================k_idx:2========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: I2BGNNT
I2BGNN(
  (gcs): ModuleList(
    (0): GCNConv(14887, 64)
    (1): GCNConv(64, 64)
  )
  (bns): ModuleList(
    (0-1): 2 x BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  )
  (drops): ModuleList(
    (0-1): 2 x Dropout(p=0.2, inplace=False)
  )
  (lin1): Linear(in_features=64, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=64, bias=True)
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=64, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=64, out_features=2, bias=True)
  )
)
Total number of parameters: 969858
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7e3645434e20>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.6939;  Loss pred: 0.6939; Loss self: 0.0000; time: 6.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6939 score: 0.5000 time: 2.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6939 score: 0.5000 time: 2.21s
Epoch 2/1000, LR 0.000029
Train loss: 0.6919;  Loss pred: 0.6919; Loss self: 0.0000; time: 6.72s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6933 score: 0.5000 time: 2.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6932 score: 0.5000 time: 2.29s
Epoch 3/1000, LR 0.000059
Train loss: 0.6874;  Loss pred: 0.6874; Loss self: 0.0000; time: 6.39s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6907 score: 0.5000 time: 2.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6903 score: 0.5000 time: 2.09s
Epoch 4/1000, LR 0.000089
Train loss: 0.6802;  Loss pred: 0.6802; Loss self: 0.0000; time: 6.05s
Val loss: 0.6760 score: 0.6373 time: 2.07s
Test loss: 0.6741 score: 0.6627 time: 2.07s
Epoch 5/1000, LR 0.000119
Train loss: 0.6672;  Loss pred: 0.6672; Loss self: 0.0000; time: 6.13s
Val loss: 0.6609 score: 0.7456 time: 2.07s
Test loss: 0.6566 score: 0.7704 time: 2.07s
Epoch 6/1000, LR 0.000149
Train loss: 0.6406;  Loss pred: 0.6406; Loss self: 0.0000; time: 6.12s
Val loss: 0.6470 score: 0.5959 time: 2.13s
Test loss: 0.6396 score: 0.6142 time: 2.19s
Epoch 7/1000, LR 0.000179
Train loss: 0.6016;  Loss pred: 0.6016; Loss self: 0.0000; time: 6.10s
Val loss: 0.6635 score: 0.5142 time: 2.13s
Test loss: 0.6529 score: 0.5183 time: 2.38s
     INFO: Early stopping counter 1 of 20
Epoch 8/1000, LR 0.000209
Train loss: 0.5313;  Loss pred: 0.5313; Loss self: 0.0000; time: 6.19s
Val loss: 0.7141 score: 0.5172 time: 2.06s
Test loss: 0.7009 score: 0.5172 time: 2.08s
     INFO: Early stopping counter 2 of 20
Epoch 9/1000, LR 0.000239
Train loss: 0.4089;  Loss pred: 0.4089; Loss self: 0.0000; time: 6.12s
Val loss: 0.9035 score: 0.5172 time: 2.07s
Test loss: 0.8837 score: 0.5201 time: 2.06s
     INFO: Early stopping counter 3 of 20
Epoch 10/1000, LR 0.000269
Train loss: 0.2430;  Loss pred: 0.2430; Loss self: 0.0000; time: 6.02s
Val loss: 1.0901 score: 0.5278 time: 2.05s
Test loss: 1.0636 score: 0.5314 time: 2.07s
     INFO: Early stopping counter 4 of 20
Epoch 11/1000, LR 0.000299
Train loss: 0.0869;  Loss pred: 0.0869; Loss self: 0.0000; time: 8.81s
Val loss: 0.7780 score: 0.6166 time: 2.35s
Test loss: 0.7456 score: 0.6308 time: 2.38s
     INFO: Early stopping counter 5 of 20
Epoch 12/1000, LR 0.000299
Train loss: 0.0240;  Loss pred: 0.0240; Loss self: 0.0000; time: 7.34s
Val loss: 1.1588 score: 0.5692 time: 2.48s
Test loss: 1.1131 score: 0.5799 time: 3.45s
     INFO: Early stopping counter 6 of 20
Epoch 13/1000, LR 0.000299
Train loss: 0.0116;  Loss pred: 0.0116; Loss self: 0.0000; time: 9.84s
Val loss: 0.9198 score: 0.6249 time: 2.38s
Test loss: 0.8762 score: 0.6361 time: 2.41s
     INFO: Early stopping counter 7 of 20
Epoch 14/1000, LR 0.000299
Train loss: 0.0072;  Loss pred: 0.0072; Loss self: 0.0000; time: 7.64s
Val loss: 1.3026 score: 0.5882 time: 3.43s
Test loss: 1.2395 score: 0.5970 time: 2.48s
     INFO: Early stopping counter 8 of 20
Epoch 15/1000, LR 0.000299
Train loss: 0.0063;  Loss pred: 0.0063; Loss self: 0.0000; time: 7.52s
Val loss: 0.7233 score: 0.7000 time: 2.42s
Test loss: 0.6883 score: 0.7065 time: 2.56s
     INFO: Early stopping counter 9 of 20
Epoch 16/1000, LR 0.000299
Train loss: 0.0043;  Loss pred: 0.0043; Loss self: 0.0000; time: 9.86s
Val loss: 1.4025 score: 0.5899 time: 2.43s
Test loss: 1.3413 score: 0.6006 time: 2.43s
     INFO: Early stopping counter 10 of 20
Epoch 17/1000, LR 0.000299
Train loss: 0.0036;  Loss pred: 0.0036; Loss self: 0.0000; time: 6.61s
Val loss: 0.7319 score: 0.7107 time: 2.16s
Test loss: 0.6971 score: 0.7207 time: 2.09s
     INFO: Early stopping counter 11 of 20
Epoch 18/1000, LR 0.000299
Train loss: 0.0025;  Loss pred: 0.0025; Loss self: 0.0000; time: 6.17s
Val loss: 1.2822 score: 0.6219 time: 2.08s
Test loss: 1.2183 score: 0.6349 time: 2.07s
     INFO: Early stopping counter 12 of 20
Epoch 19/1000, LR 0.000299
Train loss: 0.0021;  Loss pred: 0.0021; Loss self: 0.0000; time: 6.18s
Val loss: 0.9575 score: 0.6828 time: 2.07s
Test loss: 0.9048 score: 0.6822 time: 2.07s
     INFO: Early stopping counter 13 of 20
Epoch 20/1000, LR 0.000299
Train loss: 0.0016;  Loss pred: 0.0016; Loss self: 0.0000; time: 6.50s
Val loss: 1.0967 score: 0.6604 time: 2.20s
Test loss: 1.0378 score: 0.6645 time: 2.23s
     INFO: Early stopping counter 14 of 20
Epoch 21/1000, LR 0.000299
Train loss: 0.0013;  Loss pred: 0.0013; Loss self: 0.0000; time: 6.72s
Val loss: 1.1231 score: 0.6615 time: 2.30s
Test loss: 1.0615 score: 0.6651 time: 2.25s
     INFO: Early stopping counter 15 of 20
Epoch 22/1000, LR 0.000299
Train loss: 0.0012;  Loss pred: 0.0012; Loss self: 0.0000; time: 6.14s
Val loss: 1.1418 score: 0.6609 time: 2.05s
Test loss: 1.0793 score: 0.6645 time: 2.31s
     INFO: Early stopping counter 16 of 20
Epoch 23/1000, LR 0.000299
Train loss: 0.0012;  Loss pred: 0.0012; Loss self: 0.0000; time: 6.64s
Val loss: 1.0029 score: 0.6852 time: 2.15s
Test loss: 0.9481 score: 0.6870 time: 2.15s
     INFO: Early stopping counter 17 of 20
Epoch 24/1000, LR 0.000299
Train loss: 0.0010;  Loss pred: 0.0010; Loss self: 0.0000; time: 6.84s
Val loss: 1.2604 score: 0.6467 time: 2.13s
Test loss: 1.1900 score: 0.6562 time: 2.20s
     INFO: Early stopping counter 18 of 20
Epoch 25/1000, LR 0.000299
Train loss: 0.0009;  Loss pred: 0.0009; Loss self: 0.0000; time: 6.49s
Val loss: 1.1736 score: 0.6716 time: 2.44s
Test loss: 1.1036 score: 0.6692 time: 2.57s
     INFO: Early stopping counter 19 of 20
Epoch 26/1000, LR 0.000299
Train loss: 0.0007;  Loss pred: 0.0007; Loss self: 0.0000; time: 6.69s
Val loss: 1.1507 score: 0.6757 time: 2.27s
Test loss: 1.0804 score: 0.6763 time: 2.15s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 005,   Train_Loss: 0.6406,   Val_Loss: 0.6470,   Val_Precision: 0.8785,   Val_Recall: 0.2225,   Val_accuracy: 0.3551,   Val_Score: 0.5959,   Val_Loss: 0.6470,   Test_Precision: 0.9367,   Test_Recall: 0.2450,   Test_accuracy: 0.3884,   Test_Score: 0.6142,   Test_loss: 0.6396


[2.4095491940388456, 2.027767034014687, 2.065021723974496, 2.3424211490200832, 2.2917687100125477, 2.0730484529631212, 2.026286744978279, 1.9422559960512444, 2.3286584289744496, 2.053686457918957, 2.1250873879762366, 1.9866675219964236, 2.2730366210453212, 1.9878991779405624, 1.8953465439844877, 1.9914596549933776, 1.9802538480143994, 2.0674888910725713, 2.152672305004671, 2.2591636129654944, 1.9829172029858455, 1.929170661023818, 1.8773399509955198, 2.1950947149889544, 3.4955117360223085, 2.1950646810000762, 2.683234693016857, 2.4510414879769087, 2.064569848938845, 1.969017751980573, 1.8920665349578485, 2.4591254480183125, 2.1053321650251746, 2.0557578469160944, 2.0813779460731894, 2.1821899010101333, 2.119481544010341, 2.035496974014677, 2.0410604750504717, 2.073365402990021, 2.092568786931224, 2.154107589973137, 2.2420547689544037, 2.0905618300894275, 2.0803155130706728, 2.7953705249819905, 2.80577417393215, 2.3852881899802014, 2.8181204290594906, 2.314088712912053, 2.65876299503725, 2.20105577504728, 2.238556309021078, 2.3222123610321432, 2.119520177016966, 2.127837108913809, 2.130009803106077, 2.2640218869782984, 2.1332332199672237, 2.137862449977547, 2.0858564280206338, 2.120841305004433, 2.2147887039463967, 2.29843924602028, 2.0977245999965817, 2.0790324909612536, 2.0784555700374767, 2.1985292669851333, 2.382955789915286, 2.084003802970983, 2.0689215071033686, 2.0718061410589144, 2.380916644935496, 3.455221202922985, 2.4205478039802983, 2.4892681119963527, 2.5618827890139073, 2.4309092190815136, 2.092716575949453, 2.0761931679444388, 2.078534899977967, 2.232988506904803, 2.258926776004955, 2.319979925057851, 2.1569541030330583, 2.2064446319127455, 2.571882884018123, 2.152242158073932]
[0.0014257687538691394, 0.0011998621503045484, 0.0012219063455470391, 0.0013860480171716469, 0.0013560761597707382, 0.0012266558893272907, 0.0011989862396321177, 0.001149263902988902, 0.0013779043958428697, 0.00121519908752601, 0.0012574481585658204, 0.0011755429124239193, 0.00134499208345877, 0.0011762717029234096, 0.0011215068307600519, 0.0011783784940789218, 0.0011717478390617748, 0.0012233662077352492, 0.0012737705946773202, 0.0013367832029381624, 0.001173323788749021, 0.0011415211011975253, 0.0011108520420091833, 0.0012988726124194996, 0.0020683501396581705, 0.0012988548408284476, 0.001587712836104649, 0.0014503204070869281, 0.0012216389638691392, 0.0011650992615269663, 0.0011195659970164784, 0.0014551038153954512, 0.001245758677529689, 0.0012164247614888132, 0.00123158458347526, 0.0012912366278166468, 0.0012541310911303794, 0.0012044360792986255, 0.0012077280917458413, 0.0012268434337219059, 0.001238206382799541, 0.0012746198757237496, 0.001326659626600239, 0.0012370188343724423, 0.0012309559248938893, 0.0016540653993976275, 0.0016602214046935798, 0.0014114131301657996, 0.0016675268811002903, 0.0013692832620781378, 0.00157323254144216, 0.0013023998668918815, 0.0013245895319651349, 0.0013740901544568895, 0.001254153950897613, 0.001259075212375035, 0.0012603608302402822, 0.001339657921288934, 0.0012622681774953986, 0.0012650073668506194, 0.0012342345727932745, 0.0012549356834345756, 0.0013105258603233116, 0.001360023222497207, 0.001241257159761291, 0.0012301967402137595, 0.001229855366886081, 0.0013009048917071795, 0.001410033011784193, 0.0012331383449532445, 0.0012242139095286204, 0.0012259207935259849, 0.0014088264171215953, 0.0020445095875283935, 0.001432276807088934, 0.0014729397112404454, 0.0015159069757478742, 0.001438407821941724, 0.0012382938319227532, 0.001228516667422745, 0.0012299023076792705, 0.0013212949744998836, 0.0013366430627248255, 0.0013727691864247637, 0.001276304202978141, 0.0013055885395933406, 0.0015218241917267, 0.0012735160698662319]
[701.3760101603282, 833.4290732866108, 818.393327479044, 721.4757263897596, 737.4217095366256, 815.2245537649594, 834.0379288312998, 870.1221689807625, 725.7397559779871, 822.9104269950302, 795.261413512703, 850.6707746959596, 743.4987999545757, 850.1437189338837, 891.657520554105, 848.6237698878313, 853.4259391514694, 817.4167258152775, 785.0707216658008, 748.0644563771187, 852.2796602173931, 876.0241041106808, 900.2098949121192, 769.89844149322, 483.4771351456319, 769.908975634391, 629.8368176284545, 689.5028126981756, 818.5724502702741, 858.2959692974237, 893.2032615003415, 687.2361885246186, 802.723688012334, 822.0812594903729, 811.9620961625069, 774.4513890462516, 797.3648106424627, 830.2640689594137, 828.0009439495954, 815.0999324879417, 807.6198070785545, 784.5476279209784, 753.7728441790662, 808.39512884807, 812.3767713991888, 604.571016577807, 602.3293020876127, 708.509775506006, 599.6904825547198, 730.3090804471801, 635.6339407290128, 767.8133462854652, 754.9508552407324, 727.7542865411557, 797.3502768813095, 794.2337281929863, 793.4235783964774, 746.4592147806385, 792.2246776308716, 790.5092303846533, 810.2187558535463, 796.8535863632039, 763.0524740300021, 735.281562445566, 805.6348292825254, 812.8781090951676, 813.1037412406788, 768.6957027947666, 709.2032538547764, 810.9390192046261, 816.8507090276786, 815.7133848132285, 709.8106536383115, 489.11484988872047, 698.1890616747991, 678.9144133793798, 659.6710853623779, 695.2131271436552, 807.5627724376662, 813.9897703609178, 813.0727080973787, 756.8332728870816, 748.1428871230896, 728.4545791739376, 783.5122674254226, 765.9380958655443, 657.1061266054489, 785.2276258320309]
Elapsed: 2.226659582758492~0.2779010761503998
Time per graph: 0.0013175500489695216~0.0001644385065978697
Speed: 768.591043394304~78.49892698910857
Total Time: 2.1530
best val loss: 0.6470444703243188 test_score: 0.6142

Testing...
Test loss: 0.6566 score: 0.7704 time: 2.12s
test Score 0.7704
Epoch Time List: [11.790671681170352, 10.907358843949623, 10.592530591995455, 11.80493331712205, 11.60099192801863, 10.789073272026144, 10.736601696000434, 10.099645330104977, 11.108201753930189, 10.750599425984547, 10.572006760980003, 10.50924979604315, 10.511360240983777, 10.449827456963249, 14.09864011395257, 10.258792735869065, 10.191800610045902, 10.387205867911689, 10.637225125916302, 10.8424864950357, 10.595081763109192, 10.107480062055402, 9.98678844794631, 10.52395715401508, 12.32137307699304, 13.29106897301972, 12.015290773939341, 11.39050128182862, 10.90966669213958, 12.190161732956767, 10.237523401970975, 11.006591119104996, 10.41995948087424, 10.035285145044327, 10.22265453694854, 10.22904157009907, 10.278158882865682, 10.049115550937131, 9.902666807873175, 9.925769771100022, 9.900576573098078, 10.422036159201525, 10.760369924944825, 10.025798697024584, 10.006072252057493, 14.450106390053406, 13.279808441060595, 15.128187246038578, 11.911566992872395, 12.380393478902988, 12.200751139083877, 12.757169116986915, 10.697330272989348, 10.919224217883311, 10.489024708978832, 10.109250352019444, 10.159498081193306, 10.512068181065843, 10.489352784119546, 10.187858409131877, 10.138341133017093, 10.155730176949874, 10.508160797995515, 11.261068894178607, 10.677857340080664, 10.190916280960664, 10.280097921029665, 10.448878389201127, 10.613994098966941, 10.326080388971604, 10.25190426397603, 10.133813380962238, 13.537347930949181, 13.267827930976637, 14.634480386972427, 13.551686143036932, 12.493632863042876, 14.714730795938522, 10.848906247993, 10.31916785996873, 10.322473848005757, 10.918988046003506, 11.268207657849416, 10.501413694000803, 10.93239144294057, 11.164375156979077, 11.493801458971575, 11.108430091058835]
Total Epoch List: [31, 31, 26]
Total Time List: [1.8924651920096949, 2.1213791409973055, 2.1529780940618366]
========================training times:1========================
========================k_idx:0========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: I2BGNNT
I2BGNN(
  (gcs): ModuleList(
    (0): GCNConv(14887, 64)
    (1): GCNConv(64, 64)
  )
  (bns): ModuleList(
    (0-1): 2 x BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  )
  (drops): ModuleList(
    (0-1): 2 x Dropout(p=0.2, inplace=False)
  )
  (lin1): Linear(in_features=64, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=64, bias=True)
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=64, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=64, out_features=2, bias=True)
  )
)
Total number of parameters: 969858
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7e36454348b0>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.6973;  Loss pred: 0.6973; Loss self: 0.0000; time: 6.70s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6979 score: 0.5000 time: 2.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6979 score: 0.5000 time: 2.13s
Epoch 2/1000, LR 0.000029
Train loss: 0.6959;  Loss pred: 0.6959; Loss self: 0.0000; time: 6.86s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6977 score: 0.5000 time: 2.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6977 score: 0.5000 time: 2.22s
Epoch 3/1000, LR 0.000059
Train loss: 0.6919;  Loss pred: 0.6919; Loss self: 0.0000; time: 6.81s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6939 score: 0.5000 time: 2.50s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6937 score: 0.5000 time: 2.29s
Epoch 4/1000, LR 0.000089
Train loss: 0.6857;  Loss pred: 0.6857; Loss self: 0.0000; time: 6.39s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6729 score: 0.5000 time: 2.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6722 score: 0.5000 time: 2.09s
Epoch 5/1000, LR 0.000119
Train loss: 0.6739;  Loss pred: 0.6739; Loss self: 0.0000; time: 6.34s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6482 score: 0.5000 time: 2.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6475 score: 0.5000 time: 3.10s
Epoch 6/1000, LR 0.000149
Train loss: 0.6557;  Loss pred: 0.6557; Loss self: 0.0000; time: 9.96s
Val loss: 0.6256 score: 0.5030 time: 2.67s
Test loss: 0.6251 score: 0.5012 time: 2.77s
Epoch 7/1000, LR 0.000179
Train loss: 0.6210;  Loss pred: 0.6210; Loss self: 0.0000; time: 8.21s
Val loss: 0.5933 score: 0.5213 time: 3.17s
Test loss: 0.5934 score: 0.5166 time: 2.46s
Epoch 8/1000, LR 0.000209
Train loss: 0.5621;  Loss pred: 0.5621; Loss self: 0.0000; time: 8.48s
Val loss: 0.5517 score: 0.6562 time: 2.71s
Test loss: 0.5527 score: 0.6456 time: 2.74s
Epoch 9/1000, LR 0.000239
Train loss: 0.4574;  Loss pred: 0.4574; Loss self: 0.0000; time: 7.92s
Val loss: 0.5113 score: 0.7396 time: 3.33s
Test loss: 0.5176 score: 0.7296 time: 2.46s
Epoch 10/1000, LR 0.000269
Train loss: 0.2954;  Loss pred: 0.2954; Loss self: 0.0000; time: 7.27s
Val loss: 0.4320 score: 0.8373 time: 2.33s
Test loss: 0.4476 score: 0.8083 time: 2.28s
Epoch 11/1000, LR 0.000299
Train loss: 0.1171;  Loss pred: 0.1171; Loss self: 0.0000; time: 8.41s
Val loss: 0.4084 score: 0.8142 time: 5.32s
Test loss: 0.4310 score: 0.7953 time: 4.02s
Epoch 12/1000, LR 0.000299
Train loss: 0.0329;  Loss pred: 0.0329; Loss self: 0.0000; time: 6.33s
Val loss: 0.5352 score: 0.7828 time: 2.08s
Test loss: 0.5766 score: 0.7621 time: 2.09s
     INFO: Early stopping counter 1 of 20
Epoch 13/1000, LR 0.000299
Train loss: 0.0122;  Loss pred: 0.0122; Loss self: 0.0000; time: 6.76s
Val loss: 0.4441 score: 0.8290 time: 2.17s
Test loss: 0.4817 score: 0.8112 time: 2.09s
     INFO: Early stopping counter 2 of 20
Epoch 14/1000, LR 0.000299
Train loss: 0.0069;  Loss pred: 0.0069; Loss self: 0.0000; time: 6.40s
Val loss: 0.4703 score: 0.8166 time: 2.19s
Test loss: 0.5105 score: 0.8030 time: 2.11s
     INFO: Early stopping counter 3 of 20
Epoch 15/1000, LR 0.000299
Train loss: 0.0043;  Loss pred: 0.0043; Loss self: 0.0000; time: 6.53s
Val loss: 0.5361 score: 0.7947 time: 2.24s
Test loss: 0.5820 score: 0.7787 time: 2.14s
     INFO: Early stopping counter 4 of 20
Epoch 16/1000, LR 0.000299
Train loss: 0.0032;  Loss pred: 0.0032; Loss self: 0.0000; time: 6.61s
Val loss: 0.4795 score: 0.8207 time: 2.28s
Test loss: 0.5228 score: 0.8089 time: 2.34s
     INFO: Early stopping counter 5 of 20
Epoch 17/1000, LR 0.000299
Train loss: 0.0024;  Loss pred: 0.0024; Loss self: 0.0000; time: 6.50s
Val loss: 0.5672 score: 0.7964 time: 2.28s
Test loss: 0.6172 score: 0.7834 time: 2.24s
     INFO: Early stopping counter 6 of 20
Epoch 18/1000, LR 0.000299
Train loss: 0.0018;  Loss pred: 0.0018; Loss self: 0.0000; time: 6.78s
Val loss: 0.5465 score: 0.8089 time: 2.16s
Test loss: 0.5958 score: 0.7982 time: 2.18s
     INFO: Early stopping counter 7 of 20
Epoch 19/1000, LR 0.000299
Train loss: 0.0016;  Loss pred: 0.0016; Loss self: 0.0000; time: 6.53s
Val loss: 0.6484 score: 0.7870 time: 2.32s
Test loss: 0.7056 score: 0.7698 time: 2.29s
     INFO: Early stopping counter 8 of 20
Epoch 20/1000, LR 0.000299
Train loss: 0.0013;  Loss pred: 0.0013; Loss self: 0.0000; time: 6.48s
Val loss: 0.5723 score: 0.8142 time: 2.17s
Test loss: 0.6265 score: 0.8024 time: 2.03s
     INFO: Early stopping counter 9 of 20
Epoch 21/1000, LR 0.000299
Train loss: 0.0011;  Loss pred: 0.0011; Loss self: 0.0000; time: 6.22s
Val loss: 0.6237 score: 0.8030 time: 2.25s
Test loss: 0.6816 score: 0.7876 time: 2.05s
     INFO: Early stopping counter 10 of 20
Epoch 22/1000, LR 0.000299
Train loss: 0.0009;  Loss pred: 0.0009; Loss self: 0.0000; time: 6.66s
Val loss: 0.6085 score: 0.8071 time: 2.19s
Test loss: 0.6656 score: 0.7947 time: 2.09s
     INFO: Early stopping counter 11 of 20
Epoch 23/1000, LR 0.000299
Train loss: 0.0007;  Loss pred: 0.0007; Loss self: 0.0000; time: 6.49s
Val loss: 0.6187 score: 0.8059 time: 2.29s
Test loss: 0.6769 score: 0.7935 time: 2.17s
     INFO: Early stopping counter 12 of 20
Epoch 24/1000, LR 0.000299
Train loss: 0.0008;  Loss pred: 0.0008; Loss self: 0.0000; time: 6.54s
Val loss: 0.5916 score: 0.8166 time: 2.25s
Test loss: 0.6473 score: 0.8006 time: 2.10s
     INFO: Early stopping counter 13 of 20
Epoch 25/1000, LR 0.000299
Train loss: 0.0006;  Loss pred: 0.0006; Loss self: 0.0000; time: 6.07s
Val loss: 0.6473 score: 0.8041 time: 2.06s
Test loss: 0.7076 score: 0.7888 time: 1.98s
     INFO: Early stopping counter 14 of 20
Epoch 26/1000, LR 0.000299
Train loss: 0.0005;  Loss pred: 0.0005; Loss self: 0.0000; time: 6.09s
Val loss: 0.6460 score: 0.8065 time: 2.10s
Test loss: 0.7067 score: 0.7953 time: 2.01s
     INFO: Early stopping counter 15 of 20
Epoch 27/1000, LR 0.000299
Train loss: 0.0005;  Loss pred: 0.0005; Loss self: 0.0000; time: 6.29s
Val loss: 0.6471 score: 0.8077 time: 2.19s
Test loss: 0.7083 score: 0.7959 time: 2.22s
     INFO: Early stopping counter 16 of 20
Epoch 28/1000, LR 0.000299
Train loss: 0.0004;  Loss pred: 0.0004; Loss self: 0.0000; time: 6.49s
Val loss: 0.6672 score: 0.8047 time: 2.20s
Test loss: 0.7301 score: 0.7923 time: 2.18s
     INFO: Early stopping counter 17 of 20
Epoch 29/1000, LR 0.000299
Train loss: 0.0004;  Loss pred: 0.0004; Loss self: 0.0000; time: 6.40s
Val loss: 0.6466 score: 0.8118 time: 2.17s
Test loss: 0.7081 score: 0.7988 time: 2.23s
     INFO: Early stopping counter 18 of 20
Epoch 30/1000, LR 0.000299
Train loss: 0.0003;  Loss pred: 0.0003; Loss self: 0.0000; time: 6.17s
Val loss: 0.6594 score: 0.8118 time: 2.09s
Test loss: 0.7220 score: 0.7976 time: 2.00s
     INFO: Early stopping counter 19 of 20
Epoch 31/1000, LR 0.000299
Train loss: 0.0003;  Loss pred: 0.0003; Loss self: 0.0000; time: 6.26s
Val loss: 0.6799 score: 0.8059 time: 2.07s
Test loss: 0.7442 score: 0.7959 time: 2.01s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 010,   Train_Loss: 0.1171,   Val_Loss: 0.4084,   Val_Precision: 0.9854,   Val_Recall: 0.6379,   Val_accuracy: 0.7744,   Val_Score: 0.8142,   Val_Loss: 0.4084,   Test_Precision: 0.9921,   Test_Recall: 0.5953,   Test_accuracy: 0.7441,   Test_Score: 0.7953,   Test_loss: 0.4310


[2.140857676975429, 2.2274017119780183, 2.29443091689609, 2.094437174964696, 3.105155735043809, 2.7763828970491886, 2.464136164984666, 2.742967189056799, 2.469468976953067, 2.2858910689828917, 4.02115846297238, 2.099313258891925, 2.0908621759153903, 2.112115183030255, 2.147841055993922, 2.349463502992876, 2.2435665350640193, 2.1908043950097635, 2.2959469739580527, 2.037237133947201, 2.0595884129870683, 2.098642796976492, 2.1796783920144662, 2.1089960619574413, 1.9808005719678476, 2.0164724700152874, 2.2284716210560873, 2.1824544260744005, 2.2320745970355347, 2.002326796995476, 2.019772500032559]
[0.0012667796905180054, 0.001317989178685218, 0.0013576514301160295, 0.0012393119378489327, 0.0018373702574223721, 0.001642830116597153, 0.001458068736677317, 0.0016230575083176324, 0.001461224246717791, 0.00135259826567035, 0.0023793837059008166, 0.0012421971946106067, 0.001237196553796089, 0.0012497722976510385, 0.001270911867452025, 0.001390215090528329, 0.0013275541627597747, 0.0012963339615442388, 0.001358548505300623, 0.0012054657597320717, 0.0012186913686314013, 0.001241800471583723, 0.0012897505278192107, 0.0012479266638801428, 0.0011720713443596732, 0.0011931789763404068, 0.0013186222609799334, 0.001291393151523314, 0.0013207541994293105, 0.0011848087556186248, 0.0011951316568239994]
[789.403246266985, 758.7315709204582, 736.5660859757917, 806.8993523419897, 544.2561160225212, 608.7056658489632, 685.8387227194958, 616.1211139317807, 684.3576557439454, 739.3178191784818, 420.27689671069993, 805.0251637490386, 808.2790054108226, 800.1457560545322, 786.8366214919687, 719.3131529164785, 753.2649349094416, 771.4061574139157, 736.0797175060876, 829.5548769649511, 820.5522954700227, 805.2823483990595, 775.343741623321, 801.3291397194196, 853.1903836846388, 838.0972342197102, 758.3672971339424, 774.3575214259192, 757.1431538374768, 844.0180706445484, 836.7278987968976]
Elapsed: 2.2999586076701~0.3981372378295477
Time per graph: 0.0013609222530592312~0.000235584164396182
Speed: 750.4770553881712~92.89947075823606
Total Time: 2.0202
best val loss: 0.40840191433107004 test_score: 0.7953

Testing...
Test loss: 0.4476 score: 0.8083 time: 2.01s
test Score 0.8083
Epoch Time List: [10.998816858045757, 11.274765963898972, 11.594147160067223, 10.629046493093483, 11.60124322399497, 15.398501255782321, 13.83067437412683, 13.924343895050697, 13.713329028105363, 11.883464321959764, 17.744239912950434, 10.499536670045927, 11.013335558003746, 10.696330915088765, 10.903166078962386, 11.236046041944064, 11.015943237114698, 11.121820542030036, 11.138006528955884, 10.688983661006205, 10.525015530991368, 10.94429292390123, 10.9512841028627, 10.893880854011513, 10.10809383506421, 10.196990794851445, 10.711114910896868, 10.874147452996112, 10.791069471975788, 10.259492499986663, 10.350437518791296]
Total Epoch List: [31]
Total Time List: [2.0201744419755414]
========================k_idx:1========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: I2BGNNT
I2BGNN(
  (gcs): ModuleList(
    (0): GCNConv(14887, 64)
    (1): GCNConv(64, 64)
  )
  (bns): ModuleList(
    (0-1): 2 x BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  )
  (drops): ModuleList(
    (0-1): 2 x Dropout(p=0.2, inplace=False)
  )
  (lin1): Linear(in_features=64, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=64, bias=True)
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=64, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=64, out_features=2, bias=True)
  )
)
Total number of parameters: 969858
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7e3645434a00>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.7002;  Loss pred: 0.7002; Loss self: 0.0000; time: 11.09s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6962 score: 0.5000 time: 2.85s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6962 score: 0.5000 time: 3.39s
Epoch 2/1000, LR 0.000029
Train loss: 0.6959;  Loss pred: 0.6959; Loss self: 0.0000; time: 8.41s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6955 score: 0.5000 time: 3.09s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6955 score: 0.5000 time: 3.76s
Epoch 3/1000, LR 0.000059
Train loss: 0.6909;  Loss pred: 0.6909; Loss self: 0.0000; time: 7.99s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6916 score: 0.5000 time: 2.95s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6917 score: 0.5000 time: 2.78s
Epoch 4/1000, LR 0.000089
Train loss: 0.6826;  Loss pred: 0.6826; Loss self: 0.0000; time: 8.94s
Val loss: 0.6751 score: 0.6018 time: 2.47s
Test loss: 0.6753 score: 0.5893 time: 2.57s
Epoch 5/1000, LR 0.000119
Train loss: 0.6695;  Loss pred: 0.6695; Loss self: 0.0000; time: 7.52s
Val loss: 0.6509 score: 0.6858 time: 3.02s
Test loss: 0.6516 score: 0.6799 time: 2.92s
Epoch 6/1000, LR 0.000149
Train loss: 0.6430;  Loss pred: 0.6430; Loss self: 0.0000; time: 11.50s
Val loss: 0.6143 score: 0.7959 time: 2.46s
Test loss: 0.6166 score: 0.7970 time: 2.56s
Epoch 7/1000, LR 0.000179
Train loss: 0.5968;  Loss pred: 0.5968; Loss self: 0.0000; time: 6.19s
Val loss: 0.5648 score: 0.8237 time: 2.18s
Test loss: 0.5695 score: 0.8225 time: 2.18s
Epoch 8/1000, LR 0.000209
Train loss: 0.5269;  Loss pred: 0.5269; Loss self: 0.0000; time: 6.54s
Val loss: 0.5029 score: 0.8462 time: 2.16s
Test loss: 0.5105 score: 0.8396 time: 2.22s
Epoch 9/1000, LR 0.000239
Train loss: 0.4076;  Loss pred: 0.4076; Loss self: 0.0000; time: 6.26s
Val loss: 0.4428 score: 0.8740 time: 2.46s
Test loss: 0.4550 score: 0.8722 time: 2.43s
Epoch 10/1000, LR 0.000269
Train loss: 0.2376;  Loss pred: 0.2376; Loss self: 0.0000; time: 6.50s
Val loss: 0.3380 score: 0.8947 time: 2.26s
Test loss: 0.3533 score: 0.8746 time: 2.11s
Epoch 11/1000, LR 0.000299
Train loss: 0.0802;  Loss pred: 0.0802; Loss self: 0.0000; time: 6.27s
Val loss: 0.3031 score: 0.8882 time: 2.26s
Test loss: 0.3159 score: 0.8757 time: 2.25s
Epoch 12/1000, LR 0.000299
Train loss: 0.0269;  Loss pred: 0.0269; Loss self: 0.0000; time: 6.46s
Val loss: 0.3835 score: 0.8337 time: 2.26s
Test loss: 0.4049 score: 0.8178 time: 2.29s
     INFO: Early stopping counter 1 of 20
Epoch 13/1000, LR 0.000299
Train loss: 0.0121;  Loss pred: 0.0121; Loss self: 0.0000; time: 6.32s
Val loss: 0.3606 score: 0.8533 time: 2.36s
Test loss: 0.3807 score: 0.8373 time: 2.29s
     INFO: Early stopping counter 2 of 20
Epoch 14/1000, LR 0.000299
Train loss: 0.0071;  Loss pred: 0.0071; Loss self: 0.0000; time: 6.28s
Val loss: 0.3743 score: 0.8479 time: 2.17s
Test loss: 0.3956 score: 0.8331 time: 2.27s
     INFO: Early stopping counter 3 of 20
Epoch 15/1000, LR 0.000299
Train loss: 0.0054;  Loss pred: 0.0054; Loss self: 0.0000; time: 6.17s
Val loss: 0.4326 score: 0.8249 time: 2.18s
Test loss: 0.4583 score: 0.8059 time: 2.20s
     INFO: Early stopping counter 4 of 20
Epoch 16/1000, LR 0.000299
Train loss: 0.0037;  Loss pred: 0.0037; Loss self: 0.0000; time: 6.34s
Val loss: 0.4125 score: 0.8414 time: 2.30s
Test loss: 0.4352 score: 0.8207 time: 2.19s
     INFO: Early stopping counter 5 of 20
Epoch 17/1000, LR 0.000299
Train loss: 0.0029;  Loss pred: 0.0029; Loss self: 0.0000; time: 5.98s
Val loss: 0.4387 score: 0.8290 time: 2.12s
Test loss: 0.4635 score: 0.8118 time: 2.14s
     INFO: Early stopping counter 6 of 20
Epoch 18/1000, LR 0.000299
Train loss: 0.0023;  Loss pred: 0.0023; Loss self: 0.0000; time: 6.00s
Val loss: 0.4330 score: 0.8385 time: 2.11s
Test loss: 0.4567 score: 0.8219 time: 2.14s
     INFO: Early stopping counter 7 of 20
Epoch 19/1000, LR 0.000299
Train loss: 0.0017;  Loss pred: 0.0017; Loss self: 0.0000; time: 5.99s
Val loss: 0.4622 score: 0.8278 time: 2.10s
Test loss: 0.4881 score: 0.8107 time: 2.12s
     INFO: Early stopping counter 8 of 20
Epoch 20/1000, LR 0.000299
Train loss: 0.0015;  Loss pred: 0.0015; Loss self: 0.0000; time: 5.94s
Val loss: 0.4886 score: 0.8260 time: 2.11s
Test loss: 0.5164 score: 0.8041 time: 2.12s
     INFO: Early stopping counter 9 of 20
Epoch 21/1000, LR 0.000299
Train loss: 0.0014;  Loss pred: 0.0014; Loss self: 0.0000; time: 5.95s
Val loss: 0.4523 score: 0.8432 time: 2.08s
Test loss: 0.4755 score: 0.8243 time: 2.20s
     INFO: Early stopping counter 10 of 20
Epoch 22/1000, LR 0.000299
Train loss: 0.0011;  Loss pred: 0.0011; Loss self: 0.0000; time: 6.14s
Val loss: 0.4821 score: 0.8314 time: 2.22s
Test loss: 0.5077 score: 0.8148 time: 2.32s
     INFO: Early stopping counter 11 of 20
Epoch 23/1000, LR 0.000299
Train loss: 0.0009;  Loss pred: 0.0009; Loss self: 0.0000; time: 6.08s
Val loss: 0.5027 score: 0.8290 time: 2.14s
Test loss: 0.5304 score: 0.8124 time: 2.16s
     INFO: Early stopping counter 12 of 20
Epoch 24/1000, LR 0.000299
Train loss: 0.0009;  Loss pred: 0.0009; Loss self: 0.0000; time: 6.07s
Val loss: 0.5137 score: 0.8290 time: 2.13s
Test loss: 0.5428 score: 0.8124 time: 2.23s
     INFO: Early stopping counter 13 of 20
Epoch 25/1000, LR 0.000299
Train loss: 0.0008;  Loss pred: 0.0008; Loss self: 0.0000; time: 6.03s
Val loss: 0.5010 score: 0.8367 time: 2.19s
Test loss: 0.5268 score: 0.8219 time: 2.14s
     INFO: Early stopping counter 14 of 20
Epoch 26/1000, LR 0.000299
Train loss: 0.0007;  Loss pred: 0.0007; Loss self: 0.0000; time: 6.06s
Val loss: 0.4992 score: 0.8385 time: 2.10s
Test loss: 0.5241 score: 0.8254 time: 2.11s
     INFO: Early stopping counter 15 of 20
Epoch 27/1000, LR 0.000299
Train loss: 0.0006;  Loss pred: 0.0006; Loss self: 0.0000; time: 6.00s
Val loss: 0.5117 score: 0.8361 time: 2.11s
Test loss: 0.5378 score: 0.8225 time: 2.14s
     INFO: Early stopping counter 16 of 20
Epoch 28/1000, LR 0.000299
Train loss: 0.0006;  Loss pred: 0.0006; Loss self: 0.0000; time: 6.34s
Val loss: 0.5185 score: 0.8391 time: 2.37s
Test loss: 0.5481 score: 0.8225 time: 2.17s
     INFO: Early stopping counter 17 of 20
Epoch 29/1000, LR 0.000299
Train loss: 0.0005;  Loss pred: 0.0005; Loss self: 0.0000; time: 9.61s
Val loss: 0.5196 score: 0.8396 time: 2.74s
Test loss: 0.5482 score: 0.8225 time: 2.49s
     INFO: Early stopping counter 18 of 20
Epoch 30/1000, LR 0.000299
Train loss: 0.0004;  Loss pred: 0.0004; Loss self: 0.0000; time: 7.03s
Val loss: 0.5082 score: 0.8479 time: 2.30s
Test loss: 0.5353 score: 0.8284 time: 2.34s
     INFO: Early stopping counter 19 of 20
Epoch 31/1000, LR 0.000299
Train loss: 0.0004;  Loss pred: 0.0004; Loss self: 0.0000; time: 8.19s
Val loss: 0.5336 score: 0.8402 time: 2.37s
Test loss: 0.5620 score: 0.8219 time: 2.65s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 010,   Train_Loss: 0.0802,   Val_Loss: 0.3031,   Val_Precision: 0.9955,   Val_Recall: 0.7799,   Val_accuracy: 0.8746,   Val_Score: 0.8882,   Val_Loss: 0.3031,   Test_Precision: 0.9877,   Test_Recall: 0.7609,   Test_accuracy: 0.8596,   Test_Score: 0.8757,   Test_loss: 0.3159


[2.140857676975429, 2.2274017119780183, 2.29443091689609, 2.094437174964696, 3.105155735043809, 2.7763828970491886, 2.464136164984666, 2.742967189056799, 2.469468976953067, 2.2858910689828917, 4.02115846297238, 2.099313258891925, 2.0908621759153903, 2.112115183030255, 2.147841055993922, 2.349463502992876, 2.2435665350640193, 2.1908043950097635, 2.2959469739580527, 2.037237133947201, 2.0595884129870683, 2.098642796976492, 2.1796783920144662, 2.1089960619574413, 1.9808005719678476, 2.0164724700152874, 2.2284716210560873, 2.1824544260744005, 2.2320745970355347, 2.002326796995476, 2.019772500032559, 3.39758708991576, 3.762358289095573, 2.780855445889756, 2.5741751269670203, 2.921798688941635, 2.5686420359415933, 2.1862453270005062, 2.22080479899887, 2.433067837031558, 2.115481865941547, 2.259185596019961, 2.297510837088339, 2.3001070549944416, 2.272767869057134, 2.2044090079143643, 2.2005922810640186, 2.1417592759244144, 2.141071559046395, 2.122739002923481, 2.1299087849911302, 2.209513673908077, 2.327609211904928, 2.1696139900013804, 2.237867301912047, 2.141589554026723, 2.1178092160262167, 2.1405937939416617, 2.17101852002088, 2.493544989032671, 2.3420545830158517, 2.6512845010729507]
[0.0012667796905180054, 0.001317989178685218, 0.0013576514301160295, 0.0012393119378489327, 0.0018373702574223721, 0.001642830116597153, 0.001458068736677317, 0.0016230575083176324, 0.001461224246717791, 0.00135259826567035, 0.0023793837059008166, 0.0012421971946106067, 0.001237196553796089, 0.0012497722976510385, 0.001270911867452025, 0.001390215090528329, 0.0013275541627597747, 0.0012963339615442388, 0.001358548505300623, 0.0012054657597320717, 0.0012186913686314013, 0.001241800471583723, 0.0012897505278192107, 0.0012479266638801428, 0.0011720713443596732, 0.0011931789763404068, 0.0013186222609799334, 0.001291393151523314, 0.0013207541994293105, 0.0011848087556186248, 0.0011951316568239994, 0.0020104065620803312, 0.002226247508340576, 0.0016454765952010392, 0.0015231805485011955, 0.00172887496387079, 0.0015199065301429546, 0.00129363628816598, 0.001314085679880988, 0.001439685110669561, 0.0012517644177168917, 0.0013367962106626988, 0.0013594738680996088, 0.0013610100917126873, 0.0013448330586136887, 0.001304384028351695, 0.0013021256100970524, 0.0012673131810203636, 0.0012669062479564468, 0.0012560585816115274, 0.0012603010562077695, 0.00130740454077401, 0.0013772835573401942, 0.0012837952603558464, 0.0013241818354509153, 0.0012672127538619663, 0.001253141547944507, 0.0012666235467110425, 0.0012846263432076213, 0.0014754704077116396, 0.0013858311142105632, 0.001568807397084586]
[789.403246266985, 758.7315709204582, 736.5660859757917, 806.8993523419897, 544.2561160225212, 608.7056658489632, 685.8387227194958, 616.1211139317807, 684.3576557439454, 739.3178191784818, 420.27689671069993, 805.0251637490386, 808.2790054108226, 800.1457560545322, 786.8366214919687, 719.3131529164785, 753.2649349094416, 771.4061574139157, 736.0797175060876, 829.5548769649511, 820.5522954700227, 805.2823483990595, 775.343741623321, 801.3291397194196, 853.1903836846388, 838.0972342197102, 758.3672971339424, 774.3575214259192, 757.1431538374768, 844.0180706445484, 836.7278987968976, 497.41182647415286, 449.1863533832277, 607.7266628504207, 656.5209889162494, 578.4108283696197, 657.9351954662272, 773.0148026519298, 760.9853872622417, 694.5963340100984, 798.8723643574341, 748.0571773196929, 735.5786848612892, 734.7484093535307, 743.5867177676631, 766.6453883705288, 767.9750649597208, 789.0709376153263, 789.3243889301409, 796.1412108000542, 793.4612091883728, 764.8741983165972, 726.066897895156, 778.9404049698835, 755.1832937350906, 789.1334718281466, 797.9944497413497, 789.5005604440513, 778.436473210623, 677.7499533527997, 721.5886479570411, 637.4268771669251]
Elapsed: 2.344069095925548~0.3893276073696613
Time per graph: 0.0013870231336837566~0.00023037136530749196
Speed: 735.8215141703048~91.32700104574792
Total Time: 2.6518
best val loss: 0.30309748268691744 test_score: 0.8757

Testing...
Test loss: 0.3533 score: 0.8746 time: 2.46s
test Score 0.8746
Epoch Time List: [10.998816858045757, 11.274765963898972, 11.594147160067223, 10.629046493093483, 11.60124322399497, 15.398501255782321, 13.83067437412683, 13.924343895050697, 13.713329028105363, 11.883464321959764, 17.744239912950434, 10.499536670045927, 11.013335558003746, 10.696330915088765, 10.903166078962386, 11.236046041944064, 11.015943237114698, 11.121820542030036, 11.138006528955884, 10.688983661006205, 10.525015530991368, 10.94429292390123, 10.9512841028627, 10.893880854011513, 10.10809383506421, 10.196990794851445, 10.711114910896868, 10.874147452996112, 10.791069471975788, 10.259492499986663, 10.350437518791296, 17.326730969944037, 15.260092514217831, 13.72170140594244, 13.982024040888064, 13.449628110975027, 16.516328014899045, 10.55116016487591, 10.91212454403285, 11.15032859495841, 10.87329966481775, 10.787493768963031, 11.007420439040288, 10.980951666017063, 10.715309738996439, 10.547809788025916, 10.835833378136158, 10.232298900955357, 10.24752240802627, 10.207615635008551, 10.175302622956224, 10.239562032977119, 10.681900139898062, 10.38904963596724, 10.434742123936303, 10.348219939973205, 10.266357265878469, 10.24094153707847, 10.871603679959662, 14.838681461988017, 11.66372811596375, 13.204979041940533]
Total Epoch List: [31, 31]
Total Time List: [2.0201744419755414, 2.6518087020376697]
========================k_idx:2========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: I2BGNNT
I2BGNN(
  (gcs): ModuleList(
    (0): GCNConv(14887, 64)
    (1): GCNConv(64, 64)
  )
  (bns): ModuleList(
    (0-1): 2 x BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  )
  (drops): ModuleList(
    (0-1): 2 x Dropout(p=0.2, inplace=False)
  )
  (lin1): Linear(in_features=64, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=64, bias=True)
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=64, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=64, out_features=2, bias=True)
  )
)
Total number of parameters: 969858
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7e363c59f070>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.6958;  Loss pred: 0.6958; Loss self: 0.0000; time: 7.42s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6935 score: 0.5000 time: 2.56s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6935 score: 0.5000 time: 2.19s
Epoch 2/1000, LR 0.000029
Train loss: 0.6940;  Loss pred: 0.6940; Loss self: 0.0000; time: 6.71s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6935 score: 0.5000 time: 2.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6935 score: 0.5000 time: 2.25s
     INFO: Early stopping counter 1 of 20
Epoch 3/1000, LR 0.000059
Train loss: 0.6915;  Loss pred: 0.6915; Loss self: 0.0000; time: 6.87s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6921 score: 0.5000 time: 2.44s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6920 score: 0.5000 time: 2.62s
Epoch 4/1000, LR 0.000089
Train loss: 0.6874;  Loss pred: 0.6874; Loss self: 0.0000; time: 6.69s
Val loss: 0.6814 score: 0.5000 time: 2.32s
Test loss: 0.6808 score: 0.4959 time: 2.21s
Epoch 5/1000, LR 0.000119
Train loss: 0.6802;  Loss pred: 0.6802; Loss self: 0.0000; time: 6.87s
Val loss: 0.6677 score: 0.5178 time: 2.31s
Test loss: 0.6661 score: 0.5107 time: 2.25s
Epoch 6/1000, LR 0.000149
Train loss: 0.6650;  Loss pred: 0.6650; Loss self: 0.0000; time: 6.87s
Val loss: 0.6474 score: 0.5550 time: 2.29s
Test loss: 0.6437 score: 0.5515 time: 2.43s
Epoch 7/1000, LR 0.000179
Train loss: 0.6370;  Loss pred: 0.6370; Loss self: 0.0000; time: 6.66s
Val loss: 0.6220 score: 0.5746 time: 2.32s
Test loss: 0.6140 score: 0.5734 time: 2.27s
Epoch 8/1000, LR 0.000209
Train loss: 0.5835;  Loss pred: 0.5835; Loss self: 0.0000; time: 6.65s
Val loss: 0.6041 score: 0.5669 time: 2.10s
Test loss: 0.5896 score: 0.5728 time: 2.12s
Epoch 9/1000, LR 0.000239
Train loss: 0.4858;  Loss pred: 0.4858; Loss self: 0.0000; time: 6.63s
Val loss: 0.6371 score: 0.5645 time: 2.20s
Test loss: 0.6095 score: 0.5698 time: 2.29s
     INFO: Early stopping counter 1 of 20
Epoch 10/1000, LR 0.000269
Train loss: 0.3085;  Loss pred: 0.3085; Loss self: 0.0000; time: 6.83s
Val loss: 0.9957 score: 0.5438 time: 2.41s
Test loss: 0.9407 score: 0.5485 time: 2.43s
     INFO: Early stopping counter 2 of 20
Epoch 11/1000, LR 0.000299
Train loss: 0.1162;  Loss pred: 0.1162; Loss self: 0.0000; time: 6.61s
Val loss: 1.1495 score: 0.5876 time: 2.10s
Test loss: 1.0689 score: 0.5976 time: 2.32s
     INFO: Early stopping counter 3 of 20
Epoch 12/1000, LR 0.000299
Train loss: 0.0323;  Loss pred: 0.0323; Loss self: 0.0000; time: 6.36s
Val loss: 1.0721 score: 0.6172 time: 2.12s
Test loss: 0.9858 score: 0.6373 time: 2.16s
     INFO: Early stopping counter 4 of 20
Epoch 13/1000, LR 0.000299
Train loss: 0.0137;  Loss pred: 0.0137; Loss self: 0.0000; time: 6.69s
Val loss: 1.1889 score: 0.6160 time: 2.13s
Test loss: 1.0886 score: 0.6391 time: 2.14s
     INFO: Early stopping counter 5 of 20
Epoch 14/1000, LR 0.000299
Train loss: 0.0087;  Loss pred: 0.0087; Loss self: 0.0000; time: 6.63s
Val loss: 1.1306 score: 0.6302 time: 2.39s
Test loss: 1.0340 score: 0.6538 time: 2.34s
     INFO: Early stopping counter 6 of 20
Epoch 15/1000, LR 0.000299
Train loss: 0.0057;  Loss pred: 0.0057; Loss self: 0.0000; time: 6.40s
Val loss: 1.2543 score: 0.6278 time: 2.23s
Test loss: 1.1467 score: 0.6509 time: 2.25s
     INFO: Early stopping counter 7 of 20
Epoch 16/1000, LR 0.000299
Train loss: 0.0046;  Loss pred: 0.0046; Loss self: 0.0000; time: 6.56s
Val loss: 1.1203 score: 0.6538 time: 2.19s
Test loss: 1.0132 score: 0.6751 time: 2.13s
     INFO: Early stopping counter 8 of 20
Epoch 17/1000, LR 0.000299
Train loss: 0.0038;  Loss pred: 0.0038; Loss self: 0.0000; time: 6.37s
Val loss: 1.4880 score: 0.6130 time: 2.13s
Test loss: 1.3659 score: 0.6367 time: 2.14s
     INFO: Early stopping counter 9 of 20
Epoch 18/1000, LR 0.000299
Train loss: 0.0028;  Loss pred: 0.0028; Loss self: 0.0000; time: 6.34s
Val loss: 1.1587 score: 0.6556 time: 2.12s
Test loss: 1.0514 score: 0.6757 time: 2.16s
     INFO: Early stopping counter 10 of 20
Epoch 19/1000, LR 0.000299
Train loss: 0.0022;  Loss pred: 0.0022; Loss self: 0.0000; time: 6.35s
Val loss: 1.4740 score: 0.6225 time: 2.17s
Test loss: 1.3476 score: 0.6479 time: 2.17s
     INFO: Early stopping counter 11 of 20
Epoch 20/1000, LR 0.000299
Train loss: 0.0020;  Loss pred: 0.0020; Loss self: 0.0000; time: 6.46s
Val loss: 1.2647 score: 0.6550 time: 2.09s
Test loss: 1.1490 score: 0.6728 time: 2.09s
     INFO: Early stopping counter 12 of 20
Epoch 21/1000, LR 0.000299
Train loss: 0.0016;  Loss pred: 0.0016; Loss self: 0.0000; time: 6.21s
Val loss: 1.4403 score: 0.6331 time: 2.22s
Test loss: 1.3119 score: 0.6556 time: 2.24s
     INFO: Early stopping counter 13 of 20
Epoch 22/1000, LR 0.000299
Train loss: 0.0014;  Loss pred: 0.0014; Loss self: 0.0000; time: 6.56s
Val loss: 1.3945 score: 0.6432 time: 3.45s
Test loss: 1.2684 score: 0.6663 time: 3.74s
     INFO: Early stopping counter 14 of 20
Epoch 23/1000, LR 0.000299
Train loss: 0.0012;  Loss pred: 0.0012; Loss self: 0.0000; time: 7.58s
Val loss: 1.3744 score: 0.6527 time: 2.43s
Test loss: 1.2494 score: 0.6728 time: 2.58s
     INFO: Early stopping counter 15 of 20
Epoch 24/1000, LR 0.000299
Train loss: 0.0010;  Loss pred: 0.0010; Loss self: 0.0000; time: 8.31s
Val loss: 1.4676 score: 0.6408 time: 3.18s
Test loss: 1.3368 score: 0.6639 time: 2.44s
     INFO: Early stopping counter 16 of 20
Epoch 25/1000, LR 0.000299
Train loss: 0.0011;  Loss pred: 0.0011; Loss self: 0.0000; time: 7.65s
Val loss: 1.4107 score: 0.6550 time: 2.42s
Test loss: 1.2780 score: 0.6757 time: 2.58s
     INFO: Early stopping counter 17 of 20
Epoch 26/1000, LR 0.000299
Train loss: 0.0008;  Loss pred: 0.0008; Loss self: 0.0000; time: 11.03s
Val loss: 1.4976 score: 0.6491 time: 2.76s
Test loss: 1.3579 score: 0.6692 time: 2.63s
     INFO: Early stopping counter 18 of 20
Epoch 27/1000, LR 0.000299
Train loss: 0.0008;  Loss pred: 0.0008; Loss self: 0.0000; time: 8.52s
Val loss: 1.4257 score: 0.6615 time: 3.69s
Test loss: 1.2891 score: 0.6811 time: 3.49s
     INFO: Early stopping counter 19 of 20
Epoch 28/1000, LR 0.000299
Train loss: 0.0007;  Loss pred: 0.0007; Loss self: 0.0000; time: 8.38s
Val loss: 1.4976 score: 0.6550 time: 2.43s
Test loss: 1.3570 score: 0.6757 time: 2.56s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 007,   Train_Loss: 0.5835,   Val_Loss: 0.6041,   Val_Precision: 0.9520,   Val_Recall: 0.1408,   Val_accuracy: 0.2454,   Val_Score: 0.5669,   Val_Loss: 0.6041,   Test_Precision: 0.9301,   Test_Recall: 0.1574,   Test_accuracy: 0.2692,   Test_Score: 0.5728,   Test_loss: 0.5896


[2.140857676975429, 2.2274017119780183, 2.29443091689609, 2.094437174964696, 3.105155735043809, 2.7763828970491886, 2.464136164984666, 2.742967189056799, 2.469468976953067, 2.2858910689828917, 4.02115846297238, 2.099313258891925, 2.0908621759153903, 2.112115183030255, 2.147841055993922, 2.349463502992876, 2.2435665350640193, 2.1908043950097635, 2.2959469739580527, 2.037237133947201, 2.0595884129870683, 2.098642796976492, 2.1796783920144662, 2.1089960619574413, 1.9808005719678476, 2.0164724700152874, 2.2284716210560873, 2.1824544260744005, 2.2320745970355347, 2.002326796995476, 2.019772500032559, 3.39758708991576, 3.762358289095573, 2.780855445889756, 2.5741751269670203, 2.921798688941635, 2.5686420359415933, 2.1862453270005062, 2.22080479899887, 2.433067837031558, 2.115481865941547, 2.259185596019961, 2.297510837088339, 2.3001070549944416, 2.272767869057134, 2.2044090079143643, 2.2005922810640186, 2.1417592759244144, 2.141071559046395, 2.122739002923481, 2.1299087849911302, 2.209513673908077, 2.327609211904928, 2.1696139900013804, 2.237867301912047, 2.141589554026723, 2.1178092160262167, 2.1405937939416617, 2.17101852002088, 2.493544989032671, 2.3420545830158517, 2.6512845010729507, 2.200554602080956, 2.2567881770664826, 2.6292727099498734, 2.212392786052078, 2.2529815000016242, 2.435284537030384, 2.2777457509655505, 2.1263200809480622, 2.2984085789648816, 2.4371530150528997, 2.3273637329693884, 2.164191384916194, 2.149396884953603, 2.3444731649942696, 2.258764516096562, 2.136171119986102, 2.1466584079898894, 2.1699119500117376, 2.1783166870009154, 2.096245749038644, 2.241509270039387, 3.7499640750465915, 2.58650298404973, 2.4466855990467593, 2.589172002975829, 2.636732480954379, 3.4964643869316205, 2.564679148956202]
[0.0012667796905180054, 0.001317989178685218, 0.0013576514301160295, 0.0012393119378489327, 0.0018373702574223721, 0.001642830116597153, 0.001458068736677317, 0.0016230575083176324, 0.001461224246717791, 0.00135259826567035, 0.0023793837059008166, 0.0012421971946106067, 0.001237196553796089, 0.0012497722976510385, 0.001270911867452025, 0.001390215090528329, 0.0013275541627597747, 0.0012963339615442388, 0.001358548505300623, 0.0012054657597320717, 0.0012186913686314013, 0.001241800471583723, 0.0012897505278192107, 0.0012479266638801428, 0.0011720713443596732, 0.0011931789763404068, 0.0013186222609799334, 0.001291393151523314, 0.0013207541994293105, 0.0011848087556186248, 0.0011951316568239994, 0.0020104065620803312, 0.002226247508340576, 0.0016454765952010392, 0.0015231805485011955, 0.00172887496387079, 0.0015199065301429546, 0.00129363628816598, 0.001314085679880988, 0.001439685110669561, 0.0012517644177168917, 0.0013367962106626988, 0.0013594738680996088, 0.0013610100917126873, 0.0013448330586136887, 0.001304384028351695, 0.0013021256100970524, 0.0012673131810203636, 0.0012669062479564468, 0.0012560585816115274, 0.0012603010562077695, 0.00130740454077401, 0.0013772835573401942, 0.0012837952603558464, 0.0013241818354509153, 0.0012672127538619663, 0.001253141547944507, 0.0012666235467110425, 0.0012846263432076213, 0.0014754704077116396, 0.0013858311142105632, 0.001568807397084586, 0.0013021033148408025, 0.001335377619565966, 0.0015557826686093926, 0.001309108157427265, 0.001333125147929955, 0.0014409967674736, 0.0013477785508671898, 0.001258177562691161, 0.0013600050763105808, 0.0014421023757709465, 0.0013771383035321824, 0.0012805866182936058, 0.0012718324763039072, 0.00138726222780726, 0.0013365470509447113, 0.0012640065798734332, 0.0012702120757336623, 0.0012839715680542827, 0.0012889447852076422, 0.0012403821000228665, 0.001326336846177152, 0.002218913653873723, 0.0015304751384909645, 0.0014477429580158339, 0.001532054439630668, 0.001560196734292532, 0.0020689138384210773, 0.001517561626601303]
[789.403246266985, 758.7315709204582, 736.5660859757917, 806.8993523419897, 544.2561160225212, 608.7056658489632, 685.8387227194958, 616.1211139317807, 684.3576557439454, 739.3178191784818, 420.27689671069993, 805.0251637490386, 808.2790054108226, 800.1457560545322, 786.8366214919687, 719.3131529164785, 753.2649349094416, 771.4061574139157, 736.0797175060876, 829.5548769649511, 820.5522954700227, 805.2823483990595, 775.343741623321, 801.3291397194196, 853.1903836846388, 838.0972342197102, 758.3672971339424, 774.3575214259192, 757.1431538374768, 844.0180706445484, 836.7278987968976, 497.41182647415286, 449.1863533832277, 607.7266628504207, 656.5209889162494, 578.4108283696197, 657.9351954662272, 773.0148026519298, 760.9853872622417, 694.5963340100984, 798.8723643574341, 748.0571773196929, 735.5786848612892, 734.7484093535307, 743.5867177676631, 766.6453883705288, 767.9750649597208, 789.0709376153263, 789.3243889301409, 796.1412108000542, 793.4612091883728, 764.8741983165972, 726.066897895156, 778.9404049698835, 755.1832937350906, 789.1334718281466, 797.9944497413497, 789.5005604440513, 778.436473210623, 677.7499533527997, 721.5886479570411, 637.4268771669251, 767.9882146081948, 748.8518493555607, 642.7632986128013, 763.8788241647334, 750.1171225768084, 693.9640827600404, 741.9616519023682, 794.8003760781302, 735.2913731122226, 693.4320453257702, 726.1434798778952, 780.8921206224298, 786.2670741874088, 720.8442498867888, 748.1966304838802, 791.1351221764459, 787.2701095385269, 778.8334452883483, 775.8284229676334, 806.2031852777988, 753.9562840934866, 450.67098408909493, 653.3918616842031, 690.7303499307113, 652.7183200102666, 640.9448103693462, 483.3454063815268, 658.9518227602905]
Elapsed: 2.3638043247939398~0.38624588704958374
Time per graph: 0.0013987007839017394~0.00022854786215951704
Speed: 729.3367377409068~89.84192704224056
Total Time: 2.5654
best val loss: 0.6040701176993241 test_score: 0.5728

Testing...
Test loss: 1.2891 score: 0.6811 time: 2.29s
test Score 0.6811
Epoch Time List: [10.998816858045757, 11.274765963898972, 11.594147160067223, 10.629046493093483, 11.60124322399497, 15.398501255782321, 13.83067437412683, 13.924343895050697, 13.713329028105363, 11.883464321959764, 17.744239912950434, 10.499536670045927, 11.013335558003746, 10.696330915088765, 10.903166078962386, 11.236046041944064, 11.015943237114698, 11.121820542030036, 11.138006528955884, 10.688983661006205, 10.525015530991368, 10.94429292390123, 10.9512841028627, 10.893880854011513, 10.10809383506421, 10.196990794851445, 10.711114910896868, 10.874147452996112, 10.791069471975788, 10.259492499986663, 10.350437518791296, 17.326730969944037, 15.260092514217831, 13.72170140594244, 13.982024040888064, 13.449628110975027, 16.516328014899045, 10.55116016487591, 10.91212454403285, 11.15032859495841, 10.87329966481775, 10.787493768963031, 11.007420439040288, 10.980951666017063, 10.715309738996439, 10.547809788025916, 10.835833378136158, 10.232298900955357, 10.24752240802627, 10.207615635008551, 10.175302622956224, 10.239562032977119, 10.681900139898062, 10.38904963596724, 10.434742123936303, 10.348219939973205, 10.266357265878469, 10.24094153707847, 10.871603679959662, 14.838681461988017, 11.66372811596375, 13.204979041940533, 12.177345200092532, 11.142242301837541, 11.927587476093322, 11.218663286068477, 11.423371466924436, 11.591345859924331, 11.252086038002744, 10.871283849002793, 11.129545564879663, 11.670140733011067, 11.029131965828128, 10.63055057497695, 10.964568899129517, 11.362418083939701, 10.888438241905533, 10.881942043080926, 10.643306132871658, 10.626563884085044, 10.689169332035817, 10.641930586076342, 10.671545373043045, 13.759537125006318, 12.587649881956168, 13.924749115016311, 12.659137692069635, 16.42396272206679, 15.70015839394182, 13.369403062039055]
Total Epoch List: [31, 31, 28]
Total Time List: [2.0201744419755414, 2.6518087020376697, 2.565389723982662]
T-times Epoch Time: 11.396477948765876 ~ 0.29274513844190153
T-times Total Epoch: 29.666666666666664 ~ 0.3333333333333339
T-times Total Time: 2.234032549177452 ~ 0.17842507348783943
T-times Inference Elapsed: 2.2952319537762156 ~ 0.06857237101772395
T-times Time Per Graph: 0.0013581254164356305 ~ 4.0575367466108916e-05
T-times Speed: 748.9638905676054 ~ 19.627152826698648
T-times cross validation test micro f1 score:0.6318792524750751 ~ 0.0015779092702170039
T-times cross validation test precision:0.969995766188665 ~ 3.190044316148111e-05
T-times cross validation test recall:0.5027613412228797 ~ 0.0017751479289940808
T-times cross validation test f1_score:0.6318792524750751 ~ 0.0075661598361907445
