Namespace(seed=15, model='AEtransGAT', dataset='phish_hack/Volume', num_heads=1, num_layers=2, dim_hidden=64, dropout=0.1, epochs=1000, lr=0.0003, weight_decay=0.0001, batch_size=64, abs_pe=None, abs_pe_dim=20, num_class=2, outdir='./outdir/phish_hack/Volume/seed15/edge_attr/khopgnn_gat_1_0.1_0.0003_0.0001_2_1_64_BN', warmup=10, layer_norm=False, use_edge_attr=True, edge_dim=128, gnn_type='gat', k_hop=1, global_pool='mean', se='khopgnn', ie='mamba', aggr='add', not_extract_node_feature=False, k_ford=3, early_stop=1, early_stop_mindelta=-0.0, patience=2, training_times=2, Lambda=0.01, order='sd', use_cuda=True, batch_norm=True, save_logs=True)
Data(edge_index=[2, 424], edge_attr=[424, 2], x=[125, 14887], y=[1, 1], num_nodes=125)
Data(edge_index=[2, 424], edge_attr=[424, 2], x=[125, 14887], y=[1, 1], num_nodes=125)
Data(edge_index=[2, 346], edge_attr=[346, 2], x=[111, 14887], y=[1, 1], num_nodes=125)
========================training times:0========================
========================k_idx:0========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: AEtransGAT
AEtransGAT(
  (gae): GAE(
    (encoder): transGAT(
      (encoder): EdgeFeatureGAT()
      (classifier): Sequential(
        (0): Linear(in_features=64, out_features=64, bias=True)
        (1): ReLU(inplace=True)
        (2): Linear(in_features=64, out_features=2, bias=True)
      )
    )
    (decoder): InnerProductDecoder()
  )
)
Total number of parameters: 1914242
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x72c490d34ca0>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 2.2960;  Loss pred: 2.2960; Loss self: 0.0000; time: 9.42s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6949 score: 0.5000 time: 3.63s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6949 score: 0.5000 time: 2.96s
Epoch 2/1000, LR 0.000029
Train loss: 2.1923;  Loss pred: 2.1923; Loss self: 0.0000; time: 8.98s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6948 score: 0.5000 time: 3.10s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6948 score: 0.5000 time: 3.08s
Epoch 3/1000, LR 0.000059
Train loss: 2.0079;  Loss pred: 2.0079; Loss self: 0.0000; time: 9.64s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6947 score: 0.5000 time: 3.56s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6947 score: 0.5000 time: 3.36s
Epoch 4/1000, LR 0.000089
Train loss: 1.7718;  Loss pred: 1.7718; Loss self: 0.0000; time: 8.40s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6946 score: 0.5000 time: 2.70s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6946 score: 0.5000 time: 2.81s
Epoch 5/1000, LR 0.000119
Train loss: 1.5365;  Loss pred: 1.5365; Loss self: 0.0000; time: 8.39s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6943 score: 0.5000 time: 2.79s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6943 score: 0.5000 time: 2.80s
Epoch 6/1000, LR 0.000149
Train loss: 1.3382;  Loss pred: 1.3382; Loss self: 0.0000; time: 8.60s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6938 score: 0.5000 time: 3.01s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6937 score: 0.5000 time: 2.89s
Epoch 7/1000, LR 0.000179
Train loss: 1.1927;  Loss pred: 1.1927; Loss self: 0.0000; time: 8.55s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6932 score: 0.5000 time: 2.87s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6931 score: 0.5000 time: 2.79s
Epoch 8/1000, LR 0.000209
Train loss: 1.1024;  Loss pred: 1.1024; Loss self: 0.0000; time: 9.00s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6919 score: 0.5000 time: 2.82s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6918 score: 0.5000 time: 3.00s
Epoch 9/1000, LR 0.000239
Train loss: 1.0482;  Loss pred: 1.0482; Loss self: 0.0000; time: 8.78s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6898 score: 0.5000 time: 3.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6898 score: 0.5000 time: 2.77s
Epoch 10/1000, LR 0.000269
Train loss: 1.0171;  Loss pred: 1.0171; Loss self: 0.0000; time: 8.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6869 score: 0.5000 time: 2.77s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6868 score: 0.5000 time: 2.71s
Epoch 11/1000, LR 0.000299
Train loss: 0.9978;  Loss pred: 0.9978; Loss self: 0.0000; time: 8.61s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6818 score: 0.5000 time: 2.83s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6817 score: 0.5000 time: 2.94s
Epoch 12/1000, LR 0.000299
Train loss: 0.9848;  Loss pred: 0.9848; Loss self: 0.0000; time: 9.48s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6749 score: 0.5000 time: 2.98s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6748 score: 0.5000 time: 3.07s
Epoch 13/1000, LR 0.000299
Train loss: 0.9749;  Loss pred: 0.9749; Loss self: 0.0000; time: 8.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6647 score: 0.5000 time: 2.70s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6646 score: 0.5000 time: 2.71s
Epoch 14/1000, LR 0.000299
Train loss: 0.9650;  Loss pred: 0.9650; Loss self: 0.0000; time: 8.42s
Val loss: 0.6492 score: 0.5024 time: 2.80s
Test loss: 0.6492 score: 0.5006 time: 2.79s
Epoch 15/1000, LR 0.000299
Train loss: 0.9536;  Loss pred: 0.9536; Loss self: 0.0000; time: 8.80s
Val loss: 0.6293 score: 0.5391 time: 2.85s
Test loss: 0.6295 score: 0.5438 time: 3.02s
Epoch 16/1000, LR 0.000299
Train loss: 0.9408;  Loss pred: 0.9408; Loss self: 0.0000; time: 9.45s
Val loss: 0.6037 score: 0.7089 time: 3.00s
Test loss: 0.6042 score: 0.7024 time: 2.96s
Epoch 17/1000, LR 0.000299
Train loss: 0.9241;  Loss pred: 0.9241; Loss self: 0.0000; time: 8.67s
Val loss: 0.5727 score: 0.8047 time: 2.86s
Test loss: 0.5735 score: 0.7893 time: 2.80s
Epoch 18/1000, LR 0.000299
Train loss: 0.9047;  Loss pred: 0.9047; Loss self: 0.0000; time: 8.52s
Val loss: 0.5349 score: 0.8562 time: 2.78s
Test loss: 0.5362 score: 0.8432 time: 2.86s
Epoch 19/1000, LR 0.000299
Train loss: 0.8817;  Loss pred: 0.8817; Loss self: 0.0000; time: 8.55s
Val loss: 0.4931 score: 0.8822 time: 2.76s
Test loss: 0.4949 score: 0.8692 time: 3.03s
Epoch 20/1000, LR 0.000299
Train loss: 0.8565;  Loss pred: 0.8565; Loss self: 0.0000; time: 8.96s
Val loss: 0.4489 score: 0.8976 time: 3.04s
Test loss: 0.4511 score: 0.8917 time: 2.91s
Epoch 21/1000, LR 0.000299
Train loss: 0.8299;  Loss pred: 0.8299; Loss self: 0.0000; time: 8.98s
Val loss: 0.4051 score: 0.9112 time: 2.89s
Test loss: 0.4078 score: 0.9047 time: 2.78s
Epoch 22/1000, LR 0.000299
Train loss: 0.8035;  Loss pred: 0.8035; Loss self: 0.0000; time: 8.46s
Val loss: 0.3639 score: 0.9195 time: 2.76s
Test loss: 0.3669 score: 0.9107 time: 2.82s
Epoch 23/1000, LR 0.000299
Train loss: 0.7768;  Loss pred: 0.7768; Loss self: 0.0000; time: 8.68s
Val loss: 0.3265 score: 0.9243 time: 2.92s
Test loss: 0.3298 score: 0.9172 time: 2.98s
Epoch 24/1000, LR 0.000299
Train loss: 0.7535;  Loss pred: 0.7535; Loss self: 0.0000; time: 8.42s
Val loss: 0.2935 score: 0.9278 time: 2.93s
Test loss: 0.2970 score: 0.9225 time: 2.84s
Epoch 25/1000, LR 0.000299
Train loss: 0.7330;  Loss pred: 0.7330; Loss self: 0.0000; time: 8.78s
Val loss: 0.2660 score: 0.9361 time: 2.80s
Test loss: 0.2694 score: 0.9284 time: 2.82s
Epoch 26/1000, LR 0.000299
Train loss: 0.7141;  Loss pred: 0.7141; Loss self: 0.0000; time: 8.43s
Val loss: 0.2429 score: 0.9402 time: 2.80s
Test loss: 0.2461 score: 0.9325 time: 2.77s
Epoch 27/1000, LR 0.000299
Train loss: 0.6986;  Loss pred: 0.6986; Loss self: 0.0000; time: 8.48s
Val loss: 0.2237 score: 0.9385 time: 2.79s
Test loss: 0.2267 score: 0.9308 time: 2.69s
Epoch 28/1000, LR 0.000299
Train loss: 0.6849;  Loss pred: 0.6849; Loss self: 0.0000; time: 8.42s
Val loss: 0.2078 score: 0.9379 time: 2.99s
Test loss: 0.2103 score: 0.9337 time: 3.00s
Epoch 29/1000, LR 0.000299
Train loss: 0.6739;  Loss pred: 0.6739; Loss self: 0.0000; time: 9.07s
Val loss: 0.1944 score: 0.9426 time: 2.96s
Test loss: 0.1964 score: 0.9373 time: 2.92s
Epoch 30/1000, LR 0.000299
Train loss: 0.6636;  Loss pred: 0.6636; Loss self: 0.0000; time: 9.32s
Val loss: 0.1836 score: 0.9450 time: 3.12s
Test loss: 0.1852 score: 0.9402 time: 3.21s
Epoch 31/1000, LR 0.000299
Train loss: 0.6553;  Loss pred: 0.6553; Loss self: 0.0000; time: 8.25s
Val loss: 0.1737 score: 0.9473 time: 2.79s
Test loss: 0.1749 score: 0.9402 time: 2.83s
Epoch 32/1000, LR 0.000299
Train loss: 0.6481;  Loss pred: 0.6481; Loss self: 0.0000; time: 8.70s
Val loss: 0.1647 score: 0.9473 time: 3.02s
Test loss: 0.1653 score: 0.9420 time: 3.28s
Epoch 33/1000, LR 0.000299
Train loss: 0.6418;  Loss pred: 0.6418; Loss self: 0.0000; time: 9.04s
Val loss: 0.1577 score: 0.9497 time: 3.19s
Test loss: 0.1578 score: 0.9450 time: 3.00s
Epoch 34/1000, LR 0.000298
Train loss: 0.6357;  Loss pred: 0.6357; Loss self: 0.0000; time: 8.53s
Val loss: 0.1515 score: 0.9509 time: 2.99s
Test loss: 0.1510 score: 0.9462 time: 2.97s
Epoch 35/1000, LR 0.000298
Train loss: 0.6312;  Loss pred: 0.6312; Loss self: 0.0000; time: 8.78s
Val loss: 0.1457 score: 0.9527 time: 2.98s
Test loss: 0.1447 score: 0.9479 time: 2.88s
Epoch 36/1000, LR 0.000298
Train loss: 0.6266;  Loss pred: 0.6266; Loss self: 0.0000; time: 8.54s
Val loss: 0.1412 score: 0.9550 time: 2.78s
Test loss: 0.1395 score: 0.9497 time: 2.85s
Epoch 37/1000, LR 0.000298
Train loss: 0.6220;  Loss pred: 0.6220; Loss self: 0.0000; time: 8.78s
Val loss: 0.1365 score: 0.9574 time: 2.73s
Test loss: 0.1343 score: 0.9503 time: 2.81s
Epoch 38/1000, LR 0.000298
Train loss: 0.6187;  Loss pred: 0.6187; Loss self: 0.0000; time: 8.48s
Val loss: 0.1322 score: 0.9592 time: 2.89s
Test loss: 0.1295 score: 0.9503 time: 2.78s
Epoch 39/1000, LR 0.000298
Train loss: 0.6153;  Loss pred: 0.6153; Loss self: 0.0000; time: 8.71s
Val loss: 0.1287 score: 0.9609 time: 2.93s
Test loss: 0.1254 score: 0.9527 time: 3.04s
Epoch 40/1000, LR 0.000298
Train loss: 0.6126;  Loss pred: 0.6126; Loss self: 0.0000; time: 8.72s
Val loss: 0.1256 score: 0.9639 time: 2.87s
Test loss: 0.1218 score: 0.9550 time: 2.86s
Epoch 41/1000, LR 0.000298
Train loss: 0.6096;  Loss pred: 0.6096; Loss self: 0.0000; time: 9.56s
Val loss: 0.1227 score: 0.9645 time: 3.05s
Test loss: 0.1184 score: 0.9562 time: 3.15s
Epoch 42/1000, LR 0.000298
Train loss: 0.6067;  Loss pred: 0.6067; Loss self: 0.0000; time: 9.07s
Val loss: 0.1205 score: 0.9669 time: 3.31s
Test loss: 0.1157 score: 0.9604 time: 3.08s
Epoch 43/1000, LR 0.000298
Train loss: 0.6051;  Loss pred: 0.6051; Loss self: 0.0000; time: 8.38s
Val loss: 0.1178 score: 0.9680 time: 2.75s
Test loss: 0.1125 score: 0.9639 time: 2.70s
Epoch 44/1000, LR 0.000298
Train loss: 0.6034;  Loss pred: 0.6034; Loss self: 0.0000; time: 8.18s
Val loss: 0.1157 score: 0.9675 time: 2.86s
Test loss: 0.1099 score: 0.9651 time: 2.86s
Epoch 45/1000, LR 0.000298
Train loss: 0.6003;  Loss pred: 0.6003; Loss self: 0.0000; time: 8.57s
Val loss: 0.1138 score: 0.9675 time: 2.84s
Test loss: 0.1076 score: 0.9651 time: 2.72s
Epoch 46/1000, LR 0.000298
Train loss: 0.5987;  Loss pred: 0.5987; Loss self: 0.0000; time: 8.58s
Val loss: 0.1119 score: 0.9675 time: 2.82s
Test loss: 0.1051 score: 0.9657 time: 2.77s
Epoch 47/1000, LR 0.000298
Train loss: 0.5965;  Loss pred: 0.5965; Loss self: 0.0000; time: 8.37s
Val loss: 0.1105 score: 0.9675 time: 2.78s
Test loss: 0.1034 score: 0.9663 time: 2.78s
Epoch 48/1000, LR 0.000298
Train loss: 0.5946;  Loss pred: 0.5946; Loss self: 0.0000; time: 7.79s
Val loss: 0.1083 score: 0.9675 time: 2.45s
Test loss: 0.1007 score: 0.9675 time: 2.35s
Epoch 49/1000, LR 0.000298
Train loss: 0.5924;  Loss pred: 0.5924; Loss self: 0.0000; time: 7.43s
Val loss: 0.1073 score: 0.9675 time: 2.91s
Test loss: 0.0993 score: 0.9669 time: 2.73s
Epoch 50/1000, LR 0.000298
Train loss: 0.5914;  Loss pred: 0.5914; Loss self: 0.0000; time: 8.15s
Val loss: 0.1061 score: 0.9675 time: 2.69s
Test loss: 0.0978 score: 0.9669 time: 2.66s
Epoch 51/1000, LR 0.000298
Train loss: 0.5898;  Loss pred: 0.5898; Loss self: 0.0000; time: 9.00s
Val loss: 0.1046 score: 0.9675 time: 3.30s
Test loss: 0.0958 score: 0.9680 time: 3.17s
Epoch 52/1000, LR 0.000298
Train loss: 0.5873;  Loss pred: 0.5873; Loss self: 0.0000; time: 8.57s
Val loss: 0.1034 score: 0.9686 time: 2.81s
Test loss: 0.0943 score: 0.9680 time: 2.68s
Epoch 53/1000, LR 0.000298
Train loss: 0.5850;  Loss pred: 0.5850; Loss self: 0.0000; time: 8.27s
Val loss: 0.1028 score: 0.9680 time: 3.00s
Test loss: 0.0934 score: 0.9686 time: 2.96s
Epoch 54/1000, LR 0.000297
Train loss: 0.5841;  Loss pred: 0.5841; Loss self: 0.0000; time: 8.91s
Val loss: 0.1016 score: 0.9686 time: 3.18s
Test loss: 0.0919 score: 0.9698 time: 3.26s
Epoch 55/1000, LR 0.000297
Train loss: 0.5835;  Loss pred: 0.5835; Loss self: 0.0000; time: 8.67s
Val loss: 0.1003 score: 0.9698 time: 2.95s
Test loss: 0.0904 score: 0.9698 time: 2.65s
Epoch 56/1000, LR 0.000297
Train loss: 0.5809;  Loss pred: 0.5809; Loss self: 0.0000; time: 8.56s
Val loss: 0.0998 score: 0.9692 time: 2.94s
Test loss: 0.0896 score: 0.9698 time: 2.67s
Epoch 57/1000, LR 0.000297
Train loss: 0.5808;  Loss pred: 0.5808; Loss self: 0.0000; time: 8.50s
Val loss: 0.0991 score: 0.9692 time: 2.92s
Test loss: 0.0888 score: 0.9692 time: 3.24s
Epoch 58/1000, LR 0.000297
Train loss: 0.5792;  Loss pred: 0.5792; Loss self: 0.0000; time: 8.88s
Val loss: 0.0980 score: 0.9692 time: 3.23s
Test loss: 0.0874 score: 0.9698 time: 2.89s
Epoch 59/1000, LR 0.000297
Train loss: 0.5769;  Loss pred: 0.5769; Loss self: 0.0000; time: 8.72s
Val loss: 0.0980 score: 0.9692 time: 3.08s
Test loss: 0.0873 score: 0.9692 time: 3.11s
Epoch 60/1000, LR 0.000297
Train loss: 0.5757;  Loss pred: 0.5757; Loss self: 0.0000; time: 8.74s
Val loss: 0.0966 score: 0.9710 time: 2.90s
Test loss: 0.0855 score: 0.9704 time: 2.86s
Epoch 61/1000, LR 0.000297
Train loss: 0.5749;  Loss pred: 0.5749; Loss self: 0.0000; time: 8.20s
Val loss: 0.0970 score: 0.9704 time: 2.73s
Test loss: 0.0860 score: 0.9698 time: 2.94s
     INFO: Early stopping counter 1 of 2
Epoch 62/1000, LR 0.000297
Train loss: 0.5734;  Loss pred: 0.5734; Loss self: 0.0000; time: 8.43s
Val loss: 0.0958 score: 0.9710 time: 2.85s
Test loss: 0.0846 score: 0.9704 time: 2.82s
Epoch 63/1000, LR 0.000297
Train loss: 0.5718;  Loss pred: 0.5718; Loss self: 0.0000; time: 8.58s
Val loss: 0.0952 score: 0.9716 time: 3.04s
Test loss: 0.0838 score: 0.9716 time: 2.84s
Epoch 64/1000, LR 0.000297
Train loss: 0.5705;  Loss pred: 0.5705; Loss self: 0.0000; time: 7.89s
Val loss: 0.0944 score: 0.9710 time: 2.70s
Test loss: 0.0828 score: 0.9722 time: 2.63s
Epoch 65/1000, LR 0.000297
Train loss: 0.5698;  Loss pred: 0.5698; Loss self: 0.0000; time: 8.25s
Val loss: 0.0946 score: 0.9716 time: 2.88s
Test loss: 0.0831 score: 0.9716 time: 2.96s
     INFO: Early stopping counter 1 of 2
Epoch 66/1000, LR 0.000297
Train loss: 0.5673;  Loss pred: 0.5673; Loss self: 0.0000; time: 8.81s
Val loss: 0.0938 score: 0.9710 time: 3.28s
Test loss: 0.0821 score: 0.9722 time: 2.95s
Epoch 67/1000, LR 0.000297
Train loss: 0.5669;  Loss pred: 0.5669; Loss self: 0.0000; time: 8.18s
Val loss: 0.0940 score: 0.9710 time: 2.70s
Test loss: 0.0823 score: 0.9716 time: 2.69s
     INFO: Early stopping counter 1 of 2
Epoch 68/1000, LR 0.000296
Train loss: 0.5664;  Loss pred: 0.5664; Loss self: 0.0000; time: 8.33s
Val loss: 0.0926 score: 0.9716 time: 2.78s
Test loss: 0.0807 score: 0.9716 time: 2.85s
Epoch 69/1000, LR 0.000296
Train loss: 0.5639;  Loss pred: 0.5639; Loss self: 0.0000; time: 8.20s
Val loss: 0.0932 score: 0.9704 time: 2.85s
Test loss: 0.0814 score: 0.9716 time: 2.99s
     INFO: Early stopping counter 1 of 2
Epoch 70/1000, LR 0.000296
Train loss: 0.5627;  Loss pred: 0.5627; Loss self: 0.0000; time: 8.45s
Val loss: 0.0919 score: 0.9716 time: 2.84s
Test loss: 0.0799 score: 0.9716 time: 2.87s
Epoch 71/1000, LR 0.000296
Train loss: 0.5609;  Loss pred: 0.5609; Loss self: 0.0000; time: 8.11s
Val loss: 0.0925 score: 0.9710 time: 2.72s
Test loss: 0.0805 score: 0.9716 time: 2.56s
     INFO: Early stopping counter 1 of 2
Epoch 72/1000, LR 0.000296
Train loss: 0.5616;  Loss pred: 0.5616; Loss self: 0.0000; time: 8.00s
Val loss: 0.0923 score: 0.9710 time: 2.69s
Test loss: 0.0804 score: 0.9716 time: 2.62s
     INFO: Early stopping counter 2 of 2
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 069,   Train_Loss: 0.5627,   Val_Loss: 0.0919,   Val_Precision: 0.9830,   Val_Recall: 0.9598,   Val_accuracy: 0.9713,   Val_Score: 0.9716,   Val_Loss: 0.0919,   Test_Precision: 0.9902,   Test_Recall: 0.9527,   Test_accuracy: 0.9710,   Test_Score: 0.9716,   Test_loss: 0.0799


[2.962061750004068, 3.0839386499719694, 3.3673303119139746, 2.8139037280343473, 2.80905502603855, 2.898895642021671, 2.794384910026565, 3.0066743589704856, 2.7710372689180076, 2.714464638964273, 2.9409477339359, 3.0709727889625356, 2.713530980050564, 2.796701375977136, 3.0248045519692823, 2.9650405660504475, 2.800456113065593, 2.8676369020249695, 3.0303249689750373, 2.9154960609739646, 2.78667945205234, 2.827296565985307, 2.9858155540423468, 2.841761326068081, 2.8214208049466833, 2.7751818110700697, 2.692183414939791, 3.002821251982823, 2.929314394015819, 3.2114196409238502, 2.8315175590105355, 3.2813473170390353, 3.0054445679998025, 2.976236253976822, 2.885106620960869, 2.8511668499559164, 2.8197614329401404, 2.786557283019647, 3.0434639781014994, 2.865573493996635, 3.1533503950340673, 3.0877274150261655, 2.7044905450893566, 2.8696160800755024, 2.7235967270098627, 2.7765303750056773, 2.7822039420716465, 2.358088752022013, 2.7298783489968628, 2.6626191299874336, 3.1780494269914925, 2.6839467100799084, 2.96200671303086, 3.261197690037079, 2.652835274930112, 2.6789296590723097, 3.248675453942269, 2.8985182179603726, 3.1128140370128676, 2.860498477006331, 2.947222071001306, 2.827849260997027, 2.8452753929886967, 2.634863918996416, 2.9678971979301423, 2.957479799981229, 2.69161860703025, 2.854279578081332, 2.990352038992569, 2.873150640982203, 2.5679164100438356, 2.6245760628953576]
[0.0017526992603574367, 0.001824815769214183, 0.0019925031431443636, 0.0016650317917363002, 0.0016621627373009172, 0.0017153228651015805, 0.0016534821952819913, 0.0017790972538286897, 0.001639667023028407, 0.0016061920940617, 0.0017402057597253847, 0.0018171436621080093, 0.001605639633166014, 0.0016548528851935717, 0.0017898251786800487, 0.0017544618734026316, 0.0016570746231157356, 0.001696826569245544, 0.0017930916976183653, 0.0017251455982094466, 0.0016489227526936922, 0.0016729565479203, 0.0017667547657055306, 0.0016815155775550775, 0.001669479766240641, 0.0016421194148343607, 0.0015930079378341957, 0.0017768173088655757, 0.0017333221266365793, 0.0019002483082389647, 0.0016754541769293109, 0.0019416256313840444, 0.0017783695668637885, 0.0017610865408146876, 0.0017071636810419343, 0.001687080976305276, 0.0016684978893136925, 0.0016488504633252348, 0.0018008662592316565, 0.0016956056177494883, 0.0018658878077124658, 0.001827057642027317, 0.0016002902633664832, 0.0016979976805180487, 0.0016115956964555401, 0.001642917381660164, 0.0016462745219358854, 0.0013953187881787059, 0.0016153126325425223, 0.0015755142780990731, 0.0018805026195215931, 0.0015881341479762772, 0.0017526666941011005, 0.0019297027751698694, 0.0015697250147515456, 0.0015851654787410116, 0.001922293168013177, 0.0017150995372546584, 0.0018419017970490342, 0.0016926026491161721, 0.0017439183852078735, 0.0016732835863887735, 0.0016835949070939033, 0.0015590910763292403, 0.0017561521881243446, 0.001749988047326171, 0.0015926737319705622, 0.0016889228272670604, 0.0017694390763269639, 0.00170008913667586, 0.0015194771657064117, 0.001553003587512046]
[570.5485376858463, 548.0005252424074, 501.8812660048821, 600.5891328700679, 601.6258080865402, 582.9806273472491, 604.7842564337114, 562.0828191645843, 609.8799243721054, 622.5905380166727, 574.6446903829385, 550.3142216284278, 622.804756026227, 604.2833226731377, 558.7137849616547, 569.9753383985392, 603.4731242940269, 589.3354206756837, 557.6959624140964, 579.6612187620074, 606.4565476862956, 597.7441561426855, 566.0095104374392, 594.7015973851395, 598.9889905954444, 608.9691108736262, 627.7432624469964, 562.8040626407779, 576.9268069867934, 526.2470150160216, 596.8530884161513, 515.0323439473614, 562.3128165443878, 567.8312659963859, 585.7669133340919, 592.7397760064902, 599.3414833813979, 606.4831361258201, 555.2883202035515, 589.7597823055465, 535.9379035902358, 547.328106676696, 624.8866364382732, 588.9289552473982, 620.5030220664823, 608.673333889439, 607.4321060524468, 716.6821005150291, 619.0752055383778, 634.7133846394231, 531.7727237489324, 629.6697298992513, 570.5591390340622, 518.21452135911, 637.0542551099493, 630.8489639796039, 520.2120137760096, 583.0565388646128, 542.9171096972325, 590.806117739548, 573.4213300817979, 597.6273287650945, 593.9671091819384, 641.3993481089142, 569.4267312151621, 571.4324743691323, 627.8749877809144, 592.0933649870524, 565.1508511249901, 588.2044525943352, 658.1211107144842, 643.9135157453353]
Elapsed: 2.8852191979191653~0.17914023259001086
Time per graph: 0.0017072302946267253~0.00010600013762722535
Speed: 587.9968018672844~36.47175706712333
Total Time: 2.6251
best val loss: 0.09193052086957107 test_score: 0.9716

Testing...
Test loss: 0.0838 score: 0.9716 time: 2.72s
test Score 0.9716
Epoch Time List: [16.007859014091082, 15.157085902057588, 16.554072922095656, 13.908855348010547, 13.97904615092557, 14.506266833981499, 14.214631369919516, 14.822081824066117, 14.592174863093533, 13.65768329706043, 14.374243730097078, 15.529351482982747, 13.664574267109856, 14.0137144329492, 14.671796111972071, 15.416175891063176, 14.324531925027259, 14.160469675902277, 14.334310073172674, 14.91405500494875, 14.65773291804362, 14.039608516031876, 14.587813291931525, 14.192109994008206, 14.398914911900647, 14.001702500972897, 13.954592054942623, 14.403301642043516, 14.949595038895495, 15.638867919915356, 13.87005196500104, 14.995408641174436, 15.231365607120097, 14.490703088813461, 14.642232132027857, 14.163644325104542, 14.322027236921713, 14.15334254608024, 14.684480835217983, 14.449759871000424, 15.762286799843423, 15.454190404969268, 13.826963388943113, 13.907815029029734, 14.128398102940992, 14.172601917991415, 13.928043741965666, 12.600569253088906, 13.062139335903339, 13.494248122908175, 15.46742716501467, 14.061198369949125, 14.231202763970941, 15.348347980994731, 14.268895176006481, 14.17132538498845, 14.668313393951394, 15.010024868883193, 14.905146228033118, 14.494733839994296, 13.868067602976225, 14.108370709931478, 14.460939491982572, 13.211337042041123, 14.094709369004704, 15.04079029802233, 13.563642238033935, 13.964157660026103, 14.038318531122059, 14.15391842101235, 13.401441704016179, 13.310620654025115]
Total Epoch List: [72]
Total Time List: [2.625078661949374]
========================k_idx:1========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: AEtransGAT
AEtransGAT(
  (gae): GAE(
    (encoder): transGAT(
      (encoder): EdgeFeatureGAT()
      (classifier): Sequential(
        (0): Linear(in_features=64, out_features=64, bias=True)
        (1): ReLU(inplace=True)
        (2): Linear(in_features=64, out_features=2, bias=True)
      )
    )
    (decoder): InnerProductDecoder()
  )
)
Total number of parameters: 1914242
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x72c48b90fe50>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 2.6001;  Loss pred: 2.6001; Loss self: 0.0000; time: 8.37s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6956 score: 0.5000 time: 2.89s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6957 score: 0.5000 time: 2.79s
Epoch 2/1000, LR 0.000029
Train loss: 2.4756;  Loss pred: 2.4756; Loss self: 0.0000; time: 8.31s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6952 score: 0.5000 time: 3.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6952 score: 0.5000 time: 3.36s
Epoch 3/1000, LR 0.000059
Train loss: 2.2454;  Loss pred: 2.2454; Loss self: 0.0000; time: 8.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6944 score: 0.5000 time: 2.81s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6945 score: 0.5000 time: 2.66s
Epoch 4/1000, LR 0.000089
Train loss: 1.9581;  Loss pred: 1.9581; Loss self: 0.0000; time: 7.73s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6939 score: 0.5000 time: 2.61s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6940 score: 0.5000 time: 2.82s
Epoch 5/1000, LR 0.000119
Train loss: 1.6597;  Loss pred: 1.6597; Loss self: 0.0000; time: 7.83s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6932 score: 0.5000 time: 2.65s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6933 score: 0.5000 time: 2.70s
Epoch 6/1000, LR 0.000149
Train loss: 1.4096;  Loss pred: 1.4096; Loss self: 0.0000; time: 7.97s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6926 score: 0.5000 time: 2.66s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6928 score: 0.5000 time: 2.93s
Epoch 7/1000, LR 0.000179
Train loss: 1.2310;  Loss pred: 1.2310; Loss self: 0.0000; time: 8.47s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6920 score: 0.5000 time: 2.64s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6922 score: 0.5000 time: 2.65s
Epoch 8/1000, LR 0.000209
Train loss: 1.1200;  Loss pred: 1.1200; Loss self: 0.0000; time: 7.58s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6914 score: 0.5000 time: 2.54s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6916 score: 0.5000 time: 2.75s
Epoch 9/1000, LR 0.000239
Train loss: 1.0566;  Loss pred: 1.0566; Loss self: 0.0000; time: 7.74s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6901 score: 0.5000 time: 2.73s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6904 score: 0.5000 time: 2.67s
Epoch 10/1000, LR 0.000269
Train loss: 1.0221;  Loss pred: 1.0221; Loss self: 0.0000; time: 7.89s
Val loss: 0.6888 score: 0.5172 time: 2.70s
Test loss: 0.6892 score: 0.5077 time: 2.81s
Epoch 11/1000, LR 0.000299
Train loss: 1.0023;  Loss pred: 1.0023; Loss self: 0.0000; time: 8.58s
Val loss: 0.6859 score: 0.5373 time: 3.32s
Test loss: 0.6864 score: 0.5260 time: 3.23s
Epoch 12/1000, LR 0.000299
Train loss: 0.9901;  Loss pred: 0.9901; Loss self: 0.0000; time: 7.85s
Val loss: 0.6819 score: 0.6290 time: 2.67s
Test loss: 0.6826 score: 0.6249 time: 2.59s
Epoch 13/1000, LR 0.000299
Train loss: 0.9810;  Loss pred: 0.9810; Loss self: 0.0000; time: 7.75s
Val loss: 0.6754 score: 0.8716 time: 2.64s
Test loss: 0.6764 score: 0.8604 time: 2.60s
Epoch 14/1000, LR 0.000299
Train loss: 0.9748;  Loss pred: 0.9748; Loss self: 0.0000; time: 7.78s
Val loss: 0.6678 score: 0.9302 time: 2.65s
Test loss: 0.6691 score: 0.9314 time: 2.64s
Epoch 15/1000, LR 0.000299
Train loss: 0.9668;  Loss pred: 0.9668; Loss self: 0.0000; time: 8.29s
Val loss: 0.6571 score: 0.9213 time: 3.03s
Test loss: 0.6588 score: 0.9213 time: 3.55s
Epoch 16/1000, LR 0.000299
Train loss: 0.9593;  Loss pred: 0.9593; Loss self: 0.0000; time: 8.50s
Val loss: 0.6432 score: 0.9136 time: 3.06s
Test loss: 0.6455 score: 0.9107 time: 2.88s
Epoch 17/1000, LR 0.000299
Train loss: 0.9486;  Loss pred: 0.9486; Loss self: 0.0000; time: 8.50s
Val loss: 0.6238 score: 0.8947 time: 2.83s
Test loss: 0.6267 score: 0.8822 time: 2.95s
Epoch 18/1000, LR 0.000299
Train loss: 0.9384;  Loss pred: 0.9384; Loss self: 0.0000; time: 8.45s
Val loss: 0.6017 score: 0.8947 time: 3.22s
Test loss: 0.6053 score: 0.8811 time: 2.87s
Epoch 19/1000, LR 0.000299
Train loss: 0.9243;  Loss pred: 0.9243; Loss self: 0.0000; time: 7.77s
Val loss: 0.5740 score: 0.8947 time: 2.60s
Test loss: 0.5783 score: 0.8817 time: 2.73s
Epoch 20/1000, LR 0.000299
Train loss: 0.9076;  Loss pred: 0.9076; Loss self: 0.0000; time: 7.90s
Val loss: 0.5408 score: 0.8964 time: 2.61s
Test loss: 0.5456 score: 0.8893 time: 2.62s
Epoch 21/1000, LR 0.000299
Train loss: 0.8889;  Loss pred: 0.8889; Loss self: 0.0000; time: 7.77s
Val loss: 0.5031 score: 0.9018 time: 2.65s
Test loss: 0.5083 score: 0.8959 time: 2.70s
Epoch 22/1000, LR 0.000299
Train loss: 0.8683;  Loss pred: 0.8683; Loss self: 0.0000; time: 8.04s
Val loss: 0.4637 score: 0.9107 time: 2.77s
Test loss: 0.4691 score: 0.9095 time: 2.76s
Epoch 23/1000, LR 0.000299
Train loss: 0.8456;  Loss pred: 0.8456; Loss self: 0.0000; time: 7.73s
Val loss: 0.4238 score: 0.9195 time: 2.55s
Test loss: 0.4292 score: 0.9225 time: 2.62s
Epoch 24/1000, LR 0.000299
Train loss: 0.8224;  Loss pred: 0.8224; Loss self: 0.0000; time: 7.76s
Val loss: 0.3849 score: 0.9249 time: 2.77s
Test loss: 0.3901 score: 0.9266 time: 2.83s
Epoch 25/1000, LR 0.000299
Train loss: 0.7992;  Loss pred: 0.7992; Loss self: 0.0000; time: 8.07s
Val loss: 0.3461 score: 0.9325 time: 2.68s
Test loss: 0.3509 score: 0.9308 time: 2.68s
Epoch 26/1000, LR 0.000299
Train loss: 0.7750;  Loss pred: 0.7750; Loss self: 0.0000; time: 7.98s
Val loss: 0.3092 score: 0.9355 time: 2.63s
Test loss: 0.3134 score: 0.9349 time: 2.69s
Epoch 27/1000, LR 0.000299
Train loss: 0.7523;  Loss pred: 0.7523; Loss self: 0.0000; time: 7.74s
Val loss: 0.2765 score: 0.9337 time: 2.65s
Test loss: 0.2797 score: 0.9367 time: 2.70s
Epoch 28/1000, LR 0.000299
Train loss: 0.7328;  Loss pred: 0.7328; Loss self: 0.0000; time: 7.72s
Val loss: 0.2484 score: 0.9367 time: 2.69s
Test loss: 0.2506 score: 0.9402 time: 2.79s
Epoch 29/1000, LR 0.000299
Train loss: 0.7139;  Loss pred: 0.7139; Loss self: 0.0000; time: 7.88s
Val loss: 0.2247 score: 0.9408 time: 2.68s
Test loss: 0.2258 score: 0.9420 time: 2.74s
Epoch 30/1000, LR 0.000299
Train loss: 0.6968;  Loss pred: 0.6968; Loss self: 0.0000; time: 8.33s
Val loss: 0.2048 score: 0.9456 time: 2.99s
Test loss: 0.2047 score: 0.9462 time: 3.07s
Epoch 31/1000, LR 0.000299
Train loss: 0.6815;  Loss pred: 0.6815; Loss self: 0.0000; time: 7.90s
Val loss: 0.1887 score: 0.9473 time: 2.77s
Test loss: 0.1876 score: 0.9479 time: 2.66s
Epoch 32/1000, LR 0.000299
Train loss: 0.6698;  Loss pred: 0.6698; Loss self: 0.0000; time: 8.23s
Val loss: 0.1754 score: 0.9485 time: 2.86s
Test loss: 0.1732 score: 0.9515 time: 2.96s
Epoch 33/1000, LR 0.000299
Train loss: 0.6598;  Loss pred: 0.6598; Loss self: 0.0000; time: 8.44s
Val loss: 0.1642 score: 0.9491 time: 3.03s
Test loss: 0.1610 score: 0.9544 time: 2.95s
Epoch 34/1000, LR 0.000298
Train loss: 0.6508;  Loss pred: 0.6508; Loss self: 0.0000; time: 7.77s
Val loss: 0.1551 score: 0.9491 time: 2.63s
Test loss: 0.1509 score: 0.9568 time: 2.55s
Epoch 35/1000, LR 0.000298
Train loss: 0.6420;  Loss pred: 0.6420; Loss self: 0.0000; time: 7.60s
Val loss: 0.1472 score: 0.9503 time: 2.56s
Test loss: 0.1423 score: 0.9604 time: 2.60s
Epoch 36/1000, LR 0.000298
Train loss: 0.6343;  Loss pred: 0.6343; Loss self: 0.0000; time: 7.64s
Val loss: 0.1404 score: 0.9521 time: 2.54s
Test loss: 0.1347 score: 0.9621 time: 2.57s
Epoch 37/1000, LR 0.000298
Train loss: 0.6294;  Loss pred: 0.6294; Loss self: 0.0000; time: 7.92s
Val loss: 0.1349 score: 0.9538 time: 2.76s
Test loss: 0.1285 score: 0.9615 time: 3.10s
Epoch 38/1000, LR 0.000298
Train loss: 0.6242;  Loss pred: 0.6242; Loss self: 0.0000; time: 8.46s
Val loss: 0.1298 score: 0.9568 time: 2.98s
Test loss: 0.1228 score: 0.9615 time: 2.86s
Epoch 39/1000, LR 0.000298
Train loss: 0.6190;  Loss pred: 0.6190; Loss self: 0.0000; time: 7.65s
Val loss: 0.1256 score: 0.9574 time: 2.57s
Test loss: 0.1180 score: 0.9627 time: 2.59s
Epoch 40/1000, LR 0.000298
Train loss: 0.6152;  Loss pred: 0.6152; Loss self: 0.0000; time: 7.68s
Val loss: 0.1221 score: 0.9586 time: 2.57s
Test loss: 0.1140 score: 0.9627 time: 2.60s
Epoch 41/1000, LR 0.000298
Train loss: 0.6101;  Loss pred: 0.6101; Loss self: 0.0000; time: 7.82s
Val loss: 0.1189 score: 0.9586 time: 2.60s
Test loss: 0.1103 score: 0.9627 time: 2.58s
Epoch 42/1000, LR 0.000298
Train loss: 0.6080;  Loss pred: 0.6080; Loss self: 0.0000; time: 7.75s
Val loss: 0.1161 score: 0.9586 time: 2.76s
Test loss: 0.1071 score: 0.9633 time: 2.60s
Epoch 43/1000, LR 0.000298
Train loss: 0.6058;  Loss pred: 0.6058; Loss self: 0.0000; time: 7.81s
Val loss: 0.1135 score: 0.9609 time: 2.70s
Test loss: 0.1042 score: 0.9633 time: 2.58s
Epoch 44/1000, LR 0.000298
Train loss: 0.6030;  Loss pred: 0.6030; Loss self: 0.0000; time: 7.56s
Val loss: 0.1113 score: 0.9621 time: 2.53s
Test loss: 0.1017 score: 0.9645 time: 2.55s
Epoch 45/1000, LR 0.000298
Train loss: 0.6000;  Loss pred: 0.6000; Loss self: 0.0000; time: 7.72s
Val loss: 0.1092 score: 0.9633 time: 2.55s
Test loss: 0.0995 score: 0.9657 time: 2.60s
Epoch 46/1000, LR 0.000298
Train loss: 0.5980;  Loss pred: 0.5980; Loss self: 0.0000; time: 7.72s
Val loss: 0.1075 score: 0.9698 time: 2.60s
Test loss: 0.0976 score: 0.9686 time: 2.61s
Epoch 47/1000, LR 0.000298
Train loss: 0.5953;  Loss pred: 0.5953; Loss self: 0.0000; time: 8.03s
Val loss: 0.1058 score: 0.9710 time: 2.76s
Test loss: 0.0957 score: 0.9704 time: 2.71s
Epoch 48/1000, LR 0.000298
Train loss: 0.5935;  Loss pred: 0.5935; Loss self: 0.0000; time: 7.67s
Val loss: 0.1041 score: 0.9716 time: 2.56s
Test loss: 0.0939 score: 0.9728 time: 2.67s
Epoch 49/1000, LR 0.000298
Train loss: 0.5909;  Loss pred: 0.5909; Loss self: 0.0000; time: 7.69s
Val loss: 0.1030 score: 0.9722 time: 2.63s
Test loss: 0.0927 score: 0.9722 time: 2.64s
Epoch 50/1000, LR 0.000298
Train loss: 0.5900;  Loss pred: 0.5900; Loss self: 0.0000; time: 8.33s
Val loss: 0.1016 score: 0.9722 time: 3.04s
Test loss: 0.0911 score: 0.9734 time: 3.03s
Epoch 51/1000, LR 0.000298
Train loss: 0.5870;  Loss pred: 0.5870; Loss self: 0.0000; time: 8.05s
Val loss: 0.1007 score: 0.9722 time: 2.71s
Test loss: 0.0902 score: 0.9728 time: 2.64s
Epoch 52/1000, LR 0.000298
Train loss: 0.5866;  Loss pred: 0.5866; Loss self: 0.0000; time: 7.68s
Val loss: 0.0998 score: 0.9722 time: 2.67s
Test loss: 0.0892 score: 0.9734 time: 2.58s
Epoch 53/1000, LR 0.000298
Train loss: 0.5846;  Loss pred: 0.5846; Loss self: 0.0000; time: 8.07s
Val loss: 0.0986 score: 0.9728 time: 2.93s
Test loss: 0.0879 score: 0.9746 time: 2.95s
Epoch 54/1000, LR 0.000297
Train loss: 0.5826;  Loss pred: 0.5826; Loss self: 0.0000; time: 8.14s
Val loss: 0.0980 score: 0.9722 time: 2.90s
Test loss: 0.0873 score: 0.9751 time: 2.87s
Epoch 55/1000, LR 0.000297
Train loss: 0.5810;  Loss pred: 0.5810; Loss self: 0.0000; time: 7.81s
Val loss: 0.0973 score: 0.9722 time: 2.61s
Test loss: 0.0867 score: 0.9751 time: 2.69s
Epoch 56/1000, LR 0.000297
Train loss: 0.5807;  Loss pred: 0.5807; Loss self: 0.0000; time: 8.63s
Val loss: 0.0965 score: 0.9734 time: 3.00s
Test loss: 0.0858 score: 0.9751 time: 2.98s
Epoch 57/1000, LR 0.000297
Train loss: 0.5776;  Loss pred: 0.5776; Loss self: 0.0000; time: 8.31s
Val loss: 0.0963 score: 0.9722 time: 2.91s
Test loss: 0.0856 score: 0.9751 time: 2.77s
Epoch 58/1000, LR 0.000297
Train loss: 0.5783;  Loss pred: 0.5783; Loss self: 0.0000; time: 7.56s
Val loss: 0.0952 score: 0.9740 time: 2.57s
Test loss: 0.0844 score: 0.9763 time: 2.54s
Epoch 59/1000, LR 0.000297
Train loss: 0.5758;  Loss pred: 0.5758; Loss self: 0.0000; time: 7.64s
Val loss: 0.0951 score: 0.9740 time: 2.52s
Test loss: 0.0843 score: 0.9751 time: 2.61s
Epoch 60/1000, LR 0.000297
Train loss: 0.5750;  Loss pred: 0.5750; Loss self: 0.0000; time: 8.29s
Val loss: 0.0950 score: 0.9728 time: 2.79s
Test loss: 0.0841 score: 0.9751 time: 2.86s
Epoch 61/1000, LR 0.000297
Train loss: 0.5738;  Loss pred: 0.5738; Loss self: 0.0000; time: 8.06s
Val loss: 0.0944 score: 0.9740 time: 2.95s
Test loss: 0.0836 score: 0.9751 time: 3.03s
Epoch 62/1000, LR 0.000297
Train loss: 0.5730;  Loss pred: 0.5730; Loss self: 0.0000; time: 8.02s
Val loss: 0.0940 score: 0.9746 time: 2.63s
Test loss: 0.0831 score: 0.9746 time: 2.58s
Epoch 63/1000, LR 0.000297
Train loss: 0.5710;  Loss pred: 0.5710; Loss self: 0.0000; time: 7.66s
Val loss: 0.0938 score: 0.9746 time: 2.58s
Test loss: 0.0828 score: 0.9746 time: 2.61s
Epoch 64/1000, LR 0.000297
Train loss: 0.5716;  Loss pred: 0.5716; Loss self: 0.0000; time: 7.52s
Val loss: 0.0934 score: 0.9746 time: 2.54s
Test loss: 0.0825 score: 0.9751 time: 2.76s
Epoch 65/1000, LR 0.000297
Train loss: 0.5686;  Loss pred: 0.5686; Loss self: 0.0000; time: 8.04s
Val loss: 0.0934 score: 0.9746 time: 2.65s
Test loss: 0.0824 score: 0.9746 time: 2.66s
     INFO: Early stopping counter 1 of 2
Epoch 66/1000, LR 0.000297
Train loss: 0.5687;  Loss pred: 0.5687; Loss self: 0.0000; time: 7.57s
Val loss: 0.0929 score: 0.9746 time: 2.52s
Test loss: 0.0818 score: 0.9751 time: 2.58s
Epoch 67/1000, LR 0.000297
Train loss: 0.5660;  Loss pred: 0.5660; Loss self: 0.0000; time: 7.72s
Val loss: 0.0927 score: 0.9746 time: 2.54s
Test loss: 0.0816 score: 0.9751 time: 2.60s
Epoch 68/1000, LR 0.000296
Train loss: 0.5654;  Loss pred: 0.5654; Loss self: 0.0000; time: 7.52s
Val loss: 0.0924 score: 0.9746 time: 2.54s
Test loss: 0.0813 score: 0.9751 time: 2.51s
Epoch 69/1000, LR 0.000296
Train loss: 0.5631;  Loss pred: 0.5631; Loss self: 0.0000; time: 7.77s
Val loss: 0.0925 score: 0.9746 time: 2.79s
Test loss: 0.0813 score: 0.9751 time: 2.75s
     INFO: Early stopping counter 1 of 2
Epoch 70/1000, LR 0.000296
Train loss: 0.5614;  Loss pred: 0.5614; Loss self: 0.0000; time: 7.65s
Val loss: 0.0929 score: 0.9746 time: 2.53s
Test loss: 0.0817 score: 0.9751 time: 2.64s
     INFO: Early stopping counter 2 of 2
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 067,   Train_Loss: 0.5654,   Val_Loss: 0.0924,   Val_Precision: 0.9843,   Val_Recall: 0.9645,   Val_accuracy: 0.9743,   Val_Score: 0.9746,   Val_Loss: 0.0924,   Test_Precision: 0.9808,   Test_Recall: 0.9692,   Test_accuracy: 0.9750,   Test_Score: 0.9751,   Test_loss: 0.0813


[2.962061750004068, 3.0839386499719694, 3.3673303119139746, 2.8139037280343473, 2.80905502603855, 2.898895642021671, 2.794384910026565, 3.0066743589704856, 2.7710372689180076, 2.714464638964273, 2.9409477339359, 3.0709727889625356, 2.713530980050564, 2.796701375977136, 3.0248045519692823, 2.9650405660504475, 2.800456113065593, 2.8676369020249695, 3.0303249689750373, 2.9154960609739646, 2.78667945205234, 2.827296565985307, 2.9858155540423468, 2.841761326068081, 2.8214208049466833, 2.7751818110700697, 2.692183414939791, 3.002821251982823, 2.929314394015819, 3.2114196409238502, 2.8315175590105355, 3.2813473170390353, 3.0054445679998025, 2.976236253976822, 2.885106620960869, 2.8511668499559164, 2.8197614329401404, 2.786557283019647, 3.0434639781014994, 2.865573493996635, 3.1533503950340673, 3.0877274150261655, 2.7044905450893566, 2.8696160800755024, 2.7235967270098627, 2.7765303750056773, 2.7822039420716465, 2.358088752022013, 2.7298783489968628, 2.6626191299874336, 3.1780494269914925, 2.6839467100799084, 2.96200671303086, 3.261197690037079, 2.652835274930112, 2.6789296590723097, 3.248675453942269, 2.8985182179603726, 3.1128140370128676, 2.860498477006331, 2.947222071001306, 2.827849260997027, 2.8452753929886967, 2.634863918996416, 2.9678971979301423, 2.957479799981229, 2.69161860703025, 2.854279578081332, 2.990352038992569, 2.873150640982203, 2.5679164100438356, 2.6245760628953576, 2.7977247510571033, 3.3701552980346605, 2.6667943079955876, 2.8299915730021894, 2.709699586033821, 2.9373143329285085, 2.6536165290744975, 2.7524466869654134, 2.672195633989759, 2.8106813119957224, 3.2329044020734727, 2.594071606057696, 2.610559127992019, 2.6405657869763672, 3.556392960017547, 2.882992738042958, 2.954995395964943, 2.8788430109852925, 2.734420010005124, 2.6215758520411327, 2.707978207967244, 2.762545407982543, 2.627912699012086, 2.8320040890248492, 2.6892650850350037, 2.699628234957345, 2.7014681360451505, 2.793845487991348, 2.7440692760283127, 3.0771824680268764, 2.663907177047804, 2.962084955070168, 2.9531202759826556, 2.557168892933987, 2.6014734740601853, 2.5754697689553723, 3.10162842401769, 2.864785760990344, 2.5981639510719106, 2.6033489879919216, 2.588097773026675, 2.606668662978336, 2.585831211064942, 2.5592343530151993, 2.6005928789963946, 2.619162596995011, 2.7125509219476953, 2.6804676559986547, 2.6469862130470574, 3.0352648239349946, 2.6481462669325992, 2.584841944044456, 2.9541506549576297, 2.8713775869691744, 2.6910053830360994, 2.988521831925027, 2.77131326997187, 2.550092611927539, 2.619955969043076, 2.8655453349929303, 3.039167455979623, 2.5841449920553714, 2.622671141056344, 2.765168947982602, 2.6622572739142925, 2.582703761989251, 2.6016863860422745, 2.511940697906539, 2.75445787794888, 2.6464552230900154]
[0.0017526992603574367, 0.001824815769214183, 0.0019925031431443636, 0.0016650317917363002, 0.0016621627373009172, 0.0017153228651015805, 0.0016534821952819913, 0.0017790972538286897, 0.001639667023028407, 0.0016061920940617, 0.0017402057597253847, 0.0018171436621080093, 0.001605639633166014, 0.0016548528851935717, 0.0017898251786800487, 0.0017544618734026316, 0.0016570746231157356, 0.001696826569245544, 0.0017930916976183653, 0.0017251455982094466, 0.0016489227526936922, 0.0016729565479203, 0.0017667547657055306, 0.0016815155775550775, 0.001669479766240641, 0.0016421194148343607, 0.0015930079378341957, 0.0017768173088655757, 0.0017333221266365793, 0.0019002483082389647, 0.0016754541769293109, 0.0019416256313840444, 0.0017783695668637885, 0.0017610865408146876, 0.0017071636810419343, 0.001687080976305276, 0.0016684978893136925, 0.0016488504633252348, 0.0018008662592316565, 0.0016956056177494883, 0.0018658878077124658, 0.001827057642027317, 0.0016002902633664832, 0.0016979976805180487, 0.0016115956964555401, 0.001642917381660164, 0.0016462745219358854, 0.0013953187881787059, 0.0016153126325425223, 0.0015755142780990731, 0.0018805026195215931, 0.0015881341479762772, 0.0017526666941011005, 0.0019297027751698694, 0.0015697250147515456, 0.0015851654787410116, 0.001922293168013177, 0.0017150995372546584, 0.0018419017970490342, 0.0016926026491161721, 0.0017439183852078735, 0.0016732835863887735, 0.0016835949070939033, 0.0015590910763292403, 0.0017561521881243446, 0.001749988047326171, 0.0015926737319705622, 0.0016889228272670604, 0.0017694390763269639, 0.00170008913667586, 0.0015194771657064117, 0.001553003587512046, 0.0016554584325781676, 0.0019941747325648877, 0.0015779847976305252, 0.0016745512266285144, 0.001603372536114687, 0.0017380558183008926, 0.0015701872953103537, 0.0016286666786777594, 0.0015811808485146502, 0.0016631250366838594, 0.0019129611846588595, 0.001534953613051891, 0.0015447095431905438, 0.0015624649627079097, 0.0021043745325547616, 0.0017059128627473126, 0.0017485179857780727, 0.0017034574029498773, 0.0016180000059201917, 0.0015512283148172382, 0.001602353969211387, 0.0016346422532441084, 0.0015549779284095182, 0.0016757420645117452, 0.0015912811154053276, 0.001597413156779494, 0.001598501855648018, 0.001653163010645768, 0.0016237096307859839, 0.00182081802841827, 0.001576276436122961, 0.0017527129911657797, 0.001747408447327015, 0.0015131176881266195, 0.00153933341660366, 0.0015239466088493327, 0.0018352830911347279, 0.0016951395035445821, 0.001537375118977462, 0.0015404431881609003, 0.0015314188006074999, 0.0015424074928865894, 0.0015300776396833977, 0.0015143398538551476, 0.0015388123544357365, 0.0015498003532514858, 0.0016050597171288138, 0.0015860755360938784, 0.0015662640313887915, 0.0017960146887189318, 0.0015669504538062718, 0.0015294922745825183, 0.0017480181390281833, 0.0016990399922894523, 0.0015923108775361535, 0.0017683561135651048, 0.0016398303372614615, 0.0015089305396020943, 0.0015502698041675006, 0.0016955889556171185, 0.001798323938449481, 0.001529079876955841, 0.001551876414826239, 0.0016361946437766874, 0.001575300162079463, 0.0015282270781001484, 0.0015394594000250146, 0.0014863554425482478, 0.0016298567325141304, 0.001565949836147938]
[570.5485376858463, 548.0005252424074, 501.8812660048821, 600.5891328700679, 601.6258080865402, 582.9806273472491, 604.7842564337114, 562.0828191645843, 609.8799243721054, 622.5905380166727, 574.6446903829385, 550.3142216284278, 622.804756026227, 604.2833226731377, 558.7137849616547, 569.9753383985392, 603.4731242940269, 589.3354206756837, 557.6959624140964, 579.6612187620074, 606.4565476862956, 597.7441561426855, 566.0095104374392, 594.7015973851395, 598.9889905954444, 608.9691108736262, 627.7432624469964, 562.8040626407779, 576.9268069867934, 526.2470150160216, 596.8530884161513, 515.0323439473614, 562.3128165443878, 567.8312659963859, 585.7669133340919, 592.7397760064902, 599.3414833813979, 606.4831361258201, 555.2883202035515, 589.7597823055465, 535.9379035902358, 547.328106676696, 624.8866364382732, 588.9289552473982, 620.5030220664823, 608.673333889439, 607.4321060524468, 716.6821005150291, 619.0752055383778, 634.7133846394231, 531.7727237489324, 629.6697298992513, 570.5591390340622, 518.21452135911, 637.0542551099493, 630.8489639796039, 520.2120137760096, 583.0565388646128, 542.9171096972325, 590.806117739548, 573.4213300817979, 597.6273287650945, 593.9671091819384, 641.3993481089142, 569.4267312151621, 571.4324743691323, 627.8749877809144, 592.0933649870524, 565.1508511249901, 588.2044525943352, 658.1211107144842, 643.9135157453353, 604.0622828823471, 501.46057096702344, 633.7196666923426, 597.1749231066324, 623.6853740947895, 575.355514748422, 636.866699270004, 613.9991768063031, 632.4387251081322, 601.2777018821877, 522.7497599112713, 651.4854856178599, 647.3708953299627, 640.0143515966586, 475.20058075687496, 586.1964123944381, 571.9129046047589, 587.0413890410761, 618.0469693084323, 644.6504298871165, 624.0818316143711, 611.7546503006402, 643.0959447912115, 596.7505508023196, 628.4244752978685, 626.0121220086079, 625.5857611091787, 604.9010252227785, 615.8736642560488, 549.2037009698832, 634.4064892955063, 570.5440679907731, 572.2760477263831, 660.887125864012, 649.6318401288077, 656.1909677105141, 544.8750685005849, 589.9219491428128, 650.4593366029752, 649.163830049375, 652.9892408290332, 648.337099379955, 653.5616063292834, 660.3537491628705, 649.8518140418022, 645.2444006106961, 623.0297784738094, 630.4869958859329, 638.4619578560516, 556.7883193167445, 638.1822715395402, 653.8117364946838, 572.0764434149141, 588.5676644094189, 628.0180673935608, 565.4969563703683, 609.8191851176588, 662.7210290698341, 645.0490084446965, 589.7655777287396, 556.073340636394, 653.9880715655245, 644.3812087394655, 611.1742290585831, 634.7996553748574, 654.3530175130608, 649.5786767639024, 672.7865834605302, 613.5508600547075, 638.5900601132208]
Elapsed: 2.821910109960402~0.19880524518116144
Time per graph: 0.0016697692958345574~0.00011763623975216653
Speed: 601.7352718520114~40.64946345434984
Total Time: 2.6471
best val loss: 0.09236182471042907 test_score: 0.9751

Testing...
Test loss: 0.0831 score: 0.9746 time: 2.62s
test Score 0.9746
Epoch Time List: [16.007859014091082, 15.157085902057588, 16.554072922095656, 13.908855348010547, 13.97904615092557, 14.506266833981499, 14.214631369919516, 14.822081824066117, 14.592174863093533, 13.65768329706043, 14.374243730097078, 15.529351482982747, 13.664574267109856, 14.0137144329492, 14.671796111972071, 15.416175891063176, 14.324531925027259, 14.160469675902277, 14.334310073172674, 14.91405500494875, 14.65773291804362, 14.039608516031876, 14.587813291931525, 14.192109994008206, 14.398914911900647, 14.001702500972897, 13.954592054942623, 14.403301642043516, 14.949595038895495, 15.638867919915356, 13.87005196500104, 14.995408641174436, 15.231365607120097, 14.490703088813461, 14.642232132027857, 14.163644325104542, 14.322027236921713, 14.15334254608024, 14.684480835217983, 14.449759871000424, 15.762286799843423, 15.454190404969268, 13.826963388943113, 13.907815029029734, 14.128398102940992, 14.172601917991415, 13.928043741965666, 12.600569253088906, 13.062139335903339, 13.494248122908175, 15.46742716501467, 14.061198369949125, 14.231202763970941, 15.348347980994731, 14.268895176006481, 14.17132538498845, 14.668313393951394, 15.010024868883193, 14.905146228033118, 14.494733839994296, 13.868067602976225, 14.108370709931478, 14.460939491982572, 13.211337042041123, 14.094709369004704, 15.04079029802233, 13.563642238033935, 13.964157660026103, 14.038318531122059, 14.15391842101235, 13.401441704016179, 13.310620654025115, 14.055490881903097, 14.854030532995239, 13.723624909063801, 13.162792467977852, 13.176086535095237, 13.557487830054015, 13.763868040055968, 12.876754575991072, 13.12766861799173, 13.39931075682398, 15.12118836294394, 13.109556576935574, 12.985169371822849, 13.06829717499204, 14.861532666021958, 14.43083586206194, 14.283359126071446, 14.536283663008362, 13.096270092995837, 13.132960960036144, 13.123478914028965, 13.572966918116435, 12.898424147046171, 13.356906877015717, 13.432050221948884, 13.308815989177674, 13.089401938021183, 13.199661995051429, 13.299614473129623, 14.385238592047244, 13.32962863415014, 14.054592464934103, 14.41413032100536, 12.949034669087268, 12.748120732954703, 12.743358527775854, 13.78572942793835, 14.299435007036664, 12.806715731043369, 12.853972928947769, 12.998854846926406, 13.113461094093509, 13.092823756160215, 12.643776992917992, 12.860488653182983, 12.924375009024516, 13.492882444057614, 12.896204823977314, 12.959997426951304, 14.399732204969041, 13.397545082028955, 12.925259865936823, 13.943607884924859, 13.904178949887864, 13.10832111898344, 14.606604113942012, 13.98300888785161, 12.678760237060487, 12.77385491807945, 13.943897015065886, 14.045613634982146, 13.233944061910734, 12.850825592060573, 12.823278346913867, 13.344905106001534, 12.669297305983491, 12.851287882891484, 12.563983685104176, 13.312109149061143, 12.814568166038953]
Total Epoch List: [72, 70]
Total Time List: [2.625078661949374, 2.6471305689774454]
========================k_idx:2========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: AEtransGAT
AEtransGAT(
  (gae): GAE(
    (encoder): transGAT(
      (encoder): EdgeFeatureGAT()
      (classifier): Sequential(
        (0): Linear(in_features=64, out_features=64, bias=True)
        (1): ReLU(inplace=True)
        (2): Linear(in_features=64, out_features=2, bias=True)
      )
    )
    (decoder): InnerProductDecoder()
  )
)
Total number of parameters: 1914242
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x72c48b90e740>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 2.8484;  Loss pred: 2.8484; Loss self: 0.0000; time: 8.48s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6938 score: 0.5000 time: 2.94s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6938 score: 0.5000 time: 3.09s
Epoch 2/1000, LR 0.000029
Train loss: 2.7288;  Loss pred: 2.7288; Loss self: 0.0000; time: 8.54s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6937 score: 0.5000 time: 2.88s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6937 score: 0.5000 time: 2.84s
Epoch 3/1000, LR 0.000059
Train loss: 2.5175;  Loss pred: 2.5175; Loss self: 0.0000; time: 8.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6935 score: 0.5000 time: 2.80s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6935 score: 0.5000 time: 2.72s
Epoch 4/1000, LR 0.000089
Train loss: 2.2421;  Loss pred: 2.2421; Loss self: 0.0000; time: 8.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6932 score: 0.5000 time: 2.76s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6932 score: 0.5000 time: 2.71s
Epoch 5/1000, LR 0.000119
Train loss: 1.9466;  Loss pred: 1.9466; Loss self: 0.0000; time: 8.10s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6929 score: 0.5000 time: 2.73s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6928 score: 0.5000 time: 2.69s
Epoch 6/1000, LR 0.000149
Train loss: 1.6690;  Loss pred: 1.6690; Loss self: 0.0000; time: 8.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6924 score: 0.5000 time: 2.69s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6924 score: 0.5000 time: 2.77s
Epoch 7/1000, LR 0.000179
Train loss: 1.4405;  Loss pred: 1.4405; Loss self: 0.0000; time: 7.93s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6918 score: 0.5000 time: 2.67s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6917 score: 0.5000 time: 2.69s
Epoch 8/1000, LR 0.000209
Train loss: 1.2652;  Loss pred: 1.2652; Loss self: 0.0000; time: 8.52s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6916 score: 0.5000 time: 2.79s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6915 score: 0.5000 time: 2.88s
Epoch 9/1000, LR 0.000239
Train loss: 1.1544;  Loss pred: 1.1544; Loss self: 0.0000; time: 8.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6905 score: 0.5000 time: 2.76s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6904 score: 0.5000 time: 2.73s
Epoch 10/1000, LR 0.000269
Train loss: 1.0834;  Loss pred: 1.0834; Loss self: 0.0000; time: 7.94s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6891 score: 0.5000 time: 2.60s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6890 score: 0.5000 time: 2.71s
Epoch 11/1000, LR 0.000299
Train loss: 1.0411;  Loss pred: 1.0411; Loss self: 0.0000; time: 8.45s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6869 score: 0.5000 time: 3.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6867 score: 0.5000 time: 3.04s
Epoch 12/1000, LR 0.000299
Train loss: 1.0164;  Loss pred: 1.0164; Loss self: 0.0000; time: 8.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6837 score: 0.5000 time: 2.97s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6835 score: 0.5000 time: 2.65s
Epoch 13/1000, LR 0.000299
Train loss: 0.9998;  Loss pred: 0.9998; Loss self: 0.0000; time: 8.17s
Val loss: 0.6789 score: 0.5024 time: 2.81s
Test loss: 0.6787 score: 0.5006 time: 2.74s
Epoch 14/1000, LR 0.000299
Train loss: 0.9885;  Loss pred: 0.9885; Loss self: 0.0000; time: 7.90s
Val loss: 0.6712 score: 0.5367 time: 2.59s
Test loss: 0.6708 score: 0.5385 time: 2.61s
Epoch 15/1000, LR 0.000299
Train loss: 0.9776;  Loss pred: 0.9776; Loss self: 0.0000; time: 8.30s
Val loss: 0.6601 score: 0.6414 time: 2.69s
Test loss: 0.6595 score: 0.6367 time: 2.76s
Epoch 16/1000, LR 0.000299
Train loss: 0.9673;  Loss pred: 0.9673; Loss self: 0.0000; time: 8.19s
Val loss: 0.6437 score: 0.7385 time: 3.00s
Test loss: 0.6430 score: 0.7385 time: 3.07s
Epoch 17/1000, LR 0.000299
Train loss: 0.9553;  Loss pred: 0.9553; Loss self: 0.0000; time: 8.33s
Val loss: 0.6220 score: 0.8006 time: 2.82s
Test loss: 0.6210 score: 0.8036 time: 2.77s
Epoch 18/1000, LR 0.000299
Train loss: 0.9406;  Loss pred: 0.9406; Loss self: 0.0000; time: 8.81s
Val loss: 0.5931 score: 0.8302 time: 2.98s
Test loss: 0.5918 score: 0.8337 time: 3.20s
Epoch 19/1000, LR 0.000299
Train loss: 0.9230;  Loss pred: 0.9230; Loss self: 0.0000; time: 8.48s
Val loss: 0.5592 score: 0.8456 time: 3.09s
Test loss: 0.5574 score: 0.8627 time: 2.81s
Epoch 20/1000, LR 0.000299
Train loss: 0.9043;  Loss pred: 0.9043; Loss self: 0.0000; time: 8.72s
Val loss: 0.5208 score: 0.8805 time: 3.00s
Test loss: 0.5187 score: 0.8870 time: 3.30s
Epoch 21/1000, LR 0.000299
Train loss: 0.8806;  Loss pred: 0.8806; Loss self: 0.0000; time: 8.85s
Val loss: 0.4792 score: 0.8947 time: 2.99s
Test loss: 0.4768 score: 0.8964 time: 2.99s
Epoch 22/1000, LR 0.000299
Train loss: 0.8570;  Loss pred: 0.8570; Loss self: 0.0000; time: 8.36s
Val loss: 0.4366 score: 0.9059 time: 2.95s
Test loss: 0.4338 score: 0.9112 time: 2.51s
Epoch 23/1000, LR 0.000299
Train loss: 0.8320;  Loss pred: 0.8320; Loss self: 0.0000; time: 7.30s
Val loss: 0.3939 score: 0.9178 time: 2.40s
Test loss: 0.3910 score: 0.9249 time: 2.18s
Epoch 24/1000, LR 0.000299
Train loss: 0.8056;  Loss pred: 0.8056; Loss self: 0.0000; time: 6.29s
Val loss: 0.3537 score: 0.9254 time: 2.07s
Test loss: 0.3507 score: 0.9331 time: 2.08s
Epoch 25/1000, LR 0.000299
Train loss: 0.7823;  Loss pred: 0.7823; Loss self: 0.0000; time: 6.32s
Val loss: 0.3159 score: 0.9325 time: 2.14s
Test loss: 0.3129 score: 0.9367 time: 2.20s
Epoch 26/1000, LR 0.000299
Train loss: 0.7586;  Loss pred: 0.7586; Loss self: 0.0000; time: 6.47s
Val loss: 0.2829 score: 0.9308 time: 2.10s
Test loss: 0.2800 score: 0.9402 time: 2.21s
Epoch 27/1000, LR 0.000299
Train loss: 0.7372;  Loss pred: 0.7372; Loss self: 0.0000; time: 6.46s
Val loss: 0.2549 score: 0.9325 time: 2.09s
Test loss: 0.2524 score: 0.9420 time: 2.11s
Epoch 28/1000, LR 0.000299
Train loss: 0.7192;  Loss pred: 0.7192; Loss self: 0.0000; time: 6.89s
Val loss: 0.2314 score: 0.9361 time: 2.17s
Test loss: 0.2293 score: 0.9473 time: 2.33s
Epoch 29/1000, LR 0.000299
Train loss: 0.7031;  Loss pred: 0.7031; Loss self: 0.0000; time: 7.04s
Val loss: 0.2112 score: 0.9402 time: 2.31s
Test loss: 0.2098 score: 0.9503 time: 2.26s
Epoch 30/1000, LR 0.000299
Train loss: 0.6889;  Loss pred: 0.6889; Loss self: 0.0000; time: 6.15s
Val loss: 0.1945 score: 0.9444 time: 2.01s
Test loss: 0.1937 score: 0.9538 time: 1.99s
Epoch 31/1000, LR 0.000299
Train loss: 0.6769;  Loss pred: 0.6769; Loss self: 0.0000; time: 6.36s
Val loss: 0.1806 score: 0.9473 time: 2.18s
Test loss: 0.1805 score: 0.9556 time: 2.12s
Epoch 32/1000, LR 0.000299
Train loss: 0.6663;  Loss pred: 0.6663; Loss self: 0.0000; time: 6.60s
Val loss: 0.1684 score: 0.9503 time: 2.05s
Test loss: 0.1691 score: 0.9556 time: 2.03s
Epoch 33/1000, LR 0.000299
Train loss: 0.6582;  Loss pred: 0.6582; Loss self: 0.0000; time: 7.26s
Val loss: 0.1580 score: 0.9538 time: 2.13s
Test loss: 0.1596 score: 0.9586 time: 2.30s
Epoch 34/1000, LR 0.000298
Train loss: 0.6500;  Loss pred: 0.6500; Loss self: 0.0000; time: 7.09s
Val loss: 0.1491 score: 0.9574 time: 2.54s
Test loss: 0.1516 score: 0.9592 time: 2.36s
Epoch 35/1000, LR 0.000298
Train loss: 0.6429;  Loss pred: 0.6429; Loss self: 0.0000; time: 6.60s
Val loss: 0.1415 score: 0.9586 time: 2.11s
Test loss: 0.1449 score: 0.9592 time: 2.10s
Epoch 36/1000, LR 0.000298
Train loss: 0.6368;  Loss pred: 0.6368; Loss self: 0.0000; time: 6.35s
Val loss: 0.1349 score: 0.9609 time: 2.24s
Test loss: 0.1393 score: 0.9609 time: 2.25s
Epoch 37/1000, LR 0.000298
Train loss: 0.6319;  Loss pred: 0.6319; Loss self: 0.0000; time: 6.27s
Val loss: 0.1292 score: 0.9627 time: 2.18s
Test loss: 0.1345 score: 0.9609 time: 2.13s
Epoch 38/1000, LR 0.000298
Train loss: 0.6282;  Loss pred: 0.6282; Loss self: 0.0000; time: 6.60s
Val loss: 0.1240 score: 0.9627 time: 2.16s
Test loss: 0.1302 score: 0.9609 time: 2.04s
Epoch 39/1000, LR 0.000298
Train loss: 0.6236;  Loss pred: 0.6236; Loss self: 0.0000; time: 6.60s
Val loss: 0.1194 score: 0.9633 time: 2.08s
Test loss: 0.1265 score: 0.9639 time: 2.07s
Epoch 40/1000, LR 0.000298
Train loss: 0.6199;  Loss pred: 0.6199; Loss self: 0.0000; time: 6.66s
Val loss: 0.1154 score: 0.9633 time: 2.01s
Test loss: 0.1233 score: 0.9633 time: 2.13s
Epoch 41/1000, LR 0.000298
Train loss: 0.6173;  Loss pred: 0.6173; Loss self: 0.0000; time: 6.81s
Val loss: 0.1115 score: 0.9645 time: 2.56s
Test loss: 0.1202 score: 0.9651 time: 2.41s
Epoch 42/1000, LR 0.000298
Train loss: 0.6126;  Loss pred: 0.6126; Loss self: 0.0000; time: 6.83s
Val loss: 0.1084 score: 0.9645 time: 2.08s
Test loss: 0.1178 score: 0.9657 time: 2.01s
Epoch 43/1000, LR 0.000298
Train loss: 0.6120;  Loss pred: 0.6120; Loss self: 0.0000; time: 6.23s
Val loss: 0.1053 score: 0.9651 time: 1.97s
Test loss: 0.1153 score: 0.9657 time: 1.95s
Epoch 44/1000, LR 0.000298
Train loss: 0.6095;  Loss pred: 0.6095; Loss self: 0.0000; time: 6.16s
Val loss: 0.1022 score: 0.9669 time: 1.96s
Test loss: 0.1130 score: 0.9663 time: 1.95s
Epoch 45/1000, LR 0.000298
Train loss: 0.6051;  Loss pred: 0.6051; Loss self: 0.0000; time: 6.34s
Val loss: 0.1000 score: 0.9663 time: 2.03s
Test loss: 0.1114 score: 0.9663 time: 2.13s
Epoch 46/1000, LR 0.000298
Train loss: 0.6046;  Loss pred: 0.6046; Loss self: 0.0000; time: 6.52s
Val loss: 0.0974 score: 0.9675 time: 1.99s
Test loss: 0.1095 score: 0.9675 time: 2.01s
Epoch 47/1000, LR 0.000298
Train loss: 0.6017;  Loss pred: 0.6017; Loss self: 0.0000; time: 6.24s
Val loss: 0.0953 score: 0.9675 time: 2.07s
Test loss: 0.1079 score: 0.9669 time: 2.23s
Epoch 48/1000, LR 0.000298
Train loss: 0.5991;  Loss pred: 0.5991; Loss self: 0.0000; time: 6.37s
Val loss: 0.0936 score: 0.9740 time: 1.98s
Test loss: 0.1067 score: 0.9728 time: 1.98s
Epoch 49/1000, LR 0.000298
Train loss: 0.5976;  Loss pred: 0.5976; Loss self: 0.0000; time: 6.09s
Val loss: 0.0913 score: 0.9757 time: 1.96s
Test loss: 0.1049 score: 0.9740 time: 1.98s
Epoch 50/1000, LR 0.000298
Train loss: 0.5952;  Loss pred: 0.5952; Loss self: 0.0000; time: 6.48s
Val loss: 0.0898 score: 0.9757 time: 2.16s
Test loss: 0.1039 score: 0.9740 time: 2.17s
Epoch 51/1000, LR 0.000298
Train loss: 0.5940;  Loss pred: 0.5940; Loss self: 0.0000; time: 6.38s
Val loss: 0.0884 score: 0.9757 time: 1.85s
Test loss: 0.1029 score: 0.9740 time: 1.94s
Epoch 52/1000, LR 0.000298
Train loss: 0.5935;  Loss pred: 0.5935; Loss self: 0.0000; time: 6.49s
Val loss: 0.0870 score: 0.9751 time: 2.16s
Test loss: 0.1020 score: 0.9740 time: 2.09s
Epoch 53/1000, LR 0.000298
Train loss: 0.5907;  Loss pred: 0.5907; Loss self: 0.0000; time: 6.40s
Val loss: 0.0857 score: 0.9751 time: 2.04s
Test loss: 0.1010 score: 0.9740 time: 2.15s
Epoch 54/1000, LR 0.000297
Train loss: 0.5887;  Loss pred: 0.5887; Loss self: 0.0000; time: 6.59s
Val loss: 0.0847 score: 0.9751 time: 2.11s
Test loss: 0.1004 score: 0.9740 time: 2.11s
Epoch 55/1000, LR 0.000297
Train loss: 0.5880;  Loss pred: 0.5880; Loss self: 0.0000; time: 6.11s
Val loss: 0.0833 score: 0.9751 time: 1.95s
Test loss: 0.0994 score: 0.9740 time: 1.98s
Epoch 56/1000, LR 0.000297
Train loss: 0.5841;  Loss pred: 0.5841; Loss self: 0.0000; time: 6.12s
Val loss: 0.0824 score: 0.9757 time: 1.97s
Test loss: 0.0988 score: 0.9740 time: 1.97s
Epoch 57/1000, LR 0.000297
Train loss: 0.5835;  Loss pred: 0.5835; Loss self: 0.0000; time: 6.33s
Val loss: 0.0811 score: 0.9757 time: 1.96s
Test loss: 0.0978 score: 0.9740 time: 2.01s
Epoch 58/1000, LR 0.000297
Train loss: 0.5822;  Loss pred: 0.5822; Loss self: 0.0000; time: 6.21s
Val loss: 0.0806 score: 0.9757 time: 2.08s
Test loss: 0.0975 score: 0.9740 time: 2.26s
Epoch 59/1000, LR 0.000297
Train loss: 0.5812;  Loss pred: 0.5812; Loss self: 0.0000; time: 7.05s
Val loss: 0.0793 score: 0.9757 time: 2.31s
Test loss: 0.0965 score: 0.9740 time: 2.16s
Epoch 60/1000, LR 0.000297
Train loss: 0.5788;  Loss pred: 0.5788; Loss self: 0.0000; time: 6.35s
Val loss: 0.0786 score: 0.9757 time: 1.95s
Test loss: 0.0961 score: 0.9740 time: 1.99s
Epoch 61/1000, LR 0.000297
Train loss: 0.5765;  Loss pred: 0.5765; Loss self: 0.0000; time: 6.73s
Val loss: 0.0779 score: 0.9757 time: 2.16s
Test loss: 0.0957 score: 0.9740 time: 2.14s
Epoch 62/1000, LR 0.000297
Train loss: 0.5761;  Loss pred: 0.5761; Loss self: 0.0000; time: 6.71s
Val loss: 0.0771 score: 0.9763 time: 2.18s
Test loss: 0.0951 score: 0.9734 time: 2.07s
Epoch 63/1000, LR 0.000297
Train loss: 0.5751;  Loss pred: 0.5751; Loss self: 0.0000; time: 7.10s
Val loss: 0.0769 score: 0.9757 time: 2.11s
Test loss: 0.0951 score: 0.9734 time: 2.29s
Epoch 64/1000, LR 0.000297
Train loss: 0.5740;  Loss pred: 0.5740; Loss self: 0.0000; time: 6.53s
Val loss: 0.0759 score: 0.9757 time: 2.58s
Test loss: 0.0943 score: 0.9734 time: 2.57s
Epoch 65/1000, LR 0.000297
Train loss: 0.5724;  Loss pred: 0.5724; Loss self: 0.0000; time: 6.72s
Val loss: 0.0755 score: 0.9769 time: 1.99s
Test loss: 0.0941 score: 0.9740 time: 2.07s
Epoch 66/1000, LR 0.000297
Train loss: 0.5711;  Loss pred: 0.5711; Loss self: 0.0000; time: 6.20s
Val loss: 0.0749 score: 0.9769 time: 2.01s
Test loss: 0.0938 score: 0.9740 time: 1.99s
Epoch 67/1000, LR 0.000297
Train loss: 0.5691;  Loss pred: 0.5691; Loss self: 0.0000; time: 6.08s
Val loss: 0.0745 score: 0.9769 time: 1.99s
Test loss: 0.0936 score: 0.9740 time: 2.00s
Epoch 68/1000, LR 0.000296
Train loss: 0.5690;  Loss pred: 0.5690; Loss self: 0.0000; time: 6.51s
Val loss: 0.0744 score: 0.9769 time: 2.09s
Test loss: 0.0936 score: 0.9740 time: 1.99s
Epoch 69/1000, LR 0.000296
Train loss: 0.5669;  Loss pred: 0.5669; Loss self: 0.0000; time: 6.15s
Val loss: 0.0735 score: 0.9769 time: 2.01s
Test loss: 0.0928 score: 0.9740 time: 2.02s
Epoch 70/1000, LR 0.000296
Train loss: 0.5648;  Loss pred: 0.5648; Loss self: 0.0000; time: 6.60s
Val loss: 0.0737 score: 0.9769 time: 2.08s
Test loss: 0.0932 score: 0.9740 time: 2.11s
     INFO: Early stopping counter 1 of 2
Epoch 71/1000, LR 0.000296
Train loss: 0.5645;  Loss pred: 0.5645; Loss self: 0.0000; time: 6.46s
Val loss: 0.0726 score: 0.9781 time: 2.05s
Test loss: 0.0923 score: 0.9740 time: 1.98s
Epoch 72/1000, LR 0.000296
Train loss: 0.5626;  Loss pred: 0.5626; Loss self: 0.0000; time: 6.49s
Val loss: 0.0729 score: 0.9775 time: 2.00s
Test loss: 0.0927 score: 0.9740 time: 2.00s
     INFO: Early stopping counter 1 of 2
Epoch 73/1000, LR 0.000296
Train loss: 0.5620;  Loss pred: 0.5620; Loss self: 0.0000; time: 6.18s
Val loss: 0.0721 score: 0.9769 time: 2.01s
Test loss: 0.0921 score: 0.9740 time: 2.01s
Epoch 74/1000, LR 0.000296
Train loss: 0.5600;  Loss pred: 0.5600; Loss self: 0.0000; time: 6.08s
Val loss: 0.0725 score: 0.9775 time: 2.00s
Test loss: 0.0926 score: 0.9734 time: 2.09s
     INFO: Early stopping counter 1 of 2
Epoch 75/1000, LR 0.000296
Train loss: 0.5601;  Loss pred: 0.5601; Loss self: 0.0000; time: 6.63s
Val loss: 0.0721 score: 0.9775 time: 2.11s
Test loss: 0.0922 score: 0.9734 time: 2.03s
Epoch 76/1000, LR 0.000296
Train loss: 0.5573;  Loss pred: 0.5573; Loss self: 0.0000; time: 6.51s
Val loss: 0.0716 score: 0.9769 time: 2.11s
Test loss: 0.0919 score: 0.9734 time: 2.15s
Epoch 77/1000, LR 0.000296
Train loss: 0.5563;  Loss pred: 0.5563; Loss self: 0.0000; time: 6.31s
Val loss: 0.0718 score: 0.9775 time: 2.05s
Test loss: 0.0923 score: 0.9728 time: 2.07s
     INFO: Early stopping counter 1 of 2
Epoch 78/1000, LR 0.000296
Train loss: 0.5556;  Loss pred: 0.5556; Loss self: 0.0000; time: 6.68s
Val loss: 0.0713 score: 0.9775 time: 2.19s
Test loss: 0.0919 score: 0.9728 time: 2.22s
Epoch 79/1000, LR 0.000295
Train loss: 0.5540;  Loss pred: 0.5540; Loss self: 0.0000; time: 7.13s
Val loss: 0.0711 score: 0.9775 time: 2.25s
Test loss: 0.0919 score: 0.9728 time: 2.30s
Epoch 80/1000, LR 0.000295
Train loss: 0.5514;  Loss pred: 0.5514; Loss self: 0.0000; time: 6.77s
Val loss: 0.0710 score: 0.9769 time: 2.28s
Test loss: 0.0920 score: 0.9728 time: 2.25s
Epoch 81/1000, LR 0.000295
Train loss: 0.5511;  Loss pred: 0.5511; Loss self: 0.0000; time: 6.62s
Val loss: 0.0715 score: 0.9769 time: 2.13s
Test loss: 0.0927 score: 0.9722 time: 2.15s
     INFO: Early stopping counter 1 of 2
Epoch 82/1000, LR 0.000295
Train loss: 0.5503;  Loss pred: 0.5503; Loss self: 0.0000; time: 6.72s
Val loss: 0.0710 score: 0.9769 time: 2.16s
Test loss: 0.0921 score: 0.9728 time: 2.16s
     INFO: Early stopping counter 2 of 2
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 079,   Train_Loss: 0.5514,   Val_Loss: 0.0710,   Val_Precision: 0.9855,   Val_Recall: 0.9680,   Val_accuracy: 0.9767,   Val_Score: 0.9769,   Val_Loss: 0.0710,   Test_Precision: 0.9842,   Test_Recall: 0.9609,   Test_accuracy: 0.9725,   Test_Score: 0.9728,   Test_loss: 0.0920


[2.962061750004068, 3.0839386499719694, 3.3673303119139746, 2.8139037280343473, 2.80905502603855, 2.898895642021671, 2.794384910026565, 3.0066743589704856, 2.7710372689180076, 2.714464638964273, 2.9409477339359, 3.0709727889625356, 2.713530980050564, 2.796701375977136, 3.0248045519692823, 2.9650405660504475, 2.800456113065593, 2.8676369020249695, 3.0303249689750373, 2.9154960609739646, 2.78667945205234, 2.827296565985307, 2.9858155540423468, 2.841761326068081, 2.8214208049466833, 2.7751818110700697, 2.692183414939791, 3.002821251982823, 2.929314394015819, 3.2114196409238502, 2.8315175590105355, 3.2813473170390353, 3.0054445679998025, 2.976236253976822, 2.885106620960869, 2.8511668499559164, 2.8197614329401404, 2.786557283019647, 3.0434639781014994, 2.865573493996635, 3.1533503950340673, 3.0877274150261655, 2.7044905450893566, 2.8696160800755024, 2.7235967270098627, 2.7765303750056773, 2.7822039420716465, 2.358088752022013, 2.7298783489968628, 2.6626191299874336, 3.1780494269914925, 2.6839467100799084, 2.96200671303086, 3.261197690037079, 2.652835274930112, 2.6789296590723097, 3.248675453942269, 2.8985182179603726, 3.1128140370128676, 2.860498477006331, 2.947222071001306, 2.827849260997027, 2.8452753929886967, 2.634863918996416, 2.9678971979301423, 2.957479799981229, 2.69161860703025, 2.854279578081332, 2.990352038992569, 2.873150640982203, 2.5679164100438356, 2.6245760628953576, 2.7977247510571033, 3.3701552980346605, 2.6667943079955876, 2.8299915730021894, 2.709699586033821, 2.9373143329285085, 2.6536165290744975, 2.7524466869654134, 2.672195633989759, 2.8106813119957224, 3.2329044020734727, 2.594071606057696, 2.610559127992019, 2.6405657869763672, 3.556392960017547, 2.882992738042958, 2.954995395964943, 2.8788430109852925, 2.734420010005124, 2.6215758520411327, 2.707978207967244, 2.762545407982543, 2.627912699012086, 2.8320040890248492, 2.6892650850350037, 2.699628234957345, 2.7014681360451505, 2.793845487991348, 2.7440692760283127, 3.0771824680268764, 2.663907177047804, 2.962084955070168, 2.9531202759826556, 2.557168892933987, 2.6014734740601853, 2.5754697689553723, 3.10162842401769, 2.864785760990344, 2.5981639510719106, 2.6033489879919216, 2.588097773026675, 2.606668662978336, 2.585831211064942, 2.5592343530151993, 2.6005928789963946, 2.619162596995011, 2.7125509219476953, 2.6804676559986547, 2.6469862130470574, 3.0352648239349946, 2.6481462669325992, 2.584841944044456, 2.9541506549576297, 2.8713775869691744, 2.6910053830360994, 2.988521831925027, 2.77131326997187, 2.550092611927539, 2.619955969043076, 2.8655453349929303, 3.039167455979623, 2.5841449920553714, 2.622671141056344, 2.765168947982602, 2.6622572739142925, 2.582703761989251, 2.6016863860422745, 2.511940697906539, 2.75445787794888, 2.6464552230900154, 3.0904602800728753, 2.8456814870005473, 2.7274609659798443, 2.7132071699015796, 2.6960248680552468, 2.7743970789015293, 2.6962421010248363, 2.885002798982896, 2.7341833950486034, 2.715957987937145, 3.050393915036693, 2.651567129069008, 2.7457063960609958, 2.6169997170800343, 2.7655244020279497, 3.077641002018936, 2.7729819279629737, 3.207153593073599, 2.8121071660425514, 3.30244237894658, 2.997014925000258, 2.510783117963001, 2.1869327659951523, 2.088050751015544, 2.204808861017227, 2.210633871029131, 2.112328974995762, 2.335985569981858, 2.2625680550700054, 1.9955814430722967, 2.1274514289107174, 2.0353642229456455, 2.3060813440242782, 2.3689492129487917, 2.1004869010066614, 2.2506349239265546, 2.1381767210550606, 2.043432906968519, 2.074745671940036, 2.1327731730416417, 2.412105967057869, 2.0129365590400994, 1.952185105998069, 1.9591034029144794, 2.1319694679696113, 2.0178966120583937, 2.237447981024161, 1.9877545019844547, 1.9856608529808, 2.1744985609548166, 1.942250500083901, 2.0998609990347177, 2.1519998089643195, 2.1194129090290517, 1.9859203319065273, 1.9764950550161302, 2.010966102941893, 2.2674491370562464, 2.166787607013248, 1.9915951669681817, 2.1417552260681987, 2.0701436069794, 2.2978146409150213, 2.576849012984894, 2.0760640690568835, 1.9999246390070766, 2.0074976310133934, 1.9918377919821069, 2.0266768949804828, 2.1173162140185013, 1.9896106410305947, 2.0066930460743606, 2.017937169992365, 2.0929458029568195, 2.039452088996768, 2.1509030431043357, 2.073799947043881, 2.2235944049898535, 2.3094154429854825, 2.2514858979266137, 2.157776198000647, 2.165728252963163]
[0.0017526992603574367, 0.001824815769214183, 0.0019925031431443636, 0.0016650317917363002, 0.0016621627373009172, 0.0017153228651015805, 0.0016534821952819913, 0.0017790972538286897, 0.001639667023028407, 0.0016061920940617, 0.0017402057597253847, 0.0018171436621080093, 0.001605639633166014, 0.0016548528851935717, 0.0017898251786800487, 0.0017544618734026316, 0.0016570746231157356, 0.001696826569245544, 0.0017930916976183653, 0.0017251455982094466, 0.0016489227526936922, 0.0016729565479203, 0.0017667547657055306, 0.0016815155775550775, 0.001669479766240641, 0.0016421194148343607, 0.0015930079378341957, 0.0017768173088655757, 0.0017333221266365793, 0.0019002483082389647, 0.0016754541769293109, 0.0019416256313840444, 0.0017783695668637885, 0.0017610865408146876, 0.0017071636810419343, 0.001687080976305276, 0.0016684978893136925, 0.0016488504633252348, 0.0018008662592316565, 0.0016956056177494883, 0.0018658878077124658, 0.001827057642027317, 0.0016002902633664832, 0.0016979976805180487, 0.0016115956964555401, 0.001642917381660164, 0.0016462745219358854, 0.0013953187881787059, 0.0016153126325425223, 0.0015755142780990731, 0.0018805026195215931, 0.0015881341479762772, 0.0017526666941011005, 0.0019297027751698694, 0.0015697250147515456, 0.0015851654787410116, 0.001922293168013177, 0.0017150995372546584, 0.0018419017970490342, 0.0016926026491161721, 0.0017439183852078735, 0.0016732835863887735, 0.0016835949070939033, 0.0015590910763292403, 0.0017561521881243446, 0.001749988047326171, 0.0015926737319705622, 0.0016889228272670604, 0.0017694390763269639, 0.00170008913667586, 0.0015194771657064117, 0.001553003587512046, 0.0016554584325781676, 0.0019941747325648877, 0.0015779847976305252, 0.0016745512266285144, 0.001603372536114687, 0.0017380558183008926, 0.0015701872953103537, 0.0016286666786777594, 0.0015811808485146502, 0.0016631250366838594, 0.0019129611846588595, 0.001534953613051891, 0.0015447095431905438, 0.0015624649627079097, 0.0021043745325547616, 0.0017059128627473126, 0.0017485179857780727, 0.0017034574029498773, 0.0016180000059201917, 0.0015512283148172382, 0.001602353969211387, 0.0016346422532441084, 0.0015549779284095182, 0.0016757420645117452, 0.0015912811154053276, 0.001597413156779494, 0.001598501855648018, 0.001653163010645768, 0.0016237096307859839, 0.00182081802841827, 0.001576276436122961, 0.0017527129911657797, 0.001747408447327015, 0.0015131176881266195, 0.00153933341660366, 0.0015239466088493327, 0.0018352830911347279, 0.0016951395035445821, 0.001537375118977462, 0.0015404431881609003, 0.0015314188006074999, 0.0015424074928865894, 0.0015300776396833977, 0.0015143398538551476, 0.0015388123544357365, 0.0015498003532514858, 0.0016050597171288138, 0.0015860755360938784, 0.0015662640313887915, 0.0017960146887189318, 0.0015669504538062718, 0.0015294922745825183, 0.0017480181390281833, 0.0016990399922894523, 0.0015923108775361535, 0.0017683561135651048, 0.0016398303372614615, 0.0015089305396020943, 0.0015502698041675006, 0.0016955889556171185, 0.001798323938449481, 0.001529079876955841, 0.001551876414826239, 0.0016361946437766874, 0.001575300162079463, 0.0015282270781001484, 0.0015394594000250146, 0.0014863554425482478, 0.0016298567325141304, 0.001565949836147938, 0.0018286747219366127, 0.001683835199408608, 0.0016138822283904405, 0.0016054480295275618, 0.0015952809870149389, 0.0016416550762730943, 0.001595409527233631, 0.0017071022479188734, 0.0016178599970701796, 0.0016070757325071863, 0.0018049668136311794, 0.0015689746325852117, 0.0016246783408644945, 0.0015485205426509078, 0.0016364049716141714, 0.0018210893503070628, 0.0016408177088538306, 0.001897724019570177, 0.0016639687373032849, 0.001954107916536438, 0.001773381612426188, 0.0014856704840017759, 0.0012940430568018652, 0.0012355329887665941, 0.0013046206278208444, 0.0013080673793071781, 0.0012498988017726404, 0.0013822399822377858, 0.0013387976657218967, 0.0011808174219362702, 0.001258846999355454, 0.0012043575283702044, 0.0013645451739788628, 0.0014017450964194033, 0.0012428916574003914, 0.0013317366413766595, 0.0012651933260680833, 0.0012091318976145083, 0.001227660160911264, 0.0012619959603796696, 0.0014272816373123484, 0.0011910867213255026, 0.0011551391159751888, 0.0011592327827896329, 0.0012615203952482906, 0.0011940216639398779, 0.0013239337165823438, 0.0011761860958487897, 0.001174947250284497, 0.0012866855390265188, 0.0011492606509372195, 0.0012425213012039749, 0.0012733726680262247, 0.0012540904787154152, 0.0011751007881103711, 0.0011695237011929765, 0.0011899207709715344, 0.0013416858799149387, 0.0012821228443865372, 0.0011784586786793975, 0.0012673107846557388, 0.0012249370455499407, 0.0013596536336775275, 0.0015247627295768605, 0.0012284402775484518, 0.001183387360359217, 0.0011878684207179842, 0.0011786022437763946, 0.0011992170976215875, 0.0012528498307801783, 0.0011772844029766833, 0.001187392334955243, 0.0011940456627173757, 0.0012384294692052186, 0.0012067763840217561, 0.0012727236941445772, 0.0012271005603809945, 0.0013157363343135227, 0.0013665180136008773, 0.001332240176287937, 0.0012767906497045249, 0.0012814960076705105]
[570.5485376858463, 548.0005252424074, 501.8812660048821, 600.5891328700679, 601.6258080865402, 582.9806273472491, 604.7842564337114, 562.0828191645843, 609.8799243721054, 622.5905380166727, 574.6446903829385, 550.3142216284278, 622.804756026227, 604.2833226731377, 558.7137849616547, 569.9753383985392, 603.4731242940269, 589.3354206756837, 557.6959624140964, 579.6612187620074, 606.4565476862956, 597.7441561426855, 566.0095104374392, 594.7015973851395, 598.9889905954444, 608.9691108736262, 627.7432624469964, 562.8040626407779, 576.9268069867934, 526.2470150160216, 596.8530884161513, 515.0323439473614, 562.3128165443878, 567.8312659963859, 585.7669133340919, 592.7397760064902, 599.3414833813979, 606.4831361258201, 555.2883202035515, 589.7597823055465, 535.9379035902358, 547.328106676696, 624.8866364382732, 588.9289552473982, 620.5030220664823, 608.673333889439, 607.4321060524468, 716.6821005150291, 619.0752055383778, 634.7133846394231, 531.7727237489324, 629.6697298992513, 570.5591390340622, 518.21452135911, 637.0542551099493, 630.8489639796039, 520.2120137760096, 583.0565388646128, 542.9171096972325, 590.806117739548, 573.4213300817979, 597.6273287650945, 593.9671091819384, 641.3993481089142, 569.4267312151621, 571.4324743691323, 627.8749877809144, 592.0933649870524, 565.1508511249901, 588.2044525943352, 658.1211107144842, 643.9135157453353, 604.0622828823471, 501.46057096702344, 633.7196666923426, 597.1749231066324, 623.6853740947895, 575.355514748422, 636.866699270004, 613.9991768063031, 632.4387251081322, 601.2777018821877, 522.7497599112713, 651.4854856178599, 647.3708953299627, 640.0143515966586, 475.20058075687496, 586.1964123944381, 571.9129046047589, 587.0413890410761, 618.0469693084323, 644.6504298871165, 624.0818316143711, 611.7546503006402, 643.0959447912115, 596.7505508023196, 628.4244752978685, 626.0121220086079, 625.5857611091787, 604.9010252227785, 615.8736642560488, 549.2037009698832, 634.4064892955063, 570.5440679907731, 572.2760477263831, 660.887125864012, 649.6318401288077, 656.1909677105141, 544.8750685005849, 589.9219491428128, 650.4593366029752, 649.163830049375, 652.9892408290332, 648.337099379955, 653.5616063292834, 660.3537491628705, 649.8518140418022, 645.2444006106961, 623.0297784738094, 630.4869958859329, 638.4619578560516, 556.7883193167445, 638.1822715395402, 653.8117364946838, 572.0764434149141, 588.5676644094189, 628.0180673935608, 565.4969563703683, 609.8191851176588, 662.7210290698341, 645.0490084446965, 589.7655777287396, 556.073340636394, 653.9880715655245, 644.3812087394655, 611.1742290585831, 634.7996553748574, 654.3530175130608, 649.5786767639024, 672.7865834605302, 613.5508600547075, 638.5900601132208, 546.8441095642066, 593.8823468895396, 619.6238996926818, 622.8790852197637, 626.8488173178707, 609.1413564597335, 626.7983128657602, 585.7879932025742, 618.1004548050655, 622.2482113147884, 554.0268067246229, 637.3589344477114, 615.5064512449264, 645.7776777620934, 611.095674571061, 549.1218757780255, 609.4522228788812, 526.9470110972691, 600.972829345732, 511.74246393333885, 563.8944223809144, 673.0967672632344, 772.7718136917549, 809.3673006645322, 766.5063533989468, 764.4866127077131, 800.0647721093683, 723.4633731119859, 746.9388583530188, 846.8709738040865, 794.3777127101331, 830.3182206642981, 732.8449208347647, 713.3964674136439, 804.5753578325412, 750.8992160539094, 790.3930406491786, 827.0396322956132, 814.5576698177799, 792.3955633733974, 700.6325688341765, 839.5694302486628, 865.6965954752405, 862.6395102401716, 792.6942788770224, 837.505742316542, 755.3248228932794, 850.2055954660425, 851.1020386301292, 777.1906729880407, 870.1246311569984, 804.8151762316048, 785.3160548435812, 797.3906324720015, 850.9908342484025, 855.0489391364594, 840.392086931578, 745.3309414446538, 779.9564639053556, 848.5660278904462, 789.0724296737102, 816.368484921644, 735.4814308812215, 655.8397451631781, 814.0403878613125, 845.031841219303, 841.8440818517333, 848.4626643810469, 833.8773704805448, 798.1802570682211, 849.4124252997561, 842.1816198078233, 837.4889095315065, 807.4743252369193, 828.6539355927366, 785.7164949475697, 814.9291364429957, 760.0306945401366, 731.7869139279951, 750.615405389088, 783.2137557017826, 780.3379753151077]
Elapsed: 2.635913725489185~0.36020924826637357
Time per graph: 0.001559712263603068~0.00021314156702152283
Speed: 654.2828460566865~97.12624489877129
Total Time: 2.1668
best val loss: 0.07098184299914266 test_score: 0.9728

Testing...
Test loss: 0.0923 score: 0.9740 time: 2.09s
test Score 0.9740
Epoch Time List: [16.007859014091082, 15.157085902057588, 16.554072922095656, 13.908855348010547, 13.97904615092557, 14.506266833981499, 14.214631369919516, 14.822081824066117, 14.592174863093533, 13.65768329706043, 14.374243730097078, 15.529351482982747, 13.664574267109856, 14.0137144329492, 14.671796111972071, 15.416175891063176, 14.324531925027259, 14.160469675902277, 14.334310073172674, 14.91405500494875, 14.65773291804362, 14.039608516031876, 14.587813291931525, 14.192109994008206, 14.398914911900647, 14.001702500972897, 13.954592054942623, 14.403301642043516, 14.949595038895495, 15.638867919915356, 13.87005196500104, 14.995408641174436, 15.231365607120097, 14.490703088813461, 14.642232132027857, 14.163644325104542, 14.322027236921713, 14.15334254608024, 14.684480835217983, 14.449759871000424, 15.762286799843423, 15.454190404969268, 13.826963388943113, 13.907815029029734, 14.128398102940992, 14.172601917991415, 13.928043741965666, 12.600569253088906, 13.062139335903339, 13.494248122908175, 15.46742716501467, 14.061198369949125, 14.231202763970941, 15.348347980994731, 14.268895176006481, 14.17132538498845, 14.668313393951394, 15.010024868883193, 14.905146228033118, 14.494733839994296, 13.868067602976225, 14.108370709931478, 14.460939491982572, 13.211337042041123, 14.094709369004704, 15.04079029802233, 13.563642238033935, 13.964157660026103, 14.038318531122059, 14.15391842101235, 13.401441704016179, 13.310620654025115, 14.055490881903097, 14.854030532995239, 13.723624909063801, 13.162792467977852, 13.176086535095237, 13.557487830054015, 13.763868040055968, 12.876754575991072, 13.12766861799173, 13.39931075682398, 15.12118836294394, 13.109556576935574, 12.985169371822849, 13.06829717499204, 14.861532666021958, 14.43083586206194, 14.283359126071446, 14.536283663008362, 13.096270092995837, 13.132960960036144, 13.123478914028965, 13.572966918116435, 12.898424147046171, 13.356906877015717, 13.432050221948884, 13.308815989177674, 13.089401938021183, 13.199661995051429, 13.299614473129623, 14.385238592047244, 13.32962863415014, 14.054592464934103, 14.41413032100536, 12.949034669087268, 12.748120732954703, 12.743358527775854, 13.78572942793835, 14.299435007036664, 12.806715731043369, 12.853972928947769, 12.998854846926406, 13.113461094093509, 13.092823756160215, 12.643776992917992, 12.860488653182983, 12.924375009024516, 13.492882444057614, 12.896204823977314, 12.959997426951304, 14.399732204969041, 13.397545082028955, 12.925259865936823, 13.943607884924859, 13.904178949887864, 13.10832111898344, 14.606604113942012, 13.98300888785161, 12.678760237060487, 12.77385491807945, 13.943897015065886, 14.045613634982146, 13.233944061910734, 12.850825592060573, 12.823278346913867, 13.344905106001534, 12.669297305983491, 12.851287882891484, 12.563983685104176, 13.312109149061143, 12.814568166038953, 14.503138455096632, 14.261986515135504, 13.556447089998983, 13.508036782965064, 13.519582957960665, 13.53478321491275, 13.294545058975928, 14.188126376015134, 13.640451414976269, 13.25078306091018, 14.688501889933832, 13.78619916900061, 13.720388201065361, 13.1017335599754, 13.748530538054183, 14.261648204061203, 13.920673738000914, 14.99170125706587, 14.37602441792842, 15.012680188985541, 14.83369541913271, 13.817065262934193, 11.885404974105768, 10.44760921294801, 10.661021192092448, 10.769588212948292, 10.66303673305083, 11.397059223032556, 11.611161526991054, 10.150667413137853, 10.665376521879807, 10.680753974011168, 11.692958564963192, 11.988267024978995, 10.797936542076059, 10.835917927091941, 10.57814196194522, 10.800950980046764, 10.749433900113218, 10.803614175063558, 11.781638904009014, 10.922116949921474, 10.148510021856055, 10.075942796072923, 10.503913637017831, 10.520092455903068, 10.545853581861593, 10.33316182915587, 10.024509024922736, 10.810944474069402, 10.167856805957854, 10.740437839995138, 10.579858606099151, 10.819538014940917, 10.04383714508731, 10.060742909903638, 10.299362207064405, 10.55002812598832, 11.5271371769486, 10.291271000052802, 11.022094945888966, 10.954059834126383, 11.496044922154397, 11.677789848879911, 10.77671583101619, 10.207161632948555, 10.072363932034932, 10.589008096954785, 10.18534306390211, 10.790238138055429, 10.496428758953698, 10.499523078091443, 10.204207245958969, 10.168718637083657, 10.780072679044679, 10.762702754000202, 10.427060651825741, 11.083648934960365, 11.683739485917613, 11.291115192929283, 10.908998530125245, 11.04278930707369]
Total Epoch List: [72, 70, 82]
Total Time List: [2.625078661949374, 2.6471305689774454, 2.166801685001701]
========================training times:1========================
========================k_idx:0========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: AEtransGAT
AEtransGAT(
  (gae): GAE(
    (encoder): transGAT(
      (encoder): EdgeFeatureGAT()
      (classifier): Sequential(
        (0): Linear(in_features=64, out_features=64, bias=True)
        (1): ReLU(inplace=True)
        (2): Linear(in_features=64, out_features=2, bias=True)
      )
    )
    (decoder): InnerProductDecoder()
  )
)
Total number of parameters: 1914242
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x72c48b90cbb0>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 2.3737;  Loss pred: 2.3737; Loss self: 0.0000; time: 7.54s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6941 score: 0.5000 time: 2.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6941 score: 0.5000 time: 2.03s
Epoch 2/1000, LR 0.000029
Train loss: 2.2553;  Loss pred: 2.2553; Loss self: 0.0000; time: 6.77s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6941 score: 0.5000 time: 2.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6940 score: 0.5000 time: 2.17s
Epoch 3/1000, LR 0.000059
Train loss: 2.0539;  Loss pred: 2.0539; Loss self: 0.0000; time: 6.81s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6940 score: 0.5000 time: 2.41s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6940 score: 0.5000 time: 2.50s
Epoch 4/1000, LR 0.000089
Train loss: 1.7915;  Loss pred: 1.7915; Loss self: 0.0000; time: 6.49s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6937 score: 0.5000 time: 2.02s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6937 score: 0.5000 time: 2.07s
Epoch 5/1000, LR 0.000119
Train loss: 1.5387;  Loss pred: 1.5387; Loss self: 0.0000; time: 6.55s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6935 score: 0.5000 time: 2.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6935 score: 0.5000 time: 1.97s
Epoch 6/1000, LR 0.000149
Train loss: 1.3281;  Loss pred: 1.3281; Loss self: 0.0000; time: 6.29s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6932 score: 0.5000 time: 2.03s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6932 score: 0.5000 time: 2.00s
Epoch 7/1000, LR 0.000179
Train loss: 1.1857;  Loss pred: 1.1857; Loss self: 0.0000; time: 6.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6927 score: 0.5000 time: 2.02s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6927 score: 0.5000 time: 1.96s
Epoch 8/1000, LR 0.000209
Train loss: 1.0962;  Loss pred: 1.0962; Loss self: 0.0000; time: 6.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6921 score: 0.5000 time: 2.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6921 score: 0.5000 time: 2.03s
Epoch 9/1000, LR 0.000239
Train loss: 1.0449;  Loss pred: 1.0449; Loss self: 0.0000; time: 6.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6917 score: 0.5000 time: 2.02s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6916 score: 0.5000 time: 1.97s
Epoch 10/1000, LR 0.000269
Train loss: 1.0171;  Loss pred: 1.0171; Loss self: 0.0000; time: 6.38s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6905 score: 0.5000 time: 2.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6905 score: 0.5000 time: 2.08s
Epoch 11/1000, LR 0.000299
Train loss: 0.9998;  Loss pred: 0.9998; Loss self: 0.0000; time: 6.49s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6887 score: 0.5000 time: 2.09s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6887 score: 0.5000 time: 2.20s
Epoch 12/1000, LR 0.000299
Train loss: 0.9899;  Loss pred: 0.9899; Loss self: 0.0000; time: 6.55s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6863 score: 0.5000 time: 2.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6862 score: 0.5000 time: 2.00s
Epoch 13/1000, LR 0.000299
Train loss: 0.9823;  Loss pred: 0.9823; Loss self: 0.0000; time: 6.66s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6820 score: 0.5000 time: 2.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6819 score: 0.5000 time: 2.15s
Epoch 14/1000, LR 0.000299
Train loss: 0.9766;  Loss pred: 0.9766; Loss self: 0.0000; time: 6.64s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6753 score: 0.5000 time: 2.32s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6753 score: 0.5000 time: 2.15s
Epoch 15/1000, LR 0.000299
Train loss: 0.9704;  Loss pred: 0.9704; Loss self: 0.0000; time: 6.62s
Val loss: 0.6657 score: 0.5071 time: 2.12s
Test loss: 0.6656 score: 0.5059 time: 1.97s
Epoch 16/1000, LR 0.000299
Train loss: 0.9629;  Loss pred: 0.9629; Loss self: 0.0000; time: 6.56s
Val loss: 0.6521 score: 0.5456 time: 2.26s
Test loss: 0.6520 score: 0.5485 time: 2.16s
Epoch 17/1000, LR 0.000299
Train loss: 0.9535;  Loss pred: 0.9535; Loss self: 0.0000; time: 6.68s
Val loss: 0.6331 score: 0.6586 time: 2.03s
Test loss: 0.6331 score: 0.6556 time: 2.00s
Epoch 18/1000, LR 0.000299
Train loss: 0.9416;  Loss pred: 0.9416; Loss self: 0.0000; time: 6.86s
Val loss: 0.6094 score: 0.7497 time: 2.19s
Test loss: 0.6096 score: 0.7355 time: 2.19s
Epoch 19/1000, LR 0.000299
Train loss: 0.9267;  Loss pred: 0.9267; Loss self: 0.0000; time: 7.18s
Val loss: 0.5797 score: 0.8012 time: 2.56s
Test loss: 0.5803 score: 0.7805 time: 2.42s
Epoch 20/1000, LR 0.000299
Train loss: 0.9093;  Loss pred: 0.9093; Loss self: 0.0000; time: 6.61s
Val loss: 0.5459 score: 0.8391 time: 2.05s
Test loss: 0.5468 score: 0.8278 time: 2.01s
Epoch 21/1000, LR 0.000299
Train loss: 0.8884;  Loss pred: 0.8884; Loss self: 0.0000; time: 6.44s
Val loss: 0.5089 score: 0.8692 time: 2.24s
Test loss: 0.5104 score: 0.8544 time: 2.08s
Epoch 22/1000, LR 0.000299
Train loss: 0.8653;  Loss pred: 0.8653; Loss self: 0.0000; time: 6.38s
Val loss: 0.4704 score: 0.8822 time: 2.05s
Test loss: 0.4724 score: 0.8728 time: 2.02s
Epoch 23/1000, LR 0.000299
Train loss: 0.8422;  Loss pred: 0.8422; Loss self: 0.0000; time: 6.45s
Val loss: 0.4310 score: 0.8953 time: 2.06s
Test loss: 0.4334 score: 0.8899 time: 2.02s
Epoch 24/1000, LR 0.000299
Train loss: 0.8186;  Loss pred: 0.8186; Loss self: 0.0000; time: 6.77s
Val loss: 0.3916 score: 0.9101 time: 2.12s
Test loss: 0.3944 score: 0.9036 time: 2.33s
Epoch 25/1000, LR 0.000299
Train loss: 0.7946;  Loss pred: 0.7946; Loss self: 0.0000; time: 6.94s
Val loss: 0.3540 score: 0.9166 time: 2.42s
Test loss: 0.3570 score: 0.9089 time: 2.33s
Epoch 26/1000, LR 0.000299
Train loss: 0.7707;  Loss pred: 0.7707; Loss self: 0.0000; time: 6.57s
Val loss: 0.3193 score: 0.9231 time: 2.06s
Test loss: 0.3225 score: 0.9160 time: 2.05s
Epoch 27/1000, LR 0.000299
Train loss: 0.7495;  Loss pred: 0.7495; Loss self: 0.0000; time: 6.81s
Val loss: 0.2888 score: 0.9296 time: 2.07s
Test loss: 0.2919 score: 0.9231 time: 2.08s
Epoch 28/1000, LR 0.000299
Train loss: 0.7283;  Loss pred: 0.7283; Loss self: 0.0000; time: 6.80s
Val loss: 0.2629 score: 0.9331 time: 2.13s
Test loss: 0.2659 score: 0.9302 time: 2.17s
Epoch 29/1000, LR 0.000299
Train loss: 0.7122;  Loss pred: 0.7122; Loss self: 0.0000; time: 6.83s
Val loss: 0.2412 score: 0.9391 time: 2.24s
Test loss: 0.2439 score: 0.9337 time: 2.15s
Epoch 30/1000, LR 0.000299
Train loss: 0.6975;  Loss pred: 0.6975; Loss self: 0.0000; time: 6.32s
Val loss: 0.2232 score: 0.9391 time: 1.97s
Test loss: 0.2256 score: 0.9320 time: 1.94s
Epoch 31/1000, LR 0.000299
Train loss: 0.6845;  Loss pred: 0.6845; Loss self: 0.0000; time: 6.52s
Val loss: 0.2079 score: 0.9391 time: 2.12s
Test loss: 0.2099 score: 0.9331 time: 2.07s
Epoch 32/1000, LR 0.000299
Train loss: 0.6741;  Loss pred: 0.6741; Loss self: 0.0000; time: 6.64s
Val loss: 0.1946 score: 0.9426 time: 2.18s
Test loss: 0.1961 score: 0.9379 time: 2.02s
Epoch 33/1000, LR 0.000299
Train loss: 0.6646;  Loss pred: 0.6646; Loss self: 0.0000; time: 6.36s
Val loss: 0.1836 score: 0.9444 time: 2.16s
Test loss: 0.1846 score: 0.9391 time: 1.96s
Epoch 34/1000, LR 0.000298
Train loss: 0.6562;  Loss pred: 0.6562; Loss self: 0.0000; time: 6.52s
Val loss: 0.1737 score: 0.9467 time: 2.07s
Test loss: 0.1742 score: 0.9414 time: 2.06s
Epoch 35/1000, LR 0.000298
Train loss: 0.6491;  Loss pred: 0.6491; Loss self: 0.0000; time: 6.71s
Val loss: 0.1650 score: 0.9479 time: 2.24s
Test loss: 0.1650 score: 0.9450 time: 2.27s
Epoch 36/1000, LR 0.000298
Train loss: 0.6422;  Loss pred: 0.6422; Loss self: 0.0000; time: 6.59s
Val loss: 0.1574 score: 0.9509 time: 2.15s
Test loss: 0.1568 score: 0.9473 time: 2.07s
Epoch 37/1000, LR 0.000298
Train loss: 0.6361;  Loss pred: 0.6361; Loss self: 0.0000; time: 6.54s
Val loss: 0.1511 score: 0.9509 time: 2.06s
Test loss: 0.1500 score: 0.9467 time: 2.03s
Epoch 38/1000, LR 0.000298
Train loss: 0.6314;  Loss pred: 0.6314; Loss self: 0.0000; time: 6.79s
Val loss: 0.1456 score: 0.9556 time: 2.18s
Test loss: 0.1440 score: 0.9485 time: 2.14s
Epoch 39/1000, LR 0.000298
Train loss: 0.6271;  Loss pred: 0.6271; Loss self: 0.0000; time: 6.66s
Val loss: 0.1403 score: 0.9574 time: 2.20s
Test loss: 0.1381 score: 0.9509 time: 2.10s
Epoch 40/1000, LR 0.000298
Train loss: 0.6231;  Loss pred: 0.6231; Loss self: 0.0000; time: 6.42s
Val loss: 0.1357 score: 0.9598 time: 2.02s
Test loss: 0.1330 score: 0.9515 time: 2.01s
Epoch 41/1000, LR 0.000298
Train loss: 0.6191;  Loss pred: 0.6191; Loss self: 0.0000; time: 6.31s
Val loss: 0.1319 score: 0.9627 time: 2.16s
Test loss: 0.1288 score: 0.9556 time: 2.14s
Epoch 42/1000, LR 0.000298
Train loss: 0.6156;  Loss pred: 0.6156; Loss self: 0.0000; time: 6.56s
Val loss: 0.1281 score: 0.9657 time: 2.03s
Test loss: 0.1245 score: 0.9592 time: 2.02s
Epoch 43/1000, LR 0.000298
Train loss: 0.6131;  Loss pred: 0.6131; Loss self: 0.0000; time: 6.24s
Val loss: 0.1249 score: 0.9686 time: 2.19s
Test loss: 0.1208 score: 0.9615 time: 2.20s
Epoch 44/1000, LR 0.000298
Train loss: 0.6106;  Loss pred: 0.6106; Loss self: 0.0000; time: 6.21s
Val loss: 0.1217 score: 0.9686 time: 2.01s
Test loss: 0.1171 score: 0.9633 time: 1.95s
Epoch 45/1000, LR 0.000298
Train loss: 0.6068;  Loss pred: 0.6068; Loss self: 0.0000; time: 6.44s
Val loss: 0.1192 score: 0.9680 time: 2.05s
Test loss: 0.1142 score: 0.9639 time: 1.99s
Epoch 46/1000, LR 0.000298
Train loss: 0.6051;  Loss pred: 0.6051; Loss self: 0.0000; time: 6.25s
Val loss: 0.1165 score: 0.9686 time: 2.01s
Test loss: 0.1112 score: 0.9651 time: 2.01s
Epoch 47/1000, LR 0.000298
Train loss: 0.6027;  Loss pred: 0.6027; Loss self: 0.0000; time: 6.23s
Val loss: 0.1144 score: 0.9686 time: 2.01s
Test loss: 0.1087 score: 0.9663 time: 1.99s
Epoch 48/1000, LR 0.000298
Train loss: 0.6004;  Loss pred: 0.6004; Loss self: 0.0000; time: 6.47s
Val loss: 0.1128 score: 0.9686 time: 2.17s
Test loss: 0.1067 score: 0.9657 time: 2.03s
Epoch 49/1000, LR 0.000298
Train loss: 0.5994;  Loss pred: 0.5994; Loss self: 0.0000; time: 6.41s
Val loss: 0.1101 score: 0.9686 time: 2.11s
Test loss: 0.1036 score: 0.9669 time: 2.15s
Epoch 50/1000, LR 0.000298
Train loss: 0.5960;  Loss pred: 0.5960; Loss self: 0.0000; time: 6.36s
Val loss: 0.1085 score: 0.9686 time: 2.05s
Test loss: 0.1016 score: 0.9669 time: 2.04s
Epoch 51/1000, LR 0.000298
Train loss: 0.5939;  Loss pred: 0.5939; Loss self: 0.0000; time: 6.33s
Val loss: 0.1071 score: 0.9680 time: 2.01s
Test loss: 0.1000 score: 0.9663 time: 1.98s
Epoch 52/1000, LR 0.000298
Train loss: 0.5928;  Loss pred: 0.5928; Loss self: 0.0000; time: 6.28s
Val loss: 0.1054 score: 0.9692 time: 2.15s
Test loss: 0.0979 score: 0.9669 time: 2.12s
Epoch 53/1000, LR 0.000298
Train loss: 0.5908;  Loss pred: 0.5908; Loss self: 0.0000; time: 6.62s
Val loss: 0.1041 score: 0.9698 time: 2.03s
Test loss: 0.0963 score: 0.9680 time: 2.00s
Epoch 54/1000, LR 0.000297
Train loss: 0.5886;  Loss pred: 0.5886; Loss self: 0.0000; time: 6.39s
Val loss: 0.1034 score: 0.9692 time: 2.04s
Test loss: 0.0953 score: 0.9669 time: 1.99s
Epoch 55/1000, LR 0.000297
Train loss: 0.5876;  Loss pred: 0.5876; Loss self: 0.0000; time: 7.09s
Val loss: 0.1011 score: 0.9704 time: 2.05s
Test loss: 0.0927 score: 0.9686 time: 1.98s
Epoch 56/1000, LR 0.000297
Train loss: 0.5853;  Loss pred: 0.5853; Loss self: 0.0000; time: 6.70s
Val loss: 0.1012 score: 0.9698 time: 2.22s
Test loss: 0.0926 score: 0.9686 time: 2.20s
     INFO: Early stopping counter 1 of 2
Epoch 57/1000, LR 0.000297
Train loss: 0.5847;  Loss pred: 0.5847; Loss self: 0.0000; time: 6.77s
Val loss: 0.0994 score: 0.9710 time: 2.15s
Test loss: 0.0906 score: 0.9680 time: 2.05s
Epoch 58/1000, LR 0.000297
Train loss: 0.5827;  Loss pred: 0.5827; Loss self: 0.0000; time: 6.14s
Val loss: 0.0991 score: 0.9692 time: 1.97s
Test loss: 0.0901 score: 0.9680 time: 1.91s
Epoch 59/1000, LR 0.000297
Train loss: 0.5816;  Loss pred: 0.5816; Loss self: 0.0000; time: 6.09s
Val loss: 0.0985 score: 0.9698 time: 1.96s
Test loss: 0.0893 score: 0.9680 time: 1.94s
Epoch 60/1000, LR 0.000297
Train loss: 0.5801;  Loss pred: 0.5801; Loss self: 0.0000; time: 6.15s
Val loss: 0.0979 score: 0.9704 time: 1.96s
Test loss: 0.0887 score: 0.9680 time: 2.06s
Epoch 61/1000, LR 0.000297
Train loss: 0.5791;  Loss pred: 0.5791; Loss self: 0.0000; time: 6.45s
Val loss: 0.0965 score: 0.9710 time: 2.02s
Test loss: 0.0870 score: 0.9686 time: 1.97s
Epoch 62/1000, LR 0.000297
Train loss: 0.5767;  Loss pred: 0.5767; Loss self: 0.0000; time: 6.30s
Val loss: 0.0958 score: 0.9710 time: 2.02s
Test loss: 0.0861 score: 0.9686 time: 1.92s
Epoch 63/1000, LR 0.000297
Train loss: 0.5760;  Loss pred: 0.5760; Loss self: 0.0000; time: 6.17s
Val loss: 0.0951 score: 0.9710 time: 1.99s
Test loss: 0.0853 score: 0.9698 time: 2.22s
Epoch 64/1000, LR 0.000297
Train loss: 0.5738;  Loss pred: 0.5738; Loss self: 0.0000; time: 6.45s
Val loss: 0.0949 score: 0.9710 time: 2.01s
Test loss: 0.0850 score: 0.9692 time: 2.01s
Epoch 65/1000, LR 0.000297
Train loss: 0.5734;  Loss pred: 0.5734; Loss self: 0.0000; time: 6.32s
Val loss: 0.0939 score: 0.9722 time: 2.02s
Test loss: 0.0838 score: 0.9710 time: 2.02s
Epoch 66/1000, LR 0.000297
Train loss: 0.5724;  Loss pred: 0.5724; Loss self: 0.0000; time: 6.50s
Val loss: 0.0938 score: 0.9722 time: 2.22s
Test loss: 0.0838 score: 0.9710 time: 2.08s
Epoch 67/1000, LR 0.000297
Train loss: 0.5708;  Loss pred: 0.5708; Loss self: 0.0000; time: 6.54s
Val loss: 0.0933 score: 0.9722 time: 2.02s
Test loss: 0.0832 score: 0.9716 time: 2.02s
Epoch 68/1000, LR 0.000296
Train loss: 0.5689;  Loss pred: 0.5689; Loss self: 0.0000; time: 6.27s
Val loss: 0.0928 score: 0.9722 time: 2.05s
Test loss: 0.0826 score: 0.9722 time: 2.02s
Epoch 69/1000, LR 0.000296
Train loss: 0.5669;  Loss pred: 0.5669; Loss self: 0.0000; time: 6.36s
Val loss: 0.0916 score: 0.9722 time: 2.04s
Test loss: 0.0812 score: 0.9722 time: 2.01s
Epoch 70/1000, LR 0.000296
Train loss: 0.5670;  Loss pred: 0.5670; Loss self: 0.0000; time: 6.40s
Val loss: 0.0918 score: 0.9722 time: 2.04s
Test loss: 0.0815 score: 0.9722 time: 2.02s
     INFO: Early stopping counter 1 of 2
Epoch 71/1000, LR 0.000296
Train loss: 0.5645;  Loss pred: 0.5645; Loss self: 0.0000; time: 6.40s
Val loss: 0.0921 score: 0.9728 time: 2.01s
Test loss: 0.0819 score: 0.9728 time: 1.97s
     INFO: Early stopping counter 2 of 2
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 068,   Train_Loss: 0.5669,   Val_Loss: 0.0916,   Val_Precision: 0.9842,   Val_Recall: 0.9598,   Val_accuracy: 0.9718,   Val_Score: 0.9722,   Val_Loss: 0.0916,   Test_Precision: 0.9914,   Test_Recall: 0.9527,   Test_accuracy: 0.9716,   Test_Score: 0.9722,   Test_loss: 0.0812


[2.038615496014245, 2.1797736400039867, 2.508276980021037, 2.0709083219990134, 1.9787363869836554, 2.0085195670835674, 1.9623766009462997, 2.032092873007059, 1.9712995940353721, 2.0887712369440123, 2.2061336210463196, 2.006376295001246, 2.1562758079962805, 2.1511155490297824, 1.9808427940588444, 2.165679002995603, 2.0063041859539226, 2.195463145035319, 2.4259330090135336, 2.0127986050210893, 2.08797425404191, 2.0258984819520265, 2.0238514309749007, 2.3316692339722067, 2.333854664932005, 2.052857381058857, 2.085316276992671, 2.170126355951652, 2.1565107069909573, 1.9409398069838062, 2.077659692033194, 2.022638107999228, 1.968419419019483, 2.068248547031544, 2.277436733013019, 2.0734027589205652, 2.037791138049215, 2.1477141209179536, 2.1064586710417643, 2.012583974050358, 2.1490561430109665, 2.0260879449779168, 2.205405179061927, 1.9528894959948957, 1.9991489839740098, 2.011540984036401, 1.9964037740137428, 2.0313305110903457, 2.1559219850460067, 2.0427664380986243, 1.9805727240163833, 2.1297520250082016, 2.000376177020371, 1.994586731074378, 1.9862059820443392, 2.206796965096146, 2.0509002619655803, 1.918878818047233, 1.9473299869569018, 2.069372785044834, 1.975238826009445, 1.9242721749469638, 2.2250378399621695, 2.014727841014974, 2.02068390394561, 2.087973227025941, 2.0246083410456777, 2.0275183609919623, 2.017316202982329, 2.0263983299955726, 1.9722057769540697]
[0.0012062813585883105, 0.0012898068875763235, 0.0014841875621426255, 0.0012253895396443866, 0.0011708499331264234, 0.0011884731166174954, 0.0011611695863587572, 0.0012024218183473722, 0.0011664494639262557, 0.0012359593118011908, 0.0013054045094948636, 0.001187204908284761, 0.0012759028449682132, 0.0012728494372957293, 0.00117209632784547, 0.0012814668656778714, 0.0011871622402094217, 0.001299090618364094, 0.0014354633189429193, 0.0011910050917284553, 0.001235487724285154, 0.0011987564981964655, 0.0011975452254289352, 0.0013796859372616607, 0.0013809790916757426, 0.001214708509502282, 0.0012339149568003972, 0.0012840984354743502, 0.0012760418384561876, 0.0011484850928898261, 0.0012293844331557361, 0.00119682728283978, 0.0011647452183547236, 0.001223815708302689, 0.0013475957000077035, 0.0012268655378228196, 0.0012057935728101864, 0.0012708367579396175, 0.0012464252491371387, 0.0011908780911540579, 0.0012716308538526428, 0.0011988686064958087, 0.001304973478734868, 0.0011555559147898792, 0.0011829283928840295, 0.0011902609372996455, 0.0011813040082921556, 0.0012019707166215064, 0.0012756934822757437, 0.001208737537336464, 0.0011719365230866174, 0.001260208298821421, 0.001183654542615604, 0.0011802288349552531, 0.0011752698118605558, 0.0013057970207669503, 0.0012135504508672074, 0.0011354312532823863, 0.0011522662644715396, 0.0012244809378963515, 0.0011687803704197898, 0.001138622588726014, 0.0013165904378474376, 0.0011921466514881502, 0.0011956709490802426, 0.001235487116583397, 0.0011979931012104602, 0.0011997150065041196, 0.0011936782266167626, 0.0011990522662695695, 0.0011669856668367277]
[828.99399288594, 775.3098619895729, 673.7692900190895, 816.0670281959517, 854.0804177438719, 841.4157510319565, 861.2006478191016, 831.6549024155399, 857.3024643811068, 809.0881232511432, 766.04607439801, 842.3145768869595, 783.7587351918736, 785.6388750303258, 853.1721977477611, 780.3557210751751, 842.3448507118916, 769.7692415478068, 696.6391873645395, 839.6269730037362, 809.3969534004025, 834.1977720283516, 835.0415322659912, 724.8026329707727, 724.1239248499807, 823.2427715598557, 810.4286235358146, 778.7564974569856, 783.6733638842474, 870.7122157622377, 813.4152125491587, 835.5424498907172, 858.55686225745, 817.1164932887656, 742.062326255778, 815.0852470553436, 829.3293500225167, 786.8831254309013, 802.2944020848975, 839.7165145854002, 786.3917401581705, 834.1197647362835, 766.2990982540653, 865.3843463575151, 845.3597073293319, 840.1519101086423, 846.5221424633344, 831.9670239644402, 783.8873631431222, 827.3094605827899, 853.288535940688, 793.5196117461102, 844.8410950970797, 847.293313281838, 850.8684473201195, 765.8158075844417, 824.0283700487249, 880.7226303742548, 867.8549661944907, 816.6725745179766, 855.5927403544866, 878.2541378516681, 759.537644550231, 838.82297429742, 836.3505032628246, 809.3973515202565, 834.7293477646853, 833.5312924974787, 837.746704012769, 833.9920019593051, 856.9085537362555]
Elapsed: 2.074914777303823~0.11349773906267631
Time per graph: 0.0012277602232567~6.715842548087355e-05
Speed: 816.7618076737707~41.63298675842181
Total Time: 1.9737
best val loss: 0.09159776437035679 test_score: 0.9722

Testing...
Test loss: 0.0819 score: 0.9728 time: 1.98s
test Score 0.9728
Epoch Time List: [11.642819940927438, 11.075978060020134, 11.720096316887066, 10.574460907955654, 10.56905995181296, 10.320808705990203, 10.241954101016745, 10.283787759952247, 10.169229754945263, 10.585050416062586, 10.787243144935928, 10.669381032930687, 10.948988351854496, 11.09731895616278, 10.721414203988388, 10.987515883985907, 10.708370489999652, 11.235983202932402, 12.162264017038979, 10.671224665129557, 10.767038193996996, 10.448549464927055, 10.526551429880783, 11.219654626911506, 11.68865094112698, 10.678101818077266, 10.963107087998651, 11.101969216950238, 11.23012452002149, 10.230854956898838, 10.708556358120404, 10.828183440025896, 10.484837236930616, 10.651560379075818, 11.224814778077416, 10.80947511701379, 10.630397756933235, 11.113010516040958, 10.963644942967221, 10.449040114996023, 10.617351107066497, 10.607991177006625, 10.636651432025246, 10.170795076177455, 10.48250787705183, 10.270433225785382, 10.234480639919639, 10.66967289603781, 10.673855543020181, 10.454113586107269, 10.320850205956958, 10.551990934996866, 10.642094478942454, 10.4228778189281, 11.120462722959928, 11.121453920030035, 10.964376344927587, 10.02190934191458, 9.991373145021498, 10.18325537594501, 10.44610322790686, 10.244469390017912, 10.38119343703147, 10.468194533023052, 10.358507276047021, 10.803181883995421, 10.58225236099679, 10.34112817409914, 10.410149578936398, 10.460287030902691, 10.381974941934459]
Total Epoch List: [71]
Total Time List: [1.9736579819582403]
========================k_idx:1========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: AEtransGAT
AEtransGAT(
  (gae): GAE(
    (encoder): transGAT(
      (encoder): EdgeFeatureGAT()
      (classifier): Sequential(
        (0): Linear(in_features=64, out_features=64, bias=True)
        (1): ReLU(inplace=True)
        (2): Linear(in_features=64, out_features=2, bias=True)
      )
    )
    (decoder): InnerProductDecoder()
  )
)
Total number of parameters: 1914242
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x72c48b90cb20>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 2.1575;  Loss pred: 2.1575; Loss self: 0.0000; time: 6.62s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6938 score: 0.5000 time: 2.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6938 score: 0.5000 time: 2.28s
Epoch 2/1000, LR 0.000029
Train loss: 2.0605;  Loss pred: 2.0605; Loss self: 0.0000; time: 6.43s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6937 score: 0.5000 time: 2.34s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6937 score: 0.5000 time: 2.45s
Epoch 3/1000, LR 0.000059
Train loss: 1.8773;  Loss pred: 1.8773; Loss self: 0.0000; time: 6.35s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6934 score: 0.5000 time: 2.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6935 score: 0.5000 time: 2.12s
Epoch 4/1000, LR 0.000089
Train loss: 1.6565;  Loss pred: 1.6565; Loss self: 0.0000; time: 6.48s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6931 score: 0.5000 time: 2.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6932 score: 0.5000 time: 2.29s
Epoch 5/1000, LR 0.000119
Train loss: 1.4460;  Loss pred: 1.4460; Loss self: 0.0000; time: 6.65s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6927 score: 0.5000 time: 2.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6927 score: 0.5000 time: 2.20s
Epoch 6/1000, LR 0.000149
Train loss: 1.2764;  Loss pred: 1.2764; Loss self: 0.0000; time: 6.68s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6919 score: 0.5000 time: 2.58s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6920 score: 0.5000 time: 2.65s
Epoch 7/1000, LR 0.000179
Train loss: 1.1541;  Loss pred: 1.1541; Loss self: 0.0000; time: 6.70s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6908 score: 0.5000 time: 2.11s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6910 score: 0.5000 time: 2.09s
Epoch 8/1000, LR 0.000209
Train loss: 1.0787;  Loss pred: 1.0787; Loss self: 0.0000; time: 6.02s
Val loss: 0.6889 score: 0.5254 time: 2.06s
Test loss: 0.6892 score: 0.5148 time: 2.05s
Epoch 9/1000, LR 0.000239
Train loss: 1.0344;  Loss pred: 1.0344; Loss self: 0.0000; time: 6.58s
Val loss: 0.6865 score: 0.5686 time: 2.19s
Test loss: 0.6869 score: 0.5609 time: 2.26s
Epoch 10/1000, LR 0.000269
Train loss: 1.0080;  Loss pred: 1.0080; Loss self: 0.0000; time: 6.74s
Val loss: 0.6822 score: 0.8373 time: 2.39s
Test loss: 0.6827 score: 0.8320 time: 2.47s
Epoch 11/1000, LR 0.000299
Train loss: 0.9921;  Loss pred: 0.9921; Loss self: 0.0000; time: 6.74s
Val loss: 0.6763 score: 0.8763 time: 2.35s
Test loss: 0.6771 score: 0.8686 time: 2.27s
Epoch 12/1000, LR 0.000299
Train loss: 0.9801;  Loss pred: 0.9801; Loss self: 0.0000; time: 6.85s
Val loss: 0.6671 score: 0.9249 time: 2.43s
Test loss: 0.6684 score: 0.9237 time: 2.24s
Epoch 13/1000, LR 0.000299
Train loss: 0.9702;  Loss pred: 0.9702; Loss self: 0.0000; time: 6.60s
Val loss: 0.6538 score: 0.9101 time: 2.17s
Test loss: 0.6556 score: 0.9071 time: 2.20s
Epoch 14/1000, LR 0.000299
Train loss: 0.9599;  Loss pred: 0.9599; Loss self: 0.0000; time: 6.76s
Val loss: 0.6350 score: 0.9024 time: 2.20s
Test loss: 0.6376 score: 0.8953 time: 2.32s
Epoch 15/1000, LR 0.000299
Train loss: 0.9469;  Loss pred: 0.9469; Loss self: 0.0000; time: 6.79s
Val loss: 0.6094 score: 0.8982 time: 2.87s
Test loss: 0.6127 score: 0.8893 time: 2.62s
Epoch 16/1000, LR 0.000299
Train loss: 0.9315;  Loss pred: 0.9315; Loss self: 0.0000; time: 6.65s
Val loss: 0.5791 score: 0.8976 time: 2.19s
Test loss: 0.5832 score: 0.8893 time: 2.13s
Epoch 17/1000, LR 0.000299
Train loss: 0.9126;  Loss pred: 0.9126; Loss self: 0.0000; time: 6.34s
Val loss: 0.5426 score: 0.8982 time: 2.35s
Test loss: 0.5473 score: 0.8899 time: 2.34s
Epoch 18/1000, LR 0.000299
Train loss: 0.8916;  Loss pred: 0.8916; Loss self: 0.0000; time: 6.32s
Val loss: 0.5003 score: 0.9018 time: 2.13s
Test loss: 0.5056 score: 0.8941 time: 2.14s
Epoch 19/1000, LR 0.000299
Train loss: 0.8689;  Loss pred: 0.8689; Loss self: 0.0000; time: 6.38s
Val loss: 0.4553 score: 0.9101 time: 2.15s
Test loss: 0.4607 score: 0.9107 time: 2.16s
Epoch 20/1000, LR 0.000299
Train loss: 0.8424;  Loss pred: 0.8424; Loss self: 0.0000; time: 6.36s
Val loss: 0.4094 score: 0.9166 time: 2.08s
Test loss: 0.4149 score: 0.9189 time: 2.12s
Epoch 21/1000, LR 0.000299
Train loss: 0.8164;  Loss pred: 0.8164; Loss self: 0.0000; time: 6.18s
Val loss: 0.3640 score: 0.9266 time: 2.10s
Test loss: 0.3693 score: 0.9260 time: 2.12s
Epoch 22/1000, LR 0.000299
Train loss: 0.7896;  Loss pred: 0.7896; Loss self: 0.0000; time: 6.26s
Val loss: 0.3238 score: 0.9331 time: 2.15s
Test loss: 0.3284 score: 0.9308 time: 2.15s
Epoch 23/1000, LR 0.000299
Train loss: 0.7649;  Loss pred: 0.7649; Loss self: 0.0000; time: 6.68s
Val loss: 0.2881 score: 0.9379 time: 2.23s
Test loss: 0.2919 score: 0.9367 time: 2.15s
Epoch 24/1000, LR 0.000299
Train loss: 0.7416;  Loss pred: 0.7416; Loss self: 0.0000; time: 6.35s
Val loss: 0.2581 score: 0.9349 time: 2.13s
Test loss: 0.2608 score: 0.9361 time: 2.17s
Epoch 25/1000, LR 0.000299
Train loss: 0.7206;  Loss pred: 0.7206; Loss self: 0.0000; time: 6.33s
Val loss: 0.2333 score: 0.9367 time: 2.15s
Test loss: 0.2350 score: 0.9379 time: 2.22s
Epoch 26/1000, LR 0.000299
Train loss: 0.7048;  Loss pred: 0.7048; Loss self: 0.0000; time: 6.27s
Val loss: 0.2129 score: 0.9408 time: 2.11s
Test loss: 0.2133 score: 0.9402 time: 2.18s
Epoch 27/1000, LR 0.000299
Train loss: 0.6889;  Loss pred: 0.6889; Loss self: 0.0000; time: 6.33s
Val loss: 0.1962 score: 0.9444 time: 2.13s
Test loss: 0.1955 score: 0.9444 time: 2.12s
Epoch 28/1000, LR 0.000299
Train loss: 0.6754;  Loss pred: 0.6754; Loss self: 0.0000; time: 6.29s
Val loss: 0.1824 score: 0.9473 time: 2.17s
Test loss: 0.1805 score: 0.9491 time: 2.27s
Epoch 29/1000, LR 0.000299
Train loss: 0.6647;  Loss pred: 0.6647; Loss self: 0.0000; time: 6.81s
Val loss: 0.1708 score: 0.9491 time: 2.24s
Test loss: 0.1678 score: 0.9521 time: 2.17s
Epoch 30/1000, LR 0.000299
Train loss: 0.6550;  Loss pred: 0.6550; Loss self: 0.0000; time: 6.54s
Val loss: 0.1613 score: 0.9491 time: 2.18s
Test loss: 0.1574 score: 0.9544 time: 2.20s
Epoch 31/1000, LR 0.000299
Train loss: 0.6462;  Loss pred: 0.6462; Loss self: 0.0000; time: 6.97s
Val loss: 0.1530 score: 0.9491 time: 2.39s
Test loss: 0.1482 score: 0.9568 time: 2.44s
Epoch 32/1000, LR 0.000299
Train loss: 0.6390;  Loss pred: 0.6390; Loss self: 0.0000; time: 6.99s
Val loss: 0.1459 score: 0.9503 time: 2.35s
Test loss: 0.1402 score: 0.9604 time: 2.36s
Epoch 33/1000, LR 0.000299
Train loss: 0.6324;  Loss pred: 0.6324; Loss self: 0.0000; time: 6.48s
Val loss: 0.1402 score: 0.9509 time: 2.17s
Test loss: 0.1338 score: 0.9615 time: 2.18s
Epoch 34/1000, LR 0.000298
Train loss: 0.6267;  Loss pred: 0.6267; Loss self: 0.0000; time: 6.67s
Val loss: 0.1350 score: 0.9527 time: 2.18s
Test loss: 0.1279 score: 0.9615 time: 2.19s
Epoch 35/1000, LR 0.000298
Train loss: 0.6218;  Loss pred: 0.6218; Loss self: 0.0000; time: 6.62s
Val loss: 0.1303 score: 0.9538 time: 2.28s
Test loss: 0.1225 score: 0.9621 time: 2.23s
Epoch 36/1000, LR 0.000298
Train loss: 0.6170;  Loss pred: 0.6170; Loss self: 0.0000; time: 7.14s
Val loss: 0.1264 score: 0.9562 time: 2.27s
Test loss: 0.1180 score: 0.9621 time: 2.30s
Epoch 37/1000, LR 0.000298
Train loss: 0.6130;  Loss pred: 0.6130; Loss self: 0.0000; time: 6.91s
Val loss: 0.1228 score: 0.9562 time: 2.48s
Test loss: 0.1139 score: 0.9627 time: 2.50s
Epoch 38/1000, LR 0.000298
Train loss: 0.6095;  Loss pred: 0.6095; Loss self: 0.0000; time: 6.38s
Val loss: 0.1196 score: 0.9568 time: 2.11s
Test loss: 0.1102 score: 0.9627 time: 2.11s
Epoch 39/1000, LR 0.000298
Train loss: 0.6069;  Loss pred: 0.6069; Loss self: 0.0000; time: 6.51s
Val loss: 0.1169 score: 0.9580 time: 2.17s
Test loss: 0.1071 score: 0.9639 time: 2.19s
Epoch 40/1000, LR 0.000298
Train loss: 0.6022;  Loss pred: 0.6022; Loss self: 0.0000; time: 6.20s
Val loss: 0.1144 score: 0.9580 time: 2.09s
Test loss: 0.1044 score: 0.9657 time: 2.11s
Epoch 41/1000, LR 0.000298
Train loss: 0.6004;  Loss pred: 0.6004; Loss self: 0.0000; time: 6.23s
Val loss: 0.1121 score: 0.9604 time: 2.07s
Test loss: 0.1017 score: 0.9675 time: 2.10s
Epoch 42/1000, LR 0.000298
Train loss: 0.5986;  Loss pred: 0.5986; Loss self: 0.0000; time: 6.39s
Val loss: 0.1098 score: 0.9663 time: 2.19s
Test loss: 0.0992 score: 0.9692 time: 2.19s
Epoch 43/1000, LR 0.000298
Train loss: 0.5956;  Loss pred: 0.5956; Loss self: 0.0000; time: 6.27s
Val loss: 0.1084 score: 0.9680 time: 2.12s
Test loss: 0.0976 score: 0.9704 time: 2.22s
Epoch 44/1000, LR 0.000298
Train loss: 0.5947;  Loss pred: 0.5947; Loss self: 0.0000; time: 6.37s
Val loss: 0.1066 score: 0.9686 time: 2.17s
Test loss: 0.0956 score: 0.9704 time: 2.17s
Epoch 45/1000, LR 0.000298
Train loss: 0.5911;  Loss pred: 0.5911; Loss self: 0.0000; time: 6.69s
Val loss: 0.1051 score: 0.9710 time: 2.09s
Test loss: 0.0940 score: 0.9704 time: 2.11s
Epoch 46/1000, LR 0.000298
Train loss: 0.5906;  Loss pred: 0.5906; Loss self: 0.0000; time: 6.52s
Val loss: 0.1041 score: 0.9710 time: 2.21s
Test loss: 0.0930 score: 0.9704 time: 2.24s
Epoch 47/1000, LR 0.000298
Train loss: 0.5868;  Loss pred: 0.5868; Loss self: 0.0000; time: 6.61s
Val loss: 0.1028 score: 0.9722 time: 2.21s
Test loss: 0.0915 score: 0.9698 time: 2.24s
Epoch 48/1000, LR 0.000298
Train loss: 0.5856;  Loss pred: 0.5856; Loss self: 0.0000; time: 6.55s
Val loss: 0.1017 score: 0.9722 time: 2.11s
Test loss: 0.0904 score: 0.9704 time: 2.13s
Epoch 49/1000, LR 0.000298
Train loss: 0.5846;  Loss pred: 0.5846; Loss self: 0.0000; time: 6.21s
Val loss: 0.1005 score: 0.9722 time: 2.10s
Test loss: 0.0891 score: 0.9710 time: 2.13s
Epoch 50/1000, LR 0.000298
Train loss: 0.5820;  Loss pred: 0.5820; Loss self: 0.0000; time: 6.43s
Val loss: 0.0997 score: 0.9722 time: 2.20s
Test loss: 0.0884 score: 0.9716 time: 2.29s
Epoch 51/1000, LR 0.000298
Train loss: 0.5798;  Loss pred: 0.5798; Loss self: 0.0000; time: 6.98s
Val loss: 0.0992 score: 0.9722 time: 2.36s
Test loss: 0.0877 score: 0.9716 time: 2.37s
Epoch 52/1000, LR 0.000298
Train loss: 0.5790;  Loss pred: 0.5790; Loss self: 0.0000; time: 7.04s
Val loss: 0.0985 score: 0.9722 time: 2.53s
Test loss: 0.0871 score: 0.9716 time: 2.48s
Epoch 53/1000, LR 0.000298
Train loss: 0.5774;  Loss pred: 0.5774; Loss self: 0.0000; time: 6.07s
Val loss: 0.0977 score: 0.9722 time: 2.04s
Test loss: 0.0863 score: 0.9716 time: 2.07s
Epoch 54/1000, LR 0.000297
Train loss: 0.5758;  Loss pred: 0.5758; Loss self: 0.0000; time: 6.10s
Val loss: 0.0974 score: 0.9722 time: 2.17s
Test loss: 0.0860 score: 0.9716 time: 2.13s
Epoch 55/1000, LR 0.000297
Train loss: 0.5750;  Loss pred: 0.5750; Loss self: 0.0000; time: 6.60s
Val loss: 0.0978 score: 0.9722 time: 2.22s
Test loss: 0.0863 score: 0.9710 time: 2.34s
     INFO: Early stopping counter 1 of 2
Epoch 56/1000, LR 0.000297
Train loss: 0.5737;  Loss pred: 0.5737; Loss self: 0.0000; time: 6.95s
Val loss: 0.0970 score: 0.9722 time: 2.39s
Test loss: 0.0856 score: 0.9704 time: 2.46s
Epoch 57/1000, LR 0.000297
Train loss: 0.5724;  Loss pred: 0.5724; Loss self: 0.0000; time: 6.52s
Val loss: 0.0959 score: 0.9728 time: 2.26s
Test loss: 0.0846 score: 0.9716 time: 2.14s
Epoch 58/1000, LR 0.000297
Train loss: 0.5709;  Loss pred: 0.5709; Loss self: 0.0000; time: 6.46s
Val loss: 0.0961 score: 0.9728 time: 2.25s
Test loss: 0.0848 score: 0.9716 time: 2.41s
     INFO: Early stopping counter 1 of 2
Epoch 59/1000, LR 0.000297
Train loss: 0.5692;  Loss pred: 0.5692; Loss self: 0.0000; time: 6.68s
Val loss: 0.0958 score: 0.9734 time: 2.27s
Test loss: 0.0846 score: 0.9722 time: 2.17s
Epoch 60/1000, LR 0.000297
Train loss: 0.5678;  Loss pred: 0.5678; Loss self: 0.0000; time: 6.87s
Val loss: 0.0956 score: 0.9734 time: 2.42s
Test loss: 0.0843 score: 0.9722 time: 2.46s
Epoch 61/1000, LR 0.000297
Train loss: 0.5666;  Loss pred: 0.5666; Loss self: 0.0000; time: 6.91s
Val loss: 0.0949 score: 0.9728 time: 2.24s
Test loss: 0.0836 score: 0.9728 time: 2.19s
Epoch 62/1000, LR 0.000297
Train loss: 0.5657;  Loss pred: 0.5657; Loss self: 0.0000; time: 6.14s
Val loss: 0.0953 score: 0.9728 time: 2.19s
Test loss: 0.0839 score: 0.9722 time: 2.13s
     INFO: Early stopping counter 1 of 2
Epoch 63/1000, LR 0.000297
Train loss: 0.5650;  Loss pred: 0.5650; Loss self: 0.0000; time: 6.31s
Val loss: 0.0953 score: 0.9728 time: 2.11s
Test loss: 0.0839 score: 0.9728 time: 2.11s
     INFO: Early stopping counter 2 of 2
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 060,   Train_Loss: 0.5666,   Val_Loss: 0.0949,   Val_Precision: 0.9831,   Val_Recall: 0.9621,   Val_accuracy: 0.9725,   Val_Score: 0.9728,   Val_Loss: 0.0949,   Test_Precision: 0.9796,   Test_Recall: 0.9657,   Test_accuracy: 0.9726,   Test_Score: 0.9728,   Test_loss: 0.0836


[2.038615496014245, 2.1797736400039867, 2.508276980021037, 2.0709083219990134, 1.9787363869836554, 2.0085195670835674, 1.9623766009462997, 2.032092873007059, 1.9712995940353721, 2.0887712369440123, 2.2061336210463196, 2.006376295001246, 2.1562758079962805, 2.1511155490297824, 1.9808427940588444, 2.165679002995603, 2.0063041859539226, 2.195463145035319, 2.4259330090135336, 2.0127986050210893, 2.08797425404191, 2.0258984819520265, 2.0238514309749007, 2.3316692339722067, 2.333854664932005, 2.052857381058857, 2.085316276992671, 2.170126355951652, 2.1565107069909573, 1.9409398069838062, 2.077659692033194, 2.022638107999228, 1.968419419019483, 2.068248547031544, 2.277436733013019, 2.0734027589205652, 2.037791138049215, 2.1477141209179536, 2.1064586710417643, 2.012583974050358, 2.1490561430109665, 2.0260879449779168, 2.205405179061927, 1.9528894959948957, 1.9991489839740098, 2.011540984036401, 1.9964037740137428, 2.0313305110903457, 2.1559219850460067, 2.0427664380986243, 1.9805727240163833, 2.1297520250082016, 2.000376177020371, 1.994586731074378, 1.9862059820443392, 2.206796965096146, 2.0509002619655803, 1.918878818047233, 1.9473299869569018, 2.069372785044834, 1.975238826009445, 1.9242721749469638, 2.2250378399621695, 2.014727841014974, 2.02068390394561, 2.087973227025941, 2.0246083410456777, 2.0275183609919623, 2.017316202982329, 2.0263983299955726, 1.9722057769540697, 2.285455900011584, 2.456490562064573, 2.123919714940712, 2.2999873269582167, 2.204477241029963, 2.65552795806434, 2.097316027036868, 2.058954322943464, 2.269513361970894, 2.4784478259971365, 2.2722701930906624, 2.2415590309537947, 2.203143520047888, 2.3235208559781313, 2.6207863689633086, 2.1342287249863148, 2.3471294980263337, 2.148788296035491, 2.168985861935653, 2.1282924469560385, 2.1263596509816125, 2.151534255943261, 2.1563696659868583, 2.1710237929364666, 2.2239382750121877, 2.1845873809652403, 2.127945325919427, 2.2704587809275836, 2.170224132016301, 2.2047560369828716, 2.441716630011797, 2.366347378003411, 2.1875903489999473, 2.191411138046533, 2.239357273094356, 2.3006402660394087, 2.506089695962146, 2.1159373190021142, 2.1958802200388163, 2.1128611040767282, 2.1037045100238174, 2.1952009820379317, 2.2221052480163053, 2.1727849900489673, 2.1150627710158005, 2.2464506500400603, 2.250157912960276, 2.1326914880191907, 2.1311617150204256, 2.2938323550624773, 2.372392990044318, 2.484147975919768, 2.0714518160093576, 2.137157795019448, 2.345140376011841, 2.4739222140051425, 2.1416693539358675, 2.4186650450574234, 2.1746826419839635, 2.4637944570276886, 2.1908706640824676, 2.1350690710823983, 2.1147852879948914]
[0.0012062813585883105, 0.0012898068875763235, 0.0014841875621426255, 0.0012253895396443866, 0.0011708499331264234, 0.0011884731166174954, 0.0011611695863587572, 0.0012024218183473722, 0.0011664494639262557, 0.0012359593118011908, 0.0013054045094948636, 0.001187204908284761, 0.0012759028449682132, 0.0012728494372957293, 0.00117209632784547, 0.0012814668656778714, 0.0011871622402094217, 0.001299090618364094, 0.0014354633189429193, 0.0011910050917284553, 0.001235487724285154, 0.0011987564981964655, 0.0011975452254289352, 0.0013796859372616607, 0.0013809790916757426, 0.001214708509502282, 0.0012339149568003972, 0.0012840984354743502, 0.0012760418384561876, 0.0011484850928898261, 0.0012293844331557361, 0.00119682728283978, 0.0011647452183547236, 0.001223815708302689, 0.0013475957000077035, 0.0012268655378228196, 0.0012057935728101864, 0.0012708367579396175, 0.0012464252491371387, 0.0011908780911540579, 0.0012716308538526428, 0.0011988686064958087, 0.001304973478734868, 0.0011555559147898792, 0.0011829283928840295, 0.0011902609372996455, 0.0011813040082921556, 0.0012019707166215064, 0.0012756934822757437, 0.001208737537336464, 0.0011719365230866174, 0.001260208298821421, 0.001183654542615604, 0.0011802288349552531, 0.0011752698118605558, 0.0013057970207669503, 0.0012135504508672074, 0.0011354312532823863, 0.0011522662644715396, 0.0012244809378963515, 0.0011687803704197898, 0.001138622588726014, 0.0013165904378474376, 0.0011921466514881502, 0.0011956709490802426, 0.001235487116583397, 0.0011979931012104602, 0.0011997150065041196, 0.0011936782266167626, 0.0011990522662695695, 0.0011669856668367277, 0.0013523407692376237, 0.0014535447112808124, 0.001256757227775569, 0.0013609392467208382, 0.0013044244029763094, 0.0015713183183812664, 0.0012410154006135313, 0.001218316167422168, 0.001342907314775677, 0.0014665371751462347, 0.0013445385757932912, 0.001326366290505204, 0.0013036352189632475, 0.0013748644118213795, 0.0015507611650670465, 0.001262857233719713, 0.0013888340225007892, 0.0012714723645180419, 0.0012834235869441733, 0.0012593446431692534, 0.0012582009769121967, 0.001273097192865835, 0.0012759583822407445, 0.0012846294632760157, 0.0013159398076995194, 0.001292655255009018, 0.0012591392461061698, 0.0013434667342766768, 0.001284156291133906, 0.001304589370995782, 0.0014448027396519508, 0.0014002055491144443, 0.0012944321591715664, 0.001296692981092623, 0.001325063475203761, 0.0013613256012067507, 0.0014828933112202047, 0.0012520339165692984, 0.001299337408306992, 0.0012502136710513185, 0.0012447955680614305, 0.0012989354923301371, 0.001314855176341009, 0.0012856715917449512, 0.0012515164325537281, 0.0013292607396686747, 0.0013314543863670273, 0.0012619476260468585, 0.0012610424349233287, 0.0013572972515162587, 0.0014037828343457504, 0.0014699100449229398, 0.0012257111337333477, 0.0012645904112541112, 0.0013876570272259414, 0.001463859298227895, 0.001267259972743117, 0.0014311627485546884, 0.0012867944627124044, 0.0014578665426199342, 0.0012963731740132944, 0.0012633544799304133, 0.0012513522414170956]
[828.99399288594, 775.3098619895729, 673.7692900190895, 816.0670281959517, 854.0804177438719, 841.4157510319565, 861.2006478191016, 831.6549024155399, 857.3024643811068, 809.0881232511432, 766.04607439801, 842.3145768869595, 783.7587351918736, 785.6388750303258, 853.1721977477611, 780.3557210751751, 842.3448507118916, 769.7692415478068, 696.6391873645395, 839.6269730037362, 809.3969534004025, 834.1977720283516, 835.0415322659912, 724.8026329707727, 724.1239248499807, 823.2427715598557, 810.4286235358146, 778.7564974569856, 783.6733638842474, 870.7122157622377, 813.4152125491587, 835.5424498907172, 858.55686225745, 817.1164932887656, 742.062326255778, 815.0852470553436, 829.3293500225167, 786.8831254309013, 802.2944020848975, 839.7165145854002, 786.3917401581705, 834.1197647362835, 766.2990982540653, 865.3843463575151, 845.3597073293319, 840.1519101086423, 846.5221424633344, 831.9670239644402, 783.8873631431222, 827.3094605827899, 853.288535940688, 793.5196117461102, 844.8410950970797, 847.293313281838, 850.8684473201195, 765.8158075844417, 824.0283700487249, 880.7226303742548, 867.8549661944907, 816.6725745179766, 855.5927403544866, 878.2541378516681, 759.537644550231, 838.82297429742, 836.3505032628246, 809.3973515202565, 834.7293477646853, 833.5312924974787, 837.746704012769, 833.9920019593051, 856.9085537362555, 739.4585911683678, 687.9733332171359, 795.6986265119609, 734.786657383483, 766.6216591151597, 636.4082874246483, 805.7917730155657, 820.8049985217691, 744.6530292874627, 681.8783846378021, 743.7495792264568, 753.9395468344621, 767.0857502570989, 727.3444504067348, 644.8446237411197, 791.8551466459326, 720.0284438592314, 786.4897640768269, 779.1659824337468, 794.0638056659458, 794.7855854111173, 785.4859830056862, 783.7246213656854, 778.4345825681407, 759.9131769926206, 773.6014657620556, 794.1933373075726, 744.3429557922033, 778.7214117971599, 766.5247182235652, 692.1360076053686, 714.1808576836786, 772.5395208351418, 771.1925757147056, 754.6808275326022, 734.5781193812467, 674.3573475135214, 798.7004080050026, 769.6230352537744, 799.863273898684, 803.3447625117591, 769.8611716322555, 760.5400336049246, 777.8036058514529, 799.0306591176696, 752.2978526013303, 751.0583991754871, 792.4259132152512, 792.9947258759772, 736.7582884905158, 712.3608976641048, 680.3137399148974, 815.8529138542924, 790.7698738663428, 720.6391639864321, 683.1257629818457, 789.1040682326574, 698.7325522620584, 777.1248858905742, 685.9338428899662, 771.3828240553711, 791.543478798667, 799.135500702463]
Elapsed: 2.1542513075218634~0.15006236327903869
Time per graph: 0.00127470491569341~8.879429779824775e-05
Speed: 788.1078022917497~52.06139734066424
Total Time: 2.1154
best val loss: 0.0949430060809886 test_score: 0.9728

Testing...
Test loss: 0.0846 score: 0.9722 time: 2.12s
test Score 0.9722
Epoch Time List: [11.642819940927438, 11.075978060020134, 11.720096316887066, 10.574460907955654, 10.56905995181296, 10.320808705990203, 10.241954101016745, 10.283787759952247, 10.169229754945263, 10.585050416062586, 10.787243144935928, 10.669381032930687, 10.948988351854496, 11.09731895616278, 10.721414203988388, 10.987515883985907, 10.708370489999652, 11.235983202932402, 12.162264017038979, 10.671224665129557, 10.767038193996996, 10.448549464927055, 10.526551429880783, 11.219654626911506, 11.68865094112698, 10.678101818077266, 10.963107087998651, 11.101969216950238, 11.23012452002149, 10.230854956898838, 10.708556358120404, 10.828183440025896, 10.484837236930616, 10.651560379075818, 11.224814778077416, 10.80947511701379, 10.630397756933235, 11.113010516040958, 10.963644942967221, 10.449040114996023, 10.617351107066497, 10.607991177006625, 10.636651432025246, 10.170795076177455, 10.48250787705183, 10.270433225785382, 10.234480639919639, 10.66967289603781, 10.673855543020181, 10.454113586107269, 10.320850205956958, 10.551990934996866, 10.642094478942454, 10.4228778189281, 11.120462722959928, 11.121453920030035, 10.964376344927587, 10.02190934191458, 9.991373145021498, 10.18325537594501, 10.44610322790686, 10.244469390017912, 10.38119343703147, 10.468194533023052, 10.358507276047021, 10.803181883995421, 10.58225236099679, 10.34112817409914, 10.410149578936398, 10.460287030902691, 10.381974941934459, 11.016889851889573, 11.220959026948549, 10.61129227397032, 11.050687938928604, 10.995634698192589, 11.90427906520199, 10.90164769499097, 10.131700474885292, 11.036255563958548, 11.602746379096061, 11.360980779863894, 11.514085376169533, 10.964943616068922, 11.278740403940901, 12.280151630169712, 10.970988581888378, 11.034527609008364, 10.594004281796515, 10.694082473986782, 10.558566453983076, 10.406198676908389, 10.556938832974993, 11.064749691984616, 10.646905508940108, 10.704059612005949, 10.556329728919081, 10.588667462929152, 10.726816484006122, 11.219732515979558, 10.912968431017362, 11.795744671020657, 11.694138336810283, 10.832227900042199, 11.032210506033152, 11.136008618865162, 11.712491977028549, 11.893783175037242, 10.59732458088547, 10.865798792918213, 10.394756809109822, 10.400972081930377, 10.770562484161928, 10.60345678019803, 10.706168430042453, 10.887343364069238, 10.97762824606616, 11.055932844989002, 10.785647216020152, 10.43652731901966, 10.914765649125911, 11.712118248920888, 12.043888318934478, 10.181410893099383, 10.401418555062264, 11.160004139877856, 11.802085974020883, 10.914691535988823, 11.127935479045846, 11.127214533858933, 11.750600636005402, 11.336759160971269, 10.466636071098037, 10.526965265860781]
Total Epoch List: [71, 63]
Total Time List: [1.9736579819582403, 2.1153991499450058]
========================k_idx:2========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: AEtransGAT
AEtransGAT(
  (gae): GAE(
    (encoder): transGAT(
      (encoder): EdgeFeatureGAT()
      (classifier): Sequential(
        (0): Linear(in_features=64, out_features=64, bias=True)
        (1): ReLU(inplace=True)
        (2): Linear(in_features=64, out_features=2, bias=True)
      )
    )
    (decoder): InnerProductDecoder()
  )
)
Total number of parameters: 1914242
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x72c48b90c850>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 2.5878;  Loss pred: 2.5878; Loss self: 0.0000; time: 6.71s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6965 score: 0.5000 time: 2.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6965 score: 0.5000 time: 2.23s
Epoch 2/1000, LR 0.000029
Train loss: 2.4847;  Loss pred: 2.4847; Loss self: 0.0000; time: 6.79s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6962 score: 0.5000 time: 2.38s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6961 score: 0.5000 time: 2.22s
Epoch 3/1000, LR 0.000059
Train loss: 2.2891;  Loss pred: 2.2891; Loss self: 0.0000; time: 6.77s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6956 score: 0.5000 time: 2.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6956 score: 0.5000 time: 2.14s
Epoch 4/1000, LR 0.000089
Train loss: 2.0397;  Loss pred: 2.0397; Loss self: 0.0000; time: 6.74s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6953 score: 0.5000 time: 2.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6952 score: 0.5000 time: 2.24s
Epoch 5/1000, LR 0.000119
Train loss: 1.7843;  Loss pred: 1.7843; Loss self: 0.0000; time: 6.80s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6945 score: 0.5000 time: 2.39s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6944 score: 0.5000 time: 2.30s
Epoch 6/1000, LR 0.000149
Train loss: 1.5441;  Loss pred: 1.5441; Loss self: 0.0000; time: 6.53s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6936 score: 0.5000 time: 2.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6936 score: 0.5000 time: 2.17s
Epoch 7/1000, LR 0.000179
Train loss: 1.3554;  Loss pred: 1.3554; Loss self: 0.0000; time: 6.51s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6927 score: 0.5000 time: 2.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6926 score: 0.5000 time: 2.09s
Epoch 8/1000, LR 0.000209
Train loss: 1.2176;  Loss pred: 1.2176; Loss self: 0.0000; time: 6.52s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6912 score: 0.5000 time: 2.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6911 score: 0.5000 time: 2.32s
Epoch 9/1000, LR 0.000239
Train loss: 1.1253;  Loss pred: 1.1253; Loss self: 0.0000; time: 6.63s
Val loss: 0.6888 score: 0.5077 time: 2.15s
Test loss: 0.6886 score: 0.5136 time: 2.12s
Epoch 10/1000, LR 0.000269
Train loss: 1.0683;  Loss pred: 1.0683; Loss self: 0.0000; time: 6.55s
Val loss: 0.6858 score: 0.5272 time: 2.17s
Test loss: 0.6855 score: 0.5331 time: 2.10s
Epoch 11/1000, LR 0.000299
Train loss: 1.0320;  Loss pred: 1.0320; Loss self: 0.0000; time: 6.54s
Val loss: 0.6807 score: 0.6325 time: 2.07s
Test loss: 0.6804 score: 0.6462 time: 2.07s
Epoch 12/1000, LR 0.000299
Train loss: 1.0100;  Loss pred: 1.0100; Loss self: 0.0000; time: 6.57s
Val loss: 0.6747 score: 0.8089 time: 2.24s
Test loss: 0.6742 score: 0.7970 time: 2.10s
Epoch 13/1000, LR 0.000299
Train loss: 0.9933;  Loss pred: 0.9933; Loss self: 0.0000; time: 6.40s
Val loss: 0.6657 score: 0.9183 time: 2.12s
Test loss: 0.6651 score: 0.9296 time: 2.07s
Epoch 14/1000, LR 0.000299
Train loss: 0.9813;  Loss pred: 0.9813; Loss self: 0.0000; time: 6.55s
Val loss: 0.6538 score: 0.9059 time: 2.13s
Test loss: 0.6529 score: 0.9118 time: 2.06s
Epoch 15/1000, LR 0.000299
Train loss: 0.9677;  Loss pred: 0.9677; Loss self: 0.0000; time: 6.48s
Val loss: 0.6356 score: 0.8728 time: 2.06s
Test loss: 0.6345 score: 0.8817 time: 2.03s
Epoch 16/1000, LR 0.000299
Train loss: 0.9554;  Loss pred: 0.9554; Loss self: 0.0000; time: 6.55s
Val loss: 0.6140 score: 0.8722 time: 2.22s
Test loss: 0.6127 score: 0.8781 time: 2.28s
Epoch 17/1000, LR 0.000299
Train loss: 0.9395;  Loss pred: 0.9395; Loss self: 0.0000; time: 6.88s
Val loss: 0.5865 score: 0.8604 time: 2.38s
Test loss: 0.5848 score: 0.8710 time: 2.21s
Epoch 18/1000, LR 0.000299
Train loss: 0.9218;  Loss pred: 0.9218; Loss self: 0.0000; time: 6.38s
Val loss: 0.5547 score: 0.8698 time: 2.21s
Test loss: 0.5527 score: 0.8793 time: 2.07s
Epoch 19/1000, LR 0.000299
Train loss: 0.9031;  Loss pred: 0.9031; Loss self: 0.0000; time: 6.17s
Val loss: 0.5182 score: 0.8858 time: 2.10s
Test loss: 0.5158 score: 0.8905 time: 2.15s
Epoch 20/1000, LR 0.000299
Train loss: 0.8813;  Loss pred: 0.8813; Loss self: 0.0000; time: 6.58s
Val loss: 0.4793 score: 0.8976 time: 2.12s
Test loss: 0.4764 score: 0.9006 time: 2.11s
Epoch 21/1000, LR 0.000299
Train loss: 0.8579;  Loss pred: 0.8579; Loss self: 0.0000; time: 6.76s
Val loss: 0.4394 score: 0.9101 time: 2.21s
Test loss: 0.4361 score: 0.9154 time: 2.25s
Epoch 22/1000, LR 0.000299
Train loss: 0.8339;  Loss pred: 0.8339; Loss self: 0.0000; time: 7.15s
Val loss: 0.3997 score: 0.9160 time: 2.45s
Test loss: 0.3961 score: 0.9243 time: 2.41s
Epoch 23/1000, LR 0.000299
Train loss: 0.8099;  Loss pred: 0.8099; Loss self: 0.0000; time: 6.59s
Val loss: 0.3614 score: 0.9260 time: 2.16s
Test loss: 0.3576 score: 0.9325 time: 2.15s
Epoch 24/1000, LR 0.000299
Train loss: 0.7858;  Loss pred: 0.7858; Loss self: 0.0000; time: 6.71s
Val loss: 0.3258 score: 0.9278 time: 2.17s
Test loss: 0.3219 score: 0.9367 time: 2.34s
Epoch 25/1000, LR 0.000299
Train loss: 0.7635;  Loss pred: 0.7635; Loss self: 0.0000; time: 7.29s
Val loss: 0.2938 score: 0.9325 time: 2.32s
Test loss: 0.2899 score: 0.9408 time: 2.34s
Epoch 26/1000, LR 0.000299
Train loss: 0.7439;  Loss pred: 0.7439; Loss self: 0.0000; time: 6.92s
Val loss: 0.2659 score: 0.9320 time: 2.42s
Test loss: 0.2622 score: 0.9420 time: 2.22s
Epoch 27/1000, LR 0.000299
Train loss: 0.7253;  Loss pred: 0.7253; Loss self: 0.0000; time: 6.39s
Val loss: 0.2421 score: 0.9325 time: 2.08s
Test loss: 0.2386 score: 0.9432 time: 2.08s
Epoch 28/1000, LR 0.000299
Train loss: 0.7097;  Loss pred: 0.7097; Loss self: 0.0000; time: 6.41s
Val loss: 0.2219 score: 0.9361 time: 2.09s
Test loss: 0.2189 score: 0.9467 time: 2.09s
Epoch 29/1000, LR 0.000299
Train loss: 0.6967;  Loss pred: 0.6967; Loss self: 0.0000; time: 6.44s
Val loss: 0.2051 score: 0.9391 time: 2.21s
Test loss: 0.2026 score: 0.9485 time: 2.08s
Epoch 30/1000, LR 0.000299
Train loss: 0.6846;  Loss pred: 0.6846; Loss self: 0.0000; time: 6.56s
Val loss: 0.1907 score: 0.9432 time: 2.19s
Test loss: 0.1889 score: 0.9515 time: 2.25s
Epoch 31/1000, LR 0.000299
Train loss: 0.6737;  Loss pred: 0.6737; Loss self: 0.0000; time: 6.90s
Val loss: 0.1787 score: 0.9450 time: 2.21s
Test loss: 0.1775 score: 0.9538 time: 2.15s
Epoch 32/1000, LR 0.000299
Train loss: 0.6647;  Loss pred: 0.6647; Loss self: 0.0000; time: 6.49s
Val loss: 0.1682 score: 0.9485 time: 2.08s
Test loss: 0.1679 score: 0.9544 time: 2.01s
Epoch 33/1000, LR 0.000299
Train loss: 0.6577;  Loss pred: 0.6577; Loss self: 0.0000; time: 6.82s
Val loss: 0.1590 score: 0.9527 time: 2.22s
Test loss: 0.1595 score: 0.9574 time: 2.23s
Epoch 34/1000, LR 0.000298
Train loss: 0.6494;  Loss pred: 0.6494; Loss self: 0.0000; time: 6.61s
Val loss: 0.1503 score: 0.9544 time: 2.09s
Test loss: 0.1517 score: 0.9586 time: 2.06s
Epoch 35/1000, LR 0.000298
Train loss: 0.6440;  Loss pred: 0.6440; Loss self: 0.0000; time: 6.45s
Val loss: 0.1427 score: 0.9550 time: 2.08s
Test loss: 0.1451 score: 0.9586 time: 2.06s
Epoch 36/1000, LR 0.000298
Train loss: 0.6386;  Loss pred: 0.6386; Loss self: 0.0000; time: 6.94s
Val loss: 0.1362 score: 0.9580 time: 2.09s
Test loss: 0.1395 score: 0.9592 time: 2.11s
Epoch 37/1000, LR 0.000298
Train loss: 0.6331;  Loss pred: 0.6331; Loss self: 0.0000; time: 6.59s
Val loss: 0.1308 score: 0.9604 time: 2.17s
Test loss: 0.1349 score: 0.9592 time: 2.10s
Epoch 38/1000, LR 0.000298
Train loss: 0.6300;  Loss pred: 0.6300; Loss self: 0.0000; time: 6.59s
Val loss: 0.1256 score: 0.9609 time: 2.15s
Test loss: 0.1306 score: 0.9604 time: 2.12s
Epoch 39/1000, LR 0.000298
Train loss: 0.6255;  Loss pred: 0.6255; Loss self: 0.0000; time: 6.76s
Val loss: 0.1213 score: 0.9609 time: 2.17s
Test loss: 0.1272 score: 0.9604 time: 2.17s
Epoch 40/1000, LR 0.000298
Train loss: 0.6224;  Loss pred: 0.6224; Loss self: 0.0000; time: 6.95s
Val loss: 0.1175 score: 0.9609 time: 2.49s
Test loss: 0.1242 score: 0.9604 time: 2.49s
Epoch 41/1000, LR 0.000298
Train loss: 0.6193;  Loss pred: 0.6193; Loss self: 0.0000; time: 7.22s
Val loss: 0.1137 score: 0.9627 time: 2.44s
Test loss: 0.1213 score: 0.9609 time: 2.19s
Epoch 42/1000, LR 0.000298
Train loss: 0.6168;  Loss pred: 0.6168; Loss self: 0.0000; time: 6.64s
Val loss: 0.1103 score: 0.9639 time: 2.10s
Test loss: 0.1186 score: 0.9609 time: 2.11s
Epoch 43/1000, LR 0.000298
Train loss: 0.6128;  Loss pred: 0.6128; Loss self: 0.0000; time: 6.80s
Val loss: 0.1080 score: 0.9627 time: 2.20s
Test loss: 0.1169 score: 0.9598 time: 2.21s
Epoch 44/1000, LR 0.000298
Train loss: 0.6123;  Loss pred: 0.6123; Loss self: 0.0000; time: 6.95s
Val loss: 0.1046 score: 0.9645 time: 2.13s
Test loss: 0.1143 score: 0.9645 time: 2.18s
Epoch 45/1000, LR 0.000298
Train loss: 0.6083;  Loss pred: 0.6083; Loss self: 0.0000; time: 7.07s
Val loss: 0.1023 score: 0.9645 time: 2.59s
Test loss: 0.1126 score: 0.9639 time: 2.80s
Epoch 46/1000, LR 0.000298
Train loss: 0.6074;  Loss pred: 0.6074; Loss self: 0.0000; time: 7.19s
Val loss: 0.1000 score: 0.9645 time: 2.39s
Test loss: 0.1109 score: 0.9645 time: 2.31s
Epoch 47/1000, LR 0.000298
Train loss: 0.6054;  Loss pred: 0.6054; Loss self: 0.0000; time: 6.51s
Val loss: 0.0978 score: 0.9651 time: 2.13s
Test loss: 0.1093 score: 0.9639 time: 2.14s
Epoch 48/1000, LR 0.000298
Train loss: 0.6038;  Loss pred: 0.6038; Loss self: 0.0000; time: 7.08s
Val loss: 0.0959 score: 0.9675 time: 2.22s
Test loss: 0.1079 score: 0.9669 time: 2.22s
Epoch 49/1000, LR 0.000298
Train loss: 0.6010;  Loss pred: 0.6010; Loss self: 0.0000; time: 7.09s
Val loss: 0.0942 score: 0.9734 time: 2.41s
Test loss: 0.1067 score: 0.9698 time: 2.30s
Epoch 50/1000, LR 0.000298
Train loss: 0.5991;  Loss pred: 0.5991; Loss self: 0.0000; time: 6.51s
Val loss: 0.0924 score: 0.9740 time: 2.15s
Test loss: 0.1054 score: 0.9716 time: 2.14s
Epoch 51/1000, LR 0.000298
Train loss: 0.5980;  Loss pred: 0.5980; Loss self: 0.0000; time: 6.58s
Val loss: 0.0909 score: 0.9734 time: 2.12s
Test loss: 0.1043 score: 0.9716 time: 2.22s
Epoch 52/1000, LR 0.000298
Train loss: 0.5954;  Loss pred: 0.5954; Loss self: 0.0000; time: 6.87s
Val loss: 0.0893 score: 0.9740 time: 2.11s
Test loss: 0.1032 score: 0.9722 time: 2.15s
Epoch 53/1000, LR 0.000298
Train loss: 0.5945;  Loss pred: 0.5945; Loss self: 0.0000; time: 6.75s
Val loss: 0.0879 score: 0.9746 time: 2.25s
Test loss: 0.1021 score: 0.9722 time: 2.27s
Epoch 54/1000, LR 0.000297
Train loss: 0.5921;  Loss pred: 0.5921; Loss self: 0.0000; time: 6.77s
Val loss: 0.0862 score: 0.9757 time: 2.21s
Test loss: 0.1010 score: 0.9734 time: 2.17s
Epoch 55/1000, LR 0.000297
Train loss: 0.5903;  Loss pred: 0.5903; Loss self: 0.0000; time: 6.49s
Val loss: 0.0856 score: 0.9746 time: 2.10s
Test loss: 0.1008 score: 0.9722 time: 2.09s
Epoch 56/1000, LR 0.000297
Train loss: 0.5900;  Loss pred: 0.5900; Loss self: 0.0000; time: 6.37s
Val loss: 0.0841 score: 0.9751 time: 2.10s
Test loss: 0.0997 score: 0.9728 time: 2.15s
Epoch 57/1000, LR 0.000297
Train loss: 0.5879;  Loss pred: 0.5879; Loss self: 0.0000; time: 6.54s
Val loss: 0.0831 score: 0.9751 time: 2.24s
Test loss: 0.0991 score: 0.9728 time: 2.14s
Epoch 58/1000, LR 0.000297
Train loss: 0.5860;  Loss pred: 0.5860; Loss self: 0.0000; time: 6.36s
Val loss: 0.0822 score: 0.9751 time: 2.10s
Test loss: 0.0986 score: 0.9728 time: 2.08s
Epoch 59/1000, LR 0.000297
Train loss: 0.5850;  Loss pred: 0.5850; Loss self: 0.0000; time: 6.49s
Val loss: 0.0809 score: 0.9751 time: 2.20s
Test loss: 0.0976 score: 0.9728 time: 2.03s
Epoch 60/1000, LR 0.000297
Train loss: 0.5831;  Loss pred: 0.5831; Loss self: 0.0000; time: 6.43s
Val loss: 0.0804 score: 0.9751 time: 2.11s
Test loss: 0.0975 score: 0.9722 time: 2.13s
Epoch 61/1000, LR 0.000297
Train loss: 0.5823;  Loss pred: 0.5823; Loss self: 0.0000; time: 6.69s
Val loss: 0.0794 score: 0.9751 time: 2.13s
Test loss: 0.0968 score: 0.9728 time: 2.09s
Epoch 62/1000, LR 0.000297
Train loss: 0.5811;  Loss pred: 0.5811; Loss self: 0.0000; time: 6.39s
Val loss: 0.0786 score: 0.9751 time: 2.11s
Test loss: 0.0965 score: 0.9734 time: 2.16s
Epoch 63/1000, LR 0.000297
Train loss: 0.5791;  Loss pred: 0.5791; Loss self: 0.0000; time: 6.86s
Val loss: 0.0776 score: 0.9757 time: 2.09s
Test loss: 0.0959 score: 0.9734 time: 2.13s
Epoch 64/1000, LR 0.000297
Train loss: 0.5764;  Loss pred: 0.5764; Loss self: 0.0000; time: 7.09s
Val loss: 0.0766 score: 0.9763 time: 2.29s
Test loss: 0.0952 score: 0.9740 time: 2.20s
Epoch 65/1000, LR 0.000297
Train loss: 0.5760;  Loss pred: 0.5760; Loss self: 0.0000; time: 6.71s
Val loss: 0.0762 score: 0.9763 time: 2.40s
Test loss: 0.0952 score: 0.9734 time: 2.22s
Epoch 66/1000, LR 0.000297
Train loss: 0.5757;  Loss pred: 0.5757; Loss self: 0.0000; time: 6.50s
Val loss: 0.0755 score: 0.9763 time: 2.06s
Test loss: 0.0948 score: 0.9728 time: 2.03s
Epoch 67/1000, LR 0.000297
Train loss: 0.5744;  Loss pred: 0.5744; Loss self: 0.0000; time: 6.38s
Val loss: 0.0752 score: 0.9763 time: 2.07s
Test loss: 0.0948 score: 0.9734 time: 2.04s
Epoch 68/1000, LR 0.000296
Train loss: 0.5719;  Loss pred: 0.5719; Loss self: 0.0000; time: 6.70s
Val loss: 0.0742 score: 0.9769 time: 2.25s
Test loss: 0.0943 score: 0.9740 time: 2.25s
Epoch 69/1000, LR 0.000296
Train loss: 0.5699;  Loss pred: 0.5699; Loss self: 0.0000; time: 6.73s
Val loss: 0.0739 score: 0.9769 time: 2.19s
Test loss: 0.0942 score: 0.9728 time: 2.17s
Epoch 70/1000, LR 0.000296
Train loss: 0.5700;  Loss pred: 0.5700; Loss self: 0.0000; time: 6.90s
Val loss: 0.0736 score: 0.9769 time: 2.32s
Test loss: 0.0941 score: 0.9728 time: 2.37s
Epoch 71/1000, LR 0.000296
Train loss: 0.5682;  Loss pred: 0.5682; Loss self: 0.0000; time: 6.89s
Val loss: 0.0736 score: 0.9769 time: 2.30s
Test loss: 0.0944 score: 0.9728 time: 2.17s
Epoch 72/1000, LR 0.000296
Train loss: 0.5678;  Loss pred: 0.5678; Loss self: 0.0000; time: 6.33s
Val loss: 0.0733 score: 0.9769 time: 2.24s
Test loss: 0.0944 score: 0.9716 time: 2.07s
Epoch 73/1000, LR 0.000296
Train loss: 0.5662;  Loss pred: 0.5662; Loss self: 0.0000; time: 6.96s
Val loss: 0.0725 score: 0.9775 time: 2.29s
Test loss: 0.0939 score: 0.9734 time: 2.34s
Epoch 74/1000, LR 0.000296
Train loss: 0.5638;  Loss pred: 0.5638; Loss self: 0.0000; time: 7.26s
Val loss: 0.0721 score: 0.9775 time: 2.61s
Test loss: 0.0939 score: 0.9722 time: 2.47s
Epoch 75/1000, LR 0.000296
Train loss: 0.5632;  Loss pred: 0.5632; Loss self: 0.0000; time: 6.68s
Val loss: 0.0719 score: 0.9775 time: 2.09s
Test loss: 0.0941 score: 0.9716 time: 2.07s
Epoch 76/1000, LR 0.000296
Train loss: 0.5628;  Loss pred: 0.5628; Loss self: 0.0000; time: 6.47s
Val loss: 0.0714 score: 0.9775 time: 2.08s
Test loss: 0.0940 score: 0.9728 time: 2.05s
Epoch 77/1000, LR 0.000296
Train loss: 0.5604;  Loss pred: 0.5604; Loss self: 0.0000; time: 6.55s
Val loss: 0.0717 score: 0.9775 time: 2.13s
Test loss: 0.0945 score: 0.9710 time: 2.15s
     INFO: Early stopping counter 1 of 2
Epoch 78/1000, LR 0.000296
Train loss: 0.5586;  Loss pred: 0.5586; Loss self: 0.0000; time: 6.99s
Val loss: 0.0715 score: 0.9769 time: 2.11s
Test loss: 0.0946 score: 0.9716 time: 2.17s
     INFO: Early stopping counter 2 of 2
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 075,   Train_Loss: 0.5628,   Val_Loss: 0.0714,   Val_Precision: 0.9856,   Val_Recall: 0.9692,   Val_accuracy: 0.9773,   Val_Score: 0.9775,   Val_Loss: 0.0714,   Test_Precision: 0.9842,   Test_Recall: 0.9609,   Test_accuracy: 0.9725,   Test_Score: 0.9728,   Test_loss: 0.0940


[2.038615496014245, 2.1797736400039867, 2.508276980021037, 2.0709083219990134, 1.9787363869836554, 2.0085195670835674, 1.9623766009462997, 2.032092873007059, 1.9712995940353721, 2.0887712369440123, 2.2061336210463196, 2.006376295001246, 2.1562758079962805, 2.1511155490297824, 1.9808427940588444, 2.165679002995603, 2.0063041859539226, 2.195463145035319, 2.4259330090135336, 2.0127986050210893, 2.08797425404191, 2.0258984819520265, 2.0238514309749007, 2.3316692339722067, 2.333854664932005, 2.052857381058857, 2.085316276992671, 2.170126355951652, 2.1565107069909573, 1.9409398069838062, 2.077659692033194, 2.022638107999228, 1.968419419019483, 2.068248547031544, 2.277436733013019, 2.0734027589205652, 2.037791138049215, 2.1477141209179536, 2.1064586710417643, 2.012583974050358, 2.1490561430109665, 2.0260879449779168, 2.205405179061927, 1.9528894959948957, 1.9991489839740098, 2.011540984036401, 1.9964037740137428, 2.0313305110903457, 2.1559219850460067, 2.0427664380986243, 1.9805727240163833, 2.1297520250082016, 2.000376177020371, 1.994586731074378, 1.9862059820443392, 2.206796965096146, 2.0509002619655803, 1.918878818047233, 1.9473299869569018, 2.069372785044834, 1.975238826009445, 1.9242721749469638, 2.2250378399621695, 2.014727841014974, 2.02068390394561, 2.087973227025941, 2.0246083410456777, 2.0275183609919623, 2.017316202982329, 2.0263983299955726, 1.9722057769540697, 2.285455900011584, 2.456490562064573, 2.123919714940712, 2.2999873269582167, 2.204477241029963, 2.65552795806434, 2.097316027036868, 2.058954322943464, 2.269513361970894, 2.4784478259971365, 2.2722701930906624, 2.2415590309537947, 2.203143520047888, 2.3235208559781313, 2.6207863689633086, 2.1342287249863148, 2.3471294980263337, 2.148788296035491, 2.168985861935653, 2.1282924469560385, 2.1263596509816125, 2.151534255943261, 2.1563696659868583, 2.1710237929364666, 2.2239382750121877, 2.1845873809652403, 2.127945325919427, 2.2704587809275836, 2.170224132016301, 2.2047560369828716, 2.441716630011797, 2.366347378003411, 2.1875903489999473, 2.191411138046533, 2.239357273094356, 2.3006402660394087, 2.506089695962146, 2.1159373190021142, 2.1958802200388163, 2.1128611040767282, 2.1037045100238174, 2.1952009820379317, 2.2221052480163053, 2.1727849900489673, 2.1150627710158005, 2.2464506500400603, 2.250157912960276, 2.1326914880191907, 2.1311617150204256, 2.2938323550624773, 2.372392990044318, 2.484147975919768, 2.0714518160093576, 2.137157795019448, 2.345140376011841, 2.4739222140051425, 2.1416693539358675, 2.4186650450574234, 2.1746826419839635, 2.4637944570276886, 2.1908706640824676, 2.1350690710823983, 2.1147852879948914, 2.233392446069047, 2.2212952569825575, 2.1437783780274913, 2.2468069730093703, 2.310100188013166, 2.1729974569752812, 2.093778838054277, 2.324772892985493, 2.1244379789568484, 2.101443788036704, 2.0776525830151513, 2.100647817016579, 2.0704024279257283, 2.062711391947232, 2.0376744139939547, 2.29007760505192, 2.2119556550169364, 2.0744012859649956, 2.1578466979553923, 2.1170436600223184, 2.255250713089481, 2.4195105540566146, 2.154060643981211, 2.344646846060641, 2.340905705001205, 2.228368428070098, 2.083832399919629, 2.0909178900765255, 2.0861981509951875, 2.2549340639961883, 2.1526945090154186, 2.0120104090310633, 2.2309311269782484, 2.069434392033145, 2.0666293549584225, 2.11217985895928, 2.1054362789727747, 2.127840877044946, 2.1776630950625986, 2.497376893996261, 2.196090165991336, 2.111509062931873, 2.2111998249311, 2.1845439560711384, 2.807234626961872, 2.3164927299367264, 2.140278305974789, 2.2275135760428384, 2.305630836985074, 2.1453401630278677, 2.225861249025911, 2.1517522629583254, 2.2791863530874252, 2.1779241199837998, 2.093030488002114, 2.1560354130342603, 2.143124147085473, 2.0884466539137065, 2.0329577380325645, 2.1358768640784547, 2.091792413033545, 2.1615419619483873, 2.137069547083229, 2.200697944033891, 2.2255036589922383, 2.0387119870865718, 2.048192731104791, 2.2572363329818472, 2.172396773006767, 2.377516942913644, 2.1770771400770172, 2.070820976048708, 2.346462568966672, 2.480396390077658, 2.073293215013109, 2.0582972129341215, 2.1533978109946474, 2.1704993799794465]
[0.0012062813585883105, 0.0012898068875763235, 0.0014841875621426255, 0.0012253895396443866, 0.0011708499331264234, 0.0011884731166174954, 0.0011611695863587572, 0.0012024218183473722, 0.0011664494639262557, 0.0012359593118011908, 0.0013054045094948636, 0.001187204908284761, 0.0012759028449682132, 0.0012728494372957293, 0.00117209632784547, 0.0012814668656778714, 0.0011871622402094217, 0.001299090618364094, 0.0014354633189429193, 0.0011910050917284553, 0.001235487724285154, 0.0011987564981964655, 0.0011975452254289352, 0.0013796859372616607, 0.0013809790916757426, 0.001214708509502282, 0.0012339149568003972, 0.0012840984354743502, 0.0012760418384561876, 0.0011484850928898261, 0.0012293844331557361, 0.00119682728283978, 0.0011647452183547236, 0.001223815708302689, 0.0013475957000077035, 0.0012268655378228196, 0.0012057935728101864, 0.0012708367579396175, 0.0012464252491371387, 0.0011908780911540579, 0.0012716308538526428, 0.0011988686064958087, 0.001304973478734868, 0.0011555559147898792, 0.0011829283928840295, 0.0011902609372996455, 0.0011813040082921556, 0.0012019707166215064, 0.0012756934822757437, 0.001208737537336464, 0.0011719365230866174, 0.001260208298821421, 0.001183654542615604, 0.0011802288349552531, 0.0011752698118605558, 0.0013057970207669503, 0.0012135504508672074, 0.0011354312532823863, 0.0011522662644715396, 0.0012244809378963515, 0.0011687803704197898, 0.001138622588726014, 0.0013165904378474376, 0.0011921466514881502, 0.0011956709490802426, 0.001235487116583397, 0.0011979931012104602, 0.0011997150065041196, 0.0011936782266167626, 0.0011990522662695695, 0.0011669856668367277, 0.0013523407692376237, 0.0014535447112808124, 0.001256757227775569, 0.0013609392467208382, 0.0013044244029763094, 0.0015713183183812664, 0.0012410154006135313, 0.001218316167422168, 0.001342907314775677, 0.0014665371751462347, 0.0013445385757932912, 0.001326366290505204, 0.0013036352189632475, 0.0013748644118213795, 0.0015507611650670465, 0.001262857233719713, 0.0013888340225007892, 0.0012714723645180419, 0.0012834235869441733, 0.0012593446431692534, 0.0012582009769121967, 0.001273097192865835, 0.0012759583822407445, 0.0012846294632760157, 0.0013159398076995194, 0.001292655255009018, 0.0012591392461061698, 0.0013434667342766768, 0.001284156291133906, 0.001304589370995782, 0.0014448027396519508, 0.0014002055491144443, 0.0012944321591715664, 0.001296692981092623, 0.001325063475203761, 0.0013613256012067507, 0.0014828933112202047, 0.0012520339165692984, 0.001299337408306992, 0.0012502136710513185, 0.0012447955680614305, 0.0012989354923301371, 0.001314855176341009, 0.0012856715917449512, 0.0012515164325537281, 0.0013292607396686747, 0.0013314543863670273, 0.0012619476260468585, 0.0012610424349233287, 0.0013572972515162587, 0.0014037828343457504, 0.0014699100449229398, 0.0012257111337333477, 0.0012645904112541112, 0.0013876570272259414, 0.001463859298227895, 0.001267259972743117, 0.0014311627485546884, 0.0012867944627124044, 0.0014578665426199342, 0.0012963731740132944, 0.0012633544799304133, 0.0012513522414170956, 0.0013215339917568324, 0.0013143758917056553, 0.0012685079159925985, 0.0013294715816623492, 0.001366923188173471, 0.001285797311819693, 0.001238922389381229, 0.0013756052621215935, 0.0012570638928738748, 0.001243457862743612, 0.0012293802266361842, 0.0012429868739743072, 0.0012250901940388925, 0.0012205392851758771, 0.0012057245053218666, 0.0013550755059478815, 0.0013088495000100216, 0.001227456382227808, 0.0012768323656540783, 0.0012526885562262238, 0.0013344678775677403, 0.0014316630497376419, 0.0012745920970303024, 0.0013873649976690185, 0.0013851513047344409, 0.0013185612000414782, 0.0012330369230293663, 0.0012372295207553406, 0.0012344367757367973, 0.0013342805112403482, 0.001273783733145218, 0.0011905387035686765, 0.0013200775899279576, 0.0012245173917355887, 0.0012228576064842738, 0.0012498105674315265, 0.001245820283415843, 0.0012590774420384295, 0.0012885580444157389, 0.0014777378070983792, 0.001299461636681264, 0.0012494136467052503, 0.0013084022632728403, 0.0012926295598054073, 0.001661085578083948, 0.0013707057573590098, 0.0012664368674407036, 0.0013180553704395493, 0.001364278601766316, 0.0012694320491289158, 0.0013170776621455095, 0.001273226191099601, 0.0013486309781582397, 0.0012887124970318341, 0.0012384795786994757, 0.0012757605994285564, 0.0012681207970919959, 0.0012357672508365128, 0.001202933572800334, 0.0012638324639517482, 0.0012377469899606775, 0.0012790189123954956, 0.001264538193540372, 0.0013021881325644324, 0.0013168660704096086, 0.0012063384538973797, 0.0012119483616004681, 0.0013356428005809747, 0.001285441877518797, 0.0014068147591204995, 0.0012882113254893593, 0.001225337855650123, 0.0013884393899211075, 0.0014676901716435846, 0.001226800718942668, 0.0012179273449314328, 0.0012741998881625133, 0.0012843191597511519]
[828.99399288594, 775.3098619895729, 673.7692900190895, 816.0670281959517, 854.0804177438719, 841.4157510319565, 861.2006478191016, 831.6549024155399, 857.3024643811068, 809.0881232511432, 766.04607439801, 842.3145768869595, 783.7587351918736, 785.6388750303258, 853.1721977477611, 780.3557210751751, 842.3448507118916, 769.7692415478068, 696.6391873645395, 839.6269730037362, 809.3969534004025, 834.1977720283516, 835.0415322659912, 724.8026329707727, 724.1239248499807, 823.2427715598557, 810.4286235358146, 778.7564974569856, 783.6733638842474, 870.7122157622377, 813.4152125491587, 835.5424498907172, 858.55686225745, 817.1164932887656, 742.062326255778, 815.0852470553436, 829.3293500225167, 786.8831254309013, 802.2944020848975, 839.7165145854002, 786.3917401581705, 834.1197647362835, 766.2990982540653, 865.3843463575151, 845.3597073293319, 840.1519101086423, 846.5221424633344, 831.9670239644402, 783.8873631431222, 827.3094605827899, 853.288535940688, 793.5196117461102, 844.8410950970797, 847.293313281838, 850.8684473201195, 765.8158075844417, 824.0283700487249, 880.7226303742548, 867.8549661944907, 816.6725745179766, 855.5927403544866, 878.2541378516681, 759.537644550231, 838.82297429742, 836.3505032628246, 809.3973515202565, 834.7293477646853, 833.5312924974787, 837.746704012769, 833.9920019593051, 856.9085537362555, 739.4585911683678, 687.9733332171359, 795.6986265119609, 734.786657383483, 766.6216591151597, 636.4082874246483, 805.7917730155657, 820.8049985217691, 744.6530292874627, 681.8783846378021, 743.7495792264568, 753.9395468344621, 767.0857502570989, 727.3444504067348, 644.8446237411197, 791.8551466459326, 720.0284438592314, 786.4897640768269, 779.1659824337468, 794.0638056659458, 794.7855854111173, 785.4859830056862, 783.7246213656854, 778.4345825681407, 759.9131769926206, 773.6014657620556, 794.1933373075726, 744.3429557922033, 778.7214117971599, 766.5247182235652, 692.1360076053686, 714.1808576836786, 772.5395208351418, 771.1925757147056, 754.6808275326022, 734.5781193812467, 674.3573475135214, 798.7004080050026, 769.6230352537744, 799.863273898684, 803.3447625117591, 769.8611716322555, 760.5400336049246, 777.8036058514529, 799.0306591176696, 752.2978526013303, 751.0583991754871, 792.4259132152512, 792.9947258759772, 736.7582884905158, 712.3608976641048, 680.3137399148974, 815.8529138542924, 790.7698738663428, 720.6391639864321, 683.1257629818457, 789.1040682326574, 698.7325522620584, 777.1248858905742, 685.9338428899662, 771.3828240553711, 791.543478798667, 799.135500702463, 756.6963893759639, 760.8173630621815, 788.3277568808131, 752.1785450649622, 731.5700023614595, 777.7275553522309, 807.153061863256, 726.9527294899278, 795.504513071185, 804.2089965103945, 813.4179957783999, 804.5137249137758, 816.2664307214701, 819.309965804093, 829.3768564760582, 737.9662576813352, 764.0297834031669, 814.6929002764405, 783.1881669820718, 798.2830169795287, 749.36236144001, 698.4883769845525, 784.5647264955746, 720.7908529335468, 721.9427917961052, 758.4024161855687, 811.0057219885732, 808.25746817736, 810.0860405775993, 749.4675906420902, 785.062624038075, 839.9558930780403, 757.531229701865, 816.6482622044547, 817.7566993061513, 800.1212552195731, 802.6839932788359, 794.2323217077206, 776.0612758841007, 676.7100328599942, 769.549459385297, 800.3754422220669, 764.2909432903286, 773.6168435994456, 602.0159425822563, 729.5511780199549, 789.6169368638674, 758.6934679887665, 732.9881145283019, 787.7538625924877, 759.2566700820114, 785.4064006776098, 741.4926812415741, 775.9682646852596, 807.4416544276794, 783.8461232051875, 788.5684094868251, 809.2138704299554, 831.3010980914593, 791.2441154369484, 807.9195571558364, 781.8492676758655, 790.8025278384553, 767.9381918729933, 759.3786661151897, 828.954757074392, 825.1176631646463, 748.7031709114311, 777.9426028427155, 710.8256389243254, 776.2701508777102, 816.1014493993852, 720.2330957038182, 681.3427106895153, 815.128312658523, 821.0670399688729, 784.8062217632674, 778.6226596461886]
Elapsed: 2.165210606880104~0.14212141722101548
Time per graph: 0.001281189708213079~8.409551314852987e-05
Speed: 783.6931821545443~48.512059556526616
Total Time: 2.1714
best val loss: 0.0713767214496372 test_score: 0.9728

Testing...
Test loss: 0.0939 score: 0.9734 time: 2.32s
test Score 0.9734
Epoch Time List: [11.642819940927438, 11.075978060020134, 11.720096316887066, 10.574460907955654, 10.56905995181296, 10.320808705990203, 10.241954101016745, 10.283787759952247, 10.169229754945263, 10.585050416062586, 10.787243144935928, 10.669381032930687, 10.948988351854496, 11.09731895616278, 10.721414203988388, 10.987515883985907, 10.708370489999652, 11.235983202932402, 12.162264017038979, 10.671224665129557, 10.767038193996996, 10.448549464927055, 10.526551429880783, 11.219654626911506, 11.68865094112698, 10.678101818077266, 10.963107087998651, 11.101969216950238, 11.23012452002149, 10.230854956898838, 10.708556358120404, 10.828183440025896, 10.484837236930616, 10.651560379075818, 11.224814778077416, 10.80947511701379, 10.630397756933235, 11.113010516040958, 10.963644942967221, 10.449040114996023, 10.617351107066497, 10.607991177006625, 10.636651432025246, 10.170795076177455, 10.48250787705183, 10.270433225785382, 10.234480639919639, 10.66967289603781, 10.673855543020181, 10.454113586107269, 10.320850205956958, 10.551990934996866, 10.642094478942454, 10.4228778189281, 11.120462722959928, 11.121453920030035, 10.964376344927587, 10.02190934191458, 9.991373145021498, 10.18325537594501, 10.44610322790686, 10.244469390017912, 10.38119343703147, 10.468194533023052, 10.358507276047021, 10.803181883995421, 10.58225236099679, 10.34112817409914, 10.410149578936398, 10.460287030902691, 10.381974941934459, 11.016889851889573, 11.220959026948549, 10.61129227397032, 11.050687938928604, 10.995634698192589, 11.90427906520199, 10.90164769499097, 10.131700474885292, 11.036255563958548, 11.602746379096061, 11.360980779863894, 11.514085376169533, 10.964943616068922, 11.278740403940901, 12.280151630169712, 10.970988581888378, 11.034527609008364, 10.594004281796515, 10.694082473986782, 10.558566453983076, 10.406198676908389, 10.556938832974993, 11.064749691984616, 10.646905508940108, 10.704059612005949, 10.556329728919081, 10.588667462929152, 10.726816484006122, 11.219732515979558, 10.912968431017362, 11.795744671020657, 11.694138336810283, 10.832227900042199, 11.032210506033152, 11.136008618865162, 11.712491977028549, 11.893783175037242, 10.59732458088547, 10.865798792918213, 10.394756809109822, 10.400972081930377, 10.770562484161928, 10.60345678019803, 10.706168430042453, 10.887343364069238, 10.97762824606616, 11.055932844989002, 10.785647216020152, 10.43652731901966, 10.914765649125911, 11.712118248920888, 12.043888318934478, 10.181410893099383, 10.401418555062264, 11.160004139877856, 11.802085974020883, 10.914691535988823, 11.127935479045846, 11.127214533858933, 11.750600636005402, 11.336759160971269, 10.466636071098037, 10.526965265860781, 11.164951251936145, 11.384730249992572, 11.141015910892747, 11.202864160994068, 11.49489575799089, 10.840660503017716, 10.72342127119191, 11.053843764937483, 10.891375943785533, 10.824034431949258, 10.690571816056035, 10.907772622071207, 10.58525853289757, 10.733331017079763, 10.57331656909082, 11.049924648017623, 11.469307326013222, 10.654624688904732, 10.428390766959637, 10.817355499020778, 11.21624005807098, 12.01865096297115, 10.903092444059439, 11.221585492021404, 11.94882586505264, 11.567267916048877, 10.554368554032408, 10.59169248107355, 10.730516135925427, 10.99009422515519, 11.248580673942342, 10.57285653008148, 11.270012014894746, 10.76479630288668, 10.587045709020458, 11.144178138114512, 10.857515756040812, 10.861955796019174, 11.108717050985433, 11.927552924025804, 11.85305609414354, 10.848515142104588, 11.19921246310696, 11.252627526060678, 12.457356701837853, 11.894504172028974, 10.77654707513284, 11.525836148066446, 11.803610573988408, 10.8008936510887, 10.928341120947152, 11.123834053054452, 11.275573358987458, 11.155504097114317, 10.678863648092374, 10.620624694041908, 10.921699167927727, 10.544967452879064, 10.725871505099349, 10.67978370306082, 10.9040597518906, 10.64811497705523, 11.082490429049358, 11.58482493693009, 11.333081724005751, 10.593695652089082, 10.491414054995403, 11.202202852000482, 11.090776161989197, 11.591505462885834, 11.359099239925854, 10.630670417100191, 11.588890101993456, 12.347361949039623, 10.837645482039079, 10.60002152796369, 10.823521120939404, 11.270264712977223]
Total Epoch List: [71, 63, 78]
Total Time List: [1.9736579819582403, 2.1153991499450058, 2.1714082290418446]
T-times Epoch Time: 11.998732535712222 ~ 1.0749456685905683
T-times Total Epoch: 72.66666666666667 ~ 2.0
T-times Total Time: 2.2832460461456017 ~ 0.19642425916390494
T-times Inference Elapsed: 2.4005621661846446 ~ 0.23535155930454055
T-times Time Per Graph: 0.0014204509859080735 ~ 0.0001392612776949945
T-times Speed: 718.9880141056153 ~ 64.7051680489289
T-times cross validation test micro f1 score:0.972530255838582 ~ 0.0002958579881656709
T-times cross validation test precision:0.9850757034194466 ~ 4.5129787979991676e-06
T-times cross validation test recall:0.9603550295857988 ~ 0.0005917159763313418
T-times cross validation test f1_score:0.972530255838582 ~ 0.00030459316064507247
