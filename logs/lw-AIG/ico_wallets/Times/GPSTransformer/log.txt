Namespace(seed=15, model='GPSTransformer', dataset='ico_wallets/Times', num_heads=1, num_layers=2, dim_hidden=64, dropout=0.1, epochs=1000, lr=0.0003, weight_decay=0.0001, batch_size=64, abs_pe=None, abs_pe_dim=20, num_class=2, outdir='./outdir/ico_wallets/Times/seed15/edge_attr/khopgnn_gat_1_0.1_0.0003_0.0001_2_1_64_BN', warmup=10, layer_norm=False, use_edge_attr=True, edge_dim=128, gnn_type='gat', k_hop=1, global_pool='mean', se='khopgnn', ie='mamba', aggr='add', not_extract_node_feature=False, k_ford=3, early_stop=1, early_stop_mindelta=-0.0, patience=20, training_times=3, Lambda=0.01, order='sd', use_cuda=True, batch_norm=True, save_logs=True)
Data(edge_index=[2, 338], edge_attr=[338, 2], x=[122, 14887], y=[1, 1], num_nodes=122)
Data(edge_index=[2, 338], edge_attr=[338, 2], x=[122, 14887], y=[1, 1], num_nodes=122)
Data(edge_index=[2, 298], edge_attr=[298, 2], x=[109, 14887], y=[1, 1], num_nodes=122)
========================training times:0========================
========================k_idx:0========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: GPSTransformer
GPS(
  (x_lin): Linear(in_features=14887, out_features=64, bias=True)
  (pe_lin): Linear(in_features=20, out_features=8, bias=True)
  (pe_norm): BatchNorm1d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (convs): ModuleList(
    (0-1): 2 x GPSConv(64, conv=GINEConv(nn=Sequential(
      (0): Linear(in_features=64, out_features=64, bias=True)
      (1): ReLU()
      (2): Linear(in_features=64, out_features=64, bias=True)
    )), heads=1, attn_type=multihead)
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=32, bias=True)
    (1): ReLU()
    (2): Linear(in_features=32, out_features=16, bias=True)
    (3): ReLU()
    (4): Linear(in_features=16, out_features=2, bias=True)
  )
)
Total number of parameters: 1039906
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x737bd998c190>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.7323;  Loss pred: 0.7323; Loss self: 0.0000; time: 0.37s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.5931 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.5943 score: 0.5102 time: 0.07s
Epoch 2/1000, LR 0.000000
Train loss: 0.7379;  Loss pred: 0.7379; Loss self: 0.0000; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.5852 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.5960 score: 0.5102 time: 0.07s
Epoch 3/1000, LR 0.000030
Train loss: 0.6840;  Loss pred: 0.6840; Loss self: 0.0000; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.5910 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6008 score: 0.5102 time: 0.07s
     INFO: Early stopping counter 1 of 20
Epoch 4/1000, LR 0.000060
Train loss: 0.6222;  Loss pred: 0.6222; Loss self: 0.0000; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.5943 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6145 score: 0.5102 time: 0.07s
     INFO: Early stopping counter 2 of 20
Epoch 5/1000, LR 0.000090
Train loss: 0.5437;  Loss pred: 0.5437; Loss self: 0.0000; time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.5986 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6264 score: 0.5102 time: 0.06s
     INFO: Early stopping counter 3 of 20
Epoch 6/1000, LR 0.000120
Train loss: 0.4708;  Loss pred: 0.4708; Loss self: 0.0000; time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6053 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6332 score: 0.5102 time: 0.07s
     INFO: Early stopping counter 4 of 20
Epoch 7/1000, LR 0.000150
Train loss: 0.4218;  Loss pred: 0.4218; Loss self: 0.0000; time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6114 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6344 score: 0.5102 time: 0.07s
     INFO: Early stopping counter 5 of 20
Epoch 8/1000, LR 0.000180
Train loss: 0.3668;  Loss pred: 0.3668; Loss self: 0.0000; time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6162 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6355 score: 0.5102 time: 0.07s
     INFO: Early stopping counter 6 of 20
Epoch 9/1000, LR 0.000210
Train loss: 0.3298;  Loss pred: 0.3298; Loss self: 0.0000; time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6181 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6345 score: 0.5102 time: 0.07s
     INFO: Early stopping counter 7 of 20
Epoch 10/1000, LR 0.000240
Train loss: 0.2990;  Loss pred: 0.2990; Loss self: 0.0000; time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6176 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6315 score: 0.5102 time: 0.07s
     INFO: Early stopping counter 8 of 20
Epoch 11/1000, LR 0.000270
Train loss: 0.2627;  Loss pred: 0.2627; Loss self: 0.0000; time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6136 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6283 score: 0.5102 time: 0.07s
     INFO: Early stopping counter 9 of 20
Epoch 12/1000, LR 0.000270
Train loss: 0.2293;  Loss pred: 0.2293; Loss self: 0.0000; time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6113 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6270 score: 0.5102 time: 0.07s
     INFO: Early stopping counter 10 of 20
Epoch 13/1000, LR 0.000270
Train loss: 0.2018;  Loss pred: 0.2018; Loss self: 0.0000; time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6118 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6264 score: 0.5102 time: 0.07s
     INFO: Early stopping counter 11 of 20
Epoch 14/1000, LR 0.000270
Train loss: 0.1799;  Loss pred: 0.1799; Loss self: 0.0000; time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6148 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6259 score: 0.5102 time: 0.07s
     INFO: Early stopping counter 12 of 20
Epoch 15/1000, LR 0.000270
Train loss: 0.1517;  Loss pred: 0.1517; Loss self: 0.0000; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6209 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6265 score: 0.5102 time: 0.07s
     INFO: Early stopping counter 13 of 20
Epoch 16/1000, LR 0.000270
Train loss: 0.1284;  Loss pred: 0.1284; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6253 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6237 score: 0.5102 time: 0.07s
     INFO: Early stopping counter 14 of 20
Epoch 17/1000, LR 0.000270
Train loss: 0.1132;  Loss pred: 0.1132; Loss self: 0.0000; time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6252 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6194 score: 0.5102 time: 0.07s
     INFO: Early stopping counter 15 of 20
Epoch 18/1000, LR 0.000270
Train loss: 0.1001;  Loss pred: 0.1001; Loss self: 0.0000; time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6195 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6149 score: 0.5102 time: 0.06s
     INFO: Early stopping counter 16 of 20
Epoch 19/1000, LR 0.000270
Train loss: 0.0880;  Loss pred: 0.0880; Loss self: 0.0000; time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6142 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6118 score: 0.5102 time: 0.07s
     INFO: Early stopping counter 17 of 20
Epoch 20/1000, LR 0.000270
Train loss: 0.0745;  Loss pred: 0.0745; Loss self: 0.0000; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6098 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6105 score: 0.5102 time: 0.07s
     INFO: Early stopping counter 18 of 20
Epoch 21/1000, LR 0.000270
Train loss: 0.0697;  Loss pred: 0.0697; Loss self: 0.0000; time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6066 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6097 score: 0.5102 time: 0.07s
     INFO: Early stopping counter 19 of 20
Epoch 22/1000, LR 0.000270
Train loss: 0.0593;  Loss pred: 0.0593; Loss self: 0.0000; time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6048 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6095 score: 0.5102 time: 0.07s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 001,   Train_Loss: 0.7379,   Val_Loss: 0.5852,   Val_Precision: 0.4898,   Val_Recall: 1.0000,   Val_accuracy: 0.6575,   Val_Score: 0.4898,   Val_Loss: 0.5852,   Test_Precision: 0.5102,   Test_Recall: 1.0000,   Test_accuracy: 0.6757,   Test_Score: 0.5102,   Test_loss: 0.5960


[0.08205157006159425, 0.07382914598565549, 0.0740917039802298, 0.07000304409302771, 0.06937436305452138, 0.0704731330042705, 0.07001781091094017, 0.07074014702811837, 0.07157888892106712, 0.07031704706605524, 0.06993929191958159, 0.07067474396899343, 0.07609713496640325, 0.07044546492397785, 0.07054818700999022, 0.07022654893808067, 0.07056829996872693, 0.06945485004689544, 0.07004992803558707, 0.06995956797618419, 0.06963931606151164, 0.07066256599500775]
[0.0016745218379917194, 0.0015067172650133772, 0.0015120755914332612, 0.001428633552918933, 0.0014158033276432936, 0.0014382272041687856, 0.0014289349165497994, 0.0014436764699615994, 0.0014607936514503493, 0.0014350417768582702, 0.0014273324881547264, 0.001442341713652927, 0.0015530027544163928, 0.0014376625494689358, 0.0014397589185712288, 0.0014331948762873606, 0.0014401693871168761, 0.0014174459193243968, 0.0014295903680732055, 0.0014277462852282487, 0.0014212105318675845, 0.0014420931835715867]
[597.1854038041785, 663.6945253236489, 661.3425979928181, 699.9695603934514, 706.312791102541, 695.3004345220571, 699.8219361974344, 692.6759705563388, 684.5593825022102, 696.8438244280909, 700.6076077570494, 693.3169792804256, 643.9138611674857, 695.5735199260732, 694.5607261751629, 697.7418190263586, 694.3627665922922, 705.4942882594308, 699.5010755058421, 700.4045539086334, 703.6255203413951, 693.436465404636]
Elapsed: 0.07139739790529182~0.0028350033723980104
Time per graph: 0.0014570897531692205~5.785721168159206e-05
Speed: 687.2838913712525~24.820711576265932
Total Time: 0.0710
best val loss: 0.5852058529853821 test_score: 0.5102

Testing...
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.5943 score: 0.5102 time: 0.07s
test Score 0.5102
Epoch Time List: [0.526287249987945, 0.38707485795021057, 0.38565481605473906, 0.37809580797329545, 0.3692293179919943, 0.3666901789838448, 0.3683510010596365, 0.36648806696757674, 0.37065973796416074, 0.3784928888780996, 0.36792008904740214, 0.36468323098961264, 0.3823972080135718, 0.3764823488891125, 0.38325554598122835, 0.38634953007567674, 0.36495811492204666, 0.3667290669400245, 0.3650508059654385, 0.3762325479183346, 0.36458613188005984, 0.3636513599194586]
Total Epoch List: [22]
Total Time List: [0.07097345602232963]
========================k_idx:1========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: GPSTransformer
GPS(
  (x_lin): Linear(in_features=14887, out_features=64, bias=True)
  (pe_lin): Linear(in_features=20, out_features=8, bias=True)
  (pe_norm): BatchNorm1d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (convs): ModuleList(
    (0-1): 2 x GPSConv(64, conv=GINEConv(nn=Sequential(
      (0): Linear(in_features=64, out_features=64, bias=True)
      (1): ReLU()
      (2): Linear(in_features=64, out_features=64, bias=True)
    )), heads=1, attn_type=multihead)
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=32, bias=True)
    (1): ReLU()
    (2): Linear(in_features=32, out_features=16, bias=True)
    (3): ReLU()
    (4): Linear(in_features=16, out_features=2, bias=True)
  )
)
Total number of parameters: 1039906
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x737bd998ee30>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.8703;  Loss pred: 0.8703; Loss self: 0.0000; time: 0.23s
Val loss: 0.6013 score: 0.5918 time: 0.07s
Test loss: 0.5375 score: 0.7143 time: 0.08s
Epoch 2/1000, LR 0.000000
Train loss: 0.8596;  Loss pred: 0.8596; Loss self: 0.0000; time: 0.22s
Val loss: 0.5999 score: 0.5918 time: 0.07s
Test loss: 0.5352 score: 0.7143 time: 0.08s
Epoch 3/1000, LR 0.000030
Train loss: 0.8113;  Loss pred: 0.8113; Loss self: 0.0000; time: 0.22s
Val loss: 0.6026 score: 0.6122 time: 0.06s
Test loss: 0.5355 score: 0.7551 time: 0.08s
     INFO: Early stopping counter 1 of 20
Epoch 4/1000, LR 0.000060
Train loss: 0.7004;  Loss pred: 0.7004; Loss self: 0.0000; time: 0.23s
Val loss: 0.6149 score: 0.6735 time: 0.06s
Test loss: 0.5436 score: 0.8163 time: 0.08s
     INFO: Early stopping counter 2 of 20
Epoch 5/1000, LR 0.000090
Train loss: 0.5875;  Loss pred: 0.5875; Loss self: 0.0000; time: 0.22s
Val loss: 0.6285 score: 0.6122 time: 0.06s
Test loss: 0.5596 score: 0.7755 time: 0.08s
     INFO: Early stopping counter 3 of 20
Epoch 6/1000, LR 0.000120
Train loss: 0.5117;  Loss pred: 0.5117; Loss self: 0.0000; time: 0.23s
Val loss: 0.6381 score: 0.6735 time: 0.07s
Test loss: 0.5714 score: 0.7755 time: 0.08s
     INFO: Early stopping counter 4 of 20
Epoch 7/1000, LR 0.000150
Train loss: 0.4305;  Loss pred: 0.4305; Loss self: 0.0000; time: 0.22s
Val loss: 0.6442 score: 0.6939 time: 0.06s
Test loss: 0.5777 score: 0.7959 time: 0.08s
     INFO: Early stopping counter 5 of 20
Epoch 8/1000, LR 0.000180
Train loss: 0.3522;  Loss pred: 0.3522; Loss self: 0.0000; time: 0.23s
Val loss: 0.6482 score: 0.6939 time: 0.06s
Test loss: 0.5810 score: 0.8163 time: 0.08s
     INFO: Early stopping counter 6 of 20
Epoch 9/1000, LR 0.000210
Train loss: 0.2975;  Loss pred: 0.2975; Loss self: 0.0000; time: 0.22s
Val loss: 0.6480 score: 0.6735 time: 0.07s
Test loss: 0.5843 score: 0.7959 time: 0.08s
     INFO: Early stopping counter 7 of 20
Epoch 10/1000, LR 0.000240
Train loss: 0.2492;  Loss pred: 0.2492; Loss self: 0.0000; time: 0.23s
Val loss: 0.6477 score: 0.6735 time: 0.07s
Test loss: 0.5858 score: 0.7755 time: 0.09s
     INFO: Early stopping counter 8 of 20
Epoch 11/1000, LR 0.000270
Train loss: 0.2098;  Loss pred: 0.2098; Loss self: 0.0000; time: 0.22s
Val loss: 0.6486 score: 0.6735 time: 0.07s
Test loss: 0.5911 score: 0.8163 time: 0.08s
     INFO: Early stopping counter 9 of 20
Epoch 12/1000, LR 0.000270
Train loss: 0.1746;  Loss pred: 0.1746; Loss self: 0.0000; time: 0.24s
Val loss: 0.6547 score: 0.6735 time: 0.07s
Test loss: 0.5983 score: 0.8163 time: 0.09s
     INFO: Early stopping counter 10 of 20
Epoch 13/1000, LR 0.000270
Train loss: 0.1451;  Loss pred: 0.1451; Loss self: 0.0000; time: 0.24s
Val loss: 0.6638 score: 0.6531 time: 0.07s
Test loss: 0.6060 score: 0.8163 time: 0.09s
     INFO: Early stopping counter 11 of 20
Epoch 14/1000, LR 0.000270
Train loss: 0.1227;  Loss pred: 0.1227; Loss self: 0.0000; time: 0.25s
Val loss: 0.6784 score: 0.6531 time: 0.07s
Test loss: 0.6173 score: 0.7959 time: 0.09s
     INFO: Early stopping counter 12 of 20
Epoch 15/1000, LR 0.000270
Train loss: 0.1041;  Loss pred: 0.1041; Loss self: 0.0000; time: 0.24s
Val loss: 0.6980 score: 0.6122 time: 0.07s
Test loss: 0.6296 score: 0.7755 time: 0.08s
     INFO: Early stopping counter 13 of 20
Epoch 16/1000, LR 0.000270
Train loss: 0.0872;  Loss pred: 0.0872; Loss self: 0.0000; time: 0.24s
Val loss: 0.7149 score: 0.6122 time: 0.07s
Test loss: 0.6464 score: 0.7347 time: 0.09s
     INFO: Early stopping counter 14 of 20
Epoch 17/1000, LR 0.000270
Train loss: 0.0738;  Loss pred: 0.0738; Loss self: 0.0000; time: 0.25s
Val loss: 0.7342 score: 0.6122 time: 0.07s
Test loss: 0.6617 score: 0.7347 time: 0.09s
     INFO: Early stopping counter 15 of 20
Epoch 18/1000, LR 0.000270
Train loss: 0.0618;  Loss pred: 0.0618; Loss self: 0.0000; time: 0.24s
Val loss: 0.7530 score: 0.5918 time: 0.07s
Test loss: 0.6754 score: 0.7347 time: 0.09s
     INFO: Early stopping counter 16 of 20
Epoch 19/1000, LR 0.000270
Train loss: 0.0522;  Loss pred: 0.0522; Loss self: 0.0000; time: 0.24s
Val loss: 0.7756 score: 0.5714 time: 0.07s
Test loss: 0.6882 score: 0.7551 time: 0.09s
     INFO: Early stopping counter 17 of 20
Epoch 20/1000, LR 0.000270
Train loss: 0.0473;  Loss pred: 0.0473; Loss self: 0.0000; time: 0.24s
Val loss: 0.7973 score: 0.5714 time: 0.07s
Test loss: 0.7052 score: 0.7551 time: 0.09s
     INFO: Early stopping counter 18 of 20
Epoch 21/1000, LR 0.000270
Train loss: 0.0415;  Loss pred: 0.0415; Loss self: 0.0000; time: 0.24s
Val loss: 0.8219 score: 0.5510 time: 0.07s
Test loss: 0.7209 score: 0.7347 time: 0.08s
     INFO: Early stopping counter 19 of 20
Epoch 22/1000, LR 0.000270
Train loss: 0.0374;  Loss pred: 0.0374; Loss self: 0.0000; time: 0.23s
Val loss: 0.8445 score: 0.5510 time: 0.07s
Test loss: 0.7361 score: 0.7143 time: 0.09s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 001,   Train_Loss: 0.8596,   Val_Loss: 0.5999,   Val_Precision: 0.5714,   Val_Recall: 0.8000,   Val_accuracy: 0.6667,   Val_Score: 0.5918,   Val_Loss: 0.5999,   Test_Precision: 0.6562,   Test_Recall: 0.8750,   Test_accuracy: 0.7500,   Test_Score: 0.7143,   Test_loss: 0.5352


[0.08205157006159425, 0.07382914598565549, 0.0740917039802298, 0.07000304409302771, 0.06937436305452138, 0.0704731330042705, 0.07001781091094017, 0.07074014702811837, 0.07157888892106712, 0.07031704706605524, 0.06993929191958159, 0.07067474396899343, 0.07609713496640325, 0.07044546492397785, 0.07054818700999022, 0.07022654893808067, 0.07056829996872693, 0.06945485004689544, 0.07004992803558707, 0.06995956797618419, 0.06963931606151164, 0.07066256599500775, 0.08648797695059329, 0.08559156698174775, 0.08449731091968715, 0.08436995092779398, 0.085658062947914, 0.0852491419063881, 0.08486074896063656, 0.08520636602770537, 0.08609478594735265, 0.08963866392150521, 0.08550910197664052, 0.09036901604849845, 0.09033857402391732, 0.09120019897818565, 0.08565963001456112, 0.09029262606054544, 0.0903124799951911, 0.0901899979216978, 0.0943373489426449, 0.0904439379228279, 0.08904467592947185, 0.09067532501649112]
[0.0016745218379917194, 0.0015067172650133772, 0.0015120755914332612, 0.001428633552918933, 0.0014158033276432936, 0.0014382272041687856, 0.0014289349165497994, 0.0014436764699615994, 0.0014607936514503493, 0.0014350417768582702, 0.0014273324881547264, 0.001442341713652927, 0.0015530027544163928, 0.0014376625494689358, 0.0014397589185712288, 0.0014331948762873606, 0.0014401693871168761, 0.0014174459193243968, 0.0014295903680732055, 0.0014277462852282487, 0.0014212105318675845, 0.0014420931835715867, 0.0017650607540937407, 0.0017467666730968927, 0.0017244349167283093, 0.0017218357332202854, 0.001748123733630898, 0.0017397784062528185, 0.0017318520196048276, 0.0017389054291368443, 0.0017570364479051561, 0.0018293604881939839, 0.0017450837138089904, 0.0018442656336428256, 0.0018436443678350473, 0.0018612285505752175, 0.00174815571458288, 0.0018427066542968458, 0.001843111836636553, 0.0018406122024836285, 0.0019252520192376509, 0.0018457946514862838, 0.0018172382842749357, 0.0018505168370712474]
[597.1854038041785, 663.6945253236489, 661.3425979928181, 699.9695603934514, 706.312791102541, 695.3004345220571, 699.8219361974344, 692.6759705563388, 684.5593825022102, 696.8438244280909, 700.6076077570494, 693.3169792804256, 643.9138611674857, 695.5735199260732, 694.5607261751629, 697.7418190263586, 694.3627665922922, 705.4942882594308, 699.5010755058421, 700.4045539086334, 703.6255203413951, 693.436465404636, 566.5527363183845, 572.4863059283535, 579.9001112186094, 580.7754948433653, 572.0418874028867, 574.7858442235911, 577.4165394501656, 575.0744021176464, 569.1401571050275, 546.639115938947, 573.0384119036343, 542.22124067062, 542.4039567751772, 537.2795295295398, 572.0314224059873, 542.6799744105704, 542.5606738139527, 543.2974956107815, 519.4125184691268, 541.772075888709, 550.2855672001167, 540.3895711549793]
Elapsed: 0.07969932368723676~0.008768140736304489
Time per graph: 0.001626516809943607~0.00017894164767968347
Speed: 622.3279691488119~68.42888840604128
Total Time: 0.0911
best val loss: 0.5998765826225281 test_score: 0.7143

Testing...
Test loss: 0.5777 score: 0.7959 time: 0.08s
test Score 0.7959
Epoch Time List: [0.526287249987945, 0.38707485795021057, 0.38565481605473906, 0.37809580797329545, 0.3692293179919943, 0.3666901789838448, 0.3683510010596365, 0.36648806696757674, 0.37065973796416074, 0.3784928888780996, 0.36792008904740214, 0.36468323098961264, 0.3823972080135718, 0.3764823488891125, 0.38325554598122835, 0.38634953007567674, 0.36495811492204666, 0.3667290669400245, 0.3650508059654385, 0.3762325479183346, 0.36458613188005984, 0.3636513599194586, 0.3744076330913231, 0.37252875696867704, 0.3664276209892705, 0.37464779103174806, 0.3682017490500584, 0.37818767700809985, 0.3676254788879305, 0.37067748489789665, 0.3725240718340501, 0.39161301392596215, 0.37195807800162584, 0.39667305699549615, 0.39499386586248875, 0.40662991208955646, 0.38884232204873115, 0.39207145501859486, 0.4011939688352868, 0.3956933099543676, 0.4069837089627981, 0.39531465398613364, 0.38934743392746896, 0.388437645859085]
Total Epoch List: [22, 22]
Total Time List: [0.07097345602232963, 0.0910892259562388]
========================k_idx:2========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: GPSTransformer
GPS(
  (x_lin): Linear(in_features=14887, out_features=64, bias=True)
  (pe_lin): Linear(in_features=20, out_features=8, bias=True)
  (pe_norm): BatchNorm1d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (convs): ModuleList(
    (0-1): 2 x GPSConv(64, conv=GINEConv(nn=Sequential(
      (0): Linear(in_features=64, out_features=64, bias=True)
      (1): ReLU()
      (2): Linear(in_features=64, out_features=64, bias=True)
    )), heads=1, attn_type=multihead)
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=32, bias=True)
    (1): ReLU()
    (2): Linear(in_features=32, out_features=16, bias=True)
    (3): ReLU()
    (4): Linear(in_features=16, out_features=2, bias=True)
  )
)
Total number of parameters: 1039906
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x737bd9adabf0>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.5835;  Loss pred: 0.5835; Loss self: 0.0000; time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.8710 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.9276 score: 0.5000 time: 0.07s
Epoch 2/1000, LR 0.000000
Train loss: 0.5799;  Loss pred: 0.5799; Loss self: 0.0000; time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6962 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7075 score: 0.5000 time: 0.07s
Epoch 3/1000, LR 0.000030
Train loss: 0.5401;  Loss pred: 0.5401; Loss self: 0.0000; time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6301 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6226 score: 0.5000 time: 0.07s
Epoch 4/1000, LR 0.000060
Train loss: 0.4668;  Loss pred: 0.4668; Loss self: 0.0000; time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6272 score: 0.5102 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6223 score: 0.5000 time: 0.08s
Epoch 5/1000, LR 0.000090
Train loss: 0.3961;  Loss pred: 0.3961; Loss self: 0.0000; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6401 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6379 score: 0.5000 time: 0.07s
     INFO: Early stopping counter 1 of 20
Epoch 6/1000, LR 0.000120
Train loss: 0.3494;  Loss pred: 0.3494; Loss self: 0.0000; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6481 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6365 score: 0.5000 time: 0.07s
     INFO: Early stopping counter 2 of 20
Epoch 7/1000, LR 0.000150
Train loss: 0.3050;  Loss pred: 0.3050; Loss self: 0.0000; time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6470 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6394 score: 0.5000 time: 0.08s
     INFO: Early stopping counter 3 of 20
Epoch 8/1000, LR 0.000180
Train loss: 0.2748;  Loss pred: 0.2748; Loss self: 0.0000; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6400 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6391 score: 0.5000 time: 0.08s
     INFO: Early stopping counter 4 of 20
Epoch 9/1000, LR 0.000210
Train loss: 0.2439;  Loss pred: 0.2439; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6420 score: 0.5102 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6312 score: 0.5000 time: 0.08s
     INFO: Early stopping counter 5 of 20
Epoch 10/1000, LR 0.000240
Train loss: 0.2196;  Loss pred: 0.2196; Loss self: 0.0000; time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6381 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6264 score: 0.5000 time: 0.07s
     INFO: Early stopping counter 6 of 20
Epoch 11/1000, LR 0.000270
Train loss: 0.1837;  Loss pred: 0.1837; Loss self: 0.0000; time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6351 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6230 score: 0.5000 time: 0.07s
     INFO: Early stopping counter 7 of 20
Epoch 12/1000, LR 0.000270
Train loss: 0.1483;  Loss pred: 0.1483; Loss self: 0.0000; time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6340 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6225 score: 0.5000 time: 0.07s
     INFO: Early stopping counter 8 of 20
Epoch 13/1000, LR 0.000270
Train loss: 0.1228;  Loss pred: 0.1228; Loss self: 0.0000; time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6313 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6195 score: 0.5000 time: 0.07s
     INFO: Early stopping counter 9 of 20
Epoch 14/1000, LR 0.000270
Train loss: 0.1065;  Loss pred: 0.1065; Loss self: 0.0000; time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6288 score: 0.5102 time: 0.09s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6144 score: 0.5000 time: 0.09s
     INFO: Early stopping counter 10 of 20
Epoch 15/1000, LR 0.000270
Train loss: 0.0925;  Loss pred: 0.0925; Loss self: 0.0000; time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6248 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6098 score: 0.5000 time: 0.07s
Epoch 16/1000, LR 0.000270
Train loss: 0.0815;  Loss pred: 0.0815; Loss self: 0.0000; time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6198 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6059 score: 0.5000 time: 0.07s
Epoch 17/1000, LR 0.000270
Train loss: 0.0726;  Loss pred: 0.0726; Loss self: 0.0000; time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6152 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6011 score: 0.5000 time: 0.07s
Epoch 18/1000, LR 0.000270
Train loss: 0.0623;  Loss pred: 0.0623; Loss self: 0.0000; time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6114 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.5966 score: 0.5000 time: 0.07s
Epoch 19/1000, LR 0.000270
Train loss: 0.0551;  Loss pred: 0.0551; Loss self: 0.0000; time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6076 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.5923 score: 0.5000 time: 0.07s
Epoch 20/1000, LR 0.000270
Train loss: 0.0522;  Loss pred: 0.0522; Loss self: 0.0000; time: 0.23s
Val loss: 0.6044 score: 0.5306 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.5877 score: 0.5000 time: 0.07s
Epoch 21/1000, LR 0.000270
Train loss: 0.0476;  Loss pred: 0.0476; Loss self: 0.0000; time: 0.23s
Val loss: 0.6021 score: 0.5306 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.5842 score: 0.5000 time: 0.07s
Epoch 22/1000, LR 0.000270
Train loss: 0.0413;  Loss pred: 0.0413; Loss self: 0.0000; time: 0.23s
Val loss: 0.6006 score: 0.5510 time: 0.07s
Test loss: 0.5821 score: 0.5625 time: 0.07s
Epoch 23/1000, LR 0.000270
Train loss: 0.0385;  Loss pred: 0.0385; Loss self: 0.0000; time: 0.23s
Val loss: 0.6002 score: 0.6327 time: 0.07s
Test loss: 0.5808 score: 0.6042 time: 0.07s
Epoch 24/1000, LR 0.000270
Train loss: 0.0355;  Loss pred: 0.0355; Loss self: 0.0000; time: 0.23s
Val loss: 0.6003 score: 0.6531 time: 0.07s
Test loss: 0.5801 score: 0.5833 time: 0.07s
     INFO: Early stopping counter 1 of 20
Epoch 25/1000, LR 0.000270
Train loss: 0.0313;  Loss pred: 0.0313; Loss self: 0.0000; time: 0.23s
Val loss: 0.6002 score: 0.6327 time: 0.07s
Test loss: 0.5791 score: 0.6250 time: 0.07s
Epoch 26/1000, LR 0.000270
Train loss: 0.0296;  Loss pred: 0.0296; Loss self: 0.0000; time: 0.23s
Val loss: 0.5990 score: 0.5714 time: 0.07s
Test loss: 0.5779 score: 0.6875 time: 0.07s
Epoch 27/1000, LR 0.000270
Train loss: 0.0250;  Loss pred: 0.0250; Loss self: 0.0000; time: 0.23s
Val loss: 0.5982 score: 0.6122 time: 0.07s
Test loss: 0.5770 score: 0.6875 time: 0.07s
Epoch 28/1000, LR 0.000270
Train loss: 0.0221;  Loss pred: 0.0221; Loss self: 0.0000; time: 0.23s
Val loss: 0.5981 score: 0.5714 time: 0.07s
Test loss: 0.5753 score: 0.6875 time: 0.07s
Epoch 29/1000, LR 0.000270
Train loss: 0.0211;  Loss pred: 0.0211; Loss self: 0.0000; time: 0.23s
Val loss: 0.5982 score: 0.6735 time: 0.07s
Test loss: 0.5735 score: 0.7083 time: 0.07s
     INFO: Early stopping counter 1 of 20
Epoch 30/1000, LR 0.000270
Train loss: 0.0186;  Loss pred: 0.0186; Loss self: 0.0000; time: 0.26s
Val loss: 0.5976 score: 0.6531 time: 0.07s
Test loss: 0.5718 score: 0.7292 time: 0.07s
Epoch 31/1000, LR 0.000270
Train loss: 0.0150;  Loss pred: 0.0150; Loss self: 0.0000; time: 0.23s
Val loss: 0.5963 score: 0.6735 time: 0.07s
Test loss: 0.5689 score: 0.7500 time: 0.07s
Epoch 32/1000, LR 0.000270
Train loss: 0.0146;  Loss pred: 0.0146; Loss self: 0.0000; time: 0.25s
Val loss: 0.5954 score: 0.6735 time: 0.07s
Test loss: 0.5648 score: 0.7292 time: 0.07s
Epoch 33/1000, LR 0.000270
Train loss: 0.0156;  Loss pred: 0.0156; Loss self: 0.0000; time: 0.22s
Val loss: 0.5939 score: 0.6939 time: 0.07s
Test loss: 0.5602 score: 0.7083 time: 0.08s
Epoch 34/1000, LR 0.000270
Train loss: 0.0119;  Loss pred: 0.0119; Loss self: 0.0000; time: 0.25s
Val loss: 0.5910 score: 0.7347 time: 0.07s
Test loss: 0.5574 score: 0.7083 time: 0.07s
Epoch 35/1000, LR 0.000270
Train loss: 0.0114;  Loss pred: 0.0114; Loss self: 0.0000; time: 0.23s
Val loss: 0.5882 score: 0.7551 time: 0.07s
Test loss: 0.5554 score: 0.7083 time: 0.07s
Epoch 36/1000, LR 0.000270
Train loss: 0.0099;  Loss pred: 0.0099; Loss self: 0.0000; time: 0.23s
Val loss: 0.5850 score: 0.7551 time: 0.07s
Test loss: 0.5528 score: 0.7083 time: 0.07s
Epoch 37/1000, LR 0.000270
Train loss: 0.0100;  Loss pred: 0.0100; Loss self: 0.0000; time: 0.23s
Val loss: 0.5813 score: 0.7347 time: 0.07s
Test loss: 0.5504 score: 0.7292 time: 0.08s
Epoch 38/1000, LR 0.000270
Train loss: 0.0093;  Loss pred: 0.0093; Loss self: 0.0000; time: 0.24s
Val loss: 0.5778 score: 0.7347 time: 0.08s
Test loss: 0.5482 score: 0.7292 time: 0.08s
Epoch 39/1000, LR 0.000269
Train loss: 0.0081;  Loss pred: 0.0081; Loss self: 0.0000; time: 0.23s
Val loss: 0.5753 score: 0.7143 time: 0.07s
Test loss: 0.5463 score: 0.7500 time: 0.07s
Epoch 40/1000, LR 0.000269
Train loss: 0.0071;  Loss pred: 0.0071; Loss self: 0.0000; time: 0.22s
Val loss: 0.5732 score: 0.7143 time: 0.07s
Test loss: 0.5440 score: 0.7292 time: 0.07s
Epoch 41/1000, LR 0.000269
Train loss: 0.0062;  Loss pred: 0.0062; Loss self: 0.0000; time: 0.23s
Val loss: 0.5711 score: 0.7143 time: 0.07s
Test loss: 0.5406 score: 0.7500 time: 0.07s
Epoch 42/1000, LR 0.000269
Train loss: 0.0058;  Loss pred: 0.0058; Loss self: 0.0000; time: 0.23s
Val loss: 0.5682 score: 0.7143 time: 0.07s
Test loss: 0.5368 score: 0.7500 time: 0.07s
Epoch 43/1000, LR 0.000269
Train loss: 0.0046;  Loss pred: 0.0046; Loss self: 0.0000; time: 0.23s
Val loss: 0.5650 score: 0.7347 time: 0.07s
Test loss: 0.5324 score: 0.7500 time: 0.07s
Epoch 44/1000, LR 0.000269
Train loss: 0.0049;  Loss pred: 0.0049; Loss self: 0.0000; time: 0.23s
Val loss: 0.5621 score: 0.7551 time: 0.07s
Test loss: 0.5287 score: 0.7500 time: 0.07s
Epoch 45/1000, LR 0.000269
Train loss: 0.0045;  Loss pred: 0.0045; Loss self: 0.0000; time: 0.23s
Val loss: 0.5590 score: 0.7347 time: 0.07s
Test loss: 0.5259 score: 0.7500 time: 0.07s
Epoch 46/1000, LR 0.000269
Train loss: 0.0033;  Loss pred: 0.0033; Loss self: 0.0000; time: 0.23s
Val loss: 0.5562 score: 0.7347 time: 0.07s
Test loss: 0.5216 score: 0.7500 time: 0.07s
Epoch 47/1000, LR 0.000269
Train loss: 0.0033;  Loss pred: 0.0033; Loss self: 0.0000; time: 0.23s
Val loss: 0.5536 score: 0.7347 time: 0.07s
Test loss: 0.5165 score: 0.7500 time: 0.07s
Epoch 48/1000, LR 0.000269
Train loss: 0.0032;  Loss pred: 0.0032; Loss self: 0.0000; time: 0.23s
Val loss: 0.5501 score: 0.7551 time: 0.07s
Test loss: 0.5110 score: 0.7500 time: 0.07s
Epoch 49/1000, LR 0.000269
Train loss: 0.0032;  Loss pred: 0.0032; Loss self: 0.0000; time: 0.23s
Val loss: 0.5468 score: 0.7347 time: 0.07s
Test loss: 0.5054 score: 0.7500 time: 0.07s
Epoch 50/1000, LR 0.000269
Train loss: 0.0035;  Loss pred: 0.0035; Loss self: 0.0000; time: 0.23s
Val loss: 0.5441 score: 0.7347 time: 0.07s
Test loss: 0.5002 score: 0.7500 time: 0.07s
Epoch 51/1000, LR 0.000269
Train loss: 0.0025;  Loss pred: 0.0025; Loss self: 0.0000; time: 0.23s
Val loss: 0.5411 score: 0.7347 time: 0.07s
Test loss: 0.4944 score: 0.7500 time: 0.07s
Epoch 52/1000, LR 0.000269
Train loss: 0.0026;  Loss pred: 0.0026; Loss self: 0.0000; time: 0.23s
Val loss: 0.5377 score: 0.7347 time: 0.07s
Test loss: 0.4858 score: 0.7500 time: 0.07s
Epoch 53/1000, LR 0.000269
Train loss: 0.0023;  Loss pred: 0.0023; Loss self: 0.0000; time: 0.23s
Val loss: 0.5333 score: 0.7347 time: 0.07s
Test loss: 0.4751 score: 0.7708 time: 0.07s
Epoch 54/1000, LR 0.000269
Train loss: 0.0023;  Loss pred: 0.0023; Loss self: 0.0000; time: 0.23s
Val loss: 0.5295 score: 0.7347 time: 0.07s
Test loss: 0.4650 score: 0.7708 time: 0.07s
Epoch 55/1000, LR 0.000269
Train loss: 0.0021;  Loss pred: 0.0021; Loss self: 0.0000; time: 0.23s
Val loss: 0.5262 score: 0.7347 time: 0.07s
Test loss: 0.4556 score: 0.7708 time: 0.07s
Epoch 56/1000, LR 0.000269
Train loss: 0.0025;  Loss pred: 0.0025; Loss self: 0.0000; time: 0.23s
Val loss: 0.5246 score: 0.7347 time: 0.07s
Test loss: 0.4485 score: 0.7708 time: 0.07s
Epoch 57/1000, LR 0.000269
Train loss: 0.0016;  Loss pred: 0.0016; Loss self: 0.0000; time: 0.23s
Val loss: 0.5229 score: 0.7347 time: 0.07s
Test loss: 0.4410 score: 0.7917 time: 0.07s
Epoch 58/1000, LR 0.000269
Train loss: 0.0015;  Loss pred: 0.0015; Loss self: 0.0000; time: 0.23s
Val loss: 0.5214 score: 0.7551 time: 0.07s
Test loss: 0.4339 score: 0.8125 time: 0.07s
Epoch 59/1000, LR 0.000268
Train loss: 0.0021;  Loss pred: 0.0021; Loss self: 0.0000; time: 0.23s
Val loss: 0.5217 score: 0.7551 time: 0.07s
Test loss: 0.4285 score: 0.7917 time: 0.07s
     INFO: Early stopping counter 1 of 20
Epoch 60/1000, LR 0.000268
Train loss: 0.0018;  Loss pred: 0.0018; Loss self: 0.0000; time: 0.23s
Val loss: 0.5230 score: 0.7347 time: 0.07s
Test loss: 0.4237 score: 0.7917 time: 0.07s
     INFO: Early stopping counter 2 of 20
Epoch 61/1000, LR 0.000268
Train loss: 0.0016;  Loss pred: 0.0016; Loss self: 0.0000; time: 0.23s
Val loss: 0.5239 score: 0.7551 time: 0.07s
Test loss: 0.4180 score: 0.7917 time: 0.07s
     INFO: Early stopping counter 3 of 20
Epoch 62/1000, LR 0.000268
Train loss: 0.0014;  Loss pred: 0.0014; Loss self: 0.0000; time: 0.23s
Val loss: 0.5239 score: 0.7551 time: 0.07s
Test loss: 0.4123 score: 0.7917 time: 0.08s
     INFO: Early stopping counter 4 of 20
Epoch 63/1000, LR 0.000268
Train loss: 0.0012;  Loss pred: 0.0012; Loss self: 0.0000; time: 0.23s
Val loss: 0.5218 score: 0.7551 time: 0.07s
Test loss: 0.4040 score: 0.7917 time: 0.07s
     INFO: Early stopping counter 5 of 20
Epoch 64/1000, LR 0.000268
Train loss: 0.0012;  Loss pred: 0.0012; Loss self: 0.0000; time: 0.23s
Val loss: 0.5177 score: 0.7551 time: 0.07s
Test loss: 0.3941 score: 0.8125 time: 0.07s
Epoch 65/1000, LR 0.000268
Train loss: 0.0011;  Loss pred: 0.0011; Loss self: 0.0000; time: 0.23s
Val loss: 0.5126 score: 0.7551 time: 0.07s
Test loss: 0.3849 score: 0.8333 time: 0.07s
Epoch 66/1000, LR 0.000268
Train loss: 0.0014;  Loss pred: 0.0014; Loss self: 0.0000; time: 0.23s
Val loss: 0.5071 score: 0.7755 time: 0.07s
Test loss: 0.3763 score: 0.8333 time: 0.07s
Epoch 67/1000, LR 0.000268
Train loss: 0.0011;  Loss pred: 0.0011; Loss self: 0.0000; time: 0.23s
Val loss: 0.5017 score: 0.7755 time: 0.07s
Test loss: 0.3678 score: 0.8333 time: 0.07s
Epoch 68/1000, LR 0.000268
Train loss: 0.0010;  Loss pred: 0.0010; Loss self: 0.0000; time: 0.23s
Val loss: 0.4961 score: 0.7959 time: 0.07s
Test loss: 0.3588 score: 0.8333 time: 0.07s
Epoch 69/1000, LR 0.000268
Train loss: 0.0013;  Loss pred: 0.0013; Loss self: 0.0000; time: 0.23s
Val loss: 0.4900 score: 0.7959 time: 0.07s
Test loss: 0.3484 score: 0.8333 time: 0.07s
Epoch 70/1000, LR 0.000268
Train loss: 0.0014;  Loss pred: 0.0014; Loss self: 0.0000; time: 0.23s
Val loss: 0.4852 score: 0.7959 time: 0.07s
Test loss: 0.3391 score: 0.8333 time: 0.07s
Epoch 71/1000, LR 0.000268
Train loss: 0.0010;  Loss pred: 0.0010; Loss self: 0.0000; time: 0.23s
Val loss: 0.4807 score: 0.7959 time: 0.07s
Test loss: 0.3302 score: 0.8542 time: 0.07s
Epoch 72/1000, LR 0.000267
Train loss: 0.0011;  Loss pred: 0.0011; Loss self: 0.0000; time: 0.23s
Val loss: 0.4772 score: 0.7959 time: 0.07s
Test loss: 0.3222 score: 0.8542 time: 0.07s
Epoch 73/1000, LR 0.000267
Train loss: 0.0009;  Loss pred: 0.0009; Loss self: 0.0000; time: 0.23s
Val loss: 0.4741 score: 0.7959 time: 0.07s
Test loss: 0.3144 score: 0.8542 time: 0.07s
Epoch 74/1000, LR 0.000267
Train loss: 0.0012;  Loss pred: 0.0012; Loss self: 0.0000; time: 0.23s
Val loss: 0.4712 score: 0.7959 time: 0.07s
Test loss: 0.3069 score: 0.8542 time: 0.07s
Epoch 75/1000, LR 0.000267
Train loss: 0.0010;  Loss pred: 0.0010; Loss self: 0.0000; time: 0.23s
Val loss: 0.4698 score: 0.7959 time: 0.07s
Test loss: 0.3012 score: 0.8542 time: 0.07s
Epoch 76/1000, LR 0.000267
Train loss: 0.0009;  Loss pred: 0.0009; Loss self: 0.0000; time: 0.23s
Val loss: 0.4690 score: 0.7959 time: 0.07s
Test loss: 0.2960 score: 0.8542 time: 0.07s
Epoch 77/1000, LR 0.000267
Train loss: 0.0009;  Loss pred: 0.0009; Loss self: 0.0000; time: 0.23s
Val loss: 0.4695 score: 0.7959 time: 0.07s
Test loss: 0.2923 score: 0.8542 time: 0.07s
     INFO: Early stopping counter 1 of 20
Epoch 78/1000, LR 0.000267
Train loss: 0.0010;  Loss pred: 0.0010; Loss self: 0.0000; time: 0.23s
Val loss: 0.4715 score: 0.7959 time: 0.07s
Test loss: 0.2893 score: 0.8542 time: 0.07s
     INFO: Early stopping counter 2 of 20
Epoch 79/1000, LR 0.000267
Train loss: 0.0011;  Loss pred: 0.0011; Loss self: 0.0000; time: 0.23s
Val loss: 0.4739 score: 0.8163 time: 0.07s
Test loss: 0.2863 score: 0.8542 time: 0.07s
     INFO: Early stopping counter 3 of 20
Epoch 80/1000, LR 0.000267
Train loss: 0.0011;  Loss pred: 0.0011; Loss self: 0.0000; time: 0.23s
Val loss: 0.4787 score: 0.8367 time: 0.07s
Test loss: 0.2848 score: 0.8542 time: 0.07s
     INFO: Early stopping counter 4 of 20
Epoch 81/1000, LR 0.000267
Train loss: 0.0009;  Loss pred: 0.0009; Loss self: 0.0000; time: 0.23s
Val loss: 0.4842 score: 0.8367 time: 0.07s
Test loss: 0.2837 score: 0.8542 time: 0.07s
     INFO: Early stopping counter 5 of 20
Epoch 82/1000, LR 0.000267
Train loss: 0.0009;  Loss pred: 0.0009; Loss self: 0.0000; time: 0.23s
Val loss: 0.4908 score: 0.8367 time: 0.07s
Test loss: 0.2839 score: 0.8542 time: 0.07s
     INFO: Early stopping counter 6 of 20
Epoch 83/1000, LR 0.000266
Train loss: 0.0010;  Loss pred: 0.0010; Loss self: 0.0000; time: 0.23s
Val loss: 0.4996 score: 0.8367 time: 0.07s
Test loss: 0.2854 score: 0.8542 time: 0.07s
     INFO: Early stopping counter 7 of 20
Epoch 84/1000, LR 0.000266
Train loss: 0.0007;  Loss pred: 0.0007; Loss self: 0.0000; time: 0.23s
Val loss: 0.5099 score: 0.8367 time: 0.07s
Test loss: 0.2879 score: 0.8542 time: 0.07s
     INFO: Early stopping counter 8 of 20
Epoch 85/1000, LR 0.000266
Train loss: 0.0008;  Loss pred: 0.0008; Loss self: 0.0000; time: 0.23s
Val loss: 0.5209 score: 0.8367 time: 0.07s
Test loss: 0.2913 score: 0.8542 time: 0.07s
     INFO: Early stopping counter 9 of 20
Epoch 86/1000, LR 0.000266
Train loss: 0.0007;  Loss pred: 0.0007; Loss self: 0.0000; time: 0.23s
Val loss: 0.5327 score: 0.8367 time: 0.07s
Test loss: 0.2956 score: 0.8333 time: 0.07s
     INFO: Early stopping counter 10 of 20
Epoch 87/1000, LR 0.000266
Train loss: 0.0009;  Loss pred: 0.0009; Loss self: 0.0000; time: 0.28s
Val loss: 0.5447 score: 0.8367 time: 0.07s
Test loss: 0.2996 score: 0.8333 time: 0.07s
     INFO: Early stopping counter 11 of 20
Epoch 88/1000, LR 0.000266
Train loss: 0.0010;  Loss pred: 0.0010; Loss self: 0.0000; time: 0.23s
Val loss: 0.5574 score: 0.8367 time: 0.07s
Test loss: 0.3048 score: 0.8333 time: 0.07s
     INFO: Early stopping counter 12 of 20
Epoch 89/1000, LR 0.000266
Train loss: 0.0007;  Loss pred: 0.0007; Loss self: 0.0000; time: 0.23s
Val loss: 0.5697 score: 0.8367 time: 0.07s
Test loss: 0.3093 score: 0.8333 time: 0.08s
     INFO: Early stopping counter 13 of 20
Epoch 90/1000, LR 0.000266
Train loss: 0.0007;  Loss pred: 0.0007; Loss self: 0.0000; time: 0.23s
Val loss: 0.5828 score: 0.8367 time: 0.07s
Test loss: 0.3145 score: 0.8333 time: 0.07s
     INFO: Early stopping counter 14 of 20
Epoch 91/1000, LR 0.000266
Train loss: 0.0007;  Loss pred: 0.0007; Loss self: 0.0000; time: 0.23s
Val loss: 0.5957 score: 0.8163 time: 0.07s
Test loss: 0.3197 score: 0.8333 time: 0.07s
     INFO: Early stopping counter 15 of 20
Epoch 92/1000, LR 0.000266
Train loss: 0.0006;  Loss pred: 0.0006; Loss self: 0.0000; time: 0.23s
Val loss: 0.6079 score: 0.8163 time: 0.07s
Test loss: 0.3240 score: 0.8333 time: 0.07s
     INFO: Early stopping counter 16 of 20
Epoch 93/1000, LR 0.000265
Train loss: 0.0009;  Loss pred: 0.0009; Loss self: 0.0000; time: 0.23s
Val loss: 0.6208 score: 0.8163 time: 0.07s
Test loss: 0.3299 score: 0.8333 time: 0.07s
     INFO: Early stopping counter 17 of 20
Epoch 94/1000, LR 0.000265
Train loss: 0.0007;  Loss pred: 0.0007; Loss self: 0.0000; time: 0.23s
Val loss: 0.6345 score: 0.8163 time: 0.07s
Test loss: 0.3370 score: 0.8125 time: 0.07s
     INFO: Early stopping counter 18 of 20
Epoch 95/1000, LR 0.000265
Train loss: 0.0007;  Loss pred: 0.0007; Loss self: 0.0000; time: 0.23s
Val loss: 0.6479 score: 0.8163 time: 0.07s
Test loss: 0.3443 score: 0.8125 time: 0.07s
     INFO: Early stopping counter 19 of 20
Epoch 96/1000, LR 0.000265
Train loss: 0.0005;  Loss pred: 0.0005; Loss self: 0.0000; time: 0.23s
Val loss: 0.6603 score: 0.8163 time: 0.07s
Test loss: 0.3500 score: 0.8125 time: 0.07s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 075,   Train_Loss: 0.0009,   Val_Loss: 0.4690,   Val_Precision: 1.0000,   Val_Recall: 0.6000,   Val_accuracy: 0.7500,   Val_Score: 0.7959,   Val_Loss: 0.4690,   Test_Precision: 1.0000,   Test_Recall: 0.7083,   Test_accuracy: 0.8293,   Test_Score: 0.8542,   Test_loss: 0.2960


[0.08205157006159425, 0.07382914598565549, 0.0740917039802298, 0.07000304409302771, 0.06937436305452138, 0.0704731330042705, 0.07001781091094017, 0.07074014702811837, 0.07157888892106712, 0.07031704706605524, 0.06993929191958159, 0.07067474396899343, 0.07609713496640325, 0.07044546492397785, 0.07054818700999022, 0.07022654893808067, 0.07056829996872693, 0.06945485004689544, 0.07004992803558707, 0.06995956797618419, 0.06963931606151164, 0.07066256599500775, 0.08648797695059329, 0.08559156698174775, 0.08449731091968715, 0.08436995092779398, 0.085658062947914, 0.0852491419063881, 0.08486074896063656, 0.08520636602770537, 0.08609478594735265, 0.08963866392150521, 0.08550910197664052, 0.09036901604849845, 0.09033857402391732, 0.09120019897818565, 0.08565963001456112, 0.09029262606054544, 0.0903124799951911, 0.0901899979216978, 0.0943373489426449, 0.0904439379228279, 0.08904467592947185, 0.09067532501649112, 0.07573424489237368, 0.0769196399487555, 0.07581662305165082, 0.07983798894565552, 0.07928228401578963, 0.07932306395377964, 0.08094965503551066, 0.07973182899877429, 0.08119516400620341, 0.07473683508578688, 0.07632749993354082, 0.07598540396429598, 0.07543543993961066, 0.10118808201514184, 0.075125718023628, 0.0759193750564009, 0.07624420302454382, 0.07674131402745843, 0.07698040409013629, 0.07670916407369077, 0.07787467702291906, 0.07694151403848082, 0.0761577261146158, 0.07701895199716091, 0.07636329904198647, 0.07600538094993681, 0.07641419302672148, 0.0765100980643183, 0.07636856997851282, 0.07781521300785244, 0.0768241889309138, 0.07659513305407017, 0.08562339597847313, 0.07732940488494933, 0.07669134100433439, 0.07655784406233579, 0.08079990209080279, 0.08105564094148576, 0.07649604498874396, 0.07584954902995378, 0.07658739201724529, 0.076388368033804, 0.07615501002874225, 0.07603141001891345, 0.0764960169326514, 0.07606458803638816, 0.07625673699658364, 0.07813326106406748, 0.07662493700627238, 0.07607859594281763, 0.075925387092866, 0.077280777040869, 0.07635697396472096, 0.0771124450257048, 0.07627117005176842, 0.07610018900595605, 0.07604466599877924, 0.0763636709889397, 0.0765128240454942, 0.07662783307023346, 0.07678834593389183, 0.07966483698692173, 0.07726353406906128, 0.07686056999955326, 0.07697177899535745, 0.07638455310370773, 0.07603212899994105, 0.07674771698657423, 0.07617076195310801, 0.07650050893425941, 0.07712891907431185, 0.07651124300900847, 0.07727329002227634, 0.07688585994765162, 0.07739732891786844, 0.076880665961653, 0.07640168501529843, 0.07641349092591554, 0.07634974701795727, 0.07602077804040164, 0.07637784502003342, 0.07645526505075395, 0.07667677500285208, 0.07651265792082995, 0.07665554492268711, 0.07665023196022958, 0.07660853792913258, 0.07645923795644194, 0.0833562109619379, 0.07644996396265924, 0.07718936598394066, 0.0766501190373674, 0.07674670498818159, 0.07681632705498487, 0.07634602999314666, 0.07655716896988451]
[0.0016745218379917194, 0.0015067172650133772, 0.0015120755914332612, 0.001428633552918933, 0.0014158033276432936, 0.0014382272041687856, 0.0014289349165497994, 0.0014436764699615994, 0.0014607936514503493, 0.0014350417768582702, 0.0014273324881547264, 0.001442341713652927, 0.0015530027544163928, 0.0014376625494689358, 0.0014397589185712288, 0.0014331948762873606, 0.0014401693871168761, 0.0014174459193243968, 0.0014295903680732055, 0.0014277462852282487, 0.0014212105318675845, 0.0014420931835715867, 0.0017650607540937407, 0.0017467666730968927, 0.0017244349167283093, 0.0017218357332202854, 0.001748123733630898, 0.0017397784062528185, 0.0017318520196048276, 0.0017389054291368443, 0.0017570364479051561, 0.0018293604881939839, 0.0017450837138089904, 0.0018442656336428256, 0.0018436443678350473, 0.0018612285505752175, 0.00174815571458288, 0.0018427066542968458, 0.001843111836636553, 0.0018406122024836285, 0.0019252520192376509, 0.0018457946514862838, 0.0018172382842749357, 0.0018505168370712474, 0.0015777967685911183, 0.0016024924989324063, 0.0015795129802427255, 0.0016632914363678235, 0.0016517142503289506, 0.001652563832370409, 0.0016864511465731387, 0.0016610797708077978, 0.0016915659167959045, 0.00155701739762056, 0.0015901562486154337, 0.0015830292492561664, 0.0015715716654085554, 0.0021080850419821218, 0.0015651191254922499, 0.001581653647008352, 0.0015884208963446629, 0.0015987773755720507, 0.001603758418544506, 0.0015981075848685578, 0.001622389104644147, 0.0016029482091350171, 0.0015866192940544959, 0.0016045614999408524, 0.001590902063374718, 0.0015834454364570167, 0.0015919623547233641, 0.0015939603763399646, 0.0015910118745523505, 0.0016211502709969257, 0.0016005039360607043, 0.001595731938626462, 0.0017838207495515235, 0.0016110292684364442, 0.0015977362709236331, 0.0015949550846319955, 0.0016833312935583915, 0.0016886591862809535, 0.0015936676039321658, 0.0015801989381240371, 0.0015955706670259435, 0.0015914243340375833, 0.0015865627089321304, 0.0015839877087273635, 0.0015936670194302376, 0.0015846789174247533, 0.0015886820207621593, 0.0016277762721680726, 0.001596352854297341, 0.0015849707488087006, 0.0015817788977680418, 0.0016100161883514374, 0.0015907702909316868, 0.00160650927136885, 0.001588982709411842, 0.001585420604290751, 0.0015842638749745674, 0.001590909812269577, 0.0015940171676144625, 0.0015964131889631972, 0.00159975720695608, 0.0016596841038942027, 0.00160965695977211, 0.001601261874990693, 0.001603578729069947, 0.0015913448563272443, 0.0015840026874987718, 0.0015989107705536298, 0.0015868908740230836, 0.0015937606027970712, 0.0016068524807148303, 0.0015939842293543431, 0.0016098602087974239, 0.001601788748909409, 0.0016124443524555925, 0.0016016805408677708, 0.0015917017711520505, 0.0015919477276232403, 0.0015906197295407765, 0.0015837662091750342, 0.0015912051045840296, 0.0015928180218907073, 0.0015974328125594184, 0.0015940137066839573, 0.0015969905192226481, 0.0015968798325047828, 0.0015960112068569288, 0.001592900790759207, 0.0017365877283737063, 0.001592707582555401, 0.001608111791332097, 0.0015968774799451542, 0.0015988896872537832, 0.0016003401469788514, 0.0015905422915238887, 0.0015949410202059273]
[597.1854038041785, 663.6945253236489, 661.3425979928181, 699.9695603934514, 706.312791102541, 695.3004345220571, 699.8219361974344, 692.6759705563388, 684.5593825022102, 696.8438244280909, 700.6076077570494, 693.3169792804256, 643.9138611674857, 695.5735199260732, 694.5607261751629, 697.7418190263586, 694.3627665922922, 705.4942882594308, 699.5010755058421, 700.4045539086334, 703.6255203413951, 693.436465404636, 566.5527363183845, 572.4863059283535, 579.9001112186094, 580.7754948433653, 572.0418874028867, 574.7858442235911, 577.4165394501656, 575.0744021176464, 569.1401571050275, 546.639115938947, 573.0384119036343, 542.22124067062, 542.4039567751772, 537.2795295295398, 572.0314224059873, 542.6799744105704, 542.5606738139527, 543.2974956107815, 519.4125184691268, 541.772075888709, 550.2855672001167, 540.3895711549793, 633.7951882693627, 624.0278819814808, 633.1065413886811, 601.2175486117624, 605.431599201159, 605.1203471914408, 592.9611433049783, 602.0180472811918, 591.1682128794363, 642.2535814488675, 628.8690189223297, 631.7002673639037, 636.3056944908928, 474.3641646732394, 638.9290014493224, 632.2496722916983, 629.5560592921181, 625.4779528902171, 623.5353083337526, 625.7401000210187, 616.3749479933415, 623.8504739586191, 630.2709186427257, 623.223229547052, 628.5742051768664, 631.5342334987653, 628.1555572171619, 627.3681672666103, 628.5308211677311, 616.845962950153, 624.803211956657, 626.6716707198061, 560.5944432765531, 620.7211871268684, 625.8855220341911, 626.9769033845428, 594.0601257914606, 592.1858052378033, 627.4834209672275, 632.8317124343653, 626.7350112821673, 628.3679208692959, 630.2933973993825, 631.318030114917, 627.483651106438, 631.0426604432212, 629.4525820341674, 614.3350392176911, 626.4279211879914, 630.9264702529194, 632.1996085616283, 621.1117672201431, 628.6262735107514, 622.4676183461644, 629.333468562504, 630.7474478971824, 631.2079798045343, 628.5711435605575, 627.3458155388357, 626.4042460394967, 625.0948554266799, 602.5242982406401, 621.2503812871885, 624.5074685274776, 623.6051787616227, 628.3993039120113, 631.3120601954631, 625.4257701033221, 630.1630542904323, 627.4468061545672, 622.3346648194714, 627.3587790796767, 621.1719468158087, 624.3020502428041, 620.1764411138279, 624.3442275063243, 628.258394960643, 628.161328822642, 628.6857766366995, 631.4063238670111, 628.4544947217339, 627.8181099514304, 626.0044191766618, 627.3471776351973, 626.177793770974, 626.2211968895944, 626.5620164217574, 627.7854878352975, 575.8419132308899, 627.8616432500195, 621.8473152116117, 626.2221194542399, 625.4340171006903, 624.8671583274447, 628.7163851782313, 626.982432159709]
Elapsed: 0.07804867303930223~0.0055993980041178825
Time per graph: 0.0016153642092060704~0.00011269206405259409
Speed: 621.9405095586634~41.65494741037467
Total Time: 0.0772
best val loss: 0.46904224157333374 test_score: 0.8542

Testing...
Test loss: 0.2848 score: 0.8542 time: 0.07s
test Score 0.8542
Epoch Time List: [0.526287249987945, 0.38707485795021057, 0.38565481605473906, 0.37809580797329545, 0.3692293179919943, 0.3666901789838448, 0.3683510010596365, 0.36648806696757674, 0.37065973796416074, 0.3784928888780996, 0.36792008904740214, 0.36468323098961264, 0.3823972080135718, 0.3764823488891125, 0.38325554598122835, 0.38634953007567674, 0.36495811492204666, 0.3667290669400245, 0.3650508059654385, 0.3762325479183346, 0.36458613188005984, 0.3636513599194586, 0.3744076330913231, 0.37252875696867704, 0.3664276209892705, 0.37464779103174806, 0.3682017490500584, 0.37818767700809985, 0.3676254788879305, 0.37067748489789665, 0.3725240718340501, 0.39161301392596215, 0.37195807800162584, 0.39667305699549615, 0.39499386586248875, 0.40662991208955646, 0.38884232204873115, 0.39207145501859486, 0.4011939688352868, 0.3956933099543676, 0.4069837089627981, 0.39531465398613364, 0.38934743392746896, 0.388437645859085, 0.37024542409926653, 0.36265006894245744, 0.36615652113687247, 0.38738081813789904, 0.381307183066383, 0.38199520600028336, 0.38162108114920557, 0.3829622990451753, 0.41519031987991184, 0.37089699611533433, 0.3604342609178275, 0.36101579398382455, 0.36472493200562894, 0.4178348268615082, 0.36997417302336544, 0.36580953502561897, 0.36707798298448324, 0.365811028983444, 0.36993408494163305, 0.3666002998361364, 0.37005444592796266, 0.37395864503923804, 0.3692892659455538, 0.3668261809507385, 0.3669832310406491, 0.367320803925395, 0.3662644949508831, 0.3677166630513966, 0.36817511287517846, 0.3976715230382979, 0.3687849489506334, 0.39520326303318143, 0.36996256303973496, 0.3932262168964371, 0.37670394498854876, 0.3692756079835817, 0.3821526998654008, 0.3933975570835173, 0.3693491570884362, 0.36372153111733496, 0.36622573295608163, 0.36660265002865344, 0.36524948314763606, 0.36572675907518715, 0.36701563210226595, 0.36480101698543876, 0.3646544850198552, 0.36946679989341646, 0.37225988088175654, 0.3668364769546315, 0.3662516358308494, 0.3665556131163612, 0.3659766218625009, 0.366254048072733, 0.36726428219117224, 0.36541433597449213, 0.36782439902890474, 0.36762180004734546, 0.36658333998639137, 0.3686460937606171, 0.3686405449407175, 0.37729740503709763, 0.3723539289785549, 0.3699807079974562, 0.36827493691816926, 0.3680649941088632, 0.3681612011278048, 0.36821308406069875, 0.3677936157910153, 0.3678423019591719, 0.36721326305996627, 0.3655365019803867, 0.36849913198966533, 0.36872539098840207, 0.36991891090292484, 0.3740253420546651, 0.36882830096874386, 0.36699668876826763, 0.3661739149829373, 0.36657373106572777, 0.36882944009266794, 0.3687332678819075, 0.3684050369774923, 0.36784879805054516, 0.3682611519470811, 0.36646344990003854, 0.42466351692564785, 0.36939906899351627, 0.37998917989898473, 0.371058757067658, 0.37045168690383434, 0.37077574292197824, 0.36965680099092424, 0.36895529401954263, 0.3679126219358295, 0.3672408760758117]
Total Epoch List: [22, 22, 96]
Total Time List: [0.07097345602232963, 0.0910892259562388, 0.0771755080204457]
========================training times:1========================
========================k_idx:0========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: GPSTransformer
GPS(
  (x_lin): Linear(in_features=14887, out_features=64, bias=True)
  (pe_lin): Linear(in_features=20, out_features=8, bias=True)
  (pe_norm): BatchNorm1d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (convs): ModuleList(
    (0-1): 2 x GPSConv(64, conv=GINEConv(nn=Sequential(
      (0): Linear(in_features=64, out_features=64, bias=True)
      (1): ReLU()
      (2): Linear(in_features=64, out_features=64, bias=True)
    )), heads=1, attn_type=multihead)
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=32, bias=True)
    (1): ReLU()
    (2): Linear(in_features=32, out_features=16, bias=True)
    (3): ReLU()
    (4): Linear(in_features=16, out_features=2, bias=True)
  )
)
Total number of parameters: 1039906
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x737bd9adaa10>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 1.5625;  Loss pred: 1.5625; Loss self: 0.0000; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 5.7978 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 3.8803 score: 0.4898 time: 0.07s
Epoch 2/1000, LR 0.000000
Train loss: 1.5225;  Loss pred: 1.5225; Loss self: 0.0000; time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 4.5897 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 3.0174 score: 0.4898 time: 0.07s
Epoch 3/1000, LR 0.000030
Train loss: 1.4057;  Loss pred: 1.4057; Loss self: 0.0000; time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 3.8020 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 2.4643 score: 0.4898 time: 0.07s
Epoch 4/1000, LR 0.000060
Train loss: 1.1376;  Loss pred: 1.1376; Loss self: 0.0000; time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 3.0923 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 2.0056 score: 0.4898 time: 0.07s
Epoch 5/1000, LR 0.000090
Train loss: 0.8442;  Loss pred: 0.8442; Loss self: 0.0000; time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 2.4700 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 1.6355 score: 0.4898 time: 0.07s
Epoch 6/1000, LR 0.000120
Train loss: 0.5971;  Loss pred: 0.5971; Loss self: 0.0000; time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 1.8979 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 1.2957 score: 0.4898 time: 0.07s
Epoch 7/1000, LR 0.000150
Train loss: 0.4274;  Loss pred: 0.4274; Loss self: 0.0000; time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 1.4844 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 1.0806 score: 0.4898 time: 0.07s
Epoch 8/1000, LR 0.000180
Train loss: 0.3467;  Loss pred: 0.3467; Loss self: 0.0000; time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 1.1561 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.9068 score: 0.4898 time: 0.07s
Epoch 9/1000, LR 0.000210
Train loss: 0.2960;  Loss pred: 0.2960; Loss self: 0.0000; time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.8899 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7710 score: 0.4898 time: 0.07s
Epoch 10/1000, LR 0.000240
Train loss: 0.2327;  Loss pred: 0.2327; Loss self: 0.0000; time: 0.22s
Val loss: 0.7256 score: 0.5306 time: 0.07s
Test loss: 0.6761 score: 0.6327 time: 0.07s
Epoch 11/1000, LR 0.000270
Train loss: 0.1792;  Loss pred: 0.1792; Loss self: 0.0000; time: 0.23s
Val loss: 0.6278 score: 0.7143 time: 0.07s
Test loss: 0.6223 score: 0.7347 time: 0.07s
Epoch 12/1000, LR 0.000270
Train loss: 0.1464;  Loss pred: 0.1464; Loss self: 0.0000; time: 0.24s
Val loss: 0.5777 score: 0.7143 time: 0.09s
Test loss: 0.5952 score: 0.7551 time: 0.07s
Epoch 13/1000, LR 0.000270
Train loss: 0.1276;  Loss pred: 0.1276; Loss self: 0.0000; time: 0.23s
Val loss: 0.5506 score: 0.7143 time: 0.07s
Test loss: 0.5859 score: 0.7347 time: 0.07s
Epoch 14/1000, LR 0.000270
Train loss: 0.1189;  Loss pred: 0.1189; Loss self: 0.0000; time: 0.23s
Val loss: 0.5413 score: 0.7143 time: 0.08s
Test loss: 0.5865 score: 0.6939 time: 0.07s
Epoch 15/1000, LR 0.000270
Train loss: 0.1101;  Loss pred: 0.1101; Loss self: 0.0000; time: 0.23s
Val loss: 0.5360 score: 0.7551 time: 0.07s
Test loss: 0.5887 score: 0.6939 time: 0.07s
Epoch 16/1000, LR 0.000270
Train loss: 0.1053;  Loss pred: 0.1053; Loss self: 0.0000; time: 0.23s
Val loss: 0.5327 score: 0.7347 time: 0.07s
Test loss: 0.5897 score: 0.6939 time: 0.07s
Epoch 17/1000, LR 0.000270
Train loss: 0.0891;  Loss pred: 0.0891; Loss self: 0.0000; time: 0.25s
Val loss: 0.5331 score: 0.7347 time: 0.08s
Test loss: 0.5908 score: 0.6735 time: 0.07s
     INFO: Early stopping counter 1 of 20
Epoch 18/1000, LR 0.000270
Train loss: 0.0838;  Loss pred: 0.0838; Loss self: 0.0000; time: 0.23s
Val loss: 0.5339 score: 0.7347 time: 0.07s
Test loss: 0.5917 score: 0.6735 time: 0.07s
     INFO: Early stopping counter 2 of 20
Epoch 19/1000, LR 0.000270
Train loss: 0.0760;  Loss pred: 0.0760; Loss self: 0.0000; time: 0.23s
Val loss: 0.5363 score: 0.7143 time: 0.08s
Test loss: 0.5932 score: 0.7143 time: 0.07s
     INFO: Early stopping counter 3 of 20
Epoch 20/1000, LR 0.000270
Train loss: 0.0711;  Loss pred: 0.0711; Loss self: 0.0000; time: 0.22s
Val loss: 0.5404 score: 0.7143 time: 0.07s
Test loss: 0.5948 score: 0.7347 time: 0.06s
     INFO: Early stopping counter 4 of 20
Epoch 21/1000, LR 0.000270
Train loss: 0.0701;  Loss pred: 0.0701; Loss self: 0.0000; time: 0.22s
Val loss: 0.5433 score: 0.7143 time: 0.07s
Test loss: 0.5959 score: 0.7347 time: 0.06s
     INFO: Early stopping counter 5 of 20
Epoch 22/1000, LR 0.000270
Train loss: 0.0572;  Loss pred: 0.0572; Loss self: 0.0000; time: 0.22s
Val loss: 0.5461 score: 0.7143 time: 0.07s
Test loss: 0.5968 score: 0.7551 time: 0.06s
     INFO: Early stopping counter 6 of 20
Epoch 23/1000, LR 0.000270
Train loss: 0.0552;  Loss pred: 0.0552; Loss self: 0.0000; time: 0.22s
Val loss: 0.5482 score: 0.7143 time: 0.07s
Test loss: 0.5971 score: 0.7551 time: 0.06s
     INFO: Early stopping counter 7 of 20
Epoch 24/1000, LR 0.000270
Train loss: 0.0518;  Loss pred: 0.0518; Loss self: 0.0000; time: 0.22s
Val loss: 0.5494 score: 0.7347 time: 0.07s
Test loss: 0.5961 score: 0.7551 time: 0.06s
     INFO: Early stopping counter 8 of 20
Epoch 25/1000, LR 0.000270
Train loss: 0.0483;  Loss pred: 0.0483; Loss self: 0.0000; time: 0.22s
Val loss: 0.5510 score: 0.7551 time: 0.07s
Test loss: 0.5950 score: 0.7551 time: 0.06s
     INFO: Early stopping counter 9 of 20
Epoch 26/1000, LR 0.000270
Train loss: 0.0484;  Loss pred: 0.0484; Loss self: 0.0000; time: 0.22s
Val loss: 0.5514 score: 0.7347 time: 0.07s
Test loss: 0.5912 score: 0.7551 time: 0.06s
     INFO: Early stopping counter 10 of 20
Epoch 27/1000, LR 0.000270
Train loss: 0.0410;  Loss pred: 0.0410; Loss self: 0.0000; time: 0.22s
Val loss: 0.5518 score: 0.7347 time: 0.07s
Test loss: 0.5864 score: 0.7347 time: 0.07s
     INFO: Early stopping counter 11 of 20
Epoch 28/1000, LR 0.000270
Train loss: 0.0402;  Loss pred: 0.0402; Loss self: 0.0000; time: 0.23s
Val loss: 0.5523 score: 0.7347 time: 0.07s
Test loss: 0.5808 score: 0.7551 time: 0.06s
     INFO: Early stopping counter 12 of 20
Epoch 29/1000, LR 0.000270
Train loss: 0.0439;  Loss pred: 0.0439; Loss self: 0.0000; time: 0.23s
Val loss: 0.5513 score: 0.7347 time: 0.08s
Test loss: 0.5758 score: 0.7347 time: 0.06s
     INFO: Early stopping counter 13 of 20
Epoch 30/1000, LR 0.000270
Train loss: 0.0334;  Loss pred: 0.0334; Loss self: 0.0000; time: 0.24s
Val loss: 0.5490 score: 0.7347 time: 0.08s
Test loss: 0.5704 score: 0.7755 time: 0.08s
     INFO: Early stopping counter 14 of 20
Epoch 31/1000, LR 0.000270
Train loss: 0.0343;  Loss pred: 0.0343; Loss self: 0.0000; time: 0.23s
Val loss: 0.5461 score: 0.7551 time: 0.08s
Test loss: 0.5651 score: 0.7755 time: 0.07s
     INFO: Early stopping counter 15 of 20
Epoch 32/1000, LR 0.000270
Train loss: 0.0297;  Loss pred: 0.0297; Loss self: 0.0000; time: 0.22s
Val loss: 0.5423 score: 0.7551 time: 0.07s
Test loss: 0.5596 score: 0.7551 time: 0.06s
     INFO: Early stopping counter 16 of 20
Epoch 33/1000, LR 0.000270
Train loss: 0.0305;  Loss pred: 0.0305; Loss self: 0.0000; time: 0.22s
Val loss: 0.5372 score: 0.7551 time: 0.07s
Test loss: 0.5532 score: 0.8163 time: 0.06s
     INFO: Early stopping counter 17 of 20
Epoch 34/1000, LR 0.000270
Train loss: 0.0293;  Loss pred: 0.0293; Loss self: 0.0000; time: 0.22s
Val loss: 0.5309 score: 0.7959 time: 0.07s
Test loss: 0.5461 score: 0.8163 time: 0.06s
Epoch 35/1000, LR 0.000270
Train loss: 0.0270;  Loss pred: 0.0270; Loss self: 0.0000; time: 0.22s
Val loss: 0.5212 score: 0.7755 time: 0.07s
Test loss: 0.5368 score: 0.7755 time: 0.06s
Epoch 36/1000, LR 0.000270
Train loss: 0.0249;  Loss pred: 0.0249; Loss self: 0.0000; time: 0.23s
Val loss: 0.5137 score: 0.7755 time: 0.07s
Test loss: 0.5312 score: 0.7347 time: 0.06s
Epoch 37/1000, LR 0.000270
Train loss: 0.0235;  Loss pred: 0.0235; Loss self: 0.0000; time: 0.22s
Val loss: 0.5080 score: 0.7347 time: 0.07s
Test loss: 0.5286 score: 0.7755 time: 0.06s
Epoch 38/1000, LR 0.000270
Train loss: 0.0209;  Loss pred: 0.0209; Loss self: 0.0000; time: 0.22s
Val loss: 0.5040 score: 0.7347 time: 0.07s
Test loss: 0.5273 score: 0.7551 time: 0.06s
Epoch 39/1000, LR 0.000269
Train loss: 0.0193;  Loss pred: 0.0193; Loss self: 0.0000; time: 0.22s
Val loss: 0.5012 score: 0.7347 time: 0.07s
Test loss: 0.5257 score: 0.7551 time: 0.07s
Epoch 40/1000, LR 0.000269
Train loss: 0.0187;  Loss pred: 0.0187; Loss self: 0.0000; time: 0.22s
Val loss: 0.4986 score: 0.7347 time: 0.07s
Test loss: 0.5234 score: 0.7551 time: 0.07s
Epoch 41/1000, LR 0.000269
Train loss: 0.0192;  Loss pred: 0.0192; Loss self: 0.0000; time: 0.22s
Val loss: 0.4964 score: 0.7551 time: 0.07s
Test loss: 0.5214 score: 0.7551 time: 0.07s
Epoch 42/1000, LR 0.000269
Train loss: 0.0176;  Loss pred: 0.0176; Loss self: 0.0000; time: 0.24s
Val loss: 0.4937 score: 0.7755 time: 0.07s
Test loss: 0.5188 score: 0.7551 time: 0.07s
Epoch 43/1000, LR 0.000269
Train loss: 0.0167;  Loss pred: 0.0167; Loss self: 0.0000; time: 0.22s
Val loss: 0.4913 score: 0.7755 time: 0.07s
Test loss: 0.5163 score: 0.7551 time: 0.07s
Epoch 44/1000, LR 0.000269
Train loss: 0.0158;  Loss pred: 0.0158; Loss self: 0.0000; time: 0.22s
Val loss: 0.4892 score: 0.7755 time: 0.07s
Test loss: 0.5142 score: 0.7551 time: 0.07s
Epoch 45/1000, LR 0.000269
Train loss: 0.0142;  Loss pred: 0.0142; Loss self: 0.0000; time: 0.22s
Val loss: 0.4869 score: 0.7755 time: 0.07s
Test loss: 0.5119 score: 0.7755 time: 0.06s
Epoch 46/1000, LR 0.000269
Train loss: 0.0134;  Loss pred: 0.0134; Loss self: 0.0000; time: 0.22s
Val loss: 0.4860 score: 0.7755 time: 0.07s
Test loss: 0.5102 score: 0.7551 time: 0.06s
Epoch 47/1000, LR 0.000269
Train loss: 0.0148;  Loss pred: 0.0148; Loss self: 0.0000; time: 0.22s
Val loss: 0.4877 score: 0.7755 time: 0.07s
Test loss: 0.5104 score: 0.7551 time: 0.06s
     INFO: Early stopping counter 1 of 20
Epoch 48/1000, LR 0.000269
Train loss: 0.0170;  Loss pred: 0.0170; Loss self: 0.0000; time: 0.22s
Val loss: 0.4891 score: 0.7755 time: 0.07s
Test loss: 0.5112 score: 0.7755 time: 0.07s
     INFO: Early stopping counter 2 of 20
Epoch 49/1000, LR 0.000269
Train loss: 0.0113;  Loss pred: 0.0113; Loss self: 0.0000; time: 0.22s
Val loss: 0.4908 score: 0.7755 time: 0.07s
Test loss: 0.5131 score: 0.7551 time: 0.06s
     INFO: Early stopping counter 3 of 20
Epoch 50/1000, LR 0.000269
Train loss: 0.0121;  Loss pred: 0.0121; Loss self: 0.0000; time: 0.22s
Val loss: 0.4913 score: 0.7755 time: 0.07s
Test loss: 0.5142 score: 0.7551 time: 0.06s
     INFO: Early stopping counter 4 of 20
Epoch 51/1000, LR 0.000269
Train loss: 0.0116;  Loss pred: 0.0116; Loss self: 0.0000; time: 0.30s
Val loss: 0.4905 score: 0.7959 time: 0.07s
Test loss: 0.5143 score: 0.7551 time: 0.07s
     INFO: Early stopping counter 5 of 20
Epoch 52/1000, LR 0.000269
Train loss: 0.0100;  Loss pred: 0.0100; Loss self: 0.0000; time: 0.22s
Val loss: 0.4882 score: 0.7959 time: 0.07s
Test loss: 0.5125 score: 0.7551 time: 0.06s
     INFO: Early stopping counter 6 of 20
Epoch 53/1000, LR 0.000269
Train loss: 0.0106;  Loss pred: 0.0106; Loss self: 0.0000; time: 0.22s
Val loss: 0.4849 score: 0.7959 time: 0.07s
Test loss: 0.5094 score: 0.7551 time: 0.07s
Epoch 54/1000, LR 0.000269
Train loss: 0.0085;  Loss pred: 0.0085; Loss self: 0.0000; time: 0.22s
Val loss: 0.4807 score: 0.8163 time: 0.08s
Test loss: 0.5058 score: 0.7551 time: 0.07s
Epoch 55/1000, LR 0.000269
Train loss: 0.0082;  Loss pred: 0.0082; Loss self: 0.0000; time: 0.23s
Val loss: 0.4764 score: 0.8163 time: 0.08s
Test loss: 0.5028 score: 0.7551 time: 0.07s
Epoch 56/1000, LR 0.000269
Train loss: 0.0081;  Loss pred: 0.0081; Loss self: 0.0000; time: 0.24s
Val loss: 0.4738 score: 0.8163 time: 0.08s
Test loss: 0.5009 score: 0.7347 time: 0.07s
Epoch 57/1000, LR 0.000269
Train loss: 0.0080;  Loss pred: 0.0080; Loss self: 0.0000; time: 0.23s
Val loss: 0.4725 score: 0.8367 time: 0.08s
Test loss: 0.5000 score: 0.7959 time: 0.07s
Epoch 58/1000, LR 0.000269
Train loss: 0.0097;  Loss pred: 0.0097; Loss self: 0.0000; time: 0.23s
Val loss: 0.4709 score: 0.7959 time: 0.08s
Test loss: 0.4959 score: 0.7959 time: 0.07s
Epoch 59/1000, LR 0.000268
Train loss: 0.0068;  Loss pred: 0.0068; Loss self: 0.0000; time: 0.23s
Val loss: 0.4674 score: 0.8367 time: 0.08s
Test loss: 0.4909 score: 0.8163 time: 0.07s
Epoch 60/1000, LR 0.000268
Train loss: 0.0067;  Loss pred: 0.0067; Loss self: 0.0000; time: 0.23s
Val loss: 0.4624 score: 0.8367 time: 0.08s
Test loss: 0.4849 score: 0.8163 time: 0.07s
Epoch 61/1000, LR 0.000268
Train loss: 0.0070;  Loss pred: 0.0070; Loss self: 0.0000; time: 0.23s
Val loss: 0.4558 score: 0.8571 time: 0.08s
Test loss: 0.4776 score: 0.8163 time: 0.07s
Epoch 62/1000, LR 0.000268
Train loss: 0.0066;  Loss pred: 0.0066; Loss self: 0.0000; time: 0.23s
Val loss: 0.4465 score: 0.8571 time: 0.08s
Test loss: 0.4704 score: 0.8367 time: 0.07s
Epoch 63/1000, LR 0.000268
Train loss: 0.0062;  Loss pred: 0.0062; Loss self: 0.0000; time: 0.23s
Val loss: 0.4383 score: 0.8571 time: 0.08s
Test loss: 0.4637 score: 0.8367 time: 0.07s
Epoch 64/1000, LR 0.000268
Train loss: 0.0072;  Loss pred: 0.0072; Loss self: 0.0000; time: 0.23s
Val loss: 0.4315 score: 0.8571 time: 0.08s
Test loss: 0.4571 score: 0.8367 time: 0.07s
Epoch 65/1000, LR 0.000268
Train loss: 0.0061;  Loss pred: 0.0061; Loss self: 0.0000; time: 0.23s
Val loss: 0.4261 score: 0.8571 time: 0.16s
Test loss: 0.4520 score: 0.8367 time: 0.07s
Epoch 66/1000, LR 0.000268
Train loss: 0.0053;  Loss pred: 0.0053; Loss self: 0.0000; time: 0.23s
Val loss: 0.4201 score: 0.8571 time: 0.08s
Test loss: 0.4475 score: 0.8367 time: 0.07s
Epoch 67/1000, LR 0.000268
Train loss: 0.0053;  Loss pred: 0.0053; Loss self: 0.0000; time: 0.23s
Val loss: 0.4154 score: 0.8776 time: 0.08s
Test loss: 0.4448 score: 0.8367 time: 0.07s
Epoch 68/1000, LR 0.000268
Train loss: 0.0053;  Loss pred: 0.0053; Loss self: 0.0000; time: 0.23s
Val loss: 0.4112 score: 0.8776 time: 0.09s
Test loss: 0.4432 score: 0.8367 time: 0.07s
Epoch 69/1000, LR 0.000268
Train loss: 0.0052;  Loss pred: 0.0052; Loss self: 0.0000; time: 0.23s
Val loss: 0.4064 score: 0.8776 time: 0.08s
Test loss: 0.4393 score: 0.8367 time: 0.07s
Epoch 70/1000, LR 0.000268
Train loss: 0.0068;  Loss pred: 0.0068; Loss self: 0.0000; time: 0.23s
Val loss: 0.3960 score: 0.8980 time: 0.08s
Test loss: 0.4304 score: 0.8367 time: 0.07s
Epoch 71/1000, LR 0.000268
Train loss: 0.0053;  Loss pred: 0.0053; Loss self: 0.0000; time: 0.23s
Val loss: 0.3840 score: 0.9184 time: 0.08s
Test loss: 0.4200 score: 0.8367 time: 0.07s
Epoch 72/1000, LR 0.000267
Train loss: 0.0042;  Loss pred: 0.0042; Loss self: 0.0000; time: 0.23s
Val loss: 0.3701 score: 0.9184 time: 0.08s
Test loss: 0.4089 score: 0.8367 time: 0.07s
Epoch 73/1000, LR 0.000267
Train loss: 0.0046;  Loss pred: 0.0046; Loss self: 0.0000; time: 0.23s
Val loss: 0.3551 score: 0.9184 time: 0.08s
Test loss: 0.3976 score: 0.8367 time: 0.07s
Epoch 74/1000, LR 0.000267
Train loss: 0.0044;  Loss pred: 0.0044; Loss self: 0.0000; time: 0.23s
Val loss: 0.3388 score: 0.9388 time: 0.08s
Test loss: 0.3884 score: 0.8367 time: 0.07s
Epoch 75/1000, LR 0.000267
Train loss: 0.0045;  Loss pred: 0.0045; Loss self: 0.0000; time: 0.23s
Val loss: 0.3235 score: 0.9184 time: 0.08s
Test loss: 0.3808 score: 0.8367 time: 0.07s
Epoch 76/1000, LR 0.000267
Train loss: 0.0044;  Loss pred: 0.0044; Loss self: 0.0000; time: 0.23s
Val loss: 0.3095 score: 0.9184 time: 0.08s
Test loss: 0.3747 score: 0.8367 time: 0.07s
Epoch 77/1000, LR 0.000267
Train loss: 0.0042;  Loss pred: 0.0042; Loss self: 0.0000; time: 0.23s
Val loss: 0.2971 score: 0.9184 time: 0.08s
Test loss: 0.3701 score: 0.8367 time: 0.07s
Epoch 78/1000, LR 0.000267
Train loss: 0.0037;  Loss pred: 0.0037; Loss self: 0.0000; time: 0.23s
Val loss: 0.2852 score: 0.9184 time: 0.08s
Test loss: 0.3662 score: 0.8571 time: 0.07s
Epoch 79/1000, LR 0.000267
Train loss: 0.0035;  Loss pred: 0.0035; Loss self: 0.0000; time: 0.23s
Val loss: 0.2743 score: 0.9184 time: 0.08s
Test loss: 0.3633 score: 0.8571 time: 0.07s
Epoch 80/1000, LR 0.000267
Train loss: 0.0037;  Loss pred: 0.0037; Loss self: 0.0000; time: 0.23s
Val loss: 0.2651 score: 0.9184 time: 0.08s
Test loss: 0.3609 score: 0.8571 time: 0.07s
Epoch 81/1000, LR 0.000267
Train loss: 0.0035;  Loss pred: 0.0035; Loss self: 0.0000; time: 0.23s
Val loss: 0.2575 score: 0.9184 time: 0.08s
Test loss: 0.3596 score: 0.8571 time: 0.07s
Epoch 82/1000, LR 0.000267
Train loss: 0.0036;  Loss pred: 0.0036; Loss self: 0.0000; time: 0.23s
Val loss: 0.2505 score: 0.9184 time: 0.08s
Test loss: 0.3599 score: 0.8571 time: 0.07s
Epoch 83/1000, LR 0.000266
Train loss: 0.0033;  Loss pred: 0.0033; Loss self: 0.0000; time: 0.23s
Val loss: 0.2447 score: 0.9184 time: 0.08s
Test loss: 0.3606 score: 0.8571 time: 0.07s
Epoch 84/1000, LR 0.000266
Train loss: 0.0030;  Loss pred: 0.0030; Loss self: 0.0000; time: 0.23s
Val loss: 0.2397 score: 0.9184 time: 0.08s
Test loss: 0.3611 score: 0.8571 time: 0.07s
Epoch 85/1000, LR 0.000266
Train loss: 0.0030;  Loss pred: 0.0030; Loss self: 0.0000; time: 0.23s
Val loss: 0.2363 score: 0.9184 time: 0.08s
Test loss: 0.3619 score: 0.8571 time: 0.07s
Epoch 86/1000, LR 0.000266
Train loss: 0.0030;  Loss pred: 0.0030; Loss self: 0.0000; time: 0.23s
Val loss: 0.2338 score: 0.9184 time: 0.08s
Test loss: 0.3635 score: 0.8571 time: 0.07s
Epoch 87/1000, LR 0.000266
Train loss: 0.0037;  Loss pred: 0.0037; Loss self: 0.0000; time: 0.23s
Val loss: 0.2301 score: 0.9184 time: 0.08s
Test loss: 0.3664 score: 0.8571 time: 0.07s
Epoch 88/1000, LR 0.000266
Train loss: 0.0027;  Loss pred: 0.0027; Loss self: 0.0000; time: 0.23s
Val loss: 0.2249 score: 0.9184 time: 0.08s
Test loss: 0.3709 score: 0.8571 time: 0.07s
Epoch 89/1000, LR 0.000266
Train loss: 0.0026;  Loss pred: 0.0026; Loss self: 0.0000; time: 0.23s
Val loss: 0.2222 score: 0.9184 time: 0.07s
Test loss: 0.3779 score: 0.8571 time: 0.06s
Epoch 90/1000, LR 0.000266
Train loss: 0.0026;  Loss pred: 0.0026; Loss self: 0.0000; time: 0.22s
Val loss: 0.2228 score: 0.9184 time: 0.07s
Test loss: 0.3833 score: 0.8367 time: 0.06s
     INFO: Early stopping counter 1 of 20
Epoch 91/1000, LR 0.000266
Train loss: 0.0028;  Loss pred: 0.0028; Loss self: 0.0000; time: 0.22s
Val loss: 0.2234 score: 0.9184 time: 0.07s
Test loss: 0.3873 score: 0.8367 time: 0.06s
     INFO: Early stopping counter 2 of 20
Epoch 92/1000, LR 0.000266
Train loss: 0.0031;  Loss pred: 0.0031; Loss self: 0.0000; time: 0.22s
Val loss: 0.2241 score: 0.9184 time: 0.07s
Test loss: 0.3909 score: 0.8571 time: 0.06s
     INFO: Early stopping counter 3 of 20
Epoch 93/1000, LR 0.000265
Train loss: 0.0024;  Loss pred: 0.0024; Loss self: 0.0000; time: 0.22s
Val loss: 0.2249 score: 0.9184 time: 0.07s
Test loss: 0.3941 score: 0.8571 time: 0.06s
     INFO: Early stopping counter 4 of 20
Epoch 94/1000, LR 0.000265
Train loss: 0.0025;  Loss pred: 0.0025; Loss self: 0.0000; time: 0.22s
Val loss: 0.2262 score: 0.9184 time: 0.07s
Test loss: 0.3974 score: 0.8571 time: 0.07s
     INFO: Early stopping counter 5 of 20
Epoch 95/1000, LR 0.000265
Train loss: 0.0023;  Loss pred: 0.0023; Loss self: 0.0000; time: 0.22s
Val loss: 0.2285 score: 0.9184 time: 0.07s
Test loss: 0.4012 score: 0.8571 time: 0.07s
     INFO: Early stopping counter 6 of 20
Epoch 96/1000, LR 0.000265
Train loss: 0.0024;  Loss pred: 0.0024; Loss self: 0.0000; time: 0.22s
Val loss: 0.2352 score: 0.9184 time: 0.07s
Test loss: 0.4053 score: 0.8980 time: 0.06s
     INFO: Early stopping counter 7 of 20
Epoch 97/1000, LR 0.000265
Train loss: 0.0025;  Loss pred: 0.0025; Loss self: 0.0000; time: 0.22s
Val loss: 0.2446 score: 0.9184 time: 0.07s
Test loss: 0.4106 score: 0.8980 time: 0.06s
     INFO: Early stopping counter 8 of 20
Epoch 98/1000, LR 0.000265
Train loss: 0.0024;  Loss pred: 0.0024; Loss self: 0.0000; time: 0.22s
Val loss: 0.2546 score: 0.8980 time: 0.07s
Test loss: 0.4172 score: 0.8776 time: 0.07s
     INFO: Early stopping counter 9 of 20
Epoch 99/1000, LR 0.000265
Train loss: 0.0025;  Loss pred: 0.0025; Loss self: 0.0000; time: 0.22s
Val loss: 0.2656 score: 0.8980 time: 0.07s
Test loss: 0.4241 score: 0.8776 time: 0.06s
     INFO: Early stopping counter 10 of 20
Epoch 100/1000, LR 0.000265
Train loss: 0.0025;  Loss pred: 0.0025; Loss self: 0.0000; time: 0.22s
Val loss: 0.2775 score: 0.8980 time: 0.07s
Test loss: 0.4312 score: 0.8776 time: 0.07s
     INFO: Early stopping counter 11 of 20
Epoch 101/1000, LR 0.000265
Train loss: 0.0021;  Loss pred: 0.0021; Loss self: 0.0000; time: 0.22s
Val loss: 0.2877 score: 0.8980 time: 0.07s
Test loss: 0.4392 score: 0.8776 time: 0.06s
     INFO: Early stopping counter 12 of 20
Epoch 102/1000, LR 0.000264
Train loss: 0.0023;  Loss pred: 0.0023; Loss self: 0.0000; time: 0.22s
Val loss: 0.2982 score: 0.8980 time: 0.07s
Test loss: 0.4479 score: 0.8776 time: 0.06s
     INFO: Early stopping counter 13 of 20
Epoch 103/1000, LR 0.000264
Train loss: 0.0021;  Loss pred: 0.0021; Loss self: 0.0000; time: 0.22s
Val loss: 0.3070 score: 0.8980 time: 0.07s
Test loss: 0.4570 score: 0.8776 time: 0.06s
     INFO: Early stopping counter 14 of 20
Epoch 104/1000, LR 0.000264
Train loss: 0.0023;  Loss pred: 0.0023; Loss self: 0.0000; time: 0.22s
Val loss: 0.3157 score: 0.8980 time: 0.07s
Test loss: 0.4658 score: 0.8776 time: 0.06s
     INFO: Early stopping counter 15 of 20
Epoch 105/1000, LR 0.000264
Train loss: 0.0021;  Loss pred: 0.0021; Loss self: 0.0000; time: 0.22s
Val loss: 0.3258 score: 0.8980 time: 0.07s
Test loss: 0.4741 score: 0.8571 time: 0.06s
     INFO: Early stopping counter 16 of 20
Epoch 106/1000, LR 0.000264
Train loss: 0.0021;  Loss pred: 0.0021; Loss self: 0.0000; time: 0.22s
Val loss: 0.3359 score: 0.8980 time: 0.07s
Test loss: 0.4817 score: 0.8571 time: 0.06s
     INFO: Early stopping counter 17 of 20
Epoch 107/1000, LR 0.000264
Train loss: 0.0021;  Loss pred: 0.0021; Loss self: 0.0000; time: 0.22s
Val loss: 0.3467 score: 0.8980 time: 0.07s
Test loss: 0.4891 score: 0.8776 time: 0.07s
     INFO: Early stopping counter 18 of 20
Epoch 108/1000, LR 0.000264
Train loss: 0.0019;  Loss pred: 0.0019; Loss self: 0.0000; time: 0.22s
Val loss: 0.3579 score: 0.8980 time: 0.08s
Test loss: 0.4958 score: 0.8776 time: 0.07s
     INFO: Early stopping counter 19 of 20
Epoch 109/1000, LR 0.000264
Train loss: 0.0018;  Loss pred: 0.0018; Loss self: 0.0000; time: 0.22s
Val loss: 0.3681 score: 0.8980 time: 0.07s
Test loss: 0.5027 score: 0.8776 time: 0.06s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 088,   Train_Loss: 0.0026,   Val_Loss: 0.2222,   Val_Precision: 0.9545,   Val_Recall: 0.8750,   Val_accuracy: 0.9130,   Val_Score: 0.9184,   Val_Loss: 0.2222,   Test_Precision: 0.8462,   Test_Recall: 0.8800,   Test_accuracy: 0.8627,   Test_Score: 0.8571,   Test_loss: 0.3779


[0.07140269293449819, 0.07025647105183452, 0.06996766501106322, 0.07019000092986971, 0.06995220202952623, 0.07005454506725073, 0.07077746093273163, 0.07029995403718203, 0.07004583498928696, 0.0703566949814558, 0.07014449697453529, 0.07387414004188031, 0.07123668200802058, 0.07408993097487837, 0.07081565505359322, 0.0705169040011242, 0.07046438695397228, 0.07043113105464727, 0.0699055859586224, 0.06882533896714449, 0.06925370404496789, 0.06909177300985903, 0.06914540904108435, 0.0689691980369389, 0.06894625490531325, 0.06829987303353846, 0.06975654000416398, 0.06906155892647803, 0.06937944504898041, 0.08016833302099258, 0.07170937792398036, 0.06897341297008097, 0.06905112590175122, 0.06871384603437036, 0.06867225095629692, 0.06861502793617547, 0.06934155500493944, 0.06899598299060017, 0.06969069305341691, 0.06978478899691254, 0.07093609706498682, 0.06988592993002385, 0.06969245697837323, 0.06965289206709713, 0.06922693504020572, 0.06887985300272703, 0.06885823095217347, 0.06951714202295989, 0.0690673339413479, 0.06829411606304348, 0.07016174797900021, 0.06922532001044601, 0.06981348397675902, 0.07100505603011698, 0.07897043297998607, 0.07289586903061718, 0.07353302196133882, 0.07357235392555594, 0.07318592094816267, 0.07318616309203207, 0.0728717049350962, 0.07316883199382573, 0.07307384698651731, 0.07358125993050635, 0.07363991299644113, 0.0727311500813812, 0.07411551405675709, 0.0740579100092873, 0.07290751207619905, 0.07302245392929763, 0.07290364999789745, 0.07331615604925901, 0.07277754601091146, 0.07269923505373299, 0.07256347103975713, 0.07249310705810785, 0.07273959601297975, 0.07268922601360828, 0.07280515797901899, 0.07267696899361908, 0.07533581799361855, 0.07276648993138224, 0.07312183303292841, 0.07255846308544278, 0.07249504700303078, 0.07267009397037327, 0.07272250892128795, 0.07309056096710265, 0.06877232401166111, 0.06895384006202221, 0.06915609107818455, 0.06868598493747413, 0.06892176100518554, 0.07048276998102665, 0.0694201800506562, 0.06912049592938274, 0.06933541398029774, 0.06938951299525797, 0.06920018105302006, 0.06952238595113158, 0.06895315402653068, 0.06901283795014024, 0.06897767609916627, 0.06909350794740021, 0.06912156892940402, 0.06891590903978795, 0.06949843396432698, 0.0716112699592486, 0.06935271294787526]
[0.001457197814989759, 0.0014338055316700923, 0.0014279115308380248, 0.0014324489985687696, 0.0014275959597862496, 0.0014296845932091986, 0.0014444379782190128, 0.0014346929395343273, 0.0014295068365160603, 0.001435850917988894, 0.0014315203464190876, 0.0015076355110587819, 0.001453809836898379, 0.001512039407650579, 0.001445217450073331, 0.0014391204898188614, 0.001438048713346373, 0.0014373700215234136, 0.001426644611400457, 0.0014045987544315203, 0.0014133408988768958, 0.001410036183874674, 0.0014111307967568235, 0.0014075346538150798, 0.001407066426639046, 0.0013938749598681318, 0.0014236028572278364, 0.001409419569928123, 0.001415907041815927, 0.0016360884289998486, 0.0014634566923261297, 0.0014076206728587954, 0.0014092066510561474, 0.001402323388456538, 0.001401474509312182, 0.0014003066925750095, 0.0014151337756110089, 0.0014080812855224525, 0.0014222590419064676, 0.0014241793672839294, 0.0014476754503058536, 0.0014262434679596704, 0.0014222950403749639, 0.0014214875932060638, 0.0014127945926572596, 0.0014057112857699394, 0.0014052700194321116, 0.0014187171841420385, 0.0014095374273744468, 0.0013937574706743567, 0.0014318724077346983, 0.001412761632866245, 0.001424764979117531, 0.0014490827761248362, 0.001611641489387471, 0.0014876707965432077, 0.0015006739175783433, 0.0015014766107256316, 0.0014935902234318914, 0.0014935951651435118, 0.0014871776517366572, 0.0014932414692617497, 0.001491302999724843, 0.001501658365928701, 0.0015028553672743086, 0.0014843091853343102, 0.0015125615113623897, 0.0015113859185568836, 0.001487908409718348, 0.0014902541618224006, 0.0014878295917938255, 0.001496248082637939, 0.0014852560410390095, 0.001483657858239449, 0.0014808871640766762, 0.0014794511644511807, 0.001484481551285301, 0.0014834535921144547, 0.0014858195505922241, 0.0014832034488493691, 0.001537465673339154, 0.001485030406762903, 0.0014922823067944572, 0.0014807849609274038, 0.0014794907551638934, 0.0014830631422525157, 0.0014841328351283256, 0.0014916441013694418, 0.0014035168165645125, 0.0014072212257555553, 0.0014113487975139702, 0.0014017547946423292, 0.0014065665511262355, 0.0014384238771638091, 0.0014167383683807387, 0.0014106223659057701, 0.001415008448577505, 0.0014161125101073055, 0.0014122485929187766, 0.001418824203084318, 0.0014072072250312384, 0.0014084252642885763, 0.0014077076754931892, 0.0014100715907632696, 0.0014106442638653883, 0.0014064471232609786, 0.0014183353870270811, 0.0014614544889642572, 0.0014153614887321482]
[686.2486271344209, 697.4446519502565, 700.3234993228967, 698.1051339343664, 700.4783063057474, 699.454974020046, 692.3107915183708, 697.0132579899501, 699.541950031636, 696.4511339384991, 698.5580068781245, 663.2902930879628, 687.8478702093826, 661.3584242184596, 691.9373966521506, 694.8688501585212, 695.3867353164808, 695.7150803382821, 700.9454155638355, 711.9470929651561, 707.543382346499, 709.2016583943788, 708.6515313097, 710.4620815477122, 710.698500843791, 717.4244668937919, 702.4430970497539, 709.5119305396033, 706.2610541985027, 611.2139064581653, 683.3136950643368, 710.4186655408082, 709.6191316231283, 713.1022760025747, 713.5342051214201, 714.1292727531784, 706.6469737592352, 710.1862728251242, 703.1067973802787, 702.158746975188, 690.7625599292492, 701.1425625882529, 703.0890016577482, 703.4883770913339, 707.81697863038, 711.3836319897493, 711.6070122979734, 704.8621185234636, 709.4526052158158, 717.4849434286148, 698.3862490807094, 707.8334920316145, 701.870143256454, 690.0917024728003, 620.4853911896158, 672.1917256987414, 666.367282249906, 666.0110406360052, 669.5276818980869, 669.5254666976076, 672.4146229821611, 669.6840535070289, 670.5545420243288, 665.9304291103188, 665.4000256948711, 673.7140818641303, 661.1301375104297, 661.6443806455666, 672.0843792994583, 671.0264769716334, 672.119983038067, 668.3383668816232, 673.284586878671, 674.0098429341579, 675.2708945407692, 675.9263327025475, 673.6358556522142, 674.1026516202913, 673.0292380399866, 674.2163394885403, 650.4210255492364, 673.3868851748422, 670.1144920414426, 675.3175014511952, 675.9082451239941, 674.280124365557, 673.7941350873319, 670.4012029960261, 712.4959161143297, 710.6203215937758, 708.5420710751705, 713.3915316873657, 710.9510738751051, 695.205367399584, 705.8466279436973, 708.9069506975336, 706.7095613494681, 706.1585805242447, 708.0906329198328, 704.8089522480269, 710.6273917672652, 710.0128245037695, 710.3747584878735, 709.1838503452874, 708.8959460692394, 711.0114439861819, 705.0518580771378, 684.2498398350447, 706.5332835188129]
Elapsed: 0.0709163423834813~0.002154116301118848
Time per graph: 0.0014472722935404342~4.396155716569077e-05
Speed: 691.5692176323026~20.2448157200526
Total Time: 0.0699
best val loss: 0.22221887111663818 test_score: 0.8571

Testing...
Test loss: 0.3884 score: 0.8367 time: 0.07s
test Score 0.8367
Epoch Time List: [0.3768184391083196, 0.3662411590339616, 0.36470814992208034, 0.36547916010022163, 0.3644243380986154, 0.3653993421467021, 0.3671741030411795, 0.3645067849429324, 0.364803500007838, 0.3645692120771855, 0.36522559297736734, 0.3947337060235441, 0.36995676904916763, 0.3746724030934274, 0.36942692508455366, 0.3669546360615641, 0.3952438960550353, 0.36785596900153905, 0.3729710760526359, 0.36117926705628633, 0.36155636806506664, 0.3621771589387208, 0.360922793042846, 0.3618941028835252, 0.3624093937687576, 0.3614235569257289, 0.3624262398807332, 0.368403222062625, 0.3753424599999562, 0.3884230100084096, 0.37567142595071346, 0.3620737729361281, 0.36207374301739037, 0.36222367896698415, 0.36041269707493484, 0.36257877107709646, 0.3630188190145418, 0.362114483024925, 0.3620332330465317, 0.3621938119176775, 0.36413962801452726, 0.38132632488850504, 0.36211251199711114, 0.35889168025460094, 0.36085039691533893, 0.36102206795476377, 0.36056139692664146, 0.3598739851731807, 0.3638338939053938, 0.3610993321053684, 0.43954395793844014, 0.35797531995922327, 0.3619519539643079, 0.3660388808930293, 0.3804880640236661, 0.3907459101174027, 0.3754841798217967, 0.37481972703244537, 0.375848327181302, 0.37804631900507957, 0.37874079099856317, 0.37457832193467766, 0.37688539491500705, 0.3791178520768881, 0.46447021001949906, 0.3759179910412058, 0.37825432687532157, 0.3895256470423192, 0.3748272279044613, 0.3744646809063852, 0.3736355929868296, 0.377084800042212, 0.3757579190423712, 0.3746438630623743, 0.37363560986705124, 0.372343922033906, 0.37488092482089996, 0.37364008999429643, 0.37335517501924187, 0.37479882617481053, 0.3799884169129655, 0.3740142280003056, 0.3750604799715802, 0.37341895210556686, 0.37400380813051015, 0.37442802090663463, 0.37909160112030804, 0.3738373698433861, 0.36665824707597494, 0.3564791578100994, 0.3559657899895683, 0.3567651209887117, 0.35546060907654464, 0.3587199919857085, 0.3613212970085442, 0.3559026127913967, 0.35547461500391364, 0.35679357207845896, 0.35562594211660326, 0.3559393968898803, 0.35563182504847646, 0.355359049863182, 0.3561411000555381, 0.35638316918630153, 0.3560319640673697, 0.3553261240012944, 0.357397417887114, 0.3649169929558411, 0.3640331899514422]
Total Epoch List: [109]
Total Time List: [0.06989478797186166]
========================k_idx:1========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: GPSTransformer
GPS(
  (x_lin): Linear(in_features=14887, out_features=64, bias=True)
  (pe_lin): Linear(in_features=20, out_features=8, bias=True)
  (pe_norm): BatchNorm1d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (convs): ModuleList(
    (0-1): 2 x GPSConv(64, conv=GINEConv(nn=Sequential(
      (0): Linear(in_features=64, out_features=64, bias=True)
      (1): ReLU()
      (2): Linear(in_features=64, out_features=64, bias=True)
    )), heads=1, attn_type=multihead)
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=32, bias=True)
    (1): ReLU()
    (2): Linear(in_features=32, out_features=16, bias=True)
    (3): ReLU()
    (4): Linear(in_features=16, out_features=2, bias=True)
  )
)
Total number of parameters: 1039906
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x737bd9adaef0>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 1.1861;  Loss pred: 1.1861; Loss self: 0.0000; time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 1.7802 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 2.3154 score: 0.5102 time: 0.08s
Epoch 2/1000, LR 0.000000
Train loss: 1.1849;  Loss pred: 1.1849; Loss self: 0.0000; time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 1.4006 score: 0.4898 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 1.7258 score: 0.5102 time: 0.08s
Epoch 3/1000, LR 0.000030
Train loss: 1.0830;  Loss pred: 1.0830; Loss self: 0.0000; time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 1.1368 score: 0.4898 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 1.2969 score: 0.5102 time: 0.08s
Epoch 4/1000, LR 0.000060
Train loss: 0.9084;  Loss pred: 0.9084; Loss self: 0.0000; time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.9567 score: 0.4898 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.9867 score: 0.5102 time: 0.08s
Epoch 5/1000, LR 0.000090
Train loss: 0.7075;  Loss pred: 0.7075; Loss self: 0.0000; time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.8455 score: 0.4898 time: 0.06s
Test loss: 0.8016 score: 0.5918 time: 0.08s
Epoch 6/1000, LR 0.000120
Train loss: 0.5143;  Loss pred: 0.5143; Loss self: 0.0000; time: 0.22s
Val loss: 0.7860 score: 0.5306 time: 0.07s
Test loss: 0.7232 score: 0.5918 time: 0.08s
Epoch 7/1000, LR 0.000150
Train loss: 0.3611;  Loss pred: 0.3611; Loss self: 0.0000; time: 0.22s
Val loss: 0.7318 score: 0.5510 time: 0.07s
Test loss: 0.6675 score: 0.6939 time: 0.08s
Epoch 8/1000, LR 0.000180
Train loss: 0.2739;  Loss pred: 0.2739; Loss self: 0.0000; time: 0.22s
Val loss: 0.6996 score: 0.5918 time: 0.07s
Test loss: 0.6312 score: 0.7551 time: 0.08s
Epoch 9/1000, LR 0.000210
Train loss: 0.2183;  Loss pred: 0.2183; Loss self: 0.0000; time: 0.22s
Val loss: 0.6787 score: 0.6122 time: 0.06s
Test loss: 0.6044 score: 0.7755 time: 0.08s
Epoch 10/1000, LR 0.000240
Train loss: 0.1777;  Loss pred: 0.1777; Loss self: 0.0000; time: 0.22s
Val loss: 0.6608 score: 0.6327 time: 0.06s
Test loss: 0.5824 score: 0.7755 time: 0.08s
Epoch 11/1000, LR 0.000270
Train loss: 0.1478;  Loss pred: 0.1478; Loss self: 0.0000; time: 0.21s
Val loss: 0.6482 score: 0.6327 time: 0.06s
Test loss: 0.5642 score: 0.7551 time: 0.08s
Epoch 12/1000, LR 0.000270
Train loss: 0.1189;  Loss pred: 0.1189; Loss self: 0.0000; time: 0.21s
Val loss: 0.6418 score: 0.6327 time: 0.06s
Test loss: 0.5547 score: 0.7551 time: 0.08s
Epoch 13/1000, LR 0.000270
Train loss: 0.0985;  Loss pred: 0.0985; Loss self: 0.0000; time: 0.22s
Val loss: 0.6379 score: 0.6327 time: 0.06s
Test loss: 0.5481 score: 0.7551 time: 0.08s
Epoch 14/1000, LR 0.000270
Train loss: 0.0837;  Loss pred: 0.0837; Loss self: 0.0000; time: 0.22s
Val loss: 0.6342 score: 0.6122 time: 0.07s
Test loss: 0.5432 score: 0.7551 time: 0.09s
Epoch 15/1000, LR 0.000270
Train loss: 0.0704;  Loss pred: 0.0704; Loss self: 0.0000; time: 0.23s
Val loss: 0.6326 score: 0.6122 time: 0.06s
Test loss: 0.5401 score: 0.7551 time: 0.08s
Epoch 16/1000, LR 0.000270
Train loss: 0.0552;  Loss pred: 0.0552; Loss self: 0.0000; time: 0.22s
Val loss: 0.6331 score: 0.6122 time: 0.07s
Test loss: 0.5387 score: 0.7551 time: 0.08s
     INFO: Early stopping counter 1 of 20
Epoch 17/1000, LR 0.000270
Train loss: 0.0461;  Loss pred: 0.0461; Loss self: 0.0000; time: 0.22s
Val loss: 0.6332 score: 0.6122 time: 0.06s
Test loss: 0.5381 score: 0.7551 time: 0.08s
     INFO: Early stopping counter 2 of 20
Epoch 18/1000, LR 0.000270
Train loss: 0.0392;  Loss pred: 0.0392; Loss self: 0.0000; time: 0.21s
Val loss: 0.6343 score: 0.6122 time: 0.06s
Test loss: 0.5377 score: 0.7551 time: 0.08s
     INFO: Early stopping counter 3 of 20
Epoch 19/1000, LR 0.000270
Train loss: 0.0335;  Loss pred: 0.0335; Loss self: 0.0000; time: 0.22s
Val loss: 0.6311 score: 0.6122 time: 0.06s
Test loss: 0.5343 score: 0.7551 time: 0.08s
Epoch 20/1000, LR 0.000270
Train loss: 0.0314;  Loss pred: 0.0314; Loss self: 0.0000; time: 0.22s
Val loss: 0.6278 score: 0.6122 time: 0.06s
Test loss: 0.5301 score: 0.7755 time: 0.08s
Epoch 21/1000, LR 0.000270
Train loss: 0.0256;  Loss pred: 0.0256; Loss self: 0.0000; time: 0.22s
Val loss: 0.6268 score: 0.5918 time: 0.07s
Test loss: 0.5283 score: 0.7551 time: 0.08s
Epoch 22/1000, LR 0.000270
Train loss: 0.0222;  Loss pred: 0.0222; Loss self: 0.0000; time: 0.22s
Val loss: 0.6252 score: 0.5918 time: 0.06s
Test loss: 0.5260 score: 0.7551 time: 0.08s
Epoch 23/1000, LR 0.000270
Train loss: 0.0203;  Loss pred: 0.0203; Loss self: 0.0000; time: 0.21s
Val loss: 0.6215 score: 0.5918 time: 0.06s
Test loss: 0.5225 score: 0.7551 time: 0.08s
Epoch 24/1000, LR 0.000270
Train loss: 0.0171;  Loss pred: 0.0171; Loss self: 0.0000; time: 0.22s
Val loss: 0.6161 score: 0.6122 time: 0.06s
Test loss: 0.5174 score: 0.7551 time: 0.08s
Epoch 25/1000, LR 0.000270
Train loss: 0.0148;  Loss pred: 0.0148; Loss self: 0.0000; time: 0.21s
Val loss: 0.6100 score: 0.6122 time: 0.06s
Test loss: 0.5109 score: 0.7551 time: 0.08s
Epoch 26/1000, LR 0.000270
Train loss: 0.0138;  Loss pred: 0.0138; Loss self: 0.0000; time: 0.21s
Val loss: 0.6026 score: 0.6122 time: 0.06s
Test loss: 0.5038 score: 0.7551 time: 0.08s
Epoch 27/1000, LR 0.000270
Train loss: 0.0118;  Loss pred: 0.0118; Loss self: 0.0000; time: 0.21s
Val loss: 0.5954 score: 0.6122 time: 0.06s
Test loss: 0.4958 score: 0.7551 time: 0.08s
Epoch 28/1000, LR 0.000270
Train loss: 0.0101;  Loss pred: 0.0101; Loss self: 0.0000; time: 0.22s
Val loss: 0.5880 score: 0.6327 time: 0.06s
Test loss: 0.4872 score: 0.7551 time: 0.08s
Epoch 29/1000, LR 0.000270
Train loss: 0.0094;  Loss pred: 0.0094; Loss self: 0.0000; time: 0.21s
Val loss: 0.5810 score: 0.6327 time: 0.07s
Test loss: 0.4779 score: 0.7551 time: 0.08s
Epoch 30/1000, LR 0.000270
Train loss: 0.0082;  Loss pred: 0.0082; Loss self: 0.0000; time: 0.22s
Val loss: 0.5745 score: 0.6735 time: 0.06s
Test loss: 0.4692 score: 0.7755 time: 0.08s
Epoch 31/1000, LR 0.000270
Train loss: 0.0074;  Loss pred: 0.0074; Loss self: 0.0000; time: 0.22s
Val loss: 0.5682 score: 0.6939 time: 0.06s
Test loss: 0.4613 score: 0.7755 time: 0.08s
Epoch 32/1000, LR 0.000270
Train loss: 0.0077;  Loss pred: 0.0077; Loss self: 0.0000; time: 0.22s
Val loss: 0.5616 score: 0.6939 time: 0.07s
Test loss: 0.4534 score: 0.8163 time: 0.08s
Epoch 33/1000, LR 0.000270
Train loss: 0.0059;  Loss pred: 0.0059; Loss self: 0.0000; time: 0.22s
Val loss: 0.5551 score: 0.7143 time: 0.06s
Test loss: 0.4463 score: 0.8163 time: 0.08s
Epoch 34/1000, LR 0.000270
Train loss: 0.0063;  Loss pred: 0.0063; Loss self: 0.0000; time: 0.22s
Val loss: 0.5482 score: 0.7347 time: 0.06s
Test loss: 0.4397 score: 0.8163 time: 0.08s
Epoch 35/1000, LR 0.000270
Train loss: 0.0052;  Loss pred: 0.0052; Loss self: 0.0000; time: 0.22s
Val loss: 0.5416 score: 0.7143 time: 0.06s
Test loss: 0.4339 score: 0.8163 time: 0.08s
Epoch 36/1000, LR 0.000270
Train loss: 0.0045;  Loss pred: 0.0045; Loss self: 0.0000; time: 0.22s
Val loss: 0.5351 score: 0.7143 time: 0.07s
Test loss: 0.4282 score: 0.8163 time: 0.08s
Epoch 37/1000, LR 0.000270
Train loss: 0.0045;  Loss pred: 0.0045; Loss self: 0.0000; time: 0.22s
Val loss: 0.5290 score: 0.7143 time: 0.06s
Test loss: 0.4228 score: 0.8163 time: 0.08s
Epoch 38/1000, LR 0.000270
Train loss: 0.0038;  Loss pred: 0.0038; Loss self: 0.0000; time: 0.21s
Val loss: 0.5234 score: 0.7143 time: 0.06s
Test loss: 0.4177 score: 0.7959 time: 0.08s
Epoch 39/1000, LR 0.000269
Train loss: 0.0041;  Loss pred: 0.0041; Loss self: 0.0000; time: 0.21s
Val loss: 0.5182 score: 0.7143 time: 0.06s
Test loss: 0.4129 score: 0.7959 time: 0.08s
Epoch 40/1000, LR 0.000269
Train loss: 0.0043;  Loss pred: 0.0043; Loss self: 0.0000; time: 0.22s
Val loss: 0.5130 score: 0.7143 time: 0.06s
Test loss: 0.4083 score: 0.7959 time: 0.08s
Epoch 41/1000, LR 0.000269
Train loss: 0.0036;  Loss pred: 0.0036; Loss self: 0.0000; time: 0.22s
Val loss: 0.5082 score: 0.7143 time: 0.06s
Test loss: 0.4038 score: 0.7959 time: 0.08s
Epoch 42/1000, LR 0.000269
Train loss: 0.0036;  Loss pred: 0.0036; Loss self: 0.0000; time: 0.22s
Val loss: 0.5035 score: 0.7143 time: 0.06s
Test loss: 0.3992 score: 0.7959 time: 0.08s
Epoch 43/1000, LR 0.000269
Train loss: 0.0035;  Loss pred: 0.0035; Loss self: 0.0000; time: 0.22s
Val loss: 0.4988 score: 0.7347 time: 0.07s
Test loss: 0.3947 score: 0.8163 time: 0.08s
Epoch 44/1000, LR 0.000269
Train loss: 0.0032;  Loss pred: 0.0032; Loss self: 0.0000; time: 0.22s
Val loss: 0.4939 score: 0.7551 time: 0.06s
Test loss: 0.3901 score: 0.8163 time: 0.08s
Epoch 45/1000, LR 0.000269
Train loss: 0.0029;  Loss pred: 0.0029; Loss self: 0.0000; time: 0.22s
Val loss: 0.4888 score: 0.7551 time: 0.07s
Test loss: 0.3854 score: 0.8367 time: 0.08s
Epoch 46/1000, LR 0.000269
Train loss: 0.0030;  Loss pred: 0.0030; Loss self: 0.0000; time: 0.22s
Val loss: 0.4836 score: 0.7551 time: 0.06s
Test loss: 0.3807 score: 0.8571 time: 0.08s
Epoch 47/1000, LR 0.000269
Train loss: 0.0030;  Loss pred: 0.0030; Loss self: 0.0000; time: 0.22s
Val loss: 0.4784 score: 0.7755 time: 0.06s
Test loss: 0.3758 score: 0.8571 time: 0.08s
Epoch 48/1000, LR 0.000269
Train loss: 0.0028;  Loss pred: 0.0028; Loss self: 0.0000; time: 0.22s
Val loss: 0.4732 score: 0.7755 time: 0.07s
Test loss: 0.3706 score: 0.8571 time: 0.08s
Epoch 49/1000, LR 0.000269
Train loss: 0.0026;  Loss pred: 0.0026; Loss self: 0.0000; time: 0.22s
Val loss: 0.4682 score: 0.7755 time: 0.07s
Test loss: 0.3652 score: 0.8571 time: 0.08s
Epoch 50/1000, LR 0.000269
Train loss: 0.0025;  Loss pred: 0.0025; Loss self: 0.0000; time: 0.23s
Val loss: 0.4630 score: 0.7755 time: 0.06s
Test loss: 0.3596 score: 0.8571 time: 0.08s
Epoch 51/1000, LR 0.000269
Train loss: 0.0023;  Loss pred: 0.0023; Loss self: 0.0000; time: 0.22s
Val loss: 0.4577 score: 0.7755 time: 0.06s
Test loss: 0.3541 score: 0.8571 time: 0.08s
Epoch 52/1000, LR 0.000269
Train loss: 0.0022;  Loss pred: 0.0022; Loss self: 0.0000; time: 0.22s
Val loss: 0.4525 score: 0.7755 time: 0.06s
Test loss: 0.3484 score: 0.8776 time: 0.08s
Epoch 53/1000, LR 0.000269
Train loss: 0.0024;  Loss pred: 0.0024; Loss self: 0.0000; time: 0.22s
Val loss: 0.4476 score: 0.7755 time: 0.06s
Test loss: 0.3427 score: 0.8776 time: 0.08s
Epoch 54/1000, LR 0.000269
Train loss: 0.0019;  Loss pred: 0.0019; Loss self: 0.0000; time: 0.22s
Val loss: 0.4428 score: 0.7755 time: 0.06s
Test loss: 0.3369 score: 0.8776 time: 0.08s
Epoch 55/1000, LR 0.000269
Train loss: 0.0021;  Loss pred: 0.0021; Loss self: 0.0000; time: 0.22s
Val loss: 0.4380 score: 0.7755 time: 0.06s
Test loss: 0.3311 score: 0.8980 time: 0.08s
Epoch 56/1000, LR 0.000269
Train loss: 0.0019;  Loss pred: 0.0019; Loss self: 0.0000; time: 0.22s
Val loss: 0.4329 score: 0.7755 time: 0.06s
Test loss: 0.3251 score: 0.8980 time: 0.08s
Epoch 57/1000, LR 0.000269
Train loss: 0.0017;  Loss pred: 0.0017; Loss self: 0.0000; time: 0.22s
Val loss: 0.4273 score: 0.7959 time: 0.06s
Test loss: 0.3190 score: 0.8980 time: 0.08s
Epoch 58/1000, LR 0.000269
Train loss: 0.0018;  Loss pred: 0.0018; Loss self: 0.0000; time: 0.22s
Val loss: 0.4219 score: 0.7959 time: 0.06s
Test loss: 0.3127 score: 0.8980 time: 0.08s
Epoch 59/1000, LR 0.000268
Train loss: 0.0017;  Loss pred: 0.0017; Loss self: 0.0000; time: 0.22s
Val loss: 0.4169 score: 0.8163 time: 0.07s
Test loss: 0.3066 score: 0.8980 time: 0.08s
Epoch 60/1000, LR 0.000268
Train loss: 0.0017;  Loss pred: 0.0017; Loss self: 0.0000; time: 0.22s
Val loss: 0.4120 score: 0.8163 time: 0.06s
Test loss: 0.3003 score: 0.9184 time: 0.08s
Epoch 61/1000, LR 0.000268
Train loss: 0.0016;  Loss pred: 0.0016; Loss self: 0.0000; time: 0.22s
Val loss: 0.4072 score: 0.7959 time: 0.06s
Test loss: 0.2939 score: 0.9184 time: 0.08s
Epoch 62/1000, LR 0.000268
Train loss: 0.0016;  Loss pred: 0.0016; Loss self: 0.0000; time: 0.22s
Val loss: 0.4019 score: 0.7959 time: 0.06s
Test loss: 0.2877 score: 0.9184 time: 0.08s
Epoch 63/1000, LR 0.000268
Train loss: 0.0015;  Loss pred: 0.0015; Loss self: 0.0000; time: 0.22s
Val loss: 0.3971 score: 0.7959 time: 0.07s
Test loss: 0.2818 score: 0.9184 time: 0.08s
Epoch 64/1000, LR 0.000268
Train loss: 0.0016;  Loss pred: 0.0016; Loss self: 0.0000; time: 0.22s
Val loss: 0.3925 score: 0.7959 time: 0.06s
Test loss: 0.2757 score: 0.9184 time: 0.08s
Epoch 65/1000, LR 0.000268
Train loss: 0.0013;  Loss pred: 0.0013; Loss self: 0.0000; time: 0.22s
Val loss: 0.3880 score: 0.7959 time: 0.07s
Test loss: 0.2694 score: 0.9184 time: 0.08s
Epoch 66/1000, LR 0.000268
Train loss: 0.0014;  Loss pred: 0.0014; Loss self: 0.0000; time: 0.22s
Val loss: 0.3841 score: 0.7959 time: 0.06s
Test loss: 0.2635 score: 0.9184 time: 0.08s
Epoch 67/1000, LR 0.000268
Train loss: 0.0016;  Loss pred: 0.0016; Loss self: 0.0000; time: 0.23s
Val loss: 0.3800 score: 0.7959 time: 0.07s
Test loss: 0.2577 score: 0.9184 time: 0.08s
Epoch 68/1000, LR 0.000268
Train loss: 0.0014;  Loss pred: 0.0014; Loss self: 0.0000; time: 0.22s
Val loss: 0.3761 score: 0.7959 time: 0.06s
Test loss: 0.2522 score: 0.9184 time: 0.08s
Epoch 69/1000, LR 0.000268
Train loss: 0.0014;  Loss pred: 0.0014; Loss self: 0.0000; time: 0.22s
Val loss: 0.3725 score: 0.8163 time: 0.07s
Test loss: 0.2467 score: 0.9184 time: 0.08s
Epoch 70/1000, LR 0.000268
Train loss: 0.0014;  Loss pred: 0.0014; Loss self: 0.0000; time: 0.22s
Val loss: 0.3695 score: 0.8163 time: 0.07s
Test loss: 0.2412 score: 0.9184 time: 0.08s
Epoch 71/1000, LR 0.000268
Train loss: 0.0014;  Loss pred: 0.0014; Loss self: 0.0000; time: 0.22s
Val loss: 0.3671 score: 0.8163 time: 0.07s
Test loss: 0.2360 score: 0.9184 time: 0.08s
Epoch 72/1000, LR 0.000267
Train loss: 0.0014;  Loss pred: 0.0014; Loss self: 0.0000; time: 0.22s
Val loss: 0.3650 score: 0.8163 time: 0.07s
Test loss: 0.2310 score: 0.9184 time: 0.08s
Epoch 73/1000, LR 0.000267
Train loss: 0.0013;  Loss pred: 0.0013; Loss self: 0.0000; time: 0.22s
Val loss: 0.3634 score: 0.8163 time: 0.06s
Test loss: 0.2264 score: 0.9184 time: 0.08s
Epoch 74/1000, LR 0.000267
Train loss: 0.0012;  Loss pred: 0.0012; Loss self: 0.0000; time: 0.22s
Val loss: 0.3621 score: 0.8163 time: 0.07s
Test loss: 0.2220 score: 0.9184 time: 0.08s
Epoch 75/1000, LR 0.000267
Train loss: 0.0014;  Loss pred: 0.0014; Loss self: 0.0000; time: 0.22s
Val loss: 0.3606 score: 0.8163 time: 0.06s
Test loss: 0.2177 score: 0.8980 time: 0.08s
Epoch 76/1000, LR 0.000267
Train loss: 0.0012;  Loss pred: 0.0012; Loss self: 0.0000; time: 0.22s
Val loss: 0.3597 score: 0.8163 time: 0.07s
Test loss: 0.2141 score: 0.8980 time: 0.08s
Epoch 77/1000, LR 0.000267
Train loss: 0.0011;  Loss pred: 0.0011; Loss self: 0.0000; time: 0.22s
Val loss: 0.3591 score: 0.8163 time: 0.07s
Test loss: 0.2110 score: 0.8980 time: 0.08s
Epoch 78/1000, LR 0.000267
Train loss: 0.0011;  Loss pred: 0.0011; Loss self: 0.0000; time: 0.22s
Val loss: 0.3590 score: 0.8163 time: 0.07s
Test loss: 0.2083 score: 0.8980 time: 0.08s
Epoch 79/1000, LR 0.000267
Train loss: 0.0012;  Loss pred: 0.0012; Loss self: 0.0000; time: 0.22s
Val loss: 0.3598 score: 0.8163 time: 0.06s
Test loss: 0.2061 score: 0.8980 time: 0.08s
     INFO: Early stopping counter 1 of 20
Epoch 80/1000, LR 0.000267
Train loss: 0.0010;  Loss pred: 0.0010; Loss self: 0.0000; time: 0.22s
Val loss: 0.3612 score: 0.8163 time: 0.06s
Test loss: 0.2042 score: 0.8980 time: 0.08s
     INFO: Early stopping counter 2 of 20
Epoch 81/1000, LR 0.000267
Train loss: 0.0010;  Loss pred: 0.0010; Loss self: 0.0000; time: 0.22s
Val loss: 0.3634 score: 0.8163 time: 0.07s
Test loss: 0.2026 score: 0.8980 time: 0.08s
     INFO: Early stopping counter 3 of 20
Epoch 82/1000, LR 0.000267
Train loss: 0.0011;  Loss pred: 0.0011; Loss self: 0.0000; time: 0.22s
Val loss: 0.3663 score: 0.8163 time: 0.06s
Test loss: 0.2017 score: 0.8980 time: 0.08s
     INFO: Early stopping counter 4 of 20
Epoch 83/1000, LR 0.000266
Train loss: 0.0011;  Loss pred: 0.0011; Loss self: 0.0000; time: 0.22s
Val loss: 0.3694 score: 0.8163 time: 0.06s
Test loss: 0.2014 score: 0.8980 time: 0.08s
     INFO: Early stopping counter 5 of 20
Epoch 84/1000, LR 0.000266
Train loss: 0.0010;  Loss pred: 0.0010; Loss self: 0.0000; time: 0.22s
Val loss: 0.3731 score: 0.8163 time: 0.07s
Test loss: 0.2020 score: 0.8980 time: 0.08s
     INFO: Early stopping counter 6 of 20
Epoch 85/1000, LR 0.000266
Train loss: 0.0010;  Loss pred: 0.0010; Loss self: 0.0000; time: 0.22s
Val loss: 0.3775 score: 0.8163 time: 0.06s
Test loss: 0.2031 score: 0.8980 time: 0.08s
     INFO: Early stopping counter 7 of 20
Epoch 86/1000, LR 0.000266
Train loss: 0.0010;  Loss pred: 0.0010; Loss self: 0.0000; time: 0.22s
Val loss: 0.3826 score: 0.7959 time: 0.06s
Test loss: 0.2047 score: 0.8776 time: 0.08s
     INFO: Early stopping counter 8 of 20
Epoch 87/1000, LR 0.000266
Train loss: 0.0011;  Loss pred: 0.0011; Loss self: 0.0000; time: 0.22s
Val loss: 0.3884 score: 0.7959 time: 0.07s
Test loss: 0.2070 score: 0.8776 time: 0.08s
     INFO: Early stopping counter 9 of 20
Epoch 88/1000, LR 0.000266
Train loss: 0.0010;  Loss pred: 0.0010; Loss self: 0.0000; time: 0.22s
Val loss: 0.3948 score: 0.7959 time: 0.06s
Test loss: 0.2099 score: 0.8776 time: 0.08s
     INFO: Early stopping counter 10 of 20
Epoch 89/1000, LR 0.000266
Train loss: 0.0009;  Loss pred: 0.0009; Loss self: 0.0000; time: 0.22s
Val loss: 0.4024 score: 0.7959 time: 0.07s
Test loss: 0.2136 score: 0.8776 time: 0.08s
     INFO: Early stopping counter 11 of 20
Epoch 90/1000, LR 0.000266
Train loss: 0.0009;  Loss pred: 0.0009; Loss self: 0.0000; time: 0.22s
Val loss: 0.4110 score: 0.7959 time: 0.07s
Test loss: 0.2180 score: 0.8776 time: 0.08s
     INFO: Early stopping counter 12 of 20
Epoch 91/1000, LR 0.000266
Train loss: 0.0009;  Loss pred: 0.0009; Loss self: 0.0000; time: 0.22s
Val loss: 0.4204 score: 0.7959 time: 0.07s
Test loss: 0.2227 score: 0.8776 time: 0.08s
     INFO: Early stopping counter 13 of 20
Epoch 92/1000, LR 0.000266
Train loss: 0.0009;  Loss pred: 0.0009; Loss self: 0.0000; time: 0.22s
Val loss: 0.4310 score: 0.7959 time: 0.07s
Test loss: 0.2282 score: 0.8776 time: 0.08s
     INFO: Early stopping counter 14 of 20
Epoch 93/1000, LR 0.000265
Train loss: 0.0009;  Loss pred: 0.0009; Loss self: 0.0000; time: 0.22s
Val loss: 0.4427 score: 0.7959 time: 0.06s
Test loss: 0.2341 score: 0.8776 time: 0.08s
     INFO: Early stopping counter 15 of 20
Epoch 94/1000, LR 0.000265
Train loss: 0.0008;  Loss pred: 0.0008; Loss self: 0.0000; time: 0.22s
Val loss: 0.4561 score: 0.7959 time: 0.06s
Test loss: 0.2416 score: 0.8776 time: 0.08s
     INFO: Early stopping counter 16 of 20
Epoch 95/1000, LR 0.000265
Train loss: 0.0009;  Loss pred: 0.0009; Loss self: 0.0000; time: 0.22s
Val loss: 0.4703 score: 0.7959 time: 0.07s
Test loss: 0.2500 score: 0.8776 time: 0.08s
     INFO: Early stopping counter 17 of 20
Epoch 96/1000, LR 0.000265
Train loss: 0.0009;  Loss pred: 0.0009; Loss self: 0.0000; time: 0.22s
Val loss: 0.4846 score: 0.7959 time: 0.07s
Test loss: 0.2591 score: 0.8571 time: 0.08s
     INFO: Early stopping counter 18 of 20
Epoch 97/1000, LR 0.000265
Train loss: 0.0009;  Loss pred: 0.0009; Loss self: 0.0000; time: 0.31s
Val loss: 0.4991 score: 0.7959 time: 0.07s
Test loss: 0.2695 score: 0.8571 time: 0.08s
     INFO: Early stopping counter 19 of 20
Epoch 98/1000, LR 0.000265
Train loss: 0.0008;  Loss pred: 0.0008; Loss self: 0.0000; time: 0.22s
Val loss: 0.5145 score: 0.7959 time: 0.07s
Test loss: 0.2810 score: 0.8571 time: 0.08s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 077,   Train_Loss: 0.0011,   Val_Loss: 0.3590,   Val_Precision: 1.0000,   Val_Recall: 0.6400,   Val_accuracy: 0.7805,   Val_Score: 0.8163,   Val_Loss: 0.3590,   Test_Precision: 1.0000,   Test_Recall: 0.7917,   Test_accuracy: 0.8837,   Test_Score: 0.8980,   Test_loss: 0.2083


[0.07140269293449819, 0.07025647105183452, 0.06996766501106322, 0.07019000092986971, 0.06995220202952623, 0.07005454506725073, 0.07077746093273163, 0.07029995403718203, 0.07004583498928696, 0.0703566949814558, 0.07014449697453529, 0.07387414004188031, 0.07123668200802058, 0.07408993097487837, 0.07081565505359322, 0.0705169040011242, 0.07046438695397228, 0.07043113105464727, 0.0699055859586224, 0.06882533896714449, 0.06925370404496789, 0.06909177300985903, 0.06914540904108435, 0.0689691980369389, 0.06894625490531325, 0.06829987303353846, 0.06975654000416398, 0.06906155892647803, 0.06937944504898041, 0.08016833302099258, 0.07170937792398036, 0.06897341297008097, 0.06905112590175122, 0.06871384603437036, 0.06867225095629692, 0.06861502793617547, 0.06934155500493944, 0.06899598299060017, 0.06969069305341691, 0.06978478899691254, 0.07093609706498682, 0.06988592993002385, 0.06969245697837323, 0.06965289206709713, 0.06922693504020572, 0.06887985300272703, 0.06885823095217347, 0.06951714202295989, 0.0690673339413479, 0.06829411606304348, 0.07016174797900021, 0.06922532001044601, 0.06981348397675902, 0.07100505603011698, 0.07897043297998607, 0.07289586903061718, 0.07353302196133882, 0.07357235392555594, 0.07318592094816267, 0.07318616309203207, 0.0728717049350962, 0.07316883199382573, 0.07307384698651731, 0.07358125993050635, 0.07363991299644113, 0.0727311500813812, 0.07411551405675709, 0.0740579100092873, 0.07290751207619905, 0.07302245392929763, 0.07290364999789745, 0.07331615604925901, 0.07277754601091146, 0.07269923505373299, 0.07256347103975713, 0.07249310705810785, 0.07273959601297975, 0.07268922601360828, 0.07280515797901899, 0.07267696899361908, 0.07533581799361855, 0.07276648993138224, 0.07312183303292841, 0.07255846308544278, 0.07249504700303078, 0.07267009397037327, 0.07272250892128795, 0.07309056096710265, 0.06877232401166111, 0.06895384006202221, 0.06915609107818455, 0.06868598493747413, 0.06892176100518554, 0.07048276998102665, 0.0694201800506562, 0.06912049592938274, 0.06933541398029774, 0.06938951299525797, 0.06920018105302006, 0.06952238595113158, 0.06895315402653068, 0.06901283795014024, 0.06897767609916627, 0.06909350794740021, 0.06912156892940402, 0.06891590903978795, 0.06949843396432698, 0.0716112699592486, 0.06935271294787526, 0.0804011799627915, 0.08020252001006156, 0.08034412295091897, 0.08082830696366727, 0.08039718098007143, 0.080326052964665, 0.08101819397415966, 0.07984965504147112, 0.08058651303872466, 0.0798853220185265, 0.08087092905770987, 0.08024426898919046, 0.08147797302808613, 0.09898241399787366, 0.08046855498105288, 0.08042321598622948, 0.08058667508885264, 0.08049116004258394, 0.08016458991914988, 0.08093761699274182, 0.08386904292274266, 0.08091785793658346, 0.08058245002757758, 0.0806174980243668, 0.08047873002942652, 0.08024490205571055, 0.08050495793577284, 0.0804690340301022, 0.08059657597914338, 0.0801543949637562, 0.08066889108158648, 0.08111020002979785, 0.08058180310763419, 0.08018452697433531, 0.08648654294665903, 0.08043069904670119, 0.08053053810726851, 0.08029400801751763, 0.08025904407259077, 0.07987328001763672, 0.08027240203227848, 0.08064647810533643, 0.08085375500377268, 0.08034844393841922, 0.08075202698819339, 0.08065754198469222, 0.08041641104500741, 0.08057390502654016, 0.08708506193943322, 0.08056088304147124, 0.08035301696509123, 0.08062325895298272, 0.08056756004225463, 0.08045023097656667, 0.08073942502960563, 0.08047512790653855, 0.08045254496391863, 0.08048100909218192, 0.08064238098450005, 0.0805498689878732, 0.0804634930100292, 0.08096749288961291, 0.08408744097687304, 0.08064595703035593, 0.08027269295416772, 0.08063382492400706, 0.08062547200825065, 0.08074485696852207, 0.08046643203124404, 0.08090738905593753, 0.0808422570116818, 0.08077456196770072, 0.08075691794510931, 0.08034848410170525, 0.08057588001247495, 0.08166250900831074, 0.08079165907111019, 0.08033275709021837, 0.08059451100416481, 0.0803884610068053, 0.0802699790801853, 0.08072176203131676, 0.08064250100869685, 0.0808276169700548, 0.08040932205040008, 0.0811337810009718, 0.08046957000624388, 0.08040177298244089, 0.08086552296299487, 0.08193965896498412, 0.08044840791262686, 0.08047867100685835, 0.08068947494029999, 0.08072613598778844, 0.0815874149557203, 0.08702239801641554, 0.08030709798913449, 0.08083348895888776]
[0.001457197814989759, 0.0014338055316700923, 0.0014279115308380248, 0.0014324489985687696, 0.0014275959597862496, 0.0014296845932091986, 0.0014444379782190128, 0.0014346929395343273, 0.0014295068365160603, 0.001435850917988894, 0.0014315203464190876, 0.0015076355110587819, 0.001453809836898379, 0.001512039407650579, 0.001445217450073331, 0.0014391204898188614, 0.001438048713346373, 0.0014373700215234136, 0.001426644611400457, 0.0014045987544315203, 0.0014133408988768958, 0.001410036183874674, 0.0014111307967568235, 0.0014075346538150798, 0.001407066426639046, 0.0013938749598681318, 0.0014236028572278364, 0.001409419569928123, 0.001415907041815927, 0.0016360884289998486, 0.0014634566923261297, 0.0014076206728587954, 0.0014092066510561474, 0.001402323388456538, 0.001401474509312182, 0.0014003066925750095, 0.0014151337756110089, 0.0014080812855224525, 0.0014222590419064676, 0.0014241793672839294, 0.0014476754503058536, 0.0014262434679596704, 0.0014222950403749639, 0.0014214875932060638, 0.0014127945926572596, 0.0014057112857699394, 0.0014052700194321116, 0.0014187171841420385, 0.0014095374273744468, 0.0013937574706743567, 0.0014318724077346983, 0.001412761632866245, 0.001424764979117531, 0.0014490827761248362, 0.001611641489387471, 0.0014876707965432077, 0.0015006739175783433, 0.0015014766107256316, 0.0014935902234318914, 0.0014935951651435118, 0.0014871776517366572, 0.0014932414692617497, 0.001491302999724843, 0.001501658365928701, 0.0015028553672743086, 0.0014843091853343102, 0.0015125615113623897, 0.0015113859185568836, 0.001487908409718348, 0.0014902541618224006, 0.0014878295917938255, 0.001496248082637939, 0.0014852560410390095, 0.001483657858239449, 0.0014808871640766762, 0.0014794511644511807, 0.001484481551285301, 0.0014834535921144547, 0.0014858195505922241, 0.0014832034488493691, 0.001537465673339154, 0.001485030406762903, 0.0014922823067944572, 0.0014807849609274038, 0.0014794907551638934, 0.0014830631422525157, 0.0014841328351283256, 0.0014916441013694418, 0.0014035168165645125, 0.0014072212257555553, 0.0014113487975139702, 0.0014017547946423292, 0.0014065665511262355, 0.0014384238771638091, 0.0014167383683807387, 0.0014106223659057701, 0.001415008448577505, 0.0014161125101073055, 0.0014122485929187766, 0.001418824203084318, 0.0014072072250312384, 0.0014084252642885763, 0.0014077076754931892, 0.0014100715907632696, 0.0014106442638653883, 0.0014064471232609786, 0.0014183353870270811, 0.0014614544889642572, 0.0014153614887321482, 0.0016408404074039083, 0.0016367861226543176, 0.001639675978590183, 0.0016495572849728015, 0.0016407587955116617, 0.00163930720336051, 0.001653432530084891, 0.0016295847967647168, 0.0016446227150760135, 0.0016303126942556426, 0.001650427123626732, 0.00163763814263654, 0.0016628157760833903, 0.002020049265262728, 0.0016422154077765892, 0.0016412901221679486, 0.0016446260222214826, 0.0016426767355629376, 0.0016360120391663239, 0.0016517881018926902, 0.0017116131208722992, 0.001651384855848642, 0.001644539796481175, 0.0016452550617217713, 0.0016424230618250308, 0.00163765106236144, 0.0016429583252198538, 0.0016422251842877998, 0.0016448280812070078, 0.0016358039788521674, 0.0016463038996242139, 0.001655310204689752, 0.0016445265940333508, 0.0016364189178435778, 0.001765031488707327, 0.0016414428376877795, 0.0016434803695360922, 0.0016386532248472984, 0.001637939674950832, 0.0016300669391354431, 0.0016382122863730301, 0.0016458464919456414, 0.0016500766327300547, 0.0016397641620085556, 0.001648000550779457, 0.001646072285401882, 0.0016411512458164776, 0.0016443654087049011, 0.0017772461620292493, 0.0016440996539075763, 0.0016398574890834944, 0.001645372631693525, 0.0016442359192296863, 0.0016418414485013606, 0.0016477433679511352, 0.0016423495491130315, 0.0016418886727330331, 0.001642469573309835, 0.0016457628772346949, 0.0016438748773035345, 0.0016421121022454938, 0.001652397814073733, 0.0017160702240178172, 0.001645835857762366, 0.0016382182235544433, 0.001645588263755246, 0.001645417796086748, 0.0016478542238473892, 0.0016421720822702866, 0.0016511712052232148, 0.0016498419798302408, 0.001648460448320423, 0.0016481003662267206, 0.001639764981667454, 0.0016444057145403052, 0.0016665818164961375, 0.0016488093687981671, 0.0016394440222493544, 0.0016447859388605064, 0.0016405808368735776, 0.0016381628383711285, 0.0016473828985983012, 0.0016457653267080991, 0.0016495432034705061, 0.0016410065724571444, 0.0016557914489994244, 0.0016422361225764059, 0.0016408525098457324, 0.0016503167951631608, 0.0016722379380609005, 0.001641804243114834, 0.0016424218572828235, 0.0016467239783734692, 0.0016474721630160905, 0.0016650492848106185, 0.00177596730645746, 0.0016389203671251936, 0.0016496630399773012]
[686.2486271344209, 697.4446519502565, 700.3234993228967, 698.1051339343664, 700.4783063057474, 699.454974020046, 692.3107915183708, 697.0132579899501, 699.541950031636, 696.4511339384991, 698.5580068781245, 663.2902930879628, 687.8478702093826, 661.3584242184596, 691.9373966521506, 694.8688501585212, 695.3867353164808, 695.7150803382821, 700.9454155638355, 711.9470929651561, 707.543382346499, 709.2016583943788, 708.6515313097, 710.4620815477122, 710.698500843791, 717.4244668937919, 702.4430970497539, 709.5119305396033, 706.2610541985027, 611.2139064581653, 683.3136950643368, 710.4186655408082, 709.6191316231283, 713.1022760025747, 713.5342051214201, 714.1292727531784, 706.6469737592352, 710.1862728251242, 703.1067973802787, 702.158746975188, 690.7625599292492, 701.1425625882529, 703.0890016577482, 703.4883770913339, 707.81697863038, 711.3836319897493, 711.6070122979734, 704.8621185234636, 709.4526052158158, 717.4849434286148, 698.3862490807094, 707.8334920316145, 701.870143256454, 690.0917024728003, 620.4853911896158, 672.1917256987414, 666.367282249906, 666.0110406360052, 669.5276818980869, 669.5254666976076, 672.4146229821611, 669.6840535070289, 670.5545420243288, 665.9304291103188, 665.4000256948711, 673.7140818641303, 661.1301375104297, 661.6443806455666, 672.0843792994583, 671.0264769716334, 672.119983038067, 668.3383668816232, 673.284586878671, 674.0098429341579, 675.2708945407692, 675.9263327025475, 673.6358556522142, 674.1026516202913, 673.0292380399866, 674.2163394885403, 650.4210255492364, 673.3868851748422, 670.1144920414426, 675.3175014511952, 675.9082451239941, 674.280124365557, 673.7941350873319, 670.4012029960261, 712.4959161143297, 710.6203215937758, 708.5420710751705, 713.3915316873657, 710.9510738751051, 695.205367399584, 705.8466279436973, 708.9069506975336, 706.7095613494681, 706.1585805242447, 708.0906329198328, 704.8089522480269, 710.6273917672652, 710.0128245037695, 710.3747584878735, 709.1838503452874, 708.8959460692394, 711.0114439861819, 705.0518580771378, 684.2498398350447, 706.5332835188129, 609.4437920273867, 610.9533714633013, 609.8765933375533, 606.223263120255, 609.4741059658044, 610.0137899412889, 604.8024227203621, 613.6532458975697, 608.042191581782, 613.3792636979824, 605.9037601142602, 610.6355085196263, 601.3895311694769, 495.0374316093424, 608.9335146075077, 609.2768039565846, 608.0409688819393, 608.7625022931275, 611.2424456910343, 605.4045303112165, 584.2441774986886, 605.5523619817276, 608.0728494012136, 607.808493203171, 608.8565262161005, 610.6306911058527, 608.658165365323, 608.9298894984855, 607.9662740595844, 611.3201905167716, 607.4212666496511, 604.116374783919, 608.0777310796836, 611.0904665644962, 566.5621301364881, 609.2201184469216, 608.4648277741654, 610.2572434708919, 610.5230951378097, 613.4717390994882, 610.4214992880931, 607.5900789616458, 606.0324594412917, 609.843795326698, 606.795913707085, 607.5067351953222, 609.3283617516294, 608.1373365714363, 562.6682568599309, 608.2356368260724, 609.8090880805096, 607.7650622951803, 608.1852295675997, 609.0722102994656, 606.8906235340743, 608.8837790591294, 609.054692079356, 608.8392846053412, 607.6209482135466, 608.3188044337721, 608.9718227108597, 605.1811443242312, 582.7267357734992, 607.5940047627672, 610.4192870167805, 607.6854229125283, 607.7483800031047, 606.8497962551643, 608.9495801301834, 605.6307164494273, 606.1186539227801, 606.6266260854946, 606.7591637574063, 609.8434904879564, 608.1224305885794, 600.0305476165728, 606.4982519652406, 609.9628815798037, 607.9818512388252, 609.5402174181677, 610.4399248821492, 607.023419297884, 607.6200438614326, 606.2284382100938, 609.3820809642829, 603.9407925462403, 608.9258336561005, 609.4392969505935, 605.9442665377067, 598.0010243994246, 609.0860126556855, 608.8569727477762, 607.2663136828417, 606.9905291566575, 600.5828230566397, 563.0734284150254, 610.1577721888253, 606.1843999449486]
Elapsed: 0.07570715798548724~0.005499595050437473
Time per graph: 0.001545044040520148~0.0001122366336823974
Speed: 650.6094808940566~46.647944035454046
Total Time: 0.0815
best val loss: 0.3589767813682556 test_score: 0.8980

Testing...
Test loss: 0.3066 score: 0.8980 time: 0.08s
test Score 0.8980
Epoch Time List: [0.3768184391083196, 0.3662411590339616, 0.36470814992208034, 0.36547916010022163, 0.3644243380986154, 0.3653993421467021, 0.3671741030411795, 0.3645067849429324, 0.364803500007838, 0.3645692120771855, 0.36522559297736734, 0.3947337060235441, 0.36995676904916763, 0.3746724030934274, 0.36942692508455366, 0.3669546360615641, 0.3952438960550353, 0.36785596900153905, 0.3729710760526359, 0.36117926705628633, 0.36155636806506664, 0.3621771589387208, 0.360922793042846, 0.3618941028835252, 0.3624093937687576, 0.3614235569257289, 0.3624262398807332, 0.368403222062625, 0.3753424599999562, 0.3884230100084096, 0.37567142595071346, 0.3620737729361281, 0.36207374301739037, 0.36222367896698415, 0.36041269707493484, 0.36257877107709646, 0.3630188190145418, 0.362114483024925, 0.3620332330465317, 0.3621938119176775, 0.36413962801452726, 0.38132632488850504, 0.36211251199711114, 0.35889168025460094, 0.36085039691533893, 0.36102206795476377, 0.36056139692664146, 0.3598739851731807, 0.3638338939053938, 0.3610993321053684, 0.43954395793844014, 0.35797531995922327, 0.3619519539643079, 0.3660388808930293, 0.3804880640236661, 0.3907459101174027, 0.3754841798217967, 0.37481972703244537, 0.375848327181302, 0.37804631900507957, 0.37874079099856317, 0.37457832193467766, 0.37688539491500705, 0.3791178520768881, 0.46447021001949906, 0.3759179910412058, 0.37825432687532157, 0.3895256470423192, 0.3748272279044613, 0.3744646809063852, 0.3736355929868296, 0.377084800042212, 0.3757579190423712, 0.3746438630623743, 0.37363560986705124, 0.372343922033906, 0.37488092482089996, 0.37364008999429643, 0.37335517501924187, 0.37479882617481053, 0.3799884169129655, 0.3740142280003056, 0.3750604799715802, 0.37341895210556686, 0.37400380813051015, 0.37442802090663463, 0.37909160112030804, 0.3738373698433861, 0.36665824707597494, 0.3564791578100994, 0.3559657899895683, 0.3567651209887117, 0.35546060907654464, 0.3587199919857085, 0.3613212970085442, 0.3559026127913967, 0.35547461500391364, 0.35679357207845896, 0.35562594211660326, 0.3559393968898803, 0.35563182504847646, 0.355359049863182, 0.3561411000555381, 0.35638316918630153, 0.3560319640673697, 0.3553261240012944, 0.357397417887114, 0.3649169929558411, 0.3640331899514422, 0.35865470906719565, 0.3557849880307913, 0.35653063387144357, 0.3578461529687047, 0.35693567199632525, 0.3576572489691898, 0.36108792503364384, 0.36485599796287715, 0.3568174581741914, 0.35719852708280087, 0.3554939990863204, 0.3555919600185007, 0.3577415399486199, 0.37932725495193154, 0.37396188208367676, 0.3568311710841954, 0.35615878796670586, 0.3553772869054228, 0.356663974118419, 0.3569512100657448, 0.3623329170513898, 0.3567134770564735, 0.35588900197762996, 0.3560708040604368, 0.3558388688834384, 0.35539846401661634, 0.35506802692543715, 0.35785572801250964, 0.3551938469754532, 0.3558346569770947, 0.3567281310679391, 0.3573062620125711, 0.35633734217844903, 0.35719243297353387, 0.36495583003852516, 0.35629133984912187, 0.35603622486814857, 0.35532194084953517, 0.3549368919339031, 0.35552274202927947, 0.35622274910565466, 0.35601942089851946, 0.35744348797015846, 0.35598683916032314, 0.35718875692691654, 0.35873309697490185, 0.35605696495622396, 0.35711750702466816, 0.36778680805582553, 0.3746301210485399, 0.35608541895635426, 0.357910995837301, 0.35708844696637243, 0.3562629130901769, 0.35853427497204393, 0.3558561661047861, 0.35665254306513816, 0.35672815900761634, 0.35783749306574464, 0.3585844950284809, 0.35676082398276776, 0.35663532407488674, 0.3674978780327365, 0.35698872501961887, 0.3575576690491289, 0.3563950009411201, 0.3671488960972056, 0.3581526599591598, 0.35789208207279444, 0.35904146102257073, 0.3650560190435499, 0.3583679710282013, 0.3588467559311539, 0.3577733850106597, 0.35803399002179503, 0.3596492388751358, 0.3630214720033109, 0.3584089271025732, 0.3590178390732035, 0.3580057519720867, 0.35742294089868665, 0.3590500259306282, 0.3575695900944993, 0.359393117018044, 0.3581431108759716, 0.361665018950589, 0.3581663209479302, 0.3579720639390871, 0.3581961760064587, 0.35981893294956535, 0.36249827698338777, 0.35711683914996684, 0.3583982839481905, 0.36124748713336885, 0.3647438760381192, 0.3627037530532107, 0.44767773908097297, 0.3577309800311923]
Total Epoch List: [109, 98]
Total Time List: [0.06989478797186166, 0.08152352599427104]
========================k_idx:2========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: GPSTransformer
GPS(
  (x_lin): Linear(in_features=14887, out_features=64, bias=True)
  (pe_lin): Linear(in_features=20, out_features=8, bias=True)
  (pe_norm): BatchNorm1d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (convs): ModuleList(
    (0-1): 2 x GPSConv(64, conv=GINEConv(nn=Sequential(
      (0): Linear(in_features=64, out_features=64, bias=True)
      (1): ReLU()
      (2): Linear(in_features=64, out_features=64, bias=True)
    )), heads=1, attn_type=multihead)
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=32, bias=True)
    (1): ReLU()
    (2): Linear(in_features=32, out_features=16, bias=True)
    (3): ReLU()
    (4): Linear(in_features=16, out_features=2, bias=True)
  )
)
Total number of parameters: 1039906
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x737bd9adafe0>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 1.3600;  Loss pred: 1.3600; Loss self: 0.0000; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 5.8497 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 6.7103 score: 0.5000 time: 0.08s
Epoch 2/1000, LR 0.000000
Train loss: 1.3808;  Loss pred: 1.3808; Loss self: 0.0000; time: 0.31s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 4.4768 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 5.1443 score: 0.5000 time: 0.08s
Epoch 3/1000, LR 0.000030
Train loss: 1.2390;  Loss pred: 1.2390; Loss self: 0.0000; time: 0.23s
Val loss: 3.3943 score: 0.4694 time: 0.07s
Test loss: 3.8461 score: 0.4792 time: 0.08s
Epoch 4/1000, LR 0.000060
Train loss: 1.0793;  Loss pred: 1.0793; Loss self: 0.0000; time: 0.24s
Val loss: 2.6417 score: 0.4490 time: 0.07s
Test loss: 2.9058 score: 0.4583 time: 0.08s
Epoch 5/1000, LR 0.000090
Train loss: 0.8929;  Loss pred: 0.8929; Loss self: 0.0000; time: 0.23s
Val loss: 2.1586 score: 0.4286 time: 0.07s
Test loss: 2.2810 score: 0.4167 time: 0.08s
Epoch 6/1000, LR 0.000120
Train loss: 0.7157;  Loss pred: 0.7157; Loss self: 0.0000; time: 0.25s
Val loss: 1.8230 score: 0.4082 time: 0.09s
Test loss: 1.8263 score: 0.3750 time: 0.08s
Epoch 7/1000, LR 0.000150
Train loss: 0.5744;  Loss pred: 0.5744; Loss self: 0.0000; time: 0.23s
Val loss: 1.5522 score: 0.3469 time: 0.07s
Test loss: 1.5253 score: 0.3750 time: 0.08s
Epoch 8/1000, LR 0.000180
Train loss: 0.4719;  Loss pred: 0.4719; Loss self: 0.0000; time: 0.23s
Val loss: 1.3470 score: 0.3469 time: 0.07s
Test loss: 1.3307 score: 0.3750 time: 0.08s
Epoch 9/1000, LR 0.000210
Train loss: 0.4020;  Loss pred: 0.4020; Loss self: 0.0000; time: 0.24s
Val loss: 1.1856 score: 0.3878 time: 0.07s
Test loss: 1.1812 score: 0.3333 time: 0.08s
Epoch 10/1000, LR 0.000240
Train loss: 0.3450;  Loss pred: 0.3450; Loss self: 0.0000; time: 0.23s
Val loss: 1.0752 score: 0.4082 time: 0.07s
Test loss: 1.0716 score: 0.3125 time: 0.08s
Epoch 11/1000, LR 0.000270
Train loss: 0.3060;  Loss pred: 0.3060; Loss self: 0.0000; time: 0.23s
Val loss: 1.0151 score: 0.4490 time: 0.07s
Test loss: 1.0066 score: 0.3542 time: 0.08s
Epoch 12/1000, LR 0.000270
Train loss: 0.2713;  Loss pred: 0.2713; Loss self: 0.0000; time: 0.24s
Val loss: 0.9753 score: 0.4082 time: 0.07s
Test loss: 0.9567 score: 0.3750 time: 0.09s
Epoch 13/1000, LR 0.000270
Train loss: 0.2417;  Loss pred: 0.2417; Loss self: 0.0000; time: 0.23s
Val loss: 0.9489 score: 0.4082 time: 0.07s
Test loss: 0.9085 score: 0.3750 time: 0.08s
Epoch 14/1000, LR 0.000270
Train loss: 0.2050;  Loss pred: 0.2050; Loss self: 0.0000; time: 0.23s
Val loss: 0.9298 score: 0.4082 time: 0.07s
Test loss: 0.8675 score: 0.4167 time: 0.08s
Epoch 15/1000, LR 0.000270
Train loss: 0.1778;  Loss pred: 0.1778; Loss self: 0.0000; time: 0.24s
Val loss: 0.9177 score: 0.4082 time: 0.07s
Test loss: 0.8301 score: 0.4375 time: 0.08s
Epoch 16/1000, LR 0.000270
Train loss: 0.1556;  Loss pred: 0.1556; Loss self: 0.0000; time: 0.24s
Val loss: 0.9158 score: 0.4490 time: 0.07s
Test loss: 0.8035 score: 0.4583 time: 0.08s
Epoch 17/1000, LR 0.000270
Train loss: 0.1473;  Loss pred: 0.1473; Loss self: 0.0000; time: 0.24s
Val loss: 0.9195 score: 0.4490 time: 0.07s
Test loss: 0.7843 score: 0.5000 time: 0.08s
     INFO: Early stopping counter 1 of 20
Epoch 18/1000, LR 0.000270
Train loss: 0.1382;  Loss pred: 0.1382; Loss self: 0.0000; time: 0.24s
Val loss: 0.9259 score: 0.4490 time: 0.07s
Test loss: 0.7702 score: 0.5000 time: 0.08s
     INFO: Early stopping counter 2 of 20
Epoch 19/1000, LR 0.000270
Train loss: 0.1291;  Loss pred: 0.1291; Loss self: 0.0000; time: 0.24s
Val loss: 0.9288 score: 0.4694 time: 0.07s
Test loss: 0.7630 score: 0.4792 time: 0.08s
     INFO: Early stopping counter 3 of 20
Epoch 20/1000, LR 0.000270
Train loss: 0.1191;  Loss pred: 0.1191; Loss self: 0.0000; time: 0.24s
Val loss: 0.9308 score: 0.4694 time: 0.07s
Test loss: 0.7563 score: 0.5000 time: 0.08s
     INFO: Early stopping counter 4 of 20
Epoch 21/1000, LR 0.000270
Train loss: 0.1120;  Loss pred: 0.1120; Loss self: 0.0000; time: 0.24s
Val loss: 0.9344 score: 0.4694 time: 0.07s
Test loss: 0.7500 score: 0.4792 time: 0.08s
     INFO: Early stopping counter 5 of 20
Epoch 22/1000, LR 0.000270
Train loss: 0.1094;  Loss pred: 0.1094; Loss self: 0.0000; time: 0.24s
Val loss: 0.9367 score: 0.4694 time: 0.07s
Test loss: 0.7448 score: 0.5000 time: 0.08s
     INFO: Early stopping counter 6 of 20
Epoch 23/1000, LR 0.000270
Train loss: 0.1039;  Loss pred: 0.1039; Loss self: 0.0000; time: 0.24s
Val loss: 0.9408 score: 0.4694 time: 0.07s
Test loss: 0.7406 score: 0.5208 time: 0.07s
     INFO: Early stopping counter 7 of 20
Epoch 24/1000, LR 0.000270
Train loss: 0.0951;  Loss pred: 0.0951; Loss self: 0.0000; time: 0.23s
Val loss: 0.9431 score: 0.4490 time: 0.07s
Test loss: 0.7375 score: 0.5208 time: 0.07s
     INFO: Early stopping counter 8 of 20
Epoch 25/1000, LR 0.000270
Train loss: 0.0935;  Loss pred: 0.0935; Loss self: 0.0000; time: 0.23s
Val loss: 0.9454 score: 0.4490 time: 0.07s
Test loss: 0.7362 score: 0.5208 time: 0.08s
     INFO: Early stopping counter 9 of 20
Epoch 26/1000, LR 0.000270
Train loss: 0.0869;  Loss pred: 0.0869; Loss self: 0.0000; time: 0.23s
Val loss: 0.9477 score: 0.4490 time: 0.07s
Test loss: 0.7371 score: 0.5208 time: 0.07s
     INFO: Early stopping counter 10 of 20
Epoch 27/1000, LR 0.000270
Train loss: 0.0828;  Loss pred: 0.0828; Loss self: 0.0000; time: 0.23s
Val loss: 0.9476 score: 0.4490 time: 0.07s
Test loss: 0.7361 score: 0.5417 time: 0.07s
     INFO: Early stopping counter 11 of 20
Epoch 28/1000, LR 0.000270
Train loss: 0.0776;  Loss pred: 0.0776; Loss self: 0.0000; time: 0.23s
Val loss: 0.9458 score: 0.4898 time: 0.07s
Test loss: 0.7349 score: 0.5625 time: 0.07s
     INFO: Early stopping counter 12 of 20
Epoch 29/1000, LR 0.000270
Train loss: 0.0758;  Loss pred: 0.0758; Loss self: 0.0000; time: 0.23s
Val loss: 0.9431 score: 0.4898 time: 0.07s
Test loss: 0.7353 score: 0.5625 time: 0.07s
     INFO: Early stopping counter 13 of 20
Epoch 30/1000, LR 0.000270
Train loss: 0.0726;  Loss pred: 0.0726; Loss self: 0.0000; time: 0.23s
Val loss: 0.9387 score: 0.5306 time: 0.07s
Test loss: 0.7329 score: 0.5833 time: 0.07s
     INFO: Early stopping counter 14 of 20
Epoch 31/1000, LR 0.000270
Train loss: 0.0715;  Loss pred: 0.0715; Loss self: 0.0000; time: 0.22s
Val loss: 0.9287 score: 0.5306 time: 0.07s
Test loss: 0.7318 score: 0.5833 time: 0.07s
     INFO: Early stopping counter 15 of 20
Epoch 32/1000, LR 0.000270
Train loss: 0.0687;  Loss pred: 0.0687; Loss self: 0.0000; time: 0.23s
Val loss: 0.9160 score: 0.5306 time: 0.07s
Test loss: 0.7301 score: 0.6042 time: 0.07s
     INFO: Early stopping counter 16 of 20
Epoch 33/1000, LR 0.000270
Train loss: 0.0657;  Loss pred: 0.0657; Loss self: 0.0000; time: 0.23s
Val loss: 0.9018 score: 0.5306 time: 0.07s
Test loss: 0.7267 score: 0.6042 time: 0.07s
Epoch 34/1000, LR 0.000270
Train loss: 0.0649;  Loss pred: 0.0649; Loss self: 0.0000; time: 0.23s
Val loss: 0.8862 score: 0.5714 time: 0.07s
Test loss: 0.7218 score: 0.6042 time: 0.07s
Epoch 35/1000, LR 0.000270
Train loss: 0.0595;  Loss pred: 0.0595; Loss self: 0.0000; time: 0.23s
Val loss: 0.8698 score: 0.5918 time: 0.07s
Test loss: 0.7156 score: 0.5833 time: 0.07s
Epoch 36/1000, LR 0.000270
Train loss: 0.0573;  Loss pred: 0.0573; Loss self: 0.0000; time: 0.23s
Val loss: 0.8542 score: 0.6122 time: 0.07s
Test loss: 0.7113 score: 0.5833 time: 0.07s
Epoch 37/1000, LR 0.000270
Train loss: 0.0558;  Loss pred: 0.0558; Loss self: 0.0000; time: 0.23s
Val loss: 0.8401 score: 0.6327 time: 0.07s
Test loss: 0.7088 score: 0.6042 time: 0.07s
Epoch 38/1000, LR 0.000270
Train loss: 0.0554;  Loss pred: 0.0554; Loss self: 0.0000; time: 0.23s
Val loss: 0.8284 score: 0.6939 time: 0.07s
Test loss: 0.7110 score: 0.6458 time: 0.07s
Epoch 39/1000, LR 0.000269
Train loss: 0.0526;  Loss pred: 0.0526; Loss self: 0.0000; time: 0.23s
Val loss: 0.8173 score: 0.6939 time: 0.07s
Test loss: 0.7114 score: 0.6250 time: 0.07s
Epoch 40/1000, LR 0.000269
Train loss: 0.0515;  Loss pred: 0.0515; Loss self: 0.0000; time: 0.22s
Val loss: 0.8059 score: 0.7143 time: 0.07s
Test loss: 0.7137 score: 0.6667 time: 0.07s
Epoch 41/1000, LR 0.000269
Train loss: 0.0502;  Loss pred: 0.0502; Loss self: 0.0000; time: 0.23s
Val loss: 0.7962 score: 0.7143 time: 0.07s
Test loss: 0.7152 score: 0.7083 time: 0.07s
Epoch 42/1000, LR 0.000269
Train loss: 0.0491;  Loss pred: 0.0491; Loss self: 0.0000; time: 0.23s
Val loss: 0.7838 score: 0.7347 time: 0.07s
Test loss: 0.7123 score: 0.7500 time: 0.07s
Epoch 43/1000, LR 0.000269
Train loss: 0.0449;  Loss pred: 0.0449; Loss self: 0.0000; time: 0.23s
Val loss: 0.7693 score: 0.7347 time: 0.07s
Test loss: 0.7073 score: 0.7500 time: 0.07s
Epoch 44/1000, LR 0.000269
Train loss: 0.0426;  Loss pred: 0.0426; Loss self: 0.0000; time: 0.23s
Val loss: 0.7538 score: 0.7347 time: 0.07s
Test loss: 0.7044 score: 0.7500 time: 0.07s
Epoch 45/1000, LR 0.000269
Train loss: 0.0387;  Loss pred: 0.0387; Loss self: 0.0000; time: 0.23s
Val loss: 0.7372 score: 0.7347 time: 0.07s
Test loss: 0.6995 score: 0.7500 time: 0.07s
Epoch 46/1000, LR 0.000269
Train loss: 0.0372;  Loss pred: 0.0372; Loss self: 0.0000; time: 0.23s
Val loss: 0.7210 score: 0.7551 time: 0.07s
Test loss: 0.6923 score: 0.7708 time: 0.07s
Epoch 47/1000, LR 0.000269
Train loss: 0.0355;  Loss pred: 0.0355; Loss self: 0.0000; time: 0.23s
Val loss: 0.7053 score: 0.7551 time: 0.07s
Test loss: 0.6849 score: 0.7708 time: 0.07s
Epoch 48/1000, LR 0.000269
Train loss: 0.0320;  Loss pred: 0.0320; Loss self: 0.0000; time: 0.23s
Val loss: 0.6944 score: 0.7551 time: 0.07s
Test loss: 0.6828 score: 0.7708 time: 0.07s
Epoch 49/1000, LR 0.000269
Train loss: 0.0306;  Loss pred: 0.0306; Loss self: 0.0000; time: 0.23s
Val loss: 0.6876 score: 0.7347 time: 0.07s
Test loss: 0.6886 score: 0.7708 time: 0.07s
Epoch 50/1000, LR 0.000269
Train loss: 0.0284;  Loss pred: 0.0284; Loss self: 0.0000; time: 0.23s
Val loss: 0.6863 score: 0.7551 time: 0.07s
Test loss: 0.7036 score: 0.7500 time: 0.07s
Epoch 51/1000, LR 0.000269
Train loss: 0.0271;  Loss pred: 0.0271; Loss self: 0.0000; time: 0.23s
Val loss: 0.6814 score: 0.7551 time: 0.07s
Test loss: 0.7166 score: 0.7500 time: 0.07s
Epoch 52/1000, LR 0.000269
Train loss: 0.0232;  Loss pred: 0.0232; Loss self: 0.0000; time: 0.23s
Val loss: 0.6761 score: 0.7347 time: 0.08s
Test loss: 0.7222 score: 0.7292 time: 0.07s
Epoch 53/1000, LR 0.000269
Train loss: 0.0220;  Loss pred: 0.0220; Loss self: 0.0000; time: 0.23s
Val loss: 0.6537 score: 0.7551 time: 0.07s
Test loss: 0.7020 score: 0.7500 time: 0.07s
Epoch 54/1000, LR 0.000269
Train loss: 0.0213;  Loss pred: 0.0213; Loss self: 0.0000; time: 0.23s
Val loss: 0.6312 score: 0.7551 time: 0.07s
Test loss: 0.6801 score: 0.7708 time: 0.07s
Epoch 55/1000, LR 0.000269
Train loss: 0.0199;  Loss pred: 0.0199; Loss self: 0.0000; time: 0.23s
Val loss: 0.6118 score: 0.7551 time: 0.07s
Test loss: 0.6574 score: 0.7708 time: 0.07s
Epoch 56/1000, LR 0.000269
Train loss: 0.0198;  Loss pred: 0.0198; Loss self: 0.0000; time: 0.23s
Val loss: 0.5961 score: 0.7551 time: 0.07s
Test loss: 0.6387 score: 0.7708 time: 0.07s
Epoch 57/1000, LR 0.000269
Train loss: 0.0192;  Loss pred: 0.0192; Loss self: 0.0000; time: 0.23s
Val loss: 0.5782 score: 0.7755 time: 0.07s
Test loss: 0.6195 score: 0.7708 time: 0.07s
Epoch 58/1000, LR 0.000269
Train loss: 0.0181;  Loss pred: 0.0181; Loss self: 0.0000; time: 0.23s
Val loss: 0.5623 score: 0.7959 time: 0.07s
Test loss: 0.6033 score: 0.7708 time: 0.07s
Epoch 59/1000, LR 0.000268
Train loss: 0.0175;  Loss pred: 0.0175; Loss self: 0.0000; time: 0.23s
Val loss: 0.5514 score: 0.7959 time: 0.07s
Test loss: 0.5900 score: 0.7708 time: 0.08s
Epoch 60/1000, LR 0.000268
Train loss: 0.0168;  Loss pred: 0.0168; Loss self: 0.0000; time: 0.24s
Val loss: 0.5447 score: 0.7755 time: 0.07s
Test loss: 0.5857 score: 0.7500 time: 0.08s
Epoch 61/1000, LR 0.000268
Train loss: 0.0164;  Loss pred: 0.0164; Loss self: 0.0000; time: 0.24s
Val loss: 0.5416 score: 0.7755 time: 0.07s
Test loss: 0.5862 score: 0.7500 time: 0.08s
Epoch 62/1000, LR 0.000268
Train loss: 0.0176;  Loss pred: 0.0176; Loss self: 0.0000; time: 0.24s
Val loss: 0.5421 score: 0.7755 time: 0.07s
Test loss: 0.5905 score: 0.7500 time: 0.07s
     INFO: Early stopping counter 1 of 20
Epoch 63/1000, LR 0.000268
Train loss: 0.0161;  Loss pred: 0.0161; Loss self: 0.0000; time: 0.23s
Val loss: 0.5406 score: 0.7755 time: 0.07s
Test loss: 0.5906 score: 0.7292 time: 0.07s
Epoch 64/1000, LR 0.000268
Train loss: 0.0158;  Loss pred: 0.0158; Loss self: 0.0000; time: 0.23s
Val loss: 0.5407 score: 0.7755 time: 0.07s
Test loss: 0.5914 score: 0.7083 time: 0.07s
     INFO: Early stopping counter 1 of 20
Epoch 65/1000, LR 0.000268
Train loss: 0.0154;  Loss pred: 0.0154; Loss self: 0.0000; time: 0.23s
Val loss: 0.5416 score: 0.7755 time: 0.07s
Test loss: 0.5927 score: 0.6875 time: 0.08s
     INFO: Early stopping counter 2 of 20
Epoch 66/1000, LR 0.000268
Train loss: 0.0146;  Loss pred: 0.0146; Loss self: 0.0000; time: 0.23s
Val loss: 0.5399 score: 0.7755 time: 0.07s
Test loss: 0.5916 score: 0.6875 time: 0.07s
Epoch 67/1000, LR 0.000268
Train loss: 0.0155;  Loss pred: 0.0155; Loss self: 0.0000; time: 0.23s
Val loss: 0.5369 score: 0.7755 time: 0.07s
Test loss: 0.5881 score: 0.6875 time: 0.07s
Epoch 68/1000, LR 0.000268
Train loss: 0.0152;  Loss pred: 0.0152; Loss self: 0.0000; time: 0.23s
Val loss: 0.5317 score: 0.7347 time: 0.07s
Test loss: 0.5798 score: 0.6875 time: 0.07s
Epoch 69/1000, LR 0.000268
Train loss: 0.0143;  Loss pred: 0.0143; Loss self: 0.0000; time: 0.23s
Val loss: 0.5209 score: 0.7347 time: 0.07s
Test loss: 0.5614 score: 0.6875 time: 0.07s
Epoch 70/1000, LR 0.000268
Train loss: 0.0148;  Loss pred: 0.0148; Loss self: 0.0000; time: 0.23s
Val loss: 0.5042 score: 0.7347 time: 0.07s
Test loss: 0.5330 score: 0.7083 time: 0.07s
Epoch 71/1000, LR 0.000268
Train loss: 0.0144;  Loss pred: 0.0144; Loss self: 0.0000; time: 0.23s
Val loss: 0.4853 score: 0.7551 time: 0.07s
Test loss: 0.5029 score: 0.8125 time: 0.08s
Epoch 72/1000, LR 0.000267
Train loss: 0.0137;  Loss pred: 0.0137; Loss self: 0.0000; time: 0.23s
Val loss: 0.4657 score: 0.7551 time: 0.07s
Test loss: 0.4789 score: 0.8125 time: 0.07s
Epoch 73/1000, LR 0.000267
Train loss: 0.0136;  Loss pred: 0.0136; Loss self: 0.0000; time: 0.24s
Val loss: 0.4492 score: 0.7551 time: 0.07s
Test loss: 0.4601 score: 0.8125 time: 0.07s
Epoch 74/1000, LR 0.000267
Train loss: 0.0130;  Loss pred: 0.0130; Loss self: 0.0000; time: 0.23s
Val loss: 0.4334 score: 0.7755 time: 0.07s
Test loss: 0.4420 score: 0.8125 time: 0.07s
Epoch 75/1000, LR 0.000267
Train loss: 0.0129;  Loss pred: 0.0129; Loss self: 0.0000; time: 0.23s
Val loss: 0.4176 score: 0.7755 time: 0.07s
Test loss: 0.4195 score: 0.8125 time: 0.07s
Epoch 76/1000, LR 0.000267
Train loss: 0.0127;  Loss pred: 0.0127; Loss self: 0.0000; time: 0.23s
Val loss: 0.4031 score: 0.7755 time: 0.07s
Test loss: 0.3964 score: 0.8333 time: 0.07s
Epoch 77/1000, LR 0.000267
Train loss: 0.0120;  Loss pred: 0.0120; Loss self: 0.0000; time: 0.23s
Val loss: 0.3926 score: 0.7959 time: 0.07s
Test loss: 0.3784 score: 0.8750 time: 0.07s
Epoch 78/1000, LR 0.000267
Train loss: 0.0117;  Loss pred: 0.0117; Loss self: 0.0000; time: 0.23s
Val loss: 0.3842 score: 0.7959 time: 0.07s
Test loss: 0.3652 score: 0.8750 time: 0.07s
Epoch 79/1000, LR 0.000267
Train loss: 0.0117;  Loss pred: 0.0117; Loss self: 0.0000; time: 0.24s
Val loss: 0.3776 score: 0.7959 time: 0.07s
Test loss: 0.3564 score: 0.8958 time: 0.07s
Epoch 80/1000, LR 0.000267
Train loss: 0.0117;  Loss pred: 0.0117; Loss self: 0.0000; time: 0.23s
Val loss: 0.3740 score: 0.7959 time: 0.07s
Test loss: 0.3536 score: 0.8958 time: 0.07s
Epoch 81/1000, LR 0.000267
Train loss: 0.0112;  Loss pred: 0.0112; Loss self: 0.0000; time: 0.23s
Val loss: 0.3716 score: 0.8163 time: 0.07s
Test loss: 0.3536 score: 0.8958 time: 0.07s
Epoch 82/1000, LR 0.000267
Train loss: 0.0106;  Loss pred: 0.0106; Loss self: 0.0000; time: 0.23s
Val loss: 0.3723 score: 0.8163 time: 0.07s
Test loss: 0.3576 score: 0.8958 time: 0.07s
     INFO: Early stopping counter 1 of 20
Epoch 83/1000, LR 0.000266
Train loss: 0.0106;  Loss pred: 0.0106; Loss self: 0.0000; time: 0.23s
Val loss: 0.3755 score: 0.8163 time: 0.07s
Test loss: 0.3652 score: 0.8958 time: 0.08s
     INFO: Early stopping counter 2 of 20
Epoch 84/1000, LR 0.000266
Train loss: 0.0103;  Loss pred: 0.0103; Loss self: 0.0000; time: 0.24s
Val loss: 0.3704 score: 0.8163 time: 0.08s
Test loss: 0.3773 score: 0.8958 time: 0.08s
Epoch 85/1000, LR 0.000266
Train loss: 0.0101;  Loss pred: 0.0101; Loss self: 0.0000; time: 0.23s
Val loss: 0.3783 score: 0.8163 time: 0.07s
Test loss: 0.4103 score: 0.8542 time: 0.07s
     INFO: Early stopping counter 1 of 20
Epoch 86/1000, LR 0.000266
Train loss: 0.0101;  Loss pred: 0.0101; Loss self: 0.0000; time: 0.23s
Val loss: 0.3974 score: 0.8163 time: 0.07s
Test loss: 0.4629 score: 0.8125 time: 0.07s
     INFO: Early stopping counter 2 of 20
Epoch 87/1000, LR 0.000266
Train loss: 0.0091;  Loss pred: 0.0091; Loss self: 0.0000; time: 0.23s
Val loss: 0.4282 score: 0.8163 time: 0.07s
Test loss: 0.5269 score: 0.8125 time: 0.07s
     INFO: Early stopping counter 3 of 20
Epoch 88/1000, LR 0.000266
Train loss: 0.0088;  Loss pred: 0.0088; Loss self: 0.0000; time: 0.23s
Val loss: 0.4580 score: 0.7959 time: 0.07s
Test loss: 0.5780 score: 0.7500 time: 0.07s
     INFO: Early stopping counter 4 of 20
Epoch 89/1000, LR 0.000266
Train loss: 0.0088;  Loss pred: 0.0088; Loss self: 0.0000; time: 0.23s
Val loss: 0.4790 score: 0.7959 time: 0.07s
Test loss: 0.6180 score: 0.7500 time: 0.07s
     INFO: Early stopping counter 5 of 20
Epoch 90/1000, LR 0.000266
Train loss: 0.0084;  Loss pred: 0.0084; Loss self: 0.0000; time: 0.23s
Val loss: 0.4881 score: 0.7959 time: 0.07s
Test loss: 0.6388 score: 0.7500 time: 0.07s
     INFO: Early stopping counter 6 of 20
Epoch 91/1000, LR 0.000266
Train loss: 0.0082;  Loss pred: 0.0082; Loss self: 0.0000; time: 0.23s
Val loss: 0.4952 score: 0.7959 time: 0.07s
Test loss: 0.6561 score: 0.7500 time: 0.07s
     INFO: Early stopping counter 7 of 20
Epoch 92/1000, LR 0.000266
Train loss: 0.0076;  Loss pred: 0.0076; Loss self: 0.0000; time: 0.23s
Val loss: 0.4992 score: 0.7959 time: 0.07s
Test loss: 0.6677 score: 0.7500 time: 0.07s
     INFO: Early stopping counter 8 of 20
Epoch 93/1000, LR 0.000265
Train loss: 0.0076;  Loss pred: 0.0076; Loss self: 0.0000; time: 0.23s
Val loss: 0.4987 score: 0.7755 time: 0.07s
Test loss: 0.6690 score: 0.7500 time: 0.07s
     INFO: Early stopping counter 9 of 20
Epoch 94/1000, LR 0.000265
Train loss: 0.0072;  Loss pred: 0.0072; Loss self: 0.0000; time: 0.23s
Val loss: 0.4974 score: 0.8163 time: 0.07s
Test loss: 0.6631 score: 0.7708 time: 0.07s
     INFO: Early stopping counter 10 of 20
Epoch 95/1000, LR 0.000265
Train loss: 0.0068;  Loss pred: 0.0068; Loss self: 0.0000; time: 0.23s
Val loss: 0.4956 score: 0.8163 time: 0.07s
Test loss: 0.6540 score: 0.7708 time: 0.07s
     INFO: Early stopping counter 11 of 20
Epoch 96/1000, LR 0.000265
Train loss: 0.0068;  Loss pred: 0.0068; Loss self: 0.0000; time: 0.23s
Val loss: 0.5044 score: 0.8163 time: 0.07s
Test loss: 0.6630 score: 0.7708 time: 0.07s
     INFO: Early stopping counter 12 of 20
Epoch 97/1000, LR 0.000265
Train loss: 0.0059;  Loss pred: 0.0059; Loss self: 0.0000; time: 0.23s
Val loss: 0.5164 score: 0.7959 time: 0.07s
Test loss: 0.6725 score: 0.7917 time: 0.07s
     INFO: Early stopping counter 13 of 20
Epoch 98/1000, LR 0.000265
Train loss: 0.0058;  Loss pred: 0.0058; Loss self: 0.0000; time: 0.23s
Val loss: 0.5272 score: 0.7959 time: 0.07s
Test loss: 0.6774 score: 0.8125 time: 0.07s
     INFO: Early stopping counter 14 of 20
Epoch 99/1000, LR 0.000265
Train loss: 0.0053;  Loss pred: 0.0053; Loss self: 0.0000; time: 0.23s
Val loss: 0.5374 score: 0.7959 time: 0.07s
Test loss: 0.6815 score: 0.8125 time: 0.07s
     INFO: Early stopping counter 15 of 20
Epoch 100/1000, LR 0.000265
Train loss: 0.0051;  Loss pred: 0.0051; Loss self: 0.0000; time: 0.23s
Val loss: 0.5465 score: 0.8163 time: 0.07s
Test loss: 0.6883 score: 0.8125 time: 0.07s
     INFO: Early stopping counter 16 of 20
Epoch 101/1000, LR 0.000265
Train loss: 0.0046;  Loss pred: 0.0046; Loss self: 0.0000; time: 0.23s
Val loss: 0.5557 score: 0.8163 time: 0.07s
Test loss: 0.6952 score: 0.8125 time: 0.07s
     INFO: Early stopping counter 17 of 20
Epoch 102/1000, LR 0.000264
Train loss: 0.0051;  Loss pred: 0.0051; Loss self: 0.0000; time: 0.23s
Val loss: 0.5778 score: 0.7959 time: 0.07s
Test loss: 0.7256 score: 0.8125 time: 0.07s
     INFO: Early stopping counter 18 of 20
Epoch 103/1000, LR 0.000264
Train loss: 0.0043;  Loss pred: 0.0043; Loss self: 0.0000; time: 0.23s
Val loss: 0.5967 score: 0.7755 time: 0.07s
Test loss: 0.7504 score: 0.7917 time: 0.07s
     INFO: Early stopping counter 19 of 20
Epoch 104/1000, LR 0.000264
Train loss: 0.0044;  Loss pred: 0.0044; Loss self: 0.0000; time: 0.23s
Val loss: 0.6111 score: 0.7755 time: 0.07s
Test loss: 0.7704 score: 0.7917 time: 0.07s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 083,   Train_Loss: 0.0103,   Val_Loss: 0.3704,   Val_Precision: 0.9444,   Val_Recall: 0.6800,   Val_accuracy: 0.7907,   Val_Score: 0.8163,   Val_Loss: 0.3704,   Test_Precision: 1.0000,   Test_Recall: 0.7917,   Test_accuracy: 0.8837,   Test_Score: 0.8958,   Test_loss: 0.3773


[0.07140269293449819, 0.07025647105183452, 0.06996766501106322, 0.07019000092986971, 0.06995220202952623, 0.07005454506725073, 0.07077746093273163, 0.07029995403718203, 0.07004583498928696, 0.0703566949814558, 0.07014449697453529, 0.07387414004188031, 0.07123668200802058, 0.07408993097487837, 0.07081565505359322, 0.0705169040011242, 0.07046438695397228, 0.07043113105464727, 0.0699055859586224, 0.06882533896714449, 0.06925370404496789, 0.06909177300985903, 0.06914540904108435, 0.0689691980369389, 0.06894625490531325, 0.06829987303353846, 0.06975654000416398, 0.06906155892647803, 0.06937944504898041, 0.08016833302099258, 0.07170937792398036, 0.06897341297008097, 0.06905112590175122, 0.06871384603437036, 0.06867225095629692, 0.06861502793617547, 0.06934155500493944, 0.06899598299060017, 0.06969069305341691, 0.06978478899691254, 0.07093609706498682, 0.06988592993002385, 0.06969245697837323, 0.06965289206709713, 0.06922693504020572, 0.06887985300272703, 0.06885823095217347, 0.06951714202295989, 0.0690673339413479, 0.06829411606304348, 0.07016174797900021, 0.06922532001044601, 0.06981348397675902, 0.07100505603011698, 0.07897043297998607, 0.07289586903061718, 0.07353302196133882, 0.07357235392555594, 0.07318592094816267, 0.07318616309203207, 0.0728717049350962, 0.07316883199382573, 0.07307384698651731, 0.07358125993050635, 0.07363991299644113, 0.0727311500813812, 0.07411551405675709, 0.0740579100092873, 0.07290751207619905, 0.07302245392929763, 0.07290364999789745, 0.07331615604925901, 0.07277754601091146, 0.07269923505373299, 0.07256347103975713, 0.07249310705810785, 0.07273959601297975, 0.07268922601360828, 0.07280515797901899, 0.07267696899361908, 0.07533581799361855, 0.07276648993138224, 0.07312183303292841, 0.07255846308544278, 0.07249504700303078, 0.07267009397037327, 0.07272250892128795, 0.07309056096710265, 0.06877232401166111, 0.06895384006202221, 0.06915609107818455, 0.06868598493747413, 0.06892176100518554, 0.07048276998102665, 0.0694201800506562, 0.06912049592938274, 0.06933541398029774, 0.06938951299525797, 0.06920018105302006, 0.06952238595113158, 0.06895315402653068, 0.06901283795014024, 0.06897767609916627, 0.06909350794740021, 0.06912156892940402, 0.06891590903978795, 0.06949843396432698, 0.0716112699592486, 0.06935271294787526, 0.0804011799627915, 0.08020252001006156, 0.08034412295091897, 0.08082830696366727, 0.08039718098007143, 0.080326052964665, 0.08101819397415966, 0.07984965504147112, 0.08058651303872466, 0.0798853220185265, 0.08087092905770987, 0.08024426898919046, 0.08147797302808613, 0.09898241399787366, 0.08046855498105288, 0.08042321598622948, 0.08058667508885264, 0.08049116004258394, 0.08016458991914988, 0.08093761699274182, 0.08386904292274266, 0.08091785793658346, 0.08058245002757758, 0.0806174980243668, 0.08047873002942652, 0.08024490205571055, 0.08050495793577284, 0.0804690340301022, 0.08059657597914338, 0.0801543949637562, 0.08066889108158648, 0.08111020002979785, 0.08058180310763419, 0.08018452697433531, 0.08648654294665903, 0.08043069904670119, 0.08053053810726851, 0.08029400801751763, 0.08025904407259077, 0.07987328001763672, 0.08027240203227848, 0.08064647810533643, 0.08085375500377268, 0.08034844393841922, 0.08075202698819339, 0.08065754198469222, 0.08041641104500741, 0.08057390502654016, 0.08708506193943322, 0.08056088304147124, 0.08035301696509123, 0.08062325895298272, 0.08056756004225463, 0.08045023097656667, 0.08073942502960563, 0.08047512790653855, 0.08045254496391863, 0.08048100909218192, 0.08064238098450005, 0.0805498689878732, 0.0804634930100292, 0.08096749288961291, 0.08408744097687304, 0.08064595703035593, 0.08027269295416772, 0.08063382492400706, 0.08062547200825065, 0.08074485696852207, 0.08046643203124404, 0.08090738905593753, 0.0808422570116818, 0.08077456196770072, 0.08075691794510931, 0.08034848410170525, 0.08057588001247495, 0.08166250900831074, 0.08079165907111019, 0.08033275709021837, 0.08059451100416481, 0.0803884610068053, 0.0802699790801853, 0.08072176203131676, 0.08064250100869685, 0.0808276169700548, 0.08040932205040008, 0.0811337810009718, 0.08046957000624388, 0.08040177298244089, 0.08086552296299487, 0.08193965896498412, 0.08044840791262686, 0.08047867100685835, 0.08068947494029999, 0.08072613598778844, 0.0815874149557203, 0.08702239801641554, 0.08030709798913449, 0.08083348895888776, 0.08649905701167881, 0.0813364339992404, 0.08221047499682754, 0.08149870403576642, 0.08134508307557553, 0.08142576599493623, 0.0809685419080779, 0.08233796595595777, 0.08149817702360451, 0.08079761394765228, 0.0798508480656892, 0.09444205090403557, 0.08115521701984107, 0.08071911800652742, 0.08108620799612254, 0.08105440996587276, 0.08127907302696258, 0.08129894197918475, 0.08118227706290781, 0.08101192605681717, 0.08167937106918544, 0.08168677904177457, 0.07688203093130141, 0.07746480009518564, 0.08181173307821155, 0.07704017602372915, 0.0768792430171743, 0.07658463600091636, 0.0763666849816218, 0.07606250094249845, 0.07651364605408162, 0.07634741195943207, 0.0764343460323289, 0.0760752740316093, 0.07626736396923661, 0.07617454801220447, 0.07656663202214986, 0.07738358795177191, 0.0762680530315265, 0.0762130580842495, 0.07660010689869523, 0.0766146300593391, 0.07666661101393402, 0.07662181893829256, 0.07699536997824907, 0.07727231900207698, 0.07651225593872368, 0.07615863997489214, 0.07751833205111325, 0.07643168303184211, 0.07787617796566337, 0.07635849597863853, 0.07611482299398631, 0.07673466403502971, 0.07682900899089873, 0.07634208898525685, 0.07625757402274758, 0.07636794203426689, 0.08144078205805272, 0.08084352198056877, 0.08107734099030495, 0.07643325300887227, 0.07634765200782567, 0.0768581519369036, 0.08735163498204201, 0.07668147096410394, 0.07670262001920491, 0.07669008302036673, 0.07623120909556746, 0.07632181805092841, 0.0808929109480232, 0.07709342206362635, 0.0775565360672772, 0.07646866596769542, 0.07610513898544014, 0.07605903001967818, 0.07632893498521298, 0.07740492396987975, 0.07603700598701835, 0.07633085502311587, 0.07652568409685045, 0.07641316298395395, 0.07956646394450217, 0.08016488095745444, 0.07681704603601247, 0.07661099592223763, 0.07627132500056177, 0.07633752399124205, 0.07615323702339083, 0.07652014004997909, 0.07733814604580402, 0.07663739891722798, 0.07709251099731773, 0.07622157293371856, 0.07709109701681882, 0.0767369099194184, 0.07610869710333645, 0.07626729505136609, 0.07623832894023508, 0.07636077608913183, 0.0766103370115161, 0.07636648998595774, 0.07664631004445255, 0.07683454093057662]
[0.001457197814989759, 0.0014338055316700923, 0.0014279115308380248, 0.0014324489985687696, 0.0014275959597862496, 0.0014296845932091986, 0.0014444379782190128, 0.0014346929395343273, 0.0014295068365160603, 0.001435850917988894, 0.0014315203464190876, 0.0015076355110587819, 0.001453809836898379, 0.001512039407650579, 0.001445217450073331, 0.0014391204898188614, 0.001438048713346373, 0.0014373700215234136, 0.001426644611400457, 0.0014045987544315203, 0.0014133408988768958, 0.001410036183874674, 0.0014111307967568235, 0.0014075346538150798, 0.001407066426639046, 0.0013938749598681318, 0.0014236028572278364, 0.001409419569928123, 0.001415907041815927, 0.0016360884289998486, 0.0014634566923261297, 0.0014076206728587954, 0.0014092066510561474, 0.001402323388456538, 0.001401474509312182, 0.0014003066925750095, 0.0014151337756110089, 0.0014080812855224525, 0.0014222590419064676, 0.0014241793672839294, 0.0014476754503058536, 0.0014262434679596704, 0.0014222950403749639, 0.0014214875932060638, 0.0014127945926572596, 0.0014057112857699394, 0.0014052700194321116, 0.0014187171841420385, 0.0014095374273744468, 0.0013937574706743567, 0.0014318724077346983, 0.001412761632866245, 0.001424764979117531, 0.0014490827761248362, 0.001611641489387471, 0.0014876707965432077, 0.0015006739175783433, 0.0015014766107256316, 0.0014935902234318914, 0.0014935951651435118, 0.0014871776517366572, 0.0014932414692617497, 0.001491302999724843, 0.001501658365928701, 0.0015028553672743086, 0.0014843091853343102, 0.0015125615113623897, 0.0015113859185568836, 0.001487908409718348, 0.0014902541618224006, 0.0014878295917938255, 0.001496248082637939, 0.0014852560410390095, 0.001483657858239449, 0.0014808871640766762, 0.0014794511644511807, 0.001484481551285301, 0.0014834535921144547, 0.0014858195505922241, 0.0014832034488493691, 0.001537465673339154, 0.001485030406762903, 0.0014922823067944572, 0.0014807849609274038, 0.0014794907551638934, 0.0014830631422525157, 0.0014841328351283256, 0.0014916441013694418, 0.0014035168165645125, 0.0014072212257555553, 0.0014113487975139702, 0.0014017547946423292, 0.0014065665511262355, 0.0014384238771638091, 0.0014167383683807387, 0.0014106223659057701, 0.001415008448577505, 0.0014161125101073055, 0.0014122485929187766, 0.001418824203084318, 0.0014072072250312384, 0.0014084252642885763, 0.0014077076754931892, 0.0014100715907632696, 0.0014106442638653883, 0.0014064471232609786, 0.0014183353870270811, 0.0014614544889642572, 0.0014153614887321482, 0.0016408404074039083, 0.0016367861226543176, 0.001639675978590183, 0.0016495572849728015, 0.0016407587955116617, 0.00163930720336051, 0.001653432530084891, 0.0016295847967647168, 0.0016446227150760135, 0.0016303126942556426, 0.001650427123626732, 0.00163763814263654, 0.0016628157760833903, 0.002020049265262728, 0.0016422154077765892, 0.0016412901221679486, 0.0016446260222214826, 0.0016426767355629376, 0.0016360120391663239, 0.0016517881018926902, 0.0017116131208722992, 0.001651384855848642, 0.001644539796481175, 0.0016452550617217713, 0.0016424230618250308, 0.00163765106236144, 0.0016429583252198538, 0.0016422251842877998, 0.0016448280812070078, 0.0016358039788521674, 0.0016463038996242139, 0.001655310204689752, 0.0016445265940333508, 0.0016364189178435778, 0.001765031488707327, 0.0016414428376877795, 0.0016434803695360922, 0.0016386532248472984, 0.001637939674950832, 0.0016300669391354431, 0.0016382122863730301, 0.0016458464919456414, 0.0016500766327300547, 0.0016397641620085556, 0.001648000550779457, 0.001646072285401882, 0.0016411512458164776, 0.0016443654087049011, 0.0017772461620292493, 0.0016440996539075763, 0.0016398574890834944, 0.001645372631693525, 0.0016442359192296863, 0.0016418414485013606, 0.0016477433679511352, 0.0016423495491130315, 0.0016418886727330331, 0.001642469573309835, 0.0016457628772346949, 0.0016438748773035345, 0.0016421121022454938, 0.001652397814073733, 0.0017160702240178172, 0.001645835857762366, 0.0016382182235544433, 0.001645588263755246, 0.001645417796086748, 0.0016478542238473892, 0.0016421720822702866, 0.0016511712052232148, 0.0016498419798302408, 0.001648460448320423, 0.0016481003662267206, 0.001639764981667454, 0.0016444057145403052, 0.0016665818164961375, 0.0016488093687981671, 0.0016394440222493544, 0.0016447859388605064, 0.0016405808368735776, 0.0016381628383711285, 0.0016473828985983012, 0.0016457653267080991, 0.0016495432034705061, 0.0016410065724571444, 0.0016557914489994244, 0.0016422361225764059, 0.0016408525098457324, 0.0016503167951631608, 0.0016722379380609005, 0.001641804243114834, 0.0016424218572828235, 0.0016467239783734692, 0.0016474721630160905, 0.0016650492848106185, 0.00177596730645746, 0.0016389203671251936, 0.0016496630399773012, 0.0018020636877433087, 0.0016945090416508417, 0.0017127182291005738, 0.0016978896674118005, 0.001694689230741157, 0.0016963701248945047, 0.001686844623084956, 0.0017153742907491203, 0.0016978786879917607, 0.0016832836239094224, 0.0016635593347018585, 0.0019675427271674075, 0.0016907336879133557, 0.0016816482918026547, 0.0016892959999192196, 0.0016886335409556825, 0.0016933140213950537, 0.0016937279578996822, 0.0016912974388105795, 0.0016877484595170245, 0.0017016535639413632, 0.001701807896703637, 0.001601708977735446, 0.0016138500019830342, 0.0017044111057960738, 0.001605003667161024, 0.0016016508961911313, 0.001595513250019091, 0.0015909726037837875, 0.001584635436302051, 0.001594034292793367, 0.001590571082488168, 0.0015923822090068522, 0.0015849015423251938, 0.0015889034160257627, 0.0015869697502542597, 0.0015951381671281222, 0.0016121580823285815, 0.0015889177714901355, 0.0015877720434218645, 0.001595835560389484, 0.0015961381262362313, 0.001597221062790292, 0.0015962878945477617, 0.001604070207880189, 0.0016098399792099372, 0.0015940053320567433, 0.001586638332810253, 0.0016149652510648593, 0.001592326729830044, 0.0016224203742846537, 0.0015908019995549694, 0.0015857254790413815, 0.001598638834063119, 0.001600604353977057, 0.001590460187192851, 0.0015886994588072412, 0.0015909987923805602, 0.001696682959542765, 0.0016842400412618492, 0.0016891112706313531, 0.001592359437684839, 0.0015905760834963683, 0.0016012114986854915, 0.001819825728792542, 0.0015975306450854987, 0.0015979712504001025, 0.0015977100629243068, 0.0015881501894909889, 0.0015900378760610085, 0.0016852689780838166, 0.0016061129596588823, 0.0016157611680682749, 0.0015930972076603211, 0.0015855237288633361, 0.001584563125409962, 0.0015901861455252704, 0.0016126025827058281, 0.0015841042913962156, 0.001590226146314914, 0.001594285085351051, 0.0015919408954990406, 0.001657634665510462, 0.0016701016866136342, 0.0016003551257502597, 0.0015960624150466174, 0.0015889859375117037, 0.001590365083150876, 0.0015865257713206422, 0.0015941695843745645, 0.0016112113759542506, 0.0015966124774422497, 0.001606093979110786, 0.0015879494361191366, 0.0016060645211837254, 0.0015986856233212166, 0.0015855978563195094, 0.0015889019802367936, 0.0015882985195882309, 0.001590849501856913, 0.0015960486877399187, 0.0015909685413741197, 0.0015967981259260948, 0.0016007196027203463]
[686.2486271344209, 697.4446519502565, 700.3234993228967, 698.1051339343664, 700.4783063057474, 699.454974020046, 692.3107915183708, 697.0132579899501, 699.541950031636, 696.4511339384991, 698.5580068781245, 663.2902930879628, 687.8478702093826, 661.3584242184596, 691.9373966521506, 694.8688501585212, 695.3867353164808, 695.7150803382821, 700.9454155638355, 711.9470929651561, 707.543382346499, 709.2016583943788, 708.6515313097, 710.4620815477122, 710.698500843791, 717.4244668937919, 702.4430970497539, 709.5119305396033, 706.2610541985027, 611.2139064581653, 683.3136950643368, 710.4186655408082, 709.6191316231283, 713.1022760025747, 713.5342051214201, 714.1292727531784, 706.6469737592352, 710.1862728251242, 703.1067973802787, 702.158746975188, 690.7625599292492, 701.1425625882529, 703.0890016577482, 703.4883770913339, 707.81697863038, 711.3836319897493, 711.6070122979734, 704.8621185234636, 709.4526052158158, 717.4849434286148, 698.3862490807094, 707.8334920316145, 701.870143256454, 690.0917024728003, 620.4853911896158, 672.1917256987414, 666.367282249906, 666.0110406360052, 669.5276818980869, 669.5254666976076, 672.4146229821611, 669.6840535070289, 670.5545420243288, 665.9304291103188, 665.4000256948711, 673.7140818641303, 661.1301375104297, 661.6443806455666, 672.0843792994583, 671.0264769716334, 672.119983038067, 668.3383668816232, 673.284586878671, 674.0098429341579, 675.2708945407692, 675.9263327025475, 673.6358556522142, 674.1026516202913, 673.0292380399866, 674.2163394885403, 650.4210255492364, 673.3868851748422, 670.1144920414426, 675.3175014511952, 675.9082451239941, 674.280124365557, 673.7941350873319, 670.4012029960261, 712.4959161143297, 710.6203215937758, 708.5420710751705, 713.3915316873657, 710.9510738751051, 695.205367399584, 705.8466279436973, 708.9069506975336, 706.7095613494681, 706.1585805242447, 708.0906329198328, 704.8089522480269, 710.6273917672652, 710.0128245037695, 710.3747584878735, 709.1838503452874, 708.8959460692394, 711.0114439861819, 705.0518580771378, 684.2498398350447, 706.5332835188129, 609.4437920273867, 610.9533714633013, 609.8765933375533, 606.223263120255, 609.4741059658044, 610.0137899412889, 604.8024227203621, 613.6532458975697, 608.042191581782, 613.3792636979824, 605.9037601142602, 610.6355085196263, 601.3895311694769, 495.0374316093424, 608.9335146075077, 609.2768039565846, 608.0409688819393, 608.7625022931275, 611.2424456910343, 605.4045303112165, 584.2441774986886, 605.5523619817276, 608.0728494012136, 607.808493203171, 608.8565262161005, 610.6306911058527, 608.658165365323, 608.9298894984855, 607.9662740595844, 611.3201905167716, 607.4212666496511, 604.116374783919, 608.0777310796836, 611.0904665644962, 566.5621301364881, 609.2201184469216, 608.4648277741654, 610.2572434708919, 610.5230951378097, 613.4717390994882, 610.4214992880931, 607.5900789616458, 606.0324594412917, 609.843795326698, 606.795913707085, 607.5067351953222, 609.3283617516294, 608.1373365714363, 562.6682568599309, 608.2356368260724, 609.8090880805096, 607.7650622951803, 608.1852295675997, 609.0722102994656, 606.8906235340743, 608.8837790591294, 609.054692079356, 608.8392846053412, 607.6209482135466, 608.3188044337721, 608.9718227108597, 605.1811443242312, 582.7267357734992, 607.5940047627672, 610.4192870167805, 607.6854229125283, 607.7483800031047, 606.8497962551643, 608.9495801301834, 605.6307164494273, 606.1186539227801, 606.6266260854946, 606.7591637574063, 609.8434904879564, 608.1224305885794, 600.0305476165728, 606.4982519652406, 609.9628815798037, 607.9818512388252, 609.5402174181677, 610.4399248821492, 607.023419297884, 607.6200438614326, 606.2284382100938, 609.3820809642829, 603.9407925462403, 608.9258336561005, 609.4392969505935, 605.9442665377067, 598.0010243994246, 609.0860126556855, 608.8569727477762, 607.2663136828417, 606.9905291566575, 600.5828230566397, 563.0734284150254, 610.1577721888253, 606.1843999449486, 554.9193443059062, 590.1414364987808, 583.8672018602531, 588.9664206063298, 590.0786892725217, 589.4939938665736, 592.8228280866601, 582.963150020915, 588.9702291880424, 594.07694924133, 601.1207289935463, 508.24817483870345, 591.4592032729678, 594.6546640427666, 591.9625690511426, 592.1947987803497, 590.5579162311194, 590.4135875752185, 591.2620554213452, 592.5053549070728, 587.6636826615894, 587.6103888911182, 624.3331428496056, 619.636272746067, 586.7129101654928, 623.051535931272, 624.3557833845623, 626.7575653088651, 628.5463355067926, 631.0599757466157, 627.3390757783584, 628.7050047682725, 627.9899350443552, 630.9540203569426, 629.3648751169818, 630.1317336639737, 626.9049419088219, 620.2865655430094, 629.3591889668209, 629.8133312921067, 626.6309792945943, 626.5121943788455, 626.0874110018517, 626.4534132067112, 623.4141093621582, 621.1797525930317, 627.3504736083291, 630.263355750898, 619.2083695550912, 628.0118152049953, 616.3630683206337, 628.6137434324022, 630.6261791319201, 625.5321581663248, 624.7640133648756, 628.7488414060786, 629.4456729725185, 628.535989335185, 589.3853028791469, 593.7395950109285, 592.027308909153, 627.9989155299752, 628.7030280260614, 624.527116387152, 549.5031662529038, 625.9660827642406, 625.7934864282562, 625.8957887325869, 629.6633697600764, 628.9158359405211, 593.3770887642039, 622.6212135243509, 618.9033501749217, 627.7080866073674, 630.7064232440729, 631.0887739112808, 628.857195627044, 620.1155887534757, 631.2715680598333, 628.8413772577784, 627.240390811162, 628.1640246992465, 603.2692370680206, 598.765936239272, 624.861309786596, 626.5419137576723, 629.3321900418861, 628.786440669819, 630.3080719373304, 627.2858357113412, 620.6510299790699, 626.3260585323658, 622.6285715569708, 629.7429737082475, 622.6399916131421, 625.5138505108547, 630.6769374179154, 629.3654438337161, 629.604565934653, 628.5949732094417, 626.5473025237394, 628.5479404490927, 626.2532400080506, 624.7190315534012]
Elapsed: 0.07652264257999275~0.004923409893922246
Time per graph: 0.0015727972753208782~0.00010548178216669918
Speed: 638.7046191991886~43.33111108001529
Total Time: 0.0778
best val loss: 0.3703863322734833 test_score: 0.8958

Testing...
Test loss: 0.3536 score: 0.8958 time: 0.07s
test Score 0.8958
Epoch Time List: [0.3768184391083196, 0.3662411590339616, 0.36470814992208034, 0.36547916010022163, 0.3644243380986154, 0.3653993421467021, 0.3671741030411795, 0.3645067849429324, 0.364803500007838, 0.3645692120771855, 0.36522559297736734, 0.3947337060235441, 0.36995676904916763, 0.3746724030934274, 0.36942692508455366, 0.3669546360615641, 0.3952438960550353, 0.36785596900153905, 0.3729710760526359, 0.36117926705628633, 0.36155636806506664, 0.3621771589387208, 0.360922793042846, 0.3618941028835252, 0.3624093937687576, 0.3614235569257289, 0.3624262398807332, 0.368403222062625, 0.3753424599999562, 0.3884230100084096, 0.37567142595071346, 0.3620737729361281, 0.36207374301739037, 0.36222367896698415, 0.36041269707493484, 0.36257877107709646, 0.3630188190145418, 0.362114483024925, 0.3620332330465317, 0.3621938119176775, 0.36413962801452726, 0.38132632488850504, 0.36211251199711114, 0.35889168025460094, 0.36085039691533893, 0.36102206795476377, 0.36056139692664146, 0.3598739851731807, 0.3638338939053938, 0.3610993321053684, 0.43954395793844014, 0.35797531995922327, 0.3619519539643079, 0.3660388808930293, 0.3804880640236661, 0.3907459101174027, 0.3754841798217967, 0.37481972703244537, 0.375848327181302, 0.37804631900507957, 0.37874079099856317, 0.37457832193467766, 0.37688539491500705, 0.3791178520768881, 0.46447021001949906, 0.3759179910412058, 0.37825432687532157, 0.3895256470423192, 0.3748272279044613, 0.3744646809063852, 0.3736355929868296, 0.377084800042212, 0.3757579190423712, 0.3746438630623743, 0.37363560986705124, 0.372343922033906, 0.37488092482089996, 0.37364008999429643, 0.37335517501924187, 0.37479882617481053, 0.3799884169129655, 0.3740142280003056, 0.3750604799715802, 0.37341895210556686, 0.37400380813051015, 0.37442802090663463, 0.37909160112030804, 0.3738373698433861, 0.36665824707597494, 0.3564791578100994, 0.3559657899895683, 0.3567651209887117, 0.35546060907654464, 0.3587199919857085, 0.3613212970085442, 0.3559026127913967, 0.35547461500391364, 0.35679357207845896, 0.35562594211660326, 0.3559393968898803, 0.35563182504847646, 0.355359049863182, 0.3561411000555381, 0.35638316918630153, 0.3560319640673697, 0.3553261240012944, 0.357397417887114, 0.3649169929558411, 0.3640331899514422, 0.35865470906719565, 0.3557849880307913, 0.35653063387144357, 0.3578461529687047, 0.35693567199632525, 0.3576572489691898, 0.36108792503364384, 0.36485599796287715, 0.3568174581741914, 0.35719852708280087, 0.3554939990863204, 0.3555919600185007, 0.3577415399486199, 0.37932725495193154, 0.37396188208367676, 0.3568311710841954, 0.35615878796670586, 0.3553772869054228, 0.356663974118419, 0.3569512100657448, 0.3623329170513898, 0.3567134770564735, 0.35588900197762996, 0.3560708040604368, 0.3558388688834384, 0.35539846401661634, 0.35506802692543715, 0.35785572801250964, 0.3551938469754532, 0.3558346569770947, 0.3567281310679391, 0.3573062620125711, 0.35633734217844903, 0.35719243297353387, 0.36495583003852516, 0.35629133984912187, 0.35603622486814857, 0.35532194084953517, 0.3549368919339031, 0.35552274202927947, 0.35622274910565466, 0.35601942089851946, 0.35744348797015846, 0.35598683916032314, 0.35718875692691654, 0.35873309697490185, 0.35605696495622396, 0.35711750702466816, 0.36778680805582553, 0.3746301210485399, 0.35608541895635426, 0.357910995837301, 0.35708844696637243, 0.3562629130901769, 0.35853427497204393, 0.3558561661047861, 0.35665254306513816, 0.35672815900761634, 0.35783749306574464, 0.3585844950284809, 0.35676082398276776, 0.35663532407488674, 0.3674978780327365, 0.35698872501961887, 0.3575576690491289, 0.3563950009411201, 0.3671488960972056, 0.3581526599591598, 0.35789208207279444, 0.35904146102257073, 0.3650560190435499, 0.3583679710282013, 0.3588467559311539, 0.3577733850106597, 0.35803399002179503, 0.3596492388751358, 0.3630214720033109, 0.3584089271025732, 0.3590178390732035, 0.3580057519720867, 0.35742294089868665, 0.3590500259306282, 0.3575695900944993, 0.359393117018044, 0.3581431108759716, 0.361665018950589, 0.3581663209479302, 0.3579720639390871, 0.3581961760064587, 0.35981893294956535, 0.36249827698338777, 0.35711683914996684, 0.3583982839481905, 0.36124748713336885, 0.3647438760381192, 0.3627037530532107, 0.44767773908097297, 0.3577309800311923, 0.395929052028805, 0.4604720309143886, 0.38379152200650424, 0.3863287188578397, 0.3853745039086789, 0.4169308899436146, 0.38363093591760844, 0.38630883605219424, 0.3872847850434482, 0.38329838297795504, 0.3809235979570076, 0.4004563521593809, 0.3850569298956543, 0.38246997410897166, 0.39018401107750833, 0.38898225710727274, 0.3872805458959192, 0.3884892800124362, 0.3880515299970284, 0.3894568970426917, 0.3915348220616579, 0.39231247815769166, 0.3785371989943087, 0.37131609697826207, 0.3801760277710855, 0.3705733789829537, 0.3696279738796875, 0.36782258993480355, 0.3693529049633071, 0.3683107600081712, 0.362051663803868, 0.36998938594479114, 0.36765133403241634, 0.3670952169923112, 0.3667222169460729, 0.3683517079334706, 0.36801191815175116, 0.37124863313511014, 0.3695387369953096, 0.36129465512931347, 0.37016075919382274, 0.3682769809383899, 0.367714078980498, 0.36979876505210996, 0.37157509103417397, 0.3694003320997581, 0.3703574630199, 0.36938518995884806, 0.3711136030033231, 0.3694275039015338, 0.3707065558992326, 0.3817053029779345, 0.3679409821052104, 0.36800178105477244, 0.3688371068565175, 0.3687604898586869, 0.36792194296140224, 0.36802657216321677, 0.37379427801351994, 0.3913491328712553, 0.3922315458767116, 0.37947779009118676, 0.3682768710423261, 0.37030712491832674, 0.38070805999450386, 0.3714725690660998, 0.3709926650626585, 0.3711411979747936, 0.369669466977939, 0.36989942693617195, 0.37853029801044613, 0.37713587598409504, 0.3799515430582687, 0.36928653391078115, 0.3693437408655882, 0.367737818043679, 0.3679446999449283, 0.3721684318734333, 0.37992778106126934, 0.36764792702160776, 0.3689134500455111, 0.37011010595597327, 0.377968588960357, 0.3963995670201257, 0.3731269988929853, 0.3695952189154923, 0.3700301341013983, 0.37011723092291504, 0.3657924379222095, 0.37014217185787857, 0.37099020509049296, 0.37269334692973644, 0.36972637905273587, 0.36708991904743016, 0.3706423758994788, 0.3692282821284607, 0.36801106482744217, 0.36852636805269867, 0.3682860078988597, 0.36763955699279904, 0.3696195709053427, 0.3688001580303535, 0.3704426670446992, 0.36921845201868564]
Total Epoch List: [109, 98, 104]
Total Time List: [0.06989478797186166, 0.08152352599427104, 0.07777185598388314]
========================training times:2========================
========================k_idx:0========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: GPSTransformer
GPS(
  (x_lin): Linear(in_features=14887, out_features=64, bias=True)
  (pe_lin): Linear(in_features=20, out_features=8, bias=True)
  (pe_norm): BatchNorm1d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (convs): ModuleList(
    (0-1): 2 x GPSConv(64, conv=GINEConv(nn=Sequential(
      (0): Linear(in_features=64, out_features=64, bias=True)
      (1): ReLU()
      (2): Linear(in_features=64, out_features=64, bias=True)
    )), heads=1, attn_type=multihead)
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=32, bias=True)
    (1): ReLU()
    (2): Linear(in_features=32, out_features=16, bias=True)
    (3): ReLU()
    (4): Linear(in_features=16, out_features=2, bias=True)
  )
)
Total number of parameters: 1039906
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x737bd9adac80>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.7632;  Loss pred: 0.7632; Loss self: 0.0000; time: 0.23s
Val loss: 1.4173 score: 0.4490 time: 0.07s
Test loss: 1.1620 score: 0.4490 time: 0.07s
Epoch 2/1000, LR 0.000000
Train loss: 0.7551;  Loss pred: 0.7551; Loss self: 0.0000; time: 0.23s
Val loss: 1.2882 score: 0.4490 time: 0.08s
Test loss: 1.0643 score: 0.4286 time: 0.07s
Epoch 3/1000, LR 0.000030
Train loss: 0.7048;  Loss pred: 0.7048; Loss self: 0.0000; time: 0.23s
Val loss: 1.1500 score: 0.4286 time: 0.08s
Test loss: 0.9707 score: 0.4082 time: 0.07s
Epoch 4/1000, LR 0.000060
Train loss: 0.6366;  Loss pred: 0.6366; Loss self: 0.0000; time: 0.23s
Val loss: 1.0586 score: 0.3061 time: 0.07s
Test loss: 0.9082 score: 0.3469 time: 0.07s
Epoch 5/1000, LR 0.000090
Train loss: 0.5580;  Loss pred: 0.5580; Loss self: 0.0000; time: 0.22s
Val loss: 0.9857 score: 0.2653 time: 0.07s
Test loss: 0.8466 score: 0.2857 time: 0.07s
Epoch 6/1000, LR 0.000120
Train loss: 0.4839;  Loss pred: 0.4839; Loss self: 0.0000; time: 0.23s
Val loss: 0.9062 score: 0.2653 time: 0.07s
Test loss: 0.7760 score: 0.4082 time: 0.07s
Epoch 7/1000, LR 0.000150
Train loss: 0.4248;  Loss pred: 0.4248; Loss self: 0.0000; time: 0.23s
Val loss: 0.8039 score: 0.3061 time: 0.07s
Test loss: 0.7257 score: 0.4490 time: 0.07s
Epoch 8/1000, LR 0.000180
Train loss: 0.3761;  Loss pred: 0.3761; Loss self: 0.0000; time: 0.22s
Val loss: 0.7139 score: 0.3878 time: 0.07s
Test loss: 0.6842 score: 0.4694 time: 0.07s
Epoch 9/1000, LR 0.000210
Train loss: 0.3260;  Loss pred: 0.3260; Loss self: 0.0000; time: 0.23s
Val loss: 0.6549 score: 0.4490 time: 0.08s
Test loss: 0.6514 score: 0.4694 time: 0.07s
Epoch 10/1000, LR 0.000240
Train loss: 0.2812;  Loss pred: 0.2812; Loss self: 0.0000; time: 0.23s
Val loss: 0.6230 score: 0.4694 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6314 score: 0.5102 time: 0.07s
Epoch 11/1000, LR 0.000270
Train loss: 0.2459;  Loss pred: 0.2459; Loss self: 0.0000; time: 0.23s
Val loss: 0.6056 score: 0.4694 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6190 score: 0.5102 time: 0.07s
Epoch 12/1000, LR 0.000270
Train loss: 0.2089;  Loss pred: 0.2089; Loss self: 0.0000; time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.5985 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6116 score: 0.5102 time: 0.07s
Epoch 13/1000, LR 0.000270
Train loss: 0.1798;  Loss pred: 0.1798; Loss self: 0.0000; time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.5948 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6076 score: 0.5102 time: 0.07s
Epoch 14/1000, LR 0.000270
Train loss: 0.1603;  Loss pred: 0.1603; Loss self: 0.0000; time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.5920 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6033 score: 0.5102 time: 0.07s
Epoch 15/1000, LR 0.000270
Train loss: 0.1399;  Loss pred: 0.1399; Loss self: 0.0000; time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.5893 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6007 score: 0.5102 time: 0.07s
Epoch 16/1000, LR 0.000270
Train loss: 0.1247;  Loss pred: 0.1247; Loss self: 0.0000; time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.5875 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.5990 score: 0.5102 time: 0.07s
Epoch 17/1000, LR 0.000270
Train loss: 0.1142;  Loss pred: 0.1142; Loss self: 0.0000; time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.5853 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.5968 score: 0.5102 time: 0.07s
Epoch 18/1000, LR 0.000270
Train loss: 0.0995;  Loss pred: 0.0995; Loss self: 0.0000; time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.5822 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.5949 score: 0.5102 time: 0.07s
Epoch 19/1000, LR 0.000270
Train loss: 0.0881;  Loss pred: 0.0881; Loss self: 0.0000; time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.5788 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.5935 score: 0.5102 time: 0.07s
Epoch 20/1000, LR 0.000270
Train loss: 0.0780;  Loss pred: 0.0780; Loss self: 0.0000; time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.5760 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.5917 score: 0.5102 time: 0.07s
Epoch 21/1000, LR 0.000270
Train loss: 0.0720;  Loss pred: 0.0720; Loss self: 0.0000; time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.5734 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.5898 score: 0.5102 time: 0.07s
Epoch 22/1000, LR 0.000270
Train loss: 0.0645;  Loss pred: 0.0645; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.5716 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.5884 score: 0.5102 time: 0.07s
Epoch 23/1000, LR 0.000270
Train loss: 0.0531;  Loss pred: 0.0531; Loss self: 0.0000; time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.5710 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.5872 score: 0.5102 time: 0.07s
Epoch 24/1000, LR 0.000270
Train loss: 0.0507;  Loss pred: 0.0507; Loss self: 0.0000; time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.5704 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.5867 score: 0.5102 time: 0.07s
Epoch 25/1000, LR 0.000270
Train loss: 0.0479;  Loss pred: 0.0479; Loss self: 0.0000; time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.5708 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.5868 score: 0.5102 time: 0.07s
     INFO: Early stopping counter 1 of 20
Epoch 26/1000, LR 0.000270
Train loss: 0.0435;  Loss pred: 0.0435; Loss self: 0.0000; time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.5725 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.5873 score: 0.5102 time: 0.07s
     INFO: Early stopping counter 2 of 20
Epoch 27/1000, LR 0.000270
Train loss: 0.0391;  Loss pred: 0.0391; Loss self: 0.0000; time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.5751 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.5881 score: 0.5102 time: 0.07s
     INFO: Early stopping counter 3 of 20
Epoch 28/1000, LR 0.000270
Train loss: 0.0381;  Loss pred: 0.0381; Loss self: 0.0000; time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.5773 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.5894 score: 0.5102 time: 0.07s
     INFO: Early stopping counter 4 of 20
Epoch 29/1000, LR 0.000270
Train loss: 0.0329;  Loss pred: 0.0329; Loss self: 0.0000; time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.5791 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.5914 score: 0.5102 time: 0.07s
     INFO: Early stopping counter 5 of 20
Epoch 30/1000, LR 0.000270
Train loss: 0.0299;  Loss pred: 0.0299; Loss self: 0.0000; time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.5806 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.5932 score: 0.5102 time: 0.07s
     INFO: Early stopping counter 6 of 20
Epoch 31/1000, LR 0.000270
Train loss: 0.0302;  Loss pred: 0.0302; Loss self: 0.0000; time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.5808 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.5940 score: 0.5102 time: 0.07s
     INFO: Early stopping counter 7 of 20
Epoch 32/1000, LR 0.000270
Train loss: 0.0242;  Loss pred: 0.0242; Loss self: 0.0000; time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.5805 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.5941 score: 0.5102 time: 0.07s
     INFO: Early stopping counter 8 of 20
Epoch 33/1000, LR 0.000270
Train loss: 0.0231;  Loss pred: 0.0231; Loss self: 0.0000; time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.5785 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.5924 score: 0.5102 time: 0.07s
     INFO: Early stopping counter 9 of 20
Epoch 34/1000, LR 0.000270
Train loss: 0.0200;  Loss pred: 0.0200; Loss self: 0.0000; time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.5747 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.5878 score: 0.5102 time: 0.07s
     INFO: Early stopping counter 10 of 20
Epoch 35/1000, LR 0.000270
Train loss: 0.0197;  Loss pred: 0.0197; Loss self: 0.0000; time: 0.23s
Val loss: 0.5693 score: 0.5102 time: 0.08s
Test loss: 0.5819 score: 0.5306 time: 0.07s
Epoch 36/1000, LR 0.000270
Train loss: 0.0187;  Loss pred: 0.0187; Loss self: 0.0000; time: 0.23s
Val loss: 0.5637 score: 0.5102 time: 0.09s
Test loss: 0.5769 score: 0.5306 time: 0.07s
Epoch 37/1000, LR 0.000270
Train loss: 0.0189;  Loss pred: 0.0189; Loss self: 0.0000; time: 0.23s
Val loss: 0.5594 score: 0.5102 time: 0.08s
Test loss: 0.5720 score: 0.5306 time: 0.07s
Epoch 38/1000, LR 0.000270
Train loss: 0.0153;  Loss pred: 0.0153; Loss self: 0.0000; time: 0.25s
Val loss: 0.5555 score: 0.5306 time: 0.10s
Test loss: 0.5674 score: 0.5306 time: 0.09s
Epoch 39/1000, LR 0.000269
Train loss: 0.0159;  Loss pred: 0.0159; Loss self: 0.0000; time: 0.25s
Val loss: 0.5520 score: 0.5510 time: 0.08s
Test loss: 0.5629 score: 0.5510 time: 0.07s
Epoch 40/1000, LR 0.000269
Train loss: 0.0126;  Loss pred: 0.0126; Loss self: 0.0000; time: 0.24s
Val loss: 0.5500 score: 0.5714 time: 0.08s
Test loss: 0.5599 score: 0.5510 time: 0.07s
Epoch 41/1000, LR 0.000269
Train loss: 0.0124;  Loss pred: 0.0124; Loss self: 0.0000; time: 0.24s
Val loss: 0.5499 score: 0.5918 time: 0.08s
Test loss: 0.5585 score: 0.5510 time: 0.07s
Epoch 42/1000, LR 0.000269
Train loss: 0.0114;  Loss pred: 0.0114; Loss self: 0.0000; time: 0.24s
Val loss: 0.5454 score: 0.5918 time: 0.08s
Test loss: 0.5540 score: 0.5918 time: 0.16s
Epoch 43/1000, LR 0.000269
Train loss: 0.0109;  Loss pred: 0.0109; Loss self: 0.0000; time: 0.24s
Val loss: 0.5380 score: 0.5918 time: 0.09s
Test loss: 0.5478 score: 0.6327 time: 0.07s
Epoch 44/1000, LR 0.000269
Train loss: 0.0097;  Loss pred: 0.0097; Loss self: 0.0000; time: 0.24s
Val loss: 0.5286 score: 0.6122 time: 0.08s
Test loss: 0.5407 score: 0.6531 time: 0.07s
Epoch 45/1000, LR 0.000269
Train loss: 0.0092;  Loss pred: 0.0092; Loss self: 0.0000; time: 0.24s
Val loss: 0.5190 score: 0.6327 time: 0.08s
Test loss: 0.5336 score: 0.6735 time: 0.07s
Epoch 46/1000, LR 0.000269
Train loss: 0.0085;  Loss pred: 0.0085; Loss self: 0.0000; time: 0.24s
Val loss: 0.5086 score: 0.6531 time: 0.08s
Test loss: 0.5259 score: 0.7143 time: 0.07s
Epoch 47/1000, LR 0.000269
Train loss: 0.0085;  Loss pred: 0.0085; Loss self: 0.0000; time: 0.24s
Val loss: 0.4985 score: 0.6735 time: 0.08s
Test loss: 0.5187 score: 0.7143 time: 0.07s
Epoch 48/1000, LR 0.000269
Train loss: 0.0084;  Loss pred: 0.0084; Loss self: 0.0000; time: 0.24s
Val loss: 0.4899 score: 0.7347 time: 0.08s
Test loss: 0.5125 score: 0.7347 time: 0.07s
Epoch 49/1000, LR 0.000269
Train loss: 0.0063;  Loss pred: 0.0063; Loss self: 0.0000; time: 0.24s
Val loss: 0.4817 score: 0.7551 time: 0.08s
Test loss: 0.5063 score: 0.7551 time: 0.07s
Epoch 50/1000, LR 0.000269
Train loss: 0.0063;  Loss pred: 0.0063; Loss self: 0.0000; time: 0.24s
Val loss: 0.4736 score: 0.7755 time: 0.08s
Test loss: 0.4997 score: 0.7551 time: 0.07s
Epoch 51/1000, LR 0.000269
Train loss: 0.0061;  Loss pred: 0.0061; Loss self: 0.0000; time: 0.25s
Val loss: 0.4687 score: 0.8367 time: 0.11s
Test loss: 0.4941 score: 0.7551 time: 0.07s
Epoch 52/1000, LR 0.000269
Train loss: 0.0058;  Loss pred: 0.0058; Loss self: 0.0000; time: 0.23s
Val loss: 0.4646 score: 0.8367 time: 0.08s
Test loss: 0.4898 score: 0.7551 time: 0.07s
Epoch 53/1000, LR 0.000269
Train loss: 0.0055;  Loss pred: 0.0055; Loss self: 0.0000; time: 0.23s
Val loss: 0.4623 score: 0.8367 time: 0.08s
Test loss: 0.4863 score: 0.7347 time: 0.07s
Epoch 54/1000, LR 0.000269
Train loss: 0.0056;  Loss pred: 0.0056; Loss self: 0.0000; time: 0.24s
Val loss: 0.4559 score: 0.8571 time: 0.08s
Test loss: 0.4806 score: 0.7551 time: 0.07s
Epoch 55/1000, LR 0.000269
Train loss: 0.0043;  Loss pred: 0.0043; Loss self: 0.0000; time: 0.24s
Val loss: 0.4439 score: 0.8571 time: 0.08s
Test loss: 0.4717 score: 0.7755 time: 0.07s
Epoch 56/1000, LR 0.000269
Train loss: 0.0041;  Loss pred: 0.0041; Loss self: 0.0000; time: 0.23s
Val loss: 0.4324 score: 0.8776 time: 0.08s
Test loss: 0.4607 score: 0.7959 time: 0.07s
Epoch 57/1000, LR 0.000269
Train loss: 0.0041;  Loss pred: 0.0041; Loss self: 0.0000; time: 0.23s
Val loss: 0.4255 score: 0.9388 time: 0.08s
Test loss: 0.4505 score: 0.8163 time: 0.07s
Epoch 58/1000, LR 0.000269
Train loss: 0.0035;  Loss pred: 0.0035; Loss self: 0.0000; time: 0.24s
Val loss: 0.4182 score: 0.9388 time: 0.08s
Test loss: 0.4425 score: 0.8163 time: 0.07s
Epoch 59/1000, LR 0.000268
Train loss: 0.0041;  Loss pred: 0.0041; Loss self: 0.0000; time: 0.24s
Val loss: 0.4109 score: 0.9388 time: 0.08s
Test loss: 0.4361 score: 0.8163 time: 0.07s
Epoch 60/1000, LR 0.000268
Train loss: 0.0037;  Loss pred: 0.0037; Loss self: 0.0000; time: 0.24s
Val loss: 0.4062 score: 0.9592 time: 0.08s
Test loss: 0.4319 score: 0.8163 time: 0.07s
Epoch 61/1000, LR 0.000268
Train loss: 0.0030;  Loss pred: 0.0030; Loss self: 0.0000; time: 0.24s
Val loss: 0.4028 score: 0.9592 time: 0.08s
Test loss: 0.4286 score: 0.8367 time: 0.07s
Epoch 62/1000, LR 0.000268
Train loss: 0.0026;  Loss pred: 0.0026; Loss self: 0.0000; time: 0.24s
Val loss: 0.3956 score: 0.9592 time: 0.08s
Test loss: 0.4230 score: 0.8367 time: 0.07s
Epoch 63/1000, LR 0.000268
Train loss: 0.0036;  Loss pred: 0.0036; Loss self: 0.0000; time: 0.24s
Val loss: 0.3834 score: 0.9592 time: 0.08s
Test loss: 0.4141 score: 0.8367 time: 0.07s
Epoch 64/1000, LR 0.000268
Train loss: 0.0036;  Loss pred: 0.0036; Loss self: 0.0000; time: 0.24s
Val loss: 0.3635 score: 0.9592 time: 0.08s
Test loss: 0.4011 score: 0.8776 time: 0.07s
Epoch 65/1000, LR 0.000268
Train loss: 0.0025;  Loss pred: 0.0025; Loss self: 0.0000; time: 0.23s
Val loss: 0.3428 score: 0.9592 time: 0.08s
Test loss: 0.3883 score: 0.8980 time: 0.07s
Epoch 66/1000, LR 0.000268
Train loss: 0.0025;  Loss pred: 0.0025; Loss self: 0.0000; time: 0.24s
Val loss: 0.3227 score: 0.9388 time: 0.08s
Test loss: 0.3757 score: 0.8980 time: 0.07s
Epoch 67/1000, LR 0.000268
Train loss: 0.0022;  Loss pred: 0.0022; Loss self: 0.0000; time: 0.24s
Val loss: 0.3065 score: 0.9388 time: 0.08s
Test loss: 0.3653 score: 0.8776 time: 0.07s
Epoch 68/1000, LR 0.000268
Train loss: 0.0021;  Loss pred: 0.0021; Loss self: 0.0000; time: 0.24s
Val loss: 0.2935 score: 0.9388 time: 0.08s
Test loss: 0.3577 score: 0.8571 time: 0.07s
Epoch 69/1000, LR 0.000268
Train loss: 0.0029;  Loss pred: 0.0029; Loss self: 0.0000; time: 0.24s
Val loss: 0.2841 score: 0.9388 time: 0.08s
Test loss: 0.3520 score: 0.8571 time: 0.07s
Epoch 70/1000, LR 0.000268
Train loss: 0.0022;  Loss pred: 0.0022; Loss self: 0.0000; time: 0.24s
Val loss: 0.2775 score: 0.9388 time: 0.08s
Test loss: 0.3489 score: 0.8776 time: 0.07s
Epoch 71/1000, LR 0.000268
Train loss: 0.0017;  Loss pred: 0.0017; Loss self: 0.0000; time: 0.24s
Val loss: 0.2730 score: 0.9388 time: 0.08s
Test loss: 0.3472 score: 0.8776 time: 0.07s
Epoch 72/1000, LR 0.000267
Train loss: 0.0020;  Loss pred: 0.0020; Loss self: 0.0000; time: 0.24s
Val loss: 0.2690 score: 0.9388 time: 0.08s
Test loss: 0.3460 score: 0.8571 time: 0.07s
Epoch 73/1000, LR 0.000267
Train loss: 0.0022;  Loss pred: 0.0022; Loss self: 0.0000; time: 0.24s
Val loss: 0.2673 score: 0.9388 time: 0.08s
Test loss: 0.3455 score: 0.8571 time: 0.07s
Epoch 74/1000, LR 0.000267
Train loss: 0.0018;  Loss pred: 0.0018; Loss self: 0.0000; time: 0.23s
Val loss: 0.2658 score: 0.9388 time: 0.07s
Test loss: 0.3446 score: 0.8776 time: 0.07s
Epoch 75/1000, LR 0.000267
Train loss: 0.0015;  Loss pred: 0.0015; Loss self: 0.0000; time: 0.23s
Val loss: 0.2642 score: 0.9388 time: 0.07s
Test loss: 0.3441 score: 0.8980 time: 0.07s
Epoch 76/1000, LR 0.000267
Train loss: 0.0018;  Loss pred: 0.0018; Loss self: 0.0000; time: 0.22s
Val loss: 0.2581 score: 0.9388 time: 0.07s
Test loss: 0.3422 score: 0.8980 time: 0.07s
Epoch 77/1000, LR 0.000267
Train loss: 0.0018;  Loss pred: 0.0018; Loss self: 0.0000; time: 0.22s
Val loss: 0.2524 score: 0.9388 time: 0.07s
Test loss: 0.3401 score: 0.8980 time: 0.07s
Epoch 78/1000, LR 0.000267
Train loss: 0.0018;  Loss pred: 0.0018; Loss self: 0.0000; time: 0.22s
Val loss: 0.2453 score: 0.9388 time: 0.07s
Test loss: 0.3367 score: 0.8980 time: 0.07s
Epoch 79/1000, LR 0.000267
Train loss: 0.0013;  Loss pred: 0.0013; Loss self: 0.0000; time: 0.23s
Val loss: 0.2382 score: 0.9388 time: 0.07s
Test loss: 0.3320 score: 0.8776 time: 0.07s
Epoch 80/1000, LR 0.000267
Train loss: 0.0016;  Loss pred: 0.0016; Loss self: 0.0000; time: 0.22s
Val loss: 0.2321 score: 0.9388 time: 0.07s
Test loss: 0.3280 score: 0.9184 time: 0.07s
Epoch 81/1000, LR 0.000267
Train loss: 0.0013;  Loss pred: 0.0013; Loss self: 0.0000; time: 0.22s
Val loss: 0.2256 score: 0.9388 time: 0.07s
Test loss: 0.3245 score: 0.9184 time: 0.06s
Epoch 82/1000, LR 0.000267
Train loss: 0.0016;  Loss pred: 0.0016; Loss self: 0.0000; time: 0.22s
Val loss: 0.2201 score: 0.9388 time: 0.07s
Test loss: 0.3211 score: 0.9184 time: 0.07s
Epoch 83/1000, LR 0.000266
Train loss: 0.0023;  Loss pred: 0.0023; Loss self: 0.0000; time: 0.23s
Val loss: 0.2138 score: 0.9388 time: 0.07s
Test loss: 0.3191 score: 0.8980 time: 0.07s
Epoch 84/1000, LR 0.000266
Train loss: 0.0011;  Loss pred: 0.0011; Loss self: 0.0000; time: 0.22s
Val loss: 0.2090 score: 0.9388 time: 0.07s
Test loss: 0.3192 score: 0.8980 time: 0.07s
Epoch 85/1000, LR 0.000266
Train loss: 0.0012;  Loss pred: 0.0012; Loss self: 0.0000; time: 0.22s
Val loss: 0.2043 score: 0.9388 time: 0.07s
Test loss: 0.3185 score: 0.8980 time: 0.07s
Epoch 86/1000, LR 0.000266
Train loss: 0.0014;  Loss pred: 0.0014; Loss self: 0.0000; time: 0.22s
Val loss: 0.2003 score: 0.9388 time: 0.07s
Test loss: 0.3171 score: 0.8980 time: 0.07s
Epoch 87/1000, LR 0.000266
Train loss: 0.0012;  Loss pred: 0.0012; Loss self: 0.0000; time: 0.22s
Val loss: 0.1976 score: 0.9388 time: 0.07s
Test loss: 0.3160 score: 0.8980 time: 0.07s
Epoch 88/1000, LR 0.000266
Train loss: 0.0011;  Loss pred: 0.0011; Loss self: 0.0000; time: 0.23s
Val loss: 0.1954 score: 0.9388 time: 0.07s
Test loss: 0.3152 score: 0.8980 time: 0.07s
Epoch 89/1000, LR 0.000266
Train loss: 0.0017;  Loss pred: 0.0017; Loss self: 0.0000; time: 0.22s
Val loss: 0.1944 score: 0.9184 time: 0.07s
Test loss: 0.3146 score: 0.8980 time: 0.07s
Epoch 90/1000, LR 0.000266
Train loss: 0.0011;  Loss pred: 0.0011; Loss self: 0.0000; time: 0.22s
Val loss: 0.1951 score: 0.9184 time: 0.07s
Test loss: 0.3155 score: 0.9184 time: 0.07s
     INFO: Early stopping counter 1 of 20
Epoch 91/1000, LR 0.000266
Train loss: 0.0010;  Loss pred: 0.0010; Loss self: 0.0000; time: 0.22s
Val loss: 0.1923 score: 0.9184 time: 0.07s
Test loss: 0.3181 score: 0.9184 time: 0.07s
Epoch 92/1000, LR 0.000266
Train loss: 0.0008;  Loss pred: 0.0008; Loss self: 0.0000; time: 0.22s
Val loss: 0.1884 score: 0.9184 time: 0.07s
Test loss: 0.3224 score: 0.9388 time: 0.07s
Epoch 93/1000, LR 0.000265
Train loss: 0.0010;  Loss pred: 0.0010; Loss self: 0.0000; time: 0.22s
Val loss: 0.1851 score: 0.9184 time: 0.08s
Test loss: 0.3272 score: 0.9388 time: 0.07s
Epoch 94/1000, LR 0.000265
Train loss: 0.0008;  Loss pred: 0.0008; Loss self: 0.0000; time: 0.22s
Val loss: 0.1825 score: 0.9184 time: 0.07s
Test loss: 0.3328 score: 0.9388 time: 0.07s
Epoch 95/1000, LR 0.000265
Train loss: 0.0010;  Loss pred: 0.0010; Loss self: 0.0000; time: 0.23s
Val loss: 0.1803 score: 0.9184 time: 0.07s
Test loss: 0.3375 score: 0.9184 time: 0.07s
Epoch 96/1000, LR 0.000265
Train loss: 0.0011;  Loss pred: 0.0011; Loss self: 0.0000; time: 0.22s
Val loss: 0.1790 score: 0.9184 time: 0.07s
Test loss: 0.3412 score: 0.9184 time: 0.07s
Epoch 97/1000, LR 0.000265
Train loss: 0.0010;  Loss pred: 0.0010; Loss self: 0.0000; time: 0.22s
Val loss: 0.1777 score: 0.9184 time: 0.07s
Test loss: 0.3431 score: 0.9184 time: 0.07s
Epoch 98/1000, LR 0.000265
Train loss: 0.0012;  Loss pred: 0.0012; Loss self: 0.0000; time: 0.22s
Val loss: 0.1770 score: 0.9184 time: 0.07s
Test loss: 0.3450 score: 0.9184 time: 0.06s
Epoch 99/1000, LR 0.000265
Train loss: 0.0014;  Loss pred: 0.0014; Loss self: 0.0000; time: 0.23s
Val loss: 0.1792 score: 0.9184 time: 0.07s
Test loss: 0.3489 score: 0.8980 time: 0.07s
     INFO: Early stopping counter 1 of 20
Epoch 100/1000, LR 0.000265
Train loss: 0.0015;  Loss pred: 0.0015; Loss self: 0.0000; time: 0.23s
Val loss: 0.1831 score: 0.9184 time: 0.07s
Test loss: 0.3503 score: 0.9184 time: 0.07s
     INFO: Early stopping counter 2 of 20
Epoch 101/1000, LR 0.000265
Train loss: 0.0008;  Loss pred: 0.0008; Loss self: 0.0000; time: 0.22s
Val loss: 0.1867 score: 0.9184 time: 0.08s
Test loss: 0.3530 score: 0.9184 time: 0.07s
     INFO: Early stopping counter 3 of 20
Epoch 102/1000, LR 0.000264
Train loss: 0.0008;  Loss pred: 0.0008; Loss self: 0.0000; time: 0.22s
Val loss: 0.1907 score: 0.9184 time: 0.08s
Test loss: 0.3558 score: 0.9184 time: 0.07s
     INFO: Early stopping counter 4 of 20
Epoch 103/1000, LR 0.000264
Train loss: 0.0010;  Loss pred: 0.0010; Loss self: 0.0000; time: 0.23s
Val loss: 0.1958 score: 0.9184 time: 0.07s
Test loss: 0.3580 score: 0.9388 time: 0.07s
     INFO: Early stopping counter 5 of 20
Epoch 104/1000, LR 0.000264
Train loss: 0.0008;  Loss pred: 0.0008; Loss self: 0.0000; time: 0.22s
Val loss: 0.2004 score: 0.9184 time: 0.07s
Test loss: 0.3614 score: 0.9388 time: 0.06s
     INFO: Early stopping counter 6 of 20
Epoch 105/1000, LR 0.000264
Train loss: 0.0012;  Loss pred: 0.0012; Loss self: 0.0000; time: 0.23s
Val loss: 0.2054 score: 0.9184 time: 0.07s
Test loss: 0.3647 score: 0.9184 time: 0.07s
     INFO: Early stopping counter 7 of 20
Epoch 106/1000, LR 0.000264
Train loss: 0.0010;  Loss pred: 0.0010; Loss self: 0.0000; time: 0.22s
Val loss: 0.2092 score: 0.9184 time: 0.07s
Test loss: 0.3683 score: 0.9184 time: 0.07s
     INFO: Early stopping counter 8 of 20
Epoch 107/1000, LR 0.000264
Train loss: 0.0006;  Loss pred: 0.0006; Loss self: 0.0000; time: 0.22s
Val loss: 0.2117 score: 0.9184 time: 0.07s
Test loss: 0.3725 score: 0.8980 time: 0.07s
     INFO: Early stopping counter 9 of 20
Epoch 108/1000, LR 0.000264
Train loss: 0.0008;  Loss pred: 0.0008; Loss self: 0.0000; time: 0.23s
Val loss: 0.2141 score: 0.9184 time: 0.07s
Test loss: 0.3773 score: 0.9184 time: 0.07s
     INFO: Early stopping counter 10 of 20
Epoch 109/1000, LR 0.000264
Train loss: 0.0008;  Loss pred: 0.0008; Loss self: 0.0000; time: 0.22s
Val loss: 0.2151 score: 0.9184 time: 0.07s
Test loss: 0.3826 score: 0.9184 time: 0.07s
     INFO: Early stopping counter 11 of 20
Epoch 110/1000, LR 0.000263
Train loss: 0.0008;  Loss pred: 0.0008; Loss self: 0.0000; time: 0.22s
Val loss: 0.2150 score: 0.9184 time: 0.07s
Test loss: 0.3883 score: 0.9388 time: 0.07s
     INFO: Early stopping counter 12 of 20
Epoch 111/1000, LR 0.000263
Train loss: 0.0008;  Loss pred: 0.0008; Loss self: 0.0000; time: 0.22s
Val loss: 0.2146 score: 0.9184 time: 0.07s
Test loss: 0.3943 score: 0.9184 time: 0.07s
     INFO: Early stopping counter 13 of 20
Epoch 112/1000, LR 0.000263
Train loss: 0.0006;  Loss pred: 0.0006; Loss self: 0.0000; time: 0.22s
Val loss: 0.2139 score: 0.9184 time: 0.07s
Test loss: 0.4024 score: 0.9184 time: 0.07s
     INFO: Early stopping counter 14 of 20
Epoch 113/1000, LR 0.000263
Train loss: 0.0008;  Loss pred: 0.0008; Loss self: 0.0000; time: 0.22s
Val loss: 0.2134 score: 0.9184 time: 0.07s
Test loss: 0.4127 score: 0.9184 time: 0.07s
     INFO: Early stopping counter 15 of 20
Epoch 114/1000, LR 0.000263
Train loss: 0.0007;  Loss pred: 0.0007; Loss self: 0.0000; time: 0.23s
Val loss: 0.2120 score: 0.9184 time: 0.07s
Test loss: 0.4259 score: 0.8980 time: 0.07s
     INFO: Early stopping counter 16 of 20
Epoch 115/1000, LR 0.000263
Train loss: 0.0010;  Loss pred: 0.0010; Loss self: 0.0000; time: 0.23s
Val loss: 0.2110 score: 0.9184 time: 0.08s
Test loss: 0.4393 score: 0.8980 time: 0.07s
     INFO: Early stopping counter 17 of 20
Epoch 116/1000, LR 0.000263
Train loss: 0.0006;  Loss pred: 0.0006; Loss self: 0.0000; time: 0.23s
Val loss: 0.2104 score: 0.9184 time: 0.08s
Test loss: 0.4543 score: 0.8980 time: 0.07s
     INFO: Early stopping counter 18 of 20
Epoch 117/1000, LR 0.000262
Train loss: 0.0007;  Loss pred: 0.0007; Loss self: 0.0000; time: 0.22s
Val loss: 0.2119 score: 0.9184 time: 0.07s
Test loss: 0.4688 score: 0.8980 time: 0.07s
     INFO: Early stopping counter 19 of 20
Epoch 118/1000, LR 0.000262
Train loss: 0.0006;  Loss pred: 0.0006; Loss self: 0.0000; time: 0.22s
Val loss: 0.2142 score: 0.9388 time: 0.07s
Test loss: 0.4820 score: 0.8980 time: 0.07s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 097,   Train_Loss: 0.0012,   Val_Loss: 0.1770,   Val_Precision: 0.9545,   Val_Recall: 0.8750,   Val_accuracy: 0.9130,   Val_Score: 0.9184,   Val_Loss: 0.1770,   Test_Precision: 1.0000,   Test_Recall: 0.8400,   Test_accuracy: 0.9130,   Test_Score: 0.9184,   Test_loss: 0.3450


[0.06986879894975573, 0.07044279901310802, 0.0698356709908694, 0.07017734495457262, 0.07022880995646119, 0.06983176700305194, 0.07062705291900784, 0.07070833304896951, 0.07107204804196954, 0.07109356601722538, 0.07021677994634956, 0.07004082901403308, 0.07017405901569873, 0.07021321693900973, 0.07034074293915182, 0.07046229601837695, 0.07036221993621439, 0.07055566704366356, 0.0701295449398458, 0.07032404106575996, 0.07057988899759948, 0.07110155397094786, 0.0712132859043777, 0.07023007201496512, 0.0702593500027433, 0.07072017109021544, 0.07062843407038599, 0.07050140900537372, 0.07144992600660771, 0.07062201900407672, 0.07056065299548209, 0.0714041970204562, 0.07071025599725544, 0.07035042403731495, 0.07117568503599614, 0.07352192897815257, 0.07099213392939419, 0.09445665893144906, 0.07486826495733112, 0.07537927909288555, 0.07507923000957817, 0.1627832680242136, 0.07635356695391238, 0.07661106099840254, 0.0765218740561977, 0.0762040700064972, 0.07606740191113204, 0.07666937401518226, 0.07617391005624086, 0.07581361697521061, 0.0739690710324794, 0.07400462997611612, 0.07523844006936997, 0.07598071603570133, 0.07485129998531193, 0.07471864705439657, 0.07455711404327303, 0.07491577393375337, 0.07432279898785055, 0.07418989308644086, 0.07503695297054946, 0.07431367202661932, 0.07384985010139644, 0.07376762991771102, 0.07439579896163195, 0.07472316001076251, 0.07365212007425725, 0.0739020510809496, 0.07444329909048975, 0.07445200101938099, 0.07468660001177341, 0.07400844094809145, 0.07405844400636852, 0.07018386200070381, 0.06973629794083536, 0.07006018701940775, 0.07016773102805018, 0.06979875999968499, 0.07014613400679082, 0.07000476494431496, 0.06930494401603937, 0.06943141005467623, 0.06995669496245682, 0.06952244695276022, 0.07019475102424622, 0.06982359604444355, 0.07025930506642908, 0.07548314798623323, 0.07023526902776212, 0.06985194201115519, 0.06941753998398781, 0.06999840098433197, 0.06997724808752537, 0.06969873700290918, 0.06986509507987648, 0.06951155897695571, 0.06973908899817616, 0.06924264901317656, 0.06990781798958778, 0.06971417588647455, 0.07048172503709793, 0.06969214498531073, 0.06981410703156143, 0.06890083593316376, 0.07042855699546635, 0.06991626496892422, 0.0710193170234561, 0.07014241605065763, 0.0700207109330222, 0.06951653002761304, 0.06971525296103209, 0.0701925850007683, 0.06988221500068903, 0.07002617290709168, 0.06986425304785371, 0.07005094503983855, 0.06971384189091623, 0.06952565803658217]
[0.001425893856117464, 0.0014376081431246533, 0.0014252177753238653, 0.0014321907133586248, 0.0014332410195196162, 0.0014251381021031008, 0.0014413684269185274, 0.0014430272050810103, 0.0014504499600401946, 0.0014508891023923547, 0.0014329955091091748, 0.001429404673755777, 0.0014321236533816068, 0.001432922794673668, 0.0014355253661051393, 0.0014380060411913662, 0.0014359636721676405, 0.0014399115723196644, 0.001431215202853996, 0.0014351845115461216, 0.0014404058979101935, 0.0014510521218560788, 0.0014533323653954633, 0.0014332667758156145, 0.0014338642857702716, 0.0014432687977594988, 0.0014413966136813468, 0.0014388042654157902, 0.001458161755236892, 0.0014412656939607493, 0.00144001332643841, 0.001457228510621555, 0.0014430664489235804, 0.00143572293953704, 0.0014525650007346151, 0.0015004475301663791, 0.001448819059783555, 0.0019276869169683481, 0.0015279237746394106, 0.0015383526345486849, 0.0015322291838689422, 0.003322107510698237, 0.001558236060283926, 0.0015634910407837253, 0.0015616708991060756, 0.0015551851021734123, 0.0015523959573700416, 0.0015646811023506584, 0.0015545695929845072, 0.0015472166729634817, 0.0015095728782138654, 0.0015102985709411453, 0.0015354783687626524, 0.0015506268578714558, 0.0015275775507206516, 0.0015248703480489096, 0.0015215737559851638, 0.0015288933455868035, 0.0015167918160785825, 0.001514079450743691, 0.0015313663871540707, 0.0015166055515636594, 0.0015071397979876824, 0.001505461835055327, 0.0015182816114618766, 0.001524962449199235, 0.0015031044913113726, 0.0015082051241010123, 0.0015192510018467295, 0.001519428592232265, 0.0015242163267708859, 0.0015103763458794172, 0.0015113968164565004, 0.0014323237143000777, 0.0014231897538945991, 0.001429799735089954, 0.0014319945107765344, 0.0014244644897894896, 0.0014315537552406288, 0.0014286686723329583, 0.001414386612572232, 0.0014169675521362498, 0.0014276876522950372, 0.0014188254480155147, 0.001432545939270331, 0.0014249713478457866, 0.0014338633687026342, 0.0015404724078823108, 0.0014333728373012676, 0.0014255498369623507, 0.001416684489469139, 0.0014285387955986115, 0.0014281071038270484, 0.0014224232041410037, 0.0014258182669362547, 0.0014186032444276676, 0.001423246714248493, 0.0014131152859831952, 0.001426690163052812, 0.0014227382833974398, 0.0014384025517775087, 0.0014222886731696067, 0.0014247776945216619, 0.0014061395088400767, 0.0014373174897033948, 0.0014268625503862087, 0.0014493738168052264, 0.0014314778785848496, 0.0014289941006739224, 0.0014187046944410826, 0.001422760264510859, 0.0014325017347095572, 0.0014261676530752862, 0.0014291055695324832, 0.0014258010826092593, 0.0014296111232620112, 0.0014227314671615557, 0.0014188909803384117]
[701.3144742224213, 695.5998439369519, 701.647156886435, 698.2310321332163, 697.7193552101747, 701.686382901617, 693.7851428714031, 692.9876280079285, 689.4412268950582, 689.232552888509, 697.8388931739587, 699.5919478649027, 698.2637271849723, 697.8743053827536, 696.6090767961805, 695.4073705917918, 696.3964474745122, 694.4870915850915, 698.7069435860471, 696.7745205964507, 694.2487540844186, 689.155120576147, 688.0738527610592, 697.7068169538362, 697.4160734206447, 692.8716269293562, 693.7715757816208, 695.0215703669859, 685.7949719285708, 693.8345956545286, 694.4380177878655, 686.2341717246993, 692.9687823772254, 696.5132146752895, 688.437350131845, 666.4678236959834, 690.217314058109, 518.756438712926, 654.4829111229711, 650.0460151604808, 652.6438802548836, 301.01373805022376, 641.7512888373217, 639.5943269996186, 640.3397800217801, 643.0102748556899, 644.1655527718126, 639.1078658121938, 643.2648654089337, 646.3218872148249, 662.4390345322084, 662.1207350920342, 651.2628379166542, 644.900476812777, 654.6312490179232, 655.7934589517806, 657.2142796669992, 654.0678608396917, 659.2862576126871, 660.4673219155153, 653.0115904257406, 659.3672289864521, 663.5084557750978, 664.2479913569175, 658.6393409830938, 655.7538518572079, 665.2897425165414, 663.0397841912019, 658.2190821559093, 658.1421496951378, 656.0748513424867, 662.0866400140486, 661.6396098706359, 698.166196660831, 702.6469922675255, 699.3986468581114, 698.3266992118045, 702.0182020457261, 698.5417043119772, 699.9523538001574, 707.0202666733247, 705.7324626046512, 700.4333184450251, 704.8083338219524, 698.0578930050588, 701.7684962660892, 697.4165194727057, 649.1515166926625, 697.6551905942243, 701.4837181216067, 705.873472486962, 700.0159905219531, 700.2275930987215, 703.0256516406426, 701.3516541268353, 704.9187318075308, 702.6188713374427, 707.6563461729421, 700.9230356367032, 702.8699597595994, 695.2156743355662, 703.0921491988503, 701.8638794283821, 711.1669885621087, 695.7405076914218, 700.8383531611578, 689.9531290031471, 698.5787310863609, 699.7929519291883, 704.8683238437885, 702.8591006818687, 698.0794338812805, 701.1798352344279, 699.7383687526593, 701.3601070985088, 699.4909201029807, 702.8733271747105, 704.7757818303247]
Elapsed: 0.0726091868236219~0.008889646088347336
Time per graph: 0.0014818201392575896~0.0001814213487417824
Speed: 679.966074493987~43.638753120068465
Total Time: 0.0701
best val loss: 0.1770002394914627 test_score: 0.9184

Testing...
Test loss: 0.4319 score: 0.8163 time: 0.07s
test Score 0.8163
Epoch Time List: [0.36815647105686367, 0.3788550199242309, 0.36802804900798947, 0.36611902993172407, 0.3633534871041775, 0.3661565751535818, 0.36650097998790443, 0.36484617996029556, 0.37532215600367635, 0.3735433929832652, 0.36656685499474406, 0.37320137093774974, 0.36720387905370444, 0.36744041997008026, 0.36804702295921743, 0.3674572000745684, 0.3725368281593546, 0.3751593859633431, 0.3646737620001659, 0.3672148420009762, 0.3677718300605193, 0.4030144870048389, 0.3819682290777564, 0.37425475392956287, 0.36638721707277, 0.3662500879727304, 0.36682257684879005, 0.3670935711124912, 0.3695377130061388, 0.3693309429800138, 0.36751450994051993, 0.36897220311220735, 0.36459795909468085, 0.37254487886093557, 0.3713111609686166, 0.38878083787858486, 0.37870104401372373, 0.44105921604204923, 0.3979939449345693, 0.3938217388931662, 0.39171819598414004, 0.47627057309728116, 0.3948625160846859, 0.3884071591310203, 0.3898119169753045, 0.3883922391105443, 0.39532119780778885, 0.3959695460507646, 0.39392751397099346, 0.39086809707805514, 0.4346158679109067, 0.38000225997529924, 0.3828889229334891, 0.38763458491303027, 0.3836334989173338, 0.38261445006355643, 0.38219367002602667, 0.3846537759527564, 0.3866378020029515, 0.3851065299240872, 0.3877074469346553, 0.38568653108086437, 0.3830819990253076, 0.3838828532025218, 0.38279719499405473, 0.39060964004602283, 0.3831451191799715, 0.383256166940555, 0.38333317392971367, 0.3906496388372034, 0.38550013094209135, 0.3842854129616171, 0.38319900480564684, 0.36965900915674865, 0.37375497503671795, 0.3596858938690275, 0.3579637799412012, 0.36108340392820537, 0.3648312290897593, 0.3571207149652764, 0.35486420313827693, 0.36059174907859415, 0.36868674820289016, 0.3610119061777368, 0.3617439338704571, 0.36100671195890754, 0.3607629149919376, 0.37144297815393656, 0.36187563103158027, 0.3577902370598167, 0.3613632589112967, 0.3624024959281087, 0.3677019209135324, 0.36055684997700155, 0.36717917397618294, 0.36121862509753555, 0.36008984805084765, 0.36193159595131874, 0.3735940440092236, 0.3663358479971066, 0.3687708309153095, 0.3675969470059499, 0.3675707880174741, 0.3603548259707168, 0.37159035191871226, 0.3621432330692187, 0.3633138029836118, 0.3687354939756915, 0.36231563810724765, 0.3630159341264516, 0.3617157399421558, 0.3611522890860215, 0.3619406759971753, 0.3667662598891184, 0.3658399368869141, 0.37888686708174646, 0.3627620580373332, 0.36144426197279245]
Total Epoch List: [118]
Total Time List: [0.07006200007162988]
========================k_idx:1========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: GPSTransformer
GPS(
  (x_lin): Linear(in_features=14887, out_features=64, bias=True)
  (pe_lin): Linear(in_features=20, out_features=8, bias=True)
  (pe_norm): BatchNorm1d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (convs): ModuleList(
    (0-1): 2 x GPSConv(64, conv=GINEConv(nn=Sequential(
      (0): Linear(in_features=64, out_features=64, bias=True)
      (1): ReLU()
      (2): Linear(in_features=64, out_features=64, bias=True)
    )), heads=1, attn_type=multihead)
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=32, bias=True)
    (1): ReLU()
    (2): Linear(in_features=32, out_features=16, bias=True)
    (3): ReLU()
    (4): Linear(in_features=16, out_features=2, bias=True)
  )
)
Total number of parameters: 1039906
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x737bd9adaf80>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.9024;  Loss pred: 0.9024; Loss self: 0.0000; time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 1.8767 score: 0.4898 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 2.8770 score: 0.5102 time: 0.08s
Epoch 2/1000, LR 0.000000
Train loss: 0.9063;  Loss pred: 0.9063; Loss self: 0.0000; time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 1.4843 score: 0.4898 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 2.2122 score: 0.5102 time: 0.08s
Epoch 3/1000, LR 0.000030
Train loss: 0.8208;  Loss pred: 0.8208; Loss self: 0.0000; time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 1.1937 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 1.7355 score: 0.5102 time: 0.08s
Epoch 4/1000, LR 0.000060
Train loss: 0.7001;  Loss pred: 0.7001; Loss self: 0.0000; time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.9917 score: 0.4898 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 1.3817 score: 0.5102 time: 0.08s
Epoch 5/1000, LR 0.000090
Train loss: 0.5758;  Loss pred: 0.5758; Loss self: 0.0000; time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.8581 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 1.1364 score: 0.5102 time: 0.10s
Epoch 6/1000, LR 0.000120
Train loss: 0.4676;  Loss pred: 0.4676; Loss self: 0.0000; time: 0.23s
Val loss: 0.7776 score: 0.5510 time: 0.06s
Test loss: 0.9686 score: 0.5306 time: 0.08s
Epoch 7/1000, LR 0.000150
Train loss: 0.3983;  Loss pred: 0.3983; Loss self: 0.0000; time: 0.22s
Val loss: 0.7350 score: 0.5510 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.8734 score: 0.5102 time: 0.08s
Epoch 8/1000, LR 0.000180
Train loss: 0.3488;  Loss pred: 0.3488; Loss self: 0.0000; time: 0.22s
Val loss: 0.7274 score: 0.5102 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.8327 score: 0.5102 time: 0.07s
Epoch 9/1000, LR 0.000210
Train loss: 0.3002;  Loss pred: 0.3002; Loss self: 0.0000; time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7246 score: 0.4898 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.8131 score: 0.5102 time: 0.07s
Epoch 10/1000, LR 0.000240
Train loss: 0.2597;  Loss pred: 0.2597; Loss self: 0.0000; time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7279 score: 0.4898 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.8007 score: 0.5102 time: 0.08s
     INFO: Early stopping counter 1 of 20
Epoch 11/1000, LR 0.000270
Train loss: 0.2213;  Loss pred: 0.2213; Loss self: 0.0000; time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7314 score: 0.4898 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7854 score: 0.5102 time: 0.07s
     INFO: Early stopping counter 2 of 20
Epoch 12/1000, LR 0.000270
Train loss: 0.1934;  Loss pred: 0.1934; Loss self: 0.0000; time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7346 score: 0.4898 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7701 score: 0.5102 time: 0.08s
     INFO: Early stopping counter 3 of 20
Epoch 13/1000, LR 0.000270
Train loss: 0.1781;  Loss pred: 0.1781; Loss self: 0.0000; time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7372 score: 0.4898 time: 0.06s
Test loss: 0.7580 score: 0.5306 time: 0.08s
     INFO: Early stopping counter 4 of 20
Epoch 14/1000, LR 0.000270
Train loss: 0.1566;  Loss pred: 0.1566; Loss self: 0.0000; time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7395 score: 0.4898 time: 0.06s
Test loss: 0.7502 score: 0.5510 time: 0.08s
     INFO: Early stopping counter 5 of 20
Epoch 15/1000, LR 0.000270
Train loss: 0.1462;  Loss pred: 0.1462; Loss self: 0.0000; time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7421 score: 0.4898 time: 0.06s
Test loss: 0.7421 score: 0.5510 time: 0.08s
     INFO: Early stopping counter 6 of 20
Epoch 16/1000, LR 0.000270
Train loss: 0.1226;  Loss pred: 0.1226; Loss self: 0.0000; time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7449 score: 0.4898 time: 0.06s
Test loss: 0.7358 score: 0.5510 time: 0.08s
     INFO: Early stopping counter 7 of 20
Epoch 17/1000, LR 0.000270
Train loss: 0.1130;  Loss pred: 0.1130; Loss self: 0.0000; time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7496 score: 0.4898 time: 0.06s
Test loss: 0.7323 score: 0.5714 time: 0.08s
     INFO: Early stopping counter 8 of 20
Epoch 18/1000, LR 0.000270
Train loss: 0.1032;  Loss pred: 0.1032; Loss self: 0.0000; time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7541 score: 0.4898 time: 0.06s
Test loss: 0.7350 score: 0.5714 time: 0.08s
     INFO: Early stopping counter 9 of 20
Epoch 19/1000, LR 0.000270
Train loss: 0.0950;  Loss pred: 0.0950; Loss self: 0.0000; time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7578 score: 0.4898 time: 0.07s
Test loss: 0.7371 score: 0.5714 time: 0.08s
     INFO: Early stopping counter 10 of 20
Epoch 20/1000, LR 0.000270
Train loss: 0.0889;  Loss pred: 0.0889; Loss self: 0.0000; time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7640 score: 0.4898 time: 0.06s
Test loss: 0.7386 score: 0.5714 time: 0.08s
     INFO: Early stopping counter 11 of 20
Epoch 21/1000, LR 0.000270
Train loss: 0.0817;  Loss pred: 0.0817; Loss self: 0.0000; time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7701 score: 0.4898 time: 0.06s
Test loss: 0.7412 score: 0.5714 time: 0.07s
     INFO: Early stopping counter 12 of 20
Epoch 22/1000, LR 0.000270
Train loss: 0.0741;  Loss pred: 0.0741; Loss self: 0.0000; time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7761 score: 0.4898 time: 0.06s
Test loss: 0.7408 score: 0.5714 time: 0.08s
     INFO: Early stopping counter 13 of 20
Epoch 23/1000, LR 0.000270
Train loss: 0.0696;  Loss pred: 0.0696; Loss self: 0.0000; time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7839 score: 0.4898 time: 0.06s
Test loss: 0.7396 score: 0.5714 time: 0.07s
     INFO: Early stopping counter 14 of 20
Epoch 24/1000, LR 0.000270
Train loss: 0.0661;  Loss pred: 0.0661; Loss self: 0.0000; time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7943 score: 0.4898 time: 0.06s
Test loss: 0.7419 score: 0.5918 time: 0.08s
     INFO: Early stopping counter 15 of 20
Epoch 25/1000, LR 0.000270
Train loss: 0.0588;  Loss pred: 0.0588; Loss self: 0.0000; time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.8069 score: 0.4898 time: 0.06s
Test loss: 0.7461 score: 0.5918 time: 0.07s
     INFO: Early stopping counter 16 of 20
Epoch 26/1000, LR 0.000270
Train loss: 0.0578;  Loss pred: 0.0578; Loss self: 0.0000; time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.8208 score: 0.4898 time: 0.06s
Test loss: 0.7546 score: 0.5918 time: 0.08s
     INFO: Early stopping counter 17 of 20
Epoch 27/1000, LR 0.000270
Train loss: 0.0553;  Loss pred: 0.0553; Loss self: 0.0000; time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.8354 score: 0.4898 time: 0.06s
Test loss: 0.7662 score: 0.6122 time: 0.08s
     INFO: Early stopping counter 18 of 20
Epoch 28/1000, LR 0.000270
Train loss: 0.0502;  Loss pred: 0.0502; Loss self: 0.0000; time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.8471 score: 0.4898 time: 0.06s
Test loss: 0.7741 score: 0.6122 time: 0.08s
     INFO: Early stopping counter 19 of 20
Epoch 29/1000, LR 0.000270
Train loss: 0.0481;  Loss pred: 0.0481; Loss self: 0.0000; time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.8563 score: 0.4898 time: 0.06s
Test loss: 0.7789 score: 0.6122 time: 0.07s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 008,   Train_Loss: 0.3002,   Val_Loss: 0.7246,   Val_Precision: 0.0000,   Val_Recall: 0.0000,   Val_accuracy: 0.0000,   Val_Score: 0.4898,   Val_Loss: 0.7246,   Test_Precision: 0.0000,   Test_Recall: 0.0000,   Test_accuracy: 0.0000,   Test_Score: 0.5102,   Test_loss: 0.8131


[0.06986879894975573, 0.07044279901310802, 0.0698356709908694, 0.07017734495457262, 0.07022880995646119, 0.06983176700305194, 0.07062705291900784, 0.07070833304896951, 0.07107204804196954, 0.07109356601722538, 0.07021677994634956, 0.07004082901403308, 0.07017405901569873, 0.07021321693900973, 0.07034074293915182, 0.07046229601837695, 0.07036221993621439, 0.07055566704366356, 0.0701295449398458, 0.07032404106575996, 0.07057988899759948, 0.07110155397094786, 0.0712132859043777, 0.07023007201496512, 0.0702593500027433, 0.07072017109021544, 0.07062843407038599, 0.07050140900537372, 0.07144992600660771, 0.07062201900407672, 0.07056065299548209, 0.0714041970204562, 0.07071025599725544, 0.07035042403731495, 0.07117568503599614, 0.07352192897815257, 0.07099213392939419, 0.09445665893144906, 0.07486826495733112, 0.07537927909288555, 0.07507923000957817, 0.1627832680242136, 0.07635356695391238, 0.07661106099840254, 0.0765218740561977, 0.0762040700064972, 0.07606740191113204, 0.07666937401518226, 0.07617391005624086, 0.07581361697521061, 0.0739690710324794, 0.07400462997611612, 0.07523844006936997, 0.07598071603570133, 0.07485129998531193, 0.07471864705439657, 0.07455711404327303, 0.07491577393375337, 0.07432279898785055, 0.07418989308644086, 0.07503695297054946, 0.07431367202661932, 0.07384985010139644, 0.07376762991771102, 0.07439579896163195, 0.07472316001076251, 0.07365212007425725, 0.0739020510809496, 0.07444329909048975, 0.07445200101938099, 0.07468660001177341, 0.07400844094809145, 0.07405844400636852, 0.07018386200070381, 0.06973629794083536, 0.07006018701940775, 0.07016773102805018, 0.06979875999968499, 0.07014613400679082, 0.07000476494431496, 0.06930494401603937, 0.06943141005467623, 0.06995669496245682, 0.06952244695276022, 0.07019475102424622, 0.06982359604444355, 0.07025930506642908, 0.07548314798623323, 0.07023526902776212, 0.06985194201115519, 0.06941753998398781, 0.06999840098433197, 0.06997724808752537, 0.06969873700290918, 0.06986509507987648, 0.06951155897695571, 0.06973908899817616, 0.06924264901317656, 0.06990781798958778, 0.06971417588647455, 0.07048172503709793, 0.06969214498531073, 0.06981410703156143, 0.06890083593316376, 0.07042855699546635, 0.06991626496892422, 0.0710193170234561, 0.07014241605065763, 0.0700207109330222, 0.06951653002761304, 0.06971525296103209, 0.0701925850007683, 0.06988221500068903, 0.07002617290709168, 0.06986425304785371, 0.07005094503983855, 0.06971384189091623, 0.06952565803658217, 0.08010482601821423, 0.07967456197366118, 0.0797994239255786, 0.08073720103129745, 0.10993477306328714, 0.0795065910788253, 0.07984378794208169, 0.07943927892483771, 0.07926363602746278, 0.07953126193024218, 0.07922980305738747, 0.0796432209899649, 0.0801066739950329, 0.0797045259969309, 0.0796954600373283, 0.0799212648998946, 0.07991563901305199, 0.08039925596676767, 0.07964417699258775, 0.07957007410004735, 0.07924562890548259, 0.0796476670075208, 0.07910045003518462, 0.07959261792711914, 0.07934514305088669, 0.07962502795271575, 0.07956088299397379, 0.07945902098435909, 0.07923269306775182]
[0.001425893856117464, 0.0014376081431246533, 0.0014252177753238653, 0.0014321907133586248, 0.0014332410195196162, 0.0014251381021031008, 0.0014413684269185274, 0.0014430272050810103, 0.0014504499600401946, 0.0014508891023923547, 0.0014329955091091748, 0.001429404673755777, 0.0014321236533816068, 0.001432922794673668, 0.0014355253661051393, 0.0014380060411913662, 0.0014359636721676405, 0.0014399115723196644, 0.001431215202853996, 0.0014351845115461216, 0.0014404058979101935, 0.0014510521218560788, 0.0014533323653954633, 0.0014332667758156145, 0.0014338642857702716, 0.0014432687977594988, 0.0014413966136813468, 0.0014388042654157902, 0.001458161755236892, 0.0014412656939607493, 0.00144001332643841, 0.001457228510621555, 0.0014430664489235804, 0.00143572293953704, 0.0014525650007346151, 0.0015004475301663791, 0.001448819059783555, 0.0019276869169683481, 0.0015279237746394106, 0.0015383526345486849, 0.0015322291838689422, 0.003322107510698237, 0.001558236060283926, 0.0015634910407837253, 0.0015616708991060756, 0.0015551851021734123, 0.0015523959573700416, 0.0015646811023506584, 0.0015545695929845072, 0.0015472166729634817, 0.0015095728782138654, 0.0015102985709411453, 0.0015354783687626524, 0.0015506268578714558, 0.0015275775507206516, 0.0015248703480489096, 0.0015215737559851638, 0.0015288933455868035, 0.0015167918160785825, 0.001514079450743691, 0.0015313663871540707, 0.0015166055515636594, 0.0015071397979876824, 0.001505461835055327, 0.0015182816114618766, 0.001524962449199235, 0.0015031044913113726, 0.0015082051241010123, 0.0015192510018467295, 0.001519428592232265, 0.0015242163267708859, 0.0015103763458794172, 0.0015113968164565004, 0.0014323237143000777, 0.0014231897538945991, 0.001429799735089954, 0.0014319945107765344, 0.0014244644897894896, 0.0014315537552406288, 0.0014286686723329583, 0.001414386612572232, 0.0014169675521362498, 0.0014276876522950372, 0.0014188254480155147, 0.001432545939270331, 0.0014249713478457866, 0.0014338633687026342, 0.0015404724078823108, 0.0014333728373012676, 0.0014255498369623507, 0.001416684489469139, 0.0014285387955986115, 0.0014281071038270484, 0.0014224232041410037, 0.0014258182669362547, 0.0014186032444276676, 0.001423246714248493, 0.0014131152859831952, 0.001426690163052812, 0.0014227382833974398, 0.0014384025517775087, 0.0014222886731696067, 0.0014247776945216619, 0.0014061395088400767, 0.0014373174897033948, 0.0014268625503862087, 0.0014493738168052264, 0.0014314778785848496, 0.0014289941006739224, 0.0014187046944410826, 0.001422760264510859, 0.0014325017347095572, 0.0014261676530752862, 0.0014291055695324832, 0.0014258010826092593, 0.0014296111232620112, 0.0014227314671615557, 0.0014188909803384117, 0.0016347923677186578, 0.0016260114688502283, 0.0016285596719505836, 0.0016476979802305602, 0.0022435667972099415, 0.001622583491404598, 0.0016294650600424834, 0.0016212097739762797, 0.001617625225050261, 0.0016230869781682078, 0.0016169347562732138, 0.0016253718569380592, 0.0016348300815312838, 0.001626622979529202, 0.0016264379599454756, 0.0016310462224468285, 0.0016309314084296323, 0.001640801142178932, 0.0016253913671956683, 0.0016238790632662724, 0.001617257732764951, 0.0016254625919902203, 0.0016142948986772373, 0.0016243391413697783, 0.0016192886336915651, 0.0016250005704635869, 0.0016236914896729346, 0.0016216126731501855, 0.001616993736076568]
[701.3144742224213, 695.5998439369519, 701.647156886435, 698.2310321332163, 697.7193552101747, 701.686382901617, 693.7851428714031, 692.9876280079285, 689.4412268950582, 689.232552888509, 697.8388931739587, 699.5919478649027, 698.2637271849723, 697.8743053827536, 696.6090767961805, 695.4073705917918, 696.3964474745122, 694.4870915850915, 698.7069435860471, 696.7745205964507, 694.2487540844186, 689.155120576147, 688.0738527610592, 697.7068169538362, 697.4160734206447, 692.8716269293562, 693.7715757816208, 695.0215703669859, 685.7949719285708, 693.8345956545286, 694.4380177878655, 686.2341717246993, 692.9687823772254, 696.5132146752895, 688.437350131845, 666.4678236959834, 690.217314058109, 518.756438712926, 654.4829111229711, 650.0460151604808, 652.6438802548836, 301.01373805022376, 641.7512888373217, 639.5943269996186, 640.3397800217801, 643.0102748556899, 644.1655527718126, 639.1078658121938, 643.2648654089337, 646.3218872148249, 662.4390345322084, 662.1207350920342, 651.2628379166542, 644.900476812777, 654.6312490179232, 655.7934589517806, 657.2142796669992, 654.0678608396917, 659.2862576126871, 660.4673219155153, 653.0115904257406, 659.3672289864521, 663.5084557750978, 664.2479913569175, 658.6393409830938, 655.7538518572079, 665.2897425165414, 663.0397841912019, 658.2190821559093, 658.1421496951378, 656.0748513424867, 662.0866400140486, 661.6396098706359, 698.166196660831, 702.6469922675255, 699.3986468581114, 698.3266992118045, 702.0182020457261, 698.5417043119772, 699.9523538001574, 707.0202666733247, 705.7324626046512, 700.4333184450251, 704.8083338219524, 698.0578930050588, 701.7684962660892, 697.4165194727057, 649.1515166926625, 697.6551905942243, 701.4837181216067, 705.873472486962, 700.0159905219531, 700.2275930987215, 703.0256516406426, 701.3516541268353, 704.9187318075308, 702.6188713374427, 707.6563461729421, 700.9230356367032, 702.8699597595994, 695.2156743355662, 703.0921491988503, 701.8638794283821, 711.1669885621087, 695.7405076914218, 700.8383531611578, 689.9531290031471, 698.5787310863609, 699.7929519291883, 704.8683238437885, 702.8591006818687, 698.0794338812805, 701.1798352344279, 699.7383687526593, 701.3601070985088, 699.4909201029807, 702.8733271747105, 704.7757818303247, 611.6984760550929, 615.0018121994624, 614.0395204569106, 606.9073410286461, 445.71884431681804, 616.3011057966236, 613.6983385049864, 616.8233229604444, 618.1901620438252, 616.1099272255793, 618.4541436321441, 615.2438260398086, 611.6843648138268, 614.7706091607243, 614.8405439538093, 613.1034094790036, 613.1465706230193, 609.4583763343994, 615.2364410088674, 615.8094051589031, 618.3306344687227, 615.2094824745229, 619.4655021331022, 615.634983194899, 617.5551283406807, 615.3843993511435, 615.8805452638255, 616.6700695902771, 618.4315855337661]
Elapsed: 0.07420652118419632~0.008936498819125902
Time per graph: 0.0015144187996774762~0.0001823775269209368
Speed: 666.0190181049952~50.10032628256801
Total Time: 0.0798
best val loss: 0.724595308303833 test_score: 0.5102

Testing...
Test loss: 0.9686 score: 0.5306 time: 0.07s
test Score 0.5306
Epoch Time List: [0.36815647105686367, 0.3788550199242309, 0.36802804900798947, 0.36611902993172407, 0.3633534871041775, 0.3661565751535818, 0.36650097998790443, 0.36484617996029556, 0.37532215600367635, 0.3735433929832652, 0.36656685499474406, 0.37320137093774974, 0.36720387905370444, 0.36744041997008026, 0.36804702295921743, 0.3674572000745684, 0.3725368281593546, 0.3751593859633431, 0.3646737620001659, 0.3672148420009762, 0.3677718300605193, 0.4030144870048389, 0.3819682290777564, 0.37425475392956287, 0.36638721707277, 0.3662500879727304, 0.36682257684879005, 0.3670935711124912, 0.3695377130061388, 0.3693309429800138, 0.36751450994051993, 0.36897220311220735, 0.36459795909468085, 0.37254487886093557, 0.3713111609686166, 0.38878083787858486, 0.37870104401372373, 0.44105921604204923, 0.3979939449345693, 0.3938217388931662, 0.39171819598414004, 0.47627057309728116, 0.3948625160846859, 0.3884071591310203, 0.3898119169753045, 0.3883922391105443, 0.39532119780778885, 0.3959695460507646, 0.39392751397099346, 0.39086809707805514, 0.4346158679109067, 0.38000225997529924, 0.3828889229334891, 0.38763458491303027, 0.3836334989173338, 0.38261445006355643, 0.38219367002602667, 0.3846537759527564, 0.3866378020029515, 0.3851065299240872, 0.3877074469346553, 0.38568653108086437, 0.3830819990253076, 0.3838828532025218, 0.38279719499405473, 0.39060964004602283, 0.3831451191799715, 0.383256166940555, 0.38333317392971367, 0.3906496388372034, 0.38550013094209135, 0.3842854129616171, 0.38319900480564684, 0.36965900915674865, 0.37375497503671795, 0.3596858938690275, 0.3579637799412012, 0.36108340392820537, 0.3648312290897593, 0.3571207149652764, 0.35486420313827693, 0.36059174907859415, 0.36868674820289016, 0.3610119061777368, 0.3617439338704571, 0.36100671195890754, 0.3607629149919376, 0.37144297815393656, 0.36187563103158027, 0.3577902370598167, 0.3613632589112967, 0.3624024959281087, 0.3677019209135324, 0.36055684997700155, 0.36717917397618294, 0.36121862509753555, 0.36008984805084765, 0.36193159595131874, 0.3735940440092236, 0.3663358479971066, 0.3687708309153095, 0.3675969470059499, 0.3675707880174741, 0.3603548259707168, 0.37159035191871226, 0.3621432330692187, 0.3633138029836118, 0.3687354939756915, 0.36231563810724765, 0.3630159341264516, 0.3617157399421558, 0.3611522890860215, 0.3619406759971753, 0.3667662598891184, 0.3658399368869141, 0.37888686708174646, 0.3627620580373332, 0.36144426197279245, 0.36396715708542615, 0.35889826994389296, 0.35839679185301065, 0.36325636098627, 0.39290849200915545, 0.36977375706192106, 0.3596326409606263, 0.3582593189785257, 0.35842685704119503, 0.36019071901682764, 0.35768168908543885, 0.3617175299441442, 0.357267327955924, 0.3605950081255287, 0.3610706670442596, 0.3646893159020692, 0.3590486099710688, 0.3598729588557035, 0.36217008787207305, 0.3586429179413244, 0.35510249191429466, 0.3583390909479931, 0.3585274030920118, 0.35610617394559085, 0.3535414340440184, 0.35857537179253995, 0.36089789285324514, 0.3575275840703398, 0.35902206192258745]
Total Epoch List: [118, 29]
Total Time List: [0.07006200007162988, 0.07979867805261165]
========================k_idx:2========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: GPSTransformer
GPS(
  (x_lin): Linear(in_features=14887, out_features=64, bias=True)
  (pe_lin): Linear(in_features=20, out_features=8, bias=True)
  (pe_norm): BatchNorm1d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (convs): ModuleList(
    (0-1): 2 x GPSConv(64, conv=GINEConv(nn=Sequential(
      (0): Linear(in_features=64, out_features=64, bias=True)
      (1): ReLU()
      (2): Linear(in_features=64, out_features=64, bias=True)
    )), heads=1, attn_type=multihead)
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=32, bias=True)
    (1): ReLU()
    (2): Linear(in_features=32, out_features=16, bias=True)
    (3): ReLU()
    (4): Linear(in_features=16, out_features=2, bias=True)
  )
)
Total number of parameters: 1039906
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x737bd9adb3d0>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.7657;  Loss pred: 0.7657; Loss self: 0.0000; time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7188 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7359 score: 0.5000 time: 0.07s
Epoch 2/1000, LR 0.000000
Train loss: 0.7766;  Loss pred: 0.7766; Loss self: 0.0000; time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7428 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7699 score: 0.5000 time: 0.07s
     INFO: Early stopping counter 1 of 20
Epoch 3/1000, LR 0.000030
Train loss: 0.7039;  Loss pred: 0.7039; Loss self: 0.0000; time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7614 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.8079 score: 0.5000 time: 0.07s
     INFO: Early stopping counter 2 of 20
Epoch 4/1000, LR 0.000060
Train loss: 0.6386;  Loss pred: 0.6386; Loss self: 0.0000; time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7768 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.8379 score: 0.5000 time: 0.07s
     INFO: Early stopping counter 3 of 20
Epoch 5/1000, LR 0.000090
Train loss: 0.5519;  Loss pred: 0.5519; Loss self: 0.0000; time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7881 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.8546 score: 0.5000 time: 0.07s
     INFO: Early stopping counter 4 of 20
Epoch 6/1000, LR 0.000120
Train loss: 0.4776;  Loss pred: 0.4776; Loss self: 0.0000; time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7915 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.8612 score: 0.5000 time: 0.07s
     INFO: Early stopping counter 5 of 20
Epoch 7/1000, LR 0.000150
Train loss: 0.4106;  Loss pred: 0.4106; Loss self: 0.0000; time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7887 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.8611 score: 0.5000 time: 0.07s
     INFO: Early stopping counter 6 of 20
Epoch 8/1000, LR 0.000180
Train loss: 0.3608;  Loss pred: 0.3608; Loss self: 0.0000; time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7821 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.8550 score: 0.5000 time: 0.07s
     INFO: Early stopping counter 7 of 20
Epoch 9/1000, LR 0.000210
Train loss: 0.3071;  Loss pred: 0.3071; Loss self: 0.0000; time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7724 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.8443 score: 0.5000 time: 0.07s
     INFO: Early stopping counter 8 of 20
Epoch 10/1000, LR 0.000240
Train loss: 0.2639;  Loss pred: 0.2639; Loss self: 0.0000; time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7588 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.8281 score: 0.5000 time: 0.07s
     INFO: Early stopping counter 9 of 20
Epoch 11/1000, LR 0.000270
Train loss: 0.2269;  Loss pred: 0.2269; Loss self: 0.0000; time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7453 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.8103 score: 0.5000 time: 0.08s
     INFO: Early stopping counter 10 of 20
Epoch 12/1000, LR 0.000270
Train loss: 0.1928;  Loss pred: 0.1928; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7323 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7924 score: 0.5000 time: 0.07s
     INFO: Early stopping counter 11 of 20
Epoch 13/1000, LR 0.000270
Train loss: 0.1673;  Loss pred: 0.1673; Loss self: 0.0000; time: 0.23s
Val loss: 0.7183 score: 0.5306 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7727 score: 0.5000 time: 0.07s
Epoch 14/1000, LR 0.000270
Train loss: 0.1430;  Loss pred: 0.1430; Loss self: 0.0000; time: 0.23s
Val loss: 0.7022 score: 0.5306 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7487 score: 0.5000 time: 0.07s
Epoch 15/1000, LR 0.000270
Train loss: 0.1275;  Loss pred: 0.1275; Loss self: 0.0000; time: 0.22s
Val loss: 0.6851 score: 0.5306 time: 0.07s
Test loss: 0.7245 score: 0.5208 time: 0.07s
Epoch 16/1000, LR 0.000270
Train loss: 0.1184;  Loss pred: 0.1184; Loss self: 0.0000; time: 0.23s
Val loss: 0.6707 score: 0.5306 time: 0.07s
Test loss: 0.7008 score: 0.5208 time: 0.07s
Epoch 17/1000, LR 0.000270
Train loss: 0.0989;  Loss pred: 0.0989; Loss self: 0.0000; time: 0.22s
Val loss: 0.6578 score: 0.5510 time: 0.07s
Test loss: 0.6806 score: 0.5417 time: 0.07s
Epoch 18/1000, LR 0.000270
Train loss: 0.0884;  Loss pred: 0.0884; Loss self: 0.0000; time: 0.22s
Val loss: 0.6525 score: 0.5918 time: 0.07s
Test loss: 0.6619 score: 0.5417 time: 0.07s
Epoch 19/1000, LR 0.000270
Train loss: 0.0806;  Loss pred: 0.0806; Loss self: 0.0000; time: 0.23s
Val loss: 0.6508 score: 0.6122 time: 0.07s
Test loss: 0.6431 score: 0.6042 time: 0.07s
Epoch 20/1000, LR 0.000270
Train loss: 0.0751;  Loss pred: 0.0751; Loss self: 0.0000; time: 0.23s
Val loss: 0.6476 score: 0.5306 time: 0.07s
Test loss: 0.6261 score: 0.6250 time: 0.07s
Epoch 21/1000, LR 0.000270
Train loss: 0.0670;  Loss pred: 0.0670; Loss self: 0.0000; time: 0.23s
Val loss: 0.6504 score: 0.5306 time: 0.07s
Test loss: 0.6111 score: 0.6250 time: 0.07s
     INFO: Early stopping counter 1 of 20
Epoch 22/1000, LR 0.000270
Train loss: 0.0605;  Loss pred: 0.0605; Loss self: 0.0000; time: 0.22s
Val loss: 0.6616 score: 0.5306 time: 0.07s
Test loss: 0.5973 score: 0.6458 time: 0.07s
     INFO: Early stopping counter 2 of 20
Epoch 23/1000, LR 0.000270
Train loss: 0.0542;  Loss pred: 0.0542; Loss self: 0.0000; time: 0.23s
Val loss: 0.6812 score: 0.5306 time: 0.07s
Test loss: 0.5877 score: 0.6458 time: 0.07s
     INFO: Early stopping counter 3 of 20
Epoch 24/1000, LR 0.000270
Train loss: 0.0518;  Loss pred: 0.0518; Loss self: 0.0000; time: 0.23s
Val loss: 0.7064 score: 0.5510 time: 0.07s
Test loss: 0.5817 score: 0.6667 time: 0.07s
     INFO: Early stopping counter 4 of 20
Epoch 25/1000, LR 0.000270
Train loss: 0.0486;  Loss pred: 0.0486; Loss self: 0.0000; time: 0.23s
Val loss: 0.7349 score: 0.5714 time: 0.07s
Test loss: 0.5780 score: 0.6458 time: 0.07s
     INFO: Early stopping counter 5 of 20
Epoch 26/1000, LR 0.000270
Train loss: 0.0434;  Loss pred: 0.0434; Loss self: 0.0000; time: 0.23s
Val loss: 0.7664 score: 0.5714 time: 0.07s
Test loss: 0.5803 score: 0.6458 time: 0.07s
     INFO: Early stopping counter 6 of 20
Epoch 27/1000, LR 0.000270
Train loss: 0.0404;  Loss pred: 0.0404; Loss self: 0.0000; time: 0.23s
Val loss: 0.7968 score: 0.5918 time: 0.07s
Test loss: 0.5894 score: 0.6250 time: 0.07s
     INFO: Early stopping counter 7 of 20
Epoch 28/1000, LR 0.000270
Train loss: 0.0385;  Loss pred: 0.0385; Loss self: 0.0000; time: 0.23s
Val loss: 0.8253 score: 0.6122 time: 0.07s
Test loss: 0.6030 score: 0.6250 time: 0.07s
     INFO: Early stopping counter 8 of 20
Epoch 29/1000, LR 0.000270
Train loss: 0.0351;  Loss pred: 0.0351; Loss self: 0.0000; time: 0.23s
Val loss: 0.8563 score: 0.6122 time: 0.07s
Test loss: 0.6220 score: 0.6458 time: 0.07s
     INFO: Early stopping counter 9 of 20
Epoch 30/1000, LR 0.000270
Train loss: 0.0340;  Loss pred: 0.0340; Loss self: 0.0000; time: 0.23s
Val loss: 0.8854 score: 0.5918 time: 0.07s
Test loss: 0.6418 score: 0.6458 time: 0.07s
     INFO: Early stopping counter 10 of 20
Epoch 31/1000, LR 0.000270
Train loss: 0.0323;  Loss pred: 0.0323; Loss self: 0.0000; time: 0.23s
Val loss: 0.9088 score: 0.6122 time: 0.07s
Test loss: 0.6613 score: 0.6250 time: 0.07s
     INFO: Early stopping counter 11 of 20
Epoch 32/1000, LR 0.000270
Train loss: 0.0297;  Loss pred: 0.0297; Loss self: 0.0000; time: 0.23s
Val loss: 0.9176 score: 0.5918 time: 0.07s
Test loss: 0.6699 score: 0.5833 time: 0.07s
     INFO: Early stopping counter 12 of 20
Epoch 33/1000, LR 0.000270
Train loss: 0.0294;  Loss pred: 0.0294; Loss self: 0.0000; time: 0.22s
Val loss: 0.9221 score: 0.5714 time: 0.07s
Test loss: 0.6732 score: 0.5833 time: 0.07s
     INFO: Early stopping counter 13 of 20
Epoch 34/1000, LR 0.000270
Train loss: 0.0273;  Loss pred: 0.0273; Loss self: 0.0000; time: 0.22s
Val loss: 0.9246 score: 0.5714 time: 0.07s
Test loss: 0.6746 score: 0.6042 time: 0.07s
     INFO: Early stopping counter 14 of 20
Epoch 35/1000, LR 0.000270
Train loss: 0.0257;  Loss pred: 0.0257; Loss self: 0.0000; time: 0.23s
Val loss: 0.9239 score: 0.5714 time: 0.07s
Test loss: 0.6724 score: 0.6042 time: 0.07s
     INFO: Early stopping counter 15 of 20
Epoch 36/1000, LR 0.000270
Train loss: 0.0238;  Loss pred: 0.0238; Loss self: 0.0000; time: 0.23s
Val loss: 0.9249 score: 0.5714 time: 0.07s
Test loss: 0.6696 score: 0.6042 time: 0.07s
     INFO: Early stopping counter 16 of 20
Epoch 37/1000, LR 0.000270
Train loss: 0.0223;  Loss pred: 0.0223; Loss self: 0.0000; time: 0.24s
Val loss: 0.9280 score: 0.5918 time: 0.07s
Test loss: 0.6692 score: 0.6042 time: 0.08s
     INFO: Early stopping counter 17 of 20
Epoch 38/1000, LR 0.000270
Train loss: 0.0205;  Loss pred: 0.0205; Loss self: 0.0000; time: 0.24s
Val loss: 0.9288 score: 0.5918 time: 0.07s
Test loss: 0.6688 score: 0.6042 time: 0.07s
     INFO: Early stopping counter 18 of 20
Epoch 39/1000, LR 0.000269
Train loss: 0.0192;  Loss pred: 0.0192; Loss self: 0.0000; time: 0.23s
Val loss: 0.9325 score: 0.5918 time: 0.07s
Test loss: 0.6715 score: 0.6042 time: 0.07s
     INFO: Early stopping counter 19 of 20
Epoch 40/1000, LR 0.000269
Train loss: 0.0170;  Loss pred: 0.0170; Loss self: 0.0000; time: 0.22s
Val loss: 0.9324 score: 0.5918 time: 0.07s
Test loss: 0.6736 score: 0.6042 time: 0.07s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 019,   Train_Loss: 0.0751,   Val_Loss: 0.6476,   Val_Precision: 0.5294,   Val_Recall: 0.7200,   Val_accuracy: 0.6102,   Val_Score: 0.5306,   Val_Loss: 0.6476,   Test_Precision: 0.5833,   Test_Recall: 0.8750,   Test_accuracy: 0.7000,   Test_Score: 0.6250,   Test_loss: 0.6261


[0.06986879894975573, 0.07044279901310802, 0.0698356709908694, 0.07017734495457262, 0.07022880995646119, 0.06983176700305194, 0.07062705291900784, 0.07070833304896951, 0.07107204804196954, 0.07109356601722538, 0.07021677994634956, 0.07004082901403308, 0.07017405901569873, 0.07021321693900973, 0.07034074293915182, 0.07046229601837695, 0.07036221993621439, 0.07055566704366356, 0.0701295449398458, 0.07032404106575996, 0.07057988899759948, 0.07110155397094786, 0.0712132859043777, 0.07023007201496512, 0.0702593500027433, 0.07072017109021544, 0.07062843407038599, 0.07050140900537372, 0.07144992600660771, 0.07062201900407672, 0.07056065299548209, 0.0714041970204562, 0.07071025599725544, 0.07035042403731495, 0.07117568503599614, 0.07352192897815257, 0.07099213392939419, 0.09445665893144906, 0.07486826495733112, 0.07537927909288555, 0.07507923000957817, 0.1627832680242136, 0.07635356695391238, 0.07661106099840254, 0.0765218740561977, 0.0762040700064972, 0.07606740191113204, 0.07666937401518226, 0.07617391005624086, 0.07581361697521061, 0.0739690710324794, 0.07400462997611612, 0.07523844006936997, 0.07598071603570133, 0.07485129998531193, 0.07471864705439657, 0.07455711404327303, 0.07491577393375337, 0.07432279898785055, 0.07418989308644086, 0.07503695297054946, 0.07431367202661932, 0.07384985010139644, 0.07376762991771102, 0.07439579896163195, 0.07472316001076251, 0.07365212007425725, 0.0739020510809496, 0.07444329909048975, 0.07445200101938099, 0.07468660001177341, 0.07400844094809145, 0.07405844400636852, 0.07018386200070381, 0.06973629794083536, 0.07006018701940775, 0.07016773102805018, 0.06979875999968499, 0.07014613400679082, 0.07000476494431496, 0.06930494401603937, 0.06943141005467623, 0.06995669496245682, 0.06952244695276022, 0.07019475102424622, 0.06982359604444355, 0.07025930506642908, 0.07548314798623323, 0.07023526902776212, 0.06985194201115519, 0.06941753998398781, 0.06999840098433197, 0.06997724808752537, 0.06969873700290918, 0.06986509507987648, 0.06951155897695571, 0.06973908899817616, 0.06924264901317656, 0.06990781798958778, 0.06971417588647455, 0.07048172503709793, 0.06969214498531073, 0.06981410703156143, 0.06890083593316376, 0.07042855699546635, 0.06991626496892422, 0.0710193170234561, 0.07014241605065763, 0.0700207109330222, 0.06951653002761304, 0.06971525296103209, 0.0701925850007683, 0.06988221500068903, 0.07002617290709168, 0.06986425304785371, 0.07005094503983855, 0.06971384189091623, 0.06952565803658217, 0.08010482601821423, 0.07967456197366118, 0.0797994239255786, 0.08073720103129745, 0.10993477306328714, 0.0795065910788253, 0.07984378794208169, 0.07943927892483771, 0.07926363602746278, 0.07953126193024218, 0.07922980305738747, 0.0796432209899649, 0.0801066739950329, 0.0797045259969309, 0.0796954600373283, 0.0799212648998946, 0.07991563901305199, 0.08039925596676767, 0.07964417699258775, 0.07957007410004735, 0.07924562890548259, 0.0796476670075208, 0.07910045003518462, 0.07959261792711914, 0.07934514305088669, 0.07962502795271575, 0.07956088299397379, 0.07945902098435909, 0.07923269306775182, 0.07553184102289379, 0.07577751099597663, 0.07563818292692304, 0.0758828439284116, 0.07573624898213893, 0.07564036000985652, 0.07594918005634099, 0.07583667594008148, 0.07588845794089139, 0.07580541993957013, 0.08121130708605051, 0.07610157399903983, 0.07560087903402746, 0.0756425759755075, 0.07586475694552064, 0.07561473804526031, 0.0750065449392423, 0.07553603197447956, 0.07537925394717604, 0.0755657849367708, 0.0758381630294025, 0.0759087239857763, 0.07654944201931357, 0.07651126303244382, 0.0769418659619987, 0.0765365690458566, 0.07643932197242975, 0.07625869195908308, 0.07573937100823969, 0.07574734406080097, 0.0756232839776203, 0.0763936759904027, 0.0755159379914403, 0.07559468899853528, 0.07743843703065068, 0.07663454092107713, 0.08080873300787061, 0.07633313292171806, 0.0783697929000482, 0.07889014505781233]
[0.001425893856117464, 0.0014376081431246533, 0.0014252177753238653, 0.0014321907133586248, 0.0014332410195196162, 0.0014251381021031008, 0.0014413684269185274, 0.0014430272050810103, 0.0014504499600401946, 0.0014508891023923547, 0.0014329955091091748, 0.001429404673755777, 0.0014321236533816068, 0.001432922794673668, 0.0014355253661051393, 0.0014380060411913662, 0.0014359636721676405, 0.0014399115723196644, 0.001431215202853996, 0.0014351845115461216, 0.0014404058979101935, 0.0014510521218560788, 0.0014533323653954633, 0.0014332667758156145, 0.0014338642857702716, 0.0014432687977594988, 0.0014413966136813468, 0.0014388042654157902, 0.001458161755236892, 0.0014412656939607493, 0.00144001332643841, 0.001457228510621555, 0.0014430664489235804, 0.00143572293953704, 0.0014525650007346151, 0.0015004475301663791, 0.001448819059783555, 0.0019276869169683481, 0.0015279237746394106, 0.0015383526345486849, 0.0015322291838689422, 0.003322107510698237, 0.001558236060283926, 0.0015634910407837253, 0.0015616708991060756, 0.0015551851021734123, 0.0015523959573700416, 0.0015646811023506584, 0.0015545695929845072, 0.0015472166729634817, 0.0015095728782138654, 0.0015102985709411453, 0.0015354783687626524, 0.0015506268578714558, 0.0015275775507206516, 0.0015248703480489096, 0.0015215737559851638, 0.0015288933455868035, 0.0015167918160785825, 0.001514079450743691, 0.0015313663871540707, 0.0015166055515636594, 0.0015071397979876824, 0.001505461835055327, 0.0015182816114618766, 0.001524962449199235, 0.0015031044913113726, 0.0015082051241010123, 0.0015192510018467295, 0.001519428592232265, 0.0015242163267708859, 0.0015103763458794172, 0.0015113968164565004, 0.0014323237143000777, 0.0014231897538945991, 0.001429799735089954, 0.0014319945107765344, 0.0014244644897894896, 0.0014315537552406288, 0.0014286686723329583, 0.001414386612572232, 0.0014169675521362498, 0.0014276876522950372, 0.0014188254480155147, 0.001432545939270331, 0.0014249713478457866, 0.0014338633687026342, 0.0015404724078823108, 0.0014333728373012676, 0.0014255498369623507, 0.001416684489469139, 0.0014285387955986115, 0.0014281071038270484, 0.0014224232041410037, 0.0014258182669362547, 0.0014186032444276676, 0.001423246714248493, 0.0014131152859831952, 0.001426690163052812, 0.0014227382833974398, 0.0014384025517775087, 0.0014222886731696067, 0.0014247776945216619, 0.0014061395088400767, 0.0014373174897033948, 0.0014268625503862087, 0.0014493738168052264, 0.0014314778785848496, 0.0014289941006739224, 0.0014187046944410826, 0.001422760264510859, 0.0014325017347095572, 0.0014261676530752862, 0.0014291055695324832, 0.0014258010826092593, 0.0014296111232620112, 0.0014227314671615557, 0.0014188909803384117, 0.0016347923677186578, 0.0016260114688502283, 0.0016285596719505836, 0.0016476979802305602, 0.0022435667972099415, 0.001622583491404598, 0.0016294650600424834, 0.0016212097739762797, 0.001617625225050261, 0.0016230869781682078, 0.0016169347562732138, 0.0016253718569380592, 0.0016348300815312838, 0.001626622979529202, 0.0016264379599454756, 0.0016310462224468285, 0.0016309314084296323, 0.001640801142178932, 0.0016253913671956683, 0.0016238790632662724, 0.001617257732764951, 0.0016254625919902203, 0.0016142948986772373, 0.0016243391413697783, 0.0016192886336915651, 0.0016250005704635869, 0.0016236914896729346, 0.0016216126731501855, 0.001616993736076568, 0.0015735800213102873, 0.001578698145749513, 0.00157579547764423, 0.0015808925818419084, 0.0015778385204612277, 0.0015758408335386775, 0.001582274584507104, 0.0015799307487516974, 0.0015810095404352371, 0.0015792795820743777, 0.0016919022309593856, 0.0015854494583133298, 0.0015750183132089053, 0.0015758869994897395, 0.0015805157696983467, 0.0015753070426095899, 0.0015626363529008813, 0.0015736673328016575, 0.0015704011238995008, 0.001574287186182725, 0.0015799617297792186, 0.001581431749703673, 0.0015947800420690328, 0.0015939846465092462, 0.001602955540874973, 0.0015945118551220123, 0.0015924858744256198, 0.0015887227491475642, 0.0015779035626716602, 0.0015780696679333535, 0.0015754850828670897, 0.001591534916466723, 0.0015732487081550062, 0.0015748893541361515, 0.001613300771471889, 0.0015965529358557735, 0.0016835152709973045, 0.0015902736025357929, 0.001632704018751004, 0.0016435446887044236]
[701.3144742224213, 695.5998439369519, 701.647156886435, 698.2310321332163, 697.7193552101747, 701.686382901617, 693.7851428714031, 692.9876280079285, 689.4412268950582, 689.232552888509, 697.8388931739587, 699.5919478649027, 698.2637271849723, 697.8743053827536, 696.6090767961805, 695.4073705917918, 696.3964474745122, 694.4870915850915, 698.7069435860471, 696.7745205964507, 694.2487540844186, 689.155120576147, 688.0738527610592, 697.7068169538362, 697.4160734206447, 692.8716269293562, 693.7715757816208, 695.0215703669859, 685.7949719285708, 693.8345956545286, 694.4380177878655, 686.2341717246993, 692.9687823772254, 696.5132146752895, 688.437350131845, 666.4678236959834, 690.217314058109, 518.756438712926, 654.4829111229711, 650.0460151604808, 652.6438802548836, 301.01373805022376, 641.7512888373217, 639.5943269996186, 640.3397800217801, 643.0102748556899, 644.1655527718126, 639.1078658121938, 643.2648654089337, 646.3218872148249, 662.4390345322084, 662.1207350920342, 651.2628379166542, 644.900476812777, 654.6312490179232, 655.7934589517806, 657.2142796669992, 654.0678608396917, 659.2862576126871, 660.4673219155153, 653.0115904257406, 659.3672289864521, 663.5084557750978, 664.2479913569175, 658.6393409830938, 655.7538518572079, 665.2897425165414, 663.0397841912019, 658.2190821559093, 658.1421496951378, 656.0748513424867, 662.0866400140486, 661.6396098706359, 698.166196660831, 702.6469922675255, 699.3986468581114, 698.3266992118045, 702.0182020457261, 698.5417043119772, 699.9523538001574, 707.0202666733247, 705.7324626046512, 700.4333184450251, 704.8083338219524, 698.0578930050588, 701.7684962660892, 697.4165194727057, 649.1515166926625, 697.6551905942243, 701.4837181216067, 705.873472486962, 700.0159905219531, 700.2275930987215, 703.0256516406426, 701.3516541268353, 704.9187318075308, 702.6188713374427, 707.6563461729421, 700.9230356367032, 702.8699597595994, 695.2156743355662, 703.0921491988503, 701.8638794283821, 711.1669885621087, 695.7405076914218, 700.8383531611578, 689.9531290031471, 698.5787310863609, 699.7929519291883, 704.8683238437885, 702.8591006818687, 698.0794338812805, 701.1798352344279, 699.7383687526593, 701.3601070985088, 699.4909201029807, 702.8733271747105, 704.7757818303247, 611.6984760550929, 615.0018121994624, 614.0395204569106, 606.9073410286461, 445.71884431681804, 616.3011057966236, 613.6983385049864, 616.8233229604444, 618.1901620438252, 616.1099272255793, 618.4541436321441, 615.2438260398086, 611.6843648138268, 614.7706091607243, 614.8405439538093, 613.1034094790036, 613.1465706230193, 609.4583763343994, 615.2364410088674, 615.8094051589031, 618.3306344687227, 615.2094824745229, 619.4655021331022, 615.634983194899, 617.5551283406807, 615.3843993511435, 615.8805452638255, 616.6700695902771, 618.4315855337661, 635.4935792634942, 633.4333150972528, 634.6001205022951, 632.5540466733631, 633.7784171397234, 634.5818554240782, 632.0015563616672, 632.9391340665403, 632.5072521224062, 633.2001067768532, 591.0507012175015, 630.7359687541498, 634.9132525085524, 634.5632652111431, 632.704854435497, 634.796882735597, 639.9441547251847, 635.4583202916613, 636.779982376016, 635.2081175384296, 632.9267229401427, 632.3383858882174, 627.0457201750668, 627.3586148962943, 623.8476205360944, 627.1511853535136, 627.9490550336476, 629.4364454317496, 633.7522923814363, 633.6855845595234, 634.7251464800835, 628.3242608462803, 635.6274089509526, 634.9652420810945, 619.8472211028906, 626.3494166348995, 593.9952058810883, 628.8226116596767, 612.480883562096, 608.4410158559679]
Elapsed: 0.07466118667152695~0.00799393027851762
Time per graph: 0.0015306397442093305~0.00016514259628877726
Speed: 658.1021956412108~47.183417920012474
Total Time: 0.0796
best val loss: 0.647616982460022 test_score: 0.6250

Testing...
Test loss: 0.6431 score: 0.6042 time: 0.07s
test Score 0.6042
Epoch Time List: [0.36815647105686367, 0.3788550199242309, 0.36802804900798947, 0.36611902993172407, 0.3633534871041775, 0.3661565751535818, 0.36650097998790443, 0.36484617996029556, 0.37532215600367635, 0.3735433929832652, 0.36656685499474406, 0.37320137093774974, 0.36720387905370444, 0.36744041997008026, 0.36804702295921743, 0.3674572000745684, 0.3725368281593546, 0.3751593859633431, 0.3646737620001659, 0.3672148420009762, 0.3677718300605193, 0.4030144870048389, 0.3819682290777564, 0.37425475392956287, 0.36638721707277, 0.3662500879727304, 0.36682257684879005, 0.3670935711124912, 0.3695377130061388, 0.3693309429800138, 0.36751450994051993, 0.36897220311220735, 0.36459795909468085, 0.37254487886093557, 0.3713111609686166, 0.38878083787858486, 0.37870104401372373, 0.44105921604204923, 0.3979939449345693, 0.3938217388931662, 0.39171819598414004, 0.47627057309728116, 0.3948625160846859, 0.3884071591310203, 0.3898119169753045, 0.3883922391105443, 0.39532119780778885, 0.3959695460507646, 0.39392751397099346, 0.39086809707805514, 0.4346158679109067, 0.38000225997529924, 0.3828889229334891, 0.38763458491303027, 0.3836334989173338, 0.38261445006355643, 0.38219367002602667, 0.3846537759527564, 0.3866378020029515, 0.3851065299240872, 0.3877074469346553, 0.38568653108086437, 0.3830819990253076, 0.3838828532025218, 0.38279719499405473, 0.39060964004602283, 0.3831451191799715, 0.383256166940555, 0.38333317392971367, 0.3906496388372034, 0.38550013094209135, 0.3842854129616171, 0.38319900480564684, 0.36965900915674865, 0.37375497503671795, 0.3596858938690275, 0.3579637799412012, 0.36108340392820537, 0.3648312290897593, 0.3571207149652764, 0.35486420313827693, 0.36059174907859415, 0.36868674820289016, 0.3610119061777368, 0.3617439338704571, 0.36100671195890754, 0.3607629149919376, 0.37144297815393656, 0.36187563103158027, 0.3577902370598167, 0.3613632589112967, 0.3624024959281087, 0.3677019209135324, 0.36055684997700155, 0.36717917397618294, 0.36121862509753555, 0.36008984805084765, 0.36193159595131874, 0.3735940440092236, 0.3663358479971066, 0.3687708309153095, 0.3675969470059499, 0.3675707880174741, 0.3603548259707168, 0.37159035191871226, 0.3621432330692187, 0.3633138029836118, 0.3687354939756915, 0.36231563810724765, 0.3630159341264516, 0.3617157399421558, 0.3611522890860215, 0.3619406759971753, 0.3667662598891184, 0.3658399368869141, 0.37888686708174646, 0.3627620580373332, 0.36144426197279245, 0.36396715708542615, 0.35889826994389296, 0.35839679185301065, 0.36325636098627, 0.39290849200915545, 0.36977375706192106, 0.3596326409606263, 0.3582593189785257, 0.35842685704119503, 0.36019071901682764, 0.35768168908543885, 0.3617175299441442, 0.357267327955924, 0.3605950081255287, 0.3610706670442596, 0.3646893159020692, 0.3590486099710688, 0.3598729588557035, 0.36217008787207305, 0.3586429179413244, 0.35510249191429466, 0.3583390909479931, 0.3585274030920118, 0.35610617394559085, 0.3535414340440184, 0.35857537179253995, 0.36089789285324514, 0.3575275840703398, 0.35902206192258745, 0.3660976210376248, 0.36307329207193106, 0.3641113159246743, 0.36584680213127285, 0.36604699003510177, 0.363504457869567, 0.35942007799167186, 0.36136286600958556, 0.35808446898590773, 0.359387960168533, 0.3640716060763225, 0.3854284640401602, 0.3649123690556735, 0.364469773019664, 0.36328981013502926, 0.3648272880818695, 0.3596139941364527, 0.3630224709631875, 0.3631562889786437, 0.3668596980860457, 0.36428686208091676, 0.36069296300411224, 0.36609309597406536, 0.3678061789833009, 0.376689133932814, 0.3684961940161884, 0.3675114380894229, 0.36736676120199263, 0.3655630339635536, 0.36488578701391816, 0.3649582560174167, 0.3662210451439023, 0.36354982084594667, 0.3637121549108997, 0.3739077940117568, 0.36776877008378506, 0.39239291101694107, 0.3770870870212093, 0.37609395291656256, 0.3632473209872842]
Total Epoch List: [118, 29, 40]
Total Time List: [0.07006200007162988, 0.07979867805261165, 0.0795660789590329]
T-times Epoch Time: 0.3715114294721089 ~ 0.002742370951277478
T-times Total Epoch: 70.8888888888889 ~ 24.043684522803463
T-times Total Time: 0.07753945744803382 ~ 0.0015606381103782149
T-times Inference Elapsed: 0.07641083409694065 ~ 0.0013851935666487545
T-times Time Per Graph: 0.001572933742912093 ~ 3.458875260156631e-05
T-times Speed: 639.5824414663542 ~ 14.775989849137547
T-times cross validation test micro f1 score:0.7220193783470893 ~ 0.09195920291285963
T-times cross validation test precision:0.7328823623466482 ~ 0.1720155471220512
T-times cross validation test recall:0.7512962962962964 ~ 0.12806275253603458
T-times cross validation test f1_score:0.7220193783470893 ~ 0.13999225729219966
