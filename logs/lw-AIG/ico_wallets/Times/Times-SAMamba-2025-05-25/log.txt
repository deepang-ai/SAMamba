Namespace(seed=55, model='SAMamba', dataset='ico_wallets/Times', num_heads=8, num_layers=2, dim_hidden=128, dropout=0.6, epochs=1000, lr=0.0005, weight_decay=0.0001, batch_size=64, abs_pe=None, abs_pe_dim=20, num_class=2, outdir='./outdir/ico_wallets/Times/seed55/khopgnn_gat_1_0.6_0.0005_0.0001_2_8_128_BN', warmup=10, layer_norm=False, use_edge_attr=False, edge_dim=128, gnn_type='gat', k_hop=1, global_pool='mean', se='khopgnn', ie='mamba', aggr='add', not_extract_node_feature=False, profile=False, k_ford=3, early_stop=1, early_stop_mindelta=-0.0, patience=20, training_times=3, Lambda=0.01, order='sd', use_cuda=True, batch_norm=True, save_logs=True)
Data(edge_index=[2, 338], edge_attr=[338, 2], x=[122, 14887], y=[1, 1], num_nodes=122)
Data(edge_index=[2, 338], edge_attr=[338, 2], x=[122, 14887], y=[1, 1], num_nodes=122)
Data(edge_index=[2, 338], edge_attr=[338, 2], x=[122, 14887], y=[1, 1], num_nodes=122)
========================training times:0========================
========================k_idx:0========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: SAMamba
GraphEncoder(
  (subgraph_conv): GATConv(128, 128, heads=1)
  (embedding): Linear(in_features=14887, out_features=128, bias=False)
  (encoder): GraphMambaEncoder(
    (layers): ModuleList(
      (0-1): 2 x MambaEncoderLayer(
        (self_selen): Selection(
          (khop_structure_extractor): KHopStructureExtractor(
            (structure_extractor): StructureExtractor(
              (gcn): ModuleList(
                (0): GATConv(128, 128, heads=1)
              )
              (relu): ReLU()
              (bn): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (out_proj): Linear(in_features=128, out_features=128, bias=True)
            )
            (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (out_proj): Linear(in_features=256, out_features=128, bias=True)
          )
          (out_proj): Mamba(
            (in_proj): Linear(in_features=128, out_features=512, bias=False)
            (conv1d): Conv1d(256, 256, kernel_size=(4,), stride=(1,), padding=(3,), groups=256)
            (act): SiLU()
            (x_proj): Linear(in_features=256, out_features=40, bias=False)
            (dt_proj): Linear(in_features=8, out_features=256, bias=True)
            (out_proj): Linear(in_features=256, out_features=128, bias=False)
          )
        )
        (norm1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (linear1): Linear(in_features=128, out_features=256, bias=True)
        (dropout): Dropout(p=0.6, inplace=False)
        (linear2): Linear(in_features=256, out_features=128, bias=True)
        (dropout1): Dropout(p=0.6, inplace=False)
        (dropout2): Dropout(p=0.6, inplace=False)
        (activation): ReLU()
      )
    )
  )
  (classifier): Sequential(
    (0): Linear(in_features=128, out_features=128, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=128, out_features=2, bias=True)
  )
)
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x75fef9a36230>
Training...
Epoch 1/1000, LR 0.000500
Train loss: 0.7132;  Loss pred: 0.7005; Loss self: 1.2729; time: 0.50s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6939 score: 0.4898 time: 0.09s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6930 score: 0.5102 time: 0.08s
Epoch 2/1000, LR 0.000000
Train loss: 0.7081;  Loss pred: 0.6955; Loss self: 1.2604; time: 0.18s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6937 score: 0.4898 time: 0.09s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6930 score: 0.5102 time: 0.08s
Epoch 3/1000, LR 0.000050
Train loss: 0.6862;  Loss pred: 0.6740; Loss self: 1.2190; time: 0.18s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6938 score: 0.4898 time: 0.09s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6930 score: 0.5102 time: 0.08s
     INFO: Early stopping counter 1 of 20
Epoch 4/1000, LR 0.000100
Train loss: 0.6687;  Loss pred: 0.6553; Loss self: 1.3309; time: 0.18s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6938 score: 0.4898 time: 0.09s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6929 score: 0.5102 time: 0.08s
     INFO: Early stopping counter 2 of 20
Epoch 5/1000, LR 0.000150
Train loss: 0.6423;  Loss pred: 0.6301; Loss self: 1.2221; time: 0.18s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6935 score: 0.4898 time: 0.09s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6927 score: 0.5102 time: 0.08s
Epoch 6/1000, LR 0.000200
Train loss: 0.6019;  Loss pred: 0.5886; Loss self: 1.3295; time: 0.18s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6933 score: 0.4898 time: 0.09s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6926 score: 0.5102 time: 0.08s
Epoch 7/1000, LR 0.000250
Train loss: 0.5527;  Loss pred: 0.5386; Loss self: 1.4033; time: 0.19s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6930 score: 0.4898 time: 0.09s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6924 score: 0.5102 time: 0.08s
Epoch 8/1000, LR 0.000300
Train loss: 0.4906;  Loss pred: 0.4767; Loss self: 1.3975; time: 0.19s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6924 score: 0.4898 time: 0.09s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6922 score: 0.5102 time: 0.08s
Epoch 9/1000, LR 0.000350
Train loss: 0.4319;  Loss pred: 0.4177; Loss self: 1.4179; time: 0.19s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6917 score: 0.4898 time: 0.09s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6918 score: 0.5102 time: 0.08s
Epoch 10/1000, LR 0.000400
Train loss: 0.3695;  Loss pred: 0.3546; Loss self: 1.4903; time: 0.19s
Val loss: 0.6905 score: 0.5102 time: 0.09s
Test loss: 0.6911 score: 0.5306 time: 0.08s
Epoch 11/1000, LR 0.000450
Train loss: 0.3109;  Loss pred: 0.2950; Loss self: 1.5841; time: 0.19s
Val loss: 0.6883 score: 0.9388 time: 0.09s
Test loss: 0.6897 score: 0.7959 time: 0.08s
Epoch 12/1000, LR 0.000450
Train loss: 0.2605;  Loss pred: 0.2437; Loss self: 1.6801; time: 0.18s
Val loss: 0.6851 score: 0.8163 time: 0.10s
Test loss: 0.6875 score: 0.6939 time: 0.08s
Epoch 13/1000, LR 0.000450
Train loss: 0.2228;  Loss pred: 0.2050; Loss self: 1.7766; time: 0.19s
Val loss: 0.6805 score: 0.6531 time: 0.10s
Test loss: 0.6843 score: 0.5510 time: 0.08s
Epoch 14/1000, LR 0.000450
Train loss: 0.1975;  Loss pred: 0.1790; Loss self: 1.8550; time: 0.20s
Val loss: 0.6738 score: 0.7143 time: 0.09s
Test loss: 0.6793 score: 0.6531 time: 0.08s
Epoch 15/1000, LR 0.000450
Train loss: 0.1739;  Loss pred: 0.1546; Loss self: 1.9294; time: 0.20s
Val loss: 0.6642 score: 0.8367 time: 0.10s
Test loss: 0.6719 score: 0.7347 time: 0.08s
Epoch 16/1000, LR 0.000450
Train loss: 0.1528;  Loss pred: 0.1329; Loss self: 1.9947; time: 0.19s
Val loss: 0.6499 score: 0.9592 time: 0.09s
Test loss: 0.6610 score: 0.8367 time: 0.08s
Epoch 17/1000, LR 0.000450
Train loss: 0.1379;  Loss pred: 0.1175; Loss self: 2.0405; time: 0.19s
Val loss: 0.6299 score: 0.9592 time: 0.09s
Test loss: 0.6457 score: 0.8571 time: 0.08s
Epoch 18/1000, LR 0.000450
Train loss: 0.1267;  Loss pred: 0.1059; Loss self: 2.0847; time: 0.18s
Val loss: 0.6027 score: 0.9592 time: 0.09s
Test loss: 0.6252 score: 0.8571 time: 0.08s
Epoch 19/1000, LR 0.000450
Train loss: 0.1127;  Loss pred: 0.0916; Loss self: 2.1102; time: 0.19s
Val loss: 0.5666 score: 0.9388 time: 0.09s
Test loss: 0.5981 score: 0.8367 time: 0.08s
Epoch 20/1000, LR 0.000450
Train loss: 0.1024;  Loss pred: 0.0809; Loss self: 2.1556; time: 0.18s
Val loss: 0.5220 score: 0.9388 time: 0.09s
Test loss: 0.5652 score: 0.8367 time: 0.08s
Epoch 21/1000, LR 0.000450
Train loss: 0.0930;  Loss pred: 0.0713; Loss self: 2.1739; time: 0.18s
Val loss: 0.4697 score: 0.9388 time: 0.09s
Test loss: 0.5276 score: 0.8367 time: 0.08s
Epoch 22/1000, LR 0.000450
Train loss: 0.0855;  Loss pred: 0.0627; Loss self: 2.2721; time: 0.18s
Val loss: 0.4121 score: 0.9388 time: 0.09s
Test loss: 0.4869 score: 0.8367 time: 0.09s
Epoch 23/1000, LR 0.000450
Train loss: 0.0764;  Loss pred: 0.0535; Loss self: 2.2980; time: 0.19s
Val loss: 0.3504 score: 0.9184 time: 0.09s
Test loss: 0.4448 score: 0.8367 time: 0.08s
Epoch 24/1000, LR 0.000450
Train loss: 0.0678;  Loss pred: 0.0448; Loss self: 2.2942; time: 0.18s
Val loss: 0.2914 score: 0.9184 time: 0.09s
Test loss: 0.4072 score: 0.8367 time: 0.08s
Epoch 25/1000, LR 0.000450
Train loss: 0.0626;  Loss pred: 0.0389; Loss self: 2.3745; time: 0.18s
Val loss: 0.2394 score: 0.9184 time: 0.09s
Test loss: 0.3770 score: 0.8367 time: 0.08s
Epoch 26/1000, LR 0.000450
Train loss: 0.0569;  Loss pred: 0.0334; Loss self: 2.3510; time: 0.18s
Val loss: 0.1971 score: 0.9184 time: 0.09s
Test loss: 0.3560 score: 0.8367 time: 0.08s
Epoch 27/1000, LR 0.000450
Train loss: 0.0511;  Loss pred: 0.0270; Loss self: 2.4093; time: 0.19s
Val loss: 0.1655 score: 0.9388 time: 0.10s
Test loss: 0.3458 score: 0.8571 time: 0.08s
Epoch 28/1000, LR 0.000450
Train loss: 0.0474;  Loss pred: 0.0232; Loss self: 2.4203; time: 0.18s
Val loss: 0.1437 score: 0.9388 time: 0.10s
Test loss: 0.3439 score: 0.8571 time: 0.08s
Epoch 29/1000, LR 0.000450
Train loss: 0.0444;  Loss pred: 0.0199; Loss self: 2.4503; time: 0.18s
Val loss: 0.1302 score: 0.9388 time: 0.09s
Test loss: 0.3503 score: 0.8571 time: 0.08s
Epoch 30/1000, LR 0.000450
Train loss: 0.0412;  Loss pred: 0.0166; Loss self: 2.4672; time: 0.18s
Val loss: 0.1222 score: 0.9388 time: 0.09s
Test loss: 0.3618 score: 0.8571 time: 0.08s
Epoch 31/1000, LR 0.000450
Train loss: 0.0381;  Loss pred: 0.0134; Loss self: 2.4685; time: 0.20s
Val loss: 0.1180 score: 0.9388 time: 0.09s
Test loss: 0.3770 score: 0.8571 time: 0.07s
Epoch 32/1000, LR 0.000450
Train loss: 0.0366;  Loss pred: 0.0116; Loss self: 2.5010; time: 0.19s
Val loss: 0.1170 score: 0.9388 time: 0.09s
Test loss: 0.3944 score: 0.8571 time: 0.08s
Epoch 33/1000, LR 0.000449
Train loss: 0.0346;  Loss pred: 0.0094; Loss self: 2.5122; time: 0.20s
Val loss: 0.1174 score: 0.9388 time: 0.09s
Test loss: 0.4112 score: 0.8571 time: 0.08s
     INFO: Early stopping counter 1 of 20
Epoch 34/1000, LR 0.000449
Train loss: 0.0337;  Loss pred: 0.0083; Loss self: 2.5339; time: 0.20s
Val loss: 0.1194 score: 0.9388 time: 0.09s
Test loss: 0.4280 score: 0.8571 time: 0.08s
     INFO: Early stopping counter 2 of 20
Epoch 35/1000, LR 0.000449
Train loss: 0.0325;  Loss pred: 0.0071; Loss self: 2.5459; time: 0.20s
Val loss: 0.1218 score: 0.9388 time: 0.09s
Test loss: 0.4423 score: 0.8571 time: 0.08s
     INFO: Early stopping counter 3 of 20
Epoch 36/1000, LR 0.000449
Train loss: 0.0318;  Loss pred: 0.0064; Loss self: 2.5397; time: 0.20s
Val loss: 0.1244 score: 0.9388 time: 0.09s
Test loss: 0.4546 score: 0.8571 time: 0.08s
     INFO: Early stopping counter 4 of 20
Epoch 37/1000, LR 0.000449
Train loss: 0.0310;  Loss pred: 0.0053; Loss self: 2.5642; time: 0.20s
Val loss: 0.1269 score: 0.9388 time: 0.09s
Test loss: 0.4652 score: 0.8571 time: 0.08s
     INFO: Early stopping counter 5 of 20
Epoch 38/1000, LR 0.000449
Train loss: 0.0300;  Loss pred: 0.0047; Loss self: 2.5330; time: 0.20s
Val loss: 0.1286 score: 0.9388 time: 0.09s
Test loss: 0.4718 score: 0.8571 time: 0.08s
     INFO: Early stopping counter 6 of 20
Epoch 39/1000, LR 0.000449
Train loss: 0.0299;  Loss pred: 0.0043; Loss self: 2.5523; time: 0.20s
Val loss: 0.1304 score: 0.9388 time: 0.09s
Test loss: 0.4778 score: 0.8571 time: 0.08s
     INFO: Early stopping counter 7 of 20
Epoch 40/1000, LR 0.000449
Train loss: 0.0291;  Loss pred: 0.0038; Loss self: 2.5327; time: 0.20s
Val loss: 0.1318 score: 0.9388 time: 0.09s
Test loss: 0.4814 score: 0.8571 time: 0.08s
     INFO: Early stopping counter 8 of 20
Epoch 41/1000, LR 0.000449
Train loss: 0.0289;  Loss pred: 0.0034; Loss self: 2.5529; time: 0.20s
Val loss: 0.1322 score: 0.9388 time: 0.09s
Test loss: 0.4825 score: 0.8571 time: 0.08s
     INFO: Early stopping counter 9 of 20
Epoch 42/1000, LR 0.000449
Train loss: 0.0282;  Loss pred: 0.0030; Loss self: 2.5165; time: 0.20s
Val loss: 0.1338 score: 0.9388 time: 0.09s
Test loss: 0.4856 score: 0.8571 time: 0.08s
     INFO: Early stopping counter 10 of 20
Epoch 43/1000, LR 0.000449
Train loss: 0.0278;  Loss pred: 0.0027; Loss self: 2.5138; time: 0.20s
Val loss: 0.1352 score: 0.9388 time: 0.09s
Test loss: 0.4883 score: 0.8571 time: 0.08s
     INFO: Early stopping counter 11 of 20
Epoch 44/1000, LR 0.000449
Train loss: 0.0274;  Loss pred: 0.0024; Loss self: 2.4984; time: 0.20s
Val loss: 0.1370 score: 0.9388 time: 0.09s
Test loss: 0.4911 score: 0.8571 time: 0.08s
     INFO: Early stopping counter 12 of 20
Epoch 45/1000, LR 0.000449
Train loss: 0.0273;  Loss pred: 0.0023; Loss self: 2.5000; time: 0.20s
Val loss: 0.1385 score: 0.9388 time: 0.09s
Test loss: 0.4925 score: 0.8571 time: 0.08s
     INFO: Early stopping counter 13 of 20
Epoch 46/1000, LR 0.000449
Train loss: 0.0271;  Loss pred: 0.0021; Loss self: 2.5040; time: 0.20s
Val loss: 0.1404 score: 0.9388 time: 0.09s
Test loss: 0.4942 score: 0.8571 time: 0.08s
     INFO: Early stopping counter 14 of 20
Epoch 47/1000, LR 0.000449
Train loss: 0.0266;  Loss pred: 0.0020; Loss self: 2.4532; time: 0.20s
Val loss: 0.1423 score: 0.9388 time: 0.09s
Test loss: 0.4963 score: 0.8571 time: 0.08s
     INFO: Early stopping counter 15 of 20
Epoch 48/1000, LR 0.000448
Train loss: 0.0261;  Loss pred: 0.0018; Loss self: 2.4300; time: 0.20s
Val loss: 0.1445 score: 0.9388 time: 0.09s
Test loss: 0.4994 score: 0.8571 time: 0.08s
     INFO: Early stopping counter 16 of 20
Epoch 49/1000, LR 0.000448
Train loss: 0.0262;  Loss pred: 0.0018; Loss self: 2.4352; time: 0.18s
Val loss: 0.1478 score: 0.9388 time: 0.09s
Test loss: 0.5043 score: 0.8571 time: 0.08s
     INFO: Early stopping counter 17 of 20
Epoch 50/1000, LR 0.000448
Train loss: 0.0259;  Loss pred: 0.0018; Loss self: 2.4102; time: 0.18s
Val loss: 0.1505 score: 0.9388 time: 0.09s
Test loss: 0.5080 score: 0.8571 time: 0.08s
     INFO: Early stopping counter 18 of 20
Epoch 51/1000, LR 0.000448
Train loss: 0.0254;  Loss pred: 0.0017; Loss self: 2.3655; time: 0.18s
Val loss: 0.1535 score: 0.9388 time: 0.09s
Test loss: 0.5117 score: 0.8571 time: 0.08s
     INFO: Early stopping counter 19 of 20
Epoch 52/1000, LR 0.000448
Train loss: 0.0253;  Loss pred: 0.0017; Loss self: 2.3625; time: 0.18s
Val loss: 0.1571 score: 0.9388 time: 0.09s
Test loss: 0.5165 score: 0.8571 time: 0.08s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 031,   Train_Loss: 0.0366,   Val_Loss: 0.1170,   Val_Precision: 1.0000,   Val_Recall: 0.8750,   Val_accuracy: 0.9333,   Val_Score: 0.9388,   Val_Loss: 0.1170,   Test_Precision: 0.9500,   Test_Recall: 0.7600,   Test_accuracy: 0.8444,   Test_Score: 0.8571,   Test_loss: 0.3944


[0.08868790604174137, 0.08555980795063078, 0.08653830387629569, 0.08673247206024826, 0.08630424621514976, 0.08727480913512409, 0.08869838993996382, 0.08860889310017228, 0.09053831291384995, 0.0881303739733994, 0.08791551901958883, 0.08710404206067324, 0.08654724503867328, 0.08733428199775517, 0.08720076805911958, 0.08854636386968195, 0.08735093497671187, 0.08839433803223073, 0.08686166699044406, 0.08937912294641137, 0.08694926486350596, 0.09359746286645532, 0.08908455912023783, 0.08866356988437474, 0.08889057789929211, 0.08825293998233974, 0.08787804306484759, 0.08962543704546988, 0.08862893492914736, 0.08906667795963585, 0.07880508597008884, 0.08410700084641576, 0.08133646799251437, 0.08194253593683243, 0.08141453005373478, 0.08399981912225485, 0.08226044406183064, 0.0834080190397799, 0.084027084056288, 0.0835539388936013, 0.08253465313464403, 0.08295532199554145, 0.0822397219017148, 0.08182029495947063, 0.08147235703654587, 0.08331819693557918, 0.08544576494023204, 0.08544697193428874, 0.08595926687121391, 0.08565382100641727, 0.08561150799505413, 0.08553159493021667]
[0.0018099572661579872, 0.00174611852960471, 0.001766087834210116, 0.0017700504502091482, 0.0017613111472479543, 0.0017811185537780427, 0.0018101712232645676, 0.001808344757146373, 0.0018477206717112235, 0.0017985790606816203, 0.0017941942657058944, 0.001777633511442311, 0.0017662703069116995, 0.0017823322856684728, 0.0017796075114106036, 0.0018070686504016726, 0.0017826721423818749, 0.0018039660822904231, 0.001772687081437634, 0.0018240637336002321, 0.0017744747931327746, 0.0019101523033970473, 0.0018180522269436292, 0.0018094606098851987, 0.0018140934265161656, 0.0018010804078028519, 0.001793429450303012, 0.0018290905519483648, 0.001808753774064232, 0.001817687305298691, 0.0016082670606140579, 0.001716469405028893, 0.001659927918214579, 0.0016722966517720903, 0.0016615210215047914, 0.0017142820229031602, 0.0016787845726904211, 0.00170220447019959, 0.0017148384501283265, 0.0017051824264000266, 0.001684380676217225, 0.00169296575501105, 0.0016783616714635674, 0.0016698019379483803, 0.0016627011640111403, 0.0017003713660322282, 0.0017437911212292252, 0.0017438157537609947, 0.0017542707524737533, 0.001748037163396271, 0.001747173632552125, 0.0017455427536778912]
[552.4992322734278, 572.698807695708, 566.2232538095992, 564.9556485137701, 567.7588548522493, 561.4449402477096, 552.4339284305614, 552.9918982805207, 541.207345520399, 555.9944635522571, 557.3532471449336, 562.5456504747339, 566.1647575044653, 561.0626077083853, 561.9216560888481, 553.3824073466835, 560.9556441847315, 554.334147308546, 564.1153537312455, 548.2264580888616, 563.5470302931349, 523.5184640625688, 550.0392041438346, 552.6508808961833, 551.2395256954474, 555.2222963881471, 557.5909327412033, 546.7197886593369, 552.8668491748442, 550.1496308440551, 621.787279295633, 582.5912172219389, 602.4357979806747, 597.9800288066865, 601.8581691457199, 583.3345894314905, 595.66904310861, 587.4734895289907, 583.1453102332566, 586.4475170033247, 593.6900215726743, 590.6794021320726, 595.8191354119631, 598.8734216158957, 601.4309857025517, 588.1068218253249, 573.4631790618847, 573.4550785214772, 570.0374349796734, 572.0702173500102, 572.3529598711284, 572.887715235265]
Elapsed: 0.08609980125821984~0.002894893303671802
Time per graph: 0.0017571388011881598~5.907945517697556e-05
Speed: 569.7577638594738~19.373397597469527
Total Time: 0.0860
best val loss: 0.11701250076293945 test_score: 0.8571

Testing...
Test loss: 0.6610 score: 0.8367 time: 0.08s
test Score 0.8367
Epoch Time List: [0.6790760902222246, 0.35661386302672327, 0.35290941200219095, 0.3541223732754588, 0.35498873703181744, 0.3589131897315383, 0.3625092161819339, 0.36397642781957984, 0.36850034911185503, 0.3686835339758545, 0.3692579453345388, 0.3654741251375526, 0.3682142607867718, 0.37336379708722234, 0.3764678828883916, 0.3678974041249603, 0.36057250504381955, 0.3565560458227992, 0.3600418381392956, 0.3570455159060657, 0.358668162021786, 0.3635019559878856, 0.3641638557892293, 0.35952282208018005, 0.36017922731116414, 0.35776893608272076, 0.3668504108209163, 0.3605713527649641, 0.35927733732387424, 0.3600620608776808, 0.3535300891380757, 0.36210372229106724, 0.3605109441559762, 0.3610345299821347, 0.3663574359379709, 0.3676249429117888, 0.37104864581488073, 0.36558001511730254, 0.3709781488869339, 0.3669571648351848, 0.3663695661816746, 0.3644669149070978, 0.36305679799988866, 0.3680954179726541, 0.36500784289091825, 0.3709748031105846, 0.3679910618811846, 0.37095806980505586, 0.3552660408895463, 0.3524673320353031, 0.35183134605176747, 0.35209273220971227]
Total Epoch List: [52]
Total Time List: [0.08595785195939243]
========================k_idx:1========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: SAMamba
GraphEncoder(
  (subgraph_conv): GATConv(128, 128, heads=1)
  (embedding): Linear(in_features=14887, out_features=128, bias=False)
  (encoder): GraphMambaEncoder(
    (layers): ModuleList(
      (0-1): 2 x MambaEncoderLayer(
        (self_selen): Selection(
          (khop_structure_extractor): KHopStructureExtractor(
            (structure_extractor): StructureExtractor(
              (gcn): ModuleList(
                (0): GATConv(128, 128, heads=1)
              )
              (relu): ReLU()
              (bn): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (out_proj): Linear(in_features=128, out_features=128, bias=True)
            )
            (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (out_proj): Linear(in_features=256, out_features=128, bias=True)
          )
          (out_proj): Mamba(
            (in_proj): Linear(in_features=128, out_features=512, bias=False)
            (conv1d): Conv1d(256, 256, kernel_size=(4,), stride=(1,), padding=(3,), groups=256)
            (act): SiLU()
            (x_proj): Linear(in_features=256, out_features=40, bias=False)
            (dt_proj): Linear(in_features=8, out_features=256, bias=True)
            (out_proj): Linear(in_features=256, out_features=128, bias=False)
          )
        )
        (norm1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (linear1): Linear(in_features=128, out_features=256, bias=True)
        (dropout): Dropout(p=0.6, inplace=False)
        (linear2): Linear(in_features=256, out_features=128, bias=True)
        (dropout1): Dropout(p=0.6, inplace=False)
        (dropout2): Dropout(p=0.6, inplace=False)
        (activation): ReLU()
      )
    )
  )
  (classifier): Sequential(
    (0): Linear(in_features=128, out_features=128, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=128, out_features=2, bias=True)
  )
)
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x75fef9a35ed0>
Training...
Epoch 1/1000, LR 0.000500
Train loss: 0.7367;  Loss pred: 0.7237; Loss self: 1.3002; time: 0.20s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6966 score: 0.5102 time: 0.08s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.7010 score: 0.4898 time: 0.10s
Epoch 2/1000, LR 0.000000
Train loss: 0.7362;  Loss pred: 0.7232; Loss self: 1.3024; time: 0.20s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6969 score: 0.5102 time: 0.08s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.7014 score: 0.4898 time: 0.10s
     INFO: Early stopping counter 1 of 20
Epoch 3/1000, LR 0.000050
Train loss: 0.7342;  Loss pred: 0.7205; Loss self: 1.3693; time: 0.19s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6974 score: 0.5102 time: 0.08s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.7021 score: 0.4898 time: 0.10s
     INFO: Early stopping counter 2 of 20
Epoch 4/1000, LR 0.000100
Train loss: 0.6864;  Loss pred: 0.6732; Loss self: 1.3222; time: 0.20s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6973 score: 0.5102 time: 0.08s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.7019 score: 0.4898 time: 0.10s
     INFO: Early stopping counter 3 of 20
Epoch 5/1000, LR 0.000150
Train loss: 0.6500;  Loss pred: 0.6371; Loss self: 1.2892; time: 0.19s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6968 score: 0.5102 time: 0.08s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.7013 score: 0.4898 time: 0.09s
     INFO: Early stopping counter 4 of 20
Epoch 6/1000, LR 0.000200
Train loss: 0.6018;  Loss pred: 0.5886; Loss self: 1.3199; time: 0.19s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6963 score: 0.5102 time: 0.08s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.7007 score: 0.4898 time: 0.09s
Epoch 7/1000, LR 0.000250
Train loss: 0.5454;  Loss pred: 0.5322; Loss self: 1.3203; time: 0.19s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6957 score: 0.5102 time: 0.08s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6999 score: 0.4898 time: 0.10s
Epoch 8/1000, LR 0.000300
Train loss: 0.4798;  Loss pred: 0.4661; Loss self: 1.3669; time: 0.19s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6947 score: 0.5102 time: 0.08s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6986 score: 0.4898 time: 0.10s
Epoch 9/1000, LR 0.000350
Train loss: 0.4148;  Loss pred: 0.4001; Loss self: 1.4712; time: 0.18s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6937 score: 0.5102 time: 0.08s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6971 score: 0.4898 time: 0.10s
Epoch 10/1000, LR 0.000400
Train loss: 0.3415;  Loss pred: 0.3261; Loss self: 1.5397; time: 0.17s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6920 score: 0.5102 time: 0.08s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6949 score: 0.4898 time: 0.10s
Epoch 11/1000, LR 0.000450
Train loss: 0.2852;  Loss pred: 0.2681; Loss self: 1.7099; time: 0.17s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6897 score: 0.5102 time: 0.08s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6919 score: 0.4898 time: 0.09s
Epoch 12/1000, LR 0.000450
Train loss: 0.2424;  Loss pred: 0.2240; Loss self: 1.8372; time: 0.17s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6868 score: 0.5102 time: 0.08s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6880 score: 0.4898 time: 0.10s
Epoch 13/1000, LR 0.000450
Train loss: 0.2112;  Loss pred: 0.1918; Loss self: 1.9391; time: 0.17s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6822 score: 0.5102 time: 0.08s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6822 score: 0.4898 time: 0.09s
Epoch 14/1000, LR 0.000450
Train loss: 0.1885;  Loss pred: 0.1681; Loss self: 2.0351; time: 0.19s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6752 score: 0.5102 time: 0.08s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6735 score: 0.4898 time: 0.09s
Epoch 15/1000, LR 0.000450
Train loss: 0.1668;  Loss pred: 0.1455; Loss self: 2.1314; time: 0.17s
Val loss: 0.6650 score: 0.5714 time: 0.08s
Test loss: 0.6607 score: 0.5306 time: 0.10s
Epoch 16/1000, LR 0.000450
Train loss: 0.1471;  Loss pred: 0.1253; Loss self: 2.1842; time: 0.15s
Val loss: 0.6508 score: 0.6735 time: 0.08s
Test loss: 0.6426 score: 0.6122 time: 0.09s
Epoch 17/1000, LR 0.000450
Train loss: 0.1332;  Loss pred: 0.1102; Loss self: 2.2924; time: 0.17s
Val loss: 0.6305 score: 0.8163 time: 0.08s
Test loss: 0.6169 score: 0.8571 time: 0.09s
Epoch 18/1000, LR 0.000450
Train loss: 0.1145;  Loss pred: 0.0918; Loss self: 2.2750; time: 0.18s
Val loss: 0.6034 score: 0.8980 time: 0.08s
Test loss: 0.5824 score: 0.9592 time: 0.09s
Epoch 19/1000, LR 0.000450
Train loss: 0.1021;  Loss pred: 0.0784; Loss self: 2.3705; time: 0.18s
Val loss: 0.5686 score: 0.9184 time: 0.08s
Test loss: 0.5373 score: 0.9592 time: 0.09s
Epoch 20/1000, LR 0.000450
Train loss: 0.0926;  Loss pred: 0.0687; Loss self: 2.3914; time: 0.18s
Val loss: 0.5268 score: 0.8980 time: 0.08s
Test loss: 0.4823 score: 0.9592 time: 0.09s
Epoch 21/1000, LR 0.000450
Train loss: 0.0828;  Loss pred: 0.0580; Loss self: 2.4790; time: 0.18s
Val loss: 0.4804 score: 0.8367 time: 0.08s
Test loss: 0.4187 score: 0.9796 time: 0.09s
Epoch 22/1000, LR 0.000450
Train loss: 0.0752;  Loss pred: 0.0501; Loss self: 2.5094; time: 0.19s
Val loss: 0.4348 score: 0.8367 time: 0.08s
Test loss: 0.3526 score: 0.9796 time: 0.09s
Epoch 23/1000, LR 0.000450
Train loss: 0.0671;  Loss pred: 0.0416; Loss self: 2.5501; time: 0.19s
Val loss: 0.3972 score: 0.8163 time: 0.08s
Test loss: 0.2922 score: 0.9592 time: 0.09s
Epoch 24/1000, LR 0.000450
Train loss: 0.0595;  Loss pred: 0.0333; Loss self: 2.6137; time: 0.18s
Val loss: 0.3717 score: 0.7959 time: 0.08s
Test loss: 0.2421 score: 0.9592 time: 0.09s
Epoch 25/1000, LR 0.000450
Train loss: 0.0544;  Loss pred: 0.0278; Loss self: 2.6598; time: 0.17s
Val loss: 0.3618 score: 0.7959 time: 0.08s
Test loss: 0.2053 score: 0.9592 time: 0.09s
Epoch 26/1000, LR 0.000450
Train loss: 0.0501;  Loss pred: 0.0232; Loss self: 2.6916; time: 0.17s
Val loss: 0.3682 score: 0.8163 time: 0.08s
Test loss: 0.1823 score: 0.9592 time: 0.10s
     INFO: Early stopping counter 1 of 20
Epoch 27/1000, LR 0.000450
Train loss: 0.0461;  Loss pred: 0.0190; Loss self: 2.7101; time: 0.18s
Val loss: 0.3891 score: 0.8163 time: 0.08s
Test loss: 0.1724 score: 0.9592 time: 0.09s
     INFO: Early stopping counter 2 of 20
Epoch 28/1000, LR 0.000450
Train loss: 0.0438;  Loss pred: 0.0164; Loss self: 2.7367; time: 0.17s
Val loss: 0.4220 score: 0.8163 time: 0.08s
Test loss: 0.1727 score: 0.9592 time: 0.10s
     INFO: Early stopping counter 3 of 20
Epoch 29/1000, LR 0.000450
Train loss: 0.0416;  Loss pred: 0.0140; Loss self: 2.7627; time: 0.24s
Val loss: 0.4633 score: 0.8163 time: 0.08s
Test loss: 0.1796 score: 0.9592 time: 0.09s
     INFO: Early stopping counter 4 of 20
Epoch 30/1000, LR 0.000450
Train loss: 0.0397;  Loss pred: 0.0119; Loss self: 2.7797; time: 0.16s
Val loss: 0.5162 score: 0.8163 time: 0.08s
Test loss: 0.1928 score: 0.9592 time: 0.09s
     INFO: Early stopping counter 5 of 20
Epoch 31/1000, LR 0.000450
Train loss: 0.0373;  Loss pred: 0.0094; Loss self: 2.7930; time: 0.17s
Val loss: 0.5761 score: 0.8163 time: 0.08s
Test loss: 0.2096 score: 0.9592 time: 0.09s
     INFO: Early stopping counter 6 of 20
Epoch 32/1000, LR 0.000450
Train loss: 0.0363;  Loss pred: 0.0085; Loss self: 2.7778; time: 0.18s
Val loss: 0.6394 score: 0.7959 time: 0.08s
Test loss: 0.2286 score: 0.9592 time: 0.09s
     INFO: Early stopping counter 7 of 20
Epoch 33/1000, LR 0.000449
Train loss: 0.0355;  Loss pred: 0.0073; Loss self: 2.8165; time: 0.17s
Val loss: 0.7030 score: 0.7959 time: 0.08s
Test loss: 0.2487 score: 0.9592 time: 0.09s
     INFO: Early stopping counter 8 of 20
Epoch 34/1000, LR 0.000449
Train loss: 0.0341;  Loss pred: 0.0061; Loss self: 2.8064; time: 0.25s
Val loss: 0.7667 score: 0.7959 time: 0.08s
Test loss: 0.2692 score: 0.9592 time: 0.09s
     INFO: Early stopping counter 9 of 20
Epoch 35/1000, LR 0.000449
Train loss: 0.0335;  Loss pred: 0.0055; Loss self: 2.8062; time: 0.17s
Val loss: 0.8284 score: 0.7959 time: 0.08s
Test loss: 0.2898 score: 0.9592 time: 0.09s
     INFO: Early stopping counter 10 of 20
Epoch 36/1000, LR 0.000449
Train loss: 0.0331;  Loss pred: 0.0048; Loss self: 2.8253; time: 0.18s
Val loss: 0.8852 score: 0.7959 time: 0.08s
Test loss: 0.3094 score: 0.9592 time: 0.10s
     INFO: Early stopping counter 11 of 20
Epoch 37/1000, LR 0.000449
Train loss: 0.0323;  Loss pred: 0.0043; Loss self: 2.8048; time: 0.17s
Val loss: 0.9334 score: 0.7959 time: 0.08s
Test loss: 0.3264 score: 0.9592 time: 0.10s
     INFO: Early stopping counter 12 of 20
Epoch 38/1000, LR 0.000449
Train loss: 0.0317;  Loss pred: 0.0036; Loss self: 2.8021; time: 0.19s
Val loss: 0.9766 score: 0.7959 time: 0.08s
Test loss: 0.3419 score: 0.9592 time: 0.15s
     INFO: Early stopping counter 13 of 20
Epoch 39/1000, LR 0.000449
Train loss: 0.0314;  Loss pred: 0.0034; Loss self: 2.8040; time: 0.17s
Val loss: 1.0172 score: 0.7959 time: 0.08s
Test loss: 0.3566 score: 0.9592 time: 0.09s
     INFO: Early stopping counter 14 of 20
Epoch 40/1000, LR 0.000449
Train loss: 0.0309;  Loss pred: 0.0030; Loss self: 2.7947; time: 0.17s
Val loss: 1.0496 score: 0.7959 time: 0.08s
Test loss: 0.3686 score: 0.9388 time: 0.09s
     INFO: Early stopping counter 15 of 20
Epoch 41/1000, LR 0.000449
Train loss: 0.0310;  Loss pred: 0.0028; Loss self: 2.8263; time: 0.16s
Val loss: 1.0745 score: 0.7959 time: 0.08s
Test loss: 0.3777 score: 0.9388 time: 0.09s
     INFO: Early stopping counter 16 of 20
Epoch 42/1000, LR 0.000449
Train loss: 0.0304;  Loss pred: 0.0024; Loss self: 2.7977; time: 0.17s
Val loss: 1.0933 score: 0.7959 time: 0.08s
Test loss: 0.3848 score: 0.9388 time: 0.09s
     INFO: Early stopping counter 17 of 20
Epoch 43/1000, LR 0.000449
Train loss: 0.0301;  Loss pred: 0.0022; Loss self: 2.7906; time: 0.17s
Val loss: 1.1102 score: 0.7959 time: 0.08s
Test loss: 0.3909 score: 0.9184 time: 0.18s
     INFO: Early stopping counter 18 of 20
Epoch 44/1000, LR 0.000449
Train loss: 0.0303;  Loss pred: 0.0022; Loss self: 2.8096; time: 0.17s
Val loss: 1.1206 score: 0.7959 time: 0.13s
Test loss: 0.3944 score: 0.9184 time: 0.09s
     INFO: Early stopping counter 19 of 20
Epoch 45/1000, LR 0.000449
Train loss: 0.0299;  Loss pred: 0.0020; Loss self: 2.7964; time: 0.17s
Val loss: 1.1264 score: 0.7959 time: 0.08s
Test loss: 0.3961 score: 0.9184 time: 0.09s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 024,   Train_Loss: 0.0544,   Val_Loss: 0.3618,   Val_Precision: 0.9412,   Val_Recall: 0.6400,   Val_accuracy: 0.7619,   Val_Score: 0.7959,   Val_Loss: 0.3618,   Test_Precision: 1.0000,   Test_Recall: 0.9167,   Test_accuracy: 0.9565,   Test_Score: 0.9592,   Test_loss: 0.2053


[0.08868790604174137, 0.08555980795063078, 0.08653830387629569, 0.08673247206024826, 0.08630424621514976, 0.08727480913512409, 0.08869838993996382, 0.08860889310017228, 0.09053831291384995, 0.0881303739733994, 0.08791551901958883, 0.08710404206067324, 0.08654724503867328, 0.08733428199775517, 0.08720076805911958, 0.08854636386968195, 0.08735093497671187, 0.08839433803223073, 0.08686166699044406, 0.08937912294641137, 0.08694926486350596, 0.09359746286645532, 0.08908455912023783, 0.08866356988437474, 0.08889057789929211, 0.08825293998233974, 0.08787804306484759, 0.08962543704546988, 0.08862893492914736, 0.08906667795963585, 0.07880508597008884, 0.08410700084641576, 0.08133646799251437, 0.08194253593683243, 0.08141453005373478, 0.08399981912225485, 0.08226044406183064, 0.0834080190397799, 0.084027084056288, 0.0835539388936013, 0.08253465313464403, 0.08295532199554145, 0.0822397219017148, 0.08182029495947063, 0.08147235703654587, 0.08331819693557918, 0.08544576494023204, 0.08544697193428874, 0.08595926687121391, 0.08565382100641727, 0.08561150799505413, 0.08553159493021667, 0.1011912189424038, 0.10233900416642427, 0.10144496895372868, 0.1019477709196508, 0.10027416399680078, 0.09983974206261337, 0.10084437904879451, 0.10049885814078152, 0.10441705002449453, 0.10104902600869536, 0.10004121111705899, 0.10043525695800781, 0.10031904187053442, 0.09995591617189348, 0.10378323495388031, 0.09560914803296328, 0.09695570194162428, 0.09681654092855752, 0.09638602589257061, 0.09694276796653867, 0.09697968303225935, 0.09886177605949342, 0.09739911090582609, 0.09801705600693822, 0.0990943880751729, 0.10141043714247644, 0.09980793204158545, 0.10219837608747184, 0.09885643399320543, 0.09857172495685518, 0.09821742004714906, 0.098803288070485, 0.09972629882395267, 0.09971079998649657, 0.09922528406605124, 0.1009586350992322, 0.10152359399944544, 0.1584525858052075, 0.0990030427929014, 0.09815262001939118, 0.09867467498406768, 0.09486906416714191, 0.18322395207360387, 0.0985443179961294, 0.09953301399946213]
[0.0018099572661579872, 0.00174611852960471, 0.001766087834210116, 0.0017700504502091482, 0.0017613111472479543, 0.0017811185537780427, 0.0018101712232645676, 0.001808344757146373, 0.0018477206717112235, 0.0017985790606816203, 0.0017941942657058944, 0.001777633511442311, 0.0017662703069116995, 0.0017823322856684728, 0.0017796075114106036, 0.0018070686504016726, 0.0017826721423818749, 0.0018039660822904231, 0.001772687081437634, 0.0018240637336002321, 0.0017744747931327746, 0.0019101523033970473, 0.0018180522269436292, 0.0018094606098851987, 0.0018140934265161656, 0.0018010804078028519, 0.001793429450303012, 0.0018290905519483648, 0.001808753774064232, 0.001817687305298691, 0.0016082670606140579, 0.001716469405028893, 0.001659927918214579, 0.0016722966517720903, 0.0016615210215047914, 0.0017142820229031602, 0.0016787845726904211, 0.00170220447019959, 0.0017148384501283265, 0.0017051824264000266, 0.001684380676217225, 0.00169296575501105, 0.0016783616714635674, 0.0016698019379483803, 0.0016627011640111403, 0.0017003713660322282, 0.0017437911212292252, 0.0017438157537609947, 0.0017542707524737533, 0.001748037163396271, 0.001747173632552125, 0.0017455427536778912, 0.002065126917191914, 0.0020885511054372303, 0.0020703054888516056, 0.002080566753462261, 0.0020464115101387916, 0.0020375457563798645, 0.0020580485520162142, 0.0020509971049139084, 0.002130960204581521, 0.0020622250205856196, 0.002041657369735898, 0.0020496991215919963, 0.0020473273851129474, 0.002039916656569255, 0.0021180252031404146, 0.0019512071027135362, 0.001978687794727026, 0.001975847774052194, 0.0019670617529096044, 0.0019784238360518094, 0.0019791772047399866, 0.002017587266520274, 0.0019877369572617568, 0.0020003480817742496, 0.002022334450513733, 0.002069600758009723, 0.002036896572277254, 0.0020856811446422823, 0.0020174782447592945, 0.0020116678562623505, 0.0020044371438193687, 0.0020163936340915306, 0.002035230588243932, 0.0020349142854387053, 0.0020250057972663518, 0.0020603803081475957, 0.0020719100816213353, 0.003233726240922602, 0.0020204702610796205, 0.0020031146942732893, 0.002013768877225871, 0.001936103350349835, 0.003739264328032732, 0.002011108530533253, 0.002031285999989023]
[552.4992322734278, 572.698807695708, 566.2232538095992, 564.9556485137701, 567.7588548522493, 561.4449402477096, 552.4339284305614, 552.9918982805207, 541.207345520399, 555.9944635522571, 557.3532471449336, 562.5456504747339, 566.1647575044653, 561.0626077083853, 561.9216560888481, 553.3824073466835, 560.9556441847315, 554.334147308546, 564.1153537312455, 548.2264580888616, 563.5470302931349, 523.5184640625688, 550.0392041438346, 552.6508808961833, 551.2395256954474, 555.2222963881471, 557.5909327412033, 546.7197886593369, 552.8668491748442, 550.1496308440551, 621.787279295633, 582.5912172219389, 602.4357979806747, 597.9800288066865, 601.8581691457199, 583.3345894314905, 595.66904310861, 587.4734895289907, 583.1453102332566, 586.4475170033247, 593.6900215726743, 590.6794021320726, 595.8191354119631, 598.8734216158957, 601.4309857025517, 588.1068218253249, 573.4631790618847, 573.4550785214772, 570.0374349796734, 572.0702173500102, 572.3529598711284, 572.887715235265, 484.2317397904843, 478.8008286685682, 483.0205036816562, 480.63826759506986, 488.66026947443146, 490.7865243609123, 485.89718596304596, 487.5677286935885, 469.27202012032905, 484.91313509329126, 489.7981487115817, 487.876483658393, 488.44166657050397, 490.2161060255308, 472.1379134286463, 512.5032594486274, 505.3854391101437, 506.11186404767227, 508.3724486640225, 505.45286696283654, 505.260467635274, 495.6415103296606, 503.08467443175584, 499.91299469891743, 494.47805220643426, 483.1849795811207, 490.94294408969336, 479.45967319540165, 495.6682941179919, 497.0999545909062, 498.8931696279301, 495.93491225761665, 491.34481654132117, 491.4211901482675, 493.82574674598254, 485.34729051990377, 482.64642798468753, 309.24077225371246, 494.9342830048183, 499.22253721112577, 496.5813164108389, 516.5013529981804, 267.4322840734051, 497.23820709708104, 492.29896725788683]
Elapsed: 0.09379480622430361~0.013345800900284482
Time per graph: 0.0019141797188633387~0.00027236328367927513
Speed: 529.7431437089886~54.09985993552446
Total Time: 0.1001
best val loss: 0.3617675304412842 test_score: 0.9592

Testing...
Test loss: 0.5373 score: 0.9592 time: 0.09s
test Score 0.9592
Epoch Time List: [0.6790760902222246, 0.35661386302672327, 0.35290941200219095, 0.3541223732754588, 0.35498873703181744, 0.3589131897315383, 0.3625092161819339, 0.36397642781957984, 0.36850034911185503, 0.3686835339758545, 0.3692579453345388, 0.3654741251375526, 0.3682142607867718, 0.37336379708722234, 0.3764678828883916, 0.3678974041249603, 0.36057250504381955, 0.3565560458227992, 0.3600418381392956, 0.3570455159060657, 0.358668162021786, 0.3635019559878856, 0.3641638557892293, 0.35952282208018005, 0.36017922731116414, 0.35776893608272076, 0.3668504108209163, 0.3605713527649641, 0.35927733732387424, 0.3600620608776808, 0.3535300891380757, 0.36210372229106724, 0.3605109441559762, 0.3610345299821347, 0.3663574359379709, 0.3676249429117888, 0.37104864581488073, 0.36558001511730254, 0.3709781488869339, 0.3669571648351848, 0.3663695661816746, 0.3644669149070978, 0.36305679799988866, 0.3680954179726541, 0.36500784289091825, 0.3709748031105846, 0.3679910618811846, 0.37095806980505586, 0.3552660408895463, 0.3524673320353031, 0.35183134605176747, 0.35209273220971227, 0.37353807175531983, 0.37402793602086604, 0.37373947887681425, 0.3738432477694005, 0.3647671320941299, 0.3654064948204905, 0.3669402450323105, 0.36659576604142785, 0.35712517402134836, 0.34723248123191297, 0.3457614949438721, 0.34471172210760415, 0.34532080474309623, 0.3617052079644054, 0.35018516378477216, 0.3221908158157021, 0.3378669440280646, 0.34957072092220187, 0.35101412516087294, 0.3505359592381865, 0.35365676530636847, 0.3600763699505478, 0.35751147917471826, 0.3559255041182041, 0.33899599593132734, 0.34370331512764096, 0.3580065609421581, 0.34073036909103394, 0.4172920058481395, 0.3374946997500956, 0.3416732628829777, 0.35323865828104317, 0.3427486589644104, 0.4260730657260865, 0.3426027800887823, 0.358418659074232, 0.34426417597569525, 0.41976615390740335, 0.34010280785150826, 0.3360336811747402, 0.33632804709486663, 0.3366897322703153, 0.42683517280966043, 0.4001464191824198, 0.3382673820015043]
Total Epoch List: [52, 45]
Total Time List: [0.08595785195939243, 0.10010621300898492]
========================k_idx:2========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: SAMamba
GraphEncoder(
  (subgraph_conv): GATConv(128, 128, heads=1)
  (embedding): Linear(in_features=14887, out_features=128, bias=False)
  (encoder): GraphMambaEncoder(
    (layers): ModuleList(
      (0-1): 2 x MambaEncoderLayer(
        (self_selen): Selection(
          (khop_structure_extractor): KHopStructureExtractor(
            (structure_extractor): StructureExtractor(
              (gcn): ModuleList(
                (0): GATConv(128, 128, heads=1)
              )
              (relu): ReLU()
              (bn): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (out_proj): Linear(in_features=128, out_features=128, bias=True)
            )
            (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (out_proj): Linear(in_features=256, out_features=128, bias=True)
          )
          (out_proj): Mamba(
            (in_proj): Linear(in_features=128, out_features=512, bias=False)
            (conv1d): Conv1d(256, 256, kernel_size=(4,), stride=(1,), padding=(3,), groups=256)
            (act): SiLU()
            (x_proj): Linear(in_features=256, out_features=40, bias=False)
            (dt_proj): Linear(in_features=8, out_features=256, bias=True)
            (out_proj): Linear(in_features=256, out_features=128, bias=False)
          )
        )
        (norm1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (linear1): Linear(in_features=128, out_features=256, bias=True)
        (dropout): Dropout(p=0.6, inplace=False)
        (linear2): Linear(in_features=256, out_features=128, bias=True)
        (dropout1): Dropout(p=0.6, inplace=False)
        (dropout2): Dropout(p=0.6, inplace=False)
        (activation): ReLU()
      )
    )
  )
  (classifier): Sequential(
    (0): Linear(in_features=128, out_features=128, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=128, out_features=2, bias=True)
  )
)
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x75fef9a36d70>
Training...
Epoch 1/1000, LR 0.000500
Train loss: 0.7039;  Loss pred: 0.6910; Loss self: 1.2911; time: 0.19s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6951 score: 0.4898 time: 0.12s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6943 score: 0.5000 time: 0.09s
Epoch 2/1000, LR 0.000000
Train loss: 0.6975;  Loss pred: 0.6836; Loss self: 1.3927; time: 0.18s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6956 score: 0.4898 time: 0.09s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6945 score: 0.5000 time: 0.09s
     INFO: Early stopping counter 1 of 20
Epoch 3/1000, LR 0.000050
Train loss: 0.7163;  Loss pred: 0.7031; Loss self: 1.3181; time: 0.17s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6961 score: 0.4898 time: 0.09s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6948 score: 0.5000 time: 0.09s
     INFO: Early stopping counter 2 of 20
Epoch 4/1000, LR 0.000100
Train loss: 0.6710;  Loss pred: 0.6571; Loss self: 1.3829; time: 0.18s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6963 score: 0.4898 time: 0.09s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6949 score: 0.5000 time: 0.10s
     INFO: Early stopping counter 3 of 20
Epoch 5/1000, LR 0.000150
Train loss: 0.6293;  Loss pred: 0.6161; Loss self: 1.3188; time: 0.17s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6965 score: 0.4898 time: 0.09s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6950 score: 0.5000 time: 0.18s
     INFO: Early stopping counter 4 of 20
Epoch 6/1000, LR 0.000200
Train loss: 0.5812;  Loss pred: 0.5681; Loss self: 1.3064; time: 0.18s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6966 score: 0.4898 time: 0.09s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6950 score: 0.5000 time: 0.09s
     INFO: Early stopping counter 5 of 20
Epoch 7/1000, LR 0.000250
Train loss: 0.5313;  Loss pred: 0.5176; Loss self: 1.3700; time: 0.18s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6969 score: 0.4898 time: 0.09s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6952 score: 0.5000 time: 0.09s
     INFO: Early stopping counter 6 of 20
Epoch 8/1000, LR 0.000300
Train loss: 0.4746;  Loss pred: 0.4611; Loss self: 1.3513; time: 0.19s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6972 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6953 score: 0.5000 time: 0.09s
     INFO: Early stopping counter 7 of 20
Epoch 9/1000, LR 0.000350
Train loss: 0.4191;  Loss pred: 0.4046; Loss self: 1.4581; time: 0.18s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6977 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6955 score: 0.5000 time: 0.09s
     INFO: Early stopping counter 8 of 20
Epoch 10/1000, LR 0.000400
Train loss: 0.3684;  Loss pred: 0.3527; Loss self: 1.5732; time: 0.28s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6977 score: 0.4898 time: 0.10s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6952 score: 0.5000 time: 0.09s
     INFO: Early stopping counter 9 of 20
Epoch 11/1000, LR 0.000450
Train loss: 0.3275;  Loss pred: 0.3109; Loss self: 1.6660; time: 0.20s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6972 score: 0.4898 time: 0.10s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6943 score: 0.5000 time: 0.09s
     INFO: Early stopping counter 10 of 20
Epoch 12/1000, LR 0.000450
Train loss: 0.2924;  Loss pred: 0.2747; Loss self: 1.7716; time: 0.17s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6959 score: 0.4898 time: 0.09s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6923 score: 0.5000 time: 0.09s
     INFO: Early stopping counter 11 of 20
Epoch 13/1000, LR 0.000450
Train loss: 0.2640;  Loss pred: 0.2449; Loss self: 1.9117; time: 0.19s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6933 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6889 score: 0.5000 time: 0.09s
Epoch 14/1000, LR 0.000450
Train loss: 0.2370;  Loss pred: 0.2178; Loss self: 1.9185; time: 0.25s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6890 score: 0.4898 time: 0.09s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6833 score: 0.5000 time: 0.09s
Epoch 15/1000, LR 0.000450
Train loss: 0.2162;  Loss pred: 0.1961; Loss self: 2.0159; time: 0.17s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6820 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6743 score: 0.5000 time: 0.09s
Epoch 16/1000, LR 0.000450
Train loss: 0.1969;  Loss pred: 0.1764; Loss self: 2.0462; time: 0.16s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6714 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6610 score: 0.5000 time: 0.09s
Epoch 17/1000, LR 0.000450
Train loss: 0.1792;  Loss pred: 0.1579; Loss self: 2.1289; time: 0.17s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6562 score: 0.4898 time: 0.09s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6420 score: 0.5000 time: 0.09s
Epoch 18/1000, LR 0.000450
Train loss: 0.1635;  Loss pred: 0.1421; Loss self: 2.1467; time: 0.19s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6371 score: 0.4898 time: 0.11s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6175 score: 0.5000 time: 0.09s
Epoch 19/1000, LR 0.000450
Train loss: 0.1510;  Loss pred: 0.1293; Loss self: 2.1732; time: 0.17s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6128 score: 0.4898 time: 0.09s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.5859 score: 0.5000 time: 0.09s
Epoch 20/1000, LR 0.000450
Train loss: 0.1383;  Loss pred: 0.1167; Loss self: 2.1559; time: 0.19s
Val loss: 0.5850 score: 0.5510 time: 0.09s
Test loss: 0.5485 score: 0.6042 time: 0.09s
Epoch 21/1000, LR 0.000450
Train loss: 0.1275;  Loss pred: 0.1052; Loss self: 2.2256; time: 0.17s
Val loss: 0.5544 score: 0.6735 time: 0.08s
Test loss: 0.5063 score: 0.6875 time: 0.09s
Epoch 22/1000, LR 0.000450
Train loss: 0.1179;  Loss pred: 0.0952; Loss self: 2.2747; time: 0.17s
Val loss: 0.5246 score: 0.7959 time: 0.20s
Test loss: 0.4631 score: 0.8125 time: 0.09s
Epoch 23/1000, LR 0.000450
Train loss: 0.1045;  Loss pred: 0.0813; Loss self: 2.3155; time: 0.17s
Val loss: 0.4971 score: 0.7959 time: 0.09s
Test loss: 0.4195 score: 0.8958 time: 0.09s
Epoch 24/1000, LR 0.000450
Train loss: 0.0960;  Loss pred: 0.0728; Loss self: 2.3148; time: 0.18s
Val loss: 0.4754 score: 0.8163 time: 0.08s
Test loss: 0.3799 score: 0.8958 time: 0.09s
Epoch 25/1000, LR 0.000450
Train loss: 0.0858;  Loss pred: 0.0622; Loss self: 2.3543; time: 0.17s
Val loss: 0.4589 score: 0.8163 time: 0.09s
Test loss: 0.3452 score: 0.8958 time: 0.09s
Epoch 26/1000, LR 0.000450
Train loss: 0.0789;  Loss pred: 0.0550; Loss self: 2.3946; time: 0.17s
Val loss: 0.4503 score: 0.8367 time: 0.08s
Test loss: 0.3183 score: 0.8958 time: 0.09s
Epoch 27/1000, LR 0.000450
Train loss: 0.0720;  Loss pred: 0.0479; Loss self: 2.4074; time: 0.16s
Val loss: 0.4517 score: 0.8367 time: 0.09s
Test loss: 0.3015 score: 0.9167 time: 0.10s
     INFO: Early stopping counter 1 of 20
Epoch 28/1000, LR 0.000450
Train loss: 0.0652;  Loss pred: 0.0410; Loss self: 2.4151; time: 0.18s
Val loss: 0.4612 score: 0.8367 time: 0.09s
Test loss: 0.2929 score: 0.9167 time: 0.09s
     INFO: Early stopping counter 2 of 20
Epoch 29/1000, LR 0.000450
Train loss: 0.0589;  Loss pred: 0.0344; Loss self: 2.4535; time: 0.19s
Val loss: 0.4789 score: 0.8367 time: 0.08s
Test loss: 0.2931 score: 0.9167 time: 0.08s
     INFO: Early stopping counter 3 of 20
Epoch 30/1000, LR 0.000450
Train loss: 0.0546;  Loss pred: 0.0298; Loss self: 2.4855; time: 0.19s
Val loss: 0.5009 score: 0.8367 time: 0.08s
Test loss: 0.2990 score: 0.9167 time: 0.08s
     INFO: Early stopping counter 4 of 20
Epoch 31/1000, LR 0.000450
Train loss: 0.0506;  Loss pred: 0.0256; Loss self: 2.4997; time: 0.20s
Val loss: 0.5263 score: 0.8367 time: 0.08s
Test loss: 0.3103 score: 0.9167 time: 0.08s
     INFO: Early stopping counter 5 of 20
Epoch 32/1000, LR 0.000450
Train loss: 0.0472;  Loss pred: 0.0222; Loss self: 2.4968; time: 0.19s
Val loss: 0.5486 score: 0.8571 time: 0.08s
Test loss: 0.3226 score: 0.9167 time: 0.08s
     INFO: Early stopping counter 6 of 20
Epoch 33/1000, LR 0.000449
Train loss: 0.0450;  Loss pred: 0.0197; Loss self: 2.5247; time: 0.19s
Val loss: 0.5680 score: 0.8571 time: 0.08s
Test loss: 0.3363 score: 0.9167 time: 0.08s
     INFO: Early stopping counter 7 of 20
Epoch 34/1000, LR 0.000449
Train loss: 0.0423;  Loss pred: 0.0168; Loss self: 2.5499; time: 0.20s
Val loss: 0.5847 score: 0.8776 time: 0.08s
Test loss: 0.3508 score: 0.9167 time: 0.08s
     INFO: Early stopping counter 8 of 20
Epoch 35/1000, LR 0.000449
Train loss: 0.0400;  Loss pred: 0.0145; Loss self: 2.5488; time: 0.20s
Val loss: 0.6013 score: 0.8980 time: 0.08s
Test loss: 0.3663 score: 0.9167 time: 0.08s
     INFO: Early stopping counter 9 of 20
Epoch 36/1000, LR 0.000449
Train loss: 0.0383;  Loss pred: 0.0127; Loss self: 2.5665; time: 0.20s
Val loss: 0.6159 score: 0.8980 time: 0.08s
Test loss: 0.3814 score: 0.9167 time: 0.08s
     INFO: Early stopping counter 10 of 20
Epoch 37/1000, LR 0.000449
Train loss: 0.0360;  Loss pred: 0.0106; Loss self: 2.5447; time: 0.18s
Val loss: 0.6321 score: 0.8980 time: 0.08s
Test loss: 0.3975 score: 0.9167 time: 0.08s
     INFO: Early stopping counter 11 of 20
Epoch 38/1000, LR 0.000449
Train loss: 0.0347;  Loss pred: 0.0092; Loss self: 2.5495; time: 0.18s
Val loss: 0.6467 score: 0.8980 time: 0.08s
Test loss: 0.4119 score: 0.9167 time: 0.08s
     INFO: Early stopping counter 12 of 20
Epoch 39/1000, LR 0.000449
Train loss: 0.0335;  Loss pred: 0.0081; Loss self: 2.5313; time: 0.19s
Val loss: 0.6654 score: 0.8980 time: 0.08s
Test loss: 0.4286 score: 0.9167 time: 0.08s
     INFO: Early stopping counter 13 of 20
Epoch 40/1000, LR 0.000449
Train loss: 0.0326;  Loss pred: 0.0070; Loss self: 2.5539; time: 0.18s
Val loss: 0.6856 score: 0.8980 time: 0.08s
Test loss: 0.4457 score: 0.9167 time: 0.08s
     INFO: Early stopping counter 14 of 20
Epoch 41/1000, LR 0.000449
Train loss: 0.0316;  Loss pred: 0.0064; Loss self: 2.5261; time: 0.18s
Val loss: 0.7038 score: 0.8980 time: 0.08s
Test loss: 0.4608 score: 0.9167 time: 0.08s
     INFO: Early stopping counter 15 of 20
Epoch 42/1000, LR 0.000449
Train loss: 0.0312;  Loss pred: 0.0060; Loss self: 2.5263; time: 0.17s
Val loss: 0.7220 score: 0.8980 time: 0.08s
Test loss: 0.4748 score: 0.9167 time: 0.08s
     INFO: Early stopping counter 16 of 20
Epoch 43/1000, LR 0.000449
Train loss: 0.0304;  Loss pred: 0.0051; Loss self: 2.5309; time: 0.18s
Val loss: 0.7384 score: 0.8776 time: 0.08s
Test loss: 0.4872 score: 0.9167 time: 0.08s
     INFO: Early stopping counter 17 of 20
Epoch 44/1000, LR 0.000449
Train loss: 0.0298;  Loss pred: 0.0047; Loss self: 2.5091; time: 0.18s
Val loss: 0.7562 score: 0.8776 time: 0.08s
Test loss: 0.4999 score: 0.9167 time: 0.08s
     INFO: Early stopping counter 18 of 20
Epoch 45/1000, LR 0.000449
Train loss: 0.0293;  Loss pred: 0.0042; Loss self: 2.5149; time: 0.18s
Val loss: 0.7705 score: 0.8776 time: 0.08s
Test loss: 0.5095 score: 0.9167 time: 0.08s
     INFO: Early stopping counter 19 of 20
Epoch 46/1000, LR 0.000449
Train loss: 0.0290;  Loss pred: 0.0039; Loss self: 2.5096; time: 0.18s
Val loss: 0.7829 score: 0.8776 time: 0.08s
Test loss: 0.5175 score: 0.8958 time: 0.09s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 025,   Train_Loss: 0.0789,   Val_Loss: 0.4503,   Val_Precision: 0.9474,   Val_Recall: 0.7200,   Val_accuracy: 0.8182,   Val_Score: 0.8367,   Val_Loss: 0.4503,   Test_Precision: 1.0000,   Test_Recall: 0.7917,   Test_accuracy: 0.8837,   Test_Score: 0.8958,   Test_loss: 0.3183


[0.08868790604174137, 0.08555980795063078, 0.08653830387629569, 0.08673247206024826, 0.08630424621514976, 0.08727480913512409, 0.08869838993996382, 0.08860889310017228, 0.09053831291384995, 0.0881303739733994, 0.08791551901958883, 0.08710404206067324, 0.08654724503867328, 0.08733428199775517, 0.08720076805911958, 0.08854636386968195, 0.08735093497671187, 0.08839433803223073, 0.08686166699044406, 0.08937912294641137, 0.08694926486350596, 0.09359746286645532, 0.08908455912023783, 0.08866356988437474, 0.08889057789929211, 0.08825293998233974, 0.08787804306484759, 0.08962543704546988, 0.08862893492914736, 0.08906667795963585, 0.07880508597008884, 0.08410700084641576, 0.08133646799251437, 0.08194253593683243, 0.08141453005373478, 0.08399981912225485, 0.08226044406183064, 0.0834080190397799, 0.084027084056288, 0.0835539388936013, 0.08253465313464403, 0.08295532199554145, 0.0822397219017148, 0.08182029495947063, 0.08147235703654587, 0.08331819693557918, 0.08544576494023204, 0.08544697193428874, 0.08595926687121391, 0.08565382100641727, 0.08561150799505413, 0.08553159493021667, 0.1011912189424038, 0.10233900416642427, 0.10144496895372868, 0.1019477709196508, 0.10027416399680078, 0.09983974206261337, 0.10084437904879451, 0.10049885814078152, 0.10441705002449453, 0.10104902600869536, 0.10004121111705899, 0.10043525695800781, 0.10031904187053442, 0.09995591617189348, 0.10378323495388031, 0.09560914803296328, 0.09695570194162428, 0.09681654092855752, 0.09638602589257061, 0.09694276796653867, 0.09697968303225935, 0.09886177605949342, 0.09739911090582609, 0.09801705600693822, 0.0990943880751729, 0.10141043714247644, 0.09980793204158545, 0.10219837608747184, 0.09885643399320543, 0.09857172495685518, 0.09821742004714906, 0.098803288070485, 0.09972629882395267, 0.09971079998649657, 0.09922528406605124, 0.1009586350992322, 0.10152359399944544, 0.1584525858052075, 0.0990030427929014, 0.09815262001939118, 0.09867467498406768, 0.09486906416714191, 0.18322395207360387, 0.0985443179961294, 0.09953301399946213, 0.09432585909962654, 0.0956712809856981, 0.0950129609555006, 0.10110834916122258, 0.18313720892183483, 0.09429266001097858, 0.09648145386017859, 0.09286450408399105, 0.09759901906363666, 0.09604494995437562, 0.0939522439148277, 0.09590744692832232, 0.09603497898206115, 0.09323128988035023, 0.09393381420522928, 0.09544166992418468, 0.09706096001900733, 0.09523848607204854, 0.09531238209456205, 0.09550548600964248, 0.09197799093089998, 0.09235792607069016, 0.09452732698991895, 0.095194490859285, 0.09363298886455595, 0.09232630115002394, 0.10371342184953392, 0.09131094114854932, 0.08951157983392477, 0.08973708585835993, 0.09010750893503428, 0.09004179411567748, 0.08978205686435103, 0.08950372296385467, 0.08922646287828684, 0.08905841503292322, 0.08956949901767075, 0.08948263805359602, 0.08918772311881185, 0.08912605489604175, 0.08963309903629124, 0.08940311498008668, 0.08935921103693545, 0.08949558599852026, 0.08887281804345548, 0.09025224391371012]
[0.0018099572661579872, 0.00174611852960471, 0.001766087834210116, 0.0017700504502091482, 0.0017613111472479543, 0.0017811185537780427, 0.0018101712232645676, 0.001808344757146373, 0.0018477206717112235, 0.0017985790606816203, 0.0017941942657058944, 0.001777633511442311, 0.0017662703069116995, 0.0017823322856684728, 0.0017796075114106036, 0.0018070686504016726, 0.0017826721423818749, 0.0018039660822904231, 0.001772687081437634, 0.0018240637336002321, 0.0017744747931327746, 0.0019101523033970473, 0.0018180522269436292, 0.0018094606098851987, 0.0018140934265161656, 0.0018010804078028519, 0.001793429450303012, 0.0018290905519483648, 0.001808753774064232, 0.001817687305298691, 0.0016082670606140579, 0.001716469405028893, 0.001659927918214579, 0.0016722966517720903, 0.0016615210215047914, 0.0017142820229031602, 0.0016787845726904211, 0.00170220447019959, 0.0017148384501283265, 0.0017051824264000266, 0.001684380676217225, 0.00169296575501105, 0.0016783616714635674, 0.0016698019379483803, 0.0016627011640111403, 0.0017003713660322282, 0.0017437911212292252, 0.0017438157537609947, 0.0017542707524737533, 0.001748037163396271, 0.001747173632552125, 0.0017455427536778912, 0.002065126917191914, 0.0020885511054372303, 0.0020703054888516056, 0.002080566753462261, 0.0020464115101387916, 0.0020375457563798645, 0.0020580485520162142, 0.0020509971049139084, 0.002130960204581521, 0.0020622250205856196, 0.002041657369735898, 0.0020496991215919963, 0.0020473273851129474, 0.002039916656569255, 0.0021180252031404146, 0.0019512071027135362, 0.001978687794727026, 0.001975847774052194, 0.0019670617529096044, 0.0019784238360518094, 0.0019791772047399866, 0.002017587266520274, 0.0019877369572617568, 0.0020003480817742496, 0.002022334450513733, 0.002069600758009723, 0.002036896572277254, 0.0020856811446422823, 0.0020174782447592945, 0.0020116678562623505, 0.0020044371438193687, 0.0020163936340915306, 0.002035230588243932, 0.0020349142854387053, 0.0020250057972663518, 0.0020603803081475957, 0.0020719100816213353, 0.003233726240922602, 0.0020204702610796205, 0.0020031146942732893, 0.002013768877225871, 0.001936103350349835, 0.003739264328032732, 0.002011108530533253, 0.002031285999989023, 0.001965122064575553, 0.001993151687202044, 0.0019794366865729294, 0.0021064239408588037, 0.0038153585192048922, 0.0019644304168953872, 0.0020100302887537205, 0.0019346771684164803, 0.0020333128971590972, 0.0020009364573828257, 0.0019573384148922437, 0.001998071811006715, 0.0020007287287929407, 0.0019423185391739632, 0.0019569544626089432, 0.001988368123420514, 0.0020221033337293193, 0.001984135126501011, 0.001985674626970043, 0.001989697625200885, 0.0019162081443937495, 0.0019241234598060448, 0.001969319312289978, 0.0019832185595684373, 0.0019506872680115823, 0.0019234646072921653, 0.0021606962885319567, 0.0019023112739281107, 0.0018648245798734326, 0.0018695226220491652, 0.0018772397694798808, 0.0018758707107432808, 0.0018704595180073131, 0.0018646608950803056, 0.0018588846432976425, 0.0018553836465192337, 0.0018660312295348074, 0.0018642216261165838, 0.0018580775649752468, 0.001856792810334203, 0.0018673562299227342, 0.0018625648954184726, 0.0018616502299361553, 0.001864491374969172, 0.0018515170425719891, 0.0018802550815356274]
[552.4992322734278, 572.698807695708, 566.2232538095992, 564.9556485137701, 567.7588548522493, 561.4449402477096, 552.4339284305614, 552.9918982805207, 541.207345520399, 555.9944635522571, 557.3532471449336, 562.5456504747339, 566.1647575044653, 561.0626077083853, 561.9216560888481, 553.3824073466835, 560.9556441847315, 554.334147308546, 564.1153537312455, 548.2264580888616, 563.5470302931349, 523.5184640625688, 550.0392041438346, 552.6508808961833, 551.2395256954474, 555.2222963881471, 557.5909327412033, 546.7197886593369, 552.8668491748442, 550.1496308440551, 621.787279295633, 582.5912172219389, 602.4357979806747, 597.9800288066865, 601.8581691457199, 583.3345894314905, 595.66904310861, 587.4734895289907, 583.1453102332566, 586.4475170033247, 593.6900215726743, 590.6794021320726, 595.8191354119631, 598.8734216158957, 601.4309857025517, 588.1068218253249, 573.4631790618847, 573.4550785214772, 570.0374349796734, 572.0702173500102, 572.3529598711284, 572.887715235265, 484.2317397904843, 478.8008286685682, 483.0205036816562, 480.63826759506986, 488.66026947443146, 490.7865243609123, 485.89718596304596, 487.5677286935885, 469.27202012032905, 484.91313509329126, 489.7981487115817, 487.876483658393, 488.44166657050397, 490.2161060255308, 472.1379134286463, 512.5032594486274, 505.3854391101437, 506.11186404767227, 508.3724486640225, 505.45286696283654, 505.260467635274, 495.6415103296606, 503.08467443175584, 499.91299469891743, 494.47805220643426, 483.1849795811207, 490.94294408969336, 479.45967319540165, 495.6682941179919, 497.0999545909062, 498.8931696279301, 495.93491225761665, 491.34481654132117, 491.4211901482675, 493.82574674598254, 485.34729051990377, 482.64642798468753, 309.24077225371246, 494.9342830048183, 499.22253721112577, 496.5813164108389, 516.5013529981804, 267.4322840734051, 497.23820709708104, 492.29896725788683, 508.8742414665168, 501.7179607658386, 505.19423368440056, 474.73824266937123, 262.098566875544, 509.0534087638562, 497.50494089321916, 516.8821012233752, 491.80822164516803, 499.7659952220446, 510.89785618653605, 500.4825124359053, 499.8178841582936, 514.8486099634738, 510.9980937762011, 502.9249806518413, 494.53456869373866, 503.99793171520685, 503.60718035960815, 502.58892976214787, 521.8639754380025, 519.7171703840677, 507.78966811490454, 504.2308600710187, 512.6398354049555, 519.8951913171879, 462.81377225830784, 525.6763252709349, 536.2434680413055, 534.8959077606186, 532.6970034717866, 533.0857794585255, 534.627983323235, 536.2905408905102, 537.9569967429555, 538.9720890749661, 535.896711787238, 536.4169077273983, 538.1906648301423, 538.5630504568846, 535.5164611742972, 536.8940445832491, 537.1578312185393, 536.3393005862172, 540.0976480404818, 531.8427323080482]
Elapsed: 0.09414437210017984~0.013433178957037013
Time per graph: 0.0019342904687639822~0.00027738597682341166
Speed: 523.9421910518877~51.09141713827848
Total Time: 0.0909
best val loss: 0.45030248165130615 test_score: 0.8958

Testing...
Test loss: 0.3663 score: 0.9167 time: 0.08s
test Score 0.9167
Epoch Time List: [0.6790760902222246, 0.35661386302672327, 0.35290941200219095, 0.3541223732754588, 0.35498873703181744, 0.3589131897315383, 0.3625092161819339, 0.36397642781957984, 0.36850034911185503, 0.3686835339758545, 0.3692579453345388, 0.3654741251375526, 0.3682142607867718, 0.37336379708722234, 0.3764678828883916, 0.3678974041249603, 0.36057250504381955, 0.3565560458227992, 0.3600418381392956, 0.3570455159060657, 0.358668162021786, 0.3635019559878856, 0.3641638557892293, 0.35952282208018005, 0.36017922731116414, 0.35776893608272076, 0.3668504108209163, 0.3605713527649641, 0.35927733732387424, 0.3600620608776808, 0.3535300891380757, 0.36210372229106724, 0.3605109441559762, 0.3610345299821347, 0.3663574359379709, 0.3676249429117888, 0.37104864581488073, 0.36558001511730254, 0.3709781488869339, 0.3669571648351848, 0.3663695661816746, 0.3644669149070978, 0.36305679799988866, 0.3680954179726541, 0.36500784289091825, 0.3709748031105846, 0.3679910618811846, 0.37095806980505586, 0.3552660408895463, 0.3524673320353031, 0.35183134605176747, 0.35209273220971227, 0.37353807175531983, 0.37402793602086604, 0.37373947887681425, 0.3738432477694005, 0.3647671320941299, 0.3654064948204905, 0.3669402450323105, 0.36659576604142785, 0.35712517402134836, 0.34723248123191297, 0.3457614949438721, 0.34471172210760415, 0.34532080474309623, 0.3617052079644054, 0.35018516378477216, 0.3221908158157021, 0.3378669440280646, 0.34957072092220187, 0.35101412516087294, 0.3505359592381865, 0.35365676530636847, 0.3600763699505478, 0.35751147917471826, 0.3559255041182041, 0.33899599593132734, 0.34370331512764096, 0.3580065609421581, 0.34073036909103394, 0.4172920058481395, 0.3374946997500956, 0.3416732628829777, 0.35323865828104317, 0.3427486589644104, 0.4260730657260865, 0.3426027800887823, 0.358418659074232, 0.34426417597569525, 0.41976615390740335, 0.34010280785150826, 0.3360336811747402, 0.33632804709486663, 0.3366897322703153, 0.42683517280966043, 0.4001464191824198, 0.3382673820015043, 0.40278731216676533, 0.3564291337970644, 0.3438419047743082, 0.36001085001043975, 0.4335932519752532, 0.3580763693898916, 0.3584791698958725, 0.362853298895061, 0.35378874582238495, 0.46529387403279543, 0.3896678409073502, 0.35052953683771193, 0.362235153792426, 0.42386186588555574, 0.34092207183130085, 0.3395635380875319, 0.35096805007196963, 0.39452313468791544, 0.35104831121861935, 0.3631095439195633, 0.33733680099248886, 0.45570827880874276, 0.3460508829448372, 0.3562996299006045, 0.3450721730478108, 0.33425421081483364, 0.3489286880940199, 0.35274436115287244, 0.3525810099672526, 0.35603679809719324, 0.3594573796726763, 0.35917682480067015, 0.35856432211585343, 0.36054332996718585, 0.3611372539307922, 0.3612694770563394, 0.3388697123154998, 0.340101974317804, 0.35544288204982877, 0.33885077806189656, 0.3399061323143542, 0.3389638450462371, 0.3390382588841021, 0.3421766310930252, 0.33821986499242485, 0.33955589891411364]
Total Epoch List: [52, 45, 46]
Total Time List: [0.08595785195939243, 0.10010621300898492, 0.09093975997529924]
========================training times:1========================
========================k_idx:0========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: SAMamba
GraphEncoder(
  (subgraph_conv): GATConv(128, 128, heads=1)
  (embedding): Linear(in_features=14887, out_features=128, bias=False)
  (encoder): GraphMambaEncoder(
    (layers): ModuleList(
      (0-1): 2 x MambaEncoderLayer(
        (self_selen): Selection(
          (khop_structure_extractor): KHopStructureExtractor(
            (structure_extractor): StructureExtractor(
              (gcn): ModuleList(
                (0): GATConv(128, 128, heads=1)
              )
              (relu): ReLU()
              (bn): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (out_proj): Linear(in_features=128, out_features=128, bias=True)
            )
            (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (out_proj): Linear(in_features=256, out_features=128, bias=True)
          )
          (out_proj): Mamba(
            (in_proj): Linear(in_features=128, out_features=512, bias=False)
            (conv1d): Conv1d(256, 256, kernel_size=(4,), stride=(1,), padding=(3,), groups=256)
            (act): SiLU()
            (x_proj): Linear(in_features=256, out_features=40, bias=False)
            (dt_proj): Linear(in_features=8, out_features=256, bias=True)
            (out_proj): Linear(in_features=256, out_features=128, bias=False)
          )
        )
        (norm1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (linear1): Linear(in_features=128, out_features=256, bias=True)
        (dropout): Dropout(p=0.6, inplace=False)
        (linear2): Linear(in_features=256, out_features=128, bias=True)
        (dropout1): Dropout(p=0.6, inplace=False)
        (dropout2): Dropout(p=0.6, inplace=False)
        (activation): ReLU()
      )
    )
  )
  (classifier): Sequential(
    (0): Linear(in_features=128, out_features=128, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=128, out_features=2, bias=True)
  )
)
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x75fef8598d00>
Training...
Epoch 1/1000, LR 0.000500
Train loss: 0.7121;  Loss pred: 0.6992; Loss self: 1.2884; time: 0.17s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6938 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6928 score: 0.5102 time: 0.09s
Epoch 2/1000, LR 0.000000
Train loss: 0.7128;  Loss pred: 0.6995; Loss self: 1.3356; time: 0.17s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6937 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6927 score: 0.5102 time: 0.09s
Epoch 3/1000, LR 0.000050
Train loss: 0.6864;  Loss pred: 0.6738; Loss self: 1.2543; time: 0.17s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6936 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6926 score: 0.5102 time: 0.09s
Epoch 4/1000, LR 0.000100
Train loss: 0.6661;  Loss pred: 0.6533; Loss self: 1.2785; time: 0.18s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6934 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6925 score: 0.5102 time: 0.09s
Epoch 5/1000, LR 0.000150
Train loss: 0.6361;  Loss pred: 0.6235; Loss self: 1.2671; time: 0.17s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6932 score: 0.4898 time: 0.09s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6923 score: 0.5102 time: 0.10s
Epoch 6/1000, LR 0.000200
Train loss: 0.6005;  Loss pred: 0.5885; Loss self: 1.2030; time: 0.18s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6927 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6919 score: 0.5102 time: 0.09s
Epoch 7/1000, LR 0.000250
Train loss: 0.5482;  Loss pred: 0.5364; Loss self: 1.1717; time: 0.17s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6922 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6915 score: 0.5102 time: 0.09s
Epoch 8/1000, LR 0.000300
Train loss: 0.5038;  Loss pred: 0.4915; Loss self: 1.2218; time: 0.17s
Val loss: 0.6916 score: 0.5918 time: 0.08s
Test loss: 0.6908 score: 0.6939 time: 0.09s
Epoch 9/1000, LR 0.000350
Train loss: 0.4549;  Loss pred: 0.4419; Loss self: 1.2977; time: 0.18s
Val loss: 0.6905 score: 0.8163 time: 0.08s
Test loss: 0.6898 score: 0.8367 time: 0.09s
Epoch 10/1000, LR 0.000400
Train loss: 0.3905;  Loss pred: 0.3770; Loss self: 1.3520; time: 0.17s
Val loss: 0.6889 score: 0.5510 time: 0.08s
Test loss: 0.6881 score: 0.5510 time: 0.09s
Epoch 11/1000, LR 0.000450
Train loss: 0.3407;  Loss pred: 0.3250; Loss self: 1.5694; time: 0.18s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6868 score: 0.5102 time: 0.08s
Test loss: 0.6858 score: 0.5102 time: 0.10s
Epoch 12/1000, LR 0.000450
Train loss: 0.3003;  Loss pred: 0.2838; Loss self: 1.6471; time: 0.18s
Val loss: 0.6837 score: 0.5306 time: 0.09s
Test loss: 0.6823 score: 0.5102 time: 0.10s
Epoch 13/1000, LR 0.000450
Train loss: 0.2549;  Loss pred: 0.2381; Loss self: 1.6795; time: 0.17s
Val loss: 0.6792 score: 0.5510 time: 0.07s
Test loss: 0.6772 score: 0.5102 time: 0.09s
Epoch 14/1000, LR 0.000450
Train loss: 0.2293;  Loss pred: 0.2115; Loss self: 1.7823; time: 0.17s
Val loss: 0.6724 score: 0.6735 time: 0.08s
Test loss: 0.6695 score: 0.5714 time: 0.09s
Epoch 15/1000, LR 0.000450
Train loss: 0.2046;  Loss pred: 0.1855; Loss self: 1.9142; time: 0.19s
Val loss: 0.6627 score: 0.7143 time: 0.08s
Test loss: 0.6586 score: 0.6735 time: 0.09s
Epoch 16/1000, LR 0.000450
Train loss: 0.1841;  Loss pred: 0.1644; Loss self: 1.9662; time: 0.16s
Val loss: 0.6492 score: 0.7347 time: 0.08s
Test loss: 0.6433 score: 0.7347 time: 0.09s
Epoch 17/1000, LR 0.000450
Train loss: 0.1677;  Loss pred: 0.1471; Loss self: 2.0524; time: 0.17s
Val loss: 0.6305 score: 0.8367 time: 0.08s
Test loss: 0.6226 score: 0.8367 time: 0.10s
Epoch 18/1000, LR 0.000450
Train loss: 0.1523;  Loss pred: 0.1314; Loss self: 2.0831; time: 0.17s
Val loss: 0.6061 score: 0.8571 time: 0.08s
Test loss: 0.5959 score: 0.8571 time: 0.09s
Epoch 19/1000, LR 0.000450
Train loss: 0.1426;  Loss pred: 0.1212; Loss self: 2.1396; time: 0.16s
Val loss: 0.5759 score: 0.8571 time: 0.07s
Test loss: 0.5629 score: 0.8571 time: 0.09s
Epoch 20/1000, LR 0.000450
Train loss: 0.1285;  Loss pred: 0.1064; Loss self: 2.2059; time: 0.16s
Val loss: 0.5402 score: 0.8571 time: 0.09s
Test loss: 0.5241 score: 0.8776 time: 0.09s
Epoch 21/1000, LR 0.000450
Train loss: 0.1194;  Loss pred: 0.0967; Loss self: 2.2687; time: 0.17s
Val loss: 0.5001 score: 0.8571 time: 0.08s
Test loss: 0.4810 score: 0.8980 time: 0.09s
Epoch 22/1000, LR 0.000450
Train loss: 0.1092;  Loss pred: 0.0861; Loss self: 2.3149; time: 0.16s
Val loss: 0.4593 score: 0.8571 time: 0.07s
Test loss: 0.4376 score: 0.8980 time: 0.08s
Epoch 23/1000, LR 0.000450
Train loss: 0.0986;  Loss pred: 0.0748; Loss self: 2.3851; time: 0.16s
Val loss: 0.4219 score: 0.8571 time: 0.07s
Test loss: 0.3972 score: 0.8980 time: 0.09s
Epoch 24/1000, LR 0.000450
Train loss: 0.0904;  Loss pred: 0.0660; Loss self: 2.4349; time: 0.16s
Val loss: 0.3921 score: 0.8571 time: 0.07s
Test loss: 0.3638 score: 0.8980 time: 0.08s
Epoch 25/1000, LR 0.000450
Train loss: 0.0821;  Loss pred: 0.0571; Loss self: 2.5032; time: 0.16s
Val loss: 0.3718 score: 0.8571 time: 0.07s
Test loss: 0.3389 score: 0.8980 time: 0.08s
Epoch 26/1000, LR 0.000450
Train loss: 0.0751;  Loss pred: 0.0498; Loss self: 2.5277; time: 0.16s
Val loss: 0.3631 score: 0.8571 time: 0.07s
Test loss: 0.3238 score: 0.8980 time: 0.08s
Epoch 27/1000, LR 0.000450
Train loss: 0.0679;  Loss pred: 0.0423; Loss self: 2.5593; time: 0.18s
Val loss: 0.3650 score: 0.8571 time: 0.07s
Test loss: 0.3177 score: 0.9184 time: 0.08s
     INFO: Early stopping counter 1 of 20
Epoch 28/1000, LR 0.000450
Train loss: 0.0617;  Loss pred: 0.0356; Loss self: 2.6120; time: 0.16s
Val loss: 0.3750 score: 0.8571 time: 0.08s
Test loss: 0.3183 score: 0.9184 time: 0.09s
     INFO: Early stopping counter 2 of 20
Epoch 29/1000, LR 0.000450
Train loss: 0.0569;  Loss pred: 0.0304; Loss self: 2.6505; time: 0.17s
Val loss: 0.3908 score: 0.8571 time: 0.07s
Test loss: 0.3243 score: 0.9184 time: 0.08s
     INFO: Early stopping counter 3 of 20
Epoch 30/1000, LR 0.000450
Train loss: 0.0535;  Loss pred: 0.0268; Loss self: 2.6704; time: 0.16s
Val loss: 0.4126 score: 0.8571 time: 0.07s
Test loss: 0.3357 score: 0.9184 time: 0.09s
     INFO: Early stopping counter 4 of 20
Epoch 31/1000, LR 0.000450
Train loss: 0.0493;  Loss pred: 0.0223; Loss self: 2.7053; time: 0.16s
Val loss: 0.4374 score: 0.8571 time: 0.08s
Test loss: 0.3496 score: 0.9184 time: 0.08s
     INFO: Early stopping counter 5 of 20
Epoch 32/1000, LR 0.000450
Train loss: 0.0460;  Loss pred: 0.0191; Loss self: 2.6974; time: 0.18s
Val loss: 0.4609 score: 0.8571 time: 0.07s
Test loss: 0.3624 score: 0.9184 time: 0.08s
     INFO: Early stopping counter 6 of 20
Epoch 33/1000, LR 0.000449
Train loss: 0.0437;  Loss pred: 0.0165; Loss self: 2.7244; time: 0.16s
Val loss: 0.4842 score: 0.8571 time: 0.07s
Test loss: 0.3747 score: 0.9184 time: 0.08s
     INFO: Early stopping counter 7 of 20
Epoch 34/1000, LR 0.000449
Train loss: 0.0415;  Loss pred: 0.0141; Loss self: 2.7397; time: 0.16s
Val loss: 0.5036 score: 0.8571 time: 0.08s
Test loss: 0.3837 score: 0.9184 time: 0.09s
     INFO: Early stopping counter 8 of 20
Epoch 35/1000, LR 0.000449
Train loss: 0.0396;  Loss pred: 0.0121; Loss self: 2.7495; time: 0.16s
Val loss: 0.5230 score: 0.8571 time: 0.07s
Test loss: 0.3922 score: 0.9184 time: 0.08s
     INFO: Early stopping counter 9 of 20
Epoch 36/1000, LR 0.000449
Train loss: 0.0380;  Loss pred: 0.0106; Loss self: 2.7448; time: 0.16s
Val loss: 0.5428 score: 0.8571 time: 0.07s
Test loss: 0.4004 score: 0.9184 time: 0.08s
     INFO: Early stopping counter 10 of 20
Epoch 37/1000, LR 0.000449
Train loss: 0.0363;  Loss pred: 0.0089; Loss self: 2.7423; time: 0.16s
Val loss: 0.5620 score: 0.8571 time: 0.07s
Test loss: 0.4076 score: 0.9184 time: 0.08s
     INFO: Early stopping counter 11 of 20
Epoch 38/1000, LR 0.000449
Train loss: 0.0348;  Loss pred: 0.0076; Loss self: 2.7255; time: 0.18s
Val loss: 0.5815 score: 0.8571 time: 0.07s
Test loss: 0.4148 score: 0.9184 time: 0.08s
     INFO: Early stopping counter 12 of 20
Epoch 39/1000, LR 0.000449
Train loss: 0.0339;  Loss pred: 0.0065; Loss self: 2.7395; time: 0.16s
Val loss: 0.5978 score: 0.8571 time: 0.08s
Test loss: 0.4202 score: 0.9184 time: 0.08s
     INFO: Early stopping counter 13 of 20
Epoch 40/1000, LR 0.000449
Train loss: 0.0335;  Loss pred: 0.0059; Loss self: 2.7571; time: 0.16s
Val loss: 0.6108 score: 0.8367 time: 0.08s
Test loss: 0.4239 score: 0.9184 time: 0.09s
     INFO: Early stopping counter 14 of 20
Epoch 41/1000, LR 0.000449
Train loss: 0.0327;  Loss pred: 0.0053; Loss self: 2.7406; time: 0.17s
Val loss: 0.6302 score: 0.8367 time: 0.08s
Test loss: 0.4319 score: 0.9184 time: 0.09s
     INFO: Early stopping counter 15 of 20
Epoch 42/1000, LR 0.000449
Train loss: 0.0319;  Loss pred: 0.0047; Loss self: 2.7266; time: 0.16s
Val loss: 0.6476 score: 0.8367 time: 0.08s
Test loss: 0.4388 score: 0.9184 time: 0.09s
     INFO: Early stopping counter 16 of 20
Epoch 43/1000, LR 0.000449
Train loss: 0.0317;  Loss pred: 0.0042; Loss self: 2.7458; time: 0.17s
Val loss: 0.6622 score: 0.8367 time: 0.08s
Test loss: 0.4444 score: 0.8980 time: 0.09s
     INFO: Early stopping counter 17 of 20
Epoch 44/1000, LR 0.000449
Train loss: 0.0309;  Loss pred: 0.0039; Loss self: 2.6956; time: 0.18s
Val loss: 0.6774 score: 0.8367 time: 0.08s
Test loss: 0.4504 score: 0.8980 time: 0.08s
     INFO: Early stopping counter 18 of 20
Epoch 45/1000, LR 0.000449
Train loss: 0.0304;  Loss pred: 0.0034; Loss self: 2.6951; time: 0.16s
Val loss: 0.6921 score: 0.8367 time: 0.07s
Test loss: 0.4564 score: 0.8980 time: 0.08s
     INFO: Early stopping counter 19 of 20
Epoch 46/1000, LR 0.000449
Train loss: 0.0302;  Loss pred: 0.0032; Loss self: 2.6970; time: 0.16s
Val loss: 0.7060 score: 0.8367 time: 0.07s
Test loss: 0.4618 score: 0.8980 time: 0.08s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 025,   Train_Loss: 0.0751,   Val_Loss: 0.3631,   Val_Precision: 1.0000,   Val_Recall: 0.7083,   Val_accuracy: 0.8293,   Val_Score: 0.8571,   Val_Loss: 0.3631,   Test_Precision: 0.9545,   Test_Recall: 0.8400,   Test_accuracy: 0.8936,   Test_Score: 0.8980,   Test_loss: 0.3238


[0.09331009304150939, 0.09862721501849592, 0.09756098897196352, 0.09314327011816204, 0.10262147197499871, 0.095860494999215, 0.09031632402911782, 0.09413886698894203, 0.09101669583469629, 0.0974313288461417, 0.10071903211064637, 0.10063112783245742, 0.09034566092304885, 0.09105788893066347, 0.0998525710310787, 0.09067973284982145, 0.10128044593147933, 0.09246005001477897, 0.09362743492238224, 0.09596310812048614, 0.0959358501713723, 0.08965525100938976, 0.09089652798138559, 0.0898814748506993, 0.0900866650044918, 0.09002315206453204, 0.09004198713228106, 0.09256992698647082, 0.09005634696222842, 0.09036521497182548, 0.08987804991193116, 0.09013971383683383, 0.08967265789397061, 0.0941642860416323, 0.0899952941108495, 0.08934768382459879, 0.09010083507746458, 0.08975957613438368, 0.08995009586215019, 0.09788366896100342, 0.09376984112896025, 0.09108965517953038, 0.09862633398734033, 0.09009701292961836, 0.08877630205824971, 0.08888404816389084]
[0.0019042876130920283, 0.0020128003064999165, 0.0019910405912645615, 0.00190088306363596, 0.0020943157545918103, 0.0019563366326370408, 0.001843190286308527, 0.0019212013671212659, 0.0018574835884631897, 0.0019883944662477896, 0.002055490451237681, 0.0020536964863766822, 0.0018437889984295685, 0.0018583242638910912, 0.0020378075720628307, 0.0018506067928534989, 0.0020669478761526396, 0.001886939796219979, 0.0019107639780078008, 0.001958430777969105, 0.001957874493293312, 0.0018296990001916277, 0.0018550311832935835, 0.0018343158132795776, 0.0018385033674386082, 0.0018372071849904498, 0.001837591574128185, 0.0018891821833973636, 0.0018378846318822127, 0.0018441880606494996, 0.0018342459165700236, 0.0018395859966700782, 0.0018300542427340941, 0.0019217201232986183, 0.0018366386553234592, 0.001823422118869363, 0.001838792552601318, 0.0018318280843751772, 0.0018357162420846978, 0.001997625897163335, 0.0019136702271216378, 0.0018589725546842935, 0.0020127823262722517, 0.0018387145495840482, 0.0018117612664948922, 0.0018139601666100171]
[525.1307592009596, 496.8202741080224, 502.24993121053046, 526.0712871454733, 477.4829190906333, 511.1594719013422, 542.5375814033628, 520.5076454314641, 538.3627646623578, 502.9183177556591, 486.5018951549329, 486.9268690059897, 542.3614094951979, 538.1192181746198, 490.72346854993805, 540.3633034644133, 483.80513680943557, 529.9586144736862, 523.3508751000315, 510.6128903044514, 510.7579691269764, 546.5379824196593, 539.0744958931166, 545.162393934825, 543.920679021216, 544.3044247648086, 544.1905666521319, 529.3295738167903, 544.1037933789571, 542.2440483904948, 545.1831681708009, 543.600571982037, 546.4318907323773, 520.3671376888677, 544.4729136575236, 548.4193646943711, 543.8351371313266, 545.9027561208576, 544.7465011609681, 500.5942310920268, 522.5560735739227, 537.9315565903169, 496.8247122141804, 543.8582080215871, 551.9490997479161, 551.2800216935468]
Elapsed: 0.09309328814624282~0.003911380711574666
Time per graph: 0.0018998630233927106~7.982409615458501e-05
Speed: 527.2509544372631~21.366604496402676
Total Time: 0.0893
best val loss: 0.3630952537059784 test_score: 0.8980

Testing...
Test loss: 0.5959 score: 0.8571 time: 0.08s
test Score 0.8571
Epoch Time List: [0.3416102509945631, 0.3432548730634153, 0.3388896370306611, 0.34577414486557245, 0.3495469738263637, 0.35560778016224504, 0.32872064714320004, 0.33117566583678126, 0.3473748411051929, 0.3364900841843337, 0.35646245279349387, 0.36454180465079844, 0.3272405252791941, 0.33620089711621404, 0.36378342704847455, 0.3308690548874438, 0.33829173096455634, 0.3378452090546489, 0.3266258833464235, 0.34059145976789296, 0.3374342655297369, 0.32155824010260403, 0.3230122539680451, 0.3225403649266809, 0.32232672185637057, 0.3234630930237472, 0.33874106290750206, 0.3302540669683367, 0.3245953400619328, 0.3232399069238454, 0.32303114235401154, 0.3348711975850165, 0.32213733927346766, 0.3352183438837528, 0.323314833920449, 0.3212665671017021, 0.322160130366683, 0.3379887549672276, 0.3233175971545279, 0.33529756614007056, 0.3353291410021484, 0.3241305991541594, 0.3365714114625007, 0.3399199820123613, 0.31800771690905094, 0.31822981406003237]
Total Epoch List: [46]
Total Time List: [0.0892837829887867]
========================k_idx:1========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: SAMamba
GraphEncoder(
  (subgraph_conv): GATConv(128, 128, heads=1)
  (embedding): Linear(in_features=14887, out_features=128, bias=False)
  (encoder): GraphMambaEncoder(
    (layers): ModuleList(
      (0-1): 2 x MambaEncoderLayer(
        (self_selen): Selection(
          (khop_structure_extractor): KHopStructureExtractor(
            (structure_extractor): StructureExtractor(
              (gcn): ModuleList(
                (0): GATConv(128, 128, heads=1)
              )
              (relu): ReLU()
              (bn): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (out_proj): Linear(in_features=128, out_features=128, bias=True)
            )
            (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (out_proj): Linear(in_features=256, out_features=128, bias=True)
          )
          (out_proj): Mamba(
            (in_proj): Linear(in_features=128, out_features=512, bias=False)
            (conv1d): Conv1d(256, 256, kernel_size=(4,), stride=(1,), padding=(3,), groups=256)
            (act): SiLU()
            (x_proj): Linear(in_features=256, out_features=40, bias=False)
            (dt_proj): Linear(in_features=8, out_features=256, bias=True)
            (out_proj): Linear(in_features=256, out_features=128, bias=False)
          )
        )
        (norm1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (linear1): Linear(in_features=128, out_features=256, bias=True)
        (dropout): Dropout(p=0.6, inplace=False)
        (linear2): Linear(in_features=256, out_features=128, bias=True)
        (dropout1): Dropout(p=0.6, inplace=False)
        (dropout2): Dropout(p=0.6, inplace=False)
        (activation): ReLU()
      )
    )
  )
  (classifier): Sequential(
    (0): Linear(in_features=128, out_features=128, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=128, out_features=2, bias=True)
  )
)
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x75fef9a36380>
Training...
Epoch 1/1000, LR 0.000500
Train loss: 0.7101;  Loss pred: 0.6962; Loss self: 1.3886; time: 0.15s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6929 score: 0.5102 time: 0.09s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6940 score: 0.4898 time: 0.07s
Epoch 2/1000, LR 0.000000
Train loss: 0.7020;  Loss pred: 0.6886; Loss self: 1.3467; time: 0.14s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6929 score: 0.5102 time: 0.09s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6939 score: 0.4898 time: 0.07s
     INFO: Early stopping counter 1 of 20
Epoch 3/1000, LR 0.000050
Train loss: 0.6860;  Loss pred: 0.6722; Loss self: 1.3758; time: 0.15s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6930 score: 0.5102 time: 0.09s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6939 score: 0.4898 time: 0.07s
     INFO: Early stopping counter 2 of 20
Epoch 4/1000, LR 0.000100
Train loss: 0.6645;  Loss pred: 0.6509; Loss self: 1.3554; time: 0.15s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6930 score: 0.5102 time: 0.09s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6938 score: 0.4898 time: 0.07s
     INFO: Early stopping counter 3 of 20
Epoch 5/1000, LR 0.000150
Train loss: 0.6305;  Loss pred: 0.6172; Loss self: 1.3292; time: 0.15s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6928 score: 0.5102 time: 0.09s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6935 score: 0.4898 time: 0.07s
Epoch 6/1000, LR 0.000200
Train loss: 0.5952;  Loss pred: 0.5826; Loss self: 1.2572; time: 0.15s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6926 score: 0.5102 time: 0.09s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6933 score: 0.4898 time: 0.07s
Epoch 7/1000, LR 0.000250
Train loss: 0.5416;  Loss pred: 0.5289; Loss self: 1.2666; time: 0.15s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6923 score: 0.5102 time: 0.09s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6928 score: 0.4898 time: 0.08s
Epoch 8/1000, LR 0.000300
Train loss: 0.4863;  Loss pred: 0.4728; Loss self: 1.3486; time: 0.15s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6917 score: 0.5102 time: 0.09s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6922 score: 0.4898 time: 0.07s
Epoch 9/1000, LR 0.000350
Train loss: 0.4167;  Loss pred: 0.4025; Loss self: 1.4146; time: 0.15s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6909 score: 0.5102 time: 0.09s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6912 score: 0.4898 time: 0.07s
Epoch 10/1000, LR 0.000400
Train loss: 0.3753;  Loss pred: 0.3598; Loss self: 1.5474; time: 0.16s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6897 score: 0.5102 time: 0.09s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6899 score: 0.4898 time: 0.08s
Epoch 11/1000, LR 0.000450
Train loss: 0.3258;  Loss pred: 0.3092; Loss self: 1.6582; time: 0.15s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6878 score: 0.5102 time: 0.09s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6880 score: 0.4898 time: 0.08s
Epoch 12/1000, LR 0.000450
Train loss: 0.2875;  Loss pred: 0.2697; Loss self: 1.7860; time: 0.16s
Val loss: 0.6851 score: 0.5510 time: 0.09s
Test loss: 0.6852 score: 0.5306 time: 0.08s
Epoch 13/1000, LR 0.000450
Train loss: 0.2535;  Loss pred: 0.2353; Loss self: 1.8208; time: 0.15s
Val loss: 0.6810 score: 0.7143 time: 0.10s
Test loss: 0.6809 score: 0.7347 time: 0.08s
Epoch 14/1000, LR 0.000450
Train loss: 0.2239;  Loss pred: 0.2040; Loss self: 1.9914; time: 0.15s
Val loss: 0.6749 score: 0.7347 time: 0.09s
Test loss: 0.6749 score: 0.7755 time: 0.08s
Epoch 15/1000, LR 0.000450
Train loss: 0.1961;  Loss pred: 0.1760; Loss self: 2.0035; time: 0.15s
Val loss: 0.6665 score: 0.8163 time: 0.09s
Test loss: 0.6667 score: 0.7959 time: 0.07s
Epoch 16/1000, LR 0.000450
Train loss: 0.1735;  Loss pred: 0.1529; Loss self: 2.0607; time: 0.17s
Val loss: 0.6550 score: 0.8571 time: 0.09s
Test loss: 0.6553 score: 0.8571 time: 0.07s
Epoch 17/1000, LR 0.000450
Train loss: 0.1498;  Loss pred: 0.1291; Loss self: 2.0759; time: 0.15s
Val loss: 0.6386 score: 0.8571 time: 0.09s
Test loss: 0.6394 score: 0.8776 time: 0.08s
Epoch 18/1000, LR 0.000450
Train loss: 0.1324;  Loss pred: 0.1113; Loss self: 2.1124; time: 0.15s
Val loss: 0.6170 score: 0.9184 time: 0.09s
Test loss: 0.6185 score: 0.8776 time: 0.07s
Epoch 19/1000, LR 0.000450
Train loss: 0.1203;  Loss pred: 0.0988; Loss self: 2.1515; time: 0.15s
Val loss: 0.5880 score: 0.9184 time: 0.09s
Test loss: 0.5906 score: 0.8776 time: 0.07s
Epoch 20/1000, LR 0.000450
Train loss: 0.1070;  Loss pred: 0.0852; Loss self: 2.1834; time: 0.15s
Val loss: 0.5508 score: 0.9184 time: 0.09s
Test loss: 0.5553 score: 0.8571 time: 0.07s
Epoch 21/1000, LR 0.000450
Train loss: 0.0970;  Loss pred: 0.0749; Loss self: 2.2101; time: 0.15s
Val loss: 0.5047 score: 0.9184 time: 0.09s
Test loss: 0.5118 score: 0.8980 time: 0.07s
Epoch 22/1000, LR 0.000450
Train loss: 0.0865;  Loss pred: 0.0637; Loss self: 2.2798; time: 0.17s
Val loss: 0.4511 score: 0.9388 time: 0.09s
Test loss: 0.4624 score: 0.8980 time: 0.08s
Epoch 23/1000, LR 0.000450
Train loss: 0.0771;  Loss pred: 0.0535; Loss self: 2.3540; time: 0.17s
Val loss: 0.3913 score: 0.9388 time: 0.10s
Test loss: 0.4090 score: 0.8776 time: 0.09s
Epoch 24/1000, LR 0.000450
Train loss: 0.0712;  Loss pred: 0.0479; Loss self: 2.3309; time: 0.17s
Val loss: 0.3316 score: 0.9184 time: 0.10s
Test loss: 0.3575 score: 0.8980 time: 0.08s
Epoch 25/1000, LR 0.000450
Train loss: 0.0624;  Loss pred: 0.0382; Loss self: 2.4149; time: 0.17s
Val loss: 0.2776 score: 0.9184 time: 0.09s
Test loss: 0.3133 score: 0.9184 time: 0.07s
Epoch 26/1000, LR 0.000450
Train loss: 0.0571;  Loss pred: 0.0327; Loss self: 2.4314; time: 0.17s
Val loss: 0.2317 score: 0.9388 time: 0.09s
Test loss: 0.2800 score: 0.9184 time: 0.07s
Epoch 27/1000, LR 0.000450
Train loss: 0.0507;  Loss pred: 0.0260; Loss self: 2.4702; time: 0.17s
Val loss: 0.1964 score: 0.9388 time: 0.09s
Test loss: 0.2586 score: 0.9184 time: 0.07s
Epoch 28/1000, LR 0.000450
Train loss: 0.0479;  Loss pred: 0.0230; Loss self: 2.4853; time: 0.17s
Val loss: 0.1699 score: 0.9388 time: 0.09s
Test loss: 0.2482 score: 0.9184 time: 0.07s
Epoch 29/1000, LR 0.000450
Train loss: 0.0440;  Loss pred: 0.0189; Loss self: 2.5102; time: 0.17s
Val loss: 0.1518 score: 0.9388 time: 0.09s
Test loss: 0.2486 score: 0.9184 time: 0.07s
Epoch 30/1000, LR 0.000450
Train loss: 0.0408;  Loss pred: 0.0153; Loss self: 2.5502; time: 0.16s
Val loss: 0.1403 score: 0.9388 time: 0.09s
Test loss: 0.2575 score: 0.9184 time: 0.07s
Epoch 31/1000, LR 0.000450
Train loss: 0.0389;  Loss pred: 0.0133; Loss self: 2.5597; time: 0.17s
Val loss: 0.1333 score: 0.9388 time: 0.09s
Test loss: 0.2724 score: 0.9184 time: 0.07s
Epoch 32/1000, LR 0.000450
Train loss: 0.0365;  Loss pred: 0.0108; Loss self: 2.5666; time: 0.17s
Val loss: 0.1296 score: 0.9388 time: 0.09s
Test loss: 0.2913 score: 0.9184 time: 0.07s
Epoch 33/1000, LR 0.000449
Train loss: 0.0356;  Loss pred: 0.0097; Loss self: 2.5913; time: 0.15s
Val loss: 0.1277 score: 0.9388 time: 0.09s
Test loss: 0.3118 score: 0.9184 time: 0.07s
Epoch 34/1000, LR 0.000449
Train loss: 0.0338;  Loss pred: 0.0078; Loss self: 2.6011; time: 0.17s
Val loss: 0.1270 score: 0.9388 time: 0.09s
Test loss: 0.3335 score: 0.9184 time: 0.08s
Epoch 35/1000, LR 0.000449
Train loss: 0.0329;  Loss pred: 0.0067; Loss self: 2.6147; time: 0.15s
Val loss: 0.1270 score: 0.9388 time: 0.09s
Test loss: 0.3550 score: 0.9184 time: 0.07s
     INFO: Early stopping counter 1 of 20
Epoch 36/1000, LR 0.000449
Train loss: 0.0317;  Loss pred: 0.0057; Loss self: 2.5941; time: 0.16s
Val loss: 0.1274 score: 0.9184 time: 0.16s
Test loss: 0.3751 score: 0.9184 time: 0.07s
     INFO: Early stopping counter 2 of 20
Epoch 37/1000, LR 0.000449
Train loss: 0.0313;  Loss pred: 0.0052; Loss self: 2.6139; time: 0.14s
Val loss: 0.1280 score: 0.9184 time: 0.09s
Test loss: 0.3931 score: 0.9184 time: 0.07s
     INFO: Early stopping counter 3 of 20
Epoch 38/1000, LR 0.000449
Train loss: 0.0309;  Loss pred: 0.0044; Loss self: 2.6503; time: 0.16s
Val loss: 0.1287 score: 0.9184 time: 0.09s
Test loss: 0.4091 score: 0.9184 time: 0.07s
     INFO: Early stopping counter 4 of 20
Epoch 39/1000, LR 0.000449
Train loss: 0.0301;  Loss pred: 0.0037; Loss self: 2.6354; time: 0.15s
Val loss: 0.1296 score: 0.9184 time: 0.09s
Test loss: 0.4251 score: 0.9184 time: 0.07s
     INFO: Early stopping counter 5 of 20
Epoch 40/1000, LR 0.000449
Train loss: 0.0294;  Loss pred: 0.0032; Loss self: 2.6186; time: 0.16s
Val loss: 0.1302 score: 0.9184 time: 0.09s
Test loss: 0.4390 score: 0.9184 time: 0.07s
     INFO: Early stopping counter 6 of 20
Epoch 41/1000, LR 0.000449
Train loss: 0.0292;  Loss pred: 0.0031; Loss self: 2.6133; time: 0.16s
Val loss: 0.1304 score: 0.9388 time: 0.09s
Test loss: 0.4516 score: 0.9184 time: 0.07s
     INFO: Early stopping counter 7 of 20
Epoch 42/1000, LR 0.000449
Train loss: 0.0287;  Loss pred: 0.0027; Loss self: 2.5993; time: 0.14s
Val loss: 0.1307 score: 0.9388 time: 0.09s
Test loss: 0.4636 score: 0.9184 time: 0.07s
     INFO: Early stopping counter 8 of 20
Epoch 43/1000, LR 0.000449
Train loss: 0.0283;  Loss pred: 0.0025; Loss self: 2.5812; time: 0.14s
Val loss: 0.1310 score: 0.9388 time: 0.09s
Test loss: 0.4738 score: 0.9184 time: 0.07s
     INFO: Early stopping counter 9 of 20
Epoch 44/1000, LR 0.000449
Train loss: 0.0281;  Loss pred: 0.0023; Loss self: 2.5829; time: 0.14s
Val loss: 0.1313 score: 0.9388 time: 0.09s
Test loss: 0.4826 score: 0.9184 time: 0.07s
     INFO: Early stopping counter 10 of 20
Epoch 45/1000, LR 0.000449
Train loss: 0.0277;  Loss pred: 0.0020; Loss self: 2.5693; time: 0.16s
Val loss: 0.1315 score: 0.9388 time: 0.09s
Test loss: 0.4897 score: 0.9184 time: 0.07s
     INFO: Early stopping counter 11 of 20
Epoch 46/1000, LR 0.000449
Train loss: 0.0275;  Loss pred: 0.0019; Loss self: 2.5652; time: 0.16s
Val loss: 0.1314 score: 0.9388 time: 0.11s
Test loss: 0.4951 score: 0.9184 time: 0.07s
     INFO: Early stopping counter 12 of 20
Epoch 47/1000, LR 0.000449
Train loss: 0.0272;  Loss pred: 0.0018; Loss self: 2.5388; time: 0.14s
Val loss: 0.1314 score: 0.9388 time: 0.09s
Test loss: 0.5000 score: 0.9184 time: 0.07s
     INFO: Early stopping counter 13 of 20
Epoch 48/1000, LR 0.000448
Train loss: 0.0270;  Loss pred: 0.0016; Loss self: 2.5383; time: 0.15s
Val loss: 0.1316 score: 0.9388 time: 0.09s
Test loss: 0.5062 score: 0.9184 time: 0.08s
     INFO: Early stopping counter 14 of 20
Epoch 49/1000, LR 0.000448
Train loss: 0.0268;  Loss pred: 0.0016; Loss self: 2.5167; time: 0.15s
Val loss: 0.1317 score: 0.9388 time: 0.09s
Test loss: 0.5116 score: 0.9184 time: 0.07s
     INFO: Early stopping counter 15 of 20
Epoch 50/1000, LR 0.000448
Train loss: 0.0268;  Loss pred: 0.0015; Loss self: 2.5312; time: 0.15s
Val loss: 0.1318 score: 0.9388 time: 0.09s
Test loss: 0.5157 score: 0.9184 time: 0.07s
     INFO: Early stopping counter 16 of 20
Epoch 51/1000, LR 0.000448
Train loss: 0.0265;  Loss pred: 0.0015; Loss self: 2.4979; time: 0.23s
Val loss: 0.1321 score: 0.9388 time: 0.09s
Test loss: 0.5218 score: 0.9184 time: 0.07s
     INFO: Early stopping counter 17 of 20
Epoch 52/1000, LR 0.000448
Train loss: 0.0264;  Loss pred: 0.0015; Loss self: 2.4894; time: 0.16s
Val loss: 0.1322 score: 0.9388 time: 0.09s
Test loss: 0.5259 score: 0.9184 time: 0.07s
     INFO: Early stopping counter 18 of 20
Epoch 53/1000, LR 0.000448
Train loss: 0.0263;  Loss pred: 0.0014; Loss self: 2.4907; time: 0.16s
Val loss: 0.1323 score: 0.9388 time: 0.09s
Test loss: 0.5297 score: 0.9184 time: 0.07s
     INFO: Early stopping counter 19 of 20
Epoch 54/1000, LR 0.000448
Train loss: 0.0260;  Loss pred: 0.0014; Loss self: 2.4639; time: 0.15s
Val loss: 0.1325 score: 0.9388 time: 0.09s
Test loss: 0.5326 score: 0.9184 time: 0.07s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 033,   Train_Loss: 0.0338,   Val_Loss: 0.1270,   Val_Precision: 0.9583,   Val_Recall: 0.9200,   Val_accuracy: 0.9388,   Val_Score: 0.9388,   Val_Loss: 0.1270,   Test_Precision: 0.9545,   Test_Recall: 0.8750,   Test_accuracy: 0.9130,   Test_Score: 0.9184,   Test_loss: 0.3335


[0.09331009304150939, 0.09862721501849592, 0.09756098897196352, 0.09314327011816204, 0.10262147197499871, 0.095860494999215, 0.09031632402911782, 0.09413886698894203, 0.09101669583469629, 0.0974313288461417, 0.10071903211064637, 0.10063112783245742, 0.09034566092304885, 0.09105788893066347, 0.0998525710310787, 0.09067973284982145, 0.10128044593147933, 0.09246005001477897, 0.09362743492238224, 0.09596310812048614, 0.0959358501713723, 0.08965525100938976, 0.09089652798138559, 0.0898814748506993, 0.0900866650044918, 0.09002315206453204, 0.09004198713228106, 0.09256992698647082, 0.09005634696222842, 0.09036521497182548, 0.08987804991193116, 0.09013971383683383, 0.08967265789397061, 0.0941642860416323, 0.0899952941108495, 0.08934768382459879, 0.09010083507746458, 0.08975957613438368, 0.08995009586215019, 0.09788366896100342, 0.09376984112896025, 0.09108965517953038, 0.09862633398734033, 0.09009701292961836, 0.08877630205824971, 0.08888404816389084, 0.07995320786722004, 0.07920095697045326, 0.07934670196846128, 0.0798745530191809, 0.07981171505525708, 0.0796234430745244, 0.08038072707131505, 0.07941302587278187, 0.07999659981578588, 0.08133607218042016, 0.08534247311763465, 0.08028345997445285, 0.0815685901325196, 0.08138106390833855, 0.07985407207161188, 0.07972400495782495, 0.08342986484058201, 0.07980960793793201, 0.08006018493324518, 0.07992323790676892, 0.08016688097268343, 0.08970647305250168, 0.09209678904153407, 0.08161571016535163, 0.07916564284823835, 0.07979724486358464, 0.07962733204476535, 0.07972118188627064, 0.07950920285657048, 0.07964784605428576, 0.0794202119577676, 0.07936860388144851, 0.08011230593547225, 0.08070989605039358, 0.07995796389877796, 0.07825061515904963, 0.07783647300675511, 0.07920044916681945, 0.07983703212812543, 0.0791238599922508, 0.0786352118011564, 0.0783267798833549, 0.07868896797299385, 0.07973250700160861, 0.0796696818433702, 0.07830147794447839, 0.0782101540826261, 0.08022556989453733, 0.07947500399313867, 0.07913817488588393, 0.07841270114295185, 0.07827336783520877, 0.07971419882960618, 0.08007606002502143]
[0.0019042876130920283, 0.0020128003064999165, 0.0019910405912645615, 0.00190088306363596, 0.0020943157545918103, 0.0019563366326370408, 0.001843190286308527, 0.0019212013671212659, 0.0018574835884631897, 0.0019883944662477896, 0.002055490451237681, 0.0020536964863766822, 0.0018437889984295685, 0.0018583242638910912, 0.0020378075720628307, 0.0018506067928534989, 0.0020669478761526396, 0.001886939796219979, 0.0019107639780078008, 0.001958430777969105, 0.001957874493293312, 0.0018296990001916277, 0.0018550311832935835, 0.0018343158132795776, 0.0018385033674386082, 0.0018372071849904498, 0.001837591574128185, 0.0018891821833973636, 0.0018378846318822127, 0.0018441880606494996, 0.0018342459165700236, 0.0018395859966700782, 0.0018300542427340941, 0.0019217201232986183, 0.0018366386553234592, 0.001823422118869363, 0.001838792552601318, 0.0018318280843751772, 0.0018357162420846978, 0.001997625897163335, 0.0019136702271216378, 0.0018589725546842935, 0.0020127823262722517, 0.0018387145495840482, 0.0018117612664948922, 0.0018139601666100171, 0.0016316981197391845, 0.001616346060621495, 0.0016193204483359444, 0.0016300929187587937, 0.0016288105113317771, 0.0016249682260107022, 0.0016404230014554092, 0.0016206739974037117, 0.001632583669709916, 0.0016599198404167379, 0.0017416831248496867, 0.0016384379586623032, 0.001664665104745298, 0.0016608380389456846, 0.0016296749402369773, 0.0016270205093433662, 0.0017026503028690207, 0.001628767508937388, 0.001633881325168269, 0.0016310864878932433, 0.0016360587953608862, 0.0018307443480102383, 0.0018795263069700829, 0.0016656267380684006, 0.0016156253642497622, 0.0016285152012976457, 0.0016250475927503134, 0.0016269628956381763, 0.0016226367929912343, 0.001625466246005832, 0.0016208206521993388, 0.0016197674261520104, 0.0016349450190912705, 0.001647140735722318, 0.0016317951816077136, 0.001596951329776523, 0.0015884994491174513, 0.0016163356972820296, 0.0016293271862882742, 0.0016147726529030775, 0.0016048002408399265, 0.0015985057119052022, 0.001605897305571303, 0.001627194020440992, 0.0016259118743544938, 0.0015979893458056816, 0.0015961255935229817, 0.0016372565284599454, 0.00162193885700283, 0.0016150647935894678, 0.0016002592069990172, 0.0015974156701063014, 0.0016268203842776771, 0.0016342053066330906]
[525.1307592009596, 496.8202741080224, 502.24993121053046, 526.0712871454733, 477.4829190906333, 511.1594719013422, 542.5375814033628, 520.5076454314641, 538.3627646623578, 502.9183177556591, 486.5018951549329, 486.9268690059897, 542.3614094951979, 538.1192181746198, 490.72346854993805, 540.3633034644133, 483.80513680943557, 529.9586144736862, 523.3508751000315, 510.6128903044514, 510.7579691269764, 546.5379824196593, 539.0744958931166, 545.162393934825, 543.920679021216, 544.3044247648086, 544.1905666521319, 529.3295738167903, 544.1037933789571, 542.2440483904948, 545.1831681708009, 543.600571982037, 546.4318907323773, 520.3671376888677, 544.4729136575236, 548.4193646943711, 543.8351371313266, 545.9027561208576, 544.7465011609681, 500.5942310920268, 522.5560735739227, 537.9315565903169, 496.8247122141804, 543.8582080215871, 551.9490997479161, 551.2800216935468, 612.8584619315752, 618.6793932083416, 617.54299529017, 613.4619618870762, 613.9449574047518, 615.3966483732427, 609.5988651175851, 617.0272378047532, 612.5260337668843, 602.4387296611509, 574.157253826699, 610.337422123964, 600.7214286822003, 602.1056698790505, 613.6193024202644, 614.6204022981742, 587.3196617737464, 613.9611666568684, 612.0395555026083, 613.088274240827, 611.2249772658184, 546.2259113823617, 532.0489509998209, 600.3746080347408, 618.9553730263226, 614.0562883313416, 615.3665926224038, 614.6421671206889, 616.2808610770865, 615.2080994959105, 616.9714080599052, 617.3725831588324, 611.6413630568547, 607.1126639712858, 612.8220080995445, 626.1931602761744, 629.5249271604006, 618.6833599490274, 613.7502696914257, 619.2822241584137, 623.13051465933, 625.584251937477, 622.704824605361, 614.5548640407284, 615.0394838570274, 625.7864000312314, 626.5171137271169, 610.7778363483645, 616.5460526963965, 619.1702054117027, 624.8987636667378, 626.011137059557, 614.6960104904323, 611.9182185623135]
Elapsed: 0.08614356389502063~0.00716802459400679
Time per graph: 0.0017580319162249108~0.00014628621620422022
Speed: 572.6406279999617~46.03250428585022
Total Time: 0.0806
best val loss: 0.12701833248138428 test_score: 0.9184

Testing...
Test loss: 0.4624 score: 0.8980 time: 0.07s
test Score 0.8980
Epoch Time List: [0.3416102509945631, 0.3432548730634153, 0.3388896370306611, 0.34577414486557245, 0.3495469738263637, 0.35560778016224504, 0.32872064714320004, 0.33117566583678126, 0.3473748411051929, 0.3364900841843337, 0.35646245279349387, 0.36454180465079844, 0.3272405252791941, 0.33620089711621404, 0.36378342704847455, 0.3308690548874438, 0.33829173096455634, 0.3378452090546489, 0.3266258833464235, 0.34059145976789296, 0.3374342655297369, 0.32155824010260403, 0.3230122539680451, 0.3225403649266809, 0.32232672185637057, 0.3234630930237472, 0.33874106290750206, 0.3302540669683367, 0.3245953400619328, 0.3232399069238454, 0.32303114235401154, 0.3348711975850165, 0.32213733927346766, 0.3352183438837528, 0.323314833920449, 0.3212665671017021, 0.322160130366683, 0.3379887549672276, 0.3233175971545279, 0.33529756614007056, 0.3353291410021484, 0.3241305991541594, 0.3365714114625007, 0.3399199820123613, 0.31800771690905094, 0.31822981406003237, 0.31129462481476367, 0.3062651960644871, 0.3081738860346377, 0.30921016656793654, 0.31134525313973427, 0.30925795901566744, 0.310279238037765, 0.3093684306368232, 0.3102138808462769, 0.3272619880735874, 0.3199255531653762, 0.32173537206836045, 0.3279379808809608, 0.3217829200439155, 0.31109377602115273, 0.3357339450158179, 0.31596810510382056, 0.3112099068239331, 0.3124296721071005, 0.3115128029603511, 0.312243351014331, 0.34010355873033404, 0.3571797872427851, 0.3427096358500421, 0.32908107503317297, 0.329799224389717, 0.3303553347941488, 0.33298977301456034, 0.33005024306476116, 0.32044231588952243, 0.33143108198419213, 0.33074965490959585, 0.3182099617552012, 0.3325602659024298, 0.31564647099003196, 0.3961637280881405, 0.30385703477077186, 0.32146680378355086, 0.30864279507659376, 0.3210159409791231, 0.32458637608215213, 0.30377416010014713, 0.3043870385736227, 0.30771130067296326, 0.32606830983422697, 0.3428684719838202, 0.3037986101116985, 0.31628913222812116, 0.31008351198397577, 0.30856508295983076, 0.3929416169412434, 0.3212479481007904, 0.32442119414918125, 0.31065345788374543]
Total Epoch List: [46, 54]
Total Time List: [0.0892837829887867, 0.08063778281211853]
========================k_idx:2========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: SAMamba
GraphEncoder(
  (subgraph_conv): GATConv(128, 128, heads=1)
  (embedding): Linear(in_features=14887, out_features=128, bias=False)
  (encoder): GraphMambaEncoder(
    (layers): ModuleList(
      (0-1): 2 x MambaEncoderLayer(
        (self_selen): Selection(
          (khop_structure_extractor): KHopStructureExtractor(
            (structure_extractor): StructureExtractor(
              (gcn): ModuleList(
                (0): GATConv(128, 128, heads=1)
              )
              (relu): ReLU()
              (bn): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (out_proj): Linear(in_features=128, out_features=128, bias=True)
            )
            (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (out_proj): Linear(in_features=256, out_features=128, bias=True)
          )
          (out_proj): Mamba(
            (in_proj): Linear(in_features=128, out_features=512, bias=False)
            (conv1d): Conv1d(256, 256, kernel_size=(4,), stride=(1,), padding=(3,), groups=256)
            (act): SiLU()
            (x_proj): Linear(in_features=256, out_features=40, bias=False)
            (dt_proj): Linear(in_features=8, out_features=256, bias=True)
            (out_proj): Linear(in_features=256, out_features=128, bias=False)
          )
        )
        (norm1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (linear1): Linear(in_features=128, out_features=256, bias=True)
        (dropout): Dropout(p=0.6, inplace=False)
        (linear2): Linear(in_features=256, out_features=128, bias=True)
        (dropout1): Dropout(p=0.6, inplace=False)
        (dropout2): Dropout(p=0.6, inplace=False)
        (activation): ReLU()
      )
    )
  )
  (classifier): Sequential(
    (0): Linear(in_features=128, out_features=128, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=128, out_features=2, bias=True)
  )
)
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x75fef9a36b00>
Training...
Epoch 1/1000, LR 0.000500
Train loss: 0.7210;  Loss pred: 0.7082; Loss self: 1.2808; time: 0.16s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6998 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6977 score: 0.5000 time: 0.08s
Epoch 2/1000, LR 0.000000
Train loss: 0.7319;  Loss pred: 0.7185; Loss self: 1.3358; time: 0.16s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6990 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6971 score: 0.5000 time: 0.08s
Epoch 3/1000, LR 0.000050
Train loss: 0.6987;  Loss pred: 0.6856; Loss self: 1.3155; time: 0.17s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6983 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6965 score: 0.5000 time: 0.08s
Epoch 4/1000, LR 0.000100
Train loss: 0.6793;  Loss pred: 0.6665; Loss self: 1.2761; time: 0.25s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6976 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6960 score: 0.5000 time: 0.08s
Epoch 5/1000, LR 0.000150
Train loss: 0.6432;  Loss pred: 0.6296; Loss self: 1.3605; time: 0.16s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6972 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6957 score: 0.5000 time: 0.08s
Epoch 6/1000, LR 0.000200
Train loss: 0.5955;  Loss pred: 0.5820; Loss self: 1.3533; time: 0.16s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6968 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6954 score: 0.5000 time: 0.08s
Epoch 7/1000, LR 0.000250
Train loss: 0.5412;  Loss pred: 0.5288; Loss self: 1.2437; time: 0.16s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6964 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6951 score: 0.5000 time: 0.08s
Epoch 8/1000, LR 0.000300
Train loss: 0.4948;  Loss pred: 0.4812; Loss self: 1.3696; time: 0.16s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6957 score: 0.4898 time: 0.16s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6946 score: 0.5000 time: 0.08s
Epoch 9/1000, LR 0.000350
Train loss: 0.4449;  Loss pred: 0.4311; Loss self: 1.3830; time: 0.17s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6944 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6936 score: 0.5000 time: 0.08s
Epoch 10/1000, LR 0.000400
Train loss: 0.4043;  Loss pred: 0.3892; Loss self: 1.5108; time: 0.17s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6926 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6922 score: 0.5000 time: 0.08s
Epoch 11/1000, LR 0.000450
Train loss: 0.3682;  Loss pred: 0.3521; Loss self: 1.6137; time: 0.18s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6902 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6901 score: 0.5000 time: 0.08s
Epoch 12/1000, LR 0.000450
Train loss: 0.3228;  Loss pred: 0.3058; Loss self: 1.7041; time: 0.18s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6867 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6873 score: 0.5000 time: 0.16s
Epoch 13/1000, LR 0.000450
Train loss: 0.3014;  Loss pred: 0.2831; Loss self: 1.8320; time: 0.16s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6821 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6834 score: 0.5000 time: 0.08s
Epoch 14/1000, LR 0.000450
Train loss: 0.2573;  Loss pred: 0.2387; Loss self: 1.8558; time: 0.17s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6761 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6782 score: 0.5000 time: 0.08s
Epoch 15/1000, LR 0.000450
Train loss: 0.2345;  Loss pred: 0.2146; Loss self: 1.9959; time: 0.16s
Val loss: 0.6679 score: 0.5102 time: 0.08s
Test loss: 0.6709 score: 0.5625 time: 0.08s
Epoch 16/1000, LR 0.000450
Train loss: 0.2104;  Loss pred: 0.1900; Loss self: 2.0410; time: 0.16s
Val loss: 0.6572 score: 0.8367 time: 0.08s
Test loss: 0.6614 score: 0.8750 time: 0.08s
Epoch 17/1000, LR 0.000450
Train loss: 0.1859;  Loss pred: 0.1655; Loss self: 2.0438; time: 0.19s
Val loss: 0.6429 score: 0.9592 time: 0.08s
Test loss: 0.6485 score: 0.9167 time: 0.08s
Epoch 18/1000, LR 0.000450
Train loss: 0.1657;  Loss pred: 0.1444; Loss self: 2.1324; time: 0.16s
Val loss: 0.6232 score: 0.9592 time: 0.08s
Test loss: 0.6305 score: 0.9375 time: 0.08s
Epoch 19/1000, LR 0.000450
Train loss: 0.1465;  Loss pred: 0.1252; Loss self: 2.1377; time: 0.16s
Val loss: 0.5965 score: 0.9388 time: 0.08s
Test loss: 0.6056 score: 0.9167 time: 0.08s
Epoch 20/1000, LR 0.000450
Train loss: 0.1319;  Loss pred: 0.1097; Loss self: 2.2187; time: 0.17s
Val loss: 0.5620 score: 0.9388 time: 0.08s
Test loss: 0.5729 score: 0.9167 time: 0.08s
Epoch 21/1000, LR 0.000450
Train loss: 0.1182;  Loss pred: 0.0960; Loss self: 2.2249; time: 0.17s
Val loss: 0.5182 score: 0.9388 time: 0.08s
Test loss: 0.5308 score: 0.9167 time: 0.17s
Epoch 22/1000, LR 0.000450
Train loss: 0.1051;  Loss pred: 0.0821; Loss self: 2.2951; time: 0.16s
Val loss: 0.4673 score: 0.9388 time: 0.08s
Test loss: 0.4808 score: 0.9167 time: 0.08s
Epoch 23/1000, LR 0.000450
Train loss: 0.0951;  Loss pred: 0.0718; Loss self: 2.3277; time: 0.16s
Val loss: 0.4118 score: 0.9388 time: 0.08s
Test loss: 0.4260 score: 0.9375 time: 0.08s
Epoch 24/1000, LR 0.000450
Train loss: 0.0859;  Loss pred: 0.0619; Loss self: 2.3964; time: 0.17s
Val loss: 0.3566 score: 0.9388 time: 0.08s
Test loss: 0.3696 score: 0.9375 time: 0.08s
Epoch 25/1000, LR 0.000450
Train loss: 0.0765;  Loss pred: 0.0517; Loss self: 2.4741; time: 0.16s
Val loss: 0.3066 score: 0.9184 time: 0.08s
Test loss: 0.3160 score: 0.9375 time: 0.08s
Epoch 26/1000, LR 0.000450
Train loss: 0.0687;  Loss pred: 0.0434; Loss self: 2.5273; time: 0.16s
Val loss: 0.2664 score: 0.9184 time: 0.08s
Test loss: 0.2689 score: 0.9375 time: 0.08s
Epoch 27/1000, LR 0.000450
Train loss: 0.0621;  Loss pred: 0.0363; Loss self: 2.5821; time: 0.16s
Val loss: 0.2379 score: 0.9184 time: 0.08s
Test loss: 0.2321 score: 0.9375 time: 0.08s
Epoch 28/1000, LR 0.000450
Train loss: 0.0558;  Loss pred: 0.0299; Loss self: 2.5829; time: 0.16s
Val loss: 0.2210 score: 0.9184 time: 0.08s
Test loss: 0.2034 score: 0.9167 time: 0.08s
Epoch 29/1000, LR 0.000450
Train loss: 0.0519;  Loss pred: 0.0253; Loss self: 2.6644; time: 0.16s
Val loss: 0.2130 score: 0.9184 time: 0.08s
Test loss: 0.1833 score: 0.9167 time: 0.08s
Epoch 30/1000, LR 0.000450
Train loss: 0.0486;  Loss pred: 0.0216; Loss self: 2.6970; time: 0.16s
Val loss: 0.2117 score: 0.9184 time: 0.08s
Test loss: 0.1691 score: 0.9375 time: 0.08s
Epoch 31/1000, LR 0.000450
Train loss: 0.0454;  Loss pred: 0.0180; Loss self: 2.7347; time: 0.18s
Val loss: 0.2142 score: 0.9184 time: 0.08s
Test loss: 0.1597 score: 0.9583 time: 0.08s
     INFO: Early stopping counter 1 of 20
Epoch 32/1000, LR 0.000450
Train loss: 0.0428;  Loss pred: 0.0155; Loss self: 2.7314; time: 0.16s
Val loss: 0.2201 score: 0.9184 time: 0.08s
Test loss: 0.1532 score: 0.9583 time: 0.08s
     INFO: Early stopping counter 2 of 20
Epoch 33/1000, LR 0.000449
Train loss: 0.0404;  Loss pred: 0.0127; Loss self: 2.7674; time: 0.16s
Val loss: 0.2273 score: 0.9184 time: 0.09s
Test loss: 0.1496 score: 0.9583 time: 0.08s
     INFO: Early stopping counter 3 of 20
Epoch 34/1000, LR 0.000449
Train loss: 0.0390;  Loss pred: 0.0112; Loss self: 2.7774; time: 0.16s
Val loss: 0.2350 score: 0.9184 time: 0.08s
Test loss: 0.1484 score: 0.9583 time: 0.08s
     INFO: Early stopping counter 4 of 20
Epoch 35/1000, LR 0.000449
Train loss: 0.0370;  Loss pred: 0.0094; Loss self: 2.7606; time: 0.16s
Val loss: 0.2428 score: 0.9184 time: 0.08s
Test loss: 0.1492 score: 0.9583 time: 0.08s
     INFO: Early stopping counter 5 of 20
Epoch 36/1000, LR 0.000449
Train loss: 0.0360;  Loss pred: 0.0080; Loss self: 2.7939; time: 0.16s
Val loss: 0.2505 score: 0.9184 time: 0.08s
Test loss: 0.1516 score: 0.9375 time: 0.08s
     INFO: Early stopping counter 6 of 20
Epoch 37/1000, LR 0.000449
Train loss: 0.0347;  Loss pred: 0.0069; Loss self: 2.7787; time: 0.16s
Val loss: 0.2588 score: 0.9184 time: 0.08s
Test loss: 0.1550 score: 0.9167 time: 0.08s
     INFO: Early stopping counter 7 of 20
Epoch 38/1000, LR 0.000449
Train loss: 0.0337;  Loss pred: 0.0060; Loss self: 2.7701; time: 0.16s
Val loss: 0.2660 score: 0.9184 time: 0.08s
Test loss: 0.1588 score: 0.9167 time: 0.08s
     INFO: Early stopping counter 8 of 20
Epoch 39/1000, LR 0.000449
Train loss: 0.0329;  Loss pred: 0.0053; Loss self: 2.7633; time: 0.16s
Val loss: 0.2723 score: 0.9184 time: 0.08s
Test loss: 0.1627 score: 0.9167 time: 0.08s
     INFO: Early stopping counter 9 of 20
Epoch 40/1000, LR 0.000449
Train loss: 0.0325;  Loss pred: 0.0049; Loss self: 2.7601; time: 0.16s
Val loss: 0.2775 score: 0.9184 time: 0.08s
Test loss: 0.1664 score: 0.9167 time: 0.08s
     INFO: Early stopping counter 10 of 20
Epoch 41/1000, LR 0.000449
Train loss: 0.0316;  Loss pred: 0.0043; Loss self: 2.7357; time: 0.16s
Val loss: 0.2823 score: 0.9184 time: 0.08s
Test loss: 0.1698 score: 0.9167 time: 0.08s
     INFO: Early stopping counter 11 of 20
Epoch 42/1000, LR 0.000449
Train loss: 0.0313;  Loss pred: 0.0039; Loss self: 2.7433; time: 0.16s
Val loss: 0.2867 score: 0.9184 time: 0.08s
Test loss: 0.1730 score: 0.9167 time: 0.08s
     INFO: Early stopping counter 12 of 20
Epoch 43/1000, LR 0.000449
Train loss: 0.0308;  Loss pred: 0.0035; Loss self: 2.7370; time: 0.16s
Val loss: 0.2903 score: 0.9184 time: 0.08s
Test loss: 0.1760 score: 0.9167 time: 0.08s
     INFO: Early stopping counter 13 of 20
Epoch 44/1000, LR 0.000449
Train loss: 0.0305;  Loss pred: 0.0033; Loss self: 2.7221; time: 0.16s
Val loss: 0.2939 score: 0.9184 time: 0.08s
Test loss: 0.1788 score: 0.9167 time: 0.08s
     INFO: Early stopping counter 14 of 20
Epoch 45/1000, LR 0.000449
Train loss: 0.0300;  Loss pred: 0.0030; Loss self: 2.7069; time: 0.16s
Val loss: 0.2970 score: 0.9184 time: 0.08s
Test loss: 0.1813 score: 0.9167 time: 0.08s
     INFO: Early stopping counter 15 of 20
Epoch 46/1000, LR 0.000449
Train loss: 0.0300;  Loss pred: 0.0028; Loss self: 2.7227; time: 0.16s
Val loss: 0.3004 score: 0.9184 time: 0.08s
Test loss: 0.1839 score: 0.9167 time: 0.08s
     INFO: Early stopping counter 16 of 20
Epoch 47/1000, LR 0.000449
Train loss: 0.0294;  Loss pred: 0.0025; Loss self: 2.6901; time: 0.17s
Val loss: 0.3023 score: 0.9388 time: 0.08s
Test loss: 0.1857 score: 0.9167 time: 0.08s
     INFO: Early stopping counter 17 of 20
Epoch 48/1000, LR 0.000448
Train loss: 0.0292;  Loss pred: 0.0024; Loss self: 2.6812; time: 0.18s
Val loss: 0.3034 score: 0.9388 time: 0.08s
Test loss: 0.1873 score: 0.9167 time: 0.08s
     INFO: Early stopping counter 18 of 20
Epoch 49/1000, LR 0.000448
Train loss: 0.0291;  Loss pred: 0.0023; Loss self: 2.6794; time: 0.18s
Val loss: 0.3056 score: 0.9388 time: 0.08s
Test loss: 0.1889 score: 0.9167 time: 0.08s
     INFO: Early stopping counter 19 of 20
Epoch 50/1000, LR 0.000448
Train loss: 0.0291;  Loss pred: 0.0023; Loss self: 2.6795; time: 0.18s
Val loss: 0.3074 score: 0.9388 time: 0.08s
Test loss: 0.1904 score: 0.9167 time: 0.08s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 029,   Train_Loss: 0.0486,   Val_Loss: 0.2117,   Val_Precision: 0.9565,   Val_Recall: 0.8800,   Val_accuracy: 0.9167,   Val_Score: 0.9184,   Val_Loss: 0.2117,   Test_Precision: 0.9200,   Test_Recall: 0.9583,   Test_accuracy: 0.9388,   Test_Score: 0.9375,   Test_loss: 0.1691


[0.09331009304150939, 0.09862721501849592, 0.09756098897196352, 0.09314327011816204, 0.10262147197499871, 0.095860494999215, 0.09031632402911782, 0.09413886698894203, 0.09101669583469629, 0.0974313288461417, 0.10071903211064637, 0.10063112783245742, 0.09034566092304885, 0.09105788893066347, 0.0998525710310787, 0.09067973284982145, 0.10128044593147933, 0.09246005001477897, 0.09362743492238224, 0.09596310812048614, 0.0959358501713723, 0.08965525100938976, 0.09089652798138559, 0.0898814748506993, 0.0900866650044918, 0.09002315206453204, 0.09004198713228106, 0.09256992698647082, 0.09005634696222842, 0.09036521497182548, 0.08987804991193116, 0.09013971383683383, 0.08967265789397061, 0.0941642860416323, 0.0899952941108495, 0.08934768382459879, 0.09010083507746458, 0.08975957613438368, 0.08995009586215019, 0.09788366896100342, 0.09376984112896025, 0.09108965517953038, 0.09862633398734033, 0.09009701292961836, 0.08877630205824971, 0.08888404816389084, 0.07995320786722004, 0.07920095697045326, 0.07934670196846128, 0.0798745530191809, 0.07981171505525708, 0.0796234430745244, 0.08038072707131505, 0.07941302587278187, 0.07999659981578588, 0.08133607218042016, 0.08534247311763465, 0.08028345997445285, 0.0815685901325196, 0.08138106390833855, 0.07985407207161188, 0.07972400495782495, 0.08342986484058201, 0.07980960793793201, 0.08006018493324518, 0.07992323790676892, 0.08016688097268343, 0.08970647305250168, 0.09209678904153407, 0.08161571016535163, 0.07916564284823835, 0.07979724486358464, 0.07962733204476535, 0.07972118188627064, 0.07950920285657048, 0.07964784605428576, 0.0794202119577676, 0.07936860388144851, 0.08011230593547225, 0.08070989605039358, 0.07995796389877796, 0.07825061515904963, 0.07783647300675511, 0.07920044916681945, 0.07983703212812543, 0.0791238599922508, 0.0786352118011564, 0.0783267798833549, 0.07868896797299385, 0.07973250700160861, 0.0796696818433702, 0.07830147794447839, 0.0782101540826261, 0.08022556989453733, 0.07947500399313867, 0.07913817488588393, 0.07841270114295185, 0.07827336783520877, 0.07971419882960618, 0.08007606002502143, 0.08418135391548276, 0.08454984799027443, 0.08544453582726419, 0.08527377201244235, 0.08523649000562727, 0.08721575303934515, 0.08546680398285389, 0.08308733883313835, 0.08299853000789881, 0.08324841014109552, 0.08401227300055325, 0.16710465587675571, 0.08299574488773942, 0.08365576690994203, 0.08419955312274396, 0.08505349699407816, 0.08297972893342376, 0.08355213701725006, 0.08388570183888078, 0.08370633702725172, 0.1771190268918872, 0.08624603389762342, 0.08670801809057593, 0.08778773690573871, 0.08738326793536544, 0.08735444094054401, 0.08705888199619949, 0.08720535808242857, 0.08709756587632, 0.08708947105333209, 0.0871217839885503, 0.08671228378079832, 0.08759546699002385, 0.0877595329657197, 0.08528069499880075, 0.08558324491605163, 0.0852981440257281, 0.08365995110943913, 0.08420134009793401, 0.08421562518924475, 0.08354252018034458, 0.08395043294876814, 0.08419900108128786, 0.08383881789632142, 0.08396948291920125, 0.08373883087188005, 0.08397735003381968, 0.0835487621370703, 0.08404262596741319, 0.08338084607385099]
[0.0019042876130920283, 0.0020128003064999165, 0.0019910405912645615, 0.00190088306363596, 0.0020943157545918103, 0.0019563366326370408, 0.001843190286308527, 0.0019212013671212659, 0.0018574835884631897, 0.0019883944662477896, 0.002055490451237681, 0.0020536964863766822, 0.0018437889984295685, 0.0018583242638910912, 0.0020378075720628307, 0.0018506067928534989, 0.0020669478761526396, 0.001886939796219979, 0.0019107639780078008, 0.001958430777969105, 0.001957874493293312, 0.0018296990001916277, 0.0018550311832935835, 0.0018343158132795776, 0.0018385033674386082, 0.0018372071849904498, 0.001837591574128185, 0.0018891821833973636, 0.0018378846318822127, 0.0018441880606494996, 0.0018342459165700236, 0.0018395859966700782, 0.0018300542427340941, 0.0019217201232986183, 0.0018366386553234592, 0.001823422118869363, 0.001838792552601318, 0.0018318280843751772, 0.0018357162420846978, 0.001997625897163335, 0.0019136702271216378, 0.0018589725546842935, 0.0020127823262722517, 0.0018387145495840482, 0.0018117612664948922, 0.0018139601666100171, 0.0016316981197391845, 0.001616346060621495, 0.0016193204483359444, 0.0016300929187587937, 0.0016288105113317771, 0.0016249682260107022, 0.0016404230014554092, 0.0016206739974037117, 0.001632583669709916, 0.0016599198404167379, 0.0017416831248496867, 0.0016384379586623032, 0.001664665104745298, 0.0016608380389456846, 0.0016296749402369773, 0.0016270205093433662, 0.0017026503028690207, 0.001628767508937388, 0.001633881325168269, 0.0016310864878932433, 0.0016360587953608862, 0.0018307443480102383, 0.0018795263069700829, 0.0016656267380684006, 0.0016156253642497622, 0.0016285152012976457, 0.0016250475927503134, 0.0016269628956381763, 0.0016226367929912343, 0.001625466246005832, 0.0016208206521993388, 0.0016197674261520104, 0.0016349450190912705, 0.001647140735722318, 0.0016317951816077136, 0.001596951329776523, 0.0015884994491174513, 0.0016163356972820296, 0.0016293271862882742, 0.0016147726529030775, 0.0016048002408399265, 0.0015985057119052022, 0.001605897305571303, 0.001627194020440992, 0.0016259118743544938, 0.0015979893458056816, 0.0015961255935229817, 0.0016372565284599454, 0.00162193885700283, 0.0016150647935894678, 0.0016002592069990172, 0.0015974156701063014, 0.0016268203842776771, 0.0016342053066330906, 0.0017537782065725576, 0.0017614551664640505, 0.0017800944964013372, 0.0017765369169258822, 0.0017757602084505681, 0.0018169948549863573, 0.001780558416309456, 0.0017309862256903823, 0.0017291360418312252, 0.00173434187793949, 0.001750255687511526, 0.0034813469974324107, 0.0017290780184945713, 0.001742828477290459, 0.0017541573567238327, 0.001771947854043295, 0.0017287443527796615, 0.0017406695211927097, 0.0017476187883100163, 0.0017438820214010775, 0.0036899797269143164, 0.0017967923728671547, 0.0018064170435536653, 0.0018289111855362232, 0.0018204847486534466, 0.0018198841862613335, 0.001813726708254156, 0.0018167782933839287, 0.0018145326224233334, 0.0018143639802777518, 0.0018150371664281313, 0.001806505912099965, 0.0018249055622921635, 0.0018283236034524937, 0.0017766811458083491, 0.0017829842690844089, 0.001777044667202669, 0.0017429156481133152, 0.0017541945853736252, 0.0017544921914425988, 0.0017404691704238455, 0.0017489673530993362, 0.0017541458558601637, 0.0017466420395066962, 0.0017493642274833594, 0.0017445589764975011, 0.0017495281257045765, 0.0017405992111889645, 0.0017508880409877747, 0.0017371009598718956]
[525.1307592009596, 496.8202741080224, 502.24993121053046, 526.0712871454733, 477.4829190906333, 511.1594719013422, 542.5375814033628, 520.5076454314641, 538.3627646623578, 502.9183177556591, 486.5018951549329, 486.9268690059897, 542.3614094951979, 538.1192181746198, 490.72346854993805, 540.3633034644133, 483.80513680943557, 529.9586144736862, 523.3508751000315, 510.6128903044514, 510.7579691269764, 546.5379824196593, 539.0744958931166, 545.162393934825, 543.920679021216, 544.3044247648086, 544.1905666521319, 529.3295738167903, 544.1037933789571, 542.2440483904948, 545.1831681708009, 543.600571982037, 546.4318907323773, 520.3671376888677, 544.4729136575236, 548.4193646943711, 543.8351371313266, 545.9027561208576, 544.7465011609681, 500.5942310920268, 522.5560735739227, 537.9315565903169, 496.8247122141804, 543.8582080215871, 551.9490997479161, 551.2800216935468, 612.8584619315752, 618.6793932083416, 617.54299529017, 613.4619618870762, 613.9449574047518, 615.3966483732427, 609.5988651175851, 617.0272378047532, 612.5260337668843, 602.4387296611509, 574.157253826699, 610.337422123964, 600.7214286822003, 602.1056698790505, 613.6193024202644, 614.6204022981742, 587.3196617737464, 613.9611666568684, 612.0395555026083, 613.088274240827, 611.2249772658184, 546.2259113823617, 532.0489509998209, 600.3746080347408, 618.9553730263226, 614.0562883313416, 615.3665926224038, 614.6421671206889, 616.2808610770865, 615.2080994959105, 616.9714080599052, 617.3725831588324, 611.6413630568547, 607.1126639712858, 612.8220080995445, 626.1931602761744, 629.5249271604006, 618.6833599490274, 613.7502696914257, 619.2822241584137, 623.13051465933, 625.584251937477, 622.704824605361, 614.5548640407284, 615.0394838570274, 625.7864000312314, 626.5171137271169, 610.7778363483645, 616.5460526963965, 619.1702054117027, 624.8987636667378, 626.011137059557, 614.6960104904323, 611.9182185623135, 570.1975291130566, 567.7124340367983, 561.7679297484563, 562.8928903601952, 563.1390968449201, 550.3592909224327, 561.6215625616423, 577.7053480602727, 578.3234955538602, 576.5875879028329, 571.345093825564, 287.245138372454, 578.342902578019, 573.7799290235837, 570.0742844801898, 564.3506933447075, 578.4545287983686, 574.4915894860952, 572.2071693718863, 573.4332871879575, 271.0041989407441, 556.5473312891977, 553.5820222514923, 546.7734069912244, 549.3042447840705, 549.4855153691637, 551.3509810761803, 550.4248942436457, 551.1061017268933, 551.1573261319458, 550.9529052608501, 553.5547895536939, 547.9735612970322, 546.949127666274, 562.8471953784499, 560.8574440836296, 562.7320564621192, 573.7512317836423, 570.0621859957516, 569.965489089905, 574.557720983059, 571.7659613416483, 570.0780221093072, 572.5271563270227, 571.6362460655795, 573.2107733082604, 571.5826943892521, 574.5147955783125, 571.1387459336598, 575.6717790736504]
Elapsed: 0.08692580773805579~0.011562710615499787
Time per graph: 0.0017865372179285944~0.0002419115695890066
Speed: 566.2344032403676~50.73302743371289
Total Time: 0.0840
best val loss: 0.21172067523002625 test_score: 0.9375

Testing...
Test loss: 0.6485 score: 0.9167 time: 0.08s
test Score 0.9167
Epoch Time List: [0.3416102509945631, 0.3432548730634153, 0.3388896370306611, 0.34577414486557245, 0.3495469738263637, 0.35560778016224504, 0.32872064714320004, 0.33117566583678126, 0.3473748411051929, 0.3364900841843337, 0.35646245279349387, 0.36454180465079844, 0.3272405252791941, 0.33620089711621404, 0.36378342704847455, 0.3308690548874438, 0.33829173096455634, 0.3378452090546489, 0.3266258833464235, 0.34059145976789296, 0.3374342655297369, 0.32155824010260403, 0.3230122539680451, 0.3225403649266809, 0.32232672185637057, 0.3234630930237472, 0.33874106290750206, 0.3302540669683367, 0.3245953400619328, 0.3232399069238454, 0.32303114235401154, 0.3348711975850165, 0.32213733927346766, 0.3352183438837528, 0.323314833920449, 0.3212665671017021, 0.322160130366683, 0.3379887549672276, 0.3233175971545279, 0.33529756614007056, 0.3353291410021484, 0.3241305991541594, 0.3365714114625007, 0.3399199820123613, 0.31800771690905094, 0.31822981406003237, 0.31129462481476367, 0.3062651960644871, 0.3081738860346377, 0.30921016656793654, 0.31134525313973427, 0.30925795901566744, 0.310279238037765, 0.3093684306368232, 0.3102138808462769, 0.3272619880735874, 0.3199255531653762, 0.32173537206836045, 0.3279379808809608, 0.3217829200439155, 0.31109377602115273, 0.3357339450158179, 0.31596810510382056, 0.3112099068239331, 0.3124296721071005, 0.3115128029603511, 0.312243351014331, 0.34010355873033404, 0.3571797872427851, 0.3427096358500421, 0.32908107503317297, 0.329799224389717, 0.3303553347941488, 0.33298977301456034, 0.33005024306476116, 0.32044231588952243, 0.33143108198419213, 0.33074965490959585, 0.3182099617552012, 0.3325602659024298, 0.31564647099003196, 0.3961637280881405, 0.30385703477077186, 0.32146680378355086, 0.30864279507659376, 0.3210159409791231, 0.32458637608215213, 0.30377416010014713, 0.3043870385736227, 0.30771130067296326, 0.32606830983422697, 0.3428684719838202, 0.3037986101116985, 0.31628913222812116, 0.31008351198397577, 0.30856508295983076, 0.3929416169412434, 0.3212479481007904, 0.32442119414918125, 0.31065345788374543, 0.3197390690911561, 0.3193871832918376, 0.33035100204870105, 0.41057324409484863, 0.32129943184554577, 0.32240906241349876, 0.3233841648325324, 0.3995042825117707, 0.32912470726296306, 0.3311894340440631, 0.3383977690245956, 0.41981131373904645, 0.31330764200538397, 0.32969324802979827, 0.3168698449153453, 0.31863747793249786, 0.35021286201663315, 0.3138248559553176, 0.3149045829195529, 0.32494731922633946, 0.4229017519392073, 0.323210827074945, 0.32400223589502275, 0.3406117749400437, 0.3304440297652036, 0.32759329210966825, 0.3270296307746321, 0.32750593265518546, 0.3269335408695042, 0.3267814547289163, 0.3441163331735879, 0.3265817847568542, 0.3338572618085891, 0.3279891398269683, 0.32419386715628207, 0.3235634530428797, 0.32453658781014383, 0.3222020629327744, 0.3162157249171287, 0.31778907706029713, 0.31649670586921275, 0.3167841020040214, 0.31867491104640067, 0.31713722785934806, 0.3179711508564651, 0.31647649221122265, 0.3339183824136853, 0.3359926720149815, 0.33649854199029505, 0.3360335249453783]
Total Epoch List: [46, 54, 50]
Total Time List: [0.0892837829887867, 0.08063778281211853, 0.08400705899111927]
========================training times:2========================
========================k_idx:0========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: SAMamba
GraphEncoder(
  (subgraph_conv): GATConv(128, 128, heads=1)
  (embedding): Linear(in_features=14887, out_features=128, bias=False)
  (encoder): GraphMambaEncoder(
    (layers): ModuleList(
      (0-1): 2 x MambaEncoderLayer(
        (self_selen): Selection(
          (khop_structure_extractor): KHopStructureExtractor(
            (structure_extractor): StructureExtractor(
              (gcn): ModuleList(
                (0): GATConv(128, 128, heads=1)
              )
              (relu): ReLU()
              (bn): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (out_proj): Linear(in_features=128, out_features=128, bias=True)
            )
            (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (out_proj): Linear(in_features=256, out_features=128, bias=True)
          )
          (out_proj): Mamba(
            (in_proj): Linear(in_features=128, out_features=512, bias=False)
            (conv1d): Conv1d(256, 256, kernel_size=(4,), stride=(1,), padding=(3,), groups=256)
            (act): SiLU()
            (x_proj): Linear(in_features=256, out_features=40, bias=False)
            (dt_proj): Linear(in_features=8, out_features=256, bias=True)
            (out_proj): Linear(in_features=256, out_features=128, bias=False)
          )
        )
        (norm1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (linear1): Linear(in_features=128, out_features=256, bias=True)
        (dropout): Dropout(p=0.6, inplace=False)
        (linear2): Linear(in_features=256, out_features=128, bias=True)
        (dropout1): Dropout(p=0.6, inplace=False)
        (dropout2): Dropout(p=0.6, inplace=False)
        (activation): ReLU()
      )
    )
  )
  (classifier): Sequential(
    (0): Linear(in_features=128, out_features=128, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=128, out_features=2, bias=True)
  )
)
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x75fef858ece0>
Training...
Epoch 1/1000, LR 0.000500
Train loss: 0.7202;  Loss pred: 0.7072; Loss self: 1.3018; time: 0.17s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.7036 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6985 score: 0.5102 time: 0.08s
Epoch 2/1000, LR 0.000000
Train loss: 0.7222;  Loss pred: 0.7091; Loss self: 1.3152; time: 0.16s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.7008 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6964 score: 0.5102 time: 0.08s
Epoch 3/1000, LR 0.000050
Train loss: 0.7157;  Loss pred: 0.7022; Loss self: 1.3526; time: 0.16s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6983 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6947 score: 0.5102 time: 0.08s
Epoch 4/1000, LR 0.000100
Train loss: 0.6816;  Loss pred: 0.6681; Loss self: 1.3444; time: 0.16s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6965 score: 0.4898 time: 0.09s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6936 score: 0.5102 time: 0.08s
Epoch 5/1000, LR 0.000150
Train loss: 0.6447;  Loss pred: 0.6310; Loss self: 1.3686; time: 0.17s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6953 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6929 score: 0.5102 time: 0.08s
Epoch 6/1000, LR 0.000200
Train loss: 0.6052;  Loss pred: 0.5919; Loss self: 1.3250; time: 0.17s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6944 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6924 score: 0.5102 time: 0.08s
Epoch 7/1000, LR 0.000250
Train loss: 0.5454;  Loss pred: 0.5317; Loss self: 1.3676; time: 0.16s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6935 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6920 score: 0.5102 time: 0.08s
Epoch 8/1000, LR 0.000300
Train loss: 0.4982;  Loss pred: 0.4837; Loss self: 1.4570; time: 0.16s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6925 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6915 score: 0.5102 time: 0.08s
Epoch 9/1000, LR 0.000350
Train loss: 0.4421;  Loss pred: 0.4271; Loss self: 1.4975; time: 0.17s
Val loss: 0.6913 score: 0.5714 time: 0.08s
Test loss: 0.6908 score: 0.6122 time: 0.08s
Epoch 10/1000, LR 0.000400
Train loss: 0.3866;  Loss pred: 0.3701; Loss self: 1.6433; time: 0.16s
Val loss: 0.6896 score: 0.5510 time: 0.08s
Test loss: 0.6899 score: 0.5102 time: 0.08s
Epoch 11/1000, LR 0.000450
Train loss: 0.3174;  Loss pred: 0.3010; Loss self: 1.6332; time: 0.17s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6874 score: 0.5102 time: 0.08s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6884 score: 0.4898 time: 0.08s
Epoch 12/1000, LR 0.000450
Train loss: 0.2786;  Loss pred: 0.2609; Loss self: 1.7735; time: 0.14s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6841 score: 0.5102 time: 0.08s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6859 score: 0.4898 time: 0.08s
Epoch 13/1000, LR 0.000450
Train loss: 0.2415;  Loss pred: 0.2226; Loss self: 1.8899; time: 0.14s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6793 score: 0.5102 time: 0.08s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6822 score: 0.4898 time: 0.08s
Epoch 14/1000, LR 0.000450
Train loss: 0.2009;  Loss pred: 0.1811; Loss self: 1.9834; time: 0.14s
Val loss: 0.6722 score: 0.5306 time: 0.08s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6762 score: 0.4898 time: 0.08s
Epoch 15/1000, LR 0.000450
Train loss: 0.1748;  Loss pred: 0.1542; Loss self: 2.0560; time: 0.14s
Val loss: 0.6617 score: 0.6327 time: 0.08s
Test loss: 0.6673 score: 0.5714 time: 0.08s
Epoch 16/1000, LR 0.000450
Train loss: 0.1509;  Loss pred: 0.1294; Loss self: 2.1532; time: 0.14s
Val loss: 0.6465 score: 0.8776 time: 0.08s
Test loss: 0.6541 score: 0.8163 time: 0.08s
Epoch 17/1000, LR 0.000450
Train loss: 0.1337;  Loss pred: 0.1115; Loss self: 2.2231; time: 0.14s
Val loss: 0.6254 score: 0.9796 time: 0.08s
Test loss: 0.6359 score: 0.8776 time: 0.08s
Epoch 18/1000, LR 0.000450
Train loss: 0.1179;  Loss pred: 0.0950; Loss self: 2.2912; time: 0.14s
Val loss: 0.5970 score: 1.0000 time: 0.08s
Test loss: 0.6112 score: 0.9592 time: 0.08s
Epoch 19/1000, LR 0.000450
Train loss: 0.1050;  Loss pred: 0.0817; Loss self: 2.3315; time: 0.14s
Val loss: 0.5594 score: 0.9592 time: 0.08s
Test loss: 0.5784 score: 0.9388 time: 0.08s
Epoch 20/1000, LR 0.000450
Train loss: 0.0908;  Loss pred: 0.0670; Loss self: 2.3812; time: 0.16s
Val loss: 0.5116 score: 0.9592 time: 0.08s
Test loss: 0.5379 score: 0.9388 time: 0.08s
Epoch 21/1000, LR 0.000450
Train loss: 0.0804;  Loss pred: 0.0561; Loss self: 2.4350; time: 0.16s
Val loss: 0.4552 score: 0.9388 time: 0.09s
Test loss: 0.4906 score: 0.9388 time: 0.08s
Epoch 22/1000, LR 0.000450
Train loss: 0.0702;  Loss pred: 0.0455; Loss self: 2.4693; time: 0.16s
Val loss: 0.3920 score: 0.9388 time: 0.08s
Test loss: 0.4385 score: 0.9388 time: 0.08s
Epoch 23/1000, LR 0.000450
Train loss: 0.0656;  Loss pred: 0.0404; Loss self: 2.5176; time: 0.16s
Val loss: 0.3268 score: 0.9592 time: 0.08s
Test loss: 0.3866 score: 0.9388 time: 0.08s
Epoch 24/1000, LR 0.000450
Train loss: 0.0580;  Loss pred: 0.0325; Loss self: 2.5504; time: 0.17s
Val loss: 0.2655 score: 1.0000 time: 0.08s
Test loss: 0.3400 score: 0.9388 time: 0.08s
Epoch 25/1000, LR 0.000450
Train loss: 0.0555;  Loss pred: 0.0296; Loss self: 2.5909; time: 0.17s
Val loss: 0.2134 score: 1.0000 time: 0.08s
Test loss: 0.3042 score: 0.9388 time: 0.08s
Epoch 26/1000, LR 0.000450
Train loss: 0.0491;  Loss pred: 0.0233; Loss self: 2.5792; time: 0.16s
Val loss: 0.1713 score: 1.0000 time: 0.08s
Test loss: 0.2780 score: 0.9388 time: 0.08s
Epoch 27/1000, LR 0.000450
Train loss: 0.0461;  Loss pred: 0.0199; Loss self: 2.6216; time: 0.16s
Val loss: 0.1381 score: 1.0000 time: 0.08s
Test loss: 0.2616 score: 0.9388 time: 0.08s
Epoch 28/1000, LR 0.000450
Train loss: 0.0430;  Loss pred: 0.0170; Loss self: 2.6040; time: 0.16s
Val loss: 0.1133 score: 1.0000 time: 0.08s
Test loss: 0.2529 score: 0.9388 time: 0.08s
Epoch 29/1000, LR 0.000450
Train loss: 0.0403;  Loss pred: 0.0141; Loss self: 2.6183; time: 0.17s
Val loss: 0.0954 score: 0.9796 time: 0.08s
Test loss: 0.2508 score: 0.9592 time: 0.08s
Epoch 30/1000, LR 0.000450
Train loss: 0.0390;  Loss pred: 0.0126; Loss self: 2.6382; time: 0.17s
Val loss: 0.0833 score: 0.9796 time: 0.08s
Test loss: 0.2531 score: 0.9592 time: 0.08s
Epoch 31/1000, LR 0.000450
Train loss: 0.0365;  Loss pred: 0.0097; Loss self: 2.6805; time: 0.17s
Val loss: 0.0747 score: 0.9796 time: 0.08s
Test loss: 0.2583 score: 0.9592 time: 0.08s
Epoch 32/1000, LR 0.000450
Train loss: 0.0350;  Loss pred: 0.0082; Loss self: 2.6733; time: 0.16s
Val loss: 0.0693 score: 0.9796 time: 0.08s
Test loss: 0.2662 score: 0.9592 time: 0.08s
Epoch 33/1000, LR 0.000449
Train loss: 0.0340;  Loss pred: 0.0072; Loss self: 2.6850; time: 0.17s
Val loss: 0.0660 score: 0.9796 time: 0.08s
Test loss: 0.2718 score: 0.9592 time: 0.08s
Epoch 34/1000, LR 0.000449
Train loss: 0.0328;  Loss pred: 0.0060; Loss self: 2.6781; time: 0.17s
Val loss: 0.0640 score: 0.9796 time: 0.08s
Test loss: 0.2741 score: 0.9592 time: 0.08s
Epoch 35/1000, LR 0.000449
Train loss: 0.0327;  Loss pred: 0.0056; Loss self: 2.7082; time: 0.17s
Val loss: 0.0631 score: 0.9796 time: 0.08s
Test loss: 0.2685 score: 0.9592 time: 0.08s
Epoch 36/1000, LR 0.000449
Train loss: 0.0317;  Loss pred: 0.0048; Loss self: 2.6891; time: 0.17s
Val loss: 0.0622 score: 0.9796 time: 0.08s
Test loss: 0.2615 score: 0.9592 time: 0.08s
Epoch 37/1000, LR 0.000449
Train loss: 0.0312;  Loss pred: 0.0042; Loss self: 2.6959; time: 0.16s
Val loss: 0.0622 score: 0.9796 time: 0.09s
Test loss: 0.2534 score: 0.9592 time: 0.08s
Epoch 38/1000, LR 0.000449
Train loss: 0.0301;  Loss pred: 0.0034; Loss self: 2.6713; time: 0.17s
Val loss: 0.0621 score: 0.9796 time: 0.08s
Test loss: 0.2462 score: 0.9592 time: 0.08s
Epoch 39/1000, LR 0.000449
Train loss: 0.0297;  Loss pred: 0.0032; Loss self: 2.6511; time: 0.16s
Val loss: 0.0623 score: 0.9796 time: 0.08s
Test loss: 0.2395 score: 0.9592 time: 0.08s
     INFO: Early stopping counter 1 of 20
Epoch 40/1000, LR 0.000449
Train loss: 0.0296;  Loss pred: 0.0029; Loss self: 2.6751; time: 0.16s
Val loss: 0.0625 score: 0.9796 time: 0.08s
Test loss: 0.2341 score: 0.9592 time: 0.08s
     INFO: Early stopping counter 2 of 20
Epoch 41/1000, LR 0.000449
Train loss: 0.0294;  Loss pred: 0.0028; Loss self: 2.6627; time: 0.16s
Val loss: 0.0629 score: 0.9796 time: 0.08s
Test loss: 0.2310 score: 0.9592 time: 0.08s
     INFO: Early stopping counter 3 of 20
Epoch 42/1000, LR 0.000449
Train loss: 0.0293;  Loss pred: 0.0026; Loss self: 2.6699; time: 0.16s
Val loss: 0.0634 score: 0.9796 time: 0.08s
Test loss: 0.2280 score: 0.9592 time: 0.08s
     INFO: Early stopping counter 4 of 20
Epoch 43/1000, LR 0.000449
Train loss: 0.0286;  Loss pred: 0.0022; Loss self: 2.6342; time: 0.16s
Val loss: 0.0638 score: 0.9796 time: 0.08s
Test loss: 0.2249 score: 0.9592 time: 0.08s
     INFO: Early stopping counter 5 of 20
Epoch 44/1000, LR 0.000449
Train loss: 0.0283;  Loss pred: 0.0022; Loss self: 2.6131; time: 0.16s
Val loss: 0.0645 score: 0.9796 time: 0.08s
Test loss: 0.2237 score: 0.9592 time: 0.08s
     INFO: Early stopping counter 6 of 20
Epoch 45/1000, LR 0.000449
Train loss: 0.0281;  Loss pred: 0.0019; Loss self: 2.6191; time: 0.16s
Val loss: 0.0650 score: 0.9796 time: 0.08s
Test loss: 0.2218 score: 0.9592 time: 0.08s
     INFO: Early stopping counter 7 of 20
Epoch 46/1000, LR 0.000449
Train loss: 0.0281;  Loss pred: 0.0018; Loss self: 2.6223; time: 0.17s
Val loss: 0.0662 score: 0.9796 time: 0.08s
Test loss: 0.2218 score: 0.9592 time: 0.08s
     INFO: Early stopping counter 8 of 20
Epoch 47/1000, LR 0.000449
Train loss: 0.0274;  Loss pred: 0.0018; Loss self: 2.5576; time: 0.17s
Val loss: 0.0674 score: 0.9796 time: 0.08s
Test loss: 0.2217 score: 0.9592 time: 0.08s
     INFO: Early stopping counter 9 of 20
Epoch 48/1000, LR 0.000448
Train loss: 0.0276;  Loss pred: 0.0018; Loss self: 2.5826; time: 0.17s
Val loss: 0.0683 score: 0.9796 time: 0.08s
Test loss: 0.2209 score: 0.9592 time: 0.08s
     INFO: Early stopping counter 10 of 20
Epoch 49/1000, LR 0.000448
Train loss: 0.0272;  Loss pred: 0.0017; Loss self: 2.5468; time: 0.17s
Val loss: 0.0690 score: 0.9796 time: 0.08s
Test loss: 0.2198 score: 0.9592 time: 0.08s
     INFO: Early stopping counter 11 of 20
Epoch 50/1000, LR 0.000448
Train loss: 0.0271;  Loss pred: 0.0016; Loss self: 2.5508; time: 0.17s
Val loss: 0.0695 score: 0.9796 time: 0.08s
Test loss: 0.2185 score: 0.9592 time: 0.08s
     INFO: Early stopping counter 12 of 20
Epoch 51/1000, LR 0.000448
Train loss: 0.0269;  Loss pred: 0.0015; Loss self: 2.5393; time: 0.17s
Val loss: 0.0700 score: 0.9592 time: 0.08s
Test loss: 0.2176 score: 0.9592 time: 0.08s
     INFO: Early stopping counter 13 of 20
Epoch 52/1000, LR 0.000448
Train loss: 0.0266;  Loss pred: 0.0015; Loss self: 2.5140; time: 0.17s
Val loss: 0.0707 score: 0.9592 time: 0.08s
Test loss: 0.2171 score: 0.9592 time: 0.08s
     INFO: Early stopping counter 14 of 20
Epoch 53/1000, LR 0.000448
Train loss: 0.0264;  Loss pred: 0.0014; Loss self: 2.5003; time: 0.14s
Val loss: 0.0716 score: 0.9592 time: 0.08s
Test loss: 0.2171 score: 0.9592 time: 0.08s
     INFO: Early stopping counter 15 of 20
Epoch 54/1000, LR 0.000448
Train loss: 0.0262;  Loss pred: 0.0014; Loss self: 2.4880; time: 0.14s
Val loss: 0.0721 score: 0.9592 time: 0.08s
Test loss: 0.2165 score: 0.9592 time: 0.08s
     INFO: Early stopping counter 16 of 20
Epoch 55/1000, LR 0.000448
Train loss: 0.0266;  Loss pred: 0.0017; Loss self: 2.4954; time: 0.14s
Val loss: 0.0730 score: 0.9592 time: 0.08s
Test loss: 0.2163 score: 0.9592 time: 0.08s
     INFO: Early stopping counter 17 of 20
Epoch 56/1000, LR 0.000448
Train loss: 0.0260;  Loss pred: 0.0012; Loss self: 2.4834; time: 0.14s
Val loss: 0.0737 score: 0.9592 time: 0.08s
Test loss: 0.2160 score: 0.9592 time: 0.08s
     INFO: Early stopping counter 18 of 20
Epoch 57/1000, LR 0.000448
Train loss: 0.0258;  Loss pred: 0.0014; Loss self: 2.4469; time: 0.14s
Val loss: 0.0744 score: 0.9592 time: 0.08s
Test loss: 0.2160 score: 0.9592 time: 0.08s
     INFO: Early stopping counter 19 of 20
Epoch 58/1000, LR 0.000448
Train loss: 0.0257;  Loss pred: 0.0012; Loss self: 2.4449; time: 0.16s
Val loss: 0.0751 score: 0.9592 time: 0.08s
Test loss: 0.2159 score: 0.9592 time: 0.08s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 037,   Train_Loss: 0.0301,   Val_Loss: 0.0621,   Val_Precision: 1.0000,   Val_Recall: 0.9583,   Val_accuracy: 0.9787,   Val_Score: 0.9796,   Val_Loss: 0.0621,   Test_Precision: 1.0000,   Test_Recall: 0.9200,   Test_accuracy: 0.9583,   Test_Score: 0.9592,   Test_loss: 0.2462


[0.08580371108837426, 0.08559617307037115, 0.08592875790782273, 0.08626094507053494, 0.08614550600759685, 0.08681278699077666, 0.0860787290148437, 0.08620444010011852, 0.08587743295356631, 0.08605238096788526, 0.08610205189324915, 0.08630912005901337, 0.08626337302848697, 0.08598034596070647, 0.0860396979842335, 0.08584887394681573, 0.08581978804431856, 0.08637914806604385, 0.08632824500091374, 0.08596233814023435, 0.08590189181268215, 0.08603417593985796, 0.08594518899917603, 0.08588272100314498, 0.08609863882884383, 0.08555595600046217, 0.08637480111792684, 0.08567612594924867, 0.08558370498940349, 0.08524416200816631, 0.08557071094401181, 0.08606824185699224, 0.08595852693542838, 0.08524036291055381, 0.08584264502860606, 0.08524847589433193, 0.0855790339410305, 0.08498359611257911, 0.08505651890300214, 0.08519502216950059, 0.08521903003565967, 0.08525734418071806, 0.08482111897319555, 0.0850494490005076, 0.08503626892343163, 0.08515079086646438, 0.08502391492947936, 0.08566791797056794, 0.08538781804963946, 0.08548118197359145, 0.08513387897983193, 0.08533355197869241, 0.08495814795605838, 0.08526995894499123, 0.0855287229642272, 0.08541887602768838, 0.08551693009212613, 0.08513761078938842]
[0.0017510961446606992, 0.0017468606749055336, 0.0017536481205678107, 0.0017604274504190805, 0.001758071551175446, 0.0017716895304240134, 0.0017567087554049734, 0.0017592742877575208, 0.0017526006725217616, 0.0017561710401609236, 0.0017571847325152888, 0.0017614106134492525, 0.0017604770005813666, 0.0017547009379736014, 0.0017559122037598674, 0.0017520178356493007, 0.0017514242458024196, 0.0017628397564498745, 0.0017618009183859946, 0.0017543334314333542, 0.0017530998329118807, 0.001755799508976693, 0.001753983448962776, 0.001752708591900918, 0.0017571150781396701, 0.0017460399183767791, 0.0017627510432229967, 0.0017484923663111974, 0.0017466062242735404, 0.0017396767756768636, 0.0017463410396737103, 0.0017564947317753518, 0.0017542556517434363, 0.0017395992430725268, 0.0017518907148695113, 0.0017397648141700395, 0.0017465108967557245, 0.0017343591043383491, 0.0017358473245510642, 0.0017386739218265427, 0.0017391638782787689, 0.0017399457996064912, 0.0017310432443509297, 0.0017357030408266857, 0.0017354340596618702, 0.0017377712421727423, 0.0017351819373363135, 0.001748324856542203, 0.001742608531625295, 0.0017445139178283969, 0.001737426101629223, 0.001741501060789641, 0.0017338397542052731, 0.0017402032437753311, 0.0017454841421270858, 0.0017432423679120078, 0.0017452434712678802, 0.001737502261007927]
[571.0708706938333, 572.4554993798105, 570.239826491652, 568.043857622161, 568.8050633271441, 564.4329792707376, 569.2463232298687, 568.4161969278034, 570.5806323588429, 569.4206185682044, 569.0921287305797, 567.7267937211795, 568.0278695318186, 569.8976836217115, 569.5045560129593, 570.7704451703818, 570.9638897581023, 567.266534772206, 567.6010209576409, 570.017068638408, 570.4181708459868, 569.5411092709642, 570.1308074436811, 570.5455000454125, 569.11468829847, 572.7245920755687, 567.2950833553954, 571.9212844547355, 572.5388963479311, 574.8194227694543, 572.6258372687856, 569.3156841918133, 570.0423418936501, 574.8450420303548, 570.8118614433574, 574.7903347942195, 572.5701464889656, 576.581860987489, 576.087531349352, 575.1509742260712, 574.9889429567091, 574.7305463343522, 577.686319081508, 576.1354197568941, 576.2247170571487, 575.4497345402585, 576.30844263807, 571.9760811373335, 573.852349424297, 573.2255786441752, 575.5640479110322, 574.2172787116044, 576.7545689124901, 574.6455211924113, 572.9069522117674, 573.6436988952738, 572.9859566662767, 575.5388193969308]
Elapsed: 0.08567632515995025~0.0004533942522086847
Time per graph: 0.0017484964318357193~9.252943922626208e-06
Speed: 571.9359655833662~3.0255784428715202
Total Time: 0.0856
best val loss: 0.06210675835609436 test_score: 0.9592

Testing...
Test loss: 0.6112 score: 0.9592 time: 0.08s
test Score 0.9592
Epoch Time List: [0.333628183696419, 0.3288253550417721, 0.3291889021638781, 0.3303900118917227, 0.3306734939105809, 0.3313491251319647, 0.3295870379079133, 0.3298175421077758, 0.3304097573272884, 0.3300056210719049, 0.3317355290055275, 0.30912051000632346, 0.30887144268490374, 0.30891769216395915, 0.3088087949436158, 0.30880032107234, 0.3091954877600074, 0.30884792492724955, 0.3086675510276109, 0.32194894598796964, 0.32809953205287457, 0.3301270720548928, 0.3297930150292814, 0.330932242795825, 0.33045038883574307, 0.3298097951337695, 0.3298252720851451, 0.33012558380141854, 0.3311631972901523, 0.33190330816432834, 0.3304620000999421, 0.32983937906101346, 0.3324849351774901, 0.33228532667271793, 0.33306984207592905, 0.3298439681529999, 0.3300067908130586, 0.33487692079506814, 0.3266614768654108, 0.32715314719825983, 0.32699806708842516, 0.3275275188498199, 0.32670374400913715, 0.32689414685592055, 0.3269361569546163, 0.3301063561812043, 0.32969524315558374, 0.33089840109460056, 0.3305910697672516, 0.330386454006657, 0.33054715185426176, 0.3309052810072899, 0.30592633970081806, 0.30687447101809084, 0.30616434500552714, 0.30684398603625596, 0.30575073789805174, 0.3261786983348429]
Total Epoch List: [58]
Total Time List: [0.08557850285433233]
========================k_idx:1========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: SAMamba
GraphEncoder(
  (subgraph_conv): GATConv(128, 128, heads=1)
  (embedding): Linear(in_features=14887, out_features=128, bias=False)
  (encoder): GraphMambaEncoder(
    (layers): ModuleList(
      (0-1): 2 x MambaEncoderLayer(
        (self_selen): Selection(
          (khop_structure_extractor): KHopStructureExtractor(
            (structure_extractor): StructureExtractor(
              (gcn): ModuleList(
                (0): GATConv(128, 128, heads=1)
              )
              (relu): ReLU()
              (bn): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (out_proj): Linear(in_features=128, out_features=128, bias=True)
            )
            (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (out_proj): Linear(in_features=256, out_features=128, bias=True)
          )
          (out_proj): Mamba(
            (in_proj): Linear(in_features=128, out_features=512, bias=False)
            (conv1d): Conv1d(256, 256, kernel_size=(4,), stride=(1,), padding=(3,), groups=256)
            (act): SiLU()
            (x_proj): Linear(in_features=256, out_features=40, bias=False)
            (dt_proj): Linear(in_features=8, out_features=256, bias=True)
            (out_proj): Linear(in_features=256, out_features=128, bias=False)
          )
        )
        (norm1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (linear1): Linear(in_features=128, out_features=256, bias=True)
        (dropout): Dropout(p=0.6, inplace=False)
        (linear2): Linear(in_features=256, out_features=128, bias=True)
        (dropout1): Dropout(p=0.6, inplace=False)
        (dropout2): Dropout(p=0.6, inplace=False)
        (activation): ReLU()
      )
    )
  )
  (classifier): Sequential(
    (0): Linear(in_features=128, out_features=128, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=128, out_features=2, bias=True)
  )
)
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x75fef9b63cd0>
Training...
Epoch 1/1000, LR 0.000500
Train loss: 0.7025;  Loss pred: 0.6906; Loss self: 1.1928; time: 0.15s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6953 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6936 score: 0.5102 time: 0.08s
Epoch 2/1000, LR 0.000000
Train loss: 0.6964;  Loss pred: 0.6838; Loss self: 1.2650; time: 0.15s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6961 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6940 score: 0.5102 time: 0.08s
     INFO: Early stopping counter 1 of 20
Epoch 3/1000, LR 0.000050
Train loss: 0.6830;  Loss pred: 0.6697; Loss self: 1.3314; time: 0.15s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6968 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6942 score: 0.5102 time: 0.08s
     INFO: Early stopping counter 2 of 20
Epoch 4/1000, LR 0.000100
Train loss: 0.6585;  Loss pred: 0.6460; Loss self: 1.2496; time: 0.15s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6972 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6944 score: 0.5102 time: 0.08s
     INFO: Early stopping counter 3 of 20
Epoch 5/1000, LR 0.000150
Train loss: 0.6058;  Loss pred: 0.5925; Loss self: 1.3350; time: 0.15s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6977 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6946 score: 0.5102 time: 0.08s
     INFO: Early stopping counter 4 of 20
Epoch 6/1000, LR 0.000200
Train loss: 0.5436;  Loss pred: 0.5296; Loss self: 1.4028; time: 0.17s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6981 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6947 score: 0.5102 time: 0.08s
     INFO: Early stopping counter 5 of 20
Epoch 7/1000, LR 0.000250
Train loss: 0.4995;  Loss pred: 0.4850; Loss self: 1.4500; time: 0.15s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6983 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6945 score: 0.5102 time: 0.08s
     INFO: Early stopping counter 6 of 20
Epoch 8/1000, LR 0.000300
Train loss: 0.4458;  Loss pred: 0.4306; Loss self: 1.5128; time: 0.15s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6987 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6945 score: 0.5102 time: 0.08s
     INFO: Early stopping counter 7 of 20
Epoch 9/1000, LR 0.000350
Train loss: 0.4013;  Loss pred: 0.3849; Loss self: 1.6406; time: 0.15s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6985 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6940 score: 0.5102 time: 0.08s
     INFO: Early stopping counter 8 of 20
Epoch 10/1000, LR 0.000400
Train loss: 0.3428;  Loss pred: 0.3253; Loss self: 1.7486; time: 0.16s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6980 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6932 score: 0.5102 time: 0.08s
     INFO: Early stopping counter 9 of 20
Epoch 11/1000, LR 0.000450
Train loss: 0.3070;  Loss pred: 0.2889; Loss self: 1.8153; time: 0.15s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6968 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6915 score: 0.5102 time: 0.08s
     INFO: Early stopping counter 10 of 20
Epoch 12/1000, LR 0.000450
Train loss: 0.2713;  Loss pred: 0.2520; Loss self: 1.9291; time: 0.16s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6946 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6888 score: 0.5102 time: 0.08s
Epoch 13/1000, LR 0.000450
Train loss: 0.2439;  Loss pred: 0.2241; Loss self: 1.9779; time: 0.17s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6910 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6845 score: 0.5102 time: 0.08s
Epoch 14/1000, LR 0.000450
Train loss: 0.2109;  Loss pred: 0.1907; Loss self: 2.0118; time: 0.17s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6853 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6778 score: 0.5102 time: 0.08s
Epoch 15/1000, LR 0.000450
Train loss: 0.1893;  Loss pred: 0.1686; Loss self: 2.0644; time: 0.17s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6775 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6685 score: 0.5102 time: 0.08s
Epoch 16/1000, LR 0.000450
Train loss: 0.1711;  Loss pred: 0.1504; Loss self: 2.0705; time: 0.17s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6668 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6555 score: 0.5102 time: 0.08s
Epoch 17/1000, LR 0.000450
Train loss: 0.1504;  Loss pred: 0.1293; Loss self: 2.1125; time: 0.15s
Val loss: 0.6517 score: 0.5102 time: 0.08s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6373 score: 0.5102 time: 0.08s
Epoch 18/1000, LR 0.000450
Train loss: 0.1380;  Loss pred: 0.1166; Loss self: 2.1430; time: 0.17s
Val loss: 0.6313 score: 0.6327 time: 0.08s
Test loss: 0.6123 score: 0.6531 time: 0.08s
Epoch 19/1000, LR 0.000450
Train loss: 0.1249;  Loss pred: 0.1030; Loss self: 2.1910; time: 0.17s
Val loss: 0.6051 score: 0.8367 time: 0.08s
Test loss: 0.5798 score: 0.8776 time: 0.08s
Epoch 20/1000, LR 0.000450
Train loss: 0.1112;  Loss pred: 0.0893; Loss self: 2.1864; time: 0.17s
Val loss: 0.5711 score: 0.9184 time: 0.08s
Test loss: 0.5375 score: 0.9796 time: 0.08s
Epoch 21/1000, LR 0.000450
Train loss: 0.1017;  Loss pred: 0.0790; Loss self: 2.2716; time: 0.17s
Val loss: 0.5284 score: 0.9184 time: 0.08s
Test loss: 0.4849 score: 0.9796 time: 0.08s
Epoch 22/1000, LR 0.000450
Train loss: 0.0885;  Loss pred: 0.0655; Loss self: 2.3027; time: 0.17s
Val loss: 0.4782 score: 0.9388 time: 0.08s
Test loss: 0.4219 score: 0.9796 time: 0.08s
Epoch 23/1000, LR 0.000450
Train loss: 0.0794;  Loss pred: 0.0558; Loss self: 2.3580; time: 0.17s
Val loss: 0.4262 score: 0.9388 time: 0.08s
Test loss: 0.3549 score: 0.9796 time: 0.08s
Epoch 24/1000, LR 0.000450
Train loss: 0.0718;  Loss pred: 0.0481; Loss self: 2.3680; time: 0.17s
Val loss: 0.3773 score: 0.9388 time: 0.08s
Test loss: 0.2896 score: 0.9796 time: 0.08s
Epoch 25/1000, LR 0.000450
Train loss: 0.0641;  Loss pred: 0.0398; Loss self: 2.4272; time: 0.17s
Val loss: 0.3366 score: 0.9388 time: 0.08s
Test loss: 0.2321 score: 0.9796 time: 0.08s
Epoch 26/1000, LR 0.000450
Train loss: 0.0582;  Loss pred: 0.0335; Loss self: 2.4718; time: 0.17s
Val loss: 0.3072 score: 0.9388 time: 0.08s
Test loss: 0.1858 score: 0.9796 time: 0.08s
Epoch 27/1000, LR 0.000450
Train loss: 0.0530;  Loss pred: 0.0281; Loss self: 2.4929; time: 0.17s
Val loss: 0.2890 score: 0.9388 time: 0.08s
Test loss: 0.1509 score: 0.9796 time: 0.08s
Epoch 28/1000, LR 0.000450
Train loss: 0.0492;  Loss pred: 0.0241; Loss self: 2.5133; time: 0.17s
Val loss: 0.2808 score: 0.9388 time: 0.08s
Test loss: 0.1260 score: 0.9796 time: 0.08s
Epoch 29/1000, LR 0.000450
Train loss: 0.0456;  Loss pred: 0.0202; Loss self: 2.5419; time: 0.17s
Val loss: 0.2787 score: 0.9388 time: 0.08s
Test loss: 0.1082 score: 0.9796 time: 0.08s
Epoch 30/1000, LR 0.000450
Train loss: 0.0425;  Loss pred: 0.0171; Loss self: 2.5422; time: 0.17s
Val loss: 0.2821 score: 0.9388 time: 0.08s
Test loss: 0.0953 score: 0.9796 time: 0.08s
     INFO: Early stopping counter 1 of 20
Epoch 31/1000, LR 0.000450
Train loss: 0.0399;  Loss pred: 0.0142; Loss self: 2.5693; time: 0.17s
Val loss: 0.2876 score: 0.9388 time: 0.08s
Test loss: 0.0866 score: 0.9796 time: 0.08s
     INFO: Early stopping counter 2 of 20
Epoch 32/1000, LR 0.000450
Train loss: 0.0379;  Loss pred: 0.0119; Loss self: 2.6070; time: 0.16s
Val loss: 0.2966 score: 0.9388 time: 0.14s
Test loss: 0.0807 score: 0.9796 time: 0.08s
     INFO: Early stopping counter 3 of 20
Epoch 33/1000, LR 0.000449
Train loss: 0.0363;  Loss pred: 0.0105; Loss self: 2.5824; time: 0.15s
Val loss: 0.3055 score: 0.9388 time: 0.08s
Test loss: 0.0767 score: 0.9796 time: 0.08s
     INFO: Early stopping counter 4 of 20
Epoch 34/1000, LR 0.000449
Train loss: 0.0344;  Loss pred: 0.0086; Loss self: 2.5731; time: 0.15s
Val loss: 0.3142 score: 0.9388 time: 0.08s
Test loss: 0.0745 score: 0.9592 time: 0.08s
     INFO: Early stopping counter 5 of 20
Epoch 35/1000, LR 0.000449
Train loss: 0.0333;  Loss pred: 0.0075; Loss self: 2.5757; time: 0.15s
Val loss: 0.3226 score: 0.9388 time: 0.08s
Test loss: 0.0738 score: 0.9592 time: 0.08s
     INFO: Early stopping counter 6 of 20
Epoch 36/1000, LR 0.000449
Train loss: 0.0323;  Loss pred: 0.0066; Loss self: 2.5723; time: 0.15s
Val loss: 0.3300 score: 0.9388 time: 0.08s
Test loss: 0.0740 score: 0.9592 time: 0.08s
     INFO: Early stopping counter 7 of 20
Epoch 37/1000, LR 0.000449
Train loss: 0.0313;  Loss pred: 0.0057; Loss self: 2.5619; time: 0.16s
Val loss: 0.3360 score: 0.9592 time: 0.11s
Test loss: 0.0747 score: 0.9592 time: 0.08s
     INFO: Early stopping counter 8 of 20
Epoch 38/1000, LR 0.000449
Train loss: 0.0307;  Loss pred: 0.0049; Loss self: 2.5815; time: 0.15s
Val loss: 0.3417 score: 0.9592 time: 0.08s
Test loss: 0.0756 score: 0.9592 time: 0.08s
     INFO: Early stopping counter 9 of 20
Epoch 39/1000, LR 0.000449
Train loss: 0.0298;  Loss pred: 0.0044; Loss self: 2.5358; time: 0.15s
Val loss: 0.3473 score: 0.9592 time: 0.08s
Test loss: 0.0766 score: 0.9592 time: 0.08s
     INFO: Early stopping counter 10 of 20
Epoch 40/1000, LR 0.000449
Train loss: 0.0296;  Loss pred: 0.0040; Loss self: 2.5635; time: 0.15s
Val loss: 0.3514 score: 0.9592 time: 0.08s
Test loss: 0.0777 score: 0.9592 time: 0.08s
     INFO: Early stopping counter 11 of 20
Epoch 41/1000, LR 0.000449
Train loss: 0.0290;  Loss pred: 0.0038; Loss self: 2.5153; time: 0.15s
Val loss: 0.3558 score: 0.9592 time: 0.08s
Test loss: 0.0787 score: 0.9592 time: 0.08s
     INFO: Early stopping counter 12 of 20
Epoch 42/1000, LR 0.000449
Train loss: 0.0283;  Loss pred: 0.0034; Loss self: 2.4977; time: 0.17s
Val loss: 0.3588 score: 0.9592 time: 0.10s
Test loss: 0.0797 score: 0.9592 time: 0.08s
     INFO: Early stopping counter 13 of 20
Epoch 43/1000, LR 0.000449
Train loss: 0.0281;  Loss pred: 0.0032; Loss self: 2.4949; time: 0.15s
Val loss: 0.3605 score: 0.9592 time: 0.08s
Test loss: 0.0804 score: 0.9592 time: 0.08s
     INFO: Early stopping counter 14 of 20
Epoch 44/1000, LR 0.000449
Train loss: 0.0276;  Loss pred: 0.0029; Loss self: 2.4693; time: 0.15s
Val loss: 0.3639 score: 0.9388 time: 0.08s
Test loss: 0.0812 score: 0.9592 time: 0.08s
     INFO: Early stopping counter 15 of 20
Epoch 45/1000, LR 0.000449
Train loss: 0.0272;  Loss pred: 0.0026; Loss self: 2.4589; time: 0.15s
Val loss: 0.3663 score: 0.9388 time: 0.08s
Test loss: 0.0818 score: 0.9592 time: 0.08s
     INFO: Early stopping counter 16 of 20
Epoch 46/1000, LR 0.000449
Train loss: 0.0272;  Loss pred: 0.0024; Loss self: 2.4734; time: 0.15s
Val loss: 0.3675 score: 0.9388 time: 0.08s
Test loss: 0.0822 score: 0.9592 time: 0.08s
     INFO: Early stopping counter 17 of 20
Epoch 47/1000, LR 0.000449
Train loss: 0.0268;  Loss pred: 0.0024; Loss self: 2.4406; time: 0.17s
Val loss: 0.3677 score: 0.9388 time: 0.08s
Test loss: 0.0826 score: 0.9592 time: 0.08s
     INFO: Early stopping counter 18 of 20
Epoch 48/1000, LR 0.000448
Train loss: 0.0265;  Loss pred: 0.0022; Loss self: 2.4273; time: 0.15s
Val loss: 0.3683 score: 0.9388 time: 0.08s
Test loss: 0.0829 score: 0.9592 time: 0.08s
     INFO: Early stopping counter 19 of 20
Epoch 49/1000, LR 0.000448
Train loss: 0.0264;  Loss pred: 0.0021; Loss self: 2.4313; time: 0.15s
Val loss: 0.3683 score: 0.9388 time: 0.08s
Test loss: 0.0832 score: 0.9592 time: 0.08s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 028,   Train_Loss: 0.0456,   Val_Loss: 0.2787,   Val_Precision: 0.9583,   Val_Recall: 0.9200,   Val_accuracy: 0.9388,   Val_Score: 0.9388,   Val_Loss: 0.2787,   Test_Precision: 0.9600,   Test_Recall: 1.0000,   Test_accuracy: 0.9796,   Test_Score: 0.9796,   Test_loss: 0.1082


[0.08580371108837426, 0.08559617307037115, 0.08592875790782273, 0.08626094507053494, 0.08614550600759685, 0.08681278699077666, 0.0860787290148437, 0.08620444010011852, 0.08587743295356631, 0.08605238096788526, 0.08610205189324915, 0.08630912005901337, 0.08626337302848697, 0.08598034596070647, 0.0860396979842335, 0.08584887394681573, 0.08581978804431856, 0.08637914806604385, 0.08632824500091374, 0.08596233814023435, 0.08590189181268215, 0.08603417593985796, 0.08594518899917603, 0.08588272100314498, 0.08609863882884383, 0.08555595600046217, 0.08637480111792684, 0.08567612594924867, 0.08558370498940349, 0.08524416200816631, 0.08557071094401181, 0.08606824185699224, 0.08595852693542838, 0.08524036291055381, 0.08584264502860606, 0.08524847589433193, 0.0855790339410305, 0.08498359611257911, 0.08505651890300214, 0.08519502216950059, 0.08521903003565967, 0.08525734418071806, 0.08482111897319555, 0.0850494490005076, 0.08503626892343163, 0.08515079086646438, 0.08502391492947936, 0.08566791797056794, 0.08538781804963946, 0.08548118197359145, 0.08513387897983193, 0.08533355197869241, 0.08495814795605838, 0.08526995894499123, 0.0855287229642272, 0.08541887602768838, 0.08551693009212613, 0.08513761078938842, 0.08583107613958418, 0.08547529089264572, 0.08551193284802139, 0.08748067799024284, 0.08692800905555487, 0.08626890601590276, 0.08635392086580396, 0.08646225486882031, 0.08716893987730145, 0.08626827807165682, 0.08709513093344867, 0.08692665188573301, 0.08849932299926877, 0.08674690802581608, 0.0864890799857676, 0.0869831359013915, 0.08649095706641674, 0.08669689204543829, 0.08601255901157856, 0.08693224401213229, 0.0861535940784961, 0.08675257302820683, 0.08676377800293267, 0.08634543605148792, 0.08690050314180553, 0.08687796490266919, 0.08602272509597242, 0.08624989003874362, 0.08634370914660394, 0.08807747811079025, 0.08704757294617593, 0.08623319398611784, 0.08590183896012604, 0.08598419604822993, 0.08682262990623713, 0.08717725402675569, 0.08572583086788654, 0.08592675719410181, 0.08587830304168165, 0.08665771689265966, 0.08635970088653266, 0.08862718380987644, 0.08539445116184652, 0.08592587802559137, 0.08586181094869971, 0.08537112618796527, 0.08494824706576765, 0.08513565501198173, 0.08603243017569184]
[0.0017510961446606992, 0.0017468606749055336, 0.0017536481205678107, 0.0017604274504190805, 0.001758071551175446, 0.0017716895304240134, 0.0017567087554049734, 0.0017592742877575208, 0.0017526006725217616, 0.0017561710401609236, 0.0017571847325152888, 0.0017614106134492525, 0.0017604770005813666, 0.0017547009379736014, 0.0017559122037598674, 0.0017520178356493007, 0.0017514242458024196, 0.0017628397564498745, 0.0017618009183859946, 0.0017543334314333542, 0.0017530998329118807, 0.001755799508976693, 0.001753983448962776, 0.001752708591900918, 0.0017571150781396701, 0.0017460399183767791, 0.0017627510432229967, 0.0017484923663111974, 0.0017466062242735404, 0.0017396767756768636, 0.0017463410396737103, 0.0017564947317753518, 0.0017542556517434363, 0.0017395992430725268, 0.0017518907148695113, 0.0017397648141700395, 0.0017465108967557245, 0.0017343591043383491, 0.0017358473245510642, 0.0017386739218265427, 0.0017391638782787689, 0.0017399457996064912, 0.0017310432443509297, 0.0017357030408266857, 0.0017354340596618702, 0.0017377712421727423, 0.0017351819373363135, 0.001748324856542203, 0.001742608531625295, 0.0017445139178283969, 0.001737426101629223, 0.001741501060789641, 0.0017338397542052731, 0.0017402032437753311, 0.0017454841421270858, 0.0017432423679120078, 0.0017452434712678802, 0.001737502261007927, 0.0017516546150935547, 0.0017443936916866473, 0.001745141486694314, 0.0017853199589845476, 0.0017740410011337728, 0.001760589918691893, 0.0017623249156286521, 0.001764535813649394, 0.0017789579566796214, 0.0017605771035032005, 0.0017774516517030342, 0.0017740133037904696, 0.0018061086326381381, 0.0017703450617513486, 0.0017650832650156654, 0.0017751660388039083, 0.001765121572784015, 0.0017693243274579243, 0.0017553583471750726, 0.0017741274288190262, 0.0017582366138468592, 0.0017704606740450372, 0.0017706893469986258, 0.0017621517561528148, 0.001773479655955215, 0.001773019691891208, 0.0017555658182851514, 0.0017602018375253799, 0.001762116513195999, 0.0017974995532814338, 0.0017764810805342027, 0.0017598611017575069, 0.0017530987542882866, 0.0017547795111883659, 0.0017718904062497373, 0.0017791276331990957, 0.0017495067524058478, 0.0017536072896755471, 0.0017526184294220743, 0.0017685248345440747, 0.0017624428752353604, 0.001808718036936254, 0.0017427439012621738, 0.0017535893474610485, 0.0017522818560959125, 0.0017422678813870465, 0.0017336376952197478, 0.0017374623471833005, 0.0017557638811365683]
[571.0708706938333, 572.4554993798105, 570.239826491652, 568.043857622161, 568.8050633271441, 564.4329792707376, 569.2463232298687, 568.4161969278034, 570.5806323588429, 569.4206185682044, 569.0921287305797, 567.7267937211795, 568.0278695318186, 569.8976836217115, 569.5045560129593, 570.7704451703818, 570.9638897581023, 567.266534772206, 567.6010209576409, 570.017068638408, 570.4181708459868, 569.5411092709642, 570.1308074436811, 570.5455000454125, 569.11468829847, 572.7245920755687, 567.2950833553954, 571.9212844547355, 572.5388963479311, 574.8194227694543, 572.6258372687856, 569.3156841918133, 570.0423418936501, 574.8450420303548, 570.8118614433574, 574.7903347942195, 572.5701464889656, 576.581860987489, 576.087531349352, 575.1509742260712, 574.9889429567091, 574.7305463343522, 577.686319081508, 576.1354197568941, 576.2247170571487, 575.4497345402585, 576.30844263807, 571.9760811373335, 573.852349424297, 573.2255786441752, 575.5640479110322, 574.2172787116044, 576.7545689124901, 574.6455211924113, 572.9069522117674, 573.6436988952738, 572.9859566662767, 575.5388193969308, 570.8887993005349, 573.2650861819524, 573.0194414747554, 560.123688175636, 563.6848299227072, 567.9914382010057, 567.4322544791818, 566.7212828805161, 562.126831747319, 567.9955725938942, 562.6032072612876, 563.6936306302418, 553.6765518579714, 564.8616315571442, 566.5455108097265, 563.3275863444253, 566.5332152859947, 565.1875037725556, 569.6842480108501, 563.6573696770274, 568.7516640960471, 564.8247457060218, 564.7518022825582, 567.4880137356794, 563.8632485250525, 564.0095282491424, 569.61692326455, 568.1166663283757, 567.4993636977346, 556.3283719177817, 562.9105825879618, 568.2266623208716, 570.4185218053929, 569.8721654909131, 564.36899058364, 562.0732213584216, 571.5896772760907, 570.2531039232964, 570.5748514408523, 565.4430067745132, 567.394276462128, 552.8777728638551, 573.8077747830619, 570.2589385866536, 570.684445839097, 573.9645497016708, 576.8217908259342, 575.5520409528052, 569.5526663600486]
Elapsed: 0.08603129398608668~0.0007221473020882092
Time per graph: 0.0017557406935936057~1.4737700042616509e-05
Speed: 569.6000098293468~4.744479489927113
Total Time: 0.0866
best val loss: 0.27868154644966125 test_score: 0.9796

Testing...
Test loss: 0.0747 score: 0.9592 time: 0.08s
test Score 0.9592
Epoch Time List: [0.333628183696419, 0.3288253550417721, 0.3291889021638781, 0.3303900118917227, 0.3306734939105809, 0.3313491251319647, 0.3295870379079133, 0.3298175421077758, 0.3304097573272884, 0.3300056210719049, 0.3317355290055275, 0.30912051000632346, 0.30887144268490374, 0.30891769216395915, 0.3088087949436158, 0.30880032107234, 0.3091954877600074, 0.30884792492724955, 0.3086675510276109, 0.32194894598796964, 0.32809953205287457, 0.3301270720548928, 0.3297930150292814, 0.330932242795825, 0.33045038883574307, 0.3298097951337695, 0.3298252720851451, 0.33012558380141854, 0.3311631972901523, 0.33190330816432834, 0.3304620000999421, 0.32983937906101346, 0.3324849351774901, 0.33228532667271793, 0.33306984207592905, 0.3298439681529999, 0.3300067908130586, 0.33487692079506814, 0.3266614768654108, 0.32715314719825983, 0.32699806708842516, 0.3275275188498199, 0.32670374400913715, 0.32689414685592055, 0.3269361569546163, 0.3301063561812043, 0.32969524315558374, 0.33089840109460056, 0.3305910697672516, 0.330386454006657, 0.33054715185426176, 0.3309052810072899, 0.30592633970081806, 0.30687447101809084, 0.30616434500552714, 0.30684398603625596, 0.30575073789805174, 0.3261786983348429, 0.3086753652896732, 0.3067783582955599, 0.30809291871264577, 0.31137209106236696, 0.3106042533181608, 0.3270843871869147, 0.3104193601757288, 0.3112579919397831, 0.3153560140635818, 0.32329855021089315, 0.3117400442715734, 0.3258371748961508, 0.33274576673284173, 0.33036002796143293, 0.33259448898024857, 0.3333111978136003, 0.31096420995891094, 0.3310067963320762, 0.32965138205327094, 0.33381410990841687, 0.332284013973549, 0.3328550811856985, 0.33282079990021884, 0.33336904901079834, 0.3334335279650986, 0.3319808018859476, 0.3318402653094381, 0.3327841842547059, 0.3352393328677863, 0.33721655793488026, 0.33724820986390114, 0.38142893882468343, 0.3060129501391202, 0.3069077779073268, 0.3100948752835393, 0.31386516708880663, 0.3528040263336152, 0.3069943939335644, 0.3089013078715652, 0.3142410721629858, 0.3140071697998792, 0.3461838229559362, 0.3071037530899048, 0.30703528388403356, 0.3098845072090626, 0.30851562530733645, 0.32650900608859956, 0.30575261707417667, 0.306862497003749]
Total Epoch List: [58, 49]
Total Time List: [0.08557850285433233, 0.086577408015728]
========================k_idx:2========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: SAMamba
GraphEncoder(
  (subgraph_conv): GATConv(128, 128, heads=1)
  (embedding): Linear(in_features=14887, out_features=128, bias=False)
  (encoder): GraphMambaEncoder(
    (layers): ModuleList(
      (0-1): 2 x MambaEncoderLayer(
        (self_selen): Selection(
          (khop_structure_extractor): KHopStructureExtractor(
            (structure_extractor): StructureExtractor(
              (gcn): ModuleList(
                (0): GATConv(128, 128, heads=1)
              )
              (relu): ReLU()
              (bn): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (out_proj): Linear(in_features=128, out_features=128, bias=True)
            )
            (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (out_proj): Linear(in_features=256, out_features=128, bias=True)
          )
          (out_proj): Mamba(
            (in_proj): Linear(in_features=128, out_features=512, bias=False)
            (conv1d): Conv1d(256, 256, kernel_size=(4,), stride=(1,), padding=(3,), groups=256)
            (act): SiLU()
            (x_proj): Linear(in_features=256, out_features=40, bias=False)
            (dt_proj): Linear(in_features=8, out_features=256, bias=True)
            (out_proj): Linear(in_features=256, out_features=128, bias=False)
          )
        )
        (norm1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (linear1): Linear(in_features=128, out_features=256, bias=True)
        (dropout): Dropout(p=0.6, inplace=False)
        (linear2): Linear(in_features=256, out_features=128, bias=True)
        (dropout1): Dropout(p=0.6, inplace=False)
        (dropout2): Dropout(p=0.6, inplace=False)
        (activation): ReLU()
      )
    )
  )
  (classifier): Sequential(
    (0): Linear(in_features=128, out_features=128, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=128, out_features=2, bias=True)
  )
)
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x75fef9a56260>
Training...
Epoch 1/1000, LR 0.000500
Train loss: 0.7014;  Loss pred: 0.6860; Loss self: 1.5409; time: 0.16s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.7009 score: 0.4898 time: 0.11s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6988 score: 0.5000 time: 0.08s
Epoch 2/1000, LR 0.000000
Train loss: 0.7006;  Loss pred: 0.6853; Loss self: 1.5299; time: 0.15s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6993 score: 0.4898 time: 0.09s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6974 score: 0.5000 time: 0.08s
Epoch 3/1000, LR 0.000050
Train loss: 0.6912;  Loss pred: 0.6762; Loss self: 1.4927; time: 0.16s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6980 score: 0.4898 time: 0.09s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6963 score: 0.5000 time: 0.08s
Epoch 4/1000, LR 0.000100
Train loss: 0.6572;  Loss pred: 0.6421; Loss self: 1.5089; time: 0.16s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6978 score: 0.4898 time: 0.09s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6961 score: 0.5000 time: 0.07s
Epoch 5/1000, LR 0.000150
Train loss: 0.6327;  Loss pred: 0.6192; Loss self: 1.3517; time: 0.17s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6975 score: 0.4898 time: 0.09s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6958 score: 0.5000 time: 0.07s
Epoch 6/1000, LR 0.000200
Train loss: 0.5858;  Loss pred: 0.5723; Loss self: 1.3557; time: 0.15s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6974 score: 0.4898 time: 0.15s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6957 score: 0.5000 time: 0.07s
Epoch 7/1000, LR 0.000250
Train loss: 0.5226;  Loss pred: 0.5085; Loss self: 1.4057; time: 0.16s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6973 score: 0.4898 time: 0.09s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6954 score: 0.5000 time: 0.07s
Epoch 8/1000, LR 0.000300
Train loss: 0.4716;  Loss pred: 0.4575; Loss self: 1.4154; time: 0.14s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6970 score: 0.4898 time: 0.09s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6949 score: 0.5000 time: 0.07s
Epoch 9/1000, LR 0.000350
Train loss: 0.4158;  Loss pred: 0.4009; Loss self: 1.4934; time: 0.14s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6963 score: 0.4898 time: 0.09s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6940 score: 0.5000 time: 0.07s
Epoch 10/1000, LR 0.000400
Train loss: 0.3649;  Loss pred: 0.3488; Loss self: 1.6146; time: 0.16s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6950 score: 0.4898 time: 0.09s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6925 score: 0.5000 time: 0.07s
Epoch 11/1000, LR 0.000450
Train loss: 0.3166;  Loss pred: 0.2988; Loss self: 1.7811; time: 0.15s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6930 score: 0.4898 time: 0.16s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6902 score: 0.5000 time: 0.08s
Epoch 12/1000, LR 0.000450
Train loss: 0.2873;  Loss pred: 0.2685; Loss self: 1.8847; time: 0.15s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6898 score: 0.4898 time: 0.09s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6867 score: 0.5000 time: 0.08s
Epoch 13/1000, LR 0.000450
Train loss: 0.2507;  Loss pred: 0.2309; Loss self: 1.9830; time: 0.15s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6851 score: 0.4898 time: 0.09s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6816 score: 0.5000 time: 0.08s
Epoch 14/1000, LR 0.000450
Train loss: 0.2277;  Loss pred: 0.2074; Loss self: 2.0369; time: 0.17s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6780 score: 0.4898 time: 0.10s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6740 score: 0.5000 time: 0.08s
Epoch 15/1000, LR 0.000450
Train loss: 0.1980;  Loss pred: 0.1766; Loss self: 2.1386; time: 0.15s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6675 score: 0.4898 time: 0.09s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6630 score: 0.5000 time: 0.08s
Epoch 16/1000, LR 0.000450
Train loss: 0.1737;  Loss pred: 0.1518; Loss self: 2.1929; time: 0.23s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6525 score: 0.4898 time: 0.09s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6474 score: 0.5000 time: 0.07s
Epoch 17/1000, LR 0.000450
Train loss: 0.1563;  Loss pred: 0.1339; Loss self: 2.2375; time: 0.14s
Val loss: 0.6323 score: 0.6735 time: 0.09s
Test loss: 0.6267 score: 0.7708 time: 0.07s
Epoch 18/1000, LR 0.000450
Train loss: 0.1363;  Loss pred: 0.1133; Loss self: 2.3012; time: 0.14s
Val loss: 0.6059 score: 0.8980 time: 0.09s
Test loss: 0.5993 score: 0.9375 time: 0.07s
Epoch 19/1000, LR 0.000450
Train loss: 0.1233;  Loss pred: 0.1000; Loss self: 2.3298; time: 0.15s
Val loss: 0.5722 score: 0.9184 time: 0.09s
Test loss: 0.5652 score: 0.9583 time: 0.07s
Epoch 20/1000, LR 0.000450
Train loss: 0.1044;  Loss pred: 0.0806; Loss self: 2.3755; time: 0.15s
Val loss: 0.5309 score: 0.9184 time: 0.09s
Test loss: 0.5238 score: 0.9167 time: 0.15s
Epoch 21/1000, LR 0.000450
Train loss: 0.0921;  Loss pred: 0.0679; Loss self: 2.4199; time: 0.14s
Val loss: 0.4828 score: 0.9388 time: 0.09s
Test loss: 0.4756 score: 0.9167 time: 0.07s
Epoch 22/1000, LR 0.000450
Train loss: 0.0790;  Loss pred: 0.0546; Loss self: 2.4367; time: 0.14s
Val loss: 0.4283 score: 0.9388 time: 0.09s
Test loss: 0.4216 score: 0.9167 time: 0.07s
Epoch 23/1000, LR 0.000450
Train loss: 0.0717;  Loss pred: 0.0469; Loss self: 2.4854; time: 0.15s
Val loss: 0.3722 score: 0.9388 time: 0.09s
Test loss: 0.3665 score: 0.9167 time: 0.08s
Epoch 24/1000, LR 0.000450
Train loss: 0.0633;  Loss pred: 0.0377; Loss self: 2.5585; time: 0.16s
Val loss: 0.3184 score: 0.9388 time: 0.09s
Test loss: 0.3138 score: 0.9167 time: 0.07s
Epoch 25/1000, LR 0.000450
Train loss: 0.0572;  Loss pred: 0.0316; Loss self: 2.5603; time: 0.15s
Val loss: 0.2719 score: 0.9388 time: 0.17s
Test loss: 0.2683 score: 0.9167 time: 0.07s
Epoch 26/1000, LR 0.000450
Train loss: 0.0510;  Loss pred: 0.0250; Loss self: 2.6008; time: 0.16s
Val loss: 0.2359 score: 0.9388 time: 0.09s
Test loss: 0.2317 score: 0.9167 time: 0.07s
Epoch 27/1000, LR 0.000450
Train loss: 0.0466;  Loss pred: 0.0203; Loss self: 2.6368; time: 0.16s
Val loss: 0.2122 score: 0.9388 time: 0.09s
Test loss: 0.2055 score: 0.9167 time: 0.07s
Epoch 28/1000, LR 0.000450
Train loss: 0.0435;  Loss pred: 0.0169; Loss self: 2.6554; time: 0.16s
Val loss: 0.2006 score: 0.9184 time: 0.09s
Test loss: 0.1875 score: 0.9167 time: 0.07s
Epoch 29/1000, LR 0.000450
Train loss: 0.0405;  Loss pred: 0.0137; Loss self: 2.6806; time: 0.16s
Val loss: 0.1998 score: 0.9184 time: 0.09s
Test loss: 0.1761 score: 0.9375 time: 0.07s
Epoch 30/1000, LR 0.000450
Train loss: 0.0386;  Loss pred: 0.0115; Loss self: 2.7120; time: 0.16s
Val loss: 0.2089 score: 0.9388 time: 0.09s
Test loss: 0.1710 score: 0.9375 time: 0.07s
     INFO: Early stopping counter 1 of 20
Epoch 31/1000, LR 0.000450
Train loss: 0.0362;  Loss pred: 0.0094; Loss self: 2.6791; time: 0.17s
Val loss: 0.2263 score: 0.9388 time: 0.09s
Test loss: 0.1715 score: 0.9167 time: 0.07s
     INFO: Early stopping counter 2 of 20
Epoch 32/1000, LR 0.000450
Train loss: 0.0347;  Loss pred: 0.0076; Loss self: 2.7079; time: 0.17s
Val loss: 0.2506 score: 0.9388 time: 0.09s
Test loss: 0.1774 score: 0.9375 time: 0.07s
     INFO: Early stopping counter 3 of 20
Epoch 33/1000, LR 0.000449
Train loss: 0.0335;  Loss pred: 0.0065; Loss self: 2.6989; time: 0.17s
Val loss: 0.2793 score: 0.9184 time: 0.09s
Test loss: 0.1866 score: 0.9375 time: 0.07s
     INFO: Early stopping counter 4 of 20
Epoch 34/1000, LR 0.000449
Train loss: 0.0327;  Loss pred: 0.0055; Loss self: 2.7197; time: 0.17s
Val loss: 0.3113 score: 0.9184 time: 0.09s
Test loss: 0.1982 score: 0.9375 time: 0.07s
     INFO: Early stopping counter 5 of 20
Epoch 35/1000, LR 0.000449
Train loss: 0.0318;  Loss pred: 0.0046; Loss self: 2.7128; time: 0.17s
Val loss: 0.3459 score: 0.9184 time: 0.09s
Test loss: 0.2116 score: 0.9375 time: 0.07s
     INFO: Early stopping counter 6 of 20
Epoch 36/1000, LR 0.000449
Train loss: 0.0315;  Loss pred: 0.0041; Loss self: 2.7385; time: 0.17s
Val loss: 0.3800 score: 0.9184 time: 0.09s
Test loss: 0.2259 score: 0.9583 time: 0.07s
     INFO: Early stopping counter 7 of 20
Epoch 37/1000, LR 0.000449
Train loss: 0.0308;  Loss pred: 0.0038; Loss self: 2.7065; time: 0.17s
Val loss: 0.4095 score: 0.9184 time: 0.09s
Test loss: 0.2394 score: 0.9583 time: 0.07s
     INFO: Early stopping counter 8 of 20
Epoch 38/1000, LR 0.000449
Train loss: 0.0304;  Loss pred: 0.0033; Loss self: 2.7150; time: 0.17s
Val loss: 0.4349 score: 0.9184 time: 0.09s
Test loss: 0.2517 score: 0.9583 time: 0.07s
     INFO: Early stopping counter 9 of 20
Epoch 39/1000, LR 0.000449
Train loss: 0.0298;  Loss pred: 0.0028; Loss self: 2.6959; time: 0.17s
Val loss: 0.4548 score: 0.9184 time: 0.09s
Test loss: 0.2617 score: 0.9583 time: 0.07s
     INFO: Early stopping counter 10 of 20
Epoch 40/1000, LR 0.000449
Train loss: 0.0294;  Loss pred: 0.0023; Loss self: 2.7098; time: 0.17s
Val loss: 0.4706 score: 0.9184 time: 0.09s
Test loss: 0.2698 score: 0.9583 time: 0.07s
     INFO: Early stopping counter 11 of 20
Epoch 41/1000, LR 0.000449
Train loss: 0.0292;  Loss pred: 0.0021; Loss self: 2.7086; time: 0.17s
Val loss: 0.4823 score: 0.9184 time: 0.09s
Test loss: 0.2760 score: 0.9583 time: 0.07s
     INFO: Early stopping counter 12 of 20
Epoch 42/1000, LR 0.000449
Train loss: 0.0289;  Loss pred: 0.0020; Loss self: 2.6933; time: 0.17s
Val loss: 0.4876 score: 0.9184 time: 0.09s
Test loss: 0.2787 score: 0.9583 time: 0.07s
     INFO: Early stopping counter 13 of 20
Epoch 43/1000, LR 0.000449
Train loss: 0.0289;  Loss pred: 0.0017; Loss self: 2.7136; time: 0.16s
Val loss: 0.4914 score: 0.9184 time: 0.09s
Test loss: 0.2804 score: 0.9583 time: 0.07s
     INFO: Early stopping counter 14 of 20
Epoch 44/1000, LR 0.000449
Train loss: 0.0286;  Loss pred: 0.0016; Loss self: 2.6940; time: 0.17s
Val loss: 0.4950 score: 0.9184 time: 0.09s
Test loss: 0.2821 score: 0.9583 time: 0.07s
     INFO: Early stopping counter 15 of 20
Epoch 45/1000, LR 0.000449
Train loss: 0.0283;  Loss pred: 0.0014; Loss self: 2.6947; time: 0.17s
Val loss: 0.4989 score: 0.9184 time: 0.09s
Test loss: 0.2835 score: 0.9583 time: 0.07s
     INFO: Early stopping counter 16 of 20
Epoch 46/1000, LR 0.000449
Train loss: 0.0280;  Loss pred: 0.0014; Loss self: 2.6676; time: 0.17s
Val loss: 0.5008 score: 0.9184 time: 0.09s
Test loss: 0.2838 score: 0.9583 time: 0.07s
     INFO: Early stopping counter 17 of 20
Epoch 47/1000, LR 0.000449
Train loss: 0.0278;  Loss pred: 0.0012; Loss self: 2.6546; time: 0.17s
Val loss: 0.5039 score: 0.9184 time: 0.09s
Test loss: 0.2847 score: 0.9583 time: 0.07s
     INFO: Early stopping counter 18 of 20
Epoch 48/1000, LR 0.000448
Train loss: 0.0276;  Loss pred: 0.0012; Loss self: 2.6424; time: 0.17s
Val loss: 0.5081 score: 0.9184 time: 0.09s
Test loss: 0.2861 score: 0.9583 time: 0.07s
     INFO: Early stopping counter 19 of 20
Epoch 49/1000, LR 0.000448
Train loss: 0.0273;  Loss pred: 0.0011; Loss self: 2.6178; time: 0.17s
Val loss: 0.5123 score: 0.9184 time: 0.09s
Test loss: 0.2875 score: 0.9583 time: 0.07s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 028,   Train_Loss: 0.0405,   Val_Loss: 0.1998,   Val_Precision: 0.9565,   Val_Recall: 0.8800,   Val_accuracy: 0.9167,   Val_Score: 0.9184,   Val_Loss: 0.1998,   Test_Precision: 0.9200,   Test_Recall: 0.9583,   Test_accuracy: 0.9388,   Test_Score: 0.9375,   Test_loss: 0.1761


[0.08580371108837426, 0.08559617307037115, 0.08592875790782273, 0.08626094507053494, 0.08614550600759685, 0.08681278699077666, 0.0860787290148437, 0.08620444010011852, 0.08587743295356631, 0.08605238096788526, 0.08610205189324915, 0.08630912005901337, 0.08626337302848697, 0.08598034596070647, 0.0860396979842335, 0.08584887394681573, 0.08581978804431856, 0.08637914806604385, 0.08632824500091374, 0.08596233814023435, 0.08590189181268215, 0.08603417593985796, 0.08594518899917603, 0.08588272100314498, 0.08609863882884383, 0.08555595600046217, 0.08637480111792684, 0.08567612594924867, 0.08558370498940349, 0.08524416200816631, 0.08557071094401181, 0.08606824185699224, 0.08595852693542838, 0.08524036291055381, 0.08584264502860606, 0.08524847589433193, 0.0855790339410305, 0.08498359611257911, 0.08505651890300214, 0.08519502216950059, 0.08521903003565967, 0.08525734418071806, 0.08482111897319555, 0.0850494490005076, 0.08503626892343163, 0.08515079086646438, 0.08502391492947936, 0.08566791797056794, 0.08538781804963946, 0.08548118197359145, 0.08513387897983193, 0.08533355197869241, 0.08495814795605838, 0.08526995894499123, 0.0855287229642272, 0.08541887602768838, 0.08551693009212613, 0.08513761078938842, 0.08583107613958418, 0.08547529089264572, 0.08551193284802139, 0.08748067799024284, 0.08692800905555487, 0.08626890601590276, 0.08635392086580396, 0.08646225486882031, 0.08716893987730145, 0.08626827807165682, 0.08709513093344867, 0.08692665188573301, 0.08849932299926877, 0.08674690802581608, 0.0864890799857676, 0.0869831359013915, 0.08649095706641674, 0.08669689204543829, 0.08601255901157856, 0.08693224401213229, 0.0861535940784961, 0.08675257302820683, 0.08676377800293267, 0.08634543605148792, 0.08690050314180553, 0.08687796490266919, 0.08602272509597242, 0.08624989003874362, 0.08634370914660394, 0.08807747811079025, 0.08704757294617593, 0.08623319398611784, 0.08590183896012604, 0.08598419604822993, 0.08682262990623713, 0.08717725402675569, 0.08572583086788654, 0.08592675719410181, 0.08587830304168165, 0.08665771689265966, 0.08635970088653266, 0.08862718380987644, 0.08539445116184652, 0.08592587802559137, 0.08586181094869971, 0.08537112618796527, 0.08494824706576765, 0.08513565501198173, 0.08603243017569184, 0.08124127495102584, 0.0815703528933227, 0.08252382487989962, 0.0792602701112628, 0.07865276793017983, 0.07765493891201913, 0.0776006227824837, 0.07787716411985457, 0.07969522499479353, 0.07937845098786056, 0.08220772910863161, 0.08263612003065646, 0.08317334484308958, 0.08394425199367106, 0.08539955201558769, 0.07862291391938925, 0.07825759518891573, 0.07982540503144264, 0.07952191005460918, 0.15128219686448574, 0.07825381285510957, 0.07836164510808885, 0.08065845002420247, 0.0797471480909735, 0.07943043811246753, 0.07889196602627635, 0.07973372703418136, 0.07924151397310197, 0.07949316408485174, 0.07970607816241682, 0.07981578796170652, 0.08003629208542407, 0.08009510301053524, 0.07960016187280416, 0.07920719403773546, 0.07926435209810734, 0.07967241294682026, 0.07940240204334259, 0.07969184988178313, 0.07951295585371554, 0.07927143899723887, 0.07963151019066572, 0.07944539585150778, 0.0795232190284878, 0.07979165087454021, 0.07948378403671086, 0.07960700592957437, 0.07968349079601467, 0.07956193899735808]
[0.0017510961446606992, 0.0017468606749055336, 0.0017536481205678107, 0.0017604274504190805, 0.001758071551175446, 0.0017716895304240134, 0.0017567087554049734, 0.0017592742877575208, 0.0017526006725217616, 0.0017561710401609236, 0.0017571847325152888, 0.0017614106134492525, 0.0017604770005813666, 0.0017547009379736014, 0.0017559122037598674, 0.0017520178356493007, 0.0017514242458024196, 0.0017628397564498745, 0.0017618009183859946, 0.0017543334314333542, 0.0017530998329118807, 0.001755799508976693, 0.001753983448962776, 0.001752708591900918, 0.0017571150781396701, 0.0017460399183767791, 0.0017627510432229967, 0.0017484923663111974, 0.0017466062242735404, 0.0017396767756768636, 0.0017463410396737103, 0.0017564947317753518, 0.0017542556517434363, 0.0017395992430725268, 0.0017518907148695113, 0.0017397648141700395, 0.0017465108967557245, 0.0017343591043383491, 0.0017358473245510642, 0.0017386739218265427, 0.0017391638782787689, 0.0017399457996064912, 0.0017310432443509297, 0.0017357030408266857, 0.0017354340596618702, 0.0017377712421727423, 0.0017351819373363135, 0.001748324856542203, 0.001742608531625295, 0.0017445139178283969, 0.001737426101629223, 0.001741501060789641, 0.0017338397542052731, 0.0017402032437753311, 0.0017454841421270858, 0.0017432423679120078, 0.0017452434712678802, 0.001737502261007927, 0.0017516546150935547, 0.0017443936916866473, 0.001745141486694314, 0.0017853199589845476, 0.0017740410011337728, 0.001760589918691893, 0.0017623249156286521, 0.001764535813649394, 0.0017789579566796214, 0.0017605771035032005, 0.0017774516517030342, 0.0017740133037904696, 0.0018061086326381381, 0.0017703450617513486, 0.0017650832650156654, 0.0017751660388039083, 0.001765121572784015, 0.0017693243274579243, 0.0017553583471750726, 0.0017741274288190262, 0.0017582366138468592, 0.0017704606740450372, 0.0017706893469986258, 0.0017621517561528148, 0.001773479655955215, 0.001773019691891208, 0.0017555658182851514, 0.0017602018375253799, 0.001762116513195999, 0.0017974995532814338, 0.0017764810805342027, 0.0017598611017575069, 0.0017530987542882866, 0.0017547795111883659, 0.0017718904062497373, 0.0017791276331990957, 0.0017495067524058478, 0.0017536072896755471, 0.0017526184294220743, 0.0017685248345440747, 0.0017624428752353604, 0.001808718036936254, 0.0017427439012621738, 0.0017535893474610485, 0.0017522818560959125, 0.0017422678813870465, 0.0017336376952197478, 0.0017374623471833005, 0.0017557638811365683, 0.0016925265614797051, 0.001699382351944223, 0.0017192463516645755, 0.001651255627317975, 0.0016385993318787466, 0.001617811227333732, 0.0016166796413017437, 0.0016224409191636369, 0.001660317187391532, 0.0016537177289137617, 0.0017126610230964918, 0.0017215858339720096, 0.0017327780175643663, 0.0017488385832014803, 0.0017791573336580768, 0.0016379773733206093, 0.0016303665664357443, 0.0016630292714883883, 0.0016567064594710246, 0.0031517124346767864, 0.0016302877678147827, 0.0016325342730851844, 0.001680384375504218, 0.0016613989185619478, 0.0016548007940097402, 0.001643582625547424, 0.0016611193132121116, 0.0016508648744396244, 0.001656107585101078, 0.0016605432950503503, 0.0016628289158688858, 0.001667422751779668, 0.0016686479793861508, 0.0016583367056834202, 0.0016501498757861555, 0.0016513406687105696, 0.0016598419363920887, 0.001654216709236304, 0.0016602468725371484, 0.0016565199136190738, 0.0016514883124424766, 0.0016589897956388693, 0.0016551124135730788, 0.0016567337297601625, 0.0016623260598862544, 0.0016559121674314763, 0.001658479290199466, 0.0016600727249169722, 0.0016575403957782935]
[571.0708706938333, 572.4554993798105, 570.239826491652, 568.043857622161, 568.8050633271441, 564.4329792707376, 569.2463232298687, 568.4161969278034, 570.5806323588429, 569.4206185682044, 569.0921287305797, 567.7267937211795, 568.0278695318186, 569.8976836217115, 569.5045560129593, 570.7704451703818, 570.9638897581023, 567.266534772206, 567.6010209576409, 570.017068638408, 570.4181708459868, 569.5411092709642, 570.1308074436811, 570.5455000454125, 569.11468829847, 572.7245920755687, 567.2950833553954, 571.9212844547355, 572.5388963479311, 574.8194227694543, 572.6258372687856, 569.3156841918133, 570.0423418936501, 574.8450420303548, 570.8118614433574, 574.7903347942195, 572.5701464889656, 576.581860987489, 576.087531349352, 575.1509742260712, 574.9889429567091, 574.7305463343522, 577.686319081508, 576.1354197568941, 576.2247170571487, 575.4497345402585, 576.30844263807, 571.9760811373335, 573.852349424297, 573.2255786441752, 575.5640479110322, 574.2172787116044, 576.7545689124901, 574.6455211924113, 572.9069522117674, 573.6436988952738, 572.9859566662767, 575.5388193969308, 570.8887993005349, 573.2650861819524, 573.0194414747554, 560.123688175636, 563.6848299227072, 567.9914382010057, 567.4322544791818, 566.7212828805161, 562.126831747319, 567.9955725938942, 562.6032072612876, 563.6936306302418, 553.6765518579714, 564.8616315571442, 566.5455108097265, 563.3275863444253, 566.5332152859947, 565.1875037725556, 569.6842480108501, 563.6573696770274, 568.7516640960471, 564.8247457060218, 564.7518022825582, 567.4880137356794, 563.8632485250525, 564.0095282491424, 569.61692326455, 568.1166663283757, 567.4993636977346, 556.3283719177817, 562.9105825879618, 568.2266623208716, 570.4185218053929, 569.8721654909131, 564.36899058364, 562.0732213584216, 571.5896772760907, 570.2531039232964, 570.5748514408523, 565.4430067745132, 567.394276462128, 552.8777728638551, 573.8077747830619, 570.2589385866536, 570.684445839097, 573.9645497016708, 576.8217908259342, 575.5520409528052, 569.5526663600486, 590.8326774651866, 588.449090845226, 581.6502091348336, 605.5997529735803, 610.2773146217774, 618.1190877554183, 618.5517368146012, 616.3552633494332, 602.2945540731685, 604.6981189811917, 583.8867040904565, 580.8597981390328, 577.107967589307, 571.808061421751, 562.0638383587624, 610.5090438293039, 613.3589958153817, 601.3123263338676, 603.6072318564469, 317.28783025934655, 613.3886420189409, 612.5445673555062, 595.1019389239077, 601.9024021428683, 604.3023448018203, 608.4269719430335, 602.0037164376212, 605.7430959268811, 603.8255056593841, 602.2125427146291, 601.3847789491111, 599.727932782903, 599.2875743437943, 603.0138491012223, 606.0055602668124, 605.5685655588187, 602.4670048846021, 604.5157169653221, 602.3200624807229, 603.6752059413852, 605.5144274808977, 602.776462295782, 604.1885685825935, 603.5972963167504, 601.5666986947348, 603.8967643743588, 602.9620061639296, 602.3832480290973, 603.3035469584756]
Elapsed: 0.08456083498775768~0.006148757885710311
Time per graph: 0.001736595353081688~0.00012310239510896867
Speed: 577.6502413688725~26.480273471955563
Total Time: 0.0802
best val loss: 0.1998114287853241 test_score: 0.9375

Testing...
Test loss: 0.4756 score: 0.9167 time: 0.07s
test Score 0.9167
Epoch Time List: [0.333628183696419, 0.3288253550417721, 0.3291889021638781, 0.3303900118917227, 0.3306734939105809, 0.3313491251319647, 0.3295870379079133, 0.3298175421077758, 0.3304097573272884, 0.3300056210719049, 0.3317355290055275, 0.30912051000632346, 0.30887144268490374, 0.30891769216395915, 0.3088087949436158, 0.30880032107234, 0.3091954877600074, 0.30884792492724955, 0.3086675510276109, 0.32194894598796964, 0.32809953205287457, 0.3301270720548928, 0.3297930150292814, 0.330932242795825, 0.33045038883574307, 0.3298097951337695, 0.3298252720851451, 0.33012558380141854, 0.3311631972901523, 0.33190330816432834, 0.3304620000999421, 0.32983937906101346, 0.3324849351774901, 0.33228532667271793, 0.33306984207592905, 0.3298439681529999, 0.3300067908130586, 0.33487692079506814, 0.3266614768654108, 0.32715314719825983, 0.32699806708842516, 0.3275275188498199, 0.32670374400913715, 0.32689414685592055, 0.3269361569546163, 0.3301063561812043, 0.32969524315558374, 0.33089840109460056, 0.3305910697672516, 0.330386454006657, 0.33054715185426176, 0.3309052810072899, 0.30592633970081806, 0.30687447101809084, 0.30616434500552714, 0.30684398603625596, 0.30575073789805174, 0.3261786983348429, 0.3086753652896732, 0.3067783582955599, 0.30809291871264577, 0.31137209106236696, 0.3106042533181608, 0.3270843871869147, 0.3104193601757288, 0.3112579919397831, 0.3153560140635818, 0.32329855021089315, 0.3117400442715734, 0.3258371748961508, 0.33274576673284173, 0.33036002796143293, 0.33259448898024857, 0.3333111978136003, 0.31096420995891094, 0.3310067963320762, 0.32965138205327094, 0.33381410990841687, 0.332284013973549, 0.3328550811856985, 0.33282079990021884, 0.33336904901079834, 0.3334335279650986, 0.3319808018859476, 0.3318402653094381, 0.3327841842547059, 0.3352393328677863, 0.33721655793488026, 0.33724820986390114, 0.38142893882468343, 0.3060129501391202, 0.3069077779073268, 0.3100948752835393, 0.31386516708880663, 0.3528040263336152, 0.3069943939335644, 0.3089013078715652, 0.3142410721629858, 0.3140071697998792, 0.3461838229559362, 0.3071037530899048, 0.30703528388403356, 0.3098845072090626, 0.30851562530733645, 0.32650900608859956, 0.30575261707417667, 0.306862497003749, 0.34884023922495544, 0.31308258906938136, 0.3297974767629057, 0.32640226907096803, 0.3265918171964586, 0.37035741889849305, 0.3187594679184258, 0.30206463718786836, 0.30637878319248557, 0.3253776989877224, 0.38723295414820313, 0.31392364110797644, 0.3166196399834007, 0.3400046278256923, 0.31921309093013406, 0.39443470421247184, 0.304211697075516, 0.3068012308795005, 0.3104606522247195, 0.38216864597052336, 0.30469109141267836, 0.30275547388009727, 0.3097286152187735, 0.3216427380684763, 0.39743157918564975, 0.3193117461632937, 0.3204846531152725, 0.322209884878248, 0.3250025762245059, 0.32662088330835104, 0.32999635487794876, 0.3299519510474056, 0.33107117493636906, 0.33050714107230306, 0.3285032131243497, 0.32797859073616564, 0.3292605059687048, 0.3278876719996333, 0.3294787590857595, 0.3307881548535079, 0.3290166810620576, 0.3311333090532571, 0.32702921028248966, 0.33187362272292376, 0.3303466166835278, 0.33102036570198834, 0.3320466938894242, 0.33298817626200616, 0.33400801196694374]
Total Epoch List: [58, 49, 49]
Total Time List: [0.08557850285433233, 0.086577408015728, 0.08019317802973092]
T-times Epoch Time: 0.33939698396114376 ~ 0.016838456589539856
T-times Total Epoch: 49.888888888888886 ~ 1.7708197167232484
T-times Total Time: 0.08703128207061027 ~ 0.0037561731988283646
T-times Inference Elapsed: 0.08854367160866444 ~ 0.004076285770498989
T-times Time Per Graph: 0.0018191410132580882 ~ 8.393686235726852e-05
T-times Speed: 555.9422785537092 ~ 23.102446337090704
T-times cross validation test micro f1 score:0.9229804226467881 ~ 0.02321765214495085
T-times cross validation test precision:0.962121212121212 ~ 0.016521868710875888
T-times cross validation test recall:0.8911111111111111 ~ 0.0557939330300613
T-times cross validation test f1_score:0.9229804226467881 ~ 0.026710623907831008
