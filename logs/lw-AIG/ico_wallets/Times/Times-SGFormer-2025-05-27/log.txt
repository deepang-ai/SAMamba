Namespace(seed=15, model='SGFormer', dataset='ico_wallets/Times', num_heads=8, num_layers=2, dim_hidden=64, dropout=0.1, epochs=1000, lr=0.0003, weight_decay=0.0001, batch_size=64, abs_pe=None, abs_pe_dim=20, num_class=2, outdir='./outdir/ico_wallets/Times/seed15/khopgnn_gat_1_0.1_0.0003_0.0001_2_8_64_BN', warmup=10, layer_norm=False, use_edge_attr=False, edge_dim=128, gnn_type='gat', k_hop=1, global_pool='mean', se='khopgnn', ie='mamba', aggr='add', not_extract_node_feature=False, profile=False, k_ford=3, early_stop=1, early_stop_mindelta=-0.0, patience=20, training_times=3, Lambda=0.01, order='sd', use_cuda=True, batch_norm=True, save_logs=True)
Data(edge_index=[2, 338], edge_attr=[338, 2], x=[122, 14887], y=[1, 1], num_nodes=122)
Data(edge_index=[2, 338], edge_attr=[338, 2], x=[122, 14887], y=[1, 1], num_nodes=122)
Data(edge_index=[2, 298], edge_attr=[298, 2], x=[109, 14887], y=[1, 1], num_nodes=122)
========================training times:0========================
========================k_idx:0========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: SGFormer
SGFormer(
  (trans_conv): SGModule(
    (attns): ModuleList(
      (0-1): 2 x SGFormerAttention(heads=1, head_channels=64)
    )
    (fcs): ModuleList(
      (0): Linear(in_features=14887, out_features=64, bias=True)
    )
    (bns): ModuleList(
      (0-2): 3 x LayerNorm((64,), eps=1e-05, elementwise_affine=True)
    )
  )
  (graph_conv): GraphModule(
    (convs): ModuleList(
      (0-2): 3 x GCNConv(64, 64)
    )
    (fcs): ModuleList(
      (0): Linear(in_features=14887, out_features=64, bias=True)
    )
    (bns): ModuleList(
      (0-3): 4 x BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (classifier): Linear(in_features=64, out_features=2, bias=True)
)
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7e09d883b5b0>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.6984;  Loss pred: 0.6984; Loss self: 0.0000; time: 1.25s
Val loss: 0.7039 score: 0.3469 time: 0.11s
Test loss: 0.7036 score: 0.4286 time: 0.09s
Epoch 2/1000, LR 0.000000
Train loss: 0.7128;  Loss pred: 0.7128; Loss self: 0.0000; time: 0.17s
Val loss: 0.7032 score: 0.3469 time: 0.11s
Test loss: 0.7029 score: 0.4490 time: 0.09s
Epoch 3/1000, LR 0.000030
Train loss: 0.6942;  Loss pred: 0.6942; Loss self: 0.0000; time: 0.17s
Val loss: 0.7016 score: 0.4082 time: 0.11s
Test loss: 0.7012 score: 0.5102 time: 0.09s
Epoch 4/1000, LR 0.000060
Train loss: 0.6971;  Loss pred: 0.6971; Loss self: 0.0000; time: 0.18s
Val loss: 0.6991 score: 0.4490 time: 0.11s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6988 score: 0.5102 time: 0.10s
Epoch 5/1000, LR 0.000090
Train loss: 0.6787;  Loss pred: 0.6787; Loss self: 0.0000; time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6959 score: 0.4898 time: 0.11s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6957 score: 0.5102 time: 0.09s
Epoch 6/1000, LR 0.000120
Train loss: 0.6574;  Loss pred: 0.6574; Loss self: 0.0000; time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6925 score: 0.4898 time: 0.11s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6925 score: 0.5102 time: 0.22s
Epoch 7/1000, LR 0.000150
Train loss: 0.6530;  Loss pred: 0.6530; Loss self: 0.0000; time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6895 score: 0.4898 time: 0.11s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6896 score: 0.5102 time: 0.09s
Epoch 8/1000, LR 0.000180
Train loss: 0.6808;  Loss pred: 0.6808; Loss self: 0.0000; time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6860 score: 0.4898 time: 0.10s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6863 score: 0.5102 time: 0.09s
Epoch 9/1000, LR 0.000210
Train loss: 0.6410;  Loss pred: 0.6410; Loss self: 0.0000; time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6825 score: 0.4898 time: 0.10s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6832 score: 0.5102 time: 0.10s
Epoch 10/1000, LR 0.000240
Train loss: 0.6308;  Loss pred: 0.6308; Loss self: 0.0000; time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6791 score: 0.4898 time: 0.10s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6802 score: 0.5102 time: 0.09s
Epoch 11/1000, LR 0.000270
Train loss: 0.5778;  Loss pred: 0.5778; Loss self: 0.0000; time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6770 score: 0.4898 time: 0.10s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6784 score: 0.5102 time: 0.09s
Epoch 12/1000, LR 0.000270
Train loss: 0.5675;  Loss pred: 0.5675; Loss self: 0.0000; time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6755 score: 0.4898 time: 0.11s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6773 score: 0.5102 time: 0.09s
Epoch 13/1000, LR 0.000270
Train loss: 0.5923;  Loss pred: 0.5923; Loss self: 0.0000; time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6745 score: 0.4898 time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6765 score: 0.5102 time: 0.10s
Epoch 14/1000, LR 0.000270
Train loss: 0.5696;  Loss pred: 0.5696; Loss self: 0.0000; time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6742 score: 0.4898 time: 0.10s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6764 score: 0.5102 time: 0.08s
Epoch 15/1000, LR 0.000270
Train loss: 0.5329;  Loss pred: 0.5329; Loss self: 0.0000; time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6736 score: 0.4898 time: 0.10s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6761 score: 0.5102 time: 0.08s
Epoch 16/1000, LR 0.000270
Train loss: 0.4951;  Loss pred: 0.4951; Loss self: 0.0000; time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6737 score: 0.4898 time: 0.10s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6765 score: 0.5102 time: 0.10s
     INFO: Early stopping counter 1 of 20
Epoch 17/1000, LR 0.000270
Train loss: 0.5041;  Loss pred: 0.5041; Loss self: 0.0000; time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6740 score: 0.4898 time: 0.10s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6770 score: 0.5102 time: 0.09s
     INFO: Early stopping counter 2 of 20
Epoch 18/1000, LR 0.000270
Train loss: 0.4626;  Loss pred: 0.4626; Loss self: 0.0000; time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6755 score: 0.4898 time: 0.09s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6786 score: 0.5102 time: 0.08s
     INFO: Early stopping counter 3 of 20
Epoch 19/1000, LR 0.000270
Train loss: 0.4544;  Loss pred: 0.4544; Loss self: 0.0000; time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6761 score: 0.4898 time: 2.38s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6792 score: 0.5102 time: 3.17s
     INFO: Early stopping counter 4 of 20
Epoch 20/1000, LR 0.000270
Train loss: 0.4588;  Loss pred: 0.4588; Loss self: 0.0000; time: 0.93s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6762 score: 0.4898 time: 0.10s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6795 score: 0.5102 time: 0.09s
     INFO: Early stopping counter 5 of 20
Epoch 21/1000, LR 0.000270
Train loss: 0.4136;  Loss pred: 0.4136; Loss self: 0.0000; time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6767 score: 0.4898 time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6801 score: 0.5102 time: 0.08s
     INFO: Early stopping counter 6 of 20
Epoch 22/1000, LR 0.000270
Train loss: 0.4175;  Loss pred: 0.4175; Loss self: 0.0000; time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6767 score: 0.4898 time: 0.10s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6804 score: 0.5102 time: 0.08s
     INFO: Early stopping counter 7 of 20
Epoch 23/1000, LR 0.000270
Train loss: 0.4123;  Loss pred: 0.4123; Loss self: 0.0000; time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6771 score: 0.4898 time: 0.10s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6808 score: 0.5102 time: 0.08s
     INFO: Early stopping counter 8 of 20
Epoch 24/1000, LR 0.000270
Train loss: 0.4167;  Loss pred: 0.4167; Loss self: 0.0000; time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6763 score: 0.4898 time: 0.09s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6802 score: 0.5102 time: 0.08s
     INFO: Early stopping counter 9 of 20
Epoch 25/1000, LR 0.000270
Train loss: 0.3675;  Loss pred: 0.3675; Loss self: 0.0000; time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6745 score: 0.4898 time: 0.10s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6785 score: 0.5102 time: 0.08s
     INFO: Early stopping counter 10 of 20
Epoch 26/1000, LR 0.000270
Train loss: 0.3781;  Loss pred: 0.3781; Loss self: 0.0000; time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6734 score: 0.4898 time: 0.10s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6775 score: 0.5102 time: 0.08s
Epoch 27/1000, LR 0.000270
Train loss: 0.3678;  Loss pred: 0.3678; Loss self: 0.0000; time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6723 score: 0.4898 time: 0.10s
Test loss: 0.6767 score: 0.5306 time: 0.08s
Epoch 28/1000, LR 0.000270
Train loss: 0.3508;  Loss pred: 0.3508; Loss self: 0.0000; time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6702 score: 0.4898 time: 0.17s
Test loss: 0.6751 score: 0.5306 time: 0.09s
Epoch 29/1000, LR 0.000270
Train loss: 0.3467;  Loss pred: 0.3467; Loss self: 0.0000; time: 0.16s
Val loss: 0.6686 score: 0.5306 time: 0.09s
Test loss: 0.6740 score: 0.5306 time: 0.08s
Epoch 30/1000, LR 0.000270
Train loss: 0.3471;  Loss pred: 0.3471; Loss self: 0.0000; time: 0.16s
Val loss: 0.6659 score: 0.5306 time: 0.09s
Test loss: 0.6719 score: 0.5510 time: 0.08s
Epoch 31/1000, LR 0.000270
Train loss: 0.3120;  Loss pred: 0.3120; Loss self: 0.0000; time: 0.16s
Val loss: 0.6622 score: 0.5306 time: 0.09s
Test loss: 0.6688 score: 0.5510 time: 0.09s
Epoch 32/1000, LR 0.000270
Train loss: 0.3043;  Loss pred: 0.3043; Loss self: 0.0000; time: 0.16s
Val loss: 0.6579 score: 0.5510 time: 0.09s
Test loss: 0.6651 score: 0.5510 time: 0.08s
Epoch 33/1000, LR 0.000270
Train loss: 0.3229;  Loss pred: 0.3229; Loss self: 0.0000; time: 0.16s
Val loss: 0.6546 score: 0.5510 time: 0.09s
Test loss: 0.6625 score: 0.5510 time: 0.08s
Epoch 34/1000, LR 0.000270
Train loss: 0.2876;  Loss pred: 0.2876; Loss self: 0.0000; time: 0.16s
Val loss: 0.6535 score: 0.5510 time: 0.10s
Test loss: 0.6620 score: 0.5510 time: 0.08s
Epoch 35/1000, LR 0.000270
Train loss: 0.2914;  Loss pred: 0.2914; Loss self: 0.0000; time: 0.17s
Val loss: 0.6548 score: 0.5510 time: 0.16s
Test loss: 0.6634 score: 0.5714 time: 0.08s
     INFO: Early stopping counter 1 of 20
Epoch 36/1000, LR 0.000270
Train loss: 0.2700;  Loss pred: 0.2700; Loss self: 0.0000; time: 0.16s
Val loss: 0.6555 score: 0.5510 time: 0.09s
Test loss: 0.6644 score: 0.5918 time: 0.08s
     INFO: Early stopping counter 2 of 20
Epoch 37/1000, LR 0.000270
Train loss: 0.2528;  Loss pred: 0.2528; Loss self: 0.0000; time: 0.16s
Val loss: 0.6578 score: 0.5510 time: 0.09s
Test loss: 0.6667 score: 0.5918 time: 0.08s
     INFO: Early stopping counter 3 of 20
Epoch 38/1000, LR 0.000270
Train loss: 0.2515;  Loss pred: 0.2515; Loss self: 0.0000; time: 0.16s
Val loss: 0.6596 score: 0.5510 time: 0.09s
Test loss: 0.6687 score: 0.5918 time: 0.08s
     INFO: Early stopping counter 4 of 20
Epoch 39/1000, LR 0.000269
Train loss: 0.2470;  Loss pred: 0.2470; Loss self: 0.0000; time: 0.16s
Val loss: 0.6618 score: 0.5510 time: 0.09s
Test loss: 0.6708 score: 0.5918 time: 0.08s
     INFO: Early stopping counter 5 of 20
Epoch 40/1000, LR 0.000269
Train loss: 0.2510;  Loss pred: 0.2510; Loss self: 0.0000; time: 0.16s
Val loss: 0.6617 score: 0.5510 time: 0.10s
Test loss: 0.6708 score: 0.5918 time: 0.08s
     INFO: Early stopping counter 6 of 20
Epoch 41/1000, LR 0.000269
Train loss: 0.2397;  Loss pred: 0.2397; Loss self: 0.0000; time: 0.17s
Val loss: 0.6618 score: 0.5510 time: 0.10s
Test loss: 0.6711 score: 0.6122 time: 0.09s
     INFO: Early stopping counter 7 of 20
Epoch 42/1000, LR 0.000269
Train loss: 0.2251;  Loss pred: 0.2251; Loss self: 0.0000; time: 0.17s
Val loss: 0.6622 score: 0.5510 time: 0.20s
Test loss: 0.6717 score: 0.6122 time: 0.08s
     INFO: Early stopping counter 8 of 20
Epoch 43/1000, LR 0.000269
Train loss: 0.2185;  Loss pred: 0.2185; Loss self: 0.0000; time: 0.17s
Val loss: 0.6623 score: 0.5510 time: 0.10s
Test loss: 0.6720 score: 0.6122 time: 0.08s
     INFO: Early stopping counter 9 of 20
Epoch 44/1000, LR 0.000269
Train loss: 0.2213;  Loss pred: 0.2213; Loss self: 0.0000; time: 0.17s
Val loss: 0.6604 score: 0.5510 time: 0.10s
Test loss: 0.6713 score: 0.6122 time: 0.09s
     INFO: Early stopping counter 10 of 20
Epoch 45/1000, LR 0.000269
Train loss: 0.2041;  Loss pred: 0.2041; Loss self: 0.0000; time: 0.17s
Val loss: 0.6611 score: 0.5510 time: 0.10s
Test loss: 0.6729 score: 0.6122 time: 0.08s
     INFO: Early stopping counter 11 of 20
Epoch 46/1000, LR 0.000269
Train loss: 0.1992;  Loss pred: 0.1992; Loss self: 0.0000; time: 0.17s
Val loss: 0.6601 score: 0.5510 time: 0.10s
Test loss: 0.6730 score: 0.6122 time: 0.08s
     INFO: Early stopping counter 12 of 20
Epoch 47/1000, LR 0.000269
Train loss: 0.2094;  Loss pred: 0.2094; Loss self: 0.0000; time: 0.17s
Val loss: 0.6585 score: 0.5714 time: 0.09s
Test loss: 0.6727 score: 0.6122 time: 0.08s
     INFO: Early stopping counter 13 of 20
Epoch 48/1000, LR 0.000269
Train loss: 0.1990;  Loss pred: 0.1990; Loss self: 0.0000; time: 0.17s
Val loss: 0.6562 score: 0.5714 time: 0.10s
Test loss: 0.6723 score: 0.6122 time: 0.08s
     INFO: Early stopping counter 14 of 20
Epoch 49/1000, LR 0.000269
Train loss: 0.1929;  Loss pred: 0.1929; Loss self: 0.0000; time: 0.30s
Val loss: 0.6590 score: 0.5918 time: 0.10s
Test loss: 0.6754 score: 0.6122 time: 0.08s
     INFO: Early stopping counter 15 of 20
Epoch 50/1000, LR 0.000269
Train loss: 0.1967;  Loss pred: 0.1967; Loss self: 0.0000; time: 0.16s
Val loss: 0.6640 score: 0.5918 time: 0.10s
Test loss: 0.6797 score: 0.6122 time: 0.08s
     INFO: Early stopping counter 16 of 20
Epoch 51/1000, LR 0.000269
Train loss: 0.1766;  Loss pred: 0.1766; Loss self: 0.0000; time: 0.17s
Val loss: 0.6662 score: 0.5918 time: 0.09s
Test loss: 0.6819 score: 0.6122 time: 0.08s
     INFO: Early stopping counter 17 of 20
Epoch 52/1000, LR 0.000269
Train loss: 0.1717;  Loss pred: 0.1717; Loss self: 0.0000; time: 0.16s
Val loss: 0.6661 score: 0.5918 time: 0.09s
Test loss: 0.6822 score: 0.6122 time: 0.08s
     INFO: Early stopping counter 18 of 20
Epoch 53/1000, LR 0.000269
Train loss: 0.1695;  Loss pred: 0.1695; Loss self: 0.0000; time: 0.16s
Val loss: 0.6667 score: 0.5918 time: 0.10s
Test loss: 0.6829 score: 0.6122 time: 0.08s
     INFO: Early stopping counter 19 of 20
Epoch 54/1000, LR 0.000269
Train loss: 0.1724;  Loss pred: 0.1724; Loss self: 0.0000; time: 0.17s
Val loss: 0.6667 score: 0.5918 time: 0.10s
Test loss: 0.6834 score: 0.6122 time: 0.08s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 033,   Train_Loss: 0.2876,   Val_Loss: 0.6535,   Val_Precision: 0.5217,   Val_Recall: 1.0000,   Val_accuracy: 0.6857,   Val_Score: 0.5510,   Val_Loss: 0.6535,   Test_Precision: 0.5319,   Test_Recall: 1.0000,   Test_accuracy: 0.6944,   Test_Score: 0.5510,   Test_loss: 0.6620


[0.09741156897507608, 0.09895631694234908, 0.10116553399711847, 0.10344235901720822, 0.1035291738808155, 0.22272138088010252, 0.10027373605407774, 0.09795636311173439, 0.1106739689130336, 0.10211200290359557, 0.10046836291439831, 0.10226471000351012, 0.10281036212109029, 0.08790014614351094, 0.08669254113920033, 0.10100126313045621, 0.09804347809404135, 0.08292760397307575, 3.171620680950582, 0.09080365695990622, 0.08545708400197327, 0.08988778293132782, 0.08742039883509278, 0.08624385902658105, 0.08565443102270365, 0.08764484501443803, 0.08783528208732605, 0.09655742486938834, 0.08771153609268367, 0.0874567439313978, 0.0975096239708364, 0.0880998820066452, 0.08670785883441567, 0.08707945700734854, 0.08842433686368167, 0.08351755002513528, 0.08224127418361604, 0.08377406001091003, 0.08382883411832154, 0.08568734396249056, 0.09564232593402267, 0.08774824487045407, 0.08779686293564737, 0.09070023288950324, 0.08894872711971402, 0.08901703706942499, 0.08906035497784615, 0.08928603609092534, 0.0854084868915379, 0.08719704789109528, 0.08460369007661939, 0.08594054100103676, 0.08708238787949085, 0.08672135113738477]
[0.001987991203572981, 0.0020195166722928384, 0.0020646027346350707, 0.002111068551371596, 0.002112840283281949, 0.004545334303675562, 0.0020464027766138315, 0.0019991094512598856, 0.002258652426796604, 0.002083918426603991, 0.0020503747533550678, 0.0020870348980308187, 0.002098170655532455, 0.0017938805335410396, 0.001769235533453068, 0.002061250267968494, 0.00200088730804166, 0.0016924000810831785, 0.06472695267246086, 0.0018531358563246168, 0.0017440221224892505, 0.0018344445496189352, 0.0017840897721447507, 0.0017600787556445111, 0.0017480496127082377, 0.0017886703064171026, 0.0017925567772923683, 0.001970559691212007, 0.001790031348830279, 0.0017848315088040366, 0.001989992325935437, 0.0017979567756458204, 0.001769548139477871, 0.0017771317756601743, 0.0018045783033404424, 0.0017044397964313322, 0.0016783933506860416, 0.0017096746941002048, 0.0017107925330269703, 0.0017487213053569502, 0.0019518842027351564, 0.0017907805075602873, 0.0017917727129723952, 0.0018510251610102703, 0.0018152801453002862, 0.0018166742259066325, 0.001817558264854003, 0.0018221640018556192, 0.0017430303447252633, 0.0017795315896141895, 0.0017266059199310079, 0.001753888591857893, 0.0017771915893773644, 0.0017698234925996891]
[503.0203343972135, 495.1679843596735, 484.35468152024663, 473.6937601340721, 473.2965420588559, 220.00581985605658, 488.6623549517916, 500.2227363638227, 442.7418703896276, 479.8652323592275, 487.7157204377789, 479.1486720914588, 476.6056551993045, 557.450722778091, 565.2158692790164, 485.14244754256345, 499.77827136038746, 590.876832953101, 15.44951459495275, 539.6258437216426, 573.3872220454954, 545.1241359177238, 560.5099113358214, 568.1563945891824, 572.0661431632417, 559.0745239144192, 557.8623855421115, 507.4700372993739, 558.6494340746959, 560.276975763427, 502.51450066769956, 556.1868970074673, 565.1160189940148, 562.7044734082913, 554.1460839626117, 586.7030340958644, 595.8078894862465, 584.9065927283297, 584.5244123381004, 571.8464096804032, 512.3254743281951, 558.4157275435023, 558.1065013213015, 540.241170711159, 550.8791591143518, 550.4564251198855, 550.1886895935774, 548.7980220120911, 573.7134772358884, 561.9456298703889, 579.1709552576758, 570.1616423313984, 562.6855348501554, 565.0280969720334]
Elapsed: 0.15086422491973886~0.4153656826861481
Time per graph: 0.003078861733055896~0.008476850667064249
Speed: 523.4664971967594~90.29405301508409
Total Time: 0.0871
best val loss: 0.6535393595695496 test_score: 0.5510

Testing...
Test loss: 0.6754 score: 0.6122 time: 0.08s
test Score 0.6122
Epoch Time List: [1.4453481631353498, 0.37092998228035867, 0.37852770229801536, 0.3799065051134676, 0.3859045640565455, 0.5010023349896073, 0.379776333225891, 0.3636050350032747, 0.3804422430694103, 0.3849370137322694, 0.37968809902668, 0.3884903888683766, 0.4302778970450163, 0.3589623919688165, 0.33274509315378964, 0.3529301641974598, 0.3507317751646042, 0.34062226000241935, 5.7151219139341265, 1.11371267773211, 0.39827057789079845, 0.3350895349867642, 0.35606926190666854, 0.3314613699913025, 0.3392307893373072, 0.34440213115885854, 0.35132868122309446, 0.43896899465471506, 0.33831499912776053, 0.33191809081472456, 0.34606611728668213, 0.33944427501410246, 0.3408503467217088, 0.3412421189714223, 0.4108485400211066, 0.33264887891709805, 0.3249750768300146, 0.3268024430144578, 0.33577086916193366, 0.3394197609741241, 0.36330419033765793, 0.46137530426494777, 0.3486159658059478, 0.352146907011047, 0.3565537401009351, 0.34908419009298086, 0.3434180251788348, 0.34956955001689494, 0.47943836683407426, 0.34163705608807504, 0.3355522339697927, 0.33339443407021463, 0.34128523408435285, 0.3457679399289191]
Total Epoch List: [54]
Total Time List: [0.08714983006939292]
========================k_idx:1========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: SGFormer
SGFormer(
  (trans_conv): SGModule(
    (attns): ModuleList(
      (0-1): 2 x SGFormerAttention(heads=1, head_channels=64)
    )
    (fcs): ModuleList(
      (0): Linear(in_features=14887, out_features=64, bias=True)
    )
    (bns): ModuleList(
      (0-2): 3 x LayerNorm((64,), eps=1e-05, elementwise_affine=True)
    )
  )
  (graph_conv): GraphModule(
    (convs): ModuleList(
      (0-2): 3 x GCNConv(64, 64)
    )
    (fcs): ModuleList(
      (0): Linear(in_features=14887, out_features=64, bias=True)
    )
    (bns): ModuleList(
      (0-3): 4 x BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (classifier): Linear(in_features=64, out_features=2, bias=True)
)
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7e09d8972cb0>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.7412;  Loss pred: 0.7412; Loss self: 0.0000; time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7171 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6916 score: 0.5102 time: 0.09s
Epoch 2/1000, LR 0.000000
Train loss: 0.7357;  Loss pred: 0.7357; Loss self: 0.0000; time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7149 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6895 score: 0.5102 time: 0.10s
Epoch 3/1000, LR 0.000030
Train loss: 0.7337;  Loss pred: 0.7337; Loss self: 0.0000; time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7098 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6845 score: 0.5102 time: 0.09s
Epoch 4/1000, LR 0.000060
Train loss: 0.7362;  Loss pred: 0.7362; Loss self: 0.0000; time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7023 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6770 score: 0.5102 time: 0.09s
Epoch 5/1000, LR 0.000090
Train loss: 0.7164;  Loss pred: 0.7164; Loss self: 0.0000; time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6924 score: 0.4898 time: 0.10s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6674 score: 0.5102 time: 0.11s
Epoch 6/1000, LR 0.000120
Train loss: 0.6898;  Loss pred: 0.6898; Loss self: 0.0000; time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6810 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6566 score: 0.5102 time: 0.09s
Epoch 7/1000, LR 0.000150
Train loss: 0.6741;  Loss pred: 0.6741; Loss self: 0.0000; time: 0.29s
Val loss: 0.6693 score: 0.5510 time: 0.08s
Test loss: 0.6461 score: 0.6327 time: 0.09s
Epoch 8/1000, LR 0.000180
Train loss: 0.6442;  Loss pred: 0.6442; Loss self: 0.0000; time: 0.16s
Val loss: 0.6599 score: 0.8776 time: 0.10s
Test loss: 0.6382 score: 0.9592 time: 0.09s
Epoch 9/1000, LR 0.000210
Train loss: 0.6404;  Loss pred: 0.6404; Loss self: 0.0000; time: 0.15s
Val loss: 0.6538 score: 0.6531 time: 0.08s
Test loss: 0.6346 score: 0.6735 time: 0.10s
Epoch 10/1000, LR 0.000240
Train loss: 0.6089;  Loss pred: 0.6089; Loss self: 0.0000; time: 0.15s
Val loss: 0.6526 score: 0.5510 time: 0.08s
Test loss: 0.6368 score: 0.5714 time: 0.09s
Epoch 11/1000, LR 0.000270
Train loss: 0.5955;  Loss pred: 0.5955; Loss self: 0.0000; time: 0.16s
Val loss: 0.6557 score: 0.5306 time: 0.10s
Test loss: 0.6435 score: 0.5306 time: 0.10s
     INFO: Early stopping counter 1 of 20
Epoch 12/1000, LR 0.000270
Train loss: 0.5737;  Loss pred: 0.5737; Loss self: 0.0000; time: 0.19s
Val loss: 0.6615 score: 0.5306 time: 0.10s
Test loss: 0.6526 score: 0.5306 time: 0.10s
     INFO: Early stopping counter 2 of 20
Epoch 13/1000, LR 0.000270
Train loss: 0.5218;  Loss pred: 0.5218; Loss self: 0.0000; time: 0.35s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6694 score: 0.5102 time: 0.13s
Test loss: 0.6641 score: 0.5306 time: 0.09s
     INFO: Early stopping counter 3 of 20
Epoch 14/1000, LR 0.000270
Train loss: 0.5039;  Loss pred: 0.5039; Loss self: 0.0000; time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6778 score: 0.5102 time: 0.08s
Test loss: 0.6759 score: 0.5306 time: 0.09s
     INFO: Early stopping counter 4 of 20
Epoch 15/1000, LR 0.000270
Train loss: 0.4918;  Loss pred: 0.4918; Loss self: 0.0000; time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6854 score: 0.5102 time: 0.09s
Test loss: 0.6862 score: 0.5306 time: 0.09s
     INFO: Early stopping counter 5 of 20
Epoch 16/1000, LR 0.000270
Train loss: 0.4863;  Loss pred: 0.4863; Loss self: 0.0000; time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6916 score: 0.5102 time: 0.08s
Test loss: 0.6946 score: 0.5306 time: 0.09s
     INFO: Early stopping counter 6 of 20
Epoch 17/1000, LR 0.000270
Train loss: 0.4456;  Loss pred: 0.4456; Loss self: 0.0000; time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6955 score: 0.5102 time: 0.08s
Test loss: 0.6999 score: 0.5306 time: 0.10s
     INFO: Early stopping counter 7 of 20
Epoch 18/1000, LR 0.000270
Train loss: 0.4383;  Loss pred: 0.4383; Loss self: 0.0000; time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6983 score: 0.5102 time: 0.08s
Test loss: 0.7038 score: 0.5306 time: 0.09s
     INFO: Early stopping counter 8 of 20
Epoch 19/1000, LR 0.000270
Train loss: 0.4155;  Loss pred: 0.4155; Loss self: 0.0000; time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6997 score: 0.5102 time: 0.08s
Test loss: 0.7059 score: 0.5306 time: 0.09s
     INFO: Early stopping counter 9 of 20
Epoch 20/1000, LR 0.000270
Train loss: 0.3936;  Loss pred: 0.3936; Loss self: 0.0000; time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7008 score: 0.5102 time: 0.08s
Test loss: 0.7076 score: 0.5306 time: 0.10s
     INFO: Early stopping counter 10 of 20
Epoch 21/1000, LR 0.000270
Train loss: 0.3942;  Loss pred: 0.3942; Loss self: 0.0000; time: 0.36s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7011 score: 0.5102 time: 0.09s
Test loss: 0.7081 score: 0.5306 time: 0.10s
     INFO: Early stopping counter 11 of 20
Epoch 22/1000, LR 0.000270
Train loss: 0.3801;  Loss pred: 0.3801; Loss self: 0.0000; time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6998 score: 0.5102 time: 0.09s
Test loss: 0.7071 score: 0.5306 time: 0.09s
     INFO: Early stopping counter 12 of 20
Epoch 23/1000, LR 0.000270
Train loss: 0.3492;  Loss pred: 0.3492; Loss self: 0.0000; time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6975 score: 0.5102 time: 0.08s
Test loss: 0.7045 score: 0.5306 time: 0.09s
     INFO: Early stopping counter 13 of 20
Epoch 24/1000, LR 0.000270
Train loss: 0.3529;  Loss pred: 0.3529; Loss self: 0.0000; time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6948 score: 0.5102 time: 0.08s
Test loss: 0.7013 score: 0.5306 time: 0.09s
     INFO: Early stopping counter 14 of 20
Epoch 25/1000, LR 0.000270
Train loss: 0.3407;  Loss pred: 0.3407; Loss self: 0.0000; time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6928 score: 0.5102 time: 0.08s
Test loss: 0.6986 score: 0.5306 time: 0.09s
     INFO: Early stopping counter 15 of 20
Epoch 26/1000, LR 0.000270
Train loss: 0.3381;  Loss pred: 0.3381; Loss self: 0.0000; time: 0.15s
Val loss: 0.6899 score: 0.5306 time: 0.08s
Test loss: 0.6952 score: 0.5306 time: 0.10s
     INFO: Early stopping counter 16 of 20
Epoch 27/1000, LR 0.000270
Train loss: 0.3115;  Loss pred: 0.3115; Loss self: 0.0000; time: 0.16s
Val loss: 0.6867 score: 0.5306 time: 0.19s
Test loss: 0.6915 score: 0.5306 time: 0.09s
     INFO: Early stopping counter 17 of 20
Epoch 28/1000, LR 0.000270
Train loss: 0.2976;  Loss pred: 0.2976; Loss self: 0.0000; time: 0.16s
Val loss: 0.6839 score: 0.5306 time: 0.08s
Test loss: 0.6884 score: 0.5306 time: 0.10s
     INFO: Early stopping counter 18 of 20
Epoch 29/1000, LR 0.000270
Train loss: 0.2900;  Loss pred: 0.2900; Loss self: 0.0000; time: 0.17s
Val loss: 0.6815 score: 0.5306 time: 0.08s
Test loss: 0.6853 score: 0.5306 time: 0.10s
     INFO: Early stopping counter 19 of 20
Epoch 30/1000, LR 0.000270
Train loss: 0.2774;  Loss pred: 0.2774; Loss self: 0.0000; time: 0.16s
Val loss: 0.6785 score: 0.5306 time: 0.08s
Test loss: 0.6819 score: 0.5306 time: 0.10s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 009,   Train_Loss: 0.6089,   Val_Loss: 0.6526,   Val_Precision: 0.5319,   Val_Recall: 1.0000,   Val_accuracy: 0.6944,   Val_Score: 0.5510,   Val_Loss: 0.6526,   Test_Precision: 0.5333,   Test_Recall: 1.0000,   Test_accuracy: 0.6957,   Test_Score: 0.5714,   Test_loss: 0.6368


[0.09741156897507608, 0.09895631694234908, 0.10116553399711847, 0.10344235901720822, 0.1035291738808155, 0.22272138088010252, 0.10027373605407774, 0.09795636311173439, 0.1106739689130336, 0.10211200290359557, 0.10046836291439831, 0.10226471000351012, 0.10281036212109029, 0.08790014614351094, 0.08669254113920033, 0.10100126313045621, 0.09804347809404135, 0.08292760397307575, 3.171620680950582, 0.09080365695990622, 0.08545708400197327, 0.08988778293132782, 0.08742039883509278, 0.08624385902658105, 0.08565443102270365, 0.08764484501443803, 0.08783528208732605, 0.09655742486938834, 0.08771153609268367, 0.0874567439313978, 0.0975096239708364, 0.0880998820066452, 0.08670785883441567, 0.08707945700734854, 0.08842433686368167, 0.08351755002513528, 0.08224127418361604, 0.08377406001091003, 0.08382883411832154, 0.08568734396249056, 0.09564232593402267, 0.08774824487045407, 0.08779686293564737, 0.09070023288950324, 0.08894872711971402, 0.08901703706942499, 0.08906035497784615, 0.08928603609092534, 0.0854084868915379, 0.08719704789109528, 0.08460369007661939, 0.08594054100103676, 0.08708238787949085, 0.08672135113738477, 0.09524744004011154, 0.10109067591838539, 0.0989115540869534, 0.09764862922020257, 0.11801036912947893, 0.0976521538104862, 0.09595370385795832, 0.09838725300505757, 0.1083872199524194, 0.09685419593006372, 0.10577020212076604, 0.10841468814760447, 0.09724348108284175, 0.10035247611813247, 0.09781526192091405, 0.09611977799795568, 0.10853976081125438, 0.09756493987515569, 0.09590731118805707, 0.10023064306005836, 0.11173899611458182, 0.09883080492727458, 0.09737148112617433, 0.09567257482558489, 0.095655363984406, 0.10401615197770298, 0.09968829690478742, 0.10479603498242795, 0.10093978815712035, 0.10078080487437546]
[0.001987991203572981, 0.0020195166722928384, 0.0020646027346350707, 0.002111068551371596, 0.002112840283281949, 0.004545334303675562, 0.0020464027766138315, 0.0019991094512598856, 0.002258652426796604, 0.002083918426603991, 0.0020503747533550678, 0.0020870348980308187, 0.002098170655532455, 0.0017938805335410396, 0.001769235533453068, 0.002061250267968494, 0.00200088730804166, 0.0016924000810831785, 0.06472695267246086, 0.0018531358563246168, 0.0017440221224892505, 0.0018344445496189352, 0.0017840897721447507, 0.0017600787556445111, 0.0017480496127082377, 0.0017886703064171026, 0.0017925567772923683, 0.001970559691212007, 0.001790031348830279, 0.0017848315088040366, 0.001989992325935437, 0.0017979567756458204, 0.001769548139477871, 0.0017771317756601743, 0.0018045783033404424, 0.0017044397964313322, 0.0016783933506860416, 0.0017096746941002048, 0.0017107925330269703, 0.0017487213053569502, 0.0019518842027351564, 0.0017907805075602873, 0.0017917727129723952, 0.0018510251610102703, 0.0018152801453002862, 0.0018166742259066325, 0.001817558264854003, 0.0018221640018556192, 0.0017430303447252633, 0.0017795315896141895, 0.0017266059199310079, 0.001753888591857893, 0.0017771915893773644, 0.0017698234925996891, 0.001943825306941052, 0.002063075018742559, 0.002018603144631702, 0.001992829167759236, 0.0024083748801934477, 0.0019929010981731875, 0.001958238854244047, 0.0020079031225521955, 0.0022119840806616204, 0.001976616243470688, 0.002158575553485021, 0.0022125446560735604, 0.001984560838425342, 0.002048009716696581, 0.001996229835120695, 0.001961628122407259, 0.0022150971594133546, 0.001991121221941953, 0.001957292065062389, 0.002045523327756293, 0.002280387675807792, 0.0020169552025974405, 0.001987173084207639, 0.0019525015270527527, 0.0019521502853960407, 0.0021227786117898567, 0.0020344550388732125, 0.0021386945914781217, 0.0020599956766759256, 0.0020567511198852136]
[503.0203343972135, 495.1679843596735, 484.35468152024663, 473.6937601340721, 473.2965420588559, 220.00581985605658, 488.6623549517916, 500.2227363638227, 442.7418703896276, 479.8652323592275, 487.7157204377789, 479.1486720914588, 476.6056551993045, 557.450722778091, 565.2158692790164, 485.14244754256345, 499.77827136038746, 590.876832953101, 15.44951459495275, 539.6258437216426, 573.3872220454954, 545.1241359177238, 560.5099113358214, 568.1563945891824, 572.0661431632417, 559.0745239144192, 557.8623855421115, 507.4700372993739, 558.6494340746959, 560.276975763427, 502.51450066769956, 556.1868970074673, 565.1160189940148, 562.7044734082913, 554.1460839626117, 586.7030340958644, 595.8078894862465, 584.9065927283297, 584.5244123381004, 571.8464096804032, 512.3254743281951, 558.4157275435023, 558.1065013213015, 540.241170711159, 550.8791591143518, 550.4564251198855, 550.1886895935774, 548.7980220120911, 573.7134772358884, 561.9456298703889, 579.1709552576758, 570.1616423313984, 562.6855348501554, 565.0280969720334, 514.4495219962304, 484.7133482375733, 495.3920747916262, 501.799158793131, 415.21775045240344, 501.7810471963008, 510.6629346224657, 498.0319960501506, 452.0828195566818, 505.91509773496875, 463.2684727599641, 451.96827881188443, 503.88981815919243, 488.2789333699988, 500.9443213434081, 509.78061977049254, 451.44746619820484, 502.2295925431872, 510.90995454892663, 488.8724496224086, 438.5219279198943, 495.7968321320162, 503.22742792117555, 512.163491881859, 512.2556431648529, 471.0806838009514, 491.5321208346055, 467.5749422028825, 485.43791199291826, 486.2036978279655]
Elapsed: 0.13300309739064514~0.33391031733618715
Time per graph: 0.0027143489263396965~0.006814496272167085
Speed: 510.5073950578968~75.89232555723999
Total Time: 0.1013
best val loss: 0.6525946855545044 test_score: 0.5714

Testing...
Test loss: 0.6382 score: 0.9592 time: 0.09s
test Score 0.9592
Epoch Time List: [1.4453481631353498, 0.37092998228035867, 0.37852770229801536, 0.3799065051134676, 0.3859045640565455, 0.5010023349896073, 0.379776333225891, 0.3636050350032747, 0.3804422430694103, 0.3849370137322694, 0.37968809902668, 0.3884903888683766, 0.4302778970450163, 0.3589623919688165, 0.33274509315378964, 0.3529301641974598, 0.3507317751646042, 0.34062226000241935, 5.7151219139341265, 1.11371267773211, 0.39827057789079845, 0.3350895349867642, 0.35606926190666854, 0.3314613699913025, 0.3392307893373072, 0.34440213115885854, 0.35132868122309446, 0.43896899465471506, 0.33831499912776053, 0.33191809081472456, 0.34606611728668213, 0.33944427501410246, 0.3408503467217088, 0.3412421189714223, 0.4108485400211066, 0.33264887891709805, 0.3249750768300146, 0.3268024430144578, 0.33577086916193366, 0.3394197609741241, 0.36330419033765793, 0.46137530426494777, 0.3486159658059478, 0.352146907011047, 0.3565537401009351, 0.34908419009298086, 0.3434180251788348, 0.34956955001689494, 0.47943836683407426, 0.34163705608807504, 0.3355522339697927, 0.33339443407021463, 0.34128523408435285, 0.3457679399289191, 0.3330588152166456, 0.3387430552393198, 0.32687725918367505, 0.3329435798805207, 0.3818600228987634, 0.33139511989429593, 0.4652004719246179, 0.3508102830965072, 0.33518899395130575, 0.3292790160048753, 0.35643007373437285, 0.3931357772089541, 0.568860923871398, 0.3366872239857912, 0.3379405518062413, 0.3304983333218843, 0.35197992715984583, 0.3413226299453527, 0.3322870072443038, 0.3369435132481158, 0.5572187257930636, 0.35113626811653376, 0.32276370096951723, 0.33024095091968775, 0.32895444775931537, 0.3298898870125413, 0.4463914141524583, 0.3357103141024709, 0.35081540374085307, 0.33372348826378584]
Total Epoch List: [54, 30]
Total Time List: [0.08714983006939292, 0.1013303748331964]
========================k_idx:2========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: SGFormer
SGFormer(
  (trans_conv): SGModule(
    (attns): ModuleList(
      (0-1): 2 x SGFormerAttention(heads=1, head_channels=64)
    )
    (fcs): ModuleList(
      (0): Linear(in_features=14887, out_features=64, bias=True)
    )
    (bns): ModuleList(
      (0-2): 3 x LayerNorm((64,), eps=1e-05, elementwise_affine=True)
    )
  )
  (graph_conv): GraphModule(
    (convs): ModuleList(
      (0-2): 3 x GCNConv(64, 64)
    )
    (fcs): ModuleList(
      (0): Linear(in_features=14887, out_features=64, bias=True)
    )
    (bns): ModuleList(
      (0-3): 4 x BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (classifier): Linear(in_features=64, out_features=2, bias=True)
)
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7e09d882bee0>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.6574;  Loss pred: 0.6574; Loss self: 0.0000; time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7097 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7036 score: 0.5000 time: 0.10s
Epoch 2/1000, LR 0.000000
Train loss: 0.7005;  Loss pred: 0.7005; Loss self: 0.0000; time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7088 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7026 score: 0.5000 time: 0.14s
Epoch 3/1000, LR 0.000030
Train loss: 0.6747;  Loss pred: 0.6747; Loss self: 0.0000; time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7075 score: 0.4898 time: 0.11s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7010 score: 0.5000 time: 0.10s
Epoch 4/1000, LR 0.000060
Train loss: 0.6556;  Loss pred: 0.6556; Loss self: 0.0000; time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7060 score: 0.4898 time: 0.09s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6990 score: 0.5000 time: 0.09s
Epoch 5/1000, LR 0.000090
Train loss: 0.6241;  Loss pred: 0.6241; Loss self: 0.0000; time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7043 score: 0.4898 time: 0.09s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6968 score: 0.5000 time: 0.10s
Epoch 6/1000, LR 0.000120
Train loss: 0.6750;  Loss pred: 0.6750; Loss self: 0.0000; time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7020 score: 0.4898 time: 0.10s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6939 score: 0.5000 time: 0.10s
Epoch 7/1000, LR 0.000150
Train loss: 0.6325;  Loss pred: 0.6325; Loss self: 0.0000; time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6995 score: 0.4898 time: 0.09s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6907 score: 0.5000 time: 0.09s
Epoch 8/1000, LR 0.000180
Train loss: 0.6230;  Loss pred: 0.6230; Loss self: 0.0000; time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6966 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6869 score: 0.5000 time: 0.09s
Epoch 9/1000, LR 0.000210
Train loss: 0.5971;  Loss pred: 0.5971; Loss self: 0.0000; time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6930 score: 0.4898 time: 0.10s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6821 score: 0.5000 time: 0.15s
Epoch 10/1000, LR 0.000240
Train loss: 0.5799;  Loss pred: 0.5799; Loss self: 0.0000; time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6889 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6769 score: 0.5000 time: 0.09s
Epoch 11/1000, LR 0.000270
Train loss: 0.5525;  Loss pred: 0.5525; Loss self: 0.0000; time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6838 score: 0.4898 time: 0.09s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6706 score: 0.5000 time: 0.09s
Epoch 12/1000, LR 0.000270
Train loss: 0.5673;  Loss pred: 0.5673; Loss self: 0.0000; time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6779 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6635 score: 0.5000 time: 0.09s
Epoch 13/1000, LR 0.000270
Train loss: 0.5323;  Loss pred: 0.5323; Loss self: 0.0000; time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6710 score: 0.4898 time: 0.10s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6554 score: 0.5000 time: 0.09s
Epoch 14/1000, LR 0.000270
Train loss: 0.5366;  Loss pred: 0.5366; Loss self: 0.0000; time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6636 score: 0.4898 time: 0.10s
Test loss: 0.6468 score: 0.5208 time: 0.10s
Epoch 15/1000, LR 0.000270
Train loss: 0.4808;  Loss pred: 0.4808; Loss self: 0.0000; time: 0.16s
Val loss: 0.6557 score: 0.5102 time: 0.09s
Test loss: 0.6378 score: 0.5833 time: 0.10s
Epoch 16/1000, LR 0.000270
Train loss: 0.4811;  Loss pred: 0.4811; Loss self: 0.0000; time: 0.16s
Val loss: 0.6473 score: 0.5714 time: 0.10s
Test loss: 0.6283 score: 0.6458 time: 0.10s
Epoch 17/1000, LR 0.000270
Train loss: 0.4720;  Loss pred: 0.4720; Loss self: 0.0000; time: 0.17s
Val loss: 0.6381 score: 0.6939 time: 0.09s
Test loss: 0.6180 score: 0.7708 time: 0.09s
Epoch 18/1000, LR 0.000270
Train loss: 0.4602;  Loss pred: 0.4602; Loss self: 0.0000; time: 0.16s
Val loss: 0.6280 score: 0.7347 time: 0.09s
Test loss: 0.6069 score: 0.8333 time: 0.09s
Epoch 19/1000, LR 0.000270
Train loss: 0.4328;  Loss pred: 0.4328; Loss self: 0.0000; time: 0.17s
Val loss: 0.6174 score: 0.8163 time: 0.08s
Test loss: 0.5957 score: 0.9167 time: 0.09s
Epoch 20/1000, LR 0.000270
Train loss: 0.4099;  Loss pred: 0.4099; Loss self: 0.0000; time: 0.17s
Val loss: 0.6068 score: 0.8367 time: 0.09s
Test loss: 0.5849 score: 0.9167 time: 0.09s
Epoch 21/1000, LR 0.000270
Train loss: 0.4158;  Loss pred: 0.4158; Loss self: 0.0000; time: 0.16s
Val loss: 0.5967 score: 0.8776 time: 0.08s
Test loss: 0.5746 score: 0.9375 time: 0.09s
Epoch 22/1000, LR 0.000270
Train loss: 0.3966;  Loss pred: 0.3966; Loss self: 0.0000; time: 0.16s
Val loss: 0.5874 score: 0.8367 time: 0.10s
Test loss: 0.5649 score: 0.8958 time: 0.09s
Epoch 23/1000, LR 0.000270
Train loss: 0.3849;  Loss pred: 0.3849; Loss self: 0.0000; time: 0.17s
Val loss: 0.5790 score: 0.8367 time: 0.08s
Test loss: 0.5565 score: 0.8750 time: 0.09s
Epoch 24/1000, LR 0.000270
Train loss: 0.3764;  Loss pred: 0.3764; Loss self: 0.0000; time: 0.16s
Val loss: 0.5713 score: 0.8163 time: 0.10s
Test loss: 0.5489 score: 0.8542 time: 0.09s
Epoch 25/1000, LR 0.000270
Train loss: 0.3755;  Loss pred: 0.3755; Loss self: 0.0000; time: 0.16s
Val loss: 0.5646 score: 0.8163 time: 0.09s
Test loss: 0.5423 score: 0.8333 time: 0.09s
Epoch 26/1000, LR 0.000270
Train loss: 0.3403;  Loss pred: 0.3403; Loss self: 0.0000; time: 0.17s
Val loss: 0.5587 score: 0.7755 time: 0.09s
Test loss: 0.5364 score: 0.7917 time: 0.09s
Epoch 27/1000, LR 0.000270
Train loss: 0.3297;  Loss pred: 0.3297; Loss self: 0.0000; time: 0.17s
Val loss: 0.5535 score: 0.7551 time: 0.10s
Test loss: 0.5308 score: 0.7917 time: 0.10s
Epoch 28/1000, LR 0.000270
Train loss: 0.3227;  Loss pred: 0.3227; Loss self: 0.0000; time: 0.16s
Val loss: 0.5486 score: 0.7347 time: 0.09s
Test loss: 0.5253 score: 0.7917 time: 0.09s
Epoch 29/1000, LR 0.000270
Train loss: 0.3237;  Loss pred: 0.3237; Loss self: 0.0000; time: 0.16s
Val loss: 0.5432 score: 0.7347 time: 0.09s
Test loss: 0.5194 score: 0.7917 time: 0.10s
Epoch 30/1000, LR 0.000270
Train loss: 0.3308;  Loss pred: 0.3308; Loss self: 0.0000; time: 0.16s
Val loss: 0.5373 score: 0.7551 time: 0.09s
Test loss: 0.5125 score: 0.7917 time: 0.09s
Epoch 31/1000, LR 0.000270
Train loss: 0.2983;  Loss pred: 0.2983; Loss self: 0.0000; time: 0.16s
Val loss: 0.5315 score: 0.7551 time: 0.08s
Test loss: 0.5058 score: 0.7917 time: 0.09s
Epoch 32/1000, LR 0.000270
Train loss: 0.2966;  Loss pred: 0.2966; Loss self: 0.0000; time: 0.16s
Val loss: 0.5252 score: 0.7959 time: 0.09s
Test loss: 0.4983 score: 0.8125 time: 0.09s
Epoch 33/1000, LR 0.000270
Train loss: 0.3043;  Loss pred: 0.3043; Loss self: 0.0000; time: 0.17s
Val loss: 0.5196 score: 0.8163 time: 0.08s
Test loss: 0.4914 score: 0.8125 time: 0.09s
Epoch 34/1000, LR 0.000270
Train loss: 0.2827;  Loss pred: 0.2827; Loss self: 0.0000; time: 0.17s
Val loss: 0.5144 score: 0.7959 time: 0.09s
Test loss: 0.4846 score: 0.8125 time: 0.10s
Epoch 35/1000, LR 0.000270
Train loss: 0.2675;  Loss pred: 0.2675; Loss self: 0.0000; time: 0.17s
Val loss: 0.5100 score: 0.7959 time: 0.08s
Test loss: 0.4785 score: 0.8750 time: 0.09s
Epoch 36/1000, LR 0.000270
Train loss: 0.2736;  Loss pred: 0.2736; Loss self: 0.0000; time: 0.16s
Val loss: 0.5059 score: 0.8367 time: 0.09s
Test loss: 0.4731 score: 0.8750 time: 0.10s
Epoch 37/1000, LR 0.000270
Train loss: 0.2530;  Loss pred: 0.2530; Loss self: 0.0000; time: 0.17s
Val loss: 0.5025 score: 0.8367 time: 0.10s
Test loss: 0.4687 score: 0.8750 time: 0.09s
Epoch 38/1000, LR 0.000270
Train loss: 0.2628;  Loss pred: 0.2628; Loss self: 0.0000; time: 0.16s
Val loss: 0.4995 score: 0.8367 time: 0.08s
Test loss: 0.4650 score: 0.8750 time: 0.09s
Epoch 39/1000, LR 0.000269
Train loss: 0.2499;  Loss pred: 0.2499; Loss self: 0.0000; time: 0.15s
Val loss: 0.4972 score: 0.8367 time: 0.08s
Test loss: 0.4623 score: 0.8750 time: 0.09s
Epoch 40/1000, LR 0.000269
Train loss: 0.2283;  Loss pred: 0.2283; Loss self: 0.0000; time: 0.16s
Val loss: 0.4954 score: 0.7959 time: 0.08s
Test loss: 0.4603 score: 0.8125 time: 0.09s
Epoch 41/1000, LR 0.000269
Train loss: 0.2406;  Loss pred: 0.2406; Loss self: 0.0000; time: 0.15s
Val loss: 0.4944 score: 0.7959 time: 0.08s
Test loss: 0.4591 score: 0.8125 time: 0.09s
Epoch 42/1000, LR 0.000269
Train loss: 0.2314;  Loss pred: 0.2314; Loss self: 0.0000; time: 0.16s
Val loss: 0.4944 score: 0.8163 time: 0.08s
Test loss: 0.4592 score: 0.8125 time: 0.09s
Epoch 43/1000, LR 0.000269
Train loss: 0.2220;  Loss pred: 0.2220; Loss self: 0.0000; time: 0.16s
Val loss: 0.4963 score: 0.7551 time: 0.09s
Test loss: 0.4615 score: 0.7917 time: 0.09s
     INFO: Early stopping counter 1 of 20
Epoch 44/1000, LR 0.000269
Train loss: 0.2069;  Loss pred: 0.2069; Loss self: 0.0000; time: 0.16s
Val loss: 0.4983 score: 0.7347 time: 0.08s
Test loss: 0.4637 score: 0.7708 time: 0.09s
     INFO: Early stopping counter 2 of 20
Epoch 45/1000, LR 0.000269
Train loss: 0.2142;  Loss pred: 0.2142; Loss self: 0.0000; time: 0.16s
Val loss: 0.4982 score: 0.7347 time: 0.08s
Test loss: 0.4636 score: 0.7708 time: 0.09s
     INFO: Early stopping counter 3 of 20
Epoch 46/1000, LR 0.000269
Train loss: 0.2001;  Loss pred: 0.2001; Loss self: 0.0000; time: 0.17s
Val loss: 0.4972 score: 0.7347 time: 0.08s
Test loss: 0.4622 score: 0.7708 time: 0.09s
     INFO: Early stopping counter 4 of 20
Epoch 47/1000, LR 0.000269
Train loss: 0.1945;  Loss pred: 0.1945; Loss self: 0.0000; time: 0.16s
Val loss: 0.4947 score: 0.7347 time: 0.08s
Test loss: 0.4591 score: 0.7708 time: 0.10s
     INFO: Early stopping counter 5 of 20
Epoch 48/1000, LR 0.000269
Train loss: 0.2004;  Loss pred: 0.2004; Loss self: 0.0000; time: 0.17s
Val loss: 0.4917 score: 0.7347 time: 0.09s
Test loss: 0.4557 score: 0.7708 time: 0.09s
Epoch 49/1000, LR 0.000269
Train loss: 0.2030;  Loss pred: 0.2030; Loss self: 0.0000; time: 0.16s
Val loss: 0.4867 score: 0.7347 time: 0.09s
Test loss: 0.4496 score: 0.7917 time: 0.09s
Epoch 50/1000, LR 0.000269
Train loss: 0.1935;  Loss pred: 0.1935; Loss self: 0.0000; time: 0.16s
Val loss: 0.4826 score: 0.7551 time: 0.09s
Test loss: 0.4447 score: 0.7917 time: 0.09s
Epoch 51/1000, LR 0.000269
Train loss: 0.1808;  Loss pred: 0.1808; Loss self: 0.0000; time: 0.24s
Val loss: 0.4771 score: 0.7551 time: 0.09s
Test loss: 0.4385 score: 0.7917 time: 0.10s
Epoch 52/1000, LR 0.000269
Train loss: 0.1766;  Loss pred: 0.1766; Loss self: 0.0000; time: 0.15s
Val loss: 0.4717 score: 0.7959 time: 0.09s
Test loss: 0.4319 score: 0.7917 time: 0.09s
Epoch 53/1000, LR 0.000269
Train loss: 0.1835;  Loss pred: 0.1835; Loss self: 0.0000; time: 0.15s
Val loss: 0.4687 score: 0.7959 time: 0.08s
Test loss: 0.4281 score: 0.8125 time: 0.08s
Epoch 54/1000, LR 0.000269
Train loss: 0.1815;  Loss pred: 0.1815; Loss self: 0.0000; time: 0.15s
Val loss: 0.4676 score: 0.7959 time: 0.08s
Test loss: 0.4266 score: 0.7917 time: 0.08s
Epoch 55/1000, LR 0.000269
Train loss: 0.1631;  Loss pred: 0.1631; Loss self: 0.0000; time: 0.15s
Val loss: 0.4665 score: 0.7959 time: 0.08s
Test loss: 0.4250 score: 0.7917 time: 0.10s
Epoch 56/1000, LR 0.000269
Train loss: 0.1720;  Loss pred: 0.1720; Loss self: 0.0000; time: 0.15s
Val loss: 0.4658 score: 0.7755 time: 0.08s
Test loss: 0.4239 score: 0.7917 time: 0.09s
Epoch 57/1000, LR 0.000269
Train loss: 0.1571;  Loss pred: 0.1571; Loss self: 0.0000; time: 0.16s
Val loss: 0.4640 score: 0.7755 time: 0.08s
Test loss: 0.4210 score: 0.7917 time: 0.09s
Epoch 58/1000, LR 0.000269
Train loss: 0.1594;  Loss pred: 0.1594; Loss self: 0.0000; time: 0.16s
Val loss: 0.4621 score: 0.7959 time: 0.08s
Test loss: 0.4180 score: 0.7917 time: 0.09s
Epoch 59/1000, LR 0.000268
Train loss: 0.1511;  Loss pred: 0.1511; Loss self: 0.0000; time: 0.16s
Val loss: 0.4608 score: 0.7959 time: 0.12s
Test loss: 0.4160 score: 0.7917 time: 0.08s
Epoch 60/1000, LR 0.000268
Train loss: 0.1436;  Loss pred: 0.1436; Loss self: 0.0000; time: 0.16s
Val loss: 0.4600 score: 0.7959 time: 0.08s
Test loss: 0.4149 score: 0.7917 time: 0.09s
Epoch 61/1000, LR 0.000268
Train loss: 0.1539;  Loss pred: 0.1539; Loss self: 0.0000; time: 0.15s
Val loss: 0.4591 score: 0.7755 time: 0.08s
Test loss: 0.4135 score: 0.7917 time: 0.09s
Epoch 62/1000, LR 0.000268
Train loss: 0.1428;  Loss pred: 0.1428; Loss self: 0.0000; time: 0.16s
Val loss: 0.4564 score: 0.7959 time: 0.09s
Test loss: 0.4100 score: 0.7917 time: 0.11s
Epoch 63/1000, LR 0.000268
Train loss: 0.1377;  Loss pred: 0.1377; Loss self: 0.0000; time: 0.16s
Val loss: 0.4545 score: 0.7959 time: 0.08s
Test loss: 0.4076 score: 0.7917 time: 0.09s
Epoch 64/1000, LR 0.000268
Train loss: 0.1391;  Loss pred: 0.1391; Loss self: 0.0000; time: 0.16s
Val loss: 0.4536 score: 0.7551 time: 0.08s
Test loss: 0.4064 score: 0.7917 time: 0.09s
Epoch 65/1000, LR 0.000268
Train loss: 0.1293;  Loss pred: 0.1293; Loss self: 0.0000; time: 0.15s
Val loss: 0.4517 score: 0.7551 time: 0.08s
Test loss: 0.4038 score: 0.7917 time: 0.08s
Epoch 66/1000, LR 0.000268
Train loss: 0.1341;  Loss pred: 0.1341; Loss self: 0.0000; time: 0.16s
Val loss: 0.4492 score: 0.7755 time: 0.22s
Test loss: 0.4005 score: 0.7917 time: 0.08s
Epoch 67/1000, LR 0.000268
Train loss: 0.1236;  Loss pred: 0.1236; Loss self: 0.0000; time: 0.15s
Val loss: 0.4481 score: 0.7551 time: 0.08s
Test loss: 0.3987 score: 0.7917 time: 0.09s
Epoch 68/1000, LR 0.000268
Train loss: 0.1211;  Loss pred: 0.1211; Loss self: 0.0000; time: 0.15s
Val loss: 0.4452 score: 0.7755 time: 0.08s
Test loss: 0.3946 score: 0.7917 time: 0.09s
Epoch 69/1000, LR 0.000268
Train loss: 0.1179;  Loss pred: 0.1179; Loss self: 0.0000; time: 0.16s
Val loss: 0.4408 score: 0.7959 time: 0.08s
Test loss: 0.3884 score: 0.8125 time: 0.09s
Epoch 70/1000, LR 0.000268
Train loss: 0.1199;  Loss pred: 0.1199; Loss self: 0.0000; time: 0.17s
Val loss: 0.4366 score: 0.8163 time: 0.08s
Test loss: 0.3824 score: 0.8125 time: 0.09s
Epoch 71/1000, LR 0.000268
Train loss: 0.1157;  Loss pred: 0.1157; Loss self: 0.0000; time: 0.17s
Val loss: 0.4317 score: 0.8163 time: 0.08s
Test loss: 0.3756 score: 0.8125 time: 0.09s
Epoch 72/1000, LR 0.000267
Train loss: 0.1102;  Loss pred: 0.1102; Loss self: 0.0000; time: 0.15s
Val loss: 0.4262 score: 0.8163 time: 0.09s
Test loss: 0.3682 score: 0.8125 time: 0.09s
Epoch 73/1000, LR 0.000267
Train loss: 0.1174;  Loss pred: 0.1174; Loss self: 0.0000; time: 0.16s
Val loss: 0.4206 score: 0.8367 time: 0.22s
Test loss: 0.3602 score: 0.8542 time: 0.09s
Epoch 74/1000, LR 0.000267
Train loss: 0.1066;  Loss pred: 0.1066; Loss self: 0.0000; time: 0.17s
Val loss: 0.4152 score: 0.8571 time: 0.10s
Test loss: 0.3524 score: 0.8542 time: 0.10s
Epoch 75/1000, LR 0.000267
Train loss: 0.1056;  Loss pred: 0.1056; Loss self: 0.0000; time: 0.17s
Val loss: 0.4108 score: 0.8571 time: 0.09s
Test loss: 0.3459 score: 0.8542 time: 0.09s
Epoch 76/1000, LR 0.000267
Train loss: 0.1080;  Loss pred: 0.1080; Loss self: 0.0000; time: 0.16s
Val loss: 0.4072 score: 0.8367 time: 0.09s
Test loss: 0.3401 score: 0.8542 time: 0.09s
Epoch 77/1000, LR 0.000267
Train loss: 0.0994;  Loss pred: 0.0994; Loss self: 0.0000; time: 0.17s
Val loss: 0.4041 score: 0.8367 time: 0.09s
Test loss: 0.3350 score: 0.8542 time: 0.09s
Epoch 78/1000, LR 0.000267
Train loss: 0.0993;  Loss pred: 0.0993; Loss self: 0.0000; time: 0.16s
Val loss: 0.4003 score: 0.8367 time: 0.08s
Test loss: 0.3291 score: 0.8750 time: 0.09s
Epoch 79/1000, LR 0.000267
Train loss: 0.0945;  Loss pred: 0.0945; Loss self: 0.0000; time: 0.16s
Val loss: 0.3972 score: 0.8367 time: 0.08s
Test loss: 0.3240 score: 0.8750 time: 0.09s
Epoch 80/1000, LR 0.000267
Train loss: 0.1002;  Loss pred: 0.1002; Loss self: 0.0000; time: 0.17s
Val loss: 0.3939 score: 0.8367 time: 0.08s
Test loss: 0.3190 score: 0.8750 time: 0.09s
Epoch 81/1000, LR 0.000267
Train loss: 0.0874;  Loss pred: 0.0874; Loss self: 0.0000; time: 0.27s
Val loss: 0.3925 score: 0.8367 time: 0.08s
Test loss: 0.3165 score: 0.8750 time: 0.09s
Epoch 82/1000, LR 0.000267
Train loss: 0.0869;  Loss pred: 0.0869; Loss self: 0.0000; time: 0.16s
Val loss: 0.3922 score: 0.8367 time: 0.08s
Test loss: 0.3147 score: 0.8750 time: 0.09s
Epoch 83/1000, LR 0.000266
Train loss: 0.0921;  Loss pred: 0.0921; Loss self: 0.0000; time: 0.16s
Val loss: 0.3886 score: 0.8367 time: 0.08s
Test loss: 0.3084 score: 0.8750 time: 0.09s
Epoch 84/1000, LR 0.000266
Train loss: 0.0884;  Loss pred: 0.0884; Loss self: 0.0000; time: 0.16s
Val loss: 0.3827 score: 0.8367 time: 0.08s
Test loss: 0.2983 score: 0.8750 time: 0.09s
Epoch 85/1000, LR 0.000266
Train loss: 0.0851;  Loss pred: 0.0851; Loss self: 0.0000; time: 0.16s
Val loss: 0.3775 score: 0.8367 time: 0.08s
Test loss: 0.2889 score: 0.8958 time: 0.09s
Epoch 86/1000, LR 0.000266
Train loss: 0.0982;  Loss pred: 0.0982; Loss self: 0.0000; time: 0.16s
Val loss: 0.3740 score: 0.8367 time: 0.08s
Test loss: 0.2817 score: 0.8958 time: 0.09s
Epoch 87/1000, LR 0.000266
Train loss: 0.0802;  Loss pred: 0.0802; Loss self: 0.0000; time: 0.16s
Val loss: 0.3708 score: 0.8571 time: 0.08s
Test loss: 0.2740 score: 0.8958 time: 0.09s
Epoch 88/1000, LR 0.000266
Train loss: 0.0785;  Loss pred: 0.0785; Loss self: 0.0000; time: 0.15s
Val loss: 0.3680 score: 0.8571 time: 0.15s
Test loss: 0.2664 score: 0.9167 time: 0.09s
Epoch 89/1000, LR 0.000266
Train loss: 0.0794;  Loss pred: 0.0794; Loss self: 0.0000; time: 0.16s
Val loss: 0.3639 score: 0.8571 time: 0.08s
Test loss: 0.2560 score: 0.9583 time: 0.08s
Epoch 90/1000, LR 0.000266
Train loss: 0.0865;  Loss pred: 0.0865; Loss self: 0.0000; time: 0.15s
Val loss: 0.3628 score: 0.8571 time: 0.08s
Test loss: 0.2515 score: 0.9583 time: 0.09s
Epoch 91/1000, LR 0.000266
Train loss: 0.0714;  Loss pred: 0.0714; Loss self: 0.0000; time: 0.17s
Val loss: 0.3610 score: 0.8571 time: 0.08s
Test loss: 0.2488 score: 0.9583 time: 0.08s
Epoch 92/1000, LR 0.000266
Train loss: 0.0754;  Loss pred: 0.0754; Loss self: 0.0000; time: 0.17s
Val loss: 0.3589 score: 0.8571 time: 0.09s
Test loss: 0.2458 score: 0.9583 time: 0.09s
Epoch 93/1000, LR 0.000265
Train loss: 0.0685;  Loss pred: 0.0685; Loss self: 0.0000; time: 0.16s
Val loss: 0.3575 score: 0.8571 time: 0.09s
Test loss: 0.2436 score: 0.9583 time: 0.09s
Epoch 94/1000, LR 0.000265
Train loss: 0.0734;  Loss pred: 0.0734; Loss self: 0.0000; time: 0.16s
Val loss: 0.3572 score: 0.8571 time: 0.08s
Test loss: 0.2406 score: 0.9583 time: 0.09s
Epoch 95/1000, LR 0.000265
Train loss: 0.0752;  Loss pred: 0.0752; Loss self: 0.0000; time: 0.16s
Val loss: 0.3578 score: 0.8571 time: 0.08s
Test loss: 0.2365 score: 0.9583 time: 0.09s
     INFO: Early stopping counter 1 of 20
Epoch 96/1000, LR 0.000265
Train loss: 0.0629;  Loss pred: 0.0629; Loss self: 0.0000; time: 0.18s
Val loss: 0.3593 score: 0.8571 time: 0.10s
Test loss: 0.2329 score: 0.9583 time: 0.09s
     INFO: Early stopping counter 2 of 20
Epoch 97/1000, LR 0.000265
Train loss: 0.0631;  Loss pred: 0.0631; Loss self: 0.0000; time: 0.16s
Val loss: 0.3620 score: 0.8571 time: 0.09s
Test loss: 0.2264 score: 0.9583 time: 0.10s
     INFO: Early stopping counter 3 of 20
Epoch 98/1000, LR 0.000265
Train loss: 0.0631;  Loss pred: 0.0631; Loss self: 0.0000; time: 0.16s
Val loss: 0.3635 score: 0.8571 time: 0.09s
Test loss: 0.2213 score: 0.9583 time: 0.09s
     INFO: Early stopping counter 4 of 20
Epoch 99/1000, LR 0.000265
Train loss: 0.0577;  Loss pred: 0.0577; Loss self: 0.0000; time: 0.16s
Val loss: 0.3658 score: 0.8571 time: 0.09s
Test loss: 0.2179 score: 0.9583 time: 0.09s
     INFO: Early stopping counter 5 of 20
Epoch 100/1000, LR 0.000265
Train loss: 0.0611;  Loss pred: 0.0611; Loss self: 0.0000; time: 0.16s
Val loss: 0.3678 score: 0.8571 time: 0.08s
Test loss: 0.2166 score: 0.9375 time: 0.09s
     INFO: Early stopping counter 6 of 20
Epoch 101/1000, LR 0.000265
Train loss: 0.0625;  Loss pred: 0.0625; Loss self: 0.0000; time: 0.16s
Val loss: 0.3706 score: 0.8571 time: 0.09s
Test loss: 0.2159 score: 0.9375 time: 0.09s
     INFO: Early stopping counter 7 of 20
Epoch 102/1000, LR 0.000264
Train loss: 0.0578;  Loss pred: 0.0578; Loss self: 0.0000; time: 0.16s
Val loss: 0.3729 score: 0.8571 time: 0.09s
Test loss: 0.2153 score: 0.9375 time: 0.09s
     INFO: Early stopping counter 8 of 20
Epoch 103/1000, LR 0.000264
Train loss: 0.0560;  Loss pred: 0.0560; Loss self: 0.0000; time: 0.16s
Val loss: 0.3764 score: 0.8571 time: 0.09s
Test loss: 0.2149 score: 0.9583 time: 0.09s
     INFO: Early stopping counter 9 of 20
Epoch 104/1000, LR 0.000264
Train loss: 0.0526;  Loss pred: 0.0526; Loss self: 0.0000; time: 0.17s
Val loss: 0.3790 score: 0.8571 time: 0.09s
Test loss: 0.2158 score: 0.9583 time: 0.09s
     INFO: Early stopping counter 10 of 20
Epoch 105/1000, LR 0.000264
Train loss: 0.0557;  Loss pred: 0.0557; Loss self: 0.0000; time: 0.20s
Val loss: 0.3932 score: 0.8980 time: 0.09s
Test loss: 0.2306 score: 0.9375 time: 0.09s
     INFO: Early stopping counter 11 of 20
Epoch 106/1000, LR 0.000264
Train loss: 0.0556;  Loss pred: 0.0556; Loss self: 0.0000; time: 0.17s
Val loss: 0.4031 score: 0.8571 time: 0.09s
Test loss: 0.2396 score: 0.9375 time: 0.09s
     INFO: Early stopping counter 12 of 20
Epoch 107/1000, LR 0.000264
Train loss: 0.0553;  Loss pred: 0.0553; Loss self: 0.0000; time: 0.17s
Val loss: 0.3952 score: 0.8571 time: 0.09s
Test loss: 0.2240 score: 0.9375 time: 0.09s
     INFO: Early stopping counter 13 of 20
Epoch 108/1000, LR 0.000264
Train loss: 0.0487;  Loss pred: 0.0487; Loss self: 0.0000; time: 0.16s
Val loss: 0.3963 score: 0.8571 time: 0.09s
Test loss: 0.2211 score: 0.9375 time: 0.09s
     INFO: Early stopping counter 14 of 20
Epoch 109/1000, LR 0.000264
Train loss: 0.0598;  Loss pred: 0.0598; Loss self: 0.0000; time: 0.16s
Val loss: 0.4045 score: 0.8571 time: 0.08s
Test loss: 0.2248 score: 0.9375 time: 0.12s
     INFO: Early stopping counter 15 of 20
Epoch 110/1000, LR 0.000263
Train loss: 0.0502;  Loss pred: 0.0502; Loss self: 0.0000; time: 0.17s
Val loss: 0.4177 score: 0.8776 time: 0.10s
Test loss: 0.2397 score: 0.9375 time: 0.10s
     INFO: Early stopping counter 16 of 20
Epoch 111/1000, LR 0.000263
Train loss: 0.0486;  Loss pred: 0.0486; Loss self: 0.0000; time: 0.16s
Val loss: 0.4056 score: 0.8776 time: 0.09s
Test loss: 0.2286 score: 0.9375 time: 0.09s
     INFO: Early stopping counter 17 of 20
Epoch 112/1000, LR 0.000263
Train loss: 0.0464;  Loss pred: 0.0464; Loss self: 0.0000; time: 0.19s
Val loss: 0.3989 score: 0.8571 time: 0.08s
Test loss: 0.2245 score: 0.9375 time: 0.09s
     INFO: Early stopping counter 18 of 20
Epoch 113/1000, LR 0.000263
Train loss: 0.0491;  Loss pred: 0.0491; Loss self: 0.0000; time: 0.15s
Val loss: 0.3963 score: 0.8776 time: 0.09s
Test loss: 0.2250 score: 0.9375 time: 0.09s
     INFO: Early stopping counter 19 of 20
Epoch 114/1000, LR 0.000263
Train loss: 0.0435;  Loss pred: 0.0435; Loss self: 0.0000; time: 0.15s
Val loss: 0.3979 score: 0.8776 time: 0.08s
Test loss: 0.2273 score: 0.9375 time: 0.08s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 093,   Train_Loss: 0.0734,   Val_Loss: 0.3572,   Val_Precision: 0.8750,   Val_Recall: 0.8400,   Val_accuracy: 0.8571,   Val_Score: 0.8571,   Val_Loss: 0.3572,   Test_Precision: 1.0000,   Test_Recall: 0.9167,   Test_accuracy: 0.9565,   Test_Score: 0.9583,   Test_loss: 0.2406


[0.09741156897507608, 0.09895631694234908, 0.10116553399711847, 0.10344235901720822, 0.1035291738808155, 0.22272138088010252, 0.10027373605407774, 0.09795636311173439, 0.1106739689130336, 0.10211200290359557, 0.10046836291439831, 0.10226471000351012, 0.10281036212109029, 0.08790014614351094, 0.08669254113920033, 0.10100126313045621, 0.09804347809404135, 0.08292760397307575, 3.171620680950582, 0.09080365695990622, 0.08545708400197327, 0.08988778293132782, 0.08742039883509278, 0.08624385902658105, 0.08565443102270365, 0.08764484501443803, 0.08783528208732605, 0.09655742486938834, 0.08771153609268367, 0.0874567439313978, 0.0975096239708364, 0.0880998820066452, 0.08670785883441567, 0.08707945700734854, 0.08842433686368167, 0.08351755002513528, 0.08224127418361604, 0.08377406001091003, 0.08382883411832154, 0.08568734396249056, 0.09564232593402267, 0.08774824487045407, 0.08779686293564737, 0.09070023288950324, 0.08894872711971402, 0.08901703706942499, 0.08906035497784615, 0.08928603609092534, 0.0854084868915379, 0.08719704789109528, 0.08460369007661939, 0.08594054100103676, 0.08708238787949085, 0.08672135113738477, 0.09524744004011154, 0.10109067591838539, 0.0989115540869534, 0.09764862922020257, 0.11801036912947893, 0.0976521538104862, 0.09595370385795832, 0.09838725300505757, 0.1083872199524194, 0.09685419593006372, 0.10577020212076604, 0.10841468814760447, 0.09724348108284175, 0.10035247611813247, 0.09781526192091405, 0.09611977799795568, 0.10853976081125438, 0.09756493987515569, 0.09590731118805707, 0.10023064306005836, 0.11173899611458182, 0.09883080492727458, 0.09737148112617433, 0.09567257482558489, 0.095655363984406, 0.10401615197770298, 0.09968829690478742, 0.10479603498242795, 0.10093978815712035, 0.10078080487437546, 0.1016064090654254, 0.14143008994869888, 0.10334397992119193, 0.09823131514713168, 0.10475943610072136, 0.10918216104619205, 0.0969029690604657, 0.0919814680237323, 0.15537149203009903, 0.09353767707943916, 0.09284014208242297, 0.09164779097773135, 0.09827104001305997, 0.10406401683576405, 0.10297642718069255, 0.10770528297871351, 0.09902399498969316, 0.09258669312112033, 0.09294618805870414, 0.09384931693784893, 0.09746538195759058, 0.09464111598208547, 0.0953985380474478, 0.09456452797167003, 0.09490724978968501, 0.0985156400129199, 0.10523754986934364, 0.09832567209377885, 0.10185144282877445, 0.09370510117150843, 0.09060434997081757, 0.09147822391241789, 0.09139240090735257, 0.1035741618834436, 0.09187386999838054, 0.10634406586177647, 0.09941810602322221, 0.09194413898512721, 0.09244802710600197, 0.09262080397456884, 0.09227671707049012, 0.09131854795850813, 0.09238261985592544, 0.09335393784567714, 0.09249254991300404, 0.09125688392668962, 0.10004275105893612, 0.09270725422538817, 0.09193095401860774, 0.09984877519309521, 0.10682827117852867, 0.08996655908413231, 0.08925869385711849, 0.08807417401112616, 0.10076884599402547, 0.09270090493373573, 0.09902157308533788, 0.09155292599461973, 0.08961294800974429, 0.09185756486840546, 0.09076766506768763, 0.11788771999999881, 0.09123546699993312, 0.09082861593924463, 0.08978520194068551, 0.0889927139505744, 0.09042929811403155, 0.09077289700508118, 0.09142584400251508, 0.09302608389407396, 0.09379139496013522, 0.09143022005446255, 0.09527187189087272, 0.10048983595333993, 0.09589275089092553, 0.09367541689425707, 0.09455713792704046, 0.09228024701587856, 0.09248572192154825, 0.09965071687474847, 0.09058628999628127, 0.09721630997955799, 0.0913478909060359, 0.09173824195750058, 0.091741245938465, 0.09140742989256978, 0.09091791906394064, 0.09126075799576938, 0.08945048600435257, 0.09152724593877792, 0.08865333395078778, 0.099788618972525, 0.09441619203425944, 0.0901470819953829, 0.0908415000885725, 0.09411879791878164, 0.10479480098001659, 0.09541044407524168, 0.09423618600703776, 0.09477775101549923, 0.09895691298879683, 0.0958016780205071, 0.09626109502278268, 0.09834760916419327, 0.09521706309169531, 0.095762032084167, 0.09270656295120716, 0.09148464887402952, 0.1263012478593737, 0.10905926604755223, 0.09689853200688958, 0.09423345909453928, 0.09295348892919719, 0.09019935713149607]
[0.001987991203572981, 0.0020195166722928384, 0.0020646027346350707, 0.002111068551371596, 0.002112840283281949, 0.004545334303675562, 0.0020464027766138315, 0.0019991094512598856, 0.002258652426796604, 0.002083918426603991, 0.0020503747533550678, 0.0020870348980308187, 0.002098170655532455, 0.0017938805335410396, 0.001769235533453068, 0.002061250267968494, 0.00200088730804166, 0.0016924000810831785, 0.06472695267246086, 0.0018531358563246168, 0.0017440221224892505, 0.0018344445496189352, 0.0017840897721447507, 0.0017600787556445111, 0.0017480496127082377, 0.0017886703064171026, 0.0017925567772923683, 0.001970559691212007, 0.001790031348830279, 0.0017848315088040366, 0.001989992325935437, 0.0017979567756458204, 0.001769548139477871, 0.0017771317756601743, 0.0018045783033404424, 0.0017044397964313322, 0.0016783933506860416, 0.0017096746941002048, 0.0017107925330269703, 0.0017487213053569502, 0.0019518842027351564, 0.0017907805075602873, 0.0017917727129723952, 0.0018510251610102703, 0.0018152801453002862, 0.0018166742259066325, 0.001817558264854003, 0.0018221640018556192, 0.0017430303447252633, 0.0017795315896141895, 0.0017266059199310079, 0.001753888591857893, 0.0017771915893773644, 0.0017698234925996891, 0.001943825306941052, 0.002063075018742559, 0.002018603144631702, 0.001992829167759236, 0.0024083748801934477, 0.0019929010981731875, 0.001958238854244047, 0.0020079031225521955, 0.0022119840806616204, 0.001976616243470688, 0.002158575553485021, 0.0022125446560735604, 0.001984560838425342, 0.002048009716696581, 0.001996229835120695, 0.001961628122407259, 0.0022150971594133546, 0.001991121221941953, 0.001957292065062389, 0.002045523327756293, 0.002280387675807792, 0.0020169552025974405, 0.001987173084207639, 0.0019525015270527527, 0.0019521502853960407, 0.0021227786117898567, 0.0020344550388732125, 0.0021386945914781217, 0.0020599956766759256, 0.0020567511198852136, 0.0021168001888630292, 0.00294646020726456, 0.0021529995816914984, 0.00204648573223191, 0.0021824882520983615, 0.002274628355129001, 0.0020188118554263688, 0.0019162805838277563, 0.0032369060839603967, 0.0019487016058216493, 0.0019341696267171453, 0.0019093289787027363, 0.002047313333605416, 0.0021680003507450842, 0.0021453422329310947, 0.0022438600620565317, 0.0020629998956186077, 0.0019288894400233403, 0.0019363789178896695, 0.0019551941028718525, 0.002030528790783137, 0.0019716899162934474, 0.0019874695426551625, 0.0019700943327431255, 0.001977234370618438, 0.0020524091669358313, 0.002192448955611326, 0.002048451501953726, 0.0021219050589328012, 0.001952189607739759, 0.0018875906243920326, 0.001905796331508706, 0.0019040083522365119, 0.002157795039238408, 0.0019140389582995947, 0.002215501372120343, 0.0020712105421504625, 0.0019155028955234836, 0.0019260005647083744, 0.0019296000828035176, 0.0019224316056352109, 0.0019024697491355862, 0.0019246379136651133, 0.0019448737051182736, 0.0019269281231875841, 0.0019011850818060338, 0.0020842239803945026, 0.0019314011296955869, 0.0019152282087209944, 0.0020801828165228167, 0.002225588982886014, 0.0018743033142527565, 0.001859556122023302, 0.001834878625231795, 0.002099350958208864, 0.001931268852786161, 0.0020629494392778724, 0.001907352624887911, 0.0018669364168696727, 0.0019136992680917804, 0.0018909930222434923, 0.002455994166666642, 0.00190073889583194, 0.0018922628320675965, 0.0018705250404309481, 0.0018540148739703, 0.0018839437107089907, 0.0018911020209391911, 0.0019047050833857309, 0.0019380434144598742, 0.0019539873950028173, 0.0019047962511346366, 0.0019848306643931815, 0.0020935382490279153, 0.0019977656435609483, 0.0019515711852970223, 0.0019699403734800094, 0.0019225051461641367, 0.0019267858733655885, 0.00207605660155726, 0.0018872143749225263, 0.0020253397912407913, 0.0019030810605424147, 0.0019112133741145954, 0.0019112759570513542, 0.0019043214560952038, 0.0018941233138320968, 0.0019012657915785287, 0.0018635517917573452, 0.00190681762372454, 0.0018469444573080789, 0.002078929561927604, 0.001967004000713738, 0.001878064208237144, 0.0018925312518452604, 0.0019608082899746173, 0.0021832250204170123, 0.0019877175849008686, 0.00196325387514662, 0.0019745364794895672, 0.0020616023539332673, 0.0019958682920938977, 0.0020054394796413058, 0.0020489085242540264, 0.001983688814410319, 0.0019950423350868127, 0.0019313867281501491, 0.001905930184875615, 0.002631275997070285, 0.002272068042657338, 0.0020187194168101996, 0.0019631970644695684, 0.0019365310193582748, 0.001879153273572835]
[503.0203343972135, 495.1679843596735, 484.35468152024663, 473.6937601340721, 473.2965420588559, 220.00581985605658, 488.6623549517916, 500.2227363638227, 442.7418703896276, 479.8652323592275, 487.7157204377789, 479.1486720914588, 476.6056551993045, 557.450722778091, 565.2158692790164, 485.14244754256345, 499.77827136038746, 590.876832953101, 15.44951459495275, 539.6258437216426, 573.3872220454954, 545.1241359177238, 560.5099113358214, 568.1563945891824, 572.0661431632417, 559.0745239144192, 557.8623855421115, 507.4700372993739, 558.6494340746959, 560.276975763427, 502.51450066769956, 556.1868970074673, 565.1160189940148, 562.7044734082913, 554.1460839626117, 586.7030340958644, 595.8078894862465, 584.9065927283297, 584.5244123381004, 571.8464096804032, 512.3254743281951, 558.4157275435023, 558.1065013213015, 540.241170711159, 550.8791591143518, 550.4564251198855, 550.1886895935774, 548.7980220120911, 573.7134772358884, 561.9456298703889, 579.1709552576758, 570.1616423313984, 562.6855348501554, 565.0280969720334, 514.4495219962304, 484.7133482375733, 495.3920747916262, 501.799158793131, 415.21775045240344, 501.7810471963008, 510.6629346224657, 498.0319960501506, 452.0828195566818, 505.91509773496875, 463.2684727599641, 451.96827881188443, 503.88981815919243, 488.2789333699988, 500.9443213434081, 509.78061977049254, 451.44746619820484, 502.2295925431872, 510.90995454892663, 488.8724496224086, 438.5219279198943, 495.7968321320162, 503.22742792117555, 512.163491881859, 512.2556431648529, 471.0806838009514, 491.5321208346055, 467.5749422028825, 485.43791199291826, 486.2036978279655, 472.4111445479026, 339.3902953566041, 464.4682741714016, 488.6425467082997, 458.19261525854546, 439.6322580544315, 495.340859680459, 521.8442478827956, 308.93698305157096, 513.1621983645673, 517.0177352527732, 523.7442112670569, 488.4450189355981, 461.25453792308036, 466.12609617708415, 445.6605903861424, 484.73099883514135, 518.4330315935058, 516.4278493022603, 511.4581710998246, 492.4825516087948, 507.1791419818621, 503.15236462142144, 507.589907437385, 505.7569374981181, 487.2322810236528, 456.11096096016865, 488.173627272231, 471.2746198470085, 512.2453249599038, 529.77588841441, 524.715040881813, 525.2077801157577, 463.4360455073384, 522.455405447121, 451.3651007324591, 482.8094390451182, 522.0561150479035, 519.2106473506747, 518.2421004807893, 520.1745524099306, 525.6325365774484, 519.5782504854052, 514.1722042764659, 518.9607167836489, 525.9877165930885, 479.7948826069642, 517.7588356063624, 522.1309896368999, 480.72697844489255, 449.3192623119739, 533.5315753836129, 537.7627424935923, 544.9951763832187, 476.3376967008821, 517.7942980633389, 484.74285455587614, 524.2869026689634, 535.6368813442072, 522.5481436261073, 528.8226811189342, 407.1670908556081, 526.1111887555219, 528.4678127442486, 534.6092558962007, 539.3699986120065, 530.8014216749962, 528.7922010169303, 525.0156618590204, 515.9843131164822, 511.774028101424, 524.9905334517151, 503.8213173241799, 477.6602483686774, 500.5592138512976, 512.4076475067465, 507.6295777589675, 520.1546544596993, 518.9990303661833, 481.68243546437765, 529.8815085811605, 493.74431111500866, 525.4636918697414, 523.2278161841915, 523.2106835805977, 525.1214267419389, 527.9487310553448, 525.9653881269007, 536.6097172201431, 524.4340033142365, 541.4347984549031, 481.0167782081034, 508.3873747268152, 532.4631584021591, 528.3928595762831, 509.9937638538569, 458.03798996815846, 503.0895775115216, 509.35847505983804, 506.4479741891158, 485.0595936176203, 501.0350652702057, 498.6438185503661, 488.06473698677365, 504.11132670386354, 501.2424961681256, 517.7626963180925, 524.6781901747687, 380.0437510597215, 440.1276639719082, 495.3635416952153, 509.37321479246793, 516.3872873729534, 532.1545687961363]
Elapsed: 0.11197636460161044~0.21834833779527724
Time per graph: 0.0023088504715179222~0.00445455458166631
Speed: 505.0170889768719~57.05889165910118
Total Time: 0.0915
best val loss: 0.35721635818481445 test_score: 0.9583

Testing...
Test loss: 0.2306 score: 0.9375 time: 0.09s
test Score 0.9375
Epoch Time List: [1.4453481631353498, 0.37092998228035867, 0.37852770229801536, 0.3799065051134676, 0.3859045640565455, 0.5010023349896073, 0.379776333225891, 0.3636050350032747, 0.3804422430694103, 0.3849370137322694, 0.37968809902668, 0.3884903888683766, 0.4302778970450163, 0.3589623919688165, 0.33274509315378964, 0.3529301641974598, 0.3507317751646042, 0.34062226000241935, 5.7151219139341265, 1.11371267773211, 0.39827057789079845, 0.3350895349867642, 0.35606926190666854, 0.3314613699913025, 0.3392307893373072, 0.34440213115885854, 0.35132868122309446, 0.43896899465471506, 0.33831499912776053, 0.33191809081472456, 0.34606611728668213, 0.33944427501410246, 0.3408503467217088, 0.3412421189714223, 0.4108485400211066, 0.33264887891709805, 0.3249750768300146, 0.3268024430144578, 0.33577086916193366, 0.3394197609741241, 0.36330419033765793, 0.46137530426494777, 0.3486159658059478, 0.352146907011047, 0.3565537401009351, 0.34908419009298086, 0.3434180251788348, 0.34956955001689494, 0.47943836683407426, 0.34163705608807504, 0.3355522339697927, 0.33339443407021463, 0.34128523408435285, 0.3457679399289191, 0.3330588152166456, 0.3387430552393198, 0.32687725918367505, 0.3329435798805207, 0.3818600228987634, 0.33139511989429593, 0.4652004719246179, 0.3508102830965072, 0.33518899395130575, 0.3292790160048753, 0.35643007373437285, 0.3931357772089541, 0.568860923871398, 0.3366872239857912, 0.3379405518062413, 0.3304983333218843, 0.35197992715984583, 0.3413226299453527, 0.3322870072443038, 0.3369435132481158, 0.5572187257930636, 0.35113626811653376, 0.32276370096951723, 0.33024095091968775, 0.32895444775931537, 0.3298898870125413, 0.4463914141524583, 0.3357103141024709, 0.35081540374085307, 0.33372348826378584, 0.3371368879452348, 0.38705108501017094, 0.39284973801113665, 0.34609244321472943, 0.36197000881657004, 0.38294788100756705, 0.3475254268851131, 0.333956208312884, 0.41654106182977557, 0.3515986220445484, 0.34448916814289987, 0.3310938598588109, 0.34625381301157176, 0.36733113788068295, 0.34853926463983953, 0.3606363870203495, 0.35347857302986085, 0.3355833091773093, 0.3402375739533454, 0.34373347111977637, 0.3325336647685617, 0.34644457418471575, 0.34527052706107497, 0.34893712401390076, 0.3472839726600796, 0.35465284809470177, 0.3635197728872299, 0.3485622243024409, 0.3417598169762641, 0.33879615692421794, 0.32384842331521213, 0.33847923297435045, 0.34226762200705707, 0.36031653289683163, 0.3359102439135313, 0.3470120548736304, 0.36223103501833975, 0.3242436780128628, 0.3201914329547435, 0.3335923191625625, 0.322740392992273, 0.32736582891084254, 0.3382181969936937, 0.33374690311029553, 0.3328682400751859, 0.3333679602947086, 0.3374559360090643, 0.35063443589024246, 0.33971512503921986, 0.3347551648039371, 0.4295252258889377, 0.32258471893146634, 0.31165701034478843, 0.31472237105481327, 0.32465964811854064, 0.31927476404234767, 0.33163874386809766, 0.32552079297602177, 0.36724804691039026, 0.32561499113216996, 0.3196170327719301, 0.3544635579455644, 0.33417336107231677, 0.3323170398361981, 0.3180364689324051, 0.45717433211393654, 0.318592319265008, 0.32196297589689493, 0.32928087608888745, 0.342100084759295, 0.33789355610497296, 0.32543356413953006, 0.46965366299264133, 0.35632767784409225, 0.3479113739449531, 0.3389156151097268, 0.3474079589359462, 0.3336592139676213, 0.3332273098640144, 0.34264735504984856, 0.43755044508725405, 0.33237370918504894, 0.32353696902282536, 0.3296336019411683, 0.3285467028617859, 0.33265238627791405, 0.32820448325946927, 0.39300348586402833, 0.32586892205290496, 0.320721291936934, 0.3304850042331964, 0.3444068010430783, 0.33571389806456864, 0.3256254359148443, 0.3259591921232641, 0.36773003404960036, 0.34828151599504054, 0.3376323338598013, 0.3317312451545149, 0.33754960005171597, 0.3423311098013073, 0.33703874913044274, 0.3384168283082545, 0.3513511929195374, 0.3792317241895944, 0.3428253692109138, 0.3508886559866369, 0.33778968011029065, 0.3656521087978035, 0.3661387187894434, 0.34942303015850484, 0.36301837977953255, 0.32925567403435707, 0.3212313330732286]
Total Epoch List: [54, 30, 114]
Total Time List: [0.08714983006939292, 0.1013303748331964, 0.09150609513744712]
========================training times:1========================
========================k_idx:0========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: SGFormer
SGFormer(
  (trans_conv): SGModule(
    (attns): ModuleList(
      (0-1): 2 x SGFormerAttention(heads=1, head_channels=64)
    )
    (fcs): ModuleList(
      (0): Linear(in_features=14887, out_features=64, bias=True)
    )
    (bns): ModuleList(
      (0-2): 3 x LayerNorm((64,), eps=1e-05, elementwise_affine=True)
    )
  )
  (graph_conv): GraphModule(
    (convs): ModuleList(
      (0-2): 3 x GCNConv(64, 64)
    )
    (fcs): ModuleList(
      (0): Linear(in_features=14887, out_features=64, bias=True)
    )
    (bns): ModuleList(
      (0-3): 4 x BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (classifier): Linear(in_features=64, out_features=2, bias=True)
)
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7e09d87ed150>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.7753;  Loss pred: 0.7753; Loss self: 0.0000; time: 0.15s
Val loss: 0.6902 score: 0.5102 time: 0.08s
Test loss: 0.7049 score: 0.3878 time: 0.09s
Epoch 2/1000, LR 0.000000
Train loss: 0.7232;  Loss pred: 0.7232; Loss self: 0.0000; time: 0.17s
Val loss: 0.6892 score: 0.5102 time: 0.08s
Test loss: 0.7035 score: 0.3673 time: 0.09s
Epoch 3/1000, LR 0.000030
Train loss: 0.7611;  Loss pred: 0.7611; Loss self: 0.0000; time: 0.16s
Val loss: 0.6874 score: 0.5306 time: 0.08s
Test loss: 0.7016 score: 0.3673 time: 0.09s
Epoch 4/1000, LR 0.000060
Train loss: 0.7550;  Loss pred: 0.7550; Loss self: 0.0000; time: 0.30s
Val loss: 0.6851 score: 0.5102 time: 0.08s
Test loss: 0.6990 score: 0.4082 time: 0.09s
Epoch 5/1000, LR 0.000090
Train loss: 0.7380;  Loss pred: 0.7380; Loss self: 0.0000; time: 0.16s
Val loss: 0.6822 score: 0.5102 time: 0.09s
Test loss: 0.6960 score: 0.4490 time: 0.09s
Epoch 6/1000, LR 0.000120
Train loss: 0.7227;  Loss pred: 0.7227; Loss self: 0.0000; time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6787 score: 0.5102 time: 0.08s
Test loss: 0.6925 score: 0.4694 time: 0.09s
Epoch 7/1000, LR 0.000150
Train loss: 0.6700;  Loss pred: 0.6700; Loss self: 0.0000; time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6753 score: 0.5102 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6890 score: 0.4898 time: 0.09s
Epoch 8/1000, LR 0.000180
Train loss: 0.6729;  Loss pred: 0.6729; Loss self: 0.0000; time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6719 score: 0.5102 time: 0.09s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6853 score: 0.4898 time: 0.09s
Epoch 9/1000, LR 0.000210
Train loss: 0.6553;  Loss pred: 0.6553; Loss self: 0.0000; time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6689 score: 0.5102 time: 0.09s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6821 score: 0.4898 time: 0.10s
Epoch 10/1000, LR 0.000240
Train loss: 0.6251;  Loss pred: 0.6251; Loss self: 0.0000; time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6666 score: 0.5102 time: 0.09s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6794 score: 0.4898 time: 0.10s
Epoch 11/1000, LR 0.000270
Train loss: 0.6172;  Loss pred: 0.6172; Loss self: 0.0000; time: 0.29s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6648 score: 0.5102 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6779 score: 0.4898 time: 0.10s
Epoch 12/1000, LR 0.000270
Train loss: 0.5895;  Loss pred: 0.5895; Loss self: 0.0000; time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6628 score: 0.5102 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6763 score: 0.4898 time: 0.09s
Epoch 13/1000, LR 0.000270
Train loss: 0.5639;  Loss pred: 0.5639; Loss self: 0.0000; time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6607 score: 0.5102 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6744 score: 0.4898 time: 0.09s
Epoch 14/1000, LR 0.000270
Train loss: 0.5643;  Loss pred: 0.5643; Loss self: 0.0000; time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6583 score: 0.5102 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6720 score: 0.4898 time: 0.09s
Epoch 15/1000, LR 0.000270
Train loss: 0.5390;  Loss pred: 0.5390; Loss self: 0.0000; time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6549 score: 0.5102 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6687 score: 0.4898 time: 0.09s
Epoch 16/1000, LR 0.000270
Train loss: 0.5438;  Loss pred: 0.5438; Loss self: 0.0000; time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6508 score: 0.5102 time: 0.09s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6640 score: 0.4898 time: 0.09s
Epoch 17/1000, LR 0.000270
Train loss: 0.5283;  Loss pred: 0.5283; Loss self: 0.0000; time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6457 score: 0.5102 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6587 score: 0.4898 time: 0.09s
Epoch 18/1000, LR 0.000270
Train loss: 0.5131;  Loss pred: 0.5131; Loss self: 0.0000; time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6395 score: 0.5102 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6519 score: 0.4898 time: 0.09s
Epoch 19/1000, LR 0.000270
Train loss: 0.5019;  Loss pred: 0.5019; Loss self: 0.0000; time: 0.16s
Val loss: 0.6322 score: 0.5306 time: 0.09s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6437 score: 0.4898 time: 0.10s
Epoch 20/1000, LR 0.000270
Train loss: 0.4695;  Loss pred: 0.4695; Loss self: 0.0000; time: 0.16s
Val loss: 0.6241 score: 0.5714 time: 0.09s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6346 score: 0.4898 time: 0.10s
Epoch 21/1000, LR 0.000270
Train loss: 0.4651;  Loss pred: 0.4651; Loss self: 0.0000; time: 0.17s
Val loss: 0.6149 score: 0.5918 time: 0.09s
Test loss: 0.6247 score: 0.5102 time: 0.10s
Epoch 22/1000, LR 0.000270
Train loss: 0.4566;  Loss pred: 0.4566; Loss self: 0.0000; time: 0.15s
Val loss: 0.6050 score: 0.6327 time: 0.09s
Test loss: 0.6138 score: 0.5102 time: 0.09s
Epoch 23/1000, LR 0.000270
Train loss: 0.4409;  Loss pred: 0.4409; Loss self: 0.0000; time: 0.16s
Val loss: 0.5956 score: 0.6531 time: 0.08s
Test loss: 0.6038 score: 0.5510 time: 0.09s
Epoch 24/1000, LR 0.000270
Train loss: 0.4190;  Loss pred: 0.4190; Loss self: 0.0000; time: 0.16s
Val loss: 0.5856 score: 0.6939 time: 0.08s
Test loss: 0.5929 score: 0.5918 time: 0.09s
Epoch 25/1000, LR 0.000270
Train loss: 0.4247;  Loss pred: 0.4247; Loss self: 0.0000; time: 0.15s
Val loss: 0.5762 score: 0.7143 time: 0.09s
Test loss: 0.5824 score: 0.6327 time: 0.09s
Epoch 26/1000, LR 0.000270
Train loss: 0.4032;  Loss pred: 0.4032; Loss self: 0.0000; time: 0.15s
Val loss: 0.5675 score: 0.7347 time: 0.08s
Test loss: 0.5724 score: 0.6531 time: 0.09s
Epoch 27/1000, LR 0.000270
Train loss: 0.3894;  Loss pred: 0.3894; Loss self: 0.0000; time: 0.16s
Val loss: 0.5587 score: 0.7755 time: 0.19s
Test loss: 0.5615 score: 0.7143 time: 0.09s
Epoch 28/1000, LR 0.000270
Train loss: 0.3860;  Loss pred: 0.3860; Loss self: 0.0000; time: 0.15s
Val loss: 0.5502 score: 0.7959 time: 0.08s
Test loss: 0.5506 score: 0.7143 time: 0.09s
Epoch 29/1000, LR 0.000270
Train loss: 0.3677;  Loss pred: 0.3677; Loss self: 0.0000; time: 0.15s
Val loss: 0.5427 score: 0.7959 time: 0.08s
Test loss: 0.5409 score: 0.7551 time: 0.09s
Epoch 30/1000, LR 0.000270
Train loss: 0.3553;  Loss pred: 0.3553; Loss self: 0.0000; time: 0.17s
Val loss: 0.5358 score: 0.7959 time: 0.10s
Test loss: 0.5321 score: 0.7959 time: 0.10s
Epoch 31/1000, LR 0.000270
Train loss: 0.3581;  Loss pred: 0.3581; Loss self: 0.0000; time: 0.16s
Val loss: 0.5301 score: 0.7959 time: 0.09s
Test loss: 0.5252 score: 0.8163 time: 0.10s
Epoch 32/1000, LR 0.000270
Train loss: 0.3490;  Loss pred: 0.3490; Loss self: 0.0000; time: 0.16s
Val loss: 0.5256 score: 0.7959 time: 0.09s
Test loss: 0.5206 score: 0.8163 time: 0.09s
Epoch 33/1000, LR 0.000270
Train loss: 0.3346;  Loss pred: 0.3346; Loss self: 0.0000; time: 0.16s
Val loss: 0.5228 score: 0.7959 time: 0.08s
Test loss: 0.5187 score: 0.7959 time: 0.09s
Epoch 34/1000, LR 0.000270
Train loss: 0.3053;  Loss pred: 0.3053; Loss self: 0.0000; time: 0.16s
Val loss: 0.5210 score: 0.7959 time: 0.08s
Test loss: 0.5184 score: 0.7959 time: 0.09s
Epoch 35/1000, LR 0.000270
Train loss: 0.3147;  Loss pred: 0.3147; Loss self: 0.0000; time: 0.14s
Val loss: 0.5197 score: 0.7959 time: 0.08s
Test loss: 0.5188 score: 0.7551 time: 0.09s
Epoch 36/1000, LR 0.000270
Train loss: 0.3080;  Loss pred: 0.3080; Loss self: 0.0000; time: 0.16s
Val loss: 0.5187 score: 0.7755 time: 0.15s
Test loss: 0.5199 score: 0.7347 time: 0.09s
Epoch 37/1000, LR 0.000270
Train loss: 0.2865;  Loss pred: 0.2865; Loss self: 0.0000; time: 0.14s
Val loss: 0.5184 score: 0.7755 time: 0.08s
Test loss: 0.5219 score: 0.7143 time: 0.09s
Epoch 38/1000, LR 0.000270
Train loss: 0.2873;  Loss pred: 0.2873; Loss self: 0.0000; time: 0.15s
Val loss: 0.5178 score: 0.7551 time: 0.08s
Test loss: 0.5233 score: 0.7143 time: 0.09s
Epoch 39/1000, LR 0.000269
Train loss: 0.2813;  Loss pred: 0.2813; Loss self: 0.0000; time: 0.14s
Val loss: 0.5165 score: 0.7551 time: 0.08s
Test loss: 0.5233 score: 0.7143 time: 0.09s
Epoch 40/1000, LR 0.000269
Train loss: 0.2688;  Loss pred: 0.2688; Loss self: 0.0000; time: 0.14s
Val loss: 0.5145 score: 0.7551 time: 0.08s
Test loss: 0.5220 score: 0.7143 time: 0.09s
Epoch 41/1000, LR 0.000269
Train loss: 0.2656;  Loss pred: 0.2656; Loss self: 0.0000; time: 0.15s
Val loss: 0.5121 score: 0.7551 time: 0.08s
Test loss: 0.5198 score: 0.7143 time: 0.09s
Epoch 42/1000, LR 0.000269
Train loss: 0.2501;  Loss pred: 0.2501; Loss self: 0.0000; time: 0.15s
Val loss: 0.5085 score: 0.7551 time: 0.08s
Test loss: 0.5156 score: 0.7143 time: 0.09s
Epoch 43/1000, LR 0.000269
Train loss: 0.2440;  Loss pred: 0.2440; Loss self: 0.0000; time: 0.16s
Val loss: 0.5032 score: 0.7755 time: 1.00s
Test loss: 0.5088 score: 0.7347 time: 3.14s
Epoch 44/1000, LR 0.000269
Train loss: 0.2459;  Loss pred: 0.2459; Loss self: 0.0000; time: 1.39s
Val loss: 0.4976 score: 0.7959 time: 1.08s
Test loss: 0.5012 score: 0.7551 time: 0.09s
Epoch 45/1000, LR 0.000269
Train loss: 0.2564;  Loss pred: 0.2564; Loss self: 0.0000; time: 0.16s
Val loss: 0.4916 score: 0.7959 time: 0.09s
Test loss: 0.4931 score: 0.7755 time: 0.10s
Epoch 46/1000, LR 0.000269
Train loss: 0.2308;  Loss pred: 0.2308; Loss self: 0.0000; time: 0.16s
Val loss: 0.4858 score: 0.7959 time: 0.09s
Test loss: 0.4854 score: 0.7959 time: 0.09s
Epoch 47/1000, LR 0.000269
Train loss: 0.2296;  Loss pred: 0.2296; Loss self: 0.0000; time: 0.16s
Val loss: 0.4802 score: 0.7959 time: 0.08s
Test loss: 0.4780 score: 0.7959 time: 0.09s
Epoch 48/1000, LR 0.000269
Train loss: 0.2321;  Loss pred: 0.2321; Loss self: 0.0000; time: 0.16s
Val loss: 0.4750 score: 0.8163 time: 0.09s
Test loss: 0.4710 score: 0.8163 time: 0.10s
Epoch 49/1000, LR 0.000269
Train loss: 0.2042;  Loss pred: 0.2042; Loss self: 0.0000; time: 0.16s
Val loss: 0.4690 score: 0.8163 time: 0.09s
Test loss: 0.4620 score: 0.8163 time: 0.09s
Epoch 50/1000, LR 0.000269
Train loss: 0.2073;  Loss pred: 0.2073; Loss self: 0.0000; time: 0.16s
Val loss: 0.4640 score: 0.8163 time: 0.08s
Test loss: 0.4547 score: 0.8367 time: 0.10s
Epoch 51/1000, LR 0.000269
Train loss: 0.2014;  Loss pred: 0.2014; Loss self: 0.0000; time: 0.16s
Val loss: 0.4609 score: 0.8163 time: 0.09s
Test loss: 0.4512 score: 0.8571 time: 0.21s
Epoch 52/1000, LR 0.000269
Train loss: 0.1879;  Loss pred: 0.1879; Loss self: 0.0000; time: 0.16s
Val loss: 0.4588 score: 0.8163 time: 0.09s
Test loss: 0.4495 score: 0.8367 time: 0.09s
Epoch 53/1000, LR 0.000269
Train loss: 0.2019;  Loss pred: 0.2019; Loss self: 0.0000; time: 0.16s
Val loss: 0.4576 score: 0.8163 time: 0.08s
Test loss: 0.4497 score: 0.8163 time: 0.09s
Epoch 54/1000, LR 0.000269
Train loss: 0.1941;  Loss pred: 0.1941; Loss self: 0.0000; time: 0.16s
Val loss: 0.4564 score: 0.8163 time: 0.09s
Test loss: 0.4500 score: 0.8163 time: 0.09s
Epoch 55/1000, LR 0.000269
Train loss: 0.1787;  Loss pred: 0.1787; Loss self: 0.0000; time: 0.16s
Val loss: 0.4546 score: 0.8163 time: 0.08s
Test loss: 0.4491 score: 0.8163 time: 0.10s
Epoch 56/1000, LR 0.000269
Train loss: 0.1690;  Loss pred: 0.1690; Loss self: 0.0000; time: 0.16s
Val loss: 0.4518 score: 0.8163 time: 0.09s
Test loss: 0.4462 score: 0.8163 time: 0.09s
Epoch 57/1000, LR 0.000269
Train loss: 0.1695;  Loss pred: 0.1695; Loss self: 0.0000; time: 0.16s
Val loss: 0.4488 score: 0.8163 time: 0.09s
Test loss: 0.4428 score: 0.8163 time: 0.10s
Epoch 58/1000, LR 0.000269
Train loss: 0.1614;  Loss pred: 0.1614; Loss self: 0.0000; time: 0.16s
Val loss: 0.4452 score: 0.8163 time: 0.08s
Test loss: 0.4386 score: 0.8163 time: 0.09s
Epoch 59/1000, LR 0.000268
Train loss: 0.1634;  Loss pred: 0.1634; Loss self: 0.0000; time: 0.16s
Val loss: 0.4401 score: 0.8163 time: 0.08s
Test loss: 0.4317 score: 0.8367 time: 0.09s
Epoch 60/1000, LR 0.000268
Train loss: 0.1708;  Loss pred: 0.1708; Loss self: 0.0000; time: 0.16s
Val loss: 0.4354 score: 0.8163 time: 0.08s
Test loss: 0.4254 score: 0.8571 time: 0.10s
Epoch 61/1000, LR 0.000268
Train loss: 0.1640;  Loss pred: 0.1640; Loss self: 0.0000; time: 0.16s
Val loss: 0.4304 score: 0.8163 time: 0.08s
Test loss: 0.4185 score: 0.8571 time: 0.09s
Epoch 62/1000, LR 0.000268
Train loss: 0.1547;  Loss pred: 0.1547; Loss self: 0.0000; time: 0.16s
Val loss: 0.4245 score: 0.8163 time: 0.09s
Test loss: 0.4100 score: 0.8571 time: 0.10s
Epoch 63/1000, LR 0.000268
Train loss: 0.1480;  Loss pred: 0.1480; Loss self: 0.0000; time: 0.16s
Val loss: 0.4194 score: 0.8367 time: 0.08s
Test loss: 0.4027 score: 0.8571 time: 0.09s
Epoch 64/1000, LR 0.000268
Train loss: 0.1436;  Loss pred: 0.1436; Loss self: 0.0000; time: 0.16s
Val loss: 0.4142 score: 0.8367 time: 0.08s
Test loss: 0.3957 score: 0.8776 time: 0.09s
Epoch 65/1000, LR 0.000268
Train loss: 0.1399;  Loss pred: 0.1399; Loss self: 0.0000; time: 0.15s
Val loss: 0.4099 score: 0.8367 time: 0.08s
Test loss: 0.3907 score: 0.8776 time: 0.09s
Epoch 66/1000, LR 0.000268
Train loss: 0.1384;  Loss pred: 0.1384; Loss self: 0.0000; time: 0.16s
Val loss: 0.4077 score: 0.8367 time: 0.08s
Test loss: 0.3894 score: 0.8776 time: 0.09s
Epoch 67/1000, LR 0.000268
Train loss: 0.1337;  Loss pred: 0.1337; Loss self: 0.0000; time: 0.16s
Val loss: 0.4063 score: 0.8367 time: 0.08s
Test loss: 0.3897 score: 0.8571 time: 0.09s
Epoch 68/1000, LR 0.000268
Train loss: 0.1315;  Loss pred: 0.1315; Loss self: 0.0000; time: 0.15s
Val loss: 0.4052 score: 0.8163 time: 0.09s
Test loss: 0.3899 score: 0.8571 time: 0.09s
Epoch 69/1000, LR 0.000268
Train loss: 0.1374;  Loss pred: 0.1374; Loss self: 0.0000; time: 0.16s
Val loss: 0.4024 score: 0.8163 time: 0.08s
Test loss: 0.3881 score: 0.8571 time: 0.09s
Epoch 70/1000, LR 0.000268
Train loss: 0.1197;  Loss pred: 0.1197; Loss self: 0.0000; time: 0.16s
Val loss: 0.3951 score: 0.8367 time: 0.09s
Test loss: 0.3779 score: 0.8776 time: 0.09s
Epoch 71/1000, LR 0.000268
Train loss: 0.1203;  Loss pred: 0.1203; Loss self: 0.0000; time: 0.16s
Val loss: 0.3863 score: 0.8367 time: 0.08s
Test loss: 0.3644 score: 0.8980 time: 0.10s
Epoch 72/1000, LR 0.000267
Train loss: 0.1177;  Loss pred: 0.1177; Loss self: 0.0000; time: 0.16s
Val loss: 0.3783 score: 0.8367 time: 0.08s
Test loss: 0.3516 score: 0.8776 time: 0.09s
Epoch 73/1000, LR 0.000267
Train loss: 0.1139;  Loss pred: 0.1139; Loss self: 0.0000; time: 0.16s
Val loss: 0.3717 score: 0.8367 time: 0.08s
Test loss: 0.3419 score: 0.8980 time: 0.10s
Epoch 74/1000, LR 0.000267
Train loss: 0.1085;  Loss pred: 0.1085; Loss self: 0.0000; time: 0.16s
Val loss: 0.3662 score: 0.8367 time: 0.08s
Test loss: 0.3344 score: 0.8980 time: 0.09s
Epoch 75/1000, LR 0.000267
Train loss: 0.1102;  Loss pred: 0.1102; Loss self: 0.0000; time: 0.16s
Val loss: 0.3619 score: 0.8367 time: 0.08s
Test loss: 0.3289 score: 0.8980 time: 0.09s
Epoch 76/1000, LR 0.000267
Train loss: 0.1101;  Loss pred: 0.1101; Loss self: 0.0000; time: 0.16s
Val loss: 0.3596 score: 0.8367 time: 0.08s
Test loss: 0.3276 score: 0.8980 time: 0.09s
Epoch 77/1000, LR 0.000267
Train loss: 0.1047;  Loss pred: 0.1047; Loss self: 0.0000; time: 0.16s
Val loss: 0.3608 score: 0.8367 time: 0.09s
Test loss: 0.3336 score: 0.8980 time: 0.09s
     INFO: Early stopping counter 1 of 20
Epoch 78/1000, LR 0.000267
Train loss: 0.1008;  Loss pred: 0.1008; Loss self: 0.0000; time: 0.16s
Val loss: 0.3588 score: 0.8367 time: 0.08s
Test loss: 0.3331 score: 0.8980 time: 0.09s
Epoch 79/1000, LR 0.000267
Train loss: 0.0979;  Loss pred: 0.0979; Loss self: 0.0000; time: 0.17s
Val loss: 0.3548 score: 0.8367 time: 0.16s
Test loss: 0.3297 score: 0.8980 time: 0.09s
Epoch 80/1000, LR 0.000267
Train loss: 0.0974;  Loss pred: 0.0974; Loss self: 0.0000; time: 0.16s
Val loss: 0.3470 score: 0.8367 time: 0.08s
Test loss: 0.3189 score: 0.9184 time: 0.09s
Epoch 81/1000, LR 0.000267
Train loss: 0.0927;  Loss pred: 0.0927; Loss self: 0.0000; time: 0.16s
Val loss: 0.3375 score: 0.8367 time: 0.08s
Test loss: 0.3047 score: 0.8980 time: 0.09s
Epoch 82/1000, LR 0.000267
Train loss: 0.0898;  Loss pred: 0.0898; Loss self: 0.0000; time: 0.16s
Val loss: 0.3297 score: 0.8571 time: 0.08s
Test loss: 0.2926 score: 0.8980 time: 0.10s
Epoch 83/1000, LR 0.000266
Train loss: 0.0916;  Loss pred: 0.0916; Loss self: 0.0000; time: 0.17s
Val loss: 0.3239 score: 0.8571 time: 0.08s
Test loss: 0.2845 score: 0.8980 time: 0.09s
Epoch 84/1000, LR 0.000266
Train loss: 0.0839;  Loss pred: 0.0839; Loss self: 0.0000; time: 0.16s
Val loss: 0.3205 score: 0.8571 time: 0.08s
Test loss: 0.2827 score: 0.8980 time: 0.09s
Epoch 85/1000, LR 0.000266
Train loss: 0.0816;  Loss pred: 0.0816; Loss self: 0.0000; time: 0.16s
Val loss: 0.3188 score: 0.8367 time: 0.08s
Test loss: 0.2842 score: 0.8980 time: 0.09s
Epoch 86/1000, LR 0.000266
Train loss: 0.0846;  Loss pred: 0.0846; Loss self: 0.0000; time: 0.16s
Val loss: 0.3165 score: 0.8367 time: 0.21s
Test loss: 0.2821 score: 0.8980 time: 0.09s
Epoch 87/1000, LR 0.000266
Train loss: 0.0803;  Loss pred: 0.0803; Loss self: 0.0000; time: 0.15s
Val loss: 0.3064 score: 0.8367 time: 0.09s
Test loss: 0.2649 score: 0.8980 time: 0.09s
Epoch 88/1000, LR 0.000266
Train loss: 0.0833;  Loss pred: 0.0833; Loss self: 0.0000; time: 0.16s
Val loss: 0.2990 score: 0.8367 time: 0.09s
Test loss: 0.2524 score: 0.8980 time: 0.09s
Epoch 89/1000, LR 0.000266
Train loss: 0.0747;  Loss pred: 0.0747; Loss self: 0.0000; time: 0.16s
Val loss: 0.2935 score: 0.8367 time: 0.09s
Test loss: 0.2404 score: 0.9184 time: 0.10s
Epoch 90/1000, LR 0.000266
Train loss: 0.0734;  Loss pred: 0.0734; Loss self: 0.0000; time: 0.15s
Val loss: 0.2906 score: 0.8367 time: 0.08s
Test loss: 0.2376 score: 0.9184 time: 0.09s
Epoch 91/1000, LR 0.000266
Train loss: 0.0721;  Loss pred: 0.0721; Loss self: 0.0000; time: 0.15s
Val loss: 0.2925 score: 0.8571 time: 0.08s
Test loss: 0.2563 score: 0.8980 time: 0.09s
     INFO: Early stopping counter 1 of 20
Epoch 92/1000, LR 0.000266
Train loss: 0.0654;  Loss pred: 0.0654; Loss self: 0.0000; time: 0.16s
Val loss: 0.2868 score: 0.8367 time: 0.08s
Test loss: 0.2435 score: 0.8980 time: 0.09s
Epoch 93/1000, LR 0.000265
Train loss: 0.0653;  Loss pred: 0.0653; Loss self: 0.0000; time: 0.16s
Val loss: 0.2832 score: 0.8367 time: 0.16s
Test loss: 0.2419 score: 0.8980 time: 0.09s
Epoch 94/1000, LR 0.000265
Train loss: 0.0641;  Loss pred: 0.0641; Loss self: 0.0000; time: 0.15s
Val loss: 0.2796 score: 0.8367 time: 0.08s
Test loss: 0.2386 score: 0.8980 time: 0.09s
Epoch 95/1000, LR 0.000265
Train loss: 0.0611;  Loss pred: 0.0611; Loss self: 0.0000; time: 0.15s
Val loss: 0.2767 score: 0.8367 time: 0.09s
Test loss: 0.2234 score: 0.9184 time: 0.09s
Epoch 96/1000, LR 0.000265
Train loss: 0.0613;  Loss pred: 0.0613; Loss self: 0.0000; time: 0.15s
Val loss: 0.2775 score: 0.8367 time: 0.08s
Test loss: 0.2425 score: 0.9184 time: 0.09s
     INFO: Early stopping counter 1 of 20
Epoch 97/1000, LR 0.000265
Train loss: 0.0559;  Loss pred: 0.0559; Loss self: 0.0000; time: 0.16s
Val loss: 0.2735 score: 0.8367 time: 0.08s
Test loss: 0.2307 score: 0.8980 time: 0.09s
Epoch 98/1000, LR 0.000265
Train loss: 0.0569;  Loss pred: 0.0569; Loss self: 0.0000; time: 0.15s
Val loss: 0.2730 score: 0.8367 time: 0.09s
Test loss: 0.2342 score: 0.8980 time: 0.09s
Epoch 99/1000, LR 0.000265
Train loss: 0.0546;  Loss pred: 0.0546; Loss self: 0.0000; time: 0.16s
Val loss: 0.2727 score: 0.8571 time: 0.09s
Test loss: 0.2424 score: 0.9184 time: 0.09s
Epoch 100/1000, LR 0.000265
Train loss: 0.0539;  Loss pred: 0.0539; Loss self: 0.0000; time: 0.16s
Val loss: 0.2690 score: 0.8367 time: 0.08s
Test loss: 0.2319 score: 0.8980 time: 0.09s
Epoch 101/1000, LR 0.000265
Train loss: 0.0522;  Loss pred: 0.0522; Loss self: 0.0000; time: 0.17s
Val loss: 0.2672 score: 0.8367 time: 0.10s
Test loss: 0.2253 score: 0.8980 time: 0.09s
Epoch 102/1000, LR 0.000264
Train loss: 0.0503;  Loss pred: 0.0503; Loss self: 0.0000; time: 0.16s
Val loss: 0.2677 score: 0.8367 time: 0.08s
Test loss: 0.2297 score: 0.8980 time: 0.09s
     INFO: Early stopping counter 1 of 20
Epoch 103/1000, LR 0.000264
Train loss: 0.0463;  Loss pred: 0.0463; Loss self: 0.0000; time: 0.15s
Val loss: 0.2694 score: 0.8367 time: 0.08s
Test loss: 0.2388 score: 0.8980 time: 0.09s
     INFO: Early stopping counter 2 of 20
Epoch 104/1000, LR 0.000264
Train loss: 0.0470;  Loss pred: 0.0470; Loss self: 0.0000; time: 0.15s
Val loss: 0.2676 score: 0.8367 time: 0.08s
Test loss: 0.2313 score: 0.8980 time: 0.09s
     INFO: Early stopping counter 3 of 20
Epoch 105/1000, LR 0.000264
Train loss: 0.0470;  Loss pred: 0.0470; Loss self: 0.0000; time: 0.15s
Val loss: 0.2645 score: 0.8367 time: 0.08s
Test loss: 0.2257 score: 0.8980 time: 0.09s
Epoch 106/1000, LR 0.000264
Train loss: 0.0457;  Loss pred: 0.0457; Loss self: 0.0000; time: 0.16s
Val loss: 0.2642 score: 0.8367 time: 0.08s
Test loss: 0.2310 score: 0.8980 time: 0.09s
Epoch 107/1000, LR 0.000264
Train loss: 0.0410;  Loss pred: 0.0410; Loss self: 0.0000; time: 0.15s
Val loss: 0.2664 score: 0.8367 time: 0.08s
Test loss: 0.2356 score: 0.8980 time: 0.09s
     INFO: Early stopping counter 1 of 20
Epoch 108/1000, LR 0.000264
Train loss: 0.0422;  Loss pred: 0.0422; Loss self: 0.0000; time: 0.16s
Val loss: 0.2649 score: 0.8367 time: 0.09s
Test loss: 0.2325 score: 0.8980 time: 0.23s
     INFO: Early stopping counter 2 of 20
Epoch 109/1000, LR 0.000264
Train loss: 0.0473;  Loss pred: 0.0473; Loss self: 0.0000; time: 0.15s
Val loss: 0.2653 score: 0.8367 time: 0.08s
Test loss: 0.2354 score: 0.8980 time: 0.09s
     INFO: Early stopping counter 3 of 20
Epoch 110/1000, LR 0.000263
Train loss: 0.0399;  Loss pred: 0.0399; Loss self: 0.0000; time: 0.15s
Val loss: 0.2639 score: 0.8367 time: 0.08s
Test loss: 0.2314 score: 0.8980 time: 0.09s
Epoch 111/1000, LR 0.000263
Train loss: 0.0401;  Loss pred: 0.0401; Loss self: 0.0000; time: 0.15s
Val loss: 0.2621 score: 0.8367 time: 0.08s
Test loss: 0.2261 score: 0.9184 time: 0.09s
Epoch 112/1000, LR 0.000263
Train loss: 0.0375;  Loss pred: 0.0375; Loss self: 0.0000; time: 0.15s
Val loss: 0.2633 score: 0.8367 time: 0.08s
Test loss: 0.2255 score: 0.9184 time: 0.09s
     INFO: Early stopping counter 1 of 20
Epoch 113/1000, LR 0.000263
Train loss: 0.0388;  Loss pred: 0.0388; Loss self: 0.0000; time: 0.15s
Val loss: 0.2630 score: 0.8367 time: 0.09s
Test loss: 0.2261 score: 0.9184 time: 0.09s
     INFO: Early stopping counter 2 of 20
Epoch 114/1000, LR 0.000263
Train loss: 0.0345;  Loss pred: 0.0345; Loss self: 0.0000; time: 0.14s
Val loss: 0.2643 score: 0.8367 time: 0.08s
Test loss: 0.2282 score: 0.9184 time: 0.09s
     INFO: Early stopping counter 3 of 20
Epoch 115/1000, LR 0.000263
Train loss: 0.0347;  Loss pred: 0.0347; Loss self: 0.0000; time: 0.14s
Val loss: 0.2645 score: 0.8367 time: 0.08s
Test loss: 0.2316 score: 0.8980 time: 0.09s
     INFO: Early stopping counter 4 of 20
Epoch 116/1000, LR 0.000263
Train loss: 0.0364;  Loss pred: 0.0364; Loss self: 0.0000; time: 0.21s
Val loss: 0.2644 score: 0.8367 time: 0.08s
Test loss: 0.2297 score: 0.9184 time: 0.10s
     INFO: Early stopping counter 5 of 20
Epoch 117/1000, LR 0.000262
Train loss: 0.0373;  Loss pred: 0.0373; Loss self: 0.0000; time: 0.14s
Val loss: 0.2648 score: 0.8367 time: 0.09s
Test loss: 0.2297 score: 0.9184 time: 0.09s
     INFO: Early stopping counter 6 of 20
Epoch 118/1000, LR 0.000262
Train loss: 0.0342;  Loss pred: 0.0342; Loss self: 0.0000; time: 0.15s
Val loss: 0.2640 score: 0.8367 time: 0.09s
Test loss: 0.2286 score: 0.9184 time: 0.09s
     INFO: Early stopping counter 7 of 20
Epoch 119/1000, LR 0.000262
Train loss: 0.0324;  Loss pred: 0.0324; Loss self: 0.0000; time: 0.16s
Val loss: 0.2629 score: 0.8367 time: 0.08s
Test loss: 0.2298 score: 0.9184 time: 0.09s
     INFO: Early stopping counter 8 of 20
Epoch 120/1000, LR 0.000262
Train loss: 0.0316;  Loss pred: 0.0316; Loss self: 0.0000; time: 0.16s
Val loss: 0.2653 score: 0.8367 time: 0.08s
Test loss: 0.2361 score: 0.8980 time: 0.09s
     INFO: Early stopping counter 9 of 20
Epoch 121/1000, LR 0.000262
Train loss: 0.0323;  Loss pred: 0.0323; Loss self: 0.0000; time: 0.16s
Val loss: 0.2661 score: 0.8367 time: 0.09s
Test loss: 0.2358 score: 0.9184 time: 0.09s
     INFO: Early stopping counter 10 of 20
Epoch 122/1000, LR 0.000262
Train loss: 0.0325;  Loss pred: 0.0325; Loss self: 0.0000; time: 0.16s
Val loss: 0.2645 score: 0.8367 time: 0.09s
Test loss: 0.2328 score: 0.9184 time: 0.10s
     INFO: Early stopping counter 11 of 20
Epoch 123/1000, LR 0.000262
Train loss: 0.0289;  Loss pred: 0.0289; Loss self: 0.0000; time: 0.16s
Val loss: 0.2642 score: 0.8367 time: 0.18s
Test loss: 0.2317 score: 0.9184 time: 0.09s
     INFO: Early stopping counter 12 of 20
Epoch 124/1000, LR 0.000261
Train loss: 0.0285;  Loss pred: 0.0285; Loss self: 0.0000; time: 0.15s
Val loss: 0.2661 score: 0.8367 time: 0.08s
Test loss: 0.2351 score: 0.9184 time: 0.09s
     INFO: Early stopping counter 13 of 20
Epoch 125/1000, LR 0.000261
Train loss: 0.0281;  Loss pred: 0.0281; Loss self: 0.0000; time: 0.15s
Val loss: 0.2659 score: 0.8571 time: 0.09s
Test loss: 0.2360 score: 0.9184 time: 0.10s
     INFO: Early stopping counter 14 of 20
Epoch 126/1000, LR 0.000261
Train loss: 0.0296;  Loss pred: 0.0296; Loss self: 0.0000; time: 0.15s
Val loss: 0.2641 score: 0.8571 time: 0.08s
Test loss: 0.2319 score: 0.9184 time: 0.09s
     INFO: Early stopping counter 15 of 20
Epoch 127/1000, LR 0.000261
Train loss: 0.0270;  Loss pred: 0.0270; Loss self: 0.0000; time: 0.15s
Val loss: 0.2642 score: 0.8571 time: 0.08s
Test loss: 0.2327 score: 0.9184 time: 0.09s
     INFO: Early stopping counter 16 of 20
Epoch 128/1000, LR 0.000261
Train loss: 0.0258;  Loss pred: 0.0258; Loss self: 0.0000; time: 0.16s
Val loss: 0.2638 score: 0.8571 time: 0.08s
Test loss: 0.2331 score: 0.9184 time: 0.09s
     INFO: Early stopping counter 17 of 20
Epoch 129/1000, LR 0.000261
Train loss: 0.0286;  Loss pred: 0.0286; Loss self: 0.0000; time: 0.14s
Val loss: 0.2643 score: 0.8571 time: 0.08s
Test loss: 0.2342 score: 0.9184 time: 0.09s
     INFO: Early stopping counter 18 of 20
Epoch 130/1000, LR 0.000260
Train loss: 0.0280;  Loss pred: 0.0280; Loss self: 0.0000; time: 0.15s
Val loss: 0.2641 score: 0.8571 time: 0.08s
Test loss: 0.2348 score: 0.9184 time: 0.09s
     INFO: Early stopping counter 19 of 20
Epoch 131/1000, LR 0.000260
Train loss: 0.0260;  Loss pred: 0.0260; Loss self: 0.0000; time: 0.17s
Val loss: 0.2653 score: 0.8571 time: 0.10s
Test loss: 0.2348 score: 0.9184 time: 0.09s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 110,   Train_Loss: 0.0401,   Val_Loss: 0.2621,   Val_Precision: 0.9000,   Val_Recall: 0.7500,   Val_accuracy: 0.8182,   Val_Score: 0.8367,   Val_Loss: 0.2621,   Test_Precision: 0.9565,   Test_Recall: 0.8800,   Test_accuracy: 0.9167,   Test_Score: 0.9184,   Test_loss: 0.2261


[0.09637932595796883, 0.09857086394913495, 0.09941427898593247, 0.09927489422261715, 0.09862565412186086, 0.0967537818942219, 0.09689547005109489, 0.09989027911797166, 0.10120426607318223, 0.10460687195882201, 0.10105438716709614, 0.0967152260709554, 0.09538910794071853, 0.09869300690479577, 0.10012829001061618, 0.09686358296312392, 0.09705522819422185, 0.09733351110480726, 0.1009229770861566, 0.10499870195053518, 0.10071813501417637, 0.09491636394523084, 0.09443707391619682, 0.09807165013626218, 0.09758537588641047, 0.09553289809264243, 0.09478604281321168, 0.09740679105743766, 0.09965529991313815, 0.10549134900793433, 0.10052248905412853, 0.0995034200605005, 0.09661654802039266, 0.09733573393896222, 0.0985178779810667, 0.09314462705515325, 0.10024720896035433, 0.09171853610314429, 0.09426239598542452, 0.09440813399851322, 0.0929876349400729, 0.09304485702887177, 3.144433974986896, 0.0975227509625256, 0.101262500975281, 0.09925344306975603, 0.09908093302510679, 0.10118427779525518, 0.09846929018385708, 0.10028810799121857, 0.2173422120977193, 0.09995974204503, 0.09811485907994211, 0.09930934896692634, 0.10103259608149529, 0.09993958286941051, 0.10027994215488434, 0.09842154406942427, 0.09930139407515526, 0.10037223901599646, 0.0992641078773886, 0.10067941108718514, 0.10002194810658693, 0.09754292899742723, 0.09815662796609104, 0.09776058490388095, 0.09723178786225617, 0.0989613609854132, 0.09764794097281992, 0.09837785409763455, 0.1015543881803751, 0.09843742311932147, 0.10050052497535944, 0.09718872793018818, 0.09752774704247713, 0.099183278856799, 0.09693550900556147, 0.09635552205145359, 0.09801227389834821, 0.10050106002017856, 0.09697237005457282, 0.1001443020068109, 0.09738525119610131, 0.09953965595923364, 0.09837057907134295, 0.09957529092207551, 0.09759931801818311, 0.09795144898816943, 0.10047362092882395, 0.09802661300636828, 0.09780577709898353, 0.09723115316592157, 0.09962794510647655, 0.0968415210954845, 0.09877185500226915, 0.09773123217746615, 0.09830323304049671, 0.10006915288977325, 0.09808423393405974, 0.09864312992431223, 0.09726201207377017, 0.09738949197344482, 0.0978548969142139, 0.09764305199496448, 0.09836454782634974, 0.09707906399853528, 0.0967504361178726, 0.23881512600928545, 0.09737191395834088, 0.09449485107325017, 0.09586431505158544, 0.09470868902280927, 0.09525814815424383, 0.09658049698919058, 0.09928513201884925, 0.10060313809663057, 0.09808940114453435, 0.09959369897842407, 0.09958888590335846, 0.09898317907936871, 0.09929890814237297, 0.1003211138304323, 0.0962218539789319, 0.09890127298422158, 0.10373995592817664, 0.09706783597357571, 0.09925592597573996, 0.09694811701774597, 0.09368218085728586, 0.09546093386597931, 0.09614448598586023]
[0.0019669250195503843, 0.0020116502846762234, 0.0020288628364476015, 0.002026018249441166, 0.002012768451466548, 0.0019745669774331003, 0.0019774585724713243, 0.0020385771248565645, 0.002065393185166984, 0.0021348341216086125, 0.0020623344319815537, 0.001973780123897049, 0.0019467164885860924, 0.0020141429980570563, 0.002043434490012575, 0.0019768078155739575, 0.0019807189427392216, 0.001986398185812393, 0.002059652593595033, 0.002142830652051738, 0.0020554721431464565, 0.0019370686519434865, 0.001927287222779527, 0.00200146224767882, 0.0019915382833961322, 0.0019496509814824984, 0.00193440903700432, 0.001987893695049748, 0.0020337816308803706, 0.002152884673631313, 0.0020514793684516027, 0.002030682042051031, 0.0019717662861304624, 0.0019864435497747393, 0.0020105689383891163, 0.0019009107562276174, 0.00204586140735417, 0.0018718068592478425, 0.0019237223670494799, 0.0019266966122145556, 0.0018977068355116918, 0.0018988746332422811, 0.06417212193850808, 0.0019902602237250122, 0.0020665816525567552, 0.0020255804708113477, 0.0020220598576552408, 0.002064985261127657, 0.002009577350690961, 0.00204669608145344, 0.004435555348933047, 0.002039994735612857, 0.0020023440628559614, 0.0020267214074882927, 0.0020618897159488834, 0.0020395833238655205, 0.0020465294317323335, 0.002008602940192332, 0.0020265590627582706, 0.0020484130411427847, 0.002025798119946706, 0.0020546818589221457, 0.0020412642470732027, 0.0019906720203556575, 0.0020031964891038985, 0.0019951139776302235, 0.0019843222012705343, 0.002019619611947208, 0.0019928151218942844, 0.0020077113081149907, 0.002072538534293369, 0.002008927002435132, 0.0020510311219461112, 0.0019834434271466975, 0.0019903621845403494, 0.002024148548097939, 0.00197827569399105, 0.0019664392255398693, 0.002000250487721392, 0.0020510420412281336, 0.0019790279602974045, 0.0020437612654451206, 0.0019874541060428837, 0.0020314215501884415, 0.0020075628381906723, 0.0020321487943280717, 0.0019918228166976143, 0.001999009163023866, 0.002050482059771917, 0.0020005431225789444, 0.0019960362673261942, 0.001984309248284114, 0.0020332233695199296, 0.0019763575733772348, 0.002015752142903452, 0.0019945149423972685, 0.002006188429397892, 0.0020422276099953725, 0.0020017190598787702, 0.002013125100496168, 0.001984939021913677, 0.001987540652519282, 0.0019970387125349777, 0.00199271534683601, 0.002007439751558158, 0.0019812053877252098, 0.0019744986962831144, 0.004873778081822152, 0.001987181917517161, 0.001928466348433677, 0.001956414592889499, 0.0019328303882205973, 0.0019440438398825271, 0.0019710305507998076, 0.002026227184058148, 0.002053125267278175, 0.0020018245131537622, 0.00203252446894743, 0.002032426242925683, 0.0020200648791707902, 0.002026508329436183, 0.0020473696700088226, 0.0019637113056924877, 0.002018393326208604, 0.0021171419577178906, 0.001980976244358688, 0.00202563114236204, 0.0019785330003621628, 0.0019118812419854257, 0.0019481823237954962, 0.001962132367058372]
[508.4077888381267, 497.1042967147497, 492.8869423972154, 493.5789696246954, 496.82813702260563, 506.4401519061061, 505.6995953903865, 490.5382228648134, 484.169313224083, 468.42046877463855, 484.88740938062574, 506.6420458351719, 513.6854831523535, 496.4890779674781, 489.3721843727156, 505.866069590409, 504.86718656663976, 503.4237380714391, 485.51877297643875, 466.67243584671996, 486.5062284275132, 516.2439642997097, 518.8640220204456, 499.63470515606383, 502.12441725936554, 512.9123158441457, 516.9537470465026, 503.0450081360988, 491.6948726531306, 464.4930646996898, 487.45311085179037, 492.4453849948756, 507.1594980774688, 503.4122414973227, 497.3716548119001, 526.0636233047118, 488.79166321107726, 534.2431539127039, 519.8255305071675, 519.0230748631434, 526.9517826921686, 526.6277101677455, 15.58309075330615, 502.44686000325083, 483.8908730089661, 493.68564439182677, 494.54520162404566, 484.2649576365089, 497.61707338917114, 488.59232646297943, 225.4509123058391, 490.1973434257805, 499.4146703108011, 493.4077255538026, 484.99199169815887, 490.2962229092704, 488.63211273415516, 497.8584766505648, 493.44725173661556, 488.1827931744235, 493.6326034433814, 486.69335140992797, 489.89247787679426, 502.3429222767384, 499.20215287884, 501.22449705243906, 503.95041660054693, 495.14274573510113, 501.80269560050465, 498.0795774562253, 482.5000758506762, 497.7781665475373, 487.5596422209112, 504.17369424978256, 502.4211210237287, 494.03488738002187, 505.49071751600053, 508.5333871558926, 499.93738591168216, 487.5570465641039, 505.29857084471, 489.29393902678027, 503.1562726200746, 492.2661177377175, 498.1164130838643, 492.0899506921437, 502.0526884303753, 500.2478320246004, 487.6901971584349, 499.86425621802033, 500.9929009654507, 503.9537062404599, 491.8298771256564, 505.9813130329358, 496.0927381476666, 501.3750354750762, 498.45766496625913, 489.66138500216726, 499.57060410893166, 496.7401180152855, 503.7938137947943, 503.1343629286086, 500.74141964460557, 501.82782081132575, 498.1469552069039, 504.7432266213373, 506.4576653722007, 205.17963337923084, 503.22519100285854, 518.546772056568, 511.1391029459989, 517.3759715774235, 514.3916919386072, 507.3488077565406, 493.5280741803049, 487.062341464288, 499.544287438341, 491.99899695075425, 492.02277498665705, 495.03360526246405, 493.45960511211956, 488.43157864876, 509.2398241539673, 495.443572377651, 472.33488352284127, 504.80161124987916, 493.67329475095056, 505.4249789197117, 523.0450396393518, 513.2989801754159, 509.64961222223735]
Elapsed: 0.12348748580552638~0.26544517845789045
Time per graph: 0.0025201527715413546~0.005417248539956948
Speed: 490.9661926296318~55.510568488734705
Total Time: 0.0968
best val loss: 0.262052446603775 test_score: 0.9184

Testing...
Test loss: 0.2926 score: 0.8980 time: 0.09s
test Score 0.8980
Epoch Time List: [0.32390359113924205, 0.34299500612542033, 0.3324917003046721, 0.47627019113861024, 0.33961626910604537, 0.32951786811463535, 0.332793835317716, 0.3307346219662577, 0.3515393079724163, 0.3542224336415529, 0.47403525095432997, 0.33535956079140306, 0.32491709711030126, 0.3306581990327686, 0.3330622056964785, 0.3445071878377348, 0.3262014789506793, 0.41367054800502956, 0.34783385088667274, 0.3476776168681681, 0.3543270919471979, 0.3352437671273947, 0.33578795101493597, 0.33312563481740654, 0.3307323819026351, 0.31603187089785933, 0.4353439339902252, 0.32765928702428937, 0.3281651302240789, 0.3621470171492547, 0.34862269484438, 0.345520957140252, 0.32661682390607893, 0.33125763898715377, 0.32095526810735464, 0.39740577386692166, 0.3211774800438434, 0.3170023839920759, 0.3147912989370525, 0.3122640650253743, 0.31655616289936006, 0.3198075348045677, 4.299296143930405, 2.562304158229381, 0.3451368832029402, 0.342805314110592, 0.34035500790923834, 0.34253094298765063, 0.34241154696792364, 0.34204104682430625, 0.4621546249836683, 0.3408446619287133, 0.34077978204004467, 0.34011667408049107, 0.34091463428922, 0.3444245611317456, 0.34125460172072053, 0.3382228419650346, 0.3389827539213002, 0.33677664096467197, 0.3393753420095891, 0.34569947188720107, 0.3391128007788211, 0.3400337037164718, 0.33100640890188515, 0.33776143682189286, 0.33906302601099014, 0.3312275449279696, 0.3371106411796063, 0.33830376411788166, 0.3408236540853977, 0.3391530029475689, 0.33985422900877893, 0.3350246048066765, 0.3375292031560093, 0.3348221352789551, 0.33622678532265127, 0.33007902605459094, 0.42743106791749597, 0.33788915583863854, 0.3360626841895282, 0.33452356280758977, 0.34233399108052254, 0.339049561182037, 0.3384221070446074, 0.46340674813836813, 0.3317028949968517, 0.33381571085192263, 0.33895630622282624, 0.3321765176951885, 0.3312197991181165, 0.3331626539584249, 0.41060228389687836, 0.32927021803334355, 0.33340739901177585, 0.32962098391726613, 0.33695521438494325, 0.3317767612170428, 0.3351614191196859, 0.3354660039767623, 0.36229502432979643, 0.33051829994656146, 0.3312666448764503, 0.3242580017540604, 0.33110974286682904, 0.33598613389767706, 0.3278207378461957, 0.47629070398397744, 0.3213519302662462, 0.3264328632503748, 0.3205264189746231, 0.31825281376950443, 0.3243023990653455, 0.3169220348354429, 0.3155698608607054, 0.38534224196337163, 0.3216874930076301, 0.33429869799874723, 0.33669880987145007, 0.33639391232281923, 0.33511894778348505, 0.34101971006020904, 0.43392918980680406, 0.3271343286614865, 0.33932114695198834, 0.3240863881073892, 0.3328840637113899, 0.33179003768600523, 0.3152088162023574, 0.31965532386675477, 0.35494034737348557]
Total Epoch List: [131]
Total Time List: [0.0967547248583287]
========================k_idx:1========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: SGFormer
SGFormer(
  (trans_conv): SGModule(
    (attns): ModuleList(
      (0-1): 2 x SGFormerAttention(heads=1, head_channels=64)
    )
    (fcs): ModuleList(
      (0): Linear(in_features=14887, out_features=64, bias=True)
    )
    (bns): ModuleList(
      (0-2): 3 x LayerNorm((64,), eps=1e-05, elementwise_affine=True)
    )
  )
  (graph_conv): GraphModule(
    (convs): ModuleList(
      (0-2): 3 x GCNConv(64, 64)
    )
    (fcs): ModuleList(
      (0): Linear(in_features=14887, out_features=64, bias=True)
    )
    (bns): ModuleList(
      (0-3): 4 x BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (classifier): Linear(in_features=64, out_features=2, bias=True)
)
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7e09d8973c10>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.7187;  Loss pred: 0.7187; Loss self: 0.0000; time: 0.14s
Val loss: 0.6984 score: 0.5102 time: 0.10s
Test loss: 0.7034 score: 0.4694 time: 0.09s
Epoch 2/1000, LR 0.000000
Train loss: 0.7136;  Loss pred: 0.7136; Loss self: 0.0000; time: 0.14s
Val loss: 0.6972 score: 0.5102 time: 0.10s
Test loss: 0.7019 score: 0.4694 time: 0.09s
Epoch 3/1000, LR 0.000030
Train loss: 0.7070;  Loss pred: 0.7070; Loss self: 0.0000; time: 0.14s
Val loss: 0.6955 score: 0.5102 time: 0.10s
Test loss: 0.7002 score: 0.4694 time: 0.09s
Epoch 4/1000, LR 0.000060
Train loss: 0.7267;  Loss pred: 0.7267; Loss self: 0.0000; time: 0.15s
Val loss: 0.6936 score: 0.5102 time: 0.10s
Test loss: 0.6983 score: 0.4694 time: 0.09s
Epoch 5/1000, LR 0.000090
Train loss: 0.7151;  Loss pred: 0.7151; Loss self: 0.0000; time: 0.15s
Val loss: 0.6915 score: 0.5102 time: 0.10s
Test loss: 0.6964 score: 0.4694 time: 0.09s
Epoch 6/1000, LR 0.000120
Train loss: 0.6830;  Loss pred: 0.6830; Loss self: 0.0000; time: 0.27s
Val loss: 0.6892 score: 0.5306 time: 0.10s
Test loss: 0.6944 score: 0.4694 time: 0.09s
Epoch 7/1000, LR 0.000150
Train loss: 0.6927;  Loss pred: 0.6927; Loss self: 0.0000; time: 0.15s
Val loss: 0.6871 score: 0.5306 time: 0.10s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6928 score: 0.4898 time: 0.09s
Epoch 8/1000, LR 0.000180
Train loss: 0.6485;  Loss pred: 0.6485; Loss self: 0.0000; time: 0.13s
Val loss: 0.6846 score: 0.5306 time: 0.10s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6912 score: 0.4898 time: 0.08s
Epoch 9/1000, LR 0.000210
Train loss: 0.6489;  Loss pred: 0.6489; Loss self: 0.0000; time: 0.14s
Val loss: 0.6830 score: 0.5306 time: 0.10s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6905 score: 0.4898 time: 0.09s
Epoch 10/1000, LR 0.000240
Train loss: 0.6394;  Loss pred: 0.6394; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6825 score: 0.5102 time: 0.10s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6907 score: 0.4898 time: 0.09s
Epoch 11/1000, LR 0.000270
Train loss: 0.6041;  Loss pred: 0.6041; Loss self: 0.0000; time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6830 score: 0.5102 time: 0.10s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6921 score: 0.4898 time: 0.09s
     INFO: Early stopping counter 1 of 20
Epoch 12/1000, LR 0.000270
Train loss: 0.6106;  Loss pred: 0.6106; Loss self: 0.0000; time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6832 score: 0.5102 time: 0.10s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6933 score: 0.4898 time: 0.08s
     INFO: Early stopping counter 2 of 20
Epoch 13/1000, LR 0.000270
Train loss: 0.5777;  Loss pred: 0.5777; Loss self: 0.0000; time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6828 score: 0.5102 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6935 score: 0.4898 time: 0.09s
     INFO: Early stopping counter 3 of 20
Epoch 14/1000, LR 0.000270
Train loss: 0.5494;  Loss pred: 0.5494; Loss self: 0.0000; time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6830 score: 0.5102 time: 0.09s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6941 score: 0.4898 time: 0.08s
     INFO: Early stopping counter 4 of 20
Epoch 15/1000, LR 0.000270
Train loss: 0.5689;  Loss pred: 0.5689; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6825 score: 0.5102 time: 0.09s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6938 score: 0.4898 time: 0.08s
Epoch 16/1000, LR 0.000270
Train loss: 0.5174;  Loss pred: 0.5174; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6815 score: 0.5102 time: 0.10s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6930 score: 0.4898 time: 0.08s
Epoch 17/1000, LR 0.000270
Train loss: 0.5141;  Loss pred: 0.5141; Loss self: 0.0000; time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6803 score: 0.5102 time: 0.10s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6918 score: 0.4898 time: 0.08s
Epoch 18/1000, LR 0.000270
Train loss: 0.5018;  Loss pred: 0.5018; Loss self: 0.0000; time: 0.14s
Val loss: 0.6786 score: 0.5306 time: 0.10s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6899 score: 0.4898 time: 0.09s
Epoch 19/1000, LR 0.000270
Train loss: 0.4748;  Loss pred: 0.4748; Loss self: 0.0000; time: 0.14s
Val loss: 0.6768 score: 0.5306 time: 0.09s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6880 score: 0.4898 time: 0.08s
Epoch 20/1000, LR 0.000270
Train loss: 0.4740;  Loss pred: 0.4740; Loss self: 0.0000; time: 0.14s
Val loss: 0.6755 score: 0.5306 time: 0.10s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6865 score: 0.4898 time: 0.22s
Epoch 21/1000, LR 0.000270
Train loss: 0.4595;  Loss pred: 0.4595; Loss self: 0.0000; time: 0.15s
Val loss: 0.6741 score: 0.5306 time: 0.10s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6848 score: 0.4898 time: 0.09s
Epoch 22/1000, LR 0.000270
Train loss: 0.4473;  Loss pred: 0.4473; Loss self: 0.0000; time: 0.14s
Val loss: 0.6717 score: 0.5306 time: 0.10s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6824 score: 0.4898 time: 0.09s
Epoch 23/1000, LR 0.000270
Train loss: 0.4449;  Loss pred: 0.4449; Loss self: 0.0000; time: 0.15s
Val loss: 0.6691 score: 0.5306 time: 0.10s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6796 score: 0.4898 time: 0.09s
Epoch 24/1000, LR 0.000270
Train loss: 0.4253;  Loss pred: 0.4253; Loss self: 0.0000; time: 0.14s
Val loss: 0.6657 score: 0.5306 time: 0.10s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6761 score: 0.4898 time: 0.09s
Epoch 25/1000, LR 0.000270
Train loss: 0.4267;  Loss pred: 0.4267; Loss self: 0.0000; time: 0.15s
Val loss: 0.6626 score: 0.5306 time: 0.10s
Test loss: 0.6728 score: 0.5102 time: 0.09s
Epoch 26/1000, LR 0.000270
Train loss: 0.3986;  Loss pred: 0.3986; Loss self: 0.0000; time: 0.15s
Val loss: 0.6595 score: 0.5510 time: 0.10s
Test loss: 0.6694 score: 0.5306 time: 0.09s
Epoch 27/1000, LR 0.000270
Train loss: 0.3868;  Loss pred: 0.3868; Loss self: 0.0000; time: 0.15s
Val loss: 0.6559 score: 0.5510 time: 0.10s
Test loss: 0.6655 score: 0.5306 time: 0.19s
Epoch 28/1000, LR 0.000270
Train loss: 0.3819;  Loss pred: 0.3819; Loss self: 0.0000; time: 0.15s
Val loss: 0.6519 score: 0.5510 time: 0.10s
Test loss: 0.6613 score: 0.5306 time: 0.09s
Epoch 29/1000, LR 0.000270
Train loss: 0.3708;  Loss pred: 0.3708; Loss self: 0.0000; time: 0.15s
Val loss: 0.6483 score: 0.5714 time: 0.10s
Test loss: 0.6575 score: 0.5510 time: 0.09s
Epoch 30/1000, LR 0.000270
Train loss: 0.3564;  Loss pred: 0.3564; Loss self: 0.0000; time: 0.14s
Val loss: 0.6432 score: 0.5714 time: 0.10s
Test loss: 0.6521 score: 0.5510 time: 0.09s
Epoch 31/1000, LR 0.000270
Train loss: 0.3539;  Loss pred: 0.3539; Loss self: 0.0000; time: 0.15s
Val loss: 0.6391 score: 0.5714 time: 0.10s
Test loss: 0.6478 score: 0.5510 time: 0.09s
Epoch 32/1000, LR 0.000270
Train loss: 0.3399;  Loss pred: 0.3399; Loss self: 0.0000; time: 0.15s
Val loss: 0.6339 score: 0.5714 time: 0.10s
Test loss: 0.6426 score: 0.5510 time: 0.09s
Epoch 33/1000, LR 0.000270
Train loss: 0.3332;  Loss pred: 0.3332; Loss self: 0.0000; time: 0.15s
Val loss: 0.6288 score: 0.5714 time: 0.10s
Test loss: 0.6375 score: 0.5510 time: 0.09s
Epoch 34/1000, LR 0.000270
Train loss: 0.3367;  Loss pred: 0.3367; Loss self: 0.0000; time: 0.14s
Val loss: 0.6234 score: 0.5918 time: 0.10s
Test loss: 0.6320 score: 0.5510 time: 0.09s
Epoch 35/1000, LR 0.000270
Train loss: 0.3192;  Loss pred: 0.3192; Loss self: 0.0000; time: 0.17s
Val loss: 0.6178 score: 0.5918 time: 0.11s
Test loss: 0.6264 score: 0.5714 time: 0.09s
Epoch 36/1000, LR 0.000270
Train loss: 0.3416;  Loss pred: 0.3416; Loss self: 0.0000; time: 0.14s
Val loss: 0.6128 score: 0.5918 time: 0.10s
Test loss: 0.6214 score: 0.5714 time: 0.09s
Epoch 37/1000, LR 0.000270
Train loss: 0.3037;  Loss pred: 0.3037; Loss self: 0.0000; time: 0.15s
Val loss: 0.6080 score: 0.6122 time: 0.10s
Test loss: 0.6167 score: 0.5918 time: 0.14s
Epoch 38/1000, LR 0.000270
Train loss: 0.2925;  Loss pred: 0.2925; Loss self: 0.0000; time: 0.24s
Val loss: 0.6033 score: 0.6122 time: 0.10s
Test loss: 0.6119 score: 0.5918 time: 0.09s
Epoch 39/1000, LR 0.000269
Train loss: 0.2956;  Loss pred: 0.2956; Loss self: 0.0000; time: 0.15s
Val loss: 0.6001 score: 0.6122 time: 0.10s
Test loss: 0.6084 score: 0.5918 time: 0.09s
Epoch 40/1000, LR 0.000269
Train loss: 0.2848;  Loss pred: 0.2848; Loss self: 0.0000; time: 0.15s
Val loss: 0.5954 score: 0.6122 time: 0.10s
Test loss: 0.6034 score: 0.5918 time: 0.10s
Epoch 41/1000, LR 0.000269
Train loss: 0.2643;  Loss pred: 0.2643; Loss self: 0.0000; time: 0.16s
Val loss: 0.5913 score: 0.6327 time: 0.10s
Test loss: 0.5991 score: 0.6327 time: 0.21s
Epoch 42/1000, LR 0.000269
Train loss: 0.2685;  Loss pred: 0.2685; Loss self: 0.0000; time: 0.15s
Val loss: 0.5876 score: 0.6327 time: 0.10s
Test loss: 0.5952 score: 0.6327 time: 0.09s
Epoch 43/1000, LR 0.000269
Train loss: 0.2856;  Loss pred: 0.2856; Loss self: 0.0000; time: 0.15s
Val loss: 0.5845 score: 0.6327 time: 0.10s
Test loss: 0.5920 score: 0.6122 time: 0.09s
Epoch 44/1000, LR 0.000269
Train loss: 0.2589;  Loss pred: 0.2589; Loss self: 0.0000; time: 0.15s
Val loss: 0.5808 score: 0.6327 time: 0.10s
Test loss: 0.5882 score: 0.6122 time: 0.09s
Epoch 45/1000, LR 0.000269
Train loss: 0.2527;  Loss pred: 0.2527; Loss self: 0.0000; time: 0.15s
Val loss: 0.5787 score: 0.6327 time: 0.10s
Test loss: 0.5858 score: 0.6327 time: 0.09s
Epoch 46/1000, LR 0.000269
Train loss: 0.2514;  Loss pred: 0.2514; Loss self: 0.0000; time: 0.15s
Val loss: 0.5784 score: 0.6327 time: 0.10s
Test loss: 0.5853 score: 0.6327 time: 0.09s
Epoch 47/1000, LR 0.000269
Train loss: 0.2328;  Loss pred: 0.2328; Loss self: 0.0000; time: 0.15s
Val loss: 0.5780 score: 0.6327 time: 0.10s
Test loss: 0.5848 score: 0.6327 time: 0.09s
Epoch 48/1000, LR 0.000269
Train loss: 0.2405;  Loss pred: 0.2405; Loss self: 0.0000; time: 0.14s
Val loss: 0.5787 score: 0.6327 time: 0.10s
Test loss: 0.5858 score: 0.6327 time: 0.09s
     INFO: Early stopping counter 1 of 20
Epoch 49/1000, LR 0.000269
Train loss: 0.2239;  Loss pred: 0.2239; Loss self: 0.0000; time: 0.15s
Val loss: 0.5795 score: 0.6327 time: 0.12s
Test loss: 0.5869 score: 0.6122 time: 0.09s
     INFO: Early stopping counter 2 of 20
Epoch 50/1000, LR 0.000269
Train loss: 0.2365;  Loss pred: 0.2365; Loss self: 0.0000; time: 0.15s
Val loss: 0.5800 score: 0.6327 time: 0.10s
Test loss: 0.5877 score: 0.6122 time: 0.09s
     INFO: Early stopping counter 3 of 20
Epoch 51/1000, LR 0.000269
Train loss: 0.2038;  Loss pred: 0.2038; Loss self: 0.0000; time: 0.15s
Val loss: 0.5797 score: 0.6327 time: 0.10s
Test loss: 0.5877 score: 0.6122 time: 0.09s
     INFO: Early stopping counter 4 of 20
Epoch 52/1000, LR 0.000269
Train loss: 0.2138;  Loss pred: 0.2138; Loss self: 0.0000; time: 0.14s
Val loss: 0.5784 score: 0.6327 time: 0.10s
Test loss: 0.5867 score: 0.6122 time: 0.08s
     INFO: Early stopping counter 5 of 20
Epoch 53/1000, LR 0.000269
Train loss: 0.2074;  Loss pred: 0.2074; Loss self: 0.0000; time: 0.14s
Val loss: 0.5782 score: 0.6327 time: 0.10s
Test loss: 0.5867 score: 0.6122 time: 0.08s
     INFO: Early stopping counter 6 of 20
Epoch 54/1000, LR 0.000269
Train loss: 0.2074;  Loss pred: 0.2074; Loss self: 0.0000; time: 0.14s
Val loss: 0.5769 score: 0.6327 time: 0.10s
Test loss: 0.5857 score: 0.6122 time: 0.09s
Epoch 55/1000, LR 0.000269
Train loss: 0.1872;  Loss pred: 0.1872; Loss self: 0.0000; time: 0.14s
Val loss: 0.5744 score: 0.6327 time: 0.10s
Test loss: 0.5835 score: 0.6122 time: 0.09s
Epoch 56/1000, LR 0.000269
Train loss: 0.1942;  Loss pred: 0.1942; Loss self: 0.0000; time: 0.14s
Val loss: 0.5726 score: 0.6327 time: 0.10s
Test loss: 0.5820 score: 0.6327 time: 0.19s
Epoch 57/1000, LR 0.000269
Train loss: 0.1956;  Loss pred: 0.1956; Loss self: 0.0000; time: 0.14s
Val loss: 0.5690 score: 0.6327 time: 0.10s
Test loss: 0.5787 score: 0.6327 time: 0.09s
Epoch 58/1000, LR 0.000269
Train loss: 0.1914;  Loss pred: 0.1914; Loss self: 0.0000; time: 0.14s
Val loss: 0.5661 score: 0.6327 time: 0.10s
Test loss: 0.5762 score: 0.6327 time: 0.08s
Epoch 59/1000, LR 0.000268
Train loss: 0.1892;  Loss pred: 0.1892; Loss self: 0.0000; time: 0.14s
Val loss: 0.5631 score: 0.6327 time: 0.11s
Test loss: 0.5735 score: 0.6327 time: 0.08s
Epoch 60/1000, LR 0.000268
Train loss: 0.1898;  Loss pred: 0.1898; Loss self: 0.0000; time: 0.14s
Val loss: 0.5583 score: 0.6327 time: 0.10s
Test loss: 0.5692 score: 0.6327 time: 0.08s
Epoch 61/1000, LR 0.000268
Train loss: 0.1812;  Loss pred: 0.1812; Loss self: 0.0000; time: 0.14s
Val loss: 0.5538 score: 0.6327 time: 0.10s
Test loss: 0.5651 score: 0.6327 time: 0.09s
Epoch 62/1000, LR 0.000268
Train loss: 0.1866;  Loss pred: 0.1866; Loss self: 0.0000; time: 0.15s
Val loss: 0.5490 score: 0.6327 time: 0.10s
Test loss: 0.5609 score: 0.6327 time: 0.09s
Epoch 63/1000, LR 0.000268
Train loss: 0.1828;  Loss pred: 0.1828; Loss self: 0.0000; time: 0.15s
Val loss: 0.5438 score: 0.6327 time: 0.10s
Test loss: 0.5563 score: 0.6122 time: 0.09s
Epoch 64/1000, LR 0.000268
Train loss: 0.1678;  Loss pred: 0.1678; Loss self: 0.0000; time: 0.15s
Val loss: 0.5393 score: 0.6531 time: 0.20s
Test loss: 0.5523 score: 0.6122 time: 0.09s
Epoch 65/1000, LR 0.000268
Train loss: 0.1789;  Loss pred: 0.1789; Loss self: 0.0000; time: 0.14s
Val loss: 0.5357 score: 0.6531 time: 0.10s
Test loss: 0.5489 score: 0.6122 time: 0.09s
Epoch 66/1000, LR 0.000268
Train loss: 0.1694;  Loss pred: 0.1694; Loss self: 0.0000; time: 0.15s
Val loss: 0.5322 score: 0.6531 time: 0.10s
Test loss: 0.5454 score: 0.6327 time: 0.09s
Epoch 67/1000, LR 0.000268
Train loss: 0.1585;  Loss pred: 0.1585; Loss self: 0.0000; time: 0.15s
Val loss: 0.5282 score: 0.6735 time: 0.10s
Test loss: 0.5414 score: 0.6531 time: 0.09s
Epoch 68/1000, LR 0.000268
Train loss: 0.1577;  Loss pred: 0.1577; Loss self: 0.0000; time: 0.15s
Val loss: 0.5227 score: 0.6735 time: 0.10s
Test loss: 0.5360 score: 0.6531 time: 0.09s
Epoch 69/1000, LR 0.000268
Train loss: 0.1581;  Loss pred: 0.1581; Loss self: 0.0000; time: 0.15s
Val loss: 0.5165 score: 0.6735 time: 0.10s
Test loss: 0.5299 score: 0.6531 time: 0.09s
Epoch 70/1000, LR 0.000268
Train loss: 0.1620;  Loss pred: 0.1620; Loss self: 0.0000; time: 0.15s
Val loss: 0.5105 score: 0.6735 time: 0.10s
Test loss: 0.5243 score: 0.6531 time: 0.09s
Epoch 71/1000, LR 0.000268
Train loss: 0.1451;  Loss pred: 0.1451; Loss self: 0.0000; time: 0.15s
Val loss: 0.5052 score: 0.6735 time: 0.10s
Test loss: 0.5195 score: 0.6531 time: 0.09s
Epoch 72/1000, LR 0.000267
Train loss: 0.1430;  Loss pred: 0.1430; Loss self: 0.0000; time: 0.15s
Val loss: 0.4988 score: 0.6735 time: 0.10s
Test loss: 0.5139 score: 0.6531 time: 0.09s
Epoch 73/1000, LR 0.000267
Train loss: 0.1416;  Loss pred: 0.1416; Loss self: 0.0000; time: 0.15s
Val loss: 0.4920 score: 0.6939 time: 0.10s
Test loss: 0.5080 score: 0.6531 time: 0.09s
Epoch 74/1000, LR 0.000267
Train loss: 0.1535;  Loss pred: 0.1535; Loss self: 0.0000; time: 0.15s
Val loss: 0.4855 score: 0.6939 time: 0.10s
Test loss: 0.5023 score: 0.6531 time: 0.09s
Epoch 75/1000, LR 0.000267
Train loss: 0.1445;  Loss pred: 0.1445; Loss self: 0.0000; time: 0.15s
Val loss: 0.4773 score: 0.6939 time: 0.10s
Test loss: 0.4952 score: 0.6531 time: 0.08s
Epoch 76/1000, LR 0.000267
Train loss: 0.1435;  Loss pred: 0.1435; Loss self: 0.0000; time: 0.13s
Val loss: 0.4701 score: 0.6939 time: 0.10s
Test loss: 0.4890 score: 0.6531 time: 0.09s
Epoch 77/1000, LR 0.000267
Train loss: 0.1425;  Loss pred: 0.1425; Loss self: 0.0000; time: 0.13s
Val loss: 0.4617 score: 0.6939 time: 0.10s
Test loss: 0.4819 score: 0.6531 time: 0.09s
Epoch 78/1000, LR 0.000267
Train loss: 0.1255;  Loss pred: 0.1255; Loss self: 0.0000; time: 0.17s
Val loss: 0.4528 score: 0.7143 time: 0.10s
Test loss: 0.4743 score: 0.6531 time: 0.08s
Epoch 79/1000, LR 0.000267
Train loss: 0.1341;  Loss pred: 0.1341; Loss self: 0.0000; time: 0.14s
Val loss: 0.4435 score: 0.7347 time: 0.10s
Test loss: 0.4663 score: 0.6531 time: 0.09s
Epoch 80/1000, LR 0.000267
Train loss: 0.1338;  Loss pred: 0.1338; Loss self: 0.0000; time: 0.14s
Val loss: 0.4337 score: 0.7347 time: 0.10s
Test loss: 0.4581 score: 0.6531 time: 0.09s
Epoch 81/1000, LR 0.000267
Train loss: 0.1291;  Loss pred: 0.1291; Loss self: 0.0000; time: 0.14s
Val loss: 0.4243 score: 0.7347 time: 0.10s
Test loss: 0.4501 score: 0.6735 time: 0.09s
Epoch 82/1000, LR 0.000267
Train loss: 0.1456;  Loss pred: 0.1456; Loss self: 0.0000; time: 0.15s
Val loss: 0.4147 score: 0.7755 time: 0.10s
Test loss: 0.4420 score: 0.7143 time: 0.09s
Epoch 83/1000, LR 0.000266
Train loss: 0.1288;  Loss pred: 0.1288; Loss self: 0.0000; time: 0.15s
Val loss: 0.4048 score: 0.7959 time: 0.10s
Test loss: 0.4337 score: 0.7143 time: 0.09s
Epoch 84/1000, LR 0.000266
Train loss: 0.1227;  Loss pred: 0.1227; Loss self: 0.0000; time: 0.15s
Val loss: 0.3957 score: 0.7959 time: 0.10s
Test loss: 0.4258 score: 0.7143 time: 0.09s
Epoch 85/1000, LR 0.000266
Train loss: 0.1157;  Loss pred: 0.1157; Loss self: 0.0000; time: 0.14s
Val loss: 0.3863 score: 0.7959 time: 0.10s
Test loss: 0.4179 score: 0.7347 time: 0.09s
Epoch 86/1000, LR 0.000266
Train loss: 0.1212;  Loss pred: 0.1212; Loss self: 0.0000; time: 0.14s
Val loss: 0.3762 score: 0.7959 time: 0.10s
Test loss: 0.4093 score: 0.7347 time: 0.09s
Epoch 87/1000, LR 0.000266
Train loss: 0.1248;  Loss pred: 0.1248; Loss self: 0.0000; time: 0.15s
Val loss: 0.3653 score: 0.8367 time: 0.10s
Test loss: 0.4002 score: 0.7347 time: 0.09s
Epoch 88/1000, LR 0.000266
Train loss: 0.1159;  Loss pred: 0.1159; Loss self: 0.0000; time: 0.15s
Val loss: 0.3555 score: 0.8367 time: 0.11s
Test loss: 0.3921 score: 0.7551 time: 0.09s
Epoch 89/1000, LR 0.000266
Train loss: 0.1085;  Loss pred: 0.1085; Loss self: 0.0000; time: 0.14s
Val loss: 0.3447 score: 0.8367 time: 0.10s
Test loss: 0.3831 score: 0.7551 time: 0.09s
Epoch 90/1000, LR 0.000266
Train loss: 0.1057;  Loss pred: 0.1057; Loss self: 0.0000; time: 0.15s
Val loss: 0.3335 score: 0.8367 time: 0.10s
Test loss: 0.3740 score: 0.7755 time: 0.09s
Epoch 91/1000, LR 0.000266
Train loss: 0.1044;  Loss pred: 0.1044; Loss self: 0.0000; time: 0.15s
Val loss: 0.3253 score: 0.8367 time: 0.10s
Test loss: 0.3672 score: 0.7755 time: 0.09s
Epoch 92/1000, LR 0.000266
Train loss: 0.1079;  Loss pred: 0.1079; Loss self: 0.0000; time: 0.15s
Val loss: 0.3158 score: 0.8571 time: 0.10s
Test loss: 0.3595 score: 0.7755 time: 0.10s
Epoch 93/1000, LR 0.000265
Train loss: 0.1101;  Loss pred: 0.1101; Loss self: 0.0000; time: 0.15s
Val loss: 0.3069 score: 0.8571 time: 0.10s
Test loss: 0.3525 score: 0.7755 time: 0.09s
Epoch 94/1000, LR 0.000265
Train loss: 0.1044;  Loss pred: 0.1044; Loss self: 0.0000; time: 0.15s
Val loss: 0.2982 score: 0.8776 time: 0.10s
Test loss: 0.3456 score: 0.8163 time: 0.09s
Epoch 95/1000, LR 0.000265
Train loss: 0.1042;  Loss pred: 0.1042; Loss self: 0.0000; time: 0.15s
Val loss: 0.2885 score: 0.8980 time: 0.10s
Test loss: 0.3384 score: 0.8367 time: 0.09s
Epoch 96/1000, LR 0.000265
Train loss: 0.0977;  Loss pred: 0.0977; Loss self: 0.0000; time: 0.15s
Val loss: 0.2802 score: 0.9184 time: 0.13s
Test loss: 0.3323 score: 0.8367 time: 0.09s
Epoch 97/1000, LR 0.000265
Train loss: 0.1000;  Loss pred: 0.1000; Loss self: 0.0000; time: 0.15s
Val loss: 0.2711 score: 0.9184 time: 0.10s
Test loss: 0.3258 score: 0.8776 time: 0.09s
Epoch 98/1000, LR 0.000265
Train loss: 0.1014;  Loss pred: 0.1014; Loss self: 0.0000; time: 0.15s
Val loss: 0.2627 score: 0.9184 time: 0.11s
Test loss: 0.3202 score: 0.8776 time: 0.08s
Epoch 99/1000, LR 0.000265
Train loss: 0.0979;  Loss pred: 0.0979; Loss self: 0.0000; time: 0.14s
Val loss: 0.2567 score: 0.9184 time: 0.10s
Test loss: 0.3163 score: 0.8776 time: 0.09s
Epoch 100/1000, LR 0.000265
Train loss: 0.0970;  Loss pred: 0.0970; Loss self: 0.0000; time: 0.14s
Val loss: 0.2505 score: 0.9184 time: 0.09s
Test loss: 0.3123 score: 0.8776 time: 0.10s
Epoch 101/1000, LR 0.000265
Train loss: 0.0979;  Loss pred: 0.0979; Loss self: 0.0000; time: 0.14s
Val loss: 0.2441 score: 0.9184 time: 0.09s
Test loss: 0.3082 score: 0.8776 time: 0.08s
Epoch 102/1000, LR 0.000264
Train loss: 0.0940;  Loss pred: 0.0940; Loss self: 0.0000; time: 0.14s
Val loss: 0.2392 score: 0.9184 time: 0.09s
Test loss: 0.3050 score: 0.8776 time: 0.08s
Epoch 103/1000, LR 0.000264
Train loss: 0.0926;  Loss pred: 0.0926; Loss self: 0.0000; time: 0.15s
Val loss: 0.2332 score: 0.9184 time: 0.10s
Test loss: 0.3014 score: 0.8776 time: 0.08s
Epoch 104/1000, LR 0.000264
Train loss: 0.0982;  Loss pred: 0.0982; Loss self: 0.0000; time: 0.15s
Val loss: 0.2257 score: 0.9184 time: 0.10s
Test loss: 0.2967 score: 0.8776 time: 0.08s
Epoch 105/1000, LR 0.000264
Train loss: 0.0898;  Loss pred: 0.0898; Loss self: 0.0000; time: 0.14s
Val loss: 0.2186 score: 0.9184 time: 0.10s
Test loss: 0.2922 score: 0.8776 time: 0.08s
Epoch 106/1000, LR 0.000264
Train loss: 0.0930;  Loss pred: 0.0930; Loss self: 0.0000; time: 0.14s
Val loss: 0.2129 score: 0.9184 time: 0.10s
Test loss: 0.2888 score: 0.8776 time: 0.08s
Epoch 107/1000, LR 0.000264
Train loss: 0.0841;  Loss pred: 0.0841; Loss self: 0.0000; time: 0.15s
Val loss: 0.2075 score: 0.9184 time: 0.09s
Test loss: 0.2855 score: 0.8776 time: 0.08s
Epoch 108/1000, LR 0.000264
Train loss: 0.0833;  Loss pred: 0.0833; Loss self: 0.0000; time: 0.14s
Val loss: 0.2012 score: 0.9184 time: 0.09s
Test loss: 0.2815 score: 0.8980 time: 0.08s
Epoch 109/1000, LR 0.000264
Train loss: 0.0882;  Loss pred: 0.0882; Loss self: 0.0000; time: 0.14s
Val loss: 0.1956 score: 0.9184 time: 0.23s
Test loss: 0.2778 score: 0.8980 time: 0.09s
Epoch 110/1000, LR 0.000263
Train loss: 0.0816;  Loss pred: 0.0816; Loss self: 0.0000; time: 0.15s
Val loss: 0.1906 score: 0.9184 time: 0.10s
Test loss: 0.2745 score: 0.8980 time: 0.09s
Epoch 111/1000, LR 0.000263
Train loss: 0.0828;  Loss pred: 0.0828; Loss self: 0.0000; time: 0.14s
Val loss: 0.1862 score: 0.9184 time: 0.10s
Test loss: 0.2718 score: 0.8980 time: 0.08s
Epoch 112/1000, LR 0.000263
Train loss: 0.0780;  Loss pred: 0.0780; Loss self: 0.0000; time: 0.14s
Val loss: 0.1829 score: 0.9184 time: 0.10s
Test loss: 0.2698 score: 0.8980 time: 0.09s
Epoch 113/1000, LR 0.000263
Train loss: 0.0810;  Loss pred: 0.0810; Loss self: 0.0000; time: 0.14s
Val loss: 0.1794 score: 0.9184 time: 0.10s
Test loss: 0.2678 score: 0.8980 time: 0.09s
Epoch 114/1000, LR 0.000263
Train loss: 0.0720;  Loss pred: 0.0720; Loss self: 0.0000; time: 0.15s
Val loss: 0.1756 score: 0.9184 time: 0.10s
Test loss: 0.2658 score: 0.8980 time: 0.09s
Epoch 115/1000, LR 0.000263
Train loss: 0.0734;  Loss pred: 0.0734; Loss self: 0.0000; time: 0.15s
Val loss: 0.1732 score: 0.9184 time: 0.10s
Test loss: 0.2646 score: 0.8980 time: 0.09s
Epoch 116/1000, LR 0.000263
Train loss: 0.0771;  Loss pred: 0.0771; Loss self: 0.0000; time: 0.15s
Val loss: 0.1708 score: 0.9184 time: 0.17s
Test loss: 0.2635 score: 0.8980 time: 0.09s
Epoch 117/1000, LR 0.000262
Train loss: 0.0767;  Loss pred: 0.0767; Loss self: 0.0000; time: 0.14s
Val loss: 0.1680 score: 0.9184 time: 0.10s
Test loss: 0.2624 score: 0.8776 time: 0.09s
Epoch 118/1000, LR 0.000262
Train loss: 0.0682;  Loss pred: 0.0682; Loss self: 0.0000; time: 0.15s
Val loss: 0.1661 score: 0.9184 time: 0.10s
Test loss: 0.2619 score: 0.8776 time: 0.09s
Epoch 119/1000, LR 0.000262
Train loss: 0.0728;  Loss pred: 0.0728; Loss self: 0.0000; time: 0.14s
Val loss: 0.1636 score: 0.9184 time: 0.10s
Test loss: 0.2614 score: 0.8776 time: 0.09s
Epoch 120/1000, LR 0.000262
Train loss: 0.0649;  Loss pred: 0.0649; Loss self: 0.0000; time: 0.14s
Val loss: 0.1607 score: 0.9184 time: 0.10s
Test loss: 0.2609 score: 0.8776 time: 0.09s
Epoch 121/1000, LR 0.000262
Train loss: 0.0708;  Loss pred: 0.0708; Loss self: 0.0000; time: 0.14s
Val loss: 0.1584 score: 0.9184 time: 0.10s
Test loss: 0.2603 score: 0.8776 time: 0.09s
Epoch 122/1000, LR 0.000262
Train loss: 0.0685;  Loss pred: 0.0685; Loss self: 0.0000; time: 0.15s
Val loss: 0.1567 score: 0.9184 time: 0.11s
Test loss: 0.2600 score: 0.8776 time: 0.09s
Epoch 123/1000, LR 0.000262
Train loss: 0.0710;  Loss pred: 0.0710; Loss self: 0.0000; time: 0.15s
Val loss: 0.1547 score: 0.9184 time: 0.24s
Test loss: 0.2596 score: 0.8776 time: 0.09s
Epoch 124/1000, LR 0.000261
Train loss: 0.0694;  Loss pred: 0.0694; Loss self: 0.0000; time: 0.15s
Val loss: 0.1534 score: 0.9184 time: 0.10s
Test loss: 0.2594 score: 0.8776 time: 0.09s
Epoch 125/1000, LR 0.000261
Train loss: 0.0662;  Loss pred: 0.0662; Loss self: 0.0000; time: 0.15s
Val loss: 0.1517 score: 0.9184 time: 0.10s
Test loss: 0.2590 score: 0.8776 time: 0.09s
Epoch 126/1000, LR 0.000261
Train loss: 0.0659;  Loss pred: 0.0659; Loss self: 0.0000; time: 0.15s
Val loss: 0.1499 score: 0.9388 time: 0.10s
Test loss: 0.2588 score: 0.8776 time: 0.09s
Epoch 127/1000, LR 0.000261
Train loss: 0.0628;  Loss pred: 0.0628; Loss self: 0.0000; time: 0.15s
Val loss: 0.1485 score: 0.9388 time: 0.10s
Test loss: 0.2589 score: 0.8776 time: 0.09s
Epoch 128/1000, LR 0.000261
Train loss: 0.0604;  Loss pred: 0.0604; Loss self: 0.0000; time: 0.15s
Val loss: 0.1469 score: 0.9388 time: 0.10s
Test loss: 0.2590 score: 0.8776 time: 0.09s
Epoch 129/1000, LR 0.000261
Train loss: 0.0596;  Loss pred: 0.0596; Loss self: 0.0000; time: 0.15s
Val loss: 0.1457 score: 0.9388 time: 0.10s
Test loss: 0.2589 score: 0.8776 time: 0.09s
Epoch 130/1000, LR 0.000260
Train loss: 0.0601;  Loss pred: 0.0601; Loss self: 0.0000; time: 0.15s
Val loss: 0.1445 score: 0.9388 time: 0.11s
Test loss: 0.2585 score: 0.8776 time: 0.16s
Epoch 131/1000, LR 0.000260
Train loss: 0.0564;  Loss pred: 0.0564; Loss self: 0.0000; time: 0.14s
Val loss: 0.1430 score: 0.9388 time: 0.10s
Test loss: 0.2582 score: 0.8776 time: 0.08s
Epoch 132/1000, LR 0.000260
Train loss: 0.0614;  Loss pred: 0.0614; Loss self: 0.0000; time: 0.14s
Val loss: 0.1421 score: 0.9388 time: 0.10s
Test loss: 0.2580 score: 0.8776 time: 0.08s
Epoch 133/1000, LR 0.000260
Train loss: 0.0606;  Loss pred: 0.0606; Loss self: 0.0000; time: 0.14s
Val loss: 0.1410 score: 0.9388 time: 0.10s
Test loss: 0.2581 score: 0.8776 time: 0.09s
Epoch 134/1000, LR 0.000260
Train loss: 0.0631;  Loss pred: 0.0631; Loss self: 0.0000; time: 0.15s
Val loss: 0.1403 score: 0.9388 time: 0.10s
Test loss: 0.2582 score: 0.8776 time: 0.09s
Epoch 135/1000, LR 0.000260
Train loss: 0.0540;  Loss pred: 0.0540; Loss self: 0.0000; time: 0.15s
Val loss: 0.1398 score: 0.9388 time: 0.10s
Test loss: 0.2581 score: 0.8776 time: 0.08s
Epoch 136/1000, LR 0.000260
Train loss: 0.0589;  Loss pred: 0.0589; Loss self: 0.0000; time: 0.15s
Val loss: 0.1395 score: 0.9388 time: 0.10s
Test loss: 0.2582 score: 0.8776 time: 0.09s
Epoch 137/1000, LR 0.000259
Train loss: 0.0571;  Loss pred: 0.0571; Loss self: 0.0000; time: 0.15s
Val loss: 0.1387 score: 0.9388 time: 0.10s
Test loss: 0.2586 score: 0.8776 time: 0.22s
Epoch 138/1000, LR 0.000259
Train loss: 0.0508;  Loss pred: 0.0508; Loss self: 0.0000; time: 0.14s
Val loss: 0.1381 score: 0.9388 time: 0.10s
Test loss: 0.2591 score: 0.8776 time: 0.09s
Epoch 139/1000, LR 0.000259
Train loss: 0.0530;  Loss pred: 0.0530; Loss self: 0.0000; time: 0.15s
Val loss: 0.1374 score: 0.9388 time: 0.10s
Test loss: 0.2592 score: 0.8980 time: 0.09s
Epoch 140/1000, LR 0.000259
Train loss: 0.0553;  Loss pred: 0.0553; Loss self: 0.0000; time: 0.15s
Val loss: 0.1365 score: 0.9388 time: 0.10s
Test loss: 0.2597 score: 0.8980 time: 0.09s
Epoch 141/1000, LR 0.000259
Train loss: 0.0635;  Loss pred: 0.0635; Loss self: 0.0000; time: 0.15s
Val loss: 0.1354 score: 0.9388 time: 0.10s
Test loss: 0.2606 score: 0.8980 time: 0.09s
Epoch 142/1000, LR 0.000259
Train loss: 0.0639;  Loss pred: 0.0639; Loss self: 0.0000; time: 0.14s
Val loss: 0.1350 score: 0.9388 time: 0.10s
Test loss: 0.2609 score: 0.8980 time: 0.09s
Epoch 143/1000, LR 0.000258
Train loss: 0.0505;  Loss pred: 0.0505; Loss self: 0.0000; time: 0.16s
Val loss: 0.1346 score: 0.9388 time: 0.10s
Test loss: 0.2612 score: 0.8980 time: 0.09s
Epoch 144/1000, LR 0.000258
Train loss: 0.0542;  Loss pred: 0.0542; Loss self: 0.0000; time: 0.15s
Val loss: 0.1346 score: 0.9388 time: 0.10s
Test loss: 0.2616 score: 0.8980 time: 0.09s
     INFO: Early stopping counter 1 of 20
Epoch 145/1000, LR 0.000258
Train loss: 0.0570;  Loss pred: 0.0570; Loss self: 0.0000; time: 0.18s
Val loss: 0.1348 score: 0.9388 time: 0.10s
Test loss: 0.2617 score: 0.8980 time: 0.09s
     INFO: Early stopping counter 2 of 20
Epoch 146/1000, LR 0.000258
Train loss: 0.0495;  Loss pred: 0.0495; Loss self: 0.0000; time: 0.14s
Val loss: 0.1356 score: 0.9388 time: 0.09s
Test loss: 0.2619 score: 0.8980 time: 0.08s
     INFO: Early stopping counter 3 of 20
Epoch 147/1000, LR 0.000258
Train loss: 0.0546;  Loss pred: 0.0546; Loss self: 0.0000; time: 0.14s
Val loss: 0.1354 score: 0.9388 time: 0.10s
Test loss: 0.2623 score: 0.8980 time: 0.08s
     INFO: Early stopping counter 4 of 20
Epoch 148/1000, LR 0.000257
Train loss: 0.0520;  Loss pred: 0.0520; Loss self: 0.0000; time: 0.14s
Val loss: 0.1352 score: 0.9388 time: 0.10s
Test loss: 0.2624 score: 0.8980 time: 0.08s
     INFO: Early stopping counter 5 of 20
Epoch 149/1000, LR 0.000257
Train loss: 0.0546;  Loss pred: 0.0546; Loss self: 0.0000; time: 0.14s
Val loss: 0.1351 score: 0.9388 time: 0.10s
Test loss: 0.2635 score: 0.8980 time: 0.08s
     INFO: Early stopping counter 6 of 20
Epoch 150/1000, LR 0.000257
Train loss: 0.0527;  Loss pred: 0.0527; Loss self: 0.0000; time: 0.14s
Val loss: 0.1348 score: 0.9388 time: 0.10s
Test loss: 0.2649 score: 0.8980 time: 0.09s
     INFO: Early stopping counter 7 of 20
Epoch 151/1000, LR 0.000257
Train loss: 0.0441;  Loss pred: 0.0441; Loss self: 0.0000; time: 0.14s
Val loss: 0.1339 score: 0.9388 time: 0.10s
Test loss: 0.2663 score: 0.8980 time: 0.09s
Epoch 152/1000, LR 0.000257
Train loss: 0.0532;  Loss pred: 0.0532; Loss self: 0.0000; time: 0.14s
Val loss: 0.1336 score: 0.9388 time: 0.13s
Test loss: 0.2677 score: 0.8980 time: 2.82s
Epoch 153/1000, LR 0.000257
Train loss: 0.0529;  Loss pred: 0.0529; Loss self: 0.0000; time: 1.44s
Val loss: 0.1336 score: 0.9388 time: 2.23s
Test loss: 0.2693 score: 0.8980 time: 0.08s
Epoch 154/1000, LR 0.000256
Train loss: 0.0446;  Loss pred: 0.0446; Loss self: 0.0000; time: 0.14s
Val loss: 0.1332 score: 0.9388 time: 0.10s
Test loss: 0.2707 score: 0.8980 time: 0.09s
Epoch 155/1000, LR 0.000256
Train loss: 0.0419;  Loss pred: 0.0419; Loss self: 0.0000; time: 0.14s
Val loss: 0.1327 score: 0.9388 time: 0.10s
Test loss: 0.2722 score: 0.8980 time: 0.09s
Epoch 156/1000, LR 0.000256
Train loss: 0.0506;  Loss pred: 0.0506; Loss self: 0.0000; time: 0.14s
Val loss: 0.1320 score: 0.9388 time: 0.10s
Test loss: 0.2733 score: 0.8980 time: 0.09s
Epoch 157/1000, LR 0.000256
Train loss: 0.0429;  Loss pred: 0.0429; Loss self: 0.0000; time: 0.15s
Val loss: 0.1318 score: 0.9388 time: 0.10s
Test loss: 0.2744 score: 0.8980 time: 0.09s
Epoch 158/1000, LR 0.000256
Train loss: 0.0437;  Loss pred: 0.0437; Loss self: 0.0000; time: 0.15s
Val loss: 0.1307 score: 0.9388 time: 0.11s
Test loss: 0.2760 score: 0.8980 time: 0.08s
Epoch 159/1000, LR 0.000255
Train loss: 0.0444;  Loss pred: 0.0444; Loss self: 0.0000; time: 0.14s
Val loss: 0.1298 score: 0.9388 time: 0.10s
Test loss: 0.2774 score: 0.8980 time: 0.08s
Epoch 160/1000, LR 0.000255
Train loss: 0.0501;  Loss pred: 0.0501; Loss self: 0.0000; time: 0.15s
Val loss: 0.1294 score: 0.9388 time: 0.14s
Test loss: 0.2780 score: 0.8980 time: 0.08s
Epoch 161/1000, LR 0.000255
Train loss: 0.0442;  Loss pred: 0.0442; Loss self: 0.0000; time: 0.14s
Val loss: 0.1298 score: 0.9388 time: 0.10s
Test loss: 0.2780 score: 0.8980 time: 0.08s
     INFO: Early stopping counter 1 of 20
Epoch 162/1000, LR 0.000255
Train loss: 0.0485;  Loss pred: 0.0485; Loss self: 0.0000; time: 0.14s
Val loss: 0.1287 score: 0.9388 time: 0.10s
Test loss: 0.2798 score: 0.8980 time: 0.08s
Epoch 163/1000, LR 0.000255
Train loss: 0.0423;  Loss pred: 0.0423; Loss self: 0.0000; time: 0.14s
Val loss: 0.1278 score: 0.9388 time: 0.10s
Test loss: 0.2812 score: 0.8980 time: 0.09s
Epoch 164/1000, LR 0.000254
Train loss: 0.0426;  Loss pred: 0.0426; Loss self: 0.0000; time: 0.14s
Val loss: 0.1276 score: 0.9388 time: 0.10s
Test loss: 0.2821 score: 0.8980 time: 0.08s
Epoch 165/1000, LR 0.000254
Train loss: 0.0465;  Loss pred: 0.0465; Loss self: 0.0000; time: 0.14s
Val loss: 0.1271 score: 0.9388 time: 0.10s
Test loss: 0.2835 score: 0.8980 time: 0.09s
Epoch 166/1000, LR 0.000254
Train loss: 0.0534;  Loss pred: 0.0534; Loss self: 0.0000; time: 0.16s
Val loss: 0.1263 score: 0.9388 time: 0.10s
Test loss: 0.2850 score: 0.8980 time: 0.09s
Epoch 167/1000, LR 0.000254
Train loss: 0.0403;  Loss pred: 0.0403; Loss self: 0.0000; time: 0.15s
Val loss: 0.1256 score: 0.9388 time: 0.10s
Test loss: 0.2867 score: 0.8980 time: 0.21s
Epoch 168/1000, LR 0.000254
Train loss: 0.0397;  Loss pred: 0.0397; Loss self: 0.0000; time: 0.14s
Val loss: 0.1253 score: 0.9388 time: 0.10s
Test loss: 0.2877 score: 0.8980 time: 0.08s
Epoch 169/1000, LR 0.000253
Train loss: 0.0464;  Loss pred: 0.0464; Loss self: 0.0000; time: 0.13s
Val loss: 0.1252 score: 0.9388 time: 0.09s
Test loss: 0.2889 score: 0.8980 time: 0.08s
Epoch 170/1000, LR 0.000253
Train loss: 0.0358;  Loss pred: 0.0358; Loss self: 0.0000; time: 0.14s
Val loss: 0.1250 score: 0.9388 time: 0.09s
Test loss: 0.2900 score: 0.8980 time: 0.08s
Epoch 171/1000, LR 0.000253
Train loss: 0.0517;  Loss pred: 0.0517; Loss self: 0.0000; time: 0.13s
Val loss: 0.1250 score: 0.9388 time: 0.09s
Test loss: 0.2914 score: 0.8980 time: 0.08s
Epoch 172/1000, LR 0.000253
Train loss: 0.0405;  Loss pred: 0.0405; Loss self: 0.0000; time: 0.14s
Val loss: 0.1246 score: 0.9388 time: 0.09s
Test loss: 0.2926 score: 0.8980 time: 0.08s
Epoch 173/1000, LR 0.000253
Train loss: 0.0335;  Loss pred: 0.0335; Loss self: 0.0000; time: 0.15s
Val loss: 0.1235 score: 0.9388 time: 0.10s
Test loss: 0.2938 score: 0.9184 time: 0.08s
Epoch 174/1000, LR 0.000252
Train loss: 0.0351;  Loss pred: 0.0351; Loss self: 0.0000; time: 0.14s
Val loss: 0.1229 score: 0.9388 time: 0.09s
Test loss: 0.2946 score: 0.9184 time: 0.08s
Epoch 175/1000, LR 0.000252
Train loss: 0.0440;  Loss pred: 0.0440; Loss self: 0.0000; time: 0.15s
Val loss: 0.1221 score: 0.9388 time: 0.21s
Test loss: 0.2948 score: 0.9184 time: 0.08s
Epoch 176/1000, LR 0.000252
Train loss: 0.0400;  Loss pred: 0.0400; Loss self: 0.0000; time: 0.14s
Val loss: 0.1211 score: 0.9592 time: 0.10s
Test loss: 0.2956 score: 0.9184 time: 0.08s
Epoch 177/1000, LR 0.000252
Train loss: 0.0358;  Loss pred: 0.0358; Loss self: 0.0000; time: 0.14s
Val loss: 0.1203 score: 0.9592 time: 0.10s
Test loss: 0.2959 score: 0.9184 time: 0.08s
Epoch 178/1000, LR 0.000251
Train loss: 0.0339;  Loss pred: 0.0339; Loss self: 0.0000; time: 0.13s
Val loss: 0.1199 score: 0.9592 time: 0.09s
Test loss: 0.2958 score: 0.9184 time: 0.09s
Epoch 179/1000, LR 0.000251
Train loss: 0.0342;  Loss pred: 0.0342; Loss self: 0.0000; time: 0.15s
Val loss: 0.1199 score: 0.9388 time: 0.10s
Test loss: 0.2953 score: 0.9184 time: 0.08s
     INFO: Early stopping counter 1 of 20
Epoch 180/1000, LR 0.000251
Train loss: 0.0359;  Loss pred: 0.0359; Loss self: 0.0000; time: 0.14s
Val loss: 0.1196 score: 0.9388 time: 0.10s
Test loss: 0.2948 score: 0.9184 time: 0.08s
Epoch 181/1000, LR 0.000251
Train loss: 0.0300;  Loss pred: 0.0300; Loss self: 0.0000; time: 0.14s
Val loss: 0.1193 score: 0.9388 time: 0.10s
Test loss: 0.2947 score: 0.9184 time: 0.08s
Epoch 182/1000, LR 0.000251
Train loss: 0.0350;  Loss pred: 0.0350; Loss self: 0.0000; time: 0.14s
Val loss: 0.1189 score: 0.9388 time: 0.10s
Test loss: 0.2949 score: 0.9184 time: 0.22s
Epoch 183/1000, LR 0.000250
Train loss: 0.0351;  Loss pred: 0.0351; Loss self: 0.0000; time: 0.14s
Val loss: 0.1182 score: 0.9388 time: 0.11s
Test loss: 0.2952 score: 0.9184 time: 0.09s
Epoch 184/1000, LR 0.000250
Train loss: 0.0427;  Loss pred: 0.0427; Loss self: 0.0000; time: 0.14s
Val loss: 0.1186 score: 0.9388 time: 0.10s
Test loss: 0.2948 score: 0.9184 time: 0.09s
     INFO: Early stopping counter 1 of 20
Epoch 185/1000, LR 0.000250
Train loss: 0.0348;  Loss pred: 0.0348; Loss self: 0.0000; time: 0.14s
Val loss: 0.1195 score: 0.9388 time: 0.10s
Test loss: 0.2946 score: 0.8776 time: 0.09s
     INFO: Early stopping counter 2 of 20
Epoch 186/1000, LR 0.000250
Train loss: 0.0350;  Loss pred: 0.0350; Loss self: 0.0000; time: 0.14s
Val loss: 0.1198 score: 0.9388 time: 0.10s
Test loss: 0.2945 score: 0.8776 time: 0.09s
     INFO: Early stopping counter 3 of 20
Epoch 187/1000, LR 0.000249
Train loss: 0.0596;  Loss pred: 0.0596; Loss self: 0.0000; time: 0.14s
Val loss: 0.1208 score: 0.9388 time: 0.10s
Test loss: 0.2952 score: 0.8776 time: 0.09s
     INFO: Early stopping counter 4 of 20
Epoch 188/1000, LR 0.000249
Train loss: 0.0467;  Loss pred: 0.0467; Loss self: 0.0000; time: 0.15s
Val loss: 0.1214 score: 0.9388 time: 0.10s
Test loss: 0.2962 score: 0.8776 time: 0.09s
     INFO: Early stopping counter 5 of 20
Epoch 189/1000, LR 0.000249
Train loss: 0.0366;  Loss pred: 0.0366; Loss self: 0.0000; time: 0.15s
Val loss: 0.1220 score: 0.9388 time: 0.10s
Test loss: 0.2967 score: 0.8776 time: 0.21s
     INFO: Early stopping counter 6 of 20
Epoch 190/1000, LR 0.000249
Train loss: 0.0353;  Loss pred: 0.0353; Loss self: 0.0000; time: 0.14s
Val loss: 0.1226 score: 0.9388 time: 0.10s
Test loss: 0.2975 score: 0.8776 time: 0.09s
     INFO: Early stopping counter 7 of 20
Epoch 191/1000, LR 0.000249
Train loss: 0.0305;  Loss pred: 0.0305; Loss self: 0.0000; time: 0.14s
Val loss: 0.1236 score: 0.9388 time: 0.10s
Test loss: 0.2984 score: 0.8776 time: 0.08s
     INFO: Early stopping counter 8 of 20
Epoch 192/1000, LR 0.000248
Train loss: 0.0330;  Loss pred: 0.0330; Loss self: 0.0000; time: 0.13s
Val loss: 0.1247 score: 0.9388 time: 0.09s
Test loss: 0.2988 score: 0.8776 time: 0.08s
     INFO: Early stopping counter 9 of 20
Epoch 193/1000, LR 0.000248
Train loss: 0.0386;  Loss pred: 0.0386; Loss self: 0.0000; time: 0.13s
Val loss: 0.1247 score: 0.9388 time: 0.09s
Test loss: 0.2989 score: 0.8776 time: 0.09s
     INFO: Early stopping counter 10 of 20
Epoch 194/1000, LR 0.000248
Train loss: 0.0365;  Loss pred: 0.0365; Loss self: 0.0000; time: 0.14s
Val loss: 0.1249 score: 0.9388 time: 0.10s
Test loss: 0.2994 score: 0.8776 time: 0.09s
     INFO: Early stopping counter 11 of 20
Epoch 195/1000, LR 0.000248
Train loss: 0.0292;  Loss pred: 0.0292; Loss self: 0.0000; time: 0.14s
Val loss: 0.1248 score: 0.9388 time: 0.10s
Test loss: 0.2999 score: 0.8776 time: 0.09s
     INFO: Early stopping counter 12 of 20
Epoch 196/1000, LR 0.000247
Train loss: 0.0351;  Loss pred: 0.0351; Loss self: 0.0000; time: 0.14s
Val loss: 0.1240 score: 0.9388 time: 0.10s
Test loss: 0.3007 score: 0.8776 time: 0.09s
     INFO: Early stopping counter 13 of 20
Epoch 197/1000, LR 0.000247
Train loss: 0.0287;  Loss pred: 0.0287; Loss self: 0.0000; time: 0.14s
Val loss: 0.1236 score: 0.9388 time: 0.21s
Test loss: 0.3011 score: 0.8776 time: 0.08s
     INFO: Early stopping counter 14 of 20
Epoch 198/1000, LR 0.000247
Train loss: 0.0298;  Loss pred: 0.0298; Loss self: 0.0000; time: 0.13s
Val loss: 0.1229 score: 0.9388 time: 0.09s
Test loss: 0.3017 score: 0.8776 time: 0.08s
     INFO: Early stopping counter 15 of 20
Epoch 199/1000, LR 0.000247
Train loss: 0.0308;  Loss pred: 0.0308; Loss self: 0.0000; time: 0.14s
Val loss: 0.1224 score: 0.9388 time: 0.10s
Test loss: 0.3016 score: 0.8776 time: 0.09s
     INFO: Early stopping counter 16 of 20
Epoch 200/1000, LR 0.000246
Train loss: 0.0285;  Loss pred: 0.0285; Loss self: 0.0000; time: 0.14s
Val loss: 0.1217 score: 0.9388 time: 0.10s
Test loss: 0.3014 score: 0.8776 time: 0.08s
     INFO: Early stopping counter 17 of 20
Epoch 201/1000, LR 0.000246
Train loss: 0.0326;  Loss pred: 0.0326; Loss self: 0.0000; time: 0.14s
Val loss: 0.1215 score: 0.9388 time: 0.10s
Test loss: 0.3010 score: 0.8776 time: 0.09s
     INFO: Early stopping counter 18 of 20
Epoch 202/1000, LR 0.000246
Train loss: 0.0273;  Loss pred: 0.0273; Loss self: 0.0000; time: 0.14s
Val loss: 0.1210 score: 0.9388 time: 0.10s
Test loss: 0.3011 score: 0.8776 time: 0.09s
     INFO: Early stopping counter 19 of 20
Epoch 203/1000, LR 0.000246
Train loss: 0.0304;  Loss pred: 0.0304; Loss self: 0.0000; time: 0.14s
Val loss: 0.1203 score: 0.9388 time: 0.10s
Test loss: 0.3013 score: 0.8776 time: 0.08s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 182,   Train_Loss: 0.0351,   Val_Loss: 0.1182,   Val_Precision: 0.8929,   Val_Recall: 1.0000,   Val_accuracy: 0.9434,   Val_Score: 0.9388,   Val_Loss: 0.1182,   Test_Precision: 0.9545,   Test_Recall: 0.8750,   Test_accuracy: 0.9130,   Test_Score: 0.9184,   Test_loss: 0.2952


[0.09637932595796883, 0.09857086394913495, 0.09941427898593247, 0.09927489422261715, 0.09862565412186086, 0.0967537818942219, 0.09689547005109489, 0.09989027911797166, 0.10120426607318223, 0.10460687195882201, 0.10105438716709614, 0.0967152260709554, 0.09538910794071853, 0.09869300690479577, 0.10012829001061618, 0.09686358296312392, 0.09705522819422185, 0.09733351110480726, 0.1009229770861566, 0.10499870195053518, 0.10071813501417637, 0.09491636394523084, 0.09443707391619682, 0.09807165013626218, 0.09758537588641047, 0.09553289809264243, 0.09478604281321168, 0.09740679105743766, 0.09965529991313815, 0.10549134900793433, 0.10052248905412853, 0.0995034200605005, 0.09661654802039266, 0.09733573393896222, 0.0985178779810667, 0.09314462705515325, 0.10024720896035433, 0.09171853610314429, 0.09426239598542452, 0.09440813399851322, 0.0929876349400729, 0.09304485702887177, 3.144433974986896, 0.0975227509625256, 0.101262500975281, 0.09925344306975603, 0.09908093302510679, 0.10118427779525518, 0.09846929018385708, 0.10028810799121857, 0.2173422120977193, 0.09995974204503, 0.09811485907994211, 0.09930934896692634, 0.10103259608149529, 0.09993958286941051, 0.10027994215488434, 0.09842154406942427, 0.09930139407515526, 0.10037223901599646, 0.0992641078773886, 0.10067941108718514, 0.10002194810658693, 0.09754292899742723, 0.09815662796609104, 0.09776058490388095, 0.09723178786225617, 0.0989613609854132, 0.09764794097281992, 0.09837785409763455, 0.1015543881803751, 0.09843742311932147, 0.10050052497535944, 0.09718872793018818, 0.09752774704247713, 0.099183278856799, 0.09693550900556147, 0.09635552205145359, 0.09801227389834821, 0.10050106002017856, 0.09697237005457282, 0.1001443020068109, 0.09738525119610131, 0.09953965595923364, 0.09837057907134295, 0.09957529092207551, 0.09759931801818311, 0.09795144898816943, 0.10047362092882395, 0.09802661300636828, 0.09780577709898353, 0.09723115316592157, 0.09962794510647655, 0.0968415210954845, 0.09877185500226915, 0.09773123217746615, 0.09830323304049671, 0.10006915288977325, 0.09808423393405974, 0.09864312992431223, 0.09726201207377017, 0.09738949197344482, 0.0978548969142139, 0.09764305199496448, 0.09836454782634974, 0.09707906399853528, 0.0967504361178726, 0.23881512600928545, 0.09737191395834088, 0.09449485107325017, 0.09586431505158544, 0.09470868902280927, 0.09525814815424383, 0.09658049698919058, 0.09928513201884925, 0.10060313809663057, 0.09808940114453435, 0.09959369897842407, 0.09958888590335846, 0.09898317907936871, 0.09929890814237297, 0.1003211138304323, 0.0962218539789319, 0.09890127298422158, 0.10373995592817664, 0.09706783597357571, 0.09925592597573996, 0.09694811701774597, 0.09368218085728586, 0.09546093386597931, 0.09614448598586023, 0.09031840204261243, 0.09106953605078161, 0.09278617613017559, 0.0907775058876723, 0.0931558480951935, 0.09162443899549544, 0.09043845906853676, 0.08968549617566168, 0.09025308606214821, 0.09078817814588547, 0.09133483399637043, 0.08554390002973378, 0.09328552288934588, 0.08597849286161363, 0.08534336788579822, 0.08849439304322004, 0.08682167599909008, 0.0912603831384331, 0.08586760982871056, 0.22251093084923923, 0.09077621693722904, 0.09456114005297422, 0.09309211815707386, 0.09203563397750258, 0.0942818729672581, 0.09102061297744513, 0.19653489999473095, 0.09138070791959763, 0.09339908510446548, 0.09098753496073186, 0.09515938512049615, 0.09360193996690214, 0.09140476188622415, 0.09344365890137851, 0.09116338798776269, 0.09167036996223032, 0.15032579004764557, 0.09271308081224561, 0.0947604060638696, 0.10276887402869761, 0.215761257102713, 0.0913330230396241, 0.09546408290043473, 0.0912373240571469, 0.09248217102140188, 0.09336548508144915, 0.0919252261519432, 0.09145255293697119, 0.09119004802778363, 0.09490605304017663, 0.09010573383420706, 0.08861321792937815, 0.08948182314634323, 0.09151944192126393, 0.09004380903206766, 0.19651903002522886, 0.09005660400725901, 0.08877517795190215, 0.08931337599642575, 0.08821836393326521, 0.09133647009730339, 0.09084783005528152, 0.09333119401708245, 0.09071959904395044, 0.09252752899192274, 0.09326244308613241, 0.09380639297887683, 0.0935678188689053, 0.09370002802461386, 0.09219892020337284, 0.09269610187038779, 0.09221454197540879, 0.091869099996984, 0.0943295331671834, 0.08887872099876404, 0.09064353001303971, 0.09272864414379, 0.0894936139229685, 0.091329675167799, 0.0906156999990344, 0.09347165189683437, 0.09101227694191039, 0.09237611386924982, 0.0917423041537404, 0.09288345510140061, 0.09152573999017477, 0.09308996610343456, 0.09299904992803931, 0.09202656196430326, 0.09227921580895782, 0.09177153301425278, 0.10418412205763161, 0.09423888707533479, 0.095542793860659, 0.09130400698632002, 0.09474093606695533, 0.0911473329178989, 0.08740485901944339, 0.09028918389230967, 0.10817421390675008, 0.08642604202032089, 0.0886388081125915, 0.08592285797931254, 0.0870313469786197, 0.08598827803507447, 0.08881259802728891, 0.08340556896291673, 0.0888923560269177, 0.0934586119838059, 0.09069116204045713, 0.0891385751310736, 0.09173622401431203, 0.09529589209705591, 0.09333174396306276, 0.09213624382391572, 0.09240008099004626, 0.09122881502844393, 0.09085002797655761, 0.09252930898219347, 0.09085174417123199, 0.0926178339868784, 0.09283793205395341, 0.09156342200003564, 0.09150463598780334, 0.09436340909451246, 0.09499768586829305, 0.09314802195876837, 0.09941248898394406, 0.09599158610217273, 0.16705798893235624, 0.08928273292258382, 0.0891395250800997, 0.09246858395636082, 0.09268200700171292, 0.08898881310597062, 0.09259634604677558, 0.22814731416292489, 0.09063647105358541, 0.0936582200229168, 0.09117785701528192, 0.0912406521383673, 0.0946977089624852, 0.0922514540143311, 0.09354820288717747, 0.09112038603052497, 0.08848264999687672, 0.08866947307251394, 0.0876795700751245, 0.08802523906342685, 0.09759022598154843, 0.09163334802724421, 2.823735178913921, 0.09026288101449609, 0.0910231729503721, 0.09058812912553549, 0.09136734809726477, 0.09393659490160644, 0.08749623992480338, 0.08774462807923555, 0.08735522790811956, 0.0877988520078361, 0.08751244889572263, 0.09053518692962825, 0.08958805399015546, 0.09275519009679556, 0.09442283515818417, 0.22041016002185643, 0.08492342312820256, 0.08549498906359076, 0.08497774298302829, 0.08510030107572675, 0.084692268865183, 0.08647373598068953, 0.08533244300633669, 0.08190472004935145, 0.0883707741741091, 0.08767615305259824, 0.09155544196255505, 0.08900332916527987, 0.08854503417387605, 0.08928696485236287, 0.22126020397990942, 0.0912412831094116, 0.09013250120915473, 0.0912033929489553, 0.09105680393986404, 0.09071812010370195, 0.09161692089401186, 0.21194814098998904, 0.09052040800452232, 0.08769756113179028, 0.08368668099865317, 0.09939935593865812, 0.09414643491618335, 0.09000412910245359, 0.09026786801405251, 0.08255196083337069, 0.08460330287925899, 0.09804261196404696, 0.08980515599250793, 0.09037171700038016, 0.0901213800534606, 0.08978246408514678]
[0.0019669250195503843, 0.0020116502846762234, 0.0020288628364476015, 0.002026018249441166, 0.002012768451466548, 0.0019745669774331003, 0.0019774585724713243, 0.0020385771248565645, 0.002065393185166984, 0.0021348341216086125, 0.0020623344319815537, 0.001973780123897049, 0.0019467164885860924, 0.0020141429980570563, 0.002043434490012575, 0.0019768078155739575, 0.0019807189427392216, 0.001986398185812393, 0.002059652593595033, 0.002142830652051738, 0.0020554721431464565, 0.0019370686519434865, 0.001927287222779527, 0.00200146224767882, 0.0019915382833961322, 0.0019496509814824984, 0.00193440903700432, 0.001987893695049748, 0.0020337816308803706, 0.002152884673631313, 0.0020514793684516027, 0.002030682042051031, 0.0019717662861304624, 0.0019864435497747393, 0.0020105689383891163, 0.0019009107562276174, 0.00204586140735417, 0.0018718068592478425, 0.0019237223670494799, 0.0019266966122145556, 0.0018977068355116918, 0.0018988746332422811, 0.06417212193850808, 0.0019902602237250122, 0.0020665816525567552, 0.0020255804708113477, 0.0020220598576552408, 0.002064985261127657, 0.002009577350690961, 0.00204669608145344, 0.004435555348933047, 0.002039994735612857, 0.0020023440628559614, 0.0020267214074882927, 0.0020618897159488834, 0.0020395833238655205, 0.0020465294317323335, 0.002008602940192332, 0.0020265590627582706, 0.0020484130411427847, 0.002025798119946706, 0.0020546818589221457, 0.0020412642470732027, 0.0019906720203556575, 0.0020031964891038985, 0.0019951139776302235, 0.0019843222012705343, 0.002019619611947208, 0.0019928151218942844, 0.0020077113081149907, 0.002072538534293369, 0.002008927002435132, 0.0020510311219461112, 0.0019834434271466975, 0.0019903621845403494, 0.002024148548097939, 0.00197827569399105, 0.0019664392255398693, 0.002000250487721392, 0.0020510420412281336, 0.0019790279602974045, 0.0020437612654451206, 0.0019874541060428837, 0.0020314215501884415, 0.0020075628381906723, 0.0020321487943280717, 0.0019918228166976143, 0.001999009163023866, 0.002050482059771917, 0.0020005431225789444, 0.0019960362673261942, 0.001984309248284114, 0.0020332233695199296, 0.0019763575733772348, 0.002015752142903452, 0.0019945149423972685, 0.002006188429397892, 0.0020422276099953725, 0.0020017190598787702, 0.002013125100496168, 0.001984939021913677, 0.001987540652519282, 0.0019970387125349777, 0.00199271534683601, 0.002007439751558158, 0.0019812053877252098, 0.0019744986962831144, 0.004873778081822152, 0.001987181917517161, 0.001928466348433677, 0.001956414592889499, 0.0019328303882205973, 0.0019440438398825271, 0.0019710305507998076, 0.002026227184058148, 0.002053125267278175, 0.0020018245131537622, 0.00203252446894743, 0.002032426242925683, 0.0020200648791707902, 0.002026508329436183, 0.0020473696700088226, 0.0019637113056924877, 0.002018393326208604, 0.0021171419577178906, 0.001980976244358688, 0.00202563114236204, 0.0019785330003621628, 0.0019118812419854257, 0.0019481823237954962, 0.001962132367058372, 0.0018432326947471925, 0.0018585619602200327, 0.0018935954312280733, 0.0018526021609729041, 0.0019011397570447655, 0.0018698865101121518, 0.0018456828381334031, 0.0018303162484828914, 0.0018418997155540452, 0.0018528199621609279, 0.0018639762040075598, 0.001745793878157832, 0.0019037861814152222, 0.001754663119624768, 0.0017417013854244534, 0.001806008021290205, 0.0017718709387569403, 0.0018624567987435327, 0.0017524002005859297, 0.004541039405086515, 0.0018525758558618171, 0.001929819184754576, 0.0018998391460627317, 0.0018782782444388283, 0.001924119856474655, 0.0018575635301519412, 0.00401091632642308, 0.0018649124065224006, 0.0019061037776421528, 0.0018568884685863645, 0.0019420282677652277, 0.001910243672793921, 0.0018654033038004929, 0.0019070134469669085, 0.001860477305872708, 0.0018708238767802107, 0.003067873266278481, 0.0018921036900458287, 0.001933885838038155, 0.002097323959769339, 0.004403290961279857, 0.0018639392457066141, 0.0019482465898047906, 0.001861986205247896, 0.0018873912453347323, 0.0019054180628867174, 0.001876025023509045, 0.001866378631366759, 0.001861021388322115, 0.001936858225309727, 0.0018388925272287155, 0.001808433018966901, 0.001826159656047821, 0.0018677437126788559, 0.0018376287557564828, 0.004010592449494466, 0.0018378898776991635, 0.0018117383255490235, 0.0018227219591107294, 0.0018003747741482696, 0.001864009593822518, 0.0018540373480669698, 0.0019047182452465808, 0.0018514203886520497, 0.0018883169182025048, 0.0019033151650231105, 0.0019144161832423843, 0.0019095473238552104, 0.0019122454698900788, 0.001881610616395364, 0.0018917571810283223, 0.0018819294280695673, 0.0018748795917751838, 0.0019250925136159878, 0.001813851448954368, 0.00184986795944979, 0.001892421309056939, 0.0018264002841422145, 0.0018638709217918162, 0.0018492999999802939, 0.0019075847325884566, 0.001857393406977763, 0.0018852268136581596, 0.0018722919215049063, 0.0018955807163551146, 0.0018678722446974442, 0.0018997952266007053, 0.001897939794449782, 0.0018780931013123114, 0.001883249302223629, 0.0018728884288623016, 0.002126206572604727, 0.0019232425933741794, 0.0019498529359318164, 0.0018633470813534698, 0.0019334884911623535, 0.0018601496513856917, 0.0017837726330498652, 0.0018426364059655033, 0.002207637018505104, 0.001763796775924916, 0.0018089552676039083, 0.0017535277138635212, 0.0017761499383391775, 0.0017548628170423362, 0.0018125020005569166, 0.0017021544686309537, 0.0018141297148350551, 0.001907318611914406, 0.0018508400416419823, 0.001819154594511706, 0.0018721678370267761, 0.0019448141244297124, 0.001904729468633934, 0.0018803315066105249, 0.0018857159385723726, 0.0018618125516008965, 0.0018540822036032165, 0.0018883532445345605, 0.0018541172279843263, 0.0018901598772832326, 0.0018946516745704778, 0.00186864126530685, 0.0018674415507714968, 0.0019257838590716829, 0.0019387282830263888, 0.0019009800399748646, 0.0020288263057947767, 0.001959011961268831, 0.0034093467129052294, 0.0018220965902568127, 0.0018191739812265246, 0.0018871139582930779, 0.0018914695306472024, 0.0018160982266524617, 0.0018897213478933793, 0.004656067635978059, 0.0018497238990527634, 0.0019113922453656488, 0.0018607725921486104, 0.001862054125272802, 0.0019326063053568406, 0.001882682734986349, 0.0019091469976974993, 0.0018595997149086728, 0.0018057683672831983, 0.0018095810831125294, 0.0017893789811249897, 0.0017964334502740173, 0.0019916372649295597, 0.0018700683270866166, 0.057627248549263696, 0.0018420996125407365, 0.0018576157744973898, 0.001848737329092561, 0.001864639757087036, 0.001917073365338907, 0.0017856375494857831, 0.001790706695494603, 0.0017827597532269297, 0.0017918133062823694, 0.0017859683448106659, 0.0018476568761148623, 0.0018283276324521523, 0.0018929630631999094, 0.0019269966358813097, 0.004498166531058294, 0.0017331310842490317, 0.0017447956951753218, 0.0017342396527148631, 0.0017367408382801377, 0.001728413650309857, 0.0017647701220548883, 0.0017414784287007488, 0.0016715248989663562, 0.0018034851872267164, 0.0017893092459713926, 0.0018684784073990826, 0.0018163944727608136, 0.0018070415137525723, 0.0018221829561706707, 0.004515514366936927, 0.0018620670022328897, 0.0018394388001868312, 0.001861293733652149, 0.001858302121221715, 0.001851390206197999, 0.0018697330794696296, 0.004325472265101818, 0.0018473552653984148, 0.0017897461455467405, 0.0017078914489521055, 0.0020285582844624104, 0.0019213558146159869, 0.001836818961274563, 0.001842201388041888, 0.0016847338945585855, 0.001726598017944061, 0.0020008696319193256, 0.0018327582855613865, 0.0018443207551097991, 0.0018392118378257264, 0.001832295185411159]
[508.4077888381267, 497.1042967147497, 492.8869423972154, 493.5789696246954, 496.82813702260563, 506.4401519061061, 505.6995953903865, 490.5382228648134, 484.169313224083, 468.42046877463855, 484.88740938062574, 506.6420458351719, 513.6854831523535, 496.4890779674781, 489.3721843727156, 505.866069590409, 504.86718656663976, 503.4237380714391, 485.51877297643875, 466.67243584671996, 486.5062284275132, 516.2439642997097, 518.8640220204456, 499.63470515606383, 502.12441725936554, 512.9123158441457, 516.9537470465026, 503.0450081360988, 491.6948726531306, 464.4930646996898, 487.45311085179037, 492.4453849948756, 507.1594980774688, 503.4122414973227, 497.3716548119001, 526.0636233047118, 488.79166321107726, 534.2431539127039, 519.8255305071675, 519.0230748631434, 526.9517826921686, 526.6277101677455, 15.58309075330615, 502.44686000325083, 483.8908730089661, 493.68564439182677, 494.54520162404566, 484.2649576365089, 497.61707338917114, 488.59232646297943, 225.4509123058391, 490.1973434257805, 499.4146703108011, 493.4077255538026, 484.99199169815887, 490.2962229092704, 488.63211273415516, 497.8584766505648, 493.44725173661556, 488.1827931744235, 493.6326034433814, 486.69335140992797, 489.89247787679426, 502.3429222767384, 499.20215287884, 501.22449705243906, 503.95041660054693, 495.14274573510113, 501.80269560050465, 498.0795774562253, 482.5000758506762, 497.7781665475373, 487.5596422209112, 504.17369424978256, 502.4211210237287, 494.03488738002187, 505.49071751600053, 508.5333871558926, 499.93738591168216, 487.5570465641039, 505.29857084471, 489.29393902678027, 503.1562726200746, 492.2661177377175, 498.1164130838643, 492.0899506921437, 502.0526884303753, 500.2478320246004, 487.6901971584349, 499.86425621802033, 500.9929009654507, 503.9537062404599, 491.8298771256564, 505.9813130329358, 496.0927381476666, 501.3750354750762, 498.45766496625913, 489.66138500216726, 499.57060410893166, 496.7401180152855, 503.7938137947943, 503.1343629286086, 500.74141964460557, 501.82782081132575, 498.1469552069039, 504.7432266213373, 506.4576653722007, 205.17963337923084, 503.22519100285854, 518.546772056568, 511.1391029459989, 517.3759715774235, 514.3916919386072, 507.3488077565406, 493.5280741803049, 487.062341464288, 499.544287438341, 491.99899695075425, 492.02277498665705, 495.03360526246405, 493.45960511211956, 488.43157864876, 509.2398241539673, 495.443572377651, 472.33488352284127, 504.80161124987916, 493.67329475095056, 505.4249789197117, 523.0450396393518, 513.2989801754159, 509.64961222223735, 542.5250988927116, 538.0503967065006, 528.0959087187169, 539.7812984709273, 526.0002565800077, 534.7918146861341, 541.8048969948333, 546.3536702080189, 542.9177232372823, 539.7178465379382, 536.4875355436374, 572.8053079526223, 525.2690715806266, 569.9099666572171, 574.1512341716945, 553.7073967620609, 564.3751912887922, 536.9252058220244, 570.6459059212854, 220.21390056203404, 539.7889629382008, 518.1832618827316, 526.3603511236316, 532.4024824121674, 519.7181436670924, 538.3395958027901, 249.31958650251775, 536.218214057974, 524.6304066597037, 538.535306194934, 514.9255634423599, 523.4934235051807, 536.0771035210685, 524.380151377796, 537.4964783732863, 534.5238600017518, 325.95870598431253, 528.5122613844587, 517.0936051811918, 476.79806228408256, 227.10286664984338, 536.4981730511837, 513.2820481929845, 537.0609068861843, 529.8318525487534, 524.819208696382, 533.0419303946873, 535.7969616635048, 537.3393375675245, 516.3000507381421, 543.8055705773301, 552.9649091295996, 547.5972468717233, 535.4053627441884, 544.1795557821131, 249.33972040117155, 544.1022403648527, 551.956088745301, 548.6300282945406, 555.439908600744, 536.47792549678, 539.3634605271603, 525.0120339297439, 540.1258439894694, 529.5721233869489, 525.3990607424482, 522.3524585476146, 523.6843242937215, 522.9454145641055, 531.4595864237407, 528.6090678172658, 531.3695535468476, 533.3675849835106, 519.4555549549435, 551.3130640199176, 540.5791234404817, 528.4235572777056, 547.5251009773353, 536.5178394642579, 540.7451468180695, 524.2231094201887, 538.3889036341196, 530.440153277666, 534.1047453733723, 527.5428217706461, 535.368520432177, 526.3725195211146, 526.8871030178819, 532.4549668497549, 530.9971435110898, 533.9346351813688, 470.32118745402136, 519.9552066105076, 512.8591913636345, 536.6686700545532, 517.1998719262253, 537.5911552358513, 560.6095650712033, 542.7006634420753, 452.97301667696604, 566.9587413071501, 552.8052671664856, 570.2789822447203, 563.015530622977, 569.8451128421596, 551.7235289631327, 587.4907468323378, 551.2284991654648, 524.2962522115191, 540.2952051506542, 549.7058925156485, 534.1401450353502, 514.1879562877172, 525.008940360017, 531.8211158428093, 530.3025654845312, 537.110999246482, 539.3504117868149, 529.5619359853823, 539.3402234265058, 529.0557756613274, 527.8015022084218, 535.1481948761256, 535.4919941600688, 519.2690733642593, 515.8020382510656, 526.044450215912, 492.8958172238692, 510.461405938691, 293.3113244876944, 548.8183257392828, 549.7000343671249, 529.9097045016373, 528.6894574811527, 550.6310095590251, 529.1785485276856, 214.77351236757514, 540.6212248823168, 523.1788516588338, 537.411182978202, 537.0413171279293, 517.435960561744, 531.1569397311389, 523.794134870723, 537.7501362163368, 553.7808824863362, 552.6140880517895, 558.8531052104435, 556.658527955748, 502.09946239149525, 534.7398196716706, 17.35290205891284, 542.8588080645318, 538.3244553199207, 540.9097248503348, 536.2966204057628, 521.6284457758436, 560.0240655154087, 558.4387451702661, 560.9280769267561, 558.0938574871881, 559.9203384010773, 541.2260322396752, 546.947922380192, 528.272325773529, 518.9422655855604, 222.3128008034704, 576.9904014117325, 573.1330050648236, 576.6215750138981, 575.7911473943813, 578.5652062055443, 566.6460393354834, 574.2247411850299, 598.2561196775374, 554.481959199086, 558.8748855188043, 535.1948387736509, 550.5412040150388, 553.3907175842138, 548.7923134247213, 221.45871294798786, 537.037603266077, 543.6440722564025, 537.2607138357705, 538.1256301545649, 540.1346494392409, 534.8356998014182, 231.1886283650604, 541.3143961696682, 558.7384571204176, 585.5173059233714, 492.9609406145363, 520.4658046119717, 544.4194670693638, 542.828816920456, 593.5655495682946, 579.17360590437, 499.7826865115416, 545.625687728752, 542.205035230147, 543.7111590050316, 545.7635909115836]
Elapsed: 0.1153274600820694~0.22409293428134702
Time per graph: 0.0023536216343279467~0.004573325189415245
Speed: 509.64241598839277~70.06671441738709
Total Time: 0.0908
best val loss: 0.11821267753839493 test_score: 0.9184

Testing...
Test loss: 0.2956 score: 0.9184 time: 0.09s
test Score 0.9184
Epoch Time List: [0.32390359113924205, 0.34299500612542033, 0.3324917003046721, 0.47627019113861024, 0.33961626910604537, 0.32951786811463535, 0.332793835317716, 0.3307346219662577, 0.3515393079724163, 0.3542224336415529, 0.47403525095432997, 0.33535956079140306, 0.32491709711030126, 0.3306581990327686, 0.3330622056964785, 0.3445071878377348, 0.3262014789506793, 0.41367054800502956, 0.34783385088667274, 0.3476776168681681, 0.3543270919471979, 0.3352437671273947, 0.33578795101493597, 0.33312563481740654, 0.3307323819026351, 0.31603187089785933, 0.4353439339902252, 0.32765928702428937, 0.3281651302240789, 0.3621470171492547, 0.34862269484438, 0.345520957140252, 0.32661682390607893, 0.33125763898715377, 0.32095526810735464, 0.39740577386692166, 0.3211774800438434, 0.3170023839920759, 0.3147912989370525, 0.3122640650253743, 0.31655616289936006, 0.3198075348045677, 4.299296143930405, 2.562304158229381, 0.3451368832029402, 0.342805314110592, 0.34035500790923834, 0.34253094298765063, 0.34241154696792364, 0.34204104682430625, 0.4621546249836683, 0.3408446619287133, 0.34077978204004467, 0.34011667408049107, 0.34091463428922, 0.3444245611317456, 0.34125460172072053, 0.3382228419650346, 0.3389827539213002, 0.33677664096467197, 0.3393753420095891, 0.34569947188720107, 0.3391128007788211, 0.3400337037164718, 0.33100640890188515, 0.33776143682189286, 0.33906302601099014, 0.3312275449279696, 0.3371106411796063, 0.33830376411788166, 0.3408236540853977, 0.3391530029475689, 0.33985422900877893, 0.3350246048066765, 0.3375292031560093, 0.3348221352789551, 0.33622678532265127, 0.33007902605459094, 0.42743106791749597, 0.33788915583863854, 0.3360626841895282, 0.33452356280758977, 0.34233399108052254, 0.339049561182037, 0.3384221070446074, 0.46340674813836813, 0.3317028949968517, 0.33381571085192263, 0.33895630622282624, 0.3321765176951885, 0.3312197991181165, 0.3331626539584249, 0.41060228389687836, 0.32927021803334355, 0.33340739901177585, 0.32962098391726613, 0.33695521438494325, 0.3317767612170428, 0.3351614191196859, 0.3354660039767623, 0.36229502432979643, 0.33051829994656146, 0.3312666448764503, 0.3242580017540604, 0.33110974286682904, 0.33598613389767706, 0.3278207378461957, 0.47629070398397744, 0.3213519302662462, 0.3264328632503748, 0.3205264189746231, 0.31825281376950443, 0.3243023990653455, 0.3169220348354429, 0.3155698608607054, 0.38534224196337163, 0.3216874930076301, 0.33429869799874723, 0.33669880987145007, 0.33639391232281923, 0.33511894778348505, 0.34101971006020904, 0.43392918980680406, 0.3271343286614865, 0.33932114695198834, 0.3240863881073892, 0.3328840637113899, 0.33179003768600523, 0.3152088162023574, 0.31965532386675477, 0.35494034737348557, 0.3235209381673485, 0.32592411315999925, 0.33072059182450175, 0.33158974978141487, 0.3365276982076466, 0.4527761209756136, 0.3301186393946409, 0.3153293877840042, 0.32092120475135744, 0.3265782648231834, 0.33308053598739207, 0.3280218082945794, 0.4270755061879754, 0.32772422512061894, 0.3100573876872659, 0.32198687084019184, 0.3283818762283772, 0.32760624098591506, 0.31988131790421903, 0.45228045457042754, 0.33234027586877346, 0.33217472466640174, 0.3358404468744993, 0.33449235395528376, 0.34820872405543923, 0.33711652318015695, 0.4387564272619784, 0.33318432816304266, 0.34275889699347317, 0.32642911188304424, 0.3371368108782917, 0.3432505517266691, 0.3378248999360949, 0.32894463813863695, 0.372130777919665, 0.32948352792300284, 0.39056376786902547, 0.4259241339750588, 0.33774389210157096, 0.3481905967928469, 0.4733170629478991, 0.3362041232176125, 0.34047344396822155, 0.3363162553869188, 0.33636613795533776, 0.3362997309304774, 0.33463832968845963, 0.32749411021359265, 0.3630159520544112, 0.33507793792523444, 0.3290679920464754, 0.3215894971508533, 0.324769068043679, 0.3274985780008137, 0.32231248589232564, 0.4332725459244102, 0.3247178830206394, 0.31874710810370743, 0.3387086240109056, 0.3220013629179448, 0.328527778852731, 0.33366598444990814, 0.3373488502111286, 0.4317739347461611, 0.3360484109725803, 0.3449869309552014, 0.3407816116232425, 0.3379635368473828, 0.3460064351093024, 0.34355516685172915, 0.3426875160075724, 0.3374571811873466, 0.33726550289429724, 0.3386878278106451, 0.3357119760476053, 0.31025358685292304, 0.317489149980247, 0.34948262805119157, 0.32392584602348506, 0.3242585251573473, 0.32923433906398714, 0.33250283799134195, 0.3332466690335423, 0.33122852188535035, 0.32865083892829716, 0.3303742047864944, 0.3356571369804442, 0.34962259232997894, 0.33030671812593937, 0.3394518350251019, 0.33267776970751584, 0.35069771902635694, 0.34331049118191004, 0.3448247108608484, 0.3409052740316838, 0.36504536611028016, 0.3361939820460975, 0.33910695603117347, 0.32238716376014054, 0.33542248723097146, 0.3197525821160525, 0.3174533969722688, 0.32508848886936903, 0.32324400427751243, 0.3263009008951485, 0.3253643657080829, 0.3256547567434609, 0.31273894384503365, 0.45504478365182877, 0.33205064688809216, 0.32723521697334945, 0.32623538724146783, 0.3316008849069476, 0.3360432437621057, 0.3324108808301389, 0.40630664490163326, 0.32922771107405424, 0.3294098796322942, 0.33000806742347777, 0.33003986719995737, 0.3268276851158589, 0.34469592035748065, 0.47503562015481293, 0.33171292184852064, 0.34257646184414625, 0.3391854509245604, 0.3414408278185874, 0.3433375449385494, 0.34504945506341755, 0.4256902749184519, 0.3198113536927849, 0.32352278498001397, 0.327418735018, 0.3403007409069687, 0.3320901831611991, 0.3353170801419765, 0.47000589105300605, 0.3262538639828563, 0.3362679881975055, 0.3361961890477687, 0.3403708338737488, 0.3355048808734864, 0.34593312605284154, 0.3394910569768399, 0.3658863438758999, 0.3224402950145304, 0.3189597772434354, 0.32162976008839905, 0.3180383830331266, 0.33158944197930396, 0.3279801169410348, 3.092644141986966, 3.7564801801927388, 0.32375277602113783, 0.32661993987858295, 0.32978156325407326, 0.3365049872081727, 0.34160424931906164, 0.3224554529879242, 0.36774940486066043, 0.32195021072402596, 0.32307476201094687, 0.32534219115041196, 0.32736565778031945, 0.33070900035090744, 0.34822386922314763, 0.4695049040019512, 0.31229787482880056, 0.3073857701383531, 0.3071608932223171, 0.30541261518374085, 0.3104230903554708, 0.332804667763412, 0.3106770529411733, 0.4406037519220263, 0.3246883961837739, 0.3240863550454378, 0.30763614387251437, 0.32959205796942115, 0.3237774313893169, 0.3236888849642128, 0.4572576731443405, 0.3419185511302203, 0.32696566730737686, 0.3287065520416945, 0.32581368717364967, 0.3291495409794152, 0.3310825009830296, 0.45754274795763195, 0.32586996094323695, 0.3220526701770723, 0.30428895307704806, 0.3200042729731649, 0.32890007086098194, 0.32636155979707837, 0.32695911219343543, 0.4316034591756761, 0.30272917239926755, 0.3329490809701383, 0.3263995472807437, 0.3211449671071023, 0.3257106109522283, 0.32481007277965546]
Total Epoch List: [131, 203]
Total Time List: [0.0967547248583287, 0.09079011902213097]
========================k_idx:2========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: SGFormer
SGFormer(
  (trans_conv): SGModule(
    (attns): ModuleList(
      (0-1): 2 x SGFormerAttention(heads=1, head_channels=64)
    )
    (fcs): ModuleList(
      (0): Linear(in_features=14887, out_features=64, bias=True)
    )
    (bns): ModuleList(
      (0-2): 3 x LayerNorm((64,), eps=1e-05, elementwise_affine=True)
    )
  )
  (graph_conv): GraphModule(
    (convs): ModuleList(
      (0-2): 3 x GCNConv(64, 64)
    )
    (fcs): ModuleList(
      (0): Linear(in_features=14887, out_features=64, bias=True)
    )
    (bns): ModuleList(
      (0-3): 4 x BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (classifier): Linear(in_features=64, out_features=2, bias=True)
)
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7e09d8838160>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.6982;  Loss pred: 0.6982; Loss self: 0.0000; time: 0.15s
Val loss: 0.6884 score: 0.6327 time: 0.09s
Test loss: 0.6850 score: 0.5833 time: 0.08s
Epoch 2/1000, LR 0.000000
Train loss: 0.6847;  Loss pred: 0.6847; Loss self: 0.0000; time: 0.15s
Val loss: 0.6877 score: 0.6327 time: 0.09s
Test loss: 0.6844 score: 0.5833 time: 0.09s
Epoch 3/1000, LR 0.000030
Train loss: 0.6836;  Loss pred: 0.6836; Loss self: 0.0000; time: 0.15s
Val loss: 0.6863 score: 0.6531 time: 0.09s
Test loss: 0.6835 score: 0.5833 time: 0.08s
Epoch 4/1000, LR 0.000060
Train loss: 0.6929;  Loss pred: 0.6929; Loss self: 0.0000; time: 0.15s
Val loss: 0.6841 score: 0.6531 time: 0.09s
Test loss: 0.6820 score: 0.6458 time: 0.09s
Epoch 5/1000, LR 0.000090
Train loss: 0.6823;  Loss pred: 0.6823; Loss self: 0.0000; time: 0.15s
Val loss: 0.6815 score: 0.6531 time: 0.09s
Test loss: 0.6802 score: 0.6875 time: 0.08s
Epoch 6/1000, LR 0.000120
Train loss: 0.6720;  Loss pred: 0.6720; Loss self: 0.0000; time: 0.15s
Val loss: 0.6782 score: 0.7143 time: 0.08s
Test loss: 0.6780 score: 0.7917 time: 0.08s
Epoch 7/1000, LR 0.000150
Train loss: 0.6679;  Loss pred: 0.6679; Loss self: 0.0000; time: 0.29s
Val loss: 0.6743 score: 0.7959 time: 0.09s
Test loss: 0.6753 score: 0.8125 time: 0.09s
Epoch 8/1000, LR 0.000180
Train loss: 0.6408;  Loss pred: 0.6408; Loss self: 0.0000; time: 0.16s
Val loss: 0.6698 score: 0.8571 time: 0.09s
Test loss: 0.6724 score: 0.8333 time: 0.09s
Epoch 9/1000, LR 0.000210
Train loss: 0.6156;  Loss pred: 0.6156; Loss self: 0.0000; time: 0.16s
Val loss: 0.6644 score: 0.9388 time: 0.24s
Test loss: 0.6688 score: 0.8333 time: 0.09s
Epoch 10/1000, LR 0.000240
Train loss: 0.6227;  Loss pred: 0.6227; Loss self: 0.0000; time: 0.16s
Val loss: 0.6581 score: 0.9592 time: 0.09s
Test loss: 0.6643 score: 0.8542 time: 0.09s
Epoch 11/1000, LR 0.000270
Train loss: 0.5814;  Loss pred: 0.5814; Loss self: 0.0000; time: 0.16s
Val loss: 0.6514 score: 0.9796 time: 0.09s
Test loss: 0.6593 score: 0.8750 time: 0.08s
Epoch 12/1000, LR 0.000270
Train loss: 0.5858;  Loss pred: 0.5858; Loss self: 0.0000; time: 0.15s
Val loss: 0.6451 score: 0.9592 time: 0.09s
Test loss: 0.6544 score: 0.8750 time: 0.08s
Epoch 13/1000, LR 0.000270
Train loss: 0.5609;  Loss pred: 0.5609; Loss self: 0.0000; time: 0.16s
Val loss: 0.6386 score: 0.9388 time: 0.09s
Test loss: 0.6496 score: 0.8542 time: 0.09s
Epoch 14/1000, LR 0.000270
Train loss: 0.5444;  Loss pred: 0.5444; Loss self: 0.0000; time: 0.16s
Val loss: 0.6317 score: 0.9388 time: 0.09s
Test loss: 0.6447 score: 0.8125 time: 0.08s
Epoch 15/1000, LR 0.000270
Train loss: 0.5324;  Loss pred: 0.5324; Loss self: 0.0000; time: 0.16s
Val loss: 0.6244 score: 0.9184 time: 0.09s
Test loss: 0.6395 score: 0.7917 time: 0.09s
Epoch 16/1000, LR 0.000270
Train loss: 0.5124;  Loss pred: 0.5124; Loss self: 0.0000; time: 0.16s
Val loss: 0.6174 score: 0.8980 time: 0.09s
Test loss: 0.6348 score: 0.7917 time: 0.08s
Epoch 17/1000, LR 0.000270
Train loss: 0.5083;  Loss pred: 0.5083; Loss self: 0.0000; time: 0.17s
Val loss: 0.6103 score: 0.8776 time: 0.09s
Test loss: 0.6302 score: 0.7708 time: 0.09s
Epoch 18/1000, LR 0.000270
Train loss: 0.4959;  Loss pred: 0.4959; Loss self: 0.0000; time: 0.16s
Val loss: 0.6035 score: 0.8163 time: 0.09s
Test loss: 0.6271 score: 0.7292 time: 0.08s
Epoch 19/1000, LR 0.000270
Train loss: 0.4983;  Loss pred: 0.4983; Loss self: 0.0000; time: 0.16s
Val loss: 0.5966 score: 0.7959 time: 0.09s
Test loss: 0.6244 score: 0.7083 time: 0.09s
Epoch 20/1000, LR 0.000270
Train loss: 0.4716;  Loss pred: 0.4716; Loss self: 0.0000; time: 0.16s
Val loss: 0.5895 score: 0.7755 time: 0.09s
Test loss: 0.6215 score: 0.6667 time: 0.09s
Epoch 21/1000, LR 0.000270
Train loss: 0.4497;  Loss pred: 0.4497; Loss self: 0.0000; time: 0.16s
Val loss: 0.5837 score: 0.7755 time: 0.09s
Test loss: 0.6204 score: 0.6042 time: 0.09s
Epoch 22/1000, LR 0.000270
Train loss: 0.4377;  Loss pred: 0.4377; Loss self: 0.0000; time: 0.16s
Val loss: 0.5788 score: 0.7347 time: 0.09s
Test loss: 0.6202 score: 0.5833 time: 0.09s
Epoch 23/1000, LR 0.000270
Train loss: 0.4292;  Loss pred: 0.4292; Loss self: 0.0000; time: 0.16s
Val loss: 0.5743 score: 0.7143 time: 0.09s
Test loss: 0.6202 score: 0.5833 time: 0.08s
Epoch 24/1000, LR 0.000270
Train loss: 0.4208;  Loss pred: 0.4208; Loss self: 0.0000; time: 0.16s
Val loss: 0.5702 score: 0.6939 time: 0.09s
Test loss: 0.6203 score: 0.5625 time: 0.09s
Epoch 25/1000, LR 0.000270
Train loss: 0.4196;  Loss pred: 0.4196; Loss self: 0.0000; time: 0.16s
Val loss: 0.5657 score: 0.6939 time: 0.09s
Test loss: 0.6199 score: 0.5417 time: 0.08s
Epoch 26/1000, LR 0.000270
Train loss: 0.4144;  Loss pred: 0.4144; Loss self: 0.0000; time: 0.16s
Val loss: 0.5609 score: 0.6939 time: 0.09s
Test loss: 0.6192 score: 0.5417 time: 0.09s
Epoch 27/1000, LR 0.000270
Train loss: 0.4081;  Loss pred: 0.4081; Loss self: 0.0000; time: 0.15s
Val loss: 0.5565 score: 0.6735 time: 0.09s
Test loss: 0.6189 score: 0.5417 time: 0.10s
Epoch 28/1000, LR 0.000270
Train loss: 0.3863;  Loss pred: 0.3863; Loss self: 0.0000; time: 0.15s
Val loss: 0.5530 score: 0.6735 time: 0.09s
Test loss: 0.6199 score: 0.5417 time: 0.08s
Epoch 29/1000, LR 0.000270
Train loss: 0.3571;  Loss pred: 0.3571; Loss self: 0.0000; time: 0.16s
Val loss: 0.5495 score: 0.6735 time: 0.09s
Test loss: 0.6207 score: 0.5417 time: 0.08s
Epoch 30/1000, LR 0.000270
Train loss: 0.3583;  Loss pred: 0.3583; Loss self: 0.0000; time: 0.16s
Val loss: 0.5452 score: 0.6735 time: 0.09s
Test loss: 0.6207 score: 0.5417 time: 0.08s
Epoch 31/1000, LR 0.000270
Train loss: 0.3460;  Loss pred: 0.3460; Loss self: 0.0000; time: 0.15s
Val loss: 0.5408 score: 0.6735 time: 0.09s
Test loss: 0.6204 score: 0.5625 time: 0.08s
Epoch 32/1000, LR 0.000270
Train loss: 0.3527;  Loss pred: 0.3527; Loss self: 0.0000; time: 0.15s
Val loss: 0.5359 score: 0.6939 time: 0.09s
Test loss: 0.6195 score: 0.5625 time: 0.08s
Epoch 33/1000, LR 0.000270
Train loss: 0.3220;  Loss pred: 0.3220; Loss self: 0.0000; time: 0.15s
Val loss: 0.5312 score: 0.6939 time: 0.09s
Test loss: 0.6184 score: 0.5625 time: 0.08s
Epoch 34/1000, LR 0.000270
Train loss: 0.3227;  Loss pred: 0.3227; Loss self: 0.0000; time: 0.16s
Val loss: 0.5263 score: 0.6939 time: 0.09s
Test loss: 0.6171 score: 0.5625 time: 0.08s
Epoch 35/1000, LR 0.000270
Train loss: 0.3111;  Loss pred: 0.3111; Loss self: 0.0000; time: 0.15s
Val loss: 0.5224 score: 0.6939 time: 0.09s
Test loss: 0.6170 score: 0.5625 time: 0.09s
Epoch 36/1000, LR 0.000270
Train loss: 0.3067;  Loss pred: 0.3067; Loss self: 0.0000; time: 0.16s
Val loss: 0.5192 score: 0.6939 time: 0.09s
Test loss: 0.6172 score: 0.5625 time: 0.08s
Epoch 37/1000, LR 0.000270
Train loss: 0.3051;  Loss pred: 0.3051; Loss self: 0.0000; time: 0.16s
Val loss: 0.5169 score: 0.6939 time: 0.09s
Test loss: 0.6185 score: 0.5625 time: 0.08s
Epoch 38/1000, LR 0.000270
Train loss: 0.2776;  Loss pred: 0.2776; Loss self: 0.0000; time: 0.15s
Val loss: 0.5158 score: 0.6939 time: 0.09s
Test loss: 0.6212 score: 0.5625 time: 0.09s
Epoch 39/1000, LR 0.000269
Train loss: 0.2795;  Loss pred: 0.2795; Loss self: 0.0000; time: 0.16s
Val loss: 0.5143 score: 0.6939 time: 0.09s
Test loss: 0.6228 score: 0.5625 time: 0.09s
Epoch 40/1000, LR 0.000269
Train loss: 0.2912;  Loss pred: 0.2912; Loss self: 0.0000; time: 0.16s
Val loss: 0.5114 score: 0.6939 time: 0.09s
Test loss: 0.6219 score: 0.5625 time: 0.09s
Epoch 41/1000, LR 0.000269
Train loss: 0.2728;  Loss pred: 0.2728; Loss self: 0.0000; time: 0.16s
Val loss: 0.5074 score: 0.6939 time: 0.09s
Test loss: 0.6190 score: 0.5625 time: 0.09s
Epoch 42/1000, LR 0.000269
Train loss: 0.2563;  Loss pred: 0.2563; Loss self: 0.0000; time: 0.16s
Val loss: 0.5048 score: 0.6939 time: 0.09s
Test loss: 0.6177 score: 0.5625 time: 0.09s
Epoch 43/1000, LR 0.000269
Train loss: 0.2629;  Loss pred: 0.2629; Loss self: 0.0000; time: 0.16s
Val loss: 0.5059 score: 0.6939 time: 0.09s
Test loss: 0.6214 score: 0.5625 time: 0.09s
     INFO: Early stopping counter 1 of 20
Epoch 44/1000, LR 0.000269
Train loss: 0.2539;  Loss pred: 0.2539; Loss self: 0.0000; time: 0.15s
Val loss: 0.5073 score: 0.6939 time: 0.09s
Test loss: 0.6256 score: 0.5625 time: 0.08s
     INFO: Early stopping counter 2 of 20
Epoch 45/1000, LR 0.000269
Train loss: 0.2545;  Loss pred: 0.2545; Loss self: 0.0000; time: 0.15s
Val loss: 0.5097 score: 0.6939 time: 0.09s
Test loss: 0.6309 score: 0.5625 time: 0.09s
     INFO: Early stopping counter 3 of 20
Epoch 46/1000, LR 0.000269
Train loss: 0.2502;  Loss pred: 0.2502; Loss self: 0.0000; time: 0.14s
Val loss: 0.5111 score: 0.6939 time: 0.09s
Test loss: 0.6349 score: 0.5625 time: 0.08s
     INFO: Early stopping counter 4 of 20
Epoch 47/1000, LR 0.000269
Train loss: 0.2280;  Loss pred: 0.2280; Loss self: 0.0000; time: 0.14s
Val loss: 0.5123 score: 0.6939 time: 0.09s
Test loss: 0.6381 score: 0.5625 time: 0.08s
     INFO: Early stopping counter 5 of 20
Epoch 48/1000, LR 0.000269
Train loss: 0.2387;  Loss pred: 0.2387; Loss self: 0.0000; time: 0.15s
Val loss: 0.5124 score: 0.6939 time: 0.09s
Test loss: 0.6398 score: 0.5625 time: 0.09s
     INFO: Early stopping counter 6 of 20
Epoch 49/1000, LR 0.000269
Train loss: 0.2278;  Loss pred: 0.2278; Loss self: 0.0000; time: 0.16s
Val loss: 0.5109 score: 0.6939 time: 0.09s
Test loss: 0.6398 score: 0.5625 time: 0.08s
     INFO: Early stopping counter 7 of 20
Epoch 50/1000, LR 0.000269
Train loss: 0.2150;  Loss pred: 0.2150; Loss self: 0.0000; time: 0.15s
Val loss: 0.5086 score: 0.6939 time: 0.09s
Test loss: 0.6389 score: 0.5625 time: 0.08s
     INFO: Early stopping counter 8 of 20
Epoch 51/1000, LR 0.000269
Train loss: 0.2182;  Loss pred: 0.2182; Loss self: 0.0000; time: 0.17s
Val loss: 0.5065 score: 0.6939 time: 0.15s
Test loss: 0.6381 score: 0.5625 time: 0.10s
     INFO: Early stopping counter 9 of 20
Epoch 52/1000, LR 0.000269
Train loss: 0.1963;  Loss pred: 0.1963; Loss self: 0.0000; time: 0.19s
Val loss: 0.5039 score: 0.6939 time: 0.11s
Test loss: 0.6367 score: 0.5625 time: 0.10s
Epoch 53/1000, LR 0.000269
Train loss: 0.2028;  Loss pred: 0.2028; Loss self: 0.0000; time: 0.17s
Val loss: 0.5015 score: 0.6939 time: 0.10s
Test loss: 0.6355 score: 0.5625 time: 0.10s
Epoch 54/1000, LR 0.000269
Train loss: 0.1953;  Loss pred: 0.1953; Loss self: 0.0000; time: 0.18s
Val loss: 0.4997 score: 0.6939 time: 0.10s
Test loss: 0.6350 score: 0.5625 time: 0.10s
Epoch 55/1000, LR 0.000269
Train loss: 0.1832;  Loss pred: 0.1832; Loss self: 0.0000; time: 0.18s
Val loss: 0.4974 score: 0.6939 time: 0.10s
Test loss: 0.6341 score: 0.5625 time: 0.10s
Epoch 56/1000, LR 0.000269
Train loss: 0.1929;  Loss pred: 0.1929; Loss self: 0.0000; time: 0.18s
Val loss: 0.4949 score: 0.6939 time: 0.10s
Test loss: 0.6326 score: 0.5625 time: 0.10s
Epoch 57/1000, LR 0.000269
Train loss: 0.1804;  Loss pred: 0.1804; Loss self: 0.0000; time: 0.18s
Val loss: 0.4911 score: 0.6939 time: 0.09s
Test loss: 0.6298 score: 0.5833 time: 0.20s
Epoch 58/1000, LR 0.000269
Train loss: 0.1931;  Loss pred: 0.1931; Loss self: 0.0000; time: 0.16s
Val loss: 0.4883 score: 0.6939 time: 0.09s
Test loss: 0.6284 score: 0.5833 time: 0.09s
Epoch 59/1000, LR 0.000268
Train loss: 0.1725;  Loss pred: 0.1725; Loss self: 0.0000; time: 0.16s
Val loss: 0.4849 score: 0.6939 time: 0.09s
Test loss: 0.6262 score: 0.5833 time: 0.09s
Epoch 60/1000, LR 0.000268
Train loss: 0.1855;  Loss pred: 0.1855; Loss self: 0.0000; time: 0.16s
Val loss: 0.4818 score: 0.6939 time: 0.09s
Test loss: 0.6244 score: 0.6042 time: 0.09s
Epoch 61/1000, LR 0.000268
Train loss: 0.1702;  Loss pred: 0.1702; Loss self: 0.0000; time: 0.16s
Val loss: 0.4784 score: 0.6939 time: 0.09s
Test loss: 0.6219 score: 0.6042 time: 0.09s
Epoch 62/1000, LR 0.000268
Train loss: 0.1632;  Loss pred: 0.1632; Loss self: 0.0000; time: 0.14s
Val loss: 0.4735 score: 0.6939 time: 0.09s
Test loss: 0.6178 score: 0.6042 time: 0.08s
Epoch 63/1000, LR 0.000268
Train loss: 0.1563;  Loss pred: 0.1563; Loss self: 0.0000; time: 0.16s
Val loss: 0.4692 score: 0.6939 time: 0.08s
Test loss: 0.6140 score: 0.6042 time: 0.09s
Epoch 64/1000, LR 0.000268
Train loss: 0.1677;  Loss pred: 0.1677; Loss self: 0.0000; time: 0.16s
Val loss: 0.4651 score: 0.6939 time: 0.09s
Test loss: 0.6105 score: 0.6042 time: 0.08s
Epoch 65/1000, LR 0.000268
Train loss: 0.1648;  Loss pred: 0.1648; Loss self: 0.0000; time: 0.16s
Val loss: 0.4587 score: 0.7143 time: 0.20s
Test loss: 0.6042 score: 0.5833 time: 0.09s
Epoch 66/1000, LR 0.000268
Train loss: 0.1501;  Loss pred: 0.1501; Loss self: 0.0000; time: 0.16s
Val loss: 0.4516 score: 0.7551 time: 0.09s
Test loss: 0.5968 score: 0.5833 time: 0.09s
Epoch 67/1000, LR 0.000268
Train loss: 0.1482;  Loss pred: 0.1482; Loss self: 0.0000; time: 0.16s
Val loss: 0.4447 score: 0.7347 time: 0.10s
Test loss: 0.5899 score: 0.6042 time: 0.09s
Epoch 68/1000, LR 0.000268
Train loss: 0.1465;  Loss pred: 0.1465; Loss self: 0.0000; time: 0.15s
Val loss: 0.4386 score: 0.7347 time: 0.09s
Test loss: 0.5839 score: 0.6250 time: 0.09s
Epoch 69/1000, LR 0.000268
Train loss: 0.1373;  Loss pred: 0.1373; Loss self: 0.0000; time: 0.15s
Val loss: 0.4340 score: 0.7347 time: 0.09s
Test loss: 0.5797 score: 0.6250 time: 0.08s
Epoch 70/1000, LR 0.000268
Train loss: 0.1457;  Loss pred: 0.1457; Loss self: 0.0000; time: 0.15s
Val loss: 0.4309 score: 0.7347 time: 0.09s
Test loss: 0.5774 score: 0.6250 time: 0.08s
Epoch 71/1000, LR 0.000268
Train loss: 0.1324;  Loss pred: 0.1324; Loss self: 0.0000; time: 0.14s
Val loss: 0.4278 score: 0.7551 time: 0.09s
Test loss: 0.5753 score: 0.6042 time: 0.08s
Epoch 72/1000, LR 0.000267
Train loss: 0.1483;  Loss pred: 0.1483; Loss self: 0.0000; time: 0.16s
Val loss: 0.4251 score: 0.7551 time: 0.18s
Test loss: 0.5739 score: 0.6042 time: 0.10s
Epoch 73/1000, LR 0.000267
Train loss: 0.1392;  Loss pred: 0.1392; Loss self: 0.0000; time: 0.17s
Val loss: 0.4227 score: 0.7551 time: 0.10s
Test loss: 0.5731 score: 0.6250 time: 0.10s
Epoch 74/1000, LR 0.000267
Train loss: 0.1292;  Loss pred: 0.1292; Loss self: 0.0000; time: 0.17s
Val loss: 0.4191 score: 0.7551 time: 0.10s
Test loss: 0.5707 score: 0.6250 time: 0.13s
Epoch 75/1000, LR 0.000267
Train loss: 0.1237;  Loss pred: 0.1237; Loss self: 0.0000; time: 0.18s
Val loss: 0.4154 score: 0.7551 time: 0.12s
Test loss: 0.5681 score: 0.6250 time: 0.10s
Epoch 76/1000, LR 0.000267
Train loss: 0.1242;  Loss pred: 0.1242; Loss self: 0.0000; time: 0.18s
Val loss: 0.4105 score: 0.7551 time: 0.09s
Test loss: 0.5639 score: 0.6250 time: 0.09s
Epoch 77/1000, LR 0.000267
Train loss: 0.1185;  Loss pred: 0.1185; Loss self: 0.0000; time: 0.15s
Val loss: 0.4060 score: 0.7551 time: 0.12s
Test loss: 0.5602 score: 0.6458 time: 0.10s
Epoch 78/1000, LR 0.000267
Train loss: 0.1182;  Loss pred: 0.1182; Loss self: 0.0000; time: 0.32s
Val loss: 0.4003 score: 0.7551 time: 0.10s
Test loss: 0.5549 score: 0.6667 time: 0.17s
Epoch 79/1000, LR 0.000267
Train loss: 0.1200;  Loss pred: 0.1200; Loss self: 0.0000; time: 0.15s
Val loss: 0.3946 score: 0.7551 time: 0.10s
Test loss: 0.5495 score: 0.6667 time: 0.09s
Epoch 80/1000, LR 0.000267
Train loss: 0.1281;  Loss pred: 0.1281; Loss self: 0.0000; time: 0.16s
Val loss: 0.3884 score: 0.7551 time: 0.09s
Test loss: 0.5435 score: 0.6875 time: 0.09s
Epoch 81/1000, LR 0.000267
Train loss: 0.1182;  Loss pred: 0.1182; Loss self: 0.0000; time: 0.15s
Val loss: 0.3810 score: 0.7959 time: 0.09s
Test loss: 0.5362 score: 0.6875 time: 0.09s
Epoch 82/1000, LR 0.000267
Train loss: 0.1015;  Loss pred: 0.1015; Loss self: 0.0000; time: 0.15s
Val loss: 0.3729 score: 0.8163 time: 0.11s
Test loss: 0.5277 score: 0.7083 time: 0.08s
Epoch 83/1000, LR 0.000266
Train loss: 0.1053;  Loss pred: 0.1053; Loss self: 0.0000; time: 0.15s
Val loss: 0.3646 score: 0.8163 time: 0.09s
Test loss: 0.5186 score: 0.7292 time: 0.08s
Epoch 84/1000, LR 0.000266
Train loss: 0.1204;  Loss pred: 0.1204; Loss self: 0.0000; time: 0.16s
Val loss: 0.3553 score: 0.8163 time: 0.09s
Test loss: 0.5076 score: 0.7500 time: 0.08s
Epoch 85/1000, LR 0.000266
Train loss: 0.1029;  Loss pred: 0.1029; Loss self: 0.0000; time: 0.15s
Val loss: 0.3466 score: 0.8163 time: 0.19s
Test loss: 0.4978 score: 0.7917 time: 0.09s
Epoch 86/1000, LR 0.000266
Train loss: 0.1032;  Loss pred: 0.1032; Loss self: 0.0000; time: 0.15s
Val loss: 0.3388 score: 0.8163 time: 0.09s
Test loss: 0.4895 score: 0.7917 time: 0.08s
Epoch 87/1000, LR 0.000266
Train loss: 0.1005;  Loss pred: 0.1005; Loss self: 0.0000; time: 0.15s
Val loss: 0.3322 score: 0.8163 time: 0.09s
Test loss: 0.4829 score: 0.7917 time: 0.08s
Epoch 88/1000, LR 0.000266
Train loss: 0.0958;  Loss pred: 0.0958; Loss self: 0.0000; time: 0.15s
Val loss: 0.3257 score: 0.8367 time: 0.09s
Test loss: 0.4768 score: 0.7917 time: 0.08s
Epoch 89/1000, LR 0.000266
Train loss: 0.0943;  Loss pred: 0.0943; Loss self: 0.0000; time: 0.15s
Val loss: 0.3200 score: 0.8367 time: 0.09s
Test loss: 0.4717 score: 0.7917 time: 0.09s
Epoch 90/1000, LR 0.000266
Train loss: 0.0972;  Loss pred: 0.0972; Loss self: 0.0000; time: 0.15s
Val loss: 0.3136 score: 0.8571 time: 0.09s
Test loss: 0.4652 score: 0.7917 time: 0.08s
Epoch 91/1000, LR 0.000266
Train loss: 0.0987;  Loss pred: 0.0987; Loss self: 0.0000; time: 0.16s
Val loss: 0.3068 score: 0.8776 time: 0.09s
Test loss: 0.4575 score: 0.7917 time: 0.09s
Epoch 92/1000, LR 0.000266
Train loss: 0.0909;  Loss pred: 0.0909; Loss self: 0.0000; time: 0.16s
Val loss: 0.3012 score: 0.8776 time: 0.10s
Test loss: 0.4506 score: 0.7917 time: 0.32s
Epoch 93/1000, LR 0.000265
Train loss: 0.0907;  Loss pred: 0.0907; Loss self: 0.0000; time: 0.16s
Val loss: 0.2945 score: 0.8776 time: 0.09s
Test loss: 0.4422 score: 0.7917 time: 0.10s
Epoch 94/1000, LR 0.000265
Train loss: 0.0954;  Loss pred: 0.0954; Loss self: 0.0000; time: 0.16s
Val loss: 0.2876 score: 0.8776 time: 0.09s
Test loss: 0.4329 score: 0.7917 time: 0.09s
Epoch 95/1000, LR 0.000265
Train loss: 0.1032;  Loss pred: 0.1032; Loss self: 0.0000; time: 0.15s
Val loss: 0.2803 score: 0.8980 time: 0.09s
Test loss: 0.4227 score: 0.8125 time: 0.09s
Epoch 96/1000, LR 0.000265
Train loss: 0.0873;  Loss pred: 0.0873; Loss self: 0.0000; time: 0.16s
Val loss: 0.2743 score: 0.8980 time: 0.09s
Test loss: 0.4144 score: 0.8125 time: 0.09s
Epoch 97/1000, LR 0.000265
Train loss: 0.0914;  Loss pred: 0.0914; Loss self: 0.0000; time: 0.15s
Val loss: 0.2682 score: 0.8980 time: 0.09s
Test loss: 0.4052 score: 0.8125 time: 0.09s
Epoch 98/1000, LR 0.000265
Train loss: 0.0908;  Loss pred: 0.0908; Loss self: 0.0000; time: 0.15s
Val loss: 0.2633 score: 0.8980 time: 0.09s
Test loss: 0.3977 score: 0.8125 time: 0.08s
Epoch 99/1000, LR 0.000265
Train loss: 0.0853;  Loss pred: 0.0853; Loss self: 0.0000; time: 0.15s
Val loss: 0.2582 score: 0.8980 time: 0.21s
Test loss: 0.3899 score: 0.8125 time: 0.08s
Epoch 100/1000, LR 0.000265
Train loss: 0.0774;  Loss pred: 0.0774; Loss self: 0.0000; time: 0.15s
Val loss: 0.2527 score: 0.9184 time: 0.09s
Test loss: 0.3817 score: 0.8333 time: 0.09s
Epoch 101/1000, LR 0.000265
Train loss: 0.0898;  Loss pred: 0.0898; Loss self: 0.0000; time: 0.15s
Val loss: 0.2478 score: 0.9184 time: 0.09s
Test loss: 0.3744 score: 0.8333 time: 0.09s
Epoch 102/1000, LR 0.000264
Train loss: 0.0797;  Loss pred: 0.0797; Loss self: 0.0000; time: 0.15s
Val loss: 0.2433 score: 0.9388 time: 0.09s
Test loss: 0.3676 score: 0.8542 time: 0.08s
Epoch 103/1000, LR 0.000264
Train loss: 0.0817;  Loss pred: 0.0817; Loss self: 0.0000; time: 0.16s
Val loss: 0.2399 score: 0.9388 time: 0.10s
Test loss: 0.3621 score: 0.8542 time: 0.08s
Epoch 104/1000, LR 0.000264
Train loss: 0.0714;  Loss pred: 0.0714; Loss self: 0.0000; time: 0.15s
Val loss: 0.2356 score: 0.9388 time: 0.10s
Test loss: 0.3549 score: 0.8542 time: 0.09s
Epoch 105/1000, LR 0.000264
Train loss: 0.0708;  Loss pred: 0.0708; Loss self: 0.0000; time: 0.16s
Val loss: 0.2321 score: 0.9388 time: 0.09s
Test loss: 0.3491 score: 0.8542 time: 0.08s
Epoch 106/1000, LR 0.000264
Train loss: 0.0742;  Loss pred: 0.0742; Loss self: 0.0000; time: 0.15s
Val loss: 0.2293 score: 0.9388 time: 0.08s
Test loss: 0.3446 score: 0.8542 time: 0.09s
Epoch 107/1000, LR 0.000264
Train loss: 0.0770;  Loss pred: 0.0770; Loss self: 0.0000; time: 0.30s
Val loss: 0.2267 score: 0.9388 time: 0.09s
Test loss: 0.3399 score: 0.8542 time: 0.09s
Epoch 108/1000, LR 0.000264
Train loss: 0.0663;  Loss pred: 0.0663; Loss self: 0.0000; time: 0.15s
Val loss: 0.2242 score: 0.9388 time: 0.09s
Test loss: 0.3350 score: 0.8542 time: 0.09s
Epoch 109/1000, LR 0.000264
Train loss: 0.0773;  Loss pred: 0.0773; Loss self: 0.0000; time: 0.14s
Val loss: 0.2219 score: 0.9388 time: 0.09s
Test loss: 0.3301 score: 0.8542 time: 0.09s
Epoch 110/1000, LR 0.000263
Train loss: 0.0787;  Loss pred: 0.0787; Loss self: 0.0000; time: 0.16s
Val loss: 0.2196 score: 0.9388 time: 0.09s
Test loss: 0.3247 score: 0.8542 time: 0.09s
Epoch 111/1000, LR 0.000263
Train loss: 0.0673;  Loss pred: 0.0673; Loss self: 0.0000; time: 0.15s
Val loss: 0.2175 score: 0.9388 time: 0.09s
Test loss: 0.3193 score: 0.8542 time: 0.09s
Epoch 112/1000, LR 0.000263
Train loss: 0.0661;  Loss pred: 0.0661; Loss self: 0.0000; time: 0.16s
Val loss: 0.2157 score: 0.9388 time: 0.09s
Test loss: 0.3142 score: 0.8542 time: 0.09s
Epoch 113/1000, LR 0.000263
Train loss: 0.0727;  Loss pred: 0.0727; Loss self: 0.0000; time: 0.16s
Val loss: 0.2143 score: 0.9388 time: 0.09s
Test loss: 0.3104 score: 0.8750 time: 0.08s
Epoch 114/1000, LR 0.000263
Train loss: 0.0631;  Loss pred: 0.0631; Loss self: 0.0000; time: 0.17s
Val loss: 0.2131 score: 0.9388 time: 0.14s
Test loss: 0.3067 score: 0.8750 time: 0.09s
Epoch 115/1000, LR 0.000263
Train loss: 0.0608;  Loss pred: 0.0608; Loss self: 0.0000; time: 0.15s
Val loss: 0.2122 score: 0.9388 time: 0.09s
Test loss: 0.3042 score: 0.8750 time: 0.09s
Epoch 116/1000, LR 0.000263
Train loss: 0.0594;  Loss pred: 0.0594; Loss self: 0.0000; time: 0.16s
Val loss: 0.2114 score: 0.9388 time: 0.09s
Test loss: 0.3019 score: 0.8750 time: 0.09s
Epoch 117/1000, LR 0.000262
Train loss: 0.0591;  Loss pred: 0.0591; Loss self: 0.0000; time: 0.16s
Val loss: 0.2108 score: 0.9184 time: 0.09s
Test loss: 0.2994 score: 0.8750 time: 0.09s
Epoch 118/1000, LR 0.000262
Train loss: 0.0569;  Loss pred: 0.0569; Loss self: 0.0000; time: 0.16s
Val loss: 0.2104 score: 0.9184 time: 0.09s
Test loss: 0.2983 score: 0.8750 time: 0.09s
Epoch 119/1000, LR 0.000262
Train loss: 0.0616;  Loss pred: 0.0616; Loss self: 0.0000; time: 0.16s
Val loss: 0.2100 score: 0.9184 time: 0.09s
Test loss: 0.2963 score: 0.8750 time: 0.10s
Epoch 120/1000, LR 0.000262
Train loss: 0.0564;  Loss pred: 0.0564; Loss self: 0.0000; time: 0.17s
Val loss: 0.2098 score: 0.9184 time: 0.09s
Test loss: 0.2954 score: 0.8750 time: 0.09s
Epoch 121/1000, LR 0.000262
Train loss: 0.0606;  Loss pred: 0.0606; Loss self: 0.0000; time: 0.17s
Val loss: 0.2096 score: 0.9184 time: 0.09s
Test loss: 0.2943 score: 0.8750 time: 0.09s
Epoch 122/1000, LR 0.000262
Train loss: 0.0578;  Loss pred: 0.0578; Loss self: 0.0000; time: 0.28s
Val loss: 0.2093 score: 0.9184 time: 0.10s
Test loss: 0.2925 score: 0.8750 time: 0.08s
Epoch 123/1000, LR 0.000262
Train loss: 0.0633;  Loss pred: 0.0633; Loss self: 0.0000; time: 0.15s
Val loss: 0.2092 score: 0.9184 time: 0.09s
Test loss: 0.2906 score: 0.8750 time: 0.09s
Epoch 124/1000, LR 0.000261
Train loss: 0.0589;  Loss pred: 0.0589; Loss self: 0.0000; time: 0.14s
Val loss: 0.2091 score: 0.9184 time: 0.09s
Test loss: 0.2891 score: 0.8750 time: 0.09s
Epoch 125/1000, LR 0.000261
Train loss: 0.0659;  Loss pred: 0.0659; Loss self: 0.0000; time: 0.15s
Val loss: 0.2091 score: 0.9184 time: 0.09s
Test loss: 0.2876 score: 0.8750 time: 0.09s
Epoch 126/1000, LR 0.000261
Train loss: 0.0537;  Loss pred: 0.0537; Loss self: 0.0000; time: 0.16s
Val loss: 0.2089 score: 0.9184 time: 0.09s
Test loss: 0.2850 score: 0.8750 time: 0.09s
Epoch 127/1000, LR 0.000261
Train loss: 0.0545;  Loss pred: 0.0545; Loss self: 0.0000; time: 0.16s
Val loss: 0.2091 score: 0.9184 time: 0.09s
Test loss: 0.2835 score: 0.8750 time: 0.09s
     INFO: Early stopping counter 1 of 20
Epoch 128/1000, LR 0.000261
Train loss: 0.0652;  Loss pred: 0.0652; Loss self: 0.0000; time: 0.15s
Val loss: 0.2093 score: 0.9184 time: 0.09s
Test loss: 0.2826 score: 0.8750 time: 0.09s
     INFO: Early stopping counter 2 of 20
Epoch 129/1000, LR 0.000261
Train loss: 0.0493;  Loss pred: 0.0493; Loss self: 0.0000; time: 0.15s
Val loss: 0.2095 score: 0.9184 time: 0.09s
Test loss: 0.2820 score: 0.8750 time: 0.08s
     INFO: Early stopping counter 3 of 20
Epoch 130/1000, LR 0.000260
Train loss: 0.0562;  Loss pred: 0.0562; Loss self: 0.0000; time: 0.15s
Val loss: 0.2096 score: 0.9184 time: 0.09s
Test loss: 0.2813 score: 0.8750 time: 0.20s
     INFO: Early stopping counter 4 of 20
Epoch 131/1000, LR 0.000260
Train loss: 0.0463;  Loss pred: 0.0463; Loss self: 0.0000; time: 0.15s
Val loss: 0.2100 score: 0.9184 time: 0.09s
Test loss: 0.2810 score: 0.8750 time: 0.08s
     INFO: Early stopping counter 5 of 20
Epoch 132/1000, LR 0.000260
Train loss: 0.0476;  Loss pred: 0.0476; Loss self: 0.0000; time: 0.16s
Val loss: 0.2103 score: 0.9184 time: 0.09s
Test loss: 0.2798 score: 0.8750 time: 0.09s
     INFO: Early stopping counter 6 of 20
Epoch 133/1000, LR 0.000260
Train loss: 0.0484;  Loss pred: 0.0484; Loss self: 0.0000; time: 0.16s
Val loss: 0.2106 score: 0.9184 time: 0.09s
Test loss: 0.2788 score: 0.8750 time: 0.09s
     INFO: Early stopping counter 7 of 20
Epoch 134/1000, LR 0.000260
Train loss: 0.0502;  Loss pred: 0.0502; Loss self: 0.0000; time: 0.16s
Val loss: 0.2110 score: 0.9184 time: 0.09s
Test loss: 0.2775 score: 0.8750 time: 0.09s
     INFO: Early stopping counter 8 of 20
Epoch 135/1000, LR 0.000260
Train loss: 0.0479;  Loss pred: 0.0479; Loss self: 0.0000; time: 0.16s
Val loss: 0.2114 score: 0.9184 time: 0.09s
Test loss: 0.2768 score: 0.8750 time: 0.09s
     INFO: Early stopping counter 9 of 20
Epoch 136/1000, LR 0.000260
Train loss: 0.0513;  Loss pred: 0.0513; Loss self: 0.0000; time: 0.15s
Val loss: 0.2119 score: 0.9184 time: 0.08s
Test loss: 0.2768 score: 0.8750 time: 0.08s
     INFO: Early stopping counter 10 of 20
Epoch 137/1000, LR 0.000259
Train loss: 0.0492;  Loss pred: 0.0492; Loss self: 0.0000; time: 0.14s
Val loss: 0.2122 score: 0.9184 time: 0.08s
Test loss: 0.2760 score: 0.8750 time: 0.08s
     INFO: Early stopping counter 11 of 20
Epoch 138/1000, LR 0.000259
Train loss: 0.0569;  Loss pred: 0.0569; Loss self: 0.0000; time: 0.15s
Val loss: 0.2125 score: 0.9184 time: 0.19s
Test loss: 0.2750 score: 0.8750 time: 0.08s
     INFO: Early stopping counter 12 of 20
Epoch 139/1000, LR 0.000259
Train loss: 0.0472;  Loss pred: 0.0472; Loss self: 0.0000; time: 0.15s
Val loss: 0.2128 score: 0.9184 time: 0.08s
Test loss: 0.2734 score: 0.8750 time: 0.08s
     INFO: Early stopping counter 13 of 20
Epoch 140/1000, LR 0.000259
Train loss: 0.0428;  Loss pred: 0.0428; Loss self: 0.0000; time: 0.15s
Val loss: 0.2130 score: 0.9184 time: 0.09s
Test loss: 0.2721 score: 0.8750 time: 0.09s
     INFO: Early stopping counter 14 of 20
Epoch 141/1000, LR 0.000259
Train loss: 0.0467;  Loss pred: 0.0467; Loss self: 0.0000; time: 0.15s
Val loss: 0.2132 score: 0.9184 time: 0.09s
Test loss: 0.2708 score: 0.8750 time: 0.09s
     INFO: Early stopping counter 15 of 20
Epoch 142/1000, LR 0.000259
Train loss: 0.0411;  Loss pred: 0.0411; Loss self: 0.0000; time: 0.15s
Val loss: 0.2134 score: 0.9184 time: 0.09s
Test loss: 0.2701 score: 0.8750 time: 0.08s
     INFO: Early stopping counter 16 of 20
Epoch 143/1000, LR 0.000258
Train loss: 0.0429;  Loss pred: 0.0429; Loss self: 0.0000; time: 0.15s
Val loss: 0.2137 score: 0.9184 time: 0.09s
Test loss: 0.2693 score: 0.8750 time: 0.08s
     INFO: Early stopping counter 17 of 20
Epoch 144/1000, LR 0.000258
Train loss: 0.0398;  Loss pred: 0.0398; Loss self: 0.0000; time: 0.15s
Val loss: 0.2141 score: 0.9184 time: 0.09s
Test loss: 0.2688 score: 0.8750 time: 0.08s
     INFO: Early stopping counter 18 of 20
Epoch 145/1000, LR 0.000258
Train loss: 0.0463;  Loss pred: 0.0463; Loss self: 0.0000; time: 0.14s
Val loss: 0.2144 score: 0.9184 time: 0.21s
Test loss: 0.2694 score: 0.8750 time: 0.08s
     INFO: Early stopping counter 19 of 20
Epoch 146/1000, LR 0.000258
Train loss: 0.0457;  Loss pred: 0.0457; Loss self: 0.0000; time: 0.15s
Val loss: 0.2147 score: 0.9184 time: 0.09s
Test loss: 0.2683 score: 0.8750 time: 0.09s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 125,   Train_Loss: 0.0537,   Val_Loss: 0.2089,   Val_Precision: 0.9200,   Val_Recall: 0.9200,   Val_accuracy: 0.9200,   Val_Score: 0.9184,   Val_Loss: 0.2089,   Test_Precision: 0.8000,   Test_Recall: 1.0000,   Test_accuracy: 0.8889,   Test_Score: 0.8750,   Test_loss: 0.2850


[0.09637932595796883, 0.09857086394913495, 0.09941427898593247, 0.09927489422261715, 0.09862565412186086, 0.0967537818942219, 0.09689547005109489, 0.09989027911797166, 0.10120426607318223, 0.10460687195882201, 0.10105438716709614, 0.0967152260709554, 0.09538910794071853, 0.09869300690479577, 0.10012829001061618, 0.09686358296312392, 0.09705522819422185, 0.09733351110480726, 0.1009229770861566, 0.10499870195053518, 0.10071813501417637, 0.09491636394523084, 0.09443707391619682, 0.09807165013626218, 0.09758537588641047, 0.09553289809264243, 0.09478604281321168, 0.09740679105743766, 0.09965529991313815, 0.10549134900793433, 0.10052248905412853, 0.0995034200605005, 0.09661654802039266, 0.09733573393896222, 0.0985178779810667, 0.09314462705515325, 0.10024720896035433, 0.09171853610314429, 0.09426239598542452, 0.09440813399851322, 0.0929876349400729, 0.09304485702887177, 3.144433974986896, 0.0975227509625256, 0.101262500975281, 0.09925344306975603, 0.09908093302510679, 0.10118427779525518, 0.09846929018385708, 0.10028810799121857, 0.2173422120977193, 0.09995974204503, 0.09811485907994211, 0.09930934896692634, 0.10103259608149529, 0.09993958286941051, 0.10027994215488434, 0.09842154406942427, 0.09930139407515526, 0.10037223901599646, 0.0992641078773886, 0.10067941108718514, 0.10002194810658693, 0.09754292899742723, 0.09815662796609104, 0.09776058490388095, 0.09723178786225617, 0.0989613609854132, 0.09764794097281992, 0.09837785409763455, 0.1015543881803751, 0.09843742311932147, 0.10050052497535944, 0.09718872793018818, 0.09752774704247713, 0.099183278856799, 0.09693550900556147, 0.09635552205145359, 0.09801227389834821, 0.10050106002017856, 0.09697237005457282, 0.1001443020068109, 0.09738525119610131, 0.09953965595923364, 0.09837057907134295, 0.09957529092207551, 0.09759931801818311, 0.09795144898816943, 0.10047362092882395, 0.09802661300636828, 0.09780577709898353, 0.09723115316592157, 0.09962794510647655, 0.0968415210954845, 0.09877185500226915, 0.09773123217746615, 0.09830323304049671, 0.10006915288977325, 0.09808423393405974, 0.09864312992431223, 0.09726201207377017, 0.09738949197344482, 0.0978548969142139, 0.09764305199496448, 0.09836454782634974, 0.09707906399853528, 0.0967504361178726, 0.23881512600928545, 0.09737191395834088, 0.09449485107325017, 0.09586431505158544, 0.09470868902280927, 0.09525814815424383, 0.09658049698919058, 0.09928513201884925, 0.10060313809663057, 0.09808940114453435, 0.09959369897842407, 0.09958888590335846, 0.09898317907936871, 0.09929890814237297, 0.1003211138304323, 0.0962218539789319, 0.09890127298422158, 0.10373995592817664, 0.09706783597357571, 0.09925592597573996, 0.09694811701774597, 0.09368218085728586, 0.09546093386597931, 0.09614448598586023, 0.09031840204261243, 0.09106953605078161, 0.09278617613017559, 0.0907775058876723, 0.0931558480951935, 0.09162443899549544, 0.09043845906853676, 0.08968549617566168, 0.09025308606214821, 0.09078817814588547, 0.09133483399637043, 0.08554390002973378, 0.09328552288934588, 0.08597849286161363, 0.08534336788579822, 0.08849439304322004, 0.08682167599909008, 0.0912603831384331, 0.08586760982871056, 0.22251093084923923, 0.09077621693722904, 0.09456114005297422, 0.09309211815707386, 0.09203563397750258, 0.0942818729672581, 0.09102061297744513, 0.19653489999473095, 0.09138070791959763, 0.09339908510446548, 0.09098753496073186, 0.09515938512049615, 0.09360193996690214, 0.09140476188622415, 0.09344365890137851, 0.09116338798776269, 0.09167036996223032, 0.15032579004764557, 0.09271308081224561, 0.0947604060638696, 0.10276887402869761, 0.215761257102713, 0.0913330230396241, 0.09546408290043473, 0.0912373240571469, 0.09248217102140188, 0.09336548508144915, 0.0919252261519432, 0.09145255293697119, 0.09119004802778363, 0.09490605304017663, 0.09010573383420706, 0.08861321792937815, 0.08948182314634323, 0.09151944192126393, 0.09004380903206766, 0.19651903002522886, 0.09005660400725901, 0.08877517795190215, 0.08931337599642575, 0.08821836393326521, 0.09133647009730339, 0.09084783005528152, 0.09333119401708245, 0.09071959904395044, 0.09252752899192274, 0.09326244308613241, 0.09380639297887683, 0.0935678188689053, 0.09370002802461386, 0.09219892020337284, 0.09269610187038779, 0.09221454197540879, 0.091869099996984, 0.0943295331671834, 0.08887872099876404, 0.09064353001303971, 0.09272864414379, 0.0894936139229685, 0.091329675167799, 0.0906156999990344, 0.09347165189683437, 0.09101227694191039, 0.09237611386924982, 0.0917423041537404, 0.09288345510140061, 0.09152573999017477, 0.09308996610343456, 0.09299904992803931, 0.09202656196430326, 0.09227921580895782, 0.09177153301425278, 0.10418412205763161, 0.09423888707533479, 0.095542793860659, 0.09130400698632002, 0.09474093606695533, 0.0911473329178989, 0.08740485901944339, 0.09028918389230967, 0.10817421390675008, 0.08642604202032089, 0.0886388081125915, 0.08592285797931254, 0.0870313469786197, 0.08598827803507447, 0.08881259802728891, 0.08340556896291673, 0.0888923560269177, 0.0934586119838059, 0.09069116204045713, 0.0891385751310736, 0.09173622401431203, 0.09529589209705591, 0.09333174396306276, 0.09213624382391572, 0.09240008099004626, 0.09122881502844393, 0.09085002797655761, 0.09252930898219347, 0.09085174417123199, 0.0926178339868784, 0.09283793205395341, 0.09156342200003564, 0.09150463598780334, 0.09436340909451246, 0.09499768586829305, 0.09314802195876837, 0.09941248898394406, 0.09599158610217273, 0.16705798893235624, 0.08928273292258382, 0.0891395250800997, 0.09246858395636082, 0.09268200700171292, 0.08898881310597062, 0.09259634604677558, 0.22814731416292489, 0.09063647105358541, 0.0936582200229168, 0.09117785701528192, 0.0912406521383673, 0.0946977089624852, 0.0922514540143311, 0.09354820288717747, 0.09112038603052497, 0.08848264999687672, 0.08866947307251394, 0.0876795700751245, 0.08802523906342685, 0.09759022598154843, 0.09163334802724421, 2.823735178913921, 0.09026288101449609, 0.0910231729503721, 0.09058812912553549, 0.09136734809726477, 0.09393659490160644, 0.08749623992480338, 0.08774462807923555, 0.08735522790811956, 0.0877988520078361, 0.08751244889572263, 0.09053518692962825, 0.08958805399015546, 0.09275519009679556, 0.09442283515818417, 0.22041016002185643, 0.08492342312820256, 0.08549498906359076, 0.08497774298302829, 0.08510030107572675, 0.084692268865183, 0.08647373598068953, 0.08533244300633669, 0.08190472004935145, 0.0883707741741091, 0.08767615305259824, 0.09155544196255505, 0.08900332916527987, 0.08854503417387605, 0.08928696485236287, 0.22126020397990942, 0.0912412831094116, 0.09013250120915473, 0.0912033929489553, 0.09105680393986404, 0.09071812010370195, 0.09161692089401186, 0.21194814098998904, 0.09052040800452232, 0.08769756113179028, 0.08368668099865317, 0.09939935593865812, 0.09414643491618335, 0.09000412910245359, 0.09026786801405251, 0.08255196083337069, 0.08460330287925899, 0.09804261196404696, 0.08980515599250793, 0.09037171700038016, 0.0901213800534606, 0.08978246408514678, 0.08930698200128973, 0.09038690384477377, 0.0892801289446652, 0.0932461351621896, 0.08842916809953749, 0.08798707812093198, 0.09172512497752905, 0.09259570203721523, 0.09034446394070983, 0.08989401394501328, 0.0898044400382787, 0.08876909804530442, 0.09420595900155604, 0.0895902260672301, 0.09297418408095837, 0.08755095396190882, 0.09311337978579104, 0.08820021594874561, 0.09139610012061894, 0.09074611798860133, 0.0929865101352334, 0.09092219220474362, 0.08669904200360179, 0.09620997007004917, 0.08975116396322846, 0.09093414293602109, 0.10111713409423828, 0.08979739109054208, 0.08718327200040221, 0.08770360914058983, 0.08760891505517066, 0.08761249901726842, 0.08934585889801383, 0.08768264995887876, 0.09063448384404182, 0.08855276997201145, 0.08838831493631005, 0.09323324798606336, 0.09324108087457716, 0.09429667517542839, 0.09276435407809913, 0.09398964117281139, 0.09238702803850174, 0.08882899908348918, 0.09078340395353734, 0.08623214205726981, 0.08597242482937872, 0.0915086381137371, 0.0879841789137572, 0.08813501382246614, 0.1042499509640038, 0.10508361086249352, 0.10489018284715712, 0.10617388901300728, 0.10715599195100367, 0.10712448786944151, 0.20571905793622136, 0.09132663509808481, 0.0917988820001483, 0.09374317084439099, 0.09102547098882496, 0.08810249390080571, 0.09366909693926573, 0.08668971690349281, 0.0989738639909774, 0.09666130598634481, 0.09228427382186055, 0.09214448207058012, 0.08775772200897336, 0.08696377603337169, 0.0874671929050237, 0.1061846490483731, 0.10555299301631749, 0.14187605818733573, 0.1041177180595696, 0.09099646098911762, 0.10867350804619491, 0.1782597608398646, 0.09187019802629948, 0.09072991204448044, 0.09024593909271061, 0.08661616104654968, 0.08748693601228297, 0.08277272689156234, 0.09073774400167167, 0.08857117802836001, 0.08764895610511303, 0.08941998798400164, 0.09015030204318464, 0.08953298977576196, 0.08995924307964742, 0.32261473685503006, 0.1024354740511626, 0.09121381095610559, 0.09160881396383047, 0.09174088900908828, 0.09023793507367373, 0.08889912092126906, 0.0886457390151918, 0.09241374093107879, 0.09055298590101302, 0.08884839806705713, 0.09039035881869495, 0.09343299712054431, 0.08936748700216413, 0.09156172606162727, 0.09222264890559018, 0.09148709196597338, 0.0922139270696789, 0.09205256612040102, 0.09342811908572912, 0.09390572016127408, 0.0901958670001477, 0.09111924888566136, 0.09260541712865233, 0.0907995521556586, 0.09563358617015183, 0.09257510700263083, 0.10083140200003982, 0.09160775202326477, 0.09317245404236019, 0.08850012114271522, 0.09099235502071679, 0.09162706392817199, 0.0946451851632446, 0.09209007793106139, 0.09062771406024694, 0.09208833100274205, 0.08841587300412357, 0.20715281809680164, 0.08949282299727201, 0.09118837281130254, 0.09069934813305736, 0.09243433410301805, 0.09218853199854493, 0.08323968597687781, 0.08554577105678618, 0.08517532609403133, 0.08564275712706149, 0.09059649286791682, 0.09894632501527667, 0.08928646077401936, 0.08732489589601755, 0.0883708221372217, 0.08986433083191514, 0.09029034315608442]
[0.0019669250195503843, 0.0020116502846762234, 0.0020288628364476015, 0.002026018249441166, 0.002012768451466548, 0.0019745669774331003, 0.0019774585724713243, 0.0020385771248565645, 0.002065393185166984, 0.0021348341216086125, 0.0020623344319815537, 0.001973780123897049, 0.0019467164885860924, 0.0020141429980570563, 0.002043434490012575, 0.0019768078155739575, 0.0019807189427392216, 0.001986398185812393, 0.002059652593595033, 0.002142830652051738, 0.0020554721431464565, 0.0019370686519434865, 0.001927287222779527, 0.00200146224767882, 0.0019915382833961322, 0.0019496509814824984, 0.00193440903700432, 0.001987893695049748, 0.0020337816308803706, 0.002152884673631313, 0.0020514793684516027, 0.002030682042051031, 0.0019717662861304624, 0.0019864435497747393, 0.0020105689383891163, 0.0019009107562276174, 0.00204586140735417, 0.0018718068592478425, 0.0019237223670494799, 0.0019266966122145556, 0.0018977068355116918, 0.0018988746332422811, 0.06417212193850808, 0.0019902602237250122, 0.0020665816525567552, 0.0020255804708113477, 0.0020220598576552408, 0.002064985261127657, 0.002009577350690961, 0.00204669608145344, 0.004435555348933047, 0.002039994735612857, 0.0020023440628559614, 0.0020267214074882927, 0.0020618897159488834, 0.0020395833238655205, 0.0020465294317323335, 0.002008602940192332, 0.0020265590627582706, 0.0020484130411427847, 0.002025798119946706, 0.0020546818589221457, 0.0020412642470732027, 0.0019906720203556575, 0.0020031964891038985, 0.0019951139776302235, 0.0019843222012705343, 0.002019619611947208, 0.0019928151218942844, 0.0020077113081149907, 0.002072538534293369, 0.002008927002435132, 0.0020510311219461112, 0.0019834434271466975, 0.0019903621845403494, 0.002024148548097939, 0.00197827569399105, 0.0019664392255398693, 0.002000250487721392, 0.0020510420412281336, 0.0019790279602974045, 0.0020437612654451206, 0.0019874541060428837, 0.0020314215501884415, 0.0020075628381906723, 0.0020321487943280717, 0.0019918228166976143, 0.001999009163023866, 0.002050482059771917, 0.0020005431225789444, 0.0019960362673261942, 0.001984309248284114, 0.0020332233695199296, 0.0019763575733772348, 0.002015752142903452, 0.0019945149423972685, 0.002006188429397892, 0.0020422276099953725, 0.0020017190598787702, 0.002013125100496168, 0.001984939021913677, 0.001987540652519282, 0.0019970387125349777, 0.00199271534683601, 0.002007439751558158, 0.0019812053877252098, 0.0019744986962831144, 0.004873778081822152, 0.001987181917517161, 0.001928466348433677, 0.001956414592889499, 0.0019328303882205973, 0.0019440438398825271, 0.0019710305507998076, 0.002026227184058148, 0.002053125267278175, 0.0020018245131537622, 0.00203252446894743, 0.002032426242925683, 0.0020200648791707902, 0.002026508329436183, 0.0020473696700088226, 0.0019637113056924877, 0.002018393326208604, 0.0021171419577178906, 0.001980976244358688, 0.00202563114236204, 0.0019785330003621628, 0.0019118812419854257, 0.0019481823237954962, 0.001962132367058372, 0.0018432326947471925, 0.0018585619602200327, 0.0018935954312280733, 0.0018526021609729041, 0.0019011397570447655, 0.0018698865101121518, 0.0018456828381334031, 0.0018303162484828914, 0.0018418997155540452, 0.0018528199621609279, 0.0018639762040075598, 0.001745793878157832, 0.0019037861814152222, 0.001754663119624768, 0.0017417013854244534, 0.001806008021290205, 0.0017718709387569403, 0.0018624567987435327, 0.0017524002005859297, 0.004541039405086515, 0.0018525758558618171, 0.001929819184754576, 0.0018998391460627317, 0.0018782782444388283, 0.001924119856474655, 0.0018575635301519412, 0.00401091632642308, 0.0018649124065224006, 0.0019061037776421528, 0.0018568884685863645, 0.0019420282677652277, 0.001910243672793921, 0.0018654033038004929, 0.0019070134469669085, 0.001860477305872708, 0.0018708238767802107, 0.003067873266278481, 0.0018921036900458287, 0.001933885838038155, 0.002097323959769339, 0.004403290961279857, 0.0018639392457066141, 0.0019482465898047906, 0.001861986205247896, 0.0018873912453347323, 0.0019054180628867174, 0.001876025023509045, 0.001866378631366759, 0.001861021388322115, 0.001936858225309727, 0.0018388925272287155, 0.001808433018966901, 0.001826159656047821, 0.0018677437126788559, 0.0018376287557564828, 0.004010592449494466, 0.0018378898776991635, 0.0018117383255490235, 0.0018227219591107294, 0.0018003747741482696, 0.001864009593822518, 0.0018540373480669698, 0.0019047182452465808, 0.0018514203886520497, 0.0018883169182025048, 0.0019033151650231105, 0.0019144161832423843, 0.0019095473238552104, 0.0019122454698900788, 0.001881610616395364, 0.0018917571810283223, 0.0018819294280695673, 0.0018748795917751838, 0.0019250925136159878, 0.001813851448954368, 0.00184986795944979, 0.001892421309056939, 0.0018264002841422145, 0.0018638709217918162, 0.0018492999999802939, 0.0019075847325884566, 0.001857393406977763, 0.0018852268136581596, 0.0018722919215049063, 0.0018955807163551146, 0.0018678722446974442, 0.0018997952266007053, 0.001897939794449782, 0.0018780931013123114, 0.001883249302223629, 0.0018728884288623016, 0.002126206572604727, 0.0019232425933741794, 0.0019498529359318164, 0.0018633470813534698, 0.0019334884911623535, 0.0018601496513856917, 0.0017837726330498652, 0.0018426364059655033, 0.002207637018505104, 0.001763796775924916, 0.0018089552676039083, 0.0017535277138635212, 0.0017761499383391775, 0.0017548628170423362, 0.0018125020005569166, 0.0017021544686309537, 0.0018141297148350551, 0.001907318611914406, 0.0018508400416419823, 0.001819154594511706, 0.0018721678370267761, 0.0019448141244297124, 0.001904729468633934, 0.0018803315066105249, 0.0018857159385723726, 0.0018618125516008965, 0.0018540822036032165, 0.0018883532445345605, 0.0018541172279843263, 0.0018901598772832326, 0.0018946516745704778, 0.00186864126530685, 0.0018674415507714968, 0.0019257838590716829, 0.0019387282830263888, 0.0019009800399748646, 0.0020288263057947767, 0.001959011961268831, 0.0034093467129052294, 0.0018220965902568127, 0.0018191739812265246, 0.0018871139582930779, 0.0018914695306472024, 0.0018160982266524617, 0.0018897213478933793, 0.004656067635978059, 0.0018497238990527634, 0.0019113922453656488, 0.0018607725921486104, 0.001862054125272802, 0.0019326063053568406, 0.001882682734986349, 0.0019091469976974993, 0.0018595997149086728, 0.0018057683672831983, 0.0018095810831125294, 0.0017893789811249897, 0.0017964334502740173, 0.0019916372649295597, 0.0018700683270866166, 0.057627248549263696, 0.0018420996125407365, 0.0018576157744973898, 0.001848737329092561, 0.001864639757087036, 0.001917073365338907, 0.0017856375494857831, 0.001790706695494603, 0.0017827597532269297, 0.0017918133062823694, 0.0017859683448106659, 0.0018476568761148623, 0.0018283276324521523, 0.0018929630631999094, 0.0019269966358813097, 0.004498166531058294, 0.0017331310842490317, 0.0017447956951753218, 0.0017342396527148631, 0.0017367408382801377, 0.001728413650309857, 0.0017647701220548883, 0.0017414784287007488, 0.0016715248989663562, 0.0018034851872267164, 0.0017893092459713926, 0.0018684784073990826, 0.0018163944727608136, 0.0018070415137525723, 0.0018221829561706707, 0.004515514366936927, 0.0018620670022328897, 0.0018394388001868312, 0.001861293733652149, 0.001858302121221715, 0.001851390206197999, 0.0018697330794696296, 0.004325472265101818, 0.0018473552653984148, 0.0017897461455467405, 0.0017078914489521055, 0.0020285582844624104, 0.0019213558146159869, 0.001836818961274563, 0.001842201388041888, 0.0016847338945585855, 0.001726598017944061, 0.0020008696319193256, 0.0018327582855613865, 0.0018443207551097991, 0.0018392118378257264, 0.001832295185411159, 0.0018605621250268693, 0.0018830604967661202, 0.0018600026863471915, 0.00194262781587895, 0.001842274335407031, 0.0018330641275194164, 0.0019109401036985219, 0.0019290771257753174, 0.0018821763320981215, 0.0018727919571877767, 0.0018709258341308062, 0.0018493562092771754, 0.001962624145865751, 0.001866463043067294, 0.0019369621683532994, 0.001823978207539767, 0.0019398620788706467, 0.0018375044989322002, 0.0019040854191795613, 0.0018905441247625276, 0.0019372189611506958, 0.0018942123375988256, 0.0018062300417417039, 0.002004374376459358, 0.001869815915900593, 0.001894461311167106, 0.002106606960296631, 0.00187077898105296, 0.001816318166675046, 0.0018271585237622883, 0.0018251857303160552, 0.001825260396193092, 0.0018613720603752881, 0.0018267218741433073, 0.001888218413417538, 0.0018448493744169052, 0.0018414232278397928, 0.0019423593330429867, 0.0019425225182203576, 0.001964514066154758, 0.0019325907099603985, 0.0019581175244335705, 0.0019247297508021195, 0.0018506041475726913, 0.0018913209156986948, 0.0017965029595264543, 0.0017910921839453902, 0.0019064299607028563, 0.0018330037273699418, 0.0018361461213013779, 0.002171873978416746, 0.002189241892968615, 0.0021852121426491067, 0.0022119560211043185, 0.002232416498979243, 0.002231760163946698, 0.004285813707004611, 0.0019026382312101002, 0.001912476708336423, 0.0019529827259248123, 0.0018963639789338533, 0.0018354686229334523, 0.001951439519568036, 0.0018060357688227668, 0.0020619554998120293, 0.0020137772080488503, 0.0019225890379554282, 0.0019196767098037526, 0.001828285875186945, 0.0018117453340285767, 0.001822233185521327, 0.002212180188507773, 0.0021990206878399476, 0.002955751212236161, 0.0021691191262410334, 0.0018957596039399505, 0.002264031417629061, 0.0037137450174971796, 0.0019139624588812392, 0.001890206500926676, 0.0018801237310981378, 0.0018045033551364515, 0.001822644500255895, 0.0017244318102408822, 0.001890369666701493, 0.0018452328755908336, 0.0018260199188565214, 0.0018629164163333674, 0.0018781312925663467, 0.0018652706203283742, 0.0018741508974926546, 0.00672114035114646, 0.0021340723760658875, 0.0019002877282521997, 0.0019085169575798016, 0.0019112685210226725, 0.001879956980701536, 0.0018520650191931054, 0.0018467862294831623, 0.0019252862693974748, 0.0018865205396044378, 0.0018510082930636902, 0.001883132475389478, 0.0019465207733446732, 0.0018618226458784193, 0.0019075359596172348, 0.0019213051855331287, 0.0019059810826244454, 0.0019211234806183104, 0.0019177617941750214, 0.0019464191476193566, 0.001956369170026543, 0.001879080562503077, 0.001898317685117945, 0.0019292795235135902, 0.0018916573365762208, 0.00199236637854483, 0.001928648062554809, 0.002100654208334163, 0.0019084948338180159, 0.0019410927925491706, 0.001843752523806567, 0.0018956740629315998, 0.0019088971651702498, 0.0019717746909009293, 0.0019185432902304456, 0.0018880773762551446, 0.0019185068958904594, 0.0018419973542525743, 0.004315683710350034, 0.001864433812443167, 0.0018997577669021364, 0.0018895697527720283, 0.0019257152938128759, 0.0019205944166363527, 0.0017341601245182876, 0.0017822035636830453, 0.0017744859602923195, 0.001784224106813781, 0.001887426934748267, 0.002061381771151597, 0.0018601345994587366, 0.0018192686645003657, 0.001841058794525452, 0.001872173558998232, 0.0018810488157517586]
[508.4077888381267, 497.1042967147497, 492.8869423972154, 493.5789696246954, 496.82813702260563, 506.4401519061061, 505.6995953903865, 490.5382228648134, 484.169313224083, 468.42046877463855, 484.88740938062574, 506.6420458351719, 513.6854831523535, 496.4890779674781, 489.3721843727156, 505.866069590409, 504.86718656663976, 503.4237380714391, 485.51877297643875, 466.67243584671996, 486.5062284275132, 516.2439642997097, 518.8640220204456, 499.63470515606383, 502.12441725936554, 512.9123158441457, 516.9537470465026, 503.0450081360988, 491.6948726531306, 464.4930646996898, 487.45311085179037, 492.4453849948756, 507.1594980774688, 503.4122414973227, 497.3716548119001, 526.0636233047118, 488.79166321107726, 534.2431539127039, 519.8255305071675, 519.0230748631434, 526.9517826921686, 526.6277101677455, 15.58309075330615, 502.44686000325083, 483.8908730089661, 493.68564439182677, 494.54520162404566, 484.2649576365089, 497.61707338917114, 488.59232646297943, 225.4509123058391, 490.1973434257805, 499.4146703108011, 493.4077255538026, 484.99199169815887, 490.2962229092704, 488.63211273415516, 497.8584766505648, 493.44725173661556, 488.1827931744235, 493.6326034433814, 486.69335140992797, 489.89247787679426, 502.3429222767384, 499.20215287884, 501.22449705243906, 503.95041660054693, 495.14274573510113, 501.80269560050465, 498.0795774562253, 482.5000758506762, 497.7781665475373, 487.5596422209112, 504.17369424978256, 502.4211210237287, 494.03488738002187, 505.49071751600053, 508.5333871558926, 499.93738591168216, 487.5570465641039, 505.29857084471, 489.29393902678027, 503.1562726200746, 492.2661177377175, 498.1164130838643, 492.0899506921437, 502.0526884303753, 500.2478320246004, 487.6901971584349, 499.86425621802033, 500.9929009654507, 503.9537062404599, 491.8298771256564, 505.9813130329358, 496.0927381476666, 501.3750354750762, 498.45766496625913, 489.66138500216726, 499.57060410893166, 496.7401180152855, 503.7938137947943, 503.1343629286086, 500.74141964460557, 501.82782081132575, 498.1469552069039, 504.7432266213373, 506.4576653722007, 205.17963337923084, 503.22519100285854, 518.546772056568, 511.1391029459989, 517.3759715774235, 514.3916919386072, 507.3488077565406, 493.5280741803049, 487.062341464288, 499.544287438341, 491.99899695075425, 492.02277498665705, 495.03360526246405, 493.45960511211956, 488.43157864876, 509.2398241539673, 495.443572377651, 472.33488352284127, 504.80161124987916, 493.67329475095056, 505.4249789197117, 523.0450396393518, 513.2989801754159, 509.64961222223735, 542.5250988927116, 538.0503967065006, 528.0959087187169, 539.7812984709273, 526.0002565800077, 534.7918146861341, 541.8048969948333, 546.3536702080189, 542.9177232372823, 539.7178465379382, 536.4875355436374, 572.8053079526223, 525.2690715806266, 569.9099666572171, 574.1512341716945, 553.7073967620609, 564.3751912887922, 536.9252058220244, 570.6459059212854, 220.21390056203404, 539.7889629382008, 518.1832618827316, 526.3603511236316, 532.4024824121674, 519.7181436670924, 538.3395958027901, 249.31958650251775, 536.218214057974, 524.6304066597037, 538.535306194934, 514.9255634423599, 523.4934235051807, 536.0771035210685, 524.380151377796, 537.4964783732863, 534.5238600017518, 325.95870598431253, 528.5122613844587, 517.0936051811918, 476.79806228408256, 227.10286664984338, 536.4981730511837, 513.2820481929845, 537.0609068861843, 529.8318525487534, 524.819208696382, 533.0419303946873, 535.7969616635048, 537.3393375675245, 516.3000507381421, 543.8055705773301, 552.9649091295996, 547.5972468717233, 535.4053627441884, 544.1795557821131, 249.33972040117155, 544.1022403648527, 551.956088745301, 548.6300282945406, 555.439908600744, 536.47792549678, 539.3634605271603, 525.0120339297439, 540.1258439894694, 529.5721233869489, 525.3990607424482, 522.3524585476146, 523.6843242937215, 522.9454145641055, 531.4595864237407, 528.6090678172658, 531.3695535468476, 533.3675849835106, 519.4555549549435, 551.3130640199176, 540.5791234404817, 528.4235572777056, 547.5251009773353, 536.5178394642579, 540.7451468180695, 524.2231094201887, 538.3889036341196, 530.440153277666, 534.1047453733723, 527.5428217706461, 535.368520432177, 526.3725195211146, 526.8871030178819, 532.4549668497549, 530.9971435110898, 533.9346351813688, 470.32118745402136, 519.9552066105076, 512.8591913636345, 536.6686700545532, 517.1998719262253, 537.5911552358513, 560.6095650712033, 542.7006634420753, 452.97301667696604, 566.9587413071501, 552.8052671664856, 570.2789822447203, 563.015530622977, 569.8451128421596, 551.7235289631327, 587.4907468323378, 551.2284991654648, 524.2962522115191, 540.2952051506542, 549.7058925156485, 534.1401450353502, 514.1879562877172, 525.008940360017, 531.8211158428093, 530.3025654845312, 537.110999246482, 539.3504117868149, 529.5619359853823, 539.3402234265058, 529.0557756613274, 527.8015022084218, 535.1481948761256, 535.4919941600688, 519.2690733642593, 515.8020382510656, 526.044450215912, 492.8958172238692, 510.461405938691, 293.3113244876944, 548.8183257392828, 549.7000343671249, 529.9097045016373, 528.6894574811527, 550.6310095590251, 529.1785485276856, 214.77351236757514, 540.6212248823168, 523.1788516588338, 537.411182978202, 537.0413171279293, 517.435960561744, 531.1569397311389, 523.794134870723, 537.7501362163368, 553.7808824863362, 552.6140880517895, 558.8531052104435, 556.658527955748, 502.09946239149525, 534.7398196716706, 17.35290205891284, 542.8588080645318, 538.3244553199207, 540.9097248503348, 536.2966204057628, 521.6284457758436, 560.0240655154087, 558.4387451702661, 560.9280769267561, 558.0938574871881, 559.9203384010773, 541.2260322396752, 546.947922380192, 528.272325773529, 518.9422655855604, 222.3128008034704, 576.9904014117325, 573.1330050648236, 576.6215750138981, 575.7911473943813, 578.5652062055443, 566.6460393354834, 574.2247411850299, 598.2561196775374, 554.481959199086, 558.8748855188043, 535.1948387736509, 550.5412040150388, 553.3907175842138, 548.7923134247213, 221.45871294798786, 537.037603266077, 543.6440722564025, 537.2607138357705, 538.1256301545649, 540.1346494392409, 534.8356998014182, 231.1886283650604, 541.3143961696682, 558.7384571204176, 585.5173059233714, 492.9609406145363, 520.4658046119717, 544.4194670693638, 542.828816920456, 593.5655495682946, 579.17360590437, 499.7826865115416, 545.625687728752, 542.205035230147, 543.7111590050316, 545.7635909115836, 537.4719750277399, 531.0503840515762, 537.6336321125818, 514.7666433199639, 542.8073228730402, 545.5346515090251, 523.3026393996096, 518.3825916748087, 531.2998484500484, 533.9621393406776, 534.4947307676573, 540.7287114205284, 509.5219082606777, 535.7727299848528, 516.2723445704394, 548.2521643440182, 515.5005661960165, 544.2163546163365, 525.1865225830485, 528.9482466459812, 516.203908827119, 527.9239186392574, 553.6393354612374, 498.9087925612268, 534.8120055542216, 527.8545379129108, 474.69699799111567, 534.536687726283, 550.5643330268513, 547.2978873999983, 547.8894467506256, 547.8670342520329, 537.2381058510038, 547.4287104975837, 529.5997501634744, 542.0496729257728, 543.0582089339224, 514.8377969967871, 514.7945471006175, 509.0317332048175, 517.4401361064658, 510.6945765623912, 519.5534591717388, 540.364075867673, 528.7310004873385, 556.636990046258, 558.3185549931973, 524.5406443525065, 545.5526276724136, 544.6189649063687, 460.4318712492613, 456.7791266975979, 457.6214732120754, 452.0885544102049, 447.9450857208962, 448.07682122597623, 233.3279205219836, 525.5859908606947, 522.8821849913429, 512.0372990121874, 527.3249287102605, 544.8199917478277, 512.4422202033485, 553.6988897245556, 484.9765186936194, 496.57926209667477, 520.1319576145338, 520.921046180859, 546.9604144361442, 551.9539535815506, 548.7771861173233, 452.0427428086454, 454.74788187749124, 338.3234677736812, 461.0166347723586, 527.4930417979704, 441.68998372258466, 269.26996745563713, 522.4762875362384, 529.042726024775, 531.879888253909, 554.1690998542824, 548.6533440062515, 579.901155882941, 528.9970621169035, 541.9370168547455, 547.6391520560267, 536.7927359662339, 532.4441395327393, 536.1152366319658, 533.5749652484529, 148.78427584530723, 468.58766891658877, 526.2360984248188, 523.9670499276591, 523.2127191970518, 531.9270654942509, 539.9378475576807, 541.4811871755497, 519.4032783046614, 530.0763914341893, 540.2460938437252, 531.0300858112361, 513.7371322689339, 537.1080871820602, 524.2365130566973, 520.4795196149526, 524.664179050008, 520.528747937718, 521.441194123996, 513.7639553243652, 511.15097054327043, 532.1751605305973, 526.7822176654635, 518.3282089568893, 528.6369685801214, 501.91571729411174, 518.4979154130053, 476.0421758291236, 523.9731238881387, 515.1737226774894, 542.372138932954, 527.5168445642664, 523.8626879676949, 507.1573362893136, 521.2287911834844, 529.6393106427781, 521.2386789654244, 542.8889448138046, 231.71299546390821, 536.3558595247707, 526.3828986106279, 529.2210031055929, 519.2875619843164, 520.6721374059587, 576.6480187507359, 561.1031311897008, 563.5434837902382, 560.4677104076196, 529.8218339420771, 485.1114985077931, 537.5955053419149, 549.6714253991918, 543.1657060456661, 534.1385125292993, 531.618314009757]
Elapsed: 0.10940844646247569~0.1876485510520564
Time per graph: 0.0022452232932344963~0.003829145575551642
Speed: 511.2122381617423~66.72296463178391
Total Time: 0.0916
best val loss: 0.2089371532201767 test_score: 0.8750

Testing...
Test loss: 0.6593 score: 0.8750 time: 0.09s
test Score 0.8750
Epoch Time List: [0.32390359113924205, 0.34299500612542033, 0.3324917003046721, 0.47627019113861024, 0.33961626910604537, 0.32951786811463535, 0.332793835317716, 0.3307346219662577, 0.3515393079724163, 0.3542224336415529, 0.47403525095432997, 0.33535956079140306, 0.32491709711030126, 0.3306581990327686, 0.3330622056964785, 0.3445071878377348, 0.3262014789506793, 0.41367054800502956, 0.34783385088667274, 0.3476776168681681, 0.3543270919471979, 0.3352437671273947, 0.33578795101493597, 0.33312563481740654, 0.3307323819026351, 0.31603187089785933, 0.4353439339902252, 0.32765928702428937, 0.3281651302240789, 0.3621470171492547, 0.34862269484438, 0.345520957140252, 0.32661682390607893, 0.33125763898715377, 0.32095526810735464, 0.39740577386692166, 0.3211774800438434, 0.3170023839920759, 0.3147912989370525, 0.3122640650253743, 0.31655616289936006, 0.3198075348045677, 4.299296143930405, 2.562304158229381, 0.3451368832029402, 0.342805314110592, 0.34035500790923834, 0.34253094298765063, 0.34241154696792364, 0.34204104682430625, 0.4621546249836683, 0.3408446619287133, 0.34077978204004467, 0.34011667408049107, 0.34091463428922, 0.3444245611317456, 0.34125460172072053, 0.3382228419650346, 0.3389827539213002, 0.33677664096467197, 0.3393753420095891, 0.34569947188720107, 0.3391128007788211, 0.3400337037164718, 0.33100640890188515, 0.33776143682189286, 0.33906302601099014, 0.3312275449279696, 0.3371106411796063, 0.33830376411788166, 0.3408236540853977, 0.3391530029475689, 0.33985422900877893, 0.3350246048066765, 0.3375292031560093, 0.3348221352789551, 0.33622678532265127, 0.33007902605459094, 0.42743106791749597, 0.33788915583863854, 0.3360626841895282, 0.33452356280758977, 0.34233399108052254, 0.339049561182037, 0.3384221070446074, 0.46340674813836813, 0.3317028949968517, 0.33381571085192263, 0.33895630622282624, 0.3321765176951885, 0.3312197991181165, 0.3331626539584249, 0.41060228389687836, 0.32927021803334355, 0.33340739901177585, 0.32962098391726613, 0.33695521438494325, 0.3317767612170428, 0.3351614191196859, 0.3354660039767623, 0.36229502432979643, 0.33051829994656146, 0.3312666448764503, 0.3242580017540604, 0.33110974286682904, 0.33598613389767706, 0.3278207378461957, 0.47629070398397744, 0.3213519302662462, 0.3264328632503748, 0.3205264189746231, 0.31825281376950443, 0.3243023990653455, 0.3169220348354429, 0.3155698608607054, 0.38534224196337163, 0.3216874930076301, 0.33429869799874723, 0.33669880987145007, 0.33639391232281923, 0.33511894778348505, 0.34101971006020904, 0.43392918980680406, 0.3271343286614865, 0.33932114695198834, 0.3240863881073892, 0.3328840637113899, 0.33179003768600523, 0.3152088162023574, 0.31965532386675477, 0.35494034737348557, 0.3235209381673485, 0.32592411315999925, 0.33072059182450175, 0.33158974978141487, 0.3365276982076466, 0.4527761209756136, 0.3301186393946409, 0.3153293877840042, 0.32092120475135744, 0.3265782648231834, 0.33308053598739207, 0.3280218082945794, 0.4270755061879754, 0.32772422512061894, 0.3100573876872659, 0.32198687084019184, 0.3283818762283772, 0.32760624098591506, 0.31988131790421903, 0.45228045457042754, 0.33234027586877346, 0.33217472466640174, 0.3358404468744993, 0.33449235395528376, 0.34820872405543923, 0.33711652318015695, 0.4387564272619784, 0.33318432816304266, 0.34275889699347317, 0.32642911188304424, 0.3371368108782917, 0.3432505517266691, 0.3378248999360949, 0.32894463813863695, 0.372130777919665, 0.32948352792300284, 0.39056376786902547, 0.4259241339750588, 0.33774389210157096, 0.3481905967928469, 0.4733170629478991, 0.3362041232176125, 0.34047344396822155, 0.3363162553869188, 0.33636613795533776, 0.3362997309304774, 0.33463832968845963, 0.32749411021359265, 0.3630159520544112, 0.33507793792523444, 0.3290679920464754, 0.3215894971508533, 0.324769068043679, 0.3274985780008137, 0.32231248589232564, 0.4332725459244102, 0.3247178830206394, 0.31874710810370743, 0.3387086240109056, 0.3220013629179448, 0.328527778852731, 0.33366598444990814, 0.3373488502111286, 0.4317739347461611, 0.3360484109725803, 0.3449869309552014, 0.3407816116232425, 0.3379635368473828, 0.3460064351093024, 0.34355516685172915, 0.3426875160075724, 0.3374571811873466, 0.33726550289429724, 0.3386878278106451, 0.3357119760476053, 0.31025358685292304, 0.317489149980247, 0.34948262805119157, 0.32392584602348506, 0.3242585251573473, 0.32923433906398714, 0.33250283799134195, 0.3332466690335423, 0.33122852188535035, 0.32865083892829716, 0.3303742047864944, 0.3356571369804442, 0.34962259232997894, 0.33030671812593937, 0.3394518350251019, 0.33267776970751584, 0.35069771902635694, 0.34331049118191004, 0.3448247108608484, 0.3409052740316838, 0.36504536611028016, 0.3361939820460975, 0.33910695603117347, 0.32238716376014054, 0.33542248723097146, 0.3197525821160525, 0.3174533969722688, 0.32508848886936903, 0.32324400427751243, 0.3263009008951485, 0.3253643657080829, 0.3256547567434609, 0.31273894384503365, 0.45504478365182877, 0.33205064688809216, 0.32723521697334945, 0.32623538724146783, 0.3316008849069476, 0.3360432437621057, 0.3324108808301389, 0.40630664490163326, 0.32922771107405424, 0.3294098796322942, 0.33000806742347777, 0.33003986719995737, 0.3268276851158589, 0.34469592035748065, 0.47503562015481293, 0.33171292184852064, 0.34257646184414625, 0.3391854509245604, 0.3414408278185874, 0.3433375449385494, 0.34504945506341755, 0.4256902749184519, 0.3198113536927849, 0.32352278498001397, 0.327418735018, 0.3403007409069687, 0.3320901831611991, 0.3353170801419765, 0.47000589105300605, 0.3262538639828563, 0.3362679881975055, 0.3361961890477687, 0.3403708338737488, 0.3355048808734864, 0.34593312605284154, 0.3394910569768399, 0.3658863438758999, 0.3224402950145304, 0.3189597772434354, 0.32162976008839905, 0.3180383830331266, 0.33158944197930396, 0.3279801169410348, 3.092644141986966, 3.7564801801927388, 0.32375277602113783, 0.32661993987858295, 0.32978156325407326, 0.3365049872081727, 0.34160424931906164, 0.3224554529879242, 0.36774940486066043, 0.32195021072402596, 0.32307476201094687, 0.32534219115041196, 0.32736565778031945, 0.33070900035090744, 0.34822386922314763, 0.4695049040019512, 0.31229787482880056, 0.3073857701383531, 0.3071608932223171, 0.30541261518374085, 0.3104230903554708, 0.332804667763412, 0.3106770529411733, 0.4406037519220263, 0.3246883961837739, 0.3240863550454378, 0.30763614387251437, 0.32959205796942115, 0.3237774313893169, 0.3236888849642128, 0.4572576731443405, 0.3419185511302203, 0.32696566730737686, 0.3287065520416945, 0.32581368717364967, 0.3291495409794152, 0.3310825009830296, 0.45754274795763195, 0.32586996094323695, 0.3220526701770723, 0.30428895307704806, 0.3200042729731649, 0.32890007086098194, 0.32636155979707837, 0.32695911219343543, 0.4316034591756761, 0.30272917239926755, 0.3329490809701383, 0.3263995472807437, 0.3211449671071023, 0.3257106109522283, 0.32481007277965546, 0.3228812350425869, 0.32318761898204684, 0.3234130886849016, 0.32661673007532954, 0.32228870200924575, 0.3152725773397833, 0.470361104933545, 0.3336175000295043, 0.48512651794590056, 0.3357110656797886, 0.3284251398872584, 0.32708286982961, 0.3398152547888458, 0.3316451741848141, 0.3386916641611606, 0.3305304639507085, 0.35126079781912267, 0.3290147287771106, 0.3300356299150735, 0.33585688192397356, 0.3365035953465849, 0.33584743668325245, 0.3319880720227957, 0.3345688027329743, 0.33463664492592216, 0.32944453903473914, 0.3362939858343452, 0.3237429007422179, 0.33080744137987494, 0.33004399901255965, 0.32348801265470684, 0.3235394600778818, 0.324242192087695, 0.33785908692516387, 0.3236980999354273, 0.32766753691248596, 0.3310710093937814, 0.32687567616812885, 0.3357741078361869, 0.34022365091368556, 0.33727238909341395, 0.33782264101319015, 0.33757745986804366, 0.31879531498998404, 0.32419603667221963, 0.310469415737316, 0.30929071083664894, 0.3292619821149856, 0.3285962857771665, 0.31972694117575884, 0.41852635191753507, 0.3912481698207557, 0.3700710830744356, 0.37833217182196677, 0.38450152869336307, 0.3840697421692312, 0.4723078201059252, 0.3307093770708889, 0.33685972006060183, 0.3372145118191838, 0.3357397629879415, 0.3148858461063355, 0.32968763494864106, 0.32548218383453786, 0.45721629494801164, 0.3443827359005809, 0.3520683047827333, 0.32597332284785807, 0.3186571130063385, 0.3164511399809271, 0.32116884808056056, 0.4421205671969801, 0.3683065448421985, 0.40082450094632804, 0.3968956698663533, 0.3567193157505244, 0.37708119791932404, 0.5897724898532033, 0.3430117010138929, 0.33452067407779396, 0.3268952330108732, 0.3489253087900579, 0.3258290432859212, 0.319913286017254, 0.4297686561476439, 0.31978355208411813, 0.32320131501182914, 0.32141733705066144, 0.32119113579392433, 0.3281349642202258, 0.33425913704559207, 0.57406942313537, 0.3468038709834218, 0.3412948006298393, 0.33128515095449984, 0.33293279516510665, 0.3313356910366565, 0.3241913572419435, 0.4452415171544999, 0.3305904408916831, 0.32985985558480024, 0.3200516558717936, 0.34832787117920816, 0.342579907970503, 0.3341328981332481, 0.3169886467512697, 0.47711214795708656, 0.3295845303218812, 0.32122095627710223, 0.3360130332875997, 0.3361910837702453, 0.33637487469241023, 0.33676168299280107, 0.3971003657206893, 0.3290658909827471, 0.3323633901309222, 0.34042498795315623, 0.33731130906380713, 0.3488678631838411, 0.344759623054415, 0.34754139790311456, 0.4583165089134127, 0.3296953139360994, 0.3211754250805825, 0.33457890315912664, 0.3419640848878771, 0.33093329332768917, 0.32967778970487416, 0.3228275820147246, 0.44065198209136724, 0.3271288152318448, 0.3312639358919114, 0.33160418504849076, 0.3323582720477134, 0.3426081808283925, 0.31384582072496414, 0.308603391982615, 0.4223741521127522, 0.3120994879864156, 0.31935371295548975, 0.3376220711506903, 0.3173539601266384, 0.3215380599722266, 0.3153677028603852, 0.44075228087604046, 0.3224477260373533]
Total Epoch List: [131, 203, 146]
Total Time List: [0.0967547248583287, 0.09079011902213097, 0.09155810810625553]
========================training times:2========================
========================k_idx:0========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: SGFormer
SGFormer(
  (trans_conv): SGModule(
    (attns): ModuleList(
      (0-1): 2 x SGFormerAttention(heads=1, head_channels=64)
    )
    (fcs): ModuleList(
      (0): Linear(in_features=14887, out_features=64, bias=True)
    )
    (bns): ModuleList(
      (0-2): 3 x LayerNorm((64,), eps=1e-05, elementwise_affine=True)
    )
  )
  (graph_conv): GraphModule(
    (convs): ModuleList(
      (0-2): 3 x GCNConv(64, 64)
    )
    (fcs): ModuleList(
      (0): Linear(in_features=14887, out_features=64, bias=True)
    )
    (bns): ModuleList(
      (0-3): 4 x BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (classifier): Linear(in_features=64, out_features=2, bias=True)
)
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7e09d8838190>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.7230;  Loss pred: 0.7230; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6977 score: 0.4898 time: 0.10s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7016 score: 0.5102 time: 0.09s
Epoch 2/1000, LR 0.000000
Train loss: 0.6983;  Loss pred: 0.6983; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6974 score: 0.4898 time: 0.09s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7013 score: 0.5102 time: 0.09s
Epoch 3/1000, LR 0.000030
Train loss: 0.7068;  Loss pred: 0.7068; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6966 score: 0.4898 time: 0.09s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7006 score: 0.5102 time: 0.09s
Epoch 4/1000, LR 0.000060
Train loss: 0.6891;  Loss pred: 0.6891; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6947 score: 0.4898 time: 0.09s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6991 score: 0.5102 time: 0.08s
Epoch 5/1000, LR 0.000090
Train loss: 0.6888;  Loss pred: 0.6888; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6917 score: 0.4898 time: 0.09s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6968 score: 0.5102 time: 0.14s
Epoch 6/1000, LR 0.000120
Train loss: 0.6663;  Loss pred: 0.6663; Loss self: 0.0000; time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6875 score: 0.4898 time: 0.10s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6935 score: 0.5102 time: 0.09s
Epoch 7/1000, LR 0.000150
Train loss: 0.6696;  Loss pred: 0.6696; Loss self: 0.0000; time: 0.13s
Val loss: 0.6826 score: 0.5306 time: 0.09s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6897 score: 0.5102 time: 0.09s
Epoch 8/1000, LR 0.000180
Train loss: 0.6450;  Loss pred: 0.6450; Loss self: 0.0000; time: 0.14s
Val loss: 0.6773 score: 0.5306 time: 0.09s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6854 score: 0.5102 time: 0.09s
Epoch 9/1000, LR 0.000210
Train loss: 0.6273;  Loss pred: 0.6273; Loss self: 0.0000; time: 0.14s
Val loss: 0.6715 score: 0.5306 time: 0.09s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6808 score: 0.5102 time: 0.09s
Epoch 10/1000, LR 0.000240
Train loss: 0.6273;  Loss pred: 0.6273; Loss self: 0.0000; time: 0.14s
Val loss: 0.6648 score: 0.5306 time: 0.10s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6753 score: 0.5102 time: 0.09s
Epoch 11/1000, LR 0.000270
Train loss: 0.6189;  Loss pred: 0.6189; Loss self: 0.0000; time: 0.14s
Val loss: 0.6583 score: 0.6122 time: 0.10s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6697 score: 0.5102 time: 0.09s
Epoch 12/1000, LR 0.000270
Train loss: 0.5769;  Loss pred: 0.5769; Loss self: 0.0000; time: 0.14s
Val loss: 0.6518 score: 0.6531 time: 0.09s
Test loss: 0.6640 score: 0.5714 time: 0.09s
Epoch 13/1000, LR 0.000270
Train loss: 0.5964;  Loss pred: 0.5964; Loss self: 0.0000; time: 0.15s
Val loss: 0.6453 score: 0.6939 time: 0.20s
Test loss: 0.6580 score: 0.6735 time: 0.11s
Epoch 14/1000, LR 0.000270
Train loss: 0.5678;  Loss pred: 0.5678; Loss self: 0.0000; time: 0.14s
Val loss: 0.6389 score: 0.7143 time: 0.09s
Test loss: 0.6522 score: 0.7347 time: 0.09s
Epoch 15/1000, LR 0.000270
Train loss: 0.5517;  Loss pred: 0.5517; Loss self: 0.0000; time: 0.14s
Val loss: 0.6323 score: 0.7143 time: 0.11s
Test loss: 0.6465 score: 0.7551 time: 0.09s
Epoch 16/1000, LR 0.000270
Train loss: 0.5302;  Loss pred: 0.5302; Loss self: 0.0000; time: 0.14s
Val loss: 0.6255 score: 0.7347 time: 0.09s
Test loss: 0.6406 score: 0.7551 time: 0.09s
Epoch 17/1000, LR 0.000270
Train loss: 0.5504;  Loss pred: 0.5504; Loss self: 0.0000; time: 0.14s
Val loss: 0.6182 score: 0.7959 time: 0.11s
Test loss: 0.6342 score: 0.7755 time: 0.09s
Epoch 18/1000, LR 0.000270
Train loss: 0.5036;  Loss pred: 0.5036; Loss self: 0.0000; time: 0.14s
Val loss: 0.6106 score: 0.7959 time: 0.09s
Test loss: 0.6275 score: 0.7755 time: 0.10s
Epoch 19/1000, LR 0.000270
Train loss: 0.5022;  Loss pred: 0.5022; Loss self: 0.0000; time: 0.14s
Val loss: 0.6028 score: 0.8163 time: 0.09s
Test loss: 0.6206 score: 0.7959 time: 0.09s
Epoch 20/1000, LR 0.000270
Train loss: 0.4822;  Loss pred: 0.4822; Loss self: 0.0000; time: 0.13s
Val loss: 0.5952 score: 0.8163 time: 0.10s
Test loss: 0.6137 score: 0.7959 time: 0.24s
Epoch 21/1000, LR 0.000270
Train loss: 0.4789;  Loss pred: 0.4789; Loss self: 0.0000; time: 0.14s
Val loss: 0.5880 score: 0.8163 time: 0.11s
Test loss: 0.6072 score: 0.7755 time: 0.10s
Epoch 22/1000, LR 0.000270
Train loss: 0.4481;  Loss pred: 0.4481; Loss self: 0.0000; time: 0.14s
Val loss: 0.5812 score: 0.7755 time: 0.11s
Test loss: 0.6009 score: 0.7755 time: 0.09s
Epoch 23/1000, LR 0.000270
Train loss: 0.4423;  Loss pred: 0.4423; Loss self: 0.0000; time: 0.13s
Val loss: 0.5748 score: 0.7551 time: 0.10s
Test loss: 0.5951 score: 0.7755 time: 0.09s
Epoch 24/1000, LR 0.000270
Train loss: 0.4318;  Loss pred: 0.4318; Loss self: 0.0000; time: 0.13s
Val loss: 0.5683 score: 0.7551 time: 0.09s
Test loss: 0.5894 score: 0.7551 time: 0.09s
Epoch 25/1000, LR 0.000270
Train loss: 0.4283;  Loss pred: 0.4283; Loss self: 0.0000; time: 0.14s
Val loss: 0.5623 score: 0.7551 time: 0.09s
Test loss: 0.5840 score: 0.7755 time: 0.09s
Epoch 26/1000, LR 0.000270
Train loss: 0.4082;  Loss pred: 0.4082; Loss self: 0.0000; time: 0.13s
Val loss: 0.5572 score: 0.7347 time: 0.09s
Test loss: 0.5790 score: 0.7347 time: 0.09s
Epoch 27/1000, LR 0.000270
Train loss: 0.4027;  Loss pred: 0.4027; Loss self: 0.0000; time: 0.14s
Val loss: 0.5524 score: 0.7143 time: 0.09s
Test loss: 0.5744 score: 0.7143 time: 0.09s
Epoch 28/1000, LR 0.000270
Train loss: 0.3966;  Loss pred: 0.3966; Loss self: 0.0000; time: 0.14s
Val loss: 0.5480 score: 0.7347 time: 0.52s
Test loss: 0.5701 score: 0.7143 time: 2.31s
Epoch 29/1000, LR 0.000270
Train loss: 0.3861;  Loss pred: 0.3861; Loss self: 0.0000; time: 1.43s
Val loss: 0.5439 score: 0.7347 time: 2.10s
Test loss: 0.5660 score: 0.7143 time: 0.09s
Epoch 30/1000, LR 0.000270
Train loss: 0.3745;  Loss pred: 0.3745; Loss self: 0.0000; time: 0.14s
Val loss: 0.5394 score: 0.7347 time: 0.09s
Test loss: 0.5616 score: 0.7143 time: 0.09s
Epoch 31/1000, LR 0.000270
Train loss: 0.3582;  Loss pred: 0.3582; Loss self: 0.0000; time: 0.14s
Val loss: 0.5350 score: 0.7347 time: 0.09s
Test loss: 0.5570 score: 0.7143 time: 0.09s
Epoch 32/1000, LR 0.000270
Train loss: 0.3721;  Loss pred: 0.3721; Loss self: 0.0000; time: 0.14s
Val loss: 0.5311 score: 0.7347 time: 0.09s
Test loss: 0.5529 score: 0.7347 time: 0.09s
Epoch 33/1000, LR 0.000270
Train loss: 0.3377;  Loss pred: 0.3377; Loss self: 0.0000; time: 0.14s
Val loss: 0.5268 score: 0.7347 time: 0.09s
Test loss: 0.5485 score: 0.7551 time: 0.09s
Epoch 34/1000, LR 0.000270
Train loss: 0.3484;  Loss pred: 0.3484; Loss self: 0.0000; time: 0.14s
Val loss: 0.5216 score: 0.7347 time: 0.09s
Test loss: 0.5436 score: 0.7755 time: 0.09s
Epoch 35/1000, LR 0.000270
Train loss: 0.3341;  Loss pred: 0.3341; Loss self: 0.0000; time: 0.14s
Val loss: 0.5156 score: 0.7347 time: 0.09s
Test loss: 0.5382 score: 0.7551 time: 0.09s
Epoch 36/1000, LR 0.000270
Train loss: 0.3327;  Loss pred: 0.3327; Loss self: 0.0000; time: 0.14s
Val loss: 0.5091 score: 0.7347 time: 0.10s
Test loss: 0.5320 score: 0.7551 time: 0.09s
Epoch 37/1000, LR 0.000270
Train loss: 0.2995;  Loss pred: 0.2995; Loss self: 0.0000; time: 0.14s
Val loss: 0.5025 score: 0.7347 time: 0.09s
Test loss: 0.5256 score: 0.7551 time: 0.09s
Epoch 38/1000, LR 0.000270
Train loss: 0.2970;  Loss pred: 0.2970; Loss self: 0.0000; time: 0.14s
Val loss: 0.4969 score: 0.7347 time: 0.09s
Test loss: 0.5203 score: 0.7755 time: 0.09s
Epoch 39/1000, LR 0.000269
Train loss: 0.2987;  Loss pred: 0.2987; Loss self: 0.0000; time: 0.14s
Val loss: 0.4924 score: 0.7347 time: 0.09s
Test loss: 0.5159 score: 0.7755 time: 0.09s
Epoch 40/1000, LR 0.000269
Train loss: 0.2976;  Loss pred: 0.2976; Loss self: 0.0000; time: 0.14s
Val loss: 0.4878 score: 0.7347 time: 0.09s
Test loss: 0.5115 score: 0.7755 time: 0.09s
Epoch 41/1000, LR 0.000269
Train loss: 0.2958;  Loss pred: 0.2958; Loss self: 0.0000; time: 0.14s
Val loss: 0.4822 score: 0.7347 time: 0.10s
Test loss: 0.5061 score: 0.7755 time: 0.09s
Epoch 42/1000, LR 0.000269
Train loss: 0.2657;  Loss pred: 0.2657; Loss self: 0.0000; time: 0.14s
Val loss: 0.4763 score: 0.7551 time: 0.10s
Test loss: 0.5004 score: 0.7755 time: 0.09s
Epoch 43/1000, LR 0.000269
Train loss: 0.2738;  Loss pred: 0.2738; Loss self: 0.0000; time: 0.15s
Val loss: 0.4706 score: 0.7551 time: 0.10s
Test loss: 0.4949 score: 0.7959 time: 0.09s
Epoch 44/1000, LR 0.000269
Train loss: 0.2672;  Loss pred: 0.2672; Loss self: 0.0000; time: 0.14s
Val loss: 0.4666 score: 0.7551 time: 0.09s
Test loss: 0.4908 score: 0.7959 time: 0.09s
Epoch 45/1000, LR 0.000269
Train loss: 0.2481;  Loss pred: 0.2481; Loss self: 0.0000; time: 0.14s
Val loss: 0.4621 score: 0.7755 time: 0.09s
Test loss: 0.4864 score: 0.7959 time: 0.09s
Epoch 46/1000, LR 0.000269
Train loss: 0.2413;  Loss pred: 0.2413; Loss self: 0.0000; time: 0.14s
Val loss: 0.4568 score: 0.7755 time: 0.09s
Test loss: 0.4812 score: 0.7959 time: 0.21s
Epoch 47/1000, LR 0.000269
Train loss: 0.2435;  Loss pred: 0.2435; Loss self: 0.0000; time: 0.14s
Val loss: 0.4543 score: 0.7755 time: 0.09s
Test loss: 0.4785 score: 0.7959 time: 0.08s
Epoch 48/1000, LR 0.000269
Train loss: 0.2375;  Loss pred: 0.2375; Loss self: 0.0000; time: 0.14s
Val loss: 0.4533 score: 0.7755 time: 0.09s
Test loss: 0.4770 score: 0.7959 time: 0.09s
Epoch 49/1000, LR 0.000269
Train loss: 0.2162;  Loss pred: 0.2162; Loss self: 0.0000; time: 0.14s
Val loss: 0.4519 score: 0.7551 time: 0.09s
Test loss: 0.4755 score: 0.7959 time: 0.08s
Epoch 50/1000, LR 0.000269
Train loss: 0.2311;  Loss pred: 0.2311; Loss self: 0.0000; time: 0.14s
Val loss: 0.4510 score: 0.7551 time: 0.09s
Test loss: 0.4748 score: 0.7755 time: 0.09s
Epoch 51/1000, LR 0.000269
Train loss: 0.2145;  Loss pred: 0.2145; Loss self: 0.0000; time: 0.14s
Val loss: 0.4474 score: 0.7551 time: 0.09s
Test loss: 0.4710 score: 0.7755 time: 0.08s
Epoch 52/1000, LR 0.000269
Train loss: 0.2172;  Loss pred: 0.2172; Loss self: 0.0000; time: 0.15s
Val loss: 0.4438 score: 0.7551 time: 0.09s
Test loss: 0.4669 score: 0.7755 time: 0.09s
Epoch 53/1000, LR 0.000269
Train loss: 0.2065;  Loss pred: 0.2065; Loss self: 0.0000; time: 0.14s
Val loss: 0.4388 score: 0.7755 time: 0.09s
Test loss: 0.4613 score: 0.7959 time: 0.09s
Epoch 54/1000, LR 0.000269
Train loss: 0.2054;  Loss pred: 0.2054; Loss self: 0.0000; time: 0.15s
Val loss: 0.4356 score: 0.7755 time: 0.15s
Test loss: 0.4571 score: 0.7959 time: 0.09s
Epoch 55/1000, LR 0.000269
Train loss: 0.1870;  Loss pred: 0.1870; Loss self: 0.0000; time: 0.14s
Val loss: 0.4312 score: 0.7755 time: 0.09s
Test loss: 0.4519 score: 0.8163 time: 0.09s
Epoch 56/1000, LR 0.000269
Train loss: 0.1943;  Loss pred: 0.1943; Loss self: 0.0000; time: 0.14s
Val loss: 0.4274 score: 0.7755 time: 0.09s
Test loss: 0.4476 score: 0.8163 time: 0.09s
Epoch 57/1000, LR 0.000269
Train loss: 0.1880;  Loss pred: 0.1880; Loss self: 0.0000; time: 0.13s
Val loss: 0.4245 score: 0.7755 time: 0.10s
Test loss: 0.4445 score: 0.8163 time: 0.09s
Epoch 58/1000, LR 0.000269
Train loss: 0.1937;  Loss pred: 0.1937; Loss self: 0.0000; time: 0.14s
Val loss: 0.4191 score: 0.7755 time: 0.09s
Test loss: 0.4398 score: 0.8163 time: 0.09s
Epoch 59/1000, LR 0.000268
Train loss: 0.1745;  Loss pred: 0.1745; Loss self: 0.0000; time: 0.14s
Val loss: 0.4157 score: 0.7755 time: 0.09s
Test loss: 0.4364 score: 0.8163 time: 0.09s
Epoch 60/1000, LR 0.000268
Train loss: 0.1711;  Loss pred: 0.1711; Loss self: 0.0000; time: 0.14s
Val loss: 0.4127 score: 0.7755 time: 0.09s
Test loss: 0.4335 score: 0.8163 time: 0.08s
Epoch 61/1000, LR 0.000268
Train loss: 0.1669;  Loss pred: 0.1669; Loss self: 0.0000; time: 0.14s
Val loss: 0.4084 score: 0.7755 time: 0.09s
Test loss: 0.4296 score: 0.8163 time: 0.09s
Epoch 62/1000, LR 0.000268
Train loss: 0.1567;  Loss pred: 0.1567; Loss self: 0.0000; time: 0.29s
Val loss: 0.4024 score: 0.8163 time: 0.10s
Test loss: 0.4242 score: 0.8163 time: 0.09s
Epoch 63/1000, LR 0.000268
Train loss: 0.1611;  Loss pred: 0.1611; Loss self: 0.0000; time: 0.14s
Val loss: 0.3942 score: 0.8163 time: 0.10s
Test loss: 0.4171 score: 0.8163 time: 0.09s
Epoch 64/1000, LR 0.000268
Train loss: 0.1540;  Loss pred: 0.1540; Loss self: 0.0000; time: 0.15s
Val loss: 0.3866 score: 0.8571 time: 0.11s
Test loss: 0.4106 score: 0.8163 time: 0.10s
Epoch 65/1000, LR 0.000268
Train loss: 0.1560;  Loss pred: 0.1560; Loss self: 0.0000; time: 0.15s
Val loss: 0.3825 score: 0.8571 time: 0.10s
Test loss: 0.4071 score: 0.8163 time: 0.09s
Epoch 66/1000, LR 0.000268
Train loss: 0.1560;  Loss pred: 0.1560; Loss self: 0.0000; time: 0.15s
Val loss: 0.3788 score: 0.8571 time: 0.10s
Test loss: 0.4035 score: 0.8163 time: 0.09s
Epoch 67/1000, LR 0.000268
Train loss: 0.1471;  Loss pred: 0.1471; Loss self: 0.0000; time: 0.15s
Val loss: 0.3756 score: 0.8571 time: 0.10s
Test loss: 0.3996 score: 0.8163 time: 0.10s
Epoch 68/1000, LR 0.000268
Train loss: 0.1408;  Loss pred: 0.1408; Loss self: 0.0000; time: 0.14s
Val loss: 0.3713 score: 0.8571 time: 0.10s
Test loss: 0.3948 score: 0.8163 time: 0.09s
Epoch 69/1000, LR 0.000268
Train loss: 0.1472;  Loss pred: 0.1472; Loss self: 0.0000; time: 0.15s
Val loss: 0.3677 score: 0.8571 time: 0.14s
Test loss: 0.3905 score: 0.8163 time: 0.09s
Epoch 70/1000, LR 0.000268
Train loss: 0.1345;  Loss pred: 0.1345; Loss self: 0.0000; time: 0.14s
Val loss: 0.3658 score: 0.8571 time: 0.10s
Test loss: 0.3874 score: 0.8163 time: 0.10s
Epoch 71/1000, LR 0.000268
Train loss: 0.1611;  Loss pred: 0.1611; Loss self: 0.0000; time: 0.15s
Val loss: 0.3652 score: 0.8571 time: 0.10s
Test loss: 0.3863 score: 0.8163 time: 0.09s
Epoch 72/1000, LR 0.000267
Train loss: 0.1352;  Loss pred: 0.1352; Loss self: 0.0000; time: 0.14s
Val loss: 0.3613 score: 0.8571 time: 0.10s
Test loss: 0.3835 score: 0.8163 time: 0.09s
Epoch 73/1000, LR 0.000267
Train loss: 0.1334;  Loss pred: 0.1334; Loss self: 0.0000; time: 0.15s
Val loss: 0.3563 score: 0.8571 time: 0.10s
Test loss: 0.3792 score: 0.8163 time: 0.09s
Epoch 74/1000, LR 0.000267
Train loss: 0.1281;  Loss pred: 0.1281; Loss self: 0.0000; time: 0.15s
Val loss: 0.3447 score: 0.8980 time: 0.10s
Test loss: 0.3706 score: 0.8571 time: 0.09s
Epoch 75/1000, LR 0.000267
Train loss: 0.1242;  Loss pred: 0.1242; Loss self: 0.0000; time: 0.15s
Val loss: 0.3349 score: 0.8980 time: 0.10s
Test loss: 0.3628 score: 0.8571 time: 0.09s
Epoch 76/1000, LR 0.000267
Train loss: 0.1220;  Loss pred: 0.1220; Loss self: 0.0000; time: 0.15s
Val loss: 0.3254 score: 0.8980 time: 0.10s
Test loss: 0.3549 score: 0.8980 time: 0.22s
Epoch 77/1000, LR 0.000267
Train loss: 0.1143;  Loss pred: 0.1143; Loss self: 0.0000; time: 0.14s
Val loss: 0.3127 score: 0.8980 time: 0.09s
Test loss: 0.3451 score: 0.8980 time: 0.09s
Epoch 78/1000, LR 0.000267
Train loss: 0.1077;  Loss pred: 0.1077; Loss self: 0.0000; time: 0.14s
Val loss: 0.3018 score: 0.9184 time: 0.09s
Test loss: 0.3368 score: 0.8980 time: 0.09s
Epoch 79/1000, LR 0.000267
Train loss: 0.1139;  Loss pred: 0.1139; Loss self: 0.0000; time: 0.14s
Val loss: 0.3011 score: 0.9184 time: 0.09s
Test loss: 0.3343 score: 0.8980 time: 0.09s
Epoch 80/1000, LR 0.000267
Train loss: 0.1093;  Loss pred: 0.1093; Loss self: 0.0000; time: 0.14s
Val loss: 0.3015 score: 0.8980 time: 0.09s
Test loss: 0.3326 score: 0.8980 time: 0.10s
     INFO: Early stopping counter 1 of 20
Epoch 81/1000, LR 0.000267
Train loss: 0.1063;  Loss pred: 0.1063; Loss self: 0.0000; time: 0.13s
Val loss: 0.3042 score: 0.8980 time: 0.09s
Test loss: 0.3329 score: 0.8980 time: 0.09s
     INFO: Early stopping counter 2 of 20
Epoch 82/1000, LR 0.000267
Train loss: 0.0990;  Loss pred: 0.0990; Loss self: 0.0000; time: 0.14s
Val loss: 0.3001 score: 0.8980 time: 0.09s
Test loss: 0.3294 score: 0.8980 time: 0.09s
Epoch 83/1000, LR 0.000266
Train loss: 0.1002;  Loss pred: 0.1002; Loss self: 0.0000; time: 0.14s
Val loss: 0.2956 score: 0.8980 time: 0.09s
Test loss: 0.3253 score: 0.8980 time: 0.09s
Epoch 84/1000, LR 0.000266
Train loss: 0.1006;  Loss pred: 0.1006; Loss self: 0.0000; time: 0.25s
Val loss: 0.2880 score: 0.8980 time: 0.10s
Test loss: 0.3188 score: 0.8980 time: 0.26s
Epoch 85/1000, LR 0.000266
Train loss: 0.0964;  Loss pred: 0.0964; Loss self: 0.0000; time: 0.14s
Val loss: 0.2793 score: 0.8980 time: 0.10s
Test loss: 0.3114 score: 0.8980 time: 0.09s
Epoch 86/1000, LR 0.000266
Train loss: 0.0915;  Loss pred: 0.0915; Loss self: 0.0000; time: 0.14s
Val loss: 0.2658 score: 0.9184 time: 0.10s
Test loss: 0.3016 score: 0.8980 time: 0.09s
Epoch 87/1000, LR 0.000266
Train loss: 0.0915;  Loss pred: 0.0915; Loss self: 0.0000; time: 0.14s
Val loss: 0.2545 score: 0.9184 time: 0.09s
Test loss: 0.2943 score: 0.9184 time: 0.09s
Epoch 88/1000, LR 0.000266
Train loss: 0.0925;  Loss pred: 0.0925; Loss self: 0.0000; time: 1.47s
Val loss: 0.2478 score: 0.9184 time: 0.56s
Test loss: 0.2898 score: 0.8980 time: 1.85s
Epoch 89/1000, LR 0.000266
Train loss: 0.0879;  Loss pred: 0.0879; Loss self: 0.0000; time: 0.14s
Val loss: 0.2408 score: 0.9184 time: 0.10s
Test loss: 0.2850 score: 0.8980 time: 0.09s
Epoch 90/1000, LR 0.000266
Train loss: 0.0918;  Loss pred: 0.0918; Loss self: 0.0000; time: 0.15s
Val loss: 0.2308 score: 0.9388 time: 0.09s
Test loss: 0.2789 score: 0.8980 time: 0.09s
Epoch 91/1000, LR 0.000266
Train loss: 0.0875;  Loss pred: 0.0875; Loss self: 0.0000; time: 0.15s
Val loss: 0.2228 score: 0.9388 time: 0.09s
Test loss: 0.2740 score: 0.8980 time: 0.09s
Epoch 92/1000, LR 0.000266
Train loss: 0.0804;  Loss pred: 0.0804; Loss self: 0.0000; time: 0.14s
Val loss: 0.2187 score: 0.9388 time: 0.09s
Test loss: 0.2705 score: 0.8980 time: 0.09s
Epoch 93/1000, LR 0.000265
Train loss: 0.0873;  Loss pred: 0.0873; Loss self: 0.0000; time: 0.14s
Val loss: 0.2201 score: 0.9388 time: 0.10s
Test loss: 0.2705 score: 0.8980 time: 0.09s
     INFO: Early stopping counter 1 of 20
Epoch 94/1000, LR 0.000265
Train loss: 0.0764;  Loss pred: 0.0764; Loss self: 0.0000; time: 0.14s
Val loss: 0.2202 score: 0.9184 time: 0.10s
Test loss: 0.2699 score: 0.8980 time: 0.09s
     INFO: Early stopping counter 2 of 20
Epoch 95/1000, LR 0.000265
Train loss: 0.0729;  Loss pred: 0.0729; Loss self: 0.0000; time: 0.14s
Val loss: 0.2151 score: 0.9184 time: 0.23s
Test loss: 0.2668 score: 0.8980 time: 0.09s
Epoch 96/1000, LR 0.000265
Train loss: 0.0749;  Loss pred: 0.0749; Loss self: 0.0000; time: 0.14s
Val loss: 0.2062 score: 0.9388 time: 0.09s
Test loss: 0.2610 score: 0.8980 time: 0.09s
Epoch 97/1000, LR 0.000265
Train loss: 0.0772;  Loss pred: 0.0772; Loss self: 0.0000; time: 0.15s
Val loss: 0.1945 score: 0.9388 time: 0.09s
Test loss: 0.2546 score: 0.8980 time: 0.09s
Epoch 98/1000, LR 0.000265
Train loss: 0.0759;  Loss pred: 0.0759; Loss self: 0.0000; time: 0.14s
Val loss: 0.1829 score: 0.9592 time: 0.09s
Test loss: 0.2493 score: 0.8980 time: 0.09s
Epoch 99/1000, LR 0.000265
Train loss: 0.0781;  Loss pred: 0.0781; Loss self: 0.0000; time: 0.14s
Val loss: 0.1747 score: 0.9592 time: 0.09s
Test loss: 0.2460 score: 0.8980 time: 0.09s
Epoch 100/1000, LR 0.000265
Train loss: 0.0698;  Loss pred: 0.0698; Loss self: 0.0000; time: 0.14s
Val loss: 0.1725 score: 0.9592 time: 0.09s
Test loss: 0.2444 score: 0.8980 time: 0.09s
Epoch 101/1000, LR 0.000265
Train loss: 0.0666;  Loss pred: 0.0666; Loss self: 0.0000; time: 0.13s
Val loss: 0.1715 score: 0.9592 time: 0.09s
Test loss: 0.2429 score: 0.8980 time: 0.08s
Epoch 102/1000, LR 0.000264
Train loss: 0.0633;  Loss pred: 0.0633; Loss self: 0.0000; time: 0.14s
Val loss: 0.1703 score: 0.9592 time: 0.09s
Test loss: 0.2418 score: 0.8980 time: 0.12s
Epoch 103/1000, LR 0.000264
Train loss: 0.0820;  Loss pred: 0.0820; Loss self: 0.0000; time: 0.25s
Val loss: 0.1660 score: 0.9592 time: 0.10s
Test loss: 0.2394 score: 0.8980 time: 0.09s
Epoch 104/1000, LR 0.000264
Train loss: 0.0656;  Loss pred: 0.0656; Loss self: 0.0000; time: 0.14s
Val loss: 0.1579 score: 0.9592 time: 0.09s
Test loss: 0.2361 score: 0.9184 time: 0.09s
Epoch 105/1000, LR 0.000264
Train loss: 0.0664;  Loss pred: 0.0664; Loss self: 0.0000; time: 0.14s
Val loss: 0.1487 score: 0.9592 time: 0.09s
Test loss: 0.2340 score: 0.9184 time: 0.09s
Epoch 106/1000, LR 0.000264
Train loss: 0.0586;  Loss pred: 0.0586; Loss self: 0.0000; time: 0.14s
Val loss: 0.1420 score: 0.9592 time: 0.11s
Test loss: 0.2332 score: 0.9184 time: 0.10s
Epoch 107/1000, LR 0.000264
Train loss: 0.0641;  Loss pred: 0.0641; Loss self: 0.0000; time: 0.14s
Val loss: 0.1427 score: 0.9592 time: 0.10s
Test loss: 0.2314 score: 0.9184 time: 0.10s
     INFO: Early stopping counter 1 of 20
Epoch 108/1000, LR 0.000264
Train loss: 0.0605;  Loss pred: 0.0605; Loss self: 0.0000; time: 0.15s
Val loss: 0.1466 score: 0.9592 time: 0.10s
Test loss: 0.2299 score: 0.9184 time: 0.10s
     INFO: Early stopping counter 2 of 20
Epoch 109/1000, LR 0.000264
Train loss: 0.0632;  Loss pred: 0.0632; Loss self: 0.0000; time: 0.15s
Val loss: 0.1511 score: 0.9388 time: 0.10s
Test loss: 0.2295 score: 0.9184 time: 0.23s
     INFO: Early stopping counter 3 of 20
Epoch 110/1000, LR 0.000263
Train loss: 0.0638;  Loss pred: 0.0638; Loss self: 0.0000; time: 0.15s
Val loss: 0.1489 score: 0.9388 time: 0.10s
Test loss: 0.2284 score: 0.9184 time: 0.09s
     INFO: Early stopping counter 4 of 20
Epoch 111/1000, LR 0.000263
Train loss: 0.0643;  Loss pred: 0.0643; Loss self: 0.0000; time: 0.14s
Val loss: 0.1413 score: 0.9592 time: 0.09s
Test loss: 0.2267 score: 0.9184 time: 0.09s
Epoch 112/1000, LR 0.000263
Train loss: 0.0590;  Loss pred: 0.0590; Loss self: 0.0000; time: 0.14s
Val loss: 0.1307 score: 0.9592 time: 0.09s
Test loss: 0.2267 score: 0.9184 time: 0.09s
Epoch 113/1000, LR 0.000263
Train loss: 0.0650;  Loss pred: 0.0650; Loss self: 0.0000; time: 0.15s
Val loss: 0.1246 score: 0.9796 time: 0.09s
Test loss: 0.2267 score: 0.9184 time: 0.09s
Epoch 114/1000, LR 0.000263
Train loss: 0.0563;  Loss pred: 0.0563; Loss self: 0.0000; time: 0.14s
Val loss: 0.1227 score: 0.9796 time: 0.09s
Test loss: 0.2269 score: 0.9184 time: 0.09s
Epoch 115/1000, LR 0.000263
Train loss: 0.0607;  Loss pred: 0.0607; Loss self: 0.0000; time: 0.14s
Val loss: 0.1241 score: 0.9592 time: 0.10s
Test loss: 0.2260 score: 0.9184 time: 0.09s
     INFO: Early stopping counter 1 of 20
Epoch 116/1000, LR 0.000263
Train loss: 0.0591;  Loss pred: 0.0591; Loss self: 0.0000; time: 0.15s
Val loss: 0.1261 score: 0.9592 time: 0.09s
Test loss: 0.2253 score: 0.9184 time: 0.09s
     INFO: Early stopping counter 2 of 20
Epoch 117/1000, LR 0.000262
Train loss: 0.0657;  Loss pred: 0.0657; Loss self: 0.0000; time: 0.14s
Val loss: 0.1331 score: 0.9592 time: 0.09s
Test loss: 0.2246 score: 0.9184 time: 0.10s
     INFO: Early stopping counter 3 of 20
Epoch 118/1000, LR 0.000262
Train loss: 0.0527;  Loss pred: 0.0527; Loss self: 0.0000; time: 0.20s
Val loss: 0.1352 score: 0.9388 time: 0.09s
Test loss: 0.2249 score: 0.9184 time: 0.09s
     INFO: Early stopping counter 4 of 20
Epoch 119/1000, LR 0.000262
Train loss: 0.0490;  Loss pred: 0.0490; Loss self: 0.0000; time: 0.14s
Val loss: 0.1322 score: 0.9592 time: 0.09s
Test loss: 0.2244 score: 0.9184 time: 0.08s
     INFO: Early stopping counter 5 of 20
Epoch 120/1000, LR 0.000262
Train loss: 0.0470;  Loss pred: 0.0470; Loss self: 0.0000; time: 0.14s
Val loss: 0.1238 score: 0.9592 time: 0.09s
Test loss: 0.2242 score: 0.9184 time: 0.08s
     INFO: Early stopping counter 6 of 20
Epoch 121/1000, LR 0.000262
Train loss: 0.0516;  Loss pred: 0.0516; Loss self: 0.0000; time: 0.14s
Val loss: 0.1145 score: 0.9796 time: 0.09s
Test loss: 0.2269 score: 0.9388 time: 0.09s
Epoch 122/1000, LR 0.000262
Train loss: 0.0465;  Loss pred: 0.0465; Loss self: 0.0000; time: 0.14s
Val loss: 0.1092 score: 0.9796 time: 0.09s
Test loss: 0.2308 score: 0.9388 time: 0.09s
Epoch 123/1000, LR 0.000262
Train loss: 0.0511;  Loss pred: 0.0511; Loss self: 0.0000; time: 0.14s
Val loss: 0.1100 score: 0.9796 time: 0.09s
Test loss: 0.2281 score: 0.9388 time: 0.09s
     INFO: Early stopping counter 1 of 20
Epoch 124/1000, LR 0.000261
Train loss: 0.0479;  Loss pred: 0.0479; Loss self: 0.0000; time: 0.14s
Val loss: 0.1161 score: 0.9592 time: 0.09s
Test loss: 0.2245 score: 0.9184 time: 0.24s
     INFO: Early stopping counter 2 of 20
Epoch 125/1000, LR 0.000261
Train loss: 0.0436;  Loss pred: 0.0436; Loss self: 0.0000; time: 0.14s
Val loss: 0.1198 score: 0.9592 time: 0.10s
Test loss: 0.2239 score: 0.9184 time: 0.09s
     INFO: Early stopping counter 3 of 20
Epoch 126/1000, LR 0.000261
Train loss: 0.0468;  Loss pred: 0.0468; Loss self: 0.0000; time: 0.14s
Val loss: 0.1227 score: 0.9592 time: 0.09s
Test loss: 0.2243 score: 0.9184 time: 0.09s
     INFO: Early stopping counter 4 of 20
Epoch 127/1000, LR 0.000261
Train loss: 0.0542;  Loss pred: 0.0542; Loss self: 0.0000; time: 0.14s
Val loss: 0.1150 score: 0.9592 time: 0.10s
Test loss: 0.2244 score: 0.9184 time: 0.09s
     INFO: Early stopping counter 5 of 20
Epoch 128/1000, LR 0.000261
Train loss: 0.0468;  Loss pred: 0.0468; Loss self: 0.0000; time: 0.14s
Val loss: 0.1013 score: 0.9796 time: 0.09s
Test loss: 0.2309 score: 0.9388 time: 0.09s
Epoch 129/1000, LR 0.000261
Train loss: 0.0390;  Loss pred: 0.0390; Loss self: 0.0000; time: 0.14s
Val loss: 0.0985 score: 0.9796 time: 0.09s
Test loss: 0.2337 score: 0.9388 time: 0.09s
Epoch 130/1000, LR 0.000260
Train loss: 0.0455;  Loss pred: 0.0455; Loss self: 0.0000; time: 0.14s
Val loss: 0.1025 score: 0.9796 time: 0.09s
Test loss: 0.2304 score: 0.9388 time: 0.09s
     INFO: Early stopping counter 1 of 20
Epoch 131/1000, LR 0.000260
Train loss: 0.0461;  Loss pred: 0.0461; Loss self: 0.0000; time: 0.15s
Val loss: 0.1101 score: 0.9592 time: 0.09s
Test loss: 0.2275 score: 0.9184 time: 0.09s
     INFO: Early stopping counter 2 of 20
Epoch 132/1000, LR 0.000260
Train loss: 0.0458;  Loss pred: 0.0458; Loss self: 0.0000; time: 0.15s
Val loss: 0.1160 score: 0.9592 time: 0.19s
Test loss: 0.2273 score: 0.9184 time: 0.09s
     INFO: Early stopping counter 3 of 20
Epoch 133/1000, LR 0.000260
Train loss: 0.0406;  Loss pred: 0.0406; Loss self: 0.0000; time: 0.14s
Val loss: 0.1156 score: 0.9592 time: 0.10s
Test loss: 0.2270 score: 0.9184 time: 0.09s
     INFO: Early stopping counter 4 of 20
Epoch 134/1000, LR 0.000260
Train loss: 0.0397;  Loss pred: 0.0397; Loss self: 0.0000; time: 0.15s
Val loss: 0.1115 score: 0.9592 time: 0.10s
Test loss: 0.2278 score: 0.9184 time: 0.09s
     INFO: Early stopping counter 5 of 20
Epoch 135/1000, LR 0.000260
Train loss: 0.0394;  Loss pred: 0.0394; Loss self: 0.0000; time: 0.15s
Val loss: 0.1059 score: 0.9592 time: 0.10s
Test loss: 0.2296 score: 0.9184 time: 0.09s
     INFO: Early stopping counter 6 of 20
Epoch 136/1000, LR 0.000260
Train loss: 0.0437;  Loss pred: 0.0437; Loss self: 0.0000; time: 0.15s
Val loss: 0.1000 score: 0.9796 time: 0.10s
Test loss: 0.2330 score: 0.9388 time: 0.09s
     INFO: Early stopping counter 7 of 20
Epoch 137/1000, LR 0.000259
Train loss: 0.0375;  Loss pred: 0.0375; Loss self: 0.0000; time: 0.14s
Val loss: 0.0945 score: 0.9796 time: 0.10s
Test loss: 0.2374 score: 0.9388 time: 0.09s
Epoch 138/1000, LR 0.000259
Train loss: 0.0396;  Loss pred: 0.0396; Loss self: 0.0000; time: 0.15s
Val loss: 0.0978 score: 0.9796 time: 0.10s
Test loss: 0.2353 score: 0.9388 time: 0.09s
     INFO: Early stopping counter 1 of 20
Epoch 139/1000, LR 0.000259
Train loss: 0.0434;  Loss pred: 0.0434; Loss self: 0.0000; time: 0.18s
Val loss: 0.1060 score: 0.9592 time: 0.10s
Test loss: 0.2313 score: 0.9184 time: 0.09s
     INFO: Early stopping counter 2 of 20
Epoch 140/1000, LR 0.000259
Train loss: 0.0326;  Loss pred: 0.0326; Loss self: 0.0000; time: 0.14s
Val loss: 0.1073 score: 0.9592 time: 0.09s
Test loss: 0.2304 score: 0.9184 time: 0.09s
     INFO: Early stopping counter 3 of 20
Epoch 141/1000, LR 0.000259
Train loss: 0.0341;  Loss pred: 0.0341; Loss self: 0.0000; time: 0.14s
Val loss: 0.1072 score: 0.9592 time: 0.09s
Test loss: 0.2303 score: 0.9184 time: 0.09s
     INFO: Early stopping counter 4 of 20
Epoch 142/1000, LR 0.000259
Train loss: 0.0345;  Loss pred: 0.0345; Loss self: 0.0000; time: 0.15s
Val loss: 0.1035 score: 0.9796 time: 0.09s
Test loss: 0.2321 score: 0.9184 time: 0.09s
     INFO: Early stopping counter 5 of 20
Epoch 143/1000, LR 0.000258
Train loss: 0.0341;  Loss pred: 0.0341; Loss self: 0.0000; time: 0.14s
Val loss: 0.0984 score: 0.9796 time: 0.10s
Test loss: 0.2346 score: 0.9388 time: 0.09s
     INFO: Early stopping counter 6 of 20
Epoch 144/1000, LR 0.000258
Train loss: 0.0339;  Loss pred: 0.0339; Loss self: 0.0000; time: 0.15s
Val loss: 0.0962 score: 0.9796 time: 0.10s
Test loss: 0.2361 score: 0.9388 time: 0.09s
     INFO: Early stopping counter 7 of 20
Epoch 145/1000, LR 0.000258
Train loss: 0.0368;  Loss pred: 0.0368; Loss self: 0.0000; time: 0.14s
Val loss: 0.0993 score: 0.9796 time: 0.09s
Test loss: 0.2345 score: 0.9184 time: 0.09s
     INFO: Early stopping counter 8 of 20
Epoch 146/1000, LR 0.000258
Train loss: 0.0337;  Loss pred: 0.0337; Loss self: 0.0000; time: 0.15s
Val loss: 0.0993 score: 0.9796 time: 0.10s
Test loss: 0.2358 score: 0.9184 time: 0.09s
     INFO: Early stopping counter 9 of 20
Epoch 147/1000, LR 0.000258
Train loss: 0.0312;  Loss pred: 0.0312; Loss self: 0.0000; time: 0.27s
Val loss: 0.0997 score: 0.9796 time: 0.09s
Test loss: 0.2358 score: 0.9184 time: 0.09s
     INFO: Early stopping counter 10 of 20
Epoch 148/1000, LR 0.000257
Train loss: 0.0326;  Loss pred: 0.0326; Loss self: 0.0000; time: 0.14s
Val loss: 0.1011 score: 0.9592 time: 0.09s
Test loss: 0.2353 score: 0.9184 time: 0.09s
     INFO: Early stopping counter 11 of 20
Epoch 149/1000, LR 0.000257
Train loss: 0.0289;  Loss pred: 0.0289; Loss self: 0.0000; time: 0.14s
Val loss: 0.1006 score: 0.9592 time: 0.09s
Test loss: 0.2355 score: 0.9184 time: 0.09s
     INFO: Early stopping counter 12 of 20
Epoch 150/1000, LR 0.000257
Train loss: 0.0284;  Loss pred: 0.0284; Loss self: 0.0000; time: 0.14s
Val loss: 0.0976 score: 0.9592 time: 0.09s
Test loss: 0.2361 score: 0.9184 time: 0.09s
     INFO: Early stopping counter 13 of 20
Epoch 151/1000, LR 0.000257
Train loss: 0.0312;  Loss pred: 0.0312; Loss self: 0.0000; time: 0.14s
Val loss: 0.0913 score: 0.9796 time: 0.09s
Test loss: 0.2356 score: 0.9388 time: 0.09s
Epoch 152/1000, LR 0.000257
Train loss: 0.0278;  Loss pred: 0.0278; Loss self: 0.0000; time: 0.14s
Val loss: 0.0859 score: 0.9796 time: 0.10s
Test loss: 0.2365 score: 0.9388 time: 0.09s
Epoch 153/1000, LR 0.000257
Train loss: 0.0274;  Loss pred: 0.0274; Loss self: 0.0000; time: 0.14s
Val loss: 0.0893 score: 0.9796 time: 0.09s
Test loss: 0.2348 score: 0.9388 time: 0.09s
     INFO: Early stopping counter 1 of 20
Epoch 154/1000, LR 0.000256
Train loss: 0.0298;  Loss pred: 0.0298; Loss self: 0.0000; time: 0.17s
Val loss: 0.0976 score: 0.9592 time: 0.10s
Test loss: 0.2333 score: 0.9184 time: 0.09s
     INFO: Early stopping counter 2 of 20
Epoch 155/1000, LR 0.000256
Train loss: 0.0293;  Loss pred: 0.0293; Loss self: 0.0000; time: 0.14s
Val loss: 0.0973 score: 0.9796 time: 0.09s
Test loss: 0.2332 score: 0.9184 time: 0.09s
     INFO: Early stopping counter 3 of 20
Epoch 156/1000, LR 0.000256
Train loss: 0.0292;  Loss pred: 0.0292; Loss self: 0.0000; time: 0.14s
Val loss: 0.0958 score: 0.9796 time: 0.09s
Test loss: 0.2348 score: 0.9184 time: 0.09s
     INFO: Early stopping counter 4 of 20
Epoch 157/1000, LR 0.000256
Train loss: 0.0266;  Loss pred: 0.0266; Loss self: 0.0000; time: 0.14s
Val loss: 0.0965 score: 0.9796 time: 0.09s
Test loss: 0.2345 score: 0.9184 time: 0.09s
     INFO: Early stopping counter 5 of 20
Epoch 158/1000, LR 0.000256
Train loss: 0.0297;  Loss pred: 0.0297; Loss self: 0.0000; time: 0.15s
Val loss: 0.0934 score: 0.9796 time: 0.09s
Test loss: 0.2356 score: 0.9184 time: 0.09s
     INFO: Early stopping counter 6 of 20
Epoch 159/1000, LR 0.000255
Train loss: 0.0269;  Loss pred: 0.0269; Loss self: 0.0000; time: 0.15s
Val loss: 0.0874 score: 0.9796 time: 0.10s
Test loss: 0.2386 score: 0.9388 time: 0.09s
     INFO: Early stopping counter 7 of 20
Epoch 160/1000, LR 0.000255
Train loss: 0.0283;  Loss pred: 0.0283; Loss self: 0.0000; time: 0.15s
Val loss: 0.0885 score: 0.9796 time: 0.09s
Test loss: 0.2368 score: 0.9184 time: 0.09s
     INFO: Early stopping counter 8 of 20
Epoch 161/1000, LR 0.000255
Train loss: 0.0279;  Loss pred: 0.0279; Loss self: 0.0000; time: 0.14s
Val loss: 0.0827 score: 0.9796 time: 0.10s
Test loss: 0.2399 score: 0.9388 time: 0.09s
Epoch 162/1000, LR 0.000255
Train loss: 0.0259;  Loss pred: 0.0259; Loss self: 0.0000; time: 0.29s
Val loss: 0.0814 score: 0.9796 time: 0.09s
Test loss: 0.2415 score: 0.9388 time: 0.09s
Epoch 163/1000, LR 0.000255
Train loss: 0.0233;  Loss pred: 0.0233; Loss self: 0.0000; time: 0.14s
Val loss: 0.0809 score: 0.9796 time: 0.09s
Test loss: 0.2429 score: 0.9388 time: 0.09s
Epoch 164/1000, LR 0.000254
Train loss: 0.0245;  Loss pred: 0.0245; Loss self: 0.0000; time: 0.14s
Val loss: 0.0807 score: 0.9796 time: 0.09s
Test loss: 0.2424 score: 0.9388 time: 0.09s
Epoch 165/1000, LR 0.000254
Train loss: 0.0288;  Loss pred: 0.0288; Loss self: 0.0000; time: 0.14s
Val loss: 0.0852 score: 0.9796 time: 0.09s
Test loss: 0.2408 score: 0.9184 time: 0.09s
     INFO: Early stopping counter 1 of 20
Epoch 166/1000, LR 0.000254
Train loss: 0.0229;  Loss pred: 0.0229; Loss self: 0.0000; time: 0.14s
Val loss: 0.0865 score: 0.9796 time: 0.09s
Test loss: 0.2407 score: 0.9184 time: 0.09s
     INFO: Early stopping counter 2 of 20
Epoch 167/1000, LR 0.000254
Train loss: 0.0207;  Loss pred: 0.0207; Loss self: 0.0000; time: 0.15s
Val loss: 0.0789 score: 0.9796 time: 0.09s
Test loss: 0.2453 score: 0.9388 time: 0.09s
Epoch 168/1000, LR 0.000254
Train loss: 0.0227;  Loss pred: 0.0227; Loss self: 0.0000; time: 0.15s
Val loss: 0.0758 score: 0.9796 time: 0.10s
Test loss: 0.2478 score: 0.9388 time: 0.09s
Epoch 169/1000, LR 0.000253
Train loss: 0.0248;  Loss pred: 0.0248; Loss self: 0.0000; time: 0.14s
Val loss: 0.0785 score: 0.9796 time: 0.09s
Test loss: 0.2455 score: 0.9388 time: 0.09s
     INFO: Early stopping counter 1 of 20
Epoch 170/1000, LR 0.000253
Train loss: 0.0232;  Loss pred: 0.0232; Loss self: 0.0000; time: 0.14s
Val loss: 0.0838 score: 0.9796 time: 0.11s
Test loss: 0.2427 score: 0.9184 time: 0.10s
     INFO: Early stopping counter 2 of 20
Epoch 171/1000, LR 0.000253
Train loss: 0.0227;  Loss pred: 0.0227; Loss self: 0.0000; time: 0.14s
Val loss: 0.0798 score: 0.9796 time: 0.10s
Test loss: 0.2433 score: 0.9388 time: 0.09s
     INFO: Early stopping counter 3 of 20
Epoch 172/1000, LR 0.000253
Train loss: 0.0249;  Loss pred: 0.0249; Loss self: 0.0000; time: 0.15s
Val loss: 0.0736 score: 0.9796 time: 0.10s
Test loss: 0.2465 score: 0.9388 time: 0.09s
Epoch 173/1000, LR 0.000253
Train loss: 0.0211;  Loss pred: 0.0211; Loss self: 0.0000; time: 0.14s
Val loss: 0.0728 score: 0.9796 time: 0.09s
Test loss: 0.2466 score: 0.9388 time: 0.09s
Epoch 174/1000, LR 0.000252
Train loss: 0.0201;  Loss pred: 0.0201; Loss self: 0.0000; time: 0.14s
Val loss: 0.0740 score: 0.9796 time: 0.09s
Test loss: 0.2452 score: 0.9388 time: 0.09s
     INFO: Early stopping counter 1 of 20
Epoch 175/1000, LR 0.000252
Train loss: 0.0196;  Loss pred: 0.0196; Loss self: 0.0000; time: 0.13s
Val loss: 0.0757 score: 0.9796 time: 0.09s
Test loss: 0.2433 score: 0.9388 time: 0.09s
     INFO: Early stopping counter 2 of 20
Epoch 176/1000, LR 0.000252
Train loss: 0.0201;  Loss pred: 0.0201; Loss self: 0.0000; time: 0.14s
Val loss: 0.0730 score: 0.9796 time: 0.09s
Test loss: 0.2455 score: 0.9388 time: 0.09s
     INFO: Early stopping counter 3 of 20
Epoch 177/1000, LR 0.000252
Train loss: 0.0252;  Loss pred: 0.0252; Loss self: 0.0000; time: 0.14s
Val loss: 0.0716 score: 0.9796 time: 0.09s
Test loss: 0.2470 score: 0.9388 time: 0.09s
Epoch 178/1000, LR 0.000251
Train loss: 0.0213;  Loss pred: 0.0213; Loss self: 0.0000; time: 0.14s
Val loss: 0.0732 score: 0.9796 time: 0.10s
Test loss: 0.2451 score: 0.9388 time: 0.09s
     INFO: Early stopping counter 1 of 20
Epoch 179/1000, LR 0.000251
Train loss: 0.0179;  Loss pred: 0.0179; Loss self: 0.0000; time: 0.14s
Val loss: 0.0767 score: 0.9796 time: 0.09s
Test loss: 0.2444 score: 0.9184 time: 0.09s
     INFO: Early stopping counter 2 of 20
Epoch 180/1000, LR 0.000251
Train loss: 0.0235;  Loss pred: 0.0235; Loss self: 0.0000; time: 0.15s
Val loss: 0.0758 score: 0.9796 time: 0.09s
Test loss: 0.2448 score: 0.9184 time: 0.09s
     INFO: Early stopping counter 3 of 20
Epoch 181/1000, LR 0.000251
Train loss: 0.0174;  Loss pred: 0.0174; Loss self: 0.0000; time: 0.15s
Val loss: 0.0737 score: 0.9796 time: 0.10s
Test loss: 0.2453 score: 0.9388 time: 0.09s
     INFO: Early stopping counter 4 of 20
Epoch 182/1000, LR 0.000251
Train loss: 0.0232;  Loss pred: 0.0232; Loss self: 0.0000; time: 0.14s
Val loss: 0.0700 score: 0.9796 time: 0.09s
Test loss: 0.2467 score: 0.9388 time: 0.09s
Epoch 183/1000, LR 0.000250
Train loss: 0.0196;  Loss pred: 0.0196; Loss self: 0.0000; time: 0.14s
Val loss: 0.0717 score: 0.9796 time: 0.10s
Test loss: 0.2454 score: 0.9388 time: 0.09s
     INFO: Early stopping counter 1 of 20
Epoch 184/1000, LR 0.000250
Train loss: 0.0183;  Loss pred: 0.0183; Loss self: 0.0000; time: 0.15s
Val loss: 0.0774 score: 0.9796 time: 0.09s
Test loss: 0.2436 score: 0.9184 time: 0.09s
     INFO: Early stopping counter 2 of 20
Epoch 185/1000, LR 0.000250
Train loss: 0.0170;  Loss pred: 0.0170; Loss self: 0.0000; time: 0.14s
Val loss: 0.0759 score: 0.9796 time: 0.09s
Test loss: 0.2434 score: 0.9184 time: 0.08s
     INFO: Early stopping counter 3 of 20
Epoch 186/1000, LR 0.000250
Train loss: 0.0185;  Loss pred: 0.0185; Loss self: 0.0000; time: 0.14s
Val loss: 0.0689 score: 0.9796 time: 0.09s
Test loss: 0.2441 score: 0.9388 time: 0.08s
Epoch 187/1000, LR 0.000249
Train loss: 0.0185;  Loss pred: 0.0185; Loss self: 0.0000; time: 0.14s
Val loss: 0.0668 score: 0.9796 time: 0.09s
Test loss: 0.2456 score: 0.9388 time: 0.10s
Epoch 188/1000, LR 0.000249
Train loss: 0.0199;  Loss pred: 0.0199; Loss self: 0.0000; time: 0.15s
Val loss: 0.0647 score: 0.9796 time: 0.10s
Test loss: 0.2493 score: 0.9592 time: 0.10s
Epoch 189/1000, LR 0.000249
Train loss: 0.0206;  Loss pred: 0.0206; Loss self: 0.0000; time: 0.16s
Val loss: 0.0686 score: 0.9796 time: 0.09s
Test loss: 0.2457 score: 0.9388 time: 0.09s
     INFO: Early stopping counter 1 of 20
Epoch 190/1000, LR 0.000249
Train loss: 0.0155;  Loss pred: 0.0155; Loss self: 0.0000; time: 0.14s
Val loss: 0.0693 score: 0.9796 time: 0.10s
Test loss: 0.2471 score: 0.9388 time: 0.09s
     INFO: Early stopping counter 2 of 20
Epoch 191/1000, LR 0.000249
Train loss: 0.0178;  Loss pred: 0.0178; Loss self: 0.0000; time: 0.14s
Val loss: 0.0696 score: 0.9796 time: 0.10s
Test loss: 0.2483 score: 0.9388 time: 0.09s
     INFO: Early stopping counter 3 of 20
Epoch 192/1000, LR 0.000248
Train loss: 0.0223;  Loss pred: 0.0223; Loss self: 0.0000; time: 0.15s
Val loss: 0.0820 score: 0.9796 time: 0.10s
Test loss: 0.2434 score: 0.9184 time: 0.09s
     INFO: Early stopping counter 4 of 20
Epoch 193/1000, LR 0.000248
Train loss: 0.0165;  Loss pred: 0.0165; Loss self: 0.0000; time: 0.15s
Val loss: 0.0862 score: 0.9592 time: 0.09s
Test loss: 0.2422 score: 0.9184 time: 0.09s
     INFO: Early stopping counter 5 of 20
Epoch 194/1000, LR 0.000248
Train loss: 0.0192;  Loss pred: 0.0192; Loss self: 0.0000; time: 0.14s
Val loss: 0.0818 score: 0.9796 time: 0.10s
Test loss: 0.2445 score: 0.9184 time: 0.09s
     INFO: Early stopping counter 6 of 20
Epoch 195/1000, LR 0.000248
Train loss: 0.0183;  Loss pred: 0.0183; Loss self: 0.0000; time: 0.15s
Val loss: 0.0792 score: 0.9796 time: 0.09s
Test loss: 0.2473 score: 0.9184 time: 0.09s
     INFO: Early stopping counter 7 of 20
Epoch 196/1000, LR 0.000247
Train loss: 0.0160;  Loss pred: 0.0160; Loss self: 0.0000; time: 0.15s
Val loss: 0.0776 score: 0.9796 time: 0.09s
Test loss: 0.2486 score: 0.9184 time: 0.09s
     INFO: Early stopping counter 8 of 20
Epoch 197/1000, LR 0.000247
Train loss: 0.0169;  Loss pred: 0.0169; Loss self: 0.0000; time: 0.14s
Val loss: 0.0703 score: 0.9796 time: 0.09s
Test loss: 0.2506 score: 0.9388 time: 0.09s
     INFO: Early stopping counter 9 of 20
Epoch 198/1000, LR 0.000247
Train loss: 0.0159;  Loss pred: 0.0159; Loss self: 0.0000; time: 0.14s
Val loss: 0.0672 score: 0.9796 time: 0.09s
Test loss: 0.2522 score: 0.9388 time: 0.09s
     INFO: Early stopping counter 10 of 20
Epoch 199/1000, LR 0.000247
Train loss: 0.0166;  Loss pred: 0.0166; Loss self: 0.0000; time: 0.15s
Val loss: 0.0655 score: 0.9796 time: 0.09s
Test loss: 0.2531 score: 0.9388 time: 0.09s
     INFO: Early stopping counter 11 of 20
Epoch 200/1000, LR 0.000246
Train loss: 0.0154;  Loss pred: 0.0154; Loss self: 0.0000; time: 0.15s
Val loss: 0.0655 score: 0.9796 time: 0.09s
Test loss: 0.2524 score: 0.9388 time: 0.09s
     INFO: Early stopping counter 12 of 20
Epoch 201/1000, LR 0.000246
Train loss: 0.0166;  Loss pred: 0.0166; Loss self: 0.0000; time: 0.15s
Val loss: 0.0642 score: 0.9796 time: 0.09s
Test loss: 0.2536 score: 0.9388 time: 0.09s
Epoch 202/1000, LR 0.000246
Train loss: 0.0159;  Loss pred: 0.0159; Loss self: 0.0000; time: 0.14s
Val loss: 0.0663 score: 0.9796 time: 0.09s
Test loss: 0.2513 score: 0.9388 time: 0.09s
     INFO: Early stopping counter 1 of 20
Epoch 203/1000, LR 0.000246
Train loss: 0.0154;  Loss pred: 0.0154; Loss self: 0.0000; time: 0.14s
Val loss: 0.0680 score: 0.9796 time: 0.09s
Test loss: 0.2506 score: 0.9388 time: 0.09s
     INFO: Early stopping counter 2 of 20
Epoch 204/1000, LR 0.000245
Train loss: 0.0150;  Loss pred: 0.0150; Loss self: 0.0000; time: 0.14s
Val loss: 0.0687 score: 0.9796 time: 0.09s
Test loss: 0.2501 score: 0.9388 time: 0.09s
     INFO: Early stopping counter 3 of 20
Epoch 205/1000, LR 0.000245
Train loss: 0.0151;  Loss pred: 0.0151; Loss self: 0.0000; time: 0.14s
Val loss: 0.0682 score: 0.9796 time: 0.09s
Test loss: 0.2488 score: 0.9388 time: 0.09s
     INFO: Early stopping counter 4 of 20
Epoch 206/1000, LR 0.000245
Train loss: 0.0135;  Loss pred: 0.0135; Loss self: 0.0000; time: 0.15s
Val loss: 0.0674 score: 0.9796 time: 0.10s
Test loss: 0.2489 score: 0.9388 time: 0.09s
     INFO: Early stopping counter 5 of 20
Epoch 207/1000, LR 0.000245
Train loss: 0.0139;  Loss pred: 0.0139; Loss self: 0.0000; time: 0.15s
Val loss: 0.0667 score: 0.9796 time: 0.09s
Test loss: 0.2499 score: 0.9388 time: 0.09s
     INFO: Early stopping counter 6 of 20
Epoch 208/1000, LR 0.000244
Train loss: 0.0157;  Loss pred: 0.0157; Loss self: 0.0000; time: 0.15s
Val loss: 0.0670 score: 0.9796 time: 0.17s
Test loss: 0.2503 score: 0.9388 time: 0.10s
     INFO: Early stopping counter 7 of 20
Epoch 209/1000, LR 0.000244
Train loss: 0.0157;  Loss pred: 0.0157; Loss self: 0.0000; time: 0.15s
Val loss: 0.0689 score: 0.9796 time: 0.09s
Test loss: 0.2504 score: 0.9388 time: 0.09s
     INFO: Early stopping counter 8 of 20
Epoch 210/1000, LR 0.000244
Train loss: 0.0149;  Loss pred: 0.0149; Loss self: 0.0000; time: 0.14s
Val loss: 0.0762 score: 0.9796 time: 0.09s
Test loss: 0.2505 score: 0.9184 time: 0.09s
     INFO: Early stopping counter 9 of 20
Epoch 211/1000, LR 0.000244
Train loss: 0.0155;  Loss pred: 0.0155; Loss self: 0.0000; time: 0.14s
Val loss: 0.0703 score: 0.9796 time: 0.09s
Test loss: 0.2507 score: 0.9184 time: 0.09s
     INFO: Early stopping counter 10 of 20
Epoch 212/1000, LR 0.000243
Train loss: 0.0140;  Loss pred: 0.0140; Loss self: 0.0000; time: 0.14s
Val loss: 0.0622 score: 0.9796 time: 0.09s
Test loss: 0.2538 score: 0.9592 time: 0.09s
Epoch 213/1000, LR 0.000243
Train loss: 0.0144;  Loss pred: 0.0144; Loss self: 0.0000; time: 0.15s
Val loss: 0.0592 score: 0.9796 time: 0.09s
Test loss: 0.2599 score: 0.9388 time: 0.09s
Epoch 214/1000, LR 0.000243
Train loss: 0.0144;  Loss pred: 0.0144; Loss self: 0.0000; time: 0.15s
Val loss: 0.0607 score: 0.9796 time: 0.10s
Test loss: 0.2584 score: 0.9388 time: 0.09s
     INFO: Early stopping counter 1 of 20
Epoch 215/1000, LR 0.000243
Train loss: 0.0135;  Loss pred: 0.0135; Loss self: 0.0000; time: 0.14s
Val loss: 0.0645 score: 0.9796 time: 0.09s
Test loss: 0.2551 score: 0.9388 time: 0.09s
     INFO: Early stopping counter 2 of 20
Epoch 216/1000, LR 0.000242
Train loss: 0.0141;  Loss pred: 0.0141; Loss self: 0.0000; time: 0.27s
Val loss: 0.0672 score: 0.9796 time: 0.10s
Test loss: 0.2541 score: 0.9388 time: 0.09s
     INFO: Early stopping counter 3 of 20
Epoch 217/1000, LR 0.000242
Train loss: 0.0148;  Loss pred: 0.0148; Loss self: 0.0000; time: 0.15s
Val loss: 0.0665 score: 0.9796 time: 0.09s
Test loss: 0.2545 score: 0.9388 time: 0.09s
     INFO: Early stopping counter 4 of 20
Epoch 218/1000, LR 0.000242
Train loss: 0.0201;  Loss pred: 0.0201; Loss self: 0.0000; time: 0.14s
Val loss: 0.0641 score: 0.9796 time: 0.10s
Test loss: 0.2547 score: 0.9388 time: 0.09s
     INFO: Early stopping counter 5 of 20
Epoch 219/1000, LR 0.000242
Train loss: 0.0150;  Loss pred: 0.0150; Loss self: 0.0000; time: 0.15s
Val loss: 0.0604 score: 0.9796 time: 0.10s
Test loss: 0.2582 score: 0.9388 time: 0.09s
     INFO: Early stopping counter 6 of 20
Epoch 220/1000, LR 0.000241
Train loss: 0.0137;  Loss pred: 0.0137; Loss self: 0.0000; time: 0.15s
Val loss: 0.0590 score: 0.9796 time: 0.09s
Test loss: 0.2611 score: 0.9388 time: 0.09s
Epoch 221/1000, LR 0.000241
Train loss: 0.0125;  Loss pred: 0.0125; Loss self: 0.0000; time: 0.16s
Val loss: 0.0613 score: 0.9796 time: 0.10s
Test loss: 0.2544 score: 0.9388 time: 0.10s
     INFO: Early stopping counter 1 of 20
Epoch 222/1000, LR 0.000241
Train loss: 0.0119;  Loss pred: 0.0119; Loss self: 0.0000; time: 0.14s
Val loss: 0.0659 score: 0.9796 time: 0.10s
Test loss: 0.2508 score: 0.9388 time: 0.21s
     INFO: Early stopping counter 2 of 20
Epoch 223/1000, LR 0.000241
Train loss: 0.0121;  Loss pred: 0.0121; Loss self: 0.0000; time: 0.15s
Val loss: 0.0724 score: 0.9796 time: 0.10s
Test loss: 0.2518 score: 0.9184 time: 0.10s
     INFO: Early stopping counter 3 of 20
Epoch 224/1000, LR 0.000240
Train loss: 0.0150;  Loss pred: 0.0150; Loss self: 0.0000; time: 0.14s
Val loss: 0.0634 score: 0.9796 time: 0.09s
Test loss: 0.2511 score: 0.9388 time: 0.09s
     INFO: Early stopping counter 4 of 20
Epoch 225/1000, LR 0.000240
Train loss: 0.0125;  Loss pred: 0.0125; Loss self: 0.0000; time: 0.14s
Val loss: 0.0633 score: 0.9796 time: 0.09s
Test loss: 0.2511 score: 0.9388 time: 0.09s
     INFO: Early stopping counter 5 of 20
Epoch 226/1000, LR 0.000240
Train loss: 0.0113;  Loss pred: 0.0113; Loss self: 0.0000; time: 0.14s
Val loss: 0.0655 score: 0.9796 time: 0.10s
Test loss: 0.2512 score: 0.9388 time: 0.09s
     INFO: Early stopping counter 6 of 20
Epoch 227/1000, LR 0.000240
Train loss: 0.0131;  Loss pred: 0.0131; Loss self: 0.0000; time: 0.14s
Val loss: 0.0642 score: 0.9796 time: 0.09s
Test loss: 0.2524 score: 0.9388 time: 0.09s
     INFO: Early stopping counter 7 of 20
Epoch 228/1000, LR 0.000239
Train loss: 0.0121;  Loss pred: 0.0121; Loss self: 0.0000; time: 0.15s
Val loss: 0.0641 score: 0.9796 time: 0.09s
Test loss: 0.2531 score: 0.9388 time: 0.09s
     INFO: Early stopping counter 8 of 20
Epoch 229/1000, LR 0.000239
Train loss: 0.0177;  Loss pred: 0.0177; Loss self: 0.0000; time: 0.14s
Val loss: 0.0618 score: 0.9796 time: 0.09s
Test loss: 0.2541 score: 0.9388 time: 0.09s
     INFO: Early stopping counter 9 of 20
Epoch 230/1000, LR 0.000239
Train loss: 0.0130;  Loss pred: 0.0130; Loss self: 0.0000; time: 0.14s
Val loss: 0.0633 score: 0.9796 time: 0.09s
Test loss: 0.2537 score: 0.9388 time: 0.20s
     INFO: Early stopping counter 10 of 20
Epoch 231/1000, LR 0.000238
Train loss: 0.0129;  Loss pred: 0.0129; Loss self: 0.0000; time: 0.14s
Val loss: 0.0662 score: 0.9796 time: 0.09s
Test loss: 0.2525 score: 0.9388 time: 0.09s
     INFO: Early stopping counter 11 of 20
Epoch 232/1000, LR 0.000238
Train loss: 0.0116;  Loss pred: 0.0116; Loss self: 0.0000; time: 0.14s
Val loss: 0.0651 score: 0.9796 time: 0.10s
Test loss: 0.2531 score: 0.9388 time: 0.10s
     INFO: Early stopping counter 12 of 20
Epoch 233/1000, LR 0.000238
Train loss: 0.0125;  Loss pred: 0.0125; Loss self: 0.0000; time: 0.14s
Val loss: 0.0625 score: 0.9796 time: 0.09s
Test loss: 0.2541 score: 0.9388 time: 0.09s
     INFO: Early stopping counter 13 of 20
Epoch 234/1000, LR 0.000238
Train loss: 0.0114;  Loss pred: 0.0114; Loss self: 0.0000; time: 0.15s
Val loss: 0.0611 score: 0.9796 time: 0.10s
Test loss: 0.2553 score: 0.9388 time: 0.09s
     INFO: Early stopping counter 14 of 20
Epoch 235/1000, LR 0.000237
Train loss: 0.0105;  Loss pred: 0.0105; Loss self: 0.0000; time: 0.16s
Val loss: 0.0595 score: 0.9796 time: 0.10s
Test loss: 0.2561 score: 0.9388 time: 0.09s
     INFO: Early stopping counter 15 of 20
Epoch 236/1000, LR 0.000237
Train loss: 0.0170;  Loss pred: 0.0170; Loss self: 0.0000; time: 0.15s
Val loss: 0.0627 score: 0.9796 time: 0.09s
Test loss: 0.2533 score: 0.9388 time: 0.09s
     INFO: Early stopping counter 16 of 20
Epoch 237/1000, LR 0.000237
Train loss: 0.0117;  Loss pred: 0.0117; Loss self: 0.0000; time: 0.14s
Val loss: 0.0643 score: 0.9796 time: 0.10s
Test loss: 0.2530 score: 0.9388 time: 0.23s
     INFO: Early stopping counter 17 of 20
Epoch 238/1000, LR 0.000236
Train loss: 0.0109;  Loss pred: 0.0109; Loss self: 0.0000; time: 0.15s
Val loss: 0.0634 score: 0.9796 time: 0.11s
Test loss: 0.2528 score: 0.9388 time: 0.10s
     INFO: Early stopping counter 18 of 20
Epoch 239/1000, LR 0.000236
Train loss: 0.0113;  Loss pred: 0.0113; Loss self: 0.0000; time: 0.15s
Val loss: 0.0643 score: 0.9796 time: 0.09s
Test loss: 0.2531 score: 0.9388 time: 0.10s
     INFO: Early stopping counter 19 of 20
Epoch 240/1000, LR 0.000236
Train loss: 0.0118;  Loss pred: 0.0118; Loss self: 0.0000; time: 0.14s
Val loss: 0.0616 score: 0.9796 time: 0.10s
Test loss: 0.2535 score: 0.9388 time: 0.10s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 219,   Train_Loss: 0.0137,   Val_Loss: 0.0590,   Val_Precision: 0.9600,   Val_Recall: 1.0000,   Val_accuracy: 0.9796,   Val_Score: 0.9796,   Val_Loss: 0.0590,   Test_Precision: 1.0000,   Test_Recall: 0.8800,   Test_accuracy: 0.9362,   Test_Score: 0.9388,   Test_loss: 0.2611


[0.09557036589831114, 0.0942862059455365, 0.09493249002844095, 0.08603419596329331, 0.14399997401051223, 0.09640202205628157, 0.10011990601196885, 0.09626192599534988, 0.09202838293276727, 0.09758033300749958, 0.098139189183712, 0.09403934609144926, 0.1119859111495316, 0.09651086898520589, 0.09305733907967806, 0.09197020484134555, 0.09017096110619605, 0.10473718494176865, 0.09348232997581363, 0.2473335131071508, 0.10166144790127873, 0.0971588019747287, 0.0936598761472851, 0.09027533582411706, 0.0912429301533848, 0.0906289650592953, 0.0906163901090622, 2.3175381820183247, 0.09649670519866049, 0.09368237806484103, 0.09602797403931618, 0.09519176394678652, 0.09439421980641782, 0.09373134095221758, 0.09460119903087616, 0.09431733703240752, 0.09452473791316152, 0.09464783011935651, 0.09572795988060534, 0.09495457191951573, 0.09487721999175847, 0.09675538912415504, 0.09581843507476151, 0.09447879903018475, 0.09572523506358266, 0.21709277294576168, 0.08982635918073356, 0.09165733703412116, 0.08907641586847603, 0.09240778116509318, 0.08935851394198835, 0.09689472801983356, 0.09103746805340052, 0.09446587483398616, 0.09611725504510105, 0.09411596995778382, 0.09858945198357105, 0.09288656618446112, 0.09298155107535422, 0.08950469014234841, 0.09300801693461835, 0.09726476203650236, 0.0972670519258827, 0.10032848594710231, 0.09652395197190344, 0.09799121599644423, 0.10097233299165964, 0.0966276740655303, 0.09636033116839826, 0.10546067100949585, 0.09866437711752951, 0.0992555320262909, 0.09833866497501731, 0.09962834697216749, 0.09819808998145163, 0.2234561238437891, 0.09199976711533964, 0.09198854793794453, 0.09276202414184809, 0.10442222305573523, 0.091028698021546, 0.09091425710357726, 0.09403796005062759, 0.2622809619642794, 0.09389542322605848, 0.0976843279786408, 0.09577599703334272, 1.8509777300059795, 0.09570501698181033, 0.09398785303346813, 0.09468197706155479, 0.0944347670301795, 0.0962842081207782, 0.09039127896539867, 0.09364201012067497, 0.09476263914257288, 0.0924967669416219, 0.09079088317230344, 0.09250932605937123, 0.09213086008094251, 0.08929740893654525, 0.13147406792268157, 0.09389730007387698, 0.09460891713388264, 0.09344820701517165, 0.10148592898622155, 0.10226689698174596, 0.10196364508010447, 0.239697938086465, 0.0942813919391483, 0.09338195901364088, 0.09414766309782863, 0.09348623198457062, 0.09251611609943211, 0.0939382219221443, 0.09519502311013639, 0.11195819498971105, 0.09251981507986784, 0.08851631707511842, 0.08907741797156632, 0.09154917392879725, 0.09172345604747534, 0.09561106795445085, 0.2442228568252176, 0.09693623217754066, 0.09535250207409263, 0.09638634813018143, 0.09709984692744911, 0.09664492588490248, 0.09553612489253283, 0.09576063300482929, 0.09849453088827431, 0.097860855050385, 0.09932906413450837, 0.09922719211317599, 0.09959365613758564, 0.09967243787832558, 0.0997649400960654, 0.09238604293204844, 0.09443043009378016, 0.09508126182481647, 0.09581324202008545, 0.0965381080750376, 0.09524637390859425, 0.09676201897673309, 0.09594571101479232, 0.09609194588847458, 0.09660195396281779, 0.0952779040671885, 0.09392093517817557, 0.09416672005318105, 0.09540514694526792, 0.09665321605280042, 0.09411780699156225, 0.09466508612968028, 0.09470708412118256, 0.09868532908149064, 0.09505630610510707, 0.09790496597997844, 0.09501013695262372, 0.09954291488975286, 0.09245318593457341, 0.09365319390781224, 0.09429152589291334, 0.09564674808643758, 0.09738166397437453, 0.0975366709753871, 0.09654906881041825, 0.09846324403770268, 0.11127819982357323, 0.09492985415272415, 0.10039073112420738, 0.09246206399984658, 0.09058029693551362, 0.09269108902662992, 0.09517993405461311, 0.09325544093735516, 0.09253027709200978, 0.09642212302424014, 0.09622060204856098, 0.09574523800984025, 0.09493272705003619, 0.09841176890768111, 0.09353431290946901, 0.08937265002168715, 0.08935985993593931, 0.10044156503863633, 0.10409400798380375, 0.09632820705883205, 0.09836730384267867, 0.09846920799463987, 0.09753788891248405, 0.09680606215260923, 0.09615761996246874, 0.09372288407757878, 0.09966257400810719, 0.09714484680444002, 0.09596069483086467, 0.09374319203197956, 0.09364236402325332, 0.0922830319032073, 0.09186062100343406, 0.09203453501686454, 0.09457708802074194, 0.0955435298383236, 0.09845489612780511, 0.09479868109337986, 0.11260882299393415, 0.09420307981781662, 0.09351776796393096, 0.09724152600392699, 0.09521900420077145, 0.09612280898727477, 0.09383213706314564, 0.09712748718447983, 0.09648814308457077, 0.0917806860525161, 0.09915271401405334, 0.10040990705601871, 0.09939573891460896, 0.102092799032107, 0.21346629597246647, 0.10838780179619789, 0.09753713593818247, 0.09600440412759781, 0.09601914207451046, 0.09603579388931394, 0.09522227803245187, 0.09295588103123009, 0.20908631710335612, 0.09699250501580536, 0.10065579903312027, 0.0969774469267577, 0.09795219893567264, 0.0978123489767313, 0.09689250006340444, 0.23661579680629075, 0.1010393409524113, 0.10071105207316577, 0.10147450002841651]
[0.0019504156305777784, 0.0019242082846027855, 0.0019373977556824684, 0.001755799917618231, 0.0029387749798063722, 0.001967388205230236, 0.0020432633879993644, 0.001964529101945916, 0.0018781302639340259, 0.0019914353674999915, 0.002002840595585959, 0.0019191703283969238, 0.002285426758153706, 0.001969609571126651, 0.0018991293689730217, 0.0018769429559458274, 0.0018402236960448173, 0.0021374935702401766, 0.001907802652567625, 0.005047622716472465, 0.002074723426556709, 0.00198283269336181, 0.001911426043822145, 0.0018423537923289196, 0.0018621006153752, 0.0018495707154958223, 0.001849314083858412, 0.04729669759221071, 0.0019693205142583773, 0.0019118852666294088, 0.0019597545722309425, 0.0019426890601385006, 0.0019264126491105678, 0.0019128845092289302, 0.00193063671491584, 0.001924843612906276, 0.0019290762839420717, 0.001931588369782786, 0.001953631834298068, 0.0019378484065207292, 0.0019362697957501728, 0.0019745997780439804, 0.0019554782668318674, 0.001928138755718056, 0.001953576225787401, 0.004430464753995137, 0.00183319100368844, 0.0018705578986555338, 0.001817886038132164, 0.0018858730850019017, 0.0018236431416732315, 0.001977443428976195, 0.001857907511293888, 0.0019278749966119624, 0.0019615766335734906, 0.0019207340807710983, 0.0020120296323177765, 0.0018956442078461452, 0.001897582675007229, 0.0018266263294356819, 0.001898122794584048, 0.001984995143602089, 0.0019850418760384222, 0.002047520121369435, 0.0019698765708551723, 0.0019998207346213106, 0.002060659856972646, 0.0019719933482761284, 0.001966537370783638, 0.0021522585920305277, 0.0020135587166842757, 0.0020256231025773653, 0.002006911530102394, 0.002033231570860561, 0.0020040426526826863, 0.004560329058036512, 0.0018775462676599926, 0.0018773173048560108, 0.0018931025335071038, 0.0021310657766476578, 0.0018577285310519592, 0.0018553930021138216, 0.0019191420418495427, 0.0053526726931485595, 0.001916233127062418, 0.001993557713849812, 0.001954612184353933, 0.03777505571440774, 0.00195316361187368, 0.001918119449662615, 0.0019322852461541795, 0.0019272401434730511, 0.0019649838391995553, 0.0018447199788856872, 0.001911061431034183, 0.001933931411072916, 0.0018876891212575898, 0.0018528751667817027, 0.0018879454297830863, 0.0018802216343049491, 0.0018223961007458214, 0.0026831442433200322, 0.001916271430079122, 0.0019307942272220947, 0.001907106265615748, 0.0020711414078820726, 0.0020870795302397136, 0.0020808907159204993, 0.0048917946548258165, 0.0019241100395744552, 0.0019057542655845077, 0.001921380879547523, 0.0019078822853994004, 0.0018880840020292268, 0.0019171065698396794, 0.0019427555736762528, 0.002284861122239001, 0.0018881594914258743, 0.001806455450512621, 0.0018179064892156391, 0.0018683504883428008, 0.001871907266275007, 0.0019512462847847112, 0.004984139935208523, 0.0019782904526028707, 0.001945969430083523, 0.0019670683291873763, 0.0019816295291316143, 0.0019723454262224995, 0.0019497168345414863, 0.001954298632751618, 0.002010092467107639, 0.001997160307150714, 0.0020271237578471098, 0.0020250447370035916, 0.002032523594644605, 0.002034131385271951, 0.002036019185633988, 0.0018854294475928253, 0.001927151634566942, 0.0019404339147921727, 0.001955372286124193, 0.0019701654709191347, 0.0019438035491549847, 0.0019747350811578183, 0.001958075734995762, 0.0019610601201729507, 0.0019714684482207714, 0.0019444470217793572, 0.0019167537791464403, 0.0019217697970036949, 0.0019470438152095493, 0.0019725146133224574, 0.0019207715712563724, 0.0019319405332587811, 0.0019327976351261747, 0.0020139863077855234, 0.0019399246143899402, 0.0019980605302036417, 0.0019389823867882394, 0.0020314880589745485, 0.0018867997129504777, 0.0019112896715880049, 0.0019243168549574151, 0.001951974450743624, 0.001987380897436215, 0.001990544305620145, 0.0019703891593962908, 0.0020094539599531157, 0.0022709836698688417, 0.0019373439623004928, 0.002048790431106273, 0.0018869808979560525, 0.0018485774884798697, 0.0018916548780944882, 0.0019424476337676145, 0.0019031722640276564, 0.0018883730018777506, 0.0019677984290661253, 0.001963685756093081, 0.001953984449180413, 0.0019374025928578815, 0.0020084034470955327, 0.0019088635287646735, 0.0018239316330956562, 0.001823670610937537, 0.0020498278579313537, 0.002124367509873546, 0.001965881776710858, 0.0020074959967893605, 0.0020095756733599975, 0.0019905691614792663, 0.001975633921481821, 0.001962400407397321, 0.0019127119199505874, 0.002033930081798106, 0.0019825478939681637, 0.0019583815271605035, 0.0019131263679995829, 0.001911068653535782, 0.0018833271816981081, 0.001874706551090491, 0.001878255816670705, 0.0019301446534845295, 0.001949867955884155, 0.0020092835944450025, 0.0019346669610893848, 0.0022981392447741665, 0.0019225118330166656, 0.0019085258768149177, 0.001984520938855653, 0.0019432449836892132, 0.001961689979332138, 0.0019149415727172578, 0.0019821936160097924, 0.001969145777236138, 0.001873075225561553, 0.002023524775797007, 0.002049181776653443, 0.0020284844676450807, 0.0020835265108593262, 0.004356455019846254, 0.0022119959550244467, 0.001990553794656785, 0.001959273553624445, 0.0019595743280512337, 0.0019599141610064067, 0.0019433117965806503, 0.0018970587965557162, 0.0042670676959868595, 0.001979438877873579, 0.0020541999802677607, 0.0019791315699338305, 0.001999024468074952, 0.0019961703872802307, 0.0019773979604776415, 0.0048288938123732805, 0.0020620273663757406, 0.002055327593329914, 0.002070908163845235]
[512.7112315562025, 519.6942597128616, 516.1562704751558, 569.5409767170482, 340.27783919199123, 508.28809349447823, 489.41316419276586, 509.0278372610895, 532.444431146831, 502.15036667515864, 499.2908582959074, 521.058493456022, 437.55504149599403, 507.71483580270313, 526.5570720654816, 532.7812424091924, 543.4121961092527, 467.8376646006175, 524.1632296999616, 198.11306354902268, 481.9919547829267, 504.3289851674483, 523.1696006403522, 542.7839127119552, 537.0279090952919, 540.665999748988, 540.7410286486319, 21.14312522666889, 507.7893581871248, 523.0439385952104, 510.2679764954555, 514.7504150400202, 519.0995815261615, 522.7707136397339, 517.963836631788, 519.5227255320361, 518.3828178927677, 517.7086462331794, 511.8671708988075, 516.0362372180751, 516.4569535685847, 506.43173929179227, 511.3838476047764, 518.6348736751527, 511.88174118823736, 225.70995494282127, 545.4968947523566, 534.5998649487148, 550.0894880228449, 530.2583763206906, 548.3528970928372, 505.70346809756353, 538.2399252498731, 518.7058298683238, 509.7940008483155, 520.6342772855573, 497.0105727757302, 527.5251525897957, 526.9862616110734, 547.4573446606017, 526.8363052450137, 503.77957005241893, 503.76770992646004, 488.39568879605145, 507.64601944875926, 500.0448203620419, 485.28144837504567, 507.1011019759154, 508.50800745348346, 464.6281834826175, 496.63314593909564, 493.67525416135834, 498.2780680665975, 491.8278932570145, 498.99137558842006, 219.2824217887817, 532.6100438772736, 532.6750024693878, 528.2334064322605, 469.2487725897802, 538.2917812182918, 538.9693713734583, 521.0661734220912, 186.82255712739612, 521.8571716965348, 501.61577618381233, 511.6104401705316, 26.472495700875726, 511.9898783290841, 521.3439654010566, 517.5219352268495, 518.8766970149941, 508.9100378593212, 542.08769430906, 523.2694165455705, 517.0814198861453, 529.7482454811171, 539.7017661675075, 529.6763265635776, 531.8521932493688, 548.7281275408495, 372.69707079282244, 521.8467406565209, 517.9215816481578, 524.3546298544248, 482.8255551235343, 479.1384254940894, 480.56343966032915, 204.4239528765762, 519.7207952935813, 524.7266229748111, 520.4590149952443, 524.1413517242536, 529.6374520017359, 521.6194111126678, 514.7327916850147, 437.6633617977058, 529.6162768775607, 553.5702525718132, 550.0832996264091, 535.231481587261, 534.214497703161, 512.4929681085, 200.6364213283596, 505.4869464108685, 513.8826872306411, 508.37074907973033, 504.63519305660424, 507.0105807557415, 512.8949918695088, 511.69252397829166, 497.4895515323827, 500.7109326274707, 493.3097923246886, 493.8162509336338, 491.9992085872214, 491.61032922477926, 491.15450731306055, 530.383144952321, 518.9005276301021, 515.348650823341, 511.41156448633757, 507.57157952497937, 514.455280439591, 506.39704005951234, 510.7054758544181, 509.92827283225137, 507.23611676488616, 514.2850326078327, 521.7154184745186, 520.3536872934201, 513.5991250881921, 506.9670932960154, 520.6241153110686, 517.6142757940942, 517.3847390053947, 496.5277053445061, 515.4839484906871, 500.485338098381, 515.7344423620142, 492.2500014618735, 529.997960640058, 523.2069292610914, 519.6649384553303, 512.3017873615303, 503.1748072501009, 502.37515295518864, 507.5139574490914, 497.64762961940755, 440.33782068444117, 516.1706023604364, 488.09286924482365, 529.9470710504722, 540.9564955929028, 528.6376556210535, 514.8143932510437, 525.4385106914674, 529.5563953761387, 508.18213147704284, 509.2464498951118, 511.7747996507566, 516.1549817711817, 497.9079285320672, 523.871919040306, 548.2661640681983, 548.3446374594514, 487.8458433134871, 470.7283440140381, 508.6775877607008, 498.1329983219521, 497.61748873482657, 502.3688798920519, 506.1666481460045, 509.5800002030539, 522.8178846848164, 491.6589852075668, 504.4014336513468, 510.6257315702523, 522.7046246012632, 523.2674389535092, 530.9751856808792, 533.4168163109655, 532.408839692852, 518.0958837435732, 512.855240777859, 497.6898247537908, 516.884828299809, 435.1346430700147, 520.1528452653905, 523.964601239188, 503.8999490610749, 514.603155234457, 509.76454512983355, 522.2091442617871, 504.4915854451323, 507.83441813210203, 533.8813873319996, 494.1871787095512, 487.99965498088636, 492.97887952818564, 479.9554960246518, 229.54443359208412, 452.08039270078507, 502.37275811600045, 510.393251697859, 510.31491160352385, 510.2264272056204, 514.5854626928873, 527.1317904408611, 234.35297287186032, 505.1936744186082, 486.80752098422863, 505.27211792869014, 500.244001997131, 500.95923993867757, 505.7150962967765, 207.08676538665176, 484.95961610714187, 486.54044408553983, 482.879935217993]
Elapsed: 0.11785912969110844~0.18346233155589617
Time per graph: 0.002405288361043029~0.003744129215426452
Speed: 495.614592858986~75.45530031107799
Total Time: 0.1024
best val loss: 0.05904805287718773 test_score: 0.9388

Testing...
Test loss: 0.2267 score: 0.9184 time: 0.09s
test Score 0.9184
Epoch Time List: [0.3292852337472141, 0.31953307590447366, 0.3140304679982364, 0.3144084610976279, 0.36263015191070735, 0.36688820901326835, 0.31778799486346543, 0.3236396561842412, 0.3183343231212348, 0.33470472996123135, 0.33647569408640265, 0.3254747549071908, 0.4567101811990142, 0.3280171169899404, 0.3404046380892396, 0.31250299187377095, 0.3383111522998661, 0.3290527171920985, 0.31871567317284644, 0.47078991681337357, 0.3374593579210341, 0.334836712339893, 0.32385665317997336, 0.30657747900113463, 0.3217134971637279, 0.3085201538633555, 0.31387247471138835, 2.967746297828853, 3.6224805880337954, 0.32256310270167887, 0.3247767749708146, 0.327215860132128, 0.3224991981405765, 0.3207212188281119, 0.32416016003116965, 0.32632375275716186, 0.32080763997510076, 0.32509052637033165, 0.3295401050709188, 0.3224231190979481, 0.3296623891219497, 0.3329404017422348, 0.33440841315314174, 0.32746883179061115, 0.3233399090822786, 0.4459288823418319, 0.31381753901951015, 0.3143575091380626, 0.31452669110149145, 0.31824746215716004, 0.31483815982937813, 0.33134525804780424, 0.3246650437358767, 0.38334972178563476, 0.32417725096456707, 0.3209391951095313, 0.32336324895732105, 0.3160319230519235, 0.3184944309759885, 0.31466880696825683, 0.3173738941550255, 0.4747949440497905, 0.3303187408018857, 0.35663744807243347, 0.33459639293141663, 0.3340012039989233, 0.3420688270125538, 0.3316515318583697, 0.38780036685056984, 0.3405356800649315, 0.3377129202708602, 0.33389944513328373, 0.33824445586651564, 0.3390350672416389, 0.33988305018283427, 0.4629461020231247, 0.3208672869950533, 0.31842585979029536, 0.3131929330993444, 0.3362861769273877, 0.3076966009102762, 0.311138057615608, 0.3209219090640545, 0.5986058600246906, 0.32517343387007713, 0.32869174401275814, 0.32541312417015433, 3.872821615077555, 0.3293909369967878, 0.3280808157287538, 0.3328723479062319, 0.32697442383505404, 0.3261610821355134, 0.32469184789806604, 0.4616989209316671, 0.3241237439215183, 0.32779380306601524, 0.317804376129061, 0.32176613504998386, 0.3155427100136876, 0.3061078106984496, 0.34975996520370245, 0.429800073383376, 0.317694955971092, 0.320285745896399, 0.3436934482306242, 0.3385945660993457, 0.3435896879527718, 0.4914630849380046, 0.3301589253824204, 0.3220217749476433, 0.3231486240401864, 0.327604774851352, 0.32043344783596694, 0.33448246330954134, 0.33238588203676045, 0.3423533667810261, 0.37305361800827086, 0.3133628766518086, 0.3070510351099074, 0.3162237189244479, 0.31461646291427314, 0.3240599720738828, 0.47460807487368584, 0.3287365708965808, 0.3195223279763013, 0.33136449195444584, 0.3296520388685167, 0.32763148797675967, 0.3282386309001595, 0.32894634106196463, 0.438129894901067, 0.33282266021706164, 0.3357173518743366, 0.34187071095220745, 0.3385125841014087, 0.33278138493187726, 0.3371177220251411, 0.36897814087569714, 0.3241114739794284, 0.325917094014585, 0.331757253035903, 0.32884487695991993, 0.3340139801148325, 0.32922522304579616, 0.33316361904144287, 0.4518906371667981, 0.32720182999037206, 0.32716024783439934, 0.3245704749133438, 0.3257628751453012, 0.3232719290535897, 0.32816366315819323, 0.3643717900849879, 0.32380501204170287, 0.3233409677632153, 0.3246106223668903, 0.33006160613149405, 0.33459685393609107, 0.3279653931967914, 0.3406443237327039, 0.4692715499550104, 0.3181462660431862, 0.318313678028062, 0.3251092382706702, 0.3284979809541255, 0.33179710316471756, 0.3365283189341426, 0.33074077405035496, 0.35306525276973844, 0.33094779029488564, 0.3363416800275445, 0.313384389039129, 0.3167242081835866, 0.3103057760745287, 0.3207186779472977, 0.3219553849194199, 0.322518615052104, 0.3264107368886471, 0.33186207083053887, 0.33206618623808026, 0.3280190220102668, 0.3272059157025069, 0.32878297311253846, 0.3148772898130119, 0.3105273740366101, 0.3301876219920814, 0.3428978759329766, 0.3406535389367491, 0.3329032859764993, 0.32672719890251756, 0.3351822840049863, 0.33382089296355844, 0.3295020409859717, 0.33020543307065964, 0.3320440500974655, 0.32342112995684147, 0.31951086805202067, 0.3290900399442762, 0.3291367457713932, 0.3262889247853309, 0.31975220795720816, 0.3124606399796903, 0.32149834698066115, 0.3256585788913071, 0.33939504297450185, 0.3301328180823475, 0.4240346662700176, 0.33584692305885255, 0.3181366410572082, 0.3239429397508502, 0.32331082434393466, 0.33292373991571367, 0.3416322849225253, 0.3298965517897159, 0.45887578185647726, 0.324441981036216, 0.3304879569914192, 0.3397117150016129, 0.3353631941135973, 0.35664576874114573, 0.4434553300961852, 0.3530297358520329, 0.33078725123777986, 0.328251221915707, 0.330203260993585, 0.328457023948431, 0.3345922997687012, 0.3181230309419334, 0.4363577126059681, 0.3223656858317554, 0.33463753992691636, 0.32668571057729423, 0.3334426179062575, 0.3440729849971831, 0.33364550210535526, 0.4647967803757638, 0.34837832930497825, 0.3426643516868353, 0.3352172481827438]
Total Epoch List: [240]
Total Time List: [0.10240444797091186]
========================k_idx:1========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: SGFormer
SGFormer(
  (trans_conv): SGModule(
    (attns): ModuleList(
      (0-1): 2 x SGFormerAttention(heads=1, head_channels=64)
    )
    (fcs): ModuleList(
      (0): Linear(in_features=14887, out_features=64, bias=True)
    )
    (bns): ModuleList(
      (0-2): 3 x LayerNorm((64,), eps=1e-05, elementwise_affine=True)
    )
  )
  (graph_conv): GraphModule(
    (convs): ModuleList(
      (0-2): 3 x GCNConv(64, 64)
    )
    (fcs): ModuleList(
      (0): Linear(in_features=14887, out_features=64, bias=True)
    )
    (bns): ModuleList(
      (0-3): 4 x BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (classifier): Linear(in_features=64, out_features=2, bias=True)
)
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7e09d87bc0d0>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.7091;  Loss pred: 0.7091; Loss self: 0.0000; time: 0.16s
Val loss: 0.6967 score: 0.4694 time: 0.10s
Test loss: 0.7019 score: 0.4490 time: 0.09s
Epoch 2/1000, LR 0.000000
Train loss: 0.6938;  Loss pred: 0.6938; Loss self: 0.0000; time: 0.15s
Val loss: 0.6958 score: 0.4694 time: 0.09s
Test loss: 0.7009 score: 0.4490 time: 0.09s
Epoch 3/1000, LR 0.000030
Train loss: 0.7205;  Loss pred: 0.7205; Loss self: 0.0000; time: 0.16s
Val loss: 0.6942 score: 0.4898 time: 0.15s
Test loss: 0.6992 score: 0.4490 time: 0.09s
Epoch 4/1000, LR 0.000060
Train loss: 0.7097;  Loss pred: 0.7097; Loss self: 0.0000; time: 0.16s
Val loss: 0.6920 score: 0.4898 time: 0.09s
Test loss: 0.6972 score: 0.4898 time: 0.09s
Epoch 5/1000, LR 0.000090
Train loss: 0.6866;  Loss pred: 0.6866; Loss self: 0.0000; time: 0.15s
Val loss: 0.6894 score: 0.5510 time: 0.09s
Test loss: 0.6946 score: 0.4898 time: 0.09s
Epoch 6/1000, LR 0.000120
Train loss: 0.7068;  Loss pred: 0.7068; Loss self: 0.0000; time: 0.16s
Val loss: 0.6869 score: 0.5510 time: 0.09s
Test loss: 0.6921 score: 0.4898 time: 0.09s
Epoch 7/1000, LR 0.000150
Train loss: 0.6692;  Loss pred: 0.6692; Loss self: 0.0000; time: 0.16s
Val loss: 0.6845 score: 0.5306 time: 0.09s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6896 score: 0.4898 time: 0.09s
Epoch 8/1000, LR 0.000180
Train loss: 0.6821;  Loss pred: 0.6821; Loss self: 0.0000; time: 0.16s
Val loss: 0.6826 score: 0.5306 time: 0.09s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6875 score: 0.4898 time: 0.09s
Epoch 9/1000, LR 0.000210
Train loss: 0.6360;  Loss pred: 0.6360; Loss self: 0.0000; time: 0.16s
Val loss: 0.6812 score: 0.5306 time: 0.09s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6865 score: 0.4898 time: 0.09s
Epoch 10/1000, LR 0.000240
Train loss: 0.6050;  Loss pred: 0.6050; Loss self: 0.0000; time: 0.15s
Val loss: 0.6808 score: 0.5306 time: 0.10s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6862 score: 0.4898 time: 0.19s
Epoch 11/1000, LR 0.000270
Train loss: 0.6028;  Loss pred: 0.6028; Loss self: 0.0000; time: 0.14s
Val loss: 0.6807 score: 0.5306 time: 0.09s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6865 score: 0.4898 time: 0.09s
Epoch 12/1000, LR 0.000270
Train loss: 0.6124;  Loss pred: 0.6124; Loss self: 0.0000; time: 0.15s
Val loss: 0.6811 score: 0.5306 time: 0.09s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6876 score: 0.4898 time: 0.09s
     INFO: Early stopping counter 1 of 20
Epoch 13/1000, LR 0.000270
Train loss: 0.5470;  Loss pred: 0.5470; Loss self: 0.0000; time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6812 score: 0.5102 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6885 score: 0.4898 time: 0.09s
     INFO: Early stopping counter 2 of 20
Epoch 14/1000, LR 0.000270
Train loss: 0.5310;  Loss pred: 0.5310; Loss self: 0.0000; time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6819 score: 0.5102 time: 0.09s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6899 score: 0.4898 time: 0.09s
     INFO: Early stopping counter 3 of 20
Epoch 15/1000, LR 0.000270
Train loss: 0.5200;  Loss pred: 0.5200; Loss self: 0.0000; time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6828 score: 0.5102 time: 0.09s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6917 score: 0.4898 time: 0.09s
     INFO: Early stopping counter 4 of 20
Epoch 16/1000, LR 0.000270
Train loss: 0.5146;  Loss pred: 0.5146; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6832 score: 0.5102 time: 0.09s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6926 score: 0.4898 time: 3.28s
     INFO: Early stopping counter 5 of 20
Epoch 17/1000, LR 0.000270
Train loss: 0.4971;  Loss pred: 0.4971; Loss self: 0.0000; time: 1.46s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6827 score: 0.5102 time: 1.88s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6924 score: 0.4898 time: 0.10s
     INFO: Early stopping counter 6 of 20
Epoch 18/1000, LR 0.000270
Train loss: 0.4920;  Loss pred: 0.4920; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6821 score: 0.5102 time: 0.09s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6921 score: 0.4898 time: 0.09s
     INFO: Early stopping counter 7 of 20
Epoch 19/1000, LR 0.000270
Train loss: 0.4591;  Loss pred: 0.4591; Loss self: 0.0000; time: 0.14s
Val loss: 0.6812 score: 0.5306 time: 0.09s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6915 score: 0.4898 time: 0.09s
     INFO: Early stopping counter 8 of 20
Epoch 20/1000, LR 0.000270
Train loss: 0.4429;  Loss pred: 0.4429; Loss self: 0.0000; time: 0.14s
Val loss: 0.6796 score: 0.5306 time: 0.10s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6904 score: 0.4898 time: 0.08s
Epoch 21/1000, LR 0.000270
Train loss: 0.4645;  Loss pred: 0.4645; Loss self: 0.0000; time: 0.14s
Val loss: 0.6774 score: 0.5306 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6887 score: 0.4898 time: 0.08s
Epoch 22/1000, LR 0.000270
Train loss: 0.4283;  Loss pred: 0.4283; Loss self: 0.0000; time: 0.15s
Val loss: 0.6740 score: 0.5306 time: 0.09s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6860 score: 0.4898 time: 0.09s
Epoch 23/1000, LR 0.000270
Train loss: 0.4187;  Loss pred: 0.4187; Loss self: 0.0000; time: 0.16s
Val loss: 0.6705 score: 0.5306 time: 0.09s
Test loss: 0.6831 score: 0.5102 time: 0.09s
Epoch 24/1000, LR 0.000270
Train loss: 0.4061;  Loss pred: 0.4061; Loss self: 0.0000; time: 0.15s
Val loss: 0.6660 score: 0.5306 time: 0.22s
Test loss: 0.6795 score: 0.5102 time: 0.09s
Epoch 25/1000, LR 0.000270
Train loss: 0.4089;  Loss pred: 0.4089; Loss self: 0.0000; time: 0.14s
Val loss: 0.6612 score: 0.5306 time: 0.09s
Test loss: 0.6755 score: 0.5102 time: 0.09s
Epoch 26/1000, LR 0.000270
Train loss: 0.3977;  Loss pred: 0.3977; Loss self: 0.0000; time: 0.14s
Val loss: 0.6557 score: 0.5306 time: 0.09s
Test loss: 0.6707 score: 0.5102 time: 0.09s
Epoch 27/1000, LR 0.000270
Train loss: 0.3879;  Loss pred: 0.3879; Loss self: 0.0000; time: 0.14s
Val loss: 0.6507 score: 0.5306 time: 0.10s
Test loss: 0.6662 score: 0.5102 time: 0.09s
Epoch 28/1000, LR 0.000270
Train loss: 0.3891;  Loss pred: 0.3891; Loss self: 0.0000; time: 0.14s
Val loss: 0.6448 score: 0.5510 time: 0.09s
Test loss: 0.6607 score: 0.5102 time: 0.09s
Epoch 29/1000, LR 0.000270
Train loss: 0.3636;  Loss pred: 0.3636; Loss self: 0.0000; time: 0.14s
Val loss: 0.6385 score: 0.5510 time: 0.09s
Test loss: 0.6551 score: 0.5102 time: 0.10s
Epoch 30/1000, LR 0.000270
Train loss: 0.3531;  Loss pred: 0.3531; Loss self: 0.0000; time: 0.15s
Val loss: 0.6313 score: 0.5510 time: 0.09s
Test loss: 0.6489 score: 0.5306 time: 0.09s
Epoch 31/1000, LR 0.000270
Train loss: 0.3471;  Loss pred: 0.3471; Loss self: 0.0000; time: 0.15s
Val loss: 0.6240 score: 0.5714 time: 0.11s
Test loss: 0.6429 score: 0.5306 time: 0.10s
Epoch 32/1000, LR 0.000270
Train loss: 0.3306;  Loss pred: 0.3306; Loss self: 0.0000; time: 0.14s
Val loss: 0.6168 score: 0.5918 time: 0.09s
Test loss: 0.6370 score: 0.5510 time: 0.09s
Epoch 33/1000, LR 0.000270
Train loss: 0.3212;  Loss pred: 0.3212; Loss self: 0.0000; time: 0.16s
Val loss: 0.6100 score: 0.5918 time: 0.09s
Test loss: 0.6318 score: 0.5510 time: 0.10s
Epoch 34/1000, LR 0.000270
Train loss: 0.3152;  Loss pred: 0.3152; Loss self: 0.0000; time: 0.14s
Val loss: 0.6029 score: 0.6122 time: 0.09s
Test loss: 0.6266 score: 0.5510 time: 0.09s
Epoch 35/1000, LR 0.000270
Train loss: 0.3038;  Loss pred: 0.3038; Loss self: 0.0000; time: 0.14s
Val loss: 0.5955 score: 0.6122 time: 0.09s
Test loss: 0.6209 score: 0.5510 time: 0.09s
Epoch 36/1000, LR 0.000270
Train loss: 0.3187;  Loss pred: 0.3187; Loss self: 0.0000; time: 0.14s
Val loss: 0.5886 score: 0.6327 time: 0.09s
Test loss: 0.6156 score: 0.5510 time: 0.09s
Epoch 37/1000, LR 0.000270
Train loss: 0.2858;  Loss pred: 0.2858; Loss self: 0.0000; time: 0.14s
Val loss: 0.5817 score: 0.6327 time: 0.10s
Test loss: 0.6106 score: 0.5306 time: 0.09s
Epoch 38/1000, LR 0.000270
Train loss: 0.2917;  Loss pred: 0.2917; Loss self: 0.0000; time: 0.15s
Val loss: 0.5746 score: 0.6531 time: 0.09s
Test loss: 0.6056 score: 0.5714 time: 0.09s
Epoch 39/1000, LR 0.000269
Train loss: 0.2752;  Loss pred: 0.2752; Loss self: 0.0000; time: 0.28s
Val loss: 0.5684 score: 0.6531 time: 0.09s
Test loss: 0.6011 score: 0.5714 time: 0.10s
Epoch 40/1000, LR 0.000269
Train loss: 0.2689;  Loss pred: 0.2689; Loss self: 0.0000; time: 0.14s
Val loss: 0.5620 score: 0.6531 time: 0.09s
Test loss: 0.5960 score: 0.5714 time: 0.09s
Epoch 41/1000, LR 0.000269
Train loss: 0.2709;  Loss pred: 0.2709; Loss self: 0.0000; time: 0.15s
Val loss: 0.5574 score: 0.6531 time: 0.09s
Test loss: 0.5929 score: 0.5714 time: 0.11s
Epoch 42/1000, LR 0.000269
Train loss: 0.2602;  Loss pred: 0.2602; Loss self: 0.0000; time: 0.14s
Val loss: 0.5533 score: 0.6735 time: 0.09s
Test loss: 0.5896 score: 0.5714 time: 0.09s
Epoch 43/1000, LR 0.000269
Train loss: 0.2607;  Loss pred: 0.2607; Loss self: 0.0000; time: 0.14s
Val loss: 0.5501 score: 0.6735 time: 0.09s
Test loss: 0.5871 score: 0.5714 time: 0.09s
Epoch 44/1000, LR 0.000269
Train loss: 0.2488;  Loss pred: 0.2488; Loss self: 0.0000; time: 0.15s
Val loss: 0.5472 score: 0.6735 time: 0.09s
Test loss: 0.5855 score: 0.5714 time: 0.09s
Epoch 45/1000, LR 0.000269
Train loss: 0.2470;  Loss pred: 0.2470; Loss self: 0.0000; time: 0.14s
Val loss: 0.5439 score: 0.6735 time: 0.09s
Test loss: 0.5834 score: 0.5714 time: 0.09s
Epoch 46/1000, LR 0.000269
Train loss: 0.2408;  Loss pred: 0.2408; Loss self: 0.0000; time: 0.15s
Val loss: 0.5409 score: 0.6735 time: 0.16s
Test loss: 0.5817 score: 0.5714 time: 0.29s
Epoch 47/1000, LR 0.000269
Train loss: 0.2272;  Loss pred: 0.2272; Loss self: 0.0000; time: 0.14s
Val loss: 0.5379 score: 0.6735 time: 0.10s
Test loss: 0.5797 score: 0.5714 time: 0.09s
Epoch 48/1000, LR 0.000269
Train loss: 0.2278;  Loss pred: 0.2278; Loss self: 0.0000; time: 0.15s
Val loss: 0.5356 score: 0.6735 time: 0.09s
Test loss: 0.5782 score: 0.5714 time: 0.09s
Epoch 49/1000, LR 0.000269
Train loss: 0.2180;  Loss pred: 0.2180; Loss self: 0.0000; time: 0.15s
Val loss: 0.5328 score: 0.6735 time: 0.09s
Test loss: 0.5758 score: 0.5714 time: 0.09s
Epoch 50/1000, LR 0.000269
Train loss: 0.2268;  Loss pred: 0.2268; Loss self: 0.0000; time: 0.16s
Val loss: 0.5292 score: 0.6735 time: 0.09s
Test loss: 0.5727 score: 0.5714 time: 0.10s
Epoch 51/1000, LR 0.000269
Train loss: 0.2149;  Loss pred: 0.2149; Loss self: 0.0000; time: 0.16s
Val loss: 0.5259 score: 0.6735 time: 0.09s
Test loss: 0.5698 score: 0.5918 time: 0.09s
Epoch 52/1000, LR 0.000269
Train loss: 0.2248;  Loss pred: 0.2248; Loss self: 0.0000; time: 0.15s
Val loss: 0.5223 score: 0.6735 time: 0.09s
Test loss: 0.5660 score: 0.5918 time: 0.21s
Epoch 53/1000, LR 0.000269
Train loss: 0.1955;  Loss pred: 0.1955; Loss self: 0.0000; time: 0.15s
Val loss: 0.5183 score: 0.6939 time: 0.09s
Test loss: 0.5619 score: 0.5918 time: 0.09s
Epoch 54/1000, LR 0.000269
Train loss: 0.1973;  Loss pred: 0.1973; Loss self: 0.0000; time: 0.15s
Val loss: 0.5146 score: 0.6939 time: 0.09s
Test loss: 0.5579 score: 0.5918 time: 0.09s
Epoch 55/1000, LR 0.000269
Train loss: 0.2001;  Loss pred: 0.2001; Loss self: 0.0000; time: 0.15s
Val loss: 0.5105 score: 0.6735 time: 0.09s
Test loss: 0.5535 score: 0.6122 time: 0.09s
Epoch 56/1000, LR 0.000269
Train loss: 0.1852;  Loss pred: 0.1852; Loss self: 0.0000; time: 0.15s
Val loss: 0.5067 score: 0.6735 time: 0.25s
Test loss: 0.5495 score: 0.6122 time: 0.09s
Epoch 57/1000, LR 0.000269
Train loss: 0.1826;  Loss pred: 0.1826; Loss self: 0.0000; time: 0.16s
Val loss: 0.5035 score: 0.6735 time: 0.09s
Test loss: 0.5459 score: 0.6122 time: 0.09s
Epoch 58/1000, LR 0.000269
Train loss: 0.1925;  Loss pred: 0.1925; Loss self: 0.0000; time: 0.15s
Val loss: 0.4991 score: 0.6939 time: 0.09s
Test loss: 0.5410 score: 0.6327 time: 0.09s
Epoch 59/1000, LR 0.000268
Train loss: 0.1702;  Loss pred: 0.1702; Loss self: 0.0000; time: 0.16s
Val loss: 0.4955 score: 0.6939 time: 0.16s
Test loss: 0.5369 score: 0.6531 time: 0.09s
Epoch 60/1000, LR 0.000268
Train loss: 0.1736;  Loss pred: 0.1736; Loss self: 0.0000; time: 0.14s
Val loss: 0.4921 score: 0.6939 time: 0.09s
Test loss: 0.5330 score: 0.6531 time: 0.09s
Epoch 61/1000, LR 0.000268
Train loss: 0.1653;  Loss pred: 0.1653; Loss self: 0.0000; time: 0.14s
Val loss: 0.4891 score: 0.6939 time: 0.09s
Test loss: 0.5296 score: 0.6735 time: 0.09s
Epoch 62/1000, LR 0.000268
Train loss: 0.1630;  Loss pred: 0.1630; Loss self: 0.0000; time: 0.14s
Val loss: 0.4880 score: 0.6939 time: 0.09s
Test loss: 0.5279 score: 0.6735 time: 0.09s
Epoch 63/1000, LR 0.000268
Train loss: 0.1673;  Loss pred: 0.1673; Loss self: 0.0000; time: 0.14s
Val loss: 0.4870 score: 0.6939 time: 0.09s
Test loss: 0.5262 score: 0.6939 time: 0.09s
Epoch 64/1000, LR 0.000268
Train loss: 0.1592;  Loss pred: 0.1592; Loss self: 0.0000; time: 0.14s
Val loss: 0.4852 score: 0.6939 time: 0.09s
Test loss: 0.5232 score: 0.6939 time: 0.09s
Epoch 65/1000, LR 0.000268
Train loss: 0.1488;  Loss pred: 0.1488; Loss self: 0.0000; time: 0.14s
Val loss: 0.4841 score: 0.6939 time: 0.09s
Test loss: 0.5211 score: 0.6939 time: 0.09s
Epoch 66/1000, LR 0.000268
Train loss: 0.1529;  Loss pred: 0.1529; Loss self: 0.0000; time: 0.15s
Val loss: 0.4838 score: 0.6735 time: 0.26s
Test loss: 0.5195 score: 0.6939 time: 0.09s
Epoch 67/1000, LR 0.000268
Train loss: 0.1498;  Loss pred: 0.1498; Loss self: 0.0000; time: 0.14s
Val loss: 0.4845 score: 0.6735 time: 0.09s
Test loss: 0.5190 score: 0.6939 time: 0.09s
     INFO: Early stopping counter 1 of 20
Epoch 68/1000, LR 0.000268
Train loss: 0.1433;  Loss pred: 0.1433; Loss self: 0.0000; time: 0.14s
Val loss: 0.4842 score: 0.6735 time: 0.09s
Test loss: 0.5173 score: 0.6735 time: 0.09s
     INFO: Early stopping counter 2 of 20
Epoch 69/1000, LR 0.000268
Train loss: 0.1457;  Loss pred: 0.1457; Loss self: 0.0000; time: 0.14s
Val loss: 0.4824 score: 0.6735 time: 0.09s
Test loss: 0.5141 score: 0.6735 time: 0.09s
Epoch 70/1000, LR 0.000268
Train loss: 0.1572;  Loss pred: 0.1572; Loss self: 0.0000; time: 0.14s
Val loss: 0.4792 score: 0.6939 time: 0.09s
Test loss: 0.5094 score: 0.6735 time: 0.09s
Epoch 71/1000, LR 0.000268
Train loss: 0.1493;  Loss pred: 0.1493; Loss self: 0.0000; time: 0.15s
Val loss: 0.4753 score: 0.6939 time: 0.09s
Test loss: 0.5037 score: 0.6939 time: 0.09s
Epoch 72/1000, LR 0.000267
Train loss: 0.1346;  Loss pred: 0.1346; Loss self: 0.0000; time: 0.15s
Val loss: 0.4707 score: 0.6939 time: 0.09s
Test loss: 0.4973 score: 0.6939 time: 0.09s
Epoch 73/1000, LR 0.000267
Train loss: 0.1262;  Loss pred: 0.1262; Loss self: 0.0000; time: 0.15s
Val loss: 0.4647 score: 0.6939 time: 0.09s
Test loss: 0.4894 score: 0.6939 time: 0.09s
Epoch 74/1000, LR 0.000267
Train loss: 0.1398;  Loss pred: 0.1398; Loss self: 0.0000; time: 0.15s
Val loss: 0.4574 score: 0.6939 time: 0.09s
Test loss: 0.4802 score: 0.6939 time: 0.09s
Epoch 75/1000, LR 0.000267
Train loss: 0.1245;  Loss pred: 0.1245; Loss self: 0.0000; time: 0.15s
Val loss: 0.4493 score: 0.7347 time: 0.09s
Test loss: 0.4699 score: 0.6939 time: 0.09s
Epoch 76/1000, LR 0.000267
Train loss: 0.1507;  Loss pred: 0.1507; Loss self: 0.0000; time: 0.15s
Val loss: 0.4412 score: 0.7347 time: 0.09s
Test loss: 0.4593 score: 0.6939 time: 0.09s
Epoch 77/1000, LR 0.000267
Train loss: 0.1272;  Loss pred: 0.1272; Loss self: 0.0000; time: 0.15s
Val loss: 0.4342 score: 0.7347 time: 0.09s
Test loss: 0.4498 score: 0.6939 time: 0.09s
Epoch 78/1000, LR 0.000267
Train loss: 0.1188;  Loss pred: 0.1188; Loss self: 0.0000; time: 0.15s
Val loss: 0.4272 score: 0.7551 time: 0.09s
Test loss: 0.4403 score: 0.6939 time: 0.09s
Epoch 79/1000, LR 0.000267
Train loss: 0.1213;  Loss pred: 0.1213; Loss self: 0.0000; time: 0.15s
Val loss: 0.4193 score: 0.7551 time: 0.09s
Test loss: 0.4297 score: 0.6939 time: 0.09s
Epoch 80/1000, LR 0.000267
Train loss: 0.1130;  Loss pred: 0.1130; Loss self: 0.0000; time: 0.15s
Val loss: 0.4112 score: 0.7551 time: 0.09s
Test loss: 0.4186 score: 0.7143 time: 0.09s
Epoch 81/1000, LR 0.000267
Train loss: 0.1165;  Loss pred: 0.1165; Loss self: 0.0000; time: 0.15s
Val loss: 0.4039 score: 0.7755 time: 0.09s
Test loss: 0.4084 score: 0.7347 time: 0.09s
Epoch 82/1000, LR 0.000267
Train loss: 0.1087;  Loss pred: 0.1087; Loss self: 0.0000; time: 0.15s
Val loss: 0.3971 score: 0.7959 time: 0.09s
Test loss: 0.3982 score: 0.7755 time: 0.09s
Epoch 83/1000, LR 0.000266
Train loss: 0.1093;  Loss pred: 0.1093; Loss self: 0.0000; time: 0.15s
Val loss: 0.3923 score: 0.7959 time: 0.09s
Test loss: 0.3901 score: 0.7755 time: 0.09s
Epoch 84/1000, LR 0.000266
Train loss: 0.1141;  Loss pred: 0.1141; Loss self: 0.0000; time: 0.15s
Val loss: 0.3882 score: 0.7959 time: 0.09s
Test loss: 0.3827 score: 0.7959 time: 0.09s
Epoch 85/1000, LR 0.000266
Train loss: 0.1075;  Loss pred: 0.1075; Loss self: 0.0000; time: 0.15s
Val loss: 0.3833 score: 0.7959 time: 0.09s
Test loss: 0.3742 score: 0.7959 time: 0.09s
Epoch 86/1000, LR 0.000266
Train loss: 0.1050;  Loss pred: 0.1050; Loss self: 0.0000; time: 0.15s
Val loss: 0.3787 score: 0.7959 time: 0.09s
Test loss: 0.3657 score: 0.8163 time: 0.09s
Epoch 87/1000, LR 0.000266
Train loss: 0.0974;  Loss pred: 0.0974; Loss self: 0.0000; time: 0.15s
Val loss: 0.3743 score: 0.8163 time: 0.09s
Test loss: 0.3573 score: 0.8367 time: 0.09s
Epoch 88/1000, LR 0.000266
Train loss: 0.1037;  Loss pred: 0.1037; Loss self: 0.0000; time: 0.15s
Val loss: 0.3694 score: 0.8163 time: 0.09s
Test loss: 0.3481 score: 0.8367 time: 0.09s
Epoch 89/1000, LR 0.000266
Train loss: 0.1010;  Loss pred: 0.1010; Loss self: 0.0000; time: 0.15s
Val loss: 0.3639 score: 0.8163 time: 0.09s
Test loss: 0.3382 score: 0.8367 time: 0.09s
Epoch 90/1000, LR 0.000266
Train loss: 0.1036;  Loss pred: 0.1036; Loss self: 0.0000; time: 0.15s
Val loss: 0.3589 score: 0.8367 time: 0.09s
Test loss: 0.3285 score: 0.8571 time: 0.09s
Epoch 91/1000, LR 0.000266
Train loss: 0.0937;  Loss pred: 0.0937; Loss self: 0.0000; time: 0.15s
Val loss: 0.3537 score: 0.8367 time: 0.09s
Test loss: 0.3189 score: 0.8980 time: 0.09s
Epoch 92/1000, LR 0.000266
Train loss: 0.0968;  Loss pred: 0.0968; Loss self: 0.0000; time: 0.15s
Val loss: 0.3484 score: 0.8367 time: 0.09s
Test loss: 0.3093 score: 0.8980 time: 0.09s
Epoch 93/1000, LR 0.000265
Train loss: 0.0951;  Loss pred: 0.0951; Loss self: 0.0000; time: 0.15s
Val loss: 0.3428 score: 0.8367 time: 0.09s
Test loss: 0.2994 score: 0.8980 time: 0.09s
Epoch 94/1000, LR 0.000265
Train loss: 0.0952;  Loss pred: 0.0952; Loss self: 0.0000; time: 0.15s
Val loss: 0.3369 score: 0.8367 time: 0.09s
Test loss: 0.2890 score: 0.8980 time: 0.09s
Epoch 95/1000, LR 0.000265
Train loss: 0.0931;  Loss pred: 0.0931; Loss self: 0.0000; time: 0.15s
Val loss: 0.3317 score: 0.8571 time: 0.09s
Test loss: 0.2793 score: 0.9184 time: 0.09s
Epoch 96/1000, LR 0.000265
Train loss: 0.0890;  Loss pred: 0.0890; Loss self: 0.0000; time: 0.15s
Val loss: 0.3264 score: 0.8571 time: 0.09s
Test loss: 0.2695 score: 0.9184 time: 0.09s
Epoch 97/1000, LR 0.000265
Train loss: 0.0858;  Loss pred: 0.0858; Loss self: 0.0000; time: 0.15s
Val loss: 0.3213 score: 0.8571 time: 0.09s
Test loss: 0.2601 score: 0.9388 time: 0.09s
Epoch 98/1000, LR 0.000265
Train loss: 0.1082;  Loss pred: 0.1082; Loss self: 0.0000; time: 0.15s
Val loss: 0.3156 score: 0.8776 time: 0.09s
Test loss: 0.2500 score: 0.9184 time: 0.09s
Epoch 99/1000, LR 0.000265
Train loss: 0.0840;  Loss pred: 0.0840; Loss self: 0.0000; time: 0.15s
Val loss: 0.3106 score: 0.8776 time: 0.09s
Test loss: 0.2407 score: 0.9184 time: 0.09s
Epoch 100/1000, LR 0.000265
Train loss: 0.0843;  Loss pred: 0.0843; Loss self: 0.0000; time: 0.14s
Val loss: 0.3072 score: 0.8776 time: 0.27s
Test loss: 0.2333 score: 0.9184 time: 0.09s
Epoch 101/1000, LR 0.000265
Train loss: 0.0804;  Loss pred: 0.0804; Loss self: 0.0000; time: 0.15s
Val loss: 0.3041 score: 0.8980 time: 0.09s
Test loss: 0.2264 score: 0.9184 time: 0.09s
Epoch 102/1000, LR 0.000264
Train loss: 0.0855;  Loss pred: 0.0855; Loss self: 0.0000; time: 0.15s
Val loss: 0.3015 score: 0.8980 time: 0.09s
Test loss: 0.2197 score: 0.9184 time: 0.11s
Epoch 103/1000, LR 0.000264
Train loss: 0.0881;  Loss pred: 0.0881; Loss self: 0.0000; time: 0.15s
Val loss: 0.2996 score: 0.8980 time: 0.09s
Test loss: 0.2144 score: 0.9184 time: 0.09s
Epoch 104/1000, LR 0.000264
Train loss: 0.0873;  Loss pred: 0.0873; Loss self: 0.0000; time: 0.14s
Val loss: 0.2976 score: 0.8980 time: 0.10s
Test loss: 0.2086 score: 0.9184 time: 0.09s
Epoch 105/1000, LR 0.000264
Train loss: 0.0874;  Loss pred: 0.0874; Loss self: 0.0000; time: 0.14s
Val loss: 0.2964 score: 0.8980 time: 0.09s
Test loss: 0.2037 score: 0.9184 time: 0.09s
Epoch 106/1000, LR 0.000264
Train loss: 0.0799;  Loss pred: 0.0799; Loss self: 0.0000; time: 0.14s
Val loss: 0.2954 score: 0.8980 time: 0.09s
Test loss: 0.1987 score: 0.9184 time: 0.09s
Epoch 107/1000, LR 0.000264
Train loss: 0.0734;  Loss pred: 0.0734; Loss self: 0.0000; time: 0.14s
Val loss: 0.2950 score: 0.8980 time: 0.09s
Test loss: 0.1945 score: 0.9184 time: 0.09s
Epoch 108/1000, LR 0.000264
Train loss: 0.0758;  Loss pred: 0.0758; Loss self: 0.0000; time: 0.14s
Val loss: 0.2947 score: 0.8980 time: 0.09s
Test loss: 0.1907 score: 0.9184 time: 0.09s
Epoch 109/1000, LR 0.000264
Train loss: 0.0717;  Loss pred: 0.0717; Loss self: 0.0000; time: 0.14s
Val loss: 0.2944 score: 0.8980 time: 0.09s
Test loss: 0.1868 score: 0.9388 time: 0.09s
Epoch 110/1000, LR 0.000263
Train loss: 0.0738;  Loss pred: 0.0738; Loss self: 0.0000; time: 0.14s
Val loss: 0.2944 score: 0.8980 time: 0.09s
Test loss: 0.1831 score: 0.9388 time: 0.09s
     INFO: Early stopping counter 1 of 20
Epoch 111/1000, LR 0.000263
Train loss: 0.0665;  Loss pred: 0.0665; Loss self: 0.0000; time: 0.15s
Val loss: 0.2945 score: 0.8980 time: 0.16s
Test loss: 0.1795 score: 0.9388 time: 0.22s
     INFO: Early stopping counter 2 of 20
Epoch 112/1000, LR 0.000263
Train loss: 0.0738;  Loss pred: 0.0738; Loss self: 0.0000; time: 0.13s
Val loss: 0.2948 score: 0.8980 time: 0.09s
Test loss: 0.1764 score: 0.9388 time: 0.09s
     INFO: Early stopping counter 3 of 20
Epoch 113/1000, LR 0.000263
Train loss: 0.0666;  Loss pred: 0.0666; Loss self: 0.0000; time: 0.14s
Val loss: 0.2952 score: 0.8980 time: 0.09s
Test loss: 0.1735 score: 0.9388 time: 0.09s
     INFO: Early stopping counter 4 of 20
Epoch 114/1000, LR 0.000263
Train loss: 0.0687;  Loss pred: 0.0687; Loss self: 0.0000; time: 0.13s
Val loss: 0.2954 score: 0.8980 time: 0.09s
Test loss: 0.1706 score: 0.9388 time: 0.09s
     INFO: Early stopping counter 5 of 20
Epoch 115/1000, LR 0.000263
Train loss: 0.0707;  Loss pred: 0.0707; Loss self: 0.0000; time: 0.14s
Val loss: 0.2958 score: 0.8980 time: 0.10s
Test loss: 0.1675 score: 0.9388 time: 0.09s
     INFO: Early stopping counter 6 of 20
Epoch 116/1000, LR 0.000263
Train loss: 0.0683;  Loss pred: 0.0683; Loss self: 0.0000; time: 0.14s
Val loss: 0.2963 score: 0.8980 time: 0.09s
Test loss: 0.1648 score: 0.9388 time: 0.09s
     INFO: Early stopping counter 7 of 20
Epoch 117/1000, LR 0.000262
Train loss: 0.0658;  Loss pred: 0.0658; Loss self: 0.0000; time: 0.15s
Val loss: 0.2967 score: 0.8980 time: 0.09s
Test loss: 0.1627 score: 0.9388 time: 0.09s
     INFO: Early stopping counter 8 of 20
Epoch 118/1000, LR 0.000262
Train loss: 0.0629;  Loss pred: 0.0629; Loss self: 0.0000; time: 0.15s
Val loss: 0.2972 score: 0.8980 time: 0.09s
Test loss: 0.1606 score: 0.9388 time: 0.09s
     INFO: Early stopping counter 9 of 20
Epoch 119/1000, LR 0.000262
Train loss: 0.0619;  Loss pred: 0.0619; Loss self: 0.0000; time: 0.14s
Val loss: 0.2975 score: 0.8980 time: 0.27s
Test loss: 0.1585 score: 0.9388 time: 0.08s
     INFO: Early stopping counter 10 of 20
Epoch 120/1000, LR 0.000262
Train loss: 0.0608;  Loss pred: 0.0608; Loss self: 0.0000; time: 0.13s
Val loss: 0.2980 score: 0.8980 time: 0.09s
Test loss: 0.1565 score: 0.9592 time: 0.09s
     INFO: Early stopping counter 11 of 20
Epoch 121/1000, LR 0.000262
Train loss: 0.0636;  Loss pred: 0.0636; Loss self: 0.0000; time: 0.14s
Val loss: 0.2986 score: 0.8980 time: 0.09s
Test loss: 0.1548 score: 0.9592 time: 0.09s
     INFO: Early stopping counter 12 of 20
Epoch 122/1000, LR 0.000262
Train loss: 0.0586;  Loss pred: 0.0586; Loss self: 0.0000; time: 0.13s
Val loss: 0.2993 score: 0.8980 time: 0.09s
Test loss: 0.1531 score: 0.9592 time: 0.09s
     INFO: Early stopping counter 13 of 20
Epoch 123/1000, LR 0.000262
Train loss: 0.0640;  Loss pred: 0.0640; Loss self: 0.0000; time: 0.14s
Val loss: 0.3000 score: 0.8980 time: 0.09s
Test loss: 0.1514 score: 0.9592 time: 0.09s
     INFO: Early stopping counter 14 of 20
Epoch 124/1000, LR 0.000261
Train loss: 0.0651;  Loss pred: 0.0651; Loss self: 0.0000; time: 0.14s
Val loss: 0.3007 score: 0.8980 time: 0.09s
Test loss: 0.1502 score: 0.9592 time: 0.10s
     INFO: Early stopping counter 15 of 20
Epoch 125/1000, LR 0.000261
Train loss: 0.0615;  Loss pred: 0.0615; Loss self: 0.0000; time: 0.14s
Val loss: 0.3014 score: 0.8980 time: 0.09s
Test loss: 0.1487 score: 0.9592 time: 0.09s
     INFO: Early stopping counter 16 of 20
Epoch 126/1000, LR 0.000261
Train loss: 0.0542;  Loss pred: 0.0542; Loss self: 0.0000; time: 0.15s
Val loss: 0.3022 score: 0.8980 time: 0.09s
Test loss: 0.1475 score: 0.9592 time: 0.09s
     INFO: Early stopping counter 17 of 20
Epoch 127/1000, LR 0.000261
Train loss: 0.0568;  Loss pred: 0.0568; Loss self: 0.0000; time: 0.14s
Val loss: 0.3030 score: 0.8980 time: 0.19s
Test loss: 0.1467 score: 0.9592 time: 0.09s
     INFO: Early stopping counter 18 of 20
Epoch 128/1000, LR 0.000261
Train loss: 0.0602;  Loss pred: 0.0602; Loss self: 0.0000; time: 0.14s
Val loss: 0.3038 score: 0.8980 time: 0.09s
Test loss: 0.1454 score: 0.9592 time: 0.09s
     INFO: Early stopping counter 19 of 20
Epoch 129/1000, LR 0.000261
Train loss: 0.0558;  Loss pred: 0.0558; Loss self: 0.0000; time: 0.15s
Val loss: 0.3044 score: 0.8980 time: 0.09s
Test loss: 0.1446 score: 0.9592 time: 0.09s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 108,   Train_Loss: 0.0717,   Val_Loss: 0.2944,   Val_Precision: 0.8846,   Val_Recall: 0.9200,   Val_accuracy: 0.9020,   Val_Score: 0.8980,   Val_Loss: 0.2944,   Test_Precision: 0.9565,   Test_Recall: 0.9167,   Test_accuracy: 0.9362,   Test_Score: 0.9388,   Test_loss: 0.1868


[0.09557036589831114, 0.0942862059455365, 0.09493249002844095, 0.08603419596329331, 0.14399997401051223, 0.09640202205628157, 0.10011990601196885, 0.09626192599534988, 0.09202838293276727, 0.09758033300749958, 0.098139189183712, 0.09403934609144926, 0.1119859111495316, 0.09651086898520589, 0.09305733907967806, 0.09197020484134555, 0.09017096110619605, 0.10473718494176865, 0.09348232997581363, 0.2473335131071508, 0.10166144790127873, 0.0971588019747287, 0.0936598761472851, 0.09027533582411706, 0.0912429301533848, 0.0906289650592953, 0.0906163901090622, 2.3175381820183247, 0.09649670519866049, 0.09368237806484103, 0.09602797403931618, 0.09519176394678652, 0.09439421980641782, 0.09373134095221758, 0.09460119903087616, 0.09431733703240752, 0.09452473791316152, 0.09464783011935651, 0.09572795988060534, 0.09495457191951573, 0.09487721999175847, 0.09675538912415504, 0.09581843507476151, 0.09447879903018475, 0.09572523506358266, 0.21709277294576168, 0.08982635918073356, 0.09165733703412116, 0.08907641586847603, 0.09240778116509318, 0.08935851394198835, 0.09689472801983356, 0.09103746805340052, 0.09446587483398616, 0.09611725504510105, 0.09411596995778382, 0.09858945198357105, 0.09288656618446112, 0.09298155107535422, 0.08950469014234841, 0.09300801693461835, 0.09726476203650236, 0.0972670519258827, 0.10032848594710231, 0.09652395197190344, 0.09799121599644423, 0.10097233299165964, 0.0966276740655303, 0.09636033116839826, 0.10546067100949585, 0.09866437711752951, 0.0992555320262909, 0.09833866497501731, 0.09962834697216749, 0.09819808998145163, 0.2234561238437891, 0.09199976711533964, 0.09198854793794453, 0.09276202414184809, 0.10442222305573523, 0.091028698021546, 0.09091425710357726, 0.09403796005062759, 0.2622809619642794, 0.09389542322605848, 0.0976843279786408, 0.09577599703334272, 1.8509777300059795, 0.09570501698181033, 0.09398785303346813, 0.09468197706155479, 0.0944347670301795, 0.0962842081207782, 0.09039127896539867, 0.09364201012067497, 0.09476263914257288, 0.0924967669416219, 0.09079088317230344, 0.09250932605937123, 0.09213086008094251, 0.08929740893654525, 0.13147406792268157, 0.09389730007387698, 0.09460891713388264, 0.09344820701517165, 0.10148592898622155, 0.10226689698174596, 0.10196364508010447, 0.239697938086465, 0.0942813919391483, 0.09338195901364088, 0.09414766309782863, 0.09348623198457062, 0.09251611609943211, 0.0939382219221443, 0.09519502311013639, 0.11195819498971105, 0.09251981507986784, 0.08851631707511842, 0.08907741797156632, 0.09154917392879725, 0.09172345604747534, 0.09561106795445085, 0.2442228568252176, 0.09693623217754066, 0.09535250207409263, 0.09638634813018143, 0.09709984692744911, 0.09664492588490248, 0.09553612489253283, 0.09576063300482929, 0.09849453088827431, 0.097860855050385, 0.09932906413450837, 0.09922719211317599, 0.09959365613758564, 0.09967243787832558, 0.0997649400960654, 0.09238604293204844, 0.09443043009378016, 0.09508126182481647, 0.09581324202008545, 0.0965381080750376, 0.09524637390859425, 0.09676201897673309, 0.09594571101479232, 0.09609194588847458, 0.09660195396281779, 0.0952779040671885, 0.09392093517817557, 0.09416672005318105, 0.09540514694526792, 0.09665321605280042, 0.09411780699156225, 0.09466508612968028, 0.09470708412118256, 0.09868532908149064, 0.09505630610510707, 0.09790496597997844, 0.09501013695262372, 0.09954291488975286, 0.09245318593457341, 0.09365319390781224, 0.09429152589291334, 0.09564674808643758, 0.09738166397437453, 0.0975366709753871, 0.09654906881041825, 0.09846324403770268, 0.11127819982357323, 0.09492985415272415, 0.10039073112420738, 0.09246206399984658, 0.09058029693551362, 0.09269108902662992, 0.09517993405461311, 0.09325544093735516, 0.09253027709200978, 0.09642212302424014, 0.09622060204856098, 0.09574523800984025, 0.09493272705003619, 0.09841176890768111, 0.09353431290946901, 0.08937265002168715, 0.08935985993593931, 0.10044156503863633, 0.10409400798380375, 0.09632820705883205, 0.09836730384267867, 0.09846920799463987, 0.09753788891248405, 0.09680606215260923, 0.09615761996246874, 0.09372288407757878, 0.09966257400810719, 0.09714484680444002, 0.09596069483086467, 0.09374319203197956, 0.09364236402325332, 0.0922830319032073, 0.09186062100343406, 0.09203453501686454, 0.09457708802074194, 0.0955435298383236, 0.09845489612780511, 0.09479868109337986, 0.11260882299393415, 0.09420307981781662, 0.09351776796393096, 0.09724152600392699, 0.09521900420077145, 0.09612280898727477, 0.09383213706314564, 0.09712748718447983, 0.09648814308457077, 0.0917806860525161, 0.09915271401405334, 0.10040990705601871, 0.09939573891460896, 0.102092799032107, 0.21346629597246647, 0.10838780179619789, 0.09753713593818247, 0.09600440412759781, 0.09601914207451046, 0.09603579388931394, 0.09522227803245187, 0.09295588103123009, 0.20908631710335612, 0.09699250501580536, 0.10065579903312027, 0.0969774469267577, 0.09795219893567264, 0.0978123489767313, 0.09689250006340444, 0.23661579680629075, 0.1010393409524113, 0.10071105207316577, 0.10147450002841651, 0.09995642281137407, 0.09953904105350375, 0.09898673393763602, 0.09694906487129629, 0.09739055577665567, 0.100454097148031, 0.0995378049556166, 0.09762602392584085, 0.09685270488262177, 0.1937404579948634, 0.09551135310903192, 0.09574992093257606, 0.09185401606373489, 0.0939198168925941, 0.09228643006645143, 3.288827106822282, 0.10698687215335667, 0.092005634913221, 0.09214989095926285, 0.08887620479799807, 0.08793571079149842, 0.09388966695405543, 0.09190217382274568, 0.09060456184670329, 0.09339827205985785, 0.09692452382296324, 0.0956822622101754, 0.09356681792996824, 0.10084355296567082, 0.09721996006555855, 0.10494019719772041, 0.09686149796471, 0.10322633804753423, 0.09527316689491272, 0.09570303885266185, 0.09571130806580186, 0.09657387505285442, 0.09687659190967679, 0.10524185188114643, 0.09342251787893474, 0.1156068800482899, 0.09369900403544307, 0.09455431695096195, 0.09650145703926682, 0.09452046197839081, 0.29204190499149263, 0.09902701410464942, 0.09813256305642426, 0.09662647405639291, 0.10062401811592281, 0.09809742402285337, 0.21202697209082544, 0.09526969608850777, 0.09475633595138788, 0.0951991188339889, 0.09886971884407103, 0.09975430183112621, 0.09902146505191922, 0.0912895139772445, 0.09380925097502768, 0.09327091998420656, 0.09465026389807463, 0.09695902187377214, 0.09472212917171419, 0.09603460389189422, 0.09376587509177625, 0.09427184495143592, 0.09464065101929009, 0.09513225010596216, 0.09522040095180273, 0.0960225670132786, 0.09413435100577772, 0.09505681111477315, 0.09484602115117013, 0.09812189592048526, 0.09461520402692258, 0.09549098904244602, 0.0957693550735712, 0.09600238013081253, 0.09449497004970908, 0.09553240891546011, 0.09600279503501952, 0.09469224698841572, 0.09430065914057195, 0.0964902681298554, 0.09612730517983437, 0.09860990988090634, 0.09437486017122865, 0.09591121901758015, 0.09485197509638965, 0.0936362431384623, 0.09492641501128674, 0.0950832508970052, 0.09483057796023786, 0.09746993193402886, 0.0948236680123955, 0.0955317250918597, 0.09722068207338452, 0.09673387417569757, 0.09721265896223485, 0.0916636639740318, 0.11623634304851294, 0.09166726795956492, 0.09816201799549162, 0.0926318799611181, 0.094154282938689, 0.09355033794417977, 0.09585364302620292, 0.09562267200089991, 0.09060095809400082, 0.22522769100032747, 0.09151087212376297, 0.09311504289507866, 0.09179678093641996, 0.09391713095828891, 0.09462648490443826, 0.09531275602057576, 0.0944455829448998, 0.08960052602924407, 0.09184818691574037, 0.0940576670691371, 0.09415448596701026, 0.09701662207953632, 0.10108232195489109, 0.09390069893561304, 0.0925628358963877, 0.09413137100636959, 0.09809742099605501, 0.09505738993175328]
[0.0019504156305777784, 0.0019242082846027855, 0.0019373977556824684, 0.001755799917618231, 0.0029387749798063722, 0.001967388205230236, 0.0020432633879993644, 0.001964529101945916, 0.0018781302639340259, 0.0019914353674999915, 0.002002840595585959, 0.0019191703283969238, 0.002285426758153706, 0.001969609571126651, 0.0018991293689730217, 0.0018769429559458274, 0.0018402236960448173, 0.0021374935702401766, 0.001907802652567625, 0.005047622716472465, 0.002074723426556709, 0.00198283269336181, 0.001911426043822145, 0.0018423537923289196, 0.0018621006153752, 0.0018495707154958223, 0.001849314083858412, 0.04729669759221071, 0.0019693205142583773, 0.0019118852666294088, 0.0019597545722309425, 0.0019426890601385006, 0.0019264126491105678, 0.0019128845092289302, 0.00193063671491584, 0.001924843612906276, 0.0019290762839420717, 0.001931588369782786, 0.001953631834298068, 0.0019378484065207292, 0.0019362697957501728, 0.0019745997780439804, 0.0019554782668318674, 0.001928138755718056, 0.001953576225787401, 0.004430464753995137, 0.00183319100368844, 0.0018705578986555338, 0.001817886038132164, 0.0018858730850019017, 0.0018236431416732315, 0.001977443428976195, 0.001857907511293888, 0.0019278749966119624, 0.0019615766335734906, 0.0019207340807710983, 0.0020120296323177765, 0.0018956442078461452, 0.001897582675007229, 0.0018266263294356819, 0.001898122794584048, 0.001984995143602089, 0.0019850418760384222, 0.002047520121369435, 0.0019698765708551723, 0.0019998207346213106, 0.002060659856972646, 0.0019719933482761284, 0.001966537370783638, 0.0021522585920305277, 0.0020135587166842757, 0.0020256231025773653, 0.002006911530102394, 0.002033231570860561, 0.0020040426526826863, 0.004560329058036512, 0.0018775462676599926, 0.0018773173048560108, 0.0018931025335071038, 0.0021310657766476578, 0.0018577285310519592, 0.0018553930021138216, 0.0019191420418495427, 0.0053526726931485595, 0.001916233127062418, 0.001993557713849812, 0.001954612184353933, 0.03777505571440774, 0.00195316361187368, 0.001918119449662615, 0.0019322852461541795, 0.0019272401434730511, 0.0019649838391995553, 0.0018447199788856872, 0.001911061431034183, 0.001933931411072916, 0.0018876891212575898, 0.0018528751667817027, 0.0018879454297830863, 0.0018802216343049491, 0.0018223961007458214, 0.0026831442433200322, 0.001916271430079122, 0.0019307942272220947, 0.001907106265615748, 0.0020711414078820726, 0.0020870795302397136, 0.0020808907159204993, 0.0048917946548258165, 0.0019241100395744552, 0.0019057542655845077, 0.001921380879547523, 0.0019078822853994004, 0.0018880840020292268, 0.0019171065698396794, 0.0019427555736762528, 0.002284861122239001, 0.0018881594914258743, 0.001806455450512621, 0.0018179064892156391, 0.0018683504883428008, 0.001871907266275007, 0.0019512462847847112, 0.004984139935208523, 0.0019782904526028707, 0.001945969430083523, 0.0019670683291873763, 0.0019816295291316143, 0.0019723454262224995, 0.0019497168345414863, 0.001954298632751618, 0.002010092467107639, 0.001997160307150714, 0.0020271237578471098, 0.0020250447370035916, 0.002032523594644605, 0.002034131385271951, 0.002036019185633988, 0.0018854294475928253, 0.001927151634566942, 0.0019404339147921727, 0.001955372286124193, 0.0019701654709191347, 0.0019438035491549847, 0.0019747350811578183, 0.001958075734995762, 0.0019610601201729507, 0.0019714684482207714, 0.0019444470217793572, 0.0019167537791464403, 0.0019217697970036949, 0.0019470438152095493, 0.0019725146133224574, 0.0019207715712563724, 0.0019319405332587811, 0.0019327976351261747, 0.0020139863077855234, 0.0019399246143899402, 0.0019980605302036417, 0.0019389823867882394, 0.0020314880589745485, 0.0018867997129504777, 0.0019112896715880049, 0.0019243168549574151, 0.001951974450743624, 0.001987380897436215, 0.001990544305620145, 0.0019703891593962908, 0.0020094539599531157, 0.0022709836698688417, 0.0019373439623004928, 0.002048790431106273, 0.0018869808979560525, 0.0018485774884798697, 0.0018916548780944882, 0.0019424476337676145, 0.0019031722640276564, 0.0018883730018777506, 0.0019677984290661253, 0.001963685756093081, 0.001953984449180413, 0.0019374025928578815, 0.0020084034470955327, 0.0019088635287646735, 0.0018239316330956562, 0.001823670610937537, 0.0020498278579313537, 0.002124367509873546, 0.001965881776710858, 0.0020074959967893605, 0.0020095756733599975, 0.0019905691614792663, 0.001975633921481821, 0.001962400407397321, 0.0019127119199505874, 0.002033930081798106, 0.0019825478939681637, 0.0019583815271605035, 0.0019131263679995829, 0.001911068653535782, 0.0018833271816981081, 0.001874706551090491, 0.001878255816670705, 0.0019301446534845295, 0.001949867955884155, 0.0020092835944450025, 0.0019346669610893848, 0.0022981392447741665, 0.0019225118330166656, 0.0019085258768149177, 0.001984520938855653, 0.0019432449836892132, 0.001961689979332138, 0.0019149415727172578, 0.0019821936160097924, 0.001969145777236138, 0.001873075225561553, 0.002023524775797007, 0.002049181776653443, 0.0020284844676450807, 0.0020835265108593262, 0.004356455019846254, 0.0022119959550244467, 0.001990553794656785, 0.001959273553624445, 0.0019595743280512337, 0.0019599141610064067, 0.0019433117965806503, 0.0018970587965557162, 0.0042670676959868595, 0.001979438877873579, 0.0020541999802677607, 0.0019791315699338305, 0.001999024468074952, 0.0019961703872802307, 0.0019773979604776415, 0.0048288938123732805, 0.0020620273663757406, 0.002055327593329914, 0.002070908163845235, 0.002039926996150491, 0.002031409001091913, 0.0020201374272986942, 0.0019785523443121692, 0.0019875623627888914, 0.0020500836152659387, 0.0020313837746044204, 0.001992367835221242, 0.0019765858139310566, 0.003953886897854355, 0.0019492112879394268, 0.0019540800190321647, 0.001874571756402753, 0.0019167309569917163, 0.0018833965319683965, 0.06711892054739352, 0.002183405554150136, 0.0018776660186371633, 0.0018806100195767929, 0.001813800097918328, 0.0017946063426836413, 0.00191611565212358, 0.0018755545678111364, 0.0018490726907490467, 0.0019060871848950581, 0.0019780515065910866, 0.0019526992287790897, 0.0019095268965299641, 0.0020580316931769556, 0.00198408081766446, 0.002141636677504498, 0.0019767652645859184, 0.0021066599601537598, 0.001944350344794137, 0.0019531232418910582, 0.001953292001342895, 0.001970895409241927, 0.001977073304279118, 0.0021477928955336008, 0.0019065819975292804, 0.0023593240826181613, 0.0019122245721518993, 0.0019296799377747336, 0.001969417490597282, 0.0019289890199671595, 0.005960038877377401, 0.0020209594715234576, 0.0020027053684984544, 0.0019719688582937327, 0.002053551390120874, 0.0020019882453643544, 0.0043270810630780705, 0.0019442795120103626, 0.00193380277451812, 0.0019428391598773245, 0.002017749364164715, 0.0020358020781862493, 0.002020846225549372, 0.0018630513056580509, 0.0019144745096944425, 0.001903488162942991, 0.0019316380387362168, 0.0019787555484443295, 0.001933104676973759, 0.00195988987534478, 0.0019135892875872704, 0.001923915203090529, 0.0019314418575365323, 0.0019414744919584114, 0.0019432734888123007, 0.0019596442247607876, 0.0019211092041995451, 0.0019399349207096562, 0.001935633084717758, 0.002002487671846638, 0.0019309225311616854, 0.0019487956947437963, 0.001954476634154514, 0.0019592322475676027, 0.001928468776524675, 0.001949640998274696, 0.0019592407150003984, 0.00193249483649828, 0.0019245032477667745, 0.001969189145507253, 0.0019617817383639664, 0.00201244714042666, 0.0019260175545148704, 0.001957371816685309, 0.0019357545938038705, 0.001910943737519639, 0.0019372737757405456, 0.001940474508102147, 0.0019353179175558748, 0.001989182284367936, 0.0019351768982121531, 0.0019496270426910144, 0.0019840955525180517, 0.0019741606974632156, 0.0019839318155558134, 0.0018706870198782002, 0.0023721702662961824, 0.0018707605706033657, 0.0020033064897039105, 0.0018904465298187367, 0.0019215159783405916, 0.001909190570289383, 0.001956196796453121, 0.0019514831020591818, 0.001848999144775527, 0.004596483489802601, 0.0018675688188523054, 0.0019003069978587481, 0.0018734036925799993, 0.0019166761420058962, 0.0019311527531518011, 0.0019451582861341992, 0.0019274608764265264, 0.0018285821638621238, 0.001874452794198783, 0.0019195442259007571, 0.0019215201217757196, 0.00197993106284768, 0.0020629045296916546, 0.0019163407946043477, 0.0018890374672732183, 0.0019210483878850937, 0.0020019881835929595, 0.0019399467333010873]
[512.7112315562025, 519.6942597128616, 516.1562704751558, 569.5409767170482, 340.27783919199123, 508.28809349447823, 489.41316419276586, 509.0278372610895, 532.444431146831, 502.15036667515864, 499.2908582959074, 521.058493456022, 437.55504149599403, 507.71483580270313, 526.5570720654816, 532.7812424091924, 543.4121961092527, 467.8376646006175, 524.1632296999616, 198.11306354902268, 481.9919547829267, 504.3289851674483, 523.1696006403522, 542.7839127119552, 537.0279090952919, 540.665999748988, 540.7410286486319, 21.14312522666889, 507.7893581871248, 523.0439385952104, 510.2679764954555, 514.7504150400202, 519.0995815261615, 522.7707136397339, 517.963836631788, 519.5227255320361, 518.3828178927677, 517.7086462331794, 511.8671708988075, 516.0362372180751, 516.4569535685847, 506.43173929179227, 511.3838476047764, 518.6348736751527, 511.88174118823736, 225.70995494282127, 545.4968947523566, 534.5998649487148, 550.0894880228449, 530.2583763206906, 548.3528970928372, 505.70346809756353, 538.2399252498731, 518.7058298683238, 509.7940008483155, 520.6342772855573, 497.0105727757302, 527.5251525897957, 526.9862616110734, 547.4573446606017, 526.8363052450137, 503.77957005241893, 503.76770992646004, 488.39568879605145, 507.64601944875926, 500.0448203620419, 485.28144837504567, 507.1011019759154, 508.50800745348346, 464.6281834826175, 496.63314593909564, 493.67525416135834, 498.2780680665975, 491.8278932570145, 498.99137558842006, 219.2824217887817, 532.6100438772736, 532.6750024693878, 528.2334064322605, 469.2487725897802, 538.2917812182918, 538.9693713734583, 521.0661734220912, 186.82255712739612, 521.8571716965348, 501.61577618381233, 511.6104401705316, 26.472495700875726, 511.9898783290841, 521.3439654010566, 517.5219352268495, 518.8766970149941, 508.9100378593212, 542.08769430906, 523.2694165455705, 517.0814198861453, 529.7482454811171, 539.7017661675075, 529.6763265635776, 531.8521932493688, 548.7281275408495, 372.69707079282244, 521.8467406565209, 517.9215816481578, 524.3546298544248, 482.8255551235343, 479.1384254940894, 480.56343966032915, 204.4239528765762, 519.7207952935813, 524.7266229748111, 520.4590149952443, 524.1413517242536, 529.6374520017359, 521.6194111126678, 514.7327916850147, 437.6633617977058, 529.6162768775607, 553.5702525718132, 550.0832996264091, 535.231481587261, 534.214497703161, 512.4929681085, 200.6364213283596, 505.4869464108685, 513.8826872306411, 508.37074907973033, 504.63519305660424, 507.0105807557415, 512.8949918695088, 511.69252397829166, 497.4895515323827, 500.7109326274707, 493.3097923246886, 493.8162509336338, 491.9992085872214, 491.61032922477926, 491.15450731306055, 530.383144952321, 518.9005276301021, 515.348650823341, 511.41156448633757, 507.57157952497937, 514.455280439591, 506.39704005951234, 510.7054758544181, 509.92827283225137, 507.23611676488616, 514.2850326078327, 521.7154184745186, 520.3536872934201, 513.5991250881921, 506.9670932960154, 520.6241153110686, 517.6142757940942, 517.3847390053947, 496.5277053445061, 515.4839484906871, 500.485338098381, 515.7344423620142, 492.2500014618735, 529.997960640058, 523.2069292610914, 519.6649384553303, 512.3017873615303, 503.1748072501009, 502.37515295518864, 507.5139574490914, 497.64762961940755, 440.33782068444117, 516.1706023604364, 488.09286924482365, 529.9470710504722, 540.9564955929028, 528.6376556210535, 514.8143932510437, 525.4385106914674, 529.5563953761387, 508.18213147704284, 509.2464498951118, 511.7747996507566, 516.1549817711817, 497.9079285320672, 523.871919040306, 548.2661640681983, 548.3446374594514, 487.8458433134871, 470.7283440140381, 508.6775877607008, 498.1329983219521, 497.61748873482657, 502.3688798920519, 506.1666481460045, 509.5800002030539, 522.8178846848164, 491.6589852075668, 504.4014336513468, 510.6257315702523, 522.7046246012632, 523.2674389535092, 530.9751856808792, 533.4168163109655, 532.408839692852, 518.0958837435732, 512.855240777859, 497.6898247537908, 516.884828299809, 435.1346430700147, 520.1528452653905, 523.964601239188, 503.8999490610749, 514.603155234457, 509.76454512983355, 522.2091442617871, 504.4915854451323, 507.83441813210203, 533.8813873319996, 494.1871787095512, 487.99965498088636, 492.97887952818564, 479.9554960246518, 229.54443359208412, 452.08039270078507, 502.37275811600045, 510.393251697859, 510.31491160352385, 510.2264272056204, 514.5854626928873, 527.1317904408611, 234.35297287186032, 505.1936744186082, 486.80752098422863, 505.27211792869014, 500.244001997131, 500.95923993867757, 505.7150962967765, 207.08676538665176, 484.95961610714187, 486.54044408553983, 482.879935217993, 490.2136213144302, 492.2691587279985, 495.01582738219406, 505.4200374707011, 503.12886716008666, 487.78498230682123, 492.2752719114999, 501.9153503293509, 505.9228862981611, 252.9156816657217, 513.0280160942079, 511.74976984580684, 533.4551726731283, 521.7216304418052, 530.9556341568011, 14.898928526329435, 458.0001173392809, 532.5760758698792, 531.7423546563031, 551.3286724086549, 557.2252678571319, 521.8891661845811, 533.1756362423775, 540.8116214159795, 524.6349736384468, 505.5480085669606, 512.1116377073813, 523.6899264510088, 485.9011663014351, 504.0117272930139, 466.9326083662478, 505.87695864307614, 474.6850554500559, 514.3106038875301, 512.0004608781253, 511.95622534290646, 507.38359595887124, 505.7981400262853, 465.5942395933657, 524.4988158368691, 423.8502066618554, 522.9511295708651, 518.2206543294319, 507.7643540662988, 518.4062685940145, 167.78414043500848, 494.8144750503933, 499.3245715168571, 507.1073996905107, 486.96127343622953, 499.5034323081171, 231.10267300808314, 514.3293409320615, 517.1158161406546, 514.7106464866306, 495.601692539236, 491.2068863250827, 494.84220390304444, 536.7538709014718, 522.3365445380643, 525.3513100149232, 517.6953341912103, 505.3681344247835, 517.302560958821, 510.23274959470973, 522.5781762505787, 519.7734278483922, 517.7479177527276, 515.0724380577755, 514.595606720897, 510.29670966017784, 520.5326161646614, 515.4812098718165, 516.6268379556108, 499.37885464125134, 517.8871673315542, 513.1374226129269, 511.64592225099136, 510.40401220503867, 518.5461191661689, 512.9149422303564, 510.4018063445546, 517.4658069524369, 519.6146076450724, 507.8232338836121, 509.74070175306696, 496.9074615236798, 519.206067284201, 510.88913791220284, 516.5944088165338, 523.3016442953873, 516.1893029898359, 515.3378701058205, 516.7109708067533, 502.7191363298063, 516.7486243370657, 512.9186137158463, 504.0079842580575, 506.5443766989151, 504.04958081678956, 534.5629650357598, 421.5549002565328, 534.5419481860662, 499.1747419276819, 528.9755537787593, 520.4224223332211, 523.7821805543625, 511.19601147141765, 512.4307758262481, 540.8331327927156, 217.55761817017765, 535.4555023115771, 526.230762254095, 533.7877810109512, 521.7365511491423, 517.8254275162424, 514.0969797308354, 518.8172752196039, 546.8717893911387, 533.4890284219937, 520.956999326621, 520.4213001297524, 505.0680898766888, 484.75340744414933, 521.8278517138505, 529.3701249046568, 520.549095122436, 499.5034477203079, 515.4780710387661]
Elapsed: 0.1202975804172961~0.22238591968582894
Time per graph: 0.0024550526615774717~0.004538488156853653
Speed: 496.714244714911~73.43632249266062
Total Time: 0.0962
best val loss: 0.2943531274795532 test_score: 0.9388

Testing...
Test loss: 0.2264 score: 0.9184 time: 0.09s
test Score 0.9184
Epoch Time List: [0.3292852337472141, 0.31953307590447366, 0.3140304679982364, 0.3144084610976279, 0.36263015191070735, 0.36688820901326835, 0.31778799486346543, 0.3236396561842412, 0.3183343231212348, 0.33470472996123135, 0.33647569408640265, 0.3254747549071908, 0.4567101811990142, 0.3280171169899404, 0.3404046380892396, 0.31250299187377095, 0.3383111522998661, 0.3290527171920985, 0.31871567317284644, 0.47078991681337357, 0.3374593579210341, 0.334836712339893, 0.32385665317997336, 0.30657747900113463, 0.3217134971637279, 0.3085201538633555, 0.31387247471138835, 2.967746297828853, 3.6224805880337954, 0.32256310270167887, 0.3247767749708146, 0.327215860132128, 0.3224991981405765, 0.3207212188281119, 0.32416016003116965, 0.32632375275716186, 0.32080763997510076, 0.32509052637033165, 0.3295401050709188, 0.3224231190979481, 0.3296623891219497, 0.3329404017422348, 0.33440841315314174, 0.32746883179061115, 0.3233399090822786, 0.4459288823418319, 0.31381753901951015, 0.3143575091380626, 0.31452669110149145, 0.31824746215716004, 0.31483815982937813, 0.33134525804780424, 0.3246650437358767, 0.38334972178563476, 0.32417725096456707, 0.3209391951095313, 0.32336324895732105, 0.3160319230519235, 0.3184944309759885, 0.31466880696825683, 0.3173738941550255, 0.4747949440497905, 0.3303187408018857, 0.35663744807243347, 0.33459639293141663, 0.3340012039989233, 0.3420688270125538, 0.3316515318583697, 0.38780036685056984, 0.3405356800649315, 0.3377129202708602, 0.33389944513328373, 0.33824445586651564, 0.3390350672416389, 0.33988305018283427, 0.4629461020231247, 0.3208672869950533, 0.31842585979029536, 0.3131929330993444, 0.3362861769273877, 0.3076966009102762, 0.311138057615608, 0.3209219090640545, 0.5986058600246906, 0.32517343387007713, 0.32869174401275814, 0.32541312417015433, 3.872821615077555, 0.3293909369967878, 0.3280808157287538, 0.3328723479062319, 0.32697442383505404, 0.3261610821355134, 0.32469184789806604, 0.4616989209316671, 0.3241237439215183, 0.32779380306601524, 0.317804376129061, 0.32176613504998386, 0.3155427100136876, 0.3061078106984496, 0.34975996520370245, 0.429800073383376, 0.317694955971092, 0.320285745896399, 0.3436934482306242, 0.3385945660993457, 0.3435896879527718, 0.4914630849380046, 0.3301589253824204, 0.3220217749476433, 0.3231486240401864, 0.327604774851352, 0.32043344783596694, 0.33448246330954134, 0.33238588203676045, 0.3423533667810261, 0.37305361800827086, 0.3133628766518086, 0.3070510351099074, 0.3162237189244479, 0.31461646291427314, 0.3240599720738828, 0.47460807487368584, 0.3287365708965808, 0.3195223279763013, 0.33136449195444584, 0.3296520388685167, 0.32763148797675967, 0.3282386309001595, 0.32894634106196463, 0.438129894901067, 0.33282266021706164, 0.3357173518743366, 0.34187071095220745, 0.3385125841014087, 0.33278138493187726, 0.3371177220251411, 0.36897814087569714, 0.3241114739794284, 0.325917094014585, 0.331757253035903, 0.32884487695991993, 0.3340139801148325, 0.32922522304579616, 0.33316361904144287, 0.4518906371667981, 0.32720182999037206, 0.32716024783439934, 0.3245704749133438, 0.3257628751453012, 0.3232719290535897, 0.32816366315819323, 0.3643717900849879, 0.32380501204170287, 0.3233409677632153, 0.3246106223668903, 0.33006160613149405, 0.33459685393609107, 0.3279653931967914, 0.3406443237327039, 0.4692715499550104, 0.3181462660431862, 0.318313678028062, 0.3251092382706702, 0.3284979809541255, 0.33179710316471756, 0.3365283189341426, 0.33074077405035496, 0.35306525276973844, 0.33094779029488564, 0.3363416800275445, 0.313384389039129, 0.3167242081835866, 0.3103057760745287, 0.3207186779472977, 0.3219553849194199, 0.322518615052104, 0.3264107368886471, 0.33186207083053887, 0.33206618623808026, 0.3280190220102668, 0.3272059157025069, 0.32878297311253846, 0.3148772898130119, 0.3105273740366101, 0.3301876219920814, 0.3428978759329766, 0.3406535389367491, 0.3329032859764993, 0.32672719890251756, 0.3351822840049863, 0.33382089296355844, 0.3295020409859717, 0.33020543307065964, 0.3320440500974655, 0.32342112995684147, 0.31951086805202067, 0.3290900399442762, 0.3291367457713932, 0.3262889247853309, 0.31975220795720816, 0.3124606399796903, 0.32149834698066115, 0.3256585788913071, 0.33939504297450185, 0.3301328180823475, 0.4240346662700176, 0.33584692305885255, 0.3181366410572082, 0.3239429397508502, 0.32331082434393466, 0.33292373991571367, 0.3416322849225253, 0.3298965517897159, 0.45887578185647726, 0.324441981036216, 0.3304879569914192, 0.3397117150016129, 0.3353631941135973, 0.35664576874114573, 0.4434553300961852, 0.3530297358520329, 0.33078725123777986, 0.328251221915707, 0.330203260993585, 0.328457023948431, 0.3345922997687012, 0.3181230309419334, 0.4363577126059681, 0.3223656858317554, 0.33463753992691636, 0.32668571057729423, 0.3334426179062575, 0.3440729849971831, 0.33364550210535526, 0.4647967803757638, 0.34837832930497825, 0.3426643516868353, 0.3352172481827438, 0.34670704673044384, 0.33280296600423753, 0.40207787696272135, 0.3395528648979962, 0.33762675803154707, 0.35368857881985605, 0.34073040285147727, 0.3460255600512028, 0.34024287457577884, 0.4421560678165406, 0.3224131939932704, 0.3279751529917121, 0.33668597997166216, 0.32018539495766163, 0.32403972395695746, 3.507559360936284, 3.4437077010516077, 0.31481180083937943, 0.3130723121576011, 0.3259332599118352, 0.3070739610120654, 0.3296184199862182, 0.329990039113909, 0.45436065015383065, 0.31361445994116366, 0.31968254200182855, 0.33834887901321054, 0.3149172409903258, 0.32587141799740493, 0.3313238548580557, 0.3645177949219942, 0.31994029483757913, 0.34687967295758426, 0.32213814882561564, 0.3242691648192704, 0.31876029539853334, 0.3347005997784436, 0.32595326378941536, 0.46435659285634756, 0.3186233420856297, 0.3460032388102263, 0.3145001551602036, 0.3197800642810762, 0.3307089852169156, 0.3188666489440948, 0.5954780299216509, 0.3340416911523789, 0.33757782191969454, 0.334983145352453, 0.34608596307225525, 0.34171711979433894, 0.4495757860131562, 0.33071055286563933, 0.32759964116849005, 0.3314522069413215, 0.5009350709151477, 0.34690377302467823, 0.3404100148472935, 0.40815955703146756, 0.3155690699350089, 0.3237167198676616, 0.319825038081035, 0.32214269880205393, 0.32254230906255543, 0.32627180870622396, 0.49269925709813833, 0.3152257218025625, 0.3195849289186299, 0.32353495224379003, 0.32340359315276146, 0.33065288909710944, 0.330711015034467, 0.3244958738796413, 0.32641474390402436, 0.32964794826693833, 0.3320798820350319, 0.3263500949833542, 0.3276122158858925, 0.32667016494087875, 0.3237758360337466, 0.32580664404667914, 0.3298277137801051, 0.32685380009934306, 0.32496633916161954, 0.32904220139607787, 0.328274138038978, 0.33302450692281127, 0.3309001340530813, 0.3326482849661261, 0.3260382211301476, 0.3260171441361308, 0.3257401629816741, 0.3313770422246307, 0.32808761089108884, 0.325910592218861, 0.33189878379926085, 0.3272987501695752, 0.33043534494936466, 0.3267098779324442, 0.5050719548016787, 0.32148642279207706, 0.3454302391037345, 0.32200932898558676, 0.32874894025735557, 0.3190268750768155, 0.3164073920343071, 0.31573147187009454, 0.315775184892118, 0.3250622309278697, 0.31817785697057843, 0.5289875629823655, 0.30920457001775503, 0.31385210808366537, 0.3086707650218159, 0.32383204507641494, 0.3202203391119838, 0.3249143590219319, 0.32484757201746106, 0.4913903698325157, 0.30928887287154794, 0.31340107368305326, 0.31140520493499935, 0.3223291728645563, 0.32480282289907336, 0.3199971348512918, 0.3194503951817751, 0.4255621919874102, 0.3244019001722336, 0.32615646021440625]
Total Epoch List: [240, 129]
Total Time List: [0.10240444797091186, 0.09619483700953424]
========================k_idx:2========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: SGFormer
SGFormer(
  (trans_conv): SGModule(
    (attns): ModuleList(
      (0-1): 2 x SGFormerAttention(heads=1, head_channels=64)
    )
    (fcs): ModuleList(
      (0): Linear(in_features=14887, out_features=64, bias=True)
    )
    (bns): ModuleList(
      (0-2): 3 x LayerNorm((64,), eps=1e-05, elementwise_affine=True)
    )
  )
  (graph_conv): GraphModule(
    (convs): ModuleList(
      (0-2): 3 x GCNConv(64, 64)
    )
    (fcs): ModuleList(
      (0): Linear(in_features=14887, out_features=64, bias=True)
    )
    (bns): ModuleList(
      (0-3): 4 x BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (classifier): Linear(in_features=64, out_features=2, bias=True)
)
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7e09d87bc040>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.7241;  Loss pred: 0.7241; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7500 score: 0.4898 time: 0.09s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7338 score: 0.5000 time: 0.08s
Epoch 2/1000, LR 0.000000
Train loss: 0.7272;  Loss pred: 0.7272; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7473 score: 0.4898 time: 0.09s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7314 score: 0.5000 time: 0.08s
Epoch 3/1000, LR 0.000030
Train loss: 0.7346;  Loss pred: 0.7346; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7422 score: 0.4898 time: 0.09s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7266 score: 0.5000 time: 0.08s
Epoch 4/1000, LR 0.000060
Train loss: 0.7212;  Loss pred: 0.7212; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7343 score: 0.4898 time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7194 score: 0.5000 time: 0.09s
Epoch 5/1000, LR 0.000090
Train loss: 0.7218;  Loss pred: 0.7218; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7245 score: 0.4898 time: 0.10s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7105 score: 0.5000 time: 0.08s
Epoch 6/1000, LR 0.000120
Train loss: 0.7050;  Loss pred: 0.7050; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7137 score: 0.4898 time: 0.10s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7009 score: 0.5000 time: 0.08s
Epoch 7/1000, LR 0.000150
Train loss: 0.6948;  Loss pred: 0.6948; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7026 score: 0.4898 time: 0.10s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6910 score: 0.5000 time: 0.08s
Epoch 8/1000, LR 0.000180
Train loss: 0.6615;  Loss pred: 0.6615; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6927 score: 0.4898 time: 0.10s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6819 score: 0.5000 time: 0.09s
Epoch 9/1000, LR 0.000210
Train loss: 0.6533;  Loss pred: 0.6533; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6837 score: 0.4898 time: 0.10s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6735 score: 0.5000 time: 0.08s
Epoch 10/1000, LR 0.000240
Train loss: 0.6323;  Loss pred: 0.6323; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6754 score: 0.4898 time: 0.10s
Test loss: 0.6658 score: 0.5208 time: 0.08s
Epoch 11/1000, LR 0.000270
Train loss: 0.6088;  Loss pred: 0.6088; Loss self: 0.0000; time: 0.14s
Val loss: 0.6687 score: 0.6327 time: 0.10s
Test loss: 0.6598 score: 0.7083 time: 0.08s
Epoch 12/1000, LR 0.000270
Train loss: 0.5867;  Loss pred: 0.5867; Loss self: 0.0000; time: 0.27s
Val loss: 0.6638 score: 0.8980 time: 0.10s
Test loss: 0.6555 score: 0.9167 time: 0.09s
Epoch 13/1000, LR 0.000270
Train loss: 0.5879;  Loss pred: 0.5879; Loss self: 0.0000; time: 0.14s
Val loss: 0.6602 score: 0.7347 time: 0.10s
Test loss: 0.6523 score: 0.8125 time: 0.08s
Epoch 14/1000, LR 0.000270
Train loss: 0.5513;  Loss pred: 0.5513; Loss self: 0.0000; time: 0.15s
Val loss: 0.6569 score: 0.6531 time: 0.10s
Test loss: 0.6496 score: 0.6875 time: 0.08s
Epoch 15/1000, LR 0.000270
Train loss: 0.5536;  Loss pred: 0.5536; Loss self: 0.0000; time: 0.14s
Val loss: 0.6544 score: 0.6531 time: 0.10s
Test loss: 0.6475 score: 0.6250 time: 0.09s
Epoch 16/1000, LR 0.000270
Train loss: 0.5431;  Loss pred: 0.5431; Loss self: 0.0000; time: 0.15s
Val loss: 0.6522 score: 0.5918 time: 0.10s
Test loss: 0.6457 score: 0.6458 time: 0.09s
Epoch 17/1000, LR 0.000270
Train loss: 0.5141;  Loss pred: 0.5141; Loss self: 0.0000; time: 0.15s
Val loss: 0.6497 score: 0.5918 time: 0.10s
Test loss: 0.6434 score: 0.6250 time: 0.08s
Epoch 18/1000, LR 0.000270
Train loss: 0.5151;  Loss pred: 0.5151; Loss self: 0.0000; time: 0.14s
Val loss: 0.6467 score: 0.5918 time: 0.10s
Test loss: 0.6404 score: 0.6250 time: 0.08s
Epoch 19/1000, LR 0.000270
Train loss: 0.5000;  Loss pred: 0.5000; Loss self: 0.0000; time: 0.14s
Val loss: 0.6431 score: 0.5918 time: 0.22s
Test loss: 0.6366 score: 0.6250 time: 0.08s
Epoch 20/1000, LR 0.000270
Train loss: 0.4847;  Loss pred: 0.4847; Loss self: 0.0000; time: 0.14s
Val loss: 0.6386 score: 0.5918 time: 0.10s
Test loss: 0.6321 score: 0.6250 time: 0.09s
Epoch 21/1000, LR 0.000270
Train loss: 0.4694;  Loss pred: 0.4694; Loss self: 0.0000; time: 0.15s
Val loss: 0.6332 score: 0.5918 time: 0.10s
Test loss: 0.6266 score: 0.6250 time: 0.08s
Epoch 22/1000, LR 0.000270
Train loss: 0.4703;  Loss pred: 0.4703; Loss self: 0.0000; time: 0.14s
Val loss: 0.6271 score: 0.5918 time: 0.10s
Test loss: 0.6203 score: 0.6458 time: 0.09s
Epoch 23/1000, LR 0.000270
Train loss: 0.4382;  Loss pred: 0.4382; Loss self: 0.0000; time: 0.14s
Val loss: 0.6204 score: 0.5918 time: 0.10s
Test loss: 0.6132 score: 0.6458 time: 0.09s
Epoch 24/1000, LR 0.000270
Train loss: 0.4370;  Loss pred: 0.4370; Loss self: 0.0000; time: 0.14s
Val loss: 0.6126 score: 0.6531 time: 0.10s
Test loss: 0.6048 score: 0.6458 time: 0.09s
Epoch 25/1000, LR 0.000270
Train loss: 0.4282;  Loss pred: 0.4282; Loss self: 0.0000; time: 0.14s
Val loss: 0.6042 score: 0.6735 time: 0.09s
Test loss: 0.5958 score: 0.6667 time: 0.09s
Epoch 26/1000, LR 0.000270
Train loss: 0.4172;  Loss pred: 0.4172; Loss self: 0.0000; time: 0.15s
Val loss: 0.5952 score: 0.6939 time: 0.17s
Test loss: 0.5861 score: 0.6667 time: 0.09s
Epoch 27/1000, LR 0.000270
Train loss: 0.3952;  Loss pred: 0.3952; Loss self: 0.0000; time: 0.15s
Val loss: 0.5861 score: 0.6735 time: 0.10s
Test loss: 0.5766 score: 0.6875 time: 0.09s
Epoch 28/1000, LR 0.000270
Train loss: 0.3977;  Loss pred: 0.3977; Loss self: 0.0000; time: 0.14s
Val loss: 0.5767 score: 0.6735 time: 0.09s
Test loss: 0.5671 score: 0.6875 time: 0.08s
Epoch 29/1000, LR 0.000270
Train loss: 0.3793;  Loss pred: 0.3793; Loss self: 0.0000; time: 0.14s
Val loss: 0.5674 score: 0.6735 time: 0.10s
Test loss: 0.5579 score: 0.6875 time: 0.08s
Epoch 30/1000, LR 0.000270
Train loss: 0.3684;  Loss pred: 0.3684; Loss self: 0.0000; time: 0.14s
Val loss: 0.5586 score: 0.7143 time: 0.10s
Test loss: 0.5494 score: 0.6875 time: 0.09s
Epoch 31/1000, LR 0.000270
Train loss: 0.3466;  Loss pred: 0.3466; Loss self: 0.0000; time: 0.15s
Val loss: 0.5504 score: 0.7551 time: 0.10s
Test loss: 0.5415 score: 0.6875 time: 0.09s
Epoch 32/1000, LR 0.000270
Train loss: 0.3383;  Loss pred: 0.3383; Loss self: 0.0000; time: 0.15s
Val loss: 0.5429 score: 0.7755 time: 0.10s
Test loss: 0.5340 score: 0.7083 time: 0.09s
Epoch 33/1000, LR 0.000270
Train loss: 0.3368;  Loss pred: 0.3368; Loss self: 0.0000; time: 0.14s
Val loss: 0.5360 score: 0.7755 time: 0.10s
Test loss: 0.5273 score: 0.7083 time: 0.19s
Epoch 34/1000, LR 0.000270
Train loss: 0.3271;  Loss pred: 0.3271; Loss self: 0.0000; time: 0.14s
Val loss: 0.5297 score: 0.7755 time: 0.10s
Test loss: 0.5212 score: 0.7083 time: 0.09s
Epoch 35/1000, LR 0.000270
Train loss: 0.3307;  Loss pred: 0.3307; Loss self: 0.0000; time: 0.15s
Val loss: 0.5237 score: 0.7959 time: 0.09s
Test loss: 0.5154 score: 0.7083 time: 0.08s
Epoch 36/1000, LR 0.000270
Train loss: 0.3125;  Loss pred: 0.3125; Loss self: 0.0000; time: 0.14s
Val loss: 0.5183 score: 0.7755 time: 0.10s
Test loss: 0.5104 score: 0.7292 time: 0.08s
Epoch 37/1000, LR 0.000270
Train loss: 0.3033;  Loss pred: 0.3033; Loss self: 0.0000; time: 0.15s
Val loss: 0.5132 score: 0.7755 time: 0.10s
Test loss: 0.5055 score: 0.7292 time: 0.08s
Epoch 38/1000, LR 0.000270
Train loss: 0.2951;  Loss pred: 0.2951; Loss self: 0.0000; time: 0.14s
Val loss: 0.5088 score: 0.7551 time: 0.10s
Test loss: 0.5012 score: 0.7292 time: 0.09s
Epoch 39/1000, LR 0.000269
Train loss: 0.2789;  Loss pred: 0.2789; Loss self: 0.0000; time: 0.15s
Val loss: 0.5051 score: 0.7551 time: 0.10s
Test loss: 0.4976 score: 0.7292 time: 0.08s
Epoch 40/1000, LR 0.000269
Train loss: 0.2796;  Loss pred: 0.2796; Loss self: 0.0000; time: 0.14s
Val loss: 0.5023 score: 0.7347 time: 0.10s
Test loss: 0.4951 score: 0.7083 time: 0.08s
Epoch 41/1000, LR 0.000269
Train loss: 0.2814;  Loss pred: 0.2814; Loss self: 0.0000; time: 0.15s
Val loss: 0.4989 score: 0.7347 time: 0.18s
Test loss: 0.4919 score: 0.7083 time: 0.09s
Epoch 42/1000, LR 0.000269
Train loss: 0.2633;  Loss pred: 0.2633; Loss self: 0.0000; time: 0.14s
Val loss: 0.4959 score: 0.7347 time: 0.10s
Test loss: 0.4894 score: 0.7083 time: 0.08s
Epoch 43/1000, LR 0.000269
Train loss: 0.2692;  Loss pred: 0.2692; Loss self: 0.0000; time: 0.14s
Val loss: 0.4922 score: 0.7143 time: 0.10s
Test loss: 0.4859 score: 0.7083 time: 0.08s
Epoch 44/1000, LR 0.000269
Train loss: 0.2535;  Loss pred: 0.2535; Loss self: 0.0000; time: 0.13s
Val loss: 0.4891 score: 0.7143 time: 0.10s
Test loss: 0.4831 score: 0.7083 time: 0.09s
Epoch 45/1000, LR 0.000269
Train loss: 0.2374;  Loss pred: 0.2374; Loss self: 0.0000; time: 0.16s
Val loss: 0.4853 score: 0.7143 time: 0.10s
Test loss: 0.4795 score: 0.7292 time: 0.08s
Epoch 46/1000, LR 0.000269
Train loss: 0.2330;  Loss pred: 0.2330; Loss self: 0.0000; time: 0.14s
Val loss: 0.4812 score: 0.7143 time: 0.10s
Test loss: 0.4754 score: 0.7292 time: 0.08s
Epoch 47/1000, LR 0.000269
Train loss: 0.2386;  Loss pred: 0.2386; Loss self: 0.0000; time: 0.14s
Val loss: 0.4760 score: 0.7143 time: 0.09s
Test loss: 0.4703 score: 0.7292 time: 0.08s
Epoch 48/1000, LR 0.000269
Train loss: 0.2314;  Loss pred: 0.2314; Loss self: 0.0000; time: 0.14s
Val loss: 0.4717 score: 0.7143 time: 0.10s
Test loss: 0.4659 score: 0.7292 time: 0.08s
Epoch 49/1000, LR 0.000269
Train loss: 0.2295;  Loss pred: 0.2295; Loss self: 0.0000; time: 0.23s
Val loss: 0.4677 score: 0.7143 time: 0.10s
Test loss: 0.4618 score: 0.7292 time: 0.09s
Epoch 50/1000, LR 0.000269
Train loss: 0.2179;  Loss pred: 0.2179; Loss self: 0.0000; time: 0.14s
Val loss: 0.4647 score: 0.7143 time: 0.09s
Test loss: 0.4587 score: 0.7292 time: 0.08s
Epoch 51/1000, LR 0.000269
Train loss: 0.2197;  Loss pred: 0.2197; Loss self: 0.0000; time: 0.12s
Val loss: 0.4630 score: 0.7143 time: 0.09s
Test loss: 0.4567 score: 0.7292 time: 0.08s
Epoch 52/1000, LR 0.000269
Train loss: 0.2075;  Loss pred: 0.2075; Loss self: 0.0000; time: 0.14s
Val loss: 0.4618 score: 0.7143 time: 0.09s
Test loss: 0.4555 score: 0.7292 time: 0.08s
Epoch 53/1000, LR 0.000269
Train loss: 0.2027;  Loss pred: 0.2027; Loss self: 0.0000; time: 0.14s
Val loss: 0.4602 score: 0.7143 time: 0.09s
Test loss: 0.4537 score: 0.7292 time: 0.08s
Epoch 54/1000, LR 0.000269
Train loss: 0.1939;  Loss pred: 0.1939; Loss self: 0.0000; time: 0.14s
Val loss: 0.4576 score: 0.7143 time: 0.10s
Test loss: 0.4510 score: 0.7292 time: 0.09s
Epoch 55/1000, LR 0.000269
Train loss: 0.1998;  Loss pred: 0.1998; Loss self: 0.0000; time: 0.14s
Val loss: 0.4551 score: 0.7143 time: 0.09s
Test loss: 0.4484 score: 0.7292 time: 0.08s
Epoch 56/1000, LR 0.000269
Train loss: 0.1926;  Loss pred: 0.1926; Loss self: 0.0000; time: 0.13s
Val loss: 0.4533 score: 0.7143 time: 0.10s
Test loss: 0.4466 score: 0.7292 time: 0.21s
Epoch 57/1000, LR 0.000269
Train loss: 0.1938;  Loss pred: 0.1938; Loss self: 0.0000; time: 0.14s
Val loss: 0.4488 score: 0.7143 time: 0.10s
Test loss: 0.4423 score: 0.7292 time: 0.09s
Epoch 58/1000, LR 0.000269
Train loss: 0.1793;  Loss pred: 0.1793; Loss self: 0.0000; time: 0.14s
Val loss: 0.4460 score: 0.7143 time: 0.10s
Test loss: 0.4396 score: 0.7292 time: 0.09s
Epoch 59/1000, LR 0.000268
Train loss: 0.1821;  Loss pred: 0.1821; Loss self: 0.0000; time: 0.14s
Val loss: 0.4439 score: 0.7143 time: 0.10s
Test loss: 0.4374 score: 0.7292 time: 0.09s
Epoch 60/1000, LR 0.000268
Train loss: 0.1848;  Loss pred: 0.1848; Loss self: 0.0000; time: 0.15s
Val loss: 0.4429 score: 0.7143 time: 0.10s
Test loss: 0.4362 score: 0.7292 time: 0.11s
Epoch 61/1000, LR 0.000268
Train loss: 0.1740;  Loss pred: 0.1740; Loss self: 0.0000; time: 0.15s
Val loss: 0.4381 score: 0.7347 time: 0.10s
Test loss: 0.4316 score: 0.7292 time: 0.09s
Epoch 62/1000, LR 0.000268
Train loss: 0.1709;  Loss pred: 0.1709; Loss self: 0.0000; time: 0.14s
Val loss: 0.4336 score: 0.7347 time: 0.10s
Test loss: 0.4275 score: 0.7292 time: 0.09s
Epoch 63/1000, LR 0.000268
Train loss: 0.1638;  Loss pred: 0.1638; Loss self: 0.0000; time: 0.15s
Val loss: 0.4316 score: 0.7347 time: 0.10s
Test loss: 0.4258 score: 0.7292 time: 0.09s
Epoch 64/1000, LR 0.000268
Train loss: 0.1651;  Loss pred: 0.1651; Loss self: 0.0000; time: 0.15s
Val loss: 0.4295 score: 0.7347 time: 0.10s
Test loss: 0.4242 score: 0.7292 time: 0.09s
Epoch 65/1000, LR 0.000268
Train loss: 0.1565;  Loss pred: 0.1565; Loss self: 0.0000; time: 0.16s
Val loss: 0.4275 score: 0.7347 time: 0.20s
Test loss: 0.4226 score: 0.7292 time: 0.09s
Epoch 66/1000, LR 0.000268
Train loss: 0.1604;  Loss pred: 0.1604; Loss self: 0.0000; time: 0.14s
Val loss: 0.4263 score: 0.7347 time: 0.10s
Test loss: 0.4219 score: 0.7292 time: 0.08s
Epoch 67/1000, LR 0.000268
Train loss: 0.1536;  Loss pred: 0.1536; Loss self: 0.0000; time: 0.14s
Val loss: 0.4233 score: 0.7347 time: 0.10s
Test loss: 0.4192 score: 0.7292 time: 0.08s
Epoch 68/1000, LR 0.000268
Train loss: 0.1551;  Loss pred: 0.1551; Loss self: 0.0000; time: 0.18s
Val loss: 0.4220 score: 0.7347 time: 0.10s
Test loss: 0.4183 score: 0.7292 time: 0.09s
Epoch 69/1000, LR 0.000268
Train loss: 0.1448;  Loss pred: 0.1448; Loss self: 0.0000; time: 0.15s
Val loss: 0.4196 score: 0.7551 time: 0.10s
Test loss: 0.4163 score: 0.7292 time: 0.09s
Epoch 70/1000, LR 0.000268
Train loss: 0.1452;  Loss pred: 0.1452; Loss self: 0.0000; time: 0.15s
Val loss: 0.4156 score: 0.7551 time: 0.10s
Test loss: 0.4125 score: 0.7292 time: 0.09s
Epoch 71/1000, LR 0.000268
Train loss: 0.1399;  Loss pred: 0.1399; Loss self: 0.0000; time: 0.15s
Val loss: 0.4097 score: 0.7551 time: 0.10s
Test loss: 0.4070 score: 0.7292 time: 0.09s
Epoch 72/1000, LR 0.000267
Train loss: 0.1318;  Loss pred: 0.1318; Loss self: 0.0000; time: 0.14s
Val loss: 0.4040 score: 0.7551 time: 0.21s
Test loss: 0.4015 score: 0.7500 time: 0.09s
Epoch 73/1000, LR 0.000267
Train loss: 0.1308;  Loss pred: 0.1308; Loss self: 0.0000; time: 0.14s
Val loss: 0.3988 score: 0.7755 time: 0.10s
Test loss: 0.3965 score: 0.7500 time: 0.09s
Epoch 74/1000, LR 0.000267
Train loss: 0.1302;  Loss pred: 0.1302; Loss self: 0.0000; time: 0.14s
Val loss: 0.3938 score: 0.7755 time: 0.10s
Test loss: 0.3917 score: 0.7500 time: 0.09s
Epoch 75/1000, LR 0.000267
Train loss: 0.1345;  Loss pred: 0.1345; Loss self: 0.0000; time: 0.14s
Val loss: 0.3872 score: 0.7755 time: 0.22s
Test loss: 0.3854 score: 0.7500 time: 0.10s
Epoch 76/1000, LR 0.000267
Train loss: 0.1349;  Loss pred: 0.1349; Loss self: 0.0000; time: 0.15s
Val loss: 0.3783 score: 0.7755 time: 0.11s
Test loss: 0.3770 score: 0.7708 time: 0.09s
Epoch 77/1000, LR 0.000267
Train loss: 0.1143;  Loss pred: 0.1143; Loss self: 0.0000; time: 0.16s
Val loss: 0.3699 score: 0.7755 time: 0.10s
Test loss: 0.3691 score: 0.7917 time: 0.09s
Epoch 78/1000, LR 0.000267
Train loss: 0.1237;  Loss pred: 0.1237; Loss self: 0.0000; time: 0.16s
Val loss: 0.3627 score: 0.8163 time: 0.10s
Test loss: 0.3616 score: 0.7917 time: 0.09s
Epoch 79/1000, LR 0.000267
Train loss: 0.1316;  Loss pred: 0.1316; Loss self: 0.0000; time: 0.15s
Val loss: 0.3549 score: 0.8163 time: 0.09s
Test loss: 0.3536 score: 0.7917 time: 0.23s
Epoch 80/1000, LR 0.000267
Train loss: 0.1154;  Loss pred: 0.1154; Loss self: 0.0000; time: 0.14s
Val loss: 0.3503 score: 0.8163 time: 0.10s
Test loss: 0.3485 score: 0.8125 time: 0.08s
Epoch 81/1000, LR 0.000267
Train loss: 0.1125;  Loss pred: 0.1125; Loss self: 0.0000; time: 0.14s
Val loss: 0.3484 score: 0.8163 time: 0.10s
Test loss: 0.3462 score: 0.8125 time: 0.08s
Epoch 82/1000, LR 0.000267
Train loss: 0.1121;  Loss pred: 0.1121; Loss self: 0.0000; time: 0.13s
Val loss: 0.3453 score: 0.8163 time: 0.09s
Test loss: 0.3428 score: 0.8125 time: 0.08s
Epoch 83/1000, LR 0.000266
Train loss: 0.1096;  Loss pred: 0.1096; Loss self: 0.0000; time: 0.14s
Val loss: 0.3417 score: 0.8163 time: 0.10s
Test loss: 0.3389 score: 0.8333 time: 0.09s
Epoch 84/1000, LR 0.000266
Train loss: 0.1045;  Loss pred: 0.1045; Loss self: 0.0000; time: 0.15s
Val loss: 0.3357 score: 0.8163 time: 0.10s
Test loss: 0.3330 score: 0.8333 time: 0.09s
Epoch 85/1000, LR 0.000266
Train loss: 0.0989;  Loss pred: 0.0989; Loss self: 0.0000; time: 0.14s
Val loss: 0.3278 score: 0.8367 time: 0.10s
Test loss: 0.3254 score: 0.8542 time: 0.08s
Epoch 86/1000, LR 0.000266
Train loss: 0.1044;  Loss pred: 0.1044; Loss self: 0.0000; time: 0.15s
Val loss: 0.3188 score: 0.8571 time: 0.10s
Test loss: 0.3167 score: 0.8542 time: 0.09s
Epoch 87/1000, LR 0.000266
Train loss: 0.1080;  Loss pred: 0.1080; Loss self: 0.0000; time: 0.31s
Val loss: 0.3119 score: 0.8571 time: 0.10s
Test loss: 0.3098 score: 0.8542 time: 0.09s
Epoch 88/1000, LR 0.000266
Train loss: 0.1092;  Loss pred: 0.1092; Loss self: 0.0000; time: 0.15s
Val loss: 0.3058 score: 0.8571 time: 0.10s
Test loss: 0.3040 score: 0.8542 time: 0.09s
Epoch 89/1000, LR 0.000266
Train loss: 0.0989;  Loss pred: 0.0989; Loss self: 0.0000; time: 0.15s
Val loss: 0.2962 score: 0.8776 time: 0.10s
Test loss: 0.2947 score: 0.8542 time: 0.20s
Epoch 90/1000, LR 0.000266
Train loss: 0.0938;  Loss pred: 0.0938; Loss self: 0.0000; time: 0.14s
Val loss: 0.2895 score: 0.8776 time: 0.10s
Test loss: 0.2882 score: 0.8542 time: 0.08s
Epoch 91/1000, LR 0.000266
Train loss: 0.0927;  Loss pred: 0.0927; Loss self: 0.0000; time: 0.14s
Val loss: 0.2808 score: 0.8980 time: 0.10s
Test loss: 0.2802 score: 0.8542 time: 0.09s
Epoch 92/1000, LR 0.000266
Train loss: 0.0892;  Loss pred: 0.0892; Loss self: 0.0000; time: 0.15s
Val loss: 0.2737 score: 0.8980 time: 0.09s
Test loss: 0.2735 score: 0.8542 time: 0.09s
Epoch 93/1000, LR 0.000265
Train loss: 0.0907;  Loss pred: 0.0907; Loss self: 0.0000; time: 0.14s
Val loss: 0.2672 score: 0.8980 time: 0.10s
Test loss: 0.2678 score: 0.8542 time: 0.20s
Epoch 94/1000, LR 0.000265
Train loss: 0.0904;  Loss pred: 0.0904; Loss self: 0.0000; time: 0.13s
Val loss: 0.2639 score: 0.8980 time: 0.09s
Test loss: 0.2651 score: 0.8542 time: 0.08s
Epoch 95/1000, LR 0.000265
Train loss: 0.0811;  Loss pred: 0.0811; Loss self: 0.0000; time: 0.14s
Val loss: 0.2615 score: 0.8980 time: 0.09s
Test loss: 0.2634 score: 0.8542 time: 0.08s
Epoch 96/1000, LR 0.000265
Train loss: 0.0911;  Loss pred: 0.0911; Loss self: 0.0000; time: 0.13s
Val loss: 0.2623 score: 0.8980 time: 0.09s
Test loss: 0.2643 score: 0.8542 time: 0.08s
     INFO: Early stopping counter 1 of 20
Epoch 97/1000, LR 0.000265
Train loss: 0.0839;  Loss pred: 0.0839; Loss self: 0.0000; time: 0.14s
Val loss: 0.2635 score: 0.8980 time: 0.09s
Test loss: 0.2651 score: 0.8542 time: 0.08s
     INFO: Early stopping counter 2 of 20
Epoch 98/1000, LR 0.000265
Train loss: 0.0897;  Loss pred: 0.0897; Loss self: 0.0000; time: 0.14s
Val loss: 0.2578 score: 0.8980 time: 0.10s
Test loss: 0.2601 score: 0.8542 time: 0.08s
Epoch 99/1000, LR 0.000265
Train loss: 0.0832;  Loss pred: 0.0832; Loss self: 0.0000; time: 0.14s
Val loss: 0.2507 score: 0.8980 time: 0.10s
Test loss: 0.2537 score: 0.8542 time: 0.08s
Epoch 100/1000, LR 0.000265
Train loss: 0.0761;  Loss pred: 0.0761; Loss self: 0.0000; time: 0.14s
Val loss: 0.2435 score: 0.8980 time: 0.10s
Test loss: 0.2471 score: 0.8750 time: 0.08s
Epoch 101/1000, LR 0.000265
Train loss: 0.0927;  Loss pred: 0.0927; Loss self: 0.0000; time: 0.14s
Val loss: 0.2376 score: 0.9184 time: 0.19s
Test loss: 0.2425 score: 0.8750 time: 0.08s
Epoch 102/1000, LR 0.000264
Train loss: 0.0745;  Loss pred: 0.0745; Loss self: 0.0000; time: 0.14s
Val loss: 0.2343 score: 0.9184 time: 0.10s
Test loss: 0.2402 score: 0.8750 time: 0.08s
Epoch 103/1000, LR 0.000264
Train loss: 0.0740;  Loss pred: 0.0740; Loss self: 0.0000; time: 0.15s
Val loss: 0.2337 score: 0.9184 time: 0.10s
Test loss: 0.2399 score: 0.8750 time: 0.08s
Epoch 104/1000, LR 0.000264
Train loss: 0.0729;  Loss pred: 0.0729; Loss self: 0.0000; time: 0.14s
Val loss: 0.2306 score: 0.9184 time: 0.10s
Test loss: 0.2367 score: 0.8750 time: 0.08s
Epoch 105/1000, LR 0.000264
Train loss: 0.0731;  Loss pred: 0.0731; Loss self: 0.0000; time: 0.14s
Val loss: 0.2278 score: 0.9184 time: 0.10s
Test loss: 0.2350 score: 0.8750 time: 0.08s
Epoch 106/1000, LR 0.000264
Train loss: 0.0744;  Loss pred: 0.0744; Loss self: 0.0000; time: 0.15s
Val loss: 0.2261 score: 0.9184 time: 0.10s
Test loss: 0.2333 score: 0.8750 time: 0.09s
Epoch 107/1000, LR 0.000264
Train loss: 0.0633;  Loss pred: 0.0633; Loss self: 0.0000; time: 0.15s
Val loss: 0.2196 score: 0.9184 time: 0.10s
Test loss: 0.2274 score: 0.8750 time: 0.09s
Epoch 108/1000, LR 0.000264
Train loss: 0.0706;  Loss pred: 0.0706; Loss self: 0.0000; time: 0.15s
Val loss: 0.2161 score: 0.9184 time: 0.10s
Test loss: 0.2252 score: 0.8750 time: 0.08s
Epoch 109/1000, LR 0.000264
Train loss: 0.0707;  Loss pred: 0.0707; Loss self: 0.0000; time: 0.14s
Val loss: 0.2094 score: 0.9184 time: 0.10s
Test loss: 0.2183 score: 0.8750 time: 0.08s
Epoch 110/1000, LR 0.000263
Train loss: 0.0591;  Loss pred: 0.0591; Loss self: 0.0000; time: 0.14s
Val loss: 0.2029 score: 0.9388 time: 0.10s
Test loss: 0.2105 score: 0.8750 time: 0.08s
Epoch 111/1000, LR 0.000263
Train loss: 0.0600;  Loss pred: 0.0600; Loss self: 0.0000; time: 0.14s
Val loss: 0.2094 score: 0.9184 time: 0.11s
Test loss: 0.2193 score: 0.8750 time: 0.08s
     INFO: Early stopping counter 1 of 20
Epoch 112/1000, LR 0.000263
Train loss: 0.0631;  Loss pred: 0.0631; Loss self: 0.0000; time: 0.14s
Val loss: 0.2089 score: 0.9184 time: 0.10s
Test loss: 0.2185 score: 0.8750 time: 0.08s
     INFO: Early stopping counter 2 of 20
Epoch 113/1000, LR 0.000263
Train loss: 0.0613;  Loss pred: 0.0613; Loss self: 0.0000; time: 0.16s
Val loss: 0.2025 score: 0.9388 time: 0.10s
Test loss: 0.2103 score: 0.8750 time: 0.09s
Epoch 114/1000, LR 0.000263
Train loss: 0.0621;  Loss pred: 0.0621; Loss self: 0.0000; time: 0.15s
Val loss: 0.2006 score: 0.9388 time: 0.10s
Test loss: 0.2094 score: 0.8750 time: 0.09s
Epoch 115/1000, LR 0.000263
Train loss: 0.0547;  Loss pred: 0.0547; Loss self: 0.0000; time: 0.15s
Val loss: 0.2029 score: 0.9184 time: 0.10s
Test loss: 0.2137 score: 0.8750 time: 0.09s
     INFO: Early stopping counter 1 of 20
Epoch 116/1000, LR 0.000263
Train loss: 0.0610;  Loss pred: 0.0610; Loss self: 0.0000; time: 0.15s
Val loss: 0.2082 score: 0.9184 time: 0.10s
Test loss: 0.2201 score: 0.8958 time: 0.09s
     INFO: Early stopping counter 2 of 20
Epoch 117/1000, LR 0.000262
Train loss: 0.0587;  Loss pred: 0.0587; Loss self: 0.0000; time: 0.15s
Val loss: 0.2078 score: 0.9184 time: 0.10s
Test loss: 0.2210 score: 0.8958 time: 0.09s
     INFO: Early stopping counter 3 of 20
Epoch 118/1000, LR 0.000262
Train loss: 0.0599;  Loss pred: 0.0599; Loss self: 0.0000; time: 0.14s
Val loss: 0.2032 score: 0.9184 time: 0.11s
Test loss: 0.2160 score: 0.8958 time: 0.17s
     INFO: Early stopping counter 4 of 20
Epoch 119/1000, LR 0.000262
Train loss: 0.0505;  Loss pred: 0.0505; Loss self: 0.0000; time: 0.14s
Val loss: 0.2008 score: 0.9184 time: 0.10s
Test loss: 0.2115 score: 0.8750 time: 0.08s
     INFO: Early stopping counter 5 of 20
Epoch 120/1000, LR 0.000262
Train loss: 0.0610;  Loss pred: 0.0610; Loss self: 0.0000; time: 0.14s
Val loss: 0.2025 score: 0.9184 time: 0.10s
Test loss: 0.2123 score: 0.8750 time: 0.08s
     INFO: Early stopping counter 6 of 20
Epoch 121/1000, LR 0.000262
Train loss: 0.0563;  Loss pred: 0.0563; Loss self: 0.0000; time: 0.15s
Val loss: 0.2013 score: 0.9184 time: 0.10s
Test loss: 0.2105 score: 0.8750 time: 0.08s
     INFO: Early stopping counter 7 of 20
Epoch 122/1000, LR 0.000262
Train loss: 0.0504;  Loss pred: 0.0504; Loss self: 0.0000; time: 0.14s
Val loss: 0.2003 score: 0.9388 time: 0.10s
Test loss: 0.2089 score: 0.8750 time: 0.08s
Epoch 123/1000, LR 0.000262
Train loss: 0.0472;  Loss pred: 0.0472; Loss self: 0.0000; time: 0.15s
Val loss: 0.1980 score: 0.9388 time: 0.10s
Test loss: 0.2057 score: 0.8750 time: 0.09s
Epoch 124/1000, LR 0.000261
Train loss: 0.0462;  Loss pred: 0.0462; Loss self: 0.0000; time: 0.15s
Val loss: 0.1983 score: 0.9388 time: 0.10s
Test loss: 0.2064 score: 0.8750 time: 0.09s
     INFO: Early stopping counter 1 of 20
Epoch 125/1000, LR 0.000261
Train loss: 0.0475;  Loss pred: 0.0475; Loss self: 0.0000; time: 0.33s
Val loss: 0.2007 score: 0.9184 time: 0.11s
Test loss: 0.2094 score: 0.8750 time: 0.09s
     INFO: Early stopping counter 2 of 20
Epoch 126/1000, LR 0.000261
Train loss: 0.0500;  Loss pred: 0.0500; Loss self: 0.0000; time: 0.14s
Val loss: 0.2015 score: 0.9184 time: 0.10s
Test loss: 0.2109 score: 0.8750 time: 0.08s
     INFO: Early stopping counter 3 of 20
Epoch 127/1000, LR 0.000261
Train loss: 0.0479;  Loss pred: 0.0479; Loss self: 0.0000; time: 0.14s
Val loss: 0.1970 score: 0.9388 time: 0.10s
Test loss: 0.2084 score: 0.8750 time: 0.09s
Epoch 128/1000, LR 0.000261
Train loss: 0.0469;  Loss pred: 0.0469; Loss self: 0.0000; time: 0.15s
Val loss: 0.1926 score: 0.9388 time: 0.10s
Test loss: 0.2065 score: 0.8958 time: 0.09s
Epoch 129/1000, LR 0.000261
Train loss: 0.0414;  Loss pred: 0.0414; Loss self: 0.0000; time: 0.14s
Val loss: 0.1927 score: 0.9388 time: 0.10s
Test loss: 0.2071 score: 0.8958 time: 0.09s
     INFO: Early stopping counter 1 of 20
Epoch 130/1000, LR 0.000260
Train loss: 0.0416;  Loss pred: 0.0416; Loss self: 0.0000; time: 0.13s
Val loss: 0.1891 score: 0.9388 time: 0.09s
Test loss: 0.2030 score: 0.8958 time: 0.08s
Epoch 131/1000, LR 0.000260
Train loss: 0.0486;  Loss pred: 0.0486; Loss self: 0.0000; time: 0.14s
Val loss: 0.1893 score: 0.9388 time: 0.09s
Test loss: 0.2037 score: 0.8958 time: 0.08s
     INFO: Early stopping counter 1 of 20
Epoch 132/1000, LR 0.000260
Train loss: 0.0392;  Loss pred: 0.0392; Loss self: 0.0000; time: 0.13s
Val loss: 0.1919 score: 0.9388 time: 0.09s
Test loss: 0.2091 score: 0.8958 time: 0.08s
     INFO: Early stopping counter 2 of 20
Epoch 133/1000, LR 0.000260
Train loss: 0.0405;  Loss pred: 0.0405; Loss self: 0.0000; time: 0.13s
Val loss: 0.1935 score: 0.9388 time: 0.09s
Test loss: 0.2130 score: 0.8958 time: 0.08s
     INFO: Early stopping counter 3 of 20
Epoch 134/1000, LR 0.000260
Train loss: 0.0375;  Loss pred: 0.0375; Loss self: 0.0000; time: 0.14s
Val loss: 0.1885 score: 0.9388 time: 0.09s
Test loss: 0.2099 score: 0.8958 time: 0.08s
Epoch 135/1000, LR 0.000260
Train loss: 0.0436;  Loss pred: 0.0436; Loss self: 0.0000; time: 0.14s
Val loss: 0.1874 score: 0.9388 time: 0.10s
Test loss: 0.2078 score: 0.8958 time: 0.08s
Epoch 136/1000, LR 0.000260
Train loss: 0.0345;  Loss pred: 0.0345; Loss self: 0.0000; time: 0.14s
Val loss: 0.1869 score: 0.9388 time: 0.09s
Test loss: 0.2091 score: 0.8958 time: 0.08s
Epoch 137/1000, LR 0.000259
Train loss: 0.0394;  Loss pred: 0.0394; Loss self: 0.0000; time: 0.15s
Val loss: 0.1835 score: 0.9388 time: 0.10s
Test loss: 0.2072 score: 0.8958 time: 0.08s
Epoch 138/1000, LR 0.000259
Train loss: 0.0372;  Loss pred: 0.0372; Loss self: 0.0000; time: 0.15s
Val loss: 0.1834 score: 0.9388 time: 0.09s
Test loss: 0.2103 score: 0.8958 time: 0.08s
Epoch 139/1000, LR 0.000259
Train loss: 0.0372;  Loss pred: 0.0372; Loss self: 0.0000; time: 0.15s
Val loss: 0.1796 score: 0.9388 time: 0.09s
Test loss: 0.2091 score: 0.8958 time: 0.08s
Epoch 140/1000, LR 0.000259
Train loss: 0.0408;  Loss pred: 0.0408; Loss self: 0.0000; time: 0.14s
Val loss: 0.1811 score: 0.9388 time: 0.09s
Test loss: 0.2095 score: 0.8958 time: 0.08s
     INFO: Early stopping counter 1 of 20
Epoch 141/1000, LR 0.000259
Train loss: 0.0307;  Loss pred: 0.0307; Loss self: 0.0000; time: 0.14s
Val loss: 0.1824 score: 0.9388 time: 0.09s
Test loss: 0.2118 score: 0.8958 time: 0.08s
     INFO: Early stopping counter 2 of 20
Epoch 142/1000, LR 0.000259
Train loss: 0.0349;  Loss pred: 0.0349; Loss self: 0.0000; time: 0.15s
Val loss: 0.1840 score: 0.9388 time: 0.10s
Test loss: 0.2130 score: 0.8958 time: 0.08s
     INFO: Early stopping counter 3 of 20
Epoch 143/1000, LR 0.000258
Train loss: 0.0300;  Loss pred: 0.0300; Loss self: 0.0000; time: 0.13s
Val loss: 0.1811 score: 0.9388 time: 0.09s
Test loss: 0.2082 score: 0.8958 time: 0.21s
     INFO: Early stopping counter 4 of 20
Epoch 144/1000, LR 0.000258
Train loss: 0.0332;  Loss pred: 0.0332; Loss self: 0.0000; time: 0.14s
Val loss: 0.1804 score: 0.9388 time: 0.10s
Test loss: 0.2043 score: 0.8958 time: 0.08s
     INFO: Early stopping counter 5 of 20
Epoch 145/1000, LR 0.000258
Train loss: 0.0328;  Loss pred: 0.0328; Loss self: 0.0000; time: 0.14s
Val loss: 0.1828 score: 0.9388 time: 0.09s
Test loss: 0.2104 score: 0.8958 time: 0.10s
     INFO: Early stopping counter 6 of 20
Epoch 146/1000, LR 0.000258
Train loss: 0.0336;  Loss pred: 0.0336; Loss self: 0.0000; time: 0.15s
Val loss: 0.1854 score: 0.9388 time: 0.10s
Test loss: 0.2156 score: 0.8958 time: 0.08s
     INFO: Early stopping counter 7 of 20
Epoch 147/1000, LR 0.000258
Train loss: 0.0297;  Loss pred: 0.0297; Loss self: 0.0000; time: 0.15s
Val loss: 0.1763 score: 0.9388 time: 0.10s
Test loss: 0.2094 score: 0.8958 time: 0.08s
Epoch 148/1000, LR 0.000257
Train loss: 0.0270;  Loss pred: 0.0270; Loss self: 0.0000; time: 0.14s
Val loss: 0.1750 score: 0.9388 time: 0.10s
Test loss: 0.2056 score: 0.8958 time: 0.14s
Epoch 149/1000, LR 0.000257
Train loss: 0.0321;  Loss pred: 0.0321; Loss self: 0.0000; time: 1.45s
Val loss: 0.1745 score: 0.9388 time: 3.63s
Test loss: 0.2074 score: 0.8958 time: 1.22s
Epoch 150/1000, LR 0.000257
Train loss: 0.0289;  Loss pred: 0.0289; Loss self: 0.0000; time: 0.13s
Val loss: 0.1793 score: 0.9388 time: 0.10s
Test loss: 0.2127 score: 0.8750 time: 0.08s
     INFO: Early stopping counter 1 of 20
Epoch 151/1000, LR 0.000257
Train loss: 0.0268;  Loss pred: 0.0268; Loss self: 0.0000; time: 0.14s
Val loss: 0.1825 score: 0.9388 time: 0.18s
Test loss: 0.2146 score: 0.8750 time: 0.08s
     INFO: Early stopping counter 2 of 20
Epoch 152/1000, LR 0.000257
Train loss: 0.0321;  Loss pred: 0.0321; Loss self: 0.0000; time: 0.14s
Val loss: 0.1799 score: 0.9388 time: 0.10s
Test loss: 0.2131 score: 0.8750 time: 0.09s
     INFO: Early stopping counter 3 of 20
Epoch 153/1000, LR 0.000257
Train loss: 0.0299;  Loss pred: 0.0299; Loss self: 0.0000; time: 0.14s
Val loss: 0.1749 score: 0.9388 time: 0.11s
Test loss: 0.2081 score: 0.8958 time: 0.08s
     INFO: Early stopping counter 4 of 20
Epoch 154/1000, LR 0.000256
Train loss: 0.0252;  Loss pred: 0.0252; Loss self: 0.0000; time: 0.14s
Val loss: 0.1760 score: 0.9388 time: 0.10s
Test loss: 0.2121 score: 0.8750 time: 0.08s
     INFO: Early stopping counter 5 of 20
Epoch 155/1000, LR 0.000256
Train loss: 0.0271;  Loss pred: 0.0271; Loss self: 0.0000; time: 0.14s
Val loss: 0.1718 score: 0.9388 time: 0.10s
Test loss: 0.2097 score: 0.8750 time: 0.08s
Epoch 156/1000, LR 0.000256
Train loss: 0.0241;  Loss pred: 0.0241; Loss self: 0.0000; time: 0.15s
Val loss: 0.1702 score: 0.9388 time: 0.09s
Test loss: 0.2069 score: 0.8750 time: 0.09s
Epoch 157/1000, LR 0.000256
Train loss: 0.0255;  Loss pred: 0.0255; Loss self: 0.0000; time: 0.15s
Val loss: 0.1715 score: 0.9388 time: 0.10s
Test loss: 0.2089 score: 0.8750 time: 0.08s
     INFO: Early stopping counter 1 of 20
Epoch 158/1000, LR 0.000256
Train loss: 0.0251;  Loss pred: 0.0251; Loss self: 0.0000; time: 0.14s
Val loss: 0.1723 score: 0.9388 time: 0.09s
Test loss: 0.2116 score: 0.8750 time: 0.12s
     INFO: Early stopping counter 2 of 20
Epoch 159/1000, LR 0.000255
Train loss: 0.0290;  Loss pred: 0.0290; Loss self: 0.0000; time: 0.20s
Val loss: 0.1750 score: 0.9388 time: 0.10s
Test loss: 0.2151 score: 0.8750 time: 0.08s
     INFO: Early stopping counter 3 of 20
Epoch 160/1000, LR 0.000255
Train loss: 0.0249;  Loss pred: 0.0249; Loss self: 0.0000; time: 0.14s
Val loss: 0.1726 score: 0.9388 time: 0.09s
Test loss: 0.2119 score: 0.8958 time: 0.08s
     INFO: Early stopping counter 4 of 20
Epoch 161/1000, LR 0.000255
Train loss: 0.0246;  Loss pred: 0.0246; Loss self: 0.0000; time: 0.14s
Val loss: 0.1713 score: 0.9388 time: 0.09s
Test loss: 0.2120 score: 0.8958 time: 0.08s
     INFO: Early stopping counter 5 of 20
Epoch 162/1000, LR 0.000255
Train loss: 0.0209;  Loss pred: 0.0209; Loss self: 0.0000; time: 0.14s
Val loss: 0.1736 score: 0.9388 time: 0.10s
Test loss: 0.2151 score: 0.8958 time: 0.08s
     INFO: Early stopping counter 6 of 20
Epoch 163/1000, LR 0.000255
Train loss: 0.0238;  Loss pred: 0.0238; Loss self: 0.0000; time: 0.14s
Val loss: 0.1746 score: 0.9388 time: 0.10s
Test loss: 0.2137 score: 0.8958 time: 0.08s
     INFO: Early stopping counter 7 of 20
Epoch 164/1000, LR 0.000254
Train loss: 0.0236;  Loss pred: 0.0236; Loss self: 0.0000; time: 0.14s
Val loss: 0.1777 score: 0.9388 time: 0.10s
Test loss: 0.2167 score: 0.8958 time: 0.09s
     INFO: Early stopping counter 8 of 20
Epoch 165/1000, LR 0.000254
Train loss: 0.0226;  Loss pred: 0.0226; Loss self: 0.0000; time: 0.14s
Val loss: 0.1760 score: 0.9388 time: 0.10s
Test loss: 0.2140 score: 0.8958 time: 0.08s
     INFO: Early stopping counter 9 of 20
Epoch 166/1000, LR 0.000254
Train loss: 0.0246;  Loss pred: 0.0246; Loss self: 0.0000; time: 0.15s
Val loss: 0.1753 score: 0.9388 time: 0.17s
Test loss: 0.2147 score: 0.8958 time: 0.08s
     INFO: Early stopping counter 10 of 20
Epoch 167/1000, LR 0.000254
Train loss: 0.0248;  Loss pred: 0.0248; Loss self: 0.0000; time: 0.14s
Val loss: 0.1778 score: 0.9388 time: 0.09s
Test loss: 0.2168 score: 0.8750 time: 0.08s
     INFO: Early stopping counter 11 of 20
Epoch 168/1000, LR 0.000254
Train loss: 0.0217;  Loss pred: 0.0217; Loss self: 0.0000; time: 0.14s
Val loss: 0.1767 score: 0.9388 time: 0.09s
Test loss: 0.2156 score: 0.8750 time: 0.08s
     INFO: Early stopping counter 12 of 20
Epoch 169/1000, LR 0.000253
Train loss: 0.0207;  Loss pred: 0.0207; Loss self: 0.0000; time: 0.15s
Val loss: 0.1738 score: 0.9388 time: 0.09s
Test loss: 0.2144 score: 0.8958 time: 0.08s
     INFO: Early stopping counter 13 of 20
Epoch 170/1000, LR 0.000253
Train loss: 0.0221;  Loss pred: 0.0221; Loss self: 0.0000; time: 0.14s
Val loss: 0.1739 score: 0.9388 time: 0.09s
Test loss: 0.2135 score: 0.8958 time: 0.08s
     INFO: Early stopping counter 14 of 20
Epoch 171/1000, LR 0.000253
Train loss: 0.0188;  Loss pred: 0.0188; Loss self: 0.0000; time: 0.14s
Val loss: 0.1728 score: 0.9388 time: 0.09s
Test loss: 0.2144 score: 0.8958 time: 0.08s
     INFO: Early stopping counter 15 of 20
Epoch 172/1000, LR 0.000253
Train loss: 0.0215;  Loss pred: 0.0215; Loss self: 0.0000; time: 0.14s
Val loss: 0.1727 score: 0.9388 time: 0.10s
Test loss: 0.2155 score: 0.8750 time: 0.08s
     INFO: Early stopping counter 16 of 20
Epoch 173/1000, LR 0.000253
Train loss: 0.0198;  Loss pred: 0.0198; Loss self: 0.0000; time: 0.14s
Val loss: 0.1759 score: 0.9388 time: 0.09s
Test loss: 0.2174 score: 0.8750 time: 0.08s
     INFO: Early stopping counter 17 of 20
Epoch 174/1000, LR 0.000252
Train loss: 0.0208;  Loss pred: 0.0208; Loss self: 0.0000; time: 0.28s
Val loss: 0.1742 score: 0.9388 time: 0.10s
Test loss: 0.2167 score: 0.8750 time: 0.08s
     INFO: Early stopping counter 18 of 20
Epoch 175/1000, LR 0.000252
Train loss: 0.0183;  Loss pred: 0.0183; Loss self: 0.0000; time: 0.14s
Val loss: 0.1737 score: 0.9388 time: 0.10s
Test loss: 0.2139 score: 0.8958 time: 0.09s
     INFO: Early stopping counter 19 of 20
Epoch 176/1000, LR 0.000252
Train loss: 0.0201;  Loss pred: 0.0201; Loss self: 0.0000; time: 0.14s
Val loss: 0.1740 score: 0.9388 time: 0.10s
Test loss: 0.2159 score: 0.8958 time: 0.08s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 155,   Train_Loss: 0.0241,   Val_Loss: 0.1702,   Val_Precision: 1.0000,   Val_Recall: 0.8800,   Val_accuracy: 0.9362,   Val_Score: 0.9388,   Val_Loss: 0.1702,   Test_Precision: 0.8462,   Test_Recall: 0.9167,   Test_accuracy: 0.8800,   Test_Score: 0.8750,   Test_loss: 0.2069


[0.09557036589831114, 0.0942862059455365, 0.09493249002844095, 0.08603419596329331, 0.14399997401051223, 0.09640202205628157, 0.10011990601196885, 0.09626192599534988, 0.09202838293276727, 0.09758033300749958, 0.098139189183712, 0.09403934609144926, 0.1119859111495316, 0.09651086898520589, 0.09305733907967806, 0.09197020484134555, 0.09017096110619605, 0.10473718494176865, 0.09348232997581363, 0.2473335131071508, 0.10166144790127873, 0.0971588019747287, 0.0936598761472851, 0.09027533582411706, 0.0912429301533848, 0.0906289650592953, 0.0906163901090622, 2.3175381820183247, 0.09649670519866049, 0.09368237806484103, 0.09602797403931618, 0.09519176394678652, 0.09439421980641782, 0.09373134095221758, 0.09460119903087616, 0.09431733703240752, 0.09452473791316152, 0.09464783011935651, 0.09572795988060534, 0.09495457191951573, 0.09487721999175847, 0.09675538912415504, 0.09581843507476151, 0.09447879903018475, 0.09572523506358266, 0.21709277294576168, 0.08982635918073356, 0.09165733703412116, 0.08907641586847603, 0.09240778116509318, 0.08935851394198835, 0.09689472801983356, 0.09103746805340052, 0.09446587483398616, 0.09611725504510105, 0.09411596995778382, 0.09858945198357105, 0.09288656618446112, 0.09298155107535422, 0.08950469014234841, 0.09300801693461835, 0.09726476203650236, 0.0972670519258827, 0.10032848594710231, 0.09652395197190344, 0.09799121599644423, 0.10097233299165964, 0.0966276740655303, 0.09636033116839826, 0.10546067100949585, 0.09866437711752951, 0.0992555320262909, 0.09833866497501731, 0.09962834697216749, 0.09819808998145163, 0.2234561238437891, 0.09199976711533964, 0.09198854793794453, 0.09276202414184809, 0.10442222305573523, 0.091028698021546, 0.09091425710357726, 0.09403796005062759, 0.2622809619642794, 0.09389542322605848, 0.0976843279786408, 0.09577599703334272, 1.8509777300059795, 0.09570501698181033, 0.09398785303346813, 0.09468197706155479, 0.0944347670301795, 0.0962842081207782, 0.09039127896539867, 0.09364201012067497, 0.09476263914257288, 0.0924967669416219, 0.09079088317230344, 0.09250932605937123, 0.09213086008094251, 0.08929740893654525, 0.13147406792268157, 0.09389730007387698, 0.09460891713388264, 0.09344820701517165, 0.10148592898622155, 0.10226689698174596, 0.10196364508010447, 0.239697938086465, 0.0942813919391483, 0.09338195901364088, 0.09414766309782863, 0.09348623198457062, 0.09251611609943211, 0.0939382219221443, 0.09519502311013639, 0.11195819498971105, 0.09251981507986784, 0.08851631707511842, 0.08907741797156632, 0.09154917392879725, 0.09172345604747534, 0.09561106795445085, 0.2442228568252176, 0.09693623217754066, 0.09535250207409263, 0.09638634813018143, 0.09709984692744911, 0.09664492588490248, 0.09553612489253283, 0.09576063300482929, 0.09849453088827431, 0.097860855050385, 0.09932906413450837, 0.09922719211317599, 0.09959365613758564, 0.09967243787832558, 0.0997649400960654, 0.09238604293204844, 0.09443043009378016, 0.09508126182481647, 0.09581324202008545, 0.0965381080750376, 0.09524637390859425, 0.09676201897673309, 0.09594571101479232, 0.09609194588847458, 0.09660195396281779, 0.0952779040671885, 0.09392093517817557, 0.09416672005318105, 0.09540514694526792, 0.09665321605280042, 0.09411780699156225, 0.09466508612968028, 0.09470708412118256, 0.09868532908149064, 0.09505630610510707, 0.09790496597997844, 0.09501013695262372, 0.09954291488975286, 0.09245318593457341, 0.09365319390781224, 0.09429152589291334, 0.09564674808643758, 0.09738166397437453, 0.0975366709753871, 0.09654906881041825, 0.09846324403770268, 0.11127819982357323, 0.09492985415272415, 0.10039073112420738, 0.09246206399984658, 0.09058029693551362, 0.09269108902662992, 0.09517993405461311, 0.09325544093735516, 0.09253027709200978, 0.09642212302424014, 0.09622060204856098, 0.09574523800984025, 0.09493272705003619, 0.09841176890768111, 0.09353431290946901, 0.08937265002168715, 0.08935985993593931, 0.10044156503863633, 0.10409400798380375, 0.09632820705883205, 0.09836730384267867, 0.09846920799463987, 0.09753788891248405, 0.09680606215260923, 0.09615761996246874, 0.09372288407757878, 0.09966257400810719, 0.09714484680444002, 0.09596069483086467, 0.09374319203197956, 0.09364236402325332, 0.0922830319032073, 0.09186062100343406, 0.09203453501686454, 0.09457708802074194, 0.0955435298383236, 0.09845489612780511, 0.09479868109337986, 0.11260882299393415, 0.09420307981781662, 0.09351776796393096, 0.09724152600392699, 0.09521900420077145, 0.09612280898727477, 0.09383213706314564, 0.09712748718447983, 0.09648814308457077, 0.0917806860525161, 0.09915271401405334, 0.10040990705601871, 0.09939573891460896, 0.102092799032107, 0.21346629597246647, 0.10838780179619789, 0.09753713593818247, 0.09600440412759781, 0.09601914207451046, 0.09603579388931394, 0.09522227803245187, 0.09295588103123009, 0.20908631710335612, 0.09699250501580536, 0.10065579903312027, 0.0969774469267577, 0.09795219893567264, 0.0978123489767313, 0.09689250006340444, 0.23661579680629075, 0.1010393409524113, 0.10071105207316577, 0.10147450002841651, 0.09995642281137407, 0.09953904105350375, 0.09898673393763602, 0.09694906487129629, 0.09739055577665567, 0.100454097148031, 0.0995378049556166, 0.09762602392584085, 0.09685270488262177, 0.1937404579948634, 0.09551135310903192, 0.09574992093257606, 0.09185401606373489, 0.0939198168925941, 0.09228643006645143, 3.288827106822282, 0.10698687215335667, 0.092005634913221, 0.09214989095926285, 0.08887620479799807, 0.08793571079149842, 0.09388966695405543, 0.09190217382274568, 0.09060456184670329, 0.09339827205985785, 0.09692452382296324, 0.0956822622101754, 0.09356681792996824, 0.10084355296567082, 0.09721996006555855, 0.10494019719772041, 0.09686149796471, 0.10322633804753423, 0.09527316689491272, 0.09570303885266185, 0.09571130806580186, 0.09657387505285442, 0.09687659190967679, 0.10524185188114643, 0.09342251787893474, 0.1156068800482899, 0.09369900403544307, 0.09455431695096195, 0.09650145703926682, 0.09452046197839081, 0.29204190499149263, 0.09902701410464942, 0.09813256305642426, 0.09662647405639291, 0.10062401811592281, 0.09809742402285337, 0.21202697209082544, 0.09526969608850777, 0.09475633595138788, 0.0951991188339889, 0.09886971884407103, 0.09975430183112621, 0.09902146505191922, 0.0912895139772445, 0.09380925097502768, 0.09327091998420656, 0.09465026389807463, 0.09695902187377214, 0.09472212917171419, 0.09603460389189422, 0.09376587509177625, 0.09427184495143592, 0.09464065101929009, 0.09513225010596216, 0.09522040095180273, 0.0960225670132786, 0.09413435100577772, 0.09505681111477315, 0.09484602115117013, 0.09812189592048526, 0.09461520402692258, 0.09549098904244602, 0.0957693550735712, 0.09600238013081253, 0.09449497004970908, 0.09553240891546011, 0.09600279503501952, 0.09469224698841572, 0.09430065914057195, 0.0964902681298554, 0.09612730517983437, 0.09860990988090634, 0.09437486017122865, 0.09591121901758015, 0.09485197509638965, 0.0936362431384623, 0.09492641501128674, 0.0950832508970052, 0.09483057796023786, 0.09746993193402886, 0.0948236680123955, 0.0955317250918597, 0.09722068207338452, 0.09673387417569757, 0.09721265896223485, 0.0916636639740318, 0.11623634304851294, 0.09166726795956492, 0.09816201799549162, 0.0926318799611181, 0.094154282938689, 0.09355033794417977, 0.09585364302620292, 0.09562267200089991, 0.09060095809400082, 0.22522769100032747, 0.09151087212376297, 0.09311504289507866, 0.09179678093641996, 0.09391713095828891, 0.09462648490443826, 0.09531275602057576, 0.0944455829448998, 0.08960052602924407, 0.09184818691574037, 0.0940576670691371, 0.09415448596701026, 0.09701662207953632, 0.10108232195489109, 0.09390069893561304, 0.0925628358963877, 0.09413137100636959, 0.09809742099605501, 0.09505738993175328, 0.08664215705357492, 0.08481578901410103, 0.0880580130033195, 0.09941929299384356, 0.08689442602917552, 0.09000844112597406, 0.0884148539043963, 0.09103992395102978, 0.08878364390693605, 0.08666367596015334, 0.0885738309007138, 0.09163084207102656, 0.08985933009535074, 0.08896860387176275, 0.09188546799123287, 0.09304148494265974, 0.0898939052131027, 0.0888736811466515, 0.08901502401567996, 0.091357875848189, 0.0898458871524781, 0.09131189109757543, 0.09239711402915418, 0.09273732989095151, 0.09362800396047533, 0.0901190279982984, 0.09013606491498649, 0.09046370419673622, 0.08746677194721997, 0.09091777703724802, 0.0928347788285464, 0.0902012218721211, 0.19832942401990294, 0.09513659891672432, 0.0870133601129055, 0.08959386590868235, 0.08912577899172902, 0.09284553397446871, 0.08957298309542239, 0.08822001912631094, 0.09063506498932838, 0.08912712289020419, 0.08874722197651863, 0.09692798112519085, 0.08310108818113804, 0.0882752810139209, 0.08795960107818246, 0.08934879279695451, 0.09053247887641191, 0.0853994800709188, 0.08446568390354514, 0.08734234399162233, 0.08233341597951949, 0.09512437111698091, 0.08632319909520447, 0.21745580900460482, 0.08995024696923792, 0.091401127865538, 0.09433636511676013, 0.11988916294649243, 0.09157663607038558, 0.09154048189520836, 0.09016701788641512, 0.09185597789473832, 0.09096321393735707, 0.08985028392635286, 0.08975726086646318, 0.09463554807007313, 0.09442109800875187, 0.09227400715462863, 0.0943839168176055, 0.09278860897757113, 0.0908612220082432, 0.09027899708598852, 0.10927987704053521, 0.09774207719601691, 0.09712665597908199, 0.0970668459776789, 0.23073934204876423, 0.0899180700071156, 0.08060431201010942, 0.08371375501155853, 0.09280936187133193, 0.09059089282527566, 0.08887694496661425, 0.09141652192920446, 0.09471126482822001, 0.09336105617694557, 0.2078103548847139, 0.08956625685095787, 0.09435821091756225, 0.09477729490026832, 0.20199614809826016, 0.08573136595077813, 0.08528402610681951, 0.08800832810811698, 0.09018601197749376, 0.08922424400225282, 0.08958787098526955, 0.08946715900674462, 0.08809117996133864, 0.08894693595357239, 0.08952290005981922, 0.08749495097436011, 0.08842225698754191, 0.09045878192409873, 0.09171322593465447, 0.08904536301270127, 0.08956134994514287, 0.08877549320459366, 0.08998836390674114, 0.08841957710683346, 0.09324906091205776, 0.09406211692839861, 0.09288191003724933, 0.09035937604494393, 0.0912631789688021, 0.17386834882199764, 0.08938841009512544, 0.09027149202302098, 0.0893111820332706, 0.08882253686897457, 0.09134852094575763, 0.09342549601569772, 0.09102016896940768, 0.08874655701220036, 0.09130027005448937, 0.09004685189574957, 0.09001283184625208, 0.0844425461255014, 0.08593783201649785, 0.08365032984875143, 0.08307974087074399, 0.08535803901031613, 0.08514558081515133, 0.08620703406631947, 0.09014957095496356, 0.08750077988952398, 0.08547283499501646, 0.08823535894043744, 0.08646234101615846, 0.08350687590427697, 0.21867438312619925, 0.08551664603874087, 0.10401481203734875, 0.08932397398166358, 0.08950107195414603, 0.1496744048781693, 1.2209221760276705, 0.08921629004180431, 0.08962316904217005, 0.09095191396772861, 0.0883430119138211, 0.08863391121849418, 0.08855657302774489, 0.09097814885899425, 0.08797332900576293, 0.12391524389386177, 0.08600645093247294, 0.0858895389828831, 0.0867679649963975, 0.08807372604496777, 0.08786679804325104, 0.09587501804344356, 0.08881752099841833, 0.08467335696332157, 0.08459124783985317, 0.08769111917354167, 0.08387417113408446, 0.08319150190800428, 0.08844561199657619, 0.08450833591632545, 0.08910429896786809, 0.08944822382181883, 0.09061652305535972, 0.08804320194758475]
[0.0019504156305777784, 0.0019242082846027855, 0.0019373977556824684, 0.001755799917618231, 0.0029387749798063722, 0.001967388205230236, 0.0020432633879993644, 0.001964529101945916, 0.0018781302639340259, 0.0019914353674999915, 0.002002840595585959, 0.0019191703283969238, 0.002285426758153706, 0.001969609571126651, 0.0018991293689730217, 0.0018769429559458274, 0.0018402236960448173, 0.0021374935702401766, 0.001907802652567625, 0.005047622716472465, 0.002074723426556709, 0.00198283269336181, 0.001911426043822145, 0.0018423537923289196, 0.0018621006153752, 0.0018495707154958223, 0.001849314083858412, 0.04729669759221071, 0.0019693205142583773, 0.0019118852666294088, 0.0019597545722309425, 0.0019426890601385006, 0.0019264126491105678, 0.0019128845092289302, 0.00193063671491584, 0.001924843612906276, 0.0019290762839420717, 0.001931588369782786, 0.001953631834298068, 0.0019378484065207292, 0.0019362697957501728, 0.0019745997780439804, 0.0019554782668318674, 0.001928138755718056, 0.001953576225787401, 0.004430464753995137, 0.00183319100368844, 0.0018705578986555338, 0.001817886038132164, 0.0018858730850019017, 0.0018236431416732315, 0.001977443428976195, 0.001857907511293888, 0.0019278749966119624, 0.0019615766335734906, 0.0019207340807710983, 0.0020120296323177765, 0.0018956442078461452, 0.001897582675007229, 0.0018266263294356819, 0.001898122794584048, 0.001984995143602089, 0.0019850418760384222, 0.002047520121369435, 0.0019698765708551723, 0.0019998207346213106, 0.002060659856972646, 0.0019719933482761284, 0.001966537370783638, 0.0021522585920305277, 0.0020135587166842757, 0.0020256231025773653, 0.002006911530102394, 0.002033231570860561, 0.0020040426526826863, 0.004560329058036512, 0.0018775462676599926, 0.0018773173048560108, 0.0018931025335071038, 0.0021310657766476578, 0.0018577285310519592, 0.0018553930021138216, 0.0019191420418495427, 0.0053526726931485595, 0.001916233127062418, 0.001993557713849812, 0.001954612184353933, 0.03777505571440774, 0.00195316361187368, 0.001918119449662615, 0.0019322852461541795, 0.0019272401434730511, 0.0019649838391995553, 0.0018447199788856872, 0.001911061431034183, 0.001933931411072916, 0.0018876891212575898, 0.0018528751667817027, 0.0018879454297830863, 0.0018802216343049491, 0.0018223961007458214, 0.0026831442433200322, 0.001916271430079122, 0.0019307942272220947, 0.001907106265615748, 0.0020711414078820726, 0.0020870795302397136, 0.0020808907159204993, 0.0048917946548258165, 0.0019241100395744552, 0.0019057542655845077, 0.001921380879547523, 0.0019078822853994004, 0.0018880840020292268, 0.0019171065698396794, 0.0019427555736762528, 0.002284861122239001, 0.0018881594914258743, 0.001806455450512621, 0.0018179064892156391, 0.0018683504883428008, 0.001871907266275007, 0.0019512462847847112, 0.004984139935208523, 0.0019782904526028707, 0.001945969430083523, 0.0019670683291873763, 0.0019816295291316143, 0.0019723454262224995, 0.0019497168345414863, 0.001954298632751618, 0.002010092467107639, 0.001997160307150714, 0.0020271237578471098, 0.0020250447370035916, 0.002032523594644605, 0.002034131385271951, 0.002036019185633988, 0.0018854294475928253, 0.001927151634566942, 0.0019404339147921727, 0.001955372286124193, 0.0019701654709191347, 0.0019438035491549847, 0.0019747350811578183, 0.001958075734995762, 0.0019610601201729507, 0.0019714684482207714, 0.0019444470217793572, 0.0019167537791464403, 0.0019217697970036949, 0.0019470438152095493, 0.0019725146133224574, 0.0019207715712563724, 0.0019319405332587811, 0.0019327976351261747, 0.0020139863077855234, 0.0019399246143899402, 0.0019980605302036417, 0.0019389823867882394, 0.0020314880589745485, 0.0018867997129504777, 0.0019112896715880049, 0.0019243168549574151, 0.001951974450743624, 0.001987380897436215, 0.001990544305620145, 0.0019703891593962908, 0.0020094539599531157, 0.0022709836698688417, 0.0019373439623004928, 0.002048790431106273, 0.0018869808979560525, 0.0018485774884798697, 0.0018916548780944882, 0.0019424476337676145, 0.0019031722640276564, 0.0018883730018777506, 0.0019677984290661253, 0.001963685756093081, 0.001953984449180413, 0.0019374025928578815, 0.0020084034470955327, 0.0019088635287646735, 0.0018239316330956562, 0.001823670610937537, 0.0020498278579313537, 0.002124367509873546, 0.001965881776710858, 0.0020074959967893605, 0.0020095756733599975, 0.0019905691614792663, 0.001975633921481821, 0.001962400407397321, 0.0019127119199505874, 0.002033930081798106, 0.0019825478939681637, 0.0019583815271605035, 0.0019131263679995829, 0.001911068653535782, 0.0018833271816981081, 0.001874706551090491, 0.001878255816670705, 0.0019301446534845295, 0.001949867955884155, 0.0020092835944450025, 0.0019346669610893848, 0.0022981392447741665, 0.0019225118330166656, 0.0019085258768149177, 0.001984520938855653, 0.0019432449836892132, 0.001961689979332138, 0.0019149415727172578, 0.0019821936160097924, 0.001969145777236138, 0.001873075225561553, 0.002023524775797007, 0.002049181776653443, 0.0020284844676450807, 0.0020835265108593262, 0.004356455019846254, 0.0022119959550244467, 0.001990553794656785, 0.001959273553624445, 0.0019595743280512337, 0.0019599141610064067, 0.0019433117965806503, 0.0018970587965557162, 0.0042670676959868595, 0.001979438877873579, 0.0020541999802677607, 0.0019791315699338305, 0.001999024468074952, 0.0019961703872802307, 0.0019773979604776415, 0.0048288938123732805, 0.0020620273663757406, 0.002055327593329914, 0.002070908163845235, 0.002039926996150491, 0.002031409001091913, 0.0020201374272986942, 0.0019785523443121692, 0.0019875623627888914, 0.0020500836152659387, 0.0020313837746044204, 0.001992367835221242, 0.0019765858139310566, 0.003953886897854355, 0.0019492112879394268, 0.0019540800190321647, 0.001874571756402753, 0.0019167309569917163, 0.0018833965319683965, 0.06711892054739352, 0.002183405554150136, 0.0018776660186371633, 0.0018806100195767929, 0.001813800097918328, 0.0017946063426836413, 0.00191611565212358, 0.0018755545678111364, 0.0018490726907490467, 0.0019060871848950581, 0.0019780515065910866, 0.0019526992287790897, 0.0019095268965299641, 0.0020580316931769556, 0.00198408081766446, 0.002141636677504498, 0.0019767652645859184, 0.0021066599601537598, 0.001944350344794137, 0.0019531232418910582, 0.001953292001342895, 0.001970895409241927, 0.001977073304279118, 0.0021477928955336008, 0.0019065819975292804, 0.0023593240826181613, 0.0019122245721518993, 0.0019296799377747336, 0.001969417490597282, 0.0019289890199671595, 0.005960038877377401, 0.0020209594715234576, 0.0020027053684984544, 0.0019719688582937327, 0.002053551390120874, 0.0020019882453643544, 0.0043270810630780705, 0.0019442795120103626, 0.00193380277451812, 0.0019428391598773245, 0.002017749364164715, 0.0020358020781862493, 0.002020846225549372, 0.0018630513056580509, 0.0019144745096944425, 0.001903488162942991, 0.0019316380387362168, 0.0019787555484443295, 0.001933104676973759, 0.00195988987534478, 0.0019135892875872704, 0.001923915203090529, 0.0019314418575365323, 0.0019414744919584114, 0.0019432734888123007, 0.0019596442247607876, 0.0019211092041995451, 0.0019399349207096562, 0.001935633084717758, 0.002002487671846638, 0.0019309225311616854, 0.0019487956947437963, 0.001954476634154514, 0.0019592322475676027, 0.001928468776524675, 0.001949640998274696, 0.0019592407150003984, 0.00193249483649828, 0.0019245032477667745, 0.001969189145507253, 0.0019617817383639664, 0.00201244714042666, 0.0019260175545148704, 0.001957371816685309, 0.0019357545938038705, 0.001910943737519639, 0.0019372737757405456, 0.001940474508102147, 0.0019353179175558748, 0.001989182284367936, 0.0019351768982121531, 0.0019496270426910144, 0.0019840955525180517, 0.0019741606974632156, 0.0019839318155558134, 0.0018706870198782002, 0.0023721702662961824, 0.0018707605706033657, 0.0020033064897039105, 0.0018904465298187367, 0.0019215159783405916, 0.001909190570289383, 0.001956196796453121, 0.0019514831020591818, 0.001848999144775527, 0.004596483489802601, 0.0018675688188523054, 0.0019003069978587481, 0.0018734036925799993, 0.0019166761420058962, 0.0019311527531518011, 0.0019451582861341992, 0.0019274608764265264, 0.0018285821638621238, 0.001874452794198783, 0.0019195442259007571, 0.0019215201217757196, 0.00197993106284768, 0.0020629045296916546, 0.0019163407946043477, 0.0018890374672732183, 0.0019210483878850937, 0.0020019881835929595, 0.0019399467333010873, 0.0018050449386161442, 0.001766995604460438, 0.0018345419375691563, 0.002071235270705074, 0.00181030054227449, 0.0018751758567911263, 0.001841976123008256, 0.0018966650823131204, 0.0018496592480611678, 0.0018054932491698612, 0.001845288143764871, 0.00190897587647972, 0.0018720693769864738, 0.001853512580661724, 0.0019142805831506848, 0.0019383642696387444, 0.0018727896919396396, 0.001851535023888573, 0.0018544796669933323, 0.0019032890801706042, 0.001871789315676627, 0.0019023310645328213, 0.0019249398756073788, 0.0019320277060614899, 0.001950583415843236, 0.0018774797499645501, 0.0018778346857288852, 0.0018846605040986713, 0.0018222244155670826, 0.001894120354942667, 0.0019340578922613834, 0.001879192122335856, 0.004131863000414644, 0.001982012477431757, 0.0018127783356855314, 0.001866538873097549, 0.0018567870623276879, 0.0019342819578014314, 0.0018661038144879665, 0.001837917065131478, 0.001888230520611008, 0.0018568150602125872, 0.0018489004578441381, 0.002019332940108143, 0.0017312726704403758, 0.0018390683544566855, 0.0018324916891288012, 0.0018614331832698856, 0.0018860933099252482, 0.0017791558348108083, 0.0017597017479905237, 0.0018196321664921318, 0.0017152794995733227, 0.001981757731603769, 0.0017983999811500933, 0.004530329354262601, 0.0018739634785257901, 0.001904190163865375, 0.001965340939932503, 0.0024976908947185925, 0.0019078465847996995, 0.0019070933728168409, 0.001878479539300315, 0.0019136662061403815, 0.0018950669570282723, 0.0018718809151323512, 0.001869942934717983, 0.0019715739181265235, 0.001967106208515664, 0.0019223751490547631, 0.001966331600366781, 0.0019330960203660652, 0.0018929421251717333, 0.0018808124392914276, 0.0022766641050111502, 0.002036293274917019, 0.002023471999564208, 0.00202222595786831, 0.004807069626015921, 0.0018732931251482416, 0.001679256500210613, 0.0017440365627408028, 0.0019335283723194152, 0.001887310267193243, 0.0018516030201377969, 0.001904510873525093, 0.001973151350587917, 0.001945022003686366, 0.00432938239343154, 0.0018659636843949556, 0.0019657960607825467, 0.001974526977088923, 0.00420825308538042, 0.0017860701239745442, 0.0017767505438920732, 0.0018335068355857704, 0.00187887524953112, 0.0018588384167136003, 0.0018664139788597822, 0.0018638991459738463, 0.0018352329158612217, 0.001853061165699425, 0.0018650604179129004, 0.0018228114786325023, 0.0018421303539071232, 0.001884557956752057, 0.0019106922069719683, 0.0018551117294312764, 0.0018658614571904764, 0.001849489441762368, 0.0018747575813904405, 0.0018420745230590303, 0.0019426887690012034, 0.0019596274360083044, 0.0019350397924426943, 0.001882487000936332, 0.0019013162285167102, 0.003622257267124951, 0.0018622585436484467, 0.0018806560838129371, 0.0018606496256931375, 0.0018504695181036368, 0.0019030941863699506, 0.0019463645003270358, 0.0018962535201959934, 0.0018488866044208407, 0.0019020889594685286, 0.0018759760811614494, 0.0018752673301302518, 0.0017592197109479457, 0.0017903715003437053, 0.0017427152051823214, 0.0017308279348071665, 0.0017782924793815862, 0.0017738662669823195, 0.0017959798763816555, 0.001878116061561741, 0.001822932914365083, 0.0017806840623961762, 0.0018382366445924465, 0.001801298771169968, 0.0017397265813391034, 0.004555716315129151, 0.0017815967924737681, 0.002166975250778099, 0.0018609161246179913, 0.0018646056657113756, 0.0031182167682951936, 0.025435878667243134, 0.0018586727092042565, 0.0018671493550452094, 0.0018948315409943461, 0.0018404794148712729, 0.001846539817051962, 0.0018449286047446851, 0.0018953781012290467, 0.0018327776876200612, 0.0025815675811221204, 0.0017918010610931863, 0.0017893653954767312, 0.0018076659374249477, 0.0018348692926034953, 0.00183055829256773, 0.0019973962092384077, 0.001850365020800382, 0.0017640282700691994, 0.0017623176633302744, 0.0018268983161154513, 0.0017473785652934264, 0.0017331562897500892, 0.0018426169165953372, 0.0017605903315901135, 0.0018563395618305851, 0.0018635046629545589, 0.0018878442303199943, 0.0018342333739080157]
[512.7112315562025, 519.6942597128616, 516.1562704751558, 569.5409767170482, 340.27783919199123, 508.28809349447823, 489.41316419276586, 509.0278372610895, 532.444431146831, 502.15036667515864, 499.2908582959074, 521.058493456022, 437.55504149599403, 507.71483580270313, 526.5570720654816, 532.7812424091924, 543.4121961092527, 467.8376646006175, 524.1632296999616, 198.11306354902268, 481.9919547829267, 504.3289851674483, 523.1696006403522, 542.7839127119552, 537.0279090952919, 540.665999748988, 540.7410286486319, 21.14312522666889, 507.7893581871248, 523.0439385952104, 510.2679764954555, 514.7504150400202, 519.0995815261615, 522.7707136397339, 517.963836631788, 519.5227255320361, 518.3828178927677, 517.7086462331794, 511.8671708988075, 516.0362372180751, 516.4569535685847, 506.43173929179227, 511.3838476047764, 518.6348736751527, 511.88174118823736, 225.70995494282127, 545.4968947523566, 534.5998649487148, 550.0894880228449, 530.2583763206906, 548.3528970928372, 505.70346809756353, 538.2399252498731, 518.7058298683238, 509.7940008483155, 520.6342772855573, 497.0105727757302, 527.5251525897957, 526.9862616110734, 547.4573446606017, 526.8363052450137, 503.77957005241893, 503.76770992646004, 488.39568879605145, 507.64601944875926, 500.0448203620419, 485.28144837504567, 507.1011019759154, 508.50800745348346, 464.6281834826175, 496.63314593909564, 493.67525416135834, 498.2780680665975, 491.8278932570145, 498.99137558842006, 219.2824217887817, 532.6100438772736, 532.6750024693878, 528.2334064322605, 469.2487725897802, 538.2917812182918, 538.9693713734583, 521.0661734220912, 186.82255712739612, 521.8571716965348, 501.61577618381233, 511.6104401705316, 26.472495700875726, 511.9898783290841, 521.3439654010566, 517.5219352268495, 518.8766970149941, 508.9100378593212, 542.08769430906, 523.2694165455705, 517.0814198861453, 529.7482454811171, 539.7017661675075, 529.6763265635776, 531.8521932493688, 548.7281275408495, 372.69707079282244, 521.8467406565209, 517.9215816481578, 524.3546298544248, 482.8255551235343, 479.1384254940894, 480.56343966032915, 204.4239528765762, 519.7207952935813, 524.7266229748111, 520.4590149952443, 524.1413517242536, 529.6374520017359, 521.6194111126678, 514.7327916850147, 437.6633617977058, 529.6162768775607, 553.5702525718132, 550.0832996264091, 535.231481587261, 534.214497703161, 512.4929681085, 200.6364213283596, 505.4869464108685, 513.8826872306411, 508.37074907973033, 504.63519305660424, 507.0105807557415, 512.8949918695088, 511.69252397829166, 497.4895515323827, 500.7109326274707, 493.3097923246886, 493.8162509336338, 491.9992085872214, 491.61032922477926, 491.15450731306055, 530.383144952321, 518.9005276301021, 515.348650823341, 511.41156448633757, 507.57157952497937, 514.455280439591, 506.39704005951234, 510.7054758544181, 509.92827283225137, 507.23611676488616, 514.2850326078327, 521.7154184745186, 520.3536872934201, 513.5991250881921, 506.9670932960154, 520.6241153110686, 517.6142757940942, 517.3847390053947, 496.5277053445061, 515.4839484906871, 500.485338098381, 515.7344423620142, 492.2500014618735, 529.997960640058, 523.2069292610914, 519.6649384553303, 512.3017873615303, 503.1748072501009, 502.37515295518864, 507.5139574490914, 497.64762961940755, 440.33782068444117, 516.1706023604364, 488.09286924482365, 529.9470710504722, 540.9564955929028, 528.6376556210535, 514.8143932510437, 525.4385106914674, 529.5563953761387, 508.18213147704284, 509.2464498951118, 511.7747996507566, 516.1549817711817, 497.9079285320672, 523.871919040306, 548.2661640681983, 548.3446374594514, 487.8458433134871, 470.7283440140381, 508.6775877607008, 498.1329983219521, 497.61748873482657, 502.3688798920519, 506.1666481460045, 509.5800002030539, 522.8178846848164, 491.6589852075668, 504.4014336513468, 510.6257315702523, 522.7046246012632, 523.2674389535092, 530.9751856808792, 533.4168163109655, 532.408839692852, 518.0958837435732, 512.855240777859, 497.6898247537908, 516.884828299809, 435.1346430700147, 520.1528452653905, 523.964601239188, 503.8999490610749, 514.603155234457, 509.76454512983355, 522.2091442617871, 504.4915854451323, 507.83441813210203, 533.8813873319996, 494.1871787095512, 487.99965498088636, 492.97887952818564, 479.9554960246518, 229.54443359208412, 452.08039270078507, 502.37275811600045, 510.393251697859, 510.31491160352385, 510.2264272056204, 514.5854626928873, 527.1317904408611, 234.35297287186032, 505.1936744186082, 486.80752098422863, 505.27211792869014, 500.244001997131, 500.95923993867757, 505.7150962967765, 207.08676538665176, 484.95961610714187, 486.54044408553983, 482.879935217993, 490.2136213144302, 492.2691587279985, 495.01582738219406, 505.4200374707011, 503.12886716008666, 487.78498230682123, 492.2752719114999, 501.9153503293509, 505.9228862981611, 252.9156816657217, 513.0280160942079, 511.74976984580684, 533.4551726731283, 521.7216304418052, 530.9556341568011, 14.898928526329435, 458.0001173392809, 532.5760758698792, 531.7423546563031, 551.3286724086549, 557.2252678571319, 521.8891661845811, 533.1756362423775, 540.8116214159795, 524.6349736384468, 505.5480085669606, 512.1116377073813, 523.6899264510088, 485.9011663014351, 504.0117272930139, 466.9326083662478, 505.87695864307614, 474.6850554500559, 514.3106038875301, 512.0004608781253, 511.95622534290646, 507.38359595887124, 505.7981400262853, 465.5942395933657, 524.4988158368691, 423.8502066618554, 522.9511295708651, 518.2206543294319, 507.7643540662988, 518.4062685940145, 167.78414043500848, 494.8144750503933, 499.3245715168571, 507.1073996905107, 486.96127343622953, 499.5034323081171, 231.10267300808314, 514.3293409320615, 517.1158161406546, 514.7106464866306, 495.601692539236, 491.2068863250827, 494.84220390304444, 536.7538709014718, 522.3365445380643, 525.3513100149232, 517.6953341912103, 505.3681344247835, 517.302560958821, 510.23274959470973, 522.5781762505787, 519.7734278483922, 517.7479177527276, 515.0724380577755, 514.595606720897, 510.29670966017784, 520.5326161646614, 515.4812098718165, 516.6268379556108, 499.37885464125134, 517.8871673315542, 513.1374226129269, 511.64592225099136, 510.40401220503867, 518.5461191661689, 512.9149422303564, 510.4018063445546, 517.4658069524369, 519.6146076450724, 507.8232338836121, 509.74070175306696, 496.9074615236798, 519.206067284201, 510.88913791220284, 516.5944088165338, 523.3016442953873, 516.1893029898359, 515.3378701058205, 516.7109708067533, 502.7191363298063, 516.7486243370657, 512.9186137158463, 504.0079842580575, 506.5443766989151, 504.04958081678956, 534.5629650357598, 421.5549002565328, 534.5419481860662, 499.1747419276819, 528.9755537787593, 520.4224223332211, 523.7821805543625, 511.19601147141765, 512.4307758262481, 540.8331327927156, 217.55761817017765, 535.4555023115771, 526.230762254095, 533.7877810109512, 521.7365511491423, 517.8254275162424, 514.0969797308354, 518.8172752196039, 546.8717893911387, 533.4890284219937, 520.956999326621, 520.4213001297524, 505.0680898766888, 484.75340744414933, 521.8278517138505, 529.3701249046568, 520.549095122436, 499.5034477203079, 515.4780710387661, 554.0028276341197, 565.9323642207676, 545.0951976192167, 482.80367476534315, 552.3944652546943, 533.2833165371694, 542.895202336734, 527.2412137099227, 540.6401211727027, 553.8652667130076, 541.9207853141777, 523.8410879471496, 534.1682377229683, 539.5161653788124, 522.3894599370147, 515.8989028343838, 533.9627851989642, 540.0924028430265, 539.2348149178146, 525.4062614126717, 534.2481611711269, 525.6708564792228, 519.4967451565045, 517.5909211149653, 512.6671291664297, 532.6289138505391, 532.528239892346, 530.5995418406907, 548.7798272578827, 527.9495557874774, 517.0476044182716, 532.1435675012245, 242.02157716740535, 504.5376915567028, 551.6394256895363, 535.7509636756106, 538.5647176722513, 516.9877100733715, 535.8758672675381, 544.0941917193982, 529.5963544093188, 538.5565969534466, 540.862000308023, 495.21303799780844, 577.6097648128614, 543.7535791297051, 545.7050670038333, 537.2204648481395, 530.1964620401698, 562.0643118686329, 568.278119369911, 549.5616193286963, 582.9953662063535, 504.60254755294136, 556.0498278922862, 220.7345033444637, 533.6283291853052, 525.1576334004734, 508.8175693497454, 400.3697984064065, 524.151159724925, 524.3581747247994, 532.345431014103, 522.5571715648736, 527.685840487736, 534.2220180332866, 534.7756776068762, 507.2089820250027, 508.36095970363414, 520.1898289684522, 508.56122121694494, 517.3048774942037, 528.278169048236, 531.6851266555518, 439.2391472237414, 491.0883969013506, 494.2000681083644, 494.5045810084103, 208.0269431896695, 533.8192867818622, 595.5016400857044, 573.3824745213321, 517.1892041079404, 529.854585853111, 540.0725690788621, 525.069199604559, 506.80349467466937, 514.1330011201507, 230.97982786579027, 535.9161104597023, 508.69976797182034, 506.4504114673156, 237.62829366751393, 559.888431353803, 562.8252111348412, 545.4029298344661, 532.233313653769, 537.9703749441469, 535.7868143545055, 536.5097152171943, 544.8899653866163, 539.6475942134144, 536.1756597242313, 548.6030846976087, 542.8497488676733, 530.6284141685146, 523.3705336480032, 539.0510900961049, 535.9454723427062, 540.6897587082767, 533.4022968763438, 542.8662019272466, 514.7504921820962, 510.3010815346445, 516.785238166938, 531.2121674692091, 525.9514356431588, 276.07094865288724, 536.9823666056855, 531.7293303156999, 537.4466993631193, 540.403389635297, 525.4600676950446, 513.7783800680583, 527.3556459352766, 540.866052903903, 525.7377658505596, 533.0558369277729, 533.2573036029708, 568.4338310768219, 558.5432966331433, 573.8172232768123, 577.7581814401506, 562.3371923316895, 563.7403555236373, 556.7991118111473, 532.4484575082402, 548.5665391851758, 561.5819342227114, 543.9996003461833, 555.1549892805881, 574.8029665847143, 219.50444909817674, 561.2942301111175, 461.4727370056158, 537.3697324511495, 536.3064257441719, 320.6961139352492, 39.314545138470926, 538.0183369820525, 535.5757948864176, 527.7514007790014, 543.3366936461726, 541.5534454039123, 542.0263946411018, 527.5992158775897, 545.6199116536288, 387.36154238710026, 558.0976715070675, 558.857348268756, 553.1995593303693, 544.9979483721701, 546.2814290373112, 500.6517962609395, 540.4339083147207, 566.8843390819196, 567.4345895792062, 547.3758397929382, 572.2858342559995, 576.9820101707007, 542.706403590244, 567.9913049941773, 538.6945473563434, 536.6232883015784, 529.7047203044382, 545.186896185081]
Elapsed: 0.11423449569911516~0.18989325247045338
Time per graph: 0.002345255570485309~0.0038801398807273976
Speed: 503.6426340448953~74.70120239422967
Total Time: 0.0897
best val loss: 0.17024244368076324 test_score: 0.8750

Testing...
Test loss: 0.2105 score: 0.8750 time: 0.08s
test Score 0.8750
Epoch Time List: [0.3292852337472141, 0.31953307590447366, 0.3140304679982364, 0.3144084610976279, 0.36263015191070735, 0.36688820901326835, 0.31778799486346543, 0.3236396561842412, 0.3183343231212348, 0.33470472996123135, 0.33647569408640265, 0.3254747549071908, 0.4567101811990142, 0.3280171169899404, 0.3404046380892396, 0.31250299187377095, 0.3383111522998661, 0.3290527171920985, 0.31871567317284644, 0.47078991681337357, 0.3374593579210341, 0.334836712339893, 0.32385665317997336, 0.30657747900113463, 0.3217134971637279, 0.3085201538633555, 0.31387247471138835, 2.967746297828853, 3.6224805880337954, 0.32256310270167887, 0.3247767749708146, 0.327215860132128, 0.3224991981405765, 0.3207212188281119, 0.32416016003116965, 0.32632375275716186, 0.32080763997510076, 0.32509052637033165, 0.3295401050709188, 0.3224231190979481, 0.3296623891219497, 0.3329404017422348, 0.33440841315314174, 0.32746883179061115, 0.3233399090822786, 0.4459288823418319, 0.31381753901951015, 0.3143575091380626, 0.31452669110149145, 0.31824746215716004, 0.31483815982937813, 0.33134525804780424, 0.3246650437358767, 0.38334972178563476, 0.32417725096456707, 0.3209391951095313, 0.32336324895732105, 0.3160319230519235, 0.3184944309759885, 0.31466880696825683, 0.3173738941550255, 0.4747949440497905, 0.3303187408018857, 0.35663744807243347, 0.33459639293141663, 0.3340012039989233, 0.3420688270125538, 0.3316515318583697, 0.38780036685056984, 0.3405356800649315, 0.3377129202708602, 0.33389944513328373, 0.33824445586651564, 0.3390350672416389, 0.33988305018283427, 0.4629461020231247, 0.3208672869950533, 0.31842585979029536, 0.3131929330993444, 0.3362861769273877, 0.3076966009102762, 0.311138057615608, 0.3209219090640545, 0.5986058600246906, 0.32517343387007713, 0.32869174401275814, 0.32541312417015433, 3.872821615077555, 0.3293909369967878, 0.3280808157287538, 0.3328723479062319, 0.32697442383505404, 0.3261610821355134, 0.32469184789806604, 0.4616989209316671, 0.3241237439215183, 0.32779380306601524, 0.317804376129061, 0.32176613504998386, 0.3155427100136876, 0.3061078106984496, 0.34975996520370245, 0.429800073383376, 0.317694955971092, 0.320285745896399, 0.3436934482306242, 0.3385945660993457, 0.3435896879527718, 0.4914630849380046, 0.3301589253824204, 0.3220217749476433, 0.3231486240401864, 0.327604774851352, 0.32043344783596694, 0.33448246330954134, 0.33238588203676045, 0.3423533667810261, 0.37305361800827086, 0.3133628766518086, 0.3070510351099074, 0.3162237189244479, 0.31461646291427314, 0.3240599720738828, 0.47460807487368584, 0.3287365708965808, 0.3195223279763013, 0.33136449195444584, 0.3296520388685167, 0.32763148797675967, 0.3282386309001595, 0.32894634106196463, 0.438129894901067, 0.33282266021706164, 0.3357173518743366, 0.34187071095220745, 0.3385125841014087, 0.33278138493187726, 0.3371177220251411, 0.36897814087569714, 0.3241114739794284, 0.325917094014585, 0.331757253035903, 0.32884487695991993, 0.3340139801148325, 0.32922522304579616, 0.33316361904144287, 0.4518906371667981, 0.32720182999037206, 0.32716024783439934, 0.3245704749133438, 0.3257628751453012, 0.3232719290535897, 0.32816366315819323, 0.3643717900849879, 0.32380501204170287, 0.3233409677632153, 0.3246106223668903, 0.33006160613149405, 0.33459685393609107, 0.3279653931967914, 0.3406443237327039, 0.4692715499550104, 0.3181462660431862, 0.318313678028062, 0.3251092382706702, 0.3284979809541255, 0.33179710316471756, 0.3365283189341426, 0.33074077405035496, 0.35306525276973844, 0.33094779029488564, 0.3363416800275445, 0.313384389039129, 0.3167242081835866, 0.3103057760745287, 0.3207186779472977, 0.3219553849194199, 0.322518615052104, 0.3264107368886471, 0.33186207083053887, 0.33206618623808026, 0.3280190220102668, 0.3272059157025069, 0.32878297311253846, 0.3148772898130119, 0.3105273740366101, 0.3301876219920814, 0.3428978759329766, 0.3406535389367491, 0.3329032859764993, 0.32672719890251756, 0.3351822840049863, 0.33382089296355844, 0.3295020409859717, 0.33020543307065964, 0.3320440500974655, 0.32342112995684147, 0.31951086805202067, 0.3290900399442762, 0.3291367457713932, 0.3262889247853309, 0.31975220795720816, 0.3124606399796903, 0.32149834698066115, 0.3256585788913071, 0.33939504297450185, 0.3301328180823475, 0.4240346662700176, 0.33584692305885255, 0.3181366410572082, 0.3239429397508502, 0.32331082434393466, 0.33292373991571367, 0.3416322849225253, 0.3298965517897159, 0.45887578185647726, 0.324441981036216, 0.3304879569914192, 0.3397117150016129, 0.3353631941135973, 0.35664576874114573, 0.4434553300961852, 0.3530297358520329, 0.33078725123777986, 0.328251221915707, 0.330203260993585, 0.328457023948431, 0.3345922997687012, 0.3181230309419334, 0.4363577126059681, 0.3223656858317554, 0.33463753992691636, 0.32668571057729423, 0.3334426179062575, 0.3440729849971831, 0.33364550210535526, 0.4647967803757638, 0.34837832930497825, 0.3426643516868353, 0.3352172481827438, 0.34670704673044384, 0.33280296600423753, 0.40207787696272135, 0.3395528648979962, 0.33762675803154707, 0.35368857881985605, 0.34073040285147727, 0.3460255600512028, 0.34024287457577884, 0.4421560678165406, 0.3224131939932704, 0.3279751529917121, 0.33668597997166216, 0.32018539495766163, 0.32403972395695746, 3.507559360936284, 3.4437077010516077, 0.31481180083937943, 0.3130723121576011, 0.3259332599118352, 0.3070739610120654, 0.3296184199862182, 0.329990039113909, 0.45436065015383065, 0.31361445994116366, 0.31968254200182855, 0.33834887901321054, 0.3149172409903258, 0.32587141799740493, 0.3313238548580557, 0.3645177949219942, 0.31994029483757913, 0.34687967295758426, 0.32213814882561564, 0.3242691648192704, 0.31876029539853334, 0.3347005997784436, 0.32595326378941536, 0.46435659285634756, 0.3186233420856297, 0.3460032388102263, 0.3145001551602036, 0.3197800642810762, 0.3307089852169156, 0.3188666489440948, 0.5954780299216509, 0.3340416911523789, 0.33757782191969454, 0.334983145352453, 0.34608596307225525, 0.34171711979433894, 0.4495757860131562, 0.33071055286563933, 0.32759964116849005, 0.3314522069413215, 0.5009350709151477, 0.34690377302467823, 0.3404100148472935, 0.40815955703146756, 0.3155690699350089, 0.3237167198676616, 0.319825038081035, 0.32214269880205393, 0.32254230906255543, 0.32627180870622396, 0.49269925709813833, 0.3152257218025625, 0.3195849289186299, 0.32353495224379003, 0.32340359315276146, 0.33065288909710944, 0.330711015034467, 0.3244958738796413, 0.32641474390402436, 0.32964794826693833, 0.3320798820350319, 0.3263500949833542, 0.3276122158858925, 0.32667016494087875, 0.3237758360337466, 0.32580664404667914, 0.3298277137801051, 0.32685380009934306, 0.32496633916161954, 0.32904220139607787, 0.328274138038978, 0.33302450692281127, 0.3309001340530813, 0.3326482849661261, 0.3260382211301476, 0.3260171441361308, 0.3257401629816741, 0.3313770422246307, 0.32808761089108884, 0.325910592218861, 0.33189878379926085, 0.3272987501695752, 0.33043534494936466, 0.3267098779324442, 0.5050719548016787, 0.32148642279207706, 0.3454302391037345, 0.32200932898558676, 0.32874894025735557, 0.3190268750768155, 0.3164073920343071, 0.31573147187009454, 0.315775184892118, 0.3250622309278697, 0.31817785697057843, 0.5289875629823655, 0.30920457001775503, 0.31385210808366537, 0.3086707650218159, 0.32383204507641494, 0.3202203391119838, 0.3249143590219319, 0.32484757201746106, 0.4913903698325157, 0.30928887287154794, 0.31340107368305326, 0.31140520493499935, 0.3223291728645563, 0.32480282289907336, 0.3199971348512918, 0.3194503951817751, 0.4255621919874102, 0.3244019001722336, 0.32615646021440625, 0.30310687399469316, 0.31219827476888895, 0.3104072045534849, 0.4543223427608609, 0.3161860469263047, 0.3178878538310528, 0.32159516331739724, 0.32287371112033725, 0.3242648800369352, 0.32391578890383244, 0.3189797920640558, 0.4595569088123739, 0.3265973867382854, 0.3289234258700162, 0.3280474797356874, 0.3388274556491524, 0.3318638929631561, 0.3215086041018367, 0.4448639662005007, 0.32505408208817244, 0.3347188450861722, 0.3296801927499473, 0.3247300102375448, 0.33140087290667, 0.3208537851460278, 0.40199852106161416, 0.333293013041839, 0.3190473217982799, 0.32425948209129274, 0.3249208021443337, 0.33571851602755487, 0.33225763007067144, 0.44184900308027864, 0.32993076904676855, 0.3252605888992548, 0.3182451690081507, 0.32731784600764513, 0.3248631909955293, 0.3279506100807339, 0.3191108563914895, 0.414689578814432, 0.32297095004469156, 0.3189614568836987, 0.32015995401889086, 0.33501293417066336, 0.3146020278800279, 0.3131502557080239, 0.3169462298974395, 0.4147622878663242, 0.309999227989465, 0.2940559950657189, 0.31006209063343704, 0.31154210097156465, 0.3253114577382803, 0.30937863397412, 0.44175312994048, 0.3220668649300933, 0.3301373419817537, 0.33341845381073654, 0.36602262989617884, 0.3362632277421653, 0.33005440537817776, 0.3363063039723784, 0.3379882862791419, 0.44359402009285986, 0.32480856589972973, 0.32632288499735296, 0.374951713019982, 0.3457673208322376, 0.33689655223861337, 0.3370611888822168, 0.44384948909282684, 0.3295612807851285, 0.32693483284674585, 0.4607991217635572, 0.3521246600430459, 0.3576899799518287, 0.35363939101807773, 0.46972738578915596, 0.32778852619230747, 0.31827206816524267, 0.3053914667107165, 0.3249905451666564, 0.33829308208078146, 0.3261483197566122, 0.3288996829651296, 0.5032677398994565, 0.33459547301754355, 0.45089355879463255, 0.32561771222390234, 0.32810927485115826, 0.33207867201417685, 0.4363605980761349, 0.3068346872460097, 0.3090620981529355, 0.3110687369480729, 0.32035329821519554, 0.32028826884925365, 0.3234013859182596, 0.3221008370164782, 0.41277748602442443, 0.326036118902266, 0.33022063388489187, 0.3222571078222245, 0.31957150204107165, 0.33099480206146836, 0.3317452019546181, 0.33037764905020595, 0.32684463798068464, 0.32571095740422606, 0.33463175501674414, 0.32584630185738206, 0.34974306891672313, 0.3429857990704477, 0.3359711973462254, 0.3383807698264718, 0.3322305439505726, 0.42358800675719976, 0.3269426131155342, 0.3252587737515569, 0.33346529793925583, 0.3261386330705136, 0.3315090381074697, 0.3396035262849182, 0.5204557578545064, 0.3163921027444303, 0.32778933085501194, 0.3314213070552796, 0.321593209868297, 0.30323536274954677, 0.31034513004124165, 0.3075799630023539, 0.30343314120545983, 0.31108833593316376, 0.3127777208574116, 0.317726070061326, 0.3309359832201153, 0.3256208631210029, 0.32082078023813665, 0.31993550690822303, 0.3140900528524071, 0.3327363929711282, 0.4348333589732647, 0.31860925001092255, 0.33262963593006134, 0.3262851170729846, 0.3283311980776489, 0.38616717979311943, 6.299010417191312, 0.31441258592531085, 0.4092104386072606, 0.322374630253762, 0.33212089724838734, 0.32262441981583834, 0.3259827808942646, 0.33311398583464324, 0.3256727328989655, 0.3512640430126339, 0.374843591125682, 0.3113269782625139, 0.31116358982399106, 0.32021188107319176, 0.32219952601008117, 0.3339206089731306, 0.3179791229777038, 0.398870985256508, 0.3068691329099238, 0.3148883709218353, 0.31832918687723577, 0.3080266220495105, 0.31767632300034165, 0.32102127908729017, 0.31685388274490833, 0.46298586390912533, 0.3270374550484121, 0.3230587891303003]
Total Epoch List: [240, 129, 176]
Total Time List: [0.10240444797091186, 0.09619483700953424, 0.08968999702483416]
T-times Epoch Time: 0.3798463622507244 ~ 0.007979696571650851
T-times Total Epoch: 135.88888888888889 ~ 50.204274079031045
T-times Total Time: 0.09415317044800355 ~ 0.001379338150952243
T-times Inference Elapsed: 0.11187310225440043 ~ 0.001971578917666514
T-times Time Per Graph: 0.0022997764450792425 ~ 4.133898555321396e-05
T-times Speed: 506.6239870611698 ~ 3.292548932710229
T-times cross validation test micro f1 score:0.8686175352040292 ~ 0.10250203707065232
T-times cross validation test precision:0.8421101117678361 ~ 0.10939075637562096
T-times cross validation test recall:0.9316666666666666 ~ 0.029232289492463506
T-times cross validation test f1_score:0.8686175352040292 ~ 0.06127437802340853
