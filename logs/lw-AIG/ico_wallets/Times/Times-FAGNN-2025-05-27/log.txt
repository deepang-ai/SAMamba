Namespace(seed=15, model='FAGNN', dataset='ico_wallets/Times', num_heads=8, num_layers=2, dim_hidden=64, dropout=0.1, epochs=1000, lr=0.0003, weight_decay=0.0001, batch_size=64, abs_pe=None, abs_pe_dim=20, num_class=2, outdir='./outdir/ico_wallets/Times/seed15/khopgnn_gat_1_0.1_0.0003_0.0001_2_8_64_BN', warmup=10, layer_norm=False, use_edge_attr=False, edge_dim=128, gnn_type='gat', k_hop=1, global_pool='mean', se='khopgnn', ie='mamba', aggr='add', not_extract_node_feature=False, profile=False, k_ford=3, early_stop=1, early_stop_mindelta=-0.0, patience=20, training_times=3, Lambda=0.01, order='sd', use_cuda=True, batch_norm=True, save_logs=True)
Data(edge_index=[2, 338], edge_attr=[338, 2], x=[122, 14887], y=[1, 1], num_nodes=122)
Data(edge_index=[2, 338], edge_attr=[338, 2], x=[122, 14887], y=[1, 1], num_nodes=122)
Data(edge_index=[2, 298], edge_attr=[298, 2], x=[109, 14887], y=[1, 1], num_nodes=122)
========================training times:0========================
========================k_idx:0========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: FAGNN
FAGNN(
  (conv1): NeighborFilterConv()
  (conv2): NeighborFilterConv()
  (ffn): Sequential(
    (0): Linear(in_features=14951, out_features=64, bias=True)
    (1): ReLU()
    (2): Linear(in_features=64, out_features=64, bias=True)
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=64, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=64, out_features=2, bias=True)
  )
)
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7ed84880b730>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.6932;  Loss pred: 0.6932; Loss self: 0.0000; time: 1.33s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6931 score: 0.5102 time: 0.48s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6933 score: 0.4898 time: 0.41s
Epoch 2/1000, LR 0.000000
Train loss: 0.6932;  Loss pred: 0.6932; Loss self: 0.0000; time: 0.74s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6931 score: 0.5102 time: 0.49s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6933 score: 0.4898 time: 0.49s
Epoch 3/1000, LR 0.000030
Train loss: 0.6932;  Loss pred: 0.6932; Loss self: 0.0000; time: 0.55s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6931 score: 0.5102 time: 0.61s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6932 score: 0.4898 time: 0.37s
Epoch 4/1000, LR 0.000060
Train loss: 0.6931;  Loss pred: 0.6931; Loss self: 0.0000; time: 0.53s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6930 score: 0.5102 time: 0.60s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6932 score: 0.4898 time: 0.57s
Epoch 5/1000, LR 0.000090
Train loss: 0.6931;  Loss pred: 0.6931; Loss self: 0.0000; time: 0.66s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6930 score: 0.5102 time: 0.61s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6931 score: 0.4898 time: 0.38s
Epoch 6/1000, LR 0.000120
Train loss: 0.6930;  Loss pred: 0.6930; Loss self: 0.0000; time: 0.54s
Val loss: 0.6929 score: 0.5510 time: 0.65s
Test loss: 0.6930 score: 0.5510 time: 0.38s
Epoch 7/1000, LR 0.000150
Train loss: 0.6929;  Loss pred: 0.6929; Loss self: 0.0000; time: 0.54s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6929 score: 0.4898 time: 0.51s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6929 score: 0.5102 time: 0.44s
Epoch 8/1000, LR 0.000180
Train loss: 0.6929;  Loss pred: 0.6929; Loss self: 0.0000; time: 0.71s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6928 score: 0.4898 time: 0.48s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6928 score: 0.5102 time: 0.38s
Epoch 9/1000, LR 0.000210
Train loss: 0.6927;  Loss pred: 0.6927; Loss self: 0.0000; time: 0.54s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6926 score: 0.4898 time: 0.51s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6927 score: 0.5102 time: 0.51s
Epoch 10/1000, LR 0.000240
Train loss: 0.6926;  Loss pred: 0.6926; Loss self: 0.0000; time: 0.64s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6925 score: 0.4898 time: 0.49s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6925 score: 0.5102 time: 0.37s
Epoch 11/1000, LR 0.000270
Train loss: 0.6924;  Loss pred: 0.6924; Loss self: 0.0000; time: 0.57s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6922 score: 0.4898 time: 0.92s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6924 score: 0.5102 time: 0.47s
Epoch 12/1000, LR 0.000270
Train loss: 0.6922;  Loss pred: 0.6922; Loss self: 0.0000; time: 0.66s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6920 score: 0.4898 time: 0.61s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6922 score: 0.5102 time: 0.49s
Epoch 13/1000, LR 0.000270
Train loss: 0.6920;  Loss pred: 0.6920; Loss self: 0.0000; time: 0.76s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6917 score: 0.4898 time: 0.52s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6920 score: 0.5102 time: 0.47s
Epoch 14/1000, LR 0.000270
Train loss: 0.6918;  Loss pred: 0.6918; Loss self: 0.0000; time: 0.58s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6914 score: 0.4898 time: 0.53s
Test loss: 0.6918 score: 0.5306 time: 0.38s
Epoch 15/1000, LR 0.000270
Train loss: 0.6915;  Loss pred: 0.6915; Loss self: 0.0000; time: 0.73s
Val loss: 0.6910 score: 0.5306 time: 0.48s
Test loss: 0.6916 score: 0.5714 time: 0.37s
Epoch 16/1000, LR 0.000270
Train loss: 0.6912;  Loss pred: 0.6912; Loss self: 0.0000; time: 0.58s
Val loss: 0.6906 score: 0.5306 time: 0.49s
Test loss: 0.6913 score: 0.5714 time: 0.46s
Epoch 17/1000, LR 0.000270
Train loss: 0.6908;  Loss pred: 0.6908; Loss self: 0.0000; time: 0.55s
Val loss: 0.6902 score: 0.5714 time: 0.49s
Test loss: 0.6910 score: 0.5714 time: 0.45s
Epoch 18/1000, LR 0.000270
Train loss: 0.6904;  Loss pred: 0.6904; Loss self: 0.0000; time: 0.74s
Val loss: 0.6897 score: 0.5714 time: 0.55s
Test loss: 0.6907 score: 0.5714 time: 0.48s
Epoch 19/1000, LR 0.000270
Train loss: 0.6899;  Loss pred: 0.6899; Loss self: 0.0000; time: 0.55s
Val loss: 0.6891 score: 0.5714 time: 0.51s
Test loss: 0.6903 score: 0.5714 time: 0.38s
Epoch 20/1000, LR 0.000270
Train loss: 0.6894;  Loss pred: 0.6894; Loss self: 0.0000; time: 0.61s
Val loss: 0.6884 score: 0.5918 time: 0.73s
Test loss: 0.6898 score: 0.5714 time: 0.47s
Epoch 21/1000, LR 0.000270
Train loss: 0.6888;  Loss pred: 0.6888; Loss self: 0.0000; time: 0.78s
Val loss: 0.6877 score: 0.5918 time: 0.50s
Test loss: 0.6893 score: 0.5918 time: 0.37s
Epoch 22/1000, LR 0.000270
Train loss: 0.6882;  Loss pred: 0.6882; Loss self: 0.0000; time: 0.64s
Val loss: 0.6868 score: 0.5918 time: 0.52s
Test loss: 0.6887 score: 0.5918 time: 0.37s
Epoch 23/1000, LR 0.000270
Train loss: 0.6872;  Loss pred: 0.6872; Loss self: 0.0000; time: 0.54s
Val loss: 0.6859 score: 0.5918 time: 0.49s
Test loss: 0.6880 score: 0.5918 time: 0.48s
Epoch 24/1000, LR 0.000270
Train loss: 0.6863;  Loss pred: 0.6863; Loss self: 0.0000; time: 0.58s
Val loss: 0.6848 score: 0.5918 time: 0.49s
Test loss: 0.6873 score: 0.5918 time: 0.36s
Epoch 25/1000, LR 0.000270
Train loss: 0.6854;  Loss pred: 0.6854; Loss self: 0.0000; time: 0.53s
Val loss: 0.6837 score: 0.5918 time: 0.61s
Test loss: 0.6864 score: 0.5918 time: 0.39s
Epoch 26/1000, LR 0.000270
Train loss: 0.6845;  Loss pred: 0.6845; Loss self: 0.0000; time: 0.59s
Val loss: 0.6824 score: 0.5918 time: 0.48s
Test loss: 0.6855 score: 0.5918 time: 0.37s
Epoch 27/1000, LR 0.000270
Train loss: 0.6829;  Loss pred: 0.6829; Loss self: 0.0000; time: 0.64s
Val loss: 0.6811 score: 0.5918 time: 0.48s
Test loss: 0.6845 score: 0.5918 time: 0.37s
Epoch 28/1000, LR 0.000270
Train loss: 0.6814;  Loss pred: 0.6814; Loss self: 0.0000; time: 0.54s
Val loss: 0.6796 score: 0.5918 time: 0.49s
Test loss: 0.6835 score: 0.6122 time: 0.37s
Epoch 29/1000, LR 0.000270
Train loss: 0.6803;  Loss pred: 0.6803; Loss self: 0.0000; time: 0.65s
Val loss: 0.6780 score: 0.5918 time: 0.49s
Test loss: 0.6823 score: 0.6122 time: 0.40s
Epoch 30/1000, LR 0.000270
Train loss: 0.6790;  Loss pred: 0.6790; Loss self: 0.0000; time: 0.55s
Val loss: 0.6763 score: 0.5918 time: 0.54s
Test loss: 0.6810 score: 0.6122 time: 0.57s
Epoch 31/1000, LR 0.000270
Train loss: 0.6771;  Loss pred: 0.6771; Loss self: 0.0000; time: 0.66s
Val loss: 0.6744 score: 0.5918 time: 0.61s
Test loss: 0.6796 score: 0.6122 time: 0.39s
Epoch 32/1000, LR 0.000270
Train loss: 0.6752;  Loss pred: 0.6752; Loss self: 0.0000; time: 0.53s
Val loss: 0.6722 score: 0.5918 time: 0.49s
Test loss: 0.6781 score: 0.6122 time: 0.47s
Epoch 33/1000, LR 0.000270
Train loss: 0.6730;  Loss pred: 0.6730; Loss self: 0.0000; time: 0.53s
Val loss: 0.6699 score: 0.5918 time: 0.49s
Test loss: 0.6764 score: 0.6122 time: 0.37s
Epoch 34/1000, LR 0.000270
Train loss: 0.6708;  Loss pred: 0.6708; Loss self: 0.0000; time: 0.53s
Val loss: 0.6674 score: 0.5918 time: 0.58s
Test loss: 0.6745 score: 0.6327 time: 0.38s
Epoch 35/1000, LR 0.000270
Train loss: 0.6679;  Loss pred: 0.6679; Loss self: 0.0000; time: 0.53s
Val loss: 0.6646 score: 0.6122 time: 0.47s
Test loss: 0.6725 score: 0.6531 time: 0.37s
Epoch 36/1000, LR 0.000270
Train loss: 0.6649;  Loss pred: 0.6649; Loss self: 0.0000; time: 0.72s
Val loss: 0.6615 score: 0.6327 time: 0.50s
Test loss: 0.6703 score: 0.6327 time: 0.37s
Epoch 37/1000, LR 0.000270
Train loss: 0.6629;  Loss pred: 0.6629; Loss self: 0.0000; time: 0.52s
Val loss: 0.6582 score: 0.6327 time: 0.49s
Test loss: 0.6679 score: 0.6327 time: 0.47s
Epoch 38/1000, LR 0.000270
Train loss: 0.6600;  Loss pred: 0.6600; Loss self: 0.0000; time: 0.57s
Val loss: 0.6546 score: 0.6939 time: 0.49s
Test loss: 0.6654 score: 0.6327 time: 0.37s
Epoch 39/1000, LR 0.000269
Train loss: 0.6569;  Loss pred: 0.6569; Loss self: 0.0000; time: 0.55s
Val loss: 0.6508 score: 0.7959 time: 0.61s
Test loss: 0.6626 score: 0.6531 time: 0.38s
Epoch 40/1000, LR 0.000269
Train loss: 0.6514;  Loss pred: 0.6514; Loss self: 0.0000; time: 0.58s
Val loss: 0.6466 score: 0.8163 time: 0.49s
Test loss: 0.6597 score: 0.6735 time: 0.39s
Epoch 41/1000, LR 0.000269
Train loss: 0.6494;  Loss pred: 0.6494; Loss self: 0.0000; time: 0.65s
Val loss: 0.6422 score: 0.8163 time: 0.49s
Test loss: 0.6564 score: 0.6735 time: 0.41s
Epoch 42/1000, LR 0.000269
Train loss: 0.6436;  Loss pred: 0.6436; Loss self: 0.0000; time: 0.54s
Val loss: 0.6373 score: 0.8163 time: 0.64s
Test loss: 0.6529 score: 0.6735 time: 0.38s
Epoch 43/1000, LR 0.000269
Train loss: 0.6375;  Loss pred: 0.6375; Loss self: 0.0000; time: 0.66s
Val loss: 0.6321 score: 0.8571 time: 0.57s
Test loss: 0.6490 score: 0.7143 time: 0.38s
Epoch 44/1000, LR 0.000269
Train loss: 0.6314;  Loss pred: 0.6314; Loss self: 0.0000; time: 0.54s
Val loss: 0.6264 score: 0.8571 time: 0.48s
Test loss: 0.6449 score: 0.7755 time: 0.49s
Epoch 45/1000, LR 0.000269
Train loss: 0.6282;  Loss pred: 0.6282; Loss self: 0.0000; time: 0.58s
Val loss: 0.6203 score: 0.8571 time: 0.50s
Test loss: 0.6404 score: 0.8163 time: 0.37s
Epoch 46/1000, LR 0.000269
Train loss: 0.6189;  Loss pred: 0.6189; Loss self: 0.0000; time: 0.54s
Val loss: 0.6136 score: 0.8571 time: 0.50s
Test loss: 0.6355 score: 0.8367 time: 0.50s
Epoch 47/1000, LR 0.000269
Train loss: 0.6133;  Loss pred: 0.6133; Loss self: 0.0000; time: 0.59s
Val loss: 0.6065 score: 0.8776 time: 0.49s
Test loss: 0.6302 score: 0.8571 time: 0.38s
Epoch 48/1000, LR 0.000269
Train loss: 0.6066;  Loss pred: 0.6066; Loss self: 0.0000; time: 0.55s
Val loss: 0.5988 score: 0.8980 time: 0.62s
Test loss: 0.6245 score: 0.8571 time: 0.40s
Epoch 49/1000, LR 0.000269
Train loss: 0.5970;  Loss pred: 0.5970; Loss self: 0.0000; time: 0.54s
Val loss: 0.5907 score: 0.9184 time: 0.49s
Test loss: 0.6184 score: 0.8571 time: 0.39s
Epoch 50/1000, LR 0.000269
Train loss: 0.5865;  Loss pred: 0.5865; Loss self: 0.0000; time: 0.64s
Val loss: 0.5819 score: 0.9184 time: 0.78s
Test loss: 0.6119 score: 0.8571 time: 0.48s
Epoch 51/1000, LR 0.000269
Train loss: 0.5823;  Loss pred: 0.5823; Loss self: 0.0000; time: 0.69s
Val loss: 0.5727 score: 0.9592 time: 0.63s
Test loss: 0.6051 score: 0.8571 time: 0.59s
Epoch 52/1000, LR 0.000269
Train loss: 0.5688;  Loss pred: 0.5688; Loss self: 0.0000; time: 0.54s
Val loss: 0.5628 score: 0.9592 time: 0.52s
Test loss: 0.5978 score: 0.8571 time: 0.38s
Epoch 53/1000, LR 0.000269
Train loss: 0.5626;  Loss pred: 0.5626; Loss self: 0.0000; time: 0.55s
Val loss: 0.5523 score: 0.9592 time: 0.70s
Test loss: 0.5903 score: 0.8571 time: 0.36s
Epoch 54/1000, LR 0.000269
Train loss: 0.5557;  Loss pred: 0.5557; Loss self: 0.0000; time: 0.53s
Val loss: 0.5412 score: 0.9592 time: 0.59s
Test loss: 0.5824 score: 0.8571 time: 2.83s
Epoch 55/1000, LR 0.000269
Train loss: 0.5417;  Loss pred: 0.5417; Loss self: 0.0000; time: 1.95s
Val loss: 0.5295 score: 0.9388 time: 0.84s
Test loss: 0.5741 score: 0.8571 time: 0.40s
Epoch 56/1000, LR 0.000269
Train loss: 0.5267;  Loss pred: 0.5267; Loss self: 0.0000; time: 0.53s
Val loss: 0.5173 score: 0.9388 time: 0.50s
Test loss: 0.5655 score: 0.8571 time: 0.38s
Epoch 57/1000, LR 0.000269
Train loss: 0.5133;  Loss pred: 0.5133; Loss self: 0.0000; time: 0.64s
Val loss: 0.5046 score: 0.9388 time: 0.54s
Test loss: 0.5564 score: 0.8571 time: 0.37s
Epoch 58/1000, LR 0.000269
Train loss: 0.4971;  Loss pred: 0.4971; Loss self: 0.0000; time: 0.54s
Val loss: 0.4915 score: 0.9388 time: 0.51s
Test loss: 0.5470 score: 0.8367 time: 0.47s
Epoch 59/1000, LR 0.000268
Train loss: 0.4838;  Loss pred: 0.4838; Loss self: 0.0000; time: 0.57s
Val loss: 0.4779 score: 0.9388 time: 0.50s
Test loss: 0.5373 score: 0.8367 time: 0.37s
Epoch 60/1000, LR 0.000268
Train loss: 0.4746;  Loss pred: 0.4746; Loss self: 0.0000; time: 0.54s
Val loss: 0.4640 score: 0.9388 time: 0.50s
Test loss: 0.5272 score: 0.8367 time: 0.50s
Epoch 61/1000, LR 0.000268
Train loss: 0.4590;  Loss pred: 0.4590; Loss self: 0.0000; time: 0.55s
Val loss: 0.4498 score: 0.9388 time: 0.48s
Test loss: 0.5171 score: 0.8367 time: 0.38s
Epoch 62/1000, LR 0.000268
Train loss: 0.4492;  Loss pred: 0.4492; Loss self: 0.0000; time: 0.54s
Val loss: 0.4354 score: 0.9388 time: 0.63s
Test loss: 0.5069 score: 0.8163 time: 0.61s
Epoch 63/1000, LR 0.000268
Train loss: 0.4333;  Loss pred: 0.4333; Loss self: 0.0000; time: 0.56s
Val loss: 0.4209 score: 0.9592 time: 0.50s
Test loss: 0.4968 score: 0.8163 time: 0.39s
Epoch 64/1000, LR 0.000268
Train loss: 0.4148;  Loss pred: 0.4148; Loss self: 0.0000; time: 0.65s
Val loss: 0.4062 score: 0.9592 time: 0.49s
Test loss: 0.4868 score: 0.8163 time: 0.39s
Epoch 65/1000, LR 0.000268
Train loss: 0.3980;  Loss pred: 0.3980; Loss self: 0.0000; time: 0.55s
Val loss: 0.3916 score: 0.9592 time: 0.50s
Test loss: 0.4769 score: 0.8163 time: 0.48s
Epoch 66/1000, LR 0.000268
Train loss: 0.3909;  Loss pred: 0.3909; Loss self: 0.0000; time: 0.54s
Val loss: 0.3769 score: 0.9388 time: 0.49s
Test loss: 0.4671 score: 0.8163 time: 0.38s
Epoch 67/1000, LR 0.000268
Train loss: 0.3750;  Loss pred: 0.3750; Loss self: 0.0000; time: 0.55s
Val loss: 0.3625 score: 0.9388 time: 0.61s
Test loss: 0.4574 score: 0.8163 time: 0.37s
Epoch 68/1000, LR 0.000268
Train loss: 0.3523;  Loss pred: 0.3523; Loss self: 0.0000; time: 0.54s
Val loss: 0.3485 score: 0.9388 time: 0.49s
Test loss: 0.4482 score: 0.8163 time: 0.37s
Epoch 69/1000, LR 0.000268
Train loss: 0.3420;  Loss pred: 0.3420; Loss self: 0.0000; time: 0.64s
Val loss: 0.3348 score: 0.9388 time: 0.58s
Test loss: 0.4391 score: 0.8163 time: 0.46s
Epoch 70/1000, LR 0.000268
Train loss: 0.3296;  Loss pred: 0.3296; Loss self: 0.0000; time: 0.70s
Val loss: 0.3214 score: 0.9388 time: 0.52s
Test loss: 0.4303 score: 0.8163 time: 0.38s
Epoch 71/1000, LR 0.000268
Train loss: 0.3155;  Loss pred: 0.3155; Loss self: 0.0000; time: 0.68s
Val loss: 0.3085 score: 0.9388 time: 0.51s
Test loss: 0.4216 score: 0.8163 time: 0.39s
Epoch 72/1000, LR 0.000267
Train loss: 0.3020;  Loss pred: 0.3020; Loss self: 0.0000; time: 0.56s
Val loss: 0.2963 score: 0.9388 time: 0.62s
Test loss: 0.4138 score: 0.8163 time: 0.50s
Epoch 73/1000, LR 0.000267
Train loss: 0.2857;  Loss pred: 0.2857; Loss self: 0.0000; time: 0.54s
Val loss: 0.2846 score: 0.9388 time: 0.50s
Test loss: 0.4063 score: 0.8163 time: 0.42s
Epoch 74/1000, LR 0.000267
Train loss: 0.2732;  Loss pred: 0.2732; Loss self: 0.0000; time: 0.54s
Val loss: 0.2735 score: 0.9388 time: 0.51s
Test loss: 0.3991 score: 0.8163 time: 0.52s
Epoch 75/1000, LR 0.000267
Train loss: 0.2525;  Loss pred: 0.2525; Loss self: 0.0000; time: 0.54s
Val loss: 0.2628 score: 0.9388 time: 0.52s
Test loss: 0.3925 score: 0.8367 time: 0.37s
Epoch 76/1000, LR 0.000267
Train loss: 0.2503;  Loss pred: 0.2503; Loss self: 0.0000; time: 0.54s
Val loss: 0.2527 score: 0.9388 time: 0.58s
Test loss: 0.3866 score: 0.8367 time: 0.38s
Epoch 77/1000, LR 0.000267
Train loss: 0.2334;  Loss pred: 0.2334; Loss self: 0.0000; time: 0.56s
Val loss: 0.2431 score: 0.9388 time: 0.49s
Test loss: 0.3814 score: 0.8367 time: 0.37s
Epoch 78/1000, LR 0.000267
Train loss: 0.2297;  Loss pred: 0.2297; Loss self: 0.0000; time: 0.62s
Val loss: 0.2341 score: 0.9388 time: 0.49s
Test loss: 0.3763 score: 0.8367 time: 0.38s
Epoch 79/1000, LR 0.000267
Train loss: 0.2128;  Loss pred: 0.2128; Loss self: 0.0000; time: 0.57s
Val loss: 0.2256 score: 0.9388 time: 0.48s
Test loss: 0.3714 score: 0.8571 time: 0.47s
Epoch 80/1000, LR 0.000267
Train loss: 0.1971;  Loss pred: 0.1971; Loss self: 0.0000; time: 0.56s
Val loss: 0.2177 score: 0.9388 time: 0.49s
Test loss: 0.3677 score: 0.8571 time: 0.38s
Epoch 81/1000, LR 0.000267
Train loss: 0.1835;  Loss pred: 0.1835; Loss self: 0.0000; time: 0.53s
Val loss: 0.2103 score: 0.9388 time: 0.60s
Test loss: 0.3644 score: 0.8571 time: 0.38s
Epoch 82/1000, LR 0.000267
Train loss: 0.1817;  Loss pred: 0.1817; Loss self: 0.0000; time: 0.54s
Val loss: 0.2034 score: 0.9388 time: 0.52s
Test loss: 0.3619 score: 0.8571 time: 0.41s
Epoch 83/1000, LR 0.000266
Train loss: 0.1713;  Loss pred: 0.1713; Loss self: 0.0000; time: 0.63s
Val loss: 0.1971 score: 0.9388 time: 0.49s
Test loss: 0.3600 score: 0.8571 time: 0.39s
Epoch 84/1000, LR 0.000266
Train loss: 0.1901;  Loss pred: 0.1901; Loss self: 0.0000; time: 0.58s
Val loss: 0.1914 score: 0.9388 time: 0.50s
Test loss: 0.3580 score: 0.8571 time: 0.37s
Epoch 85/1000, LR 0.000266
Train loss: 0.1513;  Loss pred: 0.1513; Loss self: 0.0000; time: 0.65s
Val loss: 0.1862 score: 0.9388 time: 0.50s
Test loss: 0.3570 score: 0.8571 time: 0.38s
Epoch 86/1000, LR 0.000266
Train loss: 0.1400;  Loss pred: 0.1400; Loss self: 0.0000; time: 0.57s
Val loss: 0.1816 score: 0.9388 time: 0.49s
Test loss: 0.3570 score: 0.8571 time: 0.48s
Epoch 87/1000, LR 0.000266
Train loss: 0.1462;  Loss pred: 0.1462; Loss self: 0.0000; time: 0.56s
Val loss: 0.1778 score: 0.9388 time: 0.52s
Test loss: 0.3562 score: 0.8367 time: 0.41s
Epoch 88/1000, LR 0.000266
Train loss: 0.1502;  Loss pred: 0.1502; Loss self: 0.0000; time: 0.56s
Val loss: 0.1750 score: 0.9388 time: 0.50s
Test loss: 0.3548 score: 0.8367 time: 0.51s
Epoch 89/1000, LR 0.000266
Train loss: 0.1368;  Loss pred: 0.1368; Loss self: 0.0000; time: 0.56s
Val loss: 0.1731 score: 0.9184 time: 0.66s
Test loss: 0.3540 score: 0.8367 time: 0.42s
Epoch 90/1000, LR 0.000266
Train loss: 0.1222;  Loss pred: 0.1222; Loss self: 0.0000; time: 0.57s
Val loss: 0.1720 score: 0.9184 time: 0.59s
Test loss: 0.3540 score: 0.8571 time: 0.37s
Epoch 91/1000, LR 0.000266
Train loss: 0.1104;  Loss pred: 0.1104; Loss self: 0.0000; time: 0.57s
Val loss: 0.1712 score: 0.9184 time: 0.50s
Test loss: 0.3558 score: 0.8571 time: 0.37s
Epoch 92/1000, LR 0.000266
Train loss: 0.1209;  Loss pred: 0.1209; Loss self: 0.0000; time: 0.63s
Val loss: 0.1708 score: 0.9184 time: 0.50s
Test loss: 0.3586 score: 0.8571 time: 0.37s
Epoch 93/1000, LR 0.000265
Train loss: 0.1030;  Loss pred: 0.1030; Loss self: 0.0000; time: 0.58s
Val loss: 0.1703 score: 0.9184 time: 0.50s
Test loss: 0.3626 score: 0.8571 time: 0.48s
Epoch 94/1000, LR 0.000265
Train loss: 0.0986;  Loss pred: 0.0986; Loss self: 0.0000; time: 0.54s
Val loss: 0.1695 score: 0.9184 time: 0.49s
Test loss: 0.3680 score: 0.8571 time: 0.46s
Epoch 95/1000, LR 0.000265
Train loss: 0.0926;  Loss pred: 0.0926; Loss self: 0.0000; time: 0.53s
Val loss: 0.1683 score: 0.9184 time: 0.65s
Test loss: 0.3748 score: 0.8367 time: 0.47s
Epoch 96/1000, LR 0.000265
Train loss: 0.0866;  Loss pred: 0.0866; Loss self: 0.0000; time: 0.67s
Val loss: 0.1671 score: 0.9184 time: 0.66s
Test loss: 0.3827 score: 0.8367 time: 0.47s
Epoch 97/1000, LR 0.000265
Train loss: 0.0818;  Loss pred: 0.0818; Loss self: 0.0000; time: 0.77s
Val loss: 0.1656 score: 0.9184 time: 0.49s
Test loss: 0.3916 score: 0.8367 time: 0.40s
Epoch 98/1000, LR 0.000265
Train loss: 0.0741;  Loss pred: 0.0741; Loss self: 0.0000; time: 0.61s
Val loss: 0.1637 score: 0.9184 time: 0.49s
Test loss: 0.4016 score: 0.8163 time: 0.37s
Epoch 99/1000, LR 0.000265
Train loss: 0.0779;  Loss pred: 0.0779; Loss self: 0.0000; time: 0.64s
Val loss: 0.1616 score: 0.9388 time: 0.51s
Test loss: 0.4118 score: 0.8163 time: 0.37s
Epoch 100/1000, LR 0.000265
Train loss: 0.0716;  Loss pred: 0.0716; Loss self: 0.0000; time: 0.55s
Val loss: 0.1590 score: 0.9388 time: 0.49s
Test loss: 0.4237 score: 0.8163 time: 0.48s
Epoch 101/1000, LR 0.000265
Train loss: 0.0679;  Loss pred: 0.0679; Loss self: 0.0000; time: 0.54s
Val loss: 0.1565 score: 0.9388 time: 0.74s
Test loss: 0.4364 score: 0.8163 time: 0.47s
Epoch 102/1000, LR 0.000264
Train loss: 0.0594;  Loss pred: 0.0594; Loss self: 0.0000; time: 0.69s
Val loss: 0.1542 score: 0.9388 time: 0.64s
Test loss: 0.4499 score: 0.8163 time: 0.59s
Epoch 103/1000, LR 0.000264
Train loss: 0.0619;  Loss pred: 0.0619; Loss self: 0.0000; time: 0.70s
Val loss: 0.1526 score: 0.9388 time: 0.66s
Test loss: 0.4629 score: 0.8163 time: 0.62s
Epoch 104/1000, LR 0.000264
Train loss: 0.0558;  Loss pred: 0.0558; Loss self: 0.0000; time: 0.56s
Val loss: 0.1522 score: 0.9388 time: 0.61s
Test loss: 0.4734 score: 0.7959 time: 0.51s
Epoch 105/1000, LR 0.000264
Train loss: 0.0537;  Loss pred: 0.0537; Loss self: 0.0000; time: 0.71s
Val loss: 0.1521 score: 0.9388 time: 0.67s
Test loss: 0.4833 score: 0.7959 time: 0.50s
Epoch 106/1000, LR 0.000264
Train loss: 0.0503;  Loss pred: 0.0503; Loss self: 0.0000; time: 0.77s
Val loss: 0.1523 score: 0.9388 time: 0.54s
Test loss: 0.4922 score: 0.7959 time: 0.44s
     INFO: Early stopping counter 1 of 20
Epoch 107/1000, LR 0.000264
Train loss: 0.0498;  Loss pred: 0.0498; Loss self: 0.0000; time: 0.61s
Val loss: 0.1530 score: 0.9388 time: 0.67s
Test loss: 0.5000 score: 0.7755 time: 0.62s
     INFO: Early stopping counter 2 of 20
Epoch 108/1000, LR 0.000264
Train loss: 0.0468;  Loss pred: 0.0468; Loss self: 0.0000; time: 0.62s
Val loss: 0.1540 score: 0.9388 time: 0.62s
Test loss: 0.5072 score: 0.7755 time: 0.47s
     INFO: Early stopping counter 3 of 20
Epoch 109/1000, LR 0.000264
Train loss: 0.0530;  Loss pred: 0.0530; Loss self: 0.0000; time: 0.67s
Val loss: 0.1574 score: 0.9388 time: 0.73s
Test loss: 0.5082 score: 0.7959 time: 0.37s
     INFO: Early stopping counter 4 of 20
Epoch 110/1000, LR 0.000263
Train loss: 0.0452;  Loss pred: 0.0452; Loss self: 0.0000; time: 0.54s
Val loss: 0.1616 score: 0.9388 time: 0.49s
Test loss: 0.5092 score: 0.7959 time: 0.38s
     INFO: Early stopping counter 5 of 20
Epoch 111/1000, LR 0.000263
Train loss: 0.0515;  Loss pred: 0.0515; Loss self: 0.0000; time: 0.66s
Val loss: 0.1684 score: 0.9388 time: 0.52s
Test loss: 0.5070 score: 0.8163 time: 0.38s
     INFO: Early stopping counter 6 of 20
Epoch 112/1000, LR 0.000263
Train loss: 0.0419;  Loss pred: 0.0419; Loss self: 0.0000; time: 0.54s
Val loss: 0.1775 score: 0.9184 time: 0.50s
Test loss: 0.5052 score: 0.8163 time: 0.38s
     INFO: Early stopping counter 7 of 20
Epoch 113/1000, LR 0.000263
Train loss: 0.0357;  Loss pred: 0.0357; Loss self: 0.0000; time: 0.68s
Val loss: 0.1868 score: 0.9184 time: 0.50s
Test loss: 0.5062 score: 0.8163 time: 0.38s
     INFO: Early stopping counter 8 of 20
Epoch 114/1000, LR 0.000263
Train loss: 0.0331;  Loss pred: 0.0331; Loss self: 0.0000; time: 0.55s
Val loss: 0.1960 score: 0.9184 time: 0.62s
Test loss: 0.5090 score: 0.8163 time: 0.61s
     INFO: Early stopping counter 9 of 20
Epoch 115/1000, LR 0.000263
Train loss: 0.0444;  Loss pred: 0.0444; Loss self: 0.0000; time: 0.63s
Val loss: 0.2080 score: 0.9184 time: 0.50s
Test loss: 0.5120 score: 0.8163 time: 0.37s
     INFO: Early stopping counter 10 of 20
Epoch 116/1000, LR 0.000263
Train loss: 0.0340;  Loss pred: 0.0340; Loss self: 0.0000; time: 0.55s
Val loss: 0.2186 score: 0.9184 time: 0.66s
Test loss: 0.5161 score: 0.8163 time: 0.47s
     INFO: Early stopping counter 11 of 20
Epoch 117/1000, LR 0.000262
Train loss: 0.0332;  Loss pred: 0.0332; Loss self: 0.0000; time: 0.55s
Val loss: 0.2270 score: 0.9184 time: 0.48s
Test loss: 0.5214 score: 0.8163 time: 0.38s
     INFO: Early stopping counter 12 of 20
Epoch 118/1000, LR 0.000262
Train loss: 0.0323;  Loss pred: 0.0323; Loss self: 0.0000; time: 0.54s
Val loss: 0.2341 score: 0.9184 time: 0.69s
Test loss: 0.5273 score: 0.8163 time: 0.38s
     INFO: Early stopping counter 13 of 20
Epoch 119/1000, LR 0.000262
Train loss: 0.0316;  Loss pred: 0.0316; Loss self: 0.0000; time: 0.53s
Val loss: 0.2396 score: 0.9184 time: 0.50s
Test loss: 0.5336 score: 0.8367 time: 0.41s
     INFO: Early stopping counter 14 of 20
Epoch 120/1000, LR 0.000262
Train loss: 0.0328;  Loss pred: 0.0328; Loss self: 0.0000; time: 0.64s
Val loss: 0.2439 score: 0.9184 time: 0.49s
Test loss: 0.5399 score: 0.8367 time: 0.38s
     INFO: Early stopping counter 15 of 20
Epoch 121/1000, LR 0.000262
Train loss: 0.0325;  Loss pred: 0.0325; Loss self: 0.0000; time: 0.55s
Val loss: 0.2480 score: 0.9184 time: 0.52s
Test loss: 0.5463 score: 0.8367 time: 0.52s
     INFO: Early stopping counter 16 of 20
Epoch 122/1000, LR 0.000262
Train loss: 0.0281;  Loss pred: 0.0281; Loss self: 0.0000; time: 0.52s
Val loss: 0.2510 score: 0.9184 time: 0.80s
Test loss: 0.5526 score: 0.8367 time: 0.44s
     INFO: Early stopping counter 17 of 20
Epoch 123/1000, LR 0.000262
Train loss: 0.0263;  Loss pred: 0.0263; Loss self: 0.0000; time: 0.57s
Val loss: 0.2532 score: 0.9184 time: 0.59s
Test loss: 0.5589 score: 0.8367 time: 0.37s
     INFO: Early stopping counter 18 of 20
Epoch 124/1000, LR 0.000261
Train loss: 0.0280;  Loss pred: 0.0280; Loss self: 0.0000; time: 0.54s
Val loss: 0.2543 score: 0.9184 time: 0.51s
Test loss: 0.5653 score: 0.8163 time: 0.39s
     INFO: Early stopping counter 19 of 20
Epoch 125/1000, LR 0.000261
Train loss: 0.0254;  Loss pred: 0.0254; Loss self: 0.0000; time: 0.75s
Val loss: 0.2546 score: 0.9184 time: 0.49s
Test loss: 0.5717 score: 0.8163 time: 0.38s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 104,   Train_Loss: 0.0537,   Val_Loss: 0.1521,   Val_Precision: 0.9565,   Val_Recall: 0.9167,   Val_accuracy: 0.9362,   Val_Score: 0.9388,   Val_Loss: 0.1521,   Test_Precision: 0.7586,   Test_Recall: 0.8800,   Test_accuracy: 0.8148,   Test_Score: 0.7959,   Test_loss: 0.4833


[0.4133789490442723, 0.4975756169296801, 0.3741663589607924, 0.5817657010629773, 0.38849023496732116, 0.3834421830251813, 0.4418248950969428, 0.3851555909495801, 0.5129890020471066, 0.3796937130391598, 0.47952195489779115, 0.49915222101844847, 0.47557287104427814, 0.3833508670795709, 0.3727581568527967, 0.469041840871796, 0.46371735888533294, 0.49431128706783056, 0.38479934516362846, 0.47815606999211013, 0.3712326339446008, 0.3755764029920101, 0.4857493310701102, 0.3664041270967573, 0.39092826400883496, 0.37638417398557067, 0.37531094485893846, 0.37740006600506604, 0.40333824302069843, 0.5800906680524349, 0.39554483420215547, 0.47738473513163626, 0.3731409441679716, 0.38982169004157186, 0.3762851618230343, 0.37892656889744103, 0.47210931591689587, 0.37212703190743923, 0.38864765292964876, 0.3922332909423858, 0.41430992004461586, 0.39003651193343103, 0.3858835680875927, 0.49365174886770546, 0.3710682559758425, 0.5062659229151905, 0.3844345551915467, 0.40714388084597886, 0.3938981650862843, 0.4920285160187632, 0.5939072291366756, 0.3801809698343277, 0.3680411099921912, 2.84037639410235, 0.40507440897636116, 0.3863134679850191, 0.37455297401174903, 0.4783091351855546, 0.37872608401812613, 0.5049487731885165, 0.3890400610398501, 0.6124180601909757, 0.39941573003306985, 0.3932234081439674, 0.4861568221822381, 0.3859606261830777, 0.3798823291435838, 0.37749784998595715, 0.469161749817431, 0.3865227389615029, 0.3925897139124572, 0.5056938258931041, 0.4220457370392978, 0.5288946069777012, 0.37552555999718606, 0.38643764797598124, 0.37195522291585803, 0.3850719789043069, 0.4735727768857032, 0.3890866469591856, 0.38768420997075737, 0.4109561590012163, 0.3915888399351388, 0.3743322528898716, 0.38477279292419553, 0.48501077201217413, 0.41703406209126115, 0.5135330748744309, 0.42449320387095213, 0.37497379491105676, 0.37882392294704914, 0.3709376610349864, 0.4815021448303014, 0.46441116696223617, 0.47283138195052743, 0.4756862330250442, 0.4024641190189868, 0.3745365799404681, 0.37007173988968134, 0.4801601110957563, 0.47995613305829465, 0.5975509369745851, 0.6207707519643009, 0.5163448671810329, 0.5078979339450598, 0.4438778848852962, 0.6294343380723149, 0.4761690041050315, 0.37778652203269303, 0.38377683493308723, 0.38440864882431924, 0.388868513982743, 0.3802442350424826, 0.6145599810406566, 0.37884271214716136, 0.47956915595568717, 0.38691004598513246, 0.3805453369859606, 0.41967999702319503, 0.3880548079032451, 0.5229692098218948, 0.4410867390688509, 0.37616636184975505, 0.3971816950943321, 0.38270405400544405]
[0.008436305082536169, 0.01015460442713633, 0.007636048142056988, 0.011872769409448517, 0.007928372142190228, 0.007825350673983291, 0.009016834593815158, 0.007860318182644491, 0.010469163307083808, 0.007748851286513465, 0.00978616234485288, 0.010186780020784663, 0.009705568796822004, 0.007823487083256548, 0.007607309323526463, 0.009572282466771347, 0.009463619569088427, 0.01008798545036389, 0.007853047860482213, 0.009758287142696125, 0.0075761762029510375, 0.007664824550857349, 0.009913251654492045, 0.007477635246872598, 0.007978127836914999, 0.0076813096731749115, 0.0076594070379375195, 0.0077020421633686945, 0.008231392714708132, 0.01183858506229459, 0.00807234355514603, 0.009742545614931352, 0.007615121309550441, 0.007955544694725956, 0.0076792890167966185, 0.007733195283621245, 0.009634883998303997, 0.0075944292226008, 0.007931584753666301, 0.008004761039640526, 0.008455304490706446, 0.00795992881496798, 0.007875174858930464, 0.01007452548709603, 0.0075728215505273975, 0.010331957610514092, 0.007845603167174421, 0.00830905879277508, 0.008038738062985393, 0.010041398286097208, 0.012120555696666849, 0.007758795302741381, 0.007511043061065126, 0.057966865185762245, 0.008266824672986962, 0.007883948326224879, 0.007643938245137735, 0.009761410922154176, 0.007729103755471962, 0.010305077003847276, 0.00793959308244592, 0.012498327758999504, 0.008151341429246324, 0.008024967513142191, 0.009921567799637512, 0.007876747473124035, 0.0077527005947670155, 0.007704037754815452, 0.009574729588110837, 0.007888219162479651, 0.00801203497780525, 0.010320282161083758, 0.008613178306924445, 0.010793767489340841, 0.007663786938718083, 0.007886482611754718, 0.007590922916650164, 0.00785861181437361, 0.00966475054868782, 0.007940543815493584, 0.007911922652464437, 0.008386860387779924, 0.007991608978268139, 0.007639433732446359, 0.007852505978044808, 0.009898179020656615, 0.008510899226352269, 0.01048026683417206, 0.008663126609611268, 0.00765252642675626, 0.007731100468307125, 0.007570156347652783, 0.009826574384291865, 0.009477778917596656, 0.009649620039806682, 0.009707882306633555, 0.008213553449367078, 0.007643603672254451, 0.007552484487544517, 0.00979918594072972, 0.009795023123638667, 0.012194917081113981, 0.012668790856414303, 0.010537650350633324, 0.010365263958062445, 0.009058732344597881, 0.012845598736169691, 0.009717734777653704, 0.007709929021075368, 0.007832180304756882, 0.007845074465802434, 0.007936092122096797, 0.007760086429438421, 0.012542040429401154, 0.00773148392137064, 0.009787125631748718, 0.007896123387451683, 0.00776623136706042, 0.008564897898432552, 0.007919485875576431, 0.010672841016773363, 0.009001770185078589, 0.007676864527546021, 0.008105748879476165, 0.007810286816437633]
[118.53530547041032, 98.47749434017167, 130.95779143825845, 84.22634732585524, 126.1292964136454, 127.78980031203841, 110.90366465033323, 127.22131302623227, 95.51861697709543, 129.05138620229505, 102.18510226595198, 98.1664469007521, 103.03363161234203, 127.82024043219192, 131.45252249798588, 104.46829201617697, 105.66781480379434, 99.12781941650485, 127.33909403916397, 102.47700086879276, 131.99270624282534, 130.46613048541926, 100.87507458230061, 133.73211810755734, 125.34268946819512, 130.18613264509494, 130.5584094234629, 129.83569536350385, 121.4861244820899, 84.4695539828454, 123.87976219898508, 102.64257818484396, 131.3176716890719, 125.69849562443403, 130.22038860794768, 129.31265322084653, 103.78952151121148, 131.6754650927589, 126.07820896546045, 124.9256530017425, 118.26895188684675, 125.62926418633087, 126.98130745198604, 99.26025809164426, 132.0511771375831, 96.78707924453464, 127.45992611300375, 120.3505745884869, 124.39763457457701, 99.58772389145717, 82.50446803152762, 128.88598822122222, 133.13730088750043, 17.251234766540712, 120.96542984244527, 126.83999927721955, 130.8226162915555, 102.44420688513716, 129.38110699989394, 97.03954658724648, 125.95103925551985, 80.01070377434641, 122.67919442217014, 124.61109635177178, 100.79052224352439, 126.95595528637472, 128.9873106508187, 129.80206377817197, 104.44159187970418, 126.77132561890576, 124.81223593883158, 96.89657553849165, 116.10116084512774, 92.64605717952783, 130.48379449954663, 126.79923981693823, 131.73628700754807, 127.24893704139622, 103.46878535170984, 125.93595895142606, 126.39152882624758, 119.23412978914614, 125.13124737700942, 130.89975448740128, 127.34788140193047, 101.02868395419898, 117.49639766661828, 95.41741787903628, 115.43176558109568, 130.67579832244738, 129.34769171599828, 132.09766801052422, 101.76486340942334, 105.50995214114751, 103.6310233848372, 103.00907740884782, 121.74998387294336, 130.82834260885403, 132.40675987473924, 102.04929328298189, 102.09266352691559, 82.00137756973187, 78.93413123113423, 94.89781561597353, 96.47607663885559, 110.39072156672601, 77.8476753430164, 102.9046401121728, 129.7028801778154, 127.6783681030235, 127.46851599167262, 126.00660181547764, 128.86454411209021, 79.73184312624218, 129.34127654794628, 102.1750448115301, 126.64442422330613, 128.76258158382782, 116.75562416021427, 126.27082309521938, 93.69576464489698, 111.08926127192277, 130.26151450397666, 123.36923026717614, 128.0362710746277]
Elapsed: 0.44982372726872566~0.2244642974872142
Time per graph: 0.009180076066708688~0.004580904030351309
Speed: 115.35246861112081~17.707779695027675
Total Time: 0.3833
best val loss: 0.15206660330295563 test_score: 0.7959

Testing...
Test loss: 0.6051 score: 0.8571 time: 0.37s
test Score 0.8571
Epoch Time List: [2.2154872226528823, 1.7263218711595982, 1.5352067446801811, 1.6972300128545612, 1.650124792009592, 1.5629564530681819, 1.4781025499105453, 1.575922132236883, 1.5620515900664032, 1.5089438180439174, 1.9610912024509162, 1.7655683076009154, 1.7420374772045761, 1.490110909100622, 1.5796052489895374, 1.5278634470887482, 1.4932774791959673, 1.7752031318377703, 1.4434500893112272, 1.8080141579266638, 1.6490385076031089, 1.5328082228079438, 1.5075600810814649, 1.4263197958935052, 1.5232639703899622, 1.442019364098087, 1.4896360798738897, 1.3994019059464335, 1.537565134698525, 1.6651484926696867, 1.6637716579716653, 1.4919446650892496, 1.3804406342096627, 1.4899585728999227, 1.377669797744602, 1.587492355844006, 1.47796137095429, 1.4204738091211766, 1.5432791288476437, 1.4656159807927907, 1.5455259347800165, 1.564740390283987, 1.6052951400633901, 1.5167867271229625, 1.4395258051808923, 1.53297558426857, 1.4568788302130997, 1.5760398739948869, 1.417119415011257, 1.905023390892893, 1.9025871427729726, 1.4328237602021545, 1.614084956003353, 3.95759214903228, 3.1910674781538546, 1.4193657918367535, 1.5519783038180321, 1.519738381030038, 1.441027247812599, 1.5425868758466095, 1.4162086881697178, 1.783253117930144, 1.4540312988683581, 1.5368310620542616, 1.5250587649643421, 1.4170499350875616, 1.5372629940975457, 1.400027218973264, 1.6801361951511353, 1.5951967192813754, 1.5683879090938717, 1.6744421678595245, 1.4584217409137636, 1.5760256429202855, 1.4296018749009818, 1.5033389220479876, 1.4218515309039503, 1.4951815390959382, 1.5154012138955295, 1.436042367015034, 1.5146870280150324, 1.4676252731587738, 1.5117364907637239, 1.443506944924593, 1.5335457960609347, 1.5449995549861342, 1.4989676100667566, 1.567272363929078, 1.637793661793694, 1.5286495888140053, 1.4425930001307279, 1.4999541947618127, 1.5548318659421057, 1.4861307009123266, 1.6493138689547777, 1.8008846710436046, 1.6529598331544548, 1.467259337194264, 1.519135672133416, 1.5197944326791912, 1.7482119700871408, 1.9150033767800778, 1.9723143163137138, 1.6752435269299895, 1.8803565630223602, 1.740244162036106, 1.900683413259685, 1.7048914190381765, 1.77284677606076, 1.4128293341491371, 1.5577158341184258, 1.420098040951416, 1.549012305913493, 1.7789455149322748, 1.5002387322019786, 1.6783934312406927, 1.409467661054805, 1.609801545739174, 1.4436223888769746, 1.5147254893090576, 1.5914367749355733, 1.7583933756686747, 1.5315466648899019, 1.44227564195171, 1.6160639110021293]
Total Epoch List: [125]
Total Time List: [0.38325966498814523]
========================k_idx:1========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: FAGNN
FAGNN(
  (conv1): NeighborFilterConv()
  (conv2): NeighborFilterConv()
  (ffn): Sequential(
    (0): Linear(in_features=14951, out_features=64, bias=True)
    (1): ReLU()
    (2): Linear(in_features=64, out_features=64, bias=True)
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=64, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=64, out_features=2, bias=True)
  )
)
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7ed848944820>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.6988;  Loss pred: 0.6988; Loss self: 0.0000; time: 0.53s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6967 score: 0.5102 time: 0.65s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7010 score: 0.4898 time: 0.63s
Epoch 2/1000, LR 0.000000
Train loss: 0.6988;  Loss pred: 0.6988; Loss self: 0.0000; time: 0.57s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6967 score: 0.5102 time: 0.40s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7009 score: 0.4898 time: 0.56s
Epoch 3/1000, LR 0.000030
Train loss: 0.6988;  Loss pred: 0.6988; Loss self: 0.0000; time: 0.53s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6966 score: 0.5102 time: 0.52s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7008 score: 0.4898 time: 0.53s
Epoch 4/1000, LR 0.000060
Train loss: 0.6987;  Loss pred: 0.6987; Loss self: 0.0000; time: 0.54s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6965 score: 0.5102 time: 0.42s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7006 score: 0.4898 time: 0.52s
Epoch 5/1000, LR 0.000090
Train loss: 0.6985;  Loss pred: 0.6985; Loss self: 0.0000; time: 0.63s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6964 score: 0.5102 time: 0.39s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7004 score: 0.4898 time: 0.54s
Epoch 6/1000, LR 0.000120
Train loss: 0.6983;  Loss pred: 0.6983; Loss self: 0.0000; time: 0.56s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6962 score: 0.5102 time: 0.38s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7001 score: 0.4898 time: 0.61s
Epoch 7/1000, LR 0.000150
Train loss: 0.6981;  Loss pred: 0.6981; Loss self: 0.0000; time: 0.53s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6960 score: 0.5102 time: 0.38s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6998 score: 0.4898 time: 0.59s
Epoch 8/1000, LR 0.000180
Train loss: 0.6978;  Loss pred: 0.6978; Loss self: 0.0000; time: 0.59s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6958 score: 0.5102 time: 0.40s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6994 score: 0.4898 time: 0.62s
Epoch 9/1000, LR 0.000210
Train loss: 0.6974;  Loss pred: 0.6974; Loss self: 0.0000; time: 0.53s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6955 score: 0.5102 time: 0.38s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6989 score: 0.4898 time: 0.52s
Epoch 10/1000, LR 0.000240
Train loss: 0.6971;  Loss pred: 0.6971; Loss self: 0.0000; time: 0.52s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6952 score: 0.5102 time: 0.49s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6984 score: 0.4898 time: 0.51s
Epoch 11/1000, LR 0.000270
Train loss: 0.6966;  Loss pred: 0.6966; Loss self: 0.0000; time: 0.51s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6949 score: 0.5102 time: 0.38s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6979 score: 0.4898 time: 0.62s
Epoch 12/1000, LR 0.000270
Train loss: 0.6961;  Loss pred: 0.6961; Loss self: 0.0000; time: 0.54s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6946 score: 0.5102 time: 0.38s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6974 score: 0.4898 time: 0.52s
Epoch 13/1000, LR 0.000270
Train loss: 0.6957;  Loss pred: 0.6957; Loss self: 0.0000; time: 0.53s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6943 score: 0.5102 time: 0.38s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6969 score: 0.4898 time: 0.63s
Epoch 14/1000, LR 0.000270
Train loss: 0.6952;  Loss pred: 0.6952; Loss self: 0.0000; time: 0.51s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6941 score: 0.5102 time: 0.41s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6963 score: 0.4898 time: 0.51s
Epoch 15/1000, LR 0.000270
Train loss: 0.6947;  Loss pred: 0.6947; Loss self: 0.0000; time: 0.52s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6938 score: 0.5102 time: 0.47s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6957 score: 0.4898 time: 0.52s
Epoch 16/1000, LR 0.000270
Train loss: 0.6943;  Loss pred: 0.6943; Loss self: 0.0000; time: 0.53s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6934 score: 0.5102 time: 0.39s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6951 score: 0.4898 time: 0.51s
Epoch 17/1000, LR 0.000270
Train loss: 0.6938;  Loss pred: 0.6938; Loss self: 0.0000; time: 0.66s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6931 score: 0.5102 time: 0.38s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6945 score: 0.4898 time: 0.53s
Epoch 18/1000, LR 0.000270
Train loss: 0.6931;  Loss pred: 0.6931; Loss self: 0.0000; time: 0.55s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6928 score: 0.5102 time: 0.38s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6938 score: 0.4898 time: 0.56s
Epoch 19/1000, LR 0.000270
Train loss: 0.6924;  Loss pred: 0.6924; Loss self: 0.0000; time: 0.61s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6924 score: 0.5102 time: 0.39s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6931 score: 0.4898 time: 0.53s
Epoch 20/1000, LR 0.000270
Train loss: 0.6919;  Loss pred: 0.6919; Loss self: 0.0000; time: 0.52s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6920 score: 0.5102 time: 0.47s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6924 score: 0.4898 time: 0.61s
Epoch 21/1000, LR 0.000270
Train loss: 0.6911;  Loss pred: 0.6911; Loss self: 0.0000; time: 0.54s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6916 score: 0.5102 time: 0.41s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6916 score: 0.4898 time: 0.53s
Epoch 22/1000, LR 0.000270
Train loss: 0.6904;  Loss pred: 0.6904; Loss self: 0.0000; time: 0.58s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6912 score: 0.5102 time: 0.50s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6907 score: 0.4898 time: 0.54s
Epoch 23/1000, LR 0.000270
Train loss: 0.6896;  Loss pred: 0.6896; Loss self: 0.0000; time: 0.55s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6907 score: 0.5102 time: 0.40s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6897 score: 0.4898 time: 0.57s
Epoch 24/1000, LR 0.000270
Train loss: 0.6888;  Loss pred: 0.6888; Loss self: 0.0000; time: 0.64s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6902 score: 0.5102 time: 0.39s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6887 score: 0.4898 time: 0.55s
Epoch 25/1000, LR 0.000270
Train loss: 0.6878;  Loss pred: 0.6878; Loss self: 0.0000; time: 0.55s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6896 score: 0.5102 time: 0.40s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6875 score: 0.4898 time: 0.65s
Epoch 26/1000, LR 0.000270
Train loss: 0.6867;  Loss pred: 0.6867; Loss self: 0.0000; time: 0.55s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6889 score: 0.5102 time: 0.41s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6863 score: 0.4898 time: 0.53s
Epoch 27/1000, LR 0.000270
Train loss: 0.6853;  Loss pred: 0.6853; Loss self: 0.0000; time: 0.58s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6882 score: 0.5102 time: 0.58s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6849 score: 0.4898 time: 0.61s
Epoch 28/1000, LR 0.000270
Train loss: 0.6840;  Loss pred: 0.6840; Loss self: 0.0000; time: 0.54s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6874 score: 0.5102 time: 0.46s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6834 score: 0.4898 time: 0.57s
Epoch 29/1000, LR 0.000270
Train loss: 0.6823;  Loss pred: 0.6823; Loss self: 0.0000; time: 0.63s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6865 score: 0.5102 time: 0.38s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6817 score: 0.4898 time: 0.53s
Epoch 30/1000, LR 0.000270
Train loss: 0.6803;  Loss pred: 0.6803; Loss self: 0.0000; time: 0.81s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6855 score: 0.5102 time: 0.47s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6799 score: 0.4898 time: 0.65s
Epoch 31/1000, LR 0.000270
Train loss: 0.6791;  Loss pred: 0.6791; Loss self: 0.0000; time: 0.77s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6844 score: 0.5102 time: 0.44s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6778 score: 0.4898 time: 0.52s
Epoch 32/1000, LR 0.000270
Train loss: 0.6773;  Loss pred: 0.6773; Loss self: 0.0000; time: 0.54s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6832 score: 0.5102 time: 0.38s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6756 score: 0.4898 time: 0.63s
Epoch 33/1000, LR 0.000270
Train loss: 0.6739;  Loss pred: 0.6739; Loss self: 0.0000; time: 0.58s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6818 score: 0.5102 time: 0.46s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6731 score: 0.4898 time: 0.54s
Epoch 34/1000, LR 0.000270
Train loss: 0.6719;  Loss pred: 0.6719; Loss self: 0.0000; time: 0.53s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6802 score: 0.5102 time: 0.48s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6704 score: 0.4898 time: 0.55s
Epoch 35/1000, LR 0.000270
Train loss: 0.6684;  Loss pred: 0.6684; Loss self: 0.0000; time: 0.56s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6785 score: 0.5102 time: 0.38s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6675 score: 0.4898 time: 0.52s
Epoch 36/1000, LR 0.000270
Train loss: 0.6660;  Loss pred: 0.6660; Loss self: 0.0000; time: 0.63s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6766 score: 0.5102 time: 0.40s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6644 score: 0.4898 time: 0.62s
Epoch 37/1000, LR 0.000270
Train loss: 0.6637;  Loss pred: 0.6637; Loss self: 0.0000; time: 0.56s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6745 score: 0.5102 time: 0.40s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6611 score: 0.4898 time: 0.65s
Epoch 38/1000, LR 0.000270
Train loss: 0.6596;  Loss pred: 0.6596; Loss self: 0.0000; time: 0.58s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6723 score: 0.5102 time: 0.42s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6577 score: 0.4898 time: 0.51s
Epoch 39/1000, LR 0.000269
Train loss: 0.6560;  Loss pred: 0.6560; Loss self: 0.0000; time: 0.54s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6698 score: 0.5102 time: 0.39s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6541 score: 0.4898 time: 0.67s
Epoch 40/1000, LR 0.000269
Train loss: 0.6525;  Loss pred: 0.6525; Loss self: 0.0000; time: 0.53s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6672 score: 0.5102 time: 0.42s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6502 score: 0.4898 time: 0.52s
Epoch 41/1000, LR 0.000269
Train loss: 0.6481;  Loss pred: 0.6481; Loss self: 0.0000; time: 0.54s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6644 score: 0.5102 time: 0.43s
Test loss: 0.6462 score: 0.5306 time: 0.62s
Epoch 42/1000, LR 0.000269
Train loss: 0.6442;  Loss pred: 0.6442; Loss self: 0.0000; time: 0.54s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6613 score: 0.5102 time: 0.38s
Test loss: 0.6418 score: 0.5306 time: 0.53s
Epoch 43/1000, LR 0.000269
Train loss: 0.6388;  Loss pred: 0.6388; Loss self: 0.0000; time: 0.59s
Val loss: 0.6581 score: 0.5306 time: 0.57s
Test loss: 0.6371 score: 0.5510 time: 0.52s
Epoch 44/1000, LR 0.000269
Train loss: 0.6329;  Loss pred: 0.6329; Loss self: 0.0000; time: 0.53s
Val loss: 0.6546 score: 0.6122 time: 0.39s
Test loss: 0.6321 score: 0.5918 time: 0.52s
Epoch 45/1000, LR 0.000269
Train loss: 0.6286;  Loss pred: 0.6286; Loss self: 0.0000; time: 0.76s
Val loss: 0.6508 score: 0.6122 time: 0.38s
Test loss: 0.6267 score: 0.6122 time: 0.53s
Epoch 46/1000, LR 0.000269
Train loss: 0.6213;  Loss pred: 0.6213; Loss self: 0.0000; time: 0.54s
Val loss: 0.6467 score: 0.6122 time: 0.38s
Test loss: 0.6209 score: 0.6122 time: 0.63s
Epoch 47/1000, LR 0.000269
Train loss: 0.6150;  Loss pred: 0.6150; Loss self: 0.0000; time: 0.53s
Val loss: 0.6422 score: 0.6327 time: 0.39s
Test loss: 0.6147 score: 0.6531 time: 0.53s
Epoch 48/1000, LR 0.000269
Train loss: 0.6091;  Loss pred: 0.6091; Loss self: 0.0000; time: 0.55s
Val loss: 0.6374 score: 0.6531 time: 0.50s
Test loss: 0.6080 score: 0.7143 time: 0.52s
Epoch 49/1000, LR 0.000269
Train loss: 0.6031;  Loss pred: 0.6031; Loss self: 0.0000; time: 0.54s
Val loss: 0.6322 score: 0.6735 time: 0.38s
Test loss: 0.6008 score: 0.7347 time: 0.52s
Epoch 50/1000, LR 0.000269
Train loss: 0.5938;  Loss pred: 0.5938; Loss self: 0.0000; time: 0.63s
Val loss: 0.6266 score: 0.7143 time: 0.39s
Test loss: 0.5931 score: 0.7347 time: 0.53s
Epoch 51/1000, LR 0.000269
Train loss: 0.5859;  Loss pred: 0.5859; Loss self: 0.0000; time: 0.55s
Val loss: 0.6206 score: 0.7143 time: 0.38s
Test loss: 0.5849 score: 0.7959 time: 0.53s
Epoch 52/1000, LR 0.000269
Train loss: 0.5763;  Loss pred: 0.5763; Loss self: 0.0000; time: 0.63s
Val loss: 0.6142 score: 0.7551 time: 0.39s
Test loss: 0.5761 score: 0.7959 time: 0.53s
Epoch 53/1000, LR 0.000269
Train loss: 0.5702;  Loss pred: 0.5702; Loss self: 0.0000; time: 0.54s
Val loss: 0.6074 score: 0.7755 time: 0.40s
Test loss: 0.5669 score: 0.8367 time: 0.62s
Epoch 54/1000, LR 0.000269
Train loss: 0.5572;  Loss pred: 0.5572; Loss self: 0.0000; time: 0.54s
Val loss: 0.6003 score: 0.7755 time: 0.39s
Test loss: 0.5571 score: 0.8776 time: 0.52s
Epoch 55/1000, LR 0.000269
Train loss: 0.5489;  Loss pred: 0.5489; Loss self: 0.0000; time: 0.57s
Val loss: 0.5928 score: 0.7755 time: 0.48s
Test loss: 0.5468 score: 0.8980 time: 0.53s
Epoch 56/1000, LR 0.000269
Train loss: 0.5363;  Loss pred: 0.5363; Loss self: 0.0000; time: 0.55s
Val loss: 0.5848 score: 0.7755 time: 0.39s
Test loss: 0.5360 score: 0.9184 time: 0.57s
Epoch 57/1000, LR 0.000269
Train loss: 0.5255;  Loss pred: 0.5255; Loss self: 0.0000; time: 0.65s
Val loss: 0.5764 score: 0.7755 time: 0.38s
Test loss: 0.5246 score: 0.9184 time: 0.55s
Epoch 58/1000, LR 0.000269
Train loss: 0.5159;  Loss pred: 0.5159; Loss self: 0.0000; time: 0.55s
Val loss: 0.5676 score: 0.7755 time: 0.41s
Test loss: 0.5125 score: 0.9184 time: 0.62s
Epoch 59/1000, LR 0.000268
Train loss: 0.4972;  Loss pred: 0.4972; Loss self: 0.0000; time: 0.53s
Val loss: 0.5583 score: 0.7959 time: 0.39s
Test loss: 0.4998 score: 0.9388 time: 0.52s
Epoch 60/1000, LR 0.000268
Train loss: 0.4879;  Loss pred: 0.4879; Loss self: 0.0000; time: 0.58s
Val loss: 0.5485 score: 0.8163 time: 0.39s
Test loss: 0.4865 score: 0.9388 time: 0.62s
Epoch 61/1000, LR 0.000268
Train loss: 0.4714;  Loss pred: 0.4714; Loss self: 0.0000; time: 0.55s
Val loss: 0.5383 score: 0.8163 time: 0.43s
Test loss: 0.4728 score: 0.9388 time: 0.63s
Epoch 62/1000, LR 0.000268
Train loss: 0.4541;  Loss pred: 0.4541; Loss self: 0.0000; time: 0.54s
Val loss: 0.5278 score: 0.8163 time: 0.38s
Test loss: 0.4584 score: 0.9388 time: 0.63s
Epoch 63/1000, LR 0.000268
Train loss: 0.4435;  Loss pred: 0.4435; Loss self: 0.0000; time: 0.54s
Val loss: 0.5170 score: 0.8163 time: 0.47s
Test loss: 0.4437 score: 0.9388 time: 0.51s
Epoch 64/1000, LR 0.000268
Train loss: 0.4303;  Loss pred: 0.4303; Loss self: 0.0000; time: 0.54s
Val loss: 0.5060 score: 0.8367 time: 0.49s
Test loss: 0.4287 score: 0.9388 time: 0.51s
Epoch 65/1000, LR 0.000268
Train loss: 0.4030;  Loss pred: 0.4030; Loss self: 0.0000; time: 0.57s
Val loss: 0.4950 score: 0.8367 time: 0.38s
Test loss: 0.4135 score: 0.9388 time: 0.52s
Epoch 66/1000, LR 0.000268
Train loss: 0.3885;  Loss pred: 0.3885; Loss self: 0.0000; time: 0.63s
Val loss: 0.4837 score: 0.8367 time: 0.38s
Test loss: 0.3981 score: 0.9388 time: 0.54s
Epoch 67/1000, LR 0.000268
Train loss: 0.3727;  Loss pred: 0.3727; Loss self: 0.0000; time: 0.52s
Val loss: 0.4725 score: 0.8367 time: 0.38s
Test loss: 0.3827 score: 0.9388 time: 0.70s
Epoch 68/1000, LR 0.000268
Train loss: 0.3587;  Loss pred: 0.3587; Loss self: 0.0000; time: 0.53s
Val loss: 0.4613 score: 0.8367 time: 0.42s
Test loss: 0.3672 score: 0.9388 time: 0.51s
Epoch 69/1000, LR 0.000268
Train loss: 0.3523;  Loss pred: 0.3523; Loss self: 0.0000; time: 0.52s
Val loss: 0.4505 score: 0.8367 time: 0.48s
Test loss: 0.3520 score: 0.9592 time: 3.79s
Epoch 70/1000, LR 0.000268
Train loss: 0.3218;  Loss pred: 0.3218; Loss self: 0.0000; time: 1.80s
Val loss: 0.4398 score: 0.8367 time: 0.38s
Test loss: 0.3370 score: 0.9592 time: 0.52s
Epoch 71/1000, LR 0.000268
Train loss: 0.3094;  Loss pred: 0.3094; Loss self: 0.0000; time: 0.67s
Val loss: 0.4293 score: 0.8367 time: 0.39s
Test loss: 0.3223 score: 0.9592 time: 0.51s
Epoch 72/1000, LR 0.000267
Train loss: 0.2952;  Loss pred: 0.2952; Loss self: 0.0000; time: 0.53s
Val loss: 0.4194 score: 0.8367 time: 0.38s
Test loss: 0.3080 score: 0.9592 time: 0.54s
Epoch 73/1000, LR 0.000267
Train loss: 0.2753;  Loss pred: 0.2753; Loss self: 0.0000; time: 0.63s
Val loss: 0.4098 score: 0.8367 time: 0.37s
Test loss: 0.2940 score: 0.9592 time: 0.51s
Epoch 74/1000, LR 0.000267
Train loss: 0.2701;  Loss pred: 0.2701; Loss self: 0.0000; time: 0.53s
Val loss: 0.4010 score: 0.8367 time: 0.43s
Test loss: 0.2807 score: 0.9796 time: 0.62s
Epoch 75/1000, LR 0.000267
Train loss: 0.2568;  Loss pred: 0.2568; Loss self: 0.0000; time: 0.56s
Val loss: 0.3932 score: 0.8571 time: 0.38s
Test loss: 0.2681 score: 0.9592 time: 0.58s
Epoch 76/1000, LR 0.000267
Train loss: 0.2481;  Loss pred: 0.2481; Loss self: 0.0000; time: 0.54s
Val loss: 0.3858 score: 0.8571 time: 0.48s
Test loss: 0.2560 score: 0.9592 time: 0.51s
Epoch 77/1000, LR 0.000267
Train loss: 0.2199;  Loss pred: 0.2199; Loss self: 0.0000; time: 0.53s
Val loss: 0.3788 score: 0.8571 time: 0.42s
Test loss: 0.2443 score: 0.9592 time: 0.52s
Epoch 78/1000, LR 0.000267
Train loss: 0.1963;  Loss pred: 0.1963; Loss self: 0.0000; time: 0.63s
Val loss: 0.3718 score: 0.8571 time: 0.39s
Test loss: 0.2326 score: 0.9592 time: 0.53s
Epoch 79/1000, LR 0.000267
Train loss: 0.1977;  Loss pred: 0.1977; Loss self: 0.0000; time: 0.57s
Val loss: 0.3659 score: 0.8776 time: 0.38s
Test loss: 0.2217 score: 0.9592 time: 0.61s
Epoch 80/1000, LR 0.000267
Train loss: 0.1830;  Loss pred: 0.1830; Loss self: 0.0000; time: 0.52s
Val loss: 0.3608 score: 0.8776 time: 0.38s
Test loss: 0.2116 score: 0.9592 time: 0.53s
Epoch 81/1000, LR 0.000267
Train loss: 0.1743;  Loss pred: 0.1743; Loss self: 0.0000; time: 0.52s
Val loss: 0.3565 score: 0.8776 time: 0.38s
Test loss: 0.2020 score: 0.9592 time: 0.61s
Epoch 82/1000, LR 0.000267
Train loss: 0.1472;  Loss pred: 0.1472; Loss self: 0.0000; time: 0.57s
Val loss: 0.3524 score: 0.8776 time: 0.38s
Test loss: 0.1925 score: 0.9592 time: 0.52s
Epoch 83/1000, LR 0.000266
Train loss: 0.1522;  Loss pred: 0.1522; Loss self: 0.0000; time: 0.53s
Val loss: 0.3495 score: 0.8776 time: 0.38s
Test loss: 0.1840 score: 0.9592 time: 0.61s
Epoch 84/1000, LR 0.000266
Train loss: 0.1303;  Loss pred: 0.1303; Loss self: 0.0000; time: 0.54s
Val loss: 0.3475 score: 0.8776 time: 0.39s
Test loss: 0.1762 score: 0.9592 time: 0.51s
Epoch 85/1000, LR 0.000266
Train loss: 0.1615;  Loss pred: 0.1615; Loss self: 0.0000; time: 0.53s
Val loss: 0.3466 score: 0.8776 time: 0.47s
Test loss: 0.1695 score: 0.9592 time: 0.51s
Epoch 86/1000, LR 0.000266
Train loss: 0.1316;  Loss pred: 0.1316; Loss self: 0.0000; time: 0.53s
Val loss: 0.3460 score: 0.8776 time: 0.39s
Test loss: 0.1630 score: 0.9592 time: 0.52s
Epoch 87/1000, LR 0.000266
Train loss: 0.1332;  Loss pred: 0.1332; Loss self: 0.0000; time: 0.61s
Val loss: 0.3459 score: 0.8776 time: 0.38s
Test loss: 0.1568 score: 0.9592 time: 0.51s
Epoch 88/1000, LR 0.000266
Train loss: 0.0940;  Loss pred: 0.0940; Loss self: 0.0000; time: 0.53s
Val loss: 0.3468 score: 0.8776 time: 0.38s
Test loss: 0.1516 score: 0.9592 time: 0.63s
     INFO: Early stopping counter 1 of 20
Epoch 89/1000, LR 0.000266
Train loss: 0.1061;  Loss pred: 0.1061; Loss self: 0.0000; time: 0.55s
Val loss: 0.3474 score: 0.8776 time: 0.41s
Test loss: 0.1458 score: 0.9592 time: 0.56s
     INFO: Early stopping counter 2 of 20
Epoch 90/1000, LR 0.000266
Train loss: 0.0960;  Loss pred: 0.0960; Loss self: 0.0000; time: 0.53s
Val loss: 0.3493 score: 0.8776 time: 0.48s
Test loss: 0.1413 score: 0.9592 time: 0.52s
     INFO: Early stopping counter 3 of 20
Epoch 91/1000, LR 0.000266
Train loss: 0.0825;  Loss pred: 0.0825; Loss self: 0.0000; time: 0.53s
Val loss: 0.3509 score: 0.8776 time: 0.46s
Test loss: 0.1361 score: 0.9592 time: 0.52s
     INFO: Early stopping counter 4 of 20
Epoch 92/1000, LR 0.000266
Train loss: 0.0824;  Loss pred: 0.0824; Loss self: 0.0000; time: 0.64s
Val loss: 0.3525 score: 0.8776 time: 0.38s
Test loss: 0.1309 score: 0.9592 time: 0.53s
     INFO: Early stopping counter 5 of 20
Epoch 93/1000, LR 0.000265
Train loss: 0.0674;  Loss pred: 0.0674; Loss self: 0.0000; time: 0.56s
Val loss: 0.3553 score: 0.8776 time: 0.39s
Test loss: 0.1268 score: 0.9592 time: 0.52s
     INFO: Early stopping counter 6 of 20
Epoch 94/1000, LR 0.000265
Train loss: 0.0881;  Loss pred: 0.0881; Loss self: 0.0000; time: 0.63s
Val loss: 0.3580 score: 0.8776 time: 0.39s
Test loss: 0.1226 score: 0.9592 time: 0.56s
     INFO: Early stopping counter 7 of 20
Epoch 95/1000, LR 0.000265
Train loss: 0.0689;  Loss pred: 0.0689; Loss self: 0.0000; time: 0.54s
Val loss: 0.3613 score: 0.8776 time: 0.37s
Test loss: 0.1190 score: 0.9592 time: 0.62s
     INFO: Early stopping counter 8 of 20
Epoch 96/1000, LR 0.000265
Train loss: 0.0705;  Loss pred: 0.0705; Loss self: 0.0000; time: 0.54s
Val loss: 0.3661 score: 0.8776 time: 0.37s
Test loss: 0.1173 score: 0.9592 time: 0.55s
     INFO: Early stopping counter 9 of 20
Epoch 97/1000, LR 0.000265
Train loss: 0.0459;  Loss pred: 0.0459; Loss self: 0.0000; time: 0.52s
Val loss: 0.3705 score: 0.8776 time: 0.48s
Test loss: 0.1150 score: 0.9592 time: 0.52s
     INFO: Early stopping counter 10 of 20
Epoch 98/1000, LR 0.000265
Train loss: 0.0542;  Loss pred: 0.0542; Loss self: 0.0000; time: 0.54s
Val loss: 0.3760 score: 0.8571 time: 0.42s
Test loss: 0.1140 score: 0.9592 time: 0.55s
     INFO: Early stopping counter 11 of 20
Epoch 99/1000, LR 0.000265
Train loss: 0.0392;  Loss pred: 0.0392; Loss self: 0.0000; time: 0.62s
Val loss: 0.3820 score: 0.8571 time: 0.39s
Test loss: 0.1134 score: 0.9592 time: 0.52s
     INFO: Early stopping counter 12 of 20
Epoch 100/1000, LR 0.000265
Train loss: 0.0560;  Loss pred: 0.0560; Loss self: 0.0000; time: 0.57s
Val loss: 0.3881 score: 0.8571 time: 0.38s
Test loss: 0.1129 score: 0.9592 time: 0.61s
     INFO: Early stopping counter 13 of 20
Epoch 101/1000, LR 0.000265
Train loss: 0.0532;  Loss pred: 0.0532; Loss self: 0.0000; time: 0.53s
Val loss: 0.3933 score: 0.8571 time: 0.39s
Test loss: 0.1115 score: 0.9592 time: 0.56s
     INFO: Early stopping counter 14 of 20
Epoch 102/1000, LR 0.000264
Train loss: 0.0466;  Loss pred: 0.0466; Loss self: 0.0000; time: 0.54s
Val loss: 0.3970 score: 0.8571 time: 0.41s
Test loss: 0.1086 score: 0.9592 time: 0.62s
     INFO: Early stopping counter 15 of 20
Epoch 103/1000, LR 0.000264
Train loss: 0.0446;  Loss pred: 0.0446; Loss self: 0.0000; time: 0.55s
Val loss: 0.4005 score: 0.8571 time: 0.43s
Test loss: 0.1055 score: 0.9592 time: 0.59s
     INFO: Early stopping counter 16 of 20
Epoch 104/1000, LR 0.000264
Train loss: 0.0336;  Loss pred: 0.0336; Loss self: 0.0000; time: 0.53s
Val loss: 0.4048 score: 0.8571 time: 0.37s
Test loss: 0.1033 score: 0.9592 time: 0.63s
     INFO: Early stopping counter 17 of 20
Epoch 105/1000, LR 0.000264
Train loss: 0.0264;  Loss pred: 0.0264; Loss self: 0.0000; time: 0.53s
Val loss: 0.4092 score: 0.8571 time: 0.51s
Test loss: 0.1014 score: 0.9592 time: 0.52s
     INFO: Early stopping counter 18 of 20
Epoch 106/1000, LR 0.000264
Train loss: 0.0381;  Loss pred: 0.0381; Loss self: 0.0000; time: 0.53s
Val loss: 0.4131 score: 0.8571 time: 0.48s
Test loss: 0.0992 score: 0.9592 time: 0.55s
     INFO: Early stopping counter 19 of 20
Epoch 107/1000, LR 0.000264
Train loss: 0.0291;  Loss pred: 0.0291; Loss self: 0.0000; time: 0.53s
Val loss: 0.4183 score: 0.8571 time: 0.38s
Test loss: 0.0984 score: 0.9592 time: 0.52s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 086,   Train_Loss: 0.1332,   Val_Loss: 0.3459,   Val_Precision: 0.9524,   Val_Recall: 0.8000,   Val_accuracy: 0.8696,   Val_Score: 0.8776,   Val_Loss: 0.3459,   Test_Precision: 1.0000,   Test_Recall: 0.9167,   Test_accuracy: 0.9565,   Test_Score: 0.9592,   Test_loss: 0.1568


[0.4133789490442723, 0.4975756169296801, 0.3741663589607924, 0.5817657010629773, 0.38849023496732116, 0.3834421830251813, 0.4418248950969428, 0.3851555909495801, 0.5129890020471066, 0.3796937130391598, 0.47952195489779115, 0.49915222101844847, 0.47557287104427814, 0.3833508670795709, 0.3727581568527967, 0.469041840871796, 0.46371735888533294, 0.49431128706783056, 0.38479934516362846, 0.47815606999211013, 0.3712326339446008, 0.3755764029920101, 0.4857493310701102, 0.3664041270967573, 0.39092826400883496, 0.37638417398557067, 0.37531094485893846, 0.37740006600506604, 0.40333824302069843, 0.5800906680524349, 0.39554483420215547, 0.47738473513163626, 0.3731409441679716, 0.38982169004157186, 0.3762851618230343, 0.37892656889744103, 0.47210931591689587, 0.37212703190743923, 0.38864765292964876, 0.3922332909423858, 0.41430992004461586, 0.39003651193343103, 0.3858835680875927, 0.49365174886770546, 0.3710682559758425, 0.5062659229151905, 0.3844345551915467, 0.40714388084597886, 0.3938981650862843, 0.4920285160187632, 0.5939072291366756, 0.3801809698343277, 0.3680411099921912, 2.84037639410235, 0.40507440897636116, 0.3863134679850191, 0.37455297401174903, 0.4783091351855546, 0.37872608401812613, 0.5049487731885165, 0.3890400610398501, 0.6124180601909757, 0.39941573003306985, 0.3932234081439674, 0.4861568221822381, 0.3859606261830777, 0.3798823291435838, 0.37749784998595715, 0.469161749817431, 0.3865227389615029, 0.3925897139124572, 0.5056938258931041, 0.4220457370392978, 0.5288946069777012, 0.37552555999718606, 0.38643764797598124, 0.37195522291585803, 0.3850719789043069, 0.4735727768857032, 0.3890866469591856, 0.38768420997075737, 0.4109561590012163, 0.3915888399351388, 0.3743322528898716, 0.38477279292419553, 0.48501077201217413, 0.41703406209126115, 0.5135330748744309, 0.42449320387095213, 0.37497379491105676, 0.37882392294704914, 0.3709376610349864, 0.4815021448303014, 0.46441116696223617, 0.47283138195052743, 0.4756862330250442, 0.4024641190189868, 0.3745365799404681, 0.37007173988968134, 0.4801601110957563, 0.47995613305829465, 0.5975509369745851, 0.6207707519643009, 0.5163448671810329, 0.5078979339450598, 0.4438778848852962, 0.6294343380723149, 0.4761690041050315, 0.37778652203269303, 0.38377683493308723, 0.38440864882431924, 0.388868513982743, 0.3802442350424826, 0.6145599810406566, 0.37884271214716136, 0.47956915595568717, 0.38691004598513246, 0.3805453369859606, 0.41967999702319503, 0.3880548079032451, 0.5229692098218948, 0.4410867390688509, 0.37616636184975505, 0.3971816950943321, 0.38270405400544405, 0.63146483595483, 0.5606034810189158, 0.5323157049715519, 0.5251547801308334, 0.5460644129198045, 0.6169976249802858, 0.5910883988253772, 0.6225633199792355, 0.523415827890858, 0.5124900070950389, 0.6249912811908871, 0.5281888740137219, 0.6315777359995991, 0.5200115030165762, 0.5280440400820225, 0.5101872778031975, 0.5374010792002082, 0.5622812579385936, 0.5321618770249188, 0.6106831459328532, 0.5348705218639225, 0.5429511829279363, 0.5759899220429361, 0.554515577154234, 0.6508185579441488, 0.5308353151194751, 0.6130730351433158, 0.5747487340122461, 0.5352349530439824, 0.6582286399789155, 0.5281156059354544, 0.6314227830152959, 0.5474257241003215, 0.5542108549270779, 0.5219857080373913, 0.627032320946455, 0.6591682841535658, 0.5133304600603878, 0.6779379388317466, 0.5299840569496155, 0.6224496180657297, 0.5340745178982615, 0.5252237820532173, 0.5282566249370575, 0.5328808538615704, 0.6301360540091991, 0.5382317141629755, 0.5304551050066948, 0.528834804194048, 0.5372320879250765, 0.5318795840721577, 0.5391446030698717, 0.6255291171837598, 0.5265658260323107, 0.5312459669075906, 0.5791136939078569, 0.5501107820309699, 0.6253454620018601, 0.5271021211519837, 0.6287904810160398, 0.6311040471773595, 0.6324548749253154, 0.5110167721286416, 0.5179062441457063, 0.5273351881187409, 0.5491220159456134, 0.7082598521374166, 0.5108753119129688, 3.7962436948437244, 0.5254898299463093, 0.5183976539410651, 0.5477361739613116, 0.5143699930049479, 0.6257282879669219, 0.5865542988758534, 0.5178669518791139, 0.5263369500171393, 0.5387142889667302, 0.6168912691064179, 0.533344112103805, 0.6110126848798245, 0.5223328580614179, 0.6104065941181034, 0.5112935828510672, 0.5179041721858084, 0.5266979511361569, 0.514090487966314, 0.6308970199897885, 0.5707834209315479, 0.5299809980206192, 0.5263579578604549, 0.5378689970821142, 0.5269845770671964, 0.560235737124458, 0.6300415839068592, 0.5595239258836955, 0.528016755124554, 0.558657927904278, 0.5270733879879117, 0.6152631880249828, 0.5669012051075697, 0.6289706209208816, 0.5933161510620266, 0.6299323088023812, 0.5221075660083443, 0.5578629861120135, 0.527579601155594]
[0.008436305082536169, 0.01015460442713633, 0.007636048142056988, 0.011872769409448517, 0.007928372142190228, 0.007825350673983291, 0.009016834593815158, 0.007860318182644491, 0.010469163307083808, 0.007748851286513465, 0.00978616234485288, 0.010186780020784663, 0.009705568796822004, 0.007823487083256548, 0.007607309323526463, 0.009572282466771347, 0.009463619569088427, 0.01008798545036389, 0.007853047860482213, 0.009758287142696125, 0.0075761762029510375, 0.007664824550857349, 0.009913251654492045, 0.007477635246872598, 0.007978127836914999, 0.0076813096731749115, 0.0076594070379375195, 0.0077020421633686945, 0.008231392714708132, 0.01183858506229459, 0.00807234355514603, 0.009742545614931352, 0.007615121309550441, 0.007955544694725956, 0.0076792890167966185, 0.007733195283621245, 0.009634883998303997, 0.0075944292226008, 0.007931584753666301, 0.008004761039640526, 0.008455304490706446, 0.00795992881496798, 0.007875174858930464, 0.01007452548709603, 0.0075728215505273975, 0.010331957610514092, 0.007845603167174421, 0.00830905879277508, 0.008038738062985393, 0.010041398286097208, 0.012120555696666849, 0.007758795302741381, 0.007511043061065126, 0.057966865185762245, 0.008266824672986962, 0.007883948326224879, 0.007643938245137735, 0.009761410922154176, 0.007729103755471962, 0.010305077003847276, 0.00793959308244592, 0.012498327758999504, 0.008151341429246324, 0.008024967513142191, 0.009921567799637512, 0.007876747473124035, 0.0077527005947670155, 0.007704037754815452, 0.009574729588110837, 0.007888219162479651, 0.00801203497780525, 0.010320282161083758, 0.008613178306924445, 0.010793767489340841, 0.007663786938718083, 0.007886482611754718, 0.007590922916650164, 0.00785861181437361, 0.00966475054868782, 0.007940543815493584, 0.007911922652464437, 0.008386860387779924, 0.007991608978268139, 0.007639433732446359, 0.007852505978044808, 0.009898179020656615, 0.008510899226352269, 0.01048026683417206, 0.008663126609611268, 0.00765252642675626, 0.007731100468307125, 0.007570156347652783, 0.009826574384291865, 0.009477778917596656, 0.009649620039806682, 0.009707882306633555, 0.008213553449367078, 0.007643603672254451, 0.007552484487544517, 0.00979918594072972, 0.009795023123638667, 0.012194917081113981, 0.012668790856414303, 0.010537650350633324, 0.010365263958062445, 0.009058732344597881, 0.012845598736169691, 0.009717734777653704, 0.007709929021075368, 0.007832180304756882, 0.007845074465802434, 0.007936092122096797, 0.007760086429438421, 0.012542040429401154, 0.00773148392137064, 0.009787125631748718, 0.007896123387451683, 0.00776623136706042, 0.008564897898432552, 0.007919485875576431, 0.010672841016773363, 0.009001770185078589, 0.007676864527546021, 0.008105748879476165, 0.007810286816437633, 0.01288703746846592, 0.011440887367732975, 0.010863585815745957, 0.010717444492465988, 0.011144171692240907, 0.012591788264903791, 0.012063028547456679, 0.012705373877127255, 0.010681955671242001, 0.010458979736633447, 0.01275492410593647, 0.010779364775790244, 0.012889341551012226, 0.010612479653399514, 0.010776408981265766, 0.010411985261289745, 0.010967368963269554, 0.011475127713032524, 0.010860446469896302, 0.012462921345568433, 0.010915724935998418, 0.011080636386284414, 0.011754896368223185, 0.011316644431719062, 0.013282011386615281, 0.010833373777948472, 0.012511694594761548, 0.011729566000249922, 0.010923162307020048, 0.01343323755059011, 0.010777869508886824, 0.012886179245210119, 0.011171953553067786, 0.011310425610756692, 0.010652769551783497, 0.012796577978499082, 0.013452413962317668, 0.010476131837967098, 0.0138354681394234, 0.01081600116223705, 0.012703053429912852, 0.010899479957107378, 0.010718852694963619, 0.010780747447695051, 0.01087511946656266, 0.012859919469575492, 0.010984320697203582, 0.01082561438789173, 0.010792547024368328, 0.010963920161736255, 0.010854685389227708, 0.011002951083058606, 0.012765900350688976, 0.010746241347598178, 0.010841754426685522, 0.01181864681444606, 0.011226750653693262, 0.012762152285752247, 0.010757186145958851, 0.012832458796245711, 0.01287967443219101, 0.012907242345414599, 0.010428913716911053, 0.010569515186647067, 0.010761942614668183, 0.01120657175399211, 0.014454282696681971, 0.010426026773734056, 0.07747436111925968, 0.010724282243802232, 0.01057954395798092, 0.011178289264516562, 0.010497346796019345, 0.012769965060549428, 0.01197049589542558, 0.010568713303655386, 0.010741570408513047, 0.010994169162586331, 0.012589617736865672, 0.010884573716404182, 0.0124696466302005, 0.010659854246151387, 0.012457277430981701, 0.010434562915327902, 0.010569472901751193, 0.010748937778288916, 0.010491642611557427, 0.012875449387546705, 0.011648641243500978, 0.010815938735114676, 0.010741999140009284, 0.01097691830779825, 0.01075478728708564, 0.01143338239029506, 0.012857991508303248, 0.011418855630279499, 0.010775852145399062, 0.011401182202128123, 0.01075659975485534, 0.012556391592346589, 0.011569412349134075, 0.012836135120834318, 0.01210849287881687, 0.012855761404130228, 0.010655256449149884, 0.011384958900245172, 0.010766930635828448]
[118.53530547041032, 98.47749434017167, 130.95779143825845, 84.22634732585524, 126.1292964136454, 127.78980031203841, 110.90366465033323, 127.22131302623227, 95.51861697709543, 129.05138620229505, 102.18510226595198, 98.1664469007521, 103.03363161234203, 127.82024043219192, 131.45252249798588, 104.46829201617697, 105.66781480379434, 99.12781941650485, 127.33909403916397, 102.47700086879276, 131.99270624282534, 130.46613048541926, 100.87507458230061, 133.73211810755734, 125.34268946819512, 130.18613264509494, 130.5584094234629, 129.83569536350385, 121.4861244820899, 84.4695539828454, 123.87976219898508, 102.64257818484396, 131.3176716890719, 125.69849562443403, 130.22038860794768, 129.31265322084653, 103.78952151121148, 131.6754650927589, 126.07820896546045, 124.9256530017425, 118.26895188684675, 125.62926418633087, 126.98130745198604, 99.26025809164426, 132.0511771375831, 96.78707924453464, 127.45992611300375, 120.3505745884869, 124.39763457457701, 99.58772389145717, 82.50446803152762, 128.88598822122222, 133.13730088750043, 17.251234766540712, 120.96542984244527, 126.83999927721955, 130.8226162915555, 102.44420688513716, 129.38110699989394, 97.03954658724648, 125.95103925551985, 80.01070377434641, 122.67919442217014, 124.61109635177178, 100.79052224352439, 126.95595528637472, 128.9873106508187, 129.80206377817197, 104.44159187970418, 126.77132561890576, 124.81223593883158, 96.89657553849165, 116.10116084512774, 92.64605717952783, 130.48379449954663, 126.79923981693823, 131.73628700754807, 127.24893704139622, 103.46878535170984, 125.93595895142606, 126.39152882624758, 119.23412978914614, 125.13124737700942, 130.89975448740128, 127.34788140193047, 101.02868395419898, 117.49639766661828, 95.41741787903628, 115.43176558109568, 130.67579832244738, 129.34769171599828, 132.09766801052422, 101.76486340942334, 105.50995214114751, 103.6310233848372, 103.00907740884782, 121.74998387294336, 130.82834260885403, 132.40675987473924, 102.04929328298189, 102.09266352691559, 82.00137756973187, 78.93413123113423, 94.89781561597353, 96.47607663885559, 110.39072156672601, 77.8476753430164, 102.9046401121728, 129.7028801778154, 127.6783681030235, 127.46851599167262, 126.00660181547764, 128.86454411209021, 79.73184312624218, 129.34127654794628, 102.1750448115301, 126.64442422330613, 128.76258158382782, 116.75562416021427, 126.27082309521938, 93.69576464489698, 111.08926127192277, 130.26151450397666, 123.36923026717614, 128.0362710746277, 77.59735334416162, 87.40580759673637, 92.050637511472, 93.30582497562429, 89.73300372752213, 79.4168373039777, 82.89792203226081, 78.70685346774735, 93.61581631462893, 95.61162036650829, 78.40109370267238, 92.76984505116066, 77.58348213850132, 94.22868478053273, 92.79529031780889, 96.04316323015317, 91.17957126718963, 87.14499960329685, 92.07724588228169, 80.23800939380719, 91.61095629133622, 90.24752416186111, 85.07093288404339, 88.36541662448383, 75.28980143833734, 92.30734769213983, 79.92522455101202, 85.25464624852216, 91.54858015405708, 74.44221813497751, 92.7827154685308, 77.60252135028362, 89.50986013770196, 88.4140026568901, 93.87230195293009, 78.1458919470665, 74.33610077724025, 95.45507974382754, 72.2780024443521, 92.4556113669252, 78.72123072750553, 91.74749657188147, 93.29356680774819, 92.75794696533795, 91.95301284502337, 77.76098461314938, 91.03885689122156, 92.37351010012728, 92.65653397127807, 91.20825263667729, 92.1261155106724, 90.88470833426804, 78.33368368303377, 93.05579203499867, 92.23599434595513, 84.61205548317842, 89.07296784675896, 78.35668918607143, 92.96111329036262, 77.92738834217508, 77.64171410269772, 77.47588316998309, 95.88726373087596, 94.61171892381061, 92.92002715541636, 89.23335538754488, 69.1836475724633, 95.91381469681882, 12.907495919335897, 93.2463336255365, 94.52203270497564, 89.4591270932948, 95.26216666283769, 78.30874988760345, 83.53872794711381, 94.61889742568108, 93.09625706195317, 90.95730520529432, 79.43052925838568, 91.87314322589374, 80.19473443442085, 93.80991305402118, 80.27436215821635, 95.83535104580616, 94.6120974333844, 93.03244847317241, 95.31395959851089, 77.66719202571775, 85.84692232305817, 92.4561449995489, 93.09254143164414, 91.10024981142271, 92.9818482975299, 87.46318157335706, 77.77264430095745, 87.57444987291804, 92.80008546024524, 87.71020252736088, 92.96618102282903, 79.64071466276371, 86.43481361219233, 77.90506960127749, 82.58666128048388, 77.78613561377435, 93.85039250554911, 87.83518752785882, 92.876979876917]
Elapsed: 0.5163274888302488~0.2791925173972672
Time per graph: 0.010537295690413243~0.005697806477495248
Speed: 102.17828849092474~20.424665076589196
Total Time: 0.5284
best val loss: 0.3458600640296936 test_score: 0.9592

Testing...
Test loss: 0.2217 score: 0.9592 time: 0.62s
test Score 0.9592
Epoch Time List: [2.2154872226528823, 1.7263218711595982, 1.5352067446801811, 1.6972300128545612, 1.650124792009592, 1.5629564530681819, 1.4781025499105453, 1.575922132236883, 1.5620515900664032, 1.5089438180439174, 1.9610912024509162, 1.7655683076009154, 1.7420374772045761, 1.490110909100622, 1.5796052489895374, 1.5278634470887482, 1.4932774791959673, 1.7752031318377703, 1.4434500893112272, 1.8080141579266638, 1.6490385076031089, 1.5328082228079438, 1.5075600810814649, 1.4263197958935052, 1.5232639703899622, 1.442019364098087, 1.4896360798738897, 1.3994019059464335, 1.537565134698525, 1.6651484926696867, 1.6637716579716653, 1.4919446650892496, 1.3804406342096627, 1.4899585728999227, 1.377669797744602, 1.587492355844006, 1.47796137095429, 1.4204738091211766, 1.5432791288476437, 1.4656159807927907, 1.5455259347800165, 1.564740390283987, 1.6052951400633901, 1.5167867271229625, 1.4395258051808923, 1.53297558426857, 1.4568788302130997, 1.5760398739948869, 1.417119415011257, 1.905023390892893, 1.9025871427729726, 1.4328237602021545, 1.614084956003353, 3.95759214903228, 3.1910674781538546, 1.4193657918367535, 1.5519783038180321, 1.519738381030038, 1.441027247812599, 1.5425868758466095, 1.4162086881697178, 1.783253117930144, 1.4540312988683581, 1.5368310620542616, 1.5250587649643421, 1.4170499350875616, 1.5372629940975457, 1.400027218973264, 1.6801361951511353, 1.5951967192813754, 1.5683879090938717, 1.6744421678595245, 1.4584217409137636, 1.5760256429202855, 1.4296018749009818, 1.5033389220479876, 1.4218515309039503, 1.4951815390959382, 1.5154012138955295, 1.436042367015034, 1.5146870280150324, 1.4676252731587738, 1.5117364907637239, 1.443506944924593, 1.5335457960609347, 1.5449995549861342, 1.4989676100667566, 1.567272363929078, 1.637793661793694, 1.5286495888140053, 1.4425930001307279, 1.4999541947618127, 1.5548318659421057, 1.4861307009123266, 1.6493138689547777, 1.8008846710436046, 1.6529598331544548, 1.467259337194264, 1.519135672133416, 1.5197944326791912, 1.7482119700871408, 1.9150033767800778, 1.9723143163137138, 1.6752435269299895, 1.8803565630223602, 1.740244162036106, 1.900683413259685, 1.7048914190381765, 1.77284677606076, 1.4128293341491371, 1.5577158341184258, 1.420098040951416, 1.549012305913493, 1.7789455149322748, 1.5002387322019786, 1.6783934312406927, 1.409467661054805, 1.609801545739174, 1.4436223888769746, 1.5147254893090576, 1.5914367749355733, 1.7583933756686747, 1.5315466648899019, 1.44227564195171, 1.6160639110021293, 1.8031963217072189, 1.5238851527683437, 1.5732424759771675, 1.4815721588674933, 1.5630954261869192, 1.559324880829081, 1.4915325162000954, 1.603926612995565, 1.4341670360881835, 1.515946814790368, 1.5074898558668792, 1.4434706503525376, 1.5433337381109595, 1.439330716850236, 1.5116842312272638, 1.4203675370663404, 1.5770724669564515, 1.4874742690008134, 1.533867216669023, 1.5943847389426082, 1.4748070409987122, 1.614106965251267, 1.525378294987604, 1.5808316967450082, 1.5939515258651227, 1.4872685670852661, 1.7661417918279767, 1.5714605550747365, 1.5391673038247973, 1.9332771969493479, 1.7242729670833796, 1.5523565059993416, 1.5805463681463152, 1.5606253622099757, 1.4561187271028757, 1.6537899551913142, 1.6061573519837111, 1.5094339668285102, 1.6057487076614052, 1.4744299540761858, 1.586711955955252, 1.4564184569753706, 1.6777082639746368, 1.4432169392239302, 1.6672114059329033, 1.5439409960526973, 1.4572024291846901, 1.5747153291013092, 1.452485548099503, 1.5500138078350574, 1.46012539207004, 1.5531966341659427, 1.5554261188954115, 1.4506708320695907, 1.5764113606419414, 1.5153576559387147, 1.5779366311617196, 1.58179547986947, 1.4359619677998126, 1.5910949278622866, 1.605637752218172, 1.5469137209001929, 1.5113383850548416, 1.5399147369898856, 1.476752761984244, 1.5581895678769797, 1.6070937938056886, 1.4588170398492366, 4.797567991074175, 2.6988803681451827, 1.5696707169990987, 1.4527502949349582, 1.5119242891669273, 1.5779141942039132, 1.5156996750738472, 1.5348139218986034, 1.4719729369971901, 1.552017570938915, 1.5649716267362237, 1.4353045839816332, 1.5111783889587969, 1.4611910050734878, 1.5106524007860571, 1.432140534510836, 1.5128724607639015, 1.4331311348360032, 1.4966244550887495, 1.5340982067864388, 1.5268434931058437, 1.532853915123269, 1.5150900550652295, 1.5516033051535487, 1.4723315790761262, 1.5749545018188655, 1.5336786550469697, 1.46846306999214, 1.5259731148835272, 1.5097216828726232, 1.533663687063381, 1.5512374378740788, 1.4753276677802205, 1.5738997149746865, 1.561986792832613, 1.5240113709587604, 1.5535718742758036, 1.5634434830863029, 1.4364477682393044]
Total Epoch List: [125, 107]
Total Time List: [0.38325966498814523, 0.528366613201797]
========================k_idx:2========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: FAGNN
FAGNN(
  (conv1): NeighborFilterConv()
  (conv2): NeighborFilterConv()
  (ffn): Sequential(
    (0): Linear(in_features=14951, out_features=64, bias=True)
    (1): ReLU()
    (2): Linear(in_features=64, out_features=64, bias=True)
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=64, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=64, out_features=2, bias=True)
  )
)
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7ed848808a30>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.6937;  Loss pred: 0.6937; Loss self: 0.0000; time: 0.68s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6930 score: 0.5102 time: 0.54s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6933 score: 0.5000 time: 0.65s
Epoch 2/1000, LR 0.000000
Train loss: 0.6937;  Loss pred: 0.6937; Loss self: 0.0000; time: 0.82s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6930 score: 0.5102 time: 0.55s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6933 score: 0.5000 time: 0.60s
Epoch 3/1000, LR 0.000030
Train loss: 0.6937;  Loss pred: 0.6937; Loss self: 0.0000; time: 0.79s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6930 score: 0.5102 time: 0.54s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6933 score: 0.5000 time: 0.51s
Epoch 4/1000, LR 0.000060
Train loss: 0.6937;  Loss pred: 0.6937; Loss self: 0.0000; time: 0.66s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6930 score: 0.5102 time: 0.45s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6933 score: 0.5000 time: 0.49s
Epoch 5/1000, LR 0.000090
Train loss: 0.6936;  Loss pred: 0.6936; Loss self: 0.0000; time: 0.55s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6929 score: 0.5102 time: 0.50s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6932 score: 0.5000 time: 0.61s
Epoch 6/1000, LR 0.000120
Train loss: 0.6935;  Loss pred: 0.6935; Loss self: 0.0000; time: 0.54s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6929 score: 0.5102 time: 0.48s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6932 score: 0.5000 time: 0.51s
Epoch 7/1000, LR 0.000150
Train loss: 0.6934;  Loss pred: 0.6934; Loss self: 0.0000; time: 0.55s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6929 score: 0.5102 time: 0.57s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6931 score: 0.5000 time: 0.47s
Epoch 8/1000, LR 0.000180
Train loss: 0.6933;  Loss pred: 0.6933; Loss self: 0.0000; time: 0.67s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6928 score: 0.5102 time: 0.45s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6931 score: 0.5000 time: 0.47s
Epoch 9/1000, LR 0.000210
Train loss: 0.6932;  Loss pred: 0.6932; Loss self: 0.0000; time: 0.67s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6928 score: 0.5102 time: 0.47s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6930 score: 0.5000 time: 0.92s
Epoch 10/1000, LR 0.000240
Train loss: 0.6931;  Loss pred: 0.6931; Loss self: 0.0000; time: 0.65s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6928 score: 0.5102 time: 0.42s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6929 score: 0.5000 time: 0.46s
Epoch 11/1000, LR 0.000270
Train loss: 0.6930;  Loss pred: 0.6930; Loss self: 0.0000; time: 0.67s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6927 score: 0.5102 time: 0.51s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6929 score: 0.5000 time: 0.60s
Epoch 12/1000, LR 0.000270
Train loss: 0.6928;  Loss pred: 0.6928; Loss self: 0.0000; time: 0.54s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6927 score: 0.5102 time: 0.44s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6928 score: 0.5000 time: 0.59s
Epoch 13/1000, LR 0.000270
Train loss: 0.6927;  Loss pred: 0.6927; Loss self: 0.0000; time: 0.56s
Val loss: 0.6926 score: 0.5714 time: 0.45s
Test loss: 0.6927 score: 0.5208 time: 0.48s
Epoch 14/1000, LR 0.000270
Train loss: 0.6925;  Loss pred: 0.6925; Loss self: 0.0000; time: 0.54s
Val loss: 0.6925 score: 0.6939 time: 0.51s
Test loss: 0.6926 score: 0.7500 time: 0.47s
Epoch 15/1000, LR 0.000270
Train loss: 0.6923;  Loss pred: 0.6923; Loss self: 0.0000; time: 0.56s
Val loss: 0.6924 score: 0.6531 time: 0.45s
Test loss: 0.6925 score: 0.6667 time: 0.59s
Epoch 16/1000, LR 0.000270
Train loss: 0.6921;  Loss pred: 0.6921; Loss self: 0.0000; time: 0.78s
Val loss: 0.6923 score: 0.6122 time: 0.53s
Test loss: 0.6923 score: 0.5417 time: 0.59s
Epoch 17/1000, LR 0.000270
Train loss: 0.6919;  Loss pred: 0.6919; Loss self: 0.0000; time: 0.67s
Val loss: 0.6921 score: 0.5510 time: 0.47s
Test loss: 0.6922 score: 0.5208 time: 0.76s
Epoch 18/1000, LR 0.000270
Train loss: 0.6916;  Loss pred: 0.6916; Loss self: 0.0000; time: 0.67s
Val loss: 0.6920 score: 0.5306 time: 0.56s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6920 score: 0.5000 time: 0.59s
Epoch 19/1000, LR 0.000270
Train loss: 0.6913;  Loss pred: 0.6913; Loss self: 0.0000; time: 0.67s
Val loss: 0.6918 score: 0.5306 time: 0.64s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6918 score: 0.5000 time: 0.66s
Epoch 20/1000, LR 0.000270
Train loss: 0.6910;  Loss pred: 0.6910; Loss self: 0.0000; time: 0.66s
Val loss: 0.6915 score: 0.5306 time: 0.52s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6916 score: 0.5000 time: 0.48s
Epoch 21/1000, LR 0.000270
Train loss: 0.6907;  Loss pred: 0.6907; Loss self: 0.0000; time: 0.67s
Val loss: 0.6912 score: 0.6122 time: 0.57s
Test loss: 0.6913 score: 0.5208 time: 0.59s
Epoch 22/1000, LR 0.000270
Train loss: 0.6902;  Loss pred: 0.6902; Loss self: 0.0000; time: 0.71s
Val loss: 0.6909 score: 0.6122 time: 0.55s
Test loss: 0.6910 score: 0.5417 time: 0.62s
Epoch 23/1000, LR 0.000270
Train loss: 0.6898;  Loss pred: 0.6898; Loss self: 0.0000; time: 0.81s
Val loss: 0.6906 score: 0.6122 time: 0.54s
Test loss: 0.6906 score: 0.5833 time: 0.62s
Epoch 24/1000, LR 0.000270
Train loss: 0.6893;  Loss pred: 0.6893; Loss self: 0.0000; time: 0.60s
Val loss: 0.6901 score: 0.6327 time: 0.42s
Test loss: 0.6903 score: 0.6042 time: 0.56s
Epoch 25/1000, LR 0.000270
Train loss: 0.6887;  Loss pred: 0.6887; Loss self: 0.0000; time: 0.54s
Val loss: 0.6897 score: 0.6327 time: 0.41s
Test loss: 0.6898 score: 0.6667 time: 0.51s
Epoch 26/1000, LR 0.000270
Train loss: 0.6882;  Loss pred: 0.6882; Loss self: 0.0000; time: 0.53s
Val loss: 0.6891 score: 0.6327 time: 0.52s
Test loss: 0.6893 score: 0.7083 time: 0.51s
Epoch 27/1000, LR 0.000270
Train loss: 0.6875;  Loss pred: 0.6875; Loss self: 0.0000; time: 0.54s
Val loss: 0.6885 score: 0.7143 time: 0.47s
Test loss: 0.6887 score: 0.7292 time: 0.55s
Epoch 28/1000, LR 0.000270
Train loss: 0.6866;  Loss pred: 0.6866; Loss self: 0.0000; time: 2.03s
Val loss: 0.6878 score: 0.7143 time: 3.42s
Test loss: 0.6881 score: 0.7708 time: 0.47s
Epoch 29/1000, LR 0.000270
Train loss: 0.6855;  Loss pred: 0.6855; Loss self: 0.0000; time: 0.55s
Val loss: 0.6870 score: 0.6939 time: 0.42s
Test loss: 0.6873 score: 0.7500 time: 0.58s
Epoch 30/1000, LR 0.000270
Train loss: 0.6850;  Loss pred: 0.6850; Loss self: 0.0000; time: 0.57s
Val loss: 0.6861 score: 0.7551 time: 0.42s
Test loss: 0.6865 score: 0.7500 time: 0.47s
Epoch 31/1000, LR 0.000270
Train loss: 0.6838;  Loss pred: 0.6838; Loss self: 0.0000; time: 0.55s
Val loss: 0.6850 score: 0.7551 time: 0.51s
Test loss: 0.6855 score: 0.7500 time: 0.50s
Epoch 32/1000, LR 0.000270
Train loss: 0.6824;  Loss pred: 0.6824; Loss self: 0.0000; time: 0.54s
Val loss: 0.6840 score: 0.7347 time: 0.42s
Test loss: 0.6845 score: 0.7083 time: 0.48s
Epoch 33/1000, LR 0.000270
Train loss: 0.6811;  Loss pred: 0.6811; Loss self: 0.0000; time: 0.64s
Val loss: 0.6828 score: 0.7143 time: 0.54s
Test loss: 0.6833 score: 0.7500 time: 0.47s
Epoch 34/1000, LR 0.000270
Train loss: 0.6794;  Loss pred: 0.6794; Loss self: 0.0000; time: 0.53s
Val loss: 0.6814 score: 0.7551 time: 0.42s
Test loss: 0.6821 score: 0.7500 time: 0.57s
Epoch 35/1000, LR 0.000270
Train loss: 0.6780;  Loss pred: 0.6780; Loss self: 0.0000; time: 0.57s
Val loss: 0.6800 score: 0.7551 time: 0.42s
Test loss: 0.6807 score: 0.7500 time: 0.46s
Epoch 36/1000, LR 0.000270
Train loss: 0.6758;  Loss pred: 0.6758; Loss self: 0.0000; time: 0.54s
Val loss: 0.6784 score: 0.7143 time: 0.52s
Test loss: 0.6792 score: 0.7292 time: 0.51s
Epoch 37/1000, LR 0.000270
Train loss: 0.6740;  Loss pred: 0.6740; Loss self: 0.0000; time: 0.54s
Val loss: 0.6766 score: 0.7143 time: 0.43s
Test loss: 0.6775 score: 0.7292 time: 0.46s
Epoch 38/1000, LR 0.000270
Train loss: 0.6726;  Loss pred: 0.6726; Loss self: 0.0000; time: 0.55s
Val loss: 0.6747 score: 0.6939 time: 0.56s
Test loss: 0.6756 score: 0.7292 time: 0.46s
Epoch 39/1000, LR 0.000269
Train loss: 0.6699;  Loss pred: 0.6699; Loss self: 0.0000; time: 0.55s
Val loss: 0.6727 score: 0.6939 time: 0.42s
Test loss: 0.6736 score: 0.7292 time: 0.47s
Epoch 40/1000, LR 0.000269
Train loss: 0.6668;  Loss pred: 0.6668; Loss self: 0.0000; time: 0.66s
Val loss: 0.6706 score: 0.6531 time: 0.42s
Test loss: 0.6714 score: 0.7083 time: 0.47s
Epoch 41/1000, LR 0.000269
Train loss: 0.6648;  Loss pred: 0.6648; Loss self: 0.0000; time: 0.61s
Val loss: 0.6682 score: 0.6531 time: 0.43s
Test loss: 0.6690 score: 0.7083 time: 0.61s
Epoch 42/1000, LR 0.000269
Train loss: 0.6614;  Loss pred: 0.6614; Loss self: 0.0000; time: 0.55s
Val loss: 0.6655 score: 0.6735 time: 0.49s
Test loss: 0.6663 score: 0.7083 time: 0.54s
Epoch 43/1000, LR 0.000269
Train loss: 0.6573;  Loss pred: 0.6573; Loss self: 0.0000; time: 0.55s
Val loss: 0.6625 score: 0.6735 time: 0.47s
Test loss: 0.6633 score: 0.7083 time: 0.58s
Epoch 44/1000, LR 0.000269
Train loss: 0.6526;  Loss pred: 0.6526; Loss self: 0.0000; time: 0.57s
Val loss: 0.6592 score: 0.6735 time: 0.46s
Test loss: 0.6601 score: 0.7083 time: 0.49s
Epoch 45/1000, LR 0.000269
Train loss: 0.6502;  Loss pred: 0.6502; Loss self: 0.0000; time: 0.66s
Val loss: 0.6555 score: 0.6735 time: 0.47s
Test loss: 0.6565 score: 0.7083 time: 0.47s
Epoch 46/1000, LR 0.000269
Train loss: 0.6452;  Loss pred: 0.6452; Loss self: 0.0000; time: 0.55s
Val loss: 0.6516 score: 0.6735 time: 0.43s
Test loss: 0.6525 score: 0.7083 time: 0.49s
Epoch 47/1000, LR 0.000269
Train loss: 0.6404;  Loss pred: 0.6404; Loss self: 0.0000; time: 0.64s
Val loss: 0.6472 score: 0.6735 time: 0.43s
Test loss: 0.6481 score: 0.7083 time: 0.47s
Epoch 48/1000, LR 0.000269
Train loss: 0.6347;  Loss pred: 0.6347; Loss self: 0.0000; time: 0.54s
Val loss: 0.6425 score: 0.6939 time: 0.43s
Test loss: 0.6433 score: 0.7083 time: 0.56s
Epoch 49/1000, LR 0.000269
Train loss: 0.6299;  Loss pred: 0.6299; Loss self: 0.0000; time: 0.56s
Val loss: 0.6374 score: 0.6939 time: 0.42s
Test loss: 0.6381 score: 0.7083 time: 0.48s
Epoch 50/1000, LR 0.000269
Train loss: 0.6220;  Loss pred: 0.6220; Loss self: 0.0000; time: 0.64s
Val loss: 0.6319 score: 0.7143 time: 0.43s
Test loss: 0.6325 score: 0.7083 time: 0.47s
Epoch 51/1000, LR 0.000269
Train loss: 0.6172;  Loss pred: 0.6172; Loss self: 0.0000; time: 0.54s
Val loss: 0.6259 score: 0.7347 time: 0.42s
Test loss: 0.6264 score: 0.7500 time: 0.47s
Epoch 52/1000, LR 0.000269
Train loss: 0.6057;  Loss pred: 0.6057; Loss self: 0.0000; time: 0.64s
Val loss: 0.6195 score: 0.7347 time: 0.43s
Test loss: 0.6197 score: 0.7500 time: 0.49s
Epoch 53/1000, LR 0.000269
Train loss: 0.5978;  Loss pred: 0.5978; Loss self: 0.0000; time: 0.69s
Val loss: 0.6126 score: 0.7347 time: 0.53s
Test loss: 0.6126 score: 0.7500 time: 0.70s
Epoch 54/1000, LR 0.000269
Train loss: 0.5894;  Loss pred: 0.5894; Loss self: 0.0000; time: 0.62s
Val loss: 0.6051 score: 0.7347 time: 0.42s
Test loss: 0.6048 score: 0.7708 time: 0.50s
Epoch 55/1000, LR 0.000269
Train loss: 0.5787;  Loss pred: 0.5787; Loss self: 0.0000; time: 0.56s
Val loss: 0.5972 score: 0.7551 time: 0.52s
Test loss: 0.5963 score: 0.7708 time: 0.48s
Epoch 56/1000, LR 0.000269
Train loss: 0.5764;  Loss pred: 0.5764; Loss self: 0.0000; time: 0.56s
Val loss: 0.5888 score: 0.7755 time: 0.43s
Test loss: 0.5873 score: 0.7708 time: 0.74s
Epoch 57/1000, LR 0.000269
Train loss: 0.5545;  Loss pred: 0.5545; Loss self: 0.0000; time: 0.77s
Val loss: 0.5799 score: 0.7755 time: 0.53s
Test loss: 0.5777 score: 0.7708 time: 0.58s
Epoch 58/1000, LR 0.000269
Train loss: 0.5461;  Loss pred: 0.5461; Loss self: 0.0000; time: 0.54s
Val loss: 0.5705 score: 0.7959 time: 0.42s
Test loss: 0.5676 score: 0.7708 time: 0.58s
Epoch 59/1000, LR 0.000268
Train loss: 0.5308;  Loss pred: 0.5308; Loss self: 0.0000; time: 0.55s
Val loss: 0.5608 score: 0.7959 time: 0.46s
Test loss: 0.5570 score: 0.7708 time: 0.51s
Epoch 60/1000, LR 0.000268
Train loss: 0.5210;  Loss pred: 0.5210; Loss self: 0.0000; time: 0.54s
Val loss: 0.5507 score: 0.8163 time: 0.43s
Test loss: 0.5459 score: 0.7708 time: 0.71s
Epoch 61/1000, LR 0.000268
Train loss: 0.5053;  Loss pred: 0.5053; Loss self: 0.0000; time: 0.70s
Val loss: 0.5403 score: 0.8163 time: 0.61s
Test loss: 0.5344 score: 0.7917 time: 0.47s
Epoch 62/1000, LR 0.000268
Train loss: 0.4949;  Loss pred: 0.4949; Loss self: 0.0000; time: 0.54s
Val loss: 0.5297 score: 0.8163 time: 0.56s
Test loss: 0.5225 score: 0.8125 time: 0.50s
Epoch 63/1000, LR 0.000268
Train loss: 0.4745;  Loss pred: 0.4745; Loss self: 0.0000; time: 0.55s
Val loss: 0.5189 score: 0.8163 time: 0.42s
Test loss: 0.5103 score: 0.8125 time: 0.48s
Epoch 64/1000, LR 0.000268
Train loss: 0.4658;  Loss pred: 0.4658; Loss self: 0.0000; time: 0.68s
Val loss: 0.5080 score: 0.8163 time: 0.42s
Test loss: 0.4979 score: 0.8125 time: 0.48s
Epoch 65/1000, LR 0.000268
Train loss: 0.4558;  Loss pred: 0.4558; Loss self: 0.0000; time: 0.53s
Val loss: 0.4971 score: 0.8367 time: 0.43s
Test loss: 0.4853 score: 0.8125 time: 0.47s
Epoch 66/1000, LR 0.000268
Train loss: 0.4361;  Loss pred: 0.4361; Loss self: 0.0000; time: 0.67s
Val loss: 0.4864 score: 0.8367 time: 0.42s
Test loss: 0.4725 score: 0.8542 time: 0.48s
Epoch 67/1000, LR 0.000268
Train loss: 0.4287;  Loss pred: 0.4287; Loss self: 0.0000; time: 0.56s
Val loss: 0.4758 score: 0.8367 time: 0.42s
Test loss: 0.4596 score: 0.8750 time: 0.60s
Epoch 68/1000, LR 0.000268
Train loss: 0.4048;  Loss pred: 0.4048; Loss self: 0.0000; time: 0.54s
Val loss: 0.4655 score: 0.8367 time: 0.42s
Test loss: 0.4467 score: 0.8750 time: 0.47s
Epoch 69/1000, LR 0.000268
Train loss: 0.3944;  Loss pred: 0.3944; Loss self: 0.0000; time: 0.55s
Val loss: 0.4556 score: 0.8367 time: 0.56s
Test loss: 0.4340 score: 0.8750 time: 0.48s
Epoch 70/1000, LR 0.000268
Train loss: 0.3723;  Loss pred: 0.3723; Loss self: 0.0000; time: 0.54s
Val loss: 0.4459 score: 0.8367 time: 0.50s
Test loss: 0.4214 score: 0.8750 time: 0.58s
Epoch 71/1000, LR 0.000268
Train loss: 0.3541;  Loss pred: 0.3541; Loss self: 0.0000; time: 0.82s
Val loss: 0.4363 score: 0.8367 time: 0.53s
Test loss: 0.4091 score: 0.8750 time: 0.59s
Epoch 72/1000, LR 0.000267
Train loss: 0.3492;  Loss pred: 0.3492; Loss self: 0.0000; time: 0.57s
Val loss: 0.4271 score: 0.8571 time: 0.46s
Test loss: 0.3970 score: 0.8958 time: 0.57s
Epoch 73/1000, LR 0.000267
Train loss: 0.3525;  Loss pred: 0.3525; Loss self: 0.0000; time: 0.55s
Val loss: 0.4184 score: 0.8571 time: 0.42s
Test loss: 0.3853 score: 0.8958 time: 0.48s
Epoch 74/1000, LR 0.000267
Train loss: 0.3163;  Loss pred: 0.3163; Loss self: 0.0000; time: 0.58s
Val loss: 0.4100 score: 0.8571 time: 0.45s
Test loss: 0.3739 score: 0.8958 time: 0.76s
Epoch 75/1000, LR 0.000267
Train loss: 0.3187;  Loss pred: 0.3187; Loss self: 0.0000; time: 0.68s
Val loss: 0.4021 score: 0.8571 time: 0.57s
Test loss: 0.3630 score: 0.8958 time: 0.64s
Epoch 76/1000, LR 0.000267
Train loss: 0.2804;  Loss pred: 0.2804; Loss self: 0.0000; time: 0.73s
Val loss: 0.3946 score: 0.8571 time: 0.64s
Test loss: 0.3523 score: 0.8958 time: 0.50s
Epoch 77/1000, LR 0.000267
Train loss: 0.2923;  Loss pred: 0.2923; Loss self: 0.0000; time: 0.74s
Val loss: 0.3875 score: 0.8571 time: 0.56s
Test loss: 0.3420 score: 0.8958 time: 0.56s
Epoch 78/1000, LR 0.000267
Train loss: 0.2749;  Loss pred: 0.2749; Loss self: 0.0000; time: 0.79s
Val loss: 0.3806 score: 0.8980 time: 0.64s
Test loss: 0.3320 score: 0.8958 time: 0.47s
Epoch 79/1000, LR 0.000267
Train loss: 0.2518;  Loss pred: 0.2518; Loss self: 0.0000; time: 0.54s
Val loss: 0.3741 score: 0.8776 time: 0.42s
Test loss: 0.3223 score: 0.8958 time: 0.50s
Epoch 80/1000, LR 0.000267
Train loss: 0.2534;  Loss pred: 0.2534; Loss self: 0.0000; time: 0.71s
Val loss: 0.3679 score: 0.8776 time: 0.43s
Test loss: 0.3129 score: 0.8958 time: 0.47s
Epoch 81/1000, LR 0.000267
Train loss: 0.2317;  Loss pred: 0.2317; Loss self: 0.0000; time: 0.55s
Val loss: 0.3621 score: 0.8776 time: 0.51s
Test loss: 0.3038 score: 0.8958 time: 0.57s
Epoch 82/1000, LR 0.000267
Train loss: 0.2308;  Loss pred: 0.2308; Loss self: 0.0000; time: 0.55s
Val loss: 0.3568 score: 0.8980 time: 0.42s
Test loss: 0.2952 score: 0.8958 time: 0.52s
Epoch 83/1000, LR 0.000266
Train loss: 0.2165;  Loss pred: 0.2165; Loss self: 0.0000; time: 0.67s
Val loss: 0.3518 score: 0.8980 time: 0.64s
Test loss: 0.2868 score: 0.8958 time: 0.59s
Epoch 84/1000, LR 0.000266
Train loss: 0.2083;  Loss pred: 0.2083; Loss self: 0.0000; time: 0.66s
Val loss: 0.3472 score: 0.8980 time: 0.43s
Test loss: 0.2789 score: 0.9167 time: 0.47s
Epoch 85/1000, LR 0.000266
Train loss: 0.1983;  Loss pred: 0.1983; Loss self: 0.0000; time: 0.66s
Val loss: 0.3431 score: 0.8980 time: 0.43s
Test loss: 0.2714 score: 0.9167 time: 0.47s
Epoch 86/1000, LR 0.000266
Train loss: 0.1875;  Loss pred: 0.1875; Loss self: 0.0000; time: 0.55s
Val loss: 0.3394 score: 0.8980 time: 0.43s
Test loss: 0.2643 score: 0.9167 time: 0.58s
Epoch 87/1000, LR 0.000266
Train loss: 0.1687;  Loss pred: 0.1687; Loss self: 0.0000; time: 0.54s
Val loss: 0.3361 score: 0.8980 time: 0.43s
Test loss: 0.2576 score: 0.9167 time: 0.48s
Epoch 88/1000, LR 0.000266
Train loss: 0.1683;  Loss pred: 0.1683; Loss self: 0.0000; time: 0.55s
Val loss: 0.3334 score: 0.8980 time: 0.43s
Test loss: 0.2515 score: 0.9167 time: 0.58s
Epoch 89/1000, LR 0.000266
Train loss: 0.1495;  Loss pred: 0.1495; Loss self: 0.0000; time: 0.55s
Val loss: 0.3309 score: 0.8980 time: 0.42s
Test loss: 0.2455 score: 0.9167 time: 0.66s
Epoch 90/1000, LR 0.000266
Train loss: 0.1481;  Loss pred: 0.1481; Loss self: 0.0000; time: 0.53s
Val loss: 0.3289 score: 0.8980 time: 0.53s
Test loss: 0.2399 score: 0.9167 time: 0.47s
Epoch 91/1000, LR 0.000266
Train loss: 0.1423;  Loss pred: 0.1423; Loss self: 0.0000; time: 0.59s
Val loss: 0.3275 score: 0.8980 time: 0.43s
Test loss: 0.2349 score: 0.9167 time: 0.47s
Epoch 92/1000, LR 0.000266
Train loss: 0.1286;  Loss pred: 0.1286; Loss self: 0.0000; time: 0.67s
Val loss: 0.3264 score: 0.8980 time: 0.42s
Test loss: 0.2302 score: 0.8958 time: 0.52s
Epoch 93/1000, LR 0.000265
Train loss: 0.1292;  Loss pred: 0.1292; Loss self: 0.0000; time: 0.56s
Val loss: 0.3258 score: 0.8980 time: 0.43s
Test loss: 0.2258 score: 0.8958 time: 0.49s
Epoch 94/1000, LR 0.000265
Train loss: 0.1138;  Loss pred: 0.1138; Loss self: 0.0000; time: 0.65s
Val loss: 0.3257 score: 0.8980 time: 0.45s
Test loss: 0.2216 score: 0.8958 time: 0.47s
Epoch 95/1000, LR 0.000265
Train loss: 0.1035;  Loss pred: 0.1035; Loss self: 0.0000; time: 0.55s
Val loss: 0.3260 score: 0.8980 time: 0.43s
Test loss: 0.2181 score: 0.8958 time: 0.62s
     INFO: Early stopping counter 1 of 20
Epoch 96/1000, LR 0.000265
Train loss: 0.1024;  Loss pred: 0.1024; Loss self: 0.0000; time: 0.57s
Val loss: 0.3270 score: 0.8980 time: 0.41s
Test loss: 0.2157 score: 0.8958 time: 0.47s
     INFO: Early stopping counter 2 of 20
Epoch 97/1000, LR 0.000265
Train loss: 0.0919;  Loss pred: 0.0919; Loss self: 0.0000; time: 0.54s
Val loss: 0.3285 score: 0.8980 time: 0.51s
Test loss: 0.2138 score: 0.8958 time: 0.48s
     INFO: Early stopping counter 3 of 20
Epoch 98/1000, LR 0.000265
Train loss: 0.0849;  Loss pred: 0.0849; Loss self: 0.0000; time: 0.53s
Val loss: 0.3303 score: 0.8980 time: 0.43s
Test loss: 0.2120 score: 0.8958 time: 0.46s
     INFO: Early stopping counter 4 of 20
Epoch 99/1000, LR 0.000265
Train loss: 0.0781;  Loss pred: 0.0781; Loss self: 0.0000; time: 0.63s
Val loss: 0.3324 score: 0.8980 time: 0.47s
Test loss: 0.2103 score: 0.8958 time: 0.46s
     INFO: Early stopping counter 5 of 20
Epoch 100/1000, LR 0.000265
Train loss: 0.0743;  Loss pred: 0.0743; Loss self: 0.0000; time: 0.71s
Val loss: 0.3349 score: 0.8980 time: 3.57s
Test loss: 0.2091 score: 0.8958 time: 1.85s
     INFO: Early stopping counter 6 of 20
Epoch 101/1000, LR 0.000265
Train loss: 0.0683;  Loss pred: 0.0683; Loss self: 0.0000; time: 0.53s
Val loss: 0.3376 score: 0.8980 time: 0.43s
Test loss: 0.2076 score: 0.8958 time: 0.46s
     INFO: Early stopping counter 7 of 20
Epoch 102/1000, LR 0.000264
Train loss: 0.0704;  Loss pred: 0.0704; Loss self: 0.0000; time: 0.70s
Val loss: 0.3403 score: 0.8980 time: 0.41s
Test loss: 0.2054 score: 0.8958 time: 0.56s
     INFO: Early stopping counter 8 of 20
Epoch 103/1000, LR 0.000264
Train loss: 0.0535;  Loss pred: 0.0535; Loss self: 0.0000; time: 0.54s
Val loss: 0.3436 score: 0.8980 time: 0.45s
Test loss: 0.2036 score: 0.8958 time: 0.47s
     INFO: Early stopping counter 9 of 20
Epoch 104/1000, LR 0.000264
Train loss: 0.0587;  Loss pred: 0.0587; Loss self: 0.0000; time: 0.53s
Val loss: 0.3472 score: 0.8980 time: 0.53s
Test loss: 0.2029 score: 0.8958 time: 0.47s
     INFO: Early stopping counter 10 of 20
Epoch 105/1000, LR 0.000264
Train loss: 0.0494;  Loss pred: 0.0494; Loss self: 0.0000; time: 0.57s
Val loss: 0.3512 score: 0.8980 time: 0.41s
Test loss: 0.2030 score: 0.8958 time: 0.46s
     INFO: Early stopping counter 11 of 20
Epoch 106/1000, LR 0.000264
Train loss: 0.0509;  Loss pred: 0.0509; Loss self: 0.0000; time: 0.65s
Val loss: 0.3553 score: 0.8980 time: 0.43s
Test loss: 0.2034 score: 0.8958 time: 0.51s
     INFO: Early stopping counter 12 of 20
Epoch 107/1000, LR 0.000264
Train loss: 0.0415;  Loss pred: 0.0415; Loss self: 0.0000; time: 0.55s
Val loss: 0.3596 score: 0.8980 time: 0.42s
Test loss: 0.2043 score: 0.8958 time: 0.48s
     INFO: Early stopping counter 13 of 20
Epoch 108/1000, LR 0.000264
Train loss: 0.0431;  Loss pred: 0.0431; Loss self: 0.0000; time: 0.64s
Val loss: 0.3641 score: 0.8980 time: 0.46s
Test loss: 0.2056 score: 0.8958 time: 0.47s
     INFO: Early stopping counter 14 of 20
Epoch 109/1000, LR 0.000264
Train loss: 0.0397;  Loss pred: 0.0397; Loss self: 0.0000; time: 0.53s
Val loss: 0.3688 score: 0.8980 time: 0.43s
Test loss: 0.2074 score: 0.8958 time: 0.58s
     INFO: Early stopping counter 15 of 20
Epoch 110/1000, LR 0.000263
Train loss: 0.0312;  Loss pred: 0.0312; Loss self: 0.0000; time: 0.55s
Val loss: 0.3736 score: 0.8980 time: 0.42s
Test loss: 0.2095 score: 0.8958 time: 0.47s
     INFO: Early stopping counter 16 of 20
Epoch 111/1000, LR 0.000263
Train loss: 0.0309;  Loss pred: 0.0309; Loss self: 0.0000; time: 0.54s
Val loss: 0.3785 score: 0.8980 time: 0.53s
Test loss: 0.2119 score: 0.8958 time: 0.49s
     INFO: Early stopping counter 17 of 20
Epoch 112/1000, LR 0.000263
Train loss: 0.0282;  Loss pred: 0.0282; Loss self: 0.0000; time: 0.58s
Val loss: 0.3835 score: 0.8980 time: 0.42s
Test loss: 0.2141 score: 0.8958 time: 0.47s
     INFO: Early stopping counter 18 of 20
Epoch 113/1000, LR 0.000263
Train loss: 0.0260;  Loss pred: 0.0260; Loss self: 0.0000; time: 0.63s
Val loss: 0.3885 score: 0.8980 time: 0.46s
Test loss: 0.2166 score: 0.8958 time: 0.47s
     INFO: Early stopping counter 19 of 20
Epoch 114/1000, LR 0.000263
Train loss: 0.0300;  Loss pred: 0.0300; Loss self: 0.0000; time: 0.65s
Val loss: 0.3941 score: 0.8980 time: 0.52s
Test loss: 0.2210 score: 0.8958 time: 0.61s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 093,   Train_Loss: 0.1138,   Val_Loss: 0.3257,   Val_Precision: 0.9545,   Val_Recall: 0.8400,   Val_accuracy: 0.8936,   Val_Score: 0.8980,   Val_Loss: 0.3257,   Test_Precision: 0.9524,   Test_Recall: 0.8333,   Test_accuracy: 0.8889,   Test_Score: 0.8958,   Test_loss: 0.2216


[0.4133789490442723, 0.4975756169296801, 0.3741663589607924, 0.5817657010629773, 0.38849023496732116, 0.3834421830251813, 0.4418248950969428, 0.3851555909495801, 0.5129890020471066, 0.3796937130391598, 0.47952195489779115, 0.49915222101844847, 0.47557287104427814, 0.3833508670795709, 0.3727581568527967, 0.469041840871796, 0.46371735888533294, 0.49431128706783056, 0.38479934516362846, 0.47815606999211013, 0.3712326339446008, 0.3755764029920101, 0.4857493310701102, 0.3664041270967573, 0.39092826400883496, 0.37638417398557067, 0.37531094485893846, 0.37740006600506604, 0.40333824302069843, 0.5800906680524349, 0.39554483420215547, 0.47738473513163626, 0.3731409441679716, 0.38982169004157186, 0.3762851618230343, 0.37892656889744103, 0.47210931591689587, 0.37212703190743923, 0.38864765292964876, 0.3922332909423858, 0.41430992004461586, 0.39003651193343103, 0.3858835680875927, 0.49365174886770546, 0.3710682559758425, 0.5062659229151905, 0.3844345551915467, 0.40714388084597886, 0.3938981650862843, 0.4920285160187632, 0.5939072291366756, 0.3801809698343277, 0.3680411099921912, 2.84037639410235, 0.40507440897636116, 0.3863134679850191, 0.37455297401174903, 0.4783091351855546, 0.37872608401812613, 0.5049487731885165, 0.3890400610398501, 0.6124180601909757, 0.39941573003306985, 0.3932234081439674, 0.4861568221822381, 0.3859606261830777, 0.3798823291435838, 0.37749784998595715, 0.469161749817431, 0.3865227389615029, 0.3925897139124572, 0.5056938258931041, 0.4220457370392978, 0.5288946069777012, 0.37552555999718606, 0.38643764797598124, 0.37195522291585803, 0.3850719789043069, 0.4735727768857032, 0.3890866469591856, 0.38768420997075737, 0.4109561590012163, 0.3915888399351388, 0.3743322528898716, 0.38477279292419553, 0.48501077201217413, 0.41703406209126115, 0.5135330748744309, 0.42449320387095213, 0.37497379491105676, 0.37882392294704914, 0.3709376610349864, 0.4815021448303014, 0.46441116696223617, 0.47283138195052743, 0.4756862330250442, 0.4024641190189868, 0.3745365799404681, 0.37007173988968134, 0.4801601110957563, 0.47995613305829465, 0.5975509369745851, 0.6207707519643009, 0.5163448671810329, 0.5078979339450598, 0.4438778848852962, 0.6294343380723149, 0.4761690041050315, 0.37778652203269303, 0.38377683493308723, 0.38440864882431924, 0.388868513982743, 0.3802442350424826, 0.6145599810406566, 0.37884271214716136, 0.47956915595568717, 0.38691004598513246, 0.3805453369859606, 0.41967999702319503, 0.3880548079032451, 0.5229692098218948, 0.4410867390688509, 0.37616636184975505, 0.3971816950943321, 0.38270405400544405, 0.63146483595483, 0.5606034810189158, 0.5323157049715519, 0.5251547801308334, 0.5460644129198045, 0.6169976249802858, 0.5910883988253772, 0.6225633199792355, 0.523415827890858, 0.5124900070950389, 0.6249912811908871, 0.5281888740137219, 0.6315777359995991, 0.5200115030165762, 0.5280440400820225, 0.5101872778031975, 0.5374010792002082, 0.5622812579385936, 0.5321618770249188, 0.6106831459328532, 0.5348705218639225, 0.5429511829279363, 0.5759899220429361, 0.554515577154234, 0.6508185579441488, 0.5308353151194751, 0.6130730351433158, 0.5747487340122461, 0.5352349530439824, 0.6582286399789155, 0.5281156059354544, 0.6314227830152959, 0.5474257241003215, 0.5542108549270779, 0.5219857080373913, 0.627032320946455, 0.6591682841535658, 0.5133304600603878, 0.6779379388317466, 0.5299840569496155, 0.6224496180657297, 0.5340745178982615, 0.5252237820532173, 0.5282566249370575, 0.5328808538615704, 0.6301360540091991, 0.5382317141629755, 0.5304551050066948, 0.528834804194048, 0.5372320879250765, 0.5318795840721577, 0.5391446030698717, 0.6255291171837598, 0.5265658260323107, 0.5312459669075906, 0.5791136939078569, 0.5501107820309699, 0.6253454620018601, 0.5271021211519837, 0.6287904810160398, 0.6311040471773595, 0.6324548749253154, 0.5110167721286416, 0.5179062441457063, 0.5273351881187409, 0.5491220159456134, 0.7082598521374166, 0.5108753119129688, 3.7962436948437244, 0.5254898299463093, 0.5183976539410651, 0.5477361739613116, 0.5143699930049479, 0.6257282879669219, 0.5865542988758534, 0.5178669518791139, 0.5263369500171393, 0.5387142889667302, 0.6168912691064179, 0.533344112103805, 0.6110126848798245, 0.5223328580614179, 0.6104065941181034, 0.5112935828510672, 0.5179041721858084, 0.5266979511361569, 0.514090487966314, 0.6308970199897885, 0.5707834209315479, 0.5299809980206192, 0.5263579578604549, 0.5378689970821142, 0.5269845770671964, 0.560235737124458, 0.6300415839068592, 0.5595239258836955, 0.528016755124554, 0.558657927904278, 0.5270733879879117, 0.6152631880249828, 0.5669012051075697, 0.6289706209208816, 0.5933161510620266, 0.6299323088023812, 0.5221075660083443, 0.5578629861120135, 0.527579601155594, 0.656038846122101, 0.6068650318775326, 0.5170636530965567, 0.49351877509616315, 0.6199782050680369, 0.5189863741397858, 0.47658152296207845, 0.4795715380460024, 0.9250722848810256, 0.4654783080331981, 0.6058691879734397, 0.5958478851243854, 0.4873881321400404, 0.4721486810594797, 0.5950830809306353, 0.5950115949381143, 0.7649761799257249, 0.5968898921273649, 0.6692281430587173, 0.48318949597887695, 0.6006097078789026, 0.6233338909223676, 0.6232136120088398, 0.5702417090069503, 0.5141342598944902, 0.5160057249013335, 0.5594917270354927, 0.470707732019946, 0.5849830980878323, 0.47415141691453755, 0.5064510609954596, 0.4837272809818387, 0.47963709896430373, 0.5736095129977912, 0.4635329069569707, 0.511407446116209, 0.4676684949081391, 0.46865127398632467, 0.4797917620744556, 0.47249079402536154, 0.6104245069436729, 0.5410150228999555, 0.5814121279399842, 0.49082245398312807, 0.47421355708502233, 0.49433204415254295, 0.4789445048663765, 0.5692249550484121, 0.4872460011392832, 0.47978224512189627, 0.46992485504597425, 0.4944515449460596, 0.7112110620364547, 0.5029836238827556, 0.4835287951864302, 0.7462797870393842, 0.5861152298748493, 0.5850584940053523, 0.5191469839774072, 0.7198381549678743, 0.47873167507350445, 0.5076930378563702, 0.48320481088012457, 0.4804090531542897, 0.4712018589489162, 0.481638987082988, 0.6050586251076311, 0.47723051183857024, 0.48008530703373253, 0.5905978078953922, 0.5991924130357802, 0.5790689832065254, 0.48223991016857326, 0.762197352014482, 0.647377033950761, 0.5088703150395304, 0.562064646044746, 0.4736368821468204, 0.5066780019551516, 0.47959562414325774, 0.5783756668679416, 0.5251511260867119, 0.5978590019512922, 0.47964711394160986, 0.4790783249773085, 0.5805942518636584, 0.4880704591050744, 0.5884120031259954, 0.6676046759821475, 0.47211989597417414, 0.47790962900035083, 0.5247127949260175, 0.4915412098634988, 0.47223269287496805, 0.625204186886549, 0.47235221206210554, 0.4813826330937445, 0.4691703310236335, 0.46890027984045446, 1.8560419050045311, 0.4622048297896981, 0.5667810579761863, 0.47351227584294975, 0.47542594908736646, 0.4673712740186602, 0.5112290191464126, 0.48473657201975584, 0.4770481779705733, 0.5806032710243016, 0.47048739390447736, 0.49658766482025385, 0.4740495029836893, 0.4801965579390526, 0.6193219288252294]
[0.008436305082536169, 0.01015460442713633, 0.007636048142056988, 0.011872769409448517, 0.007928372142190228, 0.007825350673983291, 0.009016834593815158, 0.007860318182644491, 0.010469163307083808, 0.007748851286513465, 0.00978616234485288, 0.010186780020784663, 0.009705568796822004, 0.007823487083256548, 0.007607309323526463, 0.009572282466771347, 0.009463619569088427, 0.01008798545036389, 0.007853047860482213, 0.009758287142696125, 0.0075761762029510375, 0.007664824550857349, 0.009913251654492045, 0.007477635246872598, 0.007978127836914999, 0.0076813096731749115, 0.0076594070379375195, 0.0077020421633686945, 0.008231392714708132, 0.01183858506229459, 0.00807234355514603, 0.009742545614931352, 0.007615121309550441, 0.007955544694725956, 0.0076792890167966185, 0.007733195283621245, 0.009634883998303997, 0.0075944292226008, 0.007931584753666301, 0.008004761039640526, 0.008455304490706446, 0.00795992881496798, 0.007875174858930464, 0.01007452548709603, 0.0075728215505273975, 0.010331957610514092, 0.007845603167174421, 0.00830905879277508, 0.008038738062985393, 0.010041398286097208, 0.012120555696666849, 0.007758795302741381, 0.007511043061065126, 0.057966865185762245, 0.008266824672986962, 0.007883948326224879, 0.007643938245137735, 0.009761410922154176, 0.007729103755471962, 0.010305077003847276, 0.00793959308244592, 0.012498327758999504, 0.008151341429246324, 0.008024967513142191, 0.009921567799637512, 0.007876747473124035, 0.0077527005947670155, 0.007704037754815452, 0.009574729588110837, 0.007888219162479651, 0.00801203497780525, 0.010320282161083758, 0.008613178306924445, 0.010793767489340841, 0.007663786938718083, 0.007886482611754718, 0.007590922916650164, 0.00785861181437361, 0.00966475054868782, 0.007940543815493584, 0.007911922652464437, 0.008386860387779924, 0.007991608978268139, 0.007639433732446359, 0.007852505978044808, 0.009898179020656615, 0.008510899226352269, 0.01048026683417206, 0.008663126609611268, 0.00765252642675626, 0.007731100468307125, 0.007570156347652783, 0.009826574384291865, 0.009477778917596656, 0.009649620039806682, 0.009707882306633555, 0.008213553449367078, 0.007643603672254451, 0.007552484487544517, 0.00979918594072972, 0.009795023123638667, 0.012194917081113981, 0.012668790856414303, 0.010537650350633324, 0.010365263958062445, 0.009058732344597881, 0.012845598736169691, 0.009717734777653704, 0.007709929021075368, 0.007832180304756882, 0.007845074465802434, 0.007936092122096797, 0.007760086429438421, 0.012542040429401154, 0.00773148392137064, 0.009787125631748718, 0.007896123387451683, 0.00776623136706042, 0.008564897898432552, 0.007919485875576431, 0.010672841016773363, 0.009001770185078589, 0.007676864527546021, 0.008105748879476165, 0.007810286816437633, 0.01288703746846592, 0.011440887367732975, 0.010863585815745957, 0.010717444492465988, 0.011144171692240907, 0.012591788264903791, 0.012063028547456679, 0.012705373877127255, 0.010681955671242001, 0.010458979736633447, 0.01275492410593647, 0.010779364775790244, 0.012889341551012226, 0.010612479653399514, 0.010776408981265766, 0.010411985261289745, 0.010967368963269554, 0.011475127713032524, 0.010860446469896302, 0.012462921345568433, 0.010915724935998418, 0.011080636386284414, 0.011754896368223185, 0.011316644431719062, 0.013282011386615281, 0.010833373777948472, 0.012511694594761548, 0.011729566000249922, 0.010923162307020048, 0.01343323755059011, 0.010777869508886824, 0.012886179245210119, 0.011171953553067786, 0.011310425610756692, 0.010652769551783497, 0.012796577978499082, 0.013452413962317668, 0.010476131837967098, 0.0138354681394234, 0.01081600116223705, 0.012703053429912852, 0.010899479957107378, 0.010718852694963619, 0.010780747447695051, 0.01087511946656266, 0.012859919469575492, 0.010984320697203582, 0.01082561438789173, 0.010792547024368328, 0.010963920161736255, 0.010854685389227708, 0.011002951083058606, 0.012765900350688976, 0.010746241347598178, 0.010841754426685522, 0.01181864681444606, 0.011226750653693262, 0.012762152285752247, 0.010757186145958851, 0.012832458796245711, 0.01287967443219101, 0.012907242345414599, 0.010428913716911053, 0.010569515186647067, 0.010761942614668183, 0.01120657175399211, 0.014454282696681971, 0.010426026773734056, 0.07747436111925968, 0.010724282243802232, 0.01057954395798092, 0.011178289264516562, 0.010497346796019345, 0.012769965060549428, 0.01197049589542558, 0.010568713303655386, 0.010741570408513047, 0.010994169162586331, 0.012589617736865672, 0.010884573716404182, 0.0124696466302005, 0.010659854246151387, 0.012457277430981701, 0.010434562915327902, 0.010569472901751193, 0.010748937778288916, 0.010491642611557427, 0.012875449387546705, 0.011648641243500978, 0.010815938735114676, 0.010741999140009284, 0.01097691830779825, 0.01075478728708564, 0.01143338239029506, 0.012857991508303248, 0.011418855630279499, 0.010775852145399062, 0.011401182202128123, 0.01075659975485534, 0.012556391592346589, 0.011569412349134075, 0.012836135120834318, 0.01210849287881687, 0.012855761404130228, 0.010655256449149884, 0.011384958900245172, 0.010766930635828448, 0.013667475960877104, 0.012643021497448595, 0.010772159439511597, 0.010281641147836732, 0.012916212605584102, 0.010812216127912203, 0.009928781728376634, 0.009991073709291717, 0.0192723392683547, 0.009697464750691628, 0.01262227474944666, 0.012413497606758028, 0.010153919419584176, 0.009836430855405828, 0.012397564186054902, 0.012396074894544048, 0.0159370037484526, 0.012435206085986769, 0.013942252980389943, 0.01006644783289327, 0.012512702247477137, 0.012986122727549324, 0.01298361691685083, 0.011880035604311464, 0.010711130414468547, 0.01075011926877778, 0.011656077646572763, 0.009806411083748875, 0.01218714787682984, 0.009878154519052865, 0.010551063770738741, 0.010077651687121639, 0.009992439561756328, 0.011950198187453983, 0.009656935561603555, 0.010654321794087688, 0.009743093643919565, 0.009763568208048431, 0.009995661709884493, 0.0098435582088617, 0.012717177227993185, 0.01127114631041574, 0.012112752665416338, 0.010225467791315168, 0.009879449105937965, 0.010298584253177978, 0.00997801051804951, 0.011858853230175251, 0.010150958357068399, 0.009995463440039506, 0.00979010114679113, 0.010301073853042908, 0.014816897125759473, 0.010478825497557409, 0.010073516566383963, 0.015547495563320505, 0.012210733955726027, 0.012188718625111505, 0.010815562166195983, 0.01499662822849738, 0.009973576564031342, 0.010576938288674379, 0.010066766893335929, 0.010008521940714369, 0.009816705394769087, 0.010034145564228917, 0.012605388023075648, 0.009942302329970213, 0.010001777229869427, 0.01230412099782067, 0.012483175271578753, 0.012063937150135947, 0.010046664795178609, 0.015879111500301708, 0.013487021540640853, 0.010601464896656884, 0.01170968012593221, 0.009867435044725426, 0.01055579170739899, 0.009991575502984537, 0.012049493059748784, 0.010940648460139831, 0.012455395873985253, 0.009992648207116872, 0.00998079843702726, 0.012095713580492884, 0.01016813456468905, 0.012258583398458237, 0.013908430749628073, 0.009835831166128628, 0.009956450604173975, 0.010931516560958698, 0.010240441872156225, 0.009838181101561835, 0.013025087226803104, 0.009840671084627198, 0.010028804856119677, 0.009774381896325698, 0.009768755830009468, 0.0386675396875944, 0.009629267287285378, 0.011807938707837215, 0.009864839080061453, 0.009904707272653468, 0.00973690154205542, 0.010650604565550262, 0.010098678583744913, 0.009938503707720278, 0.012095901479672952, 0.009801820706343278, 0.010345576350421956, 0.009876031312160194, 0.010004094957063595, 0.012902540183858946]
[118.53530547041032, 98.47749434017167, 130.95779143825845, 84.22634732585524, 126.1292964136454, 127.78980031203841, 110.90366465033323, 127.22131302623227, 95.51861697709543, 129.05138620229505, 102.18510226595198, 98.1664469007521, 103.03363161234203, 127.82024043219192, 131.45252249798588, 104.46829201617697, 105.66781480379434, 99.12781941650485, 127.33909403916397, 102.47700086879276, 131.99270624282534, 130.46613048541926, 100.87507458230061, 133.73211810755734, 125.34268946819512, 130.18613264509494, 130.5584094234629, 129.83569536350385, 121.4861244820899, 84.4695539828454, 123.87976219898508, 102.64257818484396, 131.3176716890719, 125.69849562443403, 130.22038860794768, 129.31265322084653, 103.78952151121148, 131.6754650927589, 126.07820896546045, 124.9256530017425, 118.26895188684675, 125.62926418633087, 126.98130745198604, 99.26025809164426, 132.0511771375831, 96.78707924453464, 127.45992611300375, 120.3505745884869, 124.39763457457701, 99.58772389145717, 82.50446803152762, 128.88598822122222, 133.13730088750043, 17.251234766540712, 120.96542984244527, 126.83999927721955, 130.8226162915555, 102.44420688513716, 129.38110699989394, 97.03954658724648, 125.95103925551985, 80.01070377434641, 122.67919442217014, 124.61109635177178, 100.79052224352439, 126.95595528637472, 128.9873106508187, 129.80206377817197, 104.44159187970418, 126.77132561890576, 124.81223593883158, 96.89657553849165, 116.10116084512774, 92.64605717952783, 130.48379449954663, 126.79923981693823, 131.73628700754807, 127.24893704139622, 103.46878535170984, 125.93595895142606, 126.39152882624758, 119.23412978914614, 125.13124737700942, 130.89975448740128, 127.34788140193047, 101.02868395419898, 117.49639766661828, 95.41741787903628, 115.43176558109568, 130.67579832244738, 129.34769171599828, 132.09766801052422, 101.76486340942334, 105.50995214114751, 103.6310233848372, 103.00907740884782, 121.74998387294336, 130.82834260885403, 132.40675987473924, 102.04929328298189, 102.09266352691559, 82.00137756973187, 78.93413123113423, 94.89781561597353, 96.47607663885559, 110.39072156672601, 77.8476753430164, 102.9046401121728, 129.7028801778154, 127.6783681030235, 127.46851599167262, 126.00660181547764, 128.86454411209021, 79.73184312624218, 129.34127654794628, 102.1750448115301, 126.64442422330613, 128.76258158382782, 116.75562416021427, 126.27082309521938, 93.69576464489698, 111.08926127192277, 130.26151450397666, 123.36923026717614, 128.0362710746277, 77.59735334416162, 87.40580759673637, 92.050637511472, 93.30582497562429, 89.73300372752213, 79.4168373039777, 82.89792203226081, 78.70685346774735, 93.61581631462893, 95.61162036650829, 78.40109370267238, 92.76984505116066, 77.58348213850132, 94.22868478053273, 92.79529031780889, 96.04316323015317, 91.17957126718963, 87.14499960329685, 92.07724588228169, 80.23800939380719, 91.61095629133622, 90.24752416186111, 85.07093288404339, 88.36541662448383, 75.28980143833734, 92.30734769213983, 79.92522455101202, 85.25464624852216, 91.54858015405708, 74.44221813497751, 92.7827154685308, 77.60252135028362, 89.50986013770196, 88.4140026568901, 93.87230195293009, 78.1458919470665, 74.33610077724025, 95.45507974382754, 72.2780024443521, 92.4556113669252, 78.72123072750553, 91.74749657188147, 93.29356680774819, 92.75794696533795, 91.95301284502337, 77.76098461314938, 91.03885689122156, 92.37351010012728, 92.65653397127807, 91.20825263667729, 92.1261155106724, 90.88470833426804, 78.33368368303377, 93.05579203499867, 92.23599434595513, 84.61205548317842, 89.07296784675896, 78.35668918607143, 92.96111329036262, 77.92738834217508, 77.64171410269772, 77.47588316998309, 95.88726373087596, 94.61171892381061, 92.92002715541636, 89.23335538754488, 69.1836475724633, 95.91381469681882, 12.907495919335897, 93.2463336255365, 94.52203270497564, 89.4591270932948, 95.26216666283769, 78.30874988760345, 83.53872794711381, 94.61889742568108, 93.09625706195317, 90.95730520529432, 79.43052925838568, 91.87314322589374, 80.19473443442085, 93.80991305402118, 80.27436215821635, 95.83535104580616, 94.6120974333844, 93.03244847317241, 95.31395959851089, 77.66719202571775, 85.84692232305817, 92.4561449995489, 93.09254143164414, 91.10024981142271, 92.9818482975299, 87.46318157335706, 77.77264430095745, 87.57444987291804, 92.80008546024524, 87.71020252736088, 92.96618102282903, 79.64071466276371, 86.43481361219233, 77.90506960127749, 82.58666128048388, 77.78613561377435, 93.85039250554911, 87.83518752785882, 92.876979876917, 73.16639903830682, 79.09501697848124, 92.8318974125154, 97.26073742715685, 77.42207646595003, 92.48797731840162, 100.71729113975607, 100.08934265693568, 51.88783707445449, 103.11973548846152, 79.22502241870772, 80.55747313759422, 98.48413786613959, 101.6628912153058, 80.66100606479017, 80.67069685422241, 62.747051816254654, 80.41684175438796, 71.72441938950038, 99.3399078404187, 79.9187881419958, 77.00527871021553, 77.02014056669731, 84.17483190345686, 93.36082759754325, 93.02222375377363, 85.79215327156216, 101.9741056600405, 82.05365275834522, 101.23348425773378, 94.77717334751587, 99.22946645178388, 100.07566158591149, 83.68062054818961, 103.55251866607131, 93.85862557248097, 102.63680475082742, 102.42157157008101, 100.04340173008474, 101.58928090654723, 78.6338023031392, 88.72212040011347, 82.55761738247531, 97.79503690279417, 101.22021878719511, 97.1007252469111, 100.22037942243809, 84.32518563055206, 98.51286596045108, 100.04538618932187, 102.14399064996041, 97.07725760111921, 67.49051380410005, 95.43054230964127, 99.27019957827545, 64.31904070512745, 81.89515909738302, 82.04307858414049, 92.45936407498982, 66.68165568709288, 100.26493440742162, 94.54531857019383, 99.33675931862366, 99.91485315449324, 101.86717027617618, 99.65970630971665, 79.33115570654249, 100.58032504056794, 99.98223085929048, 81.27358306839814, 80.10782338983608, 82.89167852542494, 99.53551953678196, 62.97581574265031, 74.14535499826033, 94.32658691492199, 85.39942929657012, 101.34345911246142, 94.73472267353084, 100.08431600214548, 82.99104327803552, 91.40226044583277, 80.28648869271451, 100.07357201745471, 100.19238503907167, 82.67391529613678, 98.34645614080549, 81.57549428800803, 71.89883733121663, 101.66908958783998, 100.43739880362352, 91.47861547147485, 97.65203616056856, 101.64480503832641, 76.77491770974046, 101.61908587333744, 99.71277877540811, 102.30825955101176, 102.36718138946777, 25.8614850616117, 103.85006150160712, 84.68878648025806, 101.37012797514083, 100.9620953423796, 102.7020757764491, 93.89138370929041, 99.02285647645279, 100.6187681172967, 82.67263102965005, 102.02186205597975, 96.65967038745154, 101.25524802343595, 99.9590671911735, 77.50411823952295]
Elapsed: 0.5270960964982511~0.24415534872360697
Time per graph: 0.01083397138657388~0.005003340742155232
Speed: 98.30850012278076~19.114240008984897
Total Time: 0.6204
best val loss: 0.3256780505180359 test_score: 0.8958

Testing...
Test loss: 0.3320 score: 0.8958 time: 0.48s
test Score 0.8958
Epoch Time List: [2.2154872226528823, 1.7263218711595982, 1.5352067446801811, 1.6972300128545612, 1.650124792009592, 1.5629564530681819, 1.4781025499105453, 1.575922132236883, 1.5620515900664032, 1.5089438180439174, 1.9610912024509162, 1.7655683076009154, 1.7420374772045761, 1.490110909100622, 1.5796052489895374, 1.5278634470887482, 1.4932774791959673, 1.7752031318377703, 1.4434500893112272, 1.8080141579266638, 1.6490385076031089, 1.5328082228079438, 1.5075600810814649, 1.4263197958935052, 1.5232639703899622, 1.442019364098087, 1.4896360798738897, 1.3994019059464335, 1.537565134698525, 1.6651484926696867, 1.6637716579716653, 1.4919446650892496, 1.3804406342096627, 1.4899585728999227, 1.377669797744602, 1.587492355844006, 1.47796137095429, 1.4204738091211766, 1.5432791288476437, 1.4656159807927907, 1.5455259347800165, 1.564740390283987, 1.6052951400633901, 1.5167867271229625, 1.4395258051808923, 1.53297558426857, 1.4568788302130997, 1.5760398739948869, 1.417119415011257, 1.905023390892893, 1.9025871427729726, 1.4328237602021545, 1.614084956003353, 3.95759214903228, 3.1910674781538546, 1.4193657918367535, 1.5519783038180321, 1.519738381030038, 1.441027247812599, 1.5425868758466095, 1.4162086881697178, 1.783253117930144, 1.4540312988683581, 1.5368310620542616, 1.5250587649643421, 1.4170499350875616, 1.5372629940975457, 1.400027218973264, 1.6801361951511353, 1.5951967192813754, 1.5683879090938717, 1.6744421678595245, 1.4584217409137636, 1.5760256429202855, 1.4296018749009818, 1.5033389220479876, 1.4218515309039503, 1.4951815390959382, 1.5154012138955295, 1.436042367015034, 1.5146870280150324, 1.4676252731587738, 1.5117364907637239, 1.443506944924593, 1.5335457960609347, 1.5449995549861342, 1.4989676100667566, 1.567272363929078, 1.637793661793694, 1.5286495888140053, 1.4425930001307279, 1.4999541947618127, 1.5548318659421057, 1.4861307009123266, 1.6493138689547777, 1.8008846710436046, 1.6529598331544548, 1.467259337194264, 1.519135672133416, 1.5197944326791912, 1.7482119700871408, 1.9150033767800778, 1.9723143163137138, 1.6752435269299895, 1.8803565630223602, 1.740244162036106, 1.900683413259685, 1.7048914190381765, 1.77284677606076, 1.4128293341491371, 1.5577158341184258, 1.420098040951416, 1.549012305913493, 1.7789455149322748, 1.5002387322019786, 1.6783934312406927, 1.409467661054805, 1.609801545739174, 1.4436223888769746, 1.5147254893090576, 1.5914367749355733, 1.7583933756686747, 1.5315466648899019, 1.44227564195171, 1.6160639110021293, 1.8031963217072189, 1.5238851527683437, 1.5732424759771675, 1.4815721588674933, 1.5630954261869192, 1.559324880829081, 1.4915325162000954, 1.603926612995565, 1.4341670360881835, 1.515946814790368, 1.5074898558668792, 1.4434706503525376, 1.5433337381109595, 1.439330716850236, 1.5116842312272638, 1.4203675370663404, 1.5770724669564515, 1.4874742690008134, 1.533867216669023, 1.5943847389426082, 1.4748070409987122, 1.614106965251267, 1.525378294987604, 1.5808316967450082, 1.5939515258651227, 1.4872685670852661, 1.7661417918279767, 1.5714605550747365, 1.5391673038247973, 1.9332771969493479, 1.7242729670833796, 1.5523565059993416, 1.5805463681463152, 1.5606253622099757, 1.4561187271028757, 1.6537899551913142, 1.6061573519837111, 1.5094339668285102, 1.6057487076614052, 1.4744299540761858, 1.586711955955252, 1.4564184569753706, 1.6777082639746368, 1.4432169392239302, 1.6672114059329033, 1.5439409960526973, 1.4572024291846901, 1.5747153291013092, 1.452485548099503, 1.5500138078350574, 1.46012539207004, 1.5531966341659427, 1.5554261188954115, 1.4506708320695907, 1.5764113606419414, 1.5153576559387147, 1.5779366311617196, 1.58179547986947, 1.4359619677998126, 1.5910949278622866, 1.605637752218172, 1.5469137209001929, 1.5113383850548416, 1.5399147369898856, 1.476752761984244, 1.5581895678769797, 1.6070937938056886, 1.4588170398492366, 4.797567991074175, 2.6988803681451827, 1.5696707169990987, 1.4527502949349582, 1.5119242891669273, 1.5779141942039132, 1.5156996750738472, 1.5348139218986034, 1.4719729369971901, 1.552017570938915, 1.5649716267362237, 1.4353045839816332, 1.5111783889587969, 1.4611910050734878, 1.5106524007860571, 1.432140534510836, 1.5128724607639015, 1.4331311348360032, 1.4966244550887495, 1.5340982067864388, 1.5268434931058437, 1.532853915123269, 1.5150900550652295, 1.5516033051535487, 1.4723315790761262, 1.5749545018188655, 1.5336786550469697, 1.46846306999214, 1.5259731148835272, 1.5097216828726232, 1.533663687063381, 1.5512374378740788, 1.4753276677802205, 1.5738997149746865, 1.561986792832613, 1.5240113709587604, 1.5535718742758036, 1.5634434830863029, 1.4364477682393044, 1.863855181960389, 1.96316938707605, 1.842659360030666, 1.6013762201182544, 1.669811625033617, 1.5304654112551361, 1.5911266182083637, 1.6029136390425265, 2.0539332639891654, 1.531094741076231, 1.779695776058361, 1.579593472648412, 1.4900733542162925, 1.5243826261721551, 1.6009477807674557, 1.892990779131651, 1.8914059831295162, 1.817345391958952, 1.974378545768559, 1.6617127782665193, 1.8399440238717943, 1.8758193280082196, 1.9651698700617999, 1.581693028099835, 1.460623114835471, 1.556852401001379, 1.5595963918603957, 5.911836610175669, 1.5473339140880853, 1.4649989400058985, 1.5614777328446507, 1.430181598989293, 1.6577788209542632, 1.5199954689014703, 1.4522519039455801, 1.5646315643098205, 1.428641561185941, 1.57107138982974, 1.4416798970196396, 1.5530617607291788, 1.6437059550080448, 1.573231877060607, 1.602110147709027, 1.5211156329605728, 1.6018755498807877, 1.4706602941732854, 1.5481050873640925, 1.5313718889374286, 1.467488051392138, 1.542378299869597, 1.428087822161615, 1.556817646836862, 1.926836364902556, 1.541287516709417, 1.5601348511409014, 1.7325134749989957, 1.8785873528104275, 1.549460764741525, 1.5169525311794132, 1.677058483241126, 1.7835694758687168, 1.6004192556720227, 1.449601228814572, 1.5682627002242953, 1.4234129327815026, 1.5664784701075405, 1.5779606169089675, 1.4338941182941198, 1.5862686128821224, 1.6224556288216263, 1.9411547479685396, 1.601444992935285, 1.45036851009354, 1.7853833592962474, 1.8863190819974989, 1.8671756780240685, 1.8529191182460636, 1.8914331588894129, 1.4633014660794288, 1.6074172609951347, 1.6337095629423857, 1.489824964897707, 1.899942914955318, 1.5617255128454417, 1.558702782029286, 1.547319515608251, 1.4574502597097307, 1.5648484211415052, 1.63658379111439, 1.5313136151526123, 1.4945810751523823, 1.6076657378580421, 1.4684833227656782, 1.5729786718729883, 1.5992459198459983, 1.4507754987571388, 1.527016876032576, 1.418434085790068, 1.5632306130137295, 6.137779564829543, 1.4155414998531342, 1.6767124687321484, 1.451718470081687, 1.5320588846225291, 1.4476466609630734, 1.5835418552160263, 1.4571976310107857, 1.572964794933796, 1.5376074351370335, 1.4356354798655957, 1.5544111360795796, 1.4703185302205384, 1.5681435668375343, 1.786383581114933]
Total Epoch List: [125, 107, 114]
Total Time List: [0.38325966498814523, 0.528366613201797, 0.620382044930011]
========================training times:1========================
========================k_idx:0========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: FAGNN
FAGNN(
  (conv1): NeighborFilterConv()
  (conv2): NeighborFilterConv()
  (ffn): Sequential(
    (0): Linear(in_features=14951, out_features=64, bias=True)
    (1): ReLU()
    (2): Linear(in_features=64, out_features=64, bias=True)
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=64, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=64, out_features=2, bias=True)
  )
)
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7ed8489165c0>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.6936;  Loss pred: 0.6936; Loss self: 0.0000; time: 0.53s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6943 score: 0.4898 time: 0.42s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6931 score: 0.5102 time: 0.48s
Epoch 2/1000, LR 0.000000
Train loss: 0.6937;  Loss pred: 0.6937; Loss self: 0.0000; time: 0.62s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6943 score: 0.4898 time: 0.41s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6930 score: 0.5102 time: 0.49s
Epoch 3/1000, LR 0.000030
Train loss: 0.6936;  Loss pred: 0.6936; Loss self: 0.0000; time: 0.54s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6942 score: 0.4898 time: 0.41s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6930 score: 0.5102 time: 0.49s
Epoch 4/1000, LR 0.000060
Train loss: 0.6936;  Loss pred: 0.6936; Loss self: 0.0000; time: 0.62s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6941 score: 0.4898 time: 0.41s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6929 score: 0.5102 time: 0.50s
Epoch 5/1000, LR 0.000090
Train loss: 0.6935;  Loss pred: 0.6935; Loss self: 0.0000; time: 0.52s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6940 score: 0.4898 time: 0.43s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6929 score: 0.5102 time: 0.61s
Epoch 6/1000, LR 0.000120
Train loss: 0.6934;  Loss pred: 0.6934; Loss self: 0.0000; time: 0.71s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6939 score: 0.4898 time: 0.56s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6928 score: 0.5102 time: 0.63s
Epoch 7/1000, LR 0.000150
Train loss: 0.6933;  Loss pred: 0.6933; Loss self: 0.0000; time: 0.58s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6937 score: 0.4898 time: 0.61s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6927 score: 0.5102 time: 0.53s
Epoch 8/1000, LR 0.000180
Train loss: 0.6932;  Loss pred: 0.6932; Loss self: 0.0000; time: 0.63s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6935 score: 0.4898 time: 0.49s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6925 score: 0.5102 time: 0.54s
Epoch 9/1000, LR 0.000210
Train loss: 0.6930;  Loss pred: 0.6930; Loss self: 0.0000; time: 0.63s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6933 score: 0.4898 time: 0.42s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6924 score: 0.5102 time: 0.50s
Epoch 10/1000, LR 0.000240
Train loss: 0.6929;  Loss pred: 0.6929; Loss self: 0.0000; time: 0.52s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6930 score: 0.4898 time: 0.50s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6922 score: 0.5102 time: 0.62s
Epoch 11/1000, LR 0.000270
Train loss: 0.6927;  Loss pred: 0.6927; Loss self: 0.0000; time: 0.54s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6928 score: 0.4898 time: 0.43s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6921 score: 0.5102 time: 0.49s
Epoch 12/1000, LR 0.000270
Train loss: 0.6925;  Loss pred: 0.6925; Loss self: 0.0000; time: 0.57s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6925 score: 0.4898 time: 0.52s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6919 score: 0.5102 time: 0.50s
Epoch 13/1000, LR 0.000270
Train loss: 0.6922;  Loss pred: 0.6922; Loss self: 0.0000; time: 0.53s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6922 score: 0.4898 time: 0.42s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6917 score: 0.5102 time: 0.53s
Epoch 14/1000, LR 0.000270
Train loss: 0.6920;  Loss pred: 0.6920; Loss self: 0.0000; time: 0.61s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6920 score: 0.4898 time: 0.42s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6914 score: 0.5102 time: 0.50s
Epoch 15/1000, LR 0.000270
Train loss: 0.6917;  Loss pred: 0.6917; Loss self: 0.0000; time: 0.52s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6916 score: 0.4898 time: 0.44s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6911 score: 0.5102 time: 0.48s
Epoch 16/1000, LR 0.000270
Train loss: 0.6914;  Loss pred: 0.6914; Loss self: 0.0000; time: 0.63s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6913 score: 0.4898 time: 0.41s
Test loss: 0.6908 score: 0.5510 time: 0.50s
Epoch 17/1000, LR 0.000270
Train loss: 0.6910;  Loss pred: 0.6910; Loss self: 0.0000; time: 0.53s
Val loss: 0.6909 score: 0.5102 time: 0.57s
Test loss: 0.6904 score: 0.6122 time: 0.61s
Epoch 18/1000, LR 0.000270
Train loss: 0.6907;  Loss pred: 0.6907; Loss self: 0.0000; time: 0.53s
Val loss: 0.6904 score: 0.5714 time: 0.44s
Test loss: 0.6899 score: 0.6735 time: 0.56s
Epoch 19/1000, LR 0.000270
Train loss: 0.6902;  Loss pred: 0.6902; Loss self: 0.0000; time: 0.53s
Val loss: 0.6899 score: 0.5714 time: 0.51s
Test loss: 0.6894 score: 0.6939 time: 0.50s
Epoch 20/1000, LR 0.000270
Train loss: 0.6897;  Loss pred: 0.6897; Loss self: 0.0000; time: 0.55s
Val loss: 0.6894 score: 0.5918 time: 0.45s
Test loss: 0.6888 score: 0.7143 time: 0.51s
Epoch 21/1000, LR 0.000270
Train loss: 0.6893;  Loss pred: 0.6893; Loss self: 0.0000; time: 0.62s
Val loss: 0.6888 score: 0.5918 time: 0.42s
Test loss: 0.6881 score: 0.7143 time: 0.52s
Epoch 22/1000, LR 0.000270
Train loss: 0.6884;  Loss pred: 0.6884; Loss self: 0.0000; time: 0.56s
Val loss: 0.6881 score: 0.5918 time: 0.43s
Test loss: 0.6874 score: 0.7551 time: 0.59s
Epoch 23/1000, LR 0.000270
Train loss: 0.6877;  Loss pred: 0.6877; Loss self: 0.0000; time: 0.63s
Val loss: 0.6874 score: 0.6122 time: 0.41s
Test loss: 0.6865 score: 0.7551 time: 0.52s
Epoch 24/1000, LR 0.000270
Train loss: 0.6866;  Loss pred: 0.6866; Loss self: 0.0000; time: 0.53s
Val loss: 0.6866 score: 0.6122 time: 0.53s
Test loss: 0.6856 score: 0.7551 time: 0.50s
Epoch 25/1000, LR 0.000270
Train loss: 0.6860;  Loss pred: 0.6860; Loss self: 0.0000; time: 0.51s
Val loss: 0.6857 score: 0.6327 time: 0.45s
Test loss: 0.6845 score: 0.7755 time: 0.49s
Epoch 26/1000, LR 0.000270
Train loss: 0.6848;  Loss pred: 0.6848; Loss self: 0.0000; time: 0.61s
Val loss: 0.6846 score: 0.6327 time: 0.42s
Test loss: 0.6833 score: 0.7755 time: 0.49s
Epoch 27/1000, LR 0.000270
Train loss: 0.6833;  Loss pred: 0.6833; Loss self: 0.0000; time: 0.57s
Val loss: 0.6834 score: 0.6531 time: 0.41s
Test loss: 0.6820 score: 0.7755 time: 0.59s
Epoch 28/1000, LR 0.000270
Train loss: 0.6822;  Loss pred: 0.6822; Loss self: 0.0000; time: 0.52s
Val loss: 0.6821 score: 0.6531 time: 0.42s
Test loss: 0.6806 score: 0.7755 time: 0.72s
Epoch 29/1000, LR 0.000270
Train loss: 0.6805;  Loss pred: 0.6805; Loss self: 0.0000; time: 0.52s
Val loss: 0.6807 score: 0.6531 time: 0.52s
Test loss: 0.6789 score: 0.7755 time: 0.50s
Epoch 30/1000, LR 0.000270
Train loss: 0.6789;  Loss pred: 0.6789; Loss self: 0.0000; time: 0.53s
Val loss: 0.6790 score: 0.6531 time: 0.51s
Test loss: 0.6771 score: 0.7755 time: 0.60s
Epoch 31/1000, LR 0.000270
Train loss: 0.6770;  Loss pred: 0.6770; Loss self: 0.0000; time: 0.66s
Val loss: 0.6772 score: 0.6531 time: 0.45s
Test loss: 0.6750 score: 0.7755 time: 0.52s
Epoch 32/1000, LR 0.000270
Train loss: 0.6754;  Loss pred: 0.6754; Loss self: 0.0000; time: 0.56s
Val loss: 0.6752 score: 0.6735 time: 0.44s
Test loss: 0.6728 score: 0.7551 time: 0.55s
Epoch 33/1000, LR 0.000270
Train loss: 0.6727;  Loss pred: 0.6727; Loss self: 0.0000; time: 0.70s
Val loss: 0.6731 score: 0.6735 time: 0.47s
Test loss: 0.6703 score: 0.7755 time: 0.57s
Epoch 34/1000, LR 0.000270
Train loss: 0.6694;  Loss pred: 0.6694; Loss self: 0.0000; time: 0.57s
Val loss: 0.6707 score: 0.6735 time: 0.57s
Test loss: 0.6676 score: 0.7755 time: 0.53s
Epoch 35/1000, LR 0.000270
Train loss: 0.6676;  Loss pred: 0.6676; Loss self: 0.0000; time: 0.58s
Val loss: 0.6682 score: 0.7143 time: 0.42s
Test loss: 0.6648 score: 0.7755 time: 0.52s
Epoch 36/1000, LR 0.000270
Train loss: 0.6642;  Loss pred: 0.6642; Loss self: 0.0000; time: 0.57s
Val loss: 0.6656 score: 0.7143 time: 0.54s
Test loss: 0.6618 score: 0.7755 time: 0.56s
Epoch 37/1000, LR 0.000270
Train loss: 0.6594;  Loss pred: 0.6594; Loss self: 0.0000; time: 0.57s
Val loss: 0.6627 score: 0.7143 time: 0.42s
Test loss: 0.6585 score: 0.8163 time: 0.49s
Epoch 38/1000, LR 0.000270
Train loss: 0.6564;  Loss pred: 0.6564; Loss self: 0.0000; time: 0.62s
Val loss: 0.6597 score: 0.7347 time: 0.41s
Test loss: 0.6550 score: 0.8571 time: 0.51s
Epoch 39/1000, LR 0.000269
Train loss: 0.6517;  Loss pred: 0.6517; Loss self: 0.0000; time: 0.52s
Val loss: 0.6563 score: 0.7347 time: 0.42s
Test loss: 0.6512 score: 0.8980 time: 0.59s
Epoch 40/1000, LR 0.000269
Train loss: 0.6494;  Loss pred: 0.6494; Loss self: 0.0000; time: 0.52s
Val loss: 0.6528 score: 0.7755 time: 0.43s
Test loss: 0.6471 score: 0.8980 time: 0.49s
Epoch 41/1000, LR 0.000269
Train loss: 0.6425;  Loss pred: 0.6425; Loss self: 0.0000; time: 0.52s
Val loss: 0.6489 score: 0.8367 time: 0.41s
Test loss: 0.6429 score: 0.9184 time: 0.60s
Epoch 42/1000, LR 0.000269
Train loss: 0.6386;  Loss pred: 0.6386; Loss self: 0.0000; time: 0.52s
Val loss: 0.6448 score: 0.8367 time: 0.42s
Test loss: 0.6383 score: 0.9184 time: 0.53s
Epoch 43/1000, LR 0.000269
Train loss: 0.6310;  Loss pred: 0.6310; Loss self: 0.0000; time: 0.52s
Val loss: 0.6405 score: 0.8571 time: 0.51s
Test loss: 0.6335 score: 0.9184 time: 0.49s
Epoch 44/1000, LR 0.000269
Train loss: 0.6254;  Loss pred: 0.6254; Loss self: 0.0000; time: 0.64s
Val loss: 0.6358 score: 0.8571 time: 0.51s
Test loss: 0.6283 score: 0.9388 time: 0.48s
Epoch 45/1000, LR 0.000269
Train loss: 0.6193;  Loss pred: 0.6193; Loss self: 0.0000; time: 0.61s
Val loss: 0.6308 score: 0.8776 time: 0.41s
Test loss: 0.6226 score: 0.9388 time: 0.50s
Epoch 46/1000, LR 0.000269
Train loss: 0.6119;  Loss pred: 0.6119; Loss self: 0.0000; time: 0.52s
Val loss: 0.6255 score: 0.8776 time: 0.41s
Test loss: 0.6165 score: 0.9388 time: 0.50s
Epoch 47/1000, LR 0.000269
Train loss: 0.6037;  Loss pred: 0.6037; Loss self: 0.0000; time: 0.64s
Val loss: 0.6198 score: 0.8776 time: 0.42s
Test loss: 0.6099 score: 0.9388 time: 0.49s
Epoch 48/1000, LR 0.000269
Train loss: 0.5930;  Loss pred: 0.5930; Loss self: 0.0000; time: 0.52s
Val loss: 0.6137 score: 0.8776 time: 0.53s
Test loss: 0.6027 score: 0.9388 time: 0.53s
Epoch 49/1000, LR 0.000269
Train loss: 0.5846;  Loss pred: 0.5846; Loss self: 0.0000; time: 0.59s
Val loss: 0.6073 score: 0.8776 time: 0.40s
Test loss: 0.5951 score: 0.9388 time: 0.48s
Epoch 50/1000, LR 0.000269
Train loss: 0.5775;  Loss pred: 0.5775; Loss self: 0.0000; time: 0.52s
Val loss: 0.6004 score: 0.8776 time: 0.51s
Test loss: 0.5869 score: 0.9388 time: 0.52s
Epoch 51/1000, LR 0.000269
Train loss: 0.5659;  Loss pred: 0.5659; Loss self: 0.0000; time: 0.52s
Val loss: 0.5931 score: 0.8776 time: 0.47s
Test loss: 0.5782 score: 0.9388 time: 0.49s
Epoch 52/1000, LR 0.000269
Train loss: 0.5573;  Loss pred: 0.5573; Loss self: 0.0000; time: 0.61s
Val loss: 0.5854 score: 0.8776 time: 0.54s
Test loss: 0.5690 score: 0.9388 time: 0.48s
Epoch 53/1000, LR 0.000269
Train loss: 0.5407;  Loss pred: 0.5407; Loss self: 0.0000; time: 0.51s
Val loss: 0.5771 score: 0.8776 time: 0.41s
Test loss: 0.5592 score: 0.9388 time: 0.61s
Epoch 54/1000, LR 0.000269
Train loss: 0.5338;  Loss pred: 0.5338; Loss self: 0.0000; time: 0.61s
Val loss: 0.5684 score: 0.8776 time: 0.41s
Test loss: 0.5487 score: 0.9388 time: 0.51s
Epoch 55/1000, LR 0.000269
Train loss: 0.5183;  Loss pred: 0.5183; Loss self: 0.0000; time: 0.52s
Val loss: 0.5591 score: 0.8571 time: 0.41s
Test loss: 0.5375 score: 0.9388 time: 0.59s
Epoch 56/1000, LR 0.000269
Train loss: 0.5149;  Loss pred: 0.5149; Loss self: 0.0000; time: 0.52s
Val loss: 0.5495 score: 0.8571 time: 0.43s
Test loss: 0.5259 score: 0.9388 time: 0.49s
Epoch 57/1000, LR 0.000269
Train loss: 0.4939;  Loss pred: 0.4939; Loss self: 0.0000; time: 0.56s
Val loss: 0.5394 score: 0.8571 time: 0.51s
Test loss: 0.5137 score: 0.9388 time: 0.47s
Epoch 58/1000, LR 0.000269
Train loss: 0.4819;  Loss pred: 0.4819; Loss self: 0.0000; time: 0.53s
Val loss: 0.5290 score: 0.8571 time: 0.40s
Test loss: 0.5010 score: 0.9592 time: 0.51s
Epoch 59/1000, LR 0.000268
Train loss: 0.4689;  Loss pred: 0.4689; Loss self: 0.0000; time: 0.61s
Val loss: 0.5183 score: 0.8571 time: 0.41s
Test loss: 0.4878 score: 0.9592 time: 0.49s
Epoch 60/1000, LR 0.000268
Train loss: 0.4494;  Loss pred: 0.4494; Loss self: 0.0000; time: 0.52s
Val loss: 0.5073 score: 0.8571 time: 0.45s
Test loss: 0.4742 score: 0.9592 time: 0.47s
Epoch 61/1000, LR 0.000268
Train loss: 0.4338;  Loss pred: 0.4338; Loss self: 0.0000; time: 0.60s
Val loss: 0.4961 score: 0.8571 time: 3.64s
Test loss: 0.4602 score: 0.9592 time: 2.29s
Epoch 62/1000, LR 0.000268
Train loss: 0.4184;  Loss pred: 0.4184; Loss self: 0.0000; time: 0.52s
Val loss: 0.4849 score: 0.8571 time: 0.50s
Test loss: 0.4457 score: 0.9592 time: 0.52s
Epoch 63/1000, LR 0.000268
Train loss: 0.3956;  Loss pred: 0.3956; Loss self: 0.0000; time: 0.50s
Val loss: 0.4737 score: 0.8571 time: 0.41s
Test loss: 0.4312 score: 0.9592 time: 0.49s
Epoch 64/1000, LR 0.000268
Train loss: 0.3842;  Loss pred: 0.3842; Loss self: 0.0000; time: 0.52s
Val loss: 0.4625 score: 0.8571 time: 0.53s
Test loss: 0.4167 score: 0.9592 time: 0.49s
Epoch 65/1000, LR 0.000268
Train loss: 0.3597;  Loss pred: 0.3597; Loss self: 0.0000; time: 0.51s
Val loss: 0.4515 score: 0.8571 time: 0.42s
Test loss: 0.4024 score: 0.9592 time: 0.48s
Epoch 66/1000, LR 0.000268
Train loss: 0.3467;  Loss pred: 0.3467; Loss self: 0.0000; time: 0.63s
Val loss: 0.4409 score: 0.8571 time: 0.41s
Test loss: 0.3885 score: 0.9592 time: 0.52s
Epoch 67/1000, LR 0.000268
Train loss: 0.3268;  Loss pred: 0.3268; Loss self: 0.0000; time: 0.52s
Val loss: 0.4305 score: 0.8571 time: 0.41s
Test loss: 0.3750 score: 0.9388 time: 0.60s
Epoch 68/1000, LR 0.000268
Train loss: 0.3109;  Loss pred: 0.3109; Loss self: 0.0000; time: 0.51s
Val loss: 0.4207 score: 0.8571 time: 0.41s
Test loss: 0.3622 score: 0.9388 time: 0.49s
Epoch 69/1000, LR 0.000268
Train loss: 0.2969;  Loss pred: 0.2969; Loss self: 0.0000; time: 0.52s
Val loss: 0.4112 score: 0.8571 time: 0.44s
Test loss: 0.3495 score: 0.9388 time: 0.60s
Epoch 70/1000, LR 0.000268
Train loss: 0.2862;  Loss pred: 0.2862; Loss self: 0.0000; time: 0.53s
Val loss: 0.4024 score: 0.8367 time: 0.41s
Test loss: 0.3375 score: 0.9184 time: 0.49s
Epoch 71/1000, LR 0.000268
Train loss: 0.2701;  Loss pred: 0.2701; Loss self: 0.0000; time: 0.56s
Val loss: 0.3942 score: 0.8367 time: 0.54s
Test loss: 0.3262 score: 0.9184 time: 0.50s
Epoch 72/1000, LR 0.000267
Train loss: 0.2474;  Loss pred: 0.2474; Loss self: 0.0000; time: 0.55s
Val loss: 0.3866 score: 0.8367 time: 0.42s
Test loss: 0.3148 score: 0.9184 time: 0.54s
Epoch 73/1000, LR 0.000267
Train loss: 0.2368;  Loss pred: 0.2368; Loss self: 0.0000; time: 0.64s
Val loss: 0.3796 score: 0.8367 time: 0.43s
Test loss: 0.3040 score: 0.9184 time: 0.51s
Epoch 74/1000, LR 0.000267
Train loss: 0.2219;  Loss pred: 0.2219; Loss self: 0.0000; time: 0.53s
Val loss: 0.3732 score: 0.8367 time: 0.42s
Test loss: 0.2934 score: 0.9184 time: 0.49s
Epoch 75/1000, LR 0.000267
Train loss: 0.2021;  Loss pred: 0.2021; Loss self: 0.0000; time: 0.62s
Val loss: 0.3673 score: 0.8367 time: 0.43s
Test loss: 0.2833 score: 0.9184 time: 0.52s
Epoch 76/1000, LR 0.000267
Train loss: 0.1945;  Loss pred: 0.1945; Loss self: 0.0000; time: 0.52s
Val loss: 0.3619 score: 0.8367 time: 0.53s
Test loss: 0.2734 score: 0.9184 time: 0.52s
Epoch 77/1000, LR 0.000267
Train loss: 0.1792;  Loss pred: 0.1792; Loss self: 0.0000; time: 0.63s
Val loss: 0.3573 score: 0.8367 time: 0.42s
Test loss: 0.2646 score: 0.9184 time: 0.48s
Epoch 78/1000, LR 0.000267
Train loss: 0.1624;  Loss pred: 0.1624; Loss self: 0.0000; time: 0.51s
Val loss: 0.3533 score: 0.8367 time: 0.50s
Test loss: 0.2563 score: 0.9184 time: 0.48s
Epoch 79/1000, LR 0.000267
Train loss: 0.1608;  Loss pred: 0.1608; Loss self: 0.0000; time: 0.50s
Val loss: 0.3495 score: 0.8367 time: 0.41s
Test loss: 0.2479 score: 0.9184 time: 0.47s
Epoch 80/1000, LR 0.000267
Train loss: 0.1496;  Loss pred: 0.1496; Loss self: 0.0000; time: 0.62s
Val loss: 0.3466 score: 0.8367 time: 0.41s
Test loss: 0.2406 score: 0.9184 time: 0.48s
Epoch 81/1000, LR 0.000267
Train loss: 0.1402;  Loss pred: 0.1402; Loss self: 0.0000; time: 0.51s
Val loss: 0.3450 score: 0.8367 time: 0.40s
Test loss: 0.2352 score: 0.9184 time: 0.58s
Epoch 82/1000, LR 0.000267
Train loss: 0.1263;  Loss pred: 0.1263; Loss self: 0.0000; time: 0.50s
Val loss: 0.3441 score: 0.8367 time: 0.40s
Test loss: 0.2303 score: 0.9184 time: 0.59s
Epoch 83/1000, LR 0.000266
Train loss: 0.1089;  Loss pred: 0.1089; Loss self: 0.0000; time: 0.68s
Val loss: 0.3442 score: 0.8367 time: 0.52s
Test loss: 0.2263 score: 0.9184 time: 0.63s
     INFO: Early stopping counter 1 of 20
Epoch 84/1000, LR 0.000266
Train loss: 0.1095;  Loss pred: 0.1095; Loss self: 0.0000; time: 0.52s
Val loss: 0.3448 score: 0.8367 time: 0.42s
Test loss: 0.2228 score: 0.9184 time: 0.49s
     INFO: Early stopping counter 2 of 20
Epoch 85/1000, LR 0.000266
Train loss: 0.0976;  Loss pred: 0.0976; Loss self: 0.0000; time: 0.55s
Val loss: 0.3463 score: 0.8367 time: 0.52s
Test loss: 0.2201 score: 0.9184 time: 0.48s
     INFO: Early stopping counter 3 of 20
Epoch 86/1000, LR 0.000266
Train loss: 0.0971;  Loss pred: 0.0971; Loss self: 0.0000; time: 0.55s
Val loss: 0.3498 score: 0.8367 time: 0.51s
Test loss: 0.2194 score: 0.9184 time: 0.63s
     INFO: Early stopping counter 4 of 20
Epoch 87/1000, LR 0.000266
Train loss: 0.0794;  Loss pred: 0.0794; Loss self: 0.0000; time: 0.65s
Val loss: 0.3535 score: 0.8367 time: 0.40s
Test loss: 0.2192 score: 0.9184 time: 0.49s
     INFO: Early stopping counter 5 of 20
Epoch 88/1000, LR 0.000266
Train loss: 0.0773;  Loss pred: 0.0773; Loss self: 0.0000; time: 0.52s
Val loss: 0.3578 score: 0.8367 time: 0.43s
Test loss: 0.2193 score: 0.9184 time: 0.58s
     INFO: Early stopping counter 6 of 20
Epoch 89/1000, LR 0.000266
Train loss: 0.0699;  Loss pred: 0.0699; Loss self: 0.0000; time: 0.65s
Val loss: 0.3626 score: 0.8367 time: 0.41s
Test loss: 0.2202 score: 0.9184 time: 0.48s
     INFO: Early stopping counter 7 of 20
Epoch 90/1000, LR 0.000266
Train loss: 0.0610;  Loss pred: 0.0610; Loss self: 0.0000; time: 0.55s
Val loss: 0.3678 score: 0.8367 time: 0.53s
Test loss: 0.2212 score: 0.9184 time: 0.61s
     INFO: Early stopping counter 8 of 20
Epoch 91/1000, LR 0.000266
Train loss: 0.0573;  Loss pred: 0.0573; Loss self: 0.0000; time: 0.63s
Val loss: 0.3724 score: 0.8367 time: 0.51s
Test loss: 0.2217 score: 0.9184 time: 0.66s
     INFO: Early stopping counter 9 of 20
Epoch 92/1000, LR 0.000266
Train loss: 0.0621;  Loss pred: 0.0621; Loss self: 0.0000; time: 0.64s
Val loss: 0.3789 score: 0.8367 time: 0.62s
Test loss: 0.2242 score: 0.9184 time: 0.62s
     INFO: Early stopping counter 10 of 20
Epoch 93/1000, LR 0.000265
Train loss: 0.0620;  Loss pred: 0.0620; Loss self: 0.0000; time: 0.66s
Val loss: 0.3836 score: 0.8367 time: 0.51s
Test loss: 0.2249 score: 0.9184 time: 0.61s
     INFO: Early stopping counter 11 of 20
Epoch 94/1000, LR 0.000265
Train loss: 0.0473;  Loss pred: 0.0473; Loss self: 0.0000; time: 0.63s
Val loss: 0.3879 score: 0.8367 time: 0.41s
Test loss: 0.2255 score: 0.9184 time: 0.58s
     INFO: Early stopping counter 12 of 20
Epoch 95/1000, LR 0.000265
Train loss: 0.0432;  Loss pred: 0.0432; Loss self: 0.0000; time: 0.51s
Val loss: 0.3917 score: 0.8367 time: 0.43s
Test loss: 0.2257 score: 0.9184 time: 0.60s
     INFO: Early stopping counter 13 of 20
Epoch 96/1000, LR 0.000265
Train loss: 0.0403;  Loss pred: 0.0403; Loss self: 0.0000; time: 0.56s
Val loss: 0.3952 score: 0.8367 time: 0.46s
Test loss: 0.2261 score: 0.9184 time: 0.51s
     INFO: Early stopping counter 14 of 20
Epoch 97/1000, LR 0.000265
Train loss: 0.0362;  Loss pred: 0.0362; Loss self: 0.0000; time: 0.52s
Val loss: 0.3984 score: 0.8367 time: 0.44s
Test loss: 0.2265 score: 0.9184 time: 0.61s
     INFO: Early stopping counter 15 of 20
Epoch 98/1000, LR 0.000265
Train loss: 0.0349;  Loss pred: 0.0349; Loss self: 0.0000; time: 0.56s
Val loss: 0.4014 score: 0.8367 time: 0.42s
Test loss: 0.2271 score: 0.9184 time: 0.50s
     INFO: Early stopping counter 16 of 20
Epoch 99/1000, LR 0.000265
Train loss: 0.0338;  Loss pred: 0.0338; Loss self: 0.0000; time: 0.53s
Val loss: 0.4045 score: 0.8367 time: 0.53s
Test loss: 0.2282 score: 0.9184 time: 0.54s
     INFO: Early stopping counter 17 of 20
Epoch 100/1000, LR 0.000265
Train loss: 0.0340;  Loss pred: 0.0340; Loss self: 0.0000; time: 0.53s
Val loss: 0.4067 score: 0.8367 time: 0.43s
Test loss: 0.2291 score: 0.9184 time: 0.50s
     INFO: Early stopping counter 18 of 20
Epoch 101/1000, LR 0.000265
Train loss: 0.0256;  Loss pred: 0.0256; Loss self: 0.0000; time: 0.64s
Val loss: 0.4091 score: 0.8367 time: 0.45s
Test loss: 0.2305 score: 0.9184 time: 0.51s
     INFO: Early stopping counter 19 of 20
Epoch 102/1000, LR 0.000264
Train loss: 0.0261;  Loss pred: 0.0261; Loss self: 0.0000; time: 0.53s
Val loss: 0.4114 score: 0.8367 time: 0.42s
Test loss: 0.2322 score: 0.9184 time: 0.55s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 081,   Train_Loss: 0.1263,   Val_Loss: 0.3441,   Val_Precision: 0.9000,   Val_Recall: 0.7500,   Val_accuracy: 0.8182,   Val_Score: 0.8367,   Val_Loss: 0.3441,   Test_Precision: 0.9565,   Test_Recall: 0.8800,   Test_accuracy: 0.9167,   Test_Score: 0.9184,   Test_loss: 0.2303


[0.4864854048937559, 0.49113618093542755, 0.4953845548443496, 0.5020810109563172, 0.6105500708799809, 0.6348829148337245, 0.5343367431778461, 0.5420319230761379, 0.5006377091631293, 0.6278654281049967, 0.4935796959325671, 0.5048661360051483, 0.5387552010361105, 0.5011575769167393, 0.48138835304416716, 0.5058298499789089, 0.6189252841286361, 0.5675822379998863, 0.5045047800522298, 0.5167771831620485, 0.5274420049972832, 0.5973227471113205, 0.5289010058622807, 0.5005101580172777, 0.4971687709912658, 0.49798980192281306, 0.5915795790497214, 0.7225857749581337, 0.507525707129389, 0.6121962869074196, 0.5273409918881953, 0.5514915038365871, 0.5755820339545608, 0.533705092035234, 0.5312240819912404, 0.5686415950767696, 0.4983961330726743, 0.5102405559737235, 0.5970565949101001, 0.49013955518603325, 0.6013896730728447, 0.5341293341480196, 0.49669445399194956, 0.489765761885792, 0.5007811170071363, 0.5026739991735667, 0.49545597890391946, 0.5373096771072596, 0.4870557989925146, 0.526107651880011, 0.4926560411695391, 0.48045027116313577, 0.61098856292665, 0.5133866069372743, 0.5947287168819457, 0.49296367494389415, 0.47831373708322644, 0.5191884539090097, 0.4918215519282967, 0.47907064110040665, 2.2969875768758357, 0.5215836688876152, 0.4966632251162082, 0.49362886720336974, 0.4835226060822606, 0.5234909809660167, 0.6037856419570744, 0.4927330380305648, 0.6021768630016595, 0.4938877490349114, 0.5004533131141216, 0.5501459510996938, 0.5184926339425147, 0.4998858200851828, 0.5293831909075379, 0.5204927180893719, 0.4875845620408654, 0.4864306871313602, 0.4761741200927645, 0.48746123211458325, 0.5891961390152574, 0.5962004538159817, 0.6400923039764166, 0.491470105946064, 0.48954105214215815, 0.6366601812187582, 0.49966930295340717, 0.5844131580088288, 0.4863780119922012, 0.6167535160202533, 0.6714696460403502, 0.6242299238219857, 0.6128077402245253, 0.5806076929438859, 0.6066815480589867, 0.5140152180101722, 0.610620005056262, 0.5063326428644359, 0.540864926064387, 0.5032452151644975, 0.511703854193911, 0.5517726959660649]
[0.009928273569260324, 0.010023187366029133, 0.010109888874374482, 0.010246551244006473, 0.012460205528162877, 0.012956794180280092, 0.01090483149342543, 0.011061875981145672, 0.010217096105369985, 0.012813580165408095, 0.010073055019031982, 0.010303390530717313, 0.010995004102777766, 0.010227705651362027, 0.009824252102942186, 0.010323058162834875, 0.012631128247523186, 0.011583310979589517, 0.01029601591943326, 0.01054647312575609, 0.010764122550964964, 0.01219026014512899, 0.010793898078822056, 0.01021449302076077, 0.010146301448801342, 0.010163057182098225, 0.012073052633667782, 0.01474664846853334, 0.010357667492436511, 0.012493801773620807, 0.01076206105894276, 0.011254928649726267, 0.011746572121521649, 0.010891940653780284, 0.010841307795739599, 0.011604930511770807, 0.010171349654544373, 0.010413072570892317, 0.012184828467553064, 0.010002848065021087, 0.012273258634139689, 0.010900598656082032, 0.010136621510039787, 0.009995219630322285, 0.010220022796064007, 0.010258653044358504, 0.010111346508243255, 0.01096550361443387, 0.009939914265153359, 0.010736890854694101, 0.010054204921827329, 0.009805107574757874, 0.012469154345441838, 0.010477277692597436, 0.01213732075269277, 0.010060483162120288, 0.009761504838433193, 0.010595682732836932, 0.010037174529148912, 0.009776951859191972, 0.04687729748726195, 0.01064456467117582, 0.010135984186045065, 0.010074058514354485, 0.009867808287393073, 0.010683489407469727, 0.012322155958307641, 0.010055776286338056, 0.012289323734727745, 0.010079341817039008, 0.010213332920696358, 0.011227468389789668, 0.010581482325357442, 0.010201751430309852, 0.010803738589949754, 0.010622300369170855, 0.009950705347772764, 0.00992715688023184, 0.009717839185566622, 0.009948188410501699, 0.012024411000311375, 0.012167356200326157, 0.013063108244416664, 0.010030002162164571, 0.0099906337171869, 0.012993064922831801, 0.01019733271333484, 0.011926799143037322, 0.009926081877391862, 0.012586806449392925, 0.01370346216408878, 0.012739386200448689, 0.012506280412745416, 0.01184913659069155, 0.012381256082836462, 0.010490106490003516, 0.012461632756250245, 0.010333319242131345, 0.011038059715599733, 0.010270310513561174, 0.010442935799875734, 0.01126066726461357]
[100.72244615581256, 99.76866274985818, 98.913055566288, 97.59381241419459, 80.25549801243442, 77.17958517254016, 91.70247156986372, 90.40057958563649, 97.87516821677067, 78.04220109377614, 99.27474813853442, 97.05543015366815, 90.95039807646467, 97.77363898489097, 101.78891884304537, 96.87051881584905, 79.16949146613946, 86.33110185525189, 97.12494695278643, 94.81842774129372, 92.90120911066306, 82.03270382212328, 92.64493630545107, 97.900110947016, 98.55808099592166, 98.39558924862251, 82.82909305069441, 67.8120185840069, 96.54683361193347, 80.03968832860646, 92.9190045032357, 88.84996352458629, 85.13121867849735, 91.81100336356756, 92.23979420573028, 86.17027038513555, 98.31536953930379, 96.03313461919991, 82.06927185416656, 99.97152745895396, 81.4779538026167, 91.73808077431106, 98.65219876361695, 100.04782655964068, 97.84713986989595, 97.478684158241, 98.89879643475298, 91.19508188239544, 100.60448946785472, 93.1368320245899, 99.46087311479347, 101.9876622847449, 80.19790053890503, 95.44464023384005, 82.39050614017441, 99.39880459868945, 102.44322126059703, 94.37806182143544, 99.62963153583756, 102.28136687201058, 21.332287772598914, 93.94465916561883, 98.65840175409622, 99.26485920000407, 101.33962586986831, 93.60237670108192, 81.1546293833261, 99.44533087501324, 81.37144252894512, 99.21282740005026, 97.91123110983625, 89.06727369719485, 94.50471769948545, 98.02238437499624, 92.56055130121867, 94.1415668212795, 100.49538852276709, 100.73377625282838, 102.9035345105572, 100.5208143167414, 83.16415664551926, 82.18712294896027, 76.55145936859341, 99.7008758155831, 100.09375063762958, 76.96413478568634, 98.06485951883484, 83.84479255557719, 100.74468580373589, 79.44827022014239, 72.97425920732607, 78.49671752354752, 79.95982554340289, 84.39433475562942, 80.76724956737239, 95.32791692371704, 80.24630636771421, 96.77432551611949, 90.59563236342454, 97.36803952320382, 95.75851266000309, 88.80468417200113]
Elapsed: 0.5537296048771846~0.1811426776057912
Time per graph: 0.01130060418116703~0.0036967893388936976
Speed: 91.44991438747735~10.799096667400642
Total Time: 0.5523
best val loss: 0.344147264957428 test_score: 0.9184

Testing...
Test loss: 0.6226 score: 0.9388 time: 0.65s
test Score 0.9388
Epoch Time List: [1.4255824720021337, 1.5222562558483332, 1.4426324090454727, 1.5320483681280166, 1.55734296515584, 1.903188626980409, 1.7200794769451022, 1.653433856088668, 1.5390314583200961, 1.640339040895924, 1.4589809651952237, 1.5963708308991045, 1.4823978361673653, 1.5319831580854952, 1.444203456165269, 1.5459866570308805, 1.7201996040530503, 1.5289458658080548, 1.5466622998937964, 1.5089990419801325, 1.5711643958929926, 1.5753722609952092, 1.5588925208430737, 1.5579999298788607, 1.455913363955915, 1.5259987330064178, 1.558732729172334, 1.6577531530056149, 1.5436185549478978, 1.6461542018223554, 1.6279381667263806, 1.5393257848918438, 1.7408441409934312, 1.6649830632377416, 1.5234598109964281, 1.6733914173673838, 1.4774036633316427, 1.5350412828847766, 1.5347840518224984, 1.428236443316564, 1.5300076438579708, 1.4614985301159322, 1.518954827915877, 1.630490595009178, 1.5179133960045874, 1.4266291388776153, 1.547051940113306, 1.5782396546564996, 1.4756837890017778, 1.551181532908231, 1.4755929179955274, 1.6319930718746036, 1.5317248387727886, 1.524037712952122, 1.5191255428362638, 1.4380068997852504, 1.54425815702416, 1.4389671250246465, 1.503510129172355, 1.443460094043985, 6.540724294260144, 1.5373938435222954, 1.4058926939032972, 1.5341839049942791, 1.4087467570789158, 1.5676907000597566, 1.5248551708646119, 1.412041988922283, 1.5550116691738367, 1.4350474560633302, 1.5916957091540098, 1.515143149998039, 1.5812988481484354, 1.4471317548304796, 1.5800200041849166, 1.5642805979587138, 1.5310444151982665, 1.4868465987965465, 1.385019610170275, 1.5029891040176153, 1.4975734760519117, 1.4922171228099614, 1.8303703931160271, 1.427939127665013, 1.5526115843094885, 1.69113396294415, 1.552649380872026, 1.5259701379109174, 1.5351306069642305, 1.6901484613772482, 1.8062090743333101, 1.8815599018707871, 1.782329934882, 1.6197480051778257, 1.5437549790367484, 1.531954019330442, 1.5607598840724677, 1.4875124949030578, 1.600602338090539, 1.4565821902360767, 1.5939592670183629, 1.4979691519401968]
Total Epoch List: [102]
Total Time List: [0.552347867982462]
========================k_idx:1========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: FAGNN
FAGNN(
  (conv1): NeighborFilterConv()
  (conv2): NeighborFilterConv()
  (ffn): Sequential(
    (0): Linear(in_features=14951, out_features=64, bias=True)
    (1): ReLU()
    (2): Linear(in_features=64, out_features=64, bias=True)
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=64, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=64, out_features=2, bias=True)
  )
)
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7ed848759fc0>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.6996;  Loss pred: 0.6996; Loss self: 0.0000; time: 0.42s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6973 score: 0.5102 time: 0.75s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7019 score: 0.4898 time: 0.44s
Epoch 2/1000, LR 0.000000
Train loss: 0.6996;  Loss pred: 0.6996; Loss self: 0.0000; time: 0.45s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6972 score: 0.5102 time: 0.67s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7018 score: 0.4898 time: 0.45s
Epoch 3/1000, LR 0.000030
Train loss: 0.6995;  Loss pred: 0.6995; Loss self: 0.0000; time: 0.45s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6972 score: 0.5102 time: 0.58s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7018 score: 0.4898 time: 0.51s
Epoch 4/1000, LR 0.000060
Train loss: 0.6995;  Loss pred: 0.6995; Loss self: 0.0000; time: 0.55s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6971 score: 0.5102 time: 0.57s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7016 score: 0.4898 time: 0.44s
Epoch 5/1000, LR 0.000090
Train loss: 0.6993;  Loss pred: 0.6993; Loss self: 0.0000; time: 0.43s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6970 score: 0.5102 time: 0.67s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7015 score: 0.4898 time: 0.44s
Epoch 6/1000, LR 0.000120
Train loss: 0.6992;  Loss pred: 0.6992; Loss self: 0.0000; time: 0.44s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6968 score: 0.5102 time: 0.59s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7012 score: 0.4898 time: 0.50s
Epoch 7/1000, LR 0.000150
Train loss: 0.6990;  Loss pred: 0.6990; Loss self: 0.0000; time: 0.48s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6967 score: 0.5102 time: 0.66s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7010 score: 0.4898 time: 0.44s
Epoch 8/1000, LR 0.000180
Train loss: 0.6988;  Loss pred: 0.6988; Loss self: 0.0000; time: 0.45s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6965 score: 0.5102 time: 0.64s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7007 score: 0.4898 time: 0.42s
Epoch 9/1000, LR 0.000210
Train loss: 0.6986;  Loss pred: 0.6986; Loss self: 0.0000; time: 0.56s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6963 score: 0.5102 time: 0.59s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7004 score: 0.4898 time: 0.46s
Epoch 10/1000, LR 0.000240
Train loss: 0.6983;  Loss pred: 0.6983; Loss self: 0.0000; time: 0.44s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6960 score: 0.5102 time: 0.65s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7001 score: 0.4898 time: 0.44s
Epoch 11/1000, LR 0.000270
Train loss: 0.6981;  Loss pred: 0.6981; Loss self: 0.0000; time: 0.44s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6958 score: 0.5102 time: 0.56s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6998 score: 0.4898 time: 0.59s
Epoch 12/1000, LR 0.000270
Train loss: 0.6978;  Loss pred: 0.6978; Loss self: 0.0000; time: 0.44s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6956 score: 0.5102 time: 0.66s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6995 score: 0.4898 time: 0.43s
Epoch 13/1000, LR 0.000270
Train loss: 0.6975;  Loss pred: 0.6975; Loss self: 0.0000; time: 0.44s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6953 score: 0.5102 time: 0.55s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6991 score: 0.4898 time: 0.45s
Epoch 14/1000, LR 0.000270
Train loss: 0.6972;  Loss pred: 0.6972; Loss self: 0.0000; time: 0.54s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6951 score: 0.5102 time: 0.59s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6988 score: 0.4898 time: 0.46s
Epoch 15/1000, LR 0.000270
Train loss: 0.6969;  Loss pred: 0.6969; Loss self: 0.0000; time: 0.44s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6948 score: 0.5102 time: 0.67s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6985 score: 0.4898 time: 0.44s
Epoch 16/1000, LR 0.000270
Train loss: 0.6966;  Loss pred: 0.6966; Loss self: 0.0000; time: 0.44s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6945 score: 0.5102 time: 0.55s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6981 score: 0.4898 time: 0.48s
Epoch 17/1000, LR 0.000270
Train loss: 0.6963;  Loss pred: 0.6963; Loss self: 0.0000; time: 0.43s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6943 score: 0.5102 time: 0.68s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6978 score: 0.4898 time: 0.44s
Epoch 18/1000, LR 0.000270
Train loss: 0.6961;  Loss pred: 0.6961; Loss self: 0.0000; time: 0.44s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6939 score: 0.5102 time: 0.58s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6974 score: 0.4898 time: 0.54s
Epoch 19/1000, LR 0.000270
Train loss: 0.6957;  Loss pred: 0.6957; Loss self: 0.0000; time: 0.55s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6936 score: 0.5102 time: 0.56s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6970 score: 0.4898 time: 0.45s
Epoch 20/1000, LR 0.000270
Train loss: 0.6954;  Loss pred: 0.6954; Loss self: 0.0000; time: 0.46s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6932 score: 0.5102 time: 0.68s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6966 score: 0.4898 time: 0.43s
Epoch 21/1000, LR 0.000270
Train loss: 0.6950;  Loss pred: 0.6950; Loss self: 0.0000; time: 0.44s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6928 score: 0.5102 time: 0.57s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6961 score: 0.4898 time: 0.46s
Epoch 22/1000, LR 0.000270
Train loss: 0.6946;  Loss pred: 0.6946; Loss self: 0.0000; time: 0.53s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6924 score: 0.5102 time: 0.65s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6957 score: 0.4898 time: 0.45s
Epoch 23/1000, LR 0.000270
Train loss: 0.6941;  Loss pred: 0.6941; Loss self: 0.0000; time: 0.44s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6919 score: 0.5102 time: 0.58s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6952 score: 0.4898 time: 0.44s
Epoch 24/1000, LR 0.000270
Train loss: 0.6936;  Loss pred: 0.6936; Loss self: 0.0000; time: 0.53s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6914 score: 0.5102 time: 0.54s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6947 score: 0.4898 time: 0.45s
Epoch 25/1000, LR 0.000270
Train loss: 0.6931;  Loss pred: 0.6931; Loss self: 0.0000; time: 0.43s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6908 score: 0.5102 time: 0.65s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6941 score: 0.4898 time: 0.44s
Epoch 26/1000, LR 0.000270
Train loss: 0.6927;  Loss pred: 0.6927; Loss self: 0.0000; time: 0.42s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6902 score: 0.5102 time: 0.54s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6935 score: 0.4898 time: 0.43s
Epoch 27/1000, LR 0.000270
Train loss: 0.6920;  Loss pred: 0.6920; Loss self: 0.0000; time: 0.47s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6895 score: 0.5102 time: 0.64s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6928 score: 0.4898 time: 0.43s
Epoch 28/1000, LR 0.000270
Train loss: 0.6915;  Loss pred: 0.6915; Loss self: 0.0000; time: 0.42s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6887 score: 0.5102 time: 0.54s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6920 score: 0.4898 time: 0.44s
Epoch 29/1000, LR 0.000270
Train loss: 0.6905;  Loss pred: 0.6905; Loss self: 0.0000; time: 0.66s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6878 score: 0.5102 time: 0.54s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6912 score: 0.4898 time: 3.77s
Epoch 30/1000, LR 0.000270
Train loss: 0.6899;  Loss pred: 0.6899; Loss self: 0.0000; time: 1.79s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6868 score: 0.5102 time: 0.64s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6902 score: 0.4898 time: 0.43s
Epoch 31/1000, LR 0.000270
Train loss: 0.6894;  Loss pred: 0.6894; Loss self: 0.0000; time: 0.44s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6857 score: 0.5102 time: 0.58s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6891 score: 0.4898 time: 0.43s
Epoch 32/1000, LR 0.000270
Train loss: 0.6881;  Loss pred: 0.6881; Loss self: 0.0000; time: 0.43s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6844 score: 0.5102 time: 0.66s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6879 score: 0.4898 time: 0.44s
Epoch 33/1000, LR 0.000270
Train loss: 0.6868;  Loss pred: 0.6868; Loss self: 0.0000; time: 0.46s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6830 score: 0.5102 time: 0.56s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6866 score: 0.4898 time: 0.44s
Epoch 34/1000, LR 0.000270
Train loss: 0.6856;  Loss pred: 0.6856; Loss self: 0.0000; time: 0.54s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6814 score: 0.5102 time: 0.56s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6851 score: 0.4898 time: 0.47s
Epoch 35/1000, LR 0.000270
Train loss: 0.6842;  Loss pred: 0.6842; Loss self: 0.0000; time: 0.43s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6796 score: 0.5102 time: 0.65s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6835 score: 0.4898 time: 0.44s
Epoch 36/1000, LR 0.000270
Train loss: 0.6824;  Loss pred: 0.6824; Loss self: 0.0000; time: 0.43s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6777 score: 0.5102 time: 0.58s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6817 score: 0.4898 time: 0.43s
Epoch 37/1000, LR 0.000270
Train loss: 0.6814;  Loss pred: 0.6814; Loss self: 0.0000; time: 0.43s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6755 score: 0.5102 time: 0.65s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6798 score: 0.4898 time: 0.45s
Epoch 38/1000, LR 0.000270
Train loss: 0.6786;  Loss pred: 0.6786; Loss self: 0.0000; time: 0.43s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6732 score: 0.5102 time: 0.62s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6778 score: 0.4898 time: 0.46s
Epoch 39/1000, LR 0.000269
Train loss: 0.6767;  Loss pred: 0.6767; Loss self: 0.0000; time: 0.58s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6706 score: 0.5102 time: 0.55s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6756 score: 0.4898 time: 0.48s
Epoch 40/1000, LR 0.000269
Train loss: 0.6753;  Loss pred: 0.6753; Loss self: 0.0000; time: 0.43s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6679 score: 0.5102 time: 0.64s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6732 score: 0.4898 time: 0.44s
Epoch 41/1000, LR 0.000269
Train loss: 0.6719;  Loss pred: 0.6719; Loss self: 0.0000; time: 0.44s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6650 score: 0.5102 time: 0.55s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6705 score: 0.4898 time: 0.46s
Epoch 42/1000, LR 0.000269
Train loss: 0.6692;  Loss pred: 0.6692; Loss self: 0.0000; time: 0.43s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6618 score: 0.5102 time: 0.64s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6675 score: 0.4898 time: 0.45s
Epoch 43/1000, LR 0.000269
Train loss: 0.6663;  Loss pred: 0.6663; Loss self: 0.0000; time: 0.43s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6583 score: 0.5102 time: 0.56s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6641 score: 0.4898 time: 0.44s
Epoch 44/1000, LR 0.000269
Train loss: 0.6636;  Loss pred: 0.6636; Loss self: 0.0000; time: 0.52s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6545 score: 0.5102 time: 0.56s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6605 score: 0.4898 time: 0.43s
Epoch 45/1000, LR 0.000269
Train loss: 0.6596;  Loss pred: 0.6596; Loss self: 0.0000; time: 0.43s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6504 score: 0.5102 time: 0.64s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6565 score: 0.4898 time: 0.43s
Epoch 46/1000, LR 0.000269
Train loss: 0.6543;  Loss pred: 0.6543; Loss self: 0.0000; time: 0.45s
Val loss: 0.6459 score: 0.5306 time: 0.55s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6522 score: 0.4898 time: 0.44s
Epoch 47/1000, LR 0.000269
Train loss: 0.6507;  Loss pred: 0.6507; Loss self: 0.0000; time: 0.43s
Val loss: 0.6411 score: 0.5510 time: 0.65s
Test loss: 0.6476 score: 0.5510 time: 0.44s
Epoch 48/1000, LR 0.000269
Train loss: 0.6467;  Loss pred: 0.6467; Loss self: 0.0000; time: 0.44s
Val loss: 0.6359 score: 0.6531 time: 0.55s
Test loss: 0.6427 score: 0.6735 time: 0.44s
Epoch 49/1000, LR 0.000269
Train loss: 0.6406;  Loss pred: 0.6406; Loss self: 0.0000; time: 0.54s
Val loss: 0.6303 score: 0.7143 time: 0.56s
Test loss: 0.6374 score: 0.7143 time: 0.45s
Epoch 50/1000, LR 0.000269
Train loss: 0.6351;  Loss pred: 0.6351; Loss self: 0.0000; time: 0.45s
Val loss: 0.6244 score: 0.7959 time: 0.65s
Test loss: 0.6318 score: 0.7347 time: 0.45s
Epoch 51/1000, LR 0.000269
Train loss: 0.6292;  Loss pred: 0.6292; Loss self: 0.0000; time: 0.44s
Val loss: 0.6180 score: 0.7959 time: 0.55s
Test loss: 0.6257 score: 0.7347 time: 0.45s
Epoch 52/1000, LR 0.000269
Train loss: 0.6252;  Loss pred: 0.6252; Loss self: 0.0000; time: 0.43s
Val loss: 0.6113 score: 0.7959 time: 0.67s
Test loss: 0.6193 score: 0.7959 time: 0.44s
Epoch 53/1000, LR 0.000269
Train loss: 0.6146;  Loss pred: 0.6146; Loss self: 0.0000; time: 0.47s
Val loss: 0.6043 score: 0.7755 time: 0.55s
Test loss: 0.6126 score: 0.7755 time: 0.44s
Epoch 54/1000, LR 0.000269
Train loss: 0.6097;  Loss pred: 0.6097; Loss self: 0.0000; time: 0.53s
Val loss: 0.5968 score: 0.7755 time: 0.56s
Test loss: 0.6054 score: 0.7755 time: 0.45s
Epoch 55/1000, LR 0.000269
Train loss: 0.6043;  Loss pred: 0.6043; Loss self: 0.0000; time: 0.53s
Val loss: 0.5887 score: 0.7755 time: 0.65s
Test loss: 0.5976 score: 0.7959 time: 0.44s
Epoch 56/1000, LR 0.000269
Train loss: 0.5985;  Loss pred: 0.5985; Loss self: 0.0000; time: 0.44s
Val loss: 0.5801 score: 0.7959 time: 0.60s
Test loss: 0.5894 score: 0.7959 time: 0.43s
Epoch 57/1000, LR 0.000269
Train loss: 0.5861;  Loss pred: 0.5861; Loss self: 0.0000; time: 0.43s
Val loss: 0.5708 score: 0.8367 time: 0.66s
Test loss: 0.5804 score: 0.7959 time: 0.45s
Epoch 58/1000, LR 0.000269
Train loss: 0.5810;  Loss pred: 0.5810; Loss self: 0.0000; time: 0.46s
Val loss: 0.5610 score: 0.8367 time: 0.68s
Test loss: 0.5711 score: 0.7959 time: 0.46s
Epoch 59/1000, LR 0.000268
Train loss: 0.5681;  Loss pred: 0.5681; Loss self: 0.0000; time: 0.55s
Val loss: 0.5507 score: 0.8367 time: 0.59s
Test loss: 0.5612 score: 0.7959 time: 0.46s
Epoch 60/1000, LR 0.000268
Train loss: 0.5587;  Loss pred: 0.5587; Loss self: 0.0000; time: 0.54s
Val loss: 0.5399 score: 0.8367 time: 0.64s
Test loss: 0.5509 score: 0.7959 time: 0.44s
Epoch 61/1000, LR 0.000268
Train loss: 0.5452;  Loss pred: 0.5452; Loss self: 0.0000; time: 0.45s
Val loss: 0.5286 score: 0.8367 time: 0.56s
Test loss: 0.5402 score: 0.8163 time: 0.44s
Epoch 62/1000, LR 0.000268
Train loss: 0.5343;  Loss pred: 0.5343; Loss self: 0.0000; time: 0.43s
Val loss: 0.5170 score: 0.8367 time: 0.65s
Test loss: 0.5292 score: 0.8367 time: 0.46s
Epoch 63/1000, LR 0.000268
Train loss: 0.5216;  Loss pred: 0.5216; Loss self: 0.0000; time: 0.43s
Val loss: 0.5050 score: 0.8367 time: 0.60s
Test loss: 0.5177 score: 0.8367 time: 0.44s
Epoch 64/1000, LR 0.000268
Train loss: 0.5152;  Loss pred: 0.5152; Loss self: 0.0000; time: 0.55s
Val loss: 0.4926 score: 0.8571 time: 0.56s
Test loss: 0.5059 score: 0.8571 time: 0.45s
Epoch 65/1000, LR 0.000268
Train loss: 0.4974;  Loss pred: 0.4974; Loss self: 0.0000; time: 0.47s
Val loss: 0.4797 score: 0.8571 time: 0.66s
Test loss: 0.4935 score: 0.8571 time: 0.45s
Epoch 66/1000, LR 0.000268
Train loss: 0.4898;  Loss pred: 0.4898; Loss self: 0.0000; time: 0.46s
Val loss: 0.4665 score: 0.8571 time: 0.56s
Test loss: 0.4809 score: 0.8571 time: 0.56s
Epoch 67/1000, LR 0.000268
Train loss: 0.4736;  Loss pred: 0.4736; Loss self: 0.0000; time: 0.54s
Val loss: 0.4531 score: 0.8776 time: 0.77s
Test loss: 0.4679 score: 0.8571 time: 0.45s
Epoch 68/1000, LR 0.000268
Train loss: 0.4462;  Loss pred: 0.4462; Loss self: 0.0000; time: 0.48s
Val loss: 0.4393 score: 0.8776 time: 0.74s
Test loss: 0.4547 score: 0.8776 time: 0.55s
Epoch 69/1000, LR 0.000268
Train loss: 0.4470;  Loss pred: 0.4470; Loss self: 0.0000; time: 0.65s
Val loss: 0.4251 score: 0.8776 time: 0.69s
Test loss: 0.4412 score: 0.8776 time: 0.58s
Epoch 70/1000, LR 0.000268
Train loss: 0.4191;  Loss pred: 0.4191; Loss self: 0.0000; time: 0.54s
Val loss: 0.4109 score: 0.8776 time: 0.78s
Test loss: 0.4277 score: 0.8776 time: 0.47s
Epoch 71/1000, LR 0.000268
Train loss: 0.4213;  Loss pred: 0.4213; Loss self: 0.0000; time: 0.47s
Val loss: 0.3969 score: 0.8776 time: 0.56s
Test loss: 0.4143 score: 0.8776 time: 0.44s
Epoch 72/1000, LR 0.000267
Train loss: 0.3957;  Loss pred: 0.3957; Loss self: 0.0000; time: 0.45s
Val loss: 0.3830 score: 0.8980 time: 0.66s
Test loss: 0.4010 score: 0.8776 time: 0.49s
Epoch 73/1000, LR 0.000267
Train loss: 0.3792;  Loss pred: 0.3792; Loss self: 0.0000; time: 0.45s
Val loss: 0.3690 score: 0.9184 time: 0.67s
Test loss: 0.3878 score: 0.8776 time: 0.45s
Epoch 74/1000, LR 0.000267
Train loss: 0.3548;  Loss pred: 0.3548; Loss self: 0.0000; time: 0.54s
Val loss: 0.3559 score: 0.9184 time: 0.58s
Test loss: 0.3752 score: 0.8776 time: 0.45s
Epoch 75/1000, LR 0.000267
Train loss: 0.3708;  Loss pred: 0.3708; Loss self: 0.0000; time: 0.44s
Val loss: 0.3435 score: 0.9184 time: 0.66s
Test loss: 0.3632 score: 0.8776 time: 0.43s
Epoch 76/1000, LR 0.000267
Train loss: 0.3374;  Loss pred: 0.3374; Loss self: 0.0000; time: 0.52s
Val loss: 0.3318 score: 0.9184 time: 0.58s
Test loss: 0.3519 score: 0.8776 time: 0.47s
Epoch 77/1000, LR 0.000267
Train loss: 0.3341;  Loss pred: 0.3341; Loss self: 0.0000; time: 0.46s
Val loss: 0.3201 score: 0.9184 time: 0.70s
Test loss: 0.3405 score: 0.8776 time: 0.68s
Epoch 78/1000, LR 0.000267
Train loss: 0.2991;  Loss pred: 0.2991; Loss self: 0.0000; time: 0.53s
Val loss: 0.3092 score: 0.9184 time: 0.55s
Test loss: 0.3299 score: 0.8776 time: 0.45s
Epoch 79/1000, LR 0.000267
Train loss: 0.2999;  Loss pred: 0.2999; Loss self: 0.0000; time: 0.53s
Val loss: 0.2987 score: 0.9184 time: 0.58s
Test loss: 0.3197 score: 0.8776 time: 0.43s
Epoch 80/1000, LR 0.000267
Train loss: 0.2732;  Loss pred: 0.2732; Loss self: 0.0000; time: 0.42s
Val loss: 0.2887 score: 0.9388 time: 0.65s
Test loss: 0.3100 score: 0.8776 time: 0.43s
Epoch 81/1000, LR 0.000267
Train loss: 0.2710;  Loss pred: 0.2710; Loss self: 0.0000; time: 0.43s
Val loss: 0.2802 score: 0.9388 time: 0.56s
Test loss: 0.3014 score: 0.8776 time: 0.44s
Epoch 82/1000, LR 0.000267
Train loss: 0.2441;  Loss pred: 0.2441; Loss self: 0.0000; time: 0.46s
Val loss: 0.2728 score: 0.9388 time: 0.66s
Test loss: 0.2937 score: 0.8776 time: 0.43s
Epoch 83/1000, LR 0.000266
Train loss: 0.2303;  Loss pred: 0.2303; Loss self: 0.0000; time: 0.52s
Val loss: 0.2668 score: 0.9388 time: 0.69s
Test loss: 0.2871 score: 0.8776 time: 0.55s
Epoch 84/1000, LR 0.000266
Train loss: 0.2202;  Loss pred: 0.2202; Loss self: 0.0000; time: 0.64s
Val loss: 0.2625 score: 0.9388 time: 0.67s
Test loss: 0.2818 score: 0.8776 time: 0.44s
Epoch 85/1000, LR 0.000266
Train loss: 0.2118;  Loss pred: 0.2118; Loss self: 0.0000; time: 0.43s
Val loss: 0.2593 score: 0.9388 time: 0.65s
Test loss: 0.2776 score: 0.8776 time: 0.44s
Epoch 86/1000, LR 0.000266
Train loss: 0.1991;  Loss pred: 0.1991; Loss self: 0.0000; time: 0.43s
Val loss: 0.2556 score: 0.9388 time: 0.54s
Test loss: 0.2731 score: 0.8776 time: 0.50s
Epoch 87/1000, LR 0.000266
Train loss: 0.1947;  Loss pred: 0.1947; Loss self: 0.0000; time: 0.53s
Val loss: 0.2521 score: 0.9388 time: 0.83s
Test loss: 0.2691 score: 0.8776 time: 0.55s
Epoch 88/1000, LR 0.000266
Train loss: 0.1694;  Loss pred: 0.1694; Loss self: 0.0000; time: 0.54s
Val loss: 0.2504 score: 0.9388 time: 0.71s
Test loss: 0.2665 score: 0.8776 time: 0.45s
Epoch 89/1000, LR 0.000266
Train loss: 0.1702;  Loss pred: 0.1702; Loss self: 0.0000; time: 0.57s
Val loss: 0.2485 score: 0.9388 time: 0.56s
Test loss: 0.2640 score: 0.8776 time: 0.46s
Epoch 90/1000, LR 0.000266
Train loss: 0.1805;  Loss pred: 0.1805; Loss self: 0.0000; time: 0.48s
Val loss: 0.2455 score: 0.9388 time: 0.88s
Test loss: 0.2609 score: 0.8776 time: 0.59s
Epoch 91/1000, LR 0.000266
Train loss: 0.1457;  Loss pred: 0.1457; Loss self: 0.0000; time: 0.54s
Val loss: 0.2445 score: 0.9388 time: 0.69s
Test loss: 0.2594 score: 0.8776 time: 0.52s
Epoch 92/1000, LR 0.000266
Train loss: 0.1499;  Loss pred: 0.1499; Loss self: 0.0000; time: 0.44s
Val loss: 0.2418 score: 0.9388 time: 0.63s
Test loss: 0.2568 score: 0.8776 time: 0.44s
Epoch 93/1000, LR 0.000265
Train loss: 0.1561;  Loss pred: 0.1561; Loss self: 0.0000; time: 0.44s
Val loss: 0.2411 score: 0.9388 time: 0.59s
Test loss: 0.2559 score: 0.8776 time: 0.45s
Epoch 94/1000, LR 0.000265
Train loss: 0.1486;  Loss pred: 0.1486; Loss self: 0.0000; time: 0.54s
Val loss: 0.2413 score: 0.9388 time: 0.63s
Test loss: 0.2558 score: 0.8776 time: 0.55s
     INFO: Early stopping counter 1 of 20
Epoch 95/1000, LR 0.000265
Train loss: 0.1456;  Loss pred: 0.1456; Loss self: 0.0000; time: 0.61s
Val loss: 0.2431 score: 0.9388 time: 0.80s
Test loss: 0.2570 score: 0.8776 time: 0.55s
     INFO: Early stopping counter 2 of 20
Epoch 96/1000, LR 0.000265
Train loss: 0.1376;  Loss pred: 0.1376; Loss self: 0.0000; time: 0.55s
Val loss: 0.2457 score: 0.9388 time: 0.73s
Test loss: 0.2590 score: 0.8776 time: 0.46s
     INFO: Early stopping counter 3 of 20
Epoch 97/1000, LR 0.000265
Train loss: 0.1334;  Loss pred: 0.1334; Loss self: 0.0000; time: 0.44s
Val loss: 0.2500 score: 0.9388 time: 0.65s
Test loss: 0.2625 score: 0.8776 time: 0.44s
     INFO: Early stopping counter 4 of 20
Epoch 98/1000, LR 0.000265
Train loss: 0.1107;  Loss pred: 0.1107; Loss self: 0.0000; time: 0.48s
Val loss: 0.2567 score: 0.9184 time: 0.54s
Test loss: 0.2680 score: 0.8776 time: 0.43s
     INFO: Early stopping counter 5 of 20
Epoch 99/1000, LR 0.000265
Train loss: 0.1310;  Loss pred: 0.1310; Loss self: 0.0000; time: 0.53s
Val loss: 0.2608 score: 0.9184 time: 0.61s
Test loss: 0.2716 score: 0.8776 time: 0.46s
     INFO: Early stopping counter 6 of 20
Epoch 100/1000, LR 0.000265
Train loss: 0.1247;  Loss pred: 0.1247; Loss self: 0.0000; time: 0.43s
Val loss: 0.2627 score: 0.9184 time: 0.63s
Test loss: 0.2736 score: 0.8776 time: 0.43s
     INFO: Early stopping counter 7 of 20
Epoch 101/1000, LR 0.000265
Train loss: 0.1134;  Loss pred: 0.1134; Loss self: 0.0000; time: 0.45s
Val loss: 0.2656 score: 0.9184 time: 3.84s
Test loss: 0.2765 score: 0.8776 time: 2.15s
     INFO: Early stopping counter 8 of 20
Epoch 102/1000, LR 0.000264
Train loss: 0.1042;  Loss pred: 0.1042; Loss self: 0.0000; time: 0.46s
Val loss: 0.2719 score: 0.9184 time: 0.64s
Test loss: 0.2822 score: 0.8776 time: 0.46s
     INFO: Early stopping counter 9 of 20
Epoch 103/1000, LR 0.000264
Train loss: 0.1065;  Loss pred: 0.1065; Loss self: 0.0000; time: 0.43s
Val loss: 0.2780 score: 0.9184 time: 0.57s
Test loss: 0.2880 score: 0.8980 time: 0.44s
     INFO: Early stopping counter 10 of 20
Epoch 104/1000, LR 0.000264
Train loss: 0.0965;  Loss pred: 0.0965; Loss self: 0.0000; time: 0.54s
Val loss: 0.2787 score: 0.9184 time: 0.58s
Test loss: 0.2894 score: 0.8980 time: 0.43s
     INFO: Early stopping counter 11 of 20
Epoch 105/1000, LR 0.000264
Train loss: 0.0956;  Loss pred: 0.0956; Loss self: 0.0000; time: 0.47s
Val loss: 0.2808 score: 0.9184 time: 0.64s
Test loss: 0.2919 score: 0.8980 time: 0.44s
     INFO: Early stopping counter 12 of 20
Epoch 106/1000, LR 0.000264
Train loss: 0.0918;  Loss pred: 0.0918; Loss self: 0.0000; time: 0.45s
Val loss: 0.2862 score: 0.9184 time: 0.55s
Test loss: 0.2972 score: 0.8980 time: 0.46s
     INFO: Early stopping counter 13 of 20
Epoch 107/1000, LR 0.000264
Train loss: 0.0929;  Loss pred: 0.0929; Loss self: 0.0000; time: 0.43s
Val loss: 0.2862 score: 0.9184 time: 0.64s
Test loss: 0.2981 score: 0.8980 time: 0.45s
     INFO: Early stopping counter 14 of 20
Epoch 108/1000, LR 0.000264
Train loss: 0.0885;  Loss pred: 0.0885; Loss self: 0.0000; time: 0.44s
Val loss: 0.2858 score: 0.9184 time: 0.62s
Test loss: 0.2986 score: 0.8980 time: 0.45s
     INFO: Early stopping counter 15 of 20
Epoch 109/1000, LR 0.000264
Train loss: 0.0892;  Loss pred: 0.0892; Loss self: 0.0000; time: 0.55s
Val loss: 0.2891 score: 0.9184 time: 0.55s
Test loss: 0.3024 score: 0.8980 time: 0.45s
     INFO: Early stopping counter 16 of 20
Epoch 110/1000, LR 0.000263
Train loss: 0.0649;  Loss pred: 0.0649; Loss self: 0.0000; time: 0.45s
Val loss: 0.2930 score: 0.9184 time: 0.65s
Test loss: 0.3066 score: 0.8980 time: 0.45s
     INFO: Early stopping counter 17 of 20
Epoch 111/1000, LR 0.000263
Train loss: 0.0857;  Loss pred: 0.0857; Loss self: 0.0000; time: 0.44s
Val loss: 0.2938 score: 0.9184 time: 0.56s
Test loss: 0.3082 score: 0.8980 time: 0.48s
     INFO: Early stopping counter 18 of 20
Epoch 112/1000, LR 0.000263
Train loss: 0.1123;  Loss pred: 0.1123; Loss self: 0.0000; time: 0.43s
Val loss: 0.2870 score: 0.9184 time: 0.66s
Test loss: 0.3032 score: 0.8980 time: 0.46s
     INFO: Early stopping counter 19 of 20
Epoch 113/1000, LR 0.000263
Train loss: 0.0801;  Loss pred: 0.0801; Loss self: 0.0000; time: 0.44s
Val loss: 0.2792 score: 0.9388 time: 0.59s
Test loss: 0.2973 score: 0.8980 time: 0.44s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 092,   Train_Loss: 0.1561,   Val_Loss: 0.2411,   Val_Precision: 0.9231,   Val_Recall: 0.9600,   Val_accuracy: 0.9412,   Val_Score: 0.9388,   Val_Loss: 0.2411,   Test_Precision: 0.8750,   Test_Recall: 0.8750,   Test_accuracy: 0.8750,   Test_Score: 0.8776,   Test_loss: 0.2559


[0.4864854048937559, 0.49113618093542755, 0.4953845548443496, 0.5020810109563172, 0.6105500708799809, 0.6348829148337245, 0.5343367431778461, 0.5420319230761379, 0.5006377091631293, 0.6278654281049967, 0.4935796959325671, 0.5048661360051483, 0.5387552010361105, 0.5011575769167393, 0.48138835304416716, 0.5058298499789089, 0.6189252841286361, 0.5675822379998863, 0.5045047800522298, 0.5167771831620485, 0.5274420049972832, 0.5973227471113205, 0.5289010058622807, 0.5005101580172777, 0.4971687709912658, 0.49798980192281306, 0.5915795790497214, 0.7225857749581337, 0.507525707129389, 0.6121962869074196, 0.5273409918881953, 0.5514915038365871, 0.5755820339545608, 0.533705092035234, 0.5312240819912404, 0.5686415950767696, 0.4983961330726743, 0.5102405559737235, 0.5970565949101001, 0.49013955518603325, 0.6013896730728447, 0.5341293341480196, 0.49669445399194956, 0.489765761885792, 0.5007811170071363, 0.5026739991735667, 0.49545597890391946, 0.5373096771072596, 0.4870557989925146, 0.526107651880011, 0.4926560411695391, 0.48045027116313577, 0.61098856292665, 0.5133866069372743, 0.5947287168819457, 0.49296367494389415, 0.47831373708322644, 0.5191884539090097, 0.4918215519282967, 0.47907064110040665, 2.2969875768758357, 0.5215836688876152, 0.4966632251162082, 0.49362886720336974, 0.4835226060822606, 0.5234909809660167, 0.6037856419570744, 0.4927330380305648, 0.6021768630016595, 0.4938877490349114, 0.5004533131141216, 0.5501459510996938, 0.5184926339425147, 0.4998858200851828, 0.5293831909075379, 0.5204927180893719, 0.4875845620408654, 0.4864306871313602, 0.4761741200927645, 0.48746123211458325, 0.5891961390152574, 0.5962004538159817, 0.6400923039764166, 0.491470105946064, 0.48954105214215815, 0.6366601812187582, 0.49966930295340717, 0.5844131580088288, 0.4863780119922012, 0.6167535160202533, 0.6714696460403502, 0.6242299238219857, 0.6128077402245253, 0.5806076929438859, 0.6066815480589867, 0.5140152180101722, 0.610620005056262, 0.5063326428644359, 0.540864926064387, 0.5032452151644975, 0.511703854193911, 0.5517726959660649, 0.44460143987089396, 0.456339524127543, 0.5144379439298064, 0.44528860598802567, 0.4468949940055609, 0.5056479461491108, 0.44070236617699265, 0.4291487780865282, 0.46086748503148556, 0.4454697591718286, 0.5939233012031764, 0.43907107785344124, 0.451474976958707, 0.46642496692948043, 0.44743331195786595, 0.48038050695322454, 0.44183760113082826, 0.5430902831722051, 0.45372364413924515, 0.43019409105181694, 0.46258975099772215, 0.4552636919543147, 0.44946770602837205, 0.45550579903647304, 0.44427185296081007, 0.43511560815386474, 0.43167648487724364, 0.4460503200534731, 3.7720067780464888, 0.43889127601869404, 0.43064870894886553, 0.4434668810572475, 0.4430090528912842, 0.47687131119892, 0.4437255079392344, 0.436964601976797, 0.45143067883327603, 0.46202068380080163, 0.48478609998710454, 0.4401703921612352, 0.46875956282019615, 0.45193209196440876, 0.44315237109549344, 0.4331741838250309, 0.43698444007895887, 0.4500562238972634, 0.4404485737904906, 0.44521214812994003, 0.45730535592883825, 0.45279705501161516, 0.45503081008791924, 0.44586221408098936, 0.44567313301377, 0.4521394190378487, 0.4489766040351242, 0.43907519499771297, 0.45425764191895723, 0.4632914981339127, 0.46540283295325935, 0.44497258006595075, 0.4457428678870201, 0.4601561948657036, 0.4493715011049062, 0.4503388670273125, 0.45774362701922655, 0.5638352187816054, 0.45233415882103145, 0.5558108219411224, 0.5901306529995054, 0.471708060009405, 0.44953102711588144, 0.491190604865551, 0.45802738098427653, 0.45059118513017893, 0.43777240603230894, 0.4726888120640069, 0.6834313089493662, 0.4595549760852009, 0.4398831850849092, 0.43326869700104, 0.4454550398513675, 0.43697785399854183, 0.555471793981269, 0.4400386142078787, 0.4462576631922275, 0.510546371107921, 0.5602167558390647, 0.45843589794822037, 0.466014752862975, 0.5971240899525583, 0.5246033919975162, 0.448110185097903, 0.45371885714121163, 0.5612299989443272, 0.5568274520337582, 0.4668627539649606, 0.44835882005281746, 0.43422371707856655, 0.4658051359001547, 0.43419940606690943, 2.1524934829212725, 0.469568437198177, 0.44731462793424726, 0.4386712310370058, 0.4500220799818635, 0.4661734530236572, 0.45076700998470187, 0.450040060095489, 0.45231747697107494, 0.45175483310595155, 0.4811033180449158, 0.4617968259844929, 0.44798693200573325]
[0.009928273569260324, 0.010023187366029133, 0.010109888874374482, 0.010246551244006473, 0.012460205528162877, 0.012956794180280092, 0.01090483149342543, 0.011061875981145672, 0.010217096105369985, 0.012813580165408095, 0.010073055019031982, 0.010303390530717313, 0.010995004102777766, 0.010227705651362027, 0.009824252102942186, 0.010323058162834875, 0.012631128247523186, 0.011583310979589517, 0.01029601591943326, 0.01054647312575609, 0.010764122550964964, 0.01219026014512899, 0.010793898078822056, 0.01021449302076077, 0.010146301448801342, 0.010163057182098225, 0.012073052633667782, 0.01474664846853334, 0.010357667492436511, 0.012493801773620807, 0.01076206105894276, 0.011254928649726267, 0.011746572121521649, 0.010891940653780284, 0.010841307795739599, 0.011604930511770807, 0.010171349654544373, 0.010413072570892317, 0.012184828467553064, 0.010002848065021087, 0.012273258634139689, 0.010900598656082032, 0.010136621510039787, 0.009995219630322285, 0.010220022796064007, 0.010258653044358504, 0.010111346508243255, 0.01096550361443387, 0.009939914265153359, 0.010736890854694101, 0.010054204921827329, 0.009805107574757874, 0.012469154345441838, 0.010477277692597436, 0.01213732075269277, 0.010060483162120288, 0.009761504838433193, 0.010595682732836932, 0.010037174529148912, 0.009776951859191972, 0.04687729748726195, 0.01064456467117582, 0.010135984186045065, 0.010074058514354485, 0.009867808287393073, 0.010683489407469727, 0.012322155958307641, 0.010055776286338056, 0.012289323734727745, 0.010079341817039008, 0.010213332920696358, 0.011227468389789668, 0.010581482325357442, 0.010201751430309852, 0.010803738589949754, 0.010622300369170855, 0.009950705347772764, 0.00992715688023184, 0.009717839185566622, 0.009948188410501699, 0.012024411000311375, 0.012167356200326157, 0.013063108244416664, 0.010030002162164571, 0.0099906337171869, 0.012993064922831801, 0.01019733271333484, 0.011926799143037322, 0.009926081877391862, 0.012586806449392925, 0.01370346216408878, 0.012739386200448689, 0.012506280412745416, 0.01184913659069155, 0.012381256082836462, 0.010490106490003516, 0.012461632756250245, 0.010333319242131345, 0.011038059715599733, 0.010270310513561174, 0.010442935799875734, 0.01126066726461357, 0.009073498772875386, 0.009313051512807, 0.010498733549587885, 0.009087522571184198, 0.009120306000113487, 0.01031934583977777, 0.008993925840346789, 0.008758138328296493, 0.009405458878193582, 0.009091219574935278, 0.012120883698024007, 0.008960634241906964, 0.009213775039973612, 0.009518876876111845, 0.009131292080772775, 0.00980368381537193, 0.009017093900629148, 0.011083475166779697, 0.00925966620692337, 0.008779471245955447, 0.009440607163218819, 0.009291095754169688, 0.009172810327109635, 0.009296036715030062, 0.009066772509404287, 0.008879910370487035, 0.008809724181168238, 0.00910306775619333, 0.07697973016421406, 0.008956964816708041, 0.008788749162221745, 0.009050344511372398, 0.009041001079413963, 0.009732067575488163, 0.009055622611004783, 0.00891764493830198, 0.00921287099659747, 0.009428993546955136, 0.009893593877287847, 0.00898306922778031, 0.009566521690208085, 0.009223103917640994, 0.009043925940724356, 0.008840289465816957, 0.008918049797529772, 0.00918482089586252, 0.008988746403887565, 0.00908596220673347, 0.009332762365894658, 0.00924075622472684, 0.00928634306301876, 0.009099228858795702, 0.00909537006150551, 0.009227335082405076, 0.009162787837451515, 0.008960718265259448, 0.009270564120795046, 0.009454928533345158, 0.00949801699904611, 0.009081073062570423, 0.009096793222184084, 0.009390942752361298, 0.009170846961324617, 0.009190589123006378, 0.009341706673861767, 0.011506841199624598, 0.009231309363694519, 0.011343077998798417, 0.01204348271427562, 0.009626695102232755, 0.009174102594201662, 0.010024298058480633, 0.009347497571107685, 0.009195738472044468, 0.008934130735353244, 0.009646710450285855, 0.013947577733660534, 0.00937867298133063, 0.008977207858875697, 0.008842218306143674, 0.009090919180640153, 0.008917915387725343, 0.011336159060842224, 0.008980379881793442, 0.009107299248820968, 0.010419313696080021, 0.011432995017123769, 0.009355834652004498, 0.009510505160468877, 0.012186205917399148, 0.010706191673418698, 0.009145105818324551, 0.009259568513085951, 0.011453673447843413, 0.011363825551709351, 0.009527811305407358, 0.009150180001077908, 0.008861708511807481, 0.009506227263268463, 0.008861212368712438, 0.04392843842696475, 0.009583029330575041, 0.009128869957841781, 0.008952474102796036, 0.009184124081262521, 0.00951374393925831, 0.00919932673438167, 0.00918449102235692, 0.00923096891777704, 0.009219486389917379, 0.009818435062141138, 0.009424425020091692, 0.009142590449096598]
[100.72244615581256, 99.76866274985818, 98.913055566288, 97.59381241419459, 80.25549801243442, 77.17958517254016, 91.70247156986372, 90.40057958563649, 97.87516821677067, 78.04220109377614, 99.27474813853442, 97.05543015366815, 90.95039807646467, 97.77363898489097, 101.78891884304537, 96.87051881584905, 79.16949146613946, 86.33110185525189, 97.12494695278643, 94.81842774129372, 92.90120911066306, 82.03270382212328, 92.64493630545107, 97.900110947016, 98.55808099592166, 98.39558924862251, 82.82909305069441, 67.8120185840069, 96.54683361193347, 80.03968832860646, 92.9190045032357, 88.84996352458629, 85.13121867849735, 91.81100336356756, 92.23979420573028, 86.17027038513555, 98.31536953930379, 96.03313461919991, 82.06927185416656, 99.97152745895396, 81.4779538026167, 91.73808077431106, 98.65219876361695, 100.04782655964068, 97.84713986989595, 97.478684158241, 98.89879643475298, 91.19508188239544, 100.60448946785472, 93.1368320245899, 99.46087311479347, 101.9876622847449, 80.19790053890503, 95.44464023384005, 82.39050614017441, 99.39880459868945, 102.44322126059703, 94.37806182143544, 99.62963153583756, 102.28136687201058, 21.332287772598914, 93.94465916561883, 98.65840175409622, 99.26485920000407, 101.33962586986831, 93.60237670108192, 81.1546293833261, 99.44533087501324, 81.37144252894512, 99.21282740005026, 97.91123110983625, 89.06727369719485, 94.50471769948545, 98.02238437499624, 92.56055130121867, 94.1415668212795, 100.49538852276709, 100.73377625282838, 102.9035345105572, 100.5208143167414, 83.16415664551926, 82.18712294896027, 76.55145936859341, 99.7008758155831, 100.09375063762958, 76.96413478568634, 98.06485951883484, 83.84479255557719, 100.74468580373589, 79.44827022014239, 72.97425920732607, 78.49671752354752, 79.95982554340289, 84.39433475562942, 80.76724956737239, 95.32791692371704, 80.24630636771421, 96.77432551611949, 90.59563236342454, 97.36803952320382, 95.75851266000309, 88.80468417200113, 110.21106907397537, 107.37619121131598, 95.24958370233654, 110.04099215895425, 109.64544391246923, 96.90536740665483, 111.18615138163536, 114.17951652683082, 106.3212346096675, 109.99624327158759, 82.50223539089181, 111.59924320124736, 108.53314690900723, 105.0544106216518, 109.513526799306, 102.0024736448584, 110.90047536604085, 90.22440930776679, 107.99525357105301, 113.90207587509133, 105.92539046599258, 107.6299315450728, 109.0178434241212, 107.57272487781543, 110.29283010716047, 112.61374926976353, 113.51093171992954, 109.85307665315835, 12.990432648526935, 111.6449623799606, 113.78183419985167, 110.49303136951629, 110.607220507579, 102.75308841038742, 110.42863014021331, 112.13722983126657, 108.54379708229102, 106.05585792588921, 101.07550526160593, 111.32052694278268, 104.53120082543279, 108.42336906638381, 110.5714494517308, 113.1184678812536, 112.13213905544595, 108.87528579359338, 111.25021833606382, 110.05988988804235, 107.14941201700017, 108.21625153622753, 107.68501585756889, 109.89942285420786, 109.94604873003651, 108.37365187992644, 109.13708990539438, 111.59819675136791, 107.86830088978877, 105.76494539046502, 105.28513479186556, 110.11914485323469, 109.92884806497845, 106.48558151933743, 109.04118280647465, 108.80695313608854, 107.04682076970099, 86.90482319618907, 108.32699464421196, 88.15949252098336, 83.03246027120213, 103.87780950578421, 109.00248713503959, 99.75760837977003, 106.98050386136654, 108.74602437205593, 111.93030744926315, 103.66228002317283, 71.69703722723405, 106.62489266771743, 111.39320997355625, 113.09379223369646, 109.99987791438966, 112.1338290982671, 88.21329999278474, 111.35386399715344, 109.80203600199697, 95.97561117448862, 87.46614500419618, 106.88517242935123, 105.14688579914497, 82.05999527483989, 93.40389472783279, 109.34810595589228, 107.99639298383768, 87.30823386520485, 87.99853495195372, 104.95589888860056, 109.28746755606974, 112.84505675938045, 105.19420294778189, 112.85137500268523, 22.764296565255698, 104.35113631651525, 109.54258354189733, 111.70096539990884, 108.88354634060347, 105.11109047969182, 108.70360721753565, 108.87919619778566, 108.33098983511857, 108.46591205922499, 101.8492248174962, 106.10726891753347, 109.37819052135411]
Elapsed: 0.5307879502159478~0.2828704940610961
Time per graph: 0.01083240714726424~0.005772867225736655
Speed: 98.22454204779785~14.171214419030207
Total Time: 0.4488
best val loss: 0.24107341468334198 test_score: 0.8776

Testing...
Test loss: 0.3100 score: 0.8776 time: 0.53s
test Score 0.8776
Epoch Time List: [1.4255824720021337, 1.5222562558483332, 1.4426324090454727, 1.5320483681280166, 1.55734296515584, 1.903188626980409, 1.7200794769451022, 1.653433856088668, 1.5390314583200961, 1.640339040895924, 1.4589809651952237, 1.5963708308991045, 1.4823978361673653, 1.5319831580854952, 1.444203456165269, 1.5459866570308805, 1.7201996040530503, 1.5289458658080548, 1.5466622998937964, 1.5089990419801325, 1.5711643958929926, 1.5753722609952092, 1.5588925208430737, 1.5579999298788607, 1.455913363955915, 1.5259987330064178, 1.558732729172334, 1.6577531530056149, 1.5436185549478978, 1.6461542018223554, 1.6279381667263806, 1.5393257848918438, 1.7408441409934312, 1.6649830632377416, 1.5234598109964281, 1.6733914173673838, 1.4774036633316427, 1.5350412828847766, 1.5347840518224984, 1.428236443316564, 1.5300076438579708, 1.4614985301159322, 1.518954827915877, 1.630490595009178, 1.5179133960045874, 1.4266291388776153, 1.547051940113306, 1.5782396546564996, 1.4756837890017778, 1.551181532908231, 1.4755929179955274, 1.6319930718746036, 1.5317248387727886, 1.524037712952122, 1.5191255428362638, 1.4380068997852504, 1.54425815702416, 1.4389671250246465, 1.503510129172355, 1.443460094043985, 6.540724294260144, 1.5373938435222954, 1.4058926939032972, 1.5341839049942791, 1.4087467570789158, 1.5676907000597566, 1.5248551708646119, 1.412041988922283, 1.5550116691738367, 1.4350474560633302, 1.5916957091540098, 1.515143149998039, 1.5812988481484354, 1.4471317548304796, 1.5800200041849166, 1.5642805979587138, 1.5310444151982665, 1.4868465987965465, 1.385019610170275, 1.5029891040176153, 1.4975734760519117, 1.4922171228099614, 1.8303703931160271, 1.427939127665013, 1.5526115843094885, 1.69113396294415, 1.552649380872026, 1.5259701379109174, 1.5351306069642305, 1.6901484613772482, 1.8062090743333101, 1.8815599018707871, 1.782329934882, 1.6197480051778257, 1.5437549790367484, 1.531954019330442, 1.5607598840724677, 1.4875124949030578, 1.600602338090539, 1.4565821902360767, 1.5939592670183629, 1.4979691519401968, 1.6077668440993875, 1.5725108671467751, 1.53508360683918, 1.5555220569949597, 1.5459882891736925, 1.5302116076927632, 1.5756620930042118, 1.513675111811608, 1.6127136761788279, 1.5294916811399162, 1.5872447069268674, 1.5332346437498927, 1.442862887866795, 1.587952665053308, 1.5470736511051655, 1.4642254831269383, 1.5504791047424078, 1.5612724428065121, 1.5566350149456412, 1.5646709038410336, 1.4641822648700327, 1.628198733087629, 1.4725357010029256, 1.5172727580647916, 1.5199428552296013, 1.3966517930384725, 1.540693286107853, 1.4036461061332375, 4.958284031832591, 2.860381186939776, 1.438596862833947, 1.5288138641044497, 1.4582275690045208, 1.564861198188737, 1.518652540864423, 1.4416694520041347, 1.5246414481662214, 1.5126095777377486, 1.6006395360454917, 1.5061168600805104, 1.446309180231765, 1.515731506049633, 1.4331362061202526, 1.507474402198568, 1.509563238127157, 1.4405441801063716, 1.5123269730247557, 1.426482129143551, 1.549041188089177, 1.5404402788262814, 1.4421424339525402, 1.5477861592080444, 1.4656489258632064, 1.5399085709359497, 1.6199209990445524, 1.4806437829975039, 1.5346217502374202, 1.5941849218215793, 1.5943590439856052, 1.6210888780187815, 1.4547672348562628, 1.533165494678542, 1.4722768699284643, 1.5532072728965431, 1.5901080730836838, 1.575966291828081, 1.7518489800859243, 1.7701105931773782, 1.9242598642595112, 1.7849413016811013, 1.4740828857757151, 1.5913067392539233, 1.5747354628983885, 1.571642881957814, 1.536124114645645, 1.5660148807801306, 1.8328316938132048, 1.540830596582964, 1.539579503936693, 1.4991183916572481, 1.4281690868083388, 1.549961781129241, 1.7594366241246462, 1.738784645916894, 1.5218870849348605, 1.47778022964485, 1.9213254118803889, 1.698578838026151, 1.595566175179556, 1.9459290658123791, 1.7442039530724287, 1.5149797282647341, 1.4784372809808701, 1.728898314991966, 1.9601751998998225, 1.7408385032322258, 1.5388125120662153, 1.454643297707662, 1.6073534484021366, 1.4915594777558, 6.43701257603243, 1.5700398141052574, 1.4431158569641411, 1.558459729887545, 1.556632287800312, 1.459947454975918, 1.5254675219766796, 1.501413342077285, 1.5527259402442724, 1.5394169979263097, 1.4776353628840297, 1.5467749149538577, 1.4762494396418333]
Total Epoch List: [102, 113]
Total Time List: [0.552347867982462, 0.44884158694185317]
========================k_idx:2========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: FAGNN
FAGNN(
  (conv1): NeighborFilterConv()
  (conv2): NeighborFilterConv()
  (ffn): Sequential(
    (0): Linear(in_features=14951, out_features=64, bias=True)
    (1): ReLU()
    (2): Linear(in_features=64, out_features=64, bias=True)
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=64, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=64, out_features=2, bias=True)
  )
)
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7ed848808af0>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.6948;  Loss pred: 0.6948; Loss self: 0.0000; time: 0.65s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6933 score: 0.5102 time: 0.59s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6940 score: 0.5000 time: 0.56s
Epoch 2/1000, LR 0.000000
Train loss: 0.6948;  Loss pred: 0.6948; Loss self: 0.0000; time: 0.59s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6932 score: 0.5102 time: 0.56s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6940 score: 0.5000 time: 0.44s
Epoch 3/1000, LR 0.000030
Train loss: 0.6947;  Loss pred: 0.6947; Loss self: 0.0000; time: 0.52s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6932 score: 0.5102 time: 0.47s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6939 score: 0.5000 time: 0.43s
Epoch 4/1000, LR 0.000060
Train loss: 0.6947;  Loss pred: 0.6947; Loss self: 0.0000; time: 0.61s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6931 score: 0.5102 time: 0.47s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6938 score: 0.5000 time: 0.44s
Epoch 5/1000, LR 0.000090
Train loss: 0.6946;  Loss pred: 0.6946; Loss self: 0.0000; time: 0.52s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6931 score: 0.5102 time: 0.47s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6938 score: 0.5000 time: 0.55s
Epoch 6/1000, LR 0.000120
Train loss: 0.6944;  Loss pred: 0.6944; Loss self: 0.0000; time: 0.52s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6930 score: 0.5102 time: 0.47s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6937 score: 0.5000 time: 0.43s
Epoch 7/1000, LR 0.000150
Train loss: 0.6943;  Loss pred: 0.6943; Loss self: 0.0000; time: 0.52s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6929 score: 0.5102 time: 0.56s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6936 score: 0.5000 time: 0.45s
Epoch 8/1000, LR 0.000180
Train loss: 0.6941;  Loss pred: 0.6941; Loss self: 0.0000; time: 0.53s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6928 score: 0.5102 time: 0.47s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6934 score: 0.5000 time: 0.44s
Epoch 9/1000, LR 0.000210
Train loss: 0.6939;  Loss pred: 0.6939; Loss self: 0.0000; time: 0.62s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6927 score: 0.5102 time: 0.47s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6933 score: 0.5000 time: 0.56s
Epoch 10/1000, LR 0.000240
Train loss: 0.6937;  Loss pred: 0.6937; Loss self: 0.0000; time: 0.51s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6926 score: 0.5102 time: 0.47s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6931 score: 0.5000 time: 0.44s
Epoch 11/1000, LR 0.000270
Train loss: 0.6935;  Loss pred: 0.6935; Loss self: 0.0000; time: 0.67s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6925 score: 0.5102 time: 0.64s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6930 score: 0.5000 time: 0.44s
Epoch 12/1000, LR 0.000270
Train loss: 0.6932;  Loss pred: 0.6932; Loss self: 0.0000; time: 0.56s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6924 score: 0.5102 time: 0.48s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6928 score: 0.5000 time: 0.58s
Epoch 13/1000, LR 0.000270
Train loss: 0.6930;  Loss pred: 0.6930; Loss self: 0.0000; time: 0.52s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6923 score: 0.5102 time: 0.47s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6927 score: 0.5000 time: 0.44s
Epoch 14/1000, LR 0.000270
Train loss: 0.6927;  Loss pred: 0.6927; Loss self: 0.0000; time: 0.54s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6921 score: 0.5102 time: 0.57s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6925 score: 0.5000 time: 0.54s
Epoch 15/1000, LR 0.000270
Train loss: 0.6925;  Loss pred: 0.6925; Loss self: 0.0000; time: 0.53s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6920 score: 0.5102 time: 0.46s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6924 score: 0.5000 time: 0.45s
Epoch 16/1000, LR 0.000270
Train loss: 0.6922;  Loss pred: 0.6922; Loss self: 0.0000; time: 0.63s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6918 score: 0.5102 time: 0.66s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6922 score: 0.5000 time: 0.44s
Epoch 17/1000, LR 0.000270
Train loss: 0.6920;  Loss pred: 0.6920; Loss self: 0.0000; time: 0.51s
Val loss: 0.6917 score: 0.7551 time: 0.47s
Test loss: 0.6920 score: 0.6042 time: 0.60s
Epoch 18/1000, LR 0.000270
Train loss: 0.6916;  Loss pred: 0.6916; Loss self: 0.0000; time: 0.57s
Val loss: 0.6915 score: 0.8980 time: 0.47s
Test loss: 0.6918 score: 0.8333 time: 0.45s
Epoch 19/1000, LR 0.000270
Train loss: 0.6914;  Loss pred: 0.6914; Loss self: 0.0000; time: 0.53s
Val loss: 0.6912 score: 0.5918 time: 0.47s
Test loss: 0.6916 score: 0.6250 time: 0.68s
Epoch 20/1000, LR 0.000270
Train loss: 0.6910;  Loss pred: 0.6910; Loss self: 0.0000; time: 0.53s
Val loss: 0.6910 score: 0.5102 time: 0.47s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6913 score: 0.5000 time: 0.46s
Epoch 21/1000, LR 0.000270
Train loss: 0.6907;  Loss pred: 0.6907; Loss self: 0.0000; time: 0.61s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6907 score: 0.4898 time: 0.56s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6911 score: 0.5000 time: 0.45s
Epoch 22/1000, LR 0.000270
Train loss: 0.6902;  Loss pred: 0.6902; Loss self: 0.0000; time: 0.53s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6904 score: 0.4898 time: 0.47s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6907 score: 0.5000 time: 0.58s
Epoch 23/1000, LR 0.000270
Train loss: 0.6898;  Loss pred: 0.6898; Loss self: 0.0000; time: 0.51s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6900 score: 0.4898 time: 0.46s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6904 score: 0.5000 time: 0.44s
Epoch 24/1000, LR 0.000270
Train loss: 0.6892;  Loss pred: 0.6892; Loss self: 0.0000; time: 0.53s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6895 score: 0.4898 time: 0.60s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6900 score: 0.5000 time: 0.44s
Epoch 25/1000, LR 0.000270
Train loss: 0.6888;  Loss pred: 0.6888; Loss self: 0.0000; time: 0.51s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6890 score: 0.4898 time: 0.49s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6895 score: 0.5000 time: 0.44s
Epoch 26/1000, LR 0.000270
Train loss: 0.6880;  Loss pred: 0.6880; Loss self: 0.0000; time: 0.52s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6883 score: 0.4898 time: 0.68s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6889 score: 0.5000 time: 0.43s
Epoch 27/1000, LR 0.000270
Train loss: 0.6874;  Loss pred: 0.6874; Loss self: 0.0000; time: 0.52s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6876 score: 0.4898 time: 0.46s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6883 score: 0.5000 time: 0.44s
Epoch 28/1000, LR 0.000270
Train loss: 0.6865;  Loss pred: 0.6865; Loss self: 0.0000; time: 0.65s
Val loss: 0.6868 score: 0.5102 time: 0.46s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6876 score: 0.5000 time: 0.45s
Epoch 29/1000, LR 0.000270
Train loss: 0.6855;  Loss pred: 0.6855; Loss self: 0.0000; time: 0.53s
Val loss: 0.6859 score: 0.5102 time: 0.60s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6868 score: 0.5000 time: 0.44s
Epoch 30/1000, LR 0.000270
Train loss: 0.6847;  Loss pred: 0.6847; Loss self: 0.0000; time: 0.52s
Val loss: 0.6849 score: 0.5306 time: 0.47s
Test loss: 0.6860 score: 0.5208 time: 0.45s
Epoch 31/1000, LR 0.000270
Train loss: 0.6836;  Loss pred: 0.6836; Loss self: 0.0000; time: 0.53s
Val loss: 0.6838 score: 0.5306 time: 0.69s
Test loss: 0.6850 score: 0.5625 time: 0.45s
Epoch 32/1000, LR 0.000270
Train loss: 0.6819;  Loss pred: 0.6819; Loss self: 0.0000; time: 0.52s
Val loss: 0.6825 score: 0.5714 time: 0.50s
Test loss: 0.6839 score: 0.5833 time: 0.43s
Epoch 33/1000, LR 0.000270
Train loss: 0.6803;  Loss pred: 0.6803; Loss self: 0.0000; time: 0.63s
Val loss: 0.6811 score: 0.5918 time: 0.46s
Test loss: 0.6826 score: 0.6250 time: 0.45s
Epoch 34/1000, LR 0.000270
Train loss: 0.6790;  Loss pred: 0.6790; Loss self: 0.0000; time: 0.54s
Val loss: 0.6795 score: 0.6122 time: 0.46s
Test loss: 0.6812 score: 0.6667 time: 0.56s
Epoch 35/1000, LR 0.000270
Train loss: 0.6766;  Loss pred: 0.6766; Loss self: 0.0000; time: 0.52s
Val loss: 0.6777 score: 0.6939 time: 0.47s
Test loss: 0.6796 score: 0.7500 time: 0.44s
Epoch 36/1000, LR 0.000270
Train loss: 0.6750;  Loss pred: 0.6750; Loss self: 0.0000; time: 0.52s
Val loss: 0.6757 score: 0.7551 time: 0.60s
Test loss: 0.6778 score: 0.7917 time: 0.45s
Epoch 37/1000, LR 0.000270
Train loss: 0.6723;  Loss pred: 0.6723; Loss self: 0.0000; time: 0.52s
Val loss: 0.6734 score: 0.8163 time: 0.46s
Test loss: 0.6758 score: 0.8125 time: 0.45s
Epoch 38/1000, LR 0.000270
Train loss: 0.6695;  Loss pred: 0.6695; Loss self: 0.0000; time: 0.78s
Val loss: 0.6709 score: 0.8571 time: 0.58s
Test loss: 0.6736 score: 0.8333 time: 0.55s
Epoch 39/1000, LR 0.000269
Train loss: 0.6670;  Loss pred: 0.6670; Loss self: 0.0000; time: 0.65s
Val loss: 0.6680 score: 0.8776 time: 0.58s
Test loss: 0.6711 score: 0.8333 time: 0.57s
Epoch 40/1000, LR 0.000269
Train loss: 0.6639;  Loss pred: 0.6639; Loss self: 0.0000; time: 0.69s
Val loss: 0.6647 score: 0.9184 time: 0.46s
Test loss: 0.6683 score: 0.8542 time: 0.44s
Epoch 41/1000, LR 0.000269
Train loss: 0.6596;  Loss pred: 0.6596; Loss self: 0.0000; time: 0.52s
Val loss: 0.6611 score: 0.9184 time: 0.48s
Test loss: 0.6652 score: 0.8750 time: 0.56s
Epoch 42/1000, LR 0.000269
Train loss: 0.6562;  Loss pred: 0.6562; Loss self: 0.0000; time: 0.55s
Val loss: 0.6573 score: 0.9184 time: 0.58s
Test loss: 0.6618 score: 0.8750 time: 0.46s
Epoch 43/1000, LR 0.000269
Train loss: 0.6517;  Loss pred: 0.6517; Loss self: 0.0000; time: 0.55s
Val loss: 0.6531 score: 0.9388 time: 0.57s
Test loss: 0.6580 score: 0.8750 time: 0.47s
Epoch 44/1000, LR 0.000269
Train loss: 0.6460;  Loss pred: 0.6460; Loss self: 0.0000; time: 0.56s
Val loss: 0.6486 score: 0.9184 time: 0.56s
Test loss: 0.6540 score: 0.8958 time: 0.55s
Epoch 45/1000, LR 0.000269
Train loss: 0.6417;  Loss pred: 0.6417; Loss self: 0.0000; time: 0.76s
Val loss: 0.6437 score: 0.9184 time: 0.48s
Test loss: 0.6496 score: 0.9167 time: 0.44s
Epoch 46/1000, LR 0.000269
Train loss: 0.6362;  Loss pred: 0.6362; Loss self: 0.0000; time: 0.54s
Val loss: 0.6384 score: 0.9184 time: 0.53s
Test loss: 0.6449 score: 0.9167 time: 0.44s
Epoch 47/1000, LR 0.000269
Train loss: 0.6309;  Loss pred: 0.6309; Loss self: 0.0000; time: 0.64s
Val loss: 0.6328 score: 0.9184 time: 0.50s
Test loss: 0.6399 score: 0.9167 time: 0.45s
Epoch 48/1000, LR 0.000269
Train loss: 0.6235;  Loss pred: 0.6235; Loss self: 0.0000; time: 0.52s
Val loss: 0.6268 score: 0.9184 time: 0.46s
Test loss: 0.6346 score: 0.8958 time: 0.54s
Epoch 49/1000, LR 0.000269
Train loss: 0.6165;  Loss pred: 0.6165; Loss self: 0.0000; time: 0.56s
Val loss: 0.6204 score: 0.9184 time: 0.47s
Test loss: 0.6289 score: 0.8750 time: 0.45s
Epoch 50/1000, LR 0.000269
Train loss: 0.6098;  Loss pred: 0.6098; Loss self: 0.0000; time: 0.65s
Val loss: 0.6135 score: 0.9184 time: 0.47s
Test loss: 0.6228 score: 0.8750 time: 0.46s
Epoch 51/1000, LR 0.000269
Train loss: 0.5992;  Loss pred: 0.5992; Loss self: 0.0000; time: 0.61s
Val loss: 0.6062 score: 0.9184 time: 0.47s
Test loss: 0.6162 score: 0.8542 time: 0.45s
Epoch 52/1000, LR 0.000269
Train loss: 0.5921;  Loss pred: 0.5921; Loss self: 0.0000; time: 0.64s
Val loss: 0.5984 score: 0.9184 time: 0.50s
Test loss: 0.6092 score: 0.8542 time: 0.46s
Epoch 53/1000, LR 0.000269
Train loss: 0.5830;  Loss pred: 0.5830; Loss self: 0.0000; time: 0.52s
Val loss: 0.5901 score: 0.9184 time: 0.47s
Test loss: 0.6017 score: 0.8542 time: 0.55s
Epoch 54/1000, LR 0.000269
Train loss: 0.5719;  Loss pred: 0.5719; Loss self: 0.0000; time: 0.63s
Val loss: 0.5814 score: 0.9184 time: 0.57s
Test loss: 0.5938 score: 0.8542 time: 0.45s
Epoch 55/1000, LR 0.000269
Train loss: 0.5613;  Loss pred: 0.5613; Loss self: 0.0000; time: 0.69s
Val loss: 0.5721 score: 0.9184 time: 0.58s
Test loss: 0.5854 score: 0.8542 time: 0.45s
Epoch 56/1000, LR 0.000269
Train loss: 0.5503;  Loss pred: 0.5503; Loss self: 0.0000; time: 0.52s
Val loss: 0.5622 score: 0.8980 time: 0.47s
Test loss: 0.5766 score: 0.8542 time: 0.45s
Epoch 57/1000, LR 0.000269
Train loss: 0.5388;  Loss pred: 0.5388; Loss self: 0.0000; time: 0.61s
Val loss: 0.5518 score: 0.8980 time: 0.49s
Test loss: 0.5672 score: 0.8542 time: 0.44s
Epoch 58/1000, LR 0.000269
Train loss: 0.5267;  Loss pred: 0.5267; Loss self: 0.0000; time: 0.51s
Val loss: 0.5408 score: 0.8980 time: 3.74s
Test loss: 0.5572 score: 0.8542 time: 2.45s
Epoch 59/1000, LR 0.000268
Train loss: 0.5154;  Loss pred: 0.5154; Loss self: 0.0000; time: 0.54s
Val loss: 0.5292 score: 0.8980 time: 0.47s
Test loss: 0.5467 score: 0.8542 time: 0.48s
Epoch 60/1000, LR 0.000268
Train loss: 0.4996;  Loss pred: 0.4996; Loss self: 0.0000; time: 0.51s
Val loss: 0.5171 score: 0.8980 time: 0.57s
Test loss: 0.5355 score: 0.8542 time: 0.45s
Epoch 61/1000, LR 0.000268
Train loss: 0.4860;  Loss pred: 0.4860; Loss self: 0.0000; time: 0.52s
Val loss: 0.5044 score: 0.9184 time: 0.49s
Test loss: 0.5238 score: 0.8542 time: 0.44s
Epoch 62/1000, LR 0.000268
Train loss: 0.4704;  Loss pred: 0.4704; Loss self: 0.0000; time: 0.62s
Val loss: 0.4912 score: 0.9184 time: 0.47s
Test loss: 0.5114 score: 0.8542 time: 0.47s
Epoch 63/1000, LR 0.000268
Train loss: 0.4583;  Loss pred: 0.4583; Loss self: 0.0000; time: 0.52s
Val loss: 0.4776 score: 0.9184 time: 0.46s
Test loss: 0.4987 score: 0.8542 time: 0.54s
Epoch 64/1000, LR 0.000268
Train loss: 0.4350;  Loss pred: 0.4350; Loss self: 0.0000; time: 0.52s
Val loss: 0.4635 score: 0.9184 time: 0.49s
Test loss: 0.4857 score: 0.8542 time: 0.44s
Epoch 65/1000, LR 0.000268
Train loss: 0.4246;  Loss pred: 0.4246; Loss self: 0.0000; time: 0.52s
Val loss: 0.4489 score: 0.9184 time: 0.57s
Test loss: 0.4723 score: 0.8542 time: 0.43s
Epoch 66/1000, LR 0.000268
Train loss: 0.4060;  Loss pred: 0.4060; Loss self: 0.0000; time: 0.55s
Val loss: 0.4340 score: 0.9184 time: 0.46s
Test loss: 0.4586 score: 0.8542 time: 0.43s
Epoch 67/1000, LR 0.000268
Train loss: 0.3849;  Loss pred: 0.3849; Loss self: 0.0000; time: 0.52s
Val loss: 0.4189 score: 0.9184 time: 0.57s
Test loss: 0.4446 score: 0.8750 time: 0.45s
Epoch 68/1000, LR 0.000268
Train loss: 0.3645;  Loss pred: 0.3645; Loss self: 0.0000; time: 0.52s
Val loss: 0.4035 score: 0.9184 time: 0.46s
Test loss: 0.4305 score: 0.8750 time: 0.44s
Epoch 69/1000, LR 0.000268
Train loss: 0.3475;  Loss pred: 0.3475; Loss self: 0.0000; time: 0.64s
Val loss: 0.3881 score: 0.9184 time: 0.46s
Test loss: 0.4167 score: 0.8750 time: 0.43s
Epoch 70/1000, LR 0.000268
Train loss: 0.3369;  Loss pred: 0.3369; Loss self: 0.0000; time: 0.53s
Val loss: 0.3728 score: 0.9184 time: 0.48s
Test loss: 0.4031 score: 0.8750 time: 0.56s
Epoch 71/1000, LR 0.000268
Train loss: 0.3143;  Loss pred: 0.3143; Loss self: 0.0000; time: 0.53s
Val loss: 0.3579 score: 0.9184 time: 0.47s
Test loss: 0.3902 score: 0.8750 time: 0.46s
Epoch 72/1000, LR 0.000267
Train loss: 0.2903;  Loss pred: 0.2903; Loss self: 0.0000; time: 0.53s
Val loss: 0.3433 score: 0.9184 time: 0.57s
Test loss: 0.3777 score: 0.8750 time: 0.43s
Epoch 73/1000, LR 0.000267
Train loss: 0.2747;  Loss pred: 0.2747; Loss self: 0.0000; time: 0.52s
Val loss: 0.3289 score: 0.9184 time: 0.46s
Test loss: 0.3651 score: 0.8750 time: 0.44s
Epoch 74/1000, LR 0.000267
Train loss: 0.2638;  Loss pred: 0.2638; Loss self: 0.0000; time: 0.62s
Val loss: 0.3150 score: 0.9184 time: 0.45s
Test loss: 0.3526 score: 0.8750 time: 0.44s
Epoch 75/1000, LR 0.000267
Train loss: 0.2490;  Loss pred: 0.2490; Loss self: 0.0000; time: 0.52s
Val loss: 0.3017 score: 0.9184 time: 0.46s
Test loss: 0.3406 score: 0.8750 time: 0.43s
Epoch 76/1000, LR 0.000267
Train loss: 0.2223;  Loss pred: 0.2223; Loss self: 0.0000; time: 0.61s
Val loss: 0.2891 score: 0.9184 time: 0.46s
Test loss: 0.3292 score: 0.8750 time: 0.43s
Epoch 77/1000, LR 0.000267
Train loss: 0.2183;  Loss pred: 0.2183; Loss self: 0.0000; time: 0.60s
Val loss: 0.2772 score: 0.9184 time: 0.61s
Test loss: 0.3184 score: 0.8750 time: 0.68s
Epoch 78/1000, LR 0.000267
Train loss: 0.2011;  Loss pred: 0.2011; Loss self: 0.0000; time: 0.65s
Val loss: 0.2663 score: 0.9184 time: 0.58s
Test loss: 0.3083 score: 0.8750 time: 0.55s
Epoch 79/1000, LR 0.000267
Train loss: 0.1889;  Loss pred: 0.1889; Loss self: 0.0000; time: 0.65s
Val loss: 0.2564 score: 0.9388 time: 0.69s
Test loss: 0.2998 score: 0.8750 time: 0.55s
Epoch 80/1000, LR 0.000267
Train loss: 0.1659;  Loss pred: 0.1659; Loss self: 0.0000; time: 0.62s
Val loss: 0.2475 score: 0.9388 time: 0.47s
Test loss: 0.2924 score: 0.8958 time: 0.44s
Epoch 81/1000, LR 0.000267
Train loss: 0.1622;  Loss pred: 0.1622; Loss self: 0.0000; time: 0.62s
Val loss: 0.2396 score: 0.9388 time: 0.45s
Test loss: 0.2866 score: 0.8750 time: 0.43s
Epoch 82/1000, LR 0.000267
Train loss: 0.1506;  Loss pred: 0.1506; Loss self: 0.0000; time: 0.53s
Val loss: 0.2331 score: 0.9388 time: 0.47s
Test loss: 0.2831 score: 0.8750 time: 0.58s
Epoch 83/1000, LR 0.000266
Train loss: 0.1442;  Loss pred: 0.1442; Loss self: 0.0000; time: 0.79s
Val loss: 0.2277 score: 0.9388 time: 0.59s
Test loss: 0.2813 score: 0.8750 time: 0.50s
Epoch 84/1000, LR 0.000266
Train loss: 0.1395;  Loss pred: 0.1395; Loss self: 0.0000; time: 0.60s
Val loss: 0.2235 score: 0.9388 time: 0.47s
Test loss: 0.2811 score: 0.8750 time: 0.55s
Epoch 85/1000, LR 0.000266
Train loss: 0.1290;  Loss pred: 0.1290; Loss self: 0.0000; time: 0.53s
Val loss: 0.2202 score: 0.9388 time: 0.46s
Test loss: 0.2824 score: 0.8750 time: 0.48s
Epoch 86/1000, LR 0.000266
Train loss: 0.1273;  Loss pred: 0.1273; Loss self: 0.0000; time: 0.52s
Val loss: 0.2175 score: 0.9388 time: 0.58s
Test loss: 0.2833 score: 0.8542 time: 0.45s
Epoch 87/1000, LR 0.000266
Train loss: 0.1209;  Loss pred: 0.1209; Loss self: 0.0000; time: 0.52s
Val loss: 0.2153 score: 0.9388 time: 0.51s
Test loss: 0.2840 score: 0.8542 time: 0.54s
Epoch 88/1000, LR 0.000266
Train loss: 0.1042;  Loss pred: 0.1042; Loss self: 0.0000; time: 0.54s
Val loss: 0.2138 score: 0.9388 time: 0.47s
Test loss: 0.2852 score: 0.8542 time: 0.45s
Epoch 89/1000, LR 0.000266
Train loss: 0.1128;  Loss pred: 0.1128; Loss self: 0.0000; time: 0.55s
Val loss: 0.2123 score: 0.9388 time: 0.56s
Test loss: 0.2828 score: 0.8750 time: 0.44s
Epoch 90/1000, LR 0.000266
Train loss: 0.1005;  Loss pred: 0.1005; Loss self: 0.0000; time: 0.57s
Val loss: 0.2118 score: 0.9388 time: 0.49s
Test loss: 0.2832 score: 0.8750 time: 0.45s
Epoch 91/1000, LR 0.000266
Train loss: 0.0950;  Loss pred: 0.0950; Loss self: 0.0000; time: 0.53s
Val loss: 0.2117 score: 0.9388 time: 0.56s
Test loss: 0.2841 score: 0.8750 time: 0.44s
Epoch 92/1000, LR 0.000266
Train loss: 0.0930;  Loss pred: 0.0930; Loss self: 0.0000; time: 0.52s
Val loss: 0.2124 score: 0.9388 time: 0.50s
Test loss: 0.2871 score: 0.8750 time: 0.45s
     INFO: Early stopping counter 1 of 20
Epoch 93/1000, LR 0.000265
Train loss: 0.0963;  Loss pred: 0.0963; Loss self: 0.0000; time: 0.60s
Val loss: 0.2131 score: 0.9388 time: 0.49s
Test loss: 0.2882 score: 0.8750 time: 0.45s
     INFO: Early stopping counter 2 of 20
Epoch 94/1000, LR 0.000265
Train loss: 0.0880;  Loss pred: 0.0880; Loss self: 0.0000; time: 0.55s
Val loss: 0.2141 score: 0.9388 time: 0.57s
Test loss: 0.2893 score: 0.8750 time: 0.45s
     INFO: Early stopping counter 3 of 20
Epoch 95/1000, LR 0.000265
Train loss: 0.0846;  Loss pred: 0.0846; Loss self: 0.0000; time: 0.52s
Val loss: 0.2154 score: 0.9388 time: 0.47s
Test loss: 0.2900 score: 0.8750 time: 0.50s
     INFO: Early stopping counter 4 of 20
Epoch 96/1000, LR 0.000265
Train loss: 0.0909;  Loss pred: 0.0909; Loss self: 0.0000; time: 0.53s
Val loss: 0.2171 score: 0.9388 time: 0.56s
Test loss: 0.2921 score: 0.8750 time: 0.44s
     INFO: Early stopping counter 5 of 20
Epoch 97/1000, LR 0.000265
Train loss: 0.0830;  Loss pred: 0.0830; Loss self: 0.0000; time: 0.52s
Val loss: 0.2190 score: 0.9388 time: 0.52s
Test loss: 0.2947 score: 0.8750 time: 0.47s
     INFO: Early stopping counter 6 of 20
Epoch 98/1000, LR 0.000265
Train loss: 0.0718;  Loss pred: 0.0718; Loss self: 0.0000; time: 0.62s
Val loss: 0.2213 score: 0.9388 time: 0.48s
Test loss: 0.2989 score: 0.8750 time: 0.44s
     INFO: Early stopping counter 7 of 20
Epoch 99/1000, LR 0.000265
Train loss: 0.0723;  Loss pred: 0.0723; Loss self: 0.0000; time: 0.57s
Val loss: 0.2241 score: 0.9388 time: 0.46s
Test loss: 0.3060 score: 0.8542 time: 0.53s
     INFO: Early stopping counter 8 of 20
Epoch 100/1000, LR 0.000265
Train loss: 0.0769;  Loss pred: 0.0769; Loss self: 0.0000; time: 0.57s
Val loss: 0.2277 score: 0.9184 time: 0.47s
Test loss: 0.3173 score: 0.8542 time: 0.49s
     INFO: Early stopping counter 9 of 20
Epoch 101/1000, LR 0.000265
Train loss: 0.0671;  Loss pred: 0.0671; Loss self: 0.0000; time: 0.58s
Val loss: 0.2316 score: 0.9184 time: 0.62s
Test loss: 0.3283 score: 0.8333 time: 0.48s
     INFO: Early stopping counter 10 of 20
Epoch 102/1000, LR 0.000264
Train loss: 0.0676;  Loss pred: 0.0676; Loss self: 0.0000; time: 0.52s
Val loss: 0.2367 score: 0.9184 time: 0.51s
Test loss: 0.3440 score: 0.8333 time: 0.48s
     INFO: Early stopping counter 11 of 20
Epoch 103/1000, LR 0.000264
Train loss: 0.0646;  Loss pred: 0.0646; Loss self: 0.0000; time: 0.62s
Val loss: 0.2425 score: 0.9184 time: 0.47s
Test loss: 0.3616 score: 0.8333 time: 0.46s
     INFO: Early stopping counter 12 of 20
Epoch 104/1000, LR 0.000264
Train loss: 0.0646;  Loss pred: 0.0646; Loss self: 0.0000; time: 0.61s
Val loss: 0.2486 score: 0.9184 time: 0.48s
Test loss: 0.3804 score: 0.8333 time: 0.45s
     INFO: Early stopping counter 13 of 20
Epoch 105/1000, LR 0.000264
Train loss: 0.0548;  Loss pred: 0.0548; Loss self: 0.0000; time: 0.64s
Val loss: 0.2557 score: 0.9184 time: 0.64s
Test loss: 0.4008 score: 0.8333 time: 0.48s
     INFO: Early stopping counter 14 of 20
Epoch 106/1000, LR 0.000264
Train loss: 0.0673;  Loss pred: 0.0673; Loss self: 0.0000; time: 0.53s
Val loss: 0.2609 score: 0.9184 time: 0.47s
Test loss: 0.4122 score: 0.8333 time: 0.55s
     INFO: Early stopping counter 15 of 20
Epoch 107/1000, LR 0.000264
Train loss: 0.0576;  Loss pred: 0.0576; Loss self: 0.0000; time: 0.54s
Val loss: 0.2667 score: 0.9184 time: 0.50s
Test loss: 0.4256 score: 0.8333 time: 0.46s
     INFO: Early stopping counter 16 of 20
Epoch 108/1000, LR 0.000264
Train loss: 0.0573;  Loss pred: 0.0573; Loss self: 0.0000; time: 0.52s
Val loss: 0.2712 score: 0.8980 time: 0.57s
Test loss: 0.4334 score: 0.8333 time: 0.44s
     INFO: Early stopping counter 17 of 20
Epoch 109/1000, LR 0.000264
Train loss: 0.0568;  Loss pred: 0.0568; Loss self: 0.0000; time: 0.56s
Val loss: 0.2757 score: 0.8980 time: 0.47s
Test loss: 0.4412 score: 0.8333 time: 0.43s
     INFO: Early stopping counter 18 of 20
Epoch 110/1000, LR 0.000263
Train loss: 0.0520;  Loss pred: 0.0520; Loss self: 0.0000; time: 0.64s
Val loss: 0.2797 score: 0.8980 time: 0.48s
Test loss: 0.4471 score: 0.8333 time: 0.46s
     INFO: Early stopping counter 19 of 20
Epoch 111/1000, LR 0.000263
Train loss: 0.0464;  Loss pred: 0.0464; Loss self: 0.0000; time: 0.59s
Val loss: 0.2837 score: 0.8980 time: 0.46s
Test loss: 0.4526 score: 0.8333 time: 0.54s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 090,   Train_Loss: 0.0950,   Val_Loss: 0.2117,   Val_Precision: 0.9583,   Val_Recall: 0.9200,   Val_accuracy: 0.9388,   Val_Score: 0.9388,   Val_Loss: 0.2117,   Test_Precision: 0.8462,   Test_Recall: 0.9167,   Test_accuracy: 0.8800,   Test_Score: 0.8750,   Test_loss: 0.2841


[0.4864854048937559, 0.49113618093542755, 0.4953845548443496, 0.5020810109563172, 0.6105500708799809, 0.6348829148337245, 0.5343367431778461, 0.5420319230761379, 0.5006377091631293, 0.6278654281049967, 0.4935796959325671, 0.5048661360051483, 0.5387552010361105, 0.5011575769167393, 0.48138835304416716, 0.5058298499789089, 0.6189252841286361, 0.5675822379998863, 0.5045047800522298, 0.5167771831620485, 0.5274420049972832, 0.5973227471113205, 0.5289010058622807, 0.5005101580172777, 0.4971687709912658, 0.49798980192281306, 0.5915795790497214, 0.7225857749581337, 0.507525707129389, 0.6121962869074196, 0.5273409918881953, 0.5514915038365871, 0.5755820339545608, 0.533705092035234, 0.5312240819912404, 0.5686415950767696, 0.4983961330726743, 0.5102405559737235, 0.5970565949101001, 0.49013955518603325, 0.6013896730728447, 0.5341293341480196, 0.49669445399194956, 0.489765761885792, 0.5007811170071363, 0.5026739991735667, 0.49545597890391946, 0.5373096771072596, 0.4870557989925146, 0.526107651880011, 0.4926560411695391, 0.48045027116313577, 0.61098856292665, 0.5133866069372743, 0.5947287168819457, 0.49296367494389415, 0.47831373708322644, 0.5191884539090097, 0.4918215519282967, 0.47907064110040665, 2.2969875768758357, 0.5215836688876152, 0.4966632251162082, 0.49362886720336974, 0.4835226060822606, 0.5234909809660167, 0.6037856419570744, 0.4927330380305648, 0.6021768630016595, 0.4938877490349114, 0.5004533131141216, 0.5501459510996938, 0.5184926339425147, 0.4998858200851828, 0.5293831909075379, 0.5204927180893719, 0.4875845620408654, 0.4864306871313602, 0.4761741200927645, 0.48746123211458325, 0.5891961390152574, 0.5962004538159817, 0.6400923039764166, 0.491470105946064, 0.48954105214215815, 0.6366601812187582, 0.49966930295340717, 0.5844131580088288, 0.4863780119922012, 0.6167535160202533, 0.6714696460403502, 0.6242299238219857, 0.6128077402245253, 0.5806076929438859, 0.6066815480589867, 0.5140152180101722, 0.610620005056262, 0.5063326428644359, 0.540864926064387, 0.5032452151644975, 0.511703854193911, 0.5517726959660649, 0.44460143987089396, 0.456339524127543, 0.5144379439298064, 0.44528860598802567, 0.4468949940055609, 0.5056479461491108, 0.44070236617699265, 0.4291487780865282, 0.46086748503148556, 0.4454697591718286, 0.5939233012031764, 0.43907107785344124, 0.451474976958707, 0.46642496692948043, 0.44743331195786595, 0.48038050695322454, 0.44183760113082826, 0.5430902831722051, 0.45372364413924515, 0.43019409105181694, 0.46258975099772215, 0.4552636919543147, 0.44946770602837205, 0.45550579903647304, 0.44427185296081007, 0.43511560815386474, 0.43167648487724364, 0.4460503200534731, 3.7720067780464888, 0.43889127601869404, 0.43064870894886553, 0.4434668810572475, 0.4430090528912842, 0.47687131119892, 0.4437255079392344, 0.436964601976797, 0.45143067883327603, 0.46202068380080163, 0.48478609998710454, 0.4401703921612352, 0.46875956282019615, 0.45193209196440876, 0.44315237109549344, 0.4331741838250309, 0.43698444007895887, 0.4500562238972634, 0.4404485737904906, 0.44521214812994003, 0.45730535592883825, 0.45279705501161516, 0.45503081008791924, 0.44586221408098936, 0.44567313301377, 0.4521394190378487, 0.4489766040351242, 0.43907519499771297, 0.45425764191895723, 0.4632914981339127, 0.46540283295325935, 0.44497258006595075, 0.4457428678870201, 0.4601561948657036, 0.4493715011049062, 0.4503388670273125, 0.45774362701922655, 0.5638352187816054, 0.45233415882103145, 0.5558108219411224, 0.5901306529995054, 0.471708060009405, 0.44953102711588144, 0.491190604865551, 0.45802738098427653, 0.45059118513017893, 0.43777240603230894, 0.4726888120640069, 0.6834313089493662, 0.4595549760852009, 0.4398831850849092, 0.43326869700104, 0.4454550398513675, 0.43697785399854183, 0.555471793981269, 0.4400386142078787, 0.4462576631922275, 0.510546371107921, 0.5602167558390647, 0.45843589794822037, 0.466014752862975, 0.5971240899525583, 0.5246033919975162, 0.448110185097903, 0.45371885714121163, 0.5612299989443272, 0.5568274520337582, 0.4668627539649606, 0.44835882005281746, 0.43422371707856655, 0.4658051359001547, 0.43419940606690943, 2.1524934829212725, 0.469568437198177, 0.44731462793424726, 0.4386712310370058, 0.4500220799818635, 0.4661734530236572, 0.45076700998470187, 0.450040060095489, 0.45231747697107494, 0.45175483310595155, 0.4811033180449158, 0.4617968259844929, 0.44798693200573325, 0.5629367339424789, 0.4433743979316205, 0.43651268910616636, 0.4484644818585366, 0.5544247480574995, 0.4356411120388657, 0.4532362390309572, 0.44827904482372105, 0.5655352880712599, 0.4472813280299306, 0.4477491721045226, 0.5893919940572232, 0.4450672408565879, 0.5447282940149307, 0.4597791361156851, 0.44371866690926254, 0.6076633259654045, 0.4514934360049665, 0.6872793750371784, 0.4621203129645437, 0.45541489706374705, 0.5833108318038285, 0.4411425879225135, 0.44397443206980824, 0.4483481200877577, 0.43681742809712887, 0.4473400309216231, 0.45963185001164675, 0.4428014240693301, 0.4586017341352999, 0.45422035083174706, 0.43915203609503806, 0.4512663120403886, 0.5687921680510044, 0.4492853169795126, 0.4587363169994205, 0.4532631889451295, 0.5595400070305914, 0.5746551889460534, 0.44693316402845085, 0.567308952100575, 0.46094931196421385, 0.47815670911222696, 0.5605997040402144, 0.4470960709732026, 0.44912107894197106, 0.4552152780815959, 0.5458653380628675, 0.4505823329091072, 0.47080386988818645, 0.4566590490285307, 0.46308434498496354, 0.5521602670196444, 0.45961091690696776, 0.4533591950312257, 0.4507040739990771, 0.4476781729608774, 2.454771837219596, 0.4870506271254271, 0.45245196716859937, 0.44764141691848636, 0.4706350048072636, 0.5453098411671817, 0.44896742096170783, 0.43816783907823265, 0.43700030515901744, 0.4544260399416089, 0.4477409648243338, 0.437206074828282, 0.5682546580210328, 0.4634696780703962, 0.43937563290819526, 0.44639622094109654, 0.4460335948970169, 0.43741261796094477, 0.43903573392890394, 0.6823868500068784, 0.5559503629337996, 0.5600942908786237, 0.4436449471395463, 0.4381628539413214, 0.5838417029008269, 0.503291147062555, 0.553750776918605, 0.486841554986313, 0.4527900549583137, 0.5438947489019483, 0.4548772759735584, 0.44161914102733135, 0.4581431320402771, 0.4493927869480103, 0.45238019502721727, 0.452895883936435, 0.4542449410073459, 0.5031580252107233, 0.4490471719764173, 0.47403770103119314, 0.44864927511662245, 0.5362956968601793, 0.4949609939940274, 0.48370355390943587, 0.4827417880296707, 0.46877537085674703, 0.45224300189875066, 0.4896669639274478, 0.5535691850818694, 0.4664689600467682, 0.44976117881014943, 0.4382586660794914, 0.46573743410408497, 0.5492574030067772]
[0.009928273569260324, 0.010023187366029133, 0.010109888874374482, 0.010246551244006473, 0.012460205528162877, 0.012956794180280092, 0.01090483149342543, 0.011061875981145672, 0.010217096105369985, 0.012813580165408095, 0.010073055019031982, 0.010303390530717313, 0.010995004102777766, 0.010227705651362027, 0.009824252102942186, 0.010323058162834875, 0.012631128247523186, 0.011583310979589517, 0.01029601591943326, 0.01054647312575609, 0.010764122550964964, 0.01219026014512899, 0.010793898078822056, 0.01021449302076077, 0.010146301448801342, 0.010163057182098225, 0.012073052633667782, 0.01474664846853334, 0.010357667492436511, 0.012493801773620807, 0.01076206105894276, 0.011254928649726267, 0.011746572121521649, 0.010891940653780284, 0.010841307795739599, 0.011604930511770807, 0.010171349654544373, 0.010413072570892317, 0.012184828467553064, 0.010002848065021087, 0.012273258634139689, 0.010900598656082032, 0.010136621510039787, 0.009995219630322285, 0.010220022796064007, 0.010258653044358504, 0.010111346508243255, 0.01096550361443387, 0.009939914265153359, 0.010736890854694101, 0.010054204921827329, 0.009805107574757874, 0.012469154345441838, 0.010477277692597436, 0.01213732075269277, 0.010060483162120288, 0.009761504838433193, 0.010595682732836932, 0.010037174529148912, 0.009776951859191972, 0.04687729748726195, 0.01064456467117582, 0.010135984186045065, 0.010074058514354485, 0.009867808287393073, 0.010683489407469727, 0.012322155958307641, 0.010055776286338056, 0.012289323734727745, 0.010079341817039008, 0.010213332920696358, 0.011227468389789668, 0.010581482325357442, 0.010201751430309852, 0.010803738589949754, 0.010622300369170855, 0.009950705347772764, 0.00992715688023184, 0.009717839185566622, 0.009948188410501699, 0.012024411000311375, 0.012167356200326157, 0.013063108244416664, 0.010030002162164571, 0.0099906337171869, 0.012993064922831801, 0.01019733271333484, 0.011926799143037322, 0.009926081877391862, 0.012586806449392925, 0.01370346216408878, 0.012739386200448689, 0.012506280412745416, 0.01184913659069155, 0.012381256082836462, 0.010490106490003516, 0.012461632756250245, 0.010333319242131345, 0.011038059715599733, 0.010270310513561174, 0.010442935799875734, 0.01126066726461357, 0.009073498772875386, 0.009313051512807, 0.010498733549587885, 0.009087522571184198, 0.009120306000113487, 0.01031934583977777, 0.008993925840346789, 0.008758138328296493, 0.009405458878193582, 0.009091219574935278, 0.012120883698024007, 0.008960634241906964, 0.009213775039973612, 0.009518876876111845, 0.009131292080772775, 0.00980368381537193, 0.009017093900629148, 0.011083475166779697, 0.00925966620692337, 0.008779471245955447, 0.009440607163218819, 0.009291095754169688, 0.009172810327109635, 0.009296036715030062, 0.009066772509404287, 0.008879910370487035, 0.008809724181168238, 0.00910306775619333, 0.07697973016421406, 0.008956964816708041, 0.008788749162221745, 0.009050344511372398, 0.009041001079413963, 0.009732067575488163, 0.009055622611004783, 0.00891764493830198, 0.00921287099659747, 0.009428993546955136, 0.009893593877287847, 0.00898306922778031, 0.009566521690208085, 0.009223103917640994, 0.009043925940724356, 0.008840289465816957, 0.008918049797529772, 0.00918482089586252, 0.008988746403887565, 0.00908596220673347, 0.009332762365894658, 0.00924075622472684, 0.00928634306301876, 0.009099228858795702, 0.00909537006150551, 0.009227335082405076, 0.009162787837451515, 0.008960718265259448, 0.009270564120795046, 0.009454928533345158, 0.00949801699904611, 0.009081073062570423, 0.009096793222184084, 0.009390942752361298, 0.009170846961324617, 0.009190589123006378, 0.009341706673861767, 0.011506841199624598, 0.009231309363694519, 0.011343077998798417, 0.01204348271427562, 0.009626695102232755, 0.009174102594201662, 0.010024298058480633, 0.009347497571107685, 0.009195738472044468, 0.008934130735353244, 0.009646710450285855, 0.013947577733660534, 0.00937867298133063, 0.008977207858875697, 0.008842218306143674, 0.009090919180640153, 0.008917915387725343, 0.011336159060842224, 0.008980379881793442, 0.009107299248820968, 0.010419313696080021, 0.011432995017123769, 0.009355834652004498, 0.009510505160468877, 0.012186205917399148, 0.010706191673418698, 0.009145105818324551, 0.009259568513085951, 0.011453673447843413, 0.011363825551709351, 0.009527811305407358, 0.009150180001077908, 0.008861708511807481, 0.009506227263268463, 0.008861212368712438, 0.04392843842696475, 0.009583029330575041, 0.009128869957841781, 0.008952474102796036, 0.009184124081262521, 0.00951374393925831, 0.00919932673438167, 0.00918449102235692, 0.00923096891777704, 0.009219486389917379, 0.009818435062141138, 0.009424425020091692, 0.009142590449096598, 0.011727848623801643, 0.009236966623575427, 0.009094014356378466, 0.009343010038719513, 0.01155051558453124, 0.009075856500809701, 0.009442421646478275, 0.009339146767160855, 0.011781985168151246, 0.009318361000623554, 0.009328107752177553, 0.01227899987619215, 0.009272234184512248, 0.011348506125311056, 0.009578732002410106, 0.009244138893942969, 0.01265965262427926, 0.009406113250103468, 0.01431832031327455, 0.009627506520094661, 0.00948781035549473, 0.012152308995913094, 0.009190470581719032, 0.009249467334787672, 0.009340585835161619, 0.009100363085356852, 0.009319583977533815, 0.009575663541909307, 0.009225029668111043, 0.009554202794485414, 0.009462923975661397, 0.00914900075197996, 0.00940138150084143, 0.011849836834395925, 0.009360110770406513, 0.009557006604154594, 0.009442983103023531, 0.011657083479803987, 0.01197198310304278, 0.00931110758392606, 0.011818936502095312, 0.009603110665921122, 0.009961598106504729, 0.011679160500837801, 0.009314501478608387, 0.009356689144624397, 0.009483651626699915, 0.011372194542976407, 0.0093871319356064, 0.009808413956003884, 0.00951373018809439, 0.009647590520520074, 0.01150333889624259, 0.009575227435561828, 0.009444983229817202, 0.009389668208314106, 0.009326628603351613, 0.05114107994207492, 0.010146888065113066, 0.00942608264934582, 0.009325862852468466, 0.009804895933484659, 0.011360621690982953, 0.009353487936702246, 0.00912849664746318, 0.009104173024146197, 0.009467209165450186, 0.00932793676717362, 0.009108459892255874, 0.011838638708771517, 0.009655618293133253, 0.009153659018920735, 0.009299921269606179, 0.009292366560354518, 0.00911276287418635, 0.009146577790185498, 0.014216392708476633, 0.011582299227787493, 0.011668631059971327, 0.009242603065407215, 0.009128392790444195, 0.012163368810433894, 0.010485232230469895, 0.011536474519137604, 0.010142532395548187, 0.009433126144964868, 0.011331140602123924, 0.0094766099161158, 0.009200398771402737, 0.00954464858417244, 0.009362349728083549, 0.00942458739640036, 0.009435330915342396, 0.009463436270986373, 0.010482458858556734, 0.00935514941617536, 0.009875785438149856, 0.009346859898262968, 0.011172827017920403, 0.01031168737487557, 0.010077157373113247, 0.010057120583951473, 0.009766153559515564, 0.009421729206223972, 0.010201395081821829, 0.011532691355872279, 0.00971810333430767, 0.00937002455854478, 0.00913038887665607, 0.00970286321050177, 0.01144286256264119]
[100.72244615581256, 99.76866274985818, 98.913055566288, 97.59381241419459, 80.25549801243442, 77.17958517254016, 91.70247156986372, 90.40057958563649, 97.87516821677067, 78.04220109377614, 99.27474813853442, 97.05543015366815, 90.95039807646467, 97.77363898489097, 101.78891884304537, 96.87051881584905, 79.16949146613946, 86.33110185525189, 97.12494695278643, 94.81842774129372, 92.90120911066306, 82.03270382212328, 92.64493630545107, 97.900110947016, 98.55808099592166, 98.39558924862251, 82.82909305069441, 67.8120185840069, 96.54683361193347, 80.03968832860646, 92.9190045032357, 88.84996352458629, 85.13121867849735, 91.81100336356756, 92.23979420573028, 86.17027038513555, 98.31536953930379, 96.03313461919991, 82.06927185416656, 99.97152745895396, 81.4779538026167, 91.73808077431106, 98.65219876361695, 100.04782655964068, 97.84713986989595, 97.478684158241, 98.89879643475298, 91.19508188239544, 100.60448946785472, 93.1368320245899, 99.46087311479347, 101.9876622847449, 80.19790053890503, 95.44464023384005, 82.39050614017441, 99.39880459868945, 102.44322126059703, 94.37806182143544, 99.62963153583756, 102.28136687201058, 21.332287772598914, 93.94465916561883, 98.65840175409622, 99.26485920000407, 101.33962586986831, 93.60237670108192, 81.1546293833261, 99.44533087501324, 81.37144252894512, 99.21282740005026, 97.91123110983625, 89.06727369719485, 94.50471769948545, 98.02238437499624, 92.56055130121867, 94.1415668212795, 100.49538852276709, 100.73377625282838, 102.9035345105572, 100.5208143167414, 83.16415664551926, 82.18712294896027, 76.55145936859341, 99.7008758155831, 100.09375063762958, 76.96413478568634, 98.06485951883484, 83.84479255557719, 100.74468580373589, 79.44827022014239, 72.97425920732607, 78.49671752354752, 79.95982554340289, 84.39433475562942, 80.76724956737239, 95.32791692371704, 80.24630636771421, 96.77432551611949, 90.59563236342454, 97.36803952320382, 95.75851266000309, 88.80468417200113, 110.21106907397537, 107.37619121131598, 95.24958370233654, 110.04099215895425, 109.64544391246923, 96.90536740665483, 111.18615138163536, 114.17951652683082, 106.3212346096675, 109.99624327158759, 82.50223539089181, 111.59924320124736, 108.53314690900723, 105.0544106216518, 109.513526799306, 102.0024736448584, 110.90047536604085, 90.22440930776679, 107.99525357105301, 113.90207587509133, 105.92539046599258, 107.6299315450728, 109.0178434241212, 107.57272487781543, 110.29283010716047, 112.61374926976353, 113.51093171992954, 109.85307665315835, 12.990432648526935, 111.6449623799606, 113.78183419985167, 110.49303136951629, 110.607220507579, 102.75308841038742, 110.42863014021331, 112.13722983126657, 108.54379708229102, 106.05585792588921, 101.07550526160593, 111.32052694278268, 104.53120082543279, 108.42336906638381, 110.5714494517308, 113.1184678812536, 112.13213905544595, 108.87528579359338, 111.25021833606382, 110.05988988804235, 107.14941201700017, 108.21625153622753, 107.68501585756889, 109.89942285420786, 109.94604873003651, 108.37365187992644, 109.13708990539438, 111.59819675136791, 107.86830088978877, 105.76494539046502, 105.28513479186556, 110.11914485323469, 109.92884806497845, 106.48558151933743, 109.04118280647465, 108.80695313608854, 107.04682076970099, 86.90482319618907, 108.32699464421196, 88.15949252098336, 83.03246027120213, 103.87780950578421, 109.00248713503959, 99.75760837977003, 106.98050386136654, 108.74602437205593, 111.93030744926315, 103.66228002317283, 71.69703722723405, 106.62489266771743, 111.39320997355625, 113.09379223369646, 109.99987791438966, 112.1338290982671, 88.21329999278474, 111.35386399715344, 109.80203600199697, 95.97561117448862, 87.46614500419618, 106.88517242935123, 105.14688579914497, 82.05999527483989, 93.40389472783279, 109.34810595589228, 107.99639298383768, 87.30823386520485, 87.99853495195372, 104.95589888860056, 109.28746755606974, 112.84505675938045, 105.19420294778189, 112.85137500268523, 22.764296565255698, 104.35113631651525, 109.54258354189733, 111.70096539990884, 108.88354634060347, 105.11109047969182, 108.70360721753565, 108.87919619778566, 108.33098983511857, 108.46591205922499, 101.8492248174962, 106.10726891753347, 109.37819052135411, 85.2671305775982, 108.2606488419812, 109.96243911783671, 107.03188756683097, 86.57622187352631, 110.18243841898393, 105.90503553428674, 107.07616283709017, 84.87534025277623, 107.31500957443946, 107.20287828649492, 81.43985748700169, 107.84887224595089, 88.11732477895548, 104.3979516024031, 108.17665241434541, 78.99110897262334, 106.31383797010947, 69.8405942960291, 103.86905455870496, 105.39839673554017, 82.28888850146149, 108.80835655892552, 108.11433391833863, 107.05966602604398, 109.88572550572988, 107.30092699530822, 104.43140526224133, 108.40073538806995, 104.66598014616034, 105.67558215325369, 109.30155402857382, 106.36734610870748, 84.38934763197332, 106.83634248877267, 104.63527351391417, 105.89873868140374, 85.78475068249361, 83.52835043225559, 107.39860870325663, 84.60998160221233, 104.13292471456496, 100.3854993253562, 85.62259247385678, 107.35947622066435, 106.87541122112833, 105.44461557240649, 87.9337753342965, 106.52881059516086, 101.9532826087427, 105.11124240746426, 103.6528237670366, 86.93128221464781, 104.43616161910255, 105.87631292378208, 106.50003576426134, 107.21988003689141, 19.553752113421396, 98.55238311322174, 106.08860936196024, 107.22868391049839, 101.98986371542244, 88.02335182006023, 106.9119890641105, 109.54706329194974, 109.83974023206589, 105.62774969094579, 107.20484336033954, 109.78804450247537, 84.46917121130465, 103.5666458264165, 109.24593082755068, 107.52779201133461, 107.61521228256932, 109.73620336733353, 109.33050840862299, 70.34133204576871, 86.33864315997515, 85.6998558665936, 108.19462795527308, 109.5483096484216, 82.21406549328563, 95.37223191814657, 86.68159396019313, 98.59470603603151, 106.00939546788221, 88.25236885795582, 105.52296748011258, 108.69094099576026, 104.77075097958718, 106.81079312817955, 106.10544079435684, 105.98462406590764, 105.66986149268696, 95.39746480223094, 106.89300143843424, 101.25776894028404, 106.98780241542309, 89.50286247125035, 96.97733878516335, 99.23433394700066, 99.4320383903657, 102.39445795173465, 106.13762910309525, 98.0258084290775, 86.7100288338866, 102.90073747926876, 106.72330619326637, 109.5243601898194, 103.06236193432706, 87.39072015640677]
Elapsed: 0.5203440210366325~0.25651694121460056
Time per graph: 0.01069166560079757~0.005251740898178587
Speed: 98.78795544253764~13.619075813162684
Total Time: 0.5507
best val loss: 0.21173055469989777 test_score: 0.8750

Testing...
Test loss: 0.6580 score: 0.8750 time: 0.66s
test Score 0.8750
Epoch Time List: [1.4255824720021337, 1.5222562558483332, 1.4426324090454727, 1.5320483681280166, 1.55734296515584, 1.903188626980409, 1.7200794769451022, 1.653433856088668, 1.5390314583200961, 1.640339040895924, 1.4589809651952237, 1.5963708308991045, 1.4823978361673653, 1.5319831580854952, 1.444203456165269, 1.5459866570308805, 1.7201996040530503, 1.5289458658080548, 1.5466622998937964, 1.5089990419801325, 1.5711643958929926, 1.5753722609952092, 1.5588925208430737, 1.5579999298788607, 1.455913363955915, 1.5259987330064178, 1.558732729172334, 1.6577531530056149, 1.5436185549478978, 1.6461542018223554, 1.6279381667263806, 1.5393257848918438, 1.7408441409934312, 1.6649830632377416, 1.5234598109964281, 1.6733914173673838, 1.4774036633316427, 1.5350412828847766, 1.5347840518224984, 1.428236443316564, 1.5300076438579708, 1.4614985301159322, 1.518954827915877, 1.630490595009178, 1.5179133960045874, 1.4266291388776153, 1.547051940113306, 1.5782396546564996, 1.4756837890017778, 1.551181532908231, 1.4755929179955274, 1.6319930718746036, 1.5317248387727886, 1.524037712952122, 1.5191255428362638, 1.4380068997852504, 1.54425815702416, 1.4389671250246465, 1.503510129172355, 1.443460094043985, 6.540724294260144, 1.5373938435222954, 1.4058926939032972, 1.5341839049942791, 1.4087467570789158, 1.5676907000597566, 1.5248551708646119, 1.412041988922283, 1.5550116691738367, 1.4350474560633302, 1.5916957091540098, 1.515143149998039, 1.5812988481484354, 1.4471317548304796, 1.5800200041849166, 1.5642805979587138, 1.5310444151982665, 1.4868465987965465, 1.385019610170275, 1.5029891040176153, 1.4975734760519117, 1.4922171228099614, 1.8303703931160271, 1.427939127665013, 1.5526115843094885, 1.69113396294415, 1.552649380872026, 1.5259701379109174, 1.5351306069642305, 1.6901484613772482, 1.8062090743333101, 1.8815599018707871, 1.782329934882, 1.6197480051778257, 1.5437549790367484, 1.531954019330442, 1.5607598840724677, 1.4875124949030578, 1.600602338090539, 1.4565821902360767, 1.5939592670183629, 1.4979691519401968, 1.6077668440993875, 1.5725108671467751, 1.53508360683918, 1.5555220569949597, 1.5459882891736925, 1.5302116076927632, 1.5756620930042118, 1.513675111811608, 1.6127136761788279, 1.5294916811399162, 1.5872447069268674, 1.5332346437498927, 1.442862887866795, 1.587952665053308, 1.5470736511051655, 1.4642254831269383, 1.5504791047424078, 1.5612724428065121, 1.5566350149456412, 1.5646709038410336, 1.4641822648700327, 1.628198733087629, 1.4725357010029256, 1.5172727580647916, 1.5199428552296013, 1.3966517930384725, 1.540693286107853, 1.4036461061332375, 4.958284031832591, 2.860381186939776, 1.438596862833947, 1.5288138641044497, 1.4582275690045208, 1.564861198188737, 1.518652540864423, 1.4416694520041347, 1.5246414481662214, 1.5126095777377486, 1.6006395360454917, 1.5061168600805104, 1.446309180231765, 1.515731506049633, 1.4331362061202526, 1.507474402198568, 1.509563238127157, 1.4405441801063716, 1.5123269730247557, 1.426482129143551, 1.549041188089177, 1.5404402788262814, 1.4421424339525402, 1.5477861592080444, 1.4656489258632064, 1.5399085709359497, 1.6199209990445524, 1.4806437829975039, 1.5346217502374202, 1.5941849218215793, 1.5943590439856052, 1.6210888780187815, 1.4547672348562628, 1.533165494678542, 1.4722768699284643, 1.5532072728965431, 1.5901080730836838, 1.575966291828081, 1.7518489800859243, 1.7701105931773782, 1.9242598642595112, 1.7849413016811013, 1.4740828857757151, 1.5913067392539233, 1.5747354628983885, 1.571642881957814, 1.536124114645645, 1.5660148807801306, 1.8328316938132048, 1.540830596582964, 1.539579503936693, 1.4991183916572481, 1.4281690868083388, 1.549961781129241, 1.7594366241246462, 1.738784645916894, 1.5218870849348605, 1.47778022964485, 1.9213254118803889, 1.698578838026151, 1.595566175179556, 1.9459290658123791, 1.7442039530724287, 1.5149797282647341, 1.4784372809808701, 1.728898314991966, 1.9601751998998225, 1.7408385032322258, 1.5388125120662153, 1.454643297707662, 1.6073534484021366, 1.4915594777558, 6.43701257603243, 1.5700398141052574, 1.4431158569641411, 1.558459729887545, 1.556632287800312, 1.459947454975918, 1.5254675219766796, 1.501413342077285, 1.5527259402442724, 1.5394169979263097, 1.4776353628840297, 1.5467749149538577, 1.4762494396418333, 1.7875721603631973, 1.5945278950966895, 1.4165642659645528, 1.5212781659793109, 1.5373070589266717, 1.417993824928999, 1.5253058350645006, 1.4420250449329615, 1.6503192831296474, 1.4274839400313795, 1.7547432349529117, 1.6205202611163259, 1.4294726294465363, 1.6458627276588231, 1.4375621639192104, 1.7343183858320117, 1.5864587370306253, 1.478865847690031, 1.6798520998563617, 1.458966108970344, 1.6198792790528387, 1.5812942667398602, 1.403772229095921, 1.5677654228638858, 1.4408819687087089, 1.627300513908267, 1.422623883932829, 1.5588401718996465, 1.5764091149903834, 1.448407705873251, 1.6697424470912665, 1.4541023473720998, 1.5429603029042482, 1.555624894797802, 1.4275298400316387, 1.5790586140938103, 1.4278515020851046, 1.911671529756859, 1.8037612012121826, 1.588901940267533, 1.5594745930284262, 1.5884147556498647, 1.5947799501009285, 1.6768860600423068, 1.6831768148113042, 1.5138679849915206, 1.5816593593917787, 1.5167043029796332, 1.4825483309105039, 1.5778688872233033, 1.531748423120007, 1.5963629418984056, 1.5303021930158138, 1.6505642116535455, 1.7193695467431098, 1.4281858508475125, 1.5471109231002629, 6.7028239069040865, 1.4909157920628786, 1.5268519017845392, 1.455334970029071, 1.5553570769261569, 1.5248582600615919, 1.4479813429061323, 1.5206724538002163, 1.4370661238208413, 1.5412684597540647, 1.4235133021138608, 1.5371502707712352, 1.569060395937413, 1.4612943299580365, 1.5362915222067386, 1.4217399631161243, 1.5097203759942204, 1.4165336801670492, 1.5047800240572542, 1.8892507369164377, 1.780862675048411, 1.8878570462111384, 1.5286970720626414, 1.5076534010004252, 1.578391880961135, 1.8766371246892959, 1.6190034649334848, 1.4664309527724981, 1.5550181588623673, 1.5702002581674606, 1.458495567087084, 1.5478312082123011, 1.5083455319982022, 1.5264983219094574, 1.4700524318031967, 1.5460347011685371, 1.567023848881945, 1.4932342302054167, 1.5288997408933938, 1.5116700001526624, 1.5447220040950924, 1.5677861461881548, 1.5282913639675826, 1.6692328958306462, 1.5087268168572336, 1.5482819031458348, 1.5284914383664727, 1.7635156828910112, 1.5486813269089907, 1.5066486441064626, 1.5422138748690486, 1.4629372062627226, 1.5807940419763327, 1.5974620110355318]
Total Epoch List: [102, 113, 111]
Total Time List: [0.552347867982462, 0.44884158694185317, 0.5506876851432025]
========================training times:2========================
========================k_idx:0========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: FAGNN
FAGNN(
  (conv1): NeighborFilterConv()
  (conv2): NeighborFilterConv()
  (ffn): Sequential(
    (0): Linear(in_features=14951, out_features=64, bias=True)
    (1): ReLU()
    (2): Linear(in_features=64, out_features=64, bias=True)
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=64, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=64, out_features=2, bias=True)
  )
)
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7ed848759bd0>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.6938;  Loss pred: 0.6938; Loss self: 0.0000; time: 0.45s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6946 score: 0.4898 time: 0.53s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6932 score: 0.5102 time: 0.47s
Epoch 2/1000, LR 0.000000
Train loss: 0.6938;  Loss pred: 0.6938; Loss self: 0.0000; time: 0.45s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6946 score: 0.4898 time: 0.62s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6932 score: 0.5102 time: 0.45s
Epoch 3/1000, LR 0.000030
Train loss: 0.6938;  Loss pred: 0.6938; Loss self: 0.0000; time: 0.46s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6945 score: 0.4898 time: 0.54s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6931 score: 0.5102 time: 0.44s
Epoch 4/1000, LR 0.000060
Train loss: 0.6937;  Loss pred: 0.6937; Loss self: 0.0000; time: 0.45s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6944 score: 0.4898 time: 0.61s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6931 score: 0.5102 time: 0.55s
Epoch 5/1000, LR 0.000090
Train loss: 0.6937;  Loss pred: 0.6937; Loss self: 0.0000; time: 0.55s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6944 score: 0.4898 time: 0.60s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6931 score: 0.5102 time: 0.55s
Epoch 6/1000, LR 0.000120
Train loss: 0.6936;  Loss pred: 0.6936; Loss self: 0.0000; time: 0.46s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6942 score: 0.4898 time: 0.52s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6930 score: 0.5102 time: 0.46s
Epoch 7/1000, LR 0.000150
Train loss: 0.6935;  Loss pred: 0.6935; Loss self: 0.0000; time: 0.46s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6941 score: 0.4898 time: 0.52s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6930 score: 0.5102 time: 0.57s
Epoch 8/1000, LR 0.000180
Train loss: 0.6934;  Loss pred: 0.6934; Loss self: 0.0000; time: 0.45s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6939 score: 0.4898 time: 0.51s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6929 score: 0.5102 time: 0.44s
Epoch 9/1000, LR 0.000210
Train loss: 0.6933;  Loss pred: 0.6933; Loss self: 0.0000; time: 0.45s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6938 score: 0.4898 time: 0.65s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6928 score: 0.5102 time: 0.46s
Epoch 10/1000, LR 0.000240
Train loss: 0.6932;  Loss pred: 0.6932; Loss self: 0.0000; time: 0.44s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6936 score: 0.4898 time: 0.55s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6927 score: 0.5102 time: 0.45s
Epoch 11/1000, LR 0.000270
Train loss: 0.6931;  Loss pred: 0.6931; Loss self: 0.0000; time: 0.46s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6934 score: 0.4898 time: 0.75s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6926 score: 0.5102 time: 0.45s
Epoch 12/1000, LR 0.000270
Train loss: 0.6930;  Loss pred: 0.6930; Loss self: 0.0000; time: 0.44s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6932 score: 0.4898 time: 0.51s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6925 score: 0.5102 time: 0.46s
Epoch 13/1000, LR 0.000270
Train loss: 0.6928;  Loss pred: 0.6928; Loss self: 0.0000; time: 0.47s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6930 score: 0.4898 time: 0.65s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6924 score: 0.5102 time: 0.56s
Epoch 14/1000, LR 0.000270
Train loss: 0.6927;  Loss pred: 0.6927; Loss self: 0.0000; time: 0.55s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6928 score: 0.4898 time: 0.65s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6923 score: 0.5102 time: 0.69s
Epoch 15/1000, LR 0.000270
Train loss: 0.6925;  Loss pred: 0.6925; Loss self: 0.0000; time: 0.53s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6926 score: 0.4898 time: 0.51s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6922 score: 0.5102 time: 0.46s
Epoch 16/1000, LR 0.000270
Train loss: 0.6924;  Loss pred: 0.6924; Loss self: 0.0000; time: 0.45s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6923 score: 0.4898 time: 0.55s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6920 score: 0.5102 time: 0.55s
Epoch 17/1000, LR 0.000270
Train loss: 0.6922;  Loss pred: 0.6922; Loss self: 0.0000; time: 0.44s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6921 score: 0.4898 time: 0.50s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6919 score: 0.5102 time: 0.43s
Epoch 18/1000, LR 0.000270
Train loss: 0.6919;  Loss pred: 0.6919; Loss self: 0.0000; time: 0.46s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6918 score: 0.4898 time: 0.59s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6917 score: 0.5102 time: 0.47s
Epoch 19/1000, LR 0.000270
Train loss: 0.6917;  Loss pred: 0.6917; Loss self: 0.0000; time: 0.45s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6915 score: 0.4898 time: 0.51s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6915 score: 0.5102 time: 0.55s
Epoch 20/1000, LR 0.000270
Train loss: 0.6914;  Loss pred: 0.6914; Loss self: 0.0000; time: 1.31s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6911 score: 0.4898 time: 3.90s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6912 score: 0.5102 time: 2.77s
Epoch 21/1000, LR 0.000270
Train loss: 0.6910;  Loss pred: 0.6910; Loss self: 0.0000; time: 0.44s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6907 score: 0.4898 time: 0.61s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6910 score: 0.5102 time: 0.44s
Epoch 22/1000, LR 0.000270
Train loss: 0.6906;  Loss pred: 0.6906; Loss self: 0.0000; time: 0.46s
Val loss: 0.6903 score: 0.5918 time: 0.50s
Test loss: 0.6907 score: 0.5510 time: 0.44s
Epoch 23/1000, LR 0.000270
Train loss: 0.6903;  Loss pred: 0.6903; Loss self: 0.0000; time: 0.53s
Val loss: 0.6898 score: 0.7143 time: 0.51s
Test loss: 0.6903 score: 0.6939 time: 0.48s
Epoch 24/1000, LR 0.000270
Train loss: 0.6898;  Loss pred: 0.6898; Loss self: 0.0000; time: 0.43s
Val loss: 0.6892 score: 0.7347 time: 0.52s
Test loss: 0.6899 score: 0.7959 time: 0.54s
Epoch 25/1000, LR 0.000270
Train loss: 0.6892;  Loss pred: 0.6892; Loss self: 0.0000; time: 0.43s
Val loss: 0.6886 score: 0.8776 time: 0.67s
Test loss: 0.6894 score: 0.7959 time: 0.43s
Epoch 26/1000, LR 0.000270
Train loss: 0.6887;  Loss pred: 0.6887; Loss self: 0.0000; time: 0.43s
Val loss: 0.6879 score: 0.9184 time: 0.50s
Test loss: 0.6888 score: 0.8163 time: 0.54s
Epoch 27/1000, LR 0.000270
Train loss: 0.6878;  Loss pred: 0.6878; Loss self: 0.0000; time: 0.47s
Val loss: 0.6871 score: 0.9184 time: 0.50s
Test loss: 0.6882 score: 0.8776 time: 0.44s
Epoch 28/1000, LR 0.000270
Train loss: 0.6869;  Loss pred: 0.6869; Loss self: 0.0000; time: 0.42s
Val loss: 0.6861 score: 0.9184 time: 0.51s
Test loss: 0.6874 score: 0.8776 time: 0.56s
Epoch 29/1000, LR 0.000270
Train loss: 0.6861;  Loss pred: 0.6861; Loss self: 0.0000; time: 0.51s
Val loss: 0.6850 score: 0.9184 time: 0.51s
Test loss: 0.6865 score: 0.8776 time: 0.44s
Epoch 30/1000, LR 0.000270
Train loss: 0.6846;  Loss pred: 0.6846; Loss self: 0.0000; time: 0.43s
Val loss: 0.6838 score: 0.9184 time: 0.62s
Test loss: 0.6855 score: 0.8776 time: 0.53s
Epoch 31/1000, LR 0.000270
Train loss: 0.6838;  Loss pred: 0.6838; Loss self: 0.0000; time: 0.44s
Val loss: 0.6824 score: 0.9184 time: 0.52s
Test loss: 0.6844 score: 0.8776 time: 0.46s
Epoch 32/1000, LR 0.000270
Train loss: 0.6821;  Loss pred: 0.6821; Loss self: 0.0000; time: 0.53s
Val loss: 0.6809 score: 0.9184 time: 0.54s
Test loss: 0.6831 score: 0.8776 time: 0.44s
Epoch 33/1000, LR 0.000270
Train loss: 0.6807;  Loss pred: 0.6807; Loss self: 0.0000; time: 0.44s
Val loss: 0.6792 score: 0.9184 time: 0.52s
Test loss: 0.6817 score: 0.8776 time: 0.54s
Epoch 34/1000, LR 0.000270
Train loss: 0.6783;  Loss pred: 0.6783; Loss self: 0.0000; time: 0.46s
Val loss: 0.6773 score: 0.9184 time: 0.50s
Test loss: 0.6802 score: 0.8776 time: 0.44s
Epoch 35/1000, LR 0.000270
Train loss: 0.6763;  Loss pred: 0.6763; Loss self: 0.0000; time: 0.44s
Val loss: 0.6751 score: 0.9184 time: 0.52s
Test loss: 0.6784 score: 0.8776 time: 0.54s
Epoch 36/1000, LR 0.000270
Train loss: 0.6744;  Loss pred: 0.6744; Loss self: 0.0000; time: 0.43s
Val loss: 0.6727 score: 0.9184 time: 0.50s
Test loss: 0.6764 score: 0.8776 time: 0.46s
Epoch 37/1000, LR 0.000270
Train loss: 0.6725;  Loss pred: 0.6725; Loss self: 0.0000; time: 0.44s
Val loss: 0.6700 score: 0.9184 time: 0.60s
Test loss: 0.6741 score: 0.8776 time: 0.44s
Epoch 38/1000, LR 0.000270
Train loss: 0.6700;  Loss pred: 0.6700; Loss self: 0.0000; time: 0.43s
Val loss: 0.6670 score: 0.9184 time: 0.53s
Test loss: 0.6716 score: 0.8776 time: 0.45s
Epoch 39/1000, LR 0.000269
Train loss: 0.6662;  Loss pred: 0.6662; Loss self: 0.0000; time: 0.44s
Val loss: 0.6637 score: 0.9388 time: 0.61s
Test loss: 0.6688 score: 0.8776 time: 0.45s
Epoch 40/1000, LR 0.000269
Train loss: 0.6617;  Loss pred: 0.6617; Loss self: 0.0000; time: 0.43s
Val loss: 0.6600 score: 0.9388 time: 0.49s
Test loss: 0.6657 score: 0.8776 time: 0.45s
Epoch 41/1000, LR 0.000269
Train loss: 0.6582;  Loss pred: 0.6582; Loss self: 0.0000; time: 0.44s
Val loss: 0.6560 score: 0.9388 time: 0.60s
Test loss: 0.6623 score: 0.8776 time: 0.46s
Epoch 42/1000, LR 0.000269
Train loss: 0.6544;  Loss pred: 0.6544; Loss self: 0.0000; time: 0.44s
Val loss: 0.6516 score: 0.9388 time: 0.51s
Test loss: 0.6586 score: 0.8776 time: 0.57s
Epoch 43/1000, LR 0.000269
Train loss: 0.6490;  Loss pred: 0.6490; Loss self: 0.0000; time: 0.44s
Val loss: 0.6468 score: 0.9388 time: 0.55s
Test loss: 0.6546 score: 0.8776 time: 0.45s
Epoch 44/1000, LR 0.000269
Train loss: 0.6428;  Loss pred: 0.6428; Loss self: 0.0000; time: 0.44s
Val loss: 0.6416 score: 0.9388 time: 0.52s
Test loss: 0.6502 score: 0.8980 time: 0.56s
Epoch 45/1000, LR 0.000269
Train loss: 0.6379;  Loss pred: 0.6379; Loss self: 0.0000; time: 0.47s
Val loss: 0.6361 score: 0.9388 time: 0.51s
Test loss: 0.6454 score: 0.8980 time: 0.44s
Epoch 46/1000, LR 0.000269
Train loss: 0.6320;  Loss pred: 0.6320; Loss self: 0.0000; time: 0.43s
Val loss: 0.6301 score: 0.9388 time: 0.63s
Test loss: 0.6402 score: 0.8980 time: 0.47s
Epoch 47/1000, LR 0.000269
Train loss: 0.6273;  Loss pred: 0.6273; Loss self: 0.0000; time: 0.45s
Val loss: 0.6238 score: 0.9388 time: 0.51s
Test loss: 0.6346 score: 0.8980 time: 0.44s
Epoch 48/1000, LR 0.000269
Train loss: 0.6178;  Loss pred: 0.6178; Loss self: 0.0000; time: 0.47s
Val loss: 0.6171 score: 0.9388 time: 0.60s
Test loss: 0.6288 score: 0.8980 time: 0.48s
Epoch 49/1000, LR 0.000269
Train loss: 0.6124;  Loss pred: 0.6124; Loss self: 0.0000; time: 0.44s
Val loss: 0.6099 score: 0.9388 time: 0.51s
Test loss: 0.6226 score: 0.8980 time: 0.46s
Epoch 50/1000, LR 0.000269
Train loss: 0.6058;  Loss pred: 0.6058; Loss self: 0.0000; time: 0.43s
Val loss: 0.6022 score: 0.9388 time: 0.65s
Test loss: 0.6160 score: 0.8980 time: 0.48s
Epoch 51/1000, LR 0.000269
Train loss: 0.5936;  Loss pred: 0.5936; Loss self: 0.0000; time: 0.44s
Val loss: 0.5940 score: 0.9388 time: 0.53s
Test loss: 0.6089 score: 0.8980 time: 0.54s
Epoch 52/1000, LR 0.000269
Train loss: 0.5850;  Loss pred: 0.5850; Loss self: 0.0000; time: 0.48s
Val loss: 0.5853 score: 0.9388 time: 0.56s
Test loss: 0.6014 score: 0.8980 time: 0.50s
Epoch 53/1000, LR 0.000269
Train loss: 0.5775;  Loss pred: 0.5775; Loss self: 0.0000; time: 0.56s
Val loss: 0.5761 score: 0.9388 time: 0.53s
Test loss: 0.5935 score: 0.8980 time: 0.59s
Epoch 54/1000, LR 0.000269
Train loss: 0.5671;  Loss pred: 0.5671; Loss self: 0.0000; time: 0.46s
Val loss: 0.5664 score: 0.9388 time: 0.52s
Test loss: 0.5849 score: 0.8980 time: 0.47s
Epoch 55/1000, LR 0.000269
Train loss: 0.5524;  Loss pred: 0.5524; Loss self: 0.0000; time: 0.46s
Val loss: 0.5562 score: 0.9388 time: 0.66s
Test loss: 0.5758 score: 0.8980 time: 0.45s
Epoch 56/1000, LR 0.000269
Train loss: 0.5430;  Loss pred: 0.5430; Loss self: 0.0000; time: 0.45s
Val loss: 0.5453 score: 0.9388 time: 0.52s
Test loss: 0.5662 score: 0.8980 time: 0.56s
Epoch 57/1000, LR 0.000269
Train loss: 0.5310;  Loss pred: 0.5310; Loss self: 0.0000; time: 0.49s
Val loss: 0.5340 score: 0.9388 time: 0.52s
Test loss: 0.5563 score: 0.8980 time: 0.45s
Epoch 58/1000, LR 0.000269
Train loss: 0.5226;  Loss pred: 0.5226; Loss self: 0.0000; time: 0.44s
Val loss: 0.5221 score: 0.9388 time: 0.63s
Test loss: 0.5459 score: 0.8776 time: 0.47s
Epoch 59/1000, LR 0.000268
Train loss: 0.5032;  Loss pred: 0.5032; Loss self: 0.0000; time: 0.47s
Val loss: 0.5095 score: 0.9388 time: 0.52s
Test loss: 0.5350 score: 0.8776 time: 0.45s
Epoch 60/1000, LR 0.000268
Train loss: 0.4966;  Loss pred: 0.4966; Loss self: 0.0000; time: 0.58s
Val loss: 0.4964 score: 0.9388 time: 0.54s
Test loss: 0.5235 score: 0.8776 time: 0.51s
Epoch 61/1000, LR 0.000268
Train loss: 0.4804;  Loss pred: 0.4804; Loss self: 0.0000; time: 0.44s
Val loss: 0.4827 score: 0.9388 time: 0.52s
Test loss: 0.5116 score: 0.8776 time: 0.55s
Epoch 62/1000, LR 0.000268
Train loss: 0.4659;  Loss pred: 0.4659; Loss self: 0.0000; time: 0.48s
Val loss: 0.4686 score: 0.9388 time: 0.52s
Test loss: 0.4991 score: 0.8776 time: 0.44s
Epoch 63/1000, LR 0.000268
Train loss: 0.4503;  Loss pred: 0.4503; Loss self: 0.0000; time: 0.44s
Val loss: 0.4541 score: 0.9388 time: 0.52s
Test loss: 0.4862 score: 0.8776 time: 0.54s
Epoch 64/1000, LR 0.000268
Train loss: 0.4261;  Loss pred: 0.4261; Loss self: 0.0000; time: 0.48s
Val loss: 0.4392 score: 0.9184 time: 0.51s
Test loss: 0.4730 score: 0.8776 time: 0.44s
Epoch 65/1000, LR 0.000268
Train loss: 0.4158;  Loss pred: 0.4158; Loss self: 0.0000; time: 0.45s
Val loss: 0.4240 score: 0.9184 time: 0.51s
Test loss: 0.4595 score: 0.8776 time: 0.57s
Epoch 66/1000, LR 0.000268
Train loss: 0.4013;  Loss pred: 0.4013; Loss self: 0.0000; time: 0.44s
Val loss: 0.4085 score: 0.9184 time: 0.52s
Test loss: 0.4460 score: 0.8980 time: 0.50s
Epoch 67/1000, LR 0.000268
Train loss: 0.3785;  Loss pred: 0.3785; Loss self: 0.0000; time: 0.45s
Val loss: 0.3930 score: 0.9388 time: 0.64s
Test loss: 0.4325 score: 0.8980 time: 0.44s
Epoch 68/1000, LR 0.000268
Train loss: 0.3679;  Loss pred: 0.3679; Loss self: 0.0000; time: 0.47s
Val loss: 0.3775 score: 0.9388 time: 0.51s
Test loss: 0.4189 score: 0.9184 time: 0.46s
Epoch 69/1000, LR 0.000268
Train loss: 0.3441;  Loss pred: 0.3441; Loss self: 0.0000; time: 0.57s
Val loss: 0.3622 score: 0.9388 time: 0.52s
Test loss: 0.4054 score: 0.8980 time: 0.44s
Epoch 70/1000, LR 0.000268
Train loss: 0.3318;  Loss pred: 0.3318; Loss self: 0.0000; time: 0.45s
Val loss: 0.3471 score: 0.9388 time: 0.51s
Test loss: 0.3920 score: 0.8776 time: 0.56s
Epoch 71/1000, LR 0.000268
Train loss: 0.3192;  Loss pred: 0.3192; Loss self: 0.0000; time: 0.48s
Val loss: 0.3323 score: 0.9592 time: 0.52s
Test loss: 0.3789 score: 0.8776 time: 0.45s
Epoch 72/1000, LR 0.000267
Train loss: 0.2998;  Loss pred: 0.2998; Loss self: 0.0000; time: 0.45s
Val loss: 0.3179 score: 0.9592 time: 0.52s
Test loss: 0.3662 score: 0.8776 time: 0.56s
Epoch 73/1000, LR 0.000267
Train loss: 0.2812;  Loss pred: 0.2812; Loss self: 0.0000; time: 0.48s
Val loss: 0.3040 score: 0.9592 time: 0.60s
Test loss: 0.3539 score: 0.8776 time: 0.47s
Epoch 74/1000, LR 0.000267
Train loss: 0.2697;  Loss pred: 0.2697; Loss self: 0.0000; time: 0.46s
Val loss: 0.2907 score: 0.9592 time: 0.63s
Test loss: 0.3422 score: 0.8776 time: 0.46s
Epoch 75/1000, LR 0.000267
Train loss: 0.2588;  Loss pred: 0.2588; Loss self: 0.0000; time: 0.45s
Val loss: 0.2779 score: 0.9592 time: 0.51s
Test loss: 0.3311 score: 0.8776 time: 0.46s
Epoch 76/1000, LR 0.000267
Train loss: 0.2347;  Loss pred: 0.2347; Loss self: 0.0000; time: 0.46s
Val loss: 0.2659 score: 0.9592 time: 0.62s
Test loss: 0.3207 score: 0.8776 time: 0.48s
Epoch 77/1000, LR 0.000267
Train loss: 0.2241;  Loss pred: 0.2241; Loss self: 0.0000; time: 0.45s
Val loss: 0.2546 score: 0.9592 time: 0.51s
Test loss: 0.3109 score: 0.8776 time: 0.44s
Epoch 78/1000, LR 0.000267
Train loss: 0.2096;  Loss pred: 0.2096; Loss self: 0.0000; time: 0.44s
Val loss: 0.2442 score: 0.9592 time: 0.62s
Test loss: 0.3012 score: 0.8776 time: 0.46s
Epoch 79/1000, LR 0.000267
Train loss: 0.2017;  Loss pred: 0.2017; Loss self: 0.0000; time: 0.45s
Val loss: 0.2348 score: 0.9592 time: 0.52s
Test loss: 0.2920 score: 0.8776 time: 0.55s
Epoch 80/1000, LR 0.000267
Train loss: 0.1888;  Loss pred: 0.1888; Loss self: 0.0000; time: 0.44s
Val loss: 0.2262 score: 0.9592 time: 0.50s
Test loss: 0.2840 score: 0.8776 time: 0.46s
Epoch 81/1000, LR 0.000267
Train loss: 0.1804;  Loss pred: 0.1804; Loss self: 0.0000; time: 0.50s
Val loss: 0.2183 score: 0.9592 time: 0.51s
Test loss: 0.2770 score: 0.8776 time: 0.53s
Epoch 82/1000, LR 0.000267
Train loss: 0.1740;  Loss pred: 0.1740; Loss self: 0.0000; time: 0.44s
Val loss: 0.2114 score: 0.9592 time: 0.52s
Test loss: 0.2707 score: 0.8980 time: 0.45s
Epoch 83/1000, LR 0.000266
Train loss: 0.1668;  Loss pred: 0.1668; Loss self: 0.0000; time: 0.46s
Val loss: 0.2057 score: 0.9388 time: 0.68s
Test loss: 0.2646 score: 0.8980 time: 0.44s
Epoch 84/1000, LR 0.000266
Train loss: 0.1577;  Loss pred: 0.1577; Loss self: 0.0000; time: 0.45s
Val loss: 0.2013 score: 0.9388 time: 0.56s
Test loss: 0.2589 score: 0.8980 time: 0.45s
Epoch 85/1000, LR 0.000266
Train loss: 0.1524;  Loss pred: 0.1524; Loss self: 0.0000; time: 0.47s
Val loss: 0.1976 score: 0.9388 time: 0.62s
Test loss: 0.2541 score: 0.8980 time: 0.46s
Epoch 86/1000, LR 0.000266
Train loss: 0.1473;  Loss pred: 0.1473; Loss self: 0.0000; time: 0.47s
Val loss: 0.1952 score: 0.9388 time: 0.54s
Test loss: 0.2497 score: 0.8980 time: 0.48s
Epoch 87/1000, LR 0.000266
Train loss: 0.1416;  Loss pred: 0.1416; Loss self: 0.0000; time: 0.46s
Val loss: 0.1932 score: 0.9184 time: 0.62s
Test loss: 0.2462 score: 0.8980 time: 0.45s
Epoch 88/1000, LR 0.000266
Train loss: 0.1326;  Loss pred: 0.1326; Loss self: 0.0000; time: 0.46s
Val loss: 0.1916 score: 0.9184 time: 0.55s
Test loss: 0.2433 score: 0.8980 time: 0.55s
Epoch 89/1000, LR 0.000266
Train loss: 0.1239;  Loss pred: 0.1239; Loss self: 0.0000; time: 0.44s
Val loss: 0.1913 score: 0.9184 time: 0.52s
Test loss: 0.2406 score: 0.8776 time: 0.46s
Epoch 90/1000, LR 0.000266
Train loss: 0.1270;  Loss pred: 0.1270; Loss self: 0.0000; time: 0.45s
Val loss: 0.1914 score: 0.9184 time: 0.56s
Test loss: 0.2385 score: 0.8571 time: 0.57s
     INFO: Early stopping counter 1 of 20
Epoch 91/1000, LR 0.000266
Train loss: 0.1148;  Loss pred: 0.1148; Loss self: 0.0000; time: 0.45s
Val loss: 0.1925 score: 0.9184 time: 0.52s
Test loss: 0.2367 score: 0.8776 time: 0.46s
     INFO: Early stopping counter 2 of 20
Epoch 92/1000, LR 0.000266
Train loss: 0.1166;  Loss pred: 0.1166; Loss self: 0.0000; time: 0.51s
Val loss: 0.1952 score: 0.9184 time: 0.63s
Test loss: 0.2353 score: 0.8776 time: 0.44s
     INFO: Early stopping counter 3 of 20
Epoch 93/1000, LR 0.000265
Train loss: 0.1222;  Loss pred: 0.1222; Loss self: 0.0000; time: 0.45s
Val loss: 0.1996 score: 0.9184 time: 0.52s
Test loss: 0.2349 score: 0.8776 time: 0.59s
     INFO: Early stopping counter 4 of 20
Epoch 94/1000, LR 0.000265
Train loss: 0.1059;  Loss pred: 0.1059; Loss self: 0.0000; time: 0.44s
Val loss: 0.2050 score: 0.9184 time: 0.51s
Test loss: 0.2354 score: 0.8776 time: 0.44s
     INFO: Early stopping counter 5 of 20
Epoch 95/1000, LR 0.000265
Train loss: 0.1018;  Loss pred: 0.1018; Loss self: 0.0000; time: 0.46s
Val loss: 0.2110 score: 0.9388 time: 0.62s
Test loss: 0.2369 score: 0.8776 time: 0.48s
     INFO: Early stopping counter 6 of 20
Epoch 96/1000, LR 0.000265
Train loss: 0.0944;  Loss pred: 0.0944; Loss self: 0.0000; time: 0.45s
Val loss: 0.2161 score: 0.9388 time: 0.52s
Test loss: 0.2385 score: 0.8776 time: 0.46s
     INFO: Early stopping counter 7 of 20
Epoch 97/1000, LR 0.000265
Train loss: 0.1044;  Loss pred: 0.1044; Loss self: 0.0000; time: 0.56s
Val loss: 0.2171 score: 0.9388 time: 0.55s
Test loss: 0.2380 score: 0.8776 time: 0.46s
     INFO: Early stopping counter 8 of 20
Epoch 98/1000, LR 0.000265
Train loss: 0.0978;  Loss pred: 0.0978; Loss self: 0.0000; time: 0.45s
Val loss: 0.2170 score: 0.9388 time: 0.52s
Test loss: 0.2369 score: 0.8776 time: 0.56s
     INFO: Early stopping counter 9 of 20
Epoch 99/1000, LR 0.000265
Train loss: 0.0886;  Loss pred: 0.0886; Loss self: 0.0000; time: 0.57s
Val loss: 0.2179 score: 0.9388 time: 0.52s
Test loss: 0.2364 score: 0.8776 time: 0.44s
     INFO: Early stopping counter 10 of 20
Epoch 100/1000, LR 0.000265
Train loss: 0.0957;  Loss pred: 0.0957; Loss self: 0.0000; time: 0.44s
Val loss: 0.2169 score: 0.9388 time: 0.53s
Test loss: 0.2351 score: 0.8776 time: 0.58s
     INFO: Early stopping counter 11 of 20
Epoch 101/1000, LR 0.000265
Train loss: 0.0986;  Loss pred: 0.0986; Loss self: 0.0000; time: 0.46s
Val loss: 0.2115 score: 0.9388 time: 0.51s
Test loss: 0.2326 score: 0.8776 time: 0.44s
     INFO: Early stopping counter 12 of 20
Epoch 102/1000, LR 0.000264
Train loss: 0.0806;  Loss pred: 0.0806; Loss self: 0.0000; time: 0.46s
Val loss: 0.2076 score: 0.9388 time: 0.55s
Test loss: 0.2316 score: 0.8776 time: 0.56s
     INFO: Early stopping counter 13 of 20
Epoch 103/1000, LR 0.000264
Train loss: 0.0772;  Loss pred: 0.0772; Loss self: 0.0000; time: 0.45s
Val loss: 0.2053 score: 0.9388 time: 0.52s
Test loss: 0.2314 score: 0.8776 time: 0.46s
     INFO: Early stopping counter 14 of 20
Epoch 104/1000, LR 0.000264
Train loss: 0.0891;  Loss pred: 0.0891; Loss self: 0.0000; time: 0.57s
Val loss: 0.2032 score: 0.9388 time: 0.70s
Test loss: 0.2317 score: 0.8571 time: 0.47s
     INFO: Early stopping counter 15 of 20
Epoch 105/1000, LR 0.000264
Train loss: 0.0834;  Loss pred: 0.0834; Loss self: 0.0000; time: 0.46s
Val loss: 0.2002 score: 0.9388 time: 0.54s
Test loss: 0.2326 score: 0.8776 time: 0.47s
     INFO: Early stopping counter 16 of 20
Epoch 106/1000, LR 0.000264
Train loss: 0.0757;  Loss pred: 0.0757; Loss self: 0.0000; time: 0.67s
Val loss: 0.1993 score: 0.9388 time: 0.54s
Test loss: 0.2337 score: 0.8776 time: 0.61s
     INFO: Early stopping counter 17 of 20
Epoch 107/1000, LR 0.000264
Train loss: 0.0712;  Loss pred: 0.0712; Loss self: 0.0000; time: 0.47s
Val loss: 0.2001 score: 0.9388 time: 0.58s
Test loss: 0.2346 score: 0.8776 time: 0.56s
     INFO: Early stopping counter 18 of 20
Epoch 108/1000, LR 0.000264
Train loss: 0.0710;  Loss pred: 0.0710; Loss self: 0.0000; time: 0.49s
Val loss: 0.2033 score: 0.9388 time: 0.68s
Test loss: 0.2349 score: 0.8776 time: 0.50s
     INFO: Early stopping counter 19 of 20
Epoch 109/1000, LR 0.000264
Train loss: 0.0656;  Loss pred: 0.0656; Loss self: 0.0000; time: 0.51s
Val loss: 0.2076 score: 0.9388 time: 0.52s
Test loss: 0.2350 score: 0.8776 time: 0.56s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 088,   Train_Loss: 0.1239,   Val_Loss: 0.1913,   Val_Precision: 0.8846,   Val_Recall: 0.9583,   Val_accuracy: 0.9200,   Val_Score: 0.9184,   Val_Loss: 0.1913,   Test_Precision: 0.9130,   Test_Recall: 0.8400,   Test_accuracy: 0.8750,   Test_Score: 0.8776,   Test_loss: 0.2406


[0.47569059603847563, 0.45132147101685405, 0.44563297485001385, 0.5580142331309617, 0.5506600718945265, 0.4633305750321597, 0.5788522260263562, 0.44643664499744773, 0.4611370291095227, 0.45920622791163623, 0.4509186039213091, 0.4616614580154419, 0.5626949791330844, 0.6981428579892963, 0.4675599308684468, 0.5515576610341668, 0.4389668151270598, 0.47475144313648343, 0.5560952147934586, 2.7768845919054, 0.4467680021189153, 0.44475991697981954, 0.48681075079366565, 0.5424957610666752, 0.4337174950633198, 0.5407437509857118, 0.44338470813818276, 0.5634947181679308, 0.44345019292086363, 0.5320004390086979, 0.46075096097774804, 0.44417128805071115, 0.5427314669359475, 0.44638587604276836, 0.5443193190731108, 0.4632013849914074, 0.44247378781437874, 0.45457864995114505, 0.46007012692280114, 0.45130646182224154, 0.4651479651220143, 0.5706384810619056, 0.45046776602976024, 0.5618159910663962, 0.44742191303521395, 0.4812281639315188, 0.4485834420192987, 0.48323942394927144, 0.4637891938909888, 0.4905513960402459, 0.5411260868422687, 0.5012516570277512, 0.5992905881721526, 0.4722739290446043, 0.4516412790399045, 0.5635383401531726, 0.45252237701788545, 0.4731232519261539, 0.4520899001508951, 0.5200490329880267, 0.5552324298769236, 0.44297137996181846, 0.546603481983766, 0.44949730997905135, 0.5797838601283729, 0.5038830840494484, 0.44426238909363747, 0.4655644129961729, 0.44806572212837636, 0.5607313578948379, 0.4501445088535547, 0.5615581821184605, 0.47095722099766135, 0.4605841168668121, 0.46516172704286873, 0.48626545583829284, 0.44313494791276753, 0.4646034771576524, 0.5522780469618738, 0.46834656689316034, 0.5353503781370819, 0.45680899010039866, 0.44849530002102256, 0.45338086504489183, 0.46530835307203233, 0.48168550501577556, 0.45204843604005873, 0.5558993201702833, 0.4622029459569603, 0.5743302810005844, 0.4636002550832927, 0.44945880491286516, 0.5904106369707733, 0.44257127889432013, 0.48433129698969424, 0.4620950990356505, 0.4691790679935366, 0.5617187810130417, 0.449422013014555, 0.5833343579433858, 0.44469594210386276, 0.5698766501154751, 0.46284815203398466, 0.47052720887586474, 0.47525181504897773, 0.6149143981747329, 0.5664069140329957, 0.5068021349143237, 0.5629306701011956]
[0.009707971347723993, 0.009210642265650083, 0.00909455050714314, 0.011388045574101259, 0.011237960650908704, 0.009455726021064483, 0.01181331073523176, 0.009110951938723423, 0.009410959777745361, 0.009371555671666046, 0.009202420488189983, 0.009421662408478406, 0.011483571002716008, 0.014247813428352987, 0.009542039405478507, 0.01125627879661565, 0.008958506431164486, 0.00968880496196905, 0.011348881934560379, 0.05667111412051837, 0.009117714328957456, 0.009076732999588154, 0.00993491328150338, 0.011071342062585207, 0.008851377450271833, 0.011035586754810445, 0.009048667513024139, 0.011499892207508793, 0.009050003937160482, 0.010857151816504039, 0.009403080836280572, 0.009064720164300228, 0.011076152386447909, 0.009109915837607518, 0.011108557532104303, 0.009453089489620559, 0.00903007730233426, 0.00927711530512541, 0.009389186263730635, 0.00921033595555595, 0.009492815614734985, 0.011645683286977666, 0.009193219714893066, 0.01146563247074278, 0.009131059449698244, 0.009820982937377935, 0.00915476412284283, 0.009862029060189213, 0.009465085589612017, 0.010011252980413181, 0.01104338952739324, 0.010229625653627575, 0.012230420166778624, 0.009638243449889883, 0.009217168959998051, 0.011500782452105564, 0.009235150551385417, 0.009655576569921508, 0.00922632449287541, 0.010613245571184218, 0.01133127407912089, 0.009040232244118745, 0.01115517310170951, 0.009173414489368394, 0.011832323676089242, 0.01028332824590711, 0.009066579369257907, 0.009501314550942304, 0.009144198410783191, 0.01144349709989465, 0.009186622629664382, 0.011460371063642052, 0.00961137185709513, 0.009399675854424737, 0.009493096470262627, 0.009923784813026384, 0.009043570365566684, 0.009481703615462293, 0.011270980550242322, 0.009558093201901232, 0.010925517921164935, 0.009322632451028543, 0.009152965306551481, 0.009252670715201874, 0.009496088838204741, 0.00983031642889338, 0.009225478286531811, 0.011344884085107823, 0.009432713182795108, 0.011721026142869068, 0.009461229695577403, 0.009172628671691125, 0.012049196672872926, 0.009032066916210614, 0.009884312183463148, 0.009430512225217357, 0.009575083020276256, 0.011463648592102893, 0.009171877816623571, 0.01190478281517114, 0.009075427389874751, 0.011630135716642348, 0.009445880653754788, 0.009602596099507444, 0.009699016633652607, 0.012549273432137407, 0.011559324776183585, 0.010342900712537219, 0.01148838102247338]
[103.00813261407568, 108.57006180007366, 109.95595650544458, 87.81138023140724, 88.9841165193206, 105.75602526683873, 84.65027479702383, 109.75801504887694, 106.25908766125615, 106.70586987210665, 108.66706224556462, 106.13838159814723, 87.0809262870833, 70.18620822266043, 104.79939953149385, 88.83930631681434, 111.62575008276389, 103.2119032146118, 88.11440684343826, 17.645673911992837, 109.67661015919795, 110.17179860257802, 100.65513121908972, 90.32328640440322, 112.976766115571, 90.61593390710259, 110.51350915045302, 86.95733681286642, 110.4971894977715, 92.10518715229647, 106.34812328121536, 110.31780152886797, 90.28405940166893, 109.77049819404523, 90.02068874468613, 105.78552134706803, 110.74102319605853, 107.79212795248122, 106.5055023844704, 108.5736725376201, 105.34282351884883, 85.86872709463182, 108.77581859378326, 87.217168573276, 109.51631686430946, 101.82280189023382, 109.23274336526214, 101.39901169392965, 105.65144821273518, 99.88759668310053, 90.55190867980251, 97.75528781401549, 81.76333980056471, 103.7533452230266, 108.49318313898105, 86.95060567960921, 108.28193806217749, 103.56709335361101, 108.38552240084363, 94.22188465280378, 88.25132928719898, 110.61662720563011, 89.64450760936663, 109.0106634949242, 84.51425327560975, 97.24478068644869, 110.29517961213736, 105.24859424855309, 109.35895691204179, 87.38587437656676, 108.85393254000827, 87.25720960052446, 104.04342011403902, 106.38664731499938, 105.3397069262412, 100.76800523600201, 110.57579690068995, 105.46627911562743, 88.7234252195121, 104.62337820697195, 91.52884167283254, 107.26583990658908, 109.25421068560405, 108.07690349955159, 105.30651271677155, 101.72612522022064, 108.39546405522309, 88.1454576792616, 106.01403653658843, 85.31676218539859, 105.69450612402363, 109.02000242157774, 82.99308469678809, 110.71662879348418, 101.1704184812212, 106.03877881903193, 104.43773676764928, 87.23226222137362, 109.02892733563785, 83.9998524564116, 110.18764814488819, 85.98351939857695, 105.86625394239917, 104.13850479989406, 103.10323590231894, 79.68588822354465, 86.51024340628994, 96.6846755850458, 87.04446675678815]
Elapsed: 0.5150682236038046~0.22395196125678743
Time per graph: 0.010511596400077644~0.004570448188914029
Speed: 99.45884062178104~12.572581410659167
Total Time: 0.5635
best val loss: 0.19129802286624908 test_score: 0.8776

Testing...
Test loss: 0.3789 score: 0.8776 time: 0.50s
test Score 0.8776
Epoch Time List: [1.4446834400296211, 1.5173492198809981, 1.4439547511283308, 1.6171266881283373, 1.6920144651085138, 1.442387935006991, 1.5468192240223289, 1.4014949609991163, 1.5567117389291525, 1.4461964750662446, 1.6603012459818274, 1.4049961981363595, 1.67645474197343, 1.8866713009774685, 1.5025117918848991, 1.546161302132532, 1.3739364249631763, 1.5171756027266383, 1.5105000650510192, 7.980472585884854, 1.4902043130714446, 1.40281723998487, 1.5279595910105854, 1.4853580894414335, 1.531892633996904, 1.4720180269796401, 1.4004038127604872, 1.4857338978908956, 1.4652372552081943, 1.5832940398249775, 1.4113712888211012, 1.5133749393280596, 1.4965526319574565, 1.4004312958568335, 1.496852247044444, 1.3876855608541518, 1.4774381013121456, 1.4089015130884945, 1.4988046039361507, 1.3736089039593935, 1.4954373468644917, 1.521885374095291, 1.4387210558634251, 1.513912706868723, 1.4222782209981233, 1.536484091077, 1.401933484012261, 1.5549291127827018, 1.4103303977753967, 1.5661356169730425, 1.5092918269801885, 1.5299781439825892, 1.6782864080742002, 1.4424282838590443, 1.568008187925443, 1.530445251846686, 1.4560769409872591, 1.5457732940558344, 1.436869990779087, 1.6290802382864058, 1.5061529288068414, 1.4425094779580832, 1.5033503929153085, 1.4309271869715303, 1.5394161629956216, 1.4549784532282501, 1.5304208036977798, 1.4408991488162428, 1.5373330239672214, 1.513481937116012, 1.44654948473908, 1.5292556958738714, 1.5414456299040467, 1.5501244047190994, 1.4232214309740812, 1.564979740884155, 1.3982208957895637, 1.523490167921409, 1.5220924771856517, 1.406131395837292, 1.5447344628628343, 1.4104504219722003, 1.5838465243577957, 1.4562380292918533, 1.5509592660237104, 1.4823544039390981, 1.5196051669772714, 1.5537130658049136, 1.415250901132822, 1.5843627869617194, 1.4366332220379263, 1.582790478831157, 1.5559978180099279, 1.3915487276390195, 1.5535516368690878, 1.424420117866248, 1.5739897591993213, 1.523830747930333, 1.528723810100928, 1.5469024209305644, 1.4073046641424298, 1.5728628432843834, 1.4196468458976597, 1.7392312481533736, 1.4670056400354952, 1.8179561262950301, 1.608713734196499, 1.6751484929118305, 1.583874307340011]
Total Epoch List: [109]
Total Time List: [0.5635228881146759]
========================k_idx:1========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: FAGNN
FAGNN(
  (conv1): NeighborFilterConv()
  (conv2): NeighborFilterConv()
  (ffn): Sequential(
    (0): Linear(in_features=14951, out_features=64, bias=True)
    (1): ReLU()
    (2): Linear(in_features=64, out_features=64, bias=True)
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=64, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=64, out_features=2, bias=True)
  )
)
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7ed848916f20>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.6935;  Loss pred: 0.6935; Loss self: 0.0000; time: 0.56s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6929 score: 0.5102 time: 0.61s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6941 score: 0.4898 time: 0.82s
Epoch 2/1000, LR 0.000000
Train loss: 0.6935;  Loss pred: 0.6935; Loss self: 0.0000; time: 0.58s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6929 score: 0.5102 time: 0.56s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6941 score: 0.4898 time: 0.61s
Epoch 3/1000, LR 0.000030
Train loss: 0.6935;  Loss pred: 0.6935; Loss self: 0.0000; time: 1.77s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6929 score: 0.5102 time: 3.67s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6940 score: 0.4898 time: 1.75s
Epoch 4/1000, LR 0.000060
Train loss: 0.6935;  Loss pred: 0.6935; Loss self: 0.0000; time: 0.48s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6929 score: 0.5102 time: 0.44s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6940 score: 0.4898 time: 0.48s
Epoch 5/1000, LR 0.000090
Train loss: 0.6934;  Loss pred: 0.6934; Loss self: 0.0000; time: 0.46s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6928 score: 0.5102 time: 0.56s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6939 score: 0.4898 time: 0.51s
Epoch 6/1000, LR 0.000120
Train loss: 0.6933;  Loss pred: 0.6933; Loss self: 0.0000; time: 0.46s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6928 score: 0.5102 time: 0.45s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6938 score: 0.4898 time: 0.48s
Epoch 7/1000, LR 0.000150
Train loss: 0.6932;  Loss pred: 0.6932; Loss self: 0.0000; time: 0.57s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6927 score: 0.5102 time: 0.53s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6937 score: 0.4898 time: 0.48s
Epoch 8/1000, LR 0.000180
Train loss: 0.6931;  Loss pred: 0.6931; Loss self: 0.0000; time: 0.48s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6927 score: 0.5102 time: 0.45s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6936 score: 0.4898 time: 0.59s
Epoch 9/1000, LR 0.000210
Train loss: 0.6930;  Loss pred: 0.6930; Loss self: 0.0000; time: 0.50s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6926 score: 0.5102 time: 0.52s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6934 score: 0.4898 time: 0.47s
Epoch 10/1000, LR 0.000240
Train loss: 0.6928;  Loss pred: 0.6928; Loss self: 0.0000; time: 0.47s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6925 score: 0.5102 time: 0.47s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6932 score: 0.4898 time: 0.60s
Epoch 11/1000, LR 0.000270
Train loss: 0.6926;  Loss pred: 0.6926; Loss self: 0.0000; time: 0.54s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6924 score: 0.5102 time: 0.45s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6931 score: 0.4898 time: 0.47s
Epoch 12/1000, LR 0.000270
Train loss: 0.6925;  Loss pred: 0.6925; Loss self: 0.0000; time: 0.48s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6923 score: 0.5102 time: 0.57s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6929 score: 0.4898 time: 0.53s
Epoch 13/1000, LR 0.000270
Train loss: 0.6922;  Loss pred: 0.6922; Loss self: 0.0000; time: 0.46s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6922 score: 0.5102 time: 0.47s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6927 score: 0.4898 time: 0.51s
Epoch 14/1000, LR 0.000270
Train loss: 0.6920;  Loss pred: 0.6920; Loss self: 0.0000; time: 0.62s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6920 score: 0.5102 time: 0.53s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6926 score: 0.4898 time: 0.50s
Epoch 15/1000, LR 0.000270
Train loss: 0.6918;  Loss pred: 0.6918; Loss self: 0.0000; time: 0.47s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6918 score: 0.5102 time: 0.45s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6924 score: 0.4898 time: 0.65s
Epoch 16/1000, LR 0.000270
Train loss: 0.6915;  Loss pred: 0.6915; Loss self: 0.0000; time: 0.49s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6916 score: 0.5102 time: 0.46s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6921 score: 0.4898 time: 0.51s
Epoch 17/1000, LR 0.000270
Train loss: 0.6912;  Loss pred: 0.6912; Loss self: 0.0000; time: 0.48s
Val loss: 0.6914 score: 0.5510 time: 0.59s
Test loss: 0.6919 score: 0.5306 time: 0.48s
Epoch 18/1000, LR 0.000270
Train loss: 0.6908;  Loss pred: 0.6908; Loss self: 0.0000; time: 0.47s
Val loss: 0.6911 score: 0.5714 time: 0.45s
Test loss: 0.6916 score: 0.5102 time: 0.48s
Epoch 19/1000, LR 0.000270
Train loss: 0.6904;  Loss pred: 0.6904; Loss self: 0.0000; time: 0.61s
Val loss: 0.6907 score: 0.5918 time: 0.43s
Test loss: 0.6913 score: 0.5306 time: 0.53s
Epoch 20/1000, LR 0.000270
Train loss: 0.6899;  Loss pred: 0.6899; Loss self: 0.0000; time: 0.48s
Val loss: 0.6903 score: 0.6122 time: 0.45s
Test loss: 0.6910 score: 0.5306 time: 0.61s
Epoch 21/1000, LR 0.000270
Train loss: 0.6894;  Loss pred: 0.6894; Loss self: 0.0000; time: 0.47s
Val loss: 0.6899 score: 0.6327 time: 0.46s
Test loss: 0.6906 score: 0.5510 time: 0.48s
Epoch 22/1000, LR 0.000270
Train loss: 0.6887;  Loss pred: 0.6887; Loss self: 0.0000; time: 0.47s
Val loss: 0.6893 score: 0.6327 time: 0.58s
Test loss: 0.6901 score: 0.5510 time: 0.48s
Epoch 23/1000, LR 0.000270
Train loss: 0.6881;  Loss pred: 0.6881; Loss self: 0.0000; time: 0.48s
Val loss: 0.6888 score: 0.6327 time: 0.45s
Test loss: 0.6896 score: 0.5510 time: 0.49s
Epoch 24/1000, LR 0.000270
Train loss: 0.6874;  Loss pred: 0.6874; Loss self: 0.0000; time: 0.52s
Val loss: 0.6881 score: 0.6327 time: 0.87s
Test loss: 0.6891 score: 0.5510 time: 0.60s
Epoch 25/1000, LR 0.000270
Train loss: 0.6865;  Loss pred: 0.6865; Loss self: 0.0000; time: 0.49s
Val loss: 0.6873 score: 0.6327 time: 0.48s
Test loss: 0.6884 score: 0.5510 time: 0.49s
Epoch 26/1000, LR 0.000270
Train loss: 0.6853;  Loss pred: 0.6853; Loss self: 0.0000; time: 0.60s
Val loss: 0.6865 score: 0.5918 time: 0.45s
Test loss: 0.6877 score: 0.5306 time: 0.51s
Epoch 27/1000, LR 0.000270
Train loss: 0.6842;  Loss pred: 0.6842; Loss self: 0.0000; time: 0.51s
Val loss: 0.6855 score: 0.5918 time: 0.55s
Test loss: 0.6869 score: 0.5306 time: 0.58s
Epoch 28/1000, LR 0.000270
Train loss: 0.6829;  Loss pred: 0.6829; Loss self: 0.0000; time: 0.49s
Val loss: 0.6844 score: 0.5918 time: 0.47s
Test loss: 0.6859 score: 0.5306 time: 0.49s
Epoch 29/1000, LR 0.000270
Train loss: 0.6816;  Loss pred: 0.6816; Loss self: 0.0000; time: 0.48s
Val loss: 0.6831 score: 0.5918 time: 0.55s
Test loss: 0.6849 score: 0.5306 time: 0.47s
Epoch 30/1000, LR 0.000270
Train loss: 0.6801;  Loss pred: 0.6801; Loss self: 0.0000; time: 0.49s
Val loss: 0.6819 score: 0.5918 time: 0.48s
Test loss: 0.6838 score: 0.5306 time: 0.58s
Epoch 31/1000, LR 0.000270
Train loss: 0.6787;  Loss pred: 0.6787; Loss self: 0.0000; time: 0.47s
Val loss: 0.6805 score: 0.5918 time: 0.53s
Test loss: 0.6827 score: 0.5306 time: 0.50s
Epoch 32/1000, LR 0.000270
Train loss: 0.6769;  Loss pred: 0.6769; Loss self: 0.0000; time: 0.49s
Val loss: 0.6791 score: 0.5918 time: 0.49s
Test loss: 0.6814 score: 0.5306 time: 0.57s
Epoch 33/1000, LR 0.000270
Train loss: 0.6747;  Loss pred: 0.6747; Loss self: 0.0000; time: 0.48s
Val loss: 0.6775 score: 0.5918 time: 0.46s
Test loss: 0.6800 score: 0.5306 time: 0.47s
Epoch 34/1000, LR 0.000270
Train loss: 0.6730;  Loss pred: 0.6730; Loss self: 0.0000; time: 0.48s
Val loss: 0.6757 score: 0.5918 time: 0.46s
Test loss: 0.6784 score: 0.5306 time: 0.61s
Epoch 35/1000, LR 0.000270
Train loss: 0.6709;  Loss pred: 0.6709; Loss self: 0.0000; time: 0.58s
Val loss: 0.6738 score: 0.5918 time: 0.56s
Test loss: 0.6766 score: 0.5306 time: 0.51s
Epoch 36/1000, LR 0.000270
Train loss: 0.6683;  Loss pred: 0.6683; Loss self: 0.0000; time: 0.48s
Val loss: 0.6718 score: 0.5918 time: 0.55s
Test loss: 0.6746 score: 0.5306 time: 0.48s
Epoch 37/1000, LR 0.000270
Train loss: 0.6655;  Loss pred: 0.6655; Loss self: 0.0000; time: 0.48s
Val loss: 0.6696 score: 0.6122 time: 0.44s
Test loss: 0.6725 score: 0.5510 time: 0.57s
Epoch 38/1000, LR 0.000270
Train loss: 0.6637;  Loss pred: 0.6637; Loss self: 0.0000; time: 0.53s
Val loss: 0.6671 score: 0.6122 time: 0.46s
Test loss: 0.6701 score: 0.5510 time: 0.48s
Epoch 39/1000, LR 0.000269
Train loss: 0.6592;  Loss pred: 0.6592; Loss self: 0.0000; time: 0.46s
Val loss: 0.6645 score: 0.6122 time: 0.44s
Test loss: 0.6674 score: 0.5510 time: 0.61s
Epoch 40/1000, LR 0.000269
Train loss: 0.6562;  Loss pred: 0.6562; Loss self: 0.0000; time: 0.47s
Val loss: 0.6617 score: 0.6327 time: 0.45s
Test loss: 0.6645 score: 0.5510 time: 0.48s
Epoch 41/1000, LR 0.000269
Train loss: 0.6530;  Loss pred: 0.6530; Loss self: 0.0000; time: 0.48s
Val loss: 0.6587 score: 0.6327 time: 0.57s
Test loss: 0.6613 score: 0.5510 time: 0.47s
Epoch 42/1000, LR 0.000269
Train loss: 0.6477;  Loss pred: 0.6477; Loss self: 0.0000; time: 0.49s
Val loss: 0.6554 score: 0.6327 time: 0.45s
Test loss: 0.6579 score: 0.5510 time: 0.48s
Epoch 43/1000, LR 0.000269
Train loss: 0.6453;  Loss pred: 0.6453; Loss self: 0.0000; time: 0.57s
Val loss: 0.6518 score: 0.6327 time: 0.44s
Test loss: 0.6541 score: 0.5714 time: 0.48s
Epoch 44/1000, LR 0.000269
Train loss: 0.6401;  Loss pred: 0.6401; Loss self: 0.0000; time: 0.47s
Val loss: 0.6479 score: 0.6327 time: 0.45s
Test loss: 0.6500 score: 0.5918 time: 0.59s
Epoch 45/1000, LR 0.000269
Train loss: 0.6344;  Loss pred: 0.6344; Loss self: 0.0000; time: 0.51s
Val loss: 0.6438 score: 0.6531 time: 0.45s
Test loss: 0.6455 score: 0.5918 time: 0.48s
Epoch 46/1000, LR 0.000269
Train loss: 0.6282;  Loss pred: 0.6282; Loss self: 0.0000; time: 0.48s
Val loss: 0.6393 score: 0.6735 time: 0.55s
Test loss: 0.6408 score: 0.5918 time: 0.52s
Epoch 47/1000, LR 0.000269
Train loss: 0.6261;  Loss pred: 0.6261; Loss self: 0.0000; time: 0.47s
Val loss: 0.6346 score: 0.6939 time: 0.46s
Test loss: 0.6356 score: 0.5918 time: 0.48s
Epoch 48/1000, LR 0.000269
Train loss: 0.6166;  Loss pred: 0.6166; Loss self: 0.0000; time: 0.49s
Val loss: 0.6296 score: 0.6939 time: 0.58s
Test loss: 0.6301 score: 0.5918 time: 0.49s
Epoch 49/1000, LR 0.000269
Train loss: 0.6113;  Loss pred: 0.6113; Loss self: 0.0000; time: 0.47s
Val loss: 0.6242 score: 0.7143 time: 0.44s
Test loss: 0.6241 score: 0.5918 time: 0.49s
Epoch 50/1000, LR 0.000269
Train loss: 0.6053;  Loss pred: 0.6053; Loss self: 0.0000; time: 0.57s
Val loss: 0.6185 score: 0.7347 time: 0.48s
Test loss: 0.6178 score: 0.6327 time: 0.54s
Epoch 51/1000, LR 0.000269
Train loss: 0.5981;  Loss pred: 0.5981; Loss self: 0.0000; time: 0.52s
Val loss: 0.6125 score: 0.7551 time: 0.51s
Test loss: 0.6110 score: 0.6939 time: 0.59s
Epoch 52/1000, LR 0.000269
Train loss: 0.5930;  Loss pred: 0.5930; Loss self: 0.0000; time: 0.52s
Val loss: 0.6061 score: 0.7755 time: 0.46s
Test loss: 0.6037 score: 0.7347 time: 0.47s
Epoch 53/1000, LR 0.000269
Train loss: 0.5795;  Loss pred: 0.5795; Loss self: 0.0000; time: 0.47s
Val loss: 0.5992 score: 0.7755 time: 0.46s
Test loss: 0.5959 score: 0.7959 time: 0.60s
Epoch 54/1000, LR 0.000269
Train loss: 0.5730;  Loss pred: 0.5730; Loss self: 0.0000; time: 0.67s
Val loss: 0.5919 score: 0.8163 time: 0.57s
Test loss: 0.5875 score: 0.7959 time: 0.51s
Epoch 55/1000, LR 0.000269
Train loss: 0.5656;  Loss pred: 0.5656; Loss self: 0.0000; time: 0.48s
Val loss: 0.5842 score: 0.8367 time: 0.58s
Test loss: 0.5786 score: 0.8367 time: 0.49s
Epoch 56/1000, LR 0.000269
Train loss: 0.5575;  Loss pred: 0.5575; Loss self: 0.0000; time: 0.50s
Val loss: 0.5760 score: 0.8367 time: 0.45s
Test loss: 0.5691 score: 0.8571 time: 0.48s
Epoch 57/1000, LR 0.000269
Train loss: 0.5480;  Loss pred: 0.5480; Loss self: 0.0000; time: 0.61s
Val loss: 0.5674 score: 0.8571 time: 0.52s
Test loss: 0.5589 score: 0.8571 time: 0.48s
Epoch 58/1000, LR 0.000269
Train loss: 0.5336;  Loss pred: 0.5336; Loss self: 0.0000; time: 0.48s
Val loss: 0.5584 score: 0.8980 time: 0.46s
Test loss: 0.5483 score: 0.8776 time: 0.62s
Epoch 59/1000, LR 0.000268
Train loss: 0.5219;  Loss pred: 0.5219; Loss self: 0.0000; time: 0.49s
Val loss: 0.5491 score: 0.8980 time: 0.45s
Test loss: 0.5370 score: 0.8980 time: 0.50s
Epoch 60/1000, LR 0.000268
Train loss: 0.5074;  Loss pred: 0.5074; Loss self: 0.0000; time: 0.52s
Val loss: 0.5391 score: 0.8980 time: 0.60s
Test loss: 0.5254 score: 0.9184 time: 0.49s
Epoch 61/1000, LR 0.000268
Train loss: 0.4981;  Loss pred: 0.4981; Loss self: 0.0000; time: 0.49s
Val loss: 0.5288 score: 0.8980 time: 0.46s
Test loss: 0.5134 score: 0.9184 time: 0.50s
Epoch 62/1000, LR 0.000268
Train loss: 0.4828;  Loss pred: 0.4828; Loss self: 0.0000; time: 0.51s
Val loss: 0.5181 score: 0.8980 time: 0.68s
Test loss: 0.5010 score: 0.9184 time: 0.48s
Epoch 63/1000, LR 0.000268
Train loss: 0.4730;  Loss pred: 0.4730; Loss self: 0.0000; time: 0.49s
Val loss: 0.5071 score: 0.8980 time: 0.46s
Test loss: 0.4882 score: 0.9184 time: 0.51s
Epoch 64/1000, LR 0.000268
Train loss: 0.4527;  Loss pred: 0.4527; Loss self: 0.0000; time: 0.84s
Val loss: 0.4958 score: 0.8980 time: 0.57s
Test loss: 0.4751 score: 0.9184 time: 0.61s
Epoch 65/1000, LR 0.000268
Train loss: 0.4449;  Loss pred: 0.4449; Loss self: 0.0000; time: 0.59s
Val loss: 0.4843 score: 0.8980 time: 0.60s
Test loss: 0.4618 score: 0.9388 time: 0.67s
Epoch 66/1000, LR 0.000268
Train loss: 0.4315;  Loss pred: 0.4315; Loss self: 0.0000; time: 0.49s
Val loss: 0.4727 score: 0.8980 time: 0.46s
Test loss: 0.4483 score: 0.9388 time: 0.54s
Epoch 67/1000, LR 0.000268
Train loss: 0.4168;  Loss pred: 0.4168; Loss self: 0.0000; time: 0.64s
Val loss: 0.4610 score: 0.8980 time: 0.60s
Test loss: 0.4347 score: 0.9388 time: 0.78s
Epoch 68/1000, LR 0.000268
Train loss: 0.4023;  Loss pred: 0.4023; Loss self: 0.0000; time: 0.66s
Val loss: 0.4496 score: 0.9184 time: 0.61s
Test loss: 0.4210 score: 0.9388 time: 0.72s
Epoch 69/1000, LR 0.000268
Train loss: 0.3989;  Loss pred: 0.3989; Loss self: 0.0000; time: 0.50s
Val loss: 0.4383 score: 0.9184 time: 0.66s
Test loss: 0.4072 score: 0.9388 time: 0.61s
Epoch 70/1000, LR 0.000268
Train loss: 0.3823;  Loss pred: 0.3823; Loss self: 0.0000; time: 0.61s
Val loss: 0.4272 score: 0.8980 time: 0.56s
Test loss: 0.3936 score: 0.9388 time: 0.61s
Epoch 71/1000, LR 0.000268
Train loss: 0.3666;  Loss pred: 0.3666; Loss self: 0.0000; time: 0.65s
Val loss: 0.4163 score: 0.8980 time: 0.47s
Test loss: 0.3801 score: 0.9388 time: 0.60s
Epoch 72/1000, LR 0.000267
Train loss: 0.3442;  Loss pred: 0.3442; Loss self: 0.0000; time: 0.48s
Val loss: 0.4056 score: 0.8776 time: 0.44s
Test loss: 0.3667 score: 0.9388 time: 0.59s
Epoch 73/1000, LR 0.000267
Train loss: 0.3395;  Loss pred: 0.3395; Loss self: 0.0000; time: 0.48s
Val loss: 0.3953 score: 0.8776 time: 0.69s
Test loss: 0.3534 score: 0.9592 time: 0.51s
Epoch 74/1000, LR 0.000267
Train loss: 0.3262;  Loss pred: 0.3262; Loss self: 0.0000; time: 0.46s
Val loss: 0.3853 score: 0.8776 time: 0.55s
Test loss: 0.3405 score: 0.9592 time: 0.49s
Epoch 75/1000, LR 0.000267
Train loss: 0.3116;  Loss pred: 0.3116; Loss self: 0.0000; time: 0.48s
Val loss: 0.3757 score: 0.8980 time: 0.44s
Test loss: 0.3279 score: 0.9592 time: 0.48s
Epoch 76/1000, LR 0.000267
Train loss: 0.2959;  Loss pred: 0.2959; Loss self: 0.0000; time: 0.48s
Val loss: 0.3667 score: 0.8980 time: 0.55s
Test loss: 0.3156 score: 0.9592 time: 0.48s
Epoch 77/1000, LR 0.000267
Train loss: 0.2876;  Loss pred: 0.2876; Loss self: 0.0000; time: 0.47s
Val loss: 0.3580 score: 0.8980 time: 0.45s
Test loss: 0.3035 score: 0.9592 time: 0.48s
Epoch 78/1000, LR 0.000267
Train loss: 0.2720;  Loss pred: 0.2720; Loss self: 0.0000; time: 0.57s
Val loss: 0.3498 score: 0.8980 time: 0.46s
Test loss: 0.2915 score: 0.9592 time: 0.51s
Epoch 79/1000, LR 0.000267
Train loss: 0.2681;  Loss pred: 0.2681; Loss self: 0.0000; time: 0.56s
Val loss: 0.3421 score: 0.8980 time: 0.45s
Test loss: 0.2797 score: 0.9592 time: 0.57s
Epoch 80/1000, LR 0.000267
Train loss: 0.2474;  Loss pred: 0.2474; Loss self: 0.0000; time: 0.48s
Val loss: 0.3349 score: 0.8980 time: 0.45s
Test loss: 0.2682 score: 0.9592 time: 0.49s
Epoch 81/1000, LR 0.000267
Train loss: 0.2415;  Loss pred: 0.2415; Loss self: 0.0000; time: 0.48s
Val loss: 0.3284 score: 0.8980 time: 0.45s
Test loss: 0.2574 score: 0.9592 time: 0.57s
Epoch 82/1000, LR 0.000267
Train loss: 0.2336;  Loss pred: 0.2336; Loss self: 0.0000; time: 0.49s
Val loss: 0.3224 score: 0.8980 time: 0.45s
Test loss: 0.2469 score: 0.9592 time: 0.53s
Epoch 83/1000, LR 0.000266
Train loss: 0.2147;  Loss pred: 0.2147; Loss self: 0.0000; time: 0.48s
Val loss: 0.3171 score: 0.8980 time: 0.56s
Test loss: 0.2366 score: 0.9592 time: 0.51s
Epoch 84/1000, LR 0.000266
Train loss: 0.2028;  Loss pred: 0.2028; Loss self: 0.0000; time: 0.48s
Val loss: 0.3125 score: 0.8980 time: 0.50s
Test loss: 0.2266 score: 0.9592 time: 0.48s
Epoch 85/1000, LR 0.000266
Train loss: 0.2014;  Loss pred: 0.2014; Loss self: 0.0000; time: 0.58s
Val loss: 0.3085 score: 0.8980 time: 0.47s
Test loss: 0.2170 score: 0.9592 time: 0.50s
Epoch 86/1000, LR 0.000266
Train loss: 0.1984;  Loss pred: 0.1984; Loss self: 0.0000; time: 0.51s
Val loss: 0.3052 score: 0.8980 time: 0.46s
Test loss: 0.2084 score: 0.9388 time: 0.58s
Epoch 87/1000, LR 0.000266
Train loss: 0.1836;  Loss pred: 0.1836; Loss self: 0.0000; time: 0.48s
Val loss: 0.3027 score: 0.8980 time: 0.49s
Test loss: 0.2005 score: 0.9388 time: 0.53s
Epoch 88/1000, LR 0.000266
Train loss: 0.1717;  Loss pred: 0.1717; Loss self: 0.0000; time: 0.48s
Val loss: 0.3011 score: 0.8980 time: 0.56s
Test loss: 0.1931 score: 0.9388 time: 0.49s
Epoch 89/1000, LR 0.000266
Train loss: 0.1710;  Loss pred: 0.1710; Loss self: 0.0000; time: 0.47s
Val loss: 0.3004 score: 0.8980 time: 0.48s
Test loss: 0.1863 score: 0.9388 time: 0.47s
Epoch 90/1000, LR 0.000266
Train loss: 0.1601;  Loss pred: 0.1601; Loss self: 0.0000; time: 0.48s
Val loss: 0.3005 score: 0.8980 time: 0.54s
Test loss: 0.1791 score: 0.9388 time: 0.49s
     INFO: Early stopping counter 1 of 20
Epoch 91/1000, LR 0.000266
Train loss: 0.1558;  Loss pred: 0.1558; Loss self: 0.0000; time: 0.54s
Val loss: 0.3011 score: 0.8980 time: 3.72s
Test loss: 0.1716 score: 0.9388 time: 3.57s
     INFO: Early stopping counter 2 of 20
Epoch 92/1000, LR 0.000266
Train loss: 0.1411;  Loss pred: 0.1411; Loss self: 0.0000; time: 0.56s
Val loss: 0.3024 score: 0.8980 time: 0.45s
Test loss: 0.1640 score: 0.9388 time: 0.51s
     INFO: Early stopping counter 3 of 20
Epoch 93/1000, LR 0.000265
Train loss: 0.1380;  Loss pred: 0.1380; Loss self: 0.0000; time: 0.47s
Val loss: 0.3044 score: 0.8980 time: 0.44s
Test loss: 0.1567 score: 0.9388 time: 0.57s
     INFO: Early stopping counter 4 of 20
Epoch 94/1000, LR 0.000265
Train loss: 0.1445;  Loss pred: 0.1445; Loss self: 0.0000; time: 0.47s
Val loss: 0.3071 score: 0.8980 time: 0.48s
Test loss: 0.1505 score: 0.9388 time: 0.48s
     INFO: Early stopping counter 5 of 20
Epoch 95/1000, LR 0.000265
Train loss: 0.1304;  Loss pred: 0.1304; Loss self: 0.0000; time: 0.45s
Val loss: 0.3105 score: 0.8980 time: 0.45s
Test loss: 0.1444 score: 0.9592 time: 0.57s
     INFO: Early stopping counter 6 of 20
Epoch 96/1000, LR 0.000265
Train loss: 0.1240;  Loss pred: 0.1240; Loss self: 0.0000; time: 0.50s
Val loss: 0.3144 score: 0.8980 time: 0.47s
Test loss: 0.1385 score: 0.9592 time: 0.48s
     INFO: Early stopping counter 7 of 20
Epoch 97/1000, LR 0.000265
Train loss: 0.1239;  Loss pred: 0.1239; Loss self: 0.0000; time: 0.46s
Val loss: 0.3188 score: 0.8980 time: 0.54s
Test loss: 0.1330 score: 0.9592 time: 0.49s
     INFO: Early stopping counter 8 of 20
Epoch 98/1000, LR 0.000265
Train loss: 0.1136;  Loss pred: 0.1136; Loss self: 0.0000; time: 0.54s
Val loss: 0.3236 score: 0.8980 time: 0.45s
Test loss: 0.1271 score: 0.9592 time: 0.55s
     INFO: Early stopping counter 9 of 20
Epoch 99/1000, LR 0.000265
Train loss: 0.1016;  Loss pred: 0.1016; Loss self: 0.0000; time: 0.57s
Val loss: 0.3287 score: 0.8980 time: 0.49s
Test loss: 0.1212 score: 0.9592 time: 0.48s
     INFO: Early stopping counter 10 of 20
Epoch 100/1000, LR 0.000265
Train loss: 0.1032;  Loss pred: 0.1032; Loss self: 0.0000; time: 0.47s
Val loss: 0.3343 score: 0.8980 time: 0.46s
Test loss: 0.1154 score: 0.9592 time: 0.61s
     INFO: Early stopping counter 11 of 20
Epoch 101/1000, LR 0.000265
Train loss: 0.1061;  Loss pred: 0.1061; Loss self: 0.0000; time: 0.49s
Val loss: 0.3403 score: 0.8980 time: 0.44s
Test loss: 0.1105 score: 0.9592 time: 0.48s
     INFO: Early stopping counter 12 of 20
Epoch 102/1000, LR 0.000264
Train loss: 0.0970;  Loss pred: 0.0970; Loss self: 0.0000; time: 0.48s
Val loss: 0.3466 score: 0.8980 time: 0.58s
Test loss: 0.1059 score: 0.9592 time: 0.50s
     INFO: Early stopping counter 13 of 20
Epoch 103/1000, LR 0.000264
Train loss: 0.1092;  Loss pred: 0.1092; Loss self: 0.0000; time: 0.46s
Val loss: 0.3533 score: 0.8980 time: 0.45s
Test loss: 0.1024 score: 0.9592 time: 0.49s
     INFO: Early stopping counter 14 of 20
Epoch 104/1000, LR 0.000264
Train loss: 0.0957;  Loss pred: 0.0957; Loss self: 0.0000; time: 0.50s
Val loss: 0.3605 score: 0.8980 time: 0.55s
Test loss: 0.0996 score: 0.9592 time: 0.49s
     INFO: Early stopping counter 15 of 20
Epoch 105/1000, LR 0.000264
Train loss: 0.0888;  Loss pred: 0.0888; Loss self: 0.0000; time: 0.48s
Val loss: 0.3679 score: 0.8980 time: 0.47s
Test loss: 0.0969 score: 0.9592 time: 0.49s
     INFO: Early stopping counter 16 of 20
Epoch 106/1000, LR 0.000264
Train loss: 0.0902;  Loss pred: 0.0902; Loss self: 0.0000; time: 0.60s
Val loss: 0.3757 score: 0.8980 time: 0.44s
Test loss: 0.0945 score: 0.9592 time: 0.49s
     INFO: Early stopping counter 17 of 20
Epoch 107/1000, LR 0.000264
Train loss: 0.0752;  Loss pred: 0.0752; Loss self: 0.0000; time: 0.48s
Val loss: 0.3836 score: 0.8980 time: 0.45s
Test loss: 0.0917 score: 0.9592 time: 0.58s
     INFO: Early stopping counter 18 of 20
Epoch 108/1000, LR 0.000264
Train loss: 0.0727;  Loss pred: 0.0727; Loss self: 0.0000; time: 0.47s
Val loss: 0.3914 score: 0.8980 time: 0.44s
Test loss: 0.0885 score: 0.9592 time: 0.50s
     INFO: Early stopping counter 19 of 20
Epoch 109/1000, LR 0.000264
Train loss: 0.0808;  Loss pred: 0.0808; Loss self: 0.0000; time: 0.48s
Val loss: 0.3991 score: 0.8980 time: 0.45s
Test loss: 0.0850 score: 0.9592 time: 0.58s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 088,   Train_Loss: 0.1710,   Val_Loss: 0.3004,   Val_Precision: 0.9167,   Val_Recall: 0.8800,   Val_accuracy: 0.8980,   Val_Score: 0.8980,   Val_Loss: 0.3004,   Test_Precision: 1.0000,   Test_Recall: 0.8750,   Test_accuracy: 0.9333,   Test_Score: 0.9388,   Test_loss: 0.1863


[0.47569059603847563, 0.45132147101685405, 0.44563297485001385, 0.5580142331309617, 0.5506600718945265, 0.4633305750321597, 0.5788522260263562, 0.44643664499744773, 0.4611370291095227, 0.45920622791163623, 0.4509186039213091, 0.4616614580154419, 0.5626949791330844, 0.6981428579892963, 0.4675599308684468, 0.5515576610341668, 0.4389668151270598, 0.47475144313648343, 0.5560952147934586, 2.7768845919054, 0.4467680021189153, 0.44475991697981954, 0.48681075079366565, 0.5424957610666752, 0.4337174950633198, 0.5407437509857118, 0.44338470813818276, 0.5634947181679308, 0.44345019292086363, 0.5320004390086979, 0.46075096097774804, 0.44417128805071115, 0.5427314669359475, 0.44638587604276836, 0.5443193190731108, 0.4632013849914074, 0.44247378781437874, 0.45457864995114505, 0.46007012692280114, 0.45130646182224154, 0.4651479651220143, 0.5706384810619056, 0.45046776602976024, 0.5618159910663962, 0.44742191303521395, 0.4812281639315188, 0.4485834420192987, 0.48323942394927144, 0.4637891938909888, 0.4905513960402459, 0.5411260868422687, 0.5012516570277512, 0.5992905881721526, 0.4722739290446043, 0.4516412790399045, 0.5635383401531726, 0.45252237701788545, 0.4731232519261539, 0.4520899001508951, 0.5200490329880267, 0.5552324298769236, 0.44297137996181846, 0.546603481983766, 0.44949730997905135, 0.5797838601283729, 0.5038830840494484, 0.44426238909363747, 0.4655644129961729, 0.44806572212837636, 0.5607313578948379, 0.4501445088535547, 0.5615581821184605, 0.47095722099766135, 0.4605841168668121, 0.46516172704286873, 0.48626545583829284, 0.44313494791276753, 0.4646034771576524, 0.5522780469618738, 0.46834656689316034, 0.5353503781370819, 0.45680899010039866, 0.44849530002102256, 0.45338086504489183, 0.46530835307203233, 0.48168550501577556, 0.45204843604005873, 0.5558993201702833, 0.4622029459569603, 0.5743302810005844, 0.4636002550832927, 0.44945880491286516, 0.5904106369707733, 0.44257127889432013, 0.48433129698969424, 0.4620950990356505, 0.4691790679935366, 0.5617187810130417, 0.449422013014555, 0.5833343579433858, 0.44469594210386276, 0.5698766501154751, 0.46284815203398466, 0.47052720887586474, 0.47525181504897773, 0.6149143981747329, 0.5664069140329957, 0.5068021349143237, 0.5629306701011956, 0.8275662770029157, 0.6146490080282092, 1.755091428058222, 0.4871865180321038, 0.5181932940613478, 0.4836569200269878, 0.4829591060988605, 0.5935941750649363, 0.4767094780690968, 0.608224819181487, 0.4788183409254998, 0.5335464931558818, 0.5161989917978644, 0.5085954258684069, 0.6558945630677044, 0.5106567488983274, 0.4812549741473049, 0.4836956411600113, 0.5348749540280551, 0.6176231419667602, 0.48940634401515126, 0.48621256719343364, 0.49607345182448626, 0.6075392921920866, 0.494806915987283, 0.5142126611899585, 0.588398122927174, 0.497029447928071, 0.47673650295473635, 0.5822372629772872, 0.5055563619825989, 0.5756781990639865, 0.4770364190917462, 0.6214373130351305, 0.5170454408507794, 0.48685222398489714, 0.5699482760392129, 0.4886149459052831, 0.6167727350257337, 0.4869499960914254, 0.4747337568551302, 0.4823769030626863, 0.4855148959904909, 0.5996531471610069, 0.4888298308942467, 0.5213916150387377, 0.4893414000980556, 0.49327093409374356, 0.49515158007852733, 0.5500980010256171, 0.5959922890178859, 0.4782794669736177, 0.6077750641852617, 0.512144047068432, 0.4940414070151746, 0.4846542151644826, 0.4896771148778498, 0.6240508831106126, 0.5098212799057364, 0.4996143670286983, 0.5080307889729738, 0.48945129895582795, 0.515863869106397, 0.6173210649285465, 0.6745133169461042, 0.5462902658618987, 0.7878484840039164, 0.7277248441241682, 0.6134028770029545, 0.6173826500307769, 0.6041534210089594, 0.598986288998276, 0.5114207568112761, 0.4955671050120145, 0.48638336011208594, 0.4891646499745548, 0.489011517027393, 0.5193025819025934, 0.5769224690739065, 0.4984747169073671, 0.5798924448899925, 0.5304183168336749, 0.5126021690666676, 0.4887057109735906, 0.5064233299344778, 0.5884908630978316, 0.5306974861305207, 0.4904935860540718, 0.4782085558399558, 0.4981984740588814, 3.5769919550511986, 0.5186644229106605, 0.5803730820771307, 0.4844402091111988, 0.5781402210704982, 0.48104586102999747, 0.4988194510806352, 0.5538830880541354, 0.4847662921529263, 0.6191692280117422, 0.4799487479031086, 0.5019158809445798, 0.49082080903463066, 0.4905041470192373, 0.49390440597198904, 0.49187476793304086, 0.5851622081827372, 0.5034956210292876, 0.5832608800847083]
[0.009707971347723993, 0.009210642265650083, 0.00909455050714314, 0.011388045574101259, 0.011237960650908704, 0.009455726021064483, 0.01181331073523176, 0.009110951938723423, 0.009410959777745361, 0.009371555671666046, 0.009202420488189983, 0.009421662408478406, 0.011483571002716008, 0.014247813428352987, 0.009542039405478507, 0.01125627879661565, 0.008958506431164486, 0.00968880496196905, 0.011348881934560379, 0.05667111412051837, 0.009117714328957456, 0.009076732999588154, 0.00993491328150338, 0.011071342062585207, 0.008851377450271833, 0.011035586754810445, 0.009048667513024139, 0.011499892207508793, 0.009050003937160482, 0.010857151816504039, 0.009403080836280572, 0.009064720164300228, 0.011076152386447909, 0.009109915837607518, 0.011108557532104303, 0.009453089489620559, 0.00903007730233426, 0.00927711530512541, 0.009389186263730635, 0.00921033595555595, 0.009492815614734985, 0.011645683286977666, 0.009193219714893066, 0.01146563247074278, 0.009131059449698244, 0.009820982937377935, 0.00915476412284283, 0.009862029060189213, 0.009465085589612017, 0.010011252980413181, 0.01104338952739324, 0.010229625653627575, 0.012230420166778624, 0.009638243449889883, 0.009217168959998051, 0.011500782452105564, 0.009235150551385417, 0.009655576569921508, 0.00922632449287541, 0.010613245571184218, 0.01133127407912089, 0.009040232244118745, 0.01115517310170951, 0.009173414489368394, 0.011832323676089242, 0.01028332824590711, 0.009066579369257907, 0.009501314550942304, 0.009144198410783191, 0.01144349709989465, 0.009186622629664382, 0.011460371063642052, 0.00961137185709513, 0.009399675854424737, 0.009493096470262627, 0.009923784813026384, 0.009043570365566684, 0.009481703615462293, 0.011270980550242322, 0.009558093201901232, 0.010925517921164935, 0.009322632451028543, 0.009152965306551481, 0.009252670715201874, 0.009496088838204741, 0.00983031642889338, 0.009225478286531811, 0.011344884085107823, 0.009432713182795108, 0.011721026142869068, 0.009461229695577403, 0.009172628671691125, 0.012049196672872926, 0.009032066916210614, 0.009884312183463148, 0.009430512225217357, 0.009575083020276256, 0.011463648592102893, 0.009171877816623571, 0.01190478281517114, 0.009075427389874751, 0.011630135716642348, 0.009445880653754788, 0.009602596099507444, 0.009699016633652607, 0.012549273432137407, 0.011559324776183585, 0.010342900712537219, 0.01148838102247338, 0.016889107693937058, 0.012543857306698146, 0.03581819240935147, 0.00994258200065518, 0.010575373348190772, 0.009870549388305873, 0.009856308287731846, 0.012114166838059924, 0.009728764858552995, 0.012412751411867082, 0.009771802876030609, 0.01088870394195677, 0.010534673301997234, 0.010379498487110344, 0.013385603327912336, 0.010421566304047497, 0.009821530084638876, 0.009871339615510434, 0.010915815388327655, 0.012604553917688983, 0.009987884571737781, 0.009922705452927217, 0.010123947996418086, 0.012398761065144624, 0.010098100326271082, 0.010494135942652213, 0.012008124957697429, 0.01014345812098104, 0.009729316386831353, 0.011882393121985453, 0.01031747677515508, 0.011748534674775235, 0.009735437124321351, 0.012682394143574091, 0.010551947772464886, 0.009935759673161166, 0.01163159747018802, 0.009971733589903737, 0.012587198673994566, 0.009937755022273988, 0.009688444017451637, 0.009844426593116047, 0.00990846726511206, 0.012237819329816468, 0.00997611899784177, 0.010640645204872196, 0.009986559185674605, 0.010066753757015174, 0.010105134287316884, 0.011226489816849329, 0.012163107939140529, 0.009760805448441177, 0.01240357273847473, 0.010451919327927182, 0.010082477694187237, 0.009890902350295563, 0.009993410507711221, 0.012735732308379849, 0.010404515916443601, 0.010196211572014252, 0.010367975285162732, 0.009988802019506693, 0.010527834063395858, 0.012598389080174419, 0.013765577896859268, 0.011148780935957116, 0.016078540489875844, 0.014851527431105472, 0.012518426061284785, 0.012599645918995445, 0.01232966165324407, 0.012224209979556653, 0.010437158302270941, 0.010113614388000297, 0.009926191022695631, 0.009982952040297036, 0.009979826878110061, 0.01059801187556313, 0.011773927940283807, 0.010172953406272799, 0.0118345396916325, 0.010824863608850509, 0.010461268756462604, 0.009973585938236542, 0.010335169998662812, 0.012010017614241461, 0.010830560941439199, 0.010010073184776975, 0.009759358282448078, 0.01016731579712003, 0.0729998358173714, 0.010584988222666541, 0.011844348613818993, 0.009886534879820384, 0.011798780021846903, 0.009817262469999949, 0.010179988797563983, 0.011303736490900723, 0.009893189635774007, 0.012636106694117188, 0.00979487240618589, 0.010243181243766936, 0.010016751204788382, 0.010010288714678312, 0.010079681754530388, 0.010038260570062059, 0.011942085881280352, 0.0102754208373324, 0.011903283267034864]
[103.00813261407568, 108.57006180007366, 109.95595650544458, 87.81138023140724, 88.9841165193206, 105.75602526683873, 84.65027479702383, 109.75801504887694, 106.25908766125615, 106.70586987210665, 108.66706224556462, 106.13838159814723, 87.0809262870833, 70.18620822266043, 104.79939953149385, 88.83930631681434, 111.62575008276389, 103.2119032146118, 88.11440684343826, 17.645673911992837, 109.67661015919795, 110.17179860257802, 100.65513121908972, 90.32328640440322, 112.976766115571, 90.61593390710259, 110.51350915045302, 86.95733681286642, 110.4971894977715, 92.10518715229647, 106.34812328121536, 110.31780152886797, 90.28405940166893, 109.77049819404523, 90.02068874468613, 105.78552134706803, 110.74102319605853, 107.79212795248122, 106.5055023844704, 108.5736725376201, 105.34282351884883, 85.86872709463182, 108.77581859378326, 87.217168573276, 109.51631686430946, 101.82280189023382, 109.23274336526214, 101.39901169392965, 105.65144821273518, 99.88759668310053, 90.55190867980251, 97.75528781401549, 81.76333980056471, 103.7533452230266, 108.49318313898105, 86.95060567960921, 108.28193806217749, 103.56709335361101, 108.38552240084363, 94.22188465280378, 88.25132928719898, 110.61662720563011, 89.64450760936663, 109.0106634949242, 84.51425327560975, 97.24478068644869, 110.29517961213736, 105.24859424855309, 109.35895691204179, 87.38587437656676, 108.85393254000827, 87.25720960052446, 104.04342011403902, 106.38664731499938, 105.3397069262412, 100.76800523600201, 110.57579690068995, 105.46627911562743, 88.7234252195121, 104.62337820697195, 91.52884167283254, 107.26583990658908, 109.25421068560405, 108.07690349955159, 105.30651271677155, 101.72612522022064, 108.39546405522309, 88.1454576792616, 106.01403653658843, 85.31676218539859, 105.69450612402363, 109.02000242157774, 82.99308469678809, 110.71662879348418, 101.1704184812212, 106.03877881903193, 104.43773676764928, 87.23226222137362, 109.02892733563785, 83.9998524564116, 110.18764814488819, 85.98351939857695, 105.86625394239917, 104.13850479989406, 103.10323590231894, 79.68588822354465, 86.51024340628994, 96.6846755850458, 87.04446675678815, 59.20975922007919, 79.72029460714782, 27.918773470514903, 100.57749585913434, 94.55930935725114, 101.31148334911826, 101.45786544082642, 82.54797984606174, 102.78797098491438, 80.5623158652771, 102.33526122931869, 91.83829456017826, 94.92463328790778, 96.34376855893741, 74.70712940631891, 95.95486617128013, 101.81712944748045, 101.30337309322644, 91.61019717036493, 79.33640543967364, 100.1213012442745, 100.77896645667317, 98.77569505037026, 80.65321968427945, 99.02852691990131, 95.2913136884014, 83.27694819322993, 98.58570795807489, 102.78214421657637, 84.15813125638348, 96.92292231837558, 85.11699779437654, 102.71752436280144, 78.84946554091125, 94.76923327932703, 100.64655677021217, 85.97271377065935, 100.28346535576203, 79.44579456476066, 100.62634848199114, 103.21574839042432, 101.58031963987361, 100.9237829872056, 81.7139044996015, 100.23938168904559, 93.97926354523263, 100.13458904188619, 99.3368889452704, 98.95959534700256, 89.07503737269198, 82.21582879997545, 102.45056161422644, 80.62193217105045, 95.67620727114043, 99.1819699811011, 101.10301007775269, 100.06593837292779, 78.51923829633421, 96.11211208967164, 98.07564240278421, 96.45084719974855, 100.11210534027443, 94.98629955395023, 79.37522754981904, 72.64497048308871, 89.69590538592377, 62.19469986281832, 67.33314163400937, 79.88224678601237, 79.36730971879, 81.10522641445631, 81.80487750720623, 95.81151986383284, 98.87661934060786, 100.74357804655995, 100.17077072627563, 100.20213899636062, 94.35732019755494, 84.9334228196317, 98.29987026023187, 84.49842799606651, 92.37991684092822, 95.5906996827928, 100.2648401680903, 96.75699578520549, 83.26382459374592, 92.3313211021106, 99.899369519173, 102.46575349103341, 98.35437591928223, 13.698660946330993, 94.47341640481133, 84.4284504454119, 101.14767329058043, 84.75452531095385, 101.86138987888395, 98.23193520991836, 88.4663226894027, 101.07963526585768, 79.13829981077603, 102.09423446582687, 97.62592071760018, 99.83276808572052, 99.89721860206464, 99.20948144523933, 99.61885259109366, 83.73746512470957, 97.31961501438707, 84.01043456383294]
Elapsed: 0.5451142747369552~0.2767201356144512
Time per graph: 0.011124781117080719~0.005647349706417371
Speed: 95.36055865243426~13.74224906910764
Total Time: 0.5840
best val loss: 0.3004359006881714 test_score: 0.9388

Testing...
Test loss: 0.4210 score: 0.9388 time: 0.48s
test Score 0.9388
Epoch Time List: [1.4446834400296211, 1.5173492198809981, 1.4439547511283308, 1.6171266881283373, 1.6920144651085138, 1.442387935006991, 1.5468192240223289, 1.4014949609991163, 1.5567117389291525, 1.4461964750662446, 1.6603012459818274, 1.4049961981363595, 1.67645474197343, 1.8866713009774685, 1.5025117918848991, 1.546161302132532, 1.3739364249631763, 1.5171756027266383, 1.5105000650510192, 7.980472585884854, 1.4902043130714446, 1.40281723998487, 1.5279595910105854, 1.4853580894414335, 1.531892633996904, 1.4720180269796401, 1.4004038127604872, 1.4857338978908956, 1.4652372552081943, 1.5832940398249775, 1.4113712888211012, 1.5133749393280596, 1.4965526319574565, 1.4004312958568335, 1.496852247044444, 1.3876855608541518, 1.4774381013121456, 1.4089015130884945, 1.4988046039361507, 1.3736089039593935, 1.4954373468644917, 1.521885374095291, 1.4387210558634251, 1.513912706868723, 1.4222782209981233, 1.536484091077, 1.401933484012261, 1.5549291127827018, 1.4103303977753967, 1.5661356169730425, 1.5092918269801885, 1.5299781439825892, 1.6782864080742002, 1.4424282838590443, 1.568008187925443, 1.530445251846686, 1.4560769409872591, 1.5457732940558344, 1.436869990779087, 1.6290802382864058, 1.5061529288068414, 1.4425094779580832, 1.5033503929153085, 1.4309271869715303, 1.5394161629956216, 1.4549784532282501, 1.5304208036977798, 1.4408991488162428, 1.5373330239672214, 1.513481937116012, 1.44654948473908, 1.5292556958738714, 1.5414456299040467, 1.5501244047190994, 1.4232214309740812, 1.564979740884155, 1.3982208957895637, 1.523490167921409, 1.5220924771856517, 1.406131395837292, 1.5447344628628343, 1.4104504219722003, 1.5838465243577957, 1.4562380292918533, 1.5509592660237104, 1.4823544039390981, 1.5196051669772714, 1.5537130658049136, 1.415250901132822, 1.5843627869617194, 1.4366332220379263, 1.582790478831157, 1.5559978180099279, 1.3915487276390195, 1.5535516368690878, 1.424420117866248, 1.5739897591993213, 1.523830747930333, 1.528723810100928, 1.5469024209305644, 1.4073046641424298, 1.5728628432843834, 1.4196468458976597, 1.7392312481533736, 1.4670056400354952, 1.8179561262950301, 1.608713734196499, 1.6751484929118305, 1.583874307340011, 1.995465942658484, 1.7505325586535037, 7.186693282332271, 1.3961065621115267, 1.5392196711618453, 1.3903920783195645, 1.5773709118366241, 1.5158911237958819, 1.4844542082864791, 1.5418565808795393, 1.4632580529432744, 1.5802119942381978, 1.4354577760677785, 1.6476119153667241, 1.5706814308650792, 1.4574842990841717, 1.5475160100031644, 1.3963068767916411, 1.5720498501323164, 1.5392892640084028, 1.4146238979883492, 1.5281492450740188, 1.4174594143405557, 1.9855754461605102, 1.4655106239952147, 1.5600931157823652, 1.6444998341612518, 1.4482709791045636, 1.493876511696726, 1.5418390410486609, 1.4984377331566066, 1.5439693487714976, 1.4153810343705118, 1.56015973421745, 1.655649685766548, 1.5100218998268247, 1.486603903118521, 1.4664497850462794, 1.5158341727219522, 1.4080285001546144, 1.512663624016568, 1.4175157607533038, 1.4898346008267254, 1.5155999229755253, 1.4445011150091887, 1.5471124867908657, 1.4095422339159995, 1.5619348976761103, 1.4027224911842495, 1.587080653058365, 1.6190360698383301, 1.4477302278392017, 1.5346601239871234, 1.7423182160127908, 1.5504813289735466, 1.4265429012011737, 1.611440556589514, 1.558623629854992, 1.4446709479670972, 1.6074231970123947, 1.4462406837847084, 1.6663672199938446, 1.4635612128768116, 2.020705613307655, 1.8597839497961104, 1.486741840140894, 2.02298490004614, 1.9855277009774, 1.7683872359339148, 1.7897547667380422, 1.7199799260124564, 1.5141817932017148, 1.6829063361510634, 1.5065754770766944, 1.403051109984517, 1.5158018919173628, 1.3973195811267942, 1.543094219174236, 1.5847457919735461, 1.4178919817786664, 1.505253316136077, 1.4618828911334276, 1.5421329580713063, 1.4673830738756806, 1.5542367789894342, 1.5538082821294665, 1.4882999786641449, 1.5290048397146165, 1.4283227608539164, 1.5088920190464705, 7.833677162881941, 1.5258369261864573, 1.482954251114279, 1.428927507949993, 1.47365412581712, 1.4396672991570085, 1.498126773396507, 1.5363255369011313, 1.5375517851207405, 1.5459188411477953, 1.4021554810460657, 1.5554368770681322, 1.4040687379892915, 1.5374048857484013, 1.4443335549440235, 1.534068251028657, 1.5134225238580257, 1.4107196819968522, 1.499554448062554]
Total Epoch List: [109, 109]
Total Time List: [0.5635228881146759, 0.5840295979287475]
========================k_idx:2========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: FAGNN
FAGNN(
  (conv1): NeighborFilterConv()
  (conv2): NeighborFilterConv()
  (ffn): Sequential(
    (0): Linear(in_features=14951, out_features=64, bias=True)
    (1): ReLU()
    (2): Linear(in_features=64, out_features=64, bias=True)
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=64, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=64, out_features=2, bias=True)
  )
)
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7ed848824880>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.6956;  Loss pred: 0.6956; Loss self: 0.0000; time: 0.45s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6995 score: 0.4898 time: 0.52s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6976 score: 0.5000 time: 0.42s
Epoch 2/1000, LR 0.000000
Train loss: 0.6956;  Loss pred: 0.6956; Loss self: 0.0000; time: 0.54s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6994 score: 0.4898 time: 0.53s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6975 score: 0.5000 time: 0.42s
Epoch 3/1000, LR 0.000030
Train loss: 0.6956;  Loss pred: 0.6956; Loss self: 0.0000; time: 0.44s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6993 score: 0.4898 time: 0.71s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6974 score: 0.5000 time: 0.50s
Epoch 4/1000, LR 0.000060
Train loss: 0.6955;  Loss pred: 0.6955; Loss self: 0.0000; time: 0.45s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6992 score: 0.4898 time: 0.54s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6973 score: 0.5000 time: 0.41s
Epoch 5/1000, LR 0.000090
Train loss: 0.6954;  Loss pred: 0.6954; Loss self: 0.0000; time: 0.46s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6990 score: 0.4898 time: 0.62s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6971 score: 0.5000 time: 0.41s
Epoch 6/1000, LR 0.000120
Train loss: 0.6953;  Loss pred: 0.6953; Loss self: 0.0000; time: 0.45s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6988 score: 0.4898 time: 0.53s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6970 score: 0.5000 time: 0.42s
Epoch 7/1000, LR 0.000150
Train loss: 0.6951;  Loss pred: 0.6951; Loss self: 0.0000; time: 0.49s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6985 score: 0.4898 time: 0.62s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6967 score: 0.5000 time: 0.41s
Epoch 8/1000, LR 0.000180
Train loss: 0.6950;  Loss pred: 0.6950; Loss self: 0.0000; time: 0.46s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6982 score: 0.4898 time: 0.56s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6964 score: 0.5000 time: 0.43s
Epoch 9/1000, LR 0.000210
Train loss: 0.6947;  Loss pred: 0.6947; Loss self: 0.0000; time: 0.49s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6979 score: 0.4898 time: 0.63s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6961 score: 0.5000 time: 0.43s
Epoch 10/1000, LR 0.000240
Train loss: 0.6945;  Loss pred: 0.6945; Loss self: 0.0000; time: 0.45s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6975 score: 0.4898 time: 0.54s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6958 score: 0.5000 time: 0.42s
Epoch 11/1000, LR 0.000270
Train loss: 0.6942;  Loss pred: 0.6942; Loss self: 0.0000; time: 0.47s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6971 score: 0.4898 time: 0.72s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6955 score: 0.5000 time: 0.43s
Epoch 12/1000, LR 0.000270
Train loss: 0.6940;  Loss pred: 0.6940; Loss self: 0.0000; time: 0.49s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6967 score: 0.4898 time: 0.53s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6951 score: 0.5000 time: 0.46s
Epoch 13/1000, LR 0.000270
Train loss: 0.6937;  Loss pred: 0.6937; Loss self: 0.0000; time: 0.56s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6963 score: 0.4898 time: 0.53s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6947 score: 0.5000 time: 0.43s
Epoch 14/1000, LR 0.000270
Train loss: 0.6934;  Loss pred: 0.6934; Loss self: 0.0000; time: 0.46s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6959 score: 0.4898 time: 0.56s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6942 score: 0.5000 time: 0.60s
Epoch 15/1000, LR 0.000270
Train loss: 0.6930;  Loss pred: 0.6930; Loss self: 0.0000; time: 0.45s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6953 score: 0.4898 time: 0.54s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6937 score: 0.5000 time: 0.43s
Epoch 16/1000, LR 0.000270
Train loss: 0.6926;  Loss pred: 0.6926; Loss self: 0.0000; time: 0.51s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6948 score: 0.4898 time: 0.62s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6932 score: 0.5000 time: 0.42s
Epoch 17/1000, LR 0.000270
Train loss: 0.6921;  Loss pred: 0.6921; Loss self: 0.0000; time: 0.46s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6943 score: 0.4898 time: 0.53s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6927 score: 0.5000 time: 0.44s
Epoch 18/1000, LR 0.000270
Train loss: 0.6917;  Loss pred: 0.6917; Loss self: 0.0000; time: 0.48s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6938 score: 0.4898 time: 0.61s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6922 score: 0.5000 time: 0.43s
Epoch 19/1000, LR 0.000270
Train loss: 0.6913;  Loss pred: 0.6913; Loss self: 0.0000; time: 0.46s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6932 score: 0.4898 time: 0.54s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6916 score: 0.5000 time: 0.54s
Epoch 20/1000, LR 0.000270
Train loss: 0.6909;  Loss pred: 0.6909; Loss self: 0.0000; time: 0.45s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6926 score: 0.4898 time: 0.53s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6910 score: 0.5000 time: 0.43s
Epoch 21/1000, LR 0.000270
Train loss: 0.6903;  Loss pred: 0.6903; Loss self: 0.0000; time: 0.46s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6920 score: 0.4898 time: 0.56s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6904 score: 0.5000 time: 0.53s
Epoch 22/1000, LR 0.000270
Train loss: 0.6899;  Loss pred: 0.6899; Loss self: 0.0000; time: 0.46s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6914 score: 0.4898 time: 0.55s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6899 score: 0.5000 time: 0.53s
Epoch 23/1000, LR 0.000270
Train loss: 0.6894;  Loss pred: 0.6894; Loss self: 0.0000; time: 0.59s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6909 score: 0.4898 time: 0.63s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6893 score: 0.5000 time: 0.42s
Epoch 24/1000, LR 0.000270
Train loss: 0.6889;  Loss pred: 0.6889; Loss self: 0.0000; time: 0.46s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6903 score: 0.4898 time: 0.54s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6887 score: 0.5000 time: 0.45s
Epoch 25/1000, LR 0.000270
Train loss: 0.6886;  Loss pred: 0.6886; Loss self: 0.0000; time: 0.48s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6897 score: 0.4898 time: 0.63s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6881 score: 0.5000 time: 0.42s
Epoch 26/1000, LR 0.000270
Train loss: 0.6879;  Loss pred: 0.6879; Loss self: 0.0000; time: 0.47s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6890 score: 0.4898 time: 0.53s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6874 score: 0.5000 time: 0.66s
Epoch 27/1000, LR 0.000270
Train loss: 0.6874;  Loss pred: 0.6874; Loss self: 0.0000; time: 0.45s
Val loss: 0.6883 score: 0.5102 time: 0.58s
Test loss: 0.6866 score: 0.6042 time: 0.43s
Epoch 28/1000, LR 0.000270
Train loss: 0.6866;  Loss pred: 0.6866; Loss self: 0.0000; time: 0.47s
Val loss: 0.6875 score: 0.6327 time: 0.58s
Test loss: 0.6857 score: 0.6667 time: 0.52s
Epoch 29/1000, LR 0.000270
Train loss: 0.6858;  Loss pred: 0.6858; Loss self: 0.0000; time: 0.47s
Val loss: 0.6866 score: 0.7143 time: 0.54s
Test loss: 0.6847 score: 0.7292 time: 0.43s
Epoch 30/1000, LR 0.000270
Train loss: 0.6849;  Loss pred: 0.6849; Loss self: 0.0000; time: 0.47s
Val loss: 0.6856 score: 0.7551 time: 0.68s
Test loss: 0.6836 score: 0.7917 time: 0.42s
Epoch 31/1000, LR 0.000270
Train loss: 0.6842;  Loss pred: 0.6842; Loss self: 0.0000; time: 0.45s
Val loss: 0.6845 score: 0.7551 time: 0.55s
Test loss: 0.6825 score: 0.8333 time: 0.45s
Epoch 32/1000, LR 0.000270
Train loss: 0.6831;  Loss pred: 0.6831; Loss self: 0.0000; time: 0.50s
Val loss: 0.6834 score: 0.7551 time: 0.65s
Test loss: 0.6812 score: 0.7917 time: 0.43s
Epoch 33/1000, LR 0.000270
Train loss: 0.6819;  Loss pred: 0.6819; Loss self: 0.0000; time: 0.49s
Val loss: 0.6822 score: 0.7959 time: 0.57s
Test loss: 0.6799 score: 0.7917 time: 0.42s
Epoch 34/1000, LR 0.000270
Train loss: 0.6807;  Loss pred: 0.6807; Loss self: 0.0000; time: 0.56s
Val loss: 0.6808 score: 0.7551 time: 0.54s
Test loss: 0.6784 score: 0.8125 time: 0.43s
Epoch 35/1000, LR 0.000270
Train loss: 0.6787;  Loss pred: 0.6787; Loss self: 0.0000; time: 0.46s
Val loss: 0.6795 score: 0.7551 time: 0.58s
Test loss: 0.6769 score: 0.8125 time: 0.53s
Epoch 36/1000, LR 0.000270
Train loss: 0.6774;  Loss pred: 0.6774; Loss self: 0.0000; time: 0.46s
Val loss: 0.6780 score: 0.7551 time: 0.55s
Test loss: 0.6752 score: 0.7500 time: 0.42s
Epoch 37/1000, LR 0.000270
Train loss: 0.6761;  Loss pred: 0.6761; Loss self: 0.0000; time: 0.47s
Val loss: 0.6764 score: 0.7551 time: 0.64s
Test loss: 0.6734 score: 0.7292 time: 0.42s
Epoch 38/1000, LR 0.000270
Train loss: 0.6738;  Loss pred: 0.6738; Loss self: 0.0000; time: 0.47s
Val loss: 0.6746 score: 0.7551 time: 0.52s
Test loss: 0.6715 score: 0.7292 time: 0.41s
Epoch 39/1000, LR 0.000269
Train loss: 0.6722;  Loss pred: 0.6722; Loss self: 0.0000; time: 0.45s
Val loss: 0.6727 score: 0.7347 time: 0.62s
Test loss: 0.6693 score: 0.6667 time: 0.42s
Epoch 40/1000, LR 0.000269
Train loss: 0.6701;  Loss pred: 0.6701; Loss self: 0.0000; time: 0.46s
Val loss: 0.6705 score: 0.6531 time: 0.53s
Test loss: 0.6670 score: 0.6458 time: 0.52s
Epoch 41/1000, LR 0.000269
Train loss: 0.6680;  Loss pred: 0.6680; Loss self: 0.0000; time: 0.45s
Val loss: 0.6682 score: 0.6327 time: 0.54s
Test loss: 0.6644 score: 0.6250 time: 0.42s
Epoch 42/1000, LR 0.000269
Train loss: 0.6638;  Loss pred: 0.6638; Loss self: 0.0000; time: 0.46s
Val loss: 0.6658 score: 0.6327 time: 0.53s
Test loss: 0.6617 score: 0.6250 time: 0.51s
Epoch 43/1000, LR 0.000269
Train loss: 0.6616;  Loss pred: 0.6616; Loss self: 0.0000; time: 0.46s
Val loss: 0.6631 score: 0.6327 time: 0.53s
Test loss: 0.6587 score: 0.6250 time: 0.44s
Epoch 44/1000, LR 0.000269
Train loss: 0.6609;  Loss pred: 0.6609; Loss self: 0.0000; time: 0.45s
Val loss: 0.6602 score: 0.6122 time: 0.62s
Test loss: 0.6555 score: 0.6250 time: 0.42s
Epoch 45/1000, LR 0.000269
Train loss: 0.6559;  Loss pred: 0.6559; Loss self: 0.0000; time: 0.45s
Val loss: 0.6570 score: 0.6122 time: 0.52s
Test loss: 0.6521 score: 0.6250 time: 0.41s
Epoch 46/1000, LR 0.000269
Train loss: 0.6520;  Loss pred: 0.6520; Loss self: 0.0000; time: 0.45s
Val loss: 0.6536 score: 0.6122 time: 0.63s
Test loss: 0.6483 score: 0.6250 time: 0.45s
Epoch 47/1000, LR 0.000269
Train loss: 0.6461;  Loss pred: 0.6461; Loss self: 0.0000; time: 0.53s
Val loss: 0.6499 score: 0.6122 time: 0.52s
Test loss: 0.6443 score: 0.6250 time: 0.52s
Epoch 48/1000, LR 0.000269
Train loss: 0.6427;  Loss pred: 0.6427; Loss self: 0.0000; time: 0.47s
Val loss: 0.6458 score: 0.6122 time: 0.54s
Test loss: 0.6399 score: 0.6250 time: 0.46s
Epoch 49/1000, LR 0.000269
Train loss: 0.6379;  Loss pred: 0.6379; Loss self: 0.0000; time: 0.46s
Val loss: 0.6414 score: 0.6122 time: 0.54s
Test loss: 0.6351 score: 0.6250 time: 0.51s
Epoch 50/1000, LR 0.000269
Train loss: 0.6350;  Loss pred: 0.6350; Loss self: 0.0000; time: 0.46s
Val loss: 0.6366 score: 0.6327 time: 0.57s
Test loss: 0.6299 score: 0.6250 time: 0.43s
Epoch 51/1000, LR 0.000269
Train loss: 0.6303;  Loss pred: 0.6303; Loss self: 0.0000; time: 0.44s
Val loss: 0.6315 score: 0.6327 time: 0.64s
Test loss: 0.6243 score: 0.6250 time: 0.42s
Epoch 52/1000, LR 0.000269
Train loss: 0.6196;  Loss pred: 0.6196; Loss self: 0.0000; time: 0.49s
Val loss: 0.6259 score: 0.6327 time: 0.54s
Test loss: 0.6184 score: 0.6250 time: 0.41s
Epoch 53/1000, LR 0.000269
Train loss: 0.6168;  Loss pred: 0.6168; Loss self: 0.0000; time: 0.55s
Val loss: 0.6200 score: 0.6327 time: 0.78s
Test loss: 0.6121 score: 0.6250 time: 0.55s
Epoch 54/1000, LR 0.000269
Train loss: 0.6100;  Loss pred: 0.6100; Loss self: 0.0000; time: 0.56s
Val loss: 0.6137 score: 0.6327 time: 0.61s
Test loss: 0.6054 score: 0.6250 time: 0.43s
Epoch 55/1000, LR 0.000269
Train loss: 0.6014;  Loss pred: 0.6014; Loss self: 0.0000; time: 0.62s
Val loss: 0.6069 score: 0.6531 time: 0.69s
Test loss: 0.5983 score: 0.6458 time: 0.53s
Epoch 56/1000, LR 0.000269
Train loss: 0.5946;  Loss pred: 0.5946; Loss self: 0.0000; time: 0.56s
Val loss: 0.5998 score: 0.6735 time: 0.67s
Test loss: 0.5909 score: 0.6458 time: 0.60s
Epoch 57/1000, LR 0.000269
Train loss: 0.5849;  Loss pred: 0.5849; Loss self: 0.0000; time: 0.47s
Val loss: 0.5923 score: 0.6939 time: 0.54s
Test loss: 0.5830 score: 0.6875 time: 0.42s
Epoch 58/1000, LR 0.000269
Train loss: 0.5791;  Loss pred: 0.5791; Loss self: 0.0000; time: 0.46s
Val loss: 0.5842 score: 0.6939 time: 0.68s
Test loss: 0.5748 score: 0.7083 time: 0.43s
Epoch 59/1000, LR 0.000268
Train loss: 0.5710;  Loss pred: 0.5710; Loss self: 0.0000; time: 0.46s
Val loss: 0.5757 score: 0.7347 time: 0.54s
Test loss: 0.5661 score: 0.7292 time: 0.42s
Epoch 60/1000, LR 0.000268
Train loss: 0.5589;  Loss pred: 0.5589; Loss self: 0.0000; time: 0.48s
Val loss: 0.5667 score: 0.7551 time: 0.66s
Test loss: 0.5571 score: 0.7500 time: 0.42s
Epoch 61/1000, LR 0.000268
Train loss: 0.5500;  Loss pred: 0.5500; Loss self: 0.0000; time: 0.46s
Val loss: 0.5572 score: 0.7755 time: 0.54s
Test loss: 0.5476 score: 0.7708 time: 0.54s
Epoch 62/1000, LR 0.000268
Train loss: 0.5333;  Loss pred: 0.5333; Loss self: 0.0000; time: 0.46s
Val loss: 0.5472 score: 0.8163 time: 0.53s
Test loss: 0.5376 score: 0.7917 time: 0.42s
Epoch 63/1000, LR 0.000268
Train loss: 0.5234;  Loss pred: 0.5234; Loss self: 0.0000; time: 0.46s
Val loss: 0.5366 score: 0.8163 time: 0.52s
Test loss: 0.5273 score: 0.8125 time: 0.55s
Epoch 64/1000, LR 0.000268
Train loss: 0.5139;  Loss pred: 0.5139; Loss self: 0.0000; time: 0.46s
Val loss: 0.5256 score: 0.8163 time: 0.52s
Test loss: 0.5166 score: 0.8542 time: 0.43s
Epoch 65/1000, LR 0.000268
Train loss: 0.4985;  Loss pred: 0.4985; Loss self: 0.0000; time: 0.45s
Val loss: 0.5142 score: 0.8163 time: 0.64s
Test loss: 0.5056 score: 0.8542 time: 0.45s
Epoch 66/1000, LR 0.000268
Train loss: 0.4899;  Loss pred: 0.4899; Loss self: 0.0000; time: 0.45s
Val loss: 0.5024 score: 0.8163 time: 0.64s
Test loss: 0.4945 score: 0.8750 time: 0.53s
Epoch 67/1000, LR 0.000268
Train loss: 0.4810;  Loss pred: 0.4810; Loss self: 0.0000; time: 0.57s
Val loss: 0.4904 score: 0.8367 time: 0.64s
Test loss: 0.4832 score: 0.8750 time: 0.46s
Epoch 68/1000, LR 0.000268
Train loss: 0.4692;  Loss pred: 0.4692; Loss self: 0.0000; time: 0.44s
Val loss: 0.4783 score: 0.8571 time: 0.55s
Test loss: 0.4718 score: 0.8958 time: 0.59s
Epoch 69/1000, LR 0.000268
Train loss: 0.4488;  Loss pred: 0.4488; Loss self: 0.0000; time: 0.45s
Val loss: 0.4661 score: 0.8571 time: 0.56s
Test loss: 0.4604 score: 0.8958 time: 0.42s
Epoch 70/1000, LR 0.000268
Train loss: 0.4331;  Loss pred: 0.4331; Loss self: 0.0000; time: 0.46s
Val loss: 0.4538 score: 0.8776 time: 0.54s
Test loss: 0.4490 score: 0.8958 time: 0.52s
Epoch 71/1000, LR 0.000268
Train loss: 0.4273;  Loss pred: 0.4273; Loss self: 0.0000; time: 0.47s
Val loss: 0.4416 score: 0.8980 time: 0.53s
Test loss: 0.4379 score: 0.8958 time: 0.42s
Epoch 72/1000, LR 0.000267
Train loss: 0.4034;  Loss pred: 0.4034; Loss self: 0.0000; time: 0.46s
Val loss: 0.4295 score: 0.9184 time: 0.63s
Test loss: 0.4271 score: 0.8958 time: 0.44s
Epoch 73/1000, LR 0.000267
Train loss: 0.3904;  Loss pred: 0.3904; Loss self: 0.0000; time: 0.62s
Val loss: 0.4175 score: 0.9184 time: 0.67s
Test loss: 0.4166 score: 0.8958 time: 0.53s
Epoch 74/1000, LR 0.000267
Train loss: 0.3807;  Loss pred: 0.3807; Loss self: 0.0000; time: 0.57s
Val loss: 0.4057 score: 0.9184 time: 0.79s
Test loss: 0.4066 score: 0.8958 time: 0.52s
Epoch 75/1000, LR 0.000267
Train loss: 0.3788;  Loss pred: 0.3788; Loss self: 0.0000; time: 0.49s
Val loss: 0.3941 score: 0.9184 time: 0.56s
Test loss: 0.3971 score: 0.8958 time: 3.53s
Epoch 76/1000, LR 0.000267
Train loss: 0.3565;  Loss pred: 0.3565; Loss self: 0.0000; time: 1.95s
Val loss: 0.3828 score: 0.9184 time: 1.20s
Test loss: 0.3883 score: 0.8958 time: 0.42s
Epoch 77/1000, LR 0.000267
Train loss: 0.3500;  Loss pred: 0.3500; Loss self: 0.0000; time: 0.45s
Val loss: 0.3719 score: 0.9184 time: 0.52s
Test loss: 0.3799 score: 0.8958 time: 0.50s
Epoch 78/1000, LR 0.000267
Train loss: 0.3388;  Loss pred: 0.3388; Loss self: 0.0000; time: 0.44s
Val loss: 0.3613 score: 0.9184 time: 0.53s
Test loss: 0.3720 score: 0.8958 time: 0.41s
Epoch 79/1000, LR 0.000267
Train loss: 0.3243;  Loss pred: 0.3243; Loss self: 0.0000; time: 0.46s
Val loss: 0.3512 score: 0.9184 time: 0.62s
Test loss: 0.3644 score: 0.8958 time: 0.42s
Epoch 80/1000, LR 0.000267
Train loss: 0.3173;  Loss pred: 0.3173; Loss self: 0.0000; time: 0.44s
Val loss: 0.3413 score: 0.9184 time: 0.52s
Test loss: 0.3573 score: 0.8958 time: 0.41s
Epoch 81/1000, LR 0.000267
Train loss: 0.3065;  Loss pred: 0.3065; Loss self: 0.0000; time: 0.44s
Val loss: 0.3318 score: 0.9184 time: 0.62s
Test loss: 0.3504 score: 0.8958 time: 0.41s
Epoch 82/1000, LR 0.000267
Train loss: 0.2871;  Loss pred: 0.2871; Loss self: 0.0000; time: 0.49s
Val loss: 0.3226 score: 0.9184 time: 0.52s
Test loss: 0.3440 score: 0.8958 time: 0.51s
Epoch 83/1000, LR 0.000266
Train loss: 0.2828;  Loss pred: 0.2828; Loss self: 0.0000; time: 0.45s
Val loss: 0.3136 score: 0.9184 time: 0.53s
Test loss: 0.3378 score: 0.8958 time: 0.45s
Epoch 84/1000, LR 0.000266
Train loss: 0.2790;  Loss pred: 0.2790; Loss self: 0.0000; time: 0.45s
Val loss: 0.3050 score: 0.9184 time: 0.55s
Test loss: 0.3318 score: 0.8958 time: 0.53s
Epoch 85/1000, LR 0.000266
Train loss: 0.2582;  Loss pred: 0.2582; Loss self: 0.0000; time: 0.45s
Val loss: 0.2967 score: 0.9184 time: 0.56s
Test loss: 0.3264 score: 0.8958 time: 0.42s
Epoch 86/1000, LR 0.000266
Train loss: 0.2526;  Loss pred: 0.2526; Loss self: 0.0000; time: 0.45s
Val loss: 0.2885 score: 0.9388 time: 0.62s
Test loss: 0.3211 score: 0.8958 time: 0.41s
Epoch 87/1000, LR 0.000266
Train loss: 0.2568;  Loss pred: 0.2568; Loss self: 0.0000; time: 0.48s
Val loss: 0.2804 score: 0.9388 time: 0.52s
Test loss: 0.3161 score: 0.8958 time: 0.41s
Epoch 88/1000, LR 0.000266
Train loss: 0.2360;  Loss pred: 0.2360; Loss self: 0.0000; time: 0.44s
Val loss: 0.2724 score: 0.9388 time: 0.71s
Test loss: 0.3116 score: 0.8958 time: 0.46s
Epoch 89/1000, LR 0.000266
Train loss: 0.2433;  Loss pred: 0.2433; Loss self: 0.0000; time: 0.44s
Val loss: 0.2645 score: 0.9388 time: 0.55s
Test loss: 0.3072 score: 0.8958 time: 0.53s
Epoch 90/1000, LR 0.000266
Train loss: 0.2224;  Loss pred: 0.2224; Loss self: 0.0000; time: 0.47s
Val loss: 0.2566 score: 0.9388 time: 0.58s
Test loss: 0.3034 score: 0.8958 time: 0.43s
Epoch 91/1000, LR 0.000266
Train loss: 0.2139;  Loss pred: 0.2139; Loss self: 0.0000; time: 0.45s
Val loss: 0.2490 score: 0.9388 time: 0.53s
Test loss: 0.3001 score: 0.8958 time: 0.55s
Epoch 92/1000, LR 0.000266
Train loss: 0.2120;  Loss pred: 0.2120; Loss self: 0.0000; time: 0.46s
Val loss: 0.2415 score: 0.9388 time: 0.57s
Test loss: 0.2970 score: 0.8958 time: 0.43s
Epoch 93/1000, LR 0.000265
Train loss: 0.2048;  Loss pred: 0.2048; Loss self: 0.0000; time: 0.46s
Val loss: 0.2343 score: 0.9388 time: 0.64s
Test loss: 0.2931 score: 0.8958 time: 0.44s
Epoch 94/1000, LR 0.000265
Train loss: 0.1887;  Loss pred: 0.1887; Loss self: 0.0000; time: 0.47s
Val loss: 0.2271 score: 0.9388 time: 0.52s
Test loss: 0.2898 score: 0.8958 time: 0.41s
Epoch 95/1000, LR 0.000265
Train loss: 0.1848;  Loss pred: 0.1848; Loss self: 0.0000; time: 0.49s
Val loss: 0.2202 score: 0.9388 time: 0.61s
Test loss: 0.2870 score: 0.8958 time: 0.46s
Epoch 96/1000, LR 0.000265
Train loss: 0.1734;  Loss pred: 0.1734; Loss self: 0.0000; time: 0.46s
Val loss: 0.2135 score: 0.9388 time: 0.54s
Test loss: 0.2848 score: 0.8958 time: 0.43s
Epoch 97/1000, LR 0.000265
Train loss: 0.1715;  Loss pred: 0.1715; Loss self: 0.0000; time: 0.55s
Val loss: 0.2070 score: 0.9388 time: 0.57s
Test loss: 0.2823 score: 0.8958 time: 0.42s
Epoch 98/1000, LR 0.000265
Train loss: 0.1562;  Loss pred: 0.1562; Loss self: 0.0000; time: 0.46s
Val loss: 0.2007 score: 0.9388 time: 0.54s
Test loss: 0.2788 score: 0.8958 time: 0.51s
Epoch 99/1000, LR 0.000265
Train loss: 0.1491;  Loss pred: 0.1491; Loss self: 0.0000; time: 0.49s
Val loss: 0.1947 score: 0.9388 time: 0.54s
Test loss: 0.2752 score: 0.8958 time: 0.43s
Epoch 100/1000, LR 0.000265
Train loss: 0.1475;  Loss pred: 0.1475; Loss self: 0.0000; time: 0.45s
Val loss: 0.1890 score: 0.9388 time: 0.62s
Test loss: 0.2714 score: 0.8958 time: 0.44s
Epoch 101/1000, LR 0.000265
Train loss: 0.1474;  Loss pred: 0.1474; Loss self: 0.0000; time: 0.46s
Val loss: 0.1836 score: 0.9388 time: 0.54s
Test loss: 0.2693 score: 0.8958 time: 0.43s
Epoch 102/1000, LR 0.000264
Train loss: 0.1423;  Loss pred: 0.1423; Loss self: 0.0000; time: 0.46s
Val loss: 0.1787 score: 0.9388 time: 0.67s
Test loss: 0.2662 score: 0.8958 time: 0.42s
Epoch 103/1000, LR 0.000264
Train loss: 0.1273;  Loss pred: 0.1273; Loss self: 0.0000; time: 0.46s
Val loss: 0.1741 score: 0.9388 time: 0.57s
Test loss: 0.2631 score: 0.8958 time: 0.54s
Epoch 104/1000, LR 0.000264
Train loss: 0.1229;  Loss pred: 0.1229; Loss self: 0.0000; time: 0.46s
Val loss: 0.1698 score: 0.9388 time: 0.56s
Test loss: 0.2611 score: 0.8958 time: 0.42s
Epoch 105/1000, LR 0.000264
Train loss: 0.1194;  Loss pred: 0.1194; Loss self: 0.0000; time: 0.46s
Val loss: 0.1660 score: 0.9388 time: 0.56s
Test loss: 0.2592 score: 0.8958 time: 0.55s
Epoch 106/1000, LR 0.000264
Train loss: 0.1124;  Loss pred: 0.1124; Loss self: 0.0000; time: 0.54s
Val loss: 0.1625 score: 0.9388 time: 0.53s
Test loss: 0.2574 score: 0.8958 time: 0.42s
Epoch 107/1000, LR 0.000264
Train loss: 0.1165;  Loss pred: 0.1165; Loss self: 0.0000; time: 0.47s
Val loss: 0.1591 score: 0.9388 time: 0.65s
Test loss: 0.2571 score: 0.8958 time: 0.43s
Epoch 108/1000, LR 0.000264
Train loss: 0.1194;  Loss pred: 0.1194; Loss self: 0.0000; time: 0.49s
Val loss: 0.1564 score: 0.9388 time: 0.59s
Test loss: 0.2555 score: 0.8958 time: 0.44s
Epoch 109/1000, LR 0.000264
Train loss: 0.1063;  Loss pred: 0.1063; Loss self: 0.0000; time: 0.46s
Val loss: 0.1540 score: 0.9388 time: 0.63s
Test loss: 0.2543 score: 0.8958 time: 0.46s
Epoch 110/1000, LR 0.000263
Train loss: 0.1349;  Loss pred: 0.1349; Loss self: 0.0000; time: 0.45s
Val loss: 0.1512 score: 0.9388 time: 0.55s
Test loss: 0.2566 score: 0.8958 time: 0.51s
Epoch 111/1000, LR 0.000263
Train loss: 0.0945;  Loss pred: 0.0945; Loss self: 0.0000; time: 0.52s
Val loss: 0.1488 score: 0.9388 time: 0.53s
Test loss: 0.2583 score: 0.8958 time: 0.44s
Epoch 112/1000, LR 0.000263
Train loss: 0.1015;  Loss pred: 0.1015; Loss self: 0.0000; time: 0.55s
Val loss: 0.1464 score: 0.9388 time: 0.53s
Test loss: 0.2617 score: 0.8958 time: 0.52s
Epoch 113/1000, LR 0.000263
Train loss: 0.0985;  Loss pred: 0.0985; Loss self: 0.0000; time: 0.46s
Val loss: 0.1442 score: 0.9388 time: 0.55s
Test loss: 0.2665 score: 0.8958 time: 0.43s
Epoch 114/1000, LR 0.000263
Train loss: 0.0802;  Loss pred: 0.0802; Loss self: 0.0000; time: 0.47s
Val loss: 0.1421 score: 0.9388 time: 0.63s
Test loss: 0.2741 score: 0.8958 time: 0.42s
Epoch 115/1000, LR 0.000263
Train loss: 0.0947;  Loss pred: 0.0947; Loss self: 0.0000; time: 0.46s
Val loss: 0.1406 score: 0.9388 time: 0.54s
Test loss: 0.2823 score: 0.8958 time: 0.42s
Epoch 116/1000, LR 0.000263
Train loss: 0.0979;  Loss pred: 0.0979; Loss self: 0.0000; time: 0.46s
Val loss: 0.1399 score: 0.9388 time: 0.64s
Test loss: 0.2905 score: 0.8958 time: 0.42s
Epoch 117/1000, LR 0.000262
Train loss: 0.0836;  Loss pred: 0.0836; Loss self: 0.0000; time: 0.73s
Val loss: 0.1399 score: 0.9184 time: 0.71s
Test loss: 0.2997 score: 0.8958 time: 0.53s
     INFO: Early stopping counter 1 of 20
Epoch 118/1000, LR 0.000262
Train loss: 0.0692;  Loss pred: 0.0692; Loss self: 0.0000; time: 0.67s
Val loss: 0.1410 score: 0.9184 time: 0.67s
Test loss: 0.3101 score: 0.8958 time: 0.45s
     INFO: Early stopping counter 2 of 20
Epoch 119/1000, LR 0.000262
Train loss: 0.0884;  Loss pred: 0.0884; Loss self: 0.0000; time: 0.46s
Val loss: 0.1428 score: 0.9184 time: 0.53s
Test loss: 0.3196 score: 0.8958 time: 0.51s
     INFO: Early stopping counter 3 of 20
Epoch 120/1000, LR 0.000262
Train loss: 0.1318;  Loss pred: 0.1318; Loss self: 0.0000; time: 0.45s
Val loss: 0.1424 score: 0.9184 time: 0.53s
Test loss: 0.3219 score: 0.8958 time: 0.44s
     INFO: Early stopping counter 4 of 20
Epoch 121/1000, LR 0.000262
Train loss: 0.0567;  Loss pred: 0.0567; Loss self: 0.0000; time: 0.45s
Val loss: 0.1431 score: 0.9184 time: 0.67s
Test loss: 0.3267 score: 0.8958 time: 0.42s
     INFO: Early stopping counter 5 of 20
Epoch 122/1000, LR 0.000262
Train loss: 0.0683;  Loss pred: 0.0683; Loss self: 0.0000; time: 0.45s
Val loss: 0.1446 score: 0.9184 time: 0.66s
Test loss: 0.3332 score: 0.8958 time: 0.53s
     INFO: Early stopping counter 6 of 20
Epoch 123/1000, LR 0.000262
Train loss: 0.0596;  Loss pred: 0.0596; Loss self: 0.0000; time: 0.49s
Val loss: 0.1443 score: 0.9184 time: 0.63s
Test loss: 0.3359 score: 0.8958 time: 0.43s
     INFO: Early stopping counter 7 of 20
Epoch 124/1000, LR 0.000261
Train loss: 0.0544;  Loss pred: 0.0544; Loss self: 0.0000; time: 0.47s
Val loss: 0.1454 score: 0.9184 time: 0.53s
Test loss: 0.3412 score: 0.8958 time: 0.55s
     INFO: Early stopping counter 8 of 20
Epoch 125/1000, LR 0.000261
Train loss: 0.0498;  Loss pred: 0.0498; Loss self: 0.0000; time: 0.45s
Val loss: 0.1477 score: 0.9184 time: 0.54s
Test loss: 0.3485 score: 0.8958 time: 0.42s
     INFO: Early stopping counter 9 of 20
Epoch 126/1000, LR 0.000261
Train loss: 0.0689;  Loss pred: 0.0689; Loss self: 0.0000; time: 0.47s
Val loss: 0.1489 score: 0.9184 time: 0.57s
Test loss: 0.3533 score: 0.8958 time: 0.61s
     INFO: Early stopping counter 10 of 20
Epoch 127/1000, LR 0.000261
Train loss: 0.0875;  Loss pred: 0.0875; Loss self: 0.0000; time: 0.47s
Val loss: 0.1497 score: 0.9184 time: 0.54s
Test loss: 0.3574 score: 0.8958 time: 0.42s
     INFO: Early stopping counter 11 of 20
Epoch 128/1000, LR 0.000261
Train loss: 0.0609;  Loss pred: 0.0609; Loss self: 0.0000; time: 0.46s
Val loss: 0.1494 score: 0.9184 time: 0.77s
Test loss: 0.3595 score: 0.8958 time: 0.42s
     INFO: Early stopping counter 12 of 20
Epoch 129/1000, LR 0.000261
Train loss: 0.0642;  Loss pred: 0.0642; Loss self: 0.0000; time: 0.47s
Val loss: 0.1491 score: 0.9184 time: 0.55s
Test loss: 0.3616 score: 0.8958 time: 0.43s
     INFO: Early stopping counter 13 of 20
Epoch 130/1000, LR 0.000260
Train loss: 0.0451;  Loss pred: 0.0451; Loss self: 0.0000; time: 0.56s
Val loss: 0.1488 score: 0.9184 time: 0.63s
Test loss: 0.3636 score: 0.8958 time: 0.42s
     INFO: Early stopping counter 14 of 20
Epoch 131/1000, LR 0.000260
Train loss: 0.0418;  Loss pred: 0.0418; Loss self: 0.0000; time: 0.47s
Val loss: 0.1476 score: 0.9184 time: 0.58s
Test loss: 0.3643 score: 0.8958 time: 0.54s
     INFO: Early stopping counter 15 of 20
Epoch 132/1000, LR 0.000260
Train loss: 0.0670;  Loss pred: 0.0670; Loss self: 0.0000; time: 0.46s
Val loss: 0.1479 score: 0.9184 time: 0.53s
Test loss: 0.3671 score: 0.8958 time: 0.43s
     INFO: Early stopping counter 16 of 20
Epoch 133/1000, LR 0.000260
Train loss: 0.0412;  Loss pred: 0.0412; Loss self: 0.0000; time: 0.46s
Val loss: 0.1475 score: 0.9184 time: 0.74s
Test loss: 0.3689 score: 0.8958 time: 0.51s
     INFO: Early stopping counter 17 of 20
Epoch 134/1000, LR 0.000260
Train loss: 0.0856;  Loss pred: 0.0856; Loss self: 0.0000; time: 0.46s
Val loss: 0.1433 score: 0.9184 time: 0.54s
Test loss: 0.3631 score: 0.8958 time: 0.46s
     INFO: Early stopping counter 18 of 20
Epoch 135/1000, LR 0.000260
Train loss: 0.0460;  Loss pred: 0.0460; Loss self: 0.0000; time: 0.46s
Val loss: 0.1393 score: 0.9184 time: 0.62s
Test loss: 0.3557 score: 0.8958 time: 0.42s
Epoch 136/1000, LR 0.000260
Train loss: 0.0545;  Loss pred: 0.0545; Loss self: 0.0000; time: 0.46s
Val loss: 0.1363 score: 0.9184 time: 0.56s
Test loss: 0.3482 score: 0.8958 time: 0.56s
Epoch 137/1000, LR 0.000259
Train loss: 0.0318;  Loss pred: 0.0318; Loss self: 0.0000; time: 0.44s
Val loss: 0.1349 score: 0.9184 time: 0.62s
Test loss: 0.3439 score: 0.8958 time: 0.47s
Epoch 138/1000, LR 0.000259
Train loss: 0.0419;  Loss pred: 0.0419; Loss self: 0.0000; time: 0.47s
Val loss: 0.1344 score: 0.9184 time: 0.59s
Test loss: 0.3434 score: 0.8958 time: 0.52s
Epoch 139/1000, LR 0.000259
Train loss: 0.0509;  Loss pred: 0.0509; Loss self: 0.0000; time: 0.58s
Val loss: 0.1347 score: 0.9184 time: 0.57s
Test loss: 0.3476 score: 0.8958 time: 0.43s
     INFO: Early stopping counter 1 of 20
Epoch 140/1000, LR 0.000259
Train loss: 0.0199;  Loss pred: 0.0199; Loss self: 0.0000; time: 0.49s
Val loss: 0.1349 score: 0.9184 time: 0.75s
Test loss: 0.3508 score: 0.8958 time: 0.64s
     INFO: Early stopping counter 2 of 20
Epoch 141/1000, LR 0.000259
Train loss: 0.0273;  Loss pred: 0.0273; Loss self: 0.0000; time: 0.54s
Val loss: 0.1356 score: 0.9184 time: 0.54s
Test loss: 0.3567 score: 0.8958 time: 0.42s
     INFO: Early stopping counter 3 of 20
Epoch 142/1000, LR 0.000259
Train loss: 0.0357;  Loss pred: 0.0357; Loss self: 0.0000; time: 0.59s
Val loss: 0.1356 score: 0.9184 time: 0.66s
Test loss: 0.3594 score: 0.8958 time: 0.44s
     INFO: Early stopping counter 4 of 20
Epoch 143/1000, LR 0.000258
Train loss: 0.0464;  Loss pred: 0.0464; Loss self: 0.0000; time: 0.48s
Val loss: 0.1365 score: 0.9184 time: 0.59s
Test loss: 0.3661 score: 0.8958 time: 0.46s
     INFO: Early stopping counter 5 of 20
Epoch 144/1000, LR 0.000258
Train loss: 0.0256;  Loss pred: 0.0256; Loss self: 0.0000; time: 0.47s
Val loss: 0.1380 score: 0.9184 time: 0.67s
Test loss: 0.3743 score: 0.8958 time: 0.45s
     INFO: Early stopping counter 6 of 20
Epoch 145/1000, LR 0.000258
Train loss: 0.0284;  Loss pred: 0.0284; Loss self: 0.0000; time: 0.47s
Val loss: 0.1392 score: 0.9184 time: 0.60s
Test loss: 0.3804 score: 0.8958 time: 0.52s
     INFO: Early stopping counter 7 of 20
Epoch 146/1000, LR 0.000258
Train loss: 0.0265;  Loss pred: 0.0265; Loss self: 0.0000; time: 0.46s
Val loss: 0.1413 score: 0.9184 time: 0.57s
Test loss: 0.3884 score: 0.8958 time: 0.43s
     INFO: Early stopping counter 8 of 20
Epoch 147/1000, LR 0.000258
Train loss: 0.0279;  Loss pred: 0.0279; Loss self: 0.0000; time: 0.50s
Val loss: 0.1421 score: 0.9184 time: 0.55s
Test loss: 0.3926 score: 0.8958 time: 0.60s
     INFO: Early stopping counter 9 of 20
Epoch 148/1000, LR 0.000257
Train loss: 0.0395;  Loss pred: 0.0395; Loss self: 0.0000; time: 0.48s
Val loss: 0.1436 score: 0.9184 time: 0.60s
Test loss: 0.3980 score: 0.8958 time: 0.62s
     INFO: Early stopping counter 10 of 20
Epoch 149/1000, LR 0.000257
Train loss: 0.0169;  Loss pred: 0.0169; Loss self: 0.0000; time: 0.47s
Val loss: 0.1440 score: 0.9184 time: 0.63s
Test loss: 0.4008 score: 0.8958 time: 0.44s
     INFO: Early stopping counter 11 of 20
Epoch 150/1000, LR 0.000257
Train loss: 0.0146;  Loss pred: 0.0146; Loss self: 0.0000; time: 0.52s
Val loss: 0.1436 score: 0.9184 time: 0.56s
Test loss: 0.4019 score: 0.8958 time: 0.42s
     INFO: Early stopping counter 12 of 20
Epoch 151/1000, LR 0.000257
Train loss: 0.0208;  Loss pred: 0.0208; Loss self: 0.0000; time: 0.45s
Val loss: 0.1437 score: 0.9184 time: 0.63s
Test loss: 0.4036 score: 0.8958 time: 0.43s
     INFO: Early stopping counter 13 of 20
Epoch 152/1000, LR 0.000257
Train loss: 0.0204;  Loss pred: 0.0204; Loss self: 0.0000; time: 0.45s
Val loss: 0.1427 score: 0.9184 time: 0.54s
Test loss: 0.4026 score: 0.8958 time: 0.51s
     INFO: Early stopping counter 14 of 20
Epoch 153/1000, LR 0.000257
Train loss: 0.0118;  Loss pred: 0.0118; Loss self: 0.0000; time: 0.45s
Val loss: 0.1417 score: 0.9184 time: 0.53s
Test loss: 0.4008 score: 0.8958 time: 0.42s
     INFO: Early stopping counter 15 of 20
Epoch 154/1000, LR 0.000256
Train loss: 0.0102;  Loss pred: 0.0102; Loss self: 0.0000; time: 0.47s
Val loss: 0.1410 score: 0.9184 time: 0.54s
Test loss: 0.3987 score: 0.8958 time: 0.52s
     INFO: Early stopping counter 16 of 20
Epoch 155/1000, LR 0.000256
Train loss: 0.0114;  Loss pred: 0.0114; Loss self: 0.0000; time: 0.47s
Val loss: 0.1404 score: 0.9184 time: 0.53s
Test loss: 0.3966 score: 0.8958 time: 0.43s
     INFO: Early stopping counter 17 of 20
Epoch 156/1000, LR 0.000256
Train loss: 0.0133;  Loss pred: 0.0133; Loss self: 0.0000; time: 0.45s
Val loss: 0.1403 score: 0.9184 time: 0.65s
Test loss: 0.3955 score: 0.8958 time: 0.42s
     INFO: Early stopping counter 18 of 20
Epoch 157/1000, LR 0.000256
Train loss: 0.0145;  Loss pred: 0.0145; Loss self: 0.0000; time: 0.46s
Val loss: 0.1404 score: 0.9184 time: 0.54s
Test loss: 0.3929 score: 0.8958 time: 0.41s
     INFO: Early stopping counter 19 of 20
Epoch 158/1000, LR 0.000256
Train loss: 0.0125;  Loss pred: 0.0125; Loss self: 0.0000; time: 0.45s
Val loss: 0.1408 score: 0.9388 time: 0.67s
Test loss: 0.3916 score: 0.8958 time: 0.43s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 137,   Train_Loss: 0.0419,   Val_Loss: 0.1344,   Val_Precision: 0.9565,   Val_Recall: 0.8800,   Val_accuracy: 0.9167,   Val_Score: 0.9184,   Val_Loss: 0.1344,   Test_Precision: 0.8800,   Test_Recall: 0.9167,   Test_accuracy: 0.8980,   Test_Score: 0.8958,   Test_loss: 0.3434


[0.47569059603847563, 0.45132147101685405, 0.44563297485001385, 0.5580142331309617, 0.5506600718945265, 0.4633305750321597, 0.5788522260263562, 0.44643664499744773, 0.4611370291095227, 0.45920622791163623, 0.4509186039213091, 0.4616614580154419, 0.5626949791330844, 0.6981428579892963, 0.4675599308684468, 0.5515576610341668, 0.4389668151270598, 0.47475144313648343, 0.5560952147934586, 2.7768845919054, 0.4467680021189153, 0.44475991697981954, 0.48681075079366565, 0.5424957610666752, 0.4337174950633198, 0.5407437509857118, 0.44338470813818276, 0.5634947181679308, 0.44345019292086363, 0.5320004390086979, 0.46075096097774804, 0.44417128805071115, 0.5427314669359475, 0.44638587604276836, 0.5443193190731108, 0.4632013849914074, 0.44247378781437874, 0.45457864995114505, 0.46007012692280114, 0.45130646182224154, 0.4651479651220143, 0.5706384810619056, 0.45046776602976024, 0.5618159910663962, 0.44742191303521395, 0.4812281639315188, 0.4485834420192987, 0.48323942394927144, 0.4637891938909888, 0.4905513960402459, 0.5411260868422687, 0.5012516570277512, 0.5992905881721526, 0.4722739290446043, 0.4516412790399045, 0.5635383401531726, 0.45252237701788545, 0.4731232519261539, 0.4520899001508951, 0.5200490329880267, 0.5552324298769236, 0.44297137996181846, 0.546603481983766, 0.44949730997905135, 0.5797838601283729, 0.5038830840494484, 0.44426238909363747, 0.4655644129961729, 0.44806572212837636, 0.5607313578948379, 0.4501445088535547, 0.5615581821184605, 0.47095722099766135, 0.4605841168668121, 0.46516172704286873, 0.48626545583829284, 0.44313494791276753, 0.4646034771576524, 0.5522780469618738, 0.46834656689316034, 0.5353503781370819, 0.45680899010039866, 0.44849530002102256, 0.45338086504489183, 0.46530835307203233, 0.48168550501577556, 0.45204843604005873, 0.5558993201702833, 0.4622029459569603, 0.5743302810005844, 0.4636002550832927, 0.44945880491286516, 0.5904106369707733, 0.44257127889432013, 0.48433129698969424, 0.4620950990356505, 0.4691790679935366, 0.5617187810130417, 0.449422013014555, 0.5833343579433858, 0.44469594210386276, 0.5698766501154751, 0.46284815203398466, 0.47052720887586474, 0.47525181504897773, 0.6149143981747329, 0.5664069140329957, 0.5068021349143237, 0.5629306701011956, 0.8275662770029157, 0.6146490080282092, 1.755091428058222, 0.4871865180321038, 0.5181932940613478, 0.4836569200269878, 0.4829591060988605, 0.5935941750649363, 0.4767094780690968, 0.608224819181487, 0.4788183409254998, 0.5335464931558818, 0.5161989917978644, 0.5085954258684069, 0.6558945630677044, 0.5106567488983274, 0.4812549741473049, 0.4836956411600113, 0.5348749540280551, 0.6176231419667602, 0.48940634401515126, 0.48621256719343364, 0.49607345182448626, 0.6075392921920866, 0.494806915987283, 0.5142126611899585, 0.588398122927174, 0.497029447928071, 0.47673650295473635, 0.5822372629772872, 0.5055563619825989, 0.5756781990639865, 0.4770364190917462, 0.6214373130351305, 0.5170454408507794, 0.48685222398489714, 0.5699482760392129, 0.4886149459052831, 0.6167727350257337, 0.4869499960914254, 0.4747337568551302, 0.4823769030626863, 0.4855148959904909, 0.5996531471610069, 0.4888298308942467, 0.5213916150387377, 0.4893414000980556, 0.49327093409374356, 0.49515158007852733, 0.5500980010256171, 0.5959922890178859, 0.4782794669736177, 0.6077750641852617, 0.512144047068432, 0.4940414070151746, 0.4846542151644826, 0.4896771148778498, 0.6240508831106126, 0.5098212799057364, 0.4996143670286983, 0.5080307889729738, 0.48945129895582795, 0.515863869106397, 0.6173210649285465, 0.6745133169461042, 0.5462902658618987, 0.7878484840039164, 0.7277248441241682, 0.6134028770029545, 0.6173826500307769, 0.6041534210089594, 0.598986288998276, 0.5114207568112761, 0.4955671050120145, 0.48638336011208594, 0.4891646499745548, 0.489011517027393, 0.5193025819025934, 0.5769224690739065, 0.4984747169073671, 0.5798924448899925, 0.5304183168336749, 0.5126021690666676, 0.4887057109735906, 0.5064233299344778, 0.5884908630978316, 0.5306974861305207, 0.4904935860540718, 0.4782085558399558, 0.4981984740588814, 3.5769919550511986, 0.5186644229106605, 0.5803730820771307, 0.4844402091111988, 0.5781402210704982, 0.48104586102999747, 0.4988194510806352, 0.5538830880541354, 0.4847662921529263, 0.6191692280117422, 0.4799487479031086, 0.5019158809445798, 0.49082080903463066, 0.4905041470192373, 0.49390440597198904, 0.49187476793304086, 0.5851622081827372, 0.5034956210292876, 0.5832608800847083, 0.42483229702338576, 0.42601206502877176, 0.5047616318333894, 0.41235147207044065, 0.4183846639934927, 0.42872919398359954, 0.4159826170653105, 0.4319482399150729, 0.43071106588467956, 0.4235834691207856, 0.4341642651706934, 0.4626925529446453, 0.4341854408849031, 0.6032504721079022, 0.4313074038363993, 0.42031879583373666, 0.44059321098029613, 0.4319886800367385, 0.5489264868665487, 0.4350884568411857, 0.5338171902112663, 0.536036633187905, 0.4294586929026991, 0.45533452508971095, 0.4236512631177902, 0.6609708908945322, 0.43282745592296124, 0.5245050229132175, 0.43086685612797737, 0.4290532588493079, 0.4549747798591852, 0.43743471591733396, 0.4253436860162765, 0.43995658308267593, 0.535280360141769, 0.42450068588368595, 0.4273258000612259, 0.4183525932021439, 0.4254766460508108, 0.526654209010303, 0.4292527660727501, 0.5112320729531348, 0.44411605410277843, 0.4294930419418961, 0.4166762330569327, 0.4532533260062337, 0.5285036249551922, 0.46142360498197377, 0.5144903599284589, 0.4360517719760537, 0.42675460409373045, 0.41911962698213756, 0.5560405731666833, 0.4299859148450196, 0.5348023218102753, 0.6037102709524333, 0.42401548800989985, 0.430797932902351, 0.4247739831916988, 0.42906127497553825, 0.541454321006313, 0.42111988994292915, 0.553989646025002, 0.4323196359910071, 0.45421376591548324, 0.5341176569927484, 0.460423317970708, 0.5952205020003021, 0.4296032798010856, 0.5218037469312549, 0.42226839205250144, 0.4464613860473037, 0.535496816970408, 0.5271308310329914, 3.5314762939233333, 0.4241226220037788, 0.5087699231225997, 0.4190650878008455, 0.4269919239450246, 0.4157459700945765, 0.412075795000419, 0.5181360200513154, 0.45861331885680556, 0.530756819061935, 0.42700124392285943, 0.419501697877422, 0.40981803089380264, 0.46225454402156174, 0.5305617980193347, 0.43657591892406344, 0.5507089111488312, 0.4337274811696261, 0.44924809504300356, 0.41961995302699506, 0.4646862498484552, 0.43743021809495986, 0.425247916020453, 0.5099909619893879, 0.4313611469697207, 0.4496852101292461, 0.43262676103040576, 0.4263641450088471, 0.5492615681141615, 0.4267888111062348, 0.555690519977361, 0.4228099249303341, 0.4365520339924842, 0.4399549381341785, 0.4614137629978359, 0.5144730499014258, 0.44314756407402456, 0.5284726659301668, 0.4345282029826194, 0.42508095293305814, 0.42211409797891974, 0.42858549300581217, 0.5328239640221, 0.45591020211577415, 0.5164193231612444, 0.44187959493137896, 0.4211882380768657, 0.5331705780699849, 0.43202323094010353, 0.5512677109800279, 0.42189588211476803, 0.6119864040520042, 0.42768023489043117, 0.42788713006302714, 0.4354761920403689, 0.42499382817186415, 0.541229858994484, 0.4370854429434985, 0.5177405518479645, 0.46416557393968105, 0.4229241970460862, 0.5680139870382845, 0.47380707901902497, 0.5250909191090614, 0.43078245501965284, 0.641810430213809, 0.42498291679657996, 0.4478496869560331, 0.46286665508523583, 0.4512239871546626, 0.5279113149736077, 0.433694452047348, 0.6086194280069321, 0.6199815531726927, 0.44343213294632733, 0.4292891218792647, 0.4302559979259968, 0.5139565749559551, 0.4207291090860963, 0.5218917629681528, 0.43479688791558146, 0.4270886848680675, 0.4191247869748622, 0.4305434220004827]
[0.009707971347723993, 0.009210642265650083, 0.00909455050714314, 0.011388045574101259, 0.011237960650908704, 0.009455726021064483, 0.01181331073523176, 0.009110951938723423, 0.009410959777745361, 0.009371555671666046, 0.009202420488189983, 0.009421662408478406, 0.011483571002716008, 0.014247813428352987, 0.009542039405478507, 0.01125627879661565, 0.008958506431164486, 0.00968880496196905, 0.011348881934560379, 0.05667111412051837, 0.009117714328957456, 0.009076732999588154, 0.00993491328150338, 0.011071342062585207, 0.008851377450271833, 0.011035586754810445, 0.009048667513024139, 0.011499892207508793, 0.009050003937160482, 0.010857151816504039, 0.009403080836280572, 0.009064720164300228, 0.011076152386447909, 0.009109915837607518, 0.011108557532104303, 0.009453089489620559, 0.00903007730233426, 0.00927711530512541, 0.009389186263730635, 0.00921033595555595, 0.009492815614734985, 0.011645683286977666, 0.009193219714893066, 0.01146563247074278, 0.009131059449698244, 0.009820982937377935, 0.00915476412284283, 0.009862029060189213, 0.009465085589612017, 0.010011252980413181, 0.01104338952739324, 0.010229625653627575, 0.012230420166778624, 0.009638243449889883, 0.009217168959998051, 0.011500782452105564, 0.009235150551385417, 0.009655576569921508, 0.00922632449287541, 0.010613245571184218, 0.01133127407912089, 0.009040232244118745, 0.01115517310170951, 0.009173414489368394, 0.011832323676089242, 0.01028332824590711, 0.009066579369257907, 0.009501314550942304, 0.009144198410783191, 0.01144349709989465, 0.009186622629664382, 0.011460371063642052, 0.00961137185709513, 0.009399675854424737, 0.009493096470262627, 0.009923784813026384, 0.009043570365566684, 0.009481703615462293, 0.011270980550242322, 0.009558093201901232, 0.010925517921164935, 0.009322632451028543, 0.009152965306551481, 0.009252670715201874, 0.009496088838204741, 0.00983031642889338, 0.009225478286531811, 0.011344884085107823, 0.009432713182795108, 0.011721026142869068, 0.009461229695577403, 0.009172628671691125, 0.012049196672872926, 0.009032066916210614, 0.009884312183463148, 0.009430512225217357, 0.009575083020276256, 0.011463648592102893, 0.009171877816623571, 0.01190478281517114, 0.009075427389874751, 0.011630135716642348, 0.009445880653754788, 0.009602596099507444, 0.009699016633652607, 0.012549273432137407, 0.011559324776183585, 0.010342900712537219, 0.01148838102247338, 0.016889107693937058, 0.012543857306698146, 0.03581819240935147, 0.00994258200065518, 0.010575373348190772, 0.009870549388305873, 0.009856308287731846, 0.012114166838059924, 0.009728764858552995, 0.012412751411867082, 0.009771802876030609, 0.01088870394195677, 0.010534673301997234, 0.010379498487110344, 0.013385603327912336, 0.010421566304047497, 0.009821530084638876, 0.009871339615510434, 0.010915815388327655, 0.012604553917688983, 0.009987884571737781, 0.009922705452927217, 0.010123947996418086, 0.012398761065144624, 0.010098100326271082, 0.010494135942652213, 0.012008124957697429, 0.01014345812098104, 0.009729316386831353, 0.011882393121985453, 0.01031747677515508, 0.011748534674775235, 0.009735437124321351, 0.012682394143574091, 0.010551947772464886, 0.009935759673161166, 0.01163159747018802, 0.009971733589903737, 0.012587198673994566, 0.009937755022273988, 0.009688444017451637, 0.009844426593116047, 0.00990846726511206, 0.012237819329816468, 0.00997611899784177, 0.010640645204872196, 0.009986559185674605, 0.010066753757015174, 0.010105134287316884, 0.011226489816849329, 0.012163107939140529, 0.009760805448441177, 0.01240357273847473, 0.010451919327927182, 0.010082477694187237, 0.009890902350295563, 0.009993410507711221, 0.012735732308379849, 0.010404515916443601, 0.010196211572014252, 0.010367975285162732, 0.009988802019506693, 0.010527834063395858, 0.012598389080174419, 0.013765577896859268, 0.011148780935957116, 0.016078540489875844, 0.014851527431105472, 0.012518426061284785, 0.012599645918995445, 0.01232966165324407, 0.012224209979556653, 0.010437158302270941, 0.010113614388000297, 0.009926191022695631, 0.009982952040297036, 0.009979826878110061, 0.01059801187556313, 0.011773927940283807, 0.010172953406272799, 0.0118345396916325, 0.010824863608850509, 0.010461268756462604, 0.009973585938236542, 0.010335169998662812, 0.012010017614241461, 0.010830560941439199, 0.010010073184776975, 0.009759358282448078, 0.01016731579712003, 0.0729998358173714, 0.010584988222666541, 0.011844348613818993, 0.009886534879820384, 0.011798780021846903, 0.009817262469999949, 0.010179988797563983, 0.011303736490900723, 0.009893189635774007, 0.012636106694117188, 0.00979487240618589, 0.010243181243766936, 0.010016751204788382, 0.010010288714678312, 0.010079681754530388, 0.010038260570062059, 0.011942085881280352, 0.0102754208373324, 0.011903283267034864, 0.00885067285465387, 0.008875251354766078, 0.01051586732986228, 0.00859065566813418, 0.008716347166531099, 0.008931858207991658, 0.008666304522193968, 0.008998921664897352, 0.008973147205930824, 0.008824655606683033, 0.00904508885772278, 0.009639428186346777, 0.00904553001843548, 0.012567718168914629, 0.00898557091325832, 0.008756641579869514, 0.009179025228756169, 0.008999764167432053, 0.01143596847638643, 0.009064342850858035, 0.011121191462734714, 0.011167429858081354, 0.008947056102139564, 0.009486135939368978, 0.00882606798162063, 0.013770226893636087, 0.009017238665061692, 0.010927187977358699, 0.008976392835999528, 0.00893860955936058, 0.009478641247066358, 0.00911322324827779, 0.00886132679200576, 0.009165762147555748, 0.011151674169620188, 0.008843764289243458, 0.008902620834608873, 0.008715679025044665, 0.008864096792725226, 0.010971962687714646, 0.00894276595984896, 0.01065066818652364, 0.009252417793807885, 0.008947771707122834, 0.008680754855352765, 0.00944277762512987, 0.011010492186566504, 0.009612991770457787, 0.010718549165176228, 0.009084411916167786, 0.008890720918619385, 0.0087316588954612, 0.011584178607639236, 0.008958039892604575, 0.01114171503771407, 0.012577297311509028, 0.008833656000206247, 0.008974956935465647, 0.008849457983160391, 0.00893877656199038, 0.011280298354298187, 0.008773331040477691, 0.011541450958854208, 0.009006659083145982, 0.0094627867899059, 0.011127451187348925, 0.009592152457723083, 0.012400427125006294, 0.008950068329189284, 0.010870911394401142, 0.008797258167760447, 0.009301278875985494, 0.0111561836868835, 0.010981892313187322, 0.07357242279006944, 0.008835887958412059, 0.010599373398387494, 0.008730522662517615, 0.008895665082188012, 0.008661374376970343, 0.008584912395842062, 0.010794500417735739, 0.009554444142850116, 0.011057433730456978, 0.008895859248392904, 0.008739618705779625, 0.008537875643620888, 0.009630303000449203, 0.011053370792069472, 0.009095331644251322, 0.01147310231560065, 0.009035989191033877, 0.009359335313395908, 0.008742082354729064, 0.009680963538509483, 0.009113129543644996, 0.008859331583759436, 0.010624811708112247, 0.008986690561869182, 0.009368441877692627, 0.009013057521466786, 0.008882586354350982, 0.011442949335711697, 0.008891433564713225, 0.011576885832861686, 0.008808540102715293, 0.009094834041510088, 0.009165727877795385, 0.00961278672912158, 0.010718188539613038, 0.009232240918208845, 0.011009847206878476, 0.009052670895471238, 0.008855853186105378, 0.008794043707894161, 0.008928864437621087, 0.011100499250460416, 0.009498129210745295, 0.010758735899192592, 0.009205824894403728, 0.008774754959934702, 0.011107720376458019, 0.009000483977918824, 0.011484743978750581, 0.008789497544057667, 0.01274971675108342, 0.00891000489355065, 0.008914315209646398, 0.009072420667507686, 0.008854038086913837, 0.011275622062385082, 0.009105946727989552, 0.010786261496832594, 0.009670116123743355, 0.008810920771793462, 0.011833624729964262, 0.009870980812896354, 0.010939394148105444, 0.008974634479576101, 0.013371050629454354, 0.008853810766595416, 0.009330201811584024, 0.009643055314275747, 0.009400499732388804, 0.010998152395283492, 0.00903530108431975, 0.012679571416811086, 0.01291628235776443, 0.009238169436381819, 0.008943523372484682, 0.008963666623458266, 0.010707428644915732, 0.008765189772627005, 0.010872745061836516, 0.009058268498241281, 0.008897680934751406, 0.008731766395309629, 0.008969654625010056]
[103.00813261407568, 108.57006180007366, 109.95595650544458, 87.81138023140724, 88.9841165193206, 105.75602526683873, 84.65027479702383, 109.75801504887694, 106.25908766125615, 106.70586987210665, 108.66706224556462, 106.13838159814723, 87.0809262870833, 70.18620822266043, 104.79939953149385, 88.83930631681434, 111.62575008276389, 103.2119032146118, 88.11440684343826, 17.645673911992837, 109.67661015919795, 110.17179860257802, 100.65513121908972, 90.32328640440322, 112.976766115571, 90.61593390710259, 110.51350915045302, 86.95733681286642, 110.4971894977715, 92.10518715229647, 106.34812328121536, 110.31780152886797, 90.28405940166893, 109.77049819404523, 90.02068874468613, 105.78552134706803, 110.74102319605853, 107.79212795248122, 106.5055023844704, 108.5736725376201, 105.34282351884883, 85.86872709463182, 108.77581859378326, 87.217168573276, 109.51631686430946, 101.82280189023382, 109.23274336526214, 101.39901169392965, 105.65144821273518, 99.88759668310053, 90.55190867980251, 97.75528781401549, 81.76333980056471, 103.7533452230266, 108.49318313898105, 86.95060567960921, 108.28193806217749, 103.56709335361101, 108.38552240084363, 94.22188465280378, 88.25132928719898, 110.61662720563011, 89.64450760936663, 109.0106634949242, 84.51425327560975, 97.24478068644869, 110.29517961213736, 105.24859424855309, 109.35895691204179, 87.38587437656676, 108.85393254000827, 87.25720960052446, 104.04342011403902, 106.38664731499938, 105.3397069262412, 100.76800523600201, 110.57579690068995, 105.46627911562743, 88.7234252195121, 104.62337820697195, 91.52884167283254, 107.26583990658908, 109.25421068560405, 108.07690349955159, 105.30651271677155, 101.72612522022064, 108.39546405522309, 88.1454576792616, 106.01403653658843, 85.31676218539859, 105.69450612402363, 109.02000242157774, 82.99308469678809, 110.71662879348418, 101.1704184812212, 106.03877881903193, 104.43773676764928, 87.23226222137362, 109.02892733563785, 83.9998524564116, 110.18764814488819, 85.98351939857695, 105.86625394239917, 104.13850479989406, 103.10323590231894, 79.68588822354465, 86.51024340628994, 96.6846755850458, 87.04446675678815, 59.20975922007919, 79.72029460714782, 27.918773470514903, 100.57749585913434, 94.55930935725114, 101.31148334911826, 101.45786544082642, 82.54797984606174, 102.78797098491438, 80.5623158652771, 102.33526122931869, 91.83829456017826, 94.92463328790778, 96.34376855893741, 74.70712940631891, 95.95486617128013, 101.81712944748045, 101.30337309322644, 91.61019717036493, 79.33640543967364, 100.1213012442745, 100.77896645667317, 98.77569505037026, 80.65321968427945, 99.02852691990131, 95.2913136884014, 83.27694819322993, 98.58570795807489, 102.78214421657637, 84.15813125638348, 96.92292231837558, 85.11699779437654, 102.71752436280144, 78.84946554091125, 94.76923327932703, 100.64655677021217, 85.97271377065935, 100.28346535576203, 79.44579456476066, 100.62634848199114, 103.21574839042432, 101.58031963987361, 100.9237829872056, 81.7139044996015, 100.23938168904559, 93.97926354523263, 100.13458904188619, 99.3368889452704, 98.95959534700256, 89.07503737269198, 82.21582879997545, 102.45056161422644, 80.62193217105045, 95.67620727114043, 99.1819699811011, 101.10301007775269, 100.06593837292779, 78.51923829633421, 96.11211208967164, 98.07564240278421, 96.45084719974855, 100.11210534027443, 94.98629955395023, 79.37522754981904, 72.64497048308871, 89.69590538592377, 62.19469986281832, 67.33314163400937, 79.88224678601237, 79.36730971879, 81.10522641445631, 81.80487750720623, 95.81151986383284, 98.87661934060786, 100.74357804655995, 100.17077072627563, 100.20213899636062, 94.35732019755494, 84.9334228196317, 98.29987026023187, 84.49842799606651, 92.37991684092822, 95.5906996827928, 100.2648401680903, 96.75699578520549, 83.26382459374592, 92.3313211021106, 99.899369519173, 102.46575349103341, 98.35437591928223, 13.698660946330993, 94.47341640481133, 84.4284504454119, 101.14767329058043, 84.75452531095385, 101.86138987888395, 98.23193520991836, 88.4663226894027, 101.07963526585768, 79.13829981077603, 102.09423446582687, 97.62592071760018, 99.83276808572052, 99.89721860206464, 99.20948144523933, 99.61885259109366, 83.73746512470957, 97.31961501438707, 84.01043456383294, 112.98576011361429, 112.6728652550209, 95.09439104088588, 116.40555024331358, 114.72695854058972, 111.95878581069097, 115.38943703617274, 111.12442548541806, 111.4436191728859, 113.31886983132648, 110.55723340364875, 103.74059339083966, 110.55184140254066, 79.56893897202676, 111.28953403778583, 114.19903291450024, 108.94403001172576, 111.11402270058983, 87.44340298461393, 110.32239363114333, 89.9184231609388, 89.54611873172824, 111.76860730322947, 105.41700080955412, 113.3007361921975, 72.62044465383138, 110.89869494911039, 91.51485286718004, 111.40332406014282, 111.87422309466379, 105.50035326102254, 109.73065980677903, 112.84991779133452, 109.10167467815776, 89.67263433182416, 113.07402224822812, 112.3264731339009, 114.73575347674938, 112.81465256795269, 91.14139634467627, 111.82222642186754, 93.89082285610084, 108.0798578582609, 111.7596685221589, 115.19735514514336, 105.90104307219103, 90.82246125382689, 104.02588745297327, 93.29620871161612, 110.07867204042913, 112.4768181515799, 114.52577476655779, 86.32463585640382, 111.631563599707, 89.75278910069564, 79.50833754124078, 113.2034120387586, 111.4211474428782, 113.00127102732135, 111.87213295522088, 88.65013748674167, 113.9817927063598, 86.64421861385063, 111.02896099079449, 105.67711417388324, 89.86783973826142, 104.2518876141146, 80.64238351785745, 111.7309905599997, 91.98860736874657, 113.67178056280392, 107.51209735059659, 89.63638714336658, 91.05898796687124, 13.592049331491863, 113.1748166915094, 94.34519970323267, 114.54067971133709, 112.41430413138217, 115.4551179151073, 116.48342509404415, 92.63976666830874, 104.66333624948038, 90.43689741911501, 112.41185051131025, 114.42146776251084, 117.12515404779343, 103.83889270704725, 90.47013972583602, 109.9465131249004, 87.16038369502216, 110.66856974466813, 106.845194291598, 114.38922208952265, 103.29550318231688, 109.7317880987817, 112.87533269814158, 94.11931500268196, 111.27566851395021, 106.74133575841664, 110.95014068403059, 112.57982305009253, 87.39005746352059, 112.46780316378072, 86.37901543102721, 113.52619030385559, 109.95252859324985, 109.10208259865206, 104.02810633159447, 93.29934776796745, 108.3160641992877, 90.82778182200778, 110.46463651962297, 112.91966781574203, 113.71333066065257, 111.99632461509624, 90.08603824359729, 105.28389094440762, 92.94772261070622, 108.62687607798249, 113.96329636166178, 90.02747333462094, 111.10513639636848, 87.07203241537034, 113.7721462447045, 78.43311498783092, 112.23338392595412, 112.17911600409592, 110.22416581513225, 112.94281662036084, 88.68690299011978, 109.81834507402, 92.71052813745077, 103.41137450714444, 113.49551606471319, 84.50496131315296, 101.307055393473, 91.41274063821774, 111.42515077084599, 74.78843867340946, 112.94571641093852, 107.17881779989347, 103.70157252127161, 106.37732338362456, 90.92436293470942, 110.67699799572179, 78.86701901250068, 77.42165836122845, 108.24655326863713, 111.8127563770408, 111.56148951177644, 93.39310427950744, 114.08766107071872, 91.97309366794718, 110.39637433955023, 112.38883562281154, 114.52436479944788, 111.48701280110647]
Elapsed: 0.520484716451276~0.26716723278800636
Time per graph: 0.010709056466209897~0.005484239724402959
Speed: 98.78988422839319~14.103788459132023
Total Time: 0.4317
best val loss: 0.13441058993339539 test_score: 0.8958

Testing...
Test loss: 0.3211 score: 0.8958 time: 0.42s
test Score 0.8958
Epoch Time List: [1.4446834400296211, 1.5173492198809981, 1.4439547511283308, 1.6171266881283373, 1.6920144651085138, 1.442387935006991, 1.5468192240223289, 1.4014949609991163, 1.5567117389291525, 1.4461964750662446, 1.6603012459818274, 1.4049961981363595, 1.67645474197343, 1.8866713009774685, 1.5025117918848991, 1.546161302132532, 1.3739364249631763, 1.5171756027266383, 1.5105000650510192, 7.980472585884854, 1.4902043130714446, 1.40281723998487, 1.5279595910105854, 1.4853580894414335, 1.531892633996904, 1.4720180269796401, 1.4004038127604872, 1.4857338978908956, 1.4652372552081943, 1.5832940398249775, 1.4113712888211012, 1.5133749393280596, 1.4965526319574565, 1.4004312958568335, 1.496852247044444, 1.3876855608541518, 1.4774381013121456, 1.4089015130884945, 1.4988046039361507, 1.3736089039593935, 1.4954373468644917, 1.521885374095291, 1.4387210558634251, 1.513912706868723, 1.4222782209981233, 1.536484091077, 1.401933484012261, 1.5549291127827018, 1.4103303977753967, 1.5661356169730425, 1.5092918269801885, 1.5299781439825892, 1.6782864080742002, 1.4424282838590443, 1.568008187925443, 1.530445251846686, 1.4560769409872591, 1.5457732940558344, 1.436869990779087, 1.6290802382864058, 1.5061529288068414, 1.4425094779580832, 1.5033503929153085, 1.4309271869715303, 1.5394161629956216, 1.4549784532282501, 1.5304208036977798, 1.4408991488162428, 1.5373330239672214, 1.513481937116012, 1.44654948473908, 1.5292556958738714, 1.5414456299040467, 1.5501244047190994, 1.4232214309740812, 1.564979740884155, 1.3982208957895637, 1.523490167921409, 1.5220924771856517, 1.406131395837292, 1.5447344628628343, 1.4104504219722003, 1.5838465243577957, 1.4562380292918533, 1.5509592660237104, 1.4823544039390981, 1.5196051669772714, 1.5537130658049136, 1.415250901132822, 1.5843627869617194, 1.4366332220379263, 1.582790478831157, 1.5559978180099279, 1.3915487276390195, 1.5535516368690878, 1.424420117866248, 1.5739897591993213, 1.523830747930333, 1.528723810100928, 1.5469024209305644, 1.4073046641424298, 1.5728628432843834, 1.4196468458976597, 1.7392312481533736, 1.4670056400354952, 1.8179561262950301, 1.608713734196499, 1.6751484929118305, 1.583874307340011, 1.995465942658484, 1.7505325586535037, 7.186693282332271, 1.3961065621115267, 1.5392196711618453, 1.3903920783195645, 1.5773709118366241, 1.5158911237958819, 1.4844542082864791, 1.5418565808795393, 1.4632580529432744, 1.5802119942381978, 1.4354577760677785, 1.6476119153667241, 1.5706814308650792, 1.4574842990841717, 1.5475160100031644, 1.3963068767916411, 1.5720498501323164, 1.5392892640084028, 1.4146238979883492, 1.5281492450740188, 1.4174594143405557, 1.9855754461605102, 1.4655106239952147, 1.5600931157823652, 1.6444998341612518, 1.4482709791045636, 1.493876511696726, 1.5418390410486609, 1.4984377331566066, 1.5439693487714976, 1.4153810343705118, 1.56015973421745, 1.655649685766548, 1.5100218998268247, 1.486603903118521, 1.4664497850462794, 1.5158341727219522, 1.4080285001546144, 1.512663624016568, 1.4175157607533038, 1.4898346008267254, 1.5155999229755253, 1.4445011150091887, 1.5471124867908657, 1.4095422339159995, 1.5619348976761103, 1.4027224911842495, 1.587080653058365, 1.6190360698383301, 1.4477302278392017, 1.5346601239871234, 1.7423182160127908, 1.5504813289735466, 1.4265429012011737, 1.611440556589514, 1.558623629854992, 1.4446709479670972, 1.6074231970123947, 1.4462406837847084, 1.6663672199938446, 1.4635612128768116, 2.020705613307655, 1.8597839497961104, 1.486741840140894, 2.02298490004614, 1.9855277009774, 1.7683872359339148, 1.7897547667380422, 1.7199799260124564, 1.5141817932017148, 1.6829063361510634, 1.5065754770766944, 1.403051109984517, 1.5158018919173628, 1.3973195811267942, 1.543094219174236, 1.5847457919735461, 1.4178919817786664, 1.505253316136077, 1.4618828911334276, 1.5421329580713063, 1.4673830738756806, 1.5542367789894342, 1.5538082821294665, 1.4882999786641449, 1.5290048397146165, 1.4283227608539164, 1.5088920190464705, 7.833677162881941, 1.5258369261864573, 1.482954251114279, 1.428927507949993, 1.47365412581712, 1.4396672991570085, 1.498126773396507, 1.5363255369011313, 1.5375517851207405, 1.5459188411477953, 1.4021554810460657, 1.5554368770681322, 1.4040687379892915, 1.5374048857484013, 1.4443335549440235, 1.534068251028657, 1.5134225238580257, 1.4107196819968522, 1.499554448062554, 1.3943982082419097, 1.4888141080737114, 1.6487617169041187, 1.3951662089675665, 1.496399617055431, 1.4030985611025244, 1.5236834832467139, 1.4404880232177675, 1.5507321499753743, 1.4089300998020917, 1.624306347919628, 1.4724464020691812, 1.5219219652935863, 1.6149632902815938, 1.4162629030179232, 1.543378050904721, 1.4312312679830939, 1.5159972629044205, 1.5488937911577523, 1.4109285969752818, 1.5518099083565176, 1.5437155389226973, 1.6455958201549947, 1.4498521923087537, 1.531511998968199, 1.6519416321534663, 1.4641800811514258, 1.5696259636897594, 1.4344490030780435, 1.5653736589010805, 1.4465181156992912, 1.5862478069029748, 1.4753210823982954, 1.5335039729252458, 1.5683197909966111, 1.4336493941955268, 1.5257880876306444, 1.4067745977081358, 1.4972474270034581, 1.5061328900046647, 1.414242141181603, 1.4964011400006711, 1.4273402988910675, 1.5020255900453776, 1.386846193112433, 1.528844867600128, 1.5753739029169083, 1.4570224697235972, 1.5047364411875606, 1.4661732600070536, 1.503029857063666, 1.443665044149384, 1.8706944219302386, 1.5964942551217973, 1.8390443690586835, 1.8272898911964148, 1.4336745301261544, 1.5572167763020843, 1.4187867243308574, 1.5614382561761886, 1.5324461329728365, 1.4098026182036847, 1.5351508418098092, 1.4067429997958243, 1.5313590308651328, 1.6190044302493334, 1.671207440784201, 1.587096313945949, 1.4381900497246534, 1.5159890733193606, 1.4102802630513906, 1.5223538840655237, 1.8149220328778028, 1.8842031478416175, 4.577108150813729, 3.5711753559298813, 1.4765367470681667, 1.3770843339152634, 1.5014346872922033, 1.3669879310764372, 1.4703081420157105, 1.5202330267056823, 1.431058248039335, 1.5217396630905569, 1.4307839737739414, 1.485233428189531, 1.4056661310605705, 1.610588229028508, 1.5143816086929291, 1.4767027182970196, 1.5296002870891243, 1.4615889401175082, 1.5434159920550883, 1.409959607059136, 1.5617123309057206, 1.4270789320580661, 1.5469581838697195, 1.509653182234615, 1.4590503538493067, 1.5191925610415637, 1.4336258419789374, 1.5478658848442137, 1.578656560042873, 1.4447821520734578, 1.5679450847674161, 1.4882100347895175, 1.5507970391772687, 1.5185494187753648, 1.5554759702645242, 1.5118393439333886, 1.493868871126324, 1.5985129680484533, 1.4326301941182464, 1.5201005039270967, 1.4159638141281903, 1.5216809380799532, 1.9650443820282817, 1.792826707707718, 1.4998662248253822, 1.4139695931226015, 1.5288704801350832, 1.6328372771386057, 1.5489968489855528, 1.5438701589591801, 1.4053451449144632, 1.6458461910951883, 1.4231208451092243, 1.64966945303604, 1.4406089989934117, 1.615113158011809, 1.5798113960772753, 1.4164895189460367, 1.7149498669896275, 1.4612732061650604, 1.4975091798696667, 1.5794773041270673, 1.5323427869006991, 1.5846711569465697, 1.5829268370289356, 1.876055361237377, 1.4957404092419893, 1.6960535300895572, 1.5249928657431155, 1.5837843217886984, 1.5840818099677563, 1.4574839959386736, 1.6570786070078611, 1.6897238462697715, 1.5372030097059906, 1.4984463499858975, 1.5066805619280785, 1.4967815678101033, 1.3974785211030394, 1.5200592402834445, 1.4243142590858042, 1.519695179304108, 1.4159361342899501, 1.5442961500957608]
Total Epoch List: [109, 109, 158]
Total Time List: [0.5635228881146759, 0.5840295979287475, 0.43168111704289913]
T-times Epoch Time: 1.6100439679626148 ~ 0.015521623649997505
T-times Total Epoch: 116.44444444444444 ~ 6.849348892187749
T-times Total Time: 0.5181243406970881 ~ 0.00645341619140351
T-times Inference Elapsed: 0.5226416113287199 ~ 0.003150320342875791
T-times Time Per Graph: 0.01074489781786045 ~ 6.338341545681703e-05
T-times Speed: 98.6287799312372 ~ 0.2264733933340938
T-times cross validation test micro f1 score:0.8931316251675119 ~ 0.008497362851321619
T-times cross validation test precision:0.9090800783979195 ~ 0.016159406418482278
T-times cross validation test recall:0.8814814814814813 ~ 0.006420346601244321
T-times cross validation test f1_score:0.8931316251675119 ~ 0.006528216474519564
