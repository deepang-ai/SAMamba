Namespace(seed=15, model='I2BGNNA', dataset='ico_wallets/Times', num_heads=8, num_layers=2, dim_hidden=64, dropout=0.1, epochs=1000, lr=0.0003, weight_decay=0.0001, batch_size=64, abs_pe=None, abs_pe_dim=20, num_class=2, outdir='./outdir/ico_wallets/Times/seed15/khopgnn_gat_1_0.1_0.0003_0.0001_2_8_64_BN', warmup=10, layer_norm=False, use_edge_attr=False, edge_dim=128, gnn_type='gat', k_hop=1, global_pool='mean', se='khopgnn', ie='mamba', aggr='add', not_extract_node_feature=False, profile=False, k_ford=3, early_stop=1, early_stop_mindelta=-0.0, patience=20, training_times=3, Lambda=0.01, order='sd', use_cuda=True, batch_norm=True, save_logs=True)
Data(edge_index=[2, 338], edge_attr=[338, 2], x=[122, 14887], y=[1, 1], num_nodes=122)
Data(edge_index=[2, 338], edge_attr=[338, 2], x=[122, 14887], y=[1, 1], num_nodes=122)
Data(edge_index=[2, 298], edge_attr=[298, 2], x=[109, 14887], y=[1, 1], num_nodes=122)
========================training times:0========================
========================k_idx:0========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: I2BGNNA
I2BGNN(
  (gcs): ModuleList(
    (0): GCNConv(14887, 64)
    (1): GCNConv(64, 64)
  )
  (bns): ModuleList(
    (0-1): 2 x BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  )
  (drops): ModuleList(
    (0-1): 2 x Dropout(p=0.2, inplace=False)
  )
  (lin1): Linear(in_features=64, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=64, bias=True)
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=64, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=64, out_features=2, bias=True)
  )
)
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x759998ba5b10>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.6927;  Loss pred: 0.6927; Loss self: 0.0000; time: 0.30s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6934 score: 0.5102 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6954 score: 0.4898 time: 0.07s
Epoch 2/1000, LR 0.000000
Train loss: 0.6915;  Loss pred: 0.6915; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6934 score: 0.5102 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6954 score: 0.4898 time: 0.07s
Epoch 3/1000, LR 0.000030
Train loss: 0.6911;  Loss pred: 0.6911; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6933 score: 0.5102 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6953 score: 0.4898 time: 0.07s
Epoch 4/1000, LR 0.000060
Train loss: 0.6906;  Loss pred: 0.6906; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6933 score: 0.5102 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6953 score: 0.4898 time: 0.07s
Epoch 5/1000, LR 0.000090
Train loss: 0.6880;  Loss pred: 0.6880; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6933 score: 0.5102 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6952 score: 0.4898 time: 0.07s
Epoch 6/1000, LR 0.000120
Train loss: 0.6882;  Loss pred: 0.6882; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6933 score: 0.5102 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6952 score: 0.4898 time: 0.07s
Epoch 7/1000, LR 0.000150
Train loss: 0.6877;  Loss pred: 0.6877; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6932 score: 0.5102 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6951 score: 0.4898 time: 0.07s
Epoch 8/1000, LR 0.000180
Train loss: 0.6845;  Loss pred: 0.6845; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6932 score: 0.5102 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6951 score: 0.4898 time: 0.07s
Epoch 9/1000, LR 0.000210
Train loss: 0.6802;  Loss pred: 0.6802; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6932 score: 0.5102 time: 0.09s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6951 score: 0.4898 time: 0.08s
Epoch 10/1000, LR 0.000240
Train loss: 0.6776;  Loss pred: 0.6776; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6931 score: 0.5102 time: 0.09s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6950 score: 0.4898 time: 0.07s
Epoch 11/1000, LR 0.000270
Train loss: 0.6744;  Loss pred: 0.6744; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6931 score: 0.5102 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6950 score: 0.4898 time: 0.07s
Epoch 12/1000, LR 0.000270
Train loss: 0.6709;  Loss pred: 0.6709; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6931 score: 0.5102 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6950 score: 0.4898 time: 0.07s
Epoch 13/1000, LR 0.000270
Train loss: 0.6656;  Loss pred: 0.6656; Loss self: 0.0000; time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6930 score: 0.5102 time: 0.09s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6950 score: 0.4898 time: 0.08s
Epoch 14/1000, LR 0.000270
Train loss: 0.6607;  Loss pred: 0.6607; Loss self: 0.0000; time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6930 score: 0.5102 time: 0.09s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6950 score: 0.4898 time: 0.08s
Epoch 15/1000, LR 0.000270
Train loss: 0.6585;  Loss pred: 0.6585; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6930 score: 0.5102 time: 0.10s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6950 score: 0.4898 time: 0.08s
Epoch 16/1000, LR 0.000270
Train loss: 0.6531;  Loss pred: 0.6531; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6929 score: 0.5102 time: 0.09s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6951 score: 0.4898 time: 0.07s
Epoch 17/1000, LR 0.000270
Train loss: 0.6470;  Loss pred: 0.6470; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6928 score: 0.5102 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6951 score: 0.4898 time: 0.07s
Epoch 18/1000, LR 0.000270
Train loss: 0.6436;  Loss pred: 0.6436; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6928 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6951 score: 0.4898 time: 0.07s
Epoch 19/1000, LR 0.000270
Train loss: 0.6347;  Loss pred: 0.6347; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6927 score: 0.5102 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6951 score: 0.4898 time: 0.20s
Epoch 20/1000, LR 0.000270
Train loss: 0.6279;  Loss pred: 0.6279; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6926 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6951 score: 0.4898 time: 0.07s
Epoch 21/1000, LR 0.000270
Train loss: 0.6197;  Loss pred: 0.6197; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6924 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6950 score: 0.4898 time: 0.07s
Epoch 22/1000, LR 0.000270
Train loss: 0.6124;  Loss pred: 0.6124; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6922 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6950 score: 0.4898 time: 0.07s
Epoch 23/1000, LR 0.000270
Train loss: 0.6061;  Loss pred: 0.6061; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6919 score: 0.5102 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6949 score: 0.4898 time: 0.07s
Epoch 24/1000, LR 0.000270
Train loss: 0.5963;  Loss pred: 0.5963; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6917 score: 0.5102 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6948 score: 0.4898 time: 0.07s
Epoch 25/1000, LR 0.000270
Train loss: 0.5881;  Loss pred: 0.5881; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6914 score: 0.5102 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6947 score: 0.4898 time: 0.21s
Epoch 26/1000, LR 0.000270
Train loss: 0.5793;  Loss pred: 0.5793; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6911 score: 0.5102 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6946 score: 0.4898 time: 0.07s
Epoch 27/1000, LR 0.000270
Train loss: 0.5661;  Loss pred: 0.5661; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6907 score: 0.5102 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6945 score: 0.4898 time: 0.07s
Epoch 28/1000, LR 0.000270
Train loss: 0.5561;  Loss pred: 0.5561; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6903 score: 0.5102 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6944 score: 0.4898 time: 0.07s
Epoch 29/1000, LR 0.000270
Train loss: 0.5480;  Loss pred: 0.5480; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6898 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6941 score: 0.4898 time: 0.07s
Epoch 30/1000, LR 0.000270
Train loss: 0.5325;  Loss pred: 0.5325; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6892 score: 0.5102 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6938 score: 0.4898 time: 0.07s
Epoch 31/1000, LR 0.000270
Train loss: 0.5197;  Loss pred: 0.5197; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6886 score: 0.5102 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6936 score: 0.4898 time: 0.07s
Epoch 32/1000, LR 0.000270
Train loss: 0.5117;  Loss pred: 0.5117; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6879 score: 0.5102 time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6933 score: 0.4898 time: 0.07s
Epoch 33/1000, LR 0.000270
Train loss: 0.4960;  Loss pred: 0.4960; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6871 score: 0.5102 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6929 score: 0.4898 time: 0.07s
Epoch 34/1000, LR 0.000270
Train loss: 0.4827;  Loss pred: 0.4827; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6862 score: 0.5102 time: 0.09s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6925 score: 0.4898 time: 0.07s
Epoch 35/1000, LR 0.000270
Train loss: 0.4637;  Loss pred: 0.4637; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6853 score: 0.5102 time: 0.09s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6922 score: 0.4898 time: 0.07s
Epoch 36/1000, LR 0.000270
Train loss: 0.4488;  Loss pred: 0.4488; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6845 score: 0.5102 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6919 score: 0.4898 time: 0.07s
Epoch 37/1000, LR 0.000270
Train loss: 0.4341;  Loss pred: 0.4341; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6835 score: 0.5102 time: 0.09s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6916 score: 0.4898 time: 0.07s
Epoch 38/1000, LR 0.000270
Train loss: 0.4152;  Loss pred: 0.4152; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6825 score: 0.5102 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6913 score: 0.4898 time: 0.07s
Epoch 39/1000, LR 0.000269
Train loss: 0.4030;  Loss pred: 0.4030; Loss self: 0.0000; time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6814 score: 0.5102 time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6909 score: 0.4898 time: 0.07s
Epoch 40/1000, LR 0.000269
Train loss: 0.3837;  Loss pred: 0.3837; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6801 score: 0.5102 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6905 score: 0.4898 time: 0.07s
Epoch 41/1000, LR 0.000269
Train loss: 0.3711;  Loss pred: 0.3711; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6786 score: 0.5102 time: 0.09s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6899 score: 0.4898 time: 0.07s
Epoch 42/1000, LR 0.000269
Train loss: 0.3520;  Loss pred: 0.3520; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6770 score: 0.5102 time: 0.09s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6894 score: 0.4898 time: 0.07s
Epoch 43/1000, LR 0.000269
Train loss: 0.3505;  Loss pred: 0.3505; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6755 score: 0.5102 time: 0.09s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6889 score: 0.4898 time: 0.07s
Epoch 44/1000, LR 0.000269
Train loss: 0.3237;  Loss pred: 0.3237; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6737 score: 0.5102 time: 0.09s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6883 score: 0.4898 time: 0.07s
Epoch 45/1000, LR 0.000269
Train loss: 0.3092;  Loss pred: 0.3092; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6716 score: 0.5102 time: 0.09s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6875 score: 0.4898 time: 0.07s
Epoch 46/1000, LR 0.000269
Train loss: 0.2917;  Loss pred: 0.2917; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6691 score: 0.5102 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6864 score: 0.4898 time: 0.07s
Epoch 47/1000, LR 0.000269
Train loss: 0.2781;  Loss pred: 0.2781; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6665 score: 0.5102 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6853 score: 0.4898 time: 0.07s
Epoch 48/1000, LR 0.000269
Train loss: 0.2594;  Loss pred: 0.2594; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6634 score: 0.5102 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6839 score: 0.4898 time: 0.07s
Epoch 49/1000, LR 0.000269
Train loss: 0.2436;  Loss pred: 0.2436; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6601 score: 0.5102 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6824 score: 0.4898 time: 0.07s
Epoch 50/1000, LR 0.000269
Train loss: 0.2288;  Loss pred: 0.2288; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6566 score: 0.5102 time: 0.09s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6810 score: 0.4898 time: 0.07s
Epoch 51/1000, LR 0.000269
Train loss: 0.2150;  Loss pred: 0.2150; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6532 score: 0.5102 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6799 score: 0.4898 time: 0.07s
Epoch 52/1000, LR 0.000269
Train loss: 0.1996;  Loss pred: 0.1996; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6497 score: 0.5102 time: 0.09s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6789 score: 0.4898 time: 0.19s
Epoch 53/1000, LR 0.000269
Train loss: 0.1893;  Loss pred: 0.1893; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6460 score: 0.5102 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6779 score: 0.4898 time: 0.07s
Epoch 54/1000, LR 0.000269
Train loss: 0.1726;  Loss pred: 0.1726; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6420 score: 0.5102 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6767 score: 0.4898 time: 0.07s
Epoch 55/1000, LR 0.000269
Train loss: 0.1574;  Loss pred: 0.1574; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6380 score: 0.5102 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6758 score: 0.4898 time: 0.07s
Epoch 56/1000, LR 0.000269
Train loss: 0.1539;  Loss pred: 0.1539; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6332 score: 0.5102 time: 0.09s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6743 score: 0.4898 time: 0.07s
Epoch 57/1000, LR 0.000269
Train loss: 0.1305;  Loss pred: 0.1305; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6282 score: 0.5102 time: 0.09s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6728 score: 0.4898 time: 0.07s
Epoch 58/1000, LR 0.000269
Train loss: 0.1337;  Loss pred: 0.1337; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6232 score: 0.5102 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6717 score: 0.4898 time: 0.07s
Epoch 59/1000, LR 0.000268
Train loss: 0.1152;  Loss pred: 0.1152; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6175 score: 0.5102 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6699 score: 0.4898 time: 0.07s
Epoch 60/1000, LR 0.000268
Train loss: 0.1090;  Loss pred: 0.1090; Loss self: 0.0000; time: 0.13s
Val loss: 0.6114 score: 0.5306 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6681 score: 0.4898 time: 0.07s
Epoch 61/1000, LR 0.000268
Train loss: 0.1045;  Loss pred: 0.1045; Loss self: 0.0000; time: 0.13s
Val loss: 0.6050 score: 0.5306 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6661 score: 0.4898 time: 0.07s
Epoch 62/1000, LR 0.000268
Train loss: 0.0959;  Loss pred: 0.0959; Loss self: 0.0000; time: 0.13s
Val loss: 0.5981 score: 0.5306 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6637 score: 0.4898 time: 0.07s
Epoch 63/1000, LR 0.000268
Train loss: 0.0857;  Loss pred: 0.0857; Loss self: 0.0000; time: 0.13s
Val loss: 0.5907 score: 0.5306 time: 0.09s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6612 score: 0.4898 time: 0.07s
Epoch 64/1000, LR 0.000268
Train loss: 0.0771;  Loss pred: 0.0771; Loss self: 0.0000; time: 0.13s
Val loss: 0.5824 score: 0.5306 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6580 score: 0.4898 time: 0.07s
Epoch 65/1000, LR 0.000268
Train loss: 0.0690;  Loss pred: 0.0690; Loss self: 0.0000; time: 0.13s
Val loss: 0.5728 score: 0.5510 time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6536 score: 0.4898 time: 0.07s
Epoch 66/1000, LR 0.000268
Train loss: 0.0686;  Loss pred: 0.0686; Loss self: 0.0000; time: 0.13s
Val loss: 0.5631 score: 0.5714 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6492 score: 0.4898 time: 0.07s
Epoch 67/1000, LR 0.000268
Train loss: 0.0605;  Loss pred: 0.0605; Loss self: 0.0000; time: 0.13s
Val loss: 0.5518 score: 0.5714 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6431 score: 0.4898 time: 0.07s
Epoch 68/1000, LR 0.000268
Train loss: 0.0544;  Loss pred: 0.0544; Loss self: 0.0000; time: 0.13s
Val loss: 0.5395 score: 0.6122 time: 0.08s
Test loss: 0.6363 score: 0.5102 time: 0.07s
Epoch 69/1000, LR 0.000268
Train loss: 0.0514;  Loss pred: 0.0514; Loss self: 0.0000; time: 0.13s
Val loss: 0.5263 score: 0.6122 time: 0.09s
Test loss: 0.6287 score: 0.5306 time: 0.07s
Epoch 70/1000, LR 0.000268
Train loss: 0.0466;  Loss pred: 0.0466; Loss self: 0.0000; time: 0.13s
Val loss: 0.5117 score: 0.6531 time: 0.09s
Test loss: 0.6196 score: 0.5510 time: 0.07s
Epoch 71/1000, LR 0.000268
Train loss: 0.0415;  Loss pred: 0.0415; Loss self: 0.0000; time: 0.13s
Val loss: 0.4958 score: 0.6735 time: 0.09s
Test loss: 0.6090 score: 0.5510 time: 0.19s
Epoch 72/1000, LR 0.000267
Train loss: 0.0409;  Loss pred: 0.0409; Loss self: 0.0000; time: 0.13s
Val loss: 0.4791 score: 0.6735 time: 0.08s
Test loss: 0.5976 score: 0.5918 time: 0.07s
Epoch 73/1000, LR 0.000267
Train loss: 0.0379;  Loss pred: 0.0379; Loss self: 0.0000; time: 0.13s
Val loss: 0.4619 score: 0.7143 time: 0.08s
Test loss: 0.5856 score: 0.6122 time: 0.07s
Epoch 74/1000, LR 0.000267
Train loss: 0.0334;  Loss pred: 0.0334; Loss self: 0.0000; time: 0.13s
Val loss: 0.4439 score: 0.7551 time: 0.09s
Test loss: 0.5724 score: 0.6327 time: 0.07s
Epoch 75/1000, LR 0.000267
Train loss: 0.0310;  Loss pred: 0.0310; Loss self: 0.0000; time: 0.13s
Val loss: 0.4253 score: 0.7551 time: 0.09s
Test loss: 0.5585 score: 0.6531 time: 0.07s
Epoch 76/1000, LR 0.000267
Train loss: 0.0322;  Loss pred: 0.0322; Loss self: 0.0000; time: 0.13s
Val loss: 0.4071 score: 0.7959 time: 0.09s
Test loss: 0.5445 score: 0.6735 time: 0.07s
Epoch 77/1000, LR 0.000267
Train loss: 0.0266;  Loss pred: 0.0266; Loss self: 0.0000; time: 0.13s
Val loss: 0.3890 score: 0.7959 time: 0.09s
Test loss: 0.5300 score: 0.6939 time: 0.07s
Epoch 78/1000, LR 0.000267
Train loss: 0.0237;  Loss pred: 0.0237; Loss self: 0.0000; time: 0.25s
Val loss: 0.3712 score: 0.8776 time: 0.08s
Test loss: 0.5153 score: 0.7143 time: 0.07s
Epoch 79/1000, LR 0.000267
Train loss: 0.0227;  Loss pred: 0.0227; Loss self: 0.0000; time: 0.13s
Val loss: 0.3535 score: 0.8776 time: 0.09s
Test loss: 0.5000 score: 0.7143 time: 0.07s
Epoch 80/1000, LR 0.000267
Train loss: 0.0204;  Loss pred: 0.0204; Loss self: 0.0000; time: 0.13s
Val loss: 0.3361 score: 0.9184 time: 0.08s
Test loss: 0.4845 score: 0.7347 time: 0.07s
Epoch 81/1000, LR 0.000267
Train loss: 0.0184;  Loss pred: 0.0184; Loss self: 0.0000; time: 0.14s
Val loss: 0.3193 score: 0.9184 time: 0.09s
Test loss: 0.4693 score: 0.7347 time: 0.07s
Epoch 82/1000, LR 0.000267
Train loss: 0.0179;  Loss pred: 0.0179; Loss self: 0.0000; time: 0.13s
Val loss: 0.3031 score: 0.9184 time: 0.09s
Test loss: 0.4543 score: 0.7959 time: 0.07s
Epoch 83/1000, LR 0.000266
Train loss: 0.0169;  Loss pred: 0.0169; Loss self: 0.0000; time: 0.13s
Val loss: 0.2874 score: 0.9184 time: 0.09s
Test loss: 0.4393 score: 0.7959 time: 0.07s
Epoch 84/1000, LR 0.000266
Train loss: 0.0154;  Loss pred: 0.0154; Loss self: 0.0000; time: 0.15s
Val loss: 0.2728 score: 0.9184 time: 0.16s
Test loss: 0.4251 score: 0.8163 time: 0.07s
Epoch 85/1000, LR 0.000266
Train loss: 0.0146;  Loss pred: 0.0146; Loss self: 0.0000; time: 0.13s
Val loss: 0.2597 score: 0.9184 time: 0.09s
Test loss: 0.4120 score: 0.8367 time: 0.07s
Epoch 86/1000, LR 0.000266
Train loss: 0.0132;  Loss pred: 0.0132; Loss self: 0.0000; time: 0.13s
Val loss: 0.2477 score: 0.9388 time: 0.09s
Test loss: 0.3995 score: 0.8367 time: 0.07s
Epoch 87/1000, LR 0.000266
Train loss: 0.0126;  Loss pred: 0.0126; Loss self: 0.0000; time: 0.13s
Val loss: 0.2365 score: 0.9388 time: 0.09s
Test loss: 0.3873 score: 0.8367 time: 0.07s
Epoch 88/1000, LR 0.000266
Train loss: 0.0121;  Loss pred: 0.0121; Loss self: 0.0000; time: 0.14s
Val loss: 0.2267 score: 0.9388 time: 0.09s
Test loss: 0.3762 score: 0.8776 time: 0.07s
Epoch 89/1000, LR 0.000266
Train loss: 0.0107;  Loss pred: 0.0107; Loss self: 0.0000; time: 0.13s
Val loss: 0.2181 score: 0.9388 time: 0.08s
Test loss: 0.3662 score: 0.8980 time: 0.07s
Epoch 90/1000, LR 0.000266
Train loss: 0.0105;  Loss pred: 0.0105; Loss self: 0.0000; time: 0.13s
Val loss: 0.2106 score: 0.9388 time: 0.09s
Test loss: 0.3574 score: 0.8980 time: 0.18s
Epoch 91/1000, LR 0.000266
Train loss: 0.0095;  Loss pred: 0.0095; Loss self: 0.0000; time: 0.13s
Val loss: 0.2041 score: 0.9388 time: 0.09s
Test loss: 0.3494 score: 0.8980 time: 0.07s
Epoch 92/1000, LR 0.000266
Train loss: 0.0090;  Loss pred: 0.0090; Loss self: 0.0000; time: 0.13s
Val loss: 0.1988 score: 0.9388 time: 0.09s
Test loss: 0.3431 score: 0.9184 time: 0.07s
Epoch 93/1000, LR 0.000265
Train loss: 0.0088;  Loss pred: 0.0088; Loss self: 0.0000; time: 0.13s
Val loss: 0.1941 score: 0.9184 time: 0.08s
Test loss: 0.3374 score: 0.9184 time: 0.07s
Epoch 94/1000, LR 0.000265
Train loss: 0.0085;  Loss pred: 0.0085; Loss self: 0.0000; time: 0.13s
Val loss: 0.1902 score: 0.9184 time: 0.09s
Test loss: 0.3326 score: 0.9184 time: 0.07s
Epoch 95/1000, LR 0.000265
Train loss: 0.0080;  Loss pred: 0.0080; Loss self: 0.0000; time: 0.13s
Val loss: 0.1871 score: 0.9184 time: 0.09s
Test loss: 0.3289 score: 0.8980 time: 0.07s
Epoch 96/1000, LR 0.000265
Train loss: 0.0076;  Loss pred: 0.0076; Loss self: 0.0000; time: 0.13s
Val loss: 0.1847 score: 0.9184 time: 0.09s
Test loss: 0.3260 score: 0.8980 time: 0.20s
Epoch 97/1000, LR 0.000265
Train loss: 0.0072;  Loss pred: 0.0072; Loss self: 0.0000; time: 0.13s
Val loss: 0.1829 score: 0.9184 time: 0.08s
Test loss: 0.3239 score: 0.8776 time: 0.07s
Epoch 98/1000, LR 0.000265
Train loss: 0.0064;  Loss pred: 0.0064; Loss self: 0.0000; time: 0.13s
Val loss: 0.1816 score: 0.9184 time: 0.09s
Test loss: 0.3226 score: 0.8776 time: 0.07s
Epoch 99/1000, LR 0.000265
Train loss: 0.0064;  Loss pred: 0.0064; Loss self: 0.0000; time: 0.13s
Val loss: 0.1809 score: 0.9184 time: 0.09s
Test loss: 0.3223 score: 0.8776 time: 0.07s
Epoch 100/1000, LR 0.000265
Train loss: 0.0060;  Loss pred: 0.0060; Loss self: 0.0000; time: 0.13s
Val loss: 0.1805 score: 0.9184 time: 0.08s
Test loss: 0.3223 score: 0.8776 time: 0.07s
Epoch 101/1000, LR 0.000265
Train loss: 0.0062;  Loss pred: 0.0062; Loss self: 0.0000; time: 0.13s
Val loss: 0.1805 score: 0.9184 time: 0.08s
Test loss: 0.3229 score: 0.8776 time: 0.07s
     INFO: Early stopping counter 1 of 20
Epoch 102/1000, LR 0.000264
Train loss: 0.0061;  Loss pred: 0.0061; Loss self: 0.0000; time: 0.13s
Val loss: 0.1809 score: 0.9184 time: 0.13s
Test loss: 0.3239 score: 0.8776 time: 0.08s
     INFO: Early stopping counter 2 of 20
Epoch 103/1000, LR 0.000264
Train loss: 0.0056;  Loss pred: 0.0056; Loss self: 0.0000; time: 0.13s
Val loss: 0.1815 score: 0.9184 time: 0.09s
Test loss: 0.3253 score: 0.8776 time: 0.07s
     INFO: Early stopping counter 3 of 20
Epoch 104/1000, LR 0.000264
Train loss: 0.0052;  Loss pred: 0.0052; Loss self: 0.0000; time: 0.13s
Val loss: 0.1821 score: 0.9184 time: 0.08s
Test loss: 0.3270 score: 0.8776 time: 0.07s
     INFO: Early stopping counter 4 of 20
Epoch 105/1000, LR 0.000264
Train loss: 0.0050;  Loss pred: 0.0050; Loss self: 0.0000; time: 0.13s
Val loss: 0.1830 score: 0.9184 time: 0.08s
Test loss: 0.3289 score: 0.8776 time: 0.07s
     INFO: Early stopping counter 5 of 20
Epoch 106/1000, LR 0.000264
Train loss: 0.0048;  Loss pred: 0.0048; Loss self: 0.0000; time: 0.13s
Val loss: 0.1840 score: 0.9184 time: 0.08s
Test loss: 0.3311 score: 0.8776 time: 0.07s
     INFO: Early stopping counter 6 of 20
Epoch 107/1000, LR 0.000264
Train loss: 0.0048;  Loss pred: 0.0048; Loss self: 0.0000; time: 0.13s
Val loss: 0.1849 score: 0.9184 time: 0.08s
Test loss: 0.3333 score: 0.8776 time: 0.07s
     INFO: Early stopping counter 7 of 20
Epoch 108/1000, LR 0.000264
Train loss: 0.0046;  Loss pred: 0.0046; Loss self: 0.0000; time: 0.13s
Val loss: 0.1860 score: 0.9184 time: 0.08s
Test loss: 0.3358 score: 0.8776 time: 0.15s
     INFO: Early stopping counter 8 of 20
Epoch 109/1000, LR 0.000264
Train loss: 0.0046;  Loss pred: 0.0046; Loss self: 0.0000; time: 0.14s
Val loss: 0.1870 score: 0.9184 time: 0.09s
Test loss: 0.3383 score: 0.8776 time: 0.07s
     INFO: Early stopping counter 9 of 20
Epoch 110/1000, LR 0.000263
Train loss: 0.0043;  Loss pred: 0.0043; Loss self: 0.0000; time: 0.13s
Val loss: 0.1881 score: 0.9184 time: 0.09s
Test loss: 0.3407 score: 0.8776 time: 0.07s
     INFO: Early stopping counter 10 of 20
Epoch 111/1000, LR 0.000263
Train loss: 0.0043;  Loss pred: 0.0043; Loss self: 0.0000; time: 0.14s
Val loss: 0.1892 score: 0.9184 time: 0.09s
Test loss: 0.3431 score: 0.8776 time: 0.07s
     INFO: Early stopping counter 11 of 20
Epoch 112/1000, LR 0.000263
Train loss: 0.0038;  Loss pred: 0.0038; Loss self: 0.0000; time: 0.14s
Val loss: 0.1904 score: 0.9184 time: 0.09s
Test loss: 0.3457 score: 0.8776 time: 0.07s
     INFO: Early stopping counter 12 of 20
Epoch 113/1000, LR 0.000263
Train loss: 0.0038;  Loss pred: 0.0038; Loss self: 0.0000; time: 0.14s
Val loss: 0.1915 score: 0.9184 time: 0.08s
Test loss: 0.3481 score: 0.8776 time: 0.07s
     INFO: Early stopping counter 13 of 20
Epoch 114/1000, LR 0.000263
Train loss: 0.0042;  Loss pred: 0.0042; Loss self: 0.0000; time: 0.14s
Val loss: 0.1926 score: 0.9184 time: 0.08s
Test loss: 0.3501 score: 0.8776 time: 0.07s
     INFO: Early stopping counter 14 of 20
Epoch 115/1000, LR 0.000263
Train loss: 0.0037;  Loss pred: 0.0037; Loss self: 0.0000; time: 0.15s
Val loss: 0.1937 score: 0.9184 time: 0.15s
Test loss: 0.3520 score: 0.8776 time: 0.07s
     INFO: Early stopping counter 15 of 20
Epoch 116/1000, LR 0.000263
Train loss: 0.0035;  Loss pred: 0.0035; Loss self: 0.0000; time: 0.13s
Val loss: 0.1950 score: 0.9184 time: 0.09s
Test loss: 0.3541 score: 0.8776 time: 0.07s
     INFO: Early stopping counter 16 of 20
Epoch 117/1000, LR 0.000262
Train loss: 0.0034;  Loss pred: 0.0034; Loss self: 0.0000; time: 0.13s
Val loss: 0.1961 score: 0.9184 time: 0.09s
Test loss: 0.3562 score: 0.8776 time: 0.07s
     INFO: Early stopping counter 17 of 20
Epoch 118/1000, LR 0.000262
Train loss: 0.0033;  Loss pred: 0.0033; Loss self: 0.0000; time: 0.13s
Val loss: 0.1971 score: 0.9184 time: 0.08s
Test loss: 0.3581 score: 0.8776 time: 0.07s
     INFO: Early stopping counter 18 of 20
Epoch 119/1000, LR 0.000262
Train loss: 0.0034;  Loss pred: 0.0034; Loss self: 0.0000; time: 0.13s
Val loss: 0.1981 score: 0.9184 time: 0.09s
Test loss: 0.3600 score: 0.8776 time: 0.07s
     INFO: Early stopping counter 19 of 20
Epoch 120/1000, LR 0.000262
Train loss: 0.0032;  Loss pred: 0.0032; Loss self: 0.0000; time: 0.13s
Val loss: 0.1992 score: 0.9184 time: 0.09s
Test loss: 0.3619 score: 0.8776 time: 0.07s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 099,   Train_Loss: 0.0060,   Val_Loss: 0.1805,   Val_Precision: 0.9545,   Val_Recall: 0.8750,   Val_accuracy: 0.9130,   Val_Score: 0.9184,   Val_Loss: 0.1805,   Test_Precision: 0.9130,   Test_Recall: 0.8400,   Test_accuracy: 0.8750,   Test_Score: 0.8776,   Test_loss: 0.3223


[0.07038930093403906, 0.07231412595137954, 0.0725724590010941, 0.07215185498353094, 0.07197175500914454, 0.071133290999569, 0.07403276301920414, 0.07943737599998713, 0.08081716508604586, 0.07446761301252991, 0.07468703191261739, 0.07876408193260431, 0.0851241450291127, 0.08474375796504319, 0.09266397298779339, 0.0768191940151155, 0.07193765102420002, 0.07158423902001232, 0.20037935709115118, 0.07161905895918608, 0.07122760603670031, 0.07153613690752536, 0.0770906339166686, 0.07235636503901333, 0.21730803896207362, 0.07101385295391083, 0.07000854099169374, 0.07024954294320196, 0.07071646791882813, 0.07683377398643643, 0.07672727899625897, 0.07660546607803553, 0.07612720306497067, 0.07657695806119591, 0.07689940195996314, 0.07681189698632807, 0.07681142399087548, 0.0769569700350985, 0.07440302101895213, 0.07597932091448456, 0.07646483299322426, 0.07632164901588112, 0.07787653000559658, 0.076164641068317, 0.0763582120416686, 0.07544933399185538, 0.0758136420045048, 0.07996649900451303, 0.07629981497302651, 0.07608969905413687, 0.07535254897084087, 0.19807178201153874, 0.07495569007005543, 0.07539584895130247, 0.07613797904923558, 0.07705170603003353, 0.07602654898073524, 0.07555582805071026, 0.07454343000426888, 0.07443252997472882, 0.07584675902035087, 0.07576804200652987, 0.07674941897857934, 0.07561579497996718, 0.07540330197662115, 0.07557258999440819, 0.07531619898509234, 0.0762632469413802, 0.07653705799020827, 0.07541496795602143, 0.19383304205257446, 0.07539575302507728, 0.07535506098065525, 0.07695733802393079, 0.07678640109952539, 0.07632980996277183, 0.08131813502404839, 0.07612489396706223, 0.07650837197434157, 0.07582071493379772, 0.07763806695584208, 0.07747288700193167, 0.07635092595592141, 0.07640174904372543, 0.07694882503710687, 0.07599260902497917, 0.07601368904579431, 0.07769416901282966, 0.07631841208785772, 0.18688034103251994, 0.07614671206101775, 0.07646169699728489, 0.07738347898703068, 0.07799243705812842, 0.07647392898797989, 0.20258380903396755, 0.07715395104605705, 0.07703190704341978, 0.07766890397761017, 0.07350115908775479, 0.07381051406264305, 0.08324953995179385, 0.07640224299393594, 0.07183305500075221, 0.07328689994756132, 0.07415547093842179, 0.07656635600142181, 0.15570054203271866, 0.07788999599870294, 0.07852851902134717, 0.07795114803593606, 0.07398478698451072, 0.07264676899649203, 0.072598697966896, 0.0753917460097, 0.07626780495047569, 0.07687170500867069, 0.07586873893160373, 0.07690374401863664, 0.07647379406262189]
[0.0014365163455926338, 0.001475798488803664, 0.0014810705918590634, 0.0014724868363985906, 0.0014688113267172355, 0.0014516998163177346, 0.0015108727146776356, 0.0016211709387752475, 0.0016493298997152218, 0.0015197472043373451, 0.0015242251410738242, 0.001607430243522537, 0.0017372274495737285, 0.0017294644482661874, 0.0018911014895468038, 0.001567738653369704, 0.0014681153270244903, 0.0014609028371431086, 0.004089374634513289, 0.0014616134481466546, 0.0014536246129938839, 0.0014599211613780686, 0.0015732782431973182, 0.001476660511000272, 0.004434857938001502, 0.0014492623051818535, 0.0014287457345243618, 0.0014336641416979991, 0.001443193222833227, 0.001568036203804825, 0.0015658628366583464, 0.0015633768587354191, 0.0015536163890810342, 0.001562795062473386, 0.0015693755502033295, 0.0015675897344148584, 0.0015675800814464384, 0.001570550408879561, 0.0015184290003867782, 0.001550598386009889, 0.001560506795780087, 0.0015575846737934922, 0.001589316938889726, 0.0015543804299656529, 0.0015583308579932367, 0.0015397823263643955, 0.001547217183765404, 0.0016319693674390413, 0.0015571390810821737, 0.0015528510011048342, 0.0015378071218538954, 0.004042281265541607, 0.0015297079606133761, 0.0015386907949245401, 0.0015538363071272568, 0.0015724837965312966, 0.0015515622240966376, 0.001541955674504291, 0.0015212944898830385, 0.0015190312239740575, 0.0015478930412316505, 0.0015462865715618341, 0.0015663146730322316, 0.0015431794893870853, 0.0015388428974820643, 0.0015422977549879222, 0.0015370652854100478, 0.0015563927947220449, 0.001561980775310373, 0.0015390809786943148, 0.003955776368419887, 0.0015386888372464751, 0.0015378583873603114, 0.0015705579188557304, 0.0015670694101943957, 0.0015577512237300373, 0.0016595537760009875, 0.0015535692646339232, 0.0015613953464151341, 0.001547361529261178, 0.0015844503460375934, 0.001581079326570034, 0.001558182162365743, 0.0015592193682392945, 0.0015703841844307525, 0.0015508695719383505, 0.0015512997764447819, 0.0015855952859761156, 0.0015575186140379127, 0.003813884510867754, 0.0015540145318575051, 0.001560442795862957, 0.001579254673204708, 0.0015916823889413963, 0.0015606924283261203, 0.004134363449672807, 0.0015745704295113683, 0.0015720797355799954, 0.0015850796730124525, 0.0015000236548521385, 0.0015063370216865928, 0.0016989702030978336, 0.0015592294488558356, 0.0014659807143010655, 0.0014956510193379863, 0.0015133769579269752, 0.0015625786939065676, 0.0031775620823003806, 0.0015895917550755702, 0.0016026228371703503, 0.0015908397558354298, 0.0015098936119287902, 0.0014825871223773882, 0.0014816060809570613, 0.001538607061422449, 0.0015564858153158305, 0.0015688103062994018, 0.0015483416108490558, 0.0015694641636456459, 0.0015606896747473854]
[696.1285216615136, 677.599284446101, 675.1872635218448, 679.1232188165435, 680.8226365158691, 688.8476451946656, 661.8691239078754, 616.8380989826243, 606.3068402341236, 658.0041714477308, 656.0710573868996, 622.1109774621331, 575.6298636919274, 578.2136782299943, 528.7923496055448, 637.8614176862934, 681.1453988609701, 684.5082195579589, 244.53616735435597, 684.1754235827628, 687.9355172312341, 684.9684945014883, 635.6154763621056, 677.203725941457, 225.48636596252143, 690.0062165589271, 699.9146005030111, 697.51343492181, 692.9079101666197, 637.7403771504188, 638.6255402382915, 639.6410401065281, 643.6595333494784, 639.8791652293372, 637.196112728046, 637.9220136787095, 637.9259419252631, 636.7194547505197, 658.575409021612, 644.9123183813391, 640.8174592409299, 642.0196711132907, 629.2011212682258, 643.3431486409648, 641.7122492766168, 649.4424457781093, 646.3216738365959, 612.7565994509102, 642.2033922011798, 643.9767880424538, 650.2766086779825, 247.38506162955352, 653.7195502329896, 649.9031535761163, 643.5684347270832, 635.9366005588582, 644.5116956764191, 648.5270728171098, 657.3349253877091, 658.3143152145491, 646.0394700168076, 646.7106540218773, 638.4413152844302, 648.0127599396598, 649.838915743935, 648.3832299994699, 650.5904527882345, 642.5113270834624, 640.2127451288864, 649.7383918345569, 252.79487687506574, 649.9039804496969, 650.2549312856241, 636.7164101331425, 638.1338270625482, 641.9510283583656, 602.5716156120541, 643.6790574867841, 640.4527862183894, 646.2613817712479, 631.1336940919659, 632.4793343350979, 641.7734871779875, 641.3465740418698, 636.7868512140482, 644.7995486494409, 644.6207336481201, 630.6779597823952, 642.0469013898144, 262.1998639839451, 643.4946260153085, 640.8437416938309, 633.2100939557434, 628.2660453792448, 640.7412388567323, 241.87520332280894, 635.0938524295334, 636.100051013675, 630.8831139695928, 666.6561535648401, 663.8620611477337, 588.5918412086571, 641.3424276547632, 682.1372138423862, 668.6051672953935, 660.7739035288344, 639.9677685991754, 314.70667577832336, 629.0923419846622, 623.9771309921158, 628.5988241944896, 662.2983183050662, 674.4966180445839, 674.9432341382117, 649.9385223641802, 642.4729285419717, 637.4256951172487, 645.852306101646, 637.1601360282989, 640.7423693386456]
Elapsed: 0.08277989558506912~0.028053120594489372
Time per graph: 0.0016893856241850839~0.0005725126651936606
Speed: 623.621714033506~94.92930035691485
Total Time: 0.0770
best val loss: 0.18052706122398376 test_score: 0.8776

Testing...
Test loss: 0.3995 score: 0.8367 time: 0.07s
test Score 0.8367
Epoch Time List: [0.45147583296056837, 0.26700617792084813, 0.2705245789838955, 0.2711127110524103, 0.27711690904106945, 0.2702163300709799, 0.27937917900271714, 0.28489178407471627, 0.2999167019734159, 0.2868298531975597, 0.2871128509286791, 0.2855395779479295, 0.44470822892617434, 0.31885680998675525, 0.324563693953678, 0.29417327092960477, 0.2725801628548652, 0.2676425928948447, 0.3972845111275092, 0.26695142698008567, 0.2662792840274051, 0.2666865369537845, 0.277162668062374, 0.27496886102017015, 0.4173425429034978, 0.2752722700824961, 0.2665053380187601, 0.26860476413276047, 0.26250575797166675, 0.2812036970863119, 0.2835975360358134, 0.3678049040026963, 0.2866439529461786, 0.287601264892146, 0.28932875604368746, 0.2837535480502993, 0.2985356649151072, 0.2881823137868196, 0.3624461689032614, 0.28406377404462546, 0.28851149301044643, 0.28746576188132167, 0.2956457460531965, 0.2885552739026025, 0.2879637099104002, 0.41320351511240005, 0.2842101859860122, 0.2875938209472224, 0.2859216219512746, 0.2871118780458346, 0.2851932880003005, 0.4072667279979214, 0.28253552014939487, 0.28300272312480956, 0.2832406460074708, 0.28523488296195865, 0.28730510897003114, 0.28324073110707104, 0.3884441270492971, 0.2819499270990491, 0.2824398059165105, 0.28287428012117743, 0.28774206805974245, 0.28375966497696936, 0.4180663110455498, 0.28299058601260185, 0.28291781386360526, 0.28564787609502673, 0.29045996197964996, 0.28723464091308415, 0.40690262196585536, 0.2868193540489301, 0.2838244130834937, 0.28772560006473213, 0.28868449793662876, 0.29079251806251705, 0.2913852819474414, 0.40228643105365336, 0.28747263306286186, 0.2831268550362438, 0.2976439918857068, 0.2932638900820166, 0.28915290790610015, 0.3736255979165435, 0.28906135878060013, 0.28850617504213005, 0.28850121598225087, 0.29476363002322614, 0.28933205106295645, 0.3978961770189926, 0.28710500698070973, 0.2878032550215721, 0.28616605408024043, 0.28991878498345613, 0.29278348200023174, 0.41326879896223545, 0.2909319770988077, 0.2917484169593081, 0.29298111516982317, 0.2844532928429544, 0.2840541630284861, 0.34133313491474837, 0.2865197039209306, 0.27373663999605924, 0.27867330296430737, 0.2824410990579054, 0.2821520739234984, 0.36133375903591514, 0.3029205030761659, 0.293214876903221, 0.29662124090828, 0.2935400160495192, 0.28036419104319066, 0.29125547921285033, 0.36514891509432346, 0.29096432495862246, 0.2919218259630725, 0.2897517828969285, 0.2936396269360557, 0.29289508203510195]
Total Epoch List: [120]
Total Time List: [0.07703959301579744]
========================k_idx:1========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: I2BGNNA
I2BGNN(
  (gcs): ModuleList(
    (0): GCNConv(14887, 64)
    (1): GCNConv(64, 64)
  )
  (bns): ModuleList(
    (0-1): 2 x BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  )
  (drops): ModuleList(
    (0-1): 2 x Dropout(p=0.2, inplace=False)
  )
  (lin1): Linear(in_features=64, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=64, bias=True)
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=64, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=64, out_features=2, bias=True)
  )
)
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x759998cecfa0>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.6971;  Loss pred: 0.6971; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6929 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6937 score: 0.4898 time: 0.08s
Epoch 2/1000, LR 0.000000
Train loss: 0.6980;  Loss pred: 0.6980; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6929 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6937 score: 0.4898 time: 0.09s
     INFO: Early stopping counter 1 of 20
Epoch 3/1000, LR 0.000030
Train loss: 0.6970;  Loss pred: 0.6970; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6929 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6938 score: 0.4898 time: 0.09s
Epoch 4/1000, LR 0.000060
Train loss: 0.6951;  Loss pred: 0.6951; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6929 score: 0.5102 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6938 score: 0.4898 time: 0.09s
     INFO: Early stopping counter 1 of 20
Epoch 5/1000, LR 0.000090
Train loss: 0.6936;  Loss pred: 0.6936; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6929 score: 0.5102 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6937 score: 0.4898 time: 0.09s
     INFO: Early stopping counter 2 of 20
Epoch 6/1000, LR 0.000120
Train loss: 0.6922;  Loss pred: 0.6922; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6929 score: 0.5102 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6937 score: 0.4898 time: 0.09s
Epoch 7/1000, LR 0.000150
Train loss: 0.6901;  Loss pred: 0.6901; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6929 score: 0.5102 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6937 score: 0.4898 time: 0.09s
Epoch 8/1000, LR 0.000180
Train loss: 0.6894;  Loss pred: 0.6894; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6929 score: 0.5102 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6936 score: 0.4898 time: 0.09s
     INFO: Early stopping counter 1 of 20
Epoch 9/1000, LR 0.000210
Train loss: 0.6842;  Loss pred: 0.6842; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6929 score: 0.5102 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6935 score: 0.4898 time: 0.09s
     INFO: Early stopping counter 2 of 20
Epoch 10/1000, LR 0.000240
Train loss: 0.6809;  Loss pred: 0.6809; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6929 score: 0.5102 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6935 score: 0.4898 time: 0.09s
     INFO: Early stopping counter 3 of 20
Epoch 11/1000, LR 0.000270
Train loss: 0.6795;  Loss pred: 0.6795; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6929 score: 0.5102 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6934 score: 0.4898 time: 0.09s
     INFO: Early stopping counter 4 of 20
Epoch 12/1000, LR 0.000270
Train loss: 0.6731;  Loss pred: 0.6731; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6929 score: 0.5102 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6933 score: 0.4898 time: 0.09s
     INFO: Early stopping counter 5 of 20
Epoch 13/1000, LR 0.000270
Train loss: 0.6724;  Loss pred: 0.6724; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6929 score: 0.5102 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6933 score: 0.4898 time: 0.09s
     INFO: Early stopping counter 6 of 20
Epoch 14/1000, LR 0.000270
Train loss: 0.6670;  Loss pred: 0.6670; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6929 score: 0.5102 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6932 score: 0.4898 time: 0.09s
Epoch 15/1000, LR 0.000270
Train loss: 0.6608;  Loss pred: 0.6608; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6929 score: 0.5102 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6931 score: 0.4898 time: 0.09s
     INFO: Early stopping counter 1 of 20
Epoch 16/1000, LR 0.000270
Train loss: 0.6598;  Loss pred: 0.6598; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6929 score: 0.5102 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6930 score: 0.4898 time: 0.09s
     INFO: Early stopping counter 2 of 20
Epoch 17/1000, LR 0.000270
Train loss: 0.6532;  Loss pred: 0.6532; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6929 score: 0.5102 time: 0.09s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6930 score: 0.4898 time: 0.09s
     INFO: Early stopping counter 3 of 20
Epoch 18/1000, LR 0.000270
Train loss: 0.6446;  Loss pred: 0.6446; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6929 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6929 score: 0.4898 time: 0.09s
Epoch 19/1000, LR 0.000270
Train loss: 0.6401;  Loss pred: 0.6401; Loss self: 0.0000; time: 0.14s
Val loss: 0.6928 score: 0.8367 time: 0.07s
Test loss: 0.6927 score: 0.9388 time: 0.09s
Epoch 20/1000, LR 0.000270
Train loss: 0.6350;  Loss pred: 0.6350; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6928 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6925 score: 0.5102 time: 0.09s
Epoch 21/1000, LR 0.000270
Train loss: 0.6273;  Loss pred: 0.6273; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6927 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6923 score: 0.5102 time: 0.09s
Epoch 22/1000, LR 0.000270
Train loss: 0.6205;  Loss pred: 0.6205; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6926 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6921 score: 0.5102 time: 0.09s
Epoch 23/1000, LR 0.000270
Train loss: 0.6100;  Loss pred: 0.6100; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6926 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6919 score: 0.5102 time: 0.09s
Epoch 24/1000, LR 0.000270
Train loss: 0.6015;  Loss pred: 0.6015; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6925 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6917 score: 0.5102 time: 0.09s
Epoch 25/1000, LR 0.000270
Train loss: 0.5917;  Loss pred: 0.5917; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6923 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6914 score: 0.5102 time: 0.09s
Epoch 26/1000, LR 0.000270
Train loss: 0.5805;  Loss pred: 0.5805; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6922 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6911 score: 0.5102 time: 0.09s
Epoch 27/1000, LR 0.000270
Train loss: 0.5692;  Loss pred: 0.5692; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6920 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6907 score: 0.5102 time: 0.09s
Epoch 28/1000, LR 0.000270
Train loss: 0.5574;  Loss pred: 0.5574; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6918 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6903 score: 0.5102 time: 0.09s
Epoch 29/1000, LR 0.000270
Train loss: 0.5399;  Loss pred: 0.5399; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6914 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6897 score: 0.5102 time: 0.09s
Epoch 30/1000, LR 0.000270
Train loss: 0.5270;  Loss pred: 0.5270; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6910 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6890 score: 0.5102 time: 0.09s
Epoch 31/1000, LR 0.000270
Train loss: 0.5180;  Loss pred: 0.5180; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6905 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6883 score: 0.5102 time: 0.09s
Epoch 32/1000, LR 0.000270
Train loss: 0.5041;  Loss pred: 0.5041; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6898 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6873 score: 0.5102 time: 0.09s
Epoch 33/1000, LR 0.000270
Train loss: 0.4886;  Loss pred: 0.4886; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6889 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6860 score: 0.5102 time: 0.09s
Epoch 34/1000, LR 0.000270
Train loss: 0.4785;  Loss pred: 0.4785; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6879 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6846 score: 0.5102 time: 0.09s
Epoch 35/1000, LR 0.000270
Train loss: 0.4599;  Loss pred: 0.4599; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6869 score: 0.4898 time: 0.08s
Test loss: 0.6832 score: 0.5306 time: 0.09s
Epoch 36/1000, LR 0.000270
Train loss: 0.4475;  Loss pred: 0.4475; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6859 score: 0.4898 time: 0.07s
Test loss: 0.6816 score: 0.5714 time: 0.09s
Epoch 37/1000, LR 0.000270
Train loss: 0.4325;  Loss pred: 0.4325; Loss self: 0.0000; time: 0.13s
Val loss: 0.6847 score: 0.5102 time: 0.08s
Test loss: 0.6799 score: 0.6122 time: 0.09s
Epoch 38/1000, LR 0.000270
Train loss: 0.4095;  Loss pred: 0.4095; Loss self: 0.0000; time: 0.14s
Val loss: 0.6834 score: 0.5714 time: 0.07s
Test loss: 0.6780 score: 0.7551 time: 0.09s
Epoch 39/1000, LR 0.000269
Train loss: 0.3971;  Loss pred: 0.3971; Loss self: 0.0000; time: 0.14s
Val loss: 0.6820 score: 0.5714 time: 0.07s
Test loss: 0.6758 score: 0.8163 time: 0.09s
Epoch 40/1000, LR 0.000269
Train loss: 0.3817;  Loss pred: 0.3817; Loss self: 0.0000; time: 0.14s
Val loss: 0.6804 score: 0.6531 time: 0.08s
Test loss: 0.6735 score: 0.8571 time: 0.09s
Epoch 41/1000, LR 0.000269
Train loss: 0.3670;  Loss pred: 0.3670; Loss self: 0.0000; time: 0.14s
Val loss: 0.6786 score: 0.7755 time: 0.08s
Test loss: 0.6708 score: 0.8571 time: 0.09s
Epoch 42/1000, LR 0.000269
Train loss: 0.3503;  Loss pred: 0.3503; Loss self: 0.0000; time: 0.14s
Val loss: 0.6765 score: 0.8163 time: 0.08s
Test loss: 0.6678 score: 0.8776 time: 0.09s
Epoch 43/1000, LR 0.000269
Train loss: 0.3351;  Loss pred: 0.3351; Loss self: 0.0000; time: 0.14s
Val loss: 0.6743 score: 0.8367 time: 0.08s
Test loss: 0.6644 score: 0.8980 time: 0.09s
Epoch 44/1000, LR 0.000269
Train loss: 0.3183;  Loss pred: 0.3183; Loss self: 0.0000; time: 0.14s
Val loss: 0.6717 score: 0.8163 time: 0.07s
Test loss: 0.6606 score: 0.9388 time: 0.09s
Epoch 45/1000, LR 0.000269
Train loss: 0.3075;  Loss pred: 0.3075; Loss self: 0.0000; time: 0.14s
Val loss: 0.6689 score: 0.8163 time: 0.08s
Test loss: 0.6564 score: 0.9388 time: 0.09s
Epoch 46/1000, LR 0.000269
Train loss: 0.2917;  Loss pred: 0.2917; Loss self: 0.0000; time: 0.14s
Val loss: 0.6657 score: 0.8367 time: 0.07s
Test loss: 0.6517 score: 0.9388 time: 0.09s
Epoch 47/1000, LR 0.000269
Train loss: 0.2760;  Loss pred: 0.2760; Loss self: 0.0000; time: 0.14s
Val loss: 0.6623 score: 0.8571 time: 0.07s
Test loss: 0.6464 score: 0.9388 time: 0.09s
Epoch 48/1000, LR 0.000269
Train loss: 0.2619;  Loss pred: 0.2619; Loss self: 0.0000; time: 0.14s
Val loss: 0.6583 score: 0.8571 time: 0.07s
Test loss: 0.6405 score: 0.9388 time: 0.09s
Epoch 49/1000, LR 0.000269
Train loss: 0.2501;  Loss pred: 0.2501; Loss self: 0.0000; time: 0.14s
Val loss: 0.6539 score: 0.8571 time: 0.07s
Test loss: 0.6339 score: 0.9388 time: 0.09s
Epoch 50/1000, LR 0.000269
Train loss: 0.2374;  Loss pred: 0.2374; Loss self: 0.0000; time: 0.14s
Val loss: 0.6492 score: 0.8571 time: 0.07s
Test loss: 0.6268 score: 0.9388 time: 0.09s
Epoch 51/1000, LR 0.000269
Train loss: 0.2258;  Loss pred: 0.2258; Loss self: 0.0000; time: 0.14s
Val loss: 0.6438 score: 0.8571 time: 0.07s
Test loss: 0.6190 score: 0.9388 time: 0.09s
Epoch 52/1000, LR 0.000269
Train loss: 0.2114;  Loss pred: 0.2114; Loss self: 0.0000; time: 0.14s
Val loss: 0.6380 score: 0.8571 time: 0.07s
Test loss: 0.6105 score: 0.9388 time: 0.09s
Epoch 53/1000, LR 0.000269
Train loss: 0.1969;  Loss pred: 0.1969; Loss self: 0.0000; time: 0.14s
Val loss: 0.6318 score: 0.8776 time: 0.08s
Test loss: 0.6014 score: 0.9388 time: 0.09s
Epoch 54/1000, LR 0.000269
Train loss: 0.1881;  Loss pred: 0.1881; Loss self: 0.0000; time: 0.14s
Val loss: 0.6253 score: 0.8776 time: 0.08s
Test loss: 0.5919 score: 0.9388 time: 0.09s
Epoch 55/1000, LR 0.000269
Train loss: 0.1758;  Loss pred: 0.1758; Loss self: 0.0000; time: 0.14s
Val loss: 0.6182 score: 0.8776 time: 0.08s
Test loss: 0.5816 score: 0.9592 time: 0.09s
Epoch 56/1000, LR 0.000269
Train loss: 0.1664;  Loss pred: 0.1664; Loss self: 0.0000; time: 0.14s
Val loss: 0.6107 score: 0.8776 time: 0.08s
Test loss: 0.5708 score: 0.9592 time: 0.09s
Epoch 57/1000, LR 0.000269
Train loss: 0.1562;  Loss pred: 0.1562; Loss self: 0.0000; time: 0.14s
Val loss: 0.6027 score: 0.8776 time: 0.08s
Test loss: 0.5594 score: 0.9592 time: 0.09s
Epoch 58/1000, LR 0.000269
Train loss: 0.1433;  Loss pred: 0.1433; Loss self: 0.0000; time: 0.13s
Val loss: 0.5943 score: 0.8776 time: 0.08s
Test loss: 0.5475 score: 0.9592 time: 0.09s
Epoch 59/1000, LR 0.000268
Train loss: 0.1369;  Loss pred: 0.1369; Loss self: 0.0000; time: 0.14s
Val loss: 0.5856 score: 0.8776 time: 0.08s
Test loss: 0.5351 score: 0.9592 time: 0.09s
Epoch 60/1000, LR 0.000268
Train loss: 0.1236;  Loss pred: 0.1236; Loss self: 0.0000; time: 0.14s
Val loss: 0.5766 score: 0.8776 time: 0.08s
Test loss: 0.5223 score: 0.9592 time: 0.09s
Epoch 61/1000, LR 0.000268
Train loss: 0.1185;  Loss pred: 0.1185; Loss self: 0.0000; time: 0.14s
Val loss: 0.5668 score: 0.8776 time: 0.08s
Test loss: 0.5086 score: 0.9592 time: 0.09s
Epoch 62/1000, LR 0.000268
Train loss: 0.1058;  Loss pred: 0.1058; Loss self: 0.0000; time: 0.14s
Val loss: 0.5566 score: 0.8776 time: 0.07s
Test loss: 0.4943 score: 0.9592 time: 0.09s
Epoch 63/1000, LR 0.000268
Train loss: 0.1035;  Loss pred: 0.1035; Loss self: 0.0000; time: 0.14s
Val loss: 0.5460 score: 0.8571 time: 0.08s
Test loss: 0.4796 score: 0.9592 time: 0.09s
Epoch 64/1000, LR 0.000268
Train loss: 0.0899;  Loss pred: 0.0899; Loss self: 0.0000; time: 0.14s
Val loss: 0.5349 score: 0.8571 time: 0.08s
Test loss: 0.4642 score: 0.9592 time: 0.09s
Epoch 65/1000, LR 0.000268
Train loss: 0.0833;  Loss pred: 0.0833; Loss self: 0.0000; time: 0.14s
Val loss: 0.5236 score: 0.8571 time: 0.07s
Test loss: 0.4486 score: 0.9592 time: 0.09s
Epoch 66/1000, LR 0.000268
Train loss: 0.0783;  Loss pred: 0.0783; Loss self: 0.0000; time: 0.14s
Val loss: 0.5120 score: 0.8571 time: 0.08s
Test loss: 0.4327 score: 0.9592 time: 0.09s
Epoch 67/1000, LR 0.000268
Train loss: 0.0711;  Loss pred: 0.0711; Loss self: 0.0000; time: 0.14s
Val loss: 0.5003 score: 0.8571 time: 0.08s
Test loss: 0.4166 score: 0.9592 time: 0.09s
Epoch 68/1000, LR 0.000268
Train loss: 0.0649;  Loss pred: 0.0649; Loss self: 0.0000; time: 0.14s
Val loss: 0.4888 score: 0.8571 time: 0.08s
Test loss: 0.4005 score: 0.9592 time: 0.09s
Epoch 69/1000, LR 0.000268
Train loss: 0.0576;  Loss pred: 0.0576; Loss self: 0.0000; time: 0.14s
Val loss: 0.4773 score: 0.8571 time: 0.08s
Test loss: 0.3842 score: 0.9592 time: 0.09s
Epoch 70/1000, LR 0.000268
Train loss: 0.0552;  Loss pred: 0.0552; Loss self: 0.0000; time: 0.14s
Val loss: 0.4659 score: 0.8776 time: 0.08s
Test loss: 0.3682 score: 0.9592 time: 0.09s
Epoch 71/1000, LR 0.000268
Train loss: 0.0511;  Loss pred: 0.0511; Loss self: 0.0000; time: 0.14s
Val loss: 0.4550 score: 0.8980 time: 0.08s
Test loss: 0.3524 score: 0.9592 time: 0.09s
Epoch 72/1000, LR 0.000267
Train loss: 0.0485;  Loss pred: 0.0485; Loss self: 0.0000; time: 0.14s
Val loss: 0.4445 score: 0.8980 time: 0.08s
Test loss: 0.3369 score: 0.9592 time: 0.09s
Epoch 73/1000, LR 0.000267
Train loss: 0.0425;  Loss pred: 0.0425; Loss self: 0.0000; time: 0.14s
Val loss: 0.4343 score: 0.8980 time: 0.08s
Test loss: 0.3216 score: 0.9592 time: 0.09s
Epoch 74/1000, LR 0.000267
Train loss: 0.0414;  Loss pred: 0.0414; Loss self: 0.0000; time: 0.14s
Val loss: 0.4247 score: 0.8980 time: 0.08s
Test loss: 0.3069 score: 0.9592 time: 0.09s
Epoch 75/1000, LR 0.000267
Train loss: 0.0360;  Loss pred: 0.0360; Loss self: 0.0000; time: 0.14s
Val loss: 0.4156 score: 0.8980 time: 0.07s
Test loss: 0.2926 score: 0.9592 time: 0.08s
Epoch 76/1000, LR 0.000267
Train loss: 0.0334;  Loss pred: 0.0334; Loss self: 0.0000; time: 0.15s
Val loss: 0.4072 score: 0.8980 time: 0.14s
Test loss: 0.2789 score: 0.9592 time: 0.09s
Epoch 77/1000, LR 0.000267
Train loss: 0.0305;  Loss pred: 0.0305; Loss self: 0.0000; time: 0.14s
Val loss: 0.3995 score: 0.8980 time: 0.07s
Test loss: 0.2658 score: 0.9592 time: 0.09s
Epoch 78/1000, LR 0.000267
Train loss: 0.0272;  Loss pred: 0.0272; Loss self: 0.0000; time: 0.13s
Val loss: 0.3927 score: 0.8980 time: 0.08s
Test loss: 0.2535 score: 0.9592 time: 0.09s
Epoch 79/1000, LR 0.000267
Train loss: 0.0254;  Loss pred: 0.0254; Loss self: 0.0000; time: 0.13s
Val loss: 0.3867 score: 0.8980 time: 0.07s
Test loss: 0.2419 score: 0.9388 time: 0.09s
Epoch 80/1000, LR 0.000267
Train loss: 0.0231;  Loss pred: 0.0231; Loss self: 0.0000; time: 0.13s
Val loss: 0.3816 score: 0.8776 time: 0.07s
Test loss: 0.2311 score: 0.9388 time: 0.09s
Epoch 81/1000, LR 0.000267
Train loss: 0.0219;  Loss pred: 0.0219; Loss self: 0.0000; time: 0.14s
Val loss: 0.3773 score: 0.8776 time: 0.08s
Test loss: 0.2213 score: 0.9388 time: 0.09s
Epoch 82/1000, LR 0.000267
Train loss: 0.0202;  Loss pred: 0.0202; Loss self: 0.0000; time: 0.14s
Val loss: 0.3737 score: 0.8776 time: 0.08s
Test loss: 0.2122 score: 0.9388 time: 0.09s
Epoch 83/1000, LR 0.000266
Train loss: 0.0186;  Loss pred: 0.0186; Loss self: 0.0000; time: 0.16s
Val loss: 0.3713 score: 0.8776 time: 0.16s
Test loss: 0.2040 score: 0.9388 time: 0.09s
Epoch 84/1000, LR 0.000266
Train loss: 0.0166;  Loss pred: 0.0166; Loss self: 0.0000; time: 0.13s
Val loss: 0.3697 score: 0.8571 time: 0.08s
Test loss: 0.1969 score: 0.9388 time: 0.09s
Epoch 85/1000, LR 0.000266
Train loss: 0.0159;  Loss pred: 0.0159; Loss self: 0.0000; time: 0.14s
Val loss: 0.3693 score: 0.8571 time: 0.08s
Test loss: 0.1908 score: 0.9388 time: 0.09s
Epoch 86/1000, LR 0.000266
Train loss: 0.0142;  Loss pred: 0.0142; Loss self: 0.0000; time: 0.13s
Val loss: 0.3702 score: 0.8571 time: 0.08s
Test loss: 0.1859 score: 0.9388 time: 0.08s
     INFO: Early stopping counter 1 of 20
Epoch 87/1000, LR 0.000266
Train loss: 0.0147;  Loss pred: 0.0147; Loss self: 0.0000; time: 0.12s
Val loss: 0.3711 score: 0.8571 time: 0.07s
Test loss: 0.1814 score: 0.9388 time: 0.08s
     INFO: Early stopping counter 2 of 20
Epoch 88/1000, LR 0.000266
Train loss: 0.0136;  Loss pred: 0.0136; Loss self: 0.0000; time: 0.13s
Val loss: 0.3733 score: 0.8571 time: 0.07s
Test loss: 0.1779 score: 0.9388 time: 0.08s
     INFO: Early stopping counter 3 of 20
Epoch 89/1000, LR 0.000266
Train loss: 0.0124;  Loss pred: 0.0124; Loss self: 0.0000; time: 0.13s
Val loss: 0.3760 score: 0.8571 time: 0.07s
Test loss: 0.1749 score: 0.9388 time: 0.08s
     INFO: Early stopping counter 4 of 20
Epoch 90/1000, LR 0.000266
Train loss: 0.0114;  Loss pred: 0.0114; Loss self: 0.0000; time: 0.13s
Val loss: 0.3792 score: 0.8571 time: 0.07s
Test loss: 0.1725 score: 0.9388 time: 0.19s
     INFO: Early stopping counter 5 of 20
Epoch 91/1000, LR 0.000266
Train loss: 0.0107;  Loss pred: 0.0107; Loss self: 0.0000; time: 0.13s
Val loss: 0.3834 score: 0.8571 time: 0.07s
Test loss: 0.1710 score: 0.9388 time: 0.08s
     INFO: Early stopping counter 6 of 20
Epoch 92/1000, LR 0.000266
Train loss: 0.0100;  Loss pred: 0.0100; Loss self: 0.0000; time: 0.13s
Val loss: 0.3874 score: 0.8571 time: 0.07s
Test loss: 0.1697 score: 0.9388 time: 0.08s
     INFO: Early stopping counter 7 of 20
Epoch 93/1000, LR 0.000265
Train loss: 0.0090;  Loss pred: 0.0090; Loss self: 0.0000; time: 0.13s
Val loss: 0.3923 score: 0.8571 time: 0.07s
Test loss: 0.1691 score: 0.9388 time: 0.08s
     INFO: Early stopping counter 8 of 20
Epoch 94/1000, LR 0.000265
Train loss: 0.0081;  Loss pred: 0.0081; Loss self: 0.0000; time: 0.13s
Val loss: 0.3978 score: 0.8571 time: 0.07s
Test loss: 0.1690 score: 0.9388 time: 0.08s
     INFO: Early stopping counter 9 of 20
Epoch 95/1000, LR 0.000265
Train loss: 0.0083;  Loss pred: 0.0083; Loss self: 0.0000; time: 0.13s
Val loss: 0.4038 score: 0.8571 time: 0.07s
Test loss: 0.1695 score: 0.9388 time: 0.08s
     INFO: Early stopping counter 10 of 20
Epoch 96/1000, LR 0.000265
Train loss: 0.0073;  Loss pred: 0.0073; Loss self: 0.0000; time: 0.13s
Val loss: 0.4101 score: 0.8571 time: 0.07s
Test loss: 0.1703 score: 0.9388 time: 0.08s
     INFO: Early stopping counter 11 of 20
Epoch 97/1000, LR 0.000265
Train loss: 0.0070;  Loss pred: 0.0070; Loss self: 0.0000; time: 0.13s
Val loss: 0.4157 score: 0.8571 time: 0.07s
Test loss: 0.1710 score: 0.9388 time: 0.18s
     INFO: Early stopping counter 12 of 20
Epoch 98/1000, LR 0.000265
Train loss: 0.0071;  Loss pred: 0.0071; Loss self: 0.0000; time: 0.13s
Val loss: 0.4221 score: 0.8571 time: 0.07s
Test loss: 0.1723 score: 0.9388 time: 0.08s
     INFO: Early stopping counter 13 of 20
Epoch 99/1000, LR 0.000265
Train loss: 0.0065;  Loss pred: 0.0065; Loss self: 0.0000; time: 0.13s
Val loss: 0.4283 score: 0.8571 time: 0.07s
Test loss: 0.1736 score: 0.9388 time: 0.08s
     INFO: Early stopping counter 14 of 20
Epoch 100/1000, LR 0.000265
Train loss: 0.0060;  Loss pred: 0.0060; Loss self: 0.0000; time: 0.13s
Val loss: 0.4348 score: 0.8571 time: 0.07s
Test loss: 0.1751 score: 0.9388 time: 0.08s
     INFO: Early stopping counter 15 of 20
Epoch 101/1000, LR 0.000265
Train loss: 0.0060;  Loss pred: 0.0060; Loss self: 0.0000; time: 0.13s
Val loss: 0.4402 score: 0.8776 time: 0.07s
Test loss: 0.1763 score: 0.9388 time: 0.08s
     INFO: Early stopping counter 16 of 20
Epoch 102/1000, LR 0.000264
Train loss: 0.0052;  Loss pred: 0.0052; Loss self: 0.0000; time: 0.13s
Val loss: 0.4462 score: 0.8571 time: 0.07s
Test loss: 0.1779 score: 0.9388 time: 0.08s
     INFO: Early stopping counter 17 of 20
Epoch 103/1000, LR 0.000264
Train loss: 0.0056;  Loss pred: 0.0056; Loss self: 0.0000; time: 0.13s
Val loss: 0.4530 score: 0.8571 time: 0.07s
Test loss: 0.1799 score: 0.9388 time: 0.08s
     INFO: Early stopping counter 18 of 20
Epoch 104/1000, LR 0.000264
Train loss: 0.0052;  Loss pred: 0.0052; Loss self: 0.0000; time: 0.14s
Val loss: 0.4593 score: 0.8571 time: 0.18s
Test loss: 0.1818 score: 0.9388 time: 0.08s
     INFO: Early stopping counter 19 of 20
Epoch 105/1000, LR 0.000264
Train loss: 0.0051;  Loss pred: 0.0051; Loss self: 0.0000; time: 0.13s
Val loss: 0.4649 score: 0.8571 time: 0.07s
Test loss: 0.1835 score: 0.9388 time: 0.08s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 084,   Train_Loss: 0.0159,   Val_Loss: 0.3693,   Val_Precision: 0.9500,   Val_Recall: 0.7600,   Val_accuracy: 0.8444,   Val_Score: 0.8571,   Val_Loss: 0.3693,   Test_Precision: 1.0000,   Test_Recall: 0.8750,   Test_accuracy: 0.9333,   Test_Score: 0.9388,   Test_loss: 0.1908


[0.07038930093403906, 0.07231412595137954, 0.0725724590010941, 0.07215185498353094, 0.07197175500914454, 0.071133290999569, 0.07403276301920414, 0.07943737599998713, 0.08081716508604586, 0.07446761301252991, 0.07468703191261739, 0.07876408193260431, 0.0851241450291127, 0.08474375796504319, 0.09266397298779339, 0.0768191940151155, 0.07193765102420002, 0.07158423902001232, 0.20037935709115118, 0.07161905895918608, 0.07122760603670031, 0.07153613690752536, 0.0770906339166686, 0.07235636503901333, 0.21730803896207362, 0.07101385295391083, 0.07000854099169374, 0.07024954294320196, 0.07071646791882813, 0.07683377398643643, 0.07672727899625897, 0.07660546607803553, 0.07612720306497067, 0.07657695806119591, 0.07689940195996314, 0.07681189698632807, 0.07681142399087548, 0.0769569700350985, 0.07440302101895213, 0.07597932091448456, 0.07646483299322426, 0.07632164901588112, 0.07787653000559658, 0.076164641068317, 0.0763582120416686, 0.07544933399185538, 0.0758136420045048, 0.07996649900451303, 0.07629981497302651, 0.07608969905413687, 0.07535254897084087, 0.19807178201153874, 0.07495569007005543, 0.07539584895130247, 0.07613797904923558, 0.07705170603003353, 0.07602654898073524, 0.07555582805071026, 0.07454343000426888, 0.07443252997472882, 0.07584675902035087, 0.07576804200652987, 0.07674941897857934, 0.07561579497996718, 0.07540330197662115, 0.07557258999440819, 0.07531619898509234, 0.0762632469413802, 0.07653705799020827, 0.07541496795602143, 0.19383304205257446, 0.07539575302507728, 0.07535506098065525, 0.07695733802393079, 0.07678640109952539, 0.07632980996277183, 0.08131813502404839, 0.07612489396706223, 0.07650837197434157, 0.07582071493379772, 0.07763806695584208, 0.07747288700193167, 0.07635092595592141, 0.07640174904372543, 0.07694882503710687, 0.07599260902497917, 0.07601368904579431, 0.07769416901282966, 0.07631841208785772, 0.18688034103251994, 0.07614671206101775, 0.07646169699728489, 0.07738347898703068, 0.07799243705812842, 0.07647392898797989, 0.20258380903396755, 0.07715395104605705, 0.07703190704341978, 0.07766890397761017, 0.07350115908775479, 0.07381051406264305, 0.08324953995179385, 0.07640224299393594, 0.07183305500075221, 0.07328689994756132, 0.07415547093842179, 0.07656635600142181, 0.15570054203271866, 0.07788999599870294, 0.07852851902134717, 0.07795114803593606, 0.07398478698451072, 0.07264676899649203, 0.072598697966896, 0.0753917460097, 0.07626780495047569, 0.07687170500867069, 0.07586873893160373, 0.07690374401863664, 0.07647379406262189, 0.08579308504704386, 0.09037044399883598, 0.09093549603130668, 0.0910068069351837, 0.09114988590590656, 0.09175811393652111, 0.09189706796314567, 0.09245196008123457, 0.09226990607567132, 0.09244920802302659, 0.092139111016877, 0.09266450302675366, 0.09251149697229266, 0.09222181502263993, 0.092554931063205, 0.09842416900210083, 0.09948955406434834, 0.09075297799427062, 0.09031287406105548, 0.09048575698398054, 0.09063777211122215, 0.09042726003099233, 0.09044508705846965, 0.09040818398352712, 0.09008386009372771, 0.09064500103704631, 0.09097535791806877, 0.0902373909484595, 0.09035301301628351, 0.09030080900993198, 0.09039484697859734, 0.09021935099735856, 0.09036342205945402, 0.09008789400104433, 0.09085515106562525, 0.09018935600761324, 0.09059131401591003, 0.09053856797982007, 0.0909006759757176, 0.09134182299021631, 0.09058253502007574, 0.09093669895082712, 0.09072876290883869, 0.090459672966972, 0.09054517897311598, 0.09039741999004036, 0.09049827605485916, 0.09059770393650979, 0.09050039190333337, 0.09048596303910017, 0.09113778790924698, 0.0908625969896093, 0.09058244689367712, 0.09054353297688067, 0.0907022199826315, 0.09075357706751674, 0.09072283701971173, 0.0903053981019184, 0.09048308408819139, 0.09076834609732032, 0.09076101297978312, 0.09091027406975627, 0.09076646505855024, 0.0904763899743557, 0.0907937060110271, 0.09086215600837022, 0.09144101105630398, 0.09137854410801083, 0.09035107493400574, 0.0907387740444392, 0.09137160598766059, 0.09124359500128776, 0.09160552907269448, 0.0904536290327087, 0.08947243995498866, 0.09119471802841872, 0.09005306696053594, 0.09026624704711139, 0.09064212301746011, 0.09309150790795684, 0.09240675694309175, 0.09122533397749066, 0.09222614299505949, 0.09054329909849912, 0.09033776598516852, 0.08215832302812487, 0.08385890908539295, 0.08362452103756368, 0.08307526505086571, 0.19865264208056033, 0.08275002194568515, 0.08288622403051704, 0.08353207097388804, 0.08522486605215818, 0.08346603496465832, 0.08345171203836799, 0.18864226003643125, 0.08280691399704665, 0.08298143604770303, 0.08283669606316835, 0.08426643803250045, 0.08361653797328472, 0.08381342492066324, 0.08522639598231763, 0.08457579696550965]
[0.0014365163455926338, 0.001475798488803664, 0.0014810705918590634, 0.0014724868363985906, 0.0014688113267172355, 0.0014516998163177346, 0.0015108727146776356, 0.0016211709387752475, 0.0016493298997152218, 0.0015197472043373451, 0.0015242251410738242, 0.001607430243522537, 0.0017372274495737285, 0.0017294644482661874, 0.0018911014895468038, 0.001567738653369704, 0.0014681153270244903, 0.0014609028371431086, 0.004089374634513289, 0.0014616134481466546, 0.0014536246129938839, 0.0014599211613780686, 0.0015732782431973182, 0.001476660511000272, 0.004434857938001502, 0.0014492623051818535, 0.0014287457345243618, 0.0014336641416979991, 0.001443193222833227, 0.001568036203804825, 0.0015658628366583464, 0.0015633768587354191, 0.0015536163890810342, 0.001562795062473386, 0.0015693755502033295, 0.0015675897344148584, 0.0015675800814464384, 0.001570550408879561, 0.0015184290003867782, 0.001550598386009889, 0.001560506795780087, 0.0015575846737934922, 0.001589316938889726, 0.0015543804299656529, 0.0015583308579932367, 0.0015397823263643955, 0.001547217183765404, 0.0016319693674390413, 0.0015571390810821737, 0.0015528510011048342, 0.0015378071218538954, 0.004042281265541607, 0.0015297079606133761, 0.0015386907949245401, 0.0015538363071272568, 0.0015724837965312966, 0.0015515622240966376, 0.001541955674504291, 0.0015212944898830385, 0.0015190312239740575, 0.0015478930412316505, 0.0015462865715618341, 0.0015663146730322316, 0.0015431794893870853, 0.0015388428974820643, 0.0015422977549879222, 0.0015370652854100478, 0.0015563927947220449, 0.001561980775310373, 0.0015390809786943148, 0.003955776368419887, 0.0015386888372464751, 0.0015378583873603114, 0.0015705579188557304, 0.0015670694101943957, 0.0015577512237300373, 0.0016595537760009875, 0.0015535692646339232, 0.0015613953464151341, 0.001547361529261178, 0.0015844503460375934, 0.001581079326570034, 0.001558182162365743, 0.0015592193682392945, 0.0015703841844307525, 0.0015508695719383505, 0.0015512997764447819, 0.0015855952859761156, 0.0015575186140379127, 0.003813884510867754, 0.0015540145318575051, 0.001560442795862957, 0.001579254673204708, 0.0015916823889413963, 0.0015606924283261203, 0.004134363449672807, 0.0015745704295113683, 0.0015720797355799954, 0.0015850796730124525, 0.0015000236548521385, 0.0015063370216865928, 0.0016989702030978336, 0.0015592294488558356, 0.0014659807143010655, 0.0014956510193379863, 0.0015133769579269752, 0.0015625786939065676, 0.0031775620823003806, 0.0015895917550755702, 0.0016026228371703503, 0.0015908397558354298, 0.0015098936119287902, 0.0014825871223773882, 0.0014816060809570613, 0.001538607061422449, 0.0015564858153158305, 0.0015688103062994018, 0.0015483416108490558, 0.0015694641636456459, 0.0015606896747473854, 0.0017508792866743645, 0.0018442947754864485, 0.0018558264496185038, 0.0018572817741874226, 0.0018602017531817664, 0.0018726145701330838, 0.0018754503665948097, 0.0018867746955353994, 0.0018830593076667615, 0.0018867185310821753, 0.0018803900207525917, 0.001891112306668442, 0.0018879897341284218, 0.0018820778576048966, 0.001888876144147041, 0.0020086565102469555, 0.002030399062537721, 0.0018521015917198087, 0.0018431198787970506, 0.0018466481017138884, 0.0018497504512494315, 0.0018454542863467823, 0.0018458181032340747, 0.0018450649792556555, 0.00183844612436179, 0.001849897980347884, 0.0018566399575116075, 0.0018415794071114185, 0.001843939041148643, 0.001842873653263918, 0.0018447927954815784, 0.0018412112448440523, 0.0018441514706011026, 0.0018385284490009047, 0.0018541867564413317, 0.0018405991021961886, 0.0018488023268553068, 0.0018477258771391852, 0.0018551158362391349, 0.001864118836535027, 0.0018486231636750151, 0.001855850998996472, 0.0018516074063028305, 0.001846115774836163, 0.0018478607953697139, 0.0018448453059191911, 0.0018469035929563095, 0.001848932733398159, 0.0018469467735374157, 0.0018466523069204117, 0.0018599548552907547, 0.0018543387140736592, 0.001848621365177084, 0.0018478272036098096, 0.001851065713931255, 0.0018521138177044233, 0.0018514864697900353, 0.0018429673082024163, 0.0018465935528202324, 0.0018524152264759249, 0.001852265571015982, 0.0018553117157093116, 0.0018523768379295968, 0.0018464569382521572, 0.001852932775735247, 0.0018543297144565352, 0.0018661430827817138, 0.0018648682471022618, 0.0018438994884490967, 0.0018518117151926367, 0.0018647266528093998, 0.00186211418369975, 0.0018695005933202955, 0.001845992429238953, 0.0018259681623467073, 0.0018611166944575248, 0.0018378176930721622, 0.001842168307083906, 0.0018498392452542878, 0.001899826691999119, 0.0018858521825120766, 0.0018617415097447075, 0.0018821661835726426, 0.0018478224305816147, 0.0018436278772483371, 0.001676700469961732, 0.0017114063078651624, 0.0017066228783176262, 0.0016954135724666472, 0.004054135552664496, 0.001688775958075207, 0.0016915555924595315, 0.0017047361423242458, 0.0017392829806562892, 0.0017033884686664964, 0.0017030961640483262, 0.0038498420415598216, 0.0016899370203478908, 0.0016934986948510821, 0.0016905448176156804, 0.0017197232251530703, 0.0017064599586384638, 0.0017104780596053722, 0.001739314203720768, 0.001726036672765503]
[696.1285216615136, 677.599284446101, 675.1872635218448, 679.1232188165435, 680.8226365158691, 688.8476451946656, 661.8691239078754, 616.8380989826243, 606.3068402341236, 658.0041714477308, 656.0710573868996, 622.1109774621331, 575.6298636919274, 578.2136782299943, 528.7923496055448, 637.8614176862934, 681.1453988609701, 684.5082195579589, 244.53616735435597, 684.1754235827628, 687.9355172312341, 684.9684945014883, 635.6154763621056, 677.203725941457, 225.48636596252143, 690.0062165589271, 699.9146005030111, 697.51343492181, 692.9079101666197, 637.7403771504188, 638.6255402382915, 639.6410401065281, 643.6595333494784, 639.8791652293372, 637.196112728046, 637.9220136787095, 637.9259419252631, 636.7194547505197, 658.575409021612, 644.9123183813391, 640.8174592409299, 642.0196711132907, 629.2011212682258, 643.3431486409648, 641.7122492766168, 649.4424457781093, 646.3216738365959, 612.7565994509102, 642.2033922011798, 643.9767880424538, 650.2766086779825, 247.38506162955352, 653.7195502329896, 649.9031535761163, 643.5684347270832, 635.9366005588582, 644.5116956764191, 648.5270728171098, 657.3349253877091, 658.3143152145491, 646.0394700168076, 646.7106540218773, 638.4413152844302, 648.0127599396598, 649.838915743935, 648.3832299994699, 650.5904527882345, 642.5113270834624, 640.2127451288864, 649.7383918345569, 252.79487687506574, 649.9039804496969, 650.2549312856241, 636.7164101331425, 638.1338270625482, 641.9510283583656, 602.5716156120541, 643.6790574867841, 640.4527862183894, 646.2613817712479, 631.1336940919659, 632.4793343350979, 641.7734871779875, 641.3465740418698, 636.7868512140482, 644.7995486494409, 644.6207336481201, 630.6779597823952, 642.0469013898144, 262.1998639839451, 643.4946260153085, 640.8437416938309, 633.2100939557434, 628.2660453792448, 640.7412388567323, 241.87520332280894, 635.0938524295334, 636.100051013675, 630.8831139695928, 666.6561535648401, 663.8620611477337, 588.5918412086571, 641.3424276547632, 682.1372138423862, 668.6051672953935, 660.7739035288344, 639.9677685991754, 314.70667577832336, 629.0923419846622, 623.9771309921158, 628.5988241944896, 662.2983183050662, 674.4966180445839, 674.9432341382117, 649.9385223641802, 642.4729285419717, 637.4256951172487, 645.852306101646, 637.1601360282989, 640.7423693386456, 571.1416016003072, 542.2126729910849, 538.8434894898533, 538.4212637511662, 537.5760980170878, 534.0127199421136, 533.2052598201601, 530.0049880710509, 531.0507193950614, 530.020765432576, 531.8045665865469, 528.7893249247012, 529.6638969605646, 531.327647238029, 529.4153367856576, 497.84519896687283, 492.51401778630486, 539.9271856742095, 542.5583064367306, 541.5216895259537, 540.6134645484425, 541.8719972628401, 541.7651924899268, 541.9863317786371, 543.93761489592, 540.5703507022286, 538.6073890924262, 543.0121536646279, 542.3172771357295, 542.6307974119103, 542.0662973366353, 543.120732507094, 542.2548071249533, 543.9132587496381, 539.319999199682, 543.3013624785581, 540.8907082570236, 541.2058208267829, 539.0498967586273, 536.446486351038, 540.9431298112839, 538.8363616156348, 540.0712897323818, 541.6778371273854, 541.1663056577394, 542.0508683256517, 541.4467781717376, 540.8525588500437, 541.4341194493236, 541.5204563698621, 537.6474580312738, 539.2758035036512, 540.9436560873067, 541.1761435519821, 540.2293351737474, 539.9236215620032, 540.1065664354562, 542.6032222868754, 541.5376862291855, 539.8357699220719, 539.8793864378164, 538.992985131712, 539.846957446143, 541.577753200458, 539.6849864686528, 539.2784207705364, 535.8645911059392, 536.2309115155223, 542.3289101517673, 540.0117041034995, 536.2716291384577, 537.02399603291, 534.9022105545132, 541.7140309791356, 547.6546747205136, 537.3118208965818, 544.1236112643818, 542.838564833942, 540.5875145991592, 526.3638016095753, 530.2642536213712, 537.1314947675661, 531.3027131864867, 541.1775414400848, 542.408808383027, 596.4094469555575, 584.3147798417415, 585.9525339223105, 589.8265864093007, 246.66170802867475, 592.1448580661678, 591.1718210490466, 586.601043511986, 574.9495689440178, 587.0651459692301, 587.1659047266954, 259.75091684406743, 591.7380280799694, 590.4935167888837, 591.5252820155251, 581.4889194806283, 586.0084761659874, 584.63187784514, 574.9392478143308, 579.3619659295956]
Elapsed: 0.08695056382721911~0.023207725669562182
Time per graph: 0.0017745013025963082~0.0004736270544808608
Speed: 585.1359921805897~86.22121368060957
Total Time: 0.0853
best val loss: 0.36927783489227295 test_score: 0.9388

Testing...
Test loss: 0.3524 score: 0.9592 time: 0.08s
test Score 0.9592
Epoch Time List: [0.45147583296056837, 0.26700617792084813, 0.2705245789838955, 0.2711127110524103, 0.27711690904106945, 0.2702163300709799, 0.27937917900271714, 0.28489178407471627, 0.2999167019734159, 0.2868298531975597, 0.2871128509286791, 0.2855395779479295, 0.44470822892617434, 0.31885680998675525, 0.324563693953678, 0.29417327092960477, 0.2725801628548652, 0.2676425928948447, 0.3972845111275092, 0.26695142698008567, 0.2662792840274051, 0.2666865369537845, 0.277162668062374, 0.27496886102017015, 0.4173425429034978, 0.2752722700824961, 0.2665053380187601, 0.26860476413276047, 0.26250575797166675, 0.2812036970863119, 0.2835975360358134, 0.3678049040026963, 0.2866439529461786, 0.287601264892146, 0.28932875604368746, 0.2837535480502993, 0.2985356649151072, 0.2881823137868196, 0.3624461689032614, 0.28406377404462546, 0.28851149301044643, 0.28746576188132167, 0.2956457460531965, 0.2885552739026025, 0.2879637099104002, 0.41320351511240005, 0.2842101859860122, 0.2875938209472224, 0.2859216219512746, 0.2871118780458346, 0.2851932880003005, 0.4072667279979214, 0.28253552014939487, 0.28300272312480956, 0.2832406460074708, 0.28523488296195865, 0.28730510897003114, 0.28324073110707104, 0.3884441270492971, 0.2819499270990491, 0.2824398059165105, 0.28287428012117743, 0.28774206805974245, 0.28375966497696936, 0.4180663110455498, 0.28299058601260185, 0.28291781386360526, 0.28564787609502673, 0.29045996197964996, 0.28723464091308415, 0.40690262196585536, 0.2868193540489301, 0.2838244130834937, 0.28772560006473213, 0.28868449793662876, 0.29079251806251705, 0.2913852819474414, 0.40228643105365336, 0.28747263306286186, 0.2831268550362438, 0.2976439918857068, 0.2932638900820166, 0.28915290790610015, 0.3736255979165435, 0.28906135878060013, 0.28850617504213005, 0.28850121598225087, 0.29476363002322614, 0.28933205106295645, 0.3978961770189926, 0.28710500698070973, 0.2878032550215721, 0.28616605408024043, 0.28991878498345613, 0.29278348200023174, 0.41326879896223545, 0.2909319770988077, 0.2917484169593081, 0.29298111516982317, 0.2844532928429544, 0.2840541630284861, 0.34133313491474837, 0.2865197039209306, 0.27373663999605924, 0.27867330296430737, 0.2824410990579054, 0.2821520739234984, 0.36133375903591514, 0.3029205030761659, 0.293214876903221, 0.29662124090828, 0.2935400160495192, 0.28036419104319066, 0.29125547921285033, 0.36514891509432346, 0.29096432495862246, 0.2919218259630725, 0.2897517828969285, 0.2936396269360557, 0.29289508203510195, 0.2826005860697478, 0.28743940696585923, 0.29652998014353216, 0.30400335299782455, 0.29844979592598975, 0.30016466695815325, 0.3012407519854605, 0.30239429813809693, 0.30148344801273197, 0.30194804701022804, 0.30144284083507955, 0.30254149506799877, 0.3029746660031378, 0.30241608701180667, 0.30229293496813625, 0.3193141930969432, 0.3298436211189255, 0.29677369201090187, 0.2965917319525033, 0.2957478610333055, 0.29567148513160646, 0.2957335851388052, 0.29651935305446386, 0.2963665509596467, 0.29686583403963596, 0.2966286690207198, 0.29580466914922, 0.2952043181285262, 0.2957309200428426, 0.29576373810414225, 0.29659200308378786, 0.2954909299733117, 0.2959481660509482, 0.2959600929170847, 0.2962046960601583, 0.29590918496251106, 0.2957634360063821, 0.29645485093351454, 0.2972864379407838, 0.29657549189869314, 0.29756798909511417, 0.297466016956605, 0.2971904579317197, 0.2962828528834507, 0.29657642904203385, 0.29556276206858456, 0.29706864594481885, 0.2965014480287209, 0.29594809492118657, 0.296248781029135, 0.29746524791698903, 0.29685483896173537, 0.2975317819509655, 0.29746704001445323, 0.2974579121218994, 0.2965142489410937, 0.2974973830860108, 0.29573706490918994, 0.29727659502532333, 0.29753154690843076, 0.296747526852414, 0.29700377688277513, 0.29800810280721635, 0.29720728809479624, 0.29672953102272004, 0.297922280151397, 0.29906631784979254, 0.2982006308156997, 0.29721233097370714, 0.29732867202255875, 0.2984511989634484, 0.29941791913006455, 0.2980462070554495, 0.29950866394210607, 0.294084774912335, 0.37377614015713334, 0.29474796704016626, 0.29554515494965017, 0.2941433299565688, 0.29705605702474713, 0.3049561660736799, 0.29911105800420046, 0.4028147298377007, 0.294825367978774, 0.2972231289604679, 0.28777029807679355, 0.2716270770179108, 0.2779494959395379, 0.27651189896278083, 0.39183665707241744, 0.2761584228137508, 0.27461879805196077, 0.2760068450588733, 0.27891117706894875, 0.27770994103047997, 0.2769271908327937, 0.3826886130264029, 0.276231189025566, 0.27394983102567494, 0.27348406112287194, 0.2767295320518315, 0.2788399230921641, 0.2777145359432325, 0.3990128340665251, 0.2816809481009841]
Total Epoch List: [120, 105]
Total Time List: [0.07703959301579744, 0.08533975598402321]
========================k_idx:2========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: I2BGNNA
I2BGNN(
  (gcs): ModuleList(
    (0): GCNConv(14887, 64)
    (1): GCNConv(64, 64)
  )
  (bns): ModuleList(
    (0-1): 2 x BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  )
  (drops): ModuleList(
    (0-1): 2 x Dropout(p=0.2, inplace=False)
  )
  (lin1): Linear(in_features=64, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=64, bias=True)
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=64, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=64, out_features=2, bias=True)
  )
)
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x759998ba4580>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.7022;  Loss pred: 0.7022; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6930 score: 0.5102 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6934 score: 0.5000 time: 0.22s
Epoch 2/1000, LR 0.000000
Train loss: 0.7041;  Loss pred: 0.7041; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6930 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6934 score: 0.5000 time: 0.08s
     INFO: Early stopping counter 1 of 20
Epoch 3/1000, LR 0.000030
Train loss: 0.7053;  Loss pred: 0.7053; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6930 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6934 score: 0.5000 time: 0.08s
     INFO: Early stopping counter 2 of 20
Epoch 4/1000, LR 0.000060
Train loss: 0.7028;  Loss pred: 0.7028; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6930 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6934 score: 0.5000 time: 0.08s
     INFO: Early stopping counter 3 of 20
Epoch 5/1000, LR 0.000090
Train loss: 0.6998;  Loss pred: 0.6998; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6929 score: 0.5102 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6934 score: 0.5000 time: 0.08s
Epoch 6/1000, LR 0.000120
Train loss: 0.6978;  Loss pred: 0.6978; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6929 score: 0.5102 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6934 score: 0.5000 time: 0.08s
Epoch 7/1000, LR 0.000150
Train loss: 0.6952;  Loss pred: 0.6952; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6929 score: 0.5102 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6933 score: 0.5000 time: 0.08s
Epoch 8/1000, LR 0.000180
Train loss: 0.6930;  Loss pred: 0.6930; Loss self: 0.0000; time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6929 score: 0.5102 time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6933 score: 0.5000 time: 0.08s
Epoch 9/1000, LR 0.000210
Train loss: 0.6897;  Loss pred: 0.6897; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6929 score: 0.5102 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6933 score: 0.5000 time: 0.08s
Epoch 10/1000, LR 0.000240
Train loss: 0.6871;  Loss pred: 0.6871; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6929 score: 0.5102 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6932 score: 0.5000 time: 0.08s
Epoch 11/1000, LR 0.000270
Train loss: 0.6838;  Loss pred: 0.6838; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6929 score: 0.5102 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6932 score: 0.5000 time: 0.08s
Epoch 12/1000, LR 0.000270
Train loss: 0.6796;  Loss pred: 0.6796; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6929 score: 0.5102 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6932 score: 0.5000 time: 0.08s
Epoch 13/1000, LR 0.000270
Train loss: 0.6764;  Loss pred: 0.6764; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6929 score: 0.5102 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6931 score: 0.5000 time: 0.08s
Epoch 14/1000, LR 0.000270
Train loss: 0.6737;  Loss pred: 0.6737; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6929 score: 0.5102 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6931 score: 0.5000 time: 0.08s
Epoch 15/1000, LR 0.000270
Train loss: 0.6706;  Loss pred: 0.6706; Loss self: 0.0000; time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6929 score: 0.5102 time: 0.10s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6931 score: 0.5000 time: 0.08s
     INFO: Early stopping counter 1 of 20
Epoch 16/1000, LR 0.000270
Train loss: 0.6676;  Loss pred: 0.6676; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6929 score: 0.5102 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6931 score: 0.5000 time: 0.08s
     INFO: Early stopping counter 2 of 20
Epoch 17/1000, LR 0.000270
Train loss: 0.6636;  Loss pred: 0.6636; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6929 score: 0.5102 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6930 score: 0.5000 time: 0.08s
     INFO: Early stopping counter 3 of 20
Epoch 18/1000, LR 0.000270
Train loss: 0.6604;  Loss pred: 0.6604; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6929 score: 0.5102 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6930 score: 0.5000 time: 0.08s
Epoch 19/1000, LR 0.000270
Train loss: 0.6540;  Loss pred: 0.6540; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6929 score: 0.5102 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6929 score: 0.5000 time: 0.08s
Epoch 20/1000, LR 0.000270
Train loss: 0.6510;  Loss pred: 0.6510; Loss self: 0.0000; time: 0.13s
Val loss: 0.6929 score: 0.5306 time: 0.08s
Test loss: 0.6929 score: 0.5208 time: 0.08s
Epoch 21/1000, LR 0.000270
Train loss: 0.6444;  Loss pred: 0.6444; Loss self: 0.0000; time: 0.13s
Val loss: 0.6929 score: 0.5102 time: 0.07s
Test loss: 0.6928 score: 0.5417 time: 0.08s
Epoch 22/1000, LR 0.000270
Train loss: 0.6405;  Loss pred: 0.6405; Loss self: 0.0000; time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6928 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6928 score: 0.5000 time: 0.08s
Epoch 23/1000, LR 0.000270
Train loss: 0.6349;  Loss pred: 0.6349; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6928 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6927 score: 0.5000 time: 0.08s
Epoch 24/1000, LR 0.000270
Train loss: 0.6297;  Loss pred: 0.6297; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6927 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6926 score: 0.5000 time: 0.08s
Epoch 25/1000, LR 0.000270
Train loss: 0.6181;  Loss pred: 0.6181; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6926 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6924 score: 0.5000 time: 0.08s
Epoch 26/1000, LR 0.000270
Train loss: 0.6118;  Loss pred: 0.6118; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6924 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6923 score: 0.5000 time: 0.08s
Epoch 27/1000, LR 0.000270
Train loss: 0.6069;  Loss pred: 0.6069; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6923 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6921 score: 0.5000 time: 0.08s
Epoch 28/1000, LR 0.000270
Train loss: 0.5986;  Loss pred: 0.5986; Loss self: 0.0000; time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6921 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6919 score: 0.5000 time: 0.08s
Epoch 29/1000, LR 0.000270
Train loss: 0.5898;  Loss pred: 0.5898; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6919 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6916 score: 0.5000 time: 0.07s
Epoch 30/1000, LR 0.000270
Train loss: 0.5759;  Loss pred: 0.5759; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6917 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6913 score: 0.5000 time: 0.07s
Epoch 31/1000, LR 0.000270
Train loss: 0.5653;  Loss pred: 0.5653; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6914 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6910 score: 0.5000 time: 0.07s
Epoch 32/1000, LR 0.000270
Train loss: 0.5536;  Loss pred: 0.5536; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6910 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6906 score: 0.5000 time: 0.08s
Epoch 33/1000, LR 0.000270
Train loss: 0.5446;  Loss pred: 0.5446; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6905 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6900 score: 0.5000 time: 0.07s
Epoch 34/1000, LR 0.000270
Train loss: 0.5323;  Loss pred: 0.5323; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6900 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6895 score: 0.5000 time: 0.07s
Epoch 35/1000, LR 0.000270
Train loss: 0.5205;  Loss pred: 0.5205; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6894 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6888 score: 0.5000 time: 0.08s
Epoch 36/1000, LR 0.000270
Train loss: 0.5082;  Loss pred: 0.5082; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6888 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6881 score: 0.5000 time: 0.08s
Epoch 37/1000, LR 0.000270
Train loss: 0.4960;  Loss pred: 0.4960; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6880 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6871 score: 0.5000 time: 0.08s
Epoch 38/1000, LR 0.000270
Train loss: 0.4851;  Loss pred: 0.4851; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6870 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6861 score: 0.5000 time: 0.08s
Epoch 39/1000, LR 0.000269
Train loss: 0.4752;  Loss pred: 0.4752; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6860 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6849 score: 0.5000 time: 0.08s
Epoch 40/1000, LR 0.000269
Train loss: 0.4550;  Loss pred: 0.4550; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6849 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6836 score: 0.5000 time: 0.08s
Epoch 41/1000, LR 0.000269
Train loss: 0.4407;  Loss pred: 0.4407; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6838 score: 0.4898 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6823 score: 0.5000 time: 0.08s
Epoch 42/1000, LR 0.000269
Train loss: 0.4277;  Loss pred: 0.4277; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6824 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6807 score: 0.5000 time: 0.08s
Epoch 43/1000, LR 0.000269
Train loss: 0.4139;  Loss pred: 0.4139; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6809 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6788 score: 0.5000 time: 0.08s
Epoch 44/1000, LR 0.000269
Train loss: 0.3964;  Loss pred: 0.3964; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6793 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6769 score: 0.5000 time: 0.08s
Epoch 45/1000, LR 0.000269
Train loss: 0.3817;  Loss pred: 0.3817; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6776 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6748 score: 0.5000 time: 0.08s
Epoch 46/1000, LR 0.000269
Train loss: 0.3602;  Loss pred: 0.3602; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6756 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6725 score: 0.5000 time: 0.08s
Epoch 47/1000, LR 0.000269
Train loss: 0.3506;  Loss pred: 0.3506; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6735 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6698 score: 0.5000 time: 0.08s
Epoch 48/1000, LR 0.000269
Train loss: 0.3345;  Loss pred: 0.3345; Loss self: 0.0000; time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6711 score: 0.4898 time: 0.09s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6669 score: 0.5000 time: 0.08s
Epoch 49/1000, LR 0.000269
Train loss: 0.3189;  Loss pred: 0.3189; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6685 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6636 score: 0.5000 time: 0.08s
Epoch 50/1000, LR 0.000269
Train loss: 0.3097;  Loss pred: 0.3097; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6656 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6600 score: 0.5000 time: 0.08s
Epoch 51/1000, LR 0.000269
Train loss: 0.2893;  Loss pred: 0.2893; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6625 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6561 score: 0.5000 time: 0.08s
Epoch 52/1000, LR 0.000269
Train loss: 0.2752;  Loss pred: 0.2752; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6590 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6518 score: 0.5000 time: 0.08s
Epoch 53/1000, LR 0.000269
Train loss: 0.2652;  Loss pred: 0.2652; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6553 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6472 score: 0.5000 time: 0.08s
Epoch 54/1000, LR 0.000269
Train loss: 0.2434;  Loss pred: 0.2434; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6513 score: 0.4898 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6421 score: 0.5000 time: 0.08s
Epoch 55/1000, LR 0.000269
Train loss: 0.2380;  Loss pred: 0.2380; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6470 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6367 score: 0.5000 time: 0.08s
Epoch 56/1000, LR 0.000269
Train loss: 0.2137;  Loss pred: 0.2137; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6423 score: 0.4898 time: 0.08s
Test loss: 0.6307 score: 0.5208 time: 0.08s
Epoch 57/1000, LR 0.000269
Train loss: 0.2020;  Loss pred: 0.2020; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6374 score: 0.4898 time: 0.08s
Test loss: 0.6243 score: 0.5417 time: 0.08s
Epoch 58/1000, LR 0.000269
Train loss: 0.1851;  Loss pred: 0.1851; Loss self: 0.0000; time: 0.14s
Val loss: 0.6323 score: 0.5102 time: 0.08s
Test loss: 0.6176 score: 0.5417 time: 0.08s
Epoch 59/1000, LR 0.000268
Train loss: 0.1753;  Loss pred: 0.1753; Loss self: 0.0000; time: 0.14s
Val loss: 0.6268 score: 0.5102 time: 0.08s
Test loss: 0.6106 score: 0.5625 time: 0.08s
Epoch 60/1000, LR 0.000268
Train loss: 0.1671;  Loss pred: 0.1671; Loss self: 0.0000; time: 0.16s
Val loss: 0.6211 score: 0.5102 time: 0.12s
Test loss: 0.6031 score: 0.5625 time: 0.08s
Epoch 61/1000, LR 0.000268
Train loss: 0.1536;  Loss pred: 0.1536; Loss self: 0.0000; time: 0.14s
Val loss: 0.6154 score: 0.5102 time: 0.08s
Test loss: 0.5954 score: 0.5625 time: 0.08s
Epoch 62/1000, LR 0.000268
Train loss: 0.1400;  Loss pred: 0.1400; Loss self: 0.0000; time: 0.14s
Val loss: 0.6094 score: 0.5306 time: 0.08s
Test loss: 0.5874 score: 0.5625 time: 0.08s
Epoch 63/1000, LR 0.000268
Train loss: 0.1306;  Loss pred: 0.1306; Loss self: 0.0000; time: 0.14s
Val loss: 0.6032 score: 0.5714 time: 0.08s
Test loss: 0.5790 score: 0.6042 time: 0.08s
Epoch 64/1000, LR 0.000268
Train loss: 0.1199;  Loss pred: 0.1199; Loss self: 0.0000; time: 0.14s
Val loss: 0.5969 score: 0.5918 time: 0.08s
Test loss: 0.5703 score: 0.6250 time: 0.08s
Epoch 65/1000, LR 0.000268
Train loss: 0.1101;  Loss pred: 0.1101; Loss self: 0.0000; time: 0.14s
Val loss: 0.5900 score: 0.6122 time: 0.08s
Test loss: 0.5611 score: 0.6458 time: 0.08s
Epoch 66/1000, LR 0.000268
Train loss: 0.1023;  Loss pred: 0.1023; Loss self: 0.0000; time: 0.15s
Val loss: 0.5830 score: 0.6327 time: 0.11s
Test loss: 0.5515 score: 0.6458 time: 0.08s
Epoch 67/1000, LR 0.000268
Train loss: 0.0949;  Loss pred: 0.0949; Loss self: 0.0000; time: 0.14s
Val loss: 0.5756 score: 0.6531 time: 0.08s
Test loss: 0.5415 score: 0.6875 time: 0.08s
Epoch 68/1000, LR 0.000268
Train loss: 0.0836;  Loss pred: 0.0836; Loss self: 0.0000; time: 0.14s
Val loss: 0.5673 score: 0.6939 time: 0.08s
Test loss: 0.5306 score: 0.7292 time: 0.08s
Epoch 69/1000, LR 0.000268
Train loss: 0.0801;  Loss pred: 0.0801; Loss self: 0.0000; time: 0.14s
Val loss: 0.5591 score: 0.6939 time: 0.08s
Test loss: 0.5196 score: 0.7500 time: 0.08s
Epoch 70/1000, LR 0.000268
Train loss: 0.0701;  Loss pred: 0.0701; Loss self: 0.0000; time: 0.14s
Val loss: 0.5507 score: 0.7143 time: 0.08s
Test loss: 0.5083 score: 0.7500 time: 0.08s
Epoch 71/1000, LR 0.000268
Train loss: 0.0648;  Loss pred: 0.0648; Loss self: 0.0000; time: 0.14s
Val loss: 0.5426 score: 0.7143 time: 0.08s
Test loss: 0.4973 score: 0.8125 time: 0.08s
Epoch 72/1000, LR 0.000267
Train loss: 0.0583;  Loss pred: 0.0583; Loss self: 0.0000; time: 0.14s
Val loss: 0.5340 score: 0.7143 time: 0.17s
Test loss: 0.4859 score: 0.8125 time: 0.08s
Epoch 73/1000, LR 0.000267
Train loss: 0.0522;  Loss pred: 0.0522; Loss self: 0.0000; time: 0.13s
Val loss: 0.5254 score: 0.7143 time: 0.08s
Test loss: 0.4744 score: 0.8125 time: 0.08s
Epoch 74/1000, LR 0.000267
Train loss: 0.0484;  Loss pred: 0.0484; Loss self: 0.0000; time: 0.13s
Val loss: 0.5167 score: 0.7347 time: 0.08s
Test loss: 0.4627 score: 0.8333 time: 0.08s
Epoch 75/1000, LR 0.000267
Train loss: 0.0448;  Loss pred: 0.0448; Loss self: 0.0000; time: 0.13s
Val loss: 0.5074 score: 0.7551 time: 0.08s
Test loss: 0.4502 score: 0.8333 time: 0.08s
Epoch 76/1000, LR 0.000267
Train loss: 0.0420;  Loss pred: 0.0420; Loss self: 0.0000; time: 0.13s
Val loss: 0.4982 score: 0.7551 time: 0.08s
Test loss: 0.4379 score: 0.8542 time: 0.08s
Epoch 77/1000, LR 0.000267
Train loss: 0.0359;  Loss pred: 0.0359; Loss self: 0.0000; time: 0.13s
Val loss: 0.4890 score: 0.7959 time: 0.08s
Test loss: 0.4256 score: 0.8542 time: 0.08s
Epoch 78/1000, LR 0.000267
Train loss: 0.0333;  Loss pred: 0.0333; Loss self: 0.0000; time: 0.13s
Val loss: 0.4801 score: 0.7959 time: 0.08s
Test loss: 0.4138 score: 0.8542 time: 0.08s
Epoch 79/1000, LR 0.000267
Train loss: 0.0308;  Loss pred: 0.0308; Loss self: 0.0000; time: 0.13s
Val loss: 0.4719 score: 0.8163 time: 0.08s
Test loss: 0.4025 score: 0.8542 time: 0.08s
Epoch 80/1000, LR 0.000267
Train loss: 0.0288;  Loss pred: 0.0288; Loss self: 0.0000; time: 0.14s
Val loss: 0.4632 score: 0.8163 time: 0.08s
Test loss: 0.3910 score: 0.8542 time: 0.08s
Epoch 81/1000, LR 0.000267
Train loss: 0.0263;  Loss pred: 0.0263; Loss self: 0.0000; time: 0.13s
Val loss: 0.4552 score: 0.8367 time: 0.08s
Test loss: 0.3801 score: 0.8542 time: 0.08s
Epoch 82/1000, LR 0.000267
Train loss: 0.0243;  Loss pred: 0.0243; Loss self: 0.0000; time: 0.13s
Val loss: 0.4480 score: 0.8367 time: 0.08s
Test loss: 0.3700 score: 0.8750 time: 0.08s
Epoch 83/1000, LR 0.000266
Train loss: 0.0214;  Loss pred: 0.0214; Loss self: 0.0000; time: 0.13s
Val loss: 0.4413 score: 0.8367 time: 0.08s
Test loss: 0.3602 score: 0.8750 time: 0.08s
Epoch 84/1000, LR 0.000266
Train loss: 0.0214;  Loss pred: 0.0214; Loss self: 0.0000; time: 0.14s
Val loss: 0.4354 score: 0.8367 time: 0.08s
Test loss: 0.3512 score: 0.8958 time: 0.08s
Epoch 85/1000, LR 0.000266
Train loss: 0.0181;  Loss pred: 0.0181; Loss self: 0.0000; time: 0.13s
Val loss: 0.4296 score: 0.8367 time: 0.08s
Test loss: 0.3425 score: 0.8958 time: 0.08s
Epoch 86/1000, LR 0.000266
Train loss: 0.0173;  Loss pred: 0.0173; Loss self: 0.0000; time: 0.14s
Val loss: 0.4247 score: 0.8571 time: 0.08s
Test loss: 0.3347 score: 0.8958 time: 0.08s
Epoch 87/1000, LR 0.000266
Train loss: 0.0157;  Loss pred: 0.0157; Loss self: 0.0000; time: 0.13s
Val loss: 0.4209 score: 0.8571 time: 0.08s
Test loss: 0.3279 score: 0.8958 time: 0.08s
Epoch 88/1000, LR 0.000266
Train loss: 0.0139;  Loss pred: 0.0139; Loss self: 0.0000; time: 0.13s
Val loss: 0.4172 score: 0.8571 time: 0.08s
Test loss: 0.3212 score: 0.8958 time: 0.08s
Epoch 89/1000, LR 0.000266
Train loss: 0.0135;  Loss pred: 0.0135; Loss self: 0.0000; time: 0.13s
Val loss: 0.4147 score: 0.8571 time: 0.08s
Test loss: 0.3157 score: 0.8958 time: 0.08s
Epoch 90/1000, LR 0.000266
Train loss: 0.0127;  Loss pred: 0.0127; Loss self: 0.0000; time: 0.14s
Val loss: 0.4124 score: 0.8571 time: 0.08s
Test loss: 0.3105 score: 0.8958 time: 0.08s
Epoch 91/1000, LR 0.000266
Train loss: 0.0116;  Loss pred: 0.0116; Loss self: 0.0000; time: 0.14s
Val loss: 0.4111 score: 0.8571 time: 0.08s
Test loss: 0.3062 score: 0.8958 time: 0.08s
Epoch 92/1000, LR 0.000266
Train loss: 0.0110;  Loss pred: 0.0110; Loss self: 0.0000; time: 0.13s
Val loss: 0.4102 score: 0.8571 time: 0.08s
Test loss: 0.3025 score: 0.8958 time: 0.08s
Epoch 93/1000, LR 0.000265
Train loss: 0.0105;  Loss pred: 0.0105; Loss self: 0.0000; time: 0.14s
Val loss: 0.4094 score: 0.8367 time: 0.08s
Test loss: 0.2991 score: 0.8958 time: 0.08s
Epoch 94/1000, LR 0.000265
Train loss: 0.0092;  Loss pred: 0.0092; Loss self: 0.0000; time: 0.13s
Val loss: 0.4093 score: 0.8367 time: 0.08s
Test loss: 0.2964 score: 0.8958 time: 0.08s
Epoch 95/1000, LR 0.000265
Train loss: 0.0086;  Loss pred: 0.0086; Loss self: 0.0000; time: 0.13s
Val loss: 0.4097 score: 0.8367 time: 0.08s
Test loss: 0.2943 score: 0.8958 time: 0.08s
     INFO: Early stopping counter 1 of 20
Epoch 96/1000, LR 0.000265
Train loss: 0.0084;  Loss pred: 0.0084; Loss self: 0.0000; time: 0.14s
Val loss: 0.4104 score: 0.8367 time: 0.08s
Test loss: 0.2926 score: 0.8958 time: 0.08s
     INFO: Early stopping counter 2 of 20
Epoch 97/1000, LR 0.000265
Train loss: 0.0080;  Loss pred: 0.0080; Loss self: 0.0000; time: 0.14s
Val loss: 0.4121 score: 0.8367 time: 0.08s
Test loss: 0.2918 score: 0.8958 time: 0.09s
     INFO: Early stopping counter 3 of 20
Epoch 98/1000, LR 0.000265
Train loss: 0.0074;  Loss pred: 0.0074; Loss self: 0.0000; time: 0.16s
Val loss: 0.4142 score: 0.8571 time: 0.07s
Test loss: 0.2915 score: 0.8958 time: 0.07s
     INFO: Early stopping counter 4 of 20
Epoch 99/1000, LR 0.000265
Train loss: 0.0072;  Loss pred: 0.0072; Loss self: 0.0000; time: 0.13s
Val loss: 0.4165 score: 0.8571 time: 0.07s
Test loss: 0.2915 score: 0.8958 time: 0.08s
     INFO: Early stopping counter 5 of 20
Epoch 100/1000, LR 0.000265
Train loss: 0.0067;  Loss pred: 0.0067; Loss self: 0.0000; time: 0.13s
Val loss: 0.4196 score: 0.8571 time: 0.07s
Test loss: 0.2922 score: 0.8958 time: 0.08s
     INFO: Early stopping counter 6 of 20
Epoch 101/1000, LR 0.000265
Train loss: 0.0059;  Loss pred: 0.0059; Loss self: 0.0000; time: 0.13s
Val loss: 0.4229 score: 0.8571 time: 0.07s
Test loss: 0.2931 score: 0.8958 time: 0.08s
     INFO: Early stopping counter 7 of 20
Epoch 102/1000, LR 0.000264
Train loss: 0.0062;  Loss pred: 0.0062; Loss self: 0.0000; time: 0.13s
Val loss: 0.4261 score: 0.8571 time: 0.07s
Test loss: 0.2942 score: 0.8958 time: 0.08s
     INFO: Early stopping counter 8 of 20
Epoch 103/1000, LR 0.000264
Train loss: 0.0058;  Loss pred: 0.0058; Loss self: 0.0000; time: 0.13s
Val loss: 0.4297 score: 0.8776 time: 0.07s
Test loss: 0.2956 score: 0.8958 time: 0.08s
     INFO: Early stopping counter 9 of 20
Epoch 104/1000, LR 0.000264
Train loss: 0.0055;  Loss pred: 0.0055; Loss self: 0.0000; time: 0.13s
Val loss: 0.4332 score: 0.8776 time: 0.07s
Test loss: 0.2970 score: 0.9167 time: 0.08s
     INFO: Early stopping counter 10 of 20
Epoch 105/1000, LR 0.000264
Train loss: 0.0052;  Loss pred: 0.0052; Loss self: 0.0000; time: 0.13s
Val loss: 0.4366 score: 0.8776 time: 0.07s
Test loss: 0.2985 score: 0.9167 time: 0.08s
     INFO: Early stopping counter 11 of 20
Epoch 106/1000, LR 0.000264
Train loss: 0.0052;  Loss pred: 0.0052; Loss self: 0.0000; time: 0.14s
Val loss: 0.4397 score: 0.8776 time: 0.07s
Test loss: 0.3000 score: 0.9167 time: 0.08s
     INFO: Early stopping counter 12 of 20
Epoch 107/1000, LR 0.000264
Train loss: 0.0051;  Loss pred: 0.0051; Loss self: 0.0000; time: 0.13s
Val loss: 0.4429 score: 0.8776 time: 0.07s
Test loss: 0.3015 score: 0.9167 time: 0.08s
     INFO: Early stopping counter 13 of 20
Epoch 108/1000, LR 0.000264
Train loss: 0.0049;  Loss pred: 0.0049; Loss self: 0.0000; time: 0.14s
Val loss: 0.4462 score: 0.8776 time: 0.08s
Test loss: 0.3032 score: 0.9167 time: 0.09s
     INFO: Early stopping counter 14 of 20
Epoch 109/1000, LR 0.000264
Train loss: 0.0047;  Loss pred: 0.0047; Loss self: 0.0000; time: 0.16s
Val loss: 0.4491 score: 0.8776 time: 0.09s
Test loss: 0.3046 score: 0.9167 time: 0.09s
     INFO: Early stopping counter 15 of 20
Epoch 110/1000, LR 0.000263
Train loss: 0.0045;  Loss pred: 0.0045; Loss self: 0.0000; time: 0.16s
Val loss: 0.4520 score: 0.8776 time: 0.09s
Test loss: 0.3062 score: 0.9167 time: 0.09s
     INFO: Early stopping counter 16 of 20
Epoch 111/1000, LR 0.000263
Train loss: 0.0042;  Loss pred: 0.0042; Loss self: 0.0000; time: 0.16s
Val loss: 0.4553 score: 0.8776 time: 0.09s
Test loss: 0.3080 score: 0.9167 time: 0.09s
     INFO: Early stopping counter 17 of 20
Epoch 112/1000, LR 0.000263
Train loss: 0.0041;  Loss pred: 0.0041; Loss self: 0.0000; time: 0.16s
Val loss: 0.4580 score: 0.8776 time: 0.08s
Test loss: 0.3095 score: 0.9167 time: 0.08s
     INFO: Early stopping counter 18 of 20
Epoch 113/1000, LR 0.000263
Train loss: 0.0040;  Loss pred: 0.0040; Loss self: 0.0000; time: 0.14s
Val loss: 0.4607 score: 0.8776 time: 0.07s
Test loss: 0.3111 score: 0.9167 time: 0.08s
     INFO: Early stopping counter 19 of 20
Epoch 114/1000, LR 0.000263
Train loss: 0.0040;  Loss pred: 0.0040; Loss self: 0.0000; time: 0.14s
Val loss: 0.4635 score: 0.8776 time: 0.07s
Test loss: 0.3128 score: 0.9167 time: 0.08s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 093,   Train_Loss: 0.0092,   Val_Loss: 0.4093,   Val_Precision: 0.9474,   Val_Recall: 0.7200,   Val_accuracy: 0.8182,   Val_Score: 0.8367,   Val_Loss: 0.4093,   Test_Precision: 1.0000,   Test_Recall: 0.7917,   Test_accuracy: 0.8837,   Test_Score: 0.8958,   Test_loss: 0.2964


[0.07038930093403906, 0.07231412595137954, 0.0725724590010941, 0.07215185498353094, 0.07197175500914454, 0.071133290999569, 0.07403276301920414, 0.07943737599998713, 0.08081716508604586, 0.07446761301252991, 0.07468703191261739, 0.07876408193260431, 0.0851241450291127, 0.08474375796504319, 0.09266397298779339, 0.0768191940151155, 0.07193765102420002, 0.07158423902001232, 0.20037935709115118, 0.07161905895918608, 0.07122760603670031, 0.07153613690752536, 0.0770906339166686, 0.07235636503901333, 0.21730803896207362, 0.07101385295391083, 0.07000854099169374, 0.07024954294320196, 0.07071646791882813, 0.07683377398643643, 0.07672727899625897, 0.07660546607803553, 0.07612720306497067, 0.07657695806119591, 0.07689940195996314, 0.07681189698632807, 0.07681142399087548, 0.0769569700350985, 0.07440302101895213, 0.07597932091448456, 0.07646483299322426, 0.07632164901588112, 0.07787653000559658, 0.076164641068317, 0.0763582120416686, 0.07544933399185538, 0.0758136420045048, 0.07996649900451303, 0.07629981497302651, 0.07608969905413687, 0.07535254897084087, 0.19807178201153874, 0.07495569007005543, 0.07539584895130247, 0.07613797904923558, 0.07705170603003353, 0.07602654898073524, 0.07555582805071026, 0.07454343000426888, 0.07443252997472882, 0.07584675902035087, 0.07576804200652987, 0.07674941897857934, 0.07561579497996718, 0.07540330197662115, 0.07557258999440819, 0.07531619898509234, 0.0762632469413802, 0.07653705799020827, 0.07541496795602143, 0.19383304205257446, 0.07539575302507728, 0.07535506098065525, 0.07695733802393079, 0.07678640109952539, 0.07632980996277183, 0.08131813502404839, 0.07612489396706223, 0.07650837197434157, 0.07582071493379772, 0.07763806695584208, 0.07747288700193167, 0.07635092595592141, 0.07640174904372543, 0.07694882503710687, 0.07599260902497917, 0.07601368904579431, 0.07769416901282966, 0.07631841208785772, 0.18688034103251994, 0.07614671206101775, 0.07646169699728489, 0.07738347898703068, 0.07799243705812842, 0.07647392898797989, 0.20258380903396755, 0.07715395104605705, 0.07703190704341978, 0.07766890397761017, 0.07350115908775479, 0.07381051406264305, 0.08324953995179385, 0.07640224299393594, 0.07183305500075221, 0.07328689994756132, 0.07415547093842179, 0.07656635600142181, 0.15570054203271866, 0.07788999599870294, 0.07852851902134717, 0.07795114803593606, 0.07398478698451072, 0.07264676899649203, 0.072598697966896, 0.0753917460097, 0.07626780495047569, 0.07687170500867069, 0.07586873893160373, 0.07690374401863664, 0.07647379406262189, 0.08579308504704386, 0.09037044399883598, 0.09093549603130668, 0.0910068069351837, 0.09114988590590656, 0.09175811393652111, 0.09189706796314567, 0.09245196008123457, 0.09226990607567132, 0.09244920802302659, 0.092139111016877, 0.09266450302675366, 0.09251149697229266, 0.09222181502263993, 0.092554931063205, 0.09842416900210083, 0.09948955406434834, 0.09075297799427062, 0.09031287406105548, 0.09048575698398054, 0.09063777211122215, 0.09042726003099233, 0.09044508705846965, 0.09040818398352712, 0.09008386009372771, 0.09064500103704631, 0.09097535791806877, 0.0902373909484595, 0.09035301301628351, 0.09030080900993198, 0.09039484697859734, 0.09021935099735856, 0.09036342205945402, 0.09008789400104433, 0.09085515106562525, 0.09018935600761324, 0.09059131401591003, 0.09053856797982007, 0.0909006759757176, 0.09134182299021631, 0.09058253502007574, 0.09093669895082712, 0.09072876290883869, 0.090459672966972, 0.09054517897311598, 0.09039741999004036, 0.09049827605485916, 0.09059770393650979, 0.09050039190333337, 0.09048596303910017, 0.09113778790924698, 0.0908625969896093, 0.09058244689367712, 0.09054353297688067, 0.0907022199826315, 0.09075357706751674, 0.09072283701971173, 0.0903053981019184, 0.09048308408819139, 0.09076834609732032, 0.09076101297978312, 0.09091027406975627, 0.09076646505855024, 0.0904763899743557, 0.0907937060110271, 0.09086215600837022, 0.09144101105630398, 0.09137854410801083, 0.09035107493400574, 0.0907387740444392, 0.09137160598766059, 0.09124359500128776, 0.09160552907269448, 0.0904536290327087, 0.08947243995498866, 0.09119471802841872, 0.09005306696053594, 0.09026624704711139, 0.09064212301746011, 0.09309150790795684, 0.09240675694309175, 0.09122533397749066, 0.09222614299505949, 0.09054329909849912, 0.09033776598516852, 0.08215832302812487, 0.08385890908539295, 0.08362452103756368, 0.08307526505086571, 0.19865264208056033, 0.08275002194568515, 0.08288622403051704, 0.08353207097388804, 0.08522486605215818, 0.08346603496465832, 0.08345171203836799, 0.18864226003643125, 0.08280691399704665, 0.08298143604770303, 0.08283669606316835, 0.08426643803250045, 0.08361653797328472, 0.08381342492066324, 0.08522639598231763, 0.08457579696550965, 0.22541584190912545, 0.0815642710076645, 0.08158114994876087, 0.08215068001300097, 0.08324982004705817, 0.08267360797617584, 0.08242660807445645, 0.08223053300753236, 0.0820044350111857, 0.0827754259807989, 0.08265002397820354, 0.08358445798512548, 0.0835827769478783, 0.08265572600066662, 0.0827456871047616, 0.08259978296700865, 0.08258541999384761, 0.08325360098388046, 0.08329423691611737, 0.08292927604634315, 0.09131114999763668, 0.08315745193976909, 0.08358996792230755, 0.08314659807365388, 0.0838835509493947, 0.08296812395565212, 0.08321174792945385, 0.081009557005018, 0.07820452295709401, 0.07815547799691558, 0.07903494802303612, 0.08107828698121011, 0.07907794404309243, 0.0816432770807296, 0.08488267695065588, 0.08479537605307996, 0.08463746006600559, 0.08498085790779442, 0.08489695307798684, 0.08509924099780619, 0.08437336899805814, 0.08473751100245863, 0.0847680380102247, 0.08482722099870443, 0.08648058807011694, 0.08523033594246954, 0.08437614201102406, 0.08473488304298371, 0.08408375200815499, 0.08464312902651727, 0.087406131089665, 0.08506979700177908, 0.08517294598277658, 0.08463393792044371, 0.08501104300376028, 0.08562959998380393, 0.08661928598303348, 0.08724325895309448, 0.08537391398567706, 0.08573815901763737, 0.08547367306891829, 0.08528640493750572, 0.08792476495727897, 0.08718486106954515, 0.08607524004764855, 0.08584022498689592, 0.08605359506327659, 0.08584081893786788, 0.08619021298363805, 0.08673609304241836, 0.0861505139619112, 0.08187981194350868, 0.0826708710519597, 0.08294898201711476, 0.08323172002565116, 0.08325293206144124, 0.08336540893651545, 0.08352210302837193, 0.08410857303533703, 0.08410937001463026, 0.08374201599508524, 0.08378443191759288, 0.08364313293714076, 0.08423371298704296, 0.0835083699785173, 0.08342743501998484, 0.08355459198355675, 0.08330023498274386, 0.08430401398800313, 0.08420000004116446, 0.08347359905019403, 0.08405922399833798, 0.08371766388881952, 0.08479454799089581, 0.08434993703849614, 0.08392106904648244, 0.09426504897419363, 0.0783369280397892, 0.08033987798262388, 0.08152718690689653, 0.08029812690801919, 0.08090355701278895, 0.08113484201021492, 0.08099909208249301, 0.08121312607545406, 0.08108221809379756, 0.08108339202590287, 0.09447880799416453, 0.09561614808626473, 0.09592516999691725, 0.09629241493530571, 0.08325900393538177, 0.08237998001277447, 0.08277287299279124]
[0.0014365163455926338, 0.001475798488803664, 0.0014810705918590634, 0.0014724868363985906, 0.0014688113267172355, 0.0014516998163177346, 0.0015108727146776356, 0.0016211709387752475, 0.0016493298997152218, 0.0015197472043373451, 0.0015242251410738242, 0.001607430243522537, 0.0017372274495737285, 0.0017294644482661874, 0.0018911014895468038, 0.001567738653369704, 0.0014681153270244903, 0.0014609028371431086, 0.004089374634513289, 0.0014616134481466546, 0.0014536246129938839, 0.0014599211613780686, 0.0015732782431973182, 0.001476660511000272, 0.004434857938001502, 0.0014492623051818535, 0.0014287457345243618, 0.0014336641416979991, 0.001443193222833227, 0.001568036203804825, 0.0015658628366583464, 0.0015633768587354191, 0.0015536163890810342, 0.001562795062473386, 0.0015693755502033295, 0.0015675897344148584, 0.0015675800814464384, 0.001570550408879561, 0.0015184290003867782, 0.001550598386009889, 0.001560506795780087, 0.0015575846737934922, 0.001589316938889726, 0.0015543804299656529, 0.0015583308579932367, 0.0015397823263643955, 0.001547217183765404, 0.0016319693674390413, 0.0015571390810821737, 0.0015528510011048342, 0.0015378071218538954, 0.004042281265541607, 0.0015297079606133761, 0.0015386907949245401, 0.0015538363071272568, 0.0015724837965312966, 0.0015515622240966376, 0.001541955674504291, 0.0015212944898830385, 0.0015190312239740575, 0.0015478930412316505, 0.0015462865715618341, 0.0015663146730322316, 0.0015431794893870853, 0.0015388428974820643, 0.0015422977549879222, 0.0015370652854100478, 0.0015563927947220449, 0.001561980775310373, 0.0015390809786943148, 0.003955776368419887, 0.0015386888372464751, 0.0015378583873603114, 0.0015705579188557304, 0.0015670694101943957, 0.0015577512237300373, 0.0016595537760009875, 0.0015535692646339232, 0.0015613953464151341, 0.001547361529261178, 0.0015844503460375934, 0.001581079326570034, 0.001558182162365743, 0.0015592193682392945, 0.0015703841844307525, 0.0015508695719383505, 0.0015512997764447819, 0.0015855952859761156, 0.0015575186140379127, 0.003813884510867754, 0.0015540145318575051, 0.001560442795862957, 0.001579254673204708, 0.0015916823889413963, 0.0015606924283261203, 0.004134363449672807, 0.0015745704295113683, 0.0015720797355799954, 0.0015850796730124525, 0.0015000236548521385, 0.0015063370216865928, 0.0016989702030978336, 0.0015592294488558356, 0.0014659807143010655, 0.0014956510193379863, 0.0015133769579269752, 0.0015625786939065676, 0.0031775620823003806, 0.0015895917550755702, 0.0016026228371703503, 0.0015908397558354298, 0.0015098936119287902, 0.0014825871223773882, 0.0014816060809570613, 0.001538607061422449, 0.0015564858153158305, 0.0015688103062994018, 0.0015483416108490558, 0.0015694641636456459, 0.0015606896747473854, 0.0017508792866743645, 0.0018442947754864485, 0.0018558264496185038, 0.0018572817741874226, 0.0018602017531817664, 0.0018726145701330838, 0.0018754503665948097, 0.0018867746955353994, 0.0018830593076667615, 0.0018867185310821753, 0.0018803900207525917, 0.001891112306668442, 0.0018879897341284218, 0.0018820778576048966, 0.001888876144147041, 0.0020086565102469555, 0.002030399062537721, 0.0018521015917198087, 0.0018431198787970506, 0.0018466481017138884, 0.0018497504512494315, 0.0018454542863467823, 0.0018458181032340747, 0.0018450649792556555, 0.00183844612436179, 0.001849897980347884, 0.0018566399575116075, 0.0018415794071114185, 0.001843939041148643, 0.001842873653263918, 0.0018447927954815784, 0.0018412112448440523, 0.0018441514706011026, 0.0018385284490009047, 0.0018541867564413317, 0.0018405991021961886, 0.0018488023268553068, 0.0018477258771391852, 0.0018551158362391349, 0.001864118836535027, 0.0018486231636750151, 0.001855850998996472, 0.0018516074063028305, 0.001846115774836163, 0.0018478607953697139, 0.0018448453059191911, 0.0018469035929563095, 0.001848932733398159, 0.0018469467735374157, 0.0018466523069204117, 0.0018599548552907547, 0.0018543387140736592, 0.001848621365177084, 0.0018478272036098096, 0.001851065713931255, 0.0018521138177044233, 0.0018514864697900353, 0.0018429673082024163, 0.0018465935528202324, 0.0018524152264759249, 0.001852265571015982, 0.0018553117157093116, 0.0018523768379295968, 0.0018464569382521572, 0.001852932775735247, 0.0018543297144565352, 0.0018661430827817138, 0.0018648682471022618, 0.0018438994884490967, 0.0018518117151926367, 0.0018647266528093998, 0.00186211418369975, 0.0018695005933202955, 0.001845992429238953, 0.0018259681623467073, 0.0018611166944575248, 0.0018378176930721622, 0.001842168307083906, 0.0018498392452542878, 0.001899826691999119, 0.0018858521825120766, 0.0018617415097447075, 0.0018821661835726426, 0.0018478224305816147, 0.0018436278772483371, 0.001676700469961732, 0.0017114063078651624, 0.0017066228783176262, 0.0016954135724666472, 0.004054135552664496, 0.001688775958075207, 0.0016915555924595315, 0.0017047361423242458, 0.0017392829806562892, 0.0017033884686664964, 0.0017030961640483262, 0.0038498420415598216, 0.0016899370203478908, 0.0016934986948510821, 0.0016905448176156804, 0.0017197232251530703, 0.0017064599586384638, 0.0017104780596053722, 0.001739314203720768, 0.001726036672765503, 0.0046961633731067804, 0.0016992556459930104, 0.0016996072905991848, 0.0017114725002708535, 0.0017343712509803784, 0.0017223668328369968, 0.0017172210015511762, 0.0017131361043235909, 0.0017084257293997023, 0.0017244880412666437, 0.001721875499545907, 0.0017413428746901143, 0.001741307853080798, 0.0017219942916805546, 0.0017238684813492, 0.0017208288118126802, 0.0017205295832051586, 0.0017344500204975095, 0.0017352966024191119, 0.0017276932509654823, 0.001902315624950764, 0.001732446915411856, 0.001741457665048074, 0.0017322207932011224, 0.0017475739781123896, 0.0017285025824094191, 0.001733578081863622, 0.0016876991042712082, 0.0016292608949394587, 0.0016282391249357413, 0.0016465614171465859, 0.0016891309787752107, 0.0016474571675644256, 0.0017009016058485333, 0.001768389103138664, 0.0017665703344391659, 0.0017632804180417831, 0.001770434539745717, 0.001768686522458059, 0.0017729008541209623, 0.001757778520792878, 0.0017653648125512216, 0.0017660007918796812, 0.0017672337708063424, 0.0018016789181274362, 0.0017756319988014486, 0.0017578362918963346, 0.001765310063395494, 0.0017517448335032289, 0.0017633985213857766, 0.0018209610643680207, 0.001772287437537064, 0.0017744363746411789, 0.001763207040009244, 0.0017710633959116724, 0.0017839499996625818, 0.0018045684579798642, 0.0018175678948561351, 0.001778623208034939, 0.0017862116462007787, 0.001780701522269131, 0.0017768001028647025, 0.0018317659366099785, 0.0018163512722821906, 0.0017932341676593448, 0.0017883380205603316, 0.001792783230484929, 0.001788350394538914, 0.001795629437159126, 0.001807001938383716, 0.0017948023742064834, 0.0017058294154897642, 0.0017223098135824937, 0.001728103792023224, 0.0017339941672010657, 0.0017344360846133593, 0.0017367793528440718, 0.0017400438130910818, 0.0017522619382361881, 0.0017522785419714637, 0.0017446253332309425, 0.001745508998283185, 0.001742565269523766, 0.001754869020563395, 0.0017397577078857769, 0.0017380715629163508, 0.0017407206663240988, 0.0017354215621404971, 0.0017563336247500654, 0.0017541666675242595, 0.0017390333135457088, 0.001751233833298708, 0.00174411799768374, 0.0017665530831436627, 0.0017572903549686696, 0.0017483556051350508, 0.0019638551869623675, 0.001632019334162275, 0.0016737474579713307, 0.0016984830605603445, 0.0016728776439170663, 0.0016854907710997697, 0.0016903092085461442, 0.0016874810850519377, 0.0016919401265719596, 0.001689212876954116, 0.0016892373338729765, 0.0019683084998784275, 0.001992003085130515, 0.0019984410416024425, 0.002006091977818869, 0.0017345625819871202, 0.001716249583599468, 0.001724434854016484]
[696.1285216615136, 677.599284446101, 675.1872635218448, 679.1232188165435, 680.8226365158691, 688.8476451946656, 661.8691239078754, 616.8380989826243, 606.3068402341236, 658.0041714477308, 656.0710573868996, 622.1109774621331, 575.6298636919274, 578.2136782299943, 528.7923496055448, 637.8614176862934, 681.1453988609701, 684.5082195579589, 244.53616735435597, 684.1754235827628, 687.9355172312341, 684.9684945014883, 635.6154763621056, 677.203725941457, 225.48636596252143, 690.0062165589271, 699.9146005030111, 697.51343492181, 692.9079101666197, 637.7403771504188, 638.6255402382915, 639.6410401065281, 643.6595333494784, 639.8791652293372, 637.196112728046, 637.9220136787095, 637.9259419252631, 636.7194547505197, 658.575409021612, 644.9123183813391, 640.8174592409299, 642.0196711132907, 629.2011212682258, 643.3431486409648, 641.7122492766168, 649.4424457781093, 646.3216738365959, 612.7565994509102, 642.2033922011798, 643.9767880424538, 650.2766086779825, 247.38506162955352, 653.7195502329896, 649.9031535761163, 643.5684347270832, 635.9366005588582, 644.5116956764191, 648.5270728171098, 657.3349253877091, 658.3143152145491, 646.0394700168076, 646.7106540218773, 638.4413152844302, 648.0127599396598, 649.838915743935, 648.3832299994699, 650.5904527882345, 642.5113270834624, 640.2127451288864, 649.7383918345569, 252.79487687506574, 649.9039804496969, 650.2549312856241, 636.7164101331425, 638.1338270625482, 641.9510283583656, 602.5716156120541, 643.6790574867841, 640.4527862183894, 646.2613817712479, 631.1336940919659, 632.4793343350979, 641.7734871779875, 641.3465740418698, 636.7868512140482, 644.7995486494409, 644.6207336481201, 630.6779597823952, 642.0469013898144, 262.1998639839451, 643.4946260153085, 640.8437416938309, 633.2100939557434, 628.2660453792448, 640.7412388567323, 241.87520332280894, 635.0938524295334, 636.100051013675, 630.8831139695928, 666.6561535648401, 663.8620611477337, 588.5918412086571, 641.3424276547632, 682.1372138423862, 668.6051672953935, 660.7739035288344, 639.9677685991754, 314.70667577832336, 629.0923419846622, 623.9771309921158, 628.5988241944896, 662.2983183050662, 674.4966180445839, 674.9432341382117, 649.9385223641802, 642.4729285419717, 637.4256951172487, 645.852306101646, 637.1601360282989, 640.7423693386456, 571.1416016003072, 542.2126729910849, 538.8434894898533, 538.4212637511662, 537.5760980170878, 534.0127199421136, 533.2052598201601, 530.0049880710509, 531.0507193950614, 530.020765432576, 531.8045665865469, 528.7893249247012, 529.6638969605646, 531.327647238029, 529.4153367856576, 497.84519896687283, 492.51401778630486, 539.9271856742095, 542.5583064367306, 541.5216895259537, 540.6134645484425, 541.8719972628401, 541.7651924899268, 541.9863317786371, 543.93761489592, 540.5703507022286, 538.6073890924262, 543.0121536646279, 542.3172771357295, 542.6307974119103, 542.0662973366353, 543.120732507094, 542.2548071249533, 543.9132587496381, 539.319999199682, 543.3013624785581, 540.8907082570236, 541.2058208267829, 539.0498967586273, 536.446486351038, 540.9431298112839, 538.8363616156348, 540.0712897323818, 541.6778371273854, 541.1663056577394, 542.0508683256517, 541.4467781717376, 540.8525588500437, 541.4341194493236, 541.5204563698621, 537.6474580312738, 539.2758035036512, 540.9436560873067, 541.1761435519821, 540.2293351737474, 539.9236215620032, 540.1065664354562, 542.6032222868754, 541.5376862291855, 539.8357699220719, 539.8793864378164, 538.992985131712, 539.846957446143, 541.577753200458, 539.6849864686528, 539.2784207705364, 535.8645911059392, 536.2309115155223, 542.3289101517673, 540.0117041034995, 536.2716291384577, 537.02399603291, 534.9022105545132, 541.7140309791356, 547.6546747205136, 537.3118208965818, 544.1236112643818, 542.838564833942, 540.5875145991592, 526.3638016095753, 530.2642536213712, 537.1314947675661, 531.3027131864867, 541.1775414400848, 542.408808383027, 596.4094469555575, 584.3147798417415, 585.9525339223105, 589.8265864093007, 246.66170802867475, 592.1448580661678, 591.1718210490466, 586.601043511986, 574.9495689440178, 587.0651459692301, 587.1659047266954, 259.75091684406743, 591.7380280799694, 590.4935167888837, 591.5252820155251, 581.4889194806283, 586.0084761659874, 584.63187784514, 574.9392478143308, 579.3619659295956, 212.93978095537227, 588.4929688820426, 588.3712111210449, 584.2921810556363, 576.5778229054106, 580.5964100881168, 582.3362276006956, 583.7247825646853, 585.3341955645768, 579.8822468293232, 580.7620819645323, 574.2694414378087, 574.2809912852322, 580.7220179714213, 580.0906570420858, 581.1153283438018, 581.2163939297744, 576.5516377999523, 576.2703612776845, 578.8064515741858, 525.6751229312339, 577.2182634307552, 574.2315877500212, 577.2936128725325, 572.2218415498106, 578.5354388108959, 576.8416262652476, 592.5226821944814, 613.7752419554382, 614.1604047498031, 607.3262676912188, 592.0204013575669, 606.9960540937061, 587.9234851454722, 565.4864069367585, 566.0686022543623, 567.1247691337452, 564.8330833760323, 565.3913157037205, 564.0473338797159, 568.8998859474809, 566.4551558353808, 566.2511617198247, 565.8560947167314, 555.037853825444, 563.1797583480135, 568.8811891130151, 566.4727238208484, 570.8593973701917, 567.0867860398025, 549.1605611825962, 564.2425595419743, 563.5592317037667, 567.1483707294847, 564.6325265986541, 560.5538272872789, 554.1491072715836, 550.1857745342451, 562.2326277327852, 559.8440711810225, 561.5764278820346, 562.8095126670232, 545.9212773934888, 550.554298202203, 557.6516542205251, 559.1784039164334, 557.7919198460544, 559.1745348415501, 556.9077780224547, 553.4028374615117, 557.164406717547, 586.2250884640115, 580.6156314698969, 578.6689460528427, 576.7032086469785, 576.5562702893835, 575.7783787344345, 574.6981728141436, 570.6909327760617, 570.6855251876303, 573.188971266432, 572.8987939813322, 573.8665962700467, 569.8430984205039, 574.7926826059245, 575.3503027931006, 574.4747100129007, 576.228866700598, 569.3679070468771, 570.0712586286618, 575.0321125022634, 571.0259709386451, 573.355702611888, 566.0741302041453, 569.0579232808848, 571.9660217080126, 509.20251484875024, 612.7378389872482, 597.4616990379494, 588.7606554463317, 597.772349720979, 593.2990065246739, 591.6077336288739, 592.5992349533338, 591.0374630254206, 591.9916984075672, 591.9831275024353, 508.05044029518996, 502.0072546396084, 500.39004363028573, 498.481630481995, 576.514223461685, 582.6658369249028, 579.9001323076023]
Elapsed: 0.08642040231576942~0.020487223776850194
Time per graph: 0.001775888249260181~0.0004191272825722765
Speed: 579.4171881292388~74.23709764250938
Total Time: 0.0838
best val loss: 0.4092993438243866 test_score: 0.8958

Testing...
Test loss: 0.2956 score: 0.8958 time: 0.08s
test Score 0.8958
Epoch Time List: [0.45147583296056837, 0.26700617792084813, 0.2705245789838955, 0.2711127110524103, 0.27711690904106945, 0.2702163300709799, 0.27937917900271714, 0.28489178407471627, 0.2999167019734159, 0.2868298531975597, 0.2871128509286791, 0.2855395779479295, 0.44470822892617434, 0.31885680998675525, 0.324563693953678, 0.29417327092960477, 0.2725801628548652, 0.2676425928948447, 0.3972845111275092, 0.26695142698008567, 0.2662792840274051, 0.2666865369537845, 0.277162668062374, 0.27496886102017015, 0.4173425429034978, 0.2752722700824961, 0.2665053380187601, 0.26860476413276047, 0.26250575797166675, 0.2812036970863119, 0.2835975360358134, 0.3678049040026963, 0.2866439529461786, 0.287601264892146, 0.28932875604368746, 0.2837535480502993, 0.2985356649151072, 0.2881823137868196, 0.3624461689032614, 0.28406377404462546, 0.28851149301044643, 0.28746576188132167, 0.2956457460531965, 0.2885552739026025, 0.2879637099104002, 0.41320351511240005, 0.2842101859860122, 0.2875938209472224, 0.2859216219512746, 0.2871118780458346, 0.2851932880003005, 0.4072667279979214, 0.28253552014939487, 0.28300272312480956, 0.2832406460074708, 0.28523488296195865, 0.28730510897003114, 0.28324073110707104, 0.3884441270492971, 0.2819499270990491, 0.2824398059165105, 0.28287428012117743, 0.28774206805974245, 0.28375966497696936, 0.4180663110455498, 0.28299058601260185, 0.28291781386360526, 0.28564787609502673, 0.29045996197964996, 0.28723464091308415, 0.40690262196585536, 0.2868193540489301, 0.2838244130834937, 0.28772560006473213, 0.28868449793662876, 0.29079251806251705, 0.2913852819474414, 0.40228643105365336, 0.28747263306286186, 0.2831268550362438, 0.2976439918857068, 0.2932638900820166, 0.28915290790610015, 0.3736255979165435, 0.28906135878060013, 0.28850617504213005, 0.28850121598225087, 0.29476363002322614, 0.28933205106295645, 0.3978961770189926, 0.28710500698070973, 0.2878032550215721, 0.28616605408024043, 0.28991878498345613, 0.29278348200023174, 0.41326879896223545, 0.2909319770988077, 0.2917484169593081, 0.29298111516982317, 0.2844532928429544, 0.2840541630284861, 0.34133313491474837, 0.2865197039209306, 0.27373663999605924, 0.27867330296430737, 0.2824410990579054, 0.2821520739234984, 0.36133375903591514, 0.3029205030761659, 0.293214876903221, 0.29662124090828, 0.2935400160495192, 0.28036419104319066, 0.29125547921285033, 0.36514891509432346, 0.29096432495862246, 0.2919218259630725, 0.2897517828969285, 0.2936396269360557, 0.29289508203510195, 0.2826005860697478, 0.28743940696585923, 0.29652998014353216, 0.30400335299782455, 0.29844979592598975, 0.30016466695815325, 0.3012407519854605, 0.30239429813809693, 0.30148344801273197, 0.30194804701022804, 0.30144284083507955, 0.30254149506799877, 0.3029746660031378, 0.30241608701180667, 0.30229293496813625, 0.3193141930969432, 0.3298436211189255, 0.29677369201090187, 0.2965917319525033, 0.2957478610333055, 0.29567148513160646, 0.2957335851388052, 0.29651935305446386, 0.2963665509596467, 0.29686583403963596, 0.2966286690207198, 0.29580466914922, 0.2952043181285262, 0.2957309200428426, 0.29576373810414225, 0.29659200308378786, 0.2954909299733117, 0.2959481660509482, 0.2959600929170847, 0.2962046960601583, 0.29590918496251106, 0.2957634360063821, 0.29645485093351454, 0.2972864379407838, 0.29657549189869314, 0.29756798909511417, 0.297466016956605, 0.2971904579317197, 0.2962828528834507, 0.29657642904203385, 0.29556276206858456, 0.29706864594481885, 0.2965014480287209, 0.29594809492118657, 0.296248781029135, 0.29746524791698903, 0.29685483896173537, 0.2975317819509655, 0.29746704001445323, 0.2974579121218994, 0.2965142489410937, 0.2974973830860108, 0.29573706490918994, 0.29727659502532333, 0.29753154690843076, 0.296747526852414, 0.29700377688277513, 0.29800810280721635, 0.29720728809479624, 0.29672953102272004, 0.297922280151397, 0.29906631784979254, 0.2982006308156997, 0.29721233097370714, 0.29732867202255875, 0.2984511989634484, 0.29941791913006455, 0.2980462070554495, 0.29950866394210607, 0.294084774912335, 0.37377614015713334, 0.29474796704016626, 0.29554515494965017, 0.2941433299565688, 0.29705605702474713, 0.3049561660736799, 0.29911105800420046, 0.4028147298377007, 0.294825367978774, 0.2972231289604679, 0.28777029807679355, 0.2716270770179108, 0.2779494959395379, 0.27651189896278083, 0.39183665707241744, 0.2761584228137508, 0.27461879805196077, 0.2760068450588733, 0.27891117706894875, 0.27770994103047997, 0.2769271908327937, 0.3826886130264029, 0.276231189025566, 0.27394983102567494, 0.27348406112287194, 0.2767295320518315, 0.2788399230921641, 0.2777145359432325, 0.3990128340665251, 0.2816809481009841, 0.43188181903678924, 0.2799662071047351, 0.28143794694915414, 0.2827682507922873, 0.28564298897981644, 0.28778158605564386, 0.2853530809516087, 0.3726581659866497, 0.28517991793341935, 0.28726235998328775, 0.2856892349664122, 0.2863075148779899, 0.28813901788089424, 0.28510395204648376, 0.3284935890696943, 0.2862930690171197, 0.285227190121077, 0.28523200703784823, 0.28706328687258065, 0.28637973708100617, 0.2906172949587926, 0.3854836029931903, 0.2856419001473114, 0.286520405090414, 0.2848046589642763, 0.2887695380486548, 0.28517216502223164, 0.33164611586835235, 0.2701098098186776, 0.27158612792845815, 0.2728386950911954, 0.2800696800695732, 0.2797880310099572, 0.27589184092357755, 0.41334199102129787, 0.2912362279603258, 0.2932762319687754, 0.29279136494733393, 0.29569014301523566, 0.29310482391156256, 0.3960350139532238, 0.29409792413935065, 0.2918561709811911, 0.29400473495479673, 0.2926370541099459, 0.2943925199797377, 0.29202059796079993, 0.319808611064218, 0.28898400883190334, 0.29016073897946626, 0.29380070592742413, 0.2948736008256674, 0.29196712491102517, 0.37722604803275317, 0.29347075102850795, 0.2947181580821052, 0.2960094208829105, 0.30499886197503656, 0.2963357960106805, 0.3609181809006259, 0.29462086095009, 0.293197441031225, 0.2955327790696174, 0.3022569300374016, 0.2964752400293946, 0.3402690169168636, 0.29683619795832783, 0.296071392018348, 0.29554326203651726, 0.3024899080628529, 0.29719022510107607, 0.3908067279262468, 0.2886561689665541, 0.28726454288698733, 0.286801356007345, 0.2882035468937829, 0.2882751590805128, 0.289238641038537, 0.2902780089061707, 0.29005363292526454, 0.2901485851034522, 0.28923640691209584, 0.29007560096215457, 0.2907095670234412, 0.2889128690585494, 0.2895115298451856, 0.28904510207939893, 0.2889493270777166, 0.2897736920276657, 0.29022709280252457, 0.29050585289951414, 0.29009889508597553, 0.29048711305949837, 0.28995426278561354, 0.28995441016741097, 0.2908275480149314, 0.2987576100276783, 0.30428831302560866, 0.2826941629173234, 0.28280591091606766, 0.2835292670642957, 0.2840330321341753, 0.2839318090118468, 0.2832336608553305, 0.2836585659533739, 0.28541750088334084, 0.28351495100650936, 0.31000221497379243, 0.3335141009883955, 0.33583980496041477, 0.33500438707415015, 0.32496983115561306, 0.2961672660894692, 0.29295955691486597]
Total Epoch List: [120, 105, 114]
Total Time List: [0.07703959301579744, 0.08533975598402321, 0.0837843269109726]
========================training times:1========================
========================k_idx:0========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: I2BGNNA
I2BGNN(
  (gcs): ModuleList(
    (0): GCNConv(14887, 64)
    (1): GCNConv(64, 64)
  )
  (bns): ModuleList(
    (0-1): 2 x BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  )
  (drops): ModuleList(
    (0-1): 2 x Dropout(p=0.2, inplace=False)
  )
  (lin1): Linear(in_features=64, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=64, bias=True)
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=64, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=64, out_features=2, bias=True)
  )
)
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x759998d205e0>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.6930;  Loss pred: 0.6930; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6948 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6931 score: 0.5102 time: 0.08s
Epoch 2/1000, LR 0.000000
Train loss: 0.6938;  Loss pred: 0.6938; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6948 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6931 score: 0.5102 time: 0.08s
     INFO: Early stopping counter 1 of 20
Epoch 3/1000, LR 0.000030
Train loss: 0.6945;  Loss pred: 0.6945; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6948 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6931 score: 0.5102 time: 0.08s
Epoch 4/1000, LR 0.000060
Train loss: 0.6925;  Loss pred: 0.6925; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6948 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6931 score: 0.5102 time: 0.08s
Epoch 5/1000, LR 0.000090
Train loss: 0.6916;  Loss pred: 0.6916; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6947 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6931 score: 0.5102 time: 0.09s
Epoch 6/1000, LR 0.000120
Train loss: 0.6907;  Loss pred: 0.6907; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6947 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6931 score: 0.5102 time: 0.09s
Epoch 7/1000, LR 0.000150
Train loss: 0.6895;  Loss pred: 0.6895; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6947 score: 0.4898 time: 0.09s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6931 score: 0.5102 time: 0.09s
Epoch 8/1000, LR 0.000180
Train loss: 0.6881;  Loss pred: 0.6881; Loss self: 0.0000; time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6946 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6931 score: 0.5102 time: 0.09s
Epoch 9/1000, LR 0.000210
Train loss: 0.6855;  Loss pred: 0.6855; Loss self: 0.0000; time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6945 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6930 score: 0.5102 time: 0.09s
Epoch 10/1000, LR 0.000240
Train loss: 0.6828;  Loss pred: 0.6828; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6945 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6930 score: 0.5102 time: 0.09s
Epoch 11/1000, LR 0.000270
Train loss: 0.6810;  Loss pred: 0.6810; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6944 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6930 score: 0.5102 time: 0.10s
Epoch 12/1000, LR 0.000270
Train loss: 0.6762;  Loss pred: 0.6762; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6943 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6929 score: 0.5102 time: 0.09s
Epoch 13/1000, LR 0.000270
Train loss: 0.6734;  Loss pred: 0.6734; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6942 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6929 score: 0.5102 time: 0.08s
Epoch 14/1000, LR 0.000270
Train loss: 0.6704;  Loss pred: 0.6704; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6941 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6928 score: 0.5102 time: 0.08s
Epoch 15/1000, LR 0.000270
Train loss: 0.6670;  Loss pred: 0.6670; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6941 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6928 score: 0.5102 time: 0.08s
Epoch 16/1000, LR 0.000270
Train loss: 0.6640;  Loss pred: 0.6640; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6940 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6927 score: 0.5102 time: 0.09s
Epoch 17/1000, LR 0.000270
Train loss: 0.6582;  Loss pred: 0.6582; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6939 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6926 score: 0.5102 time: 0.08s
Epoch 18/1000, LR 0.000270
Train loss: 0.6569;  Loss pred: 0.6569; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6938 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6925 score: 0.5102 time: 0.09s
Epoch 19/1000, LR 0.000270
Train loss: 0.6488;  Loss pred: 0.6488; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6936 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6924 score: 0.5102 time: 0.08s
Epoch 20/1000, LR 0.000270
Train loss: 0.6431;  Loss pred: 0.6431; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6935 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6923 score: 0.5102 time: 0.09s
Epoch 21/1000, LR 0.000270
Train loss: 0.6410;  Loss pred: 0.6410; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6934 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6922 score: 0.5102 time: 0.08s
Epoch 22/1000, LR 0.000270
Train loss: 0.6337;  Loss pred: 0.6337; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6932 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6920 score: 0.5102 time: 0.08s
Epoch 23/1000, LR 0.000270
Train loss: 0.6273;  Loss pred: 0.6273; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6930 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6918 score: 0.5102 time: 0.08s
Epoch 24/1000, LR 0.000270
Train loss: 0.6206;  Loss pred: 0.6206; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6928 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6915 score: 0.5102 time: 0.08s
Epoch 25/1000, LR 0.000270
Train loss: 0.6125;  Loss pred: 0.6125; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6925 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6913 score: 0.5102 time: 0.08s
Epoch 26/1000, LR 0.000270
Train loss: 0.6044;  Loss pred: 0.6044; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6923 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6910 score: 0.5102 time: 0.08s
Epoch 27/1000, LR 0.000270
Train loss: 0.5982;  Loss pred: 0.5982; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6920 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6907 score: 0.5102 time: 0.08s
Epoch 28/1000, LR 0.000270
Train loss: 0.5881;  Loss pred: 0.5881; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6916 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6903 score: 0.5102 time: 0.08s
Epoch 29/1000, LR 0.000270
Train loss: 0.5772;  Loss pred: 0.5772; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6913 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6899 score: 0.5102 time: 0.08s
Epoch 30/1000, LR 0.000270
Train loss: 0.5696;  Loss pred: 0.5696; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6909 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6894 score: 0.5102 time: 0.08s
Epoch 31/1000, LR 0.000270
Train loss: 0.5587;  Loss pred: 0.5587; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6904 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6889 score: 0.5102 time: 0.08s
Epoch 32/1000, LR 0.000270
Train loss: 0.5508;  Loss pred: 0.5508; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6898 score: 0.4898 time: 0.07s
Test loss: 0.6882 score: 0.5510 time: 0.08s
Epoch 33/1000, LR 0.000270
Train loss: 0.5381;  Loss pred: 0.5381; Loss self: 0.0000; time: 0.28s
Val loss: 0.6892 score: 0.5306 time: 0.08s
Test loss: 0.6875 score: 0.5714 time: 0.09s
Epoch 34/1000, LR 0.000270
Train loss: 0.5285;  Loss pred: 0.5285; Loss self: 0.0000; time: 0.15s
Val loss: 0.6884 score: 0.5510 time: 0.08s
Test loss: 0.6866 score: 0.6735 time: 0.08s
Epoch 35/1000, LR 0.000270
Train loss: 0.5144;  Loss pred: 0.5144; Loss self: 0.0000; time: 0.13s
Val loss: 0.6876 score: 0.6327 time: 0.07s
Test loss: 0.6857 score: 0.7143 time: 0.08s
Epoch 36/1000, LR 0.000270
Train loss: 0.5006;  Loss pred: 0.5006; Loss self: 0.0000; time: 0.13s
Val loss: 0.6867 score: 0.6327 time: 0.07s
Test loss: 0.6846 score: 0.7347 time: 0.08s
Epoch 37/1000, LR 0.000270
Train loss: 0.4831;  Loss pred: 0.4831; Loss self: 0.0000; time: 0.15s
Val loss: 0.6858 score: 0.7347 time: 0.08s
Test loss: 0.6835 score: 0.8776 time: 0.08s
Epoch 38/1000, LR 0.000270
Train loss: 0.4758;  Loss pred: 0.4758; Loss self: 0.0000; time: 0.14s
Val loss: 0.6847 score: 0.8163 time: 0.07s
Test loss: 0.6823 score: 0.9184 time: 0.08s
Epoch 39/1000, LR 0.000269
Train loss: 0.4614;  Loss pred: 0.4614; Loss self: 0.0000; time: 0.14s
Val loss: 0.6835 score: 0.8776 time: 0.18s
Test loss: 0.6809 score: 0.9184 time: 0.08s
Epoch 40/1000, LR 0.000269
Train loss: 0.4487;  Loss pred: 0.4487; Loss self: 0.0000; time: 0.13s
Val loss: 0.6822 score: 0.8571 time: 0.07s
Test loss: 0.6795 score: 0.9184 time: 0.08s
Epoch 41/1000, LR 0.000269
Train loss: 0.4322;  Loss pred: 0.4322; Loss self: 0.0000; time: 0.13s
Val loss: 0.6808 score: 0.8163 time: 0.07s
Test loss: 0.6779 score: 0.8776 time: 0.08s
Epoch 42/1000, LR 0.000269
Train loss: 0.4163;  Loss pred: 0.4163; Loss self: 0.0000; time: 0.13s
Val loss: 0.6793 score: 0.8163 time: 0.07s
Test loss: 0.6762 score: 0.8776 time: 0.08s
Epoch 43/1000, LR 0.000269
Train loss: 0.3978;  Loss pred: 0.3978; Loss self: 0.0000; time: 0.13s
Val loss: 0.6777 score: 0.8367 time: 0.08s
Test loss: 0.6744 score: 0.7959 time: 0.08s
Epoch 44/1000, LR 0.000269
Train loss: 0.3854;  Loss pred: 0.3854; Loss self: 0.0000; time: 0.14s
Val loss: 0.6759 score: 0.8163 time: 0.07s
Test loss: 0.6724 score: 0.7347 time: 0.08s
Epoch 45/1000, LR 0.000269
Train loss: 0.3658;  Loss pred: 0.3658; Loss self: 0.0000; time: 0.13s
Val loss: 0.6740 score: 0.7755 time: 0.07s
Test loss: 0.6703 score: 0.6939 time: 0.08s
Epoch 46/1000, LR 0.000269
Train loss: 0.3541;  Loss pred: 0.3541; Loss self: 0.0000; time: 0.17s
Val loss: 0.6719 score: 0.7143 time: 0.07s
Test loss: 0.6681 score: 0.6735 time: 0.08s
Epoch 47/1000, LR 0.000269
Train loss: 0.3335;  Loss pred: 0.3335; Loss self: 0.0000; time: 0.14s
Val loss: 0.6698 score: 0.6735 time: 0.08s
Test loss: 0.6657 score: 0.6122 time: 0.09s
Epoch 48/1000, LR 0.000269
Train loss: 0.3190;  Loss pred: 0.3190; Loss self: 0.0000; time: 0.14s
Val loss: 0.6675 score: 0.6735 time: 0.08s
Test loss: 0.6631 score: 0.5918 time: 0.08s
Epoch 49/1000, LR 0.000269
Train loss: 0.3078;  Loss pred: 0.3078; Loss self: 0.0000; time: 0.14s
Val loss: 0.6650 score: 0.6531 time: 0.07s
Test loss: 0.6603 score: 0.5714 time: 0.09s
Epoch 50/1000, LR 0.000269
Train loss: 0.2833;  Loss pred: 0.2833; Loss self: 0.0000; time: 0.14s
Val loss: 0.6623 score: 0.6531 time: 0.08s
Test loss: 0.6572 score: 0.5714 time: 0.09s
Epoch 51/1000, LR 0.000269
Train loss: 0.2698;  Loss pred: 0.2698; Loss self: 0.0000; time: 0.14s
Val loss: 0.6594 score: 0.6531 time: 0.08s
Test loss: 0.6538 score: 0.5714 time: 0.08s
Epoch 52/1000, LR 0.000269
Train loss: 0.2554;  Loss pred: 0.2554; Loss self: 0.0000; time: 0.15s
Val loss: 0.6562 score: 0.6122 time: 0.12s
Test loss: 0.6500 score: 0.5714 time: 0.08s
Epoch 53/1000, LR 0.000269
Train loss: 0.2447;  Loss pred: 0.2447; Loss self: 0.0000; time: 0.14s
Val loss: 0.6528 score: 0.6122 time: 0.07s
Test loss: 0.6460 score: 0.5714 time: 0.08s
Epoch 54/1000, LR 0.000269
Train loss: 0.2226;  Loss pred: 0.2226; Loss self: 0.0000; time: 0.14s
Val loss: 0.6490 score: 0.6122 time: 0.07s
Test loss: 0.6416 score: 0.5714 time: 0.08s
Epoch 55/1000, LR 0.000269
Train loss: 0.2067;  Loss pred: 0.2067; Loss self: 0.0000; time: 0.14s
Val loss: 0.6448 score: 0.6122 time: 0.08s
Test loss: 0.6369 score: 0.5714 time: 0.08s
Epoch 56/1000, LR 0.000269
Train loss: 0.1956;  Loss pred: 0.1956; Loss self: 0.0000; time: 0.14s
Val loss: 0.6403 score: 0.6122 time: 0.08s
Test loss: 0.6318 score: 0.5714 time: 0.08s
Epoch 57/1000, LR 0.000269
Train loss: 0.1821;  Loss pred: 0.1821; Loss self: 0.0000; time: 0.14s
Val loss: 0.6355 score: 0.6327 time: 0.07s
Test loss: 0.6265 score: 0.5714 time: 0.08s
Epoch 58/1000, LR 0.000269
Train loss: 0.1678;  Loss pred: 0.1678; Loss self: 0.0000; time: 0.14s
Val loss: 0.6305 score: 0.6327 time: 0.07s
Test loss: 0.6208 score: 0.5714 time: 0.08s
Epoch 59/1000, LR 0.000268
Train loss: 0.1543;  Loss pred: 0.1543; Loss self: 0.0000; time: 0.25s
Val loss: 0.6252 score: 0.6327 time: 0.07s
Test loss: 0.6148 score: 0.5714 time: 0.08s
Epoch 60/1000, LR 0.000268
Train loss: 0.1377;  Loss pred: 0.1377; Loss self: 0.0000; time: 0.14s
Val loss: 0.6199 score: 0.6327 time: 0.07s
Test loss: 0.6088 score: 0.5714 time: 0.08s
Epoch 61/1000, LR 0.000268
Train loss: 0.1296;  Loss pred: 0.1296; Loss self: 0.0000; time: 0.14s
Val loss: 0.6141 score: 0.6735 time: 0.07s
Test loss: 0.6023 score: 0.5714 time: 0.08s
Epoch 62/1000, LR 0.000268
Train loss: 0.1190;  Loss pred: 0.1190; Loss self: 0.0000; time: 0.14s
Val loss: 0.6081 score: 0.6735 time: 0.07s
Test loss: 0.5955 score: 0.5714 time: 0.09s
Epoch 63/1000, LR 0.000268
Train loss: 0.1109;  Loss pred: 0.1109; Loss self: 0.0000; time: 0.14s
Val loss: 0.6019 score: 0.6735 time: 0.08s
Test loss: 0.5886 score: 0.5714 time: 0.08s
Epoch 64/1000, LR 0.000268
Train loss: 0.0980;  Loss pred: 0.0980; Loss self: 0.0000; time: 0.14s
Val loss: 0.5956 score: 0.6735 time: 0.07s
Test loss: 0.5814 score: 0.5714 time: 0.08s
Epoch 65/1000, LR 0.000268
Train loss: 0.0921;  Loss pred: 0.0921; Loss self: 0.0000; time: 0.14s
Val loss: 0.5894 score: 0.6939 time: 0.14s
Test loss: 0.5743 score: 0.5918 time: 0.08s
Epoch 66/1000, LR 0.000268
Train loss: 0.0828;  Loss pred: 0.0828; Loss self: 0.0000; time: 0.13s
Val loss: 0.5830 score: 0.6939 time: 0.07s
Test loss: 0.5672 score: 0.5918 time: 0.08s
Epoch 67/1000, LR 0.000268
Train loss: 0.0738;  Loss pred: 0.0738; Loss self: 0.0000; time: 0.13s
Val loss: 0.5764 score: 0.7143 time: 0.07s
Test loss: 0.5597 score: 0.5918 time: 0.08s
Epoch 68/1000, LR 0.000268
Train loss: 0.0667;  Loss pred: 0.0667; Loss self: 0.0000; time: 0.14s
Val loss: 0.5697 score: 0.7143 time: 0.07s
Test loss: 0.5522 score: 0.6327 time: 0.08s
Epoch 69/1000, LR 0.000268
Train loss: 0.0604;  Loss pred: 0.0604; Loss self: 0.0000; time: 0.14s
Val loss: 0.5629 score: 0.7347 time: 0.07s
Test loss: 0.5445 score: 0.6327 time: 0.08s
Epoch 70/1000, LR 0.000268
Train loss: 0.0547;  Loss pred: 0.0547; Loss self: 0.0000; time: 0.14s
Val loss: 0.5557 score: 0.7347 time: 0.07s
Test loss: 0.5365 score: 0.6531 time: 0.08s
Epoch 71/1000, LR 0.000268
Train loss: 0.0508;  Loss pred: 0.0508; Loss self: 0.0000; time: 0.27s
Val loss: 0.5487 score: 0.7347 time: 0.07s
Test loss: 0.5285 score: 0.6735 time: 0.08s
Epoch 72/1000, LR 0.000267
Train loss: 0.0438;  Loss pred: 0.0438; Loss self: 0.0000; time: 0.13s
Val loss: 0.5418 score: 0.7347 time: 0.07s
Test loss: 0.5207 score: 0.6735 time: 0.08s
Epoch 73/1000, LR 0.000267
Train loss: 0.0412;  Loss pred: 0.0412; Loss self: 0.0000; time: 0.13s
Val loss: 0.5347 score: 0.7959 time: 0.07s
Test loss: 0.5128 score: 0.6939 time: 0.08s
Epoch 74/1000, LR 0.000267
Train loss: 0.0366;  Loss pred: 0.0366; Loss self: 0.0000; time: 0.13s
Val loss: 0.5273 score: 0.7959 time: 0.07s
Test loss: 0.5044 score: 0.7143 time: 0.08s
Epoch 75/1000, LR 0.000267
Train loss: 0.0329;  Loss pred: 0.0329; Loss self: 0.0000; time: 0.14s
Val loss: 0.5198 score: 0.7959 time: 0.07s
Test loss: 0.4957 score: 0.7347 time: 0.08s
Epoch 76/1000, LR 0.000267
Train loss: 0.0308;  Loss pred: 0.0308; Loss self: 0.0000; time: 0.14s
Val loss: 0.5120 score: 0.7959 time: 0.07s
Test loss: 0.4864 score: 0.7347 time: 0.08s
Epoch 77/1000, LR 0.000267
Train loss: 0.0281;  Loss pred: 0.0281; Loss self: 0.0000; time: 0.15s
Val loss: 0.5053 score: 0.7959 time: 0.13s
Test loss: 0.4779 score: 0.7347 time: 0.08s
Epoch 78/1000, LR 0.000267
Train loss: 0.0252;  Loss pred: 0.0252; Loss self: 0.0000; time: 0.14s
Val loss: 0.4984 score: 0.7959 time: 0.07s
Test loss: 0.4688 score: 0.7347 time: 0.08s
Epoch 79/1000, LR 0.000267
Train loss: 0.0219;  Loss pred: 0.0219; Loss self: 0.0000; time: 0.14s
Val loss: 0.4919 score: 0.7959 time: 0.07s
Test loss: 0.4597 score: 0.7347 time: 0.08s
Epoch 80/1000, LR 0.000267
Train loss: 0.0205;  Loss pred: 0.0205; Loss self: 0.0000; time: 0.14s
Val loss: 0.4853 score: 0.7959 time: 0.07s
Test loss: 0.4498 score: 0.7347 time: 0.08s
Epoch 81/1000, LR 0.000267
Train loss: 0.0183;  Loss pred: 0.0183; Loss self: 0.0000; time: 0.14s
Val loss: 0.4795 score: 0.8163 time: 0.08s
Test loss: 0.4405 score: 0.7551 time: 0.08s
Epoch 82/1000, LR 0.000267
Train loss: 0.0167;  Loss pred: 0.0167; Loss self: 0.0000; time: 0.14s
Val loss: 0.4739 score: 0.8163 time: 0.07s
Test loss: 0.4308 score: 0.7551 time: 0.08s
Epoch 83/1000, LR 0.000266
Train loss: 0.0158;  Loss pred: 0.0158; Loss self: 0.0000; time: 0.13s
Val loss: 0.4688 score: 0.8163 time: 0.07s
Test loss: 0.4209 score: 0.7755 time: 0.14s
Epoch 84/1000, LR 0.000266
Train loss: 0.0139;  Loss pred: 0.0139; Loss self: 0.0000; time: 0.16s
Val loss: 0.4643 score: 0.8163 time: 0.07s
Test loss: 0.4110 score: 0.7755 time: 0.08s
Epoch 85/1000, LR 0.000266
Train loss: 0.0134;  Loss pred: 0.0134; Loss self: 0.0000; time: 0.13s
Val loss: 0.4605 score: 0.8163 time: 0.07s
Test loss: 0.4009 score: 0.7755 time: 0.08s
Epoch 86/1000, LR 0.000266
Train loss: 0.0122;  Loss pred: 0.0122; Loss self: 0.0000; time: 0.13s
Val loss: 0.4580 score: 0.8163 time: 0.07s
Test loss: 0.3916 score: 0.7755 time: 0.08s
Epoch 87/1000, LR 0.000266
Train loss: 0.0116;  Loss pred: 0.0116; Loss self: 0.0000; time: 0.14s
Val loss: 0.4568 score: 0.8163 time: 0.07s
Test loss: 0.3832 score: 0.8163 time: 0.08s
Epoch 88/1000, LR 0.000266
Train loss: 0.0107;  Loss pred: 0.0107; Loss self: 0.0000; time: 0.14s
Val loss: 0.4566 score: 0.8163 time: 0.07s
Test loss: 0.3752 score: 0.8367 time: 0.08s
Epoch 89/1000, LR 0.000266
Train loss: 0.0098;  Loss pred: 0.0098; Loss self: 0.0000; time: 0.13s
Val loss: 0.4572 score: 0.8163 time: 0.07s
Test loss: 0.3679 score: 0.8776 time: 0.16s
     INFO: Early stopping counter 1 of 20
Epoch 90/1000, LR 0.000266
Train loss: 0.0087;  Loss pred: 0.0087; Loss self: 0.0000; time: 0.14s
Val loss: 0.4588 score: 0.8163 time: 0.07s
Test loss: 0.3609 score: 0.8776 time: 0.08s
     INFO: Early stopping counter 2 of 20
Epoch 91/1000, LR 0.000266
Train loss: 0.0088;  Loss pred: 0.0088; Loss self: 0.0000; time: 0.13s
Val loss: 0.4614 score: 0.8163 time: 0.07s
Test loss: 0.3549 score: 0.8776 time: 0.08s
     INFO: Early stopping counter 3 of 20
Epoch 92/1000, LR 0.000266
Train loss: 0.0082;  Loss pred: 0.0082; Loss self: 0.0000; time: 0.13s
Val loss: 0.4649 score: 0.8163 time: 0.07s
Test loss: 0.3496 score: 0.8776 time: 0.08s
     INFO: Early stopping counter 4 of 20
Epoch 93/1000, LR 0.000265
Train loss: 0.0076;  Loss pred: 0.0076; Loss self: 0.0000; time: 0.14s
Val loss: 0.4691 score: 0.8163 time: 0.08s
Test loss: 0.3450 score: 0.8776 time: 0.08s
     INFO: Early stopping counter 5 of 20
Epoch 94/1000, LR 0.000265
Train loss: 0.0069;  Loss pred: 0.0069; Loss self: 0.0000; time: 0.14s
Val loss: 0.4741 score: 0.8163 time: 0.07s
Test loss: 0.3409 score: 0.8776 time: 0.08s
     INFO: Early stopping counter 6 of 20
Epoch 95/1000, LR 0.000265
Train loss: 0.0069;  Loss pred: 0.0069; Loss self: 0.0000; time: 0.13s
Val loss: 0.4796 score: 0.8163 time: 0.07s
Test loss: 0.3376 score: 0.8776 time: 0.08s
     INFO: Early stopping counter 7 of 20
Epoch 96/1000, LR 0.000265
Train loss: 0.0064;  Loss pred: 0.0064; Loss self: 0.0000; time: 0.26s
Val loss: 0.4857 score: 0.8163 time: 0.07s
Test loss: 0.3352 score: 0.8776 time: 0.08s
     INFO: Early stopping counter 8 of 20
Epoch 97/1000, LR 0.000265
Train loss: 0.0063;  Loss pred: 0.0063; Loss self: 0.0000; time: 0.13s
Val loss: 0.4923 score: 0.8163 time: 0.07s
Test loss: 0.3334 score: 0.8776 time: 0.08s
     INFO: Early stopping counter 9 of 20
Epoch 98/1000, LR 0.000265
Train loss: 0.0057;  Loss pred: 0.0057; Loss self: 0.0000; time: 0.13s
Val loss: 0.4994 score: 0.8163 time: 0.07s
Test loss: 0.3324 score: 0.8776 time: 0.08s
     INFO: Early stopping counter 10 of 20
Epoch 99/1000, LR 0.000265
Train loss: 0.0055;  Loss pred: 0.0055; Loss self: 0.0000; time: 0.13s
Val loss: 0.5063 score: 0.8163 time: 0.07s
Test loss: 0.3313 score: 0.8776 time: 0.08s
     INFO: Early stopping counter 11 of 20
Epoch 100/1000, LR 0.000265
Train loss: 0.0050;  Loss pred: 0.0050; Loss self: 0.0000; time: 0.14s
Val loss: 0.5135 score: 0.8163 time: 0.07s
Test loss: 0.3306 score: 0.8776 time: 0.08s
     INFO: Early stopping counter 12 of 20
Epoch 101/1000, LR 0.000265
Train loss: 0.0051;  Loss pred: 0.0051; Loss self: 0.0000; time: 0.13s
Val loss: 0.5210 score: 0.8163 time: 0.07s
Test loss: 0.3303 score: 0.8776 time: 0.19s
     INFO: Early stopping counter 13 of 20
Epoch 102/1000, LR 0.000264
Train loss: 0.0047;  Loss pred: 0.0047; Loss self: 0.0000; time: 0.13s
Val loss: 0.5282 score: 0.8163 time: 0.07s
Test loss: 0.3301 score: 0.8776 time: 0.08s
     INFO: Early stopping counter 14 of 20
Epoch 103/1000, LR 0.000264
Train loss: 0.0045;  Loss pred: 0.0045; Loss self: 0.0000; time: 0.13s
Val loss: 0.5356 score: 0.8163 time: 0.07s
Test loss: 0.3304 score: 0.8776 time: 0.08s
     INFO: Early stopping counter 15 of 20
Epoch 104/1000, LR 0.000264
Train loss: 0.0047;  Loss pred: 0.0047; Loss self: 0.0000; time: 0.13s
Val loss: 0.5426 score: 0.8367 time: 0.07s
Test loss: 0.3308 score: 0.8776 time: 0.08s
     INFO: Early stopping counter 16 of 20
Epoch 105/1000, LR 0.000264
Train loss: 0.0041;  Loss pred: 0.0041; Loss self: 0.0000; time: 0.13s
Val loss: 0.5495 score: 0.8367 time: 0.07s
Test loss: 0.3313 score: 0.8776 time: 0.08s
     INFO: Early stopping counter 17 of 20
Epoch 106/1000, LR 0.000264
Train loss: 0.0040;  Loss pred: 0.0040; Loss self: 0.0000; time: 0.14s
Val loss: 0.5563 score: 0.8367 time: 0.07s
Test loss: 0.3322 score: 0.8776 time: 0.08s
     INFO: Early stopping counter 18 of 20
Epoch 107/1000, LR 0.000264
Train loss: 0.0042;  Loss pred: 0.0042; Loss self: 0.0000; time: 0.13s
Val loss: 0.5629 score: 0.8367 time: 0.07s
Test loss: 0.3333 score: 0.8980 time: 0.08s
     INFO: Early stopping counter 19 of 20
Epoch 108/1000, LR 0.000264
Train loss: 0.0038;  Loss pred: 0.0038; Loss self: 0.0000; time: 0.15s
Val loss: 0.5692 score: 0.8367 time: 0.13s
Test loss: 0.3344 score: 0.8980 time: 0.08s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 087,   Train_Loss: 0.0107,   Val_Loss: 0.4566,   Val_Precision: 0.9412,   Val_Recall: 0.6667,   Val_accuracy: 0.7805,   Val_Score: 0.8163,   Val_Loss: 0.4566,   Test_Precision: 0.9474,   Test_Recall: 0.7200,   Test_accuracy: 0.8182,   Test_Score: 0.8367,   Test_loss: 0.3752


[0.08927176601719111, 0.08784417109563947, 0.08856501593254507, 0.08912362402770668, 0.1002694769995287, 0.09799959801603109, 0.09175213892012835, 0.09636071999557316, 0.09136826195754111, 0.0954777899896726, 0.10566082608420402, 0.09019988705404103, 0.08768372505437583, 0.08705918805208057, 0.08943387994077057, 0.09147228894289583, 0.08614747109822929, 0.0913235479965806, 0.0875944149447605, 0.09735636203549802, 0.08866511494852602, 0.08619043999351561, 0.0860522169386968, 0.086316297063604, 0.08660000993404537, 0.08643020503222942, 0.08640060992911458, 0.08684351202100515, 0.08751411305274814, 0.08851851208601147, 0.08453241607639939, 0.08861540304496884, 0.09813643200322986, 0.08358828106429428, 0.08445291395764798, 0.08601853204891086, 0.08845959696918726, 0.08624668698757887, 0.08495884609874338, 0.08481182099785656, 0.08435146696865559, 0.08531689795199782, 0.0883896229788661, 0.08624058496206999, 0.08732913504354656, 0.08584923204034567, 0.09031921997666359, 0.0875594470417127, 0.0905116859357804, 0.09280499699525535, 0.08821266610175371, 0.08823595207650214, 0.08792970306240022, 0.08759391203057021, 0.08820799901150167, 0.0880553019233048, 0.08811258303467184, 0.09143594594206661, 0.08671985403634608, 0.08705516497138888, 0.08654431498143822, 0.09091940999496728, 0.08791938610374928, 0.08730435208417475, 0.08551482204347849, 0.08471756789367646, 0.08536871999967843, 0.08716169197577983, 0.08731419499963522, 0.08640897297300398, 0.08457309298682958, 0.08386965689714998, 0.08506900502834469, 0.08459001104347408, 0.08608244406059384, 0.08532336098141968, 0.08637706993613392, 0.08654368296265602, 0.0871712319785729, 0.08720705308951437, 0.08852698106784374, 0.08657826902344823, 0.14277007698547095, 0.08485636895056814, 0.0863132809754461, 0.08698097208980471, 0.0887704809429124, 0.08661664999090135, 0.16220993897877634, 0.08586356497835368, 0.08651635400019586, 0.08772337390109897, 0.08723371697124094, 0.08449944108724594, 0.08602637494914234, 0.08471650700084865, 0.0844720850000158, 0.08575898502022028, 0.08811780600808561, 0.08513125299941748, 0.19476584100630134, 0.08678184496238828, 0.08492879697587341, 0.08561282593291253, 0.0884762309724465, 0.08630337100476027, 0.08463291998486966, 0.08503998606465757]
[0.0018218727758610432, 0.0017927381856252952, 0.0018074493047458176, 0.0018188494699531977, 0.002046315857133239, 0.001999991796245532, 0.0018724926310230274, 0.0019665453060321056, 0.0018646584072967572, 0.001948526326319849, 0.0021563433894735513, 0.0018408140215110413, 0.001789463776619915, 0.0017767181235118484, 0.001825181223281032, 0.001866781406997874, 0.0017581116550659038, 0.0018637458774812367, 0.0017876411213216428, 0.0019868645313366943, 0.0018094921418066537, 0.001758988571296237, 0.0017561676926264654, 0.0017615570829306937, 0.0017673471415111301, 0.0017638817353516209, 0.0017632777536553995, 0.001772316571857248, 0.0017860023071989417, 0.0018065002466532954, 0.0017251513484979467, 0.0018084776131626294, 0.0020027843265965277, 0.0017058832870264138, 0.0017235288562785303, 0.0017554802458961398, 0.0018052978973303521, 0.0017601364691342627, 0.001733854002015171, 0.0017308534897521747, 0.0017214585095643997, 0.001741161182693833, 0.001803869856711553, 0.0017600119380014284, 0.0017822272457866644, 0.0017520251436805238, 0.0018432493872788487, 0.001786927490647198, 0.0018471772639955186, 0.0018939795305154153, 0.0018002584918725248, 0.0018007337158469825, 0.0017944837359673514, 0.001787630857766739, 0.0018001632451326872, 0.0017970469780266285, 0.0017982159802994253, 0.0018660397131034002, 0.001769792939517267, 0.0017766360198242627, 0.0017662105098252697, 0.0018554981631625975, 0.0017942731857908015, 0.0017817214711056072, 0.001745200449866908, 0.0017289299570138054, 0.0017422187755036416, 0.0017788100403220374, 0.001781922346931331, 0.0017634484280204894, 0.0017259814895271342, 0.0017116256509622444, 0.0017361021434356058, 0.0017263267559892669, 0.0017567845726651804, 0.0017412930812534628, 0.0017627973456353862, 0.001766197611482776, 0.0017790047342565898, 0.001779735777337028, 0.0018066730830172191, 0.0017669034494581272, 0.0029136750405198155, 0.0017317626316442477, 0.001761495530111145, 0.0017751218793837695, 0.001811642468222702, 0.001767686734508191, 0.003310406917934211, 0.0017523176526194628, 0.0017656398775550176, 0.0017902729367571218, 0.0017802799381885905, 0.0017244783895356314, 0.0017556403050845374, 0.0017289083061397684, 0.0017239201020411387, 0.0017501833677595975, 0.0017983225715935839, 0.0017373725101921934, 0.003974813081761252, 0.0017710580604569036, 0.0017332407546096615, 0.0017472005292431128, 0.0018056373667846225, 0.001761293285811434, 0.0017272024486708094, 0.0017355099196868892]
[548.8857472648637, 557.8059350876193, 553.265863321478, 549.7981094750694, 488.6831114141576, 500.00205094702966, 534.0474955320143, 508.5059555620908, 536.2912563967818, 513.2083598216936, 463.7480305231624, 543.2379307819184, 558.826623408316, 562.8354811980007, 547.8907996885662, 535.6813584340241, 568.7920884424798, 536.5538360580854, 559.396395659481, 503.30557731947346, 552.6412504900787, 568.5085260463508, 569.4217039743133, 567.6795885242078, 565.8197965256405, 566.9314330762969, 567.1256260829748, 564.2332842106638, 559.9096910285293, 553.5565255817652, 579.6592866305203, 552.9512738900982, 499.30488606297934, 586.2065755642262, 580.2049651545814, 569.6446897296294, 553.9252006434978, 568.1377651880925, 576.7498294768478, 577.7496512100403, 580.9027603302744, 574.3293670565597, 554.3637176924701, 568.1779642560518, 561.0956752928586, 570.7680643779315, 542.5201857662242, 559.619797240802, 541.3665593939587, 527.9888108019132, 555.4757855689145, 555.3291923173915, 557.2633398434957, 559.3996073939367, 555.505175824369, 556.4684798046399, 556.1067251963179, 535.8942754422443, 565.0378514182356, 562.8614915163747, 566.1839256629321, 538.9388250838003, 557.3287322795628, 561.2549527056395, 573.0000814956596, 578.3924304991467, 573.980727369282, 562.1735752171486, 561.1916825231534, 567.0707371479656, 579.3804893434686, 584.2399004933225, 576.0029752748768, 579.264612872754, 569.221756360782, 574.285863055376, 567.2801825325861, 566.1880604404566, 562.1120510496447, 561.8811582786035, 553.5035692954248, 565.9618811127905, 343.20917264047216, 577.4463438159162, 567.699425236068, 563.3416001537586, 551.9852937544804, 565.7110960207674, 302.07766742586097, 570.6727878390906, 566.366909080439, 558.5740472686738, 561.7094135304843, 579.885492371569, 569.5927560468305, 578.3996736256978, 580.0732869324918, 571.3687025149233, 556.0737632925609, 575.5818019069364, 251.58415740065365, 564.6342275995269, 576.9538924932604, 572.3441489759621, 553.8210597517395, 567.7646125467949, 578.9709253652128, 576.1995299804535]
Elapsed: 0.09003473913266875~0.013875196417998724
Time per graph: 0.0018374436557687499~0.00028316727383670865
Speed: 551.4118364780334~47.756010743840406
Total Time: 0.0856
best val loss: 0.456554114818573 test_score: 0.8367

Testing...
Test loss: 0.6809 score: 0.9184 time: 0.08s
test Score 0.9184
Epoch Time List: [0.2960375288967043, 0.2918600750854239, 0.29277693398762494, 0.2927815979346633, 0.30409738793969154, 0.3104192039463669, 0.32033257791772485, 0.3290333009790629, 0.3091641031205654, 0.30193044198676944, 0.3208488639211282, 0.30565969506278634, 0.2916041340213269, 0.2957474540453404, 0.30196137889288366, 0.30021943897008896, 0.2951262620044872, 0.2998928229790181, 0.29568658594507724, 0.3069705858360976, 0.2979743389878422, 0.2885621738387272, 0.28888130094856024, 0.28964159300085157, 0.2885669590905309, 0.2897102510323748, 0.2910154699347913, 0.2922992929816246, 0.29185622406657785, 0.295863519073464, 0.28707995906006545, 0.2886653409805149, 0.4546450270572677, 0.3105903510004282, 0.28287940099835396, 0.28493878699373454, 0.30975358188152313, 0.28936929802875966, 0.40391082293353975, 0.28533446590881795, 0.28319640981499106, 0.28394166892394423, 0.29162246792111546, 0.2902432889677584, 0.28768880700226873, 0.32278659602161497, 0.2979170639300719, 0.30433345201890916, 0.3020322909578681, 0.311315402854234, 0.29849165899213403, 0.3555557099170983, 0.29479566297959536, 0.29484651889652014, 0.29743725690059364, 0.3010118049569428, 0.2964330349350348, 0.2963992148870602, 0.4084675050107762, 0.29133505397476256, 0.2906328940298408, 0.2957895799772814, 0.29979483713395894, 0.29198099102359265, 0.36175527796149254, 0.2845320908818394, 0.2864920728607103, 0.29007627407554537, 0.29827095416840166, 0.2904343269765377, 0.4193204010371119, 0.2830705359810963, 0.28517436888068914, 0.28524472704157233, 0.29327804397325963, 0.28866112895775586, 0.3605032409541309, 0.29059806815348566, 0.2912651940714568, 0.29320360301062465, 0.29667466203682125, 0.29112206504214555, 0.3421482889680192, 0.30942401592619717, 0.2895712190074846, 0.28895802109036595, 0.2936482379445806, 0.2922086529433727, 0.365514682023786, 0.28995940589811653, 0.28885858808644116, 0.2913345699198544, 0.29369096108712256, 0.2869902910897508, 0.28466465999372303, 0.4097789260558784, 0.28369828208815306, 0.2842420799424872, 0.2914394789841026, 0.28874245495535433, 0.3959725451422855, 0.288033566204831, 0.2826918389182538, 0.2832830658880994, 0.2872461940860376, 0.29558343603275716, 0.2846542219631374, 0.35946885985322297]
Total Epoch List: [108]
Total Time List: [0.08562453405465931]
========================k_idx:1========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: I2BGNNA
I2BGNN(
  (gcs): ModuleList(
    (0): GCNConv(14887, 64)
    (1): GCNConv(64, 64)
  )
  (bns): ModuleList(
    (0-1): 2 x BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  )
  (drops): ModuleList(
    (0-1): 2 x Dropout(p=0.2, inplace=False)
  )
  (lin1): Linear(in_features=64, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=64, bias=True)
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=64, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=64, out_features=2, bias=True)
  )
)
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x759998cef5e0>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.6950;  Loss pred: 0.6950; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6990 score: 0.4898 time: 0.09s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6953 score: 0.5102 time: 0.06s
Epoch 2/1000, LR 0.000000
Train loss: 0.6966;  Loss pred: 0.6966; Loss self: 0.0000; time: 0.11s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6989 score: 0.4898 time: 0.10s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6953 score: 0.5102 time: 0.08s
Epoch 3/1000, LR 0.000030
Train loss: 0.6949;  Loss pred: 0.6949; Loss self: 0.0000; time: 0.11s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6988 score: 0.4898 time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6952 score: 0.5102 time: 0.07s
Epoch 4/1000, LR 0.000060
Train loss: 0.6942;  Loss pred: 0.6942; Loss self: 0.0000; time: 0.11s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6987 score: 0.4898 time: 0.09s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6951 score: 0.5102 time: 0.06s
Epoch 5/1000, LR 0.000090
Train loss: 0.6929;  Loss pred: 0.6929; Loss self: 0.0000; time: 0.11s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6985 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6950 score: 0.5102 time: 0.05s
Epoch 6/1000, LR 0.000120
Train loss: 0.6894;  Loss pred: 0.6894; Loss self: 0.0000; time: 0.11s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6983 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6949 score: 0.5102 time: 0.06s
Epoch 7/1000, LR 0.000150
Train loss: 0.6887;  Loss pred: 0.6887; Loss self: 0.0000; time: 0.11s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6981 score: 0.4898 time: 0.09s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6948 score: 0.5102 time: 0.07s
Epoch 8/1000, LR 0.000180
Train loss: 0.6860;  Loss pred: 0.6860; Loss self: 0.0000; time: 0.11s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6979 score: 0.4898 time: 0.09s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6946 score: 0.5102 time: 0.06s
Epoch 9/1000, LR 0.000210
Train loss: 0.6830;  Loss pred: 0.6830; Loss self: 0.0000; time: 0.11s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6976 score: 0.4898 time: 0.09s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6945 score: 0.5102 time: 0.06s
Epoch 10/1000, LR 0.000240
Train loss: 0.6808;  Loss pred: 0.6808; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6973 score: 0.4898 time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6943 score: 0.5102 time: 0.06s
Epoch 11/1000, LR 0.000270
Train loss: 0.6768;  Loss pred: 0.6768; Loss self: 0.0000; time: 0.10s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6971 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6942 score: 0.5102 time: 0.06s
Epoch 12/1000, LR 0.000270
Train loss: 0.6744;  Loss pred: 0.6744; Loss self: 0.0000; time: 0.10s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6968 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6940 score: 0.5102 time: 0.06s
Epoch 13/1000, LR 0.000270
Train loss: 0.6699;  Loss pred: 0.6699; Loss self: 0.0000; time: 0.10s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6965 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6938 score: 0.5102 time: 0.06s
Epoch 14/1000, LR 0.000270
Train loss: 0.6595;  Loss pred: 0.6595; Loss self: 0.0000; time: 0.10s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6961 score: 0.4898 time: 0.09s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6936 score: 0.5102 time: 0.06s
Epoch 15/1000, LR 0.000270
Train loss: 0.6586;  Loss pred: 0.6586; Loss self: 0.0000; time: 0.10s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6958 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6934 score: 0.5102 time: 0.06s
Epoch 16/1000, LR 0.000270
Train loss: 0.6537;  Loss pred: 0.6537; Loss self: 0.0000; time: 0.10s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6955 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6932 score: 0.5102 time: 0.06s
Epoch 17/1000, LR 0.000270
Train loss: 0.6506;  Loss pred: 0.6506; Loss self: 0.0000; time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6951 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6930 score: 0.5102 time: 0.06s
Epoch 18/1000, LR 0.000270
Train loss: 0.6441;  Loss pred: 0.6441; Loss self: 0.0000; time: 0.10s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6948 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6928 score: 0.5102 time: 0.06s
Epoch 19/1000, LR 0.000270
Train loss: 0.6382;  Loss pred: 0.6382; Loss self: 0.0000; time: 0.10s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6944 score: 0.4898 time: 0.09s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6926 score: 0.5102 time: 0.06s
Epoch 20/1000, LR 0.000270
Train loss: 0.6327;  Loss pred: 0.6327; Loss self: 0.0000; time: 0.10s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6940 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6923 score: 0.5102 time: 0.06s
Epoch 21/1000, LR 0.000270
Train loss: 0.6216;  Loss pred: 0.6216; Loss self: 0.0000; time: 0.11s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6935 score: 0.4898 time: 0.10s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6920 score: 0.5102 time: 0.06s
Epoch 22/1000, LR 0.000270
Train loss: 0.6207;  Loss pred: 0.6207; Loss self: 0.0000; time: 0.11s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6931 score: 0.4898 time: 0.09s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6918 score: 0.5102 time: 0.06s
Epoch 23/1000, LR 0.000270
Train loss: 0.6083;  Loss pred: 0.6083; Loss self: 0.0000; time: 0.11s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6926 score: 0.4898 time: 0.10s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6915 score: 0.5102 time: 0.08s
Epoch 24/1000, LR 0.000270
Train loss: 0.6012;  Loss pred: 0.6012; Loss self: 0.0000; time: 0.11s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6921 score: 0.4898 time: 0.11s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6912 score: 0.5102 time: 0.21s
Epoch 25/1000, LR 0.000270
Train loss: 0.5915;  Loss pred: 0.5915; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6916 score: 0.4898 time: 0.09s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6908 score: 0.5102 time: 0.06s
Epoch 26/1000, LR 0.000270
Train loss: 0.5846;  Loss pred: 0.5846; Loss self: 0.0000; time: 0.10s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6910 score: 0.4898 time: 0.09s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6904 score: 0.5102 time: 0.06s
Epoch 27/1000, LR 0.000270
Train loss: 0.5717;  Loss pred: 0.5717; Loss self: 0.0000; time: 0.10s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6904 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6900 score: 0.5102 time: 0.06s
Epoch 28/1000, LR 0.000270
Train loss: 0.5645;  Loss pred: 0.5645; Loss self: 0.0000; time: 0.10s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6897 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6896 score: 0.5102 time: 0.06s
Epoch 29/1000, LR 0.000270
Train loss: 0.5516;  Loss pred: 0.5516; Loss self: 0.0000; time: 0.10s
Val loss: 0.6889 score: 0.7551 time: 0.08s
Test loss: 0.6890 score: 0.7551 time: 0.06s
Epoch 30/1000, LR 0.000270
Train loss: 0.5424;  Loss pred: 0.5424; Loss self: 0.0000; time: 0.10s
Val loss: 0.6881 score: 0.9388 time: 0.08s
Test loss: 0.6884 score: 0.8980 time: 0.05s
Epoch 31/1000, LR 0.000270
Train loss: 0.5265;  Loss pred: 0.5265; Loss self: 0.0000; time: 0.10s
Val loss: 0.6872 score: 0.7755 time: 0.08s
Test loss: 0.6877 score: 0.6735 time: 0.05s
Epoch 32/1000, LR 0.000270
Train loss: 0.5216;  Loss pred: 0.5216; Loss self: 0.0000; time: 0.10s
Val loss: 0.6860 score: 0.6531 time: 0.08s
Test loss: 0.6868 score: 0.5918 time: 0.05s
Epoch 33/1000, LR 0.000270
Train loss: 0.5024;  Loss pred: 0.5024; Loss self: 0.0000; time: 0.10s
Val loss: 0.6848 score: 0.6122 time: 0.08s
Test loss: 0.6858 score: 0.5918 time: 0.05s
Epoch 34/1000, LR 0.000270
Train loss: 0.4942;  Loss pred: 0.4942; Loss self: 0.0000; time: 0.10s
Val loss: 0.6835 score: 0.5918 time: 0.08s
Test loss: 0.6847 score: 0.5714 time: 0.05s
Epoch 35/1000, LR 0.000270
Train loss: 0.4785;  Loss pred: 0.4785; Loss self: 0.0000; time: 0.10s
Val loss: 0.6821 score: 0.5918 time: 0.08s
Test loss: 0.6836 score: 0.5714 time: 0.05s
Epoch 36/1000, LR 0.000270
Train loss: 0.4632;  Loss pred: 0.4632; Loss self: 0.0000; time: 0.10s
Val loss: 0.6806 score: 0.5918 time: 0.08s
Test loss: 0.6823 score: 0.5510 time: 0.05s
Epoch 37/1000, LR 0.000270
Train loss: 0.4469;  Loss pred: 0.4469; Loss self: 0.0000; time: 0.10s
Val loss: 0.6789 score: 0.5918 time: 0.08s
Test loss: 0.6809 score: 0.5510 time: 0.05s
Epoch 38/1000, LR 0.000270
Train loss: 0.4392;  Loss pred: 0.4392; Loss self: 0.0000; time: 0.10s
Val loss: 0.6768 score: 0.5918 time: 0.08s
Test loss: 0.6790 score: 0.5510 time: 0.05s
Epoch 39/1000, LR 0.000269
Train loss: 0.4264;  Loss pred: 0.4264; Loss self: 0.0000; time: 0.11s
Val loss: 0.6745 score: 0.5918 time: 0.08s
Test loss: 0.6770 score: 0.5510 time: 0.05s
Epoch 40/1000, LR 0.000269
Train loss: 0.4064;  Loss pred: 0.4064; Loss self: 0.0000; time: 0.10s
Val loss: 0.6720 score: 0.5918 time: 0.08s
Test loss: 0.6748 score: 0.5510 time: 0.05s
Epoch 41/1000, LR 0.000269
Train loss: 0.3913;  Loss pred: 0.3913; Loss self: 0.0000; time: 0.10s
Val loss: 0.6693 score: 0.5918 time: 0.09s
Test loss: 0.6723 score: 0.5510 time: 0.05s
Epoch 42/1000, LR 0.000269
Train loss: 0.3789;  Loss pred: 0.3789; Loss self: 0.0000; time: 0.11s
Val loss: 0.6662 score: 0.5918 time: 0.08s
Test loss: 0.6696 score: 0.5714 time: 0.05s
Epoch 43/1000, LR 0.000269
Train loss: 0.3553;  Loss pred: 0.3553; Loss self: 0.0000; time: 0.10s
Val loss: 0.6628 score: 0.6122 time: 0.08s
Test loss: 0.6665 score: 0.5714 time: 0.05s
Epoch 44/1000, LR 0.000269
Train loss: 0.3427;  Loss pred: 0.3427; Loss self: 0.0000; time: 0.11s
Val loss: 0.6591 score: 0.6122 time: 0.08s
Test loss: 0.6631 score: 0.5714 time: 0.05s
Epoch 45/1000, LR 0.000269
Train loss: 0.3250;  Loss pred: 0.3250; Loss self: 0.0000; time: 0.10s
Val loss: 0.6549 score: 0.6122 time: 0.09s
Test loss: 0.6593 score: 0.5714 time: 0.05s
Epoch 46/1000, LR 0.000269
Train loss: 0.3179;  Loss pred: 0.3179; Loss self: 0.0000; time: 0.11s
Val loss: 0.6501 score: 0.6122 time: 0.09s
Test loss: 0.6548 score: 0.5918 time: 0.05s
Epoch 47/1000, LR 0.000269
Train loss: 0.2985;  Loss pred: 0.2985; Loss self: 0.0000; time: 0.11s
Val loss: 0.6448 score: 0.6122 time: 0.08s
Test loss: 0.6499 score: 0.5918 time: 0.05s
Epoch 48/1000, LR 0.000269
Train loss: 0.2838;  Loss pred: 0.2838; Loss self: 0.0000; time: 0.11s
Val loss: 0.6391 score: 0.6122 time: 0.08s
Test loss: 0.6445 score: 0.5714 time: 0.06s
Epoch 49/1000, LR 0.000269
Train loss: 0.2674;  Loss pred: 0.2674; Loss self: 0.0000; time: 0.11s
Val loss: 0.6327 score: 0.6531 time: 0.09s
Test loss: 0.6385 score: 0.5918 time: 0.06s
Epoch 50/1000, LR 0.000269
Train loss: 0.2558;  Loss pred: 0.2558; Loss self: 0.0000; time: 0.11s
Val loss: 0.6257 score: 0.6531 time: 0.09s
Test loss: 0.6320 score: 0.6122 time: 0.06s
Epoch 51/1000, LR 0.000269
Train loss: 0.2444;  Loss pred: 0.2444; Loss self: 0.0000; time: 0.11s
Val loss: 0.6182 score: 0.6939 time: 0.09s
Test loss: 0.6249 score: 0.6327 time: 0.05s
Epoch 52/1000, LR 0.000269
Train loss: 0.2345;  Loss pred: 0.2345; Loss self: 0.0000; time: 0.11s
Val loss: 0.6101 score: 0.7347 time: 0.08s
Test loss: 0.6174 score: 0.6327 time: 0.05s
Epoch 53/1000, LR 0.000269
Train loss: 0.2116;  Loss pred: 0.2116; Loss self: 0.0000; time: 0.11s
Val loss: 0.6013 score: 0.7551 time: 0.09s
Test loss: 0.6093 score: 0.6531 time: 0.05s
Epoch 54/1000, LR 0.000269
Train loss: 0.2000;  Loss pred: 0.2000; Loss self: 0.0000; time: 0.11s
Val loss: 0.5919 score: 0.7551 time: 0.09s
Test loss: 0.6007 score: 0.6735 time: 0.05s
Epoch 55/1000, LR 0.000269
Train loss: 0.1837;  Loss pred: 0.1837; Loss self: 0.0000; time: 0.11s
Val loss: 0.5819 score: 0.7755 time: 0.09s
Test loss: 0.5915 score: 0.6735 time: 0.05s
Epoch 56/1000, LR 0.000269
Train loss: 0.1730;  Loss pred: 0.1730; Loss self: 0.0000; time: 0.11s
Val loss: 0.5713 score: 0.8163 time: 0.08s
Test loss: 0.5818 score: 0.6735 time: 0.05s
Epoch 57/1000, LR 0.000269
Train loss: 0.1598;  Loss pred: 0.1598; Loss self: 0.0000; time: 0.11s
Val loss: 0.5598 score: 0.8367 time: 0.09s
Test loss: 0.5714 score: 0.6939 time: 0.05s
Epoch 58/1000, LR 0.000269
Train loss: 0.1499;  Loss pred: 0.1499; Loss self: 0.0000; time: 0.11s
Val loss: 0.5479 score: 0.8367 time: 0.08s
Test loss: 0.5606 score: 0.8163 time: 0.05s
Epoch 59/1000, LR 0.000268
Train loss: 0.1364;  Loss pred: 0.1364; Loss self: 0.0000; time: 0.11s
Val loss: 0.5355 score: 0.8367 time: 0.09s
Test loss: 0.5495 score: 0.8163 time: 0.05s
Epoch 60/1000, LR 0.000268
Train loss: 0.1299;  Loss pred: 0.1299; Loss self: 0.0000; time: 0.11s
Val loss: 0.5227 score: 0.8980 time: 0.09s
Test loss: 0.5381 score: 0.8367 time: 0.05s
Epoch 61/1000, LR 0.000268
Train loss: 0.1226;  Loss pred: 0.1226; Loss self: 0.0000; time: 0.11s
Val loss: 0.5095 score: 0.8980 time: 0.09s
Test loss: 0.5265 score: 0.8367 time: 0.05s
Epoch 62/1000, LR 0.000268
Train loss: 0.1150;  Loss pred: 0.1150; Loss self: 0.0000; time: 0.11s
Val loss: 0.4956 score: 0.9184 time: 0.09s
Test loss: 0.5144 score: 0.8367 time: 0.05s
Epoch 63/1000, LR 0.000268
Train loss: 0.1002;  Loss pred: 0.1002; Loss self: 0.0000; time: 0.11s
Val loss: 0.4815 score: 0.9388 time: 0.09s
Test loss: 0.5021 score: 0.8367 time: 0.05s
Epoch 64/1000, LR 0.000268
Train loss: 0.0996;  Loss pred: 0.0996; Loss self: 0.0000; time: 0.11s
Val loss: 0.4672 score: 0.9388 time: 0.08s
Test loss: 0.4899 score: 0.8367 time: 0.05s
Epoch 65/1000, LR 0.000268
Train loss: 0.0913;  Loss pred: 0.0913; Loss self: 0.0000; time: 0.11s
Val loss: 0.4524 score: 0.9592 time: 0.09s
Test loss: 0.4773 score: 0.8367 time: 0.05s
Epoch 66/1000, LR 0.000268
Train loss: 0.0829;  Loss pred: 0.0829; Loss self: 0.0000; time: 0.11s
Val loss: 0.4373 score: 0.9592 time: 0.09s
Test loss: 0.4645 score: 0.8367 time: 0.05s
Epoch 67/1000, LR 0.000268
Train loss: 0.0712;  Loss pred: 0.0712; Loss self: 0.0000; time: 0.11s
Val loss: 0.4216 score: 0.9592 time: 0.09s
Test loss: 0.4514 score: 0.8367 time: 0.05s
Epoch 68/1000, LR 0.000268
Train loss: 0.0694;  Loss pred: 0.0694; Loss self: 0.0000; time: 0.11s
Val loss: 0.4061 score: 0.9592 time: 0.09s
Test loss: 0.4385 score: 0.8367 time: 0.05s
Epoch 69/1000, LR 0.000268
Train loss: 0.0624;  Loss pred: 0.0624; Loss self: 0.0000; time: 0.11s
Val loss: 0.3902 score: 0.9592 time: 0.09s
Test loss: 0.4254 score: 0.8776 time: 0.05s
Epoch 70/1000, LR 0.000268
Train loss: 0.0589;  Loss pred: 0.0589; Loss self: 0.0000; time: 0.11s
Val loss: 0.3741 score: 0.9592 time: 0.09s
Test loss: 0.4124 score: 0.8980 time: 0.05s
Epoch 71/1000, LR 0.000268
Train loss: 0.0546;  Loss pred: 0.0546; Loss self: 0.0000; time: 0.11s
Val loss: 0.3579 score: 0.9592 time: 0.09s
Test loss: 0.3994 score: 0.9184 time: 0.05s
Epoch 72/1000, LR 0.000267
Train loss: 0.0515;  Loss pred: 0.0515; Loss self: 0.0000; time: 0.11s
Val loss: 0.3423 score: 0.9592 time: 0.08s
Test loss: 0.3870 score: 0.9184 time: 0.05s
Epoch 73/1000, LR 0.000267
Train loss: 0.0442;  Loss pred: 0.0442; Loss self: 0.0000; time: 0.11s
Val loss: 0.3271 score: 0.9592 time: 0.09s
Test loss: 0.3752 score: 0.9184 time: 0.06s
Epoch 74/1000, LR 0.000267
Train loss: 0.0451;  Loss pred: 0.0451; Loss self: 0.0000; time: 0.11s
Val loss: 0.3124 score: 0.9592 time: 0.09s
Test loss: 0.3639 score: 0.9184 time: 0.05s
Epoch 75/1000, LR 0.000267
Train loss: 0.0379;  Loss pred: 0.0379; Loss self: 0.0000; time: 0.11s
Val loss: 0.2983 score: 0.9592 time: 0.09s
Test loss: 0.3533 score: 0.9184 time: 0.06s
Epoch 76/1000, LR 0.000267
Train loss: 0.0332;  Loss pred: 0.0332; Loss self: 0.0000; time: 0.11s
Val loss: 0.2843 score: 0.9592 time: 0.09s
Test loss: 0.3430 score: 0.9184 time: 0.06s
Epoch 77/1000, LR 0.000267
Train loss: 0.0342;  Loss pred: 0.0342; Loss self: 0.0000; time: 0.11s
Val loss: 0.2711 score: 0.9592 time: 0.09s
Test loss: 0.3334 score: 0.9184 time: 0.06s
Epoch 78/1000, LR 0.000267
Train loss: 0.0345;  Loss pred: 0.0345; Loss self: 0.0000; time: 0.10s
Val loss: 0.2582 score: 0.9592 time: 0.09s
Test loss: 0.3241 score: 0.8980 time: 0.06s
Epoch 79/1000, LR 0.000267
Train loss: 0.0277;  Loss pred: 0.0277; Loss self: 0.0000; time: 0.11s
Val loss: 0.2457 score: 0.9388 time: 0.09s
Test loss: 0.3152 score: 0.8980 time: 0.06s
Epoch 80/1000, LR 0.000267
Train loss: 0.0236;  Loss pred: 0.0236; Loss self: 0.0000; time: 0.11s
Val loss: 0.2338 score: 0.9184 time: 0.09s
Test loss: 0.3071 score: 0.8980 time: 0.06s
Epoch 81/1000, LR 0.000267
Train loss: 0.0218;  Loss pred: 0.0218; Loss self: 0.0000; time: 0.11s
Val loss: 0.2221 score: 0.9184 time: 0.09s
Test loss: 0.2994 score: 0.8980 time: 0.06s
Epoch 82/1000, LR 0.000267
Train loss: 0.0297;  Loss pred: 0.0297; Loss self: 0.0000; time: 0.11s
Val loss: 0.2112 score: 0.9184 time: 0.09s
Test loss: 0.2926 score: 0.8980 time: 0.05s
Epoch 83/1000, LR 0.000266
Train loss: 0.0191;  Loss pred: 0.0191; Loss self: 0.0000; time: 0.11s
Val loss: 0.2007 score: 0.9184 time: 0.09s
Test loss: 0.2863 score: 0.8980 time: 0.06s
Epoch 84/1000, LR 0.000266
Train loss: 0.0174;  Loss pred: 0.0174; Loss self: 0.0000; time: 0.11s
Val loss: 0.1909 score: 0.9184 time: 0.09s
Test loss: 0.2808 score: 0.8980 time: 0.06s
Epoch 85/1000, LR 0.000266
Train loss: 0.0174;  Loss pred: 0.0174; Loss self: 0.0000; time: 0.11s
Val loss: 0.1815 score: 0.9184 time: 0.09s
Test loss: 0.2759 score: 0.9184 time: 0.05s
Epoch 86/1000, LR 0.000266
Train loss: 0.0185;  Loss pred: 0.0185; Loss self: 0.0000; time: 0.11s
Val loss: 0.1727 score: 0.9184 time: 0.08s
Test loss: 0.2717 score: 0.9184 time: 0.06s
Epoch 87/1000, LR 0.000266
Train loss: 0.0145;  Loss pred: 0.0145; Loss self: 0.0000; time: 0.11s
Val loss: 0.1645 score: 0.9184 time: 0.09s
Test loss: 0.2682 score: 0.9184 time: 0.05s
Epoch 88/1000, LR 0.000266
Train loss: 0.0149;  Loss pred: 0.0149; Loss self: 0.0000; time: 0.11s
Val loss: 0.1570 score: 0.9184 time: 0.09s
Test loss: 0.2654 score: 0.9184 time: 0.06s
Epoch 89/1000, LR 0.000266
Train loss: 0.0116;  Loss pred: 0.0116; Loss self: 0.0000; time: 0.11s
Val loss: 0.1500 score: 0.9184 time: 0.09s
Test loss: 0.2633 score: 0.9184 time: 0.05s
Epoch 90/1000, LR 0.000266
Train loss: 0.0115;  Loss pred: 0.0115; Loss self: 0.0000; time: 0.11s
Val loss: 0.1435 score: 0.9184 time: 0.08s
Test loss: 0.2619 score: 0.9184 time: 0.06s
Epoch 91/1000, LR 0.000266
Train loss: 0.0110;  Loss pred: 0.0110; Loss self: 0.0000; time: 0.11s
Val loss: 0.1375 score: 0.9184 time: 0.09s
Test loss: 0.2612 score: 0.9184 time: 0.06s
Epoch 92/1000, LR 0.000266
Train loss: 0.0109;  Loss pred: 0.0109; Loss self: 0.0000; time: 0.11s
Val loss: 0.1322 score: 0.9184 time: 0.09s
Test loss: 0.2613 score: 0.9184 time: 0.06s
Epoch 93/1000, LR 0.000265
Train loss: 0.0096;  Loss pred: 0.0096; Loss self: 0.0000; time: 0.11s
Val loss: 0.1274 score: 0.9184 time: 0.09s
Test loss: 0.2621 score: 0.9184 time: 0.06s
Epoch 94/1000, LR 0.000265
Train loss: 0.0097;  Loss pred: 0.0097; Loss self: 0.0000; time: 0.11s
Val loss: 0.1231 score: 0.9184 time: 0.09s
Test loss: 0.2634 score: 0.9184 time: 0.06s
Epoch 95/1000, LR 0.000265
Train loss: 0.0097;  Loss pred: 0.0097; Loss self: 0.0000; time: 0.11s
Val loss: 0.1193 score: 0.9184 time: 0.09s
Test loss: 0.2649 score: 0.9184 time: 0.06s
Epoch 96/1000, LR 0.000265
Train loss: 0.0078;  Loss pred: 0.0078; Loss self: 0.0000; time: 0.11s
Val loss: 0.1158 score: 0.9184 time: 0.09s
Test loss: 0.2671 score: 0.9184 time: 0.06s
Epoch 97/1000, LR 0.000265
Train loss: 0.0069;  Loss pred: 0.0069; Loss self: 0.0000; time: 0.11s
Val loss: 0.1128 score: 0.9184 time: 0.09s
Test loss: 0.2696 score: 0.9184 time: 0.05s
Epoch 98/1000, LR 0.000265
Train loss: 0.0071;  Loss pred: 0.0071; Loss self: 0.0000; time: 0.10s
Val loss: 0.1102 score: 0.9184 time: 0.09s
Test loss: 0.2724 score: 0.9184 time: 0.06s
Epoch 99/1000, LR 0.000265
Train loss: 0.0065;  Loss pred: 0.0065; Loss self: 0.0000; time: 0.11s
Val loss: 0.1079 score: 0.9184 time: 0.09s
Test loss: 0.2755 score: 0.9184 time: 0.06s
Epoch 100/1000, LR 0.000265
Train loss: 0.0059;  Loss pred: 0.0059; Loss self: 0.0000; time: 0.11s
Val loss: 0.1060 score: 0.9184 time: 0.09s
Test loss: 0.2790 score: 0.9184 time: 0.05s
Epoch 101/1000, LR 0.000265
Train loss: 0.0059;  Loss pred: 0.0059; Loss self: 0.0000; time: 0.11s
Val loss: 0.1044 score: 0.9184 time: 0.09s
Test loss: 0.2826 score: 0.9184 time: 0.05s
Epoch 102/1000, LR 0.000264
Train loss: 0.0066;  Loss pred: 0.0066; Loss self: 0.0000; time: 0.11s
Val loss: 0.1030 score: 0.9184 time: 0.09s
Test loss: 0.2866 score: 0.9184 time: 0.05s
Epoch 103/1000, LR 0.000264
Train loss: 0.0045;  Loss pred: 0.0045; Loss self: 0.0000; time: 0.11s
Val loss: 0.1020 score: 0.9184 time: 0.09s
Test loss: 0.2904 score: 0.9184 time: 0.05s
Epoch 104/1000, LR 0.000264
Train loss: 0.0047;  Loss pred: 0.0047; Loss self: 0.0000; time: 0.11s
Val loss: 0.1010 score: 0.9184 time: 0.09s
Test loss: 0.2944 score: 0.9184 time: 0.06s
Epoch 105/1000, LR 0.000264
Train loss: 0.0045;  Loss pred: 0.0045; Loss self: 0.0000; time: 0.11s
Val loss: 0.1003 score: 0.9184 time: 0.09s
Test loss: 0.2986 score: 0.9184 time: 0.05s
Epoch 106/1000, LR 0.000264
Train loss: 0.0056;  Loss pred: 0.0056; Loss self: 0.0000; time: 0.11s
Val loss: 0.0997 score: 0.9388 time: 0.08s
Test loss: 0.3024 score: 0.9184 time: 0.05s
Epoch 107/1000, LR 0.000264
Train loss: 0.0044;  Loss pred: 0.0044; Loss self: 0.0000; time: 0.10s
Val loss: 0.0993 score: 0.9388 time: 0.09s
Test loss: 0.3065 score: 0.9184 time: 0.06s
Epoch 108/1000, LR 0.000264
Train loss: 0.0040;  Loss pred: 0.0040; Loss self: 0.0000; time: 0.11s
Val loss: 0.0991 score: 0.9388 time: 0.08s
Test loss: 0.3106 score: 0.9184 time: 0.06s
Epoch 109/1000, LR 0.000264
Train loss: 0.0045;  Loss pred: 0.0045; Loss self: 0.0000; time: 0.11s
Val loss: 0.0991 score: 0.9388 time: 0.08s
Test loss: 0.3146 score: 0.9184 time: 0.06s
Epoch 110/1000, LR 0.000263
Train loss: 0.0034;  Loss pred: 0.0034; Loss self: 0.0000; time: 0.11s
Val loss: 0.0992 score: 0.9388 time: 0.08s
Test loss: 0.3185 score: 0.9184 time: 0.05s
     INFO: Early stopping counter 1 of 20
Epoch 111/1000, LR 0.000263
Train loss: 0.0038;  Loss pred: 0.0038; Loss self: 0.0000; time: 0.10s
Val loss: 0.0993 score: 0.9388 time: 0.09s
Test loss: 0.3223 score: 0.9184 time: 0.06s
     INFO: Early stopping counter 2 of 20
Epoch 112/1000, LR 0.000263
Train loss: 0.0032;  Loss pred: 0.0032; Loss self: 0.0000; time: 0.11s
Val loss: 0.0995 score: 0.9388 time: 0.09s
Test loss: 0.3260 score: 0.9184 time: 0.06s
     INFO: Early stopping counter 3 of 20
Epoch 113/1000, LR 0.000263
Train loss: 0.0045;  Loss pred: 0.0045; Loss self: 0.0000; time: 0.11s
Val loss: 0.0998 score: 0.9388 time: 0.09s
Test loss: 0.3290 score: 0.9184 time: 0.06s
     INFO: Early stopping counter 4 of 20
Epoch 114/1000, LR 0.000263
Train loss: 0.0034;  Loss pred: 0.0034; Loss self: 0.0000; time: 0.11s
Val loss: 0.1002 score: 0.9388 time: 0.09s
Test loss: 0.3322 score: 0.9184 time: 0.06s
     INFO: Early stopping counter 5 of 20
Epoch 115/1000, LR 0.000263
Train loss: 0.0041;  Loss pred: 0.0041; Loss self: 0.0000; time: 0.11s
Val loss: 0.1006 score: 0.9388 time: 0.09s
Test loss: 0.3357 score: 0.9184 time: 0.06s
     INFO: Early stopping counter 6 of 20
Epoch 116/1000, LR 0.000263
Train loss: 0.0032;  Loss pred: 0.0032; Loss self: 0.0000; time: 0.11s
Val loss: 0.1012 score: 0.9388 time: 0.09s
Test loss: 0.3391 score: 0.9184 time: 0.06s
     INFO: Early stopping counter 7 of 20
Epoch 117/1000, LR 0.000262
Train loss: 0.0028;  Loss pred: 0.0028; Loss self: 0.0000; time: 0.11s
Val loss: 0.1016 score: 0.9388 time: 0.09s
Test loss: 0.3422 score: 0.9184 time: 0.05s
     INFO: Early stopping counter 8 of 20
Epoch 118/1000, LR 0.000262
Train loss: 0.0032;  Loss pred: 0.0032; Loss self: 0.0000; time: 0.11s
Val loss: 0.1021 score: 0.9388 time: 0.09s
Test loss: 0.3450 score: 0.9184 time: 0.06s
     INFO: Early stopping counter 9 of 20
Epoch 119/1000, LR 0.000262
Train loss: 0.0032;  Loss pred: 0.0032; Loss self: 0.0000; time: 0.11s
Val loss: 0.1026 score: 0.9388 time: 0.09s
Test loss: 0.3477 score: 0.9184 time: 0.06s
     INFO: Early stopping counter 10 of 20
Epoch 120/1000, LR 0.000262
Train loss: 0.0028;  Loss pred: 0.0028; Loss self: 0.0000; time: 0.11s
Val loss: 0.1032 score: 0.9388 time: 0.09s
Test loss: 0.3506 score: 0.9184 time: 0.06s
     INFO: Early stopping counter 11 of 20
Epoch 121/1000, LR 0.000262
Train loss: 0.0024;  Loss pred: 0.0024; Loss self: 0.0000; time: 0.11s
Val loss: 0.1037 score: 0.9388 time: 0.09s
Test loss: 0.3531 score: 0.9184 time: 0.06s
     INFO: Early stopping counter 12 of 20
Epoch 122/1000, LR 0.000262
Train loss: 0.0040;  Loss pred: 0.0040; Loss self: 0.0000; time: 0.11s
Val loss: 0.1042 score: 0.9388 time: 0.08s
Test loss: 0.3550 score: 0.9184 time: 0.06s
     INFO: Early stopping counter 13 of 20
Epoch 123/1000, LR 0.000262
Train loss: 0.0040;  Loss pred: 0.0040; Loss self: 0.0000; time: 0.11s
Val loss: 0.1048 score: 0.9388 time: 0.09s
Test loss: 0.3564 score: 0.9184 time: 0.06s
     INFO: Early stopping counter 14 of 20
Epoch 124/1000, LR 0.000261
Train loss: 0.0029;  Loss pred: 0.0029; Loss self: 0.0000; time: 0.11s
Val loss: 0.1052 score: 0.9388 time: 0.08s
Test loss: 0.3574 score: 0.9184 time: 0.06s
     INFO: Early stopping counter 15 of 20
Epoch 125/1000, LR 0.000261
Train loss: 0.0031;  Loss pred: 0.0031; Loss self: 0.0000; time: 0.10s
Val loss: 0.1059 score: 0.9592 time: 0.08s
Test loss: 0.3586 score: 0.9184 time: 0.06s
     INFO: Early stopping counter 16 of 20
Epoch 126/1000, LR 0.000261
Train loss: 0.0026;  Loss pred: 0.0026; Loss self: 0.0000; time: 0.10s
Val loss: 0.1064 score: 0.9592 time: 0.08s
Test loss: 0.3598 score: 0.9184 time: 0.06s
     INFO: Early stopping counter 17 of 20
Epoch 127/1000, LR 0.000261
Train loss: 0.0028;  Loss pred: 0.0028; Loss self: 0.0000; time: 0.21s
Val loss: 0.1070 score: 0.9592 time: 0.08s
Test loss: 0.3604 score: 0.9184 time: 0.06s
     INFO: Early stopping counter 18 of 20
Epoch 128/1000, LR 0.000261
Train loss: 0.0037;  Loss pred: 0.0037; Loss self: 0.0000; time: 0.10s
Val loss: 0.1076 score: 0.9592 time: 0.08s
Test loss: 0.3606 score: 0.9184 time: 0.06s
     INFO: Early stopping counter 19 of 20
Epoch 129/1000, LR 0.000261
Train loss: 0.0023;  Loss pred: 0.0023; Loss self: 0.0000; time: 0.10s
Val loss: 0.1082 score: 0.9592 time: 0.08s
Test loss: 0.3607 score: 0.9184 time: 0.06s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 108,   Train_Loss: 0.0045,   Val_Loss: 0.0991,   Val_Precision: 0.9231,   Val_Recall: 0.9600,   Val_accuracy: 0.9412,   Val_Score: 0.9388,   Val_Loss: 0.0991,   Test_Precision: 0.9545,   Test_Recall: 0.8750,   Test_accuracy: 0.9130,   Test_Score: 0.9184,   Test_loss: 0.3146


[0.08927176601719111, 0.08784417109563947, 0.08856501593254507, 0.08912362402770668, 0.1002694769995287, 0.09799959801603109, 0.09175213892012835, 0.09636071999557316, 0.09136826195754111, 0.0954777899896726, 0.10566082608420402, 0.09019988705404103, 0.08768372505437583, 0.08705918805208057, 0.08943387994077057, 0.09147228894289583, 0.08614747109822929, 0.0913235479965806, 0.0875944149447605, 0.09735636203549802, 0.08866511494852602, 0.08619043999351561, 0.0860522169386968, 0.086316297063604, 0.08660000993404537, 0.08643020503222942, 0.08640060992911458, 0.08684351202100515, 0.08751411305274814, 0.08851851208601147, 0.08453241607639939, 0.08861540304496884, 0.09813643200322986, 0.08358828106429428, 0.08445291395764798, 0.08601853204891086, 0.08845959696918726, 0.08624668698757887, 0.08495884609874338, 0.08481182099785656, 0.08435146696865559, 0.08531689795199782, 0.0883896229788661, 0.08624058496206999, 0.08732913504354656, 0.08584923204034567, 0.09031921997666359, 0.0875594470417127, 0.0905116859357804, 0.09280499699525535, 0.08821266610175371, 0.08823595207650214, 0.08792970306240022, 0.08759391203057021, 0.08820799901150167, 0.0880553019233048, 0.08811258303467184, 0.09143594594206661, 0.08671985403634608, 0.08705516497138888, 0.08654431498143822, 0.09091940999496728, 0.08791938610374928, 0.08730435208417475, 0.08551482204347849, 0.08471756789367646, 0.08536871999967843, 0.08716169197577983, 0.08731419499963522, 0.08640897297300398, 0.08457309298682958, 0.08386965689714998, 0.08506900502834469, 0.08459001104347408, 0.08608244406059384, 0.08532336098141968, 0.08637706993613392, 0.08654368296265602, 0.0871712319785729, 0.08720705308951437, 0.08852698106784374, 0.08657826902344823, 0.14277007698547095, 0.08485636895056814, 0.0863132809754461, 0.08698097208980471, 0.0887704809429124, 0.08661664999090135, 0.16220993897877634, 0.08586356497835368, 0.08651635400019586, 0.08772337390109897, 0.08723371697124094, 0.08449944108724594, 0.08602637494914234, 0.08471650700084865, 0.0844720850000158, 0.08575898502022028, 0.08811780600808561, 0.08513125299941748, 0.19476584100630134, 0.08678184496238828, 0.08492879697587341, 0.08561282593291253, 0.0884762309724465, 0.08630337100476027, 0.08463291998486966, 0.08503998606465757, 0.0658624890493229, 0.0804597019450739, 0.07331473205704242, 0.0608769099926576, 0.05999282898847014, 0.061074447934515774, 0.07216553401667625, 0.062499769032001495, 0.06786698894575238, 0.067447553970851, 0.06221901800017804, 0.06125420599710196, 0.0614026669645682, 0.0621047739405185, 0.06147303502075374, 0.06275887391529977, 0.06134258909150958, 0.0614073189208284, 0.06156477192416787, 0.06144882692024112, 0.06323572993278503, 0.06696505192667246, 0.08128993702121079, 0.2191506379749626, 0.06699492805637419, 0.06574582599569112, 0.06044082995504141, 0.06087054801173508, 0.06030193797778338, 0.06010917294770479, 0.059824301046319306, 0.06001050490885973, 0.059486026992090046, 0.059834311017766595, 0.05936336098238826, 0.05981409491505474, 0.059400498983450234, 0.05982361105270684, 0.05948578391689807, 0.05982532992493361, 0.05984647304285318, 0.05971092509571463, 0.0596358539769426, 0.05969032703433186, 0.05982713995035738, 0.05999964301008731, 0.0594205119414255, 0.06014290393795818, 0.060405511991120875, 0.0600936459377408, 0.05947571701835841, 0.05981987703125924, 0.05962101905606687, 0.0599162980215624, 0.05998835398349911, 0.05981093109585345, 0.059753878973424435, 0.05952383205294609, 0.05964037694502622, 0.05981434998102486, 0.059943838976323605, 0.05983165104407817, 0.0599097718950361, 0.059730642940849066, 0.059711945010349154, 0.059843678027391434, 0.05997916602063924, 0.059819676098413765, 0.05991968500893563, 0.059612583951093256, 0.059460312011651695, 0.05978478700853884, 0.06021877902094275, 0.059939224971458316, 0.06068656803108752, 0.06028433900792152, 0.06104202102869749, 0.0610792679945007, 0.06116258702240884, 0.06087906192988157, 0.060079612070694566, 0.06002967606764287, 0.060077628935687244, 0.06016057007946074, 0.059989869012497365, 0.06007772102020681, 0.05972431390546262, 0.06014443701133132, 0.05985251790843904, 0.06008255307096988, 0.06004303297959268, 0.060344069031998515, 0.06048442900646478, 0.06037855090107769, 0.060423668939620256, 0.06034653203096241, 0.05981909390538931, 0.06043570605106652, 0.06005064898636192, 0.05998899810947478, 0.059958993922919035, 0.059943399974144995, 0.0599585750605911, 0.060537634999491274, 0.05996331095229834, 0.05984203203115612, 0.06016641401220113, 0.06011435191612691, 0.060206196969375014, 0.05973576393444091, 0.06026124593336135, 0.060044914949685335, 0.06022960599511862, 0.06017087690997869, 0.06008617510087788, 0.060007957043126225, 0.05983484198804945, 0.06019098893739283, 0.06034661608282477, 0.060422829003073275, 0.06001020106486976, 0.06012733210809529, 0.060307536041364074, 0.06039038102608174, 0.06028234399855137, 0.061411743983626366, 0.06052050495054573, 0.06023298599757254, 0.06058201193809509]
[0.0018218727758610432, 0.0017927381856252952, 0.0018074493047458176, 0.0018188494699531977, 0.002046315857133239, 0.001999991796245532, 0.0018724926310230274, 0.0019665453060321056, 0.0018646584072967572, 0.001948526326319849, 0.0021563433894735513, 0.0018408140215110413, 0.001789463776619915, 0.0017767181235118484, 0.001825181223281032, 0.001866781406997874, 0.0017581116550659038, 0.0018637458774812367, 0.0017876411213216428, 0.0019868645313366943, 0.0018094921418066537, 0.001758988571296237, 0.0017561676926264654, 0.0017615570829306937, 0.0017673471415111301, 0.0017638817353516209, 0.0017632777536553995, 0.001772316571857248, 0.0017860023071989417, 0.0018065002466532954, 0.0017251513484979467, 0.0018084776131626294, 0.0020027843265965277, 0.0017058832870264138, 0.0017235288562785303, 0.0017554802458961398, 0.0018052978973303521, 0.0017601364691342627, 0.001733854002015171, 0.0017308534897521747, 0.0017214585095643997, 0.001741161182693833, 0.001803869856711553, 0.0017600119380014284, 0.0017822272457866644, 0.0017520251436805238, 0.0018432493872788487, 0.001786927490647198, 0.0018471772639955186, 0.0018939795305154153, 0.0018002584918725248, 0.0018007337158469825, 0.0017944837359673514, 0.001787630857766739, 0.0018001632451326872, 0.0017970469780266285, 0.0017982159802994253, 0.0018660397131034002, 0.001769792939517267, 0.0017766360198242627, 0.0017662105098252697, 0.0018554981631625975, 0.0017942731857908015, 0.0017817214711056072, 0.001745200449866908, 0.0017289299570138054, 0.0017422187755036416, 0.0017788100403220374, 0.001781922346931331, 0.0017634484280204894, 0.0017259814895271342, 0.0017116256509622444, 0.0017361021434356058, 0.0017263267559892669, 0.0017567845726651804, 0.0017412930812534628, 0.0017627973456353862, 0.001766197611482776, 0.0017790047342565898, 0.001779735777337028, 0.0018066730830172191, 0.0017669034494581272, 0.0029136750405198155, 0.0017317626316442477, 0.001761495530111145, 0.0017751218793837695, 0.001811642468222702, 0.001767686734508191, 0.003310406917934211, 0.0017523176526194628, 0.0017656398775550176, 0.0017902729367571218, 0.0017802799381885905, 0.0017244783895356314, 0.0017556403050845374, 0.0017289083061397684, 0.0017239201020411387, 0.0017501833677595975, 0.0017983225715935839, 0.0017373725101921934, 0.003974813081761252, 0.0017710580604569036, 0.0017332407546096615, 0.0017472005292431128, 0.0018056373667846225, 0.001761293285811434, 0.0017272024486708094, 0.0017355099196868892, 0.0013441324295780184, 0.0016420347335729369, 0.0014962190215722943, 0.0012423859182175022, 0.0012243434487442886, 0.0012464173047860361, 0.0014727660003403316, 0.00127550549044901, 0.0013850405907296405, 0.0013764806932826735, 0.0012697758775546538, 0.0012500858366755502, 0.0012531156523381264, 0.0012674443661330305, 0.0012545517351174233, 0.0012807933452101995, 0.0012518895732961139, 0.0012532105902209878, 0.0012564239168197525, 0.0012540576922498187, 0.0012905251006690823, 0.0013666337127892338, 0.0016589783065553223, 0.004472461999489033, 0.0013672434297219223, 0.001341751550932472, 0.0012334863256130899, 0.0012422560818721444, 0.001230651795464967, 0.0012267178152592815, 0.0012209041029861082, 0.0012247041818134639, 0.0012140005508589806, 0.0012211083881176856, 0.0012114971629058828, 0.0012206958145929538, 0.0012122550812949026, 0.001220890021483813, 0.001213995590140777, 0.0012209251005088492, 0.0012213565927112894, 0.0012185903080758087, 0.0012170582444274, 0.0012181699394761603, 0.0012209620398032119, 0.001224482510409945, 0.0012126635090086836, 0.001227406202815473, 0.0012327655508392015, 0.0012264009375049143, 0.0012137901432318042, 0.0012208138169644742, 0.0012167554909401402, 0.0012227815922767836, 0.0012242521221122267, 0.001220631246854152, 0.0012194669178249885, 0.0012147720827131855, 0.0012171505498984943, 0.0012207010200209155, 0.0012233436525780328, 0.001221054102940371, 0.0012226484060211449, 0.0012189927130785523, 0.0012186111226601868, 0.001221299551579417, 0.001224064612666107, 0.0012208097162941585, 0.001222850714468074, 0.0012165833459406787, 0.0012134757553398305, 0.001220097694051813, 0.0012289546738967908, 0.001223249489213435, 0.0012385013883895412, 0.001230292632814725, 0.001245755531197908, 0.001246515673357157, 0.0012482160616818132, 0.0012424298353037055, 0.0012261145320549911, 0.0012250954299518953, 0.0012260740599119846, 0.0012277667363155254, 0.0012242830410713749, 0.001226075939187894, 0.001218863549091074, 0.0012274374900271697, 0.0012214799573150824, 0.001226174552468773, 0.0012253680199916875, 0.0012315116128979288, 0.0012343761021727506, 0.0012322153245117894, 0.0012331361008085767, 0.0012315618781829063, 0.0012207978348038634, 0.0012333817561442147, 0.0012255234487012637, 0.0012242652675403015, 0.0012236529372024294, 0.0012233346933498979, 0.0012236443889916552, 0.0012354619387651281, 0.0012237410398428232, 0.0012212659598195127, 0.0012278860002490027, 0.001226823508492386, 0.001228697897334184, 0.0012190972231518552, 0.001229821345578803, 0.0012254064275445987, 0.0012291756325534412, 0.0012279770797954835, 0.0012262484714464874, 0.0012246521845535965, 0.0012211192242459071, 0.0012283875293345476, 0.001231563593527036, 0.0012331189592463933, 0.0012246979809157094, 0.0012270884103692916, 0.0012307660416604914, 0.0012324567556343212, 0.0012302519183377831, 0.0012533008976250279, 0.0012351123459295047, 0.001229244612195358, 0.0012363675905733692]
[548.8857472648637, 557.8059350876193, 553.265863321478, 549.7981094750694, 488.6831114141576, 500.00205094702966, 534.0474955320143, 508.5059555620908, 536.2912563967818, 513.2083598216936, 463.7480305231624, 543.2379307819184, 558.826623408316, 562.8354811980007, 547.8907996885662, 535.6813584340241, 568.7920884424798, 536.5538360580854, 559.396395659481, 503.30557731947346, 552.6412504900787, 568.5085260463508, 569.4217039743133, 567.6795885242078, 565.8197965256405, 566.9314330762969, 567.1256260829748, 564.2332842106638, 559.9096910285293, 553.5565255817652, 579.6592866305203, 552.9512738900982, 499.30488606297934, 586.2065755642262, 580.2049651545814, 569.6446897296294, 553.9252006434978, 568.1377651880925, 576.7498294768478, 577.7496512100403, 580.9027603302744, 574.3293670565597, 554.3637176924701, 568.1779642560518, 561.0956752928586, 570.7680643779315, 542.5201857662242, 559.619797240802, 541.3665593939587, 527.9888108019132, 555.4757855689145, 555.3291923173915, 557.2633398434957, 559.3996073939367, 555.505175824369, 556.4684798046399, 556.1067251963179, 535.8942754422443, 565.0378514182356, 562.8614915163747, 566.1839256629321, 538.9388250838003, 557.3287322795628, 561.2549527056395, 573.0000814956596, 578.3924304991467, 573.980727369282, 562.1735752171486, 561.1916825231534, 567.0707371479656, 579.3804893434686, 584.2399004933225, 576.0029752748768, 579.264612872754, 569.221756360782, 574.285863055376, 567.2801825325861, 566.1880604404566, 562.1120510496447, 561.8811582786035, 553.5035692954248, 565.9618811127905, 343.20917264047216, 577.4463438159162, 567.699425236068, 563.3416001537586, 551.9852937544804, 565.7110960207674, 302.07766742586097, 570.6727878390906, 566.366909080439, 558.5740472686738, 561.7094135304843, 579.885492371569, 569.5927560468305, 578.3996736256978, 580.0732869324918, 571.3687025149233, 556.0737632925609, 575.5818019069364, 251.58415740065365, 564.6342275995269, 576.9538924932604, 572.3441489759621, 553.8210597517395, 567.7646125467949, 578.9709253652128, 576.1995299804535, 743.9743123480351, 609.0005159781728, 668.3513480193261, 804.9028770663609, 816.764283768268, 802.2995157080742, 678.9944904817987, 784.00289727328, 722.0005007024373, 726.490393130883, 787.5405555237112, 799.9450682997715, 798.0109402784567, 788.9892658965358, 797.0974588038031, 780.7660804451505, 798.7924984206785, 797.9504863772837, 795.9097137622068, 797.4114796951396, 774.8783804991801, 731.7249608595181, 602.7806367621437, 223.5904967139458, 731.3986509362021, 745.2944617839524, 810.7102439931482, 804.9870027546558, 812.577533047988, 815.1834004209338, 819.065148158797, 816.52370821439, 823.7228552264149, 818.9281227864467, 825.4249622850224, 819.2049059605027, 824.9088953554422, 819.0745950930506, 823.7262211834216, 819.051061840916, 818.761699873499, 820.620345798606, 821.6533634102933, 820.9035271630671, 819.0262820629335, 816.6715257249444, 824.6310642409535, 814.7262069444984, 811.1842509869398, 815.3940276941389, 823.8656456192892, 819.1257226154904, 821.8578074608387, 817.8075351445463, 816.825212665084, 819.2482394476059, 820.0304455848427, 823.1996884275662, 821.5910513973774, 819.201412629987, 817.4317967748752, 818.9645303938134, 817.8966210362079, 820.3494485824376, 820.6063291274036, 818.7999403641584, 816.9503387749467, 819.1284740389848, 817.7613082026847, 821.974099297559, 824.0790931335524, 819.6064994427682, 813.6996597516348, 817.4947210834421, 807.4274355883675, 812.814751001272, 802.7257154045373, 802.236202379042, 801.1433522595652, 804.8744255691148, 815.5844938270009, 816.2629420952669, 815.6114158974918, 814.4869627279173, 816.804583950535, 815.6101657637632, 820.4363816981122, 814.7054396862725, 818.6790082074581, 815.5445715184723, 816.0813597916354, 812.0102072337362, 810.1258589175524, 811.5464725259811, 810.9404950064251, 811.9770656391528, 819.1364462574285, 810.7789782185442, 815.9778591423445, 816.8164420845836, 817.2251866499379, 817.4377833278536, 817.2308956722717, 809.4138464512492, 817.1663509204852, 818.8224620194827, 814.4078520295942, 815.1131707843422, 813.8697088760605, 820.2791221315383, 813.1262346315514, 816.0557815938214, 813.5533877470701, 814.3474470765753, 815.4954100129447, 816.5583768296748, 818.9208556744674, 814.0753435861795, 811.9759347027559, 810.9517678741544, 816.5278424418548, 814.937205461056, 812.5021053155215, 811.3874952840189, 812.8416506361715, 797.8929895406392, 809.6429472959669, 813.507734814521, 808.8209425938179]
Elapsed: 0.07494371240263027~0.01972010121618883
Time per graph: 0.001529463518421026~0.0004024510452283435
Speed: 686.4975698090598~135.70130527260176
Total Time: 0.0615
best val loss: 0.09908872097730637 test_score: 0.9184

Testing...
Test loss: 0.4773 score: 0.8367 time: 0.06s
test Score 0.8367
Epoch Time List: [0.2960375288967043, 0.2918600750854239, 0.29277693398762494, 0.2927815979346633, 0.30409738793969154, 0.3104192039463669, 0.32033257791772485, 0.3290333009790629, 0.3091641031205654, 0.30193044198676944, 0.3208488639211282, 0.30565969506278634, 0.2916041340213269, 0.2957474540453404, 0.30196137889288366, 0.30021943897008896, 0.2951262620044872, 0.2998928229790181, 0.29568658594507724, 0.3069705858360976, 0.2979743389878422, 0.2885621738387272, 0.28888130094856024, 0.28964159300085157, 0.2885669590905309, 0.2897102510323748, 0.2910154699347913, 0.2922992929816246, 0.29185622406657785, 0.295863519073464, 0.28707995906006545, 0.2886653409805149, 0.4546450270572677, 0.3105903510004282, 0.28287940099835396, 0.28493878699373454, 0.30975358188152313, 0.28936929802875966, 0.40391082293353975, 0.28533446590881795, 0.28319640981499106, 0.28394166892394423, 0.29162246792111546, 0.2902432889677584, 0.28768880700226873, 0.32278659602161497, 0.2979170639300719, 0.30433345201890916, 0.3020322909578681, 0.311315402854234, 0.29849165899213403, 0.3555557099170983, 0.29479566297959536, 0.29484651889652014, 0.29743725690059364, 0.3010118049569428, 0.2964330349350348, 0.2963992148870602, 0.4084675050107762, 0.29133505397476256, 0.2906328940298408, 0.2957895799772814, 0.29979483713395894, 0.29198099102359265, 0.36175527796149254, 0.2845320908818394, 0.2864920728607103, 0.29007627407554537, 0.29827095416840166, 0.2904343269765377, 0.4193204010371119, 0.2830705359810963, 0.28517436888068914, 0.28524472704157233, 0.29327804397325963, 0.28866112895775586, 0.3605032409541309, 0.29059806815348566, 0.2912651940714568, 0.29320360301062465, 0.29667466203682125, 0.29112206504214555, 0.3421482889680192, 0.30942401592619717, 0.2895712190074846, 0.28895802109036595, 0.2936482379445806, 0.2922086529433727, 0.365514682023786, 0.28995940589811653, 0.28885858808644116, 0.2913345699198544, 0.29369096108712256, 0.2869902910897508, 0.28466465999372303, 0.4097789260558784, 0.28369828208815306, 0.2842420799424872, 0.2914394789841026, 0.28874245495535433, 0.3959725451422855, 0.288033566204831, 0.2826918389182538, 0.2832830658880994, 0.2872461940860376, 0.29558343603275716, 0.2846542219631374, 0.35946885985322297, 0.2815325401024893, 0.2836698959581554, 0.3872681569773704, 0.2503085680073127, 0.24457725090906024, 0.24344502412714064, 0.2684159668860957, 0.2598439180292189, 0.2580243539996445, 0.3006749090272933, 0.24437021801713854, 0.24344728712458163, 0.24274574406445026, 0.24601774709299207, 0.24424833594821393, 0.24102856393437833, 0.3661663378588855, 0.24262640497181565, 0.24346831697039306, 0.24493072589393705, 0.2607748882146552, 0.25972073106095195, 0.27936131809838116, 0.4250950310379267, 0.2765386290848255, 0.25451642309781164, 0.2381679309764877, 0.24150929076131433, 0.243101847008802, 0.2428662059828639, 0.24286339501850307, 0.24360684806015342, 0.24155882792547345, 0.24338656093459576, 0.24256206711288542, 0.24329125101212412, 0.24316025304142386, 0.24341469898354262, 0.24445268185809255, 0.24401928903535008, 0.2444628238445148, 0.24464596586767584, 0.24451460305135697, 0.2439916100120172, 0.24502875399775803, 0.24518038099631667, 0.24642326193861663, 0.24708860914688557, 0.2460101390024647, 0.24648384505417198, 0.24540663685183972, 0.24591043998952955, 0.24713397189043462, 0.24711977690458298, 0.24620212998706847, 0.24510469811502844, 0.24611951992847025, 0.2455482081277296, 0.24652928509749472, 0.24590296996757388, 0.24667141307145357, 0.24578817503061146, 0.24650520691648126, 0.24553992715664208, 0.24617152009159327, 0.24587752704974264, 0.24692985403817147, 0.24630098207853734, 0.246008921880275, 0.246055677998811, 0.24521226389333606, 0.24514717899728566, 0.24828959594015032, 0.24656799109652638, 0.24824353901203722, 0.24734834302216768, 0.24863031809218228, 0.24624393496196717, 0.24717366613913327, 0.2466532139806077, 0.2470379580045119, 0.2456468950258568, 0.24587113608140498, 0.24545336898881942, 0.24705889611504972, 0.24605185899417847, 0.24616109498310834, 0.2467163548571989, 0.24715761095285416, 0.24638434499502182, 0.2465628630015999, 0.24655899696517736, 0.2474737949669361, 0.2464913601288572, 0.2472797540249303, 0.24675406306050718, 0.24687248596455902, 0.24610393901821226, 0.24683470593299717, 0.24616627220530063, 0.24609959789086133, 0.24542142008431256, 0.24682442797347903, 0.24631561001297086, 0.2464002671185881, 0.24443836696445942, 0.24511409306433052, 0.24505635688547045, 0.2452944389078766, 0.24452018016017973, 0.24535074492450804, 0.24590133095625788, 0.24562190694268793, 0.246403401135467, 0.24615397595334798, 0.24632343091070652, 0.24548978498205543, 0.24636943207588047, 0.2460976701695472, 0.24660390196368098, 0.24684233509469777, 0.24663156899623573, 0.24719043297227472, 0.24719430785626173, 0.24348931899294257, 0.24149995599873364, 0.3451675210380927, 0.24006022803951055, 0.23984642408322543]
Total Epoch List: [108, 129]
Total Time List: [0.08562453405465931, 0.06150296097621322]
========================k_idx:2========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: I2BGNNA
I2BGNN(
  (gcs): ModuleList(
    (0): GCNConv(14887, 64)
    (1): GCNConv(64, 64)
  )
  (bns): ModuleList(
    (0-1): 2 x BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  )
  (drops): ModuleList(
    (0-1): 2 x Dropout(p=0.2, inplace=False)
  )
  (lin1): Linear(in_features=64, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=64, bias=True)
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=64, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=64, out_features=2, bias=True)
  )
)
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x759998c753f0>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.6948;  Loss pred: 0.6948; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6934 score: 0.5102 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6944 score: 0.5000 time: 0.07s
Epoch 2/1000, LR 0.000000
Train loss: 0.6968;  Loss pred: 0.6968; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6934 score: 0.5102 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6944 score: 0.5000 time: 0.21s
Epoch 3/1000, LR 0.000030
Train loss: 0.6967;  Loss pred: 0.6967; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6933 score: 0.5102 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6943 score: 0.5000 time: 0.09s
Epoch 4/1000, LR 0.000060
Train loss: 0.6935;  Loss pred: 0.6935; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6932 score: 0.5102 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6942 score: 0.5000 time: 0.08s
Epoch 5/1000, LR 0.000090
Train loss: 0.6958;  Loss pred: 0.6958; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6932 score: 0.5102 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6941 score: 0.5000 time: 0.08s
Epoch 6/1000, LR 0.000120
Train loss: 0.6907;  Loss pred: 0.6907; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6931 score: 0.5102 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6940 score: 0.5000 time: 0.08s
Epoch 7/1000, LR 0.000150
Train loss: 0.6896;  Loss pred: 0.6896; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6930 score: 0.5102 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6939 score: 0.5000 time: 0.08s
Epoch 8/1000, LR 0.000180
Train loss: 0.6878;  Loss pred: 0.6878; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6930 score: 0.5102 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6937 score: 0.5000 time: 0.20s
Epoch 9/1000, LR 0.000210
Train loss: 0.6837;  Loss pred: 0.6837; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6929 score: 0.5102 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6936 score: 0.5000 time: 0.08s
Epoch 10/1000, LR 0.000240
Train loss: 0.6811;  Loss pred: 0.6811; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6929 score: 0.5102 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6935 score: 0.5000 time: 0.08s
Epoch 11/1000, LR 0.000270
Train loss: 0.6784;  Loss pred: 0.6784; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6928 score: 0.5102 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6934 score: 0.5000 time: 0.08s
Epoch 12/1000, LR 0.000270
Train loss: 0.6746;  Loss pred: 0.6746; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6928 score: 0.5102 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6933 score: 0.5000 time: 0.08s
Epoch 13/1000, LR 0.000270
Train loss: 0.6690;  Loss pred: 0.6690; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6927 score: 0.5102 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6932 score: 0.5000 time: 0.08s
Epoch 14/1000, LR 0.000270
Train loss: 0.6662;  Loss pred: 0.6662; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6927 score: 0.5102 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6931 score: 0.5000 time: 0.08s
Epoch 15/1000, LR 0.000270
Train loss: 0.6618;  Loss pred: 0.6618; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6926 score: 0.5102 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6930 score: 0.5000 time: 0.08s
Epoch 16/1000, LR 0.000270
Train loss: 0.6548;  Loss pred: 0.6548; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6926 score: 0.5102 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6929 score: 0.5000 time: 0.09s
Epoch 17/1000, LR 0.000270
Train loss: 0.6482;  Loss pred: 0.6482; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6926 score: 0.5102 time: 0.09s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6928 score: 0.5000 time: 0.09s
Epoch 18/1000, LR 0.000270
Train loss: 0.6447;  Loss pred: 0.6447; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6925 score: 0.4898 time: 0.08s
Test loss: 0.6927 score: 0.5208 time: 0.08s
Epoch 19/1000, LR 0.000270
Train loss: 0.6355;  Loss pred: 0.6355; Loss self: 0.0000; time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6925 score: 0.4898 time: 0.09s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6926 score: 0.5000 time: 0.08s
Epoch 20/1000, LR 0.000270
Train loss: 0.6299;  Loss pred: 0.6299; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6924 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6925 score: 0.5000 time: 0.09s
Epoch 21/1000, LR 0.000270
Train loss: 0.6182;  Loss pred: 0.6182; Loss self: 0.0000; time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6924 score: 0.4898 time: 0.09s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6924 score: 0.5000 time: 0.08s
Epoch 22/1000, LR 0.000270
Train loss: 0.6131;  Loss pred: 0.6131; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6923 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6924 score: 0.5000 time: 0.08s
Epoch 23/1000, LR 0.000270
Train loss: 0.6046;  Loss pred: 0.6046; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6922 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6923 score: 0.5000 time: 0.08s
Epoch 24/1000, LR 0.000270
Train loss: 0.5964;  Loss pred: 0.5964; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6922 score: 0.4898 time: 0.09s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6921 score: 0.5000 time: 0.08s
Epoch 25/1000, LR 0.000270
Train loss: 0.5840;  Loss pred: 0.5840; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6920 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6920 score: 0.5000 time: 0.08s
Epoch 26/1000, LR 0.000270
Train loss: 0.5778;  Loss pred: 0.5778; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6919 score: 0.4898 time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6919 score: 0.5000 time: 0.09s
Epoch 27/1000, LR 0.000270
Train loss: 0.5618;  Loss pred: 0.5618; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6917 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6917 score: 0.5000 time: 0.08s
Epoch 28/1000, LR 0.000270
Train loss: 0.5543;  Loss pred: 0.5543; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6915 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6915 score: 0.5000 time: 0.08s
Epoch 29/1000, LR 0.000270
Train loss: 0.5388;  Loss pred: 0.5388; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6911 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6912 score: 0.5000 time: 0.08s
Epoch 30/1000, LR 0.000270
Train loss: 0.5271;  Loss pred: 0.5271; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6907 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6909 score: 0.5000 time: 0.08s
Epoch 31/1000, LR 0.000270
Train loss: 0.5125;  Loss pred: 0.5125; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6902 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6905 score: 0.5000 time: 0.08s
Epoch 32/1000, LR 0.000270
Train loss: 0.4977;  Loss pred: 0.4977; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6896 score: 0.4898 time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6900 score: 0.5000 time: 0.07s
Epoch 33/1000, LR 0.000270
Train loss: 0.4783;  Loss pred: 0.4783; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6888 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6894 score: 0.5000 time: 0.07s
Epoch 34/1000, LR 0.000270
Train loss: 0.4689;  Loss pred: 0.4689; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6878 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6886 score: 0.5000 time: 0.07s
Epoch 35/1000, LR 0.000270
Train loss: 0.4458;  Loss pred: 0.4458; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6866 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6876 score: 0.5000 time: 0.07s
Epoch 36/1000, LR 0.000270
Train loss: 0.4339;  Loss pred: 0.4339; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6851 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6864 score: 0.5000 time: 0.07s
Epoch 37/1000, LR 0.000270
Train loss: 0.4315;  Loss pred: 0.4315; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6835 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6852 score: 0.5000 time: 0.07s
Epoch 38/1000, LR 0.000270
Train loss: 0.4001;  Loss pred: 0.4001; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6816 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6837 score: 0.5000 time: 0.07s
Epoch 39/1000, LR 0.000269
Train loss: 0.3918;  Loss pred: 0.3918; Loss self: 0.0000; time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6792 score: 0.4898 time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6820 score: 0.5000 time: 0.07s
Epoch 40/1000, LR 0.000269
Train loss: 0.3765;  Loss pred: 0.3765; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6764 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6798 score: 0.5000 time: 0.07s
Epoch 41/1000, LR 0.000269
Train loss: 0.3601;  Loss pred: 0.3601; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6734 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6773 score: 0.5000 time: 0.07s
Epoch 42/1000, LR 0.000269
Train loss: 0.3341;  Loss pred: 0.3341; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6704 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6748 score: 0.5000 time: 0.07s
Epoch 43/1000, LR 0.000269
Train loss: 0.3252;  Loss pred: 0.3252; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6670 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6719 score: 0.5000 time: 0.07s
Epoch 44/1000, LR 0.000269
Train loss: 0.3118;  Loss pred: 0.3118; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6633 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6688 score: 0.5000 time: 0.07s
Epoch 45/1000, LR 0.000269
Train loss: 0.2853;  Loss pred: 0.2853; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6591 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6652 score: 0.5000 time: 0.07s
Epoch 46/1000, LR 0.000269
Train loss: 0.2846;  Loss pred: 0.2846; Loss self: 0.0000; time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6544 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6611 score: 0.5000 time: 0.07s
Epoch 47/1000, LR 0.000269
Train loss: 0.2667;  Loss pred: 0.2667; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6491 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6564 score: 0.5000 time: 0.07s
Epoch 48/1000, LR 0.000269
Train loss: 0.2396;  Loss pred: 0.2396; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6432 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6512 score: 0.5000 time: 0.07s
Epoch 49/1000, LR 0.000269
Train loss: 0.2389;  Loss pred: 0.2389; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6369 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6454 score: 0.5000 time: 0.07s
Epoch 50/1000, LR 0.000269
Train loss: 0.2178;  Loss pred: 0.2178; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6301 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6391 score: 0.5000 time: 0.07s
Epoch 51/1000, LR 0.000269
Train loss: 0.1925;  Loss pred: 0.1925; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6230 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6324 score: 0.5000 time: 0.07s
Epoch 52/1000, LR 0.000269
Train loss: 0.1805;  Loss pred: 0.1805; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6156 score: 0.4898 time: 0.17s
Test loss: 0.6254 score: 0.5208 time: 0.07s
Epoch 53/1000, LR 0.000269
Train loss: 0.1797;  Loss pred: 0.1797; Loss self: 0.0000; time: 0.13s
Val loss: 0.6079 score: 0.5510 time: 0.08s
Test loss: 0.6180 score: 0.5625 time: 0.07s
Epoch 54/1000, LR 0.000269
Train loss: 0.1536;  Loss pred: 0.1536; Loss self: 0.0000; time: 0.13s
Val loss: 0.5995 score: 0.5918 time: 0.08s
Test loss: 0.6099 score: 0.6042 time: 0.07s
Epoch 55/1000, LR 0.000269
Train loss: 0.1452;  Loss pred: 0.1452; Loss self: 0.0000; time: 0.13s
Val loss: 0.5905 score: 0.6327 time: 0.08s
Test loss: 0.6013 score: 0.6458 time: 0.07s
Epoch 56/1000, LR 0.000269
Train loss: 0.1399;  Loss pred: 0.1399; Loss self: 0.0000; time: 0.13s
Val loss: 0.5811 score: 0.6735 time: 0.08s
Test loss: 0.5922 score: 0.7083 time: 0.08s
Epoch 57/1000, LR 0.000269
Train loss: 0.1240;  Loss pred: 0.1240; Loss self: 0.0000; time: 0.14s
Val loss: 0.5712 score: 0.7143 time: 0.08s
Test loss: 0.5824 score: 0.7292 time: 0.07s
Epoch 58/1000, LR 0.000269
Train loss: 0.1123;  Loss pred: 0.1123; Loss self: 0.0000; time: 0.13s
Val loss: 0.5606 score: 0.7551 time: 0.08s
Test loss: 0.5718 score: 0.7708 time: 0.07s
Epoch 59/1000, LR 0.000268
Train loss: 0.1018;  Loss pred: 0.1018; Loss self: 0.0000; time: 0.15s
Val loss: 0.5491 score: 0.7755 time: 0.15s
Test loss: 0.5604 score: 0.8125 time: 0.07s
Epoch 60/1000, LR 0.000268
Train loss: 0.0968;  Loss pred: 0.0968; Loss self: 0.0000; time: 0.13s
Val loss: 0.5367 score: 0.8163 time: 0.08s
Test loss: 0.5480 score: 0.8125 time: 0.07s
Epoch 61/1000, LR 0.000268
Train loss: 0.0924;  Loss pred: 0.0924; Loss self: 0.0000; time: 0.13s
Val loss: 0.5237 score: 0.8163 time: 0.08s
Test loss: 0.5348 score: 0.8542 time: 0.07s
Epoch 62/1000, LR 0.000268
Train loss: 0.0785;  Loss pred: 0.0785; Loss self: 0.0000; time: 0.13s
Val loss: 0.5102 score: 0.8163 time: 0.08s
Test loss: 0.5209 score: 0.8750 time: 0.07s
Epoch 63/1000, LR 0.000268
Train loss: 0.0733;  Loss pred: 0.0733; Loss self: 0.0000; time: 0.14s
Val loss: 0.4965 score: 0.8163 time: 0.08s
Test loss: 0.5067 score: 0.8750 time: 0.08s
Epoch 64/1000, LR 0.000268
Train loss: 0.0676;  Loss pred: 0.0676; Loss self: 0.0000; time: 0.13s
Val loss: 0.4818 score: 0.8367 time: 0.08s
Test loss: 0.4916 score: 0.8750 time: 0.07s
Epoch 65/1000, LR 0.000268
Train loss: 0.0632;  Loss pred: 0.0632; Loss self: 0.0000; time: 0.13s
Val loss: 0.4674 score: 0.8571 time: 0.08s
Test loss: 0.4766 score: 0.8958 time: 0.19s
Epoch 66/1000, LR 0.000268
Train loss: 0.0557;  Loss pred: 0.0557; Loss self: 0.0000; time: 0.14s
Val loss: 0.4525 score: 0.8776 time: 0.08s
Test loss: 0.4612 score: 0.9167 time: 0.08s
Epoch 67/1000, LR 0.000268
Train loss: 0.0497;  Loss pred: 0.0497; Loss self: 0.0000; time: 0.14s
Val loss: 0.4376 score: 0.8980 time: 0.08s
Test loss: 0.4457 score: 0.8958 time: 0.08s
Epoch 68/1000, LR 0.000268
Train loss: 0.0445;  Loss pred: 0.0445; Loss self: 0.0000; time: 0.14s
Val loss: 0.4225 score: 0.8980 time: 0.08s
Test loss: 0.4300 score: 0.9167 time: 0.08s
Epoch 69/1000, LR 0.000268
Train loss: 0.0413;  Loss pred: 0.0413; Loss self: 0.0000; time: 0.14s
Val loss: 0.4073 score: 0.8980 time: 0.08s
Test loss: 0.4140 score: 0.9167 time: 0.08s
Epoch 70/1000, LR 0.000268
Train loss: 0.0376;  Loss pred: 0.0376; Loss self: 0.0000; time: 0.14s
Val loss: 0.3928 score: 0.8980 time: 0.08s
Test loss: 0.3985 score: 0.9167 time: 0.08s
Epoch 71/1000, LR 0.000268
Train loss: 0.0354;  Loss pred: 0.0354; Loss self: 0.0000; time: 0.14s
Val loss: 0.3783 score: 0.8980 time: 0.08s
Test loss: 0.3830 score: 0.9167 time: 0.08s
Epoch 72/1000, LR 0.000267
Train loss: 0.0316;  Loss pred: 0.0316; Loss self: 0.0000; time: 0.15s
Val loss: 0.3646 score: 0.8980 time: 0.09s
Test loss: 0.3680 score: 0.9167 time: 0.08s
Epoch 73/1000, LR 0.000267
Train loss: 0.0296;  Loss pred: 0.0296; Loss self: 0.0000; time: 0.13s
Val loss: 0.3507 score: 0.8980 time: 0.08s
Test loss: 0.3531 score: 0.9167 time: 0.08s
Epoch 74/1000, LR 0.000267
Train loss: 0.0281;  Loss pred: 0.0281; Loss self: 0.0000; time: 0.13s
Val loss: 0.3375 score: 0.8980 time: 0.08s
Test loss: 0.3387 score: 0.9167 time: 0.08s
Epoch 75/1000, LR 0.000267
Train loss: 0.0255;  Loss pred: 0.0255; Loss self: 0.0000; time: 0.13s
Val loss: 0.3249 score: 0.8980 time: 0.08s
Test loss: 0.3249 score: 0.9167 time: 0.08s
Epoch 76/1000, LR 0.000267
Train loss: 0.0235;  Loss pred: 0.0235; Loss self: 0.0000; time: 0.13s
Val loss: 0.3129 score: 0.9184 time: 0.08s
Test loss: 0.3115 score: 0.9375 time: 0.08s
Epoch 77/1000, LR 0.000267
Train loss: 0.0204;  Loss pred: 0.0204; Loss self: 0.0000; time: 0.13s
Val loss: 0.3018 score: 0.8980 time: 0.08s
Test loss: 0.2988 score: 0.9375 time: 0.08s
Epoch 78/1000, LR 0.000267
Train loss: 0.0187;  Loss pred: 0.0187; Loss self: 0.0000; time: 0.13s
Val loss: 0.2916 score: 0.8980 time: 0.08s
Test loss: 0.2869 score: 0.9375 time: 0.08s
Epoch 79/1000, LR 0.000267
Train loss: 0.0183;  Loss pred: 0.0183; Loss self: 0.0000; time: 0.18s
Val loss: 0.2821 score: 0.8980 time: 0.08s
Test loss: 0.2755 score: 0.9375 time: 0.08s
Epoch 80/1000, LR 0.000267
Train loss: 0.0168;  Loss pred: 0.0168; Loss self: 0.0000; time: 0.13s
Val loss: 0.2732 score: 0.8980 time: 0.08s
Test loss: 0.2649 score: 0.9375 time: 0.08s
Epoch 81/1000, LR 0.000267
Train loss: 0.0154;  Loss pred: 0.0154; Loss self: 0.0000; time: 0.13s
Val loss: 0.2647 score: 0.8980 time: 0.08s
Test loss: 0.2548 score: 0.9375 time: 0.08s
Epoch 82/1000, LR 0.000267
Train loss: 0.0136;  Loss pred: 0.0136; Loss self: 0.0000; time: 0.13s
Val loss: 0.2570 score: 0.8980 time: 0.08s
Test loss: 0.2455 score: 0.9375 time: 0.08s
Epoch 83/1000, LR 0.000266
Train loss: 0.0129;  Loss pred: 0.0129; Loss self: 0.0000; time: 0.14s
Val loss: 0.2495 score: 0.8980 time: 0.08s
Test loss: 0.2369 score: 0.9375 time: 0.08s
Epoch 84/1000, LR 0.000266
Train loss: 0.0122;  Loss pred: 0.0122; Loss self: 0.0000; time: 0.13s
Val loss: 0.2424 score: 0.8980 time: 0.08s
Test loss: 0.2290 score: 0.9375 time: 0.08s
Epoch 85/1000, LR 0.000266
Train loss: 0.0111;  Loss pred: 0.0111; Loss self: 0.0000; time: 0.15s
Val loss: 0.2358 score: 0.8980 time: 0.12s
Test loss: 0.2217 score: 0.9375 time: 0.08s
Epoch 86/1000, LR 0.000266
Train loss: 0.0111;  Loss pred: 0.0111; Loss self: 0.0000; time: 0.13s
Val loss: 0.2301 score: 0.8980 time: 0.08s
Test loss: 0.2151 score: 0.9583 time: 0.08s
Epoch 87/1000, LR 0.000266
Train loss: 0.0099;  Loss pred: 0.0099; Loss self: 0.0000; time: 0.13s
Val loss: 0.2249 score: 0.8980 time: 0.08s
Test loss: 0.2091 score: 0.9583 time: 0.08s
Epoch 88/1000, LR 0.000266
Train loss: 0.0094;  Loss pred: 0.0094; Loss self: 0.0000; time: 0.13s
Val loss: 0.2201 score: 0.9184 time: 0.08s
Test loss: 0.2037 score: 0.9375 time: 0.08s
Epoch 89/1000, LR 0.000266
Train loss: 0.0088;  Loss pred: 0.0088; Loss self: 0.0000; time: 0.14s
Val loss: 0.2159 score: 0.9184 time: 0.08s
Test loss: 0.1991 score: 0.9375 time: 0.08s
Epoch 90/1000, LR 0.000266
Train loss: 0.0080;  Loss pred: 0.0080; Loss self: 0.0000; time: 0.14s
Val loss: 0.2121 score: 0.9184 time: 0.08s
Test loss: 0.1950 score: 0.9375 time: 0.08s
Epoch 91/1000, LR 0.000266
Train loss: 0.0080;  Loss pred: 0.0080; Loss self: 0.0000; time: 0.13s
Val loss: 0.2091 score: 0.9184 time: 0.09s
Test loss: 0.1916 score: 0.9375 time: 0.20s
Epoch 92/1000, LR 0.000266
Train loss: 0.0076;  Loss pred: 0.0076; Loss self: 0.0000; time: 0.13s
Val loss: 0.2068 score: 0.9184 time: 0.08s
Test loss: 0.1887 score: 0.9375 time: 0.07s
Epoch 93/1000, LR 0.000265
Train loss: 0.0071;  Loss pred: 0.0071; Loss self: 0.0000; time: 0.13s
Val loss: 0.2047 score: 0.9184 time: 0.08s
Test loss: 0.1862 score: 0.9167 time: 0.08s
Epoch 94/1000, LR 0.000265
Train loss: 0.0074;  Loss pred: 0.0074; Loss self: 0.0000; time: 0.14s
Val loss: 0.2034 score: 0.9184 time: 0.09s
Test loss: 0.1840 score: 0.9167 time: 0.07s
Epoch 95/1000, LR 0.000265
Train loss: 0.0062;  Loss pred: 0.0062; Loss self: 0.0000; time: 0.14s
Val loss: 0.2024 score: 0.9184 time: 0.09s
Test loss: 0.1824 score: 0.9167 time: 0.08s
Epoch 96/1000, LR 0.000265
Train loss: 0.0057;  Loss pred: 0.0057; Loss self: 0.0000; time: 0.15s
Val loss: 0.2017 score: 0.9184 time: 0.08s
Test loss: 0.1813 score: 0.9167 time: 0.08s
Epoch 97/1000, LR 0.000265
Train loss: 0.0058;  Loss pred: 0.0058; Loss self: 0.0000; time: 0.14s
Val loss: 0.2014 score: 0.9184 time: 0.21s
Test loss: 0.1804 score: 0.9167 time: 0.08s
Epoch 98/1000, LR 0.000265
Train loss: 0.0060;  Loss pred: 0.0060; Loss self: 0.0000; time: 0.13s
Val loss: 0.2014 score: 0.9184 time: 0.08s
Test loss: 0.1799 score: 0.9167 time: 0.07s
Epoch 99/1000, LR 0.000265
Train loss: 0.0052;  Loss pred: 0.0052; Loss self: 0.0000; time: 0.13s
Val loss: 0.2016 score: 0.8980 time: 0.08s
Test loss: 0.1798 score: 0.9167 time: 0.07s
     INFO: Early stopping counter 1 of 20
Epoch 100/1000, LR 0.000265
Train loss: 0.0050;  Loss pred: 0.0050; Loss self: 0.0000; time: 0.13s
Val loss: 0.2020 score: 0.8980 time: 0.08s
Test loss: 0.1798 score: 0.9167 time: 0.07s
     INFO: Early stopping counter 2 of 20
Epoch 101/1000, LR 0.000265
Train loss: 0.0048;  Loss pred: 0.0048; Loss self: 0.0000; time: 0.13s
Val loss: 0.2026 score: 0.8980 time: 0.08s
Test loss: 0.1803 score: 0.9167 time: 0.07s
     INFO: Early stopping counter 3 of 20
Epoch 102/1000, LR 0.000264
Train loss: 0.0047;  Loss pred: 0.0047; Loss self: 0.0000; time: 0.13s
Val loss: 0.2032 score: 0.8980 time: 0.08s
Test loss: 0.1810 score: 0.9167 time: 0.07s
     INFO: Early stopping counter 4 of 20
Epoch 103/1000, LR 0.000264
Train loss: 0.0044;  Loss pred: 0.0044; Loss self: 0.0000; time: 0.13s
Val loss: 0.2041 score: 0.8980 time: 0.08s
Test loss: 0.1819 score: 0.9167 time: 0.07s
     INFO: Early stopping counter 5 of 20
Epoch 104/1000, LR 0.000264
Train loss: 0.0042;  Loss pred: 0.0042; Loss self: 0.0000; time: 0.13s
Val loss: 0.2049 score: 0.8980 time: 0.08s
Test loss: 0.1830 score: 0.9167 time: 0.07s
     INFO: Early stopping counter 6 of 20
Epoch 105/1000, LR 0.000264
Train loss: 0.0042;  Loss pred: 0.0042; Loss self: 0.0000; time: 0.13s
Val loss: 0.2058 score: 0.8980 time: 0.08s
Test loss: 0.1842 score: 0.9167 time: 0.08s
     INFO: Early stopping counter 7 of 20
Epoch 106/1000, LR 0.000264
Train loss: 0.0039;  Loss pred: 0.0039; Loss self: 0.0000; time: 0.13s
Val loss: 0.2067 score: 0.8980 time: 0.08s
Test loss: 0.1856 score: 0.9167 time: 0.07s
     INFO: Early stopping counter 8 of 20
Epoch 107/1000, LR 0.000264
Train loss: 0.0036;  Loss pred: 0.0036; Loss self: 0.0000; time: 0.13s
Val loss: 0.2078 score: 0.8980 time: 0.08s
Test loss: 0.1869 score: 0.9167 time: 0.07s
     INFO: Early stopping counter 9 of 20
Epoch 108/1000, LR 0.000264
Train loss: 0.0034;  Loss pred: 0.0034; Loss self: 0.0000; time: 0.14s
Val loss: 0.2089 score: 0.8980 time: 0.08s
Test loss: 0.1883 score: 0.9167 time: 0.09s
     INFO: Early stopping counter 10 of 20
Epoch 109/1000, LR 0.000264
Train loss: 0.0037;  Loss pred: 0.0037; Loss self: 0.0000; time: 0.13s
Val loss: 0.2100 score: 0.8980 time: 0.09s
Test loss: 0.1900 score: 0.9167 time: 0.08s
     INFO: Early stopping counter 11 of 20
Epoch 110/1000, LR 0.000263
Train loss: 0.0034;  Loss pred: 0.0034; Loss self: 0.0000; time: 0.13s
Val loss: 0.2112 score: 0.8980 time: 0.09s
Test loss: 0.1916 score: 0.9167 time: 0.08s
     INFO: Early stopping counter 12 of 20
Epoch 111/1000, LR 0.000263
Train loss: 0.0035;  Loss pred: 0.0035; Loss self: 0.0000; time: 0.13s
Val loss: 0.2124 score: 0.8980 time: 0.08s
Test loss: 0.1930 score: 0.9167 time: 0.07s
     INFO: Early stopping counter 13 of 20
Epoch 112/1000, LR 0.000263
Train loss: 0.0031;  Loss pred: 0.0031; Loss self: 0.0000; time: 0.13s
Val loss: 0.2135 score: 0.8980 time: 0.08s
Test loss: 0.1944 score: 0.9167 time: 0.08s
     INFO: Early stopping counter 14 of 20
Epoch 113/1000, LR 0.000263
Train loss: 0.0030;  Loss pred: 0.0030; Loss self: 0.0000; time: 0.13s
Val loss: 0.2147 score: 0.8980 time: 0.08s
Test loss: 0.1959 score: 0.9167 time: 0.08s
     INFO: Early stopping counter 15 of 20
Epoch 114/1000, LR 0.000263
Train loss: 0.0029;  Loss pred: 0.0029; Loss self: 0.0000; time: 0.13s
Val loss: 0.2158 score: 0.8980 time: 0.08s
Test loss: 0.1972 score: 0.9167 time: 0.08s
     INFO: Early stopping counter 16 of 20
Epoch 115/1000, LR 0.000263
Train loss: 0.0030;  Loss pred: 0.0030; Loss self: 0.0000; time: 0.13s
Val loss: 0.2169 score: 0.8980 time: 0.08s
Test loss: 0.1988 score: 0.9167 time: 0.08s
     INFO: Early stopping counter 17 of 20
Epoch 116/1000, LR 0.000263
Train loss: 0.0030;  Loss pred: 0.0030; Loss self: 0.0000; time: 0.14s
Val loss: 0.2181 score: 0.8980 time: 0.08s
Test loss: 0.2001 score: 0.9167 time: 0.08s
     INFO: Early stopping counter 18 of 20
Epoch 117/1000, LR 0.000262
Train loss: 0.0028;  Loss pred: 0.0028; Loss self: 0.0000; time: 0.13s
Val loss: 0.2192 score: 0.8980 time: 0.08s
Test loss: 0.2014 score: 0.9167 time: 0.08s
     INFO: Early stopping counter 19 of 20
Epoch 118/1000, LR 0.000262
Train loss: 0.0030;  Loss pred: 0.0030; Loss self: 0.0000; time: 0.13s
Val loss: 0.2202 score: 0.8980 time: 0.08s
Test loss: 0.2027 score: 0.9167 time: 0.08s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 097,   Train_Loss: 0.0060,   Val_Loss: 0.2014,   Val_Precision: 0.9565,   Val_Recall: 0.8800,   Val_accuracy: 0.9167,   Val_Score: 0.9184,   Val_Loss: 0.2014,   Test_Precision: 0.8846,   Test_Recall: 0.9583,   Test_accuracy: 0.9200,   Test_Score: 0.9167,   Test_loss: 0.1799


[0.08927176601719111, 0.08784417109563947, 0.08856501593254507, 0.08912362402770668, 0.1002694769995287, 0.09799959801603109, 0.09175213892012835, 0.09636071999557316, 0.09136826195754111, 0.0954777899896726, 0.10566082608420402, 0.09019988705404103, 0.08768372505437583, 0.08705918805208057, 0.08943387994077057, 0.09147228894289583, 0.08614747109822929, 0.0913235479965806, 0.0875944149447605, 0.09735636203549802, 0.08866511494852602, 0.08619043999351561, 0.0860522169386968, 0.086316297063604, 0.08660000993404537, 0.08643020503222942, 0.08640060992911458, 0.08684351202100515, 0.08751411305274814, 0.08851851208601147, 0.08453241607639939, 0.08861540304496884, 0.09813643200322986, 0.08358828106429428, 0.08445291395764798, 0.08601853204891086, 0.08845959696918726, 0.08624668698757887, 0.08495884609874338, 0.08481182099785656, 0.08435146696865559, 0.08531689795199782, 0.0883896229788661, 0.08624058496206999, 0.08732913504354656, 0.08584923204034567, 0.09031921997666359, 0.0875594470417127, 0.0905116859357804, 0.09280499699525535, 0.08821266610175371, 0.08823595207650214, 0.08792970306240022, 0.08759391203057021, 0.08820799901150167, 0.0880553019233048, 0.08811258303467184, 0.09143594594206661, 0.08671985403634608, 0.08705516497138888, 0.08654431498143822, 0.09091940999496728, 0.08791938610374928, 0.08730435208417475, 0.08551482204347849, 0.08471756789367646, 0.08536871999967843, 0.08716169197577983, 0.08731419499963522, 0.08640897297300398, 0.08457309298682958, 0.08386965689714998, 0.08506900502834469, 0.08459001104347408, 0.08608244406059384, 0.08532336098141968, 0.08637706993613392, 0.08654368296265602, 0.0871712319785729, 0.08720705308951437, 0.08852698106784374, 0.08657826902344823, 0.14277007698547095, 0.08485636895056814, 0.0863132809754461, 0.08698097208980471, 0.0887704809429124, 0.08661664999090135, 0.16220993897877634, 0.08586356497835368, 0.08651635400019586, 0.08772337390109897, 0.08723371697124094, 0.08449944108724594, 0.08602637494914234, 0.08471650700084865, 0.0844720850000158, 0.08575898502022028, 0.08811780600808561, 0.08513125299941748, 0.19476584100630134, 0.08678184496238828, 0.08492879697587341, 0.08561282593291253, 0.0884762309724465, 0.08630337100476027, 0.08463291998486966, 0.08503998606465757, 0.0658624890493229, 0.0804597019450739, 0.07331473205704242, 0.0608769099926576, 0.05999282898847014, 0.061074447934515774, 0.07216553401667625, 0.062499769032001495, 0.06786698894575238, 0.067447553970851, 0.06221901800017804, 0.06125420599710196, 0.0614026669645682, 0.0621047739405185, 0.06147303502075374, 0.06275887391529977, 0.06134258909150958, 0.0614073189208284, 0.06156477192416787, 0.06144882692024112, 0.06323572993278503, 0.06696505192667246, 0.08128993702121079, 0.2191506379749626, 0.06699492805637419, 0.06574582599569112, 0.06044082995504141, 0.06087054801173508, 0.06030193797778338, 0.06010917294770479, 0.059824301046319306, 0.06001050490885973, 0.059486026992090046, 0.059834311017766595, 0.05936336098238826, 0.05981409491505474, 0.059400498983450234, 0.05982361105270684, 0.05948578391689807, 0.05982532992493361, 0.05984647304285318, 0.05971092509571463, 0.0596358539769426, 0.05969032703433186, 0.05982713995035738, 0.05999964301008731, 0.0594205119414255, 0.06014290393795818, 0.060405511991120875, 0.0600936459377408, 0.05947571701835841, 0.05981987703125924, 0.05962101905606687, 0.0599162980215624, 0.05998835398349911, 0.05981093109585345, 0.059753878973424435, 0.05952383205294609, 0.05964037694502622, 0.05981434998102486, 0.059943838976323605, 0.05983165104407817, 0.0599097718950361, 0.059730642940849066, 0.059711945010349154, 0.059843678027391434, 0.05997916602063924, 0.059819676098413765, 0.05991968500893563, 0.059612583951093256, 0.059460312011651695, 0.05978478700853884, 0.06021877902094275, 0.059939224971458316, 0.06068656803108752, 0.06028433900792152, 0.06104202102869749, 0.0610792679945007, 0.06116258702240884, 0.06087906192988157, 0.060079612070694566, 0.06002967606764287, 0.060077628935687244, 0.06016057007946074, 0.059989869012497365, 0.06007772102020681, 0.05972431390546262, 0.06014443701133132, 0.05985251790843904, 0.06008255307096988, 0.06004303297959268, 0.060344069031998515, 0.06048442900646478, 0.06037855090107769, 0.060423668939620256, 0.06034653203096241, 0.05981909390538931, 0.06043570605106652, 0.06005064898636192, 0.05998899810947478, 0.059958993922919035, 0.059943399974144995, 0.0599585750605911, 0.060537634999491274, 0.05996331095229834, 0.05984203203115612, 0.06016641401220113, 0.06011435191612691, 0.060206196969375014, 0.05973576393444091, 0.06026124593336135, 0.060044914949685335, 0.06022960599511862, 0.06017087690997869, 0.06008617510087788, 0.060007957043126225, 0.05983484198804945, 0.06019098893739283, 0.06034661608282477, 0.060422829003073275, 0.06001020106486976, 0.06012733210809529, 0.060307536041364074, 0.06039038102608174, 0.06028234399855137, 0.061411743983626366, 0.06052050495054573, 0.06023298599757254, 0.06058201193809509, 0.0780437629437074, 0.2198376900050789, 0.09650197601877153, 0.08299551100935787, 0.08295237401034683, 0.08513489901088178, 0.0824818869587034, 0.2083045799518004, 0.08434291102457792, 0.0848184630740434, 0.08524499600753188, 0.08633746695704758, 0.08516417909413576, 0.08432797098066658, 0.08475697506219149, 0.09950834198389202, 0.09074398095253855, 0.08655657607596368, 0.08634660893585533, 0.09330907394178212, 0.0859265640610829, 0.08438821497838944, 0.08607594901695848, 0.08519807003904134, 0.08109081804286689, 0.09115505800582469, 0.08299841301050037, 0.08386960392817855, 0.08282130002044141, 0.08419342897832394, 0.08238185406662524, 0.07709572301246226, 0.0771647640503943, 0.07727797201368958, 0.07751269999425858, 0.07901086192578077, 0.0779045979725197, 0.07751942006871104, 0.07755695609375834, 0.0781337390653789, 0.07797738199587911, 0.07872566999867558, 0.0800167340785265, 0.07825260597746819, 0.07923566899262369, 0.07896742504090071, 0.07746922608930618, 0.07737015199381858, 0.07809847698081285, 0.07722680398728698, 0.07696463400498033, 0.07938162703067064, 0.07951098494231701, 0.07878082792740315, 0.07979518303181976, 0.08119569392874837, 0.07959404797293246, 0.07908593595493585, 0.07942919398192316, 0.07920549006666988, 0.07925300300121307, 0.07987268292345107, 0.08111606305465102, 0.07942830095998943, 0.19888382602948695, 0.0852069320390001, 0.08556420600507408, 0.08520547801163048, 0.08659483108203858, 0.08409538702107966, 0.08535102300811559, 0.0838877159403637, 0.08447165798861533, 0.08394838101230562, 0.08464967494364828, 0.08466685703024268, 0.08311899600084871, 0.08594468689989299, 0.08167000010143965, 0.081752447062172, 0.0817792370216921, 0.08199515100568533, 0.08474021696019918, 0.08254107797984034, 0.08388738101348281, 0.08335867407731712, 0.08389780600555241, 0.08446380996610969, 0.08580435591284186, 0.08389321295544505, 0.20909635501448065, 0.0786339509068057, 0.08440041600260884, 0.07979382202029228, 0.08213139290455729, 0.08590142894536257, 0.08069646800868213, 0.07808055891655385, 0.07774511096067727, 0.07836994295939803, 0.07932996703311801, 0.07944417302496731, 0.07948166294954717, 0.07942638895474374, 0.07998555991798639, 0.07956987095531076, 0.07948084105737507, 0.09111416304949671, 0.08318625297397375, 0.0823080400004983, 0.07947641401551664, 0.07997531397268176, 0.08010539994575083, 0.0800246469443664, 0.08013043703977019, 0.08052093302831054, 0.0806758189573884, 0.08092888805549592]
[0.0018218727758610432, 0.0017927381856252952, 0.0018074493047458176, 0.0018188494699531977, 0.002046315857133239, 0.001999991796245532, 0.0018724926310230274, 0.0019665453060321056, 0.0018646584072967572, 0.001948526326319849, 0.0021563433894735513, 0.0018408140215110413, 0.001789463776619915, 0.0017767181235118484, 0.001825181223281032, 0.001866781406997874, 0.0017581116550659038, 0.0018637458774812367, 0.0017876411213216428, 0.0019868645313366943, 0.0018094921418066537, 0.001758988571296237, 0.0017561676926264654, 0.0017615570829306937, 0.0017673471415111301, 0.0017638817353516209, 0.0017632777536553995, 0.001772316571857248, 0.0017860023071989417, 0.0018065002466532954, 0.0017251513484979467, 0.0018084776131626294, 0.0020027843265965277, 0.0017058832870264138, 0.0017235288562785303, 0.0017554802458961398, 0.0018052978973303521, 0.0017601364691342627, 0.001733854002015171, 0.0017308534897521747, 0.0017214585095643997, 0.001741161182693833, 0.001803869856711553, 0.0017600119380014284, 0.0017822272457866644, 0.0017520251436805238, 0.0018432493872788487, 0.001786927490647198, 0.0018471772639955186, 0.0018939795305154153, 0.0018002584918725248, 0.0018007337158469825, 0.0017944837359673514, 0.001787630857766739, 0.0018001632451326872, 0.0017970469780266285, 0.0017982159802994253, 0.0018660397131034002, 0.001769792939517267, 0.0017766360198242627, 0.0017662105098252697, 0.0018554981631625975, 0.0017942731857908015, 0.0017817214711056072, 0.001745200449866908, 0.0017289299570138054, 0.0017422187755036416, 0.0017788100403220374, 0.001781922346931331, 0.0017634484280204894, 0.0017259814895271342, 0.0017116256509622444, 0.0017361021434356058, 0.0017263267559892669, 0.0017567845726651804, 0.0017412930812534628, 0.0017627973456353862, 0.001766197611482776, 0.0017790047342565898, 0.001779735777337028, 0.0018066730830172191, 0.0017669034494581272, 0.0029136750405198155, 0.0017317626316442477, 0.001761495530111145, 0.0017751218793837695, 0.001811642468222702, 0.001767686734508191, 0.003310406917934211, 0.0017523176526194628, 0.0017656398775550176, 0.0017902729367571218, 0.0017802799381885905, 0.0017244783895356314, 0.0017556403050845374, 0.0017289083061397684, 0.0017239201020411387, 0.0017501833677595975, 0.0017983225715935839, 0.0017373725101921934, 0.003974813081761252, 0.0017710580604569036, 0.0017332407546096615, 0.0017472005292431128, 0.0018056373667846225, 0.001761293285811434, 0.0017272024486708094, 0.0017355099196868892, 0.0013441324295780184, 0.0016420347335729369, 0.0014962190215722943, 0.0012423859182175022, 0.0012243434487442886, 0.0012464173047860361, 0.0014727660003403316, 0.00127550549044901, 0.0013850405907296405, 0.0013764806932826735, 0.0012697758775546538, 0.0012500858366755502, 0.0012531156523381264, 0.0012674443661330305, 0.0012545517351174233, 0.0012807933452101995, 0.0012518895732961139, 0.0012532105902209878, 0.0012564239168197525, 0.0012540576922498187, 0.0012905251006690823, 0.0013666337127892338, 0.0016589783065553223, 0.004472461999489033, 0.0013672434297219223, 0.001341751550932472, 0.0012334863256130899, 0.0012422560818721444, 0.001230651795464967, 0.0012267178152592815, 0.0012209041029861082, 0.0012247041818134639, 0.0012140005508589806, 0.0012211083881176856, 0.0012114971629058828, 0.0012206958145929538, 0.0012122550812949026, 0.001220890021483813, 0.001213995590140777, 0.0012209251005088492, 0.0012213565927112894, 0.0012185903080758087, 0.0012170582444274, 0.0012181699394761603, 0.0012209620398032119, 0.001224482510409945, 0.0012126635090086836, 0.001227406202815473, 0.0012327655508392015, 0.0012264009375049143, 0.0012137901432318042, 0.0012208138169644742, 0.0012167554909401402, 0.0012227815922767836, 0.0012242521221122267, 0.001220631246854152, 0.0012194669178249885, 0.0012147720827131855, 0.0012171505498984943, 0.0012207010200209155, 0.0012233436525780328, 0.001221054102940371, 0.0012226484060211449, 0.0012189927130785523, 0.0012186111226601868, 0.001221299551579417, 0.001224064612666107, 0.0012208097162941585, 0.001222850714468074, 0.0012165833459406787, 0.0012134757553398305, 0.001220097694051813, 0.0012289546738967908, 0.001223249489213435, 0.0012385013883895412, 0.001230292632814725, 0.001245755531197908, 0.001246515673357157, 0.0012482160616818132, 0.0012424298353037055, 0.0012261145320549911, 0.0012250954299518953, 0.0012260740599119846, 0.0012277667363155254, 0.0012242830410713749, 0.001226075939187894, 0.001218863549091074, 0.0012274374900271697, 0.0012214799573150824, 0.001226174552468773, 0.0012253680199916875, 0.0012315116128979288, 0.0012343761021727506, 0.0012322153245117894, 0.0012331361008085767, 0.0012315618781829063, 0.0012207978348038634, 0.0012333817561442147, 0.0012255234487012637, 0.0012242652675403015, 0.0012236529372024294, 0.0012233346933498979, 0.0012236443889916552, 0.0012354619387651281, 0.0012237410398428232, 0.0012212659598195127, 0.0012278860002490027, 0.001226823508492386, 0.001228697897334184, 0.0012190972231518552, 0.001229821345578803, 0.0012254064275445987, 0.0012291756325534412, 0.0012279770797954835, 0.0012262484714464874, 0.0012246521845535965, 0.0012211192242459071, 0.0012283875293345476, 0.001231563593527036, 0.0012331189592463933, 0.0012246979809157094, 0.0012270884103692916, 0.0012307660416604914, 0.0012324567556343212, 0.0012302519183377831, 0.0012533008976250279, 0.0012351123459295047, 0.001229244612195358, 0.0012363675905733692, 0.0016259117279939044, 0.00457995187510581, 0.002010457833724407, 0.0017290731460282889, 0.0017281744585488923, 0.0017736437293933704, 0.0017183726449729875, 0.0043396787489958415, 0.0017571439796787065, 0.0017670513140425708, 0.0017759374168235809, 0.0017986972282718245, 0.0017742537311278284, 0.001756832728763887, 0.001765770313795656, 0.0020730904579977505, 0.0018904996031778865, 0.0018032620015825767, 0.0017988876861636527, 0.0019439390404537942, 0.0017901367512725603, 0.00175808781204978, 0.0017932489378533016, 0.001774959792480028, 0.0016893920425597269, 0.0018990637084546809, 0.0017291336043854244, 0.0017472834151703864, 0.0017254437504258628, 0.0017540297703817487, 0.0017162886263880257, 0.0016061608960929636, 0.001607599251049881, 0.0016099577502851996, 0.0016148479165470537, 0.001646059623453766, 0.001623012457760827, 0.0016149879180981468, 0.0016157699186199654, 0.0016277862305287272, 0.0016245287915808149, 0.001640118124972408, 0.0016670152933026354, 0.0016302626245305873, 0.0016507431040129934, 0.001645154688352098, 0.0016139422101938787, 0.0016118781665378872, 0.0016270516037669343, 0.0016088917497351456, 0.0016034298751037568, 0.001653783896472305, 0.0016564788529649377, 0.0016412672484875657, 0.001662399646496245, 0.0016915769568489243, 0.0016582093327694263, 0.0016476236657278303, 0.0016547748746233992, 0.0016501143763889559, 0.001651104229191939, 0.0016640142275718972, 0.0016899179803052296, 0.0016547562699997798, 0.0041434130422809785, 0.0017751444174791686, 0.00178258762510571, 0.0017751141252423015, 0.0018040589808758039, 0.0017519872296058263, 0.0017781463126690749, 0.0017476607487575773, 0.0017598262080961529, 0.0017489246044230338, 0.0017635348946593392, 0.0017638928547967225, 0.0017316457500176814, 0.0017905143104144372, 0.0017014583354466595, 0.0017031759804619166, 0.0017037341046185854, 0.0017082323126184444, 0.0017654211866708163, 0.0017196057912466738, 0.0017476537711142253, 0.00173663904327744, 0.0017478709584490086, 0.0017596627076272853, 0.0017875907481842053, 0.0017477752699051052, 0.0043561740628016805, 0.001638207310558452, 0.0017583420000543508, 0.0016623712920894225, 0.0017110706855116102, 0.0017896131030283868, 0.0016811764168475445, 0.0016266783107615386, 0.0016196898116807763, 0.001632707144987459, 0.001652707646523292, 0.0016550869380201523, 0.001655867978115566, 0.0016547164365571614, 0.0016663658316247165, 0.0016577056449023075, 0.0016558508553619806, 0.001898211730197848, 0.0017330469369577866, 0.0017147508333437145, 0.0016557586253232632, 0.00166615237443087, 0.001668862498869809, 0.0016671801446743, 0.0016693841049952123, 0.0016775194380898029, 0.001680746228278925, 0.001686018501156165]
[548.8857472648637, 557.8059350876193, 553.265863321478, 549.7981094750694, 488.6831114141576, 500.00205094702966, 534.0474955320143, 508.5059555620908, 536.2912563967818, 513.2083598216936, 463.7480305231624, 543.2379307819184, 558.826623408316, 562.8354811980007, 547.8907996885662, 535.6813584340241, 568.7920884424798, 536.5538360580854, 559.396395659481, 503.30557731947346, 552.6412504900787, 568.5085260463508, 569.4217039743133, 567.6795885242078, 565.8197965256405, 566.9314330762969, 567.1256260829748, 564.2332842106638, 559.9096910285293, 553.5565255817652, 579.6592866305203, 552.9512738900982, 499.30488606297934, 586.2065755642262, 580.2049651545814, 569.6446897296294, 553.9252006434978, 568.1377651880925, 576.7498294768478, 577.7496512100403, 580.9027603302744, 574.3293670565597, 554.3637176924701, 568.1779642560518, 561.0956752928586, 570.7680643779315, 542.5201857662242, 559.619797240802, 541.3665593939587, 527.9888108019132, 555.4757855689145, 555.3291923173915, 557.2633398434957, 559.3996073939367, 555.505175824369, 556.4684798046399, 556.1067251963179, 535.8942754422443, 565.0378514182356, 562.8614915163747, 566.1839256629321, 538.9388250838003, 557.3287322795628, 561.2549527056395, 573.0000814956596, 578.3924304991467, 573.980727369282, 562.1735752171486, 561.1916825231534, 567.0707371479656, 579.3804893434686, 584.2399004933225, 576.0029752748768, 579.264612872754, 569.221756360782, 574.285863055376, 567.2801825325861, 566.1880604404566, 562.1120510496447, 561.8811582786035, 553.5035692954248, 565.9618811127905, 343.20917264047216, 577.4463438159162, 567.699425236068, 563.3416001537586, 551.9852937544804, 565.7110960207674, 302.07766742586097, 570.6727878390906, 566.366909080439, 558.5740472686738, 561.7094135304843, 579.885492371569, 569.5927560468305, 578.3996736256978, 580.0732869324918, 571.3687025149233, 556.0737632925609, 575.5818019069364, 251.58415740065365, 564.6342275995269, 576.9538924932604, 572.3441489759621, 553.8210597517395, 567.7646125467949, 578.9709253652128, 576.1995299804535, 743.9743123480351, 609.0005159781728, 668.3513480193261, 804.9028770663609, 816.764283768268, 802.2995157080742, 678.9944904817987, 784.00289727328, 722.0005007024373, 726.490393130883, 787.5405555237112, 799.9450682997715, 798.0109402784567, 788.9892658965358, 797.0974588038031, 780.7660804451505, 798.7924984206785, 797.9504863772837, 795.9097137622068, 797.4114796951396, 774.8783804991801, 731.7249608595181, 602.7806367621437, 223.5904967139458, 731.3986509362021, 745.2944617839524, 810.7102439931482, 804.9870027546558, 812.577533047988, 815.1834004209338, 819.065148158797, 816.52370821439, 823.7228552264149, 818.9281227864467, 825.4249622850224, 819.2049059605027, 824.9088953554422, 819.0745950930506, 823.7262211834216, 819.051061840916, 818.761699873499, 820.620345798606, 821.6533634102933, 820.9035271630671, 819.0262820629335, 816.6715257249444, 824.6310642409535, 814.7262069444984, 811.1842509869398, 815.3940276941389, 823.8656456192892, 819.1257226154904, 821.8578074608387, 817.8075351445463, 816.825212665084, 819.2482394476059, 820.0304455848427, 823.1996884275662, 821.5910513973774, 819.201412629987, 817.4317967748752, 818.9645303938134, 817.8966210362079, 820.3494485824376, 820.6063291274036, 818.7999403641584, 816.9503387749467, 819.1284740389848, 817.7613082026847, 821.974099297559, 824.0790931335524, 819.6064994427682, 813.6996597516348, 817.4947210834421, 807.4274355883675, 812.814751001272, 802.7257154045373, 802.236202379042, 801.1433522595652, 804.8744255691148, 815.5844938270009, 816.2629420952669, 815.6114158974918, 814.4869627279173, 816.804583950535, 815.6101657637632, 820.4363816981122, 814.7054396862725, 818.6790082074581, 815.5445715184723, 816.0813597916354, 812.0102072337362, 810.1258589175524, 811.5464725259811, 810.9404950064251, 811.9770656391528, 819.1364462574285, 810.7789782185442, 815.9778591423445, 816.8164420845836, 817.2251866499379, 817.4377833278536, 817.2308956722717, 809.4138464512492, 817.1663509204852, 818.8224620194827, 814.4078520295942, 815.1131707843422, 813.8697088760605, 820.2791221315383, 813.1262346315514, 816.0557815938214, 813.5533877470701, 814.3474470765753, 815.4954100129447, 816.5583768296748, 818.9208556744674, 814.0753435861795, 811.9759347027559, 810.9517678741544, 816.5278424418548, 814.937205461056, 812.5021053155215, 811.3874952840189, 812.8416506361715, 797.8929895406392, 809.6429472959669, 813.507734814521, 808.8209425938179, 615.0395392213746, 218.34290561773577, 497.39914124310843, 578.344532327633, 578.6452837867287, 563.8110875525292, 581.945949224372, 230.4318033290805, 569.1053274887865, 565.9145221494731, 563.082905133328, 555.9579368234157, 563.6172450737001, 569.2061535668242, 566.3250719457532, 482.3716187309204, 528.9607034664398, 554.5505861723816, 555.8990745734782, 514.419423237963, 558.6165410486806, 568.7998023455299, 557.6470610918639, 563.3930437391878, 591.9289157328004, 526.5752778845565, 578.3243107784167, 572.3169986721845, 579.5610548029667, 570.1157511040186, 582.6525822201165, 622.602631176323, 622.0455746958864, 621.1343122655566, 619.2533611079913, 607.5114083059748, 616.1382158333154, 619.1996787057249, 618.8999983698814, 614.3312808802826, 615.5631129362187, 609.7121815642536, 599.8745206583158, 613.3981022155475, 605.7877797998838, 607.845576516376, 619.6008715082014, 620.3942833644028, 614.6086563479668, 621.5458561240177, 623.6630709748318, 604.6739251319989, 603.6901697900316, 609.2852952019265, 601.5400701676333, 591.1643546284785, 603.0601687242162, 606.9347150086331, 604.3118102258982, 606.0185974431421, 605.6552834883153, 600.9564001500058, 591.7446951001622, 604.3186045762093, 241.34692578210667, 563.3344476952873, 560.9822406013272, 563.3440609704465, 554.3056023115925, 570.7804161477744, 562.3834174247206, 572.1934309681706, 568.2379290633694, 571.7799369229515, 567.0429335015622, 566.9278591840794, 577.4853199563416, 558.4987476411386, 587.7311122857935, 587.1383882062446, 586.9460482648904, 585.400470775056, 566.4370675678664, 581.5286300443449, 572.1957154948632, 575.8248980241561, 572.1246154735361, 568.2907273453512, 559.412159083827, 572.1559385917474, 229.55923835533065, 610.4233533539219, 568.7175759716197, 601.5503303976737, 584.4293917647242, 558.7799945741334, 594.8215725480783, 614.7496978255303, 617.402167247249, 612.4797108104044, 605.0676912541935, 604.197868419057, 603.9128802635787, 604.3331521385147, 600.1083201670033, 603.2434063762462, 603.9191251807476, 526.8116217445202, 577.0184169134006, 583.1751065838697, 603.9527650383003, 600.1852023537664, 599.2105405191994, 599.8152048501993, 599.0233146510449, 596.1182787477583, 594.9738176857279, 593.113301730831]
Elapsed: 0.07877742852753317~0.021679677694408584
Time per graph: 0.0016199239597070709~0.00045063033018212284
Speed: 648.8862333651135~129.3889382748376
Total Time: 0.0821
best val loss: 0.20141351222991943 test_score: 0.9167

Testing...
Test loss: 0.3115 score: 0.9375 time: 0.07s
test Score 0.9375
Epoch Time List: [0.2960375288967043, 0.2918600750854239, 0.29277693398762494, 0.2927815979346633, 0.30409738793969154, 0.3104192039463669, 0.32033257791772485, 0.3290333009790629, 0.3091641031205654, 0.30193044198676944, 0.3208488639211282, 0.30565969506278634, 0.2916041340213269, 0.2957474540453404, 0.30196137889288366, 0.30021943897008896, 0.2951262620044872, 0.2998928229790181, 0.29568658594507724, 0.3069705858360976, 0.2979743389878422, 0.2885621738387272, 0.28888130094856024, 0.28964159300085157, 0.2885669590905309, 0.2897102510323748, 0.2910154699347913, 0.2922992929816246, 0.29185622406657785, 0.295863519073464, 0.28707995906006545, 0.2886653409805149, 0.4546450270572677, 0.3105903510004282, 0.28287940099835396, 0.28493878699373454, 0.30975358188152313, 0.28936929802875966, 0.40391082293353975, 0.28533446590881795, 0.28319640981499106, 0.28394166892394423, 0.29162246792111546, 0.2902432889677584, 0.28768880700226873, 0.32278659602161497, 0.2979170639300719, 0.30433345201890916, 0.3020322909578681, 0.311315402854234, 0.29849165899213403, 0.3555557099170983, 0.29479566297959536, 0.29484651889652014, 0.29743725690059364, 0.3010118049569428, 0.2964330349350348, 0.2963992148870602, 0.4084675050107762, 0.29133505397476256, 0.2906328940298408, 0.2957895799772814, 0.29979483713395894, 0.29198099102359265, 0.36175527796149254, 0.2845320908818394, 0.2864920728607103, 0.29007627407554537, 0.29827095416840166, 0.2904343269765377, 0.4193204010371119, 0.2830705359810963, 0.28517436888068914, 0.28524472704157233, 0.29327804397325963, 0.28866112895775586, 0.3605032409541309, 0.29059806815348566, 0.2912651940714568, 0.29320360301062465, 0.29667466203682125, 0.29112206504214555, 0.3421482889680192, 0.30942401592619717, 0.2895712190074846, 0.28895802109036595, 0.2936482379445806, 0.2922086529433727, 0.365514682023786, 0.28995940589811653, 0.28885858808644116, 0.2913345699198544, 0.29369096108712256, 0.2869902910897508, 0.28466465999372303, 0.4097789260558784, 0.28369828208815306, 0.2842420799424872, 0.2914394789841026, 0.28874245495535433, 0.3959725451422855, 0.288033566204831, 0.2826918389182538, 0.2832830658880994, 0.2872461940860376, 0.29558343603275716, 0.2846542219631374, 0.35946885985322297, 0.2815325401024893, 0.2836698959581554, 0.3872681569773704, 0.2503085680073127, 0.24457725090906024, 0.24344502412714064, 0.2684159668860957, 0.2598439180292189, 0.2580243539996445, 0.3006749090272933, 0.24437021801713854, 0.24344728712458163, 0.24274574406445026, 0.24601774709299207, 0.24424833594821393, 0.24102856393437833, 0.3661663378588855, 0.24262640497181565, 0.24346831697039306, 0.24493072589393705, 0.2607748882146552, 0.25972073106095195, 0.27936131809838116, 0.4250950310379267, 0.2765386290848255, 0.25451642309781164, 0.2381679309764877, 0.24150929076131433, 0.243101847008802, 0.2428662059828639, 0.24286339501850307, 0.24360684806015342, 0.24155882792547345, 0.24338656093459576, 0.24256206711288542, 0.24329125101212412, 0.24316025304142386, 0.24341469898354262, 0.24445268185809255, 0.24401928903535008, 0.2444628238445148, 0.24464596586767584, 0.24451460305135697, 0.2439916100120172, 0.24502875399775803, 0.24518038099631667, 0.24642326193861663, 0.24708860914688557, 0.2460101390024647, 0.24648384505417198, 0.24540663685183972, 0.24591043998952955, 0.24713397189043462, 0.24711977690458298, 0.24620212998706847, 0.24510469811502844, 0.24611951992847025, 0.2455482081277296, 0.24652928509749472, 0.24590296996757388, 0.24667141307145357, 0.24578817503061146, 0.24650520691648126, 0.24553992715664208, 0.24617152009159327, 0.24587752704974264, 0.24692985403817147, 0.24630098207853734, 0.246008921880275, 0.246055677998811, 0.24521226389333606, 0.24514717899728566, 0.24828959594015032, 0.24656799109652638, 0.24824353901203722, 0.24734834302216768, 0.24863031809218228, 0.24624393496196717, 0.24717366613913327, 0.2466532139806077, 0.2470379580045119, 0.2456468950258568, 0.24587113608140498, 0.24545336898881942, 0.24705889611504972, 0.24605185899417847, 0.24616109498310834, 0.2467163548571989, 0.24715761095285416, 0.24638434499502182, 0.2465628630015999, 0.24655899696517736, 0.2474737949669361, 0.2464913601288572, 0.2472797540249303, 0.24675406306050718, 0.24687248596455902, 0.24610393901821226, 0.24683470593299717, 0.24616627220530063, 0.24609959789086133, 0.24542142008431256, 0.24682442797347903, 0.24631561001297086, 0.2464002671185881, 0.24443836696445942, 0.24511409306433052, 0.24505635688547045, 0.2452944389078766, 0.24452018016017973, 0.24535074492450804, 0.24590133095625788, 0.24562190694268793, 0.246403401135467, 0.24615397595334798, 0.24632343091070652, 0.24548978498205543, 0.24636943207588047, 0.2460976701695472, 0.24660390196368098, 0.24684233509469777, 0.24663156899623573, 0.24719043297227472, 0.24719430785626173, 0.24348931899294257, 0.24149995599873364, 0.3451675210380927, 0.24006022803951055, 0.23984642408322543, 0.27886066609062254, 0.4182388511253521, 0.3014370338059962, 0.293805296998471, 0.28949982998892665, 0.294854415114969, 0.2923703759443015, 0.4169414589414373, 0.29678289708681405, 0.296218395116739, 0.29651742603164166, 0.2994423119816929, 0.30036559188738465, 0.29654511308763176, 0.4131836169399321, 0.31669709098059684, 0.3136715340660885, 0.3046195290517062, 0.3167860849061981, 0.3070832950761542, 0.4369782661087811, 0.29779772902838886, 0.29720137105323374, 0.30153439205605537, 0.28585730493068695, 0.4421118119498715, 0.29547698702663183, 0.2923927700612694, 0.2945509209530428, 0.3002492591040209, 0.2899499030318111, 0.3594757850514725, 0.2727976090973243, 0.2733302260749042, 0.2745895559201017, 0.27839105704333633, 0.27985091996379197, 0.2772605190984905, 0.3483614668948576, 0.27966526290401816, 0.2794342669658363, 0.2806014559464529, 0.28361248690634966, 0.2835461519425735, 0.2807092689909041, 0.3216892860364169, 0.277928109979257, 0.2768062630202621, 0.2777012289734557, 0.28320367611013353, 0.2740791558753699, 0.3808440138818696, 0.28273342992179096, 0.2819372940575704, 0.2832617489621043, 0.28478264284785837, 0.2890489720739424, 0.2837770270416513, 0.37402704905252904, 0.284293798962608, 0.2830538800917566, 0.28579584194812924, 0.29097171290777624, 0.28492056007962674, 0.40295623906422406, 0.29826554213650525, 0.29843811003956944, 0.29965700407046825, 0.301212080870755, 0.2981218418572098, 0.29669859912246466, 0.3193588798167184, 0.2953886709874496, 0.2941737020155415, 0.2966226799180731, 0.2980777679476887, 0.2931928790640086, 0.2936354010598734, 0.33542457909788936, 0.2854845210677013, 0.28372212301474065, 0.2848699950845912, 0.3039291320601478, 0.29095600091386586, 0.34761984494980425, 0.29272195301018655, 0.2920205359114334, 0.29587657400406897, 0.3002933969255537, 0.29902834189124405, 0.427530538989231, 0.2832572910701856, 0.29353351809550077, 0.295819299062714, 0.30159927101340145, 0.310194319114089, 0.43421458813827485, 0.2787371709709987, 0.28010143304709345, 0.28179603291209787, 0.28663862496614456, 0.2859406569041312, 0.28480090002994984, 0.2843034059042111, 0.2862415799172595, 0.2849604699295014, 0.28524901310447603, 0.3022721220040694, 0.2979874599259347, 0.2961440180661157, 0.288430342101492, 0.28546973096672446, 0.2850744780153036, 0.28683193994220346, 0.2871463990304619, 0.2883938809391111, 0.28811808314640075, 0.28855073207523674]
Total Epoch List: [108, 129, 118]
Total Time List: [0.08562453405465931, 0.06150296097621322, 0.08206685597542673]
========================training times:2========================
========================k_idx:0========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: I2BGNNA
I2BGNN(
  (gcs): ModuleList(
    (0): GCNConv(14887, 64)
    (1): GCNConv(64, 64)
  )
  (bns): ModuleList(
    (0-1): 2 x BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  )
  (drops): ModuleList(
    (0-1): 2 x Dropout(p=0.2, inplace=False)
  )
  (lin1): Linear(in_features=64, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=64, bias=True)
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=64, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=64, out_features=2, bias=True)
  )
)
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x759998c77a30>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.6966;  Loss pred: 0.6966; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6948 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6932 score: 0.5102 time: 0.08s
Epoch 2/1000, LR 0.000000
Train loss: 0.6966;  Loss pred: 0.6966; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6948 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6931 score: 0.5102 time: 0.08s
     INFO: Early stopping counter 1 of 20
Epoch 3/1000, LR 0.000030
Train loss: 0.6959;  Loss pred: 0.6959; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6948 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6931 score: 0.5102 time: 0.08s
Epoch 4/1000, LR 0.000060
Train loss: 0.6957;  Loss pred: 0.6957; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6948 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6931 score: 0.5102 time: 0.08s
Epoch 5/1000, LR 0.000090
Train loss: 0.6946;  Loss pred: 0.6946; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6947 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6931 score: 0.5102 time: 0.08s
Epoch 6/1000, LR 0.000120
Train loss: 0.6920;  Loss pred: 0.6920; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6946 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6931 score: 0.5102 time: 0.08s
Epoch 7/1000, LR 0.000150
Train loss: 0.6884;  Loss pred: 0.6884; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6945 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6930 score: 0.5102 time: 0.08s
Epoch 8/1000, LR 0.000180
Train loss: 0.6867;  Loss pred: 0.6867; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6943 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6930 score: 0.5102 time: 0.08s
Epoch 9/1000, LR 0.000210
Train loss: 0.6826;  Loss pred: 0.6826; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6941 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6929 score: 0.5102 time: 0.08s
Epoch 10/1000, LR 0.000240
Train loss: 0.6794;  Loss pred: 0.6794; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6939 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6929 score: 0.5102 time: 0.08s
Epoch 11/1000, LR 0.000270
Train loss: 0.6753;  Loss pred: 0.6753; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6937 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6928 score: 0.5102 time: 0.08s
Epoch 12/1000, LR 0.000270
Train loss: 0.6750;  Loss pred: 0.6750; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6935 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6928 score: 0.5102 time: 0.08s
Epoch 13/1000, LR 0.000270
Train loss: 0.6681;  Loss pred: 0.6681; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6933 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6928 score: 0.5102 time: 0.08s
Epoch 14/1000, LR 0.000270
Train loss: 0.6662;  Loss pred: 0.6662; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6932 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6927 score: 0.5102 time: 0.08s
Epoch 15/1000, LR 0.000270
Train loss: 0.6618;  Loss pred: 0.6618; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6930 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6927 score: 0.5102 time: 0.08s
Epoch 16/1000, LR 0.000270
Train loss: 0.6575;  Loss pred: 0.6575; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6928 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6926 score: 0.5102 time: 0.08s
Epoch 17/1000, LR 0.000270
Train loss: 0.6531;  Loss pred: 0.6531; Loss self: 0.0000; time: 0.12s
Val loss: 0.6926 score: 0.5102 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6926 score: 0.5102 time: 0.08s
Epoch 18/1000, LR 0.000270
Train loss: 0.6483;  Loss pred: 0.6483; Loss self: 0.0000; time: 0.12s
Val loss: 0.6924 score: 0.5918 time: 0.08s
Test loss: 0.6925 score: 0.6122 time: 0.08s
Epoch 19/1000, LR 0.000270
Train loss: 0.6395;  Loss pred: 0.6395; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6923 score: 0.5102 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6925 score: 0.4898 time: 0.08s
Epoch 20/1000, LR 0.000270
Train loss: 0.6352;  Loss pred: 0.6352; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6920 score: 0.5102 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6924 score: 0.4898 time: 0.08s
Epoch 21/1000, LR 0.000270
Train loss: 0.6324;  Loss pred: 0.6324; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6918 score: 0.5102 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6923 score: 0.4898 time: 0.08s
Epoch 22/1000, LR 0.000270
Train loss: 0.6211;  Loss pred: 0.6211; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6915 score: 0.5102 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6921 score: 0.4898 time: 0.08s
Epoch 23/1000, LR 0.000270
Train loss: 0.6128;  Loss pred: 0.6128; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6913 score: 0.5102 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6920 score: 0.4898 time: 0.08s
Epoch 24/1000, LR 0.000270
Train loss: 0.6074;  Loss pred: 0.6074; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6909 score: 0.5102 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6918 score: 0.4898 time: 0.08s
Epoch 25/1000, LR 0.000270
Train loss: 0.5963;  Loss pred: 0.5963; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6905 score: 0.5102 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6914 score: 0.4898 time: 0.08s
Epoch 26/1000, LR 0.000270
Train loss: 0.5918;  Loss pred: 0.5918; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6900 score: 0.5102 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6911 score: 0.4898 time: 0.08s
Epoch 27/1000, LR 0.000270
Train loss: 0.5764;  Loss pred: 0.5764; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6895 score: 0.5102 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6908 score: 0.4898 time: 0.08s
Epoch 28/1000, LR 0.000270
Train loss: 0.5665;  Loss pred: 0.5665; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6887 score: 0.5102 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6902 score: 0.4898 time: 0.08s
Epoch 29/1000, LR 0.000270
Train loss: 0.5555;  Loss pred: 0.5555; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6877 score: 0.5102 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6894 score: 0.4898 time: 0.08s
Epoch 30/1000, LR 0.000270
Train loss: 0.5477;  Loss pred: 0.5477; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6868 score: 0.5102 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6887 score: 0.4898 time: 0.08s
Epoch 31/1000, LR 0.000270
Train loss: 0.5307;  Loss pred: 0.5307; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6859 score: 0.5102 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6881 score: 0.4898 time: 0.08s
Epoch 32/1000, LR 0.000270
Train loss: 0.5186;  Loss pred: 0.5186; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6848 score: 0.5102 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6872 score: 0.4898 time: 0.08s
Epoch 33/1000, LR 0.000270
Train loss: 0.5117;  Loss pred: 0.5117; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6835 score: 0.5102 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6862 score: 0.4898 time: 0.08s
Epoch 34/1000, LR 0.000270
Train loss: 0.4955;  Loss pred: 0.4955; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6821 score: 0.5102 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6851 score: 0.4898 time: 0.08s
Epoch 35/1000, LR 0.000270
Train loss: 0.4800;  Loss pred: 0.4800; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6805 score: 0.5102 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6839 score: 0.4898 time: 0.08s
Epoch 36/1000, LR 0.000270
Train loss: 0.4683;  Loss pred: 0.4683; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6787 score: 0.5102 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6824 score: 0.4898 time: 0.08s
Epoch 37/1000, LR 0.000270
Train loss: 0.4524;  Loss pred: 0.4524; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6766 score: 0.5102 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6807 score: 0.4898 time: 0.08s
Epoch 38/1000, LR 0.000270
Train loss: 0.4404;  Loss pred: 0.4404; Loss self: 0.0000; time: 0.12s
Val loss: 0.6743 score: 0.5306 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6788 score: 0.4898 time: 0.09s
Epoch 39/1000, LR 0.000269
Train loss: 0.4275;  Loss pred: 0.4275; Loss self: 0.0000; time: 0.12s
Val loss: 0.6721 score: 0.5306 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6769 score: 0.4898 time: 0.08s
Epoch 40/1000, LR 0.000269
Train loss: 0.4086;  Loss pred: 0.4086; Loss self: 0.0000; time: 0.12s
Val loss: 0.6697 score: 0.5306 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6750 score: 0.4898 time: 0.08s
Epoch 41/1000, LR 0.000269
Train loss: 0.3920;  Loss pred: 0.3920; Loss self: 0.0000; time: 0.12s
Val loss: 0.6671 score: 0.5306 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6729 score: 0.4898 time: 0.08s
Epoch 42/1000, LR 0.000269
Train loss: 0.3770;  Loss pred: 0.3770; Loss self: 0.0000; time: 0.12s
Val loss: 0.6645 score: 0.5306 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6707 score: 0.4898 time: 0.08s
Epoch 43/1000, LR 0.000269
Train loss: 0.3561;  Loss pred: 0.3561; Loss self: 0.0000; time: 0.12s
Val loss: 0.6616 score: 0.5306 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6683 score: 0.4898 time: 0.08s
Epoch 44/1000, LR 0.000269
Train loss: 0.3436;  Loss pred: 0.3436; Loss self: 0.0000; time: 0.12s
Val loss: 0.6585 score: 0.5306 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6655 score: 0.4898 time: 0.08s
Epoch 45/1000, LR 0.000269
Train loss: 0.3314;  Loss pred: 0.3314; Loss self: 0.0000; time: 0.12s
Val loss: 0.6552 score: 0.5510 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6625 score: 0.4898 time: 0.08s
Epoch 46/1000, LR 0.000269
Train loss: 0.3072;  Loss pred: 0.3072; Loss self: 0.0000; time: 0.12s
Val loss: 0.6514 score: 0.5714 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6591 score: 0.4898 time: 0.08s
Epoch 47/1000, LR 0.000269
Train loss: 0.2990;  Loss pred: 0.2990; Loss self: 0.0000; time: 0.12s
Val loss: 0.6474 score: 0.5714 time: 0.08s
Test loss: 0.6554 score: 0.5102 time: 0.08s
Epoch 48/1000, LR 0.000269
Train loss: 0.2867;  Loss pred: 0.2867; Loss self: 0.0000; time: 0.12s
Val loss: 0.6430 score: 0.5918 time: 0.08s
Test loss: 0.6514 score: 0.5510 time: 0.08s
Epoch 49/1000, LR 0.000269
Train loss: 0.2750;  Loss pred: 0.2750; Loss self: 0.0000; time: 0.12s
Val loss: 0.6382 score: 0.6531 time: 0.08s
Test loss: 0.6471 score: 0.6122 time: 0.08s
Epoch 50/1000, LR 0.000269
Train loss: 0.2579;  Loss pred: 0.2579; Loss self: 0.0000; time: 0.12s
Val loss: 0.6330 score: 0.7143 time: 0.09s
Test loss: 0.6423 score: 0.6531 time: 0.08s
Epoch 51/1000, LR 0.000269
Train loss: 0.2391;  Loss pred: 0.2391; Loss self: 0.0000; time: 0.13s
Val loss: 0.6272 score: 0.7755 time: 0.09s
Test loss: 0.6369 score: 0.7143 time: 0.08s
Epoch 52/1000, LR 0.000269
Train loss: 0.2285;  Loss pred: 0.2285; Loss self: 0.0000; time: 0.13s
Val loss: 0.6208 score: 0.7755 time: 0.09s
Test loss: 0.6312 score: 0.7347 time: 0.08s
Epoch 53/1000, LR 0.000269
Train loss: 0.2136;  Loss pred: 0.2136; Loss self: 0.0000; time: 0.12s
Val loss: 0.6139 score: 0.8776 time: 0.09s
Test loss: 0.6249 score: 0.7755 time: 0.09s
Epoch 54/1000, LR 0.000269
Train loss: 0.2024;  Loss pred: 0.2024; Loss self: 0.0000; time: 0.13s
Val loss: 0.6065 score: 0.9184 time: 0.09s
Test loss: 0.6182 score: 0.8367 time: 0.08s
Epoch 55/1000, LR 0.000269
Train loss: 0.1919;  Loss pred: 0.1919; Loss self: 0.0000; time: 0.13s
Val loss: 0.5985 score: 0.9184 time: 0.09s
Test loss: 0.6109 score: 0.8776 time: 0.08s
Epoch 56/1000, LR 0.000269
Train loss: 0.1775;  Loss pred: 0.1775; Loss self: 0.0000; time: 0.12s
Val loss: 0.5900 score: 0.9184 time: 0.09s
Test loss: 0.6031 score: 0.8776 time: 0.08s
Epoch 57/1000, LR 0.000269
Train loss: 0.1652;  Loss pred: 0.1652; Loss self: 0.0000; time: 0.14s
Val loss: 0.5809 score: 1.0000 time: 0.12s
Test loss: 0.5947 score: 0.9184 time: 0.08s
Epoch 58/1000, LR 0.000269
Train loss: 0.1559;  Loss pred: 0.1559; Loss self: 0.0000; time: 0.12s
Val loss: 0.5711 score: 1.0000 time: 0.08s
Test loss: 0.5858 score: 0.9184 time: 0.08s
Epoch 59/1000, LR 0.000268
Train loss: 0.1476;  Loss pred: 0.1476; Loss self: 0.0000; time: 0.12s
Val loss: 0.5608 score: 1.0000 time: 0.08s
Test loss: 0.5765 score: 0.9184 time: 0.08s
Epoch 60/1000, LR 0.000268
Train loss: 0.1391;  Loss pred: 0.1391; Loss self: 0.0000; time: 0.12s
Val loss: 0.5500 score: 1.0000 time: 0.08s
Test loss: 0.5667 score: 0.9184 time: 0.08s
Epoch 61/1000, LR 0.000268
Train loss: 0.1304;  Loss pred: 0.1304; Loss self: 0.0000; time: 0.12s
Val loss: 0.5388 score: 1.0000 time: 0.08s
Test loss: 0.5566 score: 0.9184 time: 0.08s
Epoch 62/1000, LR 0.000268
Train loss: 0.1222;  Loss pred: 0.1222; Loss self: 0.0000; time: 0.12s
Val loss: 0.5272 score: 1.0000 time: 0.08s
Test loss: 0.5463 score: 0.9184 time: 0.08s
Epoch 63/1000, LR 0.000268
Train loss: 0.1112;  Loss pred: 0.1112; Loss self: 0.0000; time: 0.12s
Val loss: 0.5151 score: 1.0000 time: 0.08s
Test loss: 0.5356 score: 0.9184 time: 0.08s
Epoch 64/1000, LR 0.000268
Train loss: 0.1054;  Loss pred: 0.1054; Loss self: 0.0000; time: 0.12s
Val loss: 0.5027 score: 1.0000 time: 0.13s
Test loss: 0.5248 score: 0.9184 time: 0.10s
Epoch 65/1000, LR 0.000268
Train loss: 0.0970;  Loss pred: 0.0970; Loss self: 0.0000; time: 0.12s
Val loss: 0.4897 score: 0.9796 time: 0.08s
Test loss: 0.5135 score: 0.9184 time: 0.08s
Epoch 66/1000, LR 0.000268
Train loss: 0.0870;  Loss pred: 0.0870; Loss self: 0.0000; time: 0.12s
Val loss: 0.4761 score: 0.9796 time: 0.08s
Test loss: 0.5017 score: 0.9388 time: 0.08s
Epoch 67/1000, LR 0.000268
Train loss: 0.0821;  Loss pred: 0.0821; Loss self: 0.0000; time: 0.12s
Val loss: 0.4623 score: 0.9796 time: 0.08s
Test loss: 0.4896 score: 0.9388 time: 0.08s
Epoch 68/1000, LR 0.000268
Train loss: 0.0792;  Loss pred: 0.0792; Loss self: 0.0000; time: 0.12s
Val loss: 0.4482 score: 0.9796 time: 0.08s
Test loss: 0.4772 score: 0.9388 time: 0.08s
Epoch 69/1000, LR 0.000268
Train loss: 0.0718;  Loss pred: 0.0718; Loss self: 0.0000; time: 0.12s
Val loss: 0.4343 score: 0.9796 time: 0.08s
Test loss: 0.4653 score: 0.9388 time: 0.08s
Epoch 70/1000, LR 0.000268
Train loss: 0.0658;  Loss pred: 0.0658; Loss self: 0.0000; time: 0.11s
Val loss: 0.4200 score: 0.9796 time: 0.08s
Test loss: 0.4530 score: 0.9388 time: 0.08s
Epoch 71/1000, LR 0.000268
Train loss: 0.0564;  Loss pred: 0.0564; Loss self: 0.0000; time: 0.23s
Val loss: 0.4057 score: 0.9796 time: 0.08s
Test loss: 0.4407 score: 0.9388 time: 0.08s
Epoch 72/1000, LR 0.000267
Train loss: 0.0561;  Loss pred: 0.0561; Loss self: 0.0000; time: 0.12s
Val loss: 0.3909 score: 0.9796 time: 0.08s
Test loss: 0.4280 score: 0.9388 time: 0.08s
Epoch 73/1000, LR 0.000267
Train loss: 0.0485;  Loss pred: 0.0485; Loss self: 0.0000; time: 0.12s
Val loss: 0.3760 score: 0.9796 time: 0.08s
Test loss: 0.4153 score: 0.9388 time: 0.08s
Epoch 74/1000, LR 0.000267
Train loss: 0.0499;  Loss pred: 0.0499; Loss self: 0.0000; time: 0.12s
Val loss: 0.3610 score: 0.9796 time: 0.08s
Test loss: 0.4026 score: 0.9388 time: 0.08s
Epoch 75/1000, LR 0.000267
Train loss: 0.0446;  Loss pred: 0.0446; Loss self: 0.0000; time: 0.12s
Val loss: 0.3461 score: 0.9796 time: 0.08s
Test loss: 0.3900 score: 0.9388 time: 0.08s
Epoch 76/1000, LR 0.000267
Train loss: 0.0399;  Loss pred: 0.0399; Loss self: 0.0000; time: 0.12s
Val loss: 0.3308 score: 0.9796 time: 0.08s
Test loss: 0.3772 score: 0.9388 time: 0.08s
Epoch 77/1000, LR 0.000267
Train loss: 0.0356;  Loss pred: 0.0356; Loss self: 0.0000; time: 0.12s
Val loss: 0.3156 score: 0.9796 time: 0.08s
Test loss: 0.3652 score: 0.9388 time: 0.14s
Epoch 78/1000, LR 0.000267
Train loss: 0.0342;  Loss pred: 0.0342; Loss self: 0.0000; time: 0.12s
Val loss: 0.3003 score: 0.9796 time: 0.08s
Test loss: 0.3535 score: 0.9388 time: 0.08s
Epoch 79/1000, LR 0.000267
Train loss: 0.0291;  Loss pred: 0.0291; Loss self: 0.0000; time: 0.12s
Val loss: 0.2851 score: 0.9796 time: 0.08s
Test loss: 0.3423 score: 0.9388 time: 0.08s
Epoch 80/1000, LR 0.000267
Train loss: 0.0297;  Loss pred: 0.0297; Loss self: 0.0000; time: 0.12s
Val loss: 0.2701 score: 0.9796 time: 0.08s
Test loss: 0.3316 score: 0.9388 time: 0.08s
Epoch 81/1000, LR 0.000267
Train loss: 0.0261;  Loss pred: 0.0261; Loss self: 0.0000; time: 0.11s
Val loss: 0.2556 score: 0.9796 time: 0.08s
Test loss: 0.3214 score: 0.9388 time: 0.08s
Epoch 82/1000, LR 0.000267
Train loss: 0.0244;  Loss pred: 0.0244; Loss self: 0.0000; time: 0.12s
Val loss: 0.2413 score: 0.9796 time: 0.08s
Test loss: 0.3117 score: 0.9388 time: 0.08s
Epoch 83/1000, LR 0.000266
Train loss: 0.0247;  Loss pred: 0.0247; Loss self: 0.0000; time: 0.12s
Val loss: 0.2289 score: 0.9796 time: 0.08s
Test loss: 0.3022 score: 0.9388 time: 0.08s
Epoch 84/1000, LR 0.000266
Train loss: 0.0218;  Loss pred: 0.0218; Loss self: 0.0000; time: 0.11s
Val loss: 0.2169 score: 0.9796 time: 0.08s
Test loss: 0.2932 score: 0.9388 time: 0.11s
Epoch 85/1000, LR 0.000266
Train loss: 0.0191;  Loss pred: 0.0191; Loss self: 0.0000; time: 0.14s
Val loss: 0.2053 score: 0.9796 time: 0.08s
Test loss: 0.2855 score: 0.9388 time: 0.08s
Epoch 86/1000, LR 0.000266
Train loss: 0.0191;  Loss pred: 0.0191; Loss self: 0.0000; time: 0.11s
Val loss: 0.1950 score: 0.9796 time: 0.08s
Test loss: 0.2777 score: 0.9388 time: 0.08s
Epoch 87/1000, LR 0.000266
Train loss: 0.0159;  Loss pred: 0.0159; Loss self: 0.0000; time: 0.11s
Val loss: 0.1852 score: 0.9796 time: 0.08s
Test loss: 0.2708 score: 0.9388 time: 0.08s
Epoch 88/1000, LR 0.000266
Train loss: 0.0156;  Loss pred: 0.0156; Loss self: 0.0000; time: 0.12s
Val loss: 0.1760 score: 0.9796 time: 0.08s
Test loss: 0.2649 score: 0.9388 time: 0.08s
Epoch 89/1000, LR 0.000266
Train loss: 0.0158;  Loss pred: 0.0158; Loss self: 0.0000; time: 0.12s
Val loss: 0.1687 score: 0.9796 time: 0.08s
Test loss: 0.2592 score: 0.9388 time: 0.08s
Epoch 90/1000, LR 0.000266
Train loss: 0.0149;  Loss pred: 0.0149; Loss self: 0.0000; time: 0.12s
Val loss: 0.1625 score: 0.9388 time: 0.08s
Test loss: 0.2540 score: 0.9388 time: 0.08s
Epoch 91/1000, LR 0.000266
Train loss: 0.0120;  Loss pred: 0.0120; Loss self: 0.0000; time: 0.11s
Val loss: 0.1564 score: 0.9388 time: 0.08s
Test loss: 0.2498 score: 0.9388 time: 0.21s
Epoch 92/1000, LR 0.000266
Train loss: 0.0123;  Loss pred: 0.0123; Loss self: 0.0000; time: 0.12s
Val loss: 0.1507 score: 0.9388 time: 0.09s
Test loss: 0.2464 score: 0.9388 time: 0.08s
Epoch 93/1000, LR 0.000265
Train loss: 0.0121;  Loss pred: 0.0121; Loss self: 0.0000; time: 0.12s
Val loss: 0.1459 score: 0.9388 time: 0.09s
Test loss: 0.2437 score: 0.9388 time: 0.08s
Epoch 94/1000, LR 0.000265
Train loss: 0.0112;  Loss pred: 0.0112; Loss self: 0.0000; time: 0.12s
Val loss: 0.1421 score: 0.9388 time: 0.09s
Test loss: 0.2413 score: 0.9388 time: 0.08s
Epoch 95/1000, LR 0.000265
Train loss: 0.0094;  Loss pred: 0.0094; Loss self: 0.0000; time: 0.12s
Val loss: 0.1387 score: 0.9388 time: 0.09s
Test loss: 0.2396 score: 0.9388 time: 0.08s
Epoch 96/1000, LR 0.000265
Train loss: 0.0095;  Loss pred: 0.0095; Loss self: 0.0000; time: 0.13s
Val loss: 0.1351 score: 0.9388 time: 0.09s
Test loss: 0.2387 score: 0.9388 time: 0.08s
Epoch 97/1000, LR 0.000265
Train loss: 0.0090;  Loss pred: 0.0090; Loss self: 0.0000; time: 0.12s
Val loss: 0.1322 score: 0.9388 time: 0.09s
Test loss: 0.2382 score: 0.9388 time: 0.08s
Epoch 98/1000, LR 0.000265
Train loss: 0.0095;  Loss pred: 0.0095; Loss self: 0.0000; time: 0.14s
Val loss: 0.1295 score: 0.9388 time: 0.15s
Test loss: 0.2383 score: 0.9388 time: 0.08s
Epoch 99/1000, LR 0.000265
Train loss: 0.0082;  Loss pred: 0.0082; Loss self: 0.0000; time: 0.12s
Val loss: 0.1263 score: 0.9388 time: 0.09s
Test loss: 0.2389 score: 0.9388 time: 0.08s
Epoch 100/1000, LR 0.000265
Train loss: 0.0088;  Loss pred: 0.0088; Loss self: 0.0000; time: 0.12s
Val loss: 0.1245 score: 0.9388 time: 0.09s
Test loss: 0.2394 score: 0.9388 time: 0.08s
Epoch 101/1000, LR 0.000265
Train loss: 0.0069;  Loss pred: 0.0069; Loss self: 0.0000; time: 0.12s
Val loss: 0.1223 score: 0.9388 time: 0.09s
Test loss: 0.2406 score: 0.9388 time: 0.08s
Epoch 102/1000, LR 0.000264
Train loss: 0.0074;  Loss pred: 0.0074; Loss self: 0.0000; time: 0.13s
Val loss: 0.1202 score: 0.9388 time: 0.09s
Test loss: 0.2420 score: 0.9388 time: 0.08s
Epoch 103/1000, LR 0.000264
Train loss: 0.0071;  Loss pred: 0.0071; Loss self: 0.0000; time: 0.12s
Val loss: 0.1174 score: 0.9388 time: 0.09s
Test loss: 0.2439 score: 0.9388 time: 0.08s
Epoch 104/1000, LR 0.000264
Train loss: 0.0060;  Loss pred: 0.0060; Loss self: 0.0000; time: 0.13s
Val loss: 0.1140 score: 0.9388 time: 0.16s
Test loss: 0.2464 score: 0.9388 time: 0.08s
Epoch 105/1000, LR 0.000264
Train loss: 0.0065;  Loss pred: 0.0065; Loss self: 0.0000; time: 0.12s
Val loss: 0.1112 score: 0.9388 time: 0.08s
Test loss: 0.2489 score: 0.9388 time: 0.08s
Epoch 106/1000, LR 0.000264
Train loss: 0.0063;  Loss pred: 0.0063; Loss self: 0.0000; time: 0.12s
Val loss: 0.1083 score: 0.9388 time: 0.08s
Test loss: 0.2513 score: 0.9388 time: 0.08s
Epoch 107/1000, LR 0.000264
Train loss: 0.0062;  Loss pred: 0.0062; Loss self: 0.0000; time: 0.12s
Val loss: 0.1067 score: 0.9388 time: 0.08s
Test loss: 0.2529 score: 0.9388 time: 0.08s
Epoch 108/1000, LR 0.000264
Train loss: 0.0050;  Loss pred: 0.0050; Loss self: 0.0000; time: 0.12s
Val loss: 0.1049 score: 0.9388 time: 0.08s
Test loss: 0.2547 score: 0.9388 time: 0.08s
Epoch 109/1000, LR 0.000264
Train loss: 0.0047;  Loss pred: 0.0047; Loss self: 0.0000; time: 0.12s
Val loss: 0.1029 score: 0.9388 time: 0.08s
Test loss: 0.2572 score: 0.9388 time: 0.08s
Epoch 110/1000, LR 0.000263
Train loss: 0.0049;  Loss pred: 0.0049; Loss self: 0.0000; time: 0.12s
Val loss: 0.1002 score: 0.9388 time: 0.09s
Test loss: 0.2599 score: 0.9388 time: 0.19s
Epoch 111/1000, LR 0.000263
Train loss: 0.0045;  Loss pred: 0.0045; Loss self: 0.0000; time: 0.12s
Val loss: 0.0972 score: 0.9388 time: 0.08s
Test loss: 0.2627 score: 0.9388 time: 0.08s
Epoch 112/1000, LR 0.000263
Train loss: 0.0044;  Loss pred: 0.0044; Loss self: 0.0000; time: 0.12s
Val loss: 0.0950 score: 0.9388 time: 0.08s
Test loss: 0.2649 score: 0.9388 time: 0.08s
Epoch 113/1000, LR 0.000263
Train loss: 0.0046;  Loss pred: 0.0046; Loss self: 0.0000; time: 0.12s
Val loss: 0.0928 score: 0.9388 time: 0.08s
Test loss: 0.2672 score: 0.9388 time: 0.08s
Epoch 114/1000, LR 0.000263
Train loss: 0.0042;  Loss pred: 0.0042; Loss self: 0.0000; time: 0.12s
Val loss: 0.0909 score: 0.9388 time: 0.08s
Test loss: 0.2697 score: 0.9388 time: 0.08s
Epoch 115/1000, LR 0.000263
Train loss: 0.0042;  Loss pred: 0.0042; Loss self: 0.0000; time: 0.12s
Val loss: 0.0894 score: 0.9388 time: 0.08s
Test loss: 0.2716 score: 0.9388 time: 0.08s
Epoch 116/1000, LR 0.000263
Train loss: 0.0037;  Loss pred: 0.0037; Loss self: 0.0000; time: 0.12s
Val loss: 0.0882 score: 0.9388 time: 0.08s
Test loss: 0.2734 score: 0.9388 time: 0.21s
Epoch 117/1000, LR 0.000262
Train loss: 0.0045;  Loss pred: 0.0045; Loss self: 0.0000; time: 0.12s
Val loss: 0.0889 score: 0.9388 time: 0.08s
Test loss: 0.2738 score: 0.9388 time: 0.08s
     INFO: Early stopping counter 1 of 20
Epoch 118/1000, LR 0.000262
Train loss: 0.0042;  Loss pred: 0.0042; Loss self: 0.0000; time: 0.12s
Val loss: 0.0902 score: 0.9388 time: 0.08s
Test loss: 0.2735 score: 0.9388 time: 0.08s
     INFO: Early stopping counter 2 of 20
Epoch 119/1000, LR 0.000262
Train loss: 0.0041;  Loss pred: 0.0041; Loss self: 0.0000; time: 0.12s
Val loss: 0.0926 score: 0.9388 time: 0.08s
Test loss: 0.2729 score: 0.9388 time: 0.08s
     INFO: Early stopping counter 3 of 20
Epoch 120/1000, LR 0.000262
Train loss: 0.0037;  Loss pred: 0.0037; Loss self: 0.0000; time: 0.12s
Val loss: 0.0949 score: 0.9388 time: 0.08s
Test loss: 0.2730 score: 0.9388 time: 0.08s
     INFO: Early stopping counter 4 of 20
Epoch 121/1000, LR 0.000262
Train loss: 0.0036;  Loss pred: 0.0036; Loss self: 0.0000; time: 0.12s
Val loss: 0.0967 score: 0.9388 time: 0.08s
Test loss: 0.2731 score: 0.9388 time: 0.08s
     INFO: Early stopping counter 5 of 20
Epoch 122/1000, LR 0.000262
Train loss: 0.0036;  Loss pred: 0.0036; Loss self: 0.0000; time: 0.12s
Val loss: 0.0985 score: 0.9388 time: 0.08s
Test loss: 0.2734 score: 0.9388 time: 0.08s
     INFO: Early stopping counter 6 of 20
Epoch 123/1000, LR 0.000262
Train loss: 0.0030;  Loss pred: 0.0030; Loss self: 0.0000; time: 0.12s
Val loss: 0.0996 score: 0.9388 time: 0.08s
Test loss: 0.2740 score: 0.9388 time: 0.20s
     INFO: Early stopping counter 7 of 20
Epoch 124/1000, LR 0.000261
Train loss: 0.0036;  Loss pred: 0.0036; Loss self: 0.0000; time: 0.12s
Val loss: 0.1014 score: 0.9388 time: 0.08s
Test loss: 0.2741 score: 0.9388 time: 0.08s
     INFO: Early stopping counter 8 of 20
Epoch 125/1000, LR 0.000261
Train loss: 0.0031;  Loss pred: 0.0031; Loss self: 0.0000; time: 0.12s
Val loss: 0.1038 score: 0.9388 time: 0.08s
Test loss: 0.2741 score: 0.9388 time: 0.08s
     INFO: Early stopping counter 9 of 20
Epoch 126/1000, LR 0.000261
Train loss: 0.0028;  Loss pred: 0.0028; Loss self: 0.0000; time: 0.12s
Val loss: 0.1055 score: 0.9388 time: 0.08s
Test loss: 0.2743 score: 0.9388 time: 0.08s
     INFO: Early stopping counter 10 of 20
Epoch 127/1000, LR 0.000261
Train loss: 0.0027;  Loss pred: 0.0027; Loss self: 0.0000; time: 0.12s
Val loss: 0.1062 score: 0.9388 time: 0.08s
Test loss: 0.2753 score: 0.9388 time: 0.08s
     INFO: Early stopping counter 11 of 20
Epoch 128/1000, LR 0.000261
Train loss: 0.0030;  Loss pred: 0.0030; Loss self: 0.0000; time: 0.12s
Val loss: 0.1063 score: 0.9388 time: 0.08s
Test loss: 0.2765 score: 0.9388 time: 0.08s
     INFO: Early stopping counter 12 of 20
Epoch 129/1000, LR 0.000261
Train loss: 0.0028;  Loss pred: 0.0028; Loss self: 0.0000; time: 0.12s
Val loss: 0.1053 score: 0.9388 time: 0.08s
Test loss: 0.2777 score: 0.9388 time: 0.08s
     INFO: Early stopping counter 13 of 20
Epoch 130/1000, LR 0.000260
Train loss: 0.0027;  Loss pred: 0.0027; Loss self: 0.0000; time: 0.12s
Val loss: 0.1054 score: 0.9388 time: 0.18s
Test loss: 0.2786 score: 0.9388 time: 0.08s
     INFO: Early stopping counter 14 of 20
Epoch 131/1000, LR 0.000260
Train loss: 0.0026;  Loss pred: 0.0026; Loss self: 0.0000; time: 0.12s
Val loss: 0.1046 score: 0.9388 time: 0.08s
Test loss: 0.2800 score: 0.9388 time: 0.08s
     INFO: Early stopping counter 15 of 20
Epoch 132/1000, LR 0.000260
Train loss: 0.0023;  Loss pred: 0.0023; Loss self: 0.0000; time: 0.12s
Val loss: 0.1027 score: 0.9388 time: 0.08s
Test loss: 0.2817 score: 0.9388 time: 0.08s
     INFO: Early stopping counter 16 of 20
Epoch 133/1000, LR 0.000260
Train loss: 0.0028;  Loss pred: 0.0028; Loss self: 0.0000; time: 0.12s
Val loss: 0.1013 score: 0.9388 time: 0.08s
Test loss: 0.2828 score: 0.9388 time: 0.08s
     INFO: Early stopping counter 17 of 20
Epoch 134/1000, LR 0.000260
Train loss: 0.0031;  Loss pred: 0.0031; Loss self: 0.0000; time: 0.12s
Val loss: 0.1002 score: 0.9388 time: 0.08s
Test loss: 0.2837 score: 0.9388 time: 0.08s
     INFO: Early stopping counter 18 of 20
Epoch 135/1000, LR 0.000260
Train loss: 0.0023;  Loss pred: 0.0023; Loss self: 0.0000; time: 0.12s
Val loss: 0.0999 score: 0.9388 time: 0.08s
Test loss: 0.2840 score: 0.9388 time: 0.08s
     INFO: Early stopping counter 19 of 20
Epoch 136/1000, LR 0.000260
Train loss: 0.0024;  Loss pred: 0.0024; Loss self: 0.0000; time: 0.12s
Val loss: 0.0987 score: 0.9388 time: 0.08s
Test loss: 0.2849 score: 0.9388 time: 0.20s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 115,   Train_Loss: 0.0037,   Val_Loss: 0.0882,   Val_Precision: 0.8889,   Val_Recall: 1.0000,   Val_accuracy: 0.9412,   Val_Score: 0.9388,   Val_Loss: 0.0882,   Test_Precision: 1.0000,   Test_Recall: 0.8800,   Test_accuracy: 0.9362,   Test_Score: 0.9388,   Test_loss: 0.2734


[0.0856277149869129, 0.08524923003278673, 0.08477041206788272, 0.08497397194150835, 0.08530167606659234, 0.08492067607585341, 0.08501563395839185, 0.08524377108551562, 0.08585474803112447, 0.0851465609157458, 0.08452890906482935, 0.08418850309681147, 0.08392004400957376, 0.08432520099449903, 0.08438508794642985, 0.08347318100277334, 0.08394480403512716, 0.0830164619255811, 0.08330889907665551, 0.08323197800200433, 0.08434443001169711, 0.08393686695490032, 0.08472173905465752, 0.08448618405964226, 0.08485221595037729, 0.08433042711112648, 0.08493644604459405, 0.08501973503734916, 0.08502316800877452, 0.08485796395689249, 0.08467471995390952, 0.08514159801416099, 0.0849472739500925, 0.08532436296809465, 0.08585909206885844, 0.08473956503439695, 0.08494921901728958, 0.09114887891337276, 0.08445318893063813, 0.08436000999063253, 0.08467477408703417, 0.08428751502651721, 0.08526137797161937, 0.08466824202332646, 0.08465997106395662, 0.08453641994856298, 0.08417681697756052, 0.08484289608895779, 0.08466014009900391, 0.08918965910561383, 0.08896407100837678, 0.08967135602142662, 0.0904607450356707, 0.08966146700549871, 0.08961196301970631, 0.08884774800390005, 0.08357804594561458, 0.08389795094262809, 0.08338764600921422, 0.08376439206767827, 0.0849773429799825, 0.08355957199819386, 0.08316746295895427, 0.1012585120042786, 0.08329865394625813, 0.08304291602689773, 0.08284130902029574, 0.08444675500504673, 0.08193125494290143, 0.08550784608814865, 0.08183503104373813, 0.08330793201457709, 0.08267697901464999, 0.08339644793886691, 0.08486227493267506, 0.08292346389498562, 0.14768380497116596, 0.0831423249328509, 0.08205526100937277, 0.08237907500006258, 0.08314207894727588, 0.08393530093599111, 0.08135514392051846, 0.11444024997763336, 0.08072609407827258, 0.08053090190514922, 0.08074295800179243, 0.08210090000648052, 0.08239296800456941, 0.08080919901840389, 0.211293886997737, 0.08734565996564925, 0.0874689529882744, 0.08845451299566776, 0.08921079291030765, 0.08786369697190821, 0.08726405596826226, 0.08779040398076177, 0.08725463098380715, 0.08763895090669394, 0.0887525730067864, 0.08926207502372563, 0.08766789198853076, 0.08112909807823598, 0.08076922700274736, 0.08062120003160089, 0.08152910799253732, 0.0834237999515608, 0.08206715004052967, 0.19128008501138538, 0.08207516802940518, 0.08167775499168783, 0.08223424607422203, 0.0842990759992972, 0.0821992619894445, 0.21167789492756128, 0.08320662996266037, 0.08284029900096357, 0.08282020699698478, 0.08367112698033452, 0.08428442303556949, 0.08206873200833797, 0.20189711404964328, 0.08145886904094368, 0.08097192505374551, 0.08156040508765727, 0.08423401904292405, 0.08290301798842847, 0.08113536704331636, 0.082058175932616, 0.08175102702807635, 0.08199533098377287, 0.08214297308586538, 0.08509628707543015, 0.08261619508266449, 0.20654311799444258]
[0.0017475043874880184, 0.0017397802047507496, 0.001730008409548627, 0.001734162692683844, 0.001740850531971272, 0.001733075021956192, 0.0017350129379263641, 0.001739668797663584, 0.0017521377149209076, 0.0017376849166478735, 0.001725079776833252, 0.0017181327162614586, 0.0017126539593790562, 0.0017209224692754904, 0.001722144651967956, 0.0017035343061790479, 0.0017131592660230032, 0.0016942135086853284, 0.001700181613809296, 0.0016986117959592718, 0.0017213148981979002, 0.0017129972847938842, 0.0017290150827481126, 0.0017242078379518827, 0.001731677876538312, 0.001721029124716867, 0.0017333968580529398, 0.0017350966334152892, 0.0017351666940566227, 0.0017317951827937243, 0.0017280555092634596, 0.001737583632942061, 0.0017336178357161734, 0.0017413135299611153, 0.0017522263687522132, 0.001729378878252999, 0.0017336575309650935, 0.0018601812023137296, 0.0017235344679722069, 0.0017216328569516844, 0.0017280566140211054, 0.0017201533678881064, 0.001740028121869783, 0.0017279233065984991, 0.0017277545115093188, 0.0017252330601747548, 0.0017178942240318473, 0.0017314876752848529, 0.0017277579612041615, 0.001820197124604364, 0.0018155932858852403, 0.0018300276739066656, 0.001846137653789198, 0.0018298258572550757, 0.001828815571830741, 0.0018132193470183685, 0.0017056744070533588, 0.0017122030804617976, 0.0017017886940655963, 0.0017094773891362914, 0.001734231489387398, 0.001705297387718242, 0.0016972951624276383, 0.0020665002449852775, 0.001699972529515472, 0.0016947533883040352, 0.0016906389595978723, 0.0017234031633683005, 0.0016720664274061518, 0.0017450580834316052, 0.0016701026743620026, 0.0017001618778485121, 0.0016872852860132651, 0.0017019683252829983, 0.0017318831618913279, 0.001692315589693584, 0.003013955203493183, 0.001696782141486753, 0.0016745971634565871, 0.0016812056122461753, 0.001696777121372977, 0.0017129653252243083, 0.0016603090596024174, 0.0023355153056659867, 0.0016474713077198487, 0.0016434877939826371, 0.0016478154694243353, 0.0016755285715608268, 0.001681489142950396, 0.0016491673269062018, 0.004312120142810959, 0.0017825644890948826, 0.0017850806732300896, 0.0018051941427687298, 0.0018206284267409723, 0.001793136672896086, 0.0017808991013931073, 0.0017916408975665666, 0.0017807067547715744, 0.0017885500185039578, 0.0018112770001384982, 0.0018216750004841965, 0.0017891406528271583, 0.001655695879147673, 0.00164835157148464, 0.0016453306128898142, 0.001663859346786476, 0.0017025265296236897, 0.0016748397967455034, 0.0039036752043139873, 0.0016750034291715342, 0.0016668929590140373, 0.0016782499198820821, 0.0017203893061081062, 0.001677535958968255, 0.004319957039337985, 0.0016980944890338853, 0.0016906183469584401, 0.001690208306060914, 0.001707574020006827, 0.0017200902660320305, 0.0016748720818028158, 0.00412034926631925, 0.001662425898794769, 0.0016524882664029695, 0.0016644980630134136, 0.001719061613120899, 0.0016918983262944587, 0.001655823817210538, 0.0016746566516860407, 0.0016683883066954358, 0.0016733741017096505, 0.0016763872058339873, 0.0017366589199067379, 0.0016860447976053978, 0.004215165673355971]
[572.2446290606846, 574.7852500386768, 578.0318722617701, 576.6471647780457, 574.4318547943565, 577.0090661575974, 576.3645781196135, 574.8220588556991, 570.731393705055, 575.4783220015952, 579.6833360574842, 582.0272150896078, 583.8891122889543, 581.0837024058386, 580.6713151867147, 587.014888031786, 583.7168906784949, 590.2443788067641, 588.1724586819151, 588.7160341043441, 580.9512257443029, 583.7720870178289, 578.3639541250217, 579.9764842664558, 577.4746063043993, 581.0476915459021, 576.9019341152278, 576.3367761434954, 576.3135054546912, 577.43549002533, 578.6851143608372, 575.5118666183619, 576.8283986227511, 574.2791190638314, 570.7025175703269, 578.2422883585753, 576.8151910852424, 537.5820370382093, 580.2030760525096, 580.8439331081283, 578.6847444037425, 581.3435119612245, 574.7033553259067, 578.7293893086891, 578.7859289838738, 579.6318324080264, 582.1080169028273, 577.5380410001975, 578.7847733620337, 549.3910447844263, 550.7841474047001, 546.4398239755808, 541.671417593103, 546.5000923640359, 546.8019932698556, 551.505255910922, 586.2783634817808, 584.0428693366738, 587.6170193674201, 584.9741016494212, 576.6242892713483, 586.4079820928132, 589.1727155868999, 483.9099353734286, 588.2447996292158, 590.0563509129282, 591.4923433669454, 580.2472812255681, 598.0623637969235, 573.0468283517129, 598.7655821112979, 588.1792863544621, 592.6680024353263, 587.5550003750645, 577.4061564914897, 590.9063333636625, 331.7899346483309, 589.3508515617573, 597.1585416613685, 594.8112430245517, 589.352595225254, 583.782978717945, 602.2975025140578, 428.17103256569914, 606.9908442800324, 608.4620790378469, 606.8640685533497, 596.8265877247662, 594.7109466584882, 606.3666091881507, 231.90448477349844, 560.9895216232886, 560.1987714037078, 553.9570378099293, 549.2608954755565, 557.6819743388021, 561.5141246451024, 558.1475625825549, 561.5747777226117, 559.1121241532039, 552.0966698763003, 548.945338621984, 558.9275490553654, 603.9756531343091, 606.6666949571434, 607.7805835288186, 601.012340334757, 587.362359763657, 597.0720315717174, 256.16885310921634, 597.0137031269276, 599.9185458144218, 595.8588099144747, 581.2637851500117, 596.1124079957344, 231.48378349457977, 588.8953803559781, 591.4995550587046, 591.6430515777863, 585.6261504821922, 581.3648386644487, 597.0605223317174, 242.6978722833635, 601.5305709114513, 605.1480184950032, 600.7817144524615, 581.7127160349613, 591.052065279932, 603.9289866506673, 597.137329012011, 599.3808491625625, 597.5950022044212, 596.5208971530591, 575.8183075198797, 593.1040512210875, 237.23859926099516]
Elapsed: 0.08956848646031336~0.02343524945615325
Time per graph: 0.0018279282951084359~0.000478270397064352
Speed: 565.1894593287642~70.4162947630444
Total Time: 0.2072
best val loss: 0.08818069845438004 test_score: 0.9388

Testing...
Test loss: 0.5947 score: 0.9184 time: 0.08s
test Score 0.9184
Epoch Time List: [0.2877362979343161, 0.2843286528950557, 0.2832665218738839, 0.28427670197561383, 0.2841525968397036, 0.28485411999281496, 0.2855811291374266, 0.2848712479462847, 0.2855846341699362, 0.28514901793096215, 0.2808578348485753, 0.2825691698817536, 0.28162463509943336, 0.28192724904511124, 0.282371794921346, 0.2806592429988086, 0.2800140680046752, 0.27827179490122944, 0.2780663870507851, 0.27745472511742264, 0.27905325312167406, 0.28253101208247244, 0.283092139987275, 0.28522800805512816, 0.28359472507145256, 0.2825097570894286, 0.2836159438593313, 0.2852814479265362, 0.2838711041258648, 0.2837606220273301, 0.2853145720437169, 0.2844492569565773, 0.28361421485897154, 0.2846103199990466, 0.2851144679589197, 0.28430034103803337, 0.28496810887008905, 0.2905121869407594, 0.28273238299880177, 0.28214187012054026, 0.28342367406003177, 0.28313200711272657, 0.28307685104664415, 0.28314915590453893, 0.2821917850524187, 0.28246428014244884, 0.2815815870417282, 0.28408313705585897, 0.28412788012064993, 0.2934969960479066, 0.2969193090684712, 0.29717512405477464, 0.2974234370049089, 0.2981820539571345, 0.29952693497762084, 0.2939436520682648, 0.3318917929427698, 0.28093050490133464, 0.2795163600239903, 0.27926183596719056, 0.28187233314383775, 0.2827939379494637, 0.2777206889586523, 0.3441306200111285, 0.27547101012896746, 0.27548559196293354, 0.2753863949328661, 0.2779595289612189, 0.2765854010358453, 0.27326603105757385, 0.3898353208787739, 0.27461775904521346, 0.27373237011488527, 0.2755599149968475, 0.2817473409231752, 0.2789794948184863, 0.33683497505262494, 0.28190806007478386, 0.27391367696691304, 0.27314767509233207, 0.2740970770828426, 0.2810454040300101, 0.274299115058966, 0.3020463711582124, 0.29941479698754847, 0.2701253170380369, 0.27096121199429035, 0.27378939697518945, 0.2795646330341697, 0.27382581087294966, 0.4018706309143454, 0.2890627689193934, 0.29019388812594116, 0.29378713900223374, 0.2970202862052247, 0.2943583090091124, 0.29047137405723333, 0.3731834809295833, 0.29049203998874873, 0.291826430009678, 0.29503107094205916, 0.30222498800139874, 0.2924110029125586, 0.37408187496475875, 0.271938745980151, 0.2716570470947772, 0.2736074929125607, 0.27910458017140627, 0.2773390719667077, 0.38998571201227605, 0.274966801982373, 0.27253244491294026, 0.2759304540231824, 0.2810881499899551, 0.27666280698031187, 0.4045454680453986, 0.27760292892344296, 0.27775679191108793, 0.2771691040834412, 0.2777524220291525, 0.28230172290932387, 0.27635556319728494, 0.39223616605158895, 0.271580702974461, 0.27053012989927083, 0.2708039201097563, 0.27367505722213537, 0.27883719303645194, 0.2714869669871405, 0.3828011730220169, 0.27259554504416883, 0.27441466599702835, 0.2752734050154686, 0.2830693230498582, 0.2785590400453657, 0.4023870099335909]
Total Epoch List: [136]
Total Time List: [0.20719701098278165]
========================k_idx:1========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: I2BGNNA
I2BGNN(
  (gcs): ModuleList(
    (0): GCNConv(14887, 64)
    (1): GCNConv(64, 64)
  )
  (bns): ModuleList(
    (0-1): 2 x BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  )
  (drops): ModuleList(
    (0-1): 2 x Dropout(p=0.2, inplace=False)
  )
  (lin1): Linear(in_features=64, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=64, bias=True)
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=64, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=64, out_features=2, bias=True)
  )
)
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x759998b5bd30>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.6985;  Loss pred: 0.6985; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6973 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6943 score: 0.5102 time: 0.08s
Epoch 2/1000, LR 0.000000
Train loss: 0.6982;  Loss pred: 0.6982; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6973 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6943 score: 0.5102 time: 0.21s
Epoch 3/1000, LR 0.000030
Train loss: 0.6991;  Loss pred: 0.6991; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6973 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6943 score: 0.5102 time: 0.08s
Epoch 4/1000, LR 0.000060
Train loss: 0.6982;  Loss pred: 0.6982; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6973 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6943 score: 0.5102 time: 0.08s
Epoch 5/1000, LR 0.000090
Train loss: 0.6964;  Loss pred: 0.6964; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6972 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6943 score: 0.5102 time: 0.08s
Epoch 6/1000, LR 0.000120
Train loss: 0.6954;  Loss pred: 0.6954; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6972 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6943 score: 0.5102 time: 0.08s
Epoch 7/1000, LR 0.000150
Train loss: 0.6949;  Loss pred: 0.6949; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6971 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6942 score: 0.5102 time: 0.08s
Epoch 8/1000, LR 0.000180
Train loss: 0.6935;  Loss pred: 0.6935; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6971 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6942 score: 0.5102 time: 0.08s
Epoch 9/1000, LR 0.000210
Train loss: 0.6903;  Loss pred: 0.6903; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6969 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6941 score: 0.5102 time: 0.08s
Epoch 10/1000, LR 0.000240
Train loss: 0.6883;  Loss pred: 0.6883; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6968 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6941 score: 0.5102 time: 0.08s
Epoch 11/1000, LR 0.000270
Train loss: 0.6859;  Loss pred: 0.6859; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6967 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6940 score: 0.5102 time: 0.08s
Epoch 12/1000, LR 0.000270
Train loss: 0.6837;  Loss pred: 0.6837; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6966 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6939 score: 0.5102 time: 0.08s
Epoch 13/1000, LR 0.000270
Train loss: 0.6809;  Loss pred: 0.6809; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6965 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6938 score: 0.5102 time: 0.08s
Epoch 14/1000, LR 0.000270
Train loss: 0.6767;  Loss pred: 0.6767; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6963 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6938 score: 0.5102 time: 0.08s
Epoch 15/1000, LR 0.000270
Train loss: 0.6758;  Loss pred: 0.6758; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6962 score: 0.4898 time: 0.09s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6937 score: 0.5102 time: 0.09s
Epoch 16/1000, LR 0.000270
Train loss: 0.6723;  Loss pred: 0.6723; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6960 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6936 score: 0.5102 time: 0.09s
Epoch 17/1000, LR 0.000270
Train loss: 0.6682;  Loss pred: 0.6682; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6959 score: 0.4898 time: 0.09s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6935 score: 0.5102 time: 0.09s
Epoch 18/1000, LR 0.000270
Train loss: 0.6656;  Loss pred: 0.6656; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6957 score: 0.4898 time: 0.09s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6934 score: 0.5102 time: 0.09s
Epoch 19/1000, LR 0.000270
Train loss: 0.6602;  Loss pred: 0.6602; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6956 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6933 score: 0.5102 time: 0.09s
Epoch 20/1000, LR 0.000270
Train loss: 0.6590;  Loss pred: 0.6590; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6954 score: 0.4898 time: 0.09s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6932 score: 0.5102 time: 0.24s
Epoch 21/1000, LR 0.000270
Train loss: 0.6549;  Loss pred: 0.6549; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6953 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6931 score: 0.5102 time: 0.08s
Epoch 22/1000, LR 0.000270
Train loss: 0.6524;  Loss pred: 0.6524; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6951 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6930 score: 0.5102 time: 0.08s
Epoch 23/1000, LR 0.000270
Train loss: 0.6450;  Loss pred: 0.6450; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6950 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6929 score: 0.5102 time: 0.08s
Epoch 24/1000, LR 0.000270
Train loss: 0.6420;  Loss pred: 0.6420; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6948 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6928 score: 0.5102 time: 0.08s
Epoch 25/1000, LR 0.000270
Train loss: 0.6357;  Loss pred: 0.6357; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6946 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6927 score: 0.5102 time: 0.08s
Epoch 26/1000, LR 0.000270
Train loss: 0.6309;  Loss pred: 0.6309; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6943 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6925 score: 0.5102 time: 0.08s
Epoch 27/1000, LR 0.000270
Train loss: 0.6226;  Loss pred: 0.6226; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6940 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6923 score: 0.5102 time: 0.08s
Epoch 28/1000, LR 0.000270
Train loss: 0.6160;  Loss pred: 0.6160; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6937 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6920 score: 0.5102 time: 0.08s
Epoch 29/1000, LR 0.000270
Train loss: 0.6072;  Loss pred: 0.6072; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6933 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6917 score: 0.5102 time: 0.08s
Epoch 30/1000, LR 0.000270
Train loss: 0.6015;  Loss pred: 0.6015; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6929 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6914 score: 0.5102 time: 0.08s
Epoch 31/1000, LR 0.000270
Train loss: 0.5941;  Loss pred: 0.5941; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6925 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6911 score: 0.5102 time: 0.08s
Epoch 32/1000, LR 0.000270
Train loss: 0.5857;  Loss pred: 0.5857; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6920 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6907 score: 0.5102 time: 0.08s
Epoch 33/1000, LR 0.000270
Train loss: 0.5793;  Loss pred: 0.5793; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6914 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6902 score: 0.5102 time: 0.08s
Epoch 34/1000, LR 0.000270
Train loss: 0.5677;  Loss pred: 0.5677; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6908 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6897 score: 0.5102 time: 0.08s
Epoch 35/1000, LR 0.000270
Train loss: 0.5565;  Loss pred: 0.5565; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6901 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6891 score: 0.5102 time: 0.08s
Epoch 36/1000, LR 0.000270
Train loss: 0.5408;  Loss pred: 0.5408; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6893 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6884 score: 0.5102 time: 0.08s
Epoch 37/1000, LR 0.000270
Train loss: 0.5335;  Loss pred: 0.5335; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6884 score: 0.4898 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6877 score: 0.5102 time: 0.08s
Epoch 38/1000, LR 0.000270
Train loss: 0.5220;  Loss pred: 0.5220; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6874 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6869 score: 0.5102 time: 0.08s
Epoch 39/1000, LR 0.000269
Train loss: 0.5151;  Loss pred: 0.5151; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6864 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6860 score: 0.5102 time: 0.08s
Epoch 40/1000, LR 0.000269
Train loss: 0.5011;  Loss pred: 0.5011; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6852 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6850 score: 0.5102 time: 0.08s
Epoch 41/1000, LR 0.000269
Train loss: 0.4786;  Loss pred: 0.4786; Loss self: 0.0000; time: 0.13s
Val loss: 0.6839 score: 0.5102 time: 0.08s
Test loss: 0.6839 score: 0.5714 time: 0.08s
Epoch 42/1000, LR 0.000269
Train loss: 0.4698;  Loss pred: 0.4698; Loss self: 0.0000; time: 0.13s
Val loss: 0.6825 score: 0.6531 time: 0.08s
Test loss: 0.6827 score: 0.6735 time: 0.08s
Epoch 43/1000, LR 0.000269
Train loss: 0.4535;  Loss pred: 0.4535; Loss self: 0.0000; time: 0.13s
Val loss: 0.6810 score: 0.8571 time: 0.08s
Test loss: 0.6814 score: 0.8163 time: 0.08s
Epoch 44/1000, LR 0.000269
Train loss: 0.4403;  Loss pred: 0.4403; Loss self: 0.0000; time: 0.12s
Val loss: 0.6793 score: 0.8980 time: 0.08s
Test loss: 0.6799 score: 0.8980 time: 0.08s
Epoch 45/1000, LR 0.000269
Train loss: 0.4216;  Loss pred: 0.4216; Loss self: 0.0000; time: 0.13s
Val loss: 0.6774 score: 0.9184 time: 0.08s
Test loss: 0.6783 score: 0.9388 time: 0.08s
Epoch 46/1000, LR 0.000269
Train loss: 0.4163;  Loss pred: 0.4163; Loss self: 0.0000; time: 0.13s
Val loss: 0.6754 score: 0.8980 time: 0.08s
Test loss: 0.6765 score: 0.9592 time: 0.08s
Epoch 47/1000, LR 0.000269
Train loss: 0.4015;  Loss pred: 0.4015; Loss self: 0.0000; time: 0.13s
Val loss: 0.6732 score: 0.8980 time: 0.08s
Test loss: 0.6745 score: 0.9796 time: 0.08s
Epoch 48/1000, LR 0.000269
Train loss: 0.3817;  Loss pred: 0.3817; Loss self: 0.0000; time: 0.13s
Val loss: 0.6707 score: 0.8776 time: 0.08s
Test loss: 0.6722 score: 0.9796 time: 0.08s
Epoch 49/1000, LR 0.000269
Train loss: 0.3625;  Loss pred: 0.3625; Loss self: 0.0000; time: 0.13s
Val loss: 0.6679 score: 0.8776 time: 0.08s
Test loss: 0.6696 score: 0.9796 time: 0.08s
Epoch 50/1000, LR 0.000269
Train loss: 0.3563;  Loss pred: 0.3563; Loss self: 0.0000; time: 0.13s
Val loss: 0.6648 score: 0.8776 time: 0.08s
Test loss: 0.6666 score: 0.9796 time: 0.08s
Epoch 51/1000, LR 0.000269
Train loss: 0.3345;  Loss pred: 0.3345; Loss self: 0.0000; time: 0.13s
Val loss: 0.6613 score: 0.8776 time: 0.08s
Test loss: 0.6633 score: 0.9796 time: 0.08s
Epoch 52/1000, LR 0.000269
Train loss: 0.3239;  Loss pred: 0.3239; Loss self: 0.0000; time: 0.13s
Val loss: 0.6575 score: 0.8776 time: 0.08s
Test loss: 0.6596 score: 0.9796 time: 0.08s
Epoch 53/1000, LR 0.000269
Train loss: 0.3209;  Loss pred: 0.3209; Loss self: 0.0000; time: 0.13s
Val loss: 0.6533 score: 0.8776 time: 0.08s
Test loss: 0.6555 score: 0.9796 time: 0.08s
Epoch 54/1000, LR 0.000269
Train loss: 0.2945;  Loss pred: 0.2945; Loss self: 0.0000; time: 0.13s
Val loss: 0.6488 score: 0.8776 time: 0.08s
Test loss: 0.6510 score: 0.9796 time: 0.08s
Epoch 55/1000, LR 0.000269
Train loss: 0.2855;  Loss pred: 0.2855; Loss self: 0.0000; time: 0.13s
Val loss: 0.6438 score: 0.8776 time: 0.08s
Test loss: 0.6460 score: 0.9796 time: 0.08s
Epoch 56/1000, LR 0.000269
Train loss: 0.2630;  Loss pred: 0.2630; Loss self: 0.0000; time: 0.13s
Val loss: 0.6383 score: 0.8776 time: 0.08s
Test loss: 0.6404 score: 0.9796 time: 0.08s
Epoch 57/1000, LR 0.000269
Train loss: 0.2558;  Loss pred: 0.2558; Loss self: 0.0000; time: 0.13s
Val loss: 0.6324 score: 0.8776 time: 0.08s
Test loss: 0.6344 score: 0.9796 time: 0.08s
Epoch 58/1000, LR 0.000269
Train loss: 0.2424;  Loss pred: 0.2424; Loss self: 0.0000; time: 0.13s
Val loss: 0.6262 score: 0.8776 time: 0.08s
Test loss: 0.6279 score: 0.9796 time: 0.08s
Epoch 59/1000, LR 0.000268
Train loss: 0.2303;  Loss pred: 0.2303; Loss self: 0.0000; time: 0.13s
Val loss: 0.6195 score: 0.8776 time: 0.08s
Test loss: 0.6209 score: 0.9796 time: 0.08s
Epoch 60/1000, LR 0.000268
Train loss: 0.2164;  Loss pred: 0.2164; Loss self: 0.0000; time: 0.13s
Val loss: 0.6124 score: 0.8980 time: 0.08s
Test loss: 0.6135 score: 0.9796 time: 0.08s
Epoch 61/1000, LR 0.000268
Train loss: 0.2080;  Loss pred: 0.2080; Loss self: 0.0000; time: 0.13s
Val loss: 0.6051 score: 0.8980 time: 0.08s
Test loss: 0.6058 score: 0.9796 time: 0.08s
Epoch 62/1000, LR 0.000268
Train loss: 0.1928;  Loss pred: 0.1928; Loss self: 0.0000; time: 0.13s
Val loss: 0.5975 score: 0.8980 time: 0.08s
Test loss: 0.5977 score: 0.9796 time: 0.08s
Epoch 63/1000, LR 0.000268
Train loss: 0.1825;  Loss pred: 0.1825; Loss self: 0.0000; time: 0.13s
Val loss: 0.5892 score: 0.8980 time: 0.08s
Test loss: 0.5888 score: 0.9796 time: 0.08s
Epoch 64/1000, LR 0.000268
Train loss: 0.1741;  Loss pred: 0.1741; Loss self: 0.0000; time: 0.13s
Val loss: 0.5807 score: 0.8980 time: 0.08s
Test loss: 0.5796 score: 0.9796 time: 0.08s
Epoch 65/1000, LR 0.000268
Train loss: 0.1623;  Loss pred: 0.1623; Loss self: 0.0000; time: 0.13s
Val loss: 0.5717 score: 0.8980 time: 0.08s
Test loss: 0.5698 score: 0.9796 time: 0.08s
Epoch 66/1000, LR 0.000268
Train loss: 0.1554;  Loss pred: 0.1554; Loss self: 0.0000; time: 0.13s
Val loss: 0.5620 score: 0.8980 time: 0.08s
Test loss: 0.5590 score: 0.9796 time: 0.08s
Epoch 67/1000, LR 0.000268
Train loss: 0.1394;  Loss pred: 0.1394; Loss self: 0.0000; time: 0.13s
Val loss: 0.5518 score: 0.9184 time: 0.08s
Test loss: 0.5477 score: 0.9796 time: 0.08s
Epoch 68/1000, LR 0.000268
Train loss: 0.1346;  Loss pred: 0.1346; Loss self: 0.0000; time: 0.13s
Val loss: 0.5412 score: 0.9184 time: 0.08s
Test loss: 0.5357 score: 1.0000 time: 0.08s
Epoch 69/1000, LR 0.000268
Train loss: 0.1250;  Loss pred: 0.1250; Loss self: 0.0000; time: 0.13s
Val loss: 0.5299 score: 0.9184 time: 0.08s
Test loss: 0.5227 score: 0.9796 time: 0.08s
Epoch 70/1000, LR 0.000268
Train loss: 0.1205;  Loss pred: 0.1205; Loss self: 0.0000; time: 0.13s
Val loss: 0.5183 score: 0.9184 time: 0.08s
Test loss: 0.5093 score: 0.9796 time: 0.08s
Epoch 71/1000, LR 0.000268
Train loss: 0.1136;  Loss pred: 0.1136; Loss self: 0.0000; time: 0.13s
Val loss: 0.5068 score: 0.9184 time: 0.08s
Test loss: 0.4957 score: 0.9796 time: 0.08s
Epoch 72/1000, LR 0.000267
Train loss: 0.1042;  Loss pred: 0.1042; Loss self: 0.0000; time: 0.13s
Val loss: 0.4944 score: 0.8980 time: 0.08s
Test loss: 0.4807 score: 0.9796 time: 0.08s
Epoch 73/1000, LR 0.000267
Train loss: 0.0962;  Loss pred: 0.0962; Loss self: 0.0000; time: 0.13s
Val loss: 0.4819 score: 0.8980 time: 0.08s
Test loss: 0.4655 score: 0.9796 time: 0.08s
Epoch 74/1000, LR 0.000267
Train loss: 0.0923;  Loss pred: 0.0923; Loss self: 0.0000; time: 0.13s
Val loss: 0.4695 score: 0.8980 time: 0.08s
Test loss: 0.4501 score: 0.9796 time: 0.08s
Epoch 75/1000, LR 0.000267
Train loss: 0.0808;  Loss pred: 0.0808; Loss self: 0.0000; time: 0.13s
Val loss: 0.4561 score: 0.8980 time: 0.08s
Test loss: 0.4335 score: 0.9796 time: 0.08s
Epoch 76/1000, LR 0.000267
Train loss: 0.0798;  Loss pred: 0.0798; Loss self: 0.0000; time: 0.13s
Val loss: 0.4429 score: 0.8980 time: 0.08s
Test loss: 0.4168 score: 0.9796 time: 0.08s
Epoch 77/1000, LR 0.000267
Train loss: 0.0677;  Loss pred: 0.0677; Loss self: 0.0000; time: 0.13s
Val loss: 0.4293 score: 0.8776 time: 0.08s
Test loss: 0.3995 score: 0.9796 time: 0.08s
Epoch 78/1000, LR 0.000267
Train loss: 0.0673;  Loss pred: 0.0673; Loss self: 0.0000; time: 0.13s
Val loss: 0.4154 score: 0.8776 time: 0.08s
Test loss: 0.3817 score: 0.9796 time: 0.08s
Epoch 79/1000, LR 0.000267
Train loss: 0.0606;  Loss pred: 0.0606; Loss self: 0.0000; time: 0.13s
Val loss: 0.4017 score: 0.8776 time: 0.08s
Test loss: 0.3639 score: 0.9796 time: 0.08s
Epoch 80/1000, LR 0.000267
Train loss: 0.0582;  Loss pred: 0.0582; Loss self: 0.0000; time: 0.13s
Val loss: 0.3879 score: 0.8776 time: 0.08s
Test loss: 0.3457 score: 0.9796 time: 0.08s
Epoch 81/1000, LR 0.000267
Train loss: 0.0562;  Loss pred: 0.0562; Loss self: 0.0000; time: 0.13s
Val loss: 0.3749 score: 0.8776 time: 0.08s
Test loss: 0.3282 score: 0.9796 time: 0.08s
Epoch 82/1000, LR 0.000267
Train loss: 0.0508;  Loss pred: 0.0508; Loss self: 0.0000; time: 0.13s
Val loss: 0.3626 score: 0.8776 time: 0.08s
Test loss: 0.3110 score: 0.9796 time: 0.08s
Epoch 83/1000, LR 0.000266
Train loss: 0.0435;  Loss pred: 0.0435; Loss self: 0.0000; time: 0.13s
Val loss: 0.3507 score: 0.8776 time: 0.08s
Test loss: 0.2940 score: 0.9796 time: 0.08s
Epoch 84/1000, LR 0.000266
Train loss: 0.0420;  Loss pred: 0.0420; Loss self: 0.0000; time: 0.13s
Val loss: 0.3396 score: 0.8776 time: 0.08s
Test loss: 0.2777 score: 0.9796 time: 0.08s
Epoch 85/1000, LR 0.000266
Train loss: 0.0416;  Loss pred: 0.0416; Loss self: 0.0000; time: 0.13s
Val loss: 0.3295 score: 0.8776 time: 0.08s
Test loss: 0.2625 score: 0.9796 time: 0.08s
Epoch 86/1000, LR 0.000266
Train loss: 0.0365;  Loss pred: 0.0365; Loss self: 0.0000; time: 0.13s
Val loss: 0.3205 score: 0.8776 time: 0.08s
Test loss: 0.2481 score: 0.9796 time: 0.08s
Epoch 87/1000, LR 0.000266
Train loss: 0.0330;  Loss pred: 0.0330; Loss self: 0.0000; time: 0.13s
Val loss: 0.3119 score: 0.8776 time: 0.08s
Test loss: 0.2342 score: 0.9796 time: 0.08s
Epoch 88/1000, LR 0.000266
Train loss: 0.0336;  Loss pred: 0.0336; Loss self: 0.0000; time: 0.13s
Val loss: 0.3043 score: 0.8776 time: 0.08s
Test loss: 0.2210 score: 0.9796 time: 0.08s
Epoch 89/1000, LR 0.000266
Train loss: 0.0287;  Loss pred: 0.0287; Loss self: 0.0000; time: 0.13s
Val loss: 0.2974 score: 0.8776 time: 0.08s
Test loss: 0.2085 score: 0.9796 time: 0.08s
Epoch 90/1000, LR 0.000266
Train loss: 0.0275;  Loss pred: 0.0275; Loss self: 0.0000; time: 0.13s
Val loss: 0.2913 score: 0.8776 time: 0.08s
Test loss: 0.1968 score: 0.9796 time: 0.08s
Epoch 91/1000, LR 0.000266
Train loss: 0.0248;  Loss pred: 0.0248; Loss self: 0.0000; time: 0.13s
Val loss: 0.2857 score: 0.8776 time: 0.08s
Test loss: 0.1856 score: 0.9796 time: 0.08s
Epoch 92/1000, LR 0.000266
Train loss: 0.0281;  Loss pred: 0.0281; Loss self: 0.0000; time: 0.13s
Val loss: 0.2806 score: 0.8776 time: 0.08s
Test loss: 0.1750 score: 0.9796 time: 0.08s
Epoch 93/1000, LR 0.000265
Train loss: 0.0222;  Loss pred: 0.0222; Loss self: 0.0000; time: 0.13s
Val loss: 0.2763 score: 0.8776 time: 0.08s
Test loss: 0.1651 score: 0.9796 time: 0.08s
Epoch 94/1000, LR 0.000265
Train loss: 0.0207;  Loss pred: 0.0207; Loss self: 0.0000; time: 0.13s
Val loss: 0.2727 score: 0.8776 time: 0.08s
Test loss: 0.1559 score: 0.9796 time: 0.08s
Epoch 95/1000, LR 0.000265
Train loss: 0.0215;  Loss pred: 0.0215; Loss self: 0.0000; time: 0.13s
Val loss: 0.2701 score: 0.8776 time: 0.08s
Test loss: 0.1475 score: 0.9796 time: 0.08s
Epoch 96/1000, LR 0.000265
Train loss: 0.0169;  Loss pred: 0.0169; Loss self: 0.0000; time: 0.13s
Val loss: 0.2683 score: 0.8776 time: 0.08s
Test loss: 0.1399 score: 0.9796 time: 0.08s
Epoch 97/1000, LR 0.000265
Train loss: 0.0168;  Loss pred: 0.0168; Loss self: 0.0000; time: 0.13s
Val loss: 0.2672 score: 0.8776 time: 0.08s
Test loss: 0.1329 score: 0.9796 time: 0.08s
Epoch 98/1000, LR 0.000265
Train loss: 0.0135;  Loss pred: 0.0135; Loss self: 0.0000; time: 0.13s
Val loss: 0.2668 score: 0.8776 time: 0.08s
Test loss: 0.1266 score: 0.9796 time: 0.08s
Epoch 99/1000, LR 0.000265
Train loss: 0.0152;  Loss pred: 0.0152; Loss self: 0.0000; time: 0.13s
Val loss: 0.2672 score: 0.8776 time: 0.08s
Test loss: 0.1210 score: 0.9796 time: 0.08s
     INFO: Early stopping counter 1 of 20
Epoch 100/1000, LR 0.000265
Train loss: 0.0174;  Loss pred: 0.0174; Loss self: 0.0000; time: 0.13s
Val loss: 0.2684 score: 0.8776 time: 0.08s
Test loss: 0.1161 score: 0.9796 time: 0.08s
     INFO: Early stopping counter 2 of 20
Epoch 101/1000, LR 0.000265
Train loss: 0.0129;  Loss pred: 0.0129; Loss self: 0.0000; time: 0.13s
Val loss: 0.2699 score: 0.8980 time: 0.08s
Test loss: 0.1119 score: 0.9796 time: 0.08s
     INFO: Early stopping counter 3 of 20
Epoch 102/1000, LR 0.000264
Train loss: 0.0106;  Loss pred: 0.0106; Loss self: 0.0000; time: 0.13s
Val loss: 0.2721 score: 0.8980 time: 0.08s
Test loss: 0.1083 score: 0.9796 time: 0.08s
     INFO: Early stopping counter 4 of 20
Epoch 103/1000, LR 0.000264
Train loss: 0.0107;  Loss pred: 0.0107; Loss self: 0.0000; time: 0.13s
Val loss: 0.2747 score: 0.8980 time: 0.08s
Test loss: 0.1053 score: 0.9796 time: 0.08s
     INFO: Early stopping counter 5 of 20
Epoch 104/1000, LR 0.000264
Train loss: 0.0119;  Loss pred: 0.0119; Loss self: 0.0000; time: 0.13s
Val loss: 0.2777 score: 0.8980 time: 0.08s
Test loss: 0.1029 score: 0.9796 time: 0.08s
     INFO: Early stopping counter 6 of 20
Epoch 105/1000, LR 0.000264
Train loss: 0.0089;  Loss pred: 0.0089; Loss self: 0.0000; time: 0.13s
Val loss: 0.2811 score: 0.8980 time: 0.08s
Test loss: 0.1011 score: 0.9796 time: 0.08s
     INFO: Early stopping counter 7 of 20
Epoch 106/1000, LR 0.000264
Train loss: 0.0147;  Loss pred: 0.0147; Loss self: 0.0000; time: 0.13s
Val loss: 0.2842 score: 0.8980 time: 0.08s
Test loss: 0.0995 score: 0.9796 time: 0.08s
     INFO: Early stopping counter 8 of 20
Epoch 107/1000, LR 0.000264
Train loss: 0.0085;  Loss pred: 0.0085; Loss self: 0.0000; time: 0.13s
Val loss: 0.2877 score: 0.8980 time: 0.08s
Test loss: 0.0981 score: 0.9796 time: 0.08s
     INFO: Early stopping counter 9 of 20
Epoch 108/1000, LR 0.000264
Train loss: 0.0098;  Loss pred: 0.0098; Loss self: 0.0000; time: 0.13s
Val loss: 0.2910 score: 0.8980 time: 0.08s
Test loss: 0.0970 score: 0.9796 time: 0.08s
     INFO: Early stopping counter 10 of 20
Epoch 109/1000, LR 0.000264
Train loss: 0.0102;  Loss pred: 0.0102; Loss self: 0.0000; time: 0.22s
Val loss: 0.2942 score: 0.8980 time: 0.08s
Test loss: 0.0961 score: 0.9796 time: 0.08s
     INFO: Early stopping counter 11 of 20
Epoch 110/1000, LR 0.000263
Train loss: 0.0060;  Loss pred: 0.0060; Loss self: 0.0000; time: 0.12s
Val loss: 0.2975 score: 0.8980 time: 0.08s
Test loss: 0.0955 score: 0.9796 time: 0.08s
     INFO: Early stopping counter 12 of 20
Epoch 111/1000, LR 0.000263
Train loss: 0.0073;  Loss pred: 0.0073; Loss self: 0.0000; time: 0.12s
Val loss: 0.3008 score: 0.8980 time: 0.08s
Test loss: 0.0948 score: 0.9796 time: 0.08s
     INFO: Early stopping counter 13 of 20
Epoch 112/1000, LR 0.000263
Train loss: 0.0088;  Loss pred: 0.0088; Loss self: 0.0000; time: 0.12s
Val loss: 0.3039 score: 0.8980 time: 0.08s
Test loss: 0.0942 score: 0.9796 time: 0.08s
     INFO: Early stopping counter 14 of 20
Epoch 113/1000, LR 0.000263
Train loss: 0.0060;  Loss pred: 0.0060; Loss self: 0.0000; time: 0.13s
Val loss: 0.3072 score: 0.8980 time: 0.08s
Test loss: 0.0936 score: 0.9796 time: 0.08s
     INFO: Early stopping counter 15 of 20
Epoch 114/1000, LR 0.000263
Train loss: 0.0076;  Loss pred: 0.0076; Loss self: 0.0000; time: 0.12s
Val loss: 0.3102 score: 0.8980 time: 0.08s
Test loss: 0.0929 score: 0.9796 time: 0.08s
     INFO: Early stopping counter 16 of 20
Epoch 115/1000, LR 0.000263
Train loss: 0.0084;  Loss pred: 0.0084; Loss self: 0.0000; time: 0.14s
Val loss: 0.3125 score: 0.8980 time: 0.09s
Test loss: 0.0921 score: 0.9796 time: 0.08s
     INFO: Early stopping counter 17 of 20
Epoch 116/1000, LR 0.000263
Train loss: 0.0056;  Loss pred: 0.0056; Loss self: 0.0000; time: 0.12s
Val loss: 0.3153 score: 0.8980 time: 0.08s
Test loss: 0.0916 score: 0.9796 time: 0.08s
     INFO: Early stopping counter 18 of 20
Epoch 117/1000, LR 0.000262
Train loss: 0.0047;  Loss pred: 0.0047; Loss self: 0.0000; time: 0.12s
Val loss: 0.3182 score: 0.8980 time: 0.08s
Test loss: 0.0913 score: 0.9796 time: 0.08s
     INFO: Early stopping counter 19 of 20
Epoch 118/1000, LR 0.000262
Train loss: 0.0065;  Loss pred: 0.0065; Loss self: 0.0000; time: 0.12s
Val loss: 0.3209 score: 0.8980 time: 0.08s
Test loss: 0.0909 score: 0.9796 time: 0.08s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 097,   Train_Loss: 0.0135,   Val_Loss: 0.2668,   Val_Precision: 0.8800,   Val_Recall: 0.8800,   Val_accuracy: 0.8800,   Val_Score: 0.8776,   Val_Loss: 0.2668,   Test_Precision: 1.0000,   Test_Recall: 0.9583,   Test_accuracy: 0.9787,   Test_Score: 0.9796,   Test_loss: 0.1266


[0.0856277149869129, 0.08524923003278673, 0.08477041206788272, 0.08497397194150835, 0.08530167606659234, 0.08492067607585341, 0.08501563395839185, 0.08524377108551562, 0.08585474803112447, 0.0851465609157458, 0.08452890906482935, 0.08418850309681147, 0.08392004400957376, 0.08432520099449903, 0.08438508794642985, 0.08347318100277334, 0.08394480403512716, 0.0830164619255811, 0.08330889907665551, 0.08323197800200433, 0.08434443001169711, 0.08393686695490032, 0.08472173905465752, 0.08448618405964226, 0.08485221595037729, 0.08433042711112648, 0.08493644604459405, 0.08501973503734916, 0.08502316800877452, 0.08485796395689249, 0.08467471995390952, 0.08514159801416099, 0.0849472739500925, 0.08532436296809465, 0.08585909206885844, 0.08473956503439695, 0.08494921901728958, 0.09114887891337276, 0.08445318893063813, 0.08436000999063253, 0.08467477408703417, 0.08428751502651721, 0.08526137797161937, 0.08466824202332646, 0.08465997106395662, 0.08453641994856298, 0.08417681697756052, 0.08484289608895779, 0.08466014009900391, 0.08918965910561383, 0.08896407100837678, 0.08967135602142662, 0.0904607450356707, 0.08966146700549871, 0.08961196301970631, 0.08884774800390005, 0.08357804594561458, 0.08389795094262809, 0.08338764600921422, 0.08376439206767827, 0.0849773429799825, 0.08355957199819386, 0.08316746295895427, 0.1012585120042786, 0.08329865394625813, 0.08304291602689773, 0.08284130902029574, 0.08444675500504673, 0.08193125494290143, 0.08550784608814865, 0.08183503104373813, 0.08330793201457709, 0.08267697901464999, 0.08339644793886691, 0.08486227493267506, 0.08292346389498562, 0.14768380497116596, 0.0831423249328509, 0.08205526100937277, 0.08237907500006258, 0.08314207894727588, 0.08393530093599111, 0.08135514392051846, 0.11444024997763336, 0.08072609407827258, 0.08053090190514922, 0.08074295800179243, 0.08210090000648052, 0.08239296800456941, 0.08080919901840389, 0.211293886997737, 0.08734565996564925, 0.0874689529882744, 0.08845451299566776, 0.08921079291030765, 0.08786369697190821, 0.08726405596826226, 0.08779040398076177, 0.08725463098380715, 0.08763895090669394, 0.0887525730067864, 0.08926207502372563, 0.08766789198853076, 0.08112909807823598, 0.08076922700274736, 0.08062120003160089, 0.08152910799253732, 0.0834237999515608, 0.08206715004052967, 0.19128008501138538, 0.08207516802940518, 0.08167775499168783, 0.08223424607422203, 0.0842990759992972, 0.0821992619894445, 0.21167789492756128, 0.08320662996266037, 0.08284029900096357, 0.08282020699698478, 0.08367112698033452, 0.08428442303556949, 0.08206873200833797, 0.20189711404964328, 0.08145886904094368, 0.08097192505374551, 0.08156040508765727, 0.08423401904292405, 0.08290301798842847, 0.08113536704331636, 0.082058175932616, 0.08175102702807635, 0.08199533098377287, 0.08214297308586538, 0.08509628707543015, 0.08261619508266449, 0.20654311799444258, 0.08347175794187933, 0.21136021299753338, 0.08396848093252629, 0.08352250000461936, 0.08431082498282194, 0.085251399083063, 0.08586629701312631, 0.08550883701536804, 0.08479133190121502, 0.08541834400966763, 0.0855234459741041, 0.08732377702835947, 0.08666031097527593, 0.08878253994043916, 0.09234418591950089, 0.09238981094677001, 0.09223415597807616, 0.09368531708605587, 0.09123399504460394, 0.2427879050374031, 0.08319639298133552, 0.08258735993877053, 0.08289793191943318, 0.08355919201858342, 0.08516060502734035, 0.08547494397498667, 0.08527661405969411, 0.08561429404653609, 0.08604799397289753, 0.0859159710817039, 0.08556380902882665, 0.08521337399724871, 0.08479930192697793, 0.08468065410852432, 0.08479122293647379, 0.0843285450246185, 0.08466114406473935, 0.08433755789883435, 0.08447127800900489, 0.08464197895955294, 0.08423846098594368, 0.08414773794356734, 0.08448362001217902, 0.0845912320073694, 0.08461100095883012, 0.08440900093410164, 0.08426075999159366, 0.08452241204213351, 0.08439314202405512, 0.08448939607478678, 0.08417158003430814, 0.08485907700378448, 0.0851818589726463, 0.0847264239564538, 0.08562044601421803, 0.08646926493383944, 0.08656194503419101, 0.08601367601659149, 0.08645347005221993, 0.08668488205876201, 0.08623224403709173, 0.08627785998396575, 0.08661644696258008, 0.0863683270290494, 0.08655399596318603, 0.0855661170789972, 0.08650096599012613, 0.08579907205421478, 0.08615009800996631, 0.08639356808271259, 0.08631182205863297, 0.08619539195206016, 0.0856498689390719, 0.08575538103468716, 0.08546555694192648, 0.08557752298656851, 0.084999103914015, 0.08559781697113067, 0.0851071949582547, 0.08542800298891962, 0.08534816699102521, 0.08484460704494268, 0.08513701893389225, 0.08478128409478813, 0.08485350792761892, 0.08485007705166936, 0.08482071501202881, 0.08458001294638962, 0.08519766002427787, 0.08542536303866655, 0.08458903303835541, 0.0851284769596532, 0.08488073199987411, 0.08481090294662863, 0.08484675805084407, 0.08475842501502484, 0.08497169695328921, 0.08469861489720643, 0.08571215393021703, 0.08465855999384075, 0.08493226405698806, 0.08597062004264444, 0.08603189105633646, 0.08589838200714439, 0.0860462459968403, 0.08571144402958453, 0.08476331108249724, 0.08441412495449185, 0.0831863519269973, 0.08304286398924887, 0.08311832300387323, 0.08423011202830821, 0.08298096200451255, 0.08314477698877454, 0.08197593304794282, 0.08193248405586928, 0.08172676898539066, 0.08238350506871939]
[0.0017475043874880184, 0.0017397802047507496, 0.001730008409548627, 0.001734162692683844, 0.001740850531971272, 0.001733075021956192, 0.0017350129379263641, 0.001739668797663584, 0.0017521377149209076, 0.0017376849166478735, 0.001725079776833252, 0.0017181327162614586, 0.0017126539593790562, 0.0017209224692754904, 0.001722144651967956, 0.0017035343061790479, 0.0017131592660230032, 0.0016942135086853284, 0.001700181613809296, 0.0016986117959592718, 0.0017213148981979002, 0.0017129972847938842, 0.0017290150827481126, 0.0017242078379518827, 0.001731677876538312, 0.001721029124716867, 0.0017333968580529398, 0.0017350966334152892, 0.0017351666940566227, 0.0017317951827937243, 0.0017280555092634596, 0.001737583632942061, 0.0017336178357161734, 0.0017413135299611153, 0.0017522263687522132, 0.001729378878252999, 0.0017336575309650935, 0.0018601812023137296, 0.0017235344679722069, 0.0017216328569516844, 0.0017280566140211054, 0.0017201533678881064, 0.001740028121869783, 0.0017279233065984991, 0.0017277545115093188, 0.0017252330601747548, 0.0017178942240318473, 0.0017314876752848529, 0.0017277579612041615, 0.001820197124604364, 0.0018155932858852403, 0.0018300276739066656, 0.001846137653789198, 0.0018298258572550757, 0.001828815571830741, 0.0018132193470183685, 0.0017056744070533588, 0.0017122030804617976, 0.0017017886940655963, 0.0017094773891362914, 0.001734231489387398, 0.001705297387718242, 0.0016972951624276383, 0.0020665002449852775, 0.001699972529515472, 0.0016947533883040352, 0.0016906389595978723, 0.0017234031633683005, 0.0016720664274061518, 0.0017450580834316052, 0.0016701026743620026, 0.0017001618778485121, 0.0016872852860132651, 0.0017019683252829983, 0.0017318831618913279, 0.001692315589693584, 0.003013955203493183, 0.001696782141486753, 0.0016745971634565871, 0.0016812056122461753, 0.001696777121372977, 0.0017129653252243083, 0.0016603090596024174, 0.0023355153056659867, 0.0016474713077198487, 0.0016434877939826371, 0.0016478154694243353, 0.0016755285715608268, 0.001681489142950396, 0.0016491673269062018, 0.004312120142810959, 0.0017825644890948826, 0.0017850806732300896, 0.0018051941427687298, 0.0018206284267409723, 0.001793136672896086, 0.0017808991013931073, 0.0017916408975665666, 0.0017807067547715744, 0.0017885500185039578, 0.0018112770001384982, 0.0018216750004841965, 0.0017891406528271583, 0.001655695879147673, 0.00164835157148464, 0.0016453306128898142, 0.001663859346786476, 0.0017025265296236897, 0.0016748397967455034, 0.0039036752043139873, 0.0016750034291715342, 0.0016668929590140373, 0.0016782499198820821, 0.0017203893061081062, 0.001677535958968255, 0.004319957039337985, 0.0016980944890338853, 0.0016906183469584401, 0.001690208306060914, 0.001707574020006827, 0.0017200902660320305, 0.0016748720818028158, 0.00412034926631925, 0.001662425898794769, 0.0016524882664029695, 0.0016644980630134136, 0.001719061613120899, 0.0016918983262944587, 0.001655823817210538, 0.0016746566516860407, 0.0016683883066954358, 0.0016733741017096505, 0.0016763872058339873, 0.0017366589199067379, 0.0016860447976053978, 0.004215165673355971, 0.0017035052641199864, 0.004313473734643539, 0.0017136424680107407, 0.0017045408164208032, 0.0017206290812820805, 0.0017398244710829184, 0.0017523734084311494, 0.0017450783064360826, 0.0017304353449227555, 0.0017432315104013803, 0.0017453764484511042, 0.0017821178985379484, 0.0017685777750056312, 0.001811888570213044, 0.001884575222846957, 0.0018855063458524492, 0.001882329713838289, 0.0019119452466542016, 0.001861918266216407, 0.0049548552048449615, 0.0016978855710476637, 0.0016854563252810314, 0.001691794528968024, 0.0017052896330323145, 0.0017379715311702114, 0.001744386611734422, 0.001740339062442737, 0.0017472304907456345, 0.00175608150965097, 0.0017533871649327328, 0.0017462001842617684, 0.001739048448923443, 0.0017305979985097538, 0.00172817661445968, 0.0017304331211525264, 0.0017209907147881327, 0.001727778450300803, 0.0017211746509966193, 0.0017239036328368345, 0.0017273873257051622, 0.0017191522650192588, 0.0017173007743585171, 0.001724155510452633, 0.0017263516736197838, 0.001726755121608778, 0.0017226326721245233, 0.0017196073467672175, 0.0017249471845333369, 0.001722309020899084, 0.001724273389281363, 0.0017177873476389416, 0.0017318178980364179, 0.001738405285156047, 0.001729110692988853, 0.0017473560411064904, 0.0017646788762008048, 0.0017665703068202247, 0.0017553811431957446, 0.0017643565316779576, 0.0017690792256890206, 0.0017598417150426883, 0.001760772652733995, 0.001767682591073063, 0.0017626189189601917, 0.0017664080808813475, 0.0017462472873264734, 0.0017653258365331864, 0.001751001470494179, 0.0017581652655095166, 0.0017631340425043386, 0.001761465756298632, 0.0017590896316746973, 0.001747956508960651, 0.0017501098170344318, 0.0017441950396311526, 0.0017464800609503777, 0.0017346755900819386, 0.0017468942239006258, 0.0017368815297602999, 0.001743428632426931, 0.0017417993263474532, 0.0017315225927539322, 0.0017374901823243316, 0.0017302302876487374, 0.0017317042434207943, 0.0017316342255442726, 0.001731035000245486, 0.0017261227131916248, 0.0017387277555975075, 0.001743374755891154, 0.0017263067967011308, 0.001737315856319453, 0.0017322598367321249, 0.0017308347540128293, 0.0017315664908335525, 0.0017297637758168336, 0.001734116264352841, 0.0017285431611674782, 0.0017492276312289189, 0.0017277257141600155, 0.0017333115113671034, 0.0017545024498498865, 0.001755752878700744, 0.0017530282042274366, 0.0017560458366702102, 0.0017492131434609086, 0.0017298634914795356, 0.0017227372439692216, 0.0016976806515713735, 0.0016947523263112015, 0.0016962923062014946, 0.001718981878128739, 0.001693489020500256, 0.0016968321834443783, 0.0016729782254682208, 0.001672091511344271, 0.0016678932445998095, 0.0016812960218105996]
[572.2446290606846, 574.7852500386768, 578.0318722617701, 576.6471647780457, 574.4318547943565, 577.0090661575974, 576.3645781196135, 574.8220588556991, 570.731393705055, 575.4783220015952, 579.6833360574842, 582.0272150896078, 583.8891122889543, 581.0837024058386, 580.6713151867147, 587.014888031786, 583.7168906784949, 590.2443788067641, 588.1724586819151, 588.7160341043441, 580.9512257443029, 583.7720870178289, 578.3639541250217, 579.9764842664558, 577.4746063043993, 581.0476915459021, 576.9019341152278, 576.3367761434954, 576.3135054546912, 577.43549002533, 578.6851143608372, 575.5118666183619, 576.8283986227511, 574.2791190638314, 570.7025175703269, 578.2422883585753, 576.8151910852424, 537.5820370382093, 580.2030760525096, 580.8439331081283, 578.6847444037425, 581.3435119612245, 574.7033553259067, 578.7293893086891, 578.7859289838738, 579.6318324080264, 582.1080169028273, 577.5380410001975, 578.7847733620337, 549.3910447844263, 550.7841474047001, 546.4398239755808, 541.671417593103, 546.5000923640359, 546.8019932698556, 551.505255910922, 586.2783634817808, 584.0428693366738, 587.6170193674201, 584.9741016494212, 576.6242892713483, 586.4079820928132, 589.1727155868999, 483.9099353734286, 588.2447996292158, 590.0563509129282, 591.4923433669454, 580.2472812255681, 598.0623637969235, 573.0468283517129, 598.7655821112979, 588.1792863544621, 592.6680024353263, 587.5550003750645, 577.4061564914897, 590.9063333636625, 331.7899346483309, 589.3508515617573, 597.1585416613685, 594.8112430245517, 589.352595225254, 583.782978717945, 602.2975025140578, 428.17103256569914, 606.9908442800324, 608.4620790378469, 606.8640685533497, 596.8265877247662, 594.7109466584882, 606.3666091881507, 231.90448477349844, 560.9895216232886, 560.1987714037078, 553.9570378099293, 549.2608954755565, 557.6819743388021, 561.5141246451024, 558.1475625825549, 561.5747777226117, 559.1121241532039, 552.0966698763003, 548.945338621984, 558.9275490553654, 603.9756531343091, 606.6666949571434, 607.7805835288186, 601.012340334757, 587.362359763657, 597.0720315717174, 256.16885310921634, 597.0137031269276, 599.9185458144218, 595.8588099144747, 581.2637851500117, 596.1124079957344, 231.48378349457977, 588.8953803559781, 591.4995550587046, 591.6430515777863, 585.6261504821922, 581.3648386644487, 597.0605223317174, 242.6978722833635, 601.5305709114513, 605.1480184950032, 600.7817144524615, 581.7127160349613, 591.052065279932, 603.9289866506673, 597.137329012011, 599.3808491625625, 597.5950022044212, 596.5208971530591, 575.8183075198797, 593.1040512210875, 237.23859926099516, 587.0248957032662, 231.83171186798455, 583.5522979077642, 586.6682630104459, 581.1827841796542, 574.7706257847783, 570.6546305648817, 573.0401875445165, 577.8892594479679, 573.6472717669894, 572.9423018669858, 561.130103019784, 565.4260808500864, 551.9103196740286, 530.623552658905, 530.3615138711685, 531.2565554527022, 523.0275300769961, 537.0805035562029, 201.8222447796616, 588.9678415624675, 593.3111318285041, 591.08832832672, 586.410648742301, 575.3834180049428, 573.2674128963375, 574.6006749951367, 572.3343344204392, 569.4496494065102, 570.3246949673915, 572.6720275331803, 575.0271078526015, 577.8349454125778, 578.6445619232345, 577.8900020903244, 581.0606596579504, 578.777909763779, 580.9985636384812, 580.0788286259437, 578.9089598603922, 581.6820419852675, 582.3091766633258, 579.9940863440303, 579.2562519450152, 579.1209115212139, 580.5068115692359, 581.5281040058092, 579.727894840176, 580.6158986951003, 579.954435425566, 582.144234194341, 577.4279161416608, 575.2398526044723, 578.3319738029326, 572.2932112717926, 566.6753387748995, 566.0686111044009, 569.6768498831303, 566.7788692623078, 565.2658091728595, 568.2329220021603, 567.9324917088389, 565.7124220434591, 567.3376072633569, 566.1205985318246, 572.6565803466544, 566.4676624026745, 571.1017476860105, 568.7747446826052, 567.1718518800815, 567.7090209810834, 568.4758650120489, 572.0966138880699, 571.3927150551638, 573.3303772102639, 572.5802557721915, 576.476665560714, 572.4445054074929, 575.7445069601299, 573.5824119212468, 574.1189497971596, 577.5263945066587, 575.5428204274787, 577.957747670069, 577.4658136914928, 577.4891632704293, 577.6890703297076, 579.3330870149933, 575.1331666390485, 573.6001376759834, 579.2713102392578, 575.6005716303798, 577.2806012096205, 577.7559051674715, 577.5117532556394, 578.1136210508151, 576.6626036306697, 578.5218572873752, 571.6808848356977, 578.795576059467, 576.9303402429241, 569.9621565564408, 569.5562354652093, 570.4414781168351, 569.4612174225396, 571.6856197532613, 578.0802964658845, 580.471574234954, 589.0389332495483, 590.0567206634851, 589.5210373495703, 581.7396987852989, 590.4968900858893, 589.3334707797164, 597.7364108968767, 598.053391943874, 599.5587566756635, 594.7792578032112]
Elapsed: 0.08870389617935527~0.021308988636861342
Time per graph: 0.0018102835954970462~0.0004348773191196192
Speed: 566.7816345323233~60.98162816342698
Total Time: 0.0832
best val loss: 0.2668452560901642 test_score: 0.9796

Testing...
Test loss: 0.6783 score: 0.9388 time: 0.08s
test Score 0.9388
Epoch Time List: [0.2877362979343161, 0.2843286528950557, 0.2832665218738839, 0.28427670197561383, 0.2841525968397036, 0.28485411999281496, 0.2855811291374266, 0.2848712479462847, 0.2855846341699362, 0.28514901793096215, 0.2808578348485753, 0.2825691698817536, 0.28162463509943336, 0.28192724904511124, 0.282371794921346, 0.2806592429988086, 0.2800140680046752, 0.27827179490122944, 0.2780663870507851, 0.27745472511742264, 0.27905325312167406, 0.28253101208247244, 0.283092139987275, 0.28522800805512816, 0.28359472507145256, 0.2825097570894286, 0.2836159438593313, 0.2852814479265362, 0.2838711041258648, 0.2837606220273301, 0.2853145720437169, 0.2844492569565773, 0.28361421485897154, 0.2846103199990466, 0.2851144679589197, 0.28430034103803337, 0.28496810887008905, 0.2905121869407594, 0.28273238299880177, 0.28214187012054026, 0.28342367406003177, 0.28313200711272657, 0.28307685104664415, 0.28314915590453893, 0.2821917850524187, 0.28246428014244884, 0.2815815870417282, 0.28408313705585897, 0.28412788012064993, 0.2934969960479066, 0.2969193090684712, 0.29717512405477464, 0.2974234370049089, 0.2981820539571345, 0.29952693497762084, 0.2939436520682648, 0.3318917929427698, 0.28093050490133464, 0.2795163600239903, 0.27926183596719056, 0.28187233314383775, 0.2827939379494637, 0.2777206889586523, 0.3441306200111285, 0.27547101012896746, 0.27548559196293354, 0.2753863949328661, 0.2779595289612189, 0.2765854010358453, 0.27326603105757385, 0.3898353208787739, 0.27461775904521346, 0.27373237011488527, 0.2755599149968475, 0.2817473409231752, 0.2789794948184863, 0.33683497505262494, 0.28190806007478386, 0.27391367696691304, 0.27314767509233207, 0.2740970770828426, 0.2810454040300101, 0.274299115058966, 0.3020463711582124, 0.29941479698754847, 0.2701253170380369, 0.27096121199429035, 0.27378939697518945, 0.2795646330341697, 0.27382581087294966, 0.4018706309143454, 0.2890627689193934, 0.29019388812594116, 0.29378713900223374, 0.2970202862052247, 0.2943583090091124, 0.29047137405723333, 0.3731834809295833, 0.29049203998874873, 0.291826430009678, 0.29503107094205916, 0.30222498800139874, 0.2924110029125586, 0.37408187496475875, 0.271938745980151, 0.2716570470947772, 0.2736074929125607, 0.27910458017140627, 0.2773390719667077, 0.38998571201227605, 0.274966801982373, 0.27253244491294026, 0.2759304540231824, 0.2810881499899551, 0.27666280698031187, 0.4045454680453986, 0.27760292892344296, 0.27775679191108793, 0.2771691040834412, 0.2777524220291525, 0.28230172290932387, 0.27635556319728494, 0.39223616605158895, 0.271580702974461, 0.27053012989927083, 0.2708039201097563, 0.27367505722213537, 0.27883719303645194, 0.2714869669871405, 0.3828011730220169, 0.27259554504416883, 0.27441466599702835, 0.2752734050154686, 0.2830693230498582, 0.2785590400453657, 0.4023870099335909, 0.280495151062496, 0.404762526974082, 0.27670021809171885, 0.2775738650234416, 0.27986029884777963, 0.28412452002521604, 0.2884433639701456, 0.28384514595381916, 0.4078543980140239, 0.2828144630184397, 0.2830921560525894, 0.28911775001324713, 0.2920024080667645, 0.29073917295318097, 0.42828500794712454, 0.30438260093797, 0.3051004580920562, 0.3086172849871218, 0.3074847040697932, 0.45522990997415036, 0.27221744996495545, 0.274040866876021, 0.2744631819659844, 0.27684291708283126, 0.28595552290789783, 0.2867968857754022, 0.28515475103631616, 0.28549913596361876, 0.2872735619312152, 0.2868501520715654, 0.2868768780026585, 0.28567348793148994, 0.2837377678370103, 0.283393498044461, 0.2849806279409677, 0.282922426937148, 0.39766810182482004, 0.28209414903540164, 0.28252584498841316, 0.2829179500695318, 0.282082807039842, 0.2837253351463005, 0.2824726300314069, 0.28140076505951583, 0.2830400001257658, 0.28349369508214295, 0.2824744461104274, 0.2828248069854453, 0.282057334901765, 0.28367034916300327, 0.2829745749477297, 0.2831376229878515, 0.2853687749011442, 0.2854982160497457, 0.2853959809290245, 0.2897763920482248, 0.28904004010837525, 0.28781565104145557, 0.2890286879846826, 0.2899064200464636, 0.2904198960168287, 0.28974979114718735, 0.2884431049460545, 0.2896809459198266, 0.2893890191335231, 0.28973741410300136, 0.2895574829308316, 0.2875701910816133, 0.28893923410214484, 0.2885547149926424, 0.28869206889066845, 0.2886086867656559, 0.28719487600028515, 0.28839989902917296, 0.28614055505022407, 0.2851907720323652, 0.2860745817888528, 0.28604157897643745, 0.2849280061200261, 0.285666233045049, 0.28484740294516087, 0.284722248907201, 0.2851110539631918, 0.28362499608192593, 0.28386253002099693, 0.28495602298062295, 0.28387382184155285, 0.2836468000896275, 0.28501601598691195, 0.2846235368633643, 0.2828248990699649, 0.28437302296515554, 0.2852591589326039, 0.284654812887311, 0.2835767240030691, 0.2842415760969743, 0.28512679203413427, 0.2846146129304543, 0.2841071131406352, 0.2844574550399557, 0.2847298451233655, 0.2864515690598637, 0.28943864698521793, 0.2882845759158954, 0.28885234403423965, 0.2900865270057693, 0.28442926495335996, 0.2820079019293189, 0.37621158885303885, 0.2771008249837905, 0.27608991495799273, 0.2767153949243948, 0.28208703408017755, 0.2748103690100834, 0.3033308460144326, 0.2723639828618616, 0.27172906103078276, 0.27213160297833383]
Total Epoch List: [136, 118]
Total Time List: [0.20719701098278165, 0.08322357700672]
========================k_idx:2========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: I2BGNNA
I2BGNN(
  (gcs): ModuleList(
    (0): GCNConv(14887, 64)
    (1): GCNConv(64, 64)
  )
  (bns): ModuleList(
    (0-1): 2 x BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  )
  (drops): ModuleList(
    (0-1): 2 x Dropout(p=0.2, inplace=False)
  )
  (lin1): Linear(in_features=64, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=64, bias=True)
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=64, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=64, out_features=2, bias=True)
  )
)
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x759998b88e80>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.6992;  Loss pred: 0.6992; Loss self: 0.0000; time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6964 score: 0.5102 time: 0.09s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6985 score: 0.5000 time: 0.08s
Epoch 2/1000, LR 0.000000
Train loss: 0.7011;  Loss pred: 0.7011; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6963 score: 0.5102 time: 0.09s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6984 score: 0.5000 time: 0.08s
Epoch 3/1000, LR 0.000030
Train loss: 0.7006;  Loss pred: 0.7006; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6962 score: 0.5102 time: 0.09s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6982 score: 0.5000 time: 0.08s
Epoch 4/1000, LR 0.000060
Train loss: 0.6998;  Loss pred: 0.6998; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6960 score: 0.5102 time: 0.09s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6981 score: 0.5000 time: 0.08s
Epoch 5/1000, LR 0.000090
Train loss: 0.6974;  Loss pred: 0.6974; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6958 score: 0.5102 time: 0.09s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6978 score: 0.5000 time: 0.08s
Epoch 6/1000, LR 0.000120
Train loss: 0.6956;  Loss pred: 0.6956; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6956 score: 0.5102 time: 0.09s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6975 score: 0.5000 time: 0.08s
Epoch 7/1000, LR 0.000150
Train loss: 0.6930;  Loss pred: 0.6930; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6954 score: 0.5102 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6972 score: 0.5000 time: 0.07s
Epoch 8/1000, LR 0.000180
Train loss: 0.6933;  Loss pred: 0.6933; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6952 score: 0.5102 time: 0.09s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6969 score: 0.5000 time: 0.07s
Epoch 9/1000, LR 0.000210
Train loss: 0.6876;  Loss pred: 0.6876; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6949 score: 0.5102 time: 0.09s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6966 score: 0.5000 time: 0.07s
Epoch 10/1000, LR 0.000240
Train loss: 0.6849;  Loss pred: 0.6849; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6947 score: 0.5102 time: 0.09s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6963 score: 0.5000 time: 0.07s
Epoch 11/1000, LR 0.000270
Train loss: 0.6835;  Loss pred: 0.6835; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6945 score: 0.5102 time: 0.09s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6961 score: 0.5000 time: 0.08s
Epoch 12/1000, LR 0.000270
Train loss: 0.6800;  Loss pred: 0.6800; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6943 score: 0.5102 time: 0.09s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6958 score: 0.5000 time: 0.07s
Epoch 13/1000, LR 0.000270
Train loss: 0.6733;  Loss pred: 0.6733; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6941 score: 0.5102 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6956 score: 0.5000 time: 0.07s
Epoch 14/1000, LR 0.000270
Train loss: 0.6713;  Loss pred: 0.6713; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6940 score: 0.5102 time: 0.09s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6954 score: 0.5000 time: 0.07s
Epoch 15/1000, LR 0.000270
Train loss: 0.6684;  Loss pred: 0.6684; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6938 score: 0.5102 time: 0.09s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6952 score: 0.5000 time: 0.07s
Epoch 16/1000, LR 0.000270
Train loss: 0.6631;  Loss pred: 0.6631; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6937 score: 0.5102 time: 0.09s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6950 score: 0.5000 time: 0.07s
Epoch 17/1000, LR 0.000270
Train loss: 0.6578;  Loss pred: 0.6578; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6935 score: 0.5102 time: 0.09s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6947 score: 0.5000 time: 0.08s
Epoch 18/1000, LR 0.000270
Train loss: 0.6506;  Loss pred: 0.6506; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6934 score: 0.5102 time: 0.09s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6945 score: 0.5000 time: 0.07s
Epoch 19/1000, LR 0.000270
Train loss: 0.6500;  Loss pred: 0.6500; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6932 score: 0.5102 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6944 score: 0.5000 time: 0.07s
Epoch 20/1000, LR 0.000270
Train loss: 0.6389;  Loss pred: 0.6389; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6931 score: 0.5102 time: 0.09s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6942 score: 0.5000 time: 0.07s
Epoch 21/1000, LR 0.000270
Train loss: 0.6333;  Loss pred: 0.6333; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6929 score: 0.5102 time: 0.09s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6940 score: 0.5000 time: 0.07s
Epoch 22/1000, LR 0.000270
Train loss: 0.6277;  Loss pred: 0.6277; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6928 score: 0.5102 time: 0.09s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6938 score: 0.5000 time: 0.08s
Epoch 23/1000, LR 0.000270
Train loss: 0.6204;  Loss pred: 0.6204; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6926 score: 0.5102 time: 0.09s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6936 score: 0.5000 time: 0.08s
Epoch 24/1000, LR 0.000270
Train loss: 0.6115;  Loss pred: 0.6115; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6924 score: 0.5102 time: 0.09s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6934 score: 0.5000 time: 0.07s
Epoch 25/1000, LR 0.000270
Train loss: 0.6022;  Loss pred: 0.6022; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6922 score: 0.5102 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6931 score: 0.5000 time: 0.08s
Epoch 26/1000, LR 0.000270
Train loss: 0.5888;  Loss pred: 0.5888; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6919 score: 0.5102 time: 0.09s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6929 score: 0.5000 time: 0.08s
Epoch 27/1000, LR 0.000270
Train loss: 0.5849;  Loss pred: 0.5849; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6917 score: 0.5102 time: 0.09s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6926 score: 0.5000 time: 0.08s
Epoch 28/1000, LR 0.000270
Train loss: 0.5756;  Loss pred: 0.5756; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6914 score: 0.5102 time: 0.09s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6923 score: 0.5000 time: 0.08s
Epoch 29/1000, LR 0.000270
Train loss: 0.5617;  Loss pred: 0.5617; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6910 score: 0.5102 time: 0.09s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6920 score: 0.5000 time: 0.08s
Epoch 30/1000, LR 0.000270
Train loss: 0.5527;  Loss pred: 0.5527; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6906 score: 0.5102 time: 0.09s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6915 score: 0.5000 time: 0.08s
Epoch 31/1000, LR 0.000270
Train loss: 0.5415;  Loss pred: 0.5415; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6900 score: 0.5102 time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6909 score: 0.5000 time: 0.08s
Epoch 32/1000, LR 0.000270
Train loss: 0.5316;  Loss pred: 0.5316; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6894 score: 0.5102 time: 0.09s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6904 score: 0.5000 time: 0.08s
Epoch 33/1000, LR 0.000270
Train loss: 0.5134;  Loss pred: 0.5134; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6888 score: 0.5102 time: 0.09s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6898 score: 0.5000 time: 0.08s
Epoch 34/1000, LR 0.000270
Train loss: 0.5054;  Loss pred: 0.5054; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6882 score: 0.5102 time: 0.09s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6891 score: 0.5000 time: 0.08s
Epoch 35/1000, LR 0.000270
Train loss: 0.4917;  Loss pred: 0.4917; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6875 score: 0.5102 time: 0.09s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6884 score: 0.5000 time: 0.08s
Epoch 36/1000, LR 0.000270
Train loss: 0.4775;  Loss pred: 0.4775; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6867 score: 0.5102 time: 0.09s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6877 score: 0.5000 time: 0.08s
Epoch 37/1000, LR 0.000270
Train loss: 0.4706;  Loss pred: 0.4706; Loss self: 0.0000; time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6858 score: 0.5102 time: 0.09s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6868 score: 0.5000 time: 0.07s
Epoch 38/1000, LR 0.000270
Train loss: 0.4554;  Loss pred: 0.4554; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6849 score: 0.5102 time: 0.09s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6859 score: 0.5000 time: 0.08s
Epoch 39/1000, LR 0.000269
Train loss: 0.4430;  Loss pred: 0.4430; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6839 score: 0.5102 time: 0.09s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6849 score: 0.5000 time: 0.08s
Epoch 40/1000, LR 0.000269
Train loss: 0.4281;  Loss pred: 0.4281; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6827 score: 0.5102 time: 0.09s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6837 score: 0.5000 time: 0.08s
Epoch 41/1000, LR 0.000269
Train loss: 0.4172;  Loss pred: 0.4172; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6815 score: 0.5102 time: 0.09s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6824 score: 0.5000 time: 0.07s
Epoch 42/1000, LR 0.000269
Train loss: 0.4041;  Loss pred: 0.4041; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6800 score: 0.5102 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6809 score: 0.5000 time: 0.07s
Epoch 43/1000, LR 0.000269
Train loss: 0.3869;  Loss pred: 0.3869; Loss self: 0.0000; time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6784 score: 0.5102 time: 0.09s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6793 score: 0.5000 time: 0.07s
Epoch 44/1000, LR 0.000269
Train loss: 0.3791;  Loss pred: 0.3791; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6766 score: 0.5102 time: 0.09s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6774 score: 0.5000 time: 0.07s
Epoch 45/1000, LR 0.000269
Train loss: 0.3633;  Loss pred: 0.3633; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6746 score: 0.5102 time: 0.09s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6753 score: 0.5000 time: 0.07s
Epoch 46/1000, LR 0.000269
Train loss: 0.3459;  Loss pred: 0.3459; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6724 score: 0.5102 time: 0.09s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6729 score: 0.5000 time: 0.08s
Epoch 47/1000, LR 0.000269
Train loss: 0.3359;  Loss pred: 0.3359; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6699 score: 0.5102 time: 0.09s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6703 score: 0.5000 time: 0.08s
Epoch 48/1000, LR 0.000269
Train loss: 0.3165;  Loss pred: 0.3165; Loss self: 0.0000; time: 0.12s
Val loss: 0.6671 score: 0.5306 time: 0.09s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6674 score: 0.5000 time: 0.08s
Epoch 49/1000, LR 0.000269
Train loss: 0.3098;  Loss pred: 0.3098; Loss self: 0.0000; time: 0.24s
Val loss: 0.6640 score: 0.5306 time: 0.09s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6641 score: 0.5000 time: 0.08s
Epoch 50/1000, LR 0.000269
Train loss: 0.2913;  Loss pred: 0.2913; Loss self: 0.0000; time: 0.12s
Val loss: 0.6605 score: 0.5306 time: 0.09s
Test loss: 0.6603 score: 0.5625 time: 0.08s
Epoch 51/1000, LR 0.000269
Train loss: 0.2787;  Loss pred: 0.2787; Loss self: 0.0000; time: 0.13s
Val loss: 0.6566 score: 0.5510 time: 0.09s
Test loss: 0.6561 score: 0.5833 time: 0.08s
Epoch 52/1000, LR 0.000269
Train loss: 0.2664;  Loss pred: 0.2664; Loss self: 0.0000; time: 0.13s
Val loss: 0.6522 score: 0.5510 time: 0.09s
Test loss: 0.6516 score: 0.5833 time: 0.08s
Epoch 53/1000, LR 0.000269
Train loss: 0.2534;  Loss pred: 0.2534; Loss self: 0.0000; time: 0.12s
Val loss: 0.6474 score: 0.5714 time: 0.09s
Test loss: 0.6465 score: 0.5833 time: 0.08s
Epoch 54/1000, LR 0.000269
Train loss: 0.2458;  Loss pred: 0.2458; Loss self: 0.0000; time: 0.12s
Val loss: 0.6419 score: 0.5714 time: 0.09s
Test loss: 0.6409 score: 0.5833 time: 0.18s
Epoch 55/1000, LR 0.000269
Train loss: 0.2299;  Loss pred: 0.2299; Loss self: 0.0000; time: 0.12s
Val loss: 0.6357 score: 0.5714 time: 0.09s
Test loss: 0.6345 score: 0.5833 time: 0.07s
Epoch 56/1000, LR 0.000269
Train loss: 0.2179;  Loss pred: 0.2179; Loss self: 0.0000; time: 0.12s
Val loss: 0.6290 score: 0.5714 time: 0.09s
Test loss: 0.6276 score: 0.6042 time: 0.07s
Epoch 57/1000, LR 0.000269
Train loss: 0.2051;  Loss pred: 0.2051; Loss self: 0.0000; time: 0.12s
Val loss: 0.6219 score: 0.5714 time: 0.09s
Test loss: 0.6203 score: 0.6042 time: 0.07s
Epoch 58/1000, LR 0.000269
Train loss: 0.1946;  Loss pred: 0.1946; Loss self: 0.0000; time: 0.12s
Val loss: 0.6142 score: 0.5918 time: 0.09s
Test loss: 0.6123 score: 0.6250 time: 0.08s
Epoch 59/1000, LR 0.000268
Train loss: 0.1833;  Loss pred: 0.1833; Loss self: 0.0000; time: 0.13s
Val loss: 0.6059 score: 0.5918 time: 0.09s
Test loss: 0.6039 score: 0.6250 time: 0.08s
Epoch 60/1000, LR 0.000268
Train loss: 0.1739;  Loss pred: 0.1739; Loss self: 0.0000; time: 0.12s
Val loss: 0.5971 score: 0.6122 time: 0.09s
Test loss: 0.5949 score: 0.6250 time: 0.19s
Epoch 61/1000, LR 0.000268
Train loss: 0.1587;  Loss pred: 0.1587; Loss self: 0.0000; time: 0.12s
Val loss: 0.5877 score: 0.6122 time: 0.09s
Test loss: 0.5852 score: 0.6250 time: 0.07s
Epoch 62/1000, LR 0.000268
Train loss: 0.1523;  Loss pred: 0.1523; Loss self: 0.0000; time: 0.12s
Val loss: 0.5775 score: 0.6531 time: 0.09s
Test loss: 0.5749 score: 0.6458 time: 0.07s
Epoch 63/1000, LR 0.000268
Train loss: 0.1381;  Loss pred: 0.1381; Loss self: 0.0000; time: 0.12s
Val loss: 0.5666 score: 0.6735 time: 0.09s
Test loss: 0.5638 score: 0.6875 time: 0.07s
Epoch 64/1000, LR 0.000268
Train loss: 0.1321;  Loss pred: 0.1321; Loss self: 0.0000; time: 0.13s
Val loss: 0.5548 score: 0.6735 time: 0.09s
Test loss: 0.5519 score: 0.7083 time: 0.08s
Epoch 65/1000, LR 0.000268
Train loss: 0.1232;  Loss pred: 0.1232; Loss self: 0.0000; time: 0.13s
Val loss: 0.5426 score: 0.6939 time: 0.09s
Test loss: 0.5396 score: 0.7083 time: 0.08s
Epoch 66/1000, LR 0.000268
Train loss: 0.1156;  Loss pred: 0.1156; Loss self: 0.0000; time: 0.13s
Val loss: 0.5298 score: 0.7143 time: 0.11s
Test loss: 0.5266 score: 0.7083 time: 0.09s
Epoch 67/1000, LR 0.000268
Train loss: 0.1061;  Loss pred: 0.1061; Loss self: 0.0000; time: 0.13s
Val loss: 0.5165 score: 0.7347 time: 0.09s
Test loss: 0.5130 score: 0.7292 time: 0.08s
Epoch 68/1000, LR 0.000268
Train loss: 0.0973;  Loss pred: 0.0973; Loss self: 0.0000; time: 0.12s
Val loss: 0.5023 score: 0.7755 time: 0.09s
Test loss: 0.4987 score: 0.7292 time: 0.08s
Epoch 69/1000, LR 0.000268
Train loss: 0.0890;  Loss pred: 0.0890; Loss self: 0.0000; time: 0.12s
Val loss: 0.4879 score: 0.7755 time: 0.09s
Test loss: 0.4841 score: 0.7292 time: 0.07s
Epoch 70/1000, LR 0.000268
Train loss: 0.0849;  Loss pred: 0.0849; Loss self: 0.0000; time: 0.12s
Val loss: 0.4725 score: 0.7755 time: 0.08s
Test loss: 0.4686 score: 0.7708 time: 0.07s
Epoch 71/1000, LR 0.000268
Train loss: 0.0785;  Loss pred: 0.0785; Loss self: 0.0000; time: 0.12s
Val loss: 0.4564 score: 0.8163 time: 0.09s
Test loss: 0.4525 score: 0.7917 time: 0.07s
Epoch 72/1000, LR 0.000267
Train loss: 0.0751;  Loss pred: 0.0751; Loss self: 0.0000; time: 0.12s
Val loss: 0.4402 score: 0.8163 time: 0.08s
Test loss: 0.4363 score: 0.7917 time: 0.07s
Epoch 73/1000, LR 0.000267
Train loss: 0.0669;  Loss pred: 0.0669; Loss self: 0.0000; time: 0.12s
Val loss: 0.4236 score: 0.8163 time: 0.09s
Test loss: 0.4195 score: 0.8125 time: 0.19s
Epoch 74/1000, LR 0.000267
Train loss: 0.0607;  Loss pred: 0.0607; Loss self: 0.0000; time: 0.12s
Val loss: 0.4069 score: 0.8571 time: 0.09s
Test loss: 0.4027 score: 0.8750 time: 0.07s
Epoch 75/1000, LR 0.000267
Train loss: 0.0558;  Loss pred: 0.0558; Loss self: 0.0000; time: 0.12s
Val loss: 0.3904 score: 0.8776 time: 0.09s
Test loss: 0.3859 score: 0.8958 time: 0.07s
Epoch 76/1000, LR 0.000267
Train loss: 0.0499;  Loss pred: 0.0499; Loss self: 0.0000; time: 0.12s
Val loss: 0.3742 score: 0.8980 time: 0.09s
Test loss: 0.3696 score: 0.8958 time: 0.07s
Epoch 77/1000, LR 0.000267
Train loss: 0.0501;  Loss pred: 0.0501; Loss self: 0.0000; time: 0.12s
Val loss: 0.3584 score: 0.8980 time: 0.09s
Test loss: 0.3536 score: 0.8958 time: 0.07s
Epoch 78/1000, LR 0.000267
Train loss: 0.0426;  Loss pred: 0.0426; Loss self: 0.0000; time: 0.12s
Val loss: 0.3431 score: 0.9184 time: 0.09s
Test loss: 0.3381 score: 0.8750 time: 0.07s
Epoch 79/1000, LR 0.000267
Train loss: 0.0387;  Loss pred: 0.0387; Loss self: 0.0000; time: 0.12s
Val loss: 0.3282 score: 0.9184 time: 0.22s
Test loss: 0.3227 score: 0.8750 time: 0.08s
Epoch 80/1000, LR 0.000267
Train loss: 0.0365;  Loss pred: 0.0365; Loss self: 0.0000; time: 0.12s
Val loss: 0.3140 score: 0.9184 time: 0.09s
Test loss: 0.3079 score: 0.8750 time: 0.08s
Epoch 81/1000, LR 0.000267
Train loss: 0.0324;  Loss pred: 0.0324; Loss self: 0.0000; time: 0.12s
Val loss: 0.3010 score: 0.9184 time: 0.09s
Test loss: 0.2942 score: 0.8958 time: 0.08s
Epoch 82/1000, LR 0.000267
Train loss: 0.0284;  Loss pred: 0.0284; Loss self: 0.0000; time: 0.12s
Val loss: 0.2890 score: 0.9388 time: 0.10s
Test loss: 0.2813 score: 0.8958 time: 0.08s
Epoch 83/1000, LR 0.000266
Train loss: 0.0255;  Loss pred: 0.0255; Loss self: 0.0000; time: 0.13s
Val loss: 0.2781 score: 0.9592 time: 0.10s
Test loss: 0.2691 score: 0.8958 time: 0.08s
Epoch 84/1000, LR 0.000266
Train loss: 0.0240;  Loss pred: 0.0240; Loss self: 0.0000; time: 0.13s
Val loss: 0.2679 score: 0.9592 time: 0.09s
Test loss: 0.2575 score: 0.8750 time: 0.08s
Epoch 85/1000, LR 0.000266
Train loss: 0.0226;  Loss pred: 0.0226; Loss self: 0.0000; time: 0.27s
Val loss: 0.2586 score: 0.9592 time: 0.09s
Test loss: 0.2465 score: 0.8958 time: 0.08s
Epoch 86/1000, LR 0.000266
Train loss: 0.0200;  Loss pred: 0.0200; Loss self: 0.0000; time: 0.13s
Val loss: 0.2506 score: 0.9388 time: 0.09s
Test loss: 0.2368 score: 0.8958 time: 0.08s
Epoch 87/1000, LR 0.000266
Train loss: 0.0192;  Loss pred: 0.0192; Loss self: 0.0000; time: 0.13s
Val loss: 0.2438 score: 0.9388 time: 0.09s
Test loss: 0.2279 score: 0.8958 time: 0.08s
Epoch 88/1000, LR 0.000266
Train loss: 0.0183;  Loss pred: 0.0183; Loss self: 0.0000; time: 0.13s
Val loss: 0.2379 score: 0.9388 time: 0.09s
Test loss: 0.2197 score: 0.8958 time: 0.08s
Epoch 89/1000, LR 0.000266
Train loss: 0.0166;  Loss pred: 0.0166; Loss self: 0.0000; time: 0.13s
Val loss: 0.2331 score: 0.9388 time: 0.09s
Test loss: 0.2122 score: 0.8958 time: 0.08s
Epoch 90/1000, LR 0.000266
Train loss: 0.0145;  Loss pred: 0.0145; Loss self: 0.0000; time: 0.13s
Val loss: 0.2295 score: 0.9388 time: 0.09s
Test loss: 0.2057 score: 0.8958 time: 0.08s
Epoch 91/1000, LR 0.000266
Train loss: 0.0141;  Loss pred: 0.0141; Loss self: 0.0000; time: 0.13s
Val loss: 0.2268 score: 0.9388 time: 0.09s
Test loss: 0.2001 score: 0.8958 time: 0.08s
Epoch 92/1000, LR 0.000266
Train loss: 0.0133;  Loss pred: 0.0133; Loss self: 0.0000; time: 0.13s
Val loss: 0.2251 score: 0.9388 time: 0.09s
Test loss: 0.1951 score: 0.8958 time: 0.08s
Epoch 93/1000, LR 0.000265
Train loss: 0.0125;  Loss pred: 0.0125; Loss self: 0.0000; time: 0.13s
Val loss: 0.2245 score: 0.9388 time: 0.09s
Test loss: 0.1909 score: 0.9167 time: 0.08s
Epoch 94/1000, LR 0.000265
Train loss: 0.0114;  Loss pred: 0.0114; Loss self: 0.0000; time: 0.13s
Val loss: 0.2247 score: 0.9388 time: 0.09s
Test loss: 0.1875 score: 0.9167 time: 0.08s
     INFO: Early stopping counter 1 of 20
Epoch 95/1000, LR 0.000265
Train loss: 0.0103;  Loss pred: 0.0103; Loss self: 0.0000; time: 0.13s
Val loss: 0.2258 score: 0.9388 time: 0.09s
Test loss: 0.1847 score: 0.9167 time: 0.08s
     INFO: Early stopping counter 2 of 20
Epoch 96/1000, LR 0.000265
Train loss: 0.0098;  Loss pred: 0.0098; Loss self: 0.0000; time: 0.13s
Val loss: 0.2279 score: 0.9388 time: 0.09s
Test loss: 0.1826 score: 0.9167 time: 0.08s
     INFO: Early stopping counter 3 of 20
Epoch 97/1000, LR 0.000265
Train loss: 0.0092;  Loss pred: 0.0092; Loss self: 0.0000; time: 0.13s
Val loss: 0.2302 score: 0.9388 time: 0.09s
Test loss: 0.1811 score: 0.9167 time: 0.08s
     INFO: Early stopping counter 4 of 20
Epoch 98/1000, LR 0.000265
Train loss: 0.0094;  Loss pred: 0.0094; Loss self: 0.0000; time: 0.13s
Val loss: 0.2334 score: 0.9388 time: 0.09s
Test loss: 0.1800 score: 0.9167 time: 0.08s
     INFO: Early stopping counter 5 of 20
Epoch 99/1000, LR 0.000265
Train loss: 0.0081;  Loss pred: 0.0081; Loss self: 0.0000; time: 0.13s
Val loss: 0.2369 score: 0.9388 time: 0.09s
Test loss: 0.1792 score: 0.9167 time: 0.08s
     INFO: Early stopping counter 6 of 20
Epoch 100/1000, LR 0.000265
Train loss: 0.0074;  Loss pred: 0.0074; Loss self: 0.0000; time: 0.12s
Val loss: 0.2412 score: 0.9388 time: 0.09s
Test loss: 0.1789 score: 0.9167 time: 0.07s
     INFO: Early stopping counter 7 of 20
Epoch 101/1000, LR 0.000265
Train loss: 0.0066;  Loss pred: 0.0066; Loss self: 0.0000; time: 0.12s
Val loss: 0.2455 score: 0.9388 time: 0.09s
Test loss: 0.1791 score: 0.9167 time: 0.07s
     INFO: Early stopping counter 8 of 20
Epoch 102/1000, LR 0.000264
Train loss: 0.0063;  Loss pred: 0.0063; Loss self: 0.0000; time: 0.12s
Val loss: 0.2502 score: 0.9388 time: 0.09s
Test loss: 0.1794 score: 0.9167 time: 0.07s
     INFO: Early stopping counter 9 of 20
Epoch 103/1000, LR 0.000264
Train loss: 0.0066;  Loss pred: 0.0066; Loss self: 0.0000; time: 0.12s
Val loss: 0.2550 score: 0.9184 time: 0.09s
Test loss: 0.1801 score: 0.9167 time: 0.07s
     INFO: Early stopping counter 10 of 20
Epoch 104/1000, LR 0.000264
Train loss: 0.0060;  Loss pred: 0.0060; Loss self: 0.0000; time: 0.12s
Val loss: 0.2597 score: 0.9184 time: 0.09s
Test loss: 0.1809 score: 0.9167 time: 0.07s
     INFO: Early stopping counter 11 of 20
Epoch 105/1000, LR 0.000264
Train loss: 0.0062;  Loss pred: 0.0062; Loss self: 0.0000; time: 0.12s
Val loss: 0.2646 score: 0.9184 time: 0.09s
Test loss: 0.1818 score: 0.9167 time: 0.07s
     INFO: Early stopping counter 12 of 20
Epoch 106/1000, LR 0.000264
Train loss: 0.0061;  Loss pred: 0.0061; Loss self: 0.0000; time: 0.12s
Val loss: 0.2697 score: 0.9184 time: 0.09s
Test loss: 0.1829 score: 0.9167 time: 0.07s
     INFO: Early stopping counter 13 of 20
Epoch 107/1000, LR 0.000264
Train loss: 0.0050;  Loss pred: 0.0050; Loss self: 0.0000; time: 0.12s
Val loss: 0.2744 score: 0.9184 time: 0.09s
Test loss: 0.1841 score: 0.9167 time: 0.08s
     INFO: Early stopping counter 14 of 20
Epoch 108/1000, LR 0.000264
Train loss: 0.0049;  Loss pred: 0.0049; Loss self: 0.0000; time: 0.12s
Val loss: 0.2791 score: 0.9184 time: 0.09s
Test loss: 0.1852 score: 0.9167 time: 0.08s
     INFO: Early stopping counter 15 of 20
Epoch 109/1000, LR 0.000264
Train loss: 0.0048;  Loss pred: 0.0048; Loss self: 0.0000; time: 0.12s
Val loss: 0.2841 score: 0.9184 time: 0.09s
Test loss: 0.1864 score: 0.9167 time: 0.08s
     INFO: Early stopping counter 16 of 20
Epoch 110/1000, LR 0.000263
Train loss: 0.0044;  Loss pred: 0.0044; Loss self: 0.0000; time: 0.12s
Val loss: 0.2886 score: 0.9184 time: 0.09s
Test loss: 0.1876 score: 0.9167 time: 0.08s
     INFO: Early stopping counter 17 of 20
Epoch 111/1000, LR 0.000263
Train loss: 0.0042;  Loss pred: 0.0042; Loss self: 0.0000; time: 0.12s
Val loss: 0.2929 score: 0.9184 time: 0.09s
Test loss: 0.1887 score: 0.9167 time: 0.08s
     INFO: Early stopping counter 18 of 20
Epoch 112/1000, LR 0.000263
Train loss: 0.0042;  Loss pred: 0.0042; Loss self: 0.0000; time: 0.13s
Val loss: 0.2968 score: 0.9184 time: 0.09s
Test loss: 0.1898 score: 0.9167 time: 0.08s
     INFO: Early stopping counter 19 of 20
Epoch 113/1000, LR 0.000263
Train loss: 0.0050;  Loss pred: 0.0050; Loss self: 0.0000; time: 0.12s
Val loss: 0.3008 score: 0.9184 time: 0.09s
Test loss: 0.1908 score: 0.9167 time: 0.08s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 092,   Train_Loss: 0.0125,   Val_Loss: 0.2245,   Val_Precision: 1.0000,   Val_Recall: 0.8800,   Val_accuracy: 0.9362,   Val_Score: 0.9388,   Val_Loss: 0.2245,   Test_Precision: 0.9167,   Test_Recall: 0.9167,   Test_accuracy: 0.9167,   Test_Score: 0.9167,   Test_loss: 0.1909


[0.0856277149869129, 0.08524923003278673, 0.08477041206788272, 0.08497397194150835, 0.08530167606659234, 0.08492067607585341, 0.08501563395839185, 0.08524377108551562, 0.08585474803112447, 0.0851465609157458, 0.08452890906482935, 0.08418850309681147, 0.08392004400957376, 0.08432520099449903, 0.08438508794642985, 0.08347318100277334, 0.08394480403512716, 0.0830164619255811, 0.08330889907665551, 0.08323197800200433, 0.08434443001169711, 0.08393686695490032, 0.08472173905465752, 0.08448618405964226, 0.08485221595037729, 0.08433042711112648, 0.08493644604459405, 0.08501973503734916, 0.08502316800877452, 0.08485796395689249, 0.08467471995390952, 0.08514159801416099, 0.0849472739500925, 0.08532436296809465, 0.08585909206885844, 0.08473956503439695, 0.08494921901728958, 0.09114887891337276, 0.08445318893063813, 0.08436000999063253, 0.08467477408703417, 0.08428751502651721, 0.08526137797161937, 0.08466824202332646, 0.08465997106395662, 0.08453641994856298, 0.08417681697756052, 0.08484289608895779, 0.08466014009900391, 0.08918965910561383, 0.08896407100837678, 0.08967135602142662, 0.0904607450356707, 0.08966146700549871, 0.08961196301970631, 0.08884774800390005, 0.08357804594561458, 0.08389795094262809, 0.08338764600921422, 0.08376439206767827, 0.0849773429799825, 0.08355957199819386, 0.08316746295895427, 0.1012585120042786, 0.08329865394625813, 0.08304291602689773, 0.08284130902029574, 0.08444675500504673, 0.08193125494290143, 0.08550784608814865, 0.08183503104373813, 0.08330793201457709, 0.08267697901464999, 0.08339644793886691, 0.08486227493267506, 0.08292346389498562, 0.14768380497116596, 0.0831423249328509, 0.08205526100937277, 0.08237907500006258, 0.08314207894727588, 0.08393530093599111, 0.08135514392051846, 0.11444024997763336, 0.08072609407827258, 0.08053090190514922, 0.08074295800179243, 0.08210090000648052, 0.08239296800456941, 0.08080919901840389, 0.211293886997737, 0.08734565996564925, 0.0874689529882744, 0.08845451299566776, 0.08921079291030765, 0.08786369697190821, 0.08726405596826226, 0.08779040398076177, 0.08725463098380715, 0.08763895090669394, 0.0887525730067864, 0.08926207502372563, 0.08766789198853076, 0.08112909807823598, 0.08076922700274736, 0.08062120003160089, 0.08152910799253732, 0.0834237999515608, 0.08206715004052967, 0.19128008501138538, 0.08207516802940518, 0.08167775499168783, 0.08223424607422203, 0.0842990759992972, 0.0821992619894445, 0.21167789492756128, 0.08320662996266037, 0.08284029900096357, 0.08282020699698478, 0.08367112698033452, 0.08428442303556949, 0.08206873200833797, 0.20189711404964328, 0.08145886904094368, 0.08097192505374551, 0.08156040508765727, 0.08423401904292405, 0.08290301798842847, 0.08113536704331636, 0.082058175932616, 0.08175102702807635, 0.08199533098377287, 0.08214297308586538, 0.08509628707543015, 0.08261619508266449, 0.20654311799444258, 0.08347175794187933, 0.21136021299753338, 0.08396848093252629, 0.08352250000461936, 0.08431082498282194, 0.085251399083063, 0.08586629701312631, 0.08550883701536804, 0.08479133190121502, 0.08541834400966763, 0.0855234459741041, 0.08732377702835947, 0.08666031097527593, 0.08878253994043916, 0.09234418591950089, 0.09238981094677001, 0.09223415597807616, 0.09368531708605587, 0.09123399504460394, 0.2427879050374031, 0.08319639298133552, 0.08258735993877053, 0.08289793191943318, 0.08355919201858342, 0.08516060502734035, 0.08547494397498667, 0.08527661405969411, 0.08561429404653609, 0.08604799397289753, 0.0859159710817039, 0.08556380902882665, 0.08521337399724871, 0.08479930192697793, 0.08468065410852432, 0.08479122293647379, 0.0843285450246185, 0.08466114406473935, 0.08433755789883435, 0.08447127800900489, 0.08464197895955294, 0.08423846098594368, 0.08414773794356734, 0.08448362001217902, 0.0845912320073694, 0.08461100095883012, 0.08440900093410164, 0.08426075999159366, 0.08452241204213351, 0.08439314202405512, 0.08448939607478678, 0.08417158003430814, 0.08485907700378448, 0.0851818589726463, 0.0847264239564538, 0.08562044601421803, 0.08646926493383944, 0.08656194503419101, 0.08601367601659149, 0.08645347005221993, 0.08668488205876201, 0.08623224403709173, 0.08627785998396575, 0.08661644696258008, 0.0863683270290494, 0.08655399596318603, 0.0855661170789972, 0.08650096599012613, 0.08579907205421478, 0.08615009800996631, 0.08639356808271259, 0.08631182205863297, 0.08619539195206016, 0.0856498689390719, 0.08575538103468716, 0.08546555694192648, 0.08557752298656851, 0.084999103914015, 0.08559781697113067, 0.0851071949582547, 0.08542800298891962, 0.08534816699102521, 0.08484460704494268, 0.08513701893389225, 0.08478128409478813, 0.08485350792761892, 0.08485007705166936, 0.08482071501202881, 0.08458001294638962, 0.08519766002427787, 0.08542536303866655, 0.08458903303835541, 0.0851284769596532, 0.08488073199987411, 0.08481090294662863, 0.08484675805084407, 0.08475842501502484, 0.08497169695328921, 0.08469861489720643, 0.08571215393021703, 0.08465855999384075, 0.08493226405698806, 0.08597062004264444, 0.08603189105633646, 0.08589838200714439, 0.0860462459968403, 0.08571144402958453, 0.08476331108249724, 0.08441412495449185, 0.0831863519269973, 0.08304286398924887, 0.08311832300387323, 0.08423011202830821, 0.08298096200451255, 0.08314477698877454, 0.08197593304794282, 0.08193248405586928, 0.08172676898539066, 0.08238350506871939, 0.08351430203765631, 0.08371770603116602, 0.08305642497725785, 0.08328816900029778, 0.08396163396537304, 0.08390520105604082, 0.07864412700291723, 0.07877383998129517, 0.07846399198751897, 0.07874669705051929, 0.08021053206175566, 0.07970619201660156, 0.07965107006020844, 0.07895867002662271, 0.07893376098945737, 0.07951808290090412, 0.08023690094705671, 0.07930211094208062, 0.07866061700042337, 0.07807579799555242, 0.0786760289920494, 0.0818578660255298, 0.08019794302526861, 0.07975720800459385, 0.08120169094763696, 0.0811975309625268, 0.08108382602222264, 0.0821019239956513, 0.08076566399540752, 0.08066324598621577, 0.08222611795645207, 0.08054211793933064, 0.08199933799915016, 0.08433988899923861, 0.08144230395555496, 0.08179195795673877, 0.08017775905318558, 0.08215860603377223, 0.08188363001681864, 0.084490671986714, 0.0796167649095878, 0.08022006996907294, 0.07961404998786747, 0.07936755998525769, 0.08001559902913868, 0.08316209202166647, 0.08027203800156713, 0.08109623298514634, 0.08215015789028257, 0.08170803205575794, 0.0814689879771322, 0.08255432895384729, 0.08088929508812726, 0.18468694505281746, 0.07967600505799055, 0.07923749706242234, 0.07972210005391389, 0.08082060294691473, 0.08025891100987792, 0.19320306205190718, 0.0795743060298264, 0.07947166194207966, 0.08002854301594198, 0.08402771293185651, 0.08203052799217403, 0.0976750049740076, 0.08102957101073116, 0.08079711999744177, 0.07855466206092387, 0.07731091999448836, 0.07774401002097875, 0.0769274199847132, 0.197236769949086, 0.07796836702618748, 0.07829263003077358, 0.07890876603778452, 0.07882027607411146, 0.07848519796971232, 0.08470798702910542, 0.0849471379769966, 0.08530348702333868, 0.08830481907352805, 0.0863980760332197, 0.08810812595766038, 0.0851718959165737, 0.08540633309166878, 0.08671002101618797, 0.08651078399270773, 0.08690891601145267, 0.0870087780058384, 0.08691316400654614, 0.08738042006734759, 0.08686100610066205, 0.0861188150011003, 0.08597370807547122, 0.0860624979250133, 0.08614329993724823, 0.0855487409280613, 0.08131604397203773, 0.07956760202068835, 0.07923532195854932, 0.07961898494977504, 0.07968395494390279, 0.07997598801739514, 0.0799186450894922, 0.07978081004694104, 0.08052226796280593, 0.08025607606396079, 0.08018897706642747, 0.08023524493910372, 0.08040283690206707, 0.08070796704851091, 0.08066505694296211]
[0.0017475043874880184, 0.0017397802047507496, 0.001730008409548627, 0.001734162692683844, 0.001740850531971272, 0.001733075021956192, 0.0017350129379263641, 0.001739668797663584, 0.0017521377149209076, 0.0017376849166478735, 0.001725079776833252, 0.0017181327162614586, 0.0017126539593790562, 0.0017209224692754904, 0.001722144651967956, 0.0017035343061790479, 0.0017131592660230032, 0.0016942135086853284, 0.001700181613809296, 0.0016986117959592718, 0.0017213148981979002, 0.0017129972847938842, 0.0017290150827481126, 0.0017242078379518827, 0.001731677876538312, 0.001721029124716867, 0.0017333968580529398, 0.0017350966334152892, 0.0017351666940566227, 0.0017317951827937243, 0.0017280555092634596, 0.001737583632942061, 0.0017336178357161734, 0.0017413135299611153, 0.0017522263687522132, 0.001729378878252999, 0.0017336575309650935, 0.0018601812023137296, 0.0017235344679722069, 0.0017216328569516844, 0.0017280566140211054, 0.0017201533678881064, 0.001740028121869783, 0.0017279233065984991, 0.0017277545115093188, 0.0017252330601747548, 0.0017178942240318473, 0.0017314876752848529, 0.0017277579612041615, 0.001820197124604364, 0.0018155932858852403, 0.0018300276739066656, 0.001846137653789198, 0.0018298258572550757, 0.001828815571830741, 0.0018132193470183685, 0.0017056744070533588, 0.0017122030804617976, 0.0017017886940655963, 0.0017094773891362914, 0.001734231489387398, 0.001705297387718242, 0.0016972951624276383, 0.0020665002449852775, 0.001699972529515472, 0.0016947533883040352, 0.0016906389595978723, 0.0017234031633683005, 0.0016720664274061518, 0.0017450580834316052, 0.0016701026743620026, 0.0017001618778485121, 0.0016872852860132651, 0.0017019683252829983, 0.0017318831618913279, 0.001692315589693584, 0.003013955203493183, 0.001696782141486753, 0.0016745971634565871, 0.0016812056122461753, 0.001696777121372977, 0.0017129653252243083, 0.0016603090596024174, 0.0023355153056659867, 0.0016474713077198487, 0.0016434877939826371, 0.0016478154694243353, 0.0016755285715608268, 0.001681489142950396, 0.0016491673269062018, 0.004312120142810959, 0.0017825644890948826, 0.0017850806732300896, 0.0018051941427687298, 0.0018206284267409723, 0.001793136672896086, 0.0017808991013931073, 0.0017916408975665666, 0.0017807067547715744, 0.0017885500185039578, 0.0018112770001384982, 0.0018216750004841965, 0.0017891406528271583, 0.001655695879147673, 0.00164835157148464, 0.0016453306128898142, 0.001663859346786476, 0.0017025265296236897, 0.0016748397967455034, 0.0039036752043139873, 0.0016750034291715342, 0.0016668929590140373, 0.0016782499198820821, 0.0017203893061081062, 0.001677535958968255, 0.004319957039337985, 0.0016980944890338853, 0.0016906183469584401, 0.001690208306060914, 0.001707574020006827, 0.0017200902660320305, 0.0016748720818028158, 0.00412034926631925, 0.001662425898794769, 0.0016524882664029695, 0.0016644980630134136, 0.001719061613120899, 0.0016918983262944587, 0.001655823817210538, 0.0016746566516860407, 0.0016683883066954358, 0.0016733741017096505, 0.0016763872058339873, 0.0017366589199067379, 0.0016860447976053978, 0.004215165673355971, 0.0017035052641199864, 0.004313473734643539, 0.0017136424680107407, 0.0017045408164208032, 0.0017206290812820805, 0.0017398244710829184, 0.0017523734084311494, 0.0017450783064360826, 0.0017304353449227555, 0.0017432315104013803, 0.0017453764484511042, 0.0017821178985379484, 0.0017685777750056312, 0.001811888570213044, 0.001884575222846957, 0.0018855063458524492, 0.001882329713838289, 0.0019119452466542016, 0.001861918266216407, 0.0049548552048449615, 0.0016978855710476637, 0.0016854563252810314, 0.001691794528968024, 0.0017052896330323145, 0.0017379715311702114, 0.001744386611734422, 0.001740339062442737, 0.0017472304907456345, 0.00175608150965097, 0.0017533871649327328, 0.0017462001842617684, 0.001739048448923443, 0.0017305979985097538, 0.00172817661445968, 0.0017304331211525264, 0.0017209907147881327, 0.001727778450300803, 0.0017211746509966193, 0.0017239036328368345, 0.0017273873257051622, 0.0017191522650192588, 0.0017173007743585171, 0.001724155510452633, 0.0017263516736197838, 0.001726755121608778, 0.0017226326721245233, 0.0017196073467672175, 0.0017249471845333369, 0.001722309020899084, 0.001724273389281363, 0.0017177873476389416, 0.0017318178980364179, 0.001738405285156047, 0.001729110692988853, 0.0017473560411064904, 0.0017646788762008048, 0.0017665703068202247, 0.0017553811431957446, 0.0017643565316779576, 0.0017690792256890206, 0.0017598417150426883, 0.001760772652733995, 0.001767682591073063, 0.0017626189189601917, 0.0017664080808813475, 0.0017462472873264734, 0.0017653258365331864, 0.001751001470494179, 0.0017581652655095166, 0.0017631340425043386, 0.001761465756298632, 0.0017590896316746973, 0.001747956508960651, 0.0017501098170344318, 0.0017441950396311526, 0.0017464800609503777, 0.0017346755900819386, 0.0017468942239006258, 0.0017368815297602999, 0.001743428632426931, 0.0017417993263474532, 0.0017315225927539322, 0.0017374901823243316, 0.0017302302876487374, 0.0017317042434207943, 0.0017316342255442726, 0.001731035000245486, 0.0017261227131916248, 0.0017387277555975075, 0.001743374755891154, 0.0017263067967011308, 0.001737315856319453, 0.0017322598367321249, 0.0017308347540128293, 0.0017315664908335525, 0.0017297637758168336, 0.001734116264352841, 0.0017285431611674782, 0.0017492276312289189, 0.0017277257141600155, 0.0017333115113671034, 0.0017545024498498865, 0.001755752878700744, 0.0017530282042274366, 0.0017560458366702102, 0.0017492131434609086, 0.0017298634914795356, 0.0017227372439692216, 0.0016976806515713735, 0.0016947523263112015, 0.0016962923062014946, 0.001718981878128739, 0.001693489020500256, 0.0016968321834443783, 0.0016729782254682208, 0.001672091511344271, 0.0016678932445998095, 0.0016812960218105996, 0.001739881292451173, 0.001744118875649292, 0.0017303421870262052, 0.0017351701875062038, 0.0017492007076119382, 0.0017480250220008504, 0.0016384193125607756, 0.0016411216662769827, 0.0016346664997399785, 0.001640556188552485, 0.0016710527512865763, 0.0016605456670125325, 0.0016593972929210092, 0.0016449722922213066, 0.0016444533539470285, 0.001656626727102169, 0.0016716021030636814, 0.0016521273112933461, 0.0016387628541754868, 0.001626579124907342, 0.0016390839373343624, 0.0017053722088652041, 0.0016707904796930961, 0.0016616085000957053, 0.0016917018947424367, 0.0016916152283859749, 0.0016892463754629716, 0.0017104567499094021, 0.0016826179999043234, 0.0016804842913794953, 0.0017130441240927514, 0.0016779607904027216, 0.0017083195416489616, 0.001757081020817471, 0.0016967146657407284, 0.0017039991240987244, 0.0016703699802746996, 0.0017116376257035881, 0.0017059089586837217, 0.0017602223330565419, 0.0016586826022830792, 0.0016712514576890196, 0.0016586260414139058, 0.0016534908330262017, 0.0016669916464403893, 0.001732543583784718, 0.0016723341250326484, 0.0016895048538572155, 0.0017114616227142203, 0.0017022506678282905, 0.0016972705828569208, 0.0017198818532051519, 0.001685193647669318, 0.0038476446886003637, 0.0016599167720414698, 0.0016507811888004653, 0.0016608770844565395, 0.001683762561394057, 0.0016720606460391234, 0.004025063792748067, 0.00165779804228805, 0.0016556596237933263, 0.0016672613128321245, 0.0017505773527470108, 0.0017089693331702922, 0.002034895936958492, 0.0016881160627235658, 0.001683273333280037, 0.0016365554596025806, 0.0016106441665518407, 0.0016196668754370573, 0.0016026545830148582, 0.004109099373939292, 0.0016243409797122392, 0.0016310964589744497, 0.0016439326257871774, 0.0016420890848773222, 0.0016351082910356733, 0.0017647497297730297, 0.001769732041187429, 0.001777155979652889, 0.001839683730698501, 0.0017999599173587437, 0.0018355859574512579, 0.001774414498261952, 0.001779298606076433, 0.0018064587711705826, 0.0018023079998480778, 0.0018106024169052641, 0.0018126828751216333, 0.0018106909168030445, 0.0018204254180697415, 0.0018096042937637928, 0.0017941419791895896, 0.0017911189182389837, 0.0017929687067711104, 0.0017946520820260048, 0.0017822654360012773, 0.0016940842494174528, 0.0016576583754310075, 0.0016507358741364442, 0.0016587288531203133, 0.0016600823946646415, 0.0016661664170290653, 0.0016649717726977542, 0.0016621002093112718, 0.0016775472492251235, 0.0016720015846658498, 0.0016706036888839055, 0.0016715676028979942, 0.0016750591021263972, 0.0016814159801773105, 0.0016805220196450439]
[572.2446290606846, 574.7852500386768, 578.0318722617701, 576.6471647780457, 574.4318547943565, 577.0090661575974, 576.3645781196135, 574.8220588556991, 570.731393705055, 575.4783220015952, 579.6833360574842, 582.0272150896078, 583.8891122889543, 581.0837024058386, 580.6713151867147, 587.014888031786, 583.7168906784949, 590.2443788067641, 588.1724586819151, 588.7160341043441, 580.9512257443029, 583.7720870178289, 578.3639541250217, 579.9764842664558, 577.4746063043993, 581.0476915459021, 576.9019341152278, 576.3367761434954, 576.3135054546912, 577.43549002533, 578.6851143608372, 575.5118666183619, 576.8283986227511, 574.2791190638314, 570.7025175703269, 578.2422883585753, 576.8151910852424, 537.5820370382093, 580.2030760525096, 580.8439331081283, 578.6847444037425, 581.3435119612245, 574.7033553259067, 578.7293893086891, 578.7859289838738, 579.6318324080264, 582.1080169028273, 577.5380410001975, 578.7847733620337, 549.3910447844263, 550.7841474047001, 546.4398239755808, 541.671417593103, 546.5000923640359, 546.8019932698556, 551.505255910922, 586.2783634817808, 584.0428693366738, 587.6170193674201, 584.9741016494212, 576.6242892713483, 586.4079820928132, 589.1727155868999, 483.9099353734286, 588.2447996292158, 590.0563509129282, 591.4923433669454, 580.2472812255681, 598.0623637969235, 573.0468283517129, 598.7655821112979, 588.1792863544621, 592.6680024353263, 587.5550003750645, 577.4061564914897, 590.9063333636625, 331.7899346483309, 589.3508515617573, 597.1585416613685, 594.8112430245517, 589.352595225254, 583.782978717945, 602.2975025140578, 428.17103256569914, 606.9908442800324, 608.4620790378469, 606.8640685533497, 596.8265877247662, 594.7109466584882, 606.3666091881507, 231.90448477349844, 560.9895216232886, 560.1987714037078, 553.9570378099293, 549.2608954755565, 557.6819743388021, 561.5141246451024, 558.1475625825549, 561.5747777226117, 559.1121241532039, 552.0966698763003, 548.945338621984, 558.9275490553654, 603.9756531343091, 606.6666949571434, 607.7805835288186, 601.012340334757, 587.362359763657, 597.0720315717174, 256.16885310921634, 597.0137031269276, 599.9185458144218, 595.8588099144747, 581.2637851500117, 596.1124079957344, 231.48378349457977, 588.8953803559781, 591.4995550587046, 591.6430515777863, 585.6261504821922, 581.3648386644487, 597.0605223317174, 242.6978722833635, 601.5305709114513, 605.1480184950032, 600.7817144524615, 581.7127160349613, 591.052065279932, 603.9289866506673, 597.137329012011, 599.3808491625625, 597.5950022044212, 596.5208971530591, 575.8183075198797, 593.1040512210875, 237.23859926099516, 587.0248957032662, 231.83171186798455, 583.5522979077642, 586.6682630104459, 581.1827841796542, 574.7706257847783, 570.6546305648817, 573.0401875445165, 577.8892594479679, 573.6472717669894, 572.9423018669858, 561.130103019784, 565.4260808500864, 551.9103196740286, 530.623552658905, 530.3615138711685, 531.2565554527022, 523.0275300769961, 537.0805035562029, 201.8222447796616, 588.9678415624675, 593.3111318285041, 591.08832832672, 586.410648742301, 575.3834180049428, 573.2674128963375, 574.6006749951367, 572.3343344204392, 569.4496494065102, 570.3246949673915, 572.6720275331803, 575.0271078526015, 577.8349454125778, 578.6445619232345, 577.8900020903244, 581.0606596579504, 578.777909763779, 580.9985636384812, 580.0788286259437, 578.9089598603922, 581.6820419852675, 582.3091766633258, 579.9940863440303, 579.2562519450152, 579.1209115212139, 580.5068115692359, 581.5281040058092, 579.727894840176, 580.6158986951003, 579.954435425566, 582.144234194341, 577.4279161416608, 575.2398526044723, 578.3319738029326, 572.2932112717926, 566.6753387748995, 566.0686111044009, 569.6768498831303, 566.7788692623078, 565.2658091728595, 568.2329220021603, 567.9324917088389, 565.7124220434591, 567.3376072633569, 566.1205985318246, 572.6565803466544, 566.4676624026745, 571.1017476860105, 568.7747446826052, 567.1718518800815, 567.7090209810834, 568.4758650120489, 572.0966138880699, 571.3927150551638, 573.3303772102639, 572.5802557721915, 576.476665560714, 572.4445054074929, 575.7445069601299, 573.5824119212468, 574.1189497971596, 577.5263945066587, 575.5428204274787, 577.957747670069, 577.4658136914928, 577.4891632704293, 577.6890703297076, 579.3330870149933, 575.1331666390485, 573.6001376759834, 579.2713102392578, 575.6005716303798, 577.2806012096205, 577.7559051674715, 577.5117532556394, 578.1136210508151, 576.6626036306697, 578.5218572873752, 571.6808848356977, 578.795576059467, 576.9303402429241, 569.9621565564408, 569.5562354652093, 570.4414781168351, 569.4612174225396, 571.6856197532613, 578.0802964658845, 580.471574234954, 589.0389332495483, 590.0567206634851, 589.5210373495703, 581.7396987852989, 590.4968900858893, 589.3334707797164, 597.7364108968767, 598.053391943874, 599.5587566756635, 594.7792578032112, 574.7518548183157, 573.3554139924809, 577.9203717610426, 576.3123451522674, 571.6896841216296, 572.0741908232899, 610.3443680952741, 609.3393442721288, 611.7455763356422, 609.5493753751475, 598.4251539815726, 602.2116825001794, 602.628438810887, 607.9129750262474, 608.1048134322527, 603.6362830806405, 598.2284888055707, 605.2802306240934, 610.2164187161366, 614.7871841506419, 610.0968823026216, 586.3822541505025, 598.5190915043327, 601.8264831591811, 591.120695146027, 591.1509799744074, 591.9799589482203, 584.6391614713245, 594.3119591356218, 595.0665561884596, 583.7561250966678, 595.9614823657431, 585.3705794612326, 569.1257193904219, 589.3742891433274, 586.8547617528371, 598.6697628722624, 584.2358131084777, 586.1977539361768, 568.1100513385419, 602.888098436409, 598.354003162866, 602.9086575461845, 604.7810970743699, 599.883030089173, 577.1860571700676, 597.9666294141293, 591.8893915675679, 584.2958946482786, 587.4575460004296, 589.1812478813813, 581.4352876253747, 593.4036135152985, 259.89925810009356, 602.4398432760802, 605.7738038114226, 602.0915149944483, 593.9079671495122, 598.0644316752861, 248.44326735931344, 603.2097846006778, 603.9888788909843, 599.7860037316714, 571.2401102589367, 585.1480073928008, 491.42562125052694, 592.3763312734694, 594.080581108829, 611.0394818167903, 620.8696003542839, 617.410910333124, 623.9647710730247, 243.36233052483342, 615.6342864520695, 613.0845263613343, 608.297435255999, 608.9803587450972, 611.5802882796238, 566.6525871227153, 565.0572949614647, 562.6968096493805, 543.5716929563293, 555.5679270166179, 544.7851657072583, 563.5661797057593, 562.0192117191167, 553.5692349911764, 554.844122139109, 552.3023666947435, 551.6684764470447, 552.275372190854, 549.3221474903014, 552.6070000199335, 557.3694900398563, 558.310221514043, 557.7342182401288, 557.211066153328, 561.0836521879804, 590.2894146757291, 603.260608350614, 605.7904330231714, 602.871287925602, 602.3797392309634, 600.1801439397008, 600.6107829562178, 601.6484411697248, 596.1083960299243, 598.0855575563646, 598.5860121427593, 598.2408358874038, 596.9938605333709, 594.7368240752338, 595.053196750863]
Elapsed: 0.08744375577021381~0.020428269729074373
Time per graph: 0.0017956429572214016~0.0004177894949156898
Speed: 570.6773281878159~60.44550280127934
Total Time: 0.0819
best val loss: 0.22453251481056213 test_score: 0.9167

Testing...
Test loss: 0.2691 score: 0.8958 time: 0.08s
test Score 0.8958
Epoch Time List: [0.2877362979343161, 0.2843286528950557, 0.2832665218738839, 0.28427670197561383, 0.2841525968397036, 0.28485411999281496, 0.2855811291374266, 0.2848712479462847, 0.2855846341699362, 0.28514901793096215, 0.2808578348485753, 0.2825691698817536, 0.28162463509943336, 0.28192724904511124, 0.282371794921346, 0.2806592429988086, 0.2800140680046752, 0.27827179490122944, 0.2780663870507851, 0.27745472511742264, 0.27905325312167406, 0.28253101208247244, 0.283092139987275, 0.28522800805512816, 0.28359472507145256, 0.2825097570894286, 0.2836159438593313, 0.2852814479265362, 0.2838711041258648, 0.2837606220273301, 0.2853145720437169, 0.2844492569565773, 0.28361421485897154, 0.2846103199990466, 0.2851144679589197, 0.28430034103803337, 0.28496810887008905, 0.2905121869407594, 0.28273238299880177, 0.28214187012054026, 0.28342367406003177, 0.28313200711272657, 0.28307685104664415, 0.28314915590453893, 0.2821917850524187, 0.28246428014244884, 0.2815815870417282, 0.28408313705585897, 0.28412788012064993, 0.2934969960479066, 0.2969193090684712, 0.29717512405477464, 0.2974234370049089, 0.2981820539571345, 0.29952693497762084, 0.2939436520682648, 0.3318917929427698, 0.28093050490133464, 0.2795163600239903, 0.27926183596719056, 0.28187233314383775, 0.2827939379494637, 0.2777206889586523, 0.3441306200111285, 0.27547101012896746, 0.27548559196293354, 0.2753863949328661, 0.2779595289612189, 0.2765854010358453, 0.27326603105757385, 0.3898353208787739, 0.27461775904521346, 0.27373237011488527, 0.2755599149968475, 0.2817473409231752, 0.2789794948184863, 0.33683497505262494, 0.28190806007478386, 0.27391367696691304, 0.27314767509233207, 0.2740970770828426, 0.2810454040300101, 0.274299115058966, 0.3020463711582124, 0.29941479698754847, 0.2701253170380369, 0.27096121199429035, 0.27378939697518945, 0.2795646330341697, 0.27382581087294966, 0.4018706309143454, 0.2890627689193934, 0.29019388812594116, 0.29378713900223374, 0.2970202862052247, 0.2943583090091124, 0.29047137405723333, 0.3731834809295833, 0.29049203998874873, 0.291826430009678, 0.29503107094205916, 0.30222498800139874, 0.2924110029125586, 0.37408187496475875, 0.271938745980151, 0.2716570470947772, 0.2736074929125607, 0.27910458017140627, 0.2773390719667077, 0.38998571201227605, 0.274966801982373, 0.27253244491294026, 0.2759304540231824, 0.2810881499899551, 0.27666280698031187, 0.4045454680453986, 0.27760292892344296, 0.27775679191108793, 0.2771691040834412, 0.2777524220291525, 0.28230172290932387, 0.27635556319728494, 0.39223616605158895, 0.271580702974461, 0.27053012989927083, 0.2708039201097563, 0.27367505722213537, 0.27883719303645194, 0.2714869669871405, 0.3828011730220169, 0.27259554504416883, 0.27441466599702835, 0.2752734050154686, 0.2830693230498582, 0.2785590400453657, 0.4023870099335909, 0.280495151062496, 0.404762526974082, 0.27670021809171885, 0.2775738650234416, 0.27986029884777963, 0.28412452002521604, 0.2884433639701456, 0.28384514595381916, 0.4078543980140239, 0.2828144630184397, 0.2830921560525894, 0.28911775001324713, 0.2920024080667645, 0.29073917295318097, 0.42828500794712454, 0.30438260093797, 0.3051004580920562, 0.3086172849871218, 0.3074847040697932, 0.45522990997415036, 0.27221744996495545, 0.274040866876021, 0.2744631819659844, 0.27684291708283126, 0.28595552290789783, 0.2867968857754022, 0.28515475103631616, 0.28549913596361876, 0.2872735619312152, 0.2868501520715654, 0.2868768780026585, 0.28567348793148994, 0.2837377678370103, 0.283393498044461, 0.2849806279409677, 0.282922426937148, 0.39766810182482004, 0.28209414903540164, 0.28252584498841316, 0.2829179500695318, 0.282082807039842, 0.2837253351463005, 0.2824726300314069, 0.28140076505951583, 0.2830400001257658, 0.28349369508214295, 0.2824744461104274, 0.2828248069854453, 0.282057334901765, 0.28367034916300327, 0.2829745749477297, 0.2831376229878515, 0.2853687749011442, 0.2854982160497457, 0.2853959809290245, 0.2897763920482248, 0.28904004010837525, 0.28781565104145557, 0.2890286879846826, 0.2899064200464636, 0.2904198960168287, 0.28974979114718735, 0.2884431049460545, 0.2896809459198266, 0.2893890191335231, 0.28973741410300136, 0.2895574829308316, 0.2875701910816133, 0.28893923410214484, 0.2885547149926424, 0.28869206889066845, 0.2886086867656559, 0.28719487600028515, 0.28839989902917296, 0.28614055505022407, 0.2851907720323652, 0.2860745817888528, 0.28604157897643745, 0.2849280061200261, 0.285666233045049, 0.28484740294516087, 0.284722248907201, 0.2851110539631918, 0.28362499608192593, 0.28386253002099693, 0.28495602298062295, 0.28387382184155285, 0.2836468000896275, 0.28501601598691195, 0.2846235368633643, 0.2828248990699649, 0.28437302296515554, 0.2852591589326039, 0.284654812887311, 0.2835767240030691, 0.2842415760969743, 0.28512679203413427, 0.2846146129304543, 0.2841071131406352, 0.2844574550399557, 0.2847298451233655, 0.2864515690598637, 0.28943864698521793, 0.2882845759158954, 0.28885234403423965, 0.2900865270057693, 0.28442926495335996, 0.2820079019293189, 0.37621158885303885, 0.2771008249837905, 0.27608991495799273, 0.2767153949243948, 0.28208703408017755, 0.2748103690100834, 0.3033308460144326, 0.2723639828618616, 0.27172906103078276, 0.27213160297833383, 0.3481765651376918, 0.2933478329796344, 0.29253740701824427, 0.2945404088823125, 0.30101994692813605, 0.29583355807699263, 0.3774781479733065, 0.27941953111439943, 0.2817143020220101, 0.2832609040196985, 0.2894672700203955, 0.2829990020254627, 0.3976069730706513, 0.2791017140261829, 0.28150354605168104, 0.2798901869682595, 0.28978824301157147, 0.2807196951471269, 0.3873234009370208, 0.27757382206618786, 0.2780986559810117, 0.2822967959800735, 0.28758468304295093, 0.2809094518888742, 0.370283353026025, 0.28769172180909663, 0.28484612703323364, 0.28559129079803824, 0.2839854230405763, 0.2807616388890892, 0.35243120905943215, 0.28806224709842354, 0.28690390300471336, 0.2898467549821362, 0.2936219120165333, 0.2875479649519548, 0.3682664110092446, 0.2873919499106705, 0.2889694848563522, 0.29261509503703564, 0.28252808819524944, 0.27789719111751765, 0.3793002220336348, 0.27854044816922396, 0.27818702498916537, 0.28448292100802064, 0.28900426311884075, 0.2811197389382869, 0.4027156709926203, 0.28922879602760077, 0.2916876570088789, 0.2975113400025293, 0.28757324104662985, 0.3934360168641433, 0.2820843290537596, 0.28263364313170314, 0.28334144805558026, 0.28755290899425745, 0.2879176230635494, 0.40031272906344384, 0.2818619280587882, 0.2809877011459321, 0.2834896231070161, 0.2991619820240885, 0.29405938694253564, 0.3308812679024413, 0.29097554192412645, 0.2874295060755685, 0.2841489468701184, 0.27220983302686363, 0.2786170970648527, 0.2739855471299961, 0.3949845341267064, 0.2750454299384728, 0.27750754391308874, 0.27765244082547724, 0.28397514892276376, 0.2793824599357322, 0.4168627249309793, 0.2967416220344603, 0.29699130915105343, 0.30267405381891876, 0.3044215519912541, 0.302225308958441, 0.44297957117669284, 0.2986067900201306, 0.3003850932000205, 0.30268083792179823, 0.30323123605921865, 0.30402564303949475, 0.30221848702058196, 0.3042997949523851, 0.3042017820989713, 0.3017577619757503, 0.30111494893208146, 0.30049747589509934, 0.3000069661065936, 0.30141839594580233, 0.29567278118338436, 0.28324936504941434, 0.28339303901884705, 0.28220103308558464, 0.2846207970287651, 0.28498442214913666, 0.28645721497014165, 0.2851850079605356, 0.28533617104403675, 0.285724358051084, 0.287232743925415, 0.28743042214773595, 0.2886996491579339, 0.2896761209703982, 0.2882239989703521]
Total Epoch List: [136, 118, 113]
Total Time List: [0.20719701098278165, 0.08322357700672, 0.08186295197810978]
T-times Epoch Time: 0.2931513156337013 ~ 0.006261118654484483
T-times Total Epoch: 117.88888888888887 ~ 3.8232556742411656
T-times Total Time: 0.09418239632052265 ~ 0.021276746158638643
T-times Inference Elapsed: 0.08421386220450545 ~ 0.0038667748332229636
T-times Time Per Graph: 0.0017304850553962181 ~ 7.85933798384521e-05
T-times Speed: 599.660249894056 ~ 34.99042130325136
T-times cross validation test micro f1 score:0.9083155381885025 ~ 0.023143423554130516
T-times cross validation test precision:0.9573599339045563 ~ 0.020170483085694402
T-times cross validation test recall:0.8683333333333333 ~ 0.03592115279479485
T-times cross validation test f1_score:0.9083155381885025 ~ 0.02573599147454078
