Namespace(seed=60, model='GPSPerformer', dataset='ico_wallets/Times', num_heads=1, num_layers=2, dim_hidden=64, dropout=0.1, epochs=1000, lr=0.0003, weight_decay=0.0001, batch_size=64, abs_pe=None, abs_pe_dim=20, num_class=2, outdir='./outdir/ico_wallets/Times/seed60/edge_attr/khopgnn_gat_1_0.1_0.0003_0.0001_2_1_64_BN', warmup=10, layer_norm=False, use_edge_attr=True, edge_dim=128, gnn_type='gat', k_hop=1, global_pool='mean', se='khopgnn', ie='mamba', aggr='add', not_extract_node_feature=False, k_ford=3, early_stop=1, early_stop_mindelta=-0.0, patience=20, training_times=3, Lambda=0.01, order='sd', use_cuda=True, batch_norm=True, save_logs=True)
Data(edge_index=[2, 338], edge_attr=[338, 2], x=[122, 14887], y=[1, 1], num_nodes=122)
Data(edge_index=[2, 338], edge_attr=[338, 2], x=[122, 14887], y=[1, 1], num_nodes=122)
Data(edge_index=[2, 302], edge_attr=[302, 2], x=[110, 14887], y=[1, 1], num_nodes=122)
========================training times:0========================
========================k_idx:0========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: GPSPerformer
GPS(
  (x_lin): Linear(in_features=14887, out_features=64, bias=True)
  (pe_lin): Linear(in_features=20, out_features=8, bias=True)
  (pe_norm): BatchNorm1d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (convs): ModuleList(
    (0-1): 2 x GPSConv(64, conv=GINEConv(nn=Sequential(
      (0): Linear(in_features=64, out_features=64, bias=True)
      (1): ReLU()
      (2): Linear(in_features=64, out_features=64, bias=True)
    )), heads=1, attn_type=performer)
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=32, bias=True)
    (1): ReLU()
    (2): Linear(in_features=32, out_features=16, bias=True)
    (3): ReLU()
    (4): Linear(in_features=16, out_features=2, bias=True)
  )
)
Total number of parameters: 1039522
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7e7c5707fb50>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 1.2468;  Loss pred: 1.2468; Loss self: 0.0000; time: 0.40s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 1.7097 score: 0.5102 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 1.4945 score: 0.4898 time: 0.07s
Epoch 2/1000, LR 0.000000
Train loss: 1.1516;  Loss pred: 1.1516; Loss self: 0.0000; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 1.4224 score: 0.5102 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 1.3050 score: 0.4898 time: 0.07s
Epoch 3/1000, LR 0.000030
Train loss: 1.1912;  Loss pred: 1.1912; Loss self: 0.0000; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 1.3439 score: 0.5102 time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 1.2567 score: 0.4898 time: 0.07s
Epoch 4/1000, LR 0.000060
Train loss: 1.1812;  Loss pred: 1.1812; Loss self: 0.0000; time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 1.3821 score: 0.5102 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 1.2937 score: 0.4898 time: 0.07s
     INFO: Early stopping counter 1 of 20
Epoch 5/1000, LR 0.000090
Train loss: 1.0589;  Loss pred: 1.0589; Loss self: 0.0000; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 1.4583 score: 0.5102 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 1.3337 score: 0.4898 time: 0.07s
     INFO: Early stopping counter 2 of 20
Epoch 6/1000, LR 0.000120
Train loss: 0.8929;  Loss pred: 0.8929; Loss self: 0.0000; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 1.4805 score: 0.5102 time: 0.09s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 1.3593 score: 0.4898 time: 0.07s
     INFO: Early stopping counter 3 of 20
Epoch 7/1000, LR 0.000150
Train loss: 0.8531;  Loss pred: 0.8531; Loss self: 0.0000; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 1.5312 score: 0.5102 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 1.4003 score: 0.4898 time: 0.07s
     INFO: Early stopping counter 4 of 20
Epoch 8/1000, LR 0.000180
Train loss: 0.7825;  Loss pred: 0.7825; Loss self: 0.0000; time: 0.33s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 1.5967 score: 0.5102 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 1.4481 score: 0.4898 time: 0.07s
     INFO: Early stopping counter 5 of 20
Epoch 9/1000, LR 0.000210
Train loss: 0.6865;  Loss pred: 0.6865; Loss self: 0.0000; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 1.6617 score: 0.5102 time: 0.09s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 1.4948 score: 0.4898 time: 0.07s
     INFO: Early stopping counter 6 of 20
Epoch 10/1000, LR 0.000240
Train loss: 0.6662;  Loss pred: 0.6662; Loss self: 0.0000; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 1.6820 score: 0.5102 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 1.5121 score: 0.4898 time: 0.07s
     INFO: Early stopping counter 7 of 20
Epoch 11/1000, LR 0.000270
Train loss: 0.6087;  Loss pred: 0.6087; Loss self: 0.0000; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 1.6526 score: 0.5102 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 1.4951 score: 0.4898 time: 0.07s
     INFO: Early stopping counter 8 of 20
Epoch 12/1000, LR 0.000270
Train loss: 0.5779;  Loss pred: 0.5779; Loss self: 0.0000; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 1.6807 score: 0.5102 time: 0.09s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 1.5076 score: 0.4898 time: 0.07s
     INFO: Early stopping counter 9 of 20
Epoch 13/1000, LR 0.000270
Train loss: 0.5744;  Loss pred: 0.5744; Loss self: 0.0000; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 1.7302 score: 0.5102 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 1.5270 score: 0.4898 time: 0.07s
     INFO: Early stopping counter 10 of 20
Epoch 14/1000, LR 0.000270
Train loss: 0.4820;  Loss pred: 0.4820; Loss self: 0.0000; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 1.7418 score: 0.5102 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 1.5259 score: 0.4898 time: 0.07s
     INFO: Early stopping counter 11 of 20
Epoch 15/1000, LR 0.000270
Train loss: 0.5070;  Loss pred: 0.5070; Loss self: 0.0000; time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 1.7417 score: 0.5102 time: 0.09s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 1.5056 score: 0.4898 time: 0.07s
     INFO: Early stopping counter 12 of 20
Epoch 16/1000, LR 0.000270
Train loss: 0.4227;  Loss pred: 0.4227; Loss self: 0.0000; time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 1.7447 score: 0.5102 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 1.4850 score: 0.4898 time: 0.07s
     INFO: Early stopping counter 13 of 20
Epoch 17/1000, LR 0.000270
Train loss: 0.3866;  Loss pred: 0.3866; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 1.7608 score: 0.5102 time: 0.09s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 1.4681 score: 0.4898 time: 0.07s
     INFO: Early stopping counter 14 of 20
Epoch 18/1000, LR 0.000270
Train loss: 0.3724;  Loss pred: 0.3724; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 1.7419 score: 0.5102 time: 0.10s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 1.4305 score: 0.4898 time: 0.09s
     INFO: Early stopping counter 15 of 20
Epoch 19/1000, LR 0.000270
Train loss: 0.3473;  Loss pred: 0.3473; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 1.6867 score: 0.5102 time: 0.09s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 1.3726 score: 0.4898 time: 0.07s
     INFO: Early stopping counter 16 of 20
Epoch 20/1000, LR 0.000270
Train loss: 0.3597;  Loss pred: 0.3597; Loss self: 0.0000; time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 1.6076 score: 0.5102 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 1.3015 score: 0.4898 time: 0.07s
     INFO: Early stopping counter 17 of 20
Epoch 21/1000, LR 0.000270
Train loss: 0.3508;  Loss pred: 0.3508; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 1.5321 score: 0.5102 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 1.2346 score: 0.4898 time: 0.07s
     INFO: Early stopping counter 18 of 20
Epoch 22/1000, LR 0.000270
Train loss: 0.3107;  Loss pred: 0.3107; Loss self: 0.0000; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 1.4496 score: 0.5102 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 1.1652 score: 0.4898 time: 0.07s
     INFO: Early stopping counter 19 of 20
Epoch 23/1000, LR 0.000270
Train loss: 0.2900;  Loss pred: 0.2900; Loss self: 0.0000; time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 1.3761 score: 0.5102 time: 0.08s
Test loss: 1.1051 score: 0.5102 time: 0.07s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 002,   Train_Loss: 1.1912,   Val_Loss: 1.3439,   Val_Precision: 0.0000,   Val_Recall: 0.0000,   Val_accuracy: 0.0000,   Val_Score: 0.5102,   Val_Loss: 1.3439,   Test_Precision: 0.0000,   Test_Recall: 0.0000,   Test_accuracy: 0.0000,   Test_Score: 0.4898,   Test_loss: 1.2567


[0.07314129103906453, 0.07222154701594263, 0.07299894897732884, 0.07240863796323538, 0.07285583496559411, 0.0788184500997886, 0.07466346502769738, 0.07542695908341557, 0.07441535301040858, 0.0720415289979428, 0.07173143897671252, 0.07184195599984378, 0.07173724297899753, 0.07131565199233592, 0.0717967189848423, 0.0780098260147497, 0.07316281704697758, 0.09810997196473181, 0.0721136819338426, 0.07010870601516217, 0.07171303406357765, 0.06965106190182269, 0.07069650501944125]
[0.0014926794089605005, 0.0014739091227743395, 0.0014897744689250784, 0.0014777273053721506, 0.001486853774808043, 0.0016085397979548695, 0.0015237441842387222, 0.0015393256955799095, 0.0015186806736818077, 0.001470235285672302, 0.0014639069178920923, 0.0014661623673437505, 0.001464025366918317, 0.0014554214692313451, 0.0014652391629559653, 0.001592037265607137, 0.0014931187152444404, 0.0020022443258108534, 0.0014717077945682164, 0.001430789918676779, 0.001463531307419952, 0.0014214502428943406, 0.0014427858167232908]
[669.9362193898007, 678.467881464564, 671.2425409743619, 676.7148420175942, 672.5610930564458, 621.6818516218377, 656.2781406116474, 649.63509858339, 658.4662709743016, 680.1632430844052, 683.1035414737429, 682.0526991234279, 683.0482740233787, 687.0861954016193, 682.4824406021237, 628.1260003161054, 669.7391103535185, 499.4395474663302, 679.482709604993, 698.9146253733871, 683.278857739567, 703.5068620930491, 693.1035697807862]
Elapsed: 0.07395567952493287~0.005584184751252908
Time per graph: 0.0015092995821414871~0.00011396295410720223
Speed: 665.5874615274078~40.31894346043279
Total Time: 0.0710
best val loss: 1.3438832759857178 test_score: 0.4898

Testing...
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 1.4945 score: 0.4898 time: 0.07s
test Score 0.4898
Epoch Time List: [0.5524872150272131, 0.3827338709961623, 0.43712066288571805, 0.37491501995828, 0.3877625920576975, 0.39658519614022225, 0.3835222409106791, 0.4787430779542774, 0.40432003303430974, 0.38756635901518166, 0.3896690080873668, 0.3964784271083772, 0.39066879102028906, 0.3782911730231717, 0.38654209196101874, 0.3801778459455818, 0.40622406208422035, 0.4517095491755754, 0.42368095403071493, 0.3739383629290387, 0.3963481510290876, 0.3833882430335507, 0.38113143399823457]
Total Epoch List: [23]
Total Time List: [0.07104396796785295]
========================k_idx:1========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: GPSPerformer
GPS(
  (x_lin): Linear(in_features=14887, out_features=64, bias=True)
  (pe_lin): Linear(in_features=20, out_features=8, bias=True)
  (pe_norm): BatchNorm1d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (convs): ModuleList(
    (0-1): 2 x GPSConv(64, conv=GINEConv(nn=Sequential(
      (0): Linear(in_features=64, out_features=64, bias=True)
      (1): ReLU()
      (2): Linear(in_features=64, out_features=64, bias=True)
    )), heads=1, attn_type=performer)
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=32, bias=True)
    (1): ReLU()
    (2): Linear(in_features=32, out_features=16, bias=True)
    (3): ReLU()
    (4): Linear(in_features=16, out_features=2, bias=True)
  )
)
Total number of parameters: 1039522
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7e7c5707f5b0>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.9043;  Loss pred: 0.9043; Loss self: 0.0000; time: 0.25s
Val loss: 2.4205 score: 0.4490 time: 0.07s
Test loss: 3.7625 score: 0.4694 time: 0.08s
Epoch 2/1000, LR 0.000000
Train loss: 0.9699;  Loss pred: 0.9699; Loss self: 0.0000; time: 0.24s
Val loss: 2.0651 score: 0.4898 time: 0.07s
Test loss: 3.1724 score: 0.4286 time: 0.08s
Epoch 3/1000, LR 0.000030
Train loss: 0.9634;  Loss pred: 0.9634; Loss self: 0.0000; time: 0.23s
Val loss: 1.7648 score: 0.4898 time: 0.07s
Test loss: 2.6656 score: 0.4082 time: 0.08s
Epoch 4/1000, LR 0.000060
Train loss: 0.9195;  Loss pred: 0.9195; Loss self: 0.0000; time: 0.24s
Val loss: 1.4907 score: 0.4490 time: 0.07s
Test loss: 2.2263 score: 0.3673 time: 0.08s
Epoch 5/1000, LR 0.000090
Train loss: 0.8723;  Loss pred: 0.8723; Loss self: 0.0000; time: 0.25s
Val loss: 1.1881 score: 0.4286 time: 0.07s
Test loss: 1.7381 score: 0.3265 time: 0.09s
Epoch 6/1000, LR 0.000120
Train loss: 0.8340;  Loss pred: 0.8340; Loss self: 0.0000; time: 0.24s
Val loss: 0.9547 score: 0.4082 time: 0.08s
Test loss: 1.2964 score: 0.3061 time: 0.09s
Epoch 7/1000, LR 0.000150
Train loss: 0.7202;  Loss pred: 0.7202; Loss self: 0.0000; time: 0.23s
Val loss: 0.7910 score: 0.4082 time: 0.07s
Test loss: 0.9312 score: 0.2653 time: 0.08s
Epoch 8/1000, LR 0.000180
Train loss: 0.6944;  Loss pred: 0.6944; Loss self: 0.0000; time: 0.24s
Val loss: 0.6909 score: 0.5102 time: 0.07s
Test loss: 0.7133 score: 0.3469 time: 0.08s
Epoch 9/1000, LR 0.000210
Train loss: 0.5514;  Loss pred: 0.5514; Loss self: 0.0000; time: 0.24s
Val loss: 0.6431 score: 0.5510 time: 0.08s
Test loss: 0.6228 score: 0.5102 time: 0.09s
Epoch 10/1000, LR 0.000240
Train loss: 0.5792;  Loss pred: 0.5792; Loss self: 0.0000; time: 0.23s
Val loss: 0.6188 score: 0.5510 time: 0.07s
Test loss: 0.5812 score: 0.5102 time: 0.08s
Epoch 11/1000, LR 0.000270
Train loss: 0.5174;  Loss pred: 0.5174; Loss self: 0.0000; time: 0.23s
Val loss: 0.6107 score: 0.5510 time: 0.07s
Test loss: 0.5663 score: 0.5102 time: 0.08s
Epoch 12/1000, LR 0.000270
Train loss: 0.5161;  Loss pred: 0.5161; Loss self: 0.0000; time: 0.24s
Val loss: 0.6088 score: 0.5714 time: 0.07s
Test loss: 0.5623 score: 0.5102 time: 0.09s
Epoch 13/1000, LR 0.000270
Train loss: 0.5351;  Loss pred: 0.5351; Loss self: 0.0000; time: 0.23s
Val loss: 0.6090 score: 0.5510 time: 0.07s
Test loss: 0.5606 score: 0.5102 time: 0.08s
     INFO: Early stopping counter 1 of 20
Epoch 14/1000, LR 0.000270
Train loss: 0.4731;  Loss pred: 0.4731; Loss self: 0.0000; time: 0.25s
Val loss: 0.6093 score: 0.5510 time: 0.07s
Test loss: 0.5598 score: 0.5102 time: 0.08s
     INFO: Early stopping counter 2 of 20
Epoch 15/1000, LR 0.000270
Train loss: 0.4490;  Loss pred: 0.4490; Loss self: 0.0000; time: 0.26s
Val loss: 0.6094 score: 0.5510 time: 0.07s
Test loss: 0.5594 score: 0.5306 time: 0.08s
     INFO: Early stopping counter 3 of 20
Epoch 16/1000, LR 0.000270
Train loss: 0.4200;  Loss pred: 0.4200; Loss self: 0.0000; time: 0.23s
Val loss: 0.6101 score: 0.5918 time: 0.06s
Test loss: 0.5589 score: 0.5510 time: 0.09s
     INFO: Early stopping counter 4 of 20
Epoch 17/1000, LR 0.000270
Train loss: 0.3841;  Loss pred: 0.3841; Loss self: 0.0000; time: 0.23s
Val loss: 0.6119 score: 0.5714 time: 0.07s
Test loss: 0.5580 score: 0.5510 time: 0.08s
     INFO: Early stopping counter 5 of 20
Epoch 18/1000, LR 0.000270
Train loss: 0.3480;  Loss pred: 0.3480; Loss self: 0.0000; time: 0.23s
Val loss: 0.6144 score: 0.5918 time: 0.07s
Test loss: 0.5577 score: 0.5510 time: 0.09s
     INFO: Early stopping counter 6 of 20
Epoch 19/1000, LR 0.000270
Train loss: 0.3384;  Loss pred: 0.3384; Loss self: 0.0000; time: 0.25s
Val loss: 0.6168 score: 0.6327 time: 0.07s
Test loss: 0.5570 score: 0.5918 time: 0.08s
     INFO: Early stopping counter 7 of 20
Epoch 20/1000, LR 0.000270
Train loss: 0.3264;  Loss pred: 0.3264; Loss self: 0.0000; time: 0.25s
Val loss: 0.6196 score: 0.6327 time: 0.07s
Test loss: 0.5560 score: 0.6122 time: 0.09s
     INFO: Early stopping counter 8 of 20
Epoch 21/1000, LR 0.000270
Train loss: 0.2869;  Loss pred: 0.2869; Loss self: 0.0000; time: 0.23s
Val loss: 0.6215 score: 0.6531 time: 0.07s
Test loss: 0.5550 score: 0.6122 time: 0.08s
     INFO: Early stopping counter 9 of 20
Epoch 22/1000, LR 0.000270
Train loss: 0.2484;  Loss pred: 0.2484; Loss self: 0.0000; time: 0.24s
Val loss: 0.6217 score: 0.6735 time: 0.08s
Test loss: 0.5522 score: 0.6531 time: 0.09s
     INFO: Early stopping counter 10 of 20
Epoch 23/1000, LR 0.000270
Train loss: 0.2540;  Loss pred: 0.2540; Loss self: 0.0000; time: 0.24s
Val loss: 0.6201 score: 0.6735 time: 0.08s
Test loss: 0.5480 score: 0.6735 time: 0.09s
     INFO: Early stopping counter 11 of 20
Epoch 24/1000, LR 0.000270
Train loss: 0.2359;  Loss pred: 0.2359; Loss self: 0.0000; time: 0.25s
Val loss: 0.6184 score: 0.6327 time: 0.07s
Test loss: 0.5431 score: 0.7143 time: 0.09s
     INFO: Early stopping counter 12 of 20
Epoch 25/1000, LR 0.000270
Train loss: 0.2089;  Loss pred: 0.2089; Loss self: 0.0000; time: 0.26s
Val loss: 0.6163 score: 0.6122 time: 0.07s
Test loss: 0.5388 score: 0.7347 time: 0.08s
     INFO: Early stopping counter 13 of 20
Epoch 26/1000, LR 0.000270
Train loss: 0.2074;  Loss pred: 0.2074; Loss self: 0.0000; time: 0.25s
Val loss: 0.6138 score: 0.6122 time: 0.08s
Test loss: 0.5340 score: 0.7551 time: 0.09s
     INFO: Early stopping counter 14 of 20
Epoch 27/1000, LR 0.000270
Train loss: 0.1836;  Loss pred: 0.1836; Loss self: 0.0000; time: 0.25s
Val loss: 0.6098 score: 0.6122 time: 0.08s
Test loss: 0.5286 score: 0.7551 time: 0.09s
     INFO: Early stopping counter 15 of 20
Epoch 28/1000, LR 0.000270
Train loss: 0.2101;  Loss pred: 0.2101; Loss self: 0.0000; time: 0.26s
Val loss: 0.6053 score: 0.6327 time: 0.08s
Test loss: 0.5223 score: 0.7551 time: 0.09s
Epoch 29/1000, LR 0.000270
Train loss: 0.1471;  Loss pred: 0.1471; Loss self: 0.0000; time: 0.24s
Val loss: 0.5990 score: 0.6327 time: 0.08s
Test loss: 0.5155 score: 0.7551 time: 0.09s
Epoch 30/1000, LR 0.000270
Train loss: 0.1479;  Loss pred: 0.1479; Loss self: 0.0000; time: 0.26s
Val loss: 0.5919 score: 0.6531 time: 0.07s
Test loss: 0.5088 score: 0.7347 time: 0.09s
Epoch 31/1000, LR 0.000270
Train loss: 0.1200;  Loss pred: 0.1200; Loss self: 0.0000; time: 0.27s
Val loss: 0.5840 score: 0.6531 time: 0.07s
Test loss: 0.5014 score: 0.7347 time: 0.09s
Epoch 32/1000, LR 0.000270
Train loss: 0.1259;  Loss pred: 0.1259; Loss self: 0.0000; time: 0.26s
Val loss: 0.5771 score: 0.6531 time: 0.07s
Test loss: 0.4939 score: 0.7347 time: 0.09s
Epoch 33/1000, LR 0.000270
Train loss: 0.1073;  Loss pred: 0.1073; Loss self: 0.0000; time: 0.28s
Val loss: 0.5708 score: 0.6939 time: 0.08s
Test loss: 0.4865 score: 0.7755 time: 0.10s
Epoch 34/1000, LR 0.000270
Train loss: 0.0987;  Loss pred: 0.0987; Loss self: 0.0000; time: 0.26s
Val loss: 0.5652 score: 0.6939 time: 0.08s
Test loss: 0.4796 score: 0.7755 time: 0.09s
Epoch 35/1000, LR 0.000270
Train loss: 0.0884;  Loss pred: 0.0884; Loss self: 0.0000; time: 0.25s
Val loss: 0.5599 score: 0.7347 time: 0.07s
Test loss: 0.4722 score: 0.7755 time: 0.08s
Epoch 36/1000, LR 0.000270
Train loss: 0.0896;  Loss pred: 0.0896; Loss self: 0.0000; time: 0.24s
Val loss: 0.5544 score: 0.7347 time: 0.08s
Test loss: 0.4658 score: 0.8163 time: 0.09s
Epoch 37/1000, LR 0.000270
Train loss: 0.0910;  Loss pred: 0.0910; Loss self: 0.0000; time: 0.25s
Val loss: 0.5486 score: 0.7347 time: 0.08s
Test loss: 0.4586 score: 0.8163 time: 0.09s
Epoch 38/1000, LR 0.000270
Train loss: 0.0762;  Loss pred: 0.0762; Loss self: 0.0000; time: 0.26s
Val loss: 0.5435 score: 0.7551 time: 0.07s
Test loss: 0.4515 score: 0.8163 time: 0.09s
Epoch 39/1000, LR 0.000269
Train loss: 0.0733;  Loss pred: 0.0733; Loss self: 0.0000; time: 0.26s
Val loss: 0.5380 score: 0.7551 time: 0.07s
Test loss: 0.4437 score: 0.8367 time: 0.09s
Epoch 40/1000, LR 0.000269
Train loss: 0.0642;  Loss pred: 0.0642; Loss self: 0.0000; time: 0.26s
Val loss: 0.5314 score: 0.7755 time: 0.07s
Test loss: 0.4356 score: 0.8571 time: 0.08s
Epoch 41/1000, LR 0.000269
Train loss: 0.0603;  Loss pred: 0.0603; Loss self: 0.0000; time: 0.24s
Val loss: 0.5238 score: 0.7755 time: 0.07s
Test loss: 0.4275 score: 0.8571 time: 0.08s
Epoch 42/1000, LR 0.000269
Train loss: 0.0496;  Loss pred: 0.0496; Loss self: 0.0000; time: 0.23s
Val loss: 0.5159 score: 0.7959 time: 0.07s
Test loss: 0.4191 score: 0.8571 time: 0.08s
Epoch 43/1000, LR 0.000269
Train loss: 0.0449;  Loss pred: 0.0449; Loss self: 0.0000; time: 0.25s
Val loss: 0.5085 score: 0.7755 time: 0.07s
Test loss: 0.4103 score: 0.8571 time: 0.08s
Epoch 44/1000, LR 0.000269
Train loss: 0.0374;  Loss pred: 0.0374; Loss self: 0.0000; time: 0.23s
Val loss: 0.4999 score: 0.7551 time: 0.07s
Test loss: 0.4012 score: 0.8571 time: 0.08s
Epoch 45/1000, LR 0.000269
Train loss: 0.0349;  Loss pred: 0.0349; Loss self: 0.0000; time: 0.22s
Val loss: 0.4909 score: 0.7551 time: 0.08s
Test loss: 0.3924 score: 0.8571 time: 0.09s
Epoch 46/1000, LR 0.000269
Train loss: 0.0340;  Loss pred: 0.0340; Loss self: 0.0000; time: 0.23s
Val loss: 0.4835 score: 0.7551 time: 0.07s
Test loss: 0.3829 score: 0.8571 time: 0.08s
Epoch 47/1000, LR 0.000269
Train loss: 0.0322;  Loss pred: 0.0322; Loss self: 0.0000; time: 0.25s
Val loss: 0.4763 score: 0.7551 time: 0.07s
Test loss: 0.3733 score: 0.8571 time: 0.08s
Epoch 48/1000, LR 0.000269
Train loss: 0.0278;  Loss pred: 0.0278; Loss self: 0.0000; time: 0.23s
Val loss: 0.4689 score: 0.7755 time: 0.07s
Test loss: 0.3639 score: 0.8571 time: 0.09s
Epoch 49/1000, LR 0.000269
Train loss: 0.0263;  Loss pred: 0.0263; Loss self: 0.0000; time: 0.22s
Val loss: 0.4609 score: 0.7551 time: 0.07s
Test loss: 0.3549 score: 0.8571 time: 0.09s
Epoch 50/1000, LR 0.000269
Train loss: 0.0269;  Loss pred: 0.0269; Loss self: 0.0000; time: 0.23s
Val loss: 0.4531 score: 0.7551 time: 0.07s
Test loss: 0.3462 score: 0.8776 time: 0.08s
Epoch 51/1000, LR 0.000269
Train loss: 0.0238;  Loss pred: 0.0238; Loss self: 0.0000; time: 0.24s
Val loss: 0.4448 score: 0.7551 time: 0.07s
Test loss: 0.3368 score: 0.8776 time: 0.08s
Epoch 52/1000, LR 0.000269
Train loss: 0.0238;  Loss pred: 0.0238; Loss self: 0.0000; time: 0.24s
Val loss: 0.4363 score: 0.7551 time: 0.07s
Test loss: 0.3272 score: 0.8776 time: 0.08s
Epoch 53/1000, LR 0.000269
Train loss: 0.0216;  Loss pred: 0.0216; Loss self: 0.0000; time: 0.24s
Val loss: 0.4303 score: 0.7755 time: 0.07s
Test loss: 0.3185 score: 0.8776 time: 0.08s
Epoch 54/1000, LR 0.000269
Train loss: 0.0191;  Loss pred: 0.0191; Loss self: 0.0000; time: 0.23s
Val loss: 0.4261 score: 0.7959 time: 0.07s
Test loss: 0.3115 score: 0.8776 time: 0.08s
Epoch 55/1000, LR 0.000269
Train loss: 0.0168;  Loss pred: 0.0168; Loss self: 0.0000; time: 0.23s
Val loss: 0.4211 score: 0.7959 time: 0.08s
Test loss: 0.3048 score: 0.8980 time: 0.08s
Epoch 56/1000, LR 0.000269
Train loss: 0.0180;  Loss pred: 0.0180; Loss self: 0.0000; time: 0.22s
Val loss: 0.4167 score: 0.7959 time: 0.07s
Test loss: 0.2989 score: 0.8980 time: 0.08s
Epoch 57/1000, LR 0.000269
Train loss: 0.0170;  Loss pred: 0.0170; Loss self: 0.0000; time: 0.22s
Val loss: 0.4108 score: 0.7959 time: 0.07s
Test loss: 0.2923 score: 0.8980 time: 0.08s
Epoch 58/1000, LR 0.000269
Train loss: 0.0139;  Loss pred: 0.0139; Loss self: 0.0000; time: 0.22s
Val loss: 0.4034 score: 0.7959 time: 0.07s
Test loss: 0.2844 score: 0.8980 time: 0.09s
Epoch 59/1000, LR 0.000268
Train loss: 0.0130;  Loss pred: 0.0130; Loss self: 0.0000; time: 0.22s
Val loss: 0.3956 score: 0.7959 time: 0.07s
Test loss: 0.2766 score: 0.9184 time: 0.08s
Epoch 60/1000, LR 0.000268
Train loss: 0.0115;  Loss pred: 0.0115; Loss self: 0.0000; time: 0.22s
Val loss: 0.3884 score: 0.8367 time: 0.07s
Test loss: 0.2696 score: 0.9184 time: 0.08s
Epoch 61/1000, LR 0.000268
Train loss: 0.0102;  Loss pred: 0.0102; Loss self: 0.0000; time: 0.24s
Val loss: 0.3820 score: 0.8367 time: 0.07s
Test loss: 0.2626 score: 0.9184 time: 0.08s
Epoch 62/1000, LR 0.000268
Train loss: 0.0094;  Loss pred: 0.0094; Loss self: 0.0000; time: 0.23s
Val loss: 0.3756 score: 0.8367 time: 0.07s
Test loss: 0.2550 score: 0.9184 time: 0.09s
Epoch 63/1000, LR 0.000268
Train loss: 0.0087;  Loss pred: 0.0087; Loss self: 0.0000; time: 0.25s
Val loss: 0.3694 score: 0.8367 time: 0.08s
Test loss: 0.2476 score: 0.9388 time: 0.09s
Epoch 64/1000, LR 0.000268
Train loss: 0.0086;  Loss pred: 0.0086; Loss self: 0.0000; time: 0.24s
Val loss: 0.3641 score: 0.8367 time: 0.07s
Test loss: 0.2408 score: 0.9388 time: 0.09s
Epoch 65/1000, LR 0.000268
Train loss: 0.0062;  Loss pred: 0.0062; Loss self: 0.0000; time: 0.23s
Val loss: 0.3589 score: 0.8571 time: 0.07s
Test loss: 0.2343 score: 0.9388 time: 0.08s
Epoch 66/1000, LR 0.000268
Train loss: 0.0089;  Loss pred: 0.0089; Loss self: 0.0000; time: 0.23s
Val loss: 0.3538 score: 0.8571 time: 0.07s
Test loss: 0.2270 score: 0.9388 time: 0.08s
Epoch 67/1000, LR 0.000268
Train loss: 0.0059;  Loss pred: 0.0059; Loss self: 0.0000; time: 0.22s
Val loss: 0.3494 score: 0.8367 time: 0.07s
Test loss: 0.2199 score: 0.9388 time: 0.08s
Epoch 68/1000, LR 0.000268
Train loss: 0.0069;  Loss pred: 0.0069; Loss self: 0.0000; time: 0.22s
Val loss: 0.3463 score: 0.8367 time: 0.07s
Test loss: 0.2129 score: 0.9388 time: 0.09s
Epoch 69/1000, LR 0.000268
Train loss: 0.0052;  Loss pred: 0.0052; Loss self: 0.0000; time: 0.22s
Val loss: 0.3454 score: 0.8367 time: 0.07s
Test loss: 0.2075 score: 0.9388 time: 0.09s
Epoch 70/1000, LR 0.000268
Train loss: 0.0056;  Loss pred: 0.0056; Loss self: 0.0000; time: 0.23s
Val loss: 0.3474 score: 0.8367 time: 0.07s
Test loss: 0.2041 score: 0.9388 time: 0.09s
     INFO: Early stopping counter 1 of 20
Epoch 71/1000, LR 0.000268
Train loss: 0.0061;  Loss pred: 0.0061; Loss self: 0.0000; time: 0.23s
Val loss: 0.3527 score: 0.8367 time: 0.07s
Test loss: 0.2044 score: 0.9388 time: 0.08s
     INFO: Early stopping counter 2 of 20
Epoch 72/1000, LR 0.000267
Train loss: 0.0044;  Loss pred: 0.0044; Loss self: 0.0000; time: 0.22s
Val loss: 0.3583 score: 0.8163 time: 0.07s
Test loss: 0.2058 score: 0.9388 time: 0.09s
     INFO: Early stopping counter 3 of 20
Epoch 73/1000, LR 0.000267
Train loss: 0.0049;  Loss pred: 0.0049; Loss self: 0.0000; time: 0.23s
Val loss: 0.3608 score: 0.8367 time: 0.07s
Test loss: 0.2054 score: 0.9388 time: 0.08s
     INFO: Early stopping counter 4 of 20
Epoch 74/1000, LR 0.000267
Train loss: 0.0052;  Loss pred: 0.0052; Loss self: 0.0000; time: 0.24s
Val loss: 0.3630 score: 0.8367 time: 0.07s
Test loss: 0.2042 score: 0.9388 time: 0.09s
     INFO: Early stopping counter 5 of 20
Epoch 75/1000, LR 0.000267
Train loss: 0.0050;  Loss pred: 0.0050; Loss self: 0.0000; time: 0.23s
Val loss: 0.3619 score: 0.8367 time: 0.07s
Test loss: 0.2009 score: 0.9388 time: 0.09s
     INFO: Early stopping counter 6 of 20
Epoch 76/1000, LR 0.000267
Train loss: 0.0037;  Loss pred: 0.0037; Loss self: 0.0000; time: 0.24s
Val loss: 0.3612 score: 0.8367 time: 0.07s
Test loss: 0.1973 score: 0.9388 time: 0.08s
     INFO: Early stopping counter 7 of 20
Epoch 77/1000, LR 0.000267
Train loss: 0.0042;  Loss pred: 0.0042; Loss self: 0.0000; time: 0.24s
Val loss: 0.3604 score: 0.8367 time: 0.07s
Test loss: 0.1928 score: 0.9388 time: 0.08s
     INFO: Early stopping counter 8 of 20
Epoch 78/1000, LR 0.000267
Train loss: 0.0031;  Loss pred: 0.0031; Loss self: 0.0000; time: 0.24s
Val loss: 0.3602 score: 0.8163 time: 0.07s
Test loss: 0.1891 score: 0.9388 time: 0.08s
     INFO: Early stopping counter 9 of 20
Epoch 79/1000, LR 0.000267
Train loss: 0.0031;  Loss pred: 0.0031; Loss self: 0.0000; time: 0.23s
Val loss: 0.3614 score: 0.8163 time: 0.07s
Test loss: 0.1870 score: 0.9388 time: 0.09s
     INFO: Early stopping counter 10 of 20
Epoch 80/1000, LR 0.000267
Train loss: 0.0036;  Loss pred: 0.0036; Loss self: 0.0000; time: 0.24s
Val loss: 0.3641 score: 0.8163 time: 0.07s
Test loss: 0.1859 score: 0.9388 time: 0.09s
     INFO: Early stopping counter 11 of 20
Epoch 81/1000, LR 0.000267
Train loss: 0.0031;  Loss pred: 0.0031; Loss self: 0.0000; time: 0.24s
Val loss: 0.3685 score: 0.8163 time: 0.07s
Test loss: 0.1855 score: 0.9388 time: 0.09s
     INFO: Early stopping counter 12 of 20
Epoch 82/1000, LR 0.000267
Train loss: 0.0025;  Loss pred: 0.0025; Loss self: 0.0000; time: 0.22s
Val loss: 0.3738 score: 0.8163 time: 0.07s
Test loss: 0.1862 score: 0.9388 time: 0.08s
     INFO: Early stopping counter 13 of 20
Epoch 83/1000, LR 0.000266
Train loss: 0.0025;  Loss pred: 0.0025; Loss self: 0.0000; time: 0.23s
Val loss: 0.3809 score: 0.8163 time: 0.07s
Test loss: 0.1886 score: 0.9388 time: 0.09s
     INFO: Early stopping counter 14 of 20
Epoch 84/1000, LR 0.000266
Train loss: 0.0021;  Loss pred: 0.0021; Loss self: 0.0000; time: 0.22s
Val loss: 0.3882 score: 0.8163 time: 0.07s
Test loss: 0.1913 score: 0.9388 time: 0.09s
     INFO: Early stopping counter 15 of 20
Epoch 85/1000, LR 0.000266
Train loss: 0.0030;  Loss pred: 0.0030; Loss self: 0.0000; time: 0.23s
Val loss: 0.3992 score: 0.8163 time: 0.07s
Test loss: 0.1965 score: 0.9388 time: 0.08s
     INFO: Early stopping counter 16 of 20
Epoch 86/1000, LR 0.000266
Train loss: 0.0021;  Loss pred: 0.0021; Loss self: 0.0000; time: 0.26s
Val loss: 0.4108 score: 0.8163 time: 0.07s
Test loss: 0.2024 score: 0.9184 time: 0.09s
     INFO: Early stopping counter 17 of 20
Epoch 87/1000, LR 0.000266
Train loss: 0.0024;  Loss pred: 0.0024; Loss self: 0.0000; time: 0.24s
Val loss: 0.4260 score: 0.8163 time: 0.07s
Test loss: 0.2106 score: 0.9184 time: 0.08s
     INFO: Early stopping counter 18 of 20
Epoch 88/1000, LR 0.000266
Train loss: 0.0018;  Loss pred: 0.0018; Loss self: 0.0000; time: 0.23s
Val loss: 0.4412 score: 0.8163 time: 0.08s
Test loss: 0.2188 score: 0.9184 time: 0.09s
     INFO: Early stopping counter 19 of 20
Epoch 89/1000, LR 0.000266
Train loss: 0.0017;  Loss pred: 0.0017; Loss self: 0.0000; time: 0.26s
Val loss: 0.4563 score: 0.8163 time: 0.08s
Test loss: 0.2269 score: 0.8980 time: 0.09s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 068,   Train_Loss: 0.0052,   Val_Loss: 0.3454,   Val_Precision: 1.0000,   Val_Recall: 0.6800,   Val_accuracy: 0.8095,   Val_Score: 0.8367,   Val_Loss: 0.3454,   Test_Precision: 1.0000,   Test_Recall: 0.8750,   Test_accuracy: 0.9333,   Test_Score: 0.9388,   Test_loss: 0.2075


[0.07314129103906453, 0.07222154701594263, 0.07299894897732884, 0.07240863796323538, 0.07285583496559411, 0.0788184500997886, 0.07466346502769738, 0.07542695908341557, 0.07441535301040858, 0.0720415289979428, 0.07173143897671252, 0.07184195599984378, 0.07173724297899753, 0.07131565199233592, 0.0717967189848423, 0.0780098260147497, 0.07316281704697758, 0.09810997196473181, 0.0721136819338426, 0.07010870601516217, 0.07171303406357765, 0.06965106190182269, 0.07069650501944125, 0.08222265599761158, 0.08552209904883057, 0.08889215905219316, 0.08861023001372814, 0.09154522290918976, 0.09074269398115575, 0.08526495203841478, 0.08531129197217524, 0.09617643803358078, 0.08584558998700231, 0.08588750392664224, 0.0895733799552545, 0.08164118195418268, 0.08298144699074328, 0.08585561602376401, 0.09049155307002366, 0.0862301510060206, 0.09385916090104729, 0.08607877104077488, 0.09078120905905962, 0.08924422005657107, 0.09407851693686098, 0.0901972169522196, 0.09502484695985913, 0.0849459971068427, 0.09032635693438351, 0.09491380909457803, 0.09442447207402438, 0.08968547102995217, 0.09094669797923416, 0.08980317995883524, 0.09560072107706219, 0.10805826599244028, 0.09357072203420103, 0.08598557289224118, 0.0919076050631702, 0.09525613207370043, 0.09006893599871546, 0.09434664598666131, 0.08430429501459002, 0.08796210307627916, 0.08305048604961485, 0.08686508203390986, 0.08129429002292454, 0.09215488890185952, 0.0880690929479897, 0.08759462798479944, 0.09166636003647, 0.0923796979477629, 0.08772476494777948, 0.08634085208177567, 0.087102364981547, 0.0867896870477125, 0.08639598602894694, 0.08840014389716089, 0.08589539001695812, 0.0850743199698627, 0.09069195401389152, 0.08597766095772386, 0.08676639595068991, 0.08632127603050321, 0.09098502702545375, 0.0947387219639495, 0.09339512197766453, 0.0859767870279029, 0.08696887898258865, 0.08703842002432793, 0.09140791895333678, 0.09169352101162076, 0.09051668201573193, 0.08554420701693743, 0.09229574992787093, 0.08625873702112585, 0.09044102998450398, 0.09331682790070772, 0.0880312870722264, 0.08733970101457089, 0.0872618219582364, 0.09076211601495743, 0.09122413001023233, 0.09593008202500641, 0.08574475604109466, 0.09043004002887756, 0.09183593397028744, 0.08685130707453936, 0.09181927295867354, 0.08600155403837562, 0.09501915995497257, 0.09728121501393616]
[0.0014926794089605005, 0.0014739091227743395, 0.0014897744689250784, 0.0014777273053721506, 0.001486853774808043, 0.0016085397979548695, 0.0015237441842387222, 0.0015393256955799095, 0.0015186806736818077, 0.001470235285672302, 0.0014639069178920923, 0.0014661623673437505, 0.001464025366918317, 0.0014554214692313451, 0.0014652391629559653, 0.001592037265607137, 0.0014931187152444404, 0.0020022443258108534, 0.0014717077945682164, 0.001430789918676779, 0.001463531307419952, 0.0014214502428943406, 0.0014427858167232908, 0.0016780133877063589, 0.0017453489601802156, 0.0018141256949427177, 0.0018083720410964927, 0.0018682698552895869, 0.001851891713901138, 0.0017401010620084647, 0.0017410467749423518, 0.0019627844496649138, 0.0017519508160612717, 0.0017528062025845355, 0.0018280281623521326, 0.0016661465704935242, 0.0016934989181784342, 0.0017521554290564085, 0.0018467663891841562, 0.0017597990001228694, 0.00191549307961321, 0.0017567096130770383, 0.0018526777358991759, 0.0018213106133994094, 0.0019199697334053262, 0.001840759529637135, 0.0019392825910175334, 0.0017335917776906673, 0.0018433950394772145, 0.0019370165121342456, 0.001927030042327028, 0.0018303157353051463, 0.001856055060800697, 0.0018327179583435764, 0.0019510351240216773, 0.0022052707345395976, 0.0019096065721265515, 0.0017548076100457385, 0.001875665409452453, 0.0019440026953816414, 0.0018381415509941931, 0.0019254417548298227, 0.0017204958166242863, 0.0017951449607403911, 0.0016949078785635683, 0.001772756776202242, 0.0016590671433249907, 0.0018807120184052963, 0.001797328427509994, 0.0017876454690775396, 0.0018707420415606123, 0.0018852999581176104, 0.0017903013254648872, 0.0017620582057505238, 0.001777599285337694, 0.0017712181030145409, 0.0017631833883458559, 0.001804084569329814, 0.0017529671432032269, 0.0017362106116298511, 0.001850856204365133, 0.0017546461419943645, 0.0017707427745038758, 0.0017616586945000657, 0.0018568372862337499, 0.0019334433053867246, 0.0019060228975033577, 0.0017546283066918959, 0.0017748750812773193, 0.0017762942862107741, 0.0018654677337415668, 0.0018712963471759338, 0.0018472792248108558, 0.0017458001432028047, 0.0018835867332218557, 0.0017603823881862418, 0.0018457353058062038, 0.0019044250591981168, 0.0017965568790250287, 0.0017824428778483855, 0.0017808535093517632, 0.0018522880819379067, 0.0018617169389843332, 0.0019577567760205392, 0.0017498929804305033, 0.0018455110209975012, 0.0018742027340874988, 0.001772475654582436, 0.0018738627134423172, 0.0017551337558852167, 0.0019391665296933176, 0.0019853309186517584]
[669.9362193898007, 678.467881464564, 671.2425409743619, 676.7148420175942, 672.5610930564458, 621.6818516218377, 656.2781406116474, 649.63509858339, 658.4662709743016, 680.1632430844052, 683.1035414737429, 682.0526991234279, 683.0482740233787, 687.0861954016193, 682.4824406021237, 628.1260003161054, 669.7391103535185, 499.4395474663302, 679.482709604993, 698.9146253733871, 683.278857739567, 703.5068620930491, 693.1035697807862, 595.9428019623127, 572.9513253881018, 551.2297206239481, 552.9835549733768, 535.2545817558017, 539.9883764766305, 574.6792653788608, 574.3671074162331, 509.4802947775135, 570.7922795733476, 570.5137273735608, 547.0375241447567, 600.1872930685762, 590.4934389185336, 570.7256236614419, 541.4870044509358, 568.2467144998831, 522.0587903151951, 569.246045308768, 539.7592795676694, 549.0551653534466, 520.8415438020289, 543.2540122158857, 515.6546057969325, 576.8370690659993, 542.4773196111015, 516.2578603412002, 518.933269349775, 546.353823392816, 538.7771198816713, 545.6376937037312, 512.5484352832642, 453.4590625711875, 523.6680762395957, 569.8630404126953, 533.1441284572825, 514.4025789551092, 544.0277433797911, 519.3613348685187, 581.2278009266298, 557.0580771301941, 590.0025674831946, 564.0931759078024, 602.748360139221, 531.7135160586284, 556.3813406019472, 559.3950351441999, 534.5472426362852, 530.4195736568394, 558.5651899913162, 567.5181425542434, 562.5564817945053, 564.5832087522372, 567.1559785611171, 554.2977402503268, 570.4613482787149, 575.9669900077731, 540.2904869873522, 569.9154810002779, 564.7347623825141, 567.646845056889, 538.5501505241283, 517.2119592097279, 524.6526688162404, 569.9212740306002, 563.4199333512153, 562.969778016468, 536.058588370382, 534.388901848256, 541.3366785968108, 572.8032523616495, 530.9020191969129, 568.0583983973621, 541.789495413703, 525.0928594801536, 556.6202838747242, 561.0277964178664, 561.5285000976883, 539.8728252647271, 537.1385837771631, 510.7886803143461, 571.4635187312902, 541.8553390483133, 533.560207661779, 564.1826433072123, 533.6570245122084, 569.7571462270928, 515.685468312075, 503.6943668207725]
Elapsed: 0.08623045580523987~0.007686946936128626
Time per graph: 0.0017598052205150994~0.0001568764680842577
Speed: 573.1228387029548~55.026348932470974
Total Time: 0.0979
best val loss: 0.345380574464798 test_score: 0.9388

Testing...
Test loss: 0.2343 score: 0.9388 time: 0.08s
test Score 0.9388
Epoch Time List: [0.5524872150272131, 0.3827338709961623, 0.43712066288571805, 0.37491501995828, 0.3877625920576975, 0.39658519614022225, 0.3835222409106791, 0.4787430779542774, 0.40432003303430974, 0.38756635901518166, 0.3896690080873668, 0.3964784271083772, 0.39066879102028906, 0.3782911730231717, 0.38654209196101874, 0.3801778459455818, 0.40622406208422035, 0.4517095491755754, 0.42368095403071493, 0.3739383629290387, 0.3963481510290876, 0.3833882430335507, 0.38113143399823457, 0.3935784619534388, 0.38799365994054824, 0.39019095816183835, 0.3919076930033043, 0.39931677002459764, 0.4048062519868836, 0.38257004087790847, 0.3897449900396168, 0.41108149103820324, 0.38756345806177706, 0.37905153806786984, 0.39153553708456457, 0.3767952920170501, 0.4008101460058242, 0.40557922585867345, 0.3823446410242468, 0.3866366839502007, 0.3961808829335496, 0.40315365593414754, 0.39684697799384594, 0.38598406105302274, 0.4081406721379608, 0.4018410069402307, 0.4144832898164168, 0.40520863293204457, 0.41638357704505324, 0.41962715808767825, 0.42083221208304167, 0.4056714739417657, 0.41706359502859414, 0.42511676193680614, 0.4205594230443239, 0.45985579304397106, 0.42688670300412923, 0.39699597388971597, 0.4041867849882692, 0.4122521720128134, 0.4117704341188073, 0.4197946551721543, 0.40488042507786304, 0.39012844301760197, 0.3813110819319263, 0.39578869298566133, 0.3761612050002441, 0.3814686320256442, 0.3845104188658297, 0.39455840503796935, 0.3873056110460311, 0.3822082818951458, 0.38913308491464704, 0.3890795299084857, 0.3869929440552369, 0.3984713250538334, 0.3832340909866616, 0.38751981186214834, 0.37486649595666677, 0.37002919893711805, 0.37727653607726097, 0.3780271540163085, 0.37147138512227684, 0.38327878795098513, 0.39131463202647865, 0.41367714398074895, 0.3936540560098365, 0.37340995494741946, 0.37801564391702414, 0.3713088190415874, 0.38337311486247927, 0.3818680710392073, 0.3861600258387625, 0.37683647498488426, 0.3799786389572546, 0.3811250190483406, 0.3891338191460818, 0.38985948206391186, 0.38725017406977713, 0.3861908089602366, 0.3867626239079982, 0.3861309870844707, 0.39001622097566724, 0.4009402529336512, 0.3683553639566526, 0.38694693404249847, 0.3809621511027217, 0.37955844891257584, 0.4146873940480873, 0.3944033168954775, 0.4037863799603656, 0.4327106700511649]
Total Epoch List: [23, 89]
Total Time List: [0.07104396796785295, 0.09791546897031367]
========================k_idx:2========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: GPSPerformer
GPS(
  (x_lin): Linear(in_features=14887, out_features=64, bias=True)
  (pe_lin): Linear(in_features=20, out_features=8, bias=True)
  (pe_norm): BatchNorm1d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (convs): ModuleList(
    (0-1): 2 x GPSConv(64, conv=GINEConv(nn=Sequential(
      (0): Linear(in_features=64, out_features=64, bias=True)
      (1): ReLU()
      (2): Linear(in_features=64, out_features=64, bias=True)
    )), heads=1, attn_type=performer)
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=32, bias=True)
    (1): ReLU()
    (2): Linear(in_features=32, out_features=16, bias=True)
    (3): ReLU()
    (4): Linear(in_features=16, out_features=2, bias=True)
  )
)
Total number of parameters: 1039522
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7e7c5703a230>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.9521;  Loss pred: 0.9521; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 5.2491 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 6.1307 score: 0.5000 time: 0.07s
Epoch 2/1000, LR 0.000000
Train loss: 1.0474;  Loss pred: 1.0474; Loss self: 0.0000; time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 4.1367 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 4.7737 score: 0.5000 time: 0.07s
Epoch 3/1000, LR 0.000030
Train loss: 0.9851;  Loss pred: 0.9851; Loss self: 0.0000; time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 3.0723 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 3.4174 score: 0.5000 time: 0.07s
Epoch 4/1000, LR 0.000060
Train loss: 0.9128;  Loss pred: 0.9128; Loss self: 0.0000; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 2.2541 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 2.3736 score: 0.5000 time: 0.08s
Epoch 5/1000, LR 0.000090
Train loss: 0.8200;  Loss pred: 0.8200; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 1.6814 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 1.7999 score: 0.5000 time: 0.08s
Epoch 6/1000, LR 0.000120
Train loss: 0.7509;  Loss pred: 0.7509; Loss self: 0.0000; time: 0.23s
Val loss: 1.3033 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 1.3481 score: 0.5000 time: 0.08s
Epoch 7/1000, LR 0.000150
Train loss: 0.7192;  Loss pred: 0.7192; Loss self: 0.0000; time: 0.23s
Val loss: 1.0294 score: 0.4490 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.9812 score: 0.5000 time: 0.07s
Epoch 8/1000, LR 0.000180
Train loss: 0.6950;  Loss pred: 0.6950; Loss self: 0.0000; time: 0.23s
Val loss: 0.8691 score: 0.5306 time: 0.07s
Test loss: 0.7931 score: 0.4792 time: 0.07s
Epoch 9/1000, LR 0.000210
Train loss: 0.6313;  Loss pred: 0.6313; Loss self: 0.0000; time: 0.23s
Val loss: 0.7814 score: 0.5510 time: 0.07s
Test loss: 0.6974 score: 0.6042 time: 0.07s
Epoch 10/1000, LR 0.000240
Train loss: 0.6006;  Loss pred: 0.6006; Loss self: 0.0000; time: 0.23s
Val loss: 0.7418 score: 0.6122 time: 0.07s
Test loss: 0.6828 score: 0.5625 time: 0.07s
Epoch 11/1000, LR 0.000270
Train loss: 0.5918;  Loss pred: 0.5918; Loss self: 0.0000; time: 0.23s
Val loss: 0.7193 score: 0.5714 time: 0.07s
Test loss: 0.6792 score: 0.5833 time: 0.07s
Epoch 12/1000, LR 0.000270
Train loss: 0.5036;  Loss pred: 0.5036; Loss self: 0.0000; time: 0.23s
Val loss: 0.6971 score: 0.6122 time: 0.07s
Test loss: 0.6689 score: 0.5417 time: 0.08s
Epoch 13/1000, LR 0.000270
Train loss: 0.4920;  Loss pred: 0.4920; Loss self: 0.0000; time: 0.23s
Val loss: 0.6729 score: 0.6531 time: 0.07s
Test loss: 0.6553 score: 0.6250 time: 0.07s
Epoch 14/1000, LR 0.000270
Train loss: 0.4677;  Loss pred: 0.4677; Loss self: 0.0000; time: 0.23s
Val loss: 0.6541 score: 0.5918 time: 0.07s
Test loss: 0.6443 score: 0.5833 time: 0.07s
Epoch 15/1000, LR 0.000270
Train loss: 0.4847;  Loss pred: 0.4847; Loss self: 0.0000; time: 0.23s
Val loss: 0.6354 score: 0.5102 time: 0.07s
Test loss: 0.6327 score: 0.5833 time: 0.07s
Epoch 16/1000, LR 0.000270
Train loss: 0.4849;  Loss pred: 0.4849; Loss self: 0.0000; time: 0.23s
Val loss: 0.6210 score: 0.5306 time: 0.07s
Test loss: 0.6192 score: 0.5625 time: 0.07s
Epoch 17/1000, LR 0.000270
Train loss: 0.4092;  Loss pred: 0.4092; Loss self: 0.0000; time: 0.23s
Val loss: 0.6133 score: 0.5306 time: 0.07s
Test loss: 0.6100 score: 0.5833 time: 0.07s
Epoch 18/1000, LR 0.000270
Train loss: 0.3709;  Loss pred: 0.3709; Loss self: 0.0000; time: 0.23s
Val loss: 0.6085 score: 0.5306 time: 0.07s
Test loss: 0.6031 score: 0.5625 time: 0.07s
Epoch 19/1000, LR 0.000270
Train loss: 0.3652;  Loss pred: 0.3652; Loss self: 0.0000; time: 0.23s
Val loss: 0.6053 score: 0.5306 time: 0.07s
Test loss: 0.5976 score: 0.5625 time: 0.07s
Epoch 20/1000, LR 0.000270
Train loss: 0.3202;  Loss pred: 0.3202; Loss self: 0.0000; time: 0.23s
Val loss: 0.6019 score: 0.5306 time: 0.13s
Test loss: 0.5944 score: 0.5625 time: 0.07s
Epoch 21/1000, LR 0.000270
Train loss: 0.3092;  Loss pred: 0.3092; Loss self: 0.0000; time: 0.23s
Val loss: 0.5988 score: 0.5306 time: 0.07s
Test loss: 0.5919 score: 0.5625 time: 0.07s
Epoch 22/1000, LR 0.000270
Train loss: 0.2736;  Loss pred: 0.2736; Loss self: 0.0000; time: 0.23s
Val loss: 0.5973 score: 0.5306 time: 0.07s
Test loss: 0.5908 score: 0.5625 time: 0.07s
Epoch 23/1000, LR 0.000270
Train loss: 0.2482;  Loss pred: 0.2482; Loss self: 0.0000; time: 0.24s
Val loss: 0.5970 score: 0.5306 time: 0.07s
Test loss: 0.5906 score: 0.5417 time: 0.07s
Epoch 24/1000, LR 0.000270
Train loss: 0.2466;  Loss pred: 0.2466; Loss self: 0.0000; time: 0.22s
Val loss: 0.5983 score: 0.5306 time: 0.07s
Test loss: 0.5921 score: 0.5417 time: 0.07s
     INFO: Early stopping counter 1 of 20
Epoch 25/1000, LR 0.000270
Train loss: 0.2235;  Loss pred: 0.2235; Loss self: 0.0000; time: 0.24s
Val loss: 0.5991 score: 0.5102 time: 0.08s
Test loss: 0.5951 score: 0.5417 time: 0.07s
     INFO: Early stopping counter 2 of 20
Epoch 26/1000, LR 0.000270
Train loss: 0.2154;  Loss pred: 0.2154; Loss self: 0.0000; time: 0.23s
Val loss: 0.5988 score: 0.5102 time: 0.07s
Test loss: 0.5956 score: 0.5417 time: 0.07s
     INFO: Early stopping counter 3 of 20
Epoch 27/1000, LR 0.000270
Train loss: 0.2099;  Loss pred: 0.2099; Loss self: 0.0000; time: 0.22s
Val loss: 0.5982 score: 0.5102 time: 0.07s
Test loss: 0.5903 score: 0.5417 time: 0.07s
     INFO: Early stopping counter 4 of 20
Epoch 28/1000, LR 0.000270
Train loss: 0.1665;  Loss pred: 0.1665; Loss self: 0.0000; time: 0.23s
Val loss: 0.5986 score: 0.5102 time: 0.07s
Test loss: 0.5832 score: 0.5417 time: 0.07s
     INFO: Early stopping counter 5 of 20
Epoch 29/1000, LR 0.000270
Train loss: 0.1745;  Loss pred: 0.1745; Loss self: 0.0000; time: 0.22s
Val loss: 0.5968 score: 0.5306 time: 0.07s
Test loss: 0.5759 score: 0.5625 time: 0.07s
Epoch 30/1000, LR 0.000270
Train loss: 0.1498;  Loss pred: 0.1498; Loss self: 0.0000; time: 0.32s
Val loss: 0.5922 score: 0.5306 time: 0.07s
Test loss: 0.5715 score: 0.5625 time: 0.07s
Epoch 31/1000, LR 0.000270
Train loss: 0.1324;  Loss pred: 0.1324; Loss self: 0.0000; time: 0.23s
Val loss: 0.5888 score: 0.5714 time: 0.07s
Test loss: 0.5678 score: 0.5833 time: 0.07s
Epoch 32/1000, LR 0.000270
Train loss: 0.1284;  Loss pred: 0.1284; Loss self: 0.0000; time: 0.23s
Val loss: 0.5845 score: 0.5918 time: 0.07s
Test loss: 0.5637 score: 0.6250 time: 0.09s
Epoch 33/1000, LR 0.000270
Train loss: 0.1034;  Loss pred: 0.1034; Loss self: 0.0000; time: 0.23s
Val loss: 0.5817 score: 0.6327 time: 0.07s
Test loss: 0.5649 score: 0.6250 time: 0.07s
Epoch 34/1000, LR 0.000270
Train loss: 0.1034;  Loss pred: 0.1034; Loss self: 0.0000; time: 0.24s
Val loss: 0.5811 score: 0.6531 time: 0.09s
Test loss: 0.5721 score: 0.6042 time: 0.07s
Epoch 35/1000, LR 0.000270
Train loss: 0.0941;  Loss pred: 0.0941; Loss self: 0.0000; time: 0.22s
Val loss: 0.5806 score: 0.6327 time: 0.07s
Test loss: 0.5746 score: 0.6042 time: 0.07s
Epoch 36/1000, LR 0.000270
Train loss: 0.0907;  Loss pred: 0.0907; Loss self: 0.0000; time: 0.23s
Val loss: 0.5803 score: 0.6122 time: 0.07s
Test loss: 0.5752 score: 0.5833 time: 0.08s
Epoch 37/1000, LR 0.000270
Train loss: 0.0965;  Loss pred: 0.0965; Loss self: 0.0000; time: 0.23s
Val loss: 0.5779 score: 0.6122 time: 0.07s
Test loss: 0.5668 score: 0.5833 time: 0.07s
Epoch 38/1000, LR 0.000270
Train loss: 0.0709;  Loss pred: 0.0709; Loss self: 0.0000; time: 0.24s
Val loss: 0.5749 score: 0.6327 time: 0.08s
Test loss: 0.5555 score: 0.6042 time: 0.08s
Epoch 39/1000, LR 0.000269
Train loss: 0.0817;  Loss pred: 0.0817; Loss self: 0.0000; time: 0.23s
Val loss: 0.5729 score: 0.6327 time: 0.07s
Test loss: 0.5446 score: 0.6250 time: 0.07s
Epoch 40/1000, LR 0.000269
Train loss: 0.0700;  Loss pred: 0.0700; Loss self: 0.0000; time: 0.22s
Val loss: 0.5721 score: 0.6327 time: 0.07s
Test loss: 0.5341 score: 0.6667 time: 0.07s
Epoch 41/1000, LR 0.000269
Train loss: 0.0592;  Loss pred: 0.0592; Loss self: 0.0000; time: 0.22s
Val loss: 0.5732 score: 0.6327 time: 0.07s
Test loss: 0.5298 score: 0.6667 time: 0.07s
     INFO: Early stopping counter 1 of 20
Epoch 42/1000, LR 0.000269
Train loss: 0.0591;  Loss pred: 0.0591; Loss self: 0.0000; time: 0.22s
Val loss: 0.5751 score: 0.6327 time: 0.07s
Test loss: 0.5253 score: 0.6667 time: 0.07s
     INFO: Early stopping counter 2 of 20
Epoch 43/1000, LR 0.000269
Train loss: 0.0584;  Loss pred: 0.0584; Loss self: 0.0000; time: 0.22s
Val loss: 0.5780 score: 0.6327 time: 0.07s
Test loss: 0.5216 score: 0.6667 time: 0.07s
     INFO: Early stopping counter 3 of 20
Epoch 44/1000, LR 0.000269
Train loss: 0.0727;  Loss pred: 0.0727; Loss self: 0.0000; time: 0.22s
Val loss: 0.5813 score: 0.6327 time: 0.07s
Test loss: 0.5172 score: 0.6667 time: 0.07s
     INFO: Early stopping counter 4 of 20
Epoch 45/1000, LR 0.000269
Train loss: 0.0526;  Loss pred: 0.0526; Loss self: 0.0000; time: 0.22s
Val loss: 0.5849 score: 0.6327 time: 0.07s
Test loss: 0.5154 score: 0.6875 time: 0.07s
     INFO: Early stopping counter 5 of 20
Epoch 46/1000, LR 0.000269
Train loss: 0.0494;  Loss pred: 0.0494; Loss self: 0.0000; time: 0.22s
Val loss: 0.5904 score: 0.6327 time: 0.07s
Test loss: 0.5152 score: 0.6875 time: 0.07s
     INFO: Early stopping counter 6 of 20
Epoch 47/1000, LR 0.000269
Train loss: 0.0397;  Loss pred: 0.0397; Loss self: 0.0000; time: 0.22s
Val loss: 0.5932 score: 0.6122 time: 0.07s
Test loss: 0.5134 score: 0.6875 time: 0.07s
     INFO: Early stopping counter 7 of 20
Epoch 48/1000, LR 0.000269
Train loss: 0.0352;  Loss pred: 0.0352; Loss self: 0.0000; time: 0.22s
Val loss: 0.5934 score: 0.6122 time: 0.07s
Test loss: 0.5105 score: 0.6875 time: 0.07s
     INFO: Early stopping counter 8 of 20
Epoch 49/1000, LR 0.000269
Train loss: 0.0321;  Loss pred: 0.0321; Loss self: 0.0000; time: 0.22s
Val loss: 0.5911 score: 0.6122 time: 0.07s
Test loss: 0.5063 score: 0.6875 time: 0.07s
     INFO: Early stopping counter 9 of 20
Epoch 50/1000, LR 0.000269
Train loss: 0.0343;  Loss pred: 0.0343; Loss self: 0.0000; time: 0.22s
Val loss: 0.5877 score: 0.6327 time: 0.07s
Test loss: 0.5013 score: 0.6667 time: 0.07s
     INFO: Early stopping counter 10 of 20
Epoch 51/1000, LR 0.000269
Train loss: 0.0321;  Loss pred: 0.0321; Loss self: 0.0000; time: 0.22s
Val loss: 0.5834 score: 0.6531 time: 0.07s
Test loss: 0.4936 score: 0.6667 time: 0.07s
     INFO: Early stopping counter 11 of 20
Epoch 52/1000, LR 0.000269
Train loss: 0.0312;  Loss pred: 0.0312; Loss self: 0.0000; time: 0.22s
Val loss: 0.5784 score: 0.6939 time: 0.07s
Test loss: 0.4852 score: 0.6667 time: 0.08s
     INFO: Early stopping counter 12 of 20
Epoch 53/1000, LR 0.000269
Train loss: 0.0289;  Loss pred: 0.0289; Loss self: 0.0000; time: 0.23s
Val loss: 0.5734 score: 0.6939 time: 0.07s
Test loss: 0.4756 score: 0.6875 time: 0.08s
     INFO: Early stopping counter 13 of 20
Epoch 54/1000, LR 0.000269
Train loss: 0.0269;  Loss pred: 0.0269; Loss self: 0.0000; time: 0.23s
Val loss: 0.5705 score: 0.6939 time: 0.07s
Test loss: 0.4677 score: 0.6875 time: 0.08s
Epoch 55/1000, LR 0.000269
Train loss: 0.0276;  Loss pred: 0.0276; Loss self: 0.0000; time: 0.24s
Val loss: 0.5707 score: 0.6939 time: 0.07s
Test loss: 0.4622 score: 0.6875 time: 0.08s
     INFO: Early stopping counter 1 of 20
Epoch 56/1000, LR 0.000269
Train loss: 0.0286;  Loss pred: 0.0286; Loss self: 0.0000; time: 0.23s
Val loss: 0.5720 score: 0.6735 time: 0.07s
Test loss: 0.4579 score: 0.7083 time: 0.07s
     INFO: Early stopping counter 2 of 20
Epoch 57/1000, LR 0.000269
Train loss: 0.0202;  Loss pred: 0.0202; Loss self: 0.0000; time: 0.24s
Val loss: 0.5703 score: 0.6735 time: 0.07s
Test loss: 0.4520 score: 0.7292 time: 0.08s
Epoch 58/1000, LR 0.000269
Train loss: 0.0207;  Loss pred: 0.0207; Loss self: 0.0000; time: 0.24s
Val loss: 0.5653 score: 0.6939 time: 0.07s
Test loss: 0.4438 score: 0.7083 time: 0.07s
Epoch 59/1000, LR 0.000268
Train loss: 0.0204;  Loss pred: 0.0204; Loss self: 0.0000; time: 0.23s
Val loss: 0.5570 score: 0.7347 time: 0.07s
Test loss: 0.4330 score: 0.7083 time: 0.08s
Epoch 60/1000, LR 0.000268
Train loss: 0.0193;  Loss pred: 0.0193; Loss self: 0.0000; time: 0.23s
Val loss: 0.5396 score: 0.7959 time: 0.07s
Test loss: 0.4126 score: 0.7708 time: 0.08s
Epoch 61/1000, LR 0.000268
Train loss: 0.0177;  Loss pred: 0.0177; Loss self: 0.0000; time: 0.24s
Val loss: 0.5209 score: 0.8163 time: 0.07s
Test loss: 0.3918 score: 0.8125 time: 0.08s
Epoch 62/1000, LR 0.000268
Train loss: 0.0158;  Loss pred: 0.0158; Loss self: 0.0000; time: 0.24s
Val loss: 0.5017 score: 0.7959 time: 0.07s
Test loss: 0.3722 score: 0.8542 time: 0.08s
Epoch 63/1000, LR 0.000268
Train loss: 0.0155;  Loss pred: 0.0155; Loss self: 0.0000; time: 0.24s
Val loss: 0.4855 score: 0.7959 time: 0.07s
Test loss: 0.3563 score: 0.8333 time: 0.08s
Epoch 64/1000, LR 0.000268
Train loss: 0.0138;  Loss pred: 0.0138; Loss self: 0.0000; time: 0.24s
Val loss: 0.4711 score: 0.8367 time: 0.08s
Test loss: 0.3405 score: 0.8333 time: 0.08s
Epoch 65/1000, LR 0.000268
Train loss: 0.0156;  Loss pred: 0.0156; Loss self: 0.0000; time: 0.26s
Val loss: 0.4612 score: 0.8367 time: 0.08s
Test loss: 0.3290 score: 0.8750 time: 0.08s
Epoch 66/1000, LR 0.000268
Train loss: 0.0180;  Loss pred: 0.0180; Loss self: 0.0000; time: 0.24s
Val loss: 0.4532 score: 0.8367 time: 0.07s
Test loss: 0.3194 score: 0.8750 time: 0.08s
Epoch 67/1000, LR 0.000268
Train loss: 0.0140;  Loss pred: 0.0140; Loss self: 0.0000; time: 0.24s
Val loss: 0.4444 score: 0.8367 time: 0.07s
Test loss: 0.3084 score: 0.8750 time: 0.07s
Epoch 68/1000, LR 0.000268
Train loss: 0.0128;  Loss pred: 0.0128; Loss self: 0.0000; time: 0.23s
Val loss: 0.4339 score: 0.8367 time: 0.07s
Test loss: 0.2946 score: 0.8750 time: 0.08s
Epoch 69/1000, LR 0.000268
Train loss: 0.0154;  Loss pred: 0.0154; Loss self: 0.0000; time: 0.24s
Val loss: 0.4214 score: 0.8367 time: 0.07s
Test loss: 0.2779 score: 0.8750 time: 0.08s
Epoch 70/1000, LR 0.000268
Train loss: 0.0127;  Loss pred: 0.0127; Loss self: 0.0000; time: 0.24s
Val loss: 0.4107 score: 0.8571 time: 0.07s
Test loss: 0.2670 score: 0.9167 time: 0.08s
Epoch 71/1000, LR 0.000268
Train loss: 0.0141;  Loss pred: 0.0141; Loss self: 0.0000; time: 0.23s
Val loss: 0.3998 score: 0.8776 time: 0.07s
Test loss: 0.2608 score: 0.9167 time: 0.08s
Epoch 72/1000, LR 0.000267
Train loss: 0.0106;  Loss pred: 0.0106; Loss self: 0.0000; time: 0.23s
Val loss: 0.3921 score: 0.8571 time: 0.07s
Test loss: 0.2577 score: 0.8958 time: 0.08s
Epoch 73/1000, LR 0.000267
Train loss: 0.0110;  Loss pred: 0.0110; Loss self: 0.0000; time: 0.23s
Val loss: 0.3884 score: 0.8367 time: 0.07s
Test loss: 0.2572 score: 0.8958 time: 0.08s
Epoch 74/1000, LR 0.000267
Train loss: 0.0095;  Loss pred: 0.0095; Loss self: 0.0000; time: 0.23s
Val loss: 0.3878 score: 0.8367 time: 0.08s
Test loss: 0.2587 score: 0.8958 time: 0.08s
Epoch 75/1000, LR 0.000267
Train loss: 0.0121;  Loss pred: 0.0121; Loss self: 0.0000; time: 0.24s
Val loss: 0.3876 score: 0.8163 time: 0.07s
Test loss: 0.2581 score: 0.8958 time: 0.08s
Epoch 76/1000, LR 0.000267
Train loss: 0.0085;  Loss pred: 0.0085; Loss self: 0.0000; time: 0.23s
Val loss: 0.3876 score: 0.8163 time: 0.07s
Test loss: 0.2568 score: 0.8958 time: 0.07s
     INFO: Early stopping counter 1 of 20
Epoch 77/1000, LR 0.000267
Train loss: 0.0106;  Loss pred: 0.0106; Loss self: 0.0000; time: 0.23s
Val loss: 0.3875 score: 0.8163 time: 0.08s
Test loss: 0.2533 score: 0.8958 time: 0.08s
Epoch 78/1000, LR 0.000267
Train loss: 0.0093;  Loss pred: 0.0093; Loss self: 0.0000; time: 0.25s
Val loss: 0.3865 score: 0.7959 time: 0.08s
Test loss: 0.2462 score: 0.8958 time: 0.08s
Epoch 79/1000, LR 0.000267
Train loss: 0.0086;  Loss pred: 0.0086; Loss self: 0.0000; time: 0.24s
Val loss: 0.3849 score: 0.7959 time: 0.07s
Test loss: 0.2369 score: 0.8958 time: 0.07s
Epoch 80/1000, LR 0.000267
Train loss: 0.0094;  Loss pred: 0.0094; Loss self: 0.0000; time: 0.24s
Val loss: 0.3825 score: 0.8163 time: 0.07s
Test loss: 0.2282 score: 0.8958 time: 0.08s
Epoch 81/1000, LR 0.000267
Train loss: 0.0105;  Loss pred: 0.0105; Loss self: 0.0000; time: 0.24s
Val loss: 0.3804 score: 0.7959 time: 0.07s
Test loss: 0.2211 score: 0.8958 time: 0.07s
Epoch 82/1000, LR 0.000267
Train loss: 0.0081;  Loss pred: 0.0081; Loss self: 0.0000; time: 0.24s
Val loss: 0.3803 score: 0.7959 time: 0.07s
Test loss: 0.2168 score: 0.9167 time: 0.07s
Epoch 83/1000, LR 0.000266
Train loss: 0.0067;  Loss pred: 0.0067; Loss self: 0.0000; time: 0.23s
Val loss: 0.3821 score: 0.8163 time: 0.07s
Test loss: 0.2112 score: 0.9167 time: 0.07s
     INFO: Early stopping counter 1 of 20
Epoch 84/1000, LR 0.000266
Train loss: 0.0064;  Loss pred: 0.0064; Loss self: 0.0000; time: 0.23s
Val loss: 0.3852 score: 0.8163 time: 0.07s
Test loss: 0.2067 score: 0.9375 time: 0.07s
     INFO: Early stopping counter 2 of 20
Epoch 85/1000, LR 0.000266
Train loss: 0.0063;  Loss pred: 0.0063; Loss self: 0.0000; time: 0.23s
Val loss: 0.3943 score: 0.8163 time: 0.07s
Test loss: 0.2099 score: 0.9375 time: 0.07s
     INFO: Early stopping counter 3 of 20
Epoch 86/1000, LR 0.000266
Train loss: 0.0074;  Loss pred: 0.0074; Loss self: 0.0000; time: 0.23s
Val loss: 0.4115 score: 0.8163 time: 0.07s
Test loss: 0.2191 score: 0.9167 time: 0.08s
     INFO: Early stopping counter 4 of 20
Epoch 87/1000, LR 0.000266
Train loss: 0.0047;  Loss pred: 0.0047; Loss self: 0.0000; time: 0.23s
Val loss: 0.4327 score: 0.7959 time: 0.07s
Test loss: 0.2315 score: 0.9167 time: 0.08s
     INFO: Early stopping counter 5 of 20
Epoch 88/1000, LR 0.000266
Train loss: 0.0067;  Loss pred: 0.0067; Loss self: 0.0000; time: 0.23s
Val loss: 0.4559 score: 0.7959 time: 0.08s
Test loss: 0.2467 score: 0.9167 time: 0.08s
     INFO: Early stopping counter 6 of 20
Epoch 89/1000, LR 0.000266
Train loss: 0.0051;  Loss pred: 0.0051; Loss self: 0.0000; time: 0.24s
Val loss: 0.4796 score: 0.7959 time: 0.07s
Test loss: 0.2665 score: 0.9167 time: 0.08s
     INFO: Early stopping counter 7 of 20
Epoch 90/1000, LR 0.000266
Train loss: 0.0060;  Loss pred: 0.0060; Loss self: 0.0000; time: 0.24s
Val loss: 0.5041 score: 0.7959 time: 0.07s
Test loss: 0.2898 score: 0.8958 time: 0.08s
     INFO: Early stopping counter 8 of 20
Epoch 91/1000, LR 0.000266
Train loss: 0.0051;  Loss pred: 0.0051; Loss self: 0.0000; time: 0.24s
Val loss: 0.5270 score: 0.7959 time: 0.07s
Test loss: 0.3167 score: 0.8958 time: 0.08s
     INFO: Early stopping counter 9 of 20
Epoch 92/1000, LR 0.000266
Train loss: 0.0054;  Loss pred: 0.0054; Loss self: 0.0000; time: 0.24s
Val loss: 0.5464 score: 0.7959 time: 0.07s
Test loss: 0.3408 score: 0.8958 time: 0.07s
     INFO: Early stopping counter 10 of 20
Epoch 93/1000, LR 0.000265
Train loss: 0.0046;  Loss pred: 0.0046; Loss self: 0.0000; time: 0.24s
Val loss: 0.5585 score: 0.7959 time: 0.07s
Test loss: 0.3540 score: 0.8958 time: 0.07s
     INFO: Early stopping counter 11 of 20
Epoch 94/1000, LR 0.000265
Train loss: 0.0051;  Loss pred: 0.0051; Loss self: 0.0000; time: 0.23s
Val loss: 0.5589 score: 0.7959 time: 0.07s
Test loss: 0.3516 score: 0.8958 time: 0.07s
     INFO: Early stopping counter 12 of 20
Epoch 95/1000, LR 0.000265
Train loss: 0.0052;  Loss pred: 0.0052; Loss self: 0.0000; time: 0.23s
Val loss: 0.5525 score: 0.7959 time: 0.07s
Test loss: 0.3386 score: 0.8958 time: 0.07s
     INFO: Early stopping counter 13 of 20
Epoch 96/1000, LR 0.000265
Train loss: 0.0050;  Loss pred: 0.0050; Loss self: 0.0000; time: 0.23s
Val loss: 0.5466 score: 0.7959 time: 0.07s
Test loss: 0.3231 score: 0.8958 time: 0.07s
     INFO: Early stopping counter 14 of 20
Epoch 97/1000, LR 0.000265
Train loss: 0.0051;  Loss pred: 0.0051; Loss self: 0.0000; time: 0.23s
Val loss: 0.5408 score: 0.8163 time: 0.07s
Test loss: 0.3010 score: 0.8958 time: 0.07s
     INFO: Early stopping counter 15 of 20
Epoch 98/1000, LR 0.000265
Train loss: 0.0051;  Loss pred: 0.0051; Loss self: 0.0000; time: 0.24s
Val loss: 0.5373 score: 0.8163 time: 0.07s
Test loss: 0.2839 score: 0.8958 time: 0.07s
     INFO: Early stopping counter 16 of 20
Epoch 99/1000, LR 0.000265
Train loss: 0.0041;  Loss pred: 0.0041; Loss self: 0.0000; time: 0.24s
Val loss: 0.5412 score: 0.7959 time: 0.07s
Test loss: 0.2788 score: 0.8958 time: 0.08s
     INFO: Early stopping counter 17 of 20
Epoch 100/1000, LR 0.000265
Train loss: 0.0043;  Loss pred: 0.0043; Loss self: 0.0000; time: 0.23s
Val loss: 0.5549 score: 0.7959 time: 0.07s
Test loss: 0.2855 score: 0.8958 time: 0.07s
     INFO: Early stopping counter 18 of 20
Epoch 101/1000, LR 0.000265
Train loss: 0.0043;  Loss pred: 0.0043; Loss self: 0.0000; time: 0.24s
Val loss: 0.5699 score: 0.7959 time: 0.07s
Test loss: 0.2979 score: 0.8958 time: 0.07s
     INFO: Early stopping counter 19 of 20
Epoch 102/1000, LR 0.000264
Train loss: 0.0038;  Loss pred: 0.0038; Loss self: 0.0000; time: 0.24s
Val loss: 0.5879 score: 0.7959 time: 0.07s
Test loss: 0.3150 score: 0.8750 time: 0.07s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 081,   Train_Loss: 0.0081,   Val_Loss: 0.3803,   Val_Precision: 0.8947,   Val_Recall: 0.6800,   Val_accuracy: 0.7727,   Val_Score: 0.7959,   Val_Loss: 0.3803,   Test_Precision: 1.0000,   Test_Recall: 0.8333,   Test_accuracy: 0.9091,   Test_Score: 0.9167,   Test_loss: 0.2168


[0.07314129103906453, 0.07222154701594263, 0.07299894897732884, 0.07240863796323538, 0.07285583496559411, 0.0788184500997886, 0.07466346502769738, 0.07542695908341557, 0.07441535301040858, 0.0720415289979428, 0.07173143897671252, 0.07184195599984378, 0.07173724297899753, 0.07131565199233592, 0.0717967189848423, 0.0780098260147497, 0.07316281704697758, 0.09810997196473181, 0.0721136819338426, 0.07010870601516217, 0.07171303406357765, 0.06965106190182269, 0.07069650501944125, 0.08222265599761158, 0.08552209904883057, 0.08889215905219316, 0.08861023001372814, 0.09154522290918976, 0.09074269398115575, 0.08526495203841478, 0.08531129197217524, 0.09617643803358078, 0.08584558998700231, 0.08588750392664224, 0.0895733799552545, 0.08164118195418268, 0.08298144699074328, 0.08585561602376401, 0.09049155307002366, 0.0862301510060206, 0.09385916090104729, 0.08607877104077488, 0.09078120905905962, 0.08924422005657107, 0.09407851693686098, 0.0901972169522196, 0.09502484695985913, 0.0849459971068427, 0.09032635693438351, 0.09491380909457803, 0.09442447207402438, 0.08968547102995217, 0.09094669797923416, 0.08980317995883524, 0.09560072107706219, 0.10805826599244028, 0.09357072203420103, 0.08598557289224118, 0.0919076050631702, 0.09525613207370043, 0.09006893599871546, 0.09434664598666131, 0.08430429501459002, 0.08796210307627916, 0.08305048604961485, 0.08686508203390986, 0.08129429002292454, 0.09215488890185952, 0.0880690929479897, 0.08759462798479944, 0.09166636003647, 0.0923796979477629, 0.08772476494777948, 0.08634085208177567, 0.087102364981547, 0.0867896870477125, 0.08639598602894694, 0.08840014389716089, 0.08589539001695812, 0.0850743199698627, 0.09069195401389152, 0.08597766095772386, 0.08676639595068991, 0.08632127603050321, 0.09098502702545375, 0.0947387219639495, 0.09339512197766453, 0.0859767870279029, 0.08696887898258865, 0.08703842002432793, 0.09140791895333678, 0.09169352101162076, 0.09051668201573193, 0.08554420701693743, 0.09229574992787093, 0.08625873702112585, 0.09044102998450398, 0.09331682790070772, 0.0880312870722264, 0.08733970101457089, 0.0872618219582364, 0.09076211601495743, 0.09122413001023233, 0.09593008202500641, 0.08574475604109466, 0.09043004002887756, 0.09183593397028744, 0.08685130707453936, 0.09181927295867354, 0.08600155403837562, 0.09501915995497257, 0.09728121501393616, 0.07690032897517085, 0.07608383195474744, 0.07510264904703945, 0.08346463995985687, 0.08178193599451333, 0.08069874497596174, 0.07753491401672363, 0.07744130003266037, 0.07706286199390888, 0.07752550300210714, 0.07858622493222356, 0.08596896496601403, 0.07760470698121935, 0.07769137702416629, 0.07733897201251239, 0.07730305497534573, 0.07644926593638957, 0.07647636509500444, 0.07696105900686234, 0.07595369406044483, 0.07655735092703253, 0.07676873996388167, 0.07714601699262857, 0.07553039095364511, 0.07542082702275366, 0.07672830496449023, 0.0780501359840855, 0.0772396510001272, 0.07775632094126195, 0.07615619094576687, 0.07746940501965582, 0.09226252499502152, 0.07626361097209156, 0.07884795998688787, 0.07937485096044838, 0.07984648796264082, 0.07611579401418567, 0.08357888797763735, 0.07609573903027922, 0.0762005050200969, 0.07677784701809287, 0.0763428290374577, 0.07623369398061186, 0.07603432098403573, 0.07729779998771846, 0.07740848790854216, 0.07666851195972413, 0.07594393007457256, 0.07567893399391323, 0.07622164499480277, 0.07620111107826233, 0.08281273790635169, 0.0803870459785685, 0.08553078002296388, 0.08034537790808827, 0.0794709250330925, 0.07957236201036721, 0.07913293107412755, 0.07952576701063663, 0.07968549302313477, 0.0796530960360542, 0.07974427205044776, 0.07979118498042226, 0.081200408982113, 0.0840540440985933, 0.08003448497038335, 0.07926786493044347, 0.07954974402673542, 0.07994778000283986, 0.08019115589559078, 0.07998033298645169, 0.07962015806697309, 0.08003891701810062, 0.07971988094504923, 0.08003642701078206, 0.07897267898079008, 0.08373524306807667, 0.07968832296319306, 0.07922255003359169, 0.07966389995999634, 0.07916748896241188, 0.07889097090810537, 0.07889061199966818, 0.0794491299893707, 0.07901676394976676, 0.07949920801911503, 0.08026221592444927, 0.0895009390078485, 0.08024621102958918, 0.08076971699483693, 0.08365561405662447, 0.07919312501326203, 0.07893998397048563, 0.07909983000718057, 0.07911574095487595, 0.07919833692722023, 0.0787106470670551, 0.07931089599151164, 0.07989372906740755, 0.07950015494134277, 0.0794503380311653, 0.0758347890805453]
[0.0014926794089605005, 0.0014739091227743395, 0.0014897744689250784, 0.0014777273053721506, 0.001486853774808043, 0.0016085397979548695, 0.0015237441842387222, 0.0015393256955799095, 0.0015186806736818077, 0.001470235285672302, 0.0014639069178920923, 0.0014661623673437505, 0.001464025366918317, 0.0014554214692313451, 0.0014652391629559653, 0.001592037265607137, 0.0014931187152444404, 0.0020022443258108534, 0.0014717077945682164, 0.001430789918676779, 0.001463531307419952, 0.0014214502428943406, 0.0014427858167232908, 0.0016780133877063589, 0.0017453489601802156, 0.0018141256949427177, 0.0018083720410964927, 0.0018682698552895869, 0.001851891713901138, 0.0017401010620084647, 0.0017410467749423518, 0.0019627844496649138, 0.0017519508160612717, 0.0017528062025845355, 0.0018280281623521326, 0.0016661465704935242, 0.0016934989181784342, 0.0017521554290564085, 0.0018467663891841562, 0.0017597990001228694, 0.00191549307961321, 0.0017567096130770383, 0.0018526777358991759, 0.0018213106133994094, 0.0019199697334053262, 0.001840759529637135, 0.0019392825910175334, 0.0017335917776906673, 0.0018433950394772145, 0.0019370165121342456, 0.001927030042327028, 0.0018303157353051463, 0.001856055060800697, 0.0018327179583435764, 0.0019510351240216773, 0.0022052707345395976, 0.0019096065721265515, 0.0017548076100457385, 0.001875665409452453, 0.0019440026953816414, 0.0018381415509941931, 0.0019254417548298227, 0.0017204958166242863, 0.0017951449607403911, 0.0016949078785635683, 0.001772756776202242, 0.0016590671433249907, 0.0018807120184052963, 0.001797328427509994, 0.0017876454690775396, 0.0018707420415606123, 0.0018852999581176104, 0.0017903013254648872, 0.0017620582057505238, 0.001777599285337694, 0.0017712181030145409, 0.0017631833883458559, 0.001804084569329814, 0.0017529671432032269, 0.0017362106116298511, 0.001850856204365133, 0.0017546461419943645, 0.0017707427745038758, 0.0017616586945000657, 0.0018568372862337499, 0.0019334433053867246, 0.0019060228975033577, 0.0017546283066918959, 0.0017748750812773193, 0.0017762942862107741, 0.0018654677337415668, 0.0018712963471759338, 0.0018472792248108558, 0.0017458001432028047, 0.0018835867332218557, 0.0017603823881862418, 0.0018457353058062038, 0.0019044250591981168, 0.0017965568790250287, 0.0017824428778483855, 0.0017808535093517632, 0.0018522880819379067, 0.0018617169389843332, 0.0019577567760205392, 0.0017498929804305033, 0.0018455110209975012, 0.0018742027340874988, 0.001772475654582436, 0.0018738627134423172, 0.0017551337558852167, 0.0019391665296933176, 0.0019853309186517584, 0.001602090186982726, 0.0015850798323905717, 0.0015646385218133219, 0.0017388466658303514, 0.0017037903332190278, 0.0016812238536658697, 0.0016153107086817424, 0.001613360417347091, 0.0016054762915397685, 0.001615114645877232, 0.0016372130194213241, 0.0017910201034586255, 0.0016167647287754032, 0.0016185703546701309, 0.0016112285835940081, 0.0016104803119863693, 0.0015926930403414492, 0.0015932576061459258, 0.0016033553959762987, 0.0015823686262592673, 0.0015949448109798443, 0.001599348749247535, 0.0016072086873464286, 0.0015735498115342732, 0.001571267229640701, 0.00159850635342688, 0.001626044499668448, 0.0016091593958359833, 0.0016199233529429573, 0.001586587311370143, 0.0016139459379094963, 0.0019221359373962816, 0.0015888252285852407, 0.001642665833060164, 0.0016536427283426747, 0.0016634684992216837, 0.001585745708628868, 0.0017412268328674447, 0.0015853278964641504, 0.0015875105212520186, 0.0015995384795436014, 0.0015904756049470354, 0.0015882019579294138, 0.0015840483538340777, 0.001610370833077468, 0.0016126768314279616, 0.001597260665827586, 0.0015821652098869283, 0.0015766444582065258, 0.0015879509373917244, 0.0015875231474637985, 0.0017252653730489935, 0.0016747301245535102, 0.0017818912504784141, 0.001673862039751839, 0.0016556442715227604, 0.0016577575418826502, 0.0016486027307109907, 0.0016567868127215963, 0.0016601144379819743, 0.0016594395007511291, 0.001661339001050995, 0.001662316353758797, 0.0016916751871273543, 0.0017511259187206936, 0.001667385103549653, 0.0016514138527175721, 0.0016572863338903214, 0.0016655787500591639, 0.0016706490811581414, 0.0016662569372177434, 0.0016587532930619393, 0.0016674774378770962, 0.001660830853021859, 0.0016674255627246264, 0.0016452641454331267, 0.0017444842305849306, 0.001660173395066522, 0.0016504697923664935, 0.0016596645824999239, 0.0016493226867169142, 0.0016435618939188619, 0.0016435544166597538, 0.0016551902081118897, 0.0016461825822868075, 0.0016562335003982298, 0.0016721294984260264, 0.0018646028959968437, 0.0016717960631164412, 0.001682702437392436, 0.0017428252928463432, 0.0016498567711096257, 0.0016445829993851173, 0.0016479131251495953, 0.0016482446032265823, 0.0016499653526504214, 0.0016398051472303148, 0.0016523103331564926, 0.0016644526889043239, 0.0016562532279446411, 0.0016552153756492771, 0.0015798914391780272]
[669.9362193898007, 678.467881464564, 671.2425409743619, 676.7148420175942, 672.5610930564458, 621.6818516218377, 656.2781406116474, 649.63509858339, 658.4662709743016, 680.1632430844052, 683.1035414737429, 682.0526991234279, 683.0482740233787, 687.0861954016193, 682.4824406021237, 628.1260003161054, 669.7391103535185, 499.4395474663302, 679.482709604993, 698.9146253733871, 683.278857739567, 703.5068620930491, 693.1035697807862, 595.9428019623127, 572.9513253881018, 551.2297206239481, 552.9835549733768, 535.2545817558017, 539.9883764766305, 574.6792653788608, 574.3671074162331, 509.4802947775135, 570.7922795733476, 570.5137273735608, 547.0375241447567, 600.1872930685762, 590.4934389185336, 570.7256236614419, 541.4870044509358, 568.2467144998831, 522.0587903151951, 569.246045308768, 539.7592795676694, 549.0551653534466, 520.8415438020289, 543.2540122158857, 515.6546057969325, 576.8370690659993, 542.4773196111015, 516.2578603412002, 518.933269349775, 546.353823392816, 538.7771198816713, 545.6376937037312, 512.5484352832642, 453.4590625711875, 523.6680762395957, 569.8630404126953, 533.1441284572825, 514.4025789551092, 544.0277433797911, 519.3613348685187, 581.2278009266298, 557.0580771301941, 590.0025674831946, 564.0931759078024, 602.748360139221, 531.7135160586284, 556.3813406019472, 559.3950351441999, 534.5472426362852, 530.4195736568394, 558.5651899913162, 567.5181425542434, 562.5564817945053, 564.5832087522372, 567.1559785611171, 554.2977402503268, 570.4613482787149, 575.9669900077731, 540.2904869873522, 569.9154810002779, 564.7347623825141, 567.646845056889, 538.5501505241283, 517.2119592097279, 524.6526688162404, 569.9212740306002, 563.4199333512153, 562.969778016468, 536.058588370382, 534.388901848256, 541.3366785968108, 572.8032523616495, 530.9020191969129, 568.0583983973621, 541.789495413703, 525.0928594801536, 556.6202838747242, 561.0277964178664, 561.5285000976883, 539.8728252647271, 537.1385837771631, 510.7886803143461, 571.4635187312902, 541.8553390483133, 533.560207661779, 564.1826433072123, 533.6570245122084, 569.7571462270928, 515.685468312075, 503.6943668207725, 624.1845859397815, 630.8830505349556, 639.1252586834307, 575.0938364208612, 586.9266778328686, 594.8047892727212, 619.075942866807, 619.8243053739583, 622.8681203637877, 619.1510940431481, 610.7940678076527, 558.3410247986092, 618.5191835285996, 617.829183090285, 620.6444015345104, 620.9327692845858, 627.8673759920588, 627.644893168902, 623.6920413961561, 631.963995876238, 626.9809419836015, 625.254498414109, 622.1967364120234, 635.5057797788813, 636.4289798296552, 625.5840008744406, 614.9893192983961, 621.442476480389, 617.3131575535804, 630.283623746128, 619.599440421948, 520.2545670909188, 629.3958466976532, 608.7665426978989, 604.7255449199882, 601.1535538351869, 630.6181341424916, 574.3077128860945, 630.784333153008, 629.9170850290378, 625.1803334455145, 628.742746439862, 629.642845487818, 631.2938601776709, 620.9749825690579, 620.0870382161685, 626.0718875725095, 632.0452464451967, 634.2584054349996, 629.742378339813, 629.9120750444389, 579.6209763560834, 597.1111317213596, 561.2014760898081, 597.4208006702014, 603.99447949061, 603.2245215210055, 606.5742712731837, 603.5779572371803, 602.3681121739983, 602.6131109614781, 601.9241102311935, 601.5702111928452, 591.1300275664072, 571.0611608847422, 599.7414741628229, 605.5417292003435, 603.3960333532682, 600.3919057951949, 598.5697482961363, 600.1475388722248, 602.8624052670738, 599.7082642827977, 602.1082750121807, 599.7269217619336, 607.8051374156357, 573.2353336691941, 602.3467205122455, 605.8880959985155, 602.5313852837164, 606.3094918014898, 608.4346465441764, 608.4374145836503, 604.1601715012083, 607.4660312654032, 603.7795997723491, 598.0398054943106, 536.3072223833404, 598.1590829540969, 594.2821367452399, 573.7809774190404, 606.1132199539002, 608.056875435222, 606.8281056437495, 606.706066588911, 606.0733326270459, 609.8285529162006, 605.2131854006196, 600.7980921694326, 603.7724081848094, 604.1509852503265, 632.9548823432272]
Elapsed: 0.08277632037686863~0.006909335199250617
Time per graph: 0.0017053187671293027~0.0001332347077665039
Speed: 589.955253318647~45.7444485948357
Total Time: 0.0766
best val loss: 0.3803059756755829 test_score: 0.9167

Testing...
Test loss: 0.2608 score: 0.9167 time: 0.07s
test Score 0.9167
Epoch Time List: [0.5524872150272131, 0.3827338709961623, 0.43712066288571805, 0.37491501995828, 0.3877625920576975, 0.39658519614022225, 0.3835222409106791, 0.4787430779542774, 0.40432003303430974, 0.38756635901518166, 0.3896690080873668, 0.3964784271083772, 0.39066879102028906, 0.3782911730231717, 0.38654209196101874, 0.3801778459455818, 0.40622406208422035, 0.4517095491755754, 0.42368095403071493, 0.3739383629290387, 0.3963481510290876, 0.3833882430335507, 0.38113143399823457, 0.3935784619534388, 0.38799365994054824, 0.39019095816183835, 0.3919076930033043, 0.39931677002459764, 0.4048062519868836, 0.38257004087790847, 0.3897449900396168, 0.41108149103820324, 0.38756345806177706, 0.37905153806786984, 0.39153553708456457, 0.3767952920170501, 0.4008101460058242, 0.40557922585867345, 0.3823446410242468, 0.3866366839502007, 0.3961808829335496, 0.40315365593414754, 0.39684697799384594, 0.38598406105302274, 0.4081406721379608, 0.4018410069402307, 0.4144832898164168, 0.40520863293204457, 0.41638357704505324, 0.41962715808767825, 0.42083221208304167, 0.4056714739417657, 0.41706359502859414, 0.42511676193680614, 0.4205594230443239, 0.45985579304397106, 0.42688670300412923, 0.39699597388971597, 0.4041867849882692, 0.4122521720128134, 0.4117704341188073, 0.4197946551721543, 0.40488042507786304, 0.39012844301760197, 0.3813110819319263, 0.39578869298566133, 0.3761612050002441, 0.3814686320256442, 0.3845104188658297, 0.39455840503796935, 0.3873056110460311, 0.3822082818951458, 0.38913308491464704, 0.3890795299084857, 0.3869929440552369, 0.3984713250538334, 0.3832340909866616, 0.38751981186214834, 0.37486649595666677, 0.37002919893711805, 0.37727653607726097, 0.3780271540163085, 0.37147138512227684, 0.38327878795098513, 0.39131463202647865, 0.41367714398074895, 0.3936540560098365, 0.37340995494741946, 0.37801564391702414, 0.3713088190415874, 0.38337311486247927, 0.3818680710392073, 0.3861600258387625, 0.37683647498488426, 0.3799786389572546, 0.3811250190483406, 0.3891338191460818, 0.38985948206391186, 0.38725017406977713, 0.3861908089602366, 0.3867626239079982, 0.3861309870844707, 0.39001622097566724, 0.4009402529336512, 0.3683553639566526, 0.38694693404249847, 0.3809621511027217, 0.37955844891257584, 0.4146873940480873, 0.3944033168954775, 0.4037863799603656, 0.4327106700511649, 0.3910117669729516, 0.36482108396012336, 0.3641116179060191, 0.38470466807484627, 0.40025809104554355, 0.38330436311662197, 0.3739487970015034, 0.37637579906731844, 0.37286115798633546, 0.37154985999222845, 0.376059130881913, 0.3840615740045905, 0.37403504201211035, 0.3719846890307963, 0.37300459388643503, 0.37391731597017497, 0.3702795711578801, 0.36937142407987267, 0.37280697806272656, 0.4331069180043414, 0.3687451609876007, 0.3716939410660416, 0.38269361306447536, 0.36257306998595595, 0.39186309988144785, 0.3678248819196597, 0.3675787760876119, 0.3737357771024108, 0.36335881194099784, 0.45759937912225723, 0.3717731339856982, 0.3897277609212324, 0.37433433008845896, 0.40104304894339293, 0.3653651149943471, 0.3748981519602239, 0.37380309810396284, 0.4006279589375481, 0.3725845619337633, 0.3629153298679739, 0.3629087620647624, 0.3609480259474367, 0.3613826660439372, 0.36130300792865455, 0.3624106409261003, 0.3623717810260132, 0.36179640714544803, 0.3604337479919195, 0.3584409049944952, 0.3592503860127181, 0.358671308029443, 0.3691488300682977, 0.38050072302576154, 0.38369243999477476, 0.3903907451312989, 0.3767319768667221, 0.38442824222147465, 0.3830940870102495, 0.3819315820001066, 0.3779299519956112, 0.3827216479694471, 0.38468665280379355, 0.3863052079686895, 0.39381811197381467, 0.41984771902207285, 0.38766895909793675, 0.3842752870405093, 0.3818921190686524, 0.3861782819731161, 0.38534931594040245, 0.3781569510465488, 0.37731969193555415, 0.38063754502218217, 0.3816818109480664, 0.38607310014776886, 0.37632489996030927, 0.3870146730914712, 0.39733427891042084, 0.3827252130722627, 0.3829761100932956, 0.3840729370713234, 0.38344564486760646, 0.37441759009379894, 0.37646262301132083, 0.3785234879469499, 0.3773490000749007, 0.3780637818854302, 0.40020987996831536, 0.3886560109676793, 0.38745970791205764, 0.3910025330260396, 0.38315313996281475, 0.38088146911468357, 0.37861707201227546, 0.3798770628636703, 0.37926033011171967, 0.37780862289946526, 0.38133729598484933, 0.3828005929244682, 0.3808204180095345, 0.3824459781171754, 0.37646472197957337]
Total Epoch List: [23, 89, 102]
Total Time List: [0.07104396796785295, 0.09791546897031367, 0.07658436405472457]
========================training times:1========================
========================k_idx:0========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: GPSPerformer
GPS(
  (x_lin): Linear(in_features=14887, out_features=64, bias=True)
  (pe_lin): Linear(in_features=20, out_features=8, bias=True)
  (pe_norm): BatchNorm1d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (convs): ModuleList(
    (0-1): 2 x GPSConv(64, conv=GINEConv(nn=Sequential(
      (0): Linear(in_features=64, out_features=64, bias=True)
      (1): ReLU()
      (2): Linear(in_features=64, out_features=64, bias=True)
    )), heads=1, attn_type=performer)
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=32, bias=True)
    (1): ReLU()
    (2): Linear(in_features=32, out_features=16, bias=True)
    (3): ReLU()
    (4): Linear(in_features=16, out_features=2, bias=True)
  )
)
Total number of parameters: 1039522
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7e7c5703b820>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 1.1360;  Loss pred: 1.1360; Loss self: 0.0000; time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 2.3119 score: 0.5102 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 1.6424 score: 0.4898 time: 0.07s
Epoch 2/1000, LR 0.000000
Train loss: 1.1312;  Loss pred: 1.1312; Loss self: 0.0000; time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 1.7278 score: 0.5102 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 1.2803 score: 0.4898 time: 0.07s
Epoch 3/1000, LR 0.000030
Train loss: 1.2496;  Loss pred: 1.2496; Loss self: 0.0000; time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 1.3363 score: 0.5102 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 1.0549 score: 0.4898 time: 0.07s
Epoch 4/1000, LR 0.000060
Train loss: 1.1341;  Loss pred: 1.1341; Loss self: 0.0000; time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 1.1237 score: 0.5102 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.9777 score: 0.4898 time: 0.07s
Epoch 5/1000, LR 0.000090
Train loss: 1.0806;  Loss pred: 1.0806; Loss self: 0.0000; time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 1.1233 score: 0.5102 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.9913 score: 0.4898 time: 0.07s
Epoch 6/1000, LR 0.000120
Train loss: 1.0418;  Loss pred: 1.0418; Loss self: 0.0000; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 1.1258 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.9844 score: 0.4898 time: 0.07s
     INFO: Early stopping counter 1 of 20
Epoch 7/1000, LR 0.000150
Train loss: 0.8692;  Loss pred: 0.8692; Loss self: 0.0000; time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 1.0618 score: 0.5102 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.9373 score: 0.4898 time: 0.07s
Epoch 8/1000, LR 0.000180
Train loss: 0.8487;  Loss pred: 0.8487; Loss self: 0.0000; time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 1.0197 score: 0.5102 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.9034 score: 0.4898 time: 0.07s
Epoch 9/1000, LR 0.000210
Train loss: 0.7881;  Loss pred: 0.7881; Loss self: 0.0000; time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 1.0098 score: 0.5102 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.8846 score: 0.4898 time: 0.07s
Epoch 10/1000, LR 0.000240
Train loss: 0.7024;  Loss pred: 0.7024; Loss self: 0.0000; time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.9721 score: 0.5102 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.8509 score: 0.4898 time: 0.07s
Epoch 11/1000, LR 0.000270
Train loss: 0.6909;  Loss pred: 0.6909; Loss self: 0.0000; time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.9280 score: 0.5102 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.8216 score: 0.4898 time: 0.07s
Epoch 12/1000, LR 0.000270
Train loss: 0.6566;  Loss pred: 0.6566; Loss self: 0.0000; time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.8555 score: 0.5102 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7814 score: 0.4898 time: 0.07s
Epoch 13/1000, LR 0.000270
Train loss: 0.6903;  Loss pred: 0.6903; Loss self: 0.0000; time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7874 score: 0.5102 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7416 score: 0.4898 time: 0.07s
Epoch 14/1000, LR 0.000270
Train loss: 0.5990;  Loss pred: 0.5990; Loss self: 0.0000; time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7398 score: 0.5102 time: 0.08s
Test loss: 0.7119 score: 0.5102 time: 0.07s
Epoch 15/1000, LR 0.000270
Train loss: 0.5387;  Loss pred: 0.5387; Loss self: 0.0000; time: 0.23s
Val loss: 0.7083 score: 0.5510 time: 0.07s
Test loss: 0.6923 score: 0.5102 time: 0.07s
Epoch 16/1000, LR 0.000270
Train loss: 0.4834;  Loss pred: 0.4834; Loss self: 0.0000; time: 0.22s
Val loss: 0.6817 score: 0.5510 time: 0.07s
Test loss: 0.6737 score: 0.5306 time: 0.07s
Epoch 17/1000, LR 0.000270
Train loss: 0.5023;  Loss pred: 0.5023; Loss self: 0.0000; time: 0.22s
Val loss: 0.6530 score: 0.5918 time: 0.07s
Test loss: 0.6541 score: 0.6122 time: 0.07s
Epoch 18/1000, LR 0.000270
Train loss: 0.4148;  Loss pred: 0.4148; Loss self: 0.0000; time: 0.23s
Val loss: 0.6277 score: 0.6939 time: 0.08s
Test loss: 0.6383 score: 0.6531 time: 0.07s
Epoch 19/1000, LR 0.000270
Train loss: 0.4037;  Loss pred: 0.4037; Loss self: 0.0000; time: 0.22s
Val loss: 0.6045 score: 0.7347 time: 0.08s
Test loss: 0.6253 score: 0.7143 time: 0.07s
Epoch 20/1000, LR 0.000270
Train loss: 0.4057;  Loss pred: 0.4057; Loss self: 0.0000; time: 0.23s
Val loss: 0.5892 score: 0.7959 time: 0.08s
Test loss: 0.6178 score: 0.7551 time: 0.07s
Epoch 21/1000, LR 0.000270
Train loss: 0.3237;  Loss pred: 0.3237; Loss self: 0.0000; time: 0.22s
Val loss: 0.5798 score: 0.8163 time: 0.08s
Test loss: 0.6138 score: 0.7551 time: 0.07s
Epoch 22/1000, LR 0.000270
Train loss: 0.3059;  Loss pred: 0.3059; Loss self: 0.0000; time: 0.23s
Val loss: 0.5768 score: 0.8163 time: 0.08s
Test loss: 0.6128 score: 0.7551 time: 0.07s
Epoch 23/1000, LR 0.000270
Train loss: 0.3148;  Loss pred: 0.3148; Loss self: 0.0000; time: 0.24s
Val loss: 0.5832 score: 0.7959 time: 0.09s
Test loss: 0.6195 score: 0.6939 time: 0.07s
     INFO: Early stopping counter 1 of 20
Epoch 24/1000, LR 0.000270
Train loss: 0.3059;  Loss pred: 0.3059; Loss self: 0.0000; time: 0.22s
Val loss: 0.5884 score: 0.7755 time: 0.08s
Test loss: 0.6248 score: 0.6531 time: 0.07s
     INFO: Early stopping counter 2 of 20
Epoch 25/1000, LR 0.000270
Train loss: 0.2967;  Loss pred: 0.2967; Loss self: 0.0000; time: 0.23s
Val loss: 0.5943 score: 0.7755 time: 0.09s
Test loss: 0.6299 score: 0.6327 time: 0.07s
     INFO: Early stopping counter 3 of 20
Epoch 26/1000, LR 0.000270
Train loss: 0.2684;  Loss pred: 0.2684; Loss self: 0.0000; time: 0.23s
Val loss: 0.6053 score: 0.7755 time: 0.09s
Test loss: 0.6374 score: 0.6327 time: 0.07s
     INFO: Early stopping counter 4 of 20
Epoch 27/1000, LR 0.000270
Train loss: 0.2454;  Loss pred: 0.2454; Loss self: 0.0000; time: 0.22s
Val loss: 0.6196 score: 0.6531 time: 0.08s
Test loss: 0.6475 score: 0.6327 time: 0.07s
     INFO: Early stopping counter 5 of 20
Epoch 28/1000, LR 0.000270
Train loss: 0.2377;  Loss pred: 0.2377; Loss self: 0.0000; time: 0.22s
Val loss: 0.6410 score: 0.6327 time: 0.07s
Test loss: 0.6595 score: 0.5918 time: 0.07s
     INFO: Early stopping counter 6 of 20
Epoch 29/1000, LR 0.000270
Train loss: 0.2430;  Loss pred: 0.2430; Loss self: 0.0000; time: 0.22s
Val loss: 0.6698 score: 0.5714 time: 0.08s
Test loss: 0.6718 score: 0.5714 time: 0.07s
     INFO: Early stopping counter 7 of 20
Epoch 30/1000, LR 0.000270
Train loss: 0.2078;  Loss pred: 0.2078; Loss self: 0.0000; time: 0.22s
Val loss: 0.7092 score: 0.5306 time: 0.08s
Test loss: 0.6877 score: 0.5510 time: 0.07s
     INFO: Early stopping counter 8 of 20
Epoch 31/1000, LR 0.000270
Train loss: 0.1926;  Loss pred: 0.1926; Loss self: 0.0000; time: 0.23s
Val loss: 0.7582 score: 0.5306 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7063 score: 0.4898 time: 0.07s
     INFO: Early stopping counter 9 of 20
Epoch 32/1000, LR 0.000270
Train loss: 0.2020;  Loss pred: 0.2020; Loss self: 0.0000; time: 0.22s
Val loss: 0.7969 score: 0.5306 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7170 score: 0.4898 time: 0.07s
     INFO: Early stopping counter 10 of 20
Epoch 33/1000, LR 0.000270
Train loss: 0.1734;  Loss pred: 0.1734; Loss self: 0.0000; time: 0.22s
Val loss: 0.8192 score: 0.5306 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7226 score: 0.4898 time: 0.07s
     INFO: Early stopping counter 11 of 20
Epoch 34/1000, LR 0.000270
Train loss: 0.1886;  Loss pred: 0.1886; Loss self: 0.0000; time: 0.23s
Val loss: 0.8394 score: 0.5306 time: 0.08s
Test loss: 0.7278 score: 0.5102 time: 0.07s
     INFO: Early stopping counter 12 of 20
Epoch 35/1000, LR 0.000270
Train loss: 0.1576;  Loss pred: 0.1576; Loss self: 0.0000; time: 0.22s
Val loss: 0.8504 score: 0.4898 time: 0.09s
Test loss: 0.7286 score: 0.5102 time: 0.07s
     INFO: Early stopping counter 13 of 20
Epoch 36/1000, LR 0.000270
Train loss: 0.1755;  Loss pred: 0.1755; Loss self: 0.0000; time: 0.23s
Val loss: 0.8657 score: 0.4898 time: 0.09s
Test loss: 0.7308 score: 0.5306 time: 0.07s
     INFO: Early stopping counter 14 of 20
Epoch 37/1000, LR 0.000270
Train loss: 0.1254;  Loss pred: 0.1254; Loss self: 0.0000; time: 0.22s
Val loss: 0.8649 score: 0.4898 time: 0.08s
Test loss: 0.7269 score: 0.5306 time: 0.07s
     INFO: Early stopping counter 15 of 20
Epoch 38/1000, LR 0.000270
Train loss: 0.1344;  Loss pred: 0.1344; Loss self: 0.0000; time: 0.22s
Val loss: 0.8376 score: 0.4898 time: 0.08s
Test loss: 0.7147 score: 0.6122 time: 0.07s
     INFO: Early stopping counter 16 of 20
Epoch 39/1000, LR 0.000269
Train loss: 0.1368;  Loss pred: 0.1368; Loss self: 0.0000; time: 0.22s
Val loss: 0.7730 score: 0.5510 time: 0.08s
Test loss: 0.6882 score: 0.6735 time: 0.07s
     INFO: Early stopping counter 17 of 20
Epoch 40/1000, LR 0.000269
Train loss: 0.1408;  Loss pred: 0.1408; Loss self: 0.0000; time: 0.22s
Val loss: 0.6301 score: 0.7143 time: 0.08s
Test loss: 0.6277 score: 0.6939 time: 0.07s
     INFO: Early stopping counter 18 of 20
Epoch 41/1000, LR 0.000269
Train loss: 0.1206;  Loss pred: 0.1206; Loss self: 0.0000; time: 0.22s
Val loss: 0.5684 score: 0.7959 time: 0.08s
Test loss: 0.5982 score: 0.7143 time: 0.07s
Epoch 42/1000, LR 0.000269
Train loss: 0.1145;  Loss pred: 0.1145; Loss self: 0.0000; time: 0.22s
Val loss: 0.5338 score: 0.8571 time: 0.07s
Test loss: 0.5799 score: 0.7347 time: 0.07s
Epoch 43/1000, LR 0.000269
Train loss: 0.1148;  Loss pred: 0.1148; Loss self: 0.0000; time: 0.24s
Val loss: 0.5122 score: 0.8571 time: 0.08s
Test loss: 0.5672 score: 0.7959 time: 0.07s
Epoch 44/1000, LR 0.000269
Train loss: 0.1139;  Loss pred: 0.1139; Loss self: 0.0000; time: 0.22s
Val loss: 0.5045 score: 0.8571 time: 0.08s
Test loss: 0.5581 score: 0.8163 time: 0.07s
Epoch 45/1000, LR 0.000269
Train loss: 0.1016;  Loss pred: 0.1016; Loss self: 0.0000; time: 0.22s
Val loss: 0.4983 score: 0.8571 time: 0.08s
Test loss: 0.5505 score: 0.8163 time: 0.07s
Epoch 46/1000, LR 0.000269
Train loss: 0.1089;  Loss pred: 0.1089; Loss self: 0.0000; time: 0.22s
Val loss: 0.4962 score: 0.8571 time: 0.07s
Test loss: 0.5449 score: 0.7959 time: 0.07s
Epoch 47/1000, LR 0.000269
Train loss: 0.0983;  Loss pred: 0.0983; Loss self: 0.0000; time: 0.22s
Val loss: 0.4962 score: 0.8571 time: 0.08s
Test loss: 0.5406 score: 0.7959 time: 0.07s
     INFO: Early stopping counter 1 of 20
Epoch 48/1000, LR 0.000269
Train loss: 0.0902;  Loss pred: 0.0902; Loss self: 0.0000; time: 0.22s
Val loss: 0.4953 score: 0.8367 time: 0.08s
Test loss: 0.5357 score: 0.7755 time: 0.07s
Epoch 49/1000, LR 0.000269
Train loss: 0.0943;  Loss pred: 0.0943; Loss self: 0.0000; time: 0.22s
Val loss: 0.4985 score: 0.8163 time: 0.09s
Test loss: 0.5323 score: 0.7959 time: 0.07s
     INFO: Early stopping counter 1 of 20
Epoch 50/1000, LR 0.000269
Train loss: 0.0866;  Loss pred: 0.0866; Loss self: 0.0000; time: 0.24s
Val loss: 0.5080 score: 0.8367 time: 0.08s
Test loss: 0.5322 score: 0.7755 time: 0.07s
     INFO: Early stopping counter 2 of 20
Epoch 51/1000, LR 0.000269
Train loss: 0.0794;  Loss pred: 0.0794; Loss self: 0.0000; time: 0.22s
Val loss: 0.5241 score: 0.7959 time: 0.08s
Test loss: 0.5361 score: 0.7755 time: 0.07s
     INFO: Early stopping counter 3 of 20
Epoch 52/1000, LR 0.000269
Train loss: 0.0725;  Loss pred: 0.0725; Loss self: 0.0000; time: 0.22s
Val loss: 0.5478 score: 0.7347 time: 0.08s
Test loss: 0.5453 score: 0.7755 time: 0.07s
     INFO: Early stopping counter 4 of 20
Epoch 53/1000, LR 0.000269
Train loss: 0.0722;  Loss pred: 0.0722; Loss self: 0.0000; time: 0.22s
Val loss: 0.5735 score: 0.6939 time: 0.08s
Test loss: 0.5563 score: 0.7347 time: 0.07s
     INFO: Early stopping counter 5 of 20
Epoch 54/1000, LR 0.000269
Train loss: 0.0647;  Loss pred: 0.0647; Loss self: 0.0000; time: 0.23s
Val loss: 0.5849 score: 0.6939 time: 0.08s
Test loss: 0.5640 score: 0.7551 time: 0.07s
     INFO: Early stopping counter 6 of 20
Epoch 55/1000, LR 0.000269
Train loss: 0.0691;  Loss pred: 0.0691; Loss self: 0.0000; time: 0.22s
Val loss: 0.5736 score: 0.7143 time: 0.07s
Test loss: 0.5617 score: 0.7551 time: 0.07s
     INFO: Early stopping counter 7 of 20
Epoch 56/1000, LR 0.000269
Train loss: 0.0586;  Loss pred: 0.0586; Loss self: 0.0000; time: 0.23s
Val loss: 0.5518 score: 0.7551 time: 0.09s
Test loss: 0.5506 score: 0.7551 time: 0.07s
     INFO: Early stopping counter 8 of 20
Epoch 57/1000, LR 0.000269
Train loss: 0.0617;  Loss pred: 0.0617; Loss self: 0.0000; time: 0.23s
Val loss: 0.4940 score: 0.7959 time: 0.08s
Test loss: 0.5163 score: 0.7755 time: 0.07s
Epoch 58/1000, LR 0.000269
Train loss: 0.0560;  Loss pred: 0.0560; Loss self: 0.0000; time: 0.23s
Val loss: 0.4554 score: 0.8367 time: 0.08s
Test loss: 0.4920 score: 0.7959 time: 0.07s
Epoch 59/1000, LR 0.000268
Train loss: 0.0503;  Loss pred: 0.0503; Loss self: 0.0000; time: 0.23s
Val loss: 0.4296 score: 0.8367 time: 0.08s
Test loss: 0.4757 score: 0.8163 time: 0.07s
Epoch 60/1000, LR 0.000268
Train loss: 0.0541;  Loss pred: 0.0541; Loss self: 0.0000; time: 0.23s
Val loss: 0.4112 score: 0.8776 time: 0.08s
Test loss: 0.4648 score: 0.8367 time: 0.07s
Epoch 61/1000, LR 0.000268
Train loss: 0.0490;  Loss pred: 0.0490; Loss self: 0.0000; time: 0.22s
Val loss: 0.3964 score: 0.8776 time: 0.08s
Test loss: 0.4551 score: 0.8367 time: 0.07s
Epoch 62/1000, LR 0.000268
Train loss: 0.0453;  Loss pred: 0.0453; Loss self: 0.0000; time: 0.23s
Val loss: 0.3846 score: 0.8980 time: 0.08s
Test loss: 0.4469 score: 0.8367 time: 0.07s
Epoch 63/1000, LR 0.000268
Train loss: 0.0457;  Loss pred: 0.0457; Loss self: 0.0000; time: 0.23s
Val loss: 0.3753 score: 0.8980 time: 0.09s
Test loss: 0.4401 score: 0.8367 time: 0.07s
Epoch 64/1000, LR 0.000268
Train loss: 0.0471;  Loss pred: 0.0471; Loss self: 0.0000; time: 0.22s
Val loss: 0.3668 score: 0.8980 time: 0.08s
Test loss: 0.4343 score: 0.8571 time: 0.07s
Epoch 65/1000, LR 0.000268
Train loss: 0.0440;  Loss pred: 0.0440; Loss self: 0.0000; time: 0.24s
Val loss: 0.3589 score: 0.8776 time: 0.08s
Test loss: 0.4295 score: 0.8571 time: 0.07s
Epoch 66/1000, LR 0.000268
Train loss: 0.0349;  Loss pred: 0.0349; Loss self: 0.0000; time: 0.23s
Val loss: 0.3523 score: 0.8776 time: 0.08s
Test loss: 0.4253 score: 0.8571 time: 0.07s
Epoch 67/1000, LR 0.000268
Train loss: 0.0364;  Loss pred: 0.0364; Loss self: 0.0000; time: 0.23s
Val loss: 0.3457 score: 0.8776 time: 0.17s
Test loss: 0.4213 score: 0.8571 time: 0.07s
Epoch 68/1000, LR 0.000268
Train loss: 0.0389;  Loss pred: 0.0389; Loss self: 0.0000; time: 0.23s
Val loss: 0.3389 score: 0.8776 time: 0.08s
Test loss: 0.4172 score: 0.8571 time: 0.07s
Epoch 69/1000, LR 0.000268
Train loss: 0.0303;  Loss pred: 0.0303; Loss self: 0.0000; time: 0.23s
Val loss: 0.3308 score: 0.8571 time: 0.08s
Test loss: 0.4137 score: 0.8571 time: 0.07s
Epoch 70/1000, LR 0.000268
Train loss: 0.0308;  Loss pred: 0.0308; Loss self: 0.0000; time: 0.23s
Val loss: 0.3224 score: 0.8980 time: 0.08s
Test loss: 0.4102 score: 0.8571 time: 0.07s
Epoch 71/1000, LR 0.000268
Train loss: 0.0325;  Loss pred: 0.0325; Loss self: 0.0000; time: 0.23s
Val loss: 0.3092 score: 0.8980 time: 0.10s
Test loss: 0.4045 score: 0.8571 time: 0.08s
Epoch 72/1000, LR 0.000267
Train loss: 0.0291;  Loss pred: 0.0291; Loss self: 0.0000; time: 0.23s
Val loss: 0.2963 score: 0.9184 time: 0.09s
Test loss: 0.3990 score: 0.8571 time: 0.07s
Epoch 73/1000, LR 0.000267
Train loss: 0.0283;  Loss pred: 0.0283; Loss self: 0.0000; time: 0.24s
Val loss: 0.2838 score: 0.9184 time: 0.08s
Test loss: 0.3943 score: 0.8571 time: 0.07s
Epoch 74/1000, LR 0.000267
Train loss: 0.0295;  Loss pred: 0.0295; Loss self: 0.0000; time: 0.23s
Val loss: 0.2740 score: 0.9184 time: 0.09s
Test loss: 0.3901 score: 0.8571 time: 0.07s
Epoch 75/1000, LR 0.000267
Train loss: 0.0259;  Loss pred: 0.0259; Loss self: 0.0000; time: 0.23s
Val loss: 0.2630 score: 0.9388 time: 0.08s
Test loss: 0.3865 score: 0.8571 time: 0.17s
Epoch 76/1000, LR 0.000267
Train loss: 0.0233;  Loss pred: 0.0233; Loss self: 0.0000; time: 0.26s
Val loss: 0.2528 score: 0.9388 time: 0.09s
Test loss: 0.3835 score: 0.8571 time: 0.07s
Epoch 77/1000, LR 0.000267
Train loss: 0.0258;  Loss pred: 0.0258; Loss self: 0.0000; time: 0.25s
Val loss: 0.2424 score: 0.9388 time: 0.08s
Test loss: 0.3808 score: 0.8571 time: 0.07s
Epoch 78/1000, LR 0.000267
Train loss: 0.0246;  Loss pred: 0.0246; Loss self: 0.0000; time: 0.24s
Val loss: 0.2313 score: 0.9184 time: 0.09s
Test loss: 0.3783 score: 0.8571 time: 0.07s
Epoch 79/1000, LR 0.000267
Train loss: 0.0212;  Loss pred: 0.0212; Loss self: 0.0000; time: 0.25s
Val loss: 0.2215 score: 0.9184 time: 0.09s
Test loss: 0.3764 score: 0.8571 time: 0.07s
Epoch 80/1000, LR 0.000267
Train loss: 0.0203;  Loss pred: 0.0203; Loss self: 0.0000; time: 0.33s
Val loss: 0.2115 score: 0.9184 time: 0.09s
Test loss: 0.3730 score: 0.8571 time: 0.07s
Epoch 81/1000, LR 0.000267
Train loss: 0.0182;  Loss pred: 0.0182; Loss self: 0.0000; time: 0.24s
Val loss: 0.2018 score: 0.9184 time: 0.09s
Test loss: 0.3685 score: 0.8571 time: 0.07s
Epoch 82/1000, LR 0.000267
Train loss: 0.0195;  Loss pred: 0.0195; Loss self: 0.0000; time: 0.26s
Val loss: 0.1943 score: 0.9184 time: 0.09s
Test loss: 0.3631 score: 0.8571 time: 0.07s
Epoch 83/1000, LR 0.000266
Train loss: 0.0197;  Loss pred: 0.0197; Loss self: 0.0000; time: 0.25s
Val loss: 0.1889 score: 0.9184 time: 0.09s
Test loss: 0.3604 score: 0.8571 time: 0.07s
Epoch 84/1000, LR 0.000266
Train loss: 0.0171;  Loss pred: 0.0171; Loss self: 0.0000; time: 0.24s
Val loss: 0.1843 score: 0.9184 time: 0.08s
Test loss: 0.3581 score: 0.8571 time: 0.07s
Epoch 85/1000, LR 0.000266
Train loss: 0.0183;  Loss pred: 0.0183; Loss self: 0.0000; time: 0.23s
Val loss: 0.1822 score: 0.8980 time: 0.08s
Test loss: 0.3570 score: 0.8571 time: 0.07s
Epoch 86/1000, LR 0.000266
Train loss: 0.0143;  Loss pred: 0.0143; Loss self: 0.0000; time: 0.24s
Val loss: 0.1824 score: 0.8980 time: 0.08s
Test loss: 0.3556 score: 0.8571 time: 0.07s
     INFO: Early stopping counter 1 of 20
Epoch 87/1000, LR 0.000266
Train loss: 0.0165;  Loss pred: 0.0165; Loss self: 0.0000; time: 0.24s
Val loss: 0.1873 score: 0.9184 time: 0.08s
Test loss: 0.3549 score: 0.8980 time: 0.07s
     INFO: Early stopping counter 2 of 20
Epoch 88/1000, LR 0.000266
Train loss: 0.0150;  Loss pred: 0.0150; Loss self: 0.0000; time: 0.24s
Val loss: 0.1879 score: 0.9184 time: 0.08s
Test loss: 0.3524 score: 0.8367 time: 0.07s
     INFO: Early stopping counter 3 of 20
Epoch 89/1000, LR 0.000266
Train loss: 0.0151;  Loss pred: 0.0151; Loss self: 0.0000; time: 0.23s
Val loss: 0.1853 score: 0.8980 time: 0.08s
Test loss: 0.3500 score: 0.8776 time: 0.07s
     INFO: Early stopping counter 4 of 20
Epoch 90/1000, LR 0.000266
Train loss: 0.0135;  Loss pred: 0.0135; Loss self: 0.0000; time: 0.24s
Val loss: 0.1795 score: 0.9184 time: 0.09s
Test loss: 0.3462 score: 0.9184 time: 0.07s
Epoch 91/1000, LR 0.000266
Train loss: 0.0149;  Loss pred: 0.0149; Loss self: 0.0000; time: 0.23s
Val loss: 0.1606 score: 0.9184 time: 0.08s
Test loss: 0.3385 score: 0.8776 time: 0.07s
Epoch 92/1000, LR 0.000266
Train loss: 0.0107;  Loss pred: 0.0107; Loss self: 0.0000; time: 0.23s
Val loss: 0.1534 score: 0.9184 time: 0.09s
Test loss: 0.3371 score: 0.8776 time: 0.07s
Epoch 93/1000, LR 0.000265
Train loss: 0.0123;  Loss pred: 0.0123; Loss self: 0.0000; time: 0.23s
Val loss: 0.1490 score: 0.9184 time: 0.08s
Test loss: 0.3380 score: 0.8776 time: 0.07s
Epoch 94/1000, LR 0.000265
Train loss: 0.0122;  Loss pred: 0.0122; Loss self: 0.0000; time: 0.23s
Val loss: 0.1475 score: 0.9184 time: 0.08s
Test loss: 0.3371 score: 0.8776 time: 0.07s
Epoch 95/1000, LR 0.000265
Train loss: 0.0113;  Loss pred: 0.0113; Loss self: 0.0000; time: 0.23s
Val loss: 0.1505 score: 0.9184 time: 0.08s
Test loss: 0.3343 score: 0.8776 time: 0.07s
     INFO: Early stopping counter 1 of 20
Epoch 96/1000, LR 0.000265
Train loss: 0.0104;  Loss pred: 0.0104; Loss self: 0.0000; time: 0.23s
Val loss: 0.1566 score: 0.9184 time: 0.08s
Test loss: 0.3312 score: 0.8776 time: 0.07s
     INFO: Early stopping counter 2 of 20
Epoch 97/1000, LR 0.000265
Train loss: 0.0091;  Loss pred: 0.0091; Loss self: 0.0000; time: 0.24s
Val loss: 0.1652 score: 0.9184 time: 0.09s
Test loss: 0.3320 score: 0.9184 time: 0.07s
     INFO: Early stopping counter 3 of 20
Epoch 98/1000, LR 0.000265
Train loss: 0.0094;  Loss pred: 0.0094; Loss self: 0.0000; time: 0.24s
Val loss: 0.1775 score: 0.9184 time: 0.08s
Test loss: 0.3408 score: 0.8980 time: 0.07s
     INFO: Early stopping counter 4 of 20
Epoch 99/1000, LR 0.000265
Train loss: 0.0098;  Loss pred: 0.0098; Loss self: 0.0000; time: 0.23s
Val loss: 0.1969 score: 0.9184 time: 0.08s
Test loss: 0.3668 score: 0.8571 time: 0.07s
     INFO: Early stopping counter 5 of 20
Epoch 100/1000, LR 0.000265
Train loss: 0.0092;  Loss pred: 0.0092; Loss self: 0.0000; time: 0.23s
Val loss: 0.2214 score: 0.9184 time: 0.08s
Test loss: 0.4032 score: 0.8367 time: 0.07s
     INFO: Early stopping counter 6 of 20
Epoch 101/1000, LR 0.000265
Train loss: 0.0092;  Loss pred: 0.0092; Loss self: 0.0000; time: 0.23s
Val loss: 0.2596 score: 0.8776 time: 0.08s
Test loss: 0.4516 score: 0.8367 time: 0.07s
     INFO: Early stopping counter 7 of 20
Epoch 102/1000, LR 0.000264
Train loss: 0.0083;  Loss pred: 0.0083; Loss self: 0.0000; time: 0.24s
Val loss: 0.3023 score: 0.8571 time: 0.08s
Test loss: 0.5022 score: 0.8367 time: 0.07s
     INFO: Early stopping counter 8 of 20
Epoch 103/1000, LR 0.000264
Train loss: 0.0068;  Loss pred: 0.0068; Loss self: 0.0000; time: 0.23s
Val loss: 0.3595 score: 0.8571 time: 0.08s
Test loss: 0.5545 score: 0.8571 time: 0.07s
     INFO: Early stopping counter 9 of 20
Epoch 104/1000, LR 0.000264
Train loss: 0.0069;  Loss pred: 0.0069; Loss self: 0.0000; time: 0.23s
Val loss: 0.4101 score: 0.8571 time: 0.08s
Test loss: 0.6054 score: 0.8163 time: 0.07s
     INFO: Early stopping counter 10 of 20
Epoch 105/1000, LR 0.000264
Train loss: 0.0070;  Loss pred: 0.0070; Loss self: 0.0000; time: 0.23s
Val loss: 0.4356 score: 0.8571 time: 0.08s
Test loss: 0.6355 score: 0.8163 time: 0.07s
     INFO: Early stopping counter 11 of 20
Epoch 106/1000, LR 0.000264
Train loss: 0.0059;  Loss pred: 0.0059; Loss self: 0.0000; time: 0.23s
Val loss: 0.4456 score: 0.8571 time: 0.09s
Test loss: 0.6518 score: 0.7959 time: 0.07s
     INFO: Early stopping counter 12 of 20
Epoch 107/1000, LR 0.000264
Train loss: 0.0064;  Loss pred: 0.0064; Loss self: 0.0000; time: 0.23s
Val loss: 0.4252 score: 0.8571 time: 0.08s
Test loss: 0.6364 score: 0.7959 time: 0.07s
     INFO: Early stopping counter 13 of 20
Epoch 108/1000, LR 0.000264
Train loss: 0.0076;  Loss pred: 0.0076; Loss self: 0.0000; time: 0.23s
Val loss: 0.3980 score: 0.8571 time: 0.08s
Test loss: 0.6051 score: 0.8367 time: 0.07s
     INFO: Early stopping counter 14 of 20
Epoch 109/1000, LR 0.000264
Train loss: 0.0067;  Loss pred: 0.0067; Loss self: 0.0000; time: 0.23s
Val loss: 0.3657 score: 0.8571 time: 0.08s
Test loss: 0.5748 score: 0.8571 time: 0.07s
     INFO: Early stopping counter 15 of 20
Epoch 110/1000, LR 0.000263
Train loss: 0.0056;  Loss pred: 0.0056; Loss self: 0.0000; time: 0.23s
Val loss: 0.3305 score: 0.8571 time: 0.08s
Test loss: 0.5434 score: 0.8367 time: 0.07s
     INFO: Early stopping counter 16 of 20
Epoch 111/1000, LR 0.000263
Train loss: 0.0059;  Loss pred: 0.0059; Loss self: 0.0000; time: 0.23s
Val loss: 0.3002 score: 0.8571 time: 0.09s
Test loss: 0.5178 score: 0.8367 time: 0.07s
     INFO: Early stopping counter 17 of 20
Epoch 112/1000, LR 0.000263
Train loss: 0.0054;  Loss pred: 0.0054; Loss self: 0.0000; time: 0.23s
Val loss: 0.2759 score: 0.8776 time: 0.08s
Test loss: 0.4982 score: 0.8367 time: 0.07s
     INFO: Early stopping counter 18 of 20
Epoch 113/1000, LR 0.000263
Train loss: 0.0052;  Loss pred: 0.0052; Loss self: 0.0000; time: 0.23s
Val loss: 0.2617 score: 0.8776 time: 0.08s
Test loss: 0.4803 score: 0.8367 time: 0.07s
     INFO: Early stopping counter 19 of 20
Epoch 114/1000, LR 0.000263
Train loss: 0.0055;  Loss pred: 0.0055; Loss self: 0.0000; time: 0.23s
Val loss: 0.2467 score: 0.8776 time: 0.08s
Test loss: 0.4568 score: 0.8367 time: 0.07s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 093,   Train_Loss: 0.0122,   Val_Loss: 0.1475,   Val_Precision: 0.9545,   Val_Recall: 0.8750,   Val_accuracy: 0.9130,   Val_Score: 0.9184,   Val_Loss: 0.1475,   Test_Precision: 0.8519,   Test_Recall: 0.9200,   Test_accuracy: 0.8846,   Test_Score: 0.8776,   Test_loss: 0.3371


[0.07090797193814069, 0.07478468609042466, 0.07500295399222523, 0.07073386700358242, 0.07147020800039172, 0.07484937901608646, 0.07126277300994843, 0.07091055996716022, 0.07253571297042072, 0.07160807703621686, 0.07112984103150666, 0.07123727397993207, 0.07121164095588028, 0.0712842799257487, 0.07024876901414245, 0.07034641597419977, 0.07005608198232949, 0.07109783298801631, 0.07092070404905826, 0.0712843060027808, 0.07131313404534012, 0.0718663620064035, 0.07100645697209984, 0.07151592103764415, 0.07765654497779906, 0.07291227101814002, 0.07112269895151258, 0.07108279899694026, 0.07178047299385071, 0.07115971902385354, 0.07150786696001887, 0.07094577106181532, 0.07115589594468474, 0.07083884405437857, 0.07096579298377037, 0.07502702297642827, 0.07140305102802813, 0.07178505603224039, 0.07167405402287841, 0.07145116897299886, 0.0722004669951275, 0.07076698099263012, 0.07190200395416468, 0.07137341995257884, 0.07180558203253895, 0.07747737294994295, 0.07223098701797426, 0.07189775398001075, 0.0717406690819189, 0.07102096604648978, 0.07139467296656221, 0.07099431299138814, 0.07107717299368232, 0.0706796069862321, 0.07016047206707299, 0.07091534102801234, 0.07157608598936349, 0.07048087299335748, 0.07092845602892339, 0.0702437279978767, 0.07045311003457755, 0.07111681206151843, 0.07129402400460094, 0.07105314393993467, 0.07075407507363707, 0.07076400797814131, 0.06998341006692499, 0.07011971296742558, 0.07016182201914489, 0.07048073399346322, 0.08782673696987331, 0.07231053803116083, 0.0724047739058733, 0.07273302599787712, 0.17593855503946543, 0.07614254206418991, 0.07665180193725973, 0.07801140903029591, 0.07973117695655674, 0.0759186880895868, 0.07701897202059627, 0.07802655303385109, 0.0763513109413907, 0.0727422129129991, 0.07103690505027771, 0.07228322105947882, 0.07546074304264039, 0.07256152993068099, 0.07305358001030982, 0.07290938508231193, 0.07253968110308051, 0.07243451103568077, 0.07161806209478527, 0.07154188491404057, 0.07252215396147221, 0.07172629598062485, 0.07195148302707821, 0.07302372204139829, 0.07268997200299054, 0.07307923794724047, 0.07110046199522913, 0.07179522595833987, 0.07195230794604868, 0.07145465794019401, 0.07174865598790348, 0.07181940204463899, 0.07156928698532283, 0.0713540849974379, 0.07209255499765277, 0.07278985495213419, 0.07229635899420828, 0.07182803004980087, 0.07231369498185813, 0.07244545791763812]
[0.0014471014681253203, 0.0015262180834780543, 0.001530672530453576, 0.0014435483061955596, 0.001458575673477382, 0.0015275383472670705, 0.0014543423063254782, 0.0014471542850440862, 0.0014803206728657289, 0.0014613893272697317, 0.0014516294088062582, 0.0014538219179577973, 0.001453298795017965, 0.0014547812229744634, 0.001433648347227397, 0.0014356411423306077, 0.0014297159588230507, 0.0014509761834289043, 0.001447361307123638, 0.0014547817551587917, 0.0014553700825579617, 0.0014666604491102757, 0.0014491113667775477, 0.0014595085926049826, 0.0015848274485265113, 0.0014880055309824493, 0.0014514836520716852, 0.001450669367284495, 0.0014649076121194021, 0.0014522391637521131, 0.0014593442236738546, 0.0014478728788125577, 0.00145216114172826, 0.0014456906949873178, 0.0014482814894647015, 0.001531163734212822, 0.0014572051230209823, 0.00146500114351511, 0.0014627357963852736, 0.0014581871218979359, 0.0014734789182679082, 0.0014442241018904106, 0.0014673878357992793, 0.0014566004071954865, 0.0014654200414803867, 0.0015811708765294478, 0.0014741017758770256, 0.0014673011016328723, 0.0014640952873860998, 0.001449407470336526, 0.001457034142174739, 0.0014488635304364928, 0.001450554550891476, 0.0014424409589026959, 0.0014318463687157752, 0.0014472518577145375, 0.0014607364487625202, 0.0014383851631297444, 0.001447519510794355, 0.0014335454693444225, 0.0014378185721342356, 0.0014513635114595598, 0.0014549800817265498, 0.001450064162039483, 0.0014439607157885116, 0.0014441634281253328, 0.0014282328585086732, 0.0014310145503556241, 0.001431873918758059, 0.0014383823263972085, 0.0017923823871402716, 0.0014757252659420579, 0.0014776484470586388, 0.001484347469344431, 0.003590582755907458, 0.0015539294298814268, 0.0015643224885155047, 0.0015920695720468552, 0.0016271668766644231, 0.0015493609814201386, 0.0015718157555223728, 0.0015923786333438997, 0.001558190019212055, 0.0014845349574081448, 0.0014497327561281165, 0.0014751677767240576, 0.0015400151641355182, 0.0014808475496057346, 0.0014908893879655065, 0.0014879466343328965, 0.0014804016551649083, 0.0014782553272587912, 0.0014615931039752097, 0.0014600384676334809, 0.0014800439583973922, 0.0014638019587882624, 0.0014683976127975145, 0.0014902800416611895, 0.001483468816387562, 0.0014914130193314382, 0.001451029836637329, 0.0014652086930273442, 0.0014684144478785445, 0.0014582583253100819, 0.001464258285467418, 0.0014657020825436528, 0.001460597693578017, 0.0014562058162742428, 0.0014712766326051585, 0.001485507243921106, 0.0014754358978409851, 0.0014658781642816505, 0.0014757896935073088, 0.001478478733013023]
[691.0365458307995, 655.2143568638165, 653.3076017923149, 692.7374689908912, 685.6003553219186, 654.6480497783292, 687.596032688196, 691.0113250084706, 675.5293081627484, 684.2803497602305, 688.8810559592796, 687.842154288548, 688.0897468766142, 687.3885806385292, 697.5211194111507, 696.5528992688302, 699.4396291296944, 689.1911882639104, 690.9124867980024, 687.3883291798971, 687.1104552612472, 681.8210722233853, 690.078087113307, 685.1621189945614, 630.9835186977276, 672.0405127390583, 688.9502327998748, 689.3369519974755, 682.6369060593643, 688.5918139105446, 685.2392902083996, 690.6683691873067, 688.6288107185338, 691.7108918714957, 690.4735075842262, 653.0980179687344, 686.2451855280782, 682.59332385271, 683.6504599608551, 685.7830418214281, 678.6659704473491, 692.4133163897864, 681.483092338233, 686.5300840642924, 682.398200989381, 632.4427137153737, 678.3792112352921, 681.5233757319198, 683.0156538413115, 689.93710910557, 686.3257154066553, 690.196128891956, 689.3915153934914, 693.2692765191077, 698.3989496700691, 690.9647375261787, 684.5861899640839, 695.224078802458, 690.8369749373742, 697.5711767672859, 695.4980408381033, 689.007262552958, 687.2946321116311, 689.6246567417556, 692.5396162553664, 692.4424068113254, 700.1659386580534, 698.8049141439463, 698.3855120898868, 695.2254499015935, 557.9166628586939, 677.6329057167904, 676.7509565557147, 673.69670555753, 278.5063227841597, 643.5298674253859, 639.2543783916129, 628.1132543186181, 614.5651158103274, 645.4273807020774, 636.206881427819, 627.9913451865779, 641.7702511698026, 673.611621612403, 689.7823035127914, 677.8889939019176, 649.3442553608531, 675.2889588575429, 670.7405714146355, 672.0671137835117, 675.4923547343681, 676.4731244732627, 684.1849467408005, 684.9134609589162, 675.6556076096625, 683.1525220992337, 681.0144549982291, 671.0148240899189, 674.0957335625894, 670.5050760843392, 689.1657047641677, 682.4966332501398, 681.0066473022827, 685.7495566070994, 682.9396220085459, 682.2668889605109, 684.6512249038997, 686.716114455948, 679.6818340201061, 673.1707328201418, 677.7658056600808, 682.1849348510128, 677.6033227494873, 676.3709058987132]
Elapsed: 0.07321844488717288~0.009957652837829432
Time per graph: 0.0014942539772892424~0.00020321740485366194
Speed: 674.823429248285~42.568725699025116
Total Time: 0.0730
best val loss: 0.14754952490329742 test_score: 0.8776

Testing...
Test loss: 0.3865 score: 0.8571 time: 0.07s
test Score 0.8571
Epoch Time List: [0.37369706702884287, 0.3805437319679186, 0.37999845400918275, 0.37266539700794965, 0.37918765703216195, 0.37927829707041383, 0.3824678868986666, 0.37137820513453335, 0.3789336160989478, 0.3830113089643419, 0.3713182210922241, 0.37464846309740096, 0.3811609379481524, 0.38248351006768644, 0.36779289692640305, 0.35657403094228357, 0.3569162030471489, 0.37765304709319025, 0.36768336489330977, 0.37406744505278766, 0.3701246399432421, 0.37656143005006015, 0.39571068494115025, 0.36893521493766457, 0.39189573493786156, 0.38852952898014337, 0.36882532003801316, 0.36399560305289924, 0.36800418293569237, 0.3625726510072127, 0.372899163980037, 0.36502496688626707, 0.37273028097115457, 0.3753131130943075, 0.37513027200475335, 0.3843588379677385, 0.37517358001787215, 0.3665895309532061, 0.3739858380286023, 0.37093310698401183, 0.37056887696962804, 0.35940672899596393, 0.3886556199286133, 0.3756935589481145, 0.37068246002309024, 0.37163558311294764, 0.3714381067547947, 0.37058860098477453, 0.3717899558832869, 0.3885519999312237, 0.36319021694362164, 0.361696220934391, 0.36729585693683475, 0.37246617092750967, 0.35764889512211084, 0.3805830010678619, 0.3774549738736823, 0.3707272430183366, 0.3772153981262818, 0.3744381550932303, 0.3707217819755897, 0.3719984870404005, 0.3871606240281835, 0.3707896730629727, 0.38246355787850916, 0.3726726860040799, 0.46093560697045177, 0.37045120901893824, 0.3757076009642333, 0.374354905099608, 0.4087725089630112, 0.3817834520014003, 0.39146441710181534, 0.3864116260083392, 0.4846683241194114, 0.4228440700098872, 0.4070939689408988, 0.40744022908620536, 0.41091650002636015, 0.48797929799184203, 0.40607022505719215, 0.4183427421376109, 0.4026348269544542, 0.387524682097137, 0.38042977394070476, 0.39198491210117936, 0.3878922980511561, 0.39331650687381625, 0.3822041479870677, 0.39083672093693167, 0.38269556884188205, 0.38175689708441496, 0.38008412707131356, 0.37831299693789333, 0.3788138049421832, 0.3843268540222198, 0.3898655120283365, 0.38835326093249023, 0.3763743171002716, 0.3840156779624522, 0.37372780684381723, 0.38491676200646907, 0.373839398031123, 0.38221234909724444, 0.38125606905668974, 0.38035021896939725, 0.3775801780866459, 0.3766430539544672, 0.3787912599509582, 0.3858769548824057, 0.383355621015653, 0.37577968707773834, 0.38009976490866393, 0.3792113739764318]
Total Epoch List: [114]
Total Time List: [0.07297504798043519]
========================k_idx:1========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: GPSPerformer
GPS(
  (x_lin): Linear(in_features=14887, out_features=64, bias=True)
  (pe_lin): Linear(in_features=20, out_features=8, bias=True)
  (pe_norm): BatchNorm1d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (convs): ModuleList(
    (0-1): 2 x GPSConv(64, conv=GINEConv(nn=Sequential(
      (0): Linear(in_features=64, out_features=64, bias=True)
      (1): ReLU()
      (2): Linear(in_features=64, out_features=64, bias=True)
    )), heads=1, attn_type=performer)
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=32, bias=True)
    (1): ReLU()
    (2): Linear(in_features=32, out_features=16, bias=True)
    (3): ReLU()
    (4): Linear(in_features=16, out_features=2, bias=True)
  )
)
Total number of parameters: 1039522
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7e7c5703b1c0>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.5857;  Loss pred: 0.5857; Loss self: 0.0000; time: 0.22s
Val loss: 0.6694 score: 0.5102 time: 0.07s
Test loss: 0.6101 score: 0.5714 time: 0.08s
Epoch 2/1000, LR 0.000000
Train loss: 0.5824;  Loss pred: 0.5824; Loss self: 0.0000; time: 0.22s
Val loss: 0.6205 score: 0.5510 time: 0.07s
Test loss: 0.5559 score: 0.5918 time: 0.08s
Epoch 3/1000, LR 0.000030
Train loss: 0.5935;  Loss pred: 0.5935; Loss self: 0.0000; time: 0.22s
Val loss: 0.5990 score: 0.6122 time: 0.07s
Test loss: 0.5321 score: 0.6122 time: 0.08s
Epoch 4/1000, LR 0.000060
Train loss: 0.5835;  Loss pred: 0.5835; Loss self: 0.0000; time: 0.23s
Val loss: 0.5940 score: 0.5918 time: 0.07s
Test loss: 0.5271 score: 0.6939 time: 0.08s
Epoch 5/1000, LR 0.000090
Train loss: 0.5553;  Loss pred: 0.5553; Loss self: 0.0000; time: 0.22s
Val loss: 0.5948 score: 0.6122 time: 0.07s
Test loss: 0.5301 score: 0.7143 time: 0.09s
     INFO: Early stopping counter 1 of 20
Epoch 6/1000, LR 0.000120
Train loss: 0.5348;  Loss pred: 0.5348; Loss self: 0.0000; time: 0.23s
Val loss: 0.5972 score: 0.6327 time: 0.07s
Test loss: 0.5325 score: 0.7347 time: 0.08s
     INFO: Early stopping counter 2 of 20
Epoch 7/1000, LR 0.000150
Train loss: 0.5365;  Loss pred: 0.5365; Loss self: 0.0000; time: 0.23s
Val loss: 0.5977 score: 0.6327 time: 0.07s
Test loss: 0.5292 score: 0.7755 time: 0.08s
     INFO: Early stopping counter 3 of 20
Epoch 8/1000, LR 0.000180
Train loss: 0.4571;  Loss pred: 0.4571; Loss self: 0.0000; time: 0.23s
Val loss: 0.5969 score: 0.6122 time: 0.07s
Test loss: 0.5257 score: 0.7755 time: 0.08s
     INFO: Early stopping counter 4 of 20
Epoch 9/1000, LR 0.000210
Train loss: 0.4593;  Loss pred: 0.4593; Loss self: 0.0000; time: 0.23s
Val loss: 0.5950 score: 0.6122 time: 0.07s
Test loss: 0.5234 score: 0.7551 time: 0.08s
     INFO: Early stopping counter 5 of 20
Epoch 10/1000, LR 0.000240
Train loss: 0.4204;  Loss pred: 0.4204; Loss self: 0.0000; time: 0.23s
Val loss: 0.5922 score: 0.6122 time: 0.07s
Test loss: 0.5212 score: 0.7551 time: 0.08s
Epoch 11/1000, LR 0.000270
Train loss: 0.4054;  Loss pred: 0.4054; Loss self: 0.0000; time: 0.23s
Val loss: 0.5899 score: 0.6327 time: 0.07s
Test loss: 0.5189 score: 0.7551 time: 0.08s
Epoch 12/1000, LR 0.000270
Train loss: 0.3680;  Loss pred: 0.3680; Loss self: 0.0000; time: 0.23s
Val loss: 0.5862 score: 0.6327 time: 0.07s
Test loss: 0.5151 score: 0.7959 time: 0.08s
Epoch 13/1000, LR 0.000270
Train loss: 0.3236;  Loss pred: 0.3236; Loss self: 0.0000; time: 0.22s
Val loss: 0.5826 score: 0.6327 time: 0.07s
Test loss: 0.5110 score: 0.7959 time: 0.08s
Epoch 14/1000, LR 0.000270
Train loss: 0.3091;  Loss pred: 0.3091; Loss self: 0.0000; time: 0.22s
Val loss: 0.5795 score: 0.6327 time: 0.07s
Test loss: 0.5075 score: 0.7959 time: 0.08s
Epoch 15/1000, LR 0.000270
Train loss: 0.2724;  Loss pred: 0.2724; Loss self: 0.0000; time: 0.24s
Val loss: 0.5773 score: 0.6327 time: 0.07s
Test loss: 0.5047 score: 0.7959 time: 0.08s
Epoch 16/1000, LR 0.000270
Train loss: 0.2737;  Loss pred: 0.2737; Loss self: 0.0000; time: 0.25s
Val loss: 0.5751 score: 0.6122 time: 0.07s
Test loss: 0.5027 score: 0.7959 time: 0.08s
Epoch 17/1000, LR 0.000270
Train loss: 0.2448;  Loss pred: 0.2448; Loss self: 0.0000; time: 0.25s
Val loss: 0.5740 score: 0.6327 time: 0.07s
Test loss: 0.5018 score: 0.7959 time: 0.09s
Epoch 18/1000, LR 0.000270
Train loss: 0.2174;  Loss pred: 0.2174; Loss self: 0.0000; time: 0.24s
Val loss: 0.5729 score: 0.6327 time: 0.07s
Test loss: 0.4996 score: 0.7959 time: 0.09s
Epoch 19/1000, LR 0.000270
Train loss: 0.2017;  Loss pred: 0.2017; Loss self: 0.0000; time: 0.24s
Val loss: 0.5719 score: 0.6735 time: 0.07s
Test loss: 0.4978 score: 0.7959 time: 0.09s
Epoch 20/1000, LR 0.000270
Train loss: 0.1675;  Loss pred: 0.1675; Loss self: 0.0000; time: 0.24s
Val loss: 0.5707 score: 0.6735 time: 0.07s
Test loss: 0.4953 score: 0.8367 time: 0.09s
Epoch 21/1000, LR 0.000270
Train loss: 0.1781;  Loss pred: 0.1781; Loss self: 0.0000; time: 0.24s
Val loss: 0.5680 score: 0.6735 time: 0.07s
Test loss: 0.4913 score: 0.8367 time: 0.09s
Epoch 22/1000, LR 0.000270
Train loss: 0.1698;  Loss pred: 0.1698; Loss self: 0.0000; time: 0.24s
Val loss: 0.5650 score: 0.6735 time: 0.07s
Test loss: 0.4869 score: 0.8367 time: 0.09s
Epoch 23/1000, LR 0.000270
Train loss: 0.1408;  Loss pred: 0.1408; Loss self: 0.0000; time: 0.24s
Val loss: 0.5614 score: 0.6735 time: 0.07s
Test loss: 0.4823 score: 0.8367 time: 0.09s
Epoch 24/1000, LR 0.000270
Train loss: 0.1353;  Loss pred: 0.1353; Loss self: 0.0000; time: 0.24s
Val loss: 0.5571 score: 0.6735 time: 0.07s
Test loss: 0.4778 score: 0.8367 time: 0.08s
Epoch 25/1000, LR 0.000270
Train loss: 0.1201;  Loss pred: 0.1201; Loss self: 0.0000; time: 0.25s
Val loss: 0.5528 score: 0.6735 time: 0.07s
Test loss: 0.4728 score: 0.8367 time: 0.08s
Epoch 26/1000, LR 0.000270
Train loss: 0.1152;  Loss pred: 0.1152; Loss self: 0.0000; time: 0.25s
Val loss: 0.5489 score: 0.6735 time: 0.07s
Test loss: 0.4678 score: 0.8367 time: 0.09s
Epoch 27/1000, LR 0.000270
Train loss: 0.1075;  Loss pred: 0.1075; Loss self: 0.0000; time: 0.24s
Val loss: 0.5448 score: 0.6735 time: 0.07s
Test loss: 0.4627 score: 0.8367 time: 0.09s
Epoch 28/1000, LR 0.000270
Train loss: 0.0866;  Loss pred: 0.0866; Loss self: 0.0000; time: 0.25s
Val loss: 0.5408 score: 0.6735 time: 0.07s
Test loss: 0.4573 score: 0.8367 time: 0.09s
Epoch 29/1000, LR 0.000270
Train loss: 0.0903;  Loss pred: 0.0903; Loss self: 0.0000; time: 0.23s
Val loss: 0.5364 score: 0.6735 time: 0.07s
Test loss: 0.4526 score: 0.8367 time: 0.08s
Epoch 30/1000, LR 0.000270
Train loss: 0.0682;  Loss pred: 0.0682; Loss self: 0.0000; time: 0.23s
Val loss: 0.5322 score: 0.6735 time: 0.08s
Test loss: 0.4479 score: 0.8367 time: 0.09s
Epoch 31/1000, LR 0.000270
Train loss: 0.0669;  Loss pred: 0.0669; Loss self: 0.0000; time: 0.24s
Val loss: 0.5281 score: 0.6735 time: 0.07s
Test loss: 0.4428 score: 0.8367 time: 0.08s
Epoch 32/1000, LR 0.000270
Train loss: 0.0591;  Loss pred: 0.0591; Loss self: 0.0000; time: 0.24s
Val loss: 0.5244 score: 0.6735 time: 0.07s
Test loss: 0.4382 score: 0.8367 time: 0.09s
Epoch 33/1000, LR 0.000270
Train loss: 0.0353;  Loss pred: 0.0353; Loss self: 0.0000; time: 0.24s
Val loss: 0.5212 score: 0.6735 time: 0.07s
Test loss: 0.4334 score: 0.8163 time: 0.09s
Epoch 34/1000, LR 0.000270
Train loss: 0.0391;  Loss pred: 0.0391; Loss self: 0.0000; time: 0.23s
Val loss: 0.5175 score: 0.7143 time: 0.07s
Test loss: 0.4281 score: 0.8367 time: 0.09s
Epoch 35/1000, LR 0.000270
Train loss: 0.0373;  Loss pred: 0.0373; Loss self: 0.0000; time: 0.23s
Val loss: 0.5121 score: 0.7143 time: 0.07s
Test loss: 0.4219 score: 0.8367 time: 0.08s
Epoch 36/1000, LR 0.000270
Train loss: 0.0297;  Loss pred: 0.0297; Loss self: 0.0000; time: 0.23s
Val loss: 0.5053 score: 0.7143 time: 0.08s
Test loss: 0.4140 score: 0.8367 time: 0.09s
Epoch 37/1000, LR 0.000270
Train loss: 0.0276;  Loss pred: 0.0276; Loss self: 0.0000; time: 0.23s
Val loss: 0.4994 score: 0.6939 time: 0.07s
Test loss: 0.4071 score: 0.8367 time: 0.09s
Epoch 38/1000, LR 0.000270
Train loss: 0.0256;  Loss pred: 0.0256; Loss self: 0.0000; time: 0.23s
Val loss: 0.4951 score: 0.6939 time: 0.07s
Test loss: 0.4018 score: 0.8367 time: 0.09s
Epoch 39/1000, LR 0.000269
Train loss: 0.0243;  Loss pred: 0.0243; Loss self: 0.0000; time: 0.23s
Val loss: 0.4916 score: 0.6735 time: 0.07s
Test loss: 0.3982 score: 0.8367 time: 0.09s
Epoch 40/1000, LR 0.000269
Train loss: 0.0216;  Loss pred: 0.0216; Loss self: 0.0000; time: 0.23s
Val loss: 0.4890 score: 0.7143 time: 0.07s
Test loss: 0.3971 score: 0.8571 time: 0.09s
Epoch 41/1000, LR 0.000269
Train loss: 0.0218;  Loss pred: 0.0218; Loss self: 0.0000; time: 0.23s
Val loss: 0.4877 score: 0.7143 time: 0.08s
Test loss: 0.3976 score: 0.8571 time: 0.08s
Epoch 42/1000, LR 0.000269
Train loss: 0.0157;  Loss pred: 0.0157; Loss self: 0.0000; time: 0.24s
Val loss: 0.4874 score: 0.7347 time: 0.07s
Test loss: 0.3999 score: 0.8571 time: 0.09s
Epoch 43/1000, LR 0.000269
Train loss: 0.0149;  Loss pred: 0.0149; Loss self: 0.0000; time: 0.24s
Val loss: 0.4888 score: 0.7143 time: 0.08s
Test loss: 0.4037 score: 0.8571 time: 0.09s
     INFO: Early stopping counter 1 of 20
Epoch 44/1000, LR 0.000269
Train loss: 0.0137;  Loss pred: 0.0137; Loss self: 0.0000; time: 0.23s
Val loss: 0.4902 score: 0.7143 time: 0.07s
Test loss: 0.4063 score: 0.8571 time: 0.09s
     INFO: Early stopping counter 2 of 20
Epoch 45/1000, LR 0.000269
Train loss: 0.0109;  Loss pred: 0.0109; Loss self: 0.0000; time: 0.23s
Val loss: 0.4911 score: 0.7347 time: 0.07s
Test loss: 0.4090 score: 0.8163 time: 0.09s
     INFO: Early stopping counter 3 of 20
Epoch 46/1000, LR 0.000269
Train loss: 0.0094;  Loss pred: 0.0094; Loss self: 0.0000; time: 0.24s
Val loss: 0.4902 score: 0.7347 time: 0.07s
Test loss: 0.4100 score: 0.8163 time: 0.09s
     INFO: Early stopping counter 4 of 20
Epoch 47/1000, LR 0.000269
Train loss: 0.0090;  Loss pred: 0.0090; Loss self: 0.0000; time: 0.24s
Val loss: 0.4844 score: 0.7551 time: 0.08s
Test loss: 0.4037 score: 0.8163 time: 0.08s
Epoch 48/1000, LR 0.000269
Train loss: 0.0086;  Loss pred: 0.0086; Loss self: 0.0000; time: 0.24s
Val loss: 0.4746 score: 0.7755 time: 0.07s
Test loss: 0.3923 score: 0.8367 time: 0.09s
Epoch 49/1000, LR 0.000269
Train loss: 0.0091;  Loss pred: 0.0091; Loss self: 0.0000; time: 0.24s
Val loss: 0.4651 score: 0.7347 time: 0.07s
Test loss: 0.3810 score: 0.8571 time: 0.09s
Epoch 50/1000, LR 0.000269
Train loss: 0.0065;  Loss pred: 0.0065; Loss self: 0.0000; time: 0.24s
Val loss: 0.4542 score: 0.7551 time: 0.07s
Test loss: 0.3681 score: 0.8571 time: 0.09s
Epoch 51/1000, LR 0.000269
Train loss: 0.0080;  Loss pred: 0.0080; Loss self: 0.0000; time: 0.24s
Val loss: 0.4439 score: 0.7551 time: 0.07s
Test loss: 0.3560 score: 0.8571 time: 0.08s
Epoch 52/1000, LR 0.000269
Train loss: 0.0051;  Loss pred: 0.0051; Loss self: 0.0000; time: 0.24s
Val loss: 0.4332 score: 0.7551 time: 0.07s
Test loss: 0.3427 score: 0.8571 time: 0.09s
Epoch 53/1000, LR 0.000269
Train loss: 0.0050;  Loss pred: 0.0050; Loss self: 0.0000; time: 0.24s
Val loss: 0.4230 score: 0.7551 time: 0.07s
Test loss: 0.3301 score: 0.8571 time: 0.09s
Epoch 54/1000, LR 0.000269
Train loss: 0.0054;  Loss pred: 0.0054; Loss self: 0.0000; time: 0.24s
Val loss: 0.4128 score: 0.7755 time: 0.07s
Test loss: 0.3193 score: 0.8980 time: 0.09s
Epoch 55/1000, LR 0.000269
Train loss: 0.0042;  Loss pred: 0.0042; Loss self: 0.0000; time: 0.24s
Val loss: 0.4030 score: 0.7959 time: 0.07s
Test loss: 0.3097 score: 0.8776 time: 0.09s
Epoch 56/1000, LR 0.000269
Train loss: 0.0041;  Loss pred: 0.0041; Loss self: 0.0000; time: 0.24s
Val loss: 0.3942 score: 0.7755 time: 0.07s
Test loss: 0.3007 score: 0.8776 time: 0.09s
Epoch 57/1000, LR 0.000269
Train loss: 0.0038;  Loss pred: 0.0038; Loss self: 0.0000; time: 0.24s
Val loss: 0.3874 score: 0.7959 time: 0.07s
Test loss: 0.2917 score: 0.8776 time: 0.09s
Epoch 58/1000, LR 0.000269
Train loss: 0.0031;  Loss pred: 0.0031; Loss self: 0.0000; time: 0.24s
Val loss: 0.3820 score: 0.7959 time: 0.07s
Test loss: 0.2822 score: 0.8776 time: 0.09s
Epoch 59/1000, LR 0.000268
Train loss: 0.0027;  Loss pred: 0.0027; Loss self: 0.0000; time: 0.24s
Val loss: 0.3777 score: 0.7959 time: 0.07s
Test loss: 0.2738 score: 0.8776 time: 0.08s
Epoch 60/1000, LR 0.000268
Train loss: 0.0028;  Loss pred: 0.0028; Loss self: 0.0000; time: 0.24s
Val loss: 0.3743 score: 0.7959 time: 0.07s
Test loss: 0.2668 score: 0.8776 time: 0.09s
Epoch 61/1000, LR 0.000268
Train loss: 0.0028;  Loss pred: 0.0028; Loss self: 0.0000; time: 0.24s
Val loss: 0.3721 score: 0.7959 time: 0.07s
Test loss: 0.2608 score: 0.8776 time: 0.08s
Epoch 62/1000, LR 0.000268
Train loss: 0.0027;  Loss pred: 0.0027; Loss self: 0.0000; time: 0.24s
Val loss: 0.3711 score: 0.7959 time: 0.07s
Test loss: 0.2555 score: 0.8776 time: 0.08s
Epoch 63/1000, LR 0.000268
Train loss: 0.0021;  Loss pred: 0.0021; Loss self: 0.0000; time: 0.24s
Val loss: 0.3709 score: 0.7959 time: 0.07s
Test loss: 0.2508 score: 0.8980 time: 0.09s
Epoch 64/1000, LR 0.000268
Train loss: 0.0022;  Loss pred: 0.0022; Loss self: 0.0000; time: 0.24s
Val loss: 0.3715 score: 0.7959 time: 0.07s
Test loss: 0.2461 score: 0.8980 time: 0.08s
     INFO: Early stopping counter 1 of 20
Epoch 65/1000, LR 0.000268
Train loss: 0.0020;  Loss pred: 0.0020; Loss self: 0.0000; time: 0.24s
Val loss: 0.3729 score: 0.8163 time: 0.07s
Test loss: 0.2419 score: 0.9184 time: 0.08s
     INFO: Early stopping counter 2 of 20
Epoch 66/1000, LR 0.000268
Train loss: 0.0021;  Loss pred: 0.0021; Loss self: 0.0000; time: 0.23s
Val loss: 0.3748 score: 0.8163 time: 0.07s
Test loss: 0.2381 score: 0.9184 time: 0.08s
     INFO: Early stopping counter 3 of 20
Epoch 67/1000, LR 0.000268
Train loss: 0.0023;  Loss pred: 0.0023; Loss self: 0.0000; time: 0.22s
Val loss: 0.3779 score: 0.8367 time: 0.07s
Test loss: 0.2353 score: 0.9184 time: 0.08s
     INFO: Early stopping counter 4 of 20
Epoch 68/1000, LR 0.000268
Train loss: 0.0018;  Loss pred: 0.0018; Loss self: 0.0000; time: 0.22s
Val loss: 0.3814 score: 0.8367 time: 0.07s
Test loss: 0.2329 score: 0.9184 time: 0.08s
     INFO: Early stopping counter 5 of 20
Epoch 69/1000, LR 0.000268
Train loss: 0.0018;  Loss pred: 0.0018; Loss self: 0.0000; time: 0.22s
Val loss: 0.3862 score: 0.8367 time: 0.07s
Test loss: 0.2318 score: 0.9184 time: 0.08s
     INFO: Early stopping counter 6 of 20
Epoch 70/1000, LR 0.000268
Train loss: 0.0016;  Loss pred: 0.0016; Loss self: 0.0000; time: 0.22s
Val loss: 0.3911 score: 0.8367 time: 0.07s
Test loss: 0.2314 score: 0.9184 time: 0.08s
     INFO: Early stopping counter 7 of 20
Epoch 71/1000, LR 0.000268
Train loss: 0.0018;  Loss pred: 0.0018; Loss self: 0.0000; time: 0.24s
Val loss: 0.3971 score: 0.8367 time: 0.07s
Test loss: 0.2323 score: 0.9184 time: 0.09s
     INFO: Early stopping counter 8 of 20
Epoch 72/1000, LR 0.000267
Train loss: 0.0014;  Loss pred: 0.0014; Loss self: 0.0000; time: 0.24s
Val loss: 0.4043 score: 0.8367 time: 0.07s
Test loss: 0.2338 score: 0.9184 time: 0.09s
     INFO: Early stopping counter 9 of 20
Epoch 73/1000, LR 0.000267
Train loss: 0.0014;  Loss pred: 0.0014; Loss self: 0.0000; time: 0.24s
Val loss: 0.4124 score: 0.8367 time: 0.07s
Test loss: 0.2361 score: 0.9184 time: 0.09s
     INFO: Early stopping counter 10 of 20
Epoch 74/1000, LR 0.000267
Train loss: 0.0013;  Loss pred: 0.0013; Loss self: 0.0000; time: 0.23s
Val loss: 0.4217 score: 0.8367 time: 0.07s
Test loss: 0.2392 score: 0.9184 time: 0.09s
     INFO: Early stopping counter 11 of 20
Epoch 75/1000, LR 0.000267
Train loss: 0.0016;  Loss pred: 0.0016; Loss self: 0.0000; time: 0.23s
Val loss: 0.4324 score: 0.8367 time: 0.07s
Test loss: 0.2431 score: 0.9184 time: 0.09s
     INFO: Early stopping counter 12 of 20
Epoch 76/1000, LR 0.000267
Train loss: 0.0013;  Loss pred: 0.0013; Loss self: 0.0000; time: 0.23s
Val loss: 0.4446 score: 0.8367 time: 0.07s
Test loss: 0.2479 score: 0.9184 time: 0.09s
     INFO: Early stopping counter 13 of 20
Epoch 77/1000, LR 0.000267
Train loss: 0.0011;  Loss pred: 0.0011; Loss self: 0.0000; time: 0.23s
Val loss: 0.4580 score: 0.8367 time: 0.07s
Test loss: 0.2536 score: 0.9184 time: 0.09s
     INFO: Early stopping counter 14 of 20
Epoch 78/1000, LR 0.000267
Train loss: 0.0010;  Loss pred: 0.0010; Loss self: 0.0000; time: 0.23s
Val loss: 0.4707 score: 0.8163 time: 0.07s
Test loss: 0.2593 score: 0.9184 time: 0.09s
     INFO: Early stopping counter 15 of 20
Epoch 79/1000, LR 0.000267
Train loss: 0.0011;  Loss pred: 0.0011; Loss self: 0.0000; time: 0.24s
Val loss: 0.4855 score: 0.8163 time: 0.07s
Test loss: 0.2663 score: 0.9184 time: 0.09s
     INFO: Early stopping counter 16 of 20
Epoch 80/1000, LR 0.000267
Train loss: 0.0010;  Loss pred: 0.0010; Loss self: 0.0000; time: 0.24s
Val loss: 0.5028 score: 0.7959 time: 0.07s
Test loss: 0.2741 score: 0.9184 time: 0.09s
     INFO: Early stopping counter 17 of 20
Epoch 81/1000, LR 0.000267
Train loss: 0.0009;  Loss pred: 0.0009; Loss self: 0.0000; time: 0.23s
Val loss: 0.5208 score: 0.7959 time: 0.07s
Test loss: 0.2822 score: 0.9184 time: 0.09s
     INFO: Early stopping counter 18 of 20
Epoch 82/1000, LR 0.000267
Train loss: 0.0010;  Loss pred: 0.0010; Loss self: 0.0000; time: 0.24s
Val loss: 0.5392 score: 0.7959 time: 0.08s
Test loss: 0.2905 score: 0.9184 time: 0.09s
     INFO: Early stopping counter 19 of 20
Epoch 83/1000, LR 0.000266
Train loss: 0.0010;  Loss pred: 0.0010; Loss self: 0.0000; time: 0.23s
Val loss: 0.5583 score: 0.7959 time: 0.07s
Test loss: 0.2993 score: 0.9184 time: 0.09s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 062,   Train_Loss: 0.0021,   Val_Loss: 0.3709,   Val_Precision: 0.8947,   Val_Recall: 0.6800,   Val_accuracy: 0.7727,   Val_Score: 0.7959,   Val_Loss: 0.3709,   Test_Precision: 0.9524,   Test_Recall: 0.8333,   Test_accuracy: 0.8889,   Test_Score: 0.8980,   Test_loss: 0.2508


[0.07090797193814069, 0.07478468609042466, 0.07500295399222523, 0.07073386700358242, 0.07147020800039172, 0.07484937901608646, 0.07126277300994843, 0.07091055996716022, 0.07253571297042072, 0.07160807703621686, 0.07112984103150666, 0.07123727397993207, 0.07121164095588028, 0.0712842799257487, 0.07024876901414245, 0.07034641597419977, 0.07005608198232949, 0.07109783298801631, 0.07092070404905826, 0.0712843060027808, 0.07131313404534012, 0.0718663620064035, 0.07100645697209984, 0.07151592103764415, 0.07765654497779906, 0.07291227101814002, 0.07112269895151258, 0.07108279899694026, 0.07178047299385071, 0.07115971902385354, 0.07150786696001887, 0.07094577106181532, 0.07115589594468474, 0.07083884405437857, 0.07096579298377037, 0.07502702297642827, 0.07140305102802813, 0.07178505603224039, 0.07167405402287841, 0.07145116897299886, 0.0722004669951275, 0.07076698099263012, 0.07190200395416468, 0.07137341995257884, 0.07180558203253895, 0.07747737294994295, 0.07223098701797426, 0.07189775398001075, 0.0717406690819189, 0.07102096604648978, 0.07139467296656221, 0.07099431299138814, 0.07107717299368232, 0.0706796069862321, 0.07016047206707299, 0.07091534102801234, 0.07157608598936349, 0.07048087299335748, 0.07092845602892339, 0.0702437279978767, 0.07045311003457755, 0.07111681206151843, 0.07129402400460094, 0.07105314393993467, 0.07075407507363707, 0.07076400797814131, 0.06998341006692499, 0.07011971296742558, 0.07016182201914489, 0.07048073399346322, 0.08782673696987331, 0.07231053803116083, 0.0724047739058733, 0.07273302599787712, 0.17593855503946543, 0.07614254206418991, 0.07665180193725973, 0.07801140903029591, 0.07973117695655674, 0.0759186880895868, 0.07701897202059627, 0.07802655303385109, 0.0763513109413907, 0.0727422129129991, 0.07103690505027771, 0.07228322105947882, 0.07546074304264039, 0.07256152993068099, 0.07305358001030982, 0.07290938508231193, 0.07253968110308051, 0.07243451103568077, 0.07161806209478527, 0.07154188491404057, 0.07252215396147221, 0.07172629598062485, 0.07195148302707821, 0.07302372204139829, 0.07268997200299054, 0.07307923794724047, 0.07110046199522913, 0.07179522595833987, 0.07195230794604868, 0.07145465794019401, 0.07174865598790348, 0.07181940204463899, 0.07156928698532283, 0.0713540849974379, 0.07209255499765277, 0.07278985495213419, 0.07229635899420828, 0.07182803004980087, 0.07231369498185813, 0.07244545791763812, 0.08751676906831563, 0.08721476094797254, 0.08656603703275323, 0.08635204005986452, 0.09085932490415871, 0.08634752908255905, 0.08886271098162979, 0.08646483393386006, 0.08683307701721787, 0.08592347207013518, 0.0869856639765203, 0.0867934999987483, 0.08568585896864533, 0.08483078796416521, 0.08642486203461885, 0.08692113705910742, 0.09218003402929753, 0.09150914300698787, 0.09118365996982902, 0.09243821701966226, 0.09140098700299859, 0.09140443208161741, 0.09211764007341117, 0.08632600400596857, 0.0865552700124681, 0.09079325501807034, 0.09168276702985168, 0.09136055794078857, 0.08707353100180626, 0.09132846596185118, 0.08754789794329554, 0.09116107400041074, 0.09252880397252738, 0.09178362798411399, 0.08604962704703212, 0.09267976507544518, 0.09211387997493148, 0.09008388104848564, 0.09089766489341855, 0.09144063596613705, 0.08700535399839282, 0.09100335801485926, 0.09170152293518186, 0.09055424807593226, 0.09171077597420663, 0.09223139600362629, 0.08808110002428293, 0.0922017099801451, 0.09291928994935006, 0.09258017793763429, 0.08854048303328454, 0.09297738794703037, 0.09257320198230445, 0.09273457201197743, 0.09271962696220726, 0.09313755307812244, 0.09253014309797436, 0.08970933500677347, 0.08785875292960554, 0.09223154804203659, 0.08697170403320342, 0.08804732304997742, 0.09211626905016601, 0.08749892702326179, 0.08735846600029618, 0.0832109299954027, 0.08929368504323065, 0.08731849805917591, 0.08764384198002517, 0.08263873599935323, 0.09387869108468294, 0.09199103503488004, 0.09210143506061286, 0.09127930097747594, 0.09140647202730179, 0.0914184870198369, 0.09170030406676233, 0.091788889025338, 0.09182567708194256, 0.0911584870191291, 0.09160291007719934, 0.09241285896860063, 0.09301857894752175]
[0.0014471014681253203, 0.0015262180834780543, 0.001530672530453576, 0.0014435483061955596, 0.001458575673477382, 0.0015275383472670705, 0.0014543423063254782, 0.0014471542850440862, 0.0014803206728657289, 0.0014613893272697317, 0.0014516294088062582, 0.0014538219179577973, 0.001453298795017965, 0.0014547812229744634, 0.001433648347227397, 0.0014356411423306077, 0.0014297159588230507, 0.0014509761834289043, 0.001447361307123638, 0.0014547817551587917, 0.0014553700825579617, 0.0014666604491102757, 0.0014491113667775477, 0.0014595085926049826, 0.0015848274485265113, 0.0014880055309824493, 0.0014514836520716852, 0.001450669367284495, 0.0014649076121194021, 0.0014522391637521131, 0.0014593442236738546, 0.0014478728788125577, 0.00145216114172826, 0.0014456906949873178, 0.0014482814894647015, 0.001531163734212822, 0.0014572051230209823, 0.00146500114351511, 0.0014627357963852736, 0.0014581871218979359, 0.0014734789182679082, 0.0014442241018904106, 0.0014673878357992793, 0.0014566004071954865, 0.0014654200414803867, 0.0015811708765294478, 0.0014741017758770256, 0.0014673011016328723, 0.0014640952873860998, 0.001449407470336526, 0.001457034142174739, 0.0014488635304364928, 0.001450554550891476, 0.0014424409589026959, 0.0014318463687157752, 0.0014472518577145375, 0.0014607364487625202, 0.0014383851631297444, 0.001447519510794355, 0.0014335454693444225, 0.0014378185721342356, 0.0014513635114595598, 0.0014549800817265498, 0.001450064162039483, 0.0014439607157885116, 0.0014441634281253328, 0.0014282328585086732, 0.0014310145503556241, 0.001431873918758059, 0.0014383823263972085, 0.0017923823871402716, 0.0014757252659420579, 0.0014776484470586388, 0.001484347469344431, 0.003590582755907458, 0.0015539294298814268, 0.0015643224885155047, 0.0015920695720468552, 0.0016271668766644231, 0.0015493609814201386, 0.0015718157555223728, 0.0015923786333438997, 0.001558190019212055, 0.0014845349574081448, 0.0014497327561281165, 0.0014751677767240576, 0.0015400151641355182, 0.0014808475496057346, 0.0014908893879655065, 0.0014879466343328965, 0.0014804016551649083, 0.0014782553272587912, 0.0014615931039752097, 0.0014600384676334809, 0.0014800439583973922, 0.0014638019587882624, 0.0014683976127975145, 0.0014902800416611895, 0.001483468816387562, 0.0014914130193314382, 0.001451029836637329, 0.0014652086930273442, 0.0014684144478785445, 0.0014582583253100819, 0.001464258285467418, 0.0014657020825436528, 0.001460597693578017, 0.0014562058162742428, 0.0014712766326051585, 0.001485507243921106, 0.0014754358978409851, 0.0014658781642816505, 0.0014757896935073088, 0.001478478733013023, 0.0017860565115982781, 0.0017798930805708682, 0.0017666538169949638, 0.0017622865318339697, 0.0018542719368195655, 0.0017621944710726337, 0.001813524713910812, 0.0017645884476297973, 0.001772103612596283, 0.0017535402463292893, 0.0017752176321738837, 0.0017712959183418021, 0.0017486909993601088, 0.0017312405706972492, 0.0017637726945840583, 0.0017739007563083147, 0.0018812251842713781, 0.0018675335307548546, 0.0018608910197924291, 0.0018864942248910666, 0.0018653262653673182, 0.001865396573094233, 0.001879951838232881, 0.001761755183795277, 0.001766434081887104, 0.0018529235717973538, 0.0018710768781602383, 0.0018645011824650727, 0.0017770108367715562, 0.001863846244119412, 0.0017866917947611334, 0.0018604300816410355, 0.0018883429382148447, 0.001873135264981918, 0.0017561148376945331, 0.0018914237770499016, 0.0018798751015292139, 0.001838446552009911, 0.0018550543855799704, 0.001866135427880348, 0.0017756194693549555, 0.0018572113880583523, 0.0018714596517384052, 0.0018480458791006584, 0.001871648489269523, 0.0018822733878291079, 0.0017975734698833252, 0.001881667550615206, 0.0018963120397826542, 0.0018893913864823325, 0.0018069486333323376, 0.0018974977132047014, 0.0018892490200470298, 0.0018925422859587232, 0.0018922372849430053, 0.0019007663893494376, 0.0018883702673055992, 0.001830802755240275, 0.0017930357740735825, 0.001882276490653808, 0.0017749327353714984, 0.0017968841438770903, 0.0018799238581666533, 0.0017856923882298324, 0.0017828258367407384, 0.0016981822448041365, 0.0018223201029230745, 0.0017820101644729777, 0.0017886498363270443, 0.0016865048163133313, 0.0019158916547894478, 0.0018773680619363273, 0.0018796211236859768, 0.0018628428770913457, 0.001865438204638812, 0.0018656834085681, 0.0018714347768727007, 0.0018732426331701632, 0.0018739934098355624, 0.0018603772861046754, 0.0018694471444326397, 0.0018859767136449109, 0.0018983383458677906]
[691.0365458307995, 655.2143568638165, 653.3076017923149, 692.7374689908912, 685.6003553219186, 654.6480497783292, 687.596032688196, 691.0113250084706, 675.5293081627484, 684.2803497602305, 688.8810559592796, 687.842154288548, 688.0897468766142, 687.3885806385292, 697.5211194111507, 696.5528992688302, 699.4396291296944, 689.1911882639104, 690.9124867980024, 687.3883291798971, 687.1104552612472, 681.8210722233853, 690.078087113307, 685.1621189945614, 630.9835186977276, 672.0405127390583, 688.9502327998748, 689.3369519974755, 682.6369060593643, 688.5918139105446, 685.2392902083996, 690.6683691873067, 688.6288107185338, 691.7108918714957, 690.4735075842262, 653.0980179687344, 686.2451855280782, 682.59332385271, 683.6504599608551, 685.7830418214281, 678.6659704473491, 692.4133163897864, 681.483092338233, 686.5300840642924, 682.398200989381, 632.4427137153737, 678.3792112352921, 681.5233757319198, 683.0156538413115, 689.93710910557, 686.3257154066553, 690.196128891956, 689.3915153934914, 693.2692765191077, 698.3989496700691, 690.9647375261787, 684.5861899640839, 695.224078802458, 690.8369749373742, 697.5711767672859, 695.4980408381033, 689.007262552958, 687.2946321116311, 689.6246567417556, 692.5396162553664, 692.4424068113254, 700.1659386580534, 698.8049141439463, 698.3855120898868, 695.2254499015935, 557.9166628586939, 677.6329057167904, 676.7509565557147, 673.69670555753, 278.5063227841597, 643.5298674253859, 639.2543783916129, 628.1132543186181, 614.5651158103274, 645.4273807020774, 636.206881427819, 627.9913451865779, 641.7702511698026, 673.611621612403, 689.7823035127914, 677.8889939019176, 649.3442553608531, 675.2889588575429, 670.7405714146355, 672.0671137835117, 675.4923547343681, 676.4731244732627, 684.1849467408005, 684.9134609589162, 675.6556076096625, 683.1525220992337, 681.0144549982291, 671.0148240899189, 674.0957335625894, 670.5050760843392, 689.1657047641677, 682.4966332501398, 681.0066473022827, 685.7495566070994, 682.9396220085459, 682.2668889605109, 684.6512249038997, 686.716114455948, 679.6818340201061, 673.1707328201418, 677.7658056600808, 682.1849348510128, 677.6033227494873, 676.3709058987132, 559.8926985267312, 561.831500395107, 566.0418528973471, 567.4446135381422, 539.295224256693, 567.4742580433293, 551.4123917525942, 566.704378770701, 564.30108989785, 570.2749064889239, 563.3112142849927, 564.5584058795492, 571.8563201651555, 577.6204745463275, 566.9664821723669, 563.7293949189759, 531.5684737589308, 535.465620044745, 537.376981974766, 530.0837854712987, 536.099243637188, 536.0790377893996, 531.9285205412395, 567.6157556952612, 566.1122655263123, 539.6876672198574, 534.451583295318, 536.336479378303, 562.7427696596287, 536.5249430606641, 559.693620876393, 537.5101219165016, 529.5648262626255, 533.8642748844159, 569.4388422301713, 528.7022464948197, 531.9502339206123, 543.9374883685001, 539.0677533625823, 535.8667892264664, 563.1837323586446, 538.4416692843264, 534.3422707890585, 541.112107285261, 534.2883590231654, 531.272453016687, 556.3054955772722, 531.4435058802245, 527.339371907703, 529.2709637370579, 553.419162865643, 527.0098577937629, 529.3108475319503, 528.3897788806449, 528.4749475962896, 526.1035788528769, 529.5571622332517, 546.2084853967569, 557.7133565651669, 531.271577244558, 563.4016321135106, 556.5189071357299, 531.9364375614787, 560.0068671353334, 560.907285160363, 588.8649484233284, 548.7510116339934, 561.1640269716115, 559.08092220749, 592.9422734682619, 521.9501830910672, 532.6606009098689, 532.0221120089238, 536.8139268736428, 536.067073952536, 535.99661947334, 534.3493731964683, 533.8336755168017, 533.6198061058001, 537.5253758842841, 534.917503807518, 530.2292402472784, 526.7764843800109]
Elapsed: 0.08022238044623702~0.011303594376693246
Time per graph: 0.0016371914376783064~0.00023068559952435196
Speed: 620.5027940939808~72.2298207399219
Total Time: 0.0937
best val loss: 0.3709000051021576 test_score: 0.8980

Testing...
Test loss: 0.2353 score: 0.9184 time: 0.09s
test Score 0.9184
Epoch Time List: [0.37369706702884287, 0.3805437319679186, 0.37999845400918275, 0.37266539700794965, 0.37918765703216195, 0.37927829707041383, 0.3824678868986666, 0.37137820513453335, 0.3789336160989478, 0.3830113089643419, 0.3713182210922241, 0.37464846309740096, 0.3811609379481524, 0.38248351006768644, 0.36779289692640305, 0.35657403094228357, 0.3569162030471489, 0.37765304709319025, 0.36768336489330977, 0.37406744505278766, 0.3701246399432421, 0.37656143005006015, 0.39571068494115025, 0.36893521493766457, 0.39189573493786156, 0.38852952898014337, 0.36882532003801316, 0.36399560305289924, 0.36800418293569237, 0.3625726510072127, 0.372899163980037, 0.36502496688626707, 0.37273028097115457, 0.3753131130943075, 0.37513027200475335, 0.3843588379677385, 0.37517358001787215, 0.3665895309532061, 0.3739858380286023, 0.37093310698401183, 0.37056887696962804, 0.35940672899596393, 0.3886556199286133, 0.3756935589481145, 0.37068246002309024, 0.37163558311294764, 0.3714381067547947, 0.37058860098477453, 0.3717899558832869, 0.3885519999312237, 0.36319021694362164, 0.361696220934391, 0.36729585693683475, 0.37246617092750967, 0.35764889512211084, 0.3805830010678619, 0.3774549738736823, 0.3707272430183366, 0.3772153981262818, 0.3744381550932303, 0.3707217819755897, 0.3719984870404005, 0.3871606240281835, 0.3707896730629727, 0.38246355787850916, 0.3726726860040799, 0.46093560697045177, 0.37045120901893824, 0.3757076009642333, 0.374354905099608, 0.4087725089630112, 0.3817834520014003, 0.39146441710181534, 0.3864116260083392, 0.4846683241194114, 0.4228440700098872, 0.4070939689408988, 0.40744022908620536, 0.41091650002636015, 0.48797929799184203, 0.40607022505719215, 0.4183427421376109, 0.4026348269544542, 0.387524682097137, 0.38042977394070476, 0.39198491210117936, 0.3878922980511561, 0.39331650687381625, 0.3822041479870677, 0.39083672093693167, 0.38269556884188205, 0.38175689708441496, 0.38008412707131356, 0.37831299693789333, 0.3788138049421832, 0.3843268540222198, 0.3898655120283365, 0.38835326093249023, 0.3763743171002716, 0.3840156779624522, 0.37372780684381723, 0.38491676200646907, 0.373839398031123, 0.38221234909724444, 0.38125606905668974, 0.38035021896939725, 0.3775801780866459, 0.3766430539544672, 0.3787912599509582, 0.3858769548824057, 0.383355621015653, 0.37577968707773834, 0.38009976490866393, 0.3792113739764318, 0.37257327779661864, 0.37308620603289455, 0.37340130005031824, 0.37413566501345485, 0.3767656101845205, 0.3757246278692037, 0.3785937500651926, 0.37858193600550294, 0.37868334201630205, 0.36944208294153214, 0.3750308359740302, 0.373424939927645, 0.36976710008457303, 0.37042770395055413, 0.38826272694859654, 0.4003352200379595, 0.40742890001274645, 0.3984976870706305, 0.3958034561946988, 0.3996333130635321, 0.39433262997772545, 0.39982378715649247, 0.3944474869640544, 0.3916739640990272, 0.4032518321182579, 0.402341854874976, 0.39766262704506516, 0.4037247149972245, 0.38587245892267674, 0.39716892200522125, 0.39282537798862904, 0.3982001479016617, 0.3941181841073558, 0.3906900640577078, 0.3846488499548286, 0.397380173089914, 0.39124407002236694, 0.38906192302238196, 0.3886580530088395, 0.3888897649012506, 0.3915092251263559, 0.3962852879194543, 0.4037045210134238, 0.3888259850209579, 0.3894722709665075, 0.40259739803150296, 0.3983693899353966, 0.40332691685762256, 0.39865443215239793, 0.3961125359637663, 0.39211298001464456, 0.40395753702614456, 0.39548412384465337, 0.3965381379239261, 0.39574412489309907, 0.39874352514743805, 0.40519535494968295, 0.39731230202596635, 0.3987408019602299, 0.40265123010613024, 0.39058758597821, 0.3981665639439598, 0.39457580691669136, 0.3903297109063715, 0.3904777750140056, 0.38025740114971995, 0.37575368501711637, 0.37265012110583484, 0.3657012990443036, 0.36577472696080804, 0.3918398969108239, 0.3983215889893472, 0.3929696949198842, 0.39052740787155926, 0.3907038379693404, 0.39103914611041546, 0.39057888905517757, 0.3914218848804012, 0.3932598860701546, 0.3948022121330723, 0.39128695090766996, 0.3991336530307308, 0.39264973800163716]
Total Epoch List: [114, 83]
Total Time List: [0.07297504798043519, 0.09374879102688283]
========================k_idx:2========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: GPSPerformer
GPS(
  (x_lin): Linear(in_features=14887, out_features=64, bias=True)
  (pe_lin): Linear(in_features=20, out_features=8, bias=True)
  (pe_norm): BatchNorm1d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (convs): ModuleList(
    (0-1): 2 x GPSConv(64, conv=GINEConv(nn=Sequential(
      (0): Linear(in_features=64, out_features=64, bias=True)
      (1): ReLU()
      (2): Linear(in_features=64, out_features=64, bias=True)
    )), heads=1, attn_type=performer)
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=32, bias=True)
    (1): ReLU()
    (2): Linear(in_features=32, out_features=16, bias=True)
    (3): ReLU()
    (4): Linear(in_features=16, out_features=2, bias=True)
  )
)
Total number of parameters: 1039522
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7e7c57038550>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.7141;  Loss pred: 0.7141; Loss self: 0.0000; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 2.6933 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 3.0704 score: 0.5000 time: 0.08s
Epoch 2/1000, LR 0.000000
Train loss: 0.6661;  Loss pred: 0.6661; Loss self: 0.0000; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 2.2888 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 2.5911 score: 0.5000 time: 0.08s
Epoch 3/1000, LR 0.000030
Train loss: 0.7484;  Loss pred: 0.7484; Loss self: 0.0000; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 1.9479 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 2.2004 score: 0.5000 time: 0.08s
Epoch 4/1000, LR 0.000060
Train loss: 0.7359;  Loss pred: 0.7359; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 1.6617 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 1.8387 score: 0.5000 time: 0.08s
Epoch 5/1000, LR 0.000090
Train loss: 0.7400;  Loss pred: 0.7400; Loss self: 0.0000; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 1.4241 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 1.5140 score: 0.5000 time: 0.08s
Epoch 6/1000, LR 0.000120
Train loss: 0.6526;  Loss pred: 0.6526; Loss self: 0.0000; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 1.2144 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 1.2579 score: 0.5000 time: 0.08s
Epoch 7/1000, LR 0.000150
Train loss: 0.6078;  Loss pred: 0.6078; Loss self: 0.0000; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 1.0485 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 1.0780 score: 0.5000 time: 0.08s
Epoch 8/1000, LR 0.000180
Train loss: 0.5081;  Loss pred: 0.5081; Loss self: 0.0000; time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.9123 score: 0.4898 time: 0.09s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.9336 score: 0.5000 time: 0.08s
Epoch 9/1000, LR 0.000210
Train loss: 0.5165;  Loss pred: 0.5165; Loss self: 0.0000; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.8139 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.8317 score: 0.5000 time: 0.08s
Epoch 10/1000, LR 0.000240
Train loss: 0.4817;  Loss pred: 0.4817; Loss self: 0.0000; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7463 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7507 score: 0.5000 time: 0.08s
Epoch 11/1000, LR 0.000270
Train loss: 0.4673;  Loss pred: 0.4673; Loss self: 0.0000; time: 0.25s
Val loss: 0.7081 score: 0.5102 time: 0.08s
Test loss: 0.7119 score: 0.5208 time: 0.08s
Epoch 12/1000, LR 0.000270
Train loss: 0.4468;  Loss pred: 0.4468; Loss self: 0.0000; time: 0.26s
Val loss: 0.6857 score: 0.5714 time: 0.11s
Test loss: 0.6920 score: 0.5625 time: 0.08s
Epoch 13/1000, LR 0.000270
Train loss: 0.4928;  Loss pred: 0.4928; Loss self: 0.0000; time: 0.24s
Val loss: 0.6708 score: 0.6327 time: 0.07s
Test loss: 0.6815 score: 0.5833 time: 0.08s
Epoch 14/1000, LR 0.000270
Train loss: 0.3973;  Loss pred: 0.3973; Loss self: 0.0000; time: 0.25s
Val loss: 0.6591 score: 0.6735 time: 0.08s
Test loss: 0.6762 score: 0.5833 time: 0.08s
Epoch 15/1000, LR 0.000270
Train loss: 0.3713;  Loss pred: 0.3713; Loss self: 0.0000; time: 0.25s
Val loss: 0.6447 score: 0.7347 time: 0.08s
Test loss: 0.6684 score: 0.6458 time: 0.08s
Epoch 16/1000, LR 0.000270
Train loss: 0.3854;  Loss pred: 0.3854; Loss self: 0.0000; time: 0.25s
Val loss: 0.6287 score: 0.7959 time: 0.08s
Test loss: 0.6552 score: 0.7083 time: 0.08s
Epoch 17/1000, LR 0.000270
Train loss: 0.3879;  Loss pred: 0.3879; Loss self: 0.0000; time: 0.28s
Val loss: 0.6159 score: 0.8163 time: 0.08s
Test loss: 0.6396 score: 0.7708 time: 0.08s
Epoch 18/1000, LR 0.000270
Train loss: 0.3092;  Loss pred: 0.3092; Loss self: 0.0000; time: 0.25s
Val loss: 0.6081 score: 0.7755 time: 0.08s
Test loss: 0.6288 score: 0.7917 time: 0.08s
Epoch 19/1000, LR 0.000270
Train loss: 0.3066;  Loss pred: 0.3066; Loss self: 0.0000; time: 0.24s
Val loss: 0.6021 score: 0.7551 time: 0.08s
Test loss: 0.6198 score: 0.8125 time: 0.08s
Epoch 20/1000, LR 0.000270
Train loss: 0.3152;  Loss pred: 0.3152; Loss self: 0.0000; time: 0.27s
Val loss: 0.5983 score: 0.7755 time: 0.08s
Test loss: 0.6151 score: 0.8125 time: 0.08s
Epoch 21/1000, LR 0.000270
Train loss: 0.2830;  Loss pred: 0.2830; Loss self: 0.0000; time: 0.25s
Val loss: 0.5950 score: 0.7755 time: 0.13s
Test loss: 0.6140 score: 0.8333 time: 0.08s
Epoch 22/1000, LR 0.000270
Train loss: 0.2409;  Loss pred: 0.2409; Loss self: 0.0000; time: 0.24s
Val loss: 0.5921 score: 0.7755 time: 0.08s
Test loss: 0.6145 score: 0.8333 time: 0.08s
Epoch 23/1000, LR 0.000270
Train loss: 0.2435;  Loss pred: 0.2435; Loss self: 0.0000; time: 0.24s
Val loss: 0.5883 score: 0.7959 time: 0.08s
Test loss: 0.6161 score: 0.8125 time: 0.08s
Epoch 24/1000, LR 0.000270
Train loss: 0.2118;  Loss pred: 0.2118; Loss self: 0.0000; time: 0.25s
Val loss: 0.5833 score: 0.7755 time: 0.08s
Test loss: 0.6108 score: 0.8125 time: 0.08s
Epoch 25/1000, LR 0.000270
Train loss: 0.1837;  Loss pred: 0.1837; Loss self: 0.0000; time: 0.25s
Val loss: 0.5693 score: 0.7959 time: 0.07s
Test loss: 0.5858 score: 0.8125 time: 0.08s
Epoch 26/1000, LR 0.000270
Train loss: 0.1803;  Loss pred: 0.1803; Loss self: 0.0000; time: 0.24s
Val loss: 0.5510 score: 0.7755 time: 0.07s
Test loss: 0.5521 score: 0.8333 time: 0.08s
Epoch 27/1000, LR 0.000270
Train loss: 0.1555;  Loss pred: 0.1555; Loss self: 0.0000; time: 0.25s
Val loss: 0.5375 score: 0.7551 time: 0.07s
Test loss: 0.5166 score: 0.7917 time: 0.08s
Epoch 28/1000, LR 0.000270
Train loss: 0.1472;  Loss pred: 0.1472; Loss self: 0.0000; time: 0.25s
Val loss: 0.5305 score: 0.7347 time: 0.07s
Test loss: 0.5013 score: 0.7500 time: 0.08s
Epoch 29/1000, LR 0.000270
Train loss: 0.1282;  Loss pred: 0.1282; Loss self: 0.0000; time: 0.24s
Val loss: 0.5258 score: 0.7347 time: 0.07s
Test loss: 0.4937 score: 0.7708 time: 0.08s
Epoch 30/1000, LR 0.000270
Train loss: 0.1365;  Loss pred: 0.1365; Loss self: 0.0000; time: 0.25s
Val loss: 0.5224 score: 0.7347 time: 0.07s
Test loss: 0.4889 score: 0.7708 time: 0.08s
Epoch 31/1000, LR 0.000270
Train loss: 0.1143;  Loss pred: 0.1143; Loss self: 0.0000; time: 0.25s
Val loss: 0.5179 score: 0.7347 time: 0.07s
Test loss: 0.4843 score: 0.7708 time: 0.08s
Epoch 32/1000, LR 0.000270
Train loss: 0.0919;  Loss pred: 0.0919; Loss self: 0.0000; time: 0.25s
Val loss: 0.5136 score: 0.7551 time: 0.08s
Test loss: 0.4806 score: 0.7708 time: 0.08s
Epoch 33/1000, LR 0.000270
Train loss: 0.0902;  Loss pred: 0.0902; Loss self: 0.0000; time: 0.25s
Val loss: 0.5100 score: 0.7551 time: 0.07s
Test loss: 0.4785 score: 0.8333 time: 0.08s
Epoch 34/1000, LR 0.000270
Train loss: 0.0993;  Loss pred: 0.0993; Loss self: 0.0000; time: 0.24s
Val loss: 0.5067 score: 0.7755 time: 0.07s
Test loss: 0.4769 score: 0.8750 time: 0.08s
Epoch 35/1000, LR 0.000270
Train loss: 0.0831;  Loss pred: 0.0831; Loss self: 0.0000; time: 0.25s
Val loss: 0.5045 score: 0.7755 time: 0.07s
Test loss: 0.4725 score: 0.8542 time: 0.08s
Epoch 36/1000, LR 0.000270
Train loss: 0.0700;  Loss pred: 0.0700; Loss self: 0.0000; time: 0.25s
Val loss: 0.5032 score: 0.7755 time: 0.07s
Test loss: 0.4690 score: 0.8542 time: 0.08s
Epoch 37/1000, LR 0.000270
Train loss: 0.0773;  Loss pred: 0.0773; Loss self: 0.0000; time: 0.25s
Val loss: 0.5025 score: 0.7755 time: 0.07s
Test loss: 0.4656 score: 0.8542 time: 0.08s
Epoch 38/1000, LR 0.000270
Train loss: 0.0592;  Loss pred: 0.0592; Loss self: 0.0000; time: 0.25s
Val loss: 0.5014 score: 0.8163 time: 0.07s
Test loss: 0.4600 score: 0.8333 time: 0.07s
Epoch 39/1000, LR 0.000269
Train loss: 0.0560;  Loss pred: 0.0560; Loss self: 0.0000; time: 0.23s
Val loss: 0.5005 score: 0.7959 time: 0.07s
Test loss: 0.4549 score: 0.7917 time: 0.07s
Epoch 40/1000, LR 0.000269
Train loss: 0.0609;  Loss pred: 0.0609; Loss self: 0.0000; time: 0.23s
Val loss: 0.4997 score: 0.7959 time: 0.07s
Test loss: 0.4495 score: 0.7917 time: 0.07s
Epoch 41/1000, LR 0.000269
Train loss: 0.0575;  Loss pred: 0.0575; Loss self: 0.0000; time: 0.24s
Val loss: 0.4975 score: 0.7959 time: 0.07s
Test loss: 0.4438 score: 0.8125 time: 0.07s
Epoch 42/1000, LR 0.000269
Train loss: 0.0532;  Loss pred: 0.0532; Loss self: 0.0000; time: 0.23s
Val loss: 0.4941 score: 0.8163 time: 0.07s
Test loss: 0.4366 score: 0.8542 time: 0.07s
Epoch 43/1000, LR 0.000269
Train loss: 0.0501;  Loss pred: 0.0501; Loss self: 0.0000; time: 0.23s
Val loss: 0.4894 score: 0.8163 time: 0.07s
Test loss: 0.4278 score: 0.8750 time: 0.07s
Epoch 44/1000, LR 0.000269
Train loss: 0.0508;  Loss pred: 0.0508; Loss self: 0.0000; time: 0.24s
Val loss: 0.4848 score: 0.8367 time: 0.07s
Test loss: 0.4202 score: 0.8750 time: 0.08s
Epoch 45/1000, LR 0.000269
Train loss: 0.0440;  Loss pred: 0.0440; Loss self: 0.0000; time: 0.25s
Val loss: 0.4802 score: 0.8571 time: 0.07s
Test loss: 0.4119 score: 0.8750 time: 0.07s
Epoch 46/1000, LR 0.000269
Train loss: 0.0446;  Loss pred: 0.0446; Loss self: 0.0000; time: 0.24s
Val loss: 0.4757 score: 0.8571 time: 0.07s
Test loss: 0.4031 score: 0.8958 time: 0.07s
Epoch 47/1000, LR 0.000269
Train loss: 0.0365;  Loss pred: 0.0365; Loss self: 0.0000; time: 0.23s
Val loss: 0.4703 score: 0.8776 time: 0.07s
Test loss: 0.3930 score: 0.8750 time: 0.07s
Epoch 48/1000, LR 0.000269
Train loss: 0.0397;  Loss pred: 0.0397; Loss self: 0.0000; time: 0.24s
Val loss: 0.4648 score: 0.8980 time: 0.07s
Test loss: 0.3836 score: 0.8750 time: 0.07s
Epoch 49/1000, LR 0.000269
Train loss: 0.0409;  Loss pred: 0.0409; Loss self: 0.0000; time: 0.23s
Val loss: 0.4589 score: 0.8776 time: 0.07s
Test loss: 0.3746 score: 0.8750 time: 0.07s
Epoch 50/1000, LR 0.000269
Train loss: 0.0332;  Loss pred: 0.0332; Loss self: 0.0000; time: 0.25s
Val loss: 0.4523 score: 0.8571 time: 0.07s
Test loss: 0.3658 score: 0.8750 time: 0.07s
Epoch 51/1000, LR 0.000269
Train loss: 0.0276;  Loss pred: 0.0276; Loss self: 0.0000; time: 0.24s
Val loss: 0.4454 score: 0.8571 time: 0.07s
Test loss: 0.3600 score: 0.8542 time: 0.07s
Epoch 52/1000, LR 0.000269
Train loss: 0.0263;  Loss pred: 0.0263; Loss self: 0.0000; time: 0.23s
Val loss: 0.4396 score: 0.8571 time: 0.07s
Test loss: 0.3557 score: 0.8542 time: 0.07s
Epoch 53/1000, LR 0.000269
Train loss: 0.0264;  Loss pred: 0.0264; Loss self: 0.0000; time: 0.24s
Val loss: 0.4343 score: 0.8571 time: 0.07s
Test loss: 0.3509 score: 0.8542 time: 0.07s
Epoch 54/1000, LR 0.000269
Train loss: 0.0247;  Loss pred: 0.0247; Loss self: 0.0000; time: 0.24s
Val loss: 0.4293 score: 0.8571 time: 0.07s
Test loss: 0.3452 score: 0.8750 time: 0.07s
Epoch 55/1000, LR 0.000269
Train loss: 0.0186;  Loss pred: 0.0186; Loss self: 0.0000; time: 0.24s
Val loss: 0.4233 score: 0.8571 time: 0.07s
Test loss: 0.3376 score: 0.8750 time: 0.07s
Epoch 56/1000, LR 0.000269
Train loss: 0.0213;  Loss pred: 0.0213; Loss self: 0.0000; time: 0.24s
Val loss: 0.4149 score: 0.8571 time: 0.07s
Test loss: 0.3281 score: 0.8750 time: 0.07s
Epoch 57/1000, LR 0.000269
Train loss: 0.0197;  Loss pred: 0.0197; Loss self: 0.0000; time: 0.24s
Val loss: 0.4057 score: 0.8571 time: 0.08s
Test loss: 0.3166 score: 0.8750 time: 0.08s
Epoch 58/1000, LR 0.000269
Train loss: 0.0176;  Loss pred: 0.0176; Loss self: 0.0000; time: 0.24s
Val loss: 0.3978 score: 0.8571 time: 0.07s
Test loss: 0.3038 score: 0.8750 time: 0.07s
Epoch 59/1000, LR 0.000268
Train loss: 0.0139;  Loss pred: 0.0139; Loss self: 0.0000; time: 0.23s
Val loss: 0.3913 score: 0.8571 time: 0.07s
Test loss: 0.2926 score: 0.8958 time: 0.07s
Epoch 60/1000, LR 0.000268
Train loss: 0.0140;  Loss pred: 0.0140; Loss self: 0.0000; time: 0.24s
Val loss: 0.3882 score: 0.8163 time: 0.07s
Test loss: 0.2838 score: 0.8958 time: 0.07s
Epoch 61/1000, LR 0.000268
Train loss: 0.0148;  Loss pred: 0.0148; Loss self: 0.0000; time: 0.23s
Val loss: 0.3864 score: 0.8163 time: 0.07s
Test loss: 0.2785 score: 0.8958 time: 0.07s
Epoch 62/1000, LR 0.000268
Train loss: 0.0140;  Loss pred: 0.0140; Loss self: 0.0000; time: 0.23s
Val loss: 0.3826 score: 0.8163 time: 0.07s
Test loss: 0.2721 score: 0.8958 time: 0.07s
Epoch 63/1000, LR 0.000268
Train loss: 0.0123;  Loss pred: 0.0123; Loss self: 0.0000; time: 0.24s
Val loss: 0.3776 score: 0.8163 time: 0.07s
Test loss: 0.2650 score: 0.9167 time: 0.07s
Epoch 64/1000, LR 0.000268
Train loss: 0.0111;  Loss pred: 0.0111; Loss self: 0.0000; time: 0.24s
Val loss: 0.3720 score: 0.8163 time: 0.07s
Test loss: 0.2589 score: 0.8958 time: 0.07s
Epoch 65/1000, LR 0.000268
Train loss: 0.0093;  Loss pred: 0.0093; Loss self: 0.0000; time: 0.24s
Val loss: 0.3657 score: 0.8163 time: 0.08s
Test loss: 0.2535 score: 0.9167 time: 0.07s
Epoch 66/1000, LR 0.000268
Train loss: 0.0115;  Loss pred: 0.0115; Loss self: 0.0000; time: 0.24s
Val loss: 0.3615 score: 0.8163 time: 0.07s
Test loss: 0.2489 score: 0.9167 time: 0.07s
Epoch 67/1000, LR 0.000268
Train loss: 0.0103;  Loss pred: 0.0103; Loss self: 0.0000; time: 0.24s
Val loss: 0.3572 score: 0.8163 time: 0.07s
Test loss: 0.2439 score: 0.9167 time: 0.07s
Epoch 68/1000, LR 0.000268
Train loss: 0.0085;  Loss pred: 0.0085; Loss self: 0.0000; time: 0.24s
Val loss: 0.3539 score: 0.8163 time: 0.07s
Test loss: 0.2399 score: 0.9167 time: 0.07s
Epoch 69/1000, LR 0.000268
Train loss: 0.0092;  Loss pred: 0.0092; Loss self: 0.0000; time: 0.24s
Val loss: 0.3513 score: 0.8571 time: 0.07s
Test loss: 0.2353 score: 0.8958 time: 0.07s
Epoch 70/1000, LR 0.000268
Train loss: 0.0085;  Loss pred: 0.0085; Loss self: 0.0000; time: 0.24s
Val loss: 0.3496 score: 0.8571 time: 0.07s
Test loss: 0.2304 score: 0.8958 time: 0.08s
Epoch 71/1000, LR 0.000268
Train loss: 0.0073;  Loss pred: 0.0073; Loss self: 0.0000; time: 0.25s
Val loss: 0.3490 score: 0.8571 time: 0.07s
Test loss: 0.2257 score: 0.9167 time: 0.08s
Epoch 72/1000, LR 0.000267
Train loss: 0.0053;  Loss pred: 0.0053; Loss self: 0.0000; time: 0.24s
Val loss: 0.3511 score: 0.8571 time: 0.07s
Test loss: 0.2233 score: 0.9167 time: 0.07s
     INFO: Early stopping counter 1 of 20
Epoch 73/1000, LR 0.000267
Train loss: 0.0054;  Loss pred: 0.0054; Loss self: 0.0000; time: 0.24s
Val loss: 0.3543 score: 0.8367 time: 0.07s
Test loss: 0.2212 score: 0.9167 time: 0.08s
     INFO: Early stopping counter 2 of 20
Epoch 74/1000, LR 0.000267
Train loss: 0.0043;  Loss pred: 0.0043; Loss self: 0.0000; time: 0.25s
Val loss: 0.3585 score: 0.8571 time: 0.07s
Test loss: 0.2205 score: 0.9167 time: 0.08s
     INFO: Early stopping counter 3 of 20
Epoch 75/1000, LR 0.000267
Train loss: 0.0055;  Loss pred: 0.0055; Loss self: 0.0000; time: 0.24s
Val loss: 0.3590 score: 0.8367 time: 0.07s
Test loss: 0.2178 score: 0.9167 time: 0.08s
     INFO: Early stopping counter 4 of 20
Epoch 76/1000, LR 0.000267
Train loss: 0.0047;  Loss pred: 0.0047; Loss self: 0.0000; time: 0.24s
Val loss: 0.3591 score: 0.8367 time: 0.07s
Test loss: 0.2154 score: 0.9167 time: 0.08s
     INFO: Early stopping counter 5 of 20
Epoch 77/1000, LR 0.000267
Train loss: 0.0044;  Loss pred: 0.0044; Loss self: 0.0000; time: 0.24s
Val loss: 0.3568 score: 0.8571 time: 0.07s
Test loss: 0.2101 score: 0.9375 time: 0.08s
     INFO: Early stopping counter 6 of 20
Epoch 78/1000, LR 0.000267
Train loss: 0.0035;  Loss pred: 0.0035; Loss self: 0.0000; time: 0.23s
Val loss: 0.3525 score: 0.8367 time: 0.08s
Test loss: 0.2031 score: 0.9375 time: 0.08s
     INFO: Early stopping counter 7 of 20
Epoch 79/1000, LR 0.000267
Train loss: 0.0045;  Loss pred: 0.0045; Loss self: 0.0000; time: 0.23s
Val loss: 0.3497 score: 0.8367 time: 0.07s
Test loss: 0.1972 score: 0.9375 time: 0.08s
     INFO: Early stopping counter 8 of 20
Epoch 80/1000, LR 0.000267
Train loss: 0.0037;  Loss pred: 0.0037; Loss self: 0.0000; time: 0.24s
Val loss: 0.3491 score: 0.8367 time: 0.07s
Test loss: 0.1929 score: 0.9375 time: 0.08s
     INFO: Early stopping counter 9 of 20
Epoch 81/1000, LR 0.000267
Train loss: 0.0045;  Loss pred: 0.0045; Loss self: 0.0000; time: 0.23s
Val loss: 0.3505 score: 0.8367 time: 0.07s
Test loss: 0.1907 score: 0.9375 time: 0.08s
     INFO: Early stopping counter 10 of 20
Epoch 82/1000, LR 0.000267
Train loss: 0.0037;  Loss pred: 0.0037; Loss self: 0.0000; time: 0.23s
Val loss: 0.3523 score: 0.8367 time: 0.07s
Test loss: 0.1878 score: 0.9375 time: 0.08s
     INFO: Early stopping counter 11 of 20
Epoch 83/1000, LR 0.000266
Train loss: 0.0033;  Loss pred: 0.0033; Loss self: 0.0000; time: 0.23s
Val loss: 0.3543 score: 0.8367 time: 0.07s
Test loss: 0.1840 score: 0.9375 time: 0.08s
     INFO: Early stopping counter 12 of 20
Epoch 84/1000, LR 0.000266
Train loss: 0.0034;  Loss pred: 0.0034; Loss self: 0.0000; time: 0.26s
Val loss: 0.3561 score: 0.8367 time: 0.08s
Test loss: 0.1804 score: 0.9375 time: 0.09s
     INFO: Early stopping counter 13 of 20
Epoch 85/1000, LR 0.000266
Train loss: 0.0029;  Loss pred: 0.0029; Loss self: 0.0000; time: 0.26s
Val loss: 0.3570 score: 0.8367 time: 0.08s
Test loss: 0.1777 score: 0.9375 time: 0.08s
     INFO: Early stopping counter 14 of 20
Epoch 86/1000, LR 0.000266
Train loss: 0.0029;  Loss pred: 0.0029; Loss self: 0.0000; time: 0.24s
Val loss: 0.3585 score: 0.8367 time: 0.07s
Test loss: 0.1764 score: 0.9375 time: 0.08s
     INFO: Early stopping counter 15 of 20
Epoch 87/1000, LR 0.000266
Train loss: 0.0028;  Loss pred: 0.0028; Loss self: 0.0000; time: 0.26s
Val loss: 0.3598 score: 0.8367 time: 0.08s
Test loss: 0.1747 score: 0.9375 time: 0.08s
     INFO: Early stopping counter 16 of 20
Epoch 88/1000, LR 0.000266
Train loss: 0.0028;  Loss pred: 0.0028; Loss self: 0.0000; time: 0.25s
Val loss: 0.3619 score: 0.8367 time: 0.09s
Test loss: 0.1750 score: 0.9375 time: 0.09s
     INFO: Early stopping counter 17 of 20
Epoch 89/1000, LR 0.000266
Train loss: 0.0027;  Loss pred: 0.0027; Loss self: 0.0000; time: 0.27s
Val loss: 0.3642 score: 0.8571 time: 0.08s
Test loss: 0.1754 score: 0.9375 time: 0.08s
     INFO: Early stopping counter 18 of 20
Epoch 90/1000, LR 0.000266
Train loss: 0.0026;  Loss pred: 0.0026; Loss self: 0.0000; time: 0.25s
Val loss: 0.3671 score: 0.8571 time: 0.07s
Test loss: 0.1766 score: 0.9375 time: 0.08s
     INFO: Early stopping counter 19 of 20
Epoch 91/1000, LR 0.000266
Train loss: 0.0019;  Loss pred: 0.0019; Loss self: 0.0000; time: 0.25s
Val loss: 0.3706 score: 0.8571 time: 0.08s
Test loss: 0.1779 score: 0.9375 time: 0.08s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 070,   Train_Loss: 0.0073,   Val_Loss: 0.3490,   Val_Precision: 0.9091,   Val_Recall: 0.8000,   Val_accuracy: 0.8511,   Val_Score: 0.8571,   Val_Loss: 0.3490,   Test_Precision: 0.9545,   Test_Recall: 0.8750,   Test_accuracy: 0.9130,   Test_Score: 0.9167,   Test_loss: 0.2257


[0.07090797193814069, 0.07478468609042466, 0.07500295399222523, 0.07073386700358242, 0.07147020800039172, 0.07484937901608646, 0.07126277300994843, 0.07091055996716022, 0.07253571297042072, 0.07160807703621686, 0.07112984103150666, 0.07123727397993207, 0.07121164095588028, 0.0712842799257487, 0.07024876901414245, 0.07034641597419977, 0.07005608198232949, 0.07109783298801631, 0.07092070404905826, 0.0712843060027808, 0.07131313404534012, 0.0718663620064035, 0.07100645697209984, 0.07151592103764415, 0.07765654497779906, 0.07291227101814002, 0.07112269895151258, 0.07108279899694026, 0.07178047299385071, 0.07115971902385354, 0.07150786696001887, 0.07094577106181532, 0.07115589594468474, 0.07083884405437857, 0.07096579298377037, 0.07502702297642827, 0.07140305102802813, 0.07178505603224039, 0.07167405402287841, 0.07145116897299886, 0.0722004669951275, 0.07076698099263012, 0.07190200395416468, 0.07137341995257884, 0.07180558203253895, 0.07747737294994295, 0.07223098701797426, 0.07189775398001075, 0.0717406690819189, 0.07102096604648978, 0.07139467296656221, 0.07099431299138814, 0.07107717299368232, 0.0706796069862321, 0.07016047206707299, 0.07091534102801234, 0.07157608598936349, 0.07048087299335748, 0.07092845602892339, 0.0702437279978767, 0.07045311003457755, 0.07111681206151843, 0.07129402400460094, 0.07105314393993467, 0.07075407507363707, 0.07076400797814131, 0.06998341006692499, 0.07011971296742558, 0.07016182201914489, 0.07048073399346322, 0.08782673696987331, 0.07231053803116083, 0.0724047739058733, 0.07273302599787712, 0.17593855503946543, 0.07614254206418991, 0.07665180193725973, 0.07801140903029591, 0.07973117695655674, 0.0759186880895868, 0.07701897202059627, 0.07802655303385109, 0.0763513109413907, 0.0727422129129991, 0.07103690505027771, 0.07228322105947882, 0.07546074304264039, 0.07256152993068099, 0.07305358001030982, 0.07290938508231193, 0.07253968110308051, 0.07243451103568077, 0.07161806209478527, 0.07154188491404057, 0.07252215396147221, 0.07172629598062485, 0.07195148302707821, 0.07302372204139829, 0.07268997200299054, 0.07307923794724047, 0.07110046199522913, 0.07179522595833987, 0.07195230794604868, 0.07145465794019401, 0.07174865598790348, 0.07181940204463899, 0.07156928698532283, 0.0713540849974379, 0.07209255499765277, 0.07278985495213419, 0.07229635899420828, 0.07182803004980087, 0.07231369498185813, 0.07244545791763812, 0.08751676906831563, 0.08721476094797254, 0.08656603703275323, 0.08635204005986452, 0.09085932490415871, 0.08634752908255905, 0.08886271098162979, 0.08646483393386006, 0.08683307701721787, 0.08592347207013518, 0.0869856639765203, 0.0867934999987483, 0.08568585896864533, 0.08483078796416521, 0.08642486203461885, 0.08692113705910742, 0.09218003402929753, 0.09150914300698787, 0.09118365996982902, 0.09243821701966226, 0.09140098700299859, 0.09140443208161741, 0.09211764007341117, 0.08632600400596857, 0.0865552700124681, 0.09079325501807034, 0.09168276702985168, 0.09136055794078857, 0.08707353100180626, 0.09132846596185118, 0.08754789794329554, 0.09116107400041074, 0.09252880397252738, 0.09178362798411399, 0.08604962704703212, 0.09267976507544518, 0.09211387997493148, 0.09008388104848564, 0.09089766489341855, 0.09144063596613705, 0.08700535399839282, 0.09100335801485926, 0.09170152293518186, 0.09055424807593226, 0.09171077597420663, 0.09223139600362629, 0.08808110002428293, 0.0922017099801451, 0.09291928994935006, 0.09258017793763429, 0.08854048303328454, 0.09297738794703037, 0.09257320198230445, 0.09273457201197743, 0.09271962696220726, 0.09313755307812244, 0.09253014309797436, 0.08970933500677347, 0.08785875292960554, 0.09223154804203659, 0.08697170403320342, 0.08804732304997742, 0.09211626905016601, 0.08749892702326179, 0.08735846600029618, 0.0832109299954027, 0.08929368504323065, 0.08731849805917591, 0.08764384198002517, 0.08263873599935323, 0.09387869108468294, 0.09199103503488004, 0.09210143506061286, 0.09127930097747594, 0.09140647202730179, 0.0914184870198369, 0.09170030406676233, 0.091788889025338, 0.09182567708194256, 0.0911584870191291, 0.09160291007719934, 0.09241285896860063, 0.09301857894752175, 0.08303866093046963, 0.08226492791436613, 0.08282806805800647, 0.0829040720127523, 0.08240376401226968, 0.0838663560571149, 0.08427901996765286, 0.0830507599748671, 0.08308668003883213, 0.08324715297203511, 0.08307377796154469, 0.08234154002275318, 0.08243044896516949, 0.08266246900893748, 0.08380719996057451, 0.083062847959809, 0.08322709996718913, 0.08343882602639496, 0.08438823104370385, 0.08330834598746151, 0.08294810599181801, 0.08211059402674437, 0.08267005102243274, 0.08360443299170583, 0.08189456607215106, 0.08198770100716501, 0.08181950694415718, 0.08221971290186048, 0.08191960700787604, 0.08243480208329856, 0.08245803194586188, 0.08309225097764283, 0.08224345499183983, 0.08238020807038993, 0.08208264294080436, 0.0825257160468027, 0.08241821802221239, 0.07908015395514667, 0.07845293404534459, 0.07844677497632802, 0.07932641205843538, 0.07895350502803922, 0.07867772795725614, 0.08009541791398078, 0.07874141505453736, 0.0778952669352293, 0.07812976103741676, 0.07857015705667436, 0.07825106999371201, 0.07835892995353788, 0.07849143003113568, 0.07868694700300694, 0.07830110797658563, 0.07804684003349394, 0.079564368003048, 0.07884823402855545, 0.07996567292138934, 0.07913241197820753, 0.07844249298796058, 0.07855140510946512, 0.07848428003489971, 0.07851799298077822, 0.07908970909193158, 0.07863958005327731, 0.07881222001742572, 0.07896855403669178, 0.07834084704518318, 0.07939352991525084, 0.07949511101469398, 0.08107088005635887, 0.07987781590782106, 0.07947540504392236, 0.08299038698896766, 0.08067219296935946, 0.08006757299881428, 0.07996068801730871, 0.08013493800535798, 0.08052151906304061, 0.0834078179905191, 0.0804379329783842, 0.08053603698499501, 0.08056090993341058, 0.08355379407294095, 0.09149572311434895, 0.08422189496923238, 0.08202037599403411, 0.08496346801985055, 0.09388212603516877, 0.08795934007503092, 0.0827571339905262, 0.08200372196733952]
[0.0014471014681253203, 0.0015262180834780543, 0.001530672530453576, 0.0014435483061955596, 0.001458575673477382, 0.0015275383472670705, 0.0014543423063254782, 0.0014471542850440862, 0.0014803206728657289, 0.0014613893272697317, 0.0014516294088062582, 0.0014538219179577973, 0.001453298795017965, 0.0014547812229744634, 0.001433648347227397, 0.0014356411423306077, 0.0014297159588230507, 0.0014509761834289043, 0.001447361307123638, 0.0014547817551587917, 0.0014553700825579617, 0.0014666604491102757, 0.0014491113667775477, 0.0014595085926049826, 0.0015848274485265113, 0.0014880055309824493, 0.0014514836520716852, 0.001450669367284495, 0.0014649076121194021, 0.0014522391637521131, 0.0014593442236738546, 0.0014478728788125577, 0.00145216114172826, 0.0014456906949873178, 0.0014482814894647015, 0.001531163734212822, 0.0014572051230209823, 0.00146500114351511, 0.0014627357963852736, 0.0014581871218979359, 0.0014734789182679082, 0.0014442241018904106, 0.0014673878357992793, 0.0014566004071954865, 0.0014654200414803867, 0.0015811708765294478, 0.0014741017758770256, 0.0014673011016328723, 0.0014640952873860998, 0.001449407470336526, 0.001457034142174739, 0.0014488635304364928, 0.001450554550891476, 0.0014424409589026959, 0.0014318463687157752, 0.0014472518577145375, 0.0014607364487625202, 0.0014383851631297444, 0.001447519510794355, 0.0014335454693444225, 0.0014378185721342356, 0.0014513635114595598, 0.0014549800817265498, 0.001450064162039483, 0.0014439607157885116, 0.0014441634281253328, 0.0014282328585086732, 0.0014310145503556241, 0.001431873918758059, 0.0014383823263972085, 0.0017923823871402716, 0.0014757252659420579, 0.0014776484470586388, 0.001484347469344431, 0.003590582755907458, 0.0015539294298814268, 0.0015643224885155047, 0.0015920695720468552, 0.0016271668766644231, 0.0015493609814201386, 0.0015718157555223728, 0.0015923786333438997, 0.001558190019212055, 0.0014845349574081448, 0.0014497327561281165, 0.0014751677767240576, 0.0015400151641355182, 0.0014808475496057346, 0.0014908893879655065, 0.0014879466343328965, 0.0014804016551649083, 0.0014782553272587912, 0.0014615931039752097, 0.0014600384676334809, 0.0014800439583973922, 0.0014638019587882624, 0.0014683976127975145, 0.0014902800416611895, 0.001483468816387562, 0.0014914130193314382, 0.001451029836637329, 0.0014652086930273442, 0.0014684144478785445, 0.0014582583253100819, 0.001464258285467418, 0.0014657020825436528, 0.001460597693578017, 0.0014562058162742428, 0.0014712766326051585, 0.001485507243921106, 0.0014754358978409851, 0.0014658781642816505, 0.0014757896935073088, 0.001478478733013023, 0.0017860565115982781, 0.0017798930805708682, 0.0017666538169949638, 0.0017622865318339697, 0.0018542719368195655, 0.0017621944710726337, 0.001813524713910812, 0.0017645884476297973, 0.001772103612596283, 0.0017535402463292893, 0.0017752176321738837, 0.0017712959183418021, 0.0017486909993601088, 0.0017312405706972492, 0.0017637726945840583, 0.0017739007563083147, 0.0018812251842713781, 0.0018675335307548546, 0.0018608910197924291, 0.0018864942248910666, 0.0018653262653673182, 0.001865396573094233, 0.001879951838232881, 0.001761755183795277, 0.001766434081887104, 0.0018529235717973538, 0.0018710768781602383, 0.0018645011824650727, 0.0017770108367715562, 0.001863846244119412, 0.0017866917947611334, 0.0018604300816410355, 0.0018883429382148447, 0.001873135264981918, 0.0017561148376945331, 0.0018914237770499016, 0.0018798751015292139, 0.001838446552009911, 0.0018550543855799704, 0.001866135427880348, 0.0017756194693549555, 0.0018572113880583523, 0.0018714596517384052, 0.0018480458791006584, 0.001871648489269523, 0.0018822733878291079, 0.0017975734698833252, 0.001881667550615206, 0.0018963120397826542, 0.0018893913864823325, 0.0018069486333323376, 0.0018974977132047014, 0.0018892490200470298, 0.0018925422859587232, 0.0018922372849430053, 0.0019007663893494376, 0.0018883702673055992, 0.001830802755240275, 0.0017930357740735825, 0.001882276490653808, 0.0017749327353714984, 0.0017968841438770903, 0.0018799238581666533, 0.0017856923882298324, 0.0017828258367407384, 0.0016981822448041365, 0.0018223201029230745, 0.0017820101644729777, 0.0017886498363270443, 0.0016865048163133313, 0.0019158916547894478, 0.0018773680619363273, 0.0018796211236859768, 0.0018628428770913457, 0.001865438204638812, 0.0018656834085681, 0.0018714347768727007, 0.0018732426331701632, 0.0018739934098355624, 0.0018603772861046754, 0.0018694471444326397, 0.0018859767136449109, 0.0018983383458677906, 0.0017299721027181174, 0.0017138526648826276, 0.001725584751208468, 0.0017271681669323395, 0.0017167450835889515, 0.0017472157511898938, 0.0017558129159927678, 0.0017302241661430646, 0.0017309725008090027, 0.0017343156869173981, 0.0017307037075321812, 0.0017154487504740246, 0.0017173010201076977, 0.0017221347710195307, 0.001745983332511969, 0.0017304759991626877, 0.001733897915983107, 0.001738308875549895, 0.0017580881467438303, 0.001735590541405448, 0.0017280855414962086, 0.0017106373755571742, 0.0017222927296340156, 0.0017417590206605382, 0.001706136793169814, 0.0017080771043159377, 0.001704573061336608, 0.0017129106854554266, 0.0017066584793307509, 0.0017173917100687202, 0.001717875665538789, 0.0017310885620342258, 0.0017134053123299964, 0.00171625433479979, 0.0017100550612667575, 0.0017192857509750563, 0.0017170462087960914, 0.001647503207398889, 0.001634436125944679, 0.0016343078120068337, 0.0016526335845507372, 0.0016448646880841504, 0.0016391193324428361, 0.0016686545398745996, 0.0016404461469695282, 0.0016228180611506104, 0.0016277033549461823, 0.0016368782720140491, 0.0016302306248690002, 0.0016324777073653725, 0.00163523812564866, 0.0016393113958959777, 0.001631273082845534, 0.0016259758340311237, 0.0016575910000635001, 0.0016426715422615719, 0.0016659515191956114, 0.0016485919162126568, 0.0016342186039158453, 0.00163648760644719, 0.001635089167393744, 0.0016357915204328795, 0.0016477022727485746, 0.0016383245844432774, 0.0016419212503630358, 0.0016451782090977456, 0.0016321009801079829, 0.0016540318732343924, 0.0016561481461394578, 0.0016889766678408098, 0.0016641211647462721, 0.0016557376050817159, 0.0017289663956034929, 0.0016806706868616554, 0.0016680744374752976, 0.0016658476670272648, 0.0016694778751116246, 0.0016775316471466795, 0.0017376628748024814, 0.001675790270383004, 0.0016778341038540627, 0.0016783522902793873, 0.0017407040431862697, 0.001906160898215603, 0.001754622811859008, 0.001708757833209044, 0.001770072250413553, 0.0019558776257326826, 0.0018324862515631442, 0.0017241069581359625, 0.0017084108743195732]
[691.0365458307995, 655.2143568638165, 653.3076017923149, 692.7374689908912, 685.6003553219186, 654.6480497783292, 687.596032688196, 691.0113250084706, 675.5293081627484, 684.2803497602305, 688.8810559592796, 687.842154288548, 688.0897468766142, 687.3885806385292, 697.5211194111507, 696.5528992688302, 699.4396291296944, 689.1911882639104, 690.9124867980024, 687.3883291798971, 687.1104552612472, 681.8210722233853, 690.078087113307, 685.1621189945614, 630.9835186977276, 672.0405127390583, 688.9502327998748, 689.3369519974755, 682.6369060593643, 688.5918139105446, 685.2392902083996, 690.6683691873067, 688.6288107185338, 691.7108918714957, 690.4735075842262, 653.0980179687344, 686.2451855280782, 682.59332385271, 683.6504599608551, 685.7830418214281, 678.6659704473491, 692.4133163897864, 681.483092338233, 686.5300840642924, 682.398200989381, 632.4427137153737, 678.3792112352921, 681.5233757319198, 683.0156538413115, 689.93710910557, 686.3257154066553, 690.196128891956, 689.3915153934914, 693.2692765191077, 698.3989496700691, 690.9647375261787, 684.5861899640839, 695.224078802458, 690.8369749373742, 697.5711767672859, 695.4980408381033, 689.007262552958, 687.2946321116311, 689.6246567417556, 692.5396162553664, 692.4424068113254, 700.1659386580534, 698.8049141439463, 698.3855120898868, 695.2254499015935, 557.9166628586939, 677.6329057167904, 676.7509565557147, 673.69670555753, 278.5063227841597, 643.5298674253859, 639.2543783916129, 628.1132543186181, 614.5651158103274, 645.4273807020774, 636.206881427819, 627.9913451865779, 641.7702511698026, 673.611621612403, 689.7823035127914, 677.8889939019176, 649.3442553608531, 675.2889588575429, 670.7405714146355, 672.0671137835117, 675.4923547343681, 676.4731244732627, 684.1849467408005, 684.9134609589162, 675.6556076096625, 683.1525220992337, 681.0144549982291, 671.0148240899189, 674.0957335625894, 670.5050760843392, 689.1657047641677, 682.4966332501398, 681.0066473022827, 685.7495566070994, 682.9396220085459, 682.2668889605109, 684.6512249038997, 686.716114455948, 679.6818340201061, 673.1707328201418, 677.7658056600808, 682.1849348510128, 677.6033227494873, 676.3709058987132, 559.8926985267312, 561.831500395107, 566.0418528973471, 567.4446135381422, 539.295224256693, 567.4742580433293, 551.4123917525942, 566.704378770701, 564.30108989785, 570.2749064889239, 563.3112142849927, 564.5584058795492, 571.8563201651555, 577.6204745463275, 566.9664821723669, 563.7293949189759, 531.5684737589308, 535.465620044745, 537.376981974766, 530.0837854712987, 536.099243637188, 536.0790377893996, 531.9285205412395, 567.6157556952612, 566.1122655263123, 539.6876672198574, 534.451583295318, 536.336479378303, 562.7427696596287, 536.5249430606641, 559.693620876393, 537.5101219165016, 529.5648262626255, 533.8642748844159, 569.4388422301713, 528.7022464948197, 531.9502339206123, 543.9374883685001, 539.0677533625823, 535.8667892264664, 563.1837323586446, 538.4416692843264, 534.3422707890585, 541.112107285261, 534.2883590231654, 531.272453016687, 556.3054955772722, 531.4435058802245, 527.339371907703, 529.2709637370579, 553.419162865643, 527.0098577937629, 529.3108475319503, 528.3897788806449, 528.4749475962896, 526.1035788528769, 529.5571622332517, 546.2084853967569, 557.7133565651669, 531.271577244558, 563.4016321135106, 556.5189071357299, 531.9364375614787, 560.0068671353334, 560.907285160363, 588.8649484233284, 548.7510116339934, 561.1640269716115, 559.08092220749, 592.9422734682619, 521.9501830910672, 532.6606009098689, 532.0221120089238, 536.8139268736428, 536.067073952536, 535.99661947334, 534.3493731964683, 533.8336755168017, 533.6198061058001, 537.5253758842841, 534.917503807518, 530.2292402472784, 526.7764843800109, 578.0440033852618, 583.4807276554806, 579.5136977767544, 578.982417083405, 582.4976634908685, 572.3391626471872, 569.5367603755109, 577.9597924754187, 577.7099286861178, 576.5962953246516, 577.7996520420614, 582.9378462770591, 582.3090933337282, 580.6746468558813, 572.7431536023239, 577.8756830397315, 576.7352222884515, 575.2717563981032, 568.7996940609083, 576.1727643377332, 578.675057447782, 584.5774296111638, 580.6213907739705, 574.132235365581, 586.1194741261692, 585.4536645173795, 586.6571651765219, 583.8016006853978, 585.9403109121985, 582.2783434537399, 582.1143055113731, 577.6711959929343, 583.6330684887029, 582.6642238993413, 584.7764920851321, 581.6368799851167, 582.3955085641818, 606.979091457321, 611.8318018833653, 611.8798384571502, 605.0948070693157, 607.9527436173161, 610.0837078833457, 599.2852181825189, 609.5902641164698, 616.2120227396163, 614.3625599598665, 610.9189773590061, 613.4101425559691, 612.5657921625666, 611.5317300367639, 610.0122298322965, 613.0181454693263, 615.015290553733, 603.2851288174775, 608.7644268940311, 600.2575636071573, 606.5782503030343, 611.9132395163306, 611.0648171488431, 611.5874411876591, 611.3248464176963, 606.9057599416151, 610.3796582774299, 609.0426077248807, 607.8368862838413, 612.7071867414957, 604.5832708438323, 603.8107172543935, 592.0744904536789, 600.9177824214919, 603.9604324567158, 578.3802406702946, 595.0005600843297, 599.4936302204493, 600.2949848256642, 598.9896691102529, 596.1139402054823, 575.4856218089307, 596.7333846445168, 596.0064810358507, 595.8224657551096, 574.4801960530586, 524.6146854319176, 569.9230588142807, 585.220433560209, 564.9486905217365, 511.2794312095034, 545.7066862831749, 580.0104194702405, 585.3392851987555]
Elapsed: 0.08059254421944691~0.009487539761403447
Time per graph: 0.0016556804124694792~0.00019531281416929584
Speed: 610.9748662457599~62.26279532047806
Total Time: 0.0829
best val loss: 0.34900325536727905 test_score: 0.9167

Testing...
Test loss: 0.3836 score: 0.8750 time: 0.08s
test Score 0.8750
Epoch Time List: [0.37369706702884287, 0.3805437319679186, 0.37999845400918275, 0.37266539700794965, 0.37918765703216195, 0.37927829707041383, 0.3824678868986666, 0.37137820513453335, 0.3789336160989478, 0.3830113089643419, 0.3713182210922241, 0.37464846309740096, 0.3811609379481524, 0.38248351006768644, 0.36779289692640305, 0.35657403094228357, 0.3569162030471489, 0.37765304709319025, 0.36768336489330977, 0.37406744505278766, 0.3701246399432421, 0.37656143005006015, 0.39571068494115025, 0.36893521493766457, 0.39189573493786156, 0.38852952898014337, 0.36882532003801316, 0.36399560305289924, 0.36800418293569237, 0.3625726510072127, 0.372899163980037, 0.36502496688626707, 0.37273028097115457, 0.3753131130943075, 0.37513027200475335, 0.3843588379677385, 0.37517358001787215, 0.3665895309532061, 0.3739858380286023, 0.37093310698401183, 0.37056887696962804, 0.35940672899596393, 0.3886556199286133, 0.3756935589481145, 0.37068246002309024, 0.37163558311294764, 0.3714381067547947, 0.37058860098477453, 0.3717899558832869, 0.3885519999312237, 0.36319021694362164, 0.361696220934391, 0.36729585693683475, 0.37246617092750967, 0.35764889512211084, 0.3805830010678619, 0.3774549738736823, 0.3707272430183366, 0.3772153981262818, 0.3744381550932303, 0.3707217819755897, 0.3719984870404005, 0.3871606240281835, 0.3707896730629727, 0.38246355787850916, 0.3726726860040799, 0.46093560697045177, 0.37045120901893824, 0.3757076009642333, 0.374354905099608, 0.4087725089630112, 0.3817834520014003, 0.39146441710181534, 0.3864116260083392, 0.4846683241194114, 0.4228440700098872, 0.4070939689408988, 0.40744022908620536, 0.41091650002636015, 0.48797929799184203, 0.40607022505719215, 0.4183427421376109, 0.4026348269544542, 0.387524682097137, 0.38042977394070476, 0.39198491210117936, 0.3878922980511561, 0.39331650687381625, 0.3822041479870677, 0.39083672093693167, 0.38269556884188205, 0.38175689708441496, 0.38008412707131356, 0.37831299693789333, 0.3788138049421832, 0.3843268540222198, 0.3898655120283365, 0.38835326093249023, 0.3763743171002716, 0.3840156779624522, 0.37372780684381723, 0.38491676200646907, 0.373839398031123, 0.38221234909724444, 0.38125606905668974, 0.38035021896939725, 0.3775801780866459, 0.3766430539544672, 0.3787912599509582, 0.3858769548824057, 0.383355621015653, 0.37577968707773834, 0.38009976490866393, 0.3792113739764318, 0.37257327779661864, 0.37308620603289455, 0.37340130005031824, 0.37413566501345485, 0.3767656101845205, 0.3757246278692037, 0.3785937500651926, 0.37858193600550294, 0.37868334201630205, 0.36944208294153214, 0.3750308359740302, 0.373424939927645, 0.36976710008457303, 0.37042770395055413, 0.38826272694859654, 0.4003352200379595, 0.40742890001274645, 0.3984976870706305, 0.3958034561946988, 0.3996333130635321, 0.39433262997772545, 0.39982378715649247, 0.3944474869640544, 0.3916739640990272, 0.4032518321182579, 0.402341854874976, 0.39766262704506516, 0.4037247149972245, 0.38587245892267674, 0.39716892200522125, 0.39282537798862904, 0.3982001479016617, 0.3941181841073558, 0.3906900640577078, 0.3846488499548286, 0.397380173089914, 0.39124407002236694, 0.38906192302238196, 0.3886580530088395, 0.3888897649012506, 0.3915092251263559, 0.3962852879194543, 0.4037045210134238, 0.3888259850209579, 0.3894722709665075, 0.40259739803150296, 0.3983693899353966, 0.40332691685762256, 0.39865443215239793, 0.3961125359637663, 0.39211298001464456, 0.40395753702614456, 0.39548412384465337, 0.3965381379239261, 0.39574412489309907, 0.39874352514743805, 0.40519535494968295, 0.39731230202596635, 0.3987408019602299, 0.40265123010613024, 0.39058758597821, 0.3981665639439598, 0.39457580691669136, 0.3903297109063715, 0.3904777750140056, 0.38025740114971995, 0.37575368501711637, 0.37265012110583484, 0.3657012990443036, 0.36577472696080804, 0.3918398969108239, 0.3983215889893472, 0.3929696949198842, 0.39052740787155926, 0.3907038379693404, 0.39103914611041546, 0.39057888905517757, 0.3914218848804012, 0.3932598860701546, 0.3948022121330723, 0.39128695090766996, 0.3991336530307308, 0.39264973800163716, 0.4000525129958987, 0.38962586503475904, 0.3910084030358121, 0.3985472279600799, 0.3951411750167608, 0.3915968070505187, 0.3958608949324116, 0.43319227499887347, 0.3952380900736898, 0.3939436449436471, 0.4010004921583459, 0.44658341293688864, 0.3957612019730732, 0.39919399202335626, 0.40119136101566255, 0.39931363612413406, 0.43226646701805294, 0.40053615695796907, 0.3967098309658468, 0.42115473793819547, 0.46405132592190057, 0.3923525429563597, 0.3973005360458046, 0.40022800699807703, 0.4003762990469113, 0.3949780819239095, 0.39658162894193083, 0.39858592685777694, 0.3954232938122004, 0.398410924943164, 0.3980124160880223, 0.412025734083727, 0.3991515958914533, 0.3968445180216804, 0.3984561050310731, 0.39803283102810383, 0.3973072379594669, 0.39050961297471076, 0.3778340700082481, 0.3793321669800207, 0.38235132698901, 0.3798657259903848, 0.37915987602900714, 0.3828350658295676, 0.3929838500916958, 0.3836983679793775, 0.37762199586723, 0.3852748809149489, 0.3783464409643784, 0.39064252481330186, 0.38069706805981696, 0.3781061000190675, 0.379702331032604, 0.380576687050052, 0.3864264489384368, 0.3823797340737656, 0.389048018027097, 0.38888816395774484, 0.37923569011036307, 0.3803708511404693, 0.37968580599408597, 0.37844338710419834, 0.3846987121505663, 0.38155274104792625, 0.3843143238918856, 0.3809829668607563, 0.38205042597837746, 0.3865793169243261, 0.3870470429537818, 0.3905564500018954, 0.39954735490027815, 0.38595820392947644, 0.3856839098734781, 0.39442454802338034, 0.3859482650877908, 0.3870089548872784, 0.3853439410449937, 0.3848848299821839, 0.3845520010218024, 0.3894208581186831, 0.38044657197315246, 0.3789609260857105, 0.3853559398557991, 0.42347599391359836, 0.4179111388511956, 0.3930111749796197, 0.4170578970806673, 0.4291719108587131, 0.43139935191720724, 0.40030124003533274, 0.39984983019530773]
Total Epoch List: [114, 83, 91]
Total Time List: [0.07297504798043519, 0.09374879102688283, 0.08293852093629539]
========================training times:2========================
========================k_idx:0========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: GPSPerformer
GPS(
  (x_lin): Linear(in_features=14887, out_features=64, bias=True)
  (pe_lin): Linear(in_features=20, out_features=8, bias=True)
  (pe_norm): BatchNorm1d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (convs): ModuleList(
    (0-1): 2 x GPSConv(64, conv=GINEConv(nn=Sequential(
      (0): Linear(in_features=64, out_features=64, bias=True)
      (1): ReLU()
      (2): Linear(in_features=64, out_features=64, bias=True)
    )), heads=1, attn_type=performer)
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=32, bias=True)
    (1): ReLU()
    (2): Linear(in_features=32, out_features=16, bias=True)
    (3): ReLU()
    (4): Linear(in_features=16, out_features=2, bias=True)
  )
)
Total number of parameters: 1039522
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7e7c5707cd90>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.8449;  Loss pred: 0.8449; Loss self: 0.0000; time: 0.26s
Val loss: 0.6322 score: 0.5306 time: 0.08s
Test loss: 0.6269 score: 0.5306 time: 0.07s
Epoch 2/1000, LR 0.000000
Train loss: 0.7600;  Loss pred: 0.7600; Loss self: 0.0000; time: 0.24s
Val loss: 0.6397 score: 0.5510 time: 0.08s
Test loss: 0.6306 score: 0.5510 time: 0.07s
     INFO: Early stopping counter 1 of 20
Epoch 3/1000, LR 0.000030
Train loss: 0.7655;  Loss pred: 0.7655; Loss self: 0.0000; time: 0.23s
Val loss: 0.6570 score: 0.5510 time: 0.07s
Test loss: 0.6377 score: 0.5714 time: 0.07s
     INFO: Early stopping counter 2 of 20
Epoch 4/1000, LR 0.000060
Train loss: 0.6885;  Loss pred: 0.6885; Loss self: 0.0000; time: 0.22s
Val loss: 0.6467 score: 0.5102 time: 0.07s
Test loss: 0.6402 score: 0.5510 time: 0.07s
     INFO: Early stopping counter 3 of 20
Epoch 5/1000, LR 0.000090
Train loss: 0.6365;  Loss pred: 0.6365; Loss self: 0.0000; time: 0.22s
Val loss: 0.6275 score: 0.5306 time: 0.07s
Test loss: 0.6291 score: 0.5306 time: 0.07s
Epoch 6/1000, LR 0.000120
Train loss: 0.6682;  Loss pred: 0.6682; Loss self: 0.0000; time: 0.22s
Val loss: 0.5954 score: 0.5306 time: 0.07s
Test loss: 0.6079 score: 0.5918 time: 0.07s
Epoch 7/1000, LR 0.000150
Train loss: 0.5842;  Loss pred: 0.5842; Loss self: 0.0000; time: 0.22s
Val loss: 0.5665 score: 0.5102 time: 0.07s
Test loss: 0.5912 score: 0.5510 time: 0.07s
Epoch 8/1000, LR 0.000180
Train loss: 0.5603;  Loss pred: 0.5603; Loss self: 0.0000; time: 0.24s
Val loss: 0.5608 score: 0.5102 time: 0.07s
Test loss: 0.5821 score: 0.5510 time: 0.07s
Epoch 9/1000, LR 0.000210
Train loss: 0.5199;  Loss pred: 0.5199; Loss self: 0.0000; time: 0.22s
Val loss: 0.5689 score: 0.5102 time: 0.07s
Test loss: 0.5849 score: 0.5306 time: 0.07s
     INFO: Early stopping counter 1 of 20
Epoch 10/1000, LR 0.000240
Train loss: 0.5237;  Loss pred: 0.5237; Loss self: 0.0000; time: 0.22s
Val loss: 0.5773 score: 0.5102 time: 0.07s
Test loss: 0.5916 score: 0.5306 time: 0.07s
     INFO: Early stopping counter 2 of 20
Epoch 11/1000, LR 0.000270
Train loss: 0.4144;  Loss pred: 0.4144; Loss self: 0.0000; time: 0.24s
Val loss: 0.5850 score: 0.5306 time: 0.09s
Test loss: 0.5963 score: 0.5510 time: 0.07s
     INFO: Early stopping counter 3 of 20
Epoch 12/1000, LR 0.000270
Train loss: 0.3998;  Loss pred: 0.3998; Loss self: 0.0000; time: 0.23s
Val loss: 0.5899 score: 0.5306 time: 0.08s
Test loss: 0.5991 score: 0.5510 time: 0.07s
     INFO: Early stopping counter 4 of 20
Epoch 13/1000, LR 0.000270
Train loss: 0.3866;  Loss pred: 0.3866; Loss self: 0.0000; time: 0.23s
Val loss: 0.5921 score: 0.5306 time: 0.07s
Test loss: 0.6003 score: 0.5510 time: 0.07s
     INFO: Early stopping counter 5 of 20
Epoch 14/1000, LR 0.000270
Train loss: 0.3774;  Loss pred: 0.3774; Loss self: 0.0000; time: 0.23s
Val loss: 0.5934 score: 0.5306 time: 0.07s
Test loss: 0.6008 score: 0.5510 time: 0.07s
     INFO: Early stopping counter 6 of 20
Epoch 15/1000, LR 0.000270
Train loss: 0.3322;  Loss pred: 0.3322; Loss self: 0.0000; time: 0.23s
Val loss: 0.5936 score: 0.5306 time: 0.07s
Test loss: 0.5999 score: 0.5510 time: 0.07s
     INFO: Early stopping counter 7 of 20
Epoch 16/1000, LR 0.000270
Train loss: 0.3228;  Loss pred: 0.3228; Loss self: 0.0000; time: 0.22s
Val loss: 0.5930 score: 0.5306 time: 0.07s
Test loss: 0.5987 score: 0.5510 time: 0.07s
     INFO: Early stopping counter 8 of 20
Epoch 17/1000, LR 0.000270
Train loss: 0.2909;  Loss pred: 0.2909; Loss self: 0.0000; time: 0.23s
Val loss: 0.5916 score: 0.5306 time: 0.07s
Test loss: 0.5951 score: 0.5510 time: 0.07s
     INFO: Early stopping counter 9 of 20
Epoch 18/1000, LR 0.000270
Train loss: 0.2520;  Loss pred: 0.2520; Loss self: 0.0000; time: 0.23s
Val loss: 0.5894 score: 0.5306 time: 0.07s
Test loss: 0.5918 score: 0.5510 time: 0.07s
     INFO: Early stopping counter 10 of 20
Epoch 19/1000, LR 0.000270
Train loss: 0.2523;  Loss pred: 0.2523; Loss self: 0.0000; time: 0.23s
Val loss: 0.5858 score: 0.5306 time: 0.07s
Test loss: 0.5892 score: 0.5510 time: 0.07s
     INFO: Early stopping counter 11 of 20
Epoch 20/1000, LR 0.000270
Train loss: 0.2211;  Loss pred: 0.2211; Loss self: 0.0000; time: 0.23s
Val loss: 0.5800 score: 0.5510 time: 0.07s
Test loss: 0.5860 score: 0.5714 time: 0.07s
     INFO: Early stopping counter 12 of 20
Epoch 21/1000, LR 0.000270
Train loss: 0.2216;  Loss pred: 0.2216; Loss self: 0.0000; time: 0.22s
Val loss: 0.5735 score: 0.5510 time: 0.07s
Test loss: 0.5825 score: 0.5714 time: 0.07s
     INFO: Early stopping counter 13 of 20
Epoch 22/1000, LR 0.000270
Train loss: 0.2111;  Loss pred: 0.2111; Loss self: 0.0000; time: 0.23s
Val loss: 0.5665 score: 0.5510 time: 0.07s
Test loss: 0.5778 score: 0.6122 time: 0.07s
     INFO: Early stopping counter 14 of 20
Epoch 23/1000, LR 0.000270
Train loss: 0.1552;  Loss pred: 0.1552; Loss self: 0.0000; time: 0.22s
Val loss: 0.5587 score: 0.5918 time: 0.07s
Test loss: 0.5729 score: 0.6327 time: 0.07s
Epoch 24/1000, LR 0.000270
Train loss: 0.1833;  Loss pred: 0.1833; Loss self: 0.0000; time: 0.22s
Val loss: 0.5516 score: 0.5918 time: 0.07s
Test loss: 0.5699 score: 0.6327 time: 0.07s
Epoch 25/1000, LR 0.000270
Train loss: 0.1341;  Loss pred: 0.1341; Loss self: 0.0000; time: 0.22s
Val loss: 0.5454 score: 0.5918 time: 0.07s
Test loss: 0.5670 score: 0.6327 time: 0.07s
Epoch 26/1000, LR 0.000270
Train loss: 0.1406;  Loss pred: 0.1406; Loss self: 0.0000; time: 0.23s
Val loss: 0.5435 score: 0.6122 time: 0.07s
Test loss: 0.5662 score: 0.6122 time: 0.06s
Epoch 27/1000, LR 0.000270
Train loss: 0.1232;  Loss pred: 0.1232; Loss self: 0.0000; time: 0.22s
Val loss: 0.5450 score: 0.5714 time: 0.07s
Test loss: 0.5672 score: 0.6122 time: 0.07s
     INFO: Early stopping counter 1 of 20
Epoch 28/1000, LR 0.000270
Train loss: 0.1488;  Loss pred: 0.1488; Loss self: 0.0000; time: 0.22s
Val loss: 0.5469 score: 0.5714 time: 0.07s
Test loss: 0.5687 score: 0.6122 time: 0.06s
     INFO: Early stopping counter 2 of 20
Epoch 29/1000, LR 0.000270
Train loss: 0.1112;  Loss pred: 0.1112; Loss self: 0.0000; time: 0.22s
Val loss: 0.5490 score: 0.5306 time: 0.07s
Test loss: 0.5698 score: 0.6122 time: 0.07s
     INFO: Early stopping counter 3 of 20
Epoch 30/1000, LR 0.000270
Train loss: 0.1023;  Loss pred: 0.1023; Loss self: 0.0000; time: 0.22s
Val loss: 0.5506 score: 0.5510 time: 0.07s
Test loss: 0.5698 score: 0.6122 time: 0.07s
     INFO: Early stopping counter 4 of 20
Epoch 31/1000, LR 0.000270
Train loss: 0.0871;  Loss pred: 0.0871; Loss self: 0.0000; time: 0.22s
Val loss: 0.5523 score: 0.5510 time: 0.07s
Test loss: 0.5687 score: 0.6122 time: 0.07s
     INFO: Early stopping counter 5 of 20
Epoch 32/1000, LR 0.000270
Train loss: 0.0897;  Loss pred: 0.0897; Loss self: 0.0000; time: 0.22s
Val loss: 0.5522 score: 0.5714 time: 0.07s
Test loss: 0.5667 score: 0.6327 time: 0.07s
     INFO: Early stopping counter 6 of 20
Epoch 33/1000, LR 0.000270
Train loss: 0.0679;  Loss pred: 0.0679; Loss self: 0.0000; time: 0.22s
Val loss: 0.5523 score: 0.5714 time: 0.07s
Test loss: 0.5658 score: 0.6531 time: 0.07s
     INFO: Early stopping counter 7 of 20
Epoch 34/1000, LR 0.000270
Train loss: 0.0784;  Loss pred: 0.0784; Loss self: 0.0000; time: 0.22s
Val loss: 0.5488 score: 0.5918 time: 0.07s
Test loss: 0.5630 score: 0.6735 time: 0.07s
     INFO: Early stopping counter 8 of 20
Epoch 35/1000, LR 0.000270
Train loss: 0.0818;  Loss pred: 0.0818; Loss self: 0.0000; time: 0.23s
Val loss: 0.5439 score: 0.6122 time: 0.08s
Test loss: 0.5598 score: 0.6735 time: 0.07s
     INFO: Early stopping counter 9 of 20
Epoch 36/1000, LR 0.000270
Train loss: 0.0655;  Loss pred: 0.0655; Loss self: 0.0000; time: 0.23s
Val loss: 0.5394 score: 0.6122 time: 0.07s
Test loss: 0.5578 score: 0.6939 time: 0.07s
Epoch 37/1000, LR 0.000270
Train loss: 0.0486;  Loss pred: 0.0486; Loss self: 0.0000; time: 0.23s
Val loss: 0.5356 score: 0.6531 time: 0.07s
Test loss: 0.5573 score: 0.6939 time: 0.07s
Epoch 38/1000, LR 0.000270
Train loss: 0.0523;  Loss pred: 0.0523; Loss self: 0.0000; time: 0.23s
Val loss: 0.5364 score: 0.6531 time: 0.07s
Test loss: 0.5599 score: 0.6735 time: 0.07s
     INFO: Early stopping counter 1 of 20
Epoch 39/1000, LR 0.000269
Train loss: 0.0496;  Loss pred: 0.0496; Loss self: 0.0000; time: 0.23s
Val loss: 0.5379 score: 0.6531 time: 0.07s
Test loss: 0.5637 score: 0.6327 time: 0.07s
     INFO: Early stopping counter 2 of 20
Epoch 40/1000, LR 0.000269
Train loss: 0.0528;  Loss pred: 0.0528; Loss self: 0.0000; time: 0.23s
Val loss: 0.5411 score: 0.6327 time: 0.07s
Test loss: 0.5703 score: 0.6531 time: 0.07s
     INFO: Early stopping counter 3 of 20
Epoch 41/1000, LR 0.000269
Train loss: 0.0398;  Loss pred: 0.0398; Loss self: 0.0000; time: 0.23s
Val loss: 0.5453 score: 0.6327 time: 0.07s
Test loss: 0.5782 score: 0.6531 time: 0.07s
     INFO: Early stopping counter 4 of 20
Epoch 42/1000, LR 0.000269
Train loss: 0.0305;  Loss pred: 0.0305; Loss self: 0.0000; time: 0.22s
Val loss: 0.5558 score: 0.5918 time: 0.07s
Test loss: 0.5885 score: 0.6122 time: 0.07s
     INFO: Early stopping counter 5 of 20
Epoch 43/1000, LR 0.000269
Train loss: 0.0368;  Loss pred: 0.0368; Loss self: 0.0000; time: 0.22s
Val loss: 0.5613 score: 0.5918 time: 0.07s
Test loss: 0.5970 score: 0.6122 time: 0.07s
     INFO: Early stopping counter 6 of 20
Epoch 44/1000, LR 0.000269
Train loss: 0.0428;  Loss pred: 0.0428; Loss self: 0.0000; time: 0.22s
Val loss: 0.5656 score: 0.5918 time: 0.07s
Test loss: 0.6043 score: 0.5714 time: 0.07s
     INFO: Early stopping counter 7 of 20
Epoch 45/1000, LR 0.000269
Train loss: 0.0364;  Loss pred: 0.0364; Loss self: 0.0000; time: 0.22s
Val loss: 0.5677 score: 0.5510 time: 0.07s
Test loss: 0.6104 score: 0.5714 time: 0.07s
     INFO: Early stopping counter 8 of 20
Epoch 46/1000, LR 0.000269
Train loss: 0.0282;  Loss pred: 0.0282; Loss self: 0.0000; time: 0.22s
Val loss: 0.5663 score: 0.5510 time: 0.07s
Test loss: 0.6135 score: 0.5714 time: 0.07s
     INFO: Early stopping counter 9 of 20
Epoch 47/1000, LR 0.000269
Train loss: 0.0286;  Loss pred: 0.0286; Loss self: 0.0000; time: 0.23s
Val loss: 0.5665 score: 0.5510 time: 0.07s
Test loss: 0.6176 score: 0.5714 time: 0.07s
     INFO: Early stopping counter 10 of 20
Epoch 48/1000, LR 0.000269
Train loss: 0.0263;  Loss pred: 0.0263; Loss self: 0.0000; time: 0.22s
Val loss: 0.5627 score: 0.5714 time: 0.07s
Test loss: 0.6188 score: 0.5918 time: 0.07s
     INFO: Early stopping counter 11 of 20
Epoch 49/1000, LR 0.000269
Train loss: 0.0266;  Loss pred: 0.0266; Loss self: 0.0000; time: 0.22s
Val loss: 0.5590 score: 0.5714 time: 0.07s
Test loss: 0.6198 score: 0.6122 time: 0.07s
     INFO: Early stopping counter 12 of 20
Epoch 50/1000, LR 0.000269
Train loss: 0.0225;  Loss pred: 0.0225; Loss self: 0.0000; time: 0.22s
Val loss: 0.5555 score: 0.5714 time: 0.07s
Test loss: 0.6219 score: 0.6327 time: 0.07s
     INFO: Early stopping counter 13 of 20
Epoch 51/1000, LR 0.000269
Train loss: 0.0238;  Loss pred: 0.0238; Loss self: 0.0000; time: 0.22s
Val loss: 0.5499 score: 0.6122 time: 0.07s
Test loss: 0.6235 score: 0.6327 time: 0.07s
     INFO: Early stopping counter 14 of 20
Epoch 52/1000, LR 0.000269
Train loss: 0.0173;  Loss pred: 0.0173; Loss self: 0.0000; time: 0.23s
Val loss: 0.5430 score: 0.6122 time: 0.07s
Test loss: 0.6239 score: 0.6327 time: 0.07s
     INFO: Early stopping counter 15 of 20
Epoch 53/1000, LR 0.000269
Train loss: 0.0172;  Loss pred: 0.0172; Loss self: 0.0000; time: 0.23s
Val loss: 0.5360 score: 0.6531 time: 0.08s
Test loss: 0.6226 score: 0.6735 time: 0.07s
     INFO: Early stopping counter 16 of 20
Epoch 54/1000, LR 0.000269
Train loss: 0.0145;  Loss pred: 0.0145; Loss self: 0.0000; time: 0.23s
Val loss: 0.5255 score: 0.6531 time: 0.07s
Test loss: 0.6187 score: 0.6735 time: 0.07s
Epoch 55/1000, LR 0.000269
Train loss: 0.0115;  Loss pred: 0.0115; Loss self: 0.0000; time: 0.23s
Val loss: 0.5162 score: 0.6531 time: 0.07s
Test loss: 0.6159 score: 0.6735 time: 0.07s
Epoch 56/1000, LR 0.000269
Train loss: 0.0139;  Loss pred: 0.0139; Loss self: 0.0000; time: 0.22s
Val loss: 0.5023 score: 0.6735 time: 0.07s
Test loss: 0.6077 score: 0.7143 time: 0.06s
Epoch 57/1000, LR 0.000269
Train loss: 0.0188;  Loss pred: 0.0188; Loss self: 0.0000; time: 0.22s
Val loss: 0.4839 score: 0.7143 time: 0.07s
Test loss: 0.5948 score: 0.7143 time: 0.06s
Epoch 58/1000, LR 0.000269
Train loss: 0.0141;  Loss pred: 0.0141; Loss self: 0.0000; time: 0.22s
Val loss: 0.4733 score: 0.7143 time: 0.07s
Test loss: 0.5871 score: 0.7347 time: 0.07s
Epoch 59/1000, LR 0.000268
Train loss: 0.0104;  Loss pred: 0.0104; Loss self: 0.0000; time: 0.22s
Val loss: 0.4647 score: 0.7347 time: 0.07s
Test loss: 0.5817 score: 0.7143 time: 0.07s
Epoch 60/1000, LR 0.000268
Train loss: 0.0151;  Loss pred: 0.0151; Loss self: 0.0000; time: 0.22s
Val loss: 0.4572 score: 0.7551 time: 0.07s
Test loss: 0.5786 score: 0.7143 time: 0.07s
Epoch 61/1000, LR 0.000268
Train loss: 0.0110;  Loss pred: 0.0110; Loss self: 0.0000; time: 0.22s
Val loss: 0.4537 score: 0.7551 time: 0.07s
Test loss: 0.5793 score: 0.7143 time: 0.07s
Epoch 62/1000, LR 0.000268
Train loss: 0.0092;  Loss pred: 0.0092; Loss self: 0.0000; time: 0.22s
Val loss: 0.4504 score: 0.7347 time: 0.08s
Test loss: 0.5797 score: 0.7347 time: 0.07s
Epoch 63/1000, LR 0.000268
Train loss: 0.0101;  Loss pred: 0.0101; Loss self: 0.0000; time: 0.23s
Val loss: 0.4434 score: 0.7347 time: 0.07s
Test loss: 0.5766 score: 0.7347 time: 0.07s
Epoch 64/1000, LR 0.000268
Train loss: 0.0082;  Loss pred: 0.0082; Loss self: 0.0000; time: 0.33s
Val loss: 0.4344 score: 0.7755 time: 0.08s
Test loss: 0.5726 score: 0.7347 time: 0.07s
Epoch 65/1000, LR 0.000268
Train loss: 0.0141;  Loss pred: 0.0141; Loss self: 0.0000; time: 0.22s
Val loss: 0.4157 score: 0.7959 time: 0.07s
Test loss: 0.5616 score: 0.7551 time: 0.07s
Epoch 66/1000, LR 0.000268
Train loss: 0.0083;  Loss pred: 0.0083; Loss self: 0.0000; time: 0.23s
Val loss: 0.3949 score: 0.8163 time: 0.08s
Test loss: 0.5487 score: 0.7551 time: 0.07s
Epoch 67/1000, LR 0.000268
Train loss: 0.0088;  Loss pred: 0.0088; Loss self: 0.0000; time: 0.24s
Val loss: 0.3763 score: 0.8980 time: 0.08s
Test loss: 0.5375 score: 0.7551 time: 0.07s
Epoch 68/1000, LR 0.000268
Train loss: 0.0076;  Loss pred: 0.0076; Loss self: 0.0000; time: 0.23s
Val loss: 0.3558 score: 0.8980 time: 0.08s
Test loss: 0.5247 score: 0.7755 time: 0.14s
Epoch 69/1000, LR 0.000268
Train loss: 0.0063;  Loss pred: 0.0063; Loss self: 0.0000; time: 0.24s
Val loss: 0.3344 score: 0.8980 time: 0.08s
Test loss: 0.5134 score: 0.7959 time: 0.07s
Epoch 70/1000, LR 0.000268
Train loss: 0.0061;  Loss pred: 0.0061; Loss self: 0.0000; time: 0.22s
Val loss: 0.3121 score: 0.8980 time: 0.07s
Test loss: 0.5005 score: 0.7959 time: 0.07s
Epoch 71/1000, LR 0.000268
Train loss: 0.0065;  Loss pred: 0.0065; Loss self: 0.0000; time: 0.23s
Val loss: 0.2889 score: 0.8980 time: 0.07s
Test loss: 0.4877 score: 0.7959 time: 0.07s
Epoch 72/1000, LR 0.000267
Train loss: 0.0051;  Loss pred: 0.0051; Loss self: 0.0000; time: 0.22s
Val loss: 0.2677 score: 0.9184 time: 0.08s
Test loss: 0.4738 score: 0.7755 time: 0.07s
Epoch 73/1000, LR 0.000267
Train loss: 0.0068;  Loss pred: 0.0068; Loss self: 0.0000; time: 0.23s
Val loss: 0.2504 score: 0.9184 time: 0.08s
Test loss: 0.4616 score: 0.7755 time: 0.15s
Epoch 74/1000, LR 0.000267
Train loss: 0.0049;  Loss pred: 0.0049; Loss self: 0.0000; time: 0.22s
Val loss: 0.2355 score: 0.9388 time: 0.07s
Test loss: 0.4515 score: 0.7755 time: 0.07s
Epoch 75/1000, LR 0.000267
Train loss: 0.0060;  Loss pred: 0.0060; Loss self: 0.0000; time: 0.22s
Val loss: 0.2231 score: 0.9388 time: 0.08s
Test loss: 0.4412 score: 0.7755 time: 0.07s
Epoch 76/1000, LR 0.000267
Train loss: 0.0052;  Loss pred: 0.0052; Loss self: 0.0000; time: 0.23s
Val loss: 0.2129 score: 0.9388 time: 0.07s
Test loss: 0.4312 score: 0.7959 time: 0.07s
Epoch 77/1000, LR 0.000267
Train loss: 0.0042;  Loss pred: 0.0042; Loss self: 0.0000; time: 0.22s
Val loss: 0.2033 score: 0.9388 time: 0.08s
Test loss: 0.4233 score: 0.8367 time: 0.07s
Epoch 78/1000, LR 0.000267
Train loss: 0.0041;  Loss pred: 0.0041; Loss self: 0.0000; time: 0.22s
Val loss: 0.1950 score: 0.9388 time: 0.07s
Test loss: 0.4171 score: 0.8367 time: 0.11s
Epoch 79/1000, LR 0.000267
Train loss: 0.0049;  Loss pred: 0.0049; Loss self: 0.0000; time: 0.22s
Val loss: 0.1882 score: 0.9388 time: 0.08s
Test loss: 0.4119 score: 0.8163 time: 0.07s
Epoch 80/1000, LR 0.000267
Train loss: 0.0056;  Loss pred: 0.0056; Loss self: 0.0000; time: 0.23s
Val loss: 0.1815 score: 0.9184 time: 0.07s
Test loss: 0.4061 score: 0.8367 time: 0.07s
Epoch 81/1000, LR 0.000267
Train loss: 0.0046;  Loss pred: 0.0046; Loss self: 0.0000; time: 0.23s
Val loss: 0.1770 score: 0.9184 time: 0.08s
Test loss: 0.4025 score: 0.8367 time: 0.07s
Epoch 82/1000, LR 0.000267
Train loss: 0.0038;  Loss pred: 0.0038; Loss self: 0.0000; time: 0.22s
Val loss: 0.1739 score: 0.9184 time: 0.07s
Test loss: 0.4008 score: 0.8367 time: 0.07s
Epoch 83/1000, LR 0.000266
Train loss: 0.0035;  Loss pred: 0.0035; Loss self: 0.0000; time: 0.22s
Val loss: 0.1715 score: 0.9184 time: 0.07s
Test loss: 0.4016 score: 0.8367 time: 0.07s
Epoch 84/1000, LR 0.000266
Train loss: 0.0026;  Loss pred: 0.0026; Loss self: 0.0000; time: 0.22s
Val loss: 0.1710 score: 0.9184 time: 0.07s
Test loss: 0.4024 score: 0.8367 time: 0.07s
Epoch 85/1000, LR 0.000266
Train loss: 0.0026;  Loss pred: 0.0026; Loss self: 0.0000; time: 0.22s
Val loss: 0.1723 score: 0.9184 time: 0.07s
Test loss: 0.4045 score: 0.8367 time: 0.07s
     INFO: Early stopping counter 1 of 20
Epoch 86/1000, LR 0.000266
Train loss: 0.0027;  Loss pred: 0.0027; Loss self: 0.0000; time: 0.22s
Val loss: 0.1754 score: 0.9184 time: 0.07s
Test loss: 0.4093 score: 0.8367 time: 0.07s
     INFO: Early stopping counter 2 of 20
Epoch 87/1000, LR 0.000266
Train loss: 0.0023;  Loss pred: 0.0023; Loss self: 0.0000; time: 0.22s
Val loss: 0.1790 score: 0.9184 time: 0.07s
Test loss: 0.4135 score: 0.8571 time: 0.07s
     INFO: Early stopping counter 3 of 20
Epoch 88/1000, LR 0.000266
Train loss: 0.0033;  Loss pred: 0.0033; Loss self: 0.0000; time: 0.22s
Val loss: 0.1834 score: 0.9184 time: 0.07s
Test loss: 0.4191 score: 0.8571 time: 0.07s
     INFO: Early stopping counter 4 of 20
Epoch 89/1000, LR 0.000266
Train loss: 0.0028;  Loss pred: 0.0028; Loss self: 0.0000; time: 0.22s
Val loss: 0.1879 score: 0.9184 time: 0.07s
Test loss: 0.4258 score: 0.8571 time: 0.07s
     INFO: Early stopping counter 5 of 20
Epoch 90/1000, LR 0.000266
Train loss: 0.0029;  Loss pred: 0.0029; Loss self: 0.0000; time: 0.22s
Val loss: 0.1909 score: 0.9184 time: 0.07s
Test loss: 0.4336 score: 0.8571 time: 0.06s
     INFO: Early stopping counter 6 of 20
Epoch 91/1000, LR 0.000266
Train loss: 0.0029;  Loss pred: 0.0029; Loss self: 0.0000; time: 0.21s
Val loss: 0.1950 score: 0.9184 time: 0.07s
Test loss: 0.4452 score: 0.8571 time: 0.06s
     INFO: Early stopping counter 7 of 20
Epoch 92/1000, LR 0.000266
Train loss: 0.0029;  Loss pred: 0.0029; Loss self: 0.0000; time: 0.21s
Val loss: 0.1999 score: 0.9184 time: 0.07s
Test loss: 0.4595 score: 0.8571 time: 0.06s
     INFO: Early stopping counter 8 of 20
Epoch 93/1000, LR 0.000265
Train loss: 0.0019;  Loss pred: 0.0019; Loss self: 0.0000; time: 0.22s
Val loss: 0.2052 score: 0.9184 time: 0.07s
Test loss: 0.4729 score: 0.8367 time: 0.07s
     INFO: Early stopping counter 9 of 20
Epoch 94/1000, LR 0.000265
Train loss: 0.0025;  Loss pred: 0.0025; Loss self: 0.0000; time: 0.23s
Val loss: 0.2109 score: 0.9184 time: 0.08s
Test loss: 0.4840 score: 0.8571 time: 0.07s
     INFO: Early stopping counter 10 of 20
Epoch 95/1000, LR 0.000265
Train loss: 0.0029;  Loss pred: 0.0029; Loss self: 0.0000; time: 0.23s
Val loss: 0.2160 score: 0.9184 time: 0.08s
Test loss: 0.4945 score: 0.8571 time: 0.07s
     INFO: Early stopping counter 11 of 20
Epoch 96/1000, LR 0.000265
Train loss: 0.0019;  Loss pred: 0.0019; Loss self: 0.0000; time: 0.23s
Val loss: 0.2222 score: 0.9184 time: 0.08s
Test loss: 0.5047 score: 0.8571 time: 0.07s
     INFO: Early stopping counter 12 of 20
Epoch 97/1000, LR 0.000265
Train loss: 0.0018;  Loss pred: 0.0018; Loss self: 0.0000; time: 0.23s
Val loss: 0.2295 score: 0.9184 time: 0.08s
Test loss: 0.5148 score: 0.8367 time: 0.07s
     INFO: Early stopping counter 13 of 20
Epoch 98/1000, LR 0.000265
Train loss: 0.0024;  Loss pred: 0.0024; Loss self: 0.0000; time: 0.23s
Val loss: 0.2373 score: 0.9184 time: 0.08s
Test loss: 0.5244 score: 0.8367 time: 0.07s
     INFO: Early stopping counter 14 of 20
Epoch 99/1000, LR 0.000265
Train loss: 0.0021;  Loss pred: 0.0021; Loss self: 0.0000; time: 0.23s
Val loss: 0.2461 score: 0.9184 time: 0.08s
Test loss: 0.5354 score: 0.8367 time: 0.07s
     INFO: Early stopping counter 15 of 20
Epoch 100/1000, LR 0.000265
Train loss: 0.0027;  Loss pred: 0.0027; Loss self: 0.0000; time: 0.23s
Val loss: 0.2550 score: 0.9184 time: 0.08s
Test loss: 0.5454 score: 0.8367 time: 0.07s
     INFO: Early stopping counter 16 of 20
Epoch 101/1000, LR 0.000265
Train loss: 0.0026;  Loss pred: 0.0026; Loss self: 0.0000; time: 0.23s
Val loss: 0.2645 score: 0.9184 time: 0.08s
Test loss: 0.5576 score: 0.8367 time: 0.07s
     INFO: Early stopping counter 17 of 20
Epoch 102/1000, LR 0.000264
Train loss: 0.0017;  Loss pred: 0.0017; Loss self: 0.0000; time: 0.23s
Val loss: 0.2761 score: 0.8980 time: 0.08s
Test loss: 0.5703 score: 0.8367 time: 0.07s
     INFO: Early stopping counter 18 of 20
Epoch 103/1000, LR 0.000264
Train loss: 0.0020;  Loss pred: 0.0020; Loss self: 0.0000; time: 0.23s
Val loss: 0.2872 score: 0.8980 time: 0.09s
Test loss: 0.5838 score: 0.8367 time: 0.07s
     INFO: Early stopping counter 19 of 20
Epoch 104/1000, LR 0.000264
Train loss: 0.0021;  Loss pred: 0.0021; Loss self: 0.0000; time: 0.23s
Val loss: 0.2953 score: 0.8980 time: 0.08s
Test loss: 0.5945 score: 0.8367 time: 0.07s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 083,   Train_Loss: 0.0026,   Val_Loss: 0.1710,   Val_Precision: 0.9545,   Val_Recall: 0.8750,   Val_accuracy: 0.9130,   Val_Score: 0.9184,   Val_Loss: 0.1710,   Test_Precision: 0.8148,   Test_Recall: 0.8800,   Test_accuracy: 0.8462,   Test_Score: 0.8367,   Test_loss: 0.4024


[0.07443801604676992, 0.07515534397680312, 0.06970977096352726, 0.0701145560014993, 0.07010411901865155, 0.07003115804400295, 0.0698851109482348, 0.07032394292764366, 0.07047844398766756, 0.07545706094242632, 0.07821953704115003, 0.07127622095867991, 0.06992451206315309, 0.06982740201056004, 0.07016196893528104, 0.07015579997096211, 0.06982781598344445, 0.06995753897354007, 0.07028340792749077, 0.0701320159714669, 0.07051167206373066, 0.06991060602013022, 0.07012491603381932, 0.07005927502177656, 0.07072758197318763, 0.0692550279200077, 0.07007003296166658, 0.06939170695841312, 0.06957274000160396, 0.06959549395833164, 0.06955265300348401, 0.06963080109562725, 0.06992117897607386, 0.07002775394357741, 0.07053243299014866, 0.07000190101098269, 0.07034723402466625, 0.0700900600058958, 0.07241663592867553, 0.07030806306283921, 0.06977185897994787, 0.06985066109336913, 0.07008533098269254, 0.06990105798467994, 0.06988502701278776, 0.06970717397052795, 0.06972332810983062, 0.06962149601895362, 0.06978827901184559, 0.069545833976008, 0.07007318292744458, 0.07027202099561691, 0.0704931499203667, 0.06982055795378983, 0.0706117149675265, 0.06942899199202657, 0.0695010629715398, 0.07076785003300756, 0.07009374804329127, 0.0698990699602291, 0.0703772830311209, 0.07105008594226092, 0.07407825207337737, 0.07455620192922652, 0.0702252839691937, 0.07393042405601591, 0.07154009502846748, 0.14506798202637583, 0.07309828093275428, 0.07042989192996174, 0.07038332894444466, 0.0710566610796377, 0.15609750198200345, 0.07061606808565557, 0.07039154705125839, 0.07063525100238621, 0.07139576296322048, 0.11075869807973504, 0.07198097696527839, 0.07098426797892898, 0.07077269698493183, 0.06975560600403696, 0.06997615692671388, 0.06999703799374402, 0.06984000303782523, 0.07020647393073887, 0.06980782106984407, 0.06986107910051942, 0.06980337190907449, 0.06923626398202032, 0.0694242239696905, 0.0692466429900378, 0.07181345799472183, 0.07315180893056095, 0.07331126101780683, 0.07313221006188542, 0.07310364209115505, 0.07319867599289864, 0.07392473402433097, 0.07334424299187958, 0.0730474169831723, 0.07799262693151832, 0.07435516000259668, 0.07366244797594845]
[0.0015191431846279576, 0.0015337825301388393, 0.0014226483870107606, 0.0014309093061530469, 0.0014306963065030928, 0.0014292073070204683, 0.0014262267540456081, 0.0014351825087274216, 0.0014383355915850522, 0.0015399400192331902, 0.0015963170824724498, 0.0014546167542587739, 0.0014270308584316957, 0.0014250490206236743, 0.0014318769170465519, 0.0014317510198155533, 0.0014250574690498868, 0.0014277048770110219, 0.0014343552638263423, 0.001431265632070753, 0.00143901371558634, 0.0014267470616353105, 0.0014311207353840678, 0.0014297811228933992, 0.0014434200402691352, 0.0014133679167348512, 0.0014300006726870732, 0.001416157284865574, 0.0014198518367674277, 0.001420316203231258, 0.001419441898030286, 0.0014210367570536174, 0.0014269628362464054, 0.0014291378355832125, 0.0014394374079622176, 0.0014286102247139324, 0.0014356578372380867, 0.0014304093878754244, 0.0014778905291566436, 0.0014348584298538615, 0.001423915489386691, 0.0014255236957830433, 0.001430312877197807, 0.0014265522037689782, 0.0014262250410773012, 0.0014225953871536317, 0.0014229250634659309, 0.0014208468575296657, 0.0014242505920784815, 0.0014193027342042448, 0.0014300649577029506, 0.0014341228774615697, 0.001438635712660545, 0.0014249093459957108, 0.0014410554075005408, 0.0014169182039189096, 0.001418389040235506, 0.0014442418374083176, 0.0014304846539447199, 0.0014265116318414102, 0.0014362710822677734, 0.0014500017539236922, 0.0015118010627219872, 0.0015215551414127862, 0.0014331690605957897, 0.001508784164408488, 0.0014600019393564792, 0.002960571061762772, 0.0014918016516888629, 0.0014373447332645254, 0.0014363944682539726, 0.0014501359404007696, 0.0031856633057551725, 0.001441144246646032, 0.0014365621847195588, 0.0014415357347425757, 0.0014570563870044996, 0.0022603815934639803, 0.0014689995299036406, 0.001448658530182224, 0.0014443407547945271, 0.0014235837960007544, 0.001428084835239059, 0.0014285109794641637, 0.0014253061844454128, 0.0014327851822599769, 0.0014246494095886545, 0.0014257363081738657, 0.0014245586103892752, 0.0014129849792249044, 0.0014168208973406224, 0.001413196795715057, 0.0014655807754024863, 0.0014928940598073663, 0.001496148184036874, 0.0014924940828956207, 0.001491911063084797, 0.0014938505304673193, 0.001508668041312877, 0.0014968212855485628, 0.0014907636119014754, 0.001591686263908537, 0.0015174522449509526, 0.0015033152648152746]
[658.26579753567, 651.9829117557358, 702.9143737344541, 698.856311647359, 698.9603561948095, 699.6885581873662, 701.1507792596225, 696.7754929557367, 695.248039366109, 649.3759416018994, 626.4419587937716, 687.4663013967332, 700.7556943085297, 701.7302461373216, 698.3840497007529, 698.445460251061, 701.7260859428492, 700.4248679836093, 697.1773487499609, 698.6823253438996, 694.9204091446345, 700.8950828704135, 698.7530648360228, 699.4077512901655, 692.7990273805145, 707.5298569888231, 699.300370342434, 706.1362538518616, 704.298838868073, 704.068571297696, 704.5022423162709, 703.711564839043, 700.7890987760258, 699.7225705608116, 694.7158622309807, 699.9809904064223, 696.5447992286214, 699.1005571386049, 676.6401030871, 696.9328675177026, 702.288869988148, 701.4965818935038, 699.1477291032623, 700.9908206359227, 701.1516213771205, 702.9405613361561, 702.7776976281667, 703.8056175446206, 702.1236329911923, 704.5713193532782, 699.2689350323326, 697.2903198992425, 695.1030001546724, 701.7990322052461, 693.9358436845009, 705.7570417503297, 705.0251881768364, 692.4048134448822, 699.0637734158065, 701.0107577665888, 696.2473953183467, 689.6543382061495, 661.4626915260312, 657.2223199689532, 697.7543874581587, 662.7853231691662, 684.9305970379513, 337.77267261559456, 670.3304014095331, 695.7273205633701, 696.1875878118374, 689.5905219228158, 313.9063686339403, 693.8930661016724, 696.1063089623349, 693.7046206340326, 686.3152372955569, 442.4031778048255, 680.735411852443, 690.2937988251874, 692.3573932816572, 702.4525024865274, 700.2385119736964, 700.0296213159673, 701.6036350035907, 697.9413329935956, 701.9270799324127, 701.39197147952, 701.9718196970076, 707.7216068839966, 705.8055128047612, 707.615530287142, 682.3233606659272, 669.8398948208246, 668.3829921858555, 670.0194067502619, 670.2812417868385, 669.4110150947774, 662.8363381581129, 668.0824288475526, 670.797161948765, 628.2645158628214, 658.9993216111537, 665.1964650427991]
Elapsed: 0.07285549821636568~0.01171872177339094
Time per graph: 0.00148684690237481~0.00023915758721205994
Speed: 681.7275975887957~57.890279413288056
Total Time: 0.0742
best val loss: 0.1710088700056076 test_score: 0.8367

Testing...
Test loss: 0.4515 score: 0.7755 time: 0.07s
test Score 0.7755
Epoch Time List: [0.40598622092511505, 0.3925150230061263, 0.3719427710166201, 0.36235095805022866, 0.3638636509422213, 0.3612426280742511, 0.36314024310559034, 0.3768875129753724, 0.364475739072077, 0.37043395393993706, 0.39386913494672626, 0.3805075428681448, 0.36680847499519587, 0.3663335299352184, 0.36502121412195265, 0.36311766295693815, 0.3664956738939509, 0.3652382841100916, 0.36562071181833744, 0.36582148098386824, 0.3640221789246425, 0.36479218606837094, 0.3641594839282334, 0.363830262911506, 0.3630820970283821, 0.367609893088229, 0.36315576906781644, 0.35447285894770175, 0.3556863151025027, 0.3558466361137107, 0.36047840502578765, 0.3563241600058973, 0.3633434701478109, 0.3641901550581679, 0.3664606548845768, 0.3664302929537371, 0.36617229494731873, 0.36655607412103564, 0.3680777479894459, 0.37097318889573216, 0.3649524931097403, 0.36310805508401245, 0.3634822841268033, 0.36399665696080774, 0.36387364007532597, 0.36351734795607626, 0.36517494602594525, 0.3616459210170433, 0.36256820790003985, 0.36116578395012766, 0.3621276100166142, 0.36647887993603945, 0.3691353980684653, 0.3691512190271169, 0.3659595559583977, 0.3630374049535021, 0.36316906311549246, 0.3657729639671743, 0.36286351806484163, 0.36303891404531896, 0.3638368539977819, 0.36470915214158595, 0.36940707999747247, 0.47600927494931966, 0.36007962201256305, 0.37158160493709147, 0.38086180796381086, 0.4489219569368288, 0.3850720601622015, 0.3627651579445228, 0.3673603158676997, 0.3649717739317566, 0.45397453010082245, 0.3608977959956974, 0.3632565301377326, 0.3669070260366425, 0.3642123849131167, 0.40019670396577567, 0.3630345780402422, 0.370935364975594, 0.3698880310403183, 0.3583664170000702, 0.35720202105585486, 0.3560828340705484, 0.3553011619951576, 0.35671770793851465, 0.3559674918651581, 0.35552852402906865, 0.35522070492152125, 0.3553438850212842, 0.3526659010676667, 0.3527164408005774, 0.36152526398655027, 0.38023045705631375, 0.37982918694615364, 0.37982421391643584, 0.3728317229542881, 0.3728484599851072, 0.3755434580380097, 0.3742687391350046, 0.37342023418750614, 0.3797261060681194, 0.3898288367781788, 0.3816212188685313]
Total Epoch List: [104]
Total Time List: [0.07423749496228993]
========================k_idx:1========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: GPSPerformer
GPS(
  (x_lin): Linear(in_features=14887, out_features=64, bias=True)
  (pe_lin): Linear(in_features=20, out_features=8, bias=True)
  (pe_norm): BatchNorm1d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (convs): ModuleList(
    (0-1): 2 x GPSConv(64, conv=GINEConv(nn=Sequential(
      (0): Linear(in_features=64, out_features=64, bias=True)
      (1): ReLU()
      (2): Linear(in_features=64, out_features=64, bias=True)
    )), heads=1, attn_type=performer)
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=32, bias=True)
    (1): ReLU()
    (2): Linear(in_features=32, out_features=16, bias=True)
    (3): ReLU()
    (4): Linear(in_features=16, out_features=2, bias=True)
  )
)
Total number of parameters: 1039522
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7e7c5707fc10>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.8262;  Loss pred: 0.8262; Loss self: 0.0000; time: 0.23s
Val loss: 0.9717 score: 0.3265 time: 0.07s
Test loss: 1.2935 score: 0.1837 time: 0.08s
Epoch 2/1000, LR 0.000000
Train loss: 0.8026;  Loss pred: 0.8026; Loss self: 0.0000; time: 0.23s
Val loss: 0.8354 score: 0.2857 time: 0.07s
Test loss: 1.0260 score: 0.1837 time: 0.08s
Epoch 3/1000, LR 0.000030
Train loss: 0.8200;  Loss pred: 0.8200; Loss self: 0.0000; time: 0.23s
Val loss: 0.8073 score: 0.3061 time: 0.07s
Test loss: 0.8982 score: 0.1837 time: 0.08s
Epoch 4/1000, LR 0.000060
Train loss: 0.7588;  Loss pred: 0.7588; Loss self: 0.0000; time: 0.23s
Val loss: 0.8395 score: 0.3061 time: 0.07s
Test loss: 0.9167 score: 0.2857 time: 0.08s
     INFO: Early stopping counter 1 of 20
Epoch 5/1000, LR 0.000090
Train loss: 0.7248;  Loss pred: 0.7248; Loss self: 0.0000; time: 0.23s
Val loss: 0.8764 score: 0.3265 time: 0.08s
Test loss: 0.9525 score: 0.2857 time: 0.08s
     INFO: Early stopping counter 2 of 20
Epoch 6/1000, LR 0.000120
Train loss: 0.8004;  Loss pred: 0.8004; Loss self: 0.0000; time: 0.24s
Val loss: 0.9179 score: 0.3878 time: 0.07s
Test loss: 0.9760 score: 0.3265 time: 0.08s
     INFO: Early stopping counter 3 of 20
Epoch 7/1000, LR 0.000150
Train loss: 0.6403;  Loss pred: 0.6403; Loss self: 0.0000; time: 0.22s
Val loss: 0.9746 score: 0.4286 time: 0.07s
Test loss: 1.0322 score: 0.4082 time: 0.08s
     INFO: Early stopping counter 4 of 20
Epoch 8/1000, LR 0.000180
Train loss: 0.6383;  Loss pred: 0.6383; Loss self: 0.0000; time: 0.22s
Val loss: 1.0887 score: 0.4490 time: 0.07s
Test loss: 1.1626 score: 0.4694 time: 0.08s
     INFO: Early stopping counter 5 of 20
Epoch 9/1000, LR 0.000210
Train loss: 0.6265;  Loss pred: 0.6265; Loss self: 0.0000; time: 0.23s
Val loss: 1.1991 score: 0.4490 time: 0.08s
Test loss: 1.3274 score: 0.4694 time: 0.08s
     INFO: Early stopping counter 6 of 20
Epoch 10/1000, LR 0.000240
Train loss: 0.5920;  Loss pred: 0.5920; Loss self: 0.0000; time: 0.24s
Val loss: 1.2715 score: 0.4490 time: 0.07s
Test loss: 1.4421 score: 0.4694 time: 0.08s
     INFO: Early stopping counter 7 of 20
Epoch 11/1000, LR 0.000270
Train loss: 0.4852;  Loss pred: 0.4852; Loss self: 0.0000; time: 0.23s
Val loss: 1.3147 score: 0.4694 time: 0.07s
Test loss: 1.5225 score: 0.4694 time: 0.08s
     INFO: Early stopping counter 8 of 20
Epoch 12/1000, LR 0.000270
Train loss: 0.4581;  Loss pred: 0.4581; Loss self: 0.0000; time: 0.23s
Val loss: 1.3565 score: 0.4898 time: 0.07s
Test loss: 1.6060 score: 0.4694 time: 0.08s
     INFO: Early stopping counter 9 of 20
Epoch 13/1000, LR 0.000270
Train loss: 0.4368;  Loss pred: 0.4368; Loss self: 0.0000; time: 0.23s
Val loss: 1.3967 score: 0.4898 time: 0.07s
Test loss: 1.6806 score: 0.4694 time: 0.08s
     INFO: Early stopping counter 10 of 20
Epoch 14/1000, LR 0.000270
Train loss: 0.3959;  Loss pred: 0.3959; Loss self: 0.0000; time: 0.23s
Val loss: 1.4402 score: 0.4898 time: 0.07s
Test loss: 1.7537 score: 0.4490 time: 0.08s
     INFO: Early stopping counter 11 of 20
Epoch 15/1000, LR 0.000270
Train loss: 0.3569;  Loss pred: 0.3569; Loss self: 0.0000; time: 0.23s
Val loss: 1.4721 score: 0.4898 time: 0.07s
Test loss: 1.8012 score: 0.4286 time: 0.08s
     INFO: Early stopping counter 12 of 20
Epoch 16/1000, LR 0.000270
Train loss: 0.4218;  Loss pred: 0.4218; Loss self: 0.0000; time: 0.23s
Val loss: 1.5022 score: 0.4898 time: 0.07s
Test loss: 1.8514 score: 0.4286 time: 0.08s
     INFO: Early stopping counter 13 of 20
Epoch 17/1000, LR 0.000270
Train loss: 0.3441;  Loss pred: 0.3441; Loss self: 0.0000; time: 0.23s
Val loss: 1.5268 score: 0.4898 time: 0.07s
Test loss: 1.8987 score: 0.4286 time: 0.08s
     INFO: Early stopping counter 14 of 20
Epoch 18/1000, LR 0.000270
Train loss: 0.3522;  Loss pred: 0.3522; Loss self: 0.0000; time: 0.23s
Val loss: 1.5473 score: 0.4898 time: 0.07s
Test loss: 1.9377 score: 0.4286 time: 0.08s
     INFO: Early stopping counter 15 of 20
Epoch 19/1000, LR 0.000270
Train loss: 0.2754;  Loss pred: 0.2754; Loss self: 0.0000; time: 0.23s
Val loss: 1.5594 score: 0.4898 time: 0.07s
Test loss: 1.9592 score: 0.4490 time: 0.08s
     INFO: Early stopping counter 16 of 20
Epoch 20/1000, LR 0.000270
Train loss: 0.3034;  Loss pred: 0.3034; Loss self: 0.0000; time: 0.23s
Val loss: 1.5600 score: 0.4898 time: 0.07s
Test loss: 1.9654 score: 0.4490 time: 0.08s
     INFO: Early stopping counter 17 of 20
Epoch 21/1000, LR 0.000270
Train loss: 0.2815;  Loss pred: 0.2815; Loss self: 0.0000; time: 0.23s
Val loss: 1.5573 score: 0.4694 time: 0.07s
Test loss: 1.9615 score: 0.4490 time: 0.08s
     INFO: Early stopping counter 18 of 20
Epoch 22/1000, LR 0.000270
Train loss: 0.2764;  Loss pred: 0.2764; Loss self: 0.0000; time: 0.23s
Val loss: 1.5612 score: 0.4694 time: 0.07s
Test loss: 1.9672 score: 0.4490 time: 0.08s
     INFO: Early stopping counter 19 of 20
Epoch 23/1000, LR 0.000270
Train loss: 0.2125;  Loss pred: 0.2125; Loss self: 0.0000; time: 0.23s
Val loss: 1.5625 score: 0.4898 time: 0.07s
Test loss: 1.9619 score: 0.4490 time: 0.08s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 002,   Train_Loss: 0.8200,   Val_Loss: 0.8073,   Val_Precision: 0.3200,   Val_Recall: 0.3200,   Val_accuracy: 0.3200,   Val_Score: 0.3061,   Val_Loss: 0.8073,   Test_Precision: 0.1923,   Test_Recall: 0.2083,   Test_accuracy: 0.2000,   Test_Score: 0.1837,   Test_loss: 0.8982


[0.07443801604676992, 0.07515534397680312, 0.06970977096352726, 0.0701145560014993, 0.07010411901865155, 0.07003115804400295, 0.0698851109482348, 0.07032394292764366, 0.07047844398766756, 0.07545706094242632, 0.07821953704115003, 0.07127622095867991, 0.06992451206315309, 0.06982740201056004, 0.07016196893528104, 0.07015579997096211, 0.06982781598344445, 0.06995753897354007, 0.07028340792749077, 0.0701320159714669, 0.07051167206373066, 0.06991060602013022, 0.07012491603381932, 0.07005927502177656, 0.07072758197318763, 0.0692550279200077, 0.07007003296166658, 0.06939170695841312, 0.06957274000160396, 0.06959549395833164, 0.06955265300348401, 0.06963080109562725, 0.06992117897607386, 0.07002775394357741, 0.07053243299014866, 0.07000190101098269, 0.07034723402466625, 0.0700900600058958, 0.07241663592867553, 0.07030806306283921, 0.06977185897994787, 0.06985066109336913, 0.07008533098269254, 0.06990105798467994, 0.06988502701278776, 0.06970717397052795, 0.06972332810983062, 0.06962149601895362, 0.06978827901184559, 0.069545833976008, 0.07007318292744458, 0.07027202099561691, 0.0704931499203667, 0.06982055795378983, 0.0706117149675265, 0.06942899199202657, 0.0695010629715398, 0.07076785003300756, 0.07009374804329127, 0.0698990699602291, 0.0703772830311209, 0.07105008594226092, 0.07407825207337737, 0.07455620192922652, 0.0702252839691937, 0.07393042405601591, 0.07154009502846748, 0.14506798202637583, 0.07309828093275428, 0.07042989192996174, 0.07038332894444466, 0.0710566610796377, 0.15609750198200345, 0.07061606808565557, 0.07039154705125839, 0.07063525100238621, 0.07139576296322048, 0.11075869807973504, 0.07198097696527839, 0.07098426797892898, 0.07077269698493183, 0.06975560600403696, 0.06997615692671388, 0.06999703799374402, 0.06984000303782523, 0.07020647393073887, 0.06980782106984407, 0.06986107910051942, 0.06980337190907449, 0.06923626398202032, 0.0694242239696905, 0.0692466429900378, 0.07181345799472183, 0.07315180893056095, 0.07331126101780683, 0.07313221006188542, 0.07310364209115505, 0.07319867599289864, 0.07392473402433097, 0.07334424299187958, 0.0730474169831723, 0.07799262693151832, 0.07435516000259668, 0.07366244797594845, 0.08369144599419087, 0.08440061402507126, 0.0833840660052374, 0.08352386602200568, 0.08818435797002167, 0.08691783889662474, 0.08355965896043926, 0.08252945099957287, 0.08687249303329736, 0.0854533069068566, 0.08314613997936249, 0.08370063989423215, 0.08306428801734, 0.08380979101639241, 0.0836168039822951, 0.08336541801691055, 0.08329843508545309, 0.08321589196566492, 0.08327036793343723, 0.08379609300754964, 0.08325746702030301, 0.08392773498781025, 0.08379267808049917]
[0.0015191431846279576, 0.0015337825301388393, 0.0014226483870107606, 0.0014309093061530469, 0.0014306963065030928, 0.0014292073070204683, 0.0014262267540456081, 0.0014351825087274216, 0.0014383355915850522, 0.0015399400192331902, 0.0015963170824724498, 0.0014546167542587739, 0.0014270308584316957, 0.0014250490206236743, 0.0014318769170465519, 0.0014317510198155533, 0.0014250574690498868, 0.0014277048770110219, 0.0014343552638263423, 0.001431265632070753, 0.00143901371558634, 0.0014267470616353105, 0.0014311207353840678, 0.0014297811228933992, 0.0014434200402691352, 0.0014133679167348512, 0.0014300006726870732, 0.001416157284865574, 0.0014198518367674277, 0.001420316203231258, 0.001419441898030286, 0.0014210367570536174, 0.0014269628362464054, 0.0014291378355832125, 0.0014394374079622176, 0.0014286102247139324, 0.0014356578372380867, 0.0014304093878754244, 0.0014778905291566436, 0.0014348584298538615, 0.001423915489386691, 0.0014255236957830433, 0.001430312877197807, 0.0014265522037689782, 0.0014262250410773012, 0.0014225953871536317, 0.0014229250634659309, 0.0014208468575296657, 0.0014242505920784815, 0.0014193027342042448, 0.0014300649577029506, 0.0014341228774615697, 0.001438635712660545, 0.0014249093459957108, 0.0014410554075005408, 0.0014169182039189096, 0.001418389040235506, 0.0014442418374083176, 0.0014304846539447199, 0.0014265116318414102, 0.0014362710822677734, 0.0014500017539236922, 0.0015118010627219872, 0.0015215551414127862, 0.0014331690605957897, 0.001508784164408488, 0.0014600019393564792, 0.002960571061762772, 0.0014918016516888629, 0.0014373447332645254, 0.0014363944682539726, 0.0014501359404007696, 0.0031856633057551725, 0.001441144246646032, 0.0014365621847195588, 0.0014415357347425757, 0.0014570563870044996, 0.0022603815934639803, 0.0014689995299036406, 0.001448658530182224, 0.0014443407547945271, 0.0014235837960007544, 0.001428084835239059, 0.0014285109794641637, 0.0014253061844454128, 0.0014327851822599769, 0.0014246494095886545, 0.0014257363081738657, 0.0014245586103892752, 0.0014129849792249044, 0.0014168208973406224, 0.001413196795715057, 0.0014655807754024863, 0.0014928940598073663, 0.001496148184036874, 0.0014924940828956207, 0.001491911063084797, 0.0014938505304673193, 0.001508668041312877, 0.0014968212855485628, 0.0014907636119014754, 0.001591686263908537, 0.0015174522449509526, 0.0015033152648152746, 0.0017079886937589974, 0.00172246151071574, 0.001701715632759947, 0.0017045686943266466, 0.0017996807748984014, 0.0017738334468698927, 0.0017052991624579442, 0.0016842745101953648, 0.0017729080210877012, 0.0017439450389154407, 0.0016968599995788262, 0.0017081763243720848, 0.0016951895513742858, 0.0017104038982937227, 0.0017064653873937775, 0.0017013350615696031, 0.0016999680629684304, 0.0016982835095033658, 0.0016993952639476986, 0.0017101243470928498, 0.0016991319800061838, 0.0017128109181185765, 0.0017100546547040647]
[658.26579753567, 651.9829117557358, 702.9143737344541, 698.856311647359, 698.9603561948095, 699.6885581873662, 701.1507792596225, 696.7754929557367, 695.248039366109, 649.3759416018994, 626.4419587937716, 687.4663013967332, 700.7556943085297, 701.7302461373216, 698.3840497007529, 698.445460251061, 701.7260859428492, 700.4248679836093, 697.1773487499609, 698.6823253438996, 694.9204091446345, 700.8950828704135, 698.7530648360228, 699.4077512901655, 692.7990273805145, 707.5298569888231, 699.300370342434, 706.1362538518616, 704.298838868073, 704.068571297696, 704.5022423162709, 703.711564839043, 700.7890987760258, 699.7225705608116, 694.7158622309807, 699.9809904064223, 696.5447992286214, 699.1005571386049, 676.6401030871, 696.9328675177026, 702.288869988148, 701.4965818935038, 699.1477291032623, 700.9908206359227, 701.1516213771205, 702.9405613361561, 702.7776976281667, 703.8056175446206, 702.1236329911923, 704.5713193532782, 699.2689350323326, 697.2903198992425, 695.1030001546724, 701.7990322052461, 693.9358436845009, 705.7570417503297, 705.0251881768364, 692.4048134448822, 699.0637734158065, 701.0107577665888, 696.2473953183467, 689.6543382061495, 661.4626915260312, 657.2223199689532, 697.7543874581587, 662.7853231691662, 684.9305970379513, 337.77267261559456, 670.3304014095331, 695.7273205633701, 696.1875878118374, 689.5905219228158, 313.9063686339403, 693.8930661016724, 696.1063089623349, 693.7046206340326, 686.3152372955569, 442.4031778048255, 680.735411852443, 690.2937988251874, 692.3573932816572, 702.4525024865274, 700.2385119736964, 700.0296213159673, 701.6036350035907, 697.9413329935956, 701.9270799324127, 701.39197147952, 701.9718196970076, 707.7216068839966, 705.8055128047612, 707.615530287142, 682.3233606659272, 669.8398948208246, 668.3829921858555, 670.0194067502619, 670.2812417868385, 669.4110150947774, 662.8363381581129, 668.0824288475526, 670.797161948765, 628.2645158628214, 658.9993216111537, 665.1964650427991, 585.4839693342275, 580.5644966687626, 587.6422480635842, 586.6586681594716, 555.6540992979457, 563.7507860529975, 586.4073718060375, 593.72744403999, 564.0450537228025, 573.4125661562706, 589.3238100068407, 585.4196582238627, 589.9045326166048, 584.6572268676348, 586.0066119051285, 587.7736976027688, 588.2463452012337, 588.8298357748483, 588.4446198096363, 584.7527998183081, 588.5358004952391, 583.8356057996419, 584.776631114785]
Elapsed: 0.07488780049057164~0.01146649594618126
Time per graph: 0.0015283224589912581~0.000234010121350638
Speed: 663.8387718722313~64.86223320967575
Total Time: 0.0844
best val loss: 0.8072975873947144 test_score: 0.1837

Testing...
Test loss: 1.6060 score: 0.4694 time: 0.08s
test Score 0.4694
Epoch Time List: [0.40598622092511505, 0.3925150230061263, 0.3719427710166201, 0.36235095805022866, 0.3638636509422213, 0.3612426280742511, 0.36314024310559034, 0.3768875129753724, 0.364475739072077, 0.37043395393993706, 0.39386913494672626, 0.3805075428681448, 0.36680847499519587, 0.3663335299352184, 0.36502121412195265, 0.36311766295693815, 0.3664956738939509, 0.3652382841100916, 0.36562071181833744, 0.36582148098386824, 0.3640221789246425, 0.36479218606837094, 0.3641594839282334, 0.363830262911506, 0.3630820970283821, 0.367609893088229, 0.36315576906781644, 0.35447285894770175, 0.3556863151025027, 0.3558466361137107, 0.36047840502578765, 0.3563241600058973, 0.3633434701478109, 0.3641901550581679, 0.3664606548845768, 0.3664302929537371, 0.36617229494731873, 0.36655607412103564, 0.3680777479894459, 0.37097318889573216, 0.3649524931097403, 0.36310805508401245, 0.3634822841268033, 0.36399665696080774, 0.36387364007532597, 0.36351734795607626, 0.36517494602594525, 0.3616459210170433, 0.36256820790003985, 0.36116578395012766, 0.3621276100166142, 0.36647887993603945, 0.3691353980684653, 0.3691512190271169, 0.3659595559583977, 0.3630374049535021, 0.36316906311549246, 0.3657729639671743, 0.36286351806484163, 0.36303891404531896, 0.3638368539977819, 0.36470915214158595, 0.36940707999747247, 0.47600927494931966, 0.36007962201256305, 0.37158160493709147, 0.38086180796381086, 0.4489219569368288, 0.3850720601622015, 0.3627651579445228, 0.3673603158676997, 0.3649717739317566, 0.45397453010082245, 0.3608977959956974, 0.3632565301377326, 0.3669070260366425, 0.3642123849131167, 0.40019670396577567, 0.3630345780402422, 0.370935364975594, 0.3698880310403183, 0.3583664170000702, 0.35720202105585486, 0.3560828340705484, 0.3553011619951576, 0.35671770793851465, 0.3559674918651581, 0.35552852402906865, 0.35522070492152125, 0.3553438850212842, 0.3526659010676667, 0.3527164408005774, 0.36152526398655027, 0.38023045705631375, 0.37982918694615364, 0.37982421391643584, 0.3728317229542881, 0.3728484599851072, 0.3755434580380097, 0.3742687391350046, 0.37342023418750614, 0.3797261060681194, 0.3898288367781788, 0.3816212188685313, 0.373939301003702, 0.375003284891136, 0.3720867991214618, 0.37319309380836785, 0.3947485878597945, 0.39174344413913786, 0.3717328008497134, 0.37067409709561616, 0.38377638801466674, 0.3876937549794093, 0.37933943001553416, 0.38027884904295206, 0.37779283500276506, 0.3783412529155612, 0.38014038116671145, 0.38012026285286993, 0.3804532721405849, 0.3769439230673015, 0.3775351420044899, 0.3808645820245147, 0.3778316299431026, 0.3804524299921468, 0.3807495430810377]
Total Epoch List: [104, 23]
Total Time List: [0.07423749496228993, 0.0843661311082542]
========================k_idx:2========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: GPSPerformer
GPS(
  (x_lin): Linear(in_features=14887, out_features=64, bias=True)
  (pe_lin): Linear(in_features=20, out_features=8, bias=True)
  (pe_norm): BatchNorm1d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (convs): ModuleList(
    (0-1): 2 x GPSConv(64, conv=GINEConv(nn=Sequential(
      (0): Linear(in_features=64, out_features=64, bias=True)
      (1): ReLU()
      (2): Linear(in_features=64, out_features=64, bias=True)
    )), heads=1, attn_type=performer)
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=32, bias=True)
    (1): ReLU()
    (2): Linear(in_features=32, out_features=16, bias=True)
    (3): ReLU()
    (4): Linear(in_features=16, out_features=2, bias=True)
  )
)
Total number of parameters: 1039522
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7e7c5707f610>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.6439;  Loss pred: 0.6439; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6381 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6650 score: 0.5000 time: 0.07s
Epoch 2/1000, LR 0.000000
Train loss: 0.7033;  Loss pred: 0.7033; Loss self: 0.0000; time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6185 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6234 score: 0.5000 time: 0.08s
Epoch 3/1000, LR 0.000030
Train loss: 0.6940;  Loss pred: 0.6940; Loss self: 0.0000; time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6176 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6126 score: 0.5000 time: 0.07s
Epoch 4/1000, LR 0.000060
Train loss: 0.6345;  Loss pred: 0.6345; Loss self: 0.0000; time: 0.23s
Val loss: 0.6159 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6084 score: 0.5000 time: 0.08s
Epoch 5/1000, LR 0.000090
Train loss: 0.6189;  Loss pred: 0.6189; Loss self: 0.0000; time: 0.25s
Val loss: 0.6185 score: 0.5102 time: 0.07s
Test loss: 0.6084 score: 0.5208 time: 0.07s
     INFO: Early stopping counter 1 of 20
Epoch 6/1000, LR 0.000120
Train loss: 0.5605;  Loss pred: 0.5605; Loss self: 0.0000; time: 0.23s
Val loss: 0.6264 score: 0.5102 time: 0.07s
Test loss: 0.6138 score: 0.5208 time: 0.08s
     INFO: Early stopping counter 2 of 20
Epoch 7/1000, LR 0.000150
Train loss: 0.5811;  Loss pred: 0.5811; Loss self: 0.0000; time: 0.23s
Val loss: 0.6354 score: 0.4694 time: 0.07s
Test loss: 0.6207 score: 0.5000 time: 0.08s
     INFO: Early stopping counter 3 of 20
Epoch 8/1000, LR 0.000180
Train loss: 0.5363;  Loss pred: 0.5363; Loss self: 0.0000; time: 0.25s
Val loss: 0.6398 score: 0.4694 time: 0.08s
Test loss: 0.6246 score: 0.5208 time: 0.09s
     INFO: Early stopping counter 4 of 20
Epoch 9/1000, LR 0.000210
Train loss: 0.4996;  Loss pred: 0.4996; Loss self: 0.0000; time: 0.27s
Val loss: 0.6415 score: 0.4694 time: 0.08s
Test loss: 0.6232 score: 0.4792 time: 0.09s
     INFO: Early stopping counter 5 of 20
Epoch 10/1000, LR 0.000240
Train loss: 0.4905;  Loss pred: 0.4905; Loss self: 0.0000; time: 0.25s
Val loss: 0.6406 score: 0.4694 time: 0.08s
Test loss: 0.6196 score: 0.5000 time: 0.08s
     INFO: Early stopping counter 6 of 20
Epoch 11/1000, LR 0.000270
Train loss: 0.4589;  Loss pred: 0.4589; Loss self: 0.0000; time: 0.24s
Val loss: 0.6420 score: 0.4490 time: 0.08s
Test loss: 0.6216 score: 0.4792 time: 0.08s
     INFO: Early stopping counter 7 of 20
Epoch 12/1000, LR 0.000270
Train loss: 0.4178;  Loss pred: 0.4178; Loss self: 0.0000; time: 0.25s
Val loss: 0.6437 score: 0.4490 time: 0.07s
Test loss: 0.6266 score: 0.4792 time: 0.08s
     INFO: Early stopping counter 8 of 20
Epoch 13/1000, LR 0.000270
Train loss: 0.4088;  Loss pred: 0.4088; Loss self: 0.0000; time: 0.25s
Val loss: 0.6445 score: 0.4286 time: 0.07s
Test loss: 0.6311 score: 0.4792 time: 0.08s
     INFO: Early stopping counter 9 of 20
Epoch 14/1000, LR 0.000270
Train loss: 0.3807;  Loss pred: 0.3807; Loss self: 0.0000; time: 0.25s
Val loss: 0.6446 score: 0.4490 time: 0.07s
Test loss: 0.6358 score: 0.4792 time: 0.08s
     INFO: Early stopping counter 10 of 20
Epoch 15/1000, LR 0.000270
Train loss: 0.3424;  Loss pred: 0.3424; Loss self: 0.0000; time: 0.25s
Val loss: 0.6407 score: 0.4694 time: 0.07s
Test loss: 0.6342 score: 0.4792 time: 0.08s
     INFO: Early stopping counter 11 of 20
Epoch 16/1000, LR 0.000270
Train loss: 0.2867;  Loss pred: 0.2867; Loss self: 0.0000; time: 0.25s
Val loss: 0.6365 score: 0.4694 time: 0.07s
Test loss: 0.6315 score: 0.4792 time: 0.08s
     INFO: Early stopping counter 12 of 20
Epoch 17/1000, LR 0.000270
Train loss: 0.2476;  Loss pred: 0.2476; Loss self: 0.0000; time: 0.24s
Val loss: 0.6303 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6273 score: 0.5000 time: 0.08s
     INFO: Early stopping counter 13 of 20
Epoch 18/1000, LR 0.000270
Train loss: 0.2512;  Loss pred: 0.2512; Loss self: 0.0000; time: 0.25s
Val loss: 0.6232 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6225 score: 0.5000 time: 0.08s
     INFO: Early stopping counter 14 of 20
Epoch 19/1000, LR 0.000270
Train loss: 0.2328;  Loss pred: 0.2328; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6184 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6191 score: 0.5000 time: 0.08s
     INFO: Early stopping counter 15 of 20
Epoch 20/1000, LR 0.000270
Train loss: 0.2089;  Loss pred: 0.2089; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6134 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6150 score: 0.5000 time: 0.08s
Epoch 21/1000, LR 0.000270
Train loss: 0.1800;  Loss pred: 0.1800; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6080 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6102 score: 0.5000 time: 0.08s
Epoch 22/1000, LR 0.000270
Train loss: 0.1600;  Loss pred: 0.1600; Loss self: 0.0000; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6040 score: 0.5102 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6056 score: 0.5000 time: 0.08s
Epoch 23/1000, LR 0.000270
Train loss: 0.1633;  Loss pred: 0.1633; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6017 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6020 score: 0.5000 time: 0.08s
Epoch 24/1000, LR 0.000270
Train loss: 0.1393;  Loss pred: 0.1393; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6000 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.5989 score: 0.5000 time: 0.07s
Epoch 25/1000, LR 0.000270
Train loss: 0.1375;  Loss pred: 0.1375; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6006 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.5972 score: 0.5000 time: 0.07s
     INFO: Early stopping counter 1 of 20
Epoch 26/1000, LR 0.000270
Train loss: 0.1204;  Loss pred: 0.1204; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6019 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.5975 score: 0.5000 time: 0.08s
     INFO: Early stopping counter 2 of 20
Epoch 27/1000, LR 0.000270
Train loss: 0.0997;  Loss pred: 0.0997; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6036 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.5991 score: 0.5000 time: 0.08s
     INFO: Early stopping counter 3 of 20
Epoch 28/1000, LR 0.000270
Train loss: 0.0958;  Loss pred: 0.0958; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6054 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6012 score: 0.5000 time: 0.08s
     INFO: Early stopping counter 4 of 20
Epoch 29/1000, LR 0.000270
Train loss: 0.0927;  Loss pred: 0.0927; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6064 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6026 score: 0.5000 time: 0.08s
     INFO: Early stopping counter 5 of 20
Epoch 30/1000, LR 0.000270
Train loss: 0.0924;  Loss pred: 0.0924; Loss self: 0.0000; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6064 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6014 score: 0.5000 time: 0.08s
     INFO: Early stopping counter 6 of 20
Epoch 31/1000, LR 0.000270
Train loss: 0.0835;  Loss pred: 0.0835; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6052 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.5979 score: 0.5000 time: 0.07s
     INFO: Early stopping counter 7 of 20
Epoch 32/1000, LR 0.000270
Train loss: 0.0719;  Loss pred: 0.0719; Loss self: 0.0000; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6037 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.5939 score: 0.5000 time: 0.08s
     INFO: Early stopping counter 8 of 20
Epoch 33/1000, LR 0.000270
Train loss: 0.0740;  Loss pred: 0.0740; Loss self: 0.0000; time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6025 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.5901 score: 0.5000 time: 0.08s
     INFO: Early stopping counter 9 of 20
Epoch 34/1000, LR 0.000270
Train loss: 0.0702;  Loss pred: 0.0702; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6024 score: 0.5102 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.5873 score: 0.5000 time: 0.08s
     INFO: Early stopping counter 10 of 20
Epoch 35/1000, LR 0.000270
Train loss: 0.0616;  Loss pred: 0.0616; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6023 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.5842 score: 0.5000 time: 0.07s
     INFO: Early stopping counter 11 of 20
Epoch 36/1000, LR 0.000270
Train loss: 0.0521;  Loss pred: 0.0521; Loss self: 0.0000; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6025 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.5816 score: 0.5000 time: 0.08s
     INFO: Early stopping counter 12 of 20
Epoch 37/1000, LR 0.000270
Train loss: 0.0484;  Loss pred: 0.0484; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6027 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.5782 score: 0.5000 time: 0.08s
     INFO: Early stopping counter 13 of 20
Epoch 38/1000, LR 0.000270
Train loss: 0.0520;  Loss pred: 0.0520; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6041 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.5766 score: 0.5000 time: 0.07s
     INFO: Early stopping counter 14 of 20
Epoch 39/1000, LR 0.000269
Train loss: 0.0406;  Loss pred: 0.0406; Loss self: 0.0000; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6060 score: 0.5102 time: 0.07s
Test loss: 0.5755 score: 0.5208 time: 0.07s
     INFO: Early stopping counter 15 of 20
Epoch 40/1000, LR 0.000269
Train loss: 0.0403;  Loss pred: 0.0403; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6074 score: 0.5102 time: 0.07s
Test loss: 0.5754 score: 0.5208 time: 0.07s
     INFO: Early stopping counter 16 of 20
Epoch 41/1000, LR 0.000269
Train loss: 0.0434;  Loss pred: 0.0434; Loss self: 0.0000; time: 0.24s
Val loss: 0.6082 score: 0.5306 time: 0.07s
Test loss: 0.5761 score: 0.5208 time: 0.08s
     INFO: Early stopping counter 17 of 20
Epoch 42/1000, LR 0.000269
Train loss: 0.0403;  Loss pred: 0.0403; Loss self: 0.0000; time: 0.24s
Val loss: 0.6065 score: 0.5306 time: 0.07s
Test loss: 0.5746 score: 0.5208 time: 0.07s
     INFO: Early stopping counter 18 of 20
Epoch 43/1000, LR 0.000269
Train loss: 0.0380;  Loss pred: 0.0380; Loss self: 0.0000; time: 0.24s
Val loss: 0.6026 score: 0.5306 time: 0.07s
Test loss: 0.5704 score: 0.5625 time: 0.08s
     INFO: Early stopping counter 19 of 20
Epoch 44/1000, LR 0.000269
Train loss: 0.0366;  Loss pred: 0.0366; Loss self: 0.0000; time: 0.23s
Val loss: 0.5962 score: 0.5306 time: 0.07s
Test loss: 0.5624 score: 0.5625 time: 0.08s
Epoch 45/1000, LR 0.000269
Train loss: 0.0299;  Loss pred: 0.0299; Loss self: 0.0000; time: 0.23s
Val loss: 0.5868 score: 0.5510 time: 0.07s
Test loss: 0.5505 score: 0.6042 time: 0.08s
Epoch 46/1000, LR 0.000269
Train loss: 0.0336;  Loss pred: 0.0336; Loss self: 0.0000; time: 0.24s
Val loss: 0.5729 score: 0.5714 time: 0.07s
Test loss: 0.5309 score: 0.6250 time: 0.07s
Epoch 47/1000, LR 0.000269
Train loss: 0.0294;  Loss pred: 0.0294; Loss self: 0.0000; time: 0.24s
Val loss: 0.5595 score: 0.5918 time: 0.07s
Test loss: 0.5109 score: 0.6250 time: 0.08s
Epoch 48/1000, LR 0.000269
Train loss: 0.0263;  Loss pred: 0.0263; Loss self: 0.0000; time: 0.23s
Val loss: 0.5490 score: 0.6531 time: 0.07s
Test loss: 0.4949 score: 0.6667 time: 0.08s
Epoch 49/1000, LR 0.000269
Train loss: 0.0210;  Loss pred: 0.0210; Loss self: 0.0000; time: 0.24s
Val loss: 0.5397 score: 0.6735 time: 0.07s
Test loss: 0.4808 score: 0.6667 time: 0.08s
Epoch 50/1000, LR 0.000269
Train loss: 0.0245;  Loss pred: 0.0245; Loss self: 0.0000; time: 0.24s
Val loss: 0.5317 score: 0.6939 time: 0.07s
Test loss: 0.4691 score: 0.7083 time: 0.08s
Epoch 51/1000, LR 0.000269
Train loss: 0.0220;  Loss pred: 0.0220; Loss self: 0.0000; time: 0.24s
Val loss: 0.5280 score: 0.6939 time: 0.07s
Test loss: 0.4631 score: 0.7083 time: 0.08s
Epoch 52/1000, LR 0.000269
Train loss: 0.0212;  Loss pred: 0.0212; Loss self: 0.0000; time: 0.24s
Val loss: 0.5240 score: 0.7347 time: 0.07s
Test loss: 0.4573 score: 0.7083 time: 0.07s
Epoch 53/1000, LR 0.000269
Train loss: 0.0179;  Loss pred: 0.0179; Loss self: 0.0000; time: 0.23s
Val loss: 0.5195 score: 0.7551 time: 0.07s
Test loss: 0.4511 score: 0.7292 time: 0.07s
Epoch 54/1000, LR 0.000269
Train loss: 0.0176;  Loss pred: 0.0176; Loss self: 0.0000; time: 0.25s
Val loss: 0.5115 score: 0.7755 time: 0.07s
Test loss: 0.4408 score: 0.7500 time: 0.08s
Epoch 55/1000, LR 0.000269
Train loss: 0.0164;  Loss pred: 0.0164; Loss self: 0.0000; time: 0.24s
Val loss: 0.4997 score: 0.7755 time: 0.07s
Test loss: 0.4254 score: 0.7917 time: 0.08s
Epoch 56/1000, LR 0.000269
Train loss: 0.0171;  Loss pred: 0.0171; Loss self: 0.0000; time: 0.23s
Val loss: 0.4846 score: 0.7959 time: 0.07s
Test loss: 0.4057 score: 0.7917 time: 0.08s
Epoch 57/1000, LR 0.000269
Train loss: 0.0128;  Loss pred: 0.0128; Loss self: 0.0000; time: 0.24s
Val loss: 0.4695 score: 0.8163 time: 0.07s
Test loss: 0.3860 score: 0.8125 time: 0.07s
Epoch 58/1000, LR 0.000269
Train loss: 0.0132;  Loss pred: 0.0132; Loss self: 0.0000; time: 0.23s
Val loss: 0.4518 score: 0.8367 time: 0.08s
Test loss: 0.3627 score: 0.8542 time: 0.07s
Epoch 59/1000, LR 0.000268
Train loss: 0.0139;  Loss pred: 0.0139; Loss self: 0.0000; time: 0.24s
Val loss: 0.4331 score: 0.8571 time: 0.07s
Test loss: 0.3377 score: 0.8958 time: 0.07s
Epoch 60/1000, LR 0.000268
Train loss: 0.0119;  Loss pred: 0.0119; Loss self: 0.0000; time: 0.23s
Val loss: 0.4165 score: 0.8776 time: 0.07s
Test loss: 0.3167 score: 0.9167 time: 0.07s
Epoch 61/1000, LR 0.000268
Train loss: 0.0116;  Loss pred: 0.0116; Loss self: 0.0000; time: 0.24s
Val loss: 0.4039 score: 0.8571 time: 0.07s
Test loss: 0.2998 score: 0.9167 time: 0.07s
Epoch 62/1000, LR 0.000268
Train loss: 0.0112;  Loss pred: 0.0112; Loss self: 0.0000; time: 0.23s
Val loss: 0.3937 score: 0.8776 time: 0.07s
Test loss: 0.2858 score: 0.9167 time: 0.07s
Epoch 63/1000, LR 0.000268
Train loss: 0.0109;  Loss pred: 0.0109; Loss self: 0.0000; time: 0.23s
Val loss: 0.3861 score: 0.8776 time: 0.07s
Test loss: 0.2754 score: 0.9167 time: 0.07s
Epoch 64/1000, LR 0.000268
Train loss: 0.0101;  Loss pred: 0.0101; Loss self: 0.0000; time: 0.23s
Val loss: 0.3801 score: 0.8776 time: 0.07s
Test loss: 0.2666 score: 0.9167 time: 0.07s
Epoch 65/1000, LR 0.000268
Train loss: 0.0094;  Loss pred: 0.0094; Loss self: 0.0000; time: 0.23s
Val loss: 0.3751 score: 0.8980 time: 0.07s
Test loss: 0.2591 score: 0.9167 time: 0.07s
Epoch 66/1000, LR 0.000268
Train loss: 0.0105;  Loss pred: 0.0105; Loss self: 0.0000; time: 0.24s
Val loss: 0.3680 score: 0.8980 time: 0.07s
Test loss: 0.2494 score: 0.9167 time: 0.08s
Epoch 67/1000, LR 0.000268
Train loss: 0.0080;  Loss pred: 0.0080; Loss self: 0.0000; time: 0.23s
Val loss: 0.3625 score: 0.8980 time: 0.07s
Test loss: 0.2415 score: 0.9167 time: 0.08s
Epoch 68/1000, LR 0.000268
Train loss: 0.0080;  Loss pred: 0.0080; Loss self: 0.0000; time: 0.23s
Val loss: 0.3554 score: 0.8980 time: 0.07s
Test loss: 0.2320 score: 0.9375 time: 0.08s
Epoch 69/1000, LR 0.000268
Train loss: 0.0072;  Loss pred: 0.0072; Loss self: 0.0000; time: 0.23s
Val loss: 0.3477 score: 0.8980 time: 0.07s
Test loss: 0.2220 score: 0.9375 time: 0.07s
Epoch 70/1000, LR 0.000268
Train loss: 0.0076;  Loss pred: 0.0076; Loss self: 0.0000; time: 0.23s
Val loss: 0.3395 score: 0.8980 time: 0.07s
Test loss: 0.2096 score: 0.9375 time: 0.08s
Epoch 71/1000, LR 0.000268
Train loss: 0.0081;  Loss pred: 0.0081; Loss self: 0.0000; time: 0.25s
Val loss: 0.3313 score: 0.8980 time: 0.07s
Test loss: 0.1965 score: 0.9375 time: 0.08s
Epoch 72/1000, LR 0.000267
Train loss: 0.0070;  Loss pred: 0.0070; Loss self: 0.0000; time: 0.24s
Val loss: 0.3249 score: 0.8980 time: 0.07s
Test loss: 0.1839 score: 0.9375 time: 0.07s
Epoch 73/1000, LR 0.000267
Train loss: 0.0076;  Loss pred: 0.0076; Loss self: 0.0000; time: 0.23s
Val loss: 0.3206 score: 0.8776 time: 0.07s
Test loss: 0.1732 score: 0.9583 time: 0.07s
Epoch 74/1000, LR 0.000267
Train loss: 0.0063;  Loss pred: 0.0063; Loss self: 0.0000; time: 0.23s
Val loss: 0.3188 score: 0.8776 time: 0.07s
Test loss: 0.1660 score: 0.9583 time: 0.08s
Epoch 75/1000, LR 0.000267
Train loss: 0.0064;  Loss pred: 0.0064; Loss self: 0.0000; time: 0.22s
Val loss: 0.3206 score: 0.8776 time: 0.07s
Test loss: 0.1622 score: 0.9375 time: 0.07s
     INFO: Early stopping counter 1 of 20
Epoch 76/1000, LR 0.000267
Train loss: 0.0062;  Loss pred: 0.0062; Loss self: 0.0000; time: 0.23s
Val loss: 0.3271 score: 0.8571 time: 0.07s
Test loss: 0.1619 score: 0.9375 time: 0.07s
     INFO: Early stopping counter 2 of 20
Epoch 77/1000, LR 0.000267
Train loss: 0.0052;  Loss pred: 0.0052; Loss self: 0.0000; time: 0.24s
Val loss: 0.3354 score: 0.8571 time: 0.09s
Test loss: 0.1638 score: 0.9375 time: 0.07s
     INFO: Early stopping counter 3 of 20
Epoch 78/1000, LR 0.000267
Train loss: 0.0049;  Loss pred: 0.0049; Loss self: 0.0000; time: 0.24s
Val loss: 0.3448 score: 0.8571 time: 0.07s
Test loss: 0.1671 score: 0.9167 time: 0.07s
     INFO: Early stopping counter 4 of 20
Epoch 79/1000, LR 0.000267
Train loss: 0.0053;  Loss pred: 0.0053; Loss self: 0.0000; time: 0.24s
Val loss: 0.3556 score: 0.8571 time: 0.07s
Test loss: 0.1714 score: 0.9167 time: 0.07s
     INFO: Early stopping counter 5 of 20
Epoch 80/1000, LR 0.000267
Train loss: 0.0052;  Loss pred: 0.0052; Loss self: 0.0000; time: 0.25s
Val loss: 0.3679 score: 0.8571 time: 0.07s
Test loss: 0.1760 score: 0.9167 time: 0.07s
     INFO: Early stopping counter 6 of 20
Epoch 81/1000, LR 0.000267
Train loss: 0.0051;  Loss pred: 0.0051; Loss self: 0.0000; time: 0.23s
Val loss: 0.3806 score: 0.8367 time: 0.15s
Test loss: 0.1809 score: 0.9167 time: 0.07s
     INFO: Early stopping counter 7 of 20
Epoch 82/1000, LR 0.000267
Train loss: 0.0046;  Loss pred: 0.0046; Loss self: 0.0000; time: 0.24s
Val loss: 0.3920 score: 0.8367 time: 0.07s
Test loss: 0.1855 score: 0.9167 time: 0.07s
     INFO: Early stopping counter 8 of 20
Epoch 83/1000, LR 0.000266
Train loss: 0.0052;  Loss pred: 0.0052; Loss self: 0.0000; time: 0.24s
Val loss: 0.4045 score: 0.8367 time: 0.07s
Test loss: 0.1905 score: 0.9167 time: 0.08s
     INFO: Early stopping counter 9 of 20
Epoch 84/1000, LR 0.000266
Train loss: 0.0051;  Loss pred: 0.0051; Loss self: 0.0000; time: 0.24s
Val loss: 0.4175 score: 0.8367 time: 0.08s
Test loss: 0.1962 score: 0.9167 time: 0.08s
     INFO: Early stopping counter 10 of 20
Epoch 85/1000, LR 0.000266
Train loss: 0.0039;  Loss pred: 0.0039; Loss self: 0.0000; time: 0.23s
Val loss: 0.4309 score: 0.8367 time: 0.07s
Test loss: 0.2022 score: 0.9167 time: 0.07s
     INFO: Early stopping counter 11 of 20
Epoch 86/1000, LR 0.000266
Train loss: 0.0038;  Loss pred: 0.0038; Loss self: 0.0000; time: 0.23s
Val loss: 0.4462 score: 0.8367 time: 0.08s
Test loss: 0.2093 score: 0.9167 time: 0.07s
     INFO: Early stopping counter 12 of 20
Epoch 87/1000, LR 0.000266
Train loss: 0.0044;  Loss pred: 0.0044; Loss self: 0.0000; time: 0.24s
Val loss: 0.4624 score: 0.8367 time: 0.07s
Test loss: 0.2167 score: 0.8958 time: 0.08s
     INFO: Early stopping counter 13 of 20
Epoch 88/1000, LR 0.000266
Train loss: 0.0034;  Loss pred: 0.0034; Loss self: 0.0000; time: 0.23s
Val loss: 0.4791 score: 0.8367 time: 0.07s
Test loss: 0.2237 score: 0.8958 time: 0.08s
     INFO: Early stopping counter 14 of 20
Epoch 89/1000, LR 0.000266
Train loss: 0.0026;  Loss pred: 0.0026; Loss self: 0.0000; time: 0.23s
Val loss: 0.4967 score: 0.8367 time: 0.07s
Test loss: 0.2310 score: 0.8958 time: 0.08s
     INFO: Early stopping counter 15 of 20
Epoch 90/1000, LR 0.000266
Train loss: 0.0029;  Loss pred: 0.0029; Loss self: 0.0000; time: 0.22s
Val loss: 0.5152 score: 0.8367 time: 0.07s
Test loss: 0.2384 score: 0.8958 time: 0.07s
     INFO: Early stopping counter 16 of 20
Epoch 91/1000, LR 0.000266
Train loss: 0.0030;  Loss pred: 0.0030; Loss self: 0.0000; time: 0.35s
Val loss: 0.5354 score: 0.8367 time: 0.07s
Test loss: 0.2476 score: 0.8958 time: 0.08s
     INFO: Early stopping counter 17 of 20
Epoch 92/1000, LR 0.000266
Train loss: 0.0037;  Loss pred: 0.0037; Loss self: 0.0000; time: 0.23s
Val loss: 0.5593 score: 0.8367 time: 0.07s
Test loss: 0.2596 score: 0.8958 time: 0.08s
     INFO: Early stopping counter 18 of 20
Epoch 93/1000, LR 0.000265
Train loss: 0.0027;  Loss pred: 0.0027; Loss self: 0.0000; time: 0.23s
Val loss: 0.5851 score: 0.8163 time: 0.07s
Test loss: 0.2722 score: 0.8750 time: 0.08s
     INFO: Early stopping counter 19 of 20
Epoch 94/1000, LR 0.000265
Train loss: 0.0025;  Loss pred: 0.0025; Loss self: 0.0000; time: 0.23s
Val loss: 0.6127 score: 0.7959 time: 0.07s
Test loss: 0.2868 score: 0.8750 time: 0.07s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 073,   Train_Loss: 0.0063,   Val_Loss: 0.3188,   Val_Precision: 0.9130,   Val_Recall: 0.8400,   Val_accuracy: 0.8750,   Val_Score: 0.8776,   Val_Loss: 0.3188,   Test_Precision: 1.0000,   Test_Recall: 0.9167,   Test_accuracy: 0.9565,   Test_Score: 0.9583,   Test_loss: 0.1660


[0.07443801604676992, 0.07515534397680312, 0.06970977096352726, 0.0701145560014993, 0.07010411901865155, 0.07003115804400295, 0.0698851109482348, 0.07032394292764366, 0.07047844398766756, 0.07545706094242632, 0.07821953704115003, 0.07127622095867991, 0.06992451206315309, 0.06982740201056004, 0.07016196893528104, 0.07015579997096211, 0.06982781598344445, 0.06995753897354007, 0.07028340792749077, 0.0701320159714669, 0.07051167206373066, 0.06991060602013022, 0.07012491603381932, 0.07005927502177656, 0.07072758197318763, 0.0692550279200077, 0.07007003296166658, 0.06939170695841312, 0.06957274000160396, 0.06959549395833164, 0.06955265300348401, 0.06963080109562725, 0.06992117897607386, 0.07002775394357741, 0.07053243299014866, 0.07000190101098269, 0.07034723402466625, 0.0700900600058958, 0.07241663592867553, 0.07030806306283921, 0.06977185897994787, 0.06985066109336913, 0.07008533098269254, 0.06990105798467994, 0.06988502701278776, 0.06970717397052795, 0.06972332810983062, 0.06962149601895362, 0.06978827901184559, 0.069545833976008, 0.07007318292744458, 0.07027202099561691, 0.0704931499203667, 0.06982055795378983, 0.0706117149675265, 0.06942899199202657, 0.0695010629715398, 0.07076785003300756, 0.07009374804329127, 0.0698990699602291, 0.0703772830311209, 0.07105008594226092, 0.07407825207337737, 0.07455620192922652, 0.0702252839691937, 0.07393042405601591, 0.07154009502846748, 0.14506798202637583, 0.07309828093275428, 0.07042989192996174, 0.07038332894444466, 0.0710566610796377, 0.15609750198200345, 0.07061606808565557, 0.07039154705125839, 0.07063525100238621, 0.07139576296322048, 0.11075869807973504, 0.07198097696527839, 0.07098426797892898, 0.07077269698493183, 0.06975560600403696, 0.06997615692671388, 0.06999703799374402, 0.06984000303782523, 0.07020647393073887, 0.06980782106984407, 0.06986107910051942, 0.06980337190907449, 0.06923626398202032, 0.0694242239696905, 0.0692466429900378, 0.07181345799472183, 0.07315180893056095, 0.07331126101780683, 0.07313221006188542, 0.07310364209115505, 0.07319867599289864, 0.07392473402433097, 0.07334424299187958, 0.0730474169831723, 0.07799262693151832, 0.07435516000259668, 0.07366244797594845, 0.08369144599419087, 0.08440061402507126, 0.0833840660052374, 0.08352386602200568, 0.08818435797002167, 0.08691783889662474, 0.08355965896043926, 0.08252945099957287, 0.08687249303329736, 0.0854533069068566, 0.08314613997936249, 0.08370063989423215, 0.08306428801734, 0.08380979101639241, 0.0836168039822951, 0.08336541801691055, 0.08329843508545309, 0.08321589196566492, 0.08327036793343723, 0.08379609300754964, 0.08325746702030301, 0.08392773498781025, 0.08379267808049917, 0.07583492901176214, 0.08035896299406886, 0.07625842292327434, 0.08123049500863999, 0.07611042307689786, 0.08167942496947944, 0.08190250198822469, 0.09049008600413799, 0.09068919508717954, 0.0855920979520306, 0.08049569302238524, 0.08461884409189224, 0.08505971089471132, 0.07966088701505214, 0.0796335469931364, 0.08478068897966295, 0.0848678449401632, 0.08044963900465518, 0.08021939301397651, 0.08500510500743985, 0.08026695798616856, 0.07974280708003789, 0.08386903803329915, 0.07906896795611829, 0.07945555995684117, 0.08444605802651495, 0.08419633901212364, 0.08466199203394353, 0.0797544049564749, 0.08433717302978039, 0.07930856302846223, 0.07970653602387756, 0.08483098796568811, 0.0845094519900158, 0.07929860195145011, 0.08347232104279101, 0.08438755397219211, 0.07833606901112944, 0.07833724503871053, 0.07879248901735991, 0.08403082692530006, 0.0780143189476803, 0.08303314901422709, 0.0821779859252274, 0.0849507130915299, 0.07741229399107397, 0.08278985298238695, 0.08236098592169583, 0.08264273800887167, 0.082409692928195, 0.0826125378953293, 0.07796419796068221, 0.07736550003755838, 0.08844163396861404, 0.08092263794969767, 0.08018629299476743, 0.07694832095876336, 0.07734844705555588, 0.07750019896775484, 0.07691126898862422, 0.07669501507189125, 0.0764487209962681, 0.07748079998418689, 0.07737761700991541, 0.07775434898212552, 0.08055118401534855, 0.0814690119586885, 0.08082847693003714, 0.07530201796907932, 0.08009419299196452, 0.08304826100356877, 0.07940054195933044, 0.07532491802703589, 0.08009998907800764, 0.07649925199802965, 0.07873755996115506, 0.0772684880066663, 0.07357150607276708, 0.07652872696053237, 0.07702975394204259, 0.07578664901666343, 0.0764190019108355, 0.0809412969974801, 0.08810237306170166, 0.07621887989807874, 0.07576589100062847, 0.0797013909323141, 0.08020854694768786, 0.08124069892801344, 0.0767003339715302, 0.08035870303865522, 0.08037064503878355, 0.08129754208493978, 0.07814851601142436]
[0.0015191431846279576, 0.0015337825301388393, 0.0014226483870107606, 0.0014309093061530469, 0.0014306963065030928, 0.0014292073070204683, 0.0014262267540456081, 0.0014351825087274216, 0.0014383355915850522, 0.0015399400192331902, 0.0015963170824724498, 0.0014546167542587739, 0.0014270308584316957, 0.0014250490206236743, 0.0014318769170465519, 0.0014317510198155533, 0.0014250574690498868, 0.0014277048770110219, 0.0014343552638263423, 0.001431265632070753, 0.00143901371558634, 0.0014267470616353105, 0.0014311207353840678, 0.0014297811228933992, 0.0014434200402691352, 0.0014133679167348512, 0.0014300006726870732, 0.001416157284865574, 0.0014198518367674277, 0.001420316203231258, 0.001419441898030286, 0.0014210367570536174, 0.0014269628362464054, 0.0014291378355832125, 0.0014394374079622176, 0.0014286102247139324, 0.0014356578372380867, 0.0014304093878754244, 0.0014778905291566436, 0.0014348584298538615, 0.001423915489386691, 0.0014255236957830433, 0.001430312877197807, 0.0014265522037689782, 0.0014262250410773012, 0.0014225953871536317, 0.0014229250634659309, 0.0014208468575296657, 0.0014242505920784815, 0.0014193027342042448, 0.0014300649577029506, 0.0014341228774615697, 0.001438635712660545, 0.0014249093459957108, 0.0014410554075005408, 0.0014169182039189096, 0.001418389040235506, 0.0014442418374083176, 0.0014304846539447199, 0.0014265116318414102, 0.0014362710822677734, 0.0014500017539236922, 0.0015118010627219872, 0.0015215551414127862, 0.0014331690605957897, 0.001508784164408488, 0.0014600019393564792, 0.002960571061762772, 0.0014918016516888629, 0.0014373447332645254, 0.0014363944682539726, 0.0014501359404007696, 0.0031856633057551725, 0.001441144246646032, 0.0014365621847195588, 0.0014415357347425757, 0.0014570563870044996, 0.0022603815934639803, 0.0014689995299036406, 0.001448658530182224, 0.0014443407547945271, 0.0014235837960007544, 0.001428084835239059, 0.0014285109794641637, 0.0014253061844454128, 0.0014327851822599769, 0.0014246494095886545, 0.0014257363081738657, 0.0014245586103892752, 0.0014129849792249044, 0.0014168208973406224, 0.001413196795715057, 0.0014655807754024863, 0.0014928940598073663, 0.001496148184036874, 0.0014924940828956207, 0.001491911063084797, 0.0014938505304673193, 0.001508668041312877, 0.0014968212855485628, 0.0014907636119014754, 0.001591686263908537, 0.0015174522449509526, 0.0015033152648152746, 0.0017079886937589974, 0.00172246151071574, 0.001701715632759947, 0.0017045686943266466, 0.0017996807748984014, 0.0017738334468698927, 0.0017052991624579442, 0.0016842745101953648, 0.0017729080210877012, 0.0017439450389154407, 0.0016968599995788262, 0.0017081763243720848, 0.0016951895513742858, 0.0017104038982937227, 0.0017064653873937775, 0.0017013350615696031, 0.0016999680629684304, 0.0016982835095033658, 0.0016993952639476986, 0.0017101243470928498, 0.0016991319800061838, 0.0017128109181185765, 0.0017100546547040647, 0.0015798943544117112, 0.0016741450623764347, 0.001588717144234882, 0.0016923019793466665, 0.0015856338141020387, 0.001701654686864155, 0.001706302124754681, 0.0018852101250862081, 0.0018893582309829071, 0.0017831687073339708, 0.0016769936046330258, 0.001762892585247755, 0.0017720773103064857, 0.0016596018128135863, 0.0016590322290236752, 0.0017662643537429783, 0.0017680801029200666, 0.0016760341459303163, 0.001671237354457844, 0.001770939687654997, 0.0016722282913785118, 0.0016613084808341227, 0.0017472716256937322, 0.0016472701657524642, 0.0016553241657675244, 0.0017592928755523947, 0.0017540903960859093, 0.001763791500707157, 0.0016615501032598938, 0.0017570244381204247, 0.0016522617297596298, 0.0016605528338307825, 0.001767312249285169, 0.0017606135831253293, 0.0016520542073218774, 0.0017390066883914794, 0.0017580740410873357, 0.0016320014377318632, 0.0016320259383064695, 0.001641510187861665, 0.0017506422276104179, 0.0016252983114100061, 0.001729857271129731, 0.0017120413734422375, 0.0017698065227402064, 0.001612756124814041, 0.001724788603799728, 0.001715853873368663, 0.0017217237085181598, 0.0017168686026707292, 0.001721094539486027, 0.0016242541241808794, 0.0016117812507824663, 0.0018425340410127926, 0.0016858882906187016, 0.0016705477707243215, 0.0016030900199742366, 0.001611425980324081, 0.0016145874784948926, 0.0016023181039296712, 0.0015978128139977343, 0.001592681687422252, 0.0016141833330038935, 0.001612033687706571, 0.0016198822704609483, 0.0016781496669864282, 0.001697271082472677, 0.001683926602709107, 0.0015687920410224858, 0.0016686290206659276, 0.001730172104241016, 0.001654177957486051, 0.0015692691255632478, 0.0016687497724584925, 0.0015937344166256178, 0.0016403658325240638, 0.001609760166805548, 0.001532739709849314, 0.0015943484783444244, 0.0016047865404592205, 0.0015788885211804882, 0.001592062539809073, 0.0016862770207808353, 0.0018354661054521177, 0.0015878933312099737, 0.0015784560625130932, 0.0016604456444232103, 0.0016710113947434972, 0.00169251456100028, 0.001597923624406879, 0.0016741396466386504, 0.0016743884383079906, 0.0016936987934362453, 0.0016280940835713409]
[658.26579753567, 651.9829117557358, 702.9143737344541, 698.856311647359, 698.9603561948095, 699.6885581873662, 701.1507792596225, 696.7754929557367, 695.248039366109, 649.3759416018994, 626.4419587937716, 687.4663013967332, 700.7556943085297, 701.7302461373216, 698.3840497007529, 698.445460251061, 701.7260859428492, 700.4248679836093, 697.1773487499609, 698.6823253438996, 694.9204091446345, 700.8950828704135, 698.7530648360228, 699.4077512901655, 692.7990273805145, 707.5298569888231, 699.300370342434, 706.1362538518616, 704.298838868073, 704.068571297696, 704.5022423162709, 703.711564839043, 700.7890987760258, 699.7225705608116, 694.7158622309807, 699.9809904064223, 696.5447992286214, 699.1005571386049, 676.6401030871, 696.9328675177026, 702.288869988148, 701.4965818935038, 699.1477291032623, 700.9908206359227, 701.1516213771205, 702.9405613361561, 702.7776976281667, 703.8056175446206, 702.1236329911923, 704.5713193532782, 699.2689350323326, 697.2903198992425, 695.1030001546724, 701.7990322052461, 693.9358436845009, 705.7570417503297, 705.0251881768364, 692.4048134448822, 699.0637734158065, 701.0107577665888, 696.2473953183467, 689.6543382061495, 661.4626915260312, 657.2223199689532, 697.7543874581587, 662.7853231691662, 684.9305970379513, 337.77267261559456, 670.3304014095331, 695.7273205633701, 696.1875878118374, 689.5905219228158, 313.9063686339403, 693.8930661016724, 696.1063089623349, 693.7046206340326, 686.3152372955569, 442.4031778048255, 680.735411852443, 690.2937988251874, 692.3573932816572, 702.4525024865274, 700.2385119736964, 700.0296213159673, 701.6036350035907, 697.9413329935956, 701.9270799324127, 701.39197147952, 701.9718196970076, 707.7216068839966, 705.8055128047612, 707.615530287142, 682.3233606659272, 669.8398948208246, 668.3829921858555, 670.0194067502619, 670.2812417868385, 669.4110150947774, 662.8363381581129, 668.0824288475526, 670.797161948765, 628.2645158628214, 658.9993216111537, 665.1964650427991, 585.4839693342275, 580.5644966687626, 587.6422480635842, 586.6586681594716, 555.6540992979457, 563.7507860529975, 586.4073718060375, 593.72744403999, 564.0450537228025, 573.4125661562706, 589.3238100068407, 585.4196582238627, 589.9045326166048, 584.6572268676348, 586.0066119051285, 587.7736976027688, 588.2463452012337, 588.8298357748483, 588.4446198096363, 584.7527998183081, 588.5358004952391, 583.8356057996419, 584.776631114785, 632.9537144098217, 597.3198036856546, 629.438666051278, 590.9110857307287, 630.662635412017, 587.6632948620269, 586.0626822719173, 530.444848928589, 529.2802516756003, 560.7994329908961, 596.3051959395091, 567.2495354329607, 564.3094656107567, 602.5541743080298, 602.7610449668543, 566.1666657546763, 565.5852347121906, 596.6465554584092, 598.3590525502608, 564.6719687693924, 598.004474123353, 601.9351682945194, 572.3208603029667, 607.0649616501768, 604.1112796394958, 568.4101913309989, 570.0960464930472, 566.9604369898993, 601.8476349512666, 569.1440473473147, 605.2309885223084, 602.2090834008984, 565.8309675635834, 567.9838038196114, 605.3070144841594, 575.0409165619513, 568.804257744184, 612.7445582338386, 612.735359486802, 609.1951225125586, 571.2189413852849, 615.271666117996, 578.0823751701332, 584.0980337930712, 565.0335147661743, 620.0565507790616, 579.781196256161, 582.8002113237898, 580.813283253602, 582.4557560458724, 581.0256072851358, 615.6672069429442, 620.431587422011, 542.7308140534143, 593.159111172788, 598.6060485815481, 623.7952875634963, 620.5683737324911, 619.3532486280602, 624.0958006699848, 625.8555390465268, 627.8718515427245, 619.5083170255903, 620.3344307417625, 617.3288134794162, 595.8944066030598, 589.1810744475453, 593.8501110388045, 637.4331165960236, 599.294383362045, 577.9771836274492, 604.5298787077041, 637.2393260723054, 599.2510180401379, 627.4571155445586, 609.6201104489478, 621.2105508763005, 652.4264971893443, 627.2154510652543, 623.1358344480152, 633.3569384951444, 628.1160287333463, 593.0223727634901, 544.820739009875, 629.7652243668034, 633.5304629308965, 602.2479587685451, 598.4399646499727, 590.8368666612815, 625.8121381559662, 597.3217359781229, 597.2329819778998, 590.423754138219, 614.215118211368]
Elapsed: 0.07726362949169802~0.009396261831797446
Time per graph: 0.0015913617237342925~0.00019762193461703867
Speed: 635.6445629883954~61.33114912095285
Total Time: 0.0789
best val loss: 0.3188137710094452 test_score: 0.9583

Testing...
Test loss: 0.2591 score: 0.9167 time: 0.07s
test Score 0.9167
Epoch Time List: [0.40598622092511505, 0.3925150230061263, 0.3719427710166201, 0.36235095805022866, 0.3638636509422213, 0.3612426280742511, 0.36314024310559034, 0.3768875129753724, 0.364475739072077, 0.37043395393993706, 0.39386913494672626, 0.3805075428681448, 0.36680847499519587, 0.3663335299352184, 0.36502121412195265, 0.36311766295693815, 0.3664956738939509, 0.3652382841100916, 0.36562071181833744, 0.36582148098386824, 0.3640221789246425, 0.36479218606837094, 0.3641594839282334, 0.363830262911506, 0.3630820970283821, 0.367609893088229, 0.36315576906781644, 0.35447285894770175, 0.3556863151025027, 0.3558466361137107, 0.36047840502578765, 0.3563241600058973, 0.3633434701478109, 0.3641901550581679, 0.3664606548845768, 0.3664302929537371, 0.36617229494731873, 0.36655607412103564, 0.3680777479894459, 0.37097318889573216, 0.3649524931097403, 0.36310805508401245, 0.3634822841268033, 0.36399665696080774, 0.36387364007532597, 0.36351734795607626, 0.36517494602594525, 0.3616459210170433, 0.36256820790003985, 0.36116578395012766, 0.3621276100166142, 0.36647887993603945, 0.3691353980684653, 0.3691512190271169, 0.3659595559583977, 0.3630374049535021, 0.36316906311549246, 0.3657729639671743, 0.36286351806484163, 0.36303891404531896, 0.3638368539977819, 0.36470915214158595, 0.36940707999747247, 0.47600927494931966, 0.36007962201256305, 0.37158160493709147, 0.38086180796381086, 0.4489219569368288, 0.3850720601622015, 0.3627651579445228, 0.3673603158676997, 0.3649717739317566, 0.45397453010082245, 0.3608977959956974, 0.3632565301377326, 0.3669070260366425, 0.3642123849131167, 0.40019670396577567, 0.3630345780402422, 0.370935364975594, 0.3698880310403183, 0.3583664170000702, 0.35720202105585486, 0.3560828340705484, 0.3553011619951576, 0.35671770793851465, 0.3559674918651581, 0.35552852402906865, 0.35522070492152125, 0.3553438850212842, 0.3526659010676667, 0.3527164408005774, 0.36152526398655027, 0.38023045705631375, 0.37982918694615364, 0.37982421391643584, 0.3728317229542881, 0.3728484599851072, 0.3755434580380097, 0.3742687391350046, 0.37342023418750614, 0.3797261060681194, 0.3898288367781788, 0.3816212188685313, 0.373939301003702, 0.375003284891136, 0.3720867991214618, 0.37319309380836785, 0.3947485878597945, 0.39174344413913786, 0.3717328008497134, 0.37067409709561616, 0.38377638801466674, 0.3876937549794093, 0.37933943001553416, 0.38027884904295206, 0.37779283500276506, 0.3783412529155612, 0.38014038116671145, 0.38012026285286993, 0.3804532721405849, 0.3769439230673015, 0.3775351420044899, 0.3808645820245147, 0.3778316299431026, 0.3804524299921468, 0.3807495430810377, 0.39411846606526524, 0.3710531189572066, 0.3717535190517083, 0.38145900215022266, 0.3877480640076101, 0.3778520318446681, 0.3812136941123754, 0.4129702689824626, 0.42948888591490686, 0.40452442097011954, 0.3902213660767302, 0.3964585087960586, 0.4013093520188704, 0.3949706460116431, 0.3937775449594483, 0.40271660208236426, 0.3944062350783497, 0.3977131579304114, 0.39494899788405746, 0.40733131510205567, 0.3984671349171549, 0.39396735397167504, 0.39655336108990014, 0.3978791710687801, 0.3941329299705103, 0.398131504887715, 0.3981171570485458, 0.4015390800777823, 0.3945366720436141, 0.3942756690084934, 0.39184955693781376, 0.39151988504454494, 0.41812660405412316, 0.4023811330553144, 0.39836293016560376, 0.3937089720275253, 0.4033486610278487, 0.39629585901275277, 0.3825930339517072, 0.39269456593319774, 0.38640612794552, 0.38279090402647853, 0.3880485659465194, 0.3773291199468076, 0.3880756569560617, 0.37826182413846254, 0.3848260309314355, 0.37951742310542613, 0.3848584118532017, 0.39086588798090816, 0.3837874060263857, 0.37881796702276915, 0.37662284809630364, 0.40347313589882106, 0.380402828915976, 0.3773406910477206, 0.37767084303777665, 0.384320750948973, 0.3822856030892581, 0.37388098787050694, 0.3785637500695884, 0.3739015969913453, 0.37702364195138216, 0.3744925018399954, 0.3790662180399522, 0.38147473498247564, 0.3802659489447251, 0.37559047108516097, 0.3667264268733561, 0.37681759102270007, 0.40050984802655876, 0.37664865993428975, 0.3667540509486571, 0.37352680903859437, 0.36495594400912523, 0.3738734119106084, 0.40046710381284356, 0.3773848828859627, 0.3758066708687693, 0.3938636091770604, 0.4521693579154089, 0.3758283630013466, 0.38139822089578956, 0.4026410321239382, 0.36887456802651286, 0.3834514640038833, 0.3796659529907629, 0.37086147896479815, 0.3797052799491212, 0.3620256199501455, 0.48843719880096614, 0.3697075170930475, 0.37378260295372456, 0.3751774369738996]
Total Epoch List: [104, 23, 94]
Total Time List: [0.07423749496228993, 0.0843661311082542, 0.07889693498145789]
T-times Epoch Time: 0.3851065464213583 ~ 0.004557986290414954
T-times Total Epoch: 80.33333333333333 ~ 11.118886168570556
T-times Total Time: 0.08141185799872296 ~ 0.0016834913341957575
T-times Inference Elapsed: 0.0802108313626712 ~ 0.0022666743226780087
T-times Time Per Graph: 0.0016507869677776915 ~ 4.665126854685914e-05
T-times Speed: 612.1915608509341 ~ 18.672413053240422
T-times cross validation test micro f1 score:0.7257386199415184 ~ 0.09709841665406727
T-times cross validation test precision:0.7517667517667518 ~ 0.11867486161359339
T-times cross validation test recall:0.7046296296296296 ~ 0.12779978339765816
T-times cross validation test f1_score:0.7257386199415184 ~ 0.12201528602868825
