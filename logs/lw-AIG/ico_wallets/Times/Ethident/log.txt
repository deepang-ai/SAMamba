Namespace(seed=60, model='Ethident', dataset='ico_wallets/Times', num_heads=1, num_layers=2, dim_hidden=64, dropout=0.1, epochs=1000, lr=0.0003, weight_decay=0.0001, batch_size=32, abs_pe='lap', abs_pe_dim=20, num_class=2, outdir='./outdir/ico_wallets/Times/seed60/edge_attr/khopgnn_gat_1_0.1_0.0003_0.0001_2_1_64_BN', warmup=10, layer_norm=False, use_edge_attr=True, edge_dim=128, gnn_type='gat', k_hop=1, global_pool='mean', se='khopgnn', ie='mamba', aggr='add', not_extract_node_feature=False, k_ford=3, early_stop=1, early_stop_mindelta=-0.0, patience=20, training_times=3, Lambda=0.01, order='sd', use_cuda=True, batch_norm=True, save_logs=True)
Data(edge_index=[2, 338], edge_attr=[338, 2], x=[122, 14887], y=[1, 1], num_nodes=122)
Data(edge_index=[2, 338], edge_attr=[338, 2], x=[122, 14887], y=[1, 1], num_nodes=122)
Data(edge_index=[2, 302], edge_attr=[302, 2], x=[110, 14887], y=[1, 1], num_nodes=122)
========================training times:0========================
========================k_idx:0========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
emb dim:  64
Model: Ethident
Ethident(
  (encoder): HGATE_encoder(
    (lin1): Linear(in_features=14887, out_features=64, bias=True)
    (account_convs): ModuleList(
      (0): GATEConv()
      (1): GATConv(64, 64, heads=1)
    )
    (subgraph_conv): GATConv(64, 64, heads=1)
    (lin2): Linear(in_features=64, out_features=64, bias=True)
    (lin3): Linear(in_features=64, out_features=2, bias=True)
  )
  (proj_head_g1): Project_Head(
    (block): Sequential(
      (0): Linear(in_features=64, out_features=64, bias=True)
      (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=64, bias=True)
      (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
      (5): ReLU()
    )
    (linear_shortcut): Linear(in_features=64, out_features=64, bias=True)
  )
  (proj_head_g2): Project_Head(
    (block): Sequential(
      (0): Linear(in_features=64, out_features=64, bias=True)
      (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=64, bias=True)
      (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
      (5): ReLU()
    )
    (linear_shortcut): Linear(in_features=64, out_features=64, bias=True)
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=64, bias=True)
    (1): ReLU()
    (2): Dropout(p=0.0, inplace=True)
    (3): Linear(in_features=64, out_features=2, bias=True)
  )
)
Total number of parameters: 1003972
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7798933d9600>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.8373;  Loss pred: 0.8095; Loss self: 2.7745; time: 28.51s
Val loss: 0.7800 score: 0.3469 time: 4.34s
Test loss: 0.7738 score: 0.3061 time: 0.76s
Epoch 2/1000, LR 0.000015
Train loss: 0.8400;  Loss pred: 0.8122; Loss self: 2.7829; time: 13.86s
Val loss: 0.7412 score: 0.3265 time: 3.90s
Test loss: 0.7390 score: 0.3469 time: 6.76s
Epoch 3/1000, LR 0.000045
Train loss: 0.7895;  Loss pred: 0.7614; Loss self: 2.8087; time: 14.52s
Val loss: 0.6914 score: 0.5918 time: 7.07s
Test loss: 0.6962 score: 0.5102 time: 4.62s
Epoch 4/1000, LR 0.000075
Train loss: 0.7254;  Loss pred: 0.6971; Loss self: 2.8326; time: 13.81s
Val loss: 0.6374 score: 0.7551 time: 6.18s
Test loss: 0.6482 score: 0.7347 time: 6.38s
Epoch 5/1000, LR 0.000105
Train loss: 0.6556;  Loss pred: 0.6273; Loss self: 2.8344; time: 19.73s
Val loss: 0.6061 score: 0.8163 time: 6.46s
Test loss: 0.6196 score: 0.7551 time: 2.29s
Epoch 6/1000, LR 0.000135
Train loss: 0.6127;  Loss pred: 0.5844; Loss self: 2.8292; time: 6.83s
Val loss: 0.5882 score: 0.7755 time: 0.13s
Test loss: 0.6064 score: 0.7347 time: 2.02s
Epoch 7/1000, LR 0.000165
Train loss: 0.5860;  Loss pred: 0.5578; Loss self: 2.8203; time: 16.12s
Val loss: 0.5763 score: 0.7755 time: 5.26s
Test loss: 0.6004 score: 0.7143 time: 5.70s
Epoch 8/1000, LR 0.000195
Train loss: 0.5679;  Loss pred: 0.5398; Loss self: 2.8122; time: 16.75s
Val loss: 0.5548 score: 0.7959 time: 7.84s
Test loss: 0.5853 score: 0.7143 time: 7.55s
Epoch 9/1000, LR 0.000225
Train loss: 0.5420;  Loss pred: 0.5140; Loss self: 2.8016; time: 15.68s
Val loss: 0.5132 score: 0.8163 time: 4.82s
Test loss: 0.5538 score: 0.7347 time: 0.16s
Epoch 10/1000, LR 0.000255
Train loss: 0.4983;  Loss pred: 0.4704; Loss self: 2.7815; time: 9.92s
Val loss: 0.4547 score: 0.8571 time: 6.81s
Test loss: 0.5069 score: 0.7959 time: 7.15s
Epoch 11/1000, LR 0.000285
Train loss: 0.4411;  Loss pred: 0.4136; Loss self: 2.7418; time: 12.19s
Val loss: 0.3996 score: 0.8980 time: 5.22s
Test loss: 0.4641 score: 0.8367 time: 4.94s
Epoch 12/1000, LR 0.000285
Train loss: 0.3850;  Loss pred: 0.3581; Loss self: 2.6924; time: 12.29s
Val loss: 0.3576 score: 0.8980 time: 4.94s
Test loss: 0.4336 score: 0.8571 time: 4.91s
Epoch 13/1000, LR 0.000285
Train loss: 0.3414;  Loss pred: 0.3150; Loss self: 2.6420; time: 14.72s
Val loss: 0.3314 score: 0.8980 time: 6.15s
Test loss: 0.4174 score: 0.8571 time: 6.61s
Epoch 14/1000, LR 0.000285
Train loss: 0.3177;  Loss pred: 0.2918; Loss self: 2.5926; time: 17.58s
Val loss: 0.3121 score: 0.8980 time: 7.95s
Test loss: 0.4063 score: 0.8571 time: 5.49s
Epoch 15/1000, LR 0.000285
Train loss: 0.2989;  Loss pred: 0.2733; Loss self: 2.5609; time: 18.06s
Val loss: 0.2921 score: 0.8980 time: 6.96s
Test loss: 0.3922 score: 0.8571 time: 6.61s
Epoch 16/1000, LR 0.000285
Train loss: 0.2743;  Loss pred: 0.2487; Loss self: 2.5521; time: 15.57s
Val loss: 0.2756 score: 0.9184 time: 5.60s
Test loss: 0.3778 score: 0.8776 time: 4.73s
Epoch 17/1000, LR 0.000285
Train loss: 0.2521;  Loss pred: 0.2267; Loss self: 2.5463; time: 18.06s
Val loss: 0.2600 score: 0.9184 time: 10.67s
Test loss: 0.3657 score: 0.8776 time: 8.01s
Epoch 18/1000, LR 0.000285
Train loss: 0.2349;  Loss pred: 0.2095; Loss self: 2.5341; time: 21.20s
Val loss: 0.2449 score: 0.8980 time: 6.97s
Test loss: 0.3542 score: 0.8776 time: 6.28s
Epoch 19/1000, LR 0.000285
Train loss: 0.2217;  Loss pred: 0.1965; Loss self: 2.5193; time: 17.77s
Val loss: 0.2317 score: 0.8980 time: 6.02s
Test loss: 0.3454 score: 0.8571 time: 3.04s
Epoch 20/1000, LR 0.000285
Train loss: 0.2099;  Loss pred: 0.1849; Loss self: 2.5080; time: 16.79s
Val loss: 0.2214 score: 0.8980 time: 6.25s
Test loss: 0.3398 score: 0.8571 time: 5.41s
Epoch 21/1000, LR 0.000285
Train loss: 0.2002;  Loss pred: 0.1752; Loss self: 2.5008; time: 12.63s
Val loss: 0.2139 score: 0.8980 time: 4.75s
Test loss: 0.3372 score: 0.8571 time: 7.16s
Epoch 22/1000, LR 0.000285
Train loss: 0.1919;  Loss pred: 0.1669; Loss self: 2.4944; time: 11.95s
Val loss: 0.2078 score: 0.9184 time: 5.15s
Test loss: 0.3355 score: 0.8367 time: 5.12s
Epoch 23/1000, LR 0.000285
Train loss: 0.1846;  Loss pred: 0.1597; Loss self: 2.4894; time: 17.54s
Val loss: 0.2013 score: 0.9184 time: 4.58s
Test loss: 0.3336 score: 0.8571 time: 7.69s
Epoch 24/1000, LR 0.000285
Train loss: 0.1772;  Loss pred: 0.1523; Loss self: 2.4870; time: 17.57s
Val loss: 0.1942 score: 0.9184 time: 6.33s
Test loss: 0.3304 score: 0.8571 time: 5.47s
Epoch 25/1000, LR 0.000285
Train loss: 0.1684;  Loss pred: 0.1436; Loss self: 2.4873; time: 15.91s
Val loss: 0.1857 score: 0.9388 time: 3.40s
Test loss: 0.3271 score: 0.8776 time: 6.99s
Epoch 26/1000, LR 0.000285
Train loss: 0.1590;  Loss pred: 0.1342; Loss self: 2.4877; time: 16.64s
Val loss: 0.1758 score: 0.9388 time: 7.30s
Test loss: 0.3247 score: 0.8980 time: 6.26s
Epoch 27/1000, LR 0.000285
Train loss: 0.1494;  Loss pred: 0.1246; Loss self: 2.4857; time: 15.37s
Val loss: 0.1675 score: 0.9388 time: 7.04s
Test loss: 0.3215 score: 0.8776 time: 5.94s
Epoch 28/1000, LR 0.000285
Train loss: 0.1407;  Loss pred: 0.1159; Loss self: 2.4802; time: 17.82s
Val loss: 0.1607 score: 0.9388 time: 2.55s
Test loss: 0.3190 score: 0.8776 time: 2.26s
Epoch 29/1000, LR 0.000285
Train loss: 0.1326;  Loss pred: 0.1079; Loss self: 2.4715; time: 15.87s
Val loss: 0.1556 score: 0.9592 time: 5.26s
Test loss: 0.3190 score: 0.8776 time: 5.40s
Epoch 30/1000, LR 0.000285
Train loss: 0.1249;  Loss pred: 0.1003; Loss self: 2.4607; time: 12.44s
Val loss: 0.1512 score: 0.9592 time: 4.10s
Test loss: 0.3198 score: 0.8776 time: 2.94s
Epoch 31/1000, LR 0.000285
Train loss: 0.1178;  Loss pred: 0.0933; Loss self: 2.4469; time: 18.76s
Val loss: 0.1480 score: 0.9592 time: 6.50s
Test loss: 0.3224 score: 0.8571 time: 3.78s
Epoch 32/1000, LR 0.000285
Train loss: 0.1107;  Loss pred: 0.0863; Loss self: 2.4348; time: 25.61s
Val loss: 0.1455 score: 0.9592 time: 7.08s
Test loss: 0.3264 score: 0.8571 time: 6.32s
Epoch 33/1000, LR 0.000285
Train loss: 0.1038;  Loss pred: 0.0796; Loss self: 2.4223; time: 10.11s
Val loss: 0.1437 score: 0.9388 time: 2.43s
Test loss: 0.3319 score: 0.8571 time: 2.81s
Epoch 34/1000, LR 0.000285
Train loss: 0.0963;  Loss pred: 0.0722; Loss self: 2.4120; time: 18.10s
Val loss: 0.1423 score: 0.9388 time: 3.95s
Test loss: 0.3378 score: 0.8571 time: 5.57s
Epoch 35/1000, LR 0.000285
Train loss: 0.0885;  Loss pred: 0.0644; Loss self: 2.4048; time: 16.09s
Val loss: 0.1414 score: 0.9388 time: 6.58s
Test loss: 0.3438 score: 0.8571 time: 6.77s
Epoch 36/1000, LR 0.000285
Train loss: 0.0808;  Loss pred: 0.0568; Loss self: 2.4011; time: 13.84s
Val loss: 0.1415 score: 0.9388 time: 3.69s
Test loss: 0.3491 score: 0.8776 time: 5.96s
     INFO: Early stopping counter 1 of 20
Epoch 37/1000, LR 0.000285
Train loss: 0.0739;  Loss pred: 0.0499; Loss self: 2.3980; time: 20.91s
Val loss: 0.1422 score: 0.9388 time: 3.83s
Test loss: 0.3547 score: 0.8776 time: 5.27s
     INFO: Early stopping counter 2 of 20
Epoch 38/1000, LR 0.000284
Train loss: 0.0680;  Loss pred: 0.0440; Loss self: 2.3951; time: 14.75s
Val loss: 0.1451 score: 0.9388 time: 4.19s
Test loss: 0.3621 score: 0.8776 time: 3.49s
     INFO: Early stopping counter 3 of 20
Epoch 39/1000, LR 0.000284
Train loss: 0.0629;  Loss pred: 0.0390; Loss self: 2.3911; time: 15.21s
Val loss: 0.1503 score: 0.9388 time: 4.41s
Test loss: 0.3719 score: 0.8776 time: 5.42s
     INFO: Early stopping counter 4 of 20
Epoch 40/1000, LR 0.000284
Train loss: 0.0585;  Loss pred: 0.0346; Loss self: 2.3879; time: 10.73s
Val loss: 0.1558 score: 0.9388 time: 9.13s
Test loss: 0.3835 score: 0.8776 time: 6.58s
     INFO: Early stopping counter 5 of 20
Epoch 41/1000, LR 0.000284
Train loss: 0.0547;  Loss pred: 0.0308; Loss self: 2.3876; time: 18.12s
Val loss: 0.1603 score: 0.9388 time: 6.37s
Test loss: 0.3957 score: 0.8776 time: 4.51s
     INFO: Early stopping counter 6 of 20
Epoch 42/1000, LR 0.000284
Train loss: 0.0513;  Loss pred: 0.0274; Loss self: 2.3894; time: 16.95s
Val loss: 0.1644 score: 0.9388 time: 0.12s
Test loss: 0.4066 score: 0.8571 time: 5.65s
     INFO: Early stopping counter 7 of 20
Epoch 43/1000, LR 0.000284
Train loss: 0.0484;  Loss pred: 0.0245; Loss self: 2.3931; time: 16.05s
Val loss: 0.1677 score: 0.9388 time: 5.42s
Test loss: 0.4167 score: 0.8776 time: 5.73s
     INFO: Early stopping counter 8 of 20
Epoch 44/1000, LR 0.000284
Train loss: 0.0459;  Loss pred: 0.0219; Loss self: 2.3977; time: 17.79s
Val loss: 0.1699 score: 0.9388 time: 6.69s
Test loss: 0.4249 score: 0.8776 time: 7.23s
     INFO: Early stopping counter 9 of 20
Epoch 45/1000, LR 0.000284
Train loss: 0.0438;  Loss pred: 0.0198; Loss self: 2.4028; time: 18.72s
Val loss: 0.1713 score: 0.9388 time: 3.00s
Test loss: 0.4317 score: 0.8776 time: 4.33s
     INFO: Early stopping counter 10 of 20
Epoch 46/1000, LR 0.000284
Train loss: 0.0420;  Loss pred: 0.0180; Loss self: 2.4076; time: 21.59s
Val loss: 0.1724 score: 0.9388 time: 9.62s
Test loss: 0.4381 score: 0.8776 time: 13.06s
     INFO: Early stopping counter 11 of 20
Epoch 47/1000, LR 0.000284
Train loss: 0.0406;  Loss pred: 0.0165; Loss self: 2.4118; time: 19.17s
Val loss: 0.1736 score: 0.9388 time: 6.35s
Test loss: 0.4440 score: 0.8776 time: 3.45s
     INFO: Early stopping counter 12 of 20
Epoch 48/1000, LR 0.000284
Train loss: 0.0393;  Loss pred: 0.0152; Loss self: 2.4154; time: 17.19s
Val loss: 0.1744 score: 0.9388 time: 6.75s
Test loss: 0.4495 score: 0.8776 time: 6.52s
     INFO: Early stopping counter 13 of 20
Epoch 49/1000, LR 0.000284
Train loss: 0.0383;  Loss pred: 0.0141; Loss self: 2.4183; time: 18.58s
Val loss: 0.1751 score: 0.9388 time: 5.08s
Test loss: 0.4546 score: 0.8776 time: 6.29s
     INFO: Early stopping counter 14 of 20
Epoch 50/1000, LR 0.000284
Train loss: 0.0374;  Loss pred: 0.0131; Loss self: 2.4204; time: 19.52s
Val loss: 0.1755 score: 0.9388 time: 2.92s
Test loss: 0.4588 score: 0.8776 time: 4.27s
     INFO: Early stopping counter 15 of 20
Epoch 51/1000, LR 0.000284
Train loss: 0.0365;  Loss pred: 0.0123; Loss self: 2.4223; time: 18.25s
Val loss: 0.1756 score: 0.9388 time: 6.86s
Test loss: 0.4628 score: 0.8776 time: 3.62s
     INFO: Early stopping counter 16 of 20
Epoch 52/1000, LR 0.000284
Train loss: 0.0357;  Loss pred: 0.0115; Loss self: 2.4240; time: 12.80s
Val loss: 0.1755 score: 0.9388 time: 7.49s
Test loss: 0.4660 score: 0.8776 time: 6.78s
     INFO: Early stopping counter 17 of 20
Epoch 53/1000, LR 0.000284
Train loss: 0.0350;  Loss pred: 0.0108; Loss self: 2.4254; time: 10.11s
Val loss: 0.1753 score: 0.9388 time: 1.04s
Test loss: 0.4687 score: 0.8776 time: 4.83s
     INFO: Early stopping counter 18 of 20
Epoch 54/1000, LR 0.000284
Train loss: 0.0344;  Loss pred: 0.0101; Loss self: 2.4266; time: 19.40s
Val loss: 0.1752 score: 0.9388 time: 6.04s
Test loss: 0.4718 score: 0.8776 time: 6.10s
     INFO: Early stopping counter 19 of 20
Epoch 55/1000, LR 0.000284
Train loss: 0.0338;  Loss pred: 0.0095; Loss self: 2.4274; time: 9.80s
Val loss: 0.1754 score: 0.9388 time: 5.83s
Test loss: 0.4753 score: 0.8776 time: 6.28s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 034,   Train_Loss: 0.0885,   Val_Loss: 0.1414,   Val_Precision: 0.9565,   Val_Recall: 0.9167,   Val_accuracy: 0.9362,   Val_Score: 0.9388,   Val_Loss: 0.1414,   Test_Precision: 0.8462,   Test_Recall: 0.8800,   Test_accuracy: 0.8627,   Test_Score: 0.8571,   Test_loss: 0.3438


[0.7686839587986469, 6.768804287072271, 4.625396755989641, 6.389194326940924, 2.2933257948607206, 2.026728134136647, 5.704534021671861, 7.561505440156907, 0.16603926289826632, 7.161517888773233, 4.953778941184282, 4.92294054524973, 6.614439779892564, 5.501720665954053, 6.61980864033103, 4.735360140912235, 8.024891505017877, 6.286463771015406, 3.0429352140054107, 5.419238911010325, 7.165240897797048, 5.124797634314746, 7.696248745080084, 5.480486831162125, 7.000979672651738, 6.266845820005983, 5.948832578957081, 2.2653839280828834, 5.419196620117873, 2.9433139651082456, 3.7843113159760833, 6.33168278913945, 2.812547941226512, 5.583263126201928, 6.7728747758083045, 5.976028973702341, 5.278345126193017, 3.496696935966611, 5.422840062063187, 6.587074350100011, 4.515285735018551, 5.658104234840721, 5.744357842952013, 7.238298308104277, 4.338511097244918, 13.068057974800467, 3.4594231797382236, 6.53505743900314, 6.297936560120434, 4.281038506887853, 3.628647845238447, 6.785342067945749, 4.840944152791053, 6.107787818647921, 6.2909634951502085]
[0.01568742773058463, 0.13813886300147493, 0.09439585216305389, 0.13039172095797805, 0.046802567242055525, 0.041361798655849935, 0.11641906166677268, 0.15431643755422259, 0.003388556385678904, 0.14615342630149455, 0.10109752941192413, 0.10046817439285163, 0.13498856693658293, 0.11228001359089905, 0.1350981355169598, 0.0966400028757599, 0.16377329602077298, 0.1282951790003144, 0.06210071865317165, 0.11059671246959847, 0.14622940607749077, 0.10458770682274991, 0.1570663009200017, 0.11184667002371683, 0.14287713617656608, 0.12789481265318334, 0.12140474650932818, 0.04623232506291599, 0.11059584939016068, 0.060067631940984606, 0.07723084318318538, 0.12921801610488673, 0.057398937576051266, 0.1139441454326924, 0.13822193420016948, 0.12195977497351718, 0.10772132910597994, 0.07136116195850227, 0.11067020534822831, 0.13443008877755125, 0.09214868846976636, 0.1154715149967494, 0.11723179271330639, 0.14772037363478116, 0.0885410428009167, 0.26669506071021365, 0.07060047305588212, 0.13336851916332937, 0.12852931755347824, 0.08736813279362965, 0.0740540376579275, 0.1384763687335867, 0.09879477862838884, 0.12464873099281472, 0.1283870101051063]
[63.74531358320607, 7.239092448511922, 10.593685814421784, 7.669198570684367, 21.366349303621682, 24.176898309487967, 8.589658649391188, 6.48019106615669, 295.1109222282125, 6.842124918352147, 9.891438552622565, 9.953400726580242, 7.408034789122517, 8.906304586349425, 7.402026653983416, 10.347681811284682, 6.106001553960055, 7.794525155131117, 16.102873230581, 9.041860084899778, 6.838569798129892, 9.561353149225756, 6.366738085398269, 8.940811557357517, 6.999020464437434, 7.818925406394188, 8.23691024241103, 21.629887716854697, 9.041930646711652, 16.647901168843855, 12.948194772755233, 7.738858946637103, 17.42192525210141, 8.776229758910317, 7.234741763574409, 8.199424771135762, 9.283212603292016, 14.013224736748501, 9.035855647447832, 7.438810827944585, 10.852026400007812, 8.660144452319264, 8.53010925496574, 6.7695469175590235, 11.294197226121293, 3.749600751273692, 14.164211041595626, 7.498021319224163, 7.7803260690614175, 11.445820896299546, 13.503652624847119, 7.221448750753206, 10.121992415828425, 8.022544570129995, 7.78894998163235]
Elapsed: 5.376982842436568~2.0077440682914087
Time per graph: 0.10973434372319527~0.040974368740641004
Speed: 16.115321854990746~38.84779129144598
Total Time: 6.2929
best val loss: 0.14142802844242175 test_score: 0.8571

Testing...
Test loss: 0.3190 score: 0.8776 time: 4.50s
test Score 0.8776
Epoch Time List: [33.61070671770722, 24.52804783685133, 26.211539373267442, 26.357655082363635, 28.477681430988014, 8.985771173145622, 27.076829969882965, 32.134443745948374, 20.66188925690949, 23.883159703109413, 22.348556202836335, 22.141380277927965, 27.48112146742642, 31.016394657548517, 31.627181697636843, 25.89639938529581, 36.73797589773312, 34.44417666038498, 26.829468863084912, 28.451204562094063, 24.53798401216045, 22.212581276427954, 29.809701327700168, 29.370679318904877, 26.30203254101798, 30.19423669669777, 28.347532336600125, 22.633628678508103, 26.534385952632874, 19.480416053440422, 29.038026811089367, 39.01193031668663, 15.343819150701165, 27.62115154741332, 29.43502921750769, 23.49324120860547, 30.016454943921417, 22.425110215786844, 25.04016283713281, 26.43401210056618, 28.999981353525072, 22.71918402146548, 27.204176809173077, 31.712205359246582, 26.05744017055258, 44.26764624379575, 28.977288000285625, 30.456520432140678, 29.959311977028847, 26.717631313484162, 28.721850438509136, 27.06405829777941, 15.983577619772404, 31.534447425045073, 21.916031959932297]
Total Epoch List: [55]
Total Time List: [6.292933396995068]
========================k_idx:1========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
emb dim:  64
Model: Ethident
Ethident(
  (encoder): HGATE_encoder(
    (lin1): Linear(in_features=14887, out_features=64, bias=True)
    (account_convs): ModuleList(
      (0): GATEConv()
      (1): GATConv(64, 64, heads=1)
    )
    (subgraph_conv): GATConv(64, 64, heads=1)
    (lin2): Linear(in_features=64, out_features=64, bias=True)
    (lin3): Linear(in_features=64, out_features=2, bias=True)
  )
  (proj_head_g1): Project_Head(
    (block): Sequential(
      (0): Linear(in_features=64, out_features=64, bias=True)
      (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=64, bias=True)
      (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
      (5): ReLU()
    )
    (linear_shortcut): Linear(in_features=64, out_features=64, bias=True)
  )
  (proj_head_g2): Project_Head(
    (block): Sequential(
      (0): Linear(in_features=64, out_features=64, bias=True)
      (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=64, bias=True)
      (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
      (5): ReLU()
    )
    (linear_shortcut): Linear(in_features=64, out_features=64, bias=True)
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=64, bias=True)
    (1): ReLU()
    (2): Dropout(p=0.0, inplace=True)
    (3): Linear(in_features=64, out_features=2, bias=True)
  )
)
Total number of parameters: 1003972
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7798933da050>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.8248;  Loss pred: 0.7989; Loss self: 2.5888; time: 14.54s
Val loss: 0.7606 score: 0.4286 time: 5.68s
Test loss: 0.8023 score: 0.3673 time: 3.92s
Epoch 2/1000, LR 0.000015
Train loss: 0.8096;  Loss pred: 0.7838; Loss self: 2.5831; time: 26.95s
Val loss: 0.7311 score: 0.4694 time: 6.04s
Test loss: 0.7481 score: 0.4694 time: 4.38s
Epoch 3/1000, LR 0.000045
Train loss: 0.7435;  Loss pred: 0.7177; Loss self: 2.5730; time: 16.42s
Val loss: 0.6851 score: 0.4898 time: 6.57s
Test loss: 0.6746 score: 0.4898 time: 5.93s
Epoch 4/1000, LR 0.000075
Train loss: 0.6610;  Loss pred: 0.6355; Loss self: 2.5578; time: 8.06s
Val loss: 0.6373 score: 0.5714 time: 4.85s
Test loss: 0.5966 score: 0.5918 time: 4.62s
Epoch 5/1000, LR 0.000105
Train loss: 0.5876;  Loss pred: 0.5620; Loss self: 2.5595; time: 15.39s
Val loss: 0.6088 score: 0.6327 time: 5.29s
Test loss: 0.5396 score: 0.7551 time: 6.30s
Epoch 6/1000, LR 0.000135
Train loss: 0.5324;  Loss pred: 0.5064; Loss self: 2.6014; time: 15.05s
Val loss: 0.5887 score: 0.6531 time: 6.56s
Test loss: 0.4965 score: 0.8367 time: 4.78s
Epoch 7/1000, LR 0.000165
Train loss: 0.4882;  Loss pred: 0.4619; Loss self: 2.6274; time: 17.98s
Val loss: 0.5725 score: 0.6939 time: 3.66s
Test loss: 0.4599 score: 0.8776 time: 5.55s
Epoch 8/1000, LR 0.000195
Train loss: 0.4481;  Loss pred: 0.4218; Loss self: 2.6303; time: 8.51s
Val loss: 0.5514 score: 0.7551 time: 1.41s
Test loss: 0.4282 score: 0.8980 time: 6.45s
Epoch 9/1000, LR 0.000225
Train loss: 0.4090;  Loss pred: 0.3828; Loss self: 2.6179; time: 15.24s
Val loss: 0.5301 score: 0.8163 time: 5.86s
Test loss: 0.3938 score: 0.8980 time: 5.40s
Epoch 10/1000, LR 0.000255
Train loss: 0.3629;  Loss pred: 0.3370; Loss self: 2.5890; time: 12.75s
Val loss: 0.5183 score: 0.7959 time: 4.95s
Test loss: 0.3603 score: 0.8980 time: 5.92s
Epoch 11/1000, LR 0.000285
Train loss: 0.3246;  Loss pred: 0.2989; Loss self: 2.5690; time: 16.12s
Val loss: 0.5110 score: 0.7959 time: 5.70s
Test loss: 0.3364 score: 0.9184 time: 2.51s
Epoch 12/1000, LR 0.000285
Train loss: 0.2939;  Loss pred: 0.2683; Loss self: 2.5561; time: 17.52s
Val loss: 0.4996 score: 0.8163 time: 3.21s
Test loss: 0.3161 score: 0.9184 time: 4.99s
Epoch 13/1000, LR 0.000285
Train loss: 0.2678;  Loss pred: 0.2424; Loss self: 2.5468; time: 14.00s
Val loss: 0.4872 score: 0.8163 time: 5.71s
Test loss: 0.2983 score: 0.9388 time: 4.46s
Epoch 14/1000, LR 0.000285
Train loss: 0.2414;  Loss pred: 0.2159; Loss self: 2.5487; time: 14.95s
Val loss: 0.4808 score: 0.8163 time: 4.85s
Test loss: 0.2837 score: 0.9388 time: 3.10s
Epoch 15/1000, LR 0.000285
Train loss: 0.2165;  Loss pred: 0.1910; Loss self: 2.5482; time: 11.13s
Val loss: 0.4774 score: 0.8163 time: 6.10s
Test loss: 0.2700 score: 0.9388 time: 3.34s
Epoch 16/1000, LR 0.000285
Train loss: 0.1952;  Loss pred: 0.1698; Loss self: 2.5406; time: 13.91s
Val loss: 0.4717 score: 0.8163 time: 5.10s
Test loss: 0.2576 score: 0.9388 time: 5.09s
Epoch 17/1000, LR 0.000285
Train loss: 0.1746;  Loss pred: 0.1492; Loss self: 2.5345; time: 15.31s
Val loss: 0.4662 score: 0.8367 time: 3.83s
Test loss: 0.2470 score: 0.9388 time: 5.28s
Epoch 18/1000, LR 0.000285
Train loss: 0.1542;  Loss pred: 0.1289; Loss self: 2.5312; time: 15.86s
Val loss: 0.4628 score: 0.8367 time: 4.06s
Test loss: 0.2388 score: 0.9388 time: 9.53s
Epoch 19/1000, LR 0.000285
Train loss: 0.1348;  Loss pred: 0.1095; Loss self: 2.5342; time: 16.46s
Val loss: 0.4602 score: 0.8367 time: 3.38s
Test loss: 0.2311 score: 0.9388 time: 2.08s
Epoch 20/1000, LR 0.000285
Train loss: 0.1181;  Loss pred: 0.0927; Loss self: 2.5413; time: 13.84s
Val loss: 0.4616 score: 0.8163 time: 2.81s
Test loss: 0.2237 score: 0.9388 time: 3.81s
     INFO: Early stopping counter 1 of 20
Epoch 21/1000, LR 0.000285
Train loss: 0.1044;  Loss pred: 0.0789; Loss self: 2.5487; time: 14.09s
Val loss: 0.4669 score: 0.8163 time: 4.00s
Test loss: 0.2170 score: 0.9388 time: 5.46s
     INFO: Early stopping counter 2 of 20
Epoch 22/1000, LR 0.000285
Train loss: 0.0926;  Loss pred: 0.0670; Loss self: 2.5525; time: 18.76s
Val loss: 0.4740 score: 0.7959 time: 5.76s
Test loss: 0.2121 score: 0.9388 time: 2.66s
     INFO: Early stopping counter 3 of 20
Epoch 23/1000, LR 0.000285
Train loss: 0.0832;  Loss pred: 0.0577; Loss self: 2.5539; time: 12.67s
Val loss: 0.4807 score: 0.7959 time: 5.42s
Test loss: 0.2090 score: 0.9388 time: 5.12s
     INFO: Early stopping counter 4 of 20
Epoch 24/1000, LR 0.000285
Train loss: 0.0759;  Loss pred: 0.0503; Loss self: 2.5540; time: 6.32s
Val loss: 0.4855 score: 0.7959 time: 1.72s
Test loss: 0.2070 score: 0.9388 time: 1.26s
     INFO: Early stopping counter 5 of 20
Epoch 25/1000, LR 0.000285
Train loss: 0.0700;  Loss pred: 0.0445; Loss self: 2.5532; time: 8.70s
Val loss: 0.4898 score: 0.7959 time: 5.41s
Test loss: 0.2052 score: 0.9388 time: 4.38s
     INFO: Early stopping counter 6 of 20
Epoch 26/1000, LR 0.000285
Train loss: 0.0655;  Loss pred: 0.0399; Loss self: 2.5525; time: 12.02s
Val loss: 0.4937 score: 0.7959 time: 2.96s
Test loss: 0.2036 score: 0.9388 time: 3.63s
     INFO: Early stopping counter 7 of 20
Epoch 27/1000, LR 0.000285
Train loss: 0.0617;  Loss pred: 0.0361; Loss self: 2.5532; time: 11.06s
Val loss: 0.4988 score: 0.7959 time: 1.98s
Test loss: 0.2030 score: 0.9388 time: 3.15s
     INFO: Early stopping counter 8 of 20
Epoch 28/1000, LR 0.000285
Train loss: 0.0584;  Loss pred: 0.0328; Loss self: 2.5553; time: 16.66s
Val loss: 0.5037 score: 0.8163 time: 5.86s
Test loss: 0.2031 score: 0.9388 time: 5.61s
     INFO: Early stopping counter 9 of 20
Epoch 29/1000, LR 0.000285
Train loss: 0.0554;  Loss pred: 0.0298; Loss self: 2.5583; time: 15.61s
Val loss: 0.5093 score: 0.8163 time: 3.33s
Test loss: 0.2035 score: 0.9388 time: 3.41s
     INFO: Early stopping counter 10 of 20
Epoch 30/1000, LR 0.000285
Train loss: 0.0528;  Loss pred: 0.0272; Loss self: 2.5619; time: 11.04s
Val loss: 0.5153 score: 0.8163 time: 4.55s
Test loss: 0.2044 score: 0.9388 time: 2.52s
     INFO: Early stopping counter 11 of 20
Epoch 31/1000, LR 0.000285
Train loss: 0.0503;  Loss pred: 0.0247; Loss self: 2.5653; time: 11.80s
Val loss: 0.5212 score: 0.8163 time: 2.91s
Test loss: 0.2051 score: 0.9388 time: 5.34s
     INFO: Early stopping counter 12 of 20
Epoch 32/1000, LR 0.000285
Train loss: 0.0482;  Loss pred: 0.0225; Loss self: 2.5683; time: 14.69s
Val loss: 0.5271 score: 0.8163 time: 4.93s
Test loss: 0.2063 score: 0.9388 time: 5.01s
     INFO: Early stopping counter 13 of 20
Epoch 33/1000, LR 0.000285
Train loss: 0.0464;  Loss pred: 0.0207; Loss self: 2.5717; time: 15.28s
Val loss: 0.5325 score: 0.8163 time: 5.47s
Test loss: 0.2079 score: 0.9388 time: 5.60s
     INFO: Early stopping counter 14 of 20
Epoch 34/1000, LR 0.000285
Train loss: 0.0447;  Loss pred: 0.0190; Loss self: 2.5747; time: 14.21s
Val loss: 0.5374 score: 0.8163 time: 6.72s
Test loss: 0.2095 score: 0.9388 time: 1.91s
     INFO: Early stopping counter 15 of 20
Epoch 35/1000, LR 0.000285
Train loss: 0.0433;  Loss pred: 0.0176; Loss self: 2.5775; time: 15.69s
Val loss: 0.5431 score: 0.8163 time: 5.30s
Test loss: 0.2105 score: 0.9388 time: 4.52s
     INFO: Early stopping counter 16 of 20
Epoch 36/1000, LR 0.000285
Train loss: 0.0421;  Loss pred: 0.0163; Loss self: 2.5802; time: 11.66s
Val loss: 0.5498 score: 0.8163 time: 4.39s
Test loss: 0.2114 score: 0.9388 time: 4.91s
     INFO: Early stopping counter 17 of 20
Epoch 37/1000, LR 0.000285
Train loss: 0.0410;  Loss pred: 0.0152; Loss self: 2.5825; time: 11.35s
Val loss: 0.5572 score: 0.8163 time: 4.05s
Test loss: 0.2121 score: 0.9388 time: 2.64s
     INFO: Early stopping counter 18 of 20
Epoch 38/1000, LR 0.000284
Train loss: 0.0400;  Loss pred: 0.0142; Loss self: 2.5846; time: 5.77s
Val loss: 0.5649 score: 0.8163 time: 6.49s
Test loss: 0.2129 score: 0.9388 time: 9.21s
     INFO: Early stopping counter 19 of 20
Epoch 39/1000, LR 0.000284
Train loss: 0.0391;  Loss pred: 0.0132; Loss self: 2.5867; time: 21.47s
Val loss: 0.5726 score: 0.8163 time: 0.17s
Test loss: 0.2136 score: 0.9388 time: 1.70s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 018,   Train_Loss: 0.1348,   Val_Loss: 0.4602,   Val_Precision: 1.0000,   Val_Recall: 0.6800,   Val_accuracy: 0.8095,   Val_Score: 0.8367,   Val_Loss: 0.4602,   Test_Precision: 1.0000,   Test_Recall: 0.8750,   Test_accuracy: 0.9333,   Test_Score: 0.9388,   Test_loss: 0.2311


[0.7686839587986469, 6.768804287072271, 4.625396755989641, 6.389194326940924, 2.2933257948607206, 2.026728134136647, 5.704534021671861, 7.561505440156907, 0.16603926289826632, 7.161517888773233, 4.953778941184282, 4.92294054524973, 6.614439779892564, 5.501720665954053, 6.61980864033103, 4.735360140912235, 8.024891505017877, 6.286463771015406, 3.0429352140054107, 5.419238911010325, 7.165240897797048, 5.124797634314746, 7.696248745080084, 5.480486831162125, 7.000979672651738, 6.266845820005983, 5.948832578957081, 2.2653839280828834, 5.419196620117873, 2.9433139651082456, 3.7843113159760833, 6.33168278913945, 2.812547941226512, 5.583263126201928, 6.7728747758083045, 5.976028973702341, 5.278345126193017, 3.496696935966611, 5.422840062063187, 6.587074350100011, 4.515285735018551, 5.658104234840721, 5.744357842952013, 7.238298308104277, 4.338511097244918, 13.068057974800467, 3.4594231797382236, 6.53505743900314, 6.297936560120434, 4.281038506887853, 3.628647845238447, 6.785342067945749, 4.840944152791053, 6.107787818647921, 6.2909634951502085, 3.930565817747265, 4.386770122218877, 5.93918217997998, 4.629812286235392, 6.325169467832893, 4.801031916867942, 5.5613760757260025, 6.453297808300704, 5.406311254948378, 5.935424249153584, 2.5180371198803186, 5.0006762822158635, 4.462902819272131, 3.1233134791254997, 3.345817702356726, 5.097913122735918, 5.285440533887595, 9.542519476264715, 2.083888224326074, 3.8192799552343786, 5.471008453052491, 2.6624700520187616, 5.130091194063425, 1.2700188937596977, 4.383901739958674, 3.64640199765563, 3.1567002148367465, 5.619411299005151, 3.4203233988955617, 2.533208261243999, 5.345016750041395, 5.014206029009074, 5.60920617589727, 1.9206101931631565, 4.5268496056087315, 4.920673716347665, 2.647349943872541, 9.219692599959671, 1.7065937309525907]
[0.01568742773058463, 0.13813886300147493, 0.09439585216305389, 0.13039172095797805, 0.046802567242055525, 0.041361798655849935, 0.11641906166677268, 0.15431643755422259, 0.003388556385678904, 0.14615342630149455, 0.10109752941192413, 0.10046817439285163, 0.13498856693658293, 0.11228001359089905, 0.1350981355169598, 0.0966400028757599, 0.16377329602077298, 0.1282951790003144, 0.06210071865317165, 0.11059671246959847, 0.14622940607749077, 0.10458770682274991, 0.1570663009200017, 0.11184667002371683, 0.14287713617656608, 0.12789481265318334, 0.12140474650932818, 0.04623232506291599, 0.11059584939016068, 0.060067631940984606, 0.07723084318318538, 0.12921801610488673, 0.057398937576051266, 0.1139441454326924, 0.13822193420016948, 0.12195977497351718, 0.10772132910597994, 0.07136116195850227, 0.11067020534822831, 0.13443008877755125, 0.09214868846976636, 0.1154715149967494, 0.11723179271330639, 0.14772037363478116, 0.0885410428009167, 0.26669506071021365, 0.07060047305588212, 0.13336851916332937, 0.12852931755347824, 0.08736813279362965, 0.0740540376579275, 0.1384763687335867, 0.09879477862838884, 0.12464873099281472, 0.1283870101051063, 0.08021562893361765, 0.08952592086160974, 0.12120779959142816, 0.09448596502521209, 0.12908509118026312, 0.09798024320138657, 0.11349747093318373, 0.13169995527144293, 0.11033288275404853, 0.12113110712558335, 0.05138851265061875, 0.10205461800440538, 0.09107964937290063, 0.06374109141072448, 0.06828199392564747, 0.10403904332114118, 0.10786613334464479, 0.1947452954339738, 0.04252833110869539, 0.07794448888233425, 0.11165323373576512, 0.054336123510586976, 0.10469573865435561, 0.025918752933871383, 0.08946738244813619, 0.0744163672990945, 0.06442245336401524, 0.11468186324500308, 0.06980251834480738, 0.051698127780489773, 0.10908197449064072, 0.10233073528589946, 0.11447359542647491, 0.03919612639108483, 0.09238468582874962, 0.10042191257852377, 0.05402754987494982, 0.18815699183591167, 0.03482844348882838]
[63.74531358320607, 7.239092448511922, 10.593685814421784, 7.669198570684367, 21.366349303621682, 24.176898309487967, 8.589658649391188, 6.48019106615669, 295.1109222282125, 6.842124918352147, 9.891438552622565, 9.953400726580242, 7.408034789122517, 8.906304586349425, 7.402026653983416, 10.347681811284682, 6.106001553960055, 7.794525155131117, 16.102873230581, 9.041860084899778, 6.838569798129892, 9.561353149225756, 6.366738085398269, 8.940811557357517, 6.999020464437434, 7.818925406394188, 8.23691024241103, 21.629887716854697, 9.041930646711652, 16.647901168843855, 12.948194772755233, 7.738858946637103, 17.42192525210141, 8.776229758910317, 7.234741763574409, 8.199424771135762, 9.283212603292016, 14.013224736748501, 9.035855647447832, 7.438810827944585, 10.852026400007812, 8.660144452319264, 8.53010925496574, 6.7695469175590235, 11.294197226121293, 3.749600751273692, 14.164211041595626, 7.498021319224163, 7.7803260690614175, 11.445820896299546, 13.503652624847119, 7.221448750753206, 10.121992415828425, 8.022544570129995, 7.78894998163235, 12.46639854719021, 11.169949332839728, 8.250294150795888, 10.583582437171128, 7.746828009777927, 10.206139190169397, 8.81076901342324, 7.593017005502615, 9.063481122207026, 8.255517709115335, 19.459601930859918, 9.798674666116854, 10.97940107241492, 15.688466856589613, 14.645149365276357, 9.611776195531354, 9.270750410649134, 5.134912233805611, 23.513737170738377, 12.829643433926542, 8.956301277995827, 18.403962877571818, 9.551487126915623, 38.58210318032589, 11.177257818844707, 13.437904002768597, 15.522538304301445, 8.719774615656778, 14.326130685718878, 19.343060240904652, 9.167417482764765, 9.772235069025188, 8.735638959136988, 25.512725161215176, 10.82430481880586, 9.957986004479466, 18.50907550526654, 5.314710818039024, 28.712164536458868]
Elapsed: 5.016877877421955~1.9548313970750135
Time per graph: 0.10238526280452971~0.03989451830765334
Speed: 14.850506067710512~30.064371089388047
Total Time: 1.7073
best val loss: 0.4601507183848595 test_score: 0.9388

Testing...
Test loss: 0.2470 score: 0.9388 time: 4.14s
test Score 0.9388
Epoch Time List: [33.61070671770722, 24.52804783685133, 26.211539373267442, 26.357655082363635, 28.477681430988014, 8.985771173145622, 27.076829969882965, 32.134443745948374, 20.66188925690949, 23.883159703109413, 22.348556202836335, 22.141380277927965, 27.48112146742642, 31.016394657548517, 31.627181697636843, 25.89639938529581, 36.73797589773312, 34.44417666038498, 26.829468863084912, 28.451204562094063, 24.53798401216045, 22.212581276427954, 29.809701327700168, 29.370679318904877, 26.30203254101798, 30.19423669669777, 28.347532336600125, 22.633628678508103, 26.534385952632874, 19.480416053440422, 29.038026811089367, 39.01193031668663, 15.343819150701165, 27.62115154741332, 29.43502921750769, 23.49324120860547, 30.016454943921417, 22.425110215786844, 25.04016283713281, 26.43401210056618, 28.999981353525072, 22.71918402146548, 27.204176809173077, 31.712205359246582, 26.05744017055258, 44.26764624379575, 28.977288000285625, 30.456520432140678, 29.959311977028847, 26.717631313484162, 28.721850438509136, 27.06405829777941, 15.983577619772404, 31.534447425045073, 21.916031959932297, 24.140622126869857, 37.37013842817396, 28.926946903113276, 17.52514249505475, 26.984350421000272, 26.387223314028233, 27.197496933862567, 16.36136201582849, 26.491556694265455, 23.620193145237863, 24.329825547058135, 25.713251168839633, 24.166810526978225, 22.905182558111846, 20.569832331035286, 24.092956591863185, 24.41345040081069, 29.4549403837882, 21.914746299851686, 20.46896584611386, 23.546811633277684, 27.182954618241638, 23.20916822599247, 9.300977536011487, 18.484192164614797, 18.617387145292014, 16.19122934155166, 28.12873541424051, 22.350722735282034, 18.110538573469967, 20.04564623348415, 24.623924660496414, 26.350788968615234, 22.842373642604798, 25.509984774980694, 20.960897593759, 18.033986380323768, 21.473773838486522, 23.341452303808182]
Total Epoch List: [55, 39]
Total Time List: [6.292933396995068, 1.707293578889221]
========================k_idx:2========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
emb dim:  64
Model: Ethident
Ethident(
  (encoder): HGATE_encoder(
    (lin1): Linear(in_features=14887, out_features=64, bias=True)
    (account_convs): ModuleList(
      (0): GATEConv()
      (1): GATConv(64, 64, heads=1)
    )
    (subgraph_conv): GATConv(64, 64, heads=1)
    (lin2): Linear(in_features=64, out_features=64, bias=True)
    (lin3): Linear(in_features=64, out_features=2, bias=True)
  )
  (proj_head_g1): Project_Head(
    (block): Sequential(
      (0): Linear(in_features=64, out_features=64, bias=True)
      (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=64, bias=True)
      (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
      (5): ReLU()
    )
    (linear_shortcut): Linear(in_features=64, out_features=64, bias=True)
  )
  (proj_head_g2): Project_Head(
    (block): Sequential(
      (0): Linear(in_features=64, out_features=64, bias=True)
      (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=64, bias=True)
      (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
      (5): ReLU()
    )
    (linear_shortcut): Linear(in_features=64, out_features=64, bias=True)
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=64, bias=True)
    (1): ReLU()
    (2): Dropout(p=0.0, inplace=True)
    (3): Linear(in_features=64, out_features=2, bias=True)
  )
)
Total number of parameters: 1003972
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7798933dabf0>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.7318;  Loss pred: 0.7045; Loss self: 2.7341; time: 15.43s
Val loss: 0.6837 score: 0.5510 time: 4.06s
Test loss: 0.7007 score: 0.4792 time: 6.10s
Epoch 2/1000, LR 0.000015
Train loss: 0.7156;  Loss pred: 0.6884; Loss self: 2.7268; time: 9.80s
Val loss: 0.6501 score: 0.6531 time: 5.96s
Test loss: 0.6551 score: 0.6667 time: 6.09s
Epoch 3/1000, LR 0.000045
Train loss: 0.6490;  Loss pred: 0.6220; Loss self: 2.6980; time: 12.16s
Val loss: 0.6282 score: 0.7347 time: 2.61s
Test loss: 0.6113 score: 0.7708 time: 2.19s
Epoch 4/1000, LR 0.000075
Train loss: 0.5879;  Loss pred: 0.5614; Loss self: 2.6506; time: 16.42s
Val loss: 0.6146 score: 0.7347 time: 4.39s
Test loss: 0.5875 score: 0.8333 time: 4.43s
Epoch 5/1000, LR 0.000105
Train loss: 0.5528;  Loss pred: 0.5267; Loss self: 2.6154; time: 12.42s
Val loss: 0.5943 score: 0.7143 time: 5.87s
Test loss: 0.5605 score: 0.8125 time: 5.18s
Epoch 6/1000, LR 0.000135
Train loss: 0.5273;  Loss pred: 0.5013; Loss self: 2.5992; time: 12.20s
Val loss: 0.5635 score: 0.7551 time: 5.73s
Test loss: 0.5275 score: 0.8750 time: 5.75s
Epoch 7/1000, LR 0.000165
Train loss: 0.4881;  Loss pred: 0.4623; Loss self: 2.5814; time: 16.80s
Val loss: 0.5277 score: 0.7959 time: 6.04s
Test loss: 0.4908 score: 0.8958 time: 4.52s
Epoch 8/1000, LR 0.000195
Train loss: 0.4377;  Loss pred: 0.4121; Loss self: 2.5548; time: 7.55s
Val loss: 0.5098 score: 0.8163 time: 3.35s
Test loss: 0.4606 score: 0.8958 time: 3.34s
Epoch 9/1000, LR 0.000225
Train loss: 0.3919;  Loss pred: 0.3666; Loss self: 2.5293; time: 8.45s
Val loss: 0.5010 score: 0.8367 time: 5.60s
Test loss: 0.4291 score: 0.8958 time: 4.04s
Epoch 10/1000, LR 0.000255
Train loss: 0.3457;  Loss pred: 0.3205; Loss self: 2.5195; time: 14.80s
Val loss: 0.4845 score: 0.8367 time: 4.07s
Test loss: 0.3976 score: 0.8958 time: 2.91s
Epoch 11/1000, LR 0.000285
Train loss: 0.3071;  Loss pred: 0.2818; Loss self: 2.5319; time: 11.13s
Val loss: 0.4568 score: 0.8367 time: 3.56s
Test loss: 0.3668 score: 0.8750 time: 2.16s
Epoch 12/1000, LR 0.000285
Train loss: 0.2780;  Loss pred: 0.2525; Loss self: 2.5523; time: 14.92s
Val loss: 0.4306 score: 0.8367 time: 1.57s
Test loss: 0.3434 score: 0.8958 time: 4.45s
Epoch 13/1000, LR 0.000285
Train loss: 0.2530;  Loss pred: 0.2273; Loss self: 2.5697; time: 10.73s
Val loss: 0.4196 score: 0.8367 time: 3.70s
Test loss: 0.3258 score: 0.8958 time: 5.29s
Epoch 14/1000, LR 0.000285
Train loss: 0.2310;  Loss pred: 0.2052; Loss self: 2.5752; time: 23.89s
Val loss: 0.4184 score: 0.8367 time: 2.36s
Test loss: 0.3086 score: 0.8958 time: 2.41s
Epoch 15/1000, LR 0.000285
Train loss: 0.2098;  Loss pred: 0.1841; Loss self: 2.5700; time: 6.95s
Val loss: 0.4217 score: 0.8367 time: 2.29s
Test loss: 0.2966 score: 0.8958 time: 1.05s
     INFO: Early stopping counter 1 of 20
Epoch 16/1000, LR 0.000285
Train loss: 0.1932;  Loss pred: 0.1676; Loss self: 2.5607; time: 9.93s
Val loss: 0.4128 score: 0.8367 time: 1.72s
Test loss: 0.2883 score: 0.8958 time: 1.78s
Epoch 17/1000, LR 0.000285
Train loss: 0.1744;  Loss pred: 0.1489; Loss self: 2.5565; time: 13.54s
Val loss: 0.4100 score: 0.8367 time: 5.20s
Test loss: 0.2867 score: 0.8958 time: 4.67s
Epoch 18/1000, LR 0.000285
Train loss: 0.1577;  Loss pred: 0.1321; Loss self: 2.5553; time: 16.72s
Val loss: 0.4093 score: 0.8367 time: 4.78s
Test loss: 0.2851 score: 0.8958 time: 5.07s
Epoch 19/1000, LR 0.000285
Train loss: 0.1447;  Loss pred: 0.1192; Loss self: 2.5537; time: 10.93s
Val loss: 0.4072 score: 0.8367 time: 3.87s
Test loss: 0.2780 score: 0.8958 time: 6.16s
Epoch 20/1000, LR 0.000285
Train loss: 0.1338;  Loss pred: 0.1082; Loss self: 2.5546; time: 11.79s
Val loss: 0.4085 score: 0.8367 time: 5.20s
Test loss: 0.2694 score: 0.8958 time: 3.81s
     INFO: Early stopping counter 1 of 20
Epoch 21/1000, LR 0.000285
Train loss: 0.1234;  Loss pred: 0.0978; Loss self: 2.5540; time: 8.78s
Val loss: 0.4105 score: 0.8367 time: 2.65s
Test loss: 0.2621 score: 0.9167 time: 5.08s
     INFO: Early stopping counter 2 of 20
Epoch 22/1000, LR 0.000285
Train loss: 0.1140;  Loss pred: 0.0886; Loss self: 2.5416; time: 13.64s
Val loss: 0.4114 score: 0.8367 time: 6.10s
Test loss: 0.2572 score: 0.9167 time: 2.03s
     INFO: Early stopping counter 3 of 20
Epoch 23/1000, LR 0.000285
Train loss: 0.1058;  Loss pred: 0.0807; Loss self: 2.5185; time: 7.88s
Val loss: 0.4095 score: 0.8367 time: 4.99s
Test loss: 0.2542 score: 0.9167 time: 5.63s
     INFO: Early stopping counter 4 of 20
Epoch 24/1000, LR 0.000285
Train loss: 0.0978;  Loss pred: 0.0729; Loss self: 2.4925; time: 14.99s
Val loss: 0.4083 score: 0.8367 time: 6.43s
Test loss: 0.2518 score: 0.9167 time: 6.71s
     INFO: Early stopping counter 5 of 20
Epoch 25/1000, LR 0.000285
Train loss: 0.0904;  Loss pred: 0.0657; Loss self: 2.4671; time: 15.49s
Val loss: 0.4069 score: 0.8367 time: 4.04s
Test loss: 0.2481 score: 0.9167 time: 5.09s
Epoch 26/1000, LR 0.000285
Train loss: 0.0832;  Loss pred: 0.0588; Loss self: 2.4452; time: 15.20s
Val loss: 0.4055 score: 0.8367 time: 5.74s
Test loss: 0.2437 score: 0.9167 time: 0.78s
Epoch 27/1000, LR 0.000285
Train loss: 0.0766;  Loss pred: 0.0524; Loss self: 2.4277; time: 8.99s
Val loss: 0.4056 score: 0.8367 time: 3.16s
Test loss: 0.2392 score: 0.9167 time: 4.99s
     INFO: Early stopping counter 1 of 20
Epoch 28/1000, LR 0.000285
Train loss: 0.0713;  Loss pred: 0.0472; Loss self: 2.4131; time: 15.71s
Val loss: 0.4088 score: 0.8367 time: 3.95s
Test loss: 0.2359 score: 0.9167 time: 5.45s
     INFO: Early stopping counter 2 of 20
Epoch 29/1000, LR 0.000285
Train loss: 0.0664;  Loss pred: 0.0424; Loss self: 2.4018; time: 15.96s
Val loss: 0.4130 score: 0.8367 time: 0.09s
Test loss: 0.2332 score: 0.9167 time: 5.02s
     INFO: Early stopping counter 3 of 20
Epoch 30/1000, LR 0.000285
Train loss: 0.0620;  Loss pred: 0.0381; Loss self: 2.3953; time: 11.24s
Val loss: 0.4185 score: 0.8367 time: 5.55s
Test loss: 0.2308 score: 0.9167 time: 4.36s
     INFO: Early stopping counter 4 of 20
Epoch 31/1000, LR 0.000285
Train loss: 0.0583;  Loss pred: 0.0344; Loss self: 2.3937; time: 10.23s
Val loss: 0.4238 score: 0.8367 time: 4.41s
Test loss: 0.2291 score: 0.9167 time: 3.97s
     INFO: Early stopping counter 5 of 20
Epoch 32/1000, LR 0.000285
Train loss: 0.0550;  Loss pred: 0.0311; Loss self: 2.3951; time: 9.86s
Val loss: 0.4294 score: 0.8367 time: 0.97s
Test loss: 0.2276 score: 0.9167 time: 3.88s
     INFO: Early stopping counter 6 of 20
Epoch 33/1000, LR 0.000285
Train loss: 0.0523;  Loss pred: 0.0283; Loss self: 2.3973; time: 8.34s
Val loss: 0.4352 score: 0.8367 time: 5.95s
Test loss: 0.2269 score: 0.9167 time: 4.50s
     INFO: Early stopping counter 7 of 20
Epoch 34/1000, LR 0.000285
Train loss: 0.0499;  Loss pred: 0.0259; Loss self: 2.3981; time: 11.49s
Val loss: 0.4416 score: 0.8367 time: 9.83s
Test loss: 0.2264 score: 0.9375 time: 8.64s
     INFO: Early stopping counter 8 of 20
Epoch 35/1000, LR 0.000285
Train loss: 0.0478;  Loss pred: 0.0239; Loss self: 2.3977; time: 19.16s
Val loss: 0.4484 score: 0.8367 time: 4.92s
Test loss: 0.2262 score: 0.9375 time: 5.20s
     INFO: Early stopping counter 9 of 20
Epoch 36/1000, LR 0.000285
Train loss: 0.0460;  Loss pred: 0.0221; Loss self: 2.3966; time: 6.08s
Val loss: 0.4549 score: 0.8367 time: 3.44s
Test loss: 0.2263 score: 0.9375 time: 2.09s
     INFO: Early stopping counter 10 of 20
Epoch 37/1000, LR 0.000285
Train loss: 0.0445;  Loss pred: 0.0205; Loss self: 2.3953; time: 13.95s
Val loss: 0.4616 score: 0.8367 time: 4.00s
Test loss: 0.2267 score: 0.9375 time: 4.86s
     INFO: Early stopping counter 11 of 20
Epoch 38/1000, LR 0.000284
Train loss: 0.0431;  Loss pred: 0.0192; Loss self: 2.3944; time: 11.91s
Val loss: 0.4685 score: 0.8367 time: 2.31s
Test loss: 0.2276 score: 0.9375 time: 1.93s
     INFO: Early stopping counter 12 of 20
Epoch 39/1000, LR 0.000284
Train loss: 0.0419;  Loss pred: 0.0180; Loss self: 2.3942; time: 11.67s
Val loss: 0.4751 score: 0.8367 time: 4.06s
Test loss: 0.2287 score: 0.9375 time: 2.73s
     INFO: Early stopping counter 13 of 20
Epoch 40/1000, LR 0.000284
Train loss: 0.0409;  Loss pred: 0.0169; Loss self: 2.3944; time: 15.29s
Val loss: 0.4818 score: 0.8367 time: 1.73s
Test loss: 0.2300 score: 0.9375 time: 5.18s
     INFO: Early stopping counter 14 of 20
Epoch 41/1000, LR 0.000284
Train loss: 0.0399;  Loss pred: 0.0159; Loss self: 2.3950; time: 13.40s
Val loss: 0.4883 score: 0.8367 time: 4.38s
Test loss: 0.2310 score: 0.9375 time: 3.29s
     INFO: Early stopping counter 15 of 20
Epoch 42/1000, LR 0.000284
Train loss: 0.0390;  Loss pred: 0.0150; Loss self: 2.3954; time: 16.27s
Val loss: 0.4947 score: 0.8367 time: 4.74s
Test loss: 0.2320 score: 0.9375 time: 4.34s
     INFO: Early stopping counter 16 of 20
Epoch 43/1000, LR 0.000284
Train loss: 0.0382;  Loss pred: 0.0142; Loss self: 2.3953; time: 10.01s
Val loss: 0.5009 score: 0.8367 time: 4.36s
Test loss: 0.2328 score: 0.9375 time: 4.48s
     INFO: Early stopping counter 17 of 20
Epoch 44/1000, LR 0.000284
Train loss: 0.0374;  Loss pred: 0.0135; Loss self: 2.3942; time: 10.05s
Val loss: 0.5069 score: 0.8367 time: 4.70s
Test loss: 0.2335 score: 0.9375 time: 4.93s
     INFO: Early stopping counter 18 of 20
Epoch 45/1000, LR 0.000284
Train loss: 0.0368;  Loss pred: 0.0128; Loss self: 2.3921; time: 12.90s
Val loss: 0.5124 score: 0.8367 time: 0.27s
Test loss: 0.2340 score: 0.9375 time: 3.46s
     INFO: Early stopping counter 19 of 20
Epoch 46/1000, LR 0.000284
Train loss: 0.0361;  Loss pred: 0.0122; Loss self: 2.3895; time: 9.91s
Val loss: 0.5173 score: 0.8367 time: 5.79s
Test loss: 0.2342 score: 0.9375 time: 6.04s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 025,   Train_Loss: 0.0832,   Val_Loss: 0.4055,   Val_Precision: 0.9474,   Val_Recall: 0.7200,   Val_accuracy: 0.8182,   Val_Score: 0.8367,   Val_Loss: 0.4055,   Test_Precision: 1.0000,   Test_Recall: 0.8333,   Test_accuracy: 0.9091,   Test_Score: 0.9167,   Test_loss: 0.2437


[0.7686839587986469, 6.768804287072271, 4.625396755989641, 6.389194326940924, 2.2933257948607206, 2.026728134136647, 5.704534021671861, 7.561505440156907, 0.16603926289826632, 7.161517888773233, 4.953778941184282, 4.92294054524973, 6.614439779892564, 5.501720665954053, 6.61980864033103, 4.735360140912235, 8.024891505017877, 6.286463771015406, 3.0429352140054107, 5.419238911010325, 7.165240897797048, 5.124797634314746, 7.696248745080084, 5.480486831162125, 7.000979672651738, 6.266845820005983, 5.948832578957081, 2.2653839280828834, 5.419196620117873, 2.9433139651082456, 3.7843113159760833, 6.33168278913945, 2.812547941226512, 5.583263126201928, 6.7728747758083045, 5.976028973702341, 5.278345126193017, 3.496696935966611, 5.422840062063187, 6.587074350100011, 4.515285735018551, 5.658104234840721, 5.744357842952013, 7.238298308104277, 4.338511097244918, 13.068057974800467, 3.4594231797382236, 6.53505743900314, 6.297936560120434, 4.281038506887853, 3.628647845238447, 6.785342067945749, 4.840944152791053, 6.107787818647921, 6.2909634951502085, 3.930565817747265, 4.386770122218877, 5.93918217997998, 4.629812286235392, 6.325169467832893, 4.801031916867942, 5.5613760757260025, 6.453297808300704, 5.406311254948378, 5.935424249153584, 2.5180371198803186, 5.0006762822158635, 4.462902819272131, 3.1233134791254997, 3.345817702356726, 5.097913122735918, 5.285440533887595, 9.542519476264715, 2.083888224326074, 3.8192799552343786, 5.471008453052491, 2.6624700520187616, 5.130091194063425, 1.2700188937596977, 4.383901739958674, 3.64640199765563, 3.1567002148367465, 5.619411299005151, 3.4203233988955617, 2.533208261243999, 5.345016750041395, 5.014206029009074, 5.60920617589727, 1.9206101931631565, 4.5268496056087315, 4.920673716347665, 2.647349943872541, 9.219692599959671, 1.7065937309525907, 6.11830446170643, 6.1005784389562905, 2.1936933384276927, 4.439497873187065, 5.20435299212113, 5.760232829954475, 4.530209773220122, 3.345112682785839, 4.044918773230165, 2.9139879387803376, 2.1648574508726597, 4.459334700834006, 5.309552863240242, 2.4201901461929083, 1.0606498778797686, 1.784110720269382, 4.675502604804933, 5.073324767872691, 6.169812671840191, 3.813044155947864, 5.087582734879106, 2.036657772026956, 5.634468084666878, 6.7167156198993325, 5.101476292125881, 0.786365469917655, 5.00106015894562, 5.4619997455738485, 5.027944585308433, 4.363235560711473, 3.9768434446305037, 3.8964635408483446, 4.510752446018159, 8.653861045837402, 5.2075380012393, 2.1042469842359424, 4.867825617082417, 1.94031689921394, 2.741296709049493, 5.1899061212316155, 3.3057268718257546, 4.3408685200847685, 4.481184491422027, 4.945041756611317, 3.4749100119806826, 6.060200924053788]
[0.01568742773058463, 0.13813886300147493, 0.09439585216305389, 0.13039172095797805, 0.046802567242055525, 0.041361798655849935, 0.11641906166677268, 0.15431643755422259, 0.003388556385678904, 0.14615342630149455, 0.10109752941192413, 0.10046817439285163, 0.13498856693658293, 0.11228001359089905, 0.1350981355169598, 0.0966400028757599, 0.16377329602077298, 0.1282951790003144, 0.06210071865317165, 0.11059671246959847, 0.14622940607749077, 0.10458770682274991, 0.1570663009200017, 0.11184667002371683, 0.14287713617656608, 0.12789481265318334, 0.12140474650932818, 0.04623232506291599, 0.11059584939016068, 0.060067631940984606, 0.07723084318318538, 0.12921801610488673, 0.057398937576051266, 0.1139441454326924, 0.13822193420016948, 0.12195977497351718, 0.10772132910597994, 0.07136116195850227, 0.11067020534822831, 0.13443008877755125, 0.09214868846976636, 0.1154715149967494, 0.11723179271330639, 0.14772037363478116, 0.0885410428009167, 0.26669506071021365, 0.07060047305588212, 0.13336851916332937, 0.12852931755347824, 0.08736813279362965, 0.0740540376579275, 0.1384763687335867, 0.09879477862838884, 0.12464873099281472, 0.1283870101051063, 0.08021562893361765, 0.08952592086160974, 0.12120779959142816, 0.09448596502521209, 0.12908509118026312, 0.09798024320138657, 0.11349747093318373, 0.13169995527144293, 0.11033288275404853, 0.12113110712558335, 0.05138851265061875, 0.10205461800440538, 0.09107964937290063, 0.06374109141072448, 0.06828199392564747, 0.10403904332114118, 0.10786613334464479, 0.1947452954339738, 0.04252833110869539, 0.07794448888233425, 0.11165323373576512, 0.054336123510586976, 0.10469573865435561, 0.025918752933871383, 0.08946738244813619, 0.0744163672990945, 0.06442245336401524, 0.11468186324500308, 0.06980251834480738, 0.051698127780489773, 0.10908197449064072, 0.10233073528589946, 0.11447359542647491, 0.03919612639108483, 0.09238468582874962, 0.10042191257852377, 0.05402754987494982, 0.18815699183591167, 0.03482844348882838, 0.1274646762855506, 0.12709538414492272, 0.04570194455057693, 0.09248953902473052, 0.10842402066919021, 0.12000485062405157, 0.0943793702754192, 0.06968984755803831, 0.08426914110896178, 0.0607080820579237, 0.04510119689318041, 0.09290280626737513, 0.11061568465083838, 0.05042062804568559, 0.022096872455828514, 0.037168973338945456, 0.09740630426676944, 0.10569426599734773, 0.12853776399667063, 0.0794384199155805, 0.10599130697664805, 0.04243037025056159, 0.11738475176389329, 0.13993157541456944, 0.10628075608595584, 0.016382613956617813, 0.10418875331136708, 0.11379166136612184, 0.10474884552725901, 0.09090074084815569, 0.08285090509646882, 0.08117632376767385, 0.09397400929204498, 0.1802887717882792, 0.10849037502581875, 0.0438384788382488, 0.10141303368921702, 0.04042326873362375, 0.05711034810519777, 0.10812304419232532, 0.06886930982970323, 0.09043476083509934, 0.09335801023795891, 0.10302170326273578, 0.07239395858293089, 0.12625418591778725]
[63.74531358320607, 7.239092448511922, 10.593685814421784, 7.669198570684367, 21.366349303621682, 24.176898309487967, 8.589658649391188, 6.48019106615669, 295.1109222282125, 6.842124918352147, 9.891438552622565, 9.953400726580242, 7.408034789122517, 8.906304586349425, 7.402026653983416, 10.347681811284682, 6.106001553960055, 7.794525155131117, 16.102873230581, 9.041860084899778, 6.838569798129892, 9.561353149225756, 6.366738085398269, 8.940811557357517, 6.999020464437434, 7.818925406394188, 8.23691024241103, 21.629887716854697, 9.041930646711652, 16.647901168843855, 12.948194772755233, 7.738858946637103, 17.42192525210141, 8.776229758910317, 7.234741763574409, 8.199424771135762, 9.283212603292016, 14.013224736748501, 9.035855647447832, 7.438810827944585, 10.852026400007812, 8.660144452319264, 8.53010925496574, 6.7695469175590235, 11.294197226121293, 3.749600751273692, 14.164211041595626, 7.498021319224163, 7.7803260690614175, 11.445820896299546, 13.503652624847119, 7.221448750753206, 10.121992415828425, 8.022544570129995, 7.78894998163235, 12.46639854719021, 11.169949332839728, 8.250294150795888, 10.583582437171128, 7.746828009777927, 10.206139190169397, 8.81076901342324, 7.593017005502615, 9.063481122207026, 8.255517709115335, 19.459601930859918, 9.798674666116854, 10.97940107241492, 15.688466856589613, 14.645149365276357, 9.611776195531354, 9.270750410649134, 5.134912233805611, 23.513737170738377, 12.829643433926542, 8.956301277995827, 18.403962877571818, 9.551487126915623, 38.58210318032589, 11.177257818844707, 13.437904002768597, 15.522538304301445, 8.719774615656778, 14.326130685718878, 19.343060240904652, 9.167417482764765, 9.772235069025188, 8.735638959136988, 25.512725161215176, 10.82430481880586, 9.957986004479466, 18.50907550526654, 5.314710818039024, 28.712164536458868, 7.845310788376904, 7.8681063575033745, 21.88090703434579, 10.812033561250779, 9.22304848896053, 8.332996498056374, 10.595535836717133, 14.349292401123297, 11.866740147582362, 16.472271336885015, 22.172360577668094, 10.763937497452885, 9.040309275818606, 19.833152397346396, 45.25527320660391, 26.90415984538925, 10.2662759615771, 9.461251190533778, 7.779814810112161, 12.58836720396382, 9.434736003588666, 23.568024367799726, 8.518994034347763, 7.146349900209025, 9.409041079753308, 61.04032010081313, 9.597964926324721, 8.787990156699838, 9.546644595140362, 11.001010450183689, 12.069874177423088, 12.31886286033344, 10.641240142178878, 5.546657121688879, 9.217407532806648, 22.81101047528835, 9.860665474859248, 24.73822704912055, 17.50996155999594, 9.24872220783237, 14.520255865388412, 11.057694970006292, 10.711453655140188, 9.706692554380552, 13.813307347386594, 7.920529467840128]
Elapsed: 4.772016278208633~1.8739439316241753
Time per graph: 0.09798483098892602~0.03828266526885697
Speed: 14.63573109184704~25.28203530425281
Total Time: 6.0626
best val loss: 0.4055245895774997 test_score: 0.9167

Testing...
Test loss: 0.4291 score: 0.8958 time: 6.45s
test Score 0.8958
Epoch Time List: [33.61070671770722, 24.52804783685133, 26.211539373267442, 26.357655082363635, 28.477681430988014, 8.985771173145622, 27.076829969882965, 32.134443745948374, 20.66188925690949, 23.883159703109413, 22.348556202836335, 22.141380277927965, 27.48112146742642, 31.016394657548517, 31.627181697636843, 25.89639938529581, 36.73797589773312, 34.44417666038498, 26.829468863084912, 28.451204562094063, 24.53798401216045, 22.212581276427954, 29.809701327700168, 29.370679318904877, 26.30203254101798, 30.19423669669777, 28.347532336600125, 22.633628678508103, 26.534385952632874, 19.480416053440422, 29.038026811089367, 39.01193031668663, 15.343819150701165, 27.62115154741332, 29.43502921750769, 23.49324120860547, 30.016454943921417, 22.425110215786844, 25.04016283713281, 26.43401210056618, 28.999981353525072, 22.71918402146548, 27.204176809173077, 31.712205359246582, 26.05744017055258, 44.26764624379575, 28.977288000285625, 30.456520432140678, 29.959311977028847, 26.717631313484162, 28.721850438509136, 27.06405829777941, 15.983577619772404, 31.534447425045073, 21.916031959932297, 24.140622126869857, 37.37013842817396, 28.926946903113276, 17.52514249505475, 26.984350421000272, 26.387223314028233, 27.197496933862567, 16.36136201582849, 26.491556694265455, 23.620193145237863, 24.329825547058135, 25.713251168839633, 24.166810526978225, 22.905182558111846, 20.569832331035286, 24.092956591863185, 24.41345040081069, 29.4549403837882, 21.914746299851686, 20.46896584611386, 23.546811633277684, 27.182954618241638, 23.20916822599247, 9.300977536011487, 18.484192164614797, 18.617387145292014, 16.19122934155166, 28.12873541424051, 22.350722735282034, 18.110538573469967, 20.04564623348415, 24.623924660496414, 26.350788968615234, 22.842373642604798, 25.509984774980694, 20.960897593759, 18.033986380323768, 21.473773838486522, 23.341452303808182, 25.593265464995056, 21.849231048021466, 16.956903754733503, 25.241477360017598, 23.469494611024857, 23.680112735833973, 27.366270878817886, 14.240203798282892, 18.089248643256724, 21.778451999183744, 16.849130651913583, 20.938280711881816, 19.72094874829054, 28.659831109456718, 10.29905613232404, 13.431788574904203, 23.413471221458167, 26.565615826752037, 20.965120463166386, 20.800265789031982, 16.49894985370338, 21.776345847640187, 18.49557722778991, 28.136481274850667, 24.615642834920436, 21.722069477196783, 17.142183725722134, 25.11005773115903, 21.062945194076747, 21.15193570079282, 18.608084693085402, 14.711657723411918, 18.797856859862804, 29.971995518077165, 29.282077237498015, 11.605929808691144, 22.81244587432593, 16.152992489747703, 18.46877252869308, 22.201557792257518, 21.074189107865095, 25.34569090977311, 18.843224989250302, 19.689682037569582, 16.63290127273649, 21.74176546232775]
Total Epoch List: [55, 39, 46]
Total Time List: [6.292933396995068, 1.707293578889221, 6.062569114845246]
========================training times:1========================
========================k_idx:0========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
emb dim:  64
Model: Ethident
Ethident(
  (encoder): HGATE_encoder(
    (lin1): Linear(in_features=14887, out_features=64, bias=True)
    (account_convs): ModuleList(
      (0): GATEConv()
      (1): GATConv(64, 64, heads=1)
    )
    (subgraph_conv): GATConv(64, 64, heads=1)
    (lin2): Linear(in_features=64, out_features=64, bias=True)
    (lin3): Linear(in_features=64, out_features=2, bias=True)
  )
  (proj_head_g1): Project_Head(
    (block): Sequential(
      (0): Linear(in_features=64, out_features=64, bias=True)
      (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=64, bias=True)
      (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
      (5): ReLU()
    )
    (linear_shortcut): Linear(in_features=64, out_features=64, bias=True)
  )
  (proj_head_g2): Project_Head(
    (block): Sequential(
      (0): Linear(in_features=64, out_features=64, bias=True)
      (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=64, bias=True)
      (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
      (5): ReLU()
    )
    (linear_shortcut): Linear(in_features=64, out_features=64, bias=True)
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=64, bias=True)
    (1): ReLU()
    (2): Dropout(p=0.0, inplace=True)
    (3): Linear(in_features=64, out_features=2, bias=True)
  )
)
Total number of parameters: 1003972
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7798933d8af0>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.7690;  Loss pred: 0.7416; Loss self: 2.7327; time: 12.40s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7262 score: 0.5102 time: 6.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7993 score: 0.4898 time: 3.80s
Epoch 2/1000, LR 0.000015
Train loss: 0.7546;  Loss pred: 0.7275; Loss self: 2.7062; time: 14.99s
Val loss: 0.6144 score: 0.6327 time: 5.12s
Test loss: 0.6741 score: 0.5102 time: 5.28s
Epoch 3/1000, LR 0.000045
Train loss: 0.6486;  Loss pred: 0.6225; Loss self: 2.6040; time: 11.42s
Val loss: 0.4910 score: 0.8163 time: 8.03s
Test loss: 0.5266 score: 0.7959 time: 7.60s
Epoch 4/1000, LR 0.000075
Train loss: 0.5389;  Loss pred: 0.5144; Loss self: 2.4528; time: 8.98s
Val loss: 0.4482 score: 0.8367 time: 1.99s
Test loss: 0.4882 score: 0.7959 time: 6.21s
Epoch 5/1000, LR 0.000105
Train loss: 0.4703;  Loss pred: 0.4464; Loss self: 2.3936; time: 11.91s
Val loss: 0.4355 score: 0.8367 time: 1.00s
Test loss: 0.4880 score: 0.8163 time: 3.67s
Epoch 6/1000, LR 0.000135
Train loss: 0.4222;  Loss pred: 0.3981; Loss self: 2.4079; time: 17.05s
Val loss: 0.4683 score: 0.8571 time: 3.30s
Test loss: 0.5480 score: 0.7959 time: 3.51s
     INFO: Early stopping counter 1 of 20
Epoch 7/1000, LR 0.000165
Train loss: 0.4372;  Loss pred: 0.4127; Loss self: 2.4461; time: 7.45s
Val loss: 0.4935 score: 0.8163 time: 5.45s
Test loss: 0.5945 score: 0.7143 time: 4.65s
     INFO: Early stopping counter 2 of 20
Epoch 8/1000, LR 0.000195
Train loss: 0.4559;  Loss pred: 0.4314; Loss self: 2.4496; time: 17.63s
Val loss: 0.4088 score: 0.8776 time: 5.06s
Test loss: 0.5047 score: 0.8163 time: 4.86s
Epoch 9/1000, LR 0.000225
Train loss: 0.3851;  Loss pred: 0.3612; Loss self: 2.3895; time: 15.24s
Val loss: 0.3518 score: 0.8571 time: 1.38s
Test loss: 0.4333 score: 0.8571 time: 2.82s
Epoch 10/1000, LR 0.000255
Train loss: 0.3264;  Loss pred: 0.3026; Loss self: 2.3841; time: 1.47s
Val loss: 0.3248 score: 0.8571 time: 6.61s
Test loss: 0.4147 score: 0.8367 time: 2.98s
Epoch 11/1000, LR 0.000285
Train loss: 0.2784;  Loss pred: 0.2542; Loss self: 2.4159; time: 16.29s
Val loss: 0.3245 score: 0.8776 time: 4.43s
Test loss: 0.4388 score: 0.8367 time: 5.19s
Epoch 12/1000, LR 0.000285
Train loss: 0.2669;  Loss pred: 0.2427; Loss self: 2.4185; time: 15.74s
Val loss: 0.3184 score: 0.8776 time: 5.18s
Test loss: 0.4450 score: 0.8367 time: 5.62s
Epoch 13/1000, LR 0.000285
Train loss: 0.2534;  Loss pred: 0.2294; Loss self: 2.3993; time: 10.17s
Val loss: 0.2819 score: 0.8980 time: 2.20s
Test loss: 0.4106 score: 0.8367 time: 5.36s
Epoch 14/1000, LR 0.000285
Train loss: 0.2260;  Loss pred: 0.2022; Loss self: 2.3839; time: 15.09s
Val loss: 0.2583 score: 0.8980 time: 5.37s
Test loss: 0.3860 score: 0.8571 time: 4.14s
Epoch 15/1000, LR 0.000285
Train loss: 0.2078;  Loss pred: 0.1839; Loss self: 2.3909; time: 14.19s
Val loss: 0.2474 score: 0.8980 time: 3.90s
Test loss: 0.3810 score: 0.8571 time: 5.16s
Epoch 16/1000, LR 0.000285
Train loss: 0.1944;  Loss pred: 0.1703; Loss self: 2.4100; time: 6.62s
Val loss: 0.2438 score: 0.8980 time: 2.61s
Test loss: 0.3844 score: 0.8571 time: 4.06s
Epoch 17/1000, LR 0.000285
Train loss: 0.1845;  Loss pred: 0.1602; Loss self: 2.4312; time: 15.28s
Val loss: 0.2428 score: 0.8980 time: 3.68s
Test loss: 0.3928 score: 0.8571 time: 2.97s
Epoch 18/1000, LR 0.000285
Train loss: 0.1773;  Loss pred: 0.1529; Loss self: 2.4443; time: 14.67s
Val loss: 0.2409 score: 0.8980 time: 1.49s
Test loss: 0.3994 score: 0.8571 time: 5.95s
Epoch 19/1000, LR 0.000285
Train loss: 0.1704;  Loss pred: 0.1459; Loss self: 2.4510; time: 14.43s
Val loss: 0.2337 score: 0.8980 time: 4.63s
Test loss: 0.3989 score: 0.8571 time: 0.83s
Epoch 20/1000, LR 0.000285
Train loss: 0.1627;  Loss pred: 0.1381; Loss self: 2.4567; time: 11.94s
Val loss: 0.2239 score: 0.8980 time: 0.11s
Test loss: 0.3931 score: 0.8571 time: 0.12s
Epoch 21/1000, LR 0.000285
Train loss: 0.1546;  Loss pred: 0.1300; Loss self: 2.4619; time: 13.96s
Val loss: 0.2141 score: 0.8980 time: 4.42s
Test loss: 0.3882 score: 0.8571 time: 4.77s
Epoch 22/1000, LR 0.000285
Train loss: 0.1464;  Loss pred: 0.1218; Loss self: 2.4658; time: 10.93s
Val loss: 0.2059 score: 0.8980 time: 2.80s
Test loss: 0.3855 score: 0.8367 time: 4.82s
Epoch 23/1000, LR 0.000285
Train loss: 0.1392;  Loss pred: 0.1145; Loss self: 2.4687; time: 11.68s
Val loss: 0.2001 score: 0.9184 time: 6.10s
Test loss: 0.3861 score: 0.8367 time: 5.60s
Epoch 24/1000, LR 0.000285
Train loss: 0.1324;  Loss pred: 0.1077; Loss self: 2.4706; time: 22.58s
Val loss: 0.1956 score: 0.9184 time: 4.81s
Test loss: 0.3877 score: 0.8367 time: 5.56s
Epoch 25/1000, LR 0.000285
Train loss: 0.1263;  Loss pred: 0.1016; Loss self: 2.4715; time: 12.16s
Val loss: 0.1923 score: 0.9184 time: 4.36s
Test loss: 0.3907 score: 0.8163 time: 5.33s
Epoch 26/1000, LR 0.000285
Train loss: 0.1202;  Loss pred: 0.0955; Loss self: 2.4733; time: 8.66s
Val loss: 0.1892 score: 0.9184 time: 4.06s
Test loss: 0.3939 score: 0.8163 time: 4.99s
Epoch 27/1000, LR 0.000285
Train loss: 0.1144;  Loss pred: 0.0896; Loss self: 2.4738; time: 12.59s
Val loss: 0.1863 score: 0.9184 time: 4.13s
Test loss: 0.3984 score: 0.8163 time: 4.50s
Epoch 28/1000, LR 0.000285
Train loss: 0.1087;  Loss pred: 0.0839; Loss self: 2.4726; time: 9.84s
Val loss: 0.1843 score: 0.9184 time: 4.54s
Test loss: 0.4033 score: 0.8163 time: 6.34s
Epoch 29/1000, LR 0.000285
Train loss: 0.1031;  Loss pred: 0.0784; Loss self: 2.4698; time: 9.41s
Val loss: 0.1819 score: 0.9184 time: 4.49s
Test loss: 0.4083 score: 0.8163 time: 4.83s
Epoch 30/1000, LR 0.000285
Train loss: 0.0974;  Loss pred: 0.0727; Loss self: 2.4662; time: 14.30s
Val loss: 0.1793 score: 0.9184 time: 5.27s
Test loss: 0.4136 score: 0.8163 time: 4.71s
Epoch 31/1000, LR 0.000285
Train loss: 0.0920;  Loss pred: 0.0674; Loss self: 2.4615; time: 12.80s
Val loss: 0.1769 score: 0.9184 time: 3.09s
Test loss: 0.4192 score: 0.8163 time: 3.42s
Epoch 32/1000, LR 0.000285
Train loss: 0.0866;  Loss pred: 0.0621; Loss self: 2.4563; time: 7.15s
Val loss: 0.1745 score: 0.9184 time: 2.19s
Test loss: 0.4254 score: 0.8163 time: 4.84s
Epoch 33/1000, LR 0.000285
Train loss: 0.0812;  Loss pred: 0.0567; Loss self: 2.4516; time: 9.80s
Val loss: 0.1724 score: 0.9184 time: 2.75s
Test loss: 0.4307 score: 0.8163 time: 3.29s
Epoch 34/1000, LR 0.000285
Train loss: 0.0760;  Loss pred: 0.0515; Loss self: 2.4463; time: 11.46s
Val loss: 0.1713 score: 0.9184 time: 2.96s
Test loss: 0.4336 score: 0.8163 time: 4.51s
Epoch 35/1000, LR 0.000285
Train loss: 0.0714;  Loss pred: 0.0470; Loss self: 2.4411; time: 15.62s
Val loss: 0.1709 score: 0.9184 time: 6.44s
Test loss: 0.4367 score: 0.8163 time: 3.39s
Epoch 36/1000, LR 0.000285
Train loss: 0.0673;  Loss pred: 0.0430; Loss self: 2.4368; time: 13.59s
Val loss: 0.1708 score: 0.9184 time: 4.73s
Test loss: 0.4404 score: 0.8163 time: 6.18s
Epoch 37/1000, LR 0.000285
Train loss: 0.0636;  Loss pred: 0.0393; Loss self: 2.4339; time: 13.07s
Val loss: 0.1696 score: 0.9184 time: 2.42s
Test loss: 0.4442 score: 0.8163 time: 1.98s
Epoch 38/1000, LR 0.000284
Train loss: 0.0597;  Loss pred: 0.0354; Loss self: 2.4316; time: 13.68s
Val loss: 0.1671 score: 0.9184 time: 4.23s
Test loss: 0.4503 score: 0.8163 time: 4.74s
Epoch 39/1000, LR 0.000284
Train loss: 0.0562;  Loss pred: 0.0319; Loss self: 2.4303; time: 9.69s
Val loss: 0.1655 score: 0.9388 time: 3.71s
Test loss: 0.4569 score: 0.8163 time: 2.53s
Epoch 40/1000, LR 0.000284
Train loss: 0.0530;  Loss pred: 0.0287; Loss self: 2.4301; time: 14.47s
Val loss: 0.1641 score: 0.9388 time: 4.21s
Test loss: 0.4639 score: 0.8367 time: 3.49s
Epoch 41/1000, LR 0.000284
Train loss: 0.0502;  Loss pred: 0.0259; Loss self: 2.4299; time: 13.67s
Val loss: 0.1622 score: 0.9388 time: 3.60s
Test loss: 0.4713 score: 0.8367 time: 4.63s
Epoch 42/1000, LR 0.000284
Train loss: 0.0475;  Loss pred: 0.0232; Loss self: 2.4292; time: 9.44s
Val loss: 0.1600 score: 0.9592 time: 3.38s
Test loss: 0.4787 score: 0.8367 time: 5.30s
Epoch 43/1000, LR 0.000284
Train loss: 0.0451;  Loss pred: 0.0208; Loss self: 2.4279; time: 11.08s
Val loss: 0.1572 score: 0.9592 time: 1.01s
Test loss: 0.4863 score: 0.8367 time: 3.86s
Epoch 44/1000, LR 0.000284
Train loss: 0.0430;  Loss pred: 0.0188; Loss self: 2.4254; time: 11.70s
Val loss: 0.1545 score: 0.9592 time: 4.18s
Test loss: 0.4932 score: 0.8367 time: 3.36s
Epoch 45/1000, LR 0.000284
Train loss: 0.0413;  Loss pred: 0.0171; Loss self: 2.4224; time: 17.91s
Val loss: 0.1532 score: 0.9592 time: 5.80s
Test loss: 0.4999 score: 0.8367 time: 6.54s
Epoch 46/1000, LR 0.000284
Train loss: 0.0399;  Loss pred: 0.0157; Loss self: 2.4199; time: 14.29s
Val loss: 0.1525 score: 0.9592 time: 2.86s
Test loss: 0.5064 score: 0.8163 time: 3.95s
Epoch 47/1000, LR 0.000284
Train loss: 0.0386;  Loss pred: 0.0145; Loss self: 2.4175; time: 2.91s
Val loss: 0.1527 score: 0.9592 time: 5.07s
Test loss: 0.5127 score: 0.8163 time: 4.05s
     INFO: Early stopping counter 1 of 20
Epoch 48/1000, LR 0.000284
Train loss: 0.0375;  Loss pred: 0.0134; Loss self: 2.4149; time: 14.25s
Val loss: 0.1533 score: 0.9592 time: 3.16s
Test loss: 0.5180 score: 0.8163 time: 3.51s
     INFO: Early stopping counter 2 of 20
Epoch 49/1000, LR 0.000284
Train loss: 0.0366;  Loss pred: 0.0124; Loss self: 2.4120; time: 13.81s
Val loss: 0.1541 score: 0.9592 time: 4.37s
Test loss: 0.5231 score: 0.8163 time: 1.00s
     INFO: Early stopping counter 3 of 20
Epoch 50/1000, LR 0.000284
Train loss: 0.0357;  Loss pred: 0.0116; Loss self: 2.4089; time: 7.70s
Val loss: 0.1550 score: 0.9592 time: 5.71s
Test loss: 0.5279 score: 0.8163 time: 1.84s
     INFO: Early stopping counter 4 of 20
Epoch 51/1000, LR 0.000284
Train loss: 0.0349;  Loss pred: 0.0109; Loss self: 2.4056; time: 7.74s
Val loss: 0.1556 score: 0.9592 time: 3.93s
Test loss: 0.5328 score: 0.8163 time: 5.20s
     INFO: Early stopping counter 5 of 20
Epoch 52/1000, LR 0.000284
Train loss: 0.0343;  Loss pred: 0.0102; Loss self: 2.4022; time: 13.00s
Val loss: 0.1560 score: 0.9592 time: 2.34s
Test loss: 0.5377 score: 0.8163 time: 6.32s
     INFO: Early stopping counter 6 of 20
Epoch 53/1000, LR 0.000284
Train loss: 0.0337;  Loss pred: 0.0097; Loss self: 2.3983; time: 9.46s
Val loss: 0.1563 score: 0.9592 time: 6.09s
Test loss: 0.5432 score: 0.8163 time: 5.04s
     INFO: Early stopping counter 7 of 20
Epoch 54/1000, LR 0.000284
Train loss: 0.0331;  Loss pred: 0.0092; Loss self: 2.3939; time: 17.86s
Val loss: 0.1566 score: 0.9592 time: 3.68s
Test loss: 0.5483 score: 0.8163 time: 3.35s
     INFO: Early stopping counter 8 of 20
Epoch 55/1000, LR 0.000284
Train loss: 0.0327;  Loss pred: 0.0088; Loss self: 2.3894; time: 9.06s
Val loss: 0.1567 score: 0.9592 time: 3.23s
Test loss: 0.5536 score: 0.8163 time: 3.58s
     INFO: Early stopping counter 9 of 20
Epoch 56/1000, LR 0.000284
Train loss: 0.0322;  Loss pred: 0.0084; Loss self: 2.3847; time: 10.51s
Val loss: 0.1569 score: 0.9592 time: 3.95s
Test loss: 0.5587 score: 0.8163 time: 2.73s
     INFO: Early stopping counter 10 of 20
Epoch 57/1000, LR 0.000283
Train loss: 0.0318;  Loss pred: 0.0080; Loss self: 2.3800; time: 10.30s
Val loss: 0.1569 score: 0.9592 time: 4.76s
Test loss: 0.5634 score: 0.8163 time: 4.33s
     INFO: Early stopping counter 11 of 20
Epoch 58/1000, LR 0.000283
Train loss: 0.0314;  Loss pred: 0.0077; Loss self: 2.3749; time: 10.54s
Val loss: 0.1568 score: 0.9592 time: 5.09s
Test loss: 0.5684 score: 0.8163 time: 3.61s
     INFO: Early stopping counter 12 of 20
Epoch 59/1000, LR 0.000283
Train loss: 0.0311;  Loss pred: 0.0074; Loss self: 2.3694; time: 8.10s
Val loss: 0.1565 score: 0.9592 time: 6.17s
Test loss: 0.5729 score: 0.7959 time: 5.64s
     INFO: Early stopping counter 13 of 20
Epoch 60/1000, LR 0.000283
Train loss: 0.0307;  Loss pred: 0.0071; Loss self: 2.3639; time: 13.06s
Val loss: 0.1562 score: 0.9592 time: 6.40s
Test loss: 0.5773 score: 0.7959 time: 4.87s
     INFO: Early stopping counter 14 of 20
Epoch 61/1000, LR 0.000283
Train loss: 0.0304;  Loss pred: 0.0068; Loss self: 2.3580; time: 15.04s
Val loss: 0.1560 score: 0.9592 time: 5.79s
Test loss: 0.5813 score: 0.7959 time: 3.46s
     INFO: Early stopping counter 15 of 20
Epoch 62/1000, LR 0.000283
Train loss: 0.0301;  Loss pred: 0.0066; Loss self: 2.3519; time: 9.83s
Val loss: 0.1561 score: 0.9592 time: 3.83s
Test loss: 0.5851 score: 0.7959 time: 5.08s
     INFO: Early stopping counter 16 of 20
Epoch 63/1000, LR 0.000283
Train loss: 0.0298;  Loss pred: 0.0063; Loss self: 2.3457; time: 9.12s
Val loss: 0.1560 score: 0.9592 time: 3.13s
Test loss: 0.5890 score: 0.7959 time: 1.39s
     INFO: Early stopping counter 17 of 20
Epoch 64/1000, LR 0.000283
Train loss: 0.0295;  Loss pred: 0.0061; Loss self: 2.3398; time: 11.88s
Val loss: 0.1558 score: 0.9592 time: 3.66s
Test loss: 0.5930 score: 0.7959 time: 4.85s
     INFO: Early stopping counter 18 of 20
Epoch 65/1000, LR 0.000283
Train loss: 0.0292;  Loss pred: 0.0059; Loss self: 2.3335; time: 11.85s
Val loss: 0.1555 score: 0.9592 time: 3.77s
Test loss: 0.5970 score: 0.7959 time: 1.54s
     INFO: Early stopping counter 19 of 20
Epoch 66/1000, LR 0.000283
Train loss: 0.0290;  Loss pred: 0.0057; Loss self: 2.3269; time: 3.23s
Val loss: 0.1554 score: 0.9592 time: 6.78s
Test loss: 0.6008 score: 0.7959 time: 6.06s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 045,   Train_Loss: 0.0399,   Val_Loss: 0.1525,   Val_Precision: 0.9583,   Val_Recall: 0.9583,   Val_accuracy: 0.9583,   Val_Score: 0.9592,   Val_Loss: 0.1525,   Test_Precision: 0.8333,   Test_Recall: 0.8000,   Test_accuracy: 0.8163,   Test_Score: 0.8163,   Test_loss: 0.5064


[3.810776298865676, 5.287704010959715, 7.60088209901005, 6.2141397530213, 3.677017994225025, 3.5134783750399947, 4.671118552796543, 4.87044710572809, 2.8264260198920965, 2.9869844932109118, 5.19137757178396, 5.621837673243135, 5.3641422488726676, 4.14299793401733, 5.170906960964203, 4.07267143484205, 2.9749292000196874, 5.9552081823349, 0.8353281887248158, 0.12464717496186495, 4.779290863778442, 4.831672898028046, 5.608116687275469, 5.567609582096338, 5.335563581902534, 5.000396589748561, 4.506302285939455, 6.34899910306558, 4.8401919337920845, 4.724507647100836, 3.4220323814079165, 4.844601792283356, 3.3054168890230358, 4.520521834027022, 3.3932007420808077, 6.183713354170322, 1.9961802242323756, 4.752406077925116, 2.5331639968790114, 3.50089546199888, 4.632576003670692, 5.307726819068193, 3.8695560600608587, 3.3758792630396783, 6.541768602095544, 3.9624353339895606, 4.05843336135149, 3.5142189306207, 1.0063229692168534, 1.8456364800222218, 5.204048558138311, 6.329094064887613, 5.045321858953685, 3.3582949778065085, 3.5824720324017107, 2.7380548459477723, 4.336722505744547, 3.6195021886378527, 5.659723327029496, 4.878383041359484, 3.47003486007452, 5.091717737261206, 1.3930328954011202, 4.853382098022848, 1.5430031218566, 6.069683313369751]
[0.07777094487480972, 0.10791232675427989, 0.1551200428369398, 0.12681917863308775, 0.07504118355561276, 0.07170364030693867, 0.0953289500570723, 0.09939687970873652, 0.05768216367126727, 0.060958867208385954, 0.1059464810568155, 0.1147313810865946, 0.10947229079331974, 0.08455097824525164, 0.10552871348906537, 0.0831157435682051, 0.060712840816728314, 0.12153486086397755, 0.017047514055608486, 0.0025438198971809173, 0.09753654824037637, 0.09860556934751114, 0.1144513609648055, 0.11362468534890487, 0.10888905269188845, 0.1020489099948686, 0.09196535277427459, 0.12957141026664448, 0.09877942722024662, 0.09641852341022114, 0.06983739553893707, 0.0988694243323134, 0.06745748753108237, 0.09225554763320454, 0.06924899473634301, 0.12619823171776168, 0.04073837192310971, 0.09698787914132889, 0.05169722442610227, 0.07144684616324244, 0.09454236742185086, 0.10832095549118761, 0.07897053183797671, 0.06889549516407507, 0.13350548167541929, 0.08086602722427674, 0.08282517063982632, 0.07171875368613674, 0.02053720345340517, 0.037666050612698405, 0.10620507261506756, 0.12916518499770638, 0.10296575222354458, 0.06853663220013283, 0.07311167413064716, 0.05587867032546474, 0.08850454093356218, 0.07386739160485413, 0.11550455769447952, 0.09955883757876498, 0.07081703796070449, 0.10391260688288176, 0.028429242763288166, 0.09904861424536425, 0.03148985962972653, 0.1238710880279541]
[12.858272477076508, 9.266781933791815, 6.446620189830576, 7.885242679998679, 13.326015830479319, 13.946293322338216, 10.489992802829697, 10.060677990398773, 17.33638158407226, 16.404504312416627, 9.43872783715897, 8.716011177841924, 9.13473165449665, 11.827184271001142, 9.476093917354708, 12.031414953045521, 16.47098021683196, 8.22809186509215, 58.659579146732455, 393.10959125219847, 10.25256704323312, 10.141414999346997, 8.737336031395085, 8.800904459531145, 9.183659654286751, 9.799222745743034, 10.873660240878555, 7.71775191720229, 10.123565484646099, 10.371451092913045, 14.318976134246888, 10.114350384390486, 14.824151278080588, 10.839456549278355, 14.440642839760727, 7.924041299060894, 24.54688179212014, 10.310566731156364, 19.3433982404497, 13.996419068172587, 10.577268448736534, 9.2318240313284, 12.662951315203157, 14.514737104632074, 7.4903291419240485, 12.36613240844099, 12.073624385859723, 13.943354403177539, 48.69212121644512, 26.549106788033377, 9.415746116237084, 7.74202429251936, 9.71196711921205, 14.590737360422299, 13.677706219844543, 17.895916172942382, 11.298855284167525, 13.537773275512356, 8.657667021634719, 10.044311728819252, 14.120895603610077, 9.623471395795931, 35.17504874562964, 10.0960524043556, 31.756254608896278, 8.072908827395858]
Elapsed: 4.245376218928758~1.4713272087297729
Time per graph: 0.08664033099854607~0.030027085892444343
Speed: 19.474127163964475~47.17612730361522
Total Time: 6.0705
best val loss: 0.1524761744907924 test_score: 0.8163

Testing...
Test loss: 0.4787 score: 0.8367 time: 5.84s
test Score 0.8367
Epoch Time List: [22.387202598620206, 25.39612140553072, 27.046651202253997, 17.18195004807785, 16.575291105080396, 23.852797645144165, 17.55262561235577, 27.543617988936603, 19.44154998427257, 11.066210042219609, 25.90201360033825, 26.536751833278686, 17.723794647026807, 24.59936950635165, 23.25836303504184, 13.287649114616215, 21.930921718478203, 22.105980779975653, 19.886279372964054, 12.16377060348168, 23.156220139935613, 18.557923085987568, 23.38417379744351, 32.945557525381446, 21.853595769032836, 17.71262988448143, 21.220559786073864, 20.71647239336744, 18.728497486095876, 24.278091906104237, 19.30503193102777, 14.179936722386628, 15.847283244132996, 18.930993634741753, 25.453715721610934, 24.5005402402021, 17.470066737383604, 22.643102216999978, 15.923135004006326, 22.170676514972, 21.89880469068885, 18.11504234513268, 15.953475480433553, 19.251193278934807, 30.252876022364944, 21.102300819009542, 12.024477724451572, 20.924462964758277, 19.180164713412523, 15.244787043891847, 16.871341002173722, 21.66976546589285, 20.588592275045812, 24.894771623425186, 15.866067653987557, 17.186787031125277, 19.38650817517191, 19.246893578674644, 19.916550557594746, 24.327800136990845, 24.294737607706338, 18.74013505410403, 13.633913539815694, 20.385753616224974, 17.14964487729594, 16.075563682243228]
Total Epoch List: [66]
Total Time List: [6.070481457281858]
========================k_idx:1========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
emb dim:  64
Model: Ethident
Ethident(
  (encoder): HGATE_encoder(
    (lin1): Linear(in_features=14887, out_features=64, bias=True)
    (account_convs): ModuleList(
      (0): GATEConv()
      (1): GATConv(64, 64, heads=1)
    )
    (subgraph_conv): GATConv(64, 64, heads=1)
    (lin2): Linear(in_features=64, out_features=64, bias=True)
    (lin3): Linear(in_features=64, out_features=2, bias=True)
  )
  (proj_head_g1): Project_Head(
    (block): Sequential(
      (0): Linear(in_features=64, out_features=64, bias=True)
      (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=64, bias=True)
      (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
      (5): ReLU()
    )
    (linear_shortcut): Linear(in_features=64, out_features=64, bias=True)
  )
  (proj_head_g2): Project_Head(
    (block): Sequential(
      (0): Linear(in_features=64, out_features=64, bias=True)
      (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=64, bias=True)
      (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
      (5): ReLU()
    )
    (linear_shortcut): Linear(in_features=64, out_features=64, bias=True)
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=64, bias=True)
    (1): ReLU()
    (2): Dropout(p=0.0, inplace=True)
    (3): Linear(in_features=64, out_features=2, bias=True)
  )
)
Total number of parameters: 1003972
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7798933d9810>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.7259;  Loss pred: 0.6984; Loss self: 2.7439; time: 16.33s
Val loss: 0.7206 score: 0.5102 time: 4.54s
Test loss: 0.6615 score: 0.5510 time: 3.20s
Epoch 2/1000, LR 0.000015
Train loss: 0.7019;  Loss pred: 0.6745; Loss self: 2.7375; time: 16.80s
Val loss: 0.6436 score: 0.7347 time: 5.01s
Test loss: 0.5588 score: 0.8367 time: 5.69s
Epoch 3/1000, LR 0.000045
Train loss: 0.5933;  Loss pred: 0.5661; Loss self: 2.7202; time: 12.44s
Val loss: 0.6186 score: 0.6939 time: 3.52s
Test loss: 0.5042 score: 0.8571 time: 5.04s
Epoch 4/1000, LR 0.000075
Train loss: 0.5295;  Loss pred: 0.5025; Loss self: 2.7029; time: 13.35s
Val loss: 0.6214 score: 0.6735 time: 3.36s
Test loss: 0.4800 score: 0.8163 time: 5.08s
     INFO: Early stopping counter 1 of 20
Epoch 5/1000, LR 0.000105
Train loss: 0.4830;  Loss pred: 0.4560; Loss self: 2.6967; time: 5.14s
Val loss: 0.6162 score: 0.6735 time: 4.35s
Test loss: 0.4476 score: 0.8163 time: 3.85s
Epoch 6/1000, LR 0.000135
Train loss: 0.4329;  Loss pred: 0.4058; Loss self: 2.7188; time: 12.65s
Val loss: 0.6314 score: 0.7143 time: 3.61s
Test loss: 0.4343 score: 0.8776 time: 0.96s
     INFO: Early stopping counter 1 of 20
Epoch 7/1000, LR 0.000165
Train loss: 0.4170;  Loss pred: 0.3896; Loss self: 2.7342; time: 13.72s
Val loss: 0.6548 score: 0.7143 time: 1.65s
Test loss: 0.4257 score: 0.8367 time: 4.79s
     INFO: Early stopping counter 2 of 20
Epoch 8/1000, LR 0.000195
Train loss: 0.4102;  Loss pred: 0.3830; Loss self: 2.7159; time: 13.66s
Val loss: 0.6318 score: 0.7755 time: 4.74s
Test loss: 0.3864 score: 0.8980 time: 4.88s
     INFO: Early stopping counter 3 of 20
Epoch 9/1000, LR 0.000225
Train loss: 0.3665;  Loss pred: 0.3399; Loss self: 2.6592; time: 11.03s
Val loss: 0.6078 score: 0.7755 time: 3.97s
Test loss: 0.3490 score: 0.8980 time: 4.88s
Epoch 10/1000, LR 0.000255
Train loss: 0.3054;  Loss pred: 0.2792; Loss self: 2.6110; time: 12.38s
Val loss: 0.5976 score: 0.7755 time: 2.13s
Test loss: 0.3344 score: 0.8776 time: 4.49s
Epoch 11/1000, LR 0.000285
Train loss: 0.2551;  Loss pred: 0.2294; Loss self: 2.5719; time: 9.74s
Val loss: 0.6123 score: 0.7959 time: 3.41s
Test loss: 0.3289 score: 0.8980 time: 3.03s
     INFO: Early stopping counter 1 of 20
Epoch 12/1000, LR 0.000285
Train loss: 0.2398;  Loss pred: 0.2145; Loss self: 2.5362; time: 13.17s
Val loss: 0.6020 score: 0.7959 time: 5.23s
Test loss: 0.3007 score: 0.8980 time: 3.82s
     INFO: Early stopping counter 2 of 20
Epoch 13/1000, LR 0.000285
Train loss: 0.1984;  Loss pred: 0.1734; Loss self: 2.4991; time: 14.32s
Val loss: 0.5916 score: 0.7959 time: 4.23s
Test loss: 0.2774 score: 0.9184 time: 4.41s
Epoch 14/1000, LR 0.000285
Train loss: 0.1607;  Loss pred: 0.1359; Loss self: 2.4790; time: 9.28s
Val loss: 0.5986 score: 0.7959 time: 3.22s
Test loss: 0.2696 score: 0.9184 time: 4.88s
     INFO: Early stopping counter 1 of 20
Epoch 15/1000, LR 0.000285
Train loss: 0.1407;  Loss pred: 0.1160; Loss self: 2.4698; time: 11.97s
Val loss: 0.6153 score: 0.7959 time: 5.43s
Test loss: 0.2540 score: 0.9388 time: 4.37s
     INFO: Early stopping counter 2 of 20
Epoch 16/1000, LR 0.000285
Train loss: 0.1252;  Loss pred: 0.1007; Loss self: 2.4489; time: 10.43s
Val loss: 0.6296 score: 0.7959 time: 4.18s
Test loss: 0.2431 score: 0.9388 time: 2.60s
     INFO: Early stopping counter 3 of 20
Epoch 17/1000, LR 0.000285
Train loss: 0.1114;  Loss pred: 0.0873; Loss self: 2.4134; time: 13.12s
Val loss: 0.6216 score: 0.8163 time: 2.89s
Test loss: 0.2341 score: 0.9388 time: 7.08s
     INFO: Early stopping counter 4 of 20
Epoch 18/1000, LR 0.000285
Train loss: 0.0956;  Loss pred: 0.0716; Loss self: 2.3927; time: 21.62s
Val loss: 0.6049 score: 0.8163 time: 5.39s
Test loss: 0.2301 score: 0.9388 time: 3.90s
     INFO: Early stopping counter 5 of 20
Epoch 19/1000, LR 0.000285
Train loss: 0.0852;  Loss pred: 0.0613; Loss self: 2.3902; time: 14.15s
Val loss: 0.5906 score: 0.8163 time: 6.72s
Test loss: 0.2265 score: 0.9184 time: 5.42s
Epoch 20/1000, LR 0.000285
Train loss: 0.0766;  Loss pred: 0.0526; Loss self: 2.3964; time: 6.44s
Val loss: 0.5827 score: 0.8367 time: 3.41s
Test loss: 0.2188 score: 0.9388 time: 3.90s
Epoch 21/1000, LR 0.000285
Train loss: 0.0691;  Loss pred: 0.0450; Loss self: 2.4081; time: 7.97s
Val loss: 0.5838 score: 0.8367 time: 3.23s
Test loss: 0.2096 score: 0.9388 time: 2.81s
     INFO: Early stopping counter 1 of 20
Epoch 22/1000, LR 0.000285
Train loss: 0.0634;  Loss pred: 0.0392; Loss self: 2.4200; time: 13.86s
Val loss: 0.5908 score: 0.8367 time: 4.36s
Test loss: 0.2023 score: 0.9388 time: 3.46s
     INFO: Early stopping counter 2 of 20
Epoch 23/1000, LR 0.000285
Train loss: 0.0588;  Loss pred: 0.0345; Loss self: 2.4292; time: 8.41s
Val loss: 0.5982 score: 0.8367 time: 3.19s
Test loss: 0.1973 score: 0.9388 time: 2.87s
     INFO: Early stopping counter 3 of 20
Epoch 24/1000, LR 0.000285
Train loss: 0.0548;  Loss pred: 0.0305; Loss self: 2.4336; time: 13.28s
Val loss: 0.6050 score: 0.8163 time: 4.83s
Test loss: 0.1939 score: 0.9592 time: 5.49s
     INFO: Early stopping counter 4 of 20
Epoch 25/1000, LR 0.000285
Train loss: 0.0516;  Loss pred: 0.0272; Loss self: 2.4341; time: 13.51s
Val loss: 0.6094 score: 0.8367 time: 4.77s
Test loss: 0.1928 score: 0.9592 time: 1.85s
     INFO: Early stopping counter 5 of 20
Epoch 26/1000, LR 0.000285
Train loss: 0.0488;  Loss pred: 0.0244; Loss self: 2.4327; time: 10.07s
Val loss: 0.6121 score: 0.8163 time: 4.00s
Test loss: 0.1938 score: 0.9592 time: 3.65s
     INFO: Early stopping counter 6 of 20
Epoch 27/1000, LR 0.000285
Train loss: 0.0465;  Loss pred: 0.0222; Loss self: 2.4317; time: 13.78s
Val loss: 0.6153 score: 0.8163 time: 1.05s
Test loss: 0.1953 score: 0.9592 time: 1.87s
     INFO: Early stopping counter 7 of 20
Epoch 28/1000, LR 0.000285
Train loss: 0.0448;  Loss pred: 0.0205; Loss self: 2.4333; time: 9.37s
Val loss: 0.6194 score: 0.8163 time: 1.92s
Test loss: 0.1970 score: 0.9592 time: 5.35s
     INFO: Early stopping counter 8 of 20
Epoch 29/1000, LR 0.000285
Train loss: 0.0433;  Loss pred: 0.0189; Loss self: 2.4368; time: 8.94s
Val loss: 0.6233 score: 0.8163 time: 1.81s
Test loss: 0.1985 score: 0.9592 time: 4.46s
     INFO: Early stopping counter 9 of 20
Epoch 30/1000, LR 0.000285
Train loss: 0.0419;  Loss pred: 0.0175; Loss self: 2.4416; time: 14.88s
Val loss: 0.6274 score: 0.8163 time: 3.02s
Test loss: 0.1996 score: 0.9592 time: 4.29s
     INFO: Early stopping counter 10 of 20
Epoch 31/1000, LR 0.000285
Train loss: 0.0406;  Loss pred: 0.0162; Loss self: 2.4467; time: 11.41s
Val loss: 0.6323 score: 0.8163 time: 5.58s
Test loss: 0.2006 score: 0.9592 time: 4.64s
     INFO: Early stopping counter 11 of 20
Epoch 32/1000, LR 0.000285
Train loss: 0.0396;  Loss pred: 0.0151; Loss self: 2.4509; time: 14.38s
Val loss: 0.6379 score: 0.8163 time: 3.20s
Test loss: 0.2016 score: 0.9592 time: 6.30s
     INFO: Early stopping counter 12 of 20
Epoch 33/1000, LR 0.000285
Train loss: 0.0386;  Loss pred: 0.0141; Loss self: 2.4534; time: 11.67s
Val loss: 0.6444 score: 0.8163 time: 3.07s
Test loss: 0.2027 score: 0.9592 time: 4.66s
     INFO: Early stopping counter 13 of 20
Epoch 34/1000, LR 0.000285
Train loss: 0.0378;  Loss pred: 0.0133; Loss self: 2.4537; time: 6.50s
Val loss: 0.6511 score: 0.8163 time: 1.99s
Test loss: 0.2039 score: 0.9592 time: 3.74s
     INFO: Early stopping counter 14 of 20
Epoch 35/1000, LR 0.000285
Train loss: 0.0370;  Loss pred: 0.0125; Loss self: 2.4522; time: 14.48s
Val loss: 0.6574 score: 0.8163 time: 2.71s
Test loss: 0.2054 score: 0.9592 time: 3.56s
     INFO: Early stopping counter 15 of 20
Epoch 36/1000, LR 0.000285
Train loss: 0.0363;  Loss pred: 0.0119; Loss self: 2.4495; time: 11.83s
Val loss: 0.6632 score: 0.8163 time: 5.31s
Test loss: 0.2070 score: 0.9592 time: 6.18s
     INFO: Early stopping counter 16 of 20
Epoch 37/1000, LR 0.000285
Train loss: 0.0357;  Loss pred: 0.0112; Loss self: 2.4460; time: 11.39s
Val loss: 0.6683 score: 0.8163 time: 2.23s
Test loss: 0.2086 score: 0.9592 time: 4.48s
     INFO: Early stopping counter 17 of 20
Epoch 38/1000, LR 0.000284
Train loss: 0.0351;  Loss pred: 0.0107; Loss self: 2.4422; time: 12.80s
Val loss: 0.6728 score: 0.8163 time: 5.47s
Test loss: 0.2102 score: 0.9592 time: 4.54s
     INFO: Early stopping counter 18 of 20
Epoch 39/1000, LR 0.000284
Train loss: 0.0346;  Loss pred: 0.0103; Loss self: 2.4386; time: 16.71s
Val loss: 0.6770 score: 0.8163 time: 9.28s
Test loss: 0.2118 score: 0.9592 time: 6.19s
     INFO: Early stopping counter 19 of 20
Epoch 40/1000, LR 0.000284
Train loss: 0.0342;  Loss pred: 0.0098; Loss self: 2.4352; time: 12.47s
Val loss: 0.6814 score: 0.8163 time: 4.79s
Test loss: 0.2132 score: 0.9592 time: 4.19s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 019,   Train_Loss: 0.0766,   Val_Loss: 0.5827,   Val_Precision: 1.0000,   Val_Recall: 0.6800,   Val_accuracy: 0.8095,   Val_Score: 0.8367,   Val_Loss: 0.5827,   Test_Precision: 1.0000,   Test_Recall: 0.8750,   Test_accuracy: 0.9333,   Test_Score: 0.9388,   Test_loss: 0.2188


[3.810776298865676, 5.287704010959715, 7.60088209901005, 6.2141397530213, 3.677017994225025, 3.5134783750399947, 4.671118552796543, 4.87044710572809, 2.8264260198920965, 2.9869844932109118, 5.19137757178396, 5.621837673243135, 5.3641422488726676, 4.14299793401733, 5.170906960964203, 4.07267143484205, 2.9749292000196874, 5.9552081823349, 0.8353281887248158, 0.12464717496186495, 4.779290863778442, 4.831672898028046, 5.608116687275469, 5.567609582096338, 5.335563581902534, 5.000396589748561, 4.506302285939455, 6.34899910306558, 4.8401919337920845, 4.724507647100836, 3.4220323814079165, 4.844601792283356, 3.3054168890230358, 4.520521834027022, 3.3932007420808077, 6.183713354170322, 1.9961802242323756, 4.752406077925116, 2.5331639968790114, 3.50089546199888, 4.632576003670692, 5.307726819068193, 3.8695560600608587, 3.3758792630396783, 6.541768602095544, 3.9624353339895606, 4.05843336135149, 3.5142189306207, 1.0063229692168534, 1.8456364800222218, 5.204048558138311, 6.329094064887613, 5.045321858953685, 3.3582949778065085, 3.5824720324017107, 2.7380548459477723, 4.336722505744547, 3.6195021886378527, 5.659723327029496, 4.878383041359484, 3.47003486007452, 5.091717737261206, 1.3930328954011202, 4.853382098022848, 1.5430031218566, 6.069683313369751, 3.2088029882870615, 5.7005663979798555, 5.043659045360982, 5.0859183967113495, 3.856475123204291, 0.969787506852299, 4.797383821103722, 4.887179850134999, 4.886932740919292, 4.4923274400644, 3.0408938881009817, 3.8316122950054705, 4.420094669796526, 4.882189225405455, 4.375175157096237, 2.603249988052994, 7.0887373448349535, 3.9096990418620408, 5.428581133019179, 3.9104652940295637, 2.8187025147490203, 3.461394420824945, 2.886076231021434, 5.501855520065874, 1.8546954332850873, 3.6576602710410953, 1.8774004941806197, 5.362701900769025, 4.478408069815487, 4.2972864313051105, 4.645255520008504, 6.31052358308807, 4.676896725781262, 3.744243200868368, 3.5682939998805523, 6.199555632658303, 4.483061084989458, 4.550577292218804, 6.19503855612129, 4.199200469069183]
[0.07777094487480972, 0.10791232675427989, 0.1551200428369398, 0.12681917863308775, 0.07504118355561276, 0.07170364030693867, 0.0953289500570723, 0.09939687970873652, 0.05768216367126727, 0.060958867208385954, 0.1059464810568155, 0.1147313810865946, 0.10947229079331974, 0.08455097824525164, 0.10552871348906537, 0.0831157435682051, 0.060712840816728314, 0.12153486086397755, 0.017047514055608486, 0.0025438198971809173, 0.09753654824037637, 0.09860556934751114, 0.1144513609648055, 0.11362468534890487, 0.10888905269188845, 0.1020489099948686, 0.09196535277427459, 0.12957141026664448, 0.09877942722024662, 0.09641852341022114, 0.06983739553893707, 0.0988694243323134, 0.06745748753108237, 0.09225554763320454, 0.06924899473634301, 0.12619823171776168, 0.04073837192310971, 0.09698787914132889, 0.05169722442610227, 0.07144684616324244, 0.09454236742185086, 0.10832095549118761, 0.07897053183797671, 0.06889549516407507, 0.13350548167541929, 0.08086602722427674, 0.08282517063982632, 0.07171875368613674, 0.02053720345340517, 0.037666050612698405, 0.10620507261506756, 0.12916518499770638, 0.10296575222354458, 0.06853663220013283, 0.07311167413064716, 0.05587867032546474, 0.08850454093356218, 0.07386739160485413, 0.11550455769447952, 0.09955883757876498, 0.07081703796070449, 0.10391260688288176, 0.028429242763288166, 0.09904861424536425, 0.03148985962972653, 0.1238710880279541, 0.06548577527116452, 0.11633808975469093, 0.10293181725226495, 0.10379425299410917, 0.07870357394294471, 0.019791581772495896, 0.0979057922674229, 0.09973836428846936, 0.09973332124325085, 0.09168015183804899, 0.06205905894083636, 0.07819616928582593, 0.09020601366931687, 0.09963651480419296, 0.08928928892033136, 0.05312755077659172, 0.14466810907826436, 0.07978977636453144, 0.1107873700616159, 0.07980541416386865, 0.057524541117326945, 0.0706407024658152, 0.058899514918804775, 0.11228276571563008, 0.03785092720989974, 0.07464612798043052, 0.038314295799604485, 0.10944289593406174, 0.09139608305745892, 0.0876997230878594, 0.09480113306139805, 0.12878619557322593, 0.09544687195471963, 0.07641312654833404, 0.07282232652817454, 0.12652154352363884, 0.09149104255080527, 0.09286892433099601, 0.12642935828818957, 0.08569796875651393]
[12.858272477076508, 9.266781933791815, 6.446620189830576, 7.885242679998679, 13.326015830479319, 13.946293322338216, 10.489992802829697, 10.060677990398773, 17.33638158407226, 16.404504312416627, 9.43872783715897, 8.716011177841924, 9.13473165449665, 11.827184271001142, 9.476093917354708, 12.031414953045521, 16.47098021683196, 8.22809186509215, 58.659579146732455, 393.10959125219847, 10.25256704323312, 10.141414999346997, 8.737336031395085, 8.800904459531145, 9.183659654286751, 9.799222745743034, 10.873660240878555, 7.71775191720229, 10.123565484646099, 10.371451092913045, 14.318976134246888, 10.114350384390486, 14.824151278080588, 10.839456549278355, 14.440642839760727, 7.924041299060894, 24.54688179212014, 10.310566731156364, 19.3433982404497, 13.996419068172587, 10.577268448736534, 9.2318240313284, 12.662951315203157, 14.514737104632074, 7.4903291419240485, 12.36613240844099, 12.073624385859723, 13.943354403177539, 48.69212121644512, 26.549106788033377, 9.415746116237084, 7.74202429251936, 9.71196711921205, 14.590737360422299, 13.677706219844543, 17.895916172942382, 11.298855284167525, 13.537773275512356, 8.657667021634719, 10.044311728819252, 14.120895603610077, 9.623471395795931, 35.17504874562964, 10.0960524043556, 31.756254608896278, 8.072908827395858, 15.270491887118759, 8.595637096230373, 9.715168999194908, 9.634444790086354, 12.705903301480806, 50.52653251746088, 10.213900289663856, 10.026232203966563, 10.026739183396762, 10.90748629830455, 16.113682950837912, 12.788350236758504, 11.085735410788306, 10.0364811230623, 11.199551615785102, 18.82262565058063, 6.9123734758916875, 12.532933986822465, 9.026299653505959, 12.53047816964708, 17.383884870292174, 14.15614461767173, 16.978068518535984, 8.90608628694294, 26.419432064492582, 13.396542152356027, 26.099918558605555, 9.137185118004279, 10.941387929845087, 11.402544555336615, 10.548397130995776, 7.764807365797329, 10.477032714853179, 13.086756754645606, 13.73205234816421, 7.903792288252977, 10.930031750864536, 10.76786457045502, 7.909555292691976, 11.668888008783634]
Elapsed: 4.2583338598949165~1.3943560044947447
Time per graph: 0.08690477265091666~0.02845624498968867
Speed: 17.10918692980968~37.616814399472055
Total Time: 4.2002
best val loss: 0.582723148319186 test_score: 0.9388

Testing...
Test loss: 0.2188 score: 0.9388 time: 3.87s
test Score 0.9388
Epoch Time List: [22.387202598620206, 25.39612140553072, 27.046651202253997, 17.18195004807785, 16.575291105080396, 23.852797645144165, 17.55262561235577, 27.543617988936603, 19.44154998427257, 11.066210042219609, 25.90201360033825, 26.536751833278686, 17.723794647026807, 24.59936950635165, 23.25836303504184, 13.287649114616215, 21.930921718478203, 22.105980779975653, 19.886279372964054, 12.16377060348168, 23.156220139935613, 18.557923085987568, 23.38417379744351, 32.945557525381446, 21.853595769032836, 17.71262988448143, 21.220559786073864, 20.71647239336744, 18.728497486095876, 24.278091906104237, 19.30503193102777, 14.179936722386628, 15.847283244132996, 18.930993634741753, 25.453715721610934, 24.5005402402021, 17.470066737383604, 22.643102216999978, 15.923135004006326, 22.170676514972, 21.89880469068885, 18.11504234513268, 15.953475480433553, 19.251193278934807, 30.252876022364944, 21.102300819009542, 12.024477724451572, 20.924462964758277, 19.180164713412523, 15.244787043891847, 16.871341002173722, 21.66976546589285, 20.588592275045812, 24.894771623425186, 15.866067653987557, 17.186787031125277, 19.38650817517191, 19.246893578674644, 19.916550557594746, 24.327800136990845, 24.294737607706338, 18.74013505410403, 13.633913539815694, 20.385753616224974, 17.14964487729594, 16.075563682243228, 24.066006793174893, 27.50272659258917, 20.998324641492218, 21.781952116638422, 13.345890897326171, 17.22067857114598, 20.162287706974894, 23.28083809837699, 19.884316817857325, 18.999821017496288, 16.177752176765352, 22.227262045256793, 22.952505866996944, 17.366788872051984, 21.764996321871877, 17.21211287844926, 23.093068353831768, 30.91145718842745, 26.29660627990961, 13.749480953905731, 14.01242743525654, 21.680959386285394, 14.4650087072514, 23.6086034742184, 20.126219552010298, 17.724287246353924, 16.699863084126264, 16.637637444771826, 15.215626585762948, 22.187149139121175, 21.630610153079033, 23.876952377147973, 19.400417258497328, 12.225089001469314, 20.75129406992346, 23.324686428066343, 18.090573902241886, 22.81343139708042, 32.18355860887095, 21.445867711212486]
Total Epoch List: [66, 40]
Total Time List: [6.070481457281858, 4.2001661183312535]
========================k_idx:2========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
emb dim:  64
Model: Ethident
Ethident(
  (encoder): HGATE_encoder(
    (lin1): Linear(in_features=14887, out_features=64, bias=True)
    (account_convs): ModuleList(
      (0): GATEConv()
      (1): GATConv(64, 64, heads=1)
    )
    (subgraph_conv): GATConv(64, 64, heads=1)
    (lin2): Linear(in_features=64, out_features=64, bias=True)
    (lin3): Linear(in_features=64, out_features=2, bias=True)
  )
  (proj_head_g1): Project_Head(
    (block): Sequential(
      (0): Linear(in_features=64, out_features=64, bias=True)
      (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=64, bias=True)
      (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
      (5): ReLU()
    )
    (linear_shortcut): Linear(in_features=64, out_features=64, bias=True)
  )
  (proj_head_g2): Project_Head(
    (block): Sequential(
      (0): Linear(in_features=64, out_features=64, bias=True)
      (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=64, bias=True)
      (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
      (5): ReLU()
    )
    (linear_shortcut): Linear(in_features=64, out_features=64, bias=True)
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=64, bias=True)
    (1): ReLU()
    (2): Dropout(p=0.0, inplace=True)
    (3): Linear(in_features=64, out_features=2, bias=True)
  )
)
Total number of parameters: 1003972
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x779895dfb5e0>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.8992;  Loss pred: 0.8723; Loss self: 2.6947; time: 9.54s
Val loss: 0.7911 score: 0.3673 time: 5.77s
Test loss: 0.7848 score: 0.3542 time: 4.03s
Epoch 2/1000, LR 0.000015
Train loss: 0.8892;  Loss pred: 0.8621; Loss self: 2.7028; time: 8.28s
Val loss: 0.7317 score: 0.4694 time: 2.26s
Test loss: 0.7188 score: 0.3958 time: 5.78s
Epoch 3/1000, LR 0.000045
Train loss: 0.8049;  Loss pred: 0.7776; Loss self: 2.7313; time: 15.82s
Val loss: 0.6723 score: 0.6122 time: 6.41s
Test loss: 0.6676 score: 0.5625 time: 4.88s
Epoch 4/1000, LR 0.000075
Train loss: 0.7086;  Loss pred: 0.6809; Loss self: 2.7727; time: 8.48s
Val loss: 0.6415 score: 0.7959 time: 3.87s
Test loss: 0.6318 score: 0.7917 time: 5.75s
Epoch 5/1000, LR 0.000105
Train loss: 0.6474;  Loss pred: 0.6193; Loss self: 2.8130; time: 8.88s
Val loss: 0.6369 score: 0.6531 time: 2.30s
Test loss: 0.6248 score: 0.7083 time: 6.16s
Epoch 6/1000, LR 0.000135
Train loss: 0.6383;  Loss pred: 0.6099; Loss self: 2.8422; time: 10.14s
Val loss: 0.6190 score: 0.6735 time: 3.97s
Test loss: 0.6070 score: 0.7917 time: 3.94s
Epoch 7/1000, LR 0.000165
Train loss: 0.6101;  Loss pred: 0.5815; Loss self: 2.8601; time: 13.20s
Val loss: 0.5912 score: 0.8163 time: 2.93s
Test loss: 0.5824 score: 0.8542 time: 4.02s
Epoch 8/1000, LR 0.000195
Train loss: 0.5718;  Loss pred: 0.5432; Loss self: 2.8619; time: 10.56s
Val loss: 0.5621 score: 0.8571 time: 3.07s
Test loss: 0.5559 score: 0.8125 time: 1.24s
Epoch 9/1000, LR 0.000225
Train loss: 0.5350;  Loss pred: 0.5065; Loss self: 2.8485; time: 9.71s
Val loss: 0.5343 score: 0.8571 time: 5.20s
Test loss: 0.5220 score: 0.8750 time: 5.34s
Epoch 10/1000, LR 0.000255
Train loss: 0.4903;  Loss pred: 0.4620; Loss self: 2.8272; time: 10.19s
Val loss: 0.5082 score: 0.8571 time: 6.26s
Test loss: 0.4843 score: 0.8750 time: 4.71s
Epoch 11/1000, LR 0.000285
Train loss: 0.4458;  Loss pred: 0.4178; Loss self: 2.7973; time: 9.12s
Val loss: 0.4814 score: 0.8571 time: 5.15s
Test loss: 0.4483 score: 0.8958 time: 2.68s
Epoch 12/1000, LR 0.000285
Train loss: 0.4044;  Loss pred: 0.3768; Loss self: 2.7559; time: 16.94s
Val loss: 0.4534 score: 0.8571 time: 0.89s
Test loss: 0.4125 score: 0.8958 time: 4.21s
Epoch 13/1000, LR 0.000285
Train loss: 0.3610;  Loss pred: 0.3340; Loss self: 2.6994; time: 14.02s
Val loss: 0.4252 score: 0.8776 time: 5.99s
Test loss: 0.3771 score: 0.8958 time: 4.82s
Epoch 14/1000, LR 0.000285
Train loss: 0.3169;  Loss pred: 0.2906; Loss self: 2.6323; time: 11.63s
Val loss: 0.4047 score: 0.8776 time: 0.84s
Test loss: 0.3457 score: 0.8958 time: 1.80s
Epoch 15/1000, LR 0.000285
Train loss: 0.2754;  Loss pred: 0.2497; Loss self: 2.5632; time: 17.49s
Val loss: 0.3947 score: 0.8776 time: 9.02s
Test loss: 0.3173 score: 0.8958 time: 4.94s
Epoch 16/1000, LR 0.000285
Train loss: 0.2418;  Loss pred: 0.2168; Loss self: 2.5011; time: 10.77s
Val loss: 0.3847 score: 0.8776 time: 5.53s
Test loss: 0.2977 score: 0.8958 time: 3.27s
Epoch 17/1000, LR 0.000285
Train loss: 0.2152;  Loss pred: 0.1907; Loss self: 2.4504; time: 8.77s
Val loss: 0.3717 score: 0.8776 time: 3.76s
Test loss: 0.2756 score: 0.9167 time: 3.42s
Epoch 18/1000, LR 0.000285
Train loss: 0.1883;  Loss pred: 0.1641; Loss self: 2.4234; time: 13.84s
Val loss: 0.3572 score: 0.8776 time: 4.49s
Test loss: 0.2560 score: 0.9167 time: 3.95s
Epoch 19/1000, LR 0.000285
Train loss: 0.1646;  Loss pred: 0.1405; Loss self: 2.4114; time: 12.37s
Val loss: 0.3481 score: 0.8776 time: 3.98s
Test loss: 0.2406 score: 0.9375 time: 5.78s
Epoch 20/1000, LR 0.000285
Train loss: 0.1477;  Loss pred: 0.1236; Loss self: 2.4079; time: 13.06s
Val loss: 0.3403 score: 0.8776 time: 4.69s
Test loss: 0.2280 score: 0.9167 time: 5.96s
Epoch 21/1000, LR 0.000285
Train loss: 0.1327;  Loss pred: 0.1085; Loss self: 2.4171; time: 8.90s
Val loss: 0.3372 score: 0.8776 time: 4.20s
Test loss: 0.2185 score: 0.9375 time: 6.21s
Epoch 22/1000, LR 0.000285
Train loss: 0.1201;  Loss pred: 0.0958; Loss self: 2.4348; time: 14.59s
Val loss: 0.3389 score: 0.8776 time: 5.76s
Test loss: 0.2111 score: 0.9375 time: 4.57s
     INFO: Early stopping counter 1 of 20
Epoch 23/1000, LR 0.000285
Train loss: 0.1095;  Loss pred: 0.0850; Loss self: 2.4516; time: 14.56s
Val loss: 0.3458 score: 0.8776 time: 5.99s
Test loss: 0.2062 score: 0.9375 time: 2.10s
     INFO: Early stopping counter 2 of 20
Epoch 24/1000, LR 0.000285
Train loss: 0.1011;  Loss pred: 0.0765; Loss self: 2.4625; time: 9.86s
Val loss: 0.3532 score: 0.8776 time: 5.22s
Test loss: 0.2039 score: 0.9167 time: 3.75s
     INFO: Early stopping counter 3 of 20
Epoch 25/1000, LR 0.000285
Train loss: 0.0937;  Loss pred: 0.0690; Loss self: 2.4695; time: 13.46s
Val loss: 0.3609 score: 0.8776 time: 0.42s
Test loss: 0.2032 score: 0.9167 time: 6.03s
     INFO: Early stopping counter 4 of 20
Epoch 26/1000, LR 0.000285
Train loss: 0.0871;  Loss pred: 0.0624; Loss self: 2.4734; time: 19.03s
Val loss: 0.3673 score: 0.8776 time: 6.49s
Test loss: 0.2025 score: 0.9167 time: 5.39s
     INFO: Early stopping counter 5 of 20
Epoch 27/1000, LR 0.000285
Train loss: 0.0808;  Loss pred: 0.0561; Loss self: 2.4756; time: 17.39s
Val loss: 0.3722 score: 0.8776 time: 3.31s
Test loss: 0.2017 score: 0.9167 time: 5.98s
     INFO: Early stopping counter 6 of 20
Epoch 28/1000, LR 0.000285
Train loss: 0.0745;  Loss pred: 0.0498; Loss self: 2.4764; time: 9.53s
Val loss: 0.3764 score: 0.8776 time: 3.68s
Test loss: 0.1996 score: 0.9167 time: 3.47s
     INFO: Early stopping counter 7 of 20
Epoch 29/1000, LR 0.000285
Train loss: 0.0685;  Loss pred: 0.0438; Loss self: 2.4755; time: 14.61s
Val loss: 0.3786 score: 0.8776 time: 5.03s
Test loss: 0.1970 score: 0.9167 time: 4.93s
     INFO: Early stopping counter 8 of 20
Epoch 30/1000, LR 0.000285
Train loss: 0.0632;  Loss pred: 0.0385; Loss self: 2.4730; time: 12.41s
Val loss: 0.3791 score: 0.8776 time: 4.27s
Test loss: 0.1948 score: 0.9167 time: 5.17s
     INFO: Early stopping counter 9 of 20
Epoch 31/1000, LR 0.000285
Train loss: 0.0588;  Loss pred: 0.0341; Loss self: 2.4702; time: 13.04s
Val loss: 0.3800 score: 0.8776 time: 5.46s
Test loss: 0.1933 score: 0.9167 time: 5.47s
     INFO: Early stopping counter 10 of 20
Epoch 32/1000, LR 0.000285
Train loss: 0.0554;  Loss pred: 0.0307; Loss self: 2.4676; time: 13.16s
Val loss: 0.3822 score: 0.8776 time: 4.15s
Test loss: 0.1929 score: 0.9375 time: 2.60s
     INFO: Early stopping counter 11 of 20
Epoch 33/1000, LR 0.000285
Train loss: 0.0526;  Loss pred: 0.0280; Loss self: 2.4671; time: 17.58s
Val loss: 0.3867 score: 0.8776 time: 2.54s
Test loss: 0.1932 score: 0.9375 time: 5.39s
     INFO: Early stopping counter 12 of 20
Epoch 34/1000, LR 0.000285
Train loss: 0.0503;  Loss pred: 0.0256; Loss self: 2.4688; time: 19.76s
Val loss: 0.3929 score: 0.8776 time: 6.02s
Test loss: 0.1943 score: 0.9375 time: 4.37s
     INFO: Early stopping counter 13 of 20
Epoch 35/1000, LR 0.000285
Train loss: 0.0483;  Loss pred: 0.0236; Loss self: 2.4721; time: 9.73s
Val loss: 0.4005 score: 0.8776 time: 3.43s
Test loss: 0.1963 score: 0.9375 time: 2.38s
     INFO: Early stopping counter 14 of 20
Epoch 36/1000, LR 0.000285
Train loss: 0.0467;  Loss pred: 0.0219; Loss self: 2.4762; time: 12.84s
Val loss: 0.4093 score: 0.8776 time: 4.98s
Test loss: 0.1984 score: 0.9167 time: 5.59s
     INFO: Early stopping counter 15 of 20
Epoch 37/1000, LR 0.000285
Train loss: 0.0453;  Loss pred: 0.0205; Loss self: 2.4807; time: 13.77s
Val loss: 0.4173 score: 0.8776 time: 6.40s
Test loss: 0.2003 score: 0.9167 time: 6.70s
     INFO: Early stopping counter 16 of 20
Epoch 38/1000, LR 0.000284
Train loss: 0.0440;  Loss pred: 0.0192; Loss self: 2.4855; time: 13.89s
Val loss: 0.4239 score: 0.8776 time: 5.39s
Test loss: 0.2020 score: 0.9167 time: 3.65s
     INFO: Early stopping counter 17 of 20
Epoch 39/1000, LR 0.000284
Train loss: 0.0429;  Loss pred: 0.0180; Loss self: 2.4902; time: 4.23s
Val loss: 0.4299 score: 0.8776 time: 5.31s
Test loss: 0.2033 score: 0.9167 time: 2.48s
     INFO: Early stopping counter 18 of 20
Epoch 40/1000, LR 0.000284
Train loss: 0.0418;  Loss pred: 0.0169; Loss self: 2.4946; time: 10.87s
Val loss: 0.4354 score: 0.8776 time: 2.47s
Test loss: 0.2043 score: 0.9167 time: 4.86s
     INFO: Early stopping counter 19 of 20
Epoch 41/1000, LR 0.000284
Train loss: 0.0409;  Loss pred: 0.0159; Loss self: 2.4987; time: 13.12s
Val loss: 0.4400 score: 0.8776 time: 5.38s
Test loss: 0.2050 score: 0.9167 time: 5.22s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 020,   Train_Loss: 0.1327,   Val_Loss: 0.3372,   Val_Precision: 1.0000,   Val_Recall: 0.7600,   Val_accuracy: 0.8636,   Val_Score: 0.8776,   Val_Loss: 0.3372,   Test_Precision: 1.0000,   Test_Recall: 0.8750,   Test_accuracy: 0.9333,   Test_Score: 0.9375,   Test_loss: 0.2185


[3.810776298865676, 5.287704010959715, 7.60088209901005, 6.2141397530213, 3.677017994225025, 3.5134783750399947, 4.671118552796543, 4.87044710572809, 2.8264260198920965, 2.9869844932109118, 5.19137757178396, 5.621837673243135, 5.3641422488726676, 4.14299793401733, 5.170906960964203, 4.07267143484205, 2.9749292000196874, 5.9552081823349, 0.8353281887248158, 0.12464717496186495, 4.779290863778442, 4.831672898028046, 5.608116687275469, 5.567609582096338, 5.335563581902534, 5.000396589748561, 4.506302285939455, 6.34899910306558, 4.8401919337920845, 4.724507647100836, 3.4220323814079165, 4.844601792283356, 3.3054168890230358, 4.520521834027022, 3.3932007420808077, 6.183713354170322, 1.9961802242323756, 4.752406077925116, 2.5331639968790114, 3.50089546199888, 4.632576003670692, 5.307726819068193, 3.8695560600608587, 3.3758792630396783, 6.541768602095544, 3.9624353339895606, 4.05843336135149, 3.5142189306207, 1.0063229692168534, 1.8456364800222218, 5.204048558138311, 6.329094064887613, 5.045321858953685, 3.3582949778065085, 3.5824720324017107, 2.7380548459477723, 4.336722505744547, 3.6195021886378527, 5.659723327029496, 4.878383041359484, 3.47003486007452, 5.091717737261206, 1.3930328954011202, 4.853382098022848, 1.5430031218566, 6.069683313369751, 3.2088029882870615, 5.7005663979798555, 5.043659045360982, 5.0859183967113495, 3.856475123204291, 0.969787506852299, 4.797383821103722, 4.887179850134999, 4.886932740919292, 4.4923274400644, 3.0408938881009817, 3.8316122950054705, 4.420094669796526, 4.882189225405455, 4.375175157096237, 2.603249988052994, 7.0887373448349535, 3.9096990418620408, 5.428581133019179, 3.9104652940295637, 2.8187025147490203, 3.461394420824945, 2.886076231021434, 5.501855520065874, 1.8546954332850873, 3.6576602710410953, 1.8774004941806197, 5.362701900769025, 4.478408069815487, 4.2972864313051105, 4.645255520008504, 6.31052358308807, 4.676896725781262, 3.744243200868368, 3.5682939998805523, 6.199555632658303, 4.483061084989458, 4.550577292218804, 6.19503855612129, 4.199200469069183, 4.0396649478934705, 5.795600371900946, 4.892133588902652, 5.76692146807909, 6.169844812713563, 3.9432545229792595, 4.026165000163019, 1.244650867767632, 5.351192672736943, 4.715490011963993, 2.684553130995482, 4.219904045108706, 4.845601007808, 1.8029683642089367, 4.978443570900708, 3.2742915647104383, 3.4286719686351717, 3.957331013865769, 5.786086079664528, 5.964977411087602, 6.213640558067709, 4.589145562145859, 2.106373122893274, 3.7549119042232633, 6.037730118259788, 5.396569433156401, 5.989566906820983, 3.4739208151586354, 4.941726053133607, 5.171155096031725, 5.479880659841001, 2.607269615866244, 5.39948142785579, 4.388295757584274, 2.3900880198925734, 5.603196025826037, 6.710568270646036, 3.659371356945485, 2.4829378691501915, 4.869350966997445, 5.2283323388546705]
[0.07777094487480972, 0.10791232675427989, 0.1551200428369398, 0.12681917863308775, 0.07504118355561276, 0.07170364030693867, 0.0953289500570723, 0.09939687970873652, 0.05768216367126727, 0.060958867208385954, 0.1059464810568155, 0.1147313810865946, 0.10947229079331974, 0.08455097824525164, 0.10552871348906537, 0.0831157435682051, 0.060712840816728314, 0.12153486086397755, 0.017047514055608486, 0.0025438198971809173, 0.09753654824037637, 0.09860556934751114, 0.1144513609648055, 0.11362468534890487, 0.10888905269188845, 0.1020489099948686, 0.09196535277427459, 0.12957141026664448, 0.09877942722024662, 0.09641852341022114, 0.06983739553893707, 0.0988694243323134, 0.06745748753108237, 0.09225554763320454, 0.06924899473634301, 0.12619823171776168, 0.04073837192310971, 0.09698787914132889, 0.05169722442610227, 0.07144684616324244, 0.09454236742185086, 0.10832095549118761, 0.07897053183797671, 0.06889549516407507, 0.13350548167541929, 0.08086602722427674, 0.08282517063982632, 0.07171875368613674, 0.02053720345340517, 0.037666050612698405, 0.10620507261506756, 0.12916518499770638, 0.10296575222354458, 0.06853663220013283, 0.07311167413064716, 0.05587867032546474, 0.08850454093356218, 0.07386739160485413, 0.11550455769447952, 0.09955883757876498, 0.07081703796070449, 0.10391260688288176, 0.028429242763288166, 0.09904861424536425, 0.03148985962972653, 0.1238710880279541, 0.06548577527116452, 0.11633808975469093, 0.10293181725226495, 0.10379425299410917, 0.07870357394294471, 0.019791581772495896, 0.0979057922674229, 0.09973836428846936, 0.09973332124325085, 0.09168015183804899, 0.06205905894083636, 0.07819616928582593, 0.09020601366931687, 0.09963651480419296, 0.08928928892033136, 0.05312755077659172, 0.14466810907826436, 0.07978977636453144, 0.1107873700616159, 0.07980541416386865, 0.057524541117326945, 0.0706407024658152, 0.058899514918804775, 0.11228276571563008, 0.03785092720989974, 0.07464612798043052, 0.038314295799604485, 0.10944289593406174, 0.09139608305745892, 0.0876997230878594, 0.09480113306139805, 0.12878619557322593, 0.09544687195471963, 0.07641312654833404, 0.07282232652817454, 0.12652154352363884, 0.09149104255080527, 0.09286892433099601, 0.12642935828818957, 0.08569796875651393, 0.08415968641444731, 0.12074167441460304, 0.10191944976880525, 0.12014419725164771, 0.12853843359819925, 0.08215113589540124, 0.08387843750339623, 0.025930226411825668, 0.11148318068201964, 0.09823937524924986, 0.05592819022907255, 0.08791466760643137, 0.10095002099599999, 0.03756184092101952, 0.10371757439376476, 0.06821440759813413, 0.07143066601323274, 0.08244439612220351, 0.12054345999301101, 0.12427036273099172, 0.12945084495974393, 0.09560719921137206, 0.043882773393609874, 0.07822733133798465, 0.12578604413041225, 0.11242852985742502, 0.1247826438921038, 0.0723733503158049, 0.10295262610695015, 0.10773239783399428, 0.11416418041335419, 0.054318116997213416, 0.11248919641366228, 0.0914228282830057, 0.049793500414428614, 0.11673325053804244, 0.13980350563845909, 0.0762369032696976, 0.051727872273962326, 0.10144481181244676, 0.10892359039280564]
[12.858272477076508, 9.266781933791815, 6.446620189830576, 7.885242679998679, 13.326015830479319, 13.946293322338216, 10.489992802829697, 10.060677990398773, 17.33638158407226, 16.404504312416627, 9.43872783715897, 8.716011177841924, 9.13473165449665, 11.827184271001142, 9.476093917354708, 12.031414953045521, 16.47098021683196, 8.22809186509215, 58.659579146732455, 393.10959125219847, 10.25256704323312, 10.141414999346997, 8.737336031395085, 8.800904459531145, 9.183659654286751, 9.799222745743034, 10.873660240878555, 7.71775191720229, 10.123565484646099, 10.371451092913045, 14.318976134246888, 10.114350384390486, 14.824151278080588, 10.839456549278355, 14.440642839760727, 7.924041299060894, 24.54688179212014, 10.310566731156364, 19.3433982404497, 13.996419068172587, 10.577268448736534, 9.2318240313284, 12.662951315203157, 14.514737104632074, 7.4903291419240485, 12.36613240844099, 12.073624385859723, 13.943354403177539, 48.69212121644512, 26.549106788033377, 9.415746116237084, 7.74202429251936, 9.71196711921205, 14.590737360422299, 13.677706219844543, 17.895916172942382, 11.298855284167525, 13.537773275512356, 8.657667021634719, 10.044311728819252, 14.120895603610077, 9.623471395795931, 35.17504874562964, 10.0960524043556, 31.756254608896278, 8.072908827395858, 15.270491887118759, 8.595637096230373, 9.715168999194908, 9.634444790086354, 12.705903301480806, 50.52653251746088, 10.213900289663856, 10.026232203966563, 10.026739183396762, 10.90748629830455, 16.113682950837912, 12.788350236758504, 11.085735410788306, 10.0364811230623, 11.199551615785102, 18.82262565058063, 6.9123734758916875, 12.532933986822465, 9.026299653505959, 12.53047816964708, 17.383884870292174, 14.15614461767173, 16.978068518535984, 8.90608628694294, 26.419432064492582, 13.396542152356027, 26.099918558605555, 9.137185118004279, 10.941387929845087, 11.402544555336615, 10.548397130995776, 7.764807365797329, 10.477032714853179, 13.086756754645606, 13.73205234816421, 7.903792288252977, 10.930031750864536, 10.76786457045502, 7.909555292691976, 11.668888008783634, 11.882173551306563, 8.282144544113226, 9.811669924321674, 8.32333165375813, 7.779774282343593, 12.172686221566648, 11.922015118122703, 38.56503156269946, 8.969962947615139, 10.179217828521724, 17.880070781910995, 11.374666221530994, 9.905891946665603, 26.622763301263078, 9.6415675534745, 14.65965967030333, 13.99958947344474, 12.129387163170511, 8.295763204888752, 8.046970959316356, 7.7249399207164355, 10.459463390295134, 22.787985413556797, 12.783255965609458, 7.950007545854783, 8.894539502278814, 8.013935021802176, 13.817240678183994, 9.71320536264098, 9.282258820052517, 8.759314842705484, 18.410063810777928, 8.889742587569465, 10.938187089382431, 20.082942385593583, 8.566539485457984, 7.152896455873322, 13.117007080709648, 19.331937619699055, 9.857576569305687, 9.180747681872683]
Elapsed: 4.318126853403388~1.3830232400746507
Time per graph: 0.08865543389304151~0.028452012658726877
Speed: 15.780489385714983~32.17216600180566
Total Time: 5.2291
best val loss: 0.33721064958645375 test_score: 0.9375

Testing...
Test loss: 0.3771 score: 0.8958 time: 1.49s
test Score 0.8958
Epoch Time List: [22.387202598620206, 25.39612140553072, 27.046651202253997, 17.18195004807785, 16.575291105080396, 23.852797645144165, 17.55262561235577, 27.543617988936603, 19.44154998427257, 11.066210042219609, 25.90201360033825, 26.536751833278686, 17.723794647026807, 24.59936950635165, 23.25836303504184, 13.287649114616215, 21.930921718478203, 22.105980779975653, 19.886279372964054, 12.16377060348168, 23.156220139935613, 18.557923085987568, 23.38417379744351, 32.945557525381446, 21.853595769032836, 17.71262988448143, 21.220559786073864, 20.71647239336744, 18.728497486095876, 24.278091906104237, 19.30503193102777, 14.179936722386628, 15.847283244132996, 18.930993634741753, 25.453715721610934, 24.5005402402021, 17.470066737383604, 22.643102216999978, 15.923135004006326, 22.170676514972, 21.89880469068885, 18.11504234513268, 15.953475480433553, 19.251193278934807, 30.252876022364944, 21.102300819009542, 12.024477724451572, 20.924462964758277, 19.180164713412523, 15.244787043891847, 16.871341002173722, 21.66976546589285, 20.588592275045812, 24.894771623425186, 15.866067653987557, 17.186787031125277, 19.38650817517191, 19.246893578674644, 19.916550557594746, 24.327800136990845, 24.294737607706338, 18.74013505410403, 13.633913539815694, 20.385753616224974, 17.14964487729594, 16.075563682243228, 24.066006793174893, 27.50272659258917, 20.998324641492218, 21.781952116638422, 13.345890897326171, 17.22067857114598, 20.162287706974894, 23.28083809837699, 19.884316817857325, 18.999821017496288, 16.177752176765352, 22.227262045256793, 22.952505866996944, 17.366788872051984, 21.764996321871877, 17.21211287844926, 23.093068353831768, 30.91145718842745, 26.29660627990961, 13.749480953905731, 14.01242743525654, 21.680959386285394, 14.4650087072514, 23.6086034742184, 20.126219552010298, 17.724287246353924, 16.699863084126264, 16.637637444771826, 15.215626585762948, 22.187149139121175, 21.630610153079033, 23.876952377147973, 19.400417258497328, 12.225089001469314, 20.75129406992346, 23.324686428066343, 18.090573902241886, 22.81343139708042, 32.18355860887095, 21.445867711212486, 19.335496935993433, 16.324010542128235, 27.108178596477956, 18.09396632341668, 17.351458464749157, 18.046579798683524, 20.154126103036106, 14.870888200122863, 20.260070017073303, 21.155340200290084, 16.948817058000714, 22.045716907363385, 24.83383413637057, 14.269022419117391, 31.46001410903409, 19.566723765805364, 15.957363259978592, 22.281151696573943, 22.135638982988894, 23.711386263836175, 19.29900627443567, 24.919289596378803, 22.656436332967132, 18.828260931186378, 19.910969718825072, 30.90931941336021, 26.69222709396854, 16.682649926282465, 24.58038711035624, 21.83862675121054, 23.96883190656081, 19.914594990666956, 25.511508886236697, 30.150516789406538, 15.539506021887064, 23.412755919154733, 26.872315236367285, 22.94010193971917, 12.013031154870987, 18.202793973032385, 23.727435539010912]
Total Epoch List: [66, 40, 41]
Total Time List: [6.070481457281858, 4.2001661183312535, 5.229127608705312]
========================training times:2========================
========================k_idx:0========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
emb dim:  64
Model: Ethident
Ethident(
  (encoder): HGATE_encoder(
    (lin1): Linear(in_features=14887, out_features=64, bias=True)
    (account_convs): ModuleList(
      (0): GATEConv()
      (1): GATConv(64, 64, heads=1)
    )
    (subgraph_conv): GATConv(64, 64, heads=1)
    (lin2): Linear(in_features=64, out_features=64, bias=True)
    (lin3): Linear(in_features=64, out_features=2, bias=True)
  )
  (proj_head_g1): Project_Head(
    (block): Sequential(
      (0): Linear(in_features=64, out_features=64, bias=True)
      (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=64, bias=True)
      (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
      (5): ReLU()
    )
    (linear_shortcut): Linear(in_features=64, out_features=64, bias=True)
  )
  (proj_head_g2): Project_Head(
    (block): Sequential(
      (0): Linear(in_features=64, out_features=64, bias=True)
      (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=64, bias=True)
      (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
      (5): ReLU()
    )
    (linear_shortcut): Linear(in_features=64, out_features=64, bias=True)
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=64, bias=True)
    (1): ReLU()
    (2): Dropout(p=0.0, inplace=True)
    (3): Linear(in_features=64, out_features=2, bias=True)
  )
)
Total number of parameters: 1003972
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x779895df8070>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.7789;  Loss pred: 0.7504; Loss self: 2.8483; time: 7.85s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7499 score: 0.5102 time: 2.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7395 score: 0.4898 time: 3.09s
Epoch 2/1000, LR 0.000015
Train loss: 0.7690;  Loss pred: 0.7405; Loss self: 2.8471; time: 16.63s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6906 score: 0.5102 time: 5.76s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6999 score: 0.4898 time: 5.13s
Epoch 3/1000, LR 0.000045
Train loss: 0.7097;  Loss pred: 0.6812; Loss self: 2.8459; time: 10.17s
Val loss: 0.6392 score: 0.6122 time: 5.43s
Test loss: 0.6606 score: 0.5306 time: 4.11s
Epoch 4/1000, LR 0.000075
Train loss: 0.6587;  Loss pred: 0.6304; Loss self: 2.8299; time: 9.57s
Val loss: 0.5941 score: 0.7551 time: 3.86s
Test loss: 0.6197 score: 0.7755 time: 6.19s
Epoch 5/1000, LR 0.000105
Train loss: 0.6130;  Loss pred: 0.5851; Loss self: 2.7931; time: 13.61s
Val loss: 0.5669 score: 0.7755 time: 4.45s
Test loss: 0.5964 score: 0.8367 time: 4.17s
Epoch 6/1000, LR 0.000135
Train loss: 0.5739;  Loss pred: 0.5464; Loss self: 2.7518; time: 5.94s
Val loss: 0.5516 score: 0.7959 time: 4.69s
Test loss: 0.5896 score: 0.8571 time: 6.21s
Epoch 7/1000, LR 0.000165
Train loss: 0.5468;  Loss pred: 0.5198; Loss self: 2.7037; time: 12.13s
Val loss: 0.5322 score: 0.7959 time: 4.54s
Test loss: 0.5715 score: 0.8163 time: 5.52s
Epoch 8/1000, LR 0.000195
Train loss: 0.5275;  Loss pred: 0.5012; Loss self: 2.6247; time: 11.31s
Val loss: 0.4975 score: 0.8367 time: 5.07s
Test loss: 0.5331 score: 0.8367 time: 4.90s
Epoch 9/1000, LR 0.000225
Train loss: 0.4845;  Loss pred: 0.4591; Loss self: 2.5438; time: 10.58s
Val loss: 0.4598 score: 0.8367 time: 5.02s
Test loss: 0.5007 score: 0.8367 time: 1.87s
Epoch 10/1000, LR 0.000255
Train loss: 0.4338;  Loss pred: 0.4092; Loss self: 2.4592; time: 16.90s
Val loss: 0.4235 score: 0.8367 time: 9.40s
Test loss: 0.4807 score: 0.8367 time: 7.21s
Epoch 11/1000, LR 0.000285
Train loss: 0.3954;  Loss pred: 0.3714; Loss self: 2.3990; time: 8.82s
Val loss: 0.4001 score: 0.8571 time: 4.42s
Test loss: 0.4490 score: 0.8367 time: 4.85s
Epoch 12/1000, LR 0.000285
Train loss: 0.3647;  Loss pred: 0.3411; Loss self: 2.3579; time: 9.66s
Val loss: 0.3839 score: 0.8571 time: 0.95s
Test loss: 0.4334 score: 0.8367 time: 5.72s
Epoch 13/1000, LR 0.000285
Train loss: 0.3415;  Loss pred: 0.3179; Loss self: 2.3654; time: 14.95s
Val loss: 0.3613 score: 0.8571 time: 4.63s
Test loss: 0.4164 score: 0.8367 time: 4.31s
Epoch 14/1000, LR 0.000285
Train loss: 0.3125;  Loss pred: 0.2885; Loss self: 2.3996; time: 13.48s
Val loss: 0.3425 score: 0.8776 time: 0.19s
Test loss: 0.3985 score: 0.8367 time: 6.29s
Epoch 15/1000, LR 0.000285
Train loss: 0.2878;  Loss pred: 0.2636; Loss self: 2.4186; time: 16.01s
Val loss: 0.3257 score: 0.8776 time: 5.47s
Test loss: 0.3825 score: 0.8367 time: 6.37s
Epoch 16/1000, LR 0.000285
Train loss: 0.2663;  Loss pred: 0.2420; Loss self: 2.4239; time: 9.58s
Val loss: 0.3103 score: 0.8980 time: 5.04s
Test loss: 0.3685 score: 0.8776 time: 4.87s
Epoch 17/1000, LR 0.000285
Train loss: 0.2493;  Loss pred: 0.2251; Loss self: 2.4255; time: 10.36s
Val loss: 0.3009 score: 0.8980 time: 4.49s
Test loss: 0.3589 score: 0.8776 time: 7.11s
Epoch 18/1000, LR 0.000285
Train loss: 0.2352;  Loss pred: 0.2111; Loss self: 2.4170; time: 11.91s
Val loss: 0.2962 score: 0.8980 time: 4.75s
Test loss: 0.3566 score: 0.8776 time: 4.53s
Epoch 19/1000, LR 0.000285
Train loss: 0.2247;  Loss pred: 0.2007; Loss self: 2.4056; time: 10.47s
Val loss: 0.2900 score: 0.8980 time: 5.15s
Test loss: 0.3536 score: 0.8776 time: 3.03s
Epoch 20/1000, LR 0.000285
Train loss: 0.2148;  Loss pred: 0.1909; Loss self: 2.3970; time: 13.54s
Val loss: 0.2806 score: 0.8980 time: 3.10s
Test loss: 0.3476 score: 0.8776 time: 5.72s
Epoch 21/1000, LR 0.000285
Train loss: 0.2053;  Loss pred: 0.1814; Loss self: 2.3895; time: 13.77s
Val loss: 0.2719 score: 0.8980 time: 4.69s
Test loss: 0.3419 score: 0.8776 time: 7.20s
Epoch 22/1000, LR 0.000285
Train loss: 0.1972;  Loss pred: 0.1733; Loss self: 2.3835; time: 10.70s
Val loss: 0.2659 score: 0.8980 time: 4.70s
Test loss: 0.3391 score: 0.8776 time: 5.85s
Epoch 23/1000, LR 0.000285
Train loss: 0.1886;  Loss pred: 0.1648; Loss self: 2.3758; time: 14.09s
Val loss: 0.2637 score: 0.8980 time: 5.03s
Test loss: 0.3404 score: 0.8776 time: 3.51s
Epoch 24/1000, LR 0.000285
Train loss: 0.1800;  Loss pred: 0.1564; Loss self: 2.3628; time: 10.78s
Val loss: 0.2625 score: 0.8980 time: 6.69s
Test loss: 0.3466 score: 0.8571 time: 4.63s
Epoch 25/1000, LR 0.000285
Train loss: 0.1716;  Loss pred: 0.1481; Loss self: 2.3466; time: 15.84s
Val loss: 0.2578 score: 0.8980 time: 4.77s
Test loss: 0.3492 score: 0.8571 time: 3.79s
Epoch 26/1000, LR 0.000285
Train loss: 0.1628;  Loss pred: 0.1394; Loss self: 2.3344; time: 14.40s
Val loss: 0.2472 score: 0.8980 time: 5.57s
Test loss: 0.3470 score: 0.8571 time: 3.75s
Epoch 27/1000, LR 0.000285
Train loss: 0.1530;  Loss pred: 0.1297; Loss self: 2.3292; time: 11.28s
Val loss: 0.2358 score: 0.8980 time: 3.71s
Test loss: 0.3420 score: 0.8776 time: 4.35s
Epoch 28/1000, LR 0.000285
Train loss: 0.1425;  Loss pred: 0.1192; Loss self: 2.3293; time: 13.32s
Val loss: 0.2264 score: 0.8980 time: 4.73s
Test loss: 0.3379 score: 0.8776 time: 4.37s
Epoch 29/1000, LR 0.000285
Train loss: 0.1333;  Loss pred: 0.1100; Loss self: 2.3270; time: 13.46s
Val loss: 0.2202 score: 0.8980 time: 9.20s
Test loss: 0.3384 score: 0.8776 time: 6.50s
Epoch 30/1000, LR 0.000285
Train loss: 0.1252;  Loss pred: 0.1020; Loss self: 2.3226; time: 14.27s
Val loss: 0.2183 score: 0.8980 time: 3.73s
Test loss: 0.3439 score: 0.8776 time: 0.88s
Epoch 31/1000, LR 0.000285
Train loss: 0.1184;  Loss pred: 0.0952; Loss self: 2.3170; time: 15.67s
Val loss: 0.2169 score: 0.8980 time: 7.30s
Test loss: 0.3518 score: 0.8980 time: 4.23s
Epoch 32/1000, LR 0.000285
Train loss: 0.1128;  Loss pred: 0.0897; Loss self: 2.3120; time: 1.84s
Val loss: 0.2130 score: 0.8980 time: 4.18s
Test loss: 0.3542 score: 0.8980 time: 5.28s
Epoch 33/1000, LR 0.000285
Train loss: 0.1073;  Loss pred: 0.0842; Loss self: 2.3130; time: 6.54s
Val loss: 0.2093 score: 0.8980 time: 4.26s
Test loss: 0.3532 score: 0.8980 time: 3.62s
Epoch 34/1000, LR 0.000285
Train loss: 0.1018;  Loss pred: 0.0786; Loss self: 2.3221; time: 14.38s
Val loss: 0.2067 score: 0.8980 time: 3.27s
Test loss: 0.3516 score: 0.8980 time: 1.34s
Epoch 35/1000, LR 0.000285
Train loss: 0.0959;  Loss pred: 0.0725; Loss self: 2.3347; time: 8.31s
Val loss: 0.2041 score: 0.8980 time: 3.96s
Test loss: 0.3509 score: 0.8980 time: 3.97s
Epoch 36/1000, LR 0.000285
Train loss: 0.0907;  Loss pred: 0.0672; Loss self: 2.3483; time: 10.81s
Val loss: 0.2016 score: 0.8980 time: 4.48s
Test loss: 0.3508 score: 0.8980 time: 3.39s
Epoch 37/1000, LR 0.000285
Train loss: 0.0856;  Loss pred: 0.0620; Loss self: 2.3604; time: 4.54s
Val loss: 0.1997 score: 0.8980 time: 5.13s
Test loss: 0.3514 score: 0.8980 time: 6.73s
Epoch 38/1000, LR 0.000284
Train loss: 0.0810;  Loss pred: 0.0573; Loss self: 2.3714; time: 12.84s
Val loss: 0.1965 score: 0.8980 time: 6.68s
Test loss: 0.3526 score: 0.8980 time: 4.21s
Epoch 39/1000, LR 0.000284
Train loss: 0.0767;  Loss pred: 0.0529; Loss self: 2.3800; time: 6.23s
Val loss: 0.1933 score: 0.9184 time: 3.93s
Test loss: 0.3541 score: 0.8980 time: 4.51s
Epoch 40/1000, LR 0.000284
Train loss: 0.0724;  Loss pred: 0.0486; Loss self: 2.3853; time: 11.11s
Val loss: 0.1897 score: 0.9184 time: 2.96s
Test loss: 0.3536 score: 0.8980 time: 4.67s
Epoch 41/1000, LR 0.000284
Train loss: 0.0679;  Loss pred: 0.0440; Loss self: 2.3884; time: 12.15s
Val loss: 0.1871 score: 0.9184 time: 3.33s
Test loss: 0.3534 score: 0.8980 time: 3.80s
Epoch 42/1000, LR 0.000284
Train loss: 0.0635;  Loss pred: 0.0396; Loss self: 2.3900; time: 17.10s
Val loss: 0.1848 score: 0.9184 time: 5.69s
Test loss: 0.3539 score: 0.8980 time: 6.84s
Epoch 43/1000, LR 0.000284
Train loss: 0.0597;  Loss pred: 0.0358; Loss self: 2.3908; time: 15.57s
Val loss: 0.1856 score: 0.9184 time: 1.87s
Test loss: 0.3554 score: 0.8980 time: 0.09s
     INFO: Early stopping counter 1 of 20
Epoch 44/1000, LR 0.000284
Train loss: 0.0562;  Loss pred: 0.0323; Loss self: 2.3911; time: 11.06s
Val loss: 0.1884 score: 0.9184 time: 4.89s
Test loss: 0.3575 score: 0.8776 time: 5.53s
     INFO: Early stopping counter 2 of 20
Epoch 45/1000, LR 0.000284
Train loss: 0.0529;  Loss pred: 0.0290; Loss self: 2.3917; time: 12.65s
Val loss: 0.1927 score: 0.9184 time: 4.44s
Test loss: 0.3607 score: 0.8776 time: 3.39s
     INFO: Early stopping counter 3 of 20
Epoch 46/1000, LR 0.000284
Train loss: 0.0502;  Loss pred: 0.0262; Loss self: 2.3919; time: 11.90s
Val loss: 0.1961 score: 0.9184 time: 5.71s
Test loss: 0.3636 score: 0.8776 time: 5.16s
     INFO: Early stopping counter 4 of 20
Epoch 47/1000, LR 0.000284
Train loss: 0.0478;  Loss pred: 0.0238; Loss self: 2.3922; time: 9.49s
Val loss: 0.1983 score: 0.9184 time: 4.52s
Test loss: 0.3664 score: 0.8776 time: 5.52s
     INFO: Early stopping counter 5 of 20
Epoch 48/1000, LR 0.000284
Train loss: 0.0456;  Loss pred: 0.0217; Loss self: 2.3931; time: 14.90s
Val loss: 0.1992 score: 0.9184 time: 4.94s
Test loss: 0.3687 score: 0.8776 time: 6.72s
     INFO: Early stopping counter 6 of 20
Epoch 49/1000, LR 0.000284
Train loss: 0.0438;  Loss pred: 0.0199; Loss self: 2.3948; time: 15.87s
Val loss: 0.1995 score: 0.9184 time: 4.95s
Test loss: 0.3707 score: 0.8776 time: 4.46s
     INFO: Early stopping counter 7 of 20
Epoch 50/1000, LR 0.000284
Train loss: 0.0422;  Loss pred: 0.0182; Loss self: 2.3965; time: 20.83s
Val loss: 0.1998 score: 0.9184 time: 5.89s
Test loss: 0.3728 score: 0.8776 time: 4.01s
     INFO: Early stopping counter 8 of 20
Epoch 51/1000, LR 0.000284
Train loss: 0.0408;  Loss pred: 0.0168; Loss self: 2.3985; time: 7.96s
Val loss: 0.2006 score: 0.9184 time: 3.90s
Test loss: 0.3762 score: 0.8776 time: 4.67s
     INFO: Early stopping counter 9 of 20
Epoch 52/1000, LR 0.000284
Train loss: 0.0396;  Loss pred: 0.0156; Loss self: 2.4000; time: 14.17s
Val loss: 0.2018 score: 0.9184 time: 3.89s
Test loss: 0.3803 score: 0.8776 time: 4.23s
     INFO: Early stopping counter 10 of 20
Epoch 53/1000, LR 0.000284
Train loss: 0.0385;  Loss pred: 0.0145; Loss self: 2.4001; time: 9.47s
Val loss: 0.2028 score: 0.9184 time: 5.54s
Test loss: 0.3845 score: 0.8776 time: 4.72s
     INFO: Early stopping counter 11 of 20
Epoch 54/1000, LR 0.000284
Train loss: 0.0375;  Loss pred: 0.0135; Loss self: 2.3994; time: 14.23s
Val loss: 0.2036 score: 0.9184 time: 5.96s
Test loss: 0.3883 score: 0.8776 time: 4.53s
     INFO: Early stopping counter 12 of 20
Epoch 55/1000, LR 0.000284
Train loss: 0.0366;  Loss pred: 0.0127; Loss self: 2.3988; time: 9.05s
Val loss: 0.2040 score: 0.9184 time: 5.56s
Test loss: 0.3912 score: 0.8571 time: 1.16s
     INFO: Early stopping counter 13 of 20
Epoch 56/1000, LR 0.000284
Train loss: 0.0358;  Loss pred: 0.0118; Loss self: 2.3987; time: 7.68s
Val loss: 0.2037 score: 0.9184 time: 3.14s
Test loss: 0.3929 score: 0.8571 time: 0.09s
     INFO: Early stopping counter 14 of 20
Epoch 57/1000, LR 0.000283
Train loss: 0.0351;  Loss pred: 0.0111; Loss self: 2.3993; time: 8.58s
Val loss: 0.2031 score: 0.9184 time: 3.13s
Test loss: 0.3942 score: 0.8571 time: 3.56s
     INFO: Early stopping counter 15 of 20
Epoch 58/1000, LR 0.000283
Train loss: 0.0345;  Loss pred: 0.0105; Loss self: 2.3999; time: 12.22s
Val loss: 0.2034 score: 0.9184 time: 5.01s
Test loss: 0.3962 score: 0.8571 time: 1.87s
     INFO: Early stopping counter 16 of 20
Epoch 59/1000, LR 0.000283
Train loss: 0.0339;  Loss pred: 0.0099; Loss self: 2.4004; time: 13.98s
Val loss: 0.2049 score: 0.9184 time: 4.98s
Test loss: 0.3991 score: 0.8571 time: 5.07s
     INFO: Early stopping counter 17 of 20
Epoch 60/1000, LR 0.000283
Train loss: 0.0334;  Loss pred: 0.0094; Loss self: 2.4004; time: 12.34s
Val loss: 0.2070 score: 0.9184 time: 2.41s
Test loss: 0.4024 score: 0.8571 time: 2.50s
     INFO: Early stopping counter 18 of 20
Epoch 61/1000, LR 0.000283
Train loss: 0.0329;  Loss pred: 0.0089; Loss self: 2.3998; time: 13.10s
Val loss: 0.2094 score: 0.9184 time: 4.27s
Test loss: 0.4061 score: 0.8571 time: 6.05s
     INFO: Early stopping counter 19 of 20
Epoch 62/1000, LR 0.000283
Train loss: 0.0325;  Loss pred: 0.0085; Loss self: 2.3987; time: 14.24s
Val loss: 0.2118 score: 0.9184 time: 3.79s
Test loss: 0.4095 score: 0.8571 time: 1.94s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 041,   Train_Loss: 0.0635,   Val_Loss: 0.1848,   Val_Precision: 0.9545,   Val_Recall: 0.8750,   Val_accuracy: 0.9130,   Val_Score: 0.9184,   Val_Loss: 0.1848,   Test_Precision: 1.0000,   Test_Recall: 0.8000,   Test_accuracy: 0.8889,   Test_Score: 0.8980,   Test_loss: 0.3539


[3.093264745082706, 5.140389317180961, 4.119556996971369, 6.19796695606783, 4.171529560815543, 6.214615265373141, 5.531291096005589, 4.911078346893191, 1.8754304433241487, 7.21969572128728, 4.858853172976524, 5.726258125156164, 4.311443901155144, 6.314448815304786, 6.399416361004114, 4.875978766940534, 7.120634972117841, 4.533011775929481, 3.0319977030158043, 5.732754092197865, 7.206918265670538, 5.85536127910018, 3.522882784716785, 4.634792665950954, 3.7949008918367326, 3.75464361673221, 4.352745843119919, 4.38002095092088, 6.510121461004019, 0.8826986872591078, 4.2349121007137, 5.288178241811693, 3.628614810295403, 1.343225555960089, 3.9733598427847028, 3.3945001419633627, 6.733690628781915, 4.219065522309393, 4.524024287238717, 4.6733842282556, 3.8118273126892745, 6.85236780392006, 0.0967163648456335, 5.540779908187687, 3.396784577984363, 5.178473813924938, 5.52381440019235, 6.7386662675999105, 4.4692771239206195, 4.014628142118454, 4.678608165122569, 4.241853108163923, 4.727146525867283, 4.53322677500546, 1.1650699987076223, 0.09809994092211127, 3.567427479662001, 1.8741945922374725, 5.0805041808635, 2.506158254109323, 6.056329941842705, 1.9422961347736418]
[0.06312785194046339, 0.10490590443226452, 0.08407259177492589, 0.1264891215524047, 0.08513325634317434, 0.12682888296679878, 0.1128834917552161, 0.10022608871210595, 0.038274090680084666, 0.14734072900586287, 0.09916026883625559, 0.11686241071747273, 0.08798865104398253, 0.1288663023531589, 0.13060033389804315, 0.09950977075388845, 0.14531908106362942, 0.0925104444067241, 0.061877504143179676, 0.11699498147342582, 0.1470799646055212, 0.11949716896122815, 0.07189556703503643, 0.09458760542757048, 0.07744695697625985, 0.07662537993331041, 0.08883154781877386, 0.0893881826718547, 0.13285962165314325, 0.01801425892365526, 0.08642677756558571, 0.10792200493493251, 0.07405336347541638, 0.02741276644816508, 0.08108897638336128, 0.06927551310129311, 0.13742225773024316, 0.08610337800631414, 0.0923270262701779, 0.09537518833174694, 0.07779239413651581, 0.13984424089632777, 0.0019738033641966022, 0.1130771409834222, 0.06932213424457884, 0.10568313905969262, 0.11273090612637449, 0.13752380137959, 0.09120973722286979, 0.081931186573846, 0.09548179928821568, 0.08656843077885557, 0.09647237807892416, 0.09251483214296857, 0.023776938749135147, 0.002002039610655332, 0.07280464244208165, 0.03824886922933617, 0.10368375879313264, 0.05114608681855761, 0.12359857024168786, 0.03963869662803351]
[15.8408684797815, 9.532351924439853, 11.894482837844944, 7.905818205763236, 11.746290967292198, 7.884639339303963, 8.858691243963865, 9.977442129588098, 26.127335286905577, 6.786989631089777, 10.08468423629741, 8.557071464301776, 11.365102068676267, 7.759980551467162, 7.656948264624487, 10.04926443327098, 6.881408777709928, 10.80959027289372, 16.160962111304272, 8.547375172901237, 6.799022577154342, 8.368399090060938, 13.909063399036496, 10.5722097042169, 12.912063159647891, 13.050506253545926, 11.257261913752872, 11.187161100153634, 7.52674128946943, 55.511581366628356, 11.570488084449767, 9.265950911520891, 13.503775562226442, 36.47935358479431, 12.332132487063813, 14.435115024522766, 7.2768415867753955, 11.61394620228103, 10.831064753170818, 10.484907212153168, 12.854727137528723, 7.150812887184541, 506.63608044210133, 8.843520372933785, 14.425406991536798, 9.462247326275705, 8.870681824192667, 7.271468574663837, 10.963741706178949, 12.205364548195364, 10.473200206266112, 11.551555122381298, 10.36566134175628, 10.809077602331286, 42.0575588199458, 499.49061680786014, 13.735387833207627, 26.14456375178324, 9.644712071011778, 19.551837925500188, 8.090708477004014, 25.22787288855442]
Elapsed: 4.425514657320755~1.6819778402583987
Time per graph: 0.09031662565960726~0.03432607837262038
Speed: 28.857123989039344~86.988326454648
Total Time: 1.9428
best val loss: 0.18479473189431794 test_score: 0.8980

Testing...
Test loss: 0.3541 score: 0.8980 time: 1.67s
test Score 0.8980
Epoch Time List: [12.976028454490006, 27.525766979902983, 19.712311922106892, 19.618266489822417, 22.230121315456927, 16.839620692655444, 22.186089975293726, 21.27933909650892, 17.45930476207286, 33.51027862681076, 18.098223652224988, 16.336218813434243, 23.881507258396596, 19.960599166341126, 27.845399655867368, 19.48496939614415, 21.962667694315314, 21.180531315039843, 18.642449965700507, 22.371735385619104, 25.655681800097227, 21.25370108569041, 22.634155001025647, 22.09314061468467, 24.3926573083736, 23.708388439379632, 19.333353190217167, 22.418254145421088, 29.167838055174798, 18.87651361571625, 27.200405968818814, 11.304323479533195, 14.428930562455207, 18.98376740468666, 16.23355607315898, 18.686598322354257, 16.398404289968312, 23.736930582206696, 14.679029861930758, 18.732816922012717, 19.276872029993683, 29.632624155841768, 17.52925050072372, 21.477861462626606, 20.479256246704608, 22.768225857988, 19.528184918221086, 26.563120521139354, 25.28922134079039, 30.72886284533888, 16.538651692215353, 22.300484546925873, 19.735936036799103, 24.721082744188607, 15.764562676660717, 10.919303630944341, 15.27238003630191, 19.09901731973514, 24.026303513906896, 17.253326115664095, 23.419697400182486, 19.97273503569886]
Total Epoch List: [62]
Total Time List: [1.942754534073174]
========================k_idx:1========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
emb dim:  64
Model: Ethident
Ethident(
  (encoder): HGATE_encoder(
    (lin1): Linear(in_features=14887, out_features=64, bias=True)
    (account_convs): ModuleList(
      (0): GATEConv()
      (1): GATConv(64, 64, heads=1)
    )
    (subgraph_conv): GATConv(64, 64, heads=1)
    (lin2): Linear(in_features=64, out_features=64, bias=True)
    (lin3): Linear(in_features=64, out_features=2, bias=True)
  )
  (proj_head_g1): Project_Head(
    (block): Sequential(
      (0): Linear(in_features=64, out_features=64, bias=True)
      (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=64, bias=True)
      (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
      (5): ReLU()
    )
    (linear_shortcut): Linear(in_features=64, out_features=64, bias=True)
  )
  (proj_head_g2): Project_Head(
    (block): Sequential(
      (0): Linear(in_features=64, out_features=64, bias=True)
      (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=64, bias=True)
      (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
      (5): ReLU()
    )
    (linear_shortcut): Linear(in_features=64, out_features=64, bias=True)
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=64, bias=True)
    (1): ReLU()
    (2): Dropout(p=0.0, inplace=True)
    (3): Linear(in_features=64, out_features=2, bias=True)
  )
)
Total number of parameters: 1003972
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7798933d84c0>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.7245;  Loss pred: 0.6964; Loss self: 2.8115; time: 6.83s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7286 score: 0.5102 time: 4.72s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7265 score: 0.4898 time: 3.23s
Epoch 2/1000, LR 0.000015
Train loss: 0.6842;  Loss pred: 0.6562; Loss self: 2.8056; time: 5.41s
Val loss: 0.7005 score: 0.4898 time: 3.77s
Test loss: 0.6719 score: 0.5102 time: 4.46s
Epoch 3/1000, LR 0.000045
Train loss: 0.6193;  Loss pred: 0.5915; Loss self: 2.7840; time: 5.93s
Val loss: 0.6640 score: 0.6531 time: 5.11s
Test loss: 0.6220 score: 0.7143 time: 2.37s
Epoch 4/1000, LR 0.000075
Train loss: 0.5842;  Loss pred: 0.5569; Loss self: 2.7305; time: 5.31s
Val loss: 0.6299 score: 0.6531 time: 5.24s
Test loss: 0.5587 score: 0.7959 time: 5.21s
Epoch 5/1000, LR 0.000105
Train loss: 0.5325;  Loss pred: 0.5055; Loss self: 2.6975; time: 7.96s
Val loss: 0.6007 score: 0.6531 time: 6.12s
Test loss: 0.5222 score: 0.7551 time: 3.66s
Epoch 6/1000, LR 0.000135
Train loss: 0.4899;  Loss pred: 0.4630; Loss self: 2.6876; time: 27.02s
Val loss: 0.5814 score: 0.6735 time: 5.20s
Test loss: 0.4890 score: 0.7959 time: 3.14s
Epoch 7/1000, LR 0.000165
Train loss: 0.4535;  Loss pred: 0.4264; Loss self: 2.7143; time: 15.18s
Val loss: 0.5642 score: 0.7347 time: 5.54s
Test loss: 0.4573 score: 0.8571 time: 4.78s
Epoch 8/1000, LR 0.000195
Train loss: 0.4143;  Loss pred: 0.3869; Loss self: 2.7387; time: 8.70s
Val loss: 0.5696 score: 0.7347 time: 2.35s
Test loss: 0.4409 score: 0.8776 time: 2.96s
     INFO: Early stopping counter 1 of 20
Epoch 9/1000, LR 0.000225
Train loss: 0.4095;  Loss pred: 0.3820; Loss self: 2.7480; time: 10.99s
Val loss: 0.5566 score: 0.7347 time: 0.87s
Test loss: 0.4124 score: 0.8776 time: 3.45s
Epoch 10/1000, LR 0.000255
Train loss: 0.3800;  Loss pred: 0.3529; Loss self: 2.7157; time: 7.12s
Val loss: 0.5202 score: 0.8163 time: 2.71s
Test loss: 0.3726 score: 0.8776 time: 6.72s
Epoch 11/1000, LR 0.000285
Train loss: 0.3193;  Loss pred: 0.2927; Loss self: 2.6573; time: 8.93s
Val loss: 0.5209 score: 0.8163 time: 4.31s
Test loss: 0.3530 score: 0.8980 time: 3.76s
     INFO: Early stopping counter 1 of 20
Epoch 12/1000, LR 0.000285
Train loss: 0.2800;  Loss pred: 0.2538; Loss self: 2.6236; time: 7.09s
Val loss: 0.5400 score: 0.7551 time: 4.32s
Test loss: 0.3422 score: 0.8776 time: 0.10s
     INFO: Early stopping counter 2 of 20
Epoch 13/1000, LR 0.000285
Train loss: 0.2645;  Loss pred: 0.2385; Loss self: 2.6000; time: 17.64s
Val loss: 0.5423 score: 0.7755 time: 5.77s
Test loss: 0.3309 score: 0.8980 time: 4.88s
     INFO: Early stopping counter 3 of 20
Epoch 14/1000, LR 0.000285
Train loss: 0.2457;  Loss pred: 0.2199; Loss self: 2.5793; time: 15.38s
Val loss: 0.5195 score: 0.8163 time: 4.19s
Test loss: 0.3148 score: 0.8980 time: 4.89s
Epoch 15/1000, LR 0.000285
Train loss: 0.2133;  Loss pred: 0.1877; Loss self: 2.5593; time: 8.38s
Val loss: 0.5076 score: 0.8163 time: 2.49s
Test loss: 0.3068 score: 0.8571 time: 5.37s
Epoch 16/1000, LR 0.000285
Train loss: 0.1870;  Loss pred: 0.1615; Loss self: 2.5496; time: 8.63s
Val loss: 0.5118 score: 0.8163 time: 1.64s
Test loss: 0.2974 score: 0.8776 time: 1.89s
     INFO: Early stopping counter 1 of 20
Epoch 17/1000, LR 0.000285
Train loss: 0.1675;  Loss pred: 0.1421; Loss self: 2.5410; time: 14.16s
Val loss: 0.5249 score: 0.8163 time: 4.98s
Test loss: 0.2874 score: 0.9184 time: 2.87s
     INFO: Early stopping counter 2 of 20
Epoch 18/1000, LR 0.000285
Train loss: 0.1552;  Loss pred: 0.1299; Loss self: 2.5298; time: 15.59s
Val loss: 0.5351 score: 0.8163 time: 3.19s
Test loss: 0.2822 score: 0.9184 time: 0.75s
     INFO: Early stopping counter 3 of 20
Epoch 19/1000, LR 0.000285
Train loss: 0.1460;  Loss pred: 0.1208; Loss self: 2.5212; time: 7.39s
Val loss: 0.5313 score: 0.8163 time: 6.33s
Test loss: 0.2797 score: 0.9184 time: 6.04s
     INFO: Early stopping counter 4 of 20
Epoch 20/1000, LR 0.000285
Train loss: 0.1344;  Loss pred: 0.1092; Loss self: 2.5194; time: 15.76s
Val loss: 0.5189 score: 0.8163 time: 5.20s
Test loss: 0.2791 score: 0.9184 time: 4.37s
     INFO: Early stopping counter 5 of 20
Epoch 21/1000, LR 0.000285
Train loss: 0.1227;  Loss pred: 0.0975; Loss self: 2.5258; time: 2.06s
Val loss: 0.5127 score: 0.8163 time: 1.28s
Test loss: 0.2790 score: 0.8980 time: 1.09s
     INFO: Early stopping counter 6 of 20
Epoch 22/1000, LR 0.000285
Train loss: 0.1128;  Loss pred: 0.0874; Loss self: 2.5332; time: 7.54s
Val loss: 0.5108 score: 0.8163 time: 1.58s
Test loss: 0.2747 score: 0.8980 time: 3.80s
     INFO: Early stopping counter 7 of 20
Epoch 23/1000, LR 0.000285
Train loss: 0.1032;  Loss pred: 0.0778; Loss self: 2.5393; time: 15.15s
Val loss: 0.5108 score: 0.8163 time: 0.10s
Test loss: 0.2697 score: 0.9184 time: 1.78s
     INFO: Early stopping counter 8 of 20
Epoch 24/1000, LR 0.000285
Train loss: 0.0949;  Loss pred: 0.0694; Loss self: 2.5435; time: 13.51s
Val loss: 0.5110 score: 0.8367 time: 1.98s
Test loss: 0.2657 score: 0.9184 time: 0.68s
     INFO: Early stopping counter 9 of 20
Epoch 25/1000, LR 0.000285
Train loss: 0.0877;  Loss pred: 0.0623; Loss self: 2.5459; time: 4.22s
Val loss: 0.5115 score: 0.8367 time: 0.09s
Test loss: 0.2638 score: 0.9184 time: 3.61s
     INFO: Early stopping counter 10 of 20
Epoch 26/1000, LR 0.000285
Train loss: 0.0817;  Loss pred: 0.0563; Loss self: 2.5472; time: 5.78s
Val loss: 0.5130 score: 0.8367 time: 5.79s
Test loss: 0.2634 score: 0.9184 time: 3.33s
     INFO: Early stopping counter 11 of 20
Epoch 27/1000, LR 0.000285
Train loss: 0.0769;  Loss pred: 0.0514; Loss self: 2.5468; time: 11.18s
Val loss: 0.5160 score: 0.8367 time: 7.53s
Test loss: 0.2631 score: 0.9184 time: 3.08s
     INFO: Early stopping counter 12 of 20
Epoch 28/1000, LR 0.000285
Train loss: 0.0726;  Loss pred: 0.0472; Loss self: 2.5461; time: 6.48s
Val loss: 0.5196 score: 0.8367 time: 0.83s
Test loss: 0.2631 score: 0.9184 time: 1.84s
     INFO: Early stopping counter 13 of 20
Epoch 29/1000, LR 0.000285
Train loss: 0.0687;  Loss pred: 0.0433; Loss self: 2.5449; time: 10.03s
Val loss: 0.5234 score: 0.8367 time: 4.66s
Test loss: 0.2629 score: 0.9184 time: 4.59s
     INFO: Early stopping counter 14 of 20
Epoch 30/1000, LR 0.000285
Train loss: 0.0652;  Loss pred: 0.0398; Loss self: 2.5441; time: 11.41s
Val loss: 0.5276 score: 0.8367 time: 1.88s
Test loss: 0.2624 score: 0.9184 time: 4.19s
     INFO: Early stopping counter 15 of 20
Epoch 31/1000, LR 0.000285
Train loss: 0.0622;  Loss pred: 0.0368; Loss self: 2.5434; time: 23.34s
Val loss: 0.5317 score: 0.8367 time: 6.60s
Test loss: 0.2619 score: 0.9184 time: 0.58s
     INFO: Early stopping counter 16 of 20
Epoch 32/1000, LR 0.000285
Train loss: 0.0596;  Loss pred: 0.0341; Loss self: 2.5428; time: 8.35s
Val loss: 0.5353 score: 0.8367 time: 1.98s
Test loss: 0.2616 score: 0.9184 time: 5.47s
     INFO: Early stopping counter 17 of 20
Epoch 33/1000, LR 0.000285
Train loss: 0.0571;  Loss pred: 0.0317; Loss self: 2.5427; time: 13.51s
Val loss: 0.5379 score: 0.8367 time: 2.52s
Test loss: 0.2620 score: 0.9184 time: 2.60s
     INFO: Early stopping counter 18 of 20
Epoch 34/1000, LR 0.000285
Train loss: 0.0550;  Loss pred: 0.0295; Loss self: 2.5434; time: 17.15s
Val loss: 0.5400 score: 0.8367 time: 3.05s
Test loss: 0.2631 score: 0.9184 time: 3.95s
     INFO: Early stopping counter 19 of 20
Epoch 35/1000, LR 0.000285
Train loss: 0.0531;  Loss pred: 0.0277; Loss self: 2.5444; time: 11.28s
Val loss: 0.5419 score: 0.8367 time: 3.86s
Test loss: 0.2644 score: 0.9184 time: 0.99s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 014,   Train_Loss: 0.2133,   Val_Loss: 0.5076,   Val_Precision: 0.9444,   Val_Recall: 0.6800,   Val_accuracy: 0.7907,   Val_Score: 0.8163,   Val_Loss: 0.5076,   Test_Precision: 0.8696,   Test_Recall: 0.8333,   Test_accuracy: 0.8511,   Test_Score: 0.8571,   Test_loss: 0.3068


[3.093264745082706, 5.140389317180961, 4.119556996971369, 6.19796695606783, 4.171529560815543, 6.214615265373141, 5.531291096005589, 4.911078346893191, 1.8754304433241487, 7.21969572128728, 4.858853172976524, 5.726258125156164, 4.311443901155144, 6.314448815304786, 6.399416361004114, 4.875978766940534, 7.120634972117841, 4.533011775929481, 3.0319977030158043, 5.732754092197865, 7.206918265670538, 5.85536127910018, 3.522882784716785, 4.634792665950954, 3.7949008918367326, 3.75464361673221, 4.352745843119919, 4.38002095092088, 6.510121461004019, 0.8826986872591078, 4.2349121007137, 5.288178241811693, 3.628614810295403, 1.343225555960089, 3.9733598427847028, 3.3945001419633627, 6.733690628781915, 4.219065522309393, 4.524024287238717, 4.6733842282556, 3.8118273126892745, 6.85236780392006, 0.0967163648456335, 5.540779908187687, 3.396784577984363, 5.178473813924938, 5.52381440019235, 6.7386662675999105, 4.4692771239206195, 4.014628142118454, 4.678608165122569, 4.241853108163923, 4.727146525867283, 4.53322677500546, 1.1650699987076223, 0.09809994092211127, 3.567427479662001, 1.8741945922374725, 5.0805041808635, 2.506158254109323, 6.056329941842705, 1.9422961347736418, 3.238424740731716, 4.470867450814694, 2.370878328103572, 5.215250143781304, 3.6767619140446186, 3.140701126307249, 4.785339723806828, 2.963048498146236, 3.458892971277237, 6.728299197740853, 3.768670762889087, 0.10541506018489599, 4.88765282696113, 4.903602860867977, 5.380005349870771, 1.8924045078456402, 2.874166732188314, 0.7598507250659168, 6.044045737944543, 4.379432374145836, 1.0970252277329564, 3.802893017884344, 1.7867413996718824, 0.6852059648372233, 3.6185475811362267, 3.348416324239224, 3.082258333917707, 1.845335004851222, 4.599632981233299, 4.201363826170564, 0.5808094632811844, 5.479593717027456, 2.6043737698346376, 3.9575369390659034, 0.995096658822149]
[0.06312785194046339, 0.10490590443226452, 0.08407259177492589, 0.1264891215524047, 0.08513325634317434, 0.12682888296679878, 0.1128834917552161, 0.10022608871210595, 0.038274090680084666, 0.14734072900586287, 0.09916026883625559, 0.11686241071747273, 0.08798865104398253, 0.1288663023531589, 0.13060033389804315, 0.09950977075388845, 0.14531908106362942, 0.0925104444067241, 0.061877504143179676, 0.11699498147342582, 0.1470799646055212, 0.11949716896122815, 0.07189556703503643, 0.09458760542757048, 0.07744695697625985, 0.07662537993331041, 0.08883154781877386, 0.0893881826718547, 0.13285962165314325, 0.01801425892365526, 0.08642677756558571, 0.10792200493493251, 0.07405336347541638, 0.02741276644816508, 0.08108897638336128, 0.06927551310129311, 0.13742225773024316, 0.08610337800631414, 0.0923270262701779, 0.09537518833174694, 0.07779239413651581, 0.13984424089632777, 0.0019738033641966022, 0.1130771409834222, 0.06932213424457884, 0.10568313905969262, 0.11273090612637449, 0.13752380137959, 0.09120973722286979, 0.081931186573846, 0.09548179928821568, 0.08656843077885557, 0.09647237807892416, 0.09251483214296857, 0.023776938749135147, 0.002002039610655332, 0.07280464244208165, 0.03824886922933617, 0.10368375879313264, 0.05114608681855761, 0.12359857024168786, 0.03963869662803351, 0.06609030083125951, 0.09124219287376927, 0.04838527200211372, 0.10643367640370009, 0.075035957429482, 0.06409594135320916, 0.09765999436340465, 0.06047037751318849, 0.07058965247504566, 0.13731222852532352, 0.07691164822222626, 0.0021513277588754284, 0.09974801687675776, 0.10007352777281586, 0.10979602754838308, 0.038620500160115104, 0.058656463922210494, 0.015507157654406465, 0.12334787220294986, 0.08937617090093541, 0.022388269953733803, 0.0776100615894764, 0.03646411019738535, 0.01398379520075966, 0.07384790981910666, 0.06833502702529029, 0.062903231304443, 0.0376598980581882, 0.09387006084149589, 0.08574211890144008, 0.011853254352677233, 0.11182844320464196, 0.05315048509866607, 0.0807660599809368, 0.020308095078003043]
[15.8408684797815, 9.532351924439853, 11.894482837844944, 7.905818205763236, 11.746290967292198, 7.884639339303963, 8.858691243963865, 9.977442129588098, 26.127335286905577, 6.786989631089777, 10.08468423629741, 8.557071464301776, 11.365102068676267, 7.759980551467162, 7.656948264624487, 10.04926443327098, 6.881408777709928, 10.80959027289372, 16.160962111304272, 8.547375172901237, 6.799022577154342, 8.368399090060938, 13.909063399036496, 10.5722097042169, 12.912063159647891, 13.050506253545926, 11.257261913752872, 11.187161100153634, 7.52674128946943, 55.511581366628356, 11.570488084449767, 9.265950911520891, 13.503775562226442, 36.47935358479431, 12.332132487063813, 14.435115024522766, 7.2768415867753955, 11.61394620228103, 10.831064753170818, 10.484907212153168, 12.854727137528723, 7.150812887184541, 506.63608044210133, 8.843520372933785, 14.425406991536798, 9.462247326275705, 8.870681824192667, 7.271468574663837, 10.963741706178949, 12.205364548195364, 10.473200206266112, 11.551555122381298, 10.36566134175628, 10.809077602331286, 42.0575588199458, 499.49061680786014, 13.735387833207627, 26.14456375178324, 9.644712071011778, 19.551837925500188, 8.090708477004014, 25.22787288855442, 15.13081325735195, 10.959841806777582, 20.66744607649028, 9.395522486764685, 13.326943964695717, 15.6016118788141, 10.239607390093418, 16.537022607174922, 14.16638225203776, 7.2826725684928855, 13.001931737448002, 464.82921808378177, 10.02526196822074, 9.992652625079554, 9.107797634657926, 25.89298418855639, 17.048419443186827, 64.48635025747889, 8.107152414876314, 11.188664606233806, 44.666247194023356, 12.884927282876886, 27.424226029014815, 71.51134478468876, 13.54134466973458, 14.633783632366274, 15.897434507937076, 26.55344415576757, 10.653023882540655, 11.662879490411154, 84.3650165807954, 8.94226881232744, 18.814503727362915, 12.381438443772419, 49.24144761776116]
Elapsed: 4.032066494807332~1.7488079956048825
Time per graph: 0.08228707132259862~0.035689959093977194
Speed: 30.714467168866324~83.28201792125702
Total Time: 0.9960
best val loss: 0.5075803776176608 test_score: 0.8571

Testing...
Test loss: 0.2657 score: 0.9184 time: 3.94s
test Score 0.9184
Epoch Time List: [12.976028454490006, 27.525766979902983, 19.712311922106892, 19.618266489822417, 22.230121315456927, 16.839620692655444, 22.186089975293726, 21.27933909650892, 17.45930476207286, 33.51027862681076, 18.098223652224988, 16.336218813434243, 23.881507258396596, 19.960599166341126, 27.845399655867368, 19.48496939614415, 21.962667694315314, 21.180531315039843, 18.642449965700507, 22.371735385619104, 25.655681800097227, 21.25370108569041, 22.634155001025647, 22.09314061468467, 24.3926573083736, 23.708388439379632, 19.333353190217167, 22.418254145421088, 29.167838055174798, 18.87651361571625, 27.200405968818814, 11.304323479533195, 14.428930562455207, 18.98376740468666, 16.23355607315898, 18.686598322354257, 16.398404289968312, 23.736930582206696, 14.679029861930758, 18.732816922012717, 19.276872029993683, 29.632624155841768, 17.52925050072372, 21.477861462626606, 20.479256246704608, 22.768225857988, 19.528184918221086, 26.563120521139354, 25.28922134079039, 30.72886284533888, 16.538651692215353, 22.300484546925873, 19.735936036799103, 24.721082744188607, 15.764562676660717, 10.919303630944341, 15.27238003630191, 19.09901731973514, 24.026303513906896, 17.253326115664095, 23.419697400182486, 19.97273503569886, 14.78571621561423, 13.643614710308611, 13.406410239171237, 15.764109122101218, 17.73338665952906, 35.354789985809475, 25.500994820147753, 14.008954196702689, 15.31297961389646, 16.54884703317657, 16.994773860555142, 11.50888786977157, 28.291508998721838, 24.458686012309045, 16.23853228846565, 12.15244710817933, 22.00692788278684, 19.537677484098822, 19.761906935833395, 25.327362024225295, 4.423856081441045, 12.91600461769849, 17.02243336243555, 16.172406089492142, 7.921918465290219, 14.90795498015359, 21.78255795268342, 9.149888011161238, 19.27973550837487, 17.482330149039626, 30.521344271022826, 15.807132322806865, 18.62931498233229, 24.148127175867558, 16.132889020256698]
Total Epoch List: [62, 35]
Total Time List: [1.942754534073174, 0.9960042322054505]
========================k_idx:2========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
emb dim:  64
Model: Ethident
Ethident(
  (encoder): HGATE_encoder(
    (lin1): Linear(in_features=14887, out_features=64, bias=True)
    (account_convs): ModuleList(
      (0): GATEConv()
      (1): GATConv(64, 64, heads=1)
    )
    (subgraph_conv): GATConv(64, 64, heads=1)
    (lin2): Linear(in_features=64, out_features=64, bias=True)
    (lin3): Linear(in_features=64, out_features=2, bias=True)
  )
  (proj_head_g1): Project_Head(
    (block): Sequential(
      (0): Linear(in_features=64, out_features=64, bias=True)
      (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=64, bias=True)
      (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
      (5): ReLU()
    )
    (linear_shortcut): Linear(in_features=64, out_features=64, bias=True)
  )
  (proj_head_g2): Project_Head(
    (block): Sequential(
      (0): Linear(in_features=64, out_features=64, bias=True)
      (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=64, bias=True)
      (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
      (5): ReLU()
    )
    (linear_shortcut): Linear(in_features=64, out_features=64, bias=True)
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=64, bias=True)
    (1): ReLU()
    (2): Dropout(p=0.0, inplace=True)
    (3): Linear(in_features=64, out_features=2, bias=True)
  )
)
Total number of parameters: 1003972
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7798933dae90>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.7015;  Loss pred: 0.6721; Loss self: 2.9341; time: 13.37s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6917 score: 0.4898 time: 5.84s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6834 score: 0.5000 time: 3.50s
Epoch 2/1000, LR 0.000015
Train loss: 0.6996;  Loss pred: 0.6703; Loss self: 2.9274; time: 6.57s
Val loss: 0.6541 score: 0.5510 time: 2.83s
Test loss: 0.6416 score: 0.6042 time: 5.20s
Epoch 3/1000, LR 0.000045
Train loss: 0.6447;  Loss pred: 0.6157; Loss self: 2.9039; time: 11.42s
Val loss: 0.6115 score: 0.7347 time: 1.77s
Test loss: 0.5950 score: 0.8125 time: 3.33s
Epoch 4/1000, LR 0.000075
Train loss: 0.5793;  Loss pred: 0.5508; Loss self: 2.8526; time: 8.17s
Val loss: 0.5654 score: 0.7959 time: 2.06s
Test loss: 0.5463 score: 0.8333 time: 5.54s
Epoch 5/1000, LR 0.000105
Train loss: 0.5163;  Loss pred: 0.4885; Loss self: 2.7734; time: 12.07s
Val loss: 0.5254 score: 0.8571 time: 5.09s
Test loss: 0.5016 score: 0.8333 time: 4.82s
Epoch 6/1000, LR 0.000135
Train loss: 0.4646;  Loss pred: 0.4376; Loss self: 2.7063; time: 12.72s
Val loss: 0.4794 score: 0.8367 time: 6.65s
Test loss: 0.4529 score: 0.8333 time: 5.35s
Epoch 7/1000, LR 0.000165
Train loss: 0.4024;  Loss pred: 0.3761; Loss self: 2.6301; time: 15.01s
Val loss: 0.4396 score: 0.8571 time: 3.30s
Test loss: 0.4041 score: 0.8333 time: 4.05s
Epoch 8/1000, LR 0.000195
Train loss: 0.3370;  Loss pred: 0.3118; Loss self: 2.5240; time: 6.06s
Val loss: 0.4100 score: 0.8571 time: 3.30s
Test loss: 0.3621 score: 0.8542 time: 5.32s
Epoch 9/1000, LR 0.000225
Train loss: 0.2965;  Loss pred: 0.2724; Loss self: 2.4041; time: 14.06s
Val loss: 0.3846 score: 0.8571 time: 3.81s
Test loss: 0.3330 score: 0.8958 time: 2.36s
Epoch 10/1000, LR 0.000255
Train loss: 0.2559;  Loss pred: 0.2325; Loss self: 2.3479; time: 14.25s
Val loss: 0.3643 score: 0.8776 time: 5.60s
Test loss: 0.3080 score: 0.8958 time: 6.66s
Epoch 11/1000, LR 0.000285
Train loss: 0.2210;  Loss pred: 0.1977; Loss self: 2.3292; time: 7.79s
Val loss: 0.3452 score: 0.8980 time: 2.10s
Test loss: 0.2909 score: 0.9167 time: 4.26s
Epoch 12/1000, LR 0.000285
Train loss: 0.1936;  Loss pred: 0.1704; Loss self: 2.3149; time: 14.90s
Val loss: 0.3318 score: 0.8980 time: 2.76s
Test loss: 0.2725 score: 0.9167 time: 11.34s
Epoch 13/1000, LR 0.000285
Train loss: 0.1721;  Loss pred: 0.1490; Loss self: 2.3070; time: 19.15s
Val loss: 0.3256 score: 0.8980 time: 3.92s
Test loss: 0.2565 score: 0.9167 time: 4.01s
Epoch 14/1000, LR 0.000285
Train loss: 0.1520;  Loss pred: 0.1289; Loss self: 2.3110; time: 13.09s
Val loss: 0.3265 score: 0.8980 time: 3.55s
Test loss: 0.2421 score: 0.9167 time: 1.76s
     INFO: Early stopping counter 1 of 20
Epoch 15/1000, LR 0.000285
Train loss: 0.1360;  Loss pred: 0.1128; Loss self: 2.3193; time: 14.20s
Val loss: 0.3287 score: 0.8980 time: 0.98s
Test loss: 0.2298 score: 0.9167 time: 4.82s
     INFO: Early stopping counter 2 of 20
Epoch 16/1000, LR 0.000285
Train loss: 0.1239;  Loss pred: 0.1007; Loss self: 2.3240; time: 12.37s
Val loss: 0.3305 score: 0.8980 time: 4.90s
Test loss: 0.2193 score: 0.9167 time: 4.72s
     INFO: Early stopping counter 3 of 20
Epoch 17/1000, LR 0.000285
Train loss: 0.1155;  Loss pred: 0.0922; Loss self: 2.3289; time: 6.43s
Val loss: 0.3291 score: 0.8980 time: 6.12s
Test loss: 0.2115 score: 0.9375 time: 3.96s
     INFO: Early stopping counter 4 of 20
Epoch 18/1000, LR 0.000285
Train loss: 0.1076;  Loss pred: 0.0843; Loss self: 2.3337; time: 13.17s
Val loss: 0.3252 score: 0.8980 time: 5.26s
Test loss: 0.2045 score: 0.9375 time: 3.62s
Epoch 19/1000, LR 0.000285
Train loss: 0.0995;  Loss pred: 0.0761; Loss self: 2.3379; time: 14.41s
Val loss: 0.3202 score: 0.8980 time: 1.66s
Test loss: 0.1985 score: 0.9375 time: 2.77s
Epoch 20/1000, LR 0.000285
Train loss: 0.0914;  Loss pred: 0.0680; Loss self: 2.3402; time: 7.06s
Val loss: 0.3143 score: 0.8980 time: 1.91s
Test loss: 0.1942 score: 0.9375 time: 6.95s
Epoch 21/1000, LR 0.000285
Train loss: 0.0842;  Loss pred: 0.0607; Loss self: 2.3430; time: 11.80s
Val loss: 0.3097 score: 0.8980 time: 1.84s
Test loss: 0.1911 score: 0.9375 time: 1.21s
Epoch 22/1000, LR 0.000285
Train loss: 0.0777;  Loss pred: 0.0543; Loss self: 2.3451; time: 9.73s
Val loss: 0.3072 score: 0.8980 time: 3.76s
Test loss: 0.1890 score: 0.9375 time: 2.02s
Epoch 23/1000, LR 0.000285
Train loss: 0.0721;  Loss pred: 0.0486; Loss self: 2.3456; time: 11.91s
Val loss: 0.3075 score: 0.8980 time: 4.38s
Test loss: 0.1883 score: 0.9375 time: 3.91s
     INFO: Early stopping counter 1 of 20
Epoch 24/1000, LR 0.000285
Train loss: 0.0672;  Loss pred: 0.0438; Loss self: 2.3439; time: 7.43s
Val loss: 0.3127 score: 0.8980 time: 7.18s
Test loss: 0.1905 score: 0.9375 time: 5.28s
     INFO: Early stopping counter 2 of 20
Epoch 25/1000, LR 0.000285
Train loss: 0.0629;  Loss pred: 0.0395; Loss self: 2.3416; time: 3.64s
Val loss: 0.3206 score: 0.8980 time: 2.71s
Test loss: 0.1944 score: 0.9375 time: 4.52s
     INFO: Early stopping counter 3 of 20
Epoch 26/1000, LR 0.000285
Train loss: 0.0589;  Loss pred: 0.0355; Loss self: 2.3406; time: 13.39s
Val loss: 0.3277 score: 0.8980 time: 5.29s
Test loss: 0.1988 score: 0.9375 time: 5.85s
     INFO: Early stopping counter 4 of 20
Epoch 27/1000, LR 0.000285
Train loss: 0.0554;  Loss pred: 0.0320; Loss self: 2.3399; time: 6.38s
Val loss: 0.3342 score: 0.8980 time: 3.59s
Test loss: 0.2016 score: 0.9375 time: 2.59s
     INFO: Early stopping counter 5 of 20
Epoch 28/1000, LR 0.000285
Train loss: 0.0524;  Loss pred: 0.0290; Loss self: 2.3399; time: 9.71s
Val loss: 0.3377 score: 0.8980 time: 3.39s
Test loss: 0.2034 score: 0.9375 time: 5.02s
     INFO: Early stopping counter 6 of 20
Epoch 29/1000, LR 0.000285
Train loss: 0.0499;  Loss pred: 0.0265; Loss self: 2.3411; time: 12.91s
Val loss: 0.3386 score: 0.8980 time: 1.40s
Test loss: 0.2044 score: 0.9375 time: 6.52s
     INFO: Early stopping counter 7 of 20
Epoch 30/1000, LR 0.000285
Train loss: 0.0477;  Loss pred: 0.0243; Loss self: 2.3428; time: 9.10s
Val loss: 0.3378 score: 0.8980 time: 6.05s
Test loss: 0.2037 score: 0.9375 time: 2.63s
     INFO: Early stopping counter 8 of 20
Epoch 31/1000, LR 0.000285
Train loss: 0.0458;  Loss pred: 0.0224; Loss self: 2.3444; time: 16.61s
Val loss: 0.3364 score: 0.8980 time: 6.96s
Test loss: 0.2028 score: 0.9375 time: 5.27s
     INFO: Early stopping counter 9 of 20
Epoch 32/1000, LR 0.000285
Train loss: 0.0441;  Loss pred: 0.0207; Loss self: 2.3461; time: 13.84s
Val loss: 0.3357 score: 0.8980 time: 5.45s
Test loss: 0.2018 score: 0.9375 time: 4.06s
     INFO: Early stopping counter 10 of 20
Epoch 33/1000, LR 0.000285
Train loss: 0.0427;  Loss pred: 0.0192; Loss self: 2.3477; time: 16.42s
Val loss: 0.3361 score: 0.8980 time: 7.19s
Test loss: 0.2016 score: 0.9375 time: 9.18s
     INFO: Early stopping counter 11 of 20
Epoch 34/1000, LR 0.000285
Train loss: 0.0413;  Loss pred: 0.0178; Loss self: 2.3495; time: 18.41s
Val loss: 0.3378 score: 0.8980 time: 7.34s
Test loss: 0.2021 score: 0.9375 time: 5.20s
     INFO: Early stopping counter 12 of 20
Epoch 35/1000, LR 0.000285
Train loss: 0.0401;  Loss pred: 0.0166; Loss self: 2.3512; time: 12.05s
Val loss: 0.3404 score: 0.8980 time: 6.38s
Test loss: 0.2034 score: 0.9375 time: 6.05s
     INFO: Early stopping counter 13 of 20
Epoch 36/1000, LR 0.000285
Train loss: 0.0391;  Loss pred: 0.0155; Loss self: 2.3529; time: 15.19s
Val loss: 0.3443 score: 0.8980 time: 4.75s
Test loss: 0.2058 score: 0.9375 time: 5.45s
     INFO: Early stopping counter 14 of 20
Epoch 37/1000, LR 0.000285
Train loss: 0.0381;  Loss pred: 0.0146; Loss self: 2.3546; time: 16.62s
Val loss: 0.3491 score: 0.8980 time: 5.09s
Test loss: 0.2088 score: 0.9375 time: 5.83s
     INFO: Early stopping counter 15 of 20
Epoch 38/1000, LR 0.000284
Train loss: 0.0373;  Loss pred: 0.0137; Loss self: 2.3562; time: 5.16s
Val loss: 0.3539 score: 0.8980 time: 3.43s
Test loss: 0.2124 score: 0.9375 time: 3.13s
     INFO: Early stopping counter 16 of 20
Epoch 39/1000, LR 0.000284
Train loss: 0.0365;  Loss pred: 0.0129; Loss self: 2.3578; time: 12.49s
Val loss: 0.3589 score: 0.8980 time: 5.08s
Test loss: 0.2160 score: 0.9375 time: 5.50s
     INFO: Early stopping counter 17 of 20
Epoch 40/1000, LR 0.000284
Train loss: 0.0357;  Loss pred: 0.0121; Loss self: 2.3595; time: 16.47s
Val loss: 0.3638 score: 0.8776 time: 5.48s
Test loss: 0.2196 score: 0.9375 time: 4.22s
     INFO: Early stopping counter 18 of 20
Epoch 41/1000, LR 0.000284
Train loss: 0.0351;  Loss pred: 0.0115; Loss self: 2.3611; time: 17.01s
Val loss: 0.3683 score: 0.8776 time: 2.95s
Test loss: 0.2228 score: 0.9375 time: 3.12s
     INFO: Early stopping counter 19 of 20
Epoch 42/1000, LR 0.000284
Train loss: 0.0345;  Loss pred: 0.0108; Loss self: 2.3627; time: 17.41s
Val loss: 0.3727 score: 0.8776 time: 4.24s
Test loss: 0.2259 score: 0.9375 time: 1.28s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 021,   Train_Loss: 0.0777,   Val_Loss: 0.3072,   Val_Precision: 1.0000,   Val_Recall: 0.8000,   Val_accuracy: 0.8889,   Val_Score: 0.8980,   Val_Loss: 0.3072,   Test_Precision: 1.0000,   Test_Recall: 0.8750,   Test_accuracy: 0.9333,   Test_Score: 0.9375,   Test_loss: 0.1890


[3.093264745082706, 5.140389317180961, 4.119556996971369, 6.19796695606783, 4.171529560815543, 6.214615265373141, 5.531291096005589, 4.911078346893191, 1.8754304433241487, 7.21969572128728, 4.858853172976524, 5.726258125156164, 4.311443901155144, 6.314448815304786, 6.399416361004114, 4.875978766940534, 7.120634972117841, 4.533011775929481, 3.0319977030158043, 5.732754092197865, 7.206918265670538, 5.85536127910018, 3.522882784716785, 4.634792665950954, 3.7949008918367326, 3.75464361673221, 4.352745843119919, 4.38002095092088, 6.510121461004019, 0.8826986872591078, 4.2349121007137, 5.288178241811693, 3.628614810295403, 1.343225555960089, 3.9733598427847028, 3.3945001419633627, 6.733690628781915, 4.219065522309393, 4.524024287238717, 4.6733842282556, 3.8118273126892745, 6.85236780392006, 0.0967163648456335, 5.540779908187687, 3.396784577984363, 5.178473813924938, 5.52381440019235, 6.7386662675999105, 4.4692771239206195, 4.014628142118454, 4.678608165122569, 4.241853108163923, 4.727146525867283, 4.53322677500546, 1.1650699987076223, 0.09809994092211127, 3.567427479662001, 1.8741945922374725, 5.0805041808635, 2.506158254109323, 6.056329941842705, 1.9422961347736418, 3.238424740731716, 4.470867450814694, 2.370878328103572, 5.215250143781304, 3.6767619140446186, 3.140701126307249, 4.785339723806828, 2.963048498146236, 3.458892971277237, 6.728299197740853, 3.768670762889087, 0.10541506018489599, 4.88765282696113, 4.903602860867977, 5.380005349870771, 1.8924045078456402, 2.874166732188314, 0.7598507250659168, 6.044045737944543, 4.379432374145836, 1.0970252277329564, 3.802893017884344, 1.7867413996718824, 0.6852059648372233, 3.6185475811362267, 3.348416324239224, 3.082258333917707, 1.845335004851222, 4.599632981233299, 4.201363826170564, 0.5808094632811844, 5.479593717027456, 2.6043737698346376, 3.9575369390659034, 0.995096658822149, 3.509582980070263, 5.210917397867888, 3.3326296210289, 5.551137179136276, 4.823340021073818, 5.357610692735761, 4.057440529577434, 5.324140462093055, 2.3865402447991073, 6.6674539730884135, 4.26778668910265, 11.349474108312279, 4.014951948076487, 1.7748092850670218, 4.824458637274802, 4.73294664407149, 3.969517485704273, 3.6212746491655707, 2.7755256481468678, 6.964675273280591, 1.2128647500649095, 2.0295695643872023, 3.9141876217909157, 5.286957392003387, 4.527417986653745, 5.8563137711025774, 2.598570911679417, 5.025194722227752, 6.530485971365124, 2.6449828688055277, 5.27606611745432, 4.0684952470473945, 9.190156466793269, 5.209699661936611, 6.060076078865677, 5.465768294874579, 5.853234834969044, 3.131285537034273, 5.511012724600732, 4.232929987832904, 3.1281055980362, 1.2900930657051504]
[0.06312785194046339, 0.10490590443226452, 0.08407259177492589, 0.1264891215524047, 0.08513325634317434, 0.12682888296679878, 0.1128834917552161, 0.10022608871210595, 0.038274090680084666, 0.14734072900586287, 0.09916026883625559, 0.11686241071747273, 0.08798865104398253, 0.1288663023531589, 0.13060033389804315, 0.09950977075388845, 0.14531908106362942, 0.0925104444067241, 0.061877504143179676, 0.11699498147342582, 0.1470799646055212, 0.11949716896122815, 0.07189556703503643, 0.09458760542757048, 0.07744695697625985, 0.07662537993331041, 0.08883154781877386, 0.0893881826718547, 0.13285962165314325, 0.01801425892365526, 0.08642677756558571, 0.10792200493493251, 0.07405336347541638, 0.02741276644816508, 0.08108897638336128, 0.06927551310129311, 0.13742225773024316, 0.08610337800631414, 0.0923270262701779, 0.09537518833174694, 0.07779239413651581, 0.13984424089632777, 0.0019738033641966022, 0.1130771409834222, 0.06932213424457884, 0.10568313905969262, 0.11273090612637449, 0.13752380137959, 0.09120973722286979, 0.081931186573846, 0.09548179928821568, 0.08656843077885557, 0.09647237807892416, 0.09251483214296857, 0.023776938749135147, 0.002002039610655332, 0.07280464244208165, 0.03824886922933617, 0.10368375879313264, 0.05114608681855761, 0.12359857024168786, 0.03963869662803351, 0.06609030083125951, 0.09124219287376927, 0.04838527200211372, 0.10643367640370009, 0.075035957429482, 0.06409594135320916, 0.09765999436340465, 0.06047037751318849, 0.07058965247504566, 0.13731222852532352, 0.07691164822222626, 0.0021513277588754284, 0.09974801687675776, 0.10007352777281586, 0.10979602754838308, 0.038620500160115104, 0.058656463922210494, 0.015507157654406465, 0.12334787220294986, 0.08937617090093541, 0.022388269953733803, 0.0776100615894764, 0.03646411019738535, 0.01398379520075966, 0.07384790981910666, 0.06833502702529029, 0.062903231304443, 0.0376598980581882, 0.09387006084149589, 0.08574211890144008, 0.011853254352677233, 0.11182844320464196, 0.05315048509866607, 0.0807660599809368, 0.020308095078003043, 0.07311631208479714, 0.10856077912224767, 0.06942978377143542, 0.11564869123200576, 0.10048625043903787, 0.11161688943199503, 0.08453001103286321, 0.11091959296027198, 0.04971958843331473, 0.1389052911060086, 0.08891222268963854, 0.2364473772565058, 0.08364483225159347, 0.03697519343889629, 0.10050955494322504, 0.0986030550848227, 0.08269828095217235, 0.07544322185761605, 0.057823451003059745, 0.14509740152667897, 0.02526801562635228, 0.042282699258066714, 0.0815455754539774, 0.11014494566673723, 0.09432120805528636, 0.12200653689797036, 0.05413689399332119, 0.10469155671307817, 0.13605179107010676, 0.05510380976678183, 0.10991804411363167, 0.08476031764682072, 0.1914615930581931, 0.1085354096236794, 0.12625158497636826, 0.11387017280988705, 0.1219423923951884, 0.06523511535488069, 0.11481276509584859, 0.08818604141318549, 0.06516886662575416, 0.026876938868857298]
[15.8408684797815, 9.532351924439853, 11.894482837844944, 7.905818205763236, 11.746290967292198, 7.884639339303963, 8.858691243963865, 9.977442129588098, 26.127335286905577, 6.786989631089777, 10.08468423629741, 8.557071464301776, 11.365102068676267, 7.759980551467162, 7.656948264624487, 10.04926443327098, 6.881408777709928, 10.80959027289372, 16.160962111304272, 8.547375172901237, 6.799022577154342, 8.368399090060938, 13.909063399036496, 10.5722097042169, 12.912063159647891, 13.050506253545926, 11.257261913752872, 11.187161100153634, 7.52674128946943, 55.511581366628356, 11.570488084449767, 9.265950911520891, 13.503775562226442, 36.47935358479431, 12.332132487063813, 14.435115024522766, 7.2768415867753955, 11.61394620228103, 10.831064753170818, 10.484907212153168, 12.854727137528723, 7.150812887184541, 506.63608044210133, 8.843520372933785, 14.425406991536798, 9.462247326275705, 8.870681824192667, 7.271468574663837, 10.963741706178949, 12.205364548195364, 10.473200206266112, 11.551555122381298, 10.36566134175628, 10.809077602331286, 42.0575588199458, 499.49061680786014, 13.735387833207627, 26.14456375178324, 9.644712071011778, 19.551837925500188, 8.090708477004014, 25.22787288855442, 15.13081325735195, 10.959841806777582, 20.66744607649028, 9.395522486764685, 13.326943964695717, 15.6016118788141, 10.239607390093418, 16.537022607174922, 14.16638225203776, 7.2826725684928855, 13.001931737448002, 464.82921808378177, 10.02526196822074, 9.992652625079554, 9.107797634657926, 25.89298418855639, 17.048419443186827, 64.48635025747889, 8.107152414876314, 11.188664606233806, 44.666247194023356, 12.884927282876886, 27.424226029014815, 71.51134478468876, 13.54134466973458, 14.633783632366274, 15.897434507937076, 26.55344415576757, 10.653023882540655, 11.662879490411154, 84.3650165807954, 8.94226881232744, 18.814503727362915, 12.381438443772419, 49.24144761776116, 13.676838608055656, 9.21142983760207, 14.403040679084137, 8.64687692828166, 9.951610251460933, 8.959217597703, 11.830117940138733, 9.015539755525154, 20.11279721957528, 7.199149809468583, 11.247047590865545, 4.229270849196891, 11.955311201917672, 27.045159389160727, 9.949302835584849, 10.141673593579386, 12.092149782150116, 13.255001249645717, 17.29402141610477, 6.89192218108834, 39.57572350703668, 23.65033494897371, 12.26308103698868, 9.07894587397296, 10.60206946685681, 8.196282145408846, 18.47169141479319, 9.55186866444865, 7.350142119663132, 18.147565553676635, 9.097687354827881, 11.797973718866887, 5.222979627543674, 9.21358295386972, 7.920692640707676, 8.781930994954743, 8.20059357831213, 15.329167344304896, 8.709832910697472, 11.339663102855651, 15.344750519334777, 37.20661809290769]
Elapsed: 4.199065702454783~1.8188498097587702
Time per graph: 0.08628421563592967~0.03751503845338287
Speed: 25.334273177462055~70.16578963606123
Total Time: 1.2910
best val loss: 0.30722585442114847 test_score: 0.9375

Testing...
Test loss: 0.2909 score: 0.9167 time: 4.08s
test Score 0.9167
Epoch Time List: [12.976028454490006, 27.525766979902983, 19.712311922106892, 19.618266489822417, 22.230121315456927, 16.839620692655444, 22.186089975293726, 21.27933909650892, 17.45930476207286, 33.51027862681076, 18.098223652224988, 16.336218813434243, 23.881507258396596, 19.960599166341126, 27.845399655867368, 19.48496939614415, 21.962667694315314, 21.180531315039843, 18.642449965700507, 22.371735385619104, 25.655681800097227, 21.25370108569041, 22.634155001025647, 22.09314061468467, 24.3926573083736, 23.708388439379632, 19.333353190217167, 22.418254145421088, 29.167838055174798, 18.87651361571625, 27.200405968818814, 11.304323479533195, 14.428930562455207, 18.98376740468666, 16.23355607315898, 18.686598322354257, 16.398404289968312, 23.736930582206696, 14.679029861930758, 18.732816922012717, 19.276872029993683, 29.632624155841768, 17.52925050072372, 21.477861462626606, 20.479256246704608, 22.768225857988, 19.528184918221086, 26.563120521139354, 25.28922134079039, 30.72886284533888, 16.538651692215353, 22.300484546925873, 19.735936036799103, 24.721082744188607, 15.764562676660717, 10.919303630944341, 15.27238003630191, 19.09901731973514, 24.026303513906896, 17.253326115664095, 23.419697400182486, 19.97273503569886, 14.78571621561423, 13.643614710308611, 13.406410239171237, 15.764109122101218, 17.73338665952906, 35.354789985809475, 25.500994820147753, 14.008954196702689, 15.31297961389646, 16.54884703317657, 16.994773860555142, 11.50888786977157, 28.291508998721838, 24.458686012309045, 16.23853228846565, 12.15244710817933, 22.00692788278684, 19.537677484098822, 19.761906935833395, 25.327362024225295, 4.423856081441045, 12.91600461769849, 17.02243336243555, 16.172406089492142, 7.921918465290219, 14.90795498015359, 21.78255795268342, 9.149888011161238, 19.27973550837487, 17.482330149039626, 30.521344271022826, 15.807132322806865, 18.62931498233229, 24.148127175867558, 16.132889020256698, 22.714240053202957, 14.602760442532599, 16.513457599096, 15.766192045994103, 21.977487401571125, 24.717509873677045, 22.350326623301953, 14.674809732474387, 20.232329957652837, 26.516891225706786, 14.152403742074966, 29.003085104282945, 27.073938469402492, 18.396055460441858, 19.99244774179533, 21.981822922360152, 16.508798929862678, 22.049412273336202, 18.83829354401678, 15.919389915186912, 14.844880130141973, 15.513588954228908, 20.187952609267086, 19.88408569479361, 10.86977070895955, 24.52826809696853, 12.559910838026553, 18.10763649502769, 20.83399188425392, 17.777322891168296, 28.84253688203171, 23.349690198898315, 32.787035217974335, 30.944179738871753, 24.483538687694818, 25.38959765713662, 27.54263773560524, 11.709919796790928, 23.07000950491056, 26.16875832527876, 23.086214517243207, 22.934533205349]
Total Epoch List: [62, 35, 42]
Total Time List: [1.942754534073174, 0.9960042322054505, 1.291047137696296]
T-times Epoch Time: 21.563685503941826 ~ 1.690304223007282
T-times Total Epoch: 47.333333333333336 ~ 1.1863420280034787
T-times Total Time: 3.7547085754469864 ~ 1.6694969025151065
T-times Inference Elapsed: 4.4297362780222675 ~ 0.24686107876291474
T-times Time Per Graph: 0.09097482683929907 ~ 0.005050464509267471
T-times Speed: 18.583497885008025 ~ 4.796341882084074
T-times cross validation test micro f1 score:0.8957165099724252 ~ 0.0031400244440445892
T-times cross validation test precision:0.949894710764276 ~ 0.005000255888006717
T-times cross validation test recall:0.8496296296296296 ~ 0.010889770687399354
T-times cross validation test f1_score:0.8957165099724252 ~ 0.004447990361124057
