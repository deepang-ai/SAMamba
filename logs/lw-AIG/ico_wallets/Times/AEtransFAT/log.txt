Namespace(seed=15, model='AEtransGAT', dataset='ico_wallets/Times', num_heads=1, num_layers=2, dim_hidden=64, dropout=0.1, epochs=1000, lr=0.0003, weight_decay=0.0001, batch_size=64, abs_pe=None, abs_pe_dim=20, num_class=2, outdir='./outdir/ico_wallets/Times/seed15/edge_attr/khopgnn_gat_1_0.1_0.0003_0.0001_2_1_64_BN', warmup=10, layer_norm=False, use_edge_attr=True, edge_dim=128, gnn_type='gat', k_hop=1, global_pool='mean', se='khopgnn', ie='mamba', aggr='add', not_extract_node_feature=False, k_ford=3, early_stop=1, early_stop_mindelta=-0.0, patience=2, training_times=3, Lambda=0.01, order='sd', use_cuda=True, batch_norm=True, save_logs=True)
Data(edge_index=[2, 338], edge_attr=[338, 2], x=[122, 14887], y=[1, 1], num_nodes=122)
Data(edge_index=[2, 338], edge_attr=[338, 2], x=[122, 14887], y=[1, 1], num_nodes=122)
Data(edge_index=[2, 298], edge_attr=[298, 2], x=[109, 14887], y=[1, 1], num_nodes=122)
========================training times:0========================
========================k_idx:0========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: AEtransGAT
AEtransGAT(
  (gae): GAE(
    (encoder): transGAT(
      (encoder): EdgeFeatureGAT()
      (classifier): Sequential(
        (0): Linear(in_features=64, out_features=64, bias=True)
        (1): ReLU(inplace=True)
        (2): Linear(in_features=64, out_features=2, bias=True)
      )
    )
    (decoder): InnerProductDecoder()
  )
)
Total number of parameters: 1914242
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7d35981f3cd0>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 1.8474;  Loss pred: 1.8474; Loss self: 0.0000; time: 6.35s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6998 score: 0.4898 time: 3.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6957 score: 0.5102 time: 0.23s
Epoch 2/1000, LR 0.000000
Train loss: 1.8708;  Loss pred: 1.8708; Loss self: 0.0000; time: 0.54s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6997 score: 0.4898 time: 0.09s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6956 score: 0.5102 time: 0.08s
Epoch 3/1000, LR 0.000030
Train loss: 1.8660;  Loss pred: 1.8660; Loss self: 0.0000; time: 0.33s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6996 score: 0.4898 time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6955 score: 0.5102 time: 0.15s
Epoch 4/1000, LR 0.000060
Train loss: 1.8383;  Loss pred: 1.8383; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6993 score: 0.4898 time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6954 score: 0.5102 time: 1.29s
Epoch 5/1000, LR 0.000090
Train loss: 1.8235;  Loss pred: 1.8235; Loss self: 0.0000; time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6990 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6952 score: 0.5102 time: 0.06s
Epoch 6/1000, LR 0.000120
Train loss: 1.8406;  Loss pred: 1.8406; Loss self: 0.0000; time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6986 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6949 score: 0.5102 time: 0.07s
Epoch 7/1000, LR 0.000150
Train loss: 1.8066;  Loss pred: 1.8066; Loss self: 0.0000; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6982 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6947 score: 0.5102 time: 0.07s
Epoch 8/1000, LR 0.000180
Train loss: 1.8013;  Loss pred: 1.8013; Loss self: 0.0000; time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6977 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6944 score: 0.5102 time: 0.07s
Epoch 9/1000, LR 0.000210
Train loss: 1.7745;  Loss pred: 1.7745; Loss self: 0.0000; time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6973 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6941 score: 0.5102 time: 0.07s
Epoch 10/1000, LR 0.000240
Train loss: 1.7453;  Loss pred: 1.7453; Loss self: 0.0000; time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6967 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6938 score: 0.5102 time: 0.06s
Epoch 11/1000, LR 0.000270
Train loss: 1.7090;  Loss pred: 1.7090; Loss self: 0.0000; time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6963 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6935 score: 0.5102 time: 0.06s
Epoch 12/1000, LR 0.000270
Train loss: 1.7032;  Loss pred: 1.7032; Loss self: 0.0000; time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6959 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6933 score: 0.5102 time: 0.06s
Epoch 13/1000, LR 0.000270
Train loss: 1.6648;  Loss pred: 1.6648; Loss self: 0.0000; time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6955 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6931 score: 0.5102 time: 0.06s
Epoch 14/1000, LR 0.000270
Train loss: 1.6314;  Loss pred: 1.6314; Loss self: 0.0000; time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6951 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6930 score: 0.5102 time: 0.06s
Epoch 15/1000, LR 0.000270
Train loss: 1.6195;  Loss pred: 1.6195; Loss self: 0.0000; time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6948 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6928 score: 0.5102 time: 0.06s
Epoch 16/1000, LR 0.000270
Train loss: 1.5954;  Loss pred: 1.5954; Loss self: 0.0000; time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6945 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6926 score: 0.5102 time: 0.06s
Epoch 17/1000, LR 0.000270
Train loss: 1.5531;  Loss pred: 1.5531; Loss self: 0.0000; time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6942 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6925 score: 0.5102 time: 0.06s
Epoch 18/1000, LR 0.000270
Train loss: 1.5429;  Loss pred: 1.5429; Loss self: 0.0000; time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6939 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6924 score: 0.5102 time: 0.06s
Epoch 19/1000, LR 0.000270
Train loss: 1.5380;  Loss pred: 1.5380; Loss self: 0.0000; time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6936 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6923 score: 0.5102 time: 0.07s
Epoch 20/1000, LR 0.000270
Train loss: 1.4986;  Loss pred: 1.4986; Loss self: 0.0000; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6934 score: 0.4898 time: 0.09s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6922 score: 0.5102 time: 0.11s
Epoch 21/1000, LR 0.000270
Train loss: 1.4941;  Loss pred: 1.4941; Loss self: 0.0000; time: 0.34s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6932 score: 0.4898 time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6921 score: 0.5102 time: 3.82s
Epoch 22/1000, LR 0.000270
Train loss: 1.4705;  Loss pred: 1.4705; Loss self: 0.0000; time: 5.50s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6930 score: 0.4898 time: 0.41s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6920 score: 0.5102 time: 0.42s
Epoch 23/1000, LR 0.000270
Train loss: 1.4300;  Loss pred: 1.4300; Loss self: 0.0000; time: 0.43s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6927 score: 0.4898 time: 0.09s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6919 score: 0.5102 time: 0.08s
Epoch 24/1000, LR 0.000270
Train loss: 1.4258;  Loss pred: 1.4258; Loss self: 0.0000; time: 0.30s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6925 score: 0.4898 time: 0.09s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6918 score: 0.5102 time: 0.37s
Epoch 25/1000, LR 0.000270
Train loss: 1.4034;  Loss pred: 1.4034; Loss self: 0.0000; time: 0.99s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6923 score: 0.4898 time: 0.55s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6917 score: 0.5102 time: 0.26s
Epoch 26/1000, LR 0.000270
Train loss: 1.3930;  Loss pred: 1.3930; Loss self: 0.0000; time: 1.02s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6921 score: 0.4898 time: 3.78s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6916 score: 0.5102 time: 3.68s
Epoch 27/1000, LR 0.000270
Train loss: 1.3805;  Loss pred: 1.3805; Loss self: 0.0000; time: 4.59s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6919 score: 0.4898 time: 0.32s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6915 score: 0.5102 time: 0.28s
Epoch 28/1000, LR 0.000270
Train loss: 1.3728;  Loss pred: 1.3728; Loss self: 0.0000; time: 0.47s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6917 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6914 score: 0.5102 time: 0.10s
Epoch 29/1000, LR 0.000270
Train loss: 1.3515;  Loss pred: 1.3515; Loss self: 0.0000; time: 1.01s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6916 score: 0.4898 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6914 score: 0.5102 time: 0.25s
Epoch 30/1000, LR 0.000270
Train loss: 1.3325;  Loss pred: 1.3325; Loss self: 0.0000; time: 0.93s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6914 score: 0.4898 time: 0.38s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6913 score: 0.5102 time: 0.43s
Epoch 31/1000, LR 0.000270
Train loss: 1.3306;  Loss pred: 1.3306; Loss self: 0.0000; time: 1.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6912 score: 0.4898 time: 0.37s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6912 score: 0.5102 time: 3.26s
Epoch 32/1000, LR 0.000270
Train loss: 1.3119;  Loss pred: 1.3119; Loss self: 0.0000; time: 9.82s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6910 score: 0.4898 time: 1.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6911 score: 0.5102 time: 1.06s
Epoch 33/1000, LR 0.000270
Train loss: 1.2949;  Loss pred: 1.2949; Loss self: 0.0000; time: 2.51s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6908 score: 0.4898 time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6910 score: 0.5102 time: 0.11s
Epoch 34/1000, LR 0.000270
Train loss: 1.2787;  Loss pred: 1.2787; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6906 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6909 score: 0.5102 time: 0.08s
Epoch 35/1000, LR 0.000270
Train loss: 1.2711;  Loss pred: 1.2711; Loss self: 0.0000; time: 0.48s
Val loss: 0.6904 score: 0.5102 time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6907 score: 0.5102 time: 0.41s
Epoch 36/1000, LR 0.000270
Train loss: 1.2563;  Loss pred: 1.2563; Loss self: 0.0000; time: 5.28s
Val loss: 0.6902 score: 0.5102 time: 2.64s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6906 score: 0.5102 time: 1.40s
Epoch 37/1000, LR 0.000270
Train loss: 1.2500;  Loss pred: 1.2500; Loss self: 0.0000; time: 0.47s
Val loss: 0.6898 score: 0.5102 time: 0.10s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6904 score: 0.5102 time: 0.24s
Epoch 38/1000, LR 0.000270
Train loss: 1.2390;  Loss pred: 1.2390; Loss self: 0.0000; time: 0.41s
Val loss: 0.6895 score: 0.5102 time: 0.08s
Test loss: 0.6902 score: 0.5306 time: 0.09s
Epoch 39/1000, LR 0.000269
Train loss: 1.2233;  Loss pred: 1.2233; Loss self: 0.0000; time: 0.25s
Val loss: 0.6893 score: 0.5102 time: 0.09s
Test loss: 0.6900 score: 0.5306 time: 0.21s
Epoch 40/1000, LR 0.000269
Train loss: 1.2178;  Loss pred: 1.2178; Loss self: 0.0000; time: 0.96s
Val loss: 0.6890 score: 0.5306 time: 1.24s
Test loss: 0.6899 score: 0.5510 time: 3.62s
Epoch 41/1000, LR 0.000269
Train loss: 1.2162;  Loss pred: 1.2162; Loss self: 0.0000; time: 7.52s
Val loss: 0.6888 score: 0.5510 time: 0.11s
Test loss: 0.6897 score: 0.5510 time: 0.09s
Epoch 42/1000, LR 0.000269
Train loss: 1.2056;  Loss pred: 1.2056; Loss self: 0.0000; time: 0.26s
Val loss: 0.6885 score: 0.5510 time: 0.21s
Test loss: 0.6895 score: 0.5714 time: 2.71s
Epoch 43/1000, LR 0.000269
Train loss: 1.1909;  Loss pred: 1.1909; Loss self: 0.0000; time: 8.07s
Val loss: 0.6883 score: 0.5510 time: 0.18s
Test loss: 0.6894 score: 0.6122 time: 0.20s
Epoch 44/1000, LR 0.000269
Train loss: 1.1835;  Loss pred: 1.1835; Loss self: 0.0000; time: 1.15s
Val loss: 0.6880 score: 0.5714 time: 0.07s
Test loss: 0.6892 score: 0.6122 time: 0.06s
Epoch 45/1000, LR 0.000269
Train loss: 1.1780;  Loss pred: 1.1780; Loss self: 0.0000; time: 0.23s
Val loss: 0.6878 score: 0.6327 time: 0.07s
Test loss: 0.6890 score: 0.6735 time: 0.06s
Epoch 46/1000, LR 0.000269
Train loss: 1.1766;  Loss pred: 1.1766; Loss self: 0.0000; time: 0.24s
Val loss: 0.6875 score: 0.6327 time: 0.08s
Test loss: 0.6888 score: 0.6735 time: 0.07s
Epoch 47/1000, LR 0.000269
Train loss: 1.1581;  Loss pred: 1.1581; Loss self: 0.0000; time: 0.25s
Val loss: 0.6872 score: 0.6735 time: 0.08s
Test loss: 0.6886 score: 0.6735 time: 0.08s
Epoch 48/1000, LR 0.000269
Train loss: 1.1498;  Loss pred: 1.1498; Loss self: 0.0000; time: 0.24s
Val loss: 0.6869 score: 0.6735 time: 0.08s
Test loss: 0.6884 score: 0.6939 time: 0.07s
Epoch 49/1000, LR 0.000269
Train loss: 1.1488;  Loss pred: 1.1488; Loss self: 0.0000; time: 0.25s
Val loss: 0.6866 score: 0.7143 time: 0.08s
Test loss: 0.6882 score: 0.7143 time: 0.07s
Epoch 50/1000, LR 0.000269
Train loss: 1.1437;  Loss pred: 1.1437; Loss self: 0.0000; time: 0.24s
Val loss: 0.6863 score: 0.7143 time: 0.08s
Test loss: 0.6879 score: 0.7143 time: 0.07s
Epoch 51/1000, LR 0.000269
Train loss: 1.1378;  Loss pred: 1.1378; Loss self: 0.0000; time: 0.27s
Val loss: 0.6860 score: 0.8163 time: 0.24s
Test loss: 0.6877 score: 0.7143 time: 0.46s
Epoch 52/1000, LR 0.000269
Train loss: 1.1294;  Loss pred: 1.1294; Loss self: 0.0000; time: 10.29s
Val loss: 0.6856 score: 0.8163 time: 0.95s
Test loss: 0.6875 score: 0.7347 time: 0.41s
Epoch 53/1000, LR 0.000269
Train loss: 1.1230;  Loss pred: 1.1230; Loss self: 0.0000; time: 0.34s
Val loss: 0.6853 score: 0.8571 time: 0.24s
Test loss: 0.6872 score: 0.7551 time: 0.09s
Epoch 54/1000, LR 0.000269
Train loss: 1.1277;  Loss pred: 1.1277; Loss self: 0.0000; time: 0.26s
Val loss: 0.6849 score: 0.8571 time: 0.09s
Test loss: 0.6869 score: 0.8163 time: 0.15s
Epoch 55/1000, LR 0.000269
Train loss: 1.1136;  Loss pred: 1.1136; Loss self: 0.0000; time: 1.04s
Val loss: 0.6846 score: 0.9184 time: 0.38s
Test loss: 0.6867 score: 0.8163 time: 0.43s
Epoch 56/1000, LR 0.000269
Train loss: 1.1067;  Loss pred: 1.1067; Loss self: 0.0000; time: 2.32s
Val loss: 0.6842 score: 0.9184 time: 3.42s
Test loss: 0.6864 score: 0.8163 time: 3.27s
Epoch 57/1000, LR 0.000269
Train loss: 1.1014;  Loss pred: 1.1014; Loss self: 0.0000; time: 3.94s
Val loss: 0.6838 score: 0.9592 time: 0.24s
Test loss: 0.6861 score: 0.7959 time: 0.10s
Epoch 58/1000, LR 0.000269
Train loss: 1.1037;  Loss pred: 1.1037; Loss self: 0.0000; time: 1.00s
Val loss: 0.6834 score: 0.9592 time: 0.35s
Test loss: 0.6858 score: 0.7959 time: 0.27s
Epoch 59/1000, LR 0.000268
Train loss: 1.0915;  Loss pred: 1.0915; Loss self: 0.0000; time: 3.98s
Val loss: 0.6830 score: 0.9796 time: 3.50s
Test loss: 0.6855 score: 0.7959 time: 3.80s
Epoch 60/1000, LR 0.000268
Train loss: 1.0913;  Loss pred: 1.0913; Loss self: 0.0000; time: 7.62s
Val loss: 0.6825 score: 0.9796 time: 0.20s
Test loss: 0.6852 score: 0.8163 time: 0.25s
Epoch 61/1000, LR 0.000268
Train loss: 1.0902;  Loss pred: 1.0902; Loss self: 0.0000; time: 0.55s
Val loss: 0.6821 score: 0.9796 time: 0.35s
Test loss: 0.6848 score: 0.7959 time: 0.44s
Epoch 62/1000, LR 0.000268
Train loss: 1.0848;  Loss pred: 1.0848; Loss self: 0.0000; time: 0.84s
Val loss: 0.6816 score: 0.9796 time: 0.17s
Test loss: 0.6845 score: 0.7959 time: 0.17s
Epoch 63/1000, LR 0.000268
Train loss: 1.0791;  Loss pred: 1.0791; Loss self: 0.0000; time: 0.42s
Val loss: 0.6811 score: 0.9796 time: 0.14s
Test loss: 0.6841 score: 0.7959 time: 0.07s
Epoch 64/1000, LR 0.000268
Train loss: 1.0785;  Loss pred: 1.0785; Loss self: 0.0000; time: 8.07s
Val loss: 0.6806 score: 0.9796 time: 0.43s
Test loss: 0.6838 score: 0.8163 time: 0.72s
Epoch 65/1000, LR 0.000268
Train loss: 1.0701;  Loss pred: 1.0701; Loss self: 0.0000; time: 0.81s
Val loss: 0.6801 score: 0.9592 time: 0.33s
Test loss: 0.6834 score: 0.8163 time: 0.26s
Epoch 66/1000, LR 0.000268
Train loss: 1.0682;  Loss pred: 1.0682; Loss self: 0.0000; time: 0.72s
Val loss: 0.6795 score: 0.9592 time: 0.38s
Test loss: 0.6830 score: 0.8367 time: 0.24s
Epoch 67/1000, LR 0.000268
Train loss: 1.0636;  Loss pred: 1.0636; Loss self: 0.0000; time: 0.96s
Val loss: 0.6790 score: 0.9388 time: 0.39s
Test loss: 0.6826 score: 0.8367 time: 0.11s
Epoch 68/1000, LR 0.000268
Train loss: 1.0622;  Loss pred: 1.0622; Loss self: 0.0000; time: 0.30s
Val loss: 0.6783 score: 0.9388 time: 0.08s
Test loss: 0.6821 score: 0.8367 time: 0.06s
Epoch 69/1000, LR 0.000268
Train loss: 1.0558;  Loss pred: 1.0558; Loss self: 0.0000; time: 0.25s
Val loss: 0.6777 score: 0.9388 time: 0.07s
Test loss: 0.6816 score: 0.8367 time: 0.07s
Epoch 70/1000, LR 0.000268
Train loss: 1.0537;  Loss pred: 1.0537; Loss self: 0.0000; time: 0.31s
Val loss: 0.6770 score: 0.9388 time: 0.17s
Test loss: 0.6812 score: 0.8367 time: 0.79s
Epoch 71/1000, LR 0.000268
Train loss: 1.0514;  Loss pred: 1.0514; Loss self: 0.0000; time: 0.27s
Val loss: 0.6764 score: 0.9388 time: 2.69s
Test loss: 0.6807 score: 0.8367 time: 4.10s
Epoch 72/1000, LR 0.000267
Train loss: 1.0472;  Loss pred: 1.0472; Loss self: 0.0000; time: 9.68s
Val loss: 0.6757 score: 0.9388 time: 3.31s
Test loss: 0.6802 score: 0.8367 time: 2.12s
Epoch 73/1000, LR 0.000267
Train loss: 1.0448;  Loss pred: 1.0448; Loss self: 0.0000; time: 3.24s
Val loss: 0.6749 score: 0.9388 time: 0.20s
Test loss: 0.6796 score: 0.8571 time: 0.26s
Epoch 74/1000, LR 0.000267
Train loss: 1.0440;  Loss pred: 1.0440; Loss self: 0.0000; time: 0.25s
Val loss: 0.6742 score: 0.9388 time: 0.19s
Test loss: 0.6791 score: 0.8367 time: 0.07s
Epoch 75/1000, LR 0.000267
Train loss: 1.0423;  Loss pred: 1.0423; Loss self: 0.0000; time: 0.25s
Val loss: 0.6735 score: 0.9388 time: 0.08s
Test loss: 0.6786 score: 0.8367 time: 0.08s
Epoch 76/1000, LR 0.000267
Train loss: 1.0382;  Loss pred: 1.0382; Loss self: 0.0000; time: 0.27s
Val loss: 0.6727 score: 0.9388 time: 0.08s
Test loss: 0.6780 score: 0.8367 time: 0.07s
Epoch 77/1000, LR 0.000267
Train loss: 1.0354;  Loss pred: 1.0354; Loss self: 0.0000; time: 0.28s
Val loss: 0.6719 score: 0.9388 time: 0.08s
Test loss: 0.6774 score: 0.8367 time: 0.07s
Epoch 78/1000, LR 0.000267
Train loss: 1.0364;  Loss pred: 1.0364; Loss self: 0.0000; time: 0.26s
Val loss: 0.6711 score: 0.9388 time: 0.09s
Test loss: 0.6768 score: 0.8367 time: 0.07s
Epoch 79/1000, LR 0.000267
Train loss: 1.0272;  Loss pred: 1.0272; Loss self: 0.0000; time: 0.25s
Val loss: 0.6703 score: 0.9388 time: 0.24s
Test loss: 0.6762 score: 0.8367 time: 0.24s
Epoch 80/1000, LR 0.000267
Train loss: 1.0247;  Loss pred: 1.0247; Loss self: 0.0000; time: 0.81s
Val loss: 0.6695 score: 0.9388 time: 0.25s
Test loss: 0.6756 score: 0.8367 time: 0.36s
Epoch 81/1000, LR 0.000267
Train loss: 1.0244;  Loss pred: 1.0244; Loss self: 0.0000; time: 12.00s
Val loss: 0.6686 score: 0.9388 time: 3.57s
Test loss: 0.6749 score: 0.8367 time: 3.46s
Epoch 82/1000, LR 0.000267
Train loss: 1.0212;  Loss pred: 1.0212; Loss self: 0.0000; time: 3.91s
Val loss: 0.6677 score: 0.9388 time: 0.62s
Test loss: 0.6742 score: 0.8367 time: 0.70s
Epoch 83/1000, LR 0.000266
Train loss: 1.0195;  Loss pred: 1.0195; Loss self: 0.0000; time: 0.45s
Val loss: 0.6668 score: 0.9388 time: 0.08s
Test loss: 0.6735 score: 0.8367 time: 0.37s
Epoch 84/1000, LR 0.000266
Train loss: 1.0154;  Loss pred: 1.0154; Loss self: 0.0000; time: 0.32s
Val loss: 0.6659 score: 0.9388 time: 0.13s
Test loss: 0.6728 score: 0.8367 time: 0.24s
Epoch 85/1000, LR 0.000266
Train loss: 1.0143;  Loss pred: 1.0143; Loss self: 0.0000; time: 0.99s
Val loss: 0.6649 score: 0.9388 time: 0.55s
Test loss: 0.6721 score: 0.8367 time: 4.10s
Epoch 86/1000, LR 0.000266
Train loss: 1.0138;  Loss pred: 1.0138; Loss self: 0.0000; time: 6.78s
Val loss: 0.6639 score: 0.9388 time: 0.42s
Test loss: 0.6713 score: 0.8367 time: 0.44s
Epoch 87/1000, LR 0.000266
Train loss: 1.0080;  Loss pred: 1.0080; Loss self: 0.0000; time: 0.52s
Val loss: 0.6629 score: 0.9388 time: 0.09s
Test loss: 0.6705 score: 0.8367 time: 0.11s
Epoch 88/1000, LR 0.000266
Train loss: 1.0074;  Loss pred: 1.0074; Loss self: 0.0000; time: 0.32s
Val loss: 0.6618 score: 0.9388 time: 0.09s
Test loss: 0.6697 score: 0.8367 time: 0.12s
Epoch 89/1000, LR 0.000266
Train loss: 1.0101;  Loss pred: 1.0101; Loss self: 0.0000; time: 0.34s
Val loss: 0.6607 score: 0.9388 time: 0.10s
Test loss: 0.6689 score: 0.8367 time: 0.08s
Epoch 90/1000, LR 0.000266
Train loss: 1.0069;  Loss pred: 1.0069; Loss self: 0.0000; time: 0.31s
Val loss: 0.6596 score: 0.9388 time: 0.11s
Test loss: 0.6681 score: 0.8367 time: 0.08s
Epoch 91/1000, LR 0.000266
Train loss: 1.0024;  Loss pred: 1.0024; Loss self: 0.0000; time: 0.30s
Val loss: 0.6585 score: 0.9388 time: 0.08s
Test loss: 0.6672 score: 0.8367 time: 0.07s
Epoch 92/1000, LR 0.000266
Train loss: 1.0020;  Loss pred: 1.0020; Loss self: 0.0000; time: 0.58s
Val loss: 0.6573 score: 0.9388 time: 0.18s
Test loss: 0.6663 score: 0.8367 time: 0.38s
Epoch 93/1000, LR 0.000265
Train loss: 0.9999;  Loss pred: 0.9999; Loss self: 0.0000; time: 6.51s
Val loss: 0.6560 score: 0.9388 time: 1.39s
Test loss: 0.6653 score: 0.8367 time: 1.27s
Epoch 94/1000, LR 0.000265
Train loss: 0.9931;  Loss pred: 0.9931; Loss self: 0.0000; time: 3.67s
Val loss: 0.6547 score: 0.9388 time: 0.30s
Test loss: 0.6644 score: 0.8367 time: 0.32s
Epoch 95/1000, LR 0.000265
Train loss: 0.9949;  Loss pred: 0.9949; Loss self: 0.0000; time: 0.47s
Val loss: 0.6534 score: 0.9388 time: 0.09s
Test loss: 0.6634 score: 0.8367 time: 0.08s
Epoch 96/1000, LR 0.000265
Train loss: 0.9930;  Loss pred: 0.9930; Loss self: 0.0000; time: 0.25s
Val loss: 0.6520 score: 0.9388 time: 0.10s
Test loss: 0.6623 score: 0.8367 time: 0.08s
Epoch 97/1000, LR 0.000265
Train loss: 0.9897;  Loss pred: 0.9897; Loss self: 0.0000; time: 0.29s
Val loss: 0.6506 score: 0.9388 time: 2.96s
Test loss: 0.6613 score: 0.8367 time: 4.00s
Epoch 98/1000, LR 0.000265
Train loss: 0.9908;  Loss pred: 0.9908; Loss self: 0.0000; time: 10.33s
Val loss: 0.6491 score: 0.9388 time: 3.27s
Test loss: 0.6602 score: 0.8163 time: 1.88s
Epoch 99/1000, LR 0.000265
Train loss: 0.9856;  Loss pred: 0.9856; Loss self: 0.0000; time: 0.27s
Val loss: 0.6476 score: 0.9388 time: 0.09s
Test loss: 0.6590 score: 0.8163 time: 0.07s
Epoch 100/1000, LR 0.000265
Train loss: 0.9865;  Loss pred: 0.9865; Loss self: 0.0000; time: 0.30s
Val loss: 0.6461 score: 0.9388 time: 0.17s
Test loss: 0.6579 score: 0.8163 time: 0.28s
Epoch 101/1000, LR 0.000265
Train loss: 0.9818;  Loss pred: 0.9818; Loss self: 0.0000; time: 0.46s
Val loss: 0.6446 score: 0.9388 time: 0.10s
Test loss: 0.6567 score: 0.8163 time: 0.11s
Epoch 102/1000, LR 0.000264
Train loss: 0.9805;  Loss pred: 0.9805; Loss self: 0.0000; time: 0.26s
Val loss: 0.6430 score: 0.9388 time: 0.09s
Test loss: 0.6555 score: 0.8163 time: 0.08s
Epoch 103/1000, LR 0.000264
Train loss: 0.9792;  Loss pred: 0.9792; Loss self: 0.0000; time: 0.27s
Val loss: 0.6414 score: 0.9388 time: 0.23s
Test loss: 0.6543 score: 0.8163 time: 0.45s
Epoch 104/1000, LR 0.000264
Train loss: 0.9760;  Loss pred: 0.9760; Loss self: 0.0000; time: 0.45s
Val loss: 0.6397 score: 0.9388 time: 0.09s
Test loss: 0.6531 score: 0.8163 time: 0.10s
Epoch 105/1000, LR 0.000264
Train loss: 0.9748;  Loss pred: 0.9748; Loss self: 0.0000; time: 7.35s
Val loss: 0.6380 score: 0.9388 time: 2.84s
Test loss: 0.6518 score: 0.8367 time: 3.12s
Epoch 106/1000, LR 0.000264
Train loss: 0.9704;  Loss pred: 0.9704; Loss self: 0.0000; time: 0.41s
Val loss: 0.6363 score: 0.9388 time: 0.29s
Test loss: 0.6505 score: 0.8367 time: 0.10s
Epoch 107/1000, LR 0.000264
Train loss: 0.9723;  Loss pred: 0.9723; Loss self: 0.0000; time: 0.42s
Val loss: 0.6345 score: 0.9388 time: 0.23s
Test loss: 0.6491 score: 0.8367 time: 0.09s
Epoch 108/1000, LR 0.000264
Train loss: 0.9702;  Loss pred: 0.9702; Loss self: 0.0000; time: 0.26s
Val loss: 0.6327 score: 0.9388 time: 0.09s
Test loss: 0.6478 score: 0.8367 time: 0.20s
Epoch 109/1000, LR 0.000264
Train loss: 0.9656;  Loss pred: 0.9656; Loss self: 0.0000; time: 0.30s
Val loss: 0.6308 score: 0.9388 time: 0.10s
Test loss: 0.6464 score: 0.8367 time: 0.09s
Epoch 110/1000, LR 0.000263
Train loss: 0.9673;  Loss pred: 0.9673; Loss self: 0.0000; time: 0.26s
Val loss: 0.6289 score: 0.9388 time: 0.08s
Test loss: 0.6450 score: 0.8367 time: 0.07s
Epoch 111/1000, LR 0.000263
Train loss: 0.9626;  Loss pred: 0.9626; Loss self: 0.0000; time: 0.25s
Val loss: 0.6270 score: 0.9388 time: 0.14s
Test loss: 0.6435 score: 0.8367 time: 0.10s
Epoch 112/1000, LR 0.000263
Train loss: 0.9610;  Loss pred: 0.9610; Loss self: 0.0000; time: 0.26s
Val loss: 0.6250 score: 0.9388 time: 0.09s
Test loss: 0.6420 score: 0.8367 time: 0.09s
Epoch 113/1000, LR 0.000263
Train loss: 0.9583;  Loss pred: 0.9583; Loss self: 0.0000; time: 0.27s
Val loss: 0.6229 score: 0.9388 time: 0.11s
Test loss: 0.6405 score: 0.8367 time: 0.07s
Epoch 114/1000, LR 0.000263
Train loss: 0.9551;  Loss pred: 0.9551; Loss self: 0.0000; time: 0.25s
Val loss: 0.6209 score: 0.9388 time: 0.08s
Test loss: 0.6389 score: 0.8367 time: 0.17s
Epoch 115/1000, LR 0.000263
Train loss: 0.9530;  Loss pred: 0.9530; Loss self: 0.0000; time: 0.62s
Val loss: 0.6187 score: 0.9388 time: 0.18s
Test loss: 0.6373 score: 0.8367 time: 0.09s
Epoch 116/1000, LR 0.000263
Train loss: 0.9505;  Loss pred: 0.9505; Loss self: 0.0000; time: 0.31s
Val loss: 0.6166 score: 0.9388 time: 0.13s
Test loss: 0.6357 score: 0.8367 time: 0.08s
Epoch 117/1000, LR 0.000262
Train loss: 0.9533;  Loss pred: 0.9533; Loss self: 0.0000; time: 0.43s
Val loss: 0.6144 score: 0.9388 time: 0.16s
Test loss: 0.6340 score: 0.8367 time: 0.08s
Epoch 118/1000, LR 0.000262
Train loss: 0.9463;  Loss pred: 0.9463; Loss self: 0.0000; time: 0.31s
Val loss: 0.6121 score: 0.9388 time: 0.18s
Test loss: 0.6323 score: 0.8367 time: 0.07s
Epoch 119/1000, LR 0.000262
Train loss: 0.9462;  Loss pred: 0.9462; Loss self: 0.0000; time: 0.25s
Val loss: 0.6098 score: 0.9388 time: 0.08s
Test loss: 0.6306 score: 0.8367 time: 0.07s
Epoch 120/1000, LR 0.000262
Train loss: 0.9418;  Loss pred: 0.9418; Loss self: 0.0000; time: 0.32s
Val loss: 0.6074 score: 0.9388 time: 0.11s
Test loss: 0.6289 score: 0.8367 time: 0.07s
Epoch 121/1000, LR 0.000262
Train loss: 0.9412;  Loss pred: 0.9412; Loss self: 0.0000; time: 0.54s
Val loss: 0.6050 score: 0.9388 time: 0.10s
Test loss: 0.6271 score: 0.8367 time: 0.08s
Epoch 122/1000, LR 0.000262
Train loss: 0.9378;  Loss pred: 0.9378; Loss self: 0.0000; time: 0.33s
Val loss: 0.6026 score: 0.9388 time: 0.08s
Test loss: 0.6253 score: 0.8367 time: 0.25s
Epoch 123/1000, LR 0.000262
Train loss: 0.9359;  Loss pred: 0.9359; Loss self: 0.0000; time: 0.61s
Val loss: 0.6001 score: 0.9388 time: 0.20s
Test loss: 0.6234 score: 0.8367 time: 0.28s
Epoch 124/1000, LR 0.000261
Train loss: 0.9335;  Loss pred: 0.9335; Loss self: 0.0000; time: 3.88s
Val loss: 0.5976 score: 0.9388 time: 3.01s
Test loss: 0.6215 score: 0.8367 time: 1.85s
Epoch 125/1000, LR 0.000261
Train loss: 0.9306;  Loss pred: 0.9306; Loss self: 0.0000; time: 7.28s
Val loss: 0.5950 score: 0.9388 time: 0.54s
Test loss: 0.6196 score: 0.8367 time: 0.45s
Epoch 126/1000, LR 0.000261
Train loss: 0.9289;  Loss pred: 0.9289; Loss self: 0.0000; time: 1.04s
Val loss: 0.5923 score: 0.9388 time: 0.17s
Test loss: 0.6176 score: 0.8367 time: 0.38s
Epoch 127/1000, LR 0.000261
Train loss: 0.9255;  Loss pred: 0.9255; Loss self: 0.0000; time: 0.87s
Val loss: 0.5897 score: 0.9388 time: 0.32s
Test loss: 0.6156 score: 0.8367 time: 0.09s
Epoch 128/1000, LR 0.000261
Train loss: 0.9238;  Loss pred: 0.9238; Loss self: 0.0000; time: 0.78s
Val loss: 0.5869 score: 0.9388 time: 0.36s
Test loss: 0.6136 score: 0.8367 time: 0.47s
Epoch 129/1000, LR 0.000261
Train loss: 0.9213;  Loss pred: 0.9213; Loss self: 0.0000; time: 6.09s
Val loss: 0.5842 score: 0.9388 time: 3.92s
Test loss: 0.6115 score: 0.8367 time: 3.39s
Epoch 130/1000, LR 0.000260
Train loss: 0.9213;  Loss pred: 0.9213; Loss self: 0.0000; time: 1.59s
Val loss: 0.5813 score: 0.9388 time: 0.12s
Test loss: 0.6094 score: 0.8367 time: 0.07s
Epoch 131/1000, LR 0.000260
Train loss: 0.9181;  Loss pred: 0.9181; Loss self: 0.0000; time: 0.33s
Val loss: 0.5784 score: 0.9388 time: 0.08s
Test loss: 0.6072 score: 0.8367 time: 0.07s
Epoch 132/1000, LR 0.000260
Train loss: 0.9164;  Loss pred: 0.9164; Loss self: 0.0000; time: 0.28s
Val loss: 0.5755 score: 0.9388 time: 0.08s
Test loss: 0.6050 score: 0.8163 time: 0.07s
Epoch 133/1000, LR 0.000260
Train loss: 0.9117;  Loss pred: 0.9117; Loss self: 0.0000; time: 0.26s
Val loss: 0.5725 score: 0.9388 time: 0.08s
Test loss: 0.6028 score: 0.8163 time: 0.11s
Epoch 134/1000, LR 0.000260
Train loss: 0.9092;  Loss pred: 0.9092; Loss self: 0.0000; time: 0.25s
Val loss: 0.5694 score: 0.9388 time: 0.08s
Test loss: 0.6004 score: 0.8163 time: 0.07s
Epoch 135/1000, LR 0.000260
Train loss: 0.9085;  Loss pred: 0.9085; Loss self: 0.0000; time: 0.26s
Val loss: 0.5663 score: 0.9388 time: 0.08s
Test loss: 0.5981 score: 0.8163 time: 0.08s
Epoch 136/1000, LR 0.000260
Train loss: 0.9054;  Loss pred: 0.9054; Loss self: 0.0000; time: 0.28s
Val loss: 0.5631 score: 0.9388 time: 0.15s
Test loss: 0.5957 score: 0.8163 time: 0.08s
Epoch 137/1000, LR 0.000259
Train loss: 0.9024;  Loss pred: 0.9024; Loss self: 0.0000; time: 0.25s
Val loss: 0.5599 score: 0.9388 time: 2.78s
Test loss: 0.5933 score: 0.8163 time: 3.32s
Epoch 138/1000, LR 0.000259
Train loss: 0.9009;  Loss pred: 0.9009; Loss self: 0.0000; time: 6.76s
Val loss: 0.5566 score: 0.9388 time: 0.95s
Test loss: 0.5908 score: 0.8163 time: 0.14s
Epoch 139/1000, LR 0.000259
Train loss: 0.8981;  Loss pred: 0.8981; Loss self: 0.0000; time: 0.25s
Val loss: 0.5533 score: 0.9388 time: 0.09s
Test loss: 0.5883 score: 0.8163 time: 2.70s
Epoch 140/1000, LR 0.000259
Train loss: 0.8952;  Loss pred: 0.8952; Loss self: 0.0000; time: 6.50s
Val loss: 0.5500 score: 0.9388 time: 1.08s
Test loss: 0.5858 score: 0.8163 time: 0.71s
Epoch 141/1000, LR 0.000259
Train loss: 0.8927;  Loss pred: 0.8927; Loss self: 0.0000; time: 0.85s
Val loss: 0.5466 score: 0.9388 time: 0.08s
Test loss: 0.5832 score: 0.8163 time: 0.07s
Epoch 142/1000, LR 0.000259
Train loss: 0.8894;  Loss pred: 0.8894; Loss self: 0.0000; time: 0.27s
Val loss: 0.5432 score: 0.9388 time: 0.08s
Test loss: 0.5807 score: 0.8163 time: 0.08s
Epoch 143/1000, LR 0.000258
Train loss: 0.8868;  Loss pred: 0.8868; Loss self: 0.0000; time: 0.27s
Val loss: 0.5397 score: 0.9388 time: 0.08s
Test loss: 0.5781 score: 0.8163 time: 0.07s
Epoch 144/1000, LR 0.000258
Train loss: 0.8831;  Loss pred: 0.8831; Loss self: 0.0000; time: 0.25s
Val loss: 0.5362 score: 0.9388 time: 0.08s
Test loss: 0.5755 score: 0.8163 time: 0.07s
Epoch 145/1000, LR 0.000258
Train loss: 0.8818;  Loss pred: 0.8818; Loss self: 0.0000; time: 0.24s
Val loss: 0.5327 score: 0.9388 time: 0.08s
Test loss: 0.5729 score: 0.8163 time: 0.07s
Epoch 146/1000, LR 0.000258
Train loss: 0.8775;  Loss pred: 0.8775; Loss self: 0.0000; time: 0.24s
Val loss: 0.5291 score: 0.9388 time: 0.08s
Test loss: 0.5703 score: 0.8163 time: 0.07s
Epoch 147/1000, LR 0.000258
Train loss: 0.8748;  Loss pred: 0.8748; Loss self: 0.0000; time: 0.24s
Val loss: 0.5254 score: 0.9388 time: 0.08s
Test loss: 0.5677 score: 0.8367 time: 0.07s
Epoch 148/1000, LR 0.000257
Train loss: 0.8721;  Loss pred: 0.8721; Loss self: 0.0000; time: 0.26s
Val loss: 0.5217 score: 0.9388 time: 0.78s
Test loss: 0.5651 score: 0.8367 time: 3.29s
Epoch 149/1000, LR 0.000257
Train loss: 0.8675;  Loss pred: 0.8675; Loss self: 0.0000; time: 4.39s
Val loss: 0.5180 score: 0.9388 time: 0.18s
Test loss: 0.5624 score: 0.8367 time: 0.56s
Epoch 150/1000, LR 0.000257
Train loss: 0.8667;  Loss pred: 0.8667; Loss self: 0.0000; time: 0.46s
Val loss: 0.5142 score: 0.9388 time: 0.08s
Test loss: 0.5597 score: 0.8367 time: 0.07s
Epoch 151/1000, LR 0.000257
Train loss: 0.8653;  Loss pred: 0.8653; Loss self: 0.0000; time: 0.24s
Val loss: 0.5104 score: 0.9388 time: 0.08s
Test loss: 0.5569 score: 0.8367 time: 0.07s
Epoch 152/1000, LR 0.000257
Train loss: 0.8607;  Loss pred: 0.8607; Loss self: 0.0000; time: 0.25s
Val loss: 0.5066 score: 0.9388 time: 0.08s
Test loss: 0.5541 score: 0.8367 time: 0.07s
Epoch 153/1000, LR 0.000257
Train loss: 0.8569;  Loss pred: 0.8569; Loss self: 0.0000; time: 0.24s
Val loss: 0.5027 score: 0.9388 time: 0.08s
Test loss: 0.5513 score: 0.8367 time: 0.07s
Epoch 154/1000, LR 0.000256
Train loss: 0.8543;  Loss pred: 0.8543; Loss self: 0.0000; time: 0.27s
Val loss: 0.4988 score: 0.9388 time: 0.10s
Test loss: 0.5484 score: 0.8367 time: 0.08s
Epoch 155/1000, LR 0.000256
Train loss: 0.8525;  Loss pred: 0.8525; Loss self: 0.0000; time: 2.46s
Val loss: 0.4949 score: 0.9388 time: 0.08s
Test loss: 0.5455 score: 0.8367 time: 0.07s
Epoch 156/1000, LR 0.000256
Train loss: 0.8512;  Loss pred: 0.8512; Loss self: 0.0000; time: 0.24s
Val loss: 0.4910 score: 0.9388 time: 0.08s
Test loss: 0.5426 score: 0.8367 time: 0.39s
Epoch 157/1000, LR 0.000256
Train loss: 0.8462;  Loss pred: 0.8462; Loss self: 0.0000; time: 5.02s
Val loss: 0.4871 score: 0.9388 time: 0.90s
Test loss: 0.5397 score: 0.8367 time: 0.30s
Epoch 158/1000, LR 0.000256
Train loss: 0.8431;  Loss pred: 0.8431; Loss self: 0.0000; time: 1.63s
Val loss: 0.4831 score: 0.9388 time: 0.08s
Test loss: 0.5368 score: 0.8367 time: 0.07s
Epoch 159/1000, LR 0.000255
Train loss: 0.8406;  Loss pred: 0.8406; Loss self: 0.0000; time: 0.24s
Val loss: 0.4792 score: 0.9388 time: 0.08s
Test loss: 0.5339 score: 0.8367 time: 0.07s
Epoch 160/1000, LR 0.000255
Train loss: 0.8385;  Loss pred: 0.8385; Loss self: 0.0000; time: 0.25s
Val loss: 0.4752 score: 0.9388 time: 0.08s
Test loss: 0.5310 score: 0.8367 time: 0.07s
Epoch 161/1000, LR 0.000255
Train loss: 0.8350;  Loss pred: 0.8350; Loss self: 0.0000; time: 0.25s
Val loss: 0.4712 score: 0.9388 time: 0.08s
Test loss: 0.5281 score: 0.8367 time: 0.07s
Epoch 162/1000, LR 0.000255
Train loss: 0.8326;  Loss pred: 0.8326; Loss self: 0.0000; time: 0.25s
Val loss: 0.4672 score: 0.9388 time: 0.08s
Test loss: 0.5252 score: 0.8367 time: 0.07s
Epoch 163/1000, LR 0.000255
Train loss: 0.8273;  Loss pred: 0.8273; Loss self: 0.0000; time: 0.25s
Val loss: 0.4632 score: 0.9388 time: 0.08s
Test loss: 0.5223 score: 0.8367 time: 0.07s
Epoch 164/1000, LR 0.000254
Train loss: 0.8259;  Loss pred: 0.8259; Loss self: 0.0000; time: 0.24s
Val loss: 0.4591 score: 0.9388 time: 0.08s
Test loss: 0.5194 score: 0.8367 time: 0.07s
Epoch 165/1000, LR 0.000254
Train loss: 0.8211;  Loss pred: 0.8211; Loss self: 0.0000; time: 0.24s
Val loss: 0.4551 score: 0.9388 time: 0.08s
Test loss: 0.5165 score: 0.8367 time: 0.07s
Epoch 166/1000, LR 0.000254
Train loss: 0.8194;  Loss pred: 0.8194; Loss self: 0.0000; time: 0.24s
Val loss: 0.4510 score: 0.9388 time: 0.08s
Test loss: 0.5136 score: 0.8367 time: 0.07s
Epoch 167/1000, LR 0.000254
Train loss: 0.8164;  Loss pred: 0.8164; Loss self: 0.0000; time: 0.24s
Val loss: 0.4469 score: 0.9388 time: 0.08s
Test loss: 0.5107 score: 0.8367 time: 0.07s
Epoch 168/1000, LR 0.000254
Train loss: 0.8130;  Loss pred: 0.8130; Loss self: 0.0000; time: 4.11s
Val loss: 0.4428 score: 0.9388 time: 2.41s
Test loss: 0.5078 score: 0.8367 time: 1.41s
Epoch 169/1000, LR 0.000253
Train loss: 0.8109;  Loss pred: 0.8109; Loss self: 0.0000; time: 6.05s
Val loss: 0.4388 score: 0.9388 time: 0.08s
Test loss: 0.5049 score: 0.8367 time: 0.07s
Epoch 170/1000, LR 0.000253
Train loss: 0.8075;  Loss pred: 0.8075; Loss self: 0.0000; time: 0.23s
Val loss: 0.4347 score: 0.9388 time: 0.07s
Test loss: 0.5019 score: 0.8367 time: 0.06s
Epoch 171/1000, LR 0.000253
Train loss: 0.8052;  Loss pred: 0.8052; Loss self: 0.0000; time: 0.23s
Val loss: 0.4306 score: 0.9388 time: 0.07s
Test loss: 0.4990 score: 0.8367 time: 0.06s
Epoch 172/1000, LR 0.000253
Train loss: 0.8009;  Loss pred: 0.8009; Loss self: 0.0000; time: 0.22s
Val loss: 0.4266 score: 0.9388 time: 0.07s
Test loss: 0.4961 score: 0.8367 time: 0.06s
Epoch 173/1000, LR 0.000253
Train loss: 0.7980;  Loss pred: 0.7980; Loss self: 0.0000; time: 0.22s
Val loss: 0.4226 score: 0.9388 time: 0.08s
Test loss: 0.4931 score: 0.8367 time: 0.06s
Epoch 174/1000, LR 0.000252
Train loss: 0.7952;  Loss pred: 0.7952; Loss self: 0.0000; time: 0.23s
Val loss: 0.4185 score: 0.9388 time: 0.07s
Test loss: 0.4902 score: 0.8367 time: 0.06s
Epoch 175/1000, LR 0.000252
Train loss: 0.7937;  Loss pred: 0.7937; Loss self: 0.0000; time: 0.23s
Val loss: 0.4145 score: 0.9388 time: 0.07s
Test loss: 0.4872 score: 0.8367 time: 0.06s
Epoch 176/1000, LR 0.000252
Train loss: 0.7874;  Loss pred: 0.7874; Loss self: 0.0000; time: 0.23s
Val loss: 0.4105 score: 0.9388 time: 0.07s
Test loss: 0.4843 score: 0.8367 time: 0.06s
Epoch 177/1000, LR 0.000252
Train loss: 0.7864;  Loss pred: 0.7864; Loss self: 0.0000; time: 0.23s
Val loss: 0.4065 score: 0.9388 time: 0.08s
Test loss: 0.4814 score: 0.8367 time: 0.07s
Epoch 178/1000, LR 0.000251
Train loss: 0.7847;  Loss pred: 0.7847; Loss self: 0.0000; time: 0.25s
Val loss: 0.4024 score: 0.9388 time: 0.08s
Test loss: 0.4785 score: 0.8367 time: 0.07s
Epoch 179/1000, LR 0.000251
Train loss: 0.7789;  Loss pred: 0.7789; Loss self: 0.0000; time: 0.33s
Val loss: 0.3984 score: 0.9388 time: 0.07s
Test loss: 0.4756 score: 0.8367 time: 0.06s
Epoch 180/1000, LR 0.000251
Train loss: 0.7772;  Loss pred: 0.7772; Loss self: 0.0000; time: 8.83s
Val loss: 0.3944 score: 0.9388 time: 3.04s
Test loss: 0.4728 score: 0.8367 time: 1.94s
Epoch 181/1000, LR 0.000251
Train loss: 0.7745;  Loss pred: 0.7745; Loss self: 0.0000; time: 2.19s
Val loss: 0.3903 score: 0.9388 time: 0.36s
Test loss: 0.4700 score: 0.8367 time: 0.31s
Epoch 182/1000, LR 0.000251
Train loss: 0.7717;  Loss pred: 0.7717; Loss self: 0.0000; time: 0.33s
Val loss: 0.3863 score: 0.9388 time: 0.47s
Test loss: 0.4671 score: 0.8367 time: 0.31s
Epoch 183/1000, LR 0.000250
Train loss: 0.7659;  Loss pred: 0.7659; Loss self: 0.0000; time: 0.73s
Val loss: 0.3823 score: 0.9388 time: 0.11s
Test loss: 0.4644 score: 0.8367 time: 0.34s
Epoch 184/1000, LR 0.000250
Train loss: 0.7652;  Loss pred: 0.7652; Loss self: 0.0000; time: 0.84s
Val loss: 0.3782 score: 0.9388 time: 0.65s
Test loss: 0.4616 score: 0.8367 time: 0.23s
Epoch 185/1000, LR 0.000250
Train loss: 0.7616;  Loss pred: 0.7616; Loss self: 0.0000; time: 1.02s
Val loss: 0.3742 score: 0.9388 time: 0.36s
Test loss: 0.4588 score: 0.8367 time: 2.41s
Epoch 186/1000, LR 0.000250
Train loss: 0.7600;  Loss pred: 0.7600; Loss self: 0.0000; time: 8.32s
Val loss: 0.3703 score: 0.9388 time: 0.35s
Test loss: 0.4560 score: 0.8367 time: 0.24s
Epoch 187/1000, LR 0.000249
Train loss: 0.7553;  Loss pred: 0.7553; Loss self: 0.0000; time: 0.32s
Val loss: 0.3663 score: 0.9388 time: 0.08s
Test loss: 0.4533 score: 0.8367 time: 0.07s
Epoch 188/1000, LR 0.000249
Train loss: 0.7524;  Loss pred: 0.7524; Loss self: 0.0000; time: 0.37s
Val loss: 0.3624 score: 0.9388 time: 0.26s
Test loss: 0.4505 score: 0.8367 time: 0.23s
Epoch 189/1000, LR 0.000249
Train loss: 0.7484;  Loss pred: 0.7484; Loss self: 0.0000; time: 1.02s
Val loss: 0.3585 score: 0.9388 time: 0.36s
Test loss: 0.4478 score: 0.8367 time: 3.88s
Epoch 190/1000, LR 0.000249
Train loss: 0.7479;  Loss pred: 0.7479; Loss self: 0.0000; time: 3.73s
Val loss: 0.3546 score: 0.9388 time: 3.56s
Test loss: 0.4451 score: 0.8367 time: 2.06s
Epoch 191/1000, LR 0.000249
Train loss: 0.7442;  Loss pred: 0.7442; Loss self: 0.0000; time: 1.76s
Val loss: 0.3507 score: 0.9388 time: 0.31s
Test loss: 0.4424 score: 0.8367 time: 0.08s
Epoch 192/1000, LR 0.000248
Train loss: 0.7425;  Loss pred: 0.7425; Loss self: 0.0000; time: 0.24s
Val loss: 0.3469 score: 0.9388 time: 0.08s
Test loss: 0.4398 score: 0.8367 time: 0.15s
Epoch 193/1000, LR 0.000248
Train loss: 0.7393;  Loss pred: 0.7393; Loss self: 0.0000; time: 0.72s
Val loss: 0.3431 score: 0.9388 time: 0.21s
Test loss: 0.4371 score: 0.8367 time: 0.42s
Epoch 194/1000, LR 0.000248
Train loss: 0.7362;  Loss pred: 0.7362; Loss self: 0.0000; time: 1.03s
Val loss: 0.3394 score: 0.9388 time: 0.37s
Test loss: 0.4344 score: 0.8367 time: 0.46s
Epoch 195/1000, LR 0.000248
Train loss: 0.7344;  Loss pred: 0.7344; Loss self: 0.0000; time: 3.49s
Val loss: 0.3357 score: 0.9388 time: 3.28s
Test loss: 0.4317 score: 0.8367 time: 3.34s
Epoch 196/1000, LR 0.000247
Train loss: 0.7306;  Loss pred: 0.7306; Loss self: 0.0000; time: 6.87s
Val loss: 0.3321 score: 0.9388 time: 0.09s
Test loss: 0.4291 score: 0.8367 time: 0.09s
Epoch 197/1000, LR 0.000247
Train loss: 0.7284;  Loss pred: 0.7284; Loss self: 0.0000; time: 0.35s
Val loss: 0.3285 score: 0.9388 time: 2.94s
Test loss: 0.4264 score: 0.8367 time: 4.55s
Epoch 198/1000, LR 0.000247
Train loss: 0.7269;  Loss pred: 0.7269; Loss self: 0.0000; time: 2.89s
Val loss: 0.3248 score: 0.9388 time: 0.10s
Test loss: 0.4239 score: 0.8367 time: 0.07s
Epoch 199/1000, LR 0.000247
Train loss: 0.7243;  Loss pred: 0.7243; Loss self: 0.0000; time: 0.24s
Val loss: 0.3212 score: 0.9388 time: 0.08s
Test loss: 0.4215 score: 0.8367 time: 0.07s
Epoch 200/1000, LR 0.000246
Train loss: 0.7202;  Loss pred: 0.7202; Loss self: 0.0000; time: 0.24s
Val loss: 0.3176 score: 0.9388 time: 0.07s
Test loss: 0.4191 score: 0.8367 time: 0.06s
Epoch 201/1000, LR 0.000246
Train loss: 0.7205;  Loss pred: 0.7205; Loss self: 0.0000; time: 0.33s
Val loss: 0.3141 score: 0.9388 time: 0.08s
Test loss: 0.4167 score: 0.8367 time: 0.07s
Epoch 202/1000, LR 0.000246
Train loss: 0.7150;  Loss pred: 0.7150; Loss self: 0.0000; time: 0.23s
Val loss: 0.3106 score: 0.9388 time: 0.08s
Test loss: 0.4144 score: 0.8367 time: 0.07s
Epoch 203/1000, LR 0.000246
Train loss: 0.7117;  Loss pred: 0.7117; Loss self: 0.0000; time: 0.24s
Val loss: 0.3071 score: 0.9388 time: 0.08s
Test loss: 0.4121 score: 0.8367 time: 0.07s
Epoch 204/1000, LR 0.000245
Train loss: 0.7103;  Loss pred: 0.7103; Loss self: 0.0000; time: 0.23s
Val loss: 0.3037 score: 0.9388 time: 0.09s
Test loss: 0.4098 score: 0.8367 time: 0.15s
Epoch 205/1000, LR 0.000245
Train loss: 0.7074;  Loss pred: 0.7074; Loss self: 0.0000; time: 0.29s
Val loss: 0.3004 score: 0.9388 time: 0.09s
Test loss: 0.4075 score: 0.8367 time: 0.08s
Epoch 206/1000, LR 0.000245
Train loss: 0.7037;  Loss pred: 0.7037; Loss self: 0.0000; time: 0.37s
Val loss: 0.2970 score: 0.9388 time: 0.21s
Test loss: 0.4052 score: 0.8367 time: 0.40s
Epoch 207/1000, LR 0.000245
Train loss: 0.7011;  Loss pred: 0.7011; Loss self: 0.0000; time: 1.09s
Val loss: 0.2938 score: 0.9388 time: 3.32s
Test loss: 0.4029 score: 0.8367 time: 2.64s
Epoch 208/1000, LR 0.000244
Train loss: 0.7024;  Loss pred: 0.7024; Loss self: 0.0000; time: 7.95s
Val loss: 0.2906 score: 0.9388 time: 0.37s
Test loss: 0.4006 score: 0.8367 time: 0.44s
Epoch 209/1000, LR 0.000244
Train loss: 0.6966;  Loss pred: 0.6966; Loss self: 0.0000; time: 0.40s
Val loss: 0.2874 score: 0.9388 time: 0.09s
Test loss: 0.3984 score: 0.8367 time: 0.08s
Epoch 210/1000, LR 0.000244
Train loss: 0.6963;  Loss pred: 0.6963; Loss self: 0.0000; time: 0.30s
Val loss: 0.2842 score: 0.9388 time: 0.09s
Test loss: 0.3962 score: 0.8367 time: 0.08s
Epoch 211/1000, LR 0.000244
Train loss: 0.6939;  Loss pred: 0.6939; Loss self: 0.0000; time: 0.48s
Val loss: 0.2811 score: 0.9388 time: 0.17s
Test loss: 0.3940 score: 0.8367 time: 0.08s
Epoch 212/1000, LR 0.000243
Train loss: 0.6915;  Loss pred: 0.6915; Loss self: 0.0000; time: 0.27s
Val loss: 0.2780 score: 0.9388 time: 0.09s
Test loss: 0.3919 score: 0.8367 time: 0.10s
Epoch 213/1000, LR 0.000243
Train loss: 0.6876;  Loss pred: 0.6876; Loss self: 0.0000; time: 0.26s
Val loss: 0.2750 score: 0.9388 time: 0.09s
Test loss: 0.3899 score: 0.8367 time: 0.08s
Epoch 214/1000, LR 0.000243
Train loss: 0.6866;  Loss pred: 0.6866; Loss self: 0.0000; time: 0.27s
Val loss: 0.2720 score: 0.9388 time: 0.18s
Test loss: 0.3879 score: 0.8367 time: 0.08s
Epoch 215/1000, LR 0.000243
Train loss: 0.6860;  Loss pred: 0.6860; Loss self: 0.0000; time: 1.01s
Val loss: 0.2690 score: 0.9388 time: 0.36s
Test loss: 0.3859 score: 0.8367 time: 0.43s
Epoch 216/1000, LR 0.000242
Train loss: 0.6805;  Loss pred: 0.6805; Loss self: 0.0000; time: 0.85s
Val loss: 0.2661 score: 0.9388 time: 0.16s
Test loss: 0.3840 score: 0.8367 time: 0.44s
Epoch 217/1000, LR 0.000242
Train loss: 0.6797;  Loss pred: 0.6797; Loss self: 0.0000; time: 9.47s
Val loss: 0.2632 score: 0.9388 time: 1.26s
Test loss: 0.3821 score: 0.8367 time: 0.44s
Epoch 218/1000, LR 0.000242
Train loss: 0.6781;  Loss pred: 0.6781; Loss self: 0.0000; time: 1.21s
Val loss: 0.2604 score: 0.9388 time: 0.37s
Test loss: 0.3802 score: 0.8367 time: 0.44s
Epoch 219/1000, LR 0.000242
Train loss: 0.6771;  Loss pred: 0.6771; Loss self: 0.0000; time: 0.80s
Val loss: 0.2576 score: 0.9388 time: 0.32s
Test loss: 0.3783 score: 0.8367 time: 0.38s
Epoch 220/1000, LR 0.000241
Train loss: 0.6736;  Loss pred: 0.6736; Loss self: 0.0000; time: 0.38s
Val loss: 0.2548 score: 0.9388 time: 3.21s
Test loss: 0.3764 score: 0.8367 time: 2.99s
Epoch 221/1000, LR 0.000241
Train loss: 0.6718;  Loss pred: 0.6718; Loss self: 0.0000; time: 9.17s
Val loss: 0.2521 score: 0.9388 time: 1.91s
Test loss: 0.3746 score: 0.8367 time: 0.46s
Epoch 222/1000, LR 0.000241
Train loss: 0.6683;  Loss pred: 0.6683; Loss self: 0.0000; time: 0.41s
Val loss: 0.2495 score: 0.9388 time: 0.33s
Test loss: 0.3728 score: 0.8367 time: 0.42s
Epoch 223/1000, LR 0.000241
Train loss: 0.6677;  Loss pred: 0.6677; Loss self: 0.0000; time: 0.45s
Val loss: 0.2468 score: 0.9388 time: 0.09s
Test loss: 0.3710 score: 0.8367 time: 0.18s
Epoch 224/1000, LR 0.000240
Train loss: 0.6647;  Loss pred: 0.6647; Loss self: 0.0000; time: 1.00s
Val loss: 0.2442 score: 0.9388 time: 0.35s
Test loss: 0.3693 score: 0.8367 time: 0.46s
Epoch 225/1000, LR 0.000240
Train loss: 0.6645;  Loss pred: 0.6645; Loss self: 0.0000; time: 0.69s
Val loss: 0.2417 score: 0.9388 time: 0.14s
Test loss: 0.3677 score: 0.8367 time: 0.72s
Epoch 226/1000, LR 0.000240
Train loss: 0.6629;  Loss pred: 0.6629; Loss self: 0.0000; time: 9.65s
Val loss: 0.2391 score: 0.9388 time: 2.98s
Test loss: 0.3661 score: 0.8367 time: 0.08s
Epoch 227/1000, LR 0.000240
Train loss: 0.6596;  Loss pred: 0.6596; Loss self: 0.0000; time: 0.28s
Val loss: 0.2367 score: 0.9388 time: 0.10s
Test loss: 0.3644 score: 0.8367 time: 0.12s
Epoch 228/1000, LR 0.000239
Train loss: 0.6585;  Loss pred: 0.6585; Loss self: 0.0000; time: 0.93s
Val loss: 0.2342 score: 0.9388 time: 0.08s
Test loss: 0.3628 score: 0.8367 time: 0.15s
Epoch 229/1000, LR 0.000239
Train loss: 0.6557;  Loss pred: 0.6557; Loss self: 0.0000; time: 0.32s
Val loss: 0.2319 score: 0.9388 time: 0.09s
Test loss: 0.3612 score: 0.8367 time: 0.08s
Epoch 230/1000, LR 0.000239
Train loss: 0.6540;  Loss pred: 0.6540; Loss self: 0.0000; time: 0.26s
Val loss: 0.2295 score: 0.9388 time: 0.08s
Test loss: 0.3596 score: 0.8367 time: 2.78s
Epoch 231/1000, LR 0.000238
Train loss: 0.6517;  Loss pred: 0.6517; Loss self: 0.0000; time: 5.81s
Val loss: 0.2272 score: 0.9388 time: 1.54s
Test loss: 0.3581 score: 0.8367 time: 0.20s
Epoch 232/1000, LR 0.000238
Train loss: 0.6527;  Loss pred: 0.6527; Loss self: 0.0000; time: 0.34s
Val loss: 0.2249 score: 0.9388 time: 0.09s
Test loss: 0.3566 score: 0.8367 time: 0.07s
Epoch 233/1000, LR 0.000238
Train loss: 0.6490;  Loss pred: 0.6490; Loss self: 0.0000; time: 0.27s
Val loss: 0.2227 score: 0.9388 time: 0.18s
Test loss: 0.3551 score: 0.8367 time: 0.16s
Epoch 234/1000, LR 0.000238
Train loss: 0.6468;  Loss pred: 0.6468; Loss self: 0.0000; time: 0.27s
Val loss: 0.2205 score: 0.9388 time: 0.10s
Test loss: 0.3536 score: 0.8367 time: 0.08s
Epoch 235/1000, LR 0.000237
Train loss: 0.6484;  Loss pred: 0.6484; Loss self: 0.0000; time: 0.27s
Val loss: 0.2184 score: 0.9388 time: 0.21s
Test loss: 0.3521 score: 0.8367 time: 0.09s
Epoch 236/1000, LR 0.000237
Train loss: 0.6424;  Loss pred: 0.6424; Loss self: 0.0000; time: 0.57s
Val loss: 0.2162 score: 0.9388 time: 0.15s
Test loss: 0.3507 score: 0.8367 time: 0.41s
Epoch 237/1000, LR 0.000237
Train loss: 0.6408;  Loss pred: 0.6408; Loss self: 0.0000; time: 1.02s
Val loss: 0.2141 score: 0.9388 time: 0.38s
Test loss: 0.3495 score: 0.8367 time: 2.49s
Epoch 238/1000, LR 0.000236
Train loss: 0.6400;  Loss pred: 0.6400; Loss self: 0.0000; time: 2.10s
Val loss: 0.2120 score: 0.9388 time: 3.26s
Test loss: 0.3483 score: 0.8367 time: 3.79s
Epoch 239/1000, LR 0.000236
Train loss: 0.6407;  Loss pred: 0.6407; Loss self: 0.0000; time: 4.15s
Val loss: 0.2099 score: 0.9388 time: 0.25s
Test loss: 0.3471 score: 0.8367 time: 0.13s
Epoch 240/1000, LR 0.000236
Train loss: 0.6392;  Loss pred: 0.6392; Loss self: 0.0000; time: 1.07s
Val loss: 0.2079 score: 0.9388 time: 0.19s
Test loss: 0.3458 score: 0.8367 time: 0.16s
Epoch 241/1000, LR 0.000236
Train loss: 0.6353;  Loss pred: 0.6353; Loss self: 0.0000; time: 0.60s
Val loss: 0.2059 score: 0.9388 time: 0.44s
Test loss: 0.3446 score: 0.8367 time: 0.15s
Epoch 242/1000, LR 0.000235
Train loss: 0.6348;  Loss pred: 0.6348; Loss self: 0.0000; time: 1.13s
Val loss: 0.2040 score: 0.9388 time: 4.23s
Test loss: 0.3433 score: 0.8367 time: 2.01s
Epoch 243/1000, LR 0.000235
Train loss: 0.6333;  Loss pred: 0.6333; Loss self: 0.0000; time: 4.24s
Val loss: 0.2021 score: 0.9388 time: 0.25s
Test loss: 0.3421 score: 0.8367 time: 0.23s
Epoch 244/1000, LR 0.000235
Train loss: 0.6328;  Loss pred: 0.6328; Loss self: 0.0000; time: 0.84s
Val loss: 0.2002 score: 0.9388 time: 0.33s
Test loss: 0.3409 score: 0.8367 time: 0.44s
Epoch 245/1000, LR 0.000234
Train loss: 0.6311;  Loss pred: 0.6311; Loss self: 0.0000; time: 1.02s
Val loss: 0.1983 score: 0.9388 time: 0.29s
Test loss: 0.3397 score: 0.8367 time: 0.08s
Epoch 246/1000, LR 0.000234
Train loss: 0.6298;  Loss pred: 0.6298; Loss self: 0.0000; time: 0.77s
Val loss: 0.1965 score: 0.9388 time: 0.21s
Test loss: 0.3386 score: 0.8367 time: 0.44s
Epoch 247/1000, LR 0.000234
Train loss: 0.6286;  Loss pred: 0.6286; Loss self: 0.0000; time: 8.42s
Val loss: 0.1948 score: 0.9388 time: 3.55s
Test loss: 0.3374 score: 0.8367 time: 1.57s
Epoch 248/1000, LR 0.000234
Train loss: 0.6258;  Loss pred: 0.6258; Loss self: 0.0000; time: 1.03s
Val loss: 0.1930 score: 0.9388 time: 0.14s
Test loss: 0.3363 score: 0.8367 time: 0.40s
Epoch 249/1000, LR 0.000233
Train loss: 0.6246;  Loss pred: 0.6246; Loss self: 0.0000; time: 0.79s
Val loss: 0.1913 score: 0.9388 time: 0.38s
Test loss: 0.3352 score: 0.8367 time: 0.44s
Epoch 250/1000, LR 0.000233
Train loss: 0.6233;  Loss pred: 0.6233; Loss self: 0.0000; time: 1.91s
Val loss: 0.1896 score: 0.9388 time: 2.49s
Test loss: 0.3342 score: 0.8367 time: 4.05s
Epoch 251/1000, LR 0.000233
Train loss: 0.6227;  Loss pred: 0.6227; Loss self: 0.0000; time: 8.38s
Val loss: 0.1879 score: 0.9388 time: 0.10s
Test loss: 0.3331 score: 0.8367 time: 0.08s
Epoch 252/1000, LR 0.000232
Train loss: 0.6206;  Loss pred: 0.6206; Loss self: 0.0000; time: 0.27s
Val loss: 0.1863 score: 0.9388 time: 2.39s
Test loss: 0.3321 score: 0.8367 time: 3.48s
Epoch 253/1000, LR 0.000232
Train loss: 0.6197;  Loss pred: 0.6197; Loss self: 0.0000; time: 4.71s
Val loss: 0.1847 score: 0.9388 time: 1.00s
Test loss: 0.3312 score: 0.8367 time: 2.60s
Epoch 254/1000, LR 0.000232
Train loss: 0.6192;  Loss pred: 0.6192; Loss self: 0.0000; time: 2.48s
Val loss: 0.1831 score: 0.9388 time: 2.51s
Test loss: 0.3302 score: 0.8367 time: 1.51s
Epoch 255/1000, LR 0.000232
Train loss: 0.6171;  Loss pred: 0.6171; Loss self: 0.0000; time: 1.01s
Val loss: 0.1815 score: 0.9388 time: 0.37s
Test loss: 0.3293 score: 0.8367 time: 0.58s
Epoch 256/1000, LR 0.000231
Train loss: 0.6184;  Loss pred: 0.6184; Loss self: 0.0000; time: 0.66s
Val loss: 0.1800 score: 0.9388 time: 0.56s
Test loss: 0.3284 score: 0.8367 time: 0.45s
Epoch 257/1000, LR 0.000231
Train loss: 0.6158;  Loss pred: 0.6158; Loss self: 0.0000; time: 7.55s
Val loss: 0.1785 score: 0.9388 time: 2.60s
Test loss: 0.3274 score: 0.8367 time: 2.73s
Epoch 258/1000, LR 0.000231
Train loss: 0.6131;  Loss pred: 0.6131; Loss self: 0.0000; time: 5.29s
Val loss: 0.1770 score: 0.9388 time: 2.08s
Test loss: 0.3265 score: 0.8367 time: 0.25s
Epoch 259/1000, LR 0.000230
Train loss: 0.6126;  Loss pred: 0.6126; Loss self: 0.0000; time: 1.00s
Val loss: 0.1756 score: 0.9388 time: 0.09s
Test loss: 0.3256 score: 0.8367 time: 0.08s
Epoch 260/1000, LR 0.000230
Train loss: 0.6096;  Loss pred: 0.6096; Loss self: 0.0000; time: 0.30s
Val loss: 0.1742 score: 0.9388 time: 0.09s
Test loss: 0.3247 score: 0.8367 time: 0.08s
Epoch 261/1000, LR 0.000230
Train loss: 0.6105;  Loss pred: 0.6105; Loss self: 0.0000; time: 0.33s
Val loss: 0.1728 score: 0.9388 time: 0.12s
Test loss: 0.3238 score: 0.8367 time: 0.10s
Epoch 262/1000, LR 0.000229
Train loss: 0.6110;  Loss pred: 0.6110; Loss self: 0.0000; time: 0.28s
Val loss: 0.1714 score: 0.9388 time: 0.09s
Test loss: 0.3230 score: 0.8367 time: 0.14s
Epoch 263/1000, LR 0.000229
Train loss: 0.6071;  Loss pred: 0.6071; Loss self: 0.0000; time: 0.33s
Val loss: 0.1700 score: 0.9388 time: 0.09s
Test loss: 0.3221 score: 0.8367 time: 0.12s
Epoch 264/1000, LR 0.000229
Train loss: 0.6080;  Loss pred: 0.6080; Loss self: 0.0000; time: 0.26s
Val loss: 0.1687 score: 0.9388 time: 0.09s
Test loss: 0.3213 score: 0.8367 time: 0.08s
Epoch 265/1000, LR 0.000228
Train loss: 0.6078;  Loss pred: 0.6078; Loss self: 0.0000; time: 0.65s
Val loss: 0.1674 score: 0.9388 time: 0.11s
Test loss: 0.3205 score: 0.8367 time: 0.08s
Epoch 266/1000, LR 0.000228
Train loss: 0.6069;  Loss pred: 0.6069; Loss self: 0.0000; time: 0.29s
Val loss: 0.1661 score: 0.9388 time: 0.22s
Test loss: 0.3198 score: 0.8367 time: 0.42s
Epoch 267/1000, LR 0.000228
Train loss: 0.6036;  Loss pred: 0.6036; Loss self: 0.0000; time: 1.04s
Val loss: 0.1649 score: 0.9388 time: 3.30s
Test loss: 0.3190 score: 0.8367 time: 2.29s
Epoch 268/1000, LR 0.000228
Train loss: 0.6027;  Loss pred: 0.6027; Loss self: 0.0000; time: 6.42s
Val loss: 0.1636 score: 0.9388 time: 2.52s
Test loss: 0.3183 score: 0.8367 time: 2.26s
Epoch 269/1000, LR 0.000227
Train loss: 0.6041;  Loss pred: 0.6041; Loss self: 0.0000; time: 9.85s
Val loss: 0.1624 score: 0.9388 time: 4.12s
Test loss: 0.3177 score: 0.8367 time: 2.49s
Epoch 270/1000, LR 0.000227
Train loss: 0.6010;  Loss pred: 0.6010; Loss self: 0.0000; time: 0.27s
Val loss: 0.1612 score: 0.9388 time: 0.09s
Test loss: 0.3170 score: 0.8367 time: 0.08s
Epoch 271/1000, LR 0.000227
Train loss: 0.6027;  Loss pred: 0.6027; Loss self: 0.0000; time: 0.29s
Val loss: 0.1600 score: 0.9388 time: 0.08s
Test loss: 0.3163 score: 0.8367 time: 0.08s
Epoch 272/1000, LR 0.000226
Train loss: 0.5997;  Loss pred: 0.5997; Loss self: 0.0000; time: 0.32s
Val loss: 0.1589 score: 0.9388 time: 2.61s
Test loss: 0.3156 score: 0.8367 time: 3.02s
Epoch 273/1000, LR 0.000226
Train loss: 0.5995;  Loss pred: 0.5995; Loss self: 0.0000; time: 6.75s
Val loss: 0.1577 score: 0.9388 time: 0.65s
Test loss: 0.3150 score: 0.8367 time: 0.41s
Epoch 274/1000, LR 0.000226
Train loss: 0.5980;  Loss pred: 0.5980; Loss self: 0.0000; time: 0.44s
Val loss: 0.1566 score: 0.9388 time: 0.10s
Test loss: 0.3143 score: 0.8367 time: 0.23s
Epoch 275/1000, LR 0.000225
Train loss: 0.5970;  Loss pred: 0.5970; Loss self: 0.0000; time: 0.54s
Val loss: 0.1555 score: 0.9388 time: 0.09s
Test loss: 0.3138 score: 0.8367 time: 0.16s
Epoch 276/1000, LR 0.000225
Train loss: 0.5977;  Loss pred: 0.5977; Loss self: 0.0000; time: 0.34s
Val loss: 0.1544 score: 0.9388 time: 0.10s
Test loss: 0.3131 score: 0.8367 time: 0.13s
Epoch 277/1000, LR 0.000225
Train loss: 0.5964;  Loss pred: 0.5964; Loss self: 0.0000; time: 0.29s
Val loss: 0.1534 score: 0.9388 time: 0.27s
Test loss: 0.3125 score: 0.8367 time: 0.43s
Epoch 278/1000, LR 0.000224
Train loss: 0.5928;  Loss pred: 0.5928; Loss self: 0.0000; time: 0.38s
Val loss: 0.1523 score: 0.9388 time: 0.36s
Test loss: 0.3119 score: 0.8367 time: 0.52s
Epoch 279/1000, LR 0.000224
Train loss: 0.5936;  Loss pred: 0.5936; Loss self: 0.0000; time: 8.05s
Val loss: 0.1513 score: 0.9388 time: 1.65s
Test loss: 0.3113 score: 0.8367 time: 2.66s
Epoch 280/1000, LR 0.000224
Train loss: 0.5931;  Loss pred: 0.5931; Loss self: 0.0000; time: 4.68s
Val loss: 0.1503 score: 0.9388 time: 1.57s
Test loss: 0.3106 score: 0.8367 time: 2.24s
Epoch 281/1000, LR 0.000223
Train loss: 0.5908;  Loss pred: 0.5908; Loss self: 0.0000; time: 0.78s
Val loss: 0.1493 score: 0.9388 time: 0.43s
Test loss: 0.3101 score: 0.8367 time: 0.26s
Epoch 282/1000, LR 0.000223
Train loss: 0.5940;  Loss pred: 0.5940; Loss self: 0.0000; time: 1.01s
Val loss: 0.1483 score: 0.9388 time: 0.36s
Test loss: 0.3095 score: 0.8367 time: 0.25s
Epoch 283/1000, LR 0.000223
Train loss: 0.5894;  Loss pred: 0.5894; Loss self: 0.0000; time: 0.94s
Val loss: 0.1473 score: 0.9388 time: 0.39s
Test loss: 0.3089 score: 0.8367 time: 0.45s
Epoch 284/1000, LR 0.000222
Train loss: 0.5898;  Loss pred: 0.5898; Loss self: 0.0000; time: 6.69s
Val loss: 0.1464 score: 0.9388 time: 2.89s
Test loss: 0.3084 score: 0.8367 time: 3.22s
Epoch 285/1000, LR 0.000222
Train loss: 0.5872;  Loss pred: 0.5872; Loss self: 0.0000; time: 3.59s
Val loss: 0.1455 score: 0.9388 time: 1.36s
Test loss: 0.3078 score: 0.8367 time: 1.36s
Epoch 286/1000, LR 0.000222
Train loss: 0.5873;  Loss pred: 0.5873; Loss self: 0.0000; time: 2.50s
Val loss: 0.1446 score: 0.9388 time: 0.25s
Test loss: 0.3073 score: 0.8367 time: 0.22s
Epoch 287/1000, LR 0.000221
Train loss: 0.5873;  Loss pred: 0.5873; Loss self: 0.0000; time: 0.36s
Val loss: 0.1437 score: 0.9388 time: 0.08s
Test loss: 0.3068 score: 0.8367 time: 0.07s
Epoch 288/1000, LR 0.000221
Train loss: 0.5862;  Loss pred: 0.5862; Loss self: 0.0000; time: 0.45s
Val loss: 0.1428 score: 0.9388 time: 0.10s
Test loss: 0.3063 score: 0.8367 time: 0.07s
Epoch 289/1000, LR 0.000221
Train loss: 0.5852;  Loss pred: 0.5852; Loss self: 0.0000; time: 0.44s
Val loss: 0.1419 score: 0.9388 time: 0.38s
Test loss: 0.3059 score: 0.8367 time: 0.26s
Epoch 290/1000, LR 0.000220
Train loss: 0.5838;  Loss pred: 0.5838; Loss self: 0.0000; time: 0.41s
Val loss: 0.1410 score: 0.9388 time: 0.31s
Test loss: 0.3054 score: 0.8367 time: 0.07s
Epoch 291/1000, LR 0.000220
Train loss: 0.5853;  Loss pred: 0.5853; Loss self: 0.0000; time: 0.66s
Val loss: 0.1402 score: 0.9388 time: 3.32s
Test loss: 0.3049 score: 0.8367 time: 3.31s
Epoch 292/1000, LR 0.000220
Train loss: 0.5850;  Loss pred: 0.5850; Loss self: 0.0000; time: 5.10s
Val loss: 0.1394 score: 0.9388 time: 0.12s
Test loss: 0.3045 score: 0.8367 time: 0.12s
Epoch 293/1000, LR 0.000219
Train loss: 0.5820;  Loss pred: 0.5820; Loss self: 0.0000; time: 0.30s
Val loss: 0.1386 score: 0.9388 time: 0.29s
Test loss: 0.3040 score: 0.8367 time: 0.08s
Epoch 294/1000, LR 0.000219
Train loss: 0.5818;  Loss pred: 0.5818; Loss self: 0.0000; time: 0.33s
Val loss: 0.1378 score: 0.9388 time: 0.09s
Test loss: 0.3036 score: 0.8367 time: 0.09s
Epoch 295/1000, LR 0.000219
Train loss: 0.5805;  Loss pred: 0.5805; Loss self: 0.0000; time: 0.34s
Val loss: 0.1370 score: 0.9388 time: 3.13s
Test loss: 0.3032 score: 0.8367 time: 3.58s
Epoch 296/1000, LR 0.000218
Train loss: 0.5807;  Loss pred: 0.5807; Loss self: 0.0000; time: 3.18s
Val loss: 0.1362 score: 0.9388 time: 0.11s
Test loss: 0.3028 score: 0.8367 time: 0.23s
Epoch 297/1000, LR 0.000218
Train loss: 0.5801;  Loss pred: 0.5801; Loss self: 0.0000; time: 0.25s
Val loss: 0.1355 score: 0.9388 time: 0.08s
Test loss: 0.3024 score: 0.8367 time: 0.07s
Epoch 298/1000, LR 0.000218
Train loss: 0.5775;  Loss pred: 0.5775; Loss self: 0.0000; time: 0.24s
Val loss: 0.1348 score: 0.9388 time: 0.08s
Test loss: 0.3020 score: 0.8367 time: 0.07s
Epoch 299/1000, LR 0.000217
Train loss: 0.5784;  Loss pred: 0.5784; Loss self: 0.0000; time: 0.24s
Val loss: 0.1340 score: 0.9388 time: 0.08s
Test loss: 0.3016 score: 0.8367 time: 0.07s
Epoch 300/1000, LR 0.000217
Train loss: 0.5786;  Loss pred: 0.5786; Loss self: 0.0000; time: 0.24s
Val loss: 0.1333 score: 0.9388 time: 0.08s
Test loss: 0.3013 score: 0.8367 time: 0.07s
Epoch 301/1000, LR 0.000217
Train loss: 0.5788;  Loss pred: 0.5788; Loss self: 0.0000; time: 0.24s
Val loss: 0.1326 score: 0.9388 time: 0.07s
Test loss: 0.3009 score: 0.8367 time: 0.07s
Epoch 302/1000, LR 0.000216
Train loss: 0.5773;  Loss pred: 0.5773; Loss self: 0.0000; time: 0.24s
Val loss: 0.1319 score: 0.9388 time: 0.08s
Test loss: 0.3005 score: 0.8367 time: 0.07s
Epoch 303/1000, LR 0.000216
Train loss: 0.5766;  Loss pred: 0.5766; Loss self: 0.0000; time: 0.24s
Val loss: 0.1312 score: 0.9388 time: 0.08s
Test loss: 0.3002 score: 0.8367 time: 0.07s
Epoch 304/1000, LR 0.000216
Train loss: 0.5754;  Loss pred: 0.5754; Loss self: 0.0000; time: 0.23s
Val loss: 0.1305 score: 0.9388 time: 0.08s
Test loss: 0.2998 score: 0.8367 time: 0.07s
Epoch 305/1000, LR 0.000215
Train loss: 0.5749;  Loss pred: 0.5749; Loss self: 0.0000; time: 0.26s
Val loss: 0.1298 score: 0.9388 time: 2.79s
Test loss: 0.2994 score: 0.8367 time: 2.43s
Epoch 306/1000, LR 0.000215
Train loss: 0.5742;  Loss pred: 0.5742; Loss self: 0.0000; time: 0.28s
Val loss: 0.1291 score: 0.9388 time: 0.08s
Test loss: 0.2991 score: 0.8367 time: 0.07s
Epoch 307/1000, LR 0.000215
Train loss: 0.5724;  Loss pred: 0.5724; Loss self: 0.0000; time: 0.24s
Val loss: 0.1284 score: 0.9388 time: 0.08s
Test loss: 0.2988 score: 0.8367 time: 0.07s
Epoch 308/1000, LR 0.000214
Train loss: 0.5725;  Loss pred: 0.5725; Loss self: 0.0000; time: 0.24s
Val loss: 0.1278 score: 0.9388 time: 0.08s
Test loss: 0.2985 score: 0.8367 time: 0.07s
Epoch 309/1000, LR 0.000214
Train loss: 0.5733;  Loss pred: 0.5733; Loss self: 0.0000; time: 0.24s
Val loss: 0.1271 score: 0.9388 time: 0.08s
Test loss: 0.2982 score: 0.8367 time: 0.07s
Epoch 310/1000, LR 0.000214
Train loss: 0.5724;  Loss pred: 0.5724; Loss self: 0.0000; time: 0.24s
Val loss: 0.1265 score: 0.9592 time: 0.08s
Test loss: 0.2979 score: 0.8367 time: 0.07s
Epoch 311/1000, LR 0.000213
Train loss: 0.5702;  Loss pred: 0.5702; Loss self: 0.0000; time: 0.23s
Val loss: 0.1259 score: 0.9592 time: 0.08s
Test loss: 0.2976 score: 0.8367 time: 0.08s
Epoch 312/1000, LR 0.000213
Train loss: 0.5705;  Loss pred: 0.5705; Loss self: 0.0000; time: 0.42s
Val loss: 0.1253 score: 0.9592 time: 1.70s
Test loss: 0.2973 score: 0.8367 time: 0.11s
Epoch 313/1000, LR 0.000213
Train loss: 0.5708;  Loss pred: 0.5708; Loss self: 0.0000; time: 0.26s
Val loss: 0.1247 score: 0.9592 time: 0.11s
Test loss: 0.2970 score: 0.8367 time: 0.07s
Epoch 314/1000, LR 0.000212
Train loss: 0.5709;  Loss pred: 0.5709; Loss self: 0.0000; time: 0.24s
Val loss: 0.1241 score: 0.9592 time: 0.08s
Test loss: 0.2967 score: 0.8367 time: 0.07s
Epoch 315/1000, LR 0.000212
Train loss: 0.5699;  Loss pred: 0.5699; Loss self: 0.0000; time: 8.70s
Val loss: 0.1235 score: 0.9592 time: 1.69s
Test loss: 0.2965 score: 0.8367 time: 1.59s
Epoch 316/1000, LR 0.000212
Train loss: 0.5687;  Loss pred: 0.5687; Loss self: 0.0000; time: 3.37s
Val loss: 0.1229 score: 0.9592 time: 0.09s
Test loss: 0.2962 score: 0.8367 time: 0.08s
Epoch 317/1000, LR 0.000211
Train loss: 0.5701;  Loss pred: 0.5701; Loss self: 0.0000; time: 0.23s
Val loss: 0.1224 score: 0.9592 time: 0.07s
Test loss: 0.2959 score: 0.8367 time: 0.07s
Epoch 318/1000, LR 0.000211
Train loss: 0.5663;  Loss pred: 0.5663; Loss self: 0.0000; time: 0.23s
Val loss: 0.1218 score: 0.9388 time: 0.08s
Test loss: 0.2956 score: 0.8367 time: 0.07s
Epoch 319/1000, LR 0.000210
Train loss: 0.5677;  Loss pred: 0.5677; Loss self: 0.0000; time: 0.24s
Val loss: 0.1213 score: 0.9388 time: 0.08s
Test loss: 0.2953 score: 0.8367 time: 0.06s
Epoch 320/1000, LR 0.000210
Train loss: 0.5664;  Loss pred: 0.5664; Loss self: 0.0000; time: 0.23s
Val loss: 0.1207 score: 0.9388 time: 0.07s
Test loss: 0.2951 score: 0.8367 time: 0.06s
Epoch 321/1000, LR 0.000210
Train loss: 0.5656;  Loss pred: 0.5656; Loss self: 0.0000; time: 0.23s
Val loss: 0.1202 score: 0.9388 time: 0.08s
Test loss: 0.2948 score: 0.8367 time: 0.07s
Epoch 322/1000, LR 0.000209
Train loss: 0.5678;  Loss pred: 0.5678; Loss self: 0.0000; time: 0.24s
Val loss: 0.1197 score: 0.9388 time: 0.08s
Test loss: 0.2946 score: 0.8367 time: 0.07s
Epoch 323/1000, LR 0.000209
Train loss: 0.5658;  Loss pred: 0.5658; Loss self: 0.0000; time: 0.24s
Val loss: 0.1191 score: 0.9592 time: 0.07s
Test loss: 0.2943 score: 0.8367 time: 0.06s
Epoch 324/1000, LR 0.000209
Train loss: 0.5645;  Loss pred: 0.5645; Loss self: 0.0000; time: 0.23s
Val loss: 0.1186 score: 0.9592 time: 0.08s
Test loss: 0.2941 score: 0.8367 time: 0.06s
Epoch 325/1000, LR 0.000208
Train loss: 0.5630;  Loss pred: 0.5630; Loss self: 0.0000; time: 3.83s
Val loss: 0.1181 score: 0.9592 time: 0.09s
Test loss: 0.2939 score: 0.8367 time: 0.09s
Epoch 326/1000, LR 0.000208
Train loss: 0.5646;  Loss pred: 0.5646; Loss self: 0.0000; time: 0.25s
Val loss: 0.1175 score: 0.9592 time: 0.08s
Test loss: 0.2937 score: 0.8367 time: 0.07s
Epoch 327/1000, LR 0.000208
Train loss: 0.5626;  Loss pred: 0.5626; Loss self: 0.0000; time: 0.24s
Val loss: 0.1170 score: 0.9592 time: 0.08s
Test loss: 0.2935 score: 0.8367 time: 0.08s
Epoch 328/1000, LR 0.000207
Train loss: 0.5626;  Loss pred: 0.5626; Loss self: 0.0000; time: 0.25s
Val loss: 0.1165 score: 0.9592 time: 0.09s
Test loss: 0.2933 score: 0.8571 time: 0.09s
Epoch 329/1000, LR 0.000207
Train loss: 0.5627;  Loss pred: 0.5627; Loss self: 0.0000; time: 0.25s
Val loss: 0.1161 score: 0.9592 time: 0.08s
Test loss: 0.2931 score: 0.8571 time: 0.07s
Epoch 330/1000, LR 0.000207
Train loss: 0.5623;  Loss pred: 0.5623; Loss self: 0.0000; time: 0.27s
Val loss: 0.1156 score: 0.9592 time: 0.08s
Test loss: 0.2928 score: 0.8367 time: 0.07s
Epoch 331/1000, LR 0.000206
Train loss: 0.5623;  Loss pred: 0.5623; Loss self: 0.0000; time: 0.25s
Val loss: 0.1151 score: 0.9592 time: 0.08s
Test loss: 0.2926 score: 0.8367 time: 0.07s
Epoch 332/1000, LR 0.000206
Train loss: 0.5620;  Loss pred: 0.5620; Loss self: 0.0000; time: 0.24s
Val loss: 0.1147 score: 0.9592 time: 0.08s
Test loss: 0.2924 score: 0.8367 time: 0.07s
Epoch 333/1000, LR 0.000205
Train loss: 0.5620;  Loss pred: 0.5620; Loss self: 0.0000; time: 0.25s
Val loss: 0.1143 score: 0.9592 time: 0.09s
Test loss: 0.2921 score: 0.8367 time: 0.07s
Epoch 334/1000, LR 0.000205
Train loss: 0.5618;  Loss pred: 0.5618; Loss self: 0.0000; time: 0.24s
Val loss: 0.1138 score: 0.9592 time: 0.08s
Test loss: 0.2919 score: 0.8367 time: 0.07s
Epoch 335/1000, LR 0.000205
Train loss: 0.5611;  Loss pred: 0.5611; Loss self: 0.0000; time: 0.25s
Val loss: 0.1134 score: 0.9592 time: 0.08s
Test loss: 0.2917 score: 0.8367 time: 0.07s
Epoch 336/1000, LR 0.000204
Train loss: 0.5593;  Loss pred: 0.5593; Loss self: 0.0000; time: 0.25s
Val loss: 0.1129 score: 0.9592 time: 0.08s
Test loss: 0.2914 score: 0.8367 time: 0.07s
Epoch 337/1000, LR 0.000204
Train loss: 0.5600;  Loss pred: 0.5600; Loss self: 0.0000; time: 0.24s
Val loss: 0.1125 score: 0.9592 time: 0.08s
Test loss: 0.2912 score: 0.8367 time: 0.07s
Epoch 338/1000, LR 0.000204
Train loss: 0.5575;  Loss pred: 0.5575; Loss self: 0.0000; time: 0.25s
Val loss: 0.1120 score: 0.9592 time: 0.08s
Test loss: 0.2910 score: 0.8367 time: 0.07s
Epoch 339/1000, LR 0.000203
Train loss: 0.5575;  Loss pred: 0.5575; Loss self: 0.0000; time: 0.24s
Val loss: 0.1115 score: 0.9592 time: 0.08s
Test loss: 0.2909 score: 0.8571 time: 0.07s
Epoch 340/1000, LR 0.000203
Train loss: 0.5572;  Loss pred: 0.5572; Loss self: 0.0000; time: 0.24s
Val loss: 0.1111 score: 0.9592 time: 0.08s
Test loss: 0.2907 score: 0.8571 time: 0.07s
Epoch 341/1000, LR 0.000203
Train loss: 0.5582;  Loss pred: 0.5582; Loss self: 0.0000; time: 0.26s
Val loss: 0.1106 score: 0.9592 time: 0.09s
Test loss: 0.2905 score: 0.8571 time: 0.07s
Epoch 342/1000, LR 0.000202
Train loss: 0.5574;  Loss pred: 0.5574; Loss self: 0.0000; time: 0.25s
Val loss: 0.1102 score: 0.9592 time: 0.08s
Test loss: 0.2903 score: 0.8571 time: 0.07s
Epoch 343/1000, LR 0.000202
Train loss: 0.5560;  Loss pred: 0.5560; Loss self: 0.0000; time: 0.25s
Val loss: 0.1097 score: 0.9592 time: 0.08s
Test loss: 0.2900 score: 0.8571 time: 0.07s
Epoch 344/1000, LR 0.000201
Train loss: 0.5567;  Loss pred: 0.5567; Loss self: 0.0000; time: 0.25s
Val loss: 0.1093 score: 0.9592 time: 0.08s
Test loss: 0.2898 score: 0.8571 time: 0.07s
Epoch 345/1000, LR 0.000201
Train loss: 0.5572;  Loss pred: 0.5572; Loss self: 0.0000; time: 0.25s
Val loss: 0.1089 score: 0.9592 time: 0.09s
Test loss: 0.2895 score: 0.8571 time: 0.07s
Epoch 346/1000, LR 0.000201
Train loss: 0.5574;  Loss pred: 0.5574; Loss self: 0.0000; time: 0.25s
Val loss: 0.1084 score: 0.9592 time: 0.08s
Test loss: 0.2893 score: 0.8571 time: 0.07s
Epoch 347/1000, LR 0.000200
Train loss: 0.5594;  Loss pred: 0.5594; Loss self: 0.0000; time: 0.25s
Val loss: 0.1080 score: 0.9592 time: 0.08s
Test loss: 0.2891 score: 0.8571 time: 0.07s
Epoch 348/1000, LR 0.000200
Train loss: 0.5529;  Loss pred: 0.5529; Loss self: 0.0000; time: 4.44s
Val loss: 0.1075 score: 0.9592 time: 1.13s
Test loss: 0.2889 score: 0.8571 time: 1.02s
Epoch 349/1000, LR 0.000200
Train loss: 0.5553;  Loss pred: 0.5553; Loss self: 0.0000; time: 0.49s
Val loss: 0.1071 score: 0.9592 time: 0.13s
Test loss: 0.2887 score: 0.8571 time: 0.14s
Epoch 350/1000, LR 0.000199
Train loss: 0.5547;  Loss pred: 0.5547; Loss self: 0.0000; time: 0.34s
Val loss: 0.1067 score: 0.9592 time: 0.08s
Test loss: 0.2884 score: 0.8571 time: 0.07s
Epoch 351/1000, LR 0.000199
Train loss: 0.5536;  Loss pred: 0.5536; Loss self: 0.0000; time: 0.25s
Val loss: 0.1063 score: 0.9592 time: 0.08s
Test loss: 0.2882 score: 0.8571 time: 0.07s
Epoch 352/1000, LR 0.000198
Train loss: 0.5521;  Loss pred: 0.5521; Loss self: 0.0000; time: 0.25s
Val loss: 0.1059 score: 0.9592 time: 0.08s
Test loss: 0.2879 score: 0.8571 time: 0.07s
Epoch 353/1000, LR 0.000198
Train loss: 0.5549;  Loss pred: 0.5549; Loss self: 0.0000; time: 0.25s
Val loss: 0.1055 score: 0.9592 time: 0.08s
Test loss: 0.2877 score: 0.8571 time: 0.07s
Epoch 354/1000, LR 0.000198
Train loss: 0.5539;  Loss pred: 0.5539; Loss self: 0.0000; time: 0.25s
Val loss: 0.1052 score: 0.9592 time: 0.08s
Test loss: 0.2874 score: 0.8571 time: 0.07s
Epoch 355/1000, LR 0.000197
Train loss: 0.5535;  Loss pred: 0.5535; Loss self: 0.0000; time: 0.26s
Val loss: 0.1048 score: 0.9592 time: 0.08s
Test loss: 0.2872 score: 0.8776 time: 0.07s
Epoch 356/1000, LR 0.000197
Train loss: 0.5518;  Loss pred: 0.5518; Loss self: 0.0000; time: 0.25s
Val loss: 0.1044 score: 0.9592 time: 0.08s
Test loss: 0.2870 score: 0.8776 time: 0.07s
Epoch 357/1000, LR 0.000196
Train loss: 0.5522;  Loss pred: 0.5522; Loss self: 0.0000; time: 0.25s
Val loss: 0.1040 score: 0.9592 time: 0.08s
Test loss: 0.2868 score: 0.8776 time: 0.07s
Epoch 358/1000, LR 0.000196
Train loss: 0.5502;  Loss pred: 0.5502; Loss self: 0.0000; time: 0.25s
Val loss: 0.1036 score: 0.9592 time: 0.08s
Test loss: 0.2866 score: 0.8776 time: 0.07s
Epoch 359/1000, LR 0.000196
Train loss: 0.5502;  Loss pred: 0.5502; Loss self: 0.0000; time: 1.14s
Val loss: 0.1033 score: 0.9592 time: 0.67s
Test loss: 0.2864 score: 0.8776 time: 0.88s
Epoch 360/1000, LR 0.000195
Train loss: 0.5514;  Loss pred: 0.5514; Loss self: 0.0000; time: 5.61s
Val loss: 0.1029 score: 0.9592 time: 0.09s
Test loss: 0.2862 score: 0.8776 time: 0.07s
Epoch 361/1000, LR 0.000195
Train loss: 0.5507;  Loss pred: 0.5507; Loss self: 0.0000; time: 0.25s
Val loss: 0.1025 score: 0.9592 time: 0.08s
Test loss: 0.2861 score: 0.8776 time: 0.07s
Epoch 362/1000, LR 0.000195
Train loss: 0.5503;  Loss pred: 0.5503; Loss self: 0.0000; time: 0.24s
Val loss: 0.1022 score: 0.9592 time: 0.08s
Test loss: 0.2859 score: 0.8776 time: 0.07s
Epoch 363/1000, LR 0.000194
Train loss: 0.5502;  Loss pred: 0.5502; Loss self: 0.0000; time: 0.24s
Val loss: 0.1018 score: 0.9592 time: 0.08s
Test loss: 0.2858 score: 0.8776 time: 0.07s
Epoch 364/1000, LR 0.000194
Train loss: 0.5492;  Loss pred: 0.5492; Loss self: 0.0000; time: 0.25s
Val loss: 0.1014 score: 0.9592 time: 0.08s
Test loss: 0.2856 score: 0.8776 time: 0.07s
Epoch 365/1000, LR 0.000193
Train loss: 0.5512;  Loss pred: 0.5512; Loss self: 0.0000; time: 0.24s
Val loss: 0.1010 score: 0.9592 time: 0.08s
Test loss: 0.2855 score: 0.8776 time: 0.07s
Epoch 366/1000, LR 0.000193
Train loss: 0.5489;  Loss pred: 0.5489; Loss self: 0.0000; time: 0.25s
Val loss: 0.1006 score: 0.9592 time: 0.08s
Test loss: 0.2855 score: 0.8571 time: 0.07s
Epoch 367/1000, LR 0.000193
Train loss: 0.5481;  Loss pred: 0.5481; Loss self: 0.0000; time: 0.25s
Val loss: 0.1002 score: 0.9592 time: 0.08s
Test loss: 0.2853 score: 0.8571 time: 0.07s
Epoch 368/1000, LR 0.000192
Train loss: 0.5491;  Loss pred: 0.5491; Loss self: 0.0000; time: 0.25s
Val loss: 0.0999 score: 0.9592 time: 0.08s
Test loss: 0.2852 score: 0.8571 time: 0.07s
Epoch 369/1000, LR 0.000192
Train loss: 0.5489;  Loss pred: 0.5489; Loss self: 0.0000; time: 0.24s
Val loss: 0.0995 score: 0.9592 time: 0.08s
Test loss: 0.2850 score: 0.8571 time: 0.07s
Epoch 370/1000, LR 0.000191
Train loss: 0.5474;  Loss pred: 0.5474; Loss self: 0.0000; time: 0.25s
Val loss: 0.0992 score: 0.9592 time: 0.08s
Test loss: 0.2848 score: 0.8776 time: 0.07s
Epoch 371/1000, LR 0.000191
Train loss: 0.5462;  Loss pred: 0.5462; Loss self: 0.0000; time: 0.25s
Val loss: 0.0989 score: 0.9592 time: 0.09s
Test loss: 0.2846 score: 0.8776 time: 0.07s
Epoch 372/1000, LR 0.000191
Train loss: 0.5452;  Loss pred: 0.5452; Loss self: 0.0000; time: 0.25s
Val loss: 0.0986 score: 0.9592 time: 0.08s
Test loss: 0.2844 score: 0.8776 time: 1.20s
Epoch 373/1000, LR 0.000190
Train loss: 0.5443;  Loss pred: 0.5443; Loss self: 0.0000; time: 2.15s
Val loss: 0.0983 score: 0.9592 time: 2.68s
Test loss: 0.2842 score: 0.8776 time: 2.43s
Epoch 374/1000, LR 0.000190
Train loss: 0.5467;  Loss pred: 0.5467; Loss self: 0.0000; time: 5.11s
Val loss: 0.0981 score: 0.9592 time: 0.09s
Test loss: 0.2840 score: 0.8776 time: 0.07s
Epoch 375/1000, LR 0.000190
Train loss: 0.5472;  Loss pred: 0.5472; Loss self: 0.0000; time: 0.26s
Val loss: 0.0978 score: 0.9592 time: 0.09s
Test loss: 0.2838 score: 0.8776 time: 0.07s
Epoch 376/1000, LR 0.000189
Train loss: 0.5460;  Loss pred: 0.5460; Loss self: 0.0000; time: 0.25s
Val loss: 0.0976 score: 0.9592 time: 0.08s
Test loss: 0.2836 score: 0.8776 time: 0.07s
Epoch 377/1000, LR 0.000189
Train loss: 0.5467;  Loss pred: 0.5467; Loss self: 0.0000; time: 0.25s
Val loss: 0.0973 score: 0.9592 time: 0.08s
Test loss: 0.2835 score: 0.8776 time: 0.07s
Epoch 378/1000, LR 0.000188
Train loss: 0.5452;  Loss pred: 0.5452; Loss self: 0.0000; time: 0.25s
Val loss: 0.0970 score: 0.9592 time: 0.08s
Test loss: 0.2834 score: 0.8776 time: 0.07s
Epoch 379/1000, LR 0.000188
Train loss: 0.5422;  Loss pred: 0.5422; Loss self: 0.0000; time: 0.25s
Val loss: 0.0967 score: 0.9592 time: 0.08s
Test loss: 0.2832 score: 0.8776 time: 0.07s
Epoch 380/1000, LR 0.000188
Train loss: 0.5443;  Loss pred: 0.5443; Loss self: 0.0000; time: 0.25s
Val loss: 0.0964 score: 0.9592 time: 0.08s
Test loss: 0.2831 score: 0.8776 time: 0.07s
Epoch 381/1000, LR 0.000187
Train loss: 0.5444;  Loss pred: 0.5444; Loss self: 0.0000; time: 0.25s
Val loss: 0.0961 score: 0.9592 time: 0.08s
Test loss: 0.2830 score: 0.8776 time: 0.07s
Epoch 382/1000, LR 0.000187
Train loss: 0.5451;  Loss pred: 0.5451; Loss self: 0.0000; time: 0.25s
Val loss: 0.0959 score: 0.9592 time: 0.08s
Test loss: 0.2829 score: 0.8776 time: 0.31s
Epoch 383/1000, LR 0.000186
Train loss: 0.5443;  Loss pred: 0.5443; Loss self: 0.0000; time: 5.51s
Val loss: 0.0955 score: 0.9592 time: 0.08s
Test loss: 0.2828 score: 0.8776 time: 0.08s
Epoch 384/1000, LR 0.000186
Train loss: 0.5438;  Loss pred: 0.5438; Loss self: 0.0000; time: 0.24s
Val loss: 0.0952 score: 0.9592 time: 0.08s
Test loss: 0.2827 score: 0.8776 time: 0.07s
Epoch 385/1000, LR 0.000186
Train loss: 0.5414;  Loss pred: 0.5414; Loss self: 0.0000; time: 0.24s
Val loss: 0.0949 score: 0.9592 time: 0.08s
Test loss: 0.2827 score: 0.8776 time: 0.07s
Epoch 386/1000, LR 0.000185
Train loss: 0.5418;  Loss pred: 0.5418; Loss self: 0.0000; time: 0.23s
Val loss: 0.0946 score: 0.9592 time: 0.07s
Test loss: 0.2827 score: 0.8776 time: 0.06s
Epoch 387/1000, LR 0.000185
Train loss: 0.5418;  Loss pred: 0.5418; Loss self: 0.0000; time: 0.23s
Val loss: 0.0943 score: 0.9592 time: 0.08s
Test loss: 0.2826 score: 0.8776 time: 0.07s
Epoch 388/1000, LR 0.000184
Train loss: 0.5425;  Loss pred: 0.5425; Loss self: 0.0000; time: 0.24s
Val loss: 0.0940 score: 0.9592 time: 0.08s
Test loss: 0.2826 score: 0.8776 time: 0.07s
Epoch 389/1000, LR 0.000184
Train loss: 0.5430;  Loss pred: 0.5430; Loss self: 0.0000; time: 0.24s
Val loss: 0.0937 score: 0.9592 time: 0.08s
Test loss: 0.2825 score: 0.8776 time: 0.07s
Epoch 390/1000, LR 0.000184
Train loss: 0.5417;  Loss pred: 0.5417; Loss self: 0.0000; time: 0.24s
Val loss: 0.0935 score: 0.9592 time: 0.08s
Test loss: 0.2824 score: 0.8776 time: 0.07s
Epoch 391/1000, LR 0.000183
Train loss: 0.5442;  Loss pred: 0.5442; Loss self: 0.0000; time: 0.24s
Val loss: 0.0932 score: 0.9592 time: 0.08s
Test loss: 0.2823 score: 0.8776 time: 0.07s
Epoch 392/1000, LR 0.000183
Train loss: 0.5409;  Loss pred: 0.5409; Loss self: 0.0000; time: 0.25s
Val loss: 0.0929 score: 0.9592 time: 0.08s
Test loss: 0.2823 score: 0.8776 time: 0.07s
Epoch 393/1000, LR 0.000182
Train loss: 0.5412;  Loss pred: 0.5412; Loss self: 0.0000; time: 0.26s
Val loss: 0.0927 score: 0.9592 time: 0.08s
Test loss: 0.2822 score: 0.8776 time: 0.07s
Epoch 394/1000, LR 0.000182
Train loss: 0.5396;  Loss pred: 0.5396; Loss self: 0.0000; time: 0.25s
Val loss: 0.0924 score: 0.9592 time: 0.08s
Test loss: 0.2822 score: 0.8776 time: 0.07s
Epoch 395/1000, LR 0.000182
Train loss: 0.5410;  Loss pred: 0.5410; Loss self: 0.0000; time: 0.24s
Val loss: 0.0921 score: 0.9592 time: 0.08s
Test loss: 0.2821 score: 0.8776 time: 0.07s
Epoch 396/1000, LR 0.000181
Train loss: 0.5400;  Loss pred: 0.5400; Loss self: 0.0000; time: 0.24s
Val loss: 0.0919 score: 0.9592 time: 0.08s
Test loss: 0.2820 score: 0.8776 time: 0.07s
Epoch 397/1000, LR 0.000181
Train loss: 0.5394;  Loss pred: 0.5394; Loss self: 0.0000; time: 0.24s
Val loss: 0.0917 score: 0.9592 time: 0.08s
Test loss: 0.2819 score: 0.8776 time: 0.07s
Epoch 398/1000, LR 0.000180
Train loss: 0.5407;  Loss pred: 0.5407; Loss self: 0.0000; time: 0.24s
Val loss: 0.0915 score: 0.9592 time: 0.08s
Test loss: 0.2818 score: 0.8776 time: 0.07s
Epoch 399/1000, LR 0.000180
Train loss: 0.5405;  Loss pred: 0.5405; Loss self: 0.0000; time: 0.24s
Val loss: 0.0913 score: 0.9592 time: 0.08s
Test loss: 0.2817 score: 0.8776 time: 0.07s
Epoch 400/1000, LR 0.000180
Train loss: 0.5393;  Loss pred: 0.5393; Loss self: 0.0000; time: 0.24s
Val loss: 0.0911 score: 0.9592 time: 0.08s
Test loss: 0.2816 score: 0.8776 time: 0.07s
Epoch 401/1000, LR 0.000179
Train loss: 0.5413;  Loss pred: 0.5413; Loss self: 0.0000; time: 0.24s
Val loss: 0.0910 score: 0.9592 time: 0.08s
Test loss: 0.2814 score: 0.8776 time: 0.07s
Epoch 402/1000, LR 0.000179
Train loss: 0.5392;  Loss pred: 0.5392; Loss self: 0.0000; time: 0.23s
Val loss: 0.0908 score: 0.9592 time: 0.09s
Test loss: 0.2813 score: 0.8776 time: 0.07s
Epoch 403/1000, LR 0.000178
Train loss: 0.5391;  Loss pred: 0.5391; Loss self: 0.0000; time: 0.24s
Val loss: 0.0907 score: 0.9592 time: 0.08s
Test loss: 0.2812 score: 0.8776 time: 0.07s
Epoch 404/1000, LR 0.000178
Train loss: 0.5371;  Loss pred: 0.5371; Loss self: 0.0000; time: 0.25s
Val loss: 0.0905 score: 0.9592 time: 0.08s
Test loss: 0.2811 score: 0.8776 time: 0.07s
Epoch 405/1000, LR 0.000178
Train loss: 0.5382;  Loss pred: 0.5382; Loss self: 0.0000; time: 0.25s
Val loss: 0.0903 score: 0.9592 time: 0.08s
Test loss: 0.2810 score: 0.8776 time: 0.29s
Epoch 406/1000, LR 0.000177
Train loss: 0.5377;  Loss pred: 0.5377; Loss self: 0.0000; time: 1.75s
Val loss: 0.0902 score: 0.9592 time: 0.48s
Test loss: 0.2810 score: 0.8776 time: 0.54s
Epoch 407/1000, LR 0.000177
Train loss: 0.5374;  Loss pred: 0.5374; Loss self: 0.0000; time: 4.79s
Val loss: 0.0900 score: 0.9592 time: 0.26s
Test loss: 0.2809 score: 0.8776 time: 0.12s
Epoch 408/1000, LR 0.000176
Train loss: 0.5386;  Loss pred: 0.5386; Loss self: 0.0000; time: 0.25s
Val loss: 0.0897 score: 0.9592 time: 0.08s
Test loss: 0.2809 score: 0.8776 time: 0.07s
Epoch 409/1000, LR 0.000176
Train loss: 0.5370;  Loss pred: 0.5370; Loss self: 0.0000; time: 0.23s
Val loss: 0.0894 score: 0.9592 time: 0.08s
Test loss: 0.2808 score: 0.8776 time: 0.07s
Epoch 410/1000, LR 0.000175
Train loss: 0.5380;  Loss pred: 0.5380; Loss self: 0.0000; time: 0.24s
Val loss: 0.0891 score: 0.9592 time: 0.08s
Test loss: 0.2808 score: 0.8776 time: 0.07s
Epoch 411/1000, LR 0.000175
Train loss: 0.5379;  Loss pred: 0.5379; Loss self: 0.0000; time: 0.24s
Val loss: 0.0888 score: 0.9592 time: 0.08s
Test loss: 0.2808 score: 0.8776 time: 0.07s
Epoch 412/1000, LR 0.000175
Train loss: 0.5378;  Loss pred: 0.5378; Loss self: 0.0000; time: 0.24s
Val loss: 0.0885 score: 0.9592 time: 0.08s
Test loss: 0.2807 score: 0.8776 time: 0.07s
Epoch 413/1000, LR 0.000174
Train loss: 0.5381;  Loss pred: 0.5381; Loss self: 0.0000; time: 0.25s
Val loss: 0.0883 score: 0.9592 time: 0.08s
Test loss: 0.2807 score: 0.8776 time: 0.07s
Epoch 414/1000, LR 0.000174
Train loss: 0.5351;  Loss pred: 0.5351; Loss self: 0.0000; time: 0.24s
Val loss: 0.0880 score: 0.9592 time: 0.08s
Test loss: 0.2807 score: 0.8776 time: 0.07s
Epoch 415/1000, LR 0.000173
Train loss: 0.5351;  Loss pred: 0.5351; Loss self: 0.0000; time: 0.23s
Val loss: 0.0878 score: 0.9592 time: 0.08s
Test loss: 0.2806 score: 0.8776 time: 0.07s
Epoch 416/1000, LR 0.000173
Train loss: 0.5350;  Loss pred: 0.5350; Loss self: 0.0000; time: 0.24s
Val loss: 0.0875 score: 0.9592 time: 0.08s
Test loss: 0.2806 score: 0.8776 time: 0.07s
Epoch 417/1000, LR 0.000173
Train loss: 0.5368;  Loss pred: 0.5368; Loss self: 0.0000; time: 0.23s
Val loss: 0.0873 score: 0.9592 time: 0.08s
Test loss: 0.2806 score: 0.8776 time: 0.07s
Epoch 418/1000, LR 0.000172
Train loss: 0.5367;  Loss pred: 0.5367; Loss self: 0.0000; time: 0.24s
Val loss: 0.0871 score: 0.9592 time: 0.08s
Test loss: 0.2805 score: 0.8776 time: 0.07s
Epoch 419/1000, LR 0.000172
Train loss: 0.5334;  Loss pred: 0.5334; Loss self: 0.0000; time: 0.25s
Val loss: 0.0869 score: 0.9592 time: 1.18s
Test loss: 0.2805 score: 0.8776 time: 1.56s
Epoch 420/1000, LR 0.000171
Train loss: 0.5380;  Loss pred: 0.5380; Loss self: 0.0000; time: 5.40s
Val loss: 0.0866 score: 0.9592 time: 2.09s
Test loss: 0.2805 score: 0.8776 time: 0.87s
Epoch 421/1000, LR 0.000171
Train loss: 0.5343;  Loss pred: 0.5343; Loss self: 0.0000; time: 1.68s
Val loss: 0.0864 score: 0.9592 time: 0.08s
Test loss: 0.2805 score: 0.8776 time: 0.07s
Epoch 422/1000, LR 0.000171
Train loss: 0.5336;  Loss pred: 0.5336; Loss self: 0.0000; time: 0.25s
Val loss: 0.0862 score: 0.9592 time: 0.08s
Test loss: 0.2804 score: 0.8776 time: 0.07s
Epoch 423/1000, LR 0.000170
Train loss: 0.5352;  Loss pred: 0.5352; Loss self: 0.0000; time: 0.25s
Val loss: 0.0860 score: 0.9592 time: 0.08s
Test loss: 0.2804 score: 0.8776 time: 0.07s
Epoch 424/1000, LR 0.000170
Train loss: 0.5333;  Loss pred: 0.5333; Loss self: 0.0000; time: 0.25s
Val loss: 0.0859 score: 0.9592 time: 0.08s
Test loss: 0.2803 score: 0.8776 time: 0.07s
Epoch 425/1000, LR 0.000169
Train loss: 0.5345;  Loss pred: 0.5345; Loss self: 0.0000; time: 0.25s
Val loss: 0.0857 score: 0.9592 time: 0.08s
Test loss: 0.2802 score: 0.8776 time: 0.07s
Epoch 426/1000, LR 0.000169
Train loss: 0.5338;  Loss pred: 0.5338; Loss self: 0.0000; time: 0.25s
Val loss: 0.0856 score: 0.9592 time: 0.08s
Test loss: 0.2801 score: 0.8776 time: 0.07s
Epoch 427/1000, LR 0.000168
Train loss: 0.5340;  Loss pred: 0.5340; Loss self: 0.0000; time: 0.25s
Val loss: 0.0855 score: 0.9592 time: 0.08s
Test loss: 0.2800 score: 0.8776 time: 0.07s
Epoch 428/1000, LR 0.000168
Train loss: 0.5339;  Loss pred: 0.5339; Loss self: 0.0000; time: 0.25s
Val loss: 0.0855 score: 0.9592 time: 0.08s
Test loss: 0.2799 score: 0.8776 time: 0.07s
Epoch 429/1000, LR 0.000168
Train loss: 0.5342;  Loss pred: 0.5342; Loss self: 0.0000; time: 0.25s
Val loss: 0.0854 score: 0.9592 time: 0.09s
Test loss: 0.2798 score: 0.8776 time: 0.07s
Epoch 430/1000, LR 0.000167
Train loss: 0.5326;  Loss pred: 0.5326; Loss self: 0.0000; time: 0.24s
Val loss: 0.0853 score: 0.9592 time: 0.08s
Test loss: 0.2797 score: 0.8776 time: 0.07s
Epoch 431/1000, LR 0.000167
Train loss: 0.5318;  Loss pred: 0.5318; Loss self: 0.0000; time: 0.24s
Val loss: 0.0852 score: 0.9592 time: 0.08s
Test loss: 0.2797 score: 0.8776 time: 0.07s
Epoch 432/1000, LR 0.000166
Train loss: 0.5330;  Loss pred: 0.5330; Loss self: 0.0000; time: 0.26s
Val loss: 0.0851 score: 0.9592 time: 0.33s
Test loss: 0.2796 score: 0.8776 time: 2.47s
Epoch 433/1000, LR 0.000166
Train loss: 0.5315;  Loss pred: 0.5315; Loss self: 0.0000; time: 7.81s
Val loss: 0.0850 score: 0.9592 time: 0.92s
Test loss: 0.2796 score: 0.8776 time: 0.58s
Epoch 434/1000, LR 0.000166
Train loss: 0.5320;  Loss pred: 0.5320; Loss self: 0.0000; time: 0.68s
Val loss: 0.0849 score: 0.9592 time: 0.08s
Test loss: 0.2795 score: 0.8776 time: 0.07s
Epoch 435/1000, LR 0.000165
Train loss: 0.5332;  Loss pred: 0.5332; Loss self: 0.0000; time: 0.25s
Val loss: 0.0847 score: 0.9592 time: 0.08s
Test loss: 0.2795 score: 0.8776 time: 0.07s
Epoch 436/1000, LR 0.000165
Train loss: 0.5302;  Loss pred: 0.5302; Loss self: 0.0000; time: 0.25s
Val loss: 0.0846 score: 0.9592 time: 0.09s
Test loss: 0.2794 score: 0.8776 time: 0.07s
Epoch 437/1000, LR 0.000164
Train loss: 0.5328;  Loss pred: 0.5328; Loss self: 0.0000; time: 0.25s
Val loss: 0.0844 score: 0.9592 time: 0.08s
Test loss: 0.2794 score: 0.8776 time: 0.07s
Epoch 438/1000, LR 0.000164
Train loss: 0.5328;  Loss pred: 0.5328; Loss self: 0.0000; time: 0.25s
Val loss: 0.0843 score: 0.9592 time: 0.08s
Test loss: 0.2794 score: 0.8776 time: 0.07s
Epoch 439/1000, LR 0.000163
Train loss: 0.5318;  Loss pred: 0.5318; Loss self: 0.0000; time: 0.26s
Val loss: 0.0841 score: 0.9592 time: 0.08s
Test loss: 0.2794 score: 0.8776 time: 0.07s
Epoch 440/1000, LR 0.000163
Train loss: 0.5310;  Loss pred: 0.5310; Loss self: 0.0000; time: 0.23s
Val loss: 0.0838 score: 0.9592 time: 0.08s
Test loss: 0.2793 score: 0.8776 time: 0.07s
Epoch 441/1000, LR 0.000163
Train loss: 0.5333;  Loss pred: 0.5333; Loss self: 0.0000; time: 0.26s
Val loss: 0.0836 score: 0.9592 time: 0.08s
Test loss: 0.2793 score: 0.8776 time: 0.07s
Epoch 442/1000, LR 0.000162
Train loss: 0.5322;  Loss pred: 0.5322; Loss self: 0.0000; time: 0.26s
Val loss: 0.0834 score: 0.9592 time: 0.08s
Test loss: 0.2793 score: 0.8776 time: 0.07s
Epoch 443/1000, LR 0.000162
Train loss: 0.5310;  Loss pred: 0.5310; Loss self: 0.0000; time: 0.25s
Val loss: 0.0832 score: 0.9592 time: 0.08s
Test loss: 0.2793 score: 0.8776 time: 0.07s
Epoch 444/1000, LR 0.000161
Train loss: 0.5297;  Loss pred: 0.5297; Loss self: 0.0000; time: 0.26s
Val loss: 0.0831 score: 0.9592 time: 1.78s
Test loss: 0.2793 score: 0.8776 time: 2.06s
Epoch 445/1000, LR 0.000161
Train loss: 0.5296;  Loss pred: 0.5296; Loss self: 0.0000; time: 4.61s
Val loss: 0.0829 score: 0.9592 time: 1.27s
Test loss: 0.2792 score: 0.8776 time: 1.74s
Epoch 446/1000, LR 0.000161
Train loss: 0.5284;  Loss pred: 0.5284; Loss self: 0.0000; time: 1.93s
Val loss: 0.0828 score: 0.9592 time: 0.07s
Test loss: 0.2792 score: 0.8776 time: 0.06s
Epoch 447/1000, LR 0.000160
Train loss: 0.5310;  Loss pred: 0.5310; Loss self: 0.0000; time: 0.23s
Val loss: 0.0826 score: 0.9592 time: 0.08s
Test loss: 0.2792 score: 0.8776 time: 0.06s
Epoch 448/1000, LR 0.000160
Train loss: 0.5307;  Loss pred: 0.5307; Loss self: 0.0000; time: 0.23s
Val loss: 0.0824 score: 0.9592 time: 0.07s
Test loss: 0.2792 score: 0.8776 time: 0.06s
Epoch 449/1000, LR 0.000159
Train loss: 0.5296;  Loss pred: 0.5296; Loss self: 0.0000; time: 0.22s
Val loss: 0.0823 score: 0.9592 time: 0.07s
Test loss: 0.2792 score: 0.8776 time: 0.06s
Epoch 450/1000, LR 0.000159
Train loss: 0.5304;  Loss pred: 0.5304; Loss self: 0.0000; time: 0.23s
Val loss: 0.0821 score: 0.9592 time: 0.08s
Test loss: 0.2791 score: 0.8776 time: 0.06s
Epoch 451/1000, LR 0.000158
Train loss: 0.5302;  Loss pred: 0.5302; Loss self: 0.0000; time: 0.22s
Val loss: 0.0820 score: 0.9592 time: 0.08s
Test loss: 0.2791 score: 0.8776 time: 0.07s
Epoch 452/1000, LR 0.000158
Train loss: 0.5291;  Loss pred: 0.5291; Loss self: 0.0000; time: 0.23s
Val loss: 0.0818 score: 0.9592 time: 0.08s
Test loss: 0.2791 score: 0.8776 time: 0.07s
Epoch 453/1000, LR 0.000158
Train loss: 0.5295;  Loss pred: 0.5295; Loss self: 0.0000; time: 0.24s
Val loss: 0.0816 score: 0.9592 time: 0.08s
Test loss: 0.2791 score: 0.8776 time: 0.07s
Epoch 454/1000, LR 0.000157
Train loss: 0.5291;  Loss pred: 0.5291; Loss self: 0.0000; time: 0.24s
Val loss: 0.0815 score: 0.9592 time: 0.08s
Test loss: 0.2791 score: 0.8776 time: 0.07s
Epoch 455/1000, LR 0.000157
Train loss: 0.5295;  Loss pred: 0.5295; Loss self: 0.0000; time: 0.24s
Val loss: 0.0814 score: 0.9592 time: 0.08s
Test loss: 0.2790 score: 0.8776 time: 0.07s
Epoch 456/1000, LR 0.000156
Train loss: 0.5284;  Loss pred: 0.5284; Loss self: 0.0000; time: 5.41s
Val loss: 0.0813 score: 0.9592 time: 1.31s
Test loss: 0.2790 score: 0.8776 time: 1.34s
Epoch 457/1000, LR 0.000156
Train loss: 0.5278;  Loss pred: 0.5278; Loss self: 0.0000; time: 1.50s
Val loss: 0.0813 score: 0.9592 time: 0.08s
Test loss: 0.2789 score: 0.8776 time: 0.07s
Epoch 458/1000, LR 0.000155
Train loss: 0.5286;  Loss pred: 0.5286; Loss self: 0.0000; time: 0.26s
Val loss: 0.0812 score: 0.9592 time: 0.08s
Test loss: 0.2789 score: 0.8776 time: 0.07s
Epoch 459/1000, LR 0.000155
Train loss: 0.5264;  Loss pred: 0.5264; Loss self: 0.0000; time: 0.25s
Val loss: 0.0811 score: 0.9592 time: 0.08s
Test loss: 0.2789 score: 0.8776 time: 0.07s
Epoch 460/1000, LR 0.000155
Train loss: 0.5271;  Loss pred: 0.5271; Loss self: 0.0000; time: 0.25s
Val loss: 0.0810 score: 0.9592 time: 0.08s
Test loss: 0.2789 score: 0.8776 time: 0.07s
Epoch 461/1000, LR 0.000154
Train loss: 0.5286;  Loss pred: 0.5286; Loss self: 0.0000; time: 0.26s
Val loss: 0.0808 score: 0.9592 time: 0.21s
Test loss: 0.2788 score: 0.8776 time: 0.19s
Epoch 462/1000, LR 0.000154
Train loss: 0.5259;  Loss pred: 0.5259; Loss self: 0.0000; time: 0.39s
Val loss: 0.0807 score: 0.9592 time: 0.08s
Test loss: 0.2788 score: 0.8776 time: 0.07s
Epoch 463/1000, LR 0.000153
Train loss: 0.5270;  Loss pred: 0.5270; Loss self: 0.0000; time: 0.24s
Val loss: 0.0806 score: 0.9592 time: 0.08s
Test loss: 0.2788 score: 0.8776 time: 0.07s
Epoch 464/1000, LR 0.000153
Train loss: 0.5271;  Loss pred: 0.5271; Loss self: 0.0000; time: 0.24s
Val loss: 0.0806 score: 0.9592 time: 0.08s
Test loss: 0.2788 score: 0.8776 time: 0.07s
Epoch 465/1000, LR 0.000153
Train loss: 0.5292;  Loss pred: 0.5292; Loss self: 0.0000; time: 0.97s
Val loss: 0.0804 score: 0.9592 time: 2.09s
Test loss: 0.2788 score: 0.8776 time: 1.86s
Epoch 466/1000, LR 0.000152
Train loss: 0.5256;  Loss pred: 0.5256; Loss self: 0.0000; time: 2.29s
Val loss: 0.0803 score: 0.9592 time: 0.24s
Test loss: 0.2787 score: 0.8776 time: 0.73s
Epoch 467/1000, LR 0.000152
Train loss: 0.5266;  Loss pred: 0.5266; Loss self: 0.0000; time: 1.34s
Val loss: 0.0802 score: 0.9592 time: 0.15s
Test loss: 0.2787 score: 0.8776 time: 0.16s
Epoch 468/1000, LR 0.000151
Train loss: 0.5256;  Loss pred: 0.5256; Loss self: 0.0000; time: 0.55s
Val loss: 0.0801 score: 0.9592 time: 0.24s
Test loss: 0.2787 score: 0.8776 time: 0.08s
Epoch 469/1000, LR 0.000151
Train loss: 0.5282;  Loss pred: 0.5282; Loss self: 0.0000; time: 0.26s
Val loss: 0.0800 score: 0.9592 time: 0.08s
Test loss: 0.2787 score: 0.8776 time: 0.07s
Epoch 470/1000, LR 0.000150
Train loss: 0.5273;  Loss pred: 0.5273; Loss self: 0.0000; time: 0.25s
Val loss: 0.0798 score: 0.9592 time: 0.08s
Test loss: 0.2787 score: 0.8776 time: 0.07s
Epoch 471/1000, LR 0.000150
Train loss: 0.5241;  Loss pred: 0.5241; Loss self: 0.0000; time: 0.25s
Val loss: 0.0797 score: 0.9592 time: 0.08s
Test loss: 0.2786 score: 0.8776 time: 0.07s
Epoch 472/1000, LR 0.000150
Train loss: 0.5242;  Loss pred: 0.5242; Loss self: 0.0000; time: 0.25s
Val loss: 0.0796 score: 0.9592 time: 0.08s
Test loss: 0.2786 score: 0.8776 time: 0.07s
Epoch 473/1000, LR 0.000149
Train loss: 0.5251;  Loss pred: 0.5251; Loss self: 0.0000; time: 0.25s
Val loss: 0.0794 score: 0.9592 time: 0.08s
Test loss: 0.2786 score: 0.8776 time: 0.07s
Epoch 474/1000, LR 0.000149
Train loss: 0.5248;  Loss pred: 0.5248; Loss self: 0.0000; time: 0.25s
Val loss: 0.0793 score: 0.9592 time: 0.08s
Test loss: 0.2786 score: 0.8776 time: 0.07s
Epoch 475/1000, LR 0.000148
Train loss: 0.5248;  Loss pred: 0.5248; Loss self: 0.0000; time: 0.25s
Val loss: 0.0791 score: 0.9592 time: 0.08s
Test loss: 0.2786 score: 0.8776 time: 0.07s
Epoch 476/1000, LR 0.000148
Train loss: 0.5251;  Loss pred: 0.5251; Loss self: 0.0000; time: 0.26s
Val loss: 0.0791 score: 0.9592 time: 0.09s
Test loss: 0.2786 score: 0.8776 time: 0.07s
Epoch 477/1000, LR 0.000147
Train loss: 0.5246;  Loss pred: 0.5246; Loss self: 0.0000; time: 0.77s
Val loss: 0.0790 score: 0.9592 time: 1.52s
Test loss: 0.2786 score: 0.8776 time: 2.19s
Epoch 478/1000, LR 0.000147
Train loss: 0.5257;  Loss pred: 0.5257; Loss self: 0.0000; time: 7.47s
Val loss: 0.0789 score: 0.9592 time: 0.33s
Test loss: 0.2786 score: 0.8776 time: 1.21s
Epoch 479/1000, LR 0.000147
Train loss: 0.5238;  Loss pred: 0.5238; Loss self: 0.0000; time: 1.12s
Val loss: 0.0788 score: 0.9592 time: 0.08s
Test loss: 0.2786 score: 0.8776 time: 0.07s
Epoch 480/1000, LR 0.000146
Train loss: 0.5242;  Loss pred: 0.5242; Loss self: 0.0000; time: 0.25s
Val loss: 0.0786 score: 0.9592 time: 0.08s
Test loss: 0.2786 score: 0.8776 time: 0.07s
Epoch 481/1000, LR 0.000146
Train loss: 0.5249;  Loss pred: 0.5249; Loss self: 0.0000; time: 0.24s
Val loss: 0.0785 score: 0.9592 time: 0.08s
Test loss: 0.2786 score: 0.8776 time: 0.07s
Epoch 482/1000, LR 0.000145
Train loss: 0.5241;  Loss pred: 0.5241; Loss self: 0.0000; time: 0.24s
Val loss: 0.0784 score: 0.9592 time: 0.08s
Test loss: 0.2786 score: 0.8776 time: 0.07s
Epoch 483/1000, LR 0.000145
Train loss: 0.5248;  Loss pred: 0.5248; Loss self: 0.0000; time: 0.24s
Val loss: 0.0782 score: 0.9592 time: 0.08s
Test loss: 0.2787 score: 0.8776 time: 0.07s
Epoch 484/1000, LR 0.000144
Train loss: 0.5251;  Loss pred: 0.5251; Loss self: 0.0000; time: 0.24s
Val loss: 0.0780 score: 0.9592 time: 0.08s
Test loss: 0.2787 score: 0.8776 time: 0.07s
Epoch 485/1000, LR 0.000144
Train loss: 0.5228;  Loss pred: 0.5228; Loss self: 0.0000; time: 0.25s
Val loss: 0.0777 score: 0.9592 time: 0.08s
Test loss: 0.2787 score: 0.8776 time: 0.07s
Epoch 486/1000, LR 0.000144
Train loss: 0.5257;  Loss pred: 0.5257; Loss self: 0.0000; time: 0.24s
Val loss: 0.0775 score: 0.9592 time: 0.08s
Test loss: 0.2788 score: 0.8776 time: 0.07s
Epoch 487/1000, LR 0.000143
Train loss: 0.5247;  Loss pred: 0.5247; Loss self: 0.0000; time: 0.25s
Val loss: 0.0774 score: 0.9592 time: 0.08s
Test loss: 0.2788 score: 0.8776 time: 0.07s
Epoch 488/1000, LR 0.000143
Train loss: 0.5241;  Loss pred: 0.5241; Loss self: 0.0000; time: 0.26s
Val loss: 0.0772 score: 0.9592 time: 0.46s
Test loss: 0.2788 score: 0.8776 time: 1.38s
Epoch 489/1000, LR 0.000142
Train loss: 0.5219;  Loss pred: 0.5219; Loss self: 0.0000; time: 3.35s
Val loss: 0.0771 score: 0.9592 time: 0.13s
Test loss: 0.2788 score: 0.8776 time: 0.16s
Epoch 490/1000, LR 0.000142
Train loss: 0.5218;  Loss pred: 0.5218; Loss self: 0.0000; time: 0.54s
Val loss: 0.0770 score: 0.9592 time: 0.08s
Test loss: 0.2788 score: 0.8776 time: 0.07s
Epoch 491/1000, LR 0.000141
Train loss: 0.5221;  Loss pred: 0.5221; Loss self: 0.0000; time: 0.25s
Val loss: 0.0769 score: 0.9592 time: 0.08s
Test loss: 0.2788 score: 0.8776 time: 0.07s
Epoch 492/1000, LR 0.000141
Train loss: 0.5215;  Loss pred: 0.5215; Loss self: 0.0000; time: 0.25s
Val loss: 0.0768 score: 0.9592 time: 0.08s
Test loss: 0.2788 score: 0.8776 time: 0.08s
Epoch 493/1000, LR 0.000141
Train loss: 0.5227;  Loss pred: 0.5227; Loss self: 0.0000; time: 0.25s
Val loss: 0.0767 score: 0.9592 time: 0.09s
Test loss: 0.2788 score: 0.8776 time: 0.07s
Epoch 494/1000, LR 0.000140
Train loss: 0.5206;  Loss pred: 0.5206; Loss self: 0.0000; time: 0.26s
Val loss: 0.0767 score: 0.9592 time: 0.09s
Test loss: 0.2788 score: 0.8776 time: 0.08s
Epoch 495/1000, LR 0.000140
Train loss: 0.5229;  Loss pred: 0.5229; Loss self: 0.0000; time: 1.63s
Val loss: 0.0766 score: 0.9592 time: 1.55s
Test loss: 0.2788 score: 0.8776 time: 1.52s
Epoch 496/1000, LR 0.000139
Train loss: 0.5221;  Loss pred: 0.5221; Loss self: 0.0000; time: 2.22s
Val loss: 0.0767 score: 0.9592 time: 0.08s
Test loss: 0.2787 score: 0.8776 time: 0.07s
     INFO: Early stopping counter 1 of 2
Epoch 497/1000, LR 0.000139
Train loss: 0.5221;  Loss pred: 0.5221; Loss self: 0.0000; time: 0.24s
Val loss: 0.0767 score: 0.9592 time: 0.08s
Test loss: 0.2787 score: 0.8776 time: 0.07s
     INFO: Early stopping counter 2 of 2
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 494,   Train_Loss: 0.5229,   Val_Loss: 0.0766,   Val_Precision: 0.9583,   Val_Recall: 0.9583,   Val_accuracy: 0.9583,   Val_Score: 0.9592,   Val_Loss: 0.0766,   Test_Precision: 0.8800,   Test_Recall: 0.8800,   Test_accuracy: 0.8800,   Test_Score: 0.8776,   Test_loss: 0.2788


[0.23318098695017397, 0.08915413194335997, 0.1553904350148514, 1.2970901570515707, 0.06833925400860608, 0.07030992500949651, 0.07088473904877901, 0.07030116498935968, 0.07024163601454347, 0.06723103299736977, 0.06789186701644212, 0.0667899859836325, 0.06855980795808136, 0.06746689195279032, 0.06679393199738115, 0.06712267198599875, 0.06701960798818618, 0.06740954506676644, 0.07030026195570827, 0.11338427697774023, 3.8291455439757556, 0.42795627494342625, 0.0842048330232501, 0.3764946659794077, 0.2627959660021588, 3.683049031998962, 0.28130275302100927, 0.10850626602768898, 0.2526084090350196, 0.4307472830405459, 3.2609803919913247, 1.062235123012215, 0.11042792501393706, 0.08122191694565117, 0.41604348202235997, 1.4065162999322638, 0.24340081098489463, 0.09634289203677326, 0.21544376900419593, 3.6220269959885627, 0.09560070210136473, 2.7187904400052503, 0.2078710999339819, 0.06800023699179292, 0.06682267202995718, 0.07298056094441563, 0.07971640303730965, 0.07243882399052382, 0.0707066769246012, 0.07255370600614697, 0.4617437099805102, 0.4125656640389934, 0.09377711289562285, 0.1522893359651789, 0.43140664999373257, 3.2743856819579378, 0.11159782100003213, 0.27691947703715414, 3.804454207071103, 0.2509990129619837, 0.4491250750143081, 0.17711546504870057, 0.07963992899749428, 0.7267177529865876, 0.2615894739283249, 0.24330914800520986, 0.11096688697580248, 0.06759203202091157, 0.07023688603658229, 0.7988168030278757, 4.1093093829695135, 2.1284215800696984, 0.26571146794594824, 0.07879894506186247, 0.0840049630496651, 0.07400007697287947, 0.07221362204290926, 0.07292547798715532, 0.2444048710167408, 0.3702587209409103, 3.464998741983436, 0.7085147930774838, 0.37218114000279456, 0.2449352879775688, 4.109057744033635, 0.44658450700808316, 0.1139391059987247, 0.12110082805156708, 0.08842127898242325, 0.08522002201061696, 0.07377695001196116, 0.38646809093188494, 1.2775109610520303, 0.3236273319926113, 0.08542765397578478, 0.08015552104916424, 4.00990173406899, 1.8812506700633094, 0.07749449706170708, 0.2874434710247442, 0.11702824500389397, 0.08609981799963862, 0.4541276190429926, 0.1063358229584992, 3.1249869849998504, 0.10431108600459993, 0.0939138219691813, 0.20104122790507972, 0.09252793702762574, 0.07652129302732646, 0.1040340350009501, 0.09643296094145626, 0.07584758603479713, 0.17696274898480624, 0.09873970097396523, 0.08684561401605606, 0.0802544979378581, 0.0764295479748398, 0.07486880198121071, 0.07532561395782977, 0.08653872099239379, 0.25292818795423955, 0.2848157779080793, 1.8538599329767749, 0.45628309599123895, 0.3794238710543141, 0.09326896606944501, 0.47426814096979797, 3.3941070010187104, 0.07314897805918008, 0.07333207898773253, 0.07514994707889855, 0.1109289180021733, 0.07730081607587636, 0.08002729504369199, 0.08410196809563786, 3.3234547378960997, 0.14768716599792242, 2.7006702220533043, 0.7139917779713869, 0.07774489698931575, 0.08457043196540326, 0.07584460300859064, 0.07375106995459646, 0.07382069807499647, 0.07465004397090524, 0.07319042400922626, 3.291164679918438, 0.5645491289906204, 0.07596781197935343, 0.0744674769230187, 0.07441640400793403, 0.07346781401429325, 0.09216894896235317, 0.07418619201052934, 0.39861968299373984, 0.30932084599044174, 0.0740677050780505, 0.073433026089333, 0.07313049701042473, 0.0725328039843589, 0.07408951094839722, 0.07323496206663549, 0.07466926902998239, 0.07270800101105124, 0.0742336290422827, 0.07455009105615318, 1.4103588509606197, 0.07045438198838383, 0.06857793498784304, 0.06816256290767342, 0.06776553497184068, 0.06764916493557394, 0.06829036795534194, 0.06819309899583459, 0.06801709393039346, 0.07339155301451683, 0.07862156198825687, 0.06975168699864298, 1.9483703109435737, 0.31746900698635727, 0.3192789589520544, 0.3425760939717293, 0.23239454091526568, 2.414584164042026, 0.24746851599775255, 0.07055562699679285, 0.23529418697580695, 3.881600628956221, 2.064389727078378, 0.08523847197648138, 0.15779891796410084, 0.4282604119507596, 0.46013281703926623, 3.3409330860013142, 0.09637621603906155, 4.559252391103655, 0.07476810005027801, 0.07036827597767115, 0.06921420502476394, 0.06977331405505538, 0.0713006058940664, 0.07076375803444535, 0.15819104993715882, 0.0793857870157808, 0.40730101603548974, 2.65023025800474, 0.4408392289187759, 0.08030031400267035, 0.08114385791122913, 0.08075247495435178, 0.10893647104967386, 0.08475895901210606, 0.0797769179334864, 0.43790788599289954, 0.44502640911377966, 0.4495101450011134, 0.44458773802034557, 0.38596075295936316, 2.9938261749921367, 0.4650887689786032, 0.42335198691580445, 0.1864769330713898, 0.46511844801716506, 0.7289329940686002, 0.08141366695053875, 0.12341068091336638, 0.15183134807739407, 0.08480624202638865, 2.780747360084206, 0.20626865001395345, 0.07579951803199947, 0.1638178089633584, 0.08565259294118732, 0.0913473810069263, 0.4102826309390366, 2.4935159349115565, 3.8001923830015585, 0.13446865906007588, 0.16927540500182658, 0.1526427820790559, 2.0149686249205843, 0.2361295809969306, 0.4492806929629296, 0.07962428196333349, 0.44318135001230985, 1.5730426739901304, 0.40998273296281695, 0.4444417719496414, 4.056537986034527, 0.08604860794730484, 3.4806995750404894, 2.599598899949342, 1.5112921959953383, 0.5848805810092017, 0.4593315039528534, 2.732989937067032, 0.25936915806960315, 0.08678522205445915, 0.0854891249909997, 0.10091537202242762, 0.14879952208139002, 0.12488683708943427, 0.08495983993634582, 0.08895350701641291, 0.4211612230865285, 2.2998173129744828, 2.2604189579142258, 2.497399334097281, 0.07977873599156737, 0.08318680198863149, 3.027232708991505, 0.41940846806392074, 0.23513564094901085, 0.16076997003983706, 0.13520849798806012, 0.4352562240092084, 0.5241796089103445, 2.6613076669164, 2.2430160420481116, 0.2650066970381886, 0.2519071730785072, 0.45767789194360375, 3.2272743539651856, 1.3644798370078206, 0.22492097690701485, 0.07707408897113055, 0.07747015496715903, 0.2628821940161288, 0.07657243590801954, 3.318577020079829, 0.13061022199690342, 0.08823549293447286, 0.09624280605930835, 3.585166804958135, 0.2326499429764226, 0.0721691909711808, 0.07155396393500268, 0.07110628904774785, 0.07056866702623665, 0.07547588495071977, 0.0714593039592728, 0.07254513900261372, 0.07183685700874776, 2.437410108046606, 0.07192518096417189, 0.0722841250244528, 0.0710478340042755, 0.07143871299922466, 0.07104883901774883, 0.08325342298485339, 0.11250223789829761, 0.07402831909712404, 0.07332919607870281, 1.5950878800358623, 0.0816543570253998, 0.07885405991692096, 0.07475629402324557, 0.06862198002636433, 0.06737405702006072, 0.07706611696630716, 0.06966056802775711, 0.06878474703989923, 0.06922041706275195, 0.0901484350906685, 0.07296331995166838, 0.07958729600068182, 0.089767488068901, 0.07579054404050112, 0.07518336095381528, 0.0738299370277673, 0.0736693199723959, 0.07355900201946497, 0.07264547003433108, 0.0732635511085391, 0.07316710893064737, 0.07316039607394487, 0.07361612806562334, 0.07355018507223576, 0.076388304005377, 0.07822113507427275, 0.07322514511179179, 0.07398701610509306, 0.07300570199731737, 0.07469845307059586, 0.0740685670170933, 0.07394993701018393, 1.026842368999496, 0.141587597085163, 0.07707104505971074, 0.07563296507578343, 0.07441638107411563, 0.07613064500037581, 0.07531803799793124, 0.07493409106973559, 0.07512800605036318, 0.07479634101036936, 0.07509091997053474, 0.8814717150526121, 0.07451961899641901, 0.07615504099521786, 0.07385428203269839, 0.07385344500653446, 0.07357667305041105, 0.0751358870184049, 0.07455443602520972, 0.07470882404595613, 0.07583109603729099, 0.0749007670674473, 0.07445233594626188, 0.07463027397170663, 1.2051177700050175, 2.4355517239309847, 0.0768023889977485, 0.07570971199311316, 0.07357048790436238, 0.07375211489852518, 0.07437768601812422, 0.07444452401250601, 0.07418485905509442, 0.07596777193248272, 0.3183926099445671, 0.07992728089448065, 0.07580054504796863, 0.07599687506444752, 0.06905715900938958, 0.07007613696623594, 0.07419845799449831, 0.07227080990560353, 0.0708592040464282, 0.07032784505281597, 0.07181620504707098, 0.07327521604020149, 0.07137485698331147, 0.07025524298660457, 0.07373559405095875, 0.07042885501869023, 0.06963139597792178, 0.07042304298374802, 0.07059038698207587, 0.07023125002160668, 0.07597536302637309, 0.0791482919594273, 0.0750654760049656, 0.29878277995157987, 0.5465712329605594, 0.12363310100045055, 0.0709574039792642, 0.06988388602621853, 0.07368173298891634, 0.07364413910545409, 0.07364590500947088, 0.07379131496418267, 0.06994892202783376, 0.07078003801871091, 0.07005748094525188, 0.07192587899044156, 0.07100432692095637, 1.567748832050711, 0.8700333489105105, 0.07563688897062093, 0.0740443019894883, 0.07456413307227194, 0.07451072300318629, 0.07405666902195662, 0.07420692103914917, 0.07392498606350273, 0.07443059294018894, 0.07397692708764225, 0.0721939200302586, 0.0710707560647279, 2.4702081049326807, 0.5858036589343101, 0.07809307402931154, 0.07509686797857285, 0.07565925596281886, 0.07521823490969837, 0.07594747503753752, 0.07478547899518162, 0.0747062599984929, 0.0755565509898588, 0.0752038030186668, 0.07481998694129288, 2.068312476039864, 1.7470445659710094, 0.06879084696993232, 0.06799268000759184, 0.06758477003313601, 0.06699467392172664, 0.06826093804556876, 0.0712685149628669, 0.07303125702310354, 0.07228487392421812, 0.07197060703765601, 0.07207076298072934, 1.349237116985023, 0.07799394801259041, 0.07539473997894675, 0.074214993044734, 0.07399695506319404, 0.19787237502168864, 0.07383550005033612, 0.07458865805529058, 0.07303307799156755, 1.8685675889719278, 0.7360555230407044, 0.16186639096122235, 0.08720535202883184, 0.07712222100235522, 0.07526814797893167, 0.07559245894663036, 0.07532568695023656, 0.07532823598012328, 0.07531460793688893, 0.07648374105338007, 0.07641059695743024, 2.194407671922818, 1.2162537761032581, 0.07446306897327304, 0.07338585704565048, 0.07299435406457633, 0.07313150598201901, 0.0745147910201922, 0.07297153805848211, 0.07317016809247434, 0.07359308993909508, 0.0739505480742082, 1.3898088110145181, 0.1634470330318436, 0.0746288769878447, 0.07173322897870094, 0.08277171803638339, 0.07711186306551099, 0.07958456804044545, 1.523146889056079, 0.06975837703794241, 0.07016807608306408]
[0.004758795652044367, 0.0018194720804767341, 0.0031712333676500283, 0.026471227694930016, 0.0013946786532368586, 0.0014348964287652349, 0.0014466273275261022, 0.001434717652844075, 0.0014335027758070097, 0.0013720618979055055, 0.0013855483064580025, 0.0013630609384414796, 0.0013991797542465584, 0.0013768753459753127, 0.0013631414693343093, 0.001369850448693852, 0.001367747101799718, 0.0013757050013625805, 0.0014346992235858829, 0.002313964836280413, 0.07814582742807664, 0.008733801529457678, 0.0017184659800663286, 0.007683564611824648, 0.005363182979635894, 0.0751642659591625, 0.005740872510632843, 0.002214413592401816, 0.005155273653775909, 0.008790760878378488, 0.06655062024472091, 0.021678267816575815, 0.0022536311227334092, 0.0016575901417479832, 0.008490683306578775, 0.028704414284331913, 0.0049673634894876455, 0.00196618147013823, 0.004396811612330529, 0.07391891828548088, 0.0019510347367625456, 0.05548551918378062, 0.004242267345591467, 0.0013877599386080187, 0.001363728000611371, 0.0014893992029472577, 0.00162686536810836, 0.001478343346745384, 0.0014429934066245143, 0.0014806878776764687, 0.009423341020010412, 0.008419707429367212, 0.0019138186305229152, 0.0031079456319424268, 0.00880421734681087, 0.06682419759097832, 0.0022775065510210637, 0.005651417898717432, 0.07764192259328782, 0.005122428835958851, 0.009165817857434859, 0.0036146013275245013, 0.0016253046734182506, 0.014830974550746685, 0.005338560692414793, 0.004965492816432855, 0.002264630346444949, 0.0013794292249165627, 0.0014334058374812712, 0.01630238373526277, 0.08386345679529619, 0.04343717510346323, 0.005422683019305066, 0.001608141735956377, 0.0017143870010135733, 0.0015102056525077444, 0.0014737473886308012, 0.0014882750609623535, 0.004987854510545731, 0.0075563004273655155, 0.07071426004047829, 0.014459485573009873, 0.007595533469444787, 0.004998679346480996, 0.08385832130680887, 0.009113969530777208, 0.0023252878775249937, 0.0024714454704401444, 0.0018045158976004744, 0.0017391841226656521, 0.0015056520410604319, 0.00788710389656908, 0.026071652266367967, 0.006604639428420638, 0.0017434215097098934, 0.0016358269601870253, 0.08183472926671408, 0.03839287081761856, 0.0015815203481981037, 0.005866193286219269, 0.0023883315306917137, 0.0017571391428497676, 0.009267910592714134, 0.0021701188358877388, 0.06377524459183369, 0.0021287976735632637, 0.001916608611615945, 0.004102882202144484, 0.0018883252454617498, 0.0015616590413740094, 0.002123143571447961, 0.0019680196110501277, 0.0015479099190774926, 0.003611484673159311, 0.0020150959382441882, 0.0017723594697154298, 0.0016378468966909818, 0.0015597866933640776, 0.0015279347343104227, 0.0015372574277108116, 0.0017660963467835467, 0.005161799754168154, 0.005812566896083252, 0.037833876183199486, 0.00931189991818855, 0.0077433443072309, 0.0019034482871315309, 0.009678941652444857, 0.06926748981670838, 0.0014928362869220425, 0.00149657304056597, 0.0015336723893652766, 0.002263855469432108, 0.001577567675017885, 0.0016332101029324897, 0.001716366695829344, 0.06782560689583877, 0.0030140237958759678, 0.05511571881741437, 0.014571260774926263, 0.0015866305508023622, 0.0017259271829674135, 0.0015478490409916456, 0.0015051238766244175, 0.0015065448586733974, 0.0015234702851205152, 0.0014936821226372706, 0.06716662612078446, 0.011521410795726947, 0.0015503635097827231, 0.0015197444270003814, 0.0015187021226108987, 0.001499343143148842, 0.0018809989584153708, 0.0015140039185822314, 0.008135095571300812, 0.006312670326335546, 0.0015115858179193978, 0.0014986331854965917, 0.0014924591226617293, 0.001480261305803243, 0.001512030835681576, 0.0014945910625843977, 0.0015238626332649467, 0.0014838367553275762, 0.0015149720212710755, 0.0015214304297174119, 0.028782833693073873, 0.0014378445303751802, 0.0013995496936294498, 0.0013910727124014984, 0.0013829701014661364, 0.001380595202766815, 0.0013936809786804477, 0.0013916958978741753, 0.0013881039577631317, 0.0014977867962146293, 0.001604521673229732, 0.0014235038162988363, 0.03976265940701171, 0.00647895932625219, 0.006515897121470497, 0.006991348856565904, 0.004742745732964605, 0.04927722783759236, 0.005050377877505154, 0.0014399107550365888, 0.004801922183179734, 0.07921633936645349, 0.04213040259343629, 0.0017395606525812525, 0.003220386080900017, 0.00874000840715836, 0.009390465653862576, 0.06818230787757784, 0.0019668615518175824, 0.09304596716538072, 0.0015258795928628165, 0.0014360872648504315, 0.001412534796423754, 0.0014239451847970486, 0.001455114406001355, 0.0014441583272335784, 0.003228388774227731, 0.0016201181023628736, 0.008312265633377341, 0.0540863317960151, 0.008996718957526038, 0.0016387819184218437, 0.001655997100229166, 0.0016480096929459547, 0.002223193286728038, 0.0017297746737164501, 0.0016281003659895184, 0.008936895632508154, 0.009082171614566932, 0.009173676428594152, 0.009073219143272358, 0.007876750060395166, 0.061098493367186466, 0.009491607530175574, 0.00863983646766948, 0.0038056516953344855, 0.009492213224840104, 0.014876183552420413, 0.0016615034071538522, 0.002518585324762579, 0.003098598940354981, 0.001730739633191605, 0.056749946124167464, 0.00420956428599905, 0.0015469289394285605, 0.003343220591088947, 0.0017480121008405577, 0.0018642322654474754, 0.008373114917123196, 0.05088808030431748, 0.07755494659186854, 0.0027442583481648137, 0.0034546001020780938, 0.0031151588179399166, 0.041121808671848656, 0.004818971040753686, 0.009168993733937338, 0.0016249853461904793, 0.009044517347189997, 0.032102911714084297, 0.00836699455026157, 0.009070240243870233, 0.08278648951090872, 0.0017560940397409151, 0.07103468520490795, 0.05305303877447637, 0.030842697877455885, 0.011936338387942893, 0.009374112325568437, 0.05577530483810269, 0.0052932481238694516, 0.001771126980703248, 0.0017446760202244837, 0.0020594973882128087, 0.003036724940436531, 0.0025487109610088624, 0.0017338742844152208, 0.0018153776942125084, 0.008595127001765887, 0.04693504720356087, 0.046130999141106646, 0.0509673333489241, 0.0016281374692156607, 0.0016976898365026834, 0.06178025936717357, 0.008559356491100423, 0.004798686549979813, 0.0032810197967313684, 0.0027593571017971454, 0.00888278008182058, 0.010697543038986623, 0.05431240136564082, 0.045775837592818606, 0.005408299939554869, 0.005140962715887902, 0.009340365141706199, 0.06586274191765684, 0.02784652728587389, 0.004590224018510507, 0.0015729405912475623, 0.0015810235707583477, 0.0053649427350230365, 0.001562702773633052, 0.06772606163428221, 0.002665514734630682, 0.001800724345601487, 0.001964138899169558, 0.07316666948894153, 0.004747958019926992, 0.0014728406320649143, 0.001460284978265361, 0.0014511487560764868, 0.0014401768780864623, 0.0015403241826677505, 0.0014583531420259755, 0.0014805130408696678, 0.0014660583063009747, 0.049743063429522574, 0.0014678608360035078, 0.0014751862249888328, 0.0014499557960056225, 0.0014579329183515236, 0.00144997630648467, 0.0016990494486704773, 0.0022959640387407677, 0.0015107820223902865, 0.0014965142056878125, 0.032552813878282905, 0.0016664154494979552, 0.0016092665289167542, 0.001525638653535624, 0.001400448571966619, 0.0013749807555114432, 0.0015727778972715748, 0.0014216442454644308, 0.0014037703477530455, 0.0014126615727092235, 0.0018397639814422143, 0.0014890473459524159, 0.0016242305306261595, 0.0018319895524265511, 0.0015467457967449207, 0.0015343543051799036, 0.001506733408729945, 0.0015034555096407325, 0.0015012041228462238, 0.0014825606129455324, 0.0014951745124191654, 0.0014932063047070892, 0.001493069307631528, 0.0015023699605229254, 0.0015010241851476686, 0.0015589449797015712, 0.0015963496953933214, 0.0014943907165671795, 0.0015099391041855728, 0.0014899122856595383, 0.0015244582259305278, 0.001511603408512108, 0.0015091823879629374, 0.02095596671427543, 0.0028895427976563877, 0.0015728784706063416, 0.0015435298995057844, 0.0015187016545737885, 0.0015536866326607307, 0.001537102816284311, 0.0015292671646884813, 0.0015332246132727181, 0.0015264559389871297, 0.0015324677545007091, 0.017989218674543103, 0.0015208085509473268, 0.001554184510106487, 0.0015072302455652732, 0.0015072131633986623, 0.0015015647561308376, 0.001533385449355202, 0.0015215191025553004, 0.0015246698784889007, 0.0015475733885161427, 0.0015285870830091287, 0.0015194354274747322, 0.001523066815749115, 0.02459424020418403, 0.04970513722308132, 0.0015673956938316018, 0.0015450961631247584, 0.0015014385286604567, 0.0015051452020107179, 0.0015179119595535556, 0.0015192760002552246, 0.0015139767154100903, 0.0015503626924996473, 0.006497808366215655, 0.0016311689978465438, 0.0015469498989381352, 0.0015509566339683167, 0.0014093297757018283, 0.0014301252442088966, 0.0015142542447856798, 0.0014749144878694598, 0.001446106205029147, 0.0014352621439350198, 0.001465636837695326, 0.00149541257224901, 0.0014566297343532955, 0.001433780469114379, 0.001504808041856301, 0.0014373235718100046, 0.0014210488975086079, 0.0014372049588520005, 0.0014406201424913443, 0.0014332908167674833, 0.0015505176127831243, 0.0016152712644781082, 0.0015319484898972573, 0.006097607754113875, 0.011154514958378764, 0.0025231245102132764, 0.0014481102852911061, 0.001426201755637113, 0.0015037088365084967, 0.0015029416143970222, 0.0015029776532545077, 0.0015059452033506669, 0.0014275290209761992, 0.0014444905718104268, 0.001429744509086773, 0.0014678750814375828, 0.0014490678963460484, 0.031994874123483896, 0.017755782630826746, 0.0015436099789922638, 0.0015111082038671082, 0.0015217170014749377, 0.0015206270000650262, 0.001511360592284829, 0.001514426959982636, 0.0015086731849694435, 0.0015189916926569172, 0.00150973320587025, 0.0014733453067399714, 0.0014504235931577123, 0.050412410304748585, 0.011955176712945104, 0.0015937362046798272, 0.001532589142419854, 0.0015440664482207931, 0.0015350660185652729, 0.0015499484701538269, 0.0015262342652077882, 0.0015246175509896509, 0.0015419704283644654, 0.0015347714901768736, 0.0015269385090059772, 0.0422104586946911, 0.03565397073410223, 0.0014038948361210677, 0.00138760571444065, 0.0013792810210844083, 0.0013672382433005438, 0.0013930803682769136, 0.0014544594890381001, 0.0014904338167980313, 0.0014752015086575126, 0.0014687878987276737, 0.001470831897565905, 0.027535451367041285, 0.001591713224746743, 0.001538668162835648, 0.00151459169479049, 0.0015101419400651843, 0.004038211735136503, 0.001506846939802778, 0.0015222175113324607, 0.0014904709794197459, 0.038134032427998524, 0.015021541286544988, 0.003303395733902497, 0.0017797010618128947, 0.001573922877599086, 0.0015360846526312586, 0.001542703243808783, 0.0015372589173517665, 0.0015373109383698628, 0.0015370328150385497, 0.001560892674558777, 0.0015593999379067396, 0.04478383003924118, 0.02482150563476037, 0.0015196544688423068, 0.0014976705519520507, 0.0014896806951954352, 0.0014924797139187552, 0.0015207100208202492, 0.0014892150624180023, 0.0014932687365811091, 0.0015018997946754098, 0.0015091948586573101, 0.02836344512274527, 0.003335653735343747, 0.0015230383058743818, 0.001463943448544917, 0.0016892187354363957, 0.0015737114911328774, 0.0016241748579682745, 0.031084630388899574, 0.0014236403477131104, 0.0014320015527155934]
[210.13720132538208, 549.6099724366104, 315.33472440125956, 37.77686518829378, 717.0110460062872, 696.9144113492049, 691.263037115519, 697.001251791721, 697.5919522981297, 728.8300925246381, 721.7359332323728, 733.6429148526531, 714.7044523514336, 726.2821597634518, 733.5995731157313, 730.0066959524646, 731.1293138067508, 726.9000250849857, 697.0102050383794, 432.15868466154046, 12.796588543647754, 114.49767854549525, 581.9143419769081, 130.14792619314304, 186.45643898353248, 13.304194316795565, 174.18955013334124, 451.5868234512468, 193.97612370539508, 113.75579586740578, 15.026155974546674, 46.12914687009134, 443.7283413033047, 603.2854412041008, 117.77615109318437, 34.83784724169907, 201.31403754049498, 508.60005304072786, 227.4375361445041, 13.528336496185167, 512.5485370185426, 18.022720427068066, 235.72300341683854, 720.585723927902, 733.2840563159893, 671.4116658725054, 614.6790137666716, 676.4328477559217, 693.003859483478, 675.3617795326481, 106.11947481010245, 118.76897248378089, 522.5155529637459, 321.7559502078589, 113.58193018283762, 14.964639098562197, 439.07667336968785, 176.94674467923286, 12.879639846611044, 195.21989119304442, 109.1010115577247, 276.65568326586674, 615.2692577305247, 67.42645242754149, 187.3164056036365, 201.3898795081506, 441.57316957701966, 724.9375190383448, 697.6391290251502, 61.34072269670332, 11.924144773103233, 23.021754928079343, 184.4105577331262, 621.8357360181878, 583.298869746903, 662.1614733989827, 678.5423388801111, 671.9188046820973, 200.48700255505008, 132.3398943189779, 14.14141927565359, 69.15875360508008, 131.6563219716939, 200.05284009745228, 11.924875008424538, 109.7216746910414, 430.054278296237, 404.6215107557716, 554.1652480478192, 574.982249991621, 664.1640782392851, 126.78925155721655, 38.3558352874315, 151.408719709492, 573.5847552818141, 611.31160222819, 12.219750819249617, 26.04650235066814, 632.3029616023242, 170.46830051597135, 418.7023397502849, 569.1068940494818, 107.89918504243437, 460.8042580262321, 15.68006530433673, 469.74872831675043, 521.7549341786964, 243.73110187694945, 529.5697880454229, 640.3446421442676, 470.99970696659517, 508.1250178530507, 646.0324258377838, 276.8944327611404, 496.25428795778777, 564.2196276134435, 610.5576791215018, 641.1133036679811, 654.4782166047906, 650.5091352781023, 566.22052461703, 193.73087830315382, 172.04103073185127, 26.43133881280872, 107.38947033212216, 129.1431660950655, 525.3623157301455, 103.31708113432056, 14.436787050406217, 669.8658176790561, 668.1932474353692, 652.0297339471949, 441.7243121314858, 633.8872276833785, 612.2910935981002, 582.6260800969472, 14.743694096769696, 331.7823838578452, 18.143644344234513, 68.62824126521477, 630.2664470278213, 579.398719638151, 646.0578347868727, 664.3971406810231, 663.7704773560872, 656.3961304443127, 669.486489022432, 14.888346456493426, 86.79492622300035, 645.0100209983308, 658.0053739520961, 658.4569713255129, 666.958730941239, 531.6324049655191, 660.500271978448, 122.92418586056004, 158.41156726150342, 661.5568816175033, 667.2746938195132, 670.0351016760499, 675.5564008054403, 661.3621735757998, 669.0793388465959, 656.2271284633139, 673.9285817052274, 660.0781967979783, 657.2761925011179, 34.74293082687804, 695.4854846087342, 714.5155363556272, 718.8696831480762, 723.0814310011938, 724.3252750668161, 717.5243224936678, 718.5477815430129, 720.4071383900208, 667.6517662776234, 623.2386989121225, 702.4919698494647, 25.14922328921644, 154.34577524635563, 153.47080860207345, 143.0339152738535, 210.84832632908572, 20.293349359988248, 198.00498581583204, 694.487485771012, 208.24993863974294, 12.623658300770709, 23.735828248548355, 574.8577944184624, 310.521774370769, 114.41636591344313, 106.49099169950858, 14.66656132842426, 508.4241944105812, 10.747376060077825, 655.3597051021735, 696.3365141352674, 707.9471617490722, 702.2742242304275, 687.231186685859, 692.4448525776216, 309.7520372958216, 617.2389522353601, 120.30414379258616, 18.488959535497223, 111.15163258083865, 610.2093199582075, 603.8657917103928, 606.7925475683438, 449.80344532784187, 578.1099788283309, 614.2127481141036, 111.89567844593351, 110.10582517468575, 109.00755087491657, 110.21446569396304, 126.95591358523204, 16.367015696937948, 105.35623147299498, 115.74293144806957, 262.76708434088795, 105.34950873028298, 67.22154217015533, 601.8645497170514, 397.04829142299064, 322.7265029289135, 577.7876584220412, 17.621162103167904, 237.5542768941635, 646.4421050713568, 299.11277845841516, 572.0784195482029, 536.413846350829, 119.42986688920021, 19.65096726030668, 12.894084051949404, 364.3971788110754, 289.46910509220913, 321.0109206121662, 24.31799651566844, 207.51317896353248, 109.06322209586473, 615.3901648063113, 110.56421936220686, 31.149822449322464, 119.51722855714515, 110.25066294972846, 12.079265661678173, 569.4455862668577, 14.077629782061845, 18.849061676766695, 32.422585208764716, 83.77778574123852, 106.67676738547733, 17.929081748681966, 188.91991771377297, 564.612255866001, 573.1723187616989, 485.5553620622847, 329.30213292753785, 392.3552004516697, 576.7430828107975, 550.8495577465985, 116.3449940640258, 21.306040146565184, 21.677397381773048, 19.62041045298497, 614.1987509701746, 589.0357463999693, 16.186400158289747, 116.83121284173038, 208.39035631618964, 304.78328749988776, 362.4032566675436, 112.57736775973902, 93.47940890310547, 18.41200121621986, 21.845586068683577, 184.90098758877363, 194.51609654929948, 107.06219562389941, 15.183090938579866, 35.91112061241778, 217.85429119960304, 635.7519194077507, 632.5016391250535, 186.3952793888873, 639.9169546971165, 14.765364704062616, 375.16206044854374, 555.3320820272328, 509.1289625305023, 13.667425440912558, 210.6168579846409, 678.9600844987591, 684.79784075289, 689.1092286801314, 694.3591549176115, 649.2139844666069, 685.7049717126666, 675.441534383642, 682.1011113283137, 20.10330548734356, 681.2634927454459, 677.8805164124753, 689.6761975467301, 685.9026141824784, 689.6664418085597, 588.5643886247747, 435.5468914698049, 661.9088559300224, 668.2195171948871, 30.71931058676111, 600.0904518145654, 621.4011054297699, 655.463204004781, 714.0569243436923, 727.2829063182314, 635.8176839430291, 703.4108590741803, 712.3672341424342, 707.8836285481915, 543.5479822885147, 671.5703182428936, 615.6761501179814, 545.8546412971929, 646.518647152279, 651.739951212083, 663.6874142473017, 665.1344144124099, 666.1319302161517, 674.5086786119408, 668.8182494376647, 669.6998243629586, 669.7612728951684, 665.6150124646615, 666.2117838571811, 641.4594568895112, 626.4291607821004, 669.169039203574, 662.2783642254087, 671.1804511077851, 655.9707461905695, 661.5491830521298, 662.6104359392764, 47.71910614454208, 346.07551091164555, 635.7770283514034, 647.8656489389583, 658.457174250358, 643.6304329190712, 650.5745675603748, 653.9079783379149, 652.220158314226, 655.1122600129183, 652.5422783370789, 55.588851194250005, 657.5449614463898, 643.4242482132856, 663.468639209107, 663.4761587041003, 665.9719442115527, 652.1517472469209, 657.2378870042181, 655.8796852411746, 646.1729100671785, 654.1989076810929, 658.139189015737, 656.570013645891, 40.659926539624415, 20.11864478940891, 638.000987201537, 647.2089076822433, 666.0279331529964, 664.3877272864463, 658.7997371692871, 658.2082517146384, 660.5121398641395, 645.0103610192667, 153.89804433127708, 613.0572621967386, 646.4333464751669, 644.7633532095444, 709.5571364778785, 699.2394575575587, 660.3910825698462, 678.0054086013608, 691.5121424154628, 696.7368325191987, 682.2972610134941, 668.711777978475, 686.516261762275, 697.4568433183376, 664.5365868502537, 695.7375636306526, 703.705552816097, 695.7949830612694, 694.1455075524938, 697.6951141397187, 644.9459146775091, 619.0910604251346, 652.7634620842028, 163.9987418550053, 89.64979685188781, 396.3339882562796, 690.5551394512574, 701.1630690030107, 665.0223605268742, 665.3618413521662, 665.3458871025971, 664.0347854457391, 700.5111527022839, 692.285584631174, 699.4256621686446, 681.2568812195086, 690.0987886913978, 31.255006540751186, 56.31968022991268, 647.8320389278928, 661.7659790615122, 657.1524133795845, 657.6234671337792, 661.6554679967078, 660.3157672334793, 662.8340782899604, 658.3314476531915, 662.3686861438366, 678.7275158276855, 689.4537600721892, 19.836385405000268, 83.64577320862156, 627.4564115840578, 652.4905940681975, 647.6405216577865, 651.4377804640842, 645.1827394627859, 655.2074100261768, 655.9021961611853, 648.5208675893209, 651.5627938102732, 654.9052198906102, 23.690811019918435, 28.047366938670933, 712.3040659961249, 720.6658127687983, 725.015413620197, 731.4014253916549, 717.8336747627055, 687.5406345358889, 670.945592303016, 677.8734933033228, 680.8334960182082, 679.8873492306703, 36.31681887724403, 628.2538741607237, 649.9127129251029, 660.2439478834775, 662.1894097960326, 247.63436530556197, 663.637409736442, 656.9363396198603, 670.9288632974989, 26.223295474668618, 66.57106490768125, 302.71880227278757, 561.8921185456555, 635.3551461971459, 651.0057881816836, 648.2128069758239, 650.5085049190662, 650.4864923815492, 650.6041967457405, 640.6590384458518, 641.2723097464976, 22.329488101481374, 40.28764470272853, 658.0443255379056, 667.703587211893, 671.2847949397689, 670.0258574197519, 657.5875652220759, 671.4946855132692, 669.6718249720643, 665.8233815233457, 662.6049606938583, 35.256647973206825, 299.79130909310277, 656.582304032003, 683.0864955842027, 591.9896452851373, 635.4404893365327, 615.6972539588058, 32.1702393590983, 702.4245987452994, 698.3232651554308]
Elapsed: 0.47284832942909494~0.9015195902238966
Time per graph: 0.009649965906716223~0.018398358984161155
Speed: 454.07382206103523~263.77878773702184
Total Time: 0.0717
best val loss: 0.07664584368467331 test_score: 0.8776

Testing...
Test loss: 0.6855 score: 0.7959 time: 0.07s
test Score 0.7959
Epoch Time List: [9.791504344088025, 0.7141125989146531, 0.5960637530079111, 1.770690695848316, 0.36285667412448674, 0.36110068403650075, 0.3778658480150625, 0.369993286090903, 0.37029301293659955, 0.36427700403146446, 0.3588374999817461, 0.3564380049938336, 0.35870629001874477, 0.35387867491226643, 0.3528189380886033, 0.3520884660538286, 0.3523020161082968, 0.3529084980254993, 0.36624529503751546, 0.4400629800511524, 4.28879822883755, 6.340483248815872, 0.5935611708555371, 0.7618988329777494, 1.8011790658347309, 8.479673536960036, 5.186743880040012, 0.6507161649642512, 1.4511513229226694, 1.7368644509697333, 4.676833693985827, 11.926085546961986, 2.819896937930025, 0.42066908918786794, 1.049370611901395, 9.321313038119115, 0.8049108500126749, 0.5842913101660088, 0.5562616579700261, 5.816162660950795, 7.71989314595703, 3.1768658970249817, 8.459914216888137, 1.28778335894458, 0.3679475039243698, 0.386679936083965, 0.4120546339545399, 0.3858008049428463, 0.39363479404710233, 0.38686743297148496, 0.9645626300480217, 11.651263974956237, 0.6703446110477671, 0.4992066101403907, 1.8484740209532902, 9.002799735055305, 4.278910040855408, 1.620391158037819, 11.276761388056912, 8.0684481728822, 1.346844429965131, 1.1786989690735936, 0.6350002819672227, 9.219177598133683, 1.3948310260893777, 1.3316548248985782, 1.458006887929514, 0.44509050308261067, 0.386017058044672, 1.2720112439710647, 7.069779208977707, 15.108253574930131, 3.6973957548616454, 0.5094977499684319, 0.4068975360132754, 0.4206846609013155, 0.42252324195578694, 0.4114400140242651, 0.7222872320562601, 1.4199529628967866, 19.02788251102902, 5.233059666934423, 0.8966803771909326, 0.6881888518109918, 5.646182882948779, 7.638233274105005, 0.7137740418547764, 0.5235790819860995, 0.5280283830361441, 0.49849183904007077, 0.44386296800803393, 1.1403752469923347, 9.178591525065713, 4.283896359964274, 0.6407826719805598, 0.4268434800906107, 7.250634026830085, 15.481539977015927, 0.43215504416730255, 0.7432681419886649, 0.6686337129212916, 0.4350144111085683, 0.9550108109833673, 0.646171759115532, 13.304370902944356, 0.7961710868403316, 0.7389946748735383, 0.5510749380337074, 0.4971121409907937, 0.4112602900713682, 0.4887806080514565, 0.4469495629891753, 0.4529327671043575, 0.5043962049530819, 0.8899830270092934, 0.5221824169857427, 0.6625034969765693, 0.553747285855934, 0.3987812161212787, 0.5001288829371333, 0.7129855368984863, 0.664292388013564, 1.082372328150086, 8.733015589998104, 8.274205449037254, 1.5913300738902763, 1.272691491059959, 1.615473901038058, 13.398587472969666, 1.7830365769332275, 0.4808436209568754, 0.4309548099990934, 0.4437183398986235, 0.3990358761511743, 0.42058171005919576, 0.5170186260947958, 6.345429116045125, 7.86216136906296, 3.0359080989146605, 8.285366571159102, 1.0117577619384974, 0.4285808259155601, 0.4202912870096043, 0.3988366359844804, 0.3936973309610039, 0.39055500202812254, 0.3870843310141936, 4.3257229728624225, 5.127948538982309, 0.6142592080868781, 0.393052269006148, 0.3943920099409297, 0.39318897610064596, 0.4536664988845587, 2.6110088831046596, 0.7166562899947166, 6.224221398937516, 1.7741172250825912, 0.3907422620104626, 0.3924009809270501, 0.39398659102153033, 0.3969373081345111, 0.39382496604230255, 0.38684334594290704, 0.38413563510403037, 0.39043578098062426, 0.39346795505844057, 7.921178663033061, 6.1988871219800785, 0.3711966589326039, 0.3616134349722415, 0.35893993207719177, 0.362536404049024, 0.36805543303489685, 0.3663916072109714, 0.364633179968223, 0.37490066804457456, 0.40869415807537735, 0.465380122885108, 13.81432059709914, 2.8583990728948265, 1.1105411569587886, 1.1765951809938997, 1.7177718030288815, 3.7815142510225996, 8.920126826036721, 0.47030011797323823, 0.8687703389441594, 5.258275065221824, 9.342972929007374, 2.1513166889781132, 0.4782160061877221, 1.3479941400000826, 1.8501070219790563, 10.108196928864345, 7.053806724958122, 7.8465315400389954, 3.0602685931371525, 0.38097329693846405, 0.3754576080245897, 0.4732358528999612, 0.3748277850681916, 0.3848708029836416, 0.4723614789545536, 0.4475824430119246, 0.9837264540838078, 7.051806709030643, 8.750421261880547, 0.557300150860101, 0.4638173161074519, 0.7332857140572742, 0.46039258199743927, 0.4308425378985703, 0.5279091581469402, 1.8044309570686892, 1.4529706500470638, 11.172454023151658, 2.014866207027808, 1.5054404969559982, 6.580290523939766, 11.54351308196783, 1.1564229660434648, 0.726065898896195, 1.8116492320550606, 1.5529105662135407, 12.705336259910837, 0.49341841496061534, 1.1605896919500083, 0.4904380710795522, 3.1172322179190814, 7.5510274599073455, 0.503226934117265, 0.6079601360252127, 0.4530108099570498, 0.5709767340449616, 1.1174318100092933, 3.88515841611661, 9.151130654034205, 4.534630280104466, 1.4186587170697749, 1.1900096820900217, 7.3710298899095505, 4.724464745959267, 1.6148364819819108, 1.3882208779687062, 1.418510522111319, 13.538397640222684, 1.5666188290342689, 1.6052035021129996, 8.458555100020021, 8.562084065866657, 6.132049727952108, 8.302883691969328, 6.502457805909216, 1.9608553709695116, 1.6742523559369147, 12.883484293008223, 7.625683302991092, 1.1724543699529022, 0.46609585802070796, 0.5434855420608073, 0.5137292231665924, 0.5371060039615259, 0.4291786761023104, 0.8520416209939867, 0.9246754738269374, 6.640371938003227, 11.19391930301208, 16.46409208397381, 0.430555145139806, 0.4494451809441671, 5.9546980179147795, 7.806204485008493, 0.7715124451788142, 0.7930648879846558, 0.5678650920744985, 0.9860094911418855, 1.2518637790344656, 12.353645834024064, 8.487111538997851, 1.4747344958595932, 1.6227903899271041, 1.7855017790570855, 12.798100810032338, 6.309093381976709, 2.96766234585084, 0.5154688709881157, 0.6154860638780519, 1.0759720019996166, 0.7862536769825965, 7.291356415953487, 5.341052417992614, 0.6691781372064725, 0.5053598930826411, 7.053253382095136, 3.5165523999603465, 0.3925429950468242, 0.38678354397416115, 0.38047290698159486, 0.38168894802220166, 0.381035408237949, 0.38448244286701083, 0.3804106229217723, 0.3782966759754345, 5.474929261952639, 0.4263767891097814, 0.3849964201217517, 0.3851935957791284, 0.3827999149216339, 0.3792867539450526, 0.38604073389433324, 2.2303922929568216, 0.4346663700416684, 0.386396485962905, 11.98328515002504, 3.533120305859484, 0.381004061200656, 0.382046829094179, 0.3792653901036829, 0.36369408876635134, 0.38049119093921036, 0.3829227959504351, 0.37596748501528054, 0.3720406040083617, 4.004798694979399, 0.39919261494651437, 0.3968466498190537, 0.432285274961032, 0.4033426941605285, 0.4180736531270668, 0.39461519895121455, 0.3929205930326134, 0.40343510394450277, 0.3895963348913938, 0.39770579477772117, 0.3958736459026113, 0.3913914009463042, 0.3962495488813147, 0.39278865593951195, 0.4013830909971148, 0.42020898195914924, 0.3966312779812142, 0.39930949290283024, 0.3942914019571617, 0.4136900659650564, 0.3949370358604938, 0.39669945009518415, 6.59437210788019, 0.7487019500695169, 0.4980464648688212, 0.4061325431102887, 0.40082040696870536, 0.40997563605196774, 0.40469424601178616, 0.40672096505295485, 0.40413439203985035, 0.40063640393782407, 0.40312510600779206, 2.688577842898667, 5.763549114111811, 0.39854798605665565, 0.391276236041449, 0.3907427790109068, 0.3951019700616598, 0.3962687939638272, 0.4015082159312442, 0.40015375695656985, 0.3984054910251871, 0.39726760517805815, 0.3963578548282385, 0.4029371850192547, 1.5290819120127708, 7.25278913800139, 5.2721232859184965, 0.41990978410467505, 0.4033970650052652, 0.3953868799144402, 0.3952255879994482, 0.40178813401144, 0.39497212099377066, 0.40572988719213754, 0.6416457251179963, 5.671802565921098, 0.38297279307153076, 0.39759405609220266, 0.3729140170617029, 0.3761252610711381, 0.3850541359279305, 0.38534634397365153, 0.38550438010133803, 0.38128629699349403, 0.3951148260384798, 0.4024144969880581, 0.3911515501094982, 0.37754721590317786, 0.39093534008134156, 0.38231315603479743, 0.37717340397648513, 0.38039161113556474, 0.3865030419547111, 0.38070614088792354, 0.3911111420020461, 0.3961901569273323, 0.40043812908697873, 0.6196145841386169, 2.7685052130836993, 5.1690066220471635, 0.3915521439630538, 0.375646970118396, 0.3833115821471438, 0.39029301702976227, 0.39134674391243607, 0.3967230130219832, 0.3842235410120338, 0.3741839840076864, 0.3779927780851722, 0.3791639490518719, 0.3841461519477889, 2.995639121043496, 8.34860189189203, 1.8303169769933447, 0.4033381320768967, 0.3998553629498929, 0.4001407269388437, 0.40173630113713443, 0.3998226139228791, 0.39606392208952457, 0.4026541080093011, 0.4112836408894509, 0.3805480970768258, 0.3831971469335258, 3.0551746719283983, 9.311158633907326, 0.8360018010716885, 0.4075958019820973, 0.407758206827566, 0.4013279710197821, 0.40715754497796297, 0.4077380870003253, 0.38072867796290666, 0.40998608304653317, 0.40707787906285375, 0.40304192702751607, 4.099795997026376, 7.626928568002768, 2.071900616050698, 0.37258218601346016, 0.36110349104274064, 0.3598232758231461, 0.3737938730046153, 0.36756118503399193, 0.37980977399274707, 0.3872302200179547, 0.38618721801321954, 0.38802508695516735, 8.059806192060933, 1.6600154740735888, 0.4073899150826037, 0.40378460800275207, 0.4040606978815049, 0.6615600368240848, 0.5356555231846869, 0.3946660141227767, 0.3889402230270207, 4.919183832011186, 3.262384469155222, 1.6407603099942207, 0.872092310921289, 0.41501454694662243, 0.40495114389341325, 0.4037927580066025, 0.404274033033289, 0.40453545504715294, 0.40672554704360664, 0.40498330199625343, 0.415166451013647, 4.480373556143604, 9.012863005045801, 1.2722134329378605, 0.3936072359792888, 0.3878300270298496, 0.3883174420334399, 0.3891726810252294, 0.3908976790262386, 0.3937761790584773, 0.3917856100015342, 0.39725160994566977, 2.1058327050413936, 3.6401148450095206, 0.6883702820632607, 0.39371923997532576, 0.4101715498836711, 0.4124808310298249, 0.42731552303303033, 4.693959604832344, 2.367622882942669, 0.38466769794467837]
Total Epoch List: [497]
Total Time List: [0.0717103669885546]
========================k_idx:1========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: AEtransGAT
AEtransGAT(
  (gae): GAE(
    (encoder): transGAT(
      (encoder): EdgeFeatureGAT()
      (classifier): Sequential(
        (0): Linear(in_features=64, out_features=64, bias=True)
        (1): ReLU(inplace=True)
        (2): Linear(in_features=64, out_features=2, bias=True)
      )
    )
    (decoder): InnerProductDecoder()
  )
)
Total number of parameters: 1914242
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7d35981b1cc0>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 2.4083;  Loss pred: 2.4083; Loss self: 0.0000; time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6934 score: 0.5102 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6945 score: 0.4898 time: 0.08s
Epoch 2/1000, LR 0.000000
Train loss: 2.4024;  Loss pred: 2.4024; Loss self: 0.0000; time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6934 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6945 score: 0.4898 time: 0.08s
     INFO: Early stopping counter 1 of 2
Epoch 3/1000, LR 0.000030
Train loss: 2.3831;  Loss pred: 2.3831; Loss self: 0.0000; time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6934 score: 0.5102 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6944 score: 0.4898 time: 0.08s
     INFO: Early stopping counter 2 of 2
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 000,   Train_Loss: 2.4083,   Val_Loss: 0.6934,   Val_Precision: 0.5102,   Val_Recall: 1.0000,   Val_accuracy: 0.6757,   Val_Score: 0.5102,   Val_Loss: 0.6934,   Test_Precision: 0.4898,   Test_Recall: 1.0000,   Test_accuracy: 0.6575,   Test_Score: 0.4898,   Test_loss: 0.6945


[0.23318098695017397, 0.08915413194335997, 0.1553904350148514, 1.2970901570515707, 0.06833925400860608, 0.07030992500949651, 0.07088473904877901, 0.07030116498935968, 0.07024163601454347, 0.06723103299736977, 0.06789186701644212, 0.0667899859836325, 0.06855980795808136, 0.06746689195279032, 0.06679393199738115, 0.06712267198599875, 0.06701960798818618, 0.06740954506676644, 0.07030026195570827, 0.11338427697774023, 3.8291455439757556, 0.42795627494342625, 0.0842048330232501, 0.3764946659794077, 0.2627959660021588, 3.683049031998962, 0.28130275302100927, 0.10850626602768898, 0.2526084090350196, 0.4307472830405459, 3.2609803919913247, 1.062235123012215, 0.11042792501393706, 0.08122191694565117, 0.41604348202235997, 1.4065162999322638, 0.24340081098489463, 0.09634289203677326, 0.21544376900419593, 3.6220269959885627, 0.09560070210136473, 2.7187904400052503, 0.2078710999339819, 0.06800023699179292, 0.06682267202995718, 0.07298056094441563, 0.07971640303730965, 0.07243882399052382, 0.0707066769246012, 0.07255370600614697, 0.4617437099805102, 0.4125656640389934, 0.09377711289562285, 0.1522893359651789, 0.43140664999373257, 3.2743856819579378, 0.11159782100003213, 0.27691947703715414, 3.804454207071103, 0.2509990129619837, 0.4491250750143081, 0.17711546504870057, 0.07963992899749428, 0.7267177529865876, 0.2615894739283249, 0.24330914800520986, 0.11096688697580248, 0.06759203202091157, 0.07023688603658229, 0.7988168030278757, 4.1093093829695135, 2.1284215800696984, 0.26571146794594824, 0.07879894506186247, 0.0840049630496651, 0.07400007697287947, 0.07221362204290926, 0.07292547798715532, 0.2444048710167408, 0.3702587209409103, 3.464998741983436, 0.7085147930774838, 0.37218114000279456, 0.2449352879775688, 4.109057744033635, 0.44658450700808316, 0.1139391059987247, 0.12110082805156708, 0.08842127898242325, 0.08522002201061696, 0.07377695001196116, 0.38646809093188494, 1.2775109610520303, 0.3236273319926113, 0.08542765397578478, 0.08015552104916424, 4.00990173406899, 1.8812506700633094, 0.07749449706170708, 0.2874434710247442, 0.11702824500389397, 0.08609981799963862, 0.4541276190429926, 0.1063358229584992, 3.1249869849998504, 0.10431108600459993, 0.0939138219691813, 0.20104122790507972, 0.09252793702762574, 0.07652129302732646, 0.1040340350009501, 0.09643296094145626, 0.07584758603479713, 0.17696274898480624, 0.09873970097396523, 0.08684561401605606, 0.0802544979378581, 0.0764295479748398, 0.07486880198121071, 0.07532561395782977, 0.08653872099239379, 0.25292818795423955, 0.2848157779080793, 1.8538599329767749, 0.45628309599123895, 0.3794238710543141, 0.09326896606944501, 0.47426814096979797, 3.3941070010187104, 0.07314897805918008, 0.07333207898773253, 0.07514994707889855, 0.1109289180021733, 0.07730081607587636, 0.08002729504369199, 0.08410196809563786, 3.3234547378960997, 0.14768716599792242, 2.7006702220533043, 0.7139917779713869, 0.07774489698931575, 0.08457043196540326, 0.07584460300859064, 0.07375106995459646, 0.07382069807499647, 0.07465004397090524, 0.07319042400922626, 3.291164679918438, 0.5645491289906204, 0.07596781197935343, 0.0744674769230187, 0.07441640400793403, 0.07346781401429325, 0.09216894896235317, 0.07418619201052934, 0.39861968299373984, 0.30932084599044174, 0.0740677050780505, 0.073433026089333, 0.07313049701042473, 0.0725328039843589, 0.07408951094839722, 0.07323496206663549, 0.07466926902998239, 0.07270800101105124, 0.0742336290422827, 0.07455009105615318, 1.4103588509606197, 0.07045438198838383, 0.06857793498784304, 0.06816256290767342, 0.06776553497184068, 0.06764916493557394, 0.06829036795534194, 0.06819309899583459, 0.06801709393039346, 0.07339155301451683, 0.07862156198825687, 0.06975168699864298, 1.9483703109435737, 0.31746900698635727, 0.3192789589520544, 0.3425760939717293, 0.23239454091526568, 2.414584164042026, 0.24746851599775255, 0.07055562699679285, 0.23529418697580695, 3.881600628956221, 2.064389727078378, 0.08523847197648138, 0.15779891796410084, 0.4282604119507596, 0.46013281703926623, 3.3409330860013142, 0.09637621603906155, 4.559252391103655, 0.07476810005027801, 0.07036827597767115, 0.06921420502476394, 0.06977331405505538, 0.0713006058940664, 0.07076375803444535, 0.15819104993715882, 0.0793857870157808, 0.40730101603548974, 2.65023025800474, 0.4408392289187759, 0.08030031400267035, 0.08114385791122913, 0.08075247495435178, 0.10893647104967386, 0.08475895901210606, 0.0797769179334864, 0.43790788599289954, 0.44502640911377966, 0.4495101450011134, 0.44458773802034557, 0.38596075295936316, 2.9938261749921367, 0.4650887689786032, 0.42335198691580445, 0.1864769330713898, 0.46511844801716506, 0.7289329940686002, 0.08141366695053875, 0.12341068091336638, 0.15183134807739407, 0.08480624202638865, 2.780747360084206, 0.20626865001395345, 0.07579951803199947, 0.1638178089633584, 0.08565259294118732, 0.0913473810069263, 0.4102826309390366, 2.4935159349115565, 3.8001923830015585, 0.13446865906007588, 0.16927540500182658, 0.1526427820790559, 2.0149686249205843, 0.2361295809969306, 0.4492806929629296, 0.07962428196333349, 0.44318135001230985, 1.5730426739901304, 0.40998273296281695, 0.4444417719496414, 4.056537986034527, 0.08604860794730484, 3.4806995750404894, 2.599598899949342, 1.5112921959953383, 0.5848805810092017, 0.4593315039528534, 2.732989937067032, 0.25936915806960315, 0.08678522205445915, 0.0854891249909997, 0.10091537202242762, 0.14879952208139002, 0.12488683708943427, 0.08495983993634582, 0.08895350701641291, 0.4211612230865285, 2.2998173129744828, 2.2604189579142258, 2.497399334097281, 0.07977873599156737, 0.08318680198863149, 3.027232708991505, 0.41940846806392074, 0.23513564094901085, 0.16076997003983706, 0.13520849798806012, 0.4352562240092084, 0.5241796089103445, 2.6613076669164, 2.2430160420481116, 0.2650066970381886, 0.2519071730785072, 0.45767789194360375, 3.2272743539651856, 1.3644798370078206, 0.22492097690701485, 0.07707408897113055, 0.07747015496715903, 0.2628821940161288, 0.07657243590801954, 3.318577020079829, 0.13061022199690342, 0.08823549293447286, 0.09624280605930835, 3.585166804958135, 0.2326499429764226, 0.0721691909711808, 0.07155396393500268, 0.07110628904774785, 0.07056866702623665, 0.07547588495071977, 0.0714593039592728, 0.07254513900261372, 0.07183685700874776, 2.437410108046606, 0.07192518096417189, 0.0722841250244528, 0.0710478340042755, 0.07143871299922466, 0.07104883901774883, 0.08325342298485339, 0.11250223789829761, 0.07402831909712404, 0.07332919607870281, 1.5950878800358623, 0.0816543570253998, 0.07885405991692096, 0.07475629402324557, 0.06862198002636433, 0.06737405702006072, 0.07706611696630716, 0.06966056802775711, 0.06878474703989923, 0.06922041706275195, 0.0901484350906685, 0.07296331995166838, 0.07958729600068182, 0.089767488068901, 0.07579054404050112, 0.07518336095381528, 0.0738299370277673, 0.0736693199723959, 0.07355900201946497, 0.07264547003433108, 0.0732635511085391, 0.07316710893064737, 0.07316039607394487, 0.07361612806562334, 0.07355018507223576, 0.076388304005377, 0.07822113507427275, 0.07322514511179179, 0.07398701610509306, 0.07300570199731737, 0.07469845307059586, 0.0740685670170933, 0.07394993701018393, 1.026842368999496, 0.141587597085163, 0.07707104505971074, 0.07563296507578343, 0.07441638107411563, 0.07613064500037581, 0.07531803799793124, 0.07493409106973559, 0.07512800605036318, 0.07479634101036936, 0.07509091997053474, 0.8814717150526121, 0.07451961899641901, 0.07615504099521786, 0.07385428203269839, 0.07385344500653446, 0.07357667305041105, 0.0751358870184049, 0.07455443602520972, 0.07470882404595613, 0.07583109603729099, 0.0749007670674473, 0.07445233594626188, 0.07463027397170663, 1.2051177700050175, 2.4355517239309847, 0.0768023889977485, 0.07570971199311316, 0.07357048790436238, 0.07375211489852518, 0.07437768601812422, 0.07444452401250601, 0.07418485905509442, 0.07596777193248272, 0.3183926099445671, 0.07992728089448065, 0.07580054504796863, 0.07599687506444752, 0.06905715900938958, 0.07007613696623594, 0.07419845799449831, 0.07227080990560353, 0.0708592040464282, 0.07032784505281597, 0.07181620504707098, 0.07327521604020149, 0.07137485698331147, 0.07025524298660457, 0.07373559405095875, 0.07042885501869023, 0.06963139597792178, 0.07042304298374802, 0.07059038698207587, 0.07023125002160668, 0.07597536302637309, 0.0791482919594273, 0.0750654760049656, 0.29878277995157987, 0.5465712329605594, 0.12363310100045055, 0.0709574039792642, 0.06988388602621853, 0.07368173298891634, 0.07364413910545409, 0.07364590500947088, 0.07379131496418267, 0.06994892202783376, 0.07078003801871091, 0.07005748094525188, 0.07192587899044156, 0.07100432692095637, 1.567748832050711, 0.8700333489105105, 0.07563688897062093, 0.0740443019894883, 0.07456413307227194, 0.07451072300318629, 0.07405666902195662, 0.07420692103914917, 0.07392498606350273, 0.07443059294018894, 0.07397692708764225, 0.0721939200302586, 0.0710707560647279, 2.4702081049326807, 0.5858036589343101, 0.07809307402931154, 0.07509686797857285, 0.07565925596281886, 0.07521823490969837, 0.07594747503753752, 0.07478547899518162, 0.0747062599984929, 0.0755565509898588, 0.0752038030186668, 0.07481998694129288, 2.068312476039864, 1.7470445659710094, 0.06879084696993232, 0.06799268000759184, 0.06758477003313601, 0.06699467392172664, 0.06826093804556876, 0.0712685149628669, 0.07303125702310354, 0.07228487392421812, 0.07197060703765601, 0.07207076298072934, 1.349237116985023, 0.07799394801259041, 0.07539473997894675, 0.074214993044734, 0.07399695506319404, 0.19787237502168864, 0.07383550005033612, 0.07458865805529058, 0.07303307799156755, 1.8685675889719278, 0.7360555230407044, 0.16186639096122235, 0.08720535202883184, 0.07712222100235522, 0.07526814797893167, 0.07559245894663036, 0.07532568695023656, 0.07532823598012328, 0.07531460793688893, 0.07648374105338007, 0.07641059695743024, 2.194407671922818, 1.2162537761032581, 0.07446306897327304, 0.07338585704565048, 0.07299435406457633, 0.07313150598201901, 0.0745147910201922, 0.07297153805848211, 0.07317016809247434, 0.07359308993909508, 0.0739505480742082, 1.3898088110145181, 0.1634470330318436, 0.0746288769878447, 0.07173322897870094, 0.08277171803638339, 0.07711186306551099, 0.07958456804044545, 1.523146889056079, 0.06975837703794241, 0.07016807608306408, 0.08533724898006767, 0.08051865897141397, 0.08070010400842875]
[0.004758795652044367, 0.0018194720804767341, 0.0031712333676500283, 0.026471227694930016, 0.0013946786532368586, 0.0014348964287652349, 0.0014466273275261022, 0.001434717652844075, 0.0014335027758070097, 0.0013720618979055055, 0.0013855483064580025, 0.0013630609384414796, 0.0013991797542465584, 0.0013768753459753127, 0.0013631414693343093, 0.001369850448693852, 0.001367747101799718, 0.0013757050013625805, 0.0014346992235858829, 0.002313964836280413, 0.07814582742807664, 0.008733801529457678, 0.0017184659800663286, 0.007683564611824648, 0.005363182979635894, 0.0751642659591625, 0.005740872510632843, 0.002214413592401816, 0.005155273653775909, 0.008790760878378488, 0.06655062024472091, 0.021678267816575815, 0.0022536311227334092, 0.0016575901417479832, 0.008490683306578775, 0.028704414284331913, 0.0049673634894876455, 0.00196618147013823, 0.004396811612330529, 0.07391891828548088, 0.0019510347367625456, 0.05548551918378062, 0.004242267345591467, 0.0013877599386080187, 0.001363728000611371, 0.0014893992029472577, 0.00162686536810836, 0.001478343346745384, 0.0014429934066245143, 0.0014806878776764687, 0.009423341020010412, 0.008419707429367212, 0.0019138186305229152, 0.0031079456319424268, 0.00880421734681087, 0.06682419759097832, 0.0022775065510210637, 0.005651417898717432, 0.07764192259328782, 0.005122428835958851, 0.009165817857434859, 0.0036146013275245013, 0.0016253046734182506, 0.014830974550746685, 0.005338560692414793, 0.004965492816432855, 0.002264630346444949, 0.0013794292249165627, 0.0014334058374812712, 0.01630238373526277, 0.08386345679529619, 0.04343717510346323, 0.005422683019305066, 0.001608141735956377, 0.0017143870010135733, 0.0015102056525077444, 0.0014737473886308012, 0.0014882750609623535, 0.004987854510545731, 0.0075563004273655155, 0.07071426004047829, 0.014459485573009873, 0.007595533469444787, 0.004998679346480996, 0.08385832130680887, 0.009113969530777208, 0.0023252878775249937, 0.0024714454704401444, 0.0018045158976004744, 0.0017391841226656521, 0.0015056520410604319, 0.00788710389656908, 0.026071652266367967, 0.006604639428420638, 0.0017434215097098934, 0.0016358269601870253, 0.08183472926671408, 0.03839287081761856, 0.0015815203481981037, 0.005866193286219269, 0.0023883315306917137, 0.0017571391428497676, 0.009267910592714134, 0.0021701188358877388, 0.06377524459183369, 0.0021287976735632637, 0.001916608611615945, 0.004102882202144484, 0.0018883252454617498, 0.0015616590413740094, 0.002123143571447961, 0.0019680196110501277, 0.0015479099190774926, 0.003611484673159311, 0.0020150959382441882, 0.0017723594697154298, 0.0016378468966909818, 0.0015597866933640776, 0.0015279347343104227, 0.0015372574277108116, 0.0017660963467835467, 0.005161799754168154, 0.005812566896083252, 0.037833876183199486, 0.00931189991818855, 0.0077433443072309, 0.0019034482871315309, 0.009678941652444857, 0.06926748981670838, 0.0014928362869220425, 0.00149657304056597, 0.0015336723893652766, 0.002263855469432108, 0.001577567675017885, 0.0016332101029324897, 0.001716366695829344, 0.06782560689583877, 0.0030140237958759678, 0.05511571881741437, 0.014571260774926263, 0.0015866305508023622, 0.0017259271829674135, 0.0015478490409916456, 0.0015051238766244175, 0.0015065448586733974, 0.0015234702851205152, 0.0014936821226372706, 0.06716662612078446, 0.011521410795726947, 0.0015503635097827231, 0.0015197444270003814, 0.0015187021226108987, 0.001499343143148842, 0.0018809989584153708, 0.0015140039185822314, 0.008135095571300812, 0.006312670326335546, 0.0015115858179193978, 0.0014986331854965917, 0.0014924591226617293, 0.001480261305803243, 0.001512030835681576, 0.0014945910625843977, 0.0015238626332649467, 0.0014838367553275762, 0.0015149720212710755, 0.0015214304297174119, 0.028782833693073873, 0.0014378445303751802, 0.0013995496936294498, 0.0013910727124014984, 0.0013829701014661364, 0.001380595202766815, 0.0013936809786804477, 0.0013916958978741753, 0.0013881039577631317, 0.0014977867962146293, 0.001604521673229732, 0.0014235038162988363, 0.03976265940701171, 0.00647895932625219, 0.006515897121470497, 0.006991348856565904, 0.004742745732964605, 0.04927722783759236, 0.005050377877505154, 0.0014399107550365888, 0.004801922183179734, 0.07921633936645349, 0.04213040259343629, 0.0017395606525812525, 0.003220386080900017, 0.00874000840715836, 0.009390465653862576, 0.06818230787757784, 0.0019668615518175824, 0.09304596716538072, 0.0015258795928628165, 0.0014360872648504315, 0.001412534796423754, 0.0014239451847970486, 0.001455114406001355, 0.0014441583272335784, 0.003228388774227731, 0.0016201181023628736, 0.008312265633377341, 0.0540863317960151, 0.008996718957526038, 0.0016387819184218437, 0.001655997100229166, 0.0016480096929459547, 0.002223193286728038, 0.0017297746737164501, 0.0016281003659895184, 0.008936895632508154, 0.009082171614566932, 0.009173676428594152, 0.009073219143272358, 0.007876750060395166, 0.061098493367186466, 0.009491607530175574, 0.00863983646766948, 0.0038056516953344855, 0.009492213224840104, 0.014876183552420413, 0.0016615034071538522, 0.002518585324762579, 0.003098598940354981, 0.001730739633191605, 0.056749946124167464, 0.00420956428599905, 0.0015469289394285605, 0.003343220591088947, 0.0017480121008405577, 0.0018642322654474754, 0.008373114917123196, 0.05088808030431748, 0.07755494659186854, 0.0027442583481648137, 0.0034546001020780938, 0.0031151588179399166, 0.041121808671848656, 0.004818971040753686, 0.009168993733937338, 0.0016249853461904793, 0.009044517347189997, 0.032102911714084297, 0.00836699455026157, 0.009070240243870233, 0.08278648951090872, 0.0017560940397409151, 0.07103468520490795, 0.05305303877447637, 0.030842697877455885, 0.011936338387942893, 0.009374112325568437, 0.05577530483810269, 0.0052932481238694516, 0.001771126980703248, 0.0017446760202244837, 0.0020594973882128087, 0.003036724940436531, 0.0025487109610088624, 0.0017338742844152208, 0.0018153776942125084, 0.008595127001765887, 0.04693504720356087, 0.046130999141106646, 0.0509673333489241, 0.0016281374692156607, 0.0016976898365026834, 0.06178025936717357, 0.008559356491100423, 0.004798686549979813, 0.0032810197967313684, 0.0027593571017971454, 0.00888278008182058, 0.010697543038986623, 0.05431240136564082, 0.045775837592818606, 0.005408299939554869, 0.005140962715887902, 0.009340365141706199, 0.06586274191765684, 0.02784652728587389, 0.004590224018510507, 0.0015729405912475623, 0.0015810235707583477, 0.0053649427350230365, 0.001562702773633052, 0.06772606163428221, 0.002665514734630682, 0.001800724345601487, 0.001964138899169558, 0.07316666948894153, 0.004747958019926992, 0.0014728406320649143, 0.001460284978265361, 0.0014511487560764868, 0.0014401768780864623, 0.0015403241826677505, 0.0014583531420259755, 0.0014805130408696678, 0.0014660583063009747, 0.049743063429522574, 0.0014678608360035078, 0.0014751862249888328, 0.0014499557960056225, 0.0014579329183515236, 0.00144997630648467, 0.0016990494486704773, 0.0022959640387407677, 0.0015107820223902865, 0.0014965142056878125, 0.032552813878282905, 0.0016664154494979552, 0.0016092665289167542, 0.001525638653535624, 0.001400448571966619, 0.0013749807555114432, 0.0015727778972715748, 0.0014216442454644308, 0.0014037703477530455, 0.0014126615727092235, 0.0018397639814422143, 0.0014890473459524159, 0.0016242305306261595, 0.0018319895524265511, 0.0015467457967449207, 0.0015343543051799036, 0.001506733408729945, 0.0015034555096407325, 0.0015012041228462238, 0.0014825606129455324, 0.0014951745124191654, 0.0014932063047070892, 0.001493069307631528, 0.0015023699605229254, 0.0015010241851476686, 0.0015589449797015712, 0.0015963496953933214, 0.0014943907165671795, 0.0015099391041855728, 0.0014899122856595383, 0.0015244582259305278, 0.001511603408512108, 0.0015091823879629374, 0.02095596671427543, 0.0028895427976563877, 0.0015728784706063416, 0.0015435298995057844, 0.0015187016545737885, 0.0015536866326607307, 0.001537102816284311, 0.0015292671646884813, 0.0015332246132727181, 0.0015264559389871297, 0.0015324677545007091, 0.017989218674543103, 0.0015208085509473268, 0.001554184510106487, 0.0015072302455652732, 0.0015072131633986623, 0.0015015647561308376, 0.001533385449355202, 0.0015215191025553004, 0.0015246698784889007, 0.0015475733885161427, 0.0015285870830091287, 0.0015194354274747322, 0.001523066815749115, 0.02459424020418403, 0.04970513722308132, 0.0015673956938316018, 0.0015450961631247584, 0.0015014385286604567, 0.0015051452020107179, 0.0015179119595535556, 0.0015192760002552246, 0.0015139767154100903, 0.0015503626924996473, 0.006497808366215655, 0.0016311689978465438, 0.0015469498989381352, 0.0015509566339683167, 0.0014093297757018283, 0.0014301252442088966, 0.0015142542447856798, 0.0014749144878694598, 0.001446106205029147, 0.0014352621439350198, 0.001465636837695326, 0.00149541257224901, 0.0014566297343532955, 0.001433780469114379, 0.001504808041856301, 0.0014373235718100046, 0.0014210488975086079, 0.0014372049588520005, 0.0014406201424913443, 0.0014332908167674833, 0.0015505176127831243, 0.0016152712644781082, 0.0015319484898972573, 0.006097607754113875, 0.011154514958378764, 0.0025231245102132764, 0.0014481102852911061, 0.001426201755637113, 0.0015037088365084967, 0.0015029416143970222, 0.0015029776532545077, 0.0015059452033506669, 0.0014275290209761992, 0.0014444905718104268, 0.001429744509086773, 0.0014678750814375828, 0.0014490678963460484, 0.031994874123483896, 0.017755782630826746, 0.0015436099789922638, 0.0015111082038671082, 0.0015217170014749377, 0.0015206270000650262, 0.001511360592284829, 0.001514426959982636, 0.0015086731849694435, 0.0015189916926569172, 0.00150973320587025, 0.0014733453067399714, 0.0014504235931577123, 0.050412410304748585, 0.011955176712945104, 0.0015937362046798272, 0.001532589142419854, 0.0015440664482207931, 0.0015350660185652729, 0.0015499484701538269, 0.0015262342652077882, 0.0015246175509896509, 0.0015419704283644654, 0.0015347714901768736, 0.0015269385090059772, 0.0422104586946911, 0.03565397073410223, 0.0014038948361210677, 0.00138760571444065, 0.0013792810210844083, 0.0013672382433005438, 0.0013930803682769136, 0.0014544594890381001, 0.0014904338167980313, 0.0014752015086575126, 0.0014687878987276737, 0.001470831897565905, 0.027535451367041285, 0.001591713224746743, 0.001538668162835648, 0.00151459169479049, 0.0015101419400651843, 0.004038211735136503, 0.001506846939802778, 0.0015222175113324607, 0.0014904709794197459, 0.038134032427998524, 0.015021541286544988, 0.003303395733902497, 0.0017797010618128947, 0.001573922877599086, 0.0015360846526312586, 0.001542703243808783, 0.0015372589173517665, 0.0015373109383698628, 0.0015370328150385497, 0.001560892674558777, 0.0015593999379067396, 0.04478383003924118, 0.02482150563476037, 0.0015196544688423068, 0.0014976705519520507, 0.0014896806951954352, 0.0014924797139187552, 0.0015207100208202492, 0.0014892150624180023, 0.0014932687365811091, 0.0015018997946754098, 0.0015091948586573101, 0.02836344512274527, 0.003335653735343747, 0.0015230383058743818, 0.001463943448544917, 0.0016892187354363957, 0.0015737114911328774, 0.0016241748579682745, 0.031084630388899574, 0.0014236403477131104, 0.0014320015527155934, 0.0017415765097972993, 0.0016432379381921217, 0.0016469408981311991]
[210.13720132538208, 549.6099724366104, 315.33472440125956, 37.77686518829378, 717.0110460062872, 696.9144113492049, 691.263037115519, 697.001251791721, 697.5919522981297, 728.8300925246381, 721.7359332323728, 733.6429148526531, 714.7044523514336, 726.2821597634518, 733.5995731157313, 730.0066959524646, 731.1293138067508, 726.9000250849857, 697.0102050383794, 432.15868466154046, 12.796588543647754, 114.49767854549525, 581.9143419769081, 130.14792619314304, 186.45643898353248, 13.304194316795565, 174.18955013334124, 451.5868234512468, 193.97612370539508, 113.75579586740578, 15.026155974546674, 46.12914687009134, 443.7283413033047, 603.2854412041008, 117.77615109318437, 34.83784724169907, 201.31403754049498, 508.60005304072786, 227.4375361445041, 13.528336496185167, 512.5485370185426, 18.022720427068066, 235.72300341683854, 720.585723927902, 733.2840563159893, 671.4116658725054, 614.6790137666716, 676.4328477559217, 693.003859483478, 675.3617795326481, 106.11947481010245, 118.76897248378089, 522.5155529637459, 321.7559502078589, 113.58193018283762, 14.964639098562197, 439.07667336968785, 176.94674467923286, 12.879639846611044, 195.21989119304442, 109.1010115577247, 276.65568326586674, 615.2692577305247, 67.42645242754149, 187.3164056036365, 201.3898795081506, 441.57316957701966, 724.9375190383448, 697.6391290251502, 61.34072269670332, 11.924144773103233, 23.021754928079343, 184.4105577331262, 621.8357360181878, 583.298869746903, 662.1614733989827, 678.5423388801111, 671.9188046820973, 200.48700255505008, 132.3398943189779, 14.14141927565359, 69.15875360508008, 131.6563219716939, 200.05284009745228, 11.924875008424538, 109.7216746910414, 430.054278296237, 404.6215107557716, 554.1652480478192, 574.982249991621, 664.1640782392851, 126.78925155721655, 38.3558352874315, 151.408719709492, 573.5847552818141, 611.31160222819, 12.219750819249617, 26.04650235066814, 632.3029616023242, 170.46830051597135, 418.7023397502849, 569.1068940494818, 107.89918504243437, 460.8042580262321, 15.68006530433673, 469.74872831675043, 521.7549341786964, 243.73110187694945, 529.5697880454229, 640.3446421442676, 470.99970696659517, 508.1250178530507, 646.0324258377838, 276.8944327611404, 496.25428795778777, 564.2196276134435, 610.5576791215018, 641.1133036679811, 654.4782166047906, 650.5091352781023, 566.22052461703, 193.73087830315382, 172.04103073185127, 26.43133881280872, 107.38947033212216, 129.1431660950655, 525.3623157301455, 103.31708113432056, 14.436787050406217, 669.8658176790561, 668.1932474353692, 652.0297339471949, 441.7243121314858, 633.8872276833785, 612.2910935981002, 582.6260800969472, 14.743694096769696, 331.7823838578452, 18.143644344234513, 68.62824126521477, 630.2664470278213, 579.398719638151, 646.0578347868727, 664.3971406810231, 663.7704773560872, 656.3961304443127, 669.486489022432, 14.888346456493426, 86.79492622300035, 645.0100209983308, 658.0053739520961, 658.4569713255129, 666.958730941239, 531.6324049655191, 660.500271978448, 122.92418586056004, 158.41156726150342, 661.5568816175033, 667.2746938195132, 670.0351016760499, 675.5564008054403, 661.3621735757998, 669.0793388465959, 656.2271284633139, 673.9285817052274, 660.0781967979783, 657.2761925011179, 34.74293082687804, 695.4854846087342, 714.5155363556272, 718.8696831480762, 723.0814310011938, 724.3252750668161, 717.5243224936678, 718.5477815430129, 720.4071383900208, 667.6517662776234, 623.2386989121225, 702.4919698494647, 25.14922328921644, 154.34577524635563, 153.47080860207345, 143.0339152738535, 210.84832632908572, 20.293349359988248, 198.00498581583204, 694.487485771012, 208.24993863974294, 12.623658300770709, 23.735828248548355, 574.8577944184624, 310.521774370769, 114.41636591344313, 106.49099169950858, 14.66656132842426, 508.4241944105812, 10.747376060077825, 655.3597051021735, 696.3365141352674, 707.9471617490722, 702.2742242304275, 687.231186685859, 692.4448525776216, 309.7520372958216, 617.2389522353601, 120.30414379258616, 18.488959535497223, 111.15163258083865, 610.2093199582075, 603.8657917103928, 606.7925475683438, 449.80344532784187, 578.1099788283309, 614.2127481141036, 111.89567844593351, 110.10582517468575, 109.00755087491657, 110.21446569396304, 126.95591358523204, 16.367015696937948, 105.35623147299498, 115.74293144806957, 262.76708434088795, 105.34950873028298, 67.22154217015533, 601.8645497170514, 397.04829142299064, 322.7265029289135, 577.7876584220412, 17.621162103167904, 237.5542768941635, 646.4421050713568, 299.11277845841516, 572.0784195482029, 536.413846350829, 119.42986688920021, 19.65096726030668, 12.894084051949404, 364.3971788110754, 289.46910509220913, 321.0109206121662, 24.31799651566844, 207.51317896353248, 109.06322209586473, 615.3901648063113, 110.56421936220686, 31.149822449322464, 119.51722855714515, 110.25066294972846, 12.079265661678173, 569.4455862668577, 14.077629782061845, 18.849061676766695, 32.422585208764716, 83.77778574123852, 106.67676738547733, 17.929081748681966, 188.91991771377297, 564.612255866001, 573.1723187616989, 485.5553620622847, 329.30213292753785, 392.3552004516697, 576.7430828107975, 550.8495577465985, 116.3449940640258, 21.306040146565184, 21.677397381773048, 19.62041045298497, 614.1987509701746, 589.0357463999693, 16.186400158289747, 116.83121284173038, 208.39035631618964, 304.78328749988776, 362.4032566675436, 112.57736775973902, 93.47940890310547, 18.41200121621986, 21.845586068683577, 184.90098758877363, 194.51609654929948, 107.06219562389941, 15.183090938579866, 35.91112061241778, 217.85429119960304, 635.7519194077507, 632.5016391250535, 186.3952793888873, 639.9169546971165, 14.765364704062616, 375.16206044854374, 555.3320820272328, 509.1289625305023, 13.667425440912558, 210.6168579846409, 678.9600844987591, 684.79784075289, 689.1092286801314, 694.3591549176115, 649.2139844666069, 685.7049717126666, 675.441534383642, 682.1011113283137, 20.10330548734356, 681.2634927454459, 677.8805164124753, 689.6761975467301, 685.9026141824784, 689.6664418085597, 588.5643886247747, 435.5468914698049, 661.9088559300224, 668.2195171948871, 30.71931058676111, 600.0904518145654, 621.4011054297699, 655.463204004781, 714.0569243436923, 727.2829063182314, 635.8176839430291, 703.4108590741803, 712.3672341424342, 707.8836285481915, 543.5479822885147, 671.5703182428936, 615.6761501179814, 545.8546412971929, 646.518647152279, 651.739951212083, 663.6874142473017, 665.1344144124099, 666.1319302161517, 674.5086786119408, 668.8182494376647, 669.6998243629586, 669.7612728951684, 665.6150124646615, 666.2117838571811, 641.4594568895112, 626.4291607821004, 669.169039203574, 662.2783642254087, 671.1804511077851, 655.9707461905695, 661.5491830521298, 662.6104359392764, 47.71910614454208, 346.07551091164555, 635.7770283514034, 647.8656489389583, 658.457174250358, 643.6304329190712, 650.5745675603748, 653.9079783379149, 652.220158314226, 655.1122600129183, 652.5422783370789, 55.588851194250005, 657.5449614463898, 643.4242482132856, 663.468639209107, 663.4761587041003, 665.9719442115527, 652.1517472469209, 657.2378870042181, 655.8796852411746, 646.1729100671785, 654.1989076810929, 658.139189015737, 656.570013645891, 40.659926539624415, 20.11864478940891, 638.000987201537, 647.2089076822433, 666.0279331529964, 664.3877272864463, 658.7997371692871, 658.2082517146384, 660.5121398641395, 645.0103610192667, 153.89804433127708, 613.0572621967386, 646.4333464751669, 644.7633532095444, 709.5571364778785, 699.2394575575587, 660.3910825698462, 678.0054086013608, 691.5121424154628, 696.7368325191987, 682.2972610134941, 668.711777978475, 686.516261762275, 697.4568433183376, 664.5365868502537, 695.7375636306526, 703.705552816097, 695.7949830612694, 694.1455075524938, 697.6951141397187, 644.9459146775091, 619.0910604251346, 652.7634620842028, 163.9987418550053, 89.64979685188781, 396.3339882562796, 690.5551394512574, 701.1630690030107, 665.0223605268742, 665.3618413521662, 665.3458871025971, 664.0347854457391, 700.5111527022839, 692.285584631174, 699.4256621686446, 681.2568812195086, 690.0987886913978, 31.255006540751186, 56.31968022991268, 647.8320389278928, 661.7659790615122, 657.1524133795845, 657.6234671337792, 661.6554679967078, 660.3157672334793, 662.8340782899604, 658.3314476531915, 662.3686861438366, 678.7275158276855, 689.4537600721892, 19.836385405000268, 83.64577320862156, 627.4564115840578, 652.4905940681975, 647.6405216577865, 651.4377804640842, 645.1827394627859, 655.2074100261768, 655.9021961611853, 648.5208675893209, 651.5627938102732, 654.9052198906102, 23.690811019918435, 28.047366938670933, 712.3040659961249, 720.6658127687983, 725.015413620197, 731.4014253916549, 717.8336747627055, 687.5406345358889, 670.945592303016, 677.8734933033228, 680.8334960182082, 679.8873492306703, 36.31681887724403, 628.2538741607237, 649.9127129251029, 660.2439478834775, 662.1894097960326, 247.63436530556197, 663.637409736442, 656.9363396198603, 670.9288632974989, 26.223295474668618, 66.57106490768125, 302.71880227278757, 561.8921185456555, 635.3551461971459, 651.0057881816836, 648.2128069758239, 650.5085049190662, 650.4864923815492, 650.6041967457405, 640.6590384458518, 641.2723097464976, 22.329488101481374, 40.28764470272853, 658.0443255379056, 667.703587211893, 671.2847949397689, 670.0258574197519, 657.5875652220759, 671.4946855132692, 669.6718249720643, 665.8233815233457, 662.6049606938583, 35.256647973206825, 299.79130909310277, 656.582304032003, 683.0864955842027, 591.9896452851373, 635.4404893365327, 615.6972539588058, 32.1702393590983, 702.4245987452994, 698.3232651554308, 574.1924023288469, 608.5545962383223, 607.1863302045085]
Elapsed: 0.4705043514764402~0.899317178292765
Time per graph: 0.009602129621968167~0.018353411801893163
Speed: 454.9292457862124~263.2195168737209
Total Time: 0.0821
best val loss: 0.6933901309967041 test_score: 0.4898

Testing...
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6945 score: 0.4898 time: 0.09s
test Score 0.4898
Epoch Time List: [9.791504344088025, 0.7141125989146531, 0.5960637530079111, 1.770690695848316, 0.36285667412448674, 0.36110068403650075, 0.3778658480150625, 0.369993286090903, 0.37029301293659955, 0.36427700403146446, 0.3588374999817461, 0.3564380049938336, 0.35870629001874477, 0.35387867491226643, 0.3528189380886033, 0.3520884660538286, 0.3523020161082968, 0.3529084980254993, 0.36624529503751546, 0.4400629800511524, 4.28879822883755, 6.340483248815872, 0.5935611708555371, 0.7618988329777494, 1.8011790658347309, 8.479673536960036, 5.186743880040012, 0.6507161649642512, 1.4511513229226694, 1.7368644509697333, 4.676833693985827, 11.926085546961986, 2.819896937930025, 0.42066908918786794, 1.049370611901395, 9.321313038119115, 0.8049108500126749, 0.5842913101660088, 0.5562616579700261, 5.816162660950795, 7.71989314595703, 3.1768658970249817, 8.459914216888137, 1.28778335894458, 0.3679475039243698, 0.386679936083965, 0.4120546339545399, 0.3858008049428463, 0.39363479404710233, 0.38686743297148496, 0.9645626300480217, 11.651263974956237, 0.6703446110477671, 0.4992066101403907, 1.8484740209532902, 9.002799735055305, 4.278910040855408, 1.620391158037819, 11.276761388056912, 8.0684481728822, 1.346844429965131, 1.1786989690735936, 0.6350002819672227, 9.219177598133683, 1.3948310260893777, 1.3316548248985782, 1.458006887929514, 0.44509050308261067, 0.386017058044672, 1.2720112439710647, 7.069779208977707, 15.108253574930131, 3.6973957548616454, 0.5094977499684319, 0.4068975360132754, 0.4206846609013155, 0.42252324195578694, 0.4114400140242651, 0.7222872320562601, 1.4199529628967866, 19.02788251102902, 5.233059666934423, 0.8966803771909326, 0.6881888518109918, 5.646182882948779, 7.638233274105005, 0.7137740418547764, 0.5235790819860995, 0.5280283830361441, 0.49849183904007077, 0.44386296800803393, 1.1403752469923347, 9.178591525065713, 4.283896359964274, 0.6407826719805598, 0.4268434800906107, 7.250634026830085, 15.481539977015927, 0.43215504416730255, 0.7432681419886649, 0.6686337129212916, 0.4350144111085683, 0.9550108109833673, 0.646171759115532, 13.304370902944356, 0.7961710868403316, 0.7389946748735383, 0.5510749380337074, 0.4971121409907937, 0.4112602900713682, 0.4887806080514565, 0.4469495629891753, 0.4529327671043575, 0.5043962049530819, 0.8899830270092934, 0.5221824169857427, 0.6625034969765693, 0.553747285855934, 0.3987812161212787, 0.5001288829371333, 0.7129855368984863, 0.664292388013564, 1.082372328150086, 8.733015589998104, 8.274205449037254, 1.5913300738902763, 1.272691491059959, 1.615473901038058, 13.398587472969666, 1.7830365769332275, 0.4808436209568754, 0.4309548099990934, 0.4437183398986235, 0.3990358761511743, 0.42058171005919576, 0.5170186260947958, 6.345429116045125, 7.86216136906296, 3.0359080989146605, 8.285366571159102, 1.0117577619384974, 0.4285808259155601, 0.4202912870096043, 0.3988366359844804, 0.3936973309610039, 0.39055500202812254, 0.3870843310141936, 4.3257229728624225, 5.127948538982309, 0.6142592080868781, 0.393052269006148, 0.3943920099409297, 0.39318897610064596, 0.4536664988845587, 2.6110088831046596, 0.7166562899947166, 6.224221398937516, 1.7741172250825912, 0.3907422620104626, 0.3924009809270501, 0.39398659102153033, 0.3969373081345111, 0.39382496604230255, 0.38684334594290704, 0.38413563510403037, 0.39043578098062426, 0.39346795505844057, 7.921178663033061, 6.1988871219800785, 0.3711966589326039, 0.3616134349722415, 0.35893993207719177, 0.362536404049024, 0.36805543303489685, 0.3663916072109714, 0.364633179968223, 0.37490066804457456, 0.40869415807537735, 0.465380122885108, 13.81432059709914, 2.8583990728948265, 1.1105411569587886, 1.1765951809938997, 1.7177718030288815, 3.7815142510225996, 8.920126826036721, 0.47030011797323823, 0.8687703389441594, 5.258275065221824, 9.342972929007374, 2.1513166889781132, 0.4782160061877221, 1.3479941400000826, 1.8501070219790563, 10.108196928864345, 7.053806724958122, 7.8465315400389954, 3.0602685931371525, 0.38097329693846405, 0.3754576080245897, 0.4732358528999612, 0.3748277850681916, 0.3848708029836416, 0.4723614789545536, 0.4475824430119246, 0.9837264540838078, 7.051806709030643, 8.750421261880547, 0.557300150860101, 0.4638173161074519, 0.7332857140572742, 0.46039258199743927, 0.4308425378985703, 0.5279091581469402, 1.8044309570686892, 1.4529706500470638, 11.172454023151658, 2.014866207027808, 1.5054404969559982, 6.580290523939766, 11.54351308196783, 1.1564229660434648, 0.726065898896195, 1.8116492320550606, 1.5529105662135407, 12.705336259910837, 0.49341841496061534, 1.1605896919500083, 0.4904380710795522, 3.1172322179190814, 7.5510274599073455, 0.503226934117265, 0.6079601360252127, 0.4530108099570498, 0.5709767340449616, 1.1174318100092933, 3.88515841611661, 9.151130654034205, 4.534630280104466, 1.4186587170697749, 1.1900096820900217, 7.3710298899095505, 4.724464745959267, 1.6148364819819108, 1.3882208779687062, 1.418510522111319, 13.538397640222684, 1.5666188290342689, 1.6052035021129996, 8.458555100020021, 8.562084065866657, 6.132049727952108, 8.302883691969328, 6.502457805909216, 1.9608553709695116, 1.6742523559369147, 12.883484293008223, 7.625683302991092, 1.1724543699529022, 0.46609585802070796, 0.5434855420608073, 0.5137292231665924, 0.5371060039615259, 0.4291786761023104, 0.8520416209939867, 0.9246754738269374, 6.640371938003227, 11.19391930301208, 16.46409208397381, 0.430555145139806, 0.4494451809441671, 5.9546980179147795, 7.806204485008493, 0.7715124451788142, 0.7930648879846558, 0.5678650920744985, 0.9860094911418855, 1.2518637790344656, 12.353645834024064, 8.487111538997851, 1.4747344958595932, 1.6227903899271041, 1.7855017790570855, 12.798100810032338, 6.309093381976709, 2.96766234585084, 0.5154688709881157, 0.6154860638780519, 1.0759720019996166, 0.7862536769825965, 7.291356415953487, 5.341052417992614, 0.6691781372064725, 0.5053598930826411, 7.053253382095136, 3.5165523999603465, 0.3925429950468242, 0.38678354397416115, 0.38047290698159486, 0.38168894802220166, 0.381035408237949, 0.38448244286701083, 0.3804106229217723, 0.3782966759754345, 5.474929261952639, 0.4263767891097814, 0.3849964201217517, 0.3851935957791284, 0.3827999149216339, 0.3792867539450526, 0.38604073389433324, 2.2303922929568216, 0.4346663700416684, 0.386396485962905, 11.98328515002504, 3.533120305859484, 0.381004061200656, 0.382046829094179, 0.3792653901036829, 0.36369408876635134, 0.38049119093921036, 0.3829227959504351, 0.37596748501528054, 0.3720406040083617, 4.004798694979399, 0.39919261494651437, 0.3968466498190537, 0.432285274961032, 0.4033426941605285, 0.4180736531270668, 0.39461519895121455, 0.3929205930326134, 0.40343510394450277, 0.3895963348913938, 0.39770579477772117, 0.3958736459026113, 0.3913914009463042, 0.3962495488813147, 0.39278865593951195, 0.4013830909971148, 0.42020898195914924, 0.3966312779812142, 0.39930949290283024, 0.3942914019571617, 0.4136900659650564, 0.3949370358604938, 0.39669945009518415, 6.59437210788019, 0.7487019500695169, 0.4980464648688212, 0.4061325431102887, 0.40082040696870536, 0.40997563605196774, 0.40469424601178616, 0.40672096505295485, 0.40413439203985035, 0.40063640393782407, 0.40312510600779206, 2.688577842898667, 5.763549114111811, 0.39854798605665565, 0.391276236041449, 0.3907427790109068, 0.3951019700616598, 0.3962687939638272, 0.4015082159312442, 0.40015375695656985, 0.3984054910251871, 0.39726760517805815, 0.3963578548282385, 0.4029371850192547, 1.5290819120127708, 7.25278913800139, 5.2721232859184965, 0.41990978410467505, 0.4033970650052652, 0.3953868799144402, 0.3952255879994482, 0.40178813401144, 0.39497212099377066, 0.40572988719213754, 0.6416457251179963, 5.671802565921098, 0.38297279307153076, 0.39759405609220266, 0.3729140170617029, 0.3761252610711381, 0.3850541359279305, 0.38534634397365153, 0.38550438010133803, 0.38128629699349403, 0.3951148260384798, 0.4024144969880581, 0.3911515501094982, 0.37754721590317786, 0.39093534008134156, 0.38231315603479743, 0.37717340397648513, 0.38039161113556474, 0.3865030419547111, 0.38070614088792354, 0.3911111420020461, 0.3961901569273323, 0.40043812908697873, 0.6196145841386169, 2.7685052130836993, 5.1690066220471635, 0.3915521439630538, 0.375646970118396, 0.3833115821471438, 0.39029301702976227, 0.39134674391243607, 0.3967230130219832, 0.3842235410120338, 0.3741839840076864, 0.3779927780851722, 0.3791639490518719, 0.3841461519477889, 2.995639121043496, 8.34860189189203, 1.8303169769933447, 0.4033381320768967, 0.3998553629498929, 0.4001407269388437, 0.40173630113713443, 0.3998226139228791, 0.39606392208952457, 0.4026541080093011, 0.4112836408894509, 0.3805480970768258, 0.3831971469335258, 3.0551746719283983, 9.311158633907326, 0.8360018010716885, 0.4075958019820973, 0.407758206827566, 0.4013279710197821, 0.40715754497796297, 0.4077380870003253, 0.38072867796290666, 0.40998608304653317, 0.40707787906285375, 0.40304192702751607, 4.099795997026376, 7.626928568002768, 2.071900616050698, 0.37258218601346016, 0.36110349104274064, 0.3598232758231461, 0.3737938730046153, 0.36756118503399193, 0.37980977399274707, 0.3872302200179547, 0.38618721801321954, 0.38802508695516735, 8.059806192060933, 1.6600154740735888, 0.4073899150826037, 0.40378460800275207, 0.4040606978815049, 0.6615600368240848, 0.5356555231846869, 0.3946660141227767, 0.3889402230270207, 4.919183832011186, 3.262384469155222, 1.6407603099942207, 0.872092310921289, 0.41501454694662243, 0.40495114389341325, 0.4037927580066025, 0.404274033033289, 0.40453545504715294, 0.40672554704360664, 0.40498330199625343, 0.415166451013647, 4.480373556143604, 9.012863005045801, 1.2722134329378605, 0.3936072359792888, 0.3878300270298496, 0.3883174420334399, 0.3891726810252294, 0.3908976790262386, 0.3937761790584773, 0.3917856100015342, 0.39725160994566977, 2.1058327050413936, 3.6401148450095206, 0.6883702820632607, 0.39371923997532576, 0.4101715498836711, 0.4124808310298249, 0.42731552303303033, 4.693959604832344, 2.367622882942669, 0.38466769794467837, 0.37411902588792145, 0.3764208358479664, 0.37157524400390685]
Total Epoch List: [497, 3]
Total Time List: [0.0717103669885546, 0.08205351897049695]
========================k_idx:2========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: AEtransGAT
AEtransGAT(
  (gae): GAE(
    (encoder): transGAT(
      (encoder): EdgeFeatureGAT()
      (classifier): Sequential(
        (0): Linear(in_features=64, out_features=64, bias=True)
        (1): ReLU(inplace=True)
        (2): Linear(in_features=64, out_features=2, bias=True)
      )
    )
    (decoder): InnerProductDecoder()
  )
)
Total number of parameters: 1914242
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7d35e8349000>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 2.7142;  Loss pred: 2.7142; Loss self: 0.0000; time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6935 score: 0.5102 time: 0.09s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6950 score: 0.5000 time: 0.08s
Epoch 2/1000, LR 0.000000
Train loss: 2.7096;  Loss pred: 2.7096; Loss self: 0.0000; time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6935 score: 0.5102 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6950 score: 0.5000 time: 0.08s
Epoch 3/1000, LR 0.000030
Train loss: 2.6790;  Loss pred: 2.6790; Loss self: 0.0000; time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6934 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6949 score: 0.5000 time: 0.08s
Epoch 4/1000, LR 0.000060
Train loss: 2.6891;  Loss pred: 2.6891; Loss self: 0.0000; time: 0.29s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6933 score: 0.5102 time: 0.33s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6947 score: 0.5000 time: 2.00s
Epoch 5/1000, LR 0.000090
Train loss: 2.6657;  Loss pred: 2.6657; Loss self: 0.0000; time: 1.91s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6932 score: 0.5102 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6945 score: 0.5000 time: 0.08s
Epoch 6/1000, LR 0.000120
Train loss: 2.6757;  Loss pred: 2.6757; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6931 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6943 score: 0.5000 time: 0.07s
Epoch 7/1000, LR 0.000150
Train loss: 2.6160;  Loss pred: 2.6160; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6929 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6941 score: 0.5000 time: 0.08s
Epoch 8/1000, LR 0.000180
Train loss: 2.5804;  Loss pred: 2.5804; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6928 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6939 score: 0.5000 time: 0.08s
Epoch 9/1000, LR 0.000210
Train loss: 2.5382;  Loss pred: 2.5382; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6926 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6936 score: 0.5000 time: 0.07s
Epoch 10/1000, LR 0.000240
Train loss: 2.5216;  Loss pred: 2.5216; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6925 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6934 score: 0.5000 time: 0.08s
Epoch 11/1000, LR 0.000270
Train loss: 2.4796;  Loss pred: 2.4796; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6924 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6932 score: 0.5000 time: 0.07s
Epoch 12/1000, LR 0.000270
Train loss: 2.4178;  Loss pred: 2.4178; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6924 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6931 score: 0.5000 time: 0.07s
Epoch 13/1000, LR 0.000270
Train loss: 2.3909;  Loss pred: 2.3909; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6924 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6930 score: 0.5000 time: 0.08s
Epoch 14/1000, LR 0.000270
Train loss: 2.3474;  Loss pred: 2.3474; Loss self: 0.0000; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6924 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6929 score: 0.5000 time: 0.08s
Epoch 15/1000, LR 0.000270
Train loss: 2.3031;  Loss pred: 2.3031; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6924 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6928 score: 0.5000 time: 0.08s
Epoch 16/1000, LR 0.000270
Train loss: 2.2849;  Loss pred: 2.2849; Loss self: 0.0000; time: 0.25s
Val loss: 0.6924 score: 0.5306 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6927 score: 0.5000 time: 0.08s
Epoch 17/1000, LR 0.000270
Train loss: 2.2084;  Loss pred: 2.2084; Loss self: 0.0000; time: 0.25s
Val loss: 0.6924 score: 0.5510 time: 0.07s
Test loss: 0.6927 score: 0.5417 time: 0.08s
     INFO: Early stopping counter 1 of 2
Epoch 18/1000, LR 0.000270
Train loss: 2.1902;  Loss pred: 2.1902; Loss self: 0.0000; time: 0.25s
Val loss: 0.6924 score: 0.7143 time: 0.07s
Test loss: 0.6926 score: 0.6458 time: 0.07s
     INFO: Early stopping counter 2 of 2
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 015,   Train_Loss: 2.2849,   Val_Loss: 0.6924,   Val_Precision: 0.5208,   Val_Recall: 1.0000,   Val_accuracy: 0.6849,   Val_Score: 0.5306,   Val_Loss: 0.6924,   Test_Precision: 0.5000,   Test_Recall: 1.0000,   Test_accuracy: 0.6667,   Test_Score: 0.5000,   Test_loss: 0.6927


[0.23318098695017397, 0.08915413194335997, 0.1553904350148514, 1.2970901570515707, 0.06833925400860608, 0.07030992500949651, 0.07088473904877901, 0.07030116498935968, 0.07024163601454347, 0.06723103299736977, 0.06789186701644212, 0.0667899859836325, 0.06855980795808136, 0.06746689195279032, 0.06679393199738115, 0.06712267198599875, 0.06701960798818618, 0.06740954506676644, 0.07030026195570827, 0.11338427697774023, 3.8291455439757556, 0.42795627494342625, 0.0842048330232501, 0.3764946659794077, 0.2627959660021588, 3.683049031998962, 0.28130275302100927, 0.10850626602768898, 0.2526084090350196, 0.4307472830405459, 3.2609803919913247, 1.062235123012215, 0.11042792501393706, 0.08122191694565117, 0.41604348202235997, 1.4065162999322638, 0.24340081098489463, 0.09634289203677326, 0.21544376900419593, 3.6220269959885627, 0.09560070210136473, 2.7187904400052503, 0.2078710999339819, 0.06800023699179292, 0.06682267202995718, 0.07298056094441563, 0.07971640303730965, 0.07243882399052382, 0.0707066769246012, 0.07255370600614697, 0.4617437099805102, 0.4125656640389934, 0.09377711289562285, 0.1522893359651789, 0.43140664999373257, 3.2743856819579378, 0.11159782100003213, 0.27691947703715414, 3.804454207071103, 0.2509990129619837, 0.4491250750143081, 0.17711546504870057, 0.07963992899749428, 0.7267177529865876, 0.2615894739283249, 0.24330914800520986, 0.11096688697580248, 0.06759203202091157, 0.07023688603658229, 0.7988168030278757, 4.1093093829695135, 2.1284215800696984, 0.26571146794594824, 0.07879894506186247, 0.0840049630496651, 0.07400007697287947, 0.07221362204290926, 0.07292547798715532, 0.2444048710167408, 0.3702587209409103, 3.464998741983436, 0.7085147930774838, 0.37218114000279456, 0.2449352879775688, 4.109057744033635, 0.44658450700808316, 0.1139391059987247, 0.12110082805156708, 0.08842127898242325, 0.08522002201061696, 0.07377695001196116, 0.38646809093188494, 1.2775109610520303, 0.3236273319926113, 0.08542765397578478, 0.08015552104916424, 4.00990173406899, 1.8812506700633094, 0.07749449706170708, 0.2874434710247442, 0.11702824500389397, 0.08609981799963862, 0.4541276190429926, 0.1063358229584992, 3.1249869849998504, 0.10431108600459993, 0.0939138219691813, 0.20104122790507972, 0.09252793702762574, 0.07652129302732646, 0.1040340350009501, 0.09643296094145626, 0.07584758603479713, 0.17696274898480624, 0.09873970097396523, 0.08684561401605606, 0.0802544979378581, 0.0764295479748398, 0.07486880198121071, 0.07532561395782977, 0.08653872099239379, 0.25292818795423955, 0.2848157779080793, 1.8538599329767749, 0.45628309599123895, 0.3794238710543141, 0.09326896606944501, 0.47426814096979797, 3.3941070010187104, 0.07314897805918008, 0.07333207898773253, 0.07514994707889855, 0.1109289180021733, 0.07730081607587636, 0.08002729504369199, 0.08410196809563786, 3.3234547378960997, 0.14768716599792242, 2.7006702220533043, 0.7139917779713869, 0.07774489698931575, 0.08457043196540326, 0.07584460300859064, 0.07375106995459646, 0.07382069807499647, 0.07465004397090524, 0.07319042400922626, 3.291164679918438, 0.5645491289906204, 0.07596781197935343, 0.0744674769230187, 0.07441640400793403, 0.07346781401429325, 0.09216894896235317, 0.07418619201052934, 0.39861968299373984, 0.30932084599044174, 0.0740677050780505, 0.073433026089333, 0.07313049701042473, 0.0725328039843589, 0.07408951094839722, 0.07323496206663549, 0.07466926902998239, 0.07270800101105124, 0.0742336290422827, 0.07455009105615318, 1.4103588509606197, 0.07045438198838383, 0.06857793498784304, 0.06816256290767342, 0.06776553497184068, 0.06764916493557394, 0.06829036795534194, 0.06819309899583459, 0.06801709393039346, 0.07339155301451683, 0.07862156198825687, 0.06975168699864298, 1.9483703109435737, 0.31746900698635727, 0.3192789589520544, 0.3425760939717293, 0.23239454091526568, 2.414584164042026, 0.24746851599775255, 0.07055562699679285, 0.23529418697580695, 3.881600628956221, 2.064389727078378, 0.08523847197648138, 0.15779891796410084, 0.4282604119507596, 0.46013281703926623, 3.3409330860013142, 0.09637621603906155, 4.559252391103655, 0.07476810005027801, 0.07036827597767115, 0.06921420502476394, 0.06977331405505538, 0.0713006058940664, 0.07076375803444535, 0.15819104993715882, 0.0793857870157808, 0.40730101603548974, 2.65023025800474, 0.4408392289187759, 0.08030031400267035, 0.08114385791122913, 0.08075247495435178, 0.10893647104967386, 0.08475895901210606, 0.0797769179334864, 0.43790788599289954, 0.44502640911377966, 0.4495101450011134, 0.44458773802034557, 0.38596075295936316, 2.9938261749921367, 0.4650887689786032, 0.42335198691580445, 0.1864769330713898, 0.46511844801716506, 0.7289329940686002, 0.08141366695053875, 0.12341068091336638, 0.15183134807739407, 0.08480624202638865, 2.780747360084206, 0.20626865001395345, 0.07579951803199947, 0.1638178089633584, 0.08565259294118732, 0.0913473810069263, 0.4102826309390366, 2.4935159349115565, 3.8001923830015585, 0.13446865906007588, 0.16927540500182658, 0.1526427820790559, 2.0149686249205843, 0.2361295809969306, 0.4492806929629296, 0.07962428196333349, 0.44318135001230985, 1.5730426739901304, 0.40998273296281695, 0.4444417719496414, 4.056537986034527, 0.08604860794730484, 3.4806995750404894, 2.599598899949342, 1.5112921959953383, 0.5848805810092017, 0.4593315039528534, 2.732989937067032, 0.25936915806960315, 0.08678522205445915, 0.0854891249909997, 0.10091537202242762, 0.14879952208139002, 0.12488683708943427, 0.08495983993634582, 0.08895350701641291, 0.4211612230865285, 2.2998173129744828, 2.2604189579142258, 2.497399334097281, 0.07977873599156737, 0.08318680198863149, 3.027232708991505, 0.41940846806392074, 0.23513564094901085, 0.16076997003983706, 0.13520849798806012, 0.4352562240092084, 0.5241796089103445, 2.6613076669164, 2.2430160420481116, 0.2650066970381886, 0.2519071730785072, 0.45767789194360375, 3.2272743539651856, 1.3644798370078206, 0.22492097690701485, 0.07707408897113055, 0.07747015496715903, 0.2628821940161288, 0.07657243590801954, 3.318577020079829, 0.13061022199690342, 0.08823549293447286, 0.09624280605930835, 3.585166804958135, 0.2326499429764226, 0.0721691909711808, 0.07155396393500268, 0.07110628904774785, 0.07056866702623665, 0.07547588495071977, 0.0714593039592728, 0.07254513900261372, 0.07183685700874776, 2.437410108046606, 0.07192518096417189, 0.0722841250244528, 0.0710478340042755, 0.07143871299922466, 0.07104883901774883, 0.08325342298485339, 0.11250223789829761, 0.07402831909712404, 0.07332919607870281, 1.5950878800358623, 0.0816543570253998, 0.07885405991692096, 0.07475629402324557, 0.06862198002636433, 0.06737405702006072, 0.07706611696630716, 0.06966056802775711, 0.06878474703989923, 0.06922041706275195, 0.0901484350906685, 0.07296331995166838, 0.07958729600068182, 0.089767488068901, 0.07579054404050112, 0.07518336095381528, 0.0738299370277673, 0.0736693199723959, 0.07355900201946497, 0.07264547003433108, 0.0732635511085391, 0.07316710893064737, 0.07316039607394487, 0.07361612806562334, 0.07355018507223576, 0.076388304005377, 0.07822113507427275, 0.07322514511179179, 0.07398701610509306, 0.07300570199731737, 0.07469845307059586, 0.0740685670170933, 0.07394993701018393, 1.026842368999496, 0.141587597085163, 0.07707104505971074, 0.07563296507578343, 0.07441638107411563, 0.07613064500037581, 0.07531803799793124, 0.07493409106973559, 0.07512800605036318, 0.07479634101036936, 0.07509091997053474, 0.8814717150526121, 0.07451961899641901, 0.07615504099521786, 0.07385428203269839, 0.07385344500653446, 0.07357667305041105, 0.0751358870184049, 0.07455443602520972, 0.07470882404595613, 0.07583109603729099, 0.0749007670674473, 0.07445233594626188, 0.07463027397170663, 1.2051177700050175, 2.4355517239309847, 0.0768023889977485, 0.07570971199311316, 0.07357048790436238, 0.07375211489852518, 0.07437768601812422, 0.07444452401250601, 0.07418485905509442, 0.07596777193248272, 0.3183926099445671, 0.07992728089448065, 0.07580054504796863, 0.07599687506444752, 0.06905715900938958, 0.07007613696623594, 0.07419845799449831, 0.07227080990560353, 0.0708592040464282, 0.07032784505281597, 0.07181620504707098, 0.07327521604020149, 0.07137485698331147, 0.07025524298660457, 0.07373559405095875, 0.07042885501869023, 0.06963139597792178, 0.07042304298374802, 0.07059038698207587, 0.07023125002160668, 0.07597536302637309, 0.0791482919594273, 0.0750654760049656, 0.29878277995157987, 0.5465712329605594, 0.12363310100045055, 0.0709574039792642, 0.06988388602621853, 0.07368173298891634, 0.07364413910545409, 0.07364590500947088, 0.07379131496418267, 0.06994892202783376, 0.07078003801871091, 0.07005748094525188, 0.07192587899044156, 0.07100432692095637, 1.567748832050711, 0.8700333489105105, 0.07563688897062093, 0.0740443019894883, 0.07456413307227194, 0.07451072300318629, 0.07405666902195662, 0.07420692103914917, 0.07392498606350273, 0.07443059294018894, 0.07397692708764225, 0.0721939200302586, 0.0710707560647279, 2.4702081049326807, 0.5858036589343101, 0.07809307402931154, 0.07509686797857285, 0.07565925596281886, 0.07521823490969837, 0.07594747503753752, 0.07478547899518162, 0.0747062599984929, 0.0755565509898588, 0.0752038030186668, 0.07481998694129288, 2.068312476039864, 1.7470445659710094, 0.06879084696993232, 0.06799268000759184, 0.06758477003313601, 0.06699467392172664, 0.06826093804556876, 0.0712685149628669, 0.07303125702310354, 0.07228487392421812, 0.07197060703765601, 0.07207076298072934, 1.349237116985023, 0.07799394801259041, 0.07539473997894675, 0.074214993044734, 0.07399695506319404, 0.19787237502168864, 0.07383550005033612, 0.07458865805529058, 0.07303307799156755, 1.8685675889719278, 0.7360555230407044, 0.16186639096122235, 0.08720535202883184, 0.07712222100235522, 0.07526814797893167, 0.07559245894663036, 0.07532568695023656, 0.07532823598012328, 0.07531460793688893, 0.07648374105338007, 0.07641059695743024, 2.194407671922818, 1.2162537761032581, 0.07446306897327304, 0.07338585704565048, 0.07299435406457633, 0.07313150598201901, 0.0745147910201922, 0.07297153805848211, 0.07317016809247434, 0.07359308993909508, 0.0739505480742082, 1.3898088110145181, 0.1634470330318436, 0.0746288769878447, 0.07173322897870094, 0.08277171803638339, 0.07711186306551099, 0.07958456804044545, 1.523146889056079, 0.06975837703794241, 0.07016807608306408, 0.08533724898006767, 0.08051865897141397, 0.08070010400842875, 0.08919483004137874, 0.08848356606904417, 0.08252242708113045, 2.009803829016164, 0.08402756100986153, 0.07935038197319955, 0.0826812069863081, 0.08268479502294213, 0.07768909295555204, 0.08334299304988235, 0.07779386395122856, 0.07737756799906492, 0.08329072897322476, 0.08421070501208305, 0.08415363007225096, 0.08235341706313193, 0.08285241504199803, 0.07747097010724247]
[0.004758795652044367, 0.0018194720804767341, 0.0031712333676500283, 0.026471227694930016, 0.0013946786532368586, 0.0014348964287652349, 0.0014466273275261022, 0.001434717652844075, 0.0014335027758070097, 0.0013720618979055055, 0.0013855483064580025, 0.0013630609384414796, 0.0013991797542465584, 0.0013768753459753127, 0.0013631414693343093, 0.001369850448693852, 0.001367747101799718, 0.0013757050013625805, 0.0014346992235858829, 0.002313964836280413, 0.07814582742807664, 0.008733801529457678, 0.0017184659800663286, 0.007683564611824648, 0.005363182979635894, 0.0751642659591625, 0.005740872510632843, 0.002214413592401816, 0.005155273653775909, 0.008790760878378488, 0.06655062024472091, 0.021678267816575815, 0.0022536311227334092, 0.0016575901417479832, 0.008490683306578775, 0.028704414284331913, 0.0049673634894876455, 0.00196618147013823, 0.004396811612330529, 0.07391891828548088, 0.0019510347367625456, 0.05548551918378062, 0.004242267345591467, 0.0013877599386080187, 0.001363728000611371, 0.0014893992029472577, 0.00162686536810836, 0.001478343346745384, 0.0014429934066245143, 0.0014806878776764687, 0.009423341020010412, 0.008419707429367212, 0.0019138186305229152, 0.0031079456319424268, 0.00880421734681087, 0.06682419759097832, 0.0022775065510210637, 0.005651417898717432, 0.07764192259328782, 0.005122428835958851, 0.009165817857434859, 0.0036146013275245013, 0.0016253046734182506, 0.014830974550746685, 0.005338560692414793, 0.004965492816432855, 0.002264630346444949, 0.0013794292249165627, 0.0014334058374812712, 0.01630238373526277, 0.08386345679529619, 0.04343717510346323, 0.005422683019305066, 0.001608141735956377, 0.0017143870010135733, 0.0015102056525077444, 0.0014737473886308012, 0.0014882750609623535, 0.004987854510545731, 0.0075563004273655155, 0.07071426004047829, 0.014459485573009873, 0.007595533469444787, 0.004998679346480996, 0.08385832130680887, 0.009113969530777208, 0.0023252878775249937, 0.0024714454704401444, 0.0018045158976004744, 0.0017391841226656521, 0.0015056520410604319, 0.00788710389656908, 0.026071652266367967, 0.006604639428420638, 0.0017434215097098934, 0.0016358269601870253, 0.08183472926671408, 0.03839287081761856, 0.0015815203481981037, 0.005866193286219269, 0.0023883315306917137, 0.0017571391428497676, 0.009267910592714134, 0.0021701188358877388, 0.06377524459183369, 0.0021287976735632637, 0.001916608611615945, 0.004102882202144484, 0.0018883252454617498, 0.0015616590413740094, 0.002123143571447961, 0.0019680196110501277, 0.0015479099190774926, 0.003611484673159311, 0.0020150959382441882, 0.0017723594697154298, 0.0016378468966909818, 0.0015597866933640776, 0.0015279347343104227, 0.0015372574277108116, 0.0017660963467835467, 0.005161799754168154, 0.005812566896083252, 0.037833876183199486, 0.00931189991818855, 0.0077433443072309, 0.0019034482871315309, 0.009678941652444857, 0.06926748981670838, 0.0014928362869220425, 0.00149657304056597, 0.0015336723893652766, 0.002263855469432108, 0.001577567675017885, 0.0016332101029324897, 0.001716366695829344, 0.06782560689583877, 0.0030140237958759678, 0.05511571881741437, 0.014571260774926263, 0.0015866305508023622, 0.0017259271829674135, 0.0015478490409916456, 0.0015051238766244175, 0.0015065448586733974, 0.0015234702851205152, 0.0014936821226372706, 0.06716662612078446, 0.011521410795726947, 0.0015503635097827231, 0.0015197444270003814, 0.0015187021226108987, 0.001499343143148842, 0.0018809989584153708, 0.0015140039185822314, 0.008135095571300812, 0.006312670326335546, 0.0015115858179193978, 0.0014986331854965917, 0.0014924591226617293, 0.001480261305803243, 0.001512030835681576, 0.0014945910625843977, 0.0015238626332649467, 0.0014838367553275762, 0.0015149720212710755, 0.0015214304297174119, 0.028782833693073873, 0.0014378445303751802, 0.0013995496936294498, 0.0013910727124014984, 0.0013829701014661364, 0.001380595202766815, 0.0013936809786804477, 0.0013916958978741753, 0.0013881039577631317, 0.0014977867962146293, 0.001604521673229732, 0.0014235038162988363, 0.03976265940701171, 0.00647895932625219, 0.006515897121470497, 0.006991348856565904, 0.004742745732964605, 0.04927722783759236, 0.005050377877505154, 0.0014399107550365888, 0.004801922183179734, 0.07921633936645349, 0.04213040259343629, 0.0017395606525812525, 0.003220386080900017, 0.00874000840715836, 0.009390465653862576, 0.06818230787757784, 0.0019668615518175824, 0.09304596716538072, 0.0015258795928628165, 0.0014360872648504315, 0.001412534796423754, 0.0014239451847970486, 0.001455114406001355, 0.0014441583272335784, 0.003228388774227731, 0.0016201181023628736, 0.008312265633377341, 0.0540863317960151, 0.008996718957526038, 0.0016387819184218437, 0.001655997100229166, 0.0016480096929459547, 0.002223193286728038, 0.0017297746737164501, 0.0016281003659895184, 0.008936895632508154, 0.009082171614566932, 0.009173676428594152, 0.009073219143272358, 0.007876750060395166, 0.061098493367186466, 0.009491607530175574, 0.00863983646766948, 0.0038056516953344855, 0.009492213224840104, 0.014876183552420413, 0.0016615034071538522, 0.002518585324762579, 0.003098598940354981, 0.001730739633191605, 0.056749946124167464, 0.00420956428599905, 0.0015469289394285605, 0.003343220591088947, 0.0017480121008405577, 0.0018642322654474754, 0.008373114917123196, 0.05088808030431748, 0.07755494659186854, 0.0027442583481648137, 0.0034546001020780938, 0.0031151588179399166, 0.041121808671848656, 0.004818971040753686, 0.009168993733937338, 0.0016249853461904793, 0.009044517347189997, 0.032102911714084297, 0.00836699455026157, 0.009070240243870233, 0.08278648951090872, 0.0017560940397409151, 0.07103468520490795, 0.05305303877447637, 0.030842697877455885, 0.011936338387942893, 0.009374112325568437, 0.05577530483810269, 0.0052932481238694516, 0.001771126980703248, 0.0017446760202244837, 0.0020594973882128087, 0.003036724940436531, 0.0025487109610088624, 0.0017338742844152208, 0.0018153776942125084, 0.008595127001765887, 0.04693504720356087, 0.046130999141106646, 0.0509673333489241, 0.0016281374692156607, 0.0016976898365026834, 0.06178025936717357, 0.008559356491100423, 0.004798686549979813, 0.0032810197967313684, 0.0027593571017971454, 0.00888278008182058, 0.010697543038986623, 0.05431240136564082, 0.045775837592818606, 0.005408299939554869, 0.005140962715887902, 0.009340365141706199, 0.06586274191765684, 0.02784652728587389, 0.004590224018510507, 0.0015729405912475623, 0.0015810235707583477, 0.0053649427350230365, 0.001562702773633052, 0.06772606163428221, 0.002665514734630682, 0.001800724345601487, 0.001964138899169558, 0.07316666948894153, 0.004747958019926992, 0.0014728406320649143, 0.001460284978265361, 0.0014511487560764868, 0.0014401768780864623, 0.0015403241826677505, 0.0014583531420259755, 0.0014805130408696678, 0.0014660583063009747, 0.049743063429522574, 0.0014678608360035078, 0.0014751862249888328, 0.0014499557960056225, 0.0014579329183515236, 0.00144997630648467, 0.0016990494486704773, 0.0022959640387407677, 0.0015107820223902865, 0.0014965142056878125, 0.032552813878282905, 0.0016664154494979552, 0.0016092665289167542, 0.001525638653535624, 0.001400448571966619, 0.0013749807555114432, 0.0015727778972715748, 0.0014216442454644308, 0.0014037703477530455, 0.0014126615727092235, 0.0018397639814422143, 0.0014890473459524159, 0.0016242305306261595, 0.0018319895524265511, 0.0015467457967449207, 0.0015343543051799036, 0.001506733408729945, 0.0015034555096407325, 0.0015012041228462238, 0.0014825606129455324, 0.0014951745124191654, 0.0014932063047070892, 0.001493069307631528, 0.0015023699605229254, 0.0015010241851476686, 0.0015589449797015712, 0.0015963496953933214, 0.0014943907165671795, 0.0015099391041855728, 0.0014899122856595383, 0.0015244582259305278, 0.001511603408512108, 0.0015091823879629374, 0.02095596671427543, 0.0028895427976563877, 0.0015728784706063416, 0.0015435298995057844, 0.0015187016545737885, 0.0015536866326607307, 0.001537102816284311, 0.0015292671646884813, 0.0015332246132727181, 0.0015264559389871297, 0.0015324677545007091, 0.017989218674543103, 0.0015208085509473268, 0.001554184510106487, 0.0015072302455652732, 0.0015072131633986623, 0.0015015647561308376, 0.001533385449355202, 0.0015215191025553004, 0.0015246698784889007, 0.0015475733885161427, 0.0015285870830091287, 0.0015194354274747322, 0.001523066815749115, 0.02459424020418403, 0.04970513722308132, 0.0015673956938316018, 0.0015450961631247584, 0.0015014385286604567, 0.0015051452020107179, 0.0015179119595535556, 0.0015192760002552246, 0.0015139767154100903, 0.0015503626924996473, 0.006497808366215655, 0.0016311689978465438, 0.0015469498989381352, 0.0015509566339683167, 0.0014093297757018283, 0.0014301252442088966, 0.0015142542447856798, 0.0014749144878694598, 0.001446106205029147, 0.0014352621439350198, 0.001465636837695326, 0.00149541257224901, 0.0014566297343532955, 0.001433780469114379, 0.001504808041856301, 0.0014373235718100046, 0.0014210488975086079, 0.0014372049588520005, 0.0014406201424913443, 0.0014332908167674833, 0.0015505176127831243, 0.0016152712644781082, 0.0015319484898972573, 0.006097607754113875, 0.011154514958378764, 0.0025231245102132764, 0.0014481102852911061, 0.001426201755637113, 0.0015037088365084967, 0.0015029416143970222, 0.0015029776532545077, 0.0015059452033506669, 0.0014275290209761992, 0.0014444905718104268, 0.001429744509086773, 0.0014678750814375828, 0.0014490678963460484, 0.031994874123483896, 0.017755782630826746, 0.0015436099789922638, 0.0015111082038671082, 0.0015217170014749377, 0.0015206270000650262, 0.001511360592284829, 0.001514426959982636, 0.0015086731849694435, 0.0015189916926569172, 0.00150973320587025, 0.0014733453067399714, 0.0014504235931577123, 0.050412410304748585, 0.011955176712945104, 0.0015937362046798272, 0.001532589142419854, 0.0015440664482207931, 0.0015350660185652729, 0.0015499484701538269, 0.0015262342652077882, 0.0015246175509896509, 0.0015419704283644654, 0.0015347714901768736, 0.0015269385090059772, 0.0422104586946911, 0.03565397073410223, 0.0014038948361210677, 0.00138760571444065, 0.0013792810210844083, 0.0013672382433005438, 0.0013930803682769136, 0.0014544594890381001, 0.0014904338167980313, 0.0014752015086575126, 0.0014687878987276737, 0.001470831897565905, 0.027535451367041285, 0.001591713224746743, 0.001538668162835648, 0.00151459169479049, 0.0015101419400651843, 0.004038211735136503, 0.001506846939802778, 0.0015222175113324607, 0.0014904709794197459, 0.038134032427998524, 0.015021541286544988, 0.003303395733902497, 0.0017797010618128947, 0.001573922877599086, 0.0015360846526312586, 0.001542703243808783, 0.0015372589173517665, 0.0015373109383698628, 0.0015370328150385497, 0.001560892674558777, 0.0015593999379067396, 0.04478383003924118, 0.02482150563476037, 0.0015196544688423068, 0.0014976705519520507, 0.0014896806951954352, 0.0014924797139187552, 0.0015207100208202492, 0.0014892150624180023, 0.0014932687365811091, 0.0015018997946754098, 0.0015091948586573101, 0.02836344512274527, 0.003335653735343747, 0.0015230383058743818, 0.001463943448544917, 0.0016892187354363957, 0.0015737114911328774, 0.0016241748579682745, 0.031084630388899574, 0.0014236403477131104, 0.0014320015527155934, 0.0017415765097972993, 0.0016432379381921217, 0.0016469408981311991, 0.001858225625862057, 0.0018434076264384203, 0.0017192172308568843, 0.041870913104503416, 0.0017505741877054486, 0.0016531329577749905, 0.0017225251455480854, 0.0017225998963112943, 0.0016185227699073341, 0.0017363123552058823, 0.0016207054989839282, 0.0016120326666471858, 0.0017352235202755157, 0.0017543896877517302, 0.0017532006265052285, 0.0017156961888152484, 0.0017260919800416257, 0.0016139785439008847]
[210.13720132538208, 549.6099724366104, 315.33472440125956, 37.77686518829378, 717.0110460062872, 696.9144113492049, 691.263037115519, 697.001251791721, 697.5919522981297, 728.8300925246381, 721.7359332323728, 733.6429148526531, 714.7044523514336, 726.2821597634518, 733.5995731157313, 730.0066959524646, 731.1293138067508, 726.9000250849857, 697.0102050383794, 432.15868466154046, 12.796588543647754, 114.49767854549525, 581.9143419769081, 130.14792619314304, 186.45643898353248, 13.304194316795565, 174.18955013334124, 451.5868234512468, 193.97612370539508, 113.75579586740578, 15.026155974546674, 46.12914687009134, 443.7283413033047, 603.2854412041008, 117.77615109318437, 34.83784724169907, 201.31403754049498, 508.60005304072786, 227.4375361445041, 13.528336496185167, 512.5485370185426, 18.022720427068066, 235.72300341683854, 720.585723927902, 733.2840563159893, 671.4116658725054, 614.6790137666716, 676.4328477559217, 693.003859483478, 675.3617795326481, 106.11947481010245, 118.76897248378089, 522.5155529637459, 321.7559502078589, 113.58193018283762, 14.964639098562197, 439.07667336968785, 176.94674467923286, 12.879639846611044, 195.21989119304442, 109.1010115577247, 276.65568326586674, 615.2692577305247, 67.42645242754149, 187.3164056036365, 201.3898795081506, 441.57316957701966, 724.9375190383448, 697.6391290251502, 61.34072269670332, 11.924144773103233, 23.021754928079343, 184.4105577331262, 621.8357360181878, 583.298869746903, 662.1614733989827, 678.5423388801111, 671.9188046820973, 200.48700255505008, 132.3398943189779, 14.14141927565359, 69.15875360508008, 131.6563219716939, 200.05284009745228, 11.924875008424538, 109.7216746910414, 430.054278296237, 404.6215107557716, 554.1652480478192, 574.982249991621, 664.1640782392851, 126.78925155721655, 38.3558352874315, 151.408719709492, 573.5847552818141, 611.31160222819, 12.219750819249617, 26.04650235066814, 632.3029616023242, 170.46830051597135, 418.7023397502849, 569.1068940494818, 107.89918504243437, 460.8042580262321, 15.68006530433673, 469.74872831675043, 521.7549341786964, 243.73110187694945, 529.5697880454229, 640.3446421442676, 470.99970696659517, 508.1250178530507, 646.0324258377838, 276.8944327611404, 496.25428795778777, 564.2196276134435, 610.5576791215018, 641.1133036679811, 654.4782166047906, 650.5091352781023, 566.22052461703, 193.73087830315382, 172.04103073185127, 26.43133881280872, 107.38947033212216, 129.1431660950655, 525.3623157301455, 103.31708113432056, 14.436787050406217, 669.8658176790561, 668.1932474353692, 652.0297339471949, 441.7243121314858, 633.8872276833785, 612.2910935981002, 582.6260800969472, 14.743694096769696, 331.7823838578452, 18.143644344234513, 68.62824126521477, 630.2664470278213, 579.398719638151, 646.0578347868727, 664.3971406810231, 663.7704773560872, 656.3961304443127, 669.486489022432, 14.888346456493426, 86.79492622300035, 645.0100209983308, 658.0053739520961, 658.4569713255129, 666.958730941239, 531.6324049655191, 660.500271978448, 122.92418586056004, 158.41156726150342, 661.5568816175033, 667.2746938195132, 670.0351016760499, 675.5564008054403, 661.3621735757998, 669.0793388465959, 656.2271284633139, 673.9285817052274, 660.0781967979783, 657.2761925011179, 34.74293082687804, 695.4854846087342, 714.5155363556272, 718.8696831480762, 723.0814310011938, 724.3252750668161, 717.5243224936678, 718.5477815430129, 720.4071383900208, 667.6517662776234, 623.2386989121225, 702.4919698494647, 25.14922328921644, 154.34577524635563, 153.47080860207345, 143.0339152738535, 210.84832632908572, 20.293349359988248, 198.00498581583204, 694.487485771012, 208.24993863974294, 12.623658300770709, 23.735828248548355, 574.8577944184624, 310.521774370769, 114.41636591344313, 106.49099169950858, 14.66656132842426, 508.4241944105812, 10.747376060077825, 655.3597051021735, 696.3365141352674, 707.9471617490722, 702.2742242304275, 687.231186685859, 692.4448525776216, 309.7520372958216, 617.2389522353601, 120.30414379258616, 18.488959535497223, 111.15163258083865, 610.2093199582075, 603.8657917103928, 606.7925475683438, 449.80344532784187, 578.1099788283309, 614.2127481141036, 111.89567844593351, 110.10582517468575, 109.00755087491657, 110.21446569396304, 126.95591358523204, 16.367015696937948, 105.35623147299498, 115.74293144806957, 262.76708434088795, 105.34950873028298, 67.22154217015533, 601.8645497170514, 397.04829142299064, 322.7265029289135, 577.7876584220412, 17.621162103167904, 237.5542768941635, 646.4421050713568, 299.11277845841516, 572.0784195482029, 536.413846350829, 119.42986688920021, 19.65096726030668, 12.894084051949404, 364.3971788110754, 289.46910509220913, 321.0109206121662, 24.31799651566844, 207.51317896353248, 109.06322209586473, 615.3901648063113, 110.56421936220686, 31.149822449322464, 119.51722855714515, 110.25066294972846, 12.079265661678173, 569.4455862668577, 14.077629782061845, 18.849061676766695, 32.422585208764716, 83.77778574123852, 106.67676738547733, 17.929081748681966, 188.91991771377297, 564.612255866001, 573.1723187616989, 485.5553620622847, 329.30213292753785, 392.3552004516697, 576.7430828107975, 550.8495577465985, 116.3449940640258, 21.306040146565184, 21.677397381773048, 19.62041045298497, 614.1987509701746, 589.0357463999693, 16.186400158289747, 116.83121284173038, 208.39035631618964, 304.78328749988776, 362.4032566675436, 112.57736775973902, 93.47940890310547, 18.41200121621986, 21.845586068683577, 184.90098758877363, 194.51609654929948, 107.06219562389941, 15.183090938579866, 35.91112061241778, 217.85429119960304, 635.7519194077507, 632.5016391250535, 186.3952793888873, 639.9169546971165, 14.765364704062616, 375.16206044854374, 555.3320820272328, 509.1289625305023, 13.667425440912558, 210.6168579846409, 678.9600844987591, 684.79784075289, 689.1092286801314, 694.3591549176115, 649.2139844666069, 685.7049717126666, 675.441534383642, 682.1011113283137, 20.10330548734356, 681.2634927454459, 677.8805164124753, 689.6761975467301, 685.9026141824784, 689.6664418085597, 588.5643886247747, 435.5468914698049, 661.9088559300224, 668.2195171948871, 30.71931058676111, 600.0904518145654, 621.4011054297699, 655.463204004781, 714.0569243436923, 727.2829063182314, 635.8176839430291, 703.4108590741803, 712.3672341424342, 707.8836285481915, 543.5479822885147, 671.5703182428936, 615.6761501179814, 545.8546412971929, 646.518647152279, 651.739951212083, 663.6874142473017, 665.1344144124099, 666.1319302161517, 674.5086786119408, 668.8182494376647, 669.6998243629586, 669.7612728951684, 665.6150124646615, 666.2117838571811, 641.4594568895112, 626.4291607821004, 669.169039203574, 662.2783642254087, 671.1804511077851, 655.9707461905695, 661.5491830521298, 662.6104359392764, 47.71910614454208, 346.07551091164555, 635.7770283514034, 647.8656489389583, 658.457174250358, 643.6304329190712, 650.5745675603748, 653.9079783379149, 652.220158314226, 655.1122600129183, 652.5422783370789, 55.588851194250005, 657.5449614463898, 643.4242482132856, 663.468639209107, 663.4761587041003, 665.9719442115527, 652.1517472469209, 657.2378870042181, 655.8796852411746, 646.1729100671785, 654.1989076810929, 658.139189015737, 656.570013645891, 40.659926539624415, 20.11864478940891, 638.000987201537, 647.2089076822433, 666.0279331529964, 664.3877272864463, 658.7997371692871, 658.2082517146384, 660.5121398641395, 645.0103610192667, 153.89804433127708, 613.0572621967386, 646.4333464751669, 644.7633532095444, 709.5571364778785, 699.2394575575587, 660.3910825698462, 678.0054086013608, 691.5121424154628, 696.7368325191987, 682.2972610134941, 668.711777978475, 686.516261762275, 697.4568433183376, 664.5365868502537, 695.7375636306526, 703.705552816097, 695.7949830612694, 694.1455075524938, 697.6951141397187, 644.9459146775091, 619.0910604251346, 652.7634620842028, 163.9987418550053, 89.64979685188781, 396.3339882562796, 690.5551394512574, 701.1630690030107, 665.0223605268742, 665.3618413521662, 665.3458871025971, 664.0347854457391, 700.5111527022839, 692.285584631174, 699.4256621686446, 681.2568812195086, 690.0987886913978, 31.255006540751186, 56.31968022991268, 647.8320389278928, 661.7659790615122, 657.1524133795845, 657.6234671337792, 661.6554679967078, 660.3157672334793, 662.8340782899604, 658.3314476531915, 662.3686861438366, 678.7275158276855, 689.4537600721892, 19.836385405000268, 83.64577320862156, 627.4564115840578, 652.4905940681975, 647.6405216577865, 651.4377804640842, 645.1827394627859, 655.2074100261768, 655.9021961611853, 648.5208675893209, 651.5627938102732, 654.9052198906102, 23.690811019918435, 28.047366938670933, 712.3040659961249, 720.6658127687983, 725.015413620197, 731.4014253916549, 717.8336747627055, 687.5406345358889, 670.945592303016, 677.8734933033228, 680.8334960182082, 679.8873492306703, 36.31681887724403, 628.2538741607237, 649.9127129251029, 660.2439478834775, 662.1894097960326, 247.63436530556197, 663.637409736442, 656.9363396198603, 670.9288632974989, 26.223295474668618, 66.57106490768125, 302.71880227278757, 561.8921185456555, 635.3551461971459, 651.0057881816836, 648.2128069758239, 650.5085049190662, 650.4864923815492, 650.6041967457405, 640.6590384458518, 641.2723097464976, 22.329488101481374, 40.28764470272853, 658.0443255379056, 667.703587211893, 671.2847949397689, 670.0258574197519, 657.5875652220759, 671.4946855132692, 669.6718249720643, 665.8233815233457, 662.6049606938583, 35.256647973206825, 299.79130909310277, 656.582304032003, 683.0864955842027, 591.9896452851373, 635.4404893365327, 615.6972539588058, 32.1702393590983, 702.4245987452994, 698.3232651554308, 574.1924023288469, 608.5545962383223, 607.1863302045085, 538.1477825310293, 542.4736155247784, 581.6600613650112, 23.882927929088922, 571.2411430621756, 604.9120219258922, 580.5430490141338, 580.5178568403257, 617.8473473420787, 575.9332397778311, 617.0152446739594, 620.3348236607807, 576.2946319683482, 569.9987904520295, 570.3853768255642, 582.8537747644802, 579.3434020682284, 619.5869231217057]
Elapsed: 0.4607364087251849~0.8888710883139129
Time per graph: 0.009405582163314902~0.018142651906792137
Speed: 458.33512530106884~260.3655929588261
Total Time: 0.0795
best val loss: 0.6923523545265198 test_score: 0.5000

Testing...
Test loss: 0.6926 score: 0.6458 time: 0.07s
test Score 0.6458
Epoch Time List: [9.791504344088025, 0.7141125989146531, 0.5960637530079111, 1.770690695848316, 0.36285667412448674, 0.36110068403650075, 0.3778658480150625, 0.369993286090903, 0.37029301293659955, 0.36427700403146446, 0.3588374999817461, 0.3564380049938336, 0.35870629001874477, 0.35387867491226643, 0.3528189380886033, 0.3520884660538286, 0.3523020161082968, 0.3529084980254993, 0.36624529503751546, 0.4400629800511524, 4.28879822883755, 6.340483248815872, 0.5935611708555371, 0.7618988329777494, 1.8011790658347309, 8.479673536960036, 5.186743880040012, 0.6507161649642512, 1.4511513229226694, 1.7368644509697333, 4.676833693985827, 11.926085546961986, 2.819896937930025, 0.42066908918786794, 1.049370611901395, 9.321313038119115, 0.8049108500126749, 0.5842913101660088, 0.5562616579700261, 5.816162660950795, 7.71989314595703, 3.1768658970249817, 8.459914216888137, 1.28778335894458, 0.3679475039243698, 0.386679936083965, 0.4120546339545399, 0.3858008049428463, 0.39363479404710233, 0.38686743297148496, 0.9645626300480217, 11.651263974956237, 0.6703446110477671, 0.4992066101403907, 1.8484740209532902, 9.002799735055305, 4.278910040855408, 1.620391158037819, 11.276761388056912, 8.0684481728822, 1.346844429965131, 1.1786989690735936, 0.6350002819672227, 9.219177598133683, 1.3948310260893777, 1.3316548248985782, 1.458006887929514, 0.44509050308261067, 0.386017058044672, 1.2720112439710647, 7.069779208977707, 15.108253574930131, 3.6973957548616454, 0.5094977499684319, 0.4068975360132754, 0.4206846609013155, 0.42252324195578694, 0.4114400140242651, 0.7222872320562601, 1.4199529628967866, 19.02788251102902, 5.233059666934423, 0.8966803771909326, 0.6881888518109918, 5.646182882948779, 7.638233274105005, 0.7137740418547764, 0.5235790819860995, 0.5280283830361441, 0.49849183904007077, 0.44386296800803393, 1.1403752469923347, 9.178591525065713, 4.283896359964274, 0.6407826719805598, 0.4268434800906107, 7.250634026830085, 15.481539977015927, 0.43215504416730255, 0.7432681419886649, 0.6686337129212916, 0.4350144111085683, 0.9550108109833673, 0.646171759115532, 13.304370902944356, 0.7961710868403316, 0.7389946748735383, 0.5510749380337074, 0.4971121409907937, 0.4112602900713682, 0.4887806080514565, 0.4469495629891753, 0.4529327671043575, 0.5043962049530819, 0.8899830270092934, 0.5221824169857427, 0.6625034969765693, 0.553747285855934, 0.3987812161212787, 0.5001288829371333, 0.7129855368984863, 0.664292388013564, 1.082372328150086, 8.733015589998104, 8.274205449037254, 1.5913300738902763, 1.272691491059959, 1.615473901038058, 13.398587472969666, 1.7830365769332275, 0.4808436209568754, 0.4309548099990934, 0.4437183398986235, 0.3990358761511743, 0.42058171005919576, 0.5170186260947958, 6.345429116045125, 7.86216136906296, 3.0359080989146605, 8.285366571159102, 1.0117577619384974, 0.4285808259155601, 0.4202912870096043, 0.3988366359844804, 0.3936973309610039, 0.39055500202812254, 0.3870843310141936, 4.3257229728624225, 5.127948538982309, 0.6142592080868781, 0.393052269006148, 0.3943920099409297, 0.39318897610064596, 0.4536664988845587, 2.6110088831046596, 0.7166562899947166, 6.224221398937516, 1.7741172250825912, 0.3907422620104626, 0.3924009809270501, 0.39398659102153033, 0.3969373081345111, 0.39382496604230255, 0.38684334594290704, 0.38413563510403037, 0.39043578098062426, 0.39346795505844057, 7.921178663033061, 6.1988871219800785, 0.3711966589326039, 0.3616134349722415, 0.35893993207719177, 0.362536404049024, 0.36805543303489685, 0.3663916072109714, 0.364633179968223, 0.37490066804457456, 0.40869415807537735, 0.465380122885108, 13.81432059709914, 2.8583990728948265, 1.1105411569587886, 1.1765951809938997, 1.7177718030288815, 3.7815142510225996, 8.920126826036721, 0.47030011797323823, 0.8687703389441594, 5.258275065221824, 9.342972929007374, 2.1513166889781132, 0.4782160061877221, 1.3479941400000826, 1.8501070219790563, 10.108196928864345, 7.053806724958122, 7.8465315400389954, 3.0602685931371525, 0.38097329693846405, 0.3754576080245897, 0.4732358528999612, 0.3748277850681916, 0.3848708029836416, 0.4723614789545536, 0.4475824430119246, 0.9837264540838078, 7.051806709030643, 8.750421261880547, 0.557300150860101, 0.4638173161074519, 0.7332857140572742, 0.46039258199743927, 0.4308425378985703, 0.5279091581469402, 1.8044309570686892, 1.4529706500470638, 11.172454023151658, 2.014866207027808, 1.5054404969559982, 6.580290523939766, 11.54351308196783, 1.1564229660434648, 0.726065898896195, 1.8116492320550606, 1.5529105662135407, 12.705336259910837, 0.49341841496061534, 1.1605896919500083, 0.4904380710795522, 3.1172322179190814, 7.5510274599073455, 0.503226934117265, 0.6079601360252127, 0.4530108099570498, 0.5709767340449616, 1.1174318100092933, 3.88515841611661, 9.151130654034205, 4.534630280104466, 1.4186587170697749, 1.1900096820900217, 7.3710298899095505, 4.724464745959267, 1.6148364819819108, 1.3882208779687062, 1.418510522111319, 13.538397640222684, 1.5666188290342689, 1.6052035021129996, 8.458555100020021, 8.562084065866657, 6.132049727952108, 8.302883691969328, 6.502457805909216, 1.9608553709695116, 1.6742523559369147, 12.883484293008223, 7.625683302991092, 1.1724543699529022, 0.46609585802070796, 0.5434855420608073, 0.5137292231665924, 0.5371060039615259, 0.4291786761023104, 0.8520416209939867, 0.9246754738269374, 6.640371938003227, 11.19391930301208, 16.46409208397381, 0.430555145139806, 0.4494451809441671, 5.9546980179147795, 7.806204485008493, 0.7715124451788142, 0.7930648879846558, 0.5678650920744985, 0.9860094911418855, 1.2518637790344656, 12.353645834024064, 8.487111538997851, 1.4747344958595932, 1.6227903899271041, 1.7855017790570855, 12.798100810032338, 6.309093381976709, 2.96766234585084, 0.5154688709881157, 0.6154860638780519, 1.0759720019996166, 0.7862536769825965, 7.291356415953487, 5.341052417992614, 0.6691781372064725, 0.5053598930826411, 7.053253382095136, 3.5165523999603465, 0.3925429950468242, 0.38678354397416115, 0.38047290698159486, 0.38168894802220166, 0.381035408237949, 0.38448244286701083, 0.3804106229217723, 0.3782966759754345, 5.474929261952639, 0.4263767891097814, 0.3849964201217517, 0.3851935957791284, 0.3827999149216339, 0.3792867539450526, 0.38604073389433324, 2.2303922929568216, 0.4346663700416684, 0.386396485962905, 11.98328515002504, 3.533120305859484, 0.381004061200656, 0.382046829094179, 0.3792653901036829, 0.36369408876635134, 0.38049119093921036, 0.3829227959504351, 0.37596748501528054, 0.3720406040083617, 4.004798694979399, 0.39919261494651437, 0.3968466498190537, 0.432285274961032, 0.4033426941605285, 0.4180736531270668, 0.39461519895121455, 0.3929205930326134, 0.40343510394450277, 0.3895963348913938, 0.39770579477772117, 0.3958736459026113, 0.3913914009463042, 0.3962495488813147, 0.39278865593951195, 0.4013830909971148, 0.42020898195914924, 0.3966312779812142, 0.39930949290283024, 0.3942914019571617, 0.4136900659650564, 0.3949370358604938, 0.39669945009518415, 6.59437210788019, 0.7487019500695169, 0.4980464648688212, 0.4061325431102887, 0.40082040696870536, 0.40997563605196774, 0.40469424601178616, 0.40672096505295485, 0.40413439203985035, 0.40063640393782407, 0.40312510600779206, 2.688577842898667, 5.763549114111811, 0.39854798605665565, 0.391276236041449, 0.3907427790109068, 0.3951019700616598, 0.3962687939638272, 0.4015082159312442, 0.40015375695656985, 0.3984054910251871, 0.39726760517805815, 0.3963578548282385, 0.4029371850192547, 1.5290819120127708, 7.25278913800139, 5.2721232859184965, 0.41990978410467505, 0.4033970650052652, 0.3953868799144402, 0.3952255879994482, 0.40178813401144, 0.39497212099377066, 0.40572988719213754, 0.6416457251179963, 5.671802565921098, 0.38297279307153076, 0.39759405609220266, 0.3729140170617029, 0.3761252610711381, 0.3850541359279305, 0.38534634397365153, 0.38550438010133803, 0.38128629699349403, 0.3951148260384798, 0.4024144969880581, 0.3911515501094982, 0.37754721590317786, 0.39093534008134156, 0.38231315603479743, 0.37717340397648513, 0.38039161113556474, 0.3865030419547111, 0.38070614088792354, 0.3911111420020461, 0.3961901569273323, 0.40043812908697873, 0.6196145841386169, 2.7685052130836993, 5.1690066220471635, 0.3915521439630538, 0.375646970118396, 0.3833115821471438, 0.39029301702976227, 0.39134674391243607, 0.3967230130219832, 0.3842235410120338, 0.3741839840076864, 0.3779927780851722, 0.3791639490518719, 0.3841461519477889, 2.995639121043496, 8.34860189189203, 1.8303169769933447, 0.4033381320768967, 0.3998553629498929, 0.4001407269388437, 0.40173630113713443, 0.3998226139228791, 0.39606392208952457, 0.4026541080093011, 0.4112836408894509, 0.3805480970768258, 0.3831971469335258, 3.0551746719283983, 9.311158633907326, 0.8360018010716885, 0.4075958019820973, 0.407758206827566, 0.4013279710197821, 0.40715754497796297, 0.4077380870003253, 0.38072867796290666, 0.40998608304653317, 0.40707787906285375, 0.40304192702751607, 4.099795997026376, 7.626928568002768, 2.071900616050698, 0.37258218601346016, 0.36110349104274064, 0.3598232758231461, 0.3737938730046153, 0.36756118503399193, 0.37980977399274707, 0.3872302200179547, 0.38618721801321954, 0.38802508695516735, 8.059806192060933, 1.6600154740735888, 0.4073899150826037, 0.40378460800275207, 0.4040606978815049, 0.6615600368240848, 0.5356555231846869, 0.3946660141227767, 0.3889402230270207, 4.919183832011186, 3.262384469155222, 1.6407603099942207, 0.872092310921289, 0.41501454694662243, 0.40495114389341325, 0.4037927580066025, 0.404274033033289, 0.40453545504715294, 0.40672554704360664, 0.40498330199625343, 0.415166451013647, 4.480373556143604, 9.012863005045801, 1.2722134329378605, 0.3936072359792888, 0.3878300270298496, 0.3883174420334399, 0.3891726810252294, 0.3908976790262386, 0.3937761790584773, 0.3917856100015342, 0.39725160994566977, 2.1058327050413936, 3.6401148450095206, 0.6883702820632607, 0.39371923997532576, 0.4101715498836711, 0.4124808310298249, 0.42731552303303033, 4.693959604832344, 2.367622882942669, 0.38466769794467837, 0.37411902588792145, 0.3764208358479664, 0.37157524400390685, 0.4358974399510771, 0.42989402380771935, 0.42456203093752265, 2.6262765449937433, 2.0711018888978288, 0.40630555094685405, 0.4083530140342191, 0.40005262510385364, 0.3968282990390435, 0.40264970099087805, 0.3901443559443578, 0.39401864097453654, 0.4000662511680275, 0.3857938399305567, 0.40287809423170984, 0.4025125530315563, 0.4009206489427015, 0.39109204604756087]
Total Epoch List: [497, 3, 18]
Total Time List: [0.0717103669885546, 0.08205351897049695, 0.07948578509967774]
========================training times:1========================
========================k_idx:0========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: AEtransGAT
AEtransGAT(
  (gae): GAE(
    (encoder): transGAT(
      (encoder): EdgeFeatureGAT()
      (classifier): Sequential(
        (0): Linear(in_features=64, out_features=64, bias=True)
        (1): ReLU(inplace=True)
        (2): Linear(in_features=64, out_features=2, bias=True)
      )
    )
    (decoder): InnerProductDecoder()
  )
)
Total number of parameters: 1914242
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7d35981f3790>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 2.3587;  Loss pred: 2.3587; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6932 score: 0.5102 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6943 score: 0.4898 time: 0.07s
Epoch 2/1000, LR 0.000000
Train loss: 2.3210;  Loss pred: 2.3210; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6932 score: 0.5102 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6942 score: 0.4898 time: 0.07s
Epoch 3/1000, LR 0.000030
Train loss: 2.3476;  Loss pred: 2.3476; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6932 score: 0.5102 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6941 score: 0.4898 time: 0.07s
Epoch 4/1000, LR 0.000060
Train loss: 2.3173;  Loss pred: 2.3173; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6932 score: 0.5102 time: 0.09s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6940 score: 0.4898 time: 0.07s
Epoch 5/1000, LR 0.000090
Train loss: 2.3069;  Loss pred: 2.3069; Loss self: 0.0000; time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6931 score: 0.5102 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6939 score: 0.4898 time: 0.07s
Epoch 6/1000, LR 0.000120
Train loss: 2.2955;  Loss pred: 2.2955; Loss self: 0.0000; time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6931 score: 0.5102 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6938 score: 0.4898 time: 0.07s
     INFO: Early stopping counter 1 of 2
Epoch 7/1000, LR 0.000150
Train loss: 2.2936;  Loss pred: 2.2936; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6931 score: 0.5102 time: 1.94s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6936 score: 0.4898 time: 2.76s
Epoch 8/1000, LR 0.000180
Train loss: 2.2697;  Loss pred: 2.2697; Loss self: 0.0000; time: 1.63s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6931 score: 0.5102 time: 0.11s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6935 score: 0.4898 time: 0.08s
     INFO: Early stopping counter 1 of 2
Epoch 9/1000, LR 0.000210
Train loss: 2.2359;  Loss pred: 2.2359; Loss self: 0.0000; time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6932 score: 0.5102 time: 0.10s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6933 score: 0.4898 time: 0.06s
     INFO: Early stopping counter 2 of 2
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 006,   Train_Loss: 2.2936,   Val_Loss: 0.6931,   Val_Precision: 0.0000,   Val_Recall: 0.0000,   Val_accuracy: 0.0000,   Val_Score: 0.5102,   Val_Loss: 0.6931,   Test_Precision: 0.0000,   Test_Recall: 0.0000,   Test_accuracy: 0.0000,   Test_Score: 0.4898,   Test_loss: 0.6936


[0.07285241596400738, 0.07310730300378054, 0.07274554204195738, 0.07779006497003138, 0.07738581008743495, 0.07384479802567512, 2.768984962021932, 0.08275702700484544, 0.06890303699765354]
[0.0014867839992654566, 0.001491985775587358, 0.0014846028988154568, 0.001587552346327171, 0.0015793022466823459, 0.001507036694401533, 0.05650989718412106, 0.0016889189184662334, 0.0014061844285235417]
[672.5926566966341, 670.2476768629544, 673.5807944318885, 629.9004894632399, 633.1910197055116, 663.5538495611183, 17.6960152084827, 592.0947353163259, 711.1442707767535]
Elapsed: 0.3742634400130353~0.8466701410239673
Time per graph: 0.007638029388021129~0.017278982469876886
Speed: 584.8890564469898~203.05766706314682
Total Time: 0.0691
best val loss: 0.6931363344192505 test_score: 0.4898

Testing...
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6943 score: 0.4898 time: 0.06s
test Score 0.4898
Epoch Time List: [0.4001076229615137, 0.3984399070031941, 0.39046691299881786, 0.4048434681026265, 0.42205502407159656, 0.41755470586940646, 4.961043810937554, 1.8191702539334074, 0.43878705194219947]
Total Epoch List: [9]
Total Time List: [0.06914844003040344]
========================k_idx:1========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: AEtransGAT
AEtransGAT(
  (gae): GAE(
    (encoder): transGAT(
      (encoder): EdgeFeatureGAT()
      (classifier): Sequential(
        (0): Linear(in_features=64, out_features=64, bias=True)
        (1): ReLU(inplace=True)
        (2): Linear(in_features=64, out_features=2, bias=True)
      )
    )
    (decoder): InnerProductDecoder()
  )
)
Total number of parameters: 1914242
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7d35980f7910>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 3.9165;  Loss pred: 3.9165; Loss self: 0.0000; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6991 score: 0.4898 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6958 score: 0.5102 time: 0.08s
Epoch 2/1000, LR 0.000000
Train loss: 3.9148;  Loss pred: 3.9148; Loss self: 0.0000; time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6990 score: 0.4898 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6957 score: 0.5102 time: 0.09s
Epoch 3/1000, LR 0.000030
Train loss: 4.0059;  Loss pred: 4.0059; Loss self: 0.0000; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6988 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6956 score: 0.5102 time: 0.09s
Epoch 4/1000, LR 0.000060
Train loss: 3.9038;  Loss pred: 3.9038; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6985 score: 0.4898 time: 1.48s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6954 score: 0.5102 time: 1.84s
Epoch 5/1000, LR 0.000090
Train loss: 3.9248;  Loss pred: 3.9248; Loss self: 0.0000; time: 4.97s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6982 score: 0.4898 time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6952 score: 0.5102 time: 0.30s
Epoch 6/1000, LR 0.000120
Train loss: 3.8477;  Loss pred: 3.8477; Loss self: 0.0000; time: 1.30s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6978 score: 0.4898 time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6949 score: 0.5102 time: 0.10s
Epoch 7/1000, LR 0.000150
Train loss: 3.8256;  Loss pred: 3.8256; Loss self: 0.0000; time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6973 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6947 score: 0.5102 time: 0.09s
Epoch 8/1000, LR 0.000180
Train loss: 3.7294;  Loss pred: 3.7294; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6969 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6944 score: 0.5102 time: 0.09s
Epoch 9/1000, LR 0.000210
Train loss: 3.7239;  Loss pred: 3.7239; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6964 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6941 score: 0.5102 time: 0.09s
Epoch 10/1000, LR 0.000240
Train loss: 3.6509;  Loss pred: 3.6509; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6959 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6939 score: 0.5102 time: 0.09s
Epoch 11/1000, LR 0.000270
Train loss: 3.5596;  Loss pred: 3.5596; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6954 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6937 score: 0.5102 time: 0.09s
Epoch 12/1000, LR 0.000270
Train loss: 3.5267;  Loss pred: 3.5267; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6951 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6935 score: 0.5102 time: 0.09s
Epoch 13/1000, LR 0.000270
Train loss: 3.4931;  Loss pred: 3.4931; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6947 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6933 score: 0.5102 time: 0.09s
Epoch 14/1000, LR 0.000270
Train loss: 3.4351;  Loss pred: 3.4351; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6944 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6932 score: 0.5102 time: 0.10s
Epoch 15/1000, LR 0.000270
Train loss: 3.3513;  Loss pred: 3.3513; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6942 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6931 score: 0.5102 time: 0.09s
Epoch 16/1000, LR 0.000270
Train loss: 3.3053;  Loss pred: 3.3053; Loss self: 0.0000; time: 0.59s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6940 score: 0.4898 time: 1.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6930 score: 0.5102 time: 2.02s
Epoch 17/1000, LR 0.000270
Train loss: 3.2226;  Loss pred: 3.2226; Loss self: 0.0000; time: 2.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6938 score: 0.4898 time: 0.51s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6930 score: 0.5102 time: 0.60s
Epoch 18/1000, LR 0.000270
Train loss: 3.1681;  Loss pred: 3.1681; Loss self: 0.0000; time: 1.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6937 score: 0.4898 time: 0.41s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6930 score: 0.5102 time: 0.25s
Epoch 19/1000, LR 0.000270
Train loss: 3.1108;  Loss pred: 3.1108; Loss self: 0.0000; time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6936 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6929 score: 0.5102 time: 0.09s
Epoch 20/1000, LR 0.000270
Train loss: 3.0863;  Loss pred: 3.0863; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6934 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6929 score: 0.5102 time: 0.09s
Epoch 21/1000, LR 0.000270
Train loss: 3.0073;  Loss pred: 3.0073; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6934 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6929 score: 0.5102 time: 0.09s
Epoch 22/1000, LR 0.000270
Train loss: 2.9096;  Loss pred: 2.9096; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6933 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6929 score: 0.5102 time: 0.09s
Epoch 23/1000, LR 0.000270
Train loss: 2.9099;  Loss pred: 2.9099; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6932 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6929 score: 0.5102 time: 0.09s
Epoch 24/1000, LR 0.000270
Train loss: 2.8224;  Loss pred: 2.8224; Loss self: 0.0000; time: 1.60s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6931 score: 0.4898 time: 1.79s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6929 score: 0.5102 time: 1.99s
Epoch 25/1000, LR 0.000270
Train loss: 2.8098;  Loss pred: 2.8098; Loss self: 0.0000; time: 5.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6931 score: 0.4898 time: 0.98s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6928 score: 0.5102 time: 0.96s
Epoch 26/1000, LR 0.000270
Train loss: 2.7453;  Loss pred: 2.7453; Loss self: 0.0000; time: 1.96s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6930 score: 0.4898 time: 0.09s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6928 score: 0.5102 time: 0.09s
Epoch 27/1000, LR 0.000270
Train loss: 2.7115;  Loss pred: 2.7115; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6930 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6928 score: 0.5102 time: 0.09s
Epoch 28/1000, LR 0.000270
Train loss: 2.6723;  Loss pred: 2.6723; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6929 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6928 score: 0.5102 time: 0.09s
Epoch 29/1000, LR 0.000270
Train loss: 2.6012;  Loss pred: 2.6012; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6929 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6927 score: 0.5102 time: 0.09s
Epoch 30/1000, LR 0.000270
Train loss: 2.5940;  Loss pred: 2.5940; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6928 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6927 score: 0.5102 time: 0.09s
Epoch 31/1000, LR 0.000270
Train loss: 2.5031;  Loss pred: 2.5031; Loss self: 0.0000; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6928 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6926 score: 0.5102 time: 0.09s
Epoch 32/1000, LR 0.000270
Train loss: 2.4416;  Loss pred: 2.4416; Loss self: 0.0000; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6927 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6926 score: 0.5102 time: 0.09s
Epoch 33/1000, LR 0.000270
Train loss: 2.4289;  Loss pred: 2.4289; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6927 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6926 score: 0.5102 time: 0.09s
Epoch 34/1000, LR 0.000270
Train loss: 2.4123;  Loss pred: 2.4123; Loss self: 0.0000; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6927 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6925 score: 0.5102 time: 0.09s
Epoch 35/1000, LR 0.000270
Train loss: 2.3538;  Loss pred: 2.3538; Loss self: 0.0000; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6926 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6925 score: 0.5102 time: 0.09s
Epoch 36/1000, LR 0.000270
Train loss: 2.3328;  Loss pred: 2.3328; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6926 score: 0.4898 time: 0.36s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6924 score: 0.5102 time: 2.23s
Epoch 37/1000, LR 0.000270
Train loss: 2.3130;  Loss pred: 2.3130; Loss self: 0.0000; time: 5.60s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6925 score: 0.4898 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6924 score: 0.5102 time: 0.09s
Epoch 38/1000, LR 0.000270
Train loss: 2.2450;  Loss pred: 2.2450; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6925 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6923 score: 0.5102 time: 0.09s
Epoch 39/1000, LR 0.000269
Train loss: 2.2046;  Loss pred: 2.2046; Loss self: 0.0000; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6925 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6923 score: 0.5102 time: 0.09s
Epoch 40/1000, LR 0.000269
Train loss: 2.2057;  Loss pred: 2.2057; Loss self: 0.0000; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6924 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6922 score: 0.5102 time: 0.09s
Epoch 41/1000, LR 0.000269
Train loss: 2.1630;  Loss pred: 2.1630; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6924 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6922 score: 0.5102 time: 0.08s
Epoch 42/1000, LR 0.000269
Train loss: 2.1302;  Loss pred: 2.1302; Loss self: 0.0000; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6923 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6921 score: 0.5102 time: 0.08s
Epoch 43/1000, LR 0.000269
Train loss: 2.0810;  Loss pred: 2.0810; Loss self: 0.0000; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6923 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6920 score: 0.5102 time: 0.08s
Epoch 44/1000, LR 0.000269
Train loss: 2.0942;  Loss pred: 2.0942; Loss self: 0.0000; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6922 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6920 score: 0.5102 time: 0.09s
Epoch 45/1000, LR 0.000269
Train loss: 2.0372;  Loss pred: 2.0372; Loss self: 0.0000; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6922 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6919 score: 0.5102 time: 0.08s
Epoch 46/1000, LR 0.000269
Train loss: 2.0142;  Loss pred: 2.0142; Loss self: 0.0000; time: 0.29s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6921 score: 0.4898 time: 2.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6918 score: 0.5102 time: 1.56s
Epoch 47/1000, LR 0.000269
Train loss: 1.9950;  Loss pred: 1.9950; Loss self: 0.0000; time: 2.11s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6921 score: 0.4898 time: 0.11s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6917 score: 0.5102 time: 0.11s
Epoch 48/1000, LR 0.000269
Train loss: 1.9477;  Loss pred: 1.9477; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6920 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6916 score: 0.5102 time: 0.09s
Epoch 49/1000, LR 0.000269
Train loss: 1.9100;  Loss pred: 1.9100; Loss self: 0.0000; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6920 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6916 score: 0.5102 time: 0.09s
Epoch 50/1000, LR 0.000269
Train loss: 1.9046;  Loss pred: 1.9046; Loss self: 0.0000; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6919 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6915 score: 0.5102 time: 0.09s
Epoch 51/1000, LR 0.000269
Train loss: 1.8775;  Loss pred: 1.8775; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6919 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6914 score: 0.5102 time: 0.09s
Epoch 52/1000, LR 0.000269
Train loss: 1.8560;  Loss pred: 1.8560; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6918 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6913 score: 0.5102 time: 0.09s
Epoch 53/1000, LR 0.000269
Train loss: 1.8220;  Loss pred: 1.8220; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6918 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6912 score: 0.5102 time: 0.09s
Epoch 54/1000, LR 0.000269
Train loss: 1.8121;  Loss pred: 1.8121; Loss self: 0.0000; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6917 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6911 score: 0.5102 time: 0.08s
Epoch 55/1000, LR 0.000269
Train loss: 1.7692;  Loss pred: 1.7692; Loss self: 0.0000; time: 2.41s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6916 score: 0.4898 time: 0.88s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6910 score: 0.5102 time: 0.62s
Epoch 56/1000, LR 0.000269
Train loss: 1.7624;  Loss pred: 1.7624; Loss self: 0.0000; time: 0.56s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6916 score: 0.4898 time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6909 score: 0.5102 time: 0.40s
Epoch 57/1000, LR 0.000269
Train loss: 1.7450;  Loss pred: 1.7450; Loss self: 0.0000; time: 0.92s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6915 score: 0.4898 time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6908 score: 0.5102 time: 0.11s
Epoch 58/1000, LR 0.000269
Train loss: 1.7322;  Loss pred: 1.7322; Loss self: 0.0000; time: 0.38s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6914 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6907 score: 0.5102 time: 0.10s
Epoch 59/1000, LR 0.000268
Train loss: 1.6856;  Loss pred: 1.6856; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6913 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6906 score: 0.5102 time: 0.09s
Epoch 60/1000, LR 0.000268
Train loss: 1.6739;  Loss pred: 1.6739; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6913 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6905 score: 0.5102 time: 0.09s
Epoch 61/1000, LR 0.000268
Train loss: 1.6524;  Loss pred: 1.6524; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6912 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6904 score: 0.5102 time: 0.09s
Epoch 62/1000, LR 0.000268
Train loss: 1.6307;  Loss pred: 1.6307; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6911 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6903 score: 0.5102 time: 0.09s
Epoch 63/1000, LR 0.000268
Train loss: 1.6160;  Loss pred: 1.6160; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6910 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6902 score: 0.5102 time: 0.09s
Epoch 64/1000, LR 0.000268
Train loss: 1.6053;  Loss pred: 1.6053; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6909 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6900 score: 0.5102 time: 0.09s
Epoch 65/1000, LR 0.000268
Train loss: 1.5941;  Loss pred: 1.5941; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6908 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6899 score: 0.5102 time: 0.09s
Epoch 66/1000, LR 0.000268
Train loss: 1.5629;  Loss pred: 1.5629; Loss self: 0.0000; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6907 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6898 score: 0.5102 time: 0.11s
Epoch 67/1000, LR 0.000268
Train loss: 1.5452;  Loss pred: 1.5452; Loss self: 0.0000; time: 1.61s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6906 score: 0.4898 time: 0.29s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6896 score: 0.5102 time: 0.52s
Epoch 68/1000, LR 0.000268
Train loss: 1.5511;  Loss pred: 1.5511; Loss self: 0.0000; time: 3.95s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6905 score: 0.4898 time: 1.02s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6894 score: 0.5102 time: 0.70s
Epoch 69/1000, LR 0.000268
Train loss: 1.5295;  Loss pred: 1.5295; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6903 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6892 score: 0.5102 time: 0.08s
Epoch 70/1000, LR 0.000268
Train loss: 1.5092;  Loss pred: 1.5092; Loss self: 0.0000; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6902 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6890 score: 0.5102 time: 0.08s
Epoch 71/1000, LR 0.000268
Train loss: 1.4876;  Loss pred: 1.4876; Loss self: 0.0000; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6900 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6888 score: 0.5102 time: 0.08s
Epoch 72/1000, LR 0.000267
Train loss: 1.4808;  Loss pred: 1.4808; Loss self: 0.0000; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6898 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6886 score: 0.5102 time: 0.08s
Epoch 73/1000, LR 0.000267
Train loss: 1.4607;  Loss pred: 1.4607; Loss self: 0.0000; time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6896 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6884 score: 0.5102 time: 0.08s
Epoch 74/1000, LR 0.000267
Train loss: 1.4619;  Loss pred: 1.4619; Loss self: 0.0000; time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6894 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6881 score: 0.5102 time: 0.08s
Epoch 75/1000, LR 0.000267
Train loss: 1.4450;  Loss pred: 1.4450; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6893 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6878 score: 0.5102 time: 0.08s
Epoch 76/1000, LR 0.000267
Train loss: 1.4274;  Loss pred: 1.4274; Loss self: 0.0000; time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6891 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6876 score: 0.5102 time: 0.08s
Epoch 77/1000, LR 0.000267
Train loss: 1.4107;  Loss pred: 1.4107; Loss self: 0.0000; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6890 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6873 score: 0.5102 time: 0.08s
Epoch 78/1000, LR 0.000267
Train loss: 1.4050;  Loss pred: 1.4050; Loss self: 0.0000; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6888 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6871 score: 0.5102 time: 0.08s
Epoch 79/1000, LR 0.000267
Train loss: 1.3845;  Loss pred: 1.3845; Loss self: 0.0000; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6886 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6869 score: 0.5102 time: 0.08s
Epoch 80/1000, LR 0.000267
Train loss: 1.3880;  Loss pred: 1.3880; Loss self: 0.0000; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6884 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6866 score: 0.5102 time: 0.08s
Epoch 81/1000, LR 0.000267
Train loss: 1.3686;  Loss pred: 1.3686; Loss self: 0.0000; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6882 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6863 score: 0.5102 time: 0.08s
Epoch 82/1000, LR 0.000267
Train loss: 1.3594;  Loss pred: 1.3594; Loss self: 0.0000; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6880 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6861 score: 0.5102 time: 0.08s
Epoch 83/1000, LR 0.000266
Train loss: 1.3484;  Loss pred: 1.3484; Loss self: 0.0000; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6878 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6858 score: 0.5102 time: 0.08s
Epoch 84/1000, LR 0.000266
Train loss: 1.3409;  Loss pred: 1.3409; Loss self: 0.0000; time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6876 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6855 score: 0.5102 time: 0.08s
Epoch 85/1000, LR 0.000266
Train loss: 1.3208;  Loss pred: 1.3208; Loss self: 0.0000; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6874 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6852 score: 0.5102 time: 0.08s
Epoch 86/1000, LR 0.000266
Train loss: 1.3221;  Loss pred: 1.3221; Loss self: 0.0000; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6872 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6849 score: 0.5102 time: 0.08s
Epoch 87/1000, LR 0.000266
Train loss: 1.3079;  Loss pred: 1.3079; Loss self: 0.0000; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6869 score: 0.4898 time: 0.07s
Test loss: 0.6846 score: 0.5306 time: 0.08s
Epoch 88/1000, LR 0.000266
Train loss: 1.2968;  Loss pred: 1.2968; Loss self: 0.0000; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6867 score: 0.4898 time: 0.07s
Test loss: 0.6843 score: 0.5306 time: 0.08s
Epoch 89/1000, LR 0.000266
Train loss: 1.2948;  Loss pred: 1.2948; Loss self: 0.0000; time: 0.24s
Val loss: 0.6864 score: 0.5510 time: 0.07s
Test loss: 0.6839 score: 0.5714 time: 0.08s
Epoch 90/1000, LR 0.000266
Train loss: 1.2878;  Loss pred: 1.2878; Loss self: 0.0000; time: 0.24s
Val loss: 0.6861 score: 0.5510 time: 0.07s
Test loss: 0.6836 score: 0.6122 time: 0.09s
Epoch 91/1000, LR 0.000266
Train loss: 1.2661;  Loss pred: 1.2661; Loss self: 0.0000; time: 5.67s
Val loss: 0.6858 score: 0.5714 time: 0.58s
Test loss: 0.6832 score: 0.6939 time: 1.58s
Epoch 92/1000, LR 0.000266
Train loss: 1.2650;  Loss pred: 1.2650; Loss self: 0.0000; time: 0.43s
Val loss: 0.6856 score: 0.5918 time: 0.07s
Test loss: 0.6828 score: 0.7551 time: 0.08s
Epoch 93/1000, LR 0.000265
Train loss: 1.2601;  Loss pred: 1.2601; Loss self: 0.0000; time: 0.22s
Val loss: 0.6853 score: 0.6531 time: 0.07s
Test loss: 0.6824 score: 0.7755 time: 0.08s
Epoch 94/1000, LR 0.000265
Train loss: 1.2506;  Loss pred: 1.2506; Loss self: 0.0000; time: 0.24s
Val loss: 0.6850 score: 0.6531 time: 0.07s
Test loss: 0.6820 score: 0.7755 time: 0.08s
Epoch 95/1000, LR 0.000265
Train loss: 1.2422;  Loss pred: 1.2422; Loss self: 0.0000; time: 0.24s
Val loss: 0.6847 score: 0.6531 time: 0.07s
Test loss: 0.6816 score: 0.8163 time: 0.08s
Epoch 96/1000, LR 0.000265
Train loss: 1.2410;  Loss pred: 1.2410; Loss self: 0.0000; time: 0.24s
Val loss: 0.6843 score: 0.6939 time: 0.07s
Test loss: 0.6812 score: 0.8163 time: 0.09s
Epoch 97/1000, LR 0.000265
Train loss: 1.2251;  Loss pred: 1.2251; Loss self: 0.0000; time: 0.26s
Val loss: 0.6840 score: 0.7143 time: 0.08s
Test loss: 0.6808 score: 0.8367 time: 0.09s
Epoch 98/1000, LR 0.000265
Train loss: 1.2209;  Loss pred: 1.2209; Loss self: 0.0000; time: 0.24s
Val loss: 0.6837 score: 0.7143 time: 0.07s
Test loss: 0.6803 score: 0.8571 time: 0.08s
Epoch 99/1000, LR 0.000265
Train loss: 1.2127;  Loss pred: 1.2127; Loss self: 0.0000; time: 0.23s
Val loss: 0.6833 score: 0.7755 time: 0.07s
Test loss: 0.6799 score: 0.8571 time: 0.08s
Epoch 100/1000, LR 0.000265
Train loss: 1.2189;  Loss pred: 1.2189; Loss self: 0.0000; time: 0.24s
Val loss: 0.6829 score: 0.8163 time: 0.07s
Test loss: 0.6794 score: 0.8571 time: 0.08s
Epoch 101/1000, LR 0.000265
Train loss: 1.2011;  Loss pred: 1.2011; Loss self: 0.0000; time: 0.24s
Val loss: 0.6826 score: 0.8163 time: 0.07s
Test loss: 0.6789 score: 0.8571 time: 0.09s
Epoch 102/1000, LR 0.000264
Train loss: 1.1900;  Loss pred: 1.1900; Loss self: 0.0000; time: 0.25s
Val loss: 0.6822 score: 0.8163 time: 0.08s
Test loss: 0.6784 score: 0.8980 time: 0.09s
Epoch 103/1000, LR 0.000264
Train loss: 1.1994;  Loss pred: 1.1994; Loss self: 0.0000; time: 0.25s
Val loss: 0.6818 score: 0.8163 time: 0.07s
Test loss: 0.6779 score: 0.9388 time: 0.10s
Epoch 104/1000, LR 0.000264
Train loss: 1.1919;  Loss pred: 1.1919; Loss self: 0.0000; time: 0.26s
Val loss: 0.6814 score: 0.8163 time: 0.07s
Test loss: 0.6774 score: 0.9388 time: 0.09s
Epoch 105/1000, LR 0.000264
Train loss: 1.1782;  Loss pred: 1.1782; Loss self: 0.0000; time: 0.25s
Val loss: 0.6809 score: 0.8163 time: 0.07s
Test loss: 0.6768 score: 0.9388 time: 0.09s
Epoch 106/1000, LR 0.000264
Train loss: 1.1768;  Loss pred: 1.1768; Loss self: 0.0000; time: 0.25s
Val loss: 0.6805 score: 0.8163 time: 0.08s
Test loss: 0.6762 score: 0.9388 time: 0.09s
Epoch 107/1000, LR 0.000264
Train loss: 1.1697;  Loss pred: 1.1697; Loss self: 0.0000; time: 0.25s
Val loss: 0.6801 score: 0.8367 time: 0.07s
Test loss: 0.6757 score: 0.9388 time: 0.09s
Epoch 108/1000, LR 0.000264
Train loss: 1.1611;  Loss pred: 1.1611; Loss self: 0.0000; time: 0.24s
Val loss: 0.6796 score: 0.8367 time: 0.07s
Test loss: 0.6751 score: 0.9388 time: 0.10s
Epoch 109/1000, LR 0.000264
Train loss: 1.1580;  Loss pred: 1.1580; Loss self: 0.0000; time: 0.25s
Val loss: 0.6791 score: 0.8367 time: 0.07s
Test loss: 0.6745 score: 0.9388 time: 0.09s
Epoch 110/1000, LR 0.000263
Train loss: 1.1567;  Loss pred: 1.1567; Loss self: 0.0000; time: 0.25s
Val loss: 0.6786 score: 0.8367 time: 0.07s
Test loss: 0.6738 score: 0.9388 time: 0.09s
Epoch 111/1000, LR 0.000263
Train loss: 1.1476;  Loss pred: 1.1476; Loss self: 0.0000; time: 0.24s
Val loss: 0.6781 score: 0.8367 time: 0.07s
Test loss: 0.6732 score: 0.9388 time: 0.09s
Epoch 112/1000, LR 0.000263
Train loss: 1.1475;  Loss pred: 1.1475; Loss self: 0.0000; time: 0.24s
Val loss: 0.6776 score: 0.8367 time: 0.07s
Test loss: 0.6725 score: 0.9388 time: 0.09s
Epoch 113/1000, LR 0.000263
Train loss: 1.1377;  Loss pred: 1.1377; Loss self: 0.0000; time: 0.25s
Val loss: 0.6771 score: 0.8367 time: 0.07s
Test loss: 0.6718 score: 0.9388 time: 0.09s
Epoch 114/1000, LR 0.000263
Train loss: 1.1355;  Loss pred: 1.1355; Loss self: 0.0000; time: 0.24s
Val loss: 0.6765 score: 0.8367 time: 0.07s
Test loss: 0.6710 score: 0.9388 time: 0.09s
Epoch 115/1000, LR 0.000263
Train loss: 1.1265;  Loss pred: 1.1265; Loss self: 0.0000; time: 0.25s
Val loss: 0.6759 score: 0.8367 time: 0.07s
Test loss: 0.6703 score: 0.9388 time: 0.14s
Epoch 116/1000, LR 0.000263
Train loss: 1.1307;  Loss pred: 1.1307; Loss self: 0.0000; time: 0.25s
Val loss: 0.6753 score: 0.8571 time: 0.07s
Test loss: 0.6695 score: 0.9388 time: 0.09s
Epoch 117/1000, LR 0.000262
Train loss: 1.1225;  Loss pred: 1.1225; Loss self: 0.0000; time: 0.24s
Val loss: 0.6747 score: 0.8571 time: 0.07s
Test loss: 0.6687 score: 0.9388 time: 0.09s
Epoch 118/1000, LR 0.000262
Train loss: 1.1195;  Loss pred: 1.1195; Loss self: 0.0000; time: 3.05s
Val loss: 0.6740 score: 0.8571 time: 2.62s
Test loss: 0.6678 score: 0.9388 time: 2.34s
Epoch 119/1000, LR 0.000262
Train loss: 1.1130;  Loss pred: 1.1130; Loss self: 0.0000; time: 1.61s
Val loss: 0.6732 score: 0.8571 time: 0.13s
Test loss: 0.6668 score: 0.9388 time: 0.29s
Epoch 120/1000, LR 0.000262
Train loss: 1.1128;  Loss pred: 1.1128; Loss self: 0.0000; time: 1.09s
Val loss: 0.6724 score: 0.8571 time: 0.07s
Test loss: 0.6658 score: 0.9388 time: 0.09s
Epoch 121/1000, LR 0.000262
Train loss: 1.1068;  Loss pred: 1.1068; Loss self: 0.0000; time: 0.25s
Val loss: 0.6717 score: 0.8571 time: 0.08s
Test loss: 0.6648 score: 0.9388 time: 0.09s
Epoch 122/1000, LR 0.000262
Train loss: 1.1009;  Loss pred: 1.1009; Loss self: 0.0000; time: 0.25s
Val loss: 0.6709 score: 0.8571 time: 0.07s
Test loss: 0.6637 score: 0.9388 time: 0.09s
Epoch 123/1000, LR 0.000262
Train loss: 1.1000;  Loss pred: 1.1000; Loss self: 0.0000; time: 0.25s
Val loss: 0.6701 score: 0.8571 time: 0.07s
Test loss: 0.6627 score: 0.9388 time: 0.09s
Epoch 124/1000, LR 0.000261
Train loss: 1.0967;  Loss pred: 1.0967; Loss self: 0.0000; time: 0.24s
Val loss: 0.6694 score: 0.8367 time: 0.07s
Test loss: 0.6617 score: 0.9388 time: 0.09s
Epoch 125/1000, LR 0.000261
Train loss: 1.0935;  Loss pred: 1.0935; Loss self: 0.0000; time: 0.25s
Val loss: 0.6687 score: 0.8367 time: 0.07s
Test loss: 0.6606 score: 0.9388 time: 0.09s
Epoch 126/1000, LR 0.000261
Train loss: 1.0869;  Loss pred: 1.0869; Loss self: 0.0000; time: 0.24s
Val loss: 0.6679 score: 0.8367 time: 0.08s
Test loss: 0.6596 score: 0.9388 time: 0.09s
Epoch 127/1000, LR 0.000261
Train loss: 1.0821;  Loss pred: 1.0821; Loss self: 0.0000; time: 0.24s
Val loss: 0.6671 score: 0.8367 time: 0.07s
Test loss: 0.6585 score: 0.9388 time: 0.09s
Epoch 128/1000, LR 0.000261
Train loss: 1.0782;  Loss pred: 1.0782; Loss self: 0.0000; time: 0.25s
Val loss: 0.6662 score: 0.8367 time: 0.07s
Test loss: 0.6574 score: 0.9388 time: 0.09s
Epoch 129/1000, LR 0.000261
Train loss: 1.0792;  Loss pred: 1.0792; Loss self: 0.0000; time: 0.26s
Val loss: 0.6654 score: 0.8367 time: 1.79s
Test loss: 0.6562 score: 0.9388 time: 1.61s
Epoch 130/1000, LR 0.000260
Train loss: 1.0693;  Loss pred: 1.0693; Loss self: 0.0000; time: 0.94s
Val loss: 0.6645 score: 0.8367 time: 0.10s
Test loss: 0.6551 score: 0.9388 time: 0.09s
Epoch 131/1000, LR 0.000260
Train loss: 1.0698;  Loss pred: 1.0698; Loss self: 0.0000; time: 0.24s
Val loss: 0.6636 score: 0.8367 time: 0.07s
Test loss: 0.6539 score: 0.9388 time: 0.09s
Epoch 132/1000, LR 0.000260
Train loss: 1.0637;  Loss pred: 1.0637; Loss self: 0.0000; time: 0.24s
Val loss: 0.6627 score: 0.8367 time: 0.07s
Test loss: 0.6526 score: 0.9592 time: 0.08s
Epoch 133/1000, LR 0.000260
Train loss: 1.0627;  Loss pred: 1.0627; Loss self: 0.0000; time: 0.24s
Val loss: 0.6617 score: 0.8571 time: 0.07s
Test loss: 0.6514 score: 0.9592 time: 0.08s
Epoch 134/1000, LR 0.000260
Train loss: 1.0618;  Loss pred: 1.0618; Loss self: 0.0000; time: 0.23s
Val loss: 0.6607 score: 0.8571 time: 0.07s
Test loss: 0.6501 score: 0.9592 time: 0.08s
Epoch 135/1000, LR 0.000260
Train loss: 1.0557;  Loss pred: 1.0557; Loss self: 0.0000; time: 0.23s
Val loss: 0.6597 score: 0.8571 time: 0.07s
Test loss: 0.6487 score: 0.9592 time: 0.08s
Epoch 136/1000, LR 0.000260
Train loss: 1.0552;  Loss pred: 1.0552; Loss self: 0.0000; time: 0.23s
Val loss: 0.6587 score: 0.8571 time: 0.07s
Test loss: 0.6474 score: 0.9592 time: 0.08s
Epoch 137/1000, LR 0.000259
Train loss: 1.0466;  Loss pred: 1.0466; Loss self: 0.0000; time: 0.23s
Val loss: 0.6577 score: 0.8571 time: 0.07s
Test loss: 0.6460 score: 0.9796 time: 0.08s
Epoch 138/1000, LR 0.000259
Train loss: 1.0482;  Loss pred: 1.0482; Loss self: 0.0000; time: 0.23s
Val loss: 0.6566 score: 0.8776 time: 0.07s
Test loss: 0.6446 score: 0.9796 time: 0.08s
Epoch 139/1000, LR 0.000259
Train loss: 1.0431;  Loss pred: 1.0431; Loss self: 0.0000; time: 0.24s
Val loss: 0.6555 score: 0.8980 time: 0.07s
Test loss: 0.6431 score: 0.9796 time: 0.08s
Epoch 140/1000, LR 0.000259
Train loss: 1.0412;  Loss pred: 1.0412; Loss self: 0.0000; time: 0.23s
Val loss: 0.6544 score: 0.8980 time: 0.07s
Test loss: 0.6417 score: 0.9796 time: 0.09s
Epoch 141/1000, LR 0.000259
Train loss: 1.0335;  Loss pred: 1.0335; Loss self: 0.0000; time: 0.24s
Val loss: 0.6532 score: 0.8980 time: 0.07s
Test loss: 0.6401 score: 0.9796 time: 0.09s
Epoch 142/1000, LR 0.000259
Train loss: 1.0332;  Loss pred: 1.0332; Loss self: 0.0000; time: 0.25s
Val loss: 0.6520 score: 0.8980 time: 0.07s
Test loss: 0.6386 score: 0.9796 time: 0.09s
Epoch 143/1000, LR 0.000258
Train loss: 1.0310;  Loss pred: 1.0310; Loss self: 0.0000; time: 0.25s
Val loss: 0.6508 score: 0.8980 time: 0.07s
Test loss: 0.6370 score: 0.9796 time: 0.09s
Epoch 144/1000, LR 0.000258
Train loss: 1.0275;  Loss pred: 1.0275; Loss self: 0.0000; time: 0.25s
Val loss: 0.6496 score: 0.8980 time: 0.07s
Test loss: 0.6354 score: 0.9796 time: 0.09s
Epoch 145/1000, LR 0.000258
Train loss: 1.0251;  Loss pred: 1.0251; Loss self: 0.0000; time: 0.25s
Val loss: 0.6484 score: 0.8980 time: 0.07s
Test loss: 0.6337 score: 0.9796 time: 0.09s
Epoch 146/1000, LR 0.000258
Train loss: 1.0225;  Loss pred: 1.0225; Loss self: 0.0000; time: 0.25s
Val loss: 0.6471 score: 0.8980 time: 0.07s
Test loss: 0.6320 score: 0.9796 time: 0.09s
Epoch 147/1000, LR 0.000258
Train loss: 1.0210;  Loss pred: 1.0210; Loss self: 0.0000; time: 0.26s
Val loss: 0.6458 score: 0.8776 time: 0.08s
Test loss: 0.6303 score: 0.9796 time: 0.09s
Epoch 148/1000, LR 0.000257
Train loss: 1.0181;  Loss pred: 1.0181; Loss self: 0.0000; time: 0.27s
Val loss: 0.6445 score: 0.8776 time: 0.08s
Test loss: 0.6285 score: 0.9796 time: 0.10s
Epoch 149/1000, LR 0.000257
Train loss: 1.0128;  Loss pred: 1.0128; Loss self: 0.0000; time: 0.25s
Val loss: 0.6432 score: 0.8776 time: 0.07s
Test loss: 0.6267 score: 0.9796 time: 0.09s
Epoch 150/1000, LR 0.000257
Train loss: 1.0116;  Loss pred: 1.0116; Loss self: 0.0000; time: 0.26s
Val loss: 0.6418 score: 0.8571 time: 0.07s
Test loss: 0.6249 score: 0.9796 time: 0.09s
Epoch 151/1000, LR 0.000257
Train loss: 1.0022;  Loss pred: 1.0022; Loss self: 0.0000; time: 0.26s
Val loss: 0.6404 score: 0.8571 time: 1.59s
Test loss: 0.6231 score: 0.9796 time: 0.76s
Epoch 152/1000, LR 0.000257
Train loss: 1.0074;  Loss pred: 1.0074; Loss self: 0.0000; time: 4.58s
Val loss: 0.6390 score: 0.8571 time: 0.28s
Test loss: 0.6212 score: 0.9796 time: 1.46s
Epoch 153/1000, LR 0.000257
Train loss: 1.0061;  Loss pred: 1.0061; Loss self: 0.0000; time: 2.58s
Val loss: 0.6375 score: 0.8571 time: 0.07s
Test loss: 0.6193 score: 0.9796 time: 0.09s
Epoch 154/1000, LR 0.000256
Train loss: 1.0012;  Loss pred: 1.0012; Loss self: 0.0000; time: 0.25s
Val loss: 0.6360 score: 0.8571 time: 0.07s
Test loss: 0.6173 score: 0.9796 time: 0.08s
Epoch 155/1000, LR 0.000256
Train loss: 0.9997;  Loss pred: 0.9997; Loss self: 0.0000; time: 0.23s
Val loss: 0.6345 score: 0.8776 time: 0.07s
Test loss: 0.6153 score: 0.9796 time: 0.08s
Epoch 156/1000, LR 0.000256
Train loss: 0.9937;  Loss pred: 0.9937; Loss self: 0.0000; time: 0.23s
Val loss: 0.6329 score: 0.8571 time: 0.07s
Test loss: 0.6133 score: 0.9796 time: 0.08s
Epoch 157/1000, LR 0.000256
Train loss: 0.9923;  Loss pred: 0.9923; Loss self: 0.0000; time: 0.23s
Val loss: 0.6314 score: 0.8571 time: 0.06s
Test loss: 0.6112 score: 0.9796 time: 0.08s
Epoch 158/1000, LR 0.000256
Train loss: 0.9892;  Loss pred: 0.9892; Loss self: 0.0000; time: 0.23s
Val loss: 0.6298 score: 0.8571 time: 0.07s
Test loss: 0.6091 score: 0.9796 time: 0.08s
Epoch 159/1000, LR 0.000255
Train loss: 0.9862;  Loss pred: 0.9862; Loss self: 0.0000; time: 0.24s
Val loss: 0.6282 score: 0.8571 time: 0.07s
Test loss: 0.6069 score: 0.9796 time: 0.08s
Epoch 160/1000, LR 0.000255
Train loss: 0.9819;  Loss pred: 0.9819; Loss self: 0.0000; time: 0.23s
Val loss: 0.6266 score: 0.8571 time: 0.07s
Test loss: 0.6047 score: 0.9796 time: 0.08s
Epoch 161/1000, LR 0.000255
Train loss: 0.9788;  Loss pred: 0.9788; Loss self: 0.0000; time: 0.23s
Val loss: 0.6249 score: 0.8571 time: 0.06s
Test loss: 0.6025 score: 0.9796 time: 0.08s
Epoch 162/1000, LR 0.000255
Train loss: 0.9770;  Loss pred: 0.9770; Loss self: 0.0000; time: 0.23s
Val loss: 0.6232 score: 0.8571 time: 0.07s
Test loss: 0.6002 score: 0.9796 time: 0.08s
Epoch 163/1000, LR 0.000255
Train loss: 0.9731;  Loss pred: 0.9731; Loss self: 0.0000; time: 0.23s
Val loss: 0.6215 score: 0.8571 time: 0.07s
Test loss: 0.5979 score: 0.9796 time: 0.08s
Epoch 164/1000, LR 0.000254
Train loss: 0.9677;  Loss pred: 0.9677; Loss self: 0.0000; time: 0.24s
Val loss: 0.6197 score: 0.8571 time: 0.07s
Test loss: 0.5956 score: 0.9796 time: 0.08s
Epoch 165/1000, LR 0.000254
Train loss: 0.9661;  Loss pred: 0.9661; Loss self: 0.0000; time: 0.24s
Val loss: 0.6179 score: 0.8571 time: 0.07s
Test loss: 0.5932 score: 0.9796 time: 0.08s
Epoch 166/1000, LR 0.000254
Train loss: 0.9637;  Loss pred: 0.9637; Loss self: 0.0000; time: 0.23s
Val loss: 0.6161 score: 0.8571 time: 0.07s
Test loss: 0.5908 score: 0.9796 time: 0.08s
Epoch 167/1000, LR 0.000254
Train loss: 0.9615;  Loss pred: 0.9615; Loss self: 0.0000; time: 0.23s
Val loss: 0.6143 score: 0.8571 time: 0.06s
Test loss: 0.5883 score: 0.9796 time: 0.08s
Epoch 168/1000, LR 0.000254
Train loss: 0.9609;  Loss pred: 0.9609; Loss self: 0.0000; time: 0.77s
Val loss: 0.6124 score: 0.8571 time: 1.33s
Test loss: 0.5859 score: 0.9796 time: 1.94s
Epoch 169/1000, LR 0.000253
Train loss: 0.9568;  Loss pred: 0.9568; Loss self: 0.0000; time: 1.43s
Val loss: 0.6105 score: 0.8571 time: 0.11s
Test loss: 0.5833 score: 0.9796 time: 0.12s
Epoch 170/1000, LR 0.000253
Train loss: 0.9521;  Loss pred: 0.9521; Loss self: 0.0000; time: 0.24s
Val loss: 0.6086 score: 0.8571 time: 0.07s
Test loss: 0.5808 score: 0.9796 time: 0.08s
Epoch 171/1000, LR 0.000253
Train loss: 0.9504;  Loss pred: 0.9504; Loss self: 0.0000; time: 0.24s
Val loss: 0.6066 score: 0.8571 time: 0.07s
Test loss: 0.5782 score: 0.9796 time: 0.08s
Epoch 172/1000, LR 0.000253
Train loss: 0.9478;  Loss pred: 0.9478; Loss self: 0.0000; time: 0.23s
Val loss: 0.6046 score: 0.8571 time: 0.07s
Test loss: 0.5755 score: 0.9796 time: 0.09s
Epoch 173/1000, LR 0.000253
Train loss: 0.9463;  Loss pred: 0.9463; Loss self: 0.0000; time: 0.23s
Val loss: 0.6026 score: 0.8571 time: 0.07s
Test loss: 0.5729 score: 0.9796 time: 0.08s
Epoch 174/1000, LR 0.000252
Train loss: 0.9417;  Loss pred: 0.9417; Loss self: 0.0000; time: 0.24s
Val loss: 0.6006 score: 0.8571 time: 0.07s
Test loss: 0.5701 score: 0.9796 time: 0.08s
Epoch 175/1000, LR 0.000252
Train loss: 0.9391;  Loss pred: 0.9391; Loss self: 0.0000; time: 0.24s
Val loss: 0.5985 score: 0.8571 time: 0.07s
Test loss: 0.5674 score: 0.9796 time: 0.08s
Epoch 176/1000, LR 0.000252
Train loss: 0.9360;  Loss pred: 0.9360; Loss self: 0.0000; time: 0.23s
Val loss: 0.5964 score: 0.8571 time: 0.07s
Test loss: 0.5645 score: 0.9796 time: 0.08s
Epoch 177/1000, LR 0.000252
Train loss: 0.9326;  Loss pred: 0.9326; Loss self: 0.0000; time: 0.24s
Val loss: 0.5943 score: 0.8571 time: 0.07s
Test loss: 0.5616 score: 0.9796 time: 0.08s
Epoch 178/1000, LR 0.000251
Train loss: 0.9290;  Loss pred: 0.9290; Loss self: 0.0000; time: 0.24s
Val loss: 0.5921 score: 0.8571 time: 0.74s
Test loss: 0.5587 score: 0.9796 time: 1.58s
Epoch 179/1000, LR 0.000251
Train loss: 0.9268;  Loss pred: 0.9268; Loss self: 0.0000; time: 2.95s
Val loss: 0.5899 score: 0.8571 time: 0.10s
Test loss: 0.5557 score: 0.9796 time: 0.09s
Epoch 180/1000, LR 0.000251
Train loss: 0.9265;  Loss pred: 0.9265; Loss self: 0.0000; time: 0.23s
Val loss: 0.5876 score: 0.8571 time: 0.07s
Test loss: 0.5527 score: 0.9796 time: 0.08s
Epoch 181/1000, LR 0.000251
Train loss: 0.9206;  Loss pred: 0.9206; Loss self: 0.0000; time: 0.23s
Val loss: 0.5852 score: 0.8571 time: 0.07s
Test loss: 0.5496 score: 0.9796 time: 0.08s
Epoch 182/1000, LR 0.000251
Train loss: 0.9176;  Loss pred: 0.9176; Loss self: 0.0000; time: 0.23s
Val loss: 0.5829 score: 0.8571 time: 0.07s
Test loss: 0.5465 score: 0.9796 time: 0.08s
Epoch 183/1000, LR 0.000250
Train loss: 0.9166;  Loss pred: 0.9166; Loss self: 0.0000; time: 0.25s
Val loss: 0.5805 score: 0.8571 time: 0.07s
Test loss: 0.5434 score: 0.9796 time: 0.08s
Epoch 184/1000, LR 0.000250
Train loss: 0.9132;  Loss pred: 0.9132; Loss self: 0.0000; time: 0.25s
Val loss: 0.5782 score: 0.8571 time: 0.07s
Test loss: 0.5403 score: 0.9796 time: 0.09s
Epoch 185/1000, LR 0.000250
Train loss: 0.9116;  Loss pred: 0.9116; Loss self: 0.0000; time: 0.24s
Val loss: 0.5759 score: 0.8571 time: 0.08s
Test loss: 0.5371 score: 0.9796 time: 0.09s
Epoch 186/1000, LR 0.000250
Train loss: 0.9066;  Loss pred: 0.9066; Loss self: 0.0000; time: 0.25s
Val loss: 0.5735 score: 0.8571 time: 0.08s
Test loss: 0.5339 score: 0.9796 time: 0.09s
Epoch 187/1000, LR 0.000249
Train loss: 0.9065;  Loss pred: 0.9065; Loss self: 0.0000; time: 0.25s
Val loss: 0.5712 score: 0.8571 time: 0.07s
Test loss: 0.5307 score: 0.9796 time: 0.09s
Epoch 188/1000, LR 0.000249
Train loss: 0.9014;  Loss pred: 0.9014; Loss self: 0.0000; time: 0.24s
Val loss: 0.5689 score: 0.8571 time: 0.07s
Test loss: 0.5275 score: 0.9796 time: 0.08s
Epoch 189/1000, LR 0.000249
Train loss: 0.8982;  Loss pred: 0.8982; Loss self: 0.0000; time: 0.23s
Val loss: 0.5666 score: 0.8571 time: 0.07s
Test loss: 0.5242 score: 0.9796 time: 0.08s
Epoch 190/1000, LR 0.000249
Train loss: 0.8972;  Loss pred: 0.8972; Loss self: 0.0000; time: 0.24s
Val loss: 0.5642 score: 0.8571 time: 0.07s
Test loss: 0.5210 score: 0.9796 time: 0.08s
Epoch 191/1000, LR 0.000249
Train loss: 0.8912;  Loss pred: 0.8912; Loss self: 0.0000; time: 0.26s
Val loss: 0.5619 score: 0.8571 time: 0.07s
Test loss: 0.5177 score: 0.9796 time: 0.08s
Epoch 192/1000, LR 0.000248
Train loss: 0.8903;  Loss pred: 0.8903; Loss self: 0.0000; time: 0.22s
Val loss: 0.5595 score: 0.8571 time: 0.07s
Test loss: 0.5144 score: 0.9796 time: 0.08s
Epoch 193/1000, LR 0.000248
Train loss: 0.8842;  Loss pred: 0.8842; Loss self: 0.0000; time: 0.23s
Val loss: 0.5571 score: 0.8571 time: 0.06s
Test loss: 0.5110 score: 0.9796 time: 0.08s
Epoch 194/1000, LR 0.000248
Train loss: 0.8806;  Loss pred: 0.8806; Loss self: 0.0000; time: 0.23s
Val loss: 0.5547 score: 0.8571 time: 0.06s
Test loss: 0.5077 score: 0.9796 time: 0.08s
Epoch 195/1000, LR 0.000248
Train loss: 0.8800;  Loss pred: 0.8800; Loss self: 0.0000; time: 0.23s
Val loss: 0.5522 score: 0.8571 time: 0.07s
Test loss: 0.5043 score: 0.9796 time: 0.08s
Epoch 196/1000, LR 0.000247
Train loss: 0.8779;  Loss pred: 0.8779; Loss self: 0.0000; time: 0.22s
Val loss: 0.5497 score: 0.8571 time: 0.06s
Test loss: 0.5009 score: 0.9796 time: 0.08s
Epoch 197/1000, LR 0.000247
Train loss: 0.8748;  Loss pred: 0.8748; Loss self: 0.0000; time: 0.23s
Val loss: 0.5471 score: 0.8571 time: 0.07s
Test loss: 0.4975 score: 0.9796 time: 0.08s
Epoch 198/1000, LR 0.000247
Train loss: 0.8715;  Loss pred: 0.8715; Loss self: 0.0000; time: 0.25s
Val loss: 0.5445 score: 0.8571 time: 0.06s
Test loss: 0.4941 score: 0.9796 time: 0.08s
Epoch 199/1000, LR 0.000247
Train loss: 0.8689;  Loss pred: 0.8689; Loss self: 0.0000; time: 0.22s
Val loss: 0.5419 score: 0.8571 time: 0.06s
Test loss: 0.4907 score: 0.9796 time: 0.08s
Epoch 200/1000, LR 0.000246
Train loss: 0.8661;  Loss pred: 0.8661; Loss self: 0.0000; time: 0.22s
Val loss: 0.5394 score: 0.8571 time: 0.06s
Test loss: 0.4872 score: 0.9796 time: 0.08s
Epoch 201/1000, LR 0.000246
Train loss: 0.8616;  Loss pred: 0.8616; Loss self: 0.0000; time: 0.23s
Val loss: 0.5368 score: 0.8571 time: 0.07s
Test loss: 0.4837 score: 0.9796 time: 0.08s
Epoch 202/1000, LR 0.000246
Train loss: 0.8600;  Loss pred: 0.8600; Loss self: 0.0000; time: 0.22s
Val loss: 0.5343 score: 0.8571 time: 0.07s
Test loss: 0.4803 score: 0.9796 time: 0.08s
Epoch 203/1000, LR 0.000246
Train loss: 0.8569;  Loss pred: 0.8569; Loss self: 0.0000; time: 0.22s
Val loss: 0.5318 score: 0.8571 time: 0.06s
Test loss: 0.4768 score: 0.9796 time: 0.08s
Epoch 204/1000, LR 0.000245
Train loss: 0.8531;  Loss pred: 0.8531; Loss self: 0.0000; time: 0.22s
Val loss: 0.5293 score: 0.8571 time: 0.06s
Test loss: 0.4733 score: 0.9796 time: 0.08s
Epoch 205/1000, LR 0.000245
Train loss: 0.8512;  Loss pred: 0.8512; Loss self: 0.0000; time: 0.22s
Val loss: 0.5269 score: 0.8571 time: 0.06s
Test loss: 0.4698 score: 0.9796 time: 0.08s
Epoch 206/1000, LR 0.000245
Train loss: 0.8506;  Loss pred: 0.8506; Loss self: 0.0000; time: 0.23s
Val loss: 0.5244 score: 0.8571 time: 0.07s
Test loss: 0.4662 score: 0.9796 time: 0.08s
Epoch 207/1000, LR 0.000245
Train loss: 0.8447;  Loss pred: 0.8447; Loss self: 0.0000; time: 0.25s
Val loss: 0.5219 score: 0.8571 time: 0.09s
Test loss: 0.4627 score: 0.9796 time: 0.11s
Epoch 208/1000, LR 0.000244
Train loss: 0.8430;  Loss pred: 0.8430; Loss self: 0.0000; time: 0.25s
Val loss: 0.5195 score: 0.8571 time: 0.06s
Test loss: 0.4592 score: 0.9796 time: 0.08s
Epoch 209/1000, LR 0.000244
Train loss: 0.8390;  Loss pred: 0.8390; Loss self: 0.0000; time: 5.66s
Val loss: 0.5171 score: 0.8571 time: 0.52s
Test loss: 0.4557 score: 0.9796 time: 0.26s
Epoch 210/1000, LR 0.000244
Train loss: 0.8374;  Loss pred: 0.8374; Loss self: 0.0000; time: 0.61s
Val loss: 0.5146 score: 0.8571 time: 0.07s
Test loss: 0.4522 score: 0.9796 time: 0.09s
Epoch 211/1000, LR 0.000244
Train loss: 0.8341;  Loss pred: 0.8341; Loss self: 0.0000; time: 0.25s
Val loss: 0.5121 score: 0.8571 time: 0.07s
Test loss: 0.4486 score: 0.9796 time: 0.09s
Epoch 212/1000, LR 0.000243
Train loss: 0.8300;  Loss pred: 0.8300; Loss self: 0.0000; time: 0.25s
Val loss: 0.5095 score: 0.8571 time: 0.07s
Test loss: 0.4451 score: 0.9796 time: 0.09s
Epoch 213/1000, LR 0.000243
Train loss: 0.8265;  Loss pred: 0.8265; Loss self: 0.0000; time: 0.25s
Val loss: 0.5068 score: 0.8571 time: 0.07s
Test loss: 0.4415 score: 0.9796 time: 0.09s
Epoch 214/1000, LR 0.000243
Train loss: 0.8227;  Loss pred: 0.8227; Loss self: 0.0000; time: 0.25s
Val loss: 0.5042 score: 0.8571 time: 0.07s
Test loss: 0.4380 score: 0.9796 time: 0.09s
Epoch 215/1000, LR 0.000243
Train loss: 0.8219;  Loss pred: 0.8219; Loss self: 0.0000; time: 0.86s
Val loss: 0.5017 score: 0.8571 time: 0.30s
Test loss: 0.4344 score: 0.9796 time: 1.43s
Epoch 216/1000, LR 0.000242
Train loss: 0.8186;  Loss pred: 0.8186; Loss self: 0.0000; time: 4.59s
Val loss: 0.4991 score: 0.8571 time: 1.03s
Test loss: 0.4309 score: 0.9796 time: 0.57s
Epoch 217/1000, LR 0.000242
Train loss: 0.8166;  Loss pred: 0.8166; Loss self: 0.0000; time: 0.50s
Val loss: 0.4966 score: 0.8571 time: 0.07s
Test loss: 0.4274 score: 0.9796 time: 0.09s
Epoch 218/1000, LR 0.000242
Train loss: 0.8132;  Loss pred: 0.8132; Loss self: 0.0000; time: 0.24s
Val loss: 0.4943 score: 0.8571 time: 0.07s
Test loss: 0.4239 score: 0.9796 time: 0.09s
Epoch 219/1000, LR 0.000242
Train loss: 0.8100;  Loss pred: 0.8100; Loss self: 0.0000; time: 0.24s
Val loss: 0.4920 score: 0.8571 time: 0.07s
Test loss: 0.4204 score: 0.9796 time: 0.09s
Epoch 220/1000, LR 0.000241
Train loss: 0.8076;  Loss pred: 0.8076; Loss self: 0.0000; time: 0.25s
Val loss: 0.4897 score: 0.8571 time: 0.07s
Test loss: 0.4169 score: 0.9796 time: 0.09s
Epoch 221/1000, LR 0.000241
Train loss: 0.8050;  Loss pred: 0.8050; Loss self: 0.0000; time: 0.25s
Val loss: 0.4874 score: 0.8571 time: 0.07s
Test loss: 0.4134 score: 0.9796 time: 0.09s
Epoch 222/1000, LR 0.000241
Train loss: 0.8020;  Loss pred: 0.8020; Loss self: 0.0000; time: 0.24s
Val loss: 0.4849 score: 0.8571 time: 0.07s
Test loss: 0.4099 score: 0.9796 time: 0.09s
Epoch 223/1000, LR 0.000241
Train loss: 0.7974;  Loss pred: 0.7974; Loss self: 0.0000; time: 0.24s
Val loss: 0.4824 score: 0.8571 time: 0.07s
Test loss: 0.4064 score: 0.9796 time: 0.09s
Epoch 224/1000, LR 0.000240
Train loss: 0.7944;  Loss pred: 0.7944; Loss self: 0.0000; time: 0.25s
Val loss: 0.4799 score: 0.8571 time: 0.07s
Test loss: 0.4029 score: 0.9796 time: 0.09s
Epoch 225/1000, LR 0.000240
Train loss: 0.7917;  Loss pred: 0.7917; Loss self: 0.0000; time: 0.24s
Val loss: 0.4774 score: 0.8571 time: 0.07s
Test loss: 0.3995 score: 0.9796 time: 0.09s
Epoch 226/1000, LR 0.000240
Train loss: 0.7918;  Loss pred: 0.7918; Loss self: 0.0000; time: 0.25s
Val loss: 0.4750 score: 0.8571 time: 0.08s
Test loss: 0.3960 score: 0.9796 time: 0.09s
Epoch 227/1000, LR 0.000240
Train loss: 0.7894;  Loss pred: 0.7894; Loss self: 0.0000; time: 0.25s
Val loss: 0.4727 score: 0.8571 time: 0.07s
Test loss: 0.3926 score: 0.9796 time: 0.09s
Epoch 228/1000, LR 0.000239
Train loss: 0.7838;  Loss pred: 0.7838; Loss self: 0.0000; time: 0.25s
Val loss: 0.4706 score: 0.8571 time: 0.07s
Test loss: 0.3893 score: 0.9796 time: 0.09s
Epoch 229/1000, LR 0.000239
Train loss: 0.7832;  Loss pred: 0.7832; Loss self: 0.0000; time: 5.10s
Val loss: 0.4687 score: 0.8571 time: 0.51s
Test loss: 0.3859 score: 0.9796 time: 0.15s
Epoch 230/1000, LR 0.000239
Train loss: 0.7800;  Loss pred: 0.7800; Loss self: 0.0000; time: 0.75s
Val loss: 0.4667 score: 0.8571 time: 0.10s
Test loss: 0.3827 score: 0.9796 time: 0.33s
Epoch 231/1000, LR 0.000238
Train loss: 0.7768;  Loss pred: 0.7768; Loss self: 0.0000; time: 1.36s
Val loss: 0.4645 score: 0.8571 time: 0.07s
Test loss: 0.3793 score: 0.9796 time: 0.09s
Epoch 232/1000, LR 0.000238
Train loss: 0.7758;  Loss pred: 0.7758; Loss self: 0.0000; time: 0.25s
Val loss: 0.4623 score: 0.8571 time: 0.08s
Test loss: 0.3760 score: 0.9796 time: 0.09s
Epoch 233/1000, LR 0.000238
Train loss: 0.7732;  Loss pred: 0.7732; Loss self: 0.0000; time: 0.25s
Val loss: 0.4599 score: 0.8571 time: 0.07s
Test loss: 0.3726 score: 0.9796 time: 0.09s
Epoch 234/1000, LR 0.000238
Train loss: 0.7690;  Loss pred: 0.7690; Loss self: 0.0000; time: 0.25s
Val loss: 0.4576 score: 0.8571 time: 0.07s
Test loss: 0.3693 score: 0.9796 time: 0.09s
Epoch 235/1000, LR 0.000237
Train loss: 0.7680;  Loss pred: 0.7680; Loss self: 0.0000; time: 0.25s
Val loss: 0.4553 score: 0.8571 time: 0.07s
Test loss: 0.3660 score: 0.9796 time: 0.09s
Epoch 236/1000, LR 0.000237
Train loss: 0.7638;  Loss pred: 0.7638; Loss self: 0.0000; time: 0.25s
Val loss: 0.4531 score: 0.8571 time: 0.07s
Test loss: 0.3627 score: 0.9796 time: 0.09s
Epoch 237/1000, LR 0.000237
Train loss: 0.7608;  Loss pred: 0.7608; Loss self: 0.0000; time: 0.24s
Val loss: 0.4510 score: 0.8571 time: 0.08s
Test loss: 0.3595 score: 0.9796 time: 0.09s
Epoch 238/1000, LR 0.000236
Train loss: 0.7585;  Loss pred: 0.7585; Loss self: 0.0000; time: 0.25s
Val loss: 0.4490 score: 0.8571 time: 0.07s
Test loss: 0.3563 score: 0.9796 time: 0.09s
Epoch 239/1000, LR 0.000236
Train loss: 0.7575;  Loss pred: 0.7575; Loss self: 0.0000; time: 0.25s
Val loss: 0.4472 score: 0.8571 time: 0.07s
Test loss: 0.3532 score: 0.9796 time: 0.09s
Epoch 240/1000, LR 0.000236
Train loss: 0.7518;  Loss pred: 0.7518; Loss self: 0.0000; time: 0.25s
Val loss: 0.4454 score: 0.8571 time: 0.07s
Test loss: 0.3502 score: 0.9796 time: 1.16s
Epoch 241/1000, LR 0.000236
Train loss: 0.7519;  Loss pred: 0.7519; Loss self: 0.0000; time: 2.10s
Val loss: 0.4437 score: 0.8571 time: 0.30s
Test loss: 0.3471 score: 0.9796 time: 0.77s
Epoch 242/1000, LR 0.000235
Train loss: 0.7496;  Loss pred: 0.7496; Loss self: 0.0000; time: 1.33s
Val loss: 0.4419 score: 0.8571 time: 0.33s
Test loss: 0.3441 score: 0.9796 time: 1.78s
Epoch 243/1000, LR 0.000235
Train loss: 0.7455;  Loss pred: 0.7455; Loss self: 0.0000; time: 3.68s
Val loss: 0.4399 score: 0.8571 time: 0.16s
Test loss: 0.3410 score: 0.9796 time: 0.35s
Epoch 244/1000, LR 0.000235
Train loss: 0.7455;  Loss pred: 0.7455; Loss self: 0.0000; time: 0.70s
Val loss: 0.4378 score: 0.8571 time: 0.62s
Test loss: 0.3379 score: 0.9796 time: 1.11s
Epoch 245/1000, LR 0.000234
Train loss: 0.7434;  Loss pred: 0.7434; Loss self: 0.0000; time: 0.26s
Val loss: 0.4356 score: 0.8571 time: 0.07s
Test loss: 0.3348 score: 0.9796 time: 0.09s
Epoch 246/1000, LR 0.000234
Train loss: 0.7407;  Loss pred: 0.7407; Loss self: 0.0000; time: 0.25s
Val loss: 0.4333 score: 0.8571 time: 0.07s
Test loss: 0.3317 score: 0.9796 time: 0.09s
Epoch 247/1000, LR 0.000234
Train loss: 0.7365;  Loss pred: 0.7365; Loss self: 0.0000; time: 0.26s
Val loss: 0.4311 score: 0.8571 time: 0.07s
Test loss: 0.3286 score: 0.9796 time: 0.09s
Epoch 248/1000, LR 0.000234
Train loss: 0.7351;  Loss pred: 0.7351; Loss self: 0.0000; time: 0.24s
Val loss: 0.4292 score: 0.8571 time: 0.08s
Test loss: 0.3257 score: 0.9796 time: 0.09s
Epoch 249/1000, LR 0.000233
Train loss: 0.7330;  Loss pred: 0.7330; Loss self: 0.0000; time: 0.24s
Val loss: 0.4274 score: 0.8571 time: 0.07s
Test loss: 0.3228 score: 0.9796 time: 0.08s
Epoch 250/1000, LR 0.000233
Train loss: 0.7311;  Loss pred: 0.7311; Loss self: 0.0000; time: 0.23s
Val loss: 0.4258 score: 0.8571 time: 0.06s
Test loss: 0.3200 score: 0.9796 time: 0.08s
Epoch 251/1000, LR 0.000233
Train loss: 0.7288;  Loss pred: 0.7288; Loss self: 0.0000; time: 0.23s
Val loss: 0.4242 score: 0.8571 time: 0.07s
Test loss: 0.3172 score: 0.9796 time: 0.10s
Epoch 252/1000, LR 0.000232
Train loss: 0.7255;  Loss pred: 0.7255; Loss self: 0.0000; time: 0.24s
Val loss: 0.4227 score: 0.8571 time: 0.07s
Test loss: 0.3144 score: 0.9796 time: 0.08s
Epoch 253/1000, LR 0.000232
Train loss: 0.7241;  Loss pred: 0.7241; Loss self: 0.0000; time: 0.23s
Val loss: 0.4212 score: 0.8571 time: 0.08s
Test loss: 0.3117 score: 0.9796 time: 0.09s
Epoch 254/1000, LR 0.000232
Train loss: 0.7222;  Loss pred: 0.7222; Loss self: 0.0000; time: 0.24s
Val loss: 0.4199 score: 0.8571 time: 0.07s
Test loss: 0.3091 score: 0.9796 time: 0.08s
Epoch 255/1000, LR 0.000232
Train loss: 0.7200;  Loss pred: 0.7200; Loss self: 0.0000; time: 0.24s
Val loss: 0.4183 score: 0.8571 time: 0.07s
Test loss: 0.3064 score: 0.9796 time: 0.08s
Epoch 256/1000, LR 0.000231
Train loss: 0.7177;  Loss pred: 0.7177; Loss self: 0.0000; time: 0.25s
Val loss: 0.4167 score: 0.8571 time: 0.24s
Test loss: 0.3037 score: 0.9796 time: 2.45s
Epoch 257/1000, LR 0.000231
Train loss: 0.7168;  Loss pred: 0.7168; Loss self: 0.0000; time: 3.98s
Val loss: 0.4149 score: 0.8571 time: 0.08s
Test loss: 0.3010 score: 0.9796 time: 0.09s
Epoch 258/1000, LR 0.000231
Train loss: 0.7139;  Loss pred: 0.7139; Loss self: 0.0000; time: 0.25s
Val loss: 0.4132 score: 0.8571 time: 0.07s
Test loss: 0.2983 score: 0.9796 time: 0.08s
Epoch 259/1000, LR 0.000230
Train loss: 0.7119;  Loss pred: 0.7119; Loss self: 0.0000; time: 0.23s
Val loss: 0.4114 score: 0.8571 time: 0.07s
Test loss: 0.2956 score: 0.9796 time: 0.09s
Epoch 260/1000, LR 0.000230
Train loss: 0.7079;  Loss pred: 0.7079; Loss self: 0.0000; time: 0.24s
Val loss: 0.4097 score: 0.8571 time: 0.07s
Test loss: 0.2929 score: 0.9796 time: 0.09s
Epoch 261/1000, LR 0.000230
Train loss: 0.7074;  Loss pred: 0.7074; Loss self: 0.0000; time: 0.24s
Val loss: 0.4078 score: 0.8571 time: 0.08s
Test loss: 0.2903 score: 0.9796 time: 0.09s
Epoch 262/1000, LR 0.000229
Train loss: 0.7059;  Loss pred: 0.7059; Loss self: 0.0000; time: 0.24s
Val loss: 0.4060 score: 0.8571 time: 0.07s
Test loss: 0.2876 score: 0.9796 time: 0.08s
Epoch 263/1000, LR 0.000229
Train loss: 0.7043;  Loss pred: 0.7043; Loss self: 0.0000; time: 0.24s
Val loss: 0.4042 score: 0.8571 time: 0.07s
Test loss: 0.2850 score: 0.9796 time: 0.08s
Epoch 264/1000, LR 0.000229
Train loss: 0.7023;  Loss pred: 0.7023; Loss self: 0.0000; time: 0.24s
Val loss: 0.4025 score: 0.8571 time: 0.07s
Test loss: 0.2825 score: 0.9796 time: 0.08s
Epoch 265/1000, LR 0.000228
Train loss: 0.6991;  Loss pred: 0.6991; Loss self: 0.0000; time: 2.74s
Val loss: 0.4008 score: 0.8571 time: 2.01s
Test loss: 0.2800 score: 0.9796 time: 2.21s
Epoch 266/1000, LR 0.000228
Train loss: 0.6976;  Loss pred: 0.6976; Loss self: 0.0000; time: 1.94s
Val loss: 0.3993 score: 0.8571 time: 0.08s
Test loss: 0.2775 score: 0.9796 time: 0.15s
Epoch 267/1000, LR 0.000228
Train loss: 0.6972;  Loss pred: 0.6972; Loss self: 0.0000; time: 0.24s
Val loss: 0.3981 score: 0.8571 time: 0.07s
Test loss: 0.2752 score: 0.9796 time: 0.08s
Epoch 268/1000, LR 0.000228
Train loss: 0.6939;  Loss pred: 0.6939; Loss self: 0.0000; time: 0.23s
Val loss: 0.3971 score: 0.8571 time: 0.07s
Test loss: 0.2730 score: 0.9796 time: 0.08s
Epoch 269/1000, LR 0.000227
Train loss: 0.6924;  Loss pred: 0.6924; Loss self: 0.0000; time: 0.24s
Val loss: 0.3962 score: 0.8571 time: 0.06s
Test loss: 0.2708 score: 0.9796 time: 0.08s
Epoch 270/1000, LR 0.000227
Train loss: 0.6901;  Loss pred: 0.6901; Loss self: 0.0000; time: 0.23s
Val loss: 0.3953 score: 0.8571 time: 0.06s
Test loss: 0.2686 score: 0.9796 time: 0.08s
Epoch 271/1000, LR 0.000227
Train loss: 0.6893;  Loss pred: 0.6893; Loss self: 0.0000; time: 0.23s
Val loss: 0.3942 score: 0.8571 time: 0.07s
Test loss: 0.2664 score: 0.9796 time: 0.08s
Epoch 272/1000, LR 0.000226
Train loss: 0.6873;  Loss pred: 0.6873; Loss self: 0.0000; time: 0.24s
Val loss: 0.3929 score: 0.8571 time: 0.07s
Test loss: 0.2642 score: 0.9796 time: 0.08s
Epoch 273/1000, LR 0.000226
Train loss: 0.6867;  Loss pred: 0.6867; Loss self: 0.0000; time: 0.24s
Val loss: 0.3915 score: 0.8571 time: 0.07s
Test loss: 0.2620 score: 0.9796 time: 0.08s
Epoch 274/1000, LR 0.000226
Train loss: 0.6837;  Loss pred: 0.6837; Loss self: 0.0000; time: 0.24s
Val loss: 0.3901 score: 0.8571 time: 0.07s
Test loss: 0.2597 score: 0.9796 time: 0.08s
Epoch 275/1000, LR 0.000225
Train loss: 0.6821;  Loss pred: 0.6821; Loss self: 0.0000; time: 0.23s
Val loss: 0.3887 score: 0.8571 time: 0.07s
Test loss: 0.2575 score: 0.9796 time: 0.08s
Epoch 276/1000, LR 0.000225
Train loss: 0.6801;  Loss pred: 0.6801; Loss self: 0.0000; time: 0.23s
Val loss: 0.3868 score: 0.8571 time: 0.07s
Test loss: 0.2552 score: 0.9796 time: 0.08s
Epoch 277/1000, LR 0.000225
Train loss: 0.6793;  Loss pred: 0.6793; Loss self: 0.0000; time: 0.24s
Val loss: 0.3853 score: 0.8571 time: 0.07s
Test loss: 0.2530 score: 0.9796 time: 0.48s
Epoch 278/1000, LR 0.000224
Train loss: 0.6757;  Loss pred: 0.6757; Loss self: 0.0000; time: 4.71s
Val loss: 0.3839 score: 0.8571 time: 0.08s
Test loss: 0.2508 score: 0.9796 time: 0.10s
Epoch 279/1000, LR 0.000224
Train loss: 0.6764;  Loss pred: 0.6764; Loss self: 0.0000; time: 0.24s
Val loss: 0.3826 score: 0.8571 time: 0.07s
Test loss: 0.2487 score: 0.9796 time: 0.09s
Epoch 280/1000, LR 0.000224
Train loss: 0.6756;  Loss pred: 0.6756; Loss self: 0.0000; time: 0.24s
Val loss: 0.3815 score: 0.8571 time: 0.08s
Test loss: 0.2467 score: 0.9796 time: 0.09s
Epoch 281/1000, LR 0.000223
Train loss: 0.6735;  Loss pred: 0.6735; Loss self: 0.0000; time: 0.26s
Val loss: 0.3805 score: 0.8571 time: 0.07s
Test loss: 0.2448 score: 0.9796 time: 0.09s
Epoch 282/1000, LR 0.000223
Train loss: 0.6699;  Loss pred: 0.6699; Loss self: 0.0000; time: 0.26s
Val loss: 0.3795 score: 0.8571 time: 0.07s
Test loss: 0.2428 score: 0.9796 time: 0.09s
Epoch 283/1000, LR 0.000223
Train loss: 0.6692;  Loss pred: 0.6692; Loss self: 0.0000; time: 0.24s
Val loss: 0.3785 score: 0.8571 time: 0.07s
Test loss: 0.2410 score: 0.9796 time: 0.09s
Epoch 284/1000, LR 0.000222
Train loss: 0.6674;  Loss pred: 0.6674; Loss self: 0.0000; time: 0.25s
Val loss: 0.3775 score: 0.8571 time: 0.08s
Test loss: 0.2390 score: 0.9796 time: 0.09s
Epoch 285/1000, LR 0.000222
Train loss: 0.6663;  Loss pred: 0.6663; Loss self: 0.0000; time: 0.25s
Val loss: 0.3765 score: 0.8571 time: 0.07s
Test loss: 0.2372 score: 0.9796 time: 0.09s
Epoch 286/1000, LR 0.000222
Train loss: 0.6652;  Loss pred: 0.6652; Loss self: 0.0000; time: 3.30s
Val loss: 0.3756 score: 0.8571 time: 0.97s
Test loss: 0.2353 score: 0.9796 time: 0.20s
Epoch 287/1000, LR 0.000221
Train loss: 0.6639;  Loss pred: 0.6639; Loss self: 0.0000; time: 0.30s
Val loss: 0.3746 score: 0.8571 time: 0.07s
Test loss: 0.2335 score: 0.9796 time: 0.08s
Epoch 288/1000, LR 0.000221
Train loss: 0.6621;  Loss pred: 0.6621; Loss self: 0.0000; time: 0.24s
Val loss: 0.3738 score: 0.8571 time: 0.07s
Test loss: 0.2318 score: 0.9796 time: 0.08s
Epoch 289/1000, LR 0.000221
Train loss: 0.6604;  Loss pred: 0.6604; Loss self: 0.0000; time: 0.24s
Val loss: 0.3730 score: 0.8571 time: 0.07s
Test loss: 0.2300 score: 0.9796 time: 0.08s
Epoch 290/1000, LR 0.000220
Train loss: 0.6589;  Loss pred: 0.6589; Loss self: 0.0000; time: 0.24s
Val loss: 0.3723 score: 0.8571 time: 0.07s
Test loss: 0.2283 score: 0.9796 time: 0.09s
Epoch 291/1000, LR 0.000220
Train loss: 0.6584;  Loss pred: 0.6584; Loss self: 0.0000; time: 0.24s
Val loss: 0.3716 score: 0.8571 time: 0.07s
Test loss: 0.2267 score: 0.9796 time: 0.09s
Epoch 292/1000, LR 0.000220
Train loss: 0.6572;  Loss pred: 0.6572; Loss self: 0.0000; time: 0.25s
Val loss: 0.3710 score: 0.8571 time: 0.07s
Test loss: 0.2251 score: 0.9796 time: 0.09s
Epoch 293/1000, LR 0.000219
Train loss: 0.6563;  Loss pred: 0.6563; Loss self: 0.0000; time: 0.23s
Val loss: 0.3705 score: 0.8571 time: 0.07s
Test loss: 0.2235 score: 0.9796 time: 0.08s
Epoch 294/1000, LR 0.000219
Train loss: 0.6540;  Loss pred: 0.6540; Loss self: 0.0000; time: 0.24s
Val loss: 0.3698 score: 0.8571 time: 0.07s
Test loss: 0.2219 score: 0.9796 time: 0.09s
Epoch 295/1000, LR 0.000219
Train loss: 0.6529;  Loss pred: 0.6529; Loss self: 0.0000; time: 0.25s
Val loss: 0.3689 score: 0.8571 time: 0.08s
Test loss: 0.2202 score: 0.9796 time: 0.09s
Epoch 296/1000, LR 0.000218
Train loss: 0.6521;  Loss pred: 0.6521; Loss self: 0.0000; time: 0.25s
Val loss: 0.3679 score: 0.8571 time: 0.08s
Test loss: 0.2185 score: 0.9796 time: 0.09s
Epoch 297/1000, LR 0.000218
Train loss: 0.6505;  Loss pred: 0.6505; Loss self: 0.0000; time: 0.25s
Val loss: 0.3670 score: 0.8571 time: 0.07s
Test loss: 0.2169 score: 0.9796 time: 0.09s
Epoch 298/1000, LR 0.000218
Train loss: 0.6480;  Loss pred: 0.6480; Loss self: 0.0000; time: 0.25s
Val loss: 0.3658 score: 0.8571 time: 0.07s
Test loss: 0.2152 score: 0.9796 time: 0.09s
Epoch 299/1000, LR 0.000217
Train loss: 0.6485;  Loss pred: 0.6485; Loss self: 0.0000; time: 0.25s
Val loss: 0.3649 score: 0.8571 time: 0.07s
Test loss: 0.2136 score: 0.9796 time: 0.09s
Epoch 300/1000, LR 0.000217
Train loss: 0.6471;  Loss pred: 0.6471; Loss self: 0.0000; time: 0.25s
Val loss: 0.3640 score: 0.8571 time: 0.07s
Test loss: 0.2120 score: 0.9796 time: 0.08s
Epoch 301/1000, LR 0.000217
Train loss: 0.6456;  Loss pred: 0.6456; Loss self: 0.0000; time: 0.25s
Val loss: 0.3632 score: 0.8571 time: 0.07s
Test loss: 0.2105 score: 0.9796 time: 0.09s
Epoch 302/1000, LR 0.000216
Train loss: 0.6447;  Loss pred: 0.6447; Loss self: 0.0000; time: 0.24s
Val loss: 0.3627 score: 0.8571 time: 0.08s
Test loss: 0.2090 score: 0.9796 time: 0.09s
Epoch 303/1000, LR 0.000216
Train loss: 0.6427;  Loss pred: 0.6427; Loss self: 0.0000; time: 0.25s
Val loss: 0.3620 score: 0.8571 time: 0.08s
Test loss: 0.2075 score: 0.9796 time: 0.09s
Epoch 304/1000, LR 0.000216
Train loss: 0.6434;  Loss pred: 0.6434; Loss self: 0.0000; time: 0.25s
Val loss: 0.3612 score: 0.8571 time: 0.07s
Test loss: 0.2061 score: 0.9796 time: 0.09s
Epoch 305/1000, LR 0.000215
Train loss: 0.6405;  Loss pred: 0.6405; Loss self: 0.0000; time: 0.25s
Val loss: 0.3605 score: 0.8571 time: 0.07s
Test loss: 0.2046 score: 0.9796 time: 0.09s
Epoch 306/1000, LR 0.000215
Train loss: 0.6397;  Loss pred: 0.6397; Loss self: 0.0000; time: 0.26s
Val loss: 0.3599 score: 0.8571 time: 0.07s
Test loss: 0.2032 score: 0.9796 time: 0.73s
Epoch 307/1000, LR 0.000215
Train loss: 0.6382;  Loss pred: 0.6382; Loss self: 0.0000; time: 2.65s
Val loss: 0.3593 score: 0.8571 time: 0.23s
Test loss: 0.2019 score: 0.9796 time: 0.19s
Epoch 308/1000, LR 0.000214
Train loss: 0.6382;  Loss pred: 0.6382; Loss self: 0.0000; time: 0.87s
Val loss: 0.3588 score: 0.8571 time: 0.13s
Test loss: 0.2005 score: 0.9796 time: 0.10s
Epoch 309/1000, LR 0.000214
Train loss: 0.6363;  Loss pred: 0.6363; Loss self: 0.0000; time: 0.26s
Val loss: 0.3584 score: 0.8571 time: 0.07s
Test loss: 0.1992 score: 0.9796 time: 0.09s
Epoch 310/1000, LR 0.000214
Train loss: 0.6368;  Loss pred: 0.6368; Loss self: 0.0000; time: 0.24s
Val loss: 0.3577 score: 0.8571 time: 0.07s
Test loss: 0.1979 score: 0.9796 time: 0.09s
Epoch 311/1000, LR 0.000213
Train loss: 0.6365;  Loss pred: 0.6365; Loss self: 0.0000; time: 0.25s
Val loss: 0.3572 score: 0.8571 time: 0.07s
Test loss: 0.1966 score: 0.9796 time: 0.09s
Epoch 312/1000, LR 0.000213
Train loss: 0.6336;  Loss pred: 0.6336; Loss self: 0.0000; time: 0.24s
Val loss: 0.3567 score: 0.8571 time: 0.07s
Test loss: 0.1953 score: 0.9796 time: 0.09s
Epoch 313/1000, LR 0.000213
Train loss: 0.6321;  Loss pred: 0.6321; Loss self: 0.0000; time: 0.25s
Val loss: 0.3560 score: 0.8571 time: 0.07s
Test loss: 0.1940 score: 0.9796 time: 0.09s
Epoch 314/1000, LR 0.000212
Train loss: 0.6304;  Loss pred: 0.6304; Loss self: 0.0000; time: 0.25s
Val loss: 0.3552 score: 0.8571 time: 0.07s
Test loss: 0.1926 score: 0.9796 time: 0.09s
Epoch 315/1000, LR 0.000212
Train loss: 0.6311;  Loss pred: 0.6311; Loss self: 0.0000; time: 0.25s
Val loss: 0.3544 score: 0.8571 time: 0.07s
Test loss: 0.1913 score: 0.9796 time: 0.09s
Epoch 316/1000, LR 0.000212
Train loss: 0.6287;  Loss pred: 0.6287; Loss self: 0.0000; time: 0.25s
Val loss: 0.3536 score: 0.8571 time: 0.07s
Test loss: 0.1900 score: 0.9796 time: 0.09s
Epoch 317/1000, LR 0.000211
Train loss: 0.6276;  Loss pred: 0.6276; Loss self: 0.0000; time: 0.25s
Val loss: 0.3527 score: 0.8571 time: 0.07s
Test loss: 0.1886 score: 0.9796 time: 0.09s
Epoch 318/1000, LR 0.000211
Train loss: 0.6278;  Loss pred: 0.6278; Loss self: 0.0000; time: 0.24s
Val loss: 0.3520 score: 0.8571 time: 0.07s
Test loss: 0.1873 score: 0.9796 time: 0.08s
Epoch 319/1000, LR 0.000210
Train loss: 0.6273;  Loss pred: 0.6273; Loss self: 0.0000; time: 0.24s
Val loss: 0.3515 score: 0.8571 time: 0.07s
Test loss: 0.1861 score: 0.9796 time: 0.09s
Epoch 320/1000, LR 0.000210
Train loss: 0.6268;  Loss pred: 0.6268; Loss self: 0.0000; time: 0.25s
Val loss: 0.3514 score: 0.8571 time: 0.08s
Test loss: 0.1851 score: 0.9796 time: 0.17s
Epoch 321/1000, LR 0.000210
Train loss: 0.6254;  Loss pred: 0.6254; Loss self: 0.0000; time: 0.25s
Val loss: 0.3516 score: 0.8571 time: 0.07s
Test loss: 0.1841 score: 0.9796 time: 0.09s
     INFO: Early stopping counter 1 of 2
Epoch 322/1000, LR 0.000209
Train loss: 0.6245;  Loss pred: 0.6245; Loss self: 0.0000; time: 0.25s
Val loss: 0.3517 score: 0.8367 time: 0.07s
Test loss: 0.1831 score: 0.9796 time: 0.08s
     INFO: Early stopping counter 2 of 2
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 319,   Train_Loss: 0.6268,   Val_Loss: 0.3514,   Val_Precision: 0.9500,   Val_Recall: 0.7600,   Val_accuracy: 0.8444,   Val_Score: 0.8571,   Val_Loss: 0.3514,   Test_Precision: 1.0000,   Test_Recall: 0.9583,   Test_accuracy: 0.9787,   Test_Score: 0.9796,   Test_loss: 0.1851


[0.07285241596400738, 0.07310730300378054, 0.07274554204195738, 0.07779006497003138, 0.07738581008743495, 0.07384479802567512, 2.768984962021932, 0.08275702700484544, 0.06890303699765354, 0.08429988392163068, 0.09145585203077644, 0.09746584808453918, 1.8477232749573886, 0.30587461101822555, 0.10243699292186648, 0.09528087405487895, 0.09322369110304862, 0.09372576104942709, 0.09315573296044022, 0.0937872719950974, 0.09322050504852086, 0.09365338901989162, 0.10540865897201002, 0.09250331996008754, 2.025694706942886, 0.6006730579538271, 0.25550520699471235, 0.0928001640131697, 0.09333751606754959, 0.0942672590026632, 0.0955293959705159, 0.0942400029161945, 2.0007414009887725, 0.9631382019724697, 0.09820243797730654, 0.09222086996305734, 0.09137349901720881, 0.09197054198011756, 0.0922232820885256, 0.09218762395903468, 0.09215614397544414, 0.0926953119924292, 0.09184444101992995, 0.09313041903078556, 2.231626717024483, 0.0911924650426954, 0.09048202098347247, 0.09073361603077501, 0.08963985904119909, 0.088379836990498, 0.08899841306265444, 0.08940204803366214, 0.09019203099887818, 0.08908963506110013, 1.5675729280337691, 0.11441070190630853, 0.09132417396176606, 0.09061576309613883, 0.09071535302791744, 0.09449626400601119, 0.09154808998573571, 0.09176901506725699, 0.0901662950636819, 0.6212077389936894, 0.4076754900161177, 0.1145588019862771, 0.10059829393867403, 0.09685184096451849, 0.09587441990152001, 0.09146754501853138, 0.0913835089886561, 0.09710890892893076, 0.09237471292726696, 0.09227871103212237, 0.11280902195721865, 0.5291376219829544, 0.7009190630633384, 0.08878692891448736, 0.0881393130403012, 0.08792791794985533, 0.08712372998706996, 0.08730692497920245, 0.08764419902581722, 0.08775428705848753, 0.08779307396616787, 0.08829399396199733, 0.08794465300161391, 0.08819919195957482, 0.08848579495679587, 0.08842606702819467, 0.08761513198260218, 0.0881393610034138, 0.08767104591242969, 0.08847337192855775, 0.08900243800599128, 0.08786768594291061, 0.08752786496188492, 0.08817626896779984, 0.09788822801783681, 1.5882117769215256, 0.08698581403587013, 0.08881500200368464, 0.0878309840336442, 0.0873082730686292, 0.08926935598719865, 0.0954120249953121, 0.08872128603979945, 0.08821507706306875, 0.08878858096431941, 0.09096545993816108, 0.09493056102655828, 0.10149786795955151, 0.09557001700159162, 0.09232568508014083, 0.09312881901860237, 0.09142170590348542, 0.10114602593239397, 0.09401850402355194, 0.09224524698220193, 0.09069662098772824, 0.09002049395348877, 0.09124232805334032, 0.09203007898759097, 0.14386461698450148, 0.09299333998933434, 0.09149630693718791, 2.3424065549625084, 0.29546451894566417, 0.09256751695647836, 0.09088025300297886, 0.09059923002496362, 0.09276438504457474, 0.09130054095294327, 0.09147536300588399, 0.09204723010770977, 0.09234215703327209, 0.09180717403069139, 1.6149079010356218, 0.09553759393747896, 0.08999391901306808, 0.08905608090572059, 0.08860610192641616, 0.08801395399495959, 0.08801055804360658, 0.0877555520273745, 0.08738333103246987, 0.08792124409228563, 0.08716262702364475, 0.09146387898363173, 0.09188390406779945, 0.09246815193910152, 0.09595494403038174, 0.09253654896747321, 0.09197067096829414, 0.09139782900456339, 0.09751216997392476, 0.10159637196920812, 0.09194338496308774, 0.09512234898284078, 0.7681029590312392, 1.468165852013044, 0.09389093401841819, 0.08610586496070027, 0.08897884597536176, 0.08769334305543453, 0.08632736303843558, 0.08714070497080684, 0.08685519802384079, 0.08728687197435647, 0.08698723802808672, 0.08770410600118339, 0.08710794197395444, 0.08719708200078458, 0.08673921402078122, 0.08671243698336184, 0.08677183603867888, 1.9404173309449106, 0.1279387720860541, 0.08831864991225302, 0.08829232701100409, 0.08937345701269805, 0.08712768997065723, 0.08738104708027095, 0.08790570101700723, 0.08783646405208856, 0.08926198596600443, 1.583307786961086, 0.09599048306699842, 0.08714247297029942, 0.08797865093220025, 0.08728561899624765, 0.08744452695827931, 0.09097465802915394, 0.09466177097056061, 0.09441806795075536, 0.08940675295889378, 0.08833408902864903, 0.08879181393422186, 0.08775366109330207, 0.0847684689797461, 0.08561920595820993, 0.08612996607553214, 0.0844131310004741, 0.08892288396600634, 0.08467888296581805, 0.08696554007474333, 0.08522571995854378, 0.08645314397290349, 0.08571698295418173, 0.08563920098822564, 0.0845157119911164, 0.08541176095604897, 0.08597755094524473, 0.08502023399341851, 0.08905581699218601, 0.11324689106550068, 0.08982858504168689, 0.2652058450039476, 0.09585253591649234, 0.09198635094799101, 0.09239908400923014, 0.09230886399745941, 0.09208384295925498, 1.4392001880332828, 0.5704417580273002, 0.0947235330240801, 0.09148859104607254, 0.09118617605417967, 0.09130325797013938, 0.09204202808905393, 0.09134924306999892, 0.09168934402987361, 0.09234005399048328, 0.09121411899104714, 0.09367470897268504, 0.09258739906363189, 0.09185893007088453, 0.15829253697302192, 0.3325400640023872, 0.09514019289053977, 0.09105533198453486, 0.09203948790673167, 0.09279980091378093, 0.09209690697025508, 0.09150955500081182, 0.09328558400738984, 0.09252145304344594, 0.0935427249642089, 1.1681556949624792, 0.7744131260551512, 1.7821935990359634, 0.3556343800155446, 1.1166080579860136, 0.09367983299307525, 0.09181717305909842, 0.0906438329257071, 0.09578897594474256, 0.08754955208860338, 0.08772529102861881, 0.10411030892282724, 0.08925269602332264, 0.09132515301462263, 0.08929374592844397, 0.08651498798280954, 2.451538105029613, 0.09497874404769391, 0.08746278996113688, 0.08932248700875789, 0.0947071110131219, 0.09569301805458963, 0.0881232360843569, 0.0892641139216721, 0.08824035304132849, 2.2156366870040074, 0.1509447549469769, 0.08715335302986205, 0.08744212798774242, 0.08766887604724616, 0.08810982701834291, 0.08672311797272414, 0.08670445892494172, 0.08675198594573885, 0.08712792699225247, 0.08703964296728373, 0.08725042501464486, 0.481179291033186, 0.10426913399714977, 0.0901967230020091, 0.09162944392301142, 0.09162393398582935, 0.09410286403726786, 0.09145194303710014, 0.0917247609468177, 0.09020038088783622, 0.20064386806916445, 0.08856179600115865, 0.0836206910898909, 0.08824826101772487, 0.09011134598404169, 0.09497060708235949, 0.09428409195970744, 0.08806898701004684, 0.09453116194345057, 0.09490963199641556, 0.0965501150349155, 0.09373242303263396, 0.09170654695481062, 0.09187078208196908, 0.08662984904367477, 0.09157106303609908, 0.09195898799225688, 0.09538929397240281, 0.09135302796494216, 0.09117086394689977, 0.7323360710870475, 0.2001710799522698, 0.10940417798701674, 0.09174848801922053, 0.09165921900421381, 0.09156101394910365, 0.09129280701745301, 0.09201429400127381, 0.09164166508708149, 0.09019411099143326, 0.09046756406314671, 0.09123749798163772, 0.09173879004083574, 0.09147001907695085, 0.169532747939229, 0.09281188703607768, 0.09216267103329301]
[0.0014867839992654566, 0.001491985775587358, 0.0014846028988154568, 0.001587552346327171, 0.0015793022466823459, 0.001507036694401533, 0.05650989718412106, 0.0016889189184662334, 0.0014061844285235417, 0.0017204057943189935, 0.0018664459598117641, 0.0019890989405007995, 0.0377086382644365, 0.00624233900037195, 0.002090550875956459, 0.0019445076337730397, 0.001902524308225482, 0.0019127706336617774, 0.0019011374073559229, 0.0019140259590836204, 0.0019024592867045074, 0.0019112936534671759, 0.0021511971218777554, 0.001887822856328317, 0.04134070830495686, 0.01225863383579239, 0.005214391979483926, 0.001893880898227953, 0.0019048472666846855, 0.0019238216122992489, 0.0019495795096023654, 0.0019232653656366225, 0.040831457163036176, 0.019655881672907546, 0.0020041313872919703, 0.0018820585706746395, 0.001864765286065486, 0.0018769498363289299, 0.0018821077977250122, 0.001881380080796626, 0.0018807376321519213, 0.0018917410610699837, 0.001874376347345509, 0.0019006207965466442, 0.04554340238825475, 0.001861070715157049, 0.0018465718568055605, 0.0018517064496076532, 0.0018293848783918181, 0.0018036701426632246, 0.0018162941441358048, 0.001824531592523717, 0.0018406536938546567, 0.001818155817573472, 0.03199128424558712, 0.002334912283802215, 0.00186375865228094, 0.0018493012876763027, 0.0018513337352636214, 0.0019284951837961466, 0.0018683283670558309, 0.0018728370421889182, 0.0018401284706873857, 0.012677708959054887, 0.008319907959512606, 0.0023379347344138182, 0.002053026406911715, 0.0019765681829493568, 0.0019566208143167348, 0.0018666845922149262, 0.0018649695711970633, 0.0019818144679373626, 0.0018851982230054481, 0.0018832390006555586, 0.0023022249379024214, 0.010798726979243969, 0.014304470674762008, 0.001811978141111987, 0.001798761490618392, 0.0017944473050990884, 0.0017780353058585708, 0.0017817739791673969, 0.0017886571229758616, 0.0017909038175201537, 0.0017916953870646503, 0.0018019182441223944, 0.001794788836767631, 0.001799983509379078, 0.0018058325501386912, 0.0018046136128202993, 0.0017880639180122893, 0.0017987624694574245, 0.0017892050186210141, 0.0018055790189501581, 0.0018163762858365567, 0.0017932180804675635, 0.0017862829584058147, 0.001799515693220405, 0.001997718939139527, 0.03241248524329644, 0.0017752206946095945, 0.0018125510612996866, 0.0017924690619111061, 0.0017818014911965144, 0.0018218235915754826, 0.001947184183577798, 0.001810638490608152, 0.0018003076951646684, 0.0018120118564146819, 0.0018564379579216546, 0.0019373583882971077, 0.0020713850603990107, 0.0019504085102365638, 0.0018841976546967517, 0.001900588143236783, 0.001865749100071131, 0.002064204610865183, 0.0019187449800724887, 0.001882556060861264, 0.0018509514487291478, 0.0018371529378263013, 0.00186208832761919, 0.0018781648772977749, 0.0029360125915204385, 0.001897823265088456, 0.001867271570146692, 0.04780421540739813, 0.006029888141748248, 0.0018891329991118032, 0.0018546990408771196, 0.001848963878060482, 0.0018931507151954028, 0.001863276345978434, 0.0018668441429772243, 0.001878514900157342, 0.0018845338170055527, 0.0018736157965447222, 0.032957304102767794, 0.001949746815050591, 0.0018366105921034301, 0.001817471038892257, 0.0018082877944166564, 0.0017962031427542775, 0.001796133837624624, 0.0017909296332117245, 0.001783333286376936, 0.0017943111039241965, 0.0017788291229315254, 0.0018666097751761578, 0.0018751817156693765, 0.0018871051416143167, 0.0019582641638853414, 0.001888500999336188, 0.0018769524687406967, 0.001865261816419661, 0.0019900442851821377, 0.0020733953463103697, 0.0018763956114915864, 0.0019412724282212403, 0.015675570592474267, 0.02996256840842947, 0.0019161415105799632, 0.001757262550218373, 0.0018158948158237096, 0.0017896600623558067, 0.0017617829191517464, 0.0017783817340980988, 0.0017725550617110363, 0.0017813647341705402, 0.0017752497556752392, 0.001789879714309865, 0.0017777131015092743, 0.0017795322857302974, 0.001770188041240433, 0.0017696415710890172, 0.0017708537967077323, 0.039600353692753276, 0.0026109953486949814, 0.001802421426780674, 0.0018018842247143692, 0.0018239481022999603, 0.0017781161218501475, 0.0017832866751075704, 0.00179399389830627, 0.0017925808990222154, 0.0018216731829796822, 0.03231240381553237, 0.0019589894503469064, 0.0017784178157203964, 0.0017954826720857195, 0.0017813391631887276, 0.0017845821828220267, 0.0018566256740643662, 0.0019318728769502165, 0.0019268993459337829, 0.0018246276114059954, 0.0018027365107887558, 0.001812077835392283, 0.0017908910427204504, 0.001729968754688696, 0.001747330733841019, 0.0017577544097047374, 0.0017227169591933489, 0.0018147527340001293, 0.0017281404686901643, 0.0017748069403008843, 0.00173930040731722, 0.0017643498769980303, 0.0017493261827384026, 0.0017477387956780742, 0.001724810448798294, 0.0017430971623683463, 0.001754643896841729, 0.0017351068161922147, 0.0018174656529017553, 0.0023111610421530752, 0.0018332364294221814, 0.005412364183754033, 0.0019561742023773946, 0.0018772724683263472, 0.0018856955920251049, 0.00188385436729509, 0.0018792621012092853, 0.029371432408842504, 0.011641668531169392, 0.0019331333270220428, 0.0018671141029810723, 0.0018609423684526462, 0.001863331795308967, 0.0018784087365113047, 0.0018642702667346718, 0.0018712111026504819, 0.001884490897764965, 0.0018615126324703498, 0.0019117287545445927, 0.001889538756400651, 0.0018746720422629494, 0.003230459938224937, 0.006786531918416066, 0.0019416365896028523, 0.001858272081317038, 0.0018783568960557483, 0.0018938734880363454, 0.001879528713678675, 0.0018675419387920778, 0.0019037874287222416, 0.0018881929192539988, 0.001909035203351202, 0.023839912142091413, 0.015804349511329616, 0.03637129793950946, 0.007257844490113155, 0.022787919550734972, 0.0019118333263892907, 0.0018738198583489473, 0.0018498741413409613, 0.0019548770600967868, 0.0017867255528286404, 0.0017903120618085473, 0.0021247001820985153, 0.001821483592312707, 0.0018637786329514822, 0.0018223213454784484, 0.0017656119996491745, 0.050031389898563525, 0.0019383417152590593, 0.0017849548971660587, 0.0018229078981379162, 0.0019327981839412634, 0.0019529187358079516, 0.0017984333894766715, 0.0018217166106463695, 0.0018008235314556835, 0.04521707524497974, 0.0030805052029995285, 0.0017786398577522866, 0.0017845332242396412, 0.001789160735658085, 0.0017981597350682228, 0.001769859550463758, 0.001769478753570239, 0.0017704486927701806, 0.0017781209590255606, 0.00177631924423028, 0.0017806209186662216, 0.00981998553128951, 0.0021279415101459137, 0.0018407494490205937, 0.001869988651490029, 0.0018698762037924358, 0.0019204666130054667, 0.001866366184430615, 0.0018719338968738305, 0.0018408240997517596, 0.00409477281773805, 0.0018073835918603807, 0.0017065447161202223, 0.001800984918729079, 0.00183900706089881, 0.0019381756547420304, 0.0019241651420348457, 0.00179732626551116, 0.001929207386601032, 0.0019369312652329706, 0.001970410510916643, 0.001912906592502734, 0.0018715621827512371, 0.0018749139200401853, 0.001767956102932138, 0.0018687972048183484, 0.0018767140406583036, 0.0019467202851510778, 0.0018643475094886155, 0.0018606298764673422, 0.014945634103817294, 0.004085124080658567, 0.002232738326265648, 0.0018724181228412353, 0.001870596306208445, 0.0018685921214102786, 0.0018631185105602657, 0.0018778427347198737, 0.0018702380630016631, 0.0018406961426823115, 0.001846276817615239, 0.0018619897547273003, 0.001872220204915015, 0.0018667350832030786, 0.0034598519987597757, 0.001894120143593422, 0.001880870837414143]
[672.5926566966341, 670.2476768629544, 673.5807944318885, 629.9004894632399, 633.1910197055116, 663.5538495611183, 17.6960152084827, 592.0947353163259, 711.1442707767535, 581.2582143713604, 535.7776338195468, 502.7402004187021, 26.519122567815256, 160.196362283499, 478.3428193501796, 514.2690018961986, 525.617462902599, 522.8018364573154, 526.0009066839556, 522.4589537326708, 525.6354272538613, 523.2058392418943, 464.8574460378189, 529.7107176384811, 24.189232381393364, 81.57515865105866, 191.77691357583194, 528.0163081721082, 524.9764731743885, 519.7987139799585, 512.9311192873375, 519.9490501244422, 24.490921203401925, 50.87535713945299, 498.96928232396164, 531.3330921691463, 536.2605189363668, 532.7792893793422, 531.3191950050601, 531.5247090192289, 531.7062746576778, 528.6135722160581, 533.5107868898365, 526.143879840188, 21.957077151923357, 537.3250956321741, 541.5440489437166, 540.0424026237441, 546.6318278956604, 554.4251004362923, 550.571614861315, 548.0858780947642, 543.285248788881, 550.0078652965011, 31.258513797799157, 428.28161337674806, 536.5501583459646, 540.7447702891761, 540.1511250793498, 518.5390186101216, 535.2378188079597, 533.949285214494, 543.440317309175, 78.87860521405669, 120.19363734146185, 427.7279366614681, 487.0857952111097, 505.92739912864505, 511.08523055817875, 535.7091413142505, 536.2017780044167, 504.58810154957763, 530.4481978588784, 531.0000481361624, 434.3624219930088, 92.60350798034632, 69.90821420357364, 551.883037278978, 555.9380747339728, 557.274653403534, 562.4185283076389, 561.238412779655, 559.0786446181811, 558.3772786774723, 558.1305880562144, 554.9641351719814, 557.1686092058466, 555.5606453000005, 553.7612000200119, 554.1352414144615, 559.2641235731971, 555.9377722071544, 558.9074419044082, 553.8389566475154, 550.5467164472676, 557.6566569857806, 559.8217210180741, 555.7050731857773, 500.570916362603, 30.852308685796324, 563.3102425160266, 551.708595333558, 557.889684820453, 561.2297469391389, 548.90056568826, 513.5621008191318, 552.2913630672475, 555.4606041433008, 551.8727686355428, 538.6659951294758, 516.1667588406171, 482.76876140420273, 512.7131033071173, 530.729882561574, 526.1529193257815, 535.9777474698368, 484.4480991546975, 521.1740019573737, 531.1926804147882, 540.262685272806, 544.3204969005949, 537.031452894917, 532.4346185403905, 340.59799433017474, 526.9194547224558, 535.5407408261673, 20.918657308310117, 165.84055566080028, 529.3433551106042, 539.1710342002887, 540.8434485204631, 528.2199626123187, 536.6890435540226, 535.6633566662989, 532.3354102308377, 530.6352111998494, 533.727353198119, 30.342287611929365, 512.8871052798988, 544.4812331473717, 550.2150948218118, 553.0093180342427, 556.7299022017139, 556.7513840296522, 558.3692298433143, 560.7476783162751, 557.3169545754795, 562.1675444305699, 535.7306134891675, 533.2816503295702, 529.9121802744673, 510.6563345447356, 529.5205034847755, 532.7785421603828, 536.1177670593629, 502.5013802185188, 482.30068702503416, 532.9366546562528, 515.126051069651, 63.793531093540736, 33.37497594894657, 521.8821232557755, 569.0669273500031, 550.6926895137311, 558.765332609399, 567.6068198467235, 562.3089693434954, 564.157368987289, 561.3673498850485, 563.3010210552808, 558.6967615785154, 562.5204647200959, 561.9454100489178, 564.9117363256306, 565.08618261302, 564.6993568069489, 25.252299708196706, 382.99570334348414, 554.8092056285149, 554.9746128436847, 548.2612135394756, 562.39296619137, 560.7623350517541, 557.4154967550957, 557.8548787089396, 548.9458863111306, 30.94786775099989, 510.46727169608573, 562.2975608771231, 556.9533003837635, 561.375408268646, 560.3552526892668, 538.611532722633, 517.6324032141632, 518.9684671958805, 548.0570357199814, 554.7122355459853, 551.8526745753819, 558.3812616991771, 578.0451220808019, 572.3015000152714, 568.907689537799, 580.4784092148507, 551.0392580017068, 578.6566648473577, 563.4415649909895, 574.9438083225932, 566.7810070083488, 571.6486781410862, 572.1678791320918, 579.7738532351295, 573.6914852418782, 569.916210234994, 576.3333938106208, 550.2167253633684, 432.68295967311786, 545.483377894247, 184.76214202319196, 511.2019158542584, 532.687724809353, 530.3082874187928, 530.8265953890258, 532.1237518473397, 34.046688158761434, 85.89833985760725, 517.2948942639575, 535.5859068298931, 537.3621542248454, 536.6730726742018, 532.3654966901724, 536.4029120903867, 534.4132463641047, 530.6472964056316, 537.1975363244966, 523.0867598882863, 529.2296845526907, 533.42663540919, 309.5534441295306, 147.35066629339508, 515.0294372051068, 538.1343292265666, 532.3801893558363, 528.018374150665, 532.0482697190442, 535.4632092743244, 525.2687274393688, 529.606900758365, 523.8248085968017, 41.94646331076087, 63.273720900890794, 27.494207153760076, 137.7819545957796, 43.88290022586759, 523.0581485304532, 533.669229485654, 540.5773169384955, 511.5411195988404, 559.6830461269544, 558.5618403250965, 470.65464032309916, 549.0030238100125, 536.544406250864, 548.7506374664405, 566.3758516586312, 19.987451918234864, 515.9049057902312, 560.2382455644579, 548.5740673028467, 517.3845920947892, 512.0540766312455, 556.0394985165346, 548.9328000611393, 555.3014954172977, 22.115539198016258, 324.622077906665, 562.2273647143644, 560.3706260084246, 558.9212752493041, 556.1241198419226, 565.0165854900572, 565.1381786768118, 564.8285680819831, 562.3914362654025, 562.9618680584204, 561.6018488365578, 101.83314393017085, 469.9377286603285, 543.2569872733463, 534.762603614299, 534.79476233337, 520.7067872088821, 535.8005349336506, 534.2068978343847, 543.2349566342883, 244.21379268420546, 553.2859789717785, 585.9793713893824, 555.2517345373892, 543.7717022746245, 515.949107890894, 519.705912010483, 556.3820098715354, 518.3475902825807, 516.2805815309723, 507.50845798868374, 522.7646785887539, 534.3129975676139, 533.3578194238202, 565.6249034359562, 535.1035400854007, 532.8462292791422, 513.6844813441669, 536.3806881015959, 537.452403966895, 66.90917180587125, 244.79060617389837, 447.88051883918865, 534.0687466123138, 534.5888884100938, 535.1622692518216, 536.734509550488, 532.5259573183451, 534.6912886560746, 543.2727199301745, 541.63058890143, 537.0598830961109, 534.1252045965355, 535.694651586088, 289.02970426436207, 527.949614697014, 531.6686186568871]
Elapsed: 0.2099820803615245~0.41819047416687244
Time per graph: 0.004285348578806622~0.008534499472793316
Speed: 486.40707767999794~151.94117773610657
Total Time: 0.0932
best val loss: 0.35144925117492676 test_score: 0.9796

Testing...
Test loss: 0.6431 score: 0.9796 time: 0.09s
test Score 0.9796
Epoch Time List: [0.4001076229615137, 0.3984399070031941, 0.39046691299881786, 0.4048434681026265, 0.42205502407159656, 0.41755470586940646, 4.961043810937554, 1.8191702539334074, 0.43878705194219947, 0.38062763097696006, 0.373370666988194, 0.40356782695744187, 3.5689581119222566, 5.394583981949836, 1.5462141421157867, 0.42891296092420816, 0.42606176203116775, 0.4057166330749169, 0.4081716248765588, 0.41256215900648385, 0.4076648020418361, 0.4078600879292935, 0.4159285230562091, 0.40649932296946645, 3.6579023881349713, 3.1642563530476764, 1.7732684450456873, 0.43463442707434297, 0.4032706110738218, 0.4237489808583632, 0.412271287990734, 0.4145273668691516, 5.376948590972461, 6.984726764028892, 2.1374722589971498, 0.4048669560579583, 0.4047879868885502, 0.4084268129663542, 0.40671058802399784, 0.4003670560196042, 0.39968165499158204, 0.4100406940560788, 0.40067753405310214, 0.4035586379468441, 2.841230119112879, 5.865566123160534, 0.4040833229664713, 0.3973646890372038, 0.39017101493664086, 0.4107706610811874, 0.388002090039663, 0.3916752189397812, 0.39018199394922704, 0.3882199468789622, 4.043383124051616, 2.328154378919862, 0.41086561197880656, 0.3972722911275923, 0.4013482149457559, 0.40728357213083655, 0.409798217122443, 0.4025843399576843, 0.39391957002226263, 3.906489818939008, 1.1558805849635974, 1.181927247904241, 0.5541581748984754, 0.42581939103547484, 0.42212121514603496, 0.4029983759392053, 0.4044164380757138, 0.40792862395755947, 0.4132765681715682, 0.4063807249767706, 0.420865970896557, 2.415722783887759, 5.660842376179062, 0.40318282588850707, 0.38592411391437054, 0.3862181630684063, 0.38931225799024105, 0.3824400920420885, 0.382579302880913, 0.3954155540559441, 0.3830297130625695, 0.39126565493643284, 0.39020015683490783, 0.3864470961270854, 0.3920476289931685, 0.39219358598347753, 0.3856901659164578, 0.391373775084503, 0.3832546730991453, 0.3934630178846419, 0.39480388909578323, 0.3888674018671736, 0.3861475068842992, 0.38878291787113994, 0.40221610898151994, 7.832180242054164, 0.5875608739443123, 0.382132237078622, 0.39267921005375683, 0.3910508300177753, 0.39565120195038617, 0.42806590197142214, 0.39149413094855845, 0.38512717501726, 0.3938990969909355, 0.393322771997191, 0.4160100379958749, 0.42004401003941894, 0.4193632900714874, 0.40936936205253005, 0.41117262118496, 0.4053992049302906, 0.40896017488557845, 0.4102570960531011, 0.411779573187232, 0.39665889588650316, 0.3990061420481652, 0.4065627548843622, 0.39937617897521704, 0.4610773507738486, 0.4139681690139696, 0.4002758510177955, 8.003183649852872, 2.028193167876452, 1.2493207679362968, 0.40921193396206945, 0.40558737702667713, 0.40665807691402733, 0.4011431960389018, 0.4108881460269913, 0.40972810704261065, 0.4034570809453726, 0.4019275289028883, 3.657454303931445, 1.1292157431598753, 0.3946553160203621, 0.39021593902725726, 0.38651573611423373, 0.38383490801788867, 0.383530454011634, 0.38260769296903163, 0.3819782000500709, 0.38295966398436576, 0.3883176539093256, 0.38714142609387636, 0.40131074807140976, 0.408235521055758, 0.4091117038624361, 0.40469348803162575, 0.4034306920366362, 0.4035573049914092, 0.4312469590222463, 0.44409004994668067, 0.40509566909167916, 0.4221180679742247, 2.610071361064911, 6.319308230187744, 2.7440654250094667, 0.4021746850339696, 0.38349740498233587, 0.38265727704856545, 0.38061674998607486, 0.38129807892255485, 0.3855573651380837, 0.38712908618617803, 0.3797113300533965, 0.3760629389435053, 0.37983594508841634, 0.3908339769113809, 0.38893044111318886, 0.3778149770805612, 0.3768184690270573, 4.042566895019263, 1.6591883208602667, 0.3932738769799471, 0.38518633297644556, 0.3854277179343626, 0.3810095818480477, 0.38624438794795424, 0.39337568904738873, 0.3869561010506004, 0.3908612991217524, 2.5533521169563755, 3.1444909940473735, 0.38519037794321775, 0.38281291304156184, 0.3900548688834533, 0.40345474798232317, 0.40865168790332973, 0.40659045497886837, 0.41249295487068594, 0.4078306080773473, 0.38878999394364655, 0.38732286798767745, 0.3862963580759242, 0.41821674699895084, 0.37111218797508627, 0.3744128660764545, 0.36978617403656244, 0.39368025108706206, 0.3694278900511563, 0.38079538696911186, 0.3928317561512813, 0.3693922481033951, 0.37046934908721596, 0.37588158797007054, 0.3694347059354186, 0.3695775889791548, 0.3625717699760571, 0.36665177100803703, 0.3785427851835266, 0.44426340400241315, 0.3975942999823019, 6.433348976075649, 0.7733790270285681, 0.41063012881204486, 0.408637507003732, 0.41141560615506023, 0.4045433569699526, 2.5969367718789726, 6.186321682063863, 0.6608519090805203, 0.4018445978872478, 0.4005798000143841, 0.4061287030344829, 0.4068231381243095, 0.3983837969135493, 0.40290273108985275, 0.4120720678474754, 0.4015501510584727, 0.4129402828402817, 0.4056847479660064, 0.40610788902267814, 5.76456573989708, 1.171008902019821, 1.520803474006243, 0.4121563088847324, 0.40737588389310986, 0.4058456210186705, 0.4086454041535035, 0.40109215304255486, 0.41007914091460407, 0.41055396606680006, 0.41323906811885536, 1.4833393358858302, 3.164408280979842, 3.4455467129591852, 4.191930703935213, 2.4339558840729296, 0.41981180803850293, 0.4057232370832935, 0.41564418480265886, 0.4131871310528368, 0.3889179490506649, 0.3797150840982795, 0.3961796509101987, 0.39546258212067187, 0.39446421491447836, 0.3923751140246168, 0.3892869850387797, 2.93486566003412, 4.152235544053838, 0.39707808615639806, 0.3827537769684568, 0.4049023640109226, 0.41112846811302006, 0.3867439989699051, 0.3888479861197993, 0.3876054169377312, 6.962833747966215, 2.1636775580700487, 0.3926302249310538, 0.38229543913621455, 0.3912974380655214, 0.3808604668593034, 0.38175482011865824, 0.3869848579633981, 0.3852406258229166, 0.3848600360797718, 0.3790422839811072, 0.383217524853535, 0.7806581269251183, 4.889111351920292, 0.3981694560497999, 0.40676420100498945, 0.4144878661027178, 0.4220300648594275, 0.40033109311480075, 0.41662462381646037, 0.40134882484562695, 4.46669953095261, 0.4566325698979199, 0.3813890990568325, 0.38778508093673736, 0.39407185814343393, 0.40365601191297174, 0.41506854607723653, 0.38477995700668544, 0.40185517398640513, 0.4178527860203758, 0.41955396300181746, 0.4122523949481547, 0.4085852359421551, 0.4062395648797974, 0.3987996911164373, 0.40957720996811986, 0.4090205969987437, 0.41330236301291734, 0.4038426090264693, 0.403615779010579, 1.0544054940110072, 3.0683542659971863, 1.1049182680435479, 0.41506556211970747, 0.40276275598444045, 0.4017172669991851, 0.40012461703736335, 0.40431488887406886, 0.40415624307934195, 0.4035089740063995, 0.40153675607871264, 0.40193979407195, 0.3951582059962675, 0.40112095791846514, 0.4987435588845983, 0.41293318080715835, 0.4027248070342466]
Total Epoch List: [9, 322]
Total Time List: [0.06914844003040344, 0.09317560005001724]
========================k_idx:2========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: AEtransGAT
AEtransGAT(
  (gae): GAE(
    (encoder): transGAT(
      (encoder): EdgeFeatureGAT()
      (classifier): Sequential(
        (0): Linear(in_features=64, out_features=64, bias=True)
        (1): ReLU(inplace=True)
        (2): Linear(in_features=64, out_features=2, bias=True)
      )
    )
    (decoder): InnerProductDecoder()
  )
)
Total number of parameters: 1914242
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7d35980f59f0>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 3.3639;  Loss pred: 3.3639; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7212 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7165 score: 0.5000 time: 0.08s
Epoch 2/1000, LR 0.000000
Train loss: 3.3587;  Loss pred: 3.3587; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7209 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7162 score: 0.5000 time: 0.08s
Epoch 3/1000, LR 0.000030
Train loss: 3.3304;  Loss pred: 3.3304; Loss self: 0.0000; time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7203 score: 0.4898 time: 0.09s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7157 score: 0.5000 time: 0.08s
Epoch 4/1000, LR 0.000060
Train loss: 3.3332;  Loss pred: 3.3332; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7194 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7149 score: 0.5000 time: 0.08s
Epoch 5/1000, LR 0.000090
Train loss: 3.3163;  Loss pred: 3.3163; Loss self: 0.0000; time: 3.67s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7182 score: 0.4898 time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7138 score: 0.5000 time: 0.15s
Epoch 6/1000, LR 0.000120
Train loss: 3.2427;  Loss pred: 3.2427; Loss self: 0.0000; time: 1.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7169 score: 0.4898 time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7126 score: 0.5000 time: 0.30s
Epoch 7/1000, LR 0.000150
Train loss: 3.2683;  Loss pred: 3.2683; Loss self: 0.0000; time: 0.97s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7153 score: 0.4898 time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7112 score: 0.5000 time: 0.15s
Epoch 8/1000, LR 0.000180
Train loss: 3.2422;  Loss pred: 3.2422; Loss self: 0.0000; time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7135 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7096 score: 0.5000 time: 0.08s
Epoch 9/1000, LR 0.000210
Train loss: 3.1978;  Loss pred: 3.1978; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7118 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7081 score: 0.5000 time: 0.08s
Epoch 10/1000, LR 0.000240
Train loss: 3.1801;  Loss pred: 3.1801; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7100 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7065 score: 0.5000 time: 0.08s
Epoch 11/1000, LR 0.000270
Train loss: 3.1338;  Loss pred: 3.1338; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7084 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7050 score: 0.5000 time: 0.08s
Epoch 12/1000, LR 0.000270
Train loss: 3.0593;  Loss pred: 3.0593; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7068 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7037 score: 0.5000 time: 0.08s
Epoch 13/1000, LR 0.000270
Train loss: 2.9891;  Loss pred: 2.9891; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7054 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7025 score: 0.5000 time: 0.08s
Epoch 14/1000, LR 0.000270
Train loss: 2.9561;  Loss pred: 2.9561; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7042 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7014 score: 0.5000 time: 0.08s
Epoch 15/1000, LR 0.000270
Train loss: 2.9351;  Loss pred: 2.9351; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7031 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7004 score: 0.5000 time: 0.08s
Epoch 16/1000, LR 0.000270
Train loss: 2.8751;  Loss pred: 2.8751; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7021 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6996 score: 0.5000 time: 0.08s
Epoch 17/1000, LR 0.000270
Train loss: 2.8173;  Loss pred: 2.8173; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7012 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6988 score: 0.5000 time: 0.08s
Epoch 18/1000, LR 0.000270
Train loss: 2.7704;  Loss pred: 2.7704; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7003 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6981 score: 0.5000 time: 0.08s
Epoch 19/1000, LR 0.000270
Train loss: 2.7648;  Loss pred: 2.7648; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6996 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6975 score: 0.5000 time: 0.08s
Epoch 20/1000, LR 0.000270
Train loss: 2.6767;  Loss pred: 2.6767; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6989 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6969 score: 0.5000 time: 0.08s
Epoch 21/1000, LR 0.000270
Train loss: 2.6491;  Loss pred: 2.6491; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6983 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6964 score: 0.5000 time: 0.08s
Epoch 22/1000, LR 0.000270
Train loss: 2.6167;  Loss pred: 2.6167; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6978 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6960 score: 0.5000 time: 0.08s
Epoch 23/1000, LR 0.000270
Train loss: 2.5616;  Loss pred: 2.5616; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6973 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6956 score: 0.5000 time: 0.08s
Epoch 24/1000, LR 0.000270
Train loss: 2.5240;  Loss pred: 2.5240; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6968 score: 0.4898 time: 2.36s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6952 score: 0.5000 time: 1.21s
Epoch 25/1000, LR 0.000270
Train loss: 2.5187;  Loss pred: 2.5187; Loss self: 0.0000; time: 1.79s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6964 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6949 score: 0.5000 time: 0.08s
Epoch 26/1000, LR 0.000270
Train loss: 2.4710;  Loss pred: 2.4710; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6961 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6946 score: 0.5000 time: 0.07s
Epoch 27/1000, LR 0.000270
Train loss: 2.4022;  Loss pred: 2.4022; Loss self: 0.0000; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6958 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6944 score: 0.5000 time: 0.07s
Epoch 28/1000, LR 0.000270
Train loss: 2.4133;  Loss pred: 2.4133; Loss self: 0.0000; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6955 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6941 score: 0.5000 time: 0.07s
Epoch 29/1000, LR 0.000270
Train loss: 2.3592;  Loss pred: 2.3592; Loss self: 0.0000; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6952 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6939 score: 0.5000 time: 0.08s
Epoch 30/1000, LR 0.000270
Train loss: 2.3172;  Loss pred: 2.3172; Loss self: 0.0000; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6950 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6937 score: 0.5000 time: 0.07s
Epoch 31/1000, LR 0.000270
Train loss: 2.2809;  Loss pred: 2.2809; Loss self: 0.0000; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6948 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6935 score: 0.5000 time: 0.07s
Epoch 32/1000, LR 0.000270
Train loss: 2.2591;  Loss pred: 2.2591; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6946 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6933 score: 0.5000 time: 0.07s
Epoch 33/1000, LR 0.000270
Train loss: 2.2063;  Loss pred: 2.2063; Loss self: 0.0000; time: 3.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6944 score: 0.4898 time: 0.75s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6932 score: 0.5000 time: 0.73s
Epoch 34/1000, LR 0.000270
Train loss: 2.1625;  Loss pred: 2.1625; Loss self: 0.0000; time: 1.01s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6942 score: 0.4898 time: 0.09s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6930 score: 0.5000 time: 0.08s
Epoch 35/1000, LR 0.000270
Train loss: 2.1370;  Loss pred: 2.1370; Loss self: 0.0000; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6940 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6929 score: 0.5000 time: 0.07s
Epoch 36/1000, LR 0.000270
Train loss: 2.1009;  Loss pred: 2.1009; Loss self: 0.0000; time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6938 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6927 score: 0.5000 time: 0.07s
Epoch 37/1000, LR 0.000270
Train loss: 2.0792;  Loss pred: 2.0792; Loss self: 0.0000; time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6937 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6926 score: 0.5000 time: 0.07s
Epoch 38/1000, LR 0.000270
Train loss: 2.0645;  Loss pred: 2.0645; Loss self: 0.0000; time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6935 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6924 score: 0.5000 time: 0.08s
Epoch 39/1000, LR 0.000269
Train loss: 2.0284;  Loss pred: 2.0284; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6934 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6923 score: 0.5000 time: 0.08s
Epoch 40/1000, LR 0.000269
Train loss: 1.9921;  Loss pred: 1.9921; Loss self: 0.0000; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6932 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6921 score: 0.5000 time: 0.07s
Epoch 41/1000, LR 0.000269
Train loss: 1.9775;  Loss pred: 1.9775; Loss self: 0.0000; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6931 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6920 score: 0.5000 time: 0.07s
Epoch 42/1000, LR 0.000269
Train loss: 1.9549;  Loss pred: 1.9549; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6930 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6918 score: 0.5000 time: 0.08s
Epoch 43/1000, LR 0.000269
Train loss: 1.9203;  Loss pred: 1.9203; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6928 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6917 score: 0.5000 time: 0.08s
Epoch 44/1000, LR 0.000269
Train loss: 1.8880;  Loss pred: 1.8880; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6927 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6916 score: 0.5000 time: 0.08s
Epoch 45/1000, LR 0.000269
Train loss: 1.8829;  Loss pred: 1.8829; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6926 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6914 score: 0.5000 time: 0.08s
Epoch 46/1000, LR 0.000269
Train loss: 1.8618;  Loss pred: 1.8618; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6924 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6913 score: 0.5000 time: 0.08s
Epoch 47/1000, LR 0.000269
Train loss: 1.8194;  Loss pred: 1.8194; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6923 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6911 score: 0.5000 time: 0.08s
Epoch 48/1000, LR 0.000269
Train loss: 1.7733;  Loss pred: 1.7733; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6922 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6910 score: 0.5000 time: 0.08s
Epoch 49/1000, LR 0.000269
Train loss: 1.7881;  Loss pred: 1.7881; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6920 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6908 score: 0.5000 time: 0.08s
Epoch 50/1000, LR 0.000269
Train loss: 1.7544;  Loss pred: 1.7544; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6919 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6907 score: 0.5000 time: 0.07s
Epoch 51/1000, LR 0.000269
Train loss: 1.7208;  Loss pred: 1.7208; Loss self: 0.0000; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6918 score: 0.4898 time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6905 score: 0.5000 time: 1.68s
Epoch 52/1000, LR 0.000269
Train loss: 1.7083;  Loss pred: 1.7083; Loss self: 0.0000; time: 4.03s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6916 score: 0.4898 time: 0.69s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6903 score: 0.5000 time: 0.84s
Epoch 53/1000, LR 0.000269
Train loss: 1.6919;  Loss pred: 1.6919; Loss self: 0.0000; time: 0.45s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6915 score: 0.4898 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6902 score: 0.5000 time: 0.23s
Epoch 54/1000, LR 0.000269
Train loss: 1.6665;  Loss pred: 1.6665; Loss self: 0.0000; time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6913 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6900 score: 0.5000 time: 0.08s
Epoch 55/1000, LR 0.000269
Train loss: 1.6342;  Loss pred: 1.6342; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6912 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6898 score: 0.5000 time: 0.07s
Epoch 56/1000, LR 0.000269
Train loss: 1.6245;  Loss pred: 1.6245; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6911 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6897 score: 0.5000 time: 0.08s
Epoch 57/1000, LR 0.000269
Train loss: 1.6038;  Loss pred: 1.6038; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6909 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6895 score: 0.5000 time: 0.08s
Epoch 58/1000, LR 0.000269
Train loss: 1.6011;  Loss pred: 1.6011; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6908 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6893 score: 0.5000 time: 0.07s
Epoch 59/1000, LR 0.000268
Train loss: 1.5756;  Loss pred: 1.5756; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6906 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6891 score: 0.5000 time: 0.07s
Epoch 60/1000, LR 0.000268
Train loss: 1.5649;  Loss pred: 1.5649; Loss self: 0.0000; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6904 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6889 score: 0.5000 time: 0.07s
Epoch 61/1000, LR 0.000268
Train loss: 1.5324;  Loss pred: 1.5324; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6903 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6887 score: 0.5000 time: 0.08s
Epoch 62/1000, LR 0.000268
Train loss: 1.5335;  Loss pred: 1.5335; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6901 score: 0.4898 time: 0.09s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6885 score: 0.5000 time: 0.09s
Epoch 63/1000, LR 0.000268
Train loss: 1.5113;  Loss pred: 1.5113; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6899 score: 0.4898 time: 0.09s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6883 score: 0.5000 time: 0.08s
Epoch 64/1000, LR 0.000268
Train loss: 1.4960;  Loss pred: 1.4960; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6897 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6880 score: 0.5000 time: 0.08s
Epoch 65/1000, LR 0.000268
Train loss: 1.4761;  Loss pred: 1.4761; Loss self: 0.0000; time: 1.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6895 score: 0.4898 time: 0.98s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6878 score: 0.5000 time: 0.63s
Epoch 66/1000, LR 0.000268
Train loss: 1.4633;  Loss pred: 1.4633; Loss self: 0.0000; time: 1.42s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6893 score: 0.4898 time: 0.37s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6876 score: 0.5000 time: 0.43s
Epoch 67/1000, LR 0.000268
Train loss: 1.4440;  Loss pred: 1.4440; Loss self: 0.0000; time: 2.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6891 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6873 score: 0.5000 time: 0.08s
Epoch 68/1000, LR 0.000268
Train loss: 1.4328;  Loss pred: 1.4328; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6889 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6871 score: 0.5000 time: 0.08s
Epoch 69/1000, LR 0.000268
Train loss: 1.4206;  Loss pred: 1.4206; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6887 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6868 score: 0.5000 time: 0.09s
Epoch 70/1000, LR 0.000268
Train loss: 1.4088;  Loss pred: 1.4088; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6885 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6866 score: 0.5000 time: 0.07s
Epoch 71/1000, LR 0.000268
Train loss: 1.4000;  Loss pred: 1.4000; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6883 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6863 score: 0.5000 time: 0.07s
Epoch 72/1000, LR 0.000267
Train loss: 1.3995;  Loss pred: 1.3995; Loss self: 0.0000; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6881 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6860 score: 0.5000 time: 0.08s
Epoch 73/1000, LR 0.000267
Train loss: 1.3664;  Loss pred: 1.3664; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6879 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6857 score: 0.5000 time: 0.08s
Epoch 74/1000, LR 0.000267
Train loss: 1.3512;  Loss pred: 1.3512; Loss self: 0.0000; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6876 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6854 score: 0.5000 time: 0.07s
Epoch 75/1000, LR 0.000267
Train loss: 1.3522;  Loss pred: 1.3522; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6874 score: 0.4898 time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6851 score: 0.5000 time: 0.07s
Epoch 76/1000, LR 0.000267
Train loss: 1.3337;  Loss pred: 1.3337; Loss self: 0.0000; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6872 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6848 score: 0.5000 time: 0.07s
Epoch 77/1000, LR 0.000267
Train loss: 1.3313;  Loss pred: 1.3313; Loss self: 0.0000; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6869 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6845 score: 0.5000 time: 0.08s
Epoch 78/1000, LR 0.000267
Train loss: 1.3213;  Loss pred: 1.3213; Loss self: 0.0000; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6866 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6842 score: 0.5000 time: 0.07s
Epoch 79/1000, LR 0.000267
Train loss: 1.3101;  Loss pred: 1.3101; Loss self: 0.0000; time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6864 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6838 score: 0.5000 time: 0.07s
Epoch 80/1000, LR 0.000267
Train loss: 1.3003;  Loss pred: 1.3003; Loss self: 0.0000; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6861 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6835 score: 0.5000 time: 0.07s
Epoch 81/1000, LR 0.000267
Train loss: 1.2903;  Loss pred: 1.2903; Loss self: 0.0000; time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6858 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6831 score: 0.5000 time: 0.08s
Epoch 82/1000, LR 0.000267
Train loss: 1.2721;  Loss pred: 1.2721; Loss self: 0.0000; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6855 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6827 score: 0.5000 time: 0.07s
Epoch 83/1000, LR 0.000266
Train loss: 1.2727;  Loss pred: 1.2727; Loss self: 0.0000; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6852 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6823 score: 0.5000 time: 0.08s
Epoch 84/1000, LR 0.000266
Train loss: 1.2658;  Loss pred: 1.2658; Loss self: 0.0000; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6848 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6820 score: 0.5000 time: 0.07s
Epoch 85/1000, LR 0.000266
Train loss: 1.2550;  Loss pred: 1.2550; Loss self: 0.0000; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6845 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6815 score: 0.5000 time: 0.07s
Epoch 86/1000, LR 0.000266
Train loss: 1.2476;  Loss pred: 1.2476; Loss self: 0.0000; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6842 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6811 score: 0.5000 time: 0.08s
Epoch 87/1000, LR 0.000266
Train loss: 1.2441;  Loss pred: 1.2441; Loss self: 0.0000; time: 5.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6838 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6807 score: 0.5000 time: 0.08s
Epoch 88/1000, LR 0.000266
Train loss: 1.2348;  Loss pred: 1.2348; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6835 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6802 score: 0.5000 time: 0.08s
Epoch 89/1000, LR 0.000266
Train loss: 1.2248;  Loss pred: 1.2248; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6831 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6798 score: 0.5000 time: 0.08s
Epoch 90/1000, LR 0.000266
Train loss: 1.2144;  Loss pred: 1.2144; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6827 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6793 score: 0.5000 time: 0.08s
Epoch 91/1000, LR 0.000266
Train loss: 1.2136;  Loss pred: 1.2136; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6823 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6788 score: 0.5000 time: 0.08s
Epoch 92/1000, LR 0.000266
Train loss: 1.1995;  Loss pred: 1.1995; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6819 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6783 score: 0.5000 time: 0.08s
Epoch 93/1000, LR 0.000265
Train loss: 1.1983;  Loss pred: 1.1983; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6815 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6778 score: 0.5000 time: 0.08s
Epoch 94/1000, LR 0.000265
Train loss: 1.1893;  Loss pred: 1.1893; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6810 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6773 score: 0.5000 time: 0.08s
Epoch 95/1000, LR 0.000265
Train loss: 1.1819;  Loss pred: 1.1819; Loss self: 0.0000; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6806 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6767 score: 0.5000 time: 0.08s
Epoch 96/1000, LR 0.000265
Train loss: 1.1748;  Loss pred: 1.1748; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6801 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6761 score: 0.5000 time: 0.08s
Epoch 97/1000, LR 0.000265
Train loss: 1.1642;  Loss pred: 1.1642; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6796 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6756 score: 0.5000 time: 0.08s
Epoch 98/1000, LR 0.000265
Train loss: 1.1576;  Loss pred: 1.1576; Loss self: 0.0000; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6792 score: 0.4898 time: 0.07s
Test loss: 0.6750 score: 0.5208 time: 0.07s
Epoch 99/1000, LR 0.000265
Train loss: 1.1613;  Loss pred: 1.1613; Loss self: 0.0000; time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6787 score: 0.4898 time: 0.07s
Test loss: 0.6744 score: 0.5208 time: 0.07s
Epoch 100/1000, LR 0.000265
Train loss: 1.1547;  Loss pred: 1.1547; Loss self: 0.0000; time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6782 score: 0.4898 time: 0.07s
Test loss: 0.6737 score: 0.5208 time: 0.07s
Epoch 101/1000, LR 0.000265
Train loss: 1.1500;  Loss pred: 1.1500; Loss self: 0.0000; time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6777 score: 0.4898 time: 0.07s
Test loss: 0.6731 score: 0.5208 time: 0.07s
Epoch 102/1000, LR 0.000264
Train loss: 1.1402;  Loss pred: 1.1402; Loss self: 0.0000; time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6771 score: 0.4898 time: 0.08s
Test loss: 0.6725 score: 0.5417 time: 0.07s
Epoch 103/1000, LR 0.000264
Train loss: 1.1415;  Loss pred: 1.1415; Loss self: 0.0000; time: 0.24s
Val loss: 0.6766 score: 0.5102 time: 0.07s
Test loss: 0.6718 score: 0.5833 time: 0.08s
Epoch 104/1000, LR 0.000264
Train loss: 1.1305;  Loss pred: 1.1305; Loss self: 0.0000; time: 0.25s
Val loss: 0.6761 score: 0.5510 time: 0.07s
Test loss: 0.6711 score: 0.6667 time: 0.08s
Epoch 105/1000, LR 0.000264
Train loss: 1.1292;  Loss pred: 1.1292; Loss self: 0.0000; time: 0.25s
Val loss: 0.6755 score: 0.5510 time: 0.07s
Test loss: 0.6704 score: 0.6667 time: 0.08s
Epoch 106/1000, LR 0.000264
Train loss: 1.1208;  Loss pred: 1.1208; Loss self: 0.0000; time: 0.25s
Val loss: 0.6749 score: 0.5510 time: 0.07s
Test loss: 0.6697 score: 0.6667 time: 0.08s
Epoch 107/1000, LR 0.000264
Train loss: 1.1138;  Loss pred: 1.1138; Loss self: 0.0000; time: 0.25s
Val loss: 0.6744 score: 0.5510 time: 0.08s
Test loss: 0.6690 score: 0.6667 time: 0.08s
Epoch 108/1000, LR 0.000264
Train loss: 1.1196;  Loss pred: 1.1196; Loss self: 0.0000; time: 0.26s
Val loss: 0.6738 score: 0.6122 time: 0.08s
Test loss: 0.6682 score: 0.6667 time: 0.09s
Epoch 109/1000, LR 0.000264
Train loss: 1.1119;  Loss pred: 1.1119; Loss self: 0.0000; time: 0.25s
Val loss: 0.6732 score: 0.6122 time: 0.09s
Test loss: 0.6674 score: 0.6875 time: 0.08s
Epoch 110/1000, LR 0.000263
Train loss: 1.1048;  Loss pred: 1.1048; Loss self: 0.0000; time: 0.25s
Val loss: 0.6725 score: 0.6327 time: 0.07s
Test loss: 0.6666 score: 0.6875 time: 0.08s
Epoch 111/1000, LR 0.000263
Train loss: 1.1022;  Loss pred: 1.1022; Loss self: 0.0000; time: 0.25s
Val loss: 0.6719 score: 0.6327 time: 0.07s
Test loss: 0.6658 score: 0.7083 time: 0.08s
Epoch 112/1000, LR 0.000263
Train loss: 1.0931;  Loss pred: 1.0931; Loss self: 0.0000; time: 0.25s
Val loss: 0.6712 score: 0.6327 time: 0.07s
Test loss: 0.6650 score: 0.7292 time: 0.08s
Epoch 113/1000, LR 0.000263
Train loss: 1.0901;  Loss pred: 1.0901; Loss self: 0.0000; time: 0.25s
Val loss: 0.6705 score: 0.6939 time: 0.07s
Test loss: 0.6641 score: 0.7500 time: 0.08s
Epoch 114/1000, LR 0.000263
Train loss: 1.0919;  Loss pred: 1.0919; Loss self: 0.0000; time: 0.25s
Val loss: 0.6698 score: 0.6939 time: 0.07s
Test loss: 0.6633 score: 0.7708 time: 0.08s
Epoch 115/1000, LR 0.000263
Train loss: 1.0846;  Loss pred: 1.0846; Loss self: 0.0000; time: 0.25s
Val loss: 0.6691 score: 0.7755 time: 0.07s
Test loss: 0.6624 score: 0.7708 time: 0.08s
Epoch 116/1000, LR 0.000263
Train loss: 1.0781;  Loss pred: 1.0781; Loss self: 0.0000; time: 0.25s
Val loss: 0.6684 score: 0.7755 time: 0.07s
Test loss: 0.6615 score: 0.7708 time: 0.08s
Epoch 117/1000, LR 0.000262
Train loss: 1.0756;  Loss pred: 1.0756; Loss self: 0.0000; time: 0.25s
Val loss: 0.6676 score: 0.7755 time: 0.07s
Test loss: 0.6605 score: 0.7708 time: 0.08s
Epoch 118/1000, LR 0.000262
Train loss: 1.0722;  Loss pred: 1.0722; Loss self: 0.0000; time: 0.25s
Val loss: 0.6669 score: 0.7755 time: 0.07s
Test loss: 0.6596 score: 0.7917 time: 0.08s
Epoch 119/1000, LR 0.000262
Train loss: 1.0705;  Loss pred: 1.0705; Loss self: 0.0000; time: 0.25s
Val loss: 0.6661 score: 0.7959 time: 0.08s
Test loss: 0.6586 score: 0.8125 time: 0.08s
Epoch 120/1000, LR 0.000262
Train loss: 1.0657;  Loss pred: 1.0657; Loss self: 0.0000; time: 1.55s
Val loss: 0.6653 score: 0.8163 time: 1.45s
Test loss: 0.6576 score: 0.8125 time: 1.83s
Epoch 121/1000, LR 0.000262
Train loss: 1.0615;  Loss pred: 1.0615; Loss self: 0.0000; time: 0.84s
Val loss: 0.6644 score: 0.8367 time: 0.08s
Test loss: 0.6565 score: 0.8333 time: 0.08s
Epoch 122/1000, LR 0.000262
Train loss: 1.0552;  Loss pred: 1.0552; Loss self: 0.0000; time: 0.27s
Val loss: 0.6636 score: 0.8367 time: 0.09s
Test loss: 0.6555 score: 0.8542 time: 0.08s
Epoch 123/1000, LR 0.000262
Train loss: 1.0531;  Loss pred: 1.0531; Loss self: 0.0000; time: 0.26s
Val loss: 0.6627 score: 0.8367 time: 0.08s
Test loss: 0.6544 score: 0.9167 time: 0.08s
Epoch 124/1000, LR 0.000261
Train loss: 1.0520;  Loss pred: 1.0520; Loss self: 0.0000; time: 0.25s
Val loss: 0.6619 score: 0.8367 time: 0.07s
Test loss: 0.6533 score: 0.9167 time: 0.08s
Epoch 125/1000, LR 0.000261
Train loss: 1.0466;  Loss pred: 1.0466; Loss self: 0.0000; time: 0.25s
Val loss: 0.6609 score: 0.8367 time: 0.07s
Test loss: 0.6522 score: 0.9167 time: 0.08s
Epoch 126/1000, LR 0.000261
Train loss: 1.0427;  Loss pred: 1.0427; Loss self: 0.0000; time: 0.25s
Val loss: 0.6600 score: 0.8367 time: 0.07s
Test loss: 0.6510 score: 0.9167 time: 0.08s
Epoch 127/1000, LR 0.000261
Train loss: 1.0434;  Loss pred: 1.0434; Loss self: 0.0000; time: 0.26s
Val loss: 0.6591 score: 0.8163 time: 0.07s
Test loss: 0.6498 score: 0.9167 time: 0.08s
Epoch 128/1000, LR 0.000261
Train loss: 1.0417;  Loss pred: 1.0417; Loss self: 0.0000; time: 0.25s
Val loss: 0.6581 score: 0.8367 time: 0.07s
Test loss: 0.6486 score: 0.9167 time: 0.08s
Epoch 129/1000, LR 0.000261
Train loss: 1.0323;  Loss pred: 1.0323; Loss self: 0.0000; time: 0.25s
Val loss: 0.6571 score: 0.8367 time: 0.07s
Test loss: 0.6474 score: 0.9167 time: 0.08s
Epoch 130/1000, LR 0.000260
Train loss: 1.0332;  Loss pred: 1.0332; Loss self: 0.0000; time: 0.25s
Val loss: 0.6561 score: 0.8367 time: 0.07s
Test loss: 0.6462 score: 0.9167 time: 0.08s
Epoch 131/1000, LR 0.000260
Train loss: 1.0305;  Loss pred: 1.0305; Loss self: 0.0000; time: 0.25s
Val loss: 0.6551 score: 0.8571 time: 0.07s
Test loss: 0.6449 score: 0.9167 time: 0.08s
Epoch 132/1000, LR 0.000260
Train loss: 1.0286;  Loss pred: 1.0286; Loss self: 0.0000; time: 0.25s
Val loss: 0.6541 score: 0.8571 time: 0.07s
Test loss: 0.6436 score: 0.9167 time: 0.08s
Epoch 133/1000, LR 0.000260
Train loss: 1.0272;  Loss pred: 1.0272; Loss self: 0.0000; time: 3.33s
Val loss: 0.6531 score: 0.8571 time: 0.18s
Test loss: 0.6423 score: 0.9167 time: 1.27s
Epoch 134/1000, LR 0.000260
Train loss: 1.0240;  Loss pred: 1.0240; Loss self: 0.0000; time: 2.17s
Val loss: 0.6520 score: 0.8571 time: 0.57s
Test loss: 0.6409 score: 0.9167 time: 0.08s
Epoch 135/1000, LR 0.000260
Train loss: 1.0208;  Loss pred: 1.0208; Loss self: 0.0000; time: 0.25s
Val loss: 0.6509 score: 0.8571 time: 0.07s
Test loss: 0.6396 score: 0.9167 time: 0.08s
Epoch 136/1000, LR 0.000260
Train loss: 1.0142;  Loss pred: 1.0142; Loss self: 0.0000; time: 0.25s
Val loss: 0.6498 score: 0.8571 time: 0.07s
Test loss: 0.6382 score: 0.9167 time: 0.08s
Epoch 137/1000, LR 0.000259
Train loss: 1.0121;  Loss pred: 1.0121; Loss self: 0.0000; time: 0.25s
Val loss: 0.6487 score: 0.8571 time: 0.07s
Test loss: 0.6368 score: 0.9167 time: 0.08s
Epoch 138/1000, LR 0.000259
Train loss: 1.0112;  Loss pred: 1.0112; Loss self: 0.0000; time: 0.25s
Val loss: 0.6476 score: 0.8571 time: 0.07s
Test loss: 0.6353 score: 0.9167 time: 0.08s
Epoch 139/1000, LR 0.000259
Train loss: 1.0061;  Loss pred: 1.0061; Loss self: 0.0000; time: 0.25s
Val loss: 0.6464 score: 0.8571 time: 0.08s
Test loss: 0.6338 score: 0.9167 time: 0.08s
Epoch 140/1000, LR 0.000259
Train loss: 1.0062;  Loss pred: 1.0062; Loss self: 0.0000; time: 0.24s
Val loss: 0.6452 score: 0.8571 time: 0.07s
Test loss: 0.6323 score: 0.9167 time: 0.08s
Epoch 141/1000, LR 0.000259
Train loss: 1.0014;  Loss pred: 1.0014; Loss self: 0.0000; time: 0.25s
Val loss: 0.6440 score: 0.8571 time: 0.07s
Test loss: 0.6308 score: 0.9167 time: 0.08s
Epoch 142/1000, LR 0.000259
Train loss: 0.9965;  Loss pred: 0.9965; Loss self: 0.0000; time: 0.25s
Val loss: 0.6428 score: 0.8571 time: 0.07s
Test loss: 0.6292 score: 0.9167 time: 0.08s
Epoch 143/1000, LR 0.000258
Train loss: 0.9958;  Loss pred: 0.9958; Loss self: 0.0000; time: 0.25s
Val loss: 0.6415 score: 0.8367 time: 0.07s
Test loss: 0.6276 score: 0.9167 time: 0.08s
Epoch 144/1000, LR 0.000258
Train loss: 0.9897;  Loss pred: 0.9897; Loss self: 0.0000; time: 0.24s
Val loss: 0.6402 score: 0.8367 time: 0.07s
Test loss: 0.6260 score: 0.9167 time: 0.08s
Epoch 145/1000, LR 0.000258
Train loss: 0.9895;  Loss pred: 0.9895; Loss self: 0.0000; time: 0.25s
Val loss: 0.6389 score: 0.8367 time: 0.07s
Test loss: 0.6243 score: 0.9167 time: 0.08s
Epoch 146/1000, LR 0.000258
Train loss: 0.9875;  Loss pred: 0.9875; Loss self: 0.0000; time: 0.24s
Val loss: 0.6376 score: 0.8367 time: 0.07s
Test loss: 0.6226 score: 0.9167 time: 0.07s
Epoch 147/1000, LR 0.000258
Train loss: 0.9865;  Loss pred: 0.9865; Loss self: 0.0000; time: 0.24s
Val loss: 0.6362 score: 0.8367 time: 0.07s
Test loss: 0.6209 score: 0.9167 time: 0.07s
Epoch 148/1000, LR 0.000257
Train loss: 0.9849;  Loss pred: 0.9849; Loss self: 0.0000; time: 0.24s
Val loss: 0.6348 score: 0.8367 time: 0.07s
Test loss: 0.6191 score: 0.9167 time: 0.07s
Epoch 149/1000, LR 0.000257
Train loss: 0.9792;  Loss pred: 0.9792; Loss self: 0.0000; time: 0.24s
Val loss: 0.6334 score: 0.8571 time: 0.07s
Test loss: 0.6173 score: 0.9167 time: 0.07s
Epoch 150/1000, LR 0.000257
Train loss: 0.9785;  Loss pred: 0.9785; Loss self: 0.0000; time: 0.23s
Val loss: 0.6319 score: 0.8571 time: 0.07s
Test loss: 0.6155 score: 0.9167 time: 0.07s
Epoch 151/1000, LR 0.000257
Train loss: 0.9733;  Loss pred: 0.9733; Loss self: 0.0000; time: 0.23s
Val loss: 0.6304 score: 0.8571 time: 0.07s
Test loss: 0.6136 score: 0.9375 time: 0.07s
Epoch 152/1000, LR 0.000257
Train loss: 0.9703;  Loss pred: 0.9703; Loss self: 0.0000; time: 0.23s
Val loss: 0.6289 score: 0.8571 time: 0.07s
Test loss: 0.6117 score: 0.9375 time: 0.07s
Epoch 153/1000, LR 0.000257
Train loss: 0.9661;  Loss pred: 0.9661; Loss self: 0.0000; time: 0.24s
Val loss: 0.6273 score: 0.8571 time: 0.09s
Test loss: 0.6098 score: 0.9375 time: 0.08s
Epoch 154/1000, LR 0.000256
Train loss: 0.9691;  Loss pred: 0.9691; Loss self: 0.0000; time: 0.24s
Val loss: 0.6258 score: 0.8571 time: 0.07s
Test loss: 0.6078 score: 0.9375 time: 0.07s
Epoch 155/1000, LR 0.000256
Train loss: 0.9624;  Loss pred: 0.9624; Loss self: 0.0000; time: 1.74s
Val loss: 0.6242 score: 0.8367 time: 1.52s
Test loss: 0.6058 score: 0.9375 time: 1.34s
Epoch 156/1000, LR 0.000256
Train loss: 0.9603;  Loss pred: 0.9603; Loss self: 0.0000; time: 3.86s
Val loss: 0.6226 score: 0.8367 time: 0.07s
Test loss: 0.6038 score: 0.9375 time: 0.08s
Epoch 157/1000, LR 0.000256
Train loss: 0.9621;  Loss pred: 0.9621; Loss self: 0.0000; time: 0.25s
Val loss: 0.6210 score: 0.8367 time: 0.07s
Test loss: 0.6017 score: 0.9375 time: 0.07s
Epoch 158/1000, LR 0.000256
Train loss: 0.9572;  Loss pred: 0.9572; Loss self: 0.0000; time: 0.27s
Val loss: 0.6193 score: 0.8367 time: 0.07s
Test loss: 0.5997 score: 0.9375 time: 0.07s
Epoch 159/1000, LR 0.000255
Train loss: 0.9508;  Loss pred: 0.9508; Loss self: 0.0000; time: 0.24s
Val loss: 0.6177 score: 0.8367 time: 0.07s
Test loss: 0.5976 score: 0.9375 time: 0.07s
Epoch 160/1000, LR 0.000255
Train loss: 0.9490;  Loss pred: 0.9490; Loss self: 0.0000; time: 0.23s
Val loss: 0.6160 score: 0.8367 time: 0.07s
Test loss: 0.5954 score: 0.9375 time: 0.07s
Epoch 161/1000, LR 0.000255
Train loss: 0.9466;  Loss pred: 0.9466; Loss self: 0.0000; time: 0.24s
Val loss: 0.6143 score: 0.8367 time: 0.07s
Test loss: 0.5933 score: 0.9375 time: 0.07s
Epoch 162/1000, LR 0.000255
Train loss: 0.9465;  Loss pred: 0.9465; Loss self: 0.0000; time: 0.24s
Val loss: 0.6126 score: 0.8367 time: 0.07s
Test loss: 0.5911 score: 0.9375 time: 0.13s
Epoch 163/1000, LR 0.000255
Train loss: 0.9433;  Loss pred: 0.9433; Loss self: 0.0000; time: 0.24s
Val loss: 0.6109 score: 0.8367 time: 0.07s
Test loss: 0.5889 score: 0.9375 time: 0.07s
Epoch 164/1000, LR 0.000254
Train loss: 0.9389;  Loss pred: 0.9389; Loss self: 0.0000; time: 0.24s
Val loss: 0.6092 score: 0.8367 time: 0.07s
Test loss: 0.5867 score: 0.9375 time: 0.07s
Epoch 165/1000, LR 0.000254
Train loss: 0.9327;  Loss pred: 0.9327; Loss self: 0.0000; time: 0.24s
Val loss: 0.6074 score: 0.8367 time: 0.07s
Test loss: 0.5845 score: 0.9375 time: 0.07s
Epoch 166/1000, LR 0.000254
Train loss: 0.9328;  Loss pred: 0.9328; Loss self: 0.0000; time: 0.24s
Val loss: 0.6056 score: 0.8367 time: 0.07s
Test loss: 0.5822 score: 0.9375 time: 1.04s
Epoch 167/1000, LR 0.000254
Train loss: 0.9313;  Loss pred: 0.9313; Loss self: 0.0000; time: 2.42s
Val loss: 0.6038 score: 0.8367 time: 0.09s
Test loss: 0.5799 score: 0.9375 time: 0.09s
Epoch 168/1000, LR 0.000254
Train loss: 0.9300;  Loss pred: 0.9300; Loss self: 0.0000; time: 0.26s
Val loss: 0.6020 score: 0.8367 time: 0.08s
Test loss: 0.5776 score: 0.9375 time: 0.08s
Epoch 169/1000, LR 0.000253
Train loss: 0.9257;  Loss pred: 0.9257; Loss self: 0.0000; time: 0.25s
Val loss: 0.6002 score: 0.8367 time: 0.08s
Test loss: 0.5752 score: 0.9375 time: 0.08s
Epoch 170/1000, LR 0.000253
Train loss: 0.9239;  Loss pred: 0.9239; Loss self: 0.0000; time: 0.25s
Val loss: 0.5983 score: 0.8367 time: 0.08s
Test loss: 0.5729 score: 0.9375 time: 0.08s
Epoch 171/1000, LR 0.000253
Train loss: 0.9189;  Loss pred: 0.9189; Loss self: 0.0000; time: 0.25s
Val loss: 0.5964 score: 0.8367 time: 0.07s
Test loss: 0.5705 score: 0.9375 time: 0.08s
Epoch 172/1000, LR 0.000253
Train loss: 0.9165;  Loss pred: 0.9165; Loss self: 0.0000; time: 0.25s
Val loss: 0.5945 score: 0.8367 time: 0.08s
Test loss: 0.5680 score: 0.9375 time: 0.08s
Epoch 173/1000, LR 0.000253
Train loss: 0.9161;  Loss pred: 0.9161; Loss self: 0.0000; time: 0.25s
Val loss: 0.5926 score: 0.8367 time: 0.08s
Test loss: 0.5656 score: 0.9375 time: 0.08s
Epoch 174/1000, LR 0.000252
Train loss: 0.9120;  Loss pred: 0.9120; Loss self: 0.0000; time: 0.25s
Val loss: 0.5906 score: 0.8367 time: 0.07s
Test loss: 0.5631 score: 0.9375 time: 0.08s
Epoch 175/1000, LR 0.000252
Train loss: 0.9100;  Loss pred: 0.9100; Loss self: 0.0000; time: 0.26s
Val loss: 0.5886 score: 0.8367 time: 0.07s
Test loss: 0.5605 score: 0.9375 time: 0.08s
Epoch 176/1000, LR 0.000252
Train loss: 0.9060;  Loss pred: 0.9060; Loss self: 0.0000; time: 0.25s
Val loss: 0.5866 score: 0.8367 time: 0.08s
Test loss: 0.5580 score: 0.9375 time: 0.08s
Epoch 177/1000, LR 0.000252
Train loss: 0.9044;  Loss pred: 0.9044; Loss self: 0.0000; time: 0.26s
Val loss: 0.5846 score: 0.8367 time: 0.08s
Test loss: 0.5554 score: 0.9375 time: 0.08s
Epoch 178/1000, LR 0.000251
Train loss: 0.9015;  Loss pred: 0.9015; Loss self: 0.0000; time: 0.25s
Val loss: 0.5825 score: 0.8367 time: 0.07s
Test loss: 0.5527 score: 0.9375 time: 0.08s
Epoch 179/1000, LR 0.000251
Train loss: 0.8982;  Loss pred: 0.8982; Loss self: 0.0000; time: 0.25s
Val loss: 0.5804 score: 0.8367 time: 0.07s
Test loss: 0.5501 score: 0.9375 time: 0.08s
Epoch 180/1000, LR 0.000251
Train loss: 0.8968;  Loss pred: 0.8968; Loss self: 0.0000; time: 0.26s
Val loss: 0.5783 score: 0.8367 time: 0.08s
Test loss: 0.5474 score: 0.9375 time: 0.08s
Epoch 181/1000, LR 0.000251
Train loss: 0.8942;  Loss pred: 0.8942; Loss self: 0.0000; time: 0.25s
Val loss: 0.5761 score: 0.8367 time: 0.08s
Test loss: 0.5447 score: 0.9375 time: 0.08s
Epoch 182/1000, LR 0.000251
Train loss: 0.8920;  Loss pred: 0.8920; Loss self: 0.0000; time: 0.26s
Val loss: 0.5740 score: 0.8367 time: 0.07s
Test loss: 0.5420 score: 0.9375 time: 0.08s
Epoch 183/1000, LR 0.000250
Train loss: 0.8882;  Loss pred: 0.8882; Loss self: 0.0000; time: 0.26s
Val loss: 0.5718 score: 0.8367 time: 1.81s
Test loss: 0.5393 score: 0.9375 time: 1.54s
Epoch 184/1000, LR 0.000250
Train loss: 0.8867;  Loss pred: 0.8867; Loss self: 0.0000; time: 1.46s
Val loss: 0.5697 score: 0.8367 time: 0.07s
Test loss: 0.5365 score: 0.9375 time: 0.08s
Epoch 185/1000, LR 0.000250
Train loss: 0.8820;  Loss pred: 0.8820; Loss self: 0.0000; time: 0.24s
Val loss: 0.5675 score: 0.8367 time: 0.07s
Test loss: 0.5338 score: 0.9375 time: 0.08s
Epoch 186/1000, LR 0.000250
Train loss: 0.8800;  Loss pred: 0.8800; Loss self: 0.0000; time: 0.24s
Val loss: 0.5654 score: 0.8367 time: 0.08s
Test loss: 0.5310 score: 0.9375 time: 0.08s
Epoch 187/1000, LR 0.000249
Train loss: 0.8772;  Loss pred: 0.8772; Loss self: 0.0000; time: 0.24s
Val loss: 0.5632 score: 0.8367 time: 0.07s
Test loss: 0.5283 score: 0.9375 time: 0.08s
Epoch 188/1000, LR 0.000249
Train loss: 0.8770;  Loss pred: 0.8770; Loss self: 0.0000; time: 0.25s
Val loss: 0.5611 score: 0.8367 time: 0.07s
Test loss: 0.5256 score: 0.9375 time: 0.07s
Epoch 189/1000, LR 0.000249
Train loss: 0.8711;  Loss pred: 0.8711; Loss self: 0.0000; time: 0.24s
Val loss: 0.5590 score: 0.8367 time: 0.07s
Test loss: 0.5229 score: 0.9375 time: 0.07s
Epoch 190/1000, LR 0.000249
Train loss: 0.8694;  Loss pred: 0.8694; Loss self: 0.0000; time: 0.24s
Val loss: 0.5569 score: 0.8367 time: 0.07s
Test loss: 0.5201 score: 0.9375 time: 0.07s
Epoch 191/1000, LR 0.000249
Train loss: 0.8671;  Loss pred: 0.8671; Loss self: 0.0000; time: 0.25s
Val loss: 0.5547 score: 0.8367 time: 2.05s
Test loss: 0.5173 score: 0.9375 time: 1.88s
Epoch 192/1000, LR 0.000248
Train loss: 0.8633;  Loss pred: 0.8633; Loss self: 0.0000; time: 4.96s
Val loss: 0.5525 score: 0.8367 time: 0.46s
Test loss: 0.5145 score: 0.9375 time: 0.55s
Epoch 193/1000, LR 0.000248
Train loss: 0.8604;  Loss pred: 0.8604; Loss self: 0.0000; time: 1.20s
Val loss: 0.5502 score: 0.8367 time: 0.09s
Test loss: 0.5117 score: 0.9375 time: 0.09s
Epoch 194/1000, LR 0.000248
Train loss: 0.8573;  Loss pred: 0.8573; Loss self: 0.0000; time: 0.25s
Val loss: 0.5480 score: 0.8367 time: 0.08s
Test loss: 0.5088 score: 0.9375 time: 0.08s
Epoch 195/1000, LR 0.000248
Train loss: 0.8527;  Loss pred: 0.8527; Loss self: 0.0000; time: 0.25s
Val loss: 0.5457 score: 0.8367 time: 0.07s
Test loss: 0.5059 score: 0.9375 time: 0.08s
Epoch 196/1000, LR 0.000247
Train loss: 0.8521;  Loss pred: 0.8521; Loss self: 0.0000; time: 0.25s
Val loss: 0.5434 score: 0.8367 time: 0.07s
Test loss: 0.5030 score: 0.9375 time: 0.08s
Epoch 197/1000, LR 0.000247
Train loss: 0.8514;  Loss pred: 0.8514; Loss self: 0.0000; time: 0.26s
Val loss: 0.5410 score: 0.8367 time: 0.07s
Test loss: 0.5000 score: 0.9375 time: 0.08s
Epoch 198/1000, LR 0.000247
Train loss: 0.8473;  Loss pred: 0.8473; Loss self: 0.0000; time: 0.25s
Val loss: 0.5387 score: 0.8367 time: 0.07s
Test loss: 0.4971 score: 0.9375 time: 0.08s
Epoch 199/1000, LR 0.000247
Train loss: 0.8427;  Loss pred: 0.8427; Loss self: 0.0000; time: 0.25s
Val loss: 0.5364 score: 0.8367 time: 0.07s
Test loss: 0.4942 score: 0.9375 time: 0.08s
Epoch 200/1000, LR 0.000246
Train loss: 0.8423;  Loss pred: 0.8423; Loss self: 0.0000; time: 0.25s
Val loss: 0.5342 score: 0.8367 time: 0.07s
Test loss: 0.4913 score: 0.9375 time: 0.08s
Epoch 201/1000, LR 0.000246
Train loss: 0.8381;  Loss pred: 0.8381; Loss self: 0.0000; time: 0.25s
Val loss: 0.5319 score: 0.8367 time: 0.07s
Test loss: 0.4884 score: 0.9375 time: 0.08s
Epoch 202/1000, LR 0.000246
Train loss: 0.8349;  Loss pred: 0.8349; Loss self: 0.0000; time: 0.26s
Val loss: 0.5296 score: 0.8367 time: 0.07s
Test loss: 0.4855 score: 0.9167 time: 0.08s
Epoch 203/1000, LR 0.000246
Train loss: 0.8331;  Loss pred: 0.8331; Loss self: 0.0000; time: 0.25s
Val loss: 0.5273 score: 0.8367 time: 0.07s
Test loss: 0.4826 score: 0.9167 time: 0.08s
Epoch 204/1000, LR 0.000245
Train loss: 0.8309;  Loss pred: 0.8309; Loss self: 0.0000; time: 0.25s
Val loss: 0.5250 score: 0.8367 time: 0.07s
Test loss: 0.4797 score: 0.9167 time: 0.08s
Epoch 205/1000, LR 0.000245
Train loss: 0.8286;  Loss pred: 0.8286; Loss self: 0.0000; time: 0.25s
Val loss: 0.5227 score: 0.8367 time: 0.07s
Test loss: 0.4767 score: 0.9167 time: 0.08s
Epoch 206/1000, LR 0.000245
Train loss: 0.8254;  Loss pred: 0.8254; Loss self: 0.0000; time: 0.26s
Val loss: 0.5204 score: 0.8367 time: 0.07s
Test loss: 0.4738 score: 0.9167 time: 0.09s
Epoch 207/1000, LR 0.000245
Train loss: 0.8224;  Loss pred: 0.8224; Loss self: 0.0000; time: 0.25s
Val loss: 0.5182 score: 0.8367 time: 0.07s
Test loss: 0.4709 score: 0.9167 time: 0.07s
Epoch 208/1000, LR 0.000244
Train loss: 0.8179;  Loss pred: 0.8179; Loss self: 0.0000; time: 0.24s
Val loss: 0.5159 score: 0.8367 time: 0.08s
Test loss: 0.4680 score: 0.9167 time: 0.08s
Epoch 209/1000, LR 0.000244
Train loss: 0.8185;  Loss pred: 0.8185; Loss self: 0.0000; time: 0.25s
Val loss: 0.5136 score: 0.8367 time: 0.08s
Test loss: 0.4651 score: 0.9167 time: 0.08s
Epoch 210/1000, LR 0.000244
Train loss: 0.8141;  Loss pred: 0.8141; Loss self: 0.0000; time: 6.37s
Val loss: 0.5113 score: 0.8367 time: 0.10s
Test loss: 0.4621 score: 0.9167 time: 0.08s
Epoch 211/1000, LR 0.000244
Train loss: 0.8115;  Loss pred: 0.8115; Loss self: 0.0000; time: 0.26s
Val loss: 0.5089 score: 0.8367 time: 0.07s
Test loss: 0.4592 score: 0.9167 time: 0.08s
Epoch 212/1000, LR 0.000243
Train loss: 0.8083;  Loss pred: 0.8083; Loss self: 0.0000; time: 0.25s
Val loss: 0.5065 score: 0.8367 time: 0.07s
Test loss: 0.4562 score: 0.9167 time: 0.08s
Epoch 213/1000, LR 0.000243
Train loss: 0.8053;  Loss pred: 0.8053; Loss self: 0.0000; time: 0.25s
Val loss: 0.5042 score: 0.8367 time: 0.07s
Test loss: 0.4532 score: 0.9167 time: 0.08s
Epoch 214/1000, LR 0.000243
Train loss: 0.8060;  Loss pred: 0.8060; Loss self: 0.0000; time: 0.25s
Val loss: 0.5018 score: 0.8367 time: 0.07s
Test loss: 0.4502 score: 0.9167 time: 0.08s
Epoch 215/1000, LR 0.000243
Train loss: 0.8024;  Loss pred: 0.8024; Loss self: 0.0000; time: 0.25s
Val loss: 0.4995 score: 0.8367 time: 0.07s
Test loss: 0.4473 score: 0.9167 time: 0.08s
Epoch 216/1000, LR 0.000242
Train loss: 0.7983;  Loss pred: 0.7983; Loss self: 0.0000; time: 0.25s
Val loss: 0.4973 score: 0.8367 time: 0.07s
Test loss: 0.4444 score: 0.9167 time: 0.08s
Epoch 217/1000, LR 0.000242
Train loss: 0.7983;  Loss pred: 0.7983; Loss self: 0.0000; time: 0.25s
Val loss: 0.4951 score: 0.8367 time: 0.07s
Test loss: 0.4416 score: 0.9167 time: 0.08s
Epoch 218/1000, LR 0.000242
Train loss: 0.7949;  Loss pred: 0.7949; Loss self: 0.0000; time: 0.26s
Val loss: 0.4928 score: 0.8367 time: 0.08s
Test loss: 0.4387 score: 0.9167 time: 0.08s
Epoch 219/1000, LR 0.000242
Train loss: 0.7895;  Loss pred: 0.7895; Loss self: 0.0000; time: 0.27s
Val loss: 0.4906 score: 0.8367 time: 0.08s
Test loss: 0.4359 score: 0.9167 time: 0.08s
Epoch 220/1000, LR 0.000241
Train loss: 0.7871;  Loss pred: 0.7871; Loss self: 0.0000; time: 0.26s
Val loss: 0.4885 score: 0.8367 time: 0.08s
Test loss: 0.4331 score: 0.9167 time: 0.09s
Epoch 221/1000, LR 0.000241
Train loss: 0.7858;  Loss pred: 0.7858; Loss self: 0.0000; time: 5.44s
Val loss: 0.4863 score: 0.8367 time: 0.39s
Test loss: 0.4303 score: 0.9167 time: 1.43s
Epoch 222/1000, LR 0.000241
Train loss: 0.7816;  Loss pred: 0.7816; Loss self: 0.0000; time: 1.89s
Val loss: 0.4841 score: 0.8367 time: 0.08s
Test loss: 0.4274 score: 0.9167 time: 0.08s
Epoch 223/1000, LR 0.000241
Train loss: 0.7801;  Loss pred: 0.7801; Loss self: 0.0000; time: 0.25s
Val loss: 0.4819 score: 0.8367 time: 0.08s
Test loss: 0.4246 score: 0.9167 time: 0.08s
Epoch 224/1000, LR 0.000240
Train loss: 0.7786;  Loss pred: 0.7786; Loss self: 0.0000; time: 0.25s
Val loss: 0.4797 score: 0.8367 time: 0.07s
Test loss: 0.4218 score: 0.9167 time: 0.08s
Epoch 225/1000, LR 0.000240
Train loss: 0.7758;  Loss pred: 0.7758; Loss self: 0.0000; time: 0.26s
Val loss: 0.4775 score: 0.8367 time: 0.07s
Test loss: 0.4190 score: 0.9167 time: 0.08s
Epoch 226/1000, LR 0.000240
Train loss: 0.7737;  Loss pred: 0.7737; Loss self: 0.0000; time: 0.25s
Val loss: 0.4754 score: 0.8367 time: 0.08s
Test loss: 0.4162 score: 0.9167 time: 0.08s
Epoch 227/1000, LR 0.000240
Train loss: 0.7701;  Loss pred: 0.7701; Loss self: 0.0000; time: 0.25s
Val loss: 0.4732 score: 0.8367 time: 0.07s
Test loss: 0.4134 score: 0.9167 time: 0.08s
Epoch 228/1000, LR 0.000239
Train loss: 0.7694;  Loss pred: 0.7694; Loss self: 0.0000; time: 0.25s
Val loss: 0.4710 score: 0.8367 time: 0.07s
Test loss: 0.4106 score: 0.9167 time: 0.08s
Epoch 229/1000, LR 0.000239
Train loss: 0.7654;  Loss pred: 0.7654; Loss self: 0.0000; time: 0.28s
Val loss: 0.4688 score: 0.8367 time: 0.07s
Test loss: 0.4078 score: 0.9167 time: 0.08s
Epoch 230/1000, LR 0.000239
Train loss: 0.7641;  Loss pred: 0.7641; Loss self: 0.0000; time: 0.25s
Val loss: 0.4666 score: 0.8367 time: 0.07s
Test loss: 0.4050 score: 0.9167 time: 0.08s
Epoch 231/1000, LR 0.000238
Train loss: 0.7609;  Loss pred: 0.7609; Loss self: 0.0000; time: 0.25s
Val loss: 0.4644 score: 0.8367 time: 0.08s
Test loss: 0.4022 score: 0.9167 time: 0.08s
Epoch 232/1000, LR 0.000238
Train loss: 0.7585;  Loss pred: 0.7585; Loss self: 0.0000; time: 0.26s
Val loss: 0.4622 score: 0.8367 time: 0.07s
Test loss: 0.3994 score: 0.9167 time: 0.08s
Epoch 233/1000, LR 0.000238
Train loss: 0.7560;  Loss pred: 0.7560; Loss self: 0.0000; time: 0.25s
Val loss: 0.4599 score: 0.8367 time: 0.08s
Test loss: 0.3965 score: 0.9167 time: 0.08s
Epoch 234/1000, LR 0.000238
Train loss: 0.7535;  Loss pred: 0.7535; Loss self: 0.0000; time: 0.37s
Val loss: 0.4578 score: 0.8367 time: 1.35s
Test loss: 0.3938 score: 0.9167 time: 1.56s
Epoch 235/1000, LR 0.000237
Train loss: 0.7511;  Loss pred: 0.7511; Loss self: 0.0000; time: 3.41s
Val loss: 0.4557 score: 0.8367 time: 0.08s
Test loss: 0.3910 score: 0.9167 time: 0.08s
Epoch 236/1000, LR 0.000237
Train loss: 0.7489;  Loss pred: 0.7489; Loss self: 0.0000; time: 0.27s
Val loss: 0.4536 score: 0.8367 time: 0.08s
Test loss: 0.3883 score: 0.9167 time: 0.08s
Epoch 237/1000, LR 0.000237
Train loss: 0.7471;  Loss pred: 0.7471; Loss self: 0.0000; time: 0.26s
Val loss: 0.4516 score: 0.8367 time: 0.08s
Test loss: 0.3857 score: 0.9167 time: 0.08s
Epoch 238/1000, LR 0.000236
Train loss: 0.7456;  Loss pred: 0.7456; Loss self: 0.0000; time: 0.26s
Val loss: 0.4496 score: 0.8367 time: 0.08s
Test loss: 0.3831 score: 0.9167 time: 0.08s
Epoch 239/1000, LR 0.000236
Train loss: 0.7426;  Loss pred: 0.7426; Loss self: 0.0000; time: 0.26s
Val loss: 0.4476 score: 0.8367 time: 0.08s
Test loss: 0.3806 score: 0.9167 time: 0.08s
Epoch 240/1000, LR 0.000236
Train loss: 0.7381;  Loss pred: 0.7381; Loss self: 0.0000; time: 0.26s
Val loss: 0.4457 score: 0.8367 time: 0.08s
Test loss: 0.3781 score: 0.9167 time: 0.08s
Epoch 241/1000, LR 0.000236
Train loss: 0.7377;  Loss pred: 0.7377; Loss self: 0.0000; time: 0.26s
Val loss: 0.4439 score: 0.8367 time: 0.08s
Test loss: 0.3757 score: 0.9167 time: 0.08s
Epoch 242/1000, LR 0.000235
Train loss: 0.7375;  Loss pred: 0.7375; Loss self: 0.0000; time: 0.26s
Val loss: 0.4421 score: 0.8367 time: 0.08s
Test loss: 0.3732 score: 0.9167 time: 0.08s
Epoch 243/1000, LR 0.000235
Train loss: 0.7334;  Loss pred: 0.7334; Loss self: 0.0000; time: 0.26s
Val loss: 0.4403 score: 0.8367 time: 0.07s
Test loss: 0.3708 score: 0.9167 time: 0.08s
Epoch 244/1000, LR 0.000235
Train loss: 0.7322;  Loss pred: 0.7322; Loss self: 0.0000; time: 0.26s
Val loss: 0.4385 score: 0.8571 time: 0.08s
Test loss: 0.3684 score: 0.9167 time: 0.08s
Epoch 245/1000, LR 0.000234
Train loss: 0.7263;  Loss pred: 0.7263; Loss self: 0.0000; time: 0.26s
Val loss: 0.4366 score: 0.8571 time: 0.08s
Test loss: 0.3660 score: 0.9167 time: 0.08s
Epoch 246/1000, LR 0.000234
Train loss: 0.7259;  Loss pred: 0.7259; Loss self: 0.0000; time: 0.26s
Val loss: 0.4349 score: 0.8571 time: 0.08s
Test loss: 0.3637 score: 0.9167 time: 0.08s
Epoch 247/1000, LR 0.000234
Train loss: 0.7243;  Loss pred: 0.7243; Loss self: 0.0000; time: 0.26s
Val loss: 0.4330 score: 0.8571 time: 0.08s
Test loss: 0.3612 score: 0.9167 time: 0.08s
Epoch 248/1000, LR 0.000234
Train loss: 0.7224;  Loss pred: 0.7224; Loss self: 0.0000; time: 0.26s
Val loss: 0.4309 score: 0.8571 time: 0.07s
Test loss: 0.3586 score: 0.9167 time: 0.08s
Epoch 249/1000, LR 0.000233
Train loss: 0.7208;  Loss pred: 0.7208; Loss self: 0.0000; time: 0.26s
Val loss: 0.4290 score: 0.8571 time: 0.07s
Test loss: 0.3560 score: 0.9167 time: 0.08s
Epoch 250/1000, LR 0.000233
Train loss: 0.7181;  Loss pred: 0.7181; Loss self: 0.0000; time: 0.26s
Val loss: 0.4270 score: 0.8571 time: 0.07s
Test loss: 0.3535 score: 0.9167 time: 0.08s
Epoch 251/1000, LR 0.000233
Train loss: 0.7163;  Loss pred: 0.7163; Loss self: 0.0000; time: 0.26s
Val loss: 0.4251 score: 0.8571 time: 0.07s
Test loss: 0.3510 score: 0.9167 time: 0.08s
Epoch 252/1000, LR 0.000232
Train loss: 0.7119;  Loss pred: 0.7119; Loss self: 0.0000; time: 0.26s
Val loss: 0.4233 score: 0.8571 time: 0.08s
Test loss: 0.3487 score: 0.9167 time: 0.08s
Epoch 253/1000, LR 0.000232
Train loss: 0.7136;  Loss pred: 0.7136; Loss self: 0.0000; time: 0.26s
Val loss: 0.4216 score: 0.8571 time: 0.08s
Test loss: 0.3464 score: 0.9167 time: 0.08s
Epoch 254/1000, LR 0.000232
Train loss: 0.7098;  Loss pred: 0.7098; Loss self: 0.0000; time: 0.25s
Val loss: 0.4200 score: 0.8571 time: 0.07s
Test loss: 0.3442 score: 0.9167 time: 0.08s
Epoch 255/1000, LR 0.000232
Train loss: 0.7082;  Loss pred: 0.7082; Loss self: 0.0000; time: 0.25s
Val loss: 0.4184 score: 0.8571 time: 0.07s
Test loss: 0.3421 score: 0.9167 time: 0.08s
Epoch 256/1000, LR 0.000231
Train loss: 0.7061;  Loss pred: 0.7061; Loss self: 0.0000; time: 0.25s
Val loss: 0.4170 score: 0.8571 time: 0.07s
Test loss: 0.3401 score: 0.9167 time: 0.08s
Epoch 257/1000, LR 0.000231
Train loss: 0.7058;  Loss pred: 0.7058; Loss self: 0.0000; time: 0.25s
Val loss: 0.4155 score: 0.8571 time: 0.07s
Test loss: 0.3381 score: 0.9167 time: 0.08s
Epoch 258/1000, LR 0.000231
Train loss: 0.7007;  Loss pred: 0.7007; Loss self: 0.0000; time: 0.25s
Val loss: 0.4141 score: 0.8571 time: 0.07s
Test loss: 0.3361 score: 0.9167 time: 0.08s
Epoch 259/1000, LR 0.000230
Train loss: 0.7004;  Loss pred: 0.7004; Loss self: 0.0000; time: 0.25s
Val loss: 0.4127 score: 0.8571 time: 0.07s
Test loss: 0.3341 score: 0.9167 time: 0.08s
Epoch 260/1000, LR 0.000230
Train loss: 0.6979;  Loss pred: 0.6979; Loss self: 0.0000; time: 0.25s
Val loss: 0.4112 score: 0.8571 time: 0.07s
Test loss: 0.3321 score: 0.9167 time: 0.08s
Epoch 261/1000, LR 0.000230
Train loss: 0.6941;  Loss pred: 0.6941; Loss self: 0.0000; time: 0.25s
Val loss: 0.4097 score: 0.8571 time: 0.08s
Test loss: 0.3301 score: 0.9167 time: 0.08s
Epoch 262/1000, LR 0.000229
Train loss: 0.6946;  Loss pred: 0.6946; Loss self: 0.0000; time: 0.25s
Val loss: 0.4083 score: 0.8571 time: 0.07s
Test loss: 0.3281 score: 0.9167 time: 0.08s
Epoch 263/1000, LR 0.000229
Train loss: 0.6921;  Loss pred: 0.6921; Loss self: 0.0000; time: 0.25s
Val loss: 0.4068 score: 0.8571 time: 0.07s
Test loss: 0.3261 score: 0.9167 time: 0.08s
Epoch 264/1000, LR 0.000229
Train loss: 0.6912;  Loss pred: 0.6912; Loss self: 0.0000; time: 0.26s
Val loss: 0.4051 score: 0.8571 time: 0.07s
Test loss: 0.3239 score: 0.9167 time: 0.08s
Epoch 265/1000, LR 0.000228
Train loss: 0.6906;  Loss pred: 0.6906; Loss self: 0.0000; time: 0.26s
Val loss: 0.4035 score: 0.8571 time: 0.07s
Test loss: 0.3218 score: 0.9167 time: 0.08s
Epoch 266/1000, LR 0.000228
Train loss: 0.6874;  Loss pred: 0.6874; Loss self: 0.0000; time: 0.26s
Val loss: 0.4020 score: 0.8571 time: 0.07s
Test loss: 0.3197 score: 0.9167 time: 0.08s
Epoch 267/1000, LR 0.000228
Train loss: 0.6844;  Loss pred: 0.6844; Loss self: 0.0000; time: 0.26s
Val loss: 0.4005 score: 0.8571 time: 0.07s
Test loss: 0.3177 score: 0.9167 time: 0.08s
Epoch 268/1000, LR 0.000228
Train loss: 0.6845;  Loss pred: 0.6845; Loss self: 0.0000; time: 0.27s
Val loss: 0.3990 score: 0.8571 time: 0.08s
Test loss: 0.3156 score: 0.9167 time: 0.09s
Epoch 269/1000, LR 0.000227
Train loss: 0.6819;  Loss pred: 0.6819; Loss self: 0.0000; time: 0.27s
Val loss: 0.3974 score: 0.8571 time: 0.08s
Test loss: 0.3136 score: 0.9167 time: 0.08s
Epoch 270/1000, LR 0.000227
Train loss: 0.6795;  Loss pred: 0.6795; Loss self: 0.0000; time: 0.27s
Val loss: 0.3959 score: 0.8571 time: 0.08s
Test loss: 0.3116 score: 0.9167 time: 0.08s
Epoch 271/1000, LR 0.000227
Train loss: 0.6779;  Loss pred: 0.6779; Loss self: 0.0000; time: 0.27s
Val loss: 0.3944 score: 0.8571 time: 0.41s
Test loss: 0.3096 score: 0.9167 time: 1.85s
Epoch 272/1000, LR 0.000226
Train loss: 0.6769;  Loss pred: 0.6769; Loss self: 0.0000; time: 3.36s
Val loss: 0.3930 score: 0.8571 time: 0.39s
Test loss: 0.3077 score: 0.9167 time: 0.12s
Epoch 273/1000, LR 0.000226
Train loss: 0.6740;  Loss pred: 0.6740; Loss self: 0.0000; time: 0.26s
Val loss: 0.3916 score: 0.8571 time: 0.07s
Test loss: 0.3057 score: 0.9167 time: 0.07s
Epoch 274/1000, LR 0.000226
Train loss: 0.6735;  Loss pred: 0.6735; Loss self: 0.0000; time: 0.25s
Val loss: 0.3902 score: 0.8571 time: 0.07s
Test loss: 0.3038 score: 0.9167 time: 0.08s
Epoch 275/1000, LR 0.000225
Train loss: 0.6718;  Loss pred: 0.6718; Loss self: 0.0000; time: 0.25s
Val loss: 0.3890 score: 0.8571 time: 0.07s
Test loss: 0.3021 score: 0.9167 time: 0.08s
Epoch 276/1000, LR 0.000225
Train loss: 0.6702;  Loss pred: 0.6702; Loss self: 0.0000; time: 0.25s
Val loss: 0.3878 score: 0.8571 time: 0.07s
Test loss: 0.3003 score: 0.9167 time: 0.08s
Epoch 277/1000, LR 0.000225
Train loss: 0.6685;  Loss pred: 0.6685; Loss self: 0.0000; time: 0.25s
Val loss: 0.3866 score: 0.8571 time: 0.07s
Test loss: 0.2986 score: 0.9167 time: 0.08s
Epoch 278/1000, LR 0.000224
Train loss: 0.6668;  Loss pred: 0.6668; Loss self: 0.0000; time: 0.25s
Val loss: 0.3855 score: 0.8571 time: 0.07s
Test loss: 0.2969 score: 0.9167 time: 0.08s
Epoch 279/1000, LR 0.000224
Train loss: 0.6654;  Loss pred: 0.6654; Loss self: 0.0000; time: 0.25s
Val loss: 0.3843 score: 0.8571 time: 0.08s
Test loss: 0.2953 score: 0.9167 time: 0.08s
Epoch 280/1000, LR 0.000224
Train loss: 0.6623;  Loss pred: 0.6623; Loss self: 0.0000; time: 0.25s
Val loss: 0.3831 score: 0.8571 time: 0.08s
Test loss: 0.2935 score: 0.9167 time: 0.08s
Epoch 281/1000, LR 0.000223
Train loss: 0.6620;  Loss pred: 0.6620; Loss self: 0.0000; time: 0.25s
Val loss: 0.3818 score: 0.8571 time: 0.07s
Test loss: 0.2918 score: 0.9167 time: 0.08s
Epoch 282/1000, LR 0.000223
Train loss: 0.6612;  Loss pred: 0.6612; Loss self: 0.0000; time: 0.25s
Val loss: 0.3806 score: 0.8571 time: 0.07s
Test loss: 0.2900 score: 0.9167 time: 0.08s
Epoch 283/1000, LR 0.000223
Train loss: 0.6587;  Loss pred: 0.6587; Loss self: 0.0000; time: 0.25s
Val loss: 0.3794 score: 0.8571 time: 0.07s
Test loss: 0.2883 score: 0.9167 time: 0.08s
Epoch 284/1000, LR 0.000222
Train loss: 0.6598;  Loss pred: 0.6598; Loss self: 0.0000; time: 0.25s
Val loss: 0.3782 score: 0.8571 time: 0.07s
Test loss: 0.2866 score: 0.9167 time: 0.07s
Epoch 285/1000, LR 0.000222
Train loss: 0.6570;  Loss pred: 0.6570; Loss self: 0.0000; time: 0.25s
Val loss: 0.3770 score: 0.8571 time: 0.07s
Test loss: 0.2849 score: 0.9167 time: 0.07s
Epoch 286/1000, LR 0.000222
Train loss: 0.6551;  Loss pred: 0.6551; Loss self: 0.0000; time: 0.24s
Val loss: 0.3759 score: 0.8571 time: 0.07s
Test loss: 0.2833 score: 0.9167 time: 0.08s
Epoch 287/1000, LR 0.000221
Train loss: 0.6528;  Loss pred: 0.6528; Loss self: 0.0000; time: 0.25s
Val loss: 0.3748 score: 0.8571 time: 0.07s
Test loss: 0.2817 score: 0.9167 time: 0.08s
Epoch 288/1000, LR 0.000221
Train loss: 0.6520;  Loss pred: 0.6520; Loss self: 0.0000; time: 0.25s
Val loss: 0.3738 score: 0.8571 time: 0.07s
Test loss: 0.2802 score: 0.9167 time: 0.08s
Epoch 289/1000, LR 0.000221
Train loss: 0.6516;  Loss pred: 0.6516; Loss self: 0.0000; time: 0.26s
Val loss: 0.3727 score: 0.8571 time: 0.07s
Test loss: 0.2786 score: 0.9167 time: 0.08s
Epoch 290/1000, LR 0.000220
Train loss: 0.6500;  Loss pred: 0.6500; Loss self: 0.0000; time: 0.25s
Val loss: 0.3716 score: 0.8571 time: 0.07s
Test loss: 0.2770 score: 0.9167 time: 0.08s
Epoch 291/1000, LR 0.000220
Train loss: 0.6489;  Loss pred: 0.6489; Loss self: 0.0000; time: 0.25s
Val loss: 0.3706 score: 0.8571 time: 0.07s
Test loss: 0.2754 score: 0.9167 time: 0.08s
Epoch 292/1000, LR 0.000220
Train loss: 0.6464;  Loss pred: 0.6464; Loss self: 0.0000; time: 0.26s
Val loss: 0.3696 score: 0.8571 time: 0.08s
Test loss: 0.2739 score: 0.9167 time: 0.08s
Epoch 293/1000, LR 0.000219
Train loss: 0.6456;  Loss pred: 0.6456; Loss self: 0.0000; time: 0.25s
Val loss: 0.3686 score: 0.8571 time: 0.07s
Test loss: 0.2724 score: 0.9167 time: 0.08s
Epoch 294/1000, LR 0.000219
Train loss: 0.6441;  Loss pred: 0.6441; Loss self: 0.0000; time: 0.24s
Val loss: 0.3677 score: 0.8571 time: 0.07s
Test loss: 0.2710 score: 0.9167 time: 0.08s
Epoch 295/1000, LR 0.000219
Train loss: 0.6424;  Loss pred: 0.6424; Loss self: 0.0000; time: 0.24s
Val loss: 0.3668 score: 0.8571 time: 0.07s
Test loss: 0.2696 score: 0.9167 time: 0.07s
Epoch 296/1000, LR 0.000218
Train loss: 0.6424;  Loss pred: 0.6424; Loss self: 0.0000; time: 0.24s
Val loss: 0.3660 score: 0.8571 time: 0.07s
Test loss: 0.2684 score: 0.9167 time: 0.07s
Epoch 297/1000, LR 0.000218
Train loss: 0.6385;  Loss pred: 0.6385; Loss self: 0.0000; time: 0.24s
Val loss: 0.3651 score: 0.8571 time: 0.07s
Test loss: 0.2670 score: 0.9167 time: 0.07s
Epoch 298/1000, LR 0.000218
Train loss: 0.6393;  Loss pred: 0.6393; Loss self: 0.0000; time: 0.24s
Val loss: 0.3642 score: 0.8571 time: 0.07s
Test loss: 0.2656 score: 0.9167 time: 0.07s
Epoch 299/1000, LR 0.000217
Train loss: 0.6379;  Loss pred: 0.6379; Loss self: 0.0000; time: 0.24s
Val loss: 0.3633 score: 0.8571 time: 0.07s
Test loss: 0.2642 score: 0.9167 time: 0.07s
Epoch 300/1000, LR 0.000217
Train loss: 0.6378;  Loss pred: 0.6378; Loss self: 0.0000; time: 0.24s
Val loss: 0.3625 score: 0.8571 time: 0.07s
Test loss: 0.2629 score: 0.9167 time: 0.08s
Epoch 301/1000, LR 0.000217
Train loss: 0.6344;  Loss pred: 0.6344; Loss self: 0.0000; time: 3.16s
Val loss: 0.3618 score: 0.8571 time: 1.11s
Test loss: 0.2617 score: 0.9167 time: 0.19s
Epoch 302/1000, LR 0.000216
Train loss: 0.6346;  Loss pred: 0.6346; Loss self: 0.0000; time: 0.40s
Val loss: 0.3609 score: 0.8571 time: 0.08s
Test loss: 0.2603 score: 0.9167 time: 0.08s
Epoch 303/1000, LR 0.000216
Train loss: 0.6326;  Loss pred: 0.6326; Loss self: 0.0000; time: 0.26s
Val loss: 0.3600 score: 0.8571 time: 0.08s
Test loss: 0.2590 score: 0.9167 time: 0.08s
Epoch 304/1000, LR 0.000216
Train loss: 0.6318;  Loss pred: 0.6318; Loss self: 0.0000; time: 0.25s
Val loss: 0.3592 score: 0.8571 time: 0.07s
Test loss: 0.2577 score: 0.9167 time: 0.08s
Epoch 305/1000, LR 0.000215
Train loss: 0.6312;  Loss pred: 0.6312; Loss self: 0.0000; time: 0.25s
Val loss: 0.3584 score: 0.8571 time: 0.08s
Test loss: 0.2564 score: 0.9167 time: 0.08s
Epoch 306/1000, LR 0.000215
Train loss: 0.6303;  Loss pred: 0.6303; Loss self: 0.0000; time: 0.24s
Val loss: 0.3576 score: 0.8571 time: 0.07s
Test loss: 0.2551 score: 0.9167 time: 0.08s
Epoch 307/1000, LR 0.000215
Train loss: 0.6284;  Loss pred: 0.6284; Loss self: 0.0000; time: 0.25s
Val loss: 0.3568 score: 0.8571 time: 0.07s
Test loss: 0.2539 score: 0.9167 time: 0.08s
Epoch 308/1000, LR 0.000214
Train loss: 0.6279;  Loss pred: 0.6279; Loss self: 0.0000; time: 0.25s
Val loss: 0.3560 score: 0.8571 time: 1.84s
Test loss: 0.2527 score: 0.9167 time: 0.34s
Epoch 309/1000, LR 0.000214
Train loss: 0.6272;  Loss pred: 0.6272; Loss self: 0.0000; time: 0.27s
Val loss: 0.3553 score: 0.8571 time: 0.08s
Test loss: 0.2516 score: 0.9167 time: 0.08s
Epoch 310/1000, LR 0.000214
Train loss: 0.6251;  Loss pred: 0.6251; Loss self: 0.0000; time: 0.26s
Val loss: 0.3546 score: 0.8571 time: 0.08s
Test loss: 0.2504 score: 0.9167 time: 0.08s
Epoch 311/1000, LR 0.000213
Train loss: 0.6251;  Loss pred: 0.6251; Loss self: 0.0000; time: 0.26s
Val loss: 0.3540 score: 0.8571 time: 0.07s
Test loss: 0.2494 score: 0.9167 time: 0.08s
Epoch 312/1000, LR 0.000213
Train loss: 0.6228;  Loss pred: 0.6228; Loss self: 0.0000; time: 0.26s
Val loss: 0.3534 score: 0.8571 time: 0.08s
Test loss: 0.2483 score: 0.9167 time: 0.08s
Epoch 313/1000, LR 0.000213
Train loss: 0.6226;  Loss pred: 0.6226; Loss self: 0.0000; time: 0.26s
Val loss: 0.3527 score: 0.8571 time: 0.08s
Test loss: 0.2473 score: 0.9167 time: 0.08s
Epoch 314/1000, LR 0.000212
Train loss: 0.6211;  Loss pred: 0.6211; Loss self: 0.0000; time: 0.26s
Val loss: 0.3521 score: 0.8571 time: 0.08s
Test loss: 0.2462 score: 0.9167 time: 0.08s
Epoch 315/1000, LR 0.000212
Train loss: 0.6191;  Loss pred: 0.6191; Loss self: 0.0000; time: 0.26s
Val loss: 0.3515 score: 0.8571 time: 0.07s
Test loss: 0.2452 score: 0.9167 time: 0.08s
Epoch 316/1000, LR 0.000212
Train loss: 0.6201;  Loss pred: 0.6201; Loss self: 0.0000; time: 0.26s
Val loss: 0.3510 score: 0.8571 time: 0.08s
Test loss: 0.2443 score: 0.9167 time: 0.08s
Epoch 317/1000, LR 0.000211
Train loss: 0.6170;  Loss pred: 0.6170; Loss self: 0.0000; time: 0.26s
Val loss: 0.3503 score: 0.8571 time: 0.08s
Test loss: 0.2431 score: 0.9167 time: 0.08s
Epoch 318/1000, LR 0.000211
Train loss: 0.6179;  Loss pred: 0.6179; Loss self: 0.0000; time: 0.25s
Val loss: 0.3498 score: 0.8571 time: 0.08s
Test loss: 0.2422 score: 0.9167 time: 0.08s
Epoch 319/1000, LR 0.000210
Train loss: 0.6163;  Loss pred: 0.6163; Loss self: 0.0000; time: 0.26s
Val loss: 0.3492 score: 0.8571 time: 0.08s
Test loss: 0.2412 score: 0.9167 time: 0.08s
Epoch 320/1000, LR 0.000210
Train loss: 0.6139;  Loss pred: 0.6139; Loss self: 0.0000; time: 0.26s
Val loss: 0.3486 score: 0.8571 time: 0.08s
Test loss: 0.2402 score: 0.9167 time: 0.08s
Epoch 321/1000, LR 0.000210
Train loss: 0.6131;  Loss pred: 0.6131; Loss self: 0.0000; time: 0.28s
Val loss: 0.3479 score: 0.8571 time: 0.08s
Test loss: 0.2391 score: 0.9167 time: 0.08s
Epoch 322/1000, LR 0.000209
Train loss: 0.6134;  Loss pred: 0.6134; Loss self: 0.0000; time: 0.26s
Val loss: 0.3473 score: 0.8571 time: 0.08s
Test loss: 0.2381 score: 0.9167 time: 0.08s
Epoch 323/1000, LR 0.000209
Train loss: 0.6130;  Loss pred: 0.6130; Loss self: 0.0000; time: 0.25s
Val loss: 0.3468 score: 0.8571 time: 0.08s
Test loss: 0.2372 score: 0.9167 time: 0.08s
Epoch 324/1000, LR 0.000209
Train loss: 0.6107;  Loss pred: 0.6107; Loss self: 0.0000; time: 0.26s
Val loss: 0.3462 score: 0.8571 time: 0.08s
Test loss: 0.2362 score: 0.9167 time: 0.08s
Epoch 325/1000, LR 0.000208
Train loss: 0.6101;  Loss pred: 0.6101; Loss self: 0.0000; time: 3.87s
Val loss: 0.3454 score: 0.8571 time: 1.35s
Test loss: 0.2350 score: 0.9167 time: 1.29s
Epoch 326/1000, LR 0.000208
Train loss: 0.6095;  Loss pred: 0.6095; Loss self: 0.0000; time: 1.09s
Val loss: 0.3448 score: 0.8571 time: 0.07s
Test loss: 0.2340 score: 0.9167 time: 0.08s
Epoch 327/1000, LR 0.000208
Train loss: 0.6095;  Loss pred: 0.6095; Loss self: 0.0000; time: 0.26s
Val loss: 0.3442 score: 0.8571 time: 0.07s
Test loss: 0.2330 score: 0.9167 time: 0.08s
Epoch 328/1000, LR 0.000207
Train loss: 0.6073;  Loss pred: 0.6073; Loss self: 0.0000; time: 0.24s
Val loss: 0.3438 score: 0.8571 time: 0.07s
Test loss: 0.2322 score: 0.9167 time: 0.07s
Epoch 329/1000, LR 0.000207
Train loss: 0.6053;  Loss pred: 0.6053; Loss self: 0.0000; time: 0.24s
Val loss: 0.3434 score: 0.8571 time: 0.07s
Test loss: 0.2314 score: 0.9167 time: 0.07s
Epoch 330/1000, LR 0.000207
Train loss: 0.6047;  Loss pred: 0.6047; Loss self: 0.0000; time: 0.24s
Val loss: 0.3429 score: 0.8571 time: 0.07s
Test loss: 0.2306 score: 0.9167 time: 0.07s
Epoch 331/1000, LR 0.000206
Train loss: 0.6044;  Loss pred: 0.6044; Loss self: 0.0000; time: 0.24s
Val loss: 0.3426 score: 0.8571 time: 0.07s
Test loss: 0.2299 score: 0.9167 time: 0.08s
Epoch 332/1000, LR 0.000206
Train loss: 0.6045;  Loss pred: 0.6045; Loss self: 0.0000; time: 0.25s
Val loss: 0.3424 score: 0.8571 time: 0.07s
Test loss: 0.2292 score: 0.9167 time: 0.08s
Epoch 333/1000, LR 0.000205
Train loss: 0.6038;  Loss pred: 0.6038; Loss self: 0.0000; time: 0.25s
Val loss: 0.3422 score: 0.8571 time: 0.07s
Test loss: 0.2287 score: 0.9167 time: 0.08s
Epoch 334/1000, LR 0.000205
Train loss: 0.6029;  Loss pred: 0.6029; Loss self: 0.0000; time: 0.25s
Val loss: 0.3420 score: 0.8571 time: 0.07s
Test loss: 0.2281 score: 0.9167 time: 0.08s
Epoch 335/1000, LR 0.000205
Train loss: 0.6017;  Loss pred: 0.6017; Loss self: 0.0000; time: 1.43s
Val loss: 0.3418 score: 0.8571 time: 1.16s
Test loss: 0.2275 score: 0.9167 time: 0.92s
Epoch 336/1000, LR 0.000204
Train loss: 0.6016;  Loss pred: 0.6016; Loss self: 0.0000; time: 1.29s
Val loss: 0.3414 score: 0.8571 time: 0.08s
Test loss: 0.2268 score: 0.9167 time: 0.08s
Epoch 337/1000, LR 0.000204
Train loss: 0.6005;  Loss pred: 0.6005; Loss self: 0.0000; time: 0.25s
Val loss: 0.3410 score: 0.8571 time: 0.07s
Test loss: 0.2259 score: 0.9167 time: 0.08s
Epoch 338/1000, LR 0.000204
Train loss: 0.6005;  Loss pred: 0.6005; Loss self: 0.0000; time: 0.24s
Val loss: 0.3405 score: 0.8571 time: 0.07s
Test loss: 0.2251 score: 0.9167 time: 0.08s
Epoch 339/1000, LR 0.000203
Train loss: 0.5984;  Loss pred: 0.5984; Loss self: 0.0000; time: 0.24s
Val loss: 0.3401 score: 0.8571 time: 0.07s
Test loss: 0.2243 score: 0.9167 time: 0.07s
Epoch 340/1000, LR 0.000203
Train loss: 0.5971;  Loss pred: 0.5971; Loss self: 0.0000; time: 0.24s
Val loss: 0.3397 score: 0.8571 time: 0.07s
Test loss: 0.2235 score: 0.9167 time: 0.07s
Epoch 341/1000, LR 0.000203
Train loss: 0.5965;  Loss pred: 0.5965; Loss self: 0.0000; time: 0.24s
Val loss: 0.3393 score: 0.8571 time: 0.07s
Test loss: 0.2228 score: 0.9167 time: 0.08s
Epoch 342/1000, LR 0.000202
Train loss: 0.5980;  Loss pred: 0.5980; Loss self: 0.0000; time: 0.24s
Val loss: 0.3389 score: 0.8571 time: 0.07s
Test loss: 0.2220 score: 0.9167 time: 0.07s
Epoch 343/1000, LR 0.000202
Train loss: 0.5959;  Loss pred: 0.5959; Loss self: 0.0000; time: 0.24s
Val loss: 0.3385 score: 0.8571 time: 0.07s
Test loss: 0.2212 score: 0.9167 time: 0.07s
Epoch 344/1000, LR 0.000201
Train loss: 0.5942;  Loss pred: 0.5942; Loss self: 0.0000; time: 0.24s
Val loss: 0.3382 score: 0.8571 time: 0.07s
Test loss: 0.2205 score: 0.9167 time: 0.07s
Epoch 345/1000, LR 0.000201
Train loss: 0.5948;  Loss pred: 0.5948; Loss self: 0.0000; time: 0.24s
Val loss: 0.3380 score: 0.8571 time: 0.07s
Test loss: 0.2199 score: 0.9167 time: 0.07s
Epoch 346/1000, LR 0.000201
Train loss: 0.5938;  Loss pred: 0.5938; Loss self: 0.0000; time: 0.24s
Val loss: 0.3378 score: 0.8571 time: 0.07s
Test loss: 0.2193 score: 0.9167 time: 0.07s
Epoch 347/1000, LR 0.000200
Train loss: 0.5937;  Loss pred: 0.5937; Loss self: 0.0000; time: 0.24s
Val loss: 0.3376 score: 0.8571 time: 0.07s
Test loss: 0.2187 score: 0.9167 time: 0.07s
Epoch 348/1000, LR 0.000200
Train loss: 0.5912;  Loss pred: 0.5912; Loss self: 0.0000; time: 0.24s
Val loss: 0.3373 score: 0.8571 time: 0.07s
Test loss: 0.2181 score: 0.9167 time: 0.07s
Epoch 349/1000, LR 0.000200
Train loss: 0.5911;  Loss pred: 0.5911; Loss self: 0.0000; time: 0.24s
Val loss: 0.3371 score: 0.8571 time: 0.07s
Test loss: 0.2175 score: 0.9167 time: 0.07s
Epoch 350/1000, LR 0.000199
Train loss: 0.5904;  Loss pred: 0.5904; Loss self: 0.0000; time: 0.24s
Val loss: 0.3369 score: 0.8571 time: 0.07s
Test loss: 0.2169 score: 0.9167 time: 0.07s
Epoch 351/1000, LR 0.000199
Train loss: 0.5894;  Loss pred: 0.5894; Loss self: 0.0000; time: 0.24s
Val loss: 0.3367 score: 0.8571 time: 0.07s
Test loss: 0.2164 score: 0.9167 time: 0.07s
Epoch 352/1000, LR 0.000198
Train loss: 0.5897;  Loss pred: 0.5897; Loss self: 0.0000; time: 0.24s
Val loss: 0.3366 score: 0.8571 time: 0.07s
Test loss: 0.2159 score: 0.9167 time: 0.07s
Epoch 353/1000, LR 0.000198
Train loss: 0.5894;  Loss pred: 0.5894; Loss self: 0.0000; time: 0.24s
Val loss: 0.3364 score: 0.8571 time: 0.07s
Test loss: 0.2154 score: 0.9167 time: 0.07s
Epoch 354/1000, LR 0.000198
Train loss: 0.5884;  Loss pred: 0.5884; Loss self: 0.0000; time: 0.24s
Val loss: 0.3362 score: 0.8571 time: 0.07s
Test loss: 0.2148 score: 0.9167 time: 0.07s
Epoch 355/1000, LR 0.000197
Train loss: 0.5873;  Loss pred: 0.5873; Loss self: 0.0000; time: 0.26s
Val loss: 0.3360 score: 0.8571 time: 0.07s
Test loss: 0.2142 score: 0.9167 time: 0.08s
Epoch 356/1000, LR 0.000197
Train loss: 0.5866;  Loss pred: 0.5866; Loss self: 0.0000; time: 0.26s
Val loss: 0.3358 score: 0.8571 time: 0.08s
Test loss: 0.2136 score: 0.9167 time: 0.08s
Epoch 357/1000, LR 0.000196
Train loss: 0.5859;  Loss pred: 0.5859; Loss self: 0.0000; time: 0.26s
Val loss: 0.3356 score: 0.8571 time: 0.08s
Test loss: 0.2131 score: 0.9167 time: 0.08s
Epoch 358/1000, LR 0.000196
Train loss: 0.5855;  Loss pred: 0.5855; Loss self: 0.0000; time: 0.26s
Val loss: 0.3353 score: 0.8571 time: 0.08s
Test loss: 0.2125 score: 0.9167 time: 0.08s
Epoch 359/1000, LR 0.000196
Train loss: 0.5848;  Loss pred: 0.5848; Loss self: 0.0000; time: 0.26s
Val loss: 0.3351 score: 0.8571 time: 0.07s
Test loss: 0.2119 score: 0.9167 time: 0.07s
Epoch 360/1000, LR 0.000195
Train loss: 0.5855;  Loss pred: 0.5855; Loss self: 0.0000; time: 0.25s
Val loss: 0.3349 score: 0.8571 time: 0.07s
Test loss: 0.2113 score: 0.9167 time: 0.08s
Epoch 361/1000, LR 0.000195
Train loss: 0.5846;  Loss pred: 0.5846; Loss self: 0.0000; time: 0.24s
Val loss: 0.3347 score: 0.8571 time: 0.07s
Test loss: 0.2108 score: 0.9167 time: 0.08s
Epoch 362/1000, LR 0.000195
Train loss: 0.5847;  Loss pred: 0.5847; Loss self: 0.0000; time: 0.24s
Val loss: 0.3345 score: 0.8571 time: 0.07s
Test loss: 0.2103 score: 0.9167 time: 0.08s
Epoch 363/1000, LR 0.000194
Train loss: 0.5836;  Loss pred: 0.5836; Loss self: 0.0000; time: 1.06s
Val loss: 0.3345 score: 0.8571 time: 1.50s
Test loss: 0.2099 score: 0.9167 time: 1.24s
Epoch 364/1000, LR 0.000194
Train loss: 0.5830;  Loss pred: 0.5830; Loss self: 0.0000; time: 3.99s
Val loss: 0.3345 score: 0.8571 time: 0.45s
Test loss: 0.2096 score: 0.9167 time: 0.37s
     INFO: Early stopping counter 1 of 2
Epoch 365/1000, LR 0.000193
Train loss: 0.5818;  Loss pred: 0.5818; Loss self: 0.0000; time: 2.04s
Val loss: 0.3345 score: 0.8571 time: 0.42s
Test loss: 0.2092 score: 0.9167 time: 0.28s
     INFO: Early stopping counter 2 of 2
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 362,   Train_Loss: 0.5836,   Val_Loss: 0.3345,   Val_Precision: 0.9091,   Val_Recall: 0.8000,   Val_accuracy: 0.8511,   Val_Score: 0.8571,   Val_Loss: 0.3345,   Test_Precision: 1.0000,   Test_Recall: 0.8333,   Test_accuracy: 0.9091,   Test_Score: 0.9167,   Test_loss: 0.2099


[0.07285241596400738, 0.07310730300378054, 0.07274554204195738, 0.07779006497003138, 0.07738581008743495, 0.07384479802567512, 2.768984962021932, 0.08275702700484544, 0.06890303699765354, 0.08429988392163068, 0.09145585203077644, 0.09746584808453918, 1.8477232749573886, 0.30587461101822555, 0.10243699292186648, 0.09528087405487895, 0.09322369110304862, 0.09372576104942709, 0.09315573296044022, 0.0937872719950974, 0.09322050504852086, 0.09365338901989162, 0.10540865897201002, 0.09250331996008754, 2.025694706942886, 0.6006730579538271, 0.25550520699471235, 0.0928001640131697, 0.09333751606754959, 0.0942672590026632, 0.0955293959705159, 0.0942400029161945, 2.0007414009887725, 0.9631382019724697, 0.09820243797730654, 0.09222086996305734, 0.09137349901720881, 0.09197054198011756, 0.0922232820885256, 0.09218762395903468, 0.09215614397544414, 0.0926953119924292, 0.09184444101992995, 0.09313041903078556, 2.231626717024483, 0.0911924650426954, 0.09048202098347247, 0.09073361603077501, 0.08963985904119909, 0.088379836990498, 0.08899841306265444, 0.08940204803366214, 0.09019203099887818, 0.08908963506110013, 1.5675729280337691, 0.11441070190630853, 0.09132417396176606, 0.09061576309613883, 0.09071535302791744, 0.09449626400601119, 0.09154808998573571, 0.09176901506725699, 0.0901662950636819, 0.6212077389936894, 0.4076754900161177, 0.1145588019862771, 0.10059829393867403, 0.09685184096451849, 0.09587441990152001, 0.09146754501853138, 0.0913835089886561, 0.09710890892893076, 0.09237471292726696, 0.09227871103212237, 0.11280902195721865, 0.5291376219829544, 0.7009190630633384, 0.08878692891448736, 0.0881393130403012, 0.08792791794985533, 0.08712372998706996, 0.08730692497920245, 0.08764419902581722, 0.08775428705848753, 0.08779307396616787, 0.08829399396199733, 0.08794465300161391, 0.08819919195957482, 0.08848579495679587, 0.08842606702819467, 0.08761513198260218, 0.0881393610034138, 0.08767104591242969, 0.08847337192855775, 0.08900243800599128, 0.08786768594291061, 0.08752786496188492, 0.08817626896779984, 0.09788822801783681, 1.5882117769215256, 0.08698581403587013, 0.08881500200368464, 0.0878309840336442, 0.0873082730686292, 0.08926935598719865, 0.0954120249953121, 0.08872128603979945, 0.08821507706306875, 0.08878858096431941, 0.09096545993816108, 0.09493056102655828, 0.10149786795955151, 0.09557001700159162, 0.09232568508014083, 0.09312881901860237, 0.09142170590348542, 0.10114602593239397, 0.09401850402355194, 0.09224524698220193, 0.09069662098772824, 0.09002049395348877, 0.09124232805334032, 0.09203007898759097, 0.14386461698450148, 0.09299333998933434, 0.09149630693718791, 2.3424065549625084, 0.29546451894566417, 0.09256751695647836, 0.09088025300297886, 0.09059923002496362, 0.09276438504457474, 0.09130054095294327, 0.09147536300588399, 0.09204723010770977, 0.09234215703327209, 0.09180717403069139, 1.6149079010356218, 0.09553759393747896, 0.08999391901306808, 0.08905608090572059, 0.08860610192641616, 0.08801395399495959, 0.08801055804360658, 0.0877555520273745, 0.08738333103246987, 0.08792124409228563, 0.08716262702364475, 0.09146387898363173, 0.09188390406779945, 0.09246815193910152, 0.09595494403038174, 0.09253654896747321, 0.09197067096829414, 0.09139782900456339, 0.09751216997392476, 0.10159637196920812, 0.09194338496308774, 0.09512234898284078, 0.7681029590312392, 1.468165852013044, 0.09389093401841819, 0.08610586496070027, 0.08897884597536176, 0.08769334305543453, 0.08632736303843558, 0.08714070497080684, 0.08685519802384079, 0.08728687197435647, 0.08698723802808672, 0.08770410600118339, 0.08710794197395444, 0.08719708200078458, 0.08673921402078122, 0.08671243698336184, 0.08677183603867888, 1.9404173309449106, 0.1279387720860541, 0.08831864991225302, 0.08829232701100409, 0.08937345701269805, 0.08712768997065723, 0.08738104708027095, 0.08790570101700723, 0.08783646405208856, 0.08926198596600443, 1.583307786961086, 0.09599048306699842, 0.08714247297029942, 0.08797865093220025, 0.08728561899624765, 0.08744452695827931, 0.09097465802915394, 0.09466177097056061, 0.09441806795075536, 0.08940675295889378, 0.08833408902864903, 0.08879181393422186, 0.08775366109330207, 0.0847684689797461, 0.08561920595820993, 0.08612996607553214, 0.0844131310004741, 0.08892288396600634, 0.08467888296581805, 0.08696554007474333, 0.08522571995854378, 0.08645314397290349, 0.08571698295418173, 0.08563920098822564, 0.0845157119911164, 0.08541176095604897, 0.08597755094524473, 0.08502023399341851, 0.08905581699218601, 0.11324689106550068, 0.08982858504168689, 0.2652058450039476, 0.09585253591649234, 0.09198635094799101, 0.09239908400923014, 0.09230886399745941, 0.09208384295925498, 1.4392001880332828, 0.5704417580273002, 0.0947235330240801, 0.09148859104607254, 0.09118617605417967, 0.09130325797013938, 0.09204202808905393, 0.09134924306999892, 0.09168934402987361, 0.09234005399048328, 0.09121411899104714, 0.09367470897268504, 0.09258739906363189, 0.09185893007088453, 0.15829253697302192, 0.3325400640023872, 0.09514019289053977, 0.09105533198453486, 0.09203948790673167, 0.09279980091378093, 0.09209690697025508, 0.09150955500081182, 0.09328558400738984, 0.09252145304344594, 0.0935427249642089, 1.1681556949624792, 0.7744131260551512, 1.7821935990359634, 0.3556343800155446, 1.1166080579860136, 0.09367983299307525, 0.09181717305909842, 0.0906438329257071, 0.09578897594474256, 0.08754955208860338, 0.08772529102861881, 0.10411030892282724, 0.08925269602332264, 0.09132515301462263, 0.08929374592844397, 0.08651498798280954, 2.451538105029613, 0.09497874404769391, 0.08746278996113688, 0.08932248700875789, 0.0947071110131219, 0.09569301805458963, 0.0881232360843569, 0.0892641139216721, 0.08824035304132849, 2.2156366870040074, 0.1509447549469769, 0.08715335302986205, 0.08744212798774242, 0.08766887604724616, 0.08810982701834291, 0.08672311797272414, 0.08670445892494172, 0.08675198594573885, 0.08712792699225247, 0.08703964296728373, 0.08725042501464486, 0.481179291033186, 0.10426913399714977, 0.0901967230020091, 0.09162944392301142, 0.09162393398582935, 0.09410286403726786, 0.09145194303710014, 0.0917247609468177, 0.09020038088783622, 0.20064386806916445, 0.08856179600115865, 0.0836206910898909, 0.08824826101772487, 0.09011134598404169, 0.09497060708235949, 0.09428409195970744, 0.08806898701004684, 0.09453116194345057, 0.09490963199641556, 0.0965501150349155, 0.09373242303263396, 0.09170654695481062, 0.09187078208196908, 0.08662984904367477, 0.09157106303609908, 0.09195898799225688, 0.09538929397240281, 0.09135302796494216, 0.09117086394689977, 0.7323360710870475, 0.2001710799522698, 0.10940417798701674, 0.09174848801922053, 0.09165921900421381, 0.09156101394910365, 0.09129280701745301, 0.09201429400127381, 0.09164166508708149, 0.09019411099143326, 0.09046756406314671, 0.09123749798163772, 0.09173879004083574, 0.09147001907695085, 0.169532747939229, 0.09281188703607768, 0.09216267103329301, 0.08242591400630772, 0.08167752297595143, 0.08327601198107004, 0.08176768501289189, 0.15392091101966798, 0.3016918071079999, 0.15960361901670694, 0.08552816405426711, 0.08028403599746525, 0.08017277205362916, 0.08022780006285757, 0.0810994349885732, 0.08028740901499987, 0.08001070597674698, 0.08064640092197806, 0.08079418202396482, 0.08050941291730851, 0.08038408507127315, 0.08038406004197896, 0.08060774300247431, 0.08114954095799476, 0.08063690504059196, 0.08069275203160942, 1.214892231975682, 0.08084395003970712, 0.07845211308449507, 0.07849885697942227, 0.07757899106945843, 0.08298694295808673, 0.0778551819967106, 0.07850508706178516, 0.07838465401437134, 0.7320524349343032, 0.08458870509639382, 0.07813680509570986, 0.07633036398328841, 0.07568791997618973, 0.08217860606964678, 0.08111342904157937, 0.07774543797131628, 0.07798742805607617, 0.08194066700525582, 0.08080005901865661, 0.08089208498131484, 0.08063810598105192, 0.08173070196062326, 0.08047429600264877, 0.08220419497229159, 0.0818802829599008, 0.07668897008989006, 1.6824126789579168, 0.8481330929789692, 0.23359949002042413, 0.07947728701401502, 0.07928795099724084, 0.08050640800502151, 0.08039477397687733, 0.07914392603561282, 0.07871328201144934, 0.07868913700804114, 0.0823392840102315, 0.09236265206709504, 0.08830144698731601, 0.08179404004476964, 0.6365285150241107, 0.4376748090144247, 0.08054331294260919, 0.08752461289986968, 0.09655875898897648, 0.07905898906756192, 0.07918383902870119, 0.07964318594895303, 0.08216240198817104, 0.07867902796715498, 0.07853181194514036, 0.0781985999783501, 0.08029818907380104, 0.07802110002376139, 0.07897604105528444, 0.07880772498901933, 0.08337228698655963, 0.07890230091288686, 0.08783879992552102, 0.07934067503083497, 0.07892806199379265, 0.08984624897129834, 0.08297083899378777, 0.08224792708642781, 0.08176354703027755, 0.08110794005915523, 0.08215964399278164, 0.08071262901648879, 0.08322660799603909, 0.08171101601328701, 0.08175521099474281, 0.08110822702292353, 0.08031107403803617, 0.07862264499999583, 0.07648924400564283, 0.07640236790757626, 0.07680521893780679, 0.07683951407670975, 0.081377616035752, 0.08146124496124685, 0.08138263609725982, 0.08168992004357278, 0.08590262394864112, 0.09310570498928428, 0.08332003001123667, 0.08316549006849527, 0.08101788500789553, 0.08102718100417405, 0.08133502304553986, 0.08145544794388115, 0.08190002094488591, 0.08153672493062913, 0.08203342091292143, 0.08267120807431638, 0.08252893399912864, 1.8328688270412385, 0.08599105989560485, 0.08918920007999986, 0.08302952407393605, 0.08053338201716542, 0.08033759903628379, 0.08006142301019281, 0.08022727200295776, 0.08052532002329826, 0.0801610219059512, 0.08107346401084214, 0.08112926303874701, 0.08104229101445526, 1.2707499449606985, 0.08728759596124291, 0.0808567920466885, 0.0804162360727787, 0.07999174296855927, 0.08064804703462869, 0.0808765129186213, 0.08354770403821021, 0.07993391808122396, 0.07946283207274973, 0.08010160701815039, 0.08057510701473802, 0.0798088600859046, 0.07579186395741999, 0.07587259402498603, 0.07900566898752004, 0.07653736893553287, 0.07689727505203336, 0.07677668204996735, 0.07647841505240649, 0.08040128205902874, 0.07845348306000233, 1.3418737560277805, 0.0823540260316804, 0.07893633900675923, 0.07791112805716693, 0.07731144304852933, 0.07851563196163625, 0.07799357001204044, 0.13508552906569093, 0.07833513699006289, 0.07726406992878765, 0.07856832700781524, 1.0488551809685305, 0.09842248097993433, 0.08370434900280088, 0.08292883902322501, 0.08265506604220718, 0.08155600202735513, 0.08332706103101373, 0.0830849779304117, 0.08266830700449646, 0.08318289101589471, 0.08295649592764676, 0.08294009999372065, 0.08274450898170471, 0.08240473899058998, 0.08345175092108548, 0.08459168102126569, 0.08294414693955332, 1.5459575050044805, 0.08293226605746895, 0.07977637997828424, 0.07957778009586036, 0.08066741493530571, 0.07773848704528064, 0.07772292802110314, 0.07813884294591844, 1.8815739550627768, 0.5551006229361519, 0.09215440892148763, 0.08406869205646217, 0.08377305197063833, 0.08358695800416172, 0.0832866639830172, 0.08295802399516106, 0.08356595295481384, 0.08316528296563774, 0.08459788700565696, 0.0830451090587303, 0.08203288493677974, 0.08312859002035111, 0.08302141504827887, 0.09388783201575279, 0.07869548397138715, 0.0831468339310959, 0.08871731394901872, 0.08892968203872442, 0.08118187193758786, 0.08185032405890524, 0.08137215801980346, 0.08141606405843049, 0.08143779099918902, 0.08134391892235726, 0.08080980100203305, 0.08543663192540407, 0.08575817791279405, 0.09031989297363907, 1.4392630020156503, 0.08656340301968157, 0.08367359393741935, 0.08181222609709948, 0.08313317503780127, 0.08277152001392096, 0.08426624711137265, 0.08672398398630321, 0.08086851902771741, 0.08226561802439392, 0.08454304898623377, 0.08261602697893977, 0.08737879805266857, 1.5666105540003628, 0.08749408402945846, 0.08556202799081802, 0.08481071691494435, 0.085494211059995, 0.08501888299360871, 0.08591787691693753, 0.08334227197337896, 0.08359343803022057, 0.08342261100187898, 0.08443704899400473, 0.08449522301089019, 0.08434750302694738, 0.08485976606607437, 0.08350326493382454, 0.08316908997949213, 0.0840397150022909, 0.08426662196870893, 0.08555042906664312, 0.0858598219929263, 0.08226511802058667, 0.08225125388707966, 0.08224344498012215, 0.0822049790294841, 0.08203115907963365, 0.0820034600328654, 0.0819506609113887, 0.0838789070257917, 0.0829941030824557, 0.08270509797148407, 0.08234267309308052, 0.08268185192719102, 0.08372652891557664, 0.08291590795852244, 0.09261997998692095, 0.08715753303840756, 0.08699466800317168, 1.8575267309788615, 0.12177861295640469, 0.07874738005921245, 0.08210868202149868, 0.08126535103656352, 0.08130051998887211, 0.08207792602479458, 0.08164482598658651, 0.08185827999841422, 0.08353342104237527, 0.08408527600113302, 0.08178335800766945, 0.08254262898117304, 0.07746188598684967, 0.07854600704740733, 0.08256043202709407, 0.08268934302031994, 0.08245626802090555, 0.08318453992251307, 0.0826781380455941, 0.08246412500739098, 0.08258074894547462, 0.08274848596192896, 0.07980383303947747, 0.07831551495473832, 0.07869875698816031, 0.07837094401475042, 0.07774300093296915, 0.07873637904413044, 0.08530673501081765, 0.19521268794778734, 0.08498118992429227, 0.08315741398837417, 0.07986572792287916, 0.08078157703857869, 0.08003829000517726, 0.08005708595737815, 0.34140553208999336, 0.08398536907043308, 0.08369712007697672, 0.08269978000316769, 0.08379337703809142, 0.08603314310312271, 0.08418341004289687, 0.08316281903535128, 0.08393077610526234, 0.08372075099032372, 0.08285962895024568, 0.08461088000331074, 0.08354054403025657, 0.08380594209302217, 0.08420711196959019, 0.08413141104392707, 0.08385021798312664, 1.2945218690438196, 0.08632507303263992, 0.0820991350337863, 0.07884266902692616, 0.07800362294074148, 0.07920727494638413, 0.07969336200039834, 0.08017165691126138, 0.0808567829662934, 0.08476098591927439, 0.922338108997792, 0.08083079499192536, 0.07961932092439383, 0.07976437697652727, 0.07876049203332514, 0.07887359499000013, 0.07937059691175818, 0.07851098501123488, 0.07815348997246474, 0.07836098200641572, 0.07778413302730769, 0.07795685390010476, 0.07854590902570635, 0.07839102391153574, 0.07835138007067144, 0.07807962002698332, 0.0791842780308798, 0.07903518492821604, 0.07849635509774089, 0.07835698092821985, 0.08378186996560544, 0.08557116298470646, 0.08572046307381243, 0.08504996891133487, 0.08044729207176715, 0.08026147703640163, 0.07982989901211113, 0.07995288702659309, 1.2437336639268324, 0.3789629240054637, 0.2863282549660653]
[0.0014867839992654566, 0.001491985775587358, 0.0014846028988154568, 0.001587552346327171, 0.0015793022466823459, 0.001507036694401533, 0.05650989718412106, 0.0016889189184662334, 0.0014061844285235417, 0.0017204057943189935, 0.0018664459598117641, 0.0019890989405007995, 0.0377086382644365, 0.00624233900037195, 0.002090550875956459, 0.0019445076337730397, 0.001902524308225482, 0.0019127706336617774, 0.0019011374073559229, 0.0019140259590836204, 0.0019024592867045074, 0.0019112936534671759, 0.0021511971218777554, 0.001887822856328317, 0.04134070830495686, 0.01225863383579239, 0.005214391979483926, 0.001893880898227953, 0.0019048472666846855, 0.0019238216122992489, 0.0019495795096023654, 0.0019232653656366225, 0.040831457163036176, 0.019655881672907546, 0.0020041313872919703, 0.0018820585706746395, 0.001864765286065486, 0.0018769498363289299, 0.0018821077977250122, 0.001881380080796626, 0.0018807376321519213, 0.0018917410610699837, 0.001874376347345509, 0.0019006207965466442, 0.04554340238825475, 0.001861070715157049, 0.0018465718568055605, 0.0018517064496076532, 0.0018293848783918181, 0.0018036701426632246, 0.0018162941441358048, 0.001824531592523717, 0.0018406536938546567, 0.001818155817573472, 0.03199128424558712, 0.002334912283802215, 0.00186375865228094, 0.0018493012876763027, 0.0018513337352636214, 0.0019284951837961466, 0.0018683283670558309, 0.0018728370421889182, 0.0018401284706873857, 0.012677708959054887, 0.008319907959512606, 0.0023379347344138182, 0.002053026406911715, 0.0019765681829493568, 0.0019566208143167348, 0.0018666845922149262, 0.0018649695711970633, 0.0019818144679373626, 0.0018851982230054481, 0.0018832390006555586, 0.0023022249379024214, 0.010798726979243969, 0.014304470674762008, 0.001811978141111987, 0.001798761490618392, 0.0017944473050990884, 0.0017780353058585708, 0.0017817739791673969, 0.0017886571229758616, 0.0017909038175201537, 0.0017916953870646503, 0.0018019182441223944, 0.001794788836767631, 0.001799983509379078, 0.0018058325501386912, 0.0018046136128202993, 0.0017880639180122893, 0.0017987624694574245, 0.0017892050186210141, 0.0018055790189501581, 0.0018163762858365567, 0.0017932180804675635, 0.0017862829584058147, 0.001799515693220405, 0.001997718939139527, 0.03241248524329644, 0.0017752206946095945, 0.0018125510612996866, 0.0017924690619111061, 0.0017818014911965144, 0.0018218235915754826, 0.001947184183577798, 0.001810638490608152, 0.0018003076951646684, 0.0018120118564146819, 0.0018564379579216546, 0.0019373583882971077, 0.0020713850603990107, 0.0019504085102365638, 0.0018841976546967517, 0.001900588143236783, 0.001865749100071131, 0.002064204610865183, 0.0019187449800724887, 0.001882556060861264, 0.0018509514487291478, 0.0018371529378263013, 0.00186208832761919, 0.0018781648772977749, 0.0029360125915204385, 0.001897823265088456, 0.001867271570146692, 0.04780421540739813, 0.006029888141748248, 0.0018891329991118032, 0.0018546990408771196, 0.001848963878060482, 0.0018931507151954028, 0.001863276345978434, 0.0018668441429772243, 0.001878514900157342, 0.0018845338170055527, 0.0018736157965447222, 0.032957304102767794, 0.001949746815050591, 0.0018366105921034301, 0.001817471038892257, 0.0018082877944166564, 0.0017962031427542775, 0.001796133837624624, 0.0017909296332117245, 0.001783333286376936, 0.0017943111039241965, 0.0017788291229315254, 0.0018666097751761578, 0.0018751817156693765, 0.0018871051416143167, 0.0019582641638853414, 0.001888500999336188, 0.0018769524687406967, 0.001865261816419661, 0.0019900442851821377, 0.0020733953463103697, 0.0018763956114915864, 0.0019412724282212403, 0.015675570592474267, 0.02996256840842947, 0.0019161415105799632, 0.001757262550218373, 0.0018158948158237096, 0.0017896600623558067, 0.0017617829191517464, 0.0017783817340980988, 0.0017725550617110363, 0.0017813647341705402, 0.0017752497556752392, 0.001789879714309865, 0.0017777131015092743, 0.0017795322857302974, 0.001770188041240433, 0.0017696415710890172, 0.0017708537967077323, 0.039600353692753276, 0.0026109953486949814, 0.001802421426780674, 0.0018018842247143692, 0.0018239481022999603, 0.0017781161218501475, 0.0017832866751075704, 0.00179399389830627, 0.0017925808990222154, 0.0018216731829796822, 0.03231240381553237, 0.0019589894503469064, 0.0017784178157203964, 0.0017954826720857195, 0.0017813391631887276, 0.0017845821828220267, 0.0018566256740643662, 0.0019318728769502165, 0.0019268993459337829, 0.0018246276114059954, 0.0018027365107887558, 0.001812077835392283, 0.0017908910427204504, 0.001729968754688696, 0.001747330733841019, 0.0017577544097047374, 0.0017227169591933489, 0.0018147527340001293, 0.0017281404686901643, 0.0017748069403008843, 0.00173930040731722, 0.0017643498769980303, 0.0017493261827384026, 0.0017477387956780742, 0.001724810448798294, 0.0017430971623683463, 0.001754643896841729, 0.0017351068161922147, 0.0018174656529017553, 0.0023111610421530752, 0.0018332364294221814, 0.005412364183754033, 0.0019561742023773946, 0.0018772724683263472, 0.0018856955920251049, 0.00188385436729509, 0.0018792621012092853, 0.029371432408842504, 0.011641668531169392, 0.0019331333270220428, 0.0018671141029810723, 0.0018609423684526462, 0.001863331795308967, 0.0018784087365113047, 0.0018642702667346718, 0.0018712111026504819, 0.001884490897764965, 0.0018615126324703498, 0.0019117287545445927, 0.001889538756400651, 0.0018746720422629494, 0.003230459938224937, 0.006786531918416066, 0.0019416365896028523, 0.001858272081317038, 0.0018783568960557483, 0.0018938734880363454, 0.001879528713678675, 0.0018675419387920778, 0.0019037874287222416, 0.0018881929192539988, 0.001909035203351202, 0.023839912142091413, 0.015804349511329616, 0.03637129793950946, 0.007257844490113155, 0.022787919550734972, 0.0019118333263892907, 0.0018738198583489473, 0.0018498741413409613, 0.0019548770600967868, 0.0017867255528286404, 0.0017903120618085473, 0.0021247001820985153, 0.001821483592312707, 0.0018637786329514822, 0.0018223213454784484, 0.0017656119996491745, 0.050031389898563525, 0.0019383417152590593, 0.0017849548971660587, 0.0018229078981379162, 0.0019327981839412634, 0.0019529187358079516, 0.0017984333894766715, 0.0018217166106463695, 0.0018008235314556835, 0.04521707524497974, 0.0030805052029995285, 0.0017786398577522866, 0.0017845332242396412, 0.001789160735658085, 0.0017981597350682228, 0.001769859550463758, 0.001769478753570239, 0.0017704486927701806, 0.0017781209590255606, 0.00177631924423028, 0.0017806209186662216, 0.00981998553128951, 0.0021279415101459137, 0.0018407494490205937, 0.001869988651490029, 0.0018698762037924358, 0.0019204666130054667, 0.001866366184430615, 0.0018719338968738305, 0.0018408240997517596, 0.00409477281773805, 0.0018073835918603807, 0.0017065447161202223, 0.001800984918729079, 0.00183900706089881, 0.0019381756547420304, 0.0019241651420348457, 0.00179732626551116, 0.001929207386601032, 0.0019369312652329706, 0.001970410510916643, 0.001912906592502734, 0.0018715621827512371, 0.0018749139200401853, 0.001767956102932138, 0.0018687972048183484, 0.0018767140406583036, 0.0019467202851510778, 0.0018643475094886155, 0.0018606298764673422, 0.014945634103817294, 0.004085124080658567, 0.002232738326265648, 0.0018724181228412353, 0.001870596306208445, 0.0018685921214102786, 0.0018631185105602657, 0.0018778427347198737, 0.0018702380630016631, 0.0018406961426823115, 0.001846276817615239, 0.0018619897547273003, 0.001872220204915015, 0.0018667350832030786, 0.0034598519987597757, 0.001894120143593422, 0.001880870837414143, 0.0017172065417980775, 0.0017016150619989883, 0.0017349169162722926, 0.001703493437768581, 0.003206685646243083, 0.006285245981416665, 0.003325075396181395, 0.0017818367511305648, 0.001672584083280526, 0.0016702660844506074, 0.0016714125013095327, 0.0016895715622619416, 0.0016726543544791639, 0.0016668897078488953, 0.0016801333525412094, 0.0016832121254992671, 0.0016772794357772607, 0.0016746684389848572, 0.0016746679175412282, 0.0016793279792182148, 0.0016906154366248909, 0.001679935521678999, 0.0016810990006585296, 0.02531025483282671, 0.0016842489591605652, 0.0016344190225936472, 0.001635392853737964, 0.0016162289806137171, 0.0017288946449601401, 0.0016219829582648042, 0.0016355226471205242, 0.0016330136252994028, 0.01525109239446465, 0.0017622646895082046, 0.001627850106160622, 0.0015902159163185086, 0.0015768316661706194, 0.0017120542931176412, 0.0016898631050329034, 0.0016196966244024225, 0.0016247380845015869, 0.0017070972292761628, 0.0016833345628886793, 0.001685251770444059, 0.001679960541271915, 0.0017027229575129847, 0.001676547833388516, 0.0017125873952560748, 0.0017058392283312667, 0.0015976868768727097, 0.0350502641449566, 0.017669439437061858, 0.00486665604209217, 0.0016557768127919796, 0.0016518323124425176, 0.0016772168334379482, 0.0016748911245182778, 0.0016488317924086004, 0.0016398600419051945, 0.0016393570210008572, 0.0017154017502131562, 0.00192422191806448, 0.001839613478902417, 0.001704042500932701, 0.013261010729668973, 0.009118225187800514, 0.001677985686304358, 0.0018234294354139517, 0.0020116408122703433, 0.0016470622722408734, 0.0016496633130979415, 0.001659233040603188, 0.0017117167080868967, 0.0016391464159823954, 0.0016360794155237575, 0.0016291374995489605, 0.0016728789390375216, 0.0016254395838283624, 0.0016453341886517592, 0.0016418276039379027, 0.0017369226455533255, 0.0016437979356851429, 0.0018299749984483544, 0.0016529307298090619, 0.0016443346248706803, 0.0018717968535687153, 0.0017285591457039118, 0.001713498480967246, 0.001703407229797449, 0.0016897487512324005, 0.0017116592498496175, 0.0016815131045101832, 0.0017338876665841478, 0.001702312833610146, 0.0017032335623904753, 0.0016897547296442401, 0.0016731473757924202, 0.0016379717708332464, 0.0015935259167842257, 0.0015917159980745055, 0.0016001087278709747, 0.001600823209931453, 0.0016953670007448334, 0.001697109270025976, 0.0016954715853595796, 0.0017018733342410997, 0.0017896379989300233, 0.0019397021872767557, 0.0017358339585674305, 0.0017326143764269848, 0.001687872604331157, 0.0016880662709202927, 0.0016944796467820804, 0.0016969884988308574, 0.0017062504363517899, 0.0016986817693881069, 0.0017090296023525298, 0.0017223168348815914, 0.0017193527916485134, 0.038184767230025805, 0.0017914804144917678, 0.0018581083349999972, 0.0017297817515403342, 0.0016777787920242797, 0.001673699979922579, 0.0016679463127123502, 0.00167140150006162, 0.001677610833818714, 0.0016700212897073168, 0.001689030500225878, 0.0016901929799738962, 0.0016883810628011513, 0.02647395718668122, 0.001818491582525894, 0.001684516500972677, 0.001675338251516223, 0.001666494645178318, 0.0016801676465547644, 0.0016849273524712771, 0.0017405771674627128, 0.0016652899600254993, 0.0016554756681822862, 0.0016687834795447998, 0.0016786480628070422, 0.0016626845851230125, 0.0015789971657795832, 0.001580679042187209, 0.0016459514372400008, 0.001594528519490268, 0.0016020265635840285, 0.0015995142093743198, 0.0015933003135918018, 0.0016750267095630988, 0.0016344475637500484, 0.02795570325057876, 0.0017157088756600085, 0.0016445070626408171, 0.0016231485011909779, 0.0016106550635110277, 0.0016357423325340885, 0.0016248660419175092, 0.002814281855535228, 0.0016319820206263103, 0.0016096681235164094, 0.0016368401459961508, 0.02185114960351105, 0.002050468353748632, 0.0017438406042250183, 0.0017276841463171877, 0.0017219805425459829, 0.0016990833755698986, 0.0017359804381461192, 0.0017309370402169104, 0.0017222563959270094, 0.0017329768961644731, 0.0017282603318259742, 0.0017279187498691802, 0.001723843937118848, 0.0017167653956372912, 0.0017385781441892807, 0.001762326687943035, 0.0017280030612406942, 0.03220744802092668, 0.0017277555428639364, 0.001662007916214255, 0.0016578704186637576, 0.0016805711444855358, 0.0016195518134433466, 0.0016192276671063155, 0.0016278925613733009, 0.03919945739714118, 0.011564596311169831, 0.0019198835191976589, 0.0017514310845096286, 0.0017452719160549652, 0.0017413949584200357, 0.0017351388329795252, 0.0017282921665658553, 0.0017409573532252882, 0.0017326100617841196, 0.00176245597928452, 0.001730106438723548, 0.0017090184361829113, 0.0017318456254239816, 0.0017296128135058098, 0.001955996500328183, 0.0016394892494038988, 0.0017322257068978313, 0.00184827737393789, 0.0018527017091400921, 0.001691288998699747, 0.0017052150845605258, 0.001695253292079239, 0.001696168001217302, 0.001696620645816438, 0.0016946649775491096, 0.0016835375208756886, 0.0017799298317792516, 0.0017866287065165427, 0.001881664436950814, 0.02998464587532605, 0.0018034042295766994, 0.0017431998736962366, 0.001704421377022906, 0.0017319411466208596, 0.0017244066669566867, 0.0017555468148202635, 0.001806749666381317, 0.001684760813077446, 0.0017138670421748732, 0.001761313520546537, 0.0017211672287279118, 0.001820391626097262, 0.032637719875007555, 0.0018227934172803846, 0.001782542249808709, 0.0017668899357280072, 0.0017811293970832291, 0.0017712267290335149, 0.0017899557691028651, 0.0017362973327787283, 0.0017415299589629285, 0.0017379710625391454, 0.0017591051873750985, 0.0017603171460602123, 0.001757239646394737, 0.001767911793043216, 0.0017396513527880113, 0.0017326893745727527, 0.0017508273958810605, 0.0017555546243481028, 0.001782300605555065, 0.001788746291519298, 0.001713856625428889, 0.0017135677893141594, 0.0017134051037525448, 0.0017126037297809187, 0.001708982480825701, 0.0017084054173513625, 0.0017073054356539312, 0.0017474772297039938, 0.001729043814217827, 0.001723022874405918, 0.0017154723561058443, 0.0017225385818164796, 0.0017443026857411799, 0.0017274147491358842, 0.0019295829163941864, 0.0018157819383001577, 0.0018123889167327434, 0.03869847356205961, 0.0025370544365917644, 0.0016405704179002594, 0.0017105975421145558, 0.0016930281465950732, 0.0016937608331015024, 0.0017099567921832204, 0.0017009338747205522, 0.0017053808333002962, 0.0017402796050494846, 0.001751776583356938, 0.0017038199584931135, 0.0017196381037744384, 0.0016137892913927014, 0.001636375146820986, 0.0017200090005644597, 0.0017226946462566655, 0.001717838917102199, 0.001733011248385689, 0.0017224612092832103, 0.0017180026043206453, 0.001720432269697388, 0.00172392679087352, 0.001662579854989114, 0.001631573228223715, 0.00163955743725334, 0.0016327280003073004, 0.0016196458527701907, 0.001640341230086051, 0.001777223646058701, 0.004066930998912237, 0.0017704414567560889, 0.0017324461247577954, 0.001663869331726649, 0.001682949521637056, 0.0016674643751078595, 0.0016678559574453782, 0.007112615251874861, 0.0017496951889673558, 0.0017436900016036816, 0.0017229120833993268, 0.0017456953549602379, 0.001792357147981723, 0.0017538210425603513, 0.0017325587299031515, 0.0017485578355262987, 0.0017441823122984108, 0.001726242269796785, 0.0017627266667356405, 0.0017404280006303452, 0.001745957126937962, 0.0017543148326997955, 0.001752737730081814, 0.0017468795413151383, 0.026969205605079576, 0.0017984390215133317, 0.0017103986465372145, 0.0016425556047276284, 0.0016250754779321142, 0.0016501515613830027, 0.0016602783750082988, 0.0016702428523179453, 0.001684516311797779, 0.0017658538733182165, 0.019215377270787332, 0.0016839748956651117, 0.0016587358525915381, 0.0016617578536776516, 0.001640843584027607, 0.0016431998956250027, 0.0016535541023282956, 0.0016356455210673933, 0.001628197707759682, 0.0016325204584669943, 0.0016205027714022435, 0.0016241011229188491, 0.0016363731047022156, 0.0016331463314903278, 0.0016323204181389883, 0.0016266587505621526, 0.0016496724589766625, 0.0016465663526711676, 0.001635340731202935, 0.001632437102671247, 0.0017454556242834467, 0.0017827325621813845, 0.0017858429807044256, 0.0017718743523194764, 0.001675985251495149, 0.0016721141049250339, 0.0016631228960856486, 0.001665685146387356, 0.025911117998475675, 0.007895060916780494, 0.0059651719784596935]
[672.5926566966341, 670.2476768629544, 673.5807944318885, 629.9004894632399, 633.1910197055116, 663.5538495611183, 17.6960152084827, 592.0947353163259, 711.1442707767535, 581.2582143713604, 535.7776338195468, 502.7402004187021, 26.519122567815256, 160.196362283499, 478.3428193501796, 514.2690018961986, 525.617462902599, 522.8018364573154, 526.0009066839556, 522.4589537326708, 525.6354272538613, 523.2058392418943, 464.8574460378189, 529.7107176384811, 24.189232381393364, 81.57515865105866, 191.77691357583194, 528.0163081721082, 524.9764731743885, 519.7987139799585, 512.9311192873375, 519.9490501244422, 24.490921203401925, 50.87535713945299, 498.96928232396164, 531.3330921691463, 536.2605189363668, 532.7792893793422, 531.3191950050601, 531.5247090192289, 531.7062746576778, 528.6135722160581, 533.5107868898365, 526.143879840188, 21.957077151923357, 537.3250956321741, 541.5440489437166, 540.0424026237441, 546.6318278956604, 554.4251004362923, 550.571614861315, 548.0858780947642, 543.285248788881, 550.0078652965011, 31.258513797799157, 428.28161337674806, 536.5501583459646, 540.7447702891761, 540.1511250793498, 518.5390186101216, 535.2378188079597, 533.949285214494, 543.440317309175, 78.87860521405669, 120.19363734146185, 427.7279366614681, 487.0857952111097, 505.92739912864505, 511.08523055817875, 535.7091413142505, 536.2017780044167, 504.58810154957763, 530.4481978588784, 531.0000481361624, 434.3624219930088, 92.60350798034632, 69.90821420357364, 551.883037278978, 555.9380747339728, 557.274653403534, 562.4185283076389, 561.238412779655, 559.0786446181811, 558.3772786774723, 558.1305880562144, 554.9641351719814, 557.1686092058466, 555.5606453000005, 553.7612000200119, 554.1352414144615, 559.2641235731971, 555.9377722071544, 558.9074419044082, 553.8389566475154, 550.5467164472676, 557.6566569857806, 559.8217210180741, 555.7050731857773, 500.570916362603, 30.852308685796324, 563.3102425160266, 551.708595333558, 557.889684820453, 561.2297469391389, 548.90056568826, 513.5621008191318, 552.2913630672475, 555.4606041433008, 551.8727686355428, 538.6659951294758, 516.1667588406171, 482.76876140420273, 512.7131033071173, 530.729882561574, 526.1529193257815, 535.9777474698368, 484.4480991546975, 521.1740019573737, 531.1926804147882, 540.262685272806, 544.3204969005949, 537.031452894917, 532.4346185403905, 340.59799433017474, 526.9194547224558, 535.5407408261673, 20.918657308310117, 165.84055566080028, 529.3433551106042, 539.1710342002887, 540.8434485204631, 528.2199626123187, 536.6890435540226, 535.6633566662989, 532.3354102308377, 530.6352111998494, 533.727353198119, 30.342287611929365, 512.8871052798988, 544.4812331473717, 550.2150948218118, 553.0093180342427, 556.7299022017139, 556.7513840296522, 558.3692298433143, 560.7476783162751, 557.3169545754795, 562.1675444305699, 535.7306134891675, 533.2816503295702, 529.9121802744673, 510.6563345447356, 529.5205034847755, 532.7785421603828, 536.1177670593629, 502.5013802185188, 482.30068702503416, 532.9366546562528, 515.126051069651, 63.793531093540736, 33.37497594894657, 521.8821232557755, 569.0669273500031, 550.6926895137311, 558.765332609399, 567.6068198467235, 562.3089693434954, 564.157368987289, 561.3673498850485, 563.3010210552808, 558.6967615785154, 562.5204647200959, 561.9454100489178, 564.9117363256306, 565.08618261302, 564.6993568069489, 25.252299708196706, 382.99570334348414, 554.8092056285149, 554.9746128436847, 548.2612135394756, 562.39296619137, 560.7623350517541, 557.4154967550957, 557.8548787089396, 548.9458863111306, 30.94786775099989, 510.46727169608573, 562.2975608771231, 556.9533003837635, 561.375408268646, 560.3552526892668, 538.611532722633, 517.6324032141632, 518.9684671958805, 548.0570357199814, 554.7122355459853, 551.8526745753819, 558.3812616991771, 578.0451220808019, 572.3015000152714, 568.907689537799, 580.4784092148507, 551.0392580017068, 578.6566648473577, 563.4415649909895, 574.9438083225932, 566.7810070083488, 571.6486781410862, 572.1678791320918, 579.7738532351295, 573.6914852418782, 569.916210234994, 576.3333938106208, 550.2167253633684, 432.68295967311786, 545.483377894247, 184.76214202319196, 511.2019158542584, 532.687724809353, 530.3082874187928, 530.8265953890258, 532.1237518473397, 34.046688158761434, 85.89833985760725, 517.2948942639575, 535.5859068298931, 537.3621542248454, 536.6730726742018, 532.3654966901724, 536.4029120903867, 534.4132463641047, 530.6472964056316, 537.1975363244966, 523.0867598882863, 529.2296845526907, 533.42663540919, 309.5534441295306, 147.35066629339508, 515.0294372051068, 538.1343292265666, 532.3801893558363, 528.018374150665, 532.0482697190442, 535.4632092743244, 525.2687274393688, 529.606900758365, 523.8248085968017, 41.94646331076087, 63.273720900890794, 27.494207153760076, 137.7819545957796, 43.88290022586759, 523.0581485304532, 533.669229485654, 540.5773169384955, 511.5411195988404, 559.6830461269544, 558.5618403250965, 470.65464032309916, 549.0030238100125, 536.544406250864, 548.7506374664405, 566.3758516586312, 19.987451918234864, 515.9049057902312, 560.2382455644579, 548.5740673028467, 517.3845920947892, 512.0540766312455, 556.0394985165346, 548.9328000611393, 555.3014954172977, 22.115539198016258, 324.622077906665, 562.2273647143644, 560.3706260084246, 558.9212752493041, 556.1241198419226, 565.0165854900572, 565.1381786768118, 564.8285680819831, 562.3914362654025, 562.9618680584204, 561.6018488365578, 101.83314393017085, 469.9377286603285, 543.2569872733463, 534.762603614299, 534.79476233337, 520.7067872088821, 535.8005349336506, 534.2068978343847, 543.2349566342883, 244.21379268420546, 553.2859789717785, 585.9793713893824, 555.2517345373892, 543.7717022746245, 515.949107890894, 519.705912010483, 556.3820098715354, 518.3475902825807, 516.2805815309723, 507.50845798868374, 522.7646785887539, 534.3129975676139, 533.3578194238202, 565.6249034359562, 535.1035400854007, 532.8462292791422, 513.6844813441669, 536.3806881015959, 537.452403966895, 66.90917180587125, 244.79060617389837, 447.88051883918865, 534.0687466123138, 534.5888884100938, 535.1622692518216, 536.734509550488, 532.5259573183451, 534.6912886560746, 543.2727199301745, 541.63058890143, 537.0598830961109, 534.1252045965355, 535.694651586088, 289.02970426436207, 527.949614697014, 531.6686186568871, 582.3411311681269, 587.6769795544949, 576.3964779066409, 587.028971071299, 311.8484660857196, 159.10276271710924, 300.74506014162165, 561.2186410261804, 597.8772666774684, 598.7070020217318, 598.2963506713701, 591.866022331267, 597.8521487850268, 599.9197159183916, 595.1908510639917, 594.1021840627392, 596.203577453744, 597.133126009215, 597.1333119393692, 595.4762931214512, 591.5005733038727, 595.2609413250321, 594.8489646405562, 39.50967726737493, 593.7364512301098, 611.8382043872125, 611.4738716843067, 618.7242104892082, 578.4042439573032, 616.5292889820489, 611.4253457514541, 612.364762000474, 65.56907362012625, 567.4516467098198, 614.3071749760531, 628.8454226486986, 634.1831036590788, 584.0936260140476, 591.7639109474071, 617.3995703479003, 615.4838183083308, 585.7897153427027, 594.0589720227416, 593.3831475735539, 595.2520761248892, 587.2946010316385, 596.463745373058, 583.9118066441654, 586.2217162037291, 625.9048718966675, 28.53045545860431, 56.594891058201206, 205.4799006444888, 603.9461310693165, 605.3883269309149, 596.2258308308333, 597.0537340375566, 606.4900037736462, 609.808138771524, 609.9952525225298, 582.9538181803416, 519.6905775846642, 543.5924510602292, 586.8398232160606, 75.4090333222257, 109.67046540350016, 595.9526402173475, 548.4171641514506, 497.10663747739204, 607.1415858730542, 606.184299584184, 602.688094757603, 584.2088210482282, 610.0736274987775, 611.2172737531022, 613.8217309937669, 597.771886933637, 615.2181907891783, 607.779262655104, 609.0773462460447, 575.7308781482513, 608.3472781483906, 546.4555531348271, 604.9860299442281, 608.1487216013867, 534.2460097063567, 578.5165075116798, 583.601334408837, 587.0586801013574, 591.8039585889088, 584.2284321998422, 594.7024720281887, 576.7386314997291, 587.4360929766768, 587.1185385734811, 591.8018647656274, 597.6759814875181, 610.5111320027779, 627.5392131795537, 628.2527795220361, 624.9575310613739, 624.6785989833442, 589.8427889422559, 589.2372504598328, 589.80640468116, 587.5877950963493, 558.7722213083725, 515.5430594239571, 576.0919672439705, 577.1624740077536, 592.4617755119403, 592.3938042164804, 590.1516739366334, 589.2791852678739, 586.0804361979503, 588.6917832527374, 585.1273720615901, 580.6132644977308, 581.6142009117286, 26.188453473501095, 558.1975621451011, 538.1817524649344, 578.1076133503669, 596.0261297578309, 597.4786473058674, 599.5396808509013, 598.3002886877466, 596.0858024049111, 598.7947615777146, 592.0556199939952, 591.6484163929283, 592.283354766444, 37.77297035530034, 549.9063122475359, 593.6421515744, 596.8943878019708, 600.0619341282061, 595.178702583954, 593.4973982903794, 574.5220715825702, 600.4960241186393, 604.0559938268383, 599.2389140098472, 595.7174837039969, 601.4369826650048, 633.313359689458, 632.6395006897069, 607.5513392283561, 627.1446310158665, 624.2093750073756, 625.189819596019, 627.628069529268, 597.0054055202692, 611.8275203063824, 35.77087619783978, 582.84946484019, 608.0849530643904, 616.0865744978075, 620.8653998331116, 611.3432293769656, 615.4353492549436, 355.33043644266303, 612.7518485872946, 621.2460726472265, 610.933207158979, 45.76418257826214, 487.6934570444927, 573.4469065447704, 578.8095018013835, 580.7266547399425, 588.55263630873, 576.043357417043, 577.7217638572724, 580.6336398952649, 577.0417379558025, 578.6165322347364, 578.7309154876116, 580.0989164200984, 582.4907716227492, 575.182659083933, 567.4316838310989, 578.7026785022053, 31.048718897264184, 578.7855834873461, 601.6818513583341, 603.1834507343459, 595.0358027277236, 617.4547746477399, 617.5783803071232, 614.291153929958, 25.510557196460837, 86.4708091050386, 520.8649326902454, 570.9616603498752, 572.9766180277585, 574.2522654982865, 576.32275930499, 578.6058742527407, 574.3966089389871, 577.1639112901552, 567.3900578248521, 577.9991205268204, 585.1311950931891, 577.4186713409778, 578.1640793774341, 511.24835848745994, 609.9460550678143, 577.2919752997184, 541.0443335512066, 539.7522952921208, 591.265006021321, 586.436285401336, 589.8823524911098, 589.5642408548695, 589.4069499070523, 590.0871341816711, 593.9873555534729, 561.8198999453709, 559.7133844052789, 531.4443852807639, 33.350402207780824, 554.506850765634, 573.6576826842154, 586.7093745014446, 577.3868251534245, 579.9096113243673, 569.6230892608705, 553.4801077356052, 593.5560657856015, 583.4758329508536, 567.7580898202037, 581.0010691053446, 549.3323445702177, 30.63939527116762, 548.6085205925333, 560.9965206195329, 565.9662097673178, 561.4415222372929, 564.580459185854, 558.6730226865913, 575.9382227464603, 574.2077504055655, 575.3835731528335, 568.4708380015524, 568.0794521817347, 569.0743445560556, 565.6390799218767, 574.8278230562541, 577.1374919676994, 571.1585290203749, 569.6205553110226, 561.0725805081395, 559.0507746912701, 583.4793792915741, 583.5777295978709, 583.6331395359396, 583.9062373920691, 585.1435056940118, 585.34115488252, 585.7182781222626, 572.2535223931877, 578.3543434683713, 580.3753477995994, 582.9298248034842, 580.5385206208059, 573.2949952863744, 578.8997694388313, 518.2467109880414, 550.7269231547424, 551.7579536972312, 25.84081251670894, 394.1578807206766, 609.5440885005622, 584.5910422412098, 590.6576343761005, 590.4021278900791, 584.8100984605761, 587.9123314915994, 586.3792887039633, 574.6203064717092, 570.8490509010545, 586.91647260924, 581.5177029429025, 619.6595834001348, 611.1068124828937, 581.3923064773658, 580.4859277719085, 582.1267582451138, 577.0302996772274, 580.5645982681623, 582.0712945865603, 581.2492695082341, 580.0710362493388, 601.4748687103199, 612.9053742127742, 609.9206879115164, 612.4718874250868, 617.4189241985412, 609.6292537544404, 562.6753854067111, 245.88565684233774, 564.8308766064828, 577.2185268617256, 601.0087336379166, 594.1948865033514, 599.7129623445869, 599.5721606149251, 140.59526131916152, 571.5281189006328, 573.4964351922041, 580.4126685483496, 572.8376358215017, 557.9245191875102, 570.1836023931664, 577.1810113795675, 571.8998706719985, 573.334560813337, 579.2929633901973, 567.3029283955185, 574.571312135763, 572.7517500694806, 570.0231117929125, 570.5360150792909, 572.4493168241869, 37.07932723875459, 556.0377572093217, 584.6590220499465, 608.8073956959416, 615.3560333532853, 606.0049412442415, 602.3086339331509, 598.7153296972417, 593.6422182417233, 566.2982736623046, 52.041653198257805, 593.8330806321401, 602.8687439519937, 601.7723928831693, 609.4426121625833, 608.568685199218, 604.7579565687901, 611.3794138887854, 614.1760274161973, 612.5497507939608, 617.0924343033897, 615.7252069395722, 611.1075751162375, 612.3150024697725, 612.6248185635649, 614.757090050026, 606.1809388636625, 607.3244472521467, 611.4933609367224, 612.581028919059, 572.9163125590919, 560.9366324561781, 559.959644159505, 564.3741040051443, 596.6639617549728, 598.0453110553918, 601.2784757840899, 600.3535555136957, 38.59347173127879, 126.661467281976, 167.63976019652273]
Elapsed: 0.1762309732814132~0.3516127056338908
Time per graph: 0.0036290202054681093~0.0072210403552138555
Speed: 519.6775438699857~146.05617591364003
Total Time: 0.2902
best val loss: 0.3344939053058624 test_score: 0.9167

Testing...
Test loss: 0.6449 score: 0.9167 time: 0.51s
test Score 0.9167
Epoch Time List: [0.4001076229615137, 0.3984399070031941, 0.39046691299881786, 0.4048434681026265, 0.42205502407159656, 0.41755470586940646, 4.961043810937554, 1.8191702539334074, 0.43878705194219947, 0.38062763097696006, 0.373370666988194, 0.40356782695744187, 3.5689581119222566, 5.394583981949836, 1.5462141421157867, 0.42891296092420816, 0.42606176203116775, 0.4057166330749169, 0.4081716248765588, 0.41256215900648385, 0.4076648020418361, 0.4078600879292935, 0.4159285230562091, 0.40649932296946645, 3.6579023881349713, 3.1642563530476764, 1.7732684450456873, 0.43463442707434297, 0.4032706110738218, 0.4237489808583632, 0.412271287990734, 0.4145273668691516, 5.376948590972461, 6.984726764028892, 2.1374722589971498, 0.4048669560579583, 0.4047879868885502, 0.4084268129663542, 0.40671058802399784, 0.4003670560196042, 0.39968165499158204, 0.4100406940560788, 0.40067753405310214, 0.4035586379468441, 2.841230119112879, 5.865566123160534, 0.4040833229664713, 0.3973646890372038, 0.39017101493664086, 0.4107706610811874, 0.388002090039663, 0.3916752189397812, 0.39018199394922704, 0.3882199468789622, 4.043383124051616, 2.328154378919862, 0.41086561197880656, 0.3972722911275923, 0.4013482149457559, 0.40728357213083655, 0.409798217122443, 0.4025843399576843, 0.39391957002226263, 3.906489818939008, 1.1558805849635974, 1.181927247904241, 0.5541581748984754, 0.42581939103547484, 0.42212121514603496, 0.4029983759392053, 0.4044164380757138, 0.40792862395755947, 0.4132765681715682, 0.4063807249767706, 0.420865970896557, 2.415722783887759, 5.660842376179062, 0.40318282588850707, 0.38592411391437054, 0.3862181630684063, 0.38931225799024105, 0.3824400920420885, 0.382579302880913, 0.3954155540559441, 0.3830297130625695, 0.39126565493643284, 0.39020015683490783, 0.3864470961270854, 0.3920476289931685, 0.39219358598347753, 0.3856901659164578, 0.391373775084503, 0.3832546730991453, 0.3934630178846419, 0.39480388909578323, 0.3888674018671736, 0.3861475068842992, 0.38878291787113994, 0.40221610898151994, 7.832180242054164, 0.5875608739443123, 0.382132237078622, 0.39267921005375683, 0.3910508300177753, 0.39565120195038617, 0.42806590197142214, 0.39149413094855845, 0.38512717501726, 0.3938990969909355, 0.393322771997191, 0.4160100379958749, 0.42004401003941894, 0.4193632900714874, 0.40936936205253005, 0.41117262118496, 0.4053992049302906, 0.40896017488557845, 0.4102570960531011, 0.411779573187232, 0.39665889588650316, 0.3990061420481652, 0.4065627548843622, 0.39937617897521704, 0.4610773507738486, 0.4139681690139696, 0.4002758510177955, 8.003183649852872, 2.028193167876452, 1.2493207679362968, 0.40921193396206945, 0.40558737702667713, 0.40665807691402733, 0.4011431960389018, 0.4108881460269913, 0.40972810704261065, 0.4034570809453726, 0.4019275289028883, 3.657454303931445, 1.1292157431598753, 0.3946553160203621, 0.39021593902725726, 0.38651573611423373, 0.38383490801788867, 0.383530454011634, 0.38260769296903163, 0.3819782000500709, 0.38295966398436576, 0.3883176539093256, 0.38714142609387636, 0.40131074807140976, 0.408235521055758, 0.4091117038624361, 0.40469348803162575, 0.4034306920366362, 0.4035573049914092, 0.4312469590222463, 0.44409004994668067, 0.40509566909167916, 0.4221180679742247, 2.610071361064911, 6.319308230187744, 2.7440654250094667, 0.4021746850339696, 0.38349740498233587, 0.38265727704856545, 0.38061674998607486, 0.38129807892255485, 0.3855573651380837, 0.38712908618617803, 0.3797113300533965, 0.3760629389435053, 0.37983594508841634, 0.3908339769113809, 0.38893044111318886, 0.3778149770805612, 0.3768184690270573, 4.042566895019263, 1.6591883208602667, 0.3932738769799471, 0.38518633297644556, 0.3854277179343626, 0.3810095818480477, 0.38624438794795424, 0.39337568904738873, 0.3869561010506004, 0.3908612991217524, 2.5533521169563755, 3.1444909940473735, 0.38519037794321775, 0.38281291304156184, 0.3900548688834533, 0.40345474798232317, 0.40865168790332973, 0.40659045497886837, 0.41249295487068594, 0.4078306080773473, 0.38878999394364655, 0.38732286798767745, 0.3862963580759242, 0.41821674699895084, 0.37111218797508627, 0.3744128660764545, 0.36978617403656244, 0.39368025108706206, 0.3694278900511563, 0.38079538696911186, 0.3928317561512813, 0.3693922481033951, 0.37046934908721596, 0.37588158797007054, 0.3694347059354186, 0.3695775889791548, 0.3625717699760571, 0.36665177100803703, 0.3785427851835266, 0.44426340400241315, 0.3975942999823019, 6.433348976075649, 0.7733790270285681, 0.41063012881204486, 0.408637507003732, 0.41141560615506023, 0.4045433569699526, 2.5969367718789726, 6.186321682063863, 0.6608519090805203, 0.4018445978872478, 0.4005798000143841, 0.4061287030344829, 0.4068231381243095, 0.3983837969135493, 0.40290273108985275, 0.4120720678474754, 0.4015501510584727, 0.4129402828402817, 0.4056847479660064, 0.40610788902267814, 5.76456573989708, 1.171008902019821, 1.520803474006243, 0.4121563088847324, 0.40737588389310986, 0.4058456210186705, 0.4086454041535035, 0.40109215304255486, 0.41007914091460407, 0.41055396606680006, 0.41323906811885536, 1.4833393358858302, 3.164408280979842, 3.4455467129591852, 4.191930703935213, 2.4339558840729296, 0.41981180803850293, 0.4057232370832935, 0.41564418480265886, 0.4131871310528368, 0.3889179490506649, 0.3797150840982795, 0.3961796509101987, 0.39546258212067187, 0.39446421491447836, 0.3923751140246168, 0.3892869850387797, 2.93486566003412, 4.152235544053838, 0.39707808615639806, 0.3827537769684568, 0.4049023640109226, 0.41112846811302006, 0.3867439989699051, 0.3888479861197993, 0.3876054169377312, 6.962833747966215, 2.1636775580700487, 0.3926302249310538, 0.38229543913621455, 0.3912974380655214, 0.3808604668593034, 0.38175482011865824, 0.3869848579633981, 0.3852406258229166, 0.3848600360797718, 0.3790422839811072, 0.383217524853535, 0.7806581269251183, 4.889111351920292, 0.3981694560497999, 0.40676420100498945, 0.4144878661027178, 0.4220300648594275, 0.40033109311480075, 0.41662462381646037, 0.40134882484562695, 4.46669953095261, 0.4566325698979199, 0.3813890990568325, 0.38778508093673736, 0.39407185814343393, 0.40365601191297174, 0.41506854607723653, 0.38477995700668544, 0.40185517398640513, 0.4178527860203758, 0.41955396300181746, 0.4122523949481547, 0.4085852359421551, 0.4062395648797974, 0.3987996911164373, 0.40957720996811986, 0.4090205969987437, 0.41330236301291734, 0.4038426090264693, 0.403615779010579, 1.0544054940110072, 3.0683542659971863, 1.1049182680435479, 0.41506556211970747, 0.40276275598444045, 0.4017172669991851, 0.40012461703736335, 0.40431488887406886, 0.40415624307934195, 0.4035089740063995, 0.40153675607871264, 0.40193979407195, 0.3951582059962675, 0.40112095791846514, 0.4987435588845983, 0.41293318080715835, 0.4027248070342466, 0.4143172249896452, 0.41379138093907386, 0.439122520852834, 0.40618388110306114, 4.060814009862952, 1.612077432917431, 1.2447929920163006, 0.43974446307402104, 0.39971220085863024, 0.4022812448674813, 0.39612395188305527, 0.40421748196240515, 0.39699247700627893, 0.4035405620234087, 0.40445813501719385, 0.4013947390485555, 0.40338156081270427, 0.39801340899430215, 0.40173735306598246, 0.40443820878863335, 0.4073348749661818, 0.3944063709350303, 0.39697566197719425, 3.8304086760617793, 1.9510618339991197, 0.3963062848197296, 0.38639421202242374, 0.3867001461330801, 0.38964336703065783, 0.3882295038783923, 0.38909121905453503, 0.39579004514962435, 4.623191297985613, 1.1791202737949789, 0.38670594501309097, 0.373882096959278, 0.37482376291882247, 0.38865303189959377, 0.4042058539343998, 0.3805552769917995, 0.3849071090808138, 0.3942806718405336, 0.39745437097735703, 0.4026589811546728, 0.3981806901283562, 0.40523838985245675, 0.4030128308804706, 0.40448694198857993, 0.4015516779618338, 0.39961555309128016, 2.132733770995401, 5.563038604799658, 0.8671468441607431, 0.41566635796334594, 0.39235678093973547, 0.40040696202777326, 0.3989890239899978, 0.39999401895329356, 0.3914711199468002, 0.3932966629508883, 0.4072596220066771, 0.4325598048744723, 0.4255257010227069, 0.4121309098554775, 2.862347570131533, 2.221484067849815, 2.4271923779742792, 0.4125741550233215, 0.43479043699335307, 0.4055787130491808, 0.4106737030670047, 0.3961058788700029, 0.39920775301288813, 0.38666601094882935, 0.47413371421862394, 0.3828348449897021, 0.39355994504876435, 0.38370707095600665, 0.43156739813275635, 0.3940196019830182, 0.422817847924307, 0.3846207819879055, 0.40059675998054445, 0.38174356205854565, 0.38627613487187773, 0.39237709110602736, 5.356013764976524, 0.405675265006721, 0.4026431320235133, 0.3998605980305001, 0.4074575121048838, 0.3975633520167321, 0.4098366789985448, 0.4016082308953628, 0.39070700493175536, 0.3982178720179945, 0.39535566105041653, 0.3898248380282894, 0.3716385569423437, 0.37137433094903827, 0.37190711207222193, 0.37969402491580695, 0.3862658350262791, 0.39815779484342784, 0.3960035360651091, 0.39754229388199747, 0.41711793781723827, 0.4252875978127122, 0.4195872499840334, 0.39891452400479466, 0.39864074683282524, 0.39862280909437686, 0.39902582112699747, 0.3995459381258115, 0.40216013812460005, 0.4058701628819108, 0.40352403291035444, 0.4004187809769064, 0.4080377909122035, 4.829491992015392, 0.9958703109296039, 0.44086623284965754, 0.41300722397863865, 0.3976229301188141, 0.4021354840369895, 0.39367418899200857, 0.40548339602537453, 0.3998154120054096, 0.3976507439510897, 0.4021976700751111, 0.39860869699623436, 0.40342662087641656, 4.774721090099774, 2.8278940761229023, 0.39894063398241997, 0.3972144320141524, 0.39585322607308626, 0.398450625827536, 0.3985758820781484, 0.38921260403003544, 0.39600964298006147, 0.39487498393282294, 0.3953761389711872, 0.3925857748836279, 0.3937889317749068, 0.37986591295339167, 0.3762694770703092, 0.3834557047812268, 0.3781496580922976, 0.37373304297216237, 0.376536381081678, 0.37560545303858817, 0.40099910902790725, 0.38177729304879904, 4.59576693281997, 4.006069757975638, 0.39450768718961626, 0.4122158110840246, 0.3797142399707809, 0.3764555569505319, 0.38192620500922203, 0.4354468530509621, 0.38212859618943185, 0.38039741409011185, 0.38041815208271146, 1.358529518940486, 2.608434038818814, 0.41103346599265933, 0.4082492447923869, 0.4070563258137554, 0.4044842829462141, 0.4054080368950963, 0.4088753988035023, 0.40607310796622187, 0.41211451403796673, 0.4071765140397474, 0.4089147759368643, 0.40678353095427155, 0.406562639051117, 0.4108999598538503, 0.4080426510190591, 0.4081586339743808, 3.60987166990526, 1.6105096080573276, 0.39159038581419736, 0.39071380195673555, 0.3861061241477728, 0.39269344694912434, 0.3816014379262924, 0.3817519510630518, 4.171267823083326, 5.967006304883398, 1.373872239026241, 0.4086733522126451, 0.40926122397650033, 0.40860228112433106, 0.4102598719764501, 0.4085228288313374, 0.4087144851218909, 0.4057579389773309, 0.40476934087928385, 0.4098117359681055, 0.40685280691832304, 0.4077510499628261, 0.40754455304704607, 0.41624355409294367, 0.39584632811602205, 0.39413755014538765, 0.4124757529934868, 6.555643793893978, 0.40612630906980485, 0.4021169990301132, 0.4026941671036184, 0.4018400829518214, 0.404441126040183, 0.4031568029895425, 0.40330457512754947, 0.41556283400859684, 0.4257781259948388, 0.42467125214170665, 7.263803223962896, 2.04315138887614, 0.4114756309427321, 0.4023807338671759, 0.414797795121558, 0.4060097080655396, 0.4078378329286352, 0.41187490697484463, 0.4281810650136322, 0.4024731529643759, 0.41177110001444817, 0.4096600532066077, 0.41398395993746817, 3.2810991919832304, 3.575539752957411, 0.4255089838989079, 0.4205693469848484, 0.4210189519217238, 0.41914403694681823, 0.4194161738269031, 0.4126913029467687, 0.41175345703959465, 0.41357662703376263, 0.41114914300851524, 0.41427471907809377, 0.4115461460314691, 0.41239800897892565, 0.40976371499709785, 0.41329436702653766, 0.41576280095614493, 0.41238809295464307, 0.4174686190672219, 0.42160350806079805, 0.3991938289254904, 0.4048148051369935, 0.40347252995707095, 0.4029288301244378, 0.40406619396526366, 0.40476918302010745, 0.40443357394542545, 0.4078355310484767, 0.4067420370411128, 0.4055246760835871, 0.4070508120348677, 0.4072904549539089, 0.40886903007049114, 0.4084047090727836, 0.43778699403628707, 0.42744470795150846, 0.4292471361113712, 2.529269721941091, 3.865044398000464, 0.40576803497970104, 0.40253415587358177, 0.39988403604365885, 0.4016862348653376, 0.39927604410331696, 0.40090651204809546, 0.40389435505494475, 0.40892280696425587, 0.40608060103841126, 0.40550056600477546, 0.40333351807203144, 0.3986515898723155, 0.38997635489795357, 0.3954955219523981, 0.40733034384902567, 0.4055417929776013, 0.40875307691749185, 0.40591850399505347, 0.4057440939359367, 0.41140750295016915, 0.40640018112026155, 0.38698464597109705, 0.3865297099109739, 0.3798566850600764, 0.3823676169849932, 0.37924960907548666, 0.3829559850273654, 0.3871816099854186, 4.4607799970544875, 0.5608556130900979, 0.4103870699182153, 0.3994302700739354, 0.40348502702545375, 0.39236620196606964, 0.3924969349754974, 2.4273839519592, 0.4257907741703093, 0.41249269794207066, 0.4130113300634548, 0.41319144784938544, 0.41673247306607664, 0.41684940294362605, 0.40941225993447006, 0.4136713850311935, 0.41554390301462263, 0.41096334904432297, 0.41563938197214156, 0.4105465029133484, 0.43359422590583563, 0.41357600793708116, 0.4106731270439923, 0.4105992008699104, 6.503546220017597, 1.2502041830448434, 0.40597087889909744, 0.3888627190608531, 0.38557292602490634, 0.3858055010205135, 0.391235182993114, 0.3987660249695182, 0.39752940693870187, 0.40206074505113065, 3.50825674389489, 1.4460012359777465, 0.3942878559464589, 0.39081925991922617, 0.3875110319349915, 0.38572173996362835, 0.3840311529347673, 0.3828794478904456, 0.3829069819767028, 0.3821607279824093, 0.3810808430425823, 0.3805966740474105, 0.38306563801597804, 0.3823751650052145, 0.3818465779768303, 0.3832264430820942, 0.382826616987586, 0.3891403389861807, 0.38419765897560865, 0.3814855980454013, 0.41418824705760926, 0.4213139219209552, 0.424076288822107, 0.41639025299809873, 0.39984803495462984, 0.394612954929471, 0.3884200919419527, 0.3890439480310306, 3.796575739979744, 4.816910061170347, 2.7422449650475755]
Total Epoch List: [9, 322, 365]
Total Time List: [0.06914844003040344, 0.09317560005001724, 0.29018584289588034]
========================training times:2========================
========================k_idx:0========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: AEtransGAT
AEtransGAT(
  (gae): GAE(
    (encoder): transGAT(
      (encoder): EdgeFeatureGAT()
      (classifier): Sequential(
        (0): Linear(in_features=64, out_features=64, bias=True)
        (1): ReLU(inplace=True)
        (2): Linear(in_features=64, out_features=2, bias=True)
      )
    )
    (decoder): InnerProductDecoder()
  )
)
Total number of parameters: 1914242
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7d35981f3e80>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 2.5209;  Loss pred: 2.5209; Loss self: 0.0000; time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6957 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6935 score: 0.5102 time: 0.07s
Epoch 2/1000, LR 0.000000
Train loss: 2.5189;  Loss pred: 2.5189; Loss self: 0.0000; time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6956 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6934 score: 0.5102 time: 0.07s
Epoch 3/1000, LR 0.000030
Train loss: 2.5345;  Loss pred: 2.5345; Loss self: 0.0000; time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6955 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6934 score: 0.5102 time: 0.07s
Epoch 4/1000, LR 0.000060
Train loss: 2.5156;  Loss pred: 2.5156; Loss self: 0.0000; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6954 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6933 score: 0.5102 time: 0.07s
Epoch 5/1000, LR 0.000090
Train loss: 2.4756;  Loss pred: 2.4756; Loss self: 0.0000; time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6952 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6932 score: 0.5102 time: 0.07s
Epoch 6/1000, LR 0.000120
Train loss: 2.4623;  Loss pred: 2.4623; Loss self: 0.0000; time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6950 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6931 score: 0.5102 time: 0.07s
Epoch 7/1000, LR 0.000150
Train loss: 2.4197;  Loss pred: 2.4197; Loss self: 0.0000; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6947 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6930 score: 0.5102 time: 0.07s
Epoch 8/1000, LR 0.000180
Train loss: 2.4133;  Loss pred: 2.4133; Loss self: 0.0000; time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6945 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6929 score: 0.5102 time: 0.07s
Epoch 9/1000, LR 0.000210
Train loss: 2.3828;  Loss pred: 2.3828; Loss self: 0.0000; time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6942 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6928 score: 0.5102 time: 0.07s
Epoch 10/1000, LR 0.000240
Train loss: 2.3262;  Loss pred: 2.3262; Loss self: 0.0000; time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6939 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6928 score: 0.5102 time: 0.07s
Epoch 11/1000, LR 0.000270
Train loss: 2.2898;  Loss pred: 2.2898; Loss self: 0.0000; time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6937 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6927 score: 0.5102 time: 0.07s
Epoch 12/1000, LR 0.000270
Train loss: 2.2751;  Loss pred: 2.2751; Loss self: 0.0000; time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6934 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6926 score: 0.5102 time: 0.07s
Epoch 13/1000, LR 0.000270
Train loss: 2.2165;  Loss pred: 2.2165; Loss self: 0.0000; time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6932 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6926 score: 0.5102 time: 0.07s
Epoch 14/1000, LR 0.000270
Train loss: 2.1685;  Loss pred: 2.1685; Loss self: 0.0000; time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6931 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6926 score: 0.5102 time: 0.07s
Epoch 15/1000, LR 0.000270
Train loss: 2.1403;  Loss pred: 2.1403; Loss self: 0.0000; time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6929 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6926 score: 0.5102 time: 0.07s
Epoch 16/1000, LR 0.000270
Train loss: 2.0929;  Loss pred: 2.0929; Loss self: 0.0000; time: 0.23s
Val loss: 0.6927 score: 0.5102 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6926 score: 0.5102 time: 0.07s
Epoch 17/1000, LR 0.000270
Train loss: 2.0432;  Loss pred: 2.0432; Loss self: 0.0000; time: 0.23s
Val loss: 0.6926 score: 0.5306 time: 0.07s
Test loss: 0.6926 score: 0.5510 time: 0.07s
Epoch 18/1000, LR 0.000270
Train loss: 2.0328;  Loss pred: 2.0328; Loss self: 0.0000; time: 0.23s
Val loss: 0.6925 score: 0.5510 time: 0.08s
Test loss: 0.6926 score: 0.5510 time: 0.07s
Epoch 19/1000, LR 0.000270
Train loss: 1.9903;  Loss pred: 1.9903; Loss self: 0.0000; time: 0.23s
Val loss: 0.6924 score: 0.6327 time: 0.07s
Test loss: 0.6925 score: 0.6531 time: 0.06s
Epoch 20/1000, LR 0.000270
Train loss: 1.9555;  Loss pred: 1.9555; Loss self: 0.0000; time: 0.23s
Val loss: 0.6923 score: 0.5510 time: 0.07s
Test loss: 0.6925 score: 0.5102 time: 0.07s
Epoch 21/1000, LR 0.000270
Train loss: 1.9345;  Loss pred: 1.9345; Loss self: 0.0000; time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6922 score: 0.5102 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6925 score: 0.4898 time: 0.07s
Epoch 22/1000, LR 0.000270
Train loss: 1.8978;  Loss pred: 1.8978; Loss self: 0.0000; time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6921 score: 0.5102 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6925 score: 0.4898 time: 0.07s
Epoch 23/1000, LR 0.000270
Train loss: 1.8749;  Loss pred: 1.8749; Loss self: 0.0000; time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6920 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6925 score: 0.4898 time: 0.07s
Epoch 24/1000, LR 0.000270
Train loss: 1.8234;  Loss pred: 1.8234; Loss self: 0.0000; time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6919 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6924 score: 0.4898 time: 0.07s
Epoch 25/1000, LR 0.000270
Train loss: 1.7979;  Loss pred: 1.7979; Loss self: 0.0000; time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6919 score: 0.5102 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6924 score: 0.4898 time: 0.07s
Epoch 26/1000, LR 0.000270
Train loss: 1.7844;  Loss pred: 1.7844; Loss self: 0.0000; time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6918 score: 0.5102 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6924 score: 0.4898 time: 0.07s
Epoch 27/1000, LR 0.000270
Train loss: 1.7520;  Loss pred: 1.7520; Loss self: 0.0000; time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6917 score: 0.5102 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6923 score: 0.4898 time: 0.07s
Epoch 28/1000, LR 0.000270
Train loss: 1.7090;  Loss pred: 1.7090; Loss self: 0.0000; time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6916 score: 0.5102 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6923 score: 0.4898 time: 0.07s
Epoch 29/1000, LR 0.000270
Train loss: 1.7025;  Loss pred: 1.7025; Loss self: 0.0000; time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6915 score: 0.5102 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6922 score: 0.4898 time: 0.07s
Epoch 30/1000, LR 0.000270
Train loss: 1.6827;  Loss pred: 1.6827; Loss self: 0.0000; time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6915 score: 0.5102 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6922 score: 0.4898 time: 0.07s
Epoch 31/1000, LR 0.000270
Train loss: 1.6640;  Loss pred: 1.6640; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6914 score: 0.5102 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6921 score: 0.4898 time: 0.07s
Epoch 32/1000, LR 0.000270
Train loss: 1.6023;  Loss pred: 1.6023; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6913 score: 0.5102 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6920 score: 0.4898 time: 0.07s
Epoch 33/1000, LR 0.000270
Train loss: 1.5956;  Loss pred: 1.5956; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6912 score: 0.5102 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6920 score: 0.4898 time: 0.07s
Epoch 34/1000, LR 0.000270
Train loss: 1.5753;  Loss pred: 1.5753; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6911 score: 0.5102 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6919 score: 0.4898 time: 0.07s
Epoch 35/1000, LR 0.000270
Train loss: 1.5606;  Loss pred: 1.5606; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6910 score: 0.5102 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6918 score: 0.4898 time: 0.07s
Epoch 36/1000, LR 0.000270
Train loss: 1.5321;  Loss pred: 1.5321; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6909 score: 0.5102 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6917 score: 0.4898 time: 0.07s
Epoch 37/1000, LR 0.000270
Train loss: 1.5192;  Loss pred: 1.5192; Loss self: 0.0000; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6908 score: 0.5102 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6916 score: 0.4898 time: 0.07s
Epoch 38/1000, LR 0.000270
Train loss: 1.4951;  Loss pred: 1.4951; Loss self: 0.0000; time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6907 score: 0.5102 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6915 score: 0.4898 time: 0.07s
Epoch 39/1000, LR 0.000269
Train loss: 1.4682;  Loss pred: 1.4682; Loss self: 0.0000; time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6906 score: 0.5102 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6914 score: 0.4898 time: 0.07s
Epoch 40/1000, LR 0.000269
Train loss: 1.4743;  Loss pred: 1.4743; Loss self: 0.0000; time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6905 score: 0.5102 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6913 score: 0.4898 time: 0.07s
Epoch 41/1000, LR 0.000269
Train loss: 1.4477;  Loss pred: 1.4477; Loss self: 0.0000; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6904 score: 0.5102 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6912 score: 0.4898 time: 0.07s
Epoch 42/1000, LR 0.000269
Train loss: 1.4191;  Loss pred: 1.4191; Loss self: 0.0000; time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6903 score: 0.5102 time: 0.08s
Test loss: 0.6911 score: 0.5102 time: 0.07s
Epoch 43/1000, LR 0.000269
Train loss: 1.4157;  Loss pred: 1.4157; Loss self: 0.0000; time: 0.25s
Val loss: 0.6901 score: 0.5918 time: 0.08s
Test loss: 0.6910 score: 0.5306 time: 0.07s
Epoch 44/1000, LR 0.000269
Train loss: 1.3886;  Loss pred: 1.3886; Loss self: 0.0000; time: 0.25s
Val loss: 0.6900 score: 0.7143 time: 0.08s
Test loss: 0.6908 score: 0.6531 time: 0.07s
Epoch 45/1000, LR 0.000269
Train loss: 1.3780;  Loss pred: 1.3780; Loss self: 0.0000; time: 0.26s
Val loss: 0.6899 score: 0.8163 time: 0.08s
Test loss: 0.6907 score: 0.7551 time: 0.08s
Epoch 46/1000, LR 0.000269
Train loss: 1.3623;  Loss pred: 1.3623; Loss self: 0.0000; time: 0.27s
Val loss: 0.6897 score: 0.8776 time: 0.15s
Test loss: 0.6906 score: 0.8367 time: 0.07s
Epoch 47/1000, LR 0.000269
Train loss: 1.3545;  Loss pred: 1.3545; Loss self: 0.0000; time: 0.24s
Val loss: 0.6896 score: 0.8980 time: 0.08s
Test loss: 0.6905 score: 0.8163 time: 0.07s
Epoch 48/1000, LR 0.000269
Train loss: 1.3398;  Loss pred: 1.3398; Loss self: 0.0000; time: 1.92s
Val loss: 0.6895 score: 0.8980 time: 2.48s
Test loss: 0.6903 score: 0.8163 time: 2.46s
Epoch 49/1000, LR 0.000269
Train loss: 1.3376;  Loss pred: 1.3376; Loss self: 0.0000; time: 4.67s
Val loss: 0.6893 score: 0.8776 time: 0.15s
Test loss: 0.6902 score: 0.8367 time: 0.08s
Epoch 50/1000, LR 0.000269
Train loss: 1.3171;  Loss pred: 1.3171; Loss self: 0.0000; time: 0.26s
Val loss: 0.6891 score: 0.8776 time: 0.08s
Test loss: 0.6901 score: 0.8367 time: 0.07s
Epoch 51/1000, LR 0.000269
Train loss: 1.3070;  Loss pred: 1.3070; Loss self: 0.0000; time: 0.25s
Val loss: 0.6890 score: 0.8980 time: 0.08s
Test loss: 0.6899 score: 0.8163 time: 0.07s
Epoch 52/1000, LR 0.000269
Train loss: 1.2883;  Loss pred: 1.2883; Loss self: 0.0000; time: 0.25s
Val loss: 0.6888 score: 0.8980 time: 0.08s
Test loss: 0.6898 score: 0.8163 time: 0.07s
Epoch 53/1000, LR 0.000269
Train loss: 1.2866;  Loss pred: 1.2866; Loss self: 0.0000; time: 0.25s
Val loss: 0.6886 score: 0.8776 time: 0.08s
Test loss: 0.6896 score: 0.8367 time: 0.07s
Epoch 54/1000, LR 0.000269
Train loss: 1.2651;  Loss pred: 1.2651; Loss self: 0.0000; time: 0.25s
Val loss: 0.6884 score: 0.8776 time: 0.30s
Test loss: 0.6894 score: 0.7959 time: 1.37s
Epoch 55/1000, LR 0.000269
Train loss: 1.2609;  Loss pred: 1.2609; Loss self: 0.0000; time: 6.06s
Val loss: 0.6882 score: 0.8367 time: 2.15s
Test loss: 0.6893 score: 0.7755 time: 0.60s
Epoch 56/1000, LR 0.000269
Train loss: 1.2518;  Loss pred: 1.2518; Loss self: 0.0000; time: 0.57s
Val loss: 0.6880 score: 0.8367 time: 0.08s
Test loss: 0.6891 score: 0.7755 time: 0.07s
Epoch 57/1000, LR 0.000269
Train loss: 1.2352;  Loss pred: 1.2352; Loss self: 0.0000; time: 0.25s
Val loss: 0.6878 score: 0.8367 time: 0.08s
Test loss: 0.6889 score: 0.7755 time: 0.07s
Epoch 58/1000, LR 0.000269
Train loss: 1.2307;  Loss pred: 1.2307; Loss self: 0.0000; time: 0.25s
Val loss: 0.6875 score: 0.8367 time: 0.08s
Test loss: 0.6887 score: 0.7347 time: 0.07s
Epoch 59/1000, LR 0.000268
Train loss: 1.2190;  Loss pred: 1.2190; Loss self: 0.0000; time: 0.25s
Val loss: 0.6873 score: 0.8367 time: 0.08s
Test loss: 0.6886 score: 0.7143 time: 0.07s
Epoch 60/1000, LR 0.000268
Train loss: 1.2064;  Loss pred: 1.2064; Loss self: 0.0000; time: 0.25s
Val loss: 0.6870 score: 0.8163 time: 0.08s
Test loss: 0.6883 score: 0.7143 time: 0.07s
Epoch 61/1000, LR 0.000268
Train loss: 1.2051;  Loss pred: 1.2051; Loss self: 0.0000; time: 0.25s
Val loss: 0.6868 score: 0.8163 time: 0.08s
Test loss: 0.6881 score: 0.7143 time: 0.07s
Epoch 62/1000, LR 0.000268
Train loss: 1.1984;  Loss pred: 1.1984; Loss self: 0.0000; time: 0.27s
Val loss: 0.6865 score: 0.7755 time: 0.40s
Test loss: 0.6878 score: 0.7143 time: 0.71s
Epoch 63/1000, LR 0.000268
Train loss: 1.1801;  Loss pred: 1.1801; Loss self: 0.0000; time: 1.99s
Val loss: 0.6862 score: 0.7347 time: 0.15s
Test loss: 0.6876 score: 0.7143 time: 0.29s
Epoch 64/1000, LR 0.000268
Train loss: 1.1812;  Loss pred: 1.1812; Loss self: 0.0000; time: 1.38s
Val loss: 0.6858 score: 0.7551 time: 0.09s
Test loss: 0.6873 score: 0.7143 time: 0.08s
Epoch 65/1000, LR 0.000268
Train loss: 1.1693;  Loss pred: 1.1693; Loss self: 0.0000; time: 0.27s
Val loss: 0.6855 score: 0.8367 time: 0.09s
Test loss: 0.6870 score: 0.7143 time: 0.08s
Epoch 66/1000, LR 0.000268
Train loss: 1.1652;  Loss pred: 1.1652; Loss self: 0.0000; time: 0.27s
Val loss: 0.6852 score: 0.8367 time: 0.09s
Test loss: 0.6868 score: 0.7143 time: 0.08s
Epoch 67/1000, LR 0.000268
Train loss: 1.1555;  Loss pred: 1.1555; Loss self: 0.0000; time: 0.27s
Val loss: 0.6848 score: 0.8367 time: 0.09s
Test loss: 0.6865 score: 0.7755 time: 0.08s
Epoch 68/1000, LR 0.000268
Train loss: 1.1489;  Loss pred: 1.1489; Loss self: 0.0000; time: 0.26s
Val loss: 0.6844 score: 0.8367 time: 0.08s
Test loss: 0.6862 score: 0.7959 time: 0.07s
Epoch 69/1000, LR 0.000268
Train loss: 1.1444;  Loss pred: 1.1444; Loss self: 0.0000; time: 0.24s
Val loss: 0.6841 score: 0.8776 time: 0.08s
Test loss: 0.6859 score: 0.7959 time: 0.07s
Epoch 70/1000, LR 0.000268
Train loss: 1.1385;  Loss pred: 1.1385; Loss self: 0.0000; time: 0.24s
Val loss: 0.6837 score: 0.8776 time: 0.08s
Test loss: 0.6856 score: 0.7959 time: 0.07s
Epoch 71/1000, LR 0.000268
Train loss: 1.1378;  Loss pred: 1.1378; Loss self: 0.0000; time: 0.24s
Val loss: 0.6833 score: 0.8980 time: 0.08s
Test loss: 0.6853 score: 0.7959 time: 0.07s
Epoch 72/1000, LR 0.000267
Train loss: 1.1269;  Loss pred: 1.1269; Loss self: 0.0000; time: 0.24s
Val loss: 0.6828 score: 0.9184 time: 0.08s
Test loss: 0.6850 score: 0.7959 time: 0.07s
Epoch 73/1000, LR 0.000267
Train loss: 1.1244;  Loss pred: 1.1244; Loss self: 0.0000; time: 0.25s
Val loss: 0.6824 score: 0.9184 time: 0.08s
Test loss: 0.6847 score: 0.7959 time: 0.07s
Epoch 74/1000, LR 0.000267
Train loss: 1.1194;  Loss pred: 1.1194; Loss self: 0.0000; time: 0.25s
Val loss: 0.6819 score: 0.9184 time: 0.08s
Test loss: 0.6843 score: 0.8367 time: 0.07s
Epoch 75/1000, LR 0.000267
Train loss: 1.1130;  Loss pred: 1.1130; Loss self: 0.0000; time: 0.25s
Val loss: 0.6814 score: 0.9184 time: 0.08s
Test loss: 0.6839 score: 0.8571 time: 0.07s
Epoch 76/1000, LR 0.000267
Train loss: 1.1074;  Loss pred: 1.1074; Loss self: 0.0000; time: 0.24s
Val loss: 0.6809 score: 0.9184 time: 0.08s
Test loss: 0.6835 score: 0.8571 time: 0.07s
Epoch 77/1000, LR 0.000267
Train loss: 1.1011;  Loss pred: 1.1011; Loss self: 0.0000; time: 0.25s
Val loss: 0.6804 score: 0.9184 time: 0.08s
Test loss: 0.6832 score: 0.8571 time: 0.07s
Epoch 78/1000, LR 0.000267
Train loss: 1.0958;  Loss pred: 1.0958; Loss self: 0.0000; time: 0.30s
Val loss: 0.6799 score: 0.9184 time: 0.08s
Test loss: 0.6827 score: 0.8571 time: 0.07s
Epoch 79/1000, LR 0.000267
Train loss: 1.0932;  Loss pred: 1.0932; Loss self: 0.0000; time: 0.26s
Val loss: 0.6793 score: 0.9184 time: 0.08s
Test loss: 0.6823 score: 0.8571 time: 0.07s
Epoch 80/1000, LR 0.000267
Train loss: 1.0926;  Loss pred: 1.0926; Loss self: 0.0000; time: 0.27s
Val loss: 0.6788 score: 0.9184 time: 0.08s
Test loss: 0.6819 score: 0.8571 time: 0.07s
Epoch 81/1000, LR 0.000267
Train loss: 1.0842;  Loss pred: 1.0842; Loss self: 0.0000; time: 0.28s
Val loss: 0.6782 score: 0.9184 time: 0.61s
Test loss: 0.6814 score: 0.8571 time: 1.33s
Epoch 82/1000, LR 0.000267
Train loss: 1.0845;  Loss pred: 1.0845; Loss self: 0.0000; time: 5.67s
Val loss: 0.6776 score: 0.9184 time: 1.13s
Test loss: 0.6810 score: 0.8571 time: 1.96s
Epoch 83/1000, LR 0.000266
Train loss: 1.0782;  Loss pred: 1.0782; Loss self: 0.0000; time: 0.91s
Val loss: 0.6769 score: 0.9388 time: 0.37s
Test loss: 0.6805 score: 0.8571 time: 0.76s
Epoch 84/1000, LR 0.000266
Train loss: 1.0754;  Loss pred: 1.0754; Loss self: 0.0000; time: 1.42s
Val loss: 0.6762 score: 0.9388 time: 0.10s
Test loss: 0.6799 score: 0.8571 time: 0.23s
Epoch 85/1000, LR 0.000266
Train loss: 1.0690;  Loss pred: 1.0690; Loss self: 0.0000; time: 0.68s
Val loss: 0.6755 score: 0.9388 time: 0.08s
Test loss: 0.6794 score: 0.8571 time: 0.07s
Epoch 86/1000, LR 0.000266
Train loss: 1.0670;  Loss pred: 1.0670; Loss self: 0.0000; time: 0.23s
Val loss: 0.6748 score: 0.9592 time: 0.08s
Test loss: 0.6788 score: 0.8571 time: 0.07s
Epoch 87/1000, LR 0.000266
Train loss: 1.0614;  Loss pred: 1.0614; Loss self: 0.0000; time: 0.23s
Val loss: 0.6740 score: 0.9592 time: 0.08s
Test loss: 0.6782 score: 0.8571 time: 0.07s
Epoch 88/1000, LR 0.000266
Train loss: 1.0596;  Loss pred: 1.0596; Loss self: 0.0000; time: 0.23s
Val loss: 0.6732 score: 0.9592 time: 0.07s
Test loss: 0.6776 score: 0.8571 time: 0.07s
Epoch 89/1000, LR 0.000266
Train loss: 1.0547;  Loss pred: 1.0547; Loss self: 0.0000; time: 0.23s
Val loss: 0.6723 score: 0.9592 time: 0.08s
Test loss: 0.6769 score: 0.8571 time: 0.07s
Epoch 90/1000, LR 0.000266
Train loss: 1.0534;  Loss pred: 1.0534; Loss self: 0.0000; time: 0.23s
Val loss: 0.6714 score: 0.9592 time: 0.08s
Test loss: 0.6762 score: 0.8571 time: 0.07s
Epoch 91/1000, LR 0.000266
Train loss: 1.0501;  Loss pred: 1.0501; Loss self: 0.0000; time: 4.19s
Val loss: 0.6705 score: 0.9592 time: 2.00s
Test loss: 0.6755 score: 0.8571 time: 1.30s
Epoch 92/1000, LR 0.000266
Train loss: 1.0519;  Loss pred: 1.0519; Loss self: 0.0000; time: 2.83s
Val loss: 0.6696 score: 0.9592 time: 0.09s
Test loss: 0.6748 score: 0.8571 time: 0.08s
Epoch 93/1000, LR 0.000265
Train loss: 1.0441;  Loss pred: 1.0441; Loss self: 0.0000; time: 0.26s
Val loss: 0.6686 score: 0.9592 time: 0.08s
Test loss: 0.6741 score: 0.8571 time: 0.07s
Epoch 94/1000, LR 0.000265
Train loss: 1.0397;  Loss pred: 1.0397; Loss self: 0.0000; time: 0.25s
Val loss: 0.6676 score: 0.9592 time: 0.08s
Test loss: 0.6733 score: 0.8571 time: 0.07s
Epoch 95/1000, LR 0.000265
Train loss: 1.0396;  Loss pred: 1.0396; Loss self: 0.0000; time: 0.24s
Val loss: 0.6667 score: 0.9592 time: 0.08s
Test loss: 0.6726 score: 0.8571 time: 0.07s
Epoch 96/1000, LR 0.000265
Train loss: 1.0358;  Loss pred: 1.0358; Loss self: 0.0000; time: 0.24s
Val loss: 0.6656 score: 0.9592 time: 0.08s
Test loss: 0.6718 score: 0.8571 time: 0.07s
Epoch 97/1000, LR 0.000265
Train loss: 1.0327;  Loss pred: 1.0327; Loss self: 0.0000; time: 0.24s
Val loss: 0.6646 score: 0.9592 time: 0.08s
Test loss: 0.6710 score: 0.8571 time: 0.07s
Epoch 98/1000, LR 0.000265
Train loss: 1.0312;  Loss pred: 1.0312; Loss self: 0.0000; time: 0.24s
Val loss: 0.6635 score: 0.9592 time: 0.08s
Test loss: 0.6701 score: 0.8571 time: 0.07s
Epoch 99/1000, LR 0.000265
Train loss: 1.0330;  Loss pred: 1.0330; Loss self: 0.0000; time: 0.24s
Val loss: 0.6624 score: 0.9592 time: 0.08s
Test loss: 0.6692 score: 0.8571 time: 0.07s
Epoch 100/1000, LR 0.000265
Train loss: 1.0233;  Loss pred: 1.0233; Loss self: 0.0000; time: 0.24s
Val loss: 0.6612 score: 0.9592 time: 0.08s
Test loss: 0.6683 score: 0.8571 time: 0.07s
Epoch 101/1000, LR 0.000265
Train loss: 1.0206;  Loss pred: 1.0206; Loss self: 0.0000; time: 0.24s
Val loss: 0.6600 score: 0.9592 time: 0.08s
Test loss: 0.6674 score: 0.8571 time: 0.07s
Epoch 102/1000, LR 0.000264
Train loss: 1.0186;  Loss pred: 1.0186; Loss self: 0.0000; time: 0.24s
Val loss: 0.6588 score: 0.9592 time: 0.08s
Test loss: 0.6665 score: 0.8571 time: 0.07s
Epoch 103/1000, LR 0.000264
Train loss: 1.0187;  Loss pred: 1.0187; Loss self: 0.0000; time: 0.26s
Val loss: 0.6575 score: 0.9592 time: 2.01s
Test loss: 0.6655 score: 0.8571 time: 0.95s
Epoch 104/1000, LR 0.000264
Train loss: 1.0142;  Loss pred: 1.0142; Loss self: 0.0000; time: 4.93s
Val loss: 0.6562 score: 0.9592 time: 0.11s
Test loss: 0.6645 score: 0.8571 time: 0.09s
Epoch 105/1000, LR 0.000264
Train loss: 1.0136;  Loss pred: 1.0136; Loss self: 0.0000; time: 0.28s
Val loss: 0.6549 score: 0.9592 time: 0.09s
Test loss: 0.6634 score: 0.8571 time: 0.08s
Epoch 106/1000, LR 0.000264
Train loss: 1.0119;  Loss pred: 1.0119; Loss self: 0.0000; time: 0.27s
Val loss: 0.6535 score: 0.9592 time: 0.09s
Test loss: 0.6624 score: 0.8571 time: 0.08s
Epoch 107/1000, LR 0.000264
Train loss: 1.0085;  Loss pred: 1.0085; Loss self: 0.0000; time: 0.27s
Val loss: 0.6521 score: 0.9592 time: 0.09s
Test loss: 0.6613 score: 0.8571 time: 0.08s
Epoch 108/1000, LR 0.000264
Train loss: 1.0041;  Loss pred: 1.0041; Loss self: 0.0000; time: 0.27s
Val loss: 0.6507 score: 0.9592 time: 0.09s
Test loss: 0.6601 score: 0.8571 time: 0.08s
Epoch 109/1000, LR 0.000264
Train loss: 1.0075;  Loss pred: 1.0075; Loss self: 0.0000; time: 0.27s
Val loss: 0.6492 score: 0.9592 time: 0.09s
Test loss: 0.6590 score: 0.8571 time: 1.08s
Epoch 110/1000, LR 0.000263
Train loss: 1.0008;  Loss pred: 1.0008; Loss self: 0.0000; time: 2.61s
Val loss: 0.6476 score: 0.9592 time: 0.88s
Test loss: 0.6578 score: 0.8571 time: 0.30s
Epoch 111/1000, LR 0.000263
Train loss: 0.9968;  Loss pred: 0.9968; Loss self: 0.0000; time: 1.29s
Val loss: 0.6460 score: 0.9592 time: 0.11s
Test loss: 0.6566 score: 0.8571 time: 0.26s
Epoch 112/1000, LR 0.000263
Train loss: 0.9943;  Loss pred: 0.9943; Loss self: 0.0000; time: 0.72s
Val loss: 0.6443 score: 0.9592 time: 0.23s
Test loss: 0.6553 score: 0.8571 time: 0.22s
Epoch 113/1000, LR 0.000263
Train loss: 0.9938;  Loss pred: 0.9938; Loss self: 0.0000; time: 0.43s
Val loss: 0.6425 score: 0.9592 time: 0.08s
Test loss: 0.6540 score: 0.8571 time: 0.07s
Epoch 114/1000, LR 0.000263
Train loss: 0.9915;  Loss pred: 0.9915; Loss self: 0.0000; time: 0.26s
Val loss: 0.6407 score: 0.9592 time: 0.08s
Test loss: 0.6525 score: 0.8571 time: 0.07s
Epoch 115/1000, LR 0.000263
Train loss: 0.9856;  Loss pred: 0.9856; Loss self: 0.0000; time: 0.25s
Val loss: 0.6387 score: 0.9592 time: 0.08s
Test loss: 0.6510 score: 0.8571 time: 0.07s
Epoch 116/1000, LR 0.000263
Train loss: 0.9836;  Loss pred: 0.9836; Loss self: 0.0000; time: 0.25s
Val loss: 0.6367 score: 0.9592 time: 0.08s
Test loss: 0.6495 score: 0.8571 time: 0.07s
Epoch 117/1000, LR 0.000262
Train loss: 0.9835;  Loss pred: 0.9835; Loss self: 0.0000; time: 0.25s
Val loss: 0.6346 score: 0.9592 time: 0.08s
Test loss: 0.6479 score: 0.8571 time: 0.07s
Epoch 118/1000, LR 0.000262
Train loss: 0.9803;  Loss pred: 0.9803; Loss self: 0.0000; time: 0.25s
Val loss: 0.6326 score: 0.9592 time: 0.08s
Test loss: 0.6464 score: 0.8571 time: 0.07s
Epoch 119/1000, LR 0.000262
Train loss: 0.9795;  Loss pred: 0.9795; Loss self: 0.0000; time: 0.24s
Val loss: 0.6305 score: 0.9592 time: 0.08s
Test loss: 0.6448 score: 0.8571 time: 0.07s
Epoch 120/1000, LR 0.000262
Train loss: 0.9750;  Loss pred: 0.9750; Loss self: 0.0000; time: 0.25s
Val loss: 0.6283 score: 0.9592 time: 0.08s
Test loss: 0.6432 score: 0.8367 time: 0.07s
Epoch 121/1000, LR 0.000262
Train loss: 0.9726;  Loss pred: 0.9726; Loss self: 0.0000; time: 0.25s
Val loss: 0.6262 score: 0.9592 time: 0.08s
Test loss: 0.6416 score: 0.8367 time: 0.07s
Epoch 122/1000, LR 0.000262
Train loss: 0.9709;  Loss pred: 0.9709; Loss self: 0.0000; time: 0.25s
Val loss: 0.6240 score: 0.9592 time: 0.08s
Test loss: 0.6399 score: 0.8367 time: 0.07s
Epoch 123/1000, LR 0.000262
Train loss: 0.9692;  Loss pred: 0.9692; Loss self: 0.0000; time: 0.25s
Val loss: 0.6217 score: 0.9592 time: 0.08s
Test loss: 0.6382 score: 0.8367 time: 0.07s
Epoch 124/1000, LR 0.000261
Train loss: 0.9661;  Loss pred: 0.9661; Loss self: 0.0000; time: 0.24s
Val loss: 0.6194 score: 0.9592 time: 0.08s
Test loss: 0.6365 score: 0.8367 time: 0.07s
Epoch 125/1000, LR 0.000261
Train loss: 0.9622;  Loss pred: 0.9622; Loss self: 0.0000; time: 0.25s
Val loss: 0.6171 score: 0.9592 time: 0.08s
Test loss: 0.6347 score: 0.8367 time: 0.07s
Epoch 126/1000, LR 0.000261
Train loss: 0.9633;  Loss pred: 0.9633; Loss self: 0.0000; time: 0.25s
Val loss: 0.6147 score: 0.9592 time: 0.08s
Test loss: 0.6328 score: 0.8367 time: 0.07s
Epoch 127/1000, LR 0.000261
Train loss: 0.9576;  Loss pred: 0.9576; Loss self: 0.0000; time: 0.24s
Val loss: 0.6122 score: 0.9592 time: 0.08s
Test loss: 0.6310 score: 0.8367 time: 0.07s
Epoch 128/1000, LR 0.000261
Train loss: 0.9571;  Loss pred: 0.9571; Loss self: 0.0000; time: 0.24s
Val loss: 0.6098 score: 0.9592 time: 0.08s
Test loss: 0.6291 score: 0.8367 time: 0.07s
Epoch 129/1000, LR 0.000261
Train loss: 0.9515;  Loss pred: 0.9515; Loss self: 0.0000; time: 1.24s
Val loss: 0.6072 score: 0.9592 time: 0.42s
Test loss: 0.6272 score: 0.8367 time: 1.11s
Epoch 130/1000, LR 0.000260
Train loss: 0.9503;  Loss pred: 0.9503; Loss self: 0.0000; time: 3.23s
Val loss: 0.6046 score: 0.9592 time: 0.61s
Test loss: 0.6252 score: 0.8163 time: 0.40s
Epoch 131/1000, LR 0.000260
Train loss: 0.9453;  Loss pred: 0.9453; Loss self: 0.0000; time: 1.32s
Val loss: 0.6019 score: 0.9592 time: 0.33s
Test loss: 0.6232 score: 0.8163 time: 1.12s
Epoch 132/1000, LR 0.000260
Train loss: 0.9460;  Loss pred: 0.9460; Loss self: 0.0000; time: 0.70s
Val loss: 0.5992 score: 0.9592 time: 0.08s
Test loss: 0.6212 score: 0.8163 time: 0.08s
Epoch 133/1000, LR 0.000260
Train loss: 0.9421;  Loss pred: 0.9421; Loss self: 0.0000; time: 0.26s
Val loss: 0.5964 score: 0.9592 time: 0.08s
Test loss: 0.6191 score: 0.8163 time: 0.07s
Epoch 134/1000, LR 0.000260
Train loss: 0.9414;  Loss pred: 0.9414; Loss self: 0.0000; time: 0.25s
Val loss: 0.5936 score: 0.9592 time: 0.08s
Test loss: 0.6170 score: 0.8163 time: 0.07s
Epoch 135/1000, LR 0.000260
Train loss: 0.9376;  Loss pred: 0.9376; Loss self: 0.0000; time: 0.24s
Val loss: 0.5908 score: 0.9592 time: 0.08s
Test loss: 0.6149 score: 0.8163 time: 0.07s
Epoch 136/1000, LR 0.000260
Train loss: 0.9342;  Loss pred: 0.9342; Loss self: 0.0000; time: 0.24s
Val loss: 0.5879 score: 0.9592 time: 0.08s
Test loss: 0.6127 score: 0.8163 time: 0.07s
Epoch 137/1000, LR 0.000259
Train loss: 0.9322;  Loss pred: 0.9322; Loss self: 0.0000; time: 0.24s
Val loss: 0.5849 score: 0.9592 time: 0.08s
Test loss: 0.6105 score: 0.8163 time: 0.07s
Epoch 138/1000, LR 0.000259
Train loss: 0.9291;  Loss pred: 0.9291; Loss self: 0.0000; time: 0.24s
Val loss: 0.5819 score: 0.9592 time: 0.08s
Test loss: 0.6082 score: 0.8163 time: 0.07s
Epoch 139/1000, LR 0.000259
Train loss: 0.9272;  Loss pred: 0.9272; Loss self: 0.0000; time: 0.24s
Val loss: 0.5788 score: 0.9592 time: 0.08s
Test loss: 0.6059 score: 0.8163 time: 0.07s
Epoch 140/1000, LR 0.000259
Train loss: 0.9224;  Loss pred: 0.9224; Loss self: 0.0000; time: 0.23s
Val loss: 0.5757 score: 0.9592 time: 0.08s
Test loss: 0.6035 score: 0.8163 time: 0.07s
Epoch 141/1000, LR 0.000259
Train loss: 0.9231;  Loss pred: 0.9231; Loss self: 0.0000; time: 0.23s
Val loss: 0.5726 score: 0.9592 time: 0.08s
Test loss: 0.6011 score: 0.8163 time: 0.07s
Epoch 142/1000, LR 0.000259
Train loss: 0.9187;  Loss pred: 0.9187; Loss self: 0.0000; time: 0.25s
Val loss: 0.5694 score: 0.9592 time: 0.08s
Test loss: 0.5987 score: 0.8163 time: 0.07s
Epoch 143/1000, LR 0.000258
Train loss: 0.9151;  Loss pred: 0.9151; Loss self: 0.0000; time: 0.25s
Val loss: 0.5661 score: 0.9592 time: 0.08s
Test loss: 0.5963 score: 0.8163 time: 0.07s
Epoch 144/1000, LR 0.000258
Train loss: 0.9135;  Loss pred: 0.9135; Loss self: 0.0000; time: 0.26s
Val loss: 0.5628 score: 0.9592 time: 2.04s
Test loss: 0.5939 score: 0.8163 time: 1.88s
Epoch 145/1000, LR 0.000258
Train loss: 0.9109;  Loss pred: 0.9109; Loss self: 0.0000; time: 0.95s
Val loss: 0.5595 score: 0.9592 time: 0.09s
Test loss: 0.5914 score: 0.8163 time: 0.08s
Epoch 146/1000, LR 0.000258
Train loss: 0.9071;  Loss pred: 0.9071; Loss self: 0.0000; time: 0.26s
Val loss: 0.5561 score: 0.9592 time: 0.08s
Test loss: 0.5889 score: 0.8163 time: 0.07s
Epoch 147/1000, LR 0.000258
Train loss: 0.9047;  Loss pred: 0.9047; Loss self: 0.0000; time: 0.26s
Val loss: 0.5527 score: 0.9592 time: 0.08s
Test loss: 0.5864 score: 0.8367 time: 0.07s
Epoch 148/1000, LR 0.000257
Train loss: 0.9021;  Loss pred: 0.9021; Loss self: 0.0000; time: 0.25s
Val loss: 0.5492 score: 0.9592 time: 0.08s
Test loss: 0.5838 score: 0.8367 time: 0.07s
Epoch 149/1000, LR 0.000257
Train loss: 0.8988;  Loss pred: 0.8988; Loss self: 0.0000; time: 0.25s
Val loss: 0.5457 score: 0.9592 time: 0.08s
Test loss: 0.5813 score: 0.8367 time: 0.07s
Epoch 150/1000, LR 0.000257
Train loss: 0.8952;  Loss pred: 0.8952; Loss self: 0.0000; time: 0.24s
Val loss: 0.5421 score: 0.9592 time: 0.08s
Test loss: 0.5787 score: 0.8367 time: 0.07s
Epoch 151/1000, LR 0.000257
Train loss: 0.8931;  Loss pred: 0.8931; Loss self: 0.0000; time: 0.24s
Val loss: 0.5385 score: 0.9592 time: 0.08s
Test loss: 0.5760 score: 0.8367 time: 0.07s
Epoch 152/1000, LR 0.000257
Train loss: 0.8877;  Loss pred: 0.8877; Loss self: 0.0000; time: 0.24s
Val loss: 0.5349 score: 0.9592 time: 0.08s
Test loss: 0.5734 score: 0.8367 time: 0.07s
Epoch 153/1000, LR 0.000257
Train loss: 0.8848;  Loss pred: 0.8848; Loss self: 0.0000; time: 0.26s
Val loss: 0.5312 score: 0.9592 time: 0.38s
Test loss: 0.5706 score: 0.8367 time: 2.54s
Epoch 154/1000, LR 0.000256
Train loss: 0.8840;  Loss pred: 0.8840; Loss self: 0.0000; time: 3.29s
Val loss: 0.5274 score: 0.9592 time: 0.85s
Test loss: 0.5679 score: 0.8367 time: 1.18s
Epoch 155/1000, LR 0.000256
Train loss: 0.8807;  Loss pred: 0.8807; Loss self: 0.0000; time: 0.61s
Val loss: 0.5237 score: 0.9592 time: 0.29s
Test loss: 0.5651 score: 0.8367 time: 0.07s
Epoch 156/1000, LR 0.000256
Train loss: 0.8768;  Loss pred: 0.8768; Loss self: 0.0000; time: 0.25s
Val loss: 0.5199 score: 0.9592 time: 0.08s
Test loss: 0.5623 score: 0.8367 time: 0.07s
Epoch 157/1000, LR 0.000256
Train loss: 0.8749;  Loss pred: 0.8749; Loss self: 0.0000; time: 0.25s
Val loss: 0.5161 score: 0.9592 time: 0.08s
Test loss: 0.5594 score: 0.8367 time: 0.07s
Epoch 158/1000, LR 0.000256
Train loss: 0.8723;  Loss pred: 0.8723; Loss self: 0.0000; time: 0.25s
Val loss: 0.5122 score: 0.9592 time: 0.08s
Test loss: 0.5565 score: 0.8367 time: 0.07s
Epoch 159/1000, LR 0.000255
Train loss: 0.8698;  Loss pred: 0.8698; Loss self: 0.0000; time: 0.25s
Val loss: 0.5083 score: 0.9592 time: 0.08s
Test loss: 0.5537 score: 0.8367 time: 0.07s
Epoch 160/1000, LR 0.000255
Train loss: 0.8660;  Loss pred: 0.8660; Loss self: 0.0000; time: 0.25s
Val loss: 0.5044 score: 0.9592 time: 0.08s
Test loss: 0.5508 score: 0.8367 time: 0.07s
Epoch 161/1000, LR 0.000255
Train loss: 0.8630;  Loss pred: 0.8630; Loss self: 0.0000; time: 0.25s
Val loss: 0.5004 score: 0.9592 time: 0.08s
Test loss: 0.5480 score: 0.8367 time: 0.07s
Epoch 162/1000, LR 0.000255
Train loss: 0.8597;  Loss pred: 0.8597; Loss self: 0.0000; time: 0.25s
Val loss: 0.4964 score: 0.9592 time: 0.08s
Test loss: 0.5451 score: 0.8367 time: 0.07s
Epoch 163/1000, LR 0.000255
Train loss: 0.8572;  Loss pred: 0.8572; Loss self: 0.0000; time: 3.41s
Val loss: 0.4924 score: 0.9592 time: 0.32s
Test loss: 0.5423 score: 0.8367 time: 0.62s
Epoch 164/1000, LR 0.000254
Train loss: 0.8517;  Loss pred: 0.8517; Loss self: 0.0000; time: 4.04s
Val loss: 0.4884 score: 0.9592 time: 0.18s
Test loss: 0.5395 score: 0.8367 time: 0.75s
Epoch 165/1000, LR 0.000254
Train loss: 0.8486;  Loss pred: 0.8486; Loss self: 0.0000; time: 2.49s
Val loss: 0.4843 score: 0.9592 time: 0.12s
Test loss: 0.5366 score: 0.8367 time: 0.25s
Epoch 166/1000, LR 0.000254
Train loss: 0.8487;  Loss pred: 0.8487; Loss self: 0.0000; time: 0.28s
Val loss: 0.4803 score: 0.9592 time: 0.08s
Test loss: 0.5337 score: 0.8367 time: 0.07s
Epoch 167/1000, LR 0.000254
Train loss: 0.8421;  Loss pred: 0.8421; Loss self: 0.0000; time: 0.26s
Val loss: 0.4763 score: 0.9592 time: 0.08s
Test loss: 0.5308 score: 0.8367 time: 0.07s
Epoch 168/1000, LR 0.000254
Train loss: 0.8406;  Loss pred: 0.8406; Loss self: 0.0000; time: 0.26s
Val loss: 0.4722 score: 0.9592 time: 0.08s
Test loss: 0.5279 score: 0.8367 time: 0.07s
Epoch 169/1000, LR 0.000253
Train loss: 0.8385;  Loss pred: 0.8385; Loss self: 0.0000; time: 0.26s
Val loss: 0.4682 score: 0.9592 time: 0.08s
Test loss: 0.5250 score: 0.8367 time: 0.07s
Epoch 170/1000, LR 0.000253
Train loss: 0.8340;  Loss pred: 0.8340; Loss self: 0.0000; time: 0.26s
Val loss: 0.4641 score: 0.9592 time: 0.08s
Test loss: 0.5220 score: 0.8367 time: 0.07s
Epoch 171/1000, LR 0.000253
Train loss: 0.8315;  Loss pred: 0.8315; Loss self: 0.0000; time: 0.26s
Val loss: 0.4600 score: 0.9592 time: 0.08s
Test loss: 0.5190 score: 0.8367 time: 0.07s
Epoch 172/1000, LR 0.000253
Train loss: 0.8270;  Loss pred: 0.8270; Loss self: 0.0000; time: 0.24s
Val loss: 0.4560 score: 0.9592 time: 0.08s
Test loss: 0.5161 score: 0.8367 time: 0.16s
Epoch 173/1000, LR 0.000253
Train loss: 0.8230;  Loss pred: 0.8230; Loss self: 0.0000; time: 3.26s
Val loss: 0.4519 score: 0.9592 time: 0.47s
Test loss: 0.5131 score: 0.8367 time: 0.62s
Epoch 174/1000, LR 0.000252
Train loss: 0.8208;  Loss pred: 0.8208; Loss self: 0.0000; time: 2.21s
Val loss: 0.4478 score: 0.9592 time: 1.34s
Test loss: 0.5101 score: 0.8367 time: 1.19s
Epoch 175/1000, LR 0.000252
Train loss: 0.8202;  Loss pred: 0.8202; Loss self: 0.0000; time: 0.51s
Val loss: 0.4437 score: 0.9592 time: 0.08s
Test loss: 0.5072 score: 0.8367 time: 0.07s
Epoch 176/1000, LR 0.000252
Train loss: 0.8176;  Loss pred: 0.8176; Loss self: 0.0000; time: 0.25s
Val loss: 0.4396 score: 0.9592 time: 0.08s
Test loss: 0.5043 score: 0.8367 time: 0.07s
Epoch 177/1000, LR 0.000252
Train loss: 0.8112;  Loss pred: 0.8112; Loss self: 0.0000; time: 0.25s
Val loss: 0.4355 score: 0.9592 time: 0.08s
Test loss: 0.5014 score: 0.8367 time: 0.07s
Epoch 178/1000, LR 0.000251
Train loss: 0.8100;  Loss pred: 0.8100; Loss self: 0.0000; time: 0.25s
Val loss: 0.4314 score: 0.9592 time: 0.08s
Test loss: 0.4985 score: 0.8367 time: 0.07s
Epoch 179/1000, LR 0.000251
Train loss: 0.8074;  Loss pred: 0.8074; Loss self: 0.0000; time: 0.24s
Val loss: 0.4273 score: 0.9592 time: 0.08s
Test loss: 0.4956 score: 0.8367 time: 0.07s
Epoch 180/1000, LR 0.000251
Train loss: 0.8023;  Loss pred: 0.8023; Loss self: 0.0000; time: 0.25s
Val loss: 0.4232 score: 0.9592 time: 0.08s
Test loss: 0.4928 score: 0.8367 time: 0.07s
Epoch 181/1000, LR 0.000251
Train loss: 0.8009;  Loss pred: 0.8009; Loss self: 0.0000; time: 0.25s
Val loss: 0.4190 score: 0.9592 time: 0.08s
Test loss: 0.4899 score: 0.8367 time: 0.07s
Epoch 182/1000, LR 0.000251
Train loss: 0.7955;  Loss pred: 0.7955; Loss self: 0.0000; time: 0.25s
Val loss: 0.4149 score: 0.9592 time: 0.08s
Test loss: 0.4871 score: 0.8367 time: 0.07s
Epoch 183/1000, LR 0.000250
Train loss: 0.7928;  Loss pred: 0.7928; Loss self: 0.0000; time: 0.25s
Val loss: 0.4108 score: 0.9592 time: 0.08s
Test loss: 0.4842 score: 0.8367 time: 0.07s
Epoch 184/1000, LR 0.000250
Train loss: 0.7907;  Loss pred: 0.7907; Loss self: 0.0000; time: 0.24s
Val loss: 0.4068 score: 0.9592 time: 0.08s
Test loss: 0.4813 score: 0.8367 time: 0.07s
Epoch 185/1000, LR 0.000250
Train loss: 0.7888;  Loss pred: 0.7888; Loss self: 0.0000; time: 0.24s
Val loss: 0.4027 score: 0.9592 time: 0.08s
Test loss: 0.4784 score: 0.8367 time: 0.07s
Epoch 186/1000, LR 0.000250
Train loss: 0.7849;  Loss pred: 0.7849; Loss self: 0.0000; time: 0.24s
Val loss: 0.3987 score: 0.9592 time: 0.08s
Test loss: 0.4755 score: 0.8367 time: 0.07s
Epoch 187/1000, LR 0.000249
Train loss: 0.7827;  Loss pred: 0.7827; Loss self: 0.0000; time: 0.23s
Val loss: 0.3947 score: 0.9592 time: 0.08s
Test loss: 0.4726 score: 0.8367 time: 0.07s
Epoch 188/1000, LR 0.000249
Train loss: 0.7781;  Loss pred: 0.7781; Loss self: 0.0000; time: 0.26s
Val loss: 0.3907 score: 0.9592 time: 1.91s
Test loss: 0.4697 score: 0.8367 time: 1.65s
Epoch 189/1000, LR 0.000249
Train loss: 0.7770;  Loss pred: 0.7770; Loss self: 0.0000; time: 4.88s
Val loss: 0.3868 score: 0.9592 time: 0.64s
Test loss: 0.4669 score: 0.8367 time: 0.10s
Epoch 190/1000, LR 0.000249
Train loss: 0.7737;  Loss pred: 0.7737; Loss self: 0.0000; time: 0.27s
Val loss: 0.3829 score: 0.9592 time: 0.09s
Test loss: 0.4642 score: 0.8367 time: 0.08s
Epoch 191/1000, LR 0.000249
Train loss: 0.7705;  Loss pred: 0.7705; Loss self: 0.0000; time: 0.27s
Val loss: 0.3789 score: 0.9592 time: 0.09s
Test loss: 0.4614 score: 0.8367 time: 0.08s
Epoch 192/1000, LR 0.000248
Train loss: 0.7690;  Loss pred: 0.7690; Loss self: 0.0000; time: 0.26s
Val loss: 0.3750 score: 0.9592 time: 0.09s
Test loss: 0.4588 score: 0.8367 time: 0.07s
Epoch 193/1000, LR 0.000248
Train loss: 0.7646;  Loss pred: 0.7646; Loss self: 0.0000; time: 0.26s
Val loss: 0.3711 score: 0.9592 time: 0.09s
Test loss: 0.4562 score: 0.8367 time: 0.08s
Epoch 194/1000, LR 0.000248
Train loss: 0.7617;  Loss pred: 0.7617; Loss self: 0.0000; time: 0.26s
Val loss: 0.3672 score: 0.9592 time: 0.09s
Test loss: 0.4537 score: 0.8367 time: 0.08s
Epoch 195/1000, LR 0.000248
Train loss: 0.7590;  Loss pred: 0.7590; Loss self: 0.0000; time: 0.26s
Val loss: 0.3633 score: 0.9592 time: 0.09s
Test loss: 0.4512 score: 0.8367 time: 0.08s
Epoch 196/1000, LR 0.000247
Train loss: 0.7544;  Loss pred: 0.7544; Loss self: 0.0000; time: 0.27s
Val loss: 0.3595 score: 0.9388 time: 0.09s
Test loss: 0.4487 score: 0.8367 time: 0.08s
Epoch 197/1000, LR 0.000247
Train loss: 0.7541;  Loss pred: 0.7541; Loss self: 0.0000; time: 0.26s
Val loss: 0.3557 score: 0.9388 time: 0.08s
Test loss: 0.4461 score: 0.8367 time: 0.07s
Epoch 198/1000, LR 0.000247
Train loss: 0.7485;  Loss pred: 0.7485; Loss self: 0.0000; time: 0.25s
Val loss: 0.3520 score: 0.9388 time: 0.89s
Test loss: 0.4436 score: 0.8367 time: 1.99s
Epoch 199/1000, LR 0.000247
Train loss: 0.7471;  Loss pred: 0.7471; Loss self: 0.0000; time: 2.45s
Val loss: 0.3482 score: 0.9388 time: 0.13s
Test loss: 0.4410 score: 0.8367 time: 0.20s
Epoch 200/1000, LR 0.000246
Train loss: 0.7448;  Loss pred: 0.7448; Loss self: 0.0000; time: 1.09s
Val loss: 0.3445 score: 0.9388 time: 0.51s
Test loss: 0.4384 score: 0.8367 time: 0.08s
Epoch 201/1000, LR 0.000246
Train loss: 0.7422;  Loss pred: 0.7422; Loss self: 0.0000; time: 0.27s
Val loss: 0.3408 score: 0.9388 time: 0.09s
Test loss: 0.4358 score: 0.8367 time: 0.07s
Epoch 202/1000, LR 0.000246
Train loss: 0.7412;  Loss pred: 0.7412; Loss self: 0.0000; time: 0.26s
Val loss: 0.3372 score: 0.9388 time: 0.08s
Test loss: 0.4333 score: 0.8367 time: 0.07s
Epoch 203/1000, LR 0.000246
Train loss: 0.7359;  Loss pred: 0.7359; Loss self: 0.0000; time: 0.26s
Val loss: 0.3336 score: 0.9592 time: 0.08s
Test loss: 0.4307 score: 0.8367 time: 0.07s
Epoch 204/1000, LR 0.000245
Train loss: 0.7367;  Loss pred: 0.7367; Loss self: 0.0000; time: 0.28s
Val loss: 0.3301 score: 0.9592 time: 0.08s
Test loss: 0.4282 score: 0.8367 time: 0.07s
Epoch 205/1000, LR 0.000245
Train loss: 0.7316;  Loss pred: 0.7316; Loss self: 0.0000; time: 0.24s
Val loss: 0.3265 score: 0.9592 time: 0.08s
Test loss: 0.4258 score: 0.8367 time: 0.07s
Epoch 206/1000, LR 0.000245
Train loss: 0.7283;  Loss pred: 0.7283; Loss self: 0.0000; time: 0.24s
Val loss: 0.3230 score: 0.9592 time: 0.08s
Test loss: 0.4234 score: 0.8367 time: 0.07s
Epoch 207/1000, LR 0.000245
Train loss: 0.7242;  Loss pred: 0.7242; Loss self: 0.0000; time: 0.24s
Val loss: 0.3195 score: 0.9388 time: 0.08s
Test loss: 0.4211 score: 0.8367 time: 0.07s
Epoch 208/1000, LR 0.000244
Train loss: 0.7237;  Loss pred: 0.7237; Loss self: 0.0000; time: 0.26s
Val loss: 0.3160 score: 0.9388 time: 0.08s
Test loss: 0.4188 score: 0.8367 time: 0.13s
Epoch 209/1000, LR 0.000244
Train loss: 0.7213;  Loss pred: 0.7213; Loss self: 0.0000; time: 5.11s
Val loss: 0.3126 score: 0.9388 time: 0.08s
Test loss: 0.4166 score: 0.8367 time: 0.07s
Epoch 210/1000, LR 0.000244
Train loss: 0.7162;  Loss pred: 0.7162; Loss self: 0.0000; time: 0.24s
Val loss: 0.3092 score: 0.9388 time: 0.08s
Test loss: 0.4144 score: 0.8367 time: 0.07s
Epoch 211/1000, LR 0.000244
Train loss: 0.7133;  Loss pred: 0.7133; Loss self: 0.0000; time: 0.24s
Val loss: 0.3059 score: 0.9388 time: 0.08s
Test loss: 0.4122 score: 0.8367 time: 0.07s
Epoch 212/1000, LR 0.000243
Train loss: 0.7117;  Loss pred: 0.7117; Loss self: 0.0000; time: 0.24s
Val loss: 0.3026 score: 0.9388 time: 0.08s
Test loss: 0.4100 score: 0.8367 time: 0.07s
Epoch 213/1000, LR 0.000243
Train loss: 0.7115;  Loss pred: 0.7115; Loss self: 0.0000; time: 0.25s
Val loss: 0.2993 score: 0.9388 time: 0.08s
Test loss: 0.4079 score: 0.8367 time: 0.07s
Epoch 214/1000, LR 0.000243
Train loss: 0.7085;  Loss pred: 0.7085; Loss self: 0.0000; time: 0.24s
Val loss: 0.2961 score: 0.9388 time: 0.08s
Test loss: 0.4057 score: 0.8367 time: 0.07s
Epoch 215/1000, LR 0.000243
Train loss: 0.7064;  Loss pred: 0.7064; Loss self: 0.0000; time: 0.24s
Val loss: 0.2929 score: 0.9388 time: 0.08s
Test loss: 0.4036 score: 0.8367 time: 0.07s
Epoch 216/1000, LR 0.000242
Train loss: 0.7023;  Loss pred: 0.7023; Loss self: 0.0000; time: 0.24s
Val loss: 0.2898 score: 0.9388 time: 0.08s
Test loss: 0.4015 score: 0.8367 time: 0.08s
Epoch 217/1000, LR 0.000242
Train loss: 0.7023;  Loss pred: 0.7023; Loss self: 0.0000; time: 2.06s
Val loss: 0.2867 score: 0.9388 time: 0.76s
Test loss: 0.3993 score: 0.8367 time: 0.73s
Epoch 218/1000, LR 0.000242
Train loss: 0.6983;  Loss pred: 0.6983; Loss self: 0.0000; time: 2.48s
Val loss: 0.2836 score: 0.9388 time: 0.59s
Test loss: 0.3972 score: 0.8367 time: 0.54s
Epoch 219/1000, LR 0.000242
Train loss: 0.6973;  Loss pred: 0.6973; Loss self: 0.0000; time: 1.14s
Val loss: 0.2806 score: 0.9388 time: 0.09s
Test loss: 0.3952 score: 0.8367 time: 0.07s
Epoch 220/1000, LR 0.000241
Train loss: 0.6957;  Loss pred: 0.6957; Loss self: 0.0000; time: 0.25s
Val loss: 0.2776 score: 0.9388 time: 0.08s
Test loss: 0.3932 score: 0.8367 time: 0.07s
Epoch 221/1000, LR 0.000241
Train loss: 0.6914;  Loss pred: 0.6914; Loss self: 0.0000; time: 0.25s
Val loss: 0.2747 score: 0.9388 time: 0.08s
Test loss: 0.3913 score: 0.8367 time: 0.07s
Epoch 222/1000, LR 0.000241
Train loss: 0.6902;  Loss pred: 0.6902; Loss self: 0.0000; time: 0.25s
Val loss: 0.2717 score: 0.9388 time: 0.08s
Test loss: 0.3894 score: 0.8367 time: 0.07s
Epoch 223/1000, LR 0.000241
Train loss: 0.6889;  Loss pred: 0.6889; Loss self: 0.0000; time: 0.25s
Val loss: 0.2689 score: 0.9388 time: 0.08s
Test loss: 0.3876 score: 0.8367 time: 0.07s
Epoch 224/1000, LR 0.000240
Train loss: 0.6849;  Loss pred: 0.6849; Loss self: 0.0000; time: 0.25s
Val loss: 0.2660 score: 0.9388 time: 1.64s
Test loss: 0.3858 score: 0.8367 time: 1.71s
Epoch 225/1000, LR 0.000240
Train loss: 0.6851;  Loss pred: 0.6851; Loss self: 0.0000; time: 4.84s
Val loss: 0.2632 score: 0.9388 time: 1.37s
Test loss: 0.3840 score: 0.8367 time: 1.45s
Epoch 226/1000, LR 0.000240
Train loss: 0.6830;  Loss pred: 0.6830; Loss self: 0.0000; time: 2.64s
Val loss: 0.2605 score: 0.9388 time: 0.09s
Test loss: 0.3823 score: 0.8367 time: 0.07s
Epoch 227/1000, LR 0.000240
Train loss: 0.6784;  Loss pred: 0.6784; Loss self: 0.0000; time: 0.25s
Val loss: 0.2577 score: 0.9388 time: 0.08s
Test loss: 0.3805 score: 0.8367 time: 0.07s
Epoch 228/1000, LR 0.000239
Train loss: 0.6756;  Loss pred: 0.6756; Loss self: 0.0000; time: 0.24s
Val loss: 0.2551 score: 0.9388 time: 0.08s
Test loss: 0.3788 score: 0.8367 time: 0.07s
Epoch 229/1000, LR 0.000239
Train loss: 0.6769;  Loss pred: 0.6769; Loss self: 0.0000; time: 0.24s
Val loss: 0.2524 score: 0.9388 time: 0.08s
Test loss: 0.3771 score: 0.8367 time: 0.07s
Epoch 230/1000, LR 0.000239
Train loss: 0.6719;  Loss pred: 0.6719; Loss self: 0.0000; time: 0.24s
Val loss: 0.2499 score: 0.9388 time: 0.08s
Test loss: 0.3753 score: 0.8367 time: 0.07s
Epoch 231/1000, LR 0.000238
Train loss: 0.6726;  Loss pred: 0.6726; Loss self: 0.0000; time: 0.25s
Val loss: 0.2473 score: 0.9388 time: 0.08s
Test loss: 0.3736 score: 0.8367 time: 0.07s
Epoch 232/1000, LR 0.000238
Train loss: 0.6696;  Loss pred: 0.6696; Loss self: 0.0000; time: 0.25s
Val loss: 0.2448 score: 0.9388 time: 0.08s
Test loss: 0.3719 score: 0.8367 time: 0.07s
Epoch 233/1000, LR 0.000238
Train loss: 0.6681;  Loss pred: 0.6681; Loss self: 0.0000; time: 0.25s
Val loss: 0.2423 score: 0.9388 time: 0.08s
Test loss: 0.3704 score: 0.8367 time: 0.07s
Epoch 234/1000, LR 0.000238
Train loss: 0.6666;  Loss pred: 0.6666; Loss self: 0.0000; time: 1.09s
Val loss: 0.2399 score: 0.9388 time: 0.75s
Test loss: 0.3688 score: 0.8367 time: 1.71s
Epoch 235/1000, LR 0.000237
Train loss: 0.6660;  Loss pred: 0.6660; Loss self: 0.0000; time: 1.83s
Val loss: 0.2375 score: 0.9388 time: 0.11s
Test loss: 0.3673 score: 0.8367 time: 0.11s
Epoch 236/1000, LR 0.000237
Train loss: 0.6607;  Loss pred: 0.6607; Loss self: 0.0000; time: 0.49s
Val loss: 0.2351 score: 0.9388 time: 0.08s
Test loss: 0.3658 score: 0.8367 time: 0.07s
Epoch 237/1000, LR 0.000237
Train loss: 0.6593;  Loss pred: 0.6593; Loss self: 0.0000; time: 0.25s
Val loss: 0.2328 score: 0.9388 time: 0.08s
Test loss: 0.3644 score: 0.8367 time: 0.07s
Epoch 238/1000, LR 0.000236
Train loss: 0.6580;  Loss pred: 0.6580; Loss self: 0.0000; time: 0.24s
Val loss: 0.2305 score: 0.9388 time: 0.08s
Test loss: 0.3629 score: 0.8367 time: 0.07s
Epoch 239/1000, LR 0.000236
Train loss: 0.6559;  Loss pred: 0.6559; Loss self: 0.0000; time: 0.24s
Val loss: 0.2282 score: 0.9388 time: 0.08s
Test loss: 0.3615 score: 0.8367 time: 0.07s
Epoch 240/1000, LR 0.000236
Train loss: 0.6573;  Loss pred: 0.6573; Loss self: 0.0000; time: 0.25s
Val loss: 0.2260 score: 0.9388 time: 0.08s
Test loss: 0.3601 score: 0.8367 time: 0.07s
Epoch 241/1000, LR 0.000236
Train loss: 0.6540;  Loss pred: 0.6540; Loss self: 0.0000; time: 0.25s
Val loss: 0.2238 score: 0.9388 time: 0.08s
Test loss: 0.3587 score: 0.8367 time: 0.07s
Epoch 242/1000, LR 0.000235
Train loss: 0.6512;  Loss pred: 0.6512; Loss self: 0.0000; time: 0.25s
Val loss: 0.2217 score: 0.9388 time: 0.08s
Test loss: 0.3574 score: 0.8367 time: 0.07s
Epoch 243/1000, LR 0.000235
Train loss: 0.6500;  Loss pred: 0.6500; Loss self: 0.0000; time: 0.25s
Val loss: 0.2196 score: 0.9388 time: 1.81s
Test loss: 0.3561 score: 0.8367 time: 1.15s
Epoch 244/1000, LR 0.000235
Train loss: 0.6496;  Loss pred: 0.6496; Loss self: 0.0000; time: 3.05s
Val loss: 0.2175 score: 0.9388 time: 0.76s
Test loss: 0.3549 score: 0.8367 time: 0.97s
Epoch 245/1000, LR 0.000234
Train loss: 0.6479;  Loss pred: 0.6479; Loss self: 0.0000; time: 2.18s
Val loss: 0.2154 score: 0.9388 time: 0.10s
Test loss: 0.3536 score: 0.8367 time: 0.08s
Epoch 246/1000, LR 0.000234
Train loss: 0.6443;  Loss pred: 0.6443; Loss self: 0.0000; time: 0.25s
Val loss: 0.2134 score: 0.9388 time: 0.08s
Test loss: 0.3523 score: 0.8367 time: 0.07s
Epoch 247/1000, LR 0.000234
Train loss: 0.6466;  Loss pred: 0.6466; Loss self: 0.0000; time: 0.24s
Val loss: 0.2114 score: 0.9388 time: 0.08s
Test loss: 0.3510 score: 0.8367 time: 0.07s
Epoch 248/1000, LR 0.000234
Train loss: 0.6431;  Loss pred: 0.6431; Loss self: 0.0000; time: 0.24s
Val loss: 0.2095 score: 0.9388 time: 0.08s
Test loss: 0.3497 score: 0.8367 time: 0.07s
Epoch 249/1000, LR 0.000233
Train loss: 0.6448;  Loss pred: 0.6448; Loss self: 0.0000; time: 0.27s
Val loss: 0.2076 score: 0.9388 time: 0.08s
Test loss: 0.3485 score: 0.8367 time: 0.07s
Epoch 250/1000, LR 0.000233
Train loss: 0.6420;  Loss pred: 0.6420; Loss self: 0.0000; time: 0.24s
Val loss: 0.2057 score: 0.9388 time: 0.08s
Test loss: 0.3473 score: 0.8367 time: 0.07s
Epoch 251/1000, LR 0.000233
Train loss: 0.6352;  Loss pred: 0.6352; Loss self: 0.0000; time: 0.24s
Val loss: 0.2038 score: 0.9388 time: 0.08s
Test loss: 0.3462 score: 0.8367 time: 0.07s
Epoch 252/1000, LR 0.000232
Train loss: 0.6355;  Loss pred: 0.6355; Loss self: 0.0000; time: 0.24s
Val loss: 0.2020 score: 0.9388 time: 0.08s
Test loss: 0.3450 score: 0.8367 time: 0.07s
Epoch 253/1000, LR 0.000232
Train loss: 0.6344;  Loss pred: 0.6344; Loss self: 0.0000; time: 5.44s
Val loss: 0.2002 score: 0.9388 time: 0.76s
Test loss: 0.3439 score: 0.8367 time: 0.81s
Epoch 254/1000, LR 0.000232
Train loss: 0.6326;  Loss pred: 0.6326; Loss self: 0.0000; time: 1.24s
Val loss: 0.1984 score: 0.9388 time: 0.09s
Test loss: 0.3429 score: 0.8367 time: 0.07s
Epoch 255/1000, LR 0.000232
Train loss: 0.6325;  Loss pred: 0.6325; Loss self: 0.0000; time: 0.26s
Val loss: 0.1967 score: 0.9388 time: 0.08s
Test loss: 0.3418 score: 0.8367 time: 0.07s
Epoch 256/1000, LR 0.000231
Train loss: 0.6301;  Loss pred: 0.6301; Loss self: 0.0000; time: 0.25s
Val loss: 0.1950 score: 0.9388 time: 0.08s
Test loss: 0.3408 score: 0.8367 time: 0.07s
Epoch 257/1000, LR 0.000231
Train loss: 0.6293;  Loss pred: 0.6293; Loss self: 0.0000; time: 0.25s
Val loss: 0.1933 score: 0.9388 time: 0.08s
Test loss: 0.3398 score: 0.8367 time: 0.07s
Epoch 258/1000, LR 0.000231
Train loss: 0.6292;  Loss pred: 0.6292; Loss self: 0.0000; time: 0.25s
Val loss: 0.1917 score: 0.9388 time: 0.08s
Test loss: 0.3388 score: 0.8367 time: 0.07s
Epoch 259/1000, LR 0.000230
Train loss: 0.6261;  Loss pred: 0.6261; Loss self: 0.0000; time: 0.25s
Val loss: 0.1900 score: 0.9388 time: 0.08s
Test loss: 0.3378 score: 0.8367 time: 0.07s
Epoch 260/1000, LR 0.000230
Train loss: 0.6235;  Loss pred: 0.6235; Loss self: 0.0000; time: 0.25s
Val loss: 0.1884 score: 0.9388 time: 0.08s
Test loss: 0.3369 score: 0.8367 time: 0.07s
Epoch 261/1000, LR 0.000230
Train loss: 0.6245;  Loss pred: 0.6245; Loss self: 0.0000; time: 0.25s
Val loss: 0.1869 score: 0.9388 time: 0.08s
Test loss: 0.3359 score: 0.8367 time: 0.07s
Epoch 262/1000, LR 0.000229
Train loss: 0.6223;  Loss pred: 0.6223; Loss self: 0.0000; time: 0.26s
Val loss: 0.1853 score: 0.9388 time: 0.09s
Test loss: 0.3349 score: 0.8367 time: 0.07s
Epoch 263/1000, LR 0.000229
Train loss: 0.6234;  Loss pred: 0.6234; Loss self: 0.0000; time: 2.98s
Val loss: 0.1837 score: 0.9388 time: 0.38s
Test loss: 0.3339 score: 0.8367 time: 0.09s
Epoch 264/1000, LR 0.000229
Train loss: 0.6193;  Loss pred: 0.6193; Loss self: 0.0000; time: 0.25s
Val loss: 0.1822 score: 0.9388 time: 0.08s
Test loss: 0.3329 score: 0.8367 time: 0.07s
Epoch 265/1000, LR 0.000228
Train loss: 0.6195;  Loss pred: 0.6195; Loss self: 0.0000; time: 0.24s
Val loss: 0.1807 score: 0.9388 time: 0.08s
Test loss: 0.3319 score: 0.8367 time: 0.07s
Epoch 266/1000, LR 0.000228
Train loss: 0.6162;  Loss pred: 0.6162; Loss self: 0.0000; time: 0.24s
Val loss: 0.1792 score: 0.9388 time: 0.08s
Test loss: 0.3310 score: 0.8367 time: 0.07s
Epoch 267/1000, LR 0.000228
Train loss: 0.6170;  Loss pred: 0.6170; Loss self: 0.0000; time: 0.24s
Val loss: 0.1778 score: 0.9388 time: 0.08s
Test loss: 0.3301 score: 0.8367 time: 0.07s
Epoch 268/1000, LR 0.000228
Train loss: 0.6159;  Loss pred: 0.6159; Loss self: 0.0000; time: 0.25s
Val loss: 0.1764 score: 0.9388 time: 0.08s
Test loss: 0.3293 score: 0.8367 time: 0.07s
Epoch 269/1000, LR 0.000227
Train loss: 0.6151;  Loss pred: 0.6151; Loss self: 0.0000; time: 0.25s
Val loss: 0.1750 score: 0.9388 time: 0.08s
Test loss: 0.3285 score: 0.8367 time: 0.07s
Epoch 270/1000, LR 0.000227
Train loss: 0.6115;  Loss pred: 0.6115; Loss self: 0.0000; time: 0.25s
Val loss: 0.1737 score: 0.9388 time: 0.08s
Test loss: 0.3278 score: 0.8367 time: 0.07s
Epoch 271/1000, LR 0.000227
Train loss: 0.6162;  Loss pred: 0.6162; Loss self: 0.0000; time: 0.25s
Val loss: 0.1723 score: 0.9388 time: 0.08s
Test loss: 0.3270 score: 0.8367 time: 0.07s
Epoch 272/1000, LR 0.000226
Train loss: 0.6121;  Loss pred: 0.6121; Loss self: 0.0000; time: 0.25s
Val loss: 0.1710 score: 0.9388 time: 0.09s
Test loss: 0.3263 score: 0.8367 time: 1.83s
Epoch 273/1000, LR 0.000226
Train loss: 0.6091;  Loss pred: 0.6091; Loss self: 0.0000; time: 3.92s
Val loss: 0.1697 score: 0.9388 time: 1.20s
Test loss: 0.3256 score: 0.8571 time: 1.30s
Epoch 274/1000, LR 0.000226
Train loss: 0.6078;  Loss pred: 0.6078; Loss self: 0.0000; time: 1.93s
Val loss: 0.1685 score: 0.9388 time: 0.44s
Test loss: 0.3249 score: 0.8571 time: 0.10s
Epoch 275/1000, LR 0.000225
Train loss: 0.6074;  Loss pred: 0.6074; Loss self: 0.0000; time: 0.26s
Val loss: 0.1672 score: 0.9388 time: 0.08s
Test loss: 0.3242 score: 0.8571 time: 0.07s
Epoch 276/1000, LR 0.000225
Train loss: 0.6131;  Loss pred: 0.6131; Loss self: 0.0000; time: 0.25s
Val loss: 0.1660 score: 0.9388 time: 0.08s
Test loss: 0.3235 score: 0.8571 time: 0.07s
Epoch 277/1000, LR 0.000225
Train loss: 0.6062;  Loss pred: 0.6062; Loss self: 0.0000; time: 0.25s
Val loss: 0.1648 score: 0.9388 time: 0.08s
Test loss: 0.3227 score: 0.8571 time: 0.07s
Epoch 278/1000, LR 0.000224
Train loss: 0.6057;  Loss pred: 0.6057; Loss self: 0.0000; time: 0.25s
Val loss: 0.1635 score: 0.9388 time: 0.08s
Test loss: 0.3220 score: 0.8571 time: 0.07s
Epoch 279/1000, LR 0.000224
Train loss: 0.6047;  Loss pred: 0.6047; Loss self: 0.0000; time: 0.25s
Val loss: 0.1623 score: 0.9388 time: 0.08s
Test loss: 0.3211 score: 0.8571 time: 0.07s
Epoch 280/1000, LR 0.000224
Train loss: 0.6047;  Loss pred: 0.6047; Loss self: 0.0000; time: 0.25s
Val loss: 0.1611 score: 0.9388 time: 0.08s
Test loss: 0.3203 score: 0.8367 time: 0.07s
Epoch 281/1000, LR 0.000223
Train loss: 0.6021;  Loss pred: 0.6021; Loss self: 0.0000; time: 0.25s
Val loss: 0.1599 score: 0.9388 time: 0.08s
Test loss: 0.3195 score: 0.8367 time: 0.07s
Epoch 282/1000, LR 0.000223
Train loss: 0.6021;  Loss pred: 0.6021; Loss self: 0.0000; time: 0.25s
Val loss: 0.1588 score: 0.9388 time: 0.08s
Test loss: 0.3188 score: 0.8367 time: 0.07s
Epoch 283/1000, LR 0.000223
Train loss: 0.6021;  Loss pred: 0.6021; Loss self: 0.0000; time: 0.25s
Val loss: 0.1576 score: 0.9388 time: 0.08s
Test loss: 0.3182 score: 0.8367 time: 0.07s
Epoch 284/1000, LR 0.000222
Train loss: 0.6018;  Loss pred: 0.6018; Loss self: 0.0000; time: 0.25s
Val loss: 0.1565 score: 0.9388 time: 0.08s
Test loss: 0.3175 score: 0.8367 time: 0.07s
Epoch 285/1000, LR 0.000222
Train loss: 0.5986;  Loss pred: 0.5986; Loss self: 0.0000; time: 0.25s
Val loss: 0.1555 score: 0.9388 time: 0.08s
Test loss: 0.3170 score: 0.8367 time: 0.07s
Epoch 286/1000, LR 0.000222
Train loss: 0.6001;  Loss pred: 0.6001; Loss self: 0.0000; time: 0.25s
Val loss: 0.1544 score: 0.9388 time: 0.08s
Test loss: 0.3164 score: 0.8367 time: 0.08s
Epoch 287/1000, LR 0.000221
Train loss: 0.5960;  Loss pred: 0.5960; Loss self: 0.0000; time: 6.19s
Val loss: 0.1534 score: 0.9388 time: 0.93s
Test loss: 0.3159 score: 0.8571 time: 0.33s
Epoch 288/1000, LR 0.000221
Train loss: 0.5976;  Loss pred: 0.5976; Loss self: 0.0000; time: 1.94s
Val loss: 0.1524 score: 0.9388 time: 0.35s
Test loss: 0.3155 score: 0.8571 time: 0.08s
Epoch 289/1000, LR 0.000221
Train loss: 0.5964;  Loss pred: 0.5964; Loss self: 0.0000; time: 0.25s
Val loss: 0.1515 score: 0.9388 time: 0.08s
Test loss: 0.3150 score: 0.8571 time: 0.07s
Epoch 290/1000, LR 0.000220
Train loss: 0.5924;  Loss pred: 0.5924; Loss self: 0.0000; time: 0.25s
Val loss: 0.1505 score: 0.9388 time: 0.08s
Test loss: 0.3145 score: 0.8571 time: 0.07s
Epoch 291/1000, LR 0.000220
Train loss: 0.5955;  Loss pred: 0.5955; Loss self: 0.0000; time: 0.25s
Val loss: 0.1496 score: 0.9388 time: 0.08s
Test loss: 0.3140 score: 0.8571 time: 0.07s
Epoch 292/1000, LR 0.000220
Train loss: 0.5938;  Loss pred: 0.5938; Loss self: 0.0000; time: 0.25s
Val loss: 0.1486 score: 0.9388 time: 0.08s
Test loss: 0.3134 score: 0.8571 time: 0.07s
Epoch 293/1000, LR 0.000219
Train loss: 0.5946;  Loss pred: 0.5946; Loss self: 0.0000; time: 0.25s
Val loss: 0.1476 score: 0.9388 time: 0.08s
Test loss: 0.3128 score: 0.8571 time: 0.07s
Epoch 294/1000, LR 0.000219
Train loss: 0.5904;  Loss pred: 0.5904; Loss self: 0.0000; time: 0.25s
Val loss: 0.1466 score: 0.9388 time: 0.08s
Test loss: 0.3122 score: 0.8571 time: 0.07s
Epoch 295/1000, LR 0.000219
Train loss: 0.5902;  Loss pred: 0.5902; Loss self: 0.0000; time: 0.25s
Val loss: 0.1457 score: 0.9388 time: 0.08s
Test loss: 0.3116 score: 0.8571 time: 0.07s
Epoch 296/1000, LR 0.000218
Train loss: 0.5900;  Loss pred: 0.5900; Loss self: 0.0000; time: 0.25s
Val loss: 0.1447 score: 0.9388 time: 0.09s
Test loss: 0.3110 score: 0.8571 time: 1.35s
Epoch 297/1000, LR 0.000218
Train loss: 0.5930;  Loss pred: 0.5930; Loss self: 0.0000; time: 4.59s
Val loss: 0.1438 score: 0.9388 time: 0.99s
Test loss: 0.3105 score: 0.8571 time: 1.14s
Epoch 298/1000, LR 0.000218
Train loss: 0.5871;  Loss pred: 0.5871; Loss self: 0.0000; time: 5.53s
Val loss: 0.1429 score: 0.9388 time: 0.17s
Test loss: 0.3100 score: 0.8571 time: 0.07s
Epoch 299/1000, LR 0.000217
Train loss: 0.5859;  Loss pred: 0.5859; Loss self: 0.0000; time: 0.25s
Val loss: 0.1420 score: 0.9388 time: 0.08s
Test loss: 0.3095 score: 0.8571 time: 0.07s
Epoch 300/1000, LR 0.000217
Train loss: 0.5891;  Loss pred: 0.5891; Loss self: 0.0000; time: 0.24s
Val loss: 0.1411 score: 0.9388 time: 0.08s
Test loss: 0.3090 score: 0.8571 time: 0.07s
Epoch 301/1000, LR 0.000217
Train loss: 0.5847;  Loss pred: 0.5847; Loss self: 0.0000; time: 0.24s
Val loss: 0.1403 score: 0.9388 time: 0.08s
Test loss: 0.3086 score: 0.8571 time: 0.07s
Epoch 302/1000, LR 0.000216
Train loss: 0.5875;  Loss pred: 0.5875; Loss self: 0.0000; time: 0.25s
Val loss: 0.1395 score: 0.9388 time: 0.08s
Test loss: 0.3081 score: 0.8571 time: 0.07s
Epoch 303/1000, LR 0.000216
Train loss: 0.5832;  Loss pred: 0.5832; Loss self: 0.0000; time: 0.26s
Val loss: 0.1387 score: 0.9388 time: 0.08s
Test loss: 0.3077 score: 0.8571 time: 0.07s
Epoch 304/1000, LR 0.000216
Train loss: 0.5855;  Loss pred: 0.5855; Loss self: 0.0000; time: 0.24s
Val loss: 0.1379 score: 0.9388 time: 0.08s
Test loss: 0.3073 score: 0.8571 time: 0.07s
Epoch 305/1000, LR 0.000215
Train loss: 0.5839;  Loss pred: 0.5839; Loss self: 0.0000; time: 1.67s
Val loss: 0.1371 score: 0.9388 time: 0.35s
Test loss: 0.3069 score: 0.8571 time: 0.84s
Epoch 306/1000, LR 0.000215
Train loss: 0.5839;  Loss pred: 0.5839; Loss self: 0.0000; time: 2.12s
Val loss: 0.1363 score: 0.9388 time: 0.54s
Test loss: 0.3064 score: 0.8571 time: 0.15s
Epoch 307/1000, LR 0.000215
Train loss: 0.5820;  Loss pred: 0.5820; Loss self: 0.0000; time: 0.80s
Val loss: 0.1355 score: 0.9388 time: 0.08s
Test loss: 0.3060 score: 0.8571 time: 0.08s
Epoch 308/1000, LR 0.000214
Train loss: 0.5822;  Loss pred: 0.5822; Loss self: 0.0000; time: 0.25s
Val loss: 0.1348 score: 0.9388 time: 0.08s
Test loss: 0.3056 score: 0.8571 time: 0.07s
Epoch 309/1000, LR 0.000214
Train loss: 0.5816;  Loss pred: 0.5816; Loss self: 0.0000; time: 0.24s
Val loss: 0.1340 score: 0.9388 time: 0.08s
Test loss: 0.3052 score: 0.8571 time: 0.07s
Epoch 310/1000, LR 0.000214
Train loss: 0.5808;  Loss pred: 0.5808; Loss self: 0.0000; time: 0.24s
Val loss: 0.1333 score: 0.9388 time: 0.08s
Test loss: 0.3048 score: 0.8571 time: 0.07s
Epoch 311/1000, LR 0.000213
Train loss: 0.5790;  Loss pred: 0.5790; Loss self: 0.0000; time: 0.24s
Val loss: 0.1325 score: 0.9388 time: 0.08s
Test loss: 0.3044 score: 0.8571 time: 0.07s
Epoch 312/1000, LR 0.000213
Train loss: 0.5798;  Loss pred: 0.5798; Loss self: 0.0000; time: 0.24s
Val loss: 0.1318 score: 0.9388 time: 0.08s
Test loss: 0.3040 score: 0.8571 time: 0.07s
Epoch 313/1000, LR 0.000213
Train loss: 0.5789;  Loss pred: 0.5789; Loss self: 0.0000; time: 0.24s
Val loss: 0.1311 score: 0.9388 time: 0.08s
Test loss: 0.3036 score: 0.8571 time: 0.07s
Epoch 314/1000, LR 0.000212
Train loss: 0.5784;  Loss pred: 0.5784; Loss self: 0.0000; time: 0.24s
Val loss: 0.1304 score: 0.9388 time: 0.08s
Test loss: 0.3033 score: 0.8571 time: 0.07s
Epoch 315/1000, LR 0.000212
Train loss: 0.5783;  Loss pred: 0.5783; Loss self: 0.0000; time: 0.24s
Val loss: 0.1297 score: 0.9388 time: 0.08s
Test loss: 0.3029 score: 0.8571 time: 0.16s
Epoch 316/1000, LR 0.000212
Train loss: 0.5765;  Loss pred: 0.5765; Loss self: 0.0000; time: 2.52s
Val loss: 0.1290 score: 0.9388 time: 0.27s
Test loss: 0.3025 score: 0.8571 time: 0.23s
Epoch 317/1000, LR 0.000211
Train loss: 0.5747;  Loss pred: 0.5747; Loss self: 0.0000; time: 0.41s
Val loss: 0.1283 score: 0.9388 time: 0.09s
Test loss: 0.3022 score: 0.8571 time: 0.07s
Epoch 318/1000, LR 0.000211
Train loss: 0.5760;  Loss pred: 0.5760; Loss self: 0.0000; time: 0.25s
Val loss: 0.1276 score: 0.9388 time: 0.08s
Test loss: 0.3018 score: 0.8571 time: 0.07s
Epoch 319/1000, LR 0.000210
Train loss: 0.5754;  Loss pred: 0.5754; Loss self: 0.0000; time: 0.25s
Val loss: 0.1269 score: 0.9388 time: 0.08s
Test loss: 0.3014 score: 0.8571 time: 0.07s
Epoch 320/1000, LR 0.000210
Train loss: 0.5758;  Loss pred: 0.5758; Loss self: 0.0000; time: 0.25s
Val loss: 0.1262 score: 0.9592 time: 0.08s
Test loss: 0.3011 score: 0.8571 time: 0.07s
Epoch 321/1000, LR 0.000210
Train loss: 0.5721;  Loss pred: 0.5721; Loss self: 0.0000; time: 0.24s
Val loss: 0.1255 score: 0.9592 time: 0.08s
Test loss: 0.3007 score: 0.8571 time: 0.07s
Epoch 322/1000, LR 0.000209
Train loss: 0.5737;  Loss pred: 0.5737; Loss self: 0.0000; time: 0.26s
Val loss: 0.1249 score: 0.9388 time: 0.08s
Test loss: 0.3004 score: 0.8571 time: 0.07s
Epoch 323/1000, LR 0.000209
Train loss: 0.5718;  Loss pred: 0.5718; Loss self: 0.0000; time: 0.24s
Val loss: 0.1243 score: 0.9388 time: 0.08s
Test loss: 0.3002 score: 0.8571 time: 0.07s
Epoch 324/1000, LR 0.000209
Train loss: 0.5719;  Loss pred: 0.5719; Loss self: 0.0000; time: 0.24s
Val loss: 0.1238 score: 0.9388 time: 0.08s
Test loss: 0.2999 score: 0.8571 time: 1.56s
Epoch 325/1000, LR 0.000208
Train loss: 0.5693;  Loss pred: 0.5693; Loss self: 0.0000; time: 2.69s
Val loss: 0.1232 score: 0.9388 time: 0.82s
Test loss: 0.2996 score: 0.8571 time: 0.95s
Epoch 326/1000, LR 0.000208
Train loss: 0.5716;  Loss pred: 0.5716; Loss self: 0.0000; time: 1.98s
Val loss: 0.1227 score: 0.9388 time: 0.52s
Test loss: 0.2994 score: 0.8571 time: 0.66s
Epoch 327/1000, LR 0.000208
Train loss: 0.5729;  Loss pred: 0.5729; Loss self: 0.0000; time: 1.15s
Val loss: 0.1221 score: 0.9388 time: 0.09s
Test loss: 0.2991 score: 0.8571 time: 0.09s
Epoch 328/1000, LR 0.000207
Train loss: 0.5711;  Loss pred: 0.5711; Loss self: 0.0000; time: 0.28s
Val loss: 0.1216 score: 0.9388 time: 0.08s
Test loss: 0.2989 score: 0.8571 time: 0.07s
Epoch 329/1000, LR 0.000207
Train loss: 0.5702;  Loss pred: 0.5702; Loss self: 0.0000; time: 0.25s
Val loss: 0.1211 score: 0.9388 time: 0.08s
Test loss: 0.2986 score: 0.8571 time: 0.07s
Epoch 330/1000, LR 0.000207
Train loss: 0.5682;  Loss pred: 0.5682; Loss self: 0.0000; time: 0.25s
Val loss: 0.1205 score: 0.9388 time: 0.08s
Test loss: 0.2983 score: 0.8571 time: 0.07s
Epoch 331/1000, LR 0.000206
Train loss: 0.5715;  Loss pred: 0.5715; Loss self: 0.0000; time: 0.23s
Val loss: 0.1199 score: 0.9388 time: 0.08s
Test loss: 0.2981 score: 0.8571 time: 0.07s
Epoch 332/1000, LR 0.000206
Train loss: 0.5664;  Loss pred: 0.5664; Loss self: 0.0000; time: 0.28s
Val loss: 0.1193 score: 0.9388 time: 0.08s
Test loss: 0.2978 score: 0.8571 time: 0.07s
Epoch 333/1000, LR 0.000205
Train loss: 0.5683;  Loss pred: 0.5683; Loss self: 0.0000; time: 0.23s
Val loss: 0.1187 score: 0.9388 time: 0.08s
Test loss: 0.2975 score: 0.8571 time: 0.07s
Epoch 334/1000, LR 0.000205
Train loss: 0.5665;  Loss pred: 0.5665; Loss self: 0.0000; time: 0.24s
Val loss: 0.1181 score: 0.9388 time: 0.08s
Test loss: 0.2972 score: 0.8571 time: 0.07s
Epoch 335/1000, LR 0.000205
Train loss: 0.5656;  Loss pred: 0.5656; Loss self: 0.0000; time: 0.23s
Val loss: 0.1176 score: 0.9388 time: 0.09s
Test loss: 0.2970 score: 0.8571 time: 1.09s
Epoch 336/1000, LR 0.000204
Train loss: 0.5645;  Loss pred: 0.5645; Loss self: 0.0000; time: 3.63s
Val loss: 0.1170 score: 0.9388 time: 0.19s
Test loss: 0.2967 score: 0.8571 time: 0.08s
Epoch 337/1000, LR 0.000204
Train loss: 0.5665;  Loss pred: 0.5665; Loss self: 0.0000; time: 0.24s
Val loss: 0.1165 score: 0.9388 time: 0.09s
Test loss: 0.2965 score: 0.8571 time: 0.07s
Epoch 338/1000, LR 0.000204
Train loss: 0.5648;  Loss pred: 0.5648; Loss self: 0.0000; time: 0.24s
Val loss: 0.1159 score: 0.9388 time: 0.08s
Test loss: 0.2963 score: 0.8571 time: 0.07s
Epoch 339/1000, LR 0.000203
Train loss: 0.5640;  Loss pred: 0.5640; Loss self: 0.0000; time: 0.24s
Val loss: 0.1154 score: 0.9388 time: 0.08s
Test loss: 0.2960 score: 0.8571 time: 0.07s
Epoch 340/1000, LR 0.000203
Train loss: 0.5656;  Loss pred: 0.5656; Loss self: 0.0000; time: 0.24s
Val loss: 0.1149 score: 0.9388 time: 0.08s
Test loss: 0.2958 score: 0.8571 time: 0.07s
Epoch 341/1000, LR 0.000203
Train loss: 0.5644;  Loss pred: 0.5644; Loss self: 0.0000; time: 0.23s
Val loss: 0.1144 score: 0.9388 time: 0.08s
Test loss: 0.2956 score: 0.8571 time: 0.07s
Epoch 342/1000, LR 0.000202
Train loss: 0.5640;  Loss pred: 0.5640; Loss self: 0.0000; time: 0.24s
Val loss: 0.1139 score: 0.9388 time: 0.08s
Test loss: 0.2954 score: 0.8571 time: 0.07s
Epoch 343/1000, LR 0.000202
Train loss: 0.5620;  Loss pred: 0.5620; Loss self: 0.0000; time: 0.25s
Val loss: 0.1135 score: 0.9388 time: 0.08s
Test loss: 0.2952 score: 0.8571 time: 0.07s
Epoch 344/1000, LR 0.000201
Train loss: 0.5635;  Loss pred: 0.5635; Loss self: 0.0000; time: 0.24s
Val loss: 0.1131 score: 0.9388 time: 0.08s
Test loss: 0.2950 score: 0.8571 time: 0.07s
Epoch 345/1000, LR 0.000201
Train loss: 0.5637;  Loss pred: 0.5637; Loss self: 0.0000; time: 0.24s
Val loss: 0.1127 score: 0.9388 time: 0.08s
Test loss: 0.2948 score: 0.8571 time: 0.07s
Epoch 346/1000, LR 0.000201
Train loss: 0.5600;  Loss pred: 0.5600; Loss self: 0.0000; time: 0.25s
Val loss: 0.1123 score: 0.9388 time: 0.08s
Test loss: 0.2947 score: 0.8571 time: 0.07s
Epoch 347/1000, LR 0.000200
Train loss: 0.5619;  Loss pred: 0.5619; Loss self: 0.0000; time: 0.25s
Val loss: 0.1119 score: 0.9388 time: 0.08s
Test loss: 0.2945 score: 0.8571 time: 0.07s
Epoch 348/1000, LR 0.000200
Train loss: 0.5631;  Loss pred: 0.5631; Loss self: 0.0000; time: 0.25s
Val loss: 0.1115 score: 0.9388 time: 0.08s
Test loss: 0.2943 score: 0.8571 time: 0.07s
Epoch 349/1000, LR 0.000200
Train loss: 0.5602;  Loss pred: 0.5602; Loss self: 0.0000; time: 0.25s
Val loss: 0.1110 score: 0.9388 time: 0.08s
Test loss: 0.2941 score: 0.8571 time: 0.07s
Epoch 350/1000, LR 0.000199
Train loss: 0.5603;  Loss pred: 0.5603; Loss self: 0.0000; time: 0.25s
Val loss: 0.1105 score: 0.9388 time: 0.08s
Test loss: 0.2939 score: 0.8571 time: 0.07s
Epoch 351/1000, LR 0.000199
Train loss: 0.5602;  Loss pred: 0.5602; Loss self: 0.0000; time: 0.25s
Val loss: 0.1100 score: 0.9388 time: 0.08s
Test loss: 0.2937 score: 0.8571 time: 0.07s
Epoch 352/1000, LR 0.000198
Train loss: 0.5602;  Loss pred: 0.5602; Loss self: 0.0000; time: 0.25s
Val loss: 0.1095 score: 0.9388 time: 0.08s
Test loss: 0.2935 score: 0.8571 time: 0.07s
Epoch 353/1000, LR 0.000198
Train loss: 0.5572;  Loss pred: 0.5572; Loss self: 0.0000; time: 0.25s
Val loss: 0.1090 score: 0.9388 time: 0.08s
Test loss: 0.2933 score: 0.8571 time: 0.07s
Epoch 354/1000, LR 0.000198
Train loss: 0.5600;  Loss pred: 0.5600; Loss self: 0.0000; time: 0.25s
Val loss: 0.1085 score: 0.9388 time: 0.08s
Test loss: 0.2931 score: 0.8571 time: 0.07s
Epoch 355/1000, LR 0.000197
Train loss: 0.5575;  Loss pred: 0.5575; Loss self: 0.0000; time: 0.25s
Val loss: 0.1081 score: 0.9388 time: 0.08s
Test loss: 0.2930 score: 0.8571 time: 1.40s
Epoch 356/1000, LR 0.000197
Train loss: 0.5572;  Loss pred: 0.5572; Loss self: 0.0000; time: 4.73s
Val loss: 0.1077 score: 0.9388 time: 0.41s
Test loss: 0.2928 score: 0.8571 time: 0.74s
Epoch 357/1000, LR 0.000196
Train loss: 0.5571;  Loss pred: 0.5571; Loss self: 0.0000; time: 0.36s
Val loss: 0.1073 score: 0.9388 time: 0.08s
Test loss: 0.2926 score: 0.8571 time: 0.08s
Epoch 358/1000, LR 0.000196
Train loss: 0.5582;  Loss pred: 0.5582; Loss self: 0.0000; time: 0.25s
Val loss: 0.1069 score: 0.9388 time: 0.08s
Test loss: 0.2924 score: 0.8571 time: 0.07s
Epoch 359/1000, LR 0.000196
Train loss: 0.5560;  Loss pred: 0.5560; Loss self: 0.0000; time: 0.25s
Val loss: 0.1065 score: 0.9388 time: 0.08s
Test loss: 0.2923 score: 0.8571 time: 0.07s
Epoch 360/1000, LR 0.000195
Train loss: 0.5579;  Loss pred: 0.5579; Loss self: 0.0000; time: 0.24s
Val loss: 0.1062 score: 0.9388 time: 0.08s
Test loss: 0.2921 score: 0.8571 time: 0.07s
Epoch 361/1000, LR 0.000195
Train loss: 0.5564;  Loss pred: 0.5564; Loss self: 0.0000; time: 0.24s
Val loss: 0.1059 score: 0.9388 time: 0.08s
Test loss: 0.2919 score: 0.8571 time: 0.07s
Epoch 362/1000, LR 0.000195
Train loss: 0.5556;  Loss pred: 0.5556; Loss self: 0.0000; time: 0.24s
Val loss: 0.1055 score: 0.9388 time: 0.08s
Test loss: 0.2918 score: 0.8571 time: 0.07s
Epoch 363/1000, LR 0.000194
Train loss: 0.5567;  Loss pred: 0.5567; Loss self: 0.0000; time: 0.24s
Val loss: 0.1052 score: 0.9388 time: 0.08s
Test loss: 0.2916 score: 0.8571 time: 0.07s
Epoch 364/1000, LR 0.000194
Train loss: 0.5546;  Loss pred: 0.5546; Loss self: 0.0000; time: 6.14s
Val loss: 0.1048 score: 0.9388 time: 2.03s
Test loss: 0.2915 score: 0.8571 time: 0.60s
Epoch 365/1000, LR 0.000193
Train loss: 0.5533;  Loss pred: 0.5533; Loss self: 0.0000; time: 0.34s
Val loss: 0.1043 score: 0.9388 time: 0.08s
Test loss: 0.2913 score: 0.8571 time: 0.07s
Epoch 366/1000, LR 0.000193
Train loss: 0.5524;  Loss pred: 0.5524; Loss self: 0.0000; time: 0.24s
Val loss: 0.1039 score: 0.9388 time: 0.08s
Test loss: 0.2911 score: 0.8571 time: 0.07s
Epoch 367/1000, LR 0.000193
Train loss: 0.5537;  Loss pred: 0.5537; Loss self: 0.0000; time: 0.24s
Val loss: 0.1035 score: 0.9388 time: 0.08s
Test loss: 0.2910 score: 0.8571 time: 0.07s
Epoch 368/1000, LR 0.000192
Train loss: 0.5534;  Loss pred: 0.5534; Loss self: 0.0000; time: 0.24s
Val loss: 0.1031 score: 0.9388 time: 0.08s
Test loss: 0.2909 score: 0.8571 time: 0.07s
Epoch 369/1000, LR 0.000192
Train loss: 0.5539;  Loss pred: 0.5539; Loss self: 0.0000; time: 0.24s
Val loss: 0.1028 score: 0.9388 time: 0.08s
Test loss: 0.2907 score: 0.8571 time: 0.07s
Epoch 370/1000, LR 0.000191
Train loss: 0.5516;  Loss pred: 0.5516; Loss self: 0.0000; time: 0.24s
Val loss: 0.1024 score: 0.9388 time: 0.08s
Test loss: 0.2906 score: 0.8571 time: 0.07s
Epoch 371/1000, LR 0.000191
Train loss: 0.5515;  Loss pred: 0.5515; Loss self: 0.0000; time: 0.24s
Val loss: 0.1021 score: 0.9388 time: 0.08s
Test loss: 0.2904 score: 0.8571 time: 0.07s
Epoch 372/1000, LR 0.000191
Train loss: 0.5510;  Loss pred: 0.5510; Loss self: 0.0000; time: 4.06s
Val loss: 0.1018 score: 0.9388 time: 0.32s
Test loss: 0.2903 score: 0.8571 time: 0.13s
Epoch 373/1000, LR 0.000190
Train loss: 0.5506;  Loss pred: 0.5506; Loss self: 0.0000; time: 0.52s
Val loss: 0.1014 score: 0.9388 time: 0.08s
Test loss: 0.2902 score: 0.8571 time: 0.08s
Epoch 374/1000, LR 0.000190
Train loss: 0.5538;  Loss pred: 0.5538; Loss self: 0.0000; time: 0.26s
Val loss: 0.1011 score: 0.9388 time: 0.08s
Test loss: 0.2901 score: 0.8571 time: 0.07s
Epoch 375/1000, LR 0.000190
Train loss: 0.5521;  Loss pred: 0.5521; Loss self: 0.0000; time: 0.25s
Val loss: 0.1008 score: 0.9388 time: 0.08s
Test loss: 0.2899 score: 0.8571 time: 0.07s
Epoch 376/1000, LR 0.000189
Train loss: 0.5500;  Loss pred: 0.5500; Loss self: 0.0000; time: 0.24s
Val loss: 0.1004 score: 0.9388 time: 0.08s
Test loss: 0.2898 score: 0.8571 time: 0.07s
Epoch 377/1000, LR 0.000189
Train loss: 0.5500;  Loss pred: 0.5500; Loss self: 0.0000; time: 0.24s
Val loss: 0.1001 score: 0.9388 time: 0.08s
Test loss: 0.2897 score: 0.8571 time: 0.07s
Epoch 378/1000, LR 0.000188
Train loss: 0.5489;  Loss pred: 0.5489; Loss self: 0.0000; time: 0.24s
Val loss: 0.0998 score: 0.9388 time: 0.08s
Test loss: 0.2895 score: 0.8571 time: 0.07s
Epoch 379/1000, LR 0.000188
Train loss: 0.5501;  Loss pred: 0.5501; Loss self: 0.0000; time: 0.24s
Val loss: 0.0995 score: 0.9388 time: 0.08s
Test loss: 0.2894 score: 0.8571 time: 0.07s
Epoch 380/1000, LR 0.000188
Train loss: 0.5497;  Loss pred: 0.5497; Loss self: 0.0000; time: 0.24s
Val loss: 0.0992 score: 0.9388 time: 0.08s
Test loss: 0.2893 score: 0.8571 time: 0.07s
Epoch 381/1000, LR 0.000187
Train loss: 0.5511;  Loss pred: 0.5511; Loss self: 0.0000; time: 0.23s
Val loss: 0.0989 score: 0.9388 time: 0.08s
Test loss: 0.2892 score: 0.8571 time: 0.07s
Epoch 382/1000, LR 0.000187
Train loss: 0.5488;  Loss pred: 0.5488; Loss self: 0.0000; time: 0.24s
Val loss: 0.0986 score: 0.9388 time: 0.08s
Test loss: 0.2891 score: 0.8571 time: 0.07s
Epoch 383/1000, LR 0.000186
Train loss: 0.5487;  Loss pred: 0.5487; Loss self: 0.0000; time: 0.24s
Val loss: 0.0983 score: 0.9388 time: 0.08s
Test loss: 0.2889 score: 0.8571 time: 0.07s
Epoch 384/1000, LR 0.000186
Train loss: 0.5458;  Loss pred: 0.5458; Loss self: 0.0000; time: 0.25s
Val loss: 0.0981 score: 0.9388 time: 0.08s
Test loss: 0.2888 score: 0.8571 time: 0.07s
Epoch 385/1000, LR 0.000186
Train loss: 0.5483;  Loss pred: 0.5483; Loss self: 0.0000; time: 0.25s
Val loss: 0.0978 score: 0.9388 time: 0.08s
Test loss: 0.2887 score: 0.8571 time: 0.07s
Epoch 386/1000, LR 0.000185
Train loss: 0.5468;  Loss pred: 0.5468; Loss self: 0.0000; time: 3.21s
Val loss: 0.0975 score: 0.9388 time: 0.31s
Test loss: 0.2886 score: 0.8571 time: 0.76s
Epoch 387/1000, LR 0.000185
Train loss: 0.5477;  Loss pred: 0.5477; Loss self: 0.0000; time: 0.79s
Val loss: 0.0973 score: 0.9388 time: 0.11s
Test loss: 0.2885 score: 0.8571 time: 0.21s
Epoch 388/1000, LR 0.000184
Train loss: 0.5475;  Loss pred: 0.5475; Loss self: 0.0000; time: 0.48s
Val loss: 0.0970 score: 0.9388 time: 0.09s
Test loss: 0.2884 score: 0.8571 time: 0.07s
Epoch 389/1000, LR 0.000184
Train loss: 0.5480;  Loss pred: 0.5480; Loss self: 0.0000; time: 0.25s
Val loss: 0.0966 score: 0.9388 time: 0.08s
Test loss: 0.2884 score: 0.8571 time: 0.07s
Epoch 390/1000, LR 0.000184
Train loss: 0.5457;  Loss pred: 0.5457; Loss self: 0.0000; time: 0.25s
Val loss: 0.0962 score: 0.9388 time: 0.08s
Test loss: 0.2883 score: 0.8571 time: 0.07s
Epoch 391/1000, LR 0.000183
Train loss: 0.5468;  Loss pred: 0.5468; Loss self: 0.0000; time: 0.24s
Val loss: 0.0958 score: 0.9388 time: 0.08s
Test loss: 0.2882 score: 0.8571 time: 0.07s
Epoch 392/1000, LR 0.000183
Train loss: 0.5457;  Loss pred: 0.5457; Loss self: 0.0000; time: 0.24s
Val loss: 0.0955 score: 0.9796 time: 0.08s
Test loss: 0.2881 score: 0.8571 time: 0.07s
Epoch 393/1000, LR 0.000182
Train loss: 0.5451;  Loss pred: 0.5451; Loss self: 0.0000; time: 0.24s
Val loss: 0.0951 score: 0.9796 time: 0.08s
Test loss: 0.2881 score: 0.8571 time: 0.07s
Epoch 394/1000, LR 0.000182
Train loss: 0.5470;  Loss pred: 0.5470; Loss self: 0.0000; time: 0.24s
Val loss: 0.0948 score: 0.9796 time: 0.08s
Test loss: 0.2880 score: 0.8571 time: 0.07s
Epoch 395/1000, LR 0.000182
Train loss: 0.5480;  Loss pred: 0.5480; Loss self: 0.0000; time: 0.24s
Val loss: 0.0944 score: 0.9796 time: 0.08s
Test loss: 0.2879 score: 0.8571 time: 0.07s
Epoch 396/1000, LR 0.000181
Train loss: 0.5446;  Loss pred: 0.5446; Loss self: 0.0000; time: 0.26s
Val loss: 0.0941 score: 0.9796 time: 0.72s
Test loss: 0.2878 score: 0.8571 time: 1.91s
Epoch 397/1000, LR 0.000181
Train loss: 0.5453;  Loss pred: 0.5453; Loss self: 0.0000; time: 3.42s
Val loss: 0.0939 score: 0.9796 time: 0.22s
Test loss: 0.2877 score: 0.8571 time: 0.40s
Epoch 398/1000, LR 0.000180
Train loss: 0.5457;  Loss pred: 0.5457; Loss self: 0.0000; time: 1.24s
Val loss: 0.0937 score: 0.9796 time: 0.08s
Test loss: 0.2876 score: 0.8571 time: 0.07s
Epoch 399/1000, LR 0.000180
Train loss: 0.5439;  Loss pred: 0.5439; Loss self: 0.0000; time: 0.25s
Val loss: 0.0935 score: 0.9796 time: 0.08s
Test loss: 0.2875 score: 0.8571 time: 0.07s
Epoch 400/1000, LR 0.000180
Train loss: 0.5449;  Loss pred: 0.5449; Loss self: 0.0000; time: 0.25s
Val loss: 0.0933 score: 0.9796 time: 0.08s
Test loss: 0.2873 score: 0.8571 time: 0.07s
Epoch 401/1000, LR 0.000179
Train loss: 0.5407;  Loss pred: 0.5407; Loss self: 0.0000; time: 0.25s
Val loss: 0.0933 score: 0.9592 time: 0.08s
Test loss: 0.2872 score: 0.8571 time: 0.07s
Epoch 402/1000, LR 0.000179
Train loss: 0.5425;  Loss pred: 0.5425; Loss self: 0.0000; time: 0.25s
Val loss: 0.0932 score: 0.9388 time: 0.08s
Test loss: 0.2871 score: 0.8571 time: 0.07s
Epoch 403/1000, LR 0.000178
Train loss: 0.5406;  Loss pred: 0.5406; Loss self: 0.0000; time: 0.24s
Val loss: 0.0931 score: 0.9388 time: 0.08s
Test loss: 0.2870 score: 0.8571 time: 0.07s
Epoch 404/1000, LR 0.000178
Train loss: 0.5417;  Loss pred: 0.5417; Loss self: 0.0000; time: 0.25s
Val loss: 0.0930 score: 0.9388 time: 1.62s
Test loss: 0.2869 score: 0.8571 time: 1.53s
Epoch 405/1000, LR 0.000178
Train loss: 0.5410;  Loss pred: 0.5410; Loss self: 0.0000; time: 4.79s
Val loss: 0.0929 score: 0.9388 time: 1.16s
Test loss: 0.2869 score: 0.8571 time: 1.45s
Epoch 406/1000, LR 0.000177
Train loss: 0.5429;  Loss pred: 0.5429; Loss self: 0.0000; time: 1.51s
Val loss: 0.0927 score: 0.9388 time: 0.08s
Test loss: 0.2868 score: 0.8571 time: 0.07s
Epoch 407/1000, LR 0.000177
Train loss: 0.5437;  Loss pred: 0.5437; Loss self: 0.0000; time: 0.25s
Val loss: 0.0924 score: 0.9388 time: 0.08s
Test loss: 0.2867 score: 0.8571 time: 0.07s
Epoch 408/1000, LR 0.000176
Train loss: 0.5418;  Loss pred: 0.5418; Loss self: 0.0000; time: 0.24s
Val loss: 0.0921 score: 0.9388 time: 0.08s
Test loss: 0.2866 score: 0.8571 time: 0.07s
Epoch 409/1000, LR 0.000176
Train loss: 0.5404;  Loss pred: 0.5404; Loss self: 0.0000; time: 0.24s
Val loss: 0.0919 score: 0.9388 time: 0.08s
Test loss: 0.2865 score: 0.8571 time: 0.07s
Epoch 410/1000, LR 0.000175
Train loss: 0.5425;  Loss pred: 0.5425; Loss self: 0.0000; time: 0.24s
Val loss: 0.0916 score: 0.9388 time: 0.08s
Test loss: 0.2865 score: 0.8571 time: 0.07s
Epoch 411/1000, LR 0.000175
Train loss: 0.5405;  Loss pred: 0.5405; Loss self: 0.0000; time: 0.24s
Val loss: 0.0913 score: 0.9388 time: 0.08s
Test loss: 0.2864 score: 0.8571 time: 0.07s
Epoch 412/1000, LR 0.000175
Train loss: 0.5396;  Loss pred: 0.5396; Loss self: 0.0000; time: 0.25s
Val loss: 0.0910 score: 0.9388 time: 0.09s
Test loss: 0.2863 score: 0.8571 time: 0.07s
Epoch 413/1000, LR 0.000174
Train loss: 0.5399;  Loss pred: 0.5399; Loss self: 0.0000; time: 0.26s
Val loss: 0.0908 score: 0.9388 time: 0.08s
Test loss: 0.2863 score: 0.8571 time: 0.07s
Epoch 414/1000, LR 0.000174
Train loss: 0.5398;  Loss pred: 0.5398; Loss self: 0.0000; time: 0.25s
Val loss: 0.0905 score: 0.9592 time: 0.08s
Test loss: 0.2862 score: 0.8571 time: 0.07s
Epoch 415/1000, LR 0.000173
Train loss: 0.5386;  Loss pred: 0.5386; Loss self: 0.0000; time: 0.26s
Val loss: 0.0902 score: 0.9592 time: 0.08s
Test loss: 0.2862 score: 0.8571 time: 0.07s
Epoch 416/1000, LR 0.000173
Train loss: 0.5396;  Loss pred: 0.5396; Loss self: 0.0000; time: 2.27s
Val loss: 0.0899 score: 0.9592 time: 2.33s
Test loss: 0.2861 score: 0.8571 time: 2.56s
Epoch 417/1000, LR 0.000173
Train loss: 0.5393;  Loss pred: 0.5393; Loss self: 0.0000; time: 5.07s
Val loss: 0.0897 score: 0.9592 time: 0.08s
Test loss: 0.2861 score: 0.8571 time: 0.07s
Epoch 418/1000, LR 0.000172
Train loss: 0.5394;  Loss pred: 0.5394; Loss self: 0.0000; time: 0.23s
Val loss: 0.0894 score: 0.9592 time: 0.08s
Test loss: 0.2860 score: 0.8571 time: 0.07s
Epoch 419/1000, LR 0.000172
Train loss: 0.5386;  Loss pred: 0.5386; Loss self: 0.0000; time: 0.23s
Val loss: 0.0892 score: 0.9592 time: 0.08s
Test loss: 0.2860 score: 0.8571 time: 0.07s
Epoch 420/1000, LR 0.000171
Train loss: 0.5395;  Loss pred: 0.5395; Loss self: 0.0000; time: 0.23s
Val loss: 0.0890 score: 0.9592 time: 0.08s
Test loss: 0.2859 score: 0.8571 time: 0.08s
Epoch 421/1000, LR 0.000171
Train loss: 0.5391;  Loss pred: 0.5391; Loss self: 0.0000; time: 0.24s
Val loss: 0.0888 score: 0.9592 time: 0.08s
Test loss: 0.2859 score: 0.8571 time: 0.08s
Epoch 422/1000, LR 0.000171
Train loss: 0.5417;  Loss pred: 0.5417; Loss self: 0.0000; time: 0.24s
Val loss: 0.0886 score: 0.9592 time: 0.09s
Test loss: 0.2858 score: 0.8571 time: 0.07s
Epoch 423/1000, LR 0.000170
Train loss: 0.5368;  Loss pred: 0.5368; Loss self: 0.0000; time: 0.24s
Val loss: 0.0884 score: 0.9592 time: 0.08s
Test loss: 0.2858 score: 0.8571 time: 0.07s
Epoch 424/1000, LR 0.000170
Train loss: 0.5366;  Loss pred: 0.5366; Loss self: 0.0000; time: 0.25s
Val loss: 0.0882 score: 0.9592 time: 0.08s
Test loss: 0.2857 score: 0.8571 time: 0.07s
Epoch 425/1000, LR 0.000169
Train loss: 0.5386;  Loss pred: 0.5386; Loss self: 0.0000; time: 0.24s
Val loss: 0.0880 score: 0.9592 time: 0.08s
Test loss: 0.2857 score: 0.8571 time: 2.17s
Epoch 426/1000, LR 0.000169
Train loss: 0.5349;  Loss pred: 0.5349; Loss self: 0.0000; time: 2.91s
Val loss: 0.0879 score: 0.9592 time: 0.23s
Test loss: 0.2856 score: 0.8571 time: 0.14s
Epoch 427/1000, LR 0.000168
Train loss: 0.5377;  Loss pred: 0.5377; Loss self: 0.0000; time: 0.93s
Val loss: 0.0879 score: 0.9592 time: 0.08s
Test loss: 0.2856 score: 0.8571 time: 0.07s
Epoch 428/1000, LR 0.000168
Train loss: 0.5374;  Loss pred: 0.5374; Loss self: 0.0000; time: 0.26s
Val loss: 0.0878 score: 0.9592 time: 0.08s
Test loss: 0.2855 score: 0.8571 time: 0.07s
Epoch 429/1000, LR 0.000168
Train loss: 0.5371;  Loss pred: 0.5371; Loss self: 0.0000; time: 0.26s
Val loss: 0.0876 score: 0.9592 time: 0.08s
Test loss: 0.2855 score: 0.8571 time: 0.07s
Epoch 430/1000, LR 0.000167
Train loss: 0.5367;  Loss pred: 0.5367; Loss self: 0.0000; time: 0.25s
Val loss: 0.0875 score: 0.9592 time: 0.08s
Test loss: 0.2855 score: 0.8571 time: 0.07s
Epoch 431/1000, LR 0.000167
Train loss: 0.5363;  Loss pred: 0.5363; Loss self: 0.0000; time: 0.24s
Val loss: 0.0873 score: 0.9592 time: 0.08s
Test loss: 0.2854 score: 0.8571 time: 0.07s
Epoch 432/1000, LR 0.000166
Train loss: 0.5357;  Loss pred: 0.5357; Loss self: 0.0000; time: 0.24s
Val loss: 0.0871 score: 0.9592 time: 0.08s
Test loss: 0.2854 score: 0.8571 time: 0.07s
Epoch 433/1000, LR 0.000166
Train loss: 0.5365;  Loss pred: 0.5365; Loss self: 0.0000; time: 0.24s
Val loss: 0.0869 score: 0.9592 time: 0.08s
Test loss: 0.2853 score: 0.8571 time: 0.07s
Epoch 434/1000, LR 0.000166
Train loss: 0.5358;  Loss pred: 0.5358; Loss self: 0.0000; time: 0.25s
Val loss: 0.0867 score: 0.9592 time: 0.08s
Test loss: 0.2853 score: 0.8571 time: 0.07s
Epoch 435/1000, LR 0.000165
Train loss: 0.5356;  Loss pred: 0.5356; Loss self: 0.0000; time: 0.25s
Val loss: 0.0864 score: 0.9592 time: 0.08s
Test loss: 0.2853 score: 0.8571 time: 0.07s
Epoch 436/1000, LR 0.000165
Train loss: 0.5362;  Loss pred: 0.5362; Loss self: 0.0000; time: 0.25s
Val loss: 0.0861 score: 0.9592 time: 0.08s
Test loss: 0.2852 score: 0.8571 time: 0.07s
Epoch 437/1000, LR 0.000164
Train loss: 0.5339;  Loss pred: 0.5339; Loss self: 0.0000; time: 0.25s
Val loss: 0.0857 score: 0.9592 time: 0.08s
Test loss: 0.2852 score: 0.8571 time: 0.07s
Epoch 438/1000, LR 0.000164
Train loss: 0.5338;  Loss pred: 0.5338; Loss self: 0.0000; time: 0.24s
Val loss: 0.0854 score: 0.9592 time: 0.08s
Test loss: 0.2852 score: 0.8571 time: 0.07s
Epoch 439/1000, LR 0.000163
Train loss: 0.5352;  Loss pred: 0.5352; Loss self: 0.0000; time: 0.24s
Val loss: 0.0852 score: 0.9592 time: 0.08s
Test loss: 0.2852 score: 0.8571 time: 0.07s
Epoch 440/1000, LR 0.000163
Train loss: 0.5336;  Loss pred: 0.5336; Loss self: 0.0000; time: 0.24s
Val loss: 0.0850 score: 0.9592 time: 0.08s
Test loss: 0.2852 score: 0.8571 time: 0.07s
Epoch 441/1000, LR 0.000163
Train loss: 0.5342;  Loss pred: 0.5342; Loss self: 0.0000; time: 0.24s
Val loss: 0.0847 score: 0.9592 time: 0.08s
Test loss: 0.2851 score: 0.8571 time: 0.07s
Epoch 442/1000, LR 0.000162
Train loss: 0.5350;  Loss pred: 0.5350; Loss self: 0.0000; time: 0.24s
Val loss: 0.0846 score: 0.9592 time: 0.08s
Test loss: 0.2851 score: 0.8571 time: 0.07s
Epoch 443/1000, LR 0.000162
Train loss: 0.5349;  Loss pred: 0.5349; Loss self: 0.0000; time: 0.24s
Val loss: 0.0844 score: 0.9592 time: 0.08s
Test loss: 0.2851 score: 0.8571 time: 0.07s
Epoch 444/1000, LR 0.000161
Train loss: 0.5347;  Loss pred: 0.5347; Loss self: 0.0000; time: 0.24s
Val loss: 0.0843 score: 0.9592 time: 0.08s
Test loss: 0.2850 score: 0.8571 time: 0.07s
Epoch 445/1000, LR 0.000161
Train loss: 0.5341;  Loss pred: 0.5341; Loss self: 0.0000; time: 0.24s
Val loss: 0.0842 score: 0.9592 time: 0.08s
Test loss: 0.2850 score: 0.8571 time: 0.07s
Epoch 446/1000, LR 0.000161
Train loss: 0.5320;  Loss pred: 0.5320; Loss self: 0.0000; time: 0.24s
Val loss: 0.0841 score: 0.9592 time: 0.08s
Test loss: 0.2849 score: 0.8571 time: 0.07s
Epoch 447/1000, LR 0.000160
Train loss: 0.5324;  Loss pred: 0.5324; Loss self: 0.0000; time: 0.24s
Val loss: 0.0839 score: 0.9592 time: 0.08s
Test loss: 0.2849 score: 0.8571 time: 0.07s
Epoch 448/1000, LR 0.000160
Train loss: 0.5332;  Loss pred: 0.5332; Loss self: 0.0000; time: 0.24s
Val loss: 0.0839 score: 0.9592 time: 0.08s
Test loss: 0.2849 score: 0.8571 time: 0.07s
Epoch 449/1000, LR 0.000159
Train loss: 0.5324;  Loss pred: 0.5324; Loss self: 0.0000; time: 0.24s
Val loss: 0.0839 score: 0.9592 time: 0.08s
Test loss: 0.2849 score: 0.8571 time: 0.07s
Epoch 450/1000, LR 0.000159
Train loss: 0.5333;  Loss pred: 0.5333; Loss self: 0.0000; time: 0.24s
Val loss: 0.0838 score: 0.9592 time: 0.08s
Test loss: 0.2848 score: 0.8571 time: 0.07s
Epoch 451/1000, LR 0.000158
Train loss: 0.5322;  Loss pred: 0.5322; Loss self: 0.0000; time: 0.24s
Val loss: 0.0838 score: 0.9592 time: 0.08s
Test loss: 0.2848 score: 0.8571 time: 0.07s
Epoch 452/1000, LR 0.000158
Train loss: 0.5326;  Loss pred: 0.5326; Loss self: 0.0000; time: 0.24s
Val loss: 0.0837 score: 0.9592 time: 0.08s
Test loss: 0.2848 score: 0.8571 time: 0.07s
Epoch 453/1000, LR 0.000158
Train loss: 0.5332;  Loss pred: 0.5332; Loss self: 0.0000; time: 0.24s
Val loss: 0.0835 score: 0.9592 time: 0.08s
Test loss: 0.2848 score: 0.8571 time: 0.07s
Epoch 454/1000, LR 0.000157
Train loss: 0.5311;  Loss pred: 0.5311; Loss self: 0.0000; time: 0.24s
Val loss: 0.0832 score: 0.9592 time: 0.08s
Test loss: 0.2848 score: 0.8571 time: 0.07s
Epoch 455/1000, LR 0.000157
Train loss: 0.5312;  Loss pred: 0.5312; Loss self: 0.0000; time: 0.24s
Val loss: 0.0830 score: 0.9592 time: 0.08s
Test loss: 0.2847 score: 0.8571 time: 0.07s
Epoch 456/1000, LR 0.000156
Train loss: 0.5318;  Loss pred: 0.5318; Loss self: 0.0000; time: 0.24s
Val loss: 0.0829 score: 0.9592 time: 0.08s
Test loss: 0.2847 score: 0.8571 time: 0.07s
Epoch 457/1000, LR 0.000156
Train loss: 0.5314;  Loss pred: 0.5314; Loss self: 0.0000; time: 0.24s
Val loss: 0.0827 score: 0.9592 time: 0.08s
Test loss: 0.2847 score: 0.8571 time: 0.07s
Epoch 458/1000, LR 0.000155
Train loss: 0.5302;  Loss pred: 0.5302; Loss self: 0.0000; time: 0.24s
Val loss: 0.0825 score: 0.9592 time: 0.08s
Test loss: 0.2847 score: 0.8571 time: 0.07s
Epoch 459/1000, LR 0.000155
Train loss: 0.5327;  Loss pred: 0.5327; Loss self: 0.0000; time: 0.24s
Val loss: 0.0824 score: 0.9592 time: 0.08s
Test loss: 0.2847 score: 0.8571 time: 0.07s
Epoch 460/1000, LR 0.000155
Train loss: 0.5287;  Loss pred: 0.5287; Loss self: 0.0000; time: 0.24s
Val loss: 0.0822 score: 0.9592 time: 0.08s
Test loss: 0.2846 score: 0.8571 time: 0.07s
Epoch 461/1000, LR 0.000154
Train loss: 0.5299;  Loss pred: 0.5299; Loss self: 0.0000; time: 0.24s
Val loss: 0.0821 score: 0.9592 time: 0.08s
Test loss: 0.2846 score: 0.8571 time: 0.07s
Epoch 462/1000, LR 0.000154
Train loss: 0.5302;  Loss pred: 0.5302; Loss self: 0.0000; time: 0.24s
Val loss: 0.0820 score: 0.9592 time: 0.08s
Test loss: 0.2846 score: 0.8571 time: 0.07s
Epoch 463/1000, LR 0.000153
Train loss: 0.5292;  Loss pred: 0.5292; Loss self: 0.0000; time: 0.24s
Val loss: 0.0819 score: 0.9592 time: 0.08s
Test loss: 0.2846 score: 0.8571 time: 0.07s
Epoch 464/1000, LR 0.000153
Train loss: 0.5324;  Loss pred: 0.5324; Loss self: 0.0000; time: 0.24s
Val loss: 0.0817 score: 0.9592 time: 0.08s
Test loss: 0.2846 score: 0.8571 time: 0.07s
Epoch 465/1000, LR 0.000153
Train loss: 0.5285;  Loss pred: 0.5285; Loss self: 0.0000; time: 0.24s
Val loss: 0.0816 score: 0.9592 time: 0.08s
Test loss: 0.2846 score: 0.8571 time: 0.07s
Epoch 466/1000, LR 0.000152
Train loss: 0.5302;  Loss pred: 0.5302; Loss self: 0.0000; time: 0.24s
Val loss: 0.0815 score: 0.9592 time: 0.08s
Test loss: 0.2846 score: 0.8571 time: 0.07s
Epoch 467/1000, LR 0.000152
Train loss: 0.5310;  Loss pred: 0.5310; Loss self: 0.0000; time: 0.24s
Val loss: 0.0814 score: 0.9592 time: 0.08s
Test loss: 0.2846 score: 0.8571 time: 0.07s
Epoch 468/1000, LR 0.000151
Train loss: 0.5289;  Loss pred: 0.5289; Loss self: 0.0000; time: 0.24s
Val loss: 0.0812 score: 0.9592 time: 0.08s
Test loss: 0.2846 score: 0.8571 time: 0.07s
Epoch 469/1000, LR 0.000151
Train loss: 0.5293;  Loss pred: 0.5293; Loss self: 0.0000; time: 0.24s
Val loss: 0.0810 score: 0.9592 time: 0.08s
Test loss: 0.2845 score: 0.8571 time: 0.07s
Epoch 470/1000, LR 0.000150
Train loss: 0.5267;  Loss pred: 0.5267; Loss self: 0.0000; time: 0.24s
Val loss: 0.0809 score: 0.9592 time: 0.08s
Test loss: 0.2846 score: 0.8571 time: 0.07s
Epoch 471/1000, LR 0.000150
Train loss: 0.5281;  Loss pred: 0.5281; Loss self: 0.0000; time: 0.26s
Val loss: 0.0807 score: 0.9592 time: 0.08s
Test loss: 0.2845 score: 0.8571 time: 0.07s
Epoch 472/1000, LR 0.000150
Train loss: 0.5280;  Loss pred: 0.5280; Loss self: 0.0000; time: 0.24s
Val loss: 0.0806 score: 0.9592 time: 0.08s
Test loss: 0.2845 score: 0.8571 time: 0.07s
Epoch 473/1000, LR 0.000149
Train loss: 0.5271;  Loss pred: 0.5271; Loss self: 0.0000; time: 0.26s
Val loss: 0.0804 score: 0.9592 time: 0.08s
Test loss: 0.2846 score: 0.8571 time: 0.07s
Epoch 474/1000, LR 0.000149
Train loss: 0.5266;  Loss pred: 0.5266; Loss self: 0.0000; time: 0.24s
Val loss: 0.0803 score: 0.9592 time: 0.08s
Test loss: 0.2846 score: 0.8571 time: 0.07s
Epoch 475/1000, LR 0.000148
Train loss: 0.5279;  Loss pred: 0.5279; Loss self: 0.0000; time: 0.24s
Val loss: 0.0801 score: 0.9592 time: 0.08s
Test loss: 0.2846 score: 0.8571 time: 0.07s
Epoch 476/1000, LR 0.000148
Train loss: 0.5283;  Loss pred: 0.5283; Loss self: 0.0000; time: 0.25s
Val loss: 0.0800 score: 0.9592 time: 0.08s
Test loss: 0.2846 score: 0.8571 time: 0.07s
Epoch 477/1000, LR 0.000147
Train loss: 0.5281;  Loss pred: 0.5281; Loss self: 0.0000; time: 0.24s
Val loss: 0.0798 score: 0.9592 time: 0.08s
Test loss: 0.2845 score: 0.8571 time: 0.07s
Epoch 478/1000, LR 0.000147
Train loss: 0.5263;  Loss pred: 0.5263; Loss self: 0.0000; time: 0.24s
Val loss: 0.0797 score: 0.9592 time: 0.08s
Test loss: 0.2845 score: 0.8571 time: 0.07s
Epoch 479/1000, LR 0.000147
Train loss: 0.5258;  Loss pred: 0.5258; Loss self: 0.0000; time: 0.24s
Val loss: 0.0796 score: 0.9592 time: 1.08s
Test loss: 0.2845 score: 0.8571 time: 1.28s
Epoch 480/1000, LR 0.000146
Train loss: 0.5274;  Loss pred: 0.5274; Loss self: 0.0000; time: 1.62s
Val loss: 0.0795 score: 0.9592 time: 0.50s
Test loss: 0.2845 score: 0.8571 time: 0.72s
Epoch 481/1000, LR 0.000146
Train loss: 0.5277;  Loss pred: 0.5277; Loss self: 0.0000; time: 2.44s
Val loss: 0.0793 score: 0.9592 time: 0.09s
Test loss: 0.2845 score: 0.8571 time: 0.07s
Epoch 482/1000, LR 0.000145
Train loss: 0.5263;  Loss pred: 0.5263; Loss self: 0.0000; time: 0.24s
Val loss: 0.0792 score: 0.9592 time: 0.08s
Test loss: 0.2845 score: 0.8571 time: 0.07s
Epoch 483/1000, LR 0.000145
Train loss: 0.5268;  Loss pred: 0.5268; Loss self: 0.0000; time: 0.24s
Val loss: 0.0789 score: 0.9592 time: 0.08s
Test loss: 0.2845 score: 0.8571 time: 0.07s
Epoch 484/1000, LR 0.000144
Train loss: 0.5248;  Loss pred: 0.5248; Loss self: 0.0000; time: 0.23s
Val loss: 0.0787 score: 0.9592 time: 0.08s
Test loss: 0.2845 score: 0.8571 time: 0.07s
Epoch 485/1000, LR 0.000144
Train loss: 0.5276;  Loss pred: 0.5276; Loss self: 0.0000; time: 0.24s
Val loss: 0.0785 score: 0.9592 time: 0.08s
Test loss: 0.2845 score: 0.8571 time: 0.07s
Epoch 486/1000, LR 0.000144
Train loss: 0.5265;  Loss pred: 0.5265; Loss self: 0.0000; time: 0.24s
Val loss: 0.0783 score: 0.9592 time: 0.08s
Test loss: 0.2845 score: 0.8571 time: 0.07s
Epoch 487/1000, LR 0.000143
Train loss: 0.5266;  Loss pred: 0.5266; Loss self: 0.0000; time: 0.24s
Val loss: 0.0782 score: 0.9592 time: 0.08s
Test loss: 0.2845 score: 0.8571 time: 0.07s
Epoch 488/1000, LR 0.000143
Train loss: 0.5238;  Loss pred: 0.5238; Loss self: 0.0000; time: 0.24s
Val loss: 0.0780 score: 0.9592 time: 0.08s
Test loss: 0.2845 score: 0.8571 time: 0.07s
Epoch 489/1000, LR 0.000142
Train loss: 0.5252;  Loss pred: 0.5252; Loss self: 0.0000; time: 0.24s
Val loss: 0.0780 score: 0.9592 time: 0.08s
Test loss: 0.2844 score: 0.8571 time: 0.07s
Epoch 490/1000, LR 0.000142
Train loss: 0.5271;  Loss pred: 0.5271; Loss self: 0.0000; time: 6.05s
Val loss: 0.0779 score: 0.9592 time: 1.11s
Test loss: 0.2844 score: 0.8571 time: 1.33s
Epoch 491/1000, LR 0.000141
Train loss: 0.5257;  Loss pred: 0.5257; Loss self: 0.0000; time: 3.84s
Val loss: 0.0778 score: 0.9592 time: 0.10s
Test loss: 0.2844 score: 0.8571 time: 0.13s
Epoch 492/1000, LR 0.000141
Train loss: 0.5253;  Loss pred: 0.5253; Loss self: 0.0000; time: 0.25s
Val loss: 0.0778 score: 0.9592 time: 0.08s
Test loss: 0.2844 score: 0.8571 time: 0.07s
Epoch 493/1000, LR 0.000141
Train loss: 0.5260;  Loss pred: 0.5260; Loss self: 0.0000; time: 0.25s
Val loss: 0.0778 score: 0.9592 time: 0.08s
Test loss: 0.2844 score: 0.8571 time: 0.07s
Epoch 494/1000, LR 0.000140
Train loss: 0.5235;  Loss pred: 0.5235; Loss self: 0.0000; time: 0.27s
Val loss: 0.0778 score: 0.9592 time: 0.08s
Test loss: 0.2844 score: 0.8571 time: 0.07s
Epoch 495/1000, LR 0.000140
Train loss: 0.5261;  Loss pred: 0.5261; Loss self: 0.0000; time: 0.25s
Val loss: 0.0777 score: 0.9592 time: 0.08s
Test loss: 0.2844 score: 0.8571 time: 0.07s
Epoch 496/1000, LR 0.000139
Train loss: 0.5248;  Loss pred: 0.5248; Loss self: 0.0000; time: 0.25s
Val loss: 0.0777 score: 0.9592 time: 0.08s
Test loss: 0.2844 score: 0.8571 time: 0.07s
Epoch 497/1000, LR 0.000139
Train loss: 0.5251;  Loss pred: 0.5251; Loss self: 0.0000; time: 0.26s
Val loss: 0.0777 score: 0.9592 time: 2.34s
Test loss: 0.2844 score: 0.8571 time: 0.56s
Epoch 498/1000, LR 0.000138
Train loss: 0.5247;  Loss pred: 0.5247; Loss self: 0.0000; time: 1.12s
Val loss: 0.0777 score: 0.9592 time: 0.18s
Test loss: 0.2844 score: 0.8571 time: 0.22s
Epoch 499/1000, LR 0.000138
Train loss: 0.5256;  Loss pred: 0.5256; Loss self: 0.0000; time: 0.96s
Val loss: 0.0776 score: 0.9592 time: 0.10s
Test loss: 0.2844 score: 0.8571 time: 0.08s
Epoch 500/1000, LR 0.000138
Train loss: 0.5220;  Loss pred: 0.5220; Loss self: 0.0000; time: 0.24s
Val loss: 0.0775 score: 0.9592 time: 0.08s
Test loss: 0.2844 score: 0.8571 time: 0.07s
Epoch 501/1000, LR 0.000137
Train loss: 0.5240;  Loss pred: 0.5240; Loss self: 0.0000; time: 0.23s
Val loss: 0.0773 score: 0.9592 time: 0.08s
Test loss: 0.2844 score: 0.8571 time: 0.07s
Epoch 502/1000, LR 0.000137
Train loss: 0.5261;  Loss pred: 0.5261; Loss self: 0.0000; time: 0.23s
Val loss: 0.0772 score: 0.9592 time: 0.08s
Test loss: 0.2844 score: 0.8571 time: 0.07s
Epoch 503/1000, LR 0.000136
Train loss: 0.5238;  Loss pred: 0.5238; Loss self: 0.0000; time: 0.23s
Val loss: 0.0770 score: 0.9592 time: 0.08s
Test loss: 0.2844 score: 0.8571 time: 0.07s
Epoch 504/1000, LR 0.000136
Train loss: 0.5242;  Loss pred: 0.5242; Loss self: 0.0000; time: 0.24s
Val loss: 0.0769 score: 0.9592 time: 0.08s
Test loss: 0.2844 score: 0.8571 time: 0.07s
Epoch 505/1000, LR 0.000135
Train loss: 0.5243;  Loss pred: 0.5243; Loss self: 0.0000; time: 0.24s
Val loss: 0.0767 score: 0.9592 time: 0.08s
Test loss: 0.2844 score: 0.8571 time: 0.07s
Epoch 506/1000, LR 0.000135
Train loss: 0.5238;  Loss pred: 0.5238; Loss self: 0.0000; time: 0.24s
Val loss: 0.0766 score: 0.9592 time: 0.08s
Test loss: 0.2844 score: 0.8571 time: 0.07s
Epoch 507/1000, LR 0.000135
Train loss: 0.5259;  Loss pred: 0.5259; Loss self: 0.0000; time: 0.24s
Val loss: 0.0764 score: 0.9592 time: 0.08s
Test loss: 0.2843 score: 0.8571 time: 0.07s
Epoch 508/1000, LR 0.000134
Train loss: 0.5241;  Loss pred: 0.5241; Loss self: 0.0000; time: 0.24s
Val loss: 0.0763 score: 0.9592 time: 0.08s
Test loss: 0.2843 score: 0.8571 time: 0.07s
Epoch 509/1000, LR 0.000134
Train loss: 0.5220;  Loss pred: 0.5220; Loss self: 0.0000; time: 0.24s
Val loss: 0.0762 score: 0.9592 time: 0.08s
Test loss: 0.2843 score: 0.8571 time: 0.07s
Epoch 510/1000, LR 0.000133
Train loss: 0.5226;  Loss pred: 0.5226; Loss self: 0.0000; time: 0.25s
Val loss: 0.0761 score: 0.9592 time: 0.08s
Test loss: 0.2843 score: 0.8571 time: 0.07s
Epoch 511/1000, LR 0.000133
Train loss: 0.5221;  Loss pred: 0.5221; Loss self: 0.0000; time: 0.25s
Val loss: 0.0760 score: 0.9592 time: 0.08s
Test loss: 0.2843 score: 0.8571 time: 0.07s
Epoch 512/1000, LR 0.000132
Train loss: 0.5223;  Loss pred: 0.5223; Loss self: 0.0000; time: 0.25s
Val loss: 0.0760 score: 0.9592 time: 0.08s
Test loss: 0.2843 score: 0.8571 time: 0.07s
Epoch 513/1000, LR 0.000132
Train loss: 0.5230;  Loss pred: 0.5230; Loss self: 0.0000; time: 0.25s
Val loss: 0.0759 score: 0.9592 time: 0.08s
Test loss: 0.2843 score: 0.8571 time: 0.07s
Epoch 514/1000, LR 0.000132
Train loss: 0.5229;  Loss pred: 0.5229; Loss self: 0.0000; time: 0.26s
Val loss: 0.0759 score: 0.9592 time: 0.08s
Test loss: 0.2843 score: 0.8571 time: 0.08s
Epoch 515/1000, LR 0.000131
Train loss: 0.5226;  Loss pred: 0.5226; Loss self: 0.0000; time: 0.26s
Val loss: 0.0759 score: 0.9592 time: 0.08s
Test loss: 0.2844 score: 0.8571 time: 0.07s
     INFO: Early stopping counter 1 of 2
Epoch 516/1000, LR 0.000131
Train loss: 0.5231;  Loss pred: 0.5231; Loss self: 0.0000; time: 0.26s
Val loss: 0.0759 score: 0.9592 time: 0.08s
Test loss: 0.2844 score: 0.8571 time: 0.07s
Epoch 517/1000, LR 0.000130
Train loss: 0.5213;  Loss pred: 0.5213; Loss self: 0.0000; time: 0.25s
Val loss: 0.0759 score: 0.9592 time: 0.08s
Test loss: 0.2844 score: 0.8571 time: 0.96s
Epoch 518/1000, LR 0.000130
Train loss: 0.5216;  Loss pred: 0.5216; Loss self: 0.0000; time: 4.67s
Val loss: 0.0759 score: 0.9592 time: 0.09s
Test loss: 0.2845 score: 0.8571 time: 0.07s
Epoch 519/1000, LR 0.000129
Train loss: 0.5251;  Loss pred: 0.5251; Loss self: 0.0000; time: 0.25s
Val loss: 0.0758 score: 0.9592 time: 0.08s
Test loss: 0.2844 score: 0.8571 time: 0.07s
Epoch 520/1000, LR 0.000129
Train loss: 0.5230;  Loss pred: 0.5230; Loss self: 0.0000; time: 0.24s
Val loss: 0.0756 score: 0.9592 time: 0.08s
Test loss: 0.2844 score: 0.8571 time: 0.07s
Epoch 521/1000, LR 0.000129
Train loss: 0.5212;  Loss pred: 0.5212; Loss self: 0.0000; time: 0.24s
Val loss: 0.0755 score: 0.9592 time: 0.08s
Test loss: 0.2844 score: 0.8571 time: 0.07s
Epoch 522/1000, LR 0.000128
Train loss: 0.5206;  Loss pred: 0.5206; Loss self: 0.0000; time: 0.25s
Val loss: 0.0755 score: 0.9592 time: 0.08s
Test loss: 0.2844 score: 0.8571 time: 0.07s
Epoch 523/1000, LR 0.000128
Train loss: 0.5198;  Loss pred: 0.5198; Loss self: 0.0000; time: 0.25s
Val loss: 0.0754 score: 0.9592 time: 0.08s
Test loss: 0.2844 score: 0.8571 time: 0.07s
Epoch 524/1000, LR 0.000127
Train loss: 0.5213;  Loss pred: 0.5213; Loss self: 0.0000; time: 0.25s
Val loss: 0.0753 score: 0.9592 time: 0.08s
Test loss: 0.2844 score: 0.8571 time: 0.07s
Epoch 525/1000, LR 0.000127
Train loss: 0.5222;  Loss pred: 0.5222; Loss self: 0.0000; time: 0.25s
Val loss: 0.0752 score: 0.9592 time: 0.08s
Test loss: 0.2844 score: 0.8571 time: 0.07s
Epoch 526/1000, LR 0.000126
Train loss: 0.5210;  Loss pred: 0.5210; Loss self: 0.0000; time: 0.25s
Val loss: 0.0752 score: 0.9592 time: 0.08s
Test loss: 0.2845 score: 0.8571 time: 0.07s
Epoch 527/1000, LR 0.000126
Train loss: 0.5209;  Loss pred: 0.5209; Loss self: 0.0000; time: 4.97s
Val loss: 0.0750 score: 0.9592 time: 0.52s
Test loss: 0.2844 score: 0.8571 time: 1.55s
Epoch 528/1000, LR 0.000126
Train loss: 0.5217;  Loss pred: 0.5217; Loss self: 0.0000; time: 2.91s
Val loss: 0.0749 score: 0.9592 time: 0.12s
Test loss: 0.2844 score: 0.8571 time: 0.27s
Epoch 529/1000, LR 0.000125
Train loss: 0.5204;  Loss pred: 0.5204; Loss self: 0.0000; time: 1.03s
Val loss: 0.0747 score: 0.9592 time: 0.09s
Test loss: 0.2844 score: 0.8571 time: 0.07s
Epoch 530/1000, LR 0.000125
Train loss: 0.5211;  Loss pred: 0.5211; Loss self: 0.0000; time: 0.26s
Val loss: 0.0744 score: 0.9592 time: 0.08s
Test loss: 0.2843 score: 0.8571 time: 0.07s
Epoch 531/1000, LR 0.000124
Train loss: 0.5198;  Loss pred: 0.5198; Loss self: 0.0000; time: 0.26s
Val loss: 0.0742 score: 0.9592 time: 0.08s
Test loss: 0.2843 score: 0.8571 time: 0.07s
Epoch 532/1000, LR 0.000124
Train loss: 0.5207;  Loss pred: 0.5207; Loss self: 0.0000; time: 0.26s
Val loss: 0.0741 score: 0.9592 time: 0.08s
Test loss: 0.2843 score: 0.8571 time: 0.07s
Epoch 533/1000, LR 0.000123
Train loss: 0.5210;  Loss pred: 0.5210; Loss self: 0.0000; time: 0.26s
Val loss: 0.0739 score: 0.9592 time: 0.08s
Test loss: 0.2843 score: 0.8571 time: 0.07s
Epoch 534/1000, LR 0.000123
Train loss: 0.5198;  Loss pred: 0.5198; Loss self: 0.0000; time: 0.26s
Val loss: 0.0738 score: 0.9592 time: 0.08s
Test loss: 0.2843 score: 0.8571 time: 0.08s
Epoch 535/1000, LR 0.000123
Train loss: 0.5203;  Loss pred: 0.5203; Loss self: 0.0000; time: 0.26s
Val loss: 0.0737 score: 0.9592 time: 0.08s
Test loss: 0.2843 score: 0.8571 time: 0.07s
Epoch 536/1000, LR 0.000122
Train loss: 0.5200;  Loss pred: 0.5200; Loss self: 0.0000; time: 3.97s
Val loss: 0.0737 score: 0.9592 time: 0.42s
Test loss: 0.2843 score: 0.8571 time: 1.26s
Epoch 537/1000, LR 0.000122
Train loss: 0.5199;  Loss pred: 0.5199; Loss self: 0.0000; time: 3.65s
Val loss: 0.0737 score: 0.9592 time: 0.14s
Test loss: 0.2843 score: 0.8571 time: 0.08s
Epoch 538/1000, LR 0.000121
Train loss: 0.5196;  Loss pred: 0.5196; Loss self: 0.0000; time: 0.26s
Val loss: 0.0737 score: 0.9592 time: 0.08s
Test loss: 0.2843 score: 0.8571 time: 0.07s
     INFO: Early stopping counter 1 of 2
Epoch 539/1000, LR 0.000121
Train loss: 0.5189;  Loss pred: 0.5189; Loss self: 0.0000; time: 0.26s
Val loss: 0.0738 score: 0.9592 time: 0.08s
Test loss: 0.2843 score: 0.8571 time: 0.07s
     INFO: Early stopping counter 2 of 2
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 536,   Train_Loss: 0.5199,   Val_Loss: 0.0737,   Val_Precision: 0.9583,   Val_Recall: 0.9583,   Val_accuracy: 0.9583,   Val_Score: 0.9592,   Val_Loss: 0.0737,   Test_Precision: 0.8750,   Test_Recall: 0.8400,   Test_accuracy: 0.8571,   Test_Score: 0.8571,   Test_loss: 0.2843


[0.07079106802120805, 0.07018441602122039, 0.07023633108474314, 0.0697052989853546, 0.0703910420415923, 0.07062913395930082, 0.07010162097867578, 0.07027464197017252, 0.06987066601868719, 0.07009714795276523, 0.07049367693252861, 0.07013685896527022, 0.07022120803594589, 0.06995216698851436, 0.06983618205413222, 0.06972508504986763, 0.06990352703724056, 0.07067643001209944, 0.06929189700167626, 0.06959368195384741, 0.06970364798326045, 0.06998555001337081, 0.06994720397051424, 0.0698741520754993, 0.0702572810696438, 0.07043814600910991, 0.07038082601502538, 0.07069158996455371, 0.07080697803758085, 0.07547648099716753, 0.07524220296181738, 0.07525971997529268, 0.07558234606403857, 0.07552790699992329, 0.07616590999532491, 0.07546610292047262, 0.07044299796689302, 0.07069637405220419, 0.07056315895169973, 0.070826111943461, 0.07062980299815536, 0.07562143099494278, 0.07549268391449004, 0.07611173309851438, 0.08583163807634264, 0.07355130393989384, 0.0761714419350028, 2.4632770200259984, 0.08091302390675992, 0.07595029496587813, 0.07654094696044922, 0.07641816406976432, 0.0757436299463734, 1.3762688419083133, 0.6079220100073144, 0.07742297789081931, 0.07515917695127428, 0.0758576060179621, 0.07616155501455069, 0.07585350808221847, 0.07563257205765694, 0.7125495029613376, 0.29406631097663194, 0.08429849497042596, 0.08278192998841405, 0.08096620603464544, 0.08120407396927476, 0.07401292398571968, 0.07353285199496895, 0.07407186191994697, 0.07376284198835492, 0.07481843302957714, 0.07398352201562375, 0.07388156501110643, 0.07108427409548312, 0.07642552303150296, 0.07712038594763726, 0.07851661404129118, 0.0777531280182302, 0.07896128005813807, 1.330918365973048, 1.9628155010286719, 0.7638422230957076, 0.23356156307272613, 0.07001405698247254, 0.07009359204676002, 0.07082958205137402, 0.07003154105041176, 0.07079189096111804, 0.07050531799905002, 1.301095929928124, 0.0809614920290187, 0.07863310794346035, 0.07139787299092859, 0.07099319796543568, 0.07143126800656319, 0.07198415801394731, 0.07121521304361522, 0.0706044880207628, 0.071301119052805, 0.07134654093533754, 0.07078228995669633, 0.9570135719841346, 0.09071581903845072, 0.08132547105196863, 0.08263122907374054, 0.0809444549959153, 0.08177431998774409, 1.080570682999678, 0.3054379989625886, 0.2619882479775697, 0.223650265019387, 0.07601278799120337, 0.07643709995318204, 0.07382068992592394, 0.07402475294657052, 0.07412147300783545, 0.07321592594962567, 0.07373925903812051, 0.07421306590549648, 0.07441458303947002, 0.07375607104040682, 0.0738056170521304, 0.0735326959984377, 0.07321798091288656, 0.07329075201414526, 0.07448288204614073, 0.07503925508353859, 1.1114320220658556, 0.40592054289299995, 1.1290595420869067, 0.08128957101143897, 0.0777922139968723, 0.07161635893862695, 0.07137364707887173, 0.07118470792192966, 0.0705737010575831, 0.07182544993702322, 0.0708634409820661, 0.07091379608027637, 0.07670305401552469, 0.07636678090784699, 0.07726341998204589, 1.8816777180181816, 0.08302532706875354, 0.07769285491667688, 0.07628627901431173, 0.07581749395467341, 0.07161926000844687, 0.07125062297564, 0.07343270804267377, 0.07160622801166028, 2.5457634339109063, 1.1847587230149657, 0.0761593539500609, 0.07601889304351062, 0.0756638270104304, 0.07504404603969306, 0.07630067609716207, 0.07635059696622193, 0.07645873294677585, 0.07631359796505421, 0.6198731489712372, 0.752224643016234, 0.2589336549863219, 0.07873389695305377, 0.07810413604602218, 0.07768573798239231, 0.07828763802535832, 0.07759926002472639, 0.07301040191669017, 0.16768929501995444, 0.6300503599923104, 1.192017499008216, 0.07821254199370742, 0.0754999719792977, 0.07630452304147184, 0.07455950998701155, 0.07454268401488662, 0.07398487103637308, 0.07520310103427619, 0.07473234296776354, 0.07089906500186771, 0.07036739704199135, 0.07055947289336473, 0.07076507096644491, 0.07611307594925165, 1.6593898329883814, 0.10074626398272812, 0.08078547392506152, 0.08034060907084495, 0.07922569196671247, 0.08035735797602683, 0.07951429695822299, 0.0819287640042603, 0.08125606493558735, 0.07519700808916241, 1.9958464580122381, 0.20342813699971884, 0.08292834903113544, 0.07933131500612944, 0.07934211904648691, 0.07871378306299448, 0.07418975699692965, 0.07506563398055732, 0.07485581294167787, 0.07439985894598067, 0.13452933996450156, 0.07645984995178878, 0.07391092297621071, 0.07250507303979248, 0.07235470903106034, 0.07342183997388929, 0.0731692030094564, 0.07219782401807606, 0.08610134106129408, 0.7368988529779017, 0.5475614339811727, 0.07617503497749567, 0.07644669397268444, 0.07625276606995612, 0.0754458160372451, 0.07517434807959944, 1.7140403729863465, 1.4538443150231615, 0.07930552307516336, 0.0742275039665401, 0.07525380293373019, 0.07353433698881418, 0.07632599701173604, 0.07687233993783593, 0.07690021710004658, 0.07701993093360215, 1.7101872370112687, 0.11457035201601684, 0.07626730599440634, 0.07390865497291088, 0.07395194109994918, 0.07333123497664928, 0.07484246196690947, 0.07449848100077361, 0.07374451891519129, 1.1587652700254694, 0.9716880540363491, 0.08055274398066103, 0.07235359004698694, 0.07262417103629559, 0.07238392403814942, 0.07488380302675068, 0.07331967598292977, 0.07202200999017805, 0.07273058104328811, 0.8126363729825243, 0.07772666809614748, 0.0779280960559845, 0.07842834806069732, 0.07636707101482898, 0.0768673149868846, 0.07647782494314015, 0.07733017997816205, 0.07736604998353869, 0.07718655897770077, 0.09647638606838882, 0.07198115903884172, 0.07132313295733184, 0.07486670999787748, 0.0753897309768945, 0.07577818795107305, 0.07583305600564927, 0.07599412393756211, 0.07545864896383137, 1.832222472061403, 1.3043537019984797, 0.10558439197484404, 0.07582089898642153, 0.07543166598770767, 0.0753628850216046, 0.0746354250004515, 0.07460115104913712, 0.07472972897812724, 0.07480539102107286, 0.07661975896917284, 0.07693251105956733, 0.07538391696289182, 0.07481813698541373, 0.08407897304277867, 0.33966407796833664, 0.08083293109666556, 0.07542941102292389, 0.07473284692969173, 0.07464520109351724, 0.07481097499839962, 0.07755251496564597, 0.07566923298873007, 0.0753642920171842, 1.3526444700546563, 1.142498360015452, 0.07669668097514659, 0.0725506970193237, 0.07329478196334094, 0.07369000697508454, 0.07232166000176221, 0.07463639706838876, 0.07248210092075169, 0.8473683960037306, 0.15146583004388958, 0.09036367502994835, 0.07203829102218151, 0.07184084702748805, 0.07175285299308598, 0.07297035492956638, 0.07367606391198933, 0.07136915903538465, 0.07196484401356429, 0.16834316903259605, 0.23047363793011755, 0.07613274198956788, 0.07537971297279, 0.07762923603877425, 0.07147652900312096, 0.07413994602393359, 0.0728762389626354, 0.07196811097674072, 1.5649823650019243, 0.9546250039711595, 0.6697067440254614, 0.09910916292574257, 0.07648684305604547, 0.07622695493046194, 0.07154838601127267, 0.07206896797288209, 0.07163116696756333, 0.07207878504414111, 0.07171475992072374, 1.0959005790064111, 0.08130196190904826, 0.07195680006407201, 0.0708982179639861, 0.07140820706263185, 0.07166070002131164, 0.07169972301926464, 0.07262321899179369, 0.07194010401144624, 0.07196969201322645, 0.07550320599693805, 0.07503258006181568, 0.07676662795711309, 0.07522658398374915, 0.07524492708034813, 0.07500891899690032, 0.07532408891711384, 0.07837549201212823, 0.0751217040233314, 0.07501951802987605, 1.4089046800509095, 0.7427090919809416, 0.07980849000159651, 0.07757302094250917, 0.07150855893269181, 0.07255654805339873, 0.0710336669581011, 0.07117252703756094, 0.07878000801429152, 0.6022876760689542, 0.07470769493374974, 0.07190134399570525, 0.07235775806475431, 0.07258412393275648, 0.07227962696924806, 0.07115499605424702, 0.07162317796610296, 0.13754573499318212, 0.08015248796436936, 0.07725006202235818, 0.07168986706528813, 0.07154926296789199, 0.07176149799488485, 0.07127556309569627, 0.07173472899012268, 0.07263171602971852, 0.0704352060565725, 0.07567193906288594, 0.07247904897667468, 0.0748370949877426, 0.07262987701687962, 0.7650765490252525, 0.2168380639050156, 0.07680157606955618, 0.0761233480880037, 0.07125348492991179, 0.07127411291003227, 0.07218827994074672, 0.07411653897725046, 0.0732951780082658, 0.07625077606644481, 1.9097842470509931, 0.40388829400762916, 0.07420507806818932, 0.07370426098350435, 0.07702849595807493, 0.07369586895219982, 0.07384481001645327, 0.07362777600064874, 1.5374662729445845, 1.454924462013878, 0.07805983093567193, 0.07277879596222192, 0.0712245519971475, 0.071607960970141, 0.07135662494692951, 0.07497784297447652, 0.07789807301014662, 0.07584687403868884, 0.07735531299840659, 0.07641167903784662, 2.569069736986421, 0.07321789406705648, 0.07131769403349608, 0.07133412605617195, 0.08491357695311308, 0.0855944809736684, 0.07158457802142948, 0.07125653198454529, 0.0719718059990555, 2.176973659079522, 0.1484769779490307, 0.07804734795354307, 0.07701335696037859, 0.07649646501522511, 0.07238761894404888, 0.072157112066634, 0.07176951505243778, 0.07664350711274892, 0.07628750894218683, 0.07686042401473969, 0.07659794401843101, 0.07212399097625166, 0.07188448996748775, 0.07186248106881976, 0.07212042598985136, 0.07219423400238156, 0.07218007394112647, 0.07194581592921168, 0.07254012592602521, 0.07217457506339997, 0.07246932503767312, 0.07260935101658106, 0.07233121106401086, 0.07259570294991136, 0.07203584001399577, 0.07260043395217508, 0.07254593493416905, 0.07250329398084432, 0.07225458696484566, 0.07154655607882887, 0.0719680329784751, 0.07231356902047992, 0.07227121607866138, 0.0727769109653309, 0.07230290200095624, 0.07539433101192117, 0.07370674598496407, 0.07335453201085329, 0.07331012701615691, 0.07278206502087414, 0.07349057798273861, 0.07373035291675478, 0.07302411796990782, 0.07337991602253169, 0.0730509819695726, 0.0749324340140447, 0.07294332794845104, 0.07304076291620731, 0.07331168500240892, 0.0737251719692722, 0.07409830007236451, 0.07380490098148584, 0.07344602898228914, 1.286455709952861, 0.7283633049810305, 0.07732057198882103, 0.0725549190538004, 0.07169379491824657, 0.07168781897053123, 0.07277841807808727, 0.07265463809017092, 0.07062296802178025, 0.07256280293222517, 0.07931700197514147, 1.3364310819888487, 0.13911627291236073, 0.07630393491126597, 0.07588632998522371, 0.07639297004789114, 0.07590514898765832, 0.07592302793636918, 0.5692937700077891, 0.22396281396504492, 0.0808688810793683, 0.07174545398447663, 0.07097170792985708, 0.07111416000407189, 0.07468396902550012, 0.07316304906271398, 0.07292662002146244, 0.07243384793400764, 0.07250937703065574, 0.0741132030962035, 0.07682894601020962, 0.07705742795951664, 0.07692722708452493, 0.0764945870032534, 0.0767737430287525, 0.07994903100188822, 0.07781838392838836, 0.07713190896902233, 0.9640839280327782, 0.07820469804573804, 0.07267871696967632, 0.07239837991073728, 0.07150185003411025, 0.07711926498450339, 0.07771868107374758, 0.07540383108425885, 0.07627028995193541, 0.075465610018, 1.5505055129760876, 0.27935171709395945, 0.0785616299835965, 0.0776502740336582, 0.07799595210235566, 0.07682561699766666, 0.07779261202085763, 0.08030430402141064, 0.07824731897562742, 1.265869040042162, 0.08306884206831455, 0.07735197304282337, 0.07705766102299094]
[0.0014447156739022052, 0.0014323350208412325, 0.0014333945119335335, 0.001422557122150094, 0.0014365518783998428, 0.0014414108971285882, 0.0014306453260954242, 0.0014341763667382148, 0.0014259319595650447, 0.0014305540398523516, 0.0014386464680107881, 0.0014313644686789842, 0.0014330858782846101, 0.0014275952446635585, 0.001425228205186372, 0.0014229609193850536, 0.0014266025925967463, 0.001442376122695907, 0.001414120346972985, 0.0014202792235479063, 0.001422523428229805, 0.0014282765308851187, 0.0014274939585819232, 0.0014260031035816182, 0.0014338220626457917, 0.001437513183859386, 0.0014363433880617423, 0.001442685509480688, 0.001445040368113895, 0.00154033634688097, 0.001535555162486069, 0.0015359126525569934, 0.0015424968584497668, 0.0015413858571412917, 0.0015544063264352022, 0.0015401245493974005, 0.00143761220340598, 0.0014427831439225345, 0.0014400644684020355, 0.001445430855989, 0.0014414245509827624, 0.0015432945101008732, 0.001540667018663062, 0.0015533006754798852, 0.0017516660831906662, 0.001501047019181507, 0.0015545192231633225, 0.05027095959236731, 0.0016512862021787738, 0.0015500060197117986, 0.001562060142049984, 0.0015595543687707003, 0.0015457883662525185, 0.02808711922261864, 0.012406571632802335, 0.0015800607732820268, 0.0015338607541076383, 0.0015481144085298388, 0.0015543174492765445, 0.001548030777188132, 0.0015435218787276928, 0.014541826591047705, 0.006001353285237387, 0.00172037744837604, 0.0016894271426206948, 0.001652371551727458, 0.0016572259993729542, 0.0015104678364432588, 0.0015006704488769174, 0.001511670651427489, 0.001505364122211325, 0.0015269067965219825, 0.0015098677962372194, 0.0015077870410429885, 0.0014506994713363902, 0.0015597045516633258, 0.0015738854275028013, 0.0016023798783936975, 0.00158679853098429, 0.0016114546950640424, 0.027161599305572406, 0.04005745920466677, 0.015588616797871584, 0.004766562511688288, 0.0014288583057647456, 0.0014304814703420413, 0.001445501674517837, 0.001429215123477791, 0.0014447324685942457, 0.0014388840407969393, 0.026552978161798448, 0.001652275347530994, 0.0016047573049685784, 0.0014570994487944611, 0.0014488407748048098, 0.0014577809797257793, 0.0014690644492642308, 0.0014533716947676576, 0.0014409079187910777, 0.0014551248786286736, 0.0014560518558232151, 0.0014445365297284965, 0.01953088922416601, 0.0018513432456826677, 0.0016597034908565028, 0.001686351613749807, 0.001651927652977863, 0.0016688636732192673, 0.022052462918360775, 0.00623342855025691, 0.005346698938317749, 0.004564291122844633, 0.001551281387575579, 0.001559940815371062, 0.0015065446923657948, 0.0015107092438075616, 0.0015126831226088867, 0.001494202570400524, 0.0015048828375126635, 0.0015145523654182954, 0.001518664959989184, 0.0015052259396001392, 0.0015062370826965387, 0.0015006672652742388, 0.0014942445084262564, 0.00149572963294174, 0.0015200588172681782, 0.001531413369051808, 0.022682286164609298, 0.00828409271210204, 0.02304203147116136, 0.0016589708369681422, 0.0015875962040178021, 0.0014615583456862643, 0.0014566050424259535, 0.0014527491412638705, 0.001440279613420063, 0.0014658255089188413, 0.0014461926731033897, 0.0014472203281689056, 0.0015653684492964223, 0.0015585057328132037, 0.001576804489429508, 0.038401586082003704, 0.0016943944299745621, 0.0015855684676872833, 0.00155686283702677, 0.0015472957949933348, 0.0014616175511927934, 0.0014540943464416327, 0.0014986266947484442, 0.0014613515920746997, 0.05195435579410013, 0.024178749449285015, 0.0015542725295930797, 0.0015514059804798085, 0.0015441597349067427, 0.0015315111436672052, 0.0015571566550441238, 0.0015581754482902434, 0.0015603823050362418, 0.0015574203666337595, 0.012650472427984434, 0.015351523326861918, 0.005284360305843305, 0.0016068142235317097, 0.0015939619601229016, 0.0015854232241304554, 0.0015977068984767003, 0.001583658367851559, 0.001490008202381432, 0.003422230510611315, 0.012858170612087967, 0.024326887734861552, 0.0015961743264021923, 0.0015408157546795448, 0.0015572351641116701, 0.0015216226527961542, 0.001521279265609931, 0.00150989532727292, 0.0015347571639648201, 0.0015251498564849704, 0.0014469196939156676, 0.0014360693273875787, 0.001439989242721729, 0.0014441851217641818, 0.0015533280805969725, 0.03386509863241595, 0.0020560462037291453, 0.0016486831413277862, 0.0016396042667519376, 0.0016168508564635199, 0.0016399460811434047, 0.0016227407542494486, 0.0016720155919236796, 0.0016582870395017825, 0.0015346328181461717, 0.040731560367596696, 0.004151594632647323, 0.001692415286349703, 0.0016190064286965193, 0.0016192269193160596, 0.0016064037359794791, 0.0015140766734067276, 0.001531951713888925, 0.0015276696518709768, 0.0015183644682853197, 0.0027454967339694196, 0.0015604051010569139, 0.0015083861831879737, 0.0014796953681590302, 0.0014766267149195988, 0.0014984048974263121, 0.0014932490410093143, 0.001473424979960736, 0.0017571702257406954, 0.015038752101589831, 0.011174723142472913, 0.0015545925505611362, 0.0015601366116874376, 0.0015561788993868598, 0.0015397105313723488, 0.001534170368971417, 0.03498041577523156, 0.029670292143329825, 0.001618480062758436, 0.0015148470197253081, 0.0015357918966067384, 0.0015007007548737588, 0.0015576734084027763, 0.001568823264037468, 0.0015693921857152361, 0.0015718353251755542, 0.03490178034716875, 0.0023381704493064663, 0.001556475632538905, 0.0015083398974063446, 0.001509223287754065, 0.0014965558158499853, 0.0015273971829981524, 0.001520377163281094, 0.0015049901819426793, 0.023648270816846312, 0.01983036844972141, 0.0016439335506257353, 0.0014766038785099375, 0.0014821259395162367, 0.0014772229395540698, 0.0015282408780969527, 0.0014963199180189747, 0.0014698369385750623, 0.0014842975723120024, 0.016584415775153558, 0.0015862585325744382, 0.0015903693072649898, 0.0016005785318509657, 0.0015585116533638568, 0.001568720714018053, 0.001560771937615105, 0.0015781669383298379, 0.0015788989792558917, 0.0015752358975040974, 0.001968905838130384, 0.0014690032456906475, 0.001455574141986364, 0.0015278920407730098, 0.0015385659383039694, 0.0015464936316545521, 0.0015476133878703933, 0.0015509004885216756, 0.0015399724278332932, 0.0373922953481919, 0.026619463306091422, 0.0021547835096906946, 0.001547365285437174, 0.001539421754851177, 0.0015380180616654, 0.0015231719387847247, 0.0015224724703905533, 0.0015250965097576988, 0.0015266406330831197, 0.0015636685503912823, 0.0015700512461136191, 0.0015384472849569758, 0.001526900754804362, 0.0017158974090362995, 0.006931919958537483, 0.001649651655033991, 0.001539375735161712, 0.0015251601414222802, 0.001523371450888107, 0.0015267545918040738, 0.0015827043870539994, 0.0015442700609944913, 0.001538046775860902, 0.027604989184788903, 0.023316293061539834, 0.0015652383872478896, 0.0014806264697821165, 0.0014958118768028763, 0.0015038776933690723, 0.0014759522449339227, 0.001523191776905893, 0.0014792265494030957, 0.017293232571504707, 0.003091139388650808, 0.001844156633264252, 0.0014701692045343165, 0.0014661397352548583, 0.0014643439386344077, 0.0014891909169299261, 0.0015035931410610067, 0.0014565134497017277, 0.001468670285991108, 0.003435574878216246, 0.004703543631226889, 0.0015537294283585281, 0.0015383614892406122, 0.0015842701232402908, 0.0014587046735330808, 0.0015130601229374201, 0.0014872701829109266, 0.0014687369587089941, 0.03193841561228417, 0.01948214293818693, 0.01366748457194819, 0.002022635978076379, 0.001560955980735622, 0.001555652141437999, 0.0014601711430871974, 0.0014707952647526957, 0.0014618605503584354, 0.001470995613145737, 0.001463566528994362, 0.02236531793890635, 0.0016592237124295564, 0.0014685061237565717, 0.0014469024074282876, 0.0014573103482169764, 0.001462463265741054, 0.0014632596534543804, 0.001482106510036606, 0.0014681653879886987, 0.0014687692247597234, 0.001540881755039552, 0.0015312771441186874, 0.0015666658766757772, 0.0015352364078316154, 0.0015356107567417988, 0.0015307942652428637, 0.0015372263044308946, 0.001599499836982209, 0.001533096000476151, 0.0015310105720382867, 0.028753156735732848, 0.015157328407774319, 0.0016287446939101328, 0.0015831228763777383, 0.001459358345565139, 0.0014807458786407904, 0.001449666672614308, 0.001452500551786958, 0.0016077552655977862, 0.012291585225897024, 0.0015246468353826478, 0.0014673743672592907, 0.0014766889400970266, 0.0014813086516889079, 0.0014750944279438378, 0.0014521427766172861, 0.0014616975095123053, 0.0028070558161873904, 0.0016357650604973339, 0.0015765318780073098, 0.0014630585115364924, 0.001460189040161061, 0.001464520367242548, 0.0014546033284835974, 0.0014639740610229118, 0.0014822799189738473, 0.00143745318482801, 0.0015443252869976722, 0.0014791642648300954, 0.0015272876528110734, 0.001482242388099584, 0.015613807122964335, 0.0044252666103064405, 0.0015673791034603302, 0.001553537716081708, 0.0014541527536716691, 0.0014545737328578014, 0.0014732302028723822, 0.001512582428107152, 0.0014958199593523632, 0.0015561382870703023, 0.03897518871532639, 0.008242618245053656, 0.0015143893483303944, 0.0015041685915000889, 0.0015720101215933658, 0.0015039973255550983, 0.0015070369391112911, 0.0015026076734826273, 0.031376862713154784, 0.029692335959466895, 0.0015930577741973863, 0.001485281550249427, 0.0014535622856560715, 0.001461386958574306, 0.0014562576519781534, 0.0015301600607036023, 0.0015897565920438086, 0.0015478953885446703, 0.0015786798571103386, 0.0015594220211805434, 0.05242999463237594, 0.001494242736062377, 0.0014554631435407363, 0.0014557984909422848, 0.0017329301419002669, 0.0017468261423197631, 0.0014609097555393772, 0.001454214938460108, 0.0014688123673276634, 0.04442803385876575, 0.0030301424071230753, 0.0015928030194600625, 0.0015717011624567059, 0.001561152347249492, 0.001477298345796916, 0.001472594123808857, 0.0014646839806619957, 0.001564153206382631, 0.0015568879375956496, 0.001568580081933463, 0.0015632233473149184, 0.0014719181831888094, 0.0014670304074997501, 0.0014665812463024442, 0.0014718454283643135, 0.0014733517143343175, 0.0014730627334923769, 0.0014682819577390138, 0.001480410733184188, 0.0014729505114979586, 0.0014789658170953697, 0.0014818234901343072, 0.0014761471645716503, 0.0014815449581614562, 0.0014701191839590973, 0.0014816415092280628, 0.001480529284370797, 0.0014796590608335575, 0.0014745834074458297, 0.0014601337975271198, 0.001468735366907655, 0.001475787122866937, 0.0014749227771155384, 0.0014852430809251204, 0.0014755694285909437, 0.0015386598165698197, 0.0015042193058155933, 0.0014970312655276181, 0.0014961250411460595, 0.0014853482657321254, 0.0014998077139334411, 0.001504701079933771, 0.0014902881218348534, 0.0014975493065822795, 0.0014908363667259716, 0.0015292333472254021, 0.001488639345886756, 0.0014906278146164758, 0.0014961568367838555, 0.0015045953463116775, 0.0015122102055584593, 0.001506222469009915, 0.001498898550658962, 0.02625419816230329, 0.014864557244510827, 0.001577970856914715, 0.0014807126337510286, 0.0014631386718009503, 0.0014630167136843108, 0.0014852738383283116, 0.0014827477161259372, 0.0014412850616689846, 0.001480873529229085, 0.0016187143260232952, 0.027274103714058136, 0.0028391076104563413, 0.0015572231614544075, 0.0015487006119433411, 0.0015590402050590028, 0.0015490846732175167, 0.00154944954972182, 0.011618240204240595, 0.004570669672756019, 0.0016503853281503733, 0.0014641929384587066, 0.0014484022026501444, 0.001451309387838202, 0.001524162633173472, 0.001493123450259469, 0.0014882983677849478, 0.0014782417945715847, 0.00147978320470726, 0.0015125143489021125, 0.0015679376736777474, 0.0015726005706023804, 0.0015699434098882638, 0.0015611140204745593, 0.001566811082219439, 0.0016316128775895554, 0.0015881302842528236, 0.0015741205912045374, 0.019675182204750577, 0.0015960142458313886, 0.0014832391218301288, 0.0014775179573619853, 0.001459221429267556, 0.0015738625507041508, 0.0015860955321172975, 0.0015388536955971196, 0.001556536529631335, 0.0015401144901632654, 0.03164296965257322, 0.005701055450897131, 0.001603298571093806, 0.0015846994700746573, 0.0015917541245378706, 0.0015678697346462583, 0.0015876043269562783, 0.0016388633473757275, 0.0015968840607270903, 0.025834062041676775, 0.001695282491190093, 0.0015786116947514974, 0.0015726053269998152]
[692.1777191625398, 698.1606854887096, 697.6446412167999, 702.9594695561829, 696.1113030696026, 693.7647009552129, 698.9852633351419, 697.2643136452781, 701.2957338476601, 699.0298668501965, 695.0978035505115, 698.6340808940903, 697.7948880474563, 700.4786571950722, 701.6420222116175, 702.7599889617205, 700.9660610386029, 693.3004396460242, 707.1533919588696, 704.086903068233, 702.9761198691854, 700.144529701324, 700.5283587983819, 701.2607458485551, 697.4366108962837, 695.6457938808146, 696.2123460946473, 693.1517599840329, 692.022189875032, 649.2088575491333, 651.2302680035256, 651.0786914445922, 648.2995375465565, 648.7668193963029, 643.3324305192131, 649.2981365638686, 695.5978793382579, 693.1048537767583, 694.4133557504185, 691.8352378161838, 693.7581292882799, 647.9644639794888, 649.0695185178727, 643.7903593205189, 570.8850617113602, 666.2016493962204, 643.2857085968225, 19.8922003500373, 605.5885398185727, 645.1587847290656, 640.1802165489229, 641.2088094038285, 646.9190879113144, 35.60350892784679, 80.60244438165752, 632.88704897271, 651.9496618725178, 645.94709182356, 643.3692168002417, 645.9819886891501, 647.8690155168312, 68.76715203134273, 166.62908388677613, 581.267791520957, 591.9166176345238, 605.1907629095638, 603.418001152752, 662.046536756935, 666.3688225142217, 661.5197556793788, 664.2911075434936, 654.9188216843485, 662.3096422694265, 663.2236335631758, 689.3226472873778, 641.1470678427934, 635.3702642679943, 624.0717407175931, 630.199726350705, 620.5573157365483, 36.816683316392286, 24.964139509964177, 64.14937341564112, 209.79479395221566, 699.8594583980008, 699.0653292145691, 691.8013431797378, 699.6847315515685, 692.1696727512606, 694.9830366081069, 37.66055897408494, 605.2260003118165, 623.1471867452133, 686.2949545601402, 690.2069691783223, 685.9741030426314, 680.7053295046668, 688.0552329456674, 694.0068736932201, 687.2262406388182, 686.7887266519263, 692.2635595708693, 51.200945769672245, 540.1483503029489, 602.5172601667194, 592.9961414016019, 605.3533871155558, 599.2101188654808, 45.34640886607748, 160.42535691835033, 187.03129006074795, 219.09207215002613, 644.6283749738341, 641.0499617334076, 663.7705506297693, 661.9407434614088, 661.0769863521218, 669.253299257776, 664.5035580662505, 660.261092870048, 658.4730841535463, 664.3520907337329, 663.9061084658408, 666.3702361877371, 669.2345157441492, 668.5700262775706, 657.8692802145523, 652.9915568251578, 44.0872667218298, 120.7132796255555, 43.39895122752378, 602.7833508077534, 629.8830883251386, 684.2012177970611, 686.5278993779365, 688.3500885294034, 694.3096261880825, 682.2094402884125, 691.4707968019895, 690.9797910766284, 638.8272361369393, 641.6402448484647, 634.194034012297, 26.04059108039379, 590.1813546536582, 630.6886270629512, 642.3173424254619, 646.2888371026095, 684.1735029686271, 687.7132852122949, 667.2775838734526, 684.2980193290016, 19.24766431448196, 41.35863197133096, 643.3878106703768, 644.5766050809774, 647.6013960177465, 652.9498685889407, 642.1961443382611, 641.7762525377236, 640.8685850720242, 642.0874039045866, 79.04843124971953, 65.14011532980649, 189.23766399770787, 622.3494821959208, 627.367543904809, 630.7464056157385, 625.8970283932733, 631.4493203206646, 671.1372450176665, 292.2070845021393, 77.77156099172458, 41.106779087361595, 626.4979855013828, 649.0068633858028, 642.1637675838467, 657.1931603163154, 657.3415036975916, 662.2975658889802, 651.5688758322174, 655.6732741690781, 691.1233596481023, 696.3452118423469, 694.4496322138464, 692.4320053778314, 643.779001030923, 29.5289262510162, 486.3703929348738, 606.5446870492278, 609.9032676836123, 618.4862357603374, 609.776145385694, 616.2413789025227, 598.08054711349, 603.0319095422956, 651.6216701321393, 24.550986777210063, 240.87130090596918, 590.871524303504, 617.6627728434108, 617.5786655167436, 622.5085124010035, 660.4685334395673, 652.7620883438011, 654.5917821796577, 658.603399175492, 364.23281354780823, 640.859222597175, 662.960196232042, 675.8147802031405, 677.2192253439281, 667.3763558285337, 669.6806577716479, 678.6908146668235, 569.096827018266, 66.49487891314362, 89.48767564533189, 643.2553659407708, 640.9695103035906, 642.599639664825, 649.4727285580731, 651.8180902362551, 28.587424644279555, 33.70374633216444, 617.8636506004668, 660.132664868914, 651.1298843348854, 666.3553654866533, 641.983097744084, 637.4204302825261, 637.189358467628, 636.1989605293497, 28.651833518318515, 427.68481668931105, 642.4771317292077, 662.980540208174, 662.5924792666959, 668.2009380532453, 654.7085533031324, 657.731531458892, 664.4561619061027, 42.28638989061434, 50.427706501542524, 608.2970930420922, 677.229698874362, 674.7064964846363, 676.9458916620065, 654.3471087131588, 668.3062812690026, 680.3475771737326, 673.719352947778, 60.29757174191087, 630.4142606420136, 628.7847705761707, 624.7740926798291, 641.6378073539728, 637.4620995719777, 640.7085980338823, 633.6465273175045, 633.3527433599857, 634.8255531660133, 507.89630495969834, 680.733689958495, 687.0141280713739, 654.4965045397238, 649.9558940595975, 646.6240659071624, 646.1562091912753, 644.7866948273412, 649.3622755356586, 26.74347725081164, 37.566497434648376, 464.0837446094729, 646.2598129939771, 649.5945616259494, 650.1874229728992, 656.5246998955733, 656.826326549914, 655.6962091263824, 655.0330040544347, 639.5217194524802, 636.9218854959805, 650.0060221614717, 654.9214131000463, 582.785424544484, 144.26017697569938, 606.1885834797014, 649.6139812772543, 655.6688526278002, 656.4387165172435, 654.9841116366713, 631.8299286838848, 647.5551299336931, 650.1752844547025, 36.22533569225331, 42.888464189425434, 638.8803189003492, 675.3897896659623, 668.5332664541904, 664.9476911647936, 677.5286960891945, 656.5161492870787, 676.0289696013938, 57.82608866590804, 323.505308001161, 542.2532890983065, 680.1938150491698, 682.0632276405568, 682.8996751491063, 671.5055730138159, 665.0735313240073, 686.5710716264139, 680.8880179155844, 291.0721015980886, 212.6056604133502, 643.6127048558719, 650.0422735449743, 631.205490358368, 685.5397244857883, 660.912269671494, 672.37278840807, 680.8571092804735, 31.31025696889546, 51.32905569848279, 73.1663529405, 494.40433713190777, 640.6330558589719, 642.81722974111, 684.8512277031643, 679.9042830533883, 684.0597755749061, 679.8116806490614, 683.2624142389445, 44.71208514592212, 602.6914830765811, 680.9641334296308, 691.1316166633461, 686.1956351462837, 683.7778585113957, 683.4057083712086, 674.7153414603795, 681.1221734153139, 680.8421521519764, 648.9790645709421, 653.0496480279805, 638.2981942019733, 651.3654801949433, 651.2066912853376, 653.255648198648, 650.5223057383315, 625.1954372728848, 652.2748736474549, 653.1633538419436, 34.77879000176891, 65.97468716763383, 613.9697668633975, 631.6629081174345, 685.2326592977739, 675.3353255441253, 689.8137474572789, 688.4678968071556, 621.9852121760496, 81.35647124612613, 655.889598032075, 681.4893474442819, 677.1906884697697, 675.0787547617805, 677.9227017987716, 688.6375197413189, 684.1360770558127, 356.245142769631, 611.3347351336399, 634.3036978509884, 683.4996632840118, 684.8428336989151, 682.8174072326739, 687.4726466097704, 683.0722118814574, 674.636407873811, 695.6748300082197, 647.531972971901, 676.0574357945736, 654.7555060498487, 674.6534898938643, 64.04587888941121, 225.97508535892533, 638.0077403049987, 643.6921290344811, 687.6856626479204, 687.4866343387762, 678.78054498902, 661.1209950729108, 668.5296540854853, 642.6164103208795, 25.65734850712257, 121.32067387690722, 660.3321669572586, 664.819093850851, 636.1282197002746, 664.8947993514005, 663.5537418145212, 665.5097119810906, 31.87061782249979, 33.678724414444964, 627.7236244641658, 673.2730234426379, 687.9650152374762, 684.28145887902, 686.6916706955109, 653.5264026824601, 629.0271133358781, 646.0384903273076, 633.4406532749642, 641.2632285665435, 19.073051733301, 669.2353095422744, 687.0665220469126, 686.9082542823195, 577.0573064782849, 572.4668161148611, 684.5049779483427, 687.6562559994854, 680.8221541730248, 22.50831092771164, 330.01749279151386, 627.824023298867, 636.2532674067078, 640.552475075123, 676.9113380821925, 679.0737405725246, 682.7411326967804, 639.323562371917, 642.3069868113507, 637.5192516580856, 639.7038540382965, 679.3855877461673, 681.6491293485137, 681.857894011128, 679.4191704704459, 678.7245640473667, 678.8577141105003, 681.0680978058775, 675.4882125510659, 678.9094353095555, 676.1481492276545, 674.8442082729863, 677.4392309930569, 674.9710796768286, 680.2169585373036, 674.9270952330442, 675.4341238342915, 675.8313630956686, 678.1576375744862, 684.868744010719, 680.8578471868948, 677.6045030514634, 678.0015981281878, 673.2904619068316, 677.704471659408, 649.9162382945243, 664.7966796688573, 667.9887207616582, 668.3933311041847, 673.2427829018953, 666.7521380973363, 664.5838255422896, 671.0111859234258, 667.7576461787486, 670.7644261429589, 653.922438857596, 671.7543794358853, 670.8582720612189, 668.3791267161562, 664.6305283685556, 661.2837265112227, 663.9125498222913, 667.1565594351728, 38.08914649832405, 67.27411947431392, 633.7252653418601, 675.3504881407954, 683.4622167214803, 683.5191906192944, 673.2765192481331, 674.4235645243543, 693.8252720402279, 675.2771119628164, 617.7742322554874, 36.66481621115788, 352.22335226640666, 642.1687172094364, 645.7025924107951, 641.4202768825672, 645.5425047379471, 645.3904873376063, 86.07155493609139, 218.78632051679656, 605.9191044316445, 682.9701016401959, 690.4159619270794, 689.032957672485, 656.097963717882, 669.7369864670092, 671.9082824019433, 676.4793173026299, 675.774665382708, 661.1507525372366, 637.7804531314083, 635.8893788375981, 636.9656343671471, 640.5682012233882, 638.2390393763793, 612.8904801715825, 629.6712618073873, 635.2753439523888, 50.82545053933731, 626.560823383556, 674.2001240947089, 676.810725052328, 685.2969535281164, 635.3794996599906, 630.4790472898492, 649.8343558332693, 642.4519958017622, 649.3023774446739, 31.602596437047104, 175.40611709760472, 623.7141465907861, 631.0344761791891, 628.2377313081108, 637.8080894747416, 629.8798655438154, 610.1790009528714, 626.2195387839753, 38.70858552506188, 589.8721925087525, 633.4680044020695, 635.8874555688934]
Elapsed: 0.19978641902622707~0.3912891926870687
Time per graph: 0.004077273857678102~0.007985493728307523
Speed: 578.5087421089302~198.69895471669744
Total Time: 0.0784
best val loss: 0.07369887828826904 test_score: 0.8571

Testing...
Test loss: 0.2881 score: 0.8571 time: 0.07s
test Score 0.8571
Epoch Time List: [0.37639823206700385, 0.3745251349173486, 0.3741839430294931, 0.37733101611956954, 0.37574769312050194, 0.3773876429768279, 0.37769253517035395, 0.3751075549516827, 0.3729923429200426, 0.3717101371148601, 0.37525764910969883, 0.3748965618433431, 0.3746307980036363, 0.37378589215222746, 0.3735638309735805, 0.3732948479009792, 0.37375827704090625, 0.37443498300854117, 0.37179624394048005, 0.3725330561865121, 0.3726577899651602, 0.37397791212424636, 0.3737894961377606, 0.3725962790194899, 0.3740334380418062, 0.37539219309110194, 0.37491566594690084, 0.3769662278937176, 0.37642459594644606, 0.38577186898328364, 0.4000510841142386, 0.4008177120704204, 0.4010986869689077, 0.40256917593069375, 0.40222682198509574, 0.4012949229218066, 0.3787009621737525, 0.375361421960406, 0.3745913509046659, 0.3752423881087452, 0.37702242203522474, 0.37942425697110593, 0.40326296410057694, 0.4012536540394649, 0.41923354798927903, 0.48059733083937317, 0.3954363961238414, 6.859159874962643, 4.901147312833928, 0.4090990530094132, 0.40802656509913504, 0.40907405607867986, 0.4032149401027709, 1.9206464091548696, 8.815584134077653, 0.7271287410985678, 0.4073424080852419, 0.40294874901883304, 0.4066309130284935, 0.4058587581384927, 0.40503752103541046, 1.3743229298852384, 2.426551861106418, 1.5459655129816383, 0.43660920509137213, 0.43314315495081246, 0.43133221904281527, 0.40538669598754495, 0.3915080671431497, 0.3919425060739741, 0.39275030710268766, 0.39453249296639115, 0.39828140288591385, 0.39451108407229185, 0.3903914220863953, 0.38814537203870714, 0.4077402849216014, 0.45217874995432794, 0.41081010713241994, 0.42279465589672327, 2.2119055420625955, 8.763317777076736, 2.0299253369448707, 1.744243114022538, 0.8231765491655096, 0.3737597268773243, 0.3719633030705154, 0.3714803089387715, 0.3734264369122684, 0.37394541408866644, 7.478898775880225, 2.992386863916181, 0.4193758680485189, 0.3996129990555346, 0.3806088379351422, 0.3859677469590679, 0.3855156400240958, 0.38482132798526436, 0.37856397905852646, 0.38116576604079455, 0.38041424413677305, 0.38192438799887896, 3.222672971896827, 5.127945251995698, 0.43992241797968745, 0.4365421999245882, 0.43066419812384993, 0.4337546690367162, 1.4352472589816898, 3.787375396117568, 1.658802876016125, 1.167944026994519, 0.585864320048131, 0.41477528598625213, 0.3975003978703171, 0.39871511003002524, 0.3967299780342728, 0.3947185630677268, 0.39307200198527426, 0.39582336286548525, 0.39734051807317883, 0.3940711391624063, 0.3931426372146234, 0.3923234761459753, 0.3942095370730385, 0.3987908430863172, 0.39416258793789893, 0.39568958210293204, 2.7693105079233646, 4.240803399006836, 2.7761927401879802, 0.8654545539757237, 0.4133449780056253, 0.39248825795948505, 0.38537190400529653, 0.38241000194102526, 0.38143701292574406, 0.3817182950442657, 0.38330603390932083, 0.37729308602865785, 0.38754935492761433, 0.40364538109861314, 0.4087244149995968, 4.175897175911814, 1.1167938068974763, 0.41575713094789535, 0.40978615113999695, 0.40720315591897815, 0.395340007962659, 0.3838034140644595, 0.38412120600696653, 0.38527808093931526, 3.186578231980093, 5.31846538791433, 0.9707700938452035, 0.4041344498982653, 0.40334042196627706, 0.3987495880573988, 0.399191188858822, 0.40342729981057346, 0.402631540899165, 0.40772534091956913, 4.34271121409256, 4.963843410951085, 2.8614054111531004, 0.4376893399748951, 0.4209981750464067, 0.4184986539185047, 0.4167013569967821, 0.4160413509234786, 0.4066001200117171, 0.48537492798641324, 4.355209060013294, 4.734165631933138, 0.6668816381134093, 0.40303126780781895, 0.4041271419264376, 0.39972874394152313, 0.39543600706383586, 0.4001601419877261, 0.39912928990088403, 0.40115831093862653, 0.3879940329352394, 0.3791080969385803, 0.3815666871378198, 0.3790833189850673, 0.3873963551595807, 3.828188077895902, 5.616021211026236, 0.43325509887654334, 0.4301818989915773, 0.4227458060486242, 0.42361678380984813, 0.42440694593824446, 0.42533858900424093, 0.4280497229192406, 0.4111376829678193, 3.1295161090092734, 2.775326481787488, 1.6841413890942931, 0.42599249992053956, 0.4198140511289239, 0.4190155630931258, 0.4306324570206925, 0.39183275308459997, 0.3926434420282021, 0.39410938310902566, 0.4770562439225614, 5.2581886989064515, 0.38971547107212245, 0.3865820049541071, 0.38490910711698234, 0.3952248969580978, 0.38690326595678926, 0.3843662799336016, 0.3957766619278118, 3.55523403105326, 3.609549634042196, 1.2965773639734834, 0.4063441299367696, 0.40658999886363745, 0.3974194040056318, 0.3987978649092838, 3.6001200528116897, 7.651727668941021, 2.8028952131280676, 0.3959505290258676, 0.3920335801085457, 0.39313942298758775, 0.39552830799948424, 0.40977179491892457, 0.406336713116616, 0.40775025403127074, 3.54170104698278, 2.053591786068864, 0.6413629199378192, 0.3955622299108654, 0.39226500887889415, 0.39200197404716164, 0.4027169639011845, 0.3978547038277611, 0.39836159301921725, 3.215615506982431, 4.783188074012287, 2.360598027938977, 0.3945215219864622, 0.3873755029635504, 0.38634125888347626, 0.4144276441074908, 0.38442130805924535, 0.38375332206487656, 0.3858278508996591, 7.009595821029507, 1.3972146608866751, 0.4122039220528677, 0.40963145007845014, 0.4070663449820131, 0.40794701711274683, 0.4051949220011011, 0.4092830488225445, 0.40968689194414765, 0.41321010515093803, 3.451904858928174, 0.39089781686197966, 0.3827340028947219, 0.38380007608793676, 0.3874707509530708, 0.3985572779783979, 0.4021255790721625, 0.40124434512108564, 0.4019984267652035, 2.1603467070963234, 6.417135149007663, 2.471764747868292, 0.4132019600365311, 0.39945522497873753, 0.397621170966886, 0.39774526888504624, 0.39845314691774547, 0.3966151209315285, 0.397864049882628, 0.3993426349479705, 0.40219717705622315, 0.4043162548914552, 0.39960604591760784, 0.4078171431319788, 7.455857774009928, 2.3650482239900157, 0.40121608099434525, 0.39637163595762104, 0.394886820926331, 0.39881879300810397, 0.40397183888126165, 0.40164293092675507, 0.4010211080312729, 1.6823173540178686, 6.7130033581051975, 5.765791322221048, 0.39136105601210147, 0.3899857789510861, 0.39124624186661094, 0.39211206091567874, 0.40938949084375054, 0.3872564360499382, 2.859140945132822, 2.8092970229918137, 0.9622290689731017, 0.3972199260024354, 0.3864773678360507, 0.3814722589449957, 0.3889661580324173, 0.38347560190595686, 0.38434272701852024, 0.3854549190727994, 0.48004023695830256, 3.018282183096744, 0.5676829740405083, 0.4039875650778413, 0.4067193689988926, 0.39439986494835466, 0.3886396320303902, 0.40547066496219486, 0.38681282999459654, 1.877537143882364, 4.459628034965135, 3.1649565550033003, 1.3363861348479986, 0.4346634920220822, 0.4071883821161464, 0.38982229481916875, 0.379007285926491, 0.42892516998108476, 0.37986538594122976, 0.3827294319635257, 1.4118485731305555, 3.904342661029659, 0.3942702610511333, 0.38095499691553414, 0.37933779903687537, 0.38432117097545415, 0.38007927290163934, 0.38024810899514705, 0.3921833260683343, 0.3793235900811851, 0.38611058599781245, 0.398494262015447, 0.4045670230407268, 0.40400839410722256, 0.4021751470863819, 0.40122746501583606, 0.40367367095313966, 0.40245570591650903, 0.4043250441318378, 0.40193468902725726, 1.7287328710081056, 5.874142930144444, 0.5163825169438496, 0.4105944752227515, 0.3939163440372795, 0.38207386899739504, 0.38199873198755085, 0.3849191420013085, 0.39485342404805124, 8.767159789800644, 0.4840183148626238, 0.38526118895970285, 0.3840525079285726, 0.3870778230484575, 0.38662516488693655, 0.38002653105650097, 0.3787198959616944, 4.510267983889207, 0.6748328370740637, 0.41403809597250074, 0.3993634389480576, 0.3814206811366603, 0.3822419720236212, 0.382013350026682, 0.3813521359115839, 0.3819361600326374, 0.3767880699597299, 0.3835110029904172, 0.38665990892332047, 0.3977156209293753, 0.39378377003595233, 4.287508560111746, 1.1128321591531858, 0.6359318100148812, 0.40578652499243617, 0.39699398598168045, 0.3788134570932016, 0.3802639519562945, 0.38846858707256615, 0.3878249361878261, 0.3924551389645785, 2.8850969198392704, 4.0451330648502335, 1.3879942980129272, 0.3973101628944278, 0.3977298508398235, 0.3983533139107749, 0.39493591885548085, 0.39194185694213957, 3.3949142899364233, 7.39419196092058, 1.6680286370683461, 0.4032142008654773, 0.3821699470281601, 0.3804574229288846, 0.379988576984033, 0.38801132200751454, 0.4107969640754163, 0.40936523291748017, 0.409414981957525, 0.41091295902151614, 7.170542235835455, 5.219013932044618, 0.37654287787154317, 0.37675101088825613, 0.3844189429655671, 0.3912993079284206, 0.3929520819801837, 0.37985185999423265, 0.39105064515024424, 2.487011177930981, 3.2841244869632646, 1.0860721931094304, 0.4116385809611529, 0.40949253691360354, 0.3926726559875533, 0.3827190489973873, 0.3818587842397392, 0.39107924804557115, 0.40656021900940686, 0.40806369797792286, 0.40740316582378, 0.3933105330215767, 0.3840067950077355, 0.38469301513396204, 0.38438755099195987, 0.3837021930376068, 0.38509649015031755, 0.38435089704580605, 0.38501535006798804, 0.38368607603479177, 0.3868947660084814, 0.38564270304050297, 0.38437109789811075, 0.3854771979385987, 0.3841561059234664, 0.3859409629367292, 0.38597753702197224, 0.3847185851773247, 0.3839258559746668, 0.3851251108571887, 0.3831789770629257, 0.38508706691209227, 0.3855393868871033, 0.3869238418992609, 0.3861808500951156, 0.3898768739309162, 0.3893885639263317, 0.39055403508245945, 0.3910364710027352, 0.38977411901578307, 0.38997570506762713, 0.39133613696321845, 0.39014229783788323, 0.3894282509572804, 0.39183926896657795, 0.409801309928298, 0.3912028099875897, 0.40697080199606717, 0.39160557102877647, 0.3933941089781001, 0.39661471790168434, 0.3932390679838136, 0.39207031298428774, 2.6039124481612816, 2.8449274150189012, 2.6062433240003884, 0.3865326070226729, 0.3795322630321607, 0.37861329689621925, 0.38563380390405655, 0.3842978290049359, 0.38138448097743094, 0.3831664320314303, 0.39279494096990675, 8.48659346299246, 4.073821622994728, 0.40767387906089425, 0.4058440029621124, 0.41829189902637154, 0.4003287011291832, 0.40583495795726776, 3.168678469955921, 1.521223880117759, 1.1344133930979297, 0.38264942297246307, 0.37631687498651445, 0.3791282258462161, 0.38482818799093366, 0.3852538929786533, 0.38642475090455264, 0.38446499209385365, 0.38339076505508274, 0.3864034820580855, 0.3921067470218986, 0.40756949805654585, 0.407138949027285, 0.4056746199494228, 0.4065672369906679, 0.4173177629709244, 0.4133057778235525, 0.4146180379902944, 1.2923233410110697, 4.838216295000166, 0.394286363851279, 0.3879775230307132, 0.3853663749760017, 0.4016964529873803, 0.40893882408272475, 0.4011354699032381, 0.40649465599562973, 0.405319401063025, 7.029877334134653, 3.308227017871104, 1.1908221548656002, 0.41401491593569517, 0.41849047294817865, 0.4108591249678284, 0.4137744411127642, 0.4172808889998123, 0.41622564394492656, 5.648174186004326, 3.8755639009177685, 0.41554004489444196, 0.41434582602232695]
Total Epoch List: [539]
Total Time List: [0.07838089705910534]
========================k_idx:1========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: AEtransGAT
AEtransGAT(
  (gae): GAE(
    (encoder): transGAT(
      (encoder): EdgeFeatureGAT()
      (classifier): Sequential(
        (0): Linear(in_features=64, out_features=64, bias=True)
        (1): ReLU(inplace=True)
        (2): Linear(in_features=64, out_features=2, bias=True)
      )
    )
    (decoder): InnerProductDecoder()
  )
)
Total number of parameters: 1914242
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7d3598187940>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 2.2744;  Loss pred: 2.2744; Loss self: 0.0000; time: 3.38s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6958 score: 0.4898 time: 0.35s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6937 score: 0.5102 time: 0.90s
Epoch 2/1000, LR 0.000000
Train loss: 2.2712;  Loss pred: 2.2712; Loss self: 0.0000; time: 4.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6957 score: 0.4898 time: 0.85s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6937 score: 0.5102 time: 0.22s
Epoch 3/1000, LR 0.000030
Train loss: 2.2692;  Loss pred: 2.2692; Loss self: 0.0000; time: 1.51s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6956 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6936 score: 0.5102 time: 0.09s
Epoch 4/1000, LR 0.000060
Train loss: 2.2656;  Loss pred: 2.2656; Loss self: 0.0000; time: 0.33s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6955 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6936 score: 0.5102 time: 0.08s
Epoch 5/1000, LR 0.000090
Train loss: 2.2333;  Loss pred: 2.2333; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6954 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6935 score: 0.5102 time: 0.08s
Epoch 6/1000, LR 0.000120
Train loss: 2.2106;  Loss pred: 2.2106; Loss self: 0.0000; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6952 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6935 score: 0.5102 time: 0.08s
Epoch 7/1000, LR 0.000150
Train loss: 2.1754;  Loss pred: 2.1754; Loss self: 0.0000; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6950 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6934 score: 0.5102 time: 0.08s
Epoch 8/1000, LR 0.000180
Train loss: 2.1878;  Loss pred: 2.1878; Loss self: 0.0000; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6948 score: 0.4898 time: 1.88s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6933 score: 0.5102 time: 1.14s
Epoch 9/1000, LR 0.000210
Train loss: 2.1453;  Loss pred: 2.1453; Loss self: 0.0000; time: 4.87s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6945 score: 0.4898 time: 0.69s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6932 score: 0.5102 time: 0.85s
Epoch 10/1000, LR 0.000240
Train loss: 2.1382;  Loss pred: 2.1382; Loss self: 0.0000; time: 1.65s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6943 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6932 score: 0.5102 time: 0.08s
Epoch 11/1000, LR 0.000270
Train loss: 2.0808;  Loss pred: 2.0808; Loss self: 0.0000; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6941 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6931 score: 0.5102 time: 0.08s
Epoch 12/1000, LR 0.000270
Train loss: 2.0613;  Loss pred: 2.0613; Loss self: 0.0000; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6939 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6931 score: 0.5102 time: 0.08s
Epoch 13/1000, LR 0.000270
Train loss: 2.0209;  Loss pred: 2.0209; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6938 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6931 score: 0.5102 time: 0.08s
Epoch 14/1000, LR 0.000270
Train loss: 1.9804;  Loss pred: 1.9804; Loss self: 0.0000; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6937 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6931 score: 0.5102 time: 0.08s
Epoch 15/1000, LR 0.000270
Train loss: 1.9491;  Loss pred: 1.9491; Loss self: 0.0000; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6936 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6931 score: 0.5102 time: 0.08s
Epoch 16/1000, LR 0.000270
Train loss: 1.9374;  Loss pred: 1.9374; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6935 score: 0.4898 time: 1.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6931 score: 0.5102 time: 1.07s
Epoch 17/1000, LR 0.000270
Train loss: 1.9035;  Loss pred: 1.9035; Loss self: 0.0000; time: 1.46s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6934 score: 0.4898 time: 0.55s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6931 score: 0.5102 time: 0.54s
Epoch 18/1000, LR 0.000270
Train loss: 1.8778;  Loss pred: 1.8778; Loss self: 0.0000; time: 2.59s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6934 score: 0.4898 time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6931 score: 0.5102 time: 0.35s
Epoch 19/1000, LR 0.000270
Train loss: 1.8389;  Loss pred: 1.8389; Loss self: 0.0000; time: 1.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6933 score: 0.4898 time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6931 score: 0.5102 time: 0.18s
Epoch 20/1000, LR 0.000270
Train loss: 1.8092;  Loss pred: 1.8092; Loss self: 0.0000; time: 0.37s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6933 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6931 score: 0.5102 time: 0.09s
Epoch 21/1000, LR 0.000270
Train loss: 1.8025;  Loss pred: 1.8025; Loss self: 0.0000; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6932 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6931 score: 0.5102 time: 0.08s
Epoch 22/1000, LR 0.000270
Train loss: 1.7467;  Loss pred: 1.7467; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6932 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6931 score: 0.5102 time: 0.10s
Epoch 23/1000, LR 0.000270
Train loss: 1.7303;  Loss pred: 1.7303; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6932 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6931 score: 0.5102 time: 0.08s
Epoch 24/1000, LR 0.000270
Train loss: 1.7062;  Loss pred: 1.7062; Loss self: 0.0000; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6931 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6931 score: 0.5102 time: 0.08s
Epoch 25/1000, LR 0.000270
Train loss: 1.6762;  Loss pred: 1.6762; Loss self: 0.0000; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6931 score: 0.4898 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6931 score: 0.5102 time: 0.95s
Epoch 26/1000, LR 0.000270
Train loss: 1.6529;  Loss pred: 1.6529; Loss self: 0.0000; time: 3.31s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6931 score: 0.4898 time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6930 score: 0.5102 time: 0.66s
Epoch 27/1000, LR 0.000270
Train loss: 1.6371;  Loss pred: 1.6371; Loss self: 0.0000; time: 3.61s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6930 score: 0.4898 time: 0.09s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6930 score: 0.5102 time: 0.15s
Epoch 28/1000, LR 0.000270
Train loss: 1.6067;  Loss pred: 1.6067; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6930 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6930 score: 0.5102 time: 0.09s
Epoch 29/1000, LR 0.000270
Train loss: 1.5959;  Loss pred: 1.5959; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6930 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6929 score: 0.5102 time: 0.09s
Epoch 30/1000, LR 0.000270
Train loss: 1.5705;  Loss pred: 1.5705; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6930 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6929 score: 0.5102 time: 0.09s
Epoch 31/1000, LR 0.000270
Train loss: 1.5540;  Loss pred: 1.5540; Loss self: 0.0000; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6930 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6929 score: 0.5102 time: 0.08s
Epoch 32/1000, LR 0.000270
Train loss: 1.5402;  Loss pred: 1.5402; Loss self: 0.0000; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6930 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6928 score: 0.5102 time: 0.08s
Epoch 33/1000, LR 0.000270
Train loss: 1.5120;  Loss pred: 1.5120; Loss self: 0.0000; time: 1.90s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6930 score: 0.4898 time: 1.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6928 score: 0.5102 time: 1.19s
Epoch 34/1000, LR 0.000270
Train loss: 1.4940;  Loss pred: 1.4940; Loss self: 0.0000; time: 0.85s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6929 score: 0.4898 time: 0.55s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6927 score: 0.5102 time: 0.26s
Epoch 35/1000, LR 0.000270
Train loss: 1.4882;  Loss pred: 1.4882; Loss self: 0.0000; time: 0.40s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6929 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6927 score: 0.5102 time: 0.09s
Epoch 36/1000, LR 0.000270
Train loss: 1.4753;  Loss pred: 1.4753; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6929 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6926 score: 0.5102 time: 0.09s
Epoch 37/1000, LR 0.000270
Train loss: 1.4535;  Loss pred: 1.4535; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6929 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6926 score: 0.5102 time: 0.09s
Epoch 38/1000, LR 0.000270
Train loss: 1.4229;  Loss pred: 1.4229; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6929 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6925 score: 0.5102 time: 0.09s
Epoch 39/1000, LR 0.000269
Train loss: 1.4198;  Loss pred: 1.4198; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6929 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6925 score: 0.5102 time: 1.33s
Epoch 40/1000, LR 0.000269
Train loss: 1.4125;  Loss pred: 1.4125; Loss self: 0.0000; time: 2.94s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6928 score: 0.4898 time: 0.41s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6924 score: 0.5102 time: 0.37s
Epoch 41/1000, LR 0.000269
Train loss: 1.4006;  Loss pred: 1.4006; Loss self: 0.0000; time: 4.11s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6928 score: 0.4898 time: 0.35s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6923 score: 0.5102 time: 0.31s
Epoch 42/1000, LR 0.000269
Train loss: 1.3821;  Loss pred: 1.3821; Loss self: 0.0000; time: 0.36s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6928 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6922 score: 0.5102 time: 0.09s
Epoch 43/1000, LR 0.000269
Train loss: 1.3619;  Loss pred: 1.3619; Loss self: 0.0000; time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6927 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6922 score: 0.5102 time: 0.09s
Epoch 44/1000, LR 0.000269
Train loss: 1.3584;  Loss pred: 1.3584; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6927 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6921 score: 0.5102 time: 0.08s
Epoch 45/1000, LR 0.000269
Train loss: 1.3407;  Loss pred: 1.3407; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6926 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6920 score: 0.5102 time: 0.08s
Epoch 46/1000, LR 0.000269
Train loss: 1.3309;  Loss pred: 1.3309; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6926 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6919 score: 0.5102 time: 0.08s
Epoch 47/1000, LR 0.000269
Train loss: 1.3145;  Loss pred: 1.3145; Loss self: 0.0000; time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6925 score: 0.4898 time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6918 score: 0.5102 time: 0.09s
Epoch 48/1000, LR 0.000269
Train loss: 1.3074;  Loss pred: 1.3074; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6924 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6917 score: 0.5102 time: 0.09s
Epoch 49/1000, LR 0.000269
Train loss: 1.3001;  Loss pred: 1.3001; Loss self: 0.0000; time: 2.30s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6924 score: 0.4898 time: 0.83s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6916 score: 0.5102 time: 1.38s
Epoch 50/1000, LR 0.000269
Train loss: 1.2876;  Loss pred: 1.2876; Loss self: 0.0000; time: 4.58s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6923 score: 0.4898 time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6915 score: 0.5102 time: 0.11s
Epoch 51/1000, LR 0.000269
Train loss: 1.2769;  Loss pred: 1.2769; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6922 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6914 score: 0.5102 time: 0.09s
Epoch 52/1000, LR 0.000269
Train loss: 1.2578;  Loss pred: 1.2578; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6921 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6913 score: 0.5102 time: 0.09s
Epoch 53/1000, LR 0.000269
Train loss: 1.2463;  Loss pred: 1.2463; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6921 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6912 score: 0.5102 time: 0.09s
Epoch 54/1000, LR 0.000269
Train loss: 1.2413;  Loss pred: 1.2413; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6920 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6911 score: 0.5102 time: 0.09s
Epoch 55/1000, LR 0.000269
Train loss: 1.2357;  Loss pred: 1.2357; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6919 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6910 score: 0.5102 time: 1.91s
Epoch 56/1000, LR 0.000269
Train loss: 1.2278;  Loss pred: 1.2278; Loss self: 0.0000; time: 4.09s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6918 score: 0.4898 time: 0.53s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6908 score: 0.5102 time: 0.98s
Epoch 57/1000, LR 0.000269
Train loss: 1.2240;  Loss pred: 1.2240; Loss self: 0.0000; time: 2.62s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6917 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6907 score: 0.5102 time: 0.09s
Epoch 58/1000, LR 0.000269
Train loss: 1.2103;  Loss pred: 1.2103; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6916 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6906 score: 0.5102 time: 0.09s
Epoch 59/1000, LR 0.000268
Train loss: 1.2023;  Loss pred: 1.2023; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6914 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6904 score: 0.5102 time: 0.09s
Epoch 60/1000, LR 0.000268
Train loss: 1.1897;  Loss pred: 1.1897; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6913 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6903 score: 0.5102 time: 0.09s
Epoch 61/1000, LR 0.000268
Train loss: 1.1926;  Loss pred: 1.1926; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6912 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6901 score: 0.5102 time: 1.05s
Epoch 62/1000, LR 0.000268
Train loss: 1.1828;  Loss pred: 1.1828; Loss self: 0.0000; time: 3.38s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6911 score: 0.4898 time: 0.09s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6899 score: 0.5102 time: 0.10s
Epoch 63/1000, LR 0.000268
Train loss: 1.1748;  Loss pred: 1.1748; Loss self: 0.0000; time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6909 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6898 score: 0.5102 time: 0.09s
Epoch 64/1000, LR 0.000268
Train loss: 1.1726;  Loss pred: 1.1726; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6908 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6896 score: 0.5102 time: 0.09s
Epoch 65/1000, LR 0.000268
Train loss: 1.1643;  Loss pred: 1.1643; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6906 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6894 score: 0.5102 time: 0.09s
Epoch 66/1000, LR 0.000268
Train loss: 1.1634;  Loss pred: 1.1634; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6905 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6892 score: 0.5102 time: 0.09s
Epoch 67/1000, LR 0.000268
Train loss: 1.1525;  Loss pred: 1.1525; Loss self: 0.0000; time: 1.89s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6903 score: 0.4898 time: 1.09s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6890 score: 0.5102 time: 1.06s
Epoch 68/1000, LR 0.000268
Train loss: 1.1503;  Loss pred: 1.1503; Loss self: 0.0000; time: 1.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6902 score: 0.4898 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6888 score: 0.5102 time: 0.20s
Epoch 69/1000, LR 0.000268
Train loss: 1.1427;  Loss pred: 1.1427; Loss self: 0.0000; time: 0.49s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6900 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6886 score: 0.5102 time: 0.08s
Epoch 70/1000, LR 0.000268
Train loss: 1.1362;  Loss pred: 1.1362; Loss self: 0.0000; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6898 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6884 score: 0.5102 time: 0.08s
Epoch 71/1000, LR 0.000268
Train loss: 1.1316;  Loss pred: 1.1316; Loss self: 0.0000; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6896 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6881 score: 0.5102 time: 0.08s
Epoch 72/1000, LR 0.000267
Train loss: 1.1254;  Loss pred: 1.1254; Loss self: 0.0000; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6894 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6879 score: 0.5102 time: 0.09s
Epoch 73/1000, LR 0.000267
Train loss: 1.1219;  Loss pred: 1.1219; Loss self: 0.0000; time: 0.25s
Val loss: 0.6892 score: 0.5102 time: 0.07s
Test loss: 0.6876 score: 0.5306 time: 0.08s
Epoch 74/1000, LR 0.000267
Train loss: 1.1203;  Loss pred: 1.1203; Loss self: 0.0000; time: 0.24s
Val loss: 0.6890 score: 0.5510 time: 0.07s
Test loss: 0.6874 score: 0.5306 time: 0.08s
Epoch 75/1000, LR 0.000267
Train loss: 1.1137;  Loss pred: 1.1137; Loss self: 0.0000; time: 0.24s
Val loss: 0.6887 score: 0.5510 time: 0.07s
Test loss: 0.6871 score: 0.5510 time: 0.08s
Epoch 76/1000, LR 0.000267
Train loss: 1.1112;  Loss pred: 1.1112; Loss self: 0.0000; time: 0.25s
Val loss: 0.6885 score: 0.5714 time: 0.07s
Test loss: 0.6868 score: 0.5918 time: 1.12s
Epoch 77/1000, LR 0.000267
Train loss: 1.1054;  Loss pred: 1.1054; Loss self: 0.0000; time: 2.29s
Val loss: 0.6883 score: 0.5714 time: 0.96s
Test loss: 0.6865 score: 0.6122 time: 1.60s
Epoch 78/1000, LR 0.000267
Train loss: 1.1019;  Loss pred: 1.1019; Loss self: 0.0000; time: 3.74s
Val loss: 0.6880 score: 0.6122 time: 0.31s
Test loss: 0.6861 score: 0.6735 time: 0.38s
Epoch 79/1000, LR 0.000267
Train loss: 1.0903;  Loss pred: 1.0903; Loss self: 0.0000; time: 0.78s
Val loss: 0.6878 score: 0.6531 time: 0.08s
Test loss: 0.6858 score: 0.6735 time: 0.09s
Epoch 80/1000, LR 0.000267
Train loss: 1.0968;  Loss pred: 1.0968; Loss self: 0.0000; time: 0.24s
Val loss: 0.6875 score: 0.6531 time: 0.07s
Test loss: 0.6855 score: 0.7551 time: 0.08s
Epoch 81/1000, LR 0.000267
Train loss: 1.0910;  Loss pred: 1.0910; Loss self: 0.0000; time: 0.24s
Val loss: 0.6872 score: 0.6939 time: 0.07s
Test loss: 0.6851 score: 0.7959 time: 0.08s
Epoch 82/1000, LR 0.000267
Train loss: 1.0869;  Loss pred: 1.0869; Loss self: 0.0000; time: 0.24s
Val loss: 0.6869 score: 0.7143 time: 0.07s
Test loss: 0.6847 score: 0.8163 time: 0.08s
Epoch 83/1000, LR 0.000266
Train loss: 1.0822;  Loss pred: 1.0822; Loss self: 0.0000; time: 0.24s
Val loss: 0.6866 score: 0.7551 time: 0.07s
Test loss: 0.6843 score: 0.8367 time: 0.08s
Epoch 84/1000, LR 0.000266
Train loss: 1.0788;  Loss pred: 1.0788; Loss self: 0.0000; time: 0.24s
Val loss: 0.6863 score: 0.7959 time: 0.07s
Test loss: 0.6839 score: 0.8776 time: 0.08s
Epoch 85/1000, LR 0.000266
Train loss: 1.0778;  Loss pred: 1.0778; Loss self: 0.0000; time: 0.24s
Val loss: 0.6860 score: 0.7959 time: 0.07s
Test loss: 0.6835 score: 0.8776 time: 0.08s
Epoch 86/1000, LR 0.000266
Train loss: 1.0750;  Loss pred: 1.0750; Loss self: 0.0000; time: 0.24s
Val loss: 0.6856 score: 0.8367 time: 0.07s
Test loss: 0.6831 score: 0.8776 time: 0.08s
Epoch 87/1000, LR 0.000266
Train loss: 1.0716;  Loss pred: 1.0716; Loss self: 0.0000; time: 0.24s
Val loss: 0.6853 score: 0.8367 time: 0.07s
Test loss: 0.6826 score: 0.8776 time: 0.08s
Epoch 88/1000, LR 0.000266
Train loss: 1.0655;  Loss pred: 1.0655; Loss self: 0.0000; time: 0.26s
Val loss: 0.6849 score: 0.8367 time: 0.07s
Test loss: 0.6821 score: 0.8776 time: 0.08s
Epoch 89/1000, LR 0.000266
Train loss: 1.0662;  Loss pred: 1.0662; Loss self: 0.0000; time: 4.10s
Val loss: 0.6845 score: 0.8367 time: 0.24s
Test loss: 0.6816 score: 0.8776 time: 0.28s
Epoch 90/1000, LR 0.000266
Train loss: 1.0580;  Loss pred: 1.0580; Loss self: 0.0000; time: 1.61s
Val loss: 0.6842 score: 0.8367 time: 0.13s
Test loss: 0.6811 score: 0.8776 time: 0.09s
Epoch 91/1000, LR 0.000266
Train loss: 1.0588;  Loss pred: 1.0588; Loss self: 0.0000; time: 0.26s
Val loss: 0.6837 score: 0.8571 time: 0.07s
Test loss: 0.6806 score: 0.8776 time: 0.09s
Epoch 92/1000, LR 0.000266
Train loss: 1.0504;  Loss pred: 1.0504; Loss self: 0.0000; time: 0.26s
Val loss: 0.6833 score: 0.8571 time: 0.07s
Test loss: 0.6801 score: 0.8980 time: 0.09s
Epoch 93/1000, LR 0.000265
Train loss: 1.0501;  Loss pred: 1.0501; Loss self: 0.0000; time: 0.26s
Val loss: 0.6829 score: 0.8571 time: 0.07s
Test loss: 0.6795 score: 0.9388 time: 0.09s
Epoch 94/1000, LR 0.000265
Train loss: 1.0496;  Loss pred: 1.0496; Loss self: 0.0000; time: 0.26s
Val loss: 0.6824 score: 0.8571 time: 0.08s
Test loss: 0.6789 score: 0.9388 time: 0.09s
Epoch 95/1000, LR 0.000265
Train loss: 1.0453;  Loss pred: 1.0453; Loss self: 0.0000; time: 0.26s
Val loss: 0.6819 score: 0.8571 time: 0.07s
Test loss: 0.6783 score: 0.9388 time: 0.09s
Epoch 96/1000, LR 0.000265
Train loss: 1.0451;  Loss pred: 1.0451; Loss self: 0.0000; time: 0.26s
Val loss: 0.6814 score: 0.8571 time: 0.08s
Test loss: 0.6777 score: 0.9388 time: 0.09s
Epoch 97/1000, LR 0.000265
Train loss: 1.0395;  Loss pred: 1.0395; Loss self: 0.0000; time: 0.26s
Val loss: 0.6809 score: 0.8571 time: 0.84s
Test loss: 0.6770 score: 0.9388 time: 1.38s
Epoch 98/1000, LR 0.000265
Train loss: 1.0416;  Loss pred: 1.0416; Loss self: 0.0000; time: 1.82s
Val loss: 0.6804 score: 0.8571 time: 1.51s
Test loss: 0.6763 score: 0.9388 time: 1.98s
Epoch 99/1000, LR 0.000265
Train loss: 1.0387;  Loss pred: 1.0387; Loss self: 0.0000; time: 1.66s
Val loss: 0.6799 score: 0.8571 time: 0.07s
Test loss: 0.6756 score: 0.9388 time: 0.08s
Epoch 100/1000, LR 0.000265
Train loss: 1.0395;  Loss pred: 1.0395; Loss self: 0.0000; time: 0.25s
Val loss: 0.6793 score: 0.8571 time: 0.07s
Test loss: 0.6748 score: 0.9592 time: 0.08s
Epoch 101/1000, LR 0.000265
Train loss: 1.0329;  Loss pred: 1.0329; Loss self: 0.0000; time: 0.24s
Val loss: 0.6787 score: 0.8571 time: 0.08s
Test loss: 0.6740 score: 0.9592 time: 0.09s
Epoch 102/1000, LR 0.000264
Train loss: 1.0280;  Loss pred: 1.0280; Loss self: 0.0000; time: 0.24s
Val loss: 0.6781 score: 0.8571 time: 0.07s
Test loss: 0.6732 score: 0.9592 time: 0.08s
Epoch 103/1000, LR 0.000264
Train loss: 1.0285;  Loss pred: 1.0285; Loss self: 0.0000; time: 0.25s
Val loss: 0.6774 score: 0.8571 time: 0.07s
Test loss: 0.6724 score: 0.9592 time: 0.09s
Epoch 104/1000, LR 0.000264
Train loss: 1.0254;  Loss pred: 1.0254; Loss self: 0.0000; time: 0.24s
Val loss: 0.6768 score: 0.8571 time: 0.07s
Test loss: 0.6715 score: 0.9592 time: 0.08s
Epoch 105/1000, LR 0.000264
Train loss: 1.0208;  Loss pred: 1.0208; Loss self: 0.0000; time: 0.24s
Val loss: 0.6760 score: 0.8776 time: 0.07s
Test loss: 0.6705 score: 0.9592 time: 0.08s
Epoch 106/1000, LR 0.000264
Train loss: 1.0234;  Loss pred: 1.0234; Loss self: 0.0000; time: 0.24s
Val loss: 0.6753 score: 0.8776 time: 0.07s
Test loss: 0.6695 score: 0.9592 time: 0.08s
Epoch 107/1000, LR 0.000264
Train loss: 1.0203;  Loss pred: 1.0203; Loss self: 0.0000; time: 0.24s
Val loss: 0.6744 score: 0.8980 time: 0.07s
Test loss: 0.6684 score: 0.9592 time: 0.08s
Epoch 108/1000, LR 0.000264
Train loss: 1.0166;  Loss pred: 1.0166; Loss self: 0.0000; time: 0.24s
Val loss: 0.6736 score: 0.8980 time: 0.07s
Test loss: 0.6673 score: 0.9592 time: 0.09s
Epoch 109/1000, LR 0.000264
Train loss: 1.0150;  Loss pred: 1.0150; Loss self: 0.0000; time: 3.86s
Val loss: 0.6727 score: 0.8980 time: 0.20s
Test loss: 0.6662 score: 0.9592 time: 0.42s
Epoch 110/1000, LR 0.000263
Train loss: 1.0131;  Loss pred: 1.0131; Loss self: 0.0000; time: 1.17s
Val loss: 0.6718 score: 0.8980 time: 0.07s
Test loss: 0.6650 score: 0.9592 time: 0.08s
Epoch 111/1000, LR 0.000263
Train loss: 1.0120;  Loss pred: 1.0120; Loss self: 0.0000; time: 0.25s
Val loss: 0.6708 score: 0.8980 time: 0.07s
Test loss: 0.6638 score: 0.9592 time: 0.08s
Epoch 112/1000, LR 0.000263
Train loss: 1.0073;  Loss pred: 1.0073; Loss self: 0.0000; time: 0.24s
Val loss: 0.6699 score: 0.8980 time: 0.07s
Test loss: 0.6625 score: 0.9592 time: 0.08s
Epoch 113/1000, LR 0.000263
Train loss: 1.0064;  Loss pred: 1.0064; Loss self: 0.0000; time: 0.25s
Val loss: 0.6689 score: 0.8980 time: 0.07s
Test loss: 0.6612 score: 0.9796 time: 0.08s
Epoch 114/1000, LR 0.000263
Train loss: 1.0039;  Loss pred: 1.0039; Loss self: 0.0000; time: 0.25s
Val loss: 0.6680 score: 0.8980 time: 0.07s
Test loss: 0.6600 score: 0.9796 time: 0.08s
Epoch 115/1000, LR 0.000263
Train loss: 1.0026;  Loss pred: 1.0026; Loss self: 0.0000; time: 0.25s
Val loss: 0.6670 score: 0.8980 time: 0.07s
Test loss: 0.6587 score: 0.9796 time: 0.09s
Epoch 116/1000, LR 0.000263
Train loss: 1.0000;  Loss pred: 1.0000; Loss self: 0.0000; time: 3.19s
Val loss: 0.6660 score: 0.8980 time: 0.33s
Test loss: 0.6574 score: 0.9796 time: 1.34s
Epoch 117/1000, LR 0.000262
Train loss: 0.9977;  Loss pred: 0.9977; Loss self: 0.0000; time: 0.63s
Val loss: 0.6649 score: 0.8980 time: 0.14s
Test loss: 0.6561 score: 0.9796 time: 0.14s
Epoch 118/1000, LR 0.000262
Train loss: 0.9955;  Loss pred: 0.9955; Loss self: 0.0000; time: 0.25s
Val loss: 0.6638 score: 0.8980 time: 0.07s
Test loss: 0.6547 score: 0.9796 time: 0.08s
Epoch 119/1000, LR 0.000262
Train loss: 0.9951;  Loss pred: 0.9951; Loss self: 0.0000; time: 0.23s
Val loss: 0.6627 score: 0.8980 time: 0.07s
Test loss: 0.6532 score: 0.9796 time: 0.08s
Epoch 120/1000, LR 0.000262
Train loss: 0.9905;  Loss pred: 0.9905; Loss self: 0.0000; time: 0.23s
Val loss: 0.6616 score: 0.8980 time: 0.07s
Test loss: 0.6517 score: 0.9796 time: 0.08s
Epoch 121/1000, LR 0.000262
Train loss: 0.9897;  Loss pred: 0.9897; Loss self: 0.0000; time: 0.23s
Val loss: 0.6604 score: 0.8980 time: 0.07s
Test loss: 0.6502 score: 0.9796 time: 0.08s
Epoch 122/1000, LR 0.000262
Train loss: 0.9897;  Loss pred: 0.9897; Loss self: 0.0000; time: 0.23s
Val loss: 0.6592 score: 0.8980 time: 0.07s
Test loss: 0.6486 score: 0.9796 time: 0.08s
Epoch 123/1000, LR 0.000262
Train loss: 0.9835;  Loss pred: 0.9835; Loss self: 0.0000; time: 0.23s
Val loss: 0.6580 score: 0.8980 time: 0.07s
Test loss: 0.6470 score: 0.9796 time: 0.08s
Epoch 124/1000, LR 0.000261
Train loss: 0.9814;  Loss pred: 0.9814; Loss self: 0.0000; time: 0.23s
Val loss: 0.6567 score: 0.8980 time: 0.07s
Test loss: 0.6454 score: 0.9796 time: 0.08s
Epoch 125/1000, LR 0.000261
Train loss: 0.9810;  Loss pred: 0.9810; Loss self: 0.0000; time: 5.56s
Val loss: 0.6554 score: 0.8980 time: 0.99s
Test loss: 0.6437 score: 0.9796 time: 1.76s
Epoch 126/1000, LR 0.000261
Train loss: 0.9787;  Loss pred: 0.9787; Loss self: 0.0000; time: 5.22s
Val loss: 0.6540 score: 0.8980 time: 0.81s
Test loss: 0.6419 score: 0.9796 time: 0.10s
Epoch 127/1000, LR 0.000261
Train loss: 0.9756;  Loss pred: 0.9756; Loss self: 0.0000; time: 0.25s
Val loss: 0.6527 score: 0.8980 time: 0.07s
Test loss: 0.6401 score: 0.9796 time: 0.08s
Epoch 128/1000, LR 0.000261
Train loss: 0.9763;  Loss pred: 0.9763; Loss self: 0.0000; time: 0.24s
Val loss: 0.6512 score: 0.8980 time: 0.07s
Test loss: 0.6383 score: 0.9796 time: 0.08s
Epoch 129/1000, LR 0.000261
Train loss: 0.9719;  Loss pred: 0.9719; Loss self: 0.0000; time: 0.24s
Val loss: 0.6498 score: 0.8980 time: 0.09s
Test loss: 0.6364 score: 0.9796 time: 0.09s
Epoch 130/1000, LR 0.000260
Train loss: 0.9702;  Loss pred: 0.9702; Loss self: 0.0000; time: 0.24s
Val loss: 0.6483 score: 0.8980 time: 0.07s
Test loss: 0.6344 score: 0.9796 time: 0.08s
Epoch 131/1000, LR 0.000260
Train loss: 0.9662;  Loss pred: 0.9662; Loss self: 0.0000; time: 0.25s
Val loss: 0.6468 score: 0.8980 time: 0.07s
Test loss: 0.6324 score: 0.9796 time: 0.08s
Epoch 132/1000, LR 0.000260
Train loss: 0.9649;  Loss pred: 0.9649; Loss self: 0.0000; time: 0.24s
Val loss: 0.6452 score: 0.8980 time: 0.07s
Test loss: 0.6304 score: 0.9796 time: 0.09s
Epoch 133/1000, LR 0.000260
Train loss: 0.9608;  Loss pred: 0.9608; Loss self: 0.0000; time: 0.25s
Val loss: 0.6436 score: 0.8980 time: 0.07s
Test loss: 0.6283 score: 0.9796 time: 1.01s
Epoch 134/1000, LR 0.000260
Train loss: 0.9604;  Loss pred: 0.9604; Loss self: 0.0000; time: 2.75s
Val loss: 0.6420 score: 0.8980 time: 1.11s
Test loss: 0.6262 score: 0.9796 time: 0.35s
Epoch 135/1000, LR 0.000260
Train loss: 0.9584;  Loss pred: 0.9584; Loss self: 0.0000; time: 1.48s
Val loss: 0.6403 score: 0.8980 time: 0.75s
Test loss: 0.6240 score: 0.9796 time: 0.31s
Epoch 136/1000, LR 0.000260
Train loss: 0.9573;  Loss pred: 0.9573; Loss self: 0.0000; time: 1.06s
Val loss: 0.6386 score: 0.8980 time: 0.21s
Test loss: 0.6217 score: 0.9796 time: 0.27s
Epoch 137/1000, LR 0.000259
Train loss: 0.9524;  Loss pred: 0.9524; Loss self: 0.0000; time: 0.65s
Val loss: 0.6368 score: 0.8980 time: 0.07s
Test loss: 0.6194 score: 0.9796 time: 0.08s
Epoch 138/1000, LR 0.000259
Train loss: 0.9507;  Loss pred: 0.9507; Loss self: 0.0000; time: 0.24s
Val loss: 0.6350 score: 0.8980 time: 0.07s
Test loss: 0.6171 score: 0.9796 time: 0.08s
Epoch 139/1000, LR 0.000259
Train loss: 0.9489;  Loss pred: 0.9489; Loss self: 0.0000; time: 0.23s
Val loss: 0.6332 score: 0.8980 time: 0.07s
Test loss: 0.6146 score: 1.0000 time: 0.17s
Epoch 140/1000, LR 0.000259
Train loss: 0.9467;  Loss pred: 0.9467; Loss self: 0.0000; time: 0.23s
Val loss: 0.6313 score: 0.8980 time: 0.07s
Test loss: 0.6122 score: 1.0000 time: 0.08s
Epoch 141/1000, LR 0.000259
Train loss: 0.9444;  Loss pred: 0.9444; Loss self: 0.0000; time: 0.24s
Val loss: 0.6294 score: 0.8980 time: 0.07s
Test loss: 0.6097 score: 1.0000 time: 0.08s
Epoch 142/1000, LR 0.000259
Train loss: 0.9427;  Loss pred: 0.9427; Loss self: 0.0000; time: 0.23s
Val loss: 0.6275 score: 0.8980 time: 0.07s
Test loss: 0.6071 score: 0.9796 time: 0.08s
Epoch 143/1000, LR 0.000258
Train loss: 0.9405;  Loss pred: 0.9405; Loss self: 0.0000; time: 0.23s
Val loss: 0.6255 score: 0.8980 time: 0.07s
Test loss: 0.6045 score: 0.9796 time: 0.08s
Epoch 144/1000, LR 0.000258
Train loss: 0.9370;  Loss pred: 0.9370; Loss self: 0.0000; time: 0.26s
Val loss: 0.6235 score: 0.8980 time: 1.43s
Test loss: 0.6018 score: 0.9796 time: 1.07s
Epoch 145/1000, LR 0.000258
Train loss: 0.9363;  Loss pred: 0.9363; Loss self: 0.0000; time: 0.96s
Val loss: 0.6215 score: 0.8980 time: 0.07s
Test loss: 0.5991 score: 0.9796 time: 0.09s
Epoch 146/1000, LR 0.000258
Train loss: 0.9333;  Loss pred: 0.9333; Loss self: 0.0000; time: 0.25s
Val loss: 0.6194 score: 0.8980 time: 0.07s
Test loss: 0.5964 score: 0.9796 time: 0.08s
Epoch 147/1000, LR 0.000258
Train loss: 0.9295;  Loss pred: 0.9295; Loss self: 0.0000; time: 0.24s
Val loss: 0.6173 score: 0.8980 time: 0.07s
Test loss: 0.5935 score: 0.9796 time: 0.08s
Epoch 148/1000, LR 0.000257
Train loss: 0.9270;  Loss pred: 0.9270; Loss self: 0.0000; time: 0.24s
Val loss: 0.6152 score: 0.8980 time: 0.07s
Test loss: 0.5907 score: 0.9796 time: 0.08s
Epoch 149/1000, LR 0.000257
Train loss: 0.9235;  Loss pred: 0.9235; Loss self: 0.0000; time: 0.24s
Val loss: 0.6130 score: 0.8980 time: 0.07s
Test loss: 0.5878 score: 0.9796 time: 0.08s
Epoch 150/1000, LR 0.000257
Train loss: 0.9209;  Loss pred: 0.9209; Loss self: 0.0000; time: 0.23s
Val loss: 0.6107 score: 0.8980 time: 0.07s
Test loss: 0.5849 score: 0.9796 time: 0.08s
Epoch 151/1000, LR 0.000257
Train loss: 0.9207;  Loss pred: 0.9207; Loss self: 0.0000; time: 0.23s
Val loss: 0.6085 score: 0.8980 time: 0.07s
Test loss: 0.5819 score: 0.9796 time: 0.08s
Epoch 152/1000, LR 0.000257
Train loss: 0.9171;  Loss pred: 0.9171; Loss self: 0.0000; time: 3.03s
Val loss: 0.6062 score: 0.8980 time: 0.37s
Test loss: 0.5788 score: 0.9796 time: 0.90s
Epoch 153/1000, LR 0.000257
Train loss: 0.9137;  Loss pred: 0.9137; Loss self: 0.0000; time: 3.77s
Val loss: 0.6038 score: 0.8980 time: 0.30s
Test loss: 0.5757 score: 0.9796 time: 0.13s
Epoch 154/1000, LR 0.000256
Train loss: 0.9128;  Loss pred: 0.9128; Loss self: 0.0000; time: 0.43s
Val loss: 0.6015 score: 0.8980 time: 0.07s
Test loss: 0.5726 score: 0.9796 time: 0.08s
Epoch 155/1000, LR 0.000256
Train loss: 0.9087;  Loss pred: 0.9087; Loss self: 0.0000; time: 0.24s
Val loss: 0.5991 score: 0.8980 time: 0.07s
Test loss: 0.5694 score: 0.9796 time: 0.08s
Epoch 156/1000, LR 0.000256
Train loss: 0.9069;  Loss pred: 0.9069; Loss self: 0.0000; time: 0.23s
Val loss: 0.5968 score: 0.8980 time: 0.06s
Test loss: 0.5662 score: 0.9796 time: 0.08s
Epoch 157/1000, LR 0.000256
Train loss: 0.9030;  Loss pred: 0.9030; Loss self: 0.0000; time: 0.24s
Val loss: 0.5943 score: 0.8980 time: 0.07s
Test loss: 0.5629 score: 0.9796 time: 0.08s
Epoch 158/1000, LR 0.000256
Train loss: 0.9004;  Loss pred: 0.9004; Loss self: 0.0000; time: 0.23s
Val loss: 0.5919 score: 0.8980 time: 0.07s
Test loss: 0.5596 score: 0.9796 time: 0.08s
Epoch 159/1000, LR 0.000255
Train loss: 0.8951;  Loss pred: 0.8951; Loss self: 0.0000; time: 0.23s
Val loss: 0.5894 score: 0.8980 time: 0.07s
Test loss: 0.5562 score: 0.9796 time: 0.08s
Epoch 160/1000, LR 0.000255
Train loss: 0.8943;  Loss pred: 0.8943; Loss self: 0.0000; time: 0.23s
Val loss: 0.5869 score: 0.8980 time: 0.07s
Test loss: 0.5528 score: 0.9796 time: 0.08s
Epoch 161/1000, LR 0.000255
Train loss: 0.8914;  Loss pred: 0.8914; Loss self: 0.0000; time: 0.23s
Val loss: 0.5844 score: 0.8980 time: 0.07s
Test loss: 0.5493 score: 0.9796 time: 0.08s
Epoch 162/1000, LR 0.000255
Train loss: 0.8888;  Loss pred: 0.8888; Loss self: 0.0000; time: 3.51s
Val loss: 0.5818 score: 0.8980 time: 0.37s
Test loss: 0.5459 score: 0.9796 time: 0.37s
Epoch 163/1000, LR 0.000255
Train loss: 0.8858;  Loss pred: 0.8858; Loss self: 0.0000; time: 1.21s
Val loss: 0.5792 score: 0.8980 time: 1.48s
Test loss: 0.5423 score: 0.9796 time: 1.76s
Epoch 164/1000, LR 0.000254
Train loss: 0.8837;  Loss pred: 0.8837; Loss self: 0.0000; time: 5.19s
Val loss: 0.5765 score: 0.8980 time: 0.85s
Test loss: 0.5388 score: 0.9796 time: 0.63s
Epoch 165/1000, LR 0.000254
Train loss: 0.8811;  Loss pred: 0.8811; Loss self: 0.0000; time: 1.35s
Val loss: 0.5739 score: 0.8980 time: 0.07s
Test loss: 0.5352 score: 0.9796 time: 0.09s
Epoch 166/1000, LR 0.000254
Train loss: 0.8765;  Loss pred: 0.8765; Loss self: 0.0000; time: 0.25s
Val loss: 0.5712 score: 0.8980 time: 0.07s
Test loss: 0.5316 score: 0.9796 time: 0.08s
Epoch 167/1000, LR 0.000254
Train loss: 0.8735;  Loss pred: 0.8735; Loss self: 0.0000; time: 0.24s
Val loss: 0.5685 score: 0.8980 time: 0.07s
Test loss: 0.5279 score: 0.9796 time: 0.08s
Epoch 168/1000, LR 0.000254
Train loss: 0.8699;  Loss pred: 0.8699; Loss self: 0.0000; time: 0.25s
Val loss: 0.5657 score: 0.8980 time: 0.07s
Test loss: 0.5242 score: 0.9796 time: 0.08s
Epoch 169/1000, LR 0.000253
Train loss: 0.8695;  Loss pred: 0.8695; Loss self: 0.0000; time: 0.24s
Val loss: 0.5630 score: 0.8980 time: 0.07s
Test loss: 0.5205 score: 0.9796 time: 0.08s
Epoch 170/1000, LR 0.000253
Train loss: 0.8648;  Loss pred: 0.8648; Loss self: 0.0000; time: 0.24s
Val loss: 0.5602 score: 0.8980 time: 0.07s
Test loss: 0.5167 score: 0.9796 time: 0.08s
Epoch 171/1000, LR 0.000253
Train loss: 0.8614;  Loss pred: 0.8614; Loss self: 0.0000; time: 0.25s
Val loss: 0.5574 score: 0.8980 time: 0.07s
Test loss: 0.5129 score: 0.9796 time: 0.08s
Epoch 172/1000, LR 0.000253
Train loss: 0.8595;  Loss pred: 0.8595; Loss self: 0.0000; time: 1.15s
Val loss: 0.5546 score: 0.8980 time: 0.35s
Test loss: 0.5091 score: 0.9796 time: 0.66s
Epoch 173/1000, LR 0.000253
Train loss: 0.8583;  Loss pred: 0.8583; Loss self: 0.0000; time: 2.21s
Val loss: 0.5518 score: 0.8980 time: 1.13s
Test loss: 0.5053 score: 0.9796 time: 0.63s
Epoch 174/1000, LR 0.000252
Train loss: 0.8549;  Loss pred: 0.8549; Loss self: 0.0000; time: 0.85s
Val loss: 0.5489 score: 0.8980 time: 0.11s
Test loss: 0.5014 score: 0.9796 time: 0.16s
Epoch 175/1000, LR 0.000252
Train loss: 0.8512;  Loss pred: 0.8512; Loss self: 0.0000; time: 0.40s
Val loss: 0.5460 score: 0.8980 time: 0.10s
Test loss: 0.4975 score: 0.9796 time: 0.08s
Epoch 176/1000, LR 0.000252
Train loss: 0.8468;  Loss pred: 0.8468; Loss self: 0.0000; time: 0.24s
Val loss: 0.5432 score: 0.8980 time: 0.06s
Test loss: 0.4935 score: 0.9796 time: 0.08s
Epoch 177/1000, LR 0.000252
Train loss: 0.8440;  Loss pred: 0.8440; Loss self: 0.0000; time: 0.23s
Val loss: 0.5403 score: 0.8980 time: 0.06s
Test loss: 0.4896 score: 0.9796 time: 0.08s
Epoch 178/1000, LR 0.000251
Train loss: 0.8431;  Loss pred: 0.8431; Loss self: 0.0000; time: 0.23s
Val loss: 0.5373 score: 0.8980 time: 0.06s
Test loss: 0.4856 score: 0.9796 time: 0.08s
Epoch 179/1000, LR 0.000251
Train loss: 0.8378;  Loss pred: 0.8378; Loss self: 0.0000; time: 0.23s
Val loss: 0.5344 score: 0.8980 time: 0.06s
Test loss: 0.4816 score: 0.9796 time: 0.08s
Epoch 180/1000, LR 0.000251
Train loss: 0.8352;  Loss pred: 0.8352; Loss self: 0.0000; time: 0.23s
Val loss: 0.5316 score: 0.8980 time: 0.06s
Test loss: 0.4776 score: 0.9796 time: 0.08s
Epoch 181/1000, LR 0.000251
Train loss: 0.8321;  Loss pred: 0.8321; Loss self: 0.0000; time: 0.23s
Val loss: 0.5287 score: 0.8980 time: 0.06s
Test loss: 0.4735 score: 0.9796 time: 0.08s
Epoch 182/1000, LR 0.000251
Train loss: 0.8288;  Loss pred: 0.8288; Loss self: 0.0000; time: 0.23s
Val loss: 0.5258 score: 0.8776 time: 0.06s
Test loss: 0.4695 score: 0.9796 time: 0.08s
Epoch 183/1000, LR 0.000250
Train loss: 0.8245;  Loss pred: 0.8245; Loss self: 0.0000; time: 0.23s
Val loss: 0.5229 score: 0.8776 time: 0.06s
Test loss: 0.4654 score: 0.9796 time: 0.08s
Epoch 184/1000, LR 0.000250
Train loss: 0.8230;  Loss pred: 0.8230; Loss self: 0.0000; time: 0.23s
Val loss: 0.5201 score: 0.8776 time: 0.06s
Test loss: 0.4614 score: 0.9796 time: 0.08s
Epoch 185/1000, LR 0.000250
Train loss: 0.8194;  Loss pred: 0.8194; Loss self: 0.0000; time: 2.46s
Val loss: 0.5172 score: 0.8776 time: 0.67s
Test loss: 0.4573 score: 0.9796 time: 1.75s
Epoch 186/1000, LR 0.000250
Train loss: 0.8169;  Loss pred: 0.8169; Loss self: 0.0000; time: 2.36s
Val loss: 0.5144 score: 0.8776 time: 0.22s
Test loss: 0.4532 score: 0.9796 time: 0.82s
Epoch 187/1000, LR 0.000249
Train loss: 0.8139;  Loss pred: 0.8139; Loss self: 0.0000; time: 2.48s
Val loss: 0.5116 score: 0.8776 time: 0.07s
Test loss: 0.4492 score: 0.9796 time: 0.09s
Epoch 188/1000, LR 0.000249
Train loss: 0.8101;  Loss pred: 0.8101; Loss self: 0.0000; time: 0.25s
Val loss: 0.5088 score: 0.8776 time: 0.07s
Test loss: 0.4451 score: 0.9796 time: 0.08s
Epoch 189/1000, LR 0.000249
Train loss: 0.8062;  Loss pred: 0.8062; Loss self: 0.0000; time: 0.25s
Val loss: 0.5060 score: 0.8776 time: 0.07s
Test loss: 0.4410 score: 0.9796 time: 0.08s
Epoch 190/1000, LR 0.000249
Train loss: 0.8026;  Loss pred: 0.8026; Loss self: 0.0000; time: 0.23s
Val loss: 0.5032 score: 0.8776 time: 0.07s
Test loss: 0.4369 score: 0.9796 time: 0.08s
Epoch 191/1000, LR 0.000249
Train loss: 0.7998;  Loss pred: 0.7998; Loss self: 0.0000; time: 0.24s
Val loss: 0.5004 score: 0.8776 time: 0.07s
Test loss: 0.4328 score: 0.9796 time: 0.08s
Epoch 192/1000, LR 0.000248
Train loss: 0.7969;  Loss pred: 0.7969; Loss self: 0.0000; time: 0.23s
Val loss: 0.4975 score: 0.8776 time: 0.06s
Test loss: 0.4288 score: 0.9796 time: 0.08s
Epoch 193/1000, LR 0.000248
Train loss: 0.7950;  Loss pred: 0.7950; Loss self: 0.0000; time: 0.23s
Val loss: 0.4946 score: 0.8776 time: 0.07s
Test loss: 0.4247 score: 0.9796 time: 0.08s
Epoch 194/1000, LR 0.000248
Train loss: 0.7907;  Loss pred: 0.7907; Loss self: 0.0000; time: 0.23s
Val loss: 0.4917 score: 0.8776 time: 0.06s
Test loss: 0.4206 score: 0.9796 time: 0.08s
Epoch 195/1000, LR 0.000248
Train loss: 0.7886;  Loss pred: 0.7886; Loss self: 0.0000; time: 0.23s
Val loss: 0.4889 score: 0.8776 time: 0.06s
Test loss: 0.4166 score: 0.9796 time: 0.08s
Epoch 196/1000, LR 0.000247
Train loss: 0.7843;  Loss pred: 0.7843; Loss self: 0.0000; time: 0.23s
Val loss: 0.4861 score: 0.8776 time: 0.07s
Test loss: 0.4125 score: 0.9796 time: 0.08s
Epoch 197/1000, LR 0.000247
Train loss: 0.7814;  Loss pred: 0.7814; Loss self: 0.0000; time: 0.23s
Val loss: 0.4834 score: 0.8571 time: 0.06s
Test loss: 0.4085 score: 0.9796 time: 0.08s
Epoch 198/1000, LR 0.000247
Train loss: 0.7785;  Loss pred: 0.7785; Loss self: 0.0000; time: 1.35s
Val loss: 0.4807 score: 0.8571 time: 0.67s
Test loss: 0.4044 score: 0.9796 time: 0.66s
Epoch 199/1000, LR 0.000247
Train loss: 0.7775;  Loss pred: 0.7775; Loss self: 0.0000; time: 3.77s
Val loss: 0.4781 score: 0.8571 time: 1.05s
Test loss: 0.4004 score: 0.9796 time: 0.84s
Epoch 200/1000, LR 0.000246
Train loss: 0.7708;  Loss pred: 0.7708; Loss self: 0.0000; time: 1.10s
Val loss: 0.4755 score: 0.8571 time: 0.33s
Test loss: 0.3964 score: 0.9796 time: 0.44s
Epoch 201/1000, LR 0.000246
Train loss: 0.7691;  Loss pred: 0.7691; Loss self: 0.0000; time: 1.01s
Val loss: 0.4729 score: 0.8571 time: 0.08s
Test loss: 0.3925 score: 0.9796 time: 0.10s
Epoch 202/1000, LR 0.000246
Train loss: 0.7656;  Loss pred: 0.7656; Loss self: 0.0000; time: 0.24s
Val loss: 0.4702 score: 0.8571 time: 0.07s
Test loss: 0.3885 score: 0.9796 time: 0.08s
Epoch 203/1000, LR 0.000246
Train loss: 0.7638;  Loss pred: 0.7638; Loss self: 0.0000; time: 0.23s
Val loss: 0.4676 score: 0.8571 time: 0.07s
Test loss: 0.3845 score: 0.9796 time: 0.08s
Epoch 204/1000, LR 0.000245
Train loss: 0.7620;  Loss pred: 0.7620; Loss self: 0.0000; time: 0.23s
Val loss: 0.4650 score: 0.8571 time: 0.06s
Test loss: 0.3806 score: 0.9796 time: 0.08s
Epoch 205/1000, LR 0.000245
Train loss: 0.7590;  Loss pred: 0.7590; Loss self: 0.0000; time: 0.23s
Val loss: 0.4623 score: 0.8571 time: 0.07s
Test loss: 0.3767 score: 0.9796 time: 0.08s
Epoch 206/1000, LR 0.000245
Train loss: 0.7546;  Loss pred: 0.7546; Loss self: 0.0000; time: 0.23s
Val loss: 0.4596 score: 0.8571 time: 0.07s
Test loss: 0.3727 score: 0.9796 time: 0.08s
Epoch 207/1000, LR 0.000245
Train loss: 0.7534;  Loss pred: 0.7534; Loss self: 0.0000; time: 0.23s
Val loss: 0.4569 score: 0.8571 time: 0.07s
Test loss: 0.3688 score: 0.9796 time: 0.08s
Epoch 208/1000, LR 0.000244
Train loss: 0.7508;  Loss pred: 0.7508; Loss self: 0.0000; time: 0.23s
Val loss: 0.4543 score: 0.8571 time: 0.07s
Test loss: 0.3650 score: 0.9796 time: 0.08s
Epoch 209/1000, LR 0.000244
Train loss: 0.7465;  Loss pred: 0.7465; Loss self: 0.0000; time: 0.23s
Val loss: 0.4516 score: 0.8571 time: 0.07s
Test loss: 0.3611 score: 0.9796 time: 0.08s
Epoch 210/1000, LR 0.000244
Train loss: 0.7459;  Loss pred: 0.7459; Loss self: 0.0000; time: 0.23s
Val loss: 0.4491 score: 0.8571 time: 0.07s
Test loss: 0.3573 score: 0.9796 time: 0.08s
Epoch 211/1000, LR 0.000244
Train loss: 0.7418;  Loss pred: 0.7418; Loss self: 0.0000; time: 0.24s
Val loss: 0.4467 score: 0.8571 time: 0.07s
Test loss: 0.3536 score: 0.9796 time: 0.08s
Epoch 212/1000, LR 0.000243
Train loss: 0.7391;  Loss pred: 0.7391; Loss self: 0.0000; time: 5.01s
Val loss: 0.4444 score: 0.8571 time: 0.43s
Test loss: 0.3498 score: 0.9796 time: 0.48s
Epoch 213/1000, LR 0.000243
Train loss: 0.7372;  Loss pred: 0.7372; Loss self: 0.0000; time: 0.30s
Val loss: 0.4422 score: 0.8571 time: 0.07s
Test loss: 0.3461 score: 0.9796 time: 0.08s
Epoch 214/1000, LR 0.000243
Train loss: 0.7326;  Loss pred: 0.7326; Loss self: 0.0000; time: 0.23s
Val loss: 0.4400 score: 0.8571 time: 0.07s
Test loss: 0.3425 score: 0.9796 time: 0.08s
Epoch 215/1000, LR 0.000243
Train loss: 0.7294;  Loss pred: 0.7294; Loss self: 0.0000; time: 0.23s
Val loss: 0.4379 score: 0.8571 time: 0.06s
Test loss: 0.3389 score: 0.9796 time: 0.08s
Epoch 216/1000, LR 0.000242
Train loss: 0.7278;  Loss pred: 0.7278; Loss self: 0.0000; time: 0.23s
Val loss: 0.4358 score: 0.8571 time: 0.06s
Test loss: 0.3353 score: 0.9796 time: 0.08s
Epoch 217/1000, LR 0.000242
Train loss: 0.7263;  Loss pred: 0.7263; Loss self: 0.0000; time: 0.23s
Val loss: 0.4337 score: 0.8571 time: 0.06s
Test loss: 0.3317 score: 0.9796 time: 0.08s
Epoch 218/1000, LR 0.000242
Train loss: 0.7236;  Loss pred: 0.7236; Loss self: 0.0000; time: 0.23s
Val loss: 0.4315 score: 0.8571 time: 0.06s
Test loss: 0.3282 score: 0.9796 time: 0.08s
Epoch 219/1000, LR 0.000242
Train loss: 0.7197;  Loss pred: 0.7197; Loss self: 0.0000; time: 0.23s
Val loss: 0.4294 score: 0.8571 time: 0.06s
Test loss: 0.3247 score: 0.9796 time: 0.08s
Epoch 220/1000, LR 0.000241
Train loss: 0.7158;  Loss pred: 0.7158; Loss self: 0.0000; time: 0.23s
Val loss: 0.4271 score: 0.8571 time: 0.06s
Test loss: 0.3212 score: 0.9796 time: 0.08s
Epoch 221/1000, LR 0.000241
Train loss: 0.7150;  Loss pred: 0.7150; Loss self: 0.0000; time: 0.22s
Val loss: 0.4250 score: 0.8571 time: 0.06s
Test loss: 0.3177 score: 0.9796 time: 0.08s
Epoch 222/1000, LR 0.000241
Train loss: 0.7116;  Loss pred: 0.7116; Loss self: 0.0000; time: 0.22s
Val loss: 0.4228 score: 0.8571 time: 0.07s
Test loss: 0.3143 score: 0.9796 time: 0.08s
Epoch 223/1000, LR 0.000241
Train loss: 0.7104;  Loss pred: 0.7104; Loss self: 0.0000; time: 0.23s
Val loss: 0.4207 score: 0.8571 time: 0.06s
Test loss: 0.3109 score: 0.9796 time: 0.08s
Epoch 224/1000, LR 0.000240
Train loss: 0.7084;  Loss pred: 0.7084; Loss self: 0.0000; time: 0.23s
Val loss: 0.4186 score: 0.8571 time: 0.06s
Test loss: 0.3075 score: 0.9796 time: 0.08s
Epoch 225/1000, LR 0.000240
Train loss: 0.7057;  Loss pred: 0.7057; Loss self: 0.0000; time: 0.23s
Val loss: 0.4165 score: 0.8571 time: 0.06s
Test loss: 0.3042 score: 0.9796 time: 0.08s
Epoch 226/1000, LR 0.000240
Train loss: 0.7023;  Loss pred: 0.7023; Loss self: 0.0000; time: 5.68s
Val loss: 0.4144 score: 0.8571 time: 0.17s
Test loss: 0.3009 score: 0.9796 time: 1.21s
Epoch 227/1000, LR 0.000240
Train loss: 0.7017;  Loss pred: 0.7017; Loss self: 0.0000; time: 0.29s
Val loss: 0.4125 score: 0.8571 time: 0.07s
Test loss: 0.2977 score: 0.9796 time: 0.08s
Epoch 228/1000, LR 0.000239
Train loss: 0.6989;  Loss pred: 0.6989; Loss self: 0.0000; time: 0.24s
Val loss: 0.4106 score: 0.8571 time: 0.07s
Test loss: 0.2945 score: 0.9796 time: 0.08s
Epoch 229/1000, LR 0.000239
Train loss: 0.6969;  Loss pred: 0.6969; Loss self: 0.0000; time: 0.23s
Val loss: 0.4088 score: 0.8571 time: 0.07s
Test loss: 0.2913 score: 0.9796 time: 0.08s
Epoch 230/1000, LR 0.000239
Train loss: 0.6960;  Loss pred: 0.6960; Loss self: 0.0000; time: 0.23s
Val loss: 0.4071 score: 0.8571 time: 0.07s
Test loss: 0.2882 score: 0.9796 time: 0.08s
Epoch 231/1000, LR 0.000238
Train loss: 0.6933;  Loss pred: 0.6933; Loss self: 0.0000; time: 0.23s
Val loss: 0.4055 score: 0.8571 time: 0.07s
Test loss: 0.2851 score: 0.9796 time: 0.08s
Epoch 232/1000, LR 0.000238
Train loss: 0.6904;  Loss pred: 0.6904; Loss self: 0.0000; time: 0.24s
Val loss: 0.4039 score: 0.8571 time: 0.07s
Test loss: 0.2821 score: 0.9796 time: 0.08s
Epoch 233/1000, LR 0.000238
Train loss: 0.6866;  Loss pred: 0.6866; Loss self: 0.0000; time: 0.23s
Val loss: 0.4024 score: 0.8571 time: 0.07s
Test loss: 0.2792 score: 0.9796 time: 0.08s
Epoch 234/1000, LR 0.000238
Train loss: 0.6853;  Loss pred: 0.6853; Loss self: 0.0000; time: 0.23s
Val loss: 0.4009 score: 0.8571 time: 0.06s
Test loss: 0.2763 score: 0.9796 time: 0.08s
Epoch 235/1000, LR 0.000237
Train loss: 0.6833;  Loss pred: 0.6833; Loss self: 0.0000; time: 0.23s
Val loss: 0.3993 score: 0.8571 time: 0.06s
Test loss: 0.2735 score: 0.9796 time: 0.08s
Epoch 236/1000, LR 0.000237
Train loss: 0.6812;  Loss pred: 0.6812; Loss self: 0.0000; time: 0.23s
Val loss: 0.3978 score: 0.8571 time: 0.07s
Test loss: 0.2707 score: 0.9796 time: 0.08s
Epoch 237/1000, LR 0.000237
Train loss: 0.6782;  Loss pred: 0.6782; Loss self: 0.0000; time: 0.24s
Val loss: 0.3962 score: 0.8571 time: 0.07s
Test loss: 0.2679 score: 0.9796 time: 0.08s
Epoch 238/1000, LR 0.000236
Train loss: 0.6754;  Loss pred: 0.6754; Loss self: 0.0000; time: 0.24s
Val loss: 0.3944 score: 0.8571 time: 0.07s
Test loss: 0.2651 score: 0.9796 time: 0.08s
Epoch 239/1000, LR 0.000236
Train loss: 0.6744;  Loss pred: 0.6744; Loss self: 0.0000; time: 0.24s
Val loss: 0.3927 score: 0.8571 time: 0.07s
Test loss: 0.2623 score: 0.9796 time: 1.68s
Epoch 240/1000, LR 0.000236
Train loss: 0.6723;  Loss pred: 0.6723; Loss self: 0.0000; time: 1.40s
Val loss: 0.3910 score: 0.8571 time: 0.60s
Test loss: 0.2596 score: 0.9796 time: 0.31s
Epoch 241/1000, LR 0.000236
Train loss: 0.6720;  Loss pred: 0.6720; Loss self: 0.0000; time: 1.57s
Val loss: 0.3893 score: 0.8571 time: 0.07s
Test loss: 0.2569 score: 0.9796 time: 0.08s
Epoch 242/1000, LR 0.000235
Train loss: 0.6690;  Loss pred: 0.6690; Loss self: 0.0000; time: 0.24s
Val loss: 0.3877 score: 0.8571 time: 0.07s
Test loss: 0.2542 score: 0.9796 time: 0.08s
Epoch 243/1000, LR 0.000235
Train loss: 0.6676;  Loss pred: 0.6676; Loss self: 0.0000; time: 0.24s
Val loss: 0.3862 score: 0.8571 time: 0.06s
Test loss: 0.2517 score: 0.9796 time: 0.08s
Epoch 244/1000, LR 0.000235
Train loss: 0.6663;  Loss pred: 0.6663; Loss self: 0.0000; time: 0.23s
Val loss: 0.3849 score: 0.8571 time: 0.06s
Test loss: 0.2492 score: 0.9796 time: 0.08s
Epoch 245/1000, LR 0.000234
Train loss: 0.6642;  Loss pred: 0.6642; Loss self: 0.0000; time: 0.22s
Val loss: 0.3836 score: 0.8571 time: 0.06s
Test loss: 0.2467 score: 0.9796 time: 0.08s
Epoch 246/1000, LR 0.000234
Train loss: 0.6607;  Loss pred: 0.6607; Loss self: 0.0000; time: 0.22s
Val loss: 0.3824 score: 0.8571 time: 0.06s
Test loss: 0.2443 score: 0.9796 time: 0.08s
Epoch 247/1000, LR 0.000234
Train loss: 0.6581;  Loss pred: 0.6581; Loss self: 0.0000; time: 0.23s
Val loss: 0.3811 score: 0.8571 time: 0.06s
Test loss: 0.2419 score: 0.9796 time: 0.08s
Epoch 248/1000, LR 0.000234
Train loss: 0.6572;  Loss pred: 0.6572; Loss self: 0.0000; time: 0.23s
Val loss: 0.3798 score: 0.8571 time: 0.06s
Test loss: 0.2395 score: 0.9796 time: 0.08s
Epoch 249/1000, LR 0.000233
Train loss: 0.6561;  Loss pred: 0.6561; Loss self: 0.0000; time: 0.23s
Val loss: 0.3786 score: 0.8571 time: 0.06s
Test loss: 0.2372 score: 0.9796 time: 0.08s
Epoch 250/1000, LR 0.000233
Train loss: 0.6548;  Loss pred: 0.6548; Loss self: 0.0000; time: 0.23s
Val loss: 0.3774 score: 0.8571 time: 0.06s
Test loss: 0.2349 score: 0.9796 time: 0.08s
Epoch 251/1000, LR 0.000233
Train loss: 0.6527;  Loss pred: 0.6527; Loss self: 0.0000; time: 0.25s
Val loss: 0.3761 score: 0.8571 time: 0.06s
Test loss: 0.2326 score: 0.9796 time: 0.08s
Epoch 252/1000, LR 0.000232
Train loss: 0.6516;  Loss pred: 0.6516; Loss self: 0.0000; time: 0.23s
Val loss: 0.3748 score: 0.8571 time: 0.07s
Test loss: 0.2304 score: 0.9796 time: 0.08s
Epoch 253/1000, LR 0.000232
Train loss: 0.6489;  Loss pred: 0.6489; Loss self: 0.0000; time: 4.21s
Val loss: 0.3736 score: 0.8571 time: 0.33s
Test loss: 0.2281 score: 0.9796 time: 1.55s
Epoch 254/1000, LR 0.000232
Train loss: 0.6493;  Loss pred: 0.6493; Loss self: 0.0000; time: 0.90s
Val loss: 0.3723 score: 0.8571 time: 0.07s
Test loss: 0.2260 score: 0.9796 time: 0.08s
Epoch 255/1000, LR 0.000232
Train loss: 0.6455;  Loss pred: 0.6455; Loss self: 0.0000; time: 0.24s
Val loss: 0.3712 score: 0.8571 time: 0.07s
Test loss: 0.2238 score: 0.9796 time: 0.08s
Epoch 256/1000, LR 0.000231
Train loss: 0.6448;  Loss pred: 0.6448; Loss self: 0.0000; time: 0.23s
Val loss: 0.3701 score: 0.8571 time: 0.06s
Test loss: 0.2217 score: 0.9796 time: 0.08s
Epoch 257/1000, LR 0.000231
Train loss: 0.6427;  Loss pred: 0.6427; Loss self: 0.0000; time: 0.22s
Val loss: 0.3689 score: 0.8571 time: 0.06s
Test loss: 0.2197 score: 0.9796 time: 0.08s
Epoch 258/1000, LR 0.000231
Train loss: 0.6410;  Loss pred: 0.6410; Loss self: 0.0000; time: 0.22s
Val loss: 0.3680 score: 0.8571 time: 0.06s
Test loss: 0.2177 score: 0.9796 time: 0.08s
Epoch 259/1000, LR 0.000230
Train loss: 0.6402;  Loss pred: 0.6402; Loss self: 0.0000; time: 0.22s
Val loss: 0.3671 score: 0.8571 time: 0.06s
Test loss: 0.2157 score: 0.9796 time: 0.08s
Epoch 260/1000, LR 0.000230
Train loss: 0.6375;  Loss pred: 0.6375; Loss self: 0.0000; time: 0.23s
Val loss: 0.3661 score: 0.8571 time: 0.06s
Test loss: 0.2138 score: 0.9796 time: 0.08s
Epoch 261/1000, LR 0.000230
Train loss: 0.6378;  Loss pred: 0.6378; Loss self: 0.0000; time: 0.22s
Val loss: 0.3653 score: 0.8571 time: 0.06s
Test loss: 0.2119 score: 0.9796 time: 0.08s
Epoch 262/1000, LR 0.000229
Train loss: 0.6359;  Loss pred: 0.6359; Loss self: 0.0000; time: 0.23s
Val loss: 0.3644 score: 0.8571 time: 0.06s
Test loss: 0.2101 score: 0.9796 time: 0.08s
Epoch 263/1000, LR 0.000229
Train loss: 0.6347;  Loss pred: 0.6347; Loss self: 0.0000; time: 0.23s
Val loss: 0.3636 score: 0.8571 time: 0.06s
Test loss: 0.2082 score: 0.9796 time: 0.08s
Epoch 264/1000, LR 0.000229
Train loss: 0.6325;  Loss pred: 0.6325; Loss self: 0.0000; time: 0.22s
Val loss: 0.3627 score: 0.8571 time: 0.06s
Test loss: 0.2064 score: 0.9796 time: 0.08s
Epoch 265/1000, LR 0.000228
Train loss: 0.6317;  Loss pred: 0.6317; Loss self: 0.0000; time: 0.24s
Val loss: 0.3620 score: 0.8571 time: 0.07s
Test loss: 0.2047 score: 0.9796 time: 0.08s
Epoch 266/1000, LR 0.000228
Train loss: 0.6302;  Loss pred: 0.6302; Loss self: 0.0000; time: 0.23s
Val loss: 0.3613 score: 0.8571 time: 0.07s
Test loss: 0.2030 score: 0.9796 time: 0.08s
Epoch 267/1000, LR 0.000228
Train loss: 0.6307;  Loss pred: 0.6307; Loss self: 0.0000; time: 0.24s
Val loss: 0.3605 score: 0.8571 time: 0.06s
Test loss: 0.2013 score: 0.9796 time: 0.08s
Epoch 268/1000, LR 0.000228
Train loss: 0.6274;  Loss pred: 0.6274; Loss self: 0.0000; time: 0.22s
Val loss: 0.3598 score: 0.8571 time: 0.06s
Test loss: 0.1996 score: 0.9796 time: 0.08s
Epoch 269/1000, LR 0.000227
Train loss: 0.6278;  Loss pred: 0.6278; Loss self: 0.0000; time: 0.22s
Val loss: 0.3591 score: 0.8571 time: 0.06s
Test loss: 0.1980 score: 0.9796 time: 0.08s
Epoch 270/1000, LR 0.000227
Train loss: 0.6251;  Loss pred: 0.6251; Loss self: 0.0000; time: 0.22s
Val loss: 0.3583 score: 0.8571 time: 0.06s
Test loss: 0.1963 score: 0.9796 time: 0.08s
Epoch 271/1000, LR 0.000227
Train loss: 0.6250;  Loss pred: 0.6250; Loss self: 0.0000; time: 0.22s
Val loss: 0.3575 score: 0.8571 time: 0.06s
Test loss: 0.1947 score: 0.9796 time: 0.08s
Epoch 272/1000, LR 0.000226
Train loss: 0.6250;  Loss pred: 0.6250; Loss self: 0.0000; time: 0.22s
Val loss: 0.3569 score: 0.8571 time: 0.06s
Test loss: 0.1931 score: 0.9796 time: 0.08s
Epoch 273/1000, LR 0.000226
Train loss: 0.6216;  Loss pred: 0.6216; Loss self: 0.0000; time: 0.22s
Val loss: 0.3562 score: 0.8571 time: 0.06s
Test loss: 0.1916 score: 0.9796 time: 0.08s
Epoch 274/1000, LR 0.000226
Train loss: 0.6206;  Loss pred: 0.6206; Loss self: 0.0000; time: 0.24s
Val loss: 0.3557 score: 0.8571 time: 0.06s
Test loss: 0.1901 score: 0.9796 time: 0.08s
Epoch 275/1000, LR 0.000225
Train loss: 0.6212;  Loss pred: 0.6212; Loss self: 0.0000; time: 0.22s
Val loss: 0.3551 score: 0.8571 time: 0.06s
Test loss: 0.1886 score: 0.9796 time: 0.08s
Epoch 276/1000, LR 0.000225
Train loss: 0.6177;  Loss pred: 0.6177; Loss self: 0.0000; time: 0.24s
Val loss: 0.3545 score: 0.8571 time: 0.07s
Test loss: 0.1872 score: 0.9796 time: 0.08s
Epoch 277/1000, LR 0.000225
Train loss: 0.6170;  Loss pred: 0.6170; Loss self: 0.0000; time: 0.25s
Val loss: 0.3539 score: 0.8571 time: 2.08s
Test loss: 0.1857 score: 0.9796 time: 1.39s
Epoch 278/1000, LR 0.000224
Train loss: 0.6161;  Loss pred: 0.6161; Loss self: 0.0000; time: 4.71s
Val loss: 0.3533 score: 0.8571 time: 0.40s
Test loss: 0.1843 score: 0.9796 time: 1.06s
Epoch 279/1000, LR 0.000224
Train loss: 0.6164;  Loss pred: 0.6164; Loss self: 0.0000; time: 2.76s
Val loss: 0.3530 score: 0.8571 time: 0.17s
Test loss: 0.1830 score: 0.9796 time: 0.09s
Epoch 280/1000, LR 0.000224
Train loss: 0.6141;  Loss pred: 0.6141; Loss self: 0.0000; time: 0.24s
Val loss: 0.3526 score: 0.8571 time: 0.07s
Test loss: 0.1816 score: 0.9796 time: 0.08s
Epoch 281/1000, LR 0.000223
Train loss: 0.6147;  Loss pred: 0.6147; Loss self: 0.0000; time: 0.24s
Val loss: 0.3522 score: 0.8571 time: 0.07s
Test loss: 0.1803 score: 0.9796 time: 0.08s
Epoch 282/1000, LR 0.000223
Train loss: 0.6116;  Loss pred: 0.6116; Loss self: 0.0000; time: 0.24s
Val loss: 0.3517 score: 0.8571 time: 0.07s
Test loss: 0.1790 score: 0.9796 time: 0.08s
Epoch 283/1000, LR 0.000223
Train loss: 0.6106;  Loss pred: 0.6106; Loss self: 0.0000; time: 0.23s
Val loss: 0.3513 score: 0.8571 time: 0.07s
Test loss: 0.1777 score: 0.9796 time: 0.08s
Epoch 284/1000, LR 0.000222
Train loss: 0.6113;  Loss pred: 0.6113; Loss self: 0.0000; time: 0.24s
Val loss: 0.3509 score: 0.8571 time: 0.07s
Test loss: 0.1765 score: 0.9796 time: 0.08s
Epoch 285/1000, LR 0.000222
Train loss: 0.6094;  Loss pred: 0.6094; Loss self: 0.0000; time: 0.24s
Val loss: 0.3504 score: 0.8571 time: 0.07s
Test loss: 0.1752 score: 0.9796 time: 0.08s
Epoch 286/1000, LR 0.000222
Train loss: 0.6088;  Loss pred: 0.6088; Loss self: 0.0000; time: 0.25s
Val loss: 0.3499 score: 0.8571 time: 0.07s
Test loss: 0.1740 score: 0.9796 time: 0.08s
Epoch 287/1000, LR 0.000221
Train loss: 0.6077;  Loss pred: 0.6077; Loss self: 0.0000; time: 0.24s
Val loss: 0.3495 score: 0.8571 time: 0.07s
Test loss: 0.1727 score: 0.9796 time: 0.08s
Epoch 288/1000, LR 0.000221
Train loss: 0.6066;  Loss pred: 0.6066; Loss self: 0.0000; time: 0.24s
Val loss: 0.3491 score: 0.8571 time: 0.07s
Test loss: 0.1716 score: 0.9796 time: 0.08s
Epoch 289/1000, LR 0.000221
Train loss: 0.6062;  Loss pred: 0.6062; Loss self: 0.0000; time: 0.26s
Val loss: 0.3488 score: 0.8571 time: 0.47s
Test loss: 0.1704 score: 0.9796 time: 0.50s
Epoch 290/1000, LR 0.000220
Train loss: 0.6053;  Loss pred: 0.6053; Loss self: 0.0000; time: 2.22s
Val loss: 0.3485 score: 0.8571 time: 0.26s
Test loss: 0.1693 score: 0.9796 time: 0.61s
Epoch 291/1000, LR 0.000220
Train loss: 0.6025;  Loss pred: 0.6025; Loss self: 0.0000; time: 1.30s
Val loss: 0.3483 score: 0.8571 time: 0.40s
Test loss: 0.1682 score: 0.9796 time: 1.08s
Epoch 292/1000, LR 0.000220
Train loss: 0.6033;  Loss pred: 0.6033; Loss self: 0.0000; time: 0.29s
Val loss: 0.3480 score: 0.8571 time: 0.07s
Test loss: 0.1671 score: 0.9796 time: 0.08s
Epoch 293/1000, LR 0.000219
Train loss: 0.6031;  Loss pred: 0.6031; Loss self: 0.0000; time: 0.24s
Val loss: 0.3476 score: 0.8571 time: 0.07s
Test loss: 0.1660 score: 0.9796 time: 0.08s
Epoch 294/1000, LR 0.000219
Train loss: 0.6017;  Loss pred: 0.6017; Loss self: 0.0000; time: 0.24s
Val loss: 0.3472 score: 0.8571 time: 0.07s
Test loss: 0.1649 score: 0.9796 time: 0.08s
Epoch 295/1000, LR 0.000219
Train loss: 0.5996;  Loss pred: 0.5996; Loss self: 0.0000; time: 0.24s
Val loss: 0.3468 score: 0.8571 time: 0.07s
Test loss: 0.1638 score: 0.9796 time: 0.08s
Epoch 296/1000, LR 0.000218
Train loss: 0.5996;  Loss pred: 0.5996; Loss self: 0.0000; time: 0.24s
Val loss: 0.3465 score: 0.8571 time: 0.07s
Test loss: 0.1627 score: 0.9796 time: 0.08s
Epoch 297/1000, LR 0.000218
Train loss: 0.5988;  Loss pred: 0.5988; Loss self: 0.0000; time: 0.24s
Val loss: 0.3463 score: 0.8571 time: 0.07s
Test loss: 0.1617 score: 0.9796 time: 0.08s
Epoch 298/1000, LR 0.000218
Train loss: 0.5980;  Loss pred: 0.5980; Loss self: 0.0000; time: 0.24s
Val loss: 0.3460 score: 0.8571 time: 0.06s
Test loss: 0.1607 score: 0.9796 time: 0.08s
Epoch 299/1000, LR 0.000217
Train loss: 0.5984;  Loss pred: 0.5984; Loss self: 0.0000; time: 0.25s
Val loss: 0.3458 score: 0.8571 time: 0.06s
Test loss: 0.1597 score: 0.9796 time: 0.07s
Epoch 300/1000, LR 0.000217
Train loss: 0.5965;  Loss pred: 0.5965; Loss self: 0.0000; time: 0.22s
Val loss: 0.3457 score: 0.8571 time: 0.06s
Test loss: 0.1588 score: 0.9796 time: 0.07s
Epoch 301/1000, LR 0.000217
Train loss: 0.5965;  Loss pred: 0.5965; Loss self: 0.0000; time: 0.22s
Val loss: 0.3457 score: 0.8571 time: 0.06s
Test loss: 0.1579 score: 0.9796 time: 0.07s
     INFO: Early stopping counter 1 of 2
Epoch 302/1000, LR 0.000216
Train loss: 0.5956;  Loss pred: 0.5956; Loss self: 0.0000; time: 0.42s
Val loss: 0.3458 score: 0.8571 time: 2.26s
Test loss: 0.1571 score: 0.9796 time: 2.01s
     INFO: Early stopping counter 2 of 2
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 299,   Train_Loss: 0.5965,   Val_Loss: 0.3457,   Val_Precision: 0.9500,   Val_Recall: 0.7600,   Val_accuracy: 0.8444,   Val_Score: 0.8571,   Val_Loss: 0.3457,   Test_Precision: 1.0000,   Test_Recall: 0.9583,   Test_accuracy: 0.9787,   Test_Score: 0.9796,   Test_loss: 0.1588


[0.07079106802120805, 0.07018441602122039, 0.07023633108474314, 0.0697052989853546, 0.0703910420415923, 0.07062913395930082, 0.07010162097867578, 0.07027464197017252, 0.06987066601868719, 0.07009714795276523, 0.07049367693252861, 0.07013685896527022, 0.07022120803594589, 0.06995216698851436, 0.06983618205413222, 0.06972508504986763, 0.06990352703724056, 0.07067643001209944, 0.06929189700167626, 0.06959368195384741, 0.06970364798326045, 0.06998555001337081, 0.06994720397051424, 0.0698741520754993, 0.0702572810696438, 0.07043814600910991, 0.07038082601502538, 0.07069158996455371, 0.07080697803758085, 0.07547648099716753, 0.07524220296181738, 0.07525971997529268, 0.07558234606403857, 0.07552790699992329, 0.07616590999532491, 0.07546610292047262, 0.07044299796689302, 0.07069637405220419, 0.07056315895169973, 0.070826111943461, 0.07062980299815536, 0.07562143099494278, 0.07549268391449004, 0.07611173309851438, 0.08583163807634264, 0.07355130393989384, 0.0761714419350028, 2.4632770200259984, 0.08091302390675992, 0.07595029496587813, 0.07654094696044922, 0.07641816406976432, 0.0757436299463734, 1.3762688419083133, 0.6079220100073144, 0.07742297789081931, 0.07515917695127428, 0.0758576060179621, 0.07616155501455069, 0.07585350808221847, 0.07563257205765694, 0.7125495029613376, 0.29406631097663194, 0.08429849497042596, 0.08278192998841405, 0.08096620603464544, 0.08120407396927476, 0.07401292398571968, 0.07353285199496895, 0.07407186191994697, 0.07376284198835492, 0.07481843302957714, 0.07398352201562375, 0.07388156501110643, 0.07108427409548312, 0.07642552303150296, 0.07712038594763726, 0.07851661404129118, 0.0777531280182302, 0.07896128005813807, 1.330918365973048, 1.9628155010286719, 0.7638422230957076, 0.23356156307272613, 0.07001405698247254, 0.07009359204676002, 0.07082958205137402, 0.07003154105041176, 0.07079189096111804, 0.07050531799905002, 1.301095929928124, 0.0809614920290187, 0.07863310794346035, 0.07139787299092859, 0.07099319796543568, 0.07143126800656319, 0.07198415801394731, 0.07121521304361522, 0.0706044880207628, 0.071301119052805, 0.07134654093533754, 0.07078228995669633, 0.9570135719841346, 0.09071581903845072, 0.08132547105196863, 0.08263122907374054, 0.0809444549959153, 0.08177431998774409, 1.080570682999678, 0.3054379989625886, 0.2619882479775697, 0.223650265019387, 0.07601278799120337, 0.07643709995318204, 0.07382068992592394, 0.07402475294657052, 0.07412147300783545, 0.07321592594962567, 0.07373925903812051, 0.07421306590549648, 0.07441458303947002, 0.07375607104040682, 0.0738056170521304, 0.0735326959984377, 0.07321798091288656, 0.07329075201414526, 0.07448288204614073, 0.07503925508353859, 1.1114320220658556, 0.40592054289299995, 1.1290595420869067, 0.08128957101143897, 0.0777922139968723, 0.07161635893862695, 0.07137364707887173, 0.07118470792192966, 0.0705737010575831, 0.07182544993702322, 0.0708634409820661, 0.07091379608027637, 0.07670305401552469, 0.07636678090784699, 0.07726341998204589, 1.8816777180181816, 0.08302532706875354, 0.07769285491667688, 0.07628627901431173, 0.07581749395467341, 0.07161926000844687, 0.07125062297564, 0.07343270804267377, 0.07160622801166028, 2.5457634339109063, 1.1847587230149657, 0.0761593539500609, 0.07601889304351062, 0.0756638270104304, 0.07504404603969306, 0.07630067609716207, 0.07635059696622193, 0.07645873294677585, 0.07631359796505421, 0.6198731489712372, 0.752224643016234, 0.2589336549863219, 0.07873389695305377, 0.07810413604602218, 0.07768573798239231, 0.07828763802535832, 0.07759926002472639, 0.07301040191669017, 0.16768929501995444, 0.6300503599923104, 1.192017499008216, 0.07821254199370742, 0.0754999719792977, 0.07630452304147184, 0.07455950998701155, 0.07454268401488662, 0.07398487103637308, 0.07520310103427619, 0.07473234296776354, 0.07089906500186771, 0.07036739704199135, 0.07055947289336473, 0.07076507096644491, 0.07611307594925165, 1.6593898329883814, 0.10074626398272812, 0.08078547392506152, 0.08034060907084495, 0.07922569196671247, 0.08035735797602683, 0.07951429695822299, 0.0819287640042603, 0.08125606493558735, 0.07519700808916241, 1.9958464580122381, 0.20342813699971884, 0.08292834903113544, 0.07933131500612944, 0.07934211904648691, 0.07871378306299448, 0.07418975699692965, 0.07506563398055732, 0.07485581294167787, 0.07439985894598067, 0.13452933996450156, 0.07645984995178878, 0.07391092297621071, 0.07250507303979248, 0.07235470903106034, 0.07342183997388929, 0.0731692030094564, 0.07219782401807606, 0.08610134106129408, 0.7368988529779017, 0.5475614339811727, 0.07617503497749567, 0.07644669397268444, 0.07625276606995612, 0.0754458160372451, 0.07517434807959944, 1.7140403729863465, 1.4538443150231615, 0.07930552307516336, 0.0742275039665401, 0.07525380293373019, 0.07353433698881418, 0.07632599701173604, 0.07687233993783593, 0.07690021710004658, 0.07701993093360215, 1.7101872370112687, 0.11457035201601684, 0.07626730599440634, 0.07390865497291088, 0.07395194109994918, 0.07333123497664928, 0.07484246196690947, 0.07449848100077361, 0.07374451891519129, 1.1587652700254694, 0.9716880540363491, 0.08055274398066103, 0.07235359004698694, 0.07262417103629559, 0.07238392403814942, 0.07488380302675068, 0.07331967598292977, 0.07202200999017805, 0.07273058104328811, 0.8126363729825243, 0.07772666809614748, 0.0779280960559845, 0.07842834806069732, 0.07636707101482898, 0.0768673149868846, 0.07647782494314015, 0.07733017997816205, 0.07736604998353869, 0.07718655897770077, 0.09647638606838882, 0.07198115903884172, 0.07132313295733184, 0.07486670999787748, 0.0753897309768945, 0.07577818795107305, 0.07583305600564927, 0.07599412393756211, 0.07545864896383137, 1.832222472061403, 1.3043537019984797, 0.10558439197484404, 0.07582089898642153, 0.07543166598770767, 0.0753628850216046, 0.0746354250004515, 0.07460115104913712, 0.07472972897812724, 0.07480539102107286, 0.07661975896917284, 0.07693251105956733, 0.07538391696289182, 0.07481813698541373, 0.08407897304277867, 0.33966407796833664, 0.08083293109666556, 0.07542941102292389, 0.07473284692969173, 0.07464520109351724, 0.07481097499839962, 0.07755251496564597, 0.07566923298873007, 0.0753642920171842, 1.3526444700546563, 1.142498360015452, 0.07669668097514659, 0.0725506970193237, 0.07329478196334094, 0.07369000697508454, 0.07232166000176221, 0.07463639706838876, 0.07248210092075169, 0.8473683960037306, 0.15146583004388958, 0.09036367502994835, 0.07203829102218151, 0.07184084702748805, 0.07175285299308598, 0.07297035492956638, 0.07367606391198933, 0.07136915903538465, 0.07196484401356429, 0.16834316903259605, 0.23047363793011755, 0.07613274198956788, 0.07537971297279, 0.07762923603877425, 0.07147652900312096, 0.07413994602393359, 0.0728762389626354, 0.07196811097674072, 1.5649823650019243, 0.9546250039711595, 0.6697067440254614, 0.09910916292574257, 0.07648684305604547, 0.07622695493046194, 0.07154838601127267, 0.07206896797288209, 0.07163116696756333, 0.07207878504414111, 0.07171475992072374, 1.0959005790064111, 0.08130196190904826, 0.07195680006407201, 0.0708982179639861, 0.07140820706263185, 0.07166070002131164, 0.07169972301926464, 0.07262321899179369, 0.07194010401144624, 0.07196969201322645, 0.07550320599693805, 0.07503258006181568, 0.07676662795711309, 0.07522658398374915, 0.07524492708034813, 0.07500891899690032, 0.07532408891711384, 0.07837549201212823, 0.0751217040233314, 0.07501951802987605, 1.4089046800509095, 0.7427090919809416, 0.07980849000159651, 0.07757302094250917, 0.07150855893269181, 0.07255654805339873, 0.0710336669581011, 0.07117252703756094, 0.07878000801429152, 0.6022876760689542, 0.07470769493374974, 0.07190134399570525, 0.07235775806475431, 0.07258412393275648, 0.07227962696924806, 0.07115499605424702, 0.07162317796610296, 0.13754573499318212, 0.08015248796436936, 0.07725006202235818, 0.07168986706528813, 0.07154926296789199, 0.07176149799488485, 0.07127556309569627, 0.07173472899012268, 0.07263171602971852, 0.0704352060565725, 0.07567193906288594, 0.07247904897667468, 0.0748370949877426, 0.07262987701687962, 0.7650765490252525, 0.2168380639050156, 0.07680157606955618, 0.0761233480880037, 0.07125348492991179, 0.07127411291003227, 0.07218827994074672, 0.07411653897725046, 0.0732951780082658, 0.07625077606644481, 1.9097842470509931, 0.40388829400762916, 0.07420507806818932, 0.07370426098350435, 0.07702849595807493, 0.07369586895219982, 0.07384481001645327, 0.07362777600064874, 1.5374662729445845, 1.454924462013878, 0.07805983093567193, 0.07277879596222192, 0.0712245519971475, 0.071607960970141, 0.07135662494692951, 0.07497784297447652, 0.07789807301014662, 0.07584687403868884, 0.07735531299840659, 0.07641167903784662, 2.569069736986421, 0.07321789406705648, 0.07131769403349608, 0.07133412605617195, 0.08491357695311308, 0.0855944809736684, 0.07158457802142948, 0.07125653198454529, 0.0719718059990555, 2.176973659079522, 0.1484769779490307, 0.07804734795354307, 0.07701335696037859, 0.07649646501522511, 0.07238761894404888, 0.072157112066634, 0.07176951505243778, 0.07664350711274892, 0.07628750894218683, 0.07686042401473969, 0.07659794401843101, 0.07212399097625166, 0.07188448996748775, 0.07186248106881976, 0.07212042598985136, 0.07219423400238156, 0.07218007394112647, 0.07194581592921168, 0.07254012592602521, 0.07217457506339997, 0.07246932503767312, 0.07260935101658106, 0.07233121106401086, 0.07259570294991136, 0.07203584001399577, 0.07260043395217508, 0.07254593493416905, 0.07250329398084432, 0.07225458696484566, 0.07154655607882887, 0.0719680329784751, 0.07231356902047992, 0.07227121607866138, 0.0727769109653309, 0.07230290200095624, 0.07539433101192117, 0.07370674598496407, 0.07335453201085329, 0.07331012701615691, 0.07278206502087414, 0.07349057798273861, 0.07373035291675478, 0.07302411796990782, 0.07337991602253169, 0.0730509819695726, 0.0749324340140447, 0.07294332794845104, 0.07304076291620731, 0.07331168500240892, 0.0737251719692722, 0.07409830007236451, 0.07380490098148584, 0.07344602898228914, 1.286455709952861, 0.7283633049810305, 0.07732057198882103, 0.0725549190538004, 0.07169379491824657, 0.07168781897053123, 0.07277841807808727, 0.07265463809017092, 0.07062296802178025, 0.07256280293222517, 0.07931700197514147, 1.3364310819888487, 0.13911627291236073, 0.07630393491126597, 0.07588632998522371, 0.07639297004789114, 0.07590514898765832, 0.07592302793636918, 0.5692937700077891, 0.22396281396504492, 0.0808688810793683, 0.07174545398447663, 0.07097170792985708, 0.07111416000407189, 0.07468396902550012, 0.07316304906271398, 0.07292662002146244, 0.07243384793400764, 0.07250937703065574, 0.0741132030962035, 0.07682894601020962, 0.07705742795951664, 0.07692722708452493, 0.0764945870032534, 0.0767737430287525, 0.07994903100188822, 0.07781838392838836, 0.07713190896902233, 0.9640839280327782, 0.07820469804573804, 0.07267871696967632, 0.07239837991073728, 0.07150185003411025, 0.07711926498450339, 0.07771868107374758, 0.07540383108425885, 0.07627028995193541, 0.075465610018, 1.5505055129760876, 0.27935171709395945, 0.0785616299835965, 0.0776502740336582, 0.07799595210235566, 0.07682561699766666, 0.07779261202085763, 0.08030430402141064, 0.07824731897562742, 1.265869040042162, 0.08306884206831455, 0.07735197304282337, 0.07705766102299094, 0.9046314320294186, 0.22324432502500713, 0.09614304802380502, 0.08911531697958708, 0.08808632893487811, 0.0870398780098185, 0.084341497044079, 1.1465723240980878, 0.856239284039475, 0.08773271506652236, 0.08515812107361853, 0.08524371800012887, 0.0854960810393095, 0.08589296089485288, 0.08494796603918076, 1.0766689189476892, 0.5443565710447729, 0.35591785702854395, 0.18987111805472523, 0.09100331598892808, 0.08589479001238942, 0.09996877901721746, 0.08648543001618236, 0.08624080696608871, 0.9599235110217705, 0.6641568179475144, 0.1573711499804631, 0.08980119205079973, 0.090488365967758, 0.09148599894251674, 0.08817838202230632, 0.08540446497499943, 1.1944561949931085, 0.26379113702569157, 0.09595576697029173, 0.09019028092734516, 0.09239554405212402, 0.09211887302808464, 1.335428006015718, 0.37391568603925407, 0.31407444702927023, 0.09456078393850476, 0.09249478403944522, 0.08893550897482783, 0.08816077094525099, 0.08824648300651461, 0.09529941098298877, 0.09275529603473842, 1.3817566110519692, 0.11656293098349124, 0.09063829702790827, 0.09136551897972822, 0.0903258480830118, 0.09327907499391586, 1.9173914979910478, 0.9831968249054626, 0.09045067592523992, 0.09177744796033949, 0.08970720798242837, 0.09025648597162217, 1.0556700839661062, 0.1027953380253166, 0.09216958191245794, 0.09068372298497707, 0.09270196792203933, 0.09309565601870418, 1.0625671279849485, 0.20649934106040746, 0.0868759520817548, 0.08634347503539175, 0.08547479496337473, 0.08952845796011388, 0.08636238798499107, 0.08631693501956761, 0.08838028798345476, 1.128979505971074, 1.6016215709969401, 0.38343653199262917, 0.0964121021097526, 0.08587753190658987, 0.0861017779679969, 0.08545102190691978, 0.08527936309110373, 0.08517482306342572, 0.08512525295373052, 0.08815042907372117, 0.08589584194123745, 0.08506591396871954, 0.28732802299782634, 0.09500672703143209, 0.09341722400858998, 0.09327076200861484, 0.09260475903283805, 0.09306200500577688, 0.09377972292713821, 0.09335852204822004, 1.3808980169706047, 1.9828330019954592, 0.08849550399463624, 0.08834533300250769, 0.09033790102694184, 0.08657808206044137, 0.08983989502303302, 0.08586979797109962, 0.08728030603379011, 0.08561107097193599, 0.08586321701295674, 0.09029065200593323, 0.4199288228992373, 0.08742009405978024, 0.08620199793949723, 0.08536841499153525, 0.0864664550172165, 0.08796622802037746, 0.08956291200593114, 1.3445701979799196, 0.14081659901421517, 0.08303806709591299, 0.08218756492715329, 0.08260784298181534, 0.0841549530159682, 0.08261484489776194, 0.0834454569267109, 0.08337457699235529, 1.7623554749879986, 0.1055744809564203, 0.08417705504689366, 0.0843025438953191, 0.09117275103926659, 0.08714256098028272, 0.08660370390862226, 0.09092566603794694, 1.0162527519278228, 0.3522171910153702, 0.3113861889578402, 0.2783387230010703, 0.08536276000086218, 0.08370092592667788, 0.17378449393436313, 0.08276489400304854, 0.08462290500756353, 0.08528174494858831, 0.08313767600338906, 1.0796459650155157, 0.09219285694416612, 0.08402200893033296, 0.08328825701028109, 0.08614764595404267, 0.08411354303825647, 0.08419538906309754, 0.08319074893370271, 0.9095992720685899, 0.13329044508282095, 0.08633307891432196, 0.08380930195562541, 0.08445916895288974, 0.08240828406997025, 0.08454438101034611, 0.08332976698875427, 0.08302451798226684, 0.08238700602669269, 0.3744497849838808, 1.7669916650047526, 0.6323601240292192, 0.08983990992419422, 0.08616662095300853, 0.08632807806134224, 0.08576099202036858, 0.08636574493721128, 0.08719731692690402, 0.08591536106541753, 0.668586045037955, 0.6392576249781996, 0.16138433502055705, 0.0866437389049679, 0.08184171305038035, 0.08083456999156624, 0.0801967749139294, 0.08160028595011681, 0.08121359290089458, 0.08212828601244837, 0.0817229050444439, 0.0817218740703538, 0.08151635190006346, 1.7596401229966432, 0.8223597990581766, 0.09182630002032965, 0.088273984962143, 0.0823173860553652, 0.08341144397854805, 0.08203552500344813, 0.08445515809580684, 0.08173839293885976, 0.08279312902595848, 0.08198759506922215, 0.08269579394254833, 0.08369682589545846, 0.6658831000095233, 0.8485961960395798, 0.43987275497056544, 0.10886444593779743, 0.08286354702431709, 0.08265456894878298, 0.08211457601282746, 0.08362640999257565, 0.08445100497920066, 0.08272026502527297, 0.08575325598940253, 0.08308370294980705, 0.08376366703305393, 0.0897565430495888, 0.4890235309721902, 0.08449811197351664, 0.08148371998686343, 0.08132177300285548, 0.08040444401558489, 0.08106539596337825, 0.08127397589851171, 0.08115640806499869, 0.08069753996096551, 0.08010408899281174, 0.08155426906887442, 0.08085265499539673, 0.08159387693740427, 0.09217858291231096, 1.2165955890668556, 0.0874401779146865, 0.08285566000267863, 0.08248465799260885, 0.08239731297362596, 0.08230820100288838, 0.0822038889164105, 0.08251832204405218, 0.08139041299000382, 0.08212923991959542, 0.08819364104419947, 0.08619312196969986, 0.08758366690017283, 1.6820907030487433, 0.31679127900861204, 0.08604792796541005, 0.08561309392098337, 0.08025894092861563, 0.07993068802170455, 0.08490921999327838, 0.07947053795214742, 0.07977721805218607, 0.07998785097151995, 0.08107334200758487, 0.08039974700659513, 0.08089151897002012, 0.08116065291687846, 1.5576991190901026, 0.08601568103767931, 0.08501604304183275, 0.08102422603406012, 0.08096583199221641, 0.08024146093521267, 0.08083062001969665, 0.08025932498276234, 0.07985462306533009, 0.08791608293540776, 0.08019146299920976, 0.08268892194610089, 0.08500476798508316, 0.08420298504643142, 0.08008857700042427, 0.07949949498288333, 0.08039959904272109, 0.07985039905179292, 0.07957698206882924, 0.0794957330217585, 0.08050749602261931, 0.07947641401551664, 0.08065441704820842, 0.0838456159690395, 1.3928558429470286, 1.0653785550966859, 0.08953723905142397, 0.083232939010486, 0.082437381031923, 0.08264412893913686, 0.0863129299832508, 0.08598773402627558, 0.08688078005798161, 0.08597636595368385, 0.08661054004915059, 0.08716226601973176, 0.5036463539581746, 0.6165031030541286, 1.0803453179541975, 0.08527082798536867, 0.08461658202577382, 0.08356118598021567, 0.08410686103161424, 0.08384450303856283, 0.08364197507034987, 0.08081065607257187, 0.0781983140623197, 0.07818547100760043, 0.07725685590412468, 2.013372292974964]
[0.0014447156739022052, 0.0014323350208412325, 0.0014333945119335335, 0.001422557122150094, 0.0014365518783998428, 0.0014414108971285882, 0.0014306453260954242, 0.0014341763667382148, 0.0014259319595650447, 0.0014305540398523516, 0.0014386464680107881, 0.0014313644686789842, 0.0014330858782846101, 0.0014275952446635585, 0.001425228205186372, 0.0014229609193850536, 0.0014266025925967463, 0.001442376122695907, 0.001414120346972985, 0.0014202792235479063, 0.001422523428229805, 0.0014282765308851187, 0.0014274939585819232, 0.0014260031035816182, 0.0014338220626457917, 0.001437513183859386, 0.0014363433880617423, 0.001442685509480688, 0.001445040368113895, 0.00154033634688097, 0.001535555162486069, 0.0015359126525569934, 0.0015424968584497668, 0.0015413858571412917, 0.0015544063264352022, 0.0015401245493974005, 0.00143761220340598, 0.0014427831439225345, 0.0014400644684020355, 0.001445430855989, 0.0014414245509827624, 0.0015432945101008732, 0.001540667018663062, 0.0015533006754798852, 0.0017516660831906662, 0.001501047019181507, 0.0015545192231633225, 0.05027095959236731, 0.0016512862021787738, 0.0015500060197117986, 0.001562060142049984, 0.0015595543687707003, 0.0015457883662525185, 0.02808711922261864, 0.012406571632802335, 0.0015800607732820268, 0.0015338607541076383, 0.0015481144085298388, 0.0015543174492765445, 0.001548030777188132, 0.0015435218787276928, 0.014541826591047705, 0.006001353285237387, 0.00172037744837604, 0.0016894271426206948, 0.001652371551727458, 0.0016572259993729542, 0.0015104678364432588, 0.0015006704488769174, 0.001511670651427489, 0.001505364122211325, 0.0015269067965219825, 0.0015098677962372194, 0.0015077870410429885, 0.0014506994713363902, 0.0015597045516633258, 0.0015738854275028013, 0.0016023798783936975, 0.00158679853098429, 0.0016114546950640424, 0.027161599305572406, 0.04005745920466677, 0.015588616797871584, 0.004766562511688288, 0.0014288583057647456, 0.0014304814703420413, 0.001445501674517837, 0.001429215123477791, 0.0014447324685942457, 0.0014388840407969393, 0.026552978161798448, 0.001652275347530994, 0.0016047573049685784, 0.0014570994487944611, 0.0014488407748048098, 0.0014577809797257793, 0.0014690644492642308, 0.0014533716947676576, 0.0014409079187910777, 0.0014551248786286736, 0.0014560518558232151, 0.0014445365297284965, 0.01953088922416601, 0.0018513432456826677, 0.0016597034908565028, 0.001686351613749807, 0.001651927652977863, 0.0016688636732192673, 0.022052462918360775, 0.00623342855025691, 0.005346698938317749, 0.004564291122844633, 0.001551281387575579, 0.001559940815371062, 0.0015065446923657948, 0.0015107092438075616, 0.0015126831226088867, 0.001494202570400524, 0.0015048828375126635, 0.0015145523654182954, 0.001518664959989184, 0.0015052259396001392, 0.0015062370826965387, 0.0015006672652742388, 0.0014942445084262564, 0.00149572963294174, 0.0015200588172681782, 0.001531413369051808, 0.022682286164609298, 0.00828409271210204, 0.02304203147116136, 0.0016589708369681422, 0.0015875962040178021, 0.0014615583456862643, 0.0014566050424259535, 0.0014527491412638705, 0.001440279613420063, 0.0014658255089188413, 0.0014461926731033897, 0.0014472203281689056, 0.0015653684492964223, 0.0015585057328132037, 0.001576804489429508, 0.038401586082003704, 0.0016943944299745621, 0.0015855684676872833, 0.00155686283702677, 0.0015472957949933348, 0.0014616175511927934, 0.0014540943464416327, 0.0014986266947484442, 0.0014613515920746997, 0.05195435579410013, 0.024178749449285015, 0.0015542725295930797, 0.0015514059804798085, 0.0015441597349067427, 0.0015315111436672052, 0.0015571566550441238, 0.0015581754482902434, 0.0015603823050362418, 0.0015574203666337595, 0.012650472427984434, 0.015351523326861918, 0.005284360305843305, 0.0016068142235317097, 0.0015939619601229016, 0.0015854232241304554, 0.0015977068984767003, 0.001583658367851559, 0.001490008202381432, 0.003422230510611315, 0.012858170612087967, 0.024326887734861552, 0.0015961743264021923, 0.0015408157546795448, 0.0015572351641116701, 0.0015216226527961542, 0.001521279265609931, 0.00150989532727292, 0.0015347571639648201, 0.0015251498564849704, 0.0014469196939156676, 0.0014360693273875787, 0.001439989242721729, 0.0014441851217641818, 0.0015533280805969725, 0.03386509863241595, 0.0020560462037291453, 0.0016486831413277862, 0.0016396042667519376, 0.0016168508564635199, 0.0016399460811434047, 0.0016227407542494486, 0.0016720155919236796, 0.0016582870395017825, 0.0015346328181461717, 0.040731560367596696, 0.004151594632647323, 0.001692415286349703, 0.0016190064286965193, 0.0016192269193160596, 0.0016064037359794791, 0.0015140766734067276, 0.001531951713888925, 0.0015276696518709768, 0.0015183644682853197, 0.0027454967339694196, 0.0015604051010569139, 0.0015083861831879737, 0.0014796953681590302, 0.0014766267149195988, 0.0014984048974263121, 0.0014932490410093143, 0.001473424979960736, 0.0017571702257406954, 0.015038752101589831, 0.011174723142472913, 0.0015545925505611362, 0.0015601366116874376, 0.0015561788993868598, 0.0015397105313723488, 0.001534170368971417, 0.03498041577523156, 0.029670292143329825, 0.001618480062758436, 0.0015148470197253081, 0.0015357918966067384, 0.0015007007548737588, 0.0015576734084027763, 0.001568823264037468, 0.0015693921857152361, 0.0015718353251755542, 0.03490178034716875, 0.0023381704493064663, 0.001556475632538905, 0.0015083398974063446, 0.001509223287754065, 0.0014965558158499853, 0.0015273971829981524, 0.001520377163281094, 0.0015049901819426793, 0.023648270816846312, 0.01983036844972141, 0.0016439335506257353, 0.0014766038785099375, 0.0014821259395162367, 0.0014772229395540698, 0.0015282408780969527, 0.0014963199180189747, 0.0014698369385750623, 0.0014842975723120024, 0.016584415775153558, 0.0015862585325744382, 0.0015903693072649898, 0.0016005785318509657, 0.0015585116533638568, 0.001568720714018053, 0.001560771937615105, 0.0015781669383298379, 0.0015788989792558917, 0.0015752358975040974, 0.001968905838130384, 0.0014690032456906475, 0.001455574141986364, 0.0015278920407730098, 0.0015385659383039694, 0.0015464936316545521, 0.0015476133878703933, 0.0015509004885216756, 0.0015399724278332932, 0.0373922953481919, 0.026619463306091422, 0.0021547835096906946, 0.001547365285437174, 0.001539421754851177, 0.0015380180616654, 0.0015231719387847247, 0.0015224724703905533, 0.0015250965097576988, 0.0015266406330831197, 0.0015636685503912823, 0.0015700512461136191, 0.0015384472849569758, 0.001526900754804362, 0.0017158974090362995, 0.006931919958537483, 0.001649651655033991, 0.001539375735161712, 0.0015251601414222802, 0.001523371450888107, 0.0015267545918040738, 0.0015827043870539994, 0.0015442700609944913, 0.001538046775860902, 0.027604989184788903, 0.023316293061539834, 0.0015652383872478896, 0.0014806264697821165, 0.0014958118768028763, 0.0015038776933690723, 0.0014759522449339227, 0.001523191776905893, 0.0014792265494030957, 0.017293232571504707, 0.003091139388650808, 0.001844156633264252, 0.0014701692045343165, 0.0014661397352548583, 0.0014643439386344077, 0.0014891909169299261, 0.0015035931410610067, 0.0014565134497017277, 0.001468670285991108, 0.003435574878216246, 0.004703543631226889, 0.0015537294283585281, 0.0015383614892406122, 0.0015842701232402908, 0.0014587046735330808, 0.0015130601229374201, 0.0014872701829109266, 0.0014687369587089941, 0.03193841561228417, 0.01948214293818693, 0.01366748457194819, 0.002022635978076379, 0.001560955980735622, 0.001555652141437999, 0.0014601711430871974, 0.0014707952647526957, 0.0014618605503584354, 0.001470995613145737, 0.001463566528994362, 0.02236531793890635, 0.0016592237124295564, 0.0014685061237565717, 0.0014469024074282876, 0.0014573103482169764, 0.001462463265741054, 0.0014632596534543804, 0.001482106510036606, 0.0014681653879886987, 0.0014687692247597234, 0.001540881755039552, 0.0015312771441186874, 0.0015666658766757772, 0.0015352364078316154, 0.0015356107567417988, 0.0015307942652428637, 0.0015372263044308946, 0.001599499836982209, 0.001533096000476151, 0.0015310105720382867, 0.028753156735732848, 0.015157328407774319, 0.0016287446939101328, 0.0015831228763777383, 0.001459358345565139, 0.0014807458786407904, 0.001449666672614308, 0.001452500551786958, 0.0016077552655977862, 0.012291585225897024, 0.0015246468353826478, 0.0014673743672592907, 0.0014766889400970266, 0.0014813086516889079, 0.0014750944279438378, 0.0014521427766172861, 0.0014616975095123053, 0.0028070558161873904, 0.0016357650604973339, 0.0015765318780073098, 0.0014630585115364924, 0.001460189040161061, 0.001464520367242548, 0.0014546033284835974, 0.0014639740610229118, 0.0014822799189738473, 0.00143745318482801, 0.0015443252869976722, 0.0014791642648300954, 0.0015272876528110734, 0.001482242388099584, 0.015613807122964335, 0.0044252666103064405, 0.0015673791034603302, 0.001553537716081708, 0.0014541527536716691, 0.0014545737328578014, 0.0014732302028723822, 0.001512582428107152, 0.0014958199593523632, 0.0015561382870703023, 0.03897518871532639, 0.008242618245053656, 0.0015143893483303944, 0.0015041685915000889, 0.0015720101215933658, 0.0015039973255550983, 0.0015070369391112911, 0.0015026076734826273, 0.031376862713154784, 0.029692335959466895, 0.0015930577741973863, 0.001485281550249427, 0.0014535622856560715, 0.001461386958574306, 0.0014562576519781534, 0.0015301600607036023, 0.0015897565920438086, 0.0015478953885446703, 0.0015786798571103386, 0.0015594220211805434, 0.05242999463237594, 0.001494242736062377, 0.0014554631435407363, 0.0014557984909422848, 0.0017329301419002669, 0.0017468261423197631, 0.0014609097555393772, 0.001454214938460108, 0.0014688123673276634, 0.04442803385876575, 0.0030301424071230753, 0.0015928030194600625, 0.0015717011624567059, 0.001561152347249492, 0.001477298345796916, 0.001472594123808857, 0.0014646839806619957, 0.001564153206382631, 0.0015568879375956496, 0.001568580081933463, 0.0015632233473149184, 0.0014719181831888094, 0.0014670304074997501, 0.0014665812463024442, 0.0014718454283643135, 0.0014733517143343175, 0.0014730627334923769, 0.0014682819577390138, 0.001480410733184188, 0.0014729505114979586, 0.0014789658170953697, 0.0014818234901343072, 0.0014761471645716503, 0.0014815449581614562, 0.0014701191839590973, 0.0014816415092280628, 0.001480529284370797, 0.0014796590608335575, 0.0014745834074458297, 0.0014601337975271198, 0.001468735366907655, 0.001475787122866937, 0.0014749227771155384, 0.0014852430809251204, 0.0014755694285909437, 0.0015386598165698197, 0.0015042193058155933, 0.0014970312655276181, 0.0014961250411460595, 0.0014853482657321254, 0.0014998077139334411, 0.001504701079933771, 0.0014902881218348534, 0.0014975493065822795, 0.0014908363667259716, 0.0015292333472254021, 0.001488639345886756, 0.0014906278146164758, 0.0014961568367838555, 0.0015045953463116775, 0.0015122102055584593, 0.001506222469009915, 0.001498898550658962, 0.02625419816230329, 0.014864557244510827, 0.001577970856914715, 0.0014807126337510286, 0.0014631386718009503, 0.0014630167136843108, 0.0014852738383283116, 0.0014827477161259372, 0.0014412850616689846, 0.001480873529229085, 0.0016187143260232952, 0.027274103714058136, 0.0028391076104563413, 0.0015572231614544075, 0.0015487006119433411, 0.0015590402050590028, 0.0015490846732175167, 0.00154944954972182, 0.011618240204240595, 0.004570669672756019, 0.0016503853281503733, 0.0014641929384587066, 0.0014484022026501444, 0.001451309387838202, 0.001524162633173472, 0.001493123450259469, 0.0014882983677849478, 0.0014782417945715847, 0.00147978320470726, 0.0015125143489021125, 0.0015679376736777474, 0.0015726005706023804, 0.0015699434098882638, 0.0015611140204745593, 0.001566811082219439, 0.0016316128775895554, 0.0015881302842528236, 0.0015741205912045374, 0.019675182204750577, 0.0015960142458313886, 0.0014832391218301288, 0.0014775179573619853, 0.001459221429267556, 0.0015738625507041508, 0.0015860955321172975, 0.0015388536955971196, 0.001556536529631335, 0.0015401144901632654, 0.03164296965257322, 0.005701055450897131, 0.001603298571093806, 0.0015846994700746573, 0.0015917541245378706, 0.0015678697346462583, 0.0015876043269562783, 0.0016388633473757275, 0.0015968840607270903, 0.025834062041676775, 0.001695282491190093, 0.0015786116947514974, 0.0015726053269998152, 0.018461865959784055, 0.004556006633163411, 0.00196210302089398, 0.00181867993835892, 0.0017976801823444512, 0.001776324041016704, 0.001721255041715898, 0.023399435185675263, 0.01747427110284643, 0.0017904635727861707, 0.001737920838237113, 0.0017396677142883443, 0.0017448179803940716, 0.0017529175692827118, 0.0017336319599832808, 0.021972835080565085, 0.011109317776423936, 0.007263629735276407, 0.0038749207766270457, 0.0018572105303862874, 0.001752954898212029, 0.0020401791636166827, 0.0017650087758404563, 0.001760016468695688, 0.019590275735138173, 0.013554220774439069, 0.0032116561220502673, 0.0018326773887918312, 0.0018467013462807756, 0.001867061202908505, 0.0017995588167817617, 0.0017429482647959068, 0.024376657040675685, 0.005383492592361053, 0.001958280958577382, 0.0018406179781090847, 0.001885623348002531, 0.0018799770005731558, 0.02725363277583098, 0.007630932368148042, 0.006409682592434086, 0.001929811917112342, 0.001887648653866229, 0.0018150103872413843, 0.0017991994070459384, 0.0018009486327860125, 0.0019448859384283423, 0.0018929652251987433, 0.028199114511264677, 0.002378835326193699, 0.0018497611638348625, 0.0018646024281577188, 0.001843384654755343, 0.0019036545917125685, 0.03913043873451118, 0.020065241324601277, 0.0018459321617395903, 0.0018730091420477446, 0.0018307593465801708, 0.001841969101461677, 0.021544287427879717, 0.0020978640413329918, 0.0018810118757644479, 0.0018506882241832055, 0.0018918768963681496, 0.0018999113473204933, 0.021685043428264255, 0.004214272266538928, 0.0017729786139133634, 0.001762111735416158, 0.0017443835706811171, 0.0018271113869410996, 0.0017624977139794097, 0.0017615701024401554, 0.0018036793466011177, 0.02304039808104233, 0.03268615451014163, 0.00782523534678835, 0.0019675939206071956, 0.0017526026919712217, 0.0017571791422040183, 0.001743898406263669, 0.001740395165124566, 0.0017382616951719535, 0.0017372500602802147, 0.0017989883484432893, 0.0017529763661477031, 0.0017360390605861132, 0.0058638372040372725, 0.0019389127965598386, 0.0019064739593589793, 0.0019034849389513232, 0.0018898930414864908, 0.0018992245919546302, 0.0019138718964722082, 0.001905275960167756, 0.028181592183073565, 0.04046597963256039, 0.0018060306937680865, 0.001802965979643014, 0.0018436306332028946, 0.0017668996338865586, 0.001833467245368021, 0.0017524448565530534, 0.0017812307353834717, 0.0017471647137129794, 0.0017523105512848313, 0.001842666367468025, 0.008569975977535454, 0.001784083552240413, 0.0017592244477448414, 0.001742212550847658, 0.0017646215309636022, 0.0017952291432730093, 0.0018278145307332886, 0.027440208122039175, 0.0028738081431472482, 0.0016946544305288366, 0.0016772972434112917, 0.00168587434656766, 0.001717448020734045, 0.0016860172428114681, 0.0017029685087083858, 0.0017015219794358223, 0.0359664382650612, 0.0021545812440085777, 0.0017178990825896664, 0.0017204600794963082, 0.0018606683885564609, 0.0017784196118425047, 0.0017674225287473931, 0.0018556258375091212, 0.02073985208015965, 0.007188105939089188, 0.006354820182813066, 0.00568038210206266, 0.0017420971428747385, 0.0017081821617689363, 0.0035466223251910843, 0.0016890794694499702, 0.0017269980613788475, 0.001740443774460986, 0.001696687265375287, 0.022033591122765626, 0.0018814868764115535, 0.0017147348761292441, 0.0016997603471485935, 0.0017581152235518914, 0.0017166029191480912, 0.001718273246185664, 0.001697770386402096, 0.018563250450379386, 0.0027202131649555297, 0.00176189956968004, 0.0017103939174617432, 0.0017236565092426477, 0.0016818017157136786, 0.00172539553082339, 0.0017006074895664137, 0.0016943779180054457, 0.001681367469932504, 0.007641832346609812, 0.03606105438785209, 0.012905308653657533, 0.0018334675494733515, 0.0017585024684287456, 0.001761797511455964, 0.0017502243269462974, 0.0017625662232083933, 0.0017795370801408983, 0.0017533747156207658, 0.0136446131640399, 0.01304607397914693, 0.0032935578575623886, 0.001768239569489141, 0.0016702390418444968, 0.0016496851018686987, 0.0016366688757944777, 0.0016653119581656493, 0.0016574202632835629, 0.0016760874696418035, 0.0016678143886621206, 0.0016677933483745676, 0.0016635990183686419, 0.03591102291829884, 0.016782853042003602, 0.0018740061228638704, 0.001801509897186592, 0.0016799466541911267, 0.0017022743669091438, 0.001674194387825472, 0.001723574655016466, 0.0016681304681399952, 0.001689655694407316, 0.0016732162259024928, 0.0016876692641336396, 0.0017080984876624175, 0.013589451020602517, 0.017318289715093464, 0.008976994999399295, 0.0022217233864856617, 0.0016910927964146345, 0.0016868279377302649, 0.0016758076737311725, 0.001706661428419911, 0.0017234898975347073, 0.0016881686739851625, 0.001750066448763317, 0.0016955857744858581, 0.0017094625925113047, 0.0018317661846854857, 0.009980072060656943, 0.0017244512647656457, 0.0016629330609563965, 0.0016596280204664385, 0.0016409070207262222, 0.0016543958359873112, 0.0016586525693573818, 0.0016562532258162998, 0.0016468885706319492, 0.001634777326383913, 0.0016643728381402943, 0.0016500541835795253, 0.0016651811619878424, 0.0018811955696389992, 0.024828481409527665, 0.0017844934268303368, 0.00169093183678936, 0.001683360367196099, 0.0016815778157882849, 0.0016797592041405793, 0.001677630386049194, 0.0016840473886541262, 0.0016610288365306904, 0.0016761069371346005, 0.001799870225391826, 0.0017590433055040787, 0.0017874217734729148, 0.03432838169487231, 0.006465128143032899, 0.0017560801625593888, 0.0017472059983874159, 0.0016379375699717477, 0.001631238531055195, 0.0017328412243526202, 0.0016218477133091312, 0.0016281064908609403, 0.001632405121867754, 0.0016545580001547933, 0.0016408111633999006, 0.0016508473259187779, 0.0016563398554464992, 0.03178977794061434, 0.0017554220619934555, 0.0017350212865680152, 0.0016535556333481657, 0.0016523639182084982, 0.0016375808354125035, 0.0016496044901978908, 0.0016379454078114762, 0.0016296861850067365, 0.0017942057741919951, 0.0016365604693716277, 0.0016875290193081815, 0.0017347911833690442, 0.0017184282662537024, 0.0016344607551106994, 0.001622438673120068, 0.001640808143729002, 0.0016295999806488352, 0.0016240200422210048, 0.0016223618984032347, 0.001643010122910598, 0.0016219676329697274, 0.0016460085111879269, 0.0017111350197763163, 0.028425629447898542, 0.0217424194917691, 0.0018272905928862033, 0.0016986314083772655, 0.0016823955312637346, 0.0016866148763089155, 0.0017614883670051182, 0.0017548517148219505, 0.001773077144040441, 0.0017546197133404867, 0.0017675620418193998, 0.0017788217555047298, 0.010278497019554583, 0.012581695980696502, 0.022047863631718317, 0.001740220979293238, 0.0017268690209341596, 0.0017053303261268505, 0.0017164665516655967, 0.0017111123069094457, 0.0017069790830683646, 0.0016491970627055485, 0.001595883960455504, 0.0015956218572979678, 0.0015766705286556057, 0.04108923046887681]
[692.1777191625398, 698.1606854887096, 697.6446412167999, 702.9594695561829, 696.1113030696026, 693.7647009552129, 698.9852633351419, 697.2643136452781, 701.2957338476601, 699.0298668501965, 695.0978035505115, 698.6340808940903, 697.7948880474563, 700.4786571950722, 701.6420222116175, 702.7599889617205, 700.9660610386029, 693.3004396460242, 707.1533919588696, 704.086903068233, 702.9761198691854, 700.144529701324, 700.5283587983819, 701.2607458485551, 697.4366108962837, 695.6457938808146, 696.2123460946473, 693.1517599840329, 692.022189875032, 649.2088575491333, 651.2302680035256, 651.0786914445922, 648.2995375465565, 648.7668193963029, 643.3324305192131, 649.2981365638686, 695.5978793382579, 693.1048537767583, 694.4133557504185, 691.8352378161838, 693.7581292882799, 647.9644639794888, 649.0695185178727, 643.7903593205189, 570.8850617113602, 666.2016493962204, 643.2857085968225, 19.8922003500373, 605.5885398185727, 645.1587847290656, 640.1802165489229, 641.2088094038285, 646.9190879113144, 35.60350892784679, 80.60244438165752, 632.88704897271, 651.9496618725178, 645.94709182356, 643.3692168002417, 645.9819886891501, 647.8690155168312, 68.76715203134273, 166.62908388677613, 581.267791520957, 591.9166176345238, 605.1907629095638, 603.418001152752, 662.046536756935, 666.3688225142217, 661.5197556793788, 664.2911075434936, 654.9188216843485, 662.3096422694265, 663.2236335631758, 689.3226472873778, 641.1470678427934, 635.3702642679943, 624.0717407175931, 630.199726350705, 620.5573157365483, 36.816683316392286, 24.964139509964177, 64.14937341564112, 209.79479395221566, 699.8594583980008, 699.0653292145691, 691.8013431797378, 699.6847315515685, 692.1696727512606, 694.9830366081069, 37.66055897408494, 605.2260003118165, 623.1471867452133, 686.2949545601402, 690.2069691783223, 685.9741030426314, 680.7053295046668, 688.0552329456674, 694.0068736932201, 687.2262406388182, 686.7887266519263, 692.2635595708693, 51.200945769672245, 540.1483503029489, 602.5172601667194, 592.9961414016019, 605.3533871155558, 599.2101188654808, 45.34640886607748, 160.42535691835033, 187.03129006074795, 219.09207215002613, 644.6283749738341, 641.0499617334076, 663.7705506297693, 661.9407434614088, 661.0769863521218, 669.253299257776, 664.5035580662505, 660.261092870048, 658.4730841535463, 664.3520907337329, 663.9061084658408, 666.3702361877371, 669.2345157441492, 668.5700262775706, 657.8692802145523, 652.9915568251578, 44.0872667218298, 120.7132796255555, 43.39895122752378, 602.7833508077534, 629.8830883251386, 684.2012177970611, 686.5278993779365, 688.3500885294034, 694.3096261880825, 682.2094402884125, 691.4707968019895, 690.9797910766284, 638.8272361369393, 641.6402448484647, 634.194034012297, 26.04059108039379, 590.1813546536582, 630.6886270629512, 642.3173424254619, 646.2888371026095, 684.1735029686271, 687.7132852122949, 667.2775838734526, 684.2980193290016, 19.24766431448196, 41.35863197133096, 643.3878106703768, 644.5766050809774, 647.6013960177465, 652.9498685889407, 642.1961443382611, 641.7762525377236, 640.8685850720242, 642.0874039045866, 79.04843124971953, 65.14011532980649, 189.23766399770787, 622.3494821959208, 627.367543904809, 630.7464056157385, 625.8970283932733, 631.4493203206646, 671.1372450176665, 292.2070845021393, 77.77156099172458, 41.106779087361595, 626.4979855013828, 649.0068633858028, 642.1637675838467, 657.1931603163154, 657.3415036975916, 662.2975658889802, 651.5688758322174, 655.6732741690781, 691.1233596481023, 696.3452118423469, 694.4496322138464, 692.4320053778314, 643.779001030923, 29.5289262510162, 486.3703929348738, 606.5446870492278, 609.9032676836123, 618.4862357603374, 609.776145385694, 616.2413789025227, 598.08054711349, 603.0319095422956, 651.6216701321393, 24.550986777210063, 240.87130090596918, 590.871524303504, 617.6627728434108, 617.5786655167436, 622.5085124010035, 660.4685334395673, 652.7620883438011, 654.5917821796577, 658.603399175492, 364.23281354780823, 640.859222597175, 662.960196232042, 675.8147802031405, 677.2192253439281, 667.3763558285337, 669.6806577716479, 678.6908146668235, 569.096827018266, 66.49487891314362, 89.48767564533189, 643.2553659407708, 640.9695103035906, 642.599639664825, 649.4727285580731, 651.8180902362551, 28.587424644279555, 33.70374633216444, 617.8636506004668, 660.132664868914, 651.1298843348854, 666.3553654866533, 641.983097744084, 637.4204302825261, 637.189358467628, 636.1989605293497, 28.651833518318515, 427.68481668931105, 642.4771317292077, 662.980540208174, 662.5924792666959, 668.2009380532453, 654.7085533031324, 657.731531458892, 664.4561619061027, 42.28638989061434, 50.427706501542524, 608.2970930420922, 677.229698874362, 674.7064964846363, 676.9458916620065, 654.3471087131588, 668.3062812690026, 680.3475771737326, 673.719352947778, 60.29757174191087, 630.4142606420136, 628.7847705761707, 624.7740926798291, 641.6378073539728, 637.4620995719777, 640.7085980338823, 633.6465273175045, 633.3527433599857, 634.8255531660133, 507.89630495969834, 680.733689958495, 687.0141280713739, 654.4965045397238, 649.9558940595975, 646.6240659071624, 646.1562091912753, 644.7866948273412, 649.3622755356586, 26.74347725081164, 37.566497434648376, 464.0837446094729, 646.2598129939771, 649.5945616259494, 650.1874229728992, 656.5246998955733, 656.826326549914, 655.6962091263824, 655.0330040544347, 639.5217194524802, 636.9218854959805, 650.0060221614717, 654.9214131000463, 582.785424544484, 144.26017697569938, 606.1885834797014, 649.6139812772543, 655.6688526278002, 656.4387165172435, 654.9841116366713, 631.8299286838848, 647.5551299336931, 650.1752844547025, 36.22533569225331, 42.888464189425434, 638.8803189003492, 675.3897896659623, 668.5332664541904, 664.9476911647936, 677.5286960891945, 656.5161492870787, 676.0289696013938, 57.82608866590804, 323.505308001161, 542.2532890983065, 680.1938150491698, 682.0632276405568, 682.8996751491063, 671.5055730138159, 665.0735313240073, 686.5710716264139, 680.8880179155844, 291.0721015980886, 212.6056604133502, 643.6127048558719, 650.0422735449743, 631.205490358368, 685.5397244857883, 660.912269671494, 672.37278840807, 680.8571092804735, 31.31025696889546, 51.32905569848279, 73.1663529405, 494.40433713190777, 640.6330558589719, 642.81722974111, 684.8512277031643, 679.9042830533883, 684.0597755749061, 679.8116806490614, 683.2624142389445, 44.71208514592212, 602.6914830765811, 680.9641334296308, 691.1316166633461, 686.1956351462837, 683.7778585113957, 683.4057083712086, 674.7153414603795, 681.1221734153139, 680.8421521519764, 648.9790645709421, 653.0496480279805, 638.2981942019733, 651.3654801949433, 651.2066912853376, 653.255648198648, 650.5223057383315, 625.1954372728848, 652.2748736474549, 653.1633538419436, 34.77879000176891, 65.97468716763383, 613.9697668633975, 631.6629081174345, 685.2326592977739, 675.3353255441253, 689.8137474572789, 688.4678968071556, 621.9852121760496, 81.35647124612613, 655.889598032075, 681.4893474442819, 677.1906884697697, 675.0787547617805, 677.9227017987716, 688.6375197413189, 684.1360770558127, 356.245142769631, 611.3347351336399, 634.3036978509884, 683.4996632840118, 684.8428336989151, 682.8174072326739, 687.4726466097704, 683.0722118814574, 674.636407873811, 695.6748300082197, 647.531972971901, 676.0574357945736, 654.7555060498487, 674.6534898938643, 64.04587888941121, 225.97508535892533, 638.0077403049987, 643.6921290344811, 687.6856626479204, 687.4866343387762, 678.78054498902, 661.1209950729108, 668.5296540854853, 642.6164103208795, 25.65734850712257, 121.32067387690722, 660.3321669572586, 664.819093850851, 636.1282197002746, 664.8947993514005, 663.5537418145212, 665.5097119810906, 31.87061782249979, 33.678724414444964, 627.7236244641658, 673.2730234426379, 687.9650152374762, 684.28145887902, 686.6916706955109, 653.5264026824601, 629.0271133358781, 646.0384903273076, 633.4406532749642, 641.2632285665435, 19.073051733301, 669.2353095422744, 687.0665220469126, 686.9082542823195, 577.0573064782849, 572.4668161148611, 684.5049779483427, 687.6562559994854, 680.8221541730248, 22.50831092771164, 330.01749279151386, 627.824023298867, 636.2532674067078, 640.552475075123, 676.9113380821925, 679.0737405725246, 682.7411326967804, 639.323562371917, 642.3069868113507, 637.5192516580856, 639.7038540382965, 679.3855877461673, 681.6491293485137, 681.857894011128, 679.4191704704459, 678.7245640473667, 678.8577141105003, 681.0680978058775, 675.4882125510659, 678.9094353095555, 676.1481492276545, 674.8442082729863, 677.4392309930569, 674.9710796768286, 680.2169585373036, 674.9270952330442, 675.4341238342915, 675.8313630956686, 678.1576375744862, 684.868744010719, 680.8578471868948, 677.6045030514634, 678.0015981281878, 673.2904619068316, 677.704471659408, 649.9162382945243, 664.7966796688573, 667.9887207616582, 668.3933311041847, 673.2427829018953, 666.7521380973363, 664.5838255422896, 671.0111859234258, 667.7576461787486, 670.7644261429589, 653.922438857596, 671.7543794358853, 670.8582720612189, 668.3791267161562, 664.6305283685556, 661.2837265112227, 663.9125498222913, 667.1565594351728, 38.08914649832405, 67.27411947431392, 633.7252653418601, 675.3504881407954, 683.4622167214803, 683.5191906192944, 673.2765192481331, 674.4235645243543, 693.8252720402279, 675.2771119628164, 617.7742322554874, 36.66481621115788, 352.22335226640666, 642.1687172094364, 645.7025924107951, 641.4202768825672, 645.5425047379471, 645.3904873376063, 86.07155493609139, 218.78632051679656, 605.9191044316445, 682.9701016401959, 690.4159619270794, 689.032957672485, 656.097963717882, 669.7369864670092, 671.9082824019433, 676.4793173026299, 675.774665382708, 661.1507525372366, 637.7804531314083, 635.8893788375981, 636.9656343671471, 640.5682012233882, 638.2390393763793, 612.8904801715825, 629.6712618073873, 635.2753439523888, 50.82545053933731, 626.560823383556, 674.2001240947089, 676.810725052328, 685.2969535281164, 635.3794996599906, 630.4790472898492, 649.8343558332693, 642.4519958017622, 649.3023774446739, 31.602596437047104, 175.40611709760472, 623.7141465907861, 631.0344761791891, 628.2377313081108, 637.8080894747416, 629.8798655438154, 610.1790009528714, 626.2195387839753, 38.70858552506188, 589.8721925087525, 633.4680044020695, 635.8874555688934, 54.1657057947623, 219.49046182701923, 509.6572347890156, 549.8493599167025, 556.272472612924, 562.9603478358802, 580.9714282684757, 42.73607427123639, 57.226993567537555, 558.5145742138071, 575.4002012050017, 574.8224168252015, 573.125684877541, 570.477481385047, 576.8236990795002, 45.510740709308706, 90.01452835584452, 137.67221574406784, 258.0697922992009, 538.4419179402385, 570.4653331468912, 490.15303059328966, 566.5694208935721, 568.1765016330099, 51.04573378752123, 73.77775651152356, 311.36583805915586, 545.6497723580455, 541.5060762337032, 535.6010817653977, 555.6917565986248, 573.7404948833043, 41.02285224472607, 185.75301866653584, 510.6519550322659, 543.2957908122391, 530.3286051582438, 531.9214010039092, 36.692356142951276, 131.0455854875688, 156.01396568067008, 518.1852133529892, 529.7596022182561, 550.9610341789227, 555.8027620973239, 555.2629218819143, 514.1689701392451, 528.2717224216359, 35.462106428928145, 420.3737808114987, 540.6103336750965, 536.3073569458069, 542.4803756613248, 525.3053806890349, 25.55555297462197, 49.83742701234974, 541.7317173008182, 533.9002237366063, 546.2214364045083, 542.8972718415632, 46.416016465967424, 476.6753136988781, 531.6287541212878, 540.3395271731123, 528.5756181703512, 526.3403481485241, 46.114733563161856, 237.28889277988534, 564.0225957338395, 567.5009024123149, 573.2684122962344, 547.3120068909279, 567.3766224310021, 567.675392886598, 554.422271278105, 43.402027885221365, 30.5939935421197, 127.79168365976649, 508.2349510875709, 570.5799749030742, 569.0939392472566, 573.4278994740963, 574.5821523978013, 575.2873705826426, 575.6223717377233, 555.867969275802, 570.4583469071947, 576.0239056270916, 170.53679445798676, 515.7529527755314, 524.5285387145984, 525.3521998187832, 529.1304735496843, 526.5306716415394, 522.5010105656887, 524.8583517066745, 35.48415552619558, 24.712116426692504, 553.7004456516787, 554.6416356663586, 542.4079975622473, 565.9631032920365, 545.4147067673827, 570.6313646678366, 561.4095805419141, 572.3558815899243, 570.6751005218674, 542.6918392036873, 116.68644143476106, 560.5118654584433, 568.4323005412441, 573.9827781136457, 566.693754129778, 557.0319553618816, 547.1014608899169, 36.44287228262052, 347.9703411602317, 590.0908067067918, 596.197247642402, 593.1640172566482, 582.259252057361, 593.113744395923, 587.2099189658232, 587.7091287011011, 27.803698343169774, 464.1273114118035, 582.1063705864134, 581.2398740997035, 537.4412797843131, 562.2969929824184, 565.7956621774645, 538.9017439756815, 48.21635159860322, 139.1187064400321, 157.36086486043317, 176.04449525268382, 574.0208025080854, 585.4176576603712, 281.9584123455046, 592.0384553165155, 579.0394455924246, 574.5661047336615, 589.3838071442187, 45.385248116308034, 531.4945389931391, 583.1805335745836, 588.3182306714793, 568.7909339523927, 582.5459043820548, 581.9796136731255, 589.0077998822877, 53.869876004369836, 367.6182487765984, 567.5692401591309, 584.6606385761818, 580.161995523915, 594.6004161231614, 579.5772517868885, 588.0251652043233, 590.1871060602349, 594.7539832206601, 130.8586677439522, 27.730747671561996, 77.48749191803226, 545.4146163029948, 568.6656788679509, 567.6021185735424, 571.3553312018861, 567.3545690553989, 561.9438960613414, 570.3287443870544, 73.28899602925202, 76.6514126470858, 303.62302508331067, 565.5342280847772, 598.7166956028456, 606.1762932011928, 610.9971386329299, 600.4880917936275, 603.3472753729168, 596.6275735082668, 599.5871044152433, 599.594668592845, 601.106389796154, 27.84660304094093, 59.58462470577744, 533.6161860943081, 555.0899284881501, 595.2569967059386, 587.449367410567, 597.3022053304397, 580.1895479778024, 599.4734938898536, 591.8365518548867, 597.6513880987641, 592.5331587485817, 585.4463353389705, 73.58648987982907, 57.74242240147228, 111.3958512917648, 450.1010369170247, 591.3336051813047, 592.8286920274537, 596.7271875378801, 585.9392984148216, 580.2180804369132, 592.3578700458631, 571.4068746970431, 589.766684202823, 584.9791650198903, 545.9212034595453, 100.19967730916109, 579.8946136850674, 601.3471158152774, 602.5446592055912, 609.4190513959934, 604.4502641069689, 602.8990148234805, 603.7724089606774, 607.2056226708021, 611.7041041986894, 600.8269163520845, 606.040704572902, 600.535258762014, 531.5768419505154, 40.2763255434648, 560.3831232801041, 591.389894165539, 594.0498656658152, 594.679586404524, 595.3234234615392, 596.0788552209, 593.8075179696635, 602.036507739776, 596.620643853164, 555.5956123349412, 568.4908363944091, 559.4650433607651, 29.130414852890418, 154.67597515103287, 569.4500862321432, 572.3423574111755, 610.5238797454587, 613.0311300047165, 577.0869171084007, 616.5807010077745, 614.210437470341, 612.5930301271212, 604.391021593951, 609.4546540797021, 605.7495349810442, 603.7408305497969, 31.456652571404373, 569.6635707451467, 576.3618047465371, 604.7573966260645, 605.1935587435275, 610.6568777400854, 606.2059153828063, 610.5209582877, 613.6150684715208, 557.3496721413358, 611.0376113288127, 592.5824021740139, 576.4382535412441, 581.9271130706387, 611.8225823857555, 616.3561166086648, 609.4557756931522, 613.6475281509539, 615.7559475881856, 616.3852843093903, 608.6389767510967, 616.5351143099316, 607.5302729013828, 584.4074187265026, 35.179520011435606, 45.99304140822801, 547.2583309371178, 588.7092367821685, 594.3905469416268, 592.9035810406565, 567.7017337901581, 569.8487180163033, 563.9912529249724, 569.9240652529637, 565.7510041178938, 562.1698727854023, 97.29048888154806, 79.48054074222208, 45.355868337347125, 574.6396646741573, 579.0827143676735, 586.3966556386773, 582.5921856907938, 584.4151760010229, 585.8302599715862, 606.3556761127596, 626.6119747920618, 626.7149045535161, 634.2479178910506, 24.337277398209093]
Elapsed: 0.21505735461961692~0.3920521967672753
Time per graph: 0.004388925604481978~0.008001065240148476
Speed: 541.5832277665108~203.92566194077125
Total Time: 2.0154
best val loss: 0.3457086980342865 test_score: 0.9796

Testing...
Test loss: 0.6684 score: 0.9592 time: 0.65s
test Score 0.9592
Epoch Time List: [0.37639823206700385, 0.3745251349173486, 0.3741839430294931, 0.37733101611956954, 0.37574769312050194, 0.3773876429768279, 0.37769253517035395, 0.3751075549516827, 0.3729923429200426, 0.3717101371148601, 0.37525764910969883, 0.3748965618433431, 0.3746307980036363, 0.37378589215222746, 0.3735638309735805, 0.3732948479009792, 0.37375827704090625, 0.37443498300854117, 0.37179624394048005, 0.3725330561865121, 0.3726577899651602, 0.37397791212424636, 0.3737894961377606, 0.3725962790194899, 0.3740334380418062, 0.37539219309110194, 0.37491566594690084, 0.3769662278937176, 0.37642459594644606, 0.38577186898328364, 0.4000510841142386, 0.4008177120704204, 0.4010986869689077, 0.40256917593069375, 0.40222682198509574, 0.4012949229218066, 0.3787009621737525, 0.375361421960406, 0.3745913509046659, 0.3752423881087452, 0.37702242203522474, 0.37942425697110593, 0.40326296410057694, 0.4012536540394649, 0.41923354798927903, 0.48059733083937317, 0.3954363961238414, 6.859159874962643, 4.901147312833928, 0.4090990530094132, 0.40802656509913504, 0.40907405607867986, 0.4032149401027709, 1.9206464091548696, 8.815584134077653, 0.7271287410985678, 0.4073424080852419, 0.40294874901883304, 0.4066309130284935, 0.4058587581384927, 0.40503752103541046, 1.3743229298852384, 2.426551861106418, 1.5459655129816383, 0.43660920509137213, 0.43314315495081246, 0.43133221904281527, 0.40538669598754495, 0.3915080671431497, 0.3919425060739741, 0.39275030710268766, 0.39453249296639115, 0.39828140288591385, 0.39451108407229185, 0.3903914220863953, 0.38814537203870714, 0.4077402849216014, 0.45217874995432794, 0.41081010713241994, 0.42279465589672327, 2.2119055420625955, 8.763317777076736, 2.0299253369448707, 1.744243114022538, 0.8231765491655096, 0.3737597268773243, 0.3719633030705154, 0.3714803089387715, 0.3734264369122684, 0.37394541408866644, 7.478898775880225, 2.992386863916181, 0.4193758680485189, 0.3996129990555346, 0.3806088379351422, 0.3859677469590679, 0.3855156400240958, 0.38482132798526436, 0.37856397905852646, 0.38116576604079455, 0.38041424413677305, 0.38192438799887896, 3.222672971896827, 5.127945251995698, 0.43992241797968745, 0.4365421999245882, 0.43066419812384993, 0.4337546690367162, 1.4352472589816898, 3.787375396117568, 1.658802876016125, 1.167944026994519, 0.585864320048131, 0.41477528598625213, 0.3975003978703171, 0.39871511003002524, 0.3967299780342728, 0.3947185630677268, 0.39307200198527426, 0.39582336286548525, 0.39734051807317883, 0.3940711391624063, 0.3931426372146234, 0.3923234761459753, 0.3942095370730385, 0.3987908430863172, 0.39416258793789893, 0.39568958210293204, 2.7693105079233646, 4.240803399006836, 2.7761927401879802, 0.8654545539757237, 0.4133449780056253, 0.39248825795948505, 0.38537190400529653, 0.38241000194102526, 0.38143701292574406, 0.3817182950442657, 0.38330603390932083, 0.37729308602865785, 0.38754935492761433, 0.40364538109861314, 0.4087244149995968, 4.175897175911814, 1.1167938068974763, 0.41575713094789535, 0.40978615113999695, 0.40720315591897815, 0.395340007962659, 0.3838034140644595, 0.38412120600696653, 0.38527808093931526, 3.186578231980093, 5.31846538791433, 0.9707700938452035, 0.4041344498982653, 0.40334042196627706, 0.3987495880573988, 0.399191188858822, 0.40342729981057346, 0.402631540899165, 0.40772534091956913, 4.34271121409256, 4.963843410951085, 2.8614054111531004, 0.4376893399748951, 0.4209981750464067, 0.4184986539185047, 0.4167013569967821, 0.4160413509234786, 0.4066001200117171, 0.48537492798641324, 4.355209060013294, 4.734165631933138, 0.6668816381134093, 0.40303126780781895, 0.4041271419264376, 0.39972874394152313, 0.39543600706383586, 0.4001601419877261, 0.39912928990088403, 0.40115831093862653, 0.3879940329352394, 0.3791080969385803, 0.3815666871378198, 0.3790833189850673, 0.3873963551595807, 3.828188077895902, 5.616021211026236, 0.43325509887654334, 0.4301818989915773, 0.4227458060486242, 0.42361678380984813, 0.42440694593824446, 0.42533858900424093, 0.4280497229192406, 0.4111376829678193, 3.1295161090092734, 2.775326481787488, 1.6841413890942931, 0.42599249992053956, 0.4198140511289239, 0.4190155630931258, 0.4306324570206925, 0.39183275308459997, 0.3926434420282021, 0.39410938310902566, 0.4770562439225614, 5.2581886989064515, 0.38971547107212245, 0.3865820049541071, 0.38490910711698234, 0.3952248969580978, 0.38690326595678926, 0.3843662799336016, 0.3957766619278118, 3.55523403105326, 3.609549634042196, 1.2965773639734834, 0.4063441299367696, 0.40658999886363745, 0.3974194040056318, 0.3987978649092838, 3.6001200528116897, 7.651727668941021, 2.8028952131280676, 0.3959505290258676, 0.3920335801085457, 0.39313942298758775, 0.39552830799948424, 0.40977179491892457, 0.406336713116616, 0.40775025403127074, 3.54170104698278, 2.053591786068864, 0.6413629199378192, 0.3955622299108654, 0.39226500887889415, 0.39200197404716164, 0.4027169639011845, 0.3978547038277611, 0.39836159301921725, 3.215615506982431, 4.783188074012287, 2.360598027938977, 0.3945215219864622, 0.3873755029635504, 0.38634125888347626, 0.4144276441074908, 0.38442130805924535, 0.38375332206487656, 0.3858278508996591, 7.009595821029507, 1.3972146608866751, 0.4122039220528677, 0.40963145007845014, 0.4070663449820131, 0.40794701711274683, 0.4051949220011011, 0.4092830488225445, 0.40968689194414765, 0.41321010515093803, 3.451904858928174, 0.39089781686197966, 0.3827340028947219, 0.38380007608793676, 0.3874707509530708, 0.3985572779783979, 0.4021255790721625, 0.40124434512108564, 0.4019984267652035, 2.1603467070963234, 6.417135149007663, 2.471764747868292, 0.4132019600365311, 0.39945522497873753, 0.397621170966886, 0.39774526888504624, 0.39845314691774547, 0.3966151209315285, 0.397864049882628, 0.3993426349479705, 0.40219717705622315, 0.4043162548914552, 0.39960604591760784, 0.4078171431319788, 7.455857774009928, 2.3650482239900157, 0.40121608099434525, 0.39637163595762104, 0.394886820926331, 0.39881879300810397, 0.40397183888126165, 0.40164293092675507, 0.4010211080312729, 1.6823173540178686, 6.7130033581051975, 5.765791322221048, 0.39136105601210147, 0.3899857789510861, 0.39124624186661094, 0.39211206091567874, 0.40938949084375054, 0.3872564360499382, 2.859140945132822, 2.8092970229918137, 0.9622290689731017, 0.3972199260024354, 0.3864773678360507, 0.3814722589449957, 0.3889661580324173, 0.38347560190595686, 0.38434272701852024, 0.3854549190727994, 0.48004023695830256, 3.018282183096744, 0.5676829740405083, 0.4039875650778413, 0.4067193689988926, 0.39439986494835466, 0.3886396320303902, 0.40547066496219486, 0.38681282999459654, 1.877537143882364, 4.459628034965135, 3.1649565550033003, 1.3363861348479986, 0.4346634920220822, 0.4071883821161464, 0.38982229481916875, 0.379007285926491, 0.42892516998108476, 0.37986538594122976, 0.3827294319635257, 1.4118485731305555, 3.904342661029659, 0.3942702610511333, 0.38095499691553414, 0.37933779903687537, 0.38432117097545415, 0.38007927290163934, 0.38024810899514705, 0.3921833260683343, 0.3793235900811851, 0.38611058599781245, 0.398494262015447, 0.4045670230407268, 0.40400839410722256, 0.4021751470863819, 0.40122746501583606, 0.40367367095313966, 0.40245570591650903, 0.4043250441318378, 0.40193468902725726, 1.7287328710081056, 5.874142930144444, 0.5163825169438496, 0.4105944752227515, 0.3939163440372795, 0.38207386899739504, 0.38199873198755085, 0.3849191420013085, 0.39485342404805124, 8.767159789800644, 0.4840183148626238, 0.38526118895970285, 0.3840525079285726, 0.3870778230484575, 0.38662516488693655, 0.38002653105650097, 0.3787198959616944, 4.510267983889207, 0.6748328370740637, 0.41403809597250074, 0.3993634389480576, 0.3814206811366603, 0.3822419720236212, 0.382013350026682, 0.3813521359115839, 0.3819361600326374, 0.3767880699597299, 0.3835110029904172, 0.38665990892332047, 0.3977156209293753, 0.39378377003595233, 4.287508560111746, 1.1128321591531858, 0.6359318100148812, 0.40578652499243617, 0.39699398598168045, 0.3788134570932016, 0.3802639519562945, 0.38846858707256615, 0.3878249361878261, 0.3924551389645785, 2.8850969198392704, 4.0451330648502335, 1.3879942980129272, 0.3973101628944278, 0.3977298508398235, 0.3983533139107749, 0.39493591885548085, 0.39194185694213957, 3.3949142899364233, 7.39419196092058, 1.6680286370683461, 0.4032142008654773, 0.3821699470281601, 0.3804574229288846, 0.379988576984033, 0.38801132200751454, 0.4107969640754163, 0.40936523291748017, 0.409414981957525, 0.41091295902151614, 7.170542235835455, 5.219013932044618, 0.37654287787154317, 0.37675101088825613, 0.3844189429655671, 0.3912993079284206, 0.3929520819801837, 0.37985185999423265, 0.39105064515024424, 2.487011177930981, 3.2841244869632646, 1.0860721931094304, 0.4116385809611529, 0.40949253691360354, 0.3926726559875533, 0.3827190489973873, 0.3818587842397392, 0.39107924804557115, 0.40656021900940686, 0.40806369797792286, 0.40740316582378, 0.3933105330215767, 0.3840067950077355, 0.38469301513396204, 0.38438755099195987, 0.3837021930376068, 0.38509649015031755, 0.38435089704580605, 0.38501535006798804, 0.38368607603479177, 0.3868947660084814, 0.38564270304050297, 0.38437109789811075, 0.3854771979385987, 0.3841561059234664, 0.3859409629367292, 0.38597753702197224, 0.3847185851773247, 0.3839258559746668, 0.3851251108571887, 0.3831789770629257, 0.38508706691209227, 0.3855393868871033, 0.3869238418992609, 0.3861808500951156, 0.3898768739309162, 0.3893885639263317, 0.39055403508245945, 0.3910364710027352, 0.38977411901578307, 0.38997570506762713, 0.39133613696321845, 0.39014229783788323, 0.3894282509572804, 0.39183926896657795, 0.409801309928298, 0.3912028099875897, 0.40697080199606717, 0.39160557102877647, 0.3933941089781001, 0.39661471790168434, 0.3932390679838136, 0.39207031298428774, 2.6039124481612816, 2.8449274150189012, 2.6062433240003884, 0.3865326070226729, 0.3795322630321607, 0.37861329689621925, 0.38563380390405655, 0.3842978290049359, 0.38138448097743094, 0.3831664320314303, 0.39279494096990675, 8.48659346299246, 4.073821622994728, 0.40767387906089425, 0.4058440029621124, 0.41829189902637154, 0.4003287011291832, 0.40583495795726776, 3.168678469955921, 1.521223880117759, 1.1344133930979297, 0.38264942297246307, 0.37631687498651445, 0.3791282258462161, 0.38482818799093366, 0.3852538929786533, 0.38642475090455264, 0.38446499209385365, 0.38339076505508274, 0.3864034820580855, 0.3921067470218986, 0.40756949805654585, 0.407138949027285, 0.4056746199494228, 0.4065672369906679, 0.4173177629709244, 0.4133057778235525, 0.4146180379902944, 1.2923233410110697, 4.838216295000166, 0.394286363851279, 0.3879775230307132, 0.3853663749760017, 0.4016964529873803, 0.40893882408272475, 0.4011354699032381, 0.40649465599562973, 0.405319401063025, 7.029877334134653, 3.308227017871104, 1.1908221548656002, 0.41401491593569517, 0.41849047294817865, 0.4108591249678284, 0.4137744411127642, 0.4172808889998123, 0.41622564394492656, 5.648174186004326, 3.8755639009177685, 0.41554004489444196, 0.41434582602232695, 4.630947643192485, 5.308308779960498, 1.6676219858927652, 0.48832357907667756, 0.40350525395479053, 0.38789826387073845, 0.38518094597384334, 3.2548072959762067, 6.405046401079744, 1.7992413931060582, 0.38966852391604334, 0.3911991718923673, 0.3941127579892054, 0.38511991989798844, 0.38889354502316564, 2.5919043590547517, 2.548563984106295, 3.1655393739929423, 1.5010200270917267, 0.5224498871248215, 0.3939804119290784, 0.4102764918934554, 0.4030782171757892, 0.39206114201806486, 1.3699815821601078, 4.198418471030891, 3.8529425030574203, 0.41731923492625356, 0.4109440720640123, 0.4148029339266941, 0.3939845490967855, 0.3904398970771581, 4.259248747956008, 1.663863578112796, 0.5703618321567774, 0.41547227499540895, 0.4177630899939686, 0.427328277961351, 1.6538346769521013, 3.7193131080130115, 4.767517733969726, 0.5250817738706246, 0.43868740706238896, 0.4114020010456443, 0.39970419299788773, 0.40677590295672417, 0.488289742032066, 0.42563283897470683, 4.5133501689415425, 4.815319213899784, 0.4227944650920108, 0.41929373296443373, 0.4192061070352793, 0.4258883629227057, 2.249222101061605, 5.591409487999044, 2.7759724819334224, 0.4142233161255717, 0.41289722989313304, 0.41477286093868315, 1.3820400938857347, 3.5708987751277164, 0.4311227409634739, 0.42209171096328646, 0.41774618881754577, 0.4185743370326236, 4.0319835090776905, 1.5663914639735594, 0.6423807319952175, 0.39209461212158203, 0.38838665292132646, 0.3949563557980582, 0.4064841860672459, 0.39320883306209, 0.39904770080465823, 1.4411895941011608, 4.856056351913139, 4.4273813989711925, 0.9528559320606291, 0.3938538050279021, 0.38918517308775336, 0.39379301003646106, 0.3891308361198753, 0.3871321650221944, 0.38555759692098945, 0.3923647250048816, 0.38751699903514236, 0.4059230788843706, 4.619682348915376, 1.8322329501388595, 0.4239630961092189, 0.42159168189391494, 0.4212609649403021, 0.42348272597882897, 0.4257802970241755, 0.4275361980544403, 2.471350389881991, 5.306880053016357, 1.8099589819321409, 0.40682396094780415, 0.4046790818683803, 0.3945049240719527, 0.4072266520233825, 0.39082920097280294, 0.39174298802390695, 0.3914321050979197, 0.39259053103160113, 0.39595763804391026, 4.472800902090967, 1.3265929118497297, 0.39844313682988286, 0.3934510450344533, 0.3971494409488514, 0.40237239690031856, 0.40292884910013527, 4.8652244380209595, 0.9081927371444181, 0.39656127302441746, 0.3766577310161665, 0.3769622310064733, 0.38025693490635604, 0.37656350899487734, 0.3805326510919258, 0.37954254006035626, 8.308080188115127, 6.123426331905648, 0.39424018398858607, 0.3823755559278652, 0.41176234220620245, 0.39471786201465875, 0.39676151098683476, 0.4027484611142427, 1.3286951871123165, 4.207530883024447, 2.541530270827934, 1.5363115678774193, 0.7959997250000015, 0.38430360588245094, 0.4698802678612992, 0.37895256804767996, 0.3837975838687271, 0.3815311250509694, 0.37795013398863375, 2.757267614128068, 1.117349632899277, 0.39570223493501544, 0.3805445508332923, 0.3854649889981374, 0.38628368300851434, 0.37995892914477736, 0.3759618779877201, 4.303939870907925, 4.200670350925066, 0.5804253950482234, 0.38095254893414676, 0.3781212909379974, 0.3793722951086238, 0.38022608507890254, 0.37992329290136695, 0.3787561720237136, 0.3732764400774613, 4.241336298873648, 4.454587805084884, 6.673780665034428, 1.507352028042078, 0.3965114700840786, 0.39014031598344445, 0.3966591890202835, 0.39240538189187646, 0.3940007978817448, 0.3965593099128455, 2.1704778390703723, 3.9682094550225884, 1.1113304579630494, 0.5835455998312682, 0.3788905510446057, 0.37163429800421, 0.36755977908615023, 0.3725232050055638, 0.3679520981386304, 0.36789553984999657, 0.36806391284335405, 0.368643363006413, 0.3708728919737041, 4.8822405320825055, 3.400545174954459, 2.634550762013532, 0.4100762360030785, 0.3926515900529921, 0.3793227099813521, 0.38009300094563514, 0.37498404004145414, 0.375812339829281, 0.3746644288767129, 0.37598438595887274, 0.37664414301980287, 0.37470934400334954, 2.6784044769592583, 5.661636762903072, 1.8612518169684336, 1.1906986710382625, 0.3790756929665804, 0.37589344091247767, 0.37283443298656493, 0.3763573949690908, 0.3794020189670846, 0.37751967599615455, 0.3788524819537997, 0.37712002207990736, 0.3781848499784246, 0.38916042097844183, 5.926180815091357, 0.44182513596024364, 0.3739295781124383, 0.3693424810189754, 0.3673997230362147, 0.3672590049682185, 0.3710393449291587, 0.3665185780264437, 0.36553514294791967, 0.36320158187299967, 0.3677085890667513, 0.3680724670412019, 0.3693622419377789, 0.37429198797326535, 7.060623903875239, 0.4441797990584746, 0.38219312694855034, 0.3776282190810889, 0.3771534339757636, 0.3735177780035883, 0.3824091018177569, 0.37411274982150644, 0.37081209605094045, 0.3697830728488043, 0.38425974803976715, 0.3918331649620086, 0.3957196499686688, 1.9892706599785015, 2.307794261025265, 1.7252755920635536, 0.3923860938521102, 0.3775480550248176, 0.36506813997402787, 0.36972267006058246, 0.3634927391540259, 0.3668429519748315, 0.36615361308213323, 0.36732436483725905, 0.3703750290442258, 0.38623547193128616, 0.37194085004739463, 6.092905871104449, 1.054252695175819, 0.392908257083036, 0.37423182383645326, 0.3647525029955432, 0.36346016405150294, 0.3654368099523708, 0.36450966191478074, 0.3639407280134037, 0.3719635419547558, 0.3653285930631682, 0.36722228000871837, 0.3841091759968549, 0.3814880690770224, 0.3753733749035746, 0.3607062571682036, 0.3613127830903977, 0.36077827704139054, 0.36202158697415143, 0.36045853688847274, 0.36211808293592185, 0.37294367200229317, 0.36283018602989614, 0.38315547397360206, 3.7133265369338915, 6.168171501951292, 3.012366289854981, 0.38481896684970707, 0.380473273107782, 0.38112100900616497, 0.38099163596052676, 0.39270117902196944, 0.39287288999184966, 0.39690379006788135, 0.39595146116334945, 0.3975003120722249, 1.2273912490345538, 3.091053844895214, 2.775297762011178, 0.43865547399036586, 0.3842800100101158, 0.3829360898816958, 0.38490718184038997, 0.3872428279137239, 0.383003304945305, 0.37555315194185823, 0.3834594620857388, 0.3574240478919819, 0.3552866409299895, 4.684220911120065]
Total Epoch List: [539, 302]
Total Time List: [0.07838089705910534, 2.015425389050506]
========================k_idx:2========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: AEtransGAT
AEtransGAT(
  (gae): GAE(
    (encoder): transGAT(
      (encoder): EdgeFeatureGAT()
      (classifier): Sequential(
        (0): Linear(in_features=64, out_features=64, bias=True)
        (1): ReLU(inplace=True)
        (2): Linear(in_features=64, out_features=2, bias=True)
      )
    )
    (decoder): InnerProductDecoder()
  )
)
Total number of parameters: 1914242
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7d35981f3280>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 3.1756;  Loss pred: 3.1756; Loss self: 0.0000; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6942 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6958 score: 0.5000 time: 0.07s
Epoch 2/1000, LR 0.000000
Train loss: 3.1681;  Loss pred: 3.1681; Loss self: 0.0000; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6941 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6958 score: 0.5000 time: 0.07s
Epoch 3/1000, LR 0.000030
Train loss: 3.2007;  Loss pred: 3.2007; Loss self: 0.0000; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6940 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6956 score: 0.5000 time: 0.07s
Epoch 4/1000, LR 0.000060
Train loss: 3.1649;  Loss pred: 3.1649; Loss self: 0.0000; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6939 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6954 score: 0.5000 time: 0.07s
Epoch 5/1000, LR 0.000090
Train loss: 3.1816;  Loss pred: 3.1816; Loss self: 0.0000; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6937 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6951 score: 0.5000 time: 0.07s
Epoch 6/1000, LR 0.000120
Train loss: 3.1652;  Loss pred: 3.1652; Loss self: 0.0000; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6935 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6948 score: 0.5000 time: 0.07s
Epoch 7/1000, LR 0.000150
Train loss: 3.1072;  Loss pred: 3.1072; Loss self: 0.0000; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6932 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6945 score: 0.5000 time: 0.07s
Epoch 8/1000, LR 0.000180
Train loss: 3.0757;  Loss pred: 3.0757; Loss self: 0.0000; time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6930 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6941 score: 0.5000 time: 0.07s
Epoch 9/1000, LR 0.000210
Train loss: 3.0391;  Loss pred: 3.0391; Loss self: 0.0000; time: 3.92s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6928 score: 0.5102 time: 1.51s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6937 score: 0.5000 time: 0.94s
Epoch 10/1000, LR 0.000240
Train loss: 2.9698;  Loss pred: 2.9698; Loss self: 0.0000; time: 0.54s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6926 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6934 score: 0.5000 time: 0.08s
Epoch 11/1000, LR 0.000270
Train loss: 2.9460;  Loss pred: 2.9460; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6924 score: 0.5102 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6931 score: 0.5000 time: 0.08s
Epoch 12/1000, LR 0.000270
Train loss: 2.8518;  Loss pred: 2.8518; Loss self: 0.0000; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6923 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6929 score: 0.5000 time: 0.07s
Epoch 13/1000, LR 0.000270
Train loss: 2.8027;  Loss pred: 2.8027; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6922 score: 0.5102 time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6927 score: 0.5000 time: 0.07s
Epoch 14/1000, LR 0.000270
Train loss: 2.7741;  Loss pred: 2.7741; Loss self: 0.0000; time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6922 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6926 score: 0.5000 time: 0.07s
Epoch 15/1000, LR 0.000270
Train loss: 2.6814;  Loss pred: 2.6814; Loss self: 0.0000; time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6922 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6924 score: 0.5000 time: 0.07s
Epoch 16/1000, LR 0.000270
Train loss: 2.6699;  Loss pred: 2.6699; Loss self: 0.0000; time: 0.24s
Val loss: 0.6922 score: 0.5306 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6923 score: 0.5000 time: 0.07s
Epoch 17/1000, LR 0.000270
Train loss: 2.5984;  Loss pred: 2.5984; Loss self: 0.0000; time: 0.23s
Val loss: 0.6922 score: 0.5510 time: 0.07s
Test loss: 0.6923 score: 0.5208 time: 0.07s
Epoch 18/1000, LR 0.000270
Train loss: 2.5612;  Loss pred: 2.5612; Loss self: 0.0000; time: 0.23s
Val loss: 0.6921 score: 0.7755 time: 0.07s
Test loss: 0.6922 score: 0.8542 time: 0.07s
Epoch 19/1000, LR 0.000270
Train loss: 2.4952;  Loss pred: 2.4952; Loss self: 0.0000; time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6921 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6921 score: 0.5000 time: 0.07s
Epoch 20/1000, LR 0.000270
Train loss: 2.4765;  Loss pred: 2.4765; Loss self: 0.0000; time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6921 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6921 score: 0.5000 time: 0.07s
Epoch 21/1000, LR 0.000270
Train loss: 2.4145;  Loss pred: 2.4145; Loss self: 0.0000; time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6921 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6920 score: 0.5000 time: 0.07s
Epoch 22/1000, LR 0.000270
Train loss: 2.3664;  Loss pred: 2.3664; Loss self: 0.0000; time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6920 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6919 score: 0.5000 time: 0.07s
Epoch 23/1000, LR 0.000270
Train loss: 2.2983;  Loss pred: 2.2983; Loss self: 0.0000; time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6920 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6918 score: 0.5000 time: 0.07s
Epoch 24/1000, LR 0.000270
Train loss: 2.2838;  Loss pred: 2.2838; Loss self: 0.0000; time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6919 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6917 score: 0.5000 time: 0.07s
Epoch 25/1000, LR 0.000270
Train loss: 2.2354;  Loss pred: 2.2354; Loss self: 0.0000; time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6919 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6917 score: 0.5000 time: 0.07s
Epoch 26/1000, LR 0.000270
Train loss: 2.2376;  Loss pred: 2.2376; Loss self: 0.0000; time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6919 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6916 score: 0.5000 time: 0.07s
Epoch 27/1000, LR 0.000270
Train loss: 2.1619;  Loss pred: 2.1619; Loss self: 0.0000; time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6919 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6916 score: 0.5000 time: 0.07s
Epoch 28/1000, LR 0.000270
Train loss: 2.1294;  Loss pred: 2.1294; Loss self: 0.0000; time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6918 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6915 score: 0.5000 time: 0.07s
Epoch 29/1000, LR 0.000270
Train loss: 2.0907;  Loss pred: 2.0907; Loss self: 0.0000; time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6917 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6914 score: 0.5000 time: 0.07s
Epoch 30/1000, LR 0.000270
Train loss: 2.0686;  Loss pred: 2.0686; Loss self: 0.0000; time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6917 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6913 score: 0.5000 time: 0.07s
Epoch 31/1000, LR 0.000270
Train loss: 2.0289;  Loss pred: 2.0289; Loss self: 0.0000; time: 2.47s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6916 score: 0.4898 time: 0.67s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6912 score: 0.5000 time: 0.76s
Epoch 32/1000, LR 0.000270
Train loss: 1.9740;  Loss pred: 1.9740; Loss self: 0.0000; time: 1.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6915 score: 0.4898 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6911 score: 0.5000 time: 0.08s
Epoch 33/1000, LR 0.000270
Train loss: 1.9714;  Loss pred: 1.9714; Loss self: 0.0000; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6914 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6910 score: 0.5000 time: 0.07s
Epoch 34/1000, LR 0.000270
Train loss: 1.9497;  Loss pred: 1.9497; Loss self: 0.0000; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6913 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6909 score: 0.5000 time: 0.07s
Epoch 35/1000, LR 0.000270
Train loss: 1.8889;  Loss pred: 1.8889; Loss self: 0.0000; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6912 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6908 score: 0.5000 time: 0.07s
Epoch 36/1000, LR 0.000270
Train loss: 1.8592;  Loss pred: 1.8592; Loss self: 0.0000; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6910 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6907 score: 0.5000 time: 0.07s
Epoch 37/1000, LR 0.000270
Train loss: 1.8234;  Loss pred: 1.8234; Loss self: 0.0000; time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6909 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6906 score: 0.5000 time: 0.07s
Epoch 38/1000, LR 0.000270
Train loss: 1.8015;  Loss pred: 1.8015; Loss self: 0.0000; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6908 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6904 score: 0.5000 time: 0.07s
Epoch 39/1000, LR 0.000269
Train loss: 1.7844;  Loss pred: 1.7844; Loss self: 0.0000; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6906 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6903 score: 0.5000 time: 0.07s
Epoch 40/1000, LR 0.000269
Train loss: 1.7510;  Loss pred: 1.7510; Loss self: 0.0000; time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6905 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6902 score: 0.5000 time: 0.07s
Epoch 41/1000, LR 0.000269
Train loss: 1.7108;  Loss pred: 1.7108; Loss self: 0.0000; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6903 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6900 score: 0.5000 time: 0.07s
Epoch 42/1000, LR 0.000269
Train loss: 1.6901;  Loss pred: 1.6901; Loss self: 0.0000; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6902 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6899 score: 0.5000 time: 0.07s
Epoch 43/1000, LR 0.000269
Train loss: 1.6715;  Loss pred: 1.6715; Loss self: 0.0000; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6900 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6897 score: 0.5000 time: 0.07s
Epoch 44/1000, LR 0.000269
Train loss: 1.6444;  Loss pred: 1.6444; Loss self: 0.0000; time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6898 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6895 score: 0.5000 time: 0.09s
Epoch 45/1000, LR 0.000269
Train loss: 1.6218;  Loss pred: 1.6218; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6897 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6894 score: 0.5000 time: 0.07s
Epoch 46/1000, LR 0.000269
Train loss: 1.5761;  Loss pred: 1.5761; Loss self: 0.0000; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6895 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6892 score: 0.5000 time: 0.07s
Epoch 47/1000, LR 0.000269
Train loss: 1.5896;  Loss pred: 1.5896; Loss self: 0.0000; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6893 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6890 score: 0.5000 time: 0.07s
Epoch 48/1000, LR 0.000269
Train loss: 1.5605;  Loss pred: 1.5605; Loss self: 0.0000; time: 2.39s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6891 score: 0.4898 time: 0.67s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6888 score: 0.5000 time: 1.33s
Epoch 49/1000, LR 0.000269
Train loss: 1.5545;  Loss pred: 1.5545; Loss self: 0.0000; time: 2.98s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6889 score: 0.4898 time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6886 score: 0.5000 time: 0.25s
Epoch 50/1000, LR 0.000269
Train loss: 1.5193;  Loss pred: 1.5193; Loss self: 0.0000; time: 0.38s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6887 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6883 score: 0.5000 time: 0.07s
Epoch 51/1000, LR 0.000269
Train loss: 1.5067;  Loss pred: 1.5067; Loss self: 0.0000; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6885 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6881 score: 0.5000 time: 0.07s
Epoch 52/1000, LR 0.000269
Train loss: 1.4695;  Loss pred: 1.4695; Loss self: 0.0000; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6883 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6879 score: 0.5000 time: 0.07s
Epoch 53/1000, LR 0.000269
Train loss: 1.4581;  Loss pred: 1.4581; Loss self: 0.0000; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6881 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6877 score: 0.5000 time: 0.07s
Epoch 54/1000, LR 0.000269
Train loss: 1.4488;  Loss pred: 1.4488; Loss self: 0.0000; time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6879 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6875 score: 0.5000 time: 0.07s
Epoch 55/1000, LR 0.000269
Train loss: 1.4258;  Loss pred: 1.4258; Loss self: 0.0000; time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6877 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6872 score: 0.5000 time: 0.07s
Epoch 56/1000, LR 0.000269
Train loss: 1.4041;  Loss pred: 1.4041; Loss self: 0.0000; time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6875 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6870 score: 0.5000 time: 0.07s
Epoch 57/1000, LR 0.000269
Train loss: 1.4061;  Loss pred: 1.4061; Loss self: 0.0000; time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6873 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6868 score: 0.5000 time: 0.07s
Epoch 58/1000, LR 0.000269
Train loss: 1.3824;  Loss pred: 1.3824; Loss self: 0.0000; time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6871 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6865 score: 0.5000 time: 0.07s
Epoch 59/1000, LR 0.000268
Train loss: 1.3771;  Loss pred: 1.3771; Loss self: 0.0000; time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6868 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6862 score: 0.5000 time: 0.07s
Epoch 60/1000, LR 0.000268
Train loss: 1.3545;  Loss pred: 1.3545; Loss self: 0.0000; time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6866 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6860 score: 0.5000 time: 0.08s
Epoch 61/1000, LR 0.000268
Train loss: 1.3369;  Loss pred: 1.3369; Loss self: 0.0000; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6863 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6857 score: 0.5000 time: 0.08s
Epoch 62/1000, LR 0.000268
Train loss: 1.3311;  Loss pred: 1.3311; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6861 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6854 score: 0.5000 time: 0.08s
Epoch 63/1000, LR 0.000268
Train loss: 1.3152;  Loss pred: 1.3152; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6858 score: 0.4898 time: 0.08s
Test loss: 0.6851 score: 0.5208 time: 0.09s
Epoch 64/1000, LR 0.000268
Train loss: 1.3057;  Loss pred: 1.3057; Loss self: 0.0000; time: 3.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6855 score: 0.4898 time: 0.19s
Test loss: 0.6848 score: 0.5208 time: 1.05s
Epoch 65/1000, LR 0.000268
Train loss: 1.2917;  Loss pred: 1.2917; Loss self: 0.0000; time: 2.32s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6853 score: 0.4898 time: 0.09s
Test loss: 0.6844 score: 0.5208 time: 0.08s
Epoch 66/1000, LR 0.000268
Train loss: 1.2783;  Loss pred: 1.2783; Loss self: 0.0000; time: 0.25s
Val loss: 0.6850 score: 0.5102 time: 0.07s
Test loss: 0.6841 score: 0.5417 time: 0.08s
Epoch 67/1000, LR 0.000268
Train loss: 1.2702;  Loss pred: 1.2702; Loss self: 0.0000; time: 0.25s
Val loss: 0.6847 score: 0.5102 time: 0.07s
Test loss: 0.6838 score: 0.5417 time: 0.08s
Epoch 68/1000, LR 0.000268
Train loss: 1.2608;  Loss pred: 1.2608; Loss self: 0.0000; time: 0.25s
Val loss: 0.6844 score: 0.5102 time: 0.07s
Test loss: 0.6834 score: 0.5417 time: 0.07s
Epoch 69/1000, LR 0.000268
Train loss: 1.2461;  Loss pred: 1.2461; Loss self: 0.0000; time: 0.24s
Val loss: 0.6840 score: 0.5102 time: 0.07s
Test loss: 0.6830 score: 0.5625 time: 0.07s
Epoch 70/1000, LR 0.000268
Train loss: 1.2405;  Loss pred: 1.2405; Loss self: 0.0000; time: 0.24s
Val loss: 0.6837 score: 0.5306 time: 0.07s
Test loss: 0.6826 score: 0.5625 time: 0.07s
Epoch 71/1000, LR 0.000268
Train loss: 1.2310;  Loss pred: 1.2310; Loss self: 0.0000; time: 0.24s
Val loss: 0.6833 score: 0.5306 time: 0.07s
Test loss: 0.6822 score: 0.5833 time: 0.08s
Epoch 72/1000, LR 0.000267
Train loss: 1.2217;  Loss pred: 1.2217; Loss self: 0.0000; time: 0.24s
Val loss: 0.6829 score: 0.5510 time: 0.07s
Test loss: 0.6817 score: 0.5833 time: 0.07s
Epoch 73/1000, LR 0.000267
Train loss: 1.2184;  Loss pred: 1.2184; Loss self: 0.0000; time: 0.24s
Val loss: 0.6825 score: 0.5714 time: 0.07s
Test loss: 0.6813 score: 0.6042 time: 0.08s
Epoch 74/1000, LR 0.000267
Train loss: 1.2077;  Loss pred: 1.2077; Loss self: 0.0000; time: 0.24s
Val loss: 0.6821 score: 0.5714 time: 0.07s
Test loss: 0.6808 score: 0.6042 time: 0.07s
Epoch 75/1000, LR 0.000267
Train loss: 1.1949;  Loss pred: 1.1949; Loss self: 0.0000; time: 0.24s
Val loss: 0.6817 score: 0.5918 time: 0.07s
Test loss: 0.6804 score: 0.6042 time: 0.07s
Epoch 76/1000, LR 0.000267
Train loss: 1.1907;  Loss pred: 1.1907; Loss self: 0.0000; time: 0.25s
Val loss: 0.6813 score: 0.5918 time: 0.20s
Test loss: 0.6799 score: 0.6458 time: 0.64s
Epoch 77/1000, LR 0.000267
Train loss: 1.1823;  Loss pred: 1.1823; Loss self: 0.0000; time: 2.65s
Val loss: 0.6808 score: 0.6327 time: 0.19s
Test loss: 0.6794 score: 0.6667 time: 0.32s
Epoch 78/1000, LR 0.000267
Train loss: 1.1752;  Loss pred: 1.1752; Loss self: 0.0000; time: 0.97s
Val loss: 0.6804 score: 0.6327 time: 0.25s
Test loss: 0.6788 score: 0.6875 time: 0.56s
Epoch 79/1000, LR 0.000267
Train loss: 1.1637;  Loss pred: 1.1637; Loss self: 0.0000; time: 1.18s
Val loss: 0.6799 score: 0.6327 time: 0.18s
Test loss: 0.6783 score: 0.6875 time: 0.13s
Epoch 80/1000, LR 0.000267
Train loss: 1.1603;  Loss pred: 1.1603; Loss self: 0.0000; time: 0.39s
Val loss: 0.6794 score: 0.6939 time: 0.07s
Test loss: 0.6778 score: 0.7500 time: 0.08s
Epoch 81/1000, LR 0.000267
Train loss: 1.1564;  Loss pred: 1.1564; Loss self: 0.0000; time: 0.24s
Val loss: 0.6790 score: 0.7143 time: 0.07s
Test loss: 0.6772 score: 0.7708 time: 0.08s
Epoch 82/1000, LR 0.000267
Train loss: 1.1477;  Loss pred: 1.1477; Loss self: 0.0000; time: 0.23s
Val loss: 0.6784 score: 0.7143 time: 0.07s
Test loss: 0.6766 score: 0.7708 time: 0.07s
Epoch 83/1000, LR 0.000266
Train loss: 1.1383;  Loss pred: 1.1383; Loss self: 0.0000; time: 0.23s
Val loss: 0.6779 score: 0.7551 time: 0.07s
Test loss: 0.6760 score: 0.7708 time: 0.07s
Epoch 84/1000, LR 0.000266
Train loss: 1.1268;  Loss pred: 1.1268; Loss self: 0.0000; time: 0.23s
Val loss: 0.6774 score: 0.7755 time: 0.07s
Test loss: 0.6753 score: 0.7708 time: 0.07s
Epoch 85/1000, LR 0.000266
Train loss: 1.1265;  Loss pred: 1.1265; Loss self: 0.0000; time: 0.23s
Val loss: 0.6769 score: 0.7755 time: 0.07s
Test loss: 0.6747 score: 0.7917 time: 0.07s
Epoch 86/1000, LR 0.000266
Train loss: 1.1224;  Loss pred: 1.1224; Loss self: 0.0000; time: 0.24s
Val loss: 0.6763 score: 0.7755 time: 0.07s
Test loss: 0.6740 score: 0.7917 time: 0.07s
Epoch 87/1000, LR 0.000266
Train loss: 1.1160;  Loss pred: 1.1160; Loss self: 0.0000; time: 0.24s
Val loss: 0.6758 score: 0.7755 time: 0.07s
Test loss: 0.6733 score: 0.7917 time: 0.07s
Epoch 88/1000, LR 0.000266
Train loss: 1.1128;  Loss pred: 1.1128; Loss self: 0.0000; time: 0.24s
Val loss: 0.6752 score: 0.7755 time: 0.07s
Test loss: 0.6726 score: 0.7917 time: 0.08s
Epoch 89/1000, LR 0.000266
Train loss: 1.1033;  Loss pred: 1.1033; Loss self: 0.0000; time: 0.25s
Val loss: 0.6746 score: 0.7755 time: 0.08s
Test loss: 0.6719 score: 0.7917 time: 0.11s
Epoch 90/1000, LR 0.000266
Train loss: 1.0988;  Loss pred: 1.0988; Loss self: 0.0000; time: 0.25s
Val loss: 0.6739 score: 0.7755 time: 0.07s
Test loss: 0.6711 score: 0.8125 time: 0.08s
Epoch 91/1000, LR 0.000266
Train loss: 1.1015;  Loss pred: 1.1015; Loss self: 0.0000; time: 0.25s
Val loss: 0.6733 score: 0.7755 time: 0.07s
Test loss: 0.6704 score: 0.8125 time: 0.08s
Epoch 92/1000, LR 0.000266
Train loss: 1.0934;  Loss pred: 1.0934; Loss self: 0.0000; time: 0.25s
Val loss: 0.6726 score: 0.7755 time: 0.07s
Test loss: 0.6696 score: 0.8125 time: 0.07s
Epoch 93/1000, LR 0.000265
Train loss: 1.0872;  Loss pred: 1.0872; Loss self: 0.0000; time: 0.25s
Val loss: 0.6719 score: 0.7755 time: 0.11s
Test loss: 0.6687 score: 0.8125 time: 0.07s
Epoch 94/1000, LR 0.000265
Train loss: 1.0823;  Loss pred: 1.0823; Loss self: 0.0000; time: 0.24s
Val loss: 0.6712 score: 0.7755 time: 0.07s
Test loss: 0.6679 score: 0.8125 time: 0.07s
Epoch 95/1000, LR 0.000265
Train loss: 1.0804;  Loss pred: 1.0804; Loss self: 0.0000; time: 0.24s
Val loss: 0.6705 score: 0.7959 time: 0.07s
Test loss: 0.6670 score: 0.8125 time: 0.07s
Epoch 96/1000, LR 0.000265
Train loss: 1.0738;  Loss pred: 1.0738; Loss self: 0.0000; time: 0.24s
Val loss: 0.6698 score: 0.7959 time: 0.07s
Test loss: 0.6661 score: 0.8125 time: 0.07s
Epoch 97/1000, LR 0.000265
Train loss: 1.0716;  Loss pred: 1.0716; Loss self: 0.0000; time: 0.24s
Val loss: 0.6690 score: 0.8163 time: 0.07s
Test loss: 0.6652 score: 0.8333 time: 0.07s
Epoch 98/1000, LR 0.000265
Train loss: 1.0644;  Loss pred: 1.0644; Loss self: 0.0000; time: 0.26s
Val loss: 0.6682 score: 0.8163 time: 0.07s
Test loss: 0.6643 score: 0.8542 time: 0.07s
Epoch 99/1000, LR 0.000265
Train loss: 1.0609;  Loss pred: 1.0609; Loss self: 0.0000; time: 0.24s
Val loss: 0.6674 score: 0.8163 time: 0.07s
Test loss: 0.6633 score: 0.8750 time: 0.07s
Epoch 100/1000, LR 0.000265
Train loss: 1.0560;  Loss pred: 1.0560; Loss self: 0.0000; time: 0.24s
Val loss: 0.6666 score: 0.8163 time: 0.07s
Test loss: 0.6623 score: 0.8750 time: 0.07s
Epoch 101/1000, LR 0.000265
Train loss: 1.0501;  Loss pred: 1.0501; Loss self: 0.0000; time: 0.24s
Val loss: 0.6658 score: 0.8367 time: 0.07s
Test loss: 0.6612 score: 0.8958 time: 0.07s
Epoch 102/1000, LR 0.000264
Train loss: 1.0497;  Loss pred: 1.0497; Loss self: 0.0000; time: 0.24s
Val loss: 0.6649 score: 0.8367 time: 0.07s
Test loss: 0.6602 score: 0.8958 time: 0.16s
Epoch 103/1000, LR 0.000264
Train loss: 1.0487;  Loss pred: 1.0487; Loss self: 0.0000; time: 0.24s
Val loss: 0.6640 score: 0.8367 time: 0.07s
Test loss: 0.6591 score: 0.8958 time: 0.07s
Epoch 104/1000, LR 0.000264
Train loss: 1.0369;  Loss pred: 1.0369; Loss self: 0.0000; time: 0.23s
Val loss: 0.6630 score: 0.8367 time: 0.07s
Test loss: 0.6579 score: 0.8958 time: 0.08s
Epoch 105/1000, LR 0.000264
Train loss: 1.0355;  Loss pred: 1.0355; Loss self: 0.0000; time: 0.24s
Val loss: 0.6620 score: 0.8367 time: 0.07s
Test loss: 0.6567 score: 0.8958 time: 0.07s
Epoch 106/1000, LR 0.000264
Train loss: 1.0310;  Loss pred: 1.0310; Loss self: 0.0000; time: 0.24s
Val loss: 0.6610 score: 0.8367 time: 0.07s
Test loss: 0.6554 score: 0.8958 time: 0.07s
Epoch 107/1000, LR 0.000264
Train loss: 1.0304;  Loss pred: 1.0304; Loss self: 0.0000; time: 0.24s
Val loss: 0.6599 score: 0.8367 time: 0.07s
Test loss: 0.6542 score: 0.8958 time: 0.07s
Epoch 108/1000, LR 0.000264
Train loss: 1.0269;  Loss pred: 1.0269; Loss self: 0.0000; time: 4.68s
Val loss: 0.6589 score: 0.8367 time: 1.01s
Test loss: 0.6529 score: 0.8958 time: 1.67s
Epoch 109/1000, LR 0.000264
Train loss: 1.0243;  Loss pred: 1.0243; Loss self: 0.0000; time: 4.99s
Val loss: 0.6578 score: 0.8571 time: 0.08s
Test loss: 0.6516 score: 0.9167 time: 0.08s
Epoch 110/1000, LR 0.000263
Train loss: 1.0217;  Loss pred: 1.0217; Loss self: 0.0000; time: 0.26s
Val loss: 0.6567 score: 0.8571 time: 0.07s
Test loss: 0.6502 score: 0.9167 time: 0.08s
Epoch 111/1000, LR 0.000263
Train loss: 1.0178;  Loss pred: 1.0178; Loss self: 0.0000; time: 0.25s
Val loss: 0.6555 score: 0.8571 time: 0.07s
Test loss: 0.6488 score: 0.9167 time: 0.07s
Epoch 112/1000, LR 0.000263
Train loss: 1.0136;  Loss pred: 1.0136; Loss self: 0.0000; time: 0.24s
Val loss: 0.6543 score: 0.8571 time: 0.07s
Test loss: 0.6474 score: 0.9167 time: 0.07s
Epoch 113/1000, LR 0.000263
Train loss: 1.0123;  Loss pred: 1.0123; Loss self: 0.0000; time: 0.24s
Val loss: 0.6531 score: 0.8571 time: 0.07s
Test loss: 0.6460 score: 0.9167 time: 0.07s
Epoch 114/1000, LR 0.000263
Train loss: 1.0073;  Loss pred: 1.0073; Loss self: 0.0000; time: 0.23s
Val loss: 0.6519 score: 0.8571 time: 0.07s
Test loss: 0.6445 score: 0.9167 time: 0.07s
Epoch 115/1000, LR 0.000263
Train loss: 1.0039;  Loss pred: 1.0039; Loss self: 0.0000; time: 0.23s
Val loss: 0.6507 score: 0.8571 time: 0.07s
Test loss: 0.6430 score: 0.9167 time: 0.07s
Epoch 116/1000, LR 0.000263
Train loss: 1.0046;  Loss pred: 1.0046; Loss self: 0.0000; time: 0.23s
Val loss: 0.6494 score: 0.8571 time: 0.07s
Test loss: 0.6414 score: 0.9167 time: 0.07s
Epoch 117/1000, LR 0.000262
Train loss: 0.9962;  Loss pred: 0.9962; Loss self: 0.0000; time: 0.23s
Val loss: 0.6481 score: 0.8571 time: 0.07s
Test loss: 0.6398 score: 0.9167 time: 0.08s
Epoch 118/1000, LR 0.000262
Train loss: 0.9944;  Loss pred: 0.9944; Loss self: 0.0000; time: 0.24s
Val loss: 0.6467 score: 0.8571 time: 0.07s
Test loss: 0.6382 score: 0.9167 time: 0.08s
Epoch 119/1000, LR 0.000262
Train loss: 0.9964;  Loss pred: 0.9964; Loss self: 0.0000; time: 0.26s
Val loss: 0.6454 score: 0.8571 time: 0.08s
Test loss: 0.6365 score: 0.9167 time: 0.08s
Epoch 120/1000, LR 0.000262
Train loss: 0.9898;  Loss pred: 0.9898; Loss self: 0.0000; time: 0.25s
Val loss: 0.6440 score: 0.8571 time: 0.07s
Test loss: 0.6348 score: 0.9167 time: 0.08s
Epoch 121/1000, LR 0.000262
Train loss: 0.9866;  Loss pred: 0.9866; Loss self: 0.0000; time: 2.74s
Val loss: 0.6426 score: 0.8571 time: 0.72s
Test loss: 0.6331 score: 0.9167 time: 0.69s
Epoch 122/1000, LR 0.000262
Train loss: 0.9828;  Loss pred: 0.9828; Loss self: 0.0000; time: 4.48s
Val loss: 0.6411 score: 0.8571 time: 0.15s
Test loss: 0.6313 score: 0.9167 time: 0.57s
Epoch 123/1000, LR 0.000262
Train loss: 0.9793;  Loss pred: 0.9793; Loss self: 0.0000; time: 0.29s
Val loss: 0.6396 score: 0.8571 time: 0.07s
Test loss: 0.6295 score: 0.9167 time: 0.07s
Epoch 124/1000, LR 0.000261
Train loss: 0.9763;  Loss pred: 0.9763; Loss self: 0.0000; time: 0.23s
Val loss: 0.6381 score: 0.8571 time: 0.07s
Test loss: 0.6277 score: 0.9167 time: 0.07s
Epoch 125/1000, LR 0.000261
Train loss: 0.9753;  Loss pred: 0.9753; Loss self: 0.0000; time: 0.23s
Val loss: 0.6366 score: 0.8776 time: 0.07s
Test loss: 0.6258 score: 0.9167 time: 0.07s
Epoch 126/1000, LR 0.000261
Train loss: 0.9708;  Loss pred: 0.9708; Loss self: 0.0000; time: 0.23s
Val loss: 0.6350 score: 0.8776 time: 0.07s
Test loss: 0.6239 score: 0.9167 time: 0.07s
Epoch 127/1000, LR 0.000261
Train loss: 0.9698;  Loss pred: 0.9698; Loss self: 0.0000; time: 0.23s
Val loss: 0.6334 score: 0.8776 time: 0.07s
Test loss: 0.6219 score: 0.9167 time: 0.07s
Epoch 128/1000, LR 0.000261
Train loss: 0.9659;  Loss pred: 0.9659; Loss self: 0.0000; time: 0.24s
Val loss: 0.6318 score: 0.8776 time: 0.07s
Test loss: 0.6199 score: 0.9167 time: 0.07s
Epoch 129/1000, LR 0.000261
Train loss: 0.9634;  Loss pred: 0.9634; Loss self: 0.0000; time: 0.23s
Val loss: 0.6302 score: 0.8776 time: 0.07s
Test loss: 0.6179 score: 0.9167 time: 0.07s
Epoch 130/1000, LR 0.000260
Train loss: 0.9608;  Loss pred: 0.9608; Loss self: 0.0000; time: 0.23s
Val loss: 0.6285 score: 0.8776 time: 0.07s
Test loss: 0.6158 score: 0.9167 time: 0.07s
Epoch 131/1000, LR 0.000260
Train loss: 0.9573;  Loss pred: 0.9573; Loss self: 0.0000; time: 0.23s
Val loss: 0.6268 score: 0.8776 time: 0.07s
Test loss: 0.6137 score: 0.9167 time: 0.07s
Epoch 132/1000, LR 0.000260
Train loss: 0.9534;  Loss pred: 0.9534; Loss self: 0.0000; time: 0.23s
Val loss: 0.6250 score: 0.8776 time: 0.07s
Test loss: 0.6115 score: 0.9167 time: 0.07s
Epoch 133/1000, LR 0.000260
Train loss: 0.9482;  Loss pred: 0.9482; Loss self: 0.0000; time: 0.23s
Val loss: 0.6232 score: 0.8571 time: 0.07s
Test loss: 0.6093 score: 0.9167 time: 0.07s
Epoch 134/1000, LR 0.000260
Train loss: 0.9494;  Loss pred: 0.9494; Loss self: 0.0000; time: 1.12s
Val loss: 0.6214 score: 0.8571 time: 0.57s
Test loss: 0.6071 score: 0.9167 time: 0.91s
Epoch 135/1000, LR 0.000260
Train loss: 0.9461;  Loss pred: 0.9461; Loss self: 0.0000; time: 5.14s
Val loss: 0.6196 score: 0.8571 time: 1.36s
Test loss: 0.6048 score: 0.9167 time: 0.25s
Epoch 136/1000, LR 0.000260
Train loss: 0.9421;  Loss pred: 0.9421; Loss self: 0.0000; time: 0.62s
Val loss: 0.6177 score: 0.8571 time: 0.13s
Test loss: 0.6025 score: 0.9167 time: 0.18s
Epoch 137/1000, LR 0.000259
Train loss: 0.9402;  Loss pred: 0.9402; Loss self: 0.0000; time: 0.39s
Val loss: 0.6158 score: 0.8571 time: 0.07s
Test loss: 0.6002 score: 0.9167 time: 0.08s
Epoch 138/1000, LR 0.000259
Train loss: 0.9367;  Loss pred: 0.9367; Loss self: 0.0000; time: 0.25s
Val loss: 0.6138 score: 0.8367 time: 0.07s
Test loss: 0.5978 score: 0.9167 time: 0.07s
Epoch 139/1000, LR 0.000259
Train loss: 0.9355;  Loss pred: 0.9355; Loss self: 0.0000; time: 0.24s
Val loss: 0.6118 score: 0.8367 time: 0.07s
Test loss: 0.5953 score: 0.9167 time: 0.07s
Epoch 140/1000, LR 0.000259
Train loss: 0.9304;  Loss pred: 0.9304; Loss self: 0.0000; time: 0.24s
Val loss: 0.6098 score: 0.8367 time: 0.07s
Test loss: 0.5928 score: 0.9167 time: 0.07s
Epoch 141/1000, LR 0.000259
Train loss: 0.9291;  Loss pred: 0.9291; Loss self: 0.0000; time: 0.23s
Val loss: 0.6077 score: 0.8367 time: 0.07s
Test loss: 0.5903 score: 0.9167 time: 0.07s
Epoch 142/1000, LR 0.000259
Train loss: 0.9254;  Loss pred: 0.9254; Loss self: 0.0000; time: 0.24s
Val loss: 0.6057 score: 0.8367 time: 0.07s
Test loss: 0.5878 score: 0.9167 time: 0.07s
Epoch 143/1000, LR 0.000258
Train loss: 0.9228;  Loss pred: 0.9228; Loss self: 0.0000; time: 0.24s
Val loss: 0.6036 score: 0.8367 time: 0.07s
Test loss: 0.5852 score: 0.9167 time: 0.07s
Epoch 144/1000, LR 0.000258
Train loss: 0.9179;  Loss pred: 0.9179; Loss self: 0.0000; time: 0.24s
Val loss: 0.6015 score: 0.8367 time: 0.07s
Test loss: 0.5826 score: 0.9167 time: 0.07s
Epoch 145/1000, LR 0.000258
Train loss: 0.9162;  Loss pred: 0.9162; Loss self: 0.0000; time: 0.24s
Val loss: 0.5995 score: 0.8367 time: 0.07s
Test loss: 0.5800 score: 0.9167 time: 0.07s
Epoch 146/1000, LR 0.000258
Train loss: 0.9137;  Loss pred: 0.9137; Loss self: 0.0000; time: 0.25s
Val loss: 0.5973 score: 0.8367 time: 0.07s
Test loss: 0.5774 score: 0.9167 time: 0.08s
Epoch 147/1000, LR 0.000258
Train loss: 0.9094;  Loss pred: 0.9094; Loss self: 0.0000; time: 1.43s
Val loss: 0.5952 score: 0.8367 time: 0.76s
Test loss: 0.5747 score: 0.9167 time: 0.84s
Epoch 148/1000, LR 0.000257
Train loss: 0.9070;  Loss pred: 0.9070; Loss self: 0.0000; time: 1.99s
Val loss: 0.5931 score: 0.8367 time: 0.68s
Test loss: 0.5720 score: 0.9167 time: 1.06s
Epoch 149/1000, LR 0.000257
Train loss: 0.9059;  Loss pred: 0.9059; Loss self: 0.0000; time: 1.76s
Val loss: 0.5908 score: 0.8367 time: 0.07s
Test loss: 0.5692 score: 0.9167 time: 0.07s
Epoch 150/1000, LR 0.000257
Train loss: 0.9018;  Loss pred: 0.9018; Loss self: 0.0000; time: 0.23s
Val loss: 0.5886 score: 0.8367 time: 0.07s
Test loss: 0.5664 score: 0.9167 time: 0.07s
Epoch 151/1000, LR 0.000257
Train loss: 0.8983;  Loss pred: 0.8983; Loss self: 0.0000; time: 0.23s
Val loss: 0.5863 score: 0.8367 time: 0.07s
Test loss: 0.5635 score: 0.9167 time: 0.07s
Epoch 152/1000, LR 0.000257
Train loss: 0.8944;  Loss pred: 0.8944; Loss self: 0.0000; time: 0.22s
Val loss: 0.5839 score: 0.8367 time: 0.07s
Test loss: 0.5606 score: 0.9167 time: 0.16s
Epoch 153/1000, LR 0.000257
Train loss: 0.8923;  Loss pred: 0.8923; Loss self: 0.0000; time: 0.23s
Val loss: 0.5815 score: 0.8367 time: 0.07s
Test loss: 0.5577 score: 0.9167 time: 0.07s
Epoch 154/1000, LR 0.000256
Train loss: 0.8909;  Loss pred: 0.8909; Loss self: 0.0000; time: 0.23s
Val loss: 0.5792 score: 0.8367 time: 0.07s
Test loss: 0.5547 score: 0.9167 time: 0.07s
Epoch 155/1000, LR 0.000256
Train loss: 0.8883;  Loss pred: 0.8883; Loss self: 0.0000; time: 0.23s
Val loss: 0.5767 score: 0.8367 time: 0.07s
Test loss: 0.5517 score: 0.9167 time: 0.07s
Epoch 156/1000, LR 0.000256
Train loss: 0.8821;  Loss pred: 0.8821; Loss self: 0.0000; time: 0.23s
Val loss: 0.5742 score: 0.8367 time: 0.07s
Test loss: 0.5486 score: 0.9167 time: 0.07s
Epoch 157/1000, LR 0.000256
Train loss: 0.8809;  Loss pred: 0.8809; Loss self: 0.0000; time: 0.23s
Val loss: 0.5717 score: 0.8367 time: 0.07s
Test loss: 0.5455 score: 0.9167 time: 0.17s
Epoch 158/1000, LR 0.000256
Train loss: 0.8774;  Loss pred: 0.8774; Loss self: 0.0000; time: 0.25s
Val loss: 0.5692 score: 0.8367 time: 1.71s
Test loss: 0.5424 score: 0.9167 time: 2.50s
Epoch 159/1000, LR 0.000255
Train loss: 0.8730;  Loss pred: 0.8730; Loss self: 0.0000; time: 4.29s
Val loss: 0.5667 score: 0.8367 time: 0.14s
Test loss: 0.5393 score: 0.9167 time: 0.43s
Epoch 160/1000, LR 0.000255
Train loss: 0.8738;  Loss pred: 0.8738; Loss self: 0.0000; time: 0.68s
Val loss: 0.5641 score: 0.8367 time: 0.07s
Test loss: 0.5362 score: 0.9167 time: 0.08s
Epoch 161/1000, LR 0.000255
Train loss: 0.8670;  Loss pred: 0.8670; Loss self: 0.0000; time: 0.23s
Val loss: 0.5616 score: 0.8367 time: 0.07s
Test loss: 0.5330 score: 0.9167 time: 0.07s
Epoch 162/1000, LR 0.000255
Train loss: 0.8652;  Loss pred: 0.8652; Loss self: 0.0000; time: 0.23s
Val loss: 0.5591 score: 0.8367 time: 0.07s
Test loss: 0.5299 score: 0.9167 time: 0.07s
Epoch 163/1000, LR 0.000255
Train loss: 0.8622;  Loss pred: 0.8622; Loss self: 0.0000; time: 0.23s
Val loss: 0.5566 score: 0.8367 time: 0.07s
Test loss: 0.5267 score: 0.9167 time: 0.07s
Epoch 164/1000, LR 0.000254
Train loss: 0.8588;  Loss pred: 0.8588; Loss self: 0.0000; time: 0.23s
Val loss: 0.5542 score: 0.8367 time: 0.07s
Test loss: 0.5237 score: 0.9167 time: 0.07s
Epoch 165/1000, LR 0.000254
Train loss: 0.8534;  Loss pred: 0.8534; Loss self: 0.0000; time: 0.23s
Val loss: 0.5518 score: 0.8367 time: 0.07s
Test loss: 0.5206 score: 0.9167 time: 0.07s
Epoch 166/1000, LR 0.000254
Train loss: 0.8534;  Loss pred: 0.8534; Loss self: 0.0000; time: 0.23s
Val loss: 0.5494 score: 0.8367 time: 0.07s
Test loss: 0.5175 score: 0.9167 time: 0.07s
Epoch 167/1000, LR 0.000254
Train loss: 0.8518;  Loss pred: 0.8518; Loss self: 0.0000; time: 0.23s
Val loss: 0.5469 score: 0.8367 time: 0.07s
Test loss: 0.5144 score: 0.9167 time: 0.07s
Epoch 168/1000, LR 0.000254
Train loss: 0.8477;  Loss pred: 0.8477; Loss self: 0.0000; time: 0.23s
Val loss: 0.5444 score: 0.8367 time: 0.07s
Test loss: 0.5112 score: 0.9167 time: 0.07s
Epoch 169/1000, LR 0.000253
Train loss: 0.8464;  Loss pred: 0.8464; Loss self: 0.0000; time: 0.23s
Val loss: 0.5419 score: 0.8367 time: 0.07s
Test loss: 0.5081 score: 0.9167 time: 0.07s
Epoch 170/1000, LR 0.000253
Train loss: 0.8410;  Loss pred: 0.8410; Loss self: 0.0000; time: 0.23s
Val loss: 0.5393 score: 0.8367 time: 0.07s
Test loss: 0.5048 score: 0.9167 time: 0.07s
Epoch 171/1000, LR 0.000253
Train loss: 0.8382;  Loss pred: 0.8382; Loss self: 0.0000; time: 0.23s
Val loss: 0.5368 score: 0.8367 time: 0.07s
Test loss: 0.5016 score: 0.9167 time: 0.07s
Epoch 172/1000, LR 0.000253
Train loss: 0.8342;  Loss pred: 0.8342; Loss self: 0.0000; time: 0.23s
Val loss: 0.5342 score: 0.8367 time: 0.07s
Test loss: 0.4984 score: 0.9167 time: 0.07s
Epoch 173/1000, LR 0.000253
Train loss: 0.8319;  Loss pred: 0.8319; Loss self: 0.0000; time: 0.23s
Val loss: 0.5317 score: 0.8367 time: 0.07s
Test loss: 0.4952 score: 0.9167 time: 0.07s
Epoch 174/1000, LR 0.000252
Train loss: 0.8268;  Loss pred: 0.8268; Loss self: 0.0000; time: 0.25s
Val loss: 0.5291 score: 0.8367 time: 1.91s
Test loss: 0.4919 score: 0.9167 time: 1.20s
Epoch 175/1000, LR 0.000252
Train loss: 0.8252;  Loss pred: 0.8252; Loss self: 0.0000; time: 4.72s
Val loss: 0.5265 score: 0.8367 time: 0.14s
Test loss: 0.4887 score: 0.9167 time: 0.13s
Epoch 176/1000, LR 0.000252
Train loss: 0.8243;  Loss pred: 0.8243; Loss self: 0.0000; time: 0.61s
Val loss: 0.5240 score: 0.8367 time: 0.08s
Test loss: 0.4855 score: 0.9167 time: 0.08s
Epoch 177/1000, LR 0.000252
Train loss: 0.8207;  Loss pred: 0.8207; Loss self: 0.0000; time: 0.25s
Val loss: 0.5214 score: 0.8367 time: 0.08s
Test loss: 0.4822 score: 0.9167 time: 0.08s
Epoch 178/1000, LR 0.000251
Train loss: 0.8174;  Loss pred: 0.8174; Loss self: 0.0000; time: 0.25s
Val loss: 0.5188 score: 0.8367 time: 0.07s
Test loss: 0.4789 score: 0.9167 time: 0.08s
Epoch 179/1000, LR 0.000251
Train loss: 0.8146;  Loss pred: 0.8146; Loss self: 0.0000; time: 0.25s
Val loss: 0.5161 score: 0.8367 time: 0.07s
Test loss: 0.4756 score: 0.9167 time: 0.08s
Epoch 180/1000, LR 0.000251
Train loss: 0.8110;  Loss pred: 0.8110; Loss self: 0.0000; time: 0.25s
Val loss: 0.5134 score: 0.8367 time: 0.07s
Test loss: 0.4722 score: 0.9167 time: 0.07s
Epoch 181/1000, LR 0.000251
Train loss: 0.8064;  Loss pred: 0.8064; Loss self: 0.0000; time: 0.24s
Val loss: 0.5106 score: 0.8367 time: 0.07s
Test loss: 0.4688 score: 0.9167 time: 0.07s
Epoch 182/1000, LR 0.000251
Train loss: 0.8032;  Loss pred: 0.8032; Loss self: 0.0000; time: 0.24s
Val loss: 0.5079 score: 0.8367 time: 0.07s
Test loss: 0.4655 score: 0.9167 time: 0.07s
Epoch 183/1000, LR 0.000250
Train loss: 0.8017;  Loss pred: 0.8017; Loss self: 0.0000; time: 0.23s
Val loss: 0.5053 score: 0.8367 time: 0.07s
Test loss: 0.4621 score: 0.9167 time: 0.08s
Epoch 184/1000, LR 0.000250
Train loss: 0.7996;  Loss pred: 0.7996; Loss self: 0.0000; time: 0.25s
Val loss: 0.5027 score: 0.8367 time: 0.07s
Test loss: 0.4589 score: 0.9167 time: 0.08s
Epoch 185/1000, LR 0.000250
Train loss: 0.7953;  Loss pred: 0.7953; Loss self: 0.0000; time: 0.25s
Val loss: 0.5002 score: 0.8367 time: 0.07s
Test loss: 0.4558 score: 0.9167 time: 0.08s
Epoch 186/1000, LR 0.000250
Train loss: 0.7928;  Loss pred: 0.7928; Loss self: 0.0000; time: 0.25s
Val loss: 0.4977 score: 0.8367 time: 0.07s
Test loss: 0.4526 score: 0.9167 time: 0.08s
Epoch 187/1000, LR 0.000249
Train loss: 0.7923;  Loss pred: 0.7923; Loss self: 0.0000; time: 0.25s
Val loss: 0.4953 score: 0.8367 time: 0.07s
Test loss: 0.4494 score: 0.9167 time: 1.11s
Epoch 188/1000, LR 0.000249
Train loss: 0.7885;  Loss pred: 0.7885; Loss self: 0.0000; time: 3.71s
Val loss: 0.4929 score: 0.8367 time: 0.28s
Test loss: 0.4464 score: 0.9167 time: 0.10s
Epoch 189/1000, LR 0.000249
Train loss: 0.7841;  Loss pred: 0.7841; Loss self: 0.0000; time: 0.26s
Val loss: 0.4906 score: 0.8367 time: 0.08s
Test loss: 0.4433 score: 0.9167 time: 0.08s
Epoch 190/1000, LR 0.000249
Train loss: 0.7812;  Loss pred: 0.7812; Loss self: 0.0000; time: 0.26s
Val loss: 0.4881 score: 0.8367 time: 0.08s
Test loss: 0.4402 score: 0.9167 time: 0.08s
Epoch 191/1000, LR 0.000249
Train loss: 0.7784;  Loss pred: 0.7784; Loss self: 0.0000; time: 0.25s
Val loss: 0.4857 score: 0.8367 time: 0.07s
Test loss: 0.4371 score: 0.9167 time: 0.07s
Epoch 192/1000, LR 0.000248
Train loss: 0.7761;  Loss pred: 0.7761; Loss self: 0.0000; time: 0.24s
Val loss: 0.4832 score: 0.8571 time: 0.07s
Test loss: 0.4340 score: 0.9167 time: 0.07s
Epoch 193/1000, LR 0.000248
Train loss: 0.7720;  Loss pred: 0.7720; Loss self: 0.0000; time: 0.24s
Val loss: 0.4807 score: 0.8571 time: 0.07s
Test loss: 0.4308 score: 0.9167 time: 0.07s
Epoch 194/1000, LR 0.000248
Train loss: 0.7705;  Loss pred: 0.7705; Loss self: 0.0000; time: 0.23s
Val loss: 0.4782 score: 0.8571 time: 0.07s
Test loss: 0.4276 score: 0.9167 time: 0.07s
Epoch 195/1000, LR 0.000248
Train loss: 0.7679;  Loss pred: 0.7679; Loss self: 0.0000; time: 0.24s
Val loss: 0.4757 score: 0.8571 time: 0.07s
Test loss: 0.4245 score: 0.9167 time: 0.07s
Epoch 196/1000, LR 0.000247
Train loss: 0.7650;  Loss pred: 0.7650; Loss self: 0.0000; time: 0.24s
Val loss: 0.4731 score: 0.8571 time: 0.07s
Test loss: 0.4213 score: 0.9167 time: 0.08s
Epoch 197/1000, LR 0.000247
Train loss: 0.7605;  Loss pred: 0.7605; Loss self: 0.0000; time: 0.24s
Val loss: 0.4706 score: 0.8571 time: 0.07s
Test loss: 0.4181 score: 0.9167 time: 0.08s
Epoch 198/1000, LR 0.000247
Train loss: 0.7600;  Loss pred: 0.7600; Loss self: 0.0000; time: 0.26s
Val loss: 0.4682 score: 0.8571 time: 0.08s
Test loss: 0.4151 score: 0.9167 time: 0.08s
Epoch 199/1000, LR 0.000247
Train loss: 0.7550;  Loss pred: 0.7550; Loss self: 0.0000; time: 0.25s
Val loss: 0.4659 score: 0.8571 time: 0.08s
Test loss: 0.4121 score: 0.9167 time: 0.08s
Epoch 200/1000, LR 0.000246
Train loss: 0.7552;  Loss pred: 0.7552; Loss self: 0.0000; time: 0.25s
Val loss: 0.4637 score: 0.8571 time: 0.07s
Test loss: 0.4093 score: 0.9167 time: 0.08s
Epoch 201/1000, LR 0.000246
Train loss: 0.7524;  Loss pred: 0.7524; Loss self: 0.0000; time: 0.25s
Val loss: 0.4615 score: 0.8571 time: 0.07s
Test loss: 0.4064 score: 0.9167 time: 0.08s
Epoch 202/1000, LR 0.000246
Train loss: 0.7487;  Loss pred: 0.7487; Loss self: 0.0000; time: 0.25s
Val loss: 0.4593 score: 0.8571 time: 0.08s
Test loss: 0.4035 score: 0.9167 time: 0.08s
Epoch 203/1000, LR 0.000246
Train loss: 0.7464;  Loss pred: 0.7464; Loss self: 0.0000; time: 0.25s
Val loss: 0.4570 score: 0.8571 time: 0.07s
Test loss: 0.4006 score: 0.9167 time: 0.08s
Epoch 204/1000, LR 0.000245
Train loss: 0.7432;  Loss pred: 0.7432; Loss self: 0.0000; time: 0.26s
Val loss: 0.4548 score: 0.8571 time: 0.08s
Test loss: 0.3978 score: 0.9167 time: 0.08s
Epoch 205/1000, LR 0.000245
Train loss: 0.7397;  Loss pred: 0.7397; Loss self: 0.0000; time: 0.90s
Val loss: 0.4526 score: 0.8571 time: 0.69s
Test loss: 0.3949 score: 0.9167 time: 1.57s
Epoch 206/1000, LR 0.000245
Train loss: 0.7384;  Loss pred: 0.7384; Loss self: 0.0000; time: 0.76s
Val loss: 0.4503 score: 0.8571 time: 0.07s
Test loss: 0.3920 score: 0.9167 time: 0.08s
Epoch 207/1000, LR 0.000245
Train loss: 0.7361;  Loss pred: 0.7361; Loss self: 0.0000; time: 0.24s
Val loss: 0.4480 score: 0.8571 time: 0.07s
Test loss: 0.3891 score: 0.9167 time: 0.07s
Epoch 208/1000, LR 0.000244
Train loss: 0.7344;  Loss pred: 0.7344; Loss self: 0.0000; time: 0.24s
Val loss: 0.4458 score: 0.8571 time: 0.07s
Test loss: 0.3863 score: 0.9167 time: 0.07s
Epoch 209/1000, LR 0.000244
Train loss: 0.7308;  Loss pred: 0.7308; Loss self: 0.0000; time: 0.24s
Val loss: 0.4435 score: 0.8571 time: 0.07s
Test loss: 0.3834 score: 0.9167 time: 0.07s
Epoch 210/1000, LR 0.000244
Train loss: 0.7297;  Loss pred: 0.7297; Loss self: 0.0000; time: 0.24s
Val loss: 0.4414 score: 0.8571 time: 0.07s
Test loss: 0.3806 score: 0.9167 time: 0.07s
Epoch 211/1000, LR 0.000244
Train loss: 0.7246;  Loss pred: 0.7246; Loss self: 0.0000; time: 0.24s
Val loss: 0.4392 score: 0.8571 time: 0.07s
Test loss: 0.3778 score: 0.9167 time: 0.08s
Epoch 212/1000, LR 0.000243
Train loss: 0.7255;  Loss pred: 0.7255; Loss self: 0.0000; time: 0.24s
Val loss: 0.4371 score: 0.8571 time: 0.07s
Test loss: 0.3752 score: 0.9167 time: 0.07s
Epoch 213/1000, LR 0.000243
Train loss: 0.7238;  Loss pred: 0.7238; Loss self: 0.0000; time: 0.24s
Val loss: 0.4352 score: 0.8571 time: 0.07s
Test loss: 0.3727 score: 0.9167 time: 0.07s
Epoch 214/1000, LR 0.000243
Train loss: 0.7174;  Loss pred: 0.7174; Loss self: 0.0000; time: 0.24s
Val loss: 0.4333 score: 0.8571 time: 0.07s
Test loss: 0.3701 score: 0.9167 time: 0.07s
Epoch 215/1000, LR 0.000243
Train loss: 0.7138;  Loss pred: 0.7138; Loss self: 0.0000; time: 3.34s
Val loss: 0.4313 score: 0.8571 time: 0.15s
Test loss: 0.3676 score: 0.9167 time: 0.34s
Epoch 216/1000, LR 0.000242
Train loss: 0.7147;  Loss pred: 0.7147; Loss self: 0.0000; time: 0.52s
Val loss: 0.4294 score: 0.8571 time: 0.20s
Test loss: 0.3651 score: 0.9167 time: 0.15s
Epoch 217/1000, LR 0.000242
Train loss: 0.7103;  Loss pred: 0.7103; Loss self: 0.0000; time: 0.27s
Val loss: 0.4275 score: 0.8571 time: 0.07s
Test loss: 0.3626 score: 0.9167 time: 0.08s
Epoch 218/1000, LR 0.000242
Train loss: 0.7103;  Loss pred: 0.7103; Loss self: 0.0000; time: 0.24s
Val loss: 0.4256 score: 0.8571 time: 0.07s
Test loss: 0.3601 score: 0.9167 time: 0.07s
Epoch 219/1000, LR 0.000242
Train loss: 0.7071;  Loss pred: 0.7071; Loss self: 0.0000; time: 0.24s
Val loss: 0.4237 score: 0.8571 time: 0.07s
Test loss: 0.3576 score: 0.9167 time: 0.07s
Epoch 220/1000, LR 0.000241
Train loss: 0.7021;  Loss pred: 0.7021; Loss self: 0.0000; time: 0.23s
Val loss: 0.4216 score: 0.8571 time: 0.07s
Test loss: 0.3549 score: 0.9167 time: 0.08s
Epoch 221/1000, LR 0.000241
Train loss: 0.7029;  Loss pred: 0.7029; Loss self: 0.0000; time: 0.26s
Val loss: 0.4195 score: 0.8571 time: 0.08s
Test loss: 0.3523 score: 0.9167 time: 0.08s
Epoch 222/1000, LR 0.000241
Train loss: 0.7006;  Loss pred: 0.7006; Loss self: 0.0000; time: 0.25s
Val loss: 0.4176 score: 0.8571 time: 0.08s
Test loss: 0.3498 score: 0.9167 time: 0.08s
Epoch 223/1000, LR 0.000241
Train loss: 0.6970;  Loss pred: 0.6970; Loss self: 0.0000; time: 0.24s
Val loss: 0.4158 score: 0.8571 time: 0.07s
Test loss: 0.3474 score: 0.9167 time: 0.07s
Epoch 224/1000, LR 0.000240
Train loss: 0.6969;  Loss pred: 0.6969; Loss self: 0.0000; time: 0.23s
Val loss: 0.4140 score: 0.8571 time: 0.07s
Test loss: 0.3450 score: 0.9167 time: 0.07s
Epoch 225/1000, LR 0.000240
Train loss: 0.6933;  Loss pred: 0.6933; Loss self: 0.0000; time: 0.23s
Val loss: 0.4122 score: 0.8571 time: 0.08s
Test loss: 0.3426 score: 0.9167 time: 0.09s
Epoch 226/1000, LR 0.000240
Train loss: 0.6907;  Loss pred: 0.6907; Loss self: 0.0000; time: 0.23s
Val loss: 0.4105 score: 0.8571 time: 0.07s
Test loss: 0.3404 score: 0.9167 time: 0.07s
Epoch 227/1000, LR 0.000240
Train loss: 0.6886;  Loss pred: 0.6886; Loss self: 0.0000; time: 0.24s
Val loss: 0.4090 score: 0.8571 time: 0.07s
Test loss: 0.3382 score: 0.9167 time: 0.07s
Epoch 228/1000, LR 0.000239
Train loss: 0.6869;  Loss pred: 0.6869; Loss self: 0.0000; time: 0.24s
Val loss: 0.4074 score: 0.8571 time: 0.07s
Test loss: 0.3361 score: 0.9167 time: 0.08s
Epoch 229/1000, LR 0.000239
Train loss: 0.6859;  Loss pred: 0.6859; Loss self: 0.0000; time: 0.24s
Val loss: 0.4060 score: 0.8571 time: 0.07s
Test loss: 0.3342 score: 0.9167 time: 0.07s
Epoch 230/1000, LR 0.000239
Train loss: 0.6831;  Loss pred: 0.6831; Loss self: 0.0000; time: 0.26s
Val loss: 0.4045 score: 0.8571 time: 0.07s
Test loss: 0.3321 score: 0.9167 time: 0.07s
Epoch 231/1000, LR 0.000238
Train loss: 0.6781;  Loss pred: 0.6781; Loss self: 0.0000; time: 0.24s
Val loss: 0.4030 score: 0.8571 time: 0.07s
Test loss: 0.3300 score: 0.9167 time: 0.07s
Epoch 232/1000, LR 0.000238
Train loss: 0.6787;  Loss pred: 0.6787; Loss self: 0.0000; time: 0.24s
Val loss: 0.4014 score: 0.8571 time: 0.07s
Test loss: 0.3279 score: 0.9167 time: 0.07s
Epoch 233/1000, LR 0.000238
Train loss: 0.6787;  Loss pred: 0.6787; Loss self: 0.0000; time: 0.24s
Val loss: 0.3996 score: 0.8571 time: 0.07s
Test loss: 0.3256 score: 0.9167 time: 0.07s
Epoch 234/1000, LR 0.000238
Train loss: 0.6759;  Loss pred: 0.6759; Loss self: 0.0000; time: 0.25s
Val loss: 0.3980 score: 0.8571 time: 0.07s
Test loss: 0.3234 score: 0.9167 time: 0.07s
Epoch 235/1000, LR 0.000237
Train loss: 0.6732;  Loss pred: 0.6732; Loss self: 0.0000; time: 0.24s
Val loss: 0.3964 score: 0.8571 time: 0.07s
Test loss: 0.3214 score: 0.9167 time: 0.07s
Epoch 236/1000, LR 0.000237
Train loss: 0.6711;  Loss pred: 0.6711; Loss self: 0.0000; time: 0.24s
Val loss: 0.3949 score: 0.8571 time: 0.07s
Test loss: 0.3193 score: 0.9167 time: 0.07s
Epoch 237/1000, LR 0.000237
Train loss: 0.6685;  Loss pred: 0.6685; Loss self: 0.0000; time: 0.24s
Val loss: 0.3934 score: 0.8571 time: 0.07s
Test loss: 0.3173 score: 0.9167 time: 0.07s
Epoch 238/1000, LR 0.000236
Train loss: 0.6684;  Loss pred: 0.6684; Loss self: 0.0000; time: 0.25s
Val loss: 0.3919 score: 0.8571 time: 0.12s
Test loss: 0.3153 score: 0.9167 time: 0.08s
Epoch 239/1000, LR 0.000236
Train loss: 0.6666;  Loss pred: 0.6666; Loss self: 0.0000; time: 0.25s
Val loss: 0.3906 score: 0.8571 time: 0.07s
Test loss: 0.3135 score: 0.9167 time: 0.08s
Epoch 240/1000, LR 0.000236
Train loss: 0.6635;  Loss pred: 0.6635; Loss self: 0.0000; time: 0.24s
Val loss: 0.3893 score: 0.8571 time: 0.07s
Test loss: 0.3117 score: 0.9167 time: 0.07s
Epoch 241/1000, LR 0.000236
Train loss: 0.6621;  Loss pred: 0.6621; Loss self: 0.0000; time: 0.23s
Val loss: 0.3882 score: 0.8571 time: 0.07s
Test loss: 0.3100 score: 0.9167 time: 0.07s
Epoch 242/1000, LR 0.000235
Train loss: 0.6599;  Loss pred: 0.6599; Loss self: 0.0000; time: 0.23s
Val loss: 0.3869 score: 0.8571 time: 0.07s
Test loss: 0.3083 score: 0.9167 time: 0.07s
Epoch 243/1000, LR 0.000235
Train loss: 0.6580;  Loss pred: 0.6580; Loss self: 0.0000; time: 0.23s
Val loss: 0.3857 score: 0.8571 time: 0.07s
Test loss: 0.3065 score: 0.9167 time: 0.07s
Epoch 244/1000, LR 0.000235
Train loss: 0.6569;  Loss pred: 0.6569; Loss self: 0.0000; time: 0.23s
Val loss: 0.3845 score: 0.8571 time: 0.07s
Test loss: 0.3048 score: 0.9167 time: 0.07s
Epoch 245/1000, LR 0.000234
Train loss: 0.6554;  Loss pred: 0.6554; Loss self: 0.0000; time: 0.23s
Val loss: 0.3836 score: 0.8571 time: 0.07s
Test loss: 0.3034 score: 0.9167 time: 0.07s
Epoch 246/1000, LR 0.000234
Train loss: 0.6546;  Loss pred: 0.6546; Loss self: 0.0000; time: 0.23s
Val loss: 0.3826 score: 0.8571 time: 0.07s
Test loss: 0.3018 score: 0.9167 time: 0.07s
Epoch 247/1000, LR 0.000234
Train loss: 0.6520;  Loss pred: 0.6520; Loss self: 0.0000; time: 0.23s
Val loss: 0.3815 score: 0.8571 time: 0.07s
Test loss: 0.3003 score: 0.9167 time: 0.07s
Epoch 248/1000, LR 0.000234
Train loss: 0.6513;  Loss pred: 0.6513; Loss self: 0.0000; time: 0.23s
Val loss: 0.3804 score: 0.8571 time: 0.07s
Test loss: 0.2986 score: 0.9167 time: 0.07s
Epoch 249/1000, LR 0.000233
Train loss: 0.6487;  Loss pred: 0.6487; Loss self: 0.0000; time: 0.23s
Val loss: 0.3791 score: 0.8571 time: 0.07s
Test loss: 0.2969 score: 0.9167 time: 0.07s
Epoch 250/1000, LR 0.000233
Train loss: 0.6480;  Loss pred: 0.6480; Loss self: 0.0000; time: 0.24s
Val loss: 0.3779 score: 0.8571 time: 0.07s
Test loss: 0.2952 score: 0.9167 time: 0.07s
Epoch 251/1000, LR 0.000233
Train loss: 0.6450;  Loss pred: 0.6450; Loss self: 0.0000; time: 0.23s
Val loss: 0.3766 score: 0.8571 time: 0.07s
Test loss: 0.2934 score: 0.9167 time: 0.07s
Epoch 252/1000, LR 0.000232
Train loss: 0.6428;  Loss pred: 0.6428; Loss self: 0.0000; time: 0.23s
Val loss: 0.3753 score: 0.8571 time: 0.07s
Test loss: 0.2916 score: 0.9167 time: 0.08s
Epoch 253/1000, LR 0.000232
Train loss: 0.6428;  Loss pred: 0.6428; Loss self: 0.0000; time: 0.25s
Val loss: 0.3740 score: 0.8571 time: 0.07s
Test loss: 0.2898 score: 0.9167 time: 0.98s
Epoch 254/1000, LR 0.000232
Train loss: 0.6408;  Loss pred: 0.6408; Loss self: 0.0000; time: 2.92s
Val loss: 0.3730 score: 0.8571 time: 0.63s
Test loss: 0.2883 score: 0.9167 time: 0.87s
Epoch 255/1000, LR 0.000232
Train loss: 0.6402;  Loss pred: 0.6402; Loss self: 0.0000; time: 2.63s
Val loss: 0.3720 score: 0.8571 time: 0.22s
Test loss: 0.2868 score: 0.9167 time: 0.41s
Epoch 256/1000, LR 0.000231
Train loss: 0.6379;  Loss pred: 0.6379; Loss self: 0.0000; time: 0.74s
Val loss: 0.3709 score: 0.8571 time: 0.11s
Test loss: 0.2853 score: 0.9167 time: 0.12s
Epoch 257/1000, LR 0.000231
Train loss: 0.6375;  Loss pred: 0.6375; Loss self: 0.0000; time: 1.49s
Val loss: 0.3698 score: 0.8571 time: 0.23s
Test loss: 0.2837 score: 0.9167 time: 0.48s
Epoch 258/1000, LR 0.000231
Train loss: 0.6355;  Loss pred: 0.6355; Loss self: 0.0000; time: 0.52s
Val loss: 0.3688 score: 0.8571 time: 0.10s
Test loss: 0.2822 score: 0.9167 time: 0.26s
Epoch 259/1000, LR 0.000230
Train loss: 0.6343;  Loss pred: 0.6343; Loss self: 0.0000; time: 0.48s
Val loss: 0.3677 score: 0.8571 time: 0.07s
Test loss: 0.2807 score: 0.9167 time: 0.08s
Epoch 260/1000, LR 0.000230
Train loss: 0.6337;  Loss pred: 0.6337; Loss self: 0.0000; time: 0.24s
Val loss: 0.3671 score: 0.8571 time: 0.07s
Test loss: 0.2796 score: 0.9167 time: 0.07s
Epoch 261/1000, LR 0.000230
Train loss: 0.6318;  Loss pred: 0.6318; Loss self: 0.0000; time: 0.24s
Val loss: 0.3668 score: 0.8571 time: 0.07s
Test loss: 0.2787 score: 0.9167 time: 0.07s
Epoch 262/1000, LR 0.000229
Train loss: 0.6313;  Loss pred: 0.6313; Loss self: 0.0000; time: 0.24s
Val loss: 0.3667 score: 0.8571 time: 0.07s
Test loss: 0.2782 score: 0.9167 time: 0.07s
Epoch 263/1000, LR 0.000229
Train loss: 0.6280;  Loss pred: 0.6280; Loss self: 0.0000; time: 0.24s
Val loss: 0.3668 score: 0.8571 time: 0.07s
Test loss: 0.2777 score: 0.9167 time: 0.07s
     INFO: Early stopping counter 1 of 2
Epoch 264/1000, LR 0.000229
Train loss: 0.6298;  Loss pred: 0.6298; Loss self: 0.0000; time: 0.24s
Val loss: 0.3667 score: 0.8571 time: 0.07s
Test loss: 0.2772 score: 0.9167 time: 0.07s
Epoch 265/1000, LR 0.000228
Train loss: 0.6263;  Loss pred: 0.6263; Loss self: 0.0000; time: 0.24s
Val loss: 0.3666 score: 0.8571 time: 0.07s
Test loss: 0.2766 score: 0.9167 time: 0.07s
Epoch 266/1000, LR 0.000228
Train loss: 0.6284;  Loss pred: 0.6284; Loss self: 0.0000; time: 0.24s
Val loss: 0.3663 score: 0.8571 time: 0.07s
Test loss: 0.2759 score: 0.9167 time: 0.07s
Epoch 267/1000, LR 0.000228
Train loss: 0.6239;  Loss pred: 0.6239; Loss self: 0.0000; time: 0.24s
Val loss: 0.3656 score: 0.8571 time: 0.07s
Test loss: 0.2747 score: 0.9167 time: 0.07s
Epoch 268/1000, LR 0.000228
Train loss: 0.6244;  Loss pred: 0.6244; Loss self: 0.0000; time: 0.24s
Val loss: 0.3644 score: 0.8571 time: 0.07s
Test loss: 0.2731 score: 0.9167 time: 0.66s
Epoch 269/1000, LR 0.000227
Train loss: 0.6229;  Loss pred: 0.6229; Loss self: 0.0000; time: 3.92s
Val loss: 0.3629 score: 0.8571 time: 0.08s
Test loss: 0.2712 score: 0.9167 time: 0.08s
Epoch 270/1000, LR 0.000227
Train loss: 0.6208;  Loss pred: 0.6208; Loss self: 0.0000; time: 0.25s
Val loss: 0.3614 score: 0.8571 time: 0.07s
Test loss: 0.2692 score: 0.9167 time: 0.08s
Epoch 271/1000, LR 0.000227
Train loss: 0.6218;  Loss pred: 0.6218; Loss self: 0.0000; time: 0.24s
Val loss: 0.3602 score: 0.8571 time: 0.07s
Test loss: 0.2676 score: 0.9167 time: 0.07s
Epoch 272/1000, LR 0.000226
Train loss: 0.6191;  Loss pred: 0.6191; Loss self: 0.0000; time: 0.23s
Val loss: 0.3590 score: 0.8571 time: 0.07s
Test loss: 0.2659 score: 0.9167 time: 0.07s
Epoch 273/1000, LR 0.000226
Train loss: 0.6161;  Loss pred: 0.6161; Loss self: 0.0000; time: 0.23s
Val loss: 0.3579 score: 0.8571 time: 0.07s
Test loss: 0.2644 score: 0.9167 time: 0.07s
Epoch 274/1000, LR 0.000226
Train loss: 0.6169;  Loss pred: 0.6169; Loss self: 0.0000; time: 0.23s
Val loss: 0.3570 score: 0.8571 time: 0.07s
Test loss: 0.2631 score: 0.9167 time: 0.07s
Epoch 275/1000, LR 0.000225
Train loss: 0.6168;  Loss pred: 0.6168; Loss self: 0.0000; time: 0.23s
Val loss: 0.3566 score: 0.8571 time: 0.07s
Test loss: 0.2622 score: 0.9167 time: 0.07s
Epoch 276/1000, LR 0.000225
Train loss: 0.6138;  Loss pred: 0.6138; Loss self: 0.0000; time: 0.23s
Val loss: 0.3562 score: 0.8571 time: 0.07s
Test loss: 0.2614 score: 0.9167 time: 0.07s
Epoch 277/1000, LR 0.000225
Train loss: 0.6118;  Loss pred: 0.6118; Loss self: 0.0000; time: 0.23s
Val loss: 0.3557 score: 0.8571 time: 0.07s
Test loss: 0.2605 score: 0.9167 time: 0.07s
Epoch 278/1000, LR 0.000224
Train loss: 0.6143;  Loss pred: 0.6143; Loss self: 0.0000; time: 0.23s
Val loss: 0.3552 score: 0.8571 time: 0.08s
Test loss: 0.2595 score: 0.9167 time: 0.07s
Epoch 279/1000, LR 0.000224
Train loss: 0.6110;  Loss pred: 0.6110; Loss self: 0.0000; time: 0.23s
Val loss: 0.3548 score: 0.8571 time: 0.07s
Test loss: 0.2588 score: 0.9167 time: 0.07s
Epoch 280/1000, LR 0.000224
Train loss: 0.6097;  Loss pred: 0.6097; Loss self: 0.0000; time: 0.23s
Val loss: 0.3546 score: 0.8571 time: 0.07s
Test loss: 0.2581 score: 0.9167 time: 0.07s
Epoch 281/1000, LR 0.000223
Train loss: 0.6097;  Loss pred: 0.6097; Loss self: 0.0000; time: 0.23s
Val loss: 0.3543 score: 0.8571 time: 0.07s
Test loss: 0.2573 score: 0.9167 time: 0.07s
Epoch 282/1000, LR 0.000223
Train loss: 0.6071;  Loss pred: 0.6071; Loss self: 0.0000; time: 0.23s
Val loss: 0.3540 score: 0.8571 time: 0.07s
Test loss: 0.2567 score: 0.9167 time: 0.07s
Epoch 283/1000, LR 0.000223
Train loss: 0.6061;  Loss pred: 0.6061; Loss self: 0.0000; time: 0.23s
Val loss: 0.3539 score: 0.8571 time: 0.08s
Test loss: 0.2562 score: 0.9167 time: 0.07s
Epoch 284/1000, LR 0.000222
Train loss: 0.6063;  Loss pred: 0.6063; Loss self: 0.0000; time: 0.23s
Val loss: 0.3534 score: 0.8571 time: 0.07s
Test loss: 0.2552 score: 0.9167 time: 0.07s
Epoch 285/1000, LR 0.000222
Train loss: 0.6054;  Loss pred: 0.6054; Loss self: 0.0000; time: 0.23s
Val loss: 0.3528 score: 0.8571 time: 0.07s
Test loss: 0.2543 score: 0.9167 time: 0.07s
Epoch 286/1000, LR 0.000222
Train loss: 0.6053;  Loss pred: 0.6053; Loss self: 0.0000; time: 0.24s
Val loss: 0.3524 score: 0.8571 time: 0.07s
Test loss: 0.2535 score: 0.9167 time: 0.07s
Epoch 287/1000, LR 0.000221
Train loss: 0.6045;  Loss pred: 0.6045; Loss self: 0.0000; time: 0.23s
Val loss: 0.3520 score: 0.8571 time: 0.07s
Test loss: 0.2527 score: 0.9167 time: 0.07s
Epoch 288/1000, LR 0.000221
Train loss: 0.6027;  Loss pred: 0.6027; Loss self: 0.0000; time: 0.23s
Val loss: 0.3518 score: 0.8571 time: 0.07s
Test loss: 0.2521 score: 0.9167 time: 0.07s
Epoch 289/1000, LR 0.000221
Train loss: 0.6002;  Loss pred: 0.6002; Loss self: 0.0000; time: 0.23s
Val loss: 0.3516 score: 0.8571 time: 0.07s
Test loss: 0.2515 score: 0.9167 time: 0.09s
Epoch 290/1000, LR 0.000220
Train loss: 0.6015;  Loss pred: 0.6015; Loss self: 0.0000; time: 5.90s
Val loss: 0.3512 score: 0.8571 time: 0.59s
Test loss: 0.2508 score: 0.9167 time: 0.48s
Epoch 291/1000, LR 0.000220
Train loss: 0.5990;  Loss pred: 0.5990; Loss self: 0.0000; time: 2.14s
Val loss: 0.3510 score: 0.8571 time: 0.07s
Test loss: 0.2501 score: 0.9167 time: 0.08s
Epoch 292/1000, LR 0.000220
Train loss: 0.5969;  Loss pred: 0.5969; Loss self: 0.0000; time: 0.25s
Val loss: 0.3504 score: 0.8571 time: 0.07s
Test loss: 0.2492 score: 0.9167 time: 0.08s
Epoch 293/1000, LR 0.000219
Train loss: 0.5993;  Loss pred: 0.5993; Loss self: 0.0000; time: 0.24s
Val loss: 0.3499 score: 0.8571 time: 0.07s
Test loss: 0.2483 score: 0.9167 time: 0.07s
Epoch 294/1000, LR 0.000219
Train loss: 0.5970;  Loss pred: 0.5970; Loss self: 0.0000; time: 0.24s
Val loss: 0.3494 score: 0.8571 time: 0.07s
Test loss: 0.2474 score: 0.9167 time: 0.07s
Epoch 295/1000, LR 0.000219
Train loss: 0.5983;  Loss pred: 0.5983; Loss self: 0.0000; time: 0.23s
Val loss: 0.3491 score: 0.8571 time: 0.07s
Test loss: 0.2467 score: 0.9167 time: 0.07s
Epoch 296/1000, LR 0.000218
Train loss: 0.5963;  Loss pred: 0.5963; Loss self: 0.0000; time: 0.23s
Val loss: 0.3489 score: 0.8571 time: 0.07s
Test loss: 0.2461 score: 0.9167 time: 0.07s
Epoch 297/1000, LR 0.000218
Train loss: 0.5953;  Loss pred: 0.5953; Loss self: 0.0000; time: 0.23s
Val loss: 0.3488 score: 0.8571 time: 0.07s
Test loss: 0.2457 score: 0.9167 time: 0.07s
Epoch 298/1000, LR 0.000218
Train loss: 0.5940;  Loss pred: 0.5940; Loss self: 0.0000; time: 1.20s
Val loss: 0.3487 score: 0.8571 time: 0.92s
Test loss: 0.2453 score: 0.9167 time: 0.45s
Epoch 299/1000, LR 0.000217
Train loss: 0.5935;  Loss pred: 0.5935; Loss self: 0.0000; time: 2.27s
Val loss: 0.3488 score: 0.8571 time: 0.44s
Test loss: 0.2450 score: 0.9167 time: 0.70s
     INFO: Early stopping counter 1 of 2
Epoch 300/1000, LR 0.000217
Train loss: 0.5926;  Loss pred: 0.5926; Loss self: 0.0000; time: 1.03s
Val loss: 0.3487 score: 0.8571 time: 0.36s
Test loss: 0.2445 score: 0.9167 time: 0.27s
Epoch 301/1000, LR 0.000217
Train loss: 0.5923;  Loss pred: 0.5923; Loss self: 0.0000; time: 1.18s
Val loss: 0.3485 score: 0.8571 time: 0.07s
Test loss: 0.2440 score: 0.9167 time: 0.07s
Epoch 302/1000, LR 0.000216
Train loss: 0.5904;  Loss pred: 0.5904; Loss self: 0.0000; time: 0.23s
Val loss: 0.3479 score: 0.8571 time: 0.07s
Test loss: 0.2430 score: 0.9167 time: 0.07s
Epoch 303/1000, LR 0.000216
Train loss: 0.5916;  Loss pred: 0.5916; Loss self: 0.0000; time: 0.23s
Val loss: 0.3473 score: 0.8571 time: 0.07s
Test loss: 0.2421 score: 0.9167 time: 0.07s
Epoch 304/1000, LR 0.000216
Train loss: 0.5906;  Loss pred: 0.5906; Loss self: 0.0000; time: 0.23s
Val loss: 0.3470 score: 0.8571 time: 0.07s
Test loss: 0.2414 score: 0.9167 time: 0.07s
Epoch 305/1000, LR 0.000215
Train loss: 0.5884;  Loss pred: 0.5884; Loss self: 0.0000; time: 0.25s
Val loss: 0.3469 score: 0.8571 time: 0.07s
Test loss: 0.2409 score: 0.9167 time: 0.07s
Epoch 306/1000, LR 0.000215
Train loss: 0.5884;  Loss pred: 0.5884; Loss self: 0.0000; time: 0.22s
Val loss: 0.3470 score: 0.8571 time: 0.07s
Test loss: 0.2406 score: 0.9167 time: 0.07s
     INFO: Early stopping counter 1 of 2
Epoch 307/1000, LR 0.000215
Train loss: 0.5881;  Loss pred: 0.5881; Loss self: 0.0000; time: 0.23s
Val loss: 0.3470 score: 0.8571 time: 0.07s
Test loss: 0.2403 score: 0.9167 time: 0.07s
     INFO: Early stopping counter 2 of 2
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 304,   Train_Loss: 0.5884,   Val_Loss: 0.3469,   Val_Precision: 0.9091,   Val_Recall: 0.8000,   Val_accuracy: 0.8511,   Val_Score: 0.8571,   Val_Loss: 0.3469,   Test_Precision: 1.0000,   Test_Recall: 0.8333,   Test_accuracy: 0.9091,   Test_Score: 0.9167,   Test_loss: 0.2409


[0.07079106802120805, 0.07018441602122039, 0.07023633108474314, 0.0697052989853546, 0.0703910420415923, 0.07062913395930082, 0.07010162097867578, 0.07027464197017252, 0.06987066601868719, 0.07009714795276523, 0.07049367693252861, 0.07013685896527022, 0.07022120803594589, 0.06995216698851436, 0.06983618205413222, 0.06972508504986763, 0.06990352703724056, 0.07067643001209944, 0.06929189700167626, 0.06959368195384741, 0.06970364798326045, 0.06998555001337081, 0.06994720397051424, 0.0698741520754993, 0.0702572810696438, 0.07043814600910991, 0.07038082601502538, 0.07069158996455371, 0.07080697803758085, 0.07547648099716753, 0.07524220296181738, 0.07525971997529268, 0.07558234606403857, 0.07552790699992329, 0.07616590999532491, 0.07546610292047262, 0.07044299796689302, 0.07069637405220419, 0.07056315895169973, 0.070826111943461, 0.07062980299815536, 0.07562143099494278, 0.07549268391449004, 0.07611173309851438, 0.08583163807634264, 0.07355130393989384, 0.0761714419350028, 2.4632770200259984, 0.08091302390675992, 0.07595029496587813, 0.07654094696044922, 0.07641816406976432, 0.0757436299463734, 1.3762688419083133, 0.6079220100073144, 0.07742297789081931, 0.07515917695127428, 0.0758576060179621, 0.07616155501455069, 0.07585350808221847, 0.07563257205765694, 0.7125495029613376, 0.29406631097663194, 0.08429849497042596, 0.08278192998841405, 0.08096620603464544, 0.08120407396927476, 0.07401292398571968, 0.07353285199496895, 0.07407186191994697, 0.07376284198835492, 0.07481843302957714, 0.07398352201562375, 0.07388156501110643, 0.07108427409548312, 0.07642552303150296, 0.07712038594763726, 0.07851661404129118, 0.0777531280182302, 0.07896128005813807, 1.330918365973048, 1.9628155010286719, 0.7638422230957076, 0.23356156307272613, 0.07001405698247254, 0.07009359204676002, 0.07082958205137402, 0.07003154105041176, 0.07079189096111804, 0.07050531799905002, 1.301095929928124, 0.0809614920290187, 0.07863310794346035, 0.07139787299092859, 0.07099319796543568, 0.07143126800656319, 0.07198415801394731, 0.07121521304361522, 0.0706044880207628, 0.071301119052805, 0.07134654093533754, 0.07078228995669633, 0.9570135719841346, 0.09071581903845072, 0.08132547105196863, 0.08263122907374054, 0.0809444549959153, 0.08177431998774409, 1.080570682999678, 0.3054379989625886, 0.2619882479775697, 0.223650265019387, 0.07601278799120337, 0.07643709995318204, 0.07382068992592394, 0.07402475294657052, 0.07412147300783545, 0.07321592594962567, 0.07373925903812051, 0.07421306590549648, 0.07441458303947002, 0.07375607104040682, 0.0738056170521304, 0.0735326959984377, 0.07321798091288656, 0.07329075201414526, 0.07448288204614073, 0.07503925508353859, 1.1114320220658556, 0.40592054289299995, 1.1290595420869067, 0.08128957101143897, 0.0777922139968723, 0.07161635893862695, 0.07137364707887173, 0.07118470792192966, 0.0705737010575831, 0.07182544993702322, 0.0708634409820661, 0.07091379608027637, 0.07670305401552469, 0.07636678090784699, 0.07726341998204589, 1.8816777180181816, 0.08302532706875354, 0.07769285491667688, 0.07628627901431173, 0.07581749395467341, 0.07161926000844687, 0.07125062297564, 0.07343270804267377, 0.07160622801166028, 2.5457634339109063, 1.1847587230149657, 0.0761593539500609, 0.07601889304351062, 0.0756638270104304, 0.07504404603969306, 0.07630067609716207, 0.07635059696622193, 0.07645873294677585, 0.07631359796505421, 0.6198731489712372, 0.752224643016234, 0.2589336549863219, 0.07873389695305377, 0.07810413604602218, 0.07768573798239231, 0.07828763802535832, 0.07759926002472639, 0.07301040191669017, 0.16768929501995444, 0.6300503599923104, 1.192017499008216, 0.07821254199370742, 0.0754999719792977, 0.07630452304147184, 0.07455950998701155, 0.07454268401488662, 0.07398487103637308, 0.07520310103427619, 0.07473234296776354, 0.07089906500186771, 0.07036739704199135, 0.07055947289336473, 0.07076507096644491, 0.07611307594925165, 1.6593898329883814, 0.10074626398272812, 0.08078547392506152, 0.08034060907084495, 0.07922569196671247, 0.08035735797602683, 0.07951429695822299, 0.0819287640042603, 0.08125606493558735, 0.07519700808916241, 1.9958464580122381, 0.20342813699971884, 0.08292834903113544, 0.07933131500612944, 0.07934211904648691, 0.07871378306299448, 0.07418975699692965, 0.07506563398055732, 0.07485581294167787, 0.07439985894598067, 0.13452933996450156, 0.07645984995178878, 0.07391092297621071, 0.07250507303979248, 0.07235470903106034, 0.07342183997388929, 0.0731692030094564, 0.07219782401807606, 0.08610134106129408, 0.7368988529779017, 0.5475614339811727, 0.07617503497749567, 0.07644669397268444, 0.07625276606995612, 0.0754458160372451, 0.07517434807959944, 1.7140403729863465, 1.4538443150231615, 0.07930552307516336, 0.0742275039665401, 0.07525380293373019, 0.07353433698881418, 0.07632599701173604, 0.07687233993783593, 0.07690021710004658, 0.07701993093360215, 1.7101872370112687, 0.11457035201601684, 0.07626730599440634, 0.07390865497291088, 0.07395194109994918, 0.07333123497664928, 0.07484246196690947, 0.07449848100077361, 0.07374451891519129, 1.1587652700254694, 0.9716880540363491, 0.08055274398066103, 0.07235359004698694, 0.07262417103629559, 0.07238392403814942, 0.07488380302675068, 0.07331967598292977, 0.07202200999017805, 0.07273058104328811, 0.8126363729825243, 0.07772666809614748, 0.0779280960559845, 0.07842834806069732, 0.07636707101482898, 0.0768673149868846, 0.07647782494314015, 0.07733017997816205, 0.07736604998353869, 0.07718655897770077, 0.09647638606838882, 0.07198115903884172, 0.07132313295733184, 0.07486670999787748, 0.0753897309768945, 0.07577818795107305, 0.07583305600564927, 0.07599412393756211, 0.07545864896383137, 1.832222472061403, 1.3043537019984797, 0.10558439197484404, 0.07582089898642153, 0.07543166598770767, 0.0753628850216046, 0.0746354250004515, 0.07460115104913712, 0.07472972897812724, 0.07480539102107286, 0.07661975896917284, 0.07693251105956733, 0.07538391696289182, 0.07481813698541373, 0.08407897304277867, 0.33966407796833664, 0.08083293109666556, 0.07542941102292389, 0.07473284692969173, 0.07464520109351724, 0.07481097499839962, 0.07755251496564597, 0.07566923298873007, 0.0753642920171842, 1.3526444700546563, 1.142498360015452, 0.07669668097514659, 0.0725506970193237, 0.07329478196334094, 0.07369000697508454, 0.07232166000176221, 0.07463639706838876, 0.07248210092075169, 0.8473683960037306, 0.15146583004388958, 0.09036367502994835, 0.07203829102218151, 0.07184084702748805, 0.07175285299308598, 0.07297035492956638, 0.07367606391198933, 0.07136915903538465, 0.07196484401356429, 0.16834316903259605, 0.23047363793011755, 0.07613274198956788, 0.07537971297279, 0.07762923603877425, 0.07147652900312096, 0.07413994602393359, 0.0728762389626354, 0.07196811097674072, 1.5649823650019243, 0.9546250039711595, 0.6697067440254614, 0.09910916292574257, 0.07648684305604547, 0.07622695493046194, 0.07154838601127267, 0.07206896797288209, 0.07163116696756333, 0.07207878504414111, 0.07171475992072374, 1.0959005790064111, 0.08130196190904826, 0.07195680006407201, 0.0708982179639861, 0.07140820706263185, 0.07166070002131164, 0.07169972301926464, 0.07262321899179369, 0.07194010401144624, 0.07196969201322645, 0.07550320599693805, 0.07503258006181568, 0.07676662795711309, 0.07522658398374915, 0.07524492708034813, 0.07500891899690032, 0.07532408891711384, 0.07837549201212823, 0.0751217040233314, 0.07501951802987605, 1.4089046800509095, 0.7427090919809416, 0.07980849000159651, 0.07757302094250917, 0.07150855893269181, 0.07255654805339873, 0.0710336669581011, 0.07117252703756094, 0.07878000801429152, 0.6022876760689542, 0.07470769493374974, 0.07190134399570525, 0.07235775806475431, 0.07258412393275648, 0.07227962696924806, 0.07115499605424702, 0.07162317796610296, 0.13754573499318212, 0.08015248796436936, 0.07725006202235818, 0.07168986706528813, 0.07154926296789199, 0.07176149799488485, 0.07127556309569627, 0.07173472899012268, 0.07263171602971852, 0.0704352060565725, 0.07567193906288594, 0.07247904897667468, 0.0748370949877426, 0.07262987701687962, 0.7650765490252525, 0.2168380639050156, 0.07680157606955618, 0.0761233480880037, 0.07125348492991179, 0.07127411291003227, 0.07218827994074672, 0.07411653897725046, 0.0732951780082658, 0.07625077606644481, 1.9097842470509931, 0.40388829400762916, 0.07420507806818932, 0.07370426098350435, 0.07702849595807493, 0.07369586895219982, 0.07384481001645327, 0.07362777600064874, 1.5374662729445845, 1.454924462013878, 0.07805983093567193, 0.07277879596222192, 0.0712245519971475, 0.071607960970141, 0.07135662494692951, 0.07497784297447652, 0.07789807301014662, 0.07584687403868884, 0.07735531299840659, 0.07641167903784662, 2.569069736986421, 0.07321789406705648, 0.07131769403349608, 0.07133412605617195, 0.08491357695311308, 0.0855944809736684, 0.07158457802142948, 0.07125653198454529, 0.0719718059990555, 2.176973659079522, 0.1484769779490307, 0.07804734795354307, 0.07701335696037859, 0.07649646501522511, 0.07238761894404888, 0.072157112066634, 0.07176951505243778, 0.07664350711274892, 0.07628750894218683, 0.07686042401473969, 0.07659794401843101, 0.07212399097625166, 0.07188448996748775, 0.07186248106881976, 0.07212042598985136, 0.07219423400238156, 0.07218007394112647, 0.07194581592921168, 0.07254012592602521, 0.07217457506339997, 0.07246932503767312, 0.07260935101658106, 0.07233121106401086, 0.07259570294991136, 0.07203584001399577, 0.07260043395217508, 0.07254593493416905, 0.07250329398084432, 0.07225458696484566, 0.07154655607882887, 0.0719680329784751, 0.07231356902047992, 0.07227121607866138, 0.0727769109653309, 0.07230290200095624, 0.07539433101192117, 0.07370674598496407, 0.07335453201085329, 0.07331012701615691, 0.07278206502087414, 0.07349057798273861, 0.07373035291675478, 0.07302411796990782, 0.07337991602253169, 0.0730509819695726, 0.0749324340140447, 0.07294332794845104, 0.07304076291620731, 0.07331168500240892, 0.0737251719692722, 0.07409830007236451, 0.07380490098148584, 0.07344602898228914, 1.286455709952861, 0.7283633049810305, 0.07732057198882103, 0.0725549190538004, 0.07169379491824657, 0.07168781897053123, 0.07277841807808727, 0.07265463809017092, 0.07062296802178025, 0.07256280293222517, 0.07931700197514147, 1.3364310819888487, 0.13911627291236073, 0.07630393491126597, 0.07588632998522371, 0.07639297004789114, 0.07590514898765832, 0.07592302793636918, 0.5692937700077891, 0.22396281396504492, 0.0808688810793683, 0.07174545398447663, 0.07097170792985708, 0.07111416000407189, 0.07468396902550012, 0.07316304906271398, 0.07292662002146244, 0.07243384793400764, 0.07250937703065574, 0.0741132030962035, 0.07682894601020962, 0.07705742795951664, 0.07692722708452493, 0.0764945870032534, 0.0767737430287525, 0.07994903100188822, 0.07781838392838836, 0.07713190896902233, 0.9640839280327782, 0.07820469804573804, 0.07267871696967632, 0.07239837991073728, 0.07150185003411025, 0.07711926498450339, 0.07771868107374758, 0.07540383108425885, 0.07627028995193541, 0.075465610018, 1.5505055129760876, 0.27935171709395945, 0.0785616299835965, 0.0776502740336582, 0.07799595210235566, 0.07682561699766666, 0.07779261202085763, 0.08030430402141064, 0.07824731897562742, 1.265869040042162, 0.08306884206831455, 0.07735197304282337, 0.07705766102299094, 0.9046314320294186, 0.22324432502500713, 0.09614304802380502, 0.08911531697958708, 0.08808632893487811, 0.0870398780098185, 0.084341497044079, 1.1465723240980878, 0.856239284039475, 0.08773271506652236, 0.08515812107361853, 0.08524371800012887, 0.0854960810393095, 0.08589296089485288, 0.08494796603918076, 1.0766689189476892, 0.5443565710447729, 0.35591785702854395, 0.18987111805472523, 0.09100331598892808, 0.08589479001238942, 0.09996877901721746, 0.08648543001618236, 0.08624080696608871, 0.9599235110217705, 0.6641568179475144, 0.1573711499804631, 0.08980119205079973, 0.090488365967758, 0.09148599894251674, 0.08817838202230632, 0.08540446497499943, 1.1944561949931085, 0.26379113702569157, 0.09595576697029173, 0.09019028092734516, 0.09239554405212402, 0.09211887302808464, 1.335428006015718, 0.37391568603925407, 0.31407444702927023, 0.09456078393850476, 0.09249478403944522, 0.08893550897482783, 0.08816077094525099, 0.08824648300651461, 0.09529941098298877, 0.09275529603473842, 1.3817566110519692, 0.11656293098349124, 0.09063829702790827, 0.09136551897972822, 0.0903258480830118, 0.09327907499391586, 1.9173914979910478, 0.9831968249054626, 0.09045067592523992, 0.09177744796033949, 0.08970720798242837, 0.09025648597162217, 1.0556700839661062, 0.1027953380253166, 0.09216958191245794, 0.09068372298497707, 0.09270196792203933, 0.09309565601870418, 1.0625671279849485, 0.20649934106040746, 0.0868759520817548, 0.08634347503539175, 0.08547479496337473, 0.08952845796011388, 0.08636238798499107, 0.08631693501956761, 0.08838028798345476, 1.128979505971074, 1.6016215709969401, 0.38343653199262917, 0.0964121021097526, 0.08587753190658987, 0.0861017779679969, 0.08545102190691978, 0.08527936309110373, 0.08517482306342572, 0.08512525295373052, 0.08815042907372117, 0.08589584194123745, 0.08506591396871954, 0.28732802299782634, 0.09500672703143209, 0.09341722400858998, 0.09327076200861484, 0.09260475903283805, 0.09306200500577688, 0.09377972292713821, 0.09335852204822004, 1.3808980169706047, 1.9828330019954592, 0.08849550399463624, 0.08834533300250769, 0.09033790102694184, 0.08657808206044137, 0.08983989502303302, 0.08586979797109962, 0.08728030603379011, 0.08561107097193599, 0.08586321701295674, 0.09029065200593323, 0.4199288228992373, 0.08742009405978024, 0.08620199793949723, 0.08536841499153525, 0.0864664550172165, 0.08796622802037746, 0.08956291200593114, 1.3445701979799196, 0.14081659901421517, 0.08303806709591299, 0.08218756492715329, 0.08260784298181534, 0.0841549530159682, 0.08261484489776194, 0.0834454569267109, 0.08337457699235529, 1.7623554749879986, 0.1055744809564203, 0.08417705504689366, 0.0843025438953191, 0.09117275103926659, 0.08714256098028272, 0.08660370390862226, 0.09092566603794694, 1.0162527519278228, 0.3522171910153702, 0.3113861889578402, 0.2783387230010703, 0.08536276000086218, 0.08370092592667788, 0.17378449393436313, 0.08276489400304854, 0.08462290500756353, 0.08528174494858831, 0.08313767600338906, 1.0796459650155157, 0.09219285694416612, 0.08402200893033296, 0.08328825701028109, 0.08614764595404267, 0.08411354303825647, 0.08419538906309754, 0.08319074893370271, 0.9095992720685899, 0.13329044508282095, 0.08633307891432196, 0.08380930195562541, 0.08445916895288974, 0.08240828406997025, 0.08454438101034611, 0.08332976698875427, 0.08302451798226684, 0.08238700602669269, 0.3744497849838808, 1.7669916650047526, 0.6323601240292192, 0.08983990992419422, 0.08616662095300853, 0.08632807806134224, 0.08576099202036858, 0.08636574493721128, 0.08719731692690402, 0.08591536106541753, 0.668586045037955, 0.6392576249781996, 0.16138433502055705, 0.0866437389049679, 0.08184171305038035, 0.08083456999156624, 0.0801967749139294, 0.08160028595011681, 0.08121359290089458, 0.08212828601244837, 0.0817229050444439, 0.0817218740703538, 0.08151635190006346, 1.7596401229966432, 0.8223597990581766, 0.09182630002032965, 0.088273984962143, 0.0823173860553652, 0.08341144397854805, 0.08203552500344813, 0.08445515809580684, 0.08173839293885976, 0.08279312902595848, 0.08198759506922215, 0.08269579394254833, 0.08369682589545846, 0.6658831000095233, 0.8485961960395798, 0.43987275497056544, 0.10886444593779743, 0.08286354702431709, 0.08265456894878298, 0.08211457601282746, 0.08362640999257565, 0.08445100497920066, 0.08272026502527297, 0.08575325598940253, 0.08308370294980705, 0.08376366703305393, 0.0897565430495888, 0.4890235309721902, 0.08449811197351664, 0.08148371998686343, 0.08132177300285548, 0.08040444401558489, 0.08106539596337825, 0.08127397589851171, 0.08115640806499869, 0.08069753996096551, 0.08010408899281174, 0.08155426906887442, 0.08085265499539673, 0.08159387693740427, 0.09217858291231096, 1.2165955890668556, 0.0874401779146865, 0.08285566000267863, 0.08248465799260885, 0.08239731297362596, 0.08230820100288838, 0.0822038889164105, 0.08251832204405218, 0.08139041299000382, 0.08212923991959542, 0.08819364104419947, 0.08619312196969986, 0.08758366690017283, 1.6820907030487433, 0.31679127900861204, 0.08604792796541005, 0.08561309392098337, 0.08025894092861563, 0.07993068802170455, 0.08490921999327838, 0.07947053795214742, 0.07977721805218607, 0.07998785097151995, 0.08107334200758487, 0.08039974700659513, 0.08089151897002012, 0.08116065291687846, 1.5576991190901026, 0.08601568103767931, 0.08501604304183275, 0.08102422603406012, 0.08096583199221641, 0.08024146093521267, 0.08083062001969665, 0.08025932498276234, 0.07985462306533009, 0.08791608293540776, 0.08019146299920976, 0.08268892194610089, 0.08500476798508316, 0.08420298504643142, 0.08008857700042427, 0.07949949498288333, 0.08039959904272109, 0.07985039905179292, 0.07957698206882924, 0.0794957330217585, 0.08050749602261931, 0.07947641401551664, 0.08065441704820842, 0.0838456159690395, 1.3928558429470286, 1.0653785550966859, 0.08953723905142397, 0.083232939010486, 0.082437381031923, 0.08264412893913686, 0.0863129299832508, 0.08598773402627558, 0.08688078005798161, 0.08597636595368385, 0.08661054004915059, 0.08716226601973176, 0.5036463539581746, 0.6165031030541286, 1.0803453179541975, 0.08527082798536867, 0.08461658202577382, 0.08356118598021567, 0.08410686103161424, 0.08384450303856283, 0.08364197507034987, 0.08081065607257187, 0.0781983140623197, 0.07818547100760043, 0.07725685590412468, 2.013372292974964, 0.07820990192703903, 0.0780316439922899, 0.07834979891777039, 0.07869329699315131, 0.0788129159482196, 0.07825508899986744, 0.07516932103317231, 0.07434335688594729, 0.9431010570842773, 0.08485382609069347, 0.0818516839062795, 0.07737442397046834, 0.07646820088848472, 0.07772510999348015, 0.07606478605885059, 0.07640492694918066, 0.07607173104770482, 0.07536859309766442, 0.07620403100736439, 0.07592179300263524, 0.07520651805680245, 0.07503301999531686, 0.07553392299450934, 0.07524781406391412, 0.07527769904118031, 0.07487904094159603, 0.07697160099633038, 0.07482636696659029, 0.07518707495182753, 0.07938228896819055, 0.7700187460286543, 0.08039002504665405, 0.07871646003331989, 0.07788247102871537, 0.07790477899834514, 0.07773963594809175, 0.07813502207864076, 0.07798297505360097, 0.0777515749214217, 0.07786157005466521, 0.07789524889085442, 0.07776332495268434, 0.07794635591562837, 0.10118066496215761, 0.07839903305284679, 0.07926268992014229, 0.07936207891907543, 1.3389819209696725, 0.2578871469013393, 0.07936631399206817, 0.07886988203972578, 0.07796692289412022, 0.07766394398640841, 0.07748915604315698, 0.07724892999976873, 0.07737162197008729, 0.07685982505790889, 0.07685505098197609, 0.07693979609757662, 0.08181318000424653, 0.08324383303988725, 0.0832008559955284, 0.09190145297907293, 1.0509662840049714, 0.08722518594004214, 0.08271220396272838, 0.08223873807583004, 0.07880644500255585, 0.07897416001651436, 0.07897357502952218, 0.08121885394211859, 0.07811960706021637, 0.08010422205552459, 0.07941735500935465, 0.07871968299150467, 0.649016547948122, 0.32802897703368217, 0.5683680939255282, 0.131874505081214, 0.0822055289754644, 0.07955065404530615, 0.07450117904227227, 0.07504590996541083, 0.07806991797406226, 0.07726547692436725, 0.07716167403850704, 0.07642737601418048, 0.08171591605059803, 0.11604726302903146, 0.08229281706735492, 0.08203076804056764, 0.07709587807767093, 0.07714322907850146, 0.07731460803188384, 0.07751416403334588, 0.07747537305112928, 0.07707714790012687, 0.07700552802998573, 0.07787865202408284, 0.0777996750548482, 0.07760231394786388, 0.16840476903598756, 0.077074597007595, 0.08193628001026809, 0.0776431440608576, 0.07824972493108362, 0.0781371439807117, 1.6800217109266669, 0.08700279705226421, 0.08305412903428078, 0.07700329506769776, 0.07655456499196589, 0.07657492498401552, 0.07649069896433502, 0.07586177799385041, 0.07599724305327982, 0.08026466495357454, 0.08251341397408396, 0.08287660602945834, 0.08484953094739467, 0.6907863490050659, 0.5752865540562198, 0.07765415904577821, 0.07624832401052117, 0.07570609299000353, 0.07596355606801808, 0.07747092691715807, 0.07596726308111101, 0.0757344929734245, 0.07663506199605763, 0.07607200997881591, 0.07633419905323535, 0.07666844001505524, 0.9196741400519386, 0.25995081302244216, 0.18454715295229107, 0.0832559079863131, 0.07916940608993173, 0.07637617492582649, 0.0768981350120157, 0.0778083010809496, 0.07783595402725041, 0.07810280006378889, 0.07826397393364459, 0.07929426000919193, 0.08159164199605584, 0.8399340520845726, 1.0665737580275163, 0.07520333305001259, 0.07466065301559865, 0.07450592902023345, 0.16859101504087448, 0.07424726092722267, 0.07419077202212065, 0.07493254903238267, 0.07483632804360241, 0.1704336340771988, 2.5038571070181206, 0.43323875102214515, 0.0841063839616254, 0.07691812503617257, 0.07830149307847023, 0.0767028609989211, 0.07602620299439877, 0.07548531610518694, 0.07501746399793774, 0.07638720399700105, 0.07555058400612324, 0.07600659504532814, 0.0758828220423311, 0.07620299805421382, 0.07670871808659285, 0.07643154892139137, 1.2083000399870798, 0.1335268709808588, 0.08307987998705357, 0.0830013370141387, 0.08293573895934969, 0.08282698295079172, 0.07719805603846908, 0.07646082702558488, 0.07679109205491841, 0.08223115699365735, 0.08485676697455347, 0.0818998230388388, 0.08185355796013027, 1.111385694006458, 0.10308887006249279, 0.08425777300726622, 0.08383372193202376, 0.0776548219146207, 0.07795244897715747, 0.07699318998493254, 0.07722285005729645, 0.07805168791674078, 0.07986304198857397, 0.08269807696342468, 0.08250287000555545, 0.08253537595737725, 0.08238775294739753, 0.08235605305526406, 0.08273779100272804, 0.08398307499010116, 0.08239253296051174, 1.5750591319520026, 0.08334894489962608, 0.07781841093674302, 0.07697447203099728, 0.07759444299153984, 0.07709194498602301, 0.08013089699670672, 0.07707930798642337, 0.07852102303877473, 0.07694132905453444, 0.34908359800465405, 0.15949407708831131, 0.08005059789866209, 0.07803616602905095, 0.07786931307055056, 0.08454277494456619, 0.08431522001046687, 0.0840711189666763, 0.07566773402504623, 0.07681124494411051, 0.08991459698881954, 0.07868879497982562, 0.07818379602394998, 0.08025165996514261, 0.07683983398601413, 0.07807473302818835, 0.077883091988042, 0.0792044389527291, 0.07808692404069006, 0.07904072699602693, 0.0780204450711608, 0.07828454207628965, 0.07725534099154174, 0.08415576396510005, 0.08281594200525433, 0.07862266502343118, 0.07606687303632498, 0.0787097989814356, 0.07711825799196959, 0.07746546901762486, 0.07646582298912108, 0.0762857630616054, 0.07636043196544051, 0.07667860598303378, 0.07688320300076157, 0.07708729803562164, 0.07752379204612225, 0.08148521301336586, 0.9815361360087991, 0.8779542469419539, 0.4151470350334421, 0.12618194497190416, 0.4846491679782048, 0.26665977004449815, 0.08111162506975234, 0.07909395801834762, 0.07859807705972344, 0.07874970103148371, 0.07884031999856234, 0.07917702302802354, 0.07879100809805095, 0.07901291293092072, 0.07823913800530136, 0.6690357490442693, 0.08074093703180552, 0.08024151204153895, 0.0759900629054755, 0.07470243808347732, 0.0758641290012747, 0.07632870995439589, 0.07664647197816521, 0.07657078304328024, 0.07527514093089849, 0.07525528792757541, 0.07442514097783715, 0.07491581293288618, 0.07520278089214116, 0.0749459060607478, 0.07722160301636904, 0.0762729779817164, 0.07603145390748978, 0.07600498397368938, 0.07611289597116411, 0.07576641498599201, 0.09415548504330218, 0.48485632392112166, 0.08115537895355374, 0.08006223093252629, 0.07931244897190481, 0.07654134195763618, 0.0754193679895252, 0.07653896603733301, 0.0769044259795919, 0.4548505350248888, 0.7016866520280018, 0.2709203119156882, 0.07610662106890231, 0.07540354004595429, 0.07564484293106943, 0.07500056200660765, 0.07484221702907234, 0.07631154905539006, 0.07522872602567077]
[0.0014447156739022052, 0.0014323350208412325, 0.0014333945119335335, 0.001422557122150094, 0.0014365518783998428, 0.0014414108971285882, 0.0014306453260954242, 0.0014341763667382148, 0.0014259319595650447, 0.0014305540398523516, 0.0014386464680107881, 0.0014313644686789842, 0.0014330858782846101, 0.0014275952446635585, 0.001425228205186372, 0.0014229609193850536, 0.0014266025925967463, 0.001442376122695907, 0.001414120346972985, 0.0014202792235479063, 0.001422523428229805, 0.0014282765308851187, 0.0014274939585819232, 0.0014260031035816182, 0.0014338220626457917, 0.001437513183859386, 0.0014363433880617423, 0.001442685509480688, 0.001445040368113895, 0.00154033634688097, 0.001535555162486069, 0.0015359126525569934, 0.0015424968584497668, 0.0015413858571412917, 0.0015544063264352022, 0.0015401245493974005, 0.00143761220340598, 0.0014427831439225345, 0.0014400644684020355, 0.001445430855989, 0.0014414245509827624, 0.0015432945101008732, 0.001540667018663062, 0.0015533006754798852, 0.0017516660831906662, 0.001501047019181507, 0.0015545192231633225, 0.05027095959236731, 0.0016512862021787738, 0.0015500060197117986, 0.001562060142049984, 0.0015595543687707003, 0.0015457883662525185, 0.02808711922261864, 0.012406571632802335, 0.0015800607732820268, 0.0015338607541076383, 0.0015481144085298388, 0.0015543174492765445, 0.001548030777188132, 0.0015435218787276928, 0.014541826591047705, 0.006001353285237387, 0.00172037744837604, 0.0016894271426206948, 0.001652371551727458, 0.0016572259993729542, 0.0015104678364432588, 0.0015006704488769174, 0.001511670651427489, 0.001505364122211325, 0.0015269067965219825, 0.0015098677962372194, 0.0015077870410429885, 0.0014506994713363902, 0.0015597045516633258, 0.0015738854275028013, 0.0016023798783936975, 0.00158679853098429, 0.0016114546950640424, 0.027161599305572406, 0.04005745920466677, 0.015588616797871584, 0.004766562511688288, 0.0014288583057647456, 0.0014304814703420413, 0.001445501674517837, 0.001429215123477791, 0.0014447324685942457, 0.0014388840407969393, 0.026552978161798448, 0.001652275347530994, 0.0016047573049685784, 0.0014570994487944611, 0.0014488407748048098, 0.0014577809797257793, 0.0014690644492642308, 0.0014533716947676576, 0.0014409079187910777, 0.0014551248786286736, 0.0014560518558232151, 0.0014445365297284965, 0.01953088922416601, 0.0018513432456826677, 0.0016597034908565028, 0.001686351613749807, 0.001651927652977863, 0.0016688636732192673, 0.022052462918360775, 0.00623342855025691, 0.005346698938317749, 0.004564291122844633, 0.001551281387575579, 0.001559940815371062, 0.0015065446923657948, 0.0015107092438075616, 0.0015126831226088867, 0.001494202570400524, 0.0015048828375126635, 0.0015145523654182954, 0.001518664959989184, 0.0015052259396001392, 0.0015062370826965387, 0.0015006672652742388, 0.0014942445084262564, 0.00149572963294174, 0.0015200588172681782, 0.001531413369051808, 0.022682286164609298, 0.00828409271210204, 0.02304203147116136, 0.0016589708369681422, 0.0015875962040178021, 0.0014615583456862643, 0.0014566050424259535, 0.0014527491412638705, 0.001440279613420063, 0.0014658255089188413, 0.0014461926731033897, 0.0014472203281689056, 0.0015653684492964223, 0.0015585057328132037, 0.001576804489429508, 0.038401586082003704, 0.0016943944299745621, 0.0015855684676872833, 0.00155686283702677, 0.0015472957949933348, 0.0014616175511927934, 0.0014540943464416327, 0.0014986266947484442, 0.0014613515920746997, 0.05195435579410013, 0.024178749449285015, 0.0015542725295930797, 0.0015514059804798085, 0.0015441597349067427, 0.0015315111436672052, 0.0015571566550441238, 0.0015581754482902434, 0.0015603823050362418, 0.0015574203666337595, 0.012650472427984434, 0.015351523326861918, 0.005284360305843305, 0.0016068142235317097, 0.0015939619601229016, 0.0015854232241304554, 0.0015977068984767003, 0.001583658367851559, 0.001490008202381432, 0.003422230510611315, 0.012858170612087967, 0.024326887734861552, 0.0015961743264021923, 0.0015408157546795448, 0.0015572351641116701, 0.0015216226527961542, 0.001521279265609931, 0.00150989532727292, 0.0015347571639648201, 0.0015251498564849704, 0.0014469196939156676, 0.0014360693273875787, 0.001439989242721729, 0.0014441851217641818, 0.0015533280805969725, 0.03386509863241595, 0.0020560462037291453, 0.0016486831413277862, 0.0016396042667519376, 0.0016168508564635199, 0.0016399460811434047, 0.0016227407542494486, 0.0016720155919236796, 0.0016582870395017825, 0.0015346328181461717, 0.040731560367596696, 0.004151594632647323, 0.001692415286349703, 0.0016190064286965193, 0.0016192269193160596, 0.0016064037359794791, 0.0015140766734067276, 0.001531951713888925, 0.0015276696518709768, 0.0015183644682853197, 0.0027454967339694196, 0.0015604051010569139, 0.0015083861831879737, 0.0014796953681590302, 0.0014766267149195988, 0.0014984048974263121, 0.0014932490410093143, 0.001473424979960736, 0.0017571702257406954, 0.015038752101589831, 0.011174723142472913, 0.0015545925505611362, 0.0015601366116874376, 0.0015561788993868598, 0.0015397105313723488, 0.001534170368971417, 0.03498041577523156, 0.029670292143329825, 0.001618480062758436, 0.0015148470197253081, 0.0015357918966067384, 0.0015007007548737588, 0.0015576734084027763, 0.001568823264037468, 0.0015693921857152361, 0.0015718353251755542, 0.03490178034716875, 0.0023381704493064663, 0.001556475632538905, 0.0015083398974063446, 0.001509223287754065, 0.0014965558158499853, 0.0015273971829981524, 0.001520377163281094, 0.0015049901819426793, 0.023648270816846312, 0.01983036844972141, 0.0016439335506257353, 0.0014766038785099375, 0.0014821259395162367, 0.0014772229395540698, 0.0015282408780969527, 0.0014963199180189747, 0.0014698369385750623, 0.0014842975723120024, 0.016584415775153558, 0.0015862585325744382, 0.0015903693072649898, 0.0016005785318509657, 0.0015585116533638568, 0.001568720714018053, 0.001560771937615105, 0.0015781669383298379, 0.0015788989792558917, 0.0015752358975040974, 0.001968905838130384, 0.0014690032456906475, 0.001455574141986364, 0.0015278920407730098, 0.0015385659383039694, 0.0015464936316545521, 0.0015476133878703933, 0.0015509004885216756, 0.0015399724278332932, 0.0373922953481919, 0.026619463306091422, 0.0021547835096906946, 0.001547365285437174, 0.001539421754851177, 0.0015380180616654, 0.0015231719387847247, 0.0015224724703905533, 0.0015250965097576988, 0.0015266406330831197, 0.0015636685503912823, 0.0015700512461136191, 0.0015384472849569758, 0.001526900754804362, 0.0017158974090362995, 0.006931919958537483, 0.001649651655033991, 0.001539375735161712, 0.0015251601414222802, 0.001523371450888107, 0.0015267545918040738, 0.0015827043870539994, 0.0015442700609944913, 0.001538046775860902, 0.027604989184788903, 0.023316293061539834, 0.0015652383872478896, 0.0014806264697821165, 0.0014958118768028763, 0.0015038776933690723, 0.0014759522449339227, 0.001523191776905893, 0.0014792265494030957, 0.017293232571504707, 0.003091139388650808, 0.001844156633264252, 0.0014701692045343165, 0.0014661397352548583, 0.0014643439386344077, 0.0014891909169299261, 0.0015035931410610067, 0.0014565134497017277, 0.001468670285991108, 0.003435574878216246, 0.004703543631226889, 0.0015537294283585281, 0.0015383614892406122, 0.0015842701232402908, 0.0014587046735330808, 0.0015130601229374201, 0.0014872701829109266, 0.0014687369587089941, 0.03193841561228417, 0.01948214293818693, 0.01366748457194819, 0.002022635978076379, 0.001560955980735622, 0.001555652141437999, 0.0014601711430871974, 0.0014707952647526957, 0.0014618605503584354, 0.001470995613145737, 0.001463566528994362, 0.02236531793890635, 0.0016592237124295564, 0.0014685061237565717, 0.0014469024074282876, 0.0014573103482169764, 0.001462463265741054, 0.0014632596534543804, 0.001482106510036606, 0.0014681653879886987, 0.0014687692247597234, 0.001540881755039552, 0.0015312771441186874, 0.0015666658766757772, 0.0015352364078316154, 0.0015356107567417988, 0.0015307942652428637, 0.0015372263044308946, 0.001599499836982209, 0.001533096000476151, 0.0015310105720382867, 0.028753156735732848, 0.015157328407774319, 0.0016287446939101328, 0.0015831228763777383, 0.001459358345565139, 0.0014807458786407904, 0.001449666672614308, 0.001452500551786958, 0.0016077552655977862, 0.012291585225897024, 0.0015246468353826478, 0.0014673743672592907, 0.0014766889400970266, 0.0014813086516889079, 0.0014750944279438378, 0.0014521427766172861, 0.0014616975095123053, 0.0028070558161873904, 0.0016357650604973339, 0.0015765318780073098, 0.0014630585115364924, 0.001460189040161061, 0.001464520367242548, 0.0014546033284835974, 0.0014639740610229118, 0.0014822799189738473, 0.00143745318482801, 0.0015443252869976722, 0.0014791642648300954, 0.0015272876528110734, 0.001482242388099584, 0.015613807122964335, 0.0044252666103064405, 0.0015673791034603302, 0.001553537716081708, 0.0014541527536716691, 0.0014545737328578014, 0.0014732302028723822, 0.001512582428107152, 0.0014958199593523632, 0.0015561382870703023, 0.03897518871532639, 0.008242618245053656, 0.0015143893483303944, 0.0015041685915000889, 0.0015720101215933658, 0.0015039973255550983, 0.0015070369391112911, 0.0015026076734826273, 0.031376862713154784, 0.029692335959466895, 0.0015930577741973863, 0.001485281550249427, 0.0014535622856560715, 0.001461386958574306, 0.0014562576519781534, 0.0015301600607036023, 0.0015897565920438086, 0.0015478953885446703, 0.0015786798571103386, 0.0015594220211805434, 0.05242999463237594, 0.001494242736062377, 0.0014554631435407363, 0.0014557984909422848, 0.0017329301419002669, 0.0017468261423197631, 0.0014609097555393772, 0.001454214938460108, 0.0014688123673276634, 0.04442803385876575, 0.0030301424071230753, 0.0015928030194600625, 0.0015717011624567059, 0.001561152347249492, 0.001477298345796916, 0.001472594123808857, 0.0014646839806619957, 0.001564153206382631, 0.0015568879375956496, 0.001568580081933463, 0.0015632233473149184, 0.0014719181831888094, 0.0014670304074997501, 0.0014665812463024442, 0.0014718454283643135, 0.0014733517143343175, 0.0014730627334923769, 0.0014682819577390138, 0.001480410733184188, 0.0014729505114979586, 0.0014789658170953697, 0.0014818234901343072, 0.0014761471645716503, 0.0014815449581614562, 0.0014701191839590973, 0.0014816415092280628, 0.001480529284370797, 0.0014796590608335575, 0.0014745834074458297, 0.0014601337975271198, 0.001468735366907655, 0.001475787122866937, 0.0014749227771155384, 0.0014852430809251204, 0.0014755694285909437, 0.0015386598165698197, 0.0015042193058155933, 0.0014970312655276181, 0.0014961250411460595, 0.0014853482657321254, 0.0014998077139334411, 0.001504701079933771, 0.0014902881218348534, 0.0014975493065822795, 0.0014908363667259716, 0.0015292333472254021, 0.001488639345886756, 0.0014906278146164758, 0.0014961568367838555, 0.0015045953463116775, 0.0015122102055584593, 0.001506222469009915, 0.001498898550658962, 0.02625419816230329, 0.014864557244510827, 0.001577970856914715, 0.0014807126337510286, 0.0014631386718009503, 0.0014630167136843108, 0.0014852738383283116, 0.0014827477161259372, 0.0014412850616689846, 0.001480873529229085, 0.0016187143260232952, 0.027274103714058136, 0.0028391076104563413, 0.0015572231614544075, 0.0015487006119433411, 0.0015590402050590028, 0.0015490846732175167, 0.00154944954972182, 0.011618240204240595, 0.004570669672756019, 0.0016503853281503733, 0.0014641929384587066, 0.0014484022026501444, 0.001451309387838202, 0.001524162633173472, 0.001493123450259469, 0.0014882983677849478, 0.0014782417945715847, 0.00147978320470726, 0.0015125143489021125, 0.0015679376736777474, 0.0015726005706023804, 0.0015699434098882638, 0.0015611140204745593, 0.001566811082219439, 0.0016316128775895554, 0.0015881302842528236, 0.0015741205912045374, 0.019675182204750577, 0.0015960142458313886, 0.0014832391218301288, 0.0014775179573619853, 0.001459221429267556, 0.0015738625507041508, 0.0015860955321172975, 0.0015388536955971196, 0.001556536529631335, 0.0015401144901632654, 0.03164296965257322, 0.005701055450897131, 0.001603298571093806, 0.0015846994700746573, 0.0015917541245378706, 0.0015678697346462583, 0.0015876043269562783, 0.0016388633473757275, 0.0015968840607270903, 0.025834062041676775, 0.001695282491190093, 0.0015786116947514974, 0.0015726053269998152, 0.018461865959784055, 0.004556006633163411, 0.00196210302089398, 0.00181867993835892, 0.0017976801823444512, 0.001776324041016704, 0.001721255041715898, 0.023399435185675263, 0.01747427110284643, 0.0017904635727861707, 0.001737920838237113, 0.0017396677142883443, 0.0017448179803940716, 0.0017529175692827118, 0.0017336319599832808, 0.021972835080565085, 0.011109317776423936, 0.007263629735276407, 0.0038749207766270457, 0.0018572105303862874, 0.001752954898212029, 0.0020401791636166827, 0.0017650087758404563, 0.001760016468695688, 0.019590275735138173, 0.013554220774439069, 0.0032116561220502673, 0.0018326773887918312, 0.0018467013462807756, 0.001867061202908505, 0.0017995588167817617, 0.0017429482647959068, 0.024376657040675685, 0.005383492592361053, 0.001958280958577382, 0.0018406179781090847, 0.001885623348002531, 0.0018799770005731558, 0.02725363277583098, 0.007630932368148042, 0.006409682592434086, 0.001929811917112342, 0.001887648653866229, 0.0018150103872413843, 0.0017991994070459384, 0.0018009486327860125, 0.0019448859384283423, 0.0018929652251987433, 0.028199114511264677, 0.002378835326193699, 0.0018497611638348625, 0.0018646024281577188, 0.001843384654755343, 0.0019036545917125685, 0.03913043873451118, 0.020065241324601277, 0.0018459321617395903, 0.0018730091420477446, 0.0018307593465801708, 0.001841969101461677, 0.021544287427879717, 0.0020978640413329918, 0.0018810118757644479, 0.0018506882241832055, 0.0018918768963681496, 0.0018999113473204933, 0.021685043428264255, 0.004214272266538928, 0.0017729786139133634, 0.001762111735416158, 0.0017443835706811171, 0.0018271113869410996, 0.0017624977139794097, 0.0017615701024401554, 0.0018036793466011177, 0.02304039808104233, 0.03268615451014163, 0.00782523534678835, 0.0019675939206071956, 0.0017526026919712217, 0.0017571791422040183, 0.001743898406263669, 0.001740395165124566, 0.0017382616951719535, 0.0017372500602802147, 0.0017989883484432893, 0.0017529763661477031, 0.0017360390605861132, 0.0058638372040372725, 0.0019389127965598386, 0.0019064739593589793, 0.0019034849389513232, 0.0018898930414864908, 0.0018992245919546302, 0.0019138718964722082, 0.001905275960167756, 0.028181592183073565, 0.04046597963256039, 0.0018060306937680865, 0.001802965979643014, 0.0018436306332028946, 0.0017668996338865586, 0.001833467245368021, 0.0017524448565530534, 0.0017812307353834717, 0.0017471647137129794, 0.0017523105512848313, 0.001842666367468025, 0.008569975977535454, 0.001784083552240413, 0.0017592244477448414, 0.001742212550847658, 0.0017646215309636022, 0.0017952291432730093, 0.0018278145307332886, 0.027440208122039175, 0.0028738081431472482, 0.0016946544305288366, 0.0016772972434112917, 0.00168587434656766, 0.001717448020734045, 0.0016860172428114681, 0.0017029685087083858, 0.0017015219794358223, 0.0359664382650612, 0.0021545812440085777, 0.0017178990825896664, 0.0017204600794963082, 0.0018606683885564609, 0.0017784196118425047, 0.0017674225287473931, 0.0018556258375091212, 0.02073985208015965, 0.007188105939089188, 0.006354820182813066, 0.00568038210206266, 0.0017420971428747385, 0.0017081821617689363, 0.0035466223251910843, 0.0016890794694499702, 0.0017269980613788475, 0.001740443774460986, 0.001696687265375287, 0.022033591122765626, 0.0018814868764115535, 0.0017147348761292441, 0.0016997603471485935, 0.0017581152235518914, 0.0017166029191480912, 0.001718273246185664, 0.001697770386402096, 0.018563250450379386, 0.0027202131649555297, 0.00176189956968004, 0.0017103939174617432, 0.0017236565092426477, 0.0016818017157136786, 0.00172539553082339, 0.0017006074895664137, 0.0016943779180054457, 0.001681367469932504, 0.007641832346609812, 0.03606105438785209, 0.012905308653657533, 0.0018334675494733515, 0.0017585024684287456, 0.001761797511455964, 0.0017502243269462974, 0.0017625662232083933, 0.0017795370801408983, 0.0017533747156207658, 0.0136446131640399, 0.01304607397914693, 0.0032935578575623886, 0.001768239569489141, 0.0016702390418444968, 0.0016496851018686987, 0.0016366688757944777, 0.0016653119581656493, 0.0016574202632835629, 0.0016760874696418035, 0.0016678143886621206, 0.0016677933483745676, 0.0016635990183686419, 0.03591102291829884, 0.016782853042003602, 0.0018740061228638704, 0.001801509897186592, 0.0016799466541911267, 0.0017022743669091438, 0.001674194387825472, 0.001723574655016466, 0.0016681304681399952, 0.001689655694407316, 0.0016732162259024928, 0.0016876692641336396, 0.0017080984876624175, 0.013589451020602517, 0.017318289715093464, 0.008976994999399295, 0.0022217233864856617, 0.0016910927964146345, 0.0016868279377302649, 0.0016758076737311725, 0.001706661428419911, 0.0017234898975347073, 0.0016881686739851625, 0.001750066448763317, 0.0016955857744858581, 0.0017094625925113047, 0.0018317661846854857, 0.009980072060656943, 0.0017244512647656457, 0.0016629330609563965, 0.0016596280204664385, 0.0016409070207262222, 0.0016543958359873112, 0.0016586525693573818, 0.0016562532258162998, 0.0016468885706319492, 0.001634777326383913, 0.0016643728381402943, 0.0016500541835795253, 0.0016651811619878424, 0.0018811955696389992, 0.024828481409527665, 0.0017844934268303368, 0.00169093183678936, 0.001683360367196099, 0.0016815778157882849, 0.0016797592041405793, 0.001677630386049194, 0.0016840473886541262, 0.0016610288365306904, 0.0016761069371346005, 0.001799870225391826, 0.0017590433055040787, 0.0017874217734729148, 0.03432838169487231, 0.006465128143032899, 0.0017560801625593888, 0.0017472059983874159, 0.0016379375699717477, 0.001631238531055195, 0.0017328412243526202, 0.0016218477133091312, 0.0016281064908609403, 0.001632405121867754, 0.0016545580001547933, 0.0016408111633999006, 0.0016508473259187779, 0.0016563398554464992, 0.03178977794061434, 0.0017554220619934555, 0.0017350212865680152, 0.0016535556333481657, 0.0016523639182084982, 0.0016375808354125035, 0.0016496044901978908, 0.0016379454078114762, 0.0016296861850067365, 0.0017942057741919951, 0.0016365604693716277, 0.0016875290193081815, 0.0017347911833690442, 0.0017184282662537024, 0.0016344607551106994, 0.001622438673120068, 0.001640808143729002, 0.0016295999806488352, 0.0016240200422210048, 0.0016223618984032347, 0.001643010122910598, 0.0016219676329697274, 0.0016460085111879269, 0.0017111350197763163, 0.028425629447898542, 0.0217424194917691, 0.0018272905928862033, 0.0016986314083772655, 0.0016823955312637346, 0.0016866148763089155, 0.0017614883670051182, 0.0017548517148219505, 0.001773077144040441, 0.0017546197133404867, 0.0017675620418193998, 0.0017788217555047298, 0.010278497019554583, 0.012581695980696502, 0.022047863631718317, 0.001740220979293238, 0.0017268690209341596, 0.0017053303261268505, 0.0017164665516655967, 0.0017111123069094457, 0.0017069790830683646, 0.0016491970627055485, 0.001595883960455504, 0.0015956218572979678, 0.0015766705286556057, 0.04108923046887681, 0.001629372956813313, 0.001625659249839373, 0.0016322874774535496, 0.0016394436873573188, 0.0016419357489212416, 0.001630314354163905, 0.001566027521524423, 0.0015488199351239018, 0.019647938689255778, 0.001767788043556114, 0.0017052434147141564, 0.0016119671660514239, 0.0015930875185100983, 0.0016192731248641696, 0.0015846830428927206, 0.0015917693114412639, 0.001584827730160517, 0.0015701790228680086, 0.0015875839793200914, 0.0015817040208882343, 0.0015668024595167178, 0.0015631879165691014, 0.0015736233957189445, 0.0015676627929982108, 0.0015682853966912564, 0.001559980019616584, 0.0016035750207568829, 0.0015588826451372977, 0.00156639739482974, 0.0016537976868373032, 0.0160420572089303, 0.0016747921884719592, 0.0016399262506941643, 0.0016225514797649037, 0.0016230162291321903, 0.001619575748918578, 0.0016278129599716824, 0.0016246453136166867, 0.0016198244775296189, 0.0016221160428055252, 0.0016228176852261338, 0.0016200692698475905, 0.0016238824149089244, 0.0021079305200449503, 0.0016333131886009749, 0.0016513060400029644, 0.0016533766441474047, 0.027895456686868176, 0.005372648893777902, 0.0016534648748347536, 0.001643122542494287, 0.0016243108936275046, 0.0016179988330501753, 0.0016143574175657704, 0.0016093527083285153, 0.0016119087910434853, 0.001601246355373102, 0.0016011468954578352, 0.001602912418699513, 0.0017044412500884694, 0.0017342465216643177, 0.0017333511665735084, 0.001914613603730686, 0.02189513091677024, 0.0018171913737508778, 0.0017231709158901747, 0.0017133070432464592, 0.0016418009375532467, 0.0016452950003440492, 0.0016452828131150454, 0.0016920594571274705, 0.0016274918137545076, 0.0016688379594900955, 0.0016545282293615553, 0.0016399933956563473, 0.013521178082252542, 0.0068339370215350454, 0.011841001956781838, 0.002747385522525292, 0.0017126151869888417, 0.0016573052926105447, 0.0015521078967140056, 0.0015634564576127257, 0.0016264566244596306, 0.0016096974359243177, 0.00160753487580223, 0.0015922370002954267, 0.0017024149177207921, 0.002417651313104822, 0.0017144336889032274, 0.0017089743341784924, 0.0016061641266181443, 0.0016071506058021139, 0.0016107210006642465, 0.0016148784173613724, 0.0016140702718985267, 0.0016057739145859766, 0.001604281833958036, 0.0016224719171683926, 0.0016208265636426706, 0.0016167148739138308, 0.0035084326882497407, 0.0016057207709915626, 0.001707005833547252, 0.0016175655012678665, 0.0016302026027309087, 0.0016278571662648271, 0.03500045231097223, 0.0018125582719221711, 0.0017302943548808496, 0.00160423531391037, 0.001594886770665956, 0.00159531093716699, 0.0015935562284236464, 0.0015804537082052168, 0.0015832758969433296, 0.0016721805198661361, 0.0017190294577934158, 0.0017265959589470488, 0.0017676985614040557, 0.014391382270938871, 0.011985136542837912, 0.0016177949801203795, 0.0015885067502191912, 0.0015772102706250735, 0.0015825740847503766, 0.0016139776441074598, 0.0015826513141898129, 0.0015778019369463436, 0.0015965637915845339, 0.0015848335412253316, 0.00159029581360907, 0.0015972591669803176, 0.01915987791774872, 0.005415641937967545, 0.0038447323531727307, 0.0017344980830481898, 0.0016493626268735777, 0.0015911703109547186, 0.0016020444794169937, 0.0016210062725197834, 0.0016215823755677168, 0.001627141667995602, 0.001630499456950929, 0.0016519637501914985, 0.00169982587491783, 0.017498626085095264, 0.022220286625573255, 0.0015667361052085955, 0.0015554302711583052, 0.0015522068545881969, 0.0035123128133515515, 0.0015468179359838057, 0.0015456410837941803, 0.0015610947715079722, 0.0015590901675750501, 0.003550700709941642, 0.05216368972954418, 0.009025807312961357, 0.0017522163325338624, 0.0016024609382535953, 0.0016312811058014631, 0.0015979762708108562, 0.0015838792290499744, 0.0015726107521913946, 0.0015628638332903695, 0.0015914000832708552, 0.0015739705001275677, 0.001583470730111003, 0.001580892125881898, 0.001587562459462788, 0.0015980982934706844, 0.0015923239358623202, 0.02517291749973083, 0.002781809812101225, 0.0017308308330636162, 0.0017291945211278896, 0.0017278278949864518, 0.0017255621448081608, 0.0016082928341347724, 0.0015929338963663515, 0.0015998144178108002, 0.0017131491040345281, 0.001767849311969864, 0.0017062463133091417, 0.0017052824575027141, 0.02315386862513454, 0.0021476847929685996, 0.001755370270984713, 0.0017465358735838283, 0.0016178087898879312, 0.0016240093536907807, 0.0016040247913527612, 0.0016088093761936761, 0.0016260768315987661, 0.0016638133747619577, 0.001722876603404681, 0.0017188097917824052, 0.001719486999112026, 0.0017164115197374485, 0.0017157511053180012, 0.001723703979223501, 0.0017496473956271075, 0.0017165111033439946, 0.03281373191566672, 0.0017364363520755433, 0.0016212168945154797, 0.00160363483397911, 0.00161655089565708, 0.0016060821872088127, 0.0016693936874313902, 0.0016058189163838203, 0.0016358546466411401, 0.0016029443553028007, 0.007272574958430293, 0.0033227932726731524, 0.0016677207895554602, 0.0016257534589385614, 0.00162227735563647, 0.001761307811345129, 0.0017565670835513931, 0.0017514816451390895, 0.0015764111255217965, 0.001600234269668969, 0.0018732207706004071, 0.0016393498954130337, 0.0016288290838322912, 0.0016719095826071377, 0.0016008298747086276, 0.0016265569380872573, 0.0016225644164175417, 0.0016500924781818564, 0.0016268109175143763, 0.0016466818124172278, 0.0016254259389825165, 0.0016309279599227011, 0.0016094862706571196, 0.0017532450826062511, 0.001725332125109465, 0.0016379721879881497, 0.0015847265215901036, 0.0016397874787799083, 0.0016066303748326998, 0.0016138639378671844, 0.0015930379789400224, 0.0015892867304501124, 0.001590842332613344, 0.0015974709579798703, 0.0016017333958491993, 0.0016059853757421176, 0.0016150790009608802, 0.0016976086044451222, 0.020448669500183314, 0.01829071347795737, 0.00864889656319671, 0.0026287905202480033, 0.0100968576662126, 0.005555411875927045, 0.001689825522286507, 0.0016477907920489088, 0.0016374599387442383, 0.001640618771489244, 0.0016425066666367154, 0.0016495213130838238, 0.0016414793353760615, 0.001646102352727515, 0.0016299820417771116, 0.013938244771755611, 0.0016821028548292816, 0.0016716981675320615, 0.0015831263105307396, 0.0015563007934057775, 0.0015805026875265564, 0.0015901814573832478, 0.0015968014995451085, 0.001595224646735005, 0.0015682321027270518, 0.0015678184984911543, 0.0015505237703716073, 0.0015607461027684622, 0.0015667246019196075, 0.0015613730429322459, 0.0016087833961743552, 0.0015890203746190916, 0.0015839886230727036, 0.0015834371661185287, 0.0015856853327325855, 0.0015784669788748336, 0.0019615726050687954, 0.010101173415023368, 0.0016907370615323696, 0.001667963144427631, 0.0016523426869146836, 0.001594611290784087, 0.0015712368331151083, 0.0015945617924444377, 0.001602175541241498, 0.009476052813018518, 0.014618471917250039, 0.005644173164910171, 0.0015855546056021315, 0.0015709070842907142, 0.0015759342277306132, 0.0015625117084709927, 0.0015592128547723405, 0.0015898239386539597, 0.0015672651255348076]
[692.1777191625398, 698.1606854887096, 697.6446412167999, 702.9594695561829, 696.1113030696026, 693.7647009552129, 698.9852633351419, 697.2643136452781, 701.2957338476601, 699.0298668501965, 695.0978035505115, 698.6340808940903, 697.7948880474563, 700.4786571950722, 701.6420222116175, 702.7599889617205, 700.9660610386029, 693.3004396460242, 707.1533919588696, 704.086903068233, 702.9761198691854, 700.144529701324, 700.5283587983819, 701.2607458485551, 697.4366108962837, 695.6457938808146, 696.2123460946473, 693.1517599840329, 692.022189875032, 649.2088575491333, 651.2302680035256, 651.0786914445922, 648.2995375465565, 648.7668193963029, 643.3324305192131, 649.2981365638686, 695.5978793382579, 693.1048537767583, 694.4133557504185, 691.8352378161838, 693.7581292882799, 647.9644639794888, 649.0695185178727, 643.7903593205189, 570.8850617113602, 666.2016493962204, 643.2857085968225, 19.8922003500373, 605.5885398185727, 645.1587847290656, 640.1802165489229, 641.2088094038285, 646.9190879113144, 35.60350892784679, 80.60244438165752, 632.88704897271, 651.9496618725178, 645.94709182356, 643.3692168002417, 645.9819886891501, 647.8690155168312, 68.76715203134273, 166.62908388677613, 581.267791520957, 591.9166176345238, 605.1907629095638, 603.418001152752, 662.046536756935, 666.3688225142217, 661.5197556793788, 664.2911075434936, 654.9188216843485, 662.3096422694265, 663.2236335631758, 689.3226472873778, 641.1470678427934, 635.3702642679943, 624.0717407175931, 630.199726350705, 620.5573157365483, 36.816683316392286, 24.964139509964177, 64.14937341564112, 209.79479395221566, 699.8594583980008, 699.0653292145691, 691.8013431797378, 699.6847315515685, 692.1696727512606, 694.9830366081069, 37.66055897408494, 605.2260003118165, 623.1471867452133, 686.2949545601402, 690.2069691783223, 685.9741030426314, 680.7053295046668, 688.0552329456674, 694.0068736932201, 687.2262406388182, 686.7887266519263, 692.2635595708693, 51.200945769672245, 540.1483503029489, 602.5172601667194, 592.9961414016019, 605.3533871155558, 599.2101188654808, 45.34640886607748, 160.42535691835033, 187.03129006074795, 219.09207215002613, 644.6283749738341, 641.0499617334076, 663.7705506297693, 661.9407434614088, 661.0769863521218, 669.253299257776, 664.5035580662505, 660.261092870048, 658.4730841535463, 664.3520907337329, 663.9061084658408, 666.3702361877371, 669.2345157441492, 668.5700262775706, 657.8692802145523, 652.9915568251578, 44.0872667218298, 120.7132796255555, 43.39895122752378, 602.7833508077534, 629.8830883251386, 684.2012177970611, 686.5278993779365, 688.3500885294034, 694.3096261880825, 682.2094402884125, 691.4707968019895, 690.9797910766284, 638.8272361369393, 641.6402448484647, 634.194034012297, 26.04059108039379, 590.1813546536582, 630.6886270629512, 642.3173424254619, 646.2888371026095, 684.1735029686271, 687.7132852122949, 667.2775838734526, 684.2980193290016, 19.24766431448196, 41.35863197133096, 643.3878106703768, 644.5766050809774, 647.6013960177465, 652.9498685889407, 642.1961443382611, 641.7762525377236, 640.8685850720242, 642.0874039045866, 79.04843124971953, 65.14011532980649, 189.23766399770787, 622.3494821959208, 627.367543904809, 630.7464056157385, 625.8970283932733, 631.4493203206646, 671.1372450176665, 292.2070845021393, 77.77156099172458, 41.106779087361595, 626.4979855013828, 649.0068633858028, 642.1637675838467, 657.1931603163154, 657.3415036975916, 662.2975658889802, 651.5688758322174, 655.6732741690781, 691.1233596481023, 696.3452118423469, 694.4496322138464, 692.4320053778314, 643.779001030923, 29.5289262510162, 486.3703929348738, 606.5446870492278, 609.9032676836123, 618.4862357603374, 609.776145385694, 616.2413789025227, 598.08054711349, 603.0319095422956, 651.6216701321393, 24.550986777210063, 240.87130090596918, 590.871524303504, 617.6627728434108, 617.5786655167436, 622.5085124010035, 660.4685334395673, 652.7620883438011, 654.5917821796577, 658.603399175492, 364.23281354780823, 640.859222597175, 662.960196232042, 675.8147802031405, 677.2192253439281, 667.3763558285337, 669.6806577716479, 678.6908146668235, 569.096827018266, 66.49487891314362, 89.48767564533189, 643.2553659407708, 640.9695103035906, 642.599639664825, 649.4727285580731, 651.8180902362551, 28.587424644279555, 33.70374633216444, 617.8636506004668, 660.132664868914, 651.1298843348854, 666.3553654866533, 641.983097744084, 637.4204302825261, 637.189358467628, 636.1989605293497, 28.651833518318515, 427.68481668931105, 642.4771317292077, 662.980540208174, 662.5924792666959, 668.2009380532453, 654.7085533031324, 657.731531458892, 664.4561619061027, 42.28638989061434, 50.427706501542524, 608.2970930420922, 677.229698874362, 674.7064964846363, 676.9458916620065, 654.3471087131588, 668.3062812690026, 680.3475771737326, 673.719352947778, 60.29757174191087, 630.4142606420136, 628.7847705761707, 624.7740926798291, 641.6378073539728, 637.4620995719777, 640.7085980338823, 633.6465273175045, 633.3527433599857, 634.8255531660133, 507.89630495969834, 680.733689958495, 687.0141280713739, 654.4965045397238, 649.9558940595975, 646.6240659071624, 646.1562091912753, 644.7866948273412, 649.3622755356586, 26.74347725081164, 37.566497434648376, 464.0837446094729, 646.2598129939771, 649.5945616259494, 650.1874229728992, 656.5246998955733, 656.826326549914, 655.6962091263824, 655.0330040544347, 639.5217194524802, 636.9218854959805, 650.0060221614717, 654.9214131000463, 582.785424544484, 144.26017697569938, 606.1885834797014, 649.6139812772543, 655.6688526278002, 656.4387165172435, 654.9841116366713, 631.8299286838848, 647.5551299336931, 650.1752844547025, 36.22533569225331, 42.888464189425434, 638.8803189003492, 675.3897896659623, 668.5332664541904, 664.9476911647936, 677.5286960891945, 656.5161492870787, 676.0289696013938, 57.82608866590804, 323.505308001161, 542.2532890983065, 680.1938150491698, 682.0632276405568, 682.8996751491063, 671.5055730138159, 665.0735313240073, 686.5710716264139, 680.8880179155844, 291.0721015980886, 212.6056604133502, 643.6127048558719, 650.0422735449743, 631.205490358368, 685.5397244857883, 660.912269671494, 672.37278840807, 680.8571092804735, 31.31025696889546, 51.32905569848279, 73.1663529405, 494.40433713190777, 640.6330558589719, 642.81722974111, 684.8512277031643, 679.9042830533883, 684.0597755749061, 679.8116806490614, 683.2624142389445, 44.71208514592212, 602.6914830765811, 680.9641334296308, 691.1316166633461, 686.1956351462837, 683.7778585113957, 683.4057083712086, 674.7153414603795, 681.1221734153139, 680.8421521519764, 648.9790645709421, 653.0496480279805, 638.2981942019733, 651.3654801949433, 651.2066912853376, 653.255648198648, 650.5223057383315, 625.1954372728848, 652.2748736474549, 653.1633538419436, 34.77879000176891, 65.97468716763383, 613.9697668633975, 631.6629081174345, 685.2326592977739, 675.3353255441253, 689.8137474572789, 688.4678968071556, 621.9852121760496, 81.35647124612613, 655.889598032075, 681.4893474442819, 677.1906884697697, 675.0787547617805, 677.9227017987716, 688.6375197413189, 684.1360770558127, 356.245142769631, 611.3347351336399, 634.3036978509884, 683.4996632840118, 684.8428336989151, 682.8174072326739, 687.4726466097704, 683.0722118814574, 674.636407873811, 695.6748300082197, 647.531972971901, 676.0574357945736, 654.7555060498487, 674.6534898938643, 64.04587888941121, 225.97508535892533, 638.0077403049987, 643.6921290344811, 687.6856626479204, 687.4866343387762, 678.78054498902, 661.1209950729108, 668.5296540854853, 642.6164103208795, 25.65734850712257, 121.32067387690722, 660.3321669572586, 664.819093850851, 636.1282197002746, 664.8947993514005, 663.5537418145212, 665.5097119810906, 31.87061782249979, 33.678724414444964, 627.7236244641658, 673.2730234426379, 687.9650152374762, 684.28145887902, 686.6916706955109, 653.5264026824601, 629.0271133358781, 646.0384903273076, 633.4406532749642, 641.2632285665435, 19.073051733301, 669.2353095422744, 687.0665220469126, 686.9082542823195, 577.0573064782849, 572.4668161148611, 684.5049779483427, 687.6562559994854, 680.8221541730248, 22.50831092771164, 330.01749279151386, 627.824023298867, 636.2532674067078, 640.552475075123, 676.9113380821925, 679.0737405725246, 682.7411326967804, 639.323562371917, 642.3069868113507, 637.5192516580856, 639.7038540382965, 679.3855877461673, 681.6491293485137, 681.857894011128, 679.4191704704459, 678.7245640473667, 678.8577141105003, 681.0680978058775, 675.4882125510659, 678.9094353095555, 676.1481492276545, 674.8442082729863, 677.4392309930569, 674.9710796768286, 680.2169585373036, 674.9270952330442, 675.4341238342915, 675.8313630956686, 678.1576375744862, 684.868744010719, 680.8578471868948, 677.6045030514634, 678.0015981281878, 673.2904619068316, 677.704471659408, 649.9162382945243, 664.7966796688573, 667.9887207616582, 668.3933311041847, 673.2427829018953, 666.7521380973363, 664.5838255422896, 671.0111859234258, 667.7576461787486, 670.7644261429589, 653.922438857596, 671.7543794358853, 670.8582720612189, 668.3791267161562, 664.6305283685556, 661.2837265112227, 663.9125498222913, 667.1565594351728, 38.08914649832405, 67.27411947431392, 633.7252653418601, 675.3504881407954, 683.4622167214803, 683.5191906192944, 673.2765192481331, 674.4235645243543, 693.8252720402279, 675.2771119628164, 617.7742322554874, 36.66481621115788, 352.22335226640666, 642.1687172094364, 645.7025924107951, 641.4202768825672, 645.5425047379471, 645.3904873376063, 86.07155493609139, 218.78632051679656, 605.9191044316445, 682.9701016401959, 690.4159619270794, 689.032957672485, 656.097963717882, 669.7369864670092, 671.9082824019433, 676.4793173026299, 675.774665382708, 661.1507525372366, 637.7804531314083, 635.8893788375981, 636.9656343671471, 640.5682012233882, 638.2390393763793, 612.8904801715825, 629.6712618073873, 635.2753439523888, 50.82545053933731, 626.560823383556, 674.2001240947089, 676.810725052328, 685.2969535281164, 635.3794996599906, 630.4790472898492, 649.8343558332693, 642.4519958017622, 649.3023774446739, 31.602596437047104, 175.40611709760472, 623.7141465907861, 631.0344761791891, 628.2377313081108, 637.8080894747416, 629.8798655438154, 610.1790009528714, 626.2195387839753, 38.70858552506188, 589.8721925087525, 633.4680044020695, 635.8874555688934, 54.1657057947623, 219.49046182701923, 509.6572347890156, 549.8493599167025, 556.272472612924, 562.9603478358802, 580.9714282684757, 42.73607427123639, 57.226993567537555, 558.5145742138071, 575.4002012050017, 574.8224168252015, 573.125684877541, 570.477481385047, 576.8236990795002, 45.510740709308706, 90.01452835584452, 137.67221574406784, 258.0697922992009, 538.4419179402385, 570.4653331468912, 490.15303059328966, 566.5694208935721, 568.1765016330099, 51.04573378752123, 73.77775651152356, 311.36583805915586, 545.6497723580455, 541.5060762337032, 535.6010817653977, 555.6917565986248, 573.7404948833043, 41.02285224472607, 185.75301866653584, 510.6519550322659, 543.2957908122391, 530.3286051582438, 531.9214010039092, 36.692356142951276, 131.0455854875688, 156.01396568067008, 518.1852133529892, 529.7596022182561, 550.9610341789227, 555.8027620973239, 555.2629218819143, 514.1689701392451, 528.2717224216359, 35.462106428928145, 420.3737808114987, 540.6103336750965, 536.3073569458069, 542.4803756613248, 525.3053806890349, 25.55555297462197, 49.83742701234974, 541.7317173008182, 533.9002237366063, 546.2214364045083, 542.8972718415632, 46.416016465967424, 476.6753136988781, 531.6287541212878, 540.3395271731123, 528.5756181703512, 526.3403481485241, 46.114733563161856, 237.28889277988534, 564.0225957338395, 567.5009024123149, 573.2684122962344, 547.3120068909279, 567.3766224310021, 567.675392886598, 554.422271278105, 43.402027885221365, 30.5939935421197, 127.79168365976649, 508.2349510875709, 570.5799749030742, 569.0939392472566, 573.4278994740963, 574.5821523978013, 575.2873705826426, 575.6223717377233, 555.867969275802, 570.4583469071947, 576.0239056270916, 170.53679445798676, 515.7529527755314, 524.5285387145984, 525.3521998187832, 529.1304735496843, 526.5306716415394, 522.5010105656887, 524.8583517066745, 35.48415552619558, 24.712116426692504, 553.7004456516787, 554.6416356663586, 542.4079975622473, 565.9631032920365, 545.4147067673827, 570.6313646678366, 561.4095805419141, 572.3558815899243, 570.6751005218674, 542.6918392036873, 116.68644143476106, 560.5118654584433, 568.4323005412441, 573.9827781136457, 566.693754129778, 557.0319553618816, 547.1014608899169, 36.44287228262052, 347.9703411602317, 590.0908067067918, 596.197247642402, 593.1640172566482, 582.259252057361, 593.113744395923, 587.2099189658232, 587.7091287011011, 27.803698343169774, 464.1273114118035, 582.1063705864134, 581.2398740997035, 537.4412797843131, 562.2969929824184, 565.7956621774645, 538.9017439756815, 48.21635159860322, 139.1187064400321, 157.36086486043317, 176.04449525268382, 574.0208025080854, 585.4176576603712, 281.9584123455046, 592.0384553165155, 579.0394455924246, 574.5661047336615, 589.3838071442187, 45.385248116308034, 531.4945389931391, 583.1805335745836, 588.3182306714793, 568.7909339523927, 582.5459043820548, 581.9796136731255, 589.0077998822877, 53.869876004369836, 367.6182487765984, 567.5692401591309, 584.6606385761818, 580.161995523915, 594.6004161231614, 579.5772517868885, 588.0251652043233, 590.1871060602349, 594.7539832206601, 130.8586677439522, 27.730747671561996, 77.48749191803226, 545.4146163029948, 568.6656788679509, 567.6021185735424, 571.3553312018861, 567.3545690553989, 561.9438960613414, 570.3287443870544, 73.28899602925202, 76.6514126470858, 303.62302508331067, 565.5342280847772, 598.7166956028456, 606.1762932011928, 610.9971386329299, 600.4880917936275, 603.3472753729168, 596.6275735082668, 599.5871044152433, 599.594668592845, 601.106389796154, 27.84660304094093, 59.58462470577744, 533.6161860943081, 555.0899284881501, 595.2569967059386, 587.449367410567, 597.3022053304397, 580.1895479778024, 599.4734938898536, 591.8365518548867, 597.6513880987641, 592.5331587485817, 585.4463353389705, 73.58648987982907, 57.74242240147228, 111.3958512917648, 450.1010369170247, 591.3336051813047, 592.8286920274537, 596.7271875378801, 585.9392984148216, 580.2180804369132, 592.3578700458631, 571.4068746970431, 589.766684202823, 584.9791650198903, 545.9212034595453, 100.19967730916109, 579.8946136850674, 601.3471158152774, 602.5446592055912, 609.4190513959934, 604.4502641069689, 602.8990148234805, 603.7724089606774, 607.2056226708021, 611.7041041986894, 600.8269163520845, 606.040704572902, 600.535258762014, 531.5768419505154, 40.2763255434648, 560.3831232801041, 591.389894165539, 594.0498656658152, 594.679586404524, 595.3234234615392, 596.0788552209, 593.8075179696635, 602.036507739776, 596.620643853164, 555.5956123349412, 568.4908363944091, 559.4650433607651, 29.130414852890418, 154.67597515103287, 569.4500862321432, 572.3423574111755, 610.5238797454587, 613.0311300047165, 577.0869171084007, 616.5807010077745, 614.210437470341, 612.5930301271212, 604.391021593951, 609.4546540797021, 605.7495349810442, 603.7408305497969, 31.456652571404373, 569.6635707451467, 576.3618047465371, 604.7573966260645, 605.1935587435275, 610.6568777400854, 606.2059153828063, 610.5209582877, 613.6150684715208, 557.3496721413358, 611.0376113288127, 592.5824021740139, 576.4382535412441, 581.9271130706387, 611.8225823857555, 616.3561166086648, 609.4557756931522, 613.6475281509539, 615.7559475881856, 616.3852843093903, 608.6389767510967, 616.5351143099316, 607.5302729013828, 584.4074187265026, 35.179520011435606, 45.99304140822801, 547.2583309371178, 588.7092367821685, 594.3905469416268, 592.9035810406565, 567.7017337901581, 569.8487180163033, 563.9912529249724, 569.9240652529637, 565.7510041178938, 562.1698727854023, 97.29048888154806, 79.48054074222208, 45.355868337347125, 574.6396646741573, 579.0827143676735, 586.3966556386773, 582.5921856907938, 584.4151760010229, 585.8302599715862, 606.3556761127596, 626.6119747920618, 626.7149045535161, 634.2479178910506, 24.337277398209093, 613.733028904429, 615.1350598834333, 612.6371817543134, 609.9630061779906, 609.0372297801567, 613.378639184492, 638.5583818007023, 645.6528466106051, 50.895924290869104, 565.6786760410383, 586.4265426104146, 620.3600303159641, 627.7119043247725, 617.5610430660878, 631.0410176249344, 628.2317373580676, 630.983406567928, 636.8700545836176, 629.88793854437, 632.2295364959824, 638.242551845656, 639.7183533729002, 635.4760629007603, 637.8922842759216, 637.6390433206763, 641.0338513475209, 623.6066208664206, 641.4851067329225, 638.4075990554716, 604.6688829952255, 62.33614473356445, 597.0890041661684, 609.7835189702647, 616.3132649232757, 616.1367841248817, 617.445649373127, 614.3211932760358, 615.518963812391, 617.3508388545232, 616.4787065852905, 616.2121654846605, 617.2575571994374, 615.8081341474992, 474.39893795867414, 612.2524491806478, 605.58126463233, 604.8228657032162, 35.848131515651126, 186.1279267956829, 604.7905916960827, 608.5973347319449, 615.6456894571103, 618.0474173240577, 619.441512219681, 621.3678299510906, 620.3824965509618, 624.5135213856539, 624.5523148668117, 623.8644035282524, 586.7025337177769, 576.6192911491743, 576.9171413642637, 522.298597508901, 45.67225488631655, 550.2997727398919, 580.3254864497344, 583.6665435666163, 609.0872389744679, 607.793738989597, 607.7982411465667, 590.9957807851816, 614.4424147320724, 599.2193515933354, 604.4018967182429, 609.7585530823352, 73.95805261322366, 146.32853607646783, 84.45231270545125, 363.982408657682, 583.9023311233286, 603.3891308129631, 644.2851055117477, 639.6084746273784, 614.8334883091255, 621.2347598266389, 622.0704850966025, 628.0472064237033, 587.4008677853978, 413.62457629002313, 583.2829852052947, 585.146295061653, 622.6013789173264, 622.2192222619419, 620.8399838256341, 619.2416650375129, 619.5517118494256, 622.7526745306697, 623.3318727625494, 616.3434876242683, 616.969157855227, 618.538256890744, 285.0275575612859, 622.7732854090698, 585.821079428853, 618.2129868720546, 613.4206866832405, 614.3045106927493, 28.57105934275346, 551.7064005558982, 577.9363477544614, 623.349948308064, 627.0037587573963, 626.8370489428447, 627.5272765173809, 632.7296995845659, 631.6018591141309, 598.0215581509426, 581.7235972696024, 579.1742965794049, 565.7073110959119, 69.4860285949976, 83.43667979298749, 618.1252954101702, 629.5220337351506, 634.0308699636379, 631.8819508267965, 619.5872685417564, 631.8511165625372, 633.7931121667821, 626.3451578139167, 630.9810929587211, 628.8138291269013, 626.0724750702418, 52.192399361462094, 184.650316888434, 260.0961284534631, 576.5356616841051, 606.2948097081196, 628.4682369418957, 624.2023944078718, 616.9007590856164, 616.6815914300373, 614.5746370270588, 613.3090052480132, 605.3401594823605, 588.2955511830529, 57.147343747848076, 45.003919924646844, 638.2695826537167, 642.9089227222736, 644.2440303907185, 284.7126816833182, 646.4884953405851, 646.9807321278226, 640.5760997034338, 641.399721964357, 281.63455094936336, 19.170423050684345, 110.79341330098715, 570.7057863990516, 624.0401722926406, 613.0151305275439, 625.7915203537866, 631.3612690026937, 635.8852618847509, 639.85100857741, 628.3774963393669, 635.335922699283, 631.5241456530737, 632.5542291142425, 629.8964768531929, 625.7437380952588, 628.0129171445582, 39.72523248489941, 359.4782057529143, 577.7572139906785, 578.3039373428833, 578.7613470656701, 579.5212899221169, 621.7773149116708, 627.7724407027212, 625.0725014520175, 583.7203531467073, 565.6590712959176, 586.0818524264366, 586.4131162554978, 43.18932685462576, 465.6176750303137, 569.6803782822574, 572.5619582883438, 618.1200190346796, 615.7600002286716, 623.4317607751223, 621.577680238243, 614.9770912219415, 601.0289466167263, 580.4246212548475, 581.7979422627097, 581.5688054148812, 582.6108648775355, 582.8351192084226, 580.1460181408196, 571.5437307535789, 582.577064635274, 30.475046318110373, 575.892113065193, 616.8206138135898, 623.5833612560618, 618.600999626139, 622.633142913991, 599.0198762154471, 622.735222382311, 611.3012559234863, 623.8519738329264, 137.50287975248855, 300.951614481725, 599.6207556221418, 615.0994140605368, 616.4174063858943, 567.7599301829529, 569.2922344748823, 570.945178201161, 634.3522852701241, 624.908501807591, 533.8399059495154, 609.9979039240127, 613.9379569814722, 598.1184690864818, 624.6759982424824, 614.7955700683591, 616.3083510779184, 606.0266398534484, 614.699587538982, 607.2818637208736, 615.2233553169328, 613.1478670875173, 621.3162660851533, 570.3709138675981, 579.5985511697083, 610.5109765192392, 631.0237043276129, 609.8351237222855, 622.4206984161688, 619.6309221219469, 627.7314246239008, 629.2130808370767, 628.5978060171794, 625.9894710477741, 624.323625012404, 622.6706762743123, 619.16475875487, 589.0639322759904, 48.90293718087798, 54.67255288893605, 115.62168569054906, 380.4030759764223, 99.0407147509198, 180.00465534036172, 591.7770721363573, 606.873156971931, 610.7019636565253, 609.5261235443911, 608.8255349658054, 606.2364833167711, 609.2065726620316, 607.4956386175177, 613.5036916785509, 71.74504511689987, 594.4939675531857, 598.1941114862298, 631.6615379001264, 642.5493093861503, 632.7100914741073, 628.8590496115886, 626.2519169006772, 626.8709564177876, 637.6607125061821, 637.8289329806896, 644.9433534065295, 640.7192036079367, 638.2742689907109, 640.4619347865826, 621.5877180097543, 629.3185512109703, 631.3176656914037, 631.537532020481, 630.6421452967068, 633.526081560998, 509.795047818242, 98.99839938523485, 591.4580231024614, 599.5336307884395, 605.2013350010569, 627.1120778959803, 636.441291932685, 627.1315446904168, 624.1513331461279, 105.52917124165525, 68.40660266412549, 177.17386954337275, 630.6941410070448, 636.5748872101582, 634.5442483599245, 639.995204246218, 641.3492532076444, 629.0004670873557, 638.054138835485]
Elapsed: 0.19854415046052767~0.36372759649639247
Time per graph: 0.004069352466160649~0.007443254446219376
Speed: 543.9303715095142~194.60240053810583
Total Time: 0.0781
best val loss: 0.3468859791755676 test_score: 0.9167

Testing...
Test loss: 0.6258 score: 0.9167 time: 0.07s
test Score 0.9167
Epoch Time List: [0.37639823206700385, 0.3745251349173486, 0.3741839430294931, 0.37733101611956954, 0.37574769312050194, 0.3773876429768279, 0.37769253517035395, 0.3751075549516827, 0.3729923429200426, 0.3717101371148601, 0.37525764910969883, 0.3748965618433431, 0.3746307980036363, 0.37378589215222746, 0.3735638309735805, 0.3732948479009792, 0.37375827704090625, 0.37443498300854117, 0.37179624394048005, 0.3725330561865121, 0.3726577899651602, 0.37397791212424636, 0.3737894961377606, 0.3725962790194899, 0.3740334380418062, 0.37539219309110194, 0.37491566594690084, 0.3769662278937176, 0.37642459594644606, 0.38577186898328364, 0.4000510841142386, 0.4008177120704204, 0.4010986869689077, 0.40256917593069375, 0.40222682198509574, 0.4012949229218066, 0.3787009621737525, 0.375361421960406, 0.3745913509046659, 0.3752423881087452, 0.37702242203522474, 0.37942425697110593, 0.40326296410057694, 0.4012536540394649, 0.41923354798927903, 0.48059733083937317, 0.3954363961238414, 6.859159874962643, 4.901147312833928, 0.4090990530094132, 0.40802656509913504, 0.40907405607867986, 0.4032149401027709, 1.9206464091548696, 8.815584134077653, 0.7271287410985678, 0.4073424080852419, 0.40294874901883304, 0.4066309130284935, 0.4058587581384927, 0.40503752103541046, 1.3743229298852384, 2.426551861106418, 1.5459655129816383, 0.43660920509137213, 0.43314315495081246, 0.43133221904281527, 0.40538669598754495, 0.3915080671431497, 0.3919425060739741, 0.39275030710268766, 0.39453249296639115, 0.39828140288591385, 0.39451108407229185, 0.3903914220863953, 0.38814537203870714, 0.4077402849216014, 0.45217874995432794, 0.41081010713241994, 0.42279465589672327, 2.2119055420625955, 8.763317777076736, 2.0299253369448707, 1.744243114022538, 0.8231765491655096, 0.3737597268773243, 0.3719633030705154, 0.3714803089387715, 0.3734264369122684, 0.37394541408866644, 7.478898775880225, 2.992386863916181, 0.4193758680485189, 0.3996129990555346, 0.3806088379351422, 0.3859677469590679, 0.3855156400240958, 0.38482132798526436, 0.37856397905852646, 0.38116576604079455, 0.38041424413677305, 0.38192438799887896, 3.222672971896827, 5.127945251995698, 0.43992241797968745, 0.4365421999245882, 0.43066419812384993, 0.4337546690367162, 1.4352472589816898, 3.787375396117568, 1.658802876016125, 1.167944026994519, 0.585864320048131, 0.41477528598625213, 0.3975003978703171, 0.39871511003002524, 0.3967299780342728, 0.3947185630677268, 0.39307200198527426, 0.39582336286548525, 0.39734051807317883, 0.3940711391624063, 0.3931426372146234, 0.3923234761459753, 0.3942095370730385, 0.3987908430863172, 0.39416258793789893, 0.39568958210293204, 2.7693105079233646, 4.240803399006836, 2.7761927401879802, 0.8654545539757237, 0.4133449780056253, 0.39248825795948505, 0.38537190400529653, 0.38241000194102526, 0.38143701292574406, 0.3817182950442657, 0.38330603390932083, 0.37729308602865785, 0.38754935492761433, 0.40364538109861314, 0.4087244149995968, 4.175897175911814, 1.1167938068974763, 0.41575713094789535, 0.40978615113999695, 0.40720315591897815, 0.395340007962659, 0.3838034140644595, 0.38412120600696653, 0.38527808093931526, 3.186578231980093, 5.31846538791433, 0.9707700938452035, 0.4041344498982653, 0.40334042196627706, 0.3987495880573988, 0.399191188858822, 0.40342729981057346, 0.402631540899165, 0.40772534091956913, 4.34271121409256, 4.963843410951085, 2.8614054111531004, 0.4376893399748951, 0.4209981750464067, 0.4184986539185047, 0.4167013569967821, 0.4160413509234786, 0.4066001200117171, 0.48537492798641324, 4.355209060013294, 4.734165631933138, 0.6668816381134093, 0.40303126780781895, 0.4041271419264376, 0.39972874394152313, 0.39543600706383586, 0.4001601419877261, 0.39912928990088403, 0.40115831093862653, 0.3879940329352394, 0.3791080969385803, 0.3815666871378198, 0.3790833189850673, 0.3873963551595807, 3.828188077895902, 5.616021211026236, 0.43325509887654334, 0.4301818989915773, 0.4227458060486242, 0.42361678380984813, 0.42440694593824446, 0.42533858900424093, 0.4280497229192406, 0.4111376829678193, 3.1295161090092734, 2.775326481787488, 1.6841413890942931, 0.42599249992053956, 0.4198140511289239, 0.4190155630931258, 0.4306324570206925, 0.39183275308459997, 0.3926434420282021, 0.39410938310902566, 0.4770562439225614, 5.2581886989064515, 0.38971547107212245, 0.3865820049541071, 0.38490910711698234, 0.3952248969580978, 0.38690326595678926, 0.3843662799336016, 0.3957766619278118, 3.55523403105326, 3.609549634042196, 1.2965773639734834, 0.4063441299367696, 0.40658999886363745, 0.3974194040056318, 0.3987978649092838, 3.6001200528116897, 7.651727668941021, 2.8028952131280676, 0.3959505290258676, 0.3920335801085457, 0.39313942298758775, 0.39552830799948424, 0.40977179491892457, 0.406336713116616, 0.40775025403127074, 3.54170104698278, 2.053591786068864, 0.6413629199378192, 0.3955622299108654, 0.39226500887889415, 0.39200197404716164, 0.4027169639011845, 0.3978547038277611, 0.39836159301921725, 3.215615506982431, 4.783188074012287, 2.360598027938977, 0.3945215219864622, 0.3873755029635504, 0.38634125888347626, 0.4144276441074908, 0.38442130805924535, 0.38375332206487656, 0.3858278508996591, 7.009595821029507, 1.3972146608866751, 0.4122039220528677, 0.40963145007845014, 0.4070663449820131, 0.40794701711274683, 0.4051949220011011, 0.4092830488225445, 0.40968689194414765, 0.41321010515093803, 3.451904858928174, 0.39089781686197966, 0.3827340028947219, 0.38380007608793676, 0.3874707509530708, 0.3985572779783979, 0.4021255790721625, 0.40124434512108564, 0.4019984267652035, 2.1603467070963234, 6.417135149007663, 2.471764747868292, 0.4132019600365311, 0.39945522497873753, 0.397621170966886, 0.39774526888504624, 0.39845314691774547, 0.3966151209315285, 0.397864049882628, 0.3993426349479705, 0.40219717705622315, 0.4043162548914552, 0.39960604591760784, 0.4078171431319788, 7.455857774009928, 2.3650482239900157, 0.40121608099434525, 0.39637163595762104, 0.394886820926331, 0.39881879300810397, 0.40397183888126165, 0.40164293092675507, 0.4010211080312729, 1.6823173540178686, 6.7130033581051975, 5.765791322221048, 0.39136105601210147, 0.3899857789510861, 0.39124624186661094, 0.39211206091567874, 0.40938949084375054, 0.3872564360499382, 2.859140945132822, 2.8092970229918137, 0.9622290689731017, 0.3972199260024354, 0.3864773678360507, 0.3814722589449957, 0.3889661580324173, 0.38347560190595686, 0.38434272701852024, 0.3854549190727994, 0.48004023695830256, 3.018282183096744, 0.5676829740405083, 0.4039875650778413, 0.4067193689988926, 0.39439986494835466, 0.3886396320303902, 0.40547066496219486, 0.38681282999459654, 1.877537143882364, 4.459628034965135, 3.1649565550033003, 1.3363861348479986, 0.4346634920220822, 0.4071883821161464, 0.38982229481916875, 0.379007285926491, 0.42892516998108476, 0.37986538594122976, 0.3827294319635257, 1.4118485731305555, 3.904342661029659, 0.3942702610511333, 0.38095499691553414, 0.37933779903687537, 0.38432117097545415, 0.38007927290163934, 0.38024810899514705, 0.3921833260683343, 0.3793235900811851, 0.38611058599781245, 0.398494262015447, 0.4045670230407268, 0.40400839410722256, 0.4021751470863819, 0.40122746501583606, 0.40367367095313966, 0.40245570591650903, 0.4043250441318378, 0.40193468902725726, 1.7287328710081056, 5.874142930144444, 0.5163825169438496, 0.4105944752227515, 0.3939163440372795, 0.38207386899739504, 0.38199873198755085, 0.3849191420013085, 0.39485342404805124, 8.767159789800644, 0.4840183148626238, 0.38526118895970285, 0.3840525079285726, 0.3870778230484575, 0.38662516488693655, 0.38002653105650097, 0.3787198959616944, 4.510267983889207, 0.6748328370740637, 0.41403809597250074, 0.3993634389480576, 0.3814206811366603, 0.3822419720236212, 0.382013350026682, 0.3813521359115839, 0.3819361600326374, 0.3767880699597299, 0.3835110029904172, 0.38665990892332047, 0.3977156209293753, 0.39378377003595233, 4.287508560111746, 1.1128321591531858, 0.6359318100148812, 0.40578652499243617, 0.39699398598168045, 0.3788134570932016, 0.3802639519562945, 0.38846858707256615, 0.3878249361878261, 0.3924551389645785, 2.8850969198392704, 4.0451330648502335, 1.3879942980129272, 0.3973101628944278, 0.3977298508398235, 0.3983533139107749, 0.39493591885548085, 0.39194185694213957, 3.3949142899364233, 7.39419196092058, 1.6680286370683461, 0.4032142008654773, 0.3821699470281601, 0.3804574229288846, 0.379988576984033, 0.38801132200751454, 0.4107969640754163, 0.40936523291748017, 0.409414981957525, 0.41091295902151614, 7.170542235835455, 5.219013932044618, 0.37654287787154317, 0.37675101088825613, 0.3844189429655671, 0.3912993079284206, 0.3929520819801837, 0.37985185999423265, 0.39105064515024424, 2.487011177930981, 3.2841244869632646, 1.0860721931094304, 0.4116385809611529, 0.40949253691360354, 0.3926726559875533, 0.3827190489973873, 0.3818587842397392, 0.39107924804557115, 0.40656021900940686, 0.40806369797792286, 0.40740316582378, 0.3933105330215767, 0.3840067950077355, 0.38469301513396204, 0.38438755099195987, 0.3837021930376068, 0.38509649015031755, 0.38435089704580605, 0.38501535006798804, 0.38368607603479177, 0.3868947660084814, 0.38564270304050297, 0.38437109789811075, 0.3854771979385987, 0.3841561059234664, 0.3859409629367292, 0.38597753702197224, 0.3847185851773247, 0.3839258559746668, 0.3851251108571887, 0.3831789770629257, 0.38508706691209227, 0.3855393868871033, 0.3869238418992609, 0.3861808500951156, 0.3898768739309162, 0.3893885639263317, 0.39055403508245945, 0.3910364710027352, 0.38977411901578307, 0.38997570506762713, 0.39133613696321845, 0.39014229783788323, 0.3894282509572804, 0.39183926896657795, 0.409801309928298, 0.3912028099875897, 0.40697080199606717, 0.39160557102877647, 0.3933941089781001, 0.39661471790168434, 0.3932390679838136, 0.39207031298428774, 2.6039124481612816, 2.8449274150189012, 2.6062433240003884, 0.3865326070226729, 0.3795322630321607, 0.37861329689621925, 0.38563380390405655, 0.3842978290049359, 0.38138448097743094, 0.3831664320314303, 0.39279494096990675, 8.48659346299246, 4.073821622994728, 0.40767387906089425, 0.4058440029621124, 0.41829189902637154, 0.4003287011291832, 0.40583495795726776, 3.168678469955921, 1.521223880117759, 1.1344133930979297, 0.38264942297246307, 0.37631687498651445, 0.3791282258462161, 0.38482818799093366, 0.3852538929786533, 0.38642475090455264, 0.38446499209385365, 0.38339076505508274, 0.3864034820580855, 0.3921067470218986, 0.40756949805654585, 0.407138949027285, 0.4056746199494228, 0.4065672369906679, 0.4173177629709244, 0.4133057778235525, 0.4146180379902944, 1.2923233410110697, 4.838216295000166, 0.394286363851279, 0.3879775230307132, 0.3853663749760017, 0.4016964529873803, 0.40893882408272475, 0.4011354699032381, 0.40649465599562973, 0.405319401063025, 7.029877334134653, 3.308227017871104, 1.1908221548656002, 0.41401491593569517, 0.41849047294817865, 0.4108591249678284, 0.4137744411127642, 0.4172808889998123, 0.41622564394492656, 5.648174186004326, 3.8755639009177685, 0.41554004489444196, 0.41434582602232695, 4.630947643192485, 5.308308779960498, 1.6676219858927652, 0.48832357907667756, 0.40350525395479053, 0.38789826387073845, 0.38518094597384334, 3.2548072959762067, 6.405046401079744, 1.7992413931060582, 0.38966852391604334, 0.3911991718923673, 0.3941127579892054, 0.38511991989798844, 0.38889354502316564, 2.5919043590547517, 2.548563984106295, 3.1655393739929423, 1.5010200270917267, 0.5224498871248215, 0.3939804119290784, 0.4102764918934554, 0.4030782171757892, 0.39206114201806486, 1.3699815821601078, 4.198418471030891, 3.8529425030574203, 0.41731923492625356, 0.4109440720640123, 0.4148029339266941, 0.3939845490967855, 0.3904398970771581, 4.259248747956008, 1.663863578112796, 0.5703618321567774, 0.41547227499540895, 0.4177630899939686, 0.427328277961351, 1.6538346769521013, 3.7193131080130115, 4.767517733969726, 0.5250817738706246, 0.43868740706238896, 0.4114020010456443, 0.39970419299788773, 0.40677590295672417, 0.488289742032066, 0.42563283897470683, 4.5133501689415425, 4.815319213899784, 0.4227944650920108, 0.41929373296443373, 0.4192061070352793, 0.4258883629227057, 2.249222101061605, 5.591409487999044, 2.7759724819334224, 0.4142233161255717, 0.41289722989313304, 0.41477286093868315, 1.3820400938857347, 3.5708987751277164, 0.4311227409634739, 0.42209171096328646, 0.41774618881754577, 0.4185743370326236, 4.0319835090776905, 1.5663914639735594, 0.6423807319952175, 0.39209461212158203, 0.38838665292132646, 0.3949563557980582, 0.4064841860672459, 0.39320883306209, 0.39904770080465823, 1.4411895941011608, 4.856056351913139, 4.4273813989711925, 0.9528559320606291, 0.3938538050279021, 0.38918517308775336, 0.39379301003646106, 0.3891308361198753, 0.3871321650221944, 0.38555759692098945, 0.3923647250048816, 0.38751699903514236, 0.4059230788843706, 4.619682348915376, 1.8322329501388595, 0.4239630961092189, 0.42159168189391494, 0.4212609649403021, 0.42348272597882897, 0.4257802970241755, 0.4275361980544403, 2.471350389881991, 5.306880053016357, 1.8099589819321409, 0.40682396094780415, 0.4046790818683803, 0.3945049240719527, 0.4072266520233825, 0.39082920097280294, 0.39174298802390695, 0.3914321050979197, 0.39259053103160113, 0.39595763804391026, 4.472800902090967, 1.3265929118497297, 0.39844313682988286, 0.3934510450344533, 0.3971494409488514, 0.40237239690031856, 0.40292884910013527, 4.8652244380209595, 0.9081927371444181, 0.39656127302441746, 0.3766577310161665, 0.3769622310064733, 0.38025693490635604, 0.37656350899487734, 0.3805326510919258, 0.37954254006035626, 8.308080188115127, 6.123426331905648, 0.39424018398858607, 0.3823755559278652, 0.41176234220620245, 0.39471786201465875, 0.39676151098683476, 0.4027484611142427, 1.3286951871123165, 4.207530883024447, 2.541530270827934, 1.5363115678774193, 0.7959997250000015, 0.38430360588245094, 0.4698802678612992, 0.37895256804767996, 0.3837975838687271, 0.3815311250509694, 0.37795013398863375, 2.757267614128068, 1.117349632899277, 0.39570223493501544, 0.3805445508332923, 0.3854649889981374, 0.38628368300851434, 0.37995892914477736, 0.3759618779877201, 4.303939870907925, 4.200670350925066, 0.5804253950482234, 0.38095254893414676, 0.3781212909379974, 0.3793722951086238, 0.38022608507890254, 0.37992329290136695, 0.3787561720237136, 0.3732764400774613, 4.241336298873648, 4.454587805084884, 6.673780665034428, 1.507352028042078, 0.3965114700840786, 0.39014031598344445, 0.3966591890202835, 0.39240538189187646, 0.3940007978817448, 0.3965593099128455, 2.1704778390703723, 3.9682094550225884, 1.1113304579630494, 0.5835455998312682, 0.3788905510446057, 0.37163429800421, 0.36755977908615023, 0.3725232050055638, 0.3679520981386304, 0.36789553984999657, 0.36806391284335405, 0.368643363006413, 0.3708728919737041, 4.8822405320825055, 3.400545174954459, 2.634550762013532, 0.4100762360030785, 0.3926515900529921, 0.3793227099813521, 0.38009300094563514, 0.37498404004145414, 0.375812339829281, 0.3746644288767129, 0.37598438595887274, 0.37664414301980287, 0.37470934400334954, 2.6784044769592583, 5.661636762903072, 1.8612518169684336, 1.1906986710382625, 0.3790756929665804, 0.37589344091247767, 0.37283443298656493, 0.3763573949690908, 0.3794020189670846, 0.37751967599615455, 0.3788524819537997, 0.37712002207990736, 0.3781848499784246, 0.38916042097844183, 5.926180815091357, 0.44182513596024364, 0.3739295781124383, 0.3693424810189754, 0.3673997230362147, 0.3672590049682185, 0.3710393449291587, 0.3665185780264437, 0.36553514294791967, 0.36320158187299967, 0.3677085890667513, 0.3680724670412019, 0.3693622419377789, 0.37429198797326535, 7.060623903875239, 0.4441797990584746, 0.38219312694855034, 0.3776282190810889, 0.3771534339757636, 0.3735177780035883, 0.3824091018177569, 0.37411274982150644, 0.37081209605094045, 0.3697830728488043, 0.38425974803976715, 0.3918331649620086, 0.3957196499686688, 1.9892706599785015, 2.307794261025265, 1.7252755920635536, 0.3923860938521102, 0.3775480550248176, 0.36506813997402787, 0.36972267006058246, 0.3634927391540259, 0.3668429519748315, 0.36615361308213323, 0.36732436483725905, 0.3703750290442258, 0.38623547193128616, 0.37194085004739463, 6.092905871104449, 1.054252695175819, 0.392908257083036, 0.37423182383645326, 0.3647525029955432, 0.36346016405150294, 0.3654368099523708, 0.36450966191478074, 0.3639407280134037, 0.3719635419547558, 0.3653285930631682, 0.36722228000871837, 0.3841091759968549, 0.3814880690770224, 0.3753733749035746, 0.3607062571682036, 0.3613127830903977, 0.36077827704139054, 0.36202158697415143, 0.36045853688847274, 0.36211808293592185, 0.37294367200229317, 0.36283018602989614, 0.38315547397360206, 3.7133265369338915, 6.168171501951292, 3.012366289854981, 0.38481896684970707, 0.380473273107782, 0.38112100900616497, 0.38099163596052676, 0.39270117902196944, 0.39287288999184966, 0.39690379006788135, 0.39595146116334945, 0.3975003120722249, 1.2273912490345538, 3.091053844895214, 2.775297762011178, 0.43865547399036586, 0.3842800100101158, 0.3829360898816958, 0.38490718184038997, 0.3872428279137239, 0.383003304945305, 0.37555315194185823, 0.3834594620857388, 0.3574240478919819, 0.3552866409299895, 4.684220911120065, 0.38548862701281905, 0.38386384991463274, 0.38574129808694124, 0.38472444005310535, 0.38467220414895564, 0.3856678889133036, 0.37684929999522865, 0.36515786207746714, 6.368567624944262, 0.6963421540567651, 0.4044115779688582, 0.3878009960753843, 0.45483172906097025, 0.37540567095857114, 0.3737178260926157, 0.37754832895006984, 0.3697191569954157, 0.37090328510385007, 0.37062515306752175, 0.3725858929101378, 0.3696750479284674, 0.3660529932240024, 0.3703458469826728, 0.3643253669142723, 0.36390205880161375, 0.36251205299049616, 0.3630469850031659, 0.3644288439536467, 0.36464005487505347, 0.3733485289849341, 3.9109535380266607, 1.498060584999621, 0.3857830179622397, 0.38358783500734717, 0.3820261009968817, 0.37914249289315194, 0.37744849314913154, 0.378321917145513, 0.3825605899328366, 0.3773424690589309, 0.37832213507499546, 0.37874582002405077, 0.3786597700091079, 0.3966182768344879, 0.3990491940639913, 0.38697488710749894, 0.38614084699656814, 4.398496046080254, 3.372798652970232, 0.5242954089771956, 0.3863942890893668, 0.3813666180940345, 0.37984707904979587, 0.3765314120100811, 0.37452003196813166, 0.3748711619991809, 0.37503313506022096, 0.3742792659904808, 0.3737921140855178, 0.37948792602401227, 0.3888703528791666, 0.4075208870926872, 0.4154869858175516, 4.3616817210568115, 2.4976317540276796, 0.40680785302538425, 0.4057324631139636, 0.3941208668984473, 0.38294846389908344, 0.3838277891045436, 0.38815704197622836, 0.3841833060141653, 0.3904625929426402, 0.3836125739617273, 0.38385663100052625, 1.0983308630529791, 3.159789363038726, 1.7817591081839055, 1.4815156219992787, 0.5435846580658108, 0.3919032618869096, 0.3720497259637341, 0.3643638049252331, 0.36803779599722475, 0.3752179560251534, 0.37895439902786165, 0.3782317789737135, 0.3870853818953037, 0.44018290692474693, 0.3988486840389669, 0.4028202958870679, 0.3880673418752849, 0.4301833820063621, 0.3805638998746872, 0.37949787196703255, 0.38038243108894676, 0.3770645080367103, 0.40106022893451154, 0.37684313603676856, 0.37967533501796424, 0.3818195218918845, 0.4693569368682802, 0.3791420441120863, 0.3816173099912703, 0.38067783100996166, 0.3797975070774555, 0.38202583498787135, 7.369387435959652, 5.1565671428106725, 0.4091965489787981, 0.39426158904097974, 0.376015888992697, 0.37596888304688036, 0.37519071192946285, 0.37266067299060524, 0.3709351619472727, 0.3740339280338958, 0.3880802859785035, 0.4098480090033263, 0.4087026050547138, 4.1467288550920784, 5.201908858958632, 0.43339751998428255, 0.3756101580802351, 0.3713613909203559, 0.3727302271872759, 0.37435437191743404, 0.37540263996925205, 0.3723592050373554, 0.37383778393268585, 0.37508571695070714, 0.37444157095160335, 0.37636801204644144, 2.601658583851531, 6.759547009016387, 0.929713970865123, 0.5415641639847308, 0.39226933696772903, 0.38328324002213776, 0.3782669090433046, 0.3779246589401737, 0.3773382540093735, 0.38271048304159194, 0.38237622810993344, 0.38055248314049095, 0.4027501360978931, 3.024502080050297, 3.7335821338929236, 1.897794695920311, 0.364086203975603, 0.36804451106581837, 0.4552495100069791, 0.36331172205973417, 0.3628718350082636, 0.36684323102235794, 0.3648160599404946, 0.4650306721450761, 4.454892290988937, 4.865657352958806, 0.8339787640143186, 0.37560436804778874, 0.3741068070521578, 0.37430520518682897, 0.369377517956309, 0.36638919508550316, 0.3680164770921692, 0.37030972307547927, 0.36787899502087384, 0.36841032397933304, 0.370057072956115, 0.37416890205349773, 0.3733206179458648, 0.3717063720105216, 3.3575724948896095, 4.98482627584599, 0.7635591090656817, 0.4067539480747655, 0.4060085889650509, 0.40622405894100666, 0.39184881595429033, 0.3771564911585301, 0.3845522451447323, 0.38487001706380397, 0.4025936840334907, 0.4016612130217254, 0.40570874512195587, 1.43158809596207, 4.084572572843172, 0.4183672439539805, 0.41240695596206933, 0.39644215104635805, 0.3790210799779743, 0.3783660960616544, 0.37883418099954724, 0.3802662370726466, 0.3850579949794337, 0.38578943302854896, 0.41019541118294, 0.4061523909913376, 0.4025774169713259, 0.4029366598697379, 0.4050764391431585, 0.40488220716360956, 0.40864664001856, 3.1592908788006753, 0.9123291870346293, 0.384626618004404, 0.37706729699857533, 0.37916807900182903, 0.37817537412047386, 0.38220675592310727, 0.38075690693221986, 0.3833631139714271, 0.37839447089936584, 3.8354466680902988, 0.8692693480988964, 0.4204217699589208, 0.3857457198901102, 0.3822070580208674, 0.38358590193092823, 0.4116992780473083, 0.4095522180432454, 0.37828117597382516, 0.371814223122783, 0.39758023794274777, 0.375047959969379, 0.380717386957258, 0.3839146039681509, 0.3806124848779291, 0.4044113609706983, 0.3796373170334846, 0.38319109100848436, 0.3806194639764726, 0.3958164689829573, 0.37997678096871823, 0.37943514506332576, 0.3808865698520094, 0.4487289290409535, 0.40216997801326215, 0.3849883918883279, 0.37465760693885386, 0.3759548629168421, 0.3739125879947096, 0.37515717989299446, 0.3762681039515883, 0.37218118517193943, 0.37226290395483375, 0.3752754930173978, 0.3724902479443699, 0.37829957995563745, 0.37396843708120286, 0.3807055379729718, 1.2956413491629064, 4.430662957020104, 3.2518428220646456, 0.9650272500002757, 2.2020542710088193, 0.8800613931380212, 0.6261772059369832, 0.390245939954184, 0.3878864179132506, 0.38697079091798514, 0.3859781740466133, 0.3836468370864168, 0.38566371984779835, 0.38412810303270817, 0.38451836397871375, 0.9723966480232775, 4.07119485095609, 0.39645904989447445, 0.3813497469527647, 0.36492058308795094, 0.3661203639348969, 0.3697078770492226, 0.3757654110668227, 0.3705752840032801, 0.3694069730117917, 0.37419877492357045, 0.3640752949286252, 0.36594048188999295, 0.36553794890642166, 0.37008156592492014, 0.380181920947507, 0.3670445760944858, 0.3689510099356994, 0.38671864091884345, 0.36816861398983747, 0.37081459700129926, 0.38505944702774286, 6.9728812200482935, 2.287034672102891, 0.39632591197732836, 0.3911692111287266, 0.3780516181141138, 0.3734694649465382, 0.37166584900114685, 0.37328444910235703, 2.5673578300047666, 3.405289896996692, 1.6568265210371464, 1.316172101884149, 0.3705049429554492, 0.36938109691254795, 0.36583471903577447, 0.3871530289761722, 0.3630243908846751, 0.36411336006131023]
Total Epoch List: [539, 302, 307]
Total Time List: [0.07838089705910534, 2.015425389050506, 0.07811456196941435]
T-times Epoch Time: 1.3080399965918017 ~ 0.5720828463589068
T-times Total Epoch: 262.4444444444444 ~ 88.39362023791895
T-times Total Time: 0.3175200446793396 ~ 0.2889507420975109
T-times Inference Elapsed: 0.27850384415570856 ~ 0.12917946214764664
T-times Time Per Graph: 0.005701318278314553 ~ 0.0026254715541200162
T-times Speed: 507.3143468935229 ~ 36.02104036050975
T-times cross validation test micro f1 score:0.759663599675258 ~ 0.12116174602453125
T-times cross validation test precision:0.749421768707483 ~ 0.14878160930278073
T-times cross validation test recall:0.8114814814814815 ~ 0.15522738752595958
T-times cross validation test f1_score:0.759663599675258 ~ 0.1179669177258377
