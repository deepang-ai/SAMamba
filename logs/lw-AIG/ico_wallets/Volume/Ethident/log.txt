Namespace(seed=15, model='Ethident', dataset='ico_wallets/Volume', num_heads=1, num_layers=2, dim_hidden=64, dropout=0.1, epochs=1000, lr=0.0003, weight_decay=0.0001, batch_size=64, abs_pe=None, abs_pe_dim=20, num_class=2, outdir='./outdir/ico_wallets/Volume/seed15/edge_attr/khopgnn_gat_1_0.1_0.0003_0.0001_2_1_64_BN', warmup=10, layer_norm=False, use_edge_attr=True, edge_dim=128, gnn_type='gat', k_hop=1, global_pool='mean', se='khopgnn', ie='mamba', aggr='add', not_extract_node_feature=False, k_ford=3, early_stop=1, early_stop_mindelta=-0.0, patience=20, training_times=3, Lambda=0.01, order='sd', use_cuda=True, batch_norm=True, save_logs=True)
Data(edge_index=[2, 334], edge_attr=[334, 2], x=[97, 14887], y=[1, 1], num_nodes=97)
Data(edge_index=[2, 334], edge_attr=[334, 2], x=[97, 14887], y=[1, 1], num_nodes=97)
Data(edge_index=[2, 276], edge_attr=[276, 2], x=[88, 14887], y=[1, 1], num_nodes=97)
========================training times:0========================
========================k_idx:0========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
emb dim:  64
Model: Ethident
Ethident(
  (encoder): HGATE_encoder(
    (lin1): Linear(in_features=14887, out_features=64, bias=True)
    (account_convs): ModuleList(
      (0): GATEConv()
      (1): GATConv(64, 64, heads=1)
    )
    (subgraph_conv): GATConv(64, 64, heads=1)
    (lin2): Linear(in_features=64, out_features=64, bias=True)
    (lin3): Linear(in_features=64, out_features=2, bias=True)
  )
  (proj_head_g1): Project_Head(
    (block): Sequential(
      (0): Linear(in_features=64, out_features=64, bias=True)
      (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=64, bias=True)
      (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
      (5): ReLU()
    )
    (linear_shortcut): Linear(in_features=64, out_features=64, bias=True)
  )
  (proj_head_g2): Project_Head(
    (block): Sequential(
      (0): Linear(in_features=64, out_features=64, bias=True)
      (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=64, bias=True)
      (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
      (5): ReLU()
    )
    (linear_shortcut): Linear(in_features=64, out_features=64, bias=True)
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=64, bias=True)
    (1): ReLU()
    (2): Dropout(p=0.0, inplace=True)
    (3): Linear(in_features=64, out_features=2, bias=True)
  )
)
Total number of parameters: 1003972
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7720be265d20>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.8246;  Loss pred: 0.7933; Loss self: 3.1365; time: 0.46s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.8092 score: 0.4898 time: 0.09s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7979 score: 0.5102 time: 0.08s
Epoch 2/1000, LR 0.000000
Train loss: 0.8246;  Loss pred: 0.7933; Loss self: 3.1365; time: 0.29s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7858 score: 0.4898 time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7740 score: 0.5102 time: 0.13s
Epoch 3/1000, LR 0.000030
Train loss: 0.7899;  Loss pred: 0.7584; Loss self: 3.1446; time: 0.43s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7545 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7449 score: 0.5102 time: 0.12s
Epoch 4/1000, LR 0.000060
Train loss: 0.7540;  Loss pred: 0.7222; Loss self: 3.1842; time: 0.36s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7267 score: 0.4898 time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7198 score: 0.5102 time: 0.12s
Epoch 5/1000, LR 0.000090
Train loss: 0.7282;  Loss pred: 0.6957; Loss self: 3.2521; time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7020 score: 0.4898 time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6953 score: 0.5102 time: 0.14s
Epoch 6/1000, LR 0.000120
Train loss: 0.7037;  Loss pred: 0.6705; Loss self: 3.3201; time: 0.31s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6753 score: 0.4898 time: 0.10s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6696 score: 0.5102 time: 0.11s
Epoch 7/1000, LR 0.000150
Train loss: 0.6780;  Loss pred: 0.6443; Loss self: 3.3742; time: 0.27s
Val loss: 0.6427 score: 0.5918 time: 0.12s
Test loss: 0.6503 score: 0.5306 time: 0.09s
Epoch 8/1000, LR 0.000180
Train loss: 0.6573;  Loss pred: 0.6233; Loss self: 3.4040; time: 0.24s
Val loss: 0.6152 score: 0.6531 time: 0.08s
Test loss: 0.6330 score: 0.5918 time: 0.07s
Epoch 9/1000, LR 0.000210
Train loss: 0.6377;  Loss pred: 0.6036; Loss self: 3.4141; time: 0.23s
Val loss: 0.5897 score: 0.7143 time: 0.10s
Test loss: 0.6132 score: 0.6122 time: 0.08s
Epoch 10/1000, LR 0.000240
Train loss: 0.6130;  Loss pred: 0.5790; Loss self: 3.3986; time: 0.27s
Val loss: 0.5636 score: 0.8367 time: 0.08s
Test loss: 0.5950 score: 0.7347 time: 0.08s
Epoch 11/1000, LR 0.000270
Train loss: 0.5845;  Loss pred: 0.5509; Loss self: 3.3561; time: 0.24s
Val loss: 0.5375 score: 0.8571 time: 0.08s
Test loss: 0.5794 score: 0.7347 time: 0.07s
Epoch 12/1000, LR 0.000270
Train loss: 0.5616;  Loss pred: 0.5287; Loss self: 3.2989; time: 0.26s
Val loss: 0.5087 score: 0.8367 time: 0.08s
Test loss: 0.5588 score: 0.7347 time: 0.07s
Epoch 13/1000, LR 0.000270
Train loss: 0.5328;  Loss pred: 0.5005; Loss self: 3.2299; time: 0.25s
Val loss: 0.4732 score: 0.8776 time: 0.08s
Test loss: 0.5370 score: 0.7347 time: 0.07s
Epoch 14/1000, LR 0.000270
Train loss: 0.4960;  Loss pred: 0.4646; Loss self: 3.1489; time: 0.22s
Val loss: 0.4510 score: 0.8776 time: 0.09s
Test loss: 0.5171 score: 0.8163 time: 0.11s
Epoch 15/1000, LR 0.000270
Train loss: 0.4706;  Loss pred: 0.4397; Loss self: 3.0832; time: 0.31s
Val loss: 0.4351 score: 0.8776 time: 0.08s
Test loss: 0.5031 score: 0.8163 time: 0.07s
Epoch 16/1000, LR 0.000270
Train loss: 0.4482;  Loss pred: 0.4178; Loss self: 3.0412; time: 0.28s
Val loss: 0.4143 score: 0.8776 time: 0.10s
Test loss: 0.4857 score: 0.7959 time: 0.07s
Epoch 17/1000, LR 0.000270
Train loss: 0.4207;  Loss pred: 0.3906; Loss self: 3.0134; time: 0.29s
Val loss: 0.3923 score: 0.8980 time: 0.10s
Test loss: 0.4659 score: 0.8367 time: 0.09s
Epoch 18/1000, LR 0.000270
Train loss: 0.3890;  Loss pred: 0.3591; Loss self: 2.9911; time: 0.26s
Val loss: 0.3736 score: 0.8980 time: 0.11s
Test loss: 0.4466 score: 0.8571 time: 0.07s
Epoch 19/1000, LR 0.000270
Train loss: 0.3614;  Loss pred: 0.3316; Loss self: 2.9716; time: 0.29s
Val loss: 0.3524 score: 0.8980 time: 0.12s
Test loss: 0.4274 score: 0.8571 time: 0.12s
Epoch 20/1000, LR 0.000270
Train loss: 0.3332;  Loss pred: 0.3037; Loss self: 2.9478; time: 0.30s
Val loss: 0.3299 score: 0.8980 time: 0.08s
Test loss: 0.4105 score: 0.8571 time: 0.08s
Epoch 21/1000, LR 0.000270
Train loss: 0.3063;  Loss pred: 0.2770; Loss self: 2.9308; time: 0.34s
Val loss: 0.3146 score: 0.8980 time: 0.08s
Test loss: 0.3982 score: 0.8367 time: 0.11s
Epoch 22/1000, LR 0.000270
Train loss: 0.2854;  Loss pred: 0.2562; Loss self: 2.9237; time: 0.31s
Val loss: 0.3011 score: 0.9184 time: 0.09s
Test loss: 0.3912 score: 0.8367 time: 0.07s
Epoch 23/1000, LR 0.000270
Train loss: 0.2686;  Loss pred: 0.2394; Loss self: 2.9205; time: 0.29s
Val loss: 0.2925 score: 0.9184 time: 0.11s
Test loss: 0.3814 score: 0.8367 time: 0.09s
Epoch 24/1000, LR 0.000270
Train loss: 0.2514;  Loss pred: 0.2222; Loss self: 2.9184; time: 0.31s
Val loss: 0.2880 score: 0.8980 time: 0.09s
Test loss: 0.3716 score: 0.8367 time: 0.07s
Epoch 25/1000, LR 0.000270
Train loss: 0.2337;  Loss pred: 0.2045; Loss self: 2.9184; time: 0.25s
Val loss: 0.2816 score: 0.8980 time: 0.08s
Test loss: 0.3638 score: 0.8367 time: 0.07s
Epoch 26/1000, LR 0.000270
Train loss: 0.2181;  Loss pred: 0.1889; Loss self: 2.9164; time: 0.23s
Val loss: 0.2698 score: 0.8980 time: 0.08s
Test loss: 0.3564 score: 0.8367 time: 0.07s
Epoch 27/1000, LR 0.000270
Train loss: 0.2057;  Loss pred: 0.1766; Loss self: 2.9141; time: 0.25s
Val loss: 0.2581 score: 0.8980 time: 0.08s
Test loss: 0.3499 score: 0.8367 time: 0.17s
Epoch 28/1000, LR 0.000270
Train loss: 0.1935;  Loss pred: 0.1644; Loss self: 2.9121; time: 0.32s
Val loss: 0.2481 score: 0.8980 time: 0.11s
Test loss: 0.3450 score: 0.8367 time: 0.12s
Epoch 29/1000, LR 0.000270
Train loss: 0.1825;  Loss pred: 0.1534; Loss self: 2.9114; time: 0.25s
Val loss: 0.2405 score: 0.8980 time: 0.08s
Test loss: 0.3419 score: 0.8367 time: 0.08s
Epoch 30/1000, LR 0.000270
Train loss: 0.1730;  Loss pred: 0.1439; Loss self: 2.9115; time: 0.24s
Val loss: 0.2364 score: 0.8980 time: 0.08s
Test loss: 0.3398 score: 0.8367 time: 0.08s
Epoch 31/1000, LR 0.000270
Train loss: 0.1652;  Loss pred: 0.1361; Loss self: 2.9120; time: 0.27s
Val loss: 0.2316 score: 0.8980 time: 0.11s
Test loss: 0.3387 score: 0.8367 time: 0.07s
Epoch 32/1000, LR 0.000270
Train loss: 0.1583;  Loss pred: 0.1292; Loss self: 2.9129; time: 0.27s
Val loss: 0.2233 score: 0.8980 time: 0.11s
Test loss: 0.3384 score: 0.8367 time: 0.08s
Epoch 33/1000, LR 0.000270
Train loss: 0.1517;  Loss pred: 0.1226; Loss self: 2.9170; time: 0.26s
Val loss: 0.2142 score: 0.9184 time: 0.08s
Test loss: 0.3385 score: 0.8367 time: 0.08s
Epoch 34/1000, LR 0.000270
Train loss: 0.1457;  Loss pred: 0.1165; Loss self: 2.9225; time: 0.26s
Val loss: 0.2056 score: 0.9184 time: 0.11s
Test loss: 0.3373 score: 0.8367 time: 0.13s
Epoch 35/1000, LR 0.000270
Train loss: 0.1399;  Loss pred: 0.1106; Loss self: 2.9295; time: 0.27s
Val loss: 0.1988 score: 0.9184 time: 0.09s
Test loss: 0.3355 score: 0.8367 time: 0.10s
Epoch 36/1000, LR 0.000270
Train loss: 0.1348;  Loss pred: 0.1054; Loss self: 2.9389; time: 0.28s
Val loss: 0.1937 score: 0.9184 time: 0.09s
Test loss: 0.3327 score: 0.8367 time: 0.08s
Epoch 37/1000, LR 0.000270
Train loss: 0.1306;  Loss pred: 0.1011; Loss self: 2.9497; time: 0.35s
Val loss: 0.1898 score: 0.9184 time: 0.09s
Test loss: 0.3303 score: 0.8367 time: 0.09s
Epoch 38/1000, LR 0.000270
Train loss: 0.1274;  Loss pred: 0.0978; Loss self: 2.9594; time: 0.27s
Val loss: 0.1874 score: 0.9184 time: 0.09s
Test loss: 0.3283 score: 0.8571 time: 0.07s
Epoch 39/1000, LR 0.000269
Train loss: 0.1246;  Loss pred: 0.0949; Loss self: 2.9666; time: 0.24s
Val loss: 0.1859 score: 0.9388 time: 0.08s
Test loss: 0.3270 score: 0.8571 time: 0.07s
Epoch 40/1000, LR 0.000269
Train loss: 0.1217;  Loss pred: 0.0920; Loss self: 2.9712; time: 0.23s
Val loss: 0.1849 score: 0.9388 time: 0.08s
Test loss: 0.3276 score: 0.8571 time: 0.12s
Epoch 41/1000, LR 0.000269
Train loss: 0.1191;  Loss pred: 0.0893; Loss self: 2.9739; time: 0.35s
Val loss: 0.1841 score: 0.9388 time: 0.09s
Test loss: 0.3295 score: 0.8571 time: 0.08s
Epoch 42/1000, LR 0.000269
Train loss: 0.1167;  Loss pred: 0.0870; Loss self: 2.9752; time: 0.25s
Val loss: 0.1818 score: 0.9388 time: 0.11s
Test loss: 0.3308 score: 0.8571 time: 0.07s
Epoch 43/1000, LR 0.000269
Train loss: 0.1147;  Loss pred: 0.0849; Loss self: 2.9759; time: 0.26s
Val loss: 0.1789 score: 0.9388 time: 0.09s
Test loss: 0.3312 score: 0.8571 time: 0.08s
Epoch 44/1000, LR 0.000269
Train loss: 0.1126;  Loss pred: 0.0828; Loss self: 2.9758; time: 0.25s
Val loss: 0.1757 score: 0.9388 time: 0.09s
Test loss: 0.3311 score: 0.8571 time: 0.07s
Epoch 45/1000, LR 0.000269
Train loss: 0.1105;  Loss pred: 0.0807; Loss self: 2.9753; time: 0.26s
Val loss: 0.1722 score: 0.9388 time: 0.09s
Test loss: 0.3305 score: 0.8571 time: 0.11s
Epoch 46/1000, LR 0.000269
Train loss: 0.1084;  Loss pred: 0.0786; Loss self: 2.9741; time: 0.26s
Val loss: 0.1687 score: 0.9388 time: 0.10s
Test loss: 0.3297 score: 0.8776 time: 0.07s
Epoch 47/1000, LR 0.000269
Train loss: 0.1064;  Loss pred: 0.0767; Loss self: 2.9726; time: 0.26s
Val loss: 0.1654 score: 0.9388 time: 0.11s
Test loss: 0.3286 score: 0.8776 time: 0.09s
Epoch 48/1000, LR 0.000269
Train loss: 0.1046;  Loss pred: 0.0749; Loss self: 2.9705; time: 0.26s
Val loss: 0.1626 score: 0.9388 time: 0.12s
Test loss: 0.3275 score: 0.8776 time: 0.08s
Epoch 49/1000, LR 0.000269
Train loss: 0.1027;  Loss pred: 0.0730; Loss self: 2.9677; time: 0.26s
Val loss: 0.1596 score: 0.9592 time: 0.10s
Test loss: 0.3273 score: 0.8980 time: 0.09s
Epoch 50/1000, LR 0.000269
Train loss: 0.1007;  Loss pred: 0.0711; Loss self: 2.9643; time: 0.23s
Val loss: 0.1566 score: 0.9592 time: 0.08s
Test loss: 0.3275 score: 0.8980 time: 0.14s
Epoch 51/1000, LR 0.000269
Train loss: 0.0987;  Loss pred: 0.0691; Loss self: 2.9603; time: 0.26s
Val loss: 0.1536 score: 0.9796 time: 0.08s
Test loss: 0.3283 score: 0.8980 time: 0.07s
Epoch 52/1000, LR 0.000269
Train loss: 0.0967;  Loss pred: 0.0672; Loss self: 2.9556; time: 0.23s
Val loss: 0.1510 score: 0.9796 time: 0.08s
Test loss: 0.3294 score: 0.8980 time: 0.11s
Epoch 53/1000, LR 0.000269
Train loss: 0.0947;  Loss pred: 0.0652; Loss self: 2.9498; time: 0.35s
Val loss: 0.1487 score: 0.9796 time: 0.11s
Test loss: 0.3308 score: 0.8980 time: 0.18s
Epoch 54/1000, LR 0.000269
Train loss: 0.0927;  Loss pred: 0.0633; Loss self: 2.9431; time: 0.35s
Val loss: 0.1468 score: 0.9796 time: 0.10s
Test loss: 0.3322 score: 0.8980 time: 0.14s
Epoch 55/1000, LR 0.000269
Train loss: 0.0912;  Loss pred: 0.0618; Loss self: 2.9365; time: 0.27s
Val loss: 0.1452 score: 0.9796 time: 0.13s
Test loss: 0.3330 score: 0.8980 time: 0.10s
Epoch 56/1000, LR 0.000269
Train loss: 0.0897;  Loss pred: 0.0604; Loss self: 2.9297; time: 0.27s
Val loss: 0.1439 score: 0.9796 time: 0.11s
Test loss: 0.3329 score: 0.8980 time: 0.09s
Epoch 57/1000, LR 0.000269
Train loss: 0.0881;  Loss pred: 0.0589; Loss self: 2.9235; time: 0.26s
Val loss: 0.1428 score: 0.9796 time: 0.09s
Test loss: 0.3322 score: 0.8980 time: 0.08s
Epoch 58/1000, LR 0.000269
Train loss: 0.0865;  Loss pred: 0.0573; Loss self: 2.9182; time: 0.26s
Val loss: 0.1419 score: 0.9796 time: 0.08s
Test loss: 0.3309 score: 0.8980 time: 0.07s
Epoch 59/1000, LR 0.000268
Train loss: 0.0848;  Loss pred: 0.0556; Loss self: 2.9140; time: 0.27s
Val loss: 0.1409 score: 0.9796 time: 0.10s
Test loss: 0.3295 score: 0.8980 time: 0.22s
Epoch 60/1000, LR 0.000268
Train loss: 0.0830;  Loss pred: 0.0539; Loss self: 2.9107; time: 0.23s
Val loss: 0.1400 score: 0.9796 time: 0.12s
Test loss: 0.3275 score: 0.8980 time: 0.10s
Epoch 61/1000, LR 0.000268
Train loss: 0.0815;  Loss pred: 0.0524; Loss self: 2.9083; time: 0.26s
Val loss: 0.1395 score: 0.9796 time: 0.10s
Test loss: 0.3263 score: 0.8980 time: 0.08s
Epoch 62/1000, LR 0.000268
Train loss: 0.0798;  Loss pred: 0.0507; Loss self: 2.9061; time: 0.26s
Val loss: 0.1391 score: 0.9796 time: 0.10s
Test loss: 0.3258 score: 0.8980 time: 0.08s
Epoch 63/1000, LR 0.000268
Train loss: 0.0778;  Loss pred: 0.0487; Loss self: 2.9035; time: 0.27s
Val loss: 0.1388 score: 0.9796 time: 0.13s
Test loss: 0.3260 score: 0.8980 time: 0.08s
Epoch 64/1000, LR 0.000268
Train loss: 0.0756;  Loss pred: 0.0466; Loss self: 2.9006; time: 0.24s
Val loss: 0.1388 score: 0.9796 time: 0.11s
Test loss: 0.3268 score: 0.8980 time: 0.08s
     INFO: Early stopping counter 1 of 20
Epoch 65/1000, LR 0.000268
Train loss: 0.0734;  Loss pred: 0.0444; Loss self: 2.8972; time: 0.27s
Val loss: 0.1389 score: 0.9796 time: 0.10s
Test loss: 0.3276 score: 0.8980 time: 0.09s
     INFO: Early stopping counter 2 of 20
Epoch 66/1000, LR 0.000268
Train loss: 0.0714;  Loss pred: 0.0425; Loss self: 2.8936; time: 0.28s
Val loss: 0.1386 score: 0.9796 time: 0.11s
Test loss: 0.3284 score: 0.8980 time: 0.09s
Epoch 67/1000, LR 0.000268
Train loss: 0.0694;  Loss pred: 0.0405; Loss self: 2.8901; time: 0.27s
Val loss: 0.1381 score: 0.9796 time: 0.16s
Test loss: 0.3291 score: 0.8980 time: 0.08s
Epoch 68/1000, LR 0.000268
Train loss: 0.0675;  Loss pred: 0.0387; Loss self: 2.8873; time: 0.44s
Val loss: 0.1373 score: 0.9796 time: 0.21s
Test loss: 0.3292 score: 0.8980 time: 0.08s
Epoch 69/1000, LR 0.000268
Train loss: 0.0657;  Loss pred: 0.0369; Loss self: 2.8854; time: 0.43s
Val loss: 0.1361 score: 0.9796 time: 0.13s
Test loss: 0.3283 score: 0.8980 time: 0.08s
Epoch 70/1000, LR 0.000268
Train loss: 0.0637;  Loss pred: 0.0349; Loss self: 2.8848; time: 0.31s
Val loss: 0.1348 score: 0.9796 time: 0.12s
Test loss: 0.3266 score: 0.8980 time: 0.13s
Epoch 71/1000, LR 0.000268
Train loss: 0.0620;  Loss pred: 0.0331; Loss self: 2.8857; time: 0.29s
Val loss: 0.1339 score: 0.9796 time: 0.10s
Test loss: 0.3250 score: 0.8980 time: 0.09s
Epoch 72/1000, LR 0.000267
Train loss: 0.0604;  Loss pred: 0.0315; Loss self: 2.8869; time: 0.36s
Val loss: 0.1333 score: 0.9796 time: 0.13s
Test loss: 0.3245 score: 0.8980 time: 0.11s
Epoch 73/1000, LR 0.000267
Train loss: 0.0589;  Loss pred: 0.0300; Loss self: 2.8879; time: 0.28s
Val loss: 0.1329 score: 0.9796 time: 0.12s
Test loss: 0.3245 score: 0.8980 time: 0.08s
Epoch 74/1000, LR 0.000267
Train loss: 0.0575;  Loss pred: 0.0286; Loss self: 2.8887; time: 0.26s
Val loss: 0.1326 score: 0.9796 time: 0.09s
Test loss: 0.3249 score: 0.8980 time: 0.14s
Epoch 75/1000, LR 0.000267
Train loss: 0.0559;  Loss pred: 0.0270; Loss self: 2.8885; time: 0.27s
Val loss: 0.1324 score: 0.9796 time: 0.10s
Test loss: 0.3258 score: 0.8980 time: 0.07s
Epoch 76/1000, LR 0.000267
Train loss: 0.0542;  Loss pred: 0.0253; Loss self: 2.8871; time: 0.24s
Val loss: 0.1320 score: 0.9796 time: 0.09s
Test loss: 0.3270 score: 0.8980 time: 0.07s
Epoch 77/1000, LR 0.000267
Train loss: 0.0526;  Loss pred: 0.0238; Loss self: 2.8854; time: 0.26s
Val loss: 0.1315 score: 0.9796 time: 0.08s
Test loss: 0.3282 score: 0.8980 time: 0.08s
Epoch 78/1000, LR 0.000267
Train loss: 0.0512;  Loss pred: 0.0224; Loss self: 2.8836; time: 0.25s
Val loss: 0.1310 score: 0.9796 time: 0.08s
Test loss: 0.3296 score: 0.8980 time: 0.09s
Epoch 79/1000, LR 0.000267
Train loss: 0.0500;  Loss pred: 0.0212; Loss self: 2.8820; time: 0.28s
Val loss: 0.1306 score: 0.9796 time: 0.09s
Test loss: 0.3308 score: 0.8980 time: 0.13s
Epoch 80/1000, LR 0.000267
Train loss: 0.0489;  Loss pred: 0.0201; Loss self: 2.8813; time: 0.28s
Val loss: 0.1303 score: 0.9796 time: 0.09s
Test loss: 0.3316 score: 0.8980 time: 0.09s
Epoch 81/1000, LR 0.000267
Train loss: 0.0479;  Loss pred: 0.0190; Loss self: 2.8813; time: 0.27s
Val loss: 0.1301 score: 0.9796 time: 0.10s
Test loss: 0.3321 score: 0.8980 time: 0.10s
Epoch 82/1000, LR 0.000267
Train loss: 0.0468;  Loss pred: 0.0180; Loss self: 2.8820; time: 0.30s
Val loss: 0.1300 score: 0.9796 time: 0.09s
Test loss: 0.3328 score: 0.8980 time: 0.08s
Epoch 83/1000, LR 0.000266
Train loss: 0.0458;  Loss pred: 0.0170; Loss self: 2.8832; time: 0.21s
Val loss: 0.1303 score: 0.9796 time: 0.08s
Test loss: 0.3337 score: 0.8980 time: 0.07s
     INFO: Early stopping counter 1 of 20
Epoch 84/1000, LR 0.000266
Train loss: 0.0448;  Loss pred: 0.0160; Loss self: 2.8847; time: 0.25s
Val loss: 0.1307 score: 0.9796 time: 0.08s
Test loss: 0.3347 score: 0.8980 time: 0.08s
     INFO: Early stopping counter 2 of 20
Epoch 85/1000, LR 0.000266
Train loss: 0.0439;  Loss pred: 0.0150; Loss self: 2.8865; time: 0.27s
Val loss: 0.1314 score: 0.9796 time: 0.10s
Test loss: 0.3361 score: 0.8980 time: 0.08s
     INFO: Early stopping counter 3 of 20
Epoch 86/1000, LR 0.000266
Train loss: 0.0430;  Loss pred: 0.0142; Loss self: 2.8882; time: 0.27s
Val loss: 0.1321 score: 0.9796 time: 0.09s
Test loss: 0.3377 score: 0.8980 time: 0.11s
     INFO: Early stopping counter 4 of 20
Epoch 87/1000, LR 0.000266
Train loss: 0.0422;  Loss pred: 0.0133; Loss self: 2.8898; time: 0.27s
Val loss: 0.1327 score: 0.9592 time: 0.10s
Test loss: 0.3392 score: 0.8980 time: 0.08s
     INFO: Early stopping counter 5 of 20
Epoch 88/1000, LR 0.000266
Train loss: 0.0416;  Loss pred: 0.0126; Loss self: 2.8910; time: 0.25s
Val loss: 0.1334 score: 0.9592 time: 0.10s
Test loss: 0.3406 score: 0.8980 time: 0.08s
     INFO: Early stopping counter 6 of 20
Epoch 89/1000, LR 0.000266
Train loss: 0.0409;  Loss pred: 0.0120; Loss self: 2.8917; time: 0.26s
Val loss: 0.1338 score: 0.9592 time: 0.10s
Test loss: 0.3418 score: 0.8980 time: 0.08s
     INFO: Early stopping counter 7 of 20
Epoch 90/1000, LR 0.000266
Train loss: 0.0403;  Loss pred: 0.0114; Loss self: 2.8919; time: 0.27s
Val loss: 0.1338 score: 0.9592 time: 0.11s
Test loss: 0.3431 score: 0.8980 time: 0.07s
     INFO: Early stopping counter 8 of 20
Epoch 91/1000, LR 0.000266
Train loss: 0.0398;  Loss pred: 0.0108; Loss self: 2.8917; time: 0.27s
Val loss: 0.1335 score: 0.9796 time: 0.09s
Test loss: 0.3445 score: 0.8980 time: 0.09s
     INFO: Early stopping counter 9 of 20
Epoch 92/1000, LR 0.000266
Train loss: 0.0392;  Loss pred: 0.0103; Loss self: 2.8910; time: 0.28s
Val loss: 0.1330 score: 0.9796 time: 0.09s
Test loss: 0.3459 score: 0.8980 time: 0.07s
     INFO: Early stopping counter 10 of 20
Epoch 93/1000, LR 0.000265
Train loss: 0.0388;  Loss pred: 0.0099; Loss self: 2.8898; time: 0.26s
Val loss: 0.1325 score: 0.9796 time: 0.08s
Test loss: 0.3472 score: 0.8980 time: 0.09s
     INFO: Early stopping counter 11 of 20
Epoch 94/1000, LR 0.000265
Train loss: 0.0383;  Loss pred: 0.0095; Loss self: 2.8883; time: 0.25s
Val loss: 0.1320 score: 0.9796 time: 0.10s
Test loss: 0.3486 score: 0.8980 time: 0.07s
     INFO: Early stopping counter 12 of 20
Epoch 95/1000, LR 0.000265
Train loss: 0.0379;  Loss pred: 0.0091; Loss self: 2.8866; time: 0.29s
Val loss: 0.1316 score: 0.9796 time: 0.10s
Test loss: 0.3500 score: 0.8980 time: 0.08s
     INFO: Early stopping counter 13 of 20
Epoch 96/1000, LR 0.000265
Train loss: 0.0375;  Loss pred: 0.0087; Loss self: 2.8844; time: 0.25s
Val loss: 0.1315 score: 0.9796 time: 0.09s
Test loss: 0.3516 score: 0.8980 time: 0.11s
     INFO: Early stopping counter 14 of 20
Epoch 97/1000, LR 0.000265
Train loss: 0.0372;  Loss pred: 0.0084; Loss self: 2.8821; time: 0.25s
Val loss: 0.1314 score: 0.9796 time: 0.09s
Test loss: 0.3533 score: 0.8980 time: 0.07s
     INFO: Early stopping counter 15 of 20
Epoch 98/1000, LR 0.000265
Train loss: 0.0368;  Loss pred: 0.0080; Loss self: 2.8796; time: 0.27s
Val loss: 0.1314 score: 0.9796 time: 0.07s
Test loss: 0.3551 score: 0.8980 time: 0.07s
     INFO: Early stopping counter 16 of 20
Epoch 99/1000, LR 0.000265
Train loss: 0.0365;  Loss pred: 0.0077; Loss self: 2.8770; time: 0.28s
Val loss: 0.1313 score: 0.9796 time: 0.10s
Test loss: 0.3567 score: 0.8980 time: 0.07s
     INFO: Early stopping counter 17 of 20
Epoch 100/1000, LR 0.000265
Train loss: 0.0362;  Loss pred: 0.0074; Loss self: 2.8743; time: 0.23s
Val loss: 0.1313 score: 0.9796 time: 0.09s
Test loss: 0.3586 score: 0.8980 time: 0.11s
     INFO: Early stopping counter 18 of 20
Epoch 101/1000, LR 0.000265
Train loss: 0.0359;  Loss pred: 0.0072; Loss self: 2.8716; time: 0.41s
Val loss: 0.1313 score: 0.9796 time: 0.16s
Test loss: 0.3607 score: 0.8980 time: 0.15s
     INFO: Early stopping counter 19 of 20
Epoch 102/1000, LR 0.000264
Train loss: 0.0356;  Loss pred: 0.0069; Loss self: 2.8688; time: 0.33s
Val loss: 0.1316 score: 0.9796 time: 0.09s
Test loss: 0.3623 score: 0.8980 time: 0.07s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 081,   Train_Loss: 0.0468,   Val_Loss: 0.1300,   Val_Precision: 0.9600,   Val_Recall: 1.0000,   Val_accuracy: 0.9796,   Val_Score: 0.9796,   Val_Loss: 0.1300,   Test_Precision: 0.9167,   Test_Recall: 0.8800,   Test_accuracy: 0.8980,   Test_Score: 0.8980,   Test_loss: 0.3328


[0.08933310594875365, 0.1325164180016145, 0.1264158149715513, 0.13019937300123274, 0.1420114409411326, 0.11638450401369482, 0.09018263500183821, 0.07496050407644361, 0.08110358798876405, 0.08063250395935029, 0.07832609792239964, 0.07626940705813468, 0.07174002705141902, 0.11831858905497938, 0.07016485708300024, 0.07907497009728104, 0.09813908394426107, 0.07781477901153266, 0.12308060703799129, 0.08350804005749524, 0.12056038901209831, 0.07937410799786448, 0.0900027829920873, 0.0734898999799043, 0.07265223795548081, 0.07268549501895905, 0.17077319789677858, 0.12430463801138103, 0.08541036304086447, 0.08295715996064246, 0.08090639393776655, 0.08556310599669814, 0.08601629897020757, 0.13505018805153668, 0.10933043097611517, 0.08225851599127054, 0.0975670350017026, 0.0767545480048284, 0.07178926491178572, 0.12037488399073482, 0.08885487203951925, 0.07253755698911846, 0.08581735903862864, 0.07108733791392297, 0.11462151899468154, 0.07273176498711109, 0.09300167101901025, 0.08020563295576721, 0.09041774098295718, 0.14073097496293485, 0.07177895703352988, 0.1120324389776215, 0.18368258501868695, 0.1470815520733595, 0.10402169800363481, 0.09542304696515203, 0.08583799400366843, 0.07775228505488485, 0.2269453089684248, 0.10264266096055508, 0.08991861494723707, 0.09063434798736125, 0.07988115807529539, 0.08415011002216488, 0.09618233994115144, 0.09811547095887363, 0.08750325499568135, 0.0850659339921549, 0.08745480398647487, 0.1363767309812829, 0.09993352298624814, 0.12023776699788868, 0.08381867699790746, 0.1467019469710067, 0.07873806799761951, 0.0753886109450832, 0.08313272998202592, 0.09734643204137683, 0.13083777797874063, 0.09782692999579012, 0.10459077905397862, 0.08333449403289706, 0.0838375820312649, 0.09055756591260433, 0.08289130195043981, 0.12105746904853731, 0.09133819001726806, 0.08121033501811326, 0.08209170203190297, 0.07718457607552409, 0.09204084100201726, 0.07979959796648473, 0.09504616796039045, 0.0779495439492166, 0.08889693394303322, 0.11686386191286147, 0.0723454199032858, 0.07841611793264747, 0.07892258698120713, 0.11904883000534028, 0.15723856794647872, 0.07504100108053535]
[0.0018231246111990542, 0.0027044166939105, 0.002579914591256149, 0.002657130061249648, 0.0028981926722680125, 0.00237519395946316, 0.0018404619388130245, 0.0015298062056417064, 0.0016551752650768173, 0.001645561305292863, 0.0015984917943346866, 0.0015565185113905035, 0.001464082184722837, 0.0024146650827546814, 0.0014319358588367396, 0.001613774899944511, 0.0020028384478420628, 0.0015880567145210747, 0.002511849123224312, 0.0017042457154590864, 0.002460416102287721, 0.001619879755058459, 0.0018367914896344347, 0.0014997938771409039, 0.0014826987337853228, 0.0014833774493665111, 0.0034851673040158897, 0.0025368293471710415, 0.00174306863348703, 0.0016930032645029072, 0.0016511508966891133, 0.001746185836667309, 0.001755434672861379, 0.0027561262867660547, 0.00223123328522684, 0.0016787452243116436, 0.0019911639796265836, 0.0015664193470373141, 0.001465087039016035, 0.0024566302855252003, 0.0018133647355003928, 0.0014803583059003766, 0.0017513746742577273, 0.0014507619982433257, 0.0023392146733608476, 0.0014843217344308387, 0.00189799328610225, 0.0016368496521585146, 0.0018452600200603508, 0.0028720607135292826, 0.001464876674153671, 0.002286376305665745, 0.003748624184054836, 0.0030016643280277445, 0.002122891795992547, 0.0019474091217377965, 0.0017517957959932331, 0.0015867813276507112, 0.004631536917722955, 0.002094748182868471, 0.0018350737744334098, 0.0018496805711706377, 0.0016302277158223549, 0.001717349184125814, 0.0019629048967581926, 0.0020023565501810945, 0.0017857807141975785, 0.0017360394692276509, 0.0017847919180913239, 0.002783198591454753, 0.0020394596527805744, 0.0024538319795487486, 0.0017105852448552543, 0.0029939172851225857, 0.001606899346890194, 0.0015385430805119021, 0.0016965863261637943, 0.0019866618783954456, 0.0026701587342600128, 0.001996467959097758, 0.0021345056949791555, 0.001700703959855042, 0.001710971061862549, 0.0018481135900531495, 0.0016916592234783635, 0.002470560592827292, 0.0018640446942299604, 0.0016573537758798624, 0.001675340857793938, 0.0015751954301127366, 0.0018783845102452502, 0.0016285632238058107, 0.001939717713477356, 0.0015908070193717675, 0.0018142231416945554, 0.002384976773731867, 0.0014764371408833837, 0.0016003289374009687, 0.0016106650404327987, 0.002429567959292659, 0.003208950366254668, 0.0015314490016435786]
[548.5088588334662, 369.76550331599674, 387.60973072100984, 376.34589837491814, 345.0426224483685, 421.018248221724, 543.3418528855497, 653.6775679900782, 604.1656259004026, 607.6953783390212, 625.5896987048427, 642.4594328188605, 683.0217664244773, 414.13610820892274, 698.3553025987973, 619.6651094488981, 499.291393710481, 629.7004325198671, 398.1130836060564, 586.7698483435072, 406.4353176156625, 617.3297720878743, 544.4276095807825, 666.758289416627, 674.4458447381315, 674.1372537563237, 286.93027128072725, 394.1928538138189, 573.7008748757574, 590.6663152794428, 605.6381654791209, 572.6767329120906, 569.6594783387685, 362.82807678358097, 448.1826291410556, 595.6830050909257, 502.2188078088559, 638.3986522455655, 682.5533045952058, 407.06165917278474, 551.4610383796025, 675.5121351460818, 570.980050527351, 689.2929379256302, 427.49389843868335, 673.7083859945288, 526.8722536177229, 610.9296591054037, 541.929044757223, 348.18205453991504, 682.6513232438134, 437.37332193390665, 266.7645383748001, 333.1485105321733, 471.0555676402033, 513.5027811247164, 570.8427901740796, 630.2065587578707, 215.91105021173817, 477.3843501468691, 544.9372193816873, 540.6338886757693, 613.4112371507304, 582.2927621496103, 509.44903222338263, 499.41155580386487, 559.9791688025589, 576.0237700384148, 560.2894039711985, 359.29883087405165, 490.32595405190403, 407.5258649876658, 584.595244819043, 334.01056367495977, 622.3165140587577, 649.9655503096353, 589.4188728145251, 503.3569178906597, 374.50957022490735, 500.88457239850686, 468.4925424899208, 587.9918102179489, 584.4634209718358, 541.0922820881595, 591.1356058720949, 404.76643353871646, 536.4678234891259, 603.3714796161224, 596.8934592312062, 634.8418620846495, 532.3723628180023, 614.0381812522371, 515.5389328312559, 628.6117598317578, 551.2001126090591, 419.2912949987604, 677.30618006648, 624.8715352382872, 620.8615540145405, 411.5958132289243, 311.62837871099634, 652.9763635137585]
Elapsed: 0.09758971505296216~0.02690190768177756
Time per graph: 0.0019916268378155546~0.0005490185241179094
Speed: 531.3318265785914~111.22853745669909
Total Time: 0.0758
best val loss: 0.13000254333019257 test_score: 0.8980

Testing...
Test loss: 0.3283 score: 0.8980 time: 0.07s
test Score 0.8980
Epoch Time List: [0.6323932181112468, 0.6680102088721469, 0.6248668601037934, 0.707784247933887, 0.6146137118339539, 0.5233469029190019, 0.4700630169827491, 0.38775763497687876, 0.40351178497076035, 0.43041884200647473, 0.39908200805075467, 0.41717656096443534, 0.39050029998179525, 0.42516208498273045, 0.4522315108915791, 0.44691944611258805, 0.4820707079488784, 0.4478492389898747, 0.5212276788661256, 0.46283576590940356, 0.5410454230150208, 0.46803126798477024, 0.4857000260381028, 0.46595987293403596, 0.39901139587163925, 0.38638271600939333, 0.4983892139280215, 0.5542896059341729, 0.4128830189583823, 0.39792199607472867, 0.45601770700886846, 0.459048523218371, 0.42182964796666056, 0.4919677859870717, 0.45818865194451064, 0.44737890595570207, 0.5353815431008115, 0.43681768502574414, 0.3777776740025729, 0.42287518398370594, 0.5247964310692623, 0.4265494759893045, 0.43062754289712757, 0.40848523390013725, 0.4517485399264842, 0.42963896587025374, 0.45687062200158834, 0.45278055395465344, 0.4442500490695238, 0.44560337718576193, 0.4039720119908452, 0.4085355040151626, 0.6372330249287188, 0.5926093668676913, 0.49481567298062146, 0.4748591829556972, 0.430659320903942, 0.4093341180123389, 0.5899813709547743, 0.450815471005626, 0.44627472409047186, 0.44786899513565004, 0.4731918869074434, 0.4241071371361613, 0.4604278140468523, 0.4803494669031352, 0.508032709825784, 0.7298657129285857, 0.6363277399213985, 0.5554584908531979, 0.4855876970104873, 0.5974915359402075, 0.47849214693997055, 0.48500467906706035, 0.43545747990719974, 0.40171133109834045, 0.41983152413740754, 0.4176981010241434, 0.4996301910141483, 0.4611607249826193, 0.468943067942746, 0.4629820939153433, 0.35490988788660616, 0.41935662494506687, 0.4469986920012161, 0.4688408998772502, 0.45922025199979544, 0.4173444659681991, 0.4332691120216623, 0.45427088905125856, 0.4426882689585909, 0.44747231586370617, 0.43406858993694186, 0.42428670497611165, 0.47548094496596605, 0.44880914490204304, 0.4014227361185476, 0.4163578209700063, 0.44592439103871584, 0.4335573478601873, 0.7150880978442729, 0.4926765487762168]
Total Epoch List: [102]
Total Time List: [0.07578808604739606]
========================k_idx:1========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
emb dim:  64
Model: Ethident
Ethident(
  (encoder): HGATE_encoder(
    (lin1): Linear(in_features=14887, out_features=64, bias=True)
    (account_convs): ModuleList(
      (0): GATEConv()
      (1): GATConv(64, 64, heads=1)
    )
    (subgraph_conv): GATConv(64, 64, heads=1)
    (lin2): Linear(in_features=64, out_features=64, bias=True)
    (lin3): Linear(in_features=64, out_features=2, bias=True)
  )
  (proj_head_g1): Project_Head(
    (block): Sequential(
      (0): Linear(in_features=64, out_features=64, bias=True)
      (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=64, bias=True)
      (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
      (5): ReLU()
    )
    (linear_shortcut): Linear(in_features=64, out_features=64, bias=True)
  )
  (proj_head_g2): Project_Head(
    (block): Sequential(
      (0): Linear(in_features=64, out_features=64, bias=True)
      (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=64, bias=True)
      (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
      (5): ReLU()
    )
    (linear_shortcut): Linear(in_features=64, out_features=64, bias=True)
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=64, bias=True)
    (1): ReLU()
    (2): Dropout(p=0.0, inplace=True)
    (3): Linear(in_features=64, out_features=2, bias=True)
  )
)
Total number of parameters: 1003972
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7720be265780>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.9385;  Loss pred: 0.9025; Loss self: 3.5967; time: 0.28s
Val loss: 0.8408 score: 0.4898 time: 0.11s
Test loss: 0.9215 score: 0.4490 time: 0.10s
Epoch 2/1000, LR 0.000000
Train loss: 0.9385;  Loss pred: 0.9025; Loss self: 3.5967; time: 0.30s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.8132 score: 0.5102 time: 0.09s
Test loss: 0.8824 score: 0.4490 time: 0.08s
Epoch 3/1000, LR 0.000030
Train loss: 0.8994;  Loss pred: 0.8634; Loss self: 3.6002; time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7751 score: 0.5102 time: 0.07s
Test loss: 0.8241 score: 0.4694 time: 0.10s
Epoch 4/1000, LR 0.000060
Train loss: 0.8466;  Loss pred: 0.8106; Loss self: 3.6003; time: 0.66s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7415 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7709 score: 0.4898 time: 0.29s
Epoch 5/1000, LR 0.000090
Train loss: 0.7891;  Loss pred: 0.7530; Loss self: 3.6036; time: 0.50s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7098 score: 0.5102 time: 0.11s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7241 score: 0.4898 time: 0.11s
Epoch 6/1000, LR 0.000120
Train loss: 0.7393;  Loss pred: 0.7032; Loss self: 3.6037; time: 0.37s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6839 score: 0.5102 time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6875 score: 0.4898 time: 0.09s
Epoch 7/1000, LR 0.000150
Train loss: 0.7021;  Loss pred: 0.6662; Loss self: 3.5931; time: 0.42s
Val loss: 0.6572 score: 0.5102 time: 0.13s
Test loss: 0.6515 score: 0.5306 time: 0.11s
Epoch 8/1000, LR 0.000180
Train loss: 0.6690;  Loss pred: 0.6333; Loss self: 3.5697; time: 0.36s
Val loss: 0.6301 score: 0.6531 time: 0.09s
Test loss: 0.6139 score: 0.6735 time: 0.19s
Epoch 9/1000, LR 0.000210
Train loss: 0.6353;  Loss pred: 0.6001; Loss self: 3.5184; time: 0.32s
Val loss: 0.6104 score: 0.6735 time: 0.13s
Test loss: 0.5857 score: 0.7143 time: 0.43s
Epoch 10/1000, LR 0.000240
Train loss: 0.5998;  Loss pred: 0.5651; Loss self: 3.4636; time: 0.38s
Val loss: 0.5955 score: 0.6939 time: 0.13s
Test loss: 0.5657 score: 0.7551 time: 0.09s
Epoch 11/1000, LR 0.000270
Train loss: 0.5691;  Loss pred: 0.5348; Loss self: 3.4386; time: 0.41s
Val loss: 0.5859 score: 0.6939 time: 0.09s
Test loss: 0.5461 score: 0.7755 time: 0.28s
Epoch 12/1000, LR 0.000270
Train loss: 0.5449;  Loss pred: 0.5105; Loss self: 3.4429; time: 0.45s
Val loss: 0.5845 score: 0.7347 time: 0.34s
Test loss: 0.5351 score: 0.7959 time: 0.08s
Epoch 13/1000, LR 0.000270
Train loss: 0.5274;  Loss pred: 0.4930; Loss self: 3.4482; time: 0.48s
Val loss: 0.5814 score: 0.7347 time: 0.08s
Test loss: 0.5226 score: 0.8163 time: 0.11s
Epoch 14/1000, LR 0.000270
Train loss: 0.5101;  Loss pred: 0.4757; Loss self: 3.4430; time: 0.41s
Val loss: 0.5715 score: 0.7551 time: 0.12s
Test loss: 0.5057 score: 0.8163 time: 0.10s
Epoch 15/1000, LR 0.000270
Train loss: 0.4906;  Loss pred: 0.4564; Loss self: 3.4267; time: 0.27s
Val loss: 0.5603 score: 0.7551 time: 0.08s
Test loss: 0.4868 score: 0.8163 time: 0.20s
Epoch 16/1000, LR 0.000270
Train loss: 0.4687;  Loss pred: 0.4347; Loss self: 3.4026; time: 0.35s
Val loss: 0.5505 score: 0.7347 time: 0.11s
Test loss: 0.4687 score: 0.8367 time: 0.09s
Epoch 17/1000, LR 0.000270
Train loss: 0.4429;  Loss pred: 0.4091; Loss self: 3.3779; time: 0.30s
Val loss: 0.5433 score: 0.7755 time: 0.07s
Test loss: 0.4519 score: 0.8571 time: 0.09s
Epoch 18/1000, LR 0.000270
Train loss: 0.4176;  Loss pred: 0.3841; Loss self: 3.3542; time: 0.24s
Val loss: 0.5319 score: 0.7755 time: 0.09s
Test loss: 0.4336 score: 0.8980 time: 0.09s
Epoch 19/1000, LR 0.000270
Train loss: 0.3950;  Loss pred: 0.3617; Loss self: 3.3270; time: 0.25s
Val loss: 0.5159 score: 0.7755 time: 0.09s
Test loss: 0.4136 score: 0.8980 time: 0.09s
Epoch 20/1000, LR 0.000270
Train loss: 0.3721;  Loss pred: 0.3391; Loss self: 3.2934; time: 0.26s
Val loss: 0.4989 score: 0.7755 time: 0.10s
Test loss: 0.3924 score: 0.8980 time: 0.09s
Epoch 21/1000, LR 0.000270
Train loss: 0.3468;  Loss pred: 0.3142; Loss self: 3.2561; time: 0.26s
Val loss: 0.4850 score: 0.7755 time: 0.11s
Test loss: 0.3721 score: 0.8980 time: 0.10s
Epoch 22/1000, LR 0.000270
Train loss: 0.3221;  Loss pred: 0.2899; Loss self: 3.2212; time: 0.23s
Val loss: 0.4764 score: 0.8163 time: 0.08s
Test loss: 0.3541 score: 0.8980 time: 0.08s
Epoch 23/1000, LR 0.000270
Train loss: 0.3004;  Loss pred: 0.2684; Loss self: 3.1927; time: 0.25s
Val loss: 0.4704 score: 0.8163 time: 0.08s
Test loss: 0.3392 score: 0.8980 time: 0.09s
Epoch 24/1000, LR 0.000270
Train loss: 0.2795;  Loss pred: 0.2478; Loss self: 3.1668; time: 0.23s
Val loss: 0.4643 score: 0.8163 time: 0.08s
Test loss: 0.3251 score: 0.8980 time: 0.13s
Epoch 25/1000, LR 0.000270
Train loss: 0.2602;  Loss pred: 0.2288; Loss self: 3.1425; time: 0.25s
Val loss: 0.4558 score: 0.8163 time: 0.08s
Test loss: 0.3113 score: 0.8980 time: 0.12s
Epoch 26/1000, LR 0.000270
Train loss: 0.2422;  Loss pred: 0.2110; Loss self: 3.1209; time: 0.26s
Val loss: 0.4485 score: 0.8163 time: 0.08s
Test loss: 0.2997 score: 0.8980 time: 0.11s
Epoch 27/1000, LR 0.000270
Train loss: 0.2265;  Loss pred: 0.1954; Loss self: 3.1016; time: 0.30s
Val loss: 0.4442 score: 0.8163 time: 0.08s
Test loss: 0.2904 score: 0.8980 time: 0.10s
Epoch 28/1000, LR 0.000270
Train loss: 0.2136;  Loss pred: 0.1828; Loss self: 3.0841; time: 0.23s
Val loss: 0.4420 score: 0.8163 time: 0.08s
Test loss: 0.2839 score: 0.8980 time: 0.08s
Epoch 29/1000, LR 0.000270
Train loss: 0.2031;  Loss pred: 0.1724; Loss self: 3.0695; time: 0.25s
Val loss: 0.4390 score: 0.8163 time: 0.08s
Test loss: 0.2772 score: 0.8980 time: 0.10s
Epoch 30/1000, LR 0.000270
Train loss: 0.1937;  Loss pred: 0.1632; Loss self: 3.0578; time: 0.25s
Val loss: 0.4338 score: 0.8163 time: 0.13s
Test loss: 0.2690 score: 0.8980 time: 0.10s
Epoch 31/1000, LR 0.000270
Train loss: 0.1848;  Loss pred: 0.1543; Loss self: 3.0490; time: 0.25s
Val loss: 0.4280 score: 0.8367 time: 0.09s
Test loss: 0.2596 score: 0.8980 time: 0.08s
Epoch 32/1000, LR 0.000270
Train loss: 0.1763;  Loss pred: 0.1458; Loss self: 3.0429; time: 0.26s
Val loss: 0.4234 score: 0.8367 time: 0.08s
Test loss: 0.2504 score: 0.8980 time: 0.09s
Epoch 33/1000, LR 0.000270
Train loss: 0.1685;  Loss pred: 0.1381; Loss self: 3.0384; time: 0.29s
Val loss: 0.4217 score: 0.8367 time: 0.10s
Test loss: 0.2431 score: 0.8980 time: 0.09s
Epoch 34/1000, LR 0.000270
Train loss: 0.1614;  Loss pred: 0.1310; Loss self: 3.0361; time: 0.26s
Val loss: 0.4264 score: 0.8367 time: 0.08s
Test loss: 0.2396 score: 0.8980 time: 0.08s
     INFO: Early stopping counter 1 of 20
Epoch 35/1000, LR 0.000270
Train loss: 0.1550;  Loss pred: 0.1247; Loss self: 3.0355; time: 0.29s
Val loss: 0.4333 score: 0.8367 time: 0.08s
Test loss: 0.2373 score: 0.8980 time: 0.10s
     INFO: Early stopping counter 2 of 20
Epoch 36/1000, LR 0.000270
Train loss: 0.1493;  Loss pred: 0.1189; Loss self: 3.0351; time: 0.26s
Val loss: 0.4375 score: 0.8163 time: 0.11s
Test loss: 0.2335 score: 0.9184 time: 0.08s
     INFO: Early stopping counter 3 of 20
Epoch 37/1000, LR 0.000270
Train loss: 0.1442;  Loss pred: 0.1139; Loss self: 3.0344; time: 0.26s
Val loss: 0.4380 score: 0.8163 time: 0.07s
Test loss: 0.2276 score: 0.9388 time: 0.11s
     INFO: Early stopping counter 4 of 20
Epoch 38/1000, LR 0.000270
Train loss: 0.1394;  Loss pred: 0.1091; Loss self: 3.0333; time: 0.26s
Val loss: 0.4356 score: 0.8163 time: 0.07s
Test loss: 0.2212 score: 0.9388 time: 0.11s
     INFO: Early stopping counter 5 of 20
Epoch 39/1000, LR 0.000269
Train loss: 0.1351;  Loss pred: 0.1048; Loss self: 3.0318; time: 0.27s
Val loss: 0.4334 score: 0.8163 time: 0.09s
Test loss: 0.2152 score: 0.9388 time: 0.08s
     INFO: Early stopping counter 6 of 20
Epoch 40/1000, LR 0.000269
Train loss: 0.1315;  Loss pred: 0.1012; Loss self: 3.0305; time: 0.25s
Val loss: 0.4332 score: 0.8163 time: 0.08s
Test loss: 0.2112 score: 0.9388 time: 0.08s
     INFO: Early stopping counter 7 of 20
Epoch 41/1000, LR 0.000269
Train loss: 0.1282;  Loss pred: 0.0979; Loss self: 3.0293; time: 0.24s
Val loss: 0.4353 score: 0.8163 time: 0.08s
Test loss: 0.2092 score: 0.9388 time: 0.08s
     INFO: Early stopping counter 8 of 20
Epoch 42/1000, LR 0.000269
Train loss: 0.1253;  Loss pred: 0.0950; Loss self: 3.0285; time: 0.23s
Val loss: 0.4392 score: 0.8163 time: 0.07s
Test loss: 0.2092 score: 0.9388 time: 0.10s
     INFO: Early stopping counter 9 of 20
Epoch 43/1000, LR 0.000269
Train loss: 0.1224;  Loss pred: 0.0921; Loss self: 3.0277; time: 0.29s
Val loss: 0.4430 score: 0.8163 time: 0.07s
Test loss: 0.2101 score: 0.9388 time: 0.08s
     INFO: Early stopping counter 10 of 20
Epoch 44/1000, LR 0.000269
Train loss: 0.1195;  Loss pred: 0.0892; Loss self: 3.0266; time: 0.38s
Val loss: 0.4455 score: 0.8163 time: 0.10s
Test loss: 0.2106 score: 0.9184 time: 0.09s
     INFO: Early stopping counter 11 of 20
Epoch 45/1000, LR 0.000269
Train loss: 0.1167;  Loss pred: 0.0865; Loss self: 3.0254; time: 0.28s
Val loss: 0.4473 score: 0.8163 time: 0.09s
Test loss: 0.2110 score: 0.9184 time: 0.09s
     INFO: Early stopping counter 12 of 20
Epoch 46/1000, LR 0.000269
Train loss: 0.1140;  Loss pred: 0.0838; Loss self: 3.0244; time: 0.27s
Val loss: 0.4493 score: 0.8163 time: 0.08s
Test loss: 0.2114 score: 0.9184 time: 0.09s
     INFO: Early stopping counter 13 of 20
Epoch 47/1000, LR 0.000269
Train loss: 0.1114;  Loss pred: 0.0812; Loss self: 3.0232; time: 0.25s
Val loss: 0.4503 score: 0.8163 time: 0.11s
Test loss: 0.2111 score: 0.9184 time: 0.13s
     INFO: Early stopping counter 14 of 20
Epoch 48/1000, LR 0.000269
Train loss: 0.1087;  Loss pred: 0.0785; Loss self: 3.0217; time: 0.26s
Val loss: 0.4504 score: 0.8163 time: 0.08s
Test loss: 0.2100 score: 0.9184 time: 0.10s
     INFO: Early stopping counter 15 of 20
Epoch 49/1000, LR 0.000269
Train loss: 0.1060;  Loss pred: 0.0758; Loss self: 3.0197; time: 0.30s
Val loss: 0.4506 score: 0.8163 time: 0.07s
Test loss: 0.2087 score: 0.9184 time: 0.08s
     INFO: Early stopping counter 16 of 20
Epoch 50/1000, LR 0.000269
Train loss: 0.1032;  Loss pred: 0.0730; Loss self: 3.0174; time: 0.27s
Val loss: 0.4510 score: 0.8163 time: 0.07s
Test loss: 0.2075 score: 0.9184 time: 0.08s
     INFO: Early stopping counter 17 of 20
Epoch 51/1000, LR 0.000269
Train loss: 0.1005;  Loss pred: 0.0703; Loss self: 3.0150; time: 0.27s
Val loss: 0.4521 score: 0.8163 time: 0.08s
Test loss: 0.2065 score: 0.8980 time: 0.10s
     INFO: Early stopping counter 18 of 20
Epoch 52/1000, LR 0.000269
Train loss: 0.0977;  Loss pred: 0.0676; Loss self: 3.0128; time: 0.25s
Val loss: 0.4543 score: 0.8163 time: 0.09s
Test loss: 0.2059 score: 0.8980 time: 0.13s
     INFO: Early stopping counter 19 of 20
Epoch 53/1000, LR 0.000269
Train loss: 0.0949;  Loss pred: 0.0648; Loss self: 3.0105; time: 0.29s
Val loss: 0.4577 score: 0.8163 time: 0.10s
Test loss: 0.2055 score: 0.8980 time: 0.10s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 032,   Train_Loss: 0.1685,   Val_Loss: 0.4217,   Val_Precision: 1.0000,   Val_Recall: 0.6800,   Val_accuracy: 0.8095,   Val_Score: 0.8367,   Val_Loss: 0.4217,   Test_Precision: 0.9524,   Test_Recall: 0.8333,   Test_accuracy: 0.8889,   Test_Score: 0.8980,   Test_loss: 0.2431


[0.08933310594875365, 0.1325164180016145, 0.1264158149715513, 0.13019937300123274, 0.1420114409411326, 0.11638450401369482, 0.09018263500183821, 0.07496050407644361, 0.08110358798876405, 0.08063250395935029, 0.07832609792239964, 0.07626940705813468, 0.07174002705141902, 0.11831858905497938, 0.07016485708300024, 0.07907497009728104, 0.09813908394426107, 0.07781477901153266, 0.12308060703799129, 0.08350804005749524, 0.12056038901209831, 0.07937410799786448, 0.0900027829920873, 0.0734898999799043, 0.07265223795548081, 0.07268549501895905, 0.17077319789677858, 0.12430463801138103, 0.08541036304086447, 0.08295715996064246, 0.08090639393776655, 0.08556310599669814, 0.08601629897020757, 0.13505018805153668, 0.10933043097611517, 0.08225851599127054, 0.0975670350017026, 0.0767545480048284, 0.07178926491178572, 0.12037488399073482, 0.08885487203951925, 0.07253755698911846, 0.08581735903862864, 0.07108733791392297, 0.11462151899468154, 0.07273176498711109, 0.09300167101901025, 0.08020563295576721, 0.09041774098295718, 0.14073097496293485, 0.07177895703352988, 0.1120324389776215, 0.18368258501868695, 0.1470815520733595, 0.10402169800363481, 0.09542304696515203, 0.08583799400366843, 0.07775228505488485, 0.2269453089684248, 0.10264266096055508, 0.08991861494723707, 0.09063434798736125, 0.07988115807529539, 0.08415011002216488, 0.09618233994115144, 0.09811547095887363, 0.08750325499568135, 0.0850659339921549, 0.08745480398647487, 0.1363767309812829, 0.09993352298624814, 0.12023776699788868, 0.08381867699790746, 0.1467019469710067, 0.07873806799761951, 0.0753886109450832, 0.08313272998202592, 0.09734643204137683, 0.13083777797874063, 0.09782692999579012, 0.10459077905397862, 0.08333449403289706, 0.0838375820312649, 0.09055756591260433, 0.08289130195043981, 0.12105746904853731, 0.09133819001726806, 0.08121033501811326, 0.08209170203190297, 0.07718457607552409, 0.09204084100201726, 0.07979959796648473, 0.09504616796039045, 0.0779495439492166, 0.08889693394303322, 0.11686386191286147, 0.0723454199032858, 0.07841611793264747, 0.07892258698120713, 0.11904883000534028, 0.15723856794647872, 0.07504100108053535, 0.1121549770468846, 0.08840202505234629, 0.10531830601394176, 0.29692155797965825, 0.12062632804736495, 0.09650054294615984, 0.11894398799631745, 0.20026547997258604, 0.4353803519625217, 0.09577900299336761, 0.28215465997345746, 0.08345463301520795, 0.11137664597481489, 0.10720696300268173, 0.19953610107768327, 0.0913637870689854, 0.09168238402344286, 0.0916319020325318, 0.09639635507483035, 0.09347816300578415, 0.1060151590500027, 0.0869493440259248, 0.09169552091043442, 0.1342890589730814, 0.12619544903282076, 0.11114300496410578, 0.1059510309714824, 0.08655079291202128, 0.10460378997959197, 0.10380979406181723, 0.0864687270950526, 0.09469805296976119, 0.09341621596831828, 0.08727203798480332, 0.10309809201862663, 0.08892002596985549, 0.11173332901671529, 0.11029266403056681, 0.08702832390554249, 0.0842467830516398, 0.08752005803398788, 0.10063336091116071, 0.08362018002662808, 0.09525998192839324, 0.09745450189802796, 0.09095453505869955, 0.1305154140572995, 0.10023071500472724, 0.08528496697545052, 0.08379478997085243, 0.0997695840196684, 0.13467627193313092, 0.10875356395263225]
[0.0018231246111990542, 0.0027044166939105, 0.002579914591256149, 0.002657130061249648, 0.0028981926722680125, 0.00237519395946316, 0.0018404619388130245, 0.0015298062056417064, 0.0016551752650768173, 0.001645561305292863, 0.0015984917943346866, 0.0015565185113905035, 0.001464082184722837, 0.0024146650827546814, 0.0014319358588367396, 0.001613774899944511, 0.0020028384478420628, 0.0015880567145210747, 0.002511849123224312, 0.0017042457154590864, 0.002460416102287721, 0.001619879755058459, 0.0018367914896344347, 0.0014997938771409039, 0.0014826987337853228, 0.0014833774493665111, 0.0034851673040158897, 0.0025368293471710415, 0.00174306863348703, 0.0016930032645029072, 0.0016511508966891133, 0.001746185836667309, 0.001755434672861379, 0.0027561262867660547, 0.00223123328522684, 0.0016787452243116436, 0.0019911639796265836, 0.0015664193470373141, 0.001465087039016035, 0.0024566302855252003, 0.0018133647355003928, 0.0014803583059003766, 0.0017513746742577273, 0.0014507619982433257, 0.0023392146733608476, 0.0014843217344308387, 0.00189799328610225, 0.0016368496521585146, 0.0018452600200603508, 0.0028720607135292826, 0.001464876674153671, 0.002286376305665745, 0.003748624184054836, 0.0030016643280277445, 0.002122891795992547, 0.0019474091217377965, 0.0017517957959932331, 0.0015867813276507112, 0.004631536917722955, 0.002094748182868471, 0.0018350737744334098, 0.0018496805711706377, 0.0016302277158223549, 0.001717349184125814, 0.0019629048967581926, 0.0020023565501810945, 0.0017857807141975785, 0.0017360394692276509, 0.0017847919180913239, 0.002783198591454753, 0.0020394596527805744, 0.0024538319795487486, 0.0017105852448552543, 0.0029939172851225857, 0.001606899346890194, 0.0015385430805119021, 0.0016965863261637943, 0.0019866618783954456, 0.0026701587342600128, 0.001996467959097758, 0.0021345056949791555, 0.001700703959855042, 0.001710971061862549, 0.0018481135900531495, 0.0016916592234783635, 0.002470560592827292, 0.0018640446942299604, 0.0016573537758798624, 0.001675340857793938, 0.0015751954301127366, 0.0018783845102452502, 0.0016285632238058107, 0.001939717713477356, 0.0015908070193717675, 0.0018142231416945554, 0.002384976773731867, 0.0014764371408833837, 0.0016003289374009687, 0.0016106650404327987, 0.002429567959292659, 0.003208950366254668, 0.0015314490016435786, 0.0022888770825894816, 0.001804122960251965, 0.0021493531839579952, 0.0060596236322379236, 0.0024617617968849988, 0.0019693988356359153, 0.0024274283264554583, 0.004087050611685429, 0.008885313305357585, 0.00195467353047689, 0.005758258366805254, 0.0017031557758205704, 0.0022729927749962223, 0.0021878972041363617, 0.004072165328115985, 0.0018645670830405184, 0.0018710690617029155, 0.0018700388169904448, 0.0019672725525475584, 0.001907717612362942, 0.002163574674489851, 0.0017744764086923429, 0.0018713371614374372, 0.0027405930402669676, 0.002575417327200424, 0.0022682245911041996, 0.002162265938193518, 0.0017663427124902302, 0.0021347712240733056, 0.002118567225751372, 0.0017646678998990326, 0.0019326133259134938, 0.0019064533871085364, 0.0017810619996898637, 0.0021040426942576865, 0.0018146944075480712, 0.0022802720207492915, 0.0022508706945013634, 0.001776088242970255, 0.0017193221030946896, 0.0017861236333466914, 0.002053742059411443, 0.0017065342862577159, 0.0019440812638447601, 0.00198886738567404, 0.00185621500119795, 0.0026635798787203978, 0.0020455247960148417, 0.001740509530111235, 0.0017100977545071925, 0.0020361139595850694, 0.0027484953455741003, 0.0022194604888292296]
[548.5088588334662, 369.76550331599674, 387.60973072100984, 376.34589837491814, 345.0426224483685, 421.018248221724, 543.3418528855497, 653.6775679900782, 604.1656259004026, 607.6953783390212, 625.5896987048427, 642.4594328188605, 683.0217664244773, 414.13610820892274, 698.3553025987973, 619.6651094488981, 499.291393710481, 629.7004325198671, 398.1130836060564, 586.7698483435072, 406.4353176156625, 617.3297720878743, 544.4276095807825, 666.758289416627, 674.4458447381315, 674.1372537563237, 286.93027128072725, 394.1928538138189, 573.7008748757574, 590.6663152794428, 605.6381654791209, 572.6767329120906, 569.6594783387685, 362.82807678358097, 448.1826291410556, 595.6830050909257, 502.2188078088559, 638.3986522455655, 682.5533045952058, 407.06165917278474, 551.4610383796025, 675.5121351460818, 570.980050527351, 689.2929379256302, 427.49389843868335, 673.7083859945288, 526.8722536177229, 610.9296591054037, 541.929044757223, 348.18205453991504, 682.6513232438134, 437.37332193390665, 266.7645383748001, 333.1485105321733, 471.0555676402033, 513.5027811247164, 570.8427901740796, 630.2065587578707, 215.91105021173817, 477.3843501468691, 544.9372193816873, 540.6338886757693, 613.4112371507304, 582.2927621496103, 509.44903222338263, 499.41155580386487, 559.9791688025589, 576.0237700384148, 560.2894039711985, 359.29883087405165, 490.32595405190403, 407.5258649876658, 584.595244819043, 334.01056367495977, 622.3165140587577, 649.9655503096353, 589.4188728145251, 503.3569178906597, 374.50957022490735, 500.88457239850686, 468.4925424899208, 587.9918102179489, 584.4634209718358, 541.0922820881595, 591.1356058720949, 404.76643353871646, 536.4678234891259, 603.3714796161224, 596.8934592312062, 634.8418620846495, 532.3723628180023, 614.0381812522371, 515.5389328312559, 628.6117598317578, 551.2001126090591, 419.2912949987604, 677.30618006648, 624.8715352382872, 620.8615540145405, 411.5958132289243, 311.62837871099634, 652.9763635137585, 436.8954574304476, 554.2859450446433, 465.2562489327687, 165.02675094866953, 406.21314428770256, 507.7691638205432, 411.9586103125873, 244.67521814896665, 112.54527168974775, 511.5943836186423, 173.66362123740743, 587.1453534649265, 439.9486047647741, 457.05986465426025, 245.56959735784028, 536.3175232983931, 534.45381598575, 534.7482581187028, 508.31797490644107, 524.1865952903677, 462.1980520435653, 563.546517215704, 534.3772467126481, 364.88452875242933, 388.28658541605677, 440.8734496230762, 462.47780272368243, 566.1415493883275, 468.43427001602737, 472.01711979913193, 566.6788635171615, 517.4340808849215, 524.5341988228055, 561.4627678172517, 475.27552683658996, 551.0569690635419, 438.5441696870019, 444.2725219369078, 563.0350879006114, 581.6245822699844, 559.8716580029135, 486.91606398058474, 585.9829527321803, 514.3817897932545, 502.79873218449586, 538.7306962580453, 375.434582604073, 488.872098714341, 574.5443978902491, 584.7618929177385, 491.13164579638044, 363.83543512645895, 450.55994690290805]
Elapsed: 0.10435851748597118~0.04285477237002061
Time per graph: 0.0021297656629790033~0.0008745871912249104
Speed: 509.79648708168367~114.40833406413564
Total Time: 0.1094
best val loss: 0.4217376708984375 test_score: 0.8980

Testing...
Test loss: 0.2596 score: 0.8980 time: 0.08s
test Score 0.8980
Epoch Time List: [0.6323932181112468, 0.6680102088721469, 0.6248668601037934, 0.707784247933887, 0.6146137118339539, 0.5233469029190019, 0.4700630169827491, 0.38775763497687876, 0.40351178497076035, 0.43041884200647473, 0.39908200805075467, 0.41717656096443534, 0.39050029998179525, 0.42516208498273045, 0.4522315108915791, 0.44691944611258805, 0.4820707079488784, 0.4478492389898747, 0.5212276788661256, 0.46283576590940356, 0.5410454230150208, 0.46803126798477024, 0.4857000260381028, 0.46595987293403596, 0.39901139587163925, 0.38638271600939333, 0.4983892139280215, 0.5542896059341729, 0.4128830189583823, 0.39792199607472867, 0.45601770700886846, 0.459048523218371, 0.42182964796666056, 0.4919677859870717, 0.45818865194451064, 0.44737890595570207, 0.5353815431008115, 0.43681768502574414, 0.3777776740025729, 0.42287518398370594, 0.5247964310692623, 0.4265494759893045, 0.43062754289712757, 0.40848523390013725, 0.4517485399264842, 0.42963896587025374, 0.45687062200158834, 0.45278055395465344, 0.4442500490695238, 0.44560337718576193, 0.4039720119908452, 0.4085355040151626, 0.6372330249287188, 0.5926093668676913, 0.49481567298062146, 0.4748591829556972, 0.430659320903942, 0.4093341180123389, 0.5899813709547743, 0.450815471005626, 0.44627472409047186, 0.44786899513565004, 0.4731918869074434, 0.4241071371361613, 0.4604278140468523, 0.4803494669031352, 0.508032709825784, 0.7298657129285857, 0.6363277399213985, 0.5554584908531979, 0.4855876970104873, 0.5974915359402075, 0.47849214693997055, 0.48500467906706035, 0.43545747990719974, 0.40171133109834045, 0.41983152413740754, 0.4176981010241434, 0.4996301910141483, 0.4611607249826193, 0.468943067942746, 0.4629820939153433, 0.35490988788660616, 0.41935662494506687, 0.4469986920012161, 0.4688408998772502, 0.45922025199979544, 0.4173444659681991, 0.4332691120216623, 0.45427088905125856, 0.4426882689585909, 0.44747231586370617, 0.43406858993694186, 0.42428670497611165, 0.47548094496596605, 0.44880914490204304, 0.4014227361185476, 0.4163578209700063, 0.44592439103871584, 0.4335573478601873, 0.7150880978442729, 0.4926765487762168, 0.4854634730145335, 0.47251384798437357, 0.4368902179412544, 1.0238092039944604, 0.7264143249485642, 0.6612832420505583, 0.6677867440739647, 0.6409009899944067, 0.878357098903507, 0.6000519579974934, 0.7770146379480138, 0.8676049881614745, 0.6691773609491065, 0.6288728477666155, 0.5491293431259692, 0.5476458870107308, 0.4570141539443284, 0.4083649601088837, 0.42559256101958454, 0.44404393900185823, 0.4645112990401685, 0.39585557219106704, 0.4126613800181076, 0.44381791399791837, 0.4568403080338612, 0.4481565711321309, 0.4865676090121269, 0.39750424213707447, 0.4311154020251706, 0.4871789908502251, 0.4279983659507707, 0.4299731251085177, 0.48080157407093793, 0.4276208730880171, 0.46814376197289675, 0.4564910540357232, 0.43193220696412027, 0.4353407099843025, 0.4433401939459145, 0.4145280730444938, 0.39384266699198633, 0.3915032959775999, 0.43841146980412304, 0.562366014928557, 0.45926920999772847, 0.43418653903063387, 0.4879247809294611, 0.43302645010408014, 0.4511674549430609, 0.41919727879576385, 0.44641706196125597, 0.4635943508474156, 0.5001852480927482]
Total Epoch List: [102, 53]
Total Time List: [0.07578808604739606, 0.10941758891567588]
========================k_idx:2========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
emb dim:  64
Model: Ethident
Ethident(
  (encoder): HGATE_encoder(
    (lin1): Linear(in_features=14887, out_features=64, bias=True)
    (account_convs): ModuleList(
      (0): GATEConv()
      (1): GATConv(64, 64, heads=1)
    )
    (subgraph_conv): GATConv(64, 64, heads=1)
    (lin2): Linear(in_features=64, out_features=64, bias=True)
    (lin3): Linear(in_features=64, out_features=2, bias=True)
  )
  (proj_head_g1): Project_Head(
    (block): Sequential(
      (0): Linear(in_features=64, out_features=64, bias=True)
      (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=64, bias=True)
      (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
      (5): ReLU()
    )
    (linear_shortcut): Linear(in_features=64, out_features=64, bias=True)
  )
  (proj_head_g2): Project_Head(
    (block): Sequential(
      (0): Linear(in_features=64, out_features=64, bias=True)
      (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=64, bias=True)
      (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
      (5): ReLU()
    )
    (linear_shortcut): Linear(in_features=64, out_features=64, bias=True)
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=64, bias=True)
    (1): ReLU()
    (2): Dropout(p=0.0, inplace=True)
    (3): Linear(in_features=64, out_features=2, bias=True)
  )
)
Total number of parameters: 1003972
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7720be307850>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 1.0337;  Loss pred: 1.0005; Loss self: 3.3187; time: 0.34s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 1.0589 score: 0.4898 time: 0.09s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 1.0365 score: 0.5000 time: 0.09s
Epoch 2/1000, LR 0.000000
Train loss: 1.0337;  Loss pred: 1.0005; Loss self: 3.3187; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.9923 score: 0.4898 time: 0.09s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.9699 score: 0.5000 time: 0.07s
Epoch 3/1000, LR 0.000030
Train loss: 0.9705;  Loss pred: 0.9371; Loss self: 3.3412; time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.9037 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.8733 score: 0.5000 time: 0.08s
Epoch 4/1000, LR 0.000060
Train loss: 0.8838;  Loss pred: 0.8501; Loss self: 3.3671; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.8336 score: 0.4898 time: 0.10s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7969 score: 0.5000 time: 0.08s
Epoch 5/1000, LR 0.000090
Train loss: 0.8137;  Loss pred: 0.7799; Loss self: 3.3858; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7881 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7517 score: 0.5000 time: 0.07s
Epoch 6/1000, LR 0.000120
Train loss: 0.7622;  Loss pred: 0.7281; Loss self: 3.4053; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7406 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7079 score: 0.5000 time: 0.08s
Epoch 7/1000, LR 0.000150
Train loss: 0.7096;  Loss pred: 0.6752; Loss self: 3.4418; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7006 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6742 score: 0.5000 time: 0.07s
Epoch 8/1000, LR 0.000180
Train loss: 0.6691;  Loss pred: 0.6344; Loss self: 3.4781; time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6661 score: 0.4898 time: 0.10s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6503 score: 0.5000 time: 0.09s
Epoch 9/1000, LR 0.000210
Train loss: 0.6389;  Loss pred: 0.6040; Loss self: 3.4843; time: 0.23s
Val loss: 0.6426 score: 0.5510 time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6327 score: 0.5000 time: 0.10s
Epoch 10/1000, LR 0.000240
Train loss: 0.6194;  Loss pred: 0.5847; Loss self: 3.4782; time: 0.23s
Val loss: 0.6213 score: 0.7551 time: 0.07s
Test loss: 0.6127 score: 0.7292 time: 0.08s
Epoch 11/1000, LR 0.000270
Train loss: 0.6010;  Loss pred: 0.5663; Loss self: 3.4717; time: 0.24s
Val loss: 0.6021 score: 0.7755 time: 0.07s
Test loss: 0.5880 score: 0.7708 time: 0.07s
Epoch 12/1000, LR 0.000270
Train loss: 0.5756;  Loss pred: 0.5409; Loss self: 3.4700; time: 0.23s
Val loss: 0.5874 score: 0.7755 time: 0.07s
Test loss: 0.5659 score: 0.8125 time: 0.08s
Epoch 13/1000, LR 0.000270
Train loss: 0.5447;  Loss pred: 0.5099; Loss self: 3.4785; time: 0.24s
Val loss: 0.5793 score: 0.7347 time: 0.08s
Test loss: 0.5559 score: 0.8125 time: 0.08s
Epoch 14/1000, LR 0.000270
Train loss: 0.5263;  Loss pred: 0.4915; Loss self: 3.4821; time: 0.24s
Val loss: 0.5621 score: 0.7755 time: 0.10s
Test loss: 0.5377 score: 0.8333 time: 0.09s
Epoch 15/1000, LR 0.000270
Train loss: 0.5030;  Loss pred: 0.4684; Loss self: 3.4635; time: 0.26s
Val loss: 0.5367 score: 0.7959 time: 0.09s
Test loss: 0.5101 score: 0.8542 time: 0.08s
Epoch 16/1000, LR 0.000270
Train loss: 0.4721;  Loss pred: 0.4378; Loss self: 3.4259; time: 0.22s
Val loss: 0.5167 score: 0.8367 time: 0.07s
Test loss: 0.4864 score: 0.8542 time: 0.12s
Epoch 17/1000, LR 0.000270
Train loss: 0.4488;  Loss pred: 0.4150; Loss self: 3.3870; time: 0.23s
Val loss: 0.5033 score: 0.8367 time: 0.07s
Test loss: 0.4690 score: 0.8542 time: 0.07s
Epoch 18/1000, LR 0.000270
Train loss: 0.4281;  Loss pred: 0.3944; Loss self: 3.3663; time: 0.24s
Val loss: 0.4951 score: 0.8367 time: 0.07s
Test loss: 0.4563 score: 0.8750 time: 0.11s
Epoch 19/1000, LR 0.000270
Train loss: 0.4028;  Loss pred: 0.3692; Loss self: 3.3603; time: 0.28s
Val loss: 0.4964 score: 0.8367 time: 0.08s
Test loss: 0.4522 score: 0.8542 time: 0.09s
     INFO: Early stopping counter 1 of 20
Epoch 20/1000, LR 0.000270
Train loss: 0.3828;  Loss pred: 0.3492; Loss self: 3.3592; time: 0.25s
Val loss: 0.4939 score: 0.8367 time: 0.08s
Test loss: 0.4441 score: 0.8542 time: 0.08s
Epoch 21/1000, LR 0.000270
Train loss: 0.3649;  Loss pred: 0.3314; Loss self: 3.3467; time: 0.26s
Val loss: 0.4808 score: 0.8367 time: 0.07s
Test loss: 0.4264 score: 0.8542 time: 0.08s
Epoch 22/1000, LR 0.000270
Train loss: 0.3440;  Loss pred: 0.3108; Loss self: 3.3192; time: 0.23s
Val loss: 0.4660 score: 0.8367 time: 0.07s
Test loss: 0.4093 score: 0.8750 time: 0.07s
Epoch 23/1000, LR 0.000270
Train loss: 0.3258;  Loss pred: 0.2929; Loss self: 3.2891; time: 0.23s
Val loss: 0.4563 score: 0.8367 time: 0.07s
Test loss: 0.3973 score: 0.8750 time: 0.09s
Epoch 24/1000, LR 0.000270
Train loss: 0.3085;  Loss pred: 0.2759; Loss self: 3.2650; time: 0.31s
Val loss: 0.4519 score: 0.8367 time: 0.11s
Test loss: 0.3899 score: 0.8750 time: 0.12s
Epoch 25/1000, LR 0.000270
Train loss: 0.2914;  Loss pred: 0.2589; Loss self: 3.2481; time: 0.28s
Val loss: 0.4522 score: 0.8367 time: 0.12s
Test loss: 0.3855 score: 0.8542 time: 0.08s
     INFO: Early stopping counter 1 of 20
Epoch 26/1000, LR 0.000270
Train loss: 0.2748;  Loss pred: 0.2424; Loss self: 3.2355; time: 0.38s
Val loss: 0.4532 score: 0.8367 time: 0.12s
Test loss: 0.3816 score: 0.8542 time: 0.12s
     INFO: Early stopping counter 2 of 20
Epoch 27/1000, LR 0.000270
Train loss: 0.2581;  Loss pred: 0.2258; Loss self: 3.2228; time: 0.25s
Val loss: 0.4515 score: 0.8367 time: 0.08s
Test loss: 0.3741 score: 0.8542 time: 0.08s
Epoch 28/1000, LR 0.000270
Train loss: 0.2412;  Loss pred: 0.2092; Loss self: 3.2082; time: 0.26s
Val loss: 0.4481 score: 0.8571 time: 0.07s
Test loss: 0.3640 score: 0.8542 time: 0.07s
Epoch 29/1000, LR 0.000270
Train loss: 0.2248;  Loss pred: 0.1929; Loss self: 3.1922; time: 0.31s
Val loss: 0.4443 score: 0.8571 time: 0.07s
Test loss: 0.3540 score: 0.8542 time: 0.09s
Epoch 30/1000, LR 0.000270
Train loss: 0.2095;  Loss pred: 0.1778; Loss self: 3.1773; time: 0.25s
Val loss: 0.4412 score: 0.8571 time: 0.10s
Test loss: 0.3453 score: 0.8542 time: 0.07s
Epoch 31/1000, LR 0.000270
Train loss: 0.1950;  Loss pred: 0.1634; Loss self: 3.1647; time: 0.26s
Val loss: 0.4394 score: 0.8571 time: 0.07s
Test loss: 0.3385 score: 0.8542 time: 0.07s
Epoch 32/1000, LR 0.000270
Train loss: 0.1815;  Loss pred: 0.1500; Loss self: 3.1554; time: 0.38s
Val loss: 0.4394 score: 0.8571 time: 0.09s
Test loss: 0.3344 score: 0.8542 time: 0.07s
     INFO: Early stopping counter 1 of 20
Epoch 33/1000, LR 0.000270
Train loss: 0.1699;  Loss pred: 0.1384; Loss self: 3.1493; time: 0.22s
Val loss: 0.4417 score: 0.8571 time: 0.07s
Test loss: 0.3320 score: 0.8542 time: 0.07s
     INFO: Early stopping counter 2 of 20
Epoch 34/1000, LR 0.000270
Train loss: 0.1591;  Loss pred: 0.1277; Loss self: 3.1458; time: 0.23s
Val loss: 0.4436 score: 0.8367 time: 0.08s
Test loss: 0.3295 score: 0.8542 time: 0.07s
     INFO: Early stopping counter 3 of 20
Epoch 35/1000, LR 0.000270
Train loss: 0.1492;  Loss pred: 0.1177; Loss self: 3.1443; time: 0.22s
Val loss: 0.4434 score: 0.8367 time: 0.07s
Test loss: 0.3254 score: 0.8542 time: 0.07s
     INFO: Early stopping counter 4 of 20
Epoch 36/1000, LR 0.000270
Train loss: 0.1399;  Loss pred: 0.1085; Loss self: 3.1429; time: 0.22s
Val loss: 0.4425 score: 0.8367 time: 0.08s
Test loss: 0.3202 score: 0.8542 time: 0.07s
     INFO: Early stopping counter 5 of 20
Epoch 37/1000, LR 0.000270
Train loss: 0.1311;  Loss pred: 0.0997; Loss self: 3.1410; time: 0.23s
Val loss: 0.4429 score: 0.8367 time: 0.08s
Test loss: 0.3157 score: 0.8542 time: 0.07s
     INFO: Early stopping counter 6 of 20
Epoch 38/1000, LR 0.000270
Train loss: 0.1227;  Loss pred: 0.0913; Loss self: 3.1393; time: 0.23s
Val loss: 0.4450 score: 0.8163 time: 0.08s
Test loss: 0.3125 score: 0.8542 time: 0.08s
     INFO: Early stopping counter 7 of 20
Epoch 39/1000, LR 0.000269
Train loss: 0.1151;  Loss pred: 0.0837; Loss self: 3.1380; time: 0.24s
Val loss: 0.4487 score: 0.8163 time: 0.07s
Test loss: 0.3103 score: 0.8542 time: 0.08s
     INFO: Early stopping counter 8 of 20
Epoch 40/1000, LR 0.000269
Train loss: 0.1085;  Loss pred: 0.0771; Loss self: 3.1366; time: 0.26s
Val loss: 0.4552 score: 0.8163 time: 0.08s
Test loss: 0.3104 score: 0.8542 time: 0.08s
     INFO: Early stopping counter 9 of 20
Epoch 41/1000, LR 0.000269
Train loss: 0.1027;  Loss pred: 0.0714; Loss self: 3.1352; time: 0.27s
Val loss: 0.4646 score: 0.8163 time: 0.08s
Test loss: 0.3130 score: 0.8542 time: 0.07s
     INFO: Early stopping counter 10 of 20
Epoch 42/1000, LR 0.000269
Train loss: 0.0975;  Loss pred: 0.0662; Loss self: 3.1336; time: 0.23s
Val loss: 0.4764 score: 0.8163 time: 0.17s
Test loss: 0.3178 score: 0.8542 time: 0.12s
     INFO: Early stopping counter 11 of 20
Epoch 43/1000, LR 0.000269
Train loss: 0.0929;  Loss pred: 0.0616; Loss self: 3.1318; time: 0.23s
Val loss: 0.4899 score: 0.8163 time: 0.09s
Test loss: 0.3239 score: 0.8542 time: 0.10s
     INFO: Early stopping counter 12 of 20
Epoch 44/1000, LR 0.000269
Train loss: 0.0887;  Loss pred: 0.0574; Loss self: 3.1298; time: 0.24s
Val loss: 0.5034 score: 0.7959 time: 0.07s
Test loss: 0.3299 score: 0.8542 time: 0.11s
     INFO: Early stopping counter 13 of 20
Epoch 45/1000, LR 0.000269
Train loss: 0.0849;  Loss pred: 0.0536; Loss self: 3.1275; time: 0.25s
Val loss: 0.5144 score: 0.7959 time: 0.10s
Test loss: 0.3338 score: 0.8542 time: 0.10s
     INFO: Early stopping counter 14 of 20
Epoch 46/1000, LR 0.000269
Train loss: 0.0812;  Loss pred: 0.0500; Loss self: 3.1243; time: 0.25s
Val loss: 0.5226 score: 0.7959 time: 0.11s
Test loss: 0.3351 score: 0.8750 time: 0.09s
     INFO: Early stopping counter 15 of 20
Epoch 47/1000, LR 0.000269
Train loss: 0.0778;  Loss pred: 0.0466; Loss self: 3.1205; time: 0.29s
Val loss: 0.5286 score: 0.7959 time: 0.11s
Test loss: 0.3345 score: 0.8750 time: 0.09s
     INFO: Early stopping counter 16 of 20
Epoch 48/1000, LR 0.000269
Train loss: 0.0747;  Loss pred: 0.0436; Loss self: 3.1165; time: 0.24s
Val loss: 0.5343 score: 0.7959 time: 0.10s
Test loss: 0.3341 score: 0.8750 time: 0.07s
     INFO: Early stopping counter 17 of 20
Epoch 49/1000, LR 0.000269
Train loss: 0.0719;  Loss pred: 0.0408; Loss self: 3.1126; time: 0.25s
Val loss: 0.5413 score: 0.7959 time: 0.07s
Test loss: 0.3347 score: 0.8750 time: 0.07s
     INFO: Early stopping counter 18 of 20
Epoch 50/1000, LR 0.000269
Train loss: 0.0694;  Loss pred: 0.0383; Loss self: 3.1090; time: 0.25s
Val loss: 0.5485 score: 0.7959 time: 0.09s
Test loss: 0.3358 score: 0.8750 time: 0.09s
     INFO: Early stopping counter 19 of 20
Epoch 51/1000, LR 0.000269
Train loss: 0.0672;  Loss pred: 0.0361; Loss self: 3.1058; time: 0.25s
Val loss: 0.5565 score: 0.7755 time: 0.08s
Test loss: 0.3380 score: 0.8750 time: 0.07s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 030,   Train_Loss: 0.1950,   Val_Loss: 0.4394,   Val_Precision: 1.0000,   Val_Recall: 0.7200,   Val_accuracy: 0.8372,   Val_Score: 0.8571,   Val_Loss: 0.4394,   Test_Precision: 1.0000,   Test_Recall: 0.7083,   Test_accuracy: 0.8293,   Test_Score: 0.8542,   Test_loss: 0.3385


[0.08933310594875365, 0.1325164180016145, 0.1264158149715513, 0.13019937300123274, 0.1420114409411326, 0.11638450401369482, 0.09018263500183821, 0.07496050407644361, 0.08110358798876405, 0.08063250395935029, 0.07832609792239964, 0.07626940705813468, 0.07174002705141902, 0.11831858905497938, 0.07016485708300024, 0.07907497009728104, 0.09813908394426107, 0.07781477901153266, 0.12308060703799129, 0.08350804005749524, 0.12056038901209831, 0.07937410799786448, 0.0900027829920873, 0.0734898999799043, 0.07265223795548081, 0.07268549501895905, 0.17077319789677858, 0.12430463801138103, 0.08541036304086447, 0.08295715996064246, 0.08090639393776655, 0.08556310599669814, 0.08601629897020757, 0.13505018805153668, 0.10933043097611517, 0.08225851599127054, 0.0975670350017026, 0.0767545480048284, 0.07178926491178572, 0.12037488399073482, 0.08885487203951925, 0.07253755698911846, 0.08581735903862864, 0.07108733791392297, 0.11462151899468154, 0.07273176498711109, 0.09300167101901025, 0.08020563295576721, 0.09041774098295718, 0.14073097496293485, 0.07177895703352988, 0.1120324389776215, 0.18368258501868695, 0.1470815520733595, 0.10402169800363481, 0.09542304696515203, 0.08583799400366843, 0.07775228505488485, 0.2269453089684248, 0.10264266096055508, 0.08991861494723707, 0.09063434798736125, 0.07988115807529539, 0.08415011002216488, 0.09618233994115144, 0.09811547095887363, 0.08750325499568135, 0.0850659339921549, 0.08745480398647487, 0.1363767309812829, 0.09993352298624814, 0.12023776699788868, 0.08381867699790746, 0.1467019469710067, 0.07873806799761951, 0.0753886109450832, 0.08313272998202592, 0.09734643204137683, 0.13083777797874063, 0.09782692999579012, 0.10459077905397862, 0.08333449403289706, 0.0838375820312649, 0.09055756591260433, 0.08289130195043981, 0.12105746904853731, 0.09133819001726806, 0.08121033501811326, 0.08209170203190297, 0.07718457607552409, 0.09204084100201726, 0.07979959796648473, 0.09504616796039045, 0.0779495439492166, 0.08889693394303322, 0.11686386191286147, 0.0723454199032858, 0.07841611793264747, 0.07892258698120713, 0.11904883000534028, 0.15723856794647872, 0.07504100108053535, 0.1121549770468846, 0.08840202505234629, 0.10531830601394176, 0.29692155797965825, 0.12062632804736495, 0.09650054294615984, 0.11894398799631745, 0.20026547997258604, 0.4353803519625217, 0.09577900299336761, 0.28215465997345746, 0.08345463301520795, 0.11137664597481489, 0.10720696300268173, 0.19953610107768327, 0.0913637870689854, 0.09168238402344286, 0.0916319020325318, 0.09639635507483035, 0.09347816300578415, 0.1060151590500027, 0.0869493440259248, 0.09169552091043442, 0.1342890589730814, 0.12619544903282076, 0.11114300496410578, 0.1059510309714824, 0.08655079291202128, 0.10460378997959197, 0.10380979406181723, 0.0864687270950526, 0.09469805296976119, 0.09341621596831828, 0.08727203798480332, 0.10309809201862663, 0.08892002596985549, 0.11173332901671529, 0.11029266403056681, 0.08702832390554249, 0.0842467830516398, 0.08752005803398788, 0.10063336091116071, 0.08362018002662808, 0.09525998192839324, 0.09745450189802796, 0.09095453505869955, 0.1305154140572995, 0.10023071500472724, 0.08528496697545052, 0.08379478997085243, 0.0997695840196684, 0.13467627193313092, 0.10875356395263225, 0.0982953489292413, 0.07723073405213654, 0.08924566605128348, 0.08161674707662314, 0.07710375299211591, 0.0806156430626288, 0.07730594300664961, 0.08980809105560184, 0.10190830298233777, 0.08294946805108339, 0.07941306103020906, 0.08408980502281338, 0.08096062100958079, 0.09407247300259769, 0.08577909995801747, 0.12104697397444397, 0.07158605102449656, 0.11872089200187474, 0.0915534709347412, 0.08277287299279124, 0.08412958600092679, 0.07329689699690789, 0.09324821794871241, 0.1273870940785855, 0.08360437897499651, 0.12557672709226608, 0.08544400904793292, 0.07884876802563667, 0.09392449201550335, 0.0754996610339731, 0.07918574602808803, 0.07684028905350715, 0.07523780898191035, 0.07382104999851435, 0.07601425796747208, 0.07804326596669853, 0.07694874599110335, 0.08545246091671288, 0.08290914702229202, 0.08317459502723068, 0.0791546159889549, 0.12470076105091721, 0.10452253604307771, 0.1175901690730825, 0.104976732051, 0.09103384602349252, 0.09561256296001375, 0.07565346907358617, 0.07869045808911324, 0.09020478301681578, 0.0764589550672099]
[0.0018231246111990542, 0.0027044166939105, 0.002579914591256149, 0.002657130061249648, 0.0028981926722680125, 0.00237519395946316, 0.0018404619388130245, 0.0015298062056417064, 0.0016551752650768173, 0.001645561305292863, 0.0015984917943346866, 0.0015565185113905035, 0.001464082184722837, 0.0024146650827546814, 0.0014319358588367396, 0.001613774899944511, 0.0020028384478420628, 0.0015880567145210747, 0.002511849123224312, 0.0017042457154590864, 0.002460416102287721, 0.001619879755058459, 0.0018367914896344347, 0.0014997938771409039, 0.0014826987337853228, 0.0014833774493665111, 0.0034851673040158897, 0.0025368293471710415, 0.00174306863348703, 0.0016930032645029072, 0.0016511508966891133, 0.001746185836667309, 0.001755434672861379, 0.0027561262867660547, 0.00223123328522684, 0.0016787452243116436, 0.0019911639796265836, 0.0015664193470373141, 0.001465087039016035, 0.0024566302855252003, 0.0018133647355003928, 0.0014803583059003766, 0.0017513746742577273, 0.0014507619982433257, 0.0023392146733608476, 0.0014843217344308387, 0.00189799328610225, 0.0016368496521585146, 0.0018452600200603508, 0.0028720607135292826, 0.001464876674153671, 0.002286376305665745, 0.003748624184054836, 0.0030016643280277445, 0.002122891795992547, 0.0019474091217377965, 0.0017517957959932331, 0.0015867813276507112, 0.004631536917722955, 0.002094748182868471, 0.0018350737744334098, 0.0018496805711706377, 0.0016302277158223549, 0.001717349184125814, 0.0019629048967581926, 0.0020023565501810945, 0.0017857807141975785, 0.0017360394692276509, 0.0017847919180913239, 0.002783198591454753, 0.0020394596527805744, 0.0024538319795487486, 0.0017105852448552543, 0.0029939172851225857, 0.001606899346890194, 0.0015385430805119021, 0.0016965863261637943, 0.0019866618783954456, 0.0026701587342600128, 0.001996467959097758, 0.0021345056949791555, 0.001700703959855042, 0.001710971061862549, 0.0018481135900531495, 0.0016916592234783635, 0.002470560592827292, 0.0018640446942299604, 0.0016573537758798624, 0.001675340857793938, 0.0015751954301127366, 0.0018783845102452502, 0.0016285632238058107, 0.001939717713477356, 0.0015908070193717675, 0.0018142231416945554, 0.002384976773731867, 0.0014764371408833837, 0.0016003289374009687, 0.0016106650404327987, 0.002429567959292659, 0.003208950366254668, 0.0015314490016435786, 0.0022888770825894816, 0.001804122960251965, 0.0021493531839579952, 0.0060596236322379236, 0.0024617617968849988, 0.0019693988356359153, 0.0024274283264554583, 0.004087050611685429, 0.008885313305357585, 0.00195467353047689, 0.005758258366805254, 0.0017031557758205704, 0.0022729927749962223, 0.0021878972041363617, 0.004072165328115985, 0.0018645670830405184, 0.0018710690617029155, 0.0018700388169904448, 0.0019672725525475584, 0.001907717612362942, 0.002163574674489851, 0.0017744764086923429, 0.0018713371614374372, 0.0027405930402669676, 0.002575417327200424, 0.0022682245911041996, 0.002162265938193518, 0.0017663427124902302, 0.0021347712240733056, 0.002118567225751372, 0.0017646678998990326, 0.0019326133259134938, 0.0019064533871085364, 0.0017810619996898637, 0.0021040426942576865, 0.0018146944075480712, 0.0022802720207492915, 0.0022508706945013634, 0.001776088242970255, 0.0017193221030946896, 0.0017861236333466914, 0.002053742059411443, 0.0017065342862577159, 0.0019440812638447601, 0.00198886738567404, 0.00185621500119795, 0.0026635798787203978, 0.0020455247960148417, 0.001740509530111235, 0.0017100977545071925, 0.0020361139595850694, 0.0027484953455741003, 0.0022194604888292296, 0.0020478197693591937, 0.001608973626086178, 0.001859284709401739, 0.0017003488974296488, 0.0016063281873357482, 0.0016794925638047669, 0.0016105404793052003, 0.0018710018969917048, 0.0021230896454653703, 0.001728113917730904, 0.001654438771462689, 0.0017518709379752788, 0.0016866796043662664, 0.0019598431875541187, 0.0017870645824586973, 0.002521811957800916, 0.001491376063010345, 0.002473351916705724, 0.0019073639778071083, 0.001724434854016484, 0.0017526997083526414, 0.0015270186874355811, 0.0019426712072648418, 0.0026538977933038646, 0.0017417578953124273, 0.00261618181442221, 0.001780083521831936, 0.001642682667200764, 0.0019567602503229864, 0.0015729096048744395, 0.001649703042251834, 0.0016008393552813989, 0.001567454353789799, 0.0015379385416357156, 0.001583630374322335, 0.0016259013743062194, 0.001603098874814653, 0.0017802596024315183, 0.0017272738962977503, 0.0017328040630673058, 0.0016490544997698937, 0.0025979325218941085, 0.0021775528342307857, 0.0024497951890225522, 0.0021870152510625, 0.0018965384588227607, 0.0019919283950002864, 0.0015761139390330452, 0.0016393845435231924, 0.0018792663128503289, 0.0015928948972335395]
[548.5088588334662, 369.76550331599674, 387.60973072100984, 376.34589837491814, 345.0426224483685, 421.018248221724, 543.3418528855497, 653.6775679900782, 604.1656259004026, 607.6953783390212, 625.5896987048427, 642.4594328188605, 683.0217664244773, 414.13610820892274, 698.3553025987973, 619.6651094488981, 499.291393710481, 629.7004325198671, 398.1130836060564, 586.7698483435072, 406.4353176156625, 617.3297720878743, 544.4276095807825, 666.758289416627, 674.4458447381315, 674.1372537563237, 286.93027128072725, 394.1928538138189, 573.7008748757574, 590.6663152794428, 605.6381654791209, 572.6767329120906, 569.6594783387685, 362.82807678358097, 448.1826291410556, 595.6830050909257, 502.2188078088559, 638.3986522455655, 682.5533045952058, 407.06165917278474, 551.4610383796025, 675.5121351460818, 570.980050527351, 689.2929379256302, 427.49389843868335, 673.7083859945288, 526.8722536177229, 610.9296591054037, 541.929044757223, 348.18205453991504, 682.6513232438134, 437.37332193390665, 266.7645383748001, 333.1485105321733, 471.0555676402033, 513.5027811247164, 570.8427901740796, 630.2065587578707, 215.91105021173817, 477.3843501468691, 544.9372193816873, 540.6338886757693, 613.4112371507304, 582.2927621496103, 509.44903222338263, 499.41155580386487, 559.9791688025589, 576.0237700384148, 560.2894039711985, 359.29883087405165, 490.32595405190403, 407.5258649876658, 584.595244819043, 334.01056367495977, 622.3165140587577, 649.9655503096353, 589.4188728145251, 503.3569178906597, 374.50957022490735, 500.88457239850686, 468.4925424899208, 587.9918102179489, 584.4634209718358, 541.0922820881595, 591.1356058720949, 404.76643353871646, 536.4678234891259, 603.3714796161224, 596.8934592312062, 634.8418620846495, 532.3723628180023, 614.0381812522371, 515.5389328312559, 628.6117598317578, 551.2001126090591, 419.2912949987604, 677.30618006648, 624.8715352382872, 620.8615540145405, 411.5958132289243, 311.62837871099634, 652.9763635137585, 436.8954574304476, 554.2859450446433, 465.2562489327687, 165.02675094866953, 406.21314428770256, 507.7691638205432, 411.9586103125873, 244.67521814896665, 112.54527168974775, 511.5943836186423, 173.66362123740743, 587.1453534649265, 439.9486047647741, 457.05986465426025, 245.56959735784028, 536.3175232983931, 534.45381598575, 534.7482581187028, 508.31797490644107, 524.1865952903677, 462.1980520435653, 563.546517215704, 534.3772467126481, 364.88452875242933, 388.28658541605677, 440.8734496230762, 462.47780272368243, 566.1415493883275, 468.43427001602737, 472.01711979913193, 566.6788635171615, 517.4340808849215, 524.5341988228055, 561.4627678172517, 475.27552683658996, 551.0569690635419, 438.5441696870019, 444.2725219369078, 563.0350879006114, 581.6245822699844, 559.8716580029135, 486.91606398058474, 585.9829527321803, 514.3817897932545, 502.79873218449586, 538.7306962580453, 375.434582604073, 488.872098714341, 574.5443978902491, 584.7618929177385, 491.13164579638044, 363.83543512645895, 450.55994690290805, 488.3242241151531, 621.5142273229774, 537.8412434326798, 588.1145931353624, 622.5377901502167, 595.4179384602774, 620.9095721899569, 534.4730016617581, 471.0116702494704, 578.6655554010281, 604.4345776035582, 570.8183053460251, 592.8808277584695, 510.24490446503455, 559.5768668998911, 396.5402721271991, 670.5216912101288, 404.30963068608025, 524.2837820339344, 579.9001323076023, 570.5483918519607, 654.8708330998642, 514.7551455235375, 376.80426221504547, 574.1326063118694, 382.2364311560098, 561.7713931596147, 608.7603040848198, 511.0488113374841, 635.7644437423515, 606.1697010845094, 624.6722987543106, 637.977110837196, 650.2210413014445, 631.4604823287244, 615.0434557733893, 623.7918419820599, 561.7158298903023, 578.9469765874458, 577.099293171012, 606.4080963603922, 384.92146796442466, 459.231107636131, 408.1973891045935, 457.24418223154964, 527.2764152754015, 502.0260781009933, 634.4718965010269, 609.9850117233053, 532.1225592998982, 627.7878105685128]
Elapsed: 0.1004312199764225~0.0385049546810233
Time per graph: 0.002058931799896975~0.0007836941725568079
Speed: 521.0594124911503~108.39649647090468
Total Time: 0.0772
best val loss: 0.43937018513679504 test_score: 0.8542

Testing...
Test loss: 0.3640 score: 0.8542 time: 0.09s
test Score 0.8542
Epoch Time List: [0.6323932181112468, 0.6680102088721469, 0.6248668601037934, 0.707784247933887, 0.6146137118339539, 0.5233469029190019, 0.4700630169827491, 0.38775763497687876, 0.40351178497076035, 0.43041884200647473, 0.39908200805075467, 0.41717656096443534, 0.39050029998179525, 0.42516208498273045, 0.4522315108915791, 0.44691944611258805, 0.4820707079488784, 0.4478492389898747, 0.5212276788661256, 0.46283576590940356, 0.5410454230150208, 0.46803126798477024, 0.4857000260381028, 0.46595987293403596, 0.39901139587163925, 0.38638271600939333, 0.4983892139280215, 0.5542896059341729, 0.4128830189583823, 0.39792199607472867, 0.45601770700886846, 0.459048523218371, 0.42182964796666056, 0.4919677859870717, 0.45818865194451064, 0.44737890595570207, 0.5353815431008115, 0.43681768502574414, 0.3777776740025729, 0.42287518398370594, 0.5247964310692623, 0.4265494759893045, 0.43062754289712757, 0.40848523390013725, 0.4517485399264842, 0.42963896587025374, 0.45687062200158834, 0.45278055395465344, 0.4442500490695238, 0.44560337718576193, 0.4039720119908452, 0.4085355040151626, 0.6372330249287188, 0.5926093668676913, 0.49481567298062146, 0.4748591829556972, 0.430659320903942, 0.4093341180123389, 0.5899813709547743, 0.450815471005626, 0.44627472409047186, 0.44786899513565004, 0.4731918869074434, 0.4241071371361613, 0.4604278140468523, 0.4803494669031352, 0.508032709825784, 0.7298657129285857, 0.6363277399213985, 0.5554584908531979, 0.4855876970104873, 0.5974915359402075, 0.47849214693997055, 0.48500467906706035, 0.43545747990719974, 0.40171133109834045, 0.41983152413740754, 0.4176981010241434, 0.4996301910141483, 0.4611607249826193, 0.468943067942746, 0.4629820939153433, 0.35490988788660616, 0.41935662494506687, 0.4469986920012161, 0.4688408998772502, 0.45922025199979544, 0.4173444659681991, 0.4332691120216623, 0.45427088905125856, 0.4426882689585909, 0.44747231586370617, 0.43406858993694186, 0.42428670497611165, 0.47548094496596605, 0.44880914490204304, 0.4014227361185476, 0.4163578209700063, 0.44592439103871584, 0.4335573478601873, 0.7150880978442729, 0.4926765487762168, 0.4854634730145335, 0.47251384798437357, 0.4368902179412544, 1.0238092039944604, 0.7264143249485642, 0.6612832420505583, 0.6677867440739647, 0.6409009899944067, 0.878357098903507, 0.6000519579974934, 0.7770146379480138, 0.8676049881614745, 0.6691773609491065, 0.6288728477666155, 0.5491293431259692, 0.5476458870107308, 0.4570141539443284, 0.4083649601088837, 0.42559256101958454, 0.44404393900185823, 0.4645112990401685, 0.39585557219106704, 0.4126613800181076, 0.44381791399791837, 0.4568403080338612, 0.4481565711321309, 0.4865676090121269, 0.39750424213707447, 0.4311154020251706, 0.4871789908502251, 0.4279983659507707, 0.4299731251085177, 0.48080157407093793, 0.4276208730880171, 0.46814376197289675, 0.4564910540357232, 0.43193220696412027, 0.4353407099843025, 0.4433401939459145, 0.4145280730444938, 0.39384266699198633, 0.3915032959775999, 0.43841146980412304, 0.562366014928557, 0.45926920999772847, 0.43418653903063387, 0.4879247809294611, 0.43302645010408014, 0.4511674549430609, 0.41919727879576385, 0.44641706196125597, 0.4635943508474156, 0.5001852480927482, 0.5156920059816912, 0.41242555307690054, 0.4402398969978094, 0.43639581883326173, 0.40705646097194403, 0.39750818302854896, 0.38085360592231154, 0.4063356571132317, 0.5878944478463382, 0.37604592111893, 0.38678431999869645, 0.38355436793062836, 0.39540967787615955, 0.4333452439168468, 0.4221438351087272, 0.406972995842807, 0.3608782780356705, 0.4223848560359329, 0.44319884991273284, 0.40715853590518236, 0.4106967051047832, 0.3710846829926595, 0.38634253991767764, 0.5392864550231025, 0.4821926678996533, 0.6201366860186681, 0.4131074979668483, 0.405778840999119, 0.47233220108319074, 0.4210662520490587, 0.40833633905276656, 0.5394134728703648, 0.36673638701904565, 0.37910081807058305, 0.3596642480697483, 0.37916060897987336, 0.3741179871140048, 0.38598716992419213, 0.3950666729360819, 0.41815980104729533, 0.42840822297148407, 0.5157312139635906, 0.42282808502204716, 0.4265662190737203, 0.4480004500364885, 0.45320030814036727, 0.490007336018607, 0.407818224048242, 0.3933778409846127, 0.4232377919834107, 0.40304549399297684]
Total Epoch List: [102, 53, 51]
Total Time List: [0.07578808604739606, 0.10941758891567588, 0.07718520506750792]
========================training times:1========================
========================k_idx:0========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
emb dim:  64
Model: Ethident
Ethident(
  (encoder): HGATE_encoder(
    (lin1): Linear(in_features=14887, out_features=64, bias=True)
    (account_convs): ModuleList(
      (0): GATEConv()
      (1): GATConv(64, 64, heads=1)
    )
    (subgraph_conv): GATConv(64, 64, heads=1)
    (lin2): Linear(in_features=64, out_features=64, bias=True)
    (lin3): Linear(in_features=64, out_features=2, bias=True)
  )
  (proj_head_g1): Project_Head(
    (block): Sequential(
      (0): Linear(in_features=64, out_features=64, bias=True)
      (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=64, bias=True)
      (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
      (5): ReLU()
    )
    (linear_shortcut): Linear(in_features=64, out_features=64, bias=True)
  )
  (proj_head_g2): Project_Head(
    (block): Sequential(
      (0): Linear(in_features=64, out_features=64, bias=True)
      (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=64, bias=True)
      (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
      (5): ReLU()
    )
    (linear_shortcut): Linear(in_features=64, out_features=64, bias=True)
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=64, bias=True)
    (1): ReLU()
    (2): Dropout(p=0.0, inplace=True)
    (3): Linear(in_features=64, out_features=2, bias=True)
  )
)
Total number of parameters: 1003972
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7720be304430>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.8015;  Loss pred: 0.7668; Loss self: 3.4634; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7477 score: 0.5102 time: 0.09s
Test loss: 0.7596 score: 0.5102 time: 0.09s
Epoch 2/1000, LR 0.000000
Train loss: 0.8015;  Loss pred: 0.7668; Loss self: 3.4634; time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6898 score: 0.5102 time: 0.08s
Test loss: 0.6973 score: 0.5102 time: 0.17s
Epoch 3/1000, LR 0.000030
Train loss: 0.7386;  Loss pred: 0.7044; Loss self: 3.4225; time: 0.27s
Val loss: 0.6142 score: 0.7755 time: 0.08s
Test loss: 0.6135 score: 0.6531 time: 0.15s
Epoch 4/1000, LR 0.000060
Train loss: 0.6638;  Loss pred: 0.6302; Loss self: 3.3587; time: 0.29s
Val loss: 0.6067 score: 0.6939 time: 0.11s
Test loss: 0.5935 score: 0.7347 time: 0.09s
Epoch 5/1000, LR 0.000090
Train loss: 0.6428;  Loss pred: 0.6098; Loss self: 3.3007; time: 0.23s
Val loss: 0.6454 score: 0.6122 time: 0.08s
Test loss: 0.6176 score: 0.6735 time: 0.10s
     INFO: Early stopping counter 1 of 20
Epoch 6/1000, LR 0.000120
Train loss: 0.6561;  Loss pred: 0.6232; Loss self: 3.2876; time: 0.23s
Val loss: 0.6621 score: 0.6327 time: 0.09s
Test loss: 0.6377 score: 0.6327 time: 0.07s
     INFO: Early stopping counter 2 of 20
Epoch 7/1000, LR 0.000150
Train loss: 0.6691;  Loss pred: 0.6358; Loss self: 3.3305; time: 0.26s
Val loss: 0.6546 score: 0.6122 time: 0.09s
Test loss: 0.6335 score: 0.6327 time: 0.08s
     INFO: Early stopping counter 3 of 20
Epoch 8/1000, LR 0.000180
Train loss: 0.6585;  Loss pred: 0.6246; Loss self: 3.3897; time: 0.25s
Val loss: 0.6332 score: 0.6531 time: 0.10s
Test loss: 0.6103 score: 0.6531 time: 0.09s
     INFO: Early stopping counter 4 of 20
Epoch 9/1000, LR 0.000210
Train loss: 0.6333;  Loss pred: 0.5988; Loss self: 3.4467; time: 0.25s
Val loss: 0.6102 score: 0.7143 time: 0.11s
Test loss: 0.5961 score: 0.7551 time: 0.10s
     INFO: Early stopping counter 5 of 20
Epoch 10/1000, LR 0.000240
Train loss: 0.6166;  Loss pred: 0.5816; Loss self: 3.4914; time: 0.24s
Val loss: 0.5994 score: 0.7755 time: 0.08s
Test loss: 0.5920 score: 0.8367 time: 0.07s
Epoch 11/1000, LR 0.000270
Train loss: 0.6073;  Loss pred: 0.5721; Loss self: 3.5181; time: 0.22s
Val loss: 0.5879 score: 0.7959 time: 0.09s
Test loss: 0.5844 score: 0.8776 time: 0.07s
Epoch 12/1000, LR 0.000270
Train loss: 0.5957;  Loss pred: 0.5605; Loss self: 3.5270; time: 0.27s
Val loss: 0.5745 score: 0.7959 time: 0.08s
Test loss: 0.5704 score: 0.8571 time: 0.14s
Epoch 13/1000, LR 0.000270
Train loss: 0.5781;  Loss pred: 0.5428; Loss self: 3.5223; time: 0.34s
Val loss: 0.5609 score: 0.7551 time: 0.28s
Test loss: 0.5547 score: 0.8163 time: 0.18s
Epoch 14/1000, LR 0.000270
Train loss: 0.5588;  Loss pred: 0.5237; Loss self: 3.5068; time: 0.56s
Val loss: 0.5498 score: 0.7551 time: 0.14s
Test loss: 0.5431 score: 0.7755 time: 0.08s
Epoch 15/1000, LR 0.000270
Train loss: 0.5457;  Loss pred: 0.5108; Loss self: 3.4873; time: 0.27s
Val loss: 0.5373 score: 0.7551 time: 0.10s
Test loss: 0.5329 score: 0.7959 time: 0.07s
Epoch 16/1000, LR 0.000270
Train loss: 0.5309;  Loss pred: 0.4962; Loss self: 3.4704; time: 0.24s
Val loss: 0.5225 score: 0.7959 time: 0.07s
Test loss: 0.5227 score: 0.8367 time: 0.08s
Epoch 17/1000, LR 0.000270
Train loss: 0.5127;  Loss pred: 0.4782; Loss self: 3.4580; time: 0.30s
Val loss: 0.5110 score: 0.8367 time: 0.19s
Test loss: 0.5149 score: 0.8571 time: 0.09s
Epoch 18/1000, LR 0.000270
Train loss: 0.4968;  Loss pred: 0.4623; Loss self: 3.4469; time: 0.27s
Val loss: 0.4976 score: 0.8367 time: 0.11s
Test loss: 0.5039 score: 0.8367 time: 0.10s
Epoch 19/1000, LR 0.000270
Train loss: 0.4789;  Loss pred: 0.4446; Loss self: 3.4260; time: 0.28s
Val loss: 0.4774 score: 0.8367 time: 0.12s
Test loss: 0.4859 score: 0.8571 time: 0.07s
Epoch 20/1000, LR 0.000270
Train loss: 0.4554;  Loss pred: 0.4215; Loss self: 3.3904; time: 0.30s
Val loss: 0.4564 score: 0.8367 time: 0.08s
Test loss: 0.4653 score: 0.8571 time: 0.08s
Epoch 21/1000, LR 0.000270
Train loss: 0.4310;  Loss pred: 0.3976; Loss self: 3.3442; time: 0.28s
Val loss: 0.4389 score: 0.8367 time: 0.08s
Test loss: 0.4489 score: 0.8571 time: 0.07s
Epoch 22/1000, LR 0.000270
Train loss: 0.4106;  Loss pred: 0.3776; Loss self: 3.2979; time: 0.83s
Val loss: 0.4202 score: 0.8571 time: 0.15s
Test loss: 0.4331 score: 0.8776 time: 0.14s
Epoch 23/1000, LR 0.000270
Train loss: 0.3879;  Loss pred: 0.3553; Loss self: 3.2545; time: 0.29s
Val loss: 0.4014 score: 0.8776 time: 0.09s
Test loss: 0.4172 score: 0.8776 time: 0.19s
Epoch 24/1000, LR 0.000270
Train loss: 0.3620;  Loss pred: 0.3299; Loss self: 3.2131; time: 0.26s
Val loss: 0.3828 score: 0.8571 time: 0.16s
Test loss: 0.4022 score: 0.8776 time: 0.38s
Epoch 25/1000, LR 0.000270
Train loss: 0.3354;  Loss pred: 0.3037; Loss self: 3.1697; time: 0.23s
Val loss: 0.3623 score: 0.8776 time: 0.12s
Test loss: 0.3873 score: 0.8776 time: 0.08s
Epoch 26/1000, LR 0.000270
Train loss: 0.3081;  Loss pred: 0.2769; Loss self: 3.1169; time: 0.26s
Val loss: 0.3400 score: 0.8776 time: 0.14s
Test loss: 0.3720 score: 0.8980 time: 0.07s
Epoch 27/1000, LR 0.000270
Train loss: 0.2811;  Loss pred: 0.2505; Loss self: 3.0567; time: 0.28s
Val loss: 0.3183 score: 0.8776 time: 0.08s
Test loss: 0.3565 score: 0.8776 time: 0.07s
Epoch 28/1000, LR 0.000270
Train loss: 0.2602;  Loss pred: 0.2302; Loss self: 2.9975; time: 0.23s
Val loss: 0.3007 score: 0.8776 time: 0.10s
Test loss: 0.3431 score: 0.8776 time: 0.08s
Epoch 29/1000, LR 0.000270
Train loss: 0.2435;  Loss pred: 0.2140; Loss self: 2.9478; time: 0.31s
Val loss: 0.2866 score: 0.8776 time: 0.09s
Test loss: 0.3335 score: 0.8776 time: 0.08s
Epoch 30/1000, LR 0.000270
Train loss: 0.2285;  Loss pred: 0.1994; Loss self: 2.9111; time: 0.31s
Val loss: 0.2762 score: 0.8776 time: 0.09s
Test loss: 0.3263 score: 0.8776 time: 0.06s
Epoch 31/1000, LR 0.000270
Train loss: 0.2140;  Loss pred: 0.1852; Loss self: 2.8862; time: 0.29s
Val loss: 0.2699 score: 0.8776 time: 0.13s
Test loss: 0.3232 score: 0.8980 time: 0.11s
Epoch 32/1000, LR 0.000270
Train loss: 0.2007;  Loss pred: 0.1720; Loss self: 2.8708; time: 0.23s
Val loss: 0.2677 score: 0.8776 time: 0.13s
Test loss: 0.3234 score: 0.8776 time: 0.15s
Epoch 33/1000, LR 0.000270
Train loss: 0.1892;  Loss pred: 0.1605; Loss self: 2.8652; time: 0.24s
Val loss: 0.2669 score: 0.8776 time: 0.08s
Test loss: 0.3262 score: 0.8776 time: 0.07s
Epoch 34/1000, LR 0.000270
Train loss: 0.1797;  Loss pred: 0.1510; Loss self: 2.8652; time: 0.24s
Val loss: 0.2649 score: 0.8776 time: 0.15s
Test loss: 0.3266 score: 0.8980 time: 0.10s
Epoch 35/1000, LR 0.000270
Train loss: 0.1715;  Loss pred: 0.1428; Loss self: 2.8660; time: 0.23s
Val loss: 0.2617 score: 0.8776 time: 0.09s
Test loss: 0.3256 score: 0.8980 time: 0.07s
Epoch 36/1000, LR 0.000270
Train loss: 0.1644;  Loss pred: 0.1358; Loss self: 2.8666; time: 0.26s
Val loss: 0.2583 score: 0.8776 time: 0.10s
Test loss: 0.3235 score: 0.8776 time: 0.08s
Epoch 37/1000, LR 0.000270
Train loss: 0.1581;  Loss pred: 0.1294; Loss self: 2.8690; time: 0.24s
Val loss: 0.2534 score: 0.8776 time: 0.09s
Test loss: 0.3197 score: 0.8776 time: 0.07s
Epoch 38/1000, LR 0.000270
Train loss: 0.1516;  Loss pred: 0.1229; Loss self: 2.8700; time: 0.22s
Val loss: 0.2483 score: 0.8776 time: 0.09s
Test loss: 0.3153 score: 0.8776 time: 0.07s
Epoch 39/1000, LR 0.000269
Train loss: 0.1456;  Loss pred: 0.1169; Loss self: 2.8696; time: 0.23s
Val loss: 0.2435 score: 0.8776 time: 0.10s
Test loss: 0.3106 score: 0.8776 time: 0.08s
Epoch 40/1000, LR 0.000269
Train loss: 0.1400;  Loss pred: 0.1113; Loss self: 2.8680; time: 0.24s
Val loss: 0.2405 score: 0.8776 time: 0.09s
Test loss: 0.3075 score: 0.8980 time: 0.08s
Epoch 41/1000, LR 0.000269
Train loss: 0.1348;  Loss pred: 0.1062; Loss self: 2.8654; time: 0.25s
Val loss: 0.2387 score: 0.8776 time: 0.09s
Test loss: 0.3049 score: 0.9184 time: 0.07s
Epoch 42/1000, LR 0.000269
Train loss: 0.1300;  Loss pred: 0.1014; Loss self: 2.8611; time: 0.25s
Val loss: 0.2367 score: 0.8776 time: 0.16s
Test loss: 0.3025 score: 0.9184 time: 0.08s
Epoch 43/1000, LR 0.000269
Train loss: 0.1256;  Loss pred: 0.0971; Loss self: 2.8562; time: 0.25s
Val loss: 0.2353 score: 0.8776 time: 0.07s
Test loss: 0.3012 score: 0.9184 time: 0.07s
Epoch 44/1000, LR 0.000269
Train loss: 0.1216;  Loss pred: 0.0931; Loss self: 2.8511; time: 0.22s
Val loss: 0.2337 score: 0.8776 time: 0.08s
Test loss: 0.3003 score: 0.9184 time: 0.10s
Epoch 45/1000, LR 0.000269
Train loss: 0.1175;  Loss pred: 0.0890; Loss self: 2.8461; time: 0.34s
Val loss: 0.2327 score: 0.8776 time: 0.08s
Test loss: 0.3003 score: 0.9388 time: 0.07s
Epoch 46/1000, LR 0.000269
Train loss: 0.1136;  Loss pred: 0.0852; Loss self: 2.8417; time: 0.26s
Val loss: 0.2318 score: 0.8980 time: 0.08s
Test loss: 0.3003 score: 0.9388 time: 0.08s
Epoch 47/1000, LR 0.000269
Train loss: 0.1096;  Loss pred: 0.0812; Loss self: 2.8385; time: 0.30s
Val loss: 0.2320 score: 0.8980 time: 0.12s
Test loss: 0.3004 score: 0.9184 time: 0.06s
     INFO: Early stopping counter 1 of 20
Epoch 48/1000, LR 0.000269
Train loss: 0.1055;  Loss pred: 0.0771; Loss self: 2.8358; time: 0.26s
Val loss: 0.2320 score: 0.9184 time: 0.09s
Test loss: 0.3007 score: 0.9184 time: 0.08s
     INFO: Early stopping counter 2 of 20
Epoch 49/1000, LR 0.000269
Train loss: 0.1017;  Loss pred: 0.0734; Loss self: 2.8337; time: 0.26s
Val loss: 0.2323 score: 0.9184 time: 0.13s
Test loss: 0.3009 score: 0.9184 time: 0.07s
     INFO: Early stopping counter 3 of 20
Epoch 50/1000, LR 0.000269
Train loss: 0.0983;  Loss pred: 0.0700; Loss self: 2.8317; time: 0.28s
Val loss: 0.2319 score: 0.9184 time: 0.08s
Test loss: 0.3010 score: 0.9184 time: 0.08s
     INFO: Early stopping counter 4 of 20
Epoch 51/1000, LR 0.000269
Train loss: 0.0951;  Loss pred: 0.0668; Loss self: 2.8301; time: 0.24s
Val loss: 0.2311 score: 0.9388 time: 0.10s
Test loss: 0.3016 score: 0.9184 time: 0.07s
Epoch 52/1000, LR 0.000269
Train loss: 0.0921;  Loss pred: 0.0638; Loss self: 2.8288; time: 0.26s
Val loss: 0.2307 score: 0.9388 time: 0.09s
Test loss: 0.3029 score: 0.9184 time: 0.08s
Epoch 53/1000, LR 0.000269
Train loss: 0.0892;  Loss pred: 0.0610; Loss self: 2.8272; time: 0.24s
Val loss: 0.2296 score: 0.9388 time: 0.12s
Test loss: 0.3034 score: 0.9184 time: 0.09s
Epoch 54/1000, LR 0.000269
Train loss: 0.0869;  Loss pred: 0.0586; Loss self: 2.8258; time: 0.23s
Val loss: 0.2278 score: 0.9388 time: 0.10s
Test loss: 0.3033 score: 0.9184 time: 0.10s
Epoch 55/1000, LR 0.000269
Train loss: 0.0845;  Loss pred: 0.0563; Loss self: 2.8244; time: 0.28s
Val loss: 0.2262 score: 0.9388 time: 0.13s
Test loss: 0.3029 score: 0.9388 time: 0.07s
Epoch 56/1000, LR 0.000269
Train loss: 0.0822;  Loss pred: 0.0540; Loss self: 2.8228; time: 0.22s
Val loss: 0.2246 score: 0.9388 time: 0.07s
Test loss: 0.3026 score: 0.9388 time: 0.07s
Epoch 57/1000, LR 0.000269
Train loss: 0.0799;  Loss pred: 0.0516; Loss self: 2.8215; time: 0.24s
Val loss: 0.2231 score: 0.9388 time: 0.09s
Test loss: 0.3023 score: 0.9388 time: 0.07s
Epoch 58/1000, LR 0.000269
Train loss: 0.0775;  Loss pred: 0.0492; Loss self: 2.8205; time: 0.23s
Val loss: 0.2225 score: 0.9388 time: 0.08s
Test loss: 0.3030 score: 0.9388 time: 0.07s
Epoch 59/1000, LR 0.000268
Train loss: 0.0752;  Loss pred: 0.0470; Loss self: 2.8202; time: 0.23s
Val loss: 0.2230 score: 0.9388 time: 0.09s
Test loss: 0.3060 score: 0.9388 time: 0.08s
     INFO: Early stopping counter 1 of 20
Epoch 60/1000, LR 0.000268
Train loss: 0.0729;  Loss pred: 0.0447; Loss self: 2.8213; time: 0.25s
Val loss: 0.2242 score: 0.9388 time: 0.10s
Test loss: 0.3109 score: 0.9184 time: 0.09s
     INFO: Early stopping counter 2 of 20
Epoch 61/1000, LR 0.000268
Train loss: 0.0707;  Loss pred: 0.0424; Loss self: 2.8235; time: 0.26s
Val loss: 0.2260 score: 0.9388 time: 0.13s
Test loss: 0.3166 score: 0.8980 time: 0.09s
     INFO: Early stopping counter 3 of 20
Epoch 62/1000, LR 0.000268
Train loss: 0.0685;  Loss pred: 0.0403; Loss self: 2.8262; time: 0.26s
Val loss: 0.2276 score: 0.9388 time: 0.10s
Test loss: 0.3223 score: 0.8980 time: 0.08s
     INFO: Early stopping counter 4 of 20
Epoch 63/1000, LR 0.000268
Train loss: 0.0665;  Loss pred: 0.0382; Loss self: 2.8295; time: 0.25s
Val loss: 0.2282 score: 0.9388 time: 0.08s
Test loss: 0.3252 score: 0.8980 time: 0.07s
     INFO: Early stopping counter 5 of 20
Epoch 64/1000, LR 0.000268
Train loss: 0.0645;  Loss pred: 0.0361; Loss self: 2.8332; time: 0.22s
Val loss: 0.2278 score: 0.9388 time: 0.08s
Test loss: 0.3247 score: 0.8980 time: 0.07s
     INFO: Early stopping counter 6 of 20
Epoch 65/1000, LR 0.000268
Train loss: 0.0625;  Loss pred: 0.0341; Loss self: 2.8370; time: 0.22s
Val loss: 0.2265 score: 0.9388 time: 0.10s
Test loss: 0.3205 score: 0.8980 time: 0.12s
     INFO: Early stopping counter 7 of 20
Epoch 66/1000, LR 0.000268
Train loss: 0.0604;  Loss pred: 0.0320; Loss self: 2.8407; time: 0.33s
Val loss: 0.2245 score: 0.9388 time: 0.08s
Test loss: 0.3135 score: 0.8980 time: 0.07s
     INFO: Early stopping counter 8 of 20
Epoch 67/1000, LR 0.000268
Train loss: 0.0586;  Loss pred: 0.0301; Loss self: 2.8442; time: 0.25s
Val loss: 0.2221 score: 0.9388 time: 0.09s
Test loss: 0.3057 score: 0.9184 time: 0.08s
Epoch 68/1000, LR 0.000268
Train loss: 0.0567;  Loss pred: 0.0283; Loss self: 2.8477; time: 0.23s
Val loss: 0.2199 score: 0.9388 time: 0.09s
Test loss: 0.2977 score: 0.9388 time: 0.08s
Epoch 69/1000, LR 0.000268
Train loss: 0.0551;  Loss pred: 0.0266; Loss self: 2.8516; time: 0.23s
Val loss: 0.2178 score: 0.9388 time: 0.08s
Test loss: 0.2906 score: 0.9388 time: 0.07s
Epoch 70/1000, LR 0.000268
Train loss: 0.0534;  Loss pred: 0.0249; Loss self: 2.8559; time: 0.24s
Val loss: 0.2159 score: 0.9388 time: 0.08s
Test loss: 0.2846 score: 0.9388 time: 0.07s
Epoch 71/1000, LR 0.000268
Train loss: 0.0518;  Loss pred: 0.0232; Loss self: 2.8603; time: 0.22s
Val loss: 0.2140 score: 0.9388 time: 0.08s
Test loss: 0.2798 score: 0.9388 time: 0.12s
Epoch 72/1000, LR 0.000267
Train loss: 0.0501;  Loss pred: 0.0215; Loss self: 2.8654; time: 0.25s
Val loss: 0.2127 score: 0.9388 time: 0.08s
Test loss: 0.2761 score: 0.9388 time: 0.09s
Epoch 73/1000, LR 0.000267
Train loss: 0.0487;  Loss pred: 0.0200; Loss self: 2.8709; time: 0.25s
Val loss: 0.2116 score: 0.9388 time: 0.12s
Test loss: 0.2735 score: 0.9388 time: 0.09s
Epoch 74/1000, LR 0.000267
Train loss: 0.0474;  Loss pred: 0.0186; Loss self: 2.8765; time: 0.28s
Val loss: 0.2108 score: 0.9388 time: 0.08s
Test loss: 0.2720 score: 0.9388 time: 0.09s
Epoch 75/1000, LR 0.000267
Train loss: 0.0462;  Loss pred: 0.0174; Loss self: 2.8822; time: 0.23s
Val loss: 0.2102 score: 0.9388 time: 0.10s
Test loss: 0.2714 score: 0.9388 time: 0.07s
Epoch 76/1000, LR 0.000267
Train loss: 0.0451;  Loss pred: 0.0162; Loss self: 2.8880; time: 0.26s
Val loss: 0.2100 score: 0.9388 time: 0.09s
Test loss: 0.2719 score: 0.9388 time: 0.07s
Epoch 77/1000, LR 0.000267
Train loss: 0.0441;  Loss pred: 0.0152; Loss self: 2.8934; time: 0.25s
Val loss: 0.2100 score: 0.9388 time: 0.11s
Test loss: 0.2731 score: 0.9388 time: 0.08s
     INFO: Early stopping counter 1 of 20
Epoch 78/1000, LR 0.000267
Train loss: 0.0433;  Loss pred: 0.0143; Loss self: 2.8985; time: 0.24s
Val loss: 0.2101 score: 0.9388 time: 0.10s
Test loss: 0.2749 score: 0.9388 time: 0.06s
     INFO: Early stopping counter 2 of 20
Epoch 79/1000, LR 0.000267
Train loss: 0.0425;  Loss pred: 0.0134; Loss self: 2.9036; time: 0.27s
Val loss: 0.2106 score: 0.9388 time: 0.09s
Test loss: 0.2772 score: 0.9388 time: 0.08s
     INFO: Early stopping counter 3 of 20
Epoch 80/1000, LR 0.000267
Train loss: 0.0417;  Loss pred: 0.0127; Loss self: 2.9085; time: 0.25s
Val loss: 0.2115 score: 0.9388 time: 0.10s
Test loss: 0.2797 score: 0.9388 time: 0.08s
     INFO: Early stopping counter 4 of 20
Epoch 81/1000, LR 0.000267
Train loss: 0.0411;  Loss pred: 0.0119; Loss self: 2.9135; time: 0.26s
Val loss: 0.2126 score: 0.9388 time: 0.10s
Test loss: 0.2823 score: 0.9388 time: 0.09s
     INFO: Early stopping counter 5 of 20
Epoch 82/1000, LR 0.000267
Train loss: 0.0405;  Loss pred: 0.0113; Loss self: 2.9180; time: 0.24s
Val loss: 0.2139 score: 0.9388 time: 0.14s
Test loss: 0.2849 score: 0.9388 time: 0.07s
     INFO: Early stopping counter 6 of 20
Epoch 83/1000, LR 0.000266
Train loss: 0.0399;  Loss pred: 0.0107; Loss self: 2.9220; time: 0.30s
Val loss: 0.2152 score: 0.9388 time: 0.11s
Test loss: 0.2876 score: 0.9388 time: 0.07s
     INFO: Early stopping counter 7 of 20
Epoch 84/1000, LR 0.000266
Train loss: 0.0395;  Loss pred: 0.0102; Loss self: 2.9255; time: 0.24s
Val loss: 0.2167 score: 0.9388 time: 0.08s
Test loss: 0.2902 score: 0.9388 time: 0.07s
     INFO: Early stopping counter 8 of 20
Epoch 85/1000, LR 0.000266
Train loss: 0.0390;  Loss pred: 0.0097; Loss self: 2.9285; time: 0.22s
Val loss: 0.2181 score: 0.9388 time: 0.08s
Test loss: 0.2928 score: 0.9388 time: 0.07s
     INFO: Early stopping counter 9 of 20
Epoch 86/1000, LR 0.000266
Train loss: 0.0386;  Loss pred: 0.0093; Loss self: 2.9311; time: 0.23s
Val loss: 0.2197 score: 0.9388 time: 0.10s
Test loss: 0.2952 score: 0.9388 time: 0.07s
     INFO: Early stopping counter 10 of 20
Epoch 87/1000, LR 0.000266
Train loss: 0.0382;  Loss pred: 0.0088; Loss self: 2.9333; time: 0.24s
Val loss: 0.2214 score: 0.9388 time: 0.09s
Test loss: 0.2973 score: 0.9388 time: 0.07s
     INFO: Early stopping counter 11 of 20
Epoch 88/1000, LR 0.000266
Train loss: 0.0378;  Loss pred: 0.0084; Loss self: 2.9351; time: 0.25s
Val loss: 0.2234 score: 0.9388 time: 0.07s
Test loss: 0.2992 score: 0.9184 time: 0.10s
     INFO: Early stopping counter 12 of 20
Epoch 89/1000, LR 0.000266
Train loss: 0.0374;  Loss pred: 0.0081; Loss self: 2.9365; time: 0.29s
Val loss: 0.2252 score: 0.9388 time: 0.14s
Test loss: 0.3009 score: 0.9184 time: 0.09s
     INFO: Early stopping counter 13 of 20
Epoch 90/1000, LR 0.000266
Train loss: 0.0371;  Loss pred: 0.0077; Loss self: 2.9376; time: 0.27s
Val loss: 0.2270 score: 0.9388 time: 0.09s
Test loss: 0.3025 score: 0.9184 time: 0.08s
     INFO: Early stopping counter 14 of 20
Epoch 91/1000, LR 0.000266
Train loss: 0.0368;  Loss pred: 0.0074; Loss self: 2.9384; time: 0.24s
Val loss: 0.2288 score: 0.9388 time: 0.13s
Test loss: 0.3042 score: 0.9184 time: 0.07s
     INFO: Early stopping counter 15 of 20
Epoch 92/1000, LR 0.000266
Train loss: 0.0365;  Loss pred: 0.0072; Loss self: 2.9388; time: 0.25s
Val loss: 0.2306 score: 0.9388 time: 0.08s
Test loss: 0.3059 score: 0.9184 time: 0.07s
     INFO: Early stopping counter 16 of 20
Epoch 93/1000, LR 0.000265
Train loss: 0.0363;  Loss pred: 0.0069; Loss self: 2.9390; time: 0.23s
Val loss: 0.2323 score: 0.9388 time: 0.12s
Test loss: 0.3075 score: 0.9184 time: 0.11s
     INFO: Early stopping counter 17 of 20
Epoch 94/1000, LR 0.000265
Train loss: 0.0360;  Loss pred: 0.0066; Loss self: 2.9389; time: 0.25s
Val loss: 0.2341 score: 0.9388 time: 0.09s
Test loss: 0.3090 score: 0.9388 time: 0.08s
     INFO: Early stopping counter 18 of 20
Epoch 95/1000, LR 0.000265
Train loss: 0.0358;  Loss pred: 0.0064; Loss self: 2.9386; time: 0.24s
Val loss: 0.2359 score: 0.9388 time: 0.09s
Test loss: 0.3102 score: 0.9388 time: 0.07s
     INFO: Early stopping counter 19 of 20
Epoch 96/1000, LR 0.000265
Train loss: 0.0356;  Loss pred: 0.0062; Loss self: 2.9381; time: 0.32s
Val loss: 0.2377 score: 0.9388 time: 0.08s
Test loss: 0.3110 score: 0.9388 time: 0.07s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 075,   Train_Loss: 0.0451,   Val_Loss: 0.2100,   Val_Precision: 0.9565,   Val_Recall: 0.9167,   Val_accuracy: 0.9362,   Val_Score: 0.9388,   Val_Loss: 0.2100,   Test_Precision: 1.0000,   Test_Recall: 0.8800,   Test_accuracy: 0.9362,   Test_Score: 0.9388,   Test_loss: 0.2719


[0.09452154708560556, 0.1711661390727386, 0.15822971099987626, 0.09641384205315262, 0.1071156068937853, 0.07705299998633564, 0.08886201400309801, 0.09051300003193319, 0.10040262597613037, 0.0753729670541361, 0.07291406102012843, 0.14193040190730244, 0.18352665309794247, 0.08279371110256761, 0.07200683397240937, 0.08261185593437403, 0.08992896904237568, 0.10324136191047728, 0.07358087401371449, 0.0865716258995235, 0.07187003002036363, 0.14318371401168406, 0.19200806692242622, 0.3863587260711938, 0.0799661329947412, 0.07960947195533663, 0.07038232695776969, 0.0853387441020459, 0.08042071596719325, 0.06710024701897055, 0.1112815709784627, 0.15592345397453755, 0.07359106105286628, 0.10698551603127271, 0.07796810695435852, 0.07994347100611776, 0.07355987397022545, 0.07318234292324632, 0.0850401169154793, 0.08050350507255644, 0.07813307899050415, 0.08994403993710876, 0.07911842397879809, 0.10378090501762927, 0.07355005096178502, 0.08544690196868032, 0.06728954706341028, 0.08620715199504048, 0.08130906103178859, 0.08239514497108757, 0.07803700503427535, 0.08633449499029666, 0.09254518698435277, 0.10405872599221766, 0.07200848404318094, 0.07614572194870561, 0.07885271089617163, 0.07325105904601514, 0.08571272203698754, 0.09646163799334317, 0.09229166898876429, 0.0803795619867742, 0.07794898096472025, 0.07072622899431735, 0.12156251003034413, 0.07670781807973981, 0.08280532306525856, 0.08020552096422762, 0.07431093603372574, 0.07378465193323791, 0.12139850005041808, 0.09806859993841499, 0.0943294009193778, 0.09574046405032277, 0.07118210103362799, 0.08132307697087526, 0.08765971101820469, 0.06715698703192174, 0.084098229999654, 0.081328280037269, 0.09823667991440743, 0.07623107498511672, 0.0783383579691872, 0.07018355396576226, 0.07331530808005482, 0.07373277400620282, 0.07790054194629192, 0.10264716402161866, 0.10021455306559801, 0.0807296799030155, 0.07996112911496311, 0.07204930298030376, 0.11939669901039451, 0.08157269400544465, 0.07729757903143764, 0.07567717903293669]
[0.0019290111650123584, 0.003493186511688543, 0.0032291777755076786, 0.001967629429656176, 0.00218603279375072, 0.0015725102038027681, 0.001813510489859143, 0.0018472040822843509, 0.002049033183186334, 0.001538223817431349, 0.0014880420616352741, 0.0028965388144347438, 0.0037454418999580096, 0.001689667573521788, 0.0014695272239267218, 0.0016859562435586537, 0.001835285082497463, 0.0021069665696015773, 0.0015016504900758059, 0.0017667678755004794, 0.0014667353065380333, 0.002922116612483348, 0.003918531978008699, 0.007884871960636608, 0.0016319618978518614, 0.001624683101129319, 0.0014363740195463203, 0.0017416070224907324, 0.0016412391013712908, 0.0013693927963055214, 0.0022710524689482183, 0.003182111305602807, 0.0015018583888340058, 0.002183377878189239, 0.0015911858562113984, 0.0016314994082881175, 0.0015012219177597032, 0.0014935172025152311, 0.0017355125901118225, 0.0016429286749501313, 0.0015945526324592683, 0.0018355926517777297, 0.0016146617138530224, 0.0021179776534210053, 0.0015010214481996943, 0.0017438143258914351, 0.0013732560625185771, 0.0017593296325518464, 0.0016593685924854813, 0.0016815335708385219, 0.0015925919394750073, 0.0017619284691897277, 0.0018886772853949545, 0.002123647469228932, 0.0014695608988404274, 0.001553994325483788, 0.0016092389978810536, 0.001494919572367656, 0.0017492392252446438, 0.0019686048570070037, 0.0018835034487502916, 0.0016403992242198817, 0.00159079552989225, 0.0014433924284554562, 0.002480867551639676, 0.0015654656750967307, 0.0016899045523522155, 0.0016368473666168901, 0.0015165497149739946, 0.0015058092231273043, 0.0024775204091922057, 0.002001399998743163, 0.0019250898146811798, 0.0019538870214351584, 0.0014526959394617956, 0.0016596546320586788, 0.0017889736942490752, 0.001370550755753505, 0.001716290408156204, 0.0016597608170871223, 0.0020048302023348455, 0.0015557362241860554, 0.0015987419993711673, 0.001432317427872699, 0.001496230777143976, 0.0015047504899225064, 0.0015898069784957536, 0.0020948400820738505, 0.0020451949605224083, 0.0016475444878166427, 0.00163185977785639, 0.0014703939383735461, 0.0024366673267427453, 0.0016647488572539724, 0.0015775016128864823, 0.0015444322251619734]
[518.4003172908506, 286.2715737204133, 309.6763540194946, 508.22577916754386, 457.4496790984705, 635.9259212320029, 551.4167166894474, 541.3586996642769, 488.03504413967437, 650.1004526570661, 672.0240144966443, 345.23963394398663, 266.9911926844229, 591.8323909807251, 680.4909658821435, 593.1352037282043, 544.8744772878534, 474.6159784533733, 665.933921780639, 566.0053105259942, 681.7862742803413, 342.2176910147861, 255.1976111493099, 126.82514123149592, 612.7594040745021, 615.5046478324905, 696.1974989744319, 574.1823425642057, 609.2957443948773, 730.2506648916917, 440.32448112620017, 314.2567635014149, 665.8417380991344, 458.00592283610536, 628.4620970557095, 612.9331061475955, 666.1240341416782, 669.5604163888443, 576.1986433849886, 608.6691499436843, 627.1351472780841, 544.7831788994812, 619.3247733692327, 472.1485131747152, 666.212998621297, 573.4555480778045, 728.1963118851858, 568.3983157548109, 602.6388618710399, 594.6952337688596, 627.9072342471146, 567.5599307728298, 529.4710789042412, 470.88794844234985, 680.4753724660616, 643.5029932871093, 621.4117364274282, 668.9323081215656, 571.677095715792, 507.973957516474, 530.9254945423658, 609.6077011226131, 628.6162999639139, 692.812280489845, 403.08479964562054, 638.7875607289885, 591.7493970935092, 610.9305121508336, 659.3915056831141, 664.0947502786399, 403.6293692232588, 499.6502451423898, 519.456283220538, 511.80031855960914, 688.3752978413959, 602.5349977540652, 558.9797117837172, 729.6336861674396, 582.6519773389, 602.496449913186, 498.79535874678555, 642.782487451039, 625.4917931682096, 698.1692609055355, 668.3460969228373, 664.5620032670661, 629.0071773028584, 477.3634076210819, 488.9509407673134, 606.9638831575462, 612.7977498860836, 680.0898547678554, 410.3966056526749, 600.691206750257, 633.9137734193622, 647.487137154966]
Elapsed: 0.0931671757704559~0.03866944548639445
Time per graph: 0.0019013709340909366~0.0007891723568651929
Speed: 567.0250303610792~115.47764242239055
Total Time: 0.0763
best val loss: 0.21001726388931274 test_score: 0.9388

Testing...
Test loss: 0.3016 score: 0.9184 time: 0.07s
test Score 0.9184
Epoch Time List: [0.4212449110345915, 0.5271857319166884, 0.5045939111150801, 0.492961282026954, 0.40879769902676344, 0.3912614620057866, 0.43015367176849395, 0.4443825220223516, 0.45640196604654193, 0.3873873061966151, 0.3741483569610864, 0.4820109030697495, 0.7949949179310352, 0.7813072571298108, 0.43553718796465546, 0.3867250708863139, 0.5803953430149704, 0.4752067410154268, 0.4688927868846804, 0.4607895689550787, 0.4367142770206556, 1.1154394641052932, 0.5654596409294754, 0.810825151973404, 0.4323563320795074, 0.478389129973948, 0.421190754044801, 0.4155838399892673, 0.4811278688721359, 0.46429302205797285, 0.5240831370465457, 0.5193607191322371, 0.38791009690612555, 0.49677234806586057, 0.3966254881815985, 0.4279297968605533, 0.4073607779573649, 0.3760427579982206, 0.4039058299968019, 0.4024028111016378, 0.41380055306944996, 0.4881413849070668, 0.40292614803183824, 0.40139093180187047, 0.48272958991583437, 0.42867816297803074, 0.4844732191413641, 0.43657715409062803, 0.46886783197987825, 0.44167822611052543, 0.4157691851723939, 0.42949652683455497, 0.44942378299310803, 0.4286258270731196, 0.48536610195878893, 0.36718045512679964, 0.40361273498274386, 0.37861317687202245, 0.39824283588677645, 0.43770532903727144, 0.4698431669967249, 0.43789663014467806, 0.4032576299505308, 0.3689220598898828, 0.43919198599178344, 0.4830825360259041, 0.4170082969358191, 0.4025996328564361, 0.38329677504952997, 0.38652255409397185, 0.4208958549425006, 0.4271953519200906, 0.46432478399947286, 0.4538966630352661, 0.3930676350137219, 0.4250095688039437, 0.44373951805755496, 0.4056091670645401, 0.4341758170630783, 0.4251074221683666, 0.44669476989656687, 0.4555058589903638, 0.48123304988257587, 0.3837094767950475, 0.36716674198396504, 0.3946649630088359, 0.4124348451150581, 0.4253365299664438, 0.5234826599480584, 0.4363730320474133, 0.44618152698967606, 0.3956343671306968, 0.4638734880136326, 0.414568395819515, 0.40346646995749325, 0.4714216369902715]
Total Epoch List: [96]
Total Time List: [0.07627350999973714]
========================k_idx:1========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
emb dim:  64
Model: Ethident
Ethident(
  (encoder): HGATE_encoder(
    (lin1): Linear(in_features=14887, out_features=64, bias=True)
    (account_convs): ModuleList(
      (0): GATEConv()
      (1): GATConv(64, 64, heads=1)
    )
    (subgraph_conv): GATConv(64, 64, heads=1)
    (lin2): Linear(in_features=64, out_features=64, bias=True)
    (lin3): Linear(in_features=64, out_features=2, bias=True)
  )
  (proj_head_g1): Project_Head(
    (block): Sequential(
      (0): Linear(in_features=64, out_features=64, bias=True)
      (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=64, bias=True)
      (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
      (5): ReLU()
    )
    (linear_shortcut): Linear(in_features=64, out_features=64, bias=True)
  )
  (proj_head_g2): Project_Head(
    (block): Sequential(
      (0): Linear(in_features=64, out_features=64, bias=True)
      (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=64, bias=True)
      (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
      (5): ReLU()
    )
    (linear_shortcut): Linear(in_features=64, out_features=64, bias=True)
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=64, bias=True)
    (1): ReLU()
    (2): Dropout(p=0.0, inplace=True)
    (3): Linear(in_features=64, out_features=2, bias=True)
  )
)
Total number of parameters: 1003972
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7720be3045e0>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.5833;  Loss pred: 0.5511; Loss self: 3.2161; time: 0.35s
Val loss: 0.7055 score: 0.6327 time: 0.09s
Test loss: 0.5722 score: 0.7143 time: 0.11s
Epoch 2/1000, LR 0.000000
Train loss: 0.5833;  Loss pred: 0.5511; Loss self: 3.2161; time: 0.26s
Val loss: 0.6805 score: 0.6531 time: 0.07s
Test loss: 0.5474 score: 0.7551 time: 0.10s
Epoch 3/1000, LR 0.000030
Train loss: 0.5543;  Loss pred: 0.5225; Loss self: 3.1759; time: 0.26s
Val loss: 0.6721 score: 0.5918 time: 0.09s
Test loss: 0.5416 score: 0.7347 time: 0.09s
Epoch 4/1000, LR 0.000060
Train loss: 0.5529;  Loss pred: 0.5216; Loss self: 3.1308; time: 0.30s
Val loss: 0.6673 score: 0.6122 time: 0.08s
Test loss: 0.5341 score: 0.7347 time: 0.14s
Epoch 5/1000, LR 0.000090
Train loss: 0.5396;  Loss pred: 0.5087; Loss self: 3.0897; time: 0.25s
Val loss: 0.6449 score: 0.7347 time: 0.10s
Test loss: 0.5026 score: 0.7347 time: 0.09s
Epoch 6/1000, LR 0.000120
Train loss: 0.4803;  Loss pred: 0.4499; Loss self: 3.0384; time: 0.25s
Val loss: 0.6590 score: 0.7551 time: 0.08s
Test loss: 0.4897 score: 0.8163 time: 0.08s
     INFO: Early stopping counter 1 of 20
Epoch 7/1000, LR 0.000150
Train loss: 0.4579;  Loss pred: 0.4280; Loss self: 2.9982; time: 0.23s
Val loss: 0.6571 score: 0.7551 time: 0.07s
Test loss: 0.4661 score: 0.8571 time: 0.09s
     INFO: Early stopping counter 2 of 20
Epoch 8/1000, LR 0.000180
Train loss: 0.4207;  Loss pred: 0.3912; Loss self: 2.9512; time: 0.43s
Val loss: 0.6115 score: 0.7143 time: 0.08s
Test loss: 0.4363 score: 0.7959 time: 0.08s
Epoch 9/1000, LR 0.000210
Train loss: 0.3828;  Loss pred: 0.3538; Loss self: 2.8942; time: 0.23s
Val loss: 0.5956 score: 0.7143 time: 0.11s
Test loss: 0.4201 score: 0.7959 time: 0.11s
Epoch 10/1000, LR 0.000240
Train loss: 0.3499;  Loss pred: 0.3211; Loss self: 2.8886; time: 0.22s
Val loss: 0.6264 score: 0.7755 time: 0.07s
Test loss: 0.4064 score: 0.8776 time: 0.09s
     INFO: Early stopping counter 1 of 20
Epoch 11/1000, LR 0.000270
Train loss: 0.3044;  Loss pred: 0.2748; Loss self: 2.9632; time: 0.25s
Val loss: 0.5968 score: 0.7755 time: 0.08s
Test loss: 0.3898 score: 0.8776 time: 0.09s
     INFO: Early stopping counter 2 of 20
Epoch 12/1000, LR 0.000270
Train loss: 0.2757;  Loss pred: 0.2459; Loss self: 2.9739; time: 0.26s
Val loss: 0.5290 score: 0.7755 time: 0.08s
Test loss: 0.3529 score: 0.8776 time: 0.09s
Epoch 13/1000, LR 0.000270
Train loss: 0.2511;  Loss pred: 0.2216; Loss self: 2.9417; time: 0.27s
Val loss: 0.5104 score: 0.7959 time: 0.08s
Test loss: 0.3352 score: 0.8776 time: 0.08s
Epoch 14/1000, LR 0.000270
Train loss: 0.2336;  Loss pred: 0.2043; Loss self: 2.9334; time: 0.24s
Val loss: 0.5233 score: 0.7755 time: 0.07s
Test loss: 0.3270 score: 0.8980 time: 0.08s
     INFO: Early stopping counter 1 of 20
Epoch 15/1000, LR 0.000270
Train loss: 0.2178;  Loss pred: 0.1885; Loss self: 2.9374; time: 0.26s
Val loss: 0.5223 score: 0.7755 time: 0.09s
Test loss: 0.3174 score: 0.8980 time: 0.08s
     INFO: Early stopping counter 2 of 20
Epoch 16/1000, LR 0.000270
Train loss: 0.2016;  Loss pred: 0.1722; Loss self: 2.9320; time: 0.31s
Val loss: 0.5040 score: 0.7959 time: 0.14s
Test loss: 0.3000 score: 0.8980 time: 0.09s
Epoch 17/1000, LR 0.000270
Train loss: 0.1829;  Loss pred: 0.1538; Loss self: 2.9134; time: 0.24s
Val loss: 0.4979 score: 0.8163 time: 0.09s
Test loss: 0.2867 score: 0.8980 time: 0.08s
Epoch 18/1000, LR 0.000270
Train loss: 0.1681;  Loss pred: 0.1390; Loss self: 2.9040; time: 0.25s
Val loss: 0.5058 score: 0.8163 time: 0.07s
Test loss: 0.2758 score: 0.8980 time: 0.08s
     INFO: Early stopping counter 1 of 20
Epoch 19/1000, LR 0.000270
Train loss: 0.1549;  Loss pred: 0.1259; Loss self: 2.9060; time: 0.23s
Val loss: 0.5122 score: 0.7959 time: 0.07s
Test loss: 0.2652 score: 0.8980 time: 0.08s
     INFO: Early stopping counter 2 of 20
Epoch 20/1000, LR 0.000270
Train loss: 0.1435;  Loss pred: 0.1144; Loss self: 2.9077; time: 0.26s
Val loss: 0.5047 score: 0.7959 time: 0.09s
Test loss: 0.2491 score: 0.8980 time: 0.10s
     INFO: Early stopping counter 3 of 20
Epoch 21/1000, LR 0.000270
Train loss: 0.1315;  Loss pred: 0.1025; Loss self: 2.8992; time: 0.29s
Val loss: 0.4893 score: 0.8163 time: 0.09s
Test loss: 0.2316 score: 0.8980 time: 0.08s
Epoch 22/1000, LR 0.000270
Train loss: 0.1193;  Loss pred: 0.0905; Loss self: 2.8844; time: 0.25s
Val loss: 0.4752 score: 0.8163 time: 0.11s
Test loss: 0.2154 score: 0.8980 time: 0.10s
Epoch 23/1000, LR 0.000270
Train loss: 0.1082;  Loss pred: 0.0796; Loss self: 2.8672; time: 0.29s
Val loss: 0.4729 score: 0.8163 time: 0.12s
Test loss: 0.1991 score: 0.8980 time: 0.08s
Epoch 24/1000, LR 0.000270
Train loss: 0.0976;  Loss pred: 0.0691; Loss self: 2.8492; time: 0.29s
Val loss: 0.4749 score: 0.7959 time: 0.07s
Test loss: 0.1855 score: 0.8980 time: 0.11s
     INFO: Early stopping counter 1 of 20
Epoch 25/1000, LR 0.000270
Train loss: 0.0886;  Loss pred: 0.0603; Loss self: 2.8299; time: 0.32s
Val loss: 0.4724 score: 0.7959 time: 0.13s
Test loss: 0.1736 score: 0.8980 time: 0.10s
Epoch 26/1000, LR 0.000270
Train loss: 0.0818;  Loss pred: 0.0537; Loss self: 2.8124; time: 0.26s
Val loss: 0.4623 score: 0.8163 time: 0.08s
Test loss: 0.1620 score: 0.9184 time: 0.09s
Epoch 27/1000, LR 0.000270
Train loss: 0.0764;  Loss pred: 0.0484; Loss self: 2.7999; time: 0.25s
Val loss: 0.4500 score: 0.8367 time: 0.08s
Test loss: 0.1506 score: 0.9184 time: 0.09s
Epoch 28/1000, LR 0.000270
Train loss: 0.0719;  Loss pred: 0.0439; Loss self: 2.7922; time: 0.27s
Val loss: 0.4427 score: 0.8367 time: 0.07s
Test loss: 0.1397 score: 0.9388 time: 0.08s
Epoch 29/1000, LR 0.000270
Train loss: 0.0677;  Loss pred: 0.0398; Loss self: 2.7882; time: 0.27s
Val loss: 0.4417 score: 0.8367 time: 0.08s
Test loss: 0.1302 score: 0.9388 time: 0.09s
Epoch 30/1000, LR 0.000270
Train loss: 0.0638;  Loss pred: 0.0359; Loss self: 2.7874; time: 0.23s
Val loss: 0.4465 score: 0.8571 time: 0.12s
Test loss: 0.1222 score: 0.9592 time: 0.09s
     INFO: Early stopping counter 1 of 20
Epoch 31/1000, LR 0.000270
Train loss: 0.0601;  Loss pred: 0.0323; Loss self: 2.7891; time: 0.23s
Val loss: 0.4562 score: 0.8571 time: 0.12s
Test loss: 0.1158 score: 0.9592 time: 0.09s
     INFO: Early stopping counter 2 of 20
Epoch 32/1000, LR 0.000270
Train loss: 0.0572;  Loss pred: 0.0292; Loss self: 2.7916; time: 0.23s
Val loss: 0.4650 score: 0.8571 time: 0.07s
Test loss: 0.1101 score: 0.9592 time: 0.08s
     INFO: Early stopping counter 3 of 20
Epoch 33/1000, LR 0.000270
Train loss: 0.0546;  Loss pred: 0.0267; Loss self: 2.7939; time: 0.23s
Val loss: 0.4716 score: 0.8571 time: 0.08s
Test loss: 0.1046 score: 0.9592 time: 0.08s
     INFO: Early stopping counter 4 of 20
Epoch 34/1000, LR 0.000270
Train loss: 0.0524;  Loss pred: 0.0244; Loss self: 2.7969; time: 0.24s
Val loss: 0.4731 score: 0.8571 time: 0.12s
Test loss: 0.0994 score: 0.9592 time: 0.08s
     INFO: Early stopping counter 5 of 20
Epoch 35/1000, LR 0.000270
Train loss: 0.0503;  Loss pred: 0.0223; Loss self: 2.7987; time: 0.24s
Val loss: 0.4735 score: 0.8571 time: 0.08s
Test loss: 0.0940 score: 0.9592 time: 0.09s
     INFO: Early stopping counter 6 of 20
Epoch 36/1000, LR 0.000270
Train loss: 0.0484;  Loss pred: 0.0204; Loss self: 2.7998; time: 0.30s
Val loss: 0.4752 score: 0.8571 time: 0.07s
Test loss: 0.0889 score: 0.9796 time: 0.09s
     INFO: Early stopping counter 7 of 20
Epoch 37/1000, LR 0.000270
Train loss: 0.0466;  Loss pred: 0.0186; Loss self: 2.7999; time: 0.25s
Val loss: 0.4769 score: 0.8571 time: 0.08s
Test loss: 0.0842 score: 0.9796 time: 0.09s
     INFO: Early stopping counter 8 of 20
Epoch 38/1000, LR 0.000270
Train loss: 0.0450;  Loss pred: 0.0171; Loss self: 2.7987; time: 0.26s
Val loss: 0.4794 score: 0.8571 time: 0.12s
Test loss: 0.0803 score: 0.9796 time: 0.10s
     INFO: Early stopping counter 9 of 20
Epoch 39/1000, LR 0.000269
Train loss: 0.0437;  Loss pred: 0.0157; Loss self: 2.7972; time: 0.24s
Val loss: 0.4830 score: 0.8571 time: 0.15s
Test loss: 0.0771 score: 0.9796 time: 0.09s
     INFO: Early stopping counter 10 of 20
Epoch 40/1000, LR 0.000269
Train loss: 0.0426;  Loss pred: 0.0146; Loss self: 2.7955; time: 0.33s
Val loss: 0.4889 score: 0.8571 time: 0.11s
Test loss: 0.0750 score: 0.9796 time: 0.30s
     INFO: Early stopping counter 11 of 20
Epoch 41/1000, LR 0.000269
Train loss: 0.0416;  Loss pred: 0.0137; Loss self: 2.7941; time: 0.50s
Val loss: 0.4965 score: 0.8571 time: 0.14s
Test loss: 0.0738 score: 0.9796 time: 0.31s
     INFO: Early stopping counter 12 of 20
Epoch 42/1000, LR 0.000269
Train loss: 0.0407;  Loss pred: 0.0128; Loss self: 2.7934; time: 0.36s
Val loss: 0.5070 score: 0.8571 time: 0.13s
Test loss: 0.0734 score: 0.9796 time: 0.12s
     INFO: Early stopping counter 13 of 20
Epoch 43/1000, LR 0.000269
Train loss: 0.0400;  Loss pred: 0.0120; Loss self: 2.7936; time: 0.32s
Val loss: 0.5204 score: 0.8571 time: 0.27s
Test loss: 0.0735 score: 0.9796 time: 0.21s
     INFO: Early stopping counter 14 of 20
Epoch 44/1000, LR 0.000269
Train loss: 0.0392;  Loss pred: 0.0113; Loss self: 2.7949; time: 0.30s
Val loss: 0.5346 score: 0.8367 time: 0.13s
Test loss: 0.0739 score: 0.9796 time: 0.14s
     INFO: Early stopping counter 15 of 20
Epoch 45/1000, LR 0.000269
Train loss: 0.0386;  Loss pred: 0.0106; Loss self: 2.7967; time: 0.26s
Val loss: 0.5484 score: 0.8163 time: 0.17s
Test loss: 0.0744 score: 0.9796 time: 0.08s
     INFO: Early stopping counter 16 of 20
Epoch 46/1000, LR 0.000269
Train loss: 0.0380;  Loss pred: 0.0100; Loss self: 2.7986; time: 0.36s
Val loss: 0.5607 score: 0.8163 time: 0.06s
Test loss: 0.0749 score: 0.9796 time: 0.35s
     INFO: Early stopping counter 17 of 20
Epoch 47/1000, LR 0.000269
Train loss: 0.0374;  Loss pred: 0.0094; Loss self: 2.8006; time: 0.32s
Val loss: 0.5714 score: 0.8163 time: 0.12s
Test loss: 0.0753 score: 0.9796 time: 0.11s
     INFO: Early stopping counter 18 of 20
Epoch 48/1000, LR 0.000269
Train loss: 0.0369;  Loss pred: 0.0089; Loss self: 2.8027; time: 0.39s
Val loss: 0.5807 score: 0.8163 time: 0.07s
Test loss: 0.0751 score: 0.9796 time: 0.13s
     INFO: Early stopping counter 19 of 20
Epoch 49/1000, LR 0.000269
Train loss: 0.0364;  Loss pred: 0.0084; Loss self: 2.8045; time: 0.28s
Val loss: 0.5882 score: 0.8163 time: 0.09s
Test loss: 0.0744 score: 0.9796 time: 0.13s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 028,   Train_Loss: 0.0677,   Val_Loss: 0.4417,   Val_Precision: 0.9474,   Val_Recall: 0.7200,   Val_accuracy: 0.8182,   Val_Score: 0.8367,   Val_Loss: 0.4417,   Test_Precision: 0.9565,   Test_Recall: 0.9167,   Test_accuracy: 0.9362,   Test_Score: 0.9388,   Test_loss: 0.1302


[0.09452154708560556, 0.1711661390727386, 0.15822971099987626, 0.09641384205315262, 0.1071156068937853, 0.07705299998633564, 0.08886201400309801, 0.09051300003193319, 0.10040262597613037, 0.0753729670541361, 0.07291406102012843, 0.14193040190730244, 0.18352665309794247, 0.08279371110256761, 0.07200683397240937, 0.08261185593437403, 0.08992896904237568, 0.10324136191047728, 0.07358087401371449, 0.0865716258995235, 0.07187003002036363, 0.14318371401168406, 0.19200806692242622, 0.3863587260711938, 0.0799661329947412, 0.07960947195533663, 0.07038232695776969, 0.0853387441020459, 0.08042071596719325, 0.06710024701897055, 0.1112815709784627, 0.15592345397453755, 0.07359106105286628, 0.10698551603127271, 0.07796810695435852, 0.07994347100611776, 0.07355987397022545, 0.07318234292324632, 0.0850401169154793, 0.08050350507255644, 0.07813307899050415, 0.08994403993710876, 0.07911842397879809, 0.10378090501762927, 0.07355005096178502, 0.08544690196868032, 0.06728954706341028, 0.08620715199504048, 0.08130906103178859, 0.08239514497108757, 0.07803700503427535, 0.08633449499029666, 0.09254518698435277, 0.10405872599221766, 0.07200848404318094, 0.07614572194870561, 0.07885271089617163, 0.07325105904601514, 0.08571272203698754, 0.09646163799334317, 0.09229166898876429, 0.0803795619867742, 0.07794898096472025, 0.07072622899431735, 0.12156251003034413, 0.07670781807973981, 0.08280532306525856, 0.08020552096422762, 0.07431093603372574, 0.07378465193323791, 0.12139850005041808, 0.09806859993841499, 0.0943294009193778, 0.09574046405032277, 0.07118210103362799, 0.08132307697087526, 0.08765971101820469, 0.06715698703192174, 0.084098229999654, 0.081328280037269, 0.09823667991440743, 0.07623107498511672, 0.0783383579691872, 0.07018355396576226, 0.07331530808005482, 0.07373277400620282, 0.07790054194629192, 0.10264716402161866, 0.10021455306559801, 0.0807296799030155, 0.07996112911496311, 0.07204930298030376, 0.11939669901039451, 0.08157269400544465, 0.07729757903143764, 0.07567717903293669, 0.11003958992660046, 0.1001434400677681, 0.09149374603293836, 0.14514314499683678, 0.09381097799632698, 0.08587247401010245, 0.09975736401975155, 0.08536034997086972, 0.11930120701435953, 0.09807106398511678, 0.09233319899067283, 0.09586543496698141, 0.08762083400506526, 0.08862585597671568, 0.08722371992189437, 0.0953470739768818, 0.08907399803865701, 0.08690048893913627, 0.08103356207720935, 0.09999161795713007, 0.08445156295783818, 0.10634023195598274, 0.0818817859981209, 0.11215116898529232, 0.10189404501579702, 0.09516949707176536, 0.09220588603056967, 0.08652857202105224, 0.08957349392585456, 0.0970638879807666, 0.09939845209009945, 0.08166389202233404, 0.08477216900791973, 0.08768073993269354, 0.09263150405604392, 0.09316388494335115, 0.09446116990875453, 0.10201977496035397, 0.0969672710634768, 0.3040268520126119, 0.3156051750993356, 0.1248420279007405, 0.2157203620299697, 0.14810888294596225, 0.08768468000926077, 0.3520256250631064, 0.11954753194004297, 0.13066198106389493, 0.1338559469440952]
[0.0019290111650123584, 0.003493186511688543, 0.0032291777755076786, 0.001967629429656176, 0.00218603279375072, 0.0015725102038027681, 0.001813510489859143, 0.0018472040822843509, 0.002049033183186334, 0.001538223817431349, 0.0014880420616352741, 0.0028965388144347438, 0.0037454418999580096, 0.001689667573521788, 0.0014695272239267218, 0.0016859562435586537, 0.001835285082497463, 0.0021069665696015773, 0.0015016504900758059, 0.0017667678755004794, 0.0014667353065380333, 0.002922116612483348, 0.003918531978008699, 0.007884871960636608, 0.0016319618978518614, 0.001624683101129319, 0.0014363740195463203, 0.0017416070224907324, 0.0016412391013712908, 0.0013693927963055214, 0.0022710524689482183, 0.003182111305602807, 0.0015018583888340058, 0.002183377878189239, 0.0015911858562113984, 0.0016314994082881175, 0.0015012219177597032, 0.0014935172025152311, 0.0017355125901118225, 0.0016429286749501313, 0.0015945526324592683, 0.0018355926517777297, 0.0016146617138530224, 0.0021179776534210053, 0.0015010214481996943, 0.0017438143258914351, 0.0013732560625185771, 0.0017593296325518464, 0.0016593685924854813, 0.0016815335708385219, 0.0015925919394750073, 0.0017619284691897277, 0.0018886772853949545, 0.002123647469228932, 0.0014695608988404274, 0.001553994325483788, 0.0016092389978810536, 0.001494919572367656, 0.0017492392252446438, 0.0019686048570070037, 0.0018835034487502916, 0.0016403992242198817, 0.00159079552989225, 0.0014433924284554562, 0.002480867551639676, 0.0015654656750967307, 0.0016899045523522155, 0.0016368473666168901, 0.0015165497149739946, 0.0015058092231273043, 0.0024775204091922057, 0.002001399998743163, 0.0019250898146811798, 0.0019538870214351584, 0.0014526959394617956, 0.0016596546320586788, 0.0017889736942490752, 0.001370550755753505, 0.001716290408156204, 0.0016597608170871223, 0.0020048302023348455, 0.0015557362241860554, 0.0015987419993711673, 0.001432317427872699, 0.001496230777143976, 0.0015047504899225064, 0.0015898069784957536, 0.0020948400820738505, 0.0020451949605224083, 0.0016475444878166427, 0.00163185977785639, 0.0014703939383735461, 0.0024366673267427453, 0.0016647488572539724, 0.0015775016128864823, 0.0015444322251619734, 0.002245705916869397, 0.0020437436748524103, 0.0018672193067946605, 0.0029621049999354445, 0.0019145097550270812, 0.0017524994695939276, 0.002035864571831664, 0.001742047958589178, 0.0024347185104971335, 0.0020014502854105464, 0.0018843509998096495, 0.001956437448305743, 0.0017881802858176585, 0.00180869093830032, 0.0017800759167733546, 0.0019458586525894245, 0.0018178366946664695, 0.0017734793661048217, 0.0016537461648410072, 0.002040645264431226, 0.0017235012848538403, 0.002170208815428219, 0.0016710568571045082, 0.002288799367046782, 0.0020794703064448373, 0.0019422346341176604, 0.001881752776134075, 0.0017658892249194334, 0.0018280304882827463, 0.0019808956730768693, 0.0020285398385734583, 0.0016666100412721233, 0.0017300442654677496, 0.0017894028557692559, 0.0018904388582866108, 0.0019013037743541052, 0.0019277789777296843, 0.002082036223680693, 0.0019789238992546287, 0.006204629632910448, 0.006440921940802768, 0.0025477964877702146, 0.004402456367958565, 0.0030226302642033112, 0.0017894832654951178, 0.007184196429859314, 0.0024397455497967954, 0.0026665710421203046, 0.0027317540192672487]
[518.4003172908506, 286.2715737204133, 309.6763540194946, 508.22577916754386, 457.4496790984705, 635.9259212320029, 551.4167166894474, 541.3586996642769, 488.03504413967437, 650.1004526570661, 672.0240144966443, 345.23963394398663, 266.9911926844229, 591.8323909807251, 680.4909658821435, 593.1352037282043, 544.8744772878534, 474.6159784533733, 665.933921780639, 566.0053105259942, 681.7862742803413, 342.2176910147861, 255.1976111493099, 126.82514123149592, 612.7594040745021, 615.5046478324905, 696.1974989744319, 574.1823425642057, 609.2957443948773, 730.2506648916917, 440.32448112620017, 314.2567635014149, 665.8417380991344, 458.00592283610536, 628.4620970557095, 612.9331061475955, 666.1240341416782, 669.5604163888443, 576.1986433849886, 608.6691499436843, 627.1351472780841, 544.7831788994812, 619.3247733692327, 472.1485131747152, 666.212998621297, 573.4555480778045, 728.1963118851858, 568.3983157548109, 602.6388618710399, 594.6952337688596, 627.9072342471146, 567.5599307728298, 529.4710789042412, 470.88794844234985, 680.4753724660616, 643.5029932871093, 621.4117364274282, 668.9323081215656, 571.677095715792, 507.973957516474, 530.9254945423658, 609.6077011226131, 628.6162999639139, 692.812280489845, 403.08479964562054, 638.7875607289885, 591.7493970935092, 610.9305121508336, 659.3915056831141, 664.0947502786399, 403.6293692232588, 499.6502451423898, 519.456283220538, 511.80031855960914, 688.3752978413959, 602.5349977540652, 558.9797117837172, 729.6336861674396, 582.6519773389, 602.496449913186, 498.79535874678555, 642.782487451039, 625.4917931682096, 698.1692609055355, 668.3460969228373, 664.5620032670661, 629.0071773028584, 477.3634076210819, 488.9509407673134, 606.9638831575462, 612.7977498860836, 680.0898547678554, 410.3966056526749, 600.691206750257, 633.9137734193622, 647.487137154966, 445.2942802920694, 489.29815040147605, 535.555730578128, 337.597755657478, 522.3269285383478, 570.613582115212, 491.19180805838255, 574.0370091819194, 410.7250984820479, 499.63769137282145, 530.686692713309, 511.1331317369696, 559.2277288432037, 552.8860563318403, 561.7737932282377, 513.9119425089093, 550.1044196841216, 563.8633406806126, 604.687721284083, 490.0410754530247, 580.2142468868562, 460.78515251200974, 598.423683639784, 436.9102920935754, 480.8916948228264, 514.870851561295, 531.419436539601, 566.2869368522387, 547.0368281107834, 504.82214363501953, 492.96542319979073, 600.0203858346492, 578.0198923000571, 558.8456488575931, 528.9777003982794, 525.9548807973684, 518.7316655863136, 480.2990402502059, 505.32514179886095, 161.1699745454304, 155.25727670522963, 392.4960273711587, 227.1459195548379, 330.837685258066, 558.8205373484273, 139.1944123136375, 409.87880891238405, 375.0134476840554, 366.0651701972181]
Elapsed: 0.10061486946049564~0.047328210241409
Time per graph: 0.0020533646828672578~0.0009658818416614082
Speed: 537.0736355680921~122.22791634637444
Total Time: 0.1347
best val loss: 0.4416649043560028 test_score: 0.9388

Testing...
Test loss: 0.1222 score: 0.9592 time: 0.09s
test Score 0.9592
Epoch Time List: [0.4212449110345915, 0.5271857319166884, 0.5045939111150801, 0.492961282026954, 0.40879769902676344, 0.3912614620057866, 0.43015367176849395, 0.4443825220223516, 0.45640196604654193, 0.3873873061966151, 0.3741483569610864, 0.4820109030697495, 0.7949949179310352, 0.7813072571298108, 0.43553718796465546, 0.3867250708863139, 0.5803953430149704, 0.4752067410154268, 0.4688927868846804, 0.4607895689550787, 0.4367142770206556, 1.1154394641052932, 0.5654596409294754, 0.810825151973404, 0.4323563320795074, 0.478389129973948, 0.421190754044801, 0.4155838399892673, 0.4811278688721359, 0.46429302205797285, 0.5240831370465457, 0.5193607191322371, 0.38791009690612555, 0.49677234806586057, 0.3966254881815985, 0.4279297968605533, 0.4073607779573649, 0.3760427579982206, 0.4039058299968019, 0.4024028111016378, 0.41380055306944996, 0.4881413849070668, 0.40292614803183824, 0.40139093180187047, 0.48272958991583437, 0.42867816297803074, 0.4844732191413641, 0.43657715409062803, 0.46886783197987825, 0.44167822611052543, 0.4157691851723939, 0.42949652683455497, 0.44942378299310803, 0.4286258270731196, 0.48536610195878893, 0.36718045512679964, 0.40361273498274386, 0.37861317687202245, 0.39824283588677645, 0.43770532903727144, 0.4698431669967249, 0.43789663014467806, 0.4032576299505308, 0.3689220598898828, 0.43919198599178344, 0.4830825360259041, 0.4170082969358191, 0.4025996328564361, 0.38329677504952997, 0.38652255409397185, 0.4208958549425006, 0.4271953519200906, 0.46432478399947286, 0.4538966630352661, 0.3930676350137219, 0.4250095688039437, 0.44373951805755496, 0.4056091670645401, 0.4341758170630783, 0.4251074221683666, 0.44669476989656687, 0.4555058589903638, 0.48123304988257587, 0.3837094767950475, 0.36716674198396504, 0.3946649630088359, 0.4124348451150581, 0.4253365299664438, 0.5234826599480584, 0.4363730320474133, 0.44618152698967606, 0.3956343671306968, 0.4638734880136326, 0.414568395819515, 0.40346646995749325, 0.4714216369902715, 0.5405874439748004, 0.42509151285048574, 0.43989098409656435, 0.5164204899920151, 0.4367663179291412, 0.41439614992123097, 0.394873836892657, 0.5942892880411819, 0.4597208409104496, 0.38387727190274745, 0.4145392399514094, 0.4340950589394197, 0.4275643980363384, 0.39800347085110843, 0.4305488328682259, 0.5342699839966372, 0.414149675052613, 0.40049517003353685, 0.3788184850709513, 0.446846753009595, 0.45913387602195144, 0.45663639390841126, 0.49477732519153506, 0.46735296095721424, 0.554297913913615, 0.42759886698331684, 0.41837373294401914, 0.42151628609281033, 0.4289598420727998, 0.44633658311795443, 0.44224681006744504, 0.371776745072566, 0.389076828956604, 0.44044896692503244, 0.40719897602684796, 0.45967786107212305, 0.42134561797138304, 0.47259781090542674, 0.48003501491621137, 0.7322273909812793, 0.9506331189768389, 0.6096683921059594, 0.805354690994136, 0.5714075580472127, 0.5161520950496197, 0.7680894930381328, 0.5478530400432646, 0.5865656858077273, 0.4980839130003005]
Total Epoch List: [96, 49]
Total Time List: [0.07627350999973714, 0.13472878793254495]
========================k_idx:2========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
emb dim:  64
Model: Ethident
Ethident(
  (encoder): HGATE_encoder(
    (lin1): Linear(in_features=14887, out_features=64, bias=True)
    (account_convs): ModuleList(
      (0): GATEConv()
      (1): GATConv(64, 64, heads=1)
    )
    (subgraph_conv): GATConv(64, 64, heads=1)
    (lin2): Linear(in_features=64, out_features=64, bias=True)
    (lin3): Linear(in_features=64, out_features=2, bias=True)
  )
  (proj_head_g1): Project_Head(
    (block): Sequential(
      (0): Linear(in_features=64, out_features=64, bias=True)
      (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=64, bias=True)
      (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
      (5): ReLU()
    )
    (linear_shortcut): Linear(in_features=64, out_features=64, bias=True)
  )
  (proj_head_g2): Project_Head(
    (block): Sequential(
      (0): Linear(in_features=64, out_features=64, bias=True)
      (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=64, bias=True)
      (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
      (5): ReLU()
    )
    (linear_shortcut): Linear(in_features=64, out_features=64, bias=True)
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=64, bias=True)
    (1): ReLU()
    (2): Dropout(p=0.0, inplace=True)
    (3): Linear(in_features=64, out_features=2, bias=True)
  )
)
Total number of parameters: 1003972
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7720be266b60>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.7085;  Loss pred: 0.6739; Loss self: 3.4535; time: 0.26s
Val loss: 0.7024 score: 0.4490 time: 0.08s
Test loss: 0.6922 score: 0.5208 time: 0.08s
Epoch 2/1000, LR 0.000000
Train loss: 0.7085;  Loss pred: 0.6739; Loss self: 3.4535; time: 0.27s
Val loss: 0.6724 score: 0.4694 time: 0.08s
Test loss: 0.6645 score: 0.5833 time: 0.11s
Epoch 3/1000, LR 0.000030
Train loss: 0.6700;  Loss pred: 0.6357; Loss self: 3.4361; time: 0.25s
Val loss: 0.6276 score: 0.5510 time: 0.08s
Test loss: 0.6233 score: 0.5833 time: 0.08s
Epoch 4/1000, LR 0.000060
Train loss: 0.6181;  Loss pred: 0.5844; Loss self: 3.3749; time: 0.25s
Val loss: 0.5943 score: 0.7143 time: 0.07s
Test loss: 0.5907 score: 0.7500 time: 0.07s
Epoch 5/1000, LR 0.000090
Train loss: 0.5836;  Loss pred: 0.5508; Loss self: 3.2836; time: 0.23s
Val loss: 0.5739 score: 0.6939 time: 0.10s
Test loss: 0.5735 score: 0.7292 time: 0.07s
Epoch 6/1000, LR 0.000120
Train loss: 0.5679;  Loss pred: 0.5356; Loss self: 3.2291; time: 0.24s
Val loss: 0.5637 score: 0.6939 time: 0.08s
Test loss: 0.5655 score: 0.7083 time: 0.08s
Epoch 7/1000, LR 0.000150
Train loss: 0.5498;  Loss pred: 0.5178; Loss self: 3.1982; time: 0.25s
Val loss: 0.5511 score: 0.7143 time: 0.08s
Test loss: 0.5498 score: 0.7500 time: 0.08s
Epoch 8/1000, LR 0.000180
Train loss: 0.5244;  Loss pred: 0.4928; Loss self: 3.1622; time: 0.30s
Val loss: 0.5216 score: 0.7755 time: 0.08s
Test loss: 0.5105 score: 0.8333 time: 0.08s
Epoch 9/1000, LR 0.000210
Train loss: 0.4835;  Loss pred: 0.4526; Loss self: 3.0888; time: 0.26s
Val loss: 0.4816 score: 0.7755 time: 0.08s
Test loss: 0.4649 score: 0.7917 time: 0.12s
Epoch 10/1000, LR 0.000240
Train loss: 0.4279;  Loss pred: 0.3982; Loss self: 2.9712; time: 0.27s
Val loss: 0.4735 score: 0.6939 time: 0.10s
Test loss: 0.4350 score: 0.7917 time: 0.07s
Epoch 11/1000, LR 0.000270
Train loss: 0.3951;  Loss pred: 0.3663; Loss self: 2.8797; time: 0.24s
Val loss: 0.4425 score: 0.8571 time: 0.07s
Test loss: 0.4028 score: 0.8542 time: 0.09s
Epoch 12/1000, LR 0.000270
Train loss: 0.3564;  Loss pred: 0.3281; Loss self: 2.8293; time: 0.26s
Val loss: 0.4201 score: 0.8980 time: 0.08s
Test loss: 0.3788 score: 0.8542 time: 0.09s
Epoch 13/1000, LR 0.000270
Train loss: 0.3206;  Loss pred: 0.2923; Loss self: 2.8297; time: 0.26s
Val loss: 0.4120 score: 0.8367 time: 0.08s
Test loss: 0.3646 score: 0.8542 time: 0.08s
Epoch 14/1000, LR 0.000270
Train loss: 0.3020;  Loss pred: 0.2734; Loss self: 2.8678; time: 0.26s
Val loss: 0.4009 score: 0.8367 time: 0.07s
Test loss: 0.3524 score: 0.8542 time: 0.08s
Epoch 15/1000, LR 0.000270
Train loss: 0.2790;  Loss pred: 0.2502; Loss self: 2.8776; time: 0.27s
Val loss: 0.3936 score: 0.8571 time: 0.09s
Test loss: 0.3384 score: 0.8750 time: 0.08s
Epoch 16/1000, LR 0.000270
Train loss: 0.2573;  Loss pred: 0.2286; Loss self: 2.8693; time: 0.26s
Val loss: 0.3850 score: 0.8571 time: 0.08s
Test loss: 0.3223 score: 0.8958 time: 0.09s
Epoch 17/1000, LR 0.000270
Train loss: 0.2373;  Loss pred: 0.2087; Loss self: 2.8583; time: 0.27s
Val loss: 0.3744 score: 0.8571 time: 0.09s
Test loss: 0.3035 score: 0.8958 time: 0.11s
Epoch 18/1000, LR 0.000270
Train loss: 0.2174;  Loss pred: 0.1889; Loss self: 2.8425; time: 0.31s
Val loss: 0.3671 score: 0.8367 time: 0.09s
Test loss: 0.2892 score: 0.9167 time: 0.09s
Epoch 19/1000, LR 0.000270
Train loss: 0.2024;  Loss pred: 0.1741; Loss self: 2.8246; time: 0.27s
Val loss: 0.3587 score: 0.8367 time: 0.14s
Test loss: 0.2749 score: 0.9167 time: 0.08s
Epoch 20/1000, LR 0.000270
Train loss: 0.1882;  Loss pred: 0.1601; Loss self: 2.8141; time: 0.27s
Val loss: 0.3536 score: 0.8571 time: 0.09s
Test loss: 0.2645 score: 0.9167 time: 0.08s
Epoch 21/1000, LR 0.000270
Train loss: 0.1741;  Loss pred: 0.1460; Loss self: 2.8097; time: 0.25s
Val loss: 0.3498 score: 0.8776 time: 0.08s
Test loss: 0.2579 score: 0.9167 time: 0.08s
Epoch 22/1000, LR 0.000270
Train loss: 0.1639;  Loss pred: 0.1358; Loss self: 2.8101; time: 0.24s
Val loss: 0.3455 score: 0.8776 time: 0.07s
Test loss: 0.2520 score: 0.9167 time: 0.08s
Epoch 23/1000, LR 0.000270
Train loss: 0.1558;  Loss pred: 0.1277; Loss self: 2.8127; time: 0.24s
Val loss: 0.3434 score: 0.8776 time: 0.07s
Test loss: 0.2435 score: 0.9167 time: 0.08s
Epoch 24/1000, LR 0.000270
Train loss: 0.1470;  Loss pred: 0.1188; Loss self: 2.8168; time: 0.25s
Val loss: 0.3424 score: 0.8367 time: 0.07s
Test loss: 0.2341 score: 0.9375 time: 0.07s
Epoch 25/1000, LR 0.000270
Train loss: 0.1373;  Loss pred: 0.1091; Loss self: 2.8213; time: 0.25s
Val loss: 0.3432 score: 0.8367 time: 0.08s
Test loss: 0.2252 score: 0.9583 time: 0.08s
     INFO: Early stopping counter 1 of 20
Epoch 26/1000, LR 0.000270
Train loss: 0.1285;  Loss pred: 0.1002; Loss self: 2.8262; time: 0.26s
Val loss: 0.3439 score: 0.8367 time: 0.08s
Test loss: 0.2167 score: 0.9583 time: 0.08s
     INFO: Early stopping counter 2 of 20
Epoch 27/1000, LR 0.000270
Train loss: 0.1204;  Loss pred: 0.0921; Loss self: 2.8313; time: 0.28s
Val loss: 0.3434 score: 0.8367 time: 0.08s
Test loss: 0.2087 score: 0.9583 time: 0.08s
     INFO: Early stopping counter 3 of 20
Epoch 28/1000, LR 0.000270
Train loss: 0.1122;  Loss pred: 0.0838; Loss self: 2.8376; time: 0.26s
Val loss: 0.3426 score: 0.8367 time: 0.08s
Test loss: 0.2011 score: 0.9583 time: 0.08s
     INFO: Early stopping counter 4 of 20
Epoch 29/1000, LR 0.000270
Train loss: 0.1041;  Loss pred: 0.0757; Loss self: 2.8438; time: 0.27s
Val loss: 0.3431 score: 0.8367 time: 0.08s
Test loss: 0.1944 score: 0.9583 time: 0.08s
     INFO: Early stopping counter 5 of 20
Epoch 30/1000, LR 0.000270
Train loss: 0.0969;  Loss pred: 0.0684; Loss self: 2.8488; time: 0.24s
Val loss: 0.3436 score: 0.8367 time: 0.07s
Test loss: 0.1893 score: 0.9583 time: 0.07s
     INFO: Early stopping counter 6 of 20
Epoch 31/1000, LR 0.000270
Train loss: 0.0906;  Loss pred: 0.0620; Loss self: 2.8536; time: 0.25s
Val loss: 0.3442 score: 0.8367 time: 0.08s
Test loss: 0.1856 score: 0.9583 time: 0.08s
     INFO: Early stopping counter 7 of 20
Epoch 32/1000, LR 0.000270
Train loss: 0.0855;  Loss pred: 0.0569; Loss self: 2.8588; time: 0.27s
Val loss: 0.3457 score: 0.8367 time: 0.08s
Test loss: 0.1826 score: 0.9583 time: 0.08s
     INFO: Early stopping counter 8 of 20
Epoch 33/1000, LR 0.000270
Train loss: 0.0811;  Loss pred: 0.0524; Loss self: 2.8646; time: 0.25s
Val loss: 0.3473 score: 0.8367 time: 0.08s
Test loss: 0.1801 score: 0.9583 time: 0.12s
     INFO: Early stopping counter 9 of 20
Epoch 34/1000, LR 0.000270
Train loss: 0.0773;  Loss pred: 0.0486; Loss self: 2.8703; time: 0.28s
Val loss: 0.3499 score: 0.8571 time: 0.12s
Test loss: 0.1792 score: 0.9583 time: 0.08s
     INFO: Early stopping counter 10 of 20
Epoch 35/1000, LR 0.000270
Train loss: 0.0739;  Loss pred: 0.0452; Loss self: 2.8765; time: 0.24s
Val loss: 0.3526 score: 0.8571 time: 0.07s
Test loss: 0.1793 score: 0.9583 time: 0.11s
     INFO: Early stopping counter 11 of 20
Epoch 36/1000, LR 0.000270
Train loss: 0.0710;  Loss pred: 0.0422; Loss self: 2.8831; time: 0.23s
Val loss: 0.3547 score: 0.8571 time: 0.09s
Test loss: 0.1795 score: 0.9583 time: 0.07s
     INFO: Early stopping counter 12 of 20
Epoch 37/1000, LR 0.000270
Train loss: 0.0686;  Loss pred: 0.0397; Loss self: 2.8897; time: 0.25s
Val loss: 0.3566 score: 0.8571 time: 0.09s
Test loss: 0.1792 score: 0.9583 time: 0.14s
     INFO: Early stopping counter 13 of 20
Epoch 38/1000, LR 0.000270
Train loss: 0.0665;  Loss pred: 0.0376; Loss self: 2.8957; time: 0.27s
Val loss: 0.3580 score: 0.8571 time: 0.09s
Test loss: 0.1786 score: 0.9583 time: 0.09s
     INFO: Early stopping counter 14 of 20
Epoch 39/1000, LR 0.000269
Train loss: 0.0646;  Loss pred: 0.0356; Loss self: 2.9013; time: 0.29s
Val loss: 0.3590 score: 0.8571 time: 0.08s
Test loss: 0.1778 score: 0.9583 time: 0.09s
     INFO: Early stopping counter 15 of 20
Epoch 40/1000, LR 0.000269
Train loss: 0.0627;  Loss pred: 0.0337; Loss self: 2.9066; time: 0.24s
Val loss: 0.3610 score: 0.8571 time: 0.07s
Test loss: 0.1771 score: 0.9583 time: 0.08s
     INFO: Early stopping counter 16 of 20
Epoch 41/1000, LR 0.000269
Train loss: 0.0609;  Loss pred: 0.0318; Loss self: 2.9114; time: 0.24s
Val loss: 0.3640 score: 0.8571 time: 0.07s
Test loss: 0.1765 score: 0.9583 time: 0.09s
     INFO: Early stopping counter 17 of 20
Epoch 42/1000, LR 0.000269
Train loss: 0.0591;  Loss pred: 0.0299; Loss self: 2.9155; time: 0.23s
Val loss: 0.3670 score: 0.8571 time: 0.09s
Test loss: 0.1759 score: 0.9583 time: 0.09s
     INFO: Early stopping counter 18 of 20
Epoch 43/1000, LR 0.000269
Train loss: 0.0573;  Loss pred: 0.0281; Loss self: 2.9193; time: 0.27s
Val loss: 0.3702 score: 0.8571 time: 0.21s
Test loss: 0.1754 score: 0.9583 time: 0.10s
     INFO: Early stopping counter 19 of 20
Epoch 44/1000, LR 0.000269
Train loss: 0.0555;  Loss pred: 0.0263; Loss self: 2.9226; time: 0.32s
Val loss: 0.3739 score: 0.8571 time: 0.08s
Test loss: 0.1750 score: 0.9583 time: 0.09s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 023,   Train_Loss: 0.1470,   Val_Loss: 0.3424,   Val_Precision: 0.9048,   Val_Recall: 0.7600,   Val_accuracy: 0.8261,   Val_Score: 0.8367,   Val_Loss: 0.3424,   Test_Precision: 1.0000,   Test_Recall: 0.8750,   Test_accuracy: 0.9333,   Test_Score: 0.9375,   Test_loss: 0.2341


[0.09452154708560556, 0.1711661390727386, 0.15822971099987626, 0.09641384205315262, 0.1071156068937853, 0.07705299998633564, 0.08886201400309801, 0.09051300003193319, 0.10040262597613037, 0.0753729670541361, 0.07291406102012843, 0.14193040190730244, 0.18352665309794247, 0.08279371110256761, 0.07200683397240937, 0.08261185593437403, 0.08992896904237568, 0.10324136191047728, 0.07358087401371449, 0.0865716258995235, 0.07187003002036363, 0.14318371401168406, 0.19200806692242622, 0.3863587260711938, 0.0799661329947412, 0.07960947195533663, 0.07038232695776969, 0.0853387441020459, 0.08042071596719325, 0.06710024701897055, 0.1112815709784627, 0.15592345397453755, 0.07359106105286628, 0.10698551603127271, 0.07796810695435852, 0.07994347100611776, 0.07355987397022545, 0.07318234292324632, 0.0850401169154793, 0.08050350507255644, 0.07813307899050415, 0.08994403993710876, 0.07911842397879809, 0.10378090501762927, 0.07355005096178502, 0.08544690196868032, 0.06728954706341028, 0.08620715199504048, 0.08130906103178859, 0.08239514497108757, 0.07803700503427535, 0.08633449499029666, 0.09254518698435277, 0.10405872599221766, 0.07200848404318094, 0.07614572194870561, 0.07885271089617163, 0.07325105904601514, 0.08571272203698754, 0.09646163799334317, 0.09229166898876429, 0.0803795619867742, 0.07794898096472025, 0.07072622899431735, 0.12156251003034413, 0.07670781807973981, 0.08280532306525856, 0.08020552096422762, 0.07431093603372574, 0.07378465193323791, 0.12139850005041808, 0.09806859993841499, 0.0943294009193778, 0.09574046405032277, 0.07118210103362799, 0.08132307697087526, 0.08765971101820469, 0.06715698703192174, 0.084098229999654, 0.081328280037269, 0.09823667991440743, 0.07623107498511672, 0.0783383579691872, 0.07018355396576226, 0.07331530808005482, 0.07373277400620282, 0.07790054194629192, 0.10264716402161866, 0.10021455306559801, 0.0807296799030155, 0.07996112911496311, 0.07204930298030376, 0.11939669901039451, 0.08157269400544465, 0.07729757903143764, 0.07567717903293669, 0.11003958992660046, 0.1001434400677681, 0.09149374603293836, 0.14514314499683678, 0.09381097799632698, 0.08587247401010245, 0.09975736401975155, 0.08536034997086972, 0.11930120701435953, 0.09807106398511678, 0.09233319899067283, 0.09586543496698141, 0.08762083400506526, 0.08862585597671568, 0.08722371992189437, 0.0953470739768818, 0.08907399803865701, 0.08690048893913627, 0.08103356207720935, 0.09999161795713007, 0.08445156295783818, 0.10634023195598274, 0.0818817859981209, 0.11215116898529232, 0.10189404501579702, 0.09516949707176536, 0.09220588603056967, 0.08652857202105224, 0.08957349392585456, 0.0970638879807666, 0.09939845209009945, 0.08166389202233404, 0.08477216900791973, 0.08768073993269354, 0.09263150405604392, 0.09316388494335115, 0.09446116990875453, 0.10201977496035397, 0.0969672710634768, 0.3040268520126119, 0.3156051750993356, 0.1248420279007405, 0.2157203620299697, 0.14810888294596225, 0.08768468000926077, 0.3520256250631064, 0.11954753194004297, 0.13066198106389493, 0.1338559469440952, 0.08556963095907122, 0.11868517694529146, 0.08563441596925259, 0.07629376300610602, 0.07701290503609926, 0.08871876704506576, 0.08228197100106627, 0.08237527695018798, 0.12678026303183287, 0.07822995900642127, 0.09463883505668491, 0.09169477294199169, 0.08254090906120837, 0.08889075694605708, 0.08668906590901315, 0.09143812907859683, 0.110143176978454, 0.09906421799678355, 0.08611233392730355, 0.08618344005662948, 0.08215547807049006, 0.08095834194682539, 0.0805606449721381, 0.07912473706528544, 0.0796679089544341, 0.084125972003676, 0.0867868970381096, 0.08738916402217001, 0.07984088605735451, 0.07936210301704705, 0.08418556698597968, 0.08269928093068302, 0.1275390819646418, 0.08624849398620427, 0.112760259071365, 0.07667616789694875, 0.13953293301165104, 0.09696598397567868, 0.09152398107107729, 0.08616713504306972, 0.09219630190636963, 0.09904967399779707, 0.10033054789528251, 0.09320890298113227]
[0.0019290111650123584, 0.003493186511688543, 0.0032291777755076786, 0.001967629429656176, 0.00218603279375072, 0.0015725102038027681, 0.001813510489859143, 0.0018472040822843509, 0.002049033183186334, 0.001538223817431349, 0.0014880420616352741, 0.0028965388144347438, 0.0037454418999580096, 0.001689667573521788, 0.0014695272239267218, 0.0016859562435586537, 0.001835285082497463, 0.0021069665696015773, 0.0015016504900758059, 0.0017667678755004794, 0.0014667353065380333, 0.002922116612483348, 0.003918531978008699, 0.007884871960636608, 0.0016319618978518614, 0.001624683101129319, 0.0014363740195463203, 0.0017416070224907324, 0.0016412391013712908, 0.0013693927963055214, 0.0022710524689482183, 0.003182111305602807, 0.0015018583888340058, 0.002183377878189239, 0.0015911858562113984, 0.0016314994082881175, 0.0015012219177597032, 0.0014935172025152311, 0.0017355125901118225, 0.0016429286749501313, 0.0015945526324592683, 0.0018355926517777297, 0.0016146617138530224, 0.0021179776534210053, 0.0015010214481996943, 0.0017438143258914351, 0.0013732560625185771, 0.0017593296325518464, 0.0016593685924854813, 0.0016815335708385219, 0.0015925919394750073, 0.0017619284691897277, 0.0018886772853949545, 0.002123647469228932, 0.0014695608988404274, 0.001553994325483788, 0.0016092389978810536, 0.001494919572367656, 0.0017492392252446438, 0.0019686048570070037, 0.0018835034487502916, 0.0016403992242198817, 0.00159079552989225, 0.0014433924284554562, 0.002480867551639676, 0.0015654656750967307, 0.0016899045523522155, 0.0016368473666168901, 0.0015165497149739946, 0.0015058092231273043, 0.0024775204091922057, 0.002001399998743163, 0.0019250898146811798, 0.0019538870214351584, 0.0014526959394617956, 0.0016596546320586788, 0.0017889736942490752, 0.001370550755753505, 0.001716290408156204, 0.0016597608170871223, 0.0020048302023348455, 0.0015557362241860554, 0.0015987419993711673, 0.001432317427872699, 0.001496230777143976, 0.0015047504899225064, 0.0015898069784957536, 0.0020948400820738505, 0.0020451949605224083, 0.0016475444878166427, 0.00163185977785639, 0.0014703939383735461, 0.0024366673267427453, 0.0016647488572539724, 0.0015775016128864823, 0.0015444322251619734, 0.002245705916869397, 0.0020437436748524103, 0.0018672193067946605, 0.0029621049999354445, 0.0019145097550270812, 0.0017524994695939276, 0.002035864571831664, 0.001742047958589178, 0.0024347185104971335, 0.0020014502854105464, 0.0018843509998096495, 0.001956437448305743, 0.0017881802858176585, 0.00180869093830032, 0.0017800759167733546, 0.0019458586525894245, 0.0018178366946664695, 0.0017734793661048217, 0.0016537461648410072, 0.002040645264431226, 0.0017235012848538403, 0.002170208815428219, 0.0016710568571045082, 0.002288799367046782, 0.0020794703064448373, 0.0019422346341176604, 0.001881752776134075, 0.0017658892249194334, 0.0018280304882827463, 0.0019808956730768693, 0.0020285398385734583, 0.0016666100412721233, 0.0017300442654677496, 0.0017894028557692559, 0.0018904388582866108, 0.0019013037743541052, 0.0019277789777296843, 0.002082036223680693, 0.0019789238992546287, 0.006204629632910448, 0.006440921940802768, 0.0025477964877702146, 0.004402456367958565, 0.0030226302642033112, 0.0017894832654951178, 0.007184196429859314, 0.0024397455497967954, 0.0026665710421203046, 0.0027317540192672487, 0.0017827006449806504, 0.0024726078530269056, 0.0017840503326927621, 0.0015894533959605421, 0.0016044355215854011, 0.0018483076467722033, 0.0017142077291888806, 0.0017161516031289163, 0.0026412554798298515, 0.0016297908126337763, 0.001971642397014269, 0.0019103077696248267, 0.0017196022721085076, 0.0018518907697095226, 0.001806022206437774, 0.0019049610224707674, 0.0022946495203844583, 0.002063837874932991, 0.001794006956818824, 0.001795488334513114, 0.0017115724598018762, 0.0016866321238921955, 0.0016783467702528772, 0.0016484320221934468, 0.001659748103217377, 0.0017526244167432499, 0.0018080603549606167, 0.0018206075837952085, 0.0016633517928615522, 0.00165337714618848, 0.0017538659788745765, 0.0017229016860558961, 0.0026570642075967044, 0.001796843624712589, 0.002349172063986771, 0.0015974201645197657, 0.0029069361044093966, 0.0020201246661599725, 0.0019067496056474436, 0.0017951486467306192, 0.001920756289716034, 0.002063534874954106, 0.0020902197478183857, 0.0019418521454402555]
[518.4003172908506, 286.2715737204133, 309.6763540194946, 508.22577916754386, 457.4496790984705, 635.9259212320029, 551.4167166894474, 541.3586996642769, 488.03504413967437, 650.1004526570661, 672.0240144966443, 345.23963394398663, 266.9911926844229, 591.8323909807251, 680.4909658821435, 593.1352037282043, 544.8744772878534, 474.6159784533733, 665.933921780639, 566.0053105259942, 681.7862742803413, 342.2176910147861, 255.1976111493099, 126.82514123149592, 612.7594040745021, 615.5046478324905, 696.1974989744319, 574.1823425642057, 609.2957443948773, 730.2506648916917, 440.32448112620017, 314.2567635014149, 665.8417380991344, 458.00592283610536, 628.4620970557095, 612.9331061475955, 666.1240341416782, 669.5604163888443, 576.1986433849886, 608.6691499436843, 627.1351472780841, 544.7831788994812, 619.3247733692327, 472.1485131747152, 666.212998621297, 573.4555480778045, 728.1963118851858, 568.3983157548109, 602.6388618710399, 594.6952337688596, 627.9072342471146, 567.5599307728298, 529.4710789042412, 470.88794844234985, 680.4753724660616, 643.5029932871093, 621.4117364274282, 668.9323081215656, 571.677095715792, 507.973957516474, 530.9254945423658, 609.6077011226131, 628.6162999639139, 692.812280489845, 403.08479964562054, 638.7875607289885, 591.7493970935092, 610.9305121508336, 659.3915056831141, 664.0947502786399, 403.6293692232588, 499.6502451423898, 519.456283220538, 511.80031855960914, 688.3752978413959, 602.5349977540652, 558.9797117837172, 729.6336861674396, 582.6519773389, 602.496449913186, 498.79535874678555, 642.782487451039, 625.4917931682096, 698.1692609055355, 668.3460969228373, 664.5620032670661, 629.0071773028584, 477.3634076210819, 488.9509407673134, 606.9638831575462, 612.7977498860836, 680.0898547678554, 410.3966056526749, 600.691206750257, 633.9137734193622, 647.487137154966, 445.2942802920694, 489.29815040147605, 535.555730578128, 337.597755657478, 522.3269285383478, 570.613582115212, 491.19180805838255, 574.0370091819194, 410.7250984820479, 499.63769137282145, 530.686692713309, 511.1331317369696, 559.2277288432037, 552.8860563318403, 561.7737932282377, 513.9119425089093, 550.1044196841216, 563.8633406806126, 604.687721284083, 490.0410754530247, 580.2142468868562, 460.78515251200974, 598.423683639784, 436.9102920935754, 480.8916948228264, 514.870851561295, 531.419436539601, 566.2869368522387, 547.0368281107834, 504.82214363501953, 492.96542319979073, 600.0203858346492, 578.0198923000571, 558.8456488575931, 528.9777003982794, 525.9548807973684, 518.7316655863136, 480.2990402502059, 505.32514179886095, 161.1699745454304, 155.25727670522963, 392.4960273711587, 227.1459195548379, 330.837685258066, 558.8205373484273, 139.1944123136375, 409.87880891238405, 375.0134476840554, 366.0651701972181, 560.9466753801809, 404.43129660686986, 560.5223023560366, 629.1471033635924, 623.272164288574, 541.0354719607163, 583.359871136023, 582.6991031426264, 378.6078278442113, 613.5756762452102, 507.1913656930572, 523.4758586551706, 581.5298201332567, 539.9886517911931, 553.7030477451411, 524.9451239180646, 435.7963998931107, 484.53418369040656, 557.4114393476065, 556.9515439214323, 584.2580571293811, 592.8975179793961, 595.824425395313, 606.6370869630243, 602.50106510833, 570.5729022412077, 553.0788821603148, 549.2671836043968, 601.1957327918269, 604.8226820512753, 570.1689935519939, 580.4161712147497, 376.3552258695671, 556.5314567426274, 425.68188824061866, 626.009375749073, 344.0048092158429, 495.0189543999214, 524.4527110629434, 557.056933319272, 520.6282573974238, 484.60533046345563, 478.41859739566854, 514.9722662192082]
Elapsed: 0.09839783218275341~0.042219998340266705
Time per graph: 0.002017135406032258~0.0008607141259857497
Speed: 537.2178761415485~112.15973786860626
Total Time: 0.0939
best val loss: 0.34241750836372375 test_score: 0.9375

Testing...
Test loss: 0.3788 score: 0.8542 time: 0.09s
test Score 0.8542
Epoch Time List: [0.4212449110345915, 0.5271857319166884, 0.5045939111150801, 0.492961282026954, 0.40879769902676344, 0.3912614620057866, 0.43015367176849395, 0.4443825220223516, 0.45640196604654193, 0.3873873061966151, 0.3741483569610864, 0.4820109030697495, 0.7949949179310352, 0.7813072571298108, 0.43553718796465546, 0.3867250708863139, 0.5803953430149704, 0.4752067410154268, 0.4688927868846804, 0.4607895689550787, 0.4367142770206556, 1.1154394641052932, 0.5654596409294754, 0.810825151973404, 0.4323563320795074, 0.478389129973948, 0.421190754044801, 0.4155838399892673, 0.4811278688721359, 0.46429302205797285, 0.5240831370465457, 0.5193607191322371, 0.38791009690612555, 0.49677234806586057, 0.3966254881815985, 0.4279297968605533, 0.4073607779573649, 0.3760427579982206, 0.4039058299968019, 0.4024028111016378, 0.41380055306944996, 0.4881413849070668, 0.40292614803183824, 0.40139093180187047, 0.48272958991583437, 0.42867816297803074, 0.4844732191413641, 0.43657715409062803, 0.46886783197987825, 0.44167822611052543, 0.4157691851723939, 0.42949652683455497, 0.44942378299310803, 0.4286258270731196, 0.48536610195878893, 0.36718045512679964, 0.40361273498274386, 0.37861317687202245, 0.39824283588677645, 0.43770532903727144, 0.4698431669967249, 0.43789663014467806, 0.4032576299505308, 0.3689220598898828, 0.43919198599178344, 0.4830825360259041, 0.4170082969358191, 0.4025996328564361, 0.38329677504952997, 0.38652255409397185, 0.4208958549425006, 0.4271953519200906, 0.46432478399947286, 0.4538966630352661, 0.3930676350137219, 0.4250095688039437, 0.44373951805755496, 0.4056091670645401, 0.4341758170630783, 0.4251074221683666, 0.44669476989656687, 0.4555058589903638, 0.48123304988257587, 0.3837094767950475, 0.36716674198396504, 0.3946649630088359, 0.4124348451150581, 0.4253365299664438, 0.5234826599480584, 0.4363730320474133, 0.44618152698967606, 0.3956343671306968, 0.4638734880136326, 0.414568395819515, 0.40346646995749325, 0.4714216369902715, 0.5405874439748004, 0.42509151285048574, 0.43989098409656435, 0.5164204899920151, 0.4367663179291412, 0.41439614992123097, 0.394873836892657, 0.5942892880411819, 0.4597208409104496, 0.38387727190274745, 0.4145392399514094, 0.4340950589394197, 0.4275643980363384, 0.39800347085110843, 0.4305488328682259, 0.5342699839966372, 0.414149675052613, 0.40049517003353685, 0.3788184850709513, 0.446846753009595, 0.45913387602195144, 0.45663639390841126, 0.49477732519153506, 0.46735296095721424, 0.554297913913615, 0.42759886698331684, 0.41837373294401914, 0.42151628609281033, 0.4289598420727998, 0.44633658311795443, 0.44224681006744504, 0.371776745072566, 0.389076828956604, 0.44044896692503244, 0.40719897602684796, 0.45967786107212305, 0.42134561797138304, 0.47259781090542674, 0.48003501491621137, 0.7322273909812793, 0.9506331189768389, 0.6096683921059594, 0.805354690994136, 0.5714075580472127, 0.5161520950496197, 0.7680894930381328, 0.5478530400432646, 0.5865656858077273, 0.4980839130003005, 0.42321646702475846, 0.46638813300523907, 0.4163969940273091, 0.3911454820772633, 0.40098854806274176, 0.40578288107644767, 0.4022480440326035, 0.4514731161762029, 0.456011704984121, 0.4425195718649775, 0.3996058478951454, 0.4282547030597925, 0.4233978260308504, 0.41965769010130316, 0.439913282985799, 0.42228455701842904, 0.4651081399060786, 0.4996827698778361, 0.48934924392960966, 0.437198753003031, 0.39887043787166476, 0.3904931651195511, 0.39360860316082835, 0.4033818020252511, 0.40185076009947807, 0.41956716391723603, 0.44422942004166543, 0.4273585540940985, 0.4242787380935624, 0.38939285511150956, 0.40666957094799727, 0.42980942199938, 0.45562143600545824, 0.4730041289003566, 0.4166768469149247, 0.3928396790288389, 0.47218296804931015, 0.4465527260908857, 0.45240753912366927, 0.39795510203111917, 0.39485330414026976, 0.4192261549178511, 0.580433674971573, 0.49528398597612977]
Total Epoch List: [96, 49, 44]
Total Time List: [0.07627350999973714, 0.13472878793254495, 0.09393147402442992]
========================training times:2========================
========================k_idx:0========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
emb dim:  64
Model: Ethident
Ethident(
  (encoder): HGATE_encoder(
    (lin1): Linear(in_features=14887, out_features=64, bias=True)
    (account_convs): ModuleList(
      (0): GATEConv()
      (1): GATConv(64, 64, heads=1)
    )
    (subgraph_conv): GATConv(64, 64, heads=1)
    (lin2): Linear(in_features=64, out_features=64, bias=True)
    (lin3): Linear(in_features=64, out_features=2, bias=True)
  )
  (proj_head_g1): Project_Head(
    (block): Sequential(
      (0): Linear(in_features=64, out_features=64, bias=True)
      (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=64, bias=True)
      (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
      (5): ReLU()
    )
    (linear_shortcut): Linear(in_features=64, out_features=64, bias=True)
  )
  (proj_head_g2): Project_Head(
    (block): Sequential(
      (0): Linear(in_features=64, out_features=64, bias=True)
      (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=64, bias=True)
      (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
      (5): ReLU()
    )
    (linear_shortcut): Linear(in_features=64, out_features=64, bias=True)
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=64, bias=True)
    (1): ReLU()
    (2): Dropout(p=0.0, inplace=True)
    (3): Linear(in_features=64, out_features=2, bias=True)
  )
)
Total number of parameters: 1003972
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7720be267ca0>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.6520;  Loss pred: 0.6197; Loss self: 3.2294; time: 0.22s
Val loss: 0.6066 score: 0.5714 time: 0.07s
Test loss: 0.6075 score: 0.6327 time: 0.06s
Epoch 2/1000, LR 0.000000
Train loss: 0.6520;  Loss pred: 0.6197; Loss self: 3.2294; time: 0.21s
Val loss: 0.5922 score: 0.6939 time: 0.07s
Test loss: 0.5899 score: 0.6531 time: 0.06s
Epoch 3/1000, LR 0.000030
Train loss: 0.6291;  Loss pred: 0.5974; Loss self: 3.1695; time: 0.21s
Val loss: 0.5709 score: 0.7551 time: 0.07s
Test loss: 0.5679 score: 0.8367 time: 0.06s
Epoch 4/1000, LR 0.000060
Train loss: 0.5959;  Loss pred: 0.5655; Loss self: 3.0416; time: 0.21s
Val loss: 0.5229 score: 0.8776 time: 0.07s
Test loss: 0.5402 score: 0.7959 time: 0.07s
Epoch 5/1000, LR 0.000090
Train loss: 0.5562;  Loss pred: 0.5270; Loss self: 2.9134; time: 0.21s
Val loss: 0.4600 score: 0.8776 time: 0.08s
Test loss: 0.4989 score: 0.8367 time: 0.07s
Epoch 6/1000, LR 0.000120
Train loss: 0.5013;  Loss pred: 0.4727; Loss self: 2.8661; time: 0.21s
Val loss: 0.4079 score: 0.9184 time: 0.08s
Test loss: 0.4619 score: 0.8776 time: 0.06s
Epoch 7/1000, LR 0.000150
Train loss: 0.4396;  Loss pred: 0.4115; Loss self: 2.8158; time: 0.21s
Val loss: 0.3626 score: 0.9388 time: 0.07s
Test loss: 0.4325 score: 0.8571 time: 0.07s
Epoch 8/1000, LR 0.000180
Train loss: 0.3855;  Loss pred: 0.3577; Loss self: 2.7790; time: 0.21s
Val loss: 0.3490 score: 0.9184 time: 0.07s
Test loss: 0.4196 score: 0.8571 time: 0.06s
Epoch 9/1000, LR 0.000210
Train loss: 0.3614;  Loss pred: 0.3340; Loss self: 2.7339; time: 0.22s
Val loss: 0.3386 score: 0.9184 time: 0.07s
Test loss: 0.3952 score: 0.8980 time: 0.09s
Epoch 10/1000, LR 0.000240
Train loss: 0.3319;  Loss pred: 0.3049; Loss self: 2.7003; time: 0.23s
Val loss: 0.2780 score: 0.9184 time: 0.07s
Test loss: 0.3640 score: 0.8776 time: 0.06s
Epoch 11/1000, LR 0.000270
Train loss: 0.2897;  Loss pred: 0.2626; Loss self: 2.7018; time: 0.21s
Val loss: 0.2544 score: 0.9184 time: 0.07s
Test loss: 0.3399 score: 0.8776 time: 0.06s
Epoch 12/1000, LR 0.000270
Train loss: 0.2516;  Loss pred: 0.2244; Loss self: 2.7207; time: 0.21s
Val loss: 0.2362 score: 0.9184 time: 0.09s
Test loss: 0.3289 score: 0.8980 time: 0.06s
Epoch 13/1000, LR 0.000270
Train loss: 0.2259;  Loss pred: 0.1984; Loss self: 2.7483; time: 0.22s
Val loss: 0.2022 score: 0.9592 time: 0.07s
Test loss: 0.3107 score: 0.8776 time: 0.07s
Epoch 14/1000, LR 0.000270
Train loss: 0.2019;  Loss pred: 0.1742; Loss self: 2.7728; time: 0.22s
Val loss: 0.1832 score: 0.9592 time: 0.08s
Test loss: 0.3014 score: 0.8980 time: 0.06s
Epoch 15/1000, LR 0.000270
Train loss: 0.1862;  Loss pred: 0.1582; Loss self: 2.7994; time: 0.22s
Val loss: 0.1734 score: 0.9592 time: 0.07s
Test loss: 0.2937 score: 0.8776 time: 0.06s
Epoch 16/1000, LR 0.000270
Train loss: 0.1700;  Loss pred: 0.1419; Loss self: 2.8094; time: 0.21s
Val loss: 0.1711 score: 0.9388 time: 0.09s
Test loss: 0.2957 score: 0.9184 time: 0.07s
Epoch 17/1000, LR 0.000270
Train loss: 0.1588;  Loss pred: 0.1307; Loss self: 2.8128; time: 0.21s
Val loss: 0.1622 score: 0.9388 time: 0.07s
Test loss: 0.2926 score: 0.9184 time: 0.07s
Epoch 18/1000, LR 0.000270
Train loss: 0.1480;  Loss pred: 0.1198; Loss self: 2.8165; time: 0.24s
Val loss: 0.1477 score: 0.9388 time: 0.08s
Test loss: 0.2824 score: 0.8980 time: 0.07s
Epoch 19/1000, LR 0.000270
Train loss: 0.1372;  Loss pred: 0.1089; Loss self: 2.8214; time: 0.22s
Val loss: 0.1348 score: 0.9592 time: 0.08s
Test loss: 0.2745 score: 0.8776 time: 0.06s
Epoch 20/1000, LR 0.000270
Train loss: 0.1288;  Loss pred: 0.1005; Loss self: 2.8254; time: 0.22s
Val loss: 0.1259 score: 0.9592 time: 0.07s
Test loss: 0.2702 score: 0.8776 time: 0.07s
Epoch 21/1000, LR 0.000270
Train loss: 0.1216;  Loss pred: 0.0933; Loss self: 2.8260; time: 0.24s
Val loss: 0.1196 score: 0.9592 time: 0.08s
Test loss: 0.2677 score: 0.8776 time: 0.07s
Epoch 22/1000, LR 0.000270
Train loss: 0.1148;  Loss pred: 0.0866; Loss self: 2.8258; time: 0.23s
Val loss: 0.1141 score: 0.9592 time: 0.07s
Test loss: 0.2660 score: 0.8980 time: 0.07s
Epoch 23/1000, LR 0.000270
Train loss: 0.1090;  Loss pred: 0.0807; Loss self: 2.8264; time: 0.22s
Val loss: 0.1089 score: 0.9592 time: 0.08s
Test loss: 0.2642 score: 0.8980 time: 0.07s
Epoch 24/1000, LR 0.000270
Train loss: 0.1037;  Loss pred: 0.0754; Loss self: 2.8296; time: 0.22s
Val loss: 0.1037 score: 0.9796 time: 0.08s
Test loss: 0.2615 score: 0.8980 time: 0.07s
Epoch 25/1000, LR 0.000270
Train loss: 0.0984;  Loss pred: 0.0701; Loss self: 2.8332; time: 0.22s
Val loss: 0.0988 score: 0.9796 time: 0.08s
Test loss: 0.2587 score: 0.8980 time: 0.07s
Epoch 26/1000, LR 0.000270
Train loss: 0.0933;  Loss pred: 0.0649; Loss self: 2.8363; time: 0.22s
Val loss: 0.0950 score: 0.9796 time: 0.07s
Test loss: 0.2570 score: 0.8980 time: 0.07s
Epoch 27/1000, LR 0.000270
Train loss: 0.0882;  Loss pred: 0.0598; Loss self: 2.8389; time: 0.22s
Val loss: 0.0925 score: 0.9796 time: 0.07s
Test loss: 0.2566 score: 0.8980 time: 0.09s
Epoch 28/1000, LR 0.000270
Train loss: 0.0838;  Loss pred: 0.0554; Loss self: 2.8412; time: 0.22s
Val loss: 0.0903 score: 0.9796 time: 0.08s
Test loss: 0.2568 score: 0.8980 time: 0.07s
Epoch 29/1000, LR 0.000270
Train loss: 0.0800;  Loss pred: 0.0516; Loss self: 2.8428; time: 0.22s
Val loss: 0.0875 score: 0.9796 time: 0.07s
Test loss: 0.2566 score: 0.8980 time: 0.07s
Epoch 30/1000, LR 0.000270
Train loss: 0.0763;  Loss pred: 0.0479; Loss self: 2.8446; time: 0.22s
Val loss: 0.0842 score: 0.9796 time: 0.07s
Test loss: 0.2552 score: 0.8980 time: 0.07s
Epoch 31/1000, LR 0.000270
Train loss: 0.0725;  Loss pred: 0.0440; Loss self: 2.8477; time: 0.22s
Val loss: 0.0806 score: 0.9796 time: 0.07s
Test loss: 0.2530 score: 0.8980 time: 0.07s
Epoch 32/1000, LR 0.000270
Train loss: 0.0690;  Loss pred: 0.0405; Loss self: 2.8522; time: 0.22s
Val loss: 0.0771 score: 0.9796 time: 0.08s
Test loss: 0.2504 score: 0.8980 time: 0.07s
Epoch 33/1000, LR 0.000270
Train loss: 0.0659;  Loss pred: 0.0373; Loss self: 2.8571; time: 0.22s
Val loss: 0.0736 score: 0.9796 time: 0.07s
Test loss: 0.2485 score: 0.8980 time: 0.07s
Epoch 34/1000, LR 0.000270
Train loss: 0.0632;  Loss pred: 0.0346; Loss self: 2.8623; time: 0.22s
Val loss: 0.0705 score: 0.9796 time: 0.07s
Test loss: 0.2479 score: 0.9184 time: 0.07s
Epoch 35/1000, LR 0.000270
Train loss: 0.0610;  Loss pred: 0.0324; Loss self: 2.8677; time: 0.26s
Val loss: 0.0677 score: 0.9796 time: 0.07s
Test loss: 0.2479 score: 0.9184 time: 0.07s
Epoch 36/1000, LR 0.000270
Train loss: 0.0590;  Loss pred: 0.0303; Loss self: 2.8731; time: 0.26s
Val loss: 0.0653 score: 0.9796 time: 0.07s
Test loss: 0.2483 score: 0.9184 time: 0.07s
Epoch 37/1000, LR 0.000270
Train loss: 0.0572;  Loss pred: 0.0284; Loss self: 2.8784; time: 0.21s
Val loss: 0.0633 score: 0.9796 time: 0.07s
Test loss: 0.2490 score: 0.9184 time: 0.09s
Epoch 38/1000, LR 0.000270
Train loss: 0.0555;  Loss pred: 0.0267; Loss self: 2.8838; time: 0.22s
Val loss: 0.0616 score: 0.9796 time: 0.07s
Test loss: 0.2501 score: 0.9184 time: 0.06s
Epoch 39/1000, LR 0.000269
Train loss: 0.0539;  Loss pred: 0.0250; Loss self: 2.8893; time: 0.22s
Val loss: 0.0603 score: 0.9796 time: 0.07s
Test loss: 0.2514 score: 0.9184 time: 0.07s
Epoch 40/1000, LR 0.000269
Train loss: 0.0526;  Loss pred: 0.0236; Loss self: 2.8947; time: 0.22s
Val loss: 0.0592 score: 0.9796 time: 0.08s
Test loss: 0.2525 score: 0.9184 time: 0.07s
Epoch 41/1000, LR 0.000269
Train loss: 0.0514;  Loss pred: 0.0224; Loss self: 2.9001; time: 0.21s
Val loss: 0.0581 score: 0.9796 time: 0.07s
Test loss: 0.2536 score: 0.9184 time: 0.07s
Epoch 42/1000, LR 0.000269
Train loss: 0.0504;  Loss pred: 0.0213; Loss self: 2.9054; time: 0.22s
Val loss: 0.0570 score: 0.9796 time: 0.08s
Test loss: 0.2546 score: 0.9184 time: 0.07s
Epoch 43/1000, LR 0.000269
Train loss: 0.0493;  Loss pred: 0.0202; Loss self: 2.9107; time: 0.22s
Val loss: 0.0557 score: 0.9796 time: 0.08s
Test loss: 0.2556 score: 0.9184 time: 0.07s
Epoch 44/1000, LR 0.000269
Train loss: 0.0484;  Loss pred: 0.0192; Loss self: 2.9159; time: 1.03s
Val loss: 0.0542 score: 0.9796 time: 0.51s
Test loss: 0.2566 score: 0.9184 time: 0.07s
Epoch 45/1000, LR 0.000269
Train loss: 0.0475;  Loss pred: 0.0182; Loss self: 2.9210; time: 0.21s
Val loss: 0.0528 score: 0.9796 time: 0.08s
Test loss: 0.2577 score: 0.9184 time: 0.07s
Epoch 46/1000, LR 0.000269
Train loss: 0.0466;  Loss pred: 0.0173; Loss self: 2.9259; time: 0.22s
Val loss: 0.0514 score: 0.9796 time: 0.07s
Test loss: 0.2591 score: 0.9184 time: 0.06s
Epoch 47/1000, LR 0.000269
Train loss: 0.0458;  Loss pred: 0.0165; Loss self: 2.9306; time: 0.22s
Val loss: 0.0501 score: 0.9796 time: 0.07s
Test loss: 0.2607 score: 0.9184 time: 0.07s
Epoch 48/1000, LR 0.000269
Train loss: 0.0450;  Loss pred: 0.0157; Loss self: 2.9351; time: 0.22s
Val loss: 0.0490 score: 0.9796 time: 0.07s
Test loss: 0.2626 score: 0.9184 time: 0.06s
Epoch 49/1000, LR 0.000269
Train loss: 0.0443;  Loss pred: 0.0149; Loss self: 2.9394; time: 0.22s
Val loss: 0.0481 score: 0.9796 time: 0.08s
Test loss: 0.2645 score: 0.9184 time: 0.08s
Epoch 50/1000, LR 0.000269
Train loss: 0.0437;  Loss pred: 0.0142; Loss self: 2.9432; time: 0.21s
Val loss: 0.0473 score: 0.9796 time: 0.08s
Test loss: 0.2665 score: 0.9184 time: 0.07s
Epoch 51/1000, LR 0.000269
Train loss: 0.0431;  Loss pred: 0.0136; Loss self: 2.9466; time: 0.22s
Val loss: 0.0465 score: 0.9796 time: 0.07s
Test loss: 0.2682 score: 0.9184 time: 0.07s
Epoch 52/1000, LR 0.000269
Train loss: 0.0425;  Loss pred: 0.0130; Loss self: 2.9496; time: 0.22s
Val loss: 0.0459 score: 0.9796 time: 0.09s
Test loss: 0.2695 score: 0.9184 time: 0.06s
Epoch 53/1000, LR 0.000269
Train loss: 0.0420;  Loss pred: 0.0125; Loss self: 2.9523; time: 0.21s
Val loss: 0.0454 score: 0.9796 time: 0.08s
Test loss: 0.2706 score: 0.9184 time: 0.07s
Epoch 54/1000, LR 0.000269
Train loss: 0.0415;  Loss pred: 0.0120; Loss self: 2.9546; time: 0.22s
Val loss: 0.0449 score: 0.9796 time: 0.07s
Test loss: 0.2715 score: 0.9184 time: 0.07s
Epoch 55/1000, LR 0.000269
Train loss: 0.0411;  Loss pred: 0.0115; Loss self: 2.9567; time: 0.21s
Val loss: 0.0445 score: 0.9796 time: 0.07s
Test loss: 0.2722 score: 0.9184 time: 0.07s
Epoch 56/1000, LR 0.000269
Train loss: 0.0407;  Loss pred: 0.0111; Loss self: 2.9584; time: 0.22s
Val loss: 0.0441 score: 0.9796 time: 0.08s
Test loss: 0.2729 score: 0.9184 time: 0.07s
Epoch 57/1000, LR 0.000269
Train loss: 0.0403;  Loss pred: 0.0107; Loss self: 2.9600; time: 0.21s
Val loss: 0.0439 score: 0.9796 time: 0.08s
Test loss: 0.2735 score: 0.9184 time: 0.07s
Epoch 58/1000, LR 0.000269
Train loss: 0.0400;  Loss pred: 0.0104; Loss self: 2.9614; time: 0.22s
Val loss: 0.0438 score: 0.9796 time: 0.07s
Test loss: 0.2742 score: 0.9184 time: 0.07s
Epoch 59/1000, LR 0.000268
Train loss: 0.0397;  Loss pred: 0.0101; Loss self: 2.9625; time: 0.23s
Val loss: 0.0438 score: 0.9796 time: 0.08s
Test loss: 0.2751 score: 0.9184 time: 0.07s
     INFO: Early stopping counter 1 of 20
Epoch 60/1000, LR 0.000268
Train loss: 0.0394;  Loss pred: 0.0098; Loss self: 2.9635; time: 0.24s
Val loss: 0.0440 score: 0.9796 time: 0.09s
Test loss: 0.2760 score: 0.9184 time: 0.07s
     INFO: Early stopping counter 2 of 20
Epoch 61/1000, LR 0.000268
Train loss: 0.0391;  Loss pred: 0.0095; Loss self: 2.9643; time: 0.22s
Val loss: 0.0442 score: 0.9796 time: 0.08s
Test loss: 0.2770 score: 0.9184 time: 0.07s
     INFO: Early stopping counter 3 of 20
Epoch 62/1000, LR 0.000268
Train loss: 0.0388;  Loss pred: 0.0092; Loss self: 2.9650; time: 0.23s
Val loss: 0.0445 score: 0.9796 time: 0.07s
Test loss: 0.2782 score: 0.9184 time: 0.07s
     INFO: Early stopping counter 4 of 20
Epoch 63/1000, LR 0.000268
Train loss: 0.0386;  Loss pred: 0.0089; Loss self: 2.9655; time: 0.22s
Val loss: 0.0448 score: 0.9796 time: 0.07s
Test loss: 0.2796 score: 0.9184 time: 0.07s
     INFO: Early stopping counter 5 of 20
Epoch 64/1000, LR 0.000268
Train loss: 0.0384;  Loss pred: 0.0087; Loss self: 2.9660; time: 0.22s
Val loss: 0.0451 score: 0.9796 time: 0.07s
Test loss: 0.2809 score: 0.8980 time: 0.10s
     INFO: Early stopping counter 6 of 20
Epoch 65/1000, LR 0.000268
Train loss: 0.0381;  Loss pred: 0.0085; Loss self: 2.9664; time: 0.23s
Val loss: 0.0454 score: 0.9796 time: 0.08s
Test loss: 0.2823 score: 0.8980 time: 0.07s
     INFO: Early stopping counter 7 of 20
Epoch 66/1000, LR 0.000268
Train loss: 0.0379;  Loss pred: 0.0082; Loss self: 2.9668; time: 0.24s
Val loss: 0.0457 score: 0.9796 time: 0.08s
Test loss: 0.2837 score: 0.8980 time: 0.07s
     INFO: Early stopping counter 8 of 20
Epoch 67/1000, LR 0.000268
Train loss: 0.0377;  Loss pred: 0.0080; Loss self: 2.9671; time: 0.24s
Val loss: 0.0460 score: 0.9796 time: 0.11s
Test loss: 0.2852 score: 0.8980 time: 0.07s
     INFO: Early stopping counter 9 of 20
Epoch 68/1000, LR 0.000268
Train loss: 0.0375;  Loss pred: 0.0078; Loss self: 2.9674; time: 0.22s
Val loss: 0.0463 score: 0.9796 time: 0.07s
Test loss: 0.2866 score: 0.8980 time: 0.07s
     INFO: Early stopping counter 10 of 20
Epoch 69/1000, LR 0.000268
Train loss: 0.0373;  Loss pred: 0.0076; Loss self: 2.9676; time: 0.22s
Val loss: 0.0465 score: 0.9796 time: 0.07s
Test loss: 0.2881 score: 0.8980 time: 0.07s
     INFO: Early stopping counter 11 of 20
Epoch 70/1000, LR 0.000268
Train loss: 0.0371;  Loss pred: 0.0074; Loss self: 2.9677; time: 0.22s
Val loss: 0.0468 score: 0.9796 time: 0.09s
Test loss: 0.2896 score: 0.8980 time: 0.07s
     INFO: Early stopping counter 12 of 20
Epoch 71/1000, LR 0.000268
Train loss: 0.0369;  Loss pred: 0.0073; Loss self: 2.9678; time: 0.25s
Val loss: 0.0469 score: 0.9796 time: 0.08s
Test loss: 0.2911 score: 0.8980 time: 0.07s
     INFO: Early stopping counter 13 of 20
Epoch 72/1000, LR 0.000267
Train loss: 0.0368;  Loss pred: 0.0071; Loss self: 2.9678; time: 0.22s
Val loss: 0.0470 score: 0.9796 time: 0.08s
Test loss: 0.2925 score: 0.8980 time: 0.26s
     INFO: Early stopping counter 14 of 20
Epoch 73/1000, LR 0.000267
Train loss: 0.0366;  Loss pred: 0.0069; Loss self: 2.9678; time: 0.24s
Val loss: 0.0471 score: 0.9796 time: 0.07s
Test loss: 0.2939 score: 0.8980 time: 0.07s
     INFO: Early stopping counter 15 of 20
Epoch 74/1000, LR 0.000267
Train loss: 0.0365;  Loss pred: 0.0068; Loss self: 2.9677; time: 0.21s
Val loss: 0.0471 score: 0.9796 time: 0.07s
Test loss: 0.2953 score: 0.8980 time: 0.07s
     INFO: Early stopping counter 16 of 20
Epoch 75/1000, LR 0.000267
Train loss: 0.0363;  Loss pred: 0.0067; Loss self: 2.9675; time: 0.22s
Val loss: 0.0471 score: 0.9796 time: 0.07s
Test loss: 0.2966 score: 0.8980 time: 0.07s
     INFO: Early stopping counter 17 of 20
Epoch 76/1000, LR 0.000267
Train loss: 0.0362;  Loss pred: 0.0065; Loss self: 2.9673; time: 0.22s
Val loss: 0.0472 score: 0.9796 time: 0.08s
Test loss: 0.2979 score: 0.8980 time: 0.06s
     INFO: Early stopping counter 18 of 20
Epoch 77/1000, LR 0.000267
Train loss: 0.0361;  Loss pred: 0.0064; Loss self: 2.9670; time: 0.22s
Val loss: 0.0472 score: 0.9796 time: 0.18s
Test loss: 0.2994 score: 0.8980 time: 0.07s
     INFO: Early stopping counter 19 of 20
Epoch 78/1000, LR 0.000267
Train loss: 0.0359;  Loss pred: 0.0063; Loss self: 2.9666; time: 0.24s
Val loss: 0.0474 score: 0.9796 time: 0.07s
Test loss: 0.3009 score: 0.8980 time: 0.06s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 057,   Train_Loss: 0.0400,   Val_Loss: 0.0438,   Val_Precision: 0.9600,   Val_Recall: 1.0000,   Val_accuracy: 0.9796,   Val_Score: 0.9796,   Val_Loss: 0.0438,   Test_Precision: 0.9565,   Test_Recall: 0.8800,   Test_accuracy: 0.9167,   Test_Score: 0.9184,   Test_loss: 0.2742


[0.06706651102285832, 0.06757586705498397, 0.06646898796316236, 0.0718316410202533, 0.07510654698126018, 0.06597159092780203, 0.0738949979422614, 0.06910502398386598, 0.09055500500835478, 0.06415757001377642, 0.06753980997018516, 0.06899847893510014, 0.07175968203227967, 0.06883948598988354, 0.06861220404971391, 0.07066568697337061, 0.07064964005257934, 0.07683709205593914, 0.06878010695800185, 0.07513271307107061, 0.07189878693316132, 0.06974977103527635, 0.07041120098438114, 0.07132892298977822, 0.07135446299798787, 0.07516509795095772, 0.08959802996832877, 0.07486275199335068, 0.06945901794824749, 0.07497779105324298, 0.07058932690415531, 0.06991016096435487, 0.07083920098375529, 0.0784491429803893, 0.07159295899327844, 0.06988748302683234, 0.09646255092229694, 0.0691754330182448, 0.07061747496481985, 0.07159606297500432, 0.07007017196156085, 0.07290084101259708, 0.07912946597207338, 0.07725238509010524, 0.07029037794563919, 0.06916264398023486, 0.07487984898034483, 0.06928547192364931, 0.08146865293383598, 0.06990542495623231, 0.06993303389754146, 0.06905998103320599, 0.07151305093429983, 0.06959297799039632, 0.07453786896076053, 0.07188723003491759, 0.0700574970105663, 0.07670035399496555, 0.07673974707722664, 0.07525623601395637, 0.07201656396500766, 0.07621234399266541, 0.07164364994969219, 0.10028795804828405, 0.07181643997319043, 0.07650209800340235, 0.07165027293376625, 0.0698749260045588, 0.06995400902815163, 0.07424850098323077, 0.06981106102466583, 0.26665062201209366, 0.07301209506113082, 0.06954951991792768, 0.07559041702188551, 0.06907626101747155, 0.07111492694821209, 0.06783055700361729]
[0.0013687043065889453, 0.001379099327652734, 0.001356509958431885, 0.0014659518575561897, 0.0015327866730869425, 0.001346358998526572, 0.0015080611824951305, 0.0014103066119156322, 0.001848061326701118, 0.0013093381635464576, 0.001378363468779289, 0.001408132223165309, 0.0014644833067812178, 0.0014048874691812967, 0.0014002490622390593, 0.0014421568770075636, 0.00144182938882815, 0.001568103919508962, 0.0014036756522041193, 0.0015333206749198083, 0.0014673221823094146, 0.00142346471500564, 0.0014369632853955335, 0.0014556923059138413, 0.001456213530571181, 0.0015339815908358718, 0.0018285312238434444, 0.001527811265170422, 0.001417530978535663, 0.0015301590010865914, 0.0014405985082480677, 0.0014267379788643851, 0.001445697979260312, 0.0016010029179671286, 0.0014610807957811927, 0.001426275163812905, 0.0019686234882101417, 0.001411743530984588, 0.0014411729584657112, 0.001461144142347027, 0.0014300035094196091, 0.0014877722655632058, 0.0016148870606545586, 0.0015765792875531682, 0.0014344975090946775, 0.0014114825302088748, 0.0015281601832723435, 0.0014139892229316185, 0.0016626255700782854, 0.001426641325637394, 0.0014272047734192135, 0.0014093873680246119, 0.0014594500190673433, 0.0014202648569468636, 0.0015211809991991945, 0.0014670863272432163, 0.0014297448369503325, 0.0015653133468360317, 0.0015661172872903396, 0.001535841551305232, 0.001469725795204238, 0.001555353959033988, 0.001462115305095759, 0.0020466930213935523, 0.001465641632105927, 0.0015612673061918846, 0.001462250468036046, 0.0014260188980522205, 0.001427632837309217, 0.0015152755302700158, 0.0014247155311156291, 0.005441849428818238, 0.0014900427563496086, 0.0014193779575087282, 0.0015426615718752146, 0.0014097196126014603, 0.0014513250397594304, 0.0013842970817064752]
[730.6179977559784, 725.1109328738693, 737.185889262466, 682.1506414726653, 652.4065074144066, 742.7439494922081, 663.1030700926015, 709.0656680973019, 541.1075842299292, 763.7446366730899, 725.4980436224296, 710.1605826135577, 682.8346867250376, 711.8007825799411, 714.1586643171582, 693.4058395054586, 693.5633354045809, 637.712837496855, 712.415292257689, 652.1792971012403, 681.5135844440807, 702.5112666709396, 695.9120042686014, 686.958360594088, 686.7124765746154, 651.8983056733404, 546.8870243834667, 654.5311078645918, 705.4519549428255, 653.5268552417646, 694.1559319092412, 700.8995448456149, 691.7074066269689, 624.6084805827517, 684.4248469266427, 701.1269812247656, 507.9691500121198, 708.3439577035449, 693.8792419923083, 684.3951743143604, 699.2989831233818, 672.1458808895349, 619.2383506959751, 634.2846236119135, 697.1082164033229, 708.4749393618195, 654.3816616518816, 707.2189687038093, 601.4583307250078, 700.9470299433675, 700.6703022750257, 709.5281415794108, 685.1896172772307, 704.0940252156172, 657.3839671455512, 681.6231474797313, 699.4255017790554, 638.8497242557219, 638.521781296583, 651.1088329067227, 680.3990263102354, 642.9404665039, 683.9405869802495, 488.59305696910036, 682.2950290809742, 640.5053100350369, 683.877367017091, 701.2529787409453, 700.4602120842123, 659.9459834356358, 701.8945032605533, 183.76105643502927, 671.1216814005102, 704.533978923546, 648.2303171553234, 709.3609190515735, 689.0255267460683, 722.3882887676555]
Elapsed: 0.07530691995112321~0.022610843569160974
Time per graph: 0.0015368759173698615~0.00046144578712573406
Speed: 670.4340796282486~72.73960207867853
Total Time: 0.0683
best val loss: 0.043789733201265335 test_score: 0.9184

Testing...
Test loss: 0.2615 score: 0.8980 time: 0.07s
test Score 0.8980
Epoch Time List: [0.3494270759401843, 0.34380863595288247, 0.3430066700093448, 0.34510894503910094, 0.3538594759302214, 0.3449007129529491, 0.3516954150982201, 0.3470907580340281, 0.3743883731076494, 0.36526425986085087, 0.34738179109990597, 0.36776701803319156, 0.35642679885495454, 0.3598030429566279, 0.35898670786991715, 0.36538390698842704, 0.35066479607485235, 0.3955443089362234, 0.36012659408152103, 0.3658252478344366, 0.38421811489388347, 0.3636845810106024, 0.35787462804000825, 0.36612298304680735, 0.3634975559543818, 0.366100546089001, 0.3762538230512291, 0.36748302006162703, 0.3554438811261207, 0.3686965440865606, 0.3575703620444983, 0.3731384068960324, 0.35569762578234076, 0.3652215781621635, 0.40370197186712176, 0.39809092006180435, 0.3779113250784576, 0.3542541960487142, 0.35896442201919854, 0.3632456319173798, 0.35101814405061305, 0.3714863839559257, 0.37545699009206146, 1.6109206308610737, 0.3510264160577208, 0.35510507912840694, 0.35938038281165063, 0.3578358730301261, 0.37876235297881067, 0.35379342711530626, 0.3546503980178386, 0.36985121108591557, 0.358359741861932, 0.3560706239659339, 0.35667481494601816, 0.36693616094999015, 0.354300226084888, 0.3648258730536327, 0.37712174793705344, 0.40089241205714643, 0.3634860800812021, 0.37497312808409333, 0.3626420749351382, 0.3927286311518401, 0.372055894928053, 0.38454131584148854, 0.41370646690484136, 0.3587131960084662, 0.35741325397975743, 0.3880235581891611, 0.3914188191993162, 0.564742439892143, 0.37826573697384447, 0.3523060920415446, 0.361281278077513, 0.36591598810628057, 0.47198546899016947, 0.3729447789955884]
Total Epoch List: [78]
Total Time List: [0.06832576694432646]
========================k_idx:1========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
emb dim:  64
Model: Ethident
Ethident(
  (encoder): HGATE_encoder(
    (lin1): Linear(in_features=14887, out_features=64, bias=True)
    (account_convs): ModuleList(
      (0): GATEConv()
      (1): GATConv(64, 64, heads=1)
    )
    (subgraph_conv): GATConv(64, 64, heads=1)
    (lin2): Linear(in_features=64, out_features=64, bias=True)
    (lin3): Linear(in_features=64, out_features=2, bias=True)
  )
  (proj_head_g1): Project_Head(
    (block): Sequential(
      (0): Linear(in_features=64, out_features=64, bias=True)
      (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=64, bias=True)
      (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
      (5): ReLU()
    )
    (linear_shortcut): Linear(in_features=64, out_features=64, bias=True)
  )
  (proj_head_g2): Project_Head(
    (block): Sequential(
      (0): Linear(in_features=64, out_features=64, bias=True)
      (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=64, bias=True)
      (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
      (5): ReLU()
    )
    (linear_shortcut): Linear(in_features=64, out_features=64, bias=True)
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=64, bias=True)
    (1): ReLU()
    (2): Dropout(p=0.0, inplace=True)
    (3): Linear(in_features=64, out_features=2, bias=True)
  )
)
Total number of parameters: 1003972
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7720be2669b0>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.7961;  Loss pred: 0.7629; Loss self: 3.3105; time: 0.22s
Val loss: 0.7681 score: 0.4694 time: 0.06s
Test loss: 0.7755 score: 0.4694 time: 0.08s
Epoch 2/1000, LR 0.000000
Train loss: 0.7961;  Loss pred: 0.7629; Loss self: 3.3105; time: 0.24s
Val loss: 0.7124 score: 0.5102 time: 0.07s
Test loss: 0.7135 score: 0.4898 time: 0.08s
Epoch 3/1000, LR 0.000030
Train loss: 0.7204;  Loss pred: 0.6875; Loss self: 3.2872; time: 0.23s
Val loss: 0.6438 score: 0.6939 time: 0.06s
Test loss: 0.6528 score: 0.6735 time: 0.07s
Epoch 4/1000, LR 0.000060
Train loss: 0.6430;  Loss pred: 0.6106; Loss self: 3.2430; time: 0.22s
Val loss: 0.6292 score: 0.6122 time: 0.06s
Test loss: 0.6419 score: 0.5918 time: 0.07s
Epoch 5/1000, LR 0.000090
Train loss: 0.6305;  Loss pred: 0.5981; Loss self: 3.2395; time: 0.26s
Val loss: 0.6371 score: 0.6531 time: 0.52s
Test loss: 0.6393 score: 0.6327 time: 0.23s
     INFO: Early stopping counter 1 of 20
Epoch 6/1000, LR 0.000120
Train loss: 0.6321;  Loss pred: 0.5993; Loss self: 3.2858; time: 0.31s
Val loss: 0.6186 score: 0.6122 time: 0.28s
Test loss: 0.6110 score: 0.6735 time: 0.20s
Epoch 7/1000, LR 0.000150
Train loss: 0.6029;  Loss pred: 0.5696; Loss self: 3.3294; time: 0.28s
Val loss: 0.5843 score: 0.6735 time: 0.07s
Test loss: 0.5712 score: 0.7143 time: 0.08s
Epoch 8/1000, LR 0.000180
Train loss: 0.5628;  Loss pred: 0.5291; Loss self: 3.3650; time: 0.22s
Val loss: 0.5526 score: 0.7959 time: 0.06s
Test loss: 0.5266 score: 0.8571 time: 0.07s
Epoch 9/1000, LR 0.000210
Train loss: 0.5279;  Loss pred: 0.4941; Loss self: 3.3894; time: 0.24s
Val loss: 0.5264 score: 0.8163 time: 0.06s
Test loss: 0.4884 score: 0.8571 time: 0.07s
Epoch 10/1000, LR 0.000240
Train loss: 0.4952;  Loss pred: 0.4611; Loss self: 3.4041; time: 0.23s
Val loss: 0.4963 score: 0.8367 time: 0.07s
Test loss: 0.4506 score: 0.8776 time: 0.08s
Epoch 11/1000, LR 0.000270
Train loss: 0.4568;  Loss pred: 0.4229; Loss self: 3.3922; time: 0.23s
Val loss: 0.4772 score: 0.8367 time: 0.07s
Test loss: 0.4205 score: 0.8776 time: 0.08s
Epoch 12/1000, LR 0.000270
Train loss: 0.4273;  Loss pred: 0.3936; Loss self: 3.3698; time: 0.23s
Val loss: 0.4659 score: 0.7959 time: 0.07s
Test loss: 0.3970 score: 0.8776 time: 0.08s
Epoch 13/1000, LR 0.000270
Train loss: 0.4052;  Loss pred: 0.3717; Loss self: 3.3521; time: 0.23s
Val loss: 0.4600 score: 0.8163 time: 0.07s
Test loss: 0.3774 score: 0.8776 time: 0.07s
Epoch 14/1000, LR 0.000270
Train loss: 0.3802;  Loss pred: 0.3468; Loss self: 3.3369; time: 0.21s
Val loss: 0.4617 score: 0.8571 time: 0.07s
Test loss: 0.3642 score: 0.8980 time: 0.07s
     INFO: Early stopping counter 1 of 20
Epoch 15/1000, LR 0.000270
Train loss: 0.3605;  Loss pred: 0.3274; Loss self: 3.3184; time: 0.21s
Val loss: 0.4607 score: 0.8367 time: 0.07s
Test loss: 0.3503 score: 0.8980 time: 0.08s
     INFO: Early stopping counter 2 of 20
Epoch 16/1000, LR 0.000270
Train loss: 0.3423;  Loss pred: 0.3094; Loss self: 3.2892; time: 0.22s
Val loss: 0.4494 score: 0.8367 time: 0.07s
Test loss: 0.3316 score: 0.8980 time: 0.08s
Epoch 17/1000, LR 0.000270
Train loss: 0.3216;  Loss pred: 0.2891; Loss self: 3.2466; time: 0.22s
Val loss: 0.4322 score: 0.8367 time: 0.06s
Test loss: 0.3117 score: 0.8980 time: 0.08s
Epoch 18/1000, LR 0.000270
Train loss: 0.3029;  Loss pred: 0.2709; Loss self: 3.2010; time: 0.22s
Val loss: 0.4174 score: 0.8571 time: 0.06s
Test loss: 0.2950 score: 0.8980 time: 0.07s
Epoch 19/1000, LR 0.000270
Train loss: 0.2853;  Loss pred: 0.2536; Loss self: 3.1624; time: 0.21s
Val loss: 0.4080 score: 0.8571 time: 0.06s
Test loss: 0.2822 score: 0.9184 time: 0.07s
Epoch 20/1000, LR 0.000270
Train loss: 0.2682;  Loss pred: 0.2368; Loss self: 3.1320; time: 0.21s
Val loss: 0.4067 score: 0.8571 time: 0.06s
Test loss: 0.2726 score: 0.9184 time: 0.07s
Epoch 21/1000, LR 0.000270
Train loss: 0.2530;  Loss pred: 0.2219; Loss self: 3.1100; time: 0.31s
Val loss: 0.4095 score: 0.8163 time: 0.07s
Test loss: 0.2665 score: 0.8980 time: 0.09s
     INFO: Early stopping counter 1 of 20
Epoch 22/1000, LR 0.000270
Train loss: 0.2403;  Loss pred: 0.2094; Loss self: 3.0936; time: 0.21s
Val loss: 0.4113 score: 0.8163 time: 0.06s
Test loss: 0.2593 score: 0.9184 time: 0.07s
     INFO: Early stopping counter 2 of 20
Epoch 23/1000, LR 0.000270
Train loss: 0.2281;  Loss pred: 0.1973; Loss self: 3.0787; time: 0.21s
Val loss: 0.4095 score: 0.8163 time: 0.06s
Test loss: 0.2486 score: 0.9184 time: 0.07s
     INFO: Early stopping counter 3 of 20
Epoch 24/1000, LR 0.000270
Train loss: 0.2156;  Loss pred: 0.1850; Loss self: 3.0627; time: 0.21s
Val loss: 0.4053 score: 0.8367 time: 0.06s
Test loss: 0.2366 score: 0.9184 time: 0.08s
Epoch 25/1000, LR 0.000270
Train loss: 0.2039;  Loss pred: 0.1735; Loss self: 3.0453; time: 0.21s
Val loss: 0.3998 score: 0.8367 time: 0.06s
Test loss: 0.2257 score: 0.9184 time: 0.07s
Epoch 26/1000, LR 0.000270
Train loss: 0.1943;  Loss pred: 0.1640; Loss self: 3.0275; time: 0.23s
Val loss: 0.3954 score: 0.8367 time: 0.07s
Test loss: 0.2160 score: 0.9388 time: 0.07s
Epoch 27/1000, LR 0.000270
Train loss: 0.1855;  Loss pred: 0.1554; Loss self: 3.0121; time: 0.21s
Val loss: 0.3930 score: 0.8367 time: 0.06s
Test loss: 0.2074 score: 0.9388 time: 0.10s
Epoch 28/1000, LR 0.000270
Train loss: 0.1778;  Loss pred: 0.1478; Loss self: 3.0006; time: 0.22s
Val loss: 0.3924 score: 0.8367 time: 0.07s
Test loss: 0.2003 score: 0.9388 time: 0.08s
Epoch 29/1000, LR 0.000270
Train loss: 0.1704;  Loss pred: 0.1405; Loss self: 2.9931; time: 0.21s
Val loss: 0.3944 score: 0.8367 time: 0.07s
Test loss: 0.1943 score: 0.9388 time: 0.10s
     INFO: Early stopping counter 1 of 20
Epoch 30/1000, LR 0.000270
Train loss: 0.1630;  Loss pred: 0.1331; Loss self: 2.9891; time: 0.22s
Val loss: 0.3976 score: 0.8367 time: 0.07s
Test loss: 0.1885 score: 0.9388 time: 0.09s
     INFO: Early stopping counter 2 of 20
Epoch 31/1000, LR 0.000270
Train loss: 0.1558;  Loss pred: 0.1260; Loss self: 2.9876; time: 0.21s
Val loss: 0.4003 score: 0.8163 time: 0.06s
Test loss: 0.1835 score: 0.9388 time: 0.07s
     INFO: Early stopping counter 3 of 20
Epoch 32/1000, LR 0.000270
Train loss: 0.1494;  Loss pred: 0.1195; Loss self: 2.9869; time: 0.21s
Val loss: 0.4036 score: 0.8163 time: 0.06s
Test loss: 0.1790 score: 0.9592 time: 0.07s
     INFO: Early stopping counter 4 of 20
Epoch 33/1000, LR 0.000270
Train loss: 0.1442;  Loss pred: 0.1144; Loss self: 2.9862; time: 0.21s
Val loss: 0.4057 score: 0.8163 time: 0.06s
Test loss: 0.1739 score: 0.9592 time: 0.07s
     INFO: Early stopping counter 5 of 20
Epoch 34/1000, LR 0.000270
Train loss: 0.1399;  Loss pred: 0.1101; Loss self: 2.9843; time: 0.21s
Val loss: 0.4055 score: 0.8163 time: 0.07s
Test loss: 0.1682 score: 0.9592 time: 0.07s
     INFO: Early stopping counter 6 of 20
Epoch 35/1000, LR 0.000270
Train loss: 0.1359;  Loss pred: 0.1060; Loss self: 2.9810; time: 0.22s
Val loss: 0.4033 score: 0.8163 time: 0.06s
Test loss: 0.1622 score: 0.9592 time: 0.07s
     INFO: Early stopping counter 7 of 20
Epoch 36/1000, LR 0.000270
Train loss: 0.1318;  Loss pred: 0.1021; Loss self: 2.9770; time: 0.22s
Val loss: 0.3996 score: 0.8163 time: 0.06s
Test loss: 0.1556 score: 0.9592 time: 0.08s
     INFO: Early stopping counter 8 of 20
Epoch 37/1000, LR 0.000270
Train loss: 0.1279;  Loss pred: 0.0982; Loss self: 2.9725; time: 0.20s
Val loss: 0.3960 score: 0.8367 time: 0.07s
Test loss: 0.1488 score: 0.9592 time: 0.09s
     INFO: Early stopping counter 9 of 20
Epoch 38/1000, LR 0.000270
Train loss: 0.1240;  Loss pred: 0.0943; Loss self: 2.9680; time: 0.20s
Val loss: 0.3927 score: 0.8367 time: 0.06s
Test loss: 0.1423 score: 0.9592 time: 0.08s
     INFO: Early stopping counter 10 of 20
Epoch 39/1000, LR 0.000269
Train loss: 0.1203;  Loss pred: 0.0907; Loss self: 2.9633; time: 0.21s
Val loss: 0.3887 score: 0.8367 time: 0.06s
Test loss: 0.1362 score: 0.9388 time: 0.09s
Epoch 40/1000, LR 0.000269
Train loss: 0.1165;  Loss pred: 0.0869; Loss self: 2.9590; time: 0.20s
Val loss: 0.3845 score: 0.8571 time: 0.07s
Test loss: 0.1304 score: 0.9388 time: 0.07s
Epoch 41/1000, LR 0.000269
Train loss: 0.1131;  Loss pred: 0.0835; Loss self: 2.9550; time: 0.21s
Val loss: 0.3815 score: 0.8571 time: 0.06s
Test loss: 0.1253 score: 0.9388 time: 0.07s
Epoch 42/1000, LR 0.000269
Train loss: 0.1098;  Loss pred: 0.0803; Loss self: 2.9512; time: 0.24s
Val loss: 0.3805 score: 0.8571 time: 0.09s
Test loss: 0.1204 score: 0.9388 time: 0.09s
Epoch 43/1000, LR 0.000269
Train loss: 0.1065;  Loss pred: 0.0770; Loss self: 2.9473; time: 0.22s
Val loss: 0.3807 score: 0.8776 time: 0.07s
Test loss: 0.1158 score: 0.9388 time: 0.08s
     INFO: Early stopping counter 1 of 20
Epoch 44/1000, LR 0.000269
Train loss: 0.1028;  Loss pred: 0.0734; Loss self: 2.9430; time: 0.22s
Val loss: 0.3816 score: 0.8776 time: 0.07s
Test loss: 0.1115 score: 0.9388 time: 0.11s
     INFO: Early stopping counter 2 of 20
Epoch 45/1000, LR 0.000269
Train loss: 0.0990;  Loss pred: 0.0696; Loss self: 2.9388; time: 0.22s
Val loss: 0.3827 score: 0.8776 time: 0.09s
Test loss: 0.1071 score: 0.9592 time: 0.09s
     INFO: Early stopping counter 3 of 20
Epoch 46/1000, LR 0.000269
Train loss: 0.0951;  Loss pred: 0.0657; Loss self: 2.9347; time: 0.21s
Val loss: 0.3839 score: 0.8776 time: 0.08s
Test loss: 0.1031 score: 0.9592 time: 0.07s
     INFO: Early stopping counter 4 of 20
Epoch 47/1000, LR 0.000269
Train loss: 0.0912;  Loss pred: 0.0619; Loss self: 2.9307; time: 0.21s
Val loss: 0.3853 score: 0.8776 time: 0.06s
Test loss: 0.0994 score: 0.9592 time: 0.08s
     INFO: Early stopping counter 5 of 20
Epoch 48/1000, LR 0.000269
Train loss: 0.0875;  Loss pred: 0.0583; Loss self: 2.9268; time: 0.21s
Val loss: 0.3861 score: 0.8776 time: 0.07s
Test loss: 0.0961 score: 0.9592 time: 0.08s
     INFO: Early stopping counter 6 of 20
Epoch 49/1000, LR 0.000269
Train loss: 0.0837;  Loss pred: 0.0545; Loss self: 2.9224; time: 0.23s
Val loss: 0.3858 score: 0.8776 time: 0.07s
Test loss: 0.0937 score: 0.9592 time: 0.28s
     INFO: Early stopping counter 7 of 20
Epoch 50/1000, LR 0.000269
Train loss: 0.0799;  Loss pred: 0.0507; Loss self: 2.9176; time: 0.24s
Val loss: 0.3850 score: 0.8776 time: 0.11s
Test loss: 0.0917 score: 0.9592 time: 0.08s
     INFO: Early stopping counter 8 of 20
Epoch 51/1000, LR 0.000269
Train loss: 0.0762;  Loss pred: 0.0471; Loss self: 2.9120; time: 0.23s
Val loss: 0.3835 score: 0.8776 time: 0.10s
Test loss: 0.0898 score: 0.9592 time: 0.07s
     INFO: Early stopping counter 9 of 20
Epoch 52/1000, LR 0.000269
Train loss: 0.0726;  Loss pred: 0.0435; Loss self: 2.9064; time: 0.22s
Val loss: 0.3820 score: 0.8980 time: 0.11s
Test loss: 0.0881 score: 0.9592 time: 0.07s
     INFO: Early stopping counter 10 of 20
Epoch 53/1000, LR 0.000269
Train loss: 0.0694;  Loss pred: 0.0404; Loss self: 2.9009; time: 0.21s
Val loss: 0.3810 score: 0.8980 time: 0.06s
Test loss: 0.0864 score: 0.9592 time: 0.07s
     INFO: Early stopping counter 11 of 20
Epoch 54/1000, LR 0.000269
Train loss: 0.0665;  Loss pred: 0.0375; Loss self: 2.8959; time: 0.21s
Val loss: 0.3803 score: 0.8980 time: 0.06s
Test loss: 0.0846 score: 0.9592 time: 0.08s
Epoch 55/1000, LR 0.000269
Train loss: 0.0637;  Loss pred: 0.0348; Loss self: 2.8911; time: 0.22s
Val loss: 0.3807 score: 0.8980 time: 0.06s
Test loss: 0.0826 score: 0.9592 time: 0.07s
     INFO: Early stopping counter 1 of 20
Epoch 56/1000, LR 0.000269
Train loss: 0.0611;  Loss pred: 0.0322; Loss self: 2.8863; time: 0.21s
Val loss: 0.3823 score: 0.8980 time: 0.06s
Test loss: 0.0805 score: 0.9592 time: 0.07s
     INFO: Early stopping counter 2 of 20
Epoch 57/1000, LR 0.000269
Train loss: 0.0586;  Loss pred: 0.0298; Loss self: 2.8818; time: 0.22s
Val loss: 0.3849 score: 0.8776 time: 0.06s
Test loss: 0.0781 score: 0.9592 time: 0.07s
     INFO: Early stopping counter 3 of 20
Epoch 58/1000, LR 0.000269
Train loss: 0.0563;  Loss pred: 0.0275; Loss self: 2.8778; time: 0.21s
Val loss: 0.3893 score: 0.8776 time: 0.07s
Test loss: 0.0756 score: 0.9592 time: 0.07s
     INFO: Early stopping counter 4 of 20
Epoch 59/1000, LR 0.000268
Train loss: 0.0542;  Loss pred: 0.0255; Loss self: 2.8744; time: 0.21s
Val loss: 0.3957 score: 0.8776 time: 0.07s
Test loss: 0.0733 score: 0.9796 time: 0.07s
     INFO: Early stopping counter 5 of 20
Epoch 60/1000, LR 0.000268
Train loss: 0.0524;  Loss pred: 0.0236; Loss self: 2.8716; time: 0.21s
Val loss: 0.4036 score: 0.8776 time: 0.06s
Test loss: 0.0711 score: 0.9796 time: 0.07s
     INFO: Early stopping counter 6 of 20
Epoch 61/1000, LR 0.000268
Train loss: 0.0507;  Loss pred: 0.0220; Loss self: 2.8699; time: 0.21s
Val loss: 0.4122 score: 0.8776 time: 0.06s
Test loss: 0.0691 score: 0.9796 time: 0.07s
     INFO: Early stopping counter 7 of 20
Epoch 62/1000, LR 0.000268
Train loss: 0.0492;  Loss pred: 0.0205; Loss self: 2.8694; time: 0.21s
Val loss: 0.4202 score: 0.8776 time: 0.06s
Test loss: 0.0675 score: 0.9796 time: 0.10s
     INFO: Early stopping counter 8 of 20
Epoch 63/1000, LR 0.000268
Train loss: 0.0478;  Loss pred: 0.0191; Loss self: 2.8700; time: 0.21s
Val loss: 0.4271 score: 0.8571 time: 0.07s
Test loss: 0.0661 score: 0.9796 time: 0.07s
     INFO: Early stopping counter 9 of 20
Epoch 64/1000, LR 0.000268
Train loss: 0.0466;  Loss pred: 0.0179; Loss self: 2.8717; time: 0.21s
Val loss: 0.4336 score: 0.8571 time: 0.06s
Test loss: 0.0646 score: 0.9796 time: 0.07s
     INFO: Early stopping counter 10 of 20
Epoch 65/1000, LR 0.000268
Train loss: 0.0456;  Loss pred: 0.0168; Loss self: 2.8738; time: 0.21s
Val loss: 0.4399 score: 0.8571 time: 0.07s
Test loss: 0.0630 score: 0.9796 time: 0.09s
     INFO: Early stopping counter 11 of 20
Epoch 66/1000, LR 0.000268
Train loss: 0.0446;  Loss pred: 0.0159; Loss self: 2.8761; time: 0.21s
Val loss: 0.4457 score: 0.8571 time: 0.09s
Test loss: 0.0613 score: 0.9796 time: 0.07s
     INFO: Early stopping counter 12 of 20
Epoch 67/1000, LR 0.000268
Train loss: 0.0438;  Loss pred: 0.0150; Loss self: 2.8785; time: 0.21s
Val loss: 0.4513 score: 0.8571 time: 0.06s
Test loss: 0.0598 score: 0.9796 time: 0.07s
     INFO: Early stopping counter 13 of 20
Epoch 68/1000, LR 0.000268
Train loss: 0.0431;  Loss pred: 0.0143; Loss self: 2.8815; time: 0.22s
Val loss: 0.4563 score: 0.8571 time: 0.07s
Test loss: 0.0582 score: 0.9796 time: 0.08s
     INFO: Early stopping counter 14 of 20
Epoch 69/1000, LR 0.000268
Train loss: 0.0424;  Loss pred: 0.0135; Loss self: 2.8847; time: 0.22s
Val loss: 0.4608 score: 0.8571 time: 0.06s
Test loss: 0.0568 score: 0.9796 time: 0.07s
     INFO: Early stopping counter 15 of 20
Epoch 70/1000, LR 0.000268
Train loss: 0.0418;  Loss pred: 0.0129; Loss self: 2.8880; time: 0.22s
Val loss: 0.4649 score: 0.8571 time: 0.09s
Test loss: 0.0554 score: 0.9796 time: 0.07s
     INFO: Early stopping counter 16 of 20
Epoch 71/1000, LR 0.000268
Train loss: 0.0412;  Loss pred: 0.0123; Loss self: 2.8911; time: 0.21s
Val loss: 0.4687 score: 0.8571 time: 0.07s
Test loss: 0.0540 score: 0.9796 time: 0.07s
     INFO: Early stopping counter 17 of 20
Epoch 72/1000, LR 0.000267
Train loss: 0.0407;  Loss pred: 0.0117; Loss self: 2.8939; time: 0.21s
Val loss: 0.4720 score: 0.8571 time: 0.06s
Test loss: 0.0528 score: 0.9796 time: 0.07s
     INFO: Early stopping counter 18 of 20
Epoch 73/1000, LR 0.000267
Train loss: 0.0402;  Loss pred: 0.0112; Loss self: 2.8964; time: 0.21s
Val loss: 0.4750 score: 0.8571 time: 0.06s
Test loss: 0.0517 score: 0.9796 time: 0.07s
     INFO: Early stopping counter 19 of 20
Epoch 74/1000, LR 0.000267
Train loss: 0.0398;  Loss pred: 0.0108; Loss self: 2.8986; time: 0.21s
Val loss: 0.4778 score: 0.8571 time: 0.06s
Test loss: 0.0508 score: 0.9796 time: 0.07s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 053,   Train_Loss: 0.0665,   Val_Loss: 0.3803,   Val_Precision: 1.0000,   Val_Recall: 0.8000,   Val_accuracy: 0.8889,   Val_Score: 0.8980,   Val_Loss: 0.3803,   Test_Precision: 0.9583,   Test_Recall: 0.9583,   Test_accuracy: 0.9583,   Test_Score: 0.9592,   Test_loss: 0.0846


[0.06706651102285832, 0.06757586705498397, 0.06646898796316236, 0.0718316410202533, 0.07510654698126018, 0.06597159092780203, 0.0738949979422614, 0.06910502398386598, 0.09055500500835478, 0.06415757001377642, 0.06753980997018516, 0.06899847893510014, 0.07175968203227967, 0.06883948598988354, 0.06861220404971391, 0.07066568697337061, 0.07064964005257934, 0.07683709205593914, 0.06878010695800185, 0.07513271307107061, 0.07189878693316132, 0.06974977103527635, 0.07041120098438114, 0.07132892298977822, 0.07135446299798787, 0.07516509795095772, 0.08959802996832877, 0.07486275199335068, 0.06945901794824749, 0.07497779105324298, 0.07058932690415531, 0.06991016096435487, 0.07083920098375529, 0.0784491429803893, 0.07159295899327844, 0.06988748302683234, 0.09646255092229694, 0.0691754330182448, 0.07061747496481985, 0.07159606297500432, 0.07007017196156085, 0.07290084101259708, 0.07912946597207338, 0.07725238509010524, 0.07029037794563919, 0.06916264398023486, 0.07487984898034483, 0.06928547192364931, 0.08146865293383598, 0.06990542495623231, 0.06993303389754146, 0.06905998103320599, 0.07151305093429983, 0.06959297799039632, 0.07453786896076053, 0.07188723003491759, 0.0700574970105663, 0.07670035399496555, 0.07673974707722664, 0.07525623601395637, 0.07201656396500766, 0.07621234399266541, 0.07164364994969219, 0.10028795804828405, 0.07181643997319043, 0.07650209800340235, 0.07165027293376625, 0.0698749260045588, 0.06995400902815163, 0.07424850098323077, 0.06981106102466583, 0.26665062201209366, 0.07301209506113082, 0.06954951991792768, 0.07559041702188551, 0.06907626101747155, 0.07111492694821209, 0.06783055700361729, 0.0853531239554286, 0.08741538401227444, 0.07640196091961116, 0.07627616100944579, 0.23510468401946127, 0.205961580039002, 0.08073909999802709, 0.0768189060036093, 0.07880825304891914, 0.08393342106137425, 0.0845796640496701, 0.08057688898406923, 0.07803531200625002, 0.0773524419637397, 0.07985511992592365, 0.08154732105322182, 0.08007114299107343, 0.07717055990360677, 0.07665563107002527, 0.07912467396818101, 0.0988308839732781, 0.07578818709589541, 0.0762350809527561, 0.08497154305223376, 0.07685760199092329, 0.07835833099670708, 0.10290855297353119, 0.08501558704301715, 0.1036606589332223, 0.09405160997994244, 0.07784074090886861, 0.07746710791252553, 0.07768134691286832, 0.07820729597005993, 0.07381567405536771, 0.08034147508442402, 0.09580441401340067, 0.08176478301174939, 0.09224577294662595, 0.0785391420358792, 0.07808039104565978, 0.0908339760499075, 0.07937339902855456, 0.11401962803211063, 0.08992285700514913, 0.07659940898884088, 0.08534681901801378, 0.08261122100520879, 0.2808588369516656, 0.08621910808142275, 0.07828354800585657, 0.0772779380204156, 0.07537209300789982, 0.0827043300960213, 0.07712570193689317, 0.07698561600409448, 0.07730853895191103, 0.07766085595358163, 0.07494553993456066, 0.07555786694865674, 0.07579271809663624, 0.10909658798482269, 0.07757079100701958, 0.07523413700982928, 0.09039906901307404, 0.07350921805482358, 0.07516056194435805, 0.08032081706915051, 0.07562797097489238, 0.07647843193262815, 0.07583522598724812, 0.07644141104537994, 0.076794552966021, 0.07552490301895887]
[0.0013687043065889453, 0.001379099327652734, 0.001356509958431885, 0.0014659518575561897, 0.0015327866730869425, 0.001346358998526572, 0.0015080611824951305, 0.0014103066119156322, 0.001848061326701118, 0.0013093381635464576, 0.001378363468779289, 0.001408132223165309, 0.0014644833067812178, 0.0014048874691812967, 0.0014002490622390593, 0.0014421568770075636, 0.00144182938882815, 0.001568103919508962, 0.0014036756522041193, 0.0015333206749198083, 0.0014673221823094146, 0.00142346471500564, 0.0014369632853955335, 0.0014556923059138413, 0.001456213530571181, 0.0015339815908358718, 0.0018285312238434444, 0.001527811265170422, 0.001417530978535663, 0.0015301590010865914, 0.0014405985082480677, 0.0014267379788643851, 0.001445697979260312, 0.0016010029179671286, 0.0014610807957811927, 0.001426275163812905, 0.0019686234882101417, 0.001411743530984588, 0.0014411729584657112, 0.001461144142347027, 0.0014300035094196091, 0.0014877722655632058, 0.0016148870606545586, 0.0015765792875531682, 0.0014344975090946775, 0.0014114825302088748, 0.0015281601832723435, 0.0014139892229316185, 0.0016626255700782854, 0.001426641325637394, 0.0014272047734192135, 0.0014093873680246119, 0.0014594500190673433, 0.0014202648569468636, 0.0015211809991991945, 0.0014670863272432163, 0.0014297448369503325, 0.0015653133468360317, 0.0015661172872903396, 0.001535841551305232, 0.001469725795204238, 0.001555353959033988, 0.001462115305095759, 0.0020466930213935523, 0.001465641632105927, 0.0015612673061918846, 0.001462250468036046, 0.0014260188980522205, 0.001427632837309217, 0.0015152755302700158, 0.0014247155311156291, 0.005441849428818238, 0.0014900427563496086, 0.0014193779575087282, 0.0015426615718752146, 0.0014097196126014603, 0.0014513250397594304, 0.0013842970817064752, 0.001741900488886298, 0.0017839874288219275, 0.0015592236922369624, 0.0015566563471315466, 0.004798054775907373, 0.004203297551816368, 0.001647736734653614, 0.0015677327755838633, 0.001608331694875901, 0.0017129269604362091, 0.0017261155928504101, 0.001644426305797331, 0.0015925573878826536, 0.001578621264566116, 0.0016296963250188499, 0.0016642310419024862, 0.0016341049590014986, 0.0015749093857878934, 0.0015644006340821482, 0.0016147892646567554, 0.002016956815781186, 0.0015466976958346, 0.0015558179786276756, 0.0017341131235149745, 0.0015685224896106794, 0.0015991496121776955, 0.0021001745504802285, 0.0017350119804697378, 0.002115523651698414, 0.00191942061183556, 0.0015885865491605839, 0.001580961385969909, 0.0015853336104667004, 0.0015960672646951005, 0.0015064423276605655, 0.0016396219404984493, 0.0019551921227224625, 0.00166866904105611, 0.001882566794829101, 0.0016028396333852897, 0.001593477368278771, 0.0018537546132634186, 0.0016198652862970317, 0.0023269311843287883, 0.0018351603470438598, 0.001563253244670222, 0.0017417718166941587, 0.0016859432858205875, 0.005731812999013583, 0.00175957363431475, 0.0015976234286909504, 0.0015771007759268491, 0.0015382059797530575, 0.0016878434713473733, 0.0015739939170794524, 0.0015711350204917242, 0.0015777252847328782, 0.0015849154276241148, 0.001529500814991034, 0.001541997284666464, 0.0015467901652374742, 0.002226460979282096, 0.0015830773674901954, 0.0015353905512210057, 0.0018448789594504907, 0.001500188123567828, 0.0015338890192726133, 0.0016392003483500105, 0.0015434279790794362, 0.0015607843251556766, 0.0015476576732091454, 0.0015600287968444886, 0.0015672357748167552, 0.0015413245514073238]
[730.6179977559784, 725.1109328738693, 737.185889262466, 682.1506414726653, 652.4065074144066, 742.7439494922081, 663.1030700926015, 709.0656680973019, 541.1075842299292, 763.7446366730899, 725.4980436224296, 710.1605826135577, 682.8346867250376, 711.8007825799411, 714.1586643171582, 693.4058395054586, 693.5633354045809, 637.712837496855, 712.415292257689, 652.1792971012403, 681.5135844440807, 702.5112666709396, 695.9120042686014, 686.958360594088, 686.7124765746154, 651.8983056733404, 546.8870243834667, 654.5311078645918, 705.4519549428255, 653.5268552417646, 694.1559319092412, 700.8995448456149, 691.7074066269689, 624.6084805827517, 684.4248469266427, 701.1269812247656, 507.9691500121198, 708.3439577035449, 693.8792419923083, 684.3951743143604, 699.2989831233818, 672.1458808895349, 619.2383506959751, 634.2846236119135, 697.1082164033229, 708.4749393618195, 654.3816616518816, 707.2189687038093, 601.4583307250078, 700.9470299433675, 700.6703022750257, 709.5281415794108, 685.1896172772307, 704.0940252156172, 657.3839671455512, 681.6231474797313, 699.4255017790554, 638.8497242557219, 638.521781296583, 651.1088329067227, 680.3990263102354, 642.9404665039, 683.9405869802495, 488.59305696910036, 682.2950290809742, 640.5053100350369, 683.877367017091, 701.2529787409453, 700.4602120842123, 659.9459834356358, 701.8945032605533, 183.76105643502927, 671.1216814005102, 704.533978923546, 648.2303171553234, 709.3609190515735, 689.0255267460683, 722.3882887676555, 574.0856072894039, 560.5420665213763, 641.3447954766104, 642.4025455860581, 208.41779569114806, 237.90844870543862, 606.8930666950381, 637.8638091734574, 621.7622914389933, 583.7960538290218, 579.3354768023712, 608.1148157716506, 627.9208571124246, 633.4641642337498, 613.6112505429093, 600.8781081602934, 611.9557954288559, 634.9571658052704, 639.2224461010344, 619.2758534424386, 495.79643558837967, 646.538753302014, 642.7487107984571, 576.6636480860268, 637.5426598111504, 625.3323593895736, 476.1508988723527, 576.3648961831719, 472.69620417486993, 520.9905498741574, 629.4904111635619, 632.5265176457848, 630.7820596231563, 626.5400100108135, 663.815654697484, 609.8966934389749, 511.45868908656036, 599.2800102332421, 531.1896516748984, 623.8927333534562, 627.5583324287634, 539.445724285785, 617.3352861249178, 429.7505687897915, 544.9115122887408, 639.6916196460262, 574.1280174678542, 593.1397624169053, 174.46486830119807, 568.3194954153998, 625.929729147358, 634.0748893565842, 650.1079914931414, 592.4720016849187, 635.3264705466596, 636.4825345736525, 633.8239043746505, 630.9484926265505, 653.8080857484617, 648.5095725809279, 646.5001022594897, 449.143285826838, 631.6810665958772, 651.3000872675417, 542.0409804542714, 666.5830666768286, 651.9376483144855, 610.0535550804282, 647.908430814142, 640.7035128958368, 646.1377198010786, 641.0138082211857, 638.06608812061, 648.7926239071055]
Elapsed: 0.08155908517227017~0.028861801716408318
Time per graph: 0.0016644711259646977~0.0005890163615593535
Speed: 630.7197303115331~93.09545036435138
Total Time: 0.0762
best val loss: 0.38026654720306396 test_score: 0.9592

Testing...
Test loss: 0.0881 score: 0.9592 time: 0.07s
test Score 0.9592
Epoch Time List: [0.3494270759401843, 0.34380863595288247, 0.3430066700093448, 0.34510894503910094, 0.3538594759302214, 0.3449007129529491, 0.3516954150982201, 0.3470907580340281, 0.3743883731076494, 0.36526425986085087, 0.34738179109990597, 0.36776701803319156, 0.35642679885495454, 0.3598030429566279, 0.35898670786991715, 0.36538390698842704, 0.35066479607485235, 0.3955443089362234, 0.36012659408152103, 0.3658252478344366, 0.38421811489388347, 0.3636845810106024, 0.35787462804000825, 0.36612298304680735, 0.3634975559543818, 0.366100546089001, 0.3762538230512291, 0.36748302006162703, 0.3554438811261207, 0.3686965440865606, 0.3575703620444983, 0.3731384068960324, 0.35569762578234076, 0.3652215781621635, 0.40370197186712176, 0.39809092006180435, 0.3779113250784576, 0.3542541960487142, 0.35896442201919854, 0.3632456319173798, 0.35101814405061305, 0.3714863839559257, 0.37545699009206146, 1.6109206308610737, 0.3510264160577208, 0.35510507912840694, 0.35938038281165063, 0.3578358730301261, 0.37876235297881067, 0.35379342711530626, 0.3546503980178386, 0.36985121108591557, 0.358359741861932, 0.3560706239659339, 0.35667481494601816, 0.36693616094999015, 0.354300226084888, 0.3648258730536327, 0.37712174793705344, 0.40089241205714643, 0.3634860800812021, 0.37497312808409333, 0.3626420749351382, 0.3927286311518401, 0.372055894928053, 0.38454131584148854, 0.41370646690484136, 0.3587131960084662, 0.35741325397975743, 0.3880235581891611, 0.3914188191993162, 0.564742439892143, 0.37826573697384447, 0.3523060920415446, 0.361281278077513, 0.36591598810628057, 0.47198546899016947, 0.3729447789955884, 0.36615589587017894, 0.3922486879164353, 0.36265696899499744, 0.35308761696796864, 1.0114035021979362, 0.7924972089240327, 0.4278995170025155, 0.35766495496500283, 0.38154283398762345, 0.3715513860806823, 0.37608499405905604, 0.3826562830945477, 0.3677120669744909, 0.3499098438769579, 0.3579547351691872, 0.36703296401537955, 0.35652608203236014, 0.35677889501675963, 0.34723212593235075, 0.3493086179951206, 0.4804229730507359, 0.3471498090075329, 0.34898057591635734, 0.35713532206136733, 0.3489942019805312, 0.37258532585110515, 0.37331148399971426, 0.3671359949512407, 0.37875324313063174, 0.3706112060463056, 0.3513661591568962, 0.35078291909303516, 0.35094971617218107, 0.35551079304423183, 0.3526472740340978, 0.3562930040061474, 0.36036413407418877, 0.33926311403047293, 0.359313675086014, 0.34246381593402475, 0.3490786150796339, 0.41977544198744, 0.3572315621422604, 0.39830598107073456, 0.3933265608502552, 0.3669439449440688, 0.35440563107840717, 0.35951381002087146, 0.5830250059952959, 0.4315291449893266, 0.40319607593119144, 0.40426255599595606, 0.3473539330298081, 0.35028477397281677, 0.3590719278436154, 0.3505105768563226, 0.3548807220067829, 0.35253281600307673, 0.3520167809911072, 0.3423903719522059, 0.34230014798231423, 0.3790038279257715, 0.35313491499982774, 0.34822987008374184, 0.36552410398144275, 0.36967516504228115, 0.34564252209383994, 0.36002161097712815, 0.3539026260841638, 0.3763855081051588, 0.3481312090298161, 0.34543825313448906, 0.3480444378219545, 0.34571816585958004]
Total Epoch List: [78, 74]
Total Time List: [0.06832576694432646, 0.07616047491319478]
========================k_idx:2========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
emb dim:  64
Model: Ethident
Ethident(
  (encoder): HGATE_encoder(
    (lin1): Linear(in_features=14887, out_features=64, bias=True)
    (account_convs): ModuleList(
      (0): GATEConv()
      (1): GATConv(64, 64, heads=1)
    )
    (subgraph_conv): GATConv(64, 64, heads=1)
    (lin2): Linear(in_features=64, out_features=64, bias=True)
    (lin3): Linear(in_features=64, out_features=2, bias=True)
  )
  (proj_head_g1): Project_Head(
    (block): Sequential(
      (0): Linear(in_features=64, out_features=64, bias=True)
      (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=64, bias=True)
      (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
      (5): ReLU()
    )
    (linear_shortcut): Linear(in_features=64, out_features=64, bias=True)
  )
  (proj_head_g2): Project_Head(
    (block): Sequential(
      (0): Linear(in_features=64, out_features=64, bias=True)
      (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=64, bias=True)
      (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
      (5): ReLU()
    )
    (linear_shortcut): Linear(in_features=64, out_features=64, bias=True)
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=64, bias=True)
    (1): ReLU()
    (2): Dropout(p=0.0, inplace=True)
    (3): Linear(in_features=64, out_features=2, bias=True)
  )
)
Total number of parameters: 1003972
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7720be2658d0>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.7346;  Loss pred: 0.6985; Loss self: 3.6060; time: 0.22s
Val loss: 0.7154 score: 0.3878 time: 0.07s
Test loss: 0.6883 score: 0.5833 time: 0.07s
Epoch 2/1000, LR 0.000000
Train loss: 0.7346;  Loss pred: 0.6985; Loss self: 3.6060; time: 0.21s
Val loss: 0.6982 score: 0.5306 time: 0.07s
Test loss: 0.6684 score: 0.6458 time: 0.07s
Epoch 3/1000, LR 0.000030
Train loss: 0.7095;  Loss pred: 0.6733; Loss self: 3.6247; time: 0.21s
Val loss: 0.6743 score: 0.6531 time: 0.07s
Test loss: 0.6413 score: 0.7083 time: 0.10s
Epoch 4/1000, LR 0.000060
Train loss: 0.6745;  Loss pred: 0.6381; Loss self: 3.6436; time: 0.22s
Val loss: 0.6441 score: 0.5714 time: 0.07s
Test loss: 0.6000 score: 0.7083 time: 0.07s
Epoch 5/1000, LR 0.000090
Train loss: 0.6322;  Loss pred: 0.5959; Loss self: 3.6313; time: 0.22s
Val loss: 0.6171 score: 0.6122 time: 0.07s
Test loss: 0.5739 score: 0.7500 time: 0.07s
Epoch 6/1000, LR 0.000120
Train loss: 0.5910;  Loss pred: 0.5555; Loss self: 3.5493; time: 0.22s
Val loss: 0.5842 score: 0.7347 time: 0.14s
Test loss: 0.5250 score: 0.8125 time: 0.07s
Epoch 7/1000, LR 0.000150
Train loss: 0.5319;  Loss pred: 0.4974; Loss self: 3.4474; time: 0.34s
Val loss: 0.5887 score: 0.7755 time: 0.07s
Test loss: 0.4865 score: 0.8333 time: 0.07s
     INFO: Early stopping counter 1 of 20
Epoch 8/1000, LR 0.000180
Train loss: 0.4925;  Loss pred: 0.4591; Loss self: 3.3358; time: 0.21s
Val loss: 0.5333 score: 0.7347 time: 0.07s
Test loss: 0.4299 score: 0.8125 time: 0.07s
Epoch 9/1000, LR 0.000210
Train loss: 0.4319;  Loss pred: 0.3998; Loss self: 3.2167; time: 0.21s
Val loss: 0.5275 score: 0.7143 time: 0.07s
Test loss: 0.4240 score: 0.8125 time: 0.07s
Epoch 10/1000, LR 0.000240
Train loss: 0.4157;  Loss pred: 0.3846; Loss self: 3.1120; time: 0.22s
Val loss: 0.5277 score: 0.8163 time: 0.07s
Test loss: 0.3903 score: 0.8750 time: 0.07s
     INFO: Early stopping counter 1 of 20
Epoch 11/1000, LR 0.000270
Train loss: 0.3619;  Loss pred: 0.3319; Loss self: 2.9968; time: 0.21s
Val loss: 0.5612 score: 0.7551 time: 0.07s
Test loss: 0.4002 score: 0.8958 time: 0.07s
     INFO: Early stopping counter 2 of 20
Epoch 12/1000, LR 0.000270
Train loss: 0.3490;  Loss pred: 0.3200; Loss self: 2.9049; time: 0.22s
Val loss: 0.4930 score: 0.8367 time: 0.07s
Test loss: 0.3392 score: 0.8750 time: 0.07s
Epoch 13/1000, LR 0.000270
Train loss: 0.2923;  Loss pred: 0.2638; Loss self: 2.8549; time: 0.23s
Val loss: 0.4583 score: 0.8163 time: 0.07s
Test loss: 0.3191 score: 0.8750 time: 0.07s
Epoch 14/1000, LR 0.000270
Train loss: 0.2717;  Loss pred: 0.2434; Loss self: 2.8253; time: 0.23s
Val loss: 0.4534 score: 0.8367 time: 0.08s
Test loss: 0.2996 score: 0.9167 time: 0.10s
Epoch 15/1000, LR 0.000270
Train loss: 0.2305;  Loss pred: 0.2027; Loss self: 2.7818; time: 0.29s
Val loss: 0.4998 score: 0.8367 time: 0.07s
Test loss: 0.3104 score: 0.8958 time: 0.07s
     INFO: Early stopping counter 1 of 20
Epoch 16/1000, LR 0.000270
Train loss: 0.2103;  Loss pred: 0.1828; Loss self: 2.7546; time: 0.22s
Val loss: 0.4817 score: 0.8367 time: 0.07s
Test loss: 0.2939 score: 0.8958 time: 0.20s
     INFO: Early stopping counter 2 of 20
Epoch 17/1000, LR 0.000270
Train loss: 0.1852;  Loss pred: 0.1578; Loss self: 2.7427; time: 0.22s
Val loss: 0.4298 score: 0.8367 time: 0.07s
Test loss: 0.2630 score: 0.9167 time: 0.07s
Epoch 18/1000, LR 0.000270
Train loss: 0.1621;  Loss pred: 0.1348; Loss self: 2.7274; time: 0.22s
Val loss: 0.4061 score: 0.8367 time: 0.07s
Test loss: 0.2481 score: 0.9167 time: 0.07s
Epoch 19/1000, LR 0.000270
Train loss: 0.1501;  Loss pred: 0.1229; Loss self: 2.7237; time: 0.22s
Val loss: 0.4113 score: 0.8367 time: 0.07s
Test loss: 0.2385 score: 0.9375 time: 0.07s
     INFO: Early stopping counter 1 of 20
Epoch 20/1000, LR 0.000270
Train loss: 0.1308;  Loss pred: 0.1035; Loss self: 2.7283; time: 0.22s
Val loss: 0.4417 score: 0.8367 time: 0.07s
Test loss: 0.2401 score: 0.9167 time: 0.07s
     INFO: Early stopping counter 2 of 20
Epoch 21/1000, LR 0.000270
Train loss: 0.1161;  Loss pred: 0.0887; Loss self: 2.7379; time: 0.21s
Val loss: 0.4756 score: 0.8367 time: 0.07s
Test loss: 0.2489 score: 0.9167 time: 0.07s
     INFO: Early stopping counter 3 of 20
Epoch 22/1000, LR 0.000270
Train loss: 0.1074;  Loss pred: 0.0799; Loss self: 2.7461; time: 0.22s
Val loss: 0.4874 score: 0.8367 time: 0.07s
Test loss: 0.2457 score: 0.9167 time: 0.07s
     INFO: Early stopping counter 4 of 20
Epoch 23/1000, LR 0.000270
Train loss: 0.0979;  Loss pred: 0.0704; Loss self: 2.7510; time: 0.22s
Val loss: 0.4764 score: 0.8367 time: 0.07s
Test loss: 0.2302 score: 0.9167 time: 0.07s
     INFO: Early stopping counter 5 of 20
Epoch 24/1000, LR 0.000270
Train loss: 0.0882;  Loss pred: 0.0607; Loss self: 2.7564; time: 0.24s
Val loss: 0.4609 score: 0.8367 time: 0.07s
Test loss: 0.2160 score: 0.9167 time: 0.07s
     INFO: Early stopping counter 6 of 20
Epoch 25/1000, LR 0.000270
Train loss: 0.0814;  Loss pred: 0.0538; Loss self: 2.7634; time: 0.21s
Val loss: 0.4517 score: 0.8367 time: 0.07s
Test loss: 0.2073 score: 0.9583 time: 0.07s
     INFO: Early stopping counter 7 of 20
Epoch 26/1000, LR 0.000270
Train loss: 0.0765;  Loss pred: 0.0488; Loss self: 2.7688; time: 0.21s
Val loss: 0.4512 score: 0.8367 time: 0.07s
Test loss: 0.2038 score: 0.9583 time: 0.07s
     INFO: Early stopping counter 8 of 20
Epoch 27/1000, LR 0.000270
Train loss: 0.0719;  Loss pred: 0.0442; Loss self: 2.7730; time: 0.22s
Val loss: 0.4594 score: 0.8367 time: 0.07s
Test loss: 0.2035 score: 0.9583 time: 0.08s
     INFO: Early stopping counter 9 of 20
Epoch 28/1000, LR 0.000270
Train loss: 0.0676;  Loss pred: 0.0398; Loss self: 2.7771; time: 0.21s
Val loss: 0.4758 score: 0.8367 time: 0.07s
Test loss: 0.2056 score: 0.9583 time: 0.08s
     INFO: Early stopping counter 10 of 20
Epoch 29/1000, LR 0.000270
Train loss: 0.0637;  Loss pred: 0.0359; Loss self: 2.7813; time: 0.22s
Val loss: 0.4964 score: 0.8367 time: 0.08s
Test loss: 0.2100 score: 0.9375 time: 0.08s
     INFO: Early stopping counter 11 of 20
Epoch 30/1000, LR 0.000270
Train loss: 0.0606;  Loss pred: 0.0327; Loss self: 2.7870; time: 0.21s
Val loss: 0.5155 score: 0.8367 time: 0.07s
Test loss: 0.2156 score: 0.9375 time: 0.07s
     INFO: Early stopping counter 12 of 20
Epoch 31/1000, LR 0.000270
Train loss: 0.0580;  Loss pred: 0.0300; Loss self: 2.7926; time: 0.21s
Val loss: 0.5316 score: 0.8367 time: 0.07s
Test loss: 0.2212 score: 0.9375 time: 0.07s
     INFO: Early stopping counter 13 of 20
Epoch 32/1000, LR 0.000270
Train loss: 0.0557;  Loss pred: 0.0277; Loss self: 2.7982; time: 0.22s
Val loss: 0.5443 score: 0.8367 time: 0.07s
Test loss: 0.2262 score: 0.9375 time: 0.07s
     INFO: Early stopping counter 14 of 20
Epoch 33/1000, LR 0.000270
Train loss: 0.0536;  Loss pred: 0.0255; Loss self: 2.8033; time: 0.21s
Val loss: 0.5546 score: 0.8367 time: 0.07s
Test loss: 0.2300 score: 0.9375 time: 0.07s
     INFO: Early stopping counter 15 of 20
Epoch 34/1000, LR 0.000270
Train loss: 0.0517;  Loss pred: 0.0236; Loss self: 2.8084; time: 0.21s
Val loss: 0.5623 score: 0.8367 time: 0.07s
Test loss: 0.2321 score: 0.9375 time: 0.07s
     INFO: Early stopping counter 16 of 20
Epoch 35/1000, LR 0.000270
Train loss: 0.0501;  Loss pred: 0.0220; Loss self: 2.8139; time: 0.21s
Val loss: 0.5675 score: 0.8367 time: 0.08s
Test loss: 0.2331 score: 0.9375 time: 0.07s
     INFO: Early stopping counter 17 of 20
Epoch 36/1000, LR 0.000270
Train loss: 0.0487;  Loss pred: 0.0205; Loss self: 2.8197; time: 0.22s
Val loss: 0.5715 score: 0.8367 time: 0.07s
Test loss: 0.2337 score: 0.9375 time: 0.07s
     INFO: Early stopping counter 18 of 20
Epoch 37/1000, LR 0.000270
Train loss: 0.0476;  Loss pred: 0.0193; Loss self: 2.8258; time: 0.21s
Val loss: 0.5749 score: 0.8367 time: 0.07s
Test loss: 0.2340 score: 0.9375 time: 0.07s
     INFO: Early stopping counter 19 of 20
Epoch 38/1000, LR 0.000270
Train loss: 0.0466;  Loss pred: 0.0182; Loss self: 2.8318; time: 0.21s
Val loss: 0.5778 score: 0.8367 time: 0.07s
Test loss: 0.2342 score: 0.9375 time: 0.07s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 017,   Train_Loss: 0.1621,   Val_Loss: 0.4061,   Val_Precision: 0.9474,   Val_Recall: 0.7200,   Val_accuracy: 0.8182,   Val_Score: 0.8367,   Val_Loss: 0.4061,   Test_Precision: 0.9545,   Test_Recall: 0.8750,   Test_accuracy: 0.9130,   Test_Score: 0.9167,   Test_loss: 0.2481


[0.06706651102285832, 0.06757586705498397, 0.06646898796316236, 0.0718316410202533, 0.07510654698126018, 0.06597159092780203, 0.0738949979422614, 0.06910502398386598, 0.09055500500835478, 0.06415757001377642, 0.06753980997018516, 0.06899847893510014, 0.07175968203227967, 0.06883948598988354, 0.06861220404971391, 0.07066568697337061, 0.07064964005257934, 0.07683709205593914, 0.06878010695800185, 0.07513271307107061, 0.07189878693316132, 0.06974977103527635, 0.07041120098438114, 0.07132892298977822, 0.07135446299798787, 0.07516509795095772, 0.08959802996832877, 0.07486275199335068, 0.06945901794824749, 0.07497779105324298, 0.07058932690415531, 0.06991016096435487, 0.07083920098375529, 0.0784491429803893, 0.07159295899327844, 0.06988748302683234, 0.09646255092229694, 0.0691754330182448, 0.07061747496481985, 0.07159606297500432, 0.07007017196156085, 0.07290084101259708, 0.07912946597207338, 0.07725238509010524, 0.07029037794563919, 0.06916264398023486, 0.07487984898034483, 0.06928547192364931, 0.08146865293383598, 0.06990542495623231, 0.06993303389754146, 0.06905998103320599, 0.07151305093429983, 0.06959297799039632, 0.07453786896076053, 0.07188723003491759, 0.0700574970105663, 0.07670035399496555, 0.07673974707722664, 0.07525623601395637, 0.07201656396500766, 0.07621234399266541, 0.07164364994969219, 0.10028795804828405, 0.07181643997319043, 0.07650209800340235, 0.07165027293376625, 0.0698749260045588, 0.06995400902815163, 0.07424850098323077, 0.06981106102466583, 0.26665062201209366, 0.07301209506113082, 0.06954951991792768, 0.07559041702188551, 0.06907626101747155, 0.07111492694821209, 0.06783055700361729, 0.0853531239554286, 0.08741538401227444, 0.07640196091961116, 0.07627616100944579, 0.23510468401946127, 0.205961580039002, 0.08073909999802709, 0.0768189060036093, 0.07880825304891914, 0.08393342106137425, 0.0845796640496701, 0.08057688898406923, 0.07803531200625002, 0.0773524419637397, 0.07985511992592365, 0.08154732105322182, 0.08007114299107343, 0.07717055990360677, 0.07665563107002527, 0.07912467396818101, 0.0988308839732781, 0.07578818709589541, 0.0762350809527561, 0.08497154305223376, 0.07685760199092329, 0.07835833099670708, 0.10290855297353119, 0.08501558704301715, 0.1036606589332223, 0.09405160997994244, 0.07784074090886861, 0.07746710791252553, 0.07768134691286832, 0.07820729597005993, 0.07381567405536771, 0.08034147508442402, 0.09580441401340067, 0.08176478301174939, 0.09224577294662595, 0.0785391420358792, 0.07808039104565978, 0.0908339760499075, 0.07937339902855456, 0.11401962803211063, 0.08992285700514913, 0.07659940898884088, 0.08534681901801378, 0.08261122100520879, 0.2808588369516656, 0.08621910808142275, 0.07828354800585657, 0.0772779380204156, 0.07537209300789982, 0.0827043300960213, 0.07712570193689317, 0.07698561600409448, 0.07730853895191103, 0.07766085595358163, 0.07494553993456066, 0.07555786694865674, 0.07579271809663624, 0.10909658798482269, 0.07757079100701958, 0.07523413700982928, 0.09039906901307404, 0.07350921805482358, 0.07516056194435805, 0.08032081706915051, 0.07562797097489238, 0.07647843193262815, 0.07583522598724812, 0.07644141104537994, 0.076794552966021, 0.07552490301895887, 0.07039292296394706, 0.07090964203234762, 0.09986667602788657, 0.0712742549367249, 0.07074312097392976, 0.07738436292856932, 0.07129533495754004, 0.07402723503764719, 0.07600575895048678, 0.07245754497125745, 0.07172506593633443, 0.07729789393488318, 0.07724719191901386, 0.10134239797480404, 0.07333615806419402, 0.20397584198508412, 0.07238131307531148, 0.07191409100778401, 0.07206530799157917, 0.07275251403916627, 0.07241637597326189, 0.07186594698578119, 0.07581381499767303, 0.07200161507353187, 0.07416958606336266, 0.07717612606938928, 0.08076872304081917, 0.08455885795410722, 0.08033315103966743, 0.0714891190873459, 0.0717331210616976, 0.07192579994443804, 0.07060101698152721, 0.07159044302534312, 0.07114345103036612, 0.07220472407061607, 0.07089073304086924, 0.07235312997363508]
[0.0013687043065889453, 0.001379099327652734, 0.001356509958431885, 0.0014659518575561897, 0.0015327866730869425, 0.001346358998526572, 0.0015080611824951305, 0.0014103066119156322, 0.001848061326701118, 0.0013093381635464576, 0.001378363468779289, 0.001408132223165309, 0.0014644833067812178, 0.0014048874691812967, 0.0014002490622390593, 0.0014421568770075636, 0.00144182938882815, 0.001568103919508962, 0.0014036756522041193, 0.0015333206749198083, 0.0014673221823094146, 0.00142346471500564, 0.0014369632853955335, 0.0014556923059138413, 0.001456213530571181, 0.0015339815908358718, 0.0018285312238434444, 0.001527811265170422, 0.001417530978535663, 0.0015301590010865914, 0.0014405985082480677, 0.0014267379788643851, 0.001445697979260312, 0.0016010029179671286, 0.0014610807957811927, 0.001426275163812905, 0.0019686234882101417, 0.001411743530984588, 0.0014411729584657112, 0.001461144142347027, 0.0014300035094196091, 0.0014877722655632058, 0.0016148870606545586, 0.0015765792875531682, 0.0014344975090946775, 0.0014114825302088748, 0.0015281601832723435, 0.0014139892229316185, 0.0016626255700782854, 0.001426641325637394, 0.0014272047734192135, 0.0014093873680246119, 0.0014594500190673433, 0.0014202648569468636, 0.0015211809991991945, 0.0014670863272432163, 0.0014297448369503325, 0.0015653133468360317, 0.0015661172872903396, 0.001535841551305232, 0.001469725795204238, 0.001555353959033988, 0.001462115305095759, 0.0020466930213935523, 0.001465641632105927, 0.0015612673061918846, 0.001462250468036046, 0.0014260188980522205, 0.001427632837309217, 0.0015152755302700158, 0.0014247155311156291, 0.005441849428818238, 0.0014900427563496086, 0.0014193779575087282, 0.0015426615718752146, 0.0014097196126014603, 0.0014513250397594304, 0.0013842970817064752, 0.001741900488886298, 0.0017839874288219275, 0.0015592236922369624, 0.0015566563471315466, 0.004798054775907373, 0.004203297551816368, 0.001647736734653614, 0.0015677327755838633, 0.001608331694875901, 0.0017129269604362091, 0.0017261155928504101, 0.001644426305797331, 0.0015925573878826536, 0.001578621264566116, 0.0016296963250188499, 0.0016642310419024862, 0.0016341049590014986, 0.0015749093857878934, 0.0015644006340821482, 0.0016147892646567554, 0.002016956815781186, 0.0015466976958346, 0.0015558179786276756, 0.0017341131235149745, 0.0015685224896106794, 0.0015991496121776955, 0.0021001745504802285, 0.0017350119804697378, 0.002115523651698414, 0.00191942061183556, 0.0015885865491605839, 0.001580961385969909, 0.0015853336104667004, 0.0015960672646951005, 0.0015064423276605655, 0.0016396219404984493, 0.0019551921227224625, 0.00166866904105611, 0.001882566794829101, 0.0016028396333852897, 0.001593477368278771, 0.0018537546132634186, 0.0016198652862970317, 0.0023269311843287883, 0.0018351603470438598, 0.001563253244670222, 0.0017417718166941587, 0.0016859432858205875, 0.005731812999013583, 0.00175957363431475, 0.0015976234286909504, 0.0015771007759268491, 0.0015382059797530575, 0.0016878434713473733, 0.0015739939170794524, 0.0015711350204917242, 0.0015777252847328782, 0.0015849154276241148, 0.001529500814991034, 0.001541997284666464, 0.0015467901652374742, 0.002226460979282096, 0.0015830773674901954, 0.0015353905512210057, 0.0018448789594504907, 0.001500188123567828, 0.0015338890192726133, 0.0016392003483500105, 0.0015434279790794362, 0.0015607843251556766, 0.0015476576732091454, 0.0015600287968444886, 0.0015672357748167552, 0.0015413245514073238, 0.0014665192284155637, 0.001477284209007242, 0.00208055575058097, 0.0014848803111817688, 0.0014738150202902034, 0.0016121742276785274, 0.001485319478282084, 0.0015422340632843163, 0.0015834533114684746, 0.0015095321869011968, 0.0014942722070069674, 0.0016103727903100662, 0.0016093164983127888, 0.0021112999578084177, 0.0015278366263373755, 0.004249496708022586, 0.0015079440224023226, 0.0014982102293288335, 0.0015013605831578996, 0.001515677375815964, 0.0015086744994429562, 0.0014972072288704414, 0.0015794544791181881, 0.0015000336473652471, 0.0015451997096533887, 0.0016078359597789433, 0.001682681730017066, 0.0017616428740439005, 0.0016736073133264047, 0.0014893566476530395, 0.0014944400221187, 0.001498454165509126, 0.0014708545204484835, 0.0014914675630279817, 0.0014821552297992942, 0.0015042650848045014, 0.0014768902716847758, 0.0015073568744507309]
[730.6179977559784, 725.1109328738693, 737.185889262466, 682.1506414726653, 652.4065074144066, 742.7439494922081, 663.1030700926015, 709.0656680973019, 541.1075842299292, 763.7446366730899, 725.4980436224296, 710.1605826135577, 682.8346867250376, 711.8007825799411, 714.1586643171582, 693.4058395054586, 693.5633354045809, 637.712837496855, 712.415292257689, 652.1792971012403, 681.5135844440807, 702.5112666709396, 695.9120042686014, 686.958360594088, 686.7124765746154, 651.8983056733404, 546.8870243834667, 654.5311078645918, 705.4519549428255, 653.5268552417646, 694.1559319092412, 700.8995448456149, 691.7074066269689, 624.6084805827517, 684.4248469266427, 701.1269812247656, 507.9691500121198, 708.3439577035449, 693.8792419923083, 684.3951743143604, 699.2989831233818, 672.1458808895349, 619.2383506959751, 634.2846236119135, 697.1082164033229, 708.4749393618195, 654.3816616518816, 707.2189687038093, 601.4583307250078, 700.9470299433675, 700.6703022750257, 709.5281415794108, 685.1896172772307, 704.0940252156172, 657.3839671455512, 681.6231474797313, 699.4255017790554, 638.8497242557219, 638.521781296583, 651.1088329067227, 680.3990263102354, 642.9404665039, 683.9405869802495, 488.59305696910036, 682.2950290809742, 640.5053100350369, 683.877367017091, 701.2529787409453, 700.4602120842123, 659.9459834356358, 701.8945032605533, 183.76105643502927, 671.1216814005102, 704.533978923546, 648.2303171553234, 709.3609190515735, 689.0255267460683, 722.3882887676555, 574.0856072894039, 560.5420665213763, 641.3447954766104, 642.4025455860581, 208.41779569114806, 237.90844870543862, 606.8930666950381, 637.8638091734574, 621.7622914389933, 583.7960538290218, 579.3354768023712, 608.1148157716506, 627.9208571124246, 633.4641642337498, 613.6112505429093, 600.8781081602934, 611.9557954288559, 634.9571658052704, 639.2224461010344, 619.2758534424386, 495.79643558837967, 646.538753302014, 642.7487107984571, 576.6636480860268, 637.5426598111504, 625.3323593895736, 476.1508988723527, 576.3648961831719, 472.69620417486993, 520.9905498741574, 629.4904111635619, 632.5265176457848, 630.7820596231563, 626.5400100108135, 663.815654697484, 609.8966934389749, 511.45868908656036, 599.2800102332421, 531.1896516748984, 623.8927333534562, 627.5583324287634, 539.445724285785, 617.3352861249178, 429.7505687897915, 544.9115122887408, 639.6916196460262, 574.1280174678542, 593.1397624169053, 174.46486830119807, 568.3194954153998, 625.929729147358, 634.0748893565842, 650.1079914931414, 592.4720016849187, 635.3264705466596, 636.4825345736525, 633.8239043746505, 630.9484926265505, 653.8080857484617, 648.5095725809279, 646.5001022594897, 449.143285826838, 631.6810665958772, 651.3000872675417, 542.0409804542714, 666.5830666768286, 651.9376483144855, 610.0535550804282, 647.908430814142, 640.7035128958368, 646.1377198010786, 641.0138082211857, 638.06608812061, 648.7926239071055, 681.886729218277, 676.9178157478685, 480.64080941871515, 673.4549528804325, 678.5112013603266, 620.2803535942662, 673.2558312347704, 648.4100071492497, 631.5310926803472, 662.456891398138, 669.2221104767809, 620.974227841652, 621.3818108795892, 473.6418415117221, 654.52024304278, 235.32198486284486, 663.1545900536074, 667.4630705518402, 666.0625110435773, 659.7710145680908, 662.8335007778201, 667.9102135744052, 633.1299909056585, 666.6517126175553, 647.1655370840801, 621.9539959396648, 594.2894500850484, 567.6519428165778, 597.5117293270152, 671.4308500759853, 669.1469615369898, 667.3544129794805, 679.876892036261, 670.480555386533, 674.693162966078, 664.7764480486915, 677.0983729612093, 663.4129030422157]
Elapsed: 0.08093900690161573~0.027610787182242125
Time per graph: 0.001658488125017583~0.0005645121857089665
Speed: 631.1875091001536~90.75484408475393
Total Time: 0.0730
best val loss: 0.4060693383216858 test_score: 0.9167

Testing...
Test loss: 0.3392 score: 0.8750 time: 0.07s
test Score 0.8750
Epoch Time List: [0.3494270759401843, 0.34380863595288247, 0.3430066700093448, 0.34510894503910094, 0.3538594759302214, 0.3449007129529491, 0.3516954150982201, 0.3470907580340281, 0.3743883731076494, 0.36526425986085087, 0.34738179109990597, 0.36776701803319156, 0.35642679885495454, 0.3598030429566279, 0.35898670786991715, 0.36538390698842704, 0.35066479607485235, 0.3955443089362234, 0.36012659408152103, 0.3658252478344366, 0.38421811489388347, 0.3636845810106024, 0.35787462804000825, 0.36612298304680735, 0.3634975559543818, 0.366100546089001, 0.3762538230512291, 0.36748302006162703, 0.3554438811261207, 0.3686965440865606, 0.3575703620444983, 0.3731384068960324, 0.35569762578234076, 0.3652215781621635, 0.40370197186712176, 0.39809092006180435, 0.3779113250784576, 0.3542541960487142, 0.35896442201919854, 0.3632456319173798, 0.35101814405061305, 0.3714863839559257, 0.37545699009206146, 1.6109206308610737, 0.3510264160577208, 0.35510507912840694, 0.35938038281165063, 0.3578358730301261, 0.37876235297881067, 0.35379342711530626, 0.3546503980178386, 0.36985121108591557, 0.358359741861932, 0.3560706239659339, 0.35667481494601816, 0.36693616094999015, 0.354300226084888, 0.3648258730536327, 0.37712174793705344, 0.40089241205714643, 0.3634860800812021, 0.37497312808409333, 0.3626420749351382, 0.3927286311518401, 0.372055894928053, 0.38454131584148854, 0.41370646690484136, 0.3587131960084662, 0.35741325397975743, 0.3880235581891611, 0.3914188191993162, 0.564742439892143, 0.37826573697384447, 0.3523060920415446, 0.361281278077513, 0.36591598810628057, 0.47198546899016947, 0.3729447789955884, 0.36615589587017894, 0.3922486879164353, 0.36265696899499744, 0.35308761696796864, 1.0114035021979362, 0.7924972089240327, 0.4278995170025155, 0.35766495496500283, 0.38154283398762345, 0.3715513860806823, 0.37608499405905604, 0.3826562830945477, 0.3677120669744909, 0.3499098438769579, 0.3579547351691872, 0.36703296401537955, 0.35652608203236014, 0.35677889501675963, 0.34723212593235075, 0.3493086179951206, 0.4804229730507359, 0.3471498090075329, 0.34898057591635734, 0.35713532206136733, 0.3489942019805312, 0.37258532585110515, 0.37331148399971426, 0.3671359949512407, 0.37875324313063174, 0.3706112060463056, 0.3513661591568962, 0.35078291909303516, 0.35094971617218107, 0.35551079304423183, 0.3526472740340978, 0.3562930040061474, 0.36036413407418877, 0.33926311403047293, 0.359313675086014, 0.34246381593402475, 0.3490786150796339, 0.41977544198744, 0.3572315621422604, 0.39830598107073456, 0.3933265608502552, 0.3669439449440688, 0.35440563107840717, 0.35951381002087146, 0.5830250059952959, 0.4315291449893266, 0.40319607593119144, 0.40426255599595606, 0.3473539330298081, 0.35028477397281677, 0.3590719278436154, 0.3505105768563226, 0.3548807220067829, 0.35253281600307673, 0.3520167809911072, 0.3423903719522059, 0.34230014798231423, 0.3790038279257715, 0.35313491499982774, 0.34822987008374184, 0.36552410398144275, 0.36967516504228115, 0.34564252209383994, 0.36002161097712815, 0.3539026260841638, 0.3763855081051588, 0.3481312090298161, 0.34543825313448906, 0.3480444378219545, 0.34571816585958004, 0.34853287297300994, 0.3482093629427254, 0.37905683810822666, 0.357452638912946, 0.35608783503994346, 0.4277678751386702, 0.47632621589582413, 0.34940544492565095, 0.35256216092966497, 0.35149316187016666, 0.34423921490088105, 0.3666372480802238, 0.37438093801029027, 0.40590545383747667, 0.42845221993047744, 0.4924688689643517, 0.35813677695114166, 0.3556472989730537, 0.35713815700728446, 0.35211489105131477, 0.3516080130357295, 0.35570414108224213, 0.3570630099857226, 0.37460374005604535, 0.3514426799956709, 0.3564339019358158, 0.3617421519011259, 0.3551371758803725, 0.3709414969198406, 0.34370391885749996, 0.34493435791227967, 0.35825415304861963, 0.34226201905403286, 0.3449956640833989, 0.35316515096928924, 0.34889752487652004, 0.34325123799499124, 0.34440965903922915]
Total Epoch List: [78, 74, 38]
Total Time List: [0.06832576694432646, 0.07616047491319478, 0.07300706801470369]
T-times Epoch Time: 0.43674292252508473 ~ 0.04001486658364689
T-times Total Epoch: 65.00000000000001 ~ 2.5962936545662068
T-times Total Time: 0.08720199576216854 ~ 0.011900577778852313
T-times Inference Elapsed: 0.09325601968693054 ~ 0.008748914959203687
T-times Time Per Graph: 0.0019115184436489386 ~ 0.0001797312646611373
T-times Speed: 563.1549325776175 ~ 48.556479746829304
T-times cross validation test micro f1 score:0.9122037335968226 ~ 0.024450005964142436
T-times cross validation test precision:0.9661077650208085 ~ 0.013717588884781058
T-times cross validation test recall:0.8674074074074075 ~ 0.04293341896062826
T-times cross validation test f1_score:0.9122037335968226 ~ 0.028502060670587224
