Namespace(seed=60, model='GPSPerformer', dataset='ico_wallets/Volume', num_heads=1, num_layers=2, dim_hidden=64, dropout=0.1, epochs=1000, lr=0.0003, weight_decay=0.0001, batch_size=64, abs_pe=None, abs_pe_dim=20, num_class=2, outdir='./outdir/ico_wallets/Volume/seed60/edge_attr/khopgnn_gat_1_0.1_0.0003_0.0001_2_1_64_BN', warmup=10, layer_norm=False, use_edge_attr=True, edge_dim=128, gnn_type='gat', k_hop=1, global_pool='mean', se='khopgnn', ie='mamba', aggr='add', not_extract_node_feature=False, k_ford=3, early_stop=1, early_stop_mindelta=-0.0, patience=20, training_times=3, Lambda=0.01, order='sd', use_cuda=True, batch_norm=True, save_logs=True)
Data(edge_index=[2, 334], edge_attr=[334, 2], x=[97, 14887], y=[1, 1], num_nodes=97)
Data(edge_index=[2, 334], edge_attr=[334, 2], x=[97, 14887], y=[1, 1], num_nodes=97)
Data(edge_index=[2, 276], edge_attr=[276, 2], x=[85, 14887], y=[1, 1], num_nodes=97)
========================training times:0========================
========================k_idx:0========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: GPSPerformer
GPS(
  (x_lin): Linear(in_features=14887, out_features=64, bias=True)
  (pe_lin): Linear(in_features=20, out_features=8, bias=True)
  (pe_norm): BatchNorm1d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (convs): ModuleList(
    (0-1): 2 x GPSConv(64, conv=GINEConv(nn=Sequential(
      (0): Linear(in_features=64, out_features=64, bias=True)
      (1): ReLU()
      (2): Linear(in_features=64, out_features=64, bias=True)
    )), heads=1, attn_type=performer)
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=32, bias=True)
    (1): ReLU()
    (2): Linear(in_features=32, out_features=16, bias=True)
    (3): ReLU()
    (4): Linear(in_features=16, out_features=2, bias=True)
  )
)
Total number of parameters: 1039522
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7471244af7f0>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.8670;  Loss pred: 0.8670; Loss self: 0.0000; time: 0.64s
Val loss: 3.7294 score: 0.4694 time: 0.08s
Test loss: 2.5913 score: 0.4694 time: 0.07s
Epoch 2/1000, LR 0.000000
Train loss: 0.8518;  Loss pred: 0.8518; Loss self: 0.0000; time: 0.21s
Val loss: 2.6198 score: 0.4286 time: 0.07s
Test loss: 1.8093 score: 0.4490 time: 0.07s
Epoch 3/1000, LR 0.000030
Train loss: 0.7709;  Loss pred: 0.7709; Loss self: 0.0000; time: 0.21s
Val loss: 1.8654 score: 0.4082 time: 0.07s
Test loss: 1.3232 score: 0.4286 time: 0.06s
Epoch 4/1000, LR 0.000060
Train loss: 0.7483;  Loss pred: 0.7483; Loss self: 0.0000; time: 0.20s
Val loss: 1.3243 score: 0.3469 time: 0.07s
Test loss: 1.0322 score: 0.4082 time: 0.06s
Epoch 5/1000, LR 0.000090
Train loss: 0.7726;  Loss pred: 0.7726; Loss self: 0.0000; time: 0.20s
Val loss: 0.9916 score: 0.3265 time: 0.07s
Test loss: 0.8573 score: 0.3469 time: 0.06s
Epoch 6/1000, LR 0.000120
Train loss: 0.7802;  Loss pred: 0.7802; Loss self: 0.0000; time: 0.20s
Val loss: 0.8314 score: 0.3061 time: 0.07s
Test loss: 0.7623 score: 0.3265 time: 0.06s
Epoch 7/1000, LR 0.000150
Train loss: 0.7132;  Loss pred: 0.7132; Loss self: 0.0000; time: 0.20s
Val loss: 0.7446 score: 0.2857 time: 0.07s
Test loss: 0.7199 score: 0.3469 time: 0.06s
Epoch 8/1000, LR 0.000180
Train loss: 0.6619;  Loss pred: 0.6619; Loss self: 0.0000; time: 0.20s
Val loss: 0.7008 score: 0.3469 time: 0.07s
Test loss: 0.6966 score: 0.3878 time: 0.06s
Epoch 9/1000, LR 0.000210
Train loss: 0.6683;  Loss pred: 0.6683; Loss self: 0.0000; time: 0.20s
Val loss: 0.6762 score: 0.4286 time: 0.07s
Test loss: 0.6837 score: 0.4286 time: 0.06s
Epoch 10/1000, LR 0.000240
Train loss: 0.6218;  Loss pred: 0.6218; Loss self: 0.0000; time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6605 score: 0.4898 time: 0.07s
Test loss: 0.6779 score: 0.4286 time: 0.06s
Epoch 11/1000, LR 0.000270
Train loss: 0.5685;  Loss pred: 0.5685; Loss self: 0.0000; time: 0.20s
Val loss: 0.6552 score: 0.4898 time: 0.07s
Test loss: 0.6799 score: 0.4490 time: 0.06s
Epoch 12/1000, LR 0.000270
Train loss: 0.5451;  Loss pred: 0.5451; Loss self: 0.0000; time: 0.20s
Val loss: 0.6638 score: 0.4286 time: 0.07s
Test loss: 0.6850 score: 0.4082 time: 0.07s
     INFO: Early stopping counter 1 of 20
Epoch 13/1000, LR 0.000270
Train loss: 0.5458;  Loss pred: 0.5458; Loss self: 0.0000; time: 0.20s
Val loss: 0.6753 score: 0.4490 time: 0.07s
Test loss: 0.6914 score: 0.3878 time: 0.06s
     INFO: Early stopping counter 2 of 20
Epoch 14/1000, LR 0.000270
Train loss: 0.4935;  Loss pred: 0.4935; Loss self: 0.0000; time: 0.20s
Val loss: 0.6875 score: 0.4082 time: 0.07s
Test loss: 0.6970 score: 0.3673 time: 0.06s
     INFO: Early stopping counter 3 of 20
Epoch 15/1000, LR 0.000270
Train loss: 0.4769;  Loss pred: 0.4769; Loss self: 0.0000; time: 0.20s
Val loss: 0.7007 score: 0.4082 time: 0.07s
Test loss: 0.7034 score: 0.3673 time: 0.06s
     INFO: Early stopping counter 4 of 20
Epoch 16/1000, LR 0.000270
Train loss: 0.4581;  Loss pred: 0.4581; Loss self: 0.0000; time: 0.20s
Val loss: 0.7208 score: 0.4082 time: 0.07s
Test loss: 0.7137 score: 0.3673 time: 0.06s
     INFO: Early stopping counter 5 of 20
Epoch 17/1000, LR 0.000270
Train loss: 0.4210;  Loss pred: 0.4210; Loss self: 0.0000; time: 0.20s
Val loss: 0.7415 score: 0.4082 time: 0.07s
Test loss: 0.7250 score: 0.3878 time: 0.06s
     INFO: Early stopping counter 6 of 20
Epoch 18/1000, LR 0.000270
Train loss: 0.3927;  Loss pred: 0.3927; Loss self: 0.0000; time: 0.20s
Val loss: 0.7671 score: 0.3878 time: 0.07s
Test loss: 0.7382 score: 0.3878 time: 0.06s
     INFO: Early stopping counter 7 of 20
Epoch 19/1000, LR 0.000270
Train loss: 0.3953;  Loss pred: 0.3953; Loss self: 0.0000; time: 0.20s
Val loss: 0.7948 score: 0.4082 time: 0.07s
Test loss: 0.7516 score: 0.3469 time: 0.06s
     INFO: Early stopping counter 8 of 20
Epoch 20/1000, LR 0.000270
Train loss: 0.3753;  Loss pred: 0.3753; Loss self: 0.0000; time: 0.20s
Val loss: 0.8219 score: 0.4082 time: 0.07s
Test loss: 0.7628 score: 0.3673 time: 0.06s
     INFO: Early stopping counter 9 of 20
Epoch 21/1000, LR 0.000270
Train loss: 0.3089;  Loss pred: 0.3089; Loss self: 0.0000; time: 0.20s
Val loss: 0.8509 score: 0.3673 time: 0.07s
Test loss: 0.7740 score: 0.4082 time: 0.06s
     INFO: Early stopping counter 10 of 20
Epoch 22/1000, LR 0.000270
Train loss: 0.2819;  Loss pred: 0.2819; Loss self: 0.0000; time: 0.20s
Val loss: 0.8721 score: 0.3469 time: 0.07s
Test loss: 0.7822 score: 0.4490 time: 0.06s
     INFO: Early stopping counter 11 of 20
Epoch 23/1000, LR 0.000270
Train loss: 0.2771;  Loss pred: 0.2771; Loss self: 0.0000; time: 0.21s
Val loss: 0.8807 score: 0.4082 time: 0.07s
Test loss: 0.7885 score: 0.4694 time: 0.06s
     INFO: Early stopping counter 12 of 20
Epoch 24/1000, LR 0.000270
Train loss: 0.2554;  Loss pred: 0.2554; Loss self: 0.0000; time: 0.21s
Val loss: 0.8801 score: 0.4082 time: 0.07s
Test loss: 0.7879 score: 0.4694 time: 0.06s
     INFO: Early stopping counter 13 of 20
Epoch 25/1000, LR 0.000270
Train loss: 0.2189;  Loss pred: 0.2189; Loss self: 0.0000; time: 0.20s
Val loss: 0.8816 score: 0.4082 time: 0.07s
Test loss: 0.7856 score: 0.4694 time: 0.06s
     INFO: Early stopping counter 14 of 20
Epoch 26/1000, LR 0.000270
Train loss: 0.2133;  Loss pred: 0.2133; Loss self: 0.0000; time: 0.20s
Val loss: 0.8909 score: 0.4082 time: 0.07s
Test loss: 0.7849 score: 0.4694 time: 0.06s
     INFO: Early stopping counter 15 of 20
Epoch 27/1000, LR 0.000270
Train loss: 0.2082;  Loss pred: 0.2082; Loss self: 0.0000; time: 0.20s
Val loss: 0.9072 score: 0.4082 time: 0.07s
Test loss: 0.7880 score: 0.4694 time: 0.07s
     INFO: Early stopping counter 16 of 20
Epoch 28/1000, LR 0.000270
Train loss: 0.1768;  Loss pred: 0.1768; Loss self: 0.0000; time: 0.20s
Val loss: 0.9259 score: 0.4082 time: 0.07s
Test loss: 0.7908 score: 0.4694 time: 0.06s
     INFO: Early stopping counter 17 of 20
Epoch 29/1000, LR 0.000270
Train loss: 0.1560;  Loss pred: 0.1560; Loss self: 0.0000; time: 0.20s
Val loss: 0.9392 score: 0.4082 time: 0.07s
Test loss: 0.7902 score: 0.4490 time: 0.06s
     INFO: Early stopping counter 18 of 20
Epoch 30/1000, LR 0.000270
Train loss: 0.1518;  Loss pred: 0.1518; Loss self: 0.0000; time: 0.20s
Val loss: 0.9461 score: 0.4082 time: 0.07s
Test loss: 0.7847 score: 0.4694 time: 0.06s
     INFO: Early stopping counter 19 of 20
Epoch 31/1000, LR 0.000270
Train loss: 0.1418;  Loss pred: 0.1418; Loss self: 0.0000; time: 0.20s
Val loss: 0.9563 score: 0.4082 time: 0.07s
Test loss: 0.7782 score: 0.4898 time: 0.06s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 010,   Train_Loss: 0.5685,   Val_Loss: 0.6552,   Val_Precision: 0.4894,   Val_Recall: 0.9583,   Val_accuracy: 0.6479,   Val_Score: 0.4898,   Val_Loss: 0.6552,   Test_Precision: 0.4750,   Test_Recall: 0.7600,   Test_accuracy: 0.5846,   Test_Score: 0.4490,   Test_loss: 0.6799


[0.07183226896449924, 0.07066955091431737, 0.06666709599085152, 0.06680618994869292, 0.06617387395817786, 0.06620889401528984, 0.06614625896327198, 0.06642150203697383, 0.06610952003393322, 0.06619310798123479, 0.06661582796368748, 0.06953462411183864, 0.0659220510860905, 0.06524433102458715, 0.06628224300220609, 0.06585768598597497, 0.06646143202669919, 0.06641644204501063, 0.06610813795123249, 0.0666603670688346, 0.06588912603911012, 0.06649678095709532, 0.06628153007477522, 0.06651531893294305, 0.06612605392001569, 0.06686647597234696, 0.07874028792139143, 0.06653362791985273, 0.06653946998994797, 0.06655587302520871, 0.06669855699874461]
[0.0014659646727448823, 0.0014422357329452525, 0.001360552979405133, 0.0013633916316059778, 0.001350487223636283, 0.0013512019186793845, 0.001349923652311673, 0.001355540857897425, 0.0013491738782435351, 0.0013508797547190773, 0.0013595066931364791, 0.0014190739614660948, 0.0013453479813487859, 0.0013315169596854522, 0.001352698836779716, 0.0013440344078770401, 0.001356355755646922, 0.001355437592755319, 0.0013491456724741325, 0.0013604156544660122, 0.0013446760416144924, 0.0013570771623897006, 0.0013526842872403106, 0.001357455488427409, 0.0013495113044901161, 0.0013646219586193257, 0.001606944651456968, 0.0013578291412214844, 0.0013579483671417954, 0.001358283122963443, 0.0013611950407907063]
[682.1446782394784, 693.3679267243341, 734.9952667313436, 733.4649684053521, 740.4734991179175, 740.0818383808717, 740.782634845721, 737.7129167106749, 741.194308699396, 740.2583364704843, 735.5609244504185, 704.6849051946984, 743.3021150389987, 751.023103931198, 739.2628520185884, 744.0285710985203, 737.2696992191727, 737.7691199837617, 741.2098043987709, 735.0694596296145, 743.6735459340413, 736.8777750552391, 739.2708035665571, 736.6724054860056, 741.0089835281732, 732.8036850673011, 622.2989690984879, 736.4696850595016, 736.4050240767226, 736.2235332927081, 734.6485771936906]
Elapsed: 0.06714756473628504~0.002513546609438898
Time per graph: 0.0013703584640058167~5.129686958038568e-05
Speed: 730.6454811821852~24.32847535074877
Total Time: 0.0670
best val loss: 0.6552472114562988 test_score: 0.4490

Testing...
Test loss: 0.6779 score: 0.4286 time: 0.06s
test Score 0.4286
Epoch Time List: [0.789603787008673, 0.3515524949179962, 0.34818410605657846, 0.3314679190516472, 0.33057265810202807, 0.33327458309940994, 0.33434025093447417, 0.33133895287755877, 0.33047797286417335, 0.3311267029494047, 0.33624900120776147, 0.33663346990942955, 0.3327974519925192, 0.32999489607755095, 0.3291991689475253, 0.3293298820499331, 0.33035780815407634, 0.3396348259411752, 0.3303027309011668, 0.3290999151067808, 0.3306786580942571, 0.3361299270763993, 0.3436849989229813, 0.3421441729879007, 0.3291229928145185, 0.3298890038859099, 0.34659577999264, 0.3302276679314673, 0.3295294909039512, 0.33133498299866915, 0.3296031770296395]
Total Epoch List: [31]
Total Time List: [0.06704080104827881]
========================k_idx:1========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: GPSPerformer
GPS(
  (x_lin): Linear(in_features=14887, out_features=64, bias=True)
  (pe_lin): Linear(in_features=20, out_features=8, bias=True)
  (pe_norm): BatchNorm1d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (convs): ModuleList(
    (0-1): 2 x GPSConv(64, conv=GINEConv(nn=Sequential(
      (0): Linear(in_features=64, out_features=64, bias=True)
      (1): ReLU()
      (2): Linear(in_features=64, out_features=64, bias=True)
    )), heads=1, attn_type=performer)
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=32, bias=True)
    (1): ReLU()
    (2): Linear(in_features=32, out_features=16, bias=True)
    (3): ReLU()
    (4): Linear(in_features=16, out_features=2, bias=True)
  )
)
Total number of parameters: 1039522
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7471244af4c0>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.9729;  Loss pred: 0.9729; Loss self: 0.0000; time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.8922 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.8218 score: 0.4898 time: 0.08s
Epoch 2/1000, LR 0.000000
Train loss: 0.9663;  Loss pred: 0.9663; Loss self: 0.0000; time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.8143 score: 0.5102 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7471 score: 0.4898 time: 0.08s
Epoch 3/1000, LR 0.000030
Train loss: 0.9403;  Loss pred: 0.9403; Loss self: 0.0000; time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7868 score: 0.5102 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7216 score: 0.4898 time: 0.08s
Epoch 4/1000, LR 0.000060
Train loss: 0.7992;  Loss pred: 0.7992; Loss self: 0.0000; time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7821 score: 0.5102 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7202 score: 0.4898 time: 0.07s
Epoch 5/1000, LR 0.000090
Train loss: 0.7928;  Loss pred: 0.7928; Loss self: 0.0000; time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7844 score: 0.5102 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7317 score: 0.4898 time: 0.10s
     INFO: Early stopping counter 1 of 20
Epoch 6/1000, LR 0.000120
Train loss: 0.6837;  Loss pred: 0.6837; Loss self: 0.0000; time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7922 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7474 score: 0.4898 time: 0.08s
     INFO: Early stopping counter 2 of 20
Epoch 7/1000, LR 0.000150
Train loss: 0.6989;  Loss pred: 0.6989; Loss self: 0.0000; time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.8033 score: 0.5102 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7535 score: 0.4898 time: 0.08s
     INFO: Early stopping counter 3 of 20
Epoch 8/1000, LR 0.000180
Train loss: 0.7020;  Loss pred: 0.7020; Loss self: 0.0000; time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.8088 score: 0.5102 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7608 score: 0.4898 time: 0.08s
     INFO: Early stopping counter 4 of 20
Epoch 9/1000, LR 0.000210
Train loss: 0.6485;  Loss pred: 0.6485; Loss self: 0.0000; time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.8149 score: 0.5102 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7682 score: 0.4898 time: 0.08s
     INFO: Early stopping counter 5 of 20
Epoch 10/1000, LR 0.000240
Train loss: 0.5663;  Loss pred: 0.5663; Loss self: 0.0000; time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.8196 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7723 score: 0.4898 time: 0.08s
     INFO: Early stopping counter 6 of 20
Epoch 11/1000, LR 0.000270
Train loss: 0.5995;  Loss pred: 0.5995; Loss self: 0.0000; time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.8133 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7649 score: 0.4898 time: 0.08s
     INFO: Early stopping counter 7 of 20
Epoch 12/1000, LR 0.000270
Train loss: 0.5778;  Loss pred: 0.5778; Loss self: 0.0000; time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.8056 score: 0.5102 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7561 score: 0.4898 time: 0.09s
     INFO: Early stopping counter 8 of 20
Epoch 13/1000, LR 0.000270
Train loss: 0.5257;  Loss pred: 0.5257; Loss self: 0.0000; time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7907 score: 0.5102 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7386 score: 0.4898 time: 0.07s
     INFO: Early stopping counter 9 of 20
Epoch 14/1000, LR 0.000270
Train loss: 0.4992;  Loss pred: 0.4992; Loss self: 0.0000; time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7685 score: 0.5102 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7116 score: 0.4898 time: 0.07s
Epoch 15/1000, LR 0.000270
Train loss: 0.4554;  Loss pred: 0.4554; Loss self: 0.0000; time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7467 score: 0.5102 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6850 score: 0.4898 time: 0.08s
Epoch 16/1000, LR 0.000270
Train loss: 0.4325;  Loss pred: 0.4325; Loss self: 0.0000; time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7314 score: 0.5102 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6656 score: 0.4898 time: 0.07s
Epoch 17/1000, LR 0.000270
Train loss: 0.3936;  Loss pred: 0.3936; Loss self: 0.0000; time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7152 score: 0.5102 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6459 score: 0.4898 time: 0.08s
Epoch 18/1000, LR 0.000270
Train loss: 0.3634;  Loss pred: 0.3634; Loss self: 0.0000; time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7006 score: 0.5102 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6280 score: 0.4898 time: 0.07s
Epoch 19/1000, LR 0.000270
Train loss: 0.3576;  Loss pred: 0.3576; Loss self: 0.0000; time: 0.21s
Val loss: 0.6860 score: 0.4898 time: 0.06s
Test loss: 0.6091 score: 0.5102 time: 0.07s
Epoch 20/1000, LR 0.000270
Train loss: 0.3069;  Loss pred: 0.3069; Loss self: 0.0000; time: 0.21s
Val loss: 0.6719 score: 0.4694 time: 0.07s
Test loss: 0.5912 score: 0.5102 time: 0.08s
Epoch 21/1000, LR 0.000270
Train loss: 0.2987;  Loss pred: 0.2987; Loss self: 0.0000; time: 0.20s
Val loss: 0.6596 score: 0.4694 time: 0.06s
Test loss: 0.5744 score: 0.5306 time: 0.07s
Epoch 22/1000, LR 0.000270
Train loss: 0.2725;  Loss pred: 0.2725; Loss self: 0.0000; time: 0.21s
Val loss: 0.6526 score: 0.4898 time: 0.06s
Test loss: 0.5573 score: 0.5102 time: 0.07s
Epoch 23/1000, LR 0.000270
Train loss: 0.2517;  Loss pred: 0.2517; Loss self: 0.0000; time: 0.21s
Val loss: 0.6552 score: 0.5306 time: 0.07s
Test loss: 0.5473 score: 0.4898 time: 0.07s
     INFO: Early stopping counter 1 of 20
Epoch 24/1000, LR 0.000270
Train loss: 0.2081;  Loss pred: 0.2081; Loss self: 0.0000; time: 0.20s
Val loss: 0.6613 score: 0.5102 time: 0.06s
Test loss: 0.5443 score: 0.5510 time: 0.07s
     INFO: Early stopping counter 2 of 20
Epoch 25/1000, LR 0.000270
Train loss: 0.2181;  Loss pred: 0.2181; Loss self: 0.0000; time: 0.20s
Val loss: 0.6652 score: 0.5102 time: 0.06s
Test loss: 0.5422 score: 0.5918 time: 0.07s
     INFO: Early stopping counter 3 of 20
Epoch 26/1000, LR 0.000270
Train loss: 0.1854;  Loss pred: 0.1854; Loss self: 0.0000; time: 0.20s
Val loss: 0.6682 score: 0.5102 time: 0.07s
Test loss: 0.5405 score: 0.6122 time: 0.08s
     INFO: Early stopping counter 4 of 20
Epoch 27/1000, LR 0.000270
Train loss: 0.1669;  Loss pred: 0.1669; Loss self: 0.0000; time: 0.20s
Val loss: 0.6681 score: 0.5102 time: 0.06s
Test loss: 0.5370 score: 0.6122 time: 0.08s
     INFO: Early stopping counter 5 of 20
Epoch 28/1000, LR 0.000270
Train loss: 0.1700;  Loss pred: 0.1700; Loss self: 0.0000; time: 0.20s
Val loss: 0.6643 score: 0.5102 time: 0.06s
Test loss: 0.5307 score: 0.6327 time: 0.08s
     INFO: Early stopping counter 6 of 20
Epoch 29/1000, LR 0.000270
Train loss: 0.1443;  Loss pred: 0.1443; Loss self: 0.0000; time: 0.20s
Val loss: 0.6585 score: 0.5306 time: 0.07s
Test loss: 0.5220 score: 0.7143 time: 0.08s
     INFO: Early stopping counter 7 of 20
Epoch 30/1000, LR 0.000270
Train loss: 0.1341;  Loss pred: 0.1341; Loss self: 0.0000; time: 0.20s
Val loss: 0.6624 score: 0.5918 time: 0.06s
Test loss: 0.5201 score: 0.7143 time: 0.07s
     INFO: Early stopping counter 8 of 20
Epoch 31/1000, LR 0.000270
Train loss: 0.1215;  Loss pred: 0.1215; Loss self: 0.0000; time: 0.20s
Val loss: 0.6699 score: 0.6327 time: 0.07s
Test loss: 0.5213 score: 0.7143 time: 0.07s
     INFO: Early stopping counter 9 of 20
Epoch 32/1000, LR 0.000270
Train loss: 0.1004;  Loss pred: 0.1004; Loss self: 0.0000; time: 0.20s
Val loss: 0.6685 score: 0.6327 time: 0.06s
Test loss: 0.5158 score: 0.7143 time: 0.07s
     INFO: Early stopping counter 10 of 20
Epoch 33/1000, LR 0.000270
Train loss: 0.0904;  Loss pred: 0.0904; Loss self: 0.0000; time: 0.20s
Val loss: 0.6578 score: 0.6327 time: 0.07s
Test loss: 0.5025 score: 0.7755 time: 0.08s
     INFO: Early stopping counter 11 of 20
Epoch 34/1000, LR 0.000270
Train loss: 0.0791;  Loss pred: 0.0791; Loss self: 0.0000; time: 0.20s
Val loss: 0.6375 score: 0.6735 time: 0.06s
Test loss: 0.4837 score: 0.7959 time: 0.07s
Epoch 35/1000, LR 0.000270
Train loss: 0.0894;  Loss pred: 0.0894; Loss self: 0.0000; time: 0.20s
Val loss: 0.6132 score: 0.6939 time: 0.06s
Test loss: 0.4673 score: 0.8163 time: 0.07s
Epoch 36/1000, LR 0.000270
Train loss: 0.0869;  Loss pred: 0.0869; Loss self: 0.0000; time: 0.20s
Val loss: 0.5912 score: 0.6327 time: 0.07s
Test loss: 0.4547 score: 0.7959 time: 0.07s
Epoch 37/1000, LR 0.000270
Train loss: 0.0912;  Loss pred: 0.0912; Loss self: 0.0000; time: 0.20s
Val loss: 0.5718 score: 0.6531 time: 0.06s
Test loss: 0.4453 score: 0.7959 time: 0.07s
Epoch 38/1000, LR 0.000270
Train loss: 0.0586;  Loss pred: 0.0586; Loss self: 0.0000; time: 0.20s
Val loss: 0.5518 score: 0.6531 time: 0.07s
Test loss: 0.4377 score: 0.8367 time: 0.08s
Epoch 39/1000, LR 0.000269
Train loss: 0.0456;  Loss pred: 0.0456; Loss self: 0.0000; time: 0.20s
Val loss: 0.5364 score: 0.6735 time: 0.06s
Test loss: 0.4277 score: 0.8367 time: 0.07s
Epoch 40/1000, LR 0.000269
Train loss: 0.0464;  Loss pred: 0.0464; Loss self: 0.0000; time: 0.20s
Val loss: 0.5256 score: 0.7755 time: 0.06s
Test loss: 0.4152 score: 0.8367 time: 0.07s
Epoch 41/1000, LR 0.000269
Train loss: 0.0594;  Loss pred: 0.0594; Loss self: 0.0000; time: 0.20s
Val loss: 0.5149 score: 0.7959 time: 0.06s
Test loss: 0.4032 score: 0.8367 time: 0.07s
Epoch 42/1000, LR 0.000269
Train loss: 0.0299;  Loss pred: 0.0299; Loss self: 0.0000; time: 0.20s
Val loss: 0.5044 score: 0.7959 time: 0.06s
Test loss: 0.3908 score: 0.8776 time: 0.07s
Epoch 43/1000, LR 0.000269
Train loss: 0.0279;  Loss pred: 0.0279; Loss self: 0.0000; time: 0.20s
Val loss: 0.4928 score: 0.7959 time: 0.06s
Test loss: 0.3786 score: 0.8776 time: 0.07s
Epoch 44/1000, LR 0.000269
Train loss: 0.0300;  Loss pred: 0.0300; Loss self: 0.0000; time: 0.22s
Val loss: 0.4836 score: 0.8163 time: 0.07s
Test loss: 0.3668 score: 0.8776 time: 0.07s
Epoch 45/1000, LR 0.000269
Train loss: 0.0280;  Loss pred: 0.0280; Loss self: 0.0000; time: 0.21s
Val loss: 0.4717 score: 0.8367 time: 0.06s
Test loss: 0.3557 score: 0.8980 time: 0.07s
Epoch 46/1000, LR 0.000269
Train loss: 0.0232;  Loss pred: 0.0232; Loss self: 0.0000; time: 0.21s
Val loss: 0.4601 score: 0.8980 time: 0.06s
Test loss: 0.3447 score: 0.9388 time: 0.07s
Epoch 47/1000, LR 0.000269
Train loss: 0.0290;  Loss pred: 0.0290; Loss self: 0.0000; time: 0.20s
Val loss: 0.4460 score: 0.9184 time: 0.06s
Test loss: 0.3347 score: 0.9388 time: 0.07s
Epoch 48/1000, LR 0.000269
Train loss: 0.0198;  Loss pred: 0.0198; Loss self: 0.0000; time: 0.21s
Val loss: 0.4346 score: 0.9184 time: 0.06s
Test loss: 0.3245 score: 0.9388 time: 0.08s
Epoch 49/1000, LR 0.000269
Train loss: 0.0156;  Loss pred: 0.0156; Loss self: 0.0000; time: 0.20s
Val loss: 0.4252 score: 0.9184 time: 0.07s
Test loss: 0.3148 score: 0.9388 time: 0.07s
Epoch 50/1000, LR 0.000269
Train loss: 0.0177;  Loss pred: 0.0177; Loss self: 0.0000; time: 0.21s
Val loss: 0.4176 score: 0.9184 time: 0.06s
Test loss: 0.3048 score: 0.9388 time: 0.07s
Epoch 51/1000, LR 0.000269
Train loss: 0.0163;  Loss pred: 0.0163; Loss self: 0.0000; time: 0.21s
Val loss: 0.4126 score: 0.9184 time: 0.07s
Test loss: 0.2942 score: 0.9388 time: 0.08s
Epoch 52/1000, LR 0.000269
Train loss: 0.0116;  Loss pred: 0.0116; Loss self: 0.0000; time: 0.21s
Val loss: 0.4083 score: 0.8980 time: 0.06s
Test loss: 0.2836 score: 0.9796 time: 0.07s
Epoch 53/1000, LR 0.000269
Train loss: 0.0118;  Loss pred: 0.0118; Loss self: 0.0000; time: 0.21s
Val loss: 0.4062 score: 0.8980 time: 0.06s
Test loss: 0.2734 score: 0.9796 time: 0.07s
Epoch 54/1000, LR 0.000269
Train loss: 0.0115;  Loss pred: 0.0115; Loss self: 0.0000; time: 0.20s
Val loss: 0.4050 score: 0.8776 time: 0.07s
Test loss: 0.2634 score: 0.9796 time: 0.07s
Epoch 55/1000, LR 0.000269
Train loss: 0.0107;  Loss pred: 0.0107; Loss self: 0.0000; time: 0.20s
Val loss: 0.4045 score: 0.8776 time: 0.07s
Test loss: 0.2544 score: 0.9796 time: 0.07s
Epoch 56/1000, LR 0.000269
Train loss: 0.0087;  Loss pred: 0.0087; Loss self: 0.0000; time: 0.20s
Val loss: 0.4056 score: 0.8571 time: 0.06s
Test loss: 0.2466 score: 0.9796 time: 0.07s
     INFO: Early stopping counter 1 of 20
Epoch 57/1000, LR 0.000269
Train loss: 0.0074;  Loss pred: 0.0074; Loss self: 0.0000; time: 0.20s
Val loss: 0.4048 score: 0.8367 time: 0.06s
Test loss: 0.2395 score: 0.9796 time: 0.07s
     INFO: Early stopping counter 2 of 20
Epoch 58/1000, LR 0.000269
Train loss: 0.0070;  Loss pred: 0.0070; Loss self: 0.0000; time: 0.21s
Val loss: 0.4043 score: 0.8367 time: 0.06s
Test loss: 0.2325 score: 0.9796 time: 0.07s
Epoch 59/1000, LR 0.000268
Train loss: 0.0062;  Loss pred: 0.0062; Loss self: 0.0000; time: 0.20s
Val loss: 0.4054 score: 0.8367 time: 0.06s
Test loss: 0.2257 score: 0.9796 time: 0.07s
     INFO: Early stopping counter 1 of 20
Epoch 60/1000, LR 0.000268
Train loss: 0.0085;  Loss pred: 0.0085; Loss self: 0.0000; time: 0.20s
Val loss: 0.4058 score: 0.8163 time: 0.06s
Test loss: 0.2183 score: 0.9592 time: 0.07s
     INFO: Early stopping counter 2 of 20
Epoch 61/1000, LR 0.000268
Train loss: 0.0063;  Loss pred: 0.0063; Loss self: 0.0000; time: 0.21s
Val loss: 0.4064 score: 0.7959 time: 0.06s
Test loss: 0.2113 score: 0.9592 time: 0.07s
     INFO: Early stopping counter 3 of 20
Epoch 62/1000, LR 0.000268
Train loss: 0.0062;  Loss pred: 0.0062; Loss self: 0.0000; time: 0.21s
Val loss: 0.4087 score: 0.7959 time: 0.06s
Test loss: 0.2047 score: 0.9592 time: 0.07s
     INFO: Early stopping counter 4 of 20
Epoch 63/1000, LR 0.000268
Train loss: 0.0057;  Loss pred: 0.0057; Loss self: 0.0000; time: 0.21s
Val loss: 0.4123 score: 0.7959 time: 0.06s
Test loss: 0.1984 score: 0.9592 time: 0.07s
     INFO: Early stopping counter 5 of 20
Epoch 64/1000, LR 0.000268
Train loss: 0.0046;  Loss pred: 0.0046; Loss self: 0.0000; time: 0.21s
Val loss: 0.4126 score: 0.7959 time: 0.07s
Test loss: 0.1909 score: 0.9592 time: 0.07s
     INFO: Early stopping counter 6 of 20
Epoch 65/1000, LR 0.000268
Train loss: 0.0039;  Loss pred: 0.0039; Loss self: 0.0000; time: 0.20s
Val loss: 0.4127 score: 0.7959 time: 0.06s
Test loss: 0.1833 score: 0.9592 time: 0.07s
     INFO: Early stopping counter 7 of 20
Epoch 66/1000, LR 0.000268
Train loss: 0.0048;  Loss pred: 0.0048; Loss self: 0.0000; time: 0.21s
Val loss: 0.4095 score: 0.7959 time: 0.06s
Test loss: 0.1749 score: 0.9796 time: 0.07s
     INFO: Early stopping counter 8 of 20
Epoch 67/1000, LR 0.000268
Train loss: 0.0031;  Loss pred: 0.0031; Loss self: 0.0000; time: 0.20s
Val loss: 0.4060 score: 0.7959 time: 0.06s
Test loss: 0.1667 score: 0.9796 time: 0.07s
     INFO: Early stopping counter 9 of 20
Epoch 68/1000, LR 0.000268
Train loss: 0.0036;  Loss pred: 0.0036; Loss self: 0.0000; time: 0.20s
Val loss: 0.4064 score: 0.7959 time: 0.06s
Test loss: 0.1600 score: 0.9796 time: 0.07s
     INFO: Early stopping counter 10 of 20
Epoch 69/1000, LR 0.000268
Train loss: 0.0030;  Loss pred: 0.0030; Loss self: 0.0000; time: 0.20s
Val loss: 0.4074 score: 0.7959 time: 0.06s
Test loss: 0.1534 score: 0.9796 time: 0.07s
     INFO: Early stopping counter 11 of 20
Epoch 70/1000, LR 0.000268
Train loss: 0.0033;  Loss pred: 0.0033; Loss self: 0.0000; time: 0.20s
Val loss: 0.4130 score: 0.7959 time: 0.06s
Test loss: 0.1485 score: 0.9796 time: 0.07s
     INFO: Early stopping counter 12 of 20
Epoch 71/1000, LR 0.000268
Train loss: 0.0027;  Loss pred: 0.0027; Loss self: 0.0000; time: 0.20s
Val loss: 0.4209 score: 0.7959 time: 0.06s
Test loss: 0.1443 score: 0.9796 time: 0.07s
     INFO: Early stopping counter 13 of 20
Epoch 72/1000, LR 0.000267
Train loss: 0.0031;  Loss pred: 0.0031; Loss self: 0.0000; time: 0.21s
Val loss: 0.4339 score: 0.7959 time: 0.06s
Test loss: 0.1418 score: 0.9592 time: 0.07s
     INFO: Early stopping counter 14 of 20
Epoch 73/1000, LR 0.000267
Train loss: 0.0026;  Loss pred: 0.0026; Loss self: 0.0000; time: 0.20s
Val loss: 0.4509 score: 0.7959 time: 0.06s
Test loss: 0.1412 score: 0.9592 time: 0.07s
     INFO: Early stopping counter 15 of 20
Epoch 74/1000, LR 0.000267
Train loss: 0.0028;  Loss pred: 0.0028; Loss self: 0.0000; time: 0.21s
Val loss: 0.4683 score: 0.7959 time: 0.09s
Test loss: 0.1404 score: 0.9592 time: 0.07s
     INFO: Early stopping counter 16 of 20
Epoch 75/1000, LR 0.000267
Train loss: 0.0025;  Loss pred: 0.0025; Loss self: 0.0000; time: 0.20s
Val loss: 0.4870 score: 0.7959 time: 0.06s
Test loss: 0.1404 score: 0.9388 time: 0.07s
     INFO: Early stopping counter 17 of 20
Epoch 76/1000, LR 0.000267
Train loss: 0.0022;  Loss pred: 0.0022; Loss self: 0.0000; time: 0.20s
Val loss: 0.5056 score: 0.7959 time: 0.07s
Test loss: 0.1403 score: 0.9388 time: 0.07s
     INFO: Early stopping counter 18 of 20
Epoch 77/1000, LR 0.000267
Train loss: 0.0022;  Loss pred: 0.0022; Loss self: 0.0000; time: 0.20s
Val loss: 0.5244 score: 0.7755 time: 0.06s
Test loss: 0.1409 score: 0.9388 time: 0.07s
     INFO: Early stopping counter 19 of 20
Epoch 78/1000, LR 0.000267
Train loss: 0.0026;  Loss pred: 0.0026; Loss self: 0.0000; time: 0.21s
Val loss: 0.5388 score: 0.7755 time: 0.07s
Test loss: 0.1406 score: 0.9388 time: 0.09s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 057,   Train_Loss: 0.0070,   Val_Loss: 0.4043,   Val_Precision: 1.0000,   Val_Recall: 0.6800,   Val_accuracy: 0.8095,   Val_Score: 0.8367,   Val_Loss: 0.4043,   Test_Precision: 1.0000,   Test_Recall: 0.9583,   Test_accuracy: 0.9787,   Test_Score: 0.9796,   Test_loss: 0.2325


[0.07183226896449924, 0.07066955091431737, 0.06666709599085152, 0.06680618994869292, 0.06617387395817786, 0.06620889401528984, 0.06614625896327198, 0.06642150203697383, 0.06610952003393322, 0.06619310798123479, 0.06661582796368748, 0.06953462411183864, 0.0659220510860905, 0.06524433102458715, 0.06628224300220609, 0.06585768598597497, 0.06646143202669919, 0.06641644204501063, 0.06610813795123249, 0.0666603670688346, 0.06588912603911012, 0.06649678095709532, 0.06628153007477522, 0.06651531893294305, 0.06612605392001569, 0.06686647597234696, 0.07874028792139143, 0.06653362791985273, 0.06653946998994797, 0.06655587302520871, 0.06669855699874461, 0.08047528099268675, 0.08071389002725482, 0.07968461106065661, 0.07560722797643393, 0.10051846993155777, 0.08063526195473969, 0.08002773497719318, 0.07993994106072932, 0.0803271159529686, 0.08098864508792758, 0.08349327999167144, 0.09419015294406563, 0.07882135291583836, 0.07966346596367657, 0.0797482340130955, 0.07546261698007584, 0.07994055899325758, 0.07941729307640344, 0.07583263493143022, 0.08137358690146357, 0.07510754407849163, 0.07475107698701322, 0.07457994192373008, 0.07937935995869339, 0.07841540593653917, 0.08033831208012998, 0.07969032903201878, 0.08052174304611981, 0.08101129799615592, 0.07881124399136752, 0.07504715607501566, 0.07436314492952079, 0.08022542099934071, 0.0768488550093025, 0.07466370693873614, 0.07932952104602009, 0.07886962406337261, 0.0796743449755013, 0.07465868606232107, 0.07930872996803373, 0.07878492004238069, 0.0789419780485332, 0.07432563498150557, 0.0794845090713352, 0.07841015397571027, 0.07910803891718388, 0.0742310609202832, 0.0799633659189567, 0.07495951501186937, 0.07480033696629107, 0.0805277859326452, 0.07502343005035073, 0.07398797699715942, 0.07826610608026385, 0.07784939301200211, 0.07762210699729621, 0.07370931701734662, 0.07735013298224658, 0.07877332693897188, 0.07297379104420543, 0.0729154369328171, 0.07827008608728647, 0.07494857104029506, 0.07834939402528107, 0.07329739606939256, 0.07808117603417486, 0.07776729902252555, 0.07825949799735099, 0.07789409800898284, 0.07782649400178343, 0.07315799093339592, 0.07327712792903185, 0.07796568889170885, 0.07923640904482454, 0.07751155795995146, 0.07778248202521354, 0.0772358130197972, 0.09802943305112422]
[0.0014659646727448823, 0.0014422357329452525, 0.001360552979405133, 0.0013633916316059778, 0.001350487223636283, 0.0013512019186793845, 0.001349923652311673, 0.001355540857897425, 0.0013491738782435351, 0.0013508797547190773, 0.0013595066931364791, 0.0014190739614660948, 0.0013453479813487859, 0.0013315169596854522, 0.001352698836779716, 0.0013440344078770401, 0.001356355755646922, 0.001355437592755319, 0.0013491456724741325, 0.0013604156544660122, 0.0013446760416144924, 0.0013570771623897006, 0.0013526842872403106, 0.001357455488427409, 0.0013495113044901161, 0.0013646219586193257, 0.001606944651456968, 0.0013578291412214844, 0.0013579483671417954, 0.001358283122963443, 0.0013611950407907063, 0.0016423526733201376, 0.00164722224545418, 0.0016262165522582981, 0.0015430046525802842, 0.0020513973455419956, 0.0016456175909130549, 0.0016332190811672077, 0.0016314273685863127, 0.0016393288969993591, 0.0016528294915903587, 0.0017039444896259478, 0.0019222480192666455, 0.001608599039098742, 0.0016257850196668689, 0.0016275149798590917, 0.0015400534077566496, 0.0016314399794542364, 0.001620761083191907, 0.0015476047945189842, 0.0016606854469686442, 0.0015328070220100333, 0.001525532183408433, 0.0015220396310965322, 0.001619986937932518, 0.0016003144068681464, 0.0016395573893904078, 0.0016263332455514036, 0.001643300878492241, 0.0016532917958399166, 0.0016083927345177044, 0.0015315746137758298, 0.0015176152026432814, 0.0016372534897824635, 0.0015683439797816836, 0.0015237491211986967, 0.001618969817265716, 0.0016095841645586248, 0.001626007040316353, 0.001523646654333083, 0.0016185455095517089, 0.0016078555110689936, 0.0016110607765006777, 0.0015168496935001137, 0.0016221328381905143, 0.0016002072239940871, 0.0016144497738200792, 0.0015149196106180244, 0.0016319054269174837, 0.0015297860206503952, 0.001526537489107981, 0.0016434242027070448, 0.001531090409190831, 0.0015099587142277434, 0.001597267471025793, 0.0015887631226939205, 0.001584124632597882, 0.0015042717758642168, 0.0015785741424948281, 0.001607618917121875, 0.001489261041718478, 0.0014880701414860633, 0.0015973486956589076, 0.001529562674291736, 0.0015989672250057362, 0.0014958652259059707, 0.0015934933884525482, 0.0015870877351535826, 0.0015971326121908365, 0.0015896754695710782, 0.0015882957959547639, 0.001493020223130529, 0.0014954515903884051, 0.0015911365079940582, 0.0016170695723433579, 0.0015818685297949277, 0.0015873975923512966, 0.0015762410820366777, 0.002000600674512739]
[682.1446782394784, 693.3679267243341, 734.9952667313436, 733.4649684053521, 740.4734991179175, 740.0818383808717, 740.782634845721, 737.7129167106749, 741.194308699396, 740.2583364704843, 735.5609244504185, 704.6849051946984, 743.3021150389987, 751.023103931198, 739.2628520185884, 744.0285710985203, 737.2696992191727, 737.7691199837617, 741.2098043987709, 735.0694596296145, 743.6735459340413, 736.8777750552391, 739.2708035665571, 736.6724054860056, 741.0089835281732, 732.8036850673011, 622.2989690984879, 736.4696850595016, 736.4050240767226, 736.2235332927081, 734.6485771936906, 608.8826207944886, 607.0826221292775, 614.9242538524883, 648.0861858243611, 487.4726011385142, 607.6745931265597, 612.2877276729666, 612.9601717216096, 610.0057174801275, 605.0230862215535, 586.8735783872404, 520.2242322411178, 621.6589564546023, 615.0874733763414, 614.433668737463, 649.3281304163798, 612.9554336007683, 616.9940840574801, 646.1597970887736, 602.1609943203658, 652.3978463307525, 655.5089501722223, 657.0131155386302, 617.2889278207599, 624.8772089460995, 609.9207057166831, 614.8801315691932, 608.5312879023825, 604.8539057147943, 621.7386951202946, 652.9228096401223, 658.9285599263018, 610.7789699277823, 637.6152252895452, 656.2760142649494, 617.6767406874228, 621.2784780187105, 615.0034871961205, 656.3201495281799, 617.8386669380533, 621.9464330691899, 620.7090474712327, 659.2611016669101, 616.4723236325688, 624.919063609786, 619.4060764329755, 660.1010330786077, 612.7806081807732, 653.6861930368834, 655.0772628481868, 608.4856230988944, 653.1292953030069, 662.2697631249092, 626.0692201774964, 629.4204502332552, 631.2634621179093, 664.7734910970404, 633.4830737944109, 622.0379651853706, 671.4739538516947, 672.0113334182948, 626.0373847724584, 653.7816441310913, 625.4036883066271, 668.5094236309625, 627.5520232757957, 630.0848893544187, 626.1220842696767, 629.0592131171384, 629.6056455900114, 669.7832919524853, 668.6943304799828, 628.4815884594952, 618.402582735424, 632.1637867905744, 629.9618979003066, 634.4207186301029, 499.84987645950747]
Elapsed: 0.07527458844620974~0.006550229576460979
Time per graph: 0.0015362160907389742~0.00013367815462165264
Speed: 655.7672162002265~55.911469504321396
Total Time: 0.0985
best val loss: 0.404265820980072 test_score: 0.9796

Testing...
Test loss: 0.3347 score: 0.9388 time: 0.07s
test Score 0.9388
Epoch Time List: [0.789603787008673, 0.3515524949179962, 0.34818410605657846, 0.3314679190516472, 0.33057265810202807, 0.33327458309940994, 0.33434025093447417, 0.33133895287755877, 0.33047797286417335, 0.3311267029494047, 0.33624900120776147, 0.33663346990942955, 0.3327974519925192, 0.32999489607755095, 0.3291991689475253, 0.3293298820499331, 0.33035780815407634, 0.3396348259411752, 0.3303027309011668, 0.3290999151067808, 0.3306786580942571, 0.3361299270763993, 0.3436849989229813, 0.3421441729879007, 0.3291229928145185, 0.3298890038859099, 0.34659577999264, 0.3302276679314673, 0.3295294909039512, 0.33133498299866915, 0.3296031770296395, 0.3563382610445842, 0.3391808259766549, 0.3371609520399943, 0.3323194410186261, 0.36908388207666576, 0.34497072896920145, 0.33759721007663757, 0.3377418080344796, 0.338600437855348, 0.34266115992795676, 0.34566632006317377, 0.3878183331107721, 0.34254840307403356, 0.3358580599306151, 0.3383425339125097, 0.3311860089888796, 0.34176251594908535, 0.3362549878656864, 0.3418678909074515, 0.35657673608511686, 0.3327022729208693, 0.3397766731213778, 0.3467378600034863, 0.33615051209926605, 0.33545169106218964, 0.3434386261506006, 0.33592991798650473, 0.33723837102297693, 0.34085749997757375, 0.3349334769882262, 0.33525832812301815, 0.3353286801138893, 0.34030311403330415, 0.33893420605454594, 0.33109931589569896, 0.3398703739512712, 0.334552654880099, 0.3423847451340407, 0.33110926405061036, 0.3413967191008851, 0.33643833617679775, 0.3411669759079814, 0.336107800132595, 0.3603431601077318, 0.34874223603401333, 0.34264199808239937, 0.33300125889945775, 0.35047024697996676, 0.3422087370418012, 0.3458410050952807, 0.3556070738704875, 0.340593529981561, 0.34338614298030734, 0.34164755907841027, 0.342780367936939, 0.33852437406312674, 0.3323181309970096, 0.3424418348586187, 0.3382808150490746, 0.3337827470386401, 0.3375969239277765, 0.34295678103808314, 0.33938693802338094, 0.3525477219372988, 0.33449523604940623, 0.3450193669414148, 0.3345495059620589, 0.33756287791766226, 0.3380440748296678, 0.3354573469841853, 0.33141112490557134, 0.33785278210416436, 0.33366829704027623, 0.37844079197384417, 0.3354841179680079, 0.3399147759191692, 0.3363541770959273, 0.37060591590125114]
Total Epoch List: [31, 78]
Total Time List: [0.06704080104827881, 0.09849804395344108]
========================k_idx:2========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: GPSPerformer
GPS(
  (x_lin): Linear(in_features=14887, out_features=64, bias=True)
  (pe_lin): Linear(in_features=20, out_features=8, bias=True)
  (pe_norm): BatchNorm1d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (convs): ModuleList(
    (0-1): 2 x GPSConv(64, conv=GINEConv(nn=Sequential(
      (0): Linear(in_features=64, out_features=64, bias=True)
      (1): ReLU()
      (2): Linear(in_features=64, out_features=64, bias=True)
    )), heads=1, attn_type=performer)
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=32, bias=True)
    (1): ReLU()
    (2): Linear(in_features=32, out_features=16, bias=True)
    (3): ReLU()
    (4): Linear(in_features=16, out_features=2, bias=True)
  )
)
Total number of parameters: 1039522
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x747124497fa0>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.8313;  Loss pred: 0.8313; Loss self: 0.0000; time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7022 score: 0.5102 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6848 score: 0.5000 time: 0.06s
Epoch 2/1000, LR 0.000000
Train loss: 0.9078;  Loss pred: 0.9078; Loss self: 0.0000; time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7012 score: 0.5102 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6912 score: 0.5000 time: 0.06s
Epoch 3/1000, LR 0.000030
Train loss: 0.8199;  Loss pred: 0.8199; Loss self: 0.0000; time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7054 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7023 score: 0.5000 time: 0.07s
     INFO: Early stopping counter 1 of 20
Epoch 4/1000, LR 0.000060
Train loss: 0.8105;  Loss pred: 0.8105; Loss self: 0.0000; time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7044 score: 0.5102 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7066 score: 0.5000 time: 0.07s
     INFO: Early stopping counter 2 of 20
Epoch 5/1000, LR 0.000090
Train loss: 0.6945;  Loss pred: 0.6945; Loss self: 0.0000; time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6917 score: 0.5102 time: 0.10s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7067 score: 0.5000 time: 0.07s
Epoch 6/1000, LR 0.000120
Train loss: 0.6721;  Loss pred: 0.6721; Loss self: 0.0000; time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6824 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6988 score: 0.5000 time: 0.07s
Epoch 7/1000, LR 0.000150
Train loss: 0.6612;  Loss pred: 0.6612; Loss self: 0.0000; time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6787 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6931 score: 0.5000 time: 0.07s
Epoch 8/1000, LR 0.000180
Train loss: 0.6151;  Loss pred: 0.6151; Loss self: 0.0000; time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6767 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6874 score: 0.5000 time: 0.07s
Epoch 9/1000, LR 0.000210
Train loss: 0.5264;  Loss pred: 0.5264; Loss self: 0.0000; time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6732 score: 0.5102 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6823 score: 0.5000 time: 0.07s
Epoch 10/1000, LR 0.000240
Train loss: 0.5344;  Loss pred: 0.5344; Loss self: 0.0000; time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6698 score: 0.5102 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6747 score: 0.5000 time: 0.06s
Epoch 11/1000, LR 0.000270
Train loss: 0.5451;  Loss pred: 0.5451; Loss self: 0.0000; time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6672 score: 0.5102 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6713 score: 0.5000 time: 0.06s
Epoch 12/1000, LR 0.000270
Train loss: 0.4768;  Loss pred: 0.4768; Loss self: 0.0000; time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6629 score: 0.5102 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6671 score: 0.5000 time: 0.06s
Epoch 13/1000, LR 0.000270
Train loss: 0.4685;  Loss pred: 0.4685; Loss self: 0.0000; time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6596 score: 0.5102 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6617 score: 0.5000 time: 0.06s
Epoch 14/1000, LR 0.000270
Train loss: 0.4380;  Loss pred: 0.4380; Loss self: 0.0000; time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6574 score: 0.5102 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6566 score: 0.5000 time: 0.06s
Epoch 15/1000, LR 0.000270
Train loss: 0.4236;  Loss pred: 0.4236; Loss self: 0.0000; time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6550 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6517 score: 0.5000 time: 0.06s
Epoch 16/1000, LR 0.000270
Train loss: 0.4052;  Loss pred: 0.4052; Loss self: 0.0000; time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6515 score: 0.5102 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6467 score: 0.5000 time: 0.07s
Epoch 17/1000, LR 0.000270
Train loss: 0.3491;  Loss pred: 0.3491; Loss self: 0.0000; time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6476 score: 0.5102 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6412 score: 0.5000 time: 0.06s
Epoch 18/1000, LR 0.000270
Train loss: 0.3474;  Loss pred: 0.3474; Loss self: 0.0000; time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6419 score: 0.5102 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6340 score: 0.5000 time: 0.06s
Epoch 19/1000, LR 0.000270
Train loss: 0.3113;  Loss pred: 0.3113; Loss self: 0.0000; time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6354 score: 0.5102 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6264 score: 0.5000 time: 0.07s
Epoch 20/1000, LR 0.000270
Train loss: 0.2764;  Loss pred: 0.2764; Loss self: 0.0000; time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6276 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6178 score: 0.5000 time: 0.07s
Epoch 21/1000, LR 0.000270
Train loss: 0.2178;  Loss pred: 0.2178; Loss self: 0.0000; time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6191 score: 0.5102 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6131 score: 0.5000 time: 0.06s
Epoch 22/1000, LR 0.000270
Train loss: 0.2155;  Loss pred: 0.2155; Loss self: 0.0000; time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6112 score: 0.5102 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6088 score: 0.5000 time: 0.07s
Epoch 23/1000, LR 0.000270
Train loss: 0.1789;  Loss pred: 0.1789; Loss self: 0.0000; time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6050 score: 0.5102 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6047 score: 0.5000 time: 0.07s
Epoch 24/1000, LR 0.000270
Train loss: 0.1768;  Loss pred: 0.1768; Loss self: 0.0000; time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6015 score: 0.5102 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6014 score: 0.5000 time: 0.06s
Epoch 25/1000, LR 0.000270
Train loss: 0.1634;  Loss pred: 0.1634; Loss self: 0.0000; time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.5986 score: 0.5102 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.5989 score: 0.5000 time: 0.06s
Epoch 26/1000, LR 0.000270
Train loss: 0.1423;  Loss pred: 0.1423; Loss self: 0.0000; time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.5950 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.5959 score: 0.5000 time: 0.07s
Epoch 27/1000, LR 0.000270
Train loss: 0.1307;  Loss pred: 0.1307; Loss self: 0.0000; time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.5907 score: 0.5102 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.5929 score: 0.5000 time: 0.06s
Epoch 28/1000, LR 0.000270
Train loss: 0.1203;  Loss pred: 0.1203; Loss self: 0.0000; time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.5865 score: 0.5102 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.5896 score: 0.5000 time: 0.06s
Epoch 29/1000, LR 0.000270
Train loss: 0.1013;  Loss pred: 0.1013; Loss self: 0.0000; time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.5824 score: 0.5102 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.5859 score: 0.5000 time: 0.07s
Epoch 30/1000, LR 0.000270
Train loss: 0.0995;  Loss pred: 0.0995; Loss self: 0.0000; time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.5769 score: 0.5102 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.5809 score: 0.5000 time: 0.06s
Epoch 31/1000, LR 0.000270
Train loss: 0.0922;  Loss pred: 0.0922; Loss self: 0.0000; time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.5709 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.5750 score: 0.5000 time: 0.06s
Epoch 32/1000, LR 0.000270
Train loss: 0.0621;  Loss pred: 0.0621; Loss self: 0.0000; time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.5648 score: 0.5102 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.5697 score: 0.5000 time: 0.06s
Epoch 33/1000, LR 0.000270
Train loss: 0.0626;  Loss pred: 0.0626; Loss self: 0.0000; time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.5576 score: 0.5102 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.5634 score: 0.5000 time: 0.06s
Epoch 34/1000, LR 0.000270
Train loss: 0.0594;  Loss pred: 0.0594; Loss self: 0.0000; time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.5489 score: 0.5102 time: 0.06s
Test loss: 0.5551 score: 0.5417 time: 0.07s
Epoch 35/1000, LR 0.000270
Train loss: 0.0643;  Loss pred: 0.0643; Loss self: 0.0000; time: 0.20s
Val loss: 0.5372 score: 0.5510 time: 0.06s
Test loss: 0.5438 score: 0.6250 time: 0.06s
Epoch 36/1000, LR 0.000270
Train loss: 0.0456;  Loss pred: 0.0456; Loss self: 0.0000; time: 0.20s
Val loss: 0.5249 score: 0.5714 time: 0.07s
Test loss: 0.5315 score: 0.6667 time: 0.06s
Epoch 37/1000, LR 0.000270
Train loss: 0.0323;  Loss pred: 0.0323; Loss self: 0.0000; time: 0.20s
Val loss: 0.5123 score: 0.6939 time: 0.06s
Test loss: 0.5204 score: 0.7083 time: 0.06s
Epoch 38/1000, LR 0.000270
Train loss: 0.0315;  Loss pred: 0.0315; Loss self: 0.0000; time: 0.20s
Val loss: 0.4983 score: 0.7551 time: 0.07s
Test loss: 0.5080 score: 0.7708 time: 0.07s
Epoch 39/1000, LR 0.000269
Train loss: 0.0285;  Loss pred: 0.0285; Loss self: 0.0000; time: 0.20s
Val loss: 0.4857 score: 0.8367 time: 0.06s
Test loss: 0.4966 score: 0.7708 time: 0.07s
Epoch 40/1000, LR 0.000269
Train loss: 0.0234;  Loss pred: 0.0234; Loss self: 0.0000; time: 0.19s
Val loss: 0.4745 score: 0.8571 time: 0.06s
Test loss: 0.4855 score: 0.7292 time: 0.06s
Epoch 41/1000, LR 0.000269
Train loss: 0.0206;  Loss pred: 0.0206; Loss self: 0.0000; time: 0.19s
Val loss: 0.4628 score: 0.8776 time: 0.06s
Test loss: 0.4709 score: 0.7500 time: 0.06s
Epoch 42/1000, LR 0.000269
Train loss: 0.0174;  Loss pred: 0.0174; Loss self: 0.0000; time: 0.20s
Val loss: 0.4528 score: 0.8776 time: 0.06s
Test loss: 0.4560 score: 0.7708 time: 0.06s
Epoch 43/1000, LR 0.000269
Train loss: 0.0172;  Loss pred: 0.0172; Loss self: 0.0000; time: 0.20s
Val loss: 0.4446 score: 0.8776 time: 0.07s
Test loss: 0.4428 score: 0.7917 time: 0.07s
Epoch 44/1000, LR 0.000269
Train loss: 0.0163;  Loss pred: 0.0163; Loss self: 0.0000; time: 0.20s
Val loss: 0.4365 score: 0.8776 time: 0.06s
Test loss: 0.4302 score: 0.8333 time: 0.06s
Epoch 45/1000, LR 0.000269
Train loss: 0.0139;  Loss pred: 0.0139; Loss self: 0.0000; time: 0.20s
Val loss: 0.4290 score: 0.8980 time: 0.06s
Test loss: 0.4183 score: 0.8750 time: 0.07s
Epoch 46/1000, LR 0.000269
Train loss: 0.0129;  Loss pred: 0.0129; Loss self: 0.0000; time: 0.20s
Val loss: 0.4210 score: 0.8980 time: 0.06s
Test loss: 0.4059 score: 0.8750 time: 0.06s
Epoch 47/1000, LR 0.000269
Train loss: 0.0114;  Loss pred: 0.0114; Loss self: 0.0000; time: 0.20s
Val loss: 0.4130 score: 0.8980 time: 0.07s
Test loss: 0.3939 score: 0.8750 time: 0.07s
Epoch 48/1000, LR 0.000269
Train loss: 0.0112;  Loss pred: 0.0112; Loss self: 0.0000; time: 0.20s
Val loss: 0.4047 score: 0.8776 time: 0.06s
Test loss: 0.3813 score: 0.8750 time: 0.07s
Epoch 49/1000, LR 0.000269
Train loss: 0.0099;  Loss pred: 0.0099; Loss self: 0.0000; time: 0.20s
Val loss: 0.3952 score: 0.8980 time: 0.06s
Test loss: 0.3675 score: 0.8750 time: 0.06s
Epoch 50/1000, LR 0.000269
Train loss: 0.0097;  Loss pred: 0.0097; Loss self: 0.0000; time: 0.20s
Val loss: 0.3875 score: 0.8776 time: 0.07s
Test loss: 0.3567 score: 0.8958 time: 0.07s
Epoch 51/1000, LR 0.000269
Train loss: 0.0096;  Loss pred: 0.0096; Loss self: 0.0000; time: 0.20s
Val loss: 0.3793 score: 0.8776 time: 0.06s
Test loss: 0.3465 score: 0.9375 time: 0.07s
Epoch 52/1000, LR 0.000269
Train loss: 0.0090;  Loss pred: 0.0090; Loss self: 0.0000; time: 0.20s
Val loss: 0.3724 score: 0.8776 time: 0.07s
Test loss: 0.3379 score: 0.9375 time: 0.07s
Epoch 53/1000, LR 0.000269
Train loss: 0.0069;  Loss pred: 0.0069; Loss self: 0.0000; time: 0.20s
Val loss: 0.3662 score: 0.8776 time: 0.06s
Test loss: 0.3293 score: 0.9167 time: 0.07s
Epoch 54/1000, LR 0.000269
Train loss: 0.0062;  Loss pred: 0.0062; Loss self: 0.0000; time: 0.20s
Val loss: 0.3605 score: 0.8980 time: 0.06s
Test loss: 0.3221 score: 0.9375 time: 0.07s
Epoch 55/1000, LR 0.000269
Train loss: 0.0063;  Loss pred: 0.0063; Loss self: 0.0000; time: 0.21s
Val loss: 0.3553 score: 0.8980 time: 0.06s
Test loss: 0.3159 score: 0.9375 time: 0.07s
Epoch 56/1000, LR 0.000269
Train loss: 0.0070;  Loss pred: 0.0070; Loss self: 0.0000; time: 0.20s
Val loss: 0.3514 score: 0.8980 time: 0.07s
Test loss: 0.3122 score: 0.9167 time: 0.07s
Epoch 57/1000, LR 0.000269
Train loss: 0.0066;  Loss pred: 0.0066; Loss self: 0.0000; time: 0.20s
Val loss: 0.3470 score: 0.8980 time: 0.06s
Test loss: 0.3074 score: 0.8958 time: 0.07s
Epoch 58/1000, LR 0.000269
Train loss: 0.0063;  Loss pred: 0.0063; Loss self: 0.0000; time: 0.21s
Val loss: 0.3429 score: 0.8980 time: 0.06s
Test loss: 0.3028 score: 0.8958 time: 0.07s
Epoch 59/1000, LR 0.000268
Train loss: 0.0059;  Loss pred: 0.0059; Loss self: 0.0000; time: 0.20s
Val loss: 0.3381 score: 0.8980 time: 0.06s
Test loss: 0.2972 score: 0.8958 time: 0.07s
Epoch 60/1000, LR 0.000268
Train loss: 0.0054;  Loss pred: 0.0054; Loss self: 0.0000; time: 0.20s
Val loss: 0.3330 score: 0.8980 time: 0.06s
Test loss: 0.2906 score: 0.8958 time: 0.07s
Epoch 61/1000, LR 0.000268
Train loss: 0.0047;  Loss pred: 0.0047; Loss self: 0.0000; time: 0.20s
Val loss: 0.3258 score: 0.8980 time: 0.06s
Test loss: 0.2817 score: 0.8958 time: 0.07s
Epoch 62/1000, LR 0.000268
Train loss: 0.0054;  Loss pred: 0.0054; Loss self: 0.0000; time: 0.20s
Val loss: 0.3176 score: 0.8776 time: 0.06s
Test loss: 0.2710 score: 0.8958 time: 0.06s
Epoch 63/1000, LR 0.000268
Train loss: 0.0063;  Loss pred: 0.0063; Loss self: 0.0000; time: 0.20s
Val loss: 0.3100 score: 0.8776 time: 0.06s
Test loss: 0.2605 score: 0.9375 time: 0.06s
Epoch 64/1000, LR 0.000268
Train loss: 0.0052;  Loss pred: 0.0052; Loss self: 0.0000; time: 0.20s
Val loss: 0.3026 score: 0.9184 time: 0.06s
Test loss: 0.2494 score: 0.9375 time: 0.06s
Epoch 65/1000, LR 0.000268
Train loss: 0.0038;  Loss pred: 0.0038; Loss self: 0.0000; time: 0.20s
Val loss: 0.2964 score: 0.9184 time: 0.06s
Test loss: 0.2388 score: 0.9375 time: 0.07s
Epoch 66/1000, LR 0.000268
Train loss: 0.0039;  Loss pred: 0.0039; Loss self: 0.0000; time: 0.19s
Val loss: 0.2909 score: 0.9184 time: 0.06s
Test loss: 0.2282 score: 0.9583 time: 0.06s
Epoch 67/1000, LR 0.000268
Train loss: 0.0040;  Loss pred: 0.0040; Loss self: 0.0000; time: 0.20s
Val loss: 0.2874 score: 0.9184 time: 0.06s
Test loss: 0.2195 score: 0.9375 time: 0.06s
Epoch 68/1000, LR 0.000268
Train loss: 0.0044;  Loss pred: 0.0044; Loss self: 0.0000; time: 0.20s
Val loss: 0.2860 score: 0.9184 time: 0.06s
Test loss: 0.2130 score: 0.9375 time: 0.07s
Epoch 69/1000, LR 0.000268
Train loss: 0.0038;  Loss pred: 0.0038; Loss self: 0.0000; time: 0.21s
Val loss: 0.2866 score: 0.8980 time: 0.06s
Test loss: 0.2071 score: 0.9375 time: 0.06s
     INFO: Early stopping counter 1 of 20
Epoch 70/1000, LR 0.000268
Train loss: 0.0035;  Loss pred: 0.0035; Loss self: 0.0000; time: 0.20s
Val loss: 0.2895 score: 0.8980 time: 0.06s
Test loss: 0.2047 score: 0.9375 time: 0.06s
     INFO: Early stopping counter 2 of 20
Epoch 71/1000, LR 0.000268
Train loss: 0.0032;  Loss pred: 0.0032; Loss self: 0.0000; time: 0.20s
Val loss: 0.2932 score: 0.8980 time: 0.06s
Test loss: 0.2034 score: 0.9375 time: 0.07s
     INFO: Early stopping counter 3 of 20
Epoch 72/1000, LR 0.000267
Train loss: 0.0030;  Loss pred: 0.0030; Loss self: 0.0000; time: 0.20s
Val loss: 0.2984 score: 0.8980 time: 0.06s
Test loss: 0.2039 score: 0.9167 time: 0.07s
     INFO: Early stopping counter 4 of 20
Epoch 73/1000, LR 0.000267
Train loss: 0.0033;  Loss pred: 0.0033; Loss self: 0.0000; time: 0.20s
Val loss: 0.3059 score: 0.8980 time: 0.06s
Test loss: 0.2058 score: 0.9167 time: 0.07s
     INFO: Early stopping counter 5 of 20
Epoch 74/1000, LR 0.000267
Train loss: 0.0029;  Loss pred: 0.0029; Loss self: 0.0000; time: 0.20s
Val loss: 0.3120 score: 0.8776 time: 0.06s
Test loss: 0.2072 score: 0.9167 time: 0.06s
     INFO: Early stopping counter 6 of 20
Epoch 75/1000, LR 0.000267
Train loss: 0.0027;  Loss pred: 0.0027; Loss self: 0.0000; time: 0.20s
Val loss: 0.3196 score: 0.8776 time: 0.06s
Test loss: 0.2103 score: 0.9167 time: 0.07s
     INFO: Early stopping counter 7 of 20
Epoch 76/1000, LR 0.000267
Train loss: 0.0027;  Loss pred: 0.0027; Loss self: 0.0000; time: 0.19s
Val loss: 0.3295 score: 0.8776 time: 0.06s
Test loss: 0.2158 score: 0.9167 time: 0.06s
     INFO: Early stopping counter 8 of 20
Epoch 77/1000, LR 0.000267
Train loss: 0.0028;  Loss pred: 0.0028; Loss self: 0.0000; time: 0.19s
Val loss: 0.3405 score: 0.8776 time: 0.06s
Test loss: 0.2213 score: 0.9167 time: 0.06s
     INFO: Early stopping counter 9 of 20
Epoch 78/1000, LR 0.000267
Train loss: 0.0026;  Loss pred: 0.0026; Loss self: 0.0000; time: 0.20s
Val loss: 0.3484 score: 0.8776 time: 0.07s
Test loss: 0.2249 score: 0.9167 time: 0.07s
     INFO: Early stopping counter 10 of 20
Epoch 79/1000, LR 0.000267
Train loss: 0.0026;  Loss pred: 0.0026; Loss self: 0.0000; time: 0.20s
Val loss: 0.3591 score: 0.8776 time: 0.07s
Test loss: 0.2303 score: 0.9167 time: 0.07s
     INFO: Early stopping counter 11 of 20
Epoch 80/1000, LR 0.000267
Train loss: 0.0020;  Loss pred: 0.0020; Loss self: 0.0000; time: 0.20s
Val loss: 0.3649 score: 0.8776 time: 0.06s
Test loss: 0.2323 score: 0.9167 time: 0.06s
     INFO: Early stopping counter 12 of 20
Epoch 81/1000, LR 0.000267
Train loss: 0.0026;  Loss pred: 0.0026; Loss self: 0.0000; time: 0.20s
Val loss: 0.3733 score: 0.8776 time: 0.06s
Test loss: 0.2363 score: 0.9167 time: 0.07s
     INFO: Early stopping counter 13 of 20
Epoch 82/1000, LR 0.000267
Train loss: 0.0024;  Loss pred: 0.0024; Loss self: 0.0000; time: 0.20s
Val loss: 0.3769 score: 0.8776 time: 0.06s
Test loss: 0.2376 score: 0.9167 time: 0.06s
     INFO: Early stopping counter 14 of 20
Epoch 83/1000, LR 0.000266
Train loss: 0.0020;  Loss pred: 0.0020; Loss self: 0.0000; time: 0.20s
Val loss: 0.3830 score: 0.8776 time: 0.07s
Test loss: 0.2404 score: 0.9167 time: 0.07s
     INFO: Early stopping counter 15 of 20
Epoch 84/1000, LR 0.000266
Train loss: 0.0019;  Loss pred: 0.0019; Loss self: 0.0000; time: 0.20s
Val loss: 0.3878 score: 0.8776 time: 0.06s
Test loss: 0.2422 score: 0.9167 time: 0.07s
     INFO: Early stopping counter 16 of 20
Epoch 85/1000, LR 0.000266
Train loss: 0.0021;  Loss pred: 0.0021; Loss self: 0.0000; time: 0.20s
Val loss: 0.3938 score: 0.8776 time: 0.06s
Test loss: 0.2453 score: 0.9167 time: 0.06s
     INFO: Early stopping counter 17 of 20
Epoch 86/1000, LR 0.000266
Train loss: 0.0019;  Loss pred: 0.0019; Loss self: 0.0000; time: 0.19s
Val loss: 0.3992 score: 0.8776 time: 0.06s
Test loss: 0.2479 score: 0.9167 time: 0.06s
     INFO: Early stopping counter 18 of 20
Epoch 87/1000, LR 0.000266
Train loss: 0.0021;  Loss pred: 0.0021; Loss self: 0.0000; time: 0.20s
Val loss: 0.4045 score: 0.8776 time: 0.06s
Test loss: 0.2510 score: 0.9167 time: 0.06s
     INFO: Early stopping counter 19 of 20
Epoch 88/1000, LR 0.000266
Train loss: 0.0020;  Loss pred: 0.0020; Loss self: 0.0000; time: 0.19s
Val loss: 0.4127 score: 0.8776 time: 0.06s
Test loss: 0.2569 score: 0.9167 time: 0.07s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 067,   Train_Loss: 0.0044,   Val_Loss: 0.2860,   Val_Precision: 1.0000,   Val_Recall: 0.8400,   Val_accuracy: 0.9130,   Val_Score: 0.9184,   Val_Loss: 0.2860,   Test_Precision: 1.0000,   Test_Recall: 0.8750,   Test_accuracy: 0.9333,   Test_Score: 0.9375,   Test_loss: 0.2130


[0.07183226896449924, 0.07066955091431737, 0.06666709599085152, 0.06680618994869292, 0.06617387395817786, 0.06620889401528984, 0.06614625896327198, 0.06642150203697383, 0.06610952003393322, 0.06619310798123479, 0.06661582796368748, 0.06953462411183864, 0.0659220510860905, 0.06524433102458715, 0.06628224300220609, 0.06585768598597497, 0.06646143202669919, 0.06641644204501063, 0.06610813795123249, 0.0666603670688346, 0.06588912603911012, 0.06649678095709532, 0.06628153007477522, 0.06651531893294305, 0.06612605392001569, 0.06686647597234696, 0.07874028792139143, 0.06653362791985273, 0.06653946998994797, 0.06655587302520871, 0.06669855699874461, 0.08047528099268675, 0.08071389002725482, 0.07968461106065661, 0.07560722797643393, 0.10051846993155777, 0.08063526195473969, 0.08002773497719318, 0.07993994106072932, 0.0803271159529686, 0.08098864508792758, 0.08349327999167144, 0.09419015294406563, 0.07882135291583836, 0.07966346596367657, 0.0797482340130955, 0.07546261698007584, 0.07994055899325758, 0.07941729307640344, 0.07583263493143022, 0.08137358690146357, 0.07510754407849163, 0.07475107698701322, 0.07457994192373008, 0.07937935995869339, 0.07841540593653917, 0.08033831208012998, 0.07969032903201878, 0.08052174304611981, 0.08101129799615592, 0.07881124399136752, 0.07504715607501566, 0.07436314492952079, 0.08022542099934071, 0.0768488550093025, 0.07466370693873614, 0.07932952104602009, 0.07886962406337261, 0.0796743449755013, 0.07465868606232107, 0.07930872996803373, 0.07878492004238069, 0.0789419780485332, 0.07432563498150557, 0.0794845090713352, 0.07841015397571027, 0.07910803891718388, 0.0742310609202832, 0.0799633659189567, 0.07495951501186937, 0.07480033696629107, 0.0805277859326452, 0.07502343005035073, 0.07398797699715942, 0.07826610608026385, 0.07784939301200211, 0.07762210699729621, 0.07370931701734662, 0.07735013298224658, 0.07877332693897188, 0.07297379104420543, 0.0729154369328171, 0.07827008608728647, 0.07494857104029506, 0.07834939402528107, 0.07329739606939256, 0.07808117603417486, 0.07776729902252555, 0.07825949799735099, 0.07789409800898284, 0.07782649400178343, 0.07315799093339592, 0.07327712792903185, 0.07796568889170885, 0.07923640904482454, 0.07751155795995146, 0.07778248202521354, 0.0772358130197972, 0.09802943305112422, 0.06917367305140942, 0.06910175096709281, 0.07086958107538521, 0.07432115101255476, 0.07341046002693474, 0.07351805793587118, 0.07413568103220314, 0.07519439200405031, 0.07029274001251906, 0.06891421100590378, 0.068873131996952, 0.06887890305370092, 0.06880629702936858, 0.06913431908469647, 0.06941563496366143, 0.06968455493915826, 0.06934030598495156, 0.06918102805502713, 0.07003356202039868, 0.06968814402353019, 0.06863353902008384, 0.07134064205456525, 0.07277001603506505, 0.069030303042382, 0.06943134195171297, 0.0699446639046073, 0.06934771104715765, 0.06935755896847695, 0.0698049240745604, 0.06918680795934051, 0.06917779101058841, 0.06918403890449554, 0.06919081893283874, 0.0740559280384332, 0.06887980597093701, 0.06937580497469753, 0.06942625506781042, 0.07471378601621836, 0.06973465404007584, 0.0692843480501324, 0.06923292099963874, 0.0691674220142886, 0.06986987998243421, 0.06939074106048793, 0.06953533506020904, 0.06894692697096616, 0.06956069509033114, 0.06999943195842206, 0.06945676205214113, 0.06993076205253601, 0.07008121500257403, 0.071148183895275, 0.07497517101000994, 0.06944979599211365, 0.07051959598902613, 0.06959952705074102, 0.07376151694916189, 0.0697291640099138, 0.06948882003780454, 0.06948199507314712, 0.06948188901878893, 0.06932119897101074, 0.06895556801464409, 0.06906958296895027, 0.07047289097681642, 0.06908792303875089, 0.06940823106560856, 0.0748616240452975, 0.0693068120162934, 0.06942231592256576, 0.07250318001024425, 0.06968953495379537, 0.0699630540329963, 0.07325488305650651, 0.06947820296045393, 0.06912160804495215, 0.06906637700740248, 0.06983134092297405, 0.06986808602232486, 0.06931632105261087, 0.07346310594584793, 0.06909538398031145, 0.0697135889204219, 0.06957947404589504, 0.06900477199815214, 0.06903000199235976, 0.06917885493021458, 0.07010921195615083]
[0.0014659646727448823, 0.0014422357329452525, 0.001360552979405133, 0.0013633916316059778, 0.001350487223636283, 0.0013512019186793845, 0.001349923652311673, 0.001355540857897425, 0.0013491738782435351, 0.0013508797547190773, 0.0013595066931364791, 0.0014190739614660948, 0.0013453479813487859, 0.0013315169596854522, 0.001352698836779716, 0.0013440344078770401, 0.001356355755646922, 0.001355437592755319, 0.0013491456724741325, 0.0013604156544660122, 0.0013446760416144924, 0.0013570771623897006, 0.0013526842872403106, 0.001357455488427409, 0.0013495113044901161, 0.0013646219586193257, 0.001606944651456968, 0.0013578291412214844, 0.0013579483671417954, 0.001358283122963443, 0.0013611950407907063, 0.0016423526733201376, 0.00164722224545418, 0.0016262165522582981, 0.0015430046525802842, 0.0020513973455419956, 0.0016456175909130549, 0.0016332190811672077, 0.0016314273685863127, 0.0016393288969993591, 0.0016528294915903587, 0.0017039444896259478, 0.0019222480192666455, 0.001608599039098742, 0.0016257850196668689, 0.0016275149798590917, 0.0015400534077566496, 0.0016314399794542364, 0.001620761083191907, 0.0015476047945189842, 0.0016606854469686442, 0.0015328070220100333, 0.001525532183408433, 0.0015220396310965322, 0.001619986937932518, 0.0016003144068681464, 0.0016395573893904078, 0.0016263332455514036, 0.001643300878492241, 0.0016532917958399166, 0.0016083927345177044, 0.0015315746137758298, 0.0015176152026432814, 0.0016372534897824635, 0.0015683439797816836, 0.0015237491211986967, 0.001618969817265716, 0.0016095841645586248, 0.001626007040316353, 0.001523646654333083, 0.0016185455095517089, 0.0016078555110689936, 0.0016110607765006777, 0.0015168496935001137, 0.0016221328381905143, 0.0016002072239940871, 0.0016144497738200792, 0.0015149196106180244, 0.0016319054269174837, 0.0015297860206503952, 0.001526537489107981, 0.0016434242027070448, 0.001531090409190831, 0.0015099587142277434, 0.001597267471025793, 0.0015887631226939205, 0.001584124632597882, 0.0015042717758642168, 0.0015785741424948281, 0.001607618917121875, 0.001489261041718478, 0.0014880701414860633, 0.0015973486956589076, 0.001529562674291736, 0.0015989672250057362, 0.0014958652259059707, 0.0015934933884525482, 0.0015870877351535826, 0.0015971326121908365, 0.0015896754695710782, 0.0015882957959547639, 0.001493020223130529, 0.0014954515903884051, 0.0015911365079940582, 0.0016170695723433579, 0.0015818685297949277, 0.0015873975923512966, 0.0015762410820366777, 0.002000600674512739, 0.0014411181885710296, 0.0014396198118144337, 0.0014764496057371919, 0.0015483573127615575, 0.0015293845838944737, 0.0015316262069973163, 0.0015444933548375654, 0.0015665498334177148, 0.0014644320835941471, 0.001435712729289662, 0.0014348569166031666, 0.0014349771469521027, 0.0014334645214451787, 0.0014402983142645098, 0.0014461590617429465, 0.0014517615612324637, 0.001444589708019824, 0.0014412714178130652, 0.001459032542091639, 0.0014518363338235456, 0.0014298653962517467, 0.001486263376136776, 0.001516042000730522, 0.0014381313133829583, 0.0014464862906606868, 0.001457180498012652, 0.0014447439801491175, 0.0014449491451766032, 0.0014542692515533417, 0.0014413918324862607, 0.0014412039793872584, 0.001441334143843657, 0.0014414753944341403, 0.0015428318341340248, 0.0014349959577278544, 0.0014453292703061986, 0.001446380313912717, 0.001556537208671216, 0.00145280529250158, 0.0014434239177110915, 0.001442352520825807, 0.0014409879586310126, 0.001455622499634046, 0.0014456404387601651, 0.0014486528137543548, 0.0014363943118951283, 0.001449181147715232, 0.001458321499133793, 0.0014470158760862735, 0.0014568908760945003, 0.0014600253125536256, 0.0014822538311515625, 0.001561982729375207, 0.001446870749835701, 0.0014691582497713778, 0.001449990146890438, 0.001536698269774206, 0.0014526909168732043, 0.0014476837507875946, 0.0014475415640238982, 0.0014475393545581028, 0.0014441916452293906, 0.0014365743336384185, 0.001438949645186464, 0.0014681852286836754, 0.0014393317299739767, 0.001446004813866845, 0.0015596171676103647, 0.0014438919170061126, 0.0014462982483867866, 0.0015104829168800886, 0.0014518653115374036, 0.001457563625687423, 0.0015261433970105525, 0.0014474625616761234, 0.0014400335009365033, 0.001438882854320885, 0.0014548196025619593, 0.0014555851254651013, 0.0014440900219293933, 0.001530481373871832, 0.0014394871662564885, 0.0014523664358421229, 0.0014495723759561467, 0.0014375994166281696, 0.0014381250415074949, 0.0014412261443794705, 0.001460608582419809]
[682.1446782394784, 693.3679267243341, 734.9952667313436, 733.4649684053521, 740.4734991179175, 740.0818383808717, 740.782634845721, 737.7129167106749, 741.194308699396, 740.2583364704843, 735.5609244504185, 704.6849051946984, 743.3021150389987, 751.023103931198, 739.2628520185884, 744.0285710985203, 737.2696992191727, 737.7691199837617, 741.2098043987709, 735.0694596296145, 743.6735459340413, 736.8777750552391, 739.2708035665571, 736.6724054860056, 741.0089835281732, 732.8036850673011, 622.2989690984879, 736.4696850595016, 736.4050240767226, 736.2235332927081, 734.6485771936906, 608.8826207944886, 607.0826221292775, 614.9242538524883, 648.0861858243611, 487.4726011385142, 607.6745931265597, 612.2877276729666, 612.9601717216096, 610.0057174801275, 605.0230862215535, 586.8735783872404, 520.2242322411178, 621.6589564546023, 615.0874733763414, 614.433668737463, 649.3281304163798, 612.9554336007683, 616.9940840574801, 646.1597970887736, 602.1609943203658, 652.3978463307525, 655.5089501722223, 657.0131155386302, 617.2889278207599, 624.8772089460995, 609.9207057166831, 614.8801315691932, 608.5312879023825, 604.8539057147943, 621.7386951202946, 652.9228096401223, 658.9285599263018, 610.7789699277823, 637.6152252895452, 656.2760142649494, 617.6767406874228, 621.2784780187105, 615.0034871961205, 656.3201495281799, 617.8386669380533, 621.9464330691899, 620.7090474712327, 659.2611016669101, 616.4723236325688, 624.919063609786, 619.4060764329755, 660.1010330786077, 612.7806081807732, 653.6861930368834, 655.0772628481868, 608.4856230988944, 653.1292953030069, 662.2697631249092, 626.0692201774964, 629.4204502332552, 631.2634621179093, 664.7734910970404, 633.4830737944109, 622.0379651853706, 671.4739538516947, 672.0113334182948, 626.0373847724584, 653.7816441310913, 625.4036883066271, 668.5094236309625, 627.5520232757957, 630.0848893544187, 626.1220842696767, 629.0592131171384, 629.6056455900114, 669.7832919524853, 668.6943304799828, 628.4815884594952, 618.402582735424, 632.1637867905744, 629.9618979003066, 634.4207186301029, 499.84987645950747, 693.9056129681984, 694.6278397903152, 677.3004619420786, 645.8457565046532, 653.8577742516327, 652.9008157678724, 647.4615101889967, 638.3454765804144, 682.8585710480378, 696.5181680145465, 696.9336025276773, 696.8752095627475, 697.6105686883886, 694.3006112665287, 691.4868678378818, 688.8183477947001, 692.2380759383598, 693.8318401660702, 685.3856724582856, 688.7828722170132, 699.366529619783, 672.8282591469664, 659.6123323220193, 695.3467953129194, 691.3304373892456, 686.256782439671, 692.1641576224364, 692.0658788152568, 687.6305738651042, 693.7738770692888, 693.8643067202461, 693.8016450045819, 693.7336591808811, 648.1587804164602, 696.866074510329, 691.8838637981407, 691.381091391393, 642.451715531863, 688.3234836501068, 692.7971663277891, 693.3117844363445, 693.9683250025447, 686.991304580279, 691.7349385007776, 690.2965227454201, 696.1876635954058, 690.0448584889422, 685.719850248368, 691.0774211439117, 686.3932065253291, 684.9196321473165, 674.6482815450714, 640.2119442127249, 691.1467386520563, 680.661868900518, 689.6598588235514, 650.7458358412381, 688.3776778562204, 690.7585993529058, 690.8264500676476, 690.8275045173295, 692.4288776377483, 696.100422083482, 694.9513510394011, 681.1129688973685, 694.7668693568507, 691.5606299579614, 641.1829907798422, 692.572614488683, 691.4203215798736, 662.0399269827599, 688.7691248309279, 686.0763965129655, 655.2464217706048, 690.8641552994824, 694.4282888902693, 694.9836096781999, 687.3704466443708, 687.0089440358023, 692.4776051453762, 653.3891996804815, 694.6918482090999, 688.5314720318305, 689.8586207814521, 695.6040663576916, 695.3498278228739, 693.8536356003705, 684.6461208267633]
Elapsed: 0.0730088798939085~0.005604730445928599
Time per graph: 0.001503310256473078~0.00010862453871046889
Speed: 668.4362774394923~45.20575264795975
Total Time: 0.0708
best val loss: 0.2859821617603302 test_score: 0.9375

Testing...
Test loss: 0.2494 score: 0.9375 time: 0.06s
test Score 0.9375
Epoch Time List: [0.789603787008673, 0.3515524949179962, 0.34818410605657846, 0.3314679190516472, 0.33057265810202807, 0.33327458309940994, 0.33434025093447417, 0.33133895287755877, 0.33047797286417335, 0.3311267029494047, 0.33624900120776147, 0.33663346990942955, 0.3327974519925192, 0.32999489607755095, 0.3291991689475253, 0.3293298820499331, 0.33035780815407634, 0.3396348259411752, 0.3303027309011668, 0.3290999151067808, 0.3306786580942571, 0.3361299270763993, 0.3436849989229813, 0.3421441729879007, 0.3291229928145185, 0.3298890038859099, 0.34659577999264, 0.3302276679314673, 0.3295294909039512, 0.33133498299866915, 0.3296031770296395, 0.3563382610445842, 0.3391808259766549, 0.3371609520399943, 0.3323194410186261, 0.36908388207666576, 0.34497072896920145, 0.33759721007663757, 0.3377418080344796, 0.338600437855348, 0.34266115992795676, 0.34566632006317377, 0.3878183331107721, 0.34254840307403356, 0.3358580599306151, 0.3383425339125097, 0.3311860089888796, 0.34176251594908535, 0.3362549878656864, 0.3418678909074515, 0.35657673608511686, 0.3327022729208693, 0.3397766731213778, 0.3467378600034863, 0.33615051209926605, 0.33545169106218964, 0.3434386261506006, 0.33592991798650473, 0.33723837102297693, 0.34085749997757375, 0.3349334769882262, 0.33525832812301815, 0.3353286801138893, 0.34030311403330415, 0.33893420605454594, 0.33109931589569896, 0.3398703739512712, 0.334552654880099, 0.3423847451340407, 0.33110926405061036, 0.3413967191008851, 0.33643833617679775, 0.3411669759079814, 0.336107800132595, 0.3603431601077318, 0.34874223603401333, 0.34264199808239937, 0.33300125889945775, 0.35047024697996676, 0.3422087370418012, 0.3458410050952807, 0.3556070738704875, 0.340593529981561, 0.34338614298030734, 0.34164755907841027, 0.342780367936939, 0.33852437406312674, 0.3323181309970096, 0.3424418348586187, 0.3382808150490746, 0.3337827470386401, 0.3375969239277765, 0.34295678103808314, 0.33938693802338094, 0.3525477219372988, 0.33449523604940623, 0.3450193669414148, 0.3345495059620589, 0.33756287791766226, 0.3380440748296678, 0.3354573469841853, 0.33141112490557134, 0.33785278210416436, 0.33366829704027623, 0.37844079197384417, 0.3354841179680079, 0.3399147759191692, 0.3363541770959273, 0.37060591590125114, 0.3254200801020488, 0.3240830509457737, 0.3259610540699214, 0.33054751495365053, 0.3808882258599624, 0.34707195591181517, 0.34850902389734983, 0.3421306669479236, 0.33044549298938364, 0.3240740509936586, 0.3276255258824676, 0.3218295470578596, 0.3269714688649401, 0.3244758590590209, 0.3298632090445608, 0.3229908699868247, 0.3260366090107709, 0.3268629969097674, 0.32367572898510844, 0.327996667008847, 0.32207691797520965, 0.3244666639948264, 0.3340261001139879, 0.33244035206735134, 0.33170018694363534, 0.33312361501157284, 0.32253296102862805, 0.3318673699395731, 0.32816296094097197, 0.3219010749598965, 0.33171278901863843, 0.3225882149999961, 0.32254253805149347, 0.3285947450203821, 0.323705454939045, 0.33019442798104137, 0.32931272801943123, 0.33682218904141337, 0.3246884241234511, 0.3232121499022469, 0.3227211949415505, 0.32354360504541546, 0.3297868329100311, 0.3305873260833323, 0.3240982659626752, 0.32449178001843393, 0.3300220249220729, 0.329254562035203, 0.32547074905596673, 0.33153686800505966, 0.3268139121355489, 0.32846776104997844, 0.3326382009545341, 0.33002291701268405, 0.33679427404422313, 0.3385170690016821, 0.33171327493619174, 0.33629675302654505, 0.32691813993733376, 0.3242494410369545, 0.32379189296625555, 0.3244655340677127, 0.32314053911250085, 0.32445442490279675, 0.3258593730861321, 0.3233114890754223, 0.32414888811763376, 0.3327456539263949, 0.34304452198557556, 0.3248537229374051, 0.3276294299867004, 0.3331514539895579, 0.33412026602309197, 0.3266599210910499, 0.3256681509083137, 0.3228309089317918, 0.323163376073353, 0.3296356569044292, 0.3325244940351695, 0.32552456308621913, 0.32566847396083176, 0.3314817510545254, 0.3314561560982838, 0.33168053394183517, 0.3250687699764967, 0.32282124215271324, 0.32402129215188324, 0.32437715597916394]
Total Epoch List: [31, 78, 88]
Total Time List: [0.06704080104827881, 0.09849804395344108, 0.07078160101082176]
========================training times:1========================
========================k_idx:0========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: GPSPerformer
GPS(
  (x_lin): Linear(in_features=14887, out_features=64, bias=True)
  (pe_lin): Linear(in_features=20, out_features=8, bias=True)
  (pe_norm): BatchNorm1d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (convs): ModuleList(
    (0-1): 2 x GPSConv(64, conv=GINEConv(nn=Sequential(
      (0): Linear(in_features=64, out_features=64, bias=True)
      (1): ReLU()
      (2): Linear(in_features=64, out_features=64, bias=True)
    )), heads=1, attn_type=performer)
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=32, bias=True)
    (1): ReLU()
    (2): Linear(in_features=32, out_features=16, bias=True)
    (3): ReLU()
    (4): Linear(in_features=16, out_features=2, bias=True)
  )
)
Total number of parameters: 1039522
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x747124497910>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.6518;  Loss pred: 0.6518; Loss self: 0.0000; time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7824 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.8191 score: 0.5102 time: 0.06s
Epoch 2/1000, LR 0.000000
Train loss: 0.5907;  Loss pred: 0.5907; Loss self: 0.0000; time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7063 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7416 score: 0.5102 time: 0.06s
Epoch 3/1000, LR 0.000030
Train loss: 0.6355;  Loss pred: 0.6355; Loss self: 0.0000; time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6552 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6896 score: 0.5102 time: 0.06s
Epoch 4/1000, LR 0.000060
Train loss: 0.6306;  Loss pred: 0.6306; Loss self: 0.0000; time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6215 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6569 score: 0.5102 time: 0.06s
Epoch 5/1000, LR 0.000090
Train loss: 0.6283;  Loss pred: 0.6283; Loss self: 0.0000; time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6047 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6439 score: 0.5102 time: 0.06s
Epoch 6/1000, LR 0.000120
Train loss: 0.6126;  Loss pred: 0.6126; Loss self: 0.0000; time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.5995 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6383 score: 0.5102 time: 0.06s
Epoch 7/1000, LR 0.000150
Train loss: 0.6313;  Loss pred: 0.6313; Loss self: 0.0000; time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6004 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6361 score: 0.5102 time: 0.06s
     INFO: Early stopping counter 1 of 20
Epoch 8/1000, LR 0.000180
Train loss: 0.6315;  Loss pred: 0.6315; Loss self: 0.0000; time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6031 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6367 score: 0.5102 time: 0.06s
     INFO: Early stopping counter 2 of 20
Epoch 9/1000, LR 0.000210
Train loss: 0.5685;  Loss pred: 0.5685; Loss self: 0.0000; time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6048 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6368 score: 0.5102 time: 0.06s
     INFO: Early stopping counter 3 of 20
Epoch 10/1000, LR 0.000240
Train loss: 0.5529;  Loss pred: 0.5529; Loss self: 0.0000; time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6033 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6362 score: 0.5102 time: 0.06s
     INFO: Early stopping counter 4 of 20
Epoch 11/1000, LR 0.000270
Train loss: 0.5466;  Loss pred: 0.5466; Loss self: 0.0000; time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6034 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6369 score: 0.5102 time: 0.06s
     INFO: Early stopping counter 5 of 20
Epoch 12/1000, LR 0.000270
Train loss: 0.5171;  Loss pred: 0.5171; Loss self: 0.0000; time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6097 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6403 score: 0.5102 time: 0.06s
     INFO: Early stopping counter 6 of 20
Epoch 13/1000, LR 0.000270
Train loss: 0.5132;  Loss pred: 0.5132; Loss self: 0.0000; time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6191 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6445 score: 0.5102 time: 0.06s
     INFO: Early stopping counter 7 of 20
Epoch 14/1000, LR 0.000270
Train loss: 0.5056;  Loss pred: 0.5056; Loss self: 0.0000; time: 0.20s
Val loss: 0.6315 score: 0.4694 time: 0.07s
Test loss: 0.6482 score: 0.4898 time: 0.06s
     INFO: Early stopping counter 8 of 20
Epoch 15/1000, LR 0.000270
Train loss: 0.4578;  Loss pred: 0.4578; Loss self: 0.0000; time: 0.20s
Val loss: 0.6449 score: 0.4490 time: 0.08s
Test loss: 0.6541 score: 0.4694 time: 0.06s
     INFO: Early stopping counter 9 of 20
Epoch 16/1000, LR 0.000270
Train loss: 0.4390;  Loss pred: 0.4390; Loss self: 0.0000; time: 0.20s
Val loss: 0.6538 score: 0.4286 time: 0.08s
Test loss: 0.6584 score: 0.4490 time: 0.06s
     INFO: Early stopping counter 10 of 20
Epoch 17/1000, LR 0.000270
Train loss: 0.4346;  Loss pred: 0.4346; Loss self: 0.0000; time: 0.20s
Val loss: 0.6582 score: 0.3673 time: 0.08s
Test loss: 0.6616 score: 0.4490 time: 0.06s
     INFO: Early stopping counter 11 of 20
Epoch 18/1000, LR 0.000270
Train loss: 0.3747;  Loss pred: 0.3747; Loss self: 0.0000; time: 0.20s
Val loss: 0.6591 score: 0.3673 time: 0.08s
Test loss: 0.6612 score: 0.4286 time: 0.06s
     INFO: Early stopping counter 12 of 20
Epoch 19/1000, LR 0.000270
Train loss: 0.3549;  Loss pred: 0.3549; Loss self: 0.0000; time: 0.20s
Val loss: 0.6603 score: 0.3878 time: 0.07s
Test loss: 0.6594 score: 0.4286 time: 0.07s
     INFO: Early stopping counter 13 of 20
Epoch 20/1000, LR 0.000270
Train loss: 0.3427;  Loss pred: 0.3427; Loss self: 0.0000; time: 0.20s
Val loss: 0.6596 score: 0.4694 time: 0.08s
Test loss: 0.6559 score: 0.4898 time: 0.06s
     INFO: Early stopping counter 14 of 20
Epoch 21/1000, LR 0.000270
Train loss: 0.3201;  Loss pred: 0.3201; Loss self: 0.0000; time: 0.20s
Val loss: 0.6559 score: 0.5510 time: 0.07s
Test loss: 0.6503 score: 0.5102 time: 0.07s
     INFO: Early stopping counter 15 of 20
Epoch 22/1000, LR 0.000270
Train loss: 0.2995;  Loss pred: 0.2995; Loss self: 0.0000; time: 0.20s
Val loss: 0.6531 score: 0.6122 time: 0.08s
Test loss: 0.6454 score: 0.5510 time: 0.06s
     INFO: Early stopping counter 16 of 20
Epoch 23/1000, LR 0.000270
Train loss: 0.2618;  Loss pred: 0.2618; Loss self: 0.0000; time: 0.20s
Val loss: 0.6505 score: 0.6122 time: 0.08s
Test loss: 0.6417 score: 0.5510 time: 0.06s
     INFO: Early stopping counter 17 of 20
Epoch 24/1000, LR 0.000270
Train loss: 0.2275;  Loss pred: 0.2275; Loss self: 0.0000; time: 0.19s
Val loss: 0.6457 score: 0.5918 time: 0.08s
Test loss: 0.6369 score: 0.5918 time: 0.06s
     INFO: Early stopping counter 18 of 20
Epoch 25/1000, LR 0.000270
Train loss: 0.2062;  Loss pred: 0.2062; Loss self: 0.0000; time: 0.20s
Val loss: 0.6403 score: 0.5918 time: 0.08s
Test loss: 0.6328 score: 0.6122 time: 0.06s
     INFO: Early stopping counter 19 of 20
Epoch 26/1000, LR 0.000270
Train loss: 0.2030;  Loss pred: 0.2030; Loss self: 0.0000; time: 0.20s
Val loss: 0.6329 score: 0.5918 time: 0.08s
Test loss: 0.6278 score: 0.6122 time: 0.06s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 005,   Train_Loss: 0.6126,   Val_Loss: 0.5995,   Val_Precision: 0.4898,   Val_Recall: 1.0000,   Val_accuracy: 0.6575,   Val_Score: 0.4898,   Val_Loss: 0.5995,   Test_Precision: 0.5102,   Test_Recall: 1.0000,   Test_accuracy: 0.6757,   Test_Score: 0.5102,   Test_loss: 0.6383


[0.06637286301702261, 0.06604503095149994, 0.06610185001045465, 0.06626755394972861, 0.06624213804025203, 0.06666089908685535, 0.06742979201953858, 0.06574355997145176, 0.06686882290523499, 0.06606505892705172, 0.06629644602071494, 0.06518818601034582, 0.06646155891939998, 0.06606286694295704, 0.0652678869664669, 0.06637275393586606, 0.0651394120650366, 0.06572998105548322, 0.07153654005378485, 0.06536293006502092, 0.07144412607885897, 0.06531349802389741, 0.06591310200747102, 0.06575892004184425, 0.0678081710357219, 0.06617944303434342]
[0.001354548224837196, 0.0013478577745204068, 0.001349017347152136, 0.001352399060198543, 0.0013518803681684087, 0.0013604265119766398, 0.0013761182044803792, 0.0013417053055398318, 0.0013646698552088775, 0.0013482665087153412, 0.0013529886943003048, 0.001330371143068282, 0.001356358345293877, 0.001348221774346062, 0.0013319976931932022, 0.001354545998691144, 0.0013293757564293183, 0.00134142818480578, 0.001459929388852752, 0.001333937348265733, 0.0014580433893644688, 0.0013329285310999472, 0.0013451653470912454, 0.0013420187763641684, 0.0013838402252188142, 0.0013506008782519065]
[738.2535236943598, 741.9180412828191, 741.2803120071552, 739.4267191025643, 739.7104237521011, 735.0635930690912, 726.681760872133, 745.3201503124805, 732.7779654420074, 741.6931248650703, 739.1044760482258, 751.669942038629, 737.2682915762455, 741.7177344469438, 750.7520509308819, 738.2547369866134, 752.2327642607112, 745.4741232716725, 684.9646343415446, 749.6603954451919, 685.8506456628012, 750.227770407757, 743.4030338072394, 745.1460572773993, 722.6267756755509, 740.4111874222294]
Elapsed: 0.06652436119755013~0.0015618949474545819
Time per graph: 0.0013576400244397987~3.1875407090909846e-05
Speed: 736.9573166922854~16.40572534205786
Total Time: 0.0666
best val loss: 0.599539577960968 test_score: 0.5102

Testing...
Test loss: 0.6454 score: 0.5510 time: 0.06s
test Score 0.5510
Epoch Time List: [0.3329930769978091, 0.33251633401960135, 0.34257458499632776, 0.33031432307325304, 0.35055711888708174, 0.337276719044894, 0.3305680660996586, 0.34174626995809376, 0.3409770530415699, 0.32855830783955753, 0.3451794661814347, 0.3380931280553341, 0.3363231299445033, 0.33736530295573175, 0.3342360380338505, 0.33515822794288397, 0.3367356410017237, 0.3390597610268742, 0.3379565250361338, 0.33875367511063814, 0.33270884514786303, 0.3405816978774965, 0.335437549976632, 0.3339052728842944, 0.33886442391667515, 0.3399042470846325]
Total Epoch List: [26]
Total Time List: [0.06655377801507711]
========================k_idx:1========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: GPSPerformer
GPS(
  (x_lin): Linear(in_features=14887, out_features=64, bias=True)
  (pe_lin): Linear(in_features=20, out_features=8, bias=True)
  (pe_norm): BatchNorm1d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (convs): ModuleList(
    (0-1): 2 x GPSConv(64, conv=GINEConv(nn=Sequential(
      (0): Linear(in_features=64, out_features=64, bias=True)
      (1): ReLU()
      (2): Linear(in_features=64, out_features=64, bias=True)
    )), heads=1, attn_type=performer)
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=32, bias=True)
    (1): ReLU()
    (2): Linear(in_features=32, out_features=16, bias=True)
    (3): ReLU()
    (4): Linear(in_features=16, out_features=2, bias=True)
  )
)
Total number of parameters: 1039522
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x747124497fd0>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.6196;  Loss pred: 0.6196; Loss self: 0.0000; time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.9212 score: 0.5102 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.8467 score: 0.4898 time: 0.07s
Epoch 2/1000, LR 0.000000
Train loss: 0.6672;  Loss pred: 0.6672; Loss self: 0.0000; time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.8478 score: 0.5102 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7791 score: 0.4898 time: 0.07s
Epoch 3/1000, LR 0.000030
Train loss: 0.6367;  Loss pred: 0.6367; Loss self: 0.0000; time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.8038 score: 0.5102 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7384 score: 0.4898 time: 0.07s
Epoch 4/1000, LR 0.000060
Train loss: 0.6489;  Loss pred: 0.6489; Loss self: 0.0000; time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7724 score: 0.5102 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7080 score: 0.4898 time: 0.07s
Epoch 5/1000, LR 0.000090
Train loss: 0.6418;  Loss pred: 0.6418; Loss self: 0.0000; time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7482 score: 0.5102 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6853 score: 0.4898 time: 0.07s
Epoch 6/1000, LR 0.000120
Train loss: 0.6070;  Loss pred: 0.6070; Loss self: 0.0000; time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7302 score: 0.5102 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6669 score: 0.4898 time: 0.07s
Epoch 7/1000, LR 0.000150
Train loss: 0.5925;  Loss pred: 0.5925; Loss self: 0.0000; time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7156 score: 0.5102 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6531 score: 0.4898 time: 0.07s
Epoch 8/1000, LR 0.000180
Train loss: 0.5274;  Loss pred: 0.5274; Loss self: 0.0000; time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7055 score: 0.5102 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6450 score: 0.4898 time: 0.07s
Epoch 9/1000, LR 0.000210
Train loss: 0.4944;  Loss pred: 0.4944; Loss self: 0.0000; time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7004 score: 0.5102 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6394 score: 0.4898 time: 0.07s
Epoch 10/1000, LR 0.000240
Train loss: 0.4658;  Loss pred: 0.4658; Loss self: 0.0000; time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6969 score: 0.5102 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6348 score: 0.4898 time: 0.07s
Epoch 11/1000, LR 0.000270
Train loss: 0.4503;  Loss pred: 0.4503; Loss self: 0.0000; time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6931 score: 0.5102 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6304 score: 0.4898 time: 0.07s
Epoch 12/1000, LR 0.000270
Train loss: 0.4093;  Loss pred: 0.4093; Loss self: 0.0000; time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6891 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6257 score: 0.4898 time: 0.07s
Epoch 13/1000, LR 0.000270
Train loss: 0.3861;  Loss pred: 0.3861; Loss self: 0.0000; time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6845 score: 0.5102 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6213 score: 0.4898 time: 0.08s
Epoch 14/1000, LR 0.000270
Train loss: 0.3286;  Loss pred: 0.3286; Loss self: 0.0000; time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6811 score: 0.5102 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6179 score: 0.4898 time: 0.07s
Epoch 15/1000, LR 0.000270
Train loss: 0.3142;  Loss pred: 0.3142; Loss self: 0.0000; time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6803 score: 0.5102 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6167 score: 0.4898 time: 0.07s
Epoch 16/1000, LR 0.000270
Train loss: 0.2805;  Loss pred: 0.2805; Loss self: 0.0000; time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6811 score: 0.5102 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6161 score: 0.4898 time: 0.07s
     INFO: Early stopping counter 1 of 20
Epoch 17/1000, LR 0.000270
Train loss: 0.2702;  Loss pred: 0.2702; Loss self: 0.0000; time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6821 score: 0.5102 time: 0.09s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6162 score: 0.4898 time: 0.08s
     INFO: Early stopping counter 2 of 20
Epoch 18/1000, LR 0.000270
Train loss: 0.2435;  Loss pred: 0.2435; Loss self: 0.0000; time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6822 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6160 score: 0.4898 time: 0.08s
     INFO: Early stopping counter 3 of 20
Epoch 19/1000, LR 0.000270
Train loss: 0.2344;  Loss pred: 0.2344; Loss self: 0.0000; time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6805 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6141 score: 0.4898 time: 0.08s
     INFO: Early stopping counter 4 of 20
Epoch 20/1000, LR 0.000270
Train loss: 0.2100;  Loss pred: 0.2100; Loss self: 0.0000; time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6779 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6115 score: 0.4898 time: 0.08s
Epoch 21/1000, LR 0.000270
Train loss: 0.1713;  Loss pred: 0.1713; Loss self: 0.0000; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6754 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6092 score: 0.4898 time: 0.10s
Epoch 22/1000, LR 0.000270
Train loss: 0.1660;  Loss pred: 0.1660; Loss self: 0.0000; time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6739 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6072 score: 0.4898 time: 0.08s
Epoch 23/1000, LR 0.000270
Train loss: 0.1459;  Loss pred: 0.1459; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6727 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6048 score: 0.4898 time: 0.09s
Epoch 24/1000, LR 0.000270
Train loss: 0.1335;  Loss pred: 0.1335; Loss self: 0.0000; time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6712 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6026 score: 0.4898 time: 0.08s
Epoch 25/1000, LR 0.000270
Train loss: 0.1292;  Loss pred: 0.1292; Loss self: 0.0000; time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6706 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6014 score: 0.4898 time: 0.08s
Epoch 26/1000, LR 0.000270
Train loss: 0.1072;  Loss pred: 0.1072; Loss self: 0.0000; time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6695 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6001 score: 0.4898 time: 0.08s
Epoch 27/1000, LR 0.000270
Train loss: 0.1024;  Loss pred: 0.1024; Loss self: 0.0000; time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6676 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.5985 score: 0.4898 time: 0.08s
Epoch 28/1000, LR 0.000270
Train loss: 0.0867;  Loss pred: 0.0867; Loss self: 0.0000; time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6651 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.5967 score: 0.4898 time: 0.08s
Epoch 29/1000, LR 0.000270
Train loss: 0.0823;  Loss pred: 0.0823; Loss self: 0.0000; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6616 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.5943 score: 0.4898 time: 0.08s
Epoch 30/1000, LR 0.000270
Train loss: 0.0856;  Loss pred: 0.0856; Loss self: 0.0000; time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6569 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.5917 score: 0.4898 time: 0.09s
Epoch 31/1000, LR 0.000270
Train loss: 0.0734;  Loss pred: 0.0734; Loss self: 0.0000; time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6513 score: 0.5102 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.5877 score: 0.4898 time: 0.08s
Epoch 32/1000, LR 0.000270
Train loss: 0.0620;  Loss pred: 0.0620; Loss self: 0.0000; time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6450 score: 0.5102 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.5819 score: 0.4898 time: 0.08s
Epoch 33/1000, LR 0.000270
Train loss: 0.0694;  Loss pred: 0.0694; Loss self: 0.0000; time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6386 score: 0.5102 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.5755 score: 0.4898 time: 0.07s
Epoch 34/1000, LR 0.000270
Train loss: 0.0532;  Loss pred: 0.0532; Loss self: 0.0000; time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6313 score: 0.5102 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.5675 score: 0.4898 time: 0.07s
Epoch 35/1000, LR 0.000270
Train loss: 0.0488;  Loss pred: 0.0488; Loss self: 0.0000; time: 0.20s
Val loss: 0.6231 score: 0.5306 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.5600 score: 0.4898 time: 0.07s
Epoch 36/1000, LR 0.000270
Train loss: 0.0431;  Loss pred: 0.0431; Loss self: 0.0000; time: 0.21s
Val loss: 0.6151 score: 0.5306 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.5540 score: 0.4898 time: 0.07s
Epoch 37/1000, LR 0.000270
Train loss: 0.0438;  Loss pred: 0.0438; Loss self: 0.0000; time: 0.30s
Val loss: 0.6088 score: 0.5306 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.5498 score: 0.4898 time: 0.07s
Epoch 38/1000, LR 0.000270
Train loss: 0.0382;  Loss pred: 0.0382; Loss self: 0.0000; time: 0.21s
Val loss: 0.6004 score: 0.5306 time: 0.06s
Test loss: 0.5438 score: 0.5102 time: 0.07s
Epoch 39/1000, LR 0.000269
Train loss: 0.0331;  Loss pred: 0.0331; Loss self: 0.0000; time: 0.20s
Val loss: 0.5932 score: 0.5306 time: 0.06s
Test loss: 0.5400 score: 0.5306 time: 0.08s
Epoch 40/1000, LR 0.000269
Train loss: 0.0280;  Loss pred: 0.0280; Loss self: 0.0000; time: 0.21s
Val loss: 0.5872 score: 0.5306 time: 0.07s
Test loss: 0.5374 score: 0.5306 time: 0.07s
Epoch 41/1000, LR 0.000269
Train loss: 0.0256;  Loss pred: 0.0256; Loss self: 0.0000; time: 0.21s
Val loss: 0.5789 score: 0.5510 time: 0.10s
Test loss: 0.5319 score: 0.5306 time: 0.07s
Epoch 42/1000, LR 0.000269
Train loss: 0.0223;  Loss pred: 0.0223; Loss self: 0.0000; time: 0.21s
Val loss: 0.5688 score: 0.5918 time: 0.06s
Test loss: 0.5216 score: 0.5510 time: 0.07s
Epoch 43/1000, LR 0.000269
Train loss: 0.0194;  Loss pred: 0.0194; Loss self: 0.0000; time: 0.20s
Val loss: 0.5572 score: 0.5918 time: 0.06s
Test loss: 0.5086 score: 0.5918 time: 0.07s
Epoch 44/1000, LR 0.000269
Train loss: 0.0169;  Loss pred: 0.0169; Loss self: 0.0000; time: 0.21s
Val loss: 0.5429 score: 0.6327 time: 0.06s
Test loss: 0.4927 score: 0.6327 time: 0.10s
Epoch 45/1000, LR 0.000269
Train loss: 0.0164;  Loss pred: 0.0164; Loss self: 0.0000; time: 0.21s
Val loss: 0.5222 score: 0.6939 time: 0.06s
Test loss: 0.4710 score: 0.6939 time: 0.07s
Epoch 46/1000, LR 0.000269
Train loss: 0.0132;  Loss pred: 0.0132; Loss self: 0.0000; time: 0.28s
Val loss: 0.5005 score: 0.7551 time: 0.06s
Test loss: 0.4469 score: 0.7551 time: 0.07s
Epoch 47/1000, LR 0.000269
Train loss: 0.0139;  Loss pred: 0.0139; Loss self: 0.0000; time: 0.21s
Val loss: 0.4785 score: 0.7755 time: 0.06s
Test loss: 0.4214 score: 0.7959 time: 0.07s
Epoch 48/1000, LR 0.000269
Train loss: 0.0125;  Loss pred: 0.0125; Loss self: 0.0000; time: 0.21s
Val loss: 0.4598 score: 0.7959 time: 0.06s
Test loss: 0.3984 score: 0.8980 time: 0.07s
Epoch 49/1000, LR 0.000269
Train loss: 0.0155;  Loss pred: 0.0155; Loss self: 0.0000; time: 0.21s
Val loss: 0.4473 score: 0.8163 time: 0.06s
Test loss: 0.3830 score: 0.9184 time: 0.07s
Epoch 50/1000, LR 0.000269
Train loss: 0.0100;  Loss pred: 0.0100; Loss self: 0.0000; time: 0.21s
Val loss: 0.4359 score: 0.8367 time: 0.11s
Test loss: 0.3691 score: 0.9388 time: 0.08s
Epoch 51/1000, LR 0.000269
Train loss: 0.0083;  Loss pred: 0.0083; Loss self: 0.0000; time: 0.22s
Val loss: 0.4241 score: 0.8367 time: 0.07s
Test loss: 0.3558 score: 0.9388 time: 0.08s
Epoch 52/1000, LR 0.000269
Train loss: 0.0082;  Loss pred: 0.0082; Loss self: 0.0000; time: 0.23s
Val loss: 0.4124 score: 0.8571 time: 0.07s
Test loss: 0.3428 score: 0.9388 time: 0.08s
Epoch 53/1000, LR 0.000269
Train loss: 0.0083;  Loss pred: 0.0083; Loss self: 0.0000; time: 0.22s
Val loss: 0.4043 score: 0.8571 time: 0.07s
Test loss: 0.3337 score: 0.9388 time: 0.08s
Epoch 54/1000, LR 0.000269
Train loss: 0.0061;  Loss pred: 0.0061; Loss self: 0.0000; time: 0.22s
Val loss: 0.3951 score: 0.8571 time: 0.07s
Test loss: 0.3224 score: 0.9388 time: 0.08s
Epoch 55/1000, LR 0.000269
Train loss: 0.0079;  Loss pred: 0.0079; Loss self: 0.0000; time: 0.22s
Val loss: 0.3869 score: 0.8367 time: 0.07s
Test loss: 0.3133 score: 0.9388 time: 0.08s
Epoch 56/1000, LR 0.000269
Train loss: 0.0058;  Loss pred: 0.0058; Loss self: 0.0000; time: 0.23s
Val loss: 0.3792 score: 0.8367 time: 0.07s
Test loss: 0.3044 score: 0.9388 time: 0.08s
Epoch 57/1000, LR 0.000269
Train loss: 0.0058;  Loss pred: 0.0058; Loss self: 0.0000; time: 0.23s
Val loss: 0.3713 score: 0.8367 time: 0.07s
Test loss: 0.2944 score: 0.9388 time: 0.08s
Epoch 58/1000, LR 0.000269
Train loss: 0.0062;  Loss pred: 0.0062; Loss self: 0.0000; time: 0.23s
Val loss: 0.3631 score: 0.8367 time: 0.07s
Test loss: 0.2819 score: 0.9184 time: 0.08s
Epoch 59/1000, LR 0.000268
Train loss: 0.0059;  Loss pred: 0.0059; Loss self: 0.0000; time: 0.22s
Val loss: 0.3549 score: 0.8571 time: 0.07s
Test loss: 0.2687 score: 0.9184 time: 0.08s
Epoch 60/1000, LR 0.000268
Train loss: 0.0050;  Loss pred: 0.0050; Loss self: 0.0000; time: 0.22s
Val loss: 0.3467 score: 0.8367 time: 0.07s
Test loss: 0.2558 score: 0.9184 time: 0.08s
Epoch 61/1000, LR 0.000268
Train loss: 0.0052;  Loss pred: 0.0052; Loss self: 0.0000; time: 0.22s
Val loss: 0.3389 score: 0.8367 time: 0.07s
Test loss: 0.2418 score: 0.9184 time: 0.08s
Epoch 62/1000, LR 0.000268
Train loss: 0.0053;  Loss pred: 0.0053; Loss self: 0.0000; time: 0.22s
Val loss: 0.3324 score: 0.8571 time: 0.07s
Test loss: 0.2275 score: 0.9184 time: 0.08s
Epoch 63/1000, LR 0.000268
Train loss: 0.0038;  Loss pred: 0.0038; Loss self: 0.0000; time: 0.22s
Val loss: 0.3258 score: 0.8571 time: 0.07s
Test loss: 0.2132 score: 0.9592 time: 0.08s
Epoch 64/1000, LR 0.000268
Train loss: 0.0051;  Loss pred: 0.0051; Loss self: 0.0000; time: 0.22s
Val loss: 0.3203 score: 0.8571 time: 0.07s
Test loss: 0.2000 score: 0.9592 time: 0.08s
Epoch 65/1000, LR 0.000268
Train loss: 0.0036;  Loss pred: 0.0036; Loss self: 0.0000; time: 0.22s
Val loss: 0.3161 score: 0.8776 time: 0.07s
Test loss: 0.1857 score: 0.9388 time: 0.08s
Epoch 66/1000, LR 0.000268
Train loss: 0.0034;  Loss pred: 0.0034; Loss self: 0.0000; time: 0.22s
Val loss: 0.3136 score: 0.8776 time: 0.07s
Test loss: 0.1741 score: 0.9388 time: 0.08s
Epoch 67/1000, LR 0.000268
Train loss: 0.0041;  Loss pred: 0.0041; Loss self: 0.0000; time: 0.22s
Val loss: 0.3123 score: 0.8776 time: 0.07s
Test loss: 0.1635 score: 0.9388 time: 0.08s
Epoch 68/1000, LR 0.000268
Train loss: 0.0035;  Loss pred: 0.0035; Loss self: 0.0000; time: 0.22s
Val loss: 0.3126 score: 0.8776 time: 0.07s
Test loss: 0.1540 score: 0.9388 time: 0.08s
     INFO: Early stopping counter 1 of 20
Epoch 69/1000, LR 0.000268
Train loss: 0.0033;  Loss pred: 0.0033; Loss self: 0.0000; time: 0.22s
Val loss: 0.3147 score: 0.8571 time: 0.07s
Test loss: 0.1459 score: 0.9592 time: 0.08s
     INFO: Early stopping counter 2 of 20
Epoch 70/1000, LR 0.000268
Train loss: 0.0027;  Loss pred: 0.0027; Loss self: 0.0000; time: 0.22s
Val loss: 0.3188 score: 0.8571 time: 0.07s
Test loss: 0.1398 score: 0.9592 time: 0.08s
     INFO: Early stopping counter 3 of 20
Epoch 71/1000, LR 0.000268
Train loss: 0.0028;  Loss pred: 0.0028; Loss self: 0.0000; time: 0.23s
Val loss: 0.3233 score: 0.8571 time: 0.07s
Test loss: 0.1350 score: 0.9592 time: 0.08s
     INFO: Early stopping counter 4 of 20
Epoch 72/1000, LR 0.000267
Train loss: 0.0029;  Loss pred: 0.0029; Loss self: 0.0000; time: 0.22s
Val loss: 0.3285 score: 0.8571 time: 0.07s
Test loss: 0.1316 score: 0.9592 time: 0.08s
     INFO: Early stopping counter 5 of 20
Epoch 73/1000, LR 0.000267
Train loss: 0.0028;  Loss pred: 0.0028; Loss self: 0.0000; time: 0.23s
Val loss: 0.3345 score: 0.8571 time: 0.07s
Test loss: 0.1283 score: 0.9592 time: 0.08s
     INFO: Early stopping counter 6 of 20
Epoch 74/1000, LR 0.000267
Train loss: 0.0027;  Loss pred: 0.0027; Loss self: 0.0000; time: 0.23s
Val loss: 0.3406 score: 0.8367 time: 0.07s
Test loss: 0.1254 score: 0.9592 time: 0.08s
     INFO: Early stopping counter 7 of 20
Epoch 75/1000, LR 0.000267
Train loss: 0.0025;  Loss pred: 0.0025; Loss self: 0.0000; time: 0.22s
Val loss: 0.3459 score: 0.8367 time: 0.07s
Test loss: 0.1231 score: 0.9592 time: 0.08s
     INFO: Early stopping counter 8 of 20
Epoch 76/1000, LR 0.000267
Train loss: 0.0030;  Loss pred: 0.0030; Loss self: 0.0000; time: 0.22s
Val loss: 0.3536 score: 0.8367 time: 0.07s
Test loss: 0.1217 score: 0.9592 time: 0.08s
     INFO: Early stopping counter 9 of 20
Epoch 77/1000, LR 0.000267
Train loss: 0.0022;  Loss pred: 0.0022; Loss self: 0.0000; time: 0.22s
Val loss: 0.3622 score: 0.8571 time: 0.07s
Test loss: 0.1205 score: 0.9592 time: 0.08s
     INFO: Early stopping counter 10 of 20
Epoch 78/1000, LR 0.000267
Train loss: 0.0023;  Loss pred: 0.0023; Loss self: 0.0000; time: 0.22s
Val loss: 0.3747 score: 0.8571 time: 0.07s
Test loss: 0.1205 score: 0.9592 time: 0.08s
     INFO: Early stopping counter 11 of 20
Epoch 79/1000, LR 0.000267
Train loss: 0.0019;  Loss pred: 0.0019; Loss self: 0.0000; time: 0.22s
Val loss: 0.3877 score: 0.8571 time: 0.07s
Test loss: 0.1209 score: 0.9592 time: 0.08s
     INFO: Early stopping counter 12 of 20
Epoch 80/1000, LR 0.000267
Train loss: 0.0026;  Loss pred: 0.0026; Loss self: 0.0000; time: 0.22s
Val loss: 0.4028 score: 0.8571 time: 0.07s
Test loss: 0.1219 score: 0.9592 time: 0.08s
     INFO: Early stopping counter 13 of 20
Epoch 81/1000, LR 0.000267
Train loss: 0.0024;  Loss pred: 0.0024; Loss self: 0.0000; time: 0.22s
Val loss: 0.4222 score: 0.8571 time: 0.07s
Test loss: 0.1241 score: 0.9592 time: 0.08s
     INFO: Early stopping counter 14 of 20
Epoch 82/1000, LR 0.000267
Train loss: 0.0021;  Loss pred: 0.0021; Loss self: 0.0000; time: 0.22s
Val loss: 0.4424 score: 0.8367 time: 0.07s
Test loss: 0.1269 score: 0.9388 time: 0.08s
     INFO: Early stopping counter 15 of 20
Epoch 83/1000, LR 0.000266
Train loss: 0.0017;  Loss pred: 0.0017; Loss self: 0.0000; time: 0.22s
Val loss: 0.4644 score: 0.8163 time: 0.07s
Test loss: 0.1303 score: 0.9388 time: 0.08s
     INFO: Early stopping counter 16 of 20
Epoch 84/1000, LR 0.000266
Train loss: 0.0019;  Loss pred: 0.0019; Loss self: 0.0000; time: 0.22s
Val loss: 0.4847 score: 0.7959 time: 0.07s
Test loss: 0.1334 score: 0.9388 time: 0.08s
     INFO: Early stopping counter 17 of 20
Epoch 85/1000, LR 0.000266
Train loss: 0.0020;  Loss pred: 0.0020; Loss self: 0.0000; time: 0.22s
Val loss: 0.5079 score: 0.7959 time: 0.07s
Test loss: 0.1370 score: 0.9388 time: 0.08s
     INFO: Early stopping counter 18 of 20
Epoch 86/1000, LR 0.000266
Train loss: 0.0019;  Loss pred: 0.0019; Loss self: 0.0000; time: 0.22s
Val loss: 0.5238 score: 0.7959 time: 0.07s
Test loss: 0.1390 score: 0.9388 time: 0.08s
     INFO: Early stopping counter 19 of 20
Epoch 87/1000, LR 0.000266
Train loss: 0.0020;  Loss pred: 0.0020; Loss self: 0.0000; time: 0.22s
Val loss: 0.5415 score: 0.7959 time: 0.07s
Test loss: 0.1419 score: 0.9388 time: 0.08s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 066,   Train_Loss: 0.0041,   Val_Loss: 0.3123,   Val_Precision: 0.9524,   Val_Recall: 0.8000,   Val_accuracy: 0.8696,   Val_Score: 0.8776,   Val_Loss: 0.3123,   Test_Precision: 0.9565,   Test_Recall: 0.9167,   Test_accuracy: 0.9362,   Test_Score: 0.9388,   Test_loss: 0.1635


[0.06637286301702261, 0.06604503095149994, 0.06610185001045465, 0.06626755394972861, 0.06624213804025203, 0.06666089908685535, 0.06742979201953858, 0.06574355997145176, 0.06686882290523499, 0.06606505892705172, 0.06629644602071494, 0.06518818601034582, 0.06646155891939998, 0.06606286694295704, 0.0652678869664669, 0.06637275393586606, 0.0651394120650366, 0.06572998105548322, 0.07153654005378485, 0.06536293006502092, 0.07144412607885897, 0.06531349802389741, 0.06591310200747102, 0.06575892004184425, 0.0678081710357219, 0.06617944303434342, 0.07726806495338678, 0.076911062002182, 0.07679986499715596, 0.07714617298915982, 0.07277223106939346, 0.07641657604835927, 0.07663503801450133, 0.07285562600009143, 0.07323686394374818, 0.07775542896706611, 0.07675091805867851, 0.07364030694589019, 0.08312537800520658, 0.07702338707167655, 0.07543517707381397, 0.07518171099945903, 0.08425703889224678, 0.0846936620073393, 0.08065770904067904, 0.08476921601686627, 0.10756134206894785, 0.08435409201774746, 0.09040081198327243, 0.08713996293954551, 0.08315095189027488, 0.08215527306310833, 0.0838290600804612, 0.08895794302225113, 0.08299971395172179, 0.09047198703046888, 0.08131241204682738, 0.0810748899821192, 0.07812981202732772, 0.0777679969323799, 0.07343696197494864, 0.07907146797515452, 0.07788885897025466, 0.07859994599130005, 0.08153688791207969, 0.07841119996737689, 0.07744228094816208, 0.07864545995835215, 0.07846716209314764, 0.10213577700778842, 0.08119238098151982, 0.07871469005476683, 0.07886659097857773, 0.0759620659518987, 0.07822217198554426, 0.08139933703932911, 0.08386627200525254, 0.08456716896034777, 0.08036874199751765, 0.08006963401567191, 0.0844101719558239, 0.08132505300454795, 0.08614958100952208, 0.08643141493666917, 0.08536995807662606, 0.08107691607438028, 0.0844919040100649, 0.08481291704811156, 0.0852338169934228, 0.08129130396991968, 0.08506144594866782, 0.08533294894732535, 0.08527389890514314, 0.08539619599469006, 0.08541544200852513, 0.0860082630533725, 0.08920591499190778, 0.0854052749928087, 0.08145413699094206, 0.08575887710321695, 0.085284554050304, 0.08553031692281365, 0.08166616305243224, 0.08603184891398996, 0.08505994407460093, 0.08563303807750344, 0.08532466506585479, 0.08082952699624002, 0.08113259496167302, 0.08628686307929456, 0.08465460990555584, 0.08100075903348625, 0.08670016494579613]
[0.001354548224837196, 0.0013478577745204068, 0.001349017347152136, 0.001352399060198543, 0.0013518803681684087, 0.0013604265119766398, 0.0013761182044803792, 0.0013417053055398318, 0.0013646698552088775, 0.0013482665087153412, 0.0013529886943003048, 0.001330371143068282, 0.001356358345293877, 0.001348221774346062, 0.0013319976931932022, 0.001354545998691144, 0.0013293757564293183, 0.00134142818480578, 0.001459929388852752, 0.001333937348265733, 0.0014580433893644688, 0.0013329285310999472, 0.0013451653470912454, 0.0013420187763641684, 0.0013838402252188142, 0.0013506008782519065, 0.0015768992847629956, 0.0015696135102486123, 0.0015673441836154278, 0.0015744116936563229, 0.0014851475728447645, 0.0015595219601705975, 0.0015639803676428844, 0.0014868495102059475, 0.001494629876403024, 0.0015868454891237982, 0.001566345266503643, 0.0015028634070589834, 0.0016964362858205425, 0.001571905858605644, 0.001539493409669673, 0.001534320632642021, 0.00171953140596422, 0.0017284420817824347, 0.0016460756947077354, 0.0017299840003442094, 0.0021951294299785277, 0.0017215120819948462, 0.001844914530270866, 0.0017783665906029697, 0.0016969582018423446, 0.001676638225777721, 0.0017107971444992082, 0.0018154682249439005, 0.0016938717133004445, 0.001846367082254467, 0.0016594369805474974, 0.0016545895914718205, 0.001594485959741382, 0.0015871019782118347, 0.0014987135096928294, 0.001613703428064378, 0.0015895685504133605, 0.0016040805304346948, 0.0016640181206546876, 0.0016002285707627936, 0.0015804547132277976, 0.0016050093869051458, 0.0016013706549621966, 0.0020844036124038454, 0.0016569873669697922, 0.0016064222460156496, 0.0016095222648689334, 0.0015502462439162998, 0.0015963708568478422, 0.0016612109599863083, 0.0017115565715357661, 0.0017258605910275056, 0.001640178408112605, 0.001634074163585141, 0.0017226565705270184, 0.001659694959276489, 0.0017581547144800425, 0.0017639064272789626, 0.0017422440423801237, 0.001654630940293475, 0.0017243245716339775, 0.0017308758581247256, 0.001739465652926996, 0.0016590062034677487, 0.001735947876503425, 0.0017414887540270479, 0.0017402836511253702, 0.0017427795100957155, 0.001743172285888268, 0.0017552706745586225, 0.001820528877385873, 0.0017429647957716062, 0.0016623293263457563, 0.0017501811653717744, 0.0017405011030674285, 0.001745516671894156, 0.0016666563888251477, 0.0017557520186528563, 0.001735917226012264, 0.0017476130219898662, 0.0017413196952215263, 0.0016495821835967352, 0.001655767244115776, 0.0017609563893733584, 0.0017276451001133845, 0.001653076714969107, 0.0017693911213427782]
[738.2535236943598, 741.9180412828191, 741.2803120071552, 739.4267191025643, 739.7104237521011, 735.0635930690912, 726.681760872133, 745.3201503124805, 732.7779654420074, 741.6931248650703, 739.1044760482258, 751.669942038629, 737.2682915762455, 741.7177344469438, 750.7520509308819, 738.2547369866134, 752.2327642607112, 745.4741232716725, 684.9646343415446, 749.6603954451919, 685.8506456628012, 750.227770407757, 743.4030338072394, 745.1460572773993, 722.6267756755509, 740.4111874222294, 634.1559094246769, 637.0995111029652, 638.021954879928, 635.1578840713878, 673.333760418518, 641.2221344357403, 639.394215355226, 672.5630221053691, 669.061963625804, 630.1810773978793, 638.4288454053142, 665.3964660414096, 589.4710036317774, 636.1704134667759, 649.564326627789, 651.7542544403196, 581.5537864161627, 578.555689276416, 607.5054769443955, 578.0400280008562, 455.55400348752187, 580.8846829824305, 542.0305296490794, 562.3137576268467, 589.2897060837005, 596.4315882969581, 584.5228367462141, 550.822088902658, 590.3634803910492, 541.6041098279176, 602.6140261560703, 604.3794818692543, 627.1613706540227, 630.0792348117957, 667.2389309448182, 619.6925547834342, 629.1015255302794, 623.4100975772126, 600.9549941719158, 624.9107272989896, 632.7292972271746, 623.0493155733168, 624.4650461785856, 479.75353431994233, 603.5049028941876, 622.5013395327807, 621.302371409837, 645.0588117367446, 626.4208568518831, 601.9705047023299, 584.2634807581662, 579.4210756064844, 609.689772194188, 611.9673282184517, 580.4987582023192, 602.5203573769545, 568.7781580108214, 566.9234969241651, 573.9724032196285, 604.3643785741333, 579.9372208982649, 577.7421848632316, 574.8891898597144, 602.7705007430011, 576.0541624177199, 574.2213365935228, 574.6189705071015, 573.7960506232247, 573.6667615102831, 569.7127027154705, 549.290929916979, 573.7350532988261, 601.5655166225497, 571.3694215121893, 574.5471796815398, 572.8962754133107, 600.0037000457639, 569.5565144599831, 576.0643336071919, 572.2090573926832, 574.2770857896846, 606.2141128486296, 603.949621273023, 567.8732341326482, 578.8225833733852, 604.9326029123145, 565.1661681455184]
Elapsed: 0.07859448331976886~0.008183745776171118
Time per graph: 0.0016039690473422217~0.00016701521992185954
Speed: 630.388666774603~67.24591637132023
Total Time: 0.0873
best val loss: 0.3122861683368683 test_score: 0.9388

Testing...
Test loss: 0.1857 score: 0.9388 time: 0.08s
test Score 0.9388
Epoch Time List: [0.3329930769978091, 0.33251633401960135, 0.34257458499632776, 0.33031432307325304, 0.35055711888708174, 0.337276719044894, 0.3305680660996586, 0.34174626995809376, 0.3409770530415699, 0.32855830783955753, 0.3451794661814347, 0.3380931280553341, 0.3363231299445033, 0.33736530295573175, 0.3342360380338505, 0.33515822794288397, 0.3367356410017237, 0.3390597610268742, 0.3379565250361338, 0.33875367511063814, 0.33270884514786303, 0.3405816978774965, 0.335437549976632, 0.3339052728842944, 0.33886442391667515, 0.3399042470846325, 0.3296499880962074, 0.33214695402421057, 0.33356876706238836, 0.3274783940287307, 0.3222860590321943, 0.3306766060413793, 0.3280335471499711, 0.32460675516631454, 0.3279974280158058, 0.3322980930097401, 0.32720910594798625, 0.3296917040133849, 0.33436930796597153, 0.3555102900136262, 0.3327696640044451, 0.3414842280326411, 0.37110472994390875, 0.3723056009039283, 0.36712048295885324, 0.3776683211326599, 0.40661353699397296, 0.3785742500331253, 0.4051880099577829, 0.3852372660767287, 0.3756922190077603, 0.3714109939755872, 0.3735179001232609, 0.38444136595353484, 0.3858973318710923, 0.3844154089456424, 0.34921101992949843, 0.34556887799408287, 0.34734118916094303, 0.3387654221151024, 0.3319628610042855, 0.3392228650627658, 0.43313247500918806, 0.3405794820282608, 0.34123500308487564, 0.3466344961198047, 0.38888249022420496, 0.34327571000903845, 0.33521328098140657, 0.3652929400559515, 0.34478886099532247, 0.4137031289283186, 0.34343981195706874, 0.3415254969149828, 0.34285065496806055, 0.3979953197995201, 0.37169940408784896, 0.3756842479342595, 0.3674900059122592, 0.3600947289960459, 0.3698243440594524, 0.37023517105262727, 0.376137105980888, 0.3825250160880387, 0.36653539910912514, 0.36595754395239055, 0.36462677002418786, 0.3646637520287186, 0.3658248089486733, 0.361139886896126, 0.3679056051187217, 0.36499944294337183, 0.3666828400455415, 0.3652493041008711, 0.36480168788693845, 0.3707028260687366, 0.38114632200449705, 0.3696644080337137, 0.37136418907903135, 0.37651725905016065, 0.3715075639775023, 0.37203725101426244, 0.3680309629999101, 0.3712207699427381, 0.37149085209239274, 0.37126327713485807, 0.3713603609940037, 0.36632128106430173, 0.3718069239985198, 0.37079738604370505, 0.3731387819861993, 0.36442163900937885, 0.36728859203867614]
Total Epoch List: [26, 87]
Total Time List: [0.06655377801507711, 0.08728704391978681]
========================k_idx:2========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: GPSPerformer
GPS(
  (x_lin): Linear(in_features=14887, out_features=64, bias=True)
  (pe_lin): Linear(in_features=20, out_features=8, bias=True)
  (pe_norm): BatchNorm1d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (convs): ModuleList(
    (0-1): 2 x GPSConv(64, conv=GINEConv(nn=Sequential(
      (0): Linear(in_features=64, out_features=64, bias=True)
      (1): ReLU()
      (2): Linear(in_features=64, out_features=64, bias=True)
    )), heads=1, attn_type=performer)
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=32, bias=True)
    (1): ReLU()
    (2): Linear(in_features=32, out_features=16, bias=True)
    (3): ReLU()
    (4): Linear(in_features=16, out_features=2, bias=True)
  )
)
Total number of parameters: 1039522
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x747124497670>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.8058;  Loss pred: 0.8058; Loss self: 0.0000; time: 0.20s
Val loss: 0.6744 score: 0.4490 time: 0.07s
Test loss: 0.6926 score: 0.4375 time: 0.07s
Epoch 2/1000, LR 0.000000
Train loss: 0.7960;  Loss pred: 0.7960; Loss self: 0.0000; time: 0.21s
Val loss: 0.6456 score: 0.4898 time: 0.07s
Test loss: 0.6657 score: 0.4792 time: 0.07s
Epoch 3/1000, LR 0.000030
Train loss: 0.7570;  Loss pred: 0.7570; Loss self: 0.0000; time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6276 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6469 score: 0.5000 time: 0.07s
Epoch 4/1000, LR 0.000060
Train loss: 0.7705;  Loss pred: 0.7705; Loss self: 0.0000; time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6267 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6423 score: 0.5000 time: 0.07s
Epoch 5/1000, LR 0.000090
Train loss: 0.7359;  Loss pred: 0.7359; Loss self: 0.0000; time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6267 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6415 score: 0.5000 time: 0.07s
Epoch 6/1000, LR 0.000120
Train loss: 0.6612;  Loss pred: 0.6612; Loss self: 0.0000; time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6272 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6436 score: 0.5000 time: 0.07s
     INFO: Early stopping counter 1 of 20
Epoch 7/1000, LR 0.000150
Train loss: 0.6736;  Loss pred: 0.6736; Loss self: 0.0000; time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6329 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6455 score: 0.5000 time: 0.07s
     INFO: Early stopping counter 2 of 20
Epoch 8/1000, LR 0.000180
Train loss: 0.6632;  Loss pred: 0.6632; Loss self: 0.0000; time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6367 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6526 score: 0.5000 time: 0.07s
     INFO: Early stopping counter 3 of 20
Epoch 9/1000, LR 0.000210
Train loss: 0.5913;  Loss pred: 0.5913; Loss self: 0.0000; time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6394 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6603 score: 0.5000 time: 0.07s
     INFO: Early stopping counter 4 of 20
Epoch 10/1000, LR 0.000240
Train loss: 0.6325;  Loss pred: 0.6325; Loss self: 0.0000; time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6420 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6663 score: 0.5000 time: 0.07s
     INFO: Early stopping counter 5 of 20
Epoch 11/1000, LR 0.000270
Train loss: 0.5571;  Loss pred: 0.5571; Loss self: 0.0000; time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6470 score: 0.5102 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6734 score: 0.5000 time: 0.07s
     INFO: Early stopping counter 6 of 20
Epoch 12/1000, LR 0.000270
Train loss: 0.5238;  Loss pred: 0.5238; Loss self: 0.0000; time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6522 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6765 score: 0.5000 time: 0.07s
     INFO: Early stopping counter 7 of 20
Epoch 13/1000, LR 0.000270
Train loss: 0.5784;  Loss pred: 0.5784; Loss self: 0.0000; time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6571 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6795 score: 0.5000 time: 0.07s
     INFO: Early stopping counter 8 of 20
Epoch 14/1000, LR 0.000270
Train loss: 0.5165;  Loss pred: 0.5165; Loss self: 0.0000; time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6601 score: 0.5102 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6806 score: 0.5000 time: 0.07s
     INFO: Early stopping counter 9 of 20
Epoch 15/1000, LR 0.000270
Train loss: 0.4869;  Loss pred: 0.4869; Loss self: 0.0000; time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6618 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6795 score: 0.5000 time: 0.07s
     INFO: Early stopping counter 10 of 20
Epoch 16/1000, LR 0.000270
Train loss: 0.4727;  Loss pred: 0.4727; Loss self: 0.0000; time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6640 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6796 score: 0.5000 time: 0.07s
     INFO: Early stopping counter 11 of 20
Epoch 17/1000, LR 0.000270
Train loss: 0.4652;  Loss pred: 0.4652; Loss self: 0.0000; time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6659 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6806 score: 0.5000 time: 0.07s
     INFO: Early stopping counter 12 of 20
Epoch 18/1000, LR 0.000270
Train loss: 0.4788;  Loss pred: 0.4788; Loss self: 0.0000; time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6667 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6797 score: 0.5000 time: 0.07s
     INFO: Early stopping counter 13 of 20
Epoch 19/1000, LR 0.000270
Train loss: 0.4351;  Loss pred: 0.4351; Loss self: 0.0000; time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6676 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6790 score: 0.5000 time: 0.07s
     INFO: Early stopping counter 14 of 20
Epoch 20/1000, LR 0.000270
Train loss: 0.4088;  Loss pred: 0.4088; Loss self: 0.0000; time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6675 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6783 score: 0.5000 time: 0.07s
     INFO: Early stopping counter 15 of 20
Epoch 21/1000, LR 0.000270
Train loss: 0.3673;  Loss pred: 0.3673; Loss self: 0.0000; time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6624 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6732 score: 0.5000 time: 0.07s
     INFO: Early stopping counter 16 of 20
Epoch 22/1000, LR 0.000270
Train loss: 0.3320;  Loss pred: 0.3320; Loss self: 0.0000; time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6560 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6675 score: 0.5000 time: 0.07s
     INFO: Early stopping counter 17 of 20
Epoch 23/1000, LR 0.000270
Train loss: 0.2871;  Loss pred: 0.2871; Loss self: 0.0000; time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6468 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6588 score: 0.5000 time: 0.07s
     INFO: Early stopping counter 18 of 20
Epoch 24/1000, LR 0.000270
Train loss: 0.2876;  Loss pred: 0.2876; Loss self: 0.0000; time: 0.20s
Val loss: 0.6346 score: 0.5306 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6464 score: 0.5000 time: 0.07s
     INFO: Early stopping counter 19 of 20
Epoch 25/1000, LR 0.000270
Train loss: 0.2496;  Loss pred: 0.2496; Loss self: 0.0000; time: 0.21s
Val loss: 0.6193 score: 0.5306 time: 0.07s
Test loss: 0.6319 score: 0.5208 time: 0.07s
Epoch 26/1000, LR 0.000270
Train loss: 0.2316;  Loss pred: 0.2316; Loss self: 0.0000; time: 0.20s
Val loss: 0.6090 score: 0.5306 time: 0.07s
Test loss: 0.6215 score: 0.5417 time: 0.07s
Epoch 27/1000, LR 0.000270
Train loss: 0.2224;  Loss pred: 0.2224; Loss self: 0.0000; time: 0.20s
Val loss: 0.6003 score: 0.5510 time: 0.06s
Test loss: 0.6141 score: 0.5625 time: 0.07s
Epoch 28/1000, LR 0.000270
Train loss: 0.2300;  Loss pred: 0.2300; Loss self: 0.0000; time: 0.20s
Val loss: 0.5939 score: 0.5510 time: 0.07s
Test loss: 0.6091 score: 0.6042 time: 0.07s
Epoch 29/1000, LR 0.000270
Train loss: 0.1977;  Loss pred: 0.1977; Loss self: 0.0000; time: 0.20s
Val loss: 0.5876 score: 0.5510 time: 0.07s
Test loss: 0.6034 score: 0.6250 time: 0.07s
Epoch 30/1000, LR 0.000270
Train loss: 0.1748;  Loss pred: 0.1748; Loss self: 0.0000; time: 0.20s
Val loss: 0.5815 score: 0.5510 time: 0.06s
Test loss: 0.5978 score: 0.6250 time: 0.07s
Epoch 31/1000, LR 0.000270
Train loss: 0.1575;  Loss pred: 0.1575; Loss self: 0.0000; time: 0.20s
Val loss: 0.5776 score: 0.5510 time: 0.07s
Test loss: 0.5943 score: 0.6042 time: 0.07s
Epoch 32/1000, LR 0.000270
Train loss: 0.1451;  Loss pred: 0.1451; Loss self: 0.0000; time: 0.20s
Val loss: 0.5707 score: 0.6122 time: 0.06s
Test loss: 0.5883 score: 0.6042 time: 0.07s
Epoch 33/1000, LR 0.000270
Train loss: 0.1291;  Loss pred: 0.1291; Loss self: 0.0000; time: 0.21s
Val loss: 0.5596 score: 0.6327 time: 0.07s
Test loss: 0.5780 score: 0.6250 time: 0.07s
Epoch 34/1000, LR 0.000270
Train loss: 0.1177;  Loss pred: 0.1177; Loss self: 0.0000; time: 0.21s
Val loss: 0.5465 score: 0.6735 time: 0.07s
Test loss: 0.5625 score: 0.6667 time: 0.07s
Epoch 35/1000, LR 0.000270
Train loss: 0.1037;  Loss pred: 0.1037; Loss self: 0.0000; time: 0.21s
Val loss: 0.5345 score: 0.7143 time: 0.07s
Test loss: 0.5497 score: 0.7292 time: 0.07s
Epoch 36/1000, LR 0.000270
Train loss: 0.0882;  Loss pred: 0.0882; Loss self: 0.0000; time: 0.22s
Val loss: 0.5229 score: 0.6939 time: 0.07s
Test loss: 0.5355 score: 0.7292 time: 0.07s
Epoch 37/1000, LR 0.000270
Train loss: 0.0863;  Loss pred: 0.0863; Loss self: 0.0000; time: 0.21s
Val loss: 0.5079 score: 0.7551 time: 0.07s
Test loss: 0.5160 score: 0.7917 time: 0.07s
Epoch 38/1000, LR 0.000270
Train loss: 0.0897;  Loss pred: 0.0897; Loss self: 0.0000; time: 0.21s
Val loss: 0.4885 score: 0.7755 time: 0.07s
Test loss: 0.4917 score: 0.7917 time: 0.07s
Epoch 39/1000, LR 0.000269
Train loss: 0.0791;  Loss pred: 0.0791; Loss self: 0.0000; time: 0.20s
Val loss: 0.4716 score: 0.8163 time: 0.07s
Test loss: 0.4695 score: 0.8333 time: 0.07s
Epoch 40/1000, LR 0.000269
Train loss: 0.0621;  Loss pred: 0.0621; Loss self: 0.0000; time: 0.20s
Val loss: 0.4577 score: 0.8571 time: 0.07s
Test loss: 0.4497 score: 0.8958 time: 0.07s
Epoch 41/1000, LR 0.000269
Train loss: 0.0669;  Loss pred: 0.0669; Loss self: 0.0000; time: 0.21s
Val loss: 0.4417 score: 0.8776 time: 0.07s
Test loss: 0.4259 score: 0.9375 time: 0.07s
Epoch 42/1000, LR 0.000269
Train loss: 0.0529;  Loss pred: 0.0529; Loss self: 0.0000; time: 0.20s
Val loss: 0.4280 score: 0.8776 time: 0.07s
Test loss: 0.4056 score: 0.9167 time: 0.07s
Epoch 43/1000, LR 0.000269
Train loss: 0.0556;  Loss pred: 0.0556; Loss self: 0.0000; time: 0.21s
Val loss: 0.4178 score: 0.8776 time: 0.07s
Test loss: 0.3901 score: 0.9375 time: 0.07s
Epoch 44/1000, LR 0.000269
Train loss: 0.0490;  Loss pred: 0.0490; Loss self: 0.0000; time: 0.20s
Val loss: 0.4082 score: 0.8776 time: 0.07s
Test loss: 0.3761 score: 0.9375 time: 0.07s
Epoch 45/1000, LR 0.000269
Train loss: 0.0502;  Loss pred: 0.0502; Loss self: 0.0000; time: 0.20s
Val loss: 0.4010 score: 0.8776 time: 0.07s
Test loss: 0.3651 score: 0.9167 time: 0.07s
Epoch 46/1000, LR 0.000269
Train loss: 0.0457;  Loss pred: 0.0457; Loss self: 0.0000; time: 0.21s
Val loss: 0.3955 score: 0.8980 time: 0.07s
Test loss: 0.3556 score: 0.9167 time: 0.07s
Epoch 47/1000, LR 0.000269
Train loss: 0.0333;  Loss pred: 0.0333; Loss self: 0.0000; time: 0.21s
Val loss: 0.3930 score: 0.8980 time: 0.07s
Test loss: 0.3489 score: 0.9375 time: 0.07s
Epoch 48/1000, LR 0.000269
Train loss: 0.0304;  Loss pred: 0.0304; Loss self: 0.0000; time: 0.21s
Val loss: 0.3895 score: 0.8776 time: 0.07s
Test loss: 0.3424 score: 0.9167 time: 0.07s
Epoch 49/1000, LR 0.000269
Train loss: 0.0325;  Loss pred: 0.0325; Loss self: 0.0000; time: 0.21s
Val loss: 0.3812 score: 0.8776 time: 0.07s
Test loss: 0.3328 score: 0.9375 time: 0.07s
Epoch 50/1000, LR 0.000269
Train loss: 0.0266;  Loss pred: 0.0266; Loss self: 0.0000; time: 0.22s
Val loss: 0.3754 score: 0.8776 time: 0.07s
Test loss: 0.3257 score: 0.9375 time: 0.07s
Epoch 51/1000, LR 0.000269
Train loss: 0.0257;  Loss pred: 0.0257; Loss self: 0.0000; time: 0.21s
Val loss: 0.3717 score: 0.8776 time: 0.07s
Test loss: 0.3205 score: 0.9375 time: 0.07s
Epoch 52/1000, LR 0.000269
Train loss: 0.0201;  Loss pred: 0.0201; Loss self: 0.0000; time: 0.21s
Val loss: 0.3686 score: 0.8571 time: 0.07s
Test loss: 0.3162 score: 0.9167 time: 0.07s
Epoch 53/1000, LR 0.000269
Train loss: 0.0197;  Loss pred: 0.0197; Loss self: 0.0000; time: 0.21s
Val loss: 0.3660 score: 0.8367 time: 0.07s
Test loss: 0.3121 score: 0.9167 time: 0.07s
Epoch 54/1000, LR 0.000269
Train loss: 0.0178;  Loss pred: 0.0178; Loss self: 0.0000; time: 0.21s
Val loss: 0.3660 score: 0.8367 time: 0.07s
Test loss: 0.3103 score: 0.9167 time: 0.07s
Epoch 55/1000, LR 0.000269
Train loss: 0.0189;  Loss pred: 0.0189; Loss self: 0.0000; time: 0.21s
Val loss: 0.3636 score: 0.8571 time: 0.07s
Test loss: 0.3051 score: 0.8958 time: 0.07s
Epoch 56/1000, LR 0.000269
Train loss: 0.0152;  Loss pred: 0.0152; Loss self: 0.0000; time: 0.21s
Val loss: 0.3616 score: 0.8571 time: 0.07s
Test loss: 0.3005 score: 0.9167 time: 0.07s
Epoch 57/1000, LR 0.000269
Train loss: 0.0130;  Loss pred: 0.0130; Loss self: 0.0000; time: 0.20s
Val loss: 0.3596 score: 0.8571 time: 0.07s
Test loss: 0.2957 score: 0.9167 time: 0.07s
Epoch 58/1000, LR 0.000269
Train loss: 0.0115;  Loss pred: 0.0115; Loss self: 0.0000; time: 0.21s
Val loss: 0.3556 score: 0.8571 time: 0.07s
Test loss: 0.2902 score: 0.9167 time: 0.07s
Epoch 59/1000, LR 0.000268
Train loss: 0.0129;  Loss pred: 0.0129; Loss self: 0.0000; time: 0.20s
Val loss: 0.3482 score: 0.8571 time: 0.07s
Test loss: 0.2823 score: 0.9167 time: 0.07s
Epoch 60/1000, LR 0.000268
Train loss: 0.0112;  Loss pred: 0.0112; Loss self: 0.0000; time: 0.21s
Val loss: 0.3427 score: 0.8571 time: 0.07s
Test loss: 0.2759 score: 0.9167 time: 0.07s
Epoch 61/1000, LR 0.000268
Train loss: 0.0115;  Loss pred: 0.0115; Loss self: 0.0000; time: 0.21s
Val loss: 0.3353 score: 0.8571 time: 0.07s
Test loss: 0.2672 score: 0.9167 time: 0.07s
Epoch 62/1000, LR 0.000268
Train loss: 0.0078;  Loss pred: 0.0078; Loss self: 0.0000; time: 0.21s
Val loss: 0.3281 score: 0.8571 time: 0.07s
Test loss: 0.2588 score: 0.9167 time: 0.07s
Epoch 63/1000, LR 0.000268
Train loss: 0.0084;  Loss pred: 0.0084; Loss self: 0.0000; time: 0.21s
Val loss: 0.3234 score: 0.8571 time: 0.07s
Test loss: 0.2532 score: 0.9167 time: 0.07s
Epoch 64/1000, LR 0.000268
Train loss: 0.0076;  Loss pred: 0.0076; Loss self: 0.0000; time: 0.21s
Val loss: 0.3169 score: 0.8571 time: 0.07s
Test loss: 0.2444 score: 0.9167 time: 0.07s
Epoch 65/1000, LR 0.000268
Train loss: 0.0060;  Loss pred: 0.0060; Loss self: 0.0000; time: 0.21s
Val loss: 0.3143 score: 0.8571 time: 0.07s
Test loss: 0.2380 score: 0.9167 time: 0.07s
Epoch 66/1000, LR 0.000268
Train loss: 0.0055;  Loss pred: 0.0055; Loss self: 0.0000; time: 0.21s
Val loss: 0.3121 score: 0.8571 time: 0.07s
Test loss: 0.2323 score: 0.9167 time: 0.07s
Epoch 67/1000, LR 0.000268
Train loss: 0.0050;  Loss pred: 0.0050; Loss self: 0.0000; time: 0.21s
Val loss: 0.3101 score: 0.8571 time: 0.07s
Test loss: 0.2284 score: 0.9167 time: 0.07s
Epoch 68/1000, LR 0.000268
Train loss: 0.0060;  Loss pred: 0.0060; Loss self: 0.0000; time: 0.21s
Val loss: 0.3090 score: 0.8571 time: 0.07s
Test loss: 0.2266 score: 0.9167 time: 0.07s
Epoch 69/1000, LR 0.000268
Train loss: 0.0054;  Loss pred: 0.0054; Loss self: 0.0000; time: 0.20s
Val loss: 0.3092 score: 0.8776 time: 0.07s
Test loss: 0.2271 score: 0.9167 time: 0.07s
     INFO: Early stopping counter 1 of 20
Epoch 70/1000, LR 0.000268
Train loss: 0.0044;  Loss pred: 0.0044; Loss self: 0.0000; time: 0.21s
Val loss: 0.3096 score: 0.8776 time: 0.07s
Test loss: 0.2282 score: 0.9167 time: 0.07s
     INFO: Early stopping counter 2 of 20
Epoch 71/1000, LR 0.000268
Train loss: 0.0036;  Loss pred: 0.0036; Loss self: 0.0000; time: 0.21s
Val loss: 0.3117 score: 0.8776 time: 0.07s
Test loss: 0.2291 score: 0.9167 time: 0.07s
     INFO: Early stopping counter 3 of 20
Epoch 72/1000, LR 0.000267
Train loss: 0.0035;  Loss pred: 0.0035; Loss self: 0.0000; time: 0.21s
Val loss: 0.3111 score: 0.8776 time: 0.07s
Test loss: 0.2268 score: 0.9167 time: 0.07s
     INFO: Early stopping counter 4 of 20
Epoch 73/1000, LR 0.000267
Train loss: 0.0041;  Loss pred: 0.0041; Loss self: 0.0000; time: 0.20s
Val loss: 0.3071 score: 0.8776 time: 0.07s
Test loss: 0.2214 score: 0.9167 time: 0.07s
Epoch 74/1000, LR 0.000267
Train loss: 0.0035;  Loss pred: 0.0035; Loss self: 0.0000; time: 0.21s
Val loss: 0.3039 score: 0.8980 time: 0.07s
Test loss: 0.2162 score: 0.9167 time: 0.07s
Epoch 75/1000, LR 0.000267
Train loss: 0.0035;  Loss pred: 0.0035; Loss self: 0.0000; time: 0.21s
Val loss: 0.3020 score: 0.8980 time: 0.07s
Test loss: 0.2126 score: 0.9167 time: 0.07s
Epoch 76/1000, LR 0.000267
Train loss: 0.0033;  Loss pred: 0.0033; Loss self: 0.0000; time: 0.21s
Val loss: 0.3046 score: 0.8980 time: 0.07s
Test loss: 0.2149 score: 0.8958 time: 0.07s
     INFO: Early stopping counter 1 of 20
Epoch 77/1000, LR 0.000267
Train loss: 0.0037;  Loss pred: 0.0037; Loss self: 0.0000; time: 0.20s
Val loss: 0.3108 score: 0.8980 time: 0.07s
Test loss: 0.2220 score: 0.8958 time: 0.07s
     INFO: Early stopping counter 2 of 20
Epoch 78/1000, LR 0.000267
Train loss: 0.0030;  Loss pred: 0.0030; Loss self: 0.0000; time: 0.21s
Val loss: 0.3197 score: 0.8980 time: 0.07s
Test loss: 0.2315 score: 0.8958 time: 0.07s
     INFO: Early stopping counter 3 of 20
Epoch 79/1000, LR 0.000267
Train loss: 0.0025;  Loss pred: 0.0025; Loss self: 0.0000; time: 0.21s
Val loss: 0.3278 score: 0.8980 time: 0.08s
Test loss: 0.2397 score: 0.8958 time: 0.07s
     INFO: Early stopping counter 4 of 20
Epoch 80/1000, LR 0.000267
Train loss: 0.0027;  Loss pred: 0.0027; Loss self: 0.0000; time: 0.22s
Val loss: 0.3380 score: 0.8776 time: 0.07s
Test loss: 0.2491 score: 0.8958 time: 0.07s
     INFO: Early stopping counter 5 of 20
Epoch 81/1000, LR 0.000267
Train loss: 0.0028;  Loss pred: 0.0028; Loss self: 0.0000; time: 0.21s
Val loss: 0.3495 score: 0.8776 time: 0.07s
Test loss: 0.2596 score: 0.8958 time: 0.07s
     INFO: Early stopping counter 6 of 20
Epoch 82/1000, LR 0.000267
Train loss: 0.0027;  Loss pred: 0.0027; Loss self: 0.0000; time: 0.21s
Val loss: 0.3594 score: 0.8776 time: 0.07s
Test loss: 0.2678 score: 0.8958 time: 0.07s
     INFO: Early stopping counter 7 of 20
Epoch 83/1000, LR 0.000266
Train loss: 0.0027;  Loss pred: 0.0027; Loss self: 0.0000; time: 0.21s
Val loss: 0.3671 score: 0.8776 time: 0.07s
Test loss: 0.2744 score: 0.8958 time: 0.07s
     INFO: Early stopping counter 8 of 20
Epoch 84/1000, LR 0.000266
Train loss: 0.0022;  Loss pred: 0.0022; Loss self: 0.0000; time: 0.21s
Val loss: 0.3749 score: 0.8776 time: 0.07s
Test loss: 0.2794 score: 0.8958 time: 0.07s
     INFO: Early stopping counter 9 of 20
Epoch 85/1000, LR 0.000266
Train loss: 0.0025;  Loss pred: 0.0025; Loss self: 0.0000; time: 0.21s
Val loss: 0.3812 score: 0.8776 time: 0.07s
Test loss: 0.2825 score: 0.8958 time: 0.07s
     INFO: Early stopping counter 10 of 20
Epoch 86/1000, LR 0.000266
Train loss: 0.0020;  Loss pred: 0.0020; Loss self: 0.0000; time: 0.21s
Val loss: 0.3861 score: 0.8776 time: 0.07s
Test loss: 0.2854 score: 0.8958 time: 0.07s
     INFO: Early stopping counter 11 of 20
Epoch 87/1000, LR 0.000266
Train loss: 0.0021;  Loss pred: 0.0021; Loss self: 0.0000; time: 0.21s
Val loss: 0.3920 score: 0.8571 time: 0.07s
Test loss: 0.2901 score: 0.8958 time: 0.07s
     INFO: Early stopping counter 12 of 20
Epoch 88/1000, LR 0.000266
Train loss: 0.0018;  Loss pred: 0.0018; Loss self: 0.0000; time: 0.21s
Val loss: 0.3960 score: 0.8571 time: 0.07s
Test loss: 0.2926 score: 0.8958 time: 0.07s
     INFO: Early stopping counter 13 of 20
Epoch 89/1000, LR 0.000266
Train loss: 0.0016;  Loss pred: 0.0016; Loss self: 0.0000; time: 0.21s
Val loss: 0.4014 score: 0.8571 time: 0.07s
Test loss: 0.2962 score: 0.8958 time: 0.07s
     INFO: Early stopping counter 14 of 20
Epoch 90/1000, LR 0.000266
Train loss: 0.0015;  Loss pred: 0.0015; Loss self: 0.0000; time: 0.21s
Val loss: 0.4088 score: 0.8571 time: 0.07s
Test loss: 0.3019 score: 0.8958 time: 0.07s
     INFO: Early stopping counter 15 of 20
Epoch 91/1000, LR 0.000266
Train loss: 0.0019;  Loss pred: 0.0019; Loss self: 0.0000; time: 0.21s
Val loss: 0.4167 score: 0.8571 time: 0.07s
Test loss: 0.3079 score: 0.8958 time: 0.07s
     INFO: Early stopping counter 16 of 20
Epoch 92/1000, LR 0.000266
Train loss: 0.0018;  Loss pred: 0.0018; Loss self: 0.0000; time: 0.21s
Val loss: 0.4233 score: 0.8571 time: 0.07s
Test loss: 0.3121 score: 0.8958 time: 0.07s
     INFO: Early stopping counter 17 of 20
Epoch 93/1000, LR 0.000265
Train loss: 0.0014;  Loss pred: 0.0014; Loss self: 0.0000; time: 0.21s
Val loss: 0.4310 score: 0.8571 time: 0.07s
Test loss: 0.3169 score: 0.8958 time: 0.07s
     INFO: Early stopping counter 18 of 20
Epoch 94/1000, LR 0.000265
Train loss: 0.0016;  Loss pred: 0.0016; Loss self: 0.0000; time: 0.21s
Val loss: 0.4420 score: 0.8571 time: 0.07s
Test loss: 0.3247 score: 0.8958 time: 0.07s
     INFO: Early stopping counter 19 of 20
Epoch 95/1000, LR 0.000265
Train loss: 0.0014;  Loss pred: 0.0014; Loss self: 0.0000; time: 0.20s
Val loss: 0.4532 score: 0.8367 time: 0.07s
Test loss: 0.3322 score: 0.8958 time: 0.07s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 074,   Train_Loss: 0.0035,   Val_Loss: 0.3020,   Val_Precision: 1.0000,   Val_Recall: 0.8000,   Val_accuracy: 0.8889,   Val_Score: 0.8980,   Val_Loss: 0.3020,   Test_Precision: 1.0000,   Test_Recall: 0.8333,   Test_accuracy: 0.9091,   Test_Score: 0.9167,   Test_loss: 0.2126


[0.06637286301702261, 0.06604503095149994, 0.06610185001045465, 0.06626755394972861, 0.06624213804025203, 0.06666089908685535, 0.06742979201953858, 0.06574355997145176, 0.06686882290523499, 0.06606505892705172, 0.06629644602071494, 0.06518818601034582, 0.06646155891939998, 0.06606286694295704, 0.0652678869664669, 0.06637275393586606, 0.0651394120650366, 0.06572998105548322, 0.07153654005378485, 0.06536293006502092, 0.07144412607885897, 0.06531349802389741, 0.06591310200747102, 0.06575892004184425, 0.0678081710357219, 0.06617944303434342, 0.07726806495338678, 0.076911062002182, 0.07679986499715596, 0.07714617298915982, 0.07277223106939346, 0.07641657604835927, 0.07663503801450133, 0.07285562600009143, 0.07323686394374818, 0.07775542896706611, 0.07675091805867851, 0.07364030694589019, 0.08312537800520658, 0.07702338707167655, 0.07543517707381397, 0.07518171099945903, 0.08425703889224678, 0.0846936620073393, 0.08065770904067904, 0.08476921601686627, 0.10756134206894785, 0.08435409201774746, 0.09040081198327243, 0.08713996293954551, 0.08315095189027488, 0.08215527306310833, 0.0838290600804612, 0.08895794302225113, 0.08299971395172179, 0.09047198703046888, 0.08131241204682738, 0.0810748899821192, 0.07812981202732772, 0.0777679969323799, 0.07343696197494864, 0.07907146797515452, 0.07788885897025466, 0.07859994599130005, 0.08153688791207969, 0.07841119996737689, 0.07744228094816208, 0.07864545995835215, 0.07846716209314764, 0.10213577700778842, 0.08119238098151982, 0.07871469005476683, 0.07886659097857773, 0.0759620659518987, 0.07822217198554426, 0.08139933703932911, 0.08386627200525254, 0.08456716896034777, 0.08036874199751765, 0.08006963401567191, 0.0844101719558239, 0.08132505300454795, 0.08614958100952208, 0.08643141493666917, 0.08536995807662606, 0.08107691607438028, 0.0844919040100649, 0.08481291704811156, 0.0852338169934228, 0.08129130396991968, 0.08506144594866782, 0.08533294894732535, 0.08527389890514314, 0.08539619599469006, 0.08541544200852513, 0.0860082630533725, 0.08920591499190778, 0.0854052749928087, 0.08145413699094206, 0.08575887710321695, 0.085284554050304, 0.08553031692281365, 0.08166616305243224, 0.08603184891398996, 0.08505994407460093, 0.08563303807750344, 0.08532466506585479, 0.08082952699624002, 0.08113259496167302, 0.08628686307929456, 0.08465460990555584, 0.08100075903348625, 0.08670016494579613, 0.07138835894875228, 0.0710872079944238, 0.07098552491515875, 0.07123115810099989, 0.07183427305426449, 0.07156145107001066, 0.07107510208152235, 0.07126928691286594, 0.07124888303223997, 0.07135680899955332, 0.07055104605387896, 0.07082148711197078, 0.07093510194681585, 0.07074776699300855, 0.07119820802472532, 0.07228124199900776, 0.07030110992491245, 0.07091710204258561, 0.07216935500036925, 0.07114364195149392, 0.07110075699165463, 0.07107019901741296, 0.07081565505359322, 0.0709816759917885, 0.07115414901636541, 0.07050854596309364, 0.07088466396089643, 0.07073935098014772, 0.07126081199385226, 0.07183411600999534, 0.07047368097119033, 0.07113309600390494, 0.07095606601797044, 0.07111487595830113, 0.07899525098036975, 0.07124295900575817, 0.07126247393898666, 0.07151354104280472, 0.07127812097314745, 0.07108052994590253, 0.07153845101129264, 0.07118655100930482, 0.07105861895252019, 0.07126015203539282, 0.07187619898468256, 0.07182290696073323, 0.07218669494614005, 0.07200449903029948, 0.07277679303660989, 0.07166058209259063, 0.07225203898269683, 0.07140002097003162, 0.07052444096188992, 0.0714520710753277, 0.07115807302761823, 0.07103389000985771, 0.07106248789932579, 0.07147569197695702, 0.07144318602513522, 0.07169485394842923, 0.07191383105237037, 0.07158741191960871, 0.07194081298075616, 0.0749607840552926, 0.07150757196359336, 0.07216692599467933, 0.0713718410115689, 0.07144875999074429, 0.07234764099121094, 0.07273131294641644, 0.07149475999176502, 0.07194410904776305, 0.07175613206345588, 0.07178994105197489, 0.07179843995254487, 0.07177501008845866, 0.0728844350669533, 0.07225519604980946, 0.07687532599084079, 0.07643504301086068, 0.07218765001744032, 0.07268295600079, 0.07255483593326062, 0.07238225894980133, 0.07259739807341248, 0.07228770398069173, 0.07193858898244798, 0.07174418505746871, 0.07214143499732018, 0.07245809596497566, 0.07711036002729088, 0.07227086299099028, 0.07514524296857417, 0.07209758902899921, 0.07267377595417202]
[0.001354548224837196, 0.0013478577745204068, 0.001349017347152136, 0.001352399060198543, 0.0013518803681684087, 0.0013604265119766398, 0.0013761182044803792, 0.0013417053055398318, 0.0013646698552088775, 0.0013482665087153412, 0.0013529886943003048, 0.001330371143068282, 0.001356358345293877, 0.001348221774346062, 0.0013319976931932022, 0.001354545998691144, 0.0013293757564293183, 0.00134142818480578, 0.001459929388852752, 0.001333937348265733, 0.0014580433893644688, 0.0013329285310999472, 0.0013451653470912454, 0.0013420187763641684, 0.0013838402252188142, 0.0013506008782519065, 0.0015768992847629956, 0.0015696135102486123, 0.0015673441836154278, 0.0015744116936563229, 0.0014851475728447645, 0.0015595219601705975, 0.0015639803676428844, 0.0014868495102059475, 0.001494629876403024, 0.0015868454891237982, 0.001566345266503643, 0.0015028634070589834, 0.0016964362858205425, 0.001571905858605644, 0.001539493409669673, 0.001534320632642021, 0.00171953140596422, 0.0017284420817824347, 0.0016460756947077354, 0.0017299840003442094, 0.0021951294299785277, 0.0017215120819948462, 0.001844914530270866, 0.0017783665906029697, 0.0016969582018423446, 0.001676638225777721, 0.0017107971444992082, 0.0018154682249439005, 0.0016938717133004445, 0.001846367082254467, 0.0016594369805474974, 0.0016545895914718205, 0.001594485959741382, 0.0015871019782118347, 0.0014987135096928294, 0.001613703428064378, 0.0015895685504133605, 0.0016040805304346948, 0.0016640181206546876, 0.0016002285707627936, 0.0015804547132277976, 0.0016050093869051458, 0.0016013706549621966, 0.0020844036124038454, 0.0016569873669697922, 0.0016064222460156496, 0.0016095222648689334, 0.0015502462439162998, 0.0015963708568478422, 0.0016612109599863083, 0.0017115565715357661, 0.0017258605910275056, 0.001640178408112605, 0.001634074163585141, 0.0017226565705270184, 0.001659694959276489, 0.0017581547144800425, 0.0017639064272789626, 0.0017422440423801237, 0.001654630940293475, 0.0017243245716339775, 0.0017308758581247256, 0.001739465652926996, 0.0016590062034677487, 0.001735947876503425, 0.0017414887540270479, 0.0017402836511253702, 0.0017427795100957155, 0.001743172285888268, 0.0017552706745586225, 0.001820528877385873, 0.0017429647957716062, 0.0016623293263457563, 0.0017501811653717744, 0.0017405011030674285, 0.001745516671894156, 0.0016666563888251477, 0.0017557520186528563, 0.001735917226012264, 0.0017476130219898662, 0.0017413196952215263, 0.0016495821835967352, 0.001655767244115776, 0.0017609563893733584, 0.0017276451001133845, 0.001653076714969107, 0.0017693911213427782, 0.001487257478099006, 0.0014809834998838294, 0.0014788651023991406, 0.0014839824604374978, 0.0014965473552971769, 0.0014908635639585555, 0.001480731293365049, 0.001484776810684707, 0.0014843517298383329, 0.0014866001874906942, 0.0014698134594558117, 0.001475447648166058, 0.001477814623891997, 0.0014739118123543449, 0.0014832960005151108, 0.0015058592083126616, 0.0014646064567690094, 0.0014774396258872002, 0.0015035282291743595, 0.00148215920732279, 0.0014812657706594716, 0.0014806291461961034, 0.0014753261469498586, 0.001478784916495594, 0.0014823781045076128, 0.001468928040897784, 0.0014767638325186756, 0.0014737364787530776, 0.001484600249871922, 0.0014965440835415695, 0.0014682016868997987, 0.0014819395000813529, 0.0014782513753743842, 0.0014815599157979402, 0.0016457343954243697, 0.0014842283126199618, 0.0014846348737288888, 0.0014898654383917649, 0.0014849608536072385, 0.0014808443738729693, 0.0014903843960685965, 0.0014830531460271839, 0.0014803878948441707, 0.0014845865007373504, 0.0014974208121808867, 0.0014963105616819423, 0.0015038894780445844, 0.001500093729797906, 0.001516183188262706, 0.0014929287935956381, 0.0015052508121395174, 0.0014875004368756588, 0.00146925918670604, 0.001488584814069327, 0.0014824598547420464, 0.0014798727085387025, 0.0014804684979026206, 0.0014890769161866046, 0.0014883997088569838, 0.0014936427905922756, 0.0014982048135910493, 0.0014914044149918482, 0.0014987669370990868, 0.0015616830011519294, 0.0014897410825748618, 0.0015034776248891528, 0.0014869133544076856, 0.0014885158331405062, 0.0015072425206502278, 0.0015152356863836758, 0.0014894741664951046, 0.0014988356051617302, 0.0014949194179886642, 0.0014956237719161436, 0.0014958008323446847, 0.001495312710176222, 0.0015184257305615272, 0.0015053165843710303, 0.00160156929147585, 0.0015923967293929309, 0.00150390937536334, 0.0015142282500164583, 0.0015115590819429296, 0.0015079637281208609, 0.0015124457931960933, 0.0015059938329310778, 0.0014987206038009997, 0.0014946705220305982, 0.0015029465624441702, 0.0015095436659369927, 0.0016064658339018933, 0.0015056429789789643, 0.0015655258951786284, 0.0015020331047708169, 0.0015140369990452502]
[738.2535236943598, 741.9180412828191, 741.2803120071552, 739.4267191025643, 739.7104237521011, 735.0635930690912, 726.681760872133, 745.3201503124805, 732.7779654420074, 741.6931248650703, 739.1044760482258, 751.669942038629, 737.2682915762455, 741.7177344469438, 750.7520509308819, 738.2547369866134, 752.2327642607112, 745.4741232716725, 684.9646343415446, 749.6603954451919, 685.8506456628012, 750.227770407757, 743.4030338072394, 745.1460572773993, 722.6267756755509, 740.4111874222294, 634.1559094246769, 637.0995111029652, 638.021954879928, 635.1578840713878, 673.333760418518, 641.2221344357403, 639.394215355226, 672.5630221053691, 669.061963625804, 630.1810773978793, 638.4288454053142, 665.3964660414096, 589.4710036317774, 636.1704134667759, 649.564326627789, 651.7542544403196, 581.5537864161627, 578.555689276416, 607.5054769443955, 578.0400280008562, 455.55400348752187, 580.8846829824305, 542.0305296490794, 562.3137576268467, 589.2897060837005, 596.4315882969581, 584.5228367462141, 550.822088902658, 590.3634803910492, 541.6041098279176, 602.6140261560703, 604.3794818692543, 627.1613706540227, 630.0792348117957, 667.2389309448182, 619.6925547834342, 629.1015255302794, 623.4100975772126, 600.9549941719158, 624.9107272989896, 632.7292972271746, 623.0493155733168, 624.4650461785856, 479.75353431994233, 603.5049028941876, 622.5013395327807, 621.302371409837, 645.0588117367446, 626.4208568518831, 601.9705047023299, 584.2634807581662, 579.4210756064844, 609.689772194188, 611.9673282184517, 580.4987582023192, 602.5203573769545, 568.7781580108214, 566.9234969241651, 573.9724032196285, 604.3643785741333, 579.9372208982649, 577.7421848632316, 574.8891898597144, 602.7705007430011, 576.0541624177199, 574.2213365935228, 574.6189705071015, 573.7960506232247, 573.6667615102831, 569.7127027154705, 549.290929916979, 573.7350532988261, 601.5655166225497, 571.3694215121893, 574.5471796815398, 572.8962754133107, 600.0037000457639, 569.5565144599831, 576.0643336071919, 572.2090573926832, 574.2770857896846, 606.2141128486296, 603.949621273023, 567.8732341326482, 578.8225833733852, 604.9326029123145, 565.1661681455184, 672.3785321141486, 675.2269691582935, 676.1941967375625, 673.8624118948055, 668.2047156479222, 670.7521896536194, 675.3419776301487, 673.5018979309413, 673.6947718644249, 672.6758199108997, 680.3584451936119, 677.760407997514, 676.674857477309, 678.4666434029425, 674.174271118324, 664.0727064520961, 682.7772712446229, 676.8466084693657, 665.1022445711814, 674.6913523590292, 675.0982975558748, 675.3885688182678, 677.8162252919026, 676.2308628152548, 674.5917232312065, 680.7685415201256, 677.1563455034396, 678.547362039987, 673.5819962890826, 668.2061764819525, 681.1053337716589, 674.7913797729959, 676.4749329231901, 674.9642652564739, 607.6314639715235, 673.7507912342668, 673.5662873716191, 671.2015556783772, 673.4184255233524, 675.2904070429906, 670.967840671068, 674.2846692168848, 675.4986334883956, 673.5882345039035, 667.8149467841116, 668.3104601466825, 664.9424805473332, 666.6250115815902, 659.5509089807505, 669.8243106367814, 664.3411130791092, 672.2687101191021, 680.6151079728273, 671.7789880351605, 674.5545228771161, 675.7337940149246, 675.461856443889, 671.5569821342152, 671.8625340016694, 669.504118587463, 667.4654833093871, 670.5089444203273, 667.2151454952251, 640.3348177974528, 671.2575840840776, 665.1246306866237, 672.5341439941211, 671.8101196748282, 663.4632358756689, 659.9633370480083, 671.3778744837913, 667.1845775188241, 668.9323772016071, 668.617348010478, 668.538202664648, 668.7564368272843, 658.5768272184055, 664.3120858313217, 624.3875961673176, 627.9842086721881, 664.9336830940382, 660.4024195091664, 661.5685830252952, 663.1459241039859, 661.1807209875633, 664.0133433041524, 667.2357726075408, 669.0437693528878, 665.3596508273373, 662.451853871539, 622.4844493400349, 664.1680756736497, 638.7629888970305, 665.764287633715, 660.4858405908169]
Elapsed: 0.07552311383747459~0.006963949915712116
Time per graph: 0.0015552443084780694~0.00013551650179571224
Speed: 647.6187026061335~53.655581811522836
Total Time: 0.0735
best val loss: 0.3020109534263611 test_score: 0.9167

Testing...
Test loss: 0.3556 score: 0.9167 time: 0.07s
test Score 0.9167
Epoch Time List: [0.3329930769978091, 0.33251633401960135, 0.34257458499632776, 0.33031432307325304, 0.35055711888708174, 0.337276719044894, 0.3305680660996586, 0.34174626995809376, 0.3409770530415699, 0.32855830783955753, 0.3451794661814347, 0.3380931280553341, 0.3363231299445033, 0.33736530295573175, 0.3342360380338505, 0.33515822794288397, 0.3367356410017237, 0.3390597610268742, 0.3379565250361338, 0.33875367511063814, 0.33270884514786303, 0.3405816978774965, 0.335437549976632, 0.3339052728842944, 0.33886442391667515, 0.3399042470846325, 0.3296499880962074, 0.33214695402421057, 0.33356876706238836, 0.3274783940287307, 0.3222860590321943, 0.3306766060413793, 0.3280335471499711, 0.32460675516631454, 0.3279974280158058, 0.3322980930097401, 0.32720910594798625, 0.3296917040133849, 0.33436930796597153, 0.3555102900136262, 0.3327696640044451, 0.3414842280326411, 0.37110472994390875, 0.3723056009039283, 0.36712048295885324, 0.3776683211326599, 0.40661353699397296, 0.3785742500331253, 0.4051880099577829, 0.3852372660767287, 0.3756922190077603, 0.3714109939755872, 0.3735179001232609, 0.38444136595353484, 0.3858973318710923, 0.3844154089456424, 0.34921101992949843, 0.34556887799408287, 0.34734118916094303, 0.3387654221151024, 0.3319628610042855, 0.3392228650627658, 0.43313247500918806, 0.3405794820282608, 0.34123500308487564, 0.3466344961198047, 0.38888249022420496, 0.34327571000903845, 0.33521328098140657, 0.3652929400559515, 0.34478886099532247, 0.4137031289283186, 0.34343981195706874, 0.3415254969149828, 0.34285065496806055, 0.3979953197995201, 0.37169940408784896, 0.3756842479342595, 0.3674900059122592, 0.3600947289960459, 0.3698243440594524, 0.37023517105262727, 0.376137105980888, 0.3825250160880387, 0.36653539910912514, 0.36595754395239055, 0.36462677002418786, 0.3646637520287186, 0.3658248089486733, 0.361139886896126, 0.3679056051187217, 0.36499944294337183, 0.3666828400455415, 0.3652493041008711, 0.36480168788693845, 0.3707028260687366, 0.38114632200449705, 0.3696644080337137, 0.37136418907903135, 0.37651725905016065, 0.3715075639775023, 0.37203725101426244, 0.3680309629999101, 0.3712207699427381, 0.37149085209239274, 0.37126327713485807, 0.3713603609940037, 0.36632128106430173, 0.3718069239985198, 0.37079738604370505, 0.3731387819861993, 0.36442163900937885, 0.36728859203867614, 0.3328080060891807, 0.33849878306500614, 0.33553619496524334, 0.3368281130678952, 0.33772193908225745, 0.34017729095648974, 0.3366765729151666, 0.3364818108966574, 0.3354269949486479, 0.3335896469652653, 0.3335898348595947, 0.34020849992521107, 0.3352627899730578, 0.3356142790289596, 0.3350048859138042, 0.34098011383321136, 0.33497095003258437, 0.3366056599188596, 0.355088610900566, 0.3380498879123479, 0.33400508400518447, 0.33581979910377413, 0.3387194509850815, 0.3309306910960004, 0.3395558160264045, 0.33418282598722726, 0.3319519639480859, 0.3344273869879544, 0.3352359780110419, 0.33309983497019857, 0.33580889203585684, 0.3347032971214503, 0.3373220929643139, 0.3381693920819089, 0.3475185070419684, 0.35096782306209207, 0.33831688400823623, 0.3380406459327787, 0.33667921903543174, 0.3365806699730456, 0.33861002803314477, 0.33694224595092237, 0.33733048604335636, 0.3340680830879137, 0.33779499493539333, 0.3420457710744813, 0.34297085797879845, 0.3432681909762323, 0.3435113310115412, 0.34910008893348277, 0.33952431101351976, 0.33772152103483677, 0.33622972993180156, 0.33829074690584093, 0.33860524697229266, 0.3397652000421658, 0.33689276187215, 0.3382954209810123, 0.3368762119207531, 0.3394239729968831, 0.3425861250143498, 0.33896605111658573, 0.3391180170001462, 0.3469683170551434, 0.34045886900275946, 0.3393182660220191, 0.33888887194916606, 0.3381747860694304, 0.337591465911828, 0.3389790439978242, 0.33987639495171607, 0.33851094695273787, 0.33735971385613084, 0.3395803280873224, 0.3391280029900372, 0.3396935750497505, 0.339819087064825, 0.3405226350296289, 0.3597701371181756, 0.36399709002580494, 0.34854467294644564, 0.34005948703270406, 0.3433084288844839, 0.34187013492919505, 0.3439270370872691, 0.3411508511053398, 0.3409745949320495, 0.3402887328993529, 0.3435142240487039, 0.344830053858459, 0.3443382990080863, 0.3441549789858982, 0.3487173708854243, 0.3397277209442109, 0.3357648840174079]
Total Epoch List: [26, 87, 95]
Total Time List: [0.06655377801507711, 0.08728704391978681, 0.0734512999188155]
========================training times:2========================
========================k_idx:0========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: GPSPerformer
GPS(
  (x_lin): Linear(in_features=14887, out_features=64, bias=True)
  (pe_lin): Linear(in_features=20, out_features=8, bias=True)
  (pe_norm): BatchNorm1d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (convs): ModuleList(
    (0-1): 2 x GPSConv(64, conv=GINEConv(nn=Sequential(
      (0): Linear(in_features=64, out_features=64, bias=True)
      (1): ReLU()
      (2): Linear(in_features=64, out_features=64, bias=True)
    )), heads=1, attn_type=performer)
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=32, bias=True)
    (1): ReLU()
    (2): Linear(in_features=32, out_features=16, bias=True)
    (3): ReLU()
    (4): Linear(in_features=16, out_features=2, bias=True)
  )
)
Total number of parameters: 1039522
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7471244afbb0>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.8492;  Loss pred: 0.8492; Loss self: 0.0000; time: 0.21s
Val loss: 0.9095 score: 0.4898 time: 0.07s
Test loss: 0.8207 score: 0.5102 time: 0.06s
Epoch 2/1000, LR 0.000000
Train loss: 0.7716;  Loss pred: 0.7716; Loss self: 0.0000; time: 0.21s
Val loss: 0.8755 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.8102 score: 0.4898 time: 0.06s
Epoch 3/1000, LR 0.000030
Train loss: 0.8204;  Loss pred: 0.8204; Loss self: 0.0000; time: 0.21s
Val loss: 0.8325 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7999 score: 0.4898 time: 0.06s
Epoch 4/1000, LR 0.000060
Train loss: 0.7709;  Loss pred: 0.7709; Loss self: 0.0000; time: 0.21s
Val loss: 0.7936 score: 0.5714 time: 0.07s
Test loss: 0.8058 score: 0.5306 time: 0.06s
Epoch 5/1000, LR 0.000090
Train loss: 0.7750;  Loss pred: 0.7750; Loss self: 0.0000; time: 0.21s
Val loss: 0.7978 score: 0.5714 time: 0.07s
Test loss: 0.8541 score: 0.5510 time: 0.06s
     INFO: Early stopping counter 1 of 20
Epoch 6/1000, LR 0.000120
Train loss: 0.6394;  Loss pred: 0.6394; Loss self: 0.0000; time: 0.21s
Val loss: 0.8575 score: 0.5510 time: 0.07s
Test loss: 0.9077 score: 0.5510 time: 0.06s
     INFO: Early stopping counter 2 of 20
Epoch 7/1000, LR 0.000150
Train loss: 0.6267;  Loss pred: 0.6267; Loss self: 0.0000; time: 0.21s
Val loss: 0.9170 score: 0.5510 time: 0.07s
Test loss: 0.9448 score: 0.5510 time: 0.06s
     INFO: Early stopping counter 3 of 20
Epoch 8/1000, LR 0.000180
Train loss: 0.5860;  Loss pred: 0.5860; Loss self: 0.0000; time: 0.21s
Val loss: 0.9572 score: 0.5510 time: 0.07s
Test loss: 0.9722 score: 0.5510 time: 0.06s
     INFO: Early stopping counter 4 of 20
Epoch 9/1000, LR 0.000210
Train loss: 0.6609;  Loss pred: 0.6609; Loss self: 0.0000; time: 0.21s
Val loss: 0.9951 score: 0.5510 time: 0.07s
Test loss: 0.9976 score: 0.5510 time: 0.06s
     INFO: Early stopping counter 5 of 20
Epoch 10/1000, LR 0.000240
Train loss: 0.5913;  Loss pred: 0.5913; Loss self: 0.0000; time: 0.22s
Val loss: 1.0462 score: 0.5714 time: 0.09s
Test loss: 1.0288 score: 0.5714 time: 0.06s
     INFO: Early stopping counter 6 of 20
Epoch 11/1000, LR 0.000270
Train loss: 0.5431;  Loss pred: 0.5431; Loss self: 0.0000; time: 0.20s
Val loss: 1.0909 score: 0.5714 time: 0.07s
Test loss: 1.0554 score: 0.5306 time: 0.06s
     INFO: Early stopping counter 7 of 20
Epoch 12/1000, LR 0.000270
Train loss: 0.5934;  Loss pred: 0.5934; Loss self: 0.0000; time: 0.20s
Val loss: 1.1068 score: 0.5714 time: 0.07s
Test loss: 1.0635 score: 0.5510 time: 0.06s
     INFO: Early stopping counter 8 of 20
Epoch 13/1000, LR 0.000270
Train loss: 0.5824;  Loss pred: 0.5824; Loss self: 0.0000; time: 0.21s
Val loss: 1.1137 score: 0.5714 time: 0.07s
Test loss: 1.0636 score: 0.5714 time: 0.06s
     INFO: Early stopping counter 9 of 20
Epoch 14/1000, LR 0.000270
Train loss: 0.4957;  Loss pred: 0.4957; Loss self: 0.0000; time: 0.20s
Val loss: 1.1155 score: 0.5714 time: 0.08s
Test loss: 1.0584 score: 0.5714 time: 0.06s
     INFO: Early stopping counter 10 of 20
Epoch 15/1000, LR 0.000270
Train loss: 0.5457;  Loss pred: 0.5457; Loss self: 0.0000; time: 0.24s
Val loss: 1.1206 score: 0.5714 time: 0.07s
Test loss: 1.0542 score: 0.5714 time: 0.06s
     INFO: Early stopping counter 11 of 20
Epoch 16/1000, LR 0.000270
Train loss: 0.4783;  Loss pred: 0.4783; Loss self: 0.0000; time: 0.21s
Val loss: 1.1226 score: 0.5714 time: 0.07s
Test loss: 1.0486 score: 0.5714 time: 0.06s
     INFO: Early stopping counter 12 of 20
Epoch 17/1000, LR 0.000270
Train loss: 0.4361;  Loss pred: 0.4361; Loss self: 0.0000; time: 0.20s
Val loss: 1.1065 score: 0.5714 time: 0.07s
Test loss: 1.0340 score: 0.5714 time: 0.06s
     INFO: Early stopping counter 13 of 20
Epoch 18/1000, LR 0.000270
Train loss: 0.4627;  Loss pred: 0.4627; Loss self: 0.0000; time: 0.21s
Val loss: 1.0769 score: 0.5918 time: 0.07s
Test loss: 1.0142 score: 0.5714 time: 0.06s
     INFO: Early stopping counter 14 of 20
Epoch 19/1000, LR 0.000270
Train loss: 0.4153;  Loss pred: 0.4153; Loss self: 0.0000; time: 0.20s
Val loss: 1.0450 score: 0.6122 time: 0.07s
Test loss: 0.9947 score: 0.5714 time: 0.06s
     INFO: Early stopping counter 15 of 20
Epoch 20/1000, LR 0.000270
Train loss: 0.3880;  Loss pred: 0.3880; Loss self: 0.0000; time: 0.29s
Val loss: 0.9978 score: 0.6327 time: 0.07s
Test loss: 0.9725 score: 0.5714 time: 0.06s
     INFO: Early stopping counter 16 of 20
Epoch 21/1000, LR 0.000270
Train loss: 0.3685;  Loss pred: 0.3685; Loss self: 0.0000; time: 0.21s
Val loss: 0.9565 score: 0.6327 time: 0.08s
Test loss: 0.9486 score: 0.5714 time: 0.06s
     INFO: Early stopping counter 17 of 20
Epoch 22/1000, LR 0.000270
Train loss: 0.3309;  Loss pred: 0.3309; Loss self: 0.0000; time: 0.20s
Val loss: 0.9120 score: 0.6327 time: 0.07s
Test loss: 0.9233 score: 0.5714 time: 0.06s
     INFO: Early stopping counter 18 of 20
Epoch 23/1000, LR 0.000270
Train loss: 0.3366;  Loss pred: 0.3366; Loss self: 0.0000; time: 0.22s
Val loss: 0.8680 score: 0.6327 time: 0.07s
Test loss: 0.8955 score: 0.5714 time: 0.06s
     INFO: Early stopping counter 19 of 20
Epoch 24/1000, LR 0.000270
Train loss: 0.3015;  Loss pred: 0.3015; Loss self: 0.0000; time: 0.21s
Val loss: 0.8223 score: 0.6531 time: 0.14s
Test loss: 0.8667 score: 0.5714 time: 0.06s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 003,   Train_Loss: 0.7709,   Val_Loss: 0.7936,   Val_Precision: 1.0000,   Val_Recall: 0.1250,   Val_accuracy: 0.2222,   Val_Score: 0.5714,   Val_Loss: 0.7936,   Test_Precision: 1.0000,   Test_Recall: 0.0800,   Test_accuracy: 0.1481,   Test_Score: 0.5306,   Test_loss: 0.8058


[0.06797837500926107, 0.06883950589690357, 0.06874882499687374, 0.06849809095729142, 0.06884398905094713, 0.06966677494347095, 0.06790481600910425, 0.068395329057239, 0.06715196603909135, 0.06827881699427962, 0.06769746902864426, 0.06811269395984709, 0.06924010894726962, 0.0696913090068847, 0.06859489297494292, 0.06833654700312763, 0.06825149094220251, 0.06832328194286674, 0.0686007720651105, 0.06977079901844263, 0.06931876204907894, 0.06971213803626597, 0.06782922602724284, 0.06744433892890811]
[0.0013873137756992057, 0.0014048878754470118, 0.001403037244834158, 0.0013979202236181923, 0.0014049793683866762, 0.001421770917213693, 0.0013858125716143725, 0.0013958230419844693, 0.0013704482865120684, 0.0013934452447812169, 0.0013815810005845769, 0.0013900549787723897, 0.0014130634479034617, 0.0014222716123854018, 0.0013998957749988352, 0.0013946234082270945, 0.0013928875702490307, 0.0013943526927115662, 0.0014000157564308267, 0.0014238938575192373, 0.001414668613246509, 0.0014226966946176728, 0.0013842699189233233, 0.0013764150801817982]
[720.8174657503133, 711.8005767412697, 712.739454124899, 715.3484033672049, 711.7542239415872, 703.3481891441021, 721.5983030339172, 716.423192569081, 729.6882413163526, 717.6457085380551, 723.8084481307128, 719.3960061084332, 707.6823064694534, 703.100583103689, 714.3388942657764, 717.0394488582714, 717.9330344811768, 717.178663065026, 714.2776753808691, 702.2995391961579, 706.8793289370504, 702.8905063062188, 722.4024638040202, 726.5250246080703]
Elapsed: 0.06855126328688736~0.0007163581194200546
Time per graph: 0.0013990053732017829~1.4619553457552107e-05
Speed: 714.8714867184044~7.459771724864753
Total Time: 0.0718
best val loss: 0.793611466884613 test_score: 0.5306

Testing...
Test loss: 0.8667 score: 0.5714 time: 0.06s
test Score 0.5714
Epoch Time List: [0.34395835804753006, 0.3419653399614617, 0.341520480113104, 0.34252021298743784, 0.34399150602985173, 0.3447178431088105, 0.34788122004829347, 0.3417081270599738, 0.33796982397325337, 0.37207021296489984, 0.33562202798202634, 0.33632486686110497, 0.34713741787709296, 0.3444195189513266, 0.37200054805725813, 0.34053318097721785, 0.33759914501570165, 0.33956276008393615, 0.33616698312107474, 0.42863247205968946, 0.3520493080141023, 0.339258449152112, 0.3488590008346364, 0.41366476111579686]
Total Epoch List: [24]
Total Time List: [0.07178474590182304]
========================k_idx:1========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: GPSPerformer
GPS(
  (x_lin): Linear(in_features=14887, out_features=64, bias=True)
  (pe_lin): Linear(in_features=20, out_features=8, bias=True)
  (pe_norm): BatchNorm1d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (convs): ModuleList(
    (0-1): 2 x GPSConv(64, conv=GINEConv(nn=Sequential(
      (0): Linear(in_features=64, out_features=64, bias=True)
      (1): ReLU()
      (2): Linear(in_features=64, out_features=64, bias=True)
    )), heads=1, attn_type=performer)
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=32, bias=True)
    (1): ReLU()
    (2): Linear(in_features=32, out_features=16, bias=True)
    (3): ReLU()
    (4): Linear(in_features=16, out_features=2, bias=True)
  )
)
Total number of parameters: 1039522
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7471244afb20>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.7020;  Loss pred: 0.7020; Loss self: 0.0000; time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6815 score: 0.5102 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6366 score: 0.4898 time: 0.07s
Epoch 2/1000, LR 0.000000
Train loss: 0.7436;  Loss pred: 0.7436; Loss self: 0.0000; time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6877 score: 0.5102 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6558 score: 0.4898 time: 0.07s
     INFO: Early stopping counter 1 of 20
Epoch 3/1000, LR 0.000030
Train loss: 0.6961;  Loss pred: 0.6961; Loss self: 0.0000; time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6953 score: 0.5102 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6735 score: 0.4898 time: 0.07s
     INFO: Early stopping counter 2 of 20
Epoch 4/1000, LR 0.000060
Train loss: 0.7177;  Loss pred: 0.7177; Loss self: 0.0000; time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6931 score: 0.5102 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6814 score: 0.4898 time: 0.07s
     INFO: Early stopping counter 3 of 20
Epoch 5/1000, LR 0.000090
Train loss: 0.6794;  Loss pred: 0.6794; Loss self: 0.0000; time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6876 score: 0.5102 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6805 score: 0.4898 time: 0.08s
     INFO: Early stopping counter 4 of 20
Epoch 6/1000, LR 0.000120
Train loss: 0.6237;  Loss pred: 0.6237; Loss self: 0.0000; time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6860 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6733 score: 0.4898 time: 0.08s
     INFO: Early stopping counter 5 of 20
Epoch 7/1000, LR 0.000150
Train loss: 0.6107;  Loss pred: 0.6107; Loss self: 0.0000; time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6843 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6638 score: 0.4898 time: 0.07s
     INFO: Early stopping counter 6 of 20
Epoch 8/1000, LR 0.000180
Train loss: 0.6356;  Loss pred: 0.6356; Loss self: 0.0000; time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6825 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6559 score: 0.4898 time: 0.07s
     INFO: Early stopping counter 7 of 20
Epoch 9/1000, LR 0.000210
Train loss: 0.5610;  Loss pred: 0.5610; Loss self: 0.0000; time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6822 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6500 score: 0.4898 time: 0.07s
     INFO: Early stopping counter 8 of 20
Epoch 10/1000, LR 0.000240
Train loss: 0.6030;  Loss pred: 0.6030; Loss self: 0.0000; time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6824 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6449 score: 0.4898 time: 0.08s
     INFO: Early stopping counter 9 of 20
Epoch 11/1000, LR 0.000270
Train loss: 0.5604;  Loss pred: 0.5604; Loss self: 0.0000; time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6824 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6411 score: 0.4898 time: 0.07s
     INFO: Early stopping counter 10 of 20
Epoch 12/1000, LR 0.000270
Train loss: 0.4811;  Loss pred: 0.4811; Loss self: 0.0000; time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6823 score: 0.5102 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6380 score: 0.4898 time: 0.07s
     INFO: Early stopping counter 11 of 20
Epoch 13/1000, LR 0.000270
Train loss: 0.4352;  Loss pred: 0.4352; Loss self: 0.0000; time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6823 score: 0.5102 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6355 score: 0.4898 time: 0.07s
     INFO: Early stopping counter 12 of 20
Epoch 14/1000, LR 0.000270
Train loss: 0.4901;  Loss pred: 0.4901; Loss self: 0.0000; time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6813 score: 0.5102 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6307 score: 0.4898 time: 0.07s
Epoch 15/1000, LR 0.000270
Train loss: 0.3964;  Loss pred: 0.3964; Loss self: 0.0000; time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6823 score: 0.5102 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6276 score: 0.4898 time: 0.07s
     INFO: Early stopping counter 1 of 20
Epoch 16/1000, LR 0.000270
Train loss: 0.3440;  Loss pred: 0.3440; Loss self: 0.0000; time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6815 score: 0.5102 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6231 score: 0.4898 time: 0.07s
     INFO: Early stopping counter 2 of 20
Epoch 17/1000, LR 0.000270
Train loss: 0.3534;  Loss pred: 0.3534; Loss self: 0.0000; time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6776 score: 0.5102 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6173 score: 0.4898 time: 0.07s
Epoch 18/1000, LR 0.000270
Train loss: 0.2978;  Loss pred: 0.2978; Loss self: 0.0000; time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6743 score: 0.5102 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6114 score: 0.4898 time: 0.07s
Epoch 19/1000, LR 0.000270
Train loss: 0.2928;  Loss pred: 0.2928; Loss self: 0.0000; time: 0.21s
Val loss: 0.6719 score: 0.4898 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6039 score: 0.4898 time: 0.07s
Epoch 20/1000, LR 0.000270
Train loss: 0.2620;  Loss pred: 0.2620; Loss self: 0.0000; time: 0.21s
Val loss: 0.6685 score: 0.4898 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.5967 score: 0.4898 time: 0.07s
Epoch 21/1000, LR 0.000270
Train loss: 0.2273;  Loss pred: 0.2273; Loss self: 0.0000; time: 0.21s
Val loss: 0.6630 score: 0.5102 time: 0.06s
Test loss: 0.5884 score: 0.5102 time: 0.07s
Epoch 22/1000, LR 0.000270
Train loss: 0.2236;  Loss pred: 0.2236; Loss self: 0.0000; time: 0.21s
Val loss: 0.6563 score: 0.5102 time: 0.06s
Test loss: 0.5797 score: 0.5102 time: 0.07s
Epoch 23/1000, LR 0.000270
Train loss: 0.2257;  Loss pred: 0.2257; Loss self: 0.0000; time: 0.20s
Val loss: 0.6491 score: 0.5102 time: 0.06s
Test loss: 0.5730 score: 0.5306 time: 0.07s
Epoch 24/1000, LR 0.000270
Train loss: 0.1724;  Loss pred: 0.1724; Loss self: 0.0000; time: 0.21s
Val loss: 0.6412 score: 0.5102 time: 0.06s
Test loss: 0.5676 score: 0.5306 time: 0.07s
Epoch 25/1000, LR 0.000270
Train loss: 0.1857;  Loss pred: 0.1857; Loss self: 0.0000; time: 0.21s
Val loss: 0.6339 score: 0.5102 time: 0.06s
Test loss: 0.5609 score: 0.5306 time: 0.07s
Epoch 26/1000, LR 0.000270
Train loss: 0.1284;  Loss pred: 0.1284; Loss self: 0.0000; time: 0.20s
Val loss: 0.6263 score: 0.5306 time: 0.06s
Test loss: 0.5530 score: 0.5510 time: 0.07s
Epoch 27/1000, LR 0.000270
Train loss: 0.1232;  Loss pred: 0.1232; Loss self: 0.0000; time: 0.21s
Val loss: 0.6175 score: 0.5510 time: 0.06s
Test loss: 0.5438 score: 0.5510 time: 0.07s
Epoch 28/1000, LR 0.000270
Train loss: 0.1297;  Loss pred: 0.1297; Loss self: 0.0000; time: 0.21s
Val loss: 0.6076 score: 0.5510 time: 0.06s
Test loss: 0.5333 score: 0.5714 time: 0.07s
Epoch 29/1000, LR 0.000270
Train loss: 0.0980;  Loss pred: 0.0980; Loss self: 0.0000; time: 0.21s
Val loss: 0.5977 score: 0.6122 time: 0.06s
Test loss: 0.5222 score: 0.5918 time: 0.07s
Epoch 30/1000, LR 0.000270
Train loss: 0.0883;  Loss pred: 0.0883; Loss self: 0.0000; time: 0.21s
Val loss: 0.5866 score: 0.6327 time: 0.06s
Test loss: 0.5116 score: 0.6531 time: 0.07s
Epoch 31/1000, LR 0.000270
Train loss: 0.0980;  Loss pred: 0.0980; Loss self: 0.0000; time: 0.20s
Val loss: 0.5760 score: 0.6327 time: 0.06s
Test loss: 0.5004 score: 0.6531 time: 0.07s
Epoch 32/1000, LR 0.000270
Train loss: 0.0684;  Loss pred: 0.0684; Loss self: 0.0000; time: 0.21s
Val loss: 0.5640 score: 0.6735 time: 0.06s
Test loss: 0.4898 score: 0.6735 time: 0.07s
Epoch 33/1000, LR 0.000270
Train loss: 0.0639;  Loss pred: 0.0639; Loss self: 0.0000; time: 0.21s
Val loss: 0.5515 score: 0.7347 time: 0.06s
Test loss: 0.4788 score: 0.7143 time: 0.07s
Epoch 34/1000, LR 0.000270
Train loss: 0.0577;  Loss pred: 0.0577; Loss self: 0.0000; time: 0.21s
Val loss: 0.5382 score: 0.8163 time: 0.06s
Test loss: 0.4686 score: 0.7959 time: 0.08s
Epoch 35/1000, LR 0.000270
Train loss: 0.0592;  Loss pred: 0.0592; Loss self: 0.0000; time: 0.21s
Val loss: 0.5238 score: 0.8571 time: 0.06s
Test loss: 0.4595 score: 0.7755 time: 0.07s
Epoch 36/1000, LR 0.000270
Train loss: 0.0482;  Loss pred: 0.0482; Loss self: 0.0000; time: 0.21s
Val loss: 0.5101 score: 0.8571 time: 0.06s
Test loss: 0.4510 score: 0.8163 time: 0.07s
Epoch 37/1000, LR 0.000270
Train loss: 0.0468;  Loss pred: 0.0468; Loss self: 0.0000; time: 0.21s
Val loss: 0.4982 score: 0.8571 time: 0.07s
Test loss: 0.4435 score: 0.7959 time: 0.08s
Epoch 38/1000, LR 0.000270
Train loss: 0.0399;  Loss pred: 0.0399; Loss self: 0.0000; time: 0.22s
Val loss: 0.4871 score: 0.8163 time: 0.07s
Test loss: 0.4370 score: 0.7959 time: 0.08s
Epoch 39/1000, LR 0.000269
Train loss: 0.0380;  Loss pred: 0.0380; Loss self: 0.0000; time: 0.22s
Val loss: 0.4773 score: 0.8163 time: 0.06s
Test loss: 0.4304 score: 0.7959 time: 0.07s
Epoch 40/1000, LR 0.000269
Train loss: 0.0368;  Loss pred: 0.0368; Loss self: 0.0000; time: 0.20s
Val loss: 0.4681 score: 0.7755 time: 0.06s
Test loss: 0.4246 score: 0.7959 time: 0.07s
Epoch 41/1000, LR 0.000269
Train loss: 0.0331;  Loss pred: 0.0331; Loss self: 0.0000; time: 0.21s
Val loss: 0.4599 score: 0.7755 time: 0.06s
Test loss: 0.4215 score: 0.7755 time: 0.07s
Epoch 42/1000, LR 0.000269
Train loss: 0.0363;  Loss pred: 0.0363; Loss self: 0.0000; time: 0.21s
Val loss: 0.4518 score: 0.7755 time: 0.06s
Test loss: 0.4180 score: 0.7755 time: 0.07s
Epoch 43/1000, LR 0.000269
Train loss: 0.0306;  Loss pred: 0.0306; Loss self: 0.0000; time: 0.20s
Val loss: 0.4442 score: 0.7959 time: 0.06s
Test loss: 0.4147 score: 0.7755 time: 0.07s
Epoch 44/1000, LR 0.000269
Train loss: 0.0266;  Loss pred: 0.0266; Loss self: 0.0000; time: 0.21s
Val loss: 0.4375 score: 0.8163 time: 0.06s
Test loss: 0.4127 score: 0.7755 time: 0.07s
Epoch 45/1000, LR 0.000269
Train loss: 0.0234;  Loss pred: 0.0234; Loss self: 0.0000; time: 0.20s
Val loss: 0.4314 score: 0.8163 time: 0.06s
Test loss: 0.4118 score: 0.7755 time: 0.07s
Epoch 46/1000, LR 0.000269
Train loss: 0.0216;  Loss pred: 0.0216; Loss self: 0.0000; time: 0.20s
Val loss: 0.4265 score: 0.8163 time: 0.06s
Test loss: 0.4118 score: 0.8163 time: 0.07s
Epoch 47/1000, LR 0.000269
Train loss: 0.0172;  Loss pred: 0.0172; Loss self: 0.0000; time: 0.20s
Val loss: 0.4216 score: 0.8163 time: 0.06s
Test loss: 0.4105 score: 0.8163 time: 0.07s
Epoch 48/1000, LR 0.000269
Train loss: 0.0187;  Loss pred: 0.0187; Loss self: 0.0000; time: 0.21s
Val loss: 0.4153 score: 0.8163 time: 0.06s
Test loss: 0.4063 score: 0.8571 time: 0.07s
Epoch 49/1000, LR 0.000269
Train loss: 0.0159;  Loss pred: 0.0159; Loss self: 0.0000; time: 0.21s
Val loss: 0.4060 score: 0.8367 time: 0.06s
Test loss: 0.3974 score: 0.8571 time: 0.07s
Epoch 50/1000, LR 0.000269
Train loss: 0.0172;  Loss pred: 0.0172; Loss self: 0.0000; time: 0.20s
Val loss: 0.3937 score: 0.8571 time: 0.06s
Test loss: 0.3838 score: 0.8571 time: 0.07s
Epoch 51/1000, LR 0.000269
Train loss: 0.0166;  Loss pred: 0.0166; Loss self: 0.0000; time: 0.20s
Val loss: 0.3816 score: 0.8571 time: 0.06s
Test loss: 0.3706 score: 0.8776 time: 0.07s
Epoch 52/1000, LR 0.000269
Train loss: 0.0161;  Loss pred: 0.0161; Loss self: 0.0000; time: 0.20s
Val loss: 0.3665 score: 0.9184 time: 0.06s
Test loss: 0.3521 score: 0.9388 time: 0.07s
Epoch 53/1000, LR 0.000269
Train loss: 0.0152;  Loss pred: 0.0152; Loss self: 0.0000; time: 0.21s
Val loss: 0.3522 score: 0.8980 time: 0.06s
Test loss: 0.3332 score: 0.9388 time: 0.07s
Epoch 54/1000, LR 0.000269
Train loss: 0.0112;  Loss pred: 0.0112; Loss self: 0.0000; time: 0.21s
Val loss: 0.3391 score: 0.9184 time: 0.06s
Test loss: 0.3146 score: 0.9388 time: 0.07s
Epoch 55/1000, LR 0.000269
Train loss: 0.0134;  Loss pred: 0.0134; Loss self: 0.0000; time: 0.21s
Val loss: 0.3266 score: 0.9184 time: 0.06s
Test loss: 0.2967 score: 0.9388 time: 0.07s
Epoch 56/1000, LR 0.000269
Train loss: 0.0118;  Loss pred: 0.0118; Loss self: 0.0000; time: 0.21s
Val loss: 0.3155 score: 0.9184 time: 0.06s
Test loss: 0.2779 score: 0.9592 time: 0.07s
Epoch 57/1000, LR 0.000269
Train loss: 0.0101;  Loss pred: 0.0101; Loss self: 0.0000; time: 0.20s
Val loss: 0.3055 score: 0.9184 time: 0.06s
Test loss: 0.2603 score: 0.9796 time: 0.07s
Epoch 58/1000, LR 0.000269
Train loss: 0.0120;  Loss pred: 0.0120; Loss self: 0.0000; time: 0.21s
Val loss: 0.2974 score: 0.9388 time: 0.06s
Test loss: 0.2443 score: 0.9796 time: 0.07s
Epoch 59/1000, LR 0.000268
Train loss: 0.0092;  Loss pred: 0.0092; Loss self: 0.0000; time: 0.21s
Val loss: 0.2911 score: 0.9388 time: 0.06s
Test loss: 0.2292 score: 0.9796 time: 0.07s
Epoch 60/1000, LR 0.000268
Train loss: 0.0102;  Loss pred: 0.0102; Loss self: 0.0000; time: 0.21s
Val loss: 0.2870 score: 0.9184 time: 0.06s
Test loss: 0.2126 score: 1.0000 time: 0.07s
Epoch 61/1000, LR 0.000268
Train loss: 0.0085;  Loss pred: 0.0085; Loss self: 0.0000; time: 0.21s
Val loss: 0.2850 score: 0.9184 time: 0.06s
Test loss: 0.1991 score: 1.0000 time: 0.07s
Epoch 62/1000, LR 0.000268
Train loss: 0.0059;  Loss pred: 0.0059; Loss self: 0.0000; time: 0.20s
Val loss: 0.2848 score: 0.8980 time: 0.06s
Test loss: 0.1871 score: 1.0000 time: 0.07s
Epoch 63/1000, LR 0.000268
Train loss: 0.0093;  Loss pred: 0.0093; Loss self: 0.0000; time: 0.21s
Val loss: 0.2863 score: 0.8980 time: 0.06s
Test loss: 0.1765 score: 1.0000 time: 0.07s
     INFO: Early stopping counter 1 of 20
Epoch 64/1000, LR 0.000268
Train loss: 0.0061;  Loss pred: 0.0061; Loss self: 0.0000; time: 0.21s
Val loss: 0.2875 score: 0.8980 time: 0.06s
Test loss: 0.1671 score: 1.0000 time: 0.07s
     INFO: Early stopping counter 2 of 20
Epoch 65/1000, LR 0.000268
Train loss: 0.0080;  Loss pred: 0.0080; Loss self: 0.0000; time: 0.20s
Val loss: 0.2916 score: 0.8776 time: 0.06s
Test loss: 0.1585 score: 1.0000 time: 0.07s
     INFO: Early stopping counter 3 of 20
Epoch 66/1000, LR 0.000268
Train loss: 0.0079;  Loss pred: 0.0079; Loss self: 0.0000; time: 0.21s
Val loss: 0.2949 score: 0.8776 time: 0.06s
Test loss: 0.1513 score: 1.0000 time: 0.07s
     INFO: Early stopping counter 4 of 20
Epoch 67/1000, LR 0.000268
Train loss: 0.0058;  Loss pred: 0.0058; Loss self: 0.0000; time: 0.21s
Val loss: 0.2992 score: 0.8571 time: 0.06s
Test loss: 0.1447 score: 1.0000 time: 0.07s
     INFO: Early stopping counter 5 of 20
Epoch 68/1000, LR 0.000268
Train loss: 0.0067;  Loss pred: 0.0067; Loss self: 0.0000; time: 0.21s
Val loss: 0.3040 score: 0.8571 time: 0.06s
Test loss: 0.1384 score: 0.9796 time: 0.07s
     INFO: Early stopping counter 6 of 20
Epoch 69/1000, LR 0.000268
Train loss: 0.0048;  Loss pred: 0.0048; Loss self: 0.0000; time: 0.20s
Val loss: 0.3101 score: 0.8571 time: 0.06s
Test loss: 0.1327 score: 0.9796 time: 0.07s
     INFO: Early stopping counter 7 of 20
Epoch 70/1000, LR 0.000268
Train loss: 0.0059;  Loss pred: 0.0059; Loss self: 0.0000; time: 0.21s
Val loss: 0.3161 score: 0.8571 time: 0.06s
Test loss: 0.1276 score: 0.9796 time: 0.07s
     INFO: Early stopping counter 8 of 20
Epoch 71/1000, LR 0.000268
Train loss: 0.0045;  Loss pred: 0.0045; Loss self: 0.0000; time: 0.20s
Val loss: 0.3241 score: 0.8571 time: 0.06s
Test loss: 0.1232 score: 0.9796 time: 0.07s
     INFO: Early stopping counter 9 of 20
Epoch 72/1000, LR 0.000267
Train loss: 0.0051;  Loss pred: 0.0051; Loss self: 0.0000; time: 0.20s
Val loss: 0.3304 score: 0.8571 time: 0.06s
Test loss: 0.1185 score: 0.9796 time: 0.07s
     INFO: Early stopping counter 10 of 20
Epoch 73/1000, LR 0.000267
Train loss: 0.0051;  Loss pred: 0.0051; Loss self: 0.0000; time: 0.20s
Val loss: 0.3391 score: 0.8367 time: 0.06s
Test loss: 0.1144 score: 0.9796 time: 0.07s
     INFO: Early stopping counter 11 of 20
Epoch 74/1000, LR 0.000267
Train loss: 0.0043;  Loss pred: 0.0043; Loss self: 0.0000; time: 0.20s
Val loss: 0.3489 score: 0.8367 time: 0.06s
Test loss: 0.1111 score: 0.9796 time: 0.07s
     INFO: Early stopping counter 12 of 20
Epoch 75/1000, LR 0.000267
Train loss: 0.0028;  Loss pred: 0.0028; Loss self: 0.0000; time: 0.21s
Val loss: 0.3603 score: 0.8367 time: 0.06s
Test loss: 0.1093 score: 0.9592 time: 0.07s
     INFO: Early stopping counter 13 of 20
Epoch 76/1000, LR 0.000267
Train loss: 0.0030;  Loss pred: 0.0030; Loss self: 0.0000; time: 0.20s
Val loss: 0.3713 score: 0.8367 time: 0.06s
Test loss: 0.1074 score: 0.9592 time: 0.07s
     INFO: Early stopping counter 14 of 20
Epoch 77/1000, LR 0.000267
Train loss: 0.0027;  Loss pred: 0.0027; Loss self: 0.0000; time: 0.21s
Val loss: 0.3807 score: 0.8367 time: 0.06s
Test loss: 0.1046 score: 0.9592 time: 0.07s
     INFO: Early stopping counter 15 of 20
Epoch 78/1000, LR 0.000267
Train loss: 0.0030;  Loss pred: 0.0030; Loss self: 0.0000; time: 0.21s
Val loss: 0.3903 score: 0.8367 time: 0.06s
Test loss: 0.1017 score: 0.9592 time: 0.08s
     INFO: Early stopping counter 16 of 20
Epoch 79/1000, LR 0.000267
Train loss: 0.0031;  Loss pred: 0.0031; Loss self: 0.0000; time: 0.21s
Val loss: 0.4020 score: 0.8367 time: 0.06s
Test loss: 0.1000 score: 0.9592 time: 0.07s
     INFO: Early stopping counter 17 of 20
Epoch 80/1000, LR 0.000267
Train loss: 0.0033;  Loss pred: 0.0033; Loss self: 0.0000; time: 0.21s
Val loss: 0.4107 score: 0.8367 time: 0.06s
Test loss: 0.0969 score: 0.9592 time: 0.07s
     INFO: Early stopping counter 18 of 20
Epoch 81/1000, LR 0.000267
Train loss: 0.0028;  Loss pred: 0.0028; Loss self: 0.0000; time: 0.20s
Val loss: 0.4204 score: 0.8367 time: 0.06s
Test loss: 0.0940 score: 0.9592 time: 0.07s
     INFO: Early stopping counter 19 of 20
Epoch 82/1000, LR 0.000267
Train loss: 0.0031;  Loss pred: 0.0031; Loss self: 0.0000; time: 0.21s
Val loss: 0.4295 score: 0.8367 time: 0.06s
Test loss: 0.0908 score: 0.9796 time: 0.07s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 061,   Train_Loss: 0.0059,   Val_Loss: 0.2848,   Val_Precision: 1.0000,   Val_Recall: 0.8000,   Val_accuracy: 0.8889,   Val_Score: 0.8980,   Val_Loss: 0.2848,   Test_Precision: 1.0000,   Test_Recall: 1.0000,   Test_accuracy: 1.0000,   Test_Score: 1.0000,   Test_loss: 0.1871


[0.06797837500926107, 0.06883950589690357, 0.06874882499687374, 0.06849809095729142, 0.06884398905094713, 0.06966677494347095, 0.06790481600910425, 0.068395329057239, 0.06715196603909135, 0.06827881699427962, 0.06769746902864426, 0.06811269395984709, 0.06924010894726962, 0.0696913090068847, 0.06859489297494292, 0.06833654700312763, 0.06825149094220251, 0.06832328194286674, 0.0686007720651105, 0.06977079901844263, 0.06931876204907894, 0.06971213803626597, 0.06782922602724284, 0.06744433892890811, 0.0755993080092594, 0.0757346999598667, 0.07593659590929747, 0.07468619500286877, 0.08069722098298371, 0.07998170703649521, 0.07977561594452709, 0.07967430504504591, 0.07954209996387362, 0.0798287340439856, 0.07965602108743042, 0.07410749897826463, 0.07602808892261237, 0.07522397499997169, 0.0752323369961232, 0.07530660391785204, 0.07528636092320085, 0.07617228198796511, 0.07614173204638064, 0.07509151997510344, 0.07479958003386855, 0.0750187779776752, 0.07594237104058266, 0.07600331702269614, 0.07597778190393001, 0.07644144596997648, 0.07573568203952163, 0.07624999503605068, 0.0760860099690035, 0.07513718598056585, 0.07564721000380814, 0.07570286595728248, 0.07548904803115875, 0.08255834598094225, 0.07622143696062267, 0.07607044302858412, 0.08025914093013853, 0.07998787902761251, 0.07521140901371837, 0.07568988995626569, 0.07497233408503234, 0.07479966001119465, 0.07516775093972683, 0.07512099505402148, 0.07514454901684076, 0.07480915496125817, 0.07461977296043187, 0.07685453398153186, 0.07529161195270717, 0.07484634197317064, 0.07505948899779469, 0.07512407400645316, 0.075373628991656, 0.07524750696029514, 0.0750198980094865, 0.07542888401076198, 0.0753545539919287, 0.07532995904330164, 0.07533280400093645, 0.07510094495955855, 0.07553196290973574, 0.07535725005436689, 0.07628004194702953, 0.07525699608959258, 0.07501194393262267, 0.07509127398952842, 0.07516801299061626, 0.0748055389849469, 0.07471169892232865, 0.07473339897114784, 0.07472617691382766, 0.07473796396516263, 0.07525132806040347, 0.07488805497996509, 0.07494401908479631, 0.07613072998356074, 0.07514715904835612, 0.08021794899832457, 0.07525975303724408, 0.07543846499174833, 0.07484115404076874, 0.07468306191731244]
[0.0013873137756992057, 0.0014048878754470118, 0.001403037244834158, 0.0013979202236181923, 0.0014049793683866762, 0.001421770917213693, 0.0013858125716143725, 0.0013958230419844693, 0.0013704482865120684, 0.0013934452447812169, 0.0013815810005845769, 0.0013900549787723897, 0.0014130634479034617, 0.0014222716123854018, 0.0013998957749988352, 0.0013946234082270945, 0.0013928875702490307, 0.0013943526927115662, 0.0014000157564308267, 0.0014238938575192373, 0.001414668613246509, 0.0014226966946176728, 0.0013842699189233233, 0.0013764150801817982, 0.0015428430205971307, 0.0015456061216299326, 0.0015497264471285197, 0.0015242080612830361, 0.0016468820608772185, 0.0016322797354386778, 0.001628073794786267, 0.0016260062254091002, 0.001623308162528033, 0.0016291578376323593, 0.0016256330834169472, 0.0015123979383319312, 0.0015515936514818851, 0.0015351831632647284, 0.001535353816247412, 0.0015368694677112662, 0.001536456345371446, 0.0015545363671013287, 0.0015539128989057274, 0.001532479999491907, 0.0015265220415075214, 0.0015309954689321469, 0.0015498443069506664, 0.0015510881025040028, 0.0015505669776312246, 0.0015600295095913568, 0.00154562616407187, 0.0015561223476745036, 0.0015527757136531326, 0.0015334119587870581, 0.0015438206123226151, 0.0015449564481078057, 0.0015405928169624234, 0.0016848642036926989, 0.0015555395298086259, 0.0015524580209915126, 0.0016379416516354801, 0.0016324056944410717, 0.001534926714565681, 0.0015446916317605243, 0.0015300476343884152, 0.00152652367369785, 0.001534035733463813, 0.001533081531714724, 0.0015335622248334848, 0.0015267174481889423, 0.0015228525093965689, 0.0015684598771741195, 0.0015365635092389218, 0.0015274763667994008, 0.0015318263060774427, 0.0015331443674786358, 0.0015382373263603266, 0.001535663407352962, 0.0015310183267242142, 0.0015393649798114688, 0.001537848040651606, 0.0015373461029245233, 0.0015374041632844173, 0.0015326723461134397, 0.0015414686308109335, 0.0015379030623340181, 0.0015567355499393782, 0.0015358570630529097, 0.0015308559986249525, 0.001532474979378131, 0.0015340410814411482, 0.0015266436527540184, 0.0015247285494352787, 0.0015251714075744456, 0.001525024018649544, 0.0015252645707176048, 0.0015357413889878259, 0.0015283276526523487, 0.001529469777240741, 0.0015536883670114437, 0.0015336154907827778, 0.0016371009999658077, 0.0015359133272906955, 0.0015395605100356803, 0.0015273704906279336, 0.0015241441207614784]
[720.8174657503133, 711.8005767412697, 712.739454124899, 715.3484033672049, 711.7542239415872, 703.3481891441021, 721.5983030339172, 716.423192569081, 729.6882413163526, 717.6457085380551, 723.8084481307128, 719.3960061084332, 707.6823064694534, 703.100583103689, 714.3388942657764, 717.0394488582714, 717.9330344811768, 717.178663065026, 714.2776753808691, 702.2995391961579, 706.8793289370504, 702.8905063062188, 722.4024638040202, 726.5250246080703, 648.1540809077046, 646.9953670637906, 645.2751721782221, 656.0784091105171, 607.2080228181889, 612.6400875345355, 614.2227724580995, 615.003795418066, 616.025978975345, 613.814068164992, 615.1449611852646, 661.2016418793388, 644.4986411519067, 651.3880714229532, 651.3156703150804, 650.673346702123, 650.848299733661, 643.278614230591, 643.5367134825926, 652.5370643215896, 655.0838918856664, 653.1697972283937, 645.2261014317687, 644.7087037710157, 644.9253817643423, 641.0135153545563, 646.9869773461605, 642.6229926550553, 644.0080117220235, 652.1404729300589, 647.7436510551189, 647.2674367130256, 649.100780549979, 593.5196425968993, 642.863765810585, 644.1398005476034, 610.5223583523276, 612.5928152574814, 651.4969024322164, 647.3784019016628, 653.5744231255363, 655.0831914565731, 651.8752974169813, 652.2810296211175, 652.0765729663044, 655.0000467907423, 656.6624107256786, 637.5681103183151, 650.8029079092942, 654.6746134575889, 652.8155287793081, 652.2542959503354, 650.0947434204659, 651.1843644980183, 653.1600455362362, 649.6185200487498, 650.2593062291688, 650.47161344975, 650.4470482658643, 652.4551725199494, 648.73198196315, 650.236041849307, 642.369861752654, 651.1022568807566, 653.2293049759229, 652.5392019162322, 651.8730248479097, 655.0317084121305, 655.8544472525126, 655.6640093262362, 655.7273772550355, 655.623961375777, 651.1512987607103, 654.309956549266, 653.8213535700343, 643.6297144475141, 652.0539248658649, 610.8358616975287, 651.0784054227654, 649.5360159483595, 654.7199950084668, 656.1059327515494]
Elapsed: 0.07431486243588568~0.0034928479242434095
Time per graph: 0.00151662984563032~7.128261069884508e-05
Speed: 660.8571013294138~31.97426304740588
Total Time: 0.0752
best val loss: 0.2847518026828766 test_score: 1.0000

Testing...
Test loss: 0.2443 score: 0.9796 time: 0.07s
test Score 0.9796
Epoch Time List: [0.34395835804753006, 0.3419653399614617, 0.341520480113104, 0.34252021298743784, 0.34399150602985173, 0.3447178431088105, 0.34788122004829347, 0.3417081270599738, 0.33796982397325337, 0.37207021296489984, 0.33562202798202634, 0.33632486686110497, 0.34713741787709296, 0.3444195189513266, 0.37200054805725813, 0.34053318097721785, 0.33759914501570165, 0.33956276008393615, 0.33616698312107474, 0.42863247205968946, 0.3520493080141023, 0.339258449152112, 0.3488590008346364, 0.41366476111579686, 0.33745495695620775, 0.3431598289171234, 0.33719975396525115, 0.34187777200713754, 0.350027889944613, 0.36507055803667754, 0.3611515420489013, 0.3616012280108407, 0.3554594749584794, 0.3602773539023474, 0.3607961292145774, 0.3472133668838069, 0.3414324780460447, 0.34345835295971483, 0.3423060328932479, 0.34115351689979434, 0.33888053603004664, 0.3376948400400579, 0.34134428785182536, 0.3445966640720144, 0.3382029120111838, 0.3375820239307359, 0.3336589310783893, 0.34070098807569593, 0.3426734149688855, 0.3382342930417508, 0.3442444058600813, 0.3435642608674243, 0.3397261860081926, 0.3414603980490938, 0.33850749500561506, 0.34107079287059605, 0.3418189148651436, 0.3479612459195778, 0.3458137889392674, 0.3438127200352028, 0.35180812212638557, 0.3614295670995489, 0.3528044780250639, 0.3387752769049257, 0.3393392199650407, 0.33860314602497965, 0.33792925404850394, 0.34079561301041394, 0.3379129230743274, 0.3369065591832623, 0.33644409105181694, 0.3411804800853133, 0.3451873919693753, 0.3377760130679235, 0.33663218293804675, 0.3378968130564317, 0.33949685213156044, 0.3395364049356431, 0.3390443050302565, 0.33943146001547575, 0.33380966796539724, 0.3408112700562924, 0.3396321301115677, 0.33816996589303017, 0.33914696401916444, 0.338740587932989, 0.34208616300020367, 0.3432482210919261, 0.3364305709255859, 0.34051851893309504, 0.33871718402951956, 0.33795915090013295, 0.33757328998763114, 0.3377578629879281, 0.33598935091868043, 0.33597474300768226, 0.33640410704538226, 0.3365022779908031, 0.3408027469413355, 0.3388106032507494, 0.3401689170859754, 0.3484773619566113, 0.3383021322079003, 0.34153249103110284, 0.3377497820183635, 0.33819878206122667]
Total Epoch List: [24, 82]
Total Time List: [0.07178474590182304, 0.0752365879015997]
========================k_idx:2========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: GPSPerformer
GPS(
  (x_lin): Linear(in_features=14887, out_features=64, bias=True)
  (pe_lin): Linear(in_features=20, out_features=8, bias=True)
  (pe_norm): BatchNorm1d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (convs): ModuleList(
    (0-1): 2 x GPSConv(64, conv=GINEConv(nn=Sequential(
      (0): Linear(in_features=64, out_features=64, bias=True)
      (1): ReLU()
      (2): Linear(in_features=64, out_features=64, bias=True)
    )), heads=1, attn_type=performer)
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=32, bias=True)
    (1): ReLU()
    (2): Linear(in_features=32, out_features=16, bias=True)
    (3): ReLU()
    (4): Linear(in_features=16, out_features=2, bias=True)
  )
)
Total number of parameters: 1039522
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7471244ac070>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.8393;  Loss pred: 0.8393; Loss self: 0.0000; time: 0.21s
Val loss: 0.5828 score: 0.5510 time: 0.07s
Test loss: 0.5887 score: 0.5625 time: 0.07s
Epoch 2/1000, LR 0.000000
Train loss: 0.7614;  Loss pred: 0.7614; Loss self: 0.0000; time: 0.21s
Val loss: 0.5900 score: 0.6327 time: 0.07s
Test loss: 0.5975 score: 0.6458 time: 0.07s
     INFO: Early stopping counter 1 of 20
Epoch 3/1000, LR 0.000030
Train loss: 0.7829;  Loss pred: 0.7829; Loss self: 0.0000; time: 0.21s
Val loss: 0.5975 score: 0.6531 time: 0.07s
Test loss: 0.6093 score: 0.6875 time: 0.07s
     INFO: Early stopping counter 2 of 20
Epoch 4/1000, LR 0.000060
Train loss: 0.7569;  Loss pred: 0.7569; Loss self: 0.0000; time: 0.21s
Val loss: 0.6104 score: 0.6735 time: 0.07s
Test loss: 0.6219 score: 0.6875 time: 0.07s
     INFO: Early stopping counter 3 of 20
Epoch 5/1000, LR 0.000090
Train loss: 0.7992;  Loss pred: 0.7992; Loss self: 0.0000; time: 0.21s
Val loss: 0.6319 score: 0.6122 time: 0.07s
Test loss: 0.6508 score: 0.5833 time: 0.07s
     INFO: Early stopping counter 4 of 20
Epoch 6/1000, LR 0.000120
Train loss: 0.7586;  Loss pred: 0.7586; Loss self: 0.0000; time: 0.22s
Val loss: 0.6530 score: 0.6531 time: 0.07s
Test loss: 0.6791 score: 0.6042 time: 0.07s
     INFO: Early stopping counter 5 of 20
Epoch 7/1000, LR 0.000150
Train loss: 0.6782;  Loss pred: 0.6782; Loss self: 0.0000; time: 0.22s
Val loss: 0.6734 score: 0.6531 time: 0.07s
Test loss: 0.7043 score: 0.6042 time: 0.07s
     INFO: Early stopping counter 6 of 20
Epoch 8/1000, LR 0.000180
Train loss: 0.6486;  Loss pred: 0.6486; Loss self: 0.0000; time: 0.22s
Val loss: 0.6958 score: 0.6327 time: 0.07s
Test loss: 0.7307 score: 0.6042 time: 0.07s
     INFO: Early stopping counter 7 of 20
Epoch 9/1000, LR 0.000210
Train loss: 0.6186;  Loss pred: 0.6186; Loss self: 0.0000; time: 0.21s
Val loss: 0.7195 score: 0.6122 time: 0.07s
Test loss: 0.7564 score: 0.6042 time: 0.07s
     INFO: Early stopping counter 8 of 20
Epoch 10/1000, LR 0.000240
Train loss: 0.5945;  Loss pred: 0.5945; Loss self: 0.0000; time: 0.22s
Val loss: 0.7369 score: 0.6122 time: 0.07s
Test loss: 0.7756 score: 0.6042 time: 0.07s
     INFO: Early stopping counter 9 of 20
Epoch 11/1000, LR 0.000270
Train loss: 0.5597;  Loss pred: 0.5597; Loss self: 0.0000; time: 0.22s
Val loss: 0.7482 score: 0.5918 time: 0.07s
Test loss: 0.7819 score: 0.5833 time: 0.07s
     INFO: Early stopping counter 10 of 20
Epoch 12/1000, LR 0.000270
Train loss: 0.5430;  Loss pred: 0.5430; Loss self: 0.0000; time: 0.22s
Val loss: 0.7472 score: 0.5918 time: 0.07s
Test loss: 0.7795 score: 0.5833 time: 0.07s
     INFO: Early stopping counter 11 of 20
Epoch 13/1000, LR 0.000270
Train loss: 0.4772;  Loss pred: 0.4772; Loss self: 0.0000; time: 0.22s
Val loss: 0.7355 score: 0.6122 time: 0.07s
Test loss: 0.7684 score: 0.5833 time: 0.07s
     INFO: Early stopping counter 12 of 20
Epoch 14/1000, LR 0.000270
Train loss: 0.4813;  Loss pred: 0.4813; Loss self: 0.0000; time: 0.22s
Val loss: 0.7155 score: 0.6122 time: 0.07s
Test loss: 0.7468 score: 0.5833 time: 0.07s
     INFO: Early stopping counter 13 of 20
Epoch 15/1000, LR 0.000270
Train loss: 0.4310;  Loss pred: 0.4310; Loss self: 0.0000; time: 0.22s
Val loss: 0.6965 score: 0.6327 time: 0.07s
Test loss: 0.7247 score: 0.6250 time: 0.07s
     INFO: Early stopping counter 14 of 20
Epoch 16/1000, LR 0.000270
Train loss: 0.4599;  Loss pred: 0.4599; Loss self: 0.0000; time: 0.21s
Val loss: 0.6888 score: 0.6327 time: 0.07s
Test loss: 0.7151 score: 0.6250 time: 0.07s
     INFO: Early stopping counter 15 of 20
Epoch 17/1000, LR 0.000270
Train loss: 0.3916;  Loss pred: 0.3916; Loss self: 0.0000; time: 0.20s
Val loss: 0.6915 score: 0.6327 time: 0.07s
Test loss: 0.7149 score: 0.6250 time: 0.07s
     INFO: Early stopping counter 16 of 20
Epoch 18/1000, LR 0.000270
Train loss: 0.3551;  Loss pred: 0.3551; Loss self: 0.0000; time: 0.21s
Val loss: 0.6867 score: 0.6327 time: 0.07s
Test loss: 0.7077 score: 0.6250 time: 0.07s
     INFO: Early stopping counter 17 of 20
Epoch 19/1000, LR 0.000270
Train loss: 0.3504;  Loss pred: 0.3504; Loss self: 0.0000; time: 0.21s
Val loss: 0.6800 score: 0.6327 time: 0.07s
Test loss: 0.6985 score: 0.6458 time: 0.07s
     INFO: Early stopping counter 18 of 20
Epoch 20/1000, LR 0.000270
Train loss: 0.3340;  Loss pred: 0.3340; Loss self: 0.0000; time: 0.22s
Val loss: 0.6720 score: 0.6531 time: 0.07s
Test loss: 0.6881 score: 0.6458 time: 0.07s
     INFO: Early stopping counter 19 of 20
Epoch 21/1000, LR 0.000270
Train loss: 0.3175;  Loss pred: 0.3175; Loss self: 0.0000; time: 0.22s
Val loss: 0.6645 score: 0.6735 time: 0.07s
Test loss: 0.6779 score: 0.6458 time: 0.07s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 000,   Train_Loss: 0.8393,   Val_Loss: 0.5828,   Val_Precision: 0.5333,   Val_Recall: 0.9600,   Val_accuracy: 0.6857,   Val_Score: 0.5510,   Val_Loss: 0.5828,   Test_Precision: 0.5349,   Test_Recall: 0.9583,   Test_accuracy: 0.6866,   Test_Score: 0.5625,   Test_loss: 0.5887


[0.06797837500926107, 0.06883950589690357, 0.06874882499687374, 0.06849809095729142, 0.06884398905094713, 0.06966677494347095, 0.06790481600910425, 0.068395329057239, 0.06715196603909135, 0.06827881699427962, 0.06769746902864426, 0.06811269395984709, 0.06924010894726962, 0.0696913090068847, 0.06859489297494292, 0.06833654700312763, 0.06825149094220251, 0.06832328194286674, 0.0686007720651105, 0.06977079901844263, 0.06931876204907894, 0.06971213803626597, 0.06782922602724284, 0.06744433892890811, 0.0755993080092594, 0.0757346999598667, 0.07593659590929747, 0.07468619500286877, 0.08069722098298371, 0.07998170703649521, 0.07977561594452709, 0.07967430504504591, 0.07954209996387362, 0.0798287340439856, 0.07965602108743042, 0.07410749897826463, 0.07602808892261237, 0.07522397499997169, 0.0752323369961232, 0.07530660391785204, 0.07528636092320085, 0.07617228198796511, 0.07614173204638064, 0.07509151997510344, 0.07479958003386855, 0.0750187779776752, 0.07594237104058266, 0.07600331702269614, 0.07597778190393001, 0.07644144596997648, 0.07573568203952163, 0.07624999503605068, 0.0760860099690035, 0.07513718598056585, 0.07564721000380814, 0.07570286595728248, 0.07548904803115875, 0.08255834598094225, 0.07622143696062267, 0.07607044302858412, 0.08025914093013853, 0.07998787902761251, 0.07521140901371837, 0.07568988995626569, 0.07497233408503234, 0.07479966001119465, 0.07516775093972683, 0.07512099505402148, 0.07514454901684076, 0.07480915496125817, 0.07461977296043187, 0.07685453398153186, 0.07529161195270717, 0.07484634197317064, 0.07505948899779469, 0.07512407400645316, 0.075373628991656, 0.07524750696029514, 0.0750198980094865, 0.07542888401076198, 0.0753545539919287, 0.07532995904330164, 0.07533280400093645, 0.07510094495955855, 0.07553196290973574, 0.07535725005436689, 0.07628004194702953, 0.07525699608959258, 0.07501194393262267, 0.07509127398952842, 0.07516801299061626, 0.0748055389849469, 0.07471169892232865, 0.07473339897114784, 0.07472617691382766, 0.07473796396516263, 0.07525132806040347, 0.07488805497996509, 0.07494401908479631, 0.07613072998356074, 0.07514715904835612, 0.08021794899832457, 0.07525975303724408, 0.07543846499174833, 0.07484115404076874, 0.07468306191731244, 0.07180358399637043, 0.07165414397604764, 0.07190294994506985, 0.07192081899847835, 0.07825557293836027, 0.07579022890422493, 0.07556197710800916, 0.07583149499259889, 0.07593750208616257, 0.07576922490261495, 0.07601234898902476, 0.07597466104198247, 0.07577433600090444, 0.07593847997486591, 0.07566743192728609, 0.0715314409462735, 0.07234791107475758, 0.07196250499691814, 0.07849716593045741, 0.07676781399641186, 0.07565284904558212]
[0.0013873137756992057, 0.0014048878754470118, 0.001403037244834158, 0.0013979202236181923, 0.0014049793683866762, 0.001421770917213693, 0.0013858125716143725, 0.0013958230419844693, 0.0013704482865120684, 0.0013934452447812169, 0.0013815810005845769, 0.0013900549787723897, 0.0014130634479034617, 0.0014222716123854018, 0.0013998957749988352, 0.0013946234082270945, 0.0013928875702490307, 0.0013943526927115662, 0.0014000157564308267, 0.0014238938575192373, 0.001414668613246509, 0.0014226966946176728, 0.0013842699189233233, 0.0013764150801817982, 0.0015428430205971307, 0.0015456061216299326, 0.0015497264471285197, 0.0015242080612830361, 0.0016468820608772185, 0.0016322797354386778, 0.001628073794786267, 0.0016260062254091002, 0.001623308162528033, 0.0016291578376323593, 0.0016256330834169472, 0.0015123979383319312, 0.0015515936514818851, 0.0015351831632647284, 0.001535353816247412, 0.0015368694677112662, 0.001536456345371446, 0.0015545363671013287, 0.0015539128989057274, 0.001532479999491907, 0.0015265220415075214, 0.0015309954689321469, 0.0015498443069506664, 0.0015510881025040028, 0.0015505669776312246, 0.0015600295095913568, 0.00154562616407187, 0.0015561223476745036, 0.0015527757136531326, 0.0015334119587870581, 0.0015438206123226151, 0.0015449564481078057, 0.0015405928169624234, 0.0016848642036926989, 0.0015555395298086259, 0.0015524580209915126, 0.0016379416516354801, 0.0016324056944410717, 0.001534926714565681, 0.0015446916317605243, 0.0015300476343884152, 0.00152652367369785, 0.001534035733463813, 0.001533081531714724, 0.0015335622248334848, 0.0015267174481889423, 0.0015228525093965689, 0.0015684598771741195, 0.0015365635092389218, 0.0015274763667994008, 0.0015318263060774427, 0.0015331443674786358, 0.0015382373263603266, 0.001535663407352962, 0.0015310183267242142, 0.0015393649798114688, 0.001537848040651606, 0.0015373461029245233, 0.0015374041632844173, 0.0015326723461134397, 0.0015414686308109335, 0.0015379030623340181, 0.0015567355499393782, 0.0015358570630529097, 0.0015308559986249525, 0.001532474979378131, 0.0015340410814411482, 0.0015266436527540184, 0.0015247285494352787, 0.0015251714075744456, 0.001525024018649544, 0.0015252645707176048, 0.0015357413889878259, 0.0015283276526523487, 0.001529469777240741, 0.0015536883670114437, 0.0015336154907827778, 0.0016371009999658077, 0.0015359133272906955, 0.0015395605100356803, 0.0015273704906279336, 0.0015241441207614784, 0.001495907999924384, 0.001492794666167659, 0.0014979781238556218, 0.0014983503958016324, 0.0016303244362158391, 0.0015789631021713528, 0.0015742078564168576, 0.0015798228123458102, 0.0015820312934617202, 0.0015785255188044782, 0.0015835906039380159, 0.0015828054383746348, 0.0015786320000188425, 0.00158205166614304, 0.0015764048318184602, 0.0014902383530473646, 0.0015072481473907828, 0.0014992188541024614, 0.0016353576235511962, 0.0015993294582585804, 0.0015761010217829607]
[720.8174657503133, 711.8005767412697, 712.739454124899, 715.3484033672049, 711.7542239415872, 703.3481891441021, 721.5983030339172, 716.423192569081, 729.6882413163526, 717.6457085380551, 723.8084481307128, 719.3960061084332, 707.6823064694534, 703.100583103689, 714.3388942657764, 717.0394488582714, 717.9330344811768, 717.178663065026, 714.2776753808691, 702.2995391961579, 706.8793289370504, 702.8905063062188, 722.4024638040202, 726.5250246080703, 648.1540809077046, 646.9953670637906, 645.2751721782221, 656.0784091105171, 607.2080228181889, 612.6400875345355, 614.2227724580995, 615.003795418066, 616.025978975345, 613.814068164992, 615.1449611852646, 661.2016418793388, 644.4986411519067, 651.3880714229532, 651.3156703150804, 650.673346702123, 650.848299733661, 643.278614230591, 643.5367134825926, 652.5370643215896, 655.0838918856664, 653.1697972283937, 645.2261014317687, 644.7087037710157, 644.9253817643423, 641.0135153545563, 646.9869773461605, 642.6229926550553, 644.0080117220235, 652.1404729300589, 647.7436510551189, 647.2674367130256, 649.100780549979, 593.5196425968993, 642.863765810585, 644.1398005476034, 610.5223583523276, 612.5928152574814, 651.4969024322164, 647.3784019016628, 653.5744231255363, 655.0831914565731, 651.8752974169813, 652.2810296211175, 652.0765729663044, 655.0000467907423, 656.6624107256786, 637.5681103183151, 650.8029079092942, 654.6746134575889, 652.8155287793081, 652.2542959503354, 650.0947434204659, 651.1843644980183, 653.1600455362362, 649.6185200487498, 650.2593062291688, 650.47161344975, 650.4470482658643, 652.4551725199494, 648.73198196315, 650.236041849307, 642.369861752654, 651.1022568807566, 653.2293049759229, 652.5392019162322, 651.8730248479097, 655.0317084121305, 655.8544472525126, 655.6640093262362, 655.7273772550355, 655.623961375777, 651.1512987607103, 654.309956549266, 653.8213535700343, 643.6297144475141, 652.0539248658649, 610.8358616975287, 651.0784054227654, 649.5360159483595, 654.7199950084668, 656.1059327515494, 668.4903082613025, 669.8844942735666, 667.5664911755293, 667.4006312555416, 613.374846003725, 633.327022414155, 635.24012786733, 632.9823776345801, 632.098748067019, 633.5025871215349, 631.4763408631222, 631.7895906567574, 633.4598563744204, 632.0906082909089, 634.3548178842176, 671.033595367557, 663.460759086759, 667.0140235120448, 611.4870445453329, 625.2620401857936, 634.4770964419224]
Elapsed: 0.07439314850375027~0.0033182212790284966
Time per graph: 0.0015234854160661858~6.945616564557527e-05
Speed: 657.8002058913463~30.990030667385547
Total Time: 0.0763
best val loss: 0.5827519297599792 test_score: 0.5625

Testing...
Test loss: 0.6219 score: 0.6875 time: 0.07s
test Score 0.6875
Epoch Time List: [0.34395835804753006, 0.3419653399614617, 0.341520480113104, 0.34252021298743784, 0.34399150602985173, 0.3447178431088105, 0.34788122004829347, 0.3417081270599738, 0.33796982397325337, 0.37207021296489984, 0.33562202798202634, 0.33632486686110497, 0.34713741787709296, 0.3444195189513266, 0.37200054805725813, 0.34053318097721785, 0.33759914501570165, 0.33956276008393615, 0.33616698312107474, 0.42863247205968946, 0.3520493080141023, 0.339258449152112, 0.3488590008346364, 0.41366476111579686, 0.33745495695620775, 0.3431598289171234, 0.33719975396525115, 0.34187777200713754, 0.350027889944613, 0.36507055803667754, 0.3611515420489013, 0.3616012280108407, 0.3554594749584794, 0.3602773539023474, 0.3607961292145774, 0.3472133668838069, 0.3414324780460447, 0.34345835295971483, 0.3423060328932479, 0.34115351689979434, 0.33888053603004664, 0.3376948400400579, 0.34134428785182536, 0.3445966640720144, 0.3382029120111838, 0.3375820239307359, 0.3336589310783893, 0.34070098807569593, 0.3426734149688855, 0.3382342930417508, 0.3442444058600813, 0.3435642608674243, 0.3397261860081926, 0.3414603980490938, 0.33850749500561506, 0.34107079287059605, 0.3418189148651436, 0.3479612459195778, 0.3458137889392674, 0.3438127200352028, 0.35180812212638557, 0.3614295670995489, 0.3528044780250639, 0.3387752769049257, 0.3393392199650407, 0.33860314602497965, 0.33792925404850394, 0.34079561301041394, 0.3379129230743274, 0.3369065591832623, 0.33644409105181694, 0.3411804800853133, 0.3451873919693753, 0.3377760130679235, 0.33663218293804675, 0.3378968130564317, 0.33949685213156044, 0.3395364049356431, 0.3390443050302565, 0.33943146001547575, 0.33380966796539724, 0.3408112700562924, 0.3396321301115677, 0.33816996589303017, 0.33914696401916444, 0.338740587932989, 0.34208616300020367, 0.3432482210919261, 0.3364305709255859, 0.34051851893309504, 0.33871718402951956, 0.33795915090013295, 0.33757328998763114, 0.3377578629879281, 0.33598935091868043, 0.33597474300768226, 0.33640410704538226, 0.3365022779908031, 0.3408027469413355, 0.3388106032507494, 0.3401689170859754, 0.3484773619566113, 0.3383021322079003, 0.34153249103110284, 0.3377497820183635, 0.33819878206122667, 0.34282629599329084, 0.3403443560237065, 0.3396772691048682, 0.3391840228578076, 0.35705147800035775, 0.35864780109841377, 0.35792294004932046, 0.3599665639922023, 0.3540442440425977, 0.35880509309936315, 0.3599944459274411, 0.35960442887153476, 0.35897989000659436, 0.3604234759695828, 0.36059605702757835, 0.3441795790567994, 0.3370776999508962, 0.3401682119583711, 0.3469606339931488, 0.3568971750792116, 0.3581284850370139]
Total Epoch List: [24, 82, 21]
Total Time List: [0.07178474590182304, 0.0752365879015997, 0.07627380103804171]
T-times Epoch Time: 0.3436851829206544 ~ 0.004796234648368963
T-times Total Epoch: 59.111111111111114 ~ 11.957744533188006
T-times Total Time: 0.07632307807863174 ~ 0.001816064369734995
T-times Inference Elapsed: 0.07430838074504446 ~ 0.001028180353969082
T-times Time Per Graph: 0.001527346660339111 ~ 2.1377064733270995e-05
T-times Speed: 657.9517286456575 ~ 8.499414679628579
T-times cross validation test micro f1 score:0.7613693591182035 ~ 0.04285803226403712
T-times cross validation test precision:0.83073439352148 ~ 0.010122717044673851
T-times cross validation test recall:0.8201851851851852 ~ 0.10177673998069198
T-times cross validation test f1_score:0.7613693591182035 ~ 0.10597434579892968
