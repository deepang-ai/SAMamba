Namespace(seed=15, model='SGFormer', dataset='ico_wallets/Volume', num_heads=8, num_layers=2, dim_hidden=64, dropout=0.1, epochs=1000, lr=0.0003, weight_decay=0.0001, batch_size=64, abs_pe=None, abs_pe_dim=20, num_class=2, outdir='./outdir/ico_wallets/Volume/seed15/khopgnn_gat_1_0.1_0.0003_0.0001_2_8_64_BN', warmup=10, layer_norm=False, use_edge_attr=False, edge_dim=128, gnn_type='gat', k_hop=1, global_pool='mean', se='khopgnn', ie='mamba', aggr='add', not_extract_node_feature=False, profile=False, k_ford=3, early_stop=1, early_stop_mindelta=-0.0, patience=20, training_times=3, Lambda=0.01, order='sd', use_cuda=True, batch_norm=True, save_logs=True)
Data(edge_index=[2, 334], edge_attr=[334, 2], x=[97, 14887], y=[1, 1], num_nodes=97)
Data(edge_index=[2, 334], edge_attr=[334, 2], x=[97, 14887], y=[1, 1], num_nodes=97)
Data(edge_index=[2, 276], edge_attr=[276, 2], x=[88, 14887], y=[1, 1], num_nodes=97)
========================training times:0========================
========================k_idx:0========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: SGFormer
SGFormer(
  (trans_conv): SGModule(
    (attns): ModuleList(
      (0-1): 2 x SGFormerAttention(heads=1, head_channels=64)
    )
    (fcs): ModuleList(
      (0): Linear(in_features=14887, out_features=64, bias=True)
    )
    (bns): ModuleList(
      (0-2): 3 x LayerNorm((64,), eps=1e-05, elementwise_affine=True)
    )
  )
  (graph_conv): GraphModule(
    (convs): ModuleList(
      (0-2): 3 x GCNConv(64, 64)
    )
    (fcs): ModuleList(
      (0): Linear(in_features=14887, out_features=64, bias=True)
    )
    (bns): ModuleList(
      (0-3): 4 x BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (classifier): Linear(in_features=64, out_features=2, bias=True)
)
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7f342d3778e0>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.7471;  Loss pred: 0.7471; Loss self: 0.0000; time: 0.88s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7144 score: 0.4898 time: 0.09s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7087 score: 0.5102 time: 0.08s
Epoch 2/1000, LR 0.000000
Train loss: 0.7771;  Loss pred: 0.7771; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7123 score: 0.4898 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7071 score: 0.5102 time: 0.08s
Epoch 3/1000, LR 0.000030
Train loss: 0.7706;  Loss pred: 0.7706; Loss self: 0.0000; time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7086 score: 0.4898 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7043 score: 0.5102 time: 0.08s
Epoch 4/1000, LR 0.000060
Train loss: 0.7717;  Loss pred: 0.7717; Loss self: 0.0000; time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7032 score: 0.4898 time: 0.09s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7003 score: 0.5102 time: 0.08s
Epoch 5/1000, LR 0.000090
Train loss: 0.7004;  Loss pred: 0.7004; Loss self: 0.0000; time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6966 score: 0.4898 time: 0.10s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6955 score: 0.5102 time: 0.08s
Epoch 6/1000, LR 0.000120
Train loss: 0.6980;  Loss pred: 0.6980; Loss self: 0.0000; time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6890 score: 0.4898 time: 0.09s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6900 score: 0.5102 time: 0.11s
Epoch 7/1000, LR 0.000150
Train loss: 0.6657;  Loss pred: 0.6657; Loss self: 0.0000; time: 0.15s
Val loss: 0.6810 score: 0.5102 time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6845 score: 0.5102 time: 0.09s
Epoch 8/1000, LR 0.000180
Train loss: 0.6552;  Loss pred: 0.6552; Loss self: 0.0000; time: 0.14s
Val loss: 0.6730 score: 0.5306 time: 0.09s
Test loss: 0.6791 score: 0.5510 time: 0.07s
Epoch 9/1000, LR 0.000210
Train loss: 0.6200;  Loss pred: 0.6200; Loss self: 0.0000; time: 0.14s
Val loss: 0.6648 score: 0.6327 time: 0.09s
Test loss: 0.6741 score: 0.5918 time: 0.08s
Epoch 10/1000, LR 0.000240
Train loss: 0.6474;  Loss pred: 0.6474; Loss self: 0.0000; time: 0.28s
Val loss: 0.6570 score: 0.7755 time: 0.09s
Test loss: 0.6696 score: 0.7143 time: 0.08s
Epoch 11/1000, LR 0.000270
Train loss: 0.6025;  Loss pred: 0.6025; Loss self: 0.0000; time: 0.14s
Val loss: 0.6508 score: 0.9184 time: 0.09s
Test loss: 0.6663 score: 0.8163 time: 0.08s
Epoch 12/1000, LR 0.000270
Train loss: 0.5660;  Loss pred: 0.5660; Loss self: 0.0000; time: 0.15s
Val loss: 0.6451 score: 0.9388 time: 0.09s
Test loss: 0.6635 score: 0.7347 time: 0.08s
Epoch 13/1000, LR 0.000270
Train loss: 0.5667;  Loss pred: 0.5667; Loss self: 0.0000; time: 0.14s
Val loss: 0.6398 score: 0.7551 time: 0.09s
Test loss: 0.6612 score: 0.6122 time: 0.08s
Epoch 14/1000, LR 0.000270
Train loss: 0.5503;  Loss pred: 0.5503; Loss self: 0.0000; time: 0.14s
Val loss: 0.6346 score: 0.6531 time: 0.08s
Test loss: 0.6590 score: 0.5714 time: 0.08s
Epoch 15/1000, LR 0.000270
Train loss: 0.5207;  Loss pred: 0.5207; Loss self: 0.0000; time: 0.14s
Val loss: 0.6294 score: 0.5918 time: 0.08s
Test loss: 0.6568 score: 0.5510 time: 0.08s
Epoch 16/1000, LR 0.000270
Train loss: 0.5127;  Loss pred: 0.5127; Loss self: 0.0000; time: 0.14s
Val loss: 0.6239 score: 0.5714 time: 0.09s
Test loss: 0.6544 score: 0.5510 time: 0.07s
Epoch 17/1000, LR 0.000270
Train loss: 0.4845;  Loss pred: 0.4845; Loss self: 0.0000; time: 0.14s
Val loss: 0.6180 score: 0.5918 time: 0.09s
Test loss: 0.6513 score: 0.5510 time: 0.24s
Epoch 18/1000, LR 0.000270
Train loss: 0.4698;  Loss pred: 0.4698; Loss self: 0.0000; time: 0.14s
Val loss: 0.6114 score: 0.6122 time: 0.09s
Test loss: 0.6473 score: 0.5714 time: 0.08s
Epoch 19/1000, LR 0.000270
Train loss: 0.4639;  Loss pred: 0.4639; Loss self: 0.0000; time: 0.15s
Val loss: 0.6043 score: 0.6122 time: 0.09s
Test loss: 0.6428 score: 0.5714 time: 0.08s
Epoch 20/1000, LR 0.000270
Train loss: 0.4684;  Loss pred: 0.4684; Loss self: 0.0000; time: 0.15s
Val loss: 0.5965 score: 0.6735 time: 0.09s
Test loss: 0.6376 score: 0.5714 time: 0.09s
Epoch 21/1000, LR 0.000270
Train loss: 0.4459;  Loss pred: 0.4459; Loss self: 0.0000; time: 0.15s
Val loss: 0.5882 score: 0.7551 time: 0.09s
Test loss: 0.6319 score: 0.6122 time: 0.08s
Epoch 22/1000, LR 0.000270
Train loss: 0.4134;  Loss pred: 0.4134; Loss self: 0.0000; time: 0.15s
Val loss: 0.5793 score: 0.7755 time: 0.08s
Test loss: 0.6256 score: 0.6327 time: 0.09s
Epoch 23/1000, LR 0.000270
Train loss: 0.4382;  Loss pred: 0.4382; Loss self: 0.0000; time: 0.17s
Val loss: 0.5697 score: 0.8776 time: 0.09s
Test loss: 0.6185 score: 0.6327 time: 0.08s
Epoch 24/1000, LR 0.000270
Train loss: 0.4040;  Loss pred: 0.4040; Loss self: 0.0000; time: 0.14s
Val loss: 0.5600 score: 0.9388 time: 0.08s
Test loss: 0.6110 score: 0.6735 time: 0.08s
Epoch 25/1000, LR 0.000270
Train loss: 0.4066;  Loss pred: 0.4066; Loss self: 0.0000; time: 0.14s
Val loss: 0.5501 score: 0.9592 time: 0.16s
Test loss: 0.6031 score: 0.7143 time: 0.08s
Epoch 26/1000, LR 0.000270
Train loss: 0.3956;  Loss pred: 0.3956; Loss self: 0.0000; time: 0.14s
Val loss: 0.5404 score: 0.9592 time: 0.09s
Test loss: 0.5949 score: 0.8163 time: 0.09s
Epoch 27/1000, LR 0.000270
Train loss: 0.3679;  Loss pred: 0.3679; Loss self: 0.0000; time: 0.16s
Val loss: 0.5310 score: 0.9592 time: 0.12s
Test loss: 0.5865 score: 0.8571 time: 0.12s
Epoch 28/1000, LR 0.000270
Train loss: 0.3722;  Loss pred: 0.3722; Loss self: 0.0000; time: 0.14s
Val loss: 0.5224 score: 0.9796 time: 0.09s
Test loss: 0.5787 score: 0.8776 time: 0.07s
Epoch 29/1000, LR 0.000270
Train loss: 0.3474;  Loss pred: 0.3474; Loss self: 0.0000; time: 0.14s
Val loss: 0.5142 score: 0.9796 time: 0.27s
Test loss: 0.5712 score: 0.8571 time: 0.08s
Epoch 30/1000, LR 0.000270
Train loss: 0.3500;  Loss pred: 0.3500; Loss self: 0.0000; time: 0.14s
Val loss: 0.5065 score: 0.9592 time: 0.08s
Test loss: 0.5643 score: 0.8776 time: 0.07s
Epoch 31/1000, LR 0.000270
Train loss: 0.3366;  Loss pred: 0.3366; Loss self: 0.0000; time: 0.14s
Val loss: 0.4992 score: 0.9592 time: 0.08s
Test loss: 0.5574 score: 0.8571 time: 0.09s
Epoch 32/1000, LR 0.000270
Train loss: 0.3283;  Loss pred: 0.3283; Loss self: 0.0000; time: 0.29s
Val loss: 0.4923 score: 0.9796 time: 0.09s
Test loss: 0.5509 score: 0.8163 time: 0.08s
Epoch 33/1000, LR 0.000270
Train loss: 0.3223;  Loss pred: 0.3223; Loss self: 0.0000; time: 0.14s
Val loss: 0.4857 score: 0.9796 time: 0.09s
Test loss: 0.5450 score: 0.7959 time: 0.08s
Epoch 34/1000, LR 0.000270
Train loss: 0.3044;  Loss pred: 0.3044; Loss self: 0.0000; time: 0.14s
Val loss: 0.4791 score: 0.9796 time: 0.09s
Test loss: 0.5392 score: 0.7959 time: 0.08s
Epoch 35/1000, LR 0.000270
Train loss: 0.2935;  Loss pred: 0.2935; Loss self: 0.0000; time: 0.14s
Val loss: 0.4727 score: 0.9796 time: 0.09s
Test loss: 0.5336 score: 0.7755 time: 0.08s
Epoch 36/1000, LR 0.000270
Train loss: 0.2873;  Loss pred: 0.2873; Loss self: 0.0000; time: 0.14s
Val loss: 0.4667 score: 0.9592 time: 0.09s
Test loss: 0.5284 score: 0.7755 time: 0.08s
Epoch 37/1000, LR 0.000270
Train loss: 0.2798;  Loss pred: 0.2798; Loss self: 0.0000; time: 0.14s
Val loss: 0.4610 score: 0.9592 time: 0.09s
Test loss: 0.5238 score: 0.7755 time: 0.08s
Epoch 38/1000, LR 0.000270
Train loss: 0.2736;  Loss pred: 0.2736; Loss self: 0.0000; time: 0.15s
Val loss: 0.4557 score: 0.9592 time: 0.09s
Test loss: 0.5196 score: 0.7755 time: 0.08s
Epoch 39/1000, LR 0.000269
Train loss: 0.2714;  Loss pred: 0.2714; Loss self: 0.0000; time: 0.15s
Val loss: 0.4508 score: 0.9592 time: 0.08s
Test loss: 0.5159 score: 0.7755 time: 0.09s
Epoch 40/1000, LR 0.000269
Train loss: 0.2571;  Loss pred: 0.2571; Loss self: 0.0000; time: 0.24s
Val loss: 0.4455 score: 0.9592 time: 0.08s
Test loss: 0.5122 score: 0.7755 time: 0.08s
Epoch 41/1000, LR 0.000269
Train loss: 0.2498;  Loss pred: 0.2498; Loss self: 0.0000; time: 0.14s
Val loss: 0.4400 score: 0.9592 time: 0.08s
Test loss: 0.5083 score: 0.7755 time: 0.08s
Epoch 42/1000, LR 0.000269
Train loss: 0.2320;  Loss pred: 0.2320; Loss self: 0.0000; time: 0.14s
Val loss: 0.4339 score: 0.9592 time: 0.09s
Test loss: 0.5040 score: 0.7755 time: 0.08s
Epoch 43/1000, LR 0.000269
Train loss: 0.2544;  Loss pred: 0.2544; Loss self: 0.0000; time: 0.14s
Val loss: 0.4280 score: 0.9796 time: 0.09s
Test loss: 0.4998 score: 0.7959 time: 0.08s
Epoch 44/1000, LR 0.000269
Train loss: 0.2302;  Loss pred: 0.2302; Loss self: 0.0000; time: 0.14s
Val loss: 0.4219 score: 0.9796 time: 0.09s
Test loss: 0.4956 score: 0.8163 time: 0.09s
Epoch 45/1000, LR 0.000269
Train loss: 0.2272;  Loss pred: 0.2272; Loss self: 0.0000; time: 0.15s
Val loss: 0.4160 score: 0.9796 time: 0.10s
Test loss: 0.4913 score: 0.8163 time: 0.08s
Epoch 46/1000, LR 0.000269
Train loss: 0.2250;  Loss pred: 0.2250; Loss self: 0.0000; time: 0.15s
Val loss: 0.4106 score: 0.9796 time: 0.09s
Test loss: 0.4873 score: 0.8163 time: 0.08s
Epoch 47/1000, LR 0.000269
Train loss: 0.2199;  Loss pred: 0.2199; Loss self: 0.0000; time: 0.14s
Val loss: 0.4051 score: 0.9796 time: 0.22s
Test loss: 0.4835 score: 0.8163 time: 0.07s
Epoch 48/1000, LR 0.000269
Train loss: 0.2108;  Loss pred: 0.2108; Loss self: 0.0000; time: 0.14s
Val loss: 0.3993 score: 0.9796 time: 0.08s
Test loss: 0.4794 score: 0.8367 time: 0.08s
Epoch 49/1000, LR 0.000269
Train loss: 0.2120;  Loss pred: 0.2120; Loss self: 0.0000; time: 0.14s
Val loss: 0.3937 score: 0.9796 time: 0.08s
Test loss: 0.4754 score: 0.8367 time: 0.08s
Epoch 50/1000, LR 0.000269
Train loss: 0.1958;  Loss pred: 0.1958; Loss self: 0.0000; time: 0.14s
Val loss: 0.3877 score: 0.9796 time: 0.08s
Test loss: 0.4711 score: 0.8367 time: 0.07s
Epoch 51/1000, LR 0.000269
Train loss: 0.1851;  Loss pred: 0.1851; Loss self: 0.0000; time: 0.14s
Val loss: 0.3815 score: 0.9796 time: 0.09s
Test loss: 0.4670 score: 0.8367 time: 0.07s
Epoch 52/1000, LR 0.000269
Train loss: 0.1883;  Loss pred: 0.1883; Loss self: 0.0000; time: 0.14s
Val loss: 0.3749 score: 0.9796 time: 0.08s
Test loss: 0.4627 score: 0.8367 time: 0.08s
Epoch 53/1000, LR 0.000269
Train loss: 0.1869;  Loss pred: 0.1869; Loss self: 0.0000; time: 0.14s
Val loss: 0.3690 score: 0.9796 time: 0.09s
Test loss: 0.4585 score: 0.8367 time: 0.07s
Epoch 54/1000, LR 0.000269
Train loss: 0.1682;  Loss pred: 0.1682; Loss self: 0.0000; time: 0.14s
Val loss: 0.3635 score: 0.9796 time: 0.08s
Test loss: 0.4541 score: 0.8367 time: 0.08s
Epoch 55/1000, LR 0.000269
Train loss: 0.1754;  Loss pred: 0.1754; Loss self: 0.0000; time: 0.14s
Val loss: 0.3582 score: 0.9796 time: 0.09s
Test loss: 0.4497 score: 0.8367 time: 0.07s
Epoch 56/1000, LR 0.000269
Train loss: 0.1726;  Loss pred: 0.1726; Loss self: 0.0000; time: 0.14s
Val loss: 0.3548 score: 0.9796 time: 0.12s
Test loss: 0.4464 score: 0.8367 time: 0.08s
Epoch 57/1000, LR 0.000269
Train loss: 0.1577;  Loss pred: 0.1577; Loss self: 0.0000; time: 0.14s
Val loss: 0.3516 score: 0.9796 time: 0.09s
Test loss: 0.4434 score: 0.8367 time: 0.07s
Epoch 58/1000, LR 0.000269
Train loss: 0.1671;  Loss pred: 0.1671; Loss self: 0.0000; time: 0.14s
Val loss: 0.3478 score: 0.9796 time: 0.09s
Test loss: 0.4404 score: 0.8367 time: 0.07s
Epoch 59/1000, LR 0.000268
Train loss: 0.1446;  Loss pred: 0.1446; Loss self: 0.0000; time: 0.14s
Val loss: 0.3439 score: 0.9796 time: 0.08s
Test loss: 0.4372 score: 0.8367 time: 0.08s
Epoch 60/1000, LR 0.000268
Train loss: 0.1648;  Loss pred: 0.1648; Loss self: 0.0000; time: 0.14s
Val loss: 0.3392 score: 0.9796 time: 0.08s
Test loss: 0.4337 score: 0.8367 time: 0.08s
Epoch 61/1000, LR 0.000268
Train loss: 0.1495;  Loss pred: 0.1495; Loss self: 0.0000; time: 0.14s
Val loss: 0.3320 score: 0.9796 time: 0.09s
Test loss: 0.4287 score: 0.8367 time: 0.07s
Epoch 62/1000, LR 0.000268
Train loss: 0.1521;  Loss pred: 0.1521; Loss self: 0.0000; time: 0.14s
Val loss: 0.3235 score: 0.9796 time: 0.10s
Test loss: 0.4230 score: 0.8367 time: 0.08s
Epoch 63/1000, LR 0.000268
Train loss: 0.1336;  Loss pred: 0.1336; Loss self: 0.0000; time: 0.15s
Val loss: 0.3153 score: 0.9796 time: 0.09s
Test loss: 0.4178 score: 0.8367 time: 0.08s
Epoch 64/1000, LR 0.000268
Train loss: 0.1571;  Loss pred: 0.1571; Loss self: 0.0000; time: 0.15s
Val loss: 0.3063 score: 0.9796 time: 0.08s
Test loss: 0.4125 score: 0.8367 time: 0.08s
Epoch 65/1000, LR 0.000268
Train loss: 0.1422;  Loss pred: 0.1422; Loss self: 0.0000; time: 0.15s
Val loss: 0.2982 score: 0.9796 time: 0.09s
Test loss: 0.4078 score: 0.8367 time: 0.08s
Epoch 66/1000, LR 0.000268
Train loss: 0.1297;  Loss pred: 0.1297; Loss self: 0.0000; time: 0.14s
Val loss: 0.2908 score: 0.9796 time: 0.09s
Test loss: 0.4033 score: 0.8367 time: 0.08s
Epoch 67/1000, LR 0.000268
Train loss: 0.1431;  Loss pred: 0.1431; Loss self: 0.0000; time: 0.15s
Val loss: 0.2845 score: 0.9796 time: 0.09s
Test loss: 0.3993 score: 0.8367 time: 0.08s
Epoch 68/1000, LR 0.000268
Train loss: 0.1299;  Loss pred: 0.1299; Loss self: 0.0000; time: 0.15s
Val loss: 0.2782 score: 0.9796 time: 0.09s
Test loss: 0.3952 score: 0.8367 time: 0.08s
Epoch 69/1000, LR 0.000268
Train loss: 0.1424;  Loss pred: 0.1424; Loss self: 0.0000; time: 0.15s
Val loss: 0.2735 score: 0.9796 time: 0.09s
Test loss: 0.3919 score: 0.8367 time: 0.08s
Epoch 70/1000, LR 0.000268
Train loss: 0.1143;  Loss pred: 0.1143; Loss self: 0.0000; time: 0.15s
Val loss: 0.2691 score: 0.9796 time: 0.10s
Test loss: 0.3894 score: 0.8367 time: 0.09s
Epoch 71/1000, LR 0.000268
Train loss: 0.1201;  Loss pred: 0.1201; Loss self: 0.0000; time: 0.14s
Val loss: 0.2656 score: 0.9796 time: 0.09s
Test loss: 0.3876 score: 0.8367 time: 0.07s
Epoch 72/1000, LR 0.000267
Train loss: 0.1164;  Loss pred: 0.1164; Loss self: 0.0000; time: 0.14s
Val loss: 0.2595 score: 0.9796 time: 0.09s
Test loss: 0.3836 score: 0.8367 time: 0.07s
Epoch 73/1000, LR 0.000267
Train loss: 0.1416;  Loss pred: 0.1416; Loss self: 0.0000; time: 0.14s
Val loss: 0.2529 score: 0.9796 time: 0.09s
Test loss: 0.3794 score: 0.8367 time: 0.07s
Epoch 74/1000, LR 0.000267
Train loss: 0.1144;  Loss pred: 0.1144; Loss self: 0.0000; time: 0.14s
Val loss: 0.2449 score: 0.9796 time: 0.09s
Test loss: 0.3746 score: 0.8367 time: 0.07s
Epoch 75/1000, LR 0.000267
Train loss: 0.1238;  Loss pred: 0.1238; Loss self: 0.0000; time: 0.15s
Val loss: 0.2383 score: 0.9796 time: 0.09s
Test loss: 0.3707 score: 0.8367 time: 0.08s
Epoch 76/1000, LR 0.000267
Train loss: 0.1055;  Loss pred: 0.1055; Loss self: 0.0000; time: 0.16s
Val loss: 0.2323 score: 0.9796 time: 0.10s
Test loss: 0.3673 score: 0.8367 time: 0.08s
Epoch 77/1000, LR 0.000267
Train loss: 0.1041;  Loss pred: 0.1041; Loss self: 0.0000; time: 0.16s
Val loss: 0.2273 score: 0.9796 time: 0.09s
Test loss: 0.3646 score: 0.8367 time: 0.07s
Epoch 78/1000, LR 0.000267
Train loss: 0.1014;  Loss pred: 0.1014; Loss self: 0.0000; time: 0.15s
Val loss: 0.2241 score: 0.9796 time: 0.09s
Test loss: 0.3628 score: 0.8367 time: 0.08s
Epoch 79/1000, LR 0.000267
Train loss: 0.0991;  Loss pred: 0.0991; Loss self: 0.0000; time: 0.15s
Val loss: 0.2201 score: 0.9796 time: 0.09s
Test loss: 0.3604 score: 0.8367 time: 0.08s
Epoch 80/1000, LR 0.000267
Train loss: 0.0992;  Loss pred: 0.0992; Loss self: 0.0000; time: 0.14s
Val loss: 0.2163 score: 0.9796 time: 0.09s
Test loss: 0.3585 score: 0.8367 time: 0.08s
Epoch 81/1000, LR 0.000267
Train loss: 0.0929;  Loss pred: 0.0929; Loss self: 0.0000; time: 0.15s
Val loss: 0.2111 score: 0.9796 time: 0.08s
Test loss: 0.3559 score: 0.8367 time: 0.08s
Epoch 82/1000, LR 0.000267
Train loss: 0.1022;  Loss pred: 0.1022; Loss self: 0.0000; time: 0.14s
Val loss: 0.2050 score: 0.9796 time: 0.08s
Test loss: 0.3522 score: 0.8367 time: 0.08s
Epoch 83/1000, LR 0.000266
Train loss: 0.0869;  Loss pred: 0.0869; Loss self: 0.0000; time: 0.14s
Val loss: 0.1980 score: 0.9796 time: 0.09s
Test loss: 0.3488 score: 0.8367 time: 0.08s
Epoch 84/1000, LR 0.000266
Train loss: 0.0932;  Loss pred: 0.0932; Loss self: 0.0000; time: 0.14s
Val loss: 0.1902 score: 0.9796 time: 0.09s
Test loss: 0.3449 score: 0.8367 time: 0.08s
Epoch 85/1000, LR 0.000266
Train loss: 0.0893;  Loss pred: 0.0893; Loss self: 0.0000; time: 0.15s
Val loss: 0.1834 score: 0.9592 time: 0.09s
Test loss: 0.3412 score: 0.8367 time: 0.07s
Epoch 86/1000, LR 0.000266
Train loss: 0.0845;  Loss pred: 0.0845; Loss self: 0.0000; time: 0.15s
Val loss: 0.1780 score: 0.9592 time: 0.09s
Test loss: 0.3382 score: 0.8367 time: 0.08s
Epoch 87/1000, LR 0.000266
Train loss: 0.0825;  Loss pred: 0.0825; Loss self: 0.0000; time: 0.14s
Val loss: 0.1751 score: 0.9796 time: 0.09s
Test loss: 0.3364 score: 0.8367 time: 0.08s
Epoch 88/1000, LR 0.000266
Train loss: 0.0830;  Loss pred: 0.0830; Loss self: 0.0000; time: 0.18s
Val loss: 0.1717 score: 0.9796 time: 0.09s
Test loss: 0.3328 score: 0.8367 time: 0.09s
Epoch 89/1000, LR 0.000266
Train loss: 0.0767;  Loss pred: 0.0767; Loss self: 0.0000; time: 0.14s
Val loss: 0.1688 score: 0.9796 time: 0.09s
Test loss: 0.3299 score: 0.8367 time: 0.08s
Epoch 90/1000, LR 0.000266
Train loss: 0.0828;  Loss pred: 0.0828; Loss self: 0.0000; time: 0.15s
Val loss: 0.1618 score: 0.9796 time: 0.09s
Test loss: 0.3272 score: 0.8367 time: 0.08s
Epoch 91/1000, LR 0.000266
Train loss: 0.0720;  Loss pred: 0.0720; Loss self: 0.0000; time: 0.15s
Val loss: 0.1546 score: 0.9592 time: 0.09s
Test loss: 0.3267 score: 0.8571 time: 0.08s
Epoch 92/1000, LR 0.000266
Train loss: 0.0762;  Loss pred: 0.0762; Loss self: 0.0000; time: 0.15s
Val loss: 0.1494 score: 0.9592 time: 0.09s
Test loss: 0.3246 score: 0.8776 time: 0.09s
Epoch 93/1000, LR 0.000265
Train loss: 0.0752;  Loss pred: 0.0752; Loss self: 0.0000; time: 0.14s
Val loss: 0.1448 score: 0.9592 time: 0.09s
Test loss: 0.3213 score: 0.8776 time: 0.08s
Epoch 94/1000, LR 0.000265
Train loss: 0.0666;  Loss pred: 0.0666; Loss self: 0.0000; time: 0.27s
Val loss: 0.1392 score: 0.9592 time: 0.10s
Test loss: 0.3143 score: 0.8776 time: 0.07s
Epoch 95/1000, LR 0.000265
Train loss: 0.0743;  Loss pred: 0.0743; Loss self: 0.0000; time: 0.14s
Val loss: 0.1347 score: 0.9592 time: 0.09s
Test loss: 0.3077 score: 0.8776 time: 0.08s
Epoch 96/1000, LR 0.000265
Train loss: 0.0633;  Loss pred: 0.0633; Loss self: 0.0000; time: 0.14s
Val loss: 0.1314 score: 0.9592 time: 0.09s
Test loss: 0.3044 score: 0.8776 time: 0.08s
Epoch 97/1000, LR 0.000265
Train loss: 0.0761;  Loss pred: 0.0761; Loss self: 0.0000; time: 0.14s
Val loss: 0.1273 score: 0.9592 time: 0.09s
Test loss: 0.3061 score: 0.8776 time: 0.09s
Epoch 98/1000, LR 0.000265
Train loss: 0.0684;  Loss pred: 0.0684; Loss self: 0.0000; time: 0.15s
Val loss: 0.1244 score: 0.9592 time: 0.09s
Test loss: 0.3053 score: 0.8776 time: 0.08s
Epoch 99/1000, LR 0.000265
Train loss: 0.0654;  Loss pred: 0.0654; Loss self: 0.0000; time: 0.14s
Val loss: 0.1200 score: 0.9592 time: 0.09s
Test loss: 0.3044 score: 0.8776 time: 0.08s
Epoch 100/1000, LR 0.000265
Train loss: 0.0652;  Loss pred: 0.0652; Loss self: 0.0000; time: 0.15s
Val loss: 0.1157 score: 0.9592 time: 0.09s
Test loss: 0.3032 score: 0.8980 time: 0.08s
Epoch 101/1000, LR 0.000265
Train loss: 0.0594;  Loss pred: 0.0594; Loss self: 0.0000; time: 0.28s
Val loss: 0.1123 score: 0.9592 time: 0.09s
Test loss: 0.3019 score: 0.8980 time: 0.08s
Epoch 102/1000, LR 0.000264
Train loss: 0.0628;  Loss pred: 0.0628; Loss self: 0.0000; time: 0.14s
Val loss: 0.1092 score: 0.9592 time: 0.08s
Test loss: 0.2938 score: 0.8980 time: 0.08s
Epoch 103/1000, LR 0.000264
Train loss: 0.0586;  Loss pred: 0.0586; Loss self: 0.0000; time: 0.14s
Val loss: 0.1070 score: 0.9592 time: 0.09s
Test loss: 0.2904 score: 0.8980 time: 0.08s
Epoch 104/1000, LR 0.000264
Train loss: 0.0549;  Loss pred: 0.0549; Loss self: 0.0000; time: 0.14s
Val loss: 0.1043 score: 0.9592 time: 0.09s
Test loss: 0.2961 score: 0.8980 time: 0.08s
Epoch 105/1000, LR 0.000264
Train loss: 0.0539;  Loss pred: 0.0539; Loss self: 0.0000; time: 0.15s
Val loss: 0.1024 score: 0.9592 time: 0.09s
Test loss: 0.2982 score: 0.8980 time: 0.08s
Epoch 106/1000, LR 0.000264
Train loss: 0.0554;  Loss pred: 0.0554; Loss self: 0.0000; time: 0.14s
Val loss: 0.1008 score: 0.9592 time: 0.09s
Test loss: 0.2978 score: 0.8980 time: 0.08s
Epoch 107/1000, LR 0.000264
Train loss: 0.0542;  Loss pred: 0.0542; Loss self: 0.0000; time: 0.14s
Val loss: 0.0993 score: 0.9592 time: 0.13s
Test loss: 0.2975 score: 0.8980 time: 0.08s
Epoch 108/1000, LR 0.000264
Train loss: 0.0504;  Loss pred: 0.0504; Loss self: 0.0000; time: 0.15s
Val loss: 0.0976 score: 0.9592 time: 0.34s
Test loss: 0.2920 score: 0.8980 time: 0.07s
Epoch 109/1000, LR 0.000264
Train loss: 0.0469;  Loss pred: 0.0469; Loss self: 0.0000; time: 0.14s
Val loss: 0.0937 score: 0.9592 time: 0.09s
Test loss: 0.2963 score: 0.8980 time: 0.08s
Epoch 110/1000, LR 0.000263
Train loss: 0.0473;  Loss pred: 0.0473; Loss self: 0.0000; time: 0.14s
Val loss: 0.0917 score: 0.9592 time: 0.09s
Test loss: 0.2898 score: 0.8980 time: 0.08s
Epoch 111/1000, LR 0.000263
Train loss: 0.0545;  Loss pred: 0.0545; Loss self: 0.0000; time: 0.14s
Val loss: 0.0916 score: 0.9592 time: 0.08s
Test loss: 0.2864 score: 0.8980 time: 0.08s
Epoch 112/1000, LR 0.000263
Train loss: 0.0471;  Loss pred: 0.0471; Loss self: 0.0000; time: 0.14s
Val loss: 0.0891 score: 0.9592 time: 0.09s
Test loss: 0.2928 score: 0.8980 time: 0.08s
Epoch 113/1000, LR 0.000263
Train loss: 0.0473;  Loss pred: 0.0473; Loss self: 0.0000; time: 0.14s
Val loss: 0.0903 score: 0.9796 time: 0.09s
Test loss: 0.2793 score: 0.9184 time: 0.08s
     INFO: Early stopping counter 1 of 20
Epoch 114/1000, LR 0.000263
Train loss: 0.0535;  Loss pred: 0.0535; Loss self: 0.0000; time: 0.15s
Val loss: 0.0897 score: 0.9796 time: 0.09s
Test loss: 0.2762 score: 0.9184 time: 0.08s
     INFO: Early stopping counter 2 of 20
Epoch 115/1000, LR 0.000263
Train loss: 0.0427;  Loss pred: 0.0427; Loss self: 0.0000; time: 0.14s
Val loss: 0.0858 score: 0.9592 time: 0.09s
Test loss: 0.2830 score: 0.8980 time: 0.17s
Epoch 116/1000, LR 0.000263
Train loss: 0.0422;  Loss pred: 0.0422; Loss self: 0.0000; time: 0.14s
Val loss: 0.0839 score: 0.9592 time: 0.08s
Test loss: 0.2827 score: 0.8980 time: 0.08s
Epoch 117/1000, LR 0.000262
Train loss: 0.0419;  Loss pred: 0.0419; Loss self: 0.0000; time: 0.14s
Val loss: 0.0834 score: 0.9592 time: 0.09s
Test loss: 0.2816 score: 0.8980 time: 0.08s
Epoch 118/1000, LR 0.000262
Train loss: 0.0389;  Loss pred: 0.0389; Loss self: 0.0000; time: 0.17s
Val loss: 0.0848 score: 0.9592 time: 0.08s
Test loss: 0.2760 score: 0.9184 time: 0.08s
     INFO: Early stopping counter 1 of 20
Epoch 119/1000, LR 0.000262
Train loss: 0.0416;  Loss pred: 0.0416; Loss self: 0.0000; time: 0.14s
Val loss: 0.0825 score: 0.9592 time: 0.08s
Test loss: 0.2795 score: 0.8980 time: 0.08s
Epoch 120/1000, LR 0.000262
Train loss: 0.0428;  Loss pred: 0.0428; Loss self: 0.0000; time: 0.14s
Val loss: 0.0828 score: 0.9592 time: 0.09s
Test loss: 0.2783 score: 0.9184 time: 0.07s
     INFO: Early stopping counter 1 of 20
Epoch 121/1000, LR 0.000262
Train loss: 0.0372;  Loss pred: 0.0372; Loss self: 0.0000; time: 0.15s
Val loss: 0.0810 score: 0.9592 time: 0.08s
Test loss: 0.2828 score: 0.8980 time: 0.08s
Epoch 122/1000, LR 0.000262
Train loss: 0.0335;  Loss pred: 0.0335; Loss self: 0.0000; time: 0.15s
Val loss: 0.0824 score: 0.9592 time: 0.08s
Test loss: 0.2780 score: 0.9184 time: 0.08s
     INFO: Early stopping counter 1 of 20
Epoch 123/1000, LR 0.000262
Train loss: 0.0350;  Loss pred: 0.0350; Loss self: 0.0000; time: 0.15s
Val loss: 0.0877 score: 0.9796 time: 0.08s
Test loss: 0.2741 score: 0.9184 time: 0.22s
     INFO: Early stopping counter 2 of 20
Epoch 124/1000, LR 0.000261
Train loss: 0.0350;  Loss pred: 0.0350; Loss self: 0.0000; time: 0.15s
Val loss: 0.0835 score: 0.9592 time: 0.09s
Test loss: 0.2777 score: 0.9184 time: 0.08s
     INFO: Early stopping counter 3 of 20
Epoch 125/1000, LR 0.000261
Train loss: 0.0339;  Loss pred: 0.0339; Loss self: 0.0000; time: 0.14s
Val loss: 0.0808 score: 0.9592 time: 0.09s
Test loss: 0.2838 score: 0.8980 time: 0.08s
Epoch 126/1000, LR 0.000261
Train loss: 0.0359;  Loss pred: 0.0359; Loss self: 0.0000; time: 0.14s
Val loss: 0.0813 score: 0.9592 time: 0.08s
Test loss: 0.2806 score: 0.8980 time: 0.08s
     INFO: Early stopping counter 1 of 20
Epoch 127/1000, LR 0.000261
Train loss: 0.0329;  Loss pred: 0.0329; Loss self: 0.0000; time: 0.14s
Val loss: 0.0786 score: 0.9592 time: 0.09s
Test loss: 0.2838 score: 0.8980 time: 0.08s
Epoch 128/1000, LR 0.000261
Train loss: 0.0315;  Loss pred: 0.0315; Loss self: 0.0000; time: 0.15s
Val loss: 0.0767 score: 0.9592 time: 0.12s
Test loss: 0.2845 score: 0.8980 time: 0.08s
Epoch 129/1000, LR 0.000261
Train loss: 0.0317;  Loss pred: 0.0317; Loss self: 0.0000; time: 0.15s
Val loss: 0.0786 score: 0.9592 time: 0.09s
Test loss: 0.2766 score: 0.9184 time: 0.08s
     INFO: Early stopping counter 1 of 20
Epoch 130/1000, LR 0.000260
Train loss: 0.0308;  Loss pred: 0.0308; Loss self: 0.0000; time: 0.16s
Val loss: 0.0777 score: 0.9592 time: 0.09s
Test loss: 0.2763 score: 0.9184 time: 0.21s
     INFO: Early stopping counter 2 of 20
Epoch 131/1000, LR 0.000260
Train loss: 0.0344;  Loss pred: 0.0344; Loss self: 0.0000; time: 0.15s
Val loss: 0.0753 score: 0.9592 time: 0.09s
Test loss: 0.2813 score: 0.8980 time: 0.08s
Epoch 132/1000, LR 0.000260
Train loss: 0.0334;  Loss pred: 0.0334; Loss self: 0.0000; time: 0.14s
Val loss: 0.0749 score: 0.9592 time: 0.09s
Test loss: 0.2819 score: 0.8980 time: 0.08s
Epoch 133/1000, LR 0.000260
Train loss: 0.0285;  Loss pred: 0.0285; Loss self: 0.0000; time: 0.14s
Val loss: 0.0744 score: 0.9592 time: 0.09s
Test loss: 0.2813 score: 0.8980 time: 0.07s
Epoch 134/1000, LR 0.000260
Train loss: 0.0284;  Loss pred: 0.0284; Loss self: 0.0000; time: 0.15s
Val loss: 0.0752 score: 0.9592 time: 0.09s
Test loss: 0.2803 score: 0.8980 time: 0.08s
     INFO: Early stopping counter 1 of 20
Epoch 135/1000, LR 0.000260
Train loss: 0.0301;  Loss pred: 0.0301; Loss self: 0.0000; time: 0.14s
Val loss: 0.0777 score: 0.9592 time: 0.09s
Test loss: 0.2771 score: 0.9184 time: 0.08s
     INFO: Early stopping counter 2 of 20
Epoch 136/1000, LR 0.000260
Train loss: 0.0288;  Loss pred: 0.0288; Loss self: 0.0000; time: 0.14s
Val loss: 0.0793 score: 0.9592 time: 0.08s
Test loss: 0.2756 score: 0.9184 time: 0.08s
     INFO: Early stopping counter 3 of 20
Epoch 137/1000, LR 0.000259
Train loss: 0.0284;  Loss pred: 0.0284; Loss self: 0.0000; time: 0.14s
Val loss: 0.0765 score: 0.9592 time: 0.08s
Test loss: 0.2786 score: 0.9184 time: 0.08s
     INFO: Early stopping counter 4 of 20
Epoch 138/1000, LR 0.000259
Train loss: 0.0269;  Loss pred: 0.0269; Loss self: 0.0000; time: 0.14s
Val loss: 0.0767 score: 0.9592 time: 0.09s
Test loss: 0.2787 score: 0.9184 time: 0.07s
     INFO: Early stopping counter 5 of 20
Epoch 139/1000, LR 0.000259
Train loss: 0.0259;  Loss pred: 0.0259; Loss self: 0.0000; time: 0.15s
Val loss: 0.0761 score: 0.9592 time: 0.19s
Test loss: 0.2803 score: 0.9184 time: 0.09s
     INFO: Early stopping counter 6 of 20
Epoch 140/1000, LR 0.000259
Train loss: 0.0258;  Loss pred: 0.0258; Loss self: 0.0000; time: 0.14s
Val loss: 0.0742 score: 0.9592 time: 0.09s
Test loss: 0.2832 score: 0.8980 time: 0.08s
Epoch 141/1000, LR 0.000259
Train loss: 0.0259;  Loss pred: 0.0259; Loss self: 0.0000; time: 0.15s
Val loss: 0.0741 score: 0.9592 time: 0.09s
Test loss: 0.2820 score: 0.8980 time: 0.08s
Epoch 142/1000, LR 0.000259
Train loss: 0.0264;  Loss pred: 0.0264; Loss self: 0.0000; time: 0.15s
Val loss: 0.0737 score: 0.9592 time: 0.10s
Test loss: 0.2813 score: 0.9184 time: 0.08s
Epoch 143/1000, LR 0.000258
Train loss: 0.0258;  Loss pred: 0.0258; Loss self: 0.0000; time: 0.14s
Val loss: 0.0734 score: 0.9592 time: 0.09s
Test loss: 0.2802 score: 0.9184 time: 0.07s
Epoch 144/1000, LR 0.000258
Train loss: 0.0248;  Loss pred: 0.0248; Loss self: 0.0000; time: 0.15s
Val loss: 0.0728 score: 0.9592 time: 0.09s
Test loss: 0.2812 score: 0.9184 time: 0.07s
Epoch 145/1000, LR 0.000258
Train loss: 0.0266;  Loss pred: 0.0266; Loss self: 0.0000; time: 0.14s
Val loss: 0.0717 score: 0.9592 time: 0.10s
Test loss: 0.2856 score: 0.8980 time: 0.07s
Epoch 146/1000, LR 0.000258
Train loss: 0.0248;  Loss pred: 0.0248; Loss self: 0.0000; time: 0.14s
Val loss: 0.0705 score: 0.9592 time: 0.10s
Test loss: 0.2890 score: 0.8980 time: 0.08s
Epoch 147/1000, LR 0.000258
Train loss: 0.0261;  Loss pred: 0.0261; Loss self: 0.0000; time: 0.28s
Val loss: 0.0719 score: 0.9592 time: 0.10s
Test loss: 0.2843 score: 0.9184 time: 0.08s
     INFO: Early stopping counter 1 of 20
Epoch 148/1000, LR 0.000257
Train loss: 0.0285;  Loss pred: 0.0285; Loss self: 0.0000; time: 0.14s
Val loss: 0.0706 score: 0.9592 time: 0.08s
Test loss: 0.2846 score: 0.9184 time: 0.09s
     INFO: Early stopping counter 2 of 20
Epoch 149/1000, LR 0.000257
Train loss: 0.0230;  Loss pred: 0.0230; Loss self: 0.0000; time: 0.15s
Val loss: 0.0696 score: 0.9592 time: 0.09s
Test loss: 0.2836 score: 0.9184 time: 0.09s
Epoch 150/1000, LR 0.000257
Train loss: 0.0232;  Loss pred: 0.0232; Loss self: 0.0000; time: 0.14s
Val loss: 0.0689 score: 0.9592 time: 0.09s
Test loss: 0.2828 score: 0.9184 time: 0.07s
Epoch 151/1000, LR 0.000257
Train loss: 0.0235;  Loss pred: 0.0235; Loss self: 0.0000; time: 0.14s
Val loss: 0.0680 score: 0.9592 time: 0.08s
Test loss: 0.2834 score: 0.9184 time: 0.07s
Epoch 152/1000, LR 0.000257
Train loss: 0.0215;  Loss pred: 0.0215; Loss self: 0.0000; time: 0.15s
Val loss: 0.0679 score: 0.9592 time: 0.09s
Test loss: 0.2842 score: 0.9184 time: 0.07s
Epoch 153/1000, LR 0.000257
Train loss: 0.0255;  Loss pred: 0.0255; Loss self: 0.0000; time: 0.14s
Val loss: 0.0682 score: 0.9592 time: 0.09s
Test loss: 0.2836 score: 0.9184 time: 0.07s
     INFO: Early stopping counter 1 of 20
Epoch 154/1000, LR 0.000256
Train loss: 0.0212;  Loss pred: 0.0212; Loss self: 0.0000; time: 0.14s
Val loss: 0.0681 score: 0.9592 time: 0.22s
Test loss: 0.2838 score: 0.9184 time: 0.07s
     INFO: Early stopping counter 2 of 20
Epoch 155/1000, LR 0.000256
Train loss: 0.0223;  Loss pred: 0.0223; Loss self: 0.0000; time: 0.14s
Val loss: 0.0678 score: 0.9592 time: 0.08s
Test loss: 0.2844 score: 0.9184 time: 0.07s
Epoch 156/1000, LR 0.000256
Train loss: 0.0192;  Loss pred: 0.0192; Loss self: 0.0000; time: 0.14s
Val loss: 0.0678 score: 0.9592 time: 0.08s
Test loss: 0.2847 score: 0.9184 time: 0.07s
Epoch 157/1000, LR 0.000256
Train loss: 0.0196;  Loss pred: 0.0196; Loss self: 0.0000; time: 0.14s
Val loss: 0.0676 score: 0.9592 time: 0.08s
Test loss: 0.2848 score: 0.9184 time: 0.08s
Epoch 158/1000, LR 0.000256
Train loss: 0.0206;  Loss pred: 0.0206; Loss self: 0.0000; time: 0.15s
Val loss: 0.0683 score: 0.9592 time: 0.09s
Test loss: 0.2839 score: 0.9184 time: 0.09s
     INFO: Early stopping counter 1 of 20
Epoch 159/1000, LR 0.000255
Train loss: 0.0239;  Loss pred: 0.0239; Loss self: 0.0000; time: 0.15s
Val loss: 0.0674 score: 0.9592 time: 0.09s
Test loss: 0.2845 score: 0.9184 time: 0.08s
Epoch 160/1000, LR 0.000255
Train loss: 0.0201;  Loss pred: 0.0201; Loss self: 0.0000; time: 0.14s
Val loss: 0.0659 score: 0.9592 time: 0.09s
Test loss: 0.2868 score: 0.9184 time: 0.08s
Epoch 161/1000, LR 0.000255
Train loss: 0.0214;  Loss pred: 0.0214; Loss self: 0.0000; time: 0.15s
Val loss: 0.0669 score: 0.9592 time: 0.08s
Test loss: 0.2860 score: 0.9184 time: 0.08s
     INFO: Early stopping counter 1 of 20
Epoch 162/1000, LR 0.000255
Train loss: 0.0207;  Loss pred: 0.0207; Loss self: 0.0000; time: 0.28s
Val loss: 0.0668 score: 0.9592 time: 0.09s
Test loss: 0.2860 score: 0.9184 time: 0.08s
     INFO: Early stopping counter 2 of 20
Epoch 163/1000, LR 0.000255
Train loss: 0.0198;  Loss pred: 0.0198; Loss self: 0.0000; time: 0.14s
Val loss: 0.0649 score: 0.9592 time: 0.09s
Test loss: 0.2884 score: 0.9184 time: 0.08s
Epoch 164/1000, LR 0.000254
Train loss: 0.0215;  Loss pred: 0.0215; Loss self: 0.0000; time: 0.15s
Val loss: 0.0634 score: 0.9592 time: 0.09s
Test loss: 0.2894 score: 0.9184 time: 0.08s
Epoch 165/1000, LR 0.000254
Train loss: 0.0185;  Loss pred: 0.0185; Loss self: 0.0000; time: 0.15s
Val loss: 0.0616 score: 0.9592 time: 0.09s
Test loss: 0.2899 score: 0.9184 time: 0.08s
Epoch 166/1000, LR 0.000254
Train loss: 0.0186;  Loss pred: 0.0186; Loss self: 0.0000; time: 0.15s
Val loss: 0.0608 score: 0.9592 time: 0.09s
Test loss: 0.2876 score: 0.9184 time: 0.09s
Epoch 167/1000, LR 0.000254
Train loss: 0.0200;  Loss pred: 0.0200; Loss self: 0.0000; time: 0.14s
Val loss: 0.0624 score: 0.9592 time: 0.09s
Test loss: 0.2858 score: 0.9184 time: 0.08s
     INFO: Early stopping counter 1 of 20
Epoch 168/1000, LR 0.000254
Train loss: 0.0185;  Loss pred: 0.0185; Loss self: 0.0000; time: 0.15s
Val loss: 0.0629 score: 0.9592 time: 0.09s
Test loss: 0.2841 score: 0.9184 time: 0.09s
     INFO: Early stopping counter 2 of 20
Epoch 169/1000, LR 0.000253
Train loss: 0.0177;  Loss pred: 0.0177; Loss self: 0.0000; time: 0.15s
Val loss: 0.0615 score: 0.9592 time: 0.09s
Test loss: 0.2838 score: 0.9184 time: 0.21s
     INFO: Early stopping counter 3 of 20
Epoch 170/1000, LR 0.000253
Train loss: 0.0170;  Loss pred: 0.0170; Loss self: 0.0000; time: 0.14s
Val loss: 0.0608 score: 0.9592 time: 0.09s
Test loss: 0.2851 score: 0.9184 time: 0.08s
     INFO: Early stopping counter 4 of 20
Epoch 171/1000, LR 0.000253
Train loss: 0.0176;  Loss pred: 0.0176; Loss self: 0.0000; time: 0.15s
Val loss: 0.0603 score: 0.9592 time: 0.09s
Test loss: 0.2870 score: 0.9184 time: 0.09s
Epoch 172/1000, LR 0.000253
Train loss: 0.0177;  Loss pred: 0.0177; Loss self: 0.0000; time: 0.14s
Val loss: 0.0617 score: 0.9592 time: 0.09s
Test loss: 0.2874 score: 0.9184 time: 0.09s
     INFO: Early stopping counter 1 of 20
Epoch 173/1000, LR 0.000253
Train loss: 0.0265;  Loss pred: 0.0265; Loss self: 0.0000; time: 0.15s
Val loss: 0.0619 score: 0.9592 time: 0.09s
Test loss: 0.2881 score: 0.9184 time: 0.08s
     INFO: Early stopping counter 2 of 20
Epoch 174/1000, LR 0.000252
Train loss: 0.0175;  Loss pred: 0.0175; Loss self: 0.0000; time: 0.14s
Val loss: 0.0623 score: 0.9592 time: 0.09s
Test loss: 0.2870 score: 0.9184 time: 0.08s
     INFO: Early stopping counter 3 of 20
Epoch 175/1000, LR 0.000252
Train loss: 0.0234;  Loss pred: 0.0234; Loss self: 0.0000; time: 0.14s
Val loss: 0.0623 score: 0.9592 time: 0.09s
Test loss: 0.2872 score: 0.9184 time: 0.08s
     INFO: Early stopping counter 4 of 20
Epoch 176/1000, LR 0.000252
Train loss: 0.0178;  Loss pred: 0.0178; Loss self: 0.0000; time: 0.14s
Val loss: 0.0620 score: 0.9592 time: 0.09s
Test loss: 0.2885 score: 0.9184 time: 0.08s
     INFO: Early stopping counter 5 of 20
Epoch 177/1000, LR 0.000252
Train loss: 0.0193;  Loss pred: 0.0193; Loss self: 0.0000; time: 0.14s
Val loss: 0.0628 score: 0.9592 time: 0.24s
Test loss: 0.2887 score: 0.9184 time: 0.08s
     INFO: Early stopping counter 6 of 20
Epoch 178/1000, LR 0.000251
Train loss: 0.0215;  Loss pred: 0.0215; Loss self: 0.0000; time: 0.15s
Val loss: 0.0636 score: 0.9592 time: 0.09s
Test loss: 0.2879 score: 0.9184 time: 0.08s
     INFO: Early stopping counter 7 of 20
Epoch 179/1000, LR 0.000251
Train loss: 0.0156;  Loss pred: 0.0156; Loss self: 0.0000; time: 0.14s
Val loss: 0.0636 score: 0.9592 time: 0.09s
Test loss: 0.2879 score: 0.9184 time: 0.09s
     INFO: Early stopping counter 8 of 20
Epoch 180/1000, LR 0.000251
Train loss: 0.0178;  Loss pred: 0.0178; Loss self: 0.0000; time: 0.15s
Val loss: 0.0640 score: 0.9592 time: 0.09s
Test loss: 0.2872 score: 0.9184 time: 0.08s
     INFO: Early stopping counter 9 of 20
Epoch 181/1000, LR 0.000251
Train loss: 0.0175;  Loss pred: 0.0175; Loss self: 0.0000; time: 0.15s
Val loss: 0.0632 score: 0.9592 time: 0.09s
Test loss: 0.2878 score: 0.9184 time: 0.08s
     INFO: Early stopping counter 10 of 20
Epoch 182/1000, LR 0.000251
Train loss: 0.0175;  Loss pred: 0.0175; Loss self: 0.0000; time: 0.15s
Val loss: 0.0628 score: 0.9592 time: 0.09s
Test loss: 0.2879 score: 0.9184 time: 0.08s
     INFO: Early stopping counter 11 of 20
Epoch 183/1000, LR 0.000250
Train loss: 0.0181;  Loss pred: 0.0181; Loss self: 0.0000; time: 0.15s
Val loss: 0.0643 score: 0.9592 time: 0.09s
Test loss: 0.2874 score: 0.9184 time: 0.08s
     INFO: Early stopping counter 12 of 20
Epoch 184/1000, LR 0.000250
Train loss: 0.0170;  Loss pred: 0.0170; Loss self: 0.0000; time: 0.15s
Val loss: 0.0640 score: 0.9592 time: 0.22s
Test loss: 0.2884 score: 0.9184 time: 0.08s
     INFO: Early stopping counter 13 of 20
Epoch 185/1000, LR 0.000250
Train loss: 0.0178;  Loss pred: 0.0178; Loss self: 0.0000; time: 0.15s
Val loss: 0.0621 score: 0.9592 time: 0.09s
Test loss: 0.2897 score: 0.9184 time: 0.08s
     INFO: Early stopping counter 14 of 20
Epoch 186/1000, LR 0.000250
Train loss: 0.0159;  Loss pred: 0.0159; Loss self: 0.0000; time: 0.14s
Val loss: 0.0605 score: 0.9592 time: 0.08s
Test loss: 0.2892 score: 0.9184 time: 0.08s
     INFO: Early stopping counter 15 of 20
Epoch 187/1000, LR 0.000249
Train loss: 0.0157;  Loss pred: 0.0157; Loss self: 0.0000; time: 0.14s
Val loss: 0.0594 score: 0.9592 time: 0.08s
Test loss: 0.2884 score: 0.9184 time: 0.09s
Epoch 188/1000, LR 0.000249
Train loss: 0.0142;  Loss pred: 0.0142; Loss self: 0.0000; time: 0.15s
Val loss: 0.0585 score: 0.9592 time: 0.10s
Test loss: 0.2876 score: 0.9184 time: 0.09s
Epoch 189/1000, LR 0.000249
Train loss: 0.0151;  Loss pred: 0.0151; Loss self: 0.0000; time: 0.15s
Val loss: 0.0582 score: 0.9592 time: 0.08s
Test loss: 0.2874 score: 0.9184 time: 0.08s
Epoch 190/1000, LR 0.000249
Train loss: 0.0151;  Loss pred: 0.0151; Loss self: 0.0000; time: 0.15s
Val loss: 0.0597 score: 0.9592 time: 0.09s
Test loss: 0.2868 score: 0.9184 time: 0.08s
     INFO: Early stopping counter 1 of 20
Epoch 191/1000, LR 0.000249
Train loss: 0.0164;  Loss pred: 0.0164; Loss self: 0.0000; time: 0.15s
Val loss: 0.0587 score: 0.9592 time: 0.09s
Test loss: 0.2877 score: 0.9184 time: 0.08s
     INFO: Early stopping counter 2 of 20
Epoch 192/1000, LR 0.000248
Train loss: 0.0170;  Loss pred: 0.0170; Loss self: 0.0000; time: 0.15s
Val loss: 0.0579 score: 0.9592 time: 0.15s
Test loss: 0.2880 score: 0.9184 time: 0.07s
Epoch 193/1000, LR 0.000248
Train loss: 0.0193;  Loss pred: 0.0193; Loss self: 0.0000; time: 0.14s
Val loss: 0.0588 score: 0.9592 time: 0.08s
Test loss: 0.2869 score: 0.9184 time: 0.08s
     INFO: Early stopping counter 1 of 20
Epoch 194/1000, LR 0.000248
Train loss: 0.0140;  Loss pred: 0.0140; Loss self: 0.0000; time: 0.15s
Val loss: 0.0582 score: 0.9592 time: 0.09s
Test loss: 0.2865 score: 0.9184 time: 0.08s
     INFO: Early stopping counter 2 of 20
Epoch 195/1000, LR 0.000248
Train loss: 0.0148;  Loss pred: 0.0148; Loss self: 0.0000; time: 0.14s
Val loss: 0.0585 score: 0.9592 time: 0.08s
Test loss: 0.2867 score: 0.9184 time: 0.08s
     INFO: Early stopping counter 3 of 20
Epoch 196/1000, LR 0.000247
Train loss: 0.0132;  Loss pred: 0.0132; Loss self: 0.0000; time: 0.15s
Val loss: 0.0585 score: 0.9592 time: 0.09s
Test loss: 0.2872 score: 0.9184 time: 0.08s
     INFO: Early stopping counter 4 of 20
Epoch 197/1000, LR 0.000247
Train loss: 0.0134;  Loss pred: 0.0134; Loss self: 0.0000; time: 0.15s
Val loss: 0.0579 score: 0.9592 time: 0.09s
Test loss: 0.2874 score: 0.9184 time: 0.08s
Epoch 198/1000, LR 0.000247
Train loss: 0.0137;  Loss pred: 0.0137; Loss self: 0.0000; time: 0.15s
Val loss: 0.0576 score: 0.9592 time: 0.09s
Test loss: 0.2878 score: 0.9184 time: 0.08s
Epoch 199/1000, LR 0.000247
Train loss: 0.0151;  Loss pred: 0.0151; Loss self: 0.0000; time: 0.14s
Val loss: 0.0568 score: 0.9592 time: 0.10s
Test loss: 0.2887 score: 0.9184 time: 0.08s
Epoch 200/1000, LR 0.000246
Train loss: 0.0152;  Loss pred: 0.0152; Loss self: 0.0000; time: 0.17s
Val loss: 0.0555 score: 0.9592 time: 0.13s
Test loss: 0.2888 score: 0.9184 time: 0.08s
Epoch 201/1000, LR 0.000246
Train loss: 0.0144;  Loss pred: 0.0144; Loss self: 0.0000; time: 0.14s
Val loss: 0.0546 score: 0.9592 time: 0.08s
Test loss: 0.2885 score: 0.9184 time: 0.08s
Epoch 202/1000, LR 0.000246
Train loss: 0.0143;  Loss pred: 0.0143; Loss self: 0.0000; time: 0.14s
Val loss: 0.0547 score: 0.9592 time: 0.08s
Test loss: 0.2890 score: 0.9184 time: 0.08s
     INFO: Early stopping counter 1 of 20
Epoch 203/1000, LR 0.000246
Train loss: 0.0126;  Loss pred: 0.0126; Loss self: 0.0000; time: 0.15s
Val loss: 0.0543 score: 0.9592 time: 0.08s
Test loss: 0.2893 score: 0.9184 time: 0.08s
Epoch 204/1000, LR 0.000245
Train loss: 0.0144;  Loss pred: 0.0144; Loss self: 0.0000; time: 0.14s
Val loss: 0.0541 score: 0.9592 time: 0.09s
Test loss: 0.2888 score: 0.9184 time: 0.07s
Epoch 205/1000, LR 0.000245
Train loss: 0.0136;  Loss pred: 0.0136; Loss self: 0.0000; time: 0.14s
Val loss: 0.0538 score: 0.9592 time: 0.09s
Test loss: 0.2894 score: 0.9184 time: 0.08s
Epoch 206/1000, LR 0.000245
Train loss: 0.0136;  Loss pred: 0.0136; Loss self: 0.0000; time: 0.14s
Val loss: 0.0535 score: 0.9592 time: 0.09s
Test loss: 0.2895 score: 0.9184 time: 0.08s
Epoch 207/1000, LR 0.000245
Train loss: 0.0132;  Loss pred: 0.0132; Loss self: 0.0000; time: 0.14s
Val loss: 0.0531 score: 0.9592 time: 0.09s
Test loss: 0.2897 score: 0.9184 time: 0.08s
Epoch 208/1000, LR 0.000244
Train loss: 0.0117;  Loss pred: 0.0117; Loss self: 0.0000; time: 0.28s
Val loss: 0.0523 score: 0.9796 time: 0.09s
Test loss: 0.2902 score: 0.9184 time: 0.07s
Epoch 209/1000, LR 0.000244
Train loss: 0.0134;  Loss pred: 0.0134; Loss self: 0.0000; time: 0.14s
Val loss: 0.0517 score: 0.9796 time: 0.08s
Test loss: 0.2902 score: 0.9184 time: 0.08s
Epoch 210/1000, LR 0.000244
Train loss: 0.0123;  Loss pred: 0.0123; Loss self: 0.0000; time: 0.14s
Val loss: 0.0521 score: 0.9796 time: 0.09s
Test loss: 0.2904 score: 0.9184 time: 0.07s
     INFO: Early stopping counter 1 of 20
Epoch 211/1000, LR 0.000244
Train loss: 0.0122;  Loss pred: 0.0122; Loss self: 0.0000; time: 0.14s
Val loss: 0.0540 score: 0.9592 time: 0.09s
Test loss: 0.2905 score: 0.9184 time: 0.08s
     INFO: Early stopping counter 2 of 20
Epoch 212/1000, LR 0.000243
Train loss: 0.0155;  Loss pred: 0.0155; Loss self: 0.0000; time: 0.14s
Val loss: 0.0553 score: 0.9592 time: 0.09s
Test loss: 0.2906 score: 0.9184 time: 0.07s
     INFO: Early stopping counter 3 of 20
Epoch 213/1000, LR 0.000243
Train loss: 0.0121;  Loss pred: 0.0121; Loss self: 0.0000; time: 0.14s
Val loss: 0.0561 score: 0.9592 time: 0.09s
Test loss: 0.2903 score: 0.9184 time: 0.08s
     INFO: Early stopping counter 4 of 20
Epoch 214/1000, LR 0.000243
Train loss: 0.0134;  Loss pred: 0.0134; Loss self: 0.0000; time: 0.15s
Val loss: 0.0554 score: 0.9592 time: 3.35s
Test loss: 0.2904 score: 0.9184 time: 3.13s
     INFO: Early stopping counter 5 of 20
Epoch 215/1000, LR 0.000243
Train loss: 0.0134;  Loss pred: 0.0134; Loss self: 0.0000; time: 0.55s
Val loss: 0.0539 score: 0.9592 time: 0.08s
Test loss: 0.2909 score: 0.9184 time: 0.08s
     INFO: Early stopping counter 6 of 20
Epoch 216/1000, LR 0.000242
Train loss: 0.0130;  Loss pred: 0.0130; Loss self: 0.0000; time: 0.15s
Val loss: 0.0528 score: 0.9796 time: 0.08s
Test loss: 0.2918 score: 0.9184 time: 0.08s
     INFO: Early stopping counter 7 of 20
Epoch 217/1000, LR 0.000242
Train loss: 0.0125;  Loss pred: 0.0125; Loss self: 0.0000; time: 0.15s
Val loss: 0.0517 score: 0.9796 time: 0.09s
Test loss: 0.2924 score: 0.8980 time: 0.08s
Epoch 218/1000, LR 0.000242
Train loss: 0.0150;  Loss pred: 0.0150; Loss self: 0.0000; time: 0.15s
Val loss: 0.0508 score: 0.9796 time: 0.09s
Test loss: 0.2926 score: 0.8980 time: 0.08s
Epoch 219/1000, LR 0.000242
Train loss: 0.0122;  Loss pred: 0.0122; Loss self: 0.0000; time: 0.15s
Val loss: 0.0519 score: 0.9796 time: 0.09s
Test loss: 0.2923 score: 0.8980 time: 0.09s
     INFO: Early stopping counter 1 of 20
Epoch 220/1000, LR 0.000241
Train loss: 0.0105;  Loss pred: 0.0105; Loss self: 0.0000; time: 0.15s
Val loss: 0.0531 score: 0.9592 time: 0.09s
Test loss: 0.2920 score: 0.9184 time: 0.07s
     INFO: Early stopping counter 2 of 20
Epoch 221/1000, LR 0.000241
Train loss: 0.0123;  Loss pred: 0.0123; Loss self: 0.0000; time: 0.14s
Val loss: 0.0554 score: 0.9592 time: 0.08s
Test loss: 0.2912 score: 0.9184 time: 0.08s
     INFO: Early stopping counter 3 of 20
Epoch 222/1000, LR 0.000241
Train loss: 0.0120;  Loss pred: 0.0120; Loss self: 0.0000; time: 0.14s
Val loss: 0.0565 score: 0.9592 time: 0.09s
Test loss: 0.2908 score: 0.9184 time: 0.07s
     INFO: Early stopping counter 4 of 20
Epoch 223/1000, LR 0.000241
Train loss: 0.0120;  Loss pred: 0.0120; Loss self: 0.0000; time: 0.14s
Val loss: 0.0556 score: 0.9592 time: 0.09s
Test loss: 0.2914 score: 0.9184 time: 0.07s
     INFO: Early stopping counter 5 of 20
Epoch 224/1000, LR 0.000240
Train loss: 0.0107;  Loss pred: 0.0107; Loss self: 0.0000; time: 0.14s
Val loss: 0.0544 score: 0.9592 time: 0.09s
Test loss: 0.2923 score: 0.9184 time: 0.07s
     INFO: Early stopping counter 6 of 20
Epoch 225/1000, LR 0.000240
Train loss: 0.0120;  Loss pred: 0.0120; Loss self: 0.0000; time: 0.14s
Val loss: 0.0530 score: 0.9592 time: 0.09s
Test loss: 0.2930 score: 0.9184 time: 0.07s
     INFO: Early stopping counter 7 of 20
Epoch 226/1000, LR 0.000240
Train loss: 0.0123;  Loss pred: 0.0123; Loss self: 0.0000; time: 0.14s
Val loss: 0.0513 score: 0.9796 time: 0.09s
Test loss: 0.2935 score: 0.8980 time: 0.08s
     INFO: Early stopping counter 8 of 20
Epoch 227/1000, LR 0.000240
Train loss: 0.0122;  Loss pred: 0.0122; Loss self: 0.0000; time: 0.14s
Val loss: 0.0500 score: 0.9796 time: 0.09s
Test loss: 0.2939 score: 0.8980 time: 0.08s
Epoch 228/1000, LR 0.000239
Train loss: 0.0122;  Loss pred: 0.0122; Loss self: 0.0000; time: 0.14s
Val loss: 0.0501 score: 0.9796 time: 0.10s
Test loss: 0.2942 score: 0.8980 time: 0.09s
     INFO: Early stopping counter 1 of 20
Epoch 229/1000, LR 0.000239
Train loss: 0.0113;  Loss pred: 0.0113; Loss self: 0.0000; time: 0.14s
Val loss: 0.0512 score: 0.9796 time: 0.09s
Test loss: 0.2932 score: 0.9184 time: 0.08s
     INFO: Early stopping counter 2 of 20
Epoch 230/1000, LR 0.000239
Train loss: 0.0118;  Loss pred: 0.0118; Loss self: 0.0000; time: 0.14s
Val loss: 0.0522 score: 0.9592 time: 0.09s
Test loss: 0.2926 score: 0.9184 time: 0.07s
     INFO: Early stopping counter 3 of 20
Epoch 231/1000, LR 0.000238
Train loss: 0.0112;  Loss pred: 0.0112; Loss self: 0.0000; time: 0.14s
Val loss: 0.0528 score: 0.9592 time: 0.09s
Test loss: 0.2925 score: 0.9184 time: 0.08s
     INFO: Early stopping counter 4 of 20
Epoch 232/1000, LR 0.000238
Train loss: 0.0117;  Loss pred: 0.0117; Loss self: 0.0000; time: 0.15s
Val loss: 0.0531 score: 0.9592 time: 0.09s
Test loss: 0.2928 score: 0.9184 time: 0.08s
     INFO: Early stopping counter 5 of 20
Epoch 233/1000, LR 0.000238
Train loss: 0.0106;  Loss pred: 0.0106; Loss self: 0.0000; time: 0.14s
Val loss: 0.0524 score: 0.9592 time: 0.08s
Test loss: 0.2934 score: 0.9184 time: 0.08s
     INFO: Early stopping counter 6 of 20
Epoch 234/1000, LR 0.000238
Train loss: 0.0111;  Loss pred: 0.0111; Loss self: 0.0000; time: 0.14s
Val loss: 0.0515 score: 0.9592 time: 0.08s
Test loss: 0.2938 score: 0.9184 time: 0.08s
     INFO: Early stopping counter 7 of 20
Epoch 235/1000, LR 0.000237
Train loss: 0.0114;  Loss pred: 0.0114; Loss self: 0.0000; time: 0.14s
Val loss: 0.0511 score: 0.9592 time: 0.09s
Test loss: 0.2941 score: 0.9184 time: 0.08s
     INFO: Early stopping counter 8 of 20
Epoch 236/1000, LR 0.000237
Train loss: 0.0102;  Loss pred: 0.0102; Loss self: 0.0000; time: 0.14s
Val loss: 0.0495 score: 0.9796 time: 0.10s
Test loss: 0.2946 score: 0.9184 time: 0.09s
Epoch 237/1000, LR 0.000237
Train loss: 0.0104;  Loss pred: 0.0104; Loss self: 0.0000; time: 0.14s
Val loss: 0.0481 score: 0.9796 time: 0.10s
Test loss: 0.2952 score: 0.9184 time: 0.08s
Epoch 238/1000, LR 0.000236
Train loss: 0.0100;  Loss pred: 0.0100; Loss self: 0.0000; time: 0.14s
Val loss: 0.0470 score: 0.9796 time: 0.09s
Test loss: 0.2954 score: 0.9184 time: 0.08s
Epoch 239/1000, LR 0.000236
Train loss: 0.0113;  Loss pred: 0.0113; Loss self: 0.0000; time: 0.14s
Val loss: 0.0460 score: 0.9796 time: 0.09s
Test loss: 0.2950 score: 0.9184 time: 0.08s
Epoch 240/1000, LR 0.000236
Train loss: 0.0106;  Loss pred: 0.0106; Loss self: 0.0000; time: 0.15s
Val loss: 0.0456 score: 0.9796 time: 0.09s
Test loss: 0.2943 score: 0.9184 time: 0.07s
Epoch 241/1000, LR 0.000236
Train loss: 0.0097;  Loss pred: 0.0097; Loss self: 0.0000; time: 0.14s
Val loss: 0.0452 score: 0.9796 time: 0.09s
Test loss: 0.2939 score: 0.9184 time: 0.07s
Epoch 242/1000, LR 0.000235
Train loss: 0.0095;  Loss pred: 0.0095; Loss self: 0.0000; time: 0.14s
Val loss: 0.0450 score: 0.9796 time: 0.08s
Test loss: 0.2937 score: 0.9184 time: 0.08s
Epoch 243/1000, LR 0.000235
Train loss: 0.0097;  Loss pred: 0.0097; Loss self: 0.0000; time: 0.14s
Val loss: 0.0449 score: 0.9796 time: 0.08s
Test loss: 0.2934 score: 0.9184 time: 0.08s
Epoch 244/1000, LR 0.000235
Train loss: 0.0105;  Loss pred: 0.0105; Loss self: 0.0000; time: 0.15s
Val loss: 0.0450 score: 0.9796 time: 0.09s
Test loss: 0.2933 score: 0.9184 time: 0.08s
     INFO: Early stopping counter 1 of 20
Epoch 245/1000, LR 0.000234
Train loss: 0.0116;  Loss pred: 0.0116; Loss self: 0.0000; time: 0.14s
Val loss: 0.0451 score: 0.9796 time: 0.08s
Test loss: 0.2931 score: 0.9184 time: 0.09s
     INFO: Early stopping counter 2 of 20
Epoch 246/1000, LR 0.000234
Train loss: 0.0105;  Loss pred: 0.0105; Loss self: 0.0000; time: 0.14s
Val loss: 0.0460 score: 0.9796 time: 0.11s
Test loss: 0.2929 score: 0.9184 time: 0.09s
     INFO: Early stopping counter 3 of 20
Epoch 247/1000, LR 0.000234
Train loss: 0.0104;  Loss pred: 0.0104; Loss self: 0.0000; time: 0.15s
Val loss: 0.0463 score: 0.9796 time: 0.09s
Test loss: 0.2935 score: 0.9184 time: 0.08s
     INFO: Early stopping counter 4 of 20
Epoch 248/1000, LR 0.000234
Train loss: 0.0098;  Loss pred: 0.0098; Loss self: 0.0000; time: 0.15s
Val loss: 0.0453 score: 0.9796 time: 0.09s
Test loss: 0.2940 score: 0.9184 time: 0.08s
     INFO: Early stopping counter 5 of 20
Epoch 249/1000, LR 0.000233
Train loss: 0.0093;  Loss pred: 0.0093; Loss self: 0.0000; time: 0.14s
Val loss: 0.0444 score: 0.9796 time: 0.08s
Test loss: 0.2952 score: 0.9184 time: 0.08s
Epoch 250/1000, LR 0.000233
Train loss: 0.0098;  Loss pred: 0.0098; Loss self: 0.0000; time: 0.15s
Val loss: 0.0445 score: 0.9796 time: 0.09s
Test loss: 0.2954 score: 0.9184 time: 0.07s
     INFO: Early stopping counter 1 of 20
Epoch 251/1000, LR 0.000233
Train loss: 0.0104;  Loss pred: 0.0104; Loss self: 0.0000; time: 0.14s
Val loss: 0.0449 score: 0.9796 time: 0.09s
Test loss: 0.2957 score: 0.9184 time: 0.07s
     INFO: Early stopping counter 2 of 20
Epoch 252/1000, LR 0.000232
Train loss: 0.0093;  Loss pred: 0.0093; Loss self: 0.0000; time: 0.15s
Val loss: 0.0454 score: 0.9796 time: 0.18s
Test loss: 0.2958 score: 0.9184 time: 0.08s
     INFO: Early stopping counter 3 of 20
Epoch 253/1000, LR 0.000232
Train loss: 0.0090;  Loss pred: 0.0090; Loss self: 0.0000; time: 0.14s
Val loss: 0.0448 score: 0.9796 time: 0.09s
Test loss: 0.2958 score: 0.9184 time: 0.07s
     INFO: Early stopping counter 4 of 20
Epoch 254/1000, LR 0.000232
Train loss: 0.0093;  Loss pred: 0.0093; Loss self: 0.0000; time: 0.14s
Val loss: 0.0448 score: 0.9796 time: 0.08s
Test loss: 0.2951 score: 0.9184 time: 0.08s
     INFO: Early stopping counter 5 of 20
Epoch 255/1000, LR 0.000232
Train loss: 0.0104;  Loss pred: 0.0104; Loss self: 0.0000; time: 0.14s
Val loss: 0.0423 score: 0.9796 time: 0.08s
Test loss: 0.2955 score: 0.9184 time: 0.08s
Epoch 256/1000, LR 0.000231
Train loss: 0.0099;  Loss pred: 0.0099; Loss self: 0.0000; time: 0.14s
Val loss: 0.0411 score: 0.9796 time: 0.09s
Test loss: 0.2960 score: 0.9184 time: 0.08s
Epoch 257/1000, LR 0.000231
Train loss: 0.0092;  Loss pred: 0.0092; Loss self: 0.0000; time: 0.14s
Val loss: 0.0411 score: 0.9796 time: 0.09s
Test loss: 0.2964 score: 0.9184 time: 0.08s
Epoch 258/1000, LR 0.000231
Train loss: 0.0102;  Loss pred: 0.0102; Loss self: 0.0000; time: 0.15s
Val loss: 0.0421 score: 0.9796 time: 0.11s
Test loss: 0.2959 score: 0.9184 time: 0.08s
     INFO: Early stopping counter 1 of 20
Epoch 259/1000, LR 0.000230
Train loss: 0.0089;  Loss pred: 0.0089; Loss self: 0.0000; time: 0.15s
Val loss: 0.0426 score: 0.9796 time: 0.15s
Test loss: 0.2955 score: 0.9184 time: 0.08s
     INFO: Early stopping counter 2 of 20
Epoch 260/1000, LR 0.000230
Train loss: 0.0086;  Loss pred: 0.0086; Loss self: 0.0000; time: 0.14s
Val loss: 0.0428 score: 0.9796 time: 0.08s
Test loss: 0.2956 score: 0.9184 time: 0.08s
     INFO: Early stopping counter 3 of 20
Epoch 261/1000, LR 0.000230
Train loss: 0.0090;  Loss pred: 0.0090; Loss self: 0.0000; time: 0.15s
Val loss: 0.0434 score: 0.9796 time: 0.08s
Test loss: 0.2955 score: 0.9184 time: 0.08s
     INFO: Early stopping counter 4 of 20
Epoch 262/1000, LR 0.000229
Train loss: 0.0093;  Loss pred: 0.0093; Loss self: 0.0000; time: 0.14s
Val loss: 0.0439 score: 0.9796 time: 0.09s
Test loss: 0.2954 score: 0.9184 time: 0.07s
     INFO: Early stopping counter 5 of 20
Epoch 263/1000, LR 0.000229
Train loss: 0.0104;  Loss pred: 0.0104; Loss self: 0.0000; time: 0.15s
Val loss: 0.0429 score: 0.9796 time: 0.08s
Test loss: 0.2961 score: 0.9184 time: 0.07s
     INFO: Early stopping counter 6 of 20
Epoch 264/1000, LR 0.000229
Train loss: 0.0091;  Loss pred: 0.0091; Loss self: 0.0000; time: 0.14s
Val loss: 0.0433 score: 0.9796 time: 0.08s
Test loss: 0.2964 score: 0.9184 time: 0.07s
     INFO: Early stopping counter 7 of 20
Epoch 265/1000, LR 0.000228
Train loss: 0.0099;  Loss pred: 0.0099; Loss self: 0.0000; time: 0.15s
Val loss: 0.0452 score: 0.9796 time: 0.09s
Test loss: 0.2957 score: 0.9184 time: 0.08s
     INFO: Early stopping counter 8 of 20
Epoch 266/1000, LR 0.000228
Train loss: 0.0099;  Loss pred: 0.0099; Loss self: 0.0000; time: 0.15s
Val loss: 0.0475 score: 0.9592 time: 0.10s
Test loss: 0.2952 score: 0.9184 time: 0.22s
     INFO: Early stopping counter 9 of 20
Epoch 267/1000, LR 0.000228
Train loss: 0.0087;  Loss pred: 0.0087; Loss self: 0.0000; time: 0.14s
Val loss: 0.0479 score: 0.9592 time: 0.09s
Test loss: 0.2954 score: 0.9184 time: 0.08s
     INFO: Early stopping counter 10 of 20
Epoch 268/1000, LR 0.000228
Train loss: 0.0084;  Loss pred: 0.0084; Loss self: 0.0000; time: 0.14s
Val loss: 0.0464 score: 0.9796 time: 0.08s
Test loss: 0.2959 score: 0.9184 time: 0.07s
     INFO: Early stopping counter 11 of 20
Epoch 269/1000, LR 0.000227
Train loss: 0.0099;  Loss pred: 0.0099; Loss self: 0.0000; time: 0.14s
Val loss: 0.0448 score: 0.9796 time: 0.09s
Test loss: 0.2971 score: 0.9184 time: 0.07s
     INFO: Early stopping counter 12 of 20
Epoch 270/1000, LR 0.000227
Train loss: 0.0087;  Loss pred: 0.0087; Loss self: 0.0000; time: 0.14s
Val loss: 0.0447 score: 0.9796 time: 0.08s
Test loss: 0.2984 score: 0.9184 time: 0.08s
     INFO: Early stopping counter 13 of 20
Epoch 271/1000, LR 0.000227
Train loss: 0.0090;  Loss pred: 0.0090; Loss self: 0.0000; time: 0.14s
Val loss: 0.0447 score: 0.9796 time: 0.08s
Test loss: 0.2989 score: 0.8980 time: 0.08s
     INFO: Early stopping counter 14 of 20
Epoch 272/1000, LR 0.000226
Train loss: 0.0090;  Loss pred: 0.0090; Loss self: 0.0000; time: 0.14s
Val loss: 0.0455 score: 0.9796 time: 0.09s
Test loss: 0.2989 score: 0.9184 time: 0.07s
     INFO: Early stopping counter 15 of 20
Epoch 273/1000, LR 0.000226
Train loss: 0.0180;  Loss pred: 0.0180; Loss self: 0.0000; time: 0.15s
Val loss: 0.0464 score: 0.9796 time: 0.09s
Test loss: 0.2994 score: 0.8980 time: 0.07s
     INFO: Early stopping counter 16 of 20
Epoch 274/1000, LR 0.000226
Train loss: 0.0114;  Loss pred: 0.0114; Loss self: 0.0000; time: 0.15s
Val loss: 0.0450 score: 0.9796 time: 0.18s
Test loss: 0.3001 score: 0.8980 time: 0.08s
     INFO: Early stopping counter 17 of 20
Epoch 275/1000, LR 0.000225
Train loss: 0.0085;  Loss pred: 0.0085; Loss self: 0.0000; time: 0.15s
Val loss: 0.0446 score: 0.9796 time: 0.10s
Test loss: 0.2995 score: 0.8980 time: 0.08s
     INFO: Early stopping counter 18 of 20
Epoch 276/1000, LR 0.000225
Train loss: 0.0099;  Loss pred: 0.0099; Loss self: 0.0000; time: 0.15s
Val loss: 0.0452 score: 0.9796 time: 0.09s
Test loss: 0.2995 score: 0.8980 time: 0.09s
     INFO: Early stopping counter 19 of 20
Epoch 277/1000, LR 0.000225
Train loss: 0.0088;  Loss pred: 0.0088; Loss self: 0.0000; time: 0.14s
Val loss: 0.0478 score: 0.9796 time: 0.09s
Test loss: 0.2983 score: 0.8980 time: 0.09s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 256,   Train_Loss: 0.0092,   Val_Loss: 0.0411,   Val_Precision: 0.9600,   Val_Recall: 1.0000,   Val_accuracy: 0.9796,   Val_Score: 0.9796,   Val_Loss: 0.0411,   Test_Precision: 0.9200,   Test_Recall: 0.9200,   Test_accuracy: 0.9200,   Test_Score: 0.9184,   Test_loss: 0.2964


[0.08030457305721939, 0.08224903489463031, 0.08930992404930294, 0.09014386311173439, 0.09109797701239586, 0.11844071093946695, 0.09641301794908941, 0.0792765119113028, 0.08809523889794946, 0.07997036795131862, 0.08830716903321445, 0.0856172798667103, 0.08147128485143185, 0.08817671611905098, 0.08749923179857433, 0.07961139804683626, 0.2485009920783341, 0.08937256503850222, 0.08282550307922065, 0.09621028997935355, 0.08198193786665797, 0.09247422916814685, 0.08144118287600577, 0.08936924301087856, 0.08664764906279743, 0.09125145291909575, 0.12373684905469418, 0.07643555803224444, 0.08294393913820386, 0.07940995809622109, 0.0934932001400739, 0.08350839791819453, 0.08147210790775716, 0.08141372003592551, 0.08202403411269188, 0.08147556590847671, 0.0813275899272412, 0.08863172004930675, 0.09473197697661817, 0.08491315809078515, 0.08734845509752631, 0.08073536702431738, 0.08964998694136739, 0.09078946919180453, 0.08566621784120798, 0.08715541195124388, 0.0761185169685632, 0.08477532188408077, 0.08474085899069905, 0.07614627294242382, 0.07789773400872946, 0.08448924799449742, 0.07630238891579211, 0.08805670589208603, 0.07739226799458265, 0.08294420479796827, 0.07962217694148421, 0.0772191770374775, 0.08571332995779812, 0.08586748992092907, 0.07830905099399388, 0.08032047096639872, 0.08107276004739106, 0.08169830404222012, 0.08374937996268272, 0.08809681003913283, 0.08287597191520035, 0.08143398794345558, 0.08135853102430701, 0.09978363802656531, 0.08035077899694443, 0.07949860696680844, 0.0802019510883838, 0.07999211107380688, 0.08579550893045962, 0.08731993706896901, 0.07981866295449436, 0.08068527188152075, 0.080497439019382, 0.08082063402980566, 0.08700373792089522, 0.08751884801313281, 0.08023286797106266, 0.08173221489414573, 0.07971715182065964, 0.08123901719227433, 0.08858295599929988, 0.09337678295560181, 0.0839238129556179, 0.08465743390843272, 0.08520442084409297, 0.09067314211279154, 0.08440718008205295, 0.07953035994432867, 0.08174778893589973, 0.08658345299772918, 0.09255612595006824, 0.08059333101846278, 0.08125989604741335, 0.08189757610671222, 0.08078978699631989, 0.0807079819496721, 0.08810480288229883, 0.08207276184111834, 0.08807546179741621, 0.08000069414265454, 0.08125029504299164, 0.07794837700203061, 0.08079420914873481, 0.07980709685944021, 0.08226426900364459, 0.08085592114366591, 0.08267049095593393, 0.08617170481011271, 0.17299658711999655, 0.08482114807702601, 0.0804522989783436, 0.08762490097433329, 0.0854366491548717, 0.07922095083631575, 0.0876025459729135, 0.08143193693831563, 0.2240986900869757, 0.08001524396240711, 0.0812943868804723, 0.08208154304884374, 0.08227636781521142, 0.08359456597827375, 0.0824424340389669, 0.21290116012096405, 0.08042746712453663, 0.08712090598419309, 0.07937311194837093, 0.08033056394197047, 0.08049654704518616, 0.08681910601444542, 0.0825569189619273, 0.07881353911943734, 0.09111546585336328, 0.08360462612472475, 0.0845167029183358, 0.08445483213290572, 0.07935727410949767, 0.07964708399958909, 0.07883037719875574, 0.08398926886729896, 0.086457114899531, 0.09179740399122238, 0.09486383688636124, 0.07502445694990456, 0.07620215509086847, 0.07894653407856822, 0.07977849687449634, 0.07867218204773962, 0.07770778192207217, 0.07537034898996353, 0.08335970202460885, 0.09075451013632119, 0.0828078668564558, 0.0815802679862827, 0.08806540095247328, 0.0829521119594574, 0.08340814989060163, 0.08894365094602108, 0.08431104989722371, 0.09056340320967138, 0.08439316996373236, 0.09225691086612642, 0.21504150214605033, 0.08172729285433888, 0.09246477787382901, 0.09292462514713407, 0.0815810770727694, 0.08912649191915989, 0.08133404492400587, 0.08422476006671786, 0.08559111692011356, 0.08433506591245532, 0.09098736895248294, 0.08515111915767193, 0.08235825411975384, 0.08397795399650931, 0.08750092005357146, 0.08031795104034245, 0.0806394258979708, 0.08020767103880644, 0.09038139693439007, 0.09383490006439388, 0.0876111991237849, 0.08012059493921697, 0.08372850995510817, 0.07632787898182869, 0.08655928797088563, 0.083849007030949, 0.08815013198181987, 0.08080439805053174, 0.08191174292005599, 0.08484356408007443, 0.08602913306094706, 0.08166135707870126, 0.081294174073264, 0.08829266298562288, 0.08728120103478432, 0.07839274196885526, 0.08222551480866969, 0.08677568193525076, 0.08073866693302989, 0.07968996418640018, 0.08031068299897015, 0.0793565260246396, 0.083883993094787, 0.07896813796833158, 0.08043512189760804, 3.1389988330192864, 0.08781717019155622, 0.08853760408237576, 0.08850313094444573, 0.08148031705059111, 0.09099357784725726, 0.07925136294215918, 0.08570395014248788, 0.07912908890284598, 0.07772012799978256, 0.0783867840655148, 0.07783869816921651, 0.08104374492540956, 0.08878597291186452, 0.09304617205634713, 0.08039137814193964, 0.07801122707314789, 0.08217786019667983, 0.08006361313164234, 0.08919504680670798, 0.08655560482293367, 0.08136654179543257, 0.09288546000607312, 0.0808763571549207, 0.08227156801149249, 0.08006966300308704, 0.07933972892351449, 0.07949276501312852, 0.08585914387367666, 0.08646708191372454, 0.08286619605496526, 0.0944470230024308, 0.10138511913828552, 0.08402592013590038, 0.09071871591731906, 0.08441196009516716, 0.07608430879190564, 0.07916632993146777, 0.08422963903285563, 0.07862806296907365, 0.08678169501945376, 0.08559981896542013, 0.08104844205081463, 0.08974122698418796, 0.0834859109017998, 0.08554472797550261, 0.08711268613114953, 0.0874496279284358, 0.0771687210071832, 0.07773052318952978, 0.07603398896753788, 0.08362016105093062, 0.22728594904765487, 0.08062042994424701, 0.07503805006854236, 0.07535188505426049, 0.08291839389130473, 0.08075170894153416, 0.07920359820127487, 0.07549891504459083, 0.08223382500000298, 0.08598316111601889, 0.09102195315063, 0.09403719496913254]
[0.0016388688379024364, 0.0016785517325434759, 0.001822651511210264, 0.0018396706757496815, 0.0018591423880080788, 0.0024171573661115704, 0.0019676126112059063, 0.0016178879981898532, 0.0017978620183254992, 0.0016320483255371147, 0.0018021871231268256, 0.0017472914258512308, 0.0016626792826822826, 0.0017995248187561424, 0.00178569860813417, 0.0016247224091191072, 0.005071448817925185, 0.0018239298987449432, 0.00169031638937185, 0.0019634753057010926, 0.0016731007727889382, 0.0018872291666968744, 0.001662064956653179, 0.0018238621022628279, 0.001768319368628519, 0.001862274549369301, 0.002525241817442738, 0.0015599093475968254, 0.0016927334518000788, 0.0016206113897187977, 0.0019080244926545694, 0.001704253018738664, 0.0016626960797501461, 0.001661504490529092, 0.0016739598798508548, 0.0016627666511934024, 0.0016597467332090043, 0.001808810613251158, 0.0019333056525840443, 0.0017329215936894929, 0.0017826215326025778, 0.0016476605515166813, 0.0018295915702319875, 0.0018528463100368272, 0.0017482901600246526, 0.0017786818765559975, 0.0015534391218074122, 0.0017301086098791994, 0.0017294052855244704, 0.0015540055702535473, 0.00158974967364754, 0.001724270367234641, 0.0015571916105263696, 0.0017970756304507352, 0.0015794340407057684, 0.0016927388734279238, 0.0016249423865609023, 0.0015759015721934183, 0.0017492516317917984, 0.0017523977534883485, 0.0015981438978366097, 0.0016391932850285452, 0.001654546123416144, 0.0016673123273922472, 0.0017091710196465862, 0.0017978940824312823, 0.0016913463656163337, 0.0016619181212950119, 0.0016603781841695309, 0.0020364007760523533, 0.001639811816264172, 0.0016224205503430292, 0.0016367745120078325, 0.0016324920627307526, 0.0017509287536828493, 0.0017820395320197757, 0.0016289523051937623, 0.0016466382016636888, 0.0016428048779465714, 0.0016494006944858298, 0.0017755864881815351, 0.0017860989390435268, 0.0016374054687971972, 0.0016680043855948107, 0.001626880649401217, 0.0016579391263729455, 0.0018078154285571405, 0.0019056486317469757, 0.0017127308766452633, 0.0017277027328251576, 0.0017388657315121014, 0.0018504722880161538, 0.0017225955118786317, 0.0016230685702924217, 0.0016683222231816273, 0.001767009244851616, 0.0018889005295932293, 0.0016447618575196486, 0.0016583652254574153, 0.0016713791042186168, 0.0016487711631902019, 0.0016471016724422878, 0.0017980572016795678, 0.0016749543232881293, 0.0017974584040289022, 0.001632667227401113, 0.0016581692865916661, 0.0015907832041230736, 0.0016488614111986695, 0.0016287162624375553, 0.0016788626327274405, 0.0016501208396666512, 0.0016871528766517127, 0.001758606220614545, 0.003530542594285644, 0.0017310438383066533, 0.0016418836526192573, 0.0017882632851904752, 0.0017436050847933, 0.0016167540987003215, 0.0017878070606717042, 0.0016618762640472576, 0.004573442654836239, 0.0016329641624981044, 0.0016590691200096387, 0.001675133531609056, 0.0016791095472492126, 0.0017060115505770153, 0.0016824986538564672, 0.0043449216351217155, 0.0016413768800925843, 0.0017779776731467977, 0.0016198594275177742, 0.0016393992641218463, 0.0016427866743915543, 0.0017718184900907229, 0.0016848350808556592, 0.001608439573866068, 0.0018594993031298627, 0.00170621685968826, 0.0017248306718027713, 0.0017235680027123616, 0.001619536206316279, 0.001625450693869165, 0.0016087832081378723, 0.0017140667115775297, 0.0017644309163169594, 0.0018734164079841301, 0.0019359966711502295, 0.0015311113663245828, 0.001555146022262622, 0.0016111537567054738, 0.0016281325892754355, 0.0016055547356681557, 0.0015858731004504524, 0.001538170387550276, 0.0017012184086654867, 0.0018521328599249221, 0.0016899564664582818, 0.0016649034282914838, 0.00179725308066272, 0.0016929002440705591, 0.0017022071406245232, 0.0018151765499187975, 0.0017206336713719125, 0.001848232718564722, 0.0017223095910965788, 0.0018827940993087025, 0.004388602084613272, 0.0016679039358028344, 0.0018870362831393676, 0.0018964209213700829, 0.0016649199402606, 0.0018189079983502018, 0.0016598784678368544, 0.0017188726544228134, 0.0017467574881655829, 0.0017211237941317412, 0.001856885080662917, 0.0017377779419933046, 0.001680780696321507, 0.0017138357958471288, 0.001785733062317785, 0.0016391418579661725, 0.0016457025693463428, 0.0016368912456899273, 0.0018445183047834709, 0.0019149979604978341, 0.001787983655587447, 0.0016351141824329995, 0.0017087451011246564, 0.0015577118159556876, 0.0017665160810384824, 0.001711204225121408, 0.0017989822853432627, 0.0016490693479700356, 0.0016716682228582855, 0.001731501307756621, 0.0017556965930805523, 0.001666558307728597, 0.0016590647770053878, 0.0018018910813392425, 0.001781249000709884, 0.0015998518769154136, 0.0016780717307891772, 0.0017709322843928726, 0.0016477278965924466, 0.0016263257997224526, 0.0016389935305912275, 0.0016195209392783592, 0.0017119182264242246, 0.00161159465241493, 0.0016415330999511846, 0.06406120067386299, 0.0017921871467664534, 0.0018068898792321585, 0.0018061863458050148, 0.0016628636132773695, 0.0018570117928011685, 0.0016173747539216159, 0.0017490602069895487, 0.0016148793653642036, 0.0015861250612200523, 0.0015997302870513225, 0.0015885448605962554, 0.0016539539780695827, 0.0018119586308543779, 0.0018989014705376966, 0.001640640370243666, 0.0015920658586356714, 0.0016770991876873436, 0.0016339512884008642, 0.001820307077687918, 0.0017664409147537484, 0.0016605416692945423, 0.0018956216327770023, 0.0016505379011208306, 0.0016790115920712753, 0.0016340747551650417, 0.001619178141296214, 0.0016223013267985412, 0.0017522274259934012, 0.0017646343247698887, 0.0016911468582645971, 0.0019274902653557305, 0.002069084064046643, 0.0017148146966510281, 0.0018514023656595725, 0.0017226930631666767, 0.0015527409957531765, 0.001615639386356485, 0.001718972225160319, 0.0016046543463076257, 0.0017710550003970156, 0.0017469350809269414, 0.001654049837771727, 0.0018314536119222033, 0.0017037941000367306, 0.0017458107750102573, 0.001777809921043868, 0.001784686284253792, 0.001574871857289453, 0.0015863372079495872, 0.0015517140605619975, 0.001706533898998584, 0.004638488756074589, 0.0016453148968213675, 0.0015313887769090278, 0.0015377935725359284, 0.0016922121202307089, 0.0016479940600313094, 0.001616399963291324, 0.0015407941845834864, 0.001678241326530673, 0.0017547583901228346, 0.001857590880625102, 0.0019191264279414804]
[610.1769567355279, 595.7516712843399, 548.6512335734367, 543.5755503318503, 537.882416349734, 413.7091006237127, 508.23012330009516, 618.0897572136224, 556.2162111480527, 612.7269544367783, 554.881336775386, 572.3143748117624, 601.4389007041519, 555.7022551604564, 560.0049165322887, 615.4897565191955, 197.1823113871268, 548.266685407211, 591.6052203526328, 509.30103225460874, 597.6926293166862, 529.8773554619503, 601.6612022274102, 548.2870655403831, 565.5087071605083, 536.9777513947507, 396.0016791630197, 641.0628935204383, 590.7604643463415, 617.0510748869389, 524.1022868677824, 586.7673338434869, 601.43282478315, 601.8641572744461, 597.3858824436684, 601.4072986623103, 602.5015624321006, 552.8494761552726, 517.2487850865207, 577.0601530049266, 560.9715700786065, 606.9211277040614, 546.5700740374544, 539.7101716332446, 571.98743255862, 562.2140829006853, 643.7329831351931, 577.9983951815734, 578.233458848678, 643.4983369054737, 629.0298507849909, 579.955451884141, 642.1817284656287, 556.4596075175667, 633.1381838225742, 590.758572215527, 615.4064342653052, 634.5573972669818, 571.6730411025412, 570.6467027873013, 625.7258819770168, 610.0561838151904, 604.3953600612224, 599.76765214952, 585.0789584571677, 556.2062914449918, 591.2449515540809, 601.7143607657234, 602.2724277723323, 491.0624724562034, 609.8260727735243, 616.3630014354598, 610.9577053306501, 612.5604055478524, 571.12546578302, 561.1547791347778, 613.8915159219785, 607.2979474116689, 608.7150174827535, 606.2808166282066, 563.194193386856, 559.8793986941786, 610.722279274283, 599.5188074061326, 614.6732400855932, 603.1584538255566, 553.1538143792275, 524.7557095996572, 583.862890332605, 578.8032750083051, 575.087530841389, 540.4025807228248, 580.5193344021999, 616.1169147769488, 599.4045910944698, 565.9279955176322, 529.4085021064333, 607.9907528425001, 603.0034787567238, 598.3083056835918, 606.5123058466787, 607.1270624825612, 556.1558325652256, 597.031206222319, 556.341107954741, 612.4946855164138, 603.0747331326345, 628.6211706335273, 606.4791092861054, 613.9804845463927, 595.6413470084944, 606.0162237585065, 592.7145155835424, 568.6321293976487, 283.24258192453163, 577.6861208657909, 609.0565542842967, 559.2017731849178, 573.5243655351851, 618.5232502604332, 559.3444740196327, 601.7295159897437, 218.65366540510541, 612.3833106479218, 602.7476419995029, 596.9673349201279, 595.553757429491, 586.1625026288804, 594.3541159500442, 230.1537482095432, 609.2445995362098, 562.4367589667902, 617.3375189304983, 609.9795345069011, 608.7217625930489, 564.3918976987292, 593.5299017469061, 621.7205894756654, 537.7791743814181, 586.0919696823933, 579.7670556001956, 580.1917872844646, 617.4607249284986, 615.2139857405551, 621.5877906616615, 583.4078646096899, 566.7549750756928, 533.7841580431333, 516.5298137655744, 653.1203555757604, 643.0264333281542, 620.6732261511925, 614.2005918848587, 622.837688298336, 630.5674771303954, 650.123034544061, 587.8140013688458, 539.9180704782354, 591.731219027047, 600.6354380723424, 556.4046659646061, 590.7022599249626, 587.472567899762, 550.9105987760448, 581.1812337734099, 541.0574057884701, 580.6157064731372, 531.1255226299922, 227.8629916132214, 599.5549135260339, 529.931516916225, 527.309095112462, 600.6294812250708, 549.7804181998357, 602.4537454860747, 581.7766647383157, 572.4893162188096, 581.0157313550313, 538.5362887632201, 575.4475159541718, 594.9616164610661, 583.486470771088, 559.9941117190573, 610.0753239507824, 607.6432148958669, 610.914135336165, 542.1469645525641, 522.1937676320209, 559.2892288892021, 611.5780847255761, 585.224794114595, 641.9672687572701, 566.085987404162, 584.3837838403251, 555.8698427145383, 606.4026362693453, 598.2048269662982, 577.5334939224657, 569.5744947852271, 600.0390117540684, 602.7492198375763, 554.9725010330576, 561.4038237222693, 625.0578659369672, 595.9220822638564, 564.6743293422026, 606.8963219400677, 614.8829466830442, 610.1305351945313, 617.4665456598474, 584.140050946682, 620.503424047435, 609.1866195264279, 15.610072703617007, 557.9774421461766, 553.4371582317744, 553.6527293114384, 601.3722304194755, 538.4995420473728, 618.2858966824604, 571.7356075015749, 619.2413015163335, 630.4673095769617, 625.1053743835995, 629.5069310316192, 604.6117444979653, 551.8889796774655, 526.6202673047792, 609.5180992355326, 628.1147193602625, 596.2676550925783, 612.0133489283469, 549.3578595926575, 566.110075716518, 602.2131323117209, 527.5314349177604, 605.8630942803131, 595.588502618003, 611.9671066694864, 617.597270180205, 616.4082981879857, 570.7021732256381, 566.689645533446, 591.3147016848491, 518.809364682028, 483.30564106913783, 583.1533879158864, 540.1311019950783, 580.4864612165948, 644.0224111651909, 618.9500011231798, 581.7429655716139, 623.1871694368574, 564.6352031844472, 572.4311171708739, 604.5767045007312, 546.0143754066746, 586.9253802313565, 572.7997640489558, 562.4898298535959, 560.3225669536185, 634.9722965531445, 630.3829948567778, 644.4486296900741, 585.9830857077101, 215.58745802507224, 607.7863890565445, 653.0020430333904, 650.2823381885584, 590.9424640355752, 606.7983036182798, 618.6587618845236, 649.0159490511862, 595.861860979934, 569.879024730008, 538.3316694919862, 521.07041278809]
Elapsed: 0.09797197192846814~0.1841840082663276
Time per graph: 0.001999427998540166~0.0037588573115577063
Speed: 575.5331663978591~71.53581621054451
Total Time: 0.0950
best val loss: 0.04106235131621361 test_score: 0.9184

Testing...
Test loss: 0.5787 score: 0.8776 time: 0.07s
test Score 0.8776
Epoch Time List: [1.04869039892219, 0.4112613620236516, 0.4221711193677038, 0.3240542740095407, 0.33902854775078595, 0.35985556710511446, 0.4841554563026875, 0.30481399898417294, 0.30977859697304666, 0.4460116771515459, 0.31705206213518977, 0.3189725549891591, 0.30861121299676597, 0.30697344709187746, 0.30806554621085525, 0.2988910642452538, 0.47592021501623094, 0.314807518851012, 0.31602018792182207, 0.3222094960510731, 0.31568995397537947, 0.3179572082590312, 0.33699335996061563, 0.3109961508307606, 0.38816100265830755, 0.31433799979276955, 0.404101180145517, 0.30125720403157175, 0.48263153806328773, 0.3004446930717677, 0.3140702289529145, 0.4606619228143245, 0.30990164703689516, 0.3124753732699901, 0.30621600919403136, 0.3139397331979126, 0.3110068391542882, 0.3188385658431798, 0.3192756848875433, 0.4045541079249233, 0.3037626910954714, 0.30614277301356196, 0.3202781272120774, 0.31957897916436195, 0.3291618369985372, 0.3183686910197139, 0.4277606999967247, 0.3008091184310615, 0.3004341658670455, 0.2925573440734297, 0.3021396119147539, 0.30054259300231934, 0.30382046778686345, 0.3053547809831798, 0.3040248742327094, 0.33928753016516566, 0.3080075499601662, 0.2996388857718557, 0.30009892489761114, 0.3051292500458658, 0.30196176981553435, 0.3161864229477942, 0.31264831428416073, 0.30872260709293187, 0.31519400095567107, 0.3158708040136844, 0.31783738802187145, 0.31403106707148254, 0.31622310634702444, 0.3515003868378699, 0.3101374839898199, 0.3046731890644878, 0.3096503510605544, 0.3096352522261441, 0.31398171302862465, 0.3396198321133852, 0.32327646808698773, 0.3124328644480556, 0.3113052567932755, 0.3087417888455093, 0.31169284391216934, 0.3099166229367256, 0.30713486671447754, 0.3073511519469321, 0.3126804078929126, 0.31085862102918327, 0.31728261918760836, 0.3533439680468291, 0.3125948163215071, 0.3216722388751805, 0.3190305430907756, 0.3232168359681964, 0.31441419781185687, 0.4496344532817602, 0.30654514906927943, 0.31183543917723, 0.3207151792012155, 0.3115375570487231, 0.3097281991504133, 0.31569563201628625, 0.44637257209979, 0.30404098075814545, 0.310074700973928, 0.3062147847376764, 0.3171906592324376, 0.3097621058113873, 0.3521153738256544, 0.5615558652207255, 0.30370283173397183, 0.30624680570326746, 0.3033813638612628, 0.3030554736033082, 0.30792759894393384, 0.31900586606934667, 0.401575047057122, 0.3001402411609888, 0.3107679490931332, 0.33484852011315525, 0.3063456437084824, 0.30801156093366444, 0.31248284969478846, 0.30797904287464917, 0.44901884789578617, 0.3117685930337757, 0.3063949029892683, 0.2928837148938328, 0.30409448593854904, 0.34087924007326365, 0.31592338322661817, 0.4564248670358211, 0.3108602608554065, 0.3117180778644979, 0.30717891082167625, 0.3104665938299149, 0.30697820405475795, 0.3100914820097387, 0.29955253982916474, 0.3006022642366588, 0.4277121899649501, 0.30923199909739196, 0.31422792188823223, 0.3271856892388314, 0.30121732922270894, 0.3176588991191238, 0.3134043049067259, 0.31896991841495037, 0.4550726201850921, 0.3143497759010643, 0.33021935704164207, 0.2981805391609669, 0.2888390081934631, 0.3110637969803065, 0.306721534114331, 0.4337967161554843, 0.30063633574172854, 0.2936395010910928, 0.2977746119722724, 0.31950694182887673, 0.31675960775464773, 0.31071410002186894, 0.3137172288261354, 0.45198657270520926, 0.31058384384959936, 0.3151454741600901, 0.31828061025589705, 0.32270830124616623, 0.31873652269132435, 0.3222151619847864, 0.4497459710109979, 0.309152748901397, 0.3301563661079854, 0.3173754771705717, 0.3164607468061149, 0.3135131699964404, 0.3114147160667926, 0.31441204994916916, 0.4577474088873714, 0.32146723312325776, 0.31701406789943576, 0.32164829201065004, 0.31953907571733, 0.32091404194943607, 0.3140860132407397, 0.4403564832173288, 0.3128592041321099, 0.3033284561242908, 0.31224119081161916, 0.33661800785921514, 0.31722093489952385, 0.3105383387301117, 0.31242826161906123, 0.372133731842041, 0.30671786377206445, 0.31966201215982437, 0.3123604911379516, 0.3238706060219556, 0.31357460701838136, 0.31316943001002073, 0.31665252335369587, 0.369372014189139, 0.29800919094122946, 0.30900686979293823, 0.31932580401189625, 0.30763329192996025, 0.30642817215994, 0.31071882508695126, 0.30837933090515435, 0.43983968324027956, 0.303556009195745, 0.3070426101330668, 0.3087505269795656, 0.30382634815759957, 0.3081385991536081, 6.633971838047728, 0.7178552888799459, 0.31554126809351146, 0.31551897595636547, 0.3139108100440353, 0.3240873138420284, 0.31512109795585275, 0.3070659111253917, 0.30526094301603734, 0.30539823113940656, 0.2991426058579236, 0.29925431008450687, 0.30408612359315157, 0.3211152988951653, 0.32525043725036085, 0.30432423390448093, 0.3060405838768929, 0.3078686259686947, 0.3123359817545861, 0.31176892993971705, 0.3087583847809583, 0.3136061728000641, 0.3298565912991762, 0.31491189170628786, 0.3109215300064534, 0.30966871697455645, 0.30831088521517813, 0.30480991094373167, 0.305850462988019, 0.3063270940911025, 0.3220037631690502, 0.3144272689241916, 0.3436102659907192, 0.3220976460725069, 0.3242546839173883, 0.30448475712910295, 0.30514204199425876, 0.3060695657040924, 0.40866007772274315, 0.3061901112087071, 0.3074364000931382, 0.30627069203183055, 0.3112816601060331, 0.31638680514879525, 0.3383650602772832, 0.3858680890407413, 0.3114037709310651, 0.3166159139946103, 0.3002663420047611, 0.3036463870666921, 0.2968277789186686, 0.3125814569648355, 0.47100821509957314, 0.3089855208527297, 0.29618678987026215, 0.29793122387491167, 0.3014040240086615, 0.2962248679250479, 0.3062764508649707, 0.30236075399443507, 0.3987325159832835, 0.3286707659717649, 0.3219403540715575, 0.31871160166338086]
Total Epoch List: [277]
Total Time List: [0.09498926601372659]
========================k_idx:1========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: SGFormer
SGFormer(
  (trans_conv): SGModule(
    (attns): ModuleList(
      (0-1): 2 x SGFormerAttention(heads=1, head_channels=64)
    )
    (fcs): ModuleList(
      (0): Linear(in_features=14887, out_features=64, bias=True)
    )
    (bns): ModuleList(
      (0-2): 3 x LayerNorm((64,), eps=1e-05, elementwise_affine=True)
    )
  )
  (graph_conv): GraphModule(
    (convs): ModuleList(
      (0-2): 3 x GCNConv(64, 64)
    )
    (fcs): ModuleList(
      (0): Linear(in_features=14887, out_features=64, bias=True)
    )
    (bns): ModuleList(
      (0-3): 4 x BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (classifier): Linear(in_features=64, out_features=2, bias=True)
)
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7f342fda2890>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.6773;  Loss pred: 0.6773; Loss self: 0.0000; time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7196 score: 0.5102 time: 0.09s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7317 score: 0.4898 time: 0.09s
Epoch 2/1000, LR 0.000000
Train loss: 0.6565;  Loss pred: 0.6565; Loss self: 0.0000; time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7172 score: 0.5102 time: 0.09s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7287 score: 0.4898 time: 0.10s
Epoch 3/1000, LR 0.000030
Train loss: 0.7167;  Loss pred: 0.7167; Loss self: 0.0000; time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7124 score: 0.5102 time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7226 score: 0.4898 time: 0.10s
Epoch 4/1000, LR 0.000060
Train loss: 0.6616;  Loss pred: 0.6616; Loss self: 0.0000; time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7054 score: 0.5102 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7136 score: 0.4898 time: 0.09s
Epoch 5/1000, LR 0.000090
Train loss: 0.6481;  Loss pred: 0.6481; Loss self: 0.0000; time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6971 score: 0.5102 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7026 score: 0.4898 time: 0.09s
Epoch 6/1000, LR 0.000120
Train loss: 0.6539;  Loss pred: 0.6539; Loss self: 0.0000; time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6879 score: 0.5102 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6899 score: 0.4898 time: 0.09s
Epoch 7/1000, LR 0.000150
Train loss: 0.6376;  Loss pred: 0.6376; Loss self: 0.0000; time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6775 score: 0.5102 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6755 score: 0.4898 time: 0.09s
Epoch 8/1000, LR 0.000180
Train loss: 0.5994;  Loss pred: 0.5994; Loss self: 0.0000; time: 0.15s
Val loss: 0.6679 score: 0.5510 time: 0.08s
Test loss: 0.6614 score: 0.5714 time: 0.09s
Epoch 9/1000, LR 0.000210
Train loss: 0.5686;  Loss pred: 0.5686; Loss self: 0.0000; time: 0.15s
Val loss: 0.6598 score: 0.5306 time: 0.09s
Test loss: 0.6486 score: 0.5918 time: 0.09s
Epoch 10/1000, LR 0.000240
Train loss: 0.5450;  Loss pred: 0.5450; Loss self: 0.0000; time: 0.16s
Val loss: 0.6531 score: 0.7143 time: 0.16s
Test loss: 0.6374 score: 0.7143 time: 0.08s
Epoch 11/1000, LR 0.000270
Train loss: 0.5395;  Loss pred: 0.5395; Loss self: 0.0000; time: 0.15s
Val loss: 0.6474 score: 0.8367 time: 0.08s
Test loss: 0.6277 score: 0.9592 time: 0.08s
Epoch 12/1000, LR 0.000270
Train loss: 0.5545;  Loss pred: 0.5545; Loss self: 0.0000; time: 0.16s
Val loss: 0.6421 score: 0.8571 time: 0.08s
Test loss: 0.6188 score: 0.9388 time: 0.09s
Epoch 13/1000, LR 0.000270
Train loss: 0.5085;  Loss pred: 0.5085; Loss self: 0.0000; time: 0.15s
Val loss: 0.6363 score: 0.8571 time: 0.08s
Test loss: 0.6095 score: 0.9388 time: 0.09s
Epoch 14/1000, LR 0.000270
Train loss: 0.4837;  Loss pred: 0.4837; Loss self: 0.0000; time: 0.17s
Val loss: 0.6299 score: 0.8571 time: 0.08s
Test loss: 0.5998 score: 0.9388 time: 0.09s
Epoch 15/1000, LR 0.000270
Train loss: 0.4882;  Loss pred: 0.4882; Loss self: 0.0000; time: 0.20s
Val loss: 0.6228 score: 0.8571 time: 0.08s
Test loss: 0.5895 score: 0.9388 time: 0.09s
Epoch 16/1000, LR 0.000270
Train loss: 0.4502;  Loss pred: 0.4502; Loss self: 0.0000; time: 0.16s
Val loss: 0.6148 score: 0.8571 time: 0.09s
Test loss: 0.5791 score: 0.9592 time: 0.10s
Epoch 17/1000, LR 0.000270
Train loss: 0.4328;  Loss pred: 0.4328; Loss self: 0.0000; time: 0.16s
Val loss: 0.6064 score: 0.8776 time: 0.08s
Test loss: 0.5689 score: 0.9796 time: 0.23s
Epoch 18/1000, LR 0.000270
Train loss: 0.4225;  Loss pred: 0.4225; Loss self: 0.0000; time: 0.16s
Val loss: 0.5983 score: 0.8571 time: 0.08s
Test loss: 0.5599 score: 0.9592 time: 0.09s
Epoch 19/1000, LR 0.000270
Train loss: 0.4262;  Loss pred: 0.4262; Loss self: 0.0000; time: 0.16s
Val loss: 0.5909 score: 0.7959 time: 0.09s
Test loss: 0.5525 score: 0.8776 time: 0.09s
Epoch 20/1000, LR 0.000270
Train loss: 0.4007;  Loss pred: 0.4007; Loss self: 0.0000; time: 0.16s
Val loss: 0.5850 score: 0.7143 time: 0.08s
Test loss: 0.5478 score: 0.7755 time: 0.09s
Epoch 21/1000, LR 0.000270
Train loss: 0.3731;  Loss pred: 0.3731; Loss self: 0.0000; time: 0.17s
Val loss: 0.5816 score: 0.6939 time: 0.08s
Test loss: 0.5462 score: 0.7551 time: 0.09s
Epoch 22/1000, LR 0.000270
Train loss: 0.3573;  Loss pred: 0.3573; Loss self: 0.0000; time: 0.16s
Val loss: 0.5806 score: 0.6122 time: 0.08s
Test loss: 0.5478 score: 0.6939 time: 0.09s
Epoch 23/1000, LR 0.000270
Train loss: 0.3579;  Loss pred: 0.3579; Loss self: 0.0000; time: 0.16s
Val loss: 0.5830 score: 0.5918 time: 0.08s
Test loss: 0.5538 score: 0.6531 time: 0.09s
     INFO: Early stopping counter 1 of 20
Epoch 24/1000, LR 0.000270
Train loss: 0.3466;  Loss pred: 0.3466; Loss self: 0.0000; time: 0.15s
Val loss: 0.5878 score: 0.6122 time: 0.08s
Test loss: 0.5628 score: 0.6531 time: 0.23s
     INFO: Early stopping counter 2 of 20
Epoch 25/1000, LR 0.000270
Train loss: 0.3333;  Loss pred: 0.3333; Loss self: 0.0000; time: 0.16s
Val loss: 0.5944 score: 0.6122 time: 0.09s
Test loss: 0.5735 score: 0.6122 time: 0.10s
     INFO: Early stopping counter 3 of 20
Epoch 26/1000, LR 0.000270
Train loss: 0.3206;  Loss pred: 0.3206; Loss self: 0.0000; time: 0.18s
Val loss: 0.6000 score: 0.6122 time: 0.10s
Test loss: 0.5820 score: 0.5918 time: 0.10s
     INFO: Early stopping counter 4 of 20
Epoch 27/1000, LR 0.000270
Train loss: 0.3138;  Loss pred: 0.3138; Loss self: 0.0000; time: 0.16s
Val loss: 0.6047 score: 0.6122 time: 0.09s
Test loss: 0.5893 score: 0.5918 time: 0.09s
     INFO: Early stopping counter 5 of 20
Epoch 28/1000, LR 0.000270
Train loss: 0.3103;  Loss pred: 0.3103; Loss self: 0.0000; time: 0.16s
Val loss: 0.6073 score: 0.6122 time: 0.09s
Test loss: 0.5938 score: 0.5918 time: 0.09s
     INFO: Early stopping counter 6 of 20
Epoch 29/1000, LR 0.000270
Train loss: 0.3047;  Loss pred: 0.3047; Loss self: 0.0000; time: 0.15s
Val loss: 0.6081 score: 0.6122 time: 0.10s
Test loss: 0.5954 score: 0.5918 time: 0.10s
     INFO: Early stopping counter 7 of 20
Epoch 30/1000, LR 0.000270
Train loss: 0.2853;  Loss pred: 0.2853; Loss self: 0.0000; time: 0.15s
Val loss: 0.6093 score: 0.6122 time: 0.08s
Test loss: 0.5969 score: 0.5918 time: 0.10s
     INFO: Early stopping counter 8 of 20
Epoch 31/1000, LR 0.000270
Train loss: 0.2845;  Loss pred: 0.2845; Loss self: 0.0000; time: 0.16s
Val loss: 0.6082 score: 0.5918 time: 0.09s
Test loss: 0.5955 score: 0.5918 time: 0.24s
     INFO: Early stopping counter 9 of 20
Epoch 32/1000, LR 0.000270
Train loss: 0.2696;  Loss pred: 0.2696; Loss self: 0.0000; time: 0.17s
Val loss: 0.6066 score: 0.5918 time: 0.09s
Test loss: 0.5931 score: 0.5918 time: 0.09s
     INFO: Early stopping counter 10 of 20
Epoch 33/1000, LR 0.000270
Train loss: 0.2630;  Loss pred: 0.2630; Loss self: 0.0000; time: 0.15s
Val loss: 0.6032 score: 0.5918 time: 0.08s
Test loss: 0.5881 score: 0.6122 time: 0.09s
     INFO: Early stopping counter 11 of 20
Epoch 34/1000, LR 0.000270
Train loss: 0.2557;  Loss pred: 0.2557; Loss self: 0.0000; time: 0.15s
Val loss: 0.5985 score: 0.5918 time: 0.07s
Test loss: 0.5813 score: 0.6122 time: 0.08s
     INFO: Early stopping counter 12 of 20
Epoch 35/1000, LR 0.000270
Train loss: 0.2404;  Loss pred: 0.2404; Loss self: 0.0000; time: 0.15s
Val loss: 0.5927 score: 0.5918 time: 0.08s
Test loss: 0.5732 score: 0.6327 time: 0.08s
     INFO: Early stopping counter 13 of 20
Epoch 36/1000, LR 0.000270
Train loss: 0.2447;  Loss pred: 0.2447; Loss self: 0.0000; time: 0.16s
Val loss: 0.5862 score: 0.5918 time: 0.08s
Test loss: 0.5644 score: 0.6531 time: 0.09s
     INFO: Early stopping counter 14 of 20
Epoch 37/1000, LR 0.000270
Train loss: 0.2410;  Loss pred: 0.2410; Loss self: 0.0000; time: 0.16s
Val loss: 0.5808 score: 0.5918 time: 0.08s
Test loss: 0.5573 score: 0.6735 time: 0.08s
     INFO: Early stopping counter 15 of 20
Epoch 38/1000, LR 0.000270
Train loss: 0.2303;  Loss pred: 0.2303; Loss self: 0.0000; time: 0.15s
Val loss: 0.5751 score: 0.5918 time: 0.08s
Test loss: 0.5499 score: 0.6735 time: 0.09s
Epoch 39/1000, LR 0.000269
Train loss: 0.2316;  Loss pred: 0.2316; Loss self: 0.0000; time: 0.17s
Val loss: 0.5717 score: 0.5918 time: 0.13s
Test loss: 0.5461 score: 0.6735 time: 0.09s
Epoch 40/1000, LR 0.000269
Train loss: 0.2206;  Loss pred: 0.2206; Loss self: 0.0000; time: 0.16s
Val loss: 0.5674 score: 0.5918 time: 0.08s
Test loss: 0.5413 score: 0.6735 time: 0.09s
Epoch 41/1000, LR 0.000269
Train loss: 0.2154;  Loss pred: 0.2154; Loss self: 0.0000; time: 0.15s
Val loss: 0.5645 score: 0.5918 time: 0.07s
Test loss: 0.5381 score: 0.6735 time: 0.11s
Epoch 42/1000, LR 0.000269
Train loss: 0.2050;  Loss pred: 0.2050; Loss self: 0.0000; time: 0.15s
Val loss: 0.5612 score: 0.5918 time: 0.08s
Test loss: 0.5346 score: 0.6735 time: 0.08s
Epoch 43/1000, LR 0.000269
Train loss: 0.2012;  Loss pred: 0.2012; Loss self: 0.0000; time: 0.15s
Val loss: 0.5595 score: 0.6122 time: 0.07s
Test loss: 0.5333 score: 0.6735 time: 0.08s
Epoch 44/1000, LR 0.000269
Train loss: 0.2016;  Loss pred: 0.2016; Loss self: 0.0000; time: 0.15s
Val loss: 0.5581 score: 0.6122 time: 0.08s
Test loss: 0.5327 score: 0.6735 time: 0.09s
Epoch 45/1000, LR 0.000269
Train loss: 0.1929;  Loss pred: 0.1929; Loss self: 0.0000; time: 0.17s
Val loss: 0.5580 score: 0.6122 time: 0.08s
Test loss: 0.5337 score: 0.6735 time: 0.09s
Epoch 46/1000, LR 0.000269
Train loss: 0.1821;  Loss pred: 0.1821; Loss self: 0.0000; time: 0.15s
Val loss: 0.5581 score: 0.6122 time: 0.10s
Test loss: 0.5349 score: 0.6735 time: 0.11s
     INFO: Early stopping counter 1 of 20
Epoch 47/1000, LR 0.000269
Train loss: 0.1781;  Loss pred: 0.1781; Loss self: 0.0000; time: 0.28s
Val loss: 0.5571 score: 0.6122 time: 0.09s
Test loss: 0.5347 score: 0.6735 time: 0.09s
Epoch 48/1000, LR 0.000269
Train loss: 0.1752;  Loss pred: 0.1752; Loss self: 0.0000; time: 0.16s
Val loss: 0.5560 score: 0.6122 time: 0.08s
Test loss: 0.5344 score: 0.6735 time: 0.09s
Epoch 49/1000, LR 0.000269
Train loss: 0.1691;  Loss pred: 0.1691; Loss self: 0.0000; time: 0.15s
Val loss: 0.5552 score: 0.6122 time: 0.08s
Test loss: 0.5349 score: 0.6735 time: 0.09s
Epoch 50/1000, LR 0.000269
Train loss: 0.1704;  Loss pred: 0.1704; Loss self: 0.0000; time: 0.15s
Val loss: 0.5532 score: 0.6122 time: 0.08s
Test loss: 0.5337 score: 0.6735 time: 0.09s
Epoch 51/1000, LR 0.000269
Train loss: 0.1656;  Loss pred: 0.1656; Loss self: 0.0000; time: 0.16s
Val loss: 0.5516 score: 0.6122 time: 0.08s
Test loss: 0.5330 score: 0.6735 time: 0.09s
Epoch 52/1000, LR 0.000269
Train loss: 0.1589;  Loss pred: 0.1589; Loss self: 0.0000; time: 0.16s
Val loss: 0.5493 score: 0.6122 time: 0.08s
Test loss: 0.5311 score: 0.6735 time: 0.09s
Epoch 53/1000, LR 0.000269
Train loss: 0.1608;  Loss pred: 0.1608; Loss self: 0.0000; time: 0.15s
Val loss: 0.5457 score: 0.6122 time: 0.09s
Test loss: 0.5273 score: 0.6735 time: 0.20s
Epoch 54/1000, LR 0.000269
Train loss: 0.1497;  Loss pred: 0.1497; Loss self: 0.0000; time: 0.16s
Val loss: 0.5420 score: 0.6122 time: 0.10s
Test loss: 0.5234 score: 0.6735 time: 0.09s
Epoch 55/1000, LR 0.000269
Train loss: 0.1488;  Loss pred: 0.1488; Loss self: 0.0000; time: 0.15s
Val loss: 0.5374 score: 0.6122 time: 0.09s
Test loss: 0.5184 score: 0.6735 time: 0.10s
Epoch 56/1000, LR 0.000269
Train loss: 0.1392;  Loss pred: 0.1392; Loss self: 0.0000; time: 0.17s
Val loss: 0.5336 score: 0.6122 time: 0.07s
Test loss: 0.5149 score: 0.6735 time: 0.09s
Epoch 57/1000, LR 0.000269
Train loss: 0.1407;  Loss pred: 0.1407; Loss self: 0.0000; time: 0.17s
Val loss: 0.5308 score: 0.6122 time: 0.08s
Test loss: 0.5130 score: 0.6735 time: 0.10s
Epoch 58/1000, LR 0.000269
Train loss: 0.1328;  Loss pred: 0.1328; Loss self: 0.0000; time: 0.16s
Val loss: 0.5263 score: 0.6122 time: 0.09s
Test loss: 0.5085 score: 0.6735 time: 0.09s
Epoch 59/1000, LR 0.000268
Train loss: 0.1320;  Loss pred: 0.1320; Loss self: 0.0000; time: 0.16s
Val loss: 0.5227 score: 0.6122 time: 0.09s
Test loss: 0.5055 score: 0.6735 time: 0.10s
Epoch 60/1000, LR 0.000268
Train loss: 0.1361;  Loss pred: 0.1361; Loss self: 0.0000; time: 0.17s
Val loss: 0.5171 score: 0.6327 time: 0.08s
Test loss: 0.4994 score: 0.6735 time: 0.23s
Epoch 61/1000, LR 0.000268
Train loss: 0.1308;  Loss pred: 0.1308; Loss self: 0.0000; time: 0.15s
Val loss: 0.5096 score: 0.6531 time: 0.08s
Test loss: 0.4906 score: 0.6735 time: 0.08s
Epoch 62/1000, LR 0.000268
Train loss: 0.1348;  Loss pred: 0.1348; Loss self: 0.0000; time: 0.15s
Val loss: 0.5005 score: 0.6939 time: 0.08s
Test loss: 0.4796 score: 0.6735 time: 0.11s
Epoch 63/1000, LR 0.000268
Train loss: 0.1233;  Loss pred: 0.1233; Loss self: 0.0000; time: 0.16s
Val loss: 0.4928 score: 0.6735 time: 0.08s
Test loss: 0.4706 score: 0.7143 time: 0.09s
Epoch 64/1000, LR 0.000268
Train loss: 0.1248;  Loss pred: 0.1248; Loss self: 0.0000; time: 0.17s
Val loss: 0.4860 score: 0.6735 time: 0.09s
Test loss: 0.4630 score: 0.7143 time: 0.09s
Epoch 65/1000, LR 0.000268
Train loss: 0.1254;  Loss pred: 0.1254; Loss self: 0.0000; time: 0.17s
Val loss: 0.4774 score: 0.6735 time: 0.08s
Test loss: 0.4525 score: 0.7143 time: 0.09s
Epoch 66/1000, LR 0.000268
Train loss: 0.1204;  Loss pred: 0.1204; Loss self: 0.0000; time: 0.17s
Val loss: 0.4706 score: 0.6939 time: 0.08s
Test loss: 0.4445 score: 0.7143 time: 0.09s
Epoch 67/1000, LR 0.000268
Train loss: 0.1162;  Loss pred: 0.1162; Loss self: 0.0000; time: 0.16s
Val loss: 0.4657 score: 0.7143 time: 0.08s
Test loss: 0.4396 score: 0.7143 time: 0.16s
Epoch 68/1000, LR 0.000268
Train loss: 0.1125;  Loss pred: 0.1125; Loss self: 0.0000; time: 0.16s
Val loss: 0.4631 score: 0.7143 time: 0.08s
Test loss: 0.4381 score: 0.7143 time: 0.09s
Epoch 69/1000, LR 0.000268
Train loss: 0.1192;  Loss pred: 0.1192; Loss self: 0.0000; time: 0.16s
Val loss: 0.4592 score: 0.7143 time: 0.08s
Test loss: 0.4348 score: 0.7143 time: 0.09s
Epoch 70/1000, LR 0.000268
Train loss: 0.1049;  Loss pred: 0.1049; Loss self: 0.0000; time: 0.16s
Val loss: 0.4555 score: 0.7143 time: 0.08s
Test loss: 0.4321 score: 0.7143 time: 0.09s
Epoch 71/1000, LR 0.000268
Train loss: 0.1017;  Loss pred: 0.1017; Loss self: 0.0000; time: 0.16s
Val loss: 0.4506 score: 0.7143 time: 0.08s
Test loss: 0.4278 score: 0.7143 time: 0.08s
Epoch 72/1000, LR 0.000267
Train loss: 0.1005;  Loss pred: 0.1005; Loss self: 0.0000; time: 0.16s
Val loss: 0.4446 score: 0.7551 time: 0.08s
Test loss: 0.4220 score: 0.7143 time: 0.09s
Epoch 73/1000, LR 0.000267
Train loss: 0.1031;  Loss pred: 0.1031; Loss self: 0.0000; time: 0.16s
Val loss: 0.4393 score: 0.7551 time: 0.08s
Test loss: 0.4171 score: 0.7347 time: 0.08s
Epoch 74/1000, LR 0.000267
Train loss: 0.0952;  Loss pred: 0.0952; Loss self: 0.0000; time: 0.16s
Val loss: 0.4340 score: 0.7551 time: 0.07s
Test loss: 0.4122 score: 0.7347 time: 0.09s
Epoch 75/1000, LR 0.000267
Train loss: 0.0951;  Loss pred: 0.0951; Loss self: 0.0000; time: 0.17s
Val loss: 0.4272 score: 0.7755 time: 0.13s
Test loss: 0.4049 score: 0.7551 time: 0.10s
Epoch 76/1000, LR 0.000267
Train loss: 0.0984;  Loss pred: 0.0984; Loss self: 0.0000; time: 0.17s
Val loss: 0.4189 score: 0.7755 time: 0.09s
Test loss: 0.3953 score: 0.7551 time: 0.09s
Epoch 77/1000, LR 0.000267
Train loss: 0.0910;  Loss pred: 0.0910; Loss self: 0.0000; time: 0.16s
Val loss: 0.4095 score: 0.7755 time: 0.08s
Test loss: 0.3836 score: 0.7755 time: 0.09s
Epoch 78/1000, LR 0.000267
Train loss: 0.0909;  Loss pred: 0.0909; Loss self: 0.0000; time: 0.16s
Val loss: 0.3991 score: 0.8163 time: 0.08s
Test loss: 0.3700 score: 0.7551 time: 0.09s
Epoch 79/1000, LR 0.000267
Train loss: 0.0907;  Loss pred: 0.0907; Loss self: 0.0000; time: 0.16s
Val loss: 0.3881 score: 0.8163 time: 0.08s
Test loss: 0.3550 score: 0.7551 time: 0.09s
Epoch 80/1000, LR 0.000267
Train loss: 0.0878;  Loss pred: 0.0878; Loss self: 0.0000; time: 0.16s
Val loss: 0.3790 score: 0.8367 time: 0.08s
Test loss: 0.3426 score: 0.7551 time: 0.09s
Epoch 81/1000, LR 0.000267
Train loss: 0.0870;  Loss pred: 0.0870; Loss self: 0.0000; time: 0.16s
Val loss: 0.3699 score: 0.8367 time: 0.08s
Test loss: 0.3306 score: 0.7551 time: 0.09s
Epoch 82/1000, LR 0.000267
Train loss: 0.0862;  Loss pred: 0.0862; Loss self: 0.0000; time: 0.17s
Val loss: 0.3608 score: 0.8367 time: 0.13s
Test loss: 0.3182 score: 0.7755 time: 0.09s
Epoch 83/1000, LR 0.000266
Train loss: 0.0827;  Loss pred: 0.0827; Loss self: 0.0000; time: 0.16s
Val loss: 0.3543 score: 0.8367 time: 0.08s
Test loss: 0.3107 score: 0.8163 time: 0.09s
Epoch 84/1000, LR 0.000266
Train loss: 0.0834;  Loss pred: 0.0834; Loss self: 0.0000; time: 0.16s
Val loss: 0.3491 score: 0.8367 time: 0.08s
Test loss: 0.3057 score: 0.8367 time: 0.09s
Epoch 85/1000, LR 0.000266
Train loss: 0.0813;  Loss pred: 0.0813; Loss self: 0.0000; time: 0.16s
Val loss: 0.3440 score: 0.8367 time: 0.08s
Test loss: 0.3010 score: 0.8776 time: 0.09s
Epoch 86/1000, LR 0.000266
Train loss: 0.0804;  Loss pred: 0.0804; Loss self: 0.0000; time: 0.16s
Val loss: 0.3384 score: 0.8571 time: 0.08s
Test loss: 0.2949 score: 0.8980 time: 0.09s
Epoch 87/1000, LR 0.000266
Train loss: 0.0817;  Loss pred: 0.0817; Loss self: 0.0000; time: 0.16s
Val loss: 0.3326 score: 0.8571 time: 0.08s
Test loss: 0.2884 score: 0.8980 time: 0.09s
Epoch 88/1000, LR 0.000266
Train loss: 0.0792;  Loss pred: 0.0792; Loss self: 0.0000; time: 0.17s
Val loss: 0.3264 score: 0.8571 time: 0.08s
Test loss: 0.2805 score: 0.8980 time: 0.09s
Epoch 89/1000, LR 0.000266
Train loss: 0.0787;  Loss pred: 0.0787; Loss self: 0.0000; time: 0.15s
Val loss: 0.3209 score: 0.8571 time: 0.18s
Test loss: 0.2726 score: 0.8980 time: 0.09s
Epoch 90/1000, LR 0.000266
Train loss: 0.0720;  Loss pred: 0.0720; Loss self: 0.0000; time: 0.16s
Val loss: 0.3154 score: 0.8571 time: 0.08s
Test loss: 0.2643 score: 0.8980 time: 0.09s
Epoch 91/1000, LR 0.000266
Train loss: 0.0744;  Loss pred: 0.0744; Loss self: 0.0000; time: 0.16s
Val loss: 0.3098 score: 0.8571 time: 0.08s
Test loss: 0.2545 score: 0.9388 time: 0.09s
Epoch 92/1000, LR 0.000266
Train loss: 0.0761;  Loss pred: 0.0761; Loss self: 0.0000; time: 0.16s
Val loss: 0.3046 score: 0.8571 time: 0.10s
Test loss: 0.2441 score: 0.9388 time: 0.09s
Epoch 93/1000, LR 0.000265
Train loss: 0.0679;  Loss pred: 0.0679; Loss self: 0.0000; time: 0.16s
Val loss: 0.2999 score: 0.8571 time: 0.08s
Test loss: 0.2335 score: 0.9388 time: 0.09s
Epoch 94/1000, LR 0.000265
Train loss: 0.0670;  Loss pred: 0.0670; Loss self: 0.0000; time: 0.16s
Val loss: 0.2957 score: 0.8776 time: 0.08s
Test loss: 0.2222 score: 0.9388 time: 0.09s
Epoch 95/1000, LR 0.000265
Train loss: 0.0689;  Loss pred: 0.0689; Loss self: 0.0000; time: 0.16s
Val loss: 0.2921 score: 0.8776 time: 0.08s
Test loss: 0.2109 score: 0.9388 time: 0.10s
Epoch 96/1000, LR 0.000265
Train loss: 0.0684;  Loss pred: 0.0684; Loss self: 0.0000; time: 0.16s
Val loss: 0.2892 score: 0.8776 time: 0.11s
Test loss: 0.1997 score: 0.9388 time: 0.09s
Epoch 97/1000, LR 0.000265
Train loss: 0.0689;  Loss pred: 0.0689; Loss self: 0.0000; time: 0.16s
Val loss: 0.2873 score: 0.8776 time: 0.09s
Test loss: 0.1893 score: 0.9388 time: 0.24s
Epoch 98/1000, LR 0.000265
Train loss: 0.0671;  Loss pred: 0.0671; Loss self: 0.0000; time: 0.16s
Val loss: 0.2863 score: 0.9184 time: 0.10s
Test loss: 0.1802 score: 0.9592 time: 0.10s
Epoch 99/1000, LR 0.000265
Train loss: 0.0650;  Loss pred: 0.0650; Loss self: 0.0000; time: 0.16s
Val loss: 0.2861 score: 0.9184 time: 0.08s
Test loss: 0.1718 score: 0.9592 time: 0.09s
Epoch 100/1000, LR 0.000265
Train loss: 0.0592;  Loss pred: 0.0592; Loss self: 0.0000; time: 0.16s
Val loss: 0.2861 score: 0.8776 time: 0.08s
Test loss: 0.1653 score: 0.9592 time: 0.10s
Epoch 101/1000, LR 0.000265
Train loss: 0.0638;  Loss pred: 0.0638; Loss self: 0.0000; time: 0.15s
Val loss: 0.2871 score: 0.8776 time: 0.07s
Test loss: 0.1591 score: 0.9592 time: 0.08s
     INFO: Early stopping counter 1 of 20
Epoch 102/1000, LR 0.000264
Train loss: 0.0588;  Loss pred: 0.0588; Loss self: 0.0000; time: 0.16s
Val loss: 0.2884 score: 0.8776 time: 0.09s
Test loss: 0.1530 score: 0.9796 time: 0.09s
     INFO: Early stopping counter 2 of 20
Epoch 103/1000, LR 0.000264
Train loss: 0.0595;  Loss pred: 0.0595; Loss self: 0.0000; time: 0.15s
Val loss: 0.2904 score: 0.8776 time: 0.08s
Test loss: 0.1471 score: 0.9796 time: 0.10s
     INFO: Early stopping counter 3 of 20
Epoch 104/1000, LR 0.000264
Train loss: 0.0571;  Loss pred: 0.0571; Loss self: 0.0000; time: 0.17s
Val loss: 0.2930 score: 0.8776 time: 0.08s
Test loss: 0.1416 score: 0.9796 time: 0.10s
     INFO: Early stopping counter 4 of 20
Epoch 105/1000, LR 0.000264
Train loss: 0.0600;  Loss pred: 0.0600; Loss self: 0.0000; time: 0.15s
Val loss: 0.2952 score: 0.8776 time: 0.09s
Test loss: 0.1371 score: 0.9796 time: 0.10s
     INFO: Early stopping counter 5 of 20
Epoch 106/1000, LR 0.000264
Train loss: 0.0589;  Loss pred: 0.0589; Loss self: 0.0000; time: 0.15s
Val loss: 0.2955 score: 0.8776 time: 0.08s
Test loss: 0.1347 score: 0.9796 time: 0.09s
     INFO: Early stopping counter 6 of 20
Epoch 107/1000, LR 0.000264
Train loss: 0.0559;  Loss pred: 0.0559; Loss self: 0.0000; time: 0.15s
Val loss: 0.2944 score: 0.8776 time: 0.08s
Test loss: 0.1331 score: 0.9796 time: 0.09s
     INFO: Early stopping counter 7 of 20
Epoch 108/1000, LR 0.000264
Train loss: 0.0581;  Loss pred: 0.0581; Loss self: 0.0000; time: 0.15s
Val loss: 0.2941 score: 0.8776 time: 0.08s
Test loss: 0.1313 score: 0.9796 time: 0.09s
     INFO: Early stopping counter 8 of 20
Epoch 109/1000, LR 0.000264
Train loss: 0.0536;  Loss pred: 0.0536; Loss self: 0.0000; time: 0.15s
Val loss: 0.2942 score: 0.8776 time: 0.09s
Test loss: 0.1301 score: 0.9796 time: 0.10s
     INFO: Early stopping counter 9 of 20
Epoch 110/1000, LR 0.000263
Train loss: 0.0558;  Loss pred: 0.0558; Loss self: 0.0000; time: 0.16s
Val loss: 0.2948 score: 0.8776 time: 0.09s
Test loss: 0.1282 score: 0.9796 time: 0.09s
     INFO: Early stopping counter 10 of 20
Epoch 111/1000, LR 0.000263
Train loss: 0.0500;  Loss pred: 0.0500; Loss self: 0.0000; time: 0.16s
Val loss: 0.2954 score: 0.8776 time: 0.09s
Test loss: 0.1261 score: 0.9796 time: 0.09s
     INFO: Early stopping counter 11 of 20
Epoch 112/1000, LR 0.000263
Train loss: 0.0564;  Loss pred: 0.0564; Loss self: 0.0000; time: 0.15s
Val loss: 0.2973 score: 0.8776 time: 0.08s
Test loss: 0.1236 score: 0.9796 time: 0.09s
     INFO: Early stopping counter 12 of 20
Epoch 113/1000, LR 0.000263
Train loss: 0.0504;  Loss pred: 0.0504; Loss self: 0.0000; time: 0.18s
Val loss: 0.2998 score: 0.8776 time: 0.09s
Test loss: 0.1210 score: 0.9796 time: 0.10s
     INFO: Early stopping counter 13 of 20
Epoch 114/1000, LR 0.000263
Train loss: 0.0503;  Loss pred: 0.0503; Loss self: 0.0000; time: 0.16s
Val loss: 0.3028 score: 0.8776 time: 0.10s
Test loss: 0.1185 score: 0.9796 time: 0.10s
     INFO: Early stopping counter 14 of 20
Epoch 115/1000, LR 0.000263
Train loss: 0.0524;  Loss pred: 0.0524; Loss self: 0.0000; time: 0.16s
Val loss: 0.3072 score: 0.8776 time: 0.08s
Test loss: 0.1160 score: 0.9796 time: 0.09s
     INFO: Early stopping counter 15 of 20
Epoch 116/1000, LR 0.000263
Train loss: 0.0476;  Loss pred: 0.0476; Loss self: 0.0000; time: 0.16s
Val loss: 0.3113 score: 0.8776 time: 0.08s
Test loss: 0.1137 score: 0.9796 time: 0.09s
     INFO: Early stopping counter 16 of 20
Epoch 117/1000, LR 0.000262
Train loss: 0.0490;  Loss pred: 0.0490; Loss self: 0.0000; time: 0.16s
Val loss: 0.3140 score: 0.8776 time: 0.08s
Test loss: 0.1120 score: 0.9796 time: 0.09s
     INFO: Early stopping counter 17 of 20
Epoch 118/1000, LR 0.000262
Train loss: 0.0477;  Loss pred: 0.0477; Loss self: 0.0000; time: 0.16s
Val loss: 0.3173 score: 0.8776 time: 0.08s
Test loss: 0.1106 score: 0.9796 time: 0.09s
     INFO: Early stopping counter 18 of 20
Epoch 119/1000, LR 0.000262
Train loss: 0.0460;  Loss pred: 0.0460; Loss self: 0.0000; time: 0.16s
Val loss: 0.3202 score: 0.8776 time: 0.08s
Test loss: 0.1093 score: 0.9796 time: 0.09s
     INFO: Early stopping counter 19 of 20
Epoch 120/1000, LR 0.000262
Train loss: 0.0471;  Loss pred: 0.0471; Loss self: 0.0000; time: 0.16s
Val loss: 0.3219 score: 0.8776 time: 0.10s
Test loss: 0.1085 score: 0.9796 time: 0.09s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 099,   Train_Loss: 0.0592,   Val_Loss: 0.2861,   Val_Precision: 0.9524,   Val_Recall: 0.8000,   Val_accuracy: 0.8696,   Val_Score: 0.8776,   Val_Loss: 0.2861,   Test_Precision: 0.9583,   Test_Recall: 0.9583,   Test_accuracy: 0.9583,   Test_Score: 0.9592,   Test_loss: 0.1653


[0.08030457305721939, 0.08224903489463031, 0.08930992404930294, 0.09014386311173439, 0.09109797701239586, 0.11844071093946695, 0.09641301794908941, 0.0792765119113028, 0.08809523889794946, 0.07997036795131862, 0.08830716903321445, 0.0856172798667103, 0.08147128485143185, 0.08817671611905098, 0.08749923179857433, 0.07961139804683626, 0.2485009920783341, 0.08937256503850222, 0.08282550307922065, 0.09621028997935355, 0.08198193786665797, 0.09247422916814685, 0.08144118287600577, 0.08936924301087856, 0.08664764906279743, 0.09125145291909575, 0.12373684905469418, 0.07643555803224444, 0.08294393913820386, 0.07940995809622109, 0.0934932001400739, 0.08350839791819453, 0.08147210790775716, 0.08141372003592551, 0.08202403411269188, 0.08147556590847671, 0.0813275899272412, 0.08863172004930675, 0.09473197697661817, 0.08491315809078515, 0.08734845509752631, 0.08073536702431738, 0.08964998694136739, 0.09078946919180453, 0.08566621784120798, 0.08715541195124388, 0.0761185169685632, 0.08477532188408077, 0.08474085899069905, 0.07614627294242382, 0.07789773400872946, 0.08448924799449742, 0.07630238891579211, 0.08805670589208603, 0.07739226799458265, 0.08294420479796827, 0.07962217694148421, 0.0772191770374775, 0.08571332995779812, 0.08586748992092907, 0.07830905099399388, 0.08032047096639872, 0.08107276004739106, 0.08169830404222012, 0.08374937996268272, 0.08809681003913283, 0.08287597191520035, 0.08143398794345558, 0.08135853102430701, 0.09978363802656531, 0.08035077899694443, 0.07949860696680844, 0.0802019510883838, 0.07999211107380688, 0.08579550893045962, 0.08731993706896901, 0.07981866295449436, 0.08068527188152075, 0.080497439019382, 0.08082063402980566, 0.08700373792089522, 0.08751884801313281, 0.08023286797106266, 0.08173221489414573, 0.07971715182065964, 0.08123901719227433, 0.08858295599929988, 0.09337678295560181, 0.0839238129556179, 0.08465743390843272, 0.08520442084409297, 0.09067314211279154, 0.08440718008205295, 0.07953035994432867, 0.08174778893589973, 0.08658345299772918, 0.09255612595006824, 0.08059333101846278, 0.08125989604741335, 0.08189757610671222, 0.08078978699631989, 0.0807079819496721, 0.08810480288229883, 0.08207276184111834, 0.08807546179741621, 0.08000069414265454, 0.08125029504299164, 0.07794837700203061, 0.08079420914873481, 0.07980709685944021, 0.08226426900364459, 0.08085592114366591, 0.08267049095593393, 0.08617170481011271, 0.17299658711999655, 0.08482114807702601, 0.0804522989783436, 0.08762490097433329, 0.0854366491548717, 0.07922095083631575, 0.0876025459729135, 0.08143193693831563, 0.2240986900869757, 0.08001524396240711, 0.0812943868804723, 0.08208154304884374, 0.08227636781521142, 0.08359456597827375, 0.0824424340389669, 0.21290116012096405, 0.08042746712453663, 0.08712090598419309, 0.07937311194837093, 0.08033056394197047, 0.08049654704518616, 0.08681910601444542, 0.0825569189619273, 0.07881353911943734, 0.09111546585336328, 0.08360462612472475, 0.0845167029183358, 0.08445483213290572, 0.07935727410949767, 0.07964708399958909, 0.07883037719875574, 0.08398926886729896, 0.086457114899531, 0.09179740399122238, 0.09486383688636124, 0.07502445694990456, 0.07620215509086847, 0.07894653407856822, 0.07977849687449634, 0.07867218204773962, 0.07770778192207217, 0.07537034898996353, 0.08335970202460885, 0.09075451013632119, 0.0828078668564558, 0.0815802679862827, 0.08806540095247328, 0.0829521119594574, 0.08340814989060163, 0.08894365094602108, 0.08431104989722371, 0.09056340320967138, 0.08439316996373236, 0.09225691086612642, 0.21504150214605033, 0.08172729285433888, 0.09246477787382901, 0.09292462514713407, 0.0815810770727694, 0.08912649191915989, 0.08133404492400587, 0.08422476006671786, 0.08559111692011356, 0.08433506591245532, 0.09098736895248294, 0.08515111915767193, 0.08235825411975384, 0.08397795399650931, 0.08750092005357146, 0.08031795104034245, 0.0806394258979708, 0.08020767103880644, 0.09038139693439007, 0.09383490006439388, 0.0876111991237849, 0.08012059493921697, 0.08372850995510817, 0.07632787898182869, 0.08655928797088563, 0.083849007030949, 0.08815013198181987, 0.08080439805053174, 0.08191174292005599, 0.08484356408007443, 0.08602913306094706, 0.08166135707870126, 0.081294174073264, 0.08829266298562288, 0.08728120103478432, 0.07839274196885526, 0.08222551480866969, 0.08677568193525076, 0.08073866693302989, 0.07968996418640018, 0.08031068299897015, 0.0793565260246396, 0.083883993094787, 0.07896813796833158, 0.08043512189760804, 3.1389988330192864, 0.08781717019155622, 0.08853760408237576, 0.08850313094444573, 0.08148031705059111, 0.09099357784725726, 0.07925136294215918, 0.08570395014248788, 0.07912908890284598, 0.07772012799978256, 0.0783867840655148, 0.07783869816921651, 0.08104374492540956, 0.08878597291186452, 0.09304617205634713, 0.08039137814193964, 0.07801122707314789, 0.08217786019667983, 0.08006361313164234, 0.08919504680670798, 0.08655560482293367, 0.08136654179543257, 0.09288546000607312, 0.0808763571549207, 0.08227156801149249, 0.08006966300308704, 0.07933972892351449, 0.07949276501312852, 0.08585914387367666, 0.08646708191372454, 0.08286619605496526, 0.0944470230024308, 0.10138511913828552, 0.08402592013590038, 0.09071871591731906, 0.08441196009516716, 0.07608430879190564, 0.07916632993146777, 0.08422963903285563, 0.07862806296907365, 0.08678169501945376, 0.08559981896542013, 0.08104844205081463, 0.08974122698418796, 0.0834859109017998, 0.08554472797550261, 0.08711268613114953, 0.0874496279284358, 0.0771687210071832, 0.07773052318952978, 0.07603398896753788, 0.08362016105093062, 0.22728594904765487, 0.08062042994424701, 0.07503805006854236, 0.07535188505426049, 0.08291839389130473, 0.08075170894153416, 0.07920359820127487, 0.07549891504459083, 0.08223382500000298, 0.08598316111601889, 0.09102195315063, 0.09403719496913254, 0.10055740014649928, 0.10349814407527447, 0.1059724020306021, 0.09366644197143614, 0.09390345495194197, 0.09544716798700392, 0.09176356508396566, 0.09553138795308769, 0.09667627210728824, 0.08872763998806477, 0.09016496199183166, 0.09190295985899866, 0.09520161896944046, 0.0995130620431155, 0.09546651714481413, 0.10722136893309653, 0.23343268921598792, 0.09606244880706072, 0.09771234588697553, 0.09453448699787259, 0.09721811884082854, 0.09579656715504825, 0.09326348011381924, 0.23155285511165857, 0.10935897892341018, 0.10676278010942042, 0.0965085169300437, 0.09676293912343681, 0.10080181690864265, 0.10244493815116584, 0.2426771919708699, 0.0989117540884763, 0.09172388608567417, 0.08839146886020899, 0.09051968390122056, 0.09557897504419088, 0.0889516631141305, 0.09731163713149726, 0.09938210994005203, 0.09408206306397915, 0.1176825340371579, 0.08971951296553016, 0.08713490213267505, 0.09412935585714877, 0.09503735299222171, 0.11941336607560515, 0.095992396119982, 0.09594150492921472, 0.0942650109063834, 0.09103222796693444, 0.09234098996967077, 0.09032848291099072, 0.20952927391044796, 0.09592152805998921, 0.10747068002820015, 0.09765137499198318, 0.1071139860432595, 0.09447709494270384, 0.10276143788360059, 0.23226110287941992, 0.08838011603802443, 0.11349277291446924, 0.0918549899943173, 0.09932742291130126, 0.09862561104819179, 0.09597166907042265, 0.16922593396157026, 0.09121826104819775, 0.0915480989497155, 0.0934638970065862, 0.0893515080679208, 0.0915285749360919, 0.0895526718813926, 0.09580472903326154, 0.1019158570561558, 0.09800874604843557, 0.09220308880321681, 0.0911259890999645, 0.09082124102860689, 0.09169404185377061, 0.09329417115077376, 0.09414243092760444, 0.09311960195191205, 0.09531634487211704, 0.09208682808093727, 0.09051363216713071, 0.09203573479317129, 0.09184935293160379, 0.09298204095102847, 0.09307082300074399, 0.09152893209829926, 0.09326903289183974, 0.0925841829739511, 0.09298559883609414, 0.10496832407079637, 0.09883092204108834, 0.24895496806129813, 0.10115955700166523, 0.09139556181617081, 0.10837380704469979, 0.08815274899825454, 0.09285447909496725, 0.10164532111957669, 0.10528044402599335, 0.10240499395877123, 0.0913395641837269, 0.09352334891445935, 0.1001540650613606, 0.10910576791502535, 0.09558773320168257, 0.09709813422523439, 0.09292122488841414, 0.10404382995329797, 0.1046977408695966, 0.09354816796258092, 0.09322930895723403, 0.09428084013052285, 0.09161618794314563, 0.09264071797952056, 0.09786986885592341]
[0.0016388688379024364, 0.0016785517325434759, 0.001822651511210264, 0.0018396706757496815, 0.0018591423880080788, 0.0024171573661115704, 0.0019676126112059063, 0.0016178879981898532, 0.0017978620183254992, 0.0016320483255371147, 0.0018021871231268256, 0.0017472914258512308, 0.0016626792826822826, 0.0017995248187561424, 0.00178569860813417, 0.0016247224091191072, 0.005071448817925185, 0.0018239298987449432, 0.00169031638937185, 0.0019634753057010926, 0.0016731007727889382, 0.0018872291666968744, 0.001662064956653179, 0.0018238621022628279, 0.001768319368628519, 0.001862274549369301, 0.002525241817442738, 0.0015599093475968254, 0.0016927334518000788, 0.0016206113897187977, 0.0019080244926545694, 0.001704253018738664, 0.0016626960797501461, 0.001661504490529092, 0.0016739598798508548, 0.0016627666511934024, 0.0016597467332090043, 0.001808810613251158, 0.0019333056525840443, 0.0017329215936894929, 0.0017826215326025778, 0.0016476605515166813, 0.0018295915702319875, 0.0018528463100368272, 0.0017482901600246526, 0.0017786818765559975, 0.0015534391218074122, 0.0017301086098791994, 0.0017294052855244704, 0.0015540055702535473, 0.00158974967364754, 0.001724270367234641, 0.0015571916105263696, 0.0017970756304507352, 0.0015794340407057684, 0.0016927388734279238, 0.0016249423865609023, 0.0015759015721934183, 0.0017492516317917984, 0.0017523977534883485, 0.0015981438978366097, 0.0016391932850285452, 0.001654546123416144, 0.0016673123273922472, 0.0017091710196465862, 0.0017978940824312823, 0.0016913463656163337, 0.0016619181212950119, 0.0016603781841695309, 0.0020364007760523533, 0.001639811816264172, 0.0016224205503430292, 0.0016367745120078325, 0.0016324920627307526, 0.0017509287536828493, 0.0017820395320197757, 0.0016289523051937623, 0.0016466382016636888, 0.0016428048779465714, 0.0016494006944858298, 0.0017755864881815351, 0.0017860989390435268, 0.0016374054687971972, 0.0016680043855948107, 0.001626880649401217, 0.0016579391263729455, 0.0018078154285571405, 0.0019056486317469757, 0.0017127308766452633, 0.0017277027328251576, 0.0017388657315121014, 0.0018504722880161538, 0.0017225955118786317, 0.0016230685702924217, 0.0016683222231816273, 0.001767009244851616, 0.0018889005295932293, 0.0016447618575196486, 0.0016583652254574153, 0.0016713791042186168, 0.0016487711631902019, 0.0016471016724422878, 0.0017980572016795678, 0.0016749543232881293, 0.0017974584040289022, 0.001632667227401113, 0.0016581692865916661, 0.0015907832041230736, 0.0016488614111986695, 0.0016287162624375553, 0.0016788626327274405, 0.0016501208396666512, 0.0016871528766517127, 0.001758606220614545, 0.003530542594285644, 0.0017310438383066533, 0.0016418836526192573, 0.0017882632851904752, 0.0017436050847933, 0.0016167540987003215, 0.0017878070606717042, 0.0016618762640472576, 0.004573442654836239, 0.0016329641624981044, 0.0016590691200096387, 0.001675133531609056, 0.0016791095472492126, 0.0017060115505770153, 0.0016824986538564672, 0.0043449216351217155, 0.0016413768800925843, 0.0017779776731467977, 0.0016198594275177742, 0.0016393992641218463, 0.0016427866743915543, 0.0017718184900907229, 0.0016848350808556592, 0.001608439573866068, 0.0018594993031298627, 0.00170621685968826, 0.0017248306718027713, 0.0017235680027123616, 0.001619536206316279, 0.001625450693869165, 0.0016087832081378723, 0.0017140667115775297, 0.0017644309163169594, 0.0018734164079841301, 0.0019359966711502295, 0.0015311113663245828, 0.001555146022262622, 0.0016111537567054738, 0.0016281325892754355, 0.0016055547356681557, 0.0015858731004504524, 0.001538170387550276, 0.0017012184086654867, 0.0018521328599249221, 0.0016899564664582818, 0.0016649034282914838, 0.00179725308066272, 0.0016929002440705591, 0.0017022071406245232, 0.0018151765499187975, 0.0017206336713719125, 0.001848232718564722, 0.0017223095910965788, 0.0018827940993087025, 0.004388602084613272, 0.0016679039358028344, 0.0018870362831393676, 0.0018964209213700829, 0.0016649199402606, 0.0018189079983502018, 0.0016598784678368544, 0.0017188726544228134, 0.0017467574881655829, 0.0017211237941317412, 0.001856885080662917, 0.0017377779419933046, 0.001680780696321507, 0.0017138357958471288, 0.001785733062317785, 0.0016391418579661725, 0.0016457025693463428, 0.0016368912456899273, 0.0018445183047834709, 0.0019149979604978341, 0.001787983655587447, 0.0016351141824329995, 0.0017087451011246564, 0.0015577118159556876, 0.0017665160810384824, 0.001711204225121408, 0.0017989822853432627, 0.0016490693479700356, 0.0016716682228582855, 0.001731501307756621, 0.0017556965930805523, 0.001666558307728597, 0.0016590647770053878, 0.0018018910813392425, 0.001781249000709884, 0.0015998518769154136, 0.0016780717307891772, 0.0017709322843928726, 0.0016477278965924466, 0.0016263257997224526, 0.0016389935305912275, 0.0016195209392783592, 0.0017119182264242246, 0.00161159465241493, 0.0016415330999511846, 0.06406120067386299, 0.0017921871467664534, 0.0018068898792321585, 0.0018061863458050148, 0.0016628636132773695, 0.0018570117928011685, 0.0016173747539216159, 0.0017490602069895487, 0.0016148793653642036, 0.0015861250612200523, 0.0015997302870513225, 0.0015885448605962554, 0.0016539539780695827, 0.0018119586308543779, 0.0018989014705376966, 0.001640640370243666, 0.0015920658586356714, 0.0016770991876873436, 0.0016339512884008642, 0.001820307077687918, 0.0017664409147537484, 0.0016605416692945423, 0.0018956216327770023, 0.0016505379011208306, 0.0016790115920712753, 0.0016340747551650417, 0.001619178141296214, 0.0016223013267985412, 0.0017522274259934012, 0.0017646343247698887, 0.0016911468582645971, 0.0019274902653557305, 0.002069084064046643, 0.0017148146966510281, 0.0018514023656595725, 0.0017226930631666767, 0.0015527409957531765, 0.001615639386356485, 0.001718972225160319, 0.0016046543463076257, 0.0017710550003970156, 0.0017469350809269414, 0.001654049837771727, 0.0018314536119222033, 0.0017037941000367306, 0.0017458107750102573, 0.001777809921043868, 0.001784686284253792, 0.001574871857289453, 0.0015863372079495872, 0.0015517140605619975, 0.001706533898998584, 0.004638488756074589, 0.0016453148968213675, 0.0015313887769090278, 0.0015377935725359284, 0.0016922121202307089, 0.0016479940600313094, 0.001616399963291324, 0.0015407941845834864, 0.001678241326530673, 0.0017547583901228346, 0.001857590880625102, 0.0019191264279414804, 0.002052191839724475, 0.002112207021944377, 0.0021627020822571857, 0.0019115600402333907, 0.0019163970398355502, 0.001947901387489876, 0.0018727258180401155, 0.001949620162307912, 0.0019729851450466986, 0.00181076816302173, 0.0018401012651394217, 0.0018755706093673194, 0.0019428901830498054, 0.0020308788172064387, 0.001948296268261513, 0.0021881912027162556, 0.004763932432979345, 0.0019604581389196067, 0.0019941295078974596, 0.0019292752448545427, 0.001984043241649562, 0.001955031982756087, 0.0019033363288534538, 0.004725568471666501, 0.002231815896396126, 0.002178832247131029, 0.001969561570000892, 0.001974753859661976, 0.0020571799369110745, 0.0020907130234931806, 0.004952595754507549, 0.0020186072262954346, 0.001871916042564779, 0.001803907527759367, 0.0018473404877800115, 0.001950591327432467, 0.0018153400635536835, 0.0019859517781938216, 0.0020282063253071842, 0.0019200421033465133, 0.002401684368105263, 0.001831010468684289, 0.0017782633088301032, 0.0019210072623907911, 0.0019395378161677901, 0.0024370074709307173, 0.0019590284922445305, 0.001957989896514586, 0.0019237757327833346, 0.001857800570753764, 0.0018845099993810362, 0.0018434384267549126, 0.004276107630825469, 0.0019575822053059023, 0.0021932791842489826, 0.001992885203918024, 0.002185999715168561, 0.0019281039784225275, 0.0020971722017061344, 0.004740022507743264, 0.0018036758375107025, 0.0023161790390708007, 0.0018745916325370877, 0.002027090263495944, 0.0020127675724120773, 0.0019586054912331154, 0.003453590489011638, 0.0018615971642489337, 0.0018683285499941937, 0.0019074264695221673, 0.001823500164651445, 0.0018679301007365693, 0.0018276055485998489, 0.001955198551699215, 0.0020799154501256285, 0.0020001784907843993, 0.0018816956898615677, 0.0018597140632645817, 0.0018534947148695284, 0.0018713069766075636, 0.0019039626765464032, 0.0019212741005633558, 0.0019004000398349396, 0.0019452315280023887, 0.0018793230220599442, 0.0018472169830026676, 0.0018782803019014548, 0.0018744765904408936, 0.0018975926724699686, 0.0018994045510355914, 0.0018679373897612095, 0.0019034496508538723, 0.0018894731219173694, 0.0018976652823692681, 0.002142210695322375, 0.0020169575926752724, 0.0050807136339040436, 0.0020644807551360254, 0.001865215547268792, 0.002211710347851016, 0.0017990356938419293, 0.0018949893692850458, 0.0020743943085627897, 0.0021485804903263947, 0.00208989783589329, 0.0018640727384434063, 0.0019086397737644765, 0.0020439605114563387, 0.0022266483247964357, 0.0019507700653404606, 0.0019815945760251917, 0.0018963515283349826, 0.0021233434684346524, 0.002136688589175441, 0.001909146284950631, 0.0019026389583108984, 0.0019240987781739356, 0.0018697181212886864, 0.0018906268975412358, 0.001997344262365784]
[610.1769567355279, 595.7516712843399, 548.6512335734367, 543.5755503318503, 537.882416349734, 413.7091006237127, 508.23012330009516, 618.0897572136224, 556.2162111480527, 612.7269544367783, 554.881336775386, 572.3143748117624, 601.4389007041519, 555.7022551604564, 560.0049165322887, 615.4897565191955, 197.1823113871268, 548.266685407211, 591.6052203526328, 509.30103225460874, 597.6926293166862, 529.8773554619503, 601.6612022274102, 548.2870655403831, 565.5087071605083, 536.9777513947507, 396.0016791630197, 641.0628935204383, 590.7604643463415, 617.0510748869389, 524.1022868677824, 586.7673338434869, 601.43282478315, 601.8641572744461, 597.3858824436684, 601.4072986623103, 602.5015624321006, 552.8494761552726, 517.2487850865207, 577.0601530049266, 560.9715700786065, 606.9211277040614, 546.5700740374544, 539.7101716332446, 571.98743255862, 562.2140829006853, 643.7329831351931, 577.9983951815734, 578.233458848678, 643.4983369054737, 629.0298507849909, 579.955451884141, 642.1817284656287, 556.4596075175667, 633.1381838225742, 590.758572215527, 615.4064342653052, 634.5573972669818, 571.6730411025412, 570.6467027873013, 625.7258819770168, 610.0561838151904, 604.3953600612224, 599.76765214952, 585.0789584571677, 556.2062914449918, 591.2449515540809, 601.7143607657234, 602.2724277723323, 491.0624724562034, 609.8260727735243, 616.3630014354598, 610.9577053306501, 612.5604055478524, 571.12546578302, 561.1547791347778, 613.8915159219785, 607.2979474116689, 608.7150174827535, 606.2808166282066, 563.194193386856, 559.8793986941786, 610.722279274283, 599.5188074061326, 614.6732400855932, 603.1584538255566, 553.1538143792275, 524.7557095996572, 583.862890332605, 578.8032750083051, 575.087530841389, 540.4025807228248, 580.5193344021999, 616.1169147769488, 599.4045910944698, 565.9279955176322, 529.4085021064333, 607.9907528425001, 603.0034787567238, 598.3083056835918, 606.5123058466787, 607.1270624825612, 556.1558325652256, 597.031206222319, 556.341107954741, 612.4946855164138, 603.0747331326345, 628.6211706335273, 606.4791092861054, 613.9804845463927, 595.6413470084944, 606.0162237585065, 592.7145155835424, 568.6321293976487, 283.24258192453163, 577.6861208657909, 609.0565542842967, 559.2017731849178, 573.5243655351851, 618.5232502604332, 559.3444740196327, 601.7295159897437, 218.65366540510541, 612.3833106479218, 602.7476419995029, 596.9673349201279, 595.553757429491, 586.1625026288804, 594.3541159500442, 230.1537482095432, 609.2445995362098, 562.4367589667902, 617.3375189304983, 609.9795345069011, 608.7217625930489, 564.3918976987292, 593.5299017469061, 621.7205894756654, 537.7791743814181, 586.0919696823933, 579.7670556001956, 580.1917872844646, 617.4607249284986, 615.2139857405551, 621.5877906616615, 583.4078646096899, 566.7549750756928, 533.7841580431333, 516.5298137655744, 653.1203555757604, 643.0264333281542, 620.6732261511925, 614.2005918848587, 622.837688298336, 630.5674771303954, 650.123034544061, 587.8140013688458, 539.9180704782354, 591.731219027047, 600.6354380723424, 556.4046659646061, 590.7022599249626, 587.472567899762, 550.9105987760448, 581.1812337734099, 541.0574057884701, 580.6157064731372, 531.1255226299922, 227.8629916132214, 599.5549135260339, 529.931516916225, 527.309095112462, 600.6294812250708, 549.7804181998357, 602.4537454860747, 581.7766647383157, 572.4893162188096, 581.0157313550313, 538.5362887632201, 575.4475159541718, 594.9616164610661, 583.486470771088, 559.9941117190573, 610.0753239507824, 607.6432148958669, 610.914135336165, 542.1469645525641, 522.1937676320209, 559.2892288892021, 611.5780847255761, 585.224794114595, 641.9672687572701, 566.085987404162, 584.3837838403251, 555.8698427145383, 606.4026362693453, 598.2048269662982, 577.5334939224657, 569.5744947852271, 600.0390117540684, 602.7492198375763, 554.9725010330576, 561.4038237222693, 625.0578659369672, 595.9220822638564, 564.6743293422026, 606.8963219400677, 614.8829466830442, 610.1305351945313, 617.4665456598474, 584.140050946682, 620.503424047435, 609.1866195264279, 15.610072703617007, 557.9774421461766, 553.4371582317744, 553.6527293114384, 601.3722304194755, 538.4995420473728, 618.2858966824604, 571.7356075015749, 619.2413015163335, 630.4673095769617, 625.1053743835995, 629.5069310316192, 604.6117444979653, 551.8889796774655, 526.6202673047792, 609.5180992355326, 628.1147193602625, 596.2676550925783, 612.0133489283469, 549.3578595926575, 566.110075716518, 602.2131323117209, 527.5314349177604, 605.8630942803131, 595.588502618003, 611.9671066694864, 617.597270180205, 616.4082981879857, 570.7021732256381, 566.689645533446, 591.3147016848491, 518.809364682028, 483.30564106913783, 583.1533879158864, 540.1311019950783, 580.4864612165948, 644.0224111651909, 618.9500011231798, 581.7429655716139, 623.1871694368574, 564.6352031844472, 572.4311171708739, 604.5767045007312, 546.0143754066746, 586.9253802313565, 572.7997640489558, 562.4898298535959, 560.3225669536185, 634.9722965531445, 630.3829948567778, 644.4486296900741, 585.9830857077101, 215.58745802507224, 607.7863890565445, 653.0020430333904, 650.2823381885584, 590.9424640355752, 606.7983036182798, 618.6587618845236, 649.0159490511862, 595.861860979934, 569.879024730008, 538.3316694919862, 521.07041278809, 487.2838789448938, 473.4384412184452, 462.3845365499035, 523.1329275317482, 521.8125363446668, 513.3730107809154, 533.9809973071984, 512.9204238512925, 506.84618812795514, 552.2518124745711, 543.4483519711246, 533.1710760477991, 514.6971294230711, 492.39767115969187, 513.2689603169601, 456.99845550913255, 209.91061776554287, 510.0848521820988, 501.4719435421047, 518.3293584816587, 504.02127282699024, 511.5005835302295, 525.3932186553637, 211.61475195963962, 448.0656319433749, 458.9614465807302, 507.7272095634701, 506.3922245839656, 486.1023491710375, 478.3057209493017, 201.91431919107498, 495.3910730990538, 534.2119930923101, 554.3521409005369, 541.3187263609002, 512.66504979097, 550.8609764511087, 503.5368990225319, 493.04648522311646, 520.8219123200802, 416.37444673419765, 546.1465224273518, 562.3464168857464, 520.5602391921461, 515.5867504433797, 410.33932473669853, 510.4570984847001, 510.7278652357185, 519.8111104942527, 538.270907944802, 530.6419177019216, 542.4645518322762, 233.85753735271578, 510.834230761581, 455.93830789144045, 501.7850491508463, 457.4566012342278, 518.6442283149828, 476.8325649111978, 210.9694623530601, 554.4233499186442, 431.7455529695051, 533.4495164936759, 493.31794346216634, 496.82835400692176, 510.5673421605755, 289.55372768766915, 537.1731431506842, 535.2377663998699, 524.2666052812572, 548.3958923530704, 535.3519382795299, 547.1640205766022, 511.4570073361217, 480.788774341572, 499.9553812859148, 531.4355585698173, 537.7170715397929, 539.5213657625089, 534.3858664027802, 525.2203797470963, 520.487940636258, 526.2049984417257, 514.0776229485275, 532.1065023211871, 541.354918886947, 532.4018992200802, 533.4822558465727, 526.9834851851358, 526.4807854939492, 535.3498492408445, 525.3619393354628, 529.2480683637542, 526.9633213458395, 466.80749105750914, 495.7962446169282, 196.82274421587414, 484.3832995353409, 536.1310661731752, 452.1387716848361, 555.8533404439858, 527.7074458614438, 482.06842637012136, 465.42356895742273, 478.4922893479946, 536.4597525497046, 523.9333339615287, 489.2462424763244, 449.1054958539184, 512.618077223506, 504.6440942555785, 527.3283908906968, 470.9553658491289, 468.0139188583888, 523.7943304202377, 525.585790005986, 519.7238371249574, 534.8399786117274, 528.9250889747216, 500.6648171985811]
Elapsed: 0.09973040215907998~0.1548194938942032
Time per graph: 0.002035314329777142~0.003159581508044964
Speed: 550.796127779719~81.5022900530947
Total Time: 0.0992
best val loss: 0.28610479831695557 test_score: 0.9592

Testing...
Test loss: 0.1802 score: 0.9592 time: 0.09s
test Score 0.9592
Epoch Time List: [1.04869039892219, 0.4112613620236516, 0.4221711193677038, 0.3240542740095407, 0.33902854775078595, 0.35985556710511446, 0.4841554563026875, 0.30481399898417294, 0.30977859697304666, 0.4460116771515459, 0.31705206213518977, 0.3189725549891591, 0.30861121299676597, 0.30697344709187746, 0.30806554621085525, 0.2988910642452538, 0.47592021501623094, 0.314807518851012, 0.31602018792182207, 0.3222094960510731, 0.31568995397537947, 0.3179572082590312, 0.33699335996061563, 0.3109961508307606, 0.38816100265830755, 0.31433799979276955, 0.404101180145517, 0.30125720403157175, 0.48263153806328773, 0.3004446930717677, 0.3140702289529145, 0.4606619228143245, 0.30990164703689516, 0.3124753732699901, 0.30621600919403136, 0.3139397331979126, 0.3110068391542882, 0.3188385658431798, 0.3192756848875433, 0.4045541079249233, 0.3037626910954714, 0.30614277301356196, 0.3202781272120774, 0.31957897916436195, 0.3291618369985372, 0.3183686910197139, 0.4277606999967247, 0.3008091184310615, 0.3004341658670455, 0.2925573440734297, 0.3021396119147539, 0.30054259300231934, 0.30382046778686345, 0.3053547809831798, 0.3040248742327094, 0.33928753016516566, 0.3080075499601662, 0.2996388857718557, 0.30009892489761114, 0.3051292500458658, 0.30196176981553435, 0.3161864229477942, 0.31264831428416073, 0.30872260709293187, 0.31519400095567107, 0.3158708040136844, 0.31783738802187145, 0.31403106707148254, 0.31622310634702444, 0.3515003868378699, 0.3101374839898199, 0.3046731890644878, 0.3096503510605544, 0.3096352522261441, 0.31398171302862465, 0.3396198321133852, 0.32327646808698773, 0.3124328644480556, 0.3113052567932755, 0.3087417888455093, 0.31169284391216934, 0.3099166229367256, 0.30713486671447754, 0.3073511519469321, 0.3126804078929126, 0.31085862102918327, 0.31728261918760836, 0.3533439680468291, 0.3125948163215071, 0.3216722388751805, 0.3190305430907756, 0.3232168359681964, 0.31441419781185687, 0.4496344532817602, 0.30654514906927943, 0.31183543917723, 0.3207151792012155, 0.3115375570487231, 0.3097281991504133, 0.31569563201628625, 0.44637257209979, 0.30404098075814545, 0.310074700973928, 0.3062147847376764, 0.3171906592324376, 0.3097621058113873, 0.3521153738256544, 0.5615558652207255, 0.30370283173397183, 0.30624680570326746, 0.3033813638612628, 0.3030554736033082, 0.30792759894393384, 0.31900586606934667, 0.401575047057122, 0.3001402411609888, 0.3107679490931332, 0.33484852011315525, 0.3063456437084824, 0.30801156093366444, 0.31248284969478846, 0.30797904287464917, 0.44901884789578617, 0.3117685930337757, 0.3063949029892683, 0.2928837148938328, 0.30409448593854904, 0.34087924007326365, 0.31592338322661817, 0.4564248670358211, 0.3108602608554065, 0.3117180778644979, 0.30717891082167625, 0.3104665938299149, 0.30697820405475795, 0.3100914820097387, 0.29955253982916474, 0.3006022642366588, 0.4277121899649501, 0.30923199909739196, 0.31422792188823223, 0.3271856892388314, 0.30121732922270894, 0.3176588991191238, 0.3134043049067259, 0.31896991841495037, 0.4550726201850921, 0.3143497759010643, 0.33021935704164207, 0.2981805391609669, 0.2888390081934631, 0.3110637969803065, 0.306721534114331, 0.4337967161554843, 0.30063633574172854, 0.2936395010910928, 0.2977746119722724, 0.31950694182887673, 0.31675960775464773, 0.31071410002186894, 0.3137172288261354, 0.45198657270520926, 0.31058384384959936, 0.3151454741600901, 0.31828061025589705, 0.32270830124616623, 0.31873652269132435, 0.3222151619847864, 0.4497459710109979, 0.309152748901397, 0.3301563661079854, 0.3173754771705717, 0.3164607468061149, 0.3135131699964404, 0.3114147160667926, 0.31441204994916916, 0.4577474088873714, 0.32146723312325776, 0.31701406789943576, 0.32164829201065004, 0.31953907571733, 0.32091404194943607, 0.3140860132407397, 0.4403564832173288, 0.3128592041321099, 0.3033284561242908, 0.31224119081161916, 0.33661800785921514, 0.31722093489952385, 0.3105383387301117, 0.31242826161906123, 0.372133731842041, 0.30671786377206445, 0.31966201215982437, 0.3123604911379516, 0.3238706060219556, 0.31357460701838136, 0.31316943001002073, 0.31665252335369587, 0.369372014189139, 0.29800919094122946, 0.30900686979293823, 0.31932580401189625, 0.30763329192996025, 0.30642817215994, 0.31071882508695126, 0.30837933090515435, 0.43983968324027956, 0.303556009195745, 0.3070426101330668, 0.3087505269795656, 0.30382634815759957, 0.3081385991536081, 6.633971838047728, 0.7178552888799459, 0.31554126809351146, 0.31551897595636547, 0.3139108100440353, 0.3240873138420284, 0.31512109795585275, 0.3070659111253917, 0.30526094301603734, 0.30539823113940656, 0.2991426058579236, 0.29925431008450687, 0.30408612359315157, 0.3211152988951653, 0.32525043725036085, 0.30432423390448093, 0.3060405838768929, 0.3078686259686947, 0.3123359817545861, 0.31176892993971705, 0.3087583847809583, 0.3136061728000641, 0.3298565912991762, 0.31491189170628786, 0.3109215300064534, 0.30966871697455645, 0.30831088521517813, 0.30480991094373167, 0.305850462988019, 0.3063270940911025, 0.3220037631690502, 0.3144272689241916, 0.3436102659907192, 0.3220976460725069, 0.3242546839173883, 0.30448475712910295, 0.30514204199425876, 0.3060695657040924, 0.40866007772274315, 0.3061901112087071, 0.3074364000931382, 0.30627069203183055, 0.3112816601060331, 0.31638680514879525, 0.3383650602772832, 0.3858680890407413, 0.3114037709310651, 0.3166159139946103, 0.3002663420047611, 0.3036463870666921, 0.2968277789186686, 0.3125814569648355, 0.47100821509957314, 0.3089855208527297, 0.29618678987026215, 0.29793122387491167, 0.3014040240086615, 0.2962248679250479, 0.3062764508649707, 0.30236075399443507, 0.3987325159832835, 0.3286707659717649, 0.3219403540715575, 0.31871160166338086, 0.35476096509955823, 0.3577107300516218, 0.48999795294366777, 0.31864441302604973, 0.32647833600640297, 0.32508911984041333, 0.3265323129016906, 0.3204555017873645, 0.33007494709454477, 0.40794520801864564, 0.3105448361020535, 0.3214231519959867, 0.3196492169518024, 0.348253853386268, 0.367058445001021, 0.35668080602772534, 0.4667050214484334, 0.3309942309278995, 0.3411235325038433, 0.3313591161277145, 0.3373545380309224, 0.339724580058828, 0.3268658621236682, 0.4597469016443938, 0.34944357769563794, 0.37498477124609053, 0.34304641908966005, 0.3470671863760799, 0.348527000984177, 0.33278887369669974, 0.49199270992539823, 0.3556652551051229, 0.3145419079810381, 0.30903914594091475, 0.31394653487950563, 0.3234298708848655, 0.32137617585249245, 0.32045915978960693, 0.3947914151940495, 0.3356994071509689, 0.3324971250258386, 0.31096545397304, 0.31097321514971554, 0.3189413398504257, 0.3372786270920187, 0.3650226809550077, 0.45671153091825545, 0.32891709147952497, 0.3199755637906492, 0.31853028503246605, 0.32373258122242987, 0.3200019171927124, 0.442483072867617, 0.34781900327652693, 0.34493373404257, 0.3390730631072074, 0.3520577328745276, 0.3364318977110088, 0.34706076607108116, 0.47130183107219636, 0.31866605998948216, 0.3377419072203338, 0.32652045832946897, 0.3468556522857398, 0.34674400300718844, 0.34427515766583383, 0.4053009250201285, 0.32878193398937583, 0.32376303523778915, 0.3227774710394442, 0.31874082097783685, 0.3199060352053493, 0.3181712676305324, 0.32190519478172064, 0.395381695125252, 0.34540596627630293, 0.33320676628500223, 0.3224706999026239, 0.3192082019522786, 0.32162514491938055, 0.3286327759269625, 0.3906454211100936, 0.32377808005549014, 0.32765436195768416, 0.33324168296530843, 0.3171807487960905, 0.32656737696379423, 0.3343269571196288, 0.42546746297739446, 0.3265857552178204, 0.32968007517047226, 0.3458867622539401, 0.3249417180195451, 0.33000473701395094, 0.3381173610687256, 0.3569695430342108, 0.49077229807153344, 0.35449715284630656, 0.32144253398291767, 0.3484439719468355, 0.3091442610602826, 0.3413232248276472, 0.329279859084636, 0.3516500350087881, 0.3392512770369649, 0.31958791497163475, 0.31831301492638886, 0.32133892783895135, 0.3450946102384478, 0.3346221251413226, 0.33965045493096113, 0.3265741034410894, 0.37374925683252513, 0.3527995739132166, 0.3269304879941046, 0.3268636669963598, 0.332698579877615, 0.33108457806520164, 0.32970664207823575, 0.3530570510774851]
Total Epoch List: [277, 120]
Total Time List: [0.09498926601372659, 0.0991767339874059]
========================k_idx:2========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: SGFormer
SGFormer(
  (trans_conv): SGModule(
    (attns): ModuleList(
      (0-1): 2 x SGFormerAttention(heads=1, head_channels=64)
    )
    (fcs): ModuleList(
      (0): Linear(in_features=14887, out_features=64, bias=True)
    )
    (bns): ModuleList(
      (0-2): 3 x LayerNorm((64,), eps=1e-05, elementwise_affine=True)
    )
  )
  (graph_conv): GraphModule(
    (convs): ModuleList(
      (0-2): 3 x GCNConv(64, 64)
    )
    (fcs): ModuleList(
      (0): Linear(in_features=14887, out_features=64, bias=True)
    )
    (bns): ModuleList(
      (0-3): 4 x BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (classifier): Linear(in_features=64, out_features=2, bias=True)
)
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7f342d35ef80>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.6978;  Loss pred: 0.6978; Loss self: 0.0000; time: 0.15s
Val loss: 0.6996 score: 0.4898 time: 0.07s
Test loss: 0.6972 score: 0.4792 time: 0.06s
Epoch 2/1000, LR 0.000000
Train loss: 0.6959;  Loss pred: 0.6959; Loss self: 0.0000; time: 0.13s
Val loss: 0.6989 score: 0.4898 time: 0.06s
Test loss: 0.6963 score: 0.4792 time: 0.06s
Epoch 3/1000, LR 0.000030
Train loss: 0.6974;  Loss pred: 0.6974; Loss self: 0.0000; time: 0.12s
Val loss: 0.6973 score: 0.5102 time: 0.06s
Test loss: 0.6944 score: 0.4792 time: 0.06s
Epoch 4/1000, LR 0.000060
Train loss: 0.6934;  Loss pred: 0.6934; Loss self: 0.0000; time: 0.13s
Val loss: 0.6950 score: 0.5102 time: 0.06s
Test loss: 0.6917 score: 0.4792 time: 0.07s
Epoch 5/1000, LR 0.000090
Train loss: 0.6796;  Loss pred: 0.6796; Loss self: 0.0000; time: 0.13s
Val loss: 0.6921 score: 0.5510 time: 0.07s
Test loss: 0.6883 score: 0.5000 time: 0.07s
Epoch 6/1000, LR 0.000120
Train loss: 0.6439;  Loss pred: 0.6439; Loss self: 0.0000; time: 0.14s
Val loss: 0.6884 score: 0.6327 time: 0.07s
Test loss: 0.6839 score: 0.6250 time: 0.07s
Epoch 7/1000, LR 0.000150
Train loss: 0.6540;  Loss pred: 0.6540; Loss self: 0.0000; time: 0.14s
Val loss: 0.6842 score: 0.6735 time: 0.06s
Test loss: 0.6788 score: 0.7292 time: 0.06s
Epoch 8/1000, LR 0.000180
Train loss: 0.6252;  Loss pred: 0.6252; Loss self: 0.0000; time: 0.13s
Val loss: 0.6794 score: 0.7551 time: 0.06s
Test loss: 0.6731 score: 0.8333 time: 0.06s
Epoch 9/1000, LR 0.000210
Train loss: 0.6245;  Loss pred: 0.6245; Loss self: 0.0000; time: 0.13s
Val loss: 0.6738 score: 0.8571 time: 0.06s
Test loss: 0.6667 score: 0.8750 time: 0.06s
Epoch 10/1000, LR 0.000240
Train loss: 0.5780;  Loss pred: 0.5780; Loss self: 0.0000; time: 0.14s
Val loss: 0.6675 score: 0.8776 time: 0.17s
Test loss: 0.6595 score: 0.8750 time: 0.06s
Epoch 11/1000, LR 0.000270
Train loss: 0.5566;  Loss pred: 0.5566; Loss self: 0.0000; time: 0.13s
Val loss: 0.6608 score: 0.8776 time: 0.06s
Test loss: 0.6519 score: 0.9167 time: 0.06s
Epoch 12/1000, LR 0.000270
Train loss: 0.5412;  Loss pred: 0.5412; Loss self: 0.0000; time: 0.13s
Val loss: 0.6541 score: 0.8776 time: 0.06s
Test loss: 0.6444 score: 0.9167 time: 0.06s
Epoch 13/1000, LR 0.000270
Train loss: 0.5213;  Loss pred: 0.5213; Loss self: 0.0000; time: 0.13s
Val loss: 0.6475 score: 0.9184 time: 0.06s
Test loss: 0.6370 score: 0.9167 time: 0.06s
Epoch 14/1000, LR 0.000270
Train loss: 0.5055;  Loss pred: 0.5055; Loss self: 0.0000; time: 0.13s
Val loss: 0.6407 score: 0.8980 time: 0.06s
Test loss: 0.6296 score: 0.9167 time: 0.06s
Epoch 15/1000, LR 0.000270
Train loss: 0.5092;  Loss pred: 0.5092; Loss self: 0.0000; time: 0.14s
Val loss: 0.6341 score: 0.8980 time: 0.06s
Test loss: 0.6221 score: 0.8958 time: 0.06s
Epoch 16/1000, LR 0.000270
Train loss: 0.4795;  Loss pred: 0.4795; Loss self: 0.0000; time: 0.14s
Val loss: 0.6273 score: 0.8776 time: 0.07s
Test loss: 0.6144 score: 0.8750 time: 0.06s
Epoch 17/1000, LR 0.000270
Train loss: 0.4538;  Loss pred: 0.4538; Loss self: 0.0000; time: 0.14s
Val loss: 0.6201 score: 0.7959 time: 0.06s
Test loss: 0.6064 score: 0.8750 time: 0.06s
Epoch 18/1000, LR 0.000270
Train loss: 0.4367;  Loss pred: 0.4367; Loss self: 0.0000; time: 0.13s
Val loss: 0.6129 score: 0.7755 time: 0.06s
Test loss: 0.5986 score: 0.8750 time: 0.06s
Epoch 19/1000, LR 0.000270
Train loss: 0.4339;  Loss pred: 0.4339; Loss self: 0.0000; time: 0.14s
Val loss: 0.6066 score: 0.7755 time: 0.07s
Test loss: 0.5916 score: 0.8542 time: 0.06s
Epoch 20/1000, LR 0.000270
Train loss: 0.4097;  Loss pred: 0.4097; Loss self: 0.0000; time: 0.15s
Val loss: 0.6006 score: 0.7755 time: 0.16s
Test loss: 0.5850 score: 0.8333 time: 0.07s
Epoch 21/1000, LR 0.000270
Train loss: 0.3987;  Loss pred: 0.3987; Loss self: 0.0000; time: 0.13s
Val loss: 0.5950 score: 0.7347 time: 0.06s
Test loss: 0.5786 score: 0.8125 time: 0.06s
Epoch 22/1000, LR 0.000270
Train loss: 0.3900;  Loss pred: 0.3900; Loss self: 0.0000; time: 0.13s
Val loss: 0.5899 score: 0.7143 time: 0.06s
Test loss: 0.5727 score: 0.7708 time: 0.06s
Epoch 23/1000, LR 0.000270
Train loss: 0.3673;  Loss pred: 0.3673; Loss self: 0.0000; time: 0.14s
Val loss: 0.5857 score: 0.6939 time: 0.06s
Test loss: 0.5674 score: 0.7083 time: 0.06s
Epoch 24/1000, LR 0.000270
Train loss: 0.3558;  Loss pred: 0.3558; Loss self: 0.0000; time: 0.12s
Val loss: 0.5814 score: 0.6531 time: 0.06s
Test loss: 0.5620 score: 0.6875 time: 0.07s
Epoch 25/1000, LR 0.000270
Train loss: 0.3379;  Loss pred: 0.3379; Loss self: 0.0000; time: 0.15s
Val loss: 0.5773 score: 0.6531 time: 0.06s
Test loss: 0.5567 score: 0.6875 time: 0.06s
Epoch 26/1000, LR 0.000270
Train loss: 0.3407;  Loss pred: 0.3407; Loss self: 0.0000; time: 0.13s
Val loss: 0.5728 score: 0.6531 time: 0.08s
Test loss: 0.5511 score: 0.6875 time: 0.07s
Epoch 27/1000, LR 0.000270
Train loss: 0.3290;  Loss pred: 0.3290; Loss self: 0.0000; time: 0.14s
Val loss: 0.5684 score: 0.6531 time: 0.09s
Test loss: 0.5458 score: 0.6875 time: 0.06s
Epoch 28/1000, LR 0.000270
Train loss: 0.3204;  Loss pred: 0.3204; Loss self: 0.0000; time: 0.15s
Val loss: 0.5623 score: 0.6327 time: 0.06s
Test loss: 0.5391 score: 0.7083 time: 0.06s
Epoch 29/1000, LR 0.000270
Train loss: 0.2961;  Loss pred: 0.2961; Loss self: 0.0000; time: 0.14s
Val loss: 0.5557 score: 0.6531 time: 0.16s
Test loss: 0.5320 score: 0.7083 time: 0.07s
Epoch 30/1000, LR 0.000270
Train loss: 0.2961;  Loss pred: 0.2961; Loss self: 0.0000; time: 0.16s
Val loss: 0.5479 score: 0.6531 time: 0.07s
Test loss: 0.5235 score: 0.7500 time: 0.07s
Epoch 31/1000, LR 0.000270
Train loss: 0.2831;  Loss pred: 0.2831; Loss self: 0.0000; time: 0.14s
Val loss: 0.5394 score: 0.6939 time: 0.07s
Test loss: 0.5145 score: 0.7708 time: 0.07s
Epoch 32/1000, LR 0.000270
Train loss: 0.2793;  Loss pred: 0.2793; Loss self: 0.0000; time: 0.13s
Val loss: 0.5321 score: 0.7143 time: 0.06s
Test loss: 0.5064 score: 0.7917 time: 0.06s
Epoch 33/1000, LR 0.000270
Train loss: 0.2772;  Loss pred: 0.2772; Loss self: 0.0000; time: 0.13s
Val loss: 0.5261 score: 0.7143 time: 0.06s
Test loss: 0.4992 score: 0.8125 time: 0.06s
Epoch 34/1000, LR 0.000270
Train loss: 0.2609;  Loss pred: 0.2609; Loss self: 0.0000; time: 0.13s
Val loss: 0.5206 score: 0.7347 time: 0.06s
Test loss: 0.4925 score: 0.8333 time: 0.08s
Epoch 35/1000, LR 0.000270
Train loss: 0.2597;  Loss pred: 0.2597; Loss self: 0.0000; time: 0.13s
Val loss: 0.5144 score: 0.7347 time: 0.06s
Test loss: 0.4852 score: 0.8542 time: 0.06s
Epoch 36/1000, LR 0.000270
Train loss: 0.2484;  Loss pred: 0.2484; Loss self: 0.0000; time: 0.14s
Val loss: 0.5091 score: 0.7551 time: 0.06s
Test loss: 0.4786 score: 0.8542 time: 0.06s
Epoch 37/1000, LR 0.000270
Train loss: 0.2275;  Loss pred: 0.2275; Loss self: 0.0000; time: 0.13s
Val loss: 0.5048 score: 0.7551 time: 0.06s
Test loss: 0.4731 score: 0.8542 time: 0.07s
Epoch 38/1000, LR 0.000270
Train loss: 0.2289;  Loss pred: 0.2289; Loss self: 0.0000; time: 0.24s
Val loss: 0.5003 score: 0.7551 time: 0.06s
Test loss: 0.4673 score: 0.8542 time: 0.06s
Epoch 39/1000, LR 0.000269
Train loss: 0.2241;  Loss pred: 0.2241; Loss self: 0.0000; time: 0.13s
Val loss: 0.4960 score: 0.7551 time: 0.06s
Test loss: 0.4620 score: 0.8542 time: 0.06s
Epoch 40/1000, LR 0.000269
Train loss: 0.2186;  Loss pred: 0.2186; Loss self: 0.0000; time: 0.13s
Val loss: 0.4912 score: 0.7755 time: 0.06s
Test loss: 0.4560 score: 0.8542 time: 0.06s
Epoch 41/1000, LR 0.000269
Train loss: 0.2066;  Loss pred: 0.2066; Loss self: 0.0000; time: 0.15s
Val loss: 0.4860 score: 0.7755 time: 0.06s
Test loss: 0.4494 score: 0.8542 time: 0.06s
Epoch 42/1000, LR 0.000269
Train loss: 0.2142;  Loss pred: 0.2142; Loss self: 0.0000; time: 0.13s
Val loss: 0.4811 score: 0.7755 time: 0.06s
Test loss: 0.4434 score: 0.8542 time: 0.06s
Epoch 43/1000, LR 0.000269
Train loss: 0.2051;  Loss pred: 0.2051; Loss self: 0.0000; time: 0.14s
Val loss: 0.4770 score: 0.7755 time: 0.06s
Test loss: 0.4381 score: 0.8542 time: 0.06s
Epoch 44/1000, LR 0.000269
Train loss: 0.1911;  Loss pred: 0.1911; Loss self: 0.0000; time: 0.13s
Val loss: 0.4734 score: 0.7755 time: 0.06s
Test loss: 0.4334 score: 0.8542 time: 0.06s
Epoch 45/1000, LR 0.000269
Train loss: 0.1830;  Loss pred: 0.1830; Loss self: 0.0000; time: 0.13s
Val loss: 0.4705 score: 0.7755 time: 0.06s
Test loss: 0.4297 score: 0.8542 time: 0.06s
Epoch 46/1000, LR 0.000269
Train loss: 0.1823;  Loss pred: 0.1823; Loss self: 0.0000; time: 0.14s
Val loss: 0.4677 score: 0.7755 time: 0.06s
Test loss: 0.4259 score: 0.8542 time: 0.06s
Epoch 47/1000, LR 0.000269
Train loss: 0.1779;  Loss pred: 0.1779; Loss self: 0.0000; time: 0.13s
Val loss: 0.4624 score: 0.8163 time: 0.20s
Test loss: 0.4201 score: 0.8542 time: 0.06s
Epoch 48/1000, LR 0.000269
Train loss: 0.1757;  Loss pred: 0.1757; Loss self: 0.0000; time: 0.13s
Val loss: 0.4579 score: 0.8163 time: 0.06s
Test loss: 0.4145 score: 0.8542 time: 0.06s
Epoch 49/1000, LR 0.000269
Train loss: 0.1594;  Loss pred: 0.1594; Loss self: 0.0000; time: 0.13s
Val loss: 0.4542 score: 0.8163 time: 0.06s
Test loss: 0.4099 score: 0.8542 time: 0.06s
Epoch 50/1000, LR 0.000269
Train loss: 0.1805;  Loss pred: 0.1805; Loss self: 0.0000; time: 0.13s
Val loss: 0.4500 score: 0.8163 time: 0.06s
Test loss: 0.4044 score: 0.8750 time: 0.06s
Epoch 51/1000, LR 0.000269
Train loss: 0.1579;  Loss pred: 0.1579; Loss self: 0.0000; time: 0.12s
Val loss: 0.4460 score: 0.8163 time: 0.06s
Test loss: 0.3994 score: 0.8750 time: 0.06s
Epoch 52/1000, LR 0.000269
Train loss: 0.1520;  Loss pred: 0.1520; Loss self: 0.0000; time: 0.14s
Val loss: 0.4428 score: 0.8163 time: 0.06s
Test loss: 0.3949 score: 0.8958 time: 0.06s
Epoch 53/1000, LR 0.000269
Train loss: 0.1543;  Loss pred: 0.1543; Loss self: 0.0000; time: 0.13s
Val loss: 0.4398 score: 0.8163 time: 0.06s
Test loss: 0.3908 score: 0.8958 time: 0.06s
Epoch 54/1000, LR 0.000269
Train loss: 0.1486;  Loss pred: 0.1486; Loss self: 0.0000; time: 0.13s
Val loss: 0.4382 score: 0.8163 time: 0.06s
Test loss: 0.3876 score: 0.8958 time: 0.06s
Epoch 55/1000, LR 0.000269
Train loss: 0.1463;  Loss pred: 0.1463; Loss self: 0.0000; time: 0.14s
Val loss: 0.4376 score: 0.8163 time: 0.06s
Test loss: 0.3855 score: 0.8750 time: 0.06s
Epoch 56/1000, LR 0.000269
Train loss: 0.1401;  Loss pred: 0.1401; Loss self: 0.0000; time: 0.13s
Val loss: 0.4381 score: 0.8163 time: 0.07s
Test loss: 0.3845 score: 0.8750 time: 0.06s
     INFO: Early stopping counter 1 of 20
Epoch 57/1000, LR 0.000269
Train loss: 0.1336;  Loss pred: 0.1336; Loss self: 0.0000; time: 0.23s
Val loss: 0.4387 score: 0.8163 time: 0.06s
Test loss: 0.3833 score: 0.8750 time: 0.06s
     INFO: Early stopping counter 2 of 20
Epoch 58/1000, LR 0.000269
Train loss: 0.1342;  Loss pred: 0.1342; Loss self: 0.0000; time: 0.14s
Val loss: 0.4388 score: 0.8163 time: 0.07s
Test loss: 0.3822 score: 0.8750 time: 0.07s
     INFO: Early stopping counter 3 of 20
Epoch 59/1000, LR 0.000268
Train loss: 0.1307;  Loss pred: 0.1307; Loss self: 0.0000; time: 0.16s
Val loss: 0.4389 score: 0.7959 time: 0.08s
Test loss: 0.3811 score: 0.8750 time: 0.07s
     INFO: Early stopping counter 4 of 20
Epoch 60/1000, LR 0.000268
Train loss: 0.1252;  Loss pred: 0.1252; Loss self: 0.0000; time: 0.19s
Val loss: 0.4391 score: 0.7959 time: 0.09s
Test loss: 0.3803 score: 0.8750 time: 0.08s
     INFO: Early stopping counter 5 of 20
Epoch 61/1000, LR 0.000268
Train loss: 0.1213;  Loss pred: 0.1213; Loss self: 0.0000; time: 0.15s
Val loss: 0.4356 score: 0.7959 time: 0.08s
Test loss: 0.3748 score: 0.8750 time: 0.08s
Epoch 62/1000, LR 0.000268
Train loss: 0.1186;  Loss pred: 0.1186; Loss self: 0.0000; time: 0.15s
Val loss: 0.4316 score: 0.8163 time: 0.08s
Test loss: 0.3683 score: 0.8750 time: 0.08s
Epoch 63/1000, LR 0.000268
Train loss: 0.1167;  Loss pred: 0.1167; Loss self: 0.0000; time: 0.15s
Val loss: 0.4265 score: 0.8163 time: 0.07s
Test loss: 0.3606 score: 0.8542 time: 0.07s
Epoch 64/1000, LR 0.000268
Train loss: 0.1215;  Loss pred: 0.1215; Loss self: 0.0000; time: 0.14s
Val loss: 0.4218 score: 0.8163 time: 0.07s
Test loss: 0.3537 score: 0.8542 time: 0.20s
Epoch 65/1000, LR 0.000268
Train loss: 0.1095;  Loss pred: 0.1095; Loss self: 0.0000; time: 0.15s
Val loss: 0.4210 score: 0.8163 time: 0.07s
Test loss: 0.3494 score: 0.8542 time: 0.07s
Epoch 66/1000, LR 0.000268
Train loss: 0.1071;  Loss pred: 0.1071; Loss self: 0.0000; time: 0.14s
Val loss: 0.4217 score: 0.8163 time: 0.06s
Test loss: 0.3498 score: 0.8542 time: 0.06s
     INFO: Early stopping counter 1 of 20
Epoch 67/1000, LR 0.000268
Train loss: 0.1030;  Loss pred: 0.1030; Loss self: 0.0000; time: 0.13s
Val loss: 0.4209 score: 0.8163 time: 0.06s
Test loss: 0.3492 score: 0.8542 time: 0.06s
Epoch 68/1000, LR 0.000268
Train loss: 0.1023;  Loss pred: 0.1023; Loss self: 0.0000; time: 0.13s
Val loss: 0.4196 score: 0.8163 time: 0.07s
Test loss: 0.3469 score: 0.8542 time: 0.07s
Epoch 69/1000, LR 0.000268
Train loss: 0.1039;  Loss pred: 0.1039; Loss self: 0.0000; time: 0.13s
Val loss: 0.4172 score: 0.8163 time: 0.06s
Test loss: 0.3418 score: 0.8542 time: 0.06s
Epoch 70/1000, LR 0.000268
Train loss: 0.1000;  Loss pred: 0.1000; Loss self: 0.0000; time: 0.13s
Val loss: 0.4142 score: 0.8163 time: 3.23s
Test loss: 0.3340 score: 0.8542 time: 3.09s
Epoch 71/1000, LR 0.000268
Train loss: 0.0965;  Loss pred: 0.0965; Loss self: 0.0000; time: 0.15s
Val loss: 0.4108 score: 0.8367 time: 0.07s
Test loss: 0.3270 score: 0.8750 time: 0.06s
Epoch 72/1000, LR 0.000267
Train loss: 0.0956;  Loss pred: 0.0956; Loss self: 0.0000; time: 0.14s
Val loss: 0.4065 score: 0.8367 time: 0.07s
Test loss: 0.3179 score: 0.9167 time: 0.07s
Epoch 73/1000, LR 0.000267
Train loss: 0.0886;  Loss pred: 0.0886; Loss self: 0.0000; time: 0.13s
Val loss: 0.4051 score: 0.8367 time: 0.06s
Test loss: 0.3121 score: 0.9167 time: 0.07s
Epoch 74/1000, LR 0.000267
Train loss: 0.0899;  Loss pred: 0.0899; Loss self: 0.0000; time: 0.13s
Val loss: 0.4064 score: 0.8367 time: 0.08s
Test loss: 0.3115 score: 0.9167 time: 0.07s
     INFO: Early stopping counter 1 of 20
Epoch 75/1000, LR 0.000267
Train loss: 0.0830;  Loss pred: 0.0830; Loss self: 0.0000; time: 0.15s
Val loss: 0.4063 score: 0.8367 time: 0.07s
Test loss: 0.3084 score: 0.9167 time: 0.06s
     INFO: Early stopping counter 2 of 20
Epoch 76/1000, LR 0.000267
Train loss: 0.0863;  Loss pred: 0.0863; Loss self: 0.0000; time: 0.13s
Val loss: 0.4028 score: 0.8367 time: 0.08s
Test loss: 0.3057 score: 0.9167 time: 0.06s
Epoch 77/1000, LR 0.000267
Train loss: 0.0819;  Loss pred: 0.0819; Loss self: 0.0000; time: 0.13s
Val loss: 0.3999 score: 0.8367 time: 0.06s
Test loss: 0.3006 score: 0.9167 time: 0.07s
Epoch 78/1000, LR 0.000267
Train loss: 0.0783;  Loss pred: 0.0783; Loss self: 0.0000; time: 0.13s
Val loss: 0.3958 score: 0.8163 time: 0.06s
Test loss: 0.2955 score: 0.8958 time: 0.06s
Epoch 79/1000, LR 0.000267
Train loss: 0.0797;  Loss pred: 0.0797; Loss self: 0.0000; time: 0.30s
Val loss: 0.3946 score: 0.8367 time: 0.06s
Test loss: 0.2939 score: 0.9167 time: 0.06s
Epoch 80/1000, LR 0.000267
Train loss: 0.0789;  Loss pred: 0.0789; Loss self: 0.0000; time: 0.13s
Val loss: 0.3883 score: 0.8776 time: 0.06s
Test loss: 0.2888 score: 0.9167 time: 0.06s
Epoch 81/1000, LR 0.000267
Train loss: 0.0727;  Loss pred: 0.0727; Loss self: 0.0000; time: 0.14s
Val loss: 0.3878 score: 0.8367 time: 0.06s
Test loss: 0.2856 score: 0.9167 time: 0.07s
Epoch 82/1000, LR 0.000267
Train loss: 0.0718;  Loss pred: 0.0718; Loss self: 0.0000; time: 0.14s
Val loss: 0.3828 score: 0.8367 time: 0.06s
Test loss: 0.2805 score: 0.9167 time: 0.06s
Epoch 83/1000, LR 0.000266
Train loss: 0.0715;  Loss pred: 0.0715; Loss self: 0.0000; time: 0.14s
Val loss: 0.3798 score: 0.8367 time: 0.19s
Test loss: 0.2744 score: 0.9167 time: 0.06s
Epoch 84/1000, LR 0.000266
Train loss: 0.0735;  Loss pred: 0.0735; Loss self: 0.0000; time: 0.13s
Val loss: 0.3790 score: 0.8571 time: 0.07s
Test loss: 0.2695 score: 0.8958 time: 0.06s
Epoch 85/1000, LR 0.000266
Train loss: 0.0648;  Loss pred: 0.0648; Loss self: 0.0000; time: 0.13s
Val loss: 0.3775 score: 0.8571 time: 0.06s
Test loss: 0.2651 score: 0.9167 time: 0.06s
Epoch 86/1000, LR 0.000266
Train loss: 0.0652;  Loss pred: 0.0652; Loss self: 0.0000; time: 0.12s
Val loss: 0.3773 score: 0.8776 time: 0.06s
Test loss: 0.2617 score: 0.8958 time: 0.06s
Epoch 87/1000, LR 0.000266
Train loss: 0.0650;  Loss pred: 0.0650; Loss self: 0.0000; time: 0.13s
Val loss: 0.3730 score: 0.8776 time: 0.06s
Test loss: 0.2558 score: 0.9375 time: 0.06s
Epoch 88/1000, LR 0.000266
Train loss: 0.0644;  Loss pred: 0.0644; Loss self: 0.0000; time: 0.14s
Val loss: 0.3689 score: 0.8571 time: 0.07s
Test loss: 0.2510 score: 0.9167 time: 0.06s
Epoch 89/1000, LR 0.000266
Train loss: 0.0646;  Loss pred: 0.0646; Loss self: 0.0000; time: 0.13s
Val loss: 0.3677 score: 0.8776 time: 0.06s
Test loss: 0.2483 score: 0.9167 time: 0.06s
Epoch 90/1000, LR 0.000266
Train loss: 0.0615;  Loss pred: 0.0615; Loss self: 0.0000; time: 0.13s
Val loss: 0.3676 score: 0.8776 time: 0.06s
Test loss: 0.2447 score: 0.9167 time: 0.06s
Epoch 91/1000, LR 0.000266
Train loss: 0.0593;  Loss pred: 0.0593; Loss self: 0.0000; time: 0.13s
Val loss: 0.3629 score: 0.8571 time: 0.06s
Test loss: 0.2418 score: 0.9167 time: 0.06s
Epoch 92/1000, LR 0.000266
Train loss: 0.0610;  Loss pred: 0.0610; Loss self: 0.0000; time: 0.13s
Val loss: 0.3593 score: 0.8571 time: 0.06s
Test loss: 0.2399 score: 0.9167 time: 0.19s
Epoch 93/1000, LR 0.000265
Train loss: 0.0581;  Loss pred: 0.0581; Loss self: 0.0000; time: 0.12s
Val loss: 0.3627 score: 0.8571 time: 0.06s
Test loss: 0.2361 score: 0.9167 time: 0.06s
     INFO: Early stopping counter 1 of 20
Epoch 94/1000, LR 0.000265
Train loss: 0.0573;  Loss pred: 0.0573; Loss self: 0.0000; time: 0.13s
Val loss: 0.3631 score: 0.8571 time: 0.06s
Test loss: 0.2340 score: 0.9167 time: 0.06s
     INFO: Early stopping counter 2 of 20
Epoch 95/1000, LR 0.000265
Train loss: 0.0565;  Loss pred: 0.0565; Loss self: 0.0000; time: 0.13s
Val loss: 0.3619 score: 0.8571 time: 0.06s
Test loss: 0.2312 score: 0.9167 time: 0.06s
     INFO: Early stopping counter 3 of 20
Epoch 96/1000, LR 0.000265
Train loss: 0.0542;  Loss pred: 0.0542; Loss self: 0.0000; time: 0.13s
Val loss: 0.3616 score: 0.8571 time: 0.06s
Test loss: 0.2293 score: 0.9167 time: 0.06s
     INFO: Early stopping counter 4 of 20
Epoch 97/1000, LR 0.000265
Train loss: 0.0526;  Loss pred: 0.0526; Loss self: 0.0000; time: 0.13s
Val loss: 0.3595 score: 0.8571 time: 0.06s
Test loss: 0.2275 score: 0.9167 time: 0.06s
     INFO: Early stopping counter 5 of 20
Epoch 98/1000, LR 0.000265
Train loss: 0.0515;  Loss pred: 0.0515; Loss self: 0.0000; time: 0.12s
Val loss: 0.3601 score: 0.8571 time: 0.07s
Test loss: 0.2251 score: 0.9167 time: 0.07s
     INFO: Early stopping counter 6 of 20
Epoch 99/1000, LR 0.000265
Train loss: 0.0501;  Loss pred: 0.0501; Loss self: 0.0000; time: 0.13s
Val loss: 0.3622 score: 0.8571 time: 0.06s
Test loss: 0.2235 score: 0.9167 time: 0.06s
     INFO: Early stopping counter 7 of 20
Epoch 100/1000, LR 0.000265
Train loss: 0.0494;  Loss pred: 0.0494; Loss self: 0.0000; time: 0.13s
Val loss: 0.3624 score: 0.8571 time: 0.06s
Test loss: 0.2210 score: 0.9167 time: 0.06s
     INFO: Early stopping counter 8 of 20
Epoch 101/1000, LR 0.000265
Train loss: 0.0494;  Loss pred: 0.0494; Loss self: 0.0000; time: 0.13s
Val loss: 0.3602 score: 0.8571 time: 0.06s
Test loss: 0.2189 score: 0.9167 time: 0.19s
     INFO: Early stopping counter 9 of 20
Epoch 102/1000, LR 0.000264
Train loss: 0.0479;  Loss pred: 0.0479; Loss self: 0.0000; time: 0.32s
Val loss: 0.3576 score: 0.8571 time: 0.06s
Test loss: 0.2174 score: 0.9167 time: 0.06s
Epoch 103/1000, LR 0.000264
Train loss: 0.0451;  Loss pred: 0.0451; Loss self: 0.0000; time: 0.13s
Val loss: 0.3572 score: 0.8571 time: 0.06s
Test loss: 0.2159 score: 0.9167 time: 0.06s
Epoch 104/1000, LR 0.000264
Train loss: 0.0477;  Loss pred: 0.0477; Loss self: 0.0000; time: 0.13s
Val loss: 0.3573 score: 0.8571 time: 0.06s
Test loss: 0.2150 score: 0.9167 time: 0.06s
     INFO: Early stopping counter 1 of 20
Epoch 105/1000, LR 0.000264
Train loss: 0.0438;  Loss pred: 0.0438; Loss self: 0.0000; time: 0.13s
Val loss: 0.3613 score: 0.8571 time: 0.06s
Test loss: 0.2145 score: 0.9167 time: 0.06s
     INFO: Early stopping counter 2 of 20
Epoch 106/1000, LR 0.000264
Train loss: 0.0442;  Loss pred: 0.0442; Loss self: 0.0000; time: 0.13s
Val loss: 0.3633 score: 0.8571 time: 0.06s
Test loss: 0.2138 score: 0.9167 time: 0.06s
     INFO: Early stopping counter 3 of 20
Epoch 107/1000, LR 0.000264
Train loss: 0.0430;  Loss pred: 0.0430; Loss self: 0.0000; time: 0.13s
Val loss: 0.3623 score: 0.8571 time: 0.06s
Test loss: 0.2146 score: 0.9167 time: 0.06s
     INFO: Early stopping counter 4 of 20
Epoch 108/1000, LR 0.000264
Train loss: 0.0465;  Loss pred: 0.0465; Loss self: 0.0000; time: 0.13s
Val loss: 0.3626 score: 0.8980 time: 0.06s
Test loss: 0.2160 score: 0.9167 time: 0.06s
     INFO: Early stopping counter 5 of 20
Epoch 109/1000, LR 0.000264
Train loss: 0.0435;  Loss pred: 0.0435; Loss self: 0.0000; time: 0.13s
Val loss: 0.3635 score: 0.8980 time: 0.06s
Test loss: 0.2161 score: 0.9167 time: 0.06s
     INFO: Early stopping counter 6 of 20
Epoch 110/1000, LR 0.000263
Train loss: 0.0418;  Loss pred: 0.0418; Loss self: 0.0000; time: 0.13s
Val loss: 0.3661 score: 0.8980 time: 0.18s
Test loss: 0.2159 score: 0.8958 time: 0.06s
     INFO: Early stopping counter 7 of 20
Epoch 111/1000, LR 0.000263
Train loss: 0.0407;  Loss pred: 0.0407; Loss self: 0.0000; time: 0.14s
Val loss: 0.3692 score: 0.8980 time: 0.06s
Test loss: 0.2155 score: 0.8958 time: 0.06s
     INFO: Early stopping counter 8 of 20
Epoch 112/1000, LR 0.000263
Train loss: 0.0394;  Loss pred: 0.0394; Loss self: 0.0000; time: 0.14s
Val loss: 0.3685 score: 0.8980 time: 0.06s
Test loss: 0.2158 score: 0.8958 time: 0.06s
     INFO: Early stopping counter 9 of 20
Epoch 113/1000, LR 0.000263
Train loss: 0.0414;  Loss pred: 0.0414; Loss self: 0.0000; time: 0.13s
Val loss: 0.3710 score: 0.8571 time: 0.06s
Test loss: 0.2119 score: 0.8958 time: 0.06s
     INFO: Early stopping counter 10 of 20
Epoch 114/1000, LR 0.000263
Train loss: 0.0381;  Loss pred: 0.0381; Loss self: 0.0000; time: 0.15s
Val loss: 0.3693 score: 0.8980 time: 0.15s
Test loss: 0.2132 score: 0.8958 time: 0.06s
     INFO: Early stopping counter 11 of 20
Epoch 115/1000, LR 0.000263
Train loss: 0.0397;  Loss pred: 0.0397; Loss self: 0.0000; time: 0.13s
Val loss: 0.3684 score: 0.8980 time: 0.06s
Test loss: 0.2165 score: 0.8958 time: 0.06s
     INFO: Early stopping counter 12 of 20
Epoch 116/1000, LR 0.000263
Train loss: 0.0376;  Loss pred: 0.0376; Loss self: 0.0000; time: 0.13s
Val loss: 0.3768 score: 0.8980 time: 0.06s
Test loss: 0.2149 score: 0.8958 time: 0.06s
     INFO: Early stopping counter 13 of 20
Epoch 117/1000, LR 0.000262
Train loss: 0.0376;  Loss pred: 0.0376; Loss self: 0.0000; time: 0.13s
Val loss: 0.3792 score: 0.8980 time: 0.06s
Test loss: 0.2155 score: 0.8958 time: 0.06s
     INFO: Early stopping counter 14 of 20
Epoch 118/1000, LR 0.000262
Train loss: 0.0385;  Loss pred: 0.0385; Loss self: 0.0000; time: 0.13s
Val loss: 0.3801 score: 0.8980 time: 0.09s
Test loss: 0.2188 score: 0.8958 time: 0.06s
     INFO: Early stopping counter 15 of 20
Epoch 119/1000, LR 0.000262
Train loss: 0.0363;  Loss pred: 0.0363; Loss self: 0.0000; time: 0.13s
Val loss: 0.3834 score: 0.8980 time: 0.14s
Test loss: 0.2131 score: 0.8958 time: 0.06s
     INFO: Early stopping counter 16 of 20
Epoch 120/1000, LR 0.000262
Train loss: 0.0352;  Loss pred: 0.0352; Loss self: 0.0000; time: 0.13s
Val loss: 0.3820 score: 0.8776 time: 0.21s
Test loss: 0.2109 score: 0.8958 time: 0.06s
     INFO: Early stopping counter 17 of 20
Epoch 121/1000, LR 0.000262
Train loss: 0.0363;  Loss pred: 0.0363; Loss self: 0.0000; time: 0.14s
Val loss: 0.3812 score: 0.8980 time: 0.06s
Test loss: 0.2117 score: 0.8958 time: 0.06s
     INFO: Early stopping counter 18 of 20
Epoch 122/1000, LR 0.000262
Train loss: 0.0331;  Loss pred: 0.0331; Loss self: 0.0000; time: 0.14s
Val loss: 0.3817 score: 0.8980 time: 0.06s
Test loss: 0.2156 score: 0.8958 time: 0.06s
     INFO: Early stopping counter 19 of 20
Epoch 123/1000, LR 0.000262
Train loss: 0.0347;  Loss pred: 0.0347; Loss self: 0.0000; time: 0.13s
Val loss: 0.3817 score: 0.8980 time: 0.06s
Test loss: 0.2200 score: 0.8958 time: 0.06s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 102,   Train_Loss: 0.0451,   Val_Loss: 0.3572,   Val_Precision: 0.8750,   Val_Recall: 0.8400,   Val_accuracy: 0.8571,   Val_Score: 0.8571,   Val_Loss: 0.3572,   Test_Precision: 0.9545,   Test_Recall: 0.8750,   Test_accuracy: 0.9130,   Test_Score: 0.9167,   Test_loss: 0.2159


[0.08030457305721939, 0.08224903489463031, 0.08930992404930294, 0.09014386311173439, 0.09109797701239586, 0.11844071093946695, 0.09641301794908941, 0.0792765119113028, 0.08809523889794946, 0.07997036795131862, 0.08830716903321445, 0.0856172798667103, 0.08147128485143185, 0.08817671611905098, 0.08749923179857433, 0.07961139804683626, 0.2485009920783341, 0.08937256503850222, 0.08282550307922065, 0.09621028997935355, 0.08198193786665797, 0.09247422916814685, 0.08144118287600577, 0.08936924301087856, 0.08664764906279743, 0.09125145291909575, 0.12373684905469418, 0.07643555803224444, 0.08294393913820386, 0.07940995809622109, 0.0934932001400739, 0.08350839791819453, 0.08147210790775716, 0.08141372003592551, 0.08202403411269188, 0.08147556590847671, 0.0813275899272412, 0.08863172004930675, 0.09473197697661817, 0.08491315809078515, 0.08734845509752631, 0.08073536702431738, 0.08964998694136739, 0.09078946919180453, 0.08566621784120798, 0.08715541195124388, 0.0761185169685632, 0.08477532188408077, 0.08474085899069905, 0.07614627294242382, 0.07789773400872946, 0.08448924799449742, 0.07630238891579211, 0.08805670589208603, 0.07739226799458265, 0.08294420479796827, 0.07962217694148421, 0.0772191770374775, 0.08571332995779812, 0.08586748992092907, 0.07830905099399388, 0.08032047096639872, 0.08107276004739106, 0.08169830404222012, 0.08374937996268272, 0.08809681003913283, 0.08287597191520035, 0.08143398794345558, 0.08135853102430701, 0.09978363802656531, 0.08035077899694443, 0.07949860696680844, 0.0802019510883838, 0.07999211107380688, 0.08579550893045962, 0.08731993706896901, 0.07981866295449436, 0.08068527188152075, 0.080497439019382, 0.08082063402980566, 0.08700373792089522, 0.08751884801313281, 0.08023286797106266, 0.08173221489414573, 0.07971715182065964, 0.08123901719227433, 0.08858295599929988, 0.09337678295560181, 0.0839238129556179, 0.08465743390843272, 0.08520442084409297, 0.09067314211279154, 0.08440718008205295, 0.07953035994432867, 0.08174778893589973, 0.08658345299772918, 0.09255612595006824, 0.08059333101846278, 0.08125989604741335, 0.08189757610671222, 0.08078978699631989, 0.0807079819496721, 0.08810480288229883, 0.08207276184111834, 0.08807546179741621, 0.08000069414265454, 0.08125029504299164, 0.07794837700203061, 0.08079420914873481, 0.07980709685944021, 0.08226426900364459, 0.08085592114366591, 0.08267049095593393, 0.08617170481011271, 0.17299658711999655, 0.08482114807702601, 0.0804522989783436, 0.08762490097433329, 0.0854366491548717, 0.07922095083631575, 0.0876025459729135, 0.08143193693831563, 0.2240986900869757, 0.08001524396240711, 0.0812943868804723, 0.08208154304884374, 0.08227636781521142, 0.08359456597827375, 0.0824424340389669, 0.21290116012096405, 0.08042746712453663, 0.08712090598419309, 0.07937311194837093, 0.08033056394197047, 0.08049654704518616, 0.08681910601444542, 0.0825569189619273, 0.07881353911943734, 0.09111546585336328, 0.08360462612472475, 0.0845167029183358, 0.08445483213290572, 0.07935727410949767, 0.07964708399958909, 0.07883037719875574, 0.08398926886729896, 0.086457114899531, 0.09179740399122238, 0.09486383688636124, 0.07502445694990456, 0.07620215509086847, 0.07894653407856822, 0.07977849687449634, 0.07867218204773962, 0.07770778192207217, 0.07537034898996353, 0.08335970202460885, 0.09075451013632119, 0.0828078668564558, 0.0815802679862827, 0.08806540095247328, 0.0829521119594574, 0.08340814989060163, 0.08894365094602108, 0.08431104989722371, 0.09056340320967138, 0.08439316996373236, 0.09225691086612642, 0.21504150214605033, 0.08172729285433888, 0.09246477787382901, 0.09292462514713407, 0.0815810770727694, 0.08912649191915989, 0.08133404492400587, 0.08422476006671786, 0.08559111692011356, 0.08433506591245532, 0.09098736895248294, 0.08515111915767193, 0.08235825411975384, 0.08397795399650931, 0.08750092005357146, 0.08031795104034245, 0.0806394258979708, 0.08020767103880644, 0.09038139693439007, 0.09383490006439388, 0.0876111991237849, 0.08012059493921697, 0.08372850995510817, 0.07632787898182869, 0.08655928797088563, 0.083849007030949, 0.08815013198181987, 0.08080439805053174, 0.08191174292005599, 0.08484356408007443, 0.08602913306094706, 0.08166135707870126, 0.081294174073264, 0.08829266298562288, 0.08728120103478432, 0.07839274196885526, 0.08222551480866969, 0.08677568193525076, 0.08073866693302989, 0.07968996418640018, 0.08031068299897015, 0.0793565260246396, 0.083883993094787, 0.07896813796833158, 0.08043512189760804, 3.1389988330192864, 0.08781717019155622, 0.08853760408237576, 0.08850313094444573, 0.08148031705059111, 0.09099357784725726, 0.07925136294215918, 0.08570395014248788, 0.07912908890284598, 0.07772012799978256, 0.0783867840655148, 0.07783869816921651, 0.08104374492540956, 0.08878597291186452, 0.09304617205634713, 0.08039137814193964, 0.07801122707314789, 0.08217786019667983, 0.08006361313164234, 0.08919504680670798, 0.08655560482293367, 0.08136654179543257, 0.09288546000607312, 0.0808763571549207, 0.08227156801149249, 0.08006966300308704, 0.07933972892351449, 0.07949276501312852, 0.08585914387367666, 0.08646708191372454, 0.08286619605496526, 0.0944470230024308, 0.10138511913828552, 0.08402592013590038, 0.09071871591731906, 0.08441196009516716, 0.07608430879190564, 0.07916632993146777, 0.08422963903285563, 0.07862806296907365, 0.08678169501945376, 0.08559981896542013, 0.08104844205081463, 0.08974122698418796, 0.0834859109017998, 0.08554472797550261, 0.08711268613114953, 0.0874496279284358, 0.0771687210071832, 0.07773052318952978, 0.07603398896753788, 0.08362016105093062, 0.22728594904765487, 0.08062042994424701, 0.07503805006854236, 0.07535188505426049, 0.08291839389130473, 0.08075170894153416, 0.07920359820127487, 0.07549891504459083, 0.08223382500000298, 0.08598316111601889, 0.09102195315063, 0.09403719496913254, 0.10055740014649928, 0.10349814407527447, 0.1059724020306021, 0.09366644197143614, 0.09390345495194197, 0.09544716798700392, 0.09176356508396566, 0.09553138795308769, 0.09667627210728824, 0.08872763998806477, 0.09016496199183166, 0.09190295985899866, 0.09520161896944046, 0.0995130620431155, 0.09546651714481413, 0.10722136893309653, 0.23343268921598792, 0.09606244880706072, 0.09771234588697553, 0.09453448699787259, 0.09721811884082854, 0.09579656715504825, 0.09326348011381924, 0.23155285511165857, 0.10935897892341018, 0.10676278010942042, 0.0965085169300437, 0.09676293912343681, 0.10080181690864265, 0.10244493815116584, 0.2426771919708699, 0.0989117540884763, 0.09172388608567417, 0.08839146886020899, 0.09051968390122056, 0.09557897504419088, 0.0889516631141305, 0.09731163713149726, 0.09938210994005203, 0.09408206306397915, 0.1176825340371579, 0.08971951296553016, 0.08713490213267505, 0.09412935585714877, 0.09503735299222171, 0.11941336607560515, 0.095992396119982, 0.09594150492921472, 0.0942650109063834, 0.09103222796693444, 0.09234098996967077, 0.09032848291099072, 0.20952927391044796, 0.09592152805998921, 0.10747068002820015, 0.09765137499198318, 0.1071139860432595, 0.09447709494270384, 0.10276143788360059, 0.23226110287941992, 0.08838011603802443, 0.11349277291446924, 0.0918549899943173, 0.09932742291130126, 0.09862561104819179, 0.09597166907042265, 0.16922593396157026, 0.09121826104819775, 0.0915480989497155, 0.0934638970065862, 0.0893515080679208, 0.0915285749360919, 0.0895526718813926, 0.09580472903326154, 0.1019158570561558, 0.09800874604843557, 0.09220308880321681, 0.0911259890999645, 0.09082124102860689, 0.09169404185377061, 0.09329417115077376, 0.09414243092760444, 0.09311960195191205, 0.09531634487211704, 0.09208682808093727, 0.09051363216713071, 0.09203573479317129, 0.09184935293160379, 0.09298204095102847, 0.09307082300074399, 0.09152893209829926, 0.09326903289183974, 0.0925841829739511, 0.09298559883609414, 0.10496832407079637, 0.09883092204108834, 0.24895496806129813, 0.10115955700166523, 0.09139556181617081, 0.10837380704469979, 0.08815274899825454, 0.09285447909496725, 0.10164532111957669, 0.10528044402599335, 0.10240499395877123, 0.0913395641837269, 0.09352334891445935, 0.1001540650613606, 0.10910576791502535, 0.09558773320168257, 0.09709813422523439, 0.09292122488841414, 0.10404382995329797, 0.1046977408695966, 0.09354816796258092, 0.09322930895723403, 0.09428084013052285, 0.09161618794314563, 0.09264071797952056, 0.09786986885592341, 0.06658551096916199, 0.06746192090213299, 0.06330023799091578, 0.07247142097912729, 0.07497126399539411, 0.07300261897034943, 0.06562251481227577, 0.06622639484703541, 0.06684802914969623, 0.06758839497342706, 0.06789872096851468, 0.06745852809399366, 0.06810442986898124, 0.06743709417060018, 0.06667873892001808, 0.06939747487194836, 0.06843920308165252, 0.06740538589656353, 0.06787471892312169, 0.07370050111785531, 0.06617916701361537, 0.06438974780030549, 0.06312258099205792, 0.07502088299952447, 0.06765399593859911, 0.07013704697601497, 0.0675201581325382, 0.06449571391567588, 0.0811802230309695, 0.07369082910008729, 0.07079912908375263, 0.06453263387084007, 0.0640874260570854, 0.08163894293829799, 0.06932092295028269, 0.0688386659603566, 0.07124229008331895, 0.06339712301269174, 0.0627420349046588, 0.06445990409702063, 0.06393900490365922, 0.0692273611202836, 0.06848493218421936, 0.06778843514621258, 0.06809147796593606, 0.06671270006336272, 0.06752628111280501, 0.06813372881151736, 0.06726583419367671, 0.06321201706305146, 0.06725888606160879, 0.06705947499722242, 0.06666651391424239, 0.06739843008108437, 0.06672146497294307, 0.06716638896614313, 0.07157686213031411, 0.07366652903147042, 0.07979958388023078, 0.08311496209353209, 0.08299684501253068, 0.08259157184511423, 0.07380631007254124, 0.20987479481846094, 0.07885138713754714, 0.06792073207907379, 0.06757490406744182, 0.07077000197023153, 0.0688488760497421, 3.099480517907068, 0.07002181699499488, 0.07150496984831989, 0.07586893578991294, 0.07637046789750457, 0.06875174213200808, 0.06990065309219062, 0.0709730179514736, 0.06726316804997623, 0.06888252403587103, 0.06979026109911501, 0.07249840209260583, 0.06754786102101207, 0.06527630798518658, 0.06815902912057936, 0.06814852496609092, 0.06860266206786036, 0.06887182011269033, 0.06914318609051406, 0.06817964091897011, 0.06818214897066355, 0.06761050992645323, 0.1962495669722557, 0.06701786699704826, 0.06674532499164343, 0.06890068599022925, 0.06170982704497874, 0.06146640004590154, 0.07531679607927799, 0.06864473689347506, 0.0668200810905546, 0.1968195359222591, 0.06285765091888607, 0.06224680691957474, 0.062406960874795914, 0.06213562306948006, 0.06347453314810991, 0.06341942586004734, 0.06250102398917079, 0.06238032295368612, 0.06831023003906012, 0.06896779616363347, 0.06602873583324254, 0.06760845193639398, 0.06806121184490621, 0.06788965803571045, 0.0683250199072063, 0.06858721491880715, 0.06862794910557568, 0.06847074418328702, 0.0691543179564178, 0.06961625698022544, 0.06898248381912708, 0.06956648314371705]
[0.0016388688379024364, 0.0016785517325434759, 0.001822651511210264, 0.0018396706757496815, 0.0018591423880080788, 0.0024171573661115704, 0.0019676126112059063, 0.0016178879981898532, 0.0017978620183254992, 0.0016320483255371147, 0.0018021871231268256, 0.0017472914258512308, 0.0016626792826822826, 0.0017995248187561424, 0.00178569860813417, 0.0016247224091191072, 0.005071448817925185, 0.0018239298987449432, 0.00169031638937185, 0.0019634753057010926, 0.0016731007727889382, 0.0018872291666968744, 0.001662064956653179, 0.0018238621022628279, 0.001768319368628519, 0.001862274549369301, 0.002525241817442738, 0.0015599093475968254, 0.0016927334518000788, 0.0016206113897187977, 0.0019080244926545694, 0.001704253018738664, 0.0016626960797501461, 0.001661504490529092, 0.0016739598798508548, 0.0016627666511934024, 0.0016597467332090043, 0.001808810613251158, 0.0019333056525840443, 0.0017329215936894929, 0.0017826215326025778, 0.0016476605515166813, 0.0018295915702319875, 0.0018528463100368272, 0.0017482901600246526, 0.0017786818765559975, 0.0015534391218074122, 0.0017301086098791994, 0.0017294052855244704, 0.0015540055702535473, 0.00158974967364754, 0.001724270367234641, 0.0015571916105263696, 0.0017970756304507352, 0.0015794340407057684, 0.0016927388734279238, 0.0016249423865609023, 0.0015759015721934183, 0.0017492516317917984, 0.0017523977534883485, 0.0015981438978366097, 0.0016391932850285452, 0.001654546123416144, 0.0016673123273922472, 0.0017091710196465862, 0.0017978940824312823, 0.0016913463656163337, 0.0016619181212950119, 0.0016603781841695309, 0.0020364007760523533, 0.001639811816264172, 0.0016224205503430292, 0.0016367745120078325, 0.0016324920627307526, 0.0017509287536828493, 0.0017820395320197757, 0.0016289523051937623, 0.0016466382016636888, 0.0016428048779465714, 0.0016494006944858298, 0.0017755864881815351, 0.0017860989390435268, 0.0016374054687971972, 0.0016680043855948107, 0.001626880649401217, 0.0016579391263729455, 0.0018078154285571405, 0.0019056486317469757, 0.0017127308766452633, 0.0017277027328251576, 0.0017388657315121014, 0.0018504722880161538, 0.0017225955118786317, 0.0016230685702924217, 0.0016683222231816273, 0.001767009244851616, 0.0018889005295932293, 0.0016447618575196486, 0.0016583652254574153, 0.0016713791042186168, 0.0016487711631902019, 0.0016471016724422878, 0.0017980572016795678, 0.0016749543232881293, 0.0017974584040289022, 0.001632667227401113, 0.0016581692865916661, 0.0015907832041230736, 0.0016488614111986695, 0.0016287162624375553, 0.0016788626327274405, 0.0016501208396666512, 0.0016871528766517127, 0.001758606220614545, 0.003530542594285644, 0.0017310438383066533, 0.0016418836526192573, 0.0017882632851904752, 0.0017436050847933, 0.0016167540987003215, 0.0017878070606717042, 0.0016618762640472576, 0.004573442654836239, 0.0016329641624981044, 0.0016590691200096387, 0.001675133531609056, 0.0016791095472492126, 0.0017060115505770153, 0.0016824986538564672, 0.0043449216351217155, 0.0016413768800925843, 0.0017779776731467977, 0.0016198594275177742, 0.0016393992641218463, 0.0016427866743915543, 0.0017718184900907229, 0.0016848350808556592, 0.001608439573866068, 0.0018594993031298627, 0.00170621685968826, 0.0017248306718027713, 0.0017235680027123616, 0.001619536206316279, 0.001625450693869165, 0.0016087832081378723, 0.0017140667115775297, 0.0017644309163169594, 0.0018734164079841301, 0.0019359966711502295, 0.0015311113663245828, 0.001555146022262622, 0.0016111537567054738, 0.0016281325892754355, 0.0016055547356681557, 0.0015858731004504524, 0.001538170387550276, 0.0017012184086654867, 0.0018521328599249221, 0.0016899564664582818, 0.0016649034282914838, 0.00179725308066272, 0.0016929002440705591, 0.0017022071406245232, 0.0018151765499187975, 0.0017206336713719125, 0.001848232718564722, 0.0017223095910965788, 0.0018827940993087025, 0.004388602084613272, 0.0016679039358028344, 0.0018870362831393676, 0.0018964209213700829, 0.0016649199402606, 0.0018189079983502018, 0.0016598784678368544, 0.0017188726544228134, 0.0017467574881655829, 0.0017211237941317412, 0.001856885080662917, 0.0017377779419933046, 0.001680780696321507, 0.0017138357958471288, 0.001785733062317785, 0.0016391418579661725, 0.0016457025693463428, 0.0016368912456899273, 0.0018445183047834709, 0.0019149979604978341, 0.001787983655587447, 0.0016351141824329995, 0.0017087451011246564, 0.0015577118159556876, 0.0017665160810384824, 0.001711204225121408, 0.0017989822853432627, 0.0016490693479700356, 0.0016716682228582855, 0.001731501307756621, 0.0017556965930805523, 0.001666558307728597, 0.0016590647770053878, 0.0018018910813392425, 0.001781249000709884, 0.0015998518769154136, 0.0016780717307891772, 0.0017709322843928726, 0.0016477278965924466, 0.0016263257997224526, 0.0016389935305912275, 0.0016195209392783592, 0.0017119182264242246, 0.00161159465241493, 0.0016415330999511846, 0.06406120067386299, 0.0017921871467664534, 0.0018068898792321585, 0.0018061863458050148, 0.0016628636132773695, 0.0018570117928011685, 0.0016173747539216159, 0.0017490602069895487, 0.0016148793653642036, 0.0015861250612200523, 0.0015997302870513225, 0.0015885448605962554, 0.0016539539780695827, 0.0018119586308543779, 0.0018989014705376966, 0.001640640370243666, 0.0015920658586356714, 0.0016770991876873436, 0.0016339512884008642, 0.001820307077687918, 0.0017664409147537484, 0.0016605416692945423, 0.0018956216327770023, 0.0016505379011208306, 0.0016790115920712753, 0.0016340747551650417, 0.001619178141296214, 0.0016223013267985412, 0.0017522274259934012, 0.0017646343247698887, 0.0016911468582645971, 0.0019274902653557305, 0.002069084064046643, 0.0017148146966510281, 0.0018514023656595725, 0.0017226930631666767, 0.0015527409957531765, 0.001615639386356485, 0.001718972225160319, 0.0016046543463076257, 0.0017710550003970156, 0.0017469350809269414, 0.001654049837771727, 0.0018314536119222033, 0.0017037941000367306, 0.0017458107750102573, 0.001777809921043868, 0.001784686284253792, 0.001574871857289453, 0.0015863372079495872, 0.0015517140605619975, 0.001706533898998584, 0.004638488756074589, 0.0016453148968213675, 0.0015313887769090278, 0.0015377935725359284, 0.0016922121202307089, 0.0016479940600313094, 0.001616399963291324, 0.0015407941845834864, 0.001678241326530673, 0.0017547583901228346, 0.001857590880625102, 0.0019191264279414804, 0.002052191839724475, 0.002112207021944377, 0.0021627020822571857, 0.0019115600402333907, 0.0019163970398355502, 0.001947901387489876, 0.0018727258180401155, 0.001949620162307912, 0.0019729851450466986, 0.00181076816302173, 0.0018401012651394217, 0.0018755706093673194, 0.0019428901830498054, 0.0020308788172064387, 0.001948296268261513, 0.0021881912027162556, 0.004763932432979345, 0.0019604581389196067, 0.0019941295078974596, 0.0019292752448545427, 0.001984043241649562, 0.001955031982756087, 0.0019033363288534538, 0.004725568471666501, 0.002231815896396126, 0.002178832247131029, 0.001969561570000892, 0.001974753859661976, 0.0020571799369110745, 0.0020907130234931806, 0.004952595754507549, 0.0020186072262954346, 0.001871916042564779, 0.001803907527759367, 0.0018473404877800115, 0.001950591327432467, 0.0018153400635536835, 0.0019859517781938216, 0.0020282063253071842, 0.0019200421033465133, 0.002401684368105263, 0.001831010468684289, 0.0017782633088301032, 0.0019210072623907911, 0.0019395378161677901, 0.0024370074709307173, 0.0019590284922445305, 0.001957989896514586, 0.0019237757327833346, 0.001857800570753764, 0.0018845099993810362, 0.0018434384267549126, 0.004276107630825469, 0.0019575822053059023, 0.0021932791842489826, 0.001992885203918024, 0.002185999715168561, 0.0019281039784225275, 0.0020971722017061344, 0.004740022507743264, 0.0018036758375107025, 0.0023161790390708007, 0.0018745916325370877, 0.002027090263495944, 0.0020127675724120773, 0.0019586054912331154, 0.003453590489011638, 0.0018615971642489337, 0.0018683285499941937, 0.0019074264695221673, 0.001823500164651445, 0.0018679301007365693, 0.0018276055485998489, 0.001955198551699215, 0.0020799154501256285, 0.0020001784907843993, 0.0018816956898615677, 0.0018597140632645817, 0.0018534947148695284, 0.0018713069766075636, 0.0019039626765464032, 0.0019212741005633558, 0.0019004000398349396, 0.0019452315280023887, 0.0018793230220599442, 0.0018472169830026676, 0.0018782803019014548, 0.0018744765904408936, 0.0018975926724699686, 0.0018994045510355914, 0.0018679373897612095, 0.0019034496508538723, 0.0018894731219173694, 0.0018976652823692681, 0.002142210695322375, 0.0020169575926752724, 0.0050807136339040436, 0.0020644807551360254, 0.001865215547268792, 0.002211710347851016, 0.0017990356938419293, 0.0018949893692850458, 0.0020743943085627897, 0.0021485804903263947, 0.00208989783589329, 0.0018640727384434063, 0.0019086397737644765, 0.0020439605114563387, 0.0022266483247964357, 0.0019507700653404606, 0.0019815945760251917, 0.0018963515283349826, 0.0021233434684346524, 0.002136688589175441, 0.001909146284950631, 0.0019026389583108984, 0.0019240987781739356, 0.0018697181212886864, 0.0018906268975412358, 0.001997344262365784, 0.0013871981451908748, 0.001405456685461104, 0.0013187549581440787, 0.0015098212703984852, 0.0015619013332373772, 0.001520887895215613, 0.0013671357252557452, 0.0013797165593132377, 0.0013926672739520047, 0.001408091561946397, 0.0014145566868440558, 0.0014053860019582014, 0.0014188422889371093, 0.0014049394618875037, 0.0013891403941670433, 0.0014457807264989242, 0.001425816730867761, 0.0014042788728450735, 0.001414056644231702, 0.0015354271066219856, 0.0013787326461169869, 0.001341453079173031, 0.0013150537706678733, 0.0015629350624900933, 0.0014094582487208147, 0.0014611884786669787, 0.001406669961094546, 0.0013436607065765809, 0.0016912546464785312, 0.0015352256062518184, 0.001474981855911513, 0.001344429872309168, 0.0013351547095226124, 0.0017008113112145413, 0.001444185894797556, 0.0014341388741740957, 0.0014842143767358114, 0.0013207733960977446, 0.0013071257271803916, 0.0013429146686879296, 0.0013320626021595672, 0.0014422366900059085, 0.00142676942050457, 0.0014122590655460954, 0.001418572457623668, 0.0013898479179867234, 0.0014067975231834378, 0.0014194526835732784, 0.001401371545701598, 0.0013169170221469055, 0.0014012267929501832, 0.0013970723957754672, 0.0013888857065467164, 0.001404133960022591, 0.0013900305202696472, 0.001399299770127982, 0.0014911846277148773, 0.0015347193548223004, 0.0016624913308381413, 0.0017315617102819185, 0.001729100937761056, 0.0017206577467732131, 0.001537631459844609, 0.004372391558717936, 0.001642737232032232, 0.0014150152516473706, 0.001407810501405038, 0.0014743750410464902, 0.001434351584369627, 0.06457251078973059, 0.0014587878540623933, 0.0014896868718399976, 0.0015806028289565195, 0.0015910514145313452, 0.0014323279610835016, 0.0014562636060873047, 0.0014786045406556998, 0.0014013160010411714, 0.0014350525840806465, 0.0014539637728982295, 0.001510383376929288, 0.0014072471046044182, 0.0013599230830247204, 0.0014199797733454034, 0.0014197609367935609, 0.0014292221264137577, 0.0014348295856810485, 0.0014404830435523763, 0.0014204091858118773, 0.001420461436888824, 0.0014085522901344423, 0.004088532645255327, 0.0013962055624385055, 0.0013905276039925714, 0.001435430958129776, 0.0012856213967703904, 0.001280550000956282, 0.0015690999183182914, 0.0014300986852807303, 0.0013920850227198873, 0.0041004069983803975, 0.0013095343941434596, 0.0012968084774911404, 0.0013001450182249148, 0.0012944921472808346, 0.0013223861072522898, 0.0013212380387509863, 0.001302104666441058, 0.0012995900615351275, 0.0014231297924804192, 0.0014368290867423639, 0.0013755986631925528, 0.0014085094153415412, 0.001417941913435546, 0.0014143678757439677, 0.0014234379147334646, 0.0014289003108084823, 0.0014297489396994933, 0.001426473837151813, 0.0014407149574253708, 0.0014503386870880302, 0.0014371350795651476, 0.001449301732160772]
[610.1769567355279, 595.7516712843399, 548.6512335734367, 543.5755503318503, 537.882416349734, 413.7091006237127, 508.23012330009516, 618.0897572136224, 556.2162111480527, 612.7269544367783, 554.881336775386, 572.3143748117624, 601.4389007041519, 555.7022551604564, 560.0049165322887, 615.4897565191955, 197.1823113871268, 548.266685407211, 591.6052203526328, 509.30103225460874, 597.6926293166862, 529.8773554619503, 601.6612022274102, 548.2870655403831, 565.5087071605083, 536.9777513947507, 396.0016791630197, 641.0628935204383, 590.7604643463415, 617.0510748869389, 524.1022868677824, 586.7673338434869, 601.43282478315, 601.8641572744461, 597.3858824436684, 601.4072986623103, 602.5015624321006, 552.8494761552726, 517.2487850865207, 577.0601530049266, 560.9715700786065, 606.9211277040614, 546.5700740374544, 539.7101716332446, 571.98743255862, 562.2140829006853, 643.7329831351931, 577.9983951815734, 578.233458848678, 643.4983369054737, 629.0298507849909, 579.955451884141, 642.1817284656287, 556.4596075175667, 633.1381838225742, 590.758572215527, 615.4064342653052, 634.5573972669818, 571.6730411025412, 570.6467027873013, 625.7258819770168, 610.0561838151904, 604.3953600612224, 599.76765214952, 585.0789584571677, 556.2062914449918, 591.2449515540809, 601.7143607657234, 602.2724277723323, 491.0624724562034, 609.8260727735243, 616.3630014354598, 610.9577053306501, 612.5604055478524, 571.12546578302, 561.1547791347778, 613.8915159219785, 607.2979474116689, 608.7150174827535, 606.2808166282066, 563.194193386856, 559.8793986941786, 610.722279274283, 599.5188074061326, 614.6732400855932, 603.1584538255566, 553.1538143792275, 524.7557095996572, 583.862890332605, 578.8032750083051, 575.087530841389, 540.4025807228248, 580.5193344021999, 616.1169147769488, 599.4045910944698, 565.9279955176322, 529.4085021064333, 607.9907528425001, 603.0034787567238, 598.3083056835918, 606.5123058466787, 607.1270624825612, 556.1558325652256, 597.031206222319, 556.341107954741, 612.4946855164138, 603.0747331326345, 628.6211706335273, 606.4791092861054, 613.9804845463927, 595.6413470084944, 606.0162237585065, 592.7145155835424, 568.6321293976487, 283.24258192453163, 577.6861208657909, 609.0565542842967, 559.2017731849178, 573.5243655351851, 618.5232502604332, 559.3444740196327, 601.7295159897437, 218.65366540510541, 612.3833106479218, 602.7476419995029, 596.9673349201279, 595.553757429491, 586.1625026288804, 594.3541159500442, 230.1537482095432, 609.2445995362098, 562.4367589667902, 617.3375189304983, 609.9795345069011, 608.7217625930489, 564.3918976987292, 593.5299017469061, 621.7205894756654, 537.7791743814181, 586.0919696823933, 579.7670556001956, 580.1917872844646, 617.4607249284986, 615.2139857405551, 621.5877906616615, 583.4078646096899, 566.7549750756928, 533.7841580431333, 516.5298137655744, 653.1203555757604, 643.0264333281542, 620.6732261511925, 614.2005918848587, 622.837688298336, 630.5674771303954, 650.123034544061, 587.8140013688458, 539.9180704782354, 591.731219027047, 600.6354380723424, 556.4046659646061, 590.7022599249626, 587.472567899762, 550.9105987760448, 581.1812337734099, 541.0574057884701, 580.6157064731372, 531.1255226299922, 227.8629916132214, 599.5549135260339, 529.931516916225, 527.309095112462, 600.6294812250708, 549.7804181998357, 602.4537454860747, 581.7766647383157, 572.4893162188096, 581.0157313550313, 538.5362887632201, 575.4475159541718, 594.9616164610661, 583.486470771088, 559.9941117190573, 610.0753239507824, 607.6432148958669, 610.914135336165, 542.1469645525641, 522.1937676320209, 559.2892288892021, 611.5780847255761, 585.224794114595, 641.9672687572701, 566.085987404162, 584.3837838403251, 555.8698427145383, 606.4026362693453, 598.2048269662982, 577.5334939224657, 569.5744947852271, 600.0390117540684, 602.7492198375763, 554.9725010330576, 561.4038237222693, 625.0578659369672, 595.9220822638564, 564.6743293422026, 606.8963219400677, 614.8829466830442, 610.1305351945313, 617.4665456598474, 584.140050946682, 620.503424047435, 609.1866195264279, 15.610072703617007, 557.9774421461766, 553.4371582317744, 553.6527293114384, 601.3722304194755, 538.4995420473728, 618.2858966824604, 571.7356075015749, 619.2413015163335, 630.4673095769617, 625.1053743835995, 629.5069310316192, 604.6117444979653, 551.8889796774655, 526.6202673047792, 609.5180992355326, 628.1147193602625, 596.2676550925783, 612.0133489283469, 549.3578595926575, 566.110075716518, 602.2131323117209, 527.5314349177604, 605.8630942803131, 595.588502618003, 611.9671066694864, 617.597270180205, 616.4082981879857, 570.7021732256381, 566.689645533446, 591.3147016848491, 518.809364682028, 483.30564106913783, 583.1533879158864, 540.1311019950783, 580.4864612165948, 644.0224111651909, 618.9500011231798, 581.7429655716139, 623.1871694368574, 564.6352031844472, 572.4311171708739, 604.5767045007312, 546.0143754066746, 586.9253802313565, 572.7997640489558, 562.4898298535959, 560.3225669536185, 634.9722965531445, 630.3829948567778, 644.4486296900741, 585.9830857077101, 215.58745802507224, 607.7863890565445, 653.0020430333904, 650.2823381885584, 590.9424640355752, 606.7983036182798, 618.6587618845236, 649.0159490511862, 595.861860979934, 569.879024730008, 538.3316694919862, 521.07041278809, 487.2838789448938, 473.4384412184452, 462.3845365499035, 523.1329275317482, 521.8125363446668, 513.3730107809154, 533.9809973071984, 512.9204238512925, 506.84618812795514, 552.2518124745711, 543.4483519711246, 533.1710760477991, 514.6971294230711, 492.39767115969187, 513.2689603169601, 456.99845550913255, 209.91061776554287, 510.0848521820988, 501.4719435421047, 518.3293584816587, 504.02127282699024, 511.5005835302295, 525.3932186553637, 211.61475195963962, 448.0656319433749, 458.9614465807302, 507.7272095634701, 506.3922245839656, 486.1023491710375, 478.3057209493017, 201.91431919107498, 495.3910730990538, 534.2119930923101, 554.3521409005369, 541.3187263609002, 512.66504979097, 550.8609764511087, 503.5368990225319, 493.04648522311646, 520.8219123200802, 416.37444673419765, 546.1465224273518, 562.3464168857464, 520.5602391921461, 515.5867504433797, 410.33932473669853, 510.4570984847001, 510.7278652357185, 519.8111104942527, 538.270907944802, 530.6419177019216, 542.4645518322762, 233.85753735271578, 510.834230761581, 455.93830789144045, 501.7850491508463, 457.4566012342278, 518.6442283149828, 476.8325649111978, 210.9694623530601, 554.4233499186442, 431.7455529695051, 533.4495164936759, 493.31794346216634, 496.82835400692176, 510.5673421605755, 289.55372768766915, 537.1731431506842, 535.2377663998699, 524.2666052812572, 548.3958923530704, 535.3519382795299, 547.1640205766022, 511.4570073361217, 480.788774341572, 499.9553812859148, 531.4355585698173, 537.7170715397929, 539.5213657625089, 534.3858664027802, 525.2203797470963, 520.487940636258, 526.2049984417257, 514.0776229485275, 532.1065023211871, 541.354918886947, 532.4018992200802, 533.4822558465727, 526.9834851851358, 526.4807854939492, 535.3498492408445, 525.3619393354628, 529.2480683637542, 526.9633213458395, 466.80749105750914, 495.7962446169282, 196.82274421587414, 484.3832995353409, 536.1310661731752, 452.1387716848361, 555.8533404439858, 527.7074458614438, 482.06842637012136, 465.42356895742273, 478.4922893479946, 536.4597525497046, 523.9333339615287, 489.2462424763244, 449.1054958539184, 512.618077223506, 504.6440942555785, 527.3283908906968, 470.9553658491289, 468.0139188583888, 523.7943304202377, 525.585790005986, 519.7238371249574, 534.8399786117274, 528.9250889747216, 500.6648171985811, 720.877549805549, 711.5125000610878, 758.2909878931022, 662.3300516464915, 640.2453078948876, 657.510657521692, 731.4562713317536, 724.7865463742467, 718.0465992873347, 710.1810897990935, 706.9352605663674, 711.5482853868227, 704.7999681128234, 711.774440911869, 719.869643269297, 691.6678177205899, 701.3524097107444, 712.109267850763, 707.1852489638625, 651.2845811352443, 725.3037801174609, 745.4602889401638, 760.4251797948401, 639.8218480086972, 709.4924598919991, 684.3744079561083, 710.8988090012874, 744.2355016452257, 591.2770156062327, 651.370063088938, 677.9744415106837, 743.8097148811624, 748.9768735171913, 587.9546975060418, 692.4316347378386, 697.2825421637697, 673.7571173507093, 757.132149204794, 765.0373481341429, 744.648951505632, 750.7154681610155, 693.3674666090371, 700.884099160434, 708.0853820636073, 704.9340304232026, 719.5031823687292, 710.8343478861856, 704.4968892394754, 713.5866309454349, 759.3492856290587, 713.6603475120336, 715.7825199494648, 720.0016497299622, 712.1827606704356, 719.4086643550916, 714.6431532026469, 670.6077714417033, 651.5849277966436, 601.506895976325, 577.5133476687864, 578.3352366316221, 581.1731018997367, 650.3508975428082, 228.70778761937277, 608.7400836242683, 706.7061636514469, 710.3228730017069, 678.2534783620688, 697.1791371775023, 15.4864661102668, 685.5006348011651, 671.2820116115024, 632.6700051904745, 628.5152012479475, 698.1641266316815, 686.6888630739076, 676.3133566170042, 713.614915734213, 696.8385765742801, 687.7750454584368, 662.0835579063826, 710.607253500872, 735.3357057340443, 704.2353833280657, 704.3439314920412, 699.6813032199738, 696.9468778589098, 694.2115733163365, 704.0224816825724, 703.9965845114791, 709.9487942364942, 244.58652694395948, 716.2269130725111, 719.1514912244362, 696.6548926204717, 777.833973914949, 780.9144502387455, 637.3080441376649, 699.2524434100145, 718.3469282976533, 243.87822974523888, 763.6301913659015, 771.1238917366129, 769.144969201434, 772.5037205521604, 756.2087914533846, 756.8658868960024, 767.9874174272201, 769.473412884337, 702.6765972322648, 695.9770018765696, 726.9562167784853, 709.9704049599951, 705.2475073376523, 707.0296329192239, 702.5244934460297, 699.8388847953939, 699.4234947362028, 701.029331176983, 694.0998251222779, 689.4941222369143, 695.8288154114107, 689.9874455466871]
Elapsed: 0.09898130069800223~0.18944133967374074
Time per graph: 0.002029737843511203~0.0039057441781282123
Speed: 582.4487465715224~103.87909606229825
Total Time: 0.0712
best val loss: 0.3572121560573578 test_score: 0.9167

Testing...
Test loss: 0.6370 score: 0.9167 time: 0.06s
test Score 0.9167
Epoch Time List: [1.04869039892219, 0.4112613620236516, 0.4221711193677038, 0.3240542740095407, 0.33902854775078595, 0.35985556710511446, 0.4841554563026875, 0.30481399898417294, 0.30977859697304666, 0.4460116771515459, 0.31705206213518977, 0.3189725549891591, 0.30861121299676597, 0.30697344709187746, 0.30806554621085525, 0.2988910642452538, 0.47592021501623094, 0.314807518851012, 0.31602018792182207, 0.3222094960510731, 0.31568995397537947, 0.3179572082590312, 0.33699335996061563, 0.3109961508307606, 0.38816100265830755, 0.31433799979276955, 0.404101180145517, 0.30125720403157175, 0.48263153806328773, 0.3004446930717677, 0.3140702289529145, 0.4606619228143245, 0.30990164703689516, 0.3124753732699901, 0.30621600919403136, 0.3139397331979126, 0.3110068391542882, 0.3188385658431798, 0.3192756848875433, 0.4045541079249233, 0.3037626910954714, 0.30614277301356196, 0.3202781272120774, 0.31957897916436195, 0.3291618369985372, 0.3183686910197139, 0.4277606999967247, 0.3008091184310615, 0.3004341658670455, 0.2925573440734297, 0.3021396119147539, 0.30054259300231934, 0.30382046778686345, 0.3053547809831798, 0.3040248742327094, 0.33928753016516566, 0.3080075499601662, 0.2996388857718557, 0.30009892489761114, 0.3051292500458658, 0.30196176981553435, 0.3161864229477942, 0.31264831428416073, 0.30872260709293187, 0.31519400095567107, 0.3158708040136844, 0.31783738802187145, 0.31403106707148254, 0.31622310634702444, 0.3515003868378699, 0.3101374839898199, 0.3046731890644878, 0.3096503510605544, 0.3096352522261441, 0.31398171302862465, 0.3396198321133852, 0.32327646808698773, 0.3124328644480556, 0.3113052567932755, 0.3087417888455093, 0.31169284391216934, 0.3099166229367256, 0.30713486671447754, 0.3073511519469321, 0.3126804078929126, 0.31085862102918327, 0.31728261918760836, 0.3533439680468291, 0.3125948163215071, 0.3216722388751805, 0.3190305430907756, 0.3232168359681964, 0.31441419781185687, 0.4496344532817602, 0.30654514906927943, 0.31183543917723, 0.3207151792012155, 0.3115375570487231, 0.3097281991504133, 0.31569563201628625, 0.44637257209979, 0.30404098075814545, 0.310074700973928, 0.3062147847376764, 0.3171906592324376, 0.3097621058113873, 0.3521153738256544, 0.5615558652207255, 0.30370283173397183, 0.30624680570326746, 0.3033813638612628, 0.3030554736033082, 0.30792759894393384, 0.31900586606934667, 0.401575047057122, 0.3001402411609888, 0.3107679490931332, 0.33484852011315525, 0.3063456437084824, 0.30801156093366444, 0.31248284969478846, 0.30797904287464917, 0.44901884789578617, 0.3117685930337757, 0.3063949029892683, 0.2928837148938328, 0.30409448593854904, 0.34087924007326365, 0.31592338322661817, 0.4564248670358211, 0.3108602608554065, 0.3117180778644979, 0.30717891082167625, 0.3104665938299149, 0.30697820405475795, 0.3100914820097387, 0.29955253982916474, 0.3006022642366588, 0.4277121899649501, 0.30923199909739196, 0.31422792188823223, 0.3271856892388314, 0.30121732922270894, 0.3176588991191238, 0.3134043049067259, 0.31896991841495037, 0.4550726201850921, 0.3143497759010643, 0.33021935704164207, 0.2981805391609669, 0.2888390081934631, 0.3110637969803065, 0.306721534114331, 0.4337967161554843, 0.30063633574172854, 0.2936395010910928, 0.2977746119722724, 0.31950694182887673, 0.31675960775464773, 0.31071410002186894, 0.3137172288261354, 0.45198657270520926, 0.31058384384959936, 0.3151454741600901, 0.31828061025589705, 0.32270830124616623, 0.31873652269132435, 0.3222151619847864, 0.4497459710109979, 0.309152748901397, 0.3301563661079854, 0.3173754771705717, 0.3164607468061149, 0.3135131699964404, 0.3114147160667926, 0.31441204994916916, 0.4577474088873714, 0.32146723312325776, 0.31701406789943576, 0.32164829201065004, 0.31953907571733, 0.32091404194943607, 0.3140860132407397, 0.4403564832173288, 0.3128592041321099, 0.3033284561242908, 0.31224119081161916, 0.33661800785921514, 0.31722093489952385, 0.3105383387301117, 0.31242826161906123, 0.372133731842041, 0.30671786377206445, 0.31966201215982437, 0.3123604911379516, 0.3238706060219556, 0.31357460701838136, 0.31316943001002073, 0.31665252335369587, 0.369372014189139, 0.29800919094122946, 0.30900686979293823, 0.31932580401189625, 0.30763329192996025, 0.30642817215994, 0.31071882508695126, 0.30837933090515435, 0.43983968324027956, 0.303556009195745, 0.3070426101330668, 0.3087505269795656, 0.30382634815759957, 0.3081385991536081, 6.633971838047728, 0.7178552888799459, 0.31554126809351146, 0.31551897595636547, 0.3139108100440353, 0.3240873138420284, 0.31512109795585275, 0.3070659111253917, 0.30526094301603734, 0.30539823113940656, 0.2991426058579236, 0.29925431008450687, 0.30408612359315157, 0.3211152988951653, 0.32525043725036085, 0.30432423390448093, 0.3060405838768929, 0.3078686259686947, 0.3123359817545861, 0.31176892993971705, 0.3087583847809583, 0.3136061728000641, 0.3298565912991762, 0.31491189170628786, 0.3109215300064534, 0.30966871697455645, 0.30831088521517813, 0.30480991094373167, 0.305850462988019, 0.3063270940911025, 0.3220037631690502, 0.3144272689241916, 0.3436102659907192, 0.3220976460725069, 0.3242546839173883, 0.30448475712910295, 0.30514204199425876, 0.3060695657040924, 0.40866007772274315, 0.3061901112087071, 0.3074364000931382, 0.30627069203183055, 0.3112816601060331, 0.31638680514879525, 0.3383650602772832, 0.3858680890407413, 0.3114037709310651, 0.3166159139946103, 0.3002663420047611, 0.3036463870666921, 0.2968277789186686, 0.3125814569648355, 0.47100821509957314, 0.3089855208527297, 0.29618678987026215, 0.29793122387491167, 0.3014040240086615, 0.2962248679250479, 0.3062764508649707, 0.30236075399443507, 0.3987325159832835, 0.3286707659717649, 0.3219403540715575, 0.31871160166338086, 0.35476096509955823, 0.3577107300516218, 0.48999795294366777, 0.31864441302604973, 0.32647833600640297, 0.32508911984041333, 0.3265323129016906, 0.3204555017873645, 0.33007494709454477, 0.40794520801864564, 0.3105448361020535, 0.3214231519959867, 0.3196492169518024, 0.348253853386268, 0.367058445001021, 0.35668080602772534, 0.4667050214484334, 0.3309942309278995, 0.3411235325038433, 0.3313591161277145, 0.3373545380309224, 0.339724580058828, 0.3268658621236682, 0.4597469016443938, 0.34944357769563794, 0.37498477124609053, 0.34304641908966005, 0.3470671863760799, 0.348527000984177, 0.33278887369669974, 0.49199270992539823, 0.3556652551051229, 0.3145419079810381, 0.30903914594091475, 0.31394653487950563, 0.3234298708848655, 0.32137617585249245, 0.32045915978960693, 0.3947914151940495, 0.3356994071509689, 0.3324971250258386, 0.31096545397304, 0.31097321514971554, 0.3189413398504257, 0.3372786270920187, 0.3650226809550077, 0.45671153091825545, 0.32891709147952497, 0.3199755637906492, 0.31853028503246605, 0.32373258122242987, 0.3200019171927124, 0.442483072867617, 0.34781900327652693, 0.34493373404257, 0.3390730631072074, 0.3520577328745276, 0.3364318977110088, 0.34706076607108116, 0.47130183107219636, 0.31866605998948216, 0.3377419072203338, 0.32652045832946897, 0.3468556522857398, 0.34674400300718844, 0.34427515766583383, 0.4053009250201285, 0.32878193398937583, 0.32376303523778915, 0.3227774710394442, 0.31874082097783685, 0.3199060352053493, 0.3181712676305324, 0.32190519478172064, 0.395381695125252, 0.34540596627630293, 0.33320676628500223, 0.3224706999026239, 0.3192082019522786, 0.32162514491938055, 0.3286327759269625, 0.3906454211100936, 0.32377808005549014, 0.32765436195768416, 0.33324168296530843, 0.3171807487960905, 0.32656737696379423, 0.3343269571196288, 0.42546746297739446, 0.3265857552178204, 0.32968007517047226, 0.3458867622539401, 0.3249417180195451, 0.33000473701395094, 0.3381173610687256, 0.3569695430342108, 0.49077229807153344, 0.35449715284630656, 0.32144253398291767, 0.3484439719468355, 0.3091442610602826, 0.3413232248276472, 0.329279859084636, 0.3516500350087881, 0.3392512770369649, 0.31958791497163475, 0.31831301492638886, 0.32133892783895135, 0.3450946102384478, 0.3346221251413226, 0.33965045493096113, 0.3265741034410894, 0.37374925683252513, 0.3527995739132166, 0.3269304879941046, 0.3268636669963598, 0.332698579877615, 0.33108457806520164, 0.32970664207823575, 0.3530570510774851, 0.27377788978628814, 0.2478383861016482, 0.24106633476912975, 0.25406009890139103, 0.26527383807115257, 0.2714990379754454, 0.26426445529796183, 0.25579767394810915, 0.25490145408548415, 0.3668347739148885, 0.2594434800557792, 0.2567501151934266, 0.2522620973177254, 0.2535576189402491, 0.2605242298450321, 0.2668000399135053, 0.26440150872804224, 0.25492291897535324, 0.264419685350731, 0.3714785447809845, 0.24754195078276098, 0.2507779800798744, 0.25192914600484073, 0.2513986087869853, 0.27048551803454757, 0.2759137221146375, 0.2932006719056517, 0.26962627680040896, 0.36932621826417744, 0.29206548002548516, 0.272765648085624, 0.24917320790700614, 0.24895302392542362, 0.2636539828963578, 0.2577191651798785, 0.26326618483290076, 0.2549887129571289, 0.3584179838653654, 0.25291905901394784, 0.24747138191014528, 0.26449960097670555, 0.26132482127286494, 0.2616139810997993, 0.2528398821596056, 0.254316687816754, 0.26145721902139485, 0.38584834104403853, 0.25811161496676505, 0.25674341805279255, 0.24881340726278722, 0.23776221903972328, 0.2613314650952816, 0.25406677508726716, 0.2534557089675218, 0.26692935824394226, 0.2645885602105409, 0.35605374118313193, 0.2754712379537523, 0.31145396805368364, 0.3532003699801862, 0.30849943798966706, 0.30499967001378536, 0.2860046981368214, 0.4101901729591191, 0.29495559516362846, 0.2719909381121397, 0.25181442382745445, 0.26090163085609674, 0.2556265431921929, 6.457569219172001, 0.2894663470797241, 0.27697742404416203, 0.26034443406388164, 0.28531726519577205, 0.2805202123709023, 0.27622879296541214, 0.25649167597293854, 0.25537011004053056, 0.4188714169431478, 0.25770431384444237, 0.26301595591939986, 0.25880300416611135, 0.388215275015682, 0.2577099271584302, 0.2574439898598939, 0.25029822695069015, 0.2605077938642353, 0.2729399208910763, 0.25815972383134067, 0.25491203484125435, 0.25059685995802283, 0.382904316065833, 0.24992410722188652, 0.2513460027985275, 0.25501014897599816, 0.2393523019272834, 0.24598583718761802, 0.2637241589836776, 0.25282690301537514, 0.2528599980287254, 0.38396011013537645, 0.43465558090247214, 0.2442799061536789, 0.24250771524384618, 0.2425172571092844, 0.24389662104658782, 0.24750625109300017, 0.24319138890132308, 0.24077360099181533, 0.36406559916213155, 0.26416907692328095, 0.2566907631698996, 0.25707028177566826, 0.35797613533213735, 0.2535855520982295, 0.26074034394696355, 0.2597387006971985, 0.28230275004170835, 0.33446665620431304, 0.39960565720684826, 0.2644167949911207, 0.26280076894909143, 0.2618042309768498]
Total Epoch List: [277, 120, 123]
Total Time List: [0.09498926601372659, 0.0991767339874059, 0.07122320216149092]
========================training times:1========================
========================k_idx:0========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: SGFormer
SGFormer(
  (trans_conv): SGModule(
    (attns): ModuleList(
      (0-1): 2 x SGFormerAttention(heads=1, head_channels=64)
    )
    (fcs): ModuleList(
      (0): Linear(in_features=14887, out_features=64, bias=True)
    )
    (bns): ModuleList(
      (0-2): 3 x LayerNorm((64,), eps=1e-05, elementwise_affine=True)
    )
  )
  (graph_conv): GraphModule(
    (convs): ModuleList(
      (0-2): 3 x GCNConv(64, 64)
    )
    (fcs): ModuleList(
      (0): Linear(in_features=14887, out_features=64, bias=True)
    )
    (bns): ModuleList(
      (0-3): 4 x BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (classifier): Linear(in_features=64, out_features=2, bias=True)
)
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7f342d318310>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.7945;  Loss pred: 0.7945; Loss self: 0.0000; time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7122 score: 0.5102 time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7203 score: 0.4898 time: 0.09s
Epoch 2/1000, LR 0.000000
Train loss: 0.7733;  Loss pred: 0.7733; Loss self: 0.0000; time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7110 score: 0.5102 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7188 score: 0.4898 time: 0.09s
Epoch 3/1000, LR 0.000030
Train loss: 0.7693;  Loss pred: 0.7693; Loss self: 0.0000; time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7081 score: 0.5102 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7152 score: 0.4898 time: 0.08s
Epoch 4/1000, LR 0.000060
Train loss: 0.8062;  Loss pred: 0.8062; Loss self: 0.0000; time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7035 score: 0.5102 time: 0.09s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7094 score: 0.4898 time: 0.09s
Epoch 5/1000, LR 0.000090
Train loss: 0.7746;  Loss pred: 0.7746; Loss self: 0.0000; time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6974 score: 0.5102 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7017 score: 0.4898 time: 0.08s
Epoch 6/1000, LR 0.000120
Train loss: 0.7572;  Loss pred: 0.7572; Loss self: 0.0000; time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6906 score: 0.5102 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6929 score: 0.4898 time: 0.08s
Epoch 7/1000, LR 0.000150
Train loss: 0.7347;  Loss pred: 0.7347; Loss self: 0.0000; time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6844 score: 0.5102 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6846 score: 0.4898 time: 0.08s
Epoch 8/1000, LR 0.000180
Train loss: 0.6942;  Loss pred: 0.6942; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6792 score: 0.5102 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6768 score: 0.4898 time: 0.20s
Epoch 9/1000, LR 0.000210
Train loss: 0.6795;  Loss pred: 0.6795; Loss self: 0.0000; time: 0.15s
Val loss: 0.6750 score: 0.6735 time: 0.08s
Test loss: 0.6700 score: 0.8163 time: 0.08s
Epoch 10/1000, LR 0.000240
Train loss: 0.6510;  Loss pred: 0.6510; Loss self: 0.0000; time: 0.15s
Val loss: 0.6728 score: 0.7755 time: 0.08s
Test loss: 0.6650 score: 0.8980 time: 0.08s
Epoch 11/1000, LR 0.000270
Train loss: 0.6221;  Loss pred: 0.6221; Loss self: 0.0000; time: 0.15s
Val loss: 0.6717 score: 0.6735 time: 0.08s
Test loss: 0.6616 score: 0.7143 time: 0.08s
Epoch 12/1000, LR 0.000270
Train loss: 0.5937;  Loss pred: 0.5937; Loss self: 0.0000; time: 0.15s
Val loss: 0.6710 score: 0.5714 time: 0.08s
Test loss: 0.6589 score: 0.6122 time: 0.08s
Epoch 13/1000, LR 0.000270
Train loss: 0.6092;  Loss pred: 0.6092; Loss self: 0.0000; time: 0.15s
Val loss: 0.6707 score: 0.5510 time: 0.08s
Test loss: 0.6567 score: 0.6122 time: 0.08s
Epoch 14/1000, LR 0.000270
Train loss: 0.5657;  Loss pred: 0.5657; Loss self: 0.0000; time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6707 score: 0.4898 time: 0.08s
Test loss: 0.6548 score: 0.5918 time: 0.08s
     INFO: Early stopping counter 1 of 20
Epoch 15/1000, LR 0.000270
Train loss: 0.5315;  Loss pred: 0.5315; Loss self: 0.0000; time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6710 score: 0.4898 time: 0.09s
Test loss: 0.6535 score: 0.5918 time: 0.08s
     INFO: Early stopping counter 2 of 20
Epoch 16/1000, LR 0.000270
Train loss: 0.5170;  Loss pred: 0.5170; Loss self: 0.0000; time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6715 score: 0.4898 time: 0.08s
Test loss: 0.6523 score: 0.5918 time: 0.08s
     INFO: Early stopping counter 3 of 20
Epoch 17/1000, LR 0.000270
Train loss: 0.5030;  Loss pred: 0.5030; Loss self: 0.0000; time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6718 score: 0.4898 time: 0.08s
Test loss: 0.6511 score: 0.5510 time: 0.08s
     INFO: Early stopping counter 4 of 20
Epoch 18/1000, LR 0.000270
Train loss: 0.4690;  Loss pred: 0.4690; Loss self: 0.0000; time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6723 score: 0.4898 time: 0.10s
Test loss: 0.6501 score: 0.5510 time: 0.09s
     INFO: Early stopping counter 5 of 20
Epoch 19/1000, LR 0.000270
Train loss: 0.4871;  Loss pred: 0.4871; Loss self: 0.0000; time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6724 score: 0.4898 time: 0.09s
Test loss: 0.6487 score: 0.5510 time: 0.08s
     INFO: Early stopping counter 6 of 20
Epoch 20/1000, LR 0.000270
Train loss: 0.4496;  Loss pred: 0.4496; Loss self: 0.0000; time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6728 score: 0.4898 time: 0.09s
Test loss: 0.6476 score: 0.5306 time: 0.08s
     INFO: Early stopping counter 7 of 20
Epoch 21/1000, LR 0.000270
Train loss: 0.4423;  Loss pred: 0.4423; Loss self: 0.0000; time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6727 score: 0.4898 time: 0.09s
Test loss: 0.6460 score: 0.5306 time: 0.08s
     INFO: Early stopping counter 8 of 20
Epoch 22/1000, LR 0.000270
Train loss: 0.4333;  Loss pred: 0.4333; Loss self: 0.0000; time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6723 score: 0.4898 time: 0.08s
Test loss: 0.6440 score: 0.5306 time: 0.09s
     INFO: Early stopping counter 9 of 20
Epoch 23/1000, LR 0.000270
Train loss: 0.4100;  Loss pred: 0.4100; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6717 score: 0.4898 time: 0.08s
Test loss: 0.6419 score: 0.5306 time: 0.08s
     INFO: Early stopping counter 10 of 20
Epoch 24/1000, LR 0.000270
Train loss: 0.4105;  Loss pred: 0.4105; Loss self: 0.0000; time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6705 score: 0.4898 time: 0.09s
Test loss: 0.6392 score: 0.5306 time: 0.08s
Epoch 25/1000, LR 0.000270
Train loss: 0.3933;  Loss pred: 0.3933; Loss self: 0.0000; time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6685 score: 0.4898 time: 0.08s
Test loss: 0.6359 score: 0.5510 time: 0.08s
Epoch 26/1000, LR 0.000270
Train loss: 0.3765;  Loss pred: 0.3765; Loss self: 0.0000; time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6666 score: 0.4898 time: 0.09s
Test loss: 0.6326 score: 0.5714 time: 0.08s
Epoch 27/1000, LR 0.000270
Train loss: 0.3717;  Loss pred: 0.3717; Loss self: 0.0000; time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6638 score: 0.4898 time: 0.09s
Test loss: 0.6285 score: 0.5714 time: 0.09s
Epoch 28/1000, LR 0.000270
Train loss: 0.3637;  Loss pred: 0.3637; Loss self: 0.0000; time: 0.14s
Val loss: 0.6603 score: 0.5102 time: 0.08s
Test loss: 0.6238 score: 0.5714 time: 0.09s
Epoch 29/1000, LR 0.000270
Train loss: 0.3533;  Loss pred: 0.3533; Loss self: 0.0000; time: 0.14s
Val loss: 0.6568 score: 0.5102 time: 0.09s
Test loss: 0.6189 score: 0.5714 time: 0.10s
Epoch 30/1000, LR 0.000270
Train loss: 0.3434;  Loss pred: 0.3434; Loss self: 0.0000; time: 0.15s
Val loss: 0.6530 score: 0.5102 time: 0.10s
Test loss: 0.6139 score: 0.5918 time: 0.09s
Epoch 31/1000, LR 0.000270
Train loss: 0.3344;  Loss pred: 0.3344; Loss self: 0.0000; time: 0.15s
Val loss: 0.6490 score: 0.5102 time: 0.08s
Test loss: 0.6085 score: 0.5918 time: 0.08s
Epoch 32/1000, LR 0.000270
Train loss: 0.3279;  Loss pred: 0.3279; Loss self: 0.0000; time: 0.15s
Val loss: 0.6453 score: 0.5306 time: 0.08s
Test loss: 0.6031 score: 0.6122 time: 0.08s
Epoch 33/1000, LR 0.000270
Train loss: 0.3149;  Loss pred: 0.3149; Loss self: 0.0000; time: 0.14s
Val loss: 0.6416 score: 0.5306 time: 0.08s
Test loss: 0.5977 score: 0.6122 time: 0.08s
Epoch 34/1000, LR 0.000270
Train loss: 0.3039;  Loss pred: 0.3039; Loss self: 0.0000; time: 0.14s
Val loss: 0.6384 score: 0.5306 time: 0.08s
Test loss: 0.5927 score: 0.6122 time: 0.08s
Epoch 35/1000, LR 0.000270
Train loss: 0.2956;  Loss pred: 0.2956; Loss self: 0.0000; time: 0.15s
Val loss: 0.6353 score: 0.5306 time: 0.08s
Test loss: 0.5878 score: 0.6122 time: 0.08s
Epoch 36/1000, LR 0.000270
Train loss: 0.2933;  Loss pred: 0.2933; Loss self: 0.0000; time: 0.14s
Val loss: 0.6329 score: 0.5306 time: 0.08s
Test loss: 0.5835 score: 0.6327 time: 0.08s
Epoch 37/1000, LR 0.000270
Train loss: 0.2861;  Loss pred: 0.2861; Loss self: 0.0000; time: 0.14s
Val loss: 0.6306 score: 0.5306 time: 0.08s
Test loss: 0.5791 score: 0.6327 time: 0.08s
Epoch 38/1000, LR 0.000270
Train loss: 0.2761;  Loss pred: 0.2761; Loss self: 0.0000; time: 0.14s
Val loss: 0.6280 score: 0.5510 time: 0.08s
Test loss: 0.5744 score: 0.6327 time: 0.08s
Epoch 39/1000, LR 0.000269
Train loss: 0.2656;  Loss pred: 0.2656; Loss self: 0.0000; time: 0.14s
Val loss: 0.6258 score: 0.5714 time: 0.08s
Test loss: 0.5701 score: 0.6327 time: 0.08s
Epoch 40/1000, LR 0.000269
Train loss: 0.2587;  Loss pred: 0.2587; Loss self: 0.0000; time: 0.14s
Val loss: 0.6250 score: 0.5714 time: 0.08s
Test loss: 0.5670 score: 0.6327 time: 0.08s
Epoch 41/1000, LR 0.000269
Train loss: 0.2626;  Loss pred: 0.2626; Loss self: 0.0000; time: 0.15s
Val loss: 0.6236 score: 0.5918 time: 0.08s
Test loss: 0.5635 score: 0.6327 time: 0.08s
Epoch 42/1000, LR 0.000269
Train loss: 0.2432;  Loss pred: 0.2432; Loss self: 0.0000; time: 0.15s
Val loss: 0.6222 score: 0.5918 time: 0.08s
Test loss: 0.5602 score: 0.6327 time: 0.09s
Epoch 43/1000, LR 0.000269
Train loss: 0.2406;  Loss pred: 0.2406; Loss self: 0.0000; time: 0.15s
Val loss: 0.6209 score: 0.5918 time: 0.08s
Test loss: 0.5568 score: 0.6327 time: 0.08s
Epoch 44/1000, LR 0.000269
Train loss: 0.2431;  Loss pred: 0.2431; Loss self: 0.0000; time: 0.14s
Val loss: 0.6188 score: 0.5918 time: 0.08s
Test loss: 0.5529 score: 0.6327 time: 0.08s
Epoch 45/1000, LR 0.000269
Train loss: 0.2295;  Loss pred: 0.2295; Loss self: 0.0000; time: 0.14s
Val loss: 0.6158 score: 0.5918 time: 0.08s
Test loss: 0.5482 score: 0.6327 time: 0.09s
Epoch 46/1000, LR 0.000269
Train loss: 0.2273;  Loss pred: 0.2273; Loss self: 0.0000; time: 0.15s
Val loss: 0.6122 score: 0.5918 time: 0.08s
Test loss: 0.5430 score: 0.6327 time: 0.08s
Epoch 47/1000, LR 0.000269
Train loss: 0.2231;  Loss pred: 0.2231; Loss self: 0.0000; time: 0.15s
Val loss: 0.6086 score: 0.5918 time: 0.08s
Test loss: 0.5378 score: 0.6327 time: 0.09s
Epoch 48/1000, LR 0.000269
Train loss: 0.2144;  Loss pred: 0.2144; Loss self: 0.0000; time: 0.15s
Val loss: 0.6040 score: 0.5918 time: 0.08s
Test loss: 0.5317 score: 0.6531 time: 0.08s
Epoch 49/1000, LR 0.000269
Train loss: 0.1971;  Loss pred: 0.1971; Loss self: 0.0000; time: 0.15s
Val loss: 0.6011 score: 0.5918 time: 0.10s
Test loss: 0.5269 score: 0.6531 time: 0.08s
Epoch 50/1000, LR 0.000269
Train loss: 0.2027;  Loss pred: 0.2027; Loss self: 0.0000; time: 0.16s
Val loss: 0.5976 score: 0.5918 time: 0.08s
Test loss: 0.5215 score: 0.6531 time: 0.08s
Epoch 51/1000, LR 0.000269
Train loss: 0.2038;  Loss pred: 0.2038; Loss self: 0.0000; time: 0.16s
Val loss: 0.5933 score: 0.5918 time: 0.09s
Test loss: 0.5156 score: 0.6735 time: 0.09s
Epoch 52/1000, LR 0.000269
Train loss: 0.1864;  Loss pred: 0.1864; Loss self: 0.0000; time: 0.16s
Val loss: 0.5885 score: 0.5918 time: 0.08s
Test loss: 0.5092 score: 0.6735 time: 0.09s
Epoch 53/1000, LR 0.000269
Train loss: 0.1967;  Loss pred: 0.1967; Loss self: 0.0000; time: 0.16s
Val loss: 0.5840 score: 0.5918 time: 0.08s
Test loss: 0.5030 score: 0.6939 time: 0.08s
Epoch 54/1000, LR 0.000269
Train loss: 0.1848;  Loss pred: 0.1848; Loss self: 0.0000; time: 0.15s
Val loss: 0.5797 score: 0.5918 time: 0.08s
Test loss: 0.4972 score: 0.6939 time: 0.08s
Epoch 55/1000, LR 0.000269
Train loss: 0.1837;  Loss pred: 0.1837; Loss self: 0.0000; time: 0.16s
Val loss: 0.5751 score: 0.5918 time: 0.16s
Test loss: 0.4912 score: 0.6939 time: 0.09s
Epoch 56/1000, LR 0.000269
Train loss: 0.1810;  Loss pred: 0.1810; Loss self: 0.0000; time: 0.15s
Val loss: 0.5708 score: 0.5918 time: 0.08s
Test loss: 0.4854 score: 0.7143 time: 0.08s
Epoch 57/1000, LR 0.000269
Train loss: 0.1744;  Loss pred: 0.1744; Loss self: 0.0000; time: 0.15s
Val loss: 0.5659 score: 0.6327 time: 0.08s
Test loss: 0.4792 score: 0.7143 time: 0.08s
Epoch 58/1000, LR 0.000269
Train loss: 0.1728;  Loss pred: 0.1728; Loss self: 0.0000; time: 0.15s
Val loss: 0.5611 score: 0.6735 time: 0.08s
Test loss: 0.4731 score: 0.7143 time: 0.08s
Epoch 59/1000, LR 0.000268
Train loss: 0.1710;  Loss pred: 0.1710; Loss self: 0.0000; time: 0.15s
Val loss: 0.5566 score: 0.6735 time: 0.08s
Test loss: 0.4672 score: 0.7143 time: 0.08s
Epoch 60/1000, LR 0.000268
Train loss: 0.1754;  Loss pred: 0.1754; Loss self: 0.0000; time: 0.15s
Val loss: 0.5524 score: 0.6939 time: 0.08s
Test loss: 0.4618 score: 0.7143 time: 0.08s
Epoch 61/1000, LR 0.000268
Train loss: 0.1665;  Loss pred: 0.1665; Loss self: 0.0000; time: 0.15s
Val loss: 0.5478 score: 0.6939 time: 0.08s
Test loss: 0.4560 score: 0.7143 time: 0.08s
Epoch 62/1000, LR 0.000268
Train loss: 0.1572;  Loss pred: 0.1572; Loss self: 0.0000; time: 0.15s
Val loss: 0.5428 score: 0.6939 time: 0.08s
Test loss: 0.4499 score: 0.6939 time: 0.08s
Epoch 63/1000, LR 0.000268
Train loss: 0.1521;  Loss pred: 0.1521; Loss self: 0.0000; time: 0.16s
Val loss: 0.5381 score: 0.6939 time: 0.18s
Test loss: 0.4441 score: 0.6939 time: 0.07s
Epoch 64/1000, LR 0.000268
Train loss: 0.1594;  Loss pred: 0.1594; Loss self: 0.0000; time: 0.14s
Val loss: 0.5341 score: 0.6939 time: 0.08s
Test loss: 0.4387 score: 0.7143 time: 0.09s
Epoch 65/1000, LR 0.000268
Train loss: 0.1515;  Loss pred: 0.1515; Loss self: 0.0000; time: 0.14s
Val loss: 0.5286 score: 0.7347 time: 0.08s
Test loss: 0.4321 score: 0.7551 time: 0.08s
Epoch 66/1000, LR 0.000268
Train loss: 0.1457;  Loss pred: 0.1457; Loss self: 0.0000; time: 0.14s
Val loss: 0.5230 score: 0.7347 time: 0.08s
Test loss: 0.4256 score: 0.7551 time: 0.08s
Epoch 67/1000, LR 0.000268
Train loss: 0.1446;  Loss pred: 0.1446; Loss self: 0.0000; time: 0.15s
Val loss: 0.5178 score: 0.7347 time: 0.08s
Test loss: 0.4192 score: 0.7551 time: 0.09s
Epoch 68/1000, LR 0.000268
Train loss: 0.1399;  Loss pred: 0.1399; Loss self: 0.0000; time: 0.14s
Val loss: 0.5119 score: 0.7347 time: 0.08s
Test loss: 0.4123 score: 0.7551 time: 0.08s
Epoch 69/1000, LR 0.000268
Train loss: 0.1390;  Loss pred: 0.1390; Loss self: 0.0000; time: 0.14s
Val loss: 0.5056 score: 0.7551 time: 0.08s
Test loss: 0.4052 score: 0.7755 time: 0.08s
Epoch 70/1000, LR 0.000268
Train loss: 0.1309;  Loss pred: 0.1309; Loss self: 0.0000; time: 0.14s
Val loss: 0.5009 score: 0.7551 time: 0.08s
Test loss: 0.3995 score: 0.7755 time: 0.08s
Epoch 71/1000, LR 0.000268
Train loss: 0.1326;  Loss pred: 0.1326; Loss self: 0.0000; time: 0.15s
Val loss: 0.4958 score: 0.7551 time: 0.19s
Test loss: 0.3934 score: 0.7755 time: 0.08s
Epoch 72/1000, LR 0.000267
Train loss: 0.1225;  Loss pred: 0.1225; Loss self: 0.0000; time: 0.14s
Val loss: 0.4899 score: 0.7551 time: 0.08s
Test loss: 0.3869 score: 0.7755 time: 0.08s
Epoch 73/1000, LR 0.000267
Train loss: 0.1295;  Loss pred: 0.1295; Loss self: 0.0000; time: 0.14s
Val loss: 0.4851 score: 0.7551 time: 0.08s
Test loss: 0.3814 score: 0.7755 time: 0.08s
Epoch 74/1000, LR 0.000267
Train loss: 0.1281;  Loss pred: 0.1281; Loss self: 0.0000; time: 0.15s
Val loss: 0.4789 score: 0.7551 time: 0.08s
Test loss: 0.3747 score: 0.8163 time: 0.08s
Epoch 75/1000, LR 0.000267
Train loss: 0.1203;  Loss pred: 0.1203; Loss self: 0.0000; time: 0.15s
Val loss: 0.4728 score: 0.7551 time: 0.09s
Test loss: 0.3682 score: 0.8163 time: 0.09s
Epoch 76/1000, LR 0.000267
Train loss: 0.1193;  Loss pred: 0.1193; Loss self: 0.0000; time: 0.15s
Val loss: 0.4658 score: 0.7551 time: 0.08s
Test loss: 0.3609 score: 0.8163 time: 0.08s
Epoch 77/1000, LR 0.000267
Train loss: 0.1227;  Loss pred: 0.1227; Loss self: 0.0000; time: 0.14s
Val loss: 0.4595 score: 0.7347 time: 0.08s
Test loss: 0.3543 score: 0.8367 time: 0.08s
Epoch 78/1000, LR 0.000267
Train loss: 0.1196;  Loss pred: 0.1196; Loss self: 0.0000; time: 0.14s
Val loss: 0.4523 score: 0.7347 time: 0.08s
Test loss: 0.3471 score: 0.8367 time: 0.08s
Epoch 79/1000, LR 0.000267
Train loss: 0.1124;  Loss pred: 0.1124; Loss self: 0.0000; time: 0.16s
Val loss: 0.4448 score: 0.7551 time: 0.17s
Test loss: 0.3397 score: 0.8571 time: 0.08s
Epoch 80/1000, LR 0.000267
Train loss: 0.1107;  Loss pred: 0.1107; Loss self: 0.0000; time: 0.14s
Val loss: 0.4367 score: 0.7347 time: 0.08s
Test loss: 0.3318 score: 0.8571 time: 0.08s
Epoch 81/1000, LR 0.000267
Train loss: 0.1101;  Loss pred: 0.1101; Loss self: 0.0000; time: 0.16s
Val loss: 0.4288 score: 0.7551 time: 0.08s
Test loss: 0.3242 score: 0.8571 time: 0.09s
Epoch 82/1000, LR 0.000267
Train loss: 0.1131;  Loss pred: 0.1131; Loss self: 0.0000; time: 0.16s
Val loss: 0.4203 score: 0.7755 time: 0.08s
Test loss: 0.3163 score: 0.8571 time: 0.08s
Epoch 83/1000, LR 0.000266
Train loss: 0.1156;  Loss pred: 0.1156; Loss self: 0.0000; time: 0.15s
Val loss: 0.4125 score: 0.7755 time: 0.09s
Test loss: 0.3088 score: 0.8571 time: 0.09s
Epoch 84/1000, LR 0.000266
Train loss: 0.1072;  Loss pred: 0.1072; Loss self: 0.0000; time: 0.15s
Val loss: 0.4041 score: 0.7755 time: 0.09s
Test loss: 0.3011 score: 0.8776 time: 0.09s
Epoch 85/1000, LR 0.000266
Train loss: 0.1010;  Loss pred: 0.1010; Loss self: 0.0000; time: 0.15s
Val loss: 0.3954 score: 0.7755 time: 0.08s
Test loss: 0.2932 score: 0.8776 time: 0.08s
Epoch 86/1000, LR 0.000266
Train loss: 0.1058;  Loss pred: 0.1058; Loss self: 0.0000; time: 0.15s
Val loss: 0.3866 score: 0.7755 time: 0.08s
Test loss: 0.2852 score: 0.8776 time: 0.09s
Epoch 87/1000, LR 0.000266
Train loss: 0.1039;  Loss pred: 0.1039; Loss self: 0.0000; time: 0.16s
Val loss: 0.3785 score: 0.7755 time: 0.09s
Test loss: 0.2779 score: 0.8776 time: 0.08s
Epoch 88/1000, LR 0.000266
Train loss: 0.1000;  Loss pred: 0.1000; Loss self: 0.0000; time: 0.15s
Val loss: 0.3697 score: 0.7755 time: 0.08s
Test loss: 0.2703 score: 0.9184 time: 0.08s
Epoch 89/1000, LR 0.000266
Train loss: 0.0976;  Loss pred: 0.0976; Loss self: 0.0000; time: 0.16s
Val loss: 0.3616 score: 0.7755 time: 0.09s
Test loss: 0.2635 score: 0.9184 time: 0.09s
Epoch 90/1000, LR 0.000266
Train loss: 0.0969;  Loss pred: 0.0969; Loss self: 0.0000; time: 0.15s
Val loss: 0.3541 score: 0.7959 time: 0.09s
Test loss: 0.2573 score: 0.9388 time: 0.09s
Epoch 91/1000, LR 0.000266
Train loss: 0.0960;  Loss pred: 0.0960; Loss self: 0.0000; time: 0.16s
Val loss: 0.3465 score: 0.7959 time: 0.08s
Test loss: 0.2509 score: 0.9388 time: 0.08s
Epoch 92/1000, LR 0.000266
Train loss: 0.0900;  Loss pred: 0.0900; Loss self: 0.0000; time: 0.14s
Val loss: 0.3388 score: 0.7959 time: 0.08s
Test loss: 0.2446 score: 0.9388 time: 0.09s
Epoch 93/1000, LR 0.000265
Train loss: 0.0906;  Loss pred: 0.0906; Loss self: 0.0000; time: 0.15s
Val loss: 0.3325 score: 0.7959 time: 0.08s
Test loss: 0.2394 score: 0.9388 time: 0.08s
Epoch 94/1000, LR 0.000265
Train loss: 0.0946;  Loss pred: 0.0946; Loss self: 0.0000; time: 0.15s
Val loss: 0.3265 score: 0.8163 time: 0.08s
Test loss: 0.2345 score: 0.9388 time: 0.08s
Epoch 95/1000, LR 0.000265
Train loss: 0.0887;  Loss pred: 0.0887; Loss self: 0.0000; time: 0.27s
Val loss: 0.3207 score: 0.8367 time: 0.08s
Test loss: 0.2300 score: 0.9388 time: 0.08s
Epoch 96/1000, LR 0.000265
Train loss: 0.0928;  Loss pred: 0.0928; Loss self: 0.0000; time: 0.14s
Val loss: 0.3151 score: 0.8367 time: 0.09s
Test loss: 0.2258 score: 0.9388 time: 0.08s
Epoch 97/1000, LR 0.000265
Train loss: 0.0891;  Loss pred: 0.0891; Loss self: 0.0000; time: 0.14s
Val loss: 0.3102 score: 0.8367 time: 0.09s
Test loss: 0.2222 score: 0.9388 time: 0.08s
Epoch 98/1000, LR 0.000265
Train loss: 0.0827;  Loss pred: 0.0827; Loss self: 0.0000; time: 0.14s
Val loss: 0.3052 score: 0.8571 time: 0.08s
Test loss: 0.2186 score: 0.9388 time: 0.08s
Epoch 99/1000, LR 0.000265
Train loss: 0.0838;  Loss pred: 0.0838; Loss self: 0.0000; time: 0.14s
Val loss: 0.3011 score: 0.8571 time: 0.08s
Test loss: 0.2157 score: 0.9388 time: 0.08s
Epoch 100/1000, LR 0.000265
Train loss: 0.0844;  Loss pred: 0.0844; Loss self: 0.0000; time: 0.15s
Val loss: 0.2970 score: 0.8571 time: 0.08s
Test loss: 0.2131 score: 0.9388 time: 0.08s
Epoch 101/1000, LR 0.000265
Train loss: 0.0834;  Loss pred: 0.0834; Loss self: 0.0000; time: 0.14s
Val loss: 0.2932 score: 0.8571 time: 0.11s
Test loss: 0.2106 score: 0.9388 time: 0.08s
Epoch 102/1000, LR 0.000264
Train loss: 0.0853;  Loss pred: 0.0853; Loss self: 0.0000; time: 0.15s
Val loss: 0.2901 score: 0.8571 time: 0.23s
Test loss: 0.2086 score: 0.9592 time: 0.08s
Epoch 103/1000, LR 0.000264
Train loss: 0.0842;  Loss pred: 0.0842; Loss self: 0.0000; time: 0.15s
Val loss: 0.2874 score: 0.8776 time: 0.08s
Test loss: 0.2069 score: 0.9592 time: 0.08s
Epoch 104/1000, LR 0.000264
Train loss: 0.0806;  Loss pred: 0.0806; Loss self: 0.0000; time: 0.15s
Val loss: 0.2852 score: 0.8776 time: 0.08s
Test loss: 0.2055 score: 0.9592 time: 0.09s
Epoch 105/1000, LR 0.000264
Train loss: 0.0800;  Loss pred: 0.0800; Loss self: 0.0000; time: 0.16s
Val loss: 0.2835 score: 0.8776 time: 0.09s
Test loss: 0.2043 score: 0.9592 time: 0.09s
Epoch 106/1000, LR 0.000264
Train loss: 0.0732;  Loss pred: 0.0732; Loss self: 0.0000; time: 0.17s
Val loss: 0.2821 score: 0.8980 time: 0.09s
Test loss: 0.2034 score: 0.9592 time: 0.10s
Epoch 107/1000, LR 0.000264
Train loss: 0.0747;  Loss pred: 0.0747; Loss self: 0.0000; time: 0.18s
Val loss: 0.2811 score: 0.9184 time: 0.09s
Test loss: 0.2027 score: 0.9592 time: 0.09s
Epoch 108/1000, LR 0.000264
Train loss: 0.0753;  Loss pred: 0.0753; Loss self: 0.0000; time: 0.17s
Val loss: 0.2805 score: 0.9184 time: 0.09s
Test loss: 0.2024 score: 0.9592 time: 0.23s
Epoch 109/1000, LR 0.000264
Train loss: 0.0705;  Loss pred: 0.0705; Loss self: 0.0000; time: 0.17s
Val loss: 0.2805 score: 0.9184 time: 0.09s
Test loss: 0.2022 score: 0.9592 time: 0.09s
Epoch 110/1000, LR 0.000263
Train loss: 0.0724;  Loss pred: 0.0724; Loss self: 0.0000; time: 0.17s
Val loss: 0.2806 score: 0.8980 time: 0.09s
Test loss: 0.2019 score: 0.9592 time: 0.09s
     INFO: Early stopping counter 1 of 20
Epoch 111/1000, LR 0.000263
Train loss: 0.0714;  Loss pred: 0.0714; Loss self: 0.0000; time: 0.17s
Val loss: 0.2807 score: 0.8980 time: 0.09s
Test loss: 0.2019 score: 0.9592 time: 0.09s
     INFO: Early stopping counter 2 of 20
Epoch 112/1000, LR 0.000263
Train loss: 0.0719;  Loss pred: 0.0719; Loss self: 0.0000; time: 0.16s
Val loss: 0.2815 score: 0.8980 time: 0.08s
Test loss: 0.2021 score: 0.9592 time: 0.08s
     INFO: Early stopping counter 3 of 20
Epoch 113/1000, LR 0.000263
Train loss: 0.0679;  Loss pred: 0.0679; Loss self: 0.0000; time: 0.15s
Val loss: 0.2827 score: 0.8980 time: 0.09s
Test loss: 0.2025 score: 0.9592 time: 0.08s
     INFO: Early stopping counter 4 of 20
Epoch 114/1000, LR 0.000263
Train loss: 0.0666;  Loss pred: 0.0666; Loss self: 0.0000; time: 0.14s
Val loss: 0.2838 score: 0.8980 time: 0.09s
Test loss: 0.2028 score: 0.9592 time: 0.08s
     INFO: Early stopping counter 5 of 20
Epoch 115/1000, LR 0.000263
Train loss: 0.0668;  Loss pred: 0.0668; Loss self: 0.0000; time: 0.14s
Val loss: 0.2846 score: 0.8980 time: 0.22s
Test loss: 0.2031 score: 0.9592 time: 0.09s
     INFO: Early stopping counter 6 of 20
Epoch 116/1000, LR 0.000263
Train loss: 0.0657;  Loss pred: 0.0657; Loss self: 0.0000; time: 0.16s
Val loss: 0.2857 score: 0.8980 time: 0.19s
Test loss: 0.2035 score: 0.9592 time: 0.09s
     INFO: Early stopping counter 7 of 20
Epoch 117/1000, LR 0.000262
Train loss: 0.0618;  Loss pred: 0.0618; Loss self: 0.0000; time: 0.16s
Val loss: 0.2865 score: 0.8980 time: 0.09s
Test loss: 0.2037 score: 0.9592 time: 0.09s
     INFO: Early stopping counter 8 of 20
Epoch 118/1000, LR 0.000262
Train loss: 0.0636;  Loss pred: 0.0636; Loss self: 0.0000; time: 0.15s
Val loss: 0.2882 score: 0.8980 time: 0.08s
Test loss: 0.2044 score: 0.9592 time: 0.09s
     INFO: Early stopping counter 9 of 20
Epoch 119/1000, LR 0.000262
Train loss: 0.0641;  Loss pred: 0.0641; Loss self: 0.0000; time: 0.17s
Val loss: 0.2900 score: 0.8776 time: 0.10s
Test loss: 0.2054 score: 0.9592 time: 0.09s
     INFO: Early stopping counter 10 of 20
Epoch 120/1000, LR 0.000262
Train loss: 0.0704;  Loss pred: 0.0704; Loss self: 0.0000; time: 0.16s
Val loss: 0.2919 score: 0.8776 time: 0.09s
Test loss: 0.2061 score: 0.9592 time: 0.08s
     INFO: Early stopping counter 11 of 20
Epoch 121/1000, LR 0.000262
Train loss: 0.0616;  Loss pred: 0.0616; Loss self: 0.0000; time: 0.15s
Val loss: 0.2928 score: 0.8980 time: 0.29s
Test loss: 0.2065 score: 0.9592 time: 0.08s
     INFO: Early stopping counter 12 of 20
Epoch 122/1000, LR 0.000262
Train loss: 0.0593;  Loss pred: 0.0593; Loss self: 0.0000; time: 0.16s
Val loss: 0.2937 score: 0.8980 time: 0.21s
Test loss: 0.2070 score: 0.9592 time: 0.09s
     INFO: Early stopping counter 13 of 20
Epoch 123/1000, LR 0.000262
Train loss: 0.0615;  Loss pred: 0.0615; Loss self: 0.0000; time: 0.15s
Val loss: 0.2946 score: 0.8980 time: 0.08s
Test loss: 0.2075 score: 0.9592 time: 0.08s
     INFO: Early stopping counter 14 of 20
Epoch 124/1000, LR 0.000261
Train loss: 0.0609;  Loss pred: 0.0609; Loss self: 0.0000; time: 0.14s
Val loss: 0.2956 score: 0.8980 time: 0.08s
Test loss: 0.2081 score: 0.9592 time: 0.08s
     INFO: Early stopping counter 15 of 20
Epoch 125/1000, LR 0.000261
Train loss: 0.0572;  Loss pred: 0.0572; Loss self: 0.0000; time: 0.14s
Val loss: 0.2964 score: 0.8980 time: 0.08s
Test loss: 0.2083 score: 0.9592 time: 0.09s
     INFO: Early stopping counter 16 of 20
Epoch 126/1000, LR 0.000261
Train loss: 0.0617;  Loss pred: 0.0617; Loss self: 0.0000; time: 0.16s
Val loss: 0.2977 score: 0.8980 time: 0.08s
Test loss: 0.2088 score: 0.9592 time: 0.08s
     INFO: Early stopping counter 17 of 20
Epoch 127/1000, LR 0.000261
Train loss: 0.0563;  Loss pred: 0.0563; Loss self: 0.0000; time: 0.16s
Val loss: 0.2988 score: 0.8980 time: 0.08s
Test loss: 0.2094 score: 0.9592 time: 0.08s
     INFO: Early stopping counter 18 of 20
Epoch 128/1000, LR 0.000261
Train loss: 0.0614;  Loss pred: 0.0614; Loss self: 0.0000; time: 0.15s
Val loss: 0.2991 score: 0.8980 time: 0.09s
Test loss: 0.2096 score: 0.9592 time: 0.09s
     INFO: Early stopping counter 19 of 20
Epoch 129/1000, LR 0.000261
Train loss: 0.0572;  Loss pred: 0.0572; Loss self: 0.0000; time: 0.19s
Val loss: 0.2996 score: 0.8980 time: 0.09s
Test loss: 0.2100 score: 0.9592 time: 0.23s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 108,   Train_Loss: 0.0705,   Val_Loss: 0.2805,   Val_Precision: 0.9167,   Val_Recall: 0.9167,   Val_accuracy: 0.9167,   Val_Score: 0.9184,   Val_Loss: 0.2805,   Test_Precision: 0.9600,   Test_Recall: 0.9600,   Test_accuracy: 0.9600,   Test_Score: 0.9592,   Test_loss: 0.2022


[0.0923724400345236, 0.09272063802927732, 0.08889736793935299, 0.09417051286436617, 0.09018707810901105, 0.08776998892426491, 0.08725390210747719, 0.20725050591863692, 0.08858782309107482, 0.08919495297595859, 0.08934879209846258, 0.08880400890484452, 0.0894871880300343, 0.08816483709961176, 0.0875755709130317, 0.0855906531214714, 0.08672121609561145, 0.09077785513363779, 0.08842640393413603, 0.08714691898785532, 0.08716778503730893, 0.09772477601654828, 0.08830070099793375, 0.08922215108759701, 0.08772663702256978, 0.08844832889735699, 0.0913308730814606, 0.09526202897541225, 0.10000373213551939, 0.09139185608364642, 0.0877063418738544, 0.08642259007319808, 0.08876497694291174, 0.08700242708437145, 0.0867588750552386, 0.0852962601929903, 0.08861434808932245, 0.08640759508125484, 0.08718537003733218, 0.08595339697785676, 0.0881145850289613, 0.09242727211676538, 0.08877051109448075, 0.0899763721972704, 0.0965567990206182, 0.08937351894564927, 0.08996126800775528, 0.08943881699815392, 0.08976403111591935, 0.0875426169950515, 0.09064319496974349, 0.09473756817169487, 0.08808105695061386, 0.08961903001181781, 0.09991109208203852, 0.08954954100772738, 0.08896022709086537, 0.08985830983147025, 0.08667816501110792, 0.08882927102968097, 0.08979957597330213, 0.08840710297226906, 0.07976822415366769, 0.09475376293994486, 0.08674033009447157, 0.08774955198168755, 0.09162684483453631, 0.08909940300509334, 0.0891390519682318, 0.08898501098155975, 0.08847574098035693, 0.08648484176956117, 0.08646886493079364, 0.08767650602385402, 0.09117117198184133, 0.08694075886160135, 0.08676877897232771, 0.08838087599724531, 0.08881456102244556, 0.08996102889068425, 0.09085428318940103, 0.0901144880335778, 0.09139849292114377, 0.09101863391697407, 0.08963741804473102, 0.09346674289554358, 0.089841578155756, 0.08780689095146954, 0.09459696407429874, 0.0944067221134901, 0.08965900586917996, 0.09251349093392491, 0.08970654499717057, 0.09053152496926486, 0.08625785913318396, 0.08695084089413285, 0.08992516784928739, 0.08656944101676345, 0.09050293290056288, 0.08631433197297156, 0.08718661894090474, 0.08699646894820035, 0.08878183807246387, 0.09023214387707412, 0.10148900700733066, 0.11179829691536725, 0.1014670068398118, 0.23500279593281448, 0.10184356686659157, 0.10023184306919575, 0.09936410398222506, 0.08917102008126676, 0.08813585084863007, 0.08465180802159011, 0.09733205405063927, 0.09464075113646686, 0.0924620651639998, 0.09749532910063863, 0.09290332300588489, 0.08715918194502592, 0.08909616083838046, 0.09065122087486088, 0.08670487604103982, 0.0856805460061878, 0.09604316693730652, 0.09002745500765741, 0.08653434994630516, 0.09356915997341275, 0.23805228690616786]
[0.0018851518374392573, 0.0018922579189648433, 0.0018142319987623058, 0.0019218472013135953, 0.0018405526144696133, 0.0017912242637605083, 0.0017806918797444325, 0.004229602161604835, 0.0018079147569607108, 0.001820305162774665, 0.001823444736703318, 0.0018123267123437657, 0.0018262691434700877, 0.001799282389787995, 0.0017872565492455447, 0.0017467480228871715, 0.0017698207366451317, 0.0018526092884415875, 0.0018046204884517559, 0.0017785085507725574, 0.0017789343885165087, 0.0019943831840111893, 0.0018020551224068111, 0.0018208602262774901, 0.0017903395310728525, 0.001805067936680755, 0.0018638953690093998, 0.0019441230403145356, 0.0020408924925616203, 0.0018651399200744166, 0.0017899253443643755, 0.0017637263280244507, 0.0018115301416920765, 0.0017755597364157438, 0.0017705892868416042, 0.0017407400039385777, 0.0018084560834555601, 0.001763420307780711, 0.0017792932660680037, 0.0017541509587317705, 0.0017982568373257409, 0.001886270859525824, 0.0018116430835608318, 0.0018362524938218448, 0.0019705469187881264, 0.00182394936623774, 0.0018359442450562302, 0.0018252819795541617, 0.001831919002365701, 0.0017865840203071736, 0.0018498611218314997, 0.0019334197586060179, 0.0017975725908288543, 0.0018289597961595472, 0.002039001879225276, 0.0018275416532189262, 0.0018155148385890893, 0.0018338430577851071, 0.001768942143083835, 0.0018128422659118564, 0.0018326444076184108, 0.001804226591270797, 0.0016279229419115856, 0.0019337502640805074, 0.001770210818254522, 0.0017908071832997458, 0.001869935608868088, 0.0018183551633692517, 0.0018191643258822815, 0.0018160206322767297, 0.0018056273669460599, 0.0017649967708073709, 0.0017646707128733397, 0.0017893164494664085, 0.0018606361628947209, 0.0017743012012571705, 0.0017707914075985247, 0.0018036913468825575, 0.0018125420616825624, 0.0018359393651160051, 0.0018541690446816537, 0.0018390711843587306, 0.001865275365737628, 0.001857523141162736, 0.0018293350621373678, 0.0019074845488886445, 0.0018335015950154286, 0.0017919773663565212, 0.0019305502872305866, 0.001926667798234492, 0.0018297756299832646, 0.0018880304272229575, 0.001830745816268787, 0.0018475821422298951, 0.001760364472105795, 0.0017745069570231195, 0.0018352075071283141, 0.0017667232860563969, 0.0018469986306237324, 0.001761516979040236, 0.001779318753896015, 0.0017754381418000071, 0.0018118742463768137, 0.0018414723240219209, 0.002071204224639401, 0.0022815978962319847, 0.002070755241628812, 0.004795975427200295, 0.002078440140134522, 0.002045547817738689, 0.002027838856780103, 0.0018198167363523829, 0.0017986908336455117, 0.0017275879188079614, 0.0019863684500130464, 0.0019314439007442218, 0.0018869809217142816, 0.0019897005938905843, 0.001895986183793569, 0.0017787588152046107, 0.001818288996701642, 0.0018500249158134874, 0.00176948726614367, 0.001748582571554853, 0.0019600646313736023, 0.0018372950001562737, 0.0017660071417613297, 0.0019095746933349542, 0.004858209936860569]
[530.4612499321937, 528.4691848704474, 551.1974216540187, 520.3327295304712, 543.3150849035452, 558.2773861607889, 561.5794688430441, 236.42885590463447, 553.1234236292767, 549.3584375027066, 548.4125621530718, 551.7768916547967, 547.5644176410402, 555.7771285239041, 559.5167635122839, 572.4924184239901, 565.0289768304997, 539.7792217921992, 554.1331301507795, 562.268873863786, 562.1342790691236, 501.4081586813006, 554.9219818894372, 549.1909733480031, 558.553270284299, 553.9957691779999, 536.5108023909452, 514.370736452057, 489.9817132184425, 536.15280507218, 558.6825188818768, 566.9813871407702, 552.0195203961337, 563.2026788457496, 564.7837177326485, 574.4683282612061, 552.9578567864478, 567.0797798957606, 562.0208984491152, 570.0763637372394, 556.094090256395, 530.1465560738094, 551.9851062685449, 544.5874156002759, 507.4733265498664, 548.2608336122289, 544.6788499665863, 547.8605559039471, 545.8756630116404, 559.7273840096625, 540.581121576265, 517.2182582436175, 556.3057676234947, 546.7588746892095, 490.43603646895735, 547.1831507854597, 550.8079464539882, 545.3029340513946, 565.3096139462641, 551.6199720205673, 545.6595921407018, 554.2541080140358, 614.279689937751, 517.1298582733469, 564.9044676983888, 558.4074094215981, 534.777772698452, 549.9475680796529, 549.7029519392138, 550.6545367528719, 553.8241268968723, 566.5732745463127, 566.6779602024118, 558.8726355800337, 537.4505881065057, 563.6021659070376, 564.7192524816683, 554.4185826074778, 551.7113346719862, 544.6802977269429, 539.325150998674, 543.7527424196427, 536.113872712058, 538.3513011708912, 546.6467137144439, 524.2506423355454, 545.404488721803, 558.0427625786463, 517.9870250541468, 519.0308370318709, 546.5150937708937, 529.6524810094652, 546.2254733090602, 541.2479245946131, 568.064179802364, 563.5368157009549, 544.8975094727978, 566.0195956505202, 541.418917924319, 567.6925127028017, 562.0128477881715, 563.241250965895, 551.9146828206702, 543.0437302559732, 482.8109116927381, 438.28932418437137, 482.9155951880731, 208.50815755404344, 481.13004588877766, 488.866596677989, 493.1358311122642, 549.5058815671665, 555.9599133405499, 578.8417417794874, 503.4312742902416, 517.7473700451155, 529.9470643781186, 502.58817988521497, 527.4300037351315, 562.189764824845, 549.9675804088293, 540.5332606346456, 565.1354599343057, 571.891780386895, 510.1872581105683, 544.2784092456267, 566.2491256986926, 523.6768184508991, 205.83713198820953]
Elapsed: 0.09341025265301903~0.021111808436692334
Time per graph: 0.0019063316867963065~0.0004308532334018844
Speed: 536.5479986937416~54.771229909070236
Total Time: 0.2387
best val loss: 0.2804819345474243 test_score: 0.9592

Testing...
Test loss: 0.2027 score: 0.9592 time: 0.10s
test Score 0.9592
Epoch Time List: [0.3944286396726966, 0.3265792690217495, 0.33373961318284273, 0.33247716003097594, 0.3256023209542036, 0.313202099641785, 0.31486809928901494, 0.42849059915170074, 0.3136003268882632, 0.3209924118127674, 0.31533136311918497, 0.3194416519254446, 0.3189978750888258, 0.32083444483578205, 0.3197097971569747, 0.30616185907274485, 0.3118660179898143, 0.3370774262584746, 0.329371121712029, 0.33165341592393816, 0.32496556430123746, 0.3262148310896009, 0.3068312150426209, 0.32139254128560424, 0.3119534910656512, 0.31794219207949936, 0.3295165719464421, 0.31241299910470843, 0.3244255380704999, 0.33130609313957393, 0.3108803366776556, 0.31296571413986385, 0.3028184117283672, 0.30762363923713565, 0.30715922615490854, 0.3043460950721055, 0.30791961890645325, 0.30012724408879876, 0.29813182004727423, 0.30006941594183445, 0.31553454301320016, 0.3219171760138124, 0.3171825523022562, 0.30888749985024333, 0.31494595808908343, 0.30955043877474964, 0.31979612191207707, 0.3159781419672072, 0.3313588828314096, 0.3271702993661165, 0.3325364328920841, 0.33099319390021265, 0.32596855401061475, 0.31559884105809033, 0.4111499229911715, 0.3202768031042069, 0.31779479375109076, 0.3148329029791057, 0.3130052969790995, 0.3137565969955176, 0.318698224844411, 0.3123018878977746, 0.4069000561721623, 0.3053554801736027, 0.30579477408900857, 0.3017322460655123, 0.3157752288971096, 0.3039442766457796, 0.3040503079537302, 0.30720041994936764, 0.4163598781451583, 0.3038248971570283, 0.30278034997172654, 0.30900313914753497, 0.3202691311016679, 0.3087431329768151, 0.3049145289696753, 0.30580599000677466, 0.4104957371018827, 0.3103020691778511, 0.3310665839817375, 0.3242657820228487, 0.3194034178741276, 0.3251930898986757, 0.31259172898717225, 0.32520848512649536, 0.33863706211559474, 0.31783980457112193, 0.33494991110637784, 0.32845502789132297, 0.32818651874549687, 0.3085983272176236, 0.31436308519914746, 0.3123734160326421, 0.43101109587587416, 0.31729108607396483, 0.31424672715365887, 0.30448735714890063, 0.30527249188162386, 0.31394068989902735, 0.32913225912488997, 0.46229023090563715, 0.32116860104724765, 0.31675202888436615, 0.3439998719841242, 0.3600955680012703, 0.36222268594428897, 0.48898637224920094, 0.3557972810231149, 0.3562939870171249, 0.353372807148844, 0.325491817900911, 0.32348638703115284, 0.30541313788853586, 0.4523427567910403, 0.4364569941535592, 0.32888247119262815, 0.32805662089958787, 0.35421790019609034, 0.3248003839980811, 0.5334707333240658, 0.45019044703803957, 0.3153932101558894, 0.3037351821549237, 0.3105179921258241, 0.3244987758807838, 0.3185082252603024, 0.3347621476277709, 0.5078657518606633]
Total Epoch List: [129]
Total Time List: [0.23868092894554138]
========================k_idx:1========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: SGFormer
SGFormer(
  (trans_conv): SGModule(
    (attns): ModuleList(
      (0-1): 2 x SGFormerAttention(heads=1, head_channels=64)
    )
    (fcs): ModuleList(
      (0): Linear(in_features=14887, out_features=64, bias=True)
    )
    (bns): ModuleList(
      (0-2): 3 x LayerNorm((64,), eps=1e-05, elementwise_affine=True)
    )
  )
  (graph_conv): GraphModule(
    (convs): ModuleList(
      (0-2): 3 x GCNConv(64, 64)
    )
    (fcs): ModuleList(
      (0): Linear(in_features=14887, out_features=64, bias=True)
    )
    (bns): ModuleList(
      (0-3): 4 x BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (classifier): Linear(in_features=64, out_features=2, bias=True)
)
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7f342d3187f0>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.7869;  Loss pred: 0.7869; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7130 score: 0.5102 time: 0.10s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7176 score: 0.4898 time: 0.10s
Epoch 2/1000, LR 0.000000
Train loss: 0.7895;  Loss pred: 0.7895; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7127 score: 0.5102 time: 0.09s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7174 score: 0.4898 time: 0.08s
Epoch 3/1000, LR 0.000030
Train loss: 0.7522;  Loss pred: 0.7522; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7105 score: 0.5102 time: 0.09s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7152 score: 0.4898 time: 0.09s
Epoch 4/1000, LR 0.000060
Train loss: 0.7842;  Loss pred: 0.7842; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7064 score: 0.5102 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7111 score: 0.4898 time: 0.08s
Epoch 5/1000, LR 0.000090
Train loss: 0.7205;  Loss pred: 0.7205; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7004 score: 0.5102 time: 0.09s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7050 score: 0.4898 time: 0.08s
Epoch 6/1000, LR 0.000120
Train loss: 0.7072;  Loss pred: 0.7072; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6927 score: 0.5102 time: 0.09s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6972 score: 0.4898 time: 0.09s
Epoch 7/1000, LR 0.000150
Train loss: 0.6925;  Loss pred: 0.6925; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6840 score: 0.5102 time: 0.09s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6882 score: 0.4898 time: 0.08s
Epoch 8/1000, LR 0.000180
Train loss: 0.7086;  Loss pred: 0.7086; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6749 score: 0.5102 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6785 score: 0.4898 time: 0.08s
Epoch 9/1000, LR 0.000210
Train loss: 0.6882;  Loss pred: 0.6882; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6665 score: 0.5102 time: 0.09s
Test loss: 0.6696 score: 0.5102 time: 0.09s
Epoch 10/1000, LR 0.000240
Train loss: 0.6497;  Loss pred: 0.6497; Loss self: 0.0000; time: 0.13s
Val loss: 0.6594 score: 0.5918 time: 0.09s
Test loss: 0.6621 score: 0.5918 time: 0.09s
Epoch 11/1000, LR 0.000270
Train loss: 0.6255;  Loss pred: 0.6255; Loss self: 0.0000; time: 0.12s
Val loss: 0.6540 score: 0.7143 time: 0.09s
Test loss: 0.6566 score: 0.6735 time: 0.08s
Epoch 12/1000, LR 0.000270
Train loss: 0.6229;  Loss pred: 0.6229; Loss self: 0.0000; time: 0.13s
Val loss: 0.6501 score: 0.9184 time: 0.10s
Test loss: 0.6526 score: 0.8776 time: 0.10s
Epoch 13/1000, LR 0.000270
Train loss: 0.5920;  Loss pred: 0.5920; Loss self: 0.0000; time: 0.12s
Val loss: 0.6475 score: 0.8571 time: 0.10s
Test loss: 0.6501 score: 0.8367 time: 0.09s
Epoch 14/1000, LR 0.000270
Train loss: 0.5813;  Loss pred: 0.5813; Loss self: 0.0000; time: 0.12s
Val loss: 0.6462 score: 0.8571 time: 0.08s
Test loss: 0.6489 score: 0.8367 time: 0.09s
Epoch 15/1000, LR 0.000270
Train loss: 0.5401;  Loss pred: 0.5401; Loss self: 0.0000; time: 0.12s
Val loss: 0.6453 score: 0.6939 time: 0.09s
Test loss: 0.6482 score: 0.7347 time: 0.08s
Epoch 16/1000, LR 0.000270
Train loss: 0.5340;  Loss pred: 0.5340; Loss self: 0.0000; time: 0.12s
Val loss: 0.6448 score: 0.6122 time: 0.20s
Test loss: 0.6479 score: 0.6122 time: 0.08s
Epoch 17/1000, LR 0.000270
Train loss: 0.5287;  Loss pred: 0.5287; Loss self: 0.0000; time: 0.11s
Val loss: 0.6442 score: 0.5714 time: 0.08s
Test loss: 0.6475 score: 0.5918 time: 0.08s
Epoch 18/1000, LR 0.000270
Train loss: 0.5110;  Loss pred: 0.5110; Loss self: 0.0000; time: 0.12s
Val loss: 0.6436 score: 0.5714 time: 0.09s
Test loss: 0.6472 score: 0.5510 time: 0.08s
Epoch 19/1000, LR 0.000270
Train loss: 0.5166;  Loss pred: 0.5166; Loss self: 0.0000; time: 0.12s
Val loss: 0.6425 score: 0.5714 time: 0.09s
Test loss: 0.6464 score: 0.5510 time: 0.08s
Epoch 20/1000, LR 0.000270
Train loss: 0.5096;  Loss pred: 0.5096; Loss self: 0.0000; time: 0.13s
Val loss: 0.6406 score: 0.5714 time: 0.09s
Test loss: 0.6450 score: 0.5510 time: 0.08s
Epoch 21/1000, LR 0.000270
Train loss: 0.4904;  Loss pred: 0.4904; Loss self: 0.0000; time: 0.12s
Val loss: 0.6382 score: 0.5714 time: 0.09s
Test loss: 0.6430 score: 0.5510 time: 0.09s
Epoch 22/1000, LR 0.000270
Train loss: 0.4729;  Loss pred: 0.4729; Loss self: 0.0000; time: 0.13s
Val loss: 0.6348 score: 0.5714 time: 0.09s
Test loss: 0.6403 score: 0.5510 time: 0.08s
Epoch 23/1000, LR 0.000270
Train loss: 0.4537;  Loss pred: 0.4537; Loss self: 0.0000; time: 0.14s
Val loss: 0.6305 score: 0.5714 time: 0.09s
Test loss: 0.6366 score: 0.5510 time: 0.09s
Epoch 24/1000, LR 0.000270
Train loss: 0.4605;  Loss pred: 0.4605; Loss self: 0.0000; time: 0.13s
Val loss: 0.6255 score: 0.5918 time: 0.09s
Test loss: 0.6323 score: 0.5918 time: 0.16s
Epoch 25/1000, LR 0.000270
Train loss: 0.4587;  Loss pred: 0.4587; Loss self: 0.0000; time: 0.13s
Val loss: 0.6195 score: 0.6327 time: 0.09s
Test loss: 0.6271 score: 0.5918 time: 0.09s
Epoch 26/1000, LR 0.000270
Train loss: 0.4464;  Loss pred: 0.4464; Loss self: 0.0000; time: 0.13s
Val loss: 0.6128 score: 0.6327 time: 0.09s
Test loss: 0.6214 score: 0.6122 time: 0.08s
Epoch 27/1000, LR 0.000270
Train loss: 0.4380;  Loss pred: 0.4380; Loss self: 0.0000; time: 0.13s
Val loss: 0.6052 score: 0.6531 time: 0.09s
Test loss: 0.6148 score: 0.6327 time: 0.10s
Epoch 28/1000, LR 0.000270
Train loss: 0.4242;  Loss pred: 0.4242; Loss self: 0.0000; time: 0.15s
Val loss: 0.5968 score: 0.6939 time: 0.09s
Test loss: 0.6075 score: 0.6531 time: 0.09s
Epoch 29/1000, LR 0.000270
Train loss: 0.4174;  Loss pred: 0.4174; Loss self: 0.0000; time: 0.14s
Val loss: 0.5879 score: 0.7347 time: 0.09s
Test loss: 0.5997 score: 0.7551 time: 0.09s
Epoch 30/1000, LR 0.000270
Train loss: 0.4293;  Loss pred: 0.4293; Loss self: 0.0000; time: 0.13s
Val loss: 0.5787 score: 0.8163 time: 0.09s
Test loss: 0.5915 score: 0.7551 time: 0.09s
Epoch 31/1000, LR 0.000270
Train loss: 0.4091;  Loss pred: 0.4091; Loss self: 0.0000; time: 0.13s
Val loss: 0.5695 score: 0.8571 time: 0.08s
Test loss: 0.5831 score: 0.7551 time: 0.08s
Epoch 32/1000, LR 0.000270
Train loss: 0.4002;  Loss pred: 0.4002; Loss self: 0.0000; time: 0.14s
Val loss: 0.5604 score: 0.8776 time: 0.22s
Test loss: 0.5747 score: 0.8163 time: 0.08s
Epoch 33/1000, LR 0.000270
Train loss: 0.3822;  Loss pred: 0.3822; Loss self: 0.0000; time: 0.12s
Val loss: 0.5515 score: 0.8776 time: 0.09s
Test loss: 0.5663 score: 0.8571 time: 0.08s
Epoch 34/1000, LR 0.000270
Train loss: 0.3777;  Loss pred: 0.3777; Loss self: 0.0000; time: 0.13s
Val loss: 0.5428 score: 0.8776 time: 0.08s
Test loss: 0.5581 score: 0.8776 time: 0.08s
Epoch 35/1000, LR 0.000270
Train loss: 0.3644;  Loss pred: 0.3644; Loss self: 0.0000; time: 0.12s
Val loss: 0.5347 score: 0.8776 time: 0.08s
Test loss: 0.5505 score: 0.8571 time: 0.08s
Epoch 36/1000, LR 0.000270
Train loss: 0.3678;  Loss pred: 0.3678; Loss self: 0.0000; time: 0.13s
Val loss: 0.5268 score: 0.8776 time: 0.08s
Test loss: 0.5429 score: 0.8571 time: 0.08s
Epoch 37/1000, LR 0.000270
Train loss: 0.3506;  Loss pred: 0.3506; Loss self: 0.0000; time: 0.12s
Val loss: 0.5194 score: 0.8776 time: 0.09s
Test loss: 0.5358 score: 0.8571 time: 0.09s
Epoch 38/1000, LR 0.000270
Train loss: 0.3396;  Loss pred: 0.3396; Loss self: 0.0000; time: 0.12s
Val loss: 0.5125 score: 0.8776 time: 0.09s
Test loss: 0.5293 score: 0.8571 time: 0.08s
Epoch 39/1000, LR 0.000269
Train loss: 0.3472;  Loss pred: 0.3472; Loss self: 0.0000; time: 0.13s
Val loss: 0.5057 score: 0.8776 time: 0.09s
Test loss: 0.5230 score: 0.8571 time: 0.08s
Epoch 40/1000, LR 0.000269
Train loss: 0.3334;  Loss pred: 0.3334; Loss self: 0.0000; time: 0.12s
Val loss: 0.4997 score: 0.8776 time: 0.09s
Test loss: 0.5173 score: 0.8776 time: 0.18s
Epoch 41/1000, LR 0.000269
Train loss: 0.3170;  Loss pred: 0.3170; Loss self: 0.0000; time: 0.12s
Val loss: 0.4936 score: 0.8776 time: 0.08s
Test loss: 0.5118 score: 0.8980 time: 0.08s
Epoch 42/1000, LR 0.000269
Train loss: 0.3165;  Loss pred: 0.3165; Loss self: 0.0000; time: 0.12s
Val loss: 0.4880 score: 0.8776 time: 0.08s
Test loss: 0.5067 score: 0.8980 time: 0.08s
Epoch 43/1000, LR 0.000269
Train loss: 0.3084;  Loss pred: 0.3084; Loss self: 0.0000; time: 0.13s
Val loss: 0.4830 score: 0.8980 time: 0.08s
Test loss: 0.5022 score: 0.8980 time: 0.08s
Epoch 44/1000, LR 0.000269
Train loss: 0.3186;  Loss pred: 0.3186; Loss self: 0.0000; time: 0.13s
Val loss: 0.4781 score: 0.8980 time: 0.08s
Test loss: 0.4976 score: 0.8980 time: 0.08s
Epoch 45/1000, LR 0.000269
Train loss: 0.2997;  Loss pred: 0.2997; Loss self: 0.0000; time: 0.12s
Val loss: 0.4731 score: 0.8980 time: 0.08s
Test loss: 0.4932 score: 0.8980 time: 0.08s
Epoch 46/1000, LR 0.000269
Train loss: 0.3085;  Loss pred: 0.3085; Loss self: 0.0000; time: 0.12s
Val loss: 0.4684 score: 0.9388 time: 0.09s
Test loss: 0.4890 score: 0.8980 time: 0.08s
Epoch 47/1000, LR 0.000269
Train loss: 0.2965;  Loss pred: 0.2965; Loss self: 0.0000; time: 0.13s
Val loss: 0.4636 score: 0.9388 time: 0.09s
Test loss: 0.4849 score: 0.8980 time: 0.08s
Epoch 48/1000, LR 0.000269
Train loss: 0.2879;  Loss pred: 0.2879; Loss self: 0.0000; time: 0.13s
Val loss: 0.4589 score: 0.9184 time: 0.21s
Test loss: 0.4811 score: 0.8980 time: 0.08s
Epoch 49/1000, LR 0.000269
Train loss: 0.2800;  Loss pred: 0.2800; Loss self: 0.0000; time: 0.12s
Val loss: 0.4541 score: 0.9184 time: 0.08s
Test loss: 0.4768 score: 0.8980 time: 0.08s
Epoch 50/1000, LR 0.000269
Train loss: 0.2722;  Loss pred: 0.2722; Loss self: 0.0000; time: 0.12s
Val loss: 0.4493 score: 0.9184 time: 0.08s
Test loss: 0.4726 score: 0.8980 time: 0.08s
Epoch 51/1000, LR 0.000269
Train loss: 0.2572;  Loss pred: 0.2572; Loss self: 0.0000; time: 0.12s
Val loss: 0.4433 score: 0.9184 time: 0.08s
Test loss: 0.4673 score: 0.8980 time: 0.08s
Epoch 52/1000, LR 0.000269
Train loss: 0.2688;  Loss pred: 0.2688; Loss self: 0.0000; time: 0.14s
Val loss: 0.4378 score: 0.9184 time: 0.09s
Test loss: 0.4624 score: 0.8980 time: 0.09s
Epoch 53/1000, LR 0.000269
Train loss: 0.2596;  Loss pred: 0.2596; Loss self: 0.0000; time: 0.12s
Val loss: 0.4322 score: 0.9184 time: 0.08s
Test loss: 0.4574 score: 0.9184 time: 0.08s
Epoch 54/1000, LR 0.000269
Train loss: 0.2538;  Loss pred: 0.2538; Loss self: 0.0000; time: 0.13s
Val loss: 0.4264 score: 0.9388 time: 0.10s
Test loss: 0.4523 score: 0.8980 time: 0.10s
Epoch 55/1000, LR 0.000269
Train loss: 0.2493;  Loss pred: 0.2493; Loss self: 0.0000; time: 0.13s
Val loss: 0.4201 score: 0.9388 time: 0.09s
Test loss: 0.4469 score: 0.8980 time: 0.09s
Epoch 56/1000, LR 0.000269
Train loss: 0.2402;  Loss pred: 0.2402; Loss self: 0.0000; time: 0.13s
Val loss: 0.4139 score: 0.9388 time: 0.09s
Test loss: 0.4416 score: 0.8980 time: 0.09s
Epoch 57/1000, LR 0.000269
Train loss: 0.2381;  Loss pred: 0.2381; Loss self: 0.0000; time: 0.13s
Val loss: 0.4083 score: 0.9388 time: 0.09s
Test loss: 0.4367 score: 0.8980 time: 0.08s
Epoch 58/1000, LR 0.000269
Train loss: 0.2389;  Loss pred: 0.2389; Loss self: 0.0000; time: 0.13s
Val loss: 0.4026 score: 0.9388 time: 0.09s
Test loss: 0.4318 score: 0.8980 time: 0.09s
Epoch 59/1000, LR 0.000268
Train loss: 0.2331;  Loss pred: 0.2331; Loss self: 0.0000; time: 0.13s
Val loss: 0.3971 score: 0.9388 time: 0.09s
Test loss: 0.4272 score: 0.8980 time: 0.08s
Epoch 60/1000, LR 0.000268
Train loss: 0.2174;  Loss pred: 0.2174; Loss self: 0.0000; time: 0.13s
Val loss: 0.3917 score: 0.9388 time: 0.09s
Test loss: 0.4226 score: 0.8980 time: 0.09s
Epoch 61/1000, LR 0.000268
Train loss: 0.2338;  Loss pred: 0.2338; Loss self: 0.0000; time: 0.13s
Val loss: 0.3870 score: 0.9388 time: 0.09s
Test loss: 0.4185 score: 0.8776 time: 0.09s
Epoch 62/1000, LR 0.000268
Train loss: 0.2165;  Loss pred: 0.2165; Loss self: 0.0000; time: 0.13s
Val loss: 0.3831 score: 0.9388 time: 0.09s
Test loss: 0.4152 score: 0.8776 time: 0.09s
Epoch 63/1000, LR 0.000268
Train loss: 0.2165;  Loss pred: 0.2165; Loss self: 0.0000; time: 0.13s
Val loss: 0.3801 score: 0.9388 time: 0.09s
Test loss: 0.4127 score: 0.8776 time: 0.09s
Epoch 64/1000, LR 0.000268
Train loss: 0.2150;  Loss pred: 0.2150; Loss self: 0.0000; time: 0.13s
Val loss: 0.3777 score: 0.9388 time: 0.09s
Test loss: 0.4108 score: 0.8571 time: 0.09s
Epoch 65/1000, LR 0.000268
Train loss: 0.1971;  Loss pred: 0.1971; Loss self: 0.0000; time: 0.13s
Val loss: 0.3746 score: 0.9388 time: 0.09s
Test loss: 0.4082 score: 0.8571 time: 0.09s
Epoch 66/1000, LR 0.000268
Train loss: 0.2068;  Loss pred: 0.2068; Loss self: 0.0000; time: 0.14s
Val loss: 0.3710 score: 0.9388 time: 0.09s
Test loss: 0.4051 score: 0.8571 time: 0.09s
Epoch 67/1000, LR 0.000268
Train loss: 0.1975;  Loss pred: 0.1975; Loss self: 0.0000; time: 0.13s
Val loss: 0.3679 score: 0.9388 time: 0.09s
Test loss: 0.4024 score: 0.8571 time: 0.09s
Epoch 68/1000, LR 0.000268
Train loss: 0.1985;  Loss pred: 0.1985; Loss self: 0.0000; time: 0.13s
Val loss: 0.3643 score: 0.9184 time: 0.09s
Test loss: 0.3993 score: 0.8571 time: 0.09s
Epoch 69/1000, LR 0.000268
Train loss: 0.1871;  Loss pred: 0.1871; Loss self: 0.0000; time: 0.13s
Val loss: 0.3588 score: 0.9388 time: 0.09s
Test loss: 0.3949 score: 0.8571 time: 0.09s
Epoch 70/1000, LR 0.000268
Train loss: 0.1813;  Loss pred: 0.1813; Loss self: 0.0000; time: 0.13s
Val loss: 0.3519 score: 0.9388 time: 0.09s
Test loss: 0.3893 score: 0.8776 time: 0.09s
Epoch 71/1000, LR 0.000268
Train loss: 0.1913;  Loss pred: 0.1913; Loss self: 0.0000; time: 0.13s
Val loss: 0.3450 score: 0.9388 time: 0.09s
Test loss: 0.3838 score: 0.8776 time: 0.09s
Epoch 72/1000, LR 0.000267
Train loss: 0.1844;  Loss pred: 0.1844; Loss self: 0.0000; time: 0.13s
Val loss: 0.3384 score: 0.9388 time: 0.09s
Test loss: 0.3789 score: 0.8980 time: 0.09s
Epoch 73/1000, LR 0.000267
Train loss: 0.1797;  Loss pred: 0.1797; Loss self: 0.0000; time: 0.14s
Val loss: 0.3320 score: 0.9388 time: 0.21s
Test loss: 0.3743 score: 0.8980 time: 0.09s
Epoch 74/1000, LR 0.000267
Train loss: 0.1725;  Loss pred: 0.1725; Loss self: 0.0000; time: 0.13s
Val loss: 0.3267 score: 0.9388 time: 0.08s
Test loss: 0.3706 score: 0.8980 time: 0.09s
Epoch 75/1000, LR 0.000267
Train loss: 0.1765;  Loss pred: 0.1765; Loss self: 0.0000; time: 0.13s
Val loss: 0.3219 score: 0.9184 time: 0.09s
Test loss: 0.3673 score: 0.8980 time: 0.08s
Epoch 76/1000, LR 0.000267
Train loss: 0.1708;  Loss pred: 0.1708; Loss self: 0.0000; time: 0.14s
Val loss: 0.3172 score: 0.9184 time: 0.09s
Test loss: 0.3640 score: 0.9184 time: 0.09s
Epoch 77/1000, LR 0.000267
Train loss: 0.1790;  Loss pred: 0.1790; Loss self: 0.0000; time: 0.13s
Val loss: 0.3118 score: 0.9184 time: 0.09s
Test loss: 0.3603 score: 0.9184 time: 0.08s
Epoch 78/1000, LR 0.000267
Train loss: 0.1633;  Loss pred: 0.1633; Loss self: 0.0000; time: 0.13s
Val loss: 0.3064 score: 0.9184 time: 0.09s
Test loss: 0.3564 score: 0.9184 time: 0.08s
Epoch 79/1000, LR 0.000267
Train loss: 0.1650;  Loss pred: 0.1650; Loss self: 0.0000; time: 0.13s
Val loss: 0.3017 score: 0.9184 time: 0.09s
Test loss: 0.3529 score: 0.8980 time: 0.08s
Epoch 80/1000, LR 0.000267
Train loss: 0.1684;  Loss pred: 0.1684; Loss self: 0.0000; time: 0.13s
Val loss: 0.2972 score: 0.9388 time: 0.09s
Test loss: 0.3498 score: 0.8980 time: 0.09s
Epoch 81/1000, LR 0.000267
Train loss: 0.1591;  Loss pred: 0.1591; Loss self: 0.0000; time: 0.13s
Val loss: 0.2933 score: 0.9388 time: 0.08s
Test loss: 0.3471 score: 0.8980 time: 0.08s
Epoch 82/1000, LR 0.000267
Train loss: 0.1533;  Loss pred: 0.1533; Loss self: 0.0000; time: 0.14s
Val loss: 0.2908 score: 0.9388 time: 0.10s
Test loss: 0.3454 score: 0.8980 time: 0.09s
Epoch 83/1000, LR 0.000266
Train loss: 0.1601;  Loss pred: 0.1601; Loss self: 0.0000; time: 0.15s
Val loss: 0.2894 score: 0.9388 time: 0.10s
Test loss: 0.3446 score: 0.8776 time: 0.10s
Epoch 84/1000, LR 0.000266
Train loss: 0.1465;  Loss pred: 0.1465; Loss self: 0.0000; time: 0.15s
Val loss: 0.2886 score: 0.9388 time: 0.10s
Test loss: 0.3444 score: 0.8776 time: 0.09s
Epoch 85/1000, LR 0.000266
Train loss: 0.1582;  Loss pred: 0.1582; Loss self: 0.0000; time: 0.15s
Val loss: 0.2880 score: 0.9388 time: 0.10s
Test loss: 0.3443 score: 0.8776 time: 0.09s
Epoch 86/1000, LR 0.000266
Train loss: 0.1445;  Loss pred: 0.1445; Loss self: 0.0000; time: 0.14s
Val loss: 0.2851 score: 0.9388 time: 0.10s
Test loss: 0.3421 score: 0.8776 time: 0.09s
Epoch 87/1000, LR 0.000266
Train loss: 0.1419;  Loss pred: 0.1419; Loss self: 0.0000; time: 0.15s
Val loss: 0.2814 score: 0.9388 time: 0.09s
Test loss: 0.3392 score: 0.8776 time: 0.09s
Epoch 88/1000, LR 0.000266
Train loss: 0.1334;  Loss pred: 0.1334; Loss self: 0.0000; time: 0.15s
Val loss: 0.2766 score: 0.9388 time: 0.10s
Test loss: 0.3354 score: 0.8776 time: 0.09s
Epoch 89/1000, LR 0.000266
Train loss: 0.1376;  Loss pred: 0.1376; Loss self: 0.0000; time: 0.15s
Val loss: 0.2711 score: 0.9388 time: 0.09s
Test loss: 0.3309 score: 0.8776 time: 0.09s
Epoch 90/1000, LR 0.000266
Train loss: 0.1438;  Loss pred: 0.1438; Loss self: 0.0000; time: 0.14s
Val loss: 0.2644 score: 0.9388 time: 0.11s
Test loss: 0.3251 score: 0.8776 time: 0.55s
Epoch 91/1000, LR 0.000266
Train loss: 0.1374;  Loss pred: 0.1374; Loss self: 0.0000; time: 1.42s
Val loss: 0.2583 score: 0.9388 time: 2.43s
Test loss: 0.3197 score: 0.8776 time: 2.72s
Epoch 92/1000, LR 0.000266
Train loss: 0.1403;  Loss pred: 0.1403; Loss self: 0.0000; time: 0.75s
Val loss: 0.2521 score: 0.9388 time: 0.21s
Test loss: 0.3144 score: 0.8980 time: 0.09s
Epoch 93/1000, LR 0.000265
Train loss: 0.1342;  Loss pred: 0.1342; Loss self: 0.0000; time: 0.13s
Val loss: 0.2486 score: 0.9388 time: 0.08s
Test loss: 0.3113 score: 0.8980 time: 0.08s
Epoch 94/1000, LR 0.000265
Train loss: 0.1336;  Loss pred: 0.1336; Loss self: 0.0000; time: 0.13s
Val loss: 0.2454 score: 0.9388 time: 0.09s
Test loss: 0.3081 score: 0.8980 time: 0.08s
Epoch 95/1000, LR 0.000265
Train loss: 0.1307;  Loss pred: 0.1307; Loss self: 0.0000; time: 0.14s
Val loss: 0.2439 score: 0.9388 time: 0.09s
Test loss: 0.3064 score: 0.8776 time: 0.09s
Epoch 96/1000, LR 0.000265
Train loss: 0.1339;  Loss pred: 0.1339; Loss self: 0.0000; time: 0.14s
Val loss: 0.2436 score: 0.9388 time: 0.08s
Test loss: 0.3060 score: 0.8776 time: 0.08s
Epoch 97/1000, LR 0.000265
Train loss: 0.1292;  Loss pred: 0.1292; Loss self: 0.0000; time: 0.13s
Val loss: 0.2449 score: 0.9388 time: 0.08s
Test loss: 0.3069 score: 0.8776 time: 0.08s
     INFO: Early stopping counter 1 of 20
Epoch 98/1000, LR 0.000265
Train loss: 0.1192;  Loss pred: 0.1192; Loss self: 0.0000; time: 0.13s
Val loss: 0.2443 score: 0.9388 time: 0.17s
Test loss: 0.3063 score: 0.8776 time: 0.08s
     INFO: Early stopping counter 2 of 20
Epoch 99/1000, LR 0.000265
Train loss: 0.1233;  Loss pred: 0.1233; Loss self: 0.0000; time: 0.13s
Val loss: 0.2410 score: 0.9388 time: 0.08s
Test loss: 0.3033 score: 0.8776 time: 0.08s
Epoch 100/1000, LR 0.000265
Train loss: 0.1174;  Loss pred: 0.1174; Loss self: 0.0000; time: 0.13s
Val loss: 0.2391 score: 0.9388 time: 0.09s
Test loss: 0.3015 score: 0.8776 time: 0.08s
Epoch 101/1000, LR 0.000265
Train loss: 0.1183;  Loss pred: 0.1183; Loss self: 0.0000; time: 0.13s
Val loss: 0.2386 score: 0.9388 time: 0.09s
Test loss: 0.3008 score: 0.8776 time: 0.09s
Epoch 102/1000, LR 0.000264
Train loss: 0.1087;  Loss pred: 0.1087; Loss self: 0.0000; time: 0.12s
Val loss: 0.2351 score: 0.9388 time: 0.08s
Test loss: 0.2976 score: 0.8776 time: 0.08s
Epoch 103/1000, LR 0.000264
Train loss: 0.1244;  Loss pred: 0.1244; Loss self: 0.0000; time: 0.13s
Val loss: 0.2326 score: 0.9388 time: 0.08s
Test loss: 0.2953 score: 0.8776 time: 0.08s
Epoch 104/1000, LR 0.000264
Train loss: 0.1100;  Loss pred: 0.1100; Loss self: 0.0000; time: 0.13s
Val loss: 0.2298 score: 0.9388 time: 0.09s
Test loss: 0.2925 score: 0.8980 time: 0.08s
Epoch 105/1000, LR 0.000264
Train loss: 0.1132;  Loss pred: 0.1132; Loss self: 0.0000; time: 0.13s
Val loss: 0.2292 score: 0.9388 time: 0.09s
Test loss: 0.2917 score: 0.8980 time: 0.09s
Epoch 106/1000, LR 0.000264
Train loss: 0.1082;  Loss pred: 0.1082; Loss self: 0.0000; time: 0.14s
Val loss: 0.2293 score: 0.9388 time: 0.17s
Test loss: 0.2915 score: 0.8980 time: 0.08s
     INFO: Early stopping counter 1 of 20
Epoch 107/1000, LR 0.000264
Train loss: 0.1141;  Loss pred: 0.1141; Loss self: 0.0000; time: 0.13s
Val loss: 0.2302 score: 0.9184 time: 0.09s
Test loss: 0.2920 score: 0.8776 time: 0.08s
     INFO: Early stopping counter 2 of 20
Epoch 108/1000, LR 0.000264
Train loss: 0.1043;  Loss pred: 0.1043; Loss self: 0.0000; time: 0.14s
Val loss: 0.2313 score: 0.9184 time: 0.09s
Test loss: 0.2926 score: 0.8776 time: 0.08s
     INFO: Early stopping counter 3 of 20
Epoch 109/1000, LR 0.000264
Train loss: 0.1054;  Loss pred: 0.1054; Loss self: 0.0000; time: 0.13s
Val loss: 0.2335 score: 0.9184 time: 0.09s
Test loss: 0.2939 score: 0.8776 time: 0.08s
     INFO: Early stopping counter 4 of 20
Epoch 110/1000, LR 0.000263
Train loss: 0.0915;  Loss pred: 0.0915; Loss self: 0.0000; time: 0.14s
Val loss: 0.2369 score: 0.9184 time: 0.09s
Test loss: 0.2963 score: 0.8571 time: 0.09s
     INFO: Early stopping counter 5 of 20
Epoch 111/1000, LR 0.000263
Train loss: 0.1107;  Loss pred: 0.1107; Loss self: 0.0000; time: 0.14s
Val loss: 0.2419 score: 0.9184 time: 0.09s
Test loss: 0.3002 score: 0.8571 time: 0.09s
     INFO: Early stopping counter 6 of 20
Epoch 112/1000, LR 0.000263
Train loss: 0.0948;  Loss pred: 0.0948; Loss self: 0.0000; time: 0.14s
Val loss: 0.2437 score: 0.9184 time: 0.09s
Test loss: 0.3015 score: 0.8571 time: 0.09s
     INFO: Early stopping counter 7 of 20
Epoch 113/1000, LR 0.000263
Train loss: 0.0979;  Loss pred: 0.0979; Loss self: 0.0000; time: 0.14s
Val loss: 0.2427 score: 0.9184 time: 0.09s
Test loss: 0.3004 score: 0.8571 time: 0.09s
     INFO: Early stopping counter 8 of 20
Epoch 114/1000, LR 0.000263
Train loss: 0.0985;  Loss pred: 0.0985; Loss self: 0.0000; time: 0.14s
Val loss: 0.2397 score: 0.9184 time: 0.20s
Test loss: 0.2975 score: 0.8571 time: 0.09s
     INFO: Early stopping counter 9 of 20
Epoch 115/1000, LR 0.000263
Train loss: 0.0945;  Loss pred: 0.0945; Loss self: 0.0000; time: 0.14s
Val loss: 0.2338 score: 0.9184 time: 0.09s
Test loss: 0.2922 score: 0.8571 time: 0.08s
     INFO: Early stopping counter 10 of 20
Epoch 116/1000, LR 0.000263
Train loss: 0.0919;  Loss pred: 0.0919; Loss self: 0.0000; time: 0.14s
Val loss: 0.2308 score: 0.9184 time: 0.10s
Test loss: 0.2896 score: 0.8776 time: 0.10s
     INFO: Early stopping counter 11 of 20
Epoch 117/1000, LR 0.000262
Train loss: 0.0886;  Loss pred: 0.0886; Loss self: 0.0000; time: 0.13s
Val loss: 0.2263 score: 0.9184 time: 0.10s
Test loss: 0.2857 score: 0.8776 time: 0.09s
Epoch 118/1000, LR 0.000262
Train loss: 0.0850;  Loss pred: 0.0850; Loss self: 0.0000; time: 0.14s
Val loss: 0.2219 score: 0.9184 time: 0.09s
Test loss: 0.2818 score: 0.8980 time: 0.08s
Epoch 119/1000, LR 0.000262
Train loss: 0.0880;  Loss pred: 0.0880; Loss self: 0.0000; time: 0.14s
Val loss: 0.2187 score: 0.9184 time: 0.09s
Test loss: 0.2793 score: 0.8980 time: 0.08s
Epoch 120/1000, LR 0.000262
Train loss: 0.0969;  Loss pred: 0.0969; Loss self: 0.0000; time: 0.13s
Val loss: 0.2176 score: 0.9184 time: 0.09s
Test loss: 0.2782 score: 0.8980 time: 0.09s
Epoch 121/1000, LR 0.000262
Train loss: 0.0974;  Loss pred: 0.0974; Loss self: 0.0000; time: 0.14s
Val loss: 0.2226 score: 0.9184 time: 0.09s
Test loss: 0.2823 score: 0.8980 time: 0.21s
     INFO: Early stopping counter 1 of 20
Epoch 122/1000, LR 0.000262
Train loss: 0.0880;  Loss pred: 0.0880; Loss self: 0.0000; time: 0.13s
Val loss: 0.2280 score: 0.9184 time: 0.09s
Test loss: 0.2869 score: 0.8776 time: 0.08s
     INFO: Early stopping counter 2 of 20
Epoch 123/1000, LR 0.000262
Train loss: 0.0803;  Loss pred: 0.0803; Loss self: 0.0000; time: 0.13s
Val loss: 0.2297 score: 0.9184 time: 0.09s
Test loss: 0.2884 score: 0.8776 time: 0.09s
     INFO: Early stopping counter 3 of 20
Epoch 124/1000, LR 0.000261
Train loss: 0.0813;  Loss pred: 0.0813; Loss self: 0.0000; time: 0.13s
Val loss: 0.2325 score: 0.9184 time: 0.10s
Test loss: 0.2910 score: 0.8571 time: 0.08s
     INFO: Early stopping counter 4 of 20
Epoch 125/1000, LR 0.000261
Train loss: 0.0819;  Loss pred: 0.0819; Loss self: 0.0000; time: 0.14s
Val loss: 0.2377 score: 0.9184 time: 0.08s
Test loss: 0.2959 score: 0.8571 time: 0.08s
     INFO: Early stopping counter 5 of 20
Epoch 126/1000, LR 0.000261
Train loss: 0.0778;  Loss pred: 0.0778; Loss self: 0.0000; time: 0.13s
Val loss: 0.2386 score: 0.9184 time: 0.08s
Test loss: 0.2972 score: 0.8571 time: 0.08s
     INFO: Early stopping counter 6 of 20
Epoch 127/1000, LR 0.000261
Train loss: 0.0797;  Loss pred: 0.0797; Loss self: 0.0000; time: 0.13s
Val loss: 0.2396 score: 0.9184 time: 0.08s
Test loss: 0.2986 score: 0.8571 time: 0.08s
     INFO: Early stopping counter 7 of 20
Epoch 128/1000, LR 0.000261
Train loss: 0.0823;  Loss pred: 0.0823; Loss self: 0.0000; time: 0.13s
Val loss: 0.2408 score: 0.9184 time: 0.08s
Test loss: 0.3002 score: 0.8571 time: 0.08s
     INFO: Early stopping counter 8 of 20
Epoch 129/1000, LR 0.000261
Train loss: 0.0824;  Loss pred: 0.0824; Loss self: 0.0000; time: 0.13s
Val loss: 0.2443 score: 0.9184 time: 0.19s
Test loss: 0.3040 score: 0.8571 time: 0.08s
     INFO: Early stopping counter 9 of 20
Epoch 130/1000, LR 0.000260
Train loss: 0.0792;  Loss pred: 0.0792; Loss self: 0.0000; time: 0.13s
Val loss: 0.2349 score: 0.9184 time: 0.09s
Test loss: 0.2956 score: 0.8776 time: 0.09s
     INFO: Early stopping counter 10 of 20
Epoch 131/1000, LR 0.000260
Train loss: 0.0759;  Loss pred: 0.0759; Loss self: 0.0000; time: 0.14s
Val loss: 0.2280 score: 0.9184 time: 0.09s
Test loss: 0.2896 score: 0.8776 time: 0.09s
     INFO: Early stopping counter 11 of 20
Epoch 132/1000, LR 0.000260
Train loss: 0.0839;  Loss pred: 0.0839; Loss self: 0.0000; time: 0.14s
Val loss: 0.2295 score: 0.9184 time: 0.09s
Test loss: 0.2910 score: 0.8776 time: 0.09s
     INFO: Early stopping counter 12 of 20
Epoch 133/1000, LR 0.000260
Train loss: 0.0698;  Loss pred: 0.0698; Loss self: 0.0000; time: 0.14s
Val loss: 0.2287 score: 0.9184 time: 0.10s
Test loss: 0.2906 score: 0.8980 time: 0.09s
     INFO: Early stopping counter 13 of 20
Epoch 134/1000, LR 0.000260
Train loss: 0.0782;  Loss pred: 0.0782; Loss self: 0.0000; time: 0.13s
Val loss: 0.2292 score: 0.9184 time: 0.10s
Test loss: 0.2914 score: 0.8980 time: 0.09s
     INFO: Early stopping counter 14 of 20
Epoch 135/1000, LR 0.000260
Train loss: 0.0633;  Loss pred: 0.0633; Loss self: 0.0000; time: 0.13s
Val loss: 0.2302 score: 0.9184 time: 0.09s
Test loss: 0.2925 score: 0.8980 time: 0.09s
     INFO: Early stopping counter 15 of 20
Epoch 136/1000, LR 0.000260
Train loss: 0.0676;  Loss pred: 0.0676; Loss self: 0.0000; time: 0.13s
Val loss: 0.2299 score: 0.9184 time: 0.09s
Test loss: 0.2925 score: 0.8980 time: 0.09s
     INFO: Early stopping counter 16 of 20
Epoch 137/1000, LR 0.000259
Train loss: 0.0683;  Loss pred: 0.0683; Loss self: 0.0000; time: 0.15s
Val loss: 0.2278 score: 0.9184 time: 0.12s
Test loss: 0.2909 score: 0.8980 time: 0.09s
     INFO: Early stopping counter 17 of 20
Epoch 138/1000, LR 0.000259
Train loss: 0.0600;  Loss pred: 0.0600; Loss self: 0.0000; time: 0.13s
Val loss: 0.2223 score: 0.9184 time: 0.09s
Test loss: 0.2858 score: 0.8980 time: 0.09s
     INFO: Early stopping counter 18 of 20
Epoch 139/1000, LR 0.000259
Train loss: 0.0665;  Loss pred: 0.0665; Loss self: 0.0000; time: 0.13s
Val loss: 0.2204 score: 0.9184 time: 0.09s
Test loss: 0.2839 score: 0.8980 time: 0.09s
     INFO: Early stopping counter 19 of 20
Epoch 140/1000, LR 0.000259
Train loss: 0.0702;  Loss pred: 0.0702; Loss self: 0.0000; time: 0.13s
Val loss: 0.2210 score: 0.9184 time: 0.09s
Test loss: 0.2842 score: 0.8980 time: 0.09s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 119,   Train_Loss: 0.0969,   Val_Loss: 0.2176,   Val_Precision: 0.8621,   Val_Recall: 1.0000,   Val_accuracy: 0.9259,   Val_Score: 0.9184,   Val_Loss: 0.2176,   Test_Precision: 0.8800,   Test_Recall: 0.9167,   Test_accuracy: 0.8980,   Test_Score: 0.8980,   Test_loss: 0.2782


[0.0923724400345236, 0.09272063802927732, 0.08889736793935299, 0.09417051286436617, 0.09018707810901105, 0.08776998892426491, 0.08725390210747719, 0.20725050591863692, 0.08858782309107482, 0.08919495297595859, 0.08934879209846258, 0.08880400890484452, 0.0894871880300343, 0.08816483709961176, 0.0875755709130317, 0.0855906531214714, 0.08672121609561145, 0.09077785513363779, 0.08842640393413603, 0.08714691898785532, 0.08716778503730893, 0.09772477601654828, 0.08830070099793375, 0.08922215108759701, 0.08772663702256978, 0.08844832889735699, 0.0913308730814606, 0.09526202897541225, 0.10000373213551939, 0.09139185608364642, 0.0877063418738544, 0.08642259007319808, 0.08876497694291174, 0.08700242708437145, 0.0867588750552386, 0.0852962601929903, 0.08861434808932245, 0.08640759508125484, 0.08718537003733218, 0.08595339697785676, 0.0881145850289613, 0.09242727211676538, 0.08877051109448075, 0.0899763721972704, 0.0965567990206182, 0.08937351894564927, 0.08996126800775528, 0.08943881699815392, 0.08976403111591935, 0.0875426169950515, 0.09064319496974349, 0.09473756817169487, 0.08808105695061386, 0.08961903001181781, 0.09991109208203852, 0.08954954100772738, 0.08896022709086537, 0.08985830983147025, 0.08667816501110792, 0.08882927102968097, 0.08979957597330213, 0.08840710297226906, 0.07976822415366769, 0.09475376293994486, 0.08674033009447157, 0.08774955198168755, 0.09162684483453631, 0.08909940300509334, 0.0891390519682318, 0.08898501098155975, 0.08847574098035693, 0.08648484176956117, 0.08646886493079364, 0.08767650602385402, 0.09117117198184133, 0.08694075886160135, 0.08676877897232771, 0.08838087599724531, 0.08881456102244556, 0.08996102889068425, 0.09085428318940103, 0.0901144880335778, 0.09139849292114377, 0.09101863391697407, 0.08963741804473102, 0.09346674289554358, 0.089841578155756, 0.08780689095146954, 0.09459696407429874, 0.0944067221134901, 0.08965900586917996, 0.09251349093392491, 0.08970654499717057, 0.09053152496926486, 0.08625785913318396, 0.08695084089413285, 0.08992516784928739, 0.08656944101676345, 0.09050293290056288, 0.08631433197297156, 0.08718661894090474, 0.08699646894820035, 0.08878183807246387, 0.09023214387707412, 0.10148900700733066, 0.11179829691536725, 0.1014670068398118, 0.23500279593281448, 0.10184356686659157, 0.10023184306919575, 0.09936410398222506, 0.08917102008126676, 0.08813585084863007, 0.08465180802159011, 0.09733205405063927, 0.09464075113646686, 0.0924620651639998, 0.09749532910063863, 0.09290332300588489, 0.08715918194502592, 0.08909616083838046, 0.09065122087486088, 0.08670487604103982, 0.0856805460061878, 0.09604316693730652, 0.09002745500765741, 0.08653434994630516, 0.09356915997341275, 0.23805228690616786, 0.10475258296355605, 0.08832069882191718, 0.09451443795114756, 0.08938370505347848, 0.08985678991302848, 0.09041765797883272, 0.09007414802908897, 0.08911701291799545, 0.09512634295970201, 0.08992218296043575, 0.08947149803861976, 0.10258375410921872, 0.09410705394111574, 0.0914903711527586, 0.08903187606483698, 0.08665797905996442, 0.08603515103459358, 0.08707149582915008, 0.08908553118817508, 0.08840600308030844, 0.0916309270542115, 0.08849171316251159, 0.0909760519862175, 0.16866309591569006, 0.09263469208963215, 0.09038714994676411, 0.10537788621149957, 0.09338978817686439, 0.09560600109398365, 0.09504669206216931, 0.08627667208202183, 0.08581839897669852, 0.0884807191323489, 0.08985734987072647, 0.08865122590214014, 0.08488967618905008, 0.10026020486839116, 0.09081005211919546, 0.0901593950111419, 0.18782902183011174, 0.08628835203126073, 0.08757788385264575, 0.08853289508260787, 0.08661541901528835, 0.08935973816551268, 0.08666541101410985, 0.08494064304977655, 0.0854553859680891, 0.08795131300576031, 0.08939407579600811, 0.08841505693271756, 0.09368856810033321, 0.08401955291628838, 0.10370001499541104, 0.09022161597386003, 0.09257938200607896, 0.08985321805812418, 0.09044827404431999, 0.0897187979426235, 0.09142533596605062, 0.09127300907857716, 0.09015703783370554, 0.0908733441028744, 0.09184172097593546, 0.09203068003989756, 0.09210269805043936, 0.090764642925933, 0.09244918404147029, 0.0923298648558557, 0.09192892699502409, 0.09144886396825314, 0.09264862490817904, 0.0923298851121217, 0.09398985607549548, 0.0895528809633106, 0.09013208793476224, 0.0882149210665375, 0.08907083095982671, 0.08856367995031178, 0.09030620404519141, 0.08784094615839422, 0.10179581516422331, 0.10262293391861022, 0.10003199405036867, 0.09984514699317515, 0.10123768704943359, 0.09926067083142698, 0.09932352206669748, 0.09942462295293808, 0.5569795970804989, 2.7246354471426457, 0.09131729300133884, 0.08823105390183628, 0.09012799407355487, 0.09088687296025455, 0.08814184297807515, 0.08781369193457067, 0.08671797788701952, 0.08712142705917358, 0.08779424196109176, 0.0986485870089382, 0.0892530819401145, 0.08841069019399583, 0.09005871694535017, 0.09048103704117239, 0.08919897116720676, 0.08833276317454875, 0.08915791194885969, 0.08926596399396658, 0.09173791809007525, 0.09480129112489522, 0.09638373996131122, 0.09612559899687767, 0.09616988408379257, 0.08894405397586524, 0.10091914003714919, 0.10011113598011434, 0.08912738994695246, 0.09004595410078764, 0.0904825848992914, 0.2178799589164555, 0.08986485889181495, 0.09043143410235643, 0.08958013798110187, 0.08721459493972361, 0.08684592810459435, 0.08736448595300317, 0.08789149206131697, 0.08748801914043725, 0.0937425410374999, 0.09378761192783713, 0.09584449208341539, 0.09613060601986945, 0.09268487012013793, 0.09268352505750954, 0.09338273108005524, 0.09456400689668953, 0.09126216592267156, 0.09358339011669159, 0.0931904399767518]
[0.0018851518374392573, 0.0018922579189648433, 0.0018142319987623058, 0.0019218472013135953, 0.0018405526144696133, 0.0017912242637605083, 0.0017806918797444325, 0.004229602161604835, 0.0018079147569607108, 0.001820305162774665, 0.001823444736703318, 0.0018123267123437657, 0.0018262691434700877, 0.001799282389787995, 0.0017872565492455447, 0.0017467480228871715, 0.0017698207366451317, 0.0018526092884415875, 0.0018046204884517559, 0.0017785085507725574, 0.0017789343885165087, 0.0019943831840111893, 0.0018020551224068111, 0.0018208602262774901, 0.0017903395310728525, 0.001805067936680755, 0.0018638953690093998, 0.0019441230403145356, 0.0020408924925616203, 0.0018651399200744166, 0.0017899253443643755, 0.0017637263280244507, 0.0018115301416920765, 0.0017755597364157438, 0.0017705892868416042, 0.0017407400039385777, 0.0018084560834555601, 0.001763420307780711, 0.0017792932660680037, 0.0017541509587317705, 0.0017982568373257409, 0.001886270859525824, 0.0018116430835608318, 0.0018362524938218448, 0.0019705469187881264, 0.00182394936623774, 0.0018359442450562302, 0.0018252819795541617, 0.001831919002365701, 0.0017865840203071736, 0.0018498611218314997, 0.0019334197586060179, 0.0017975725908288543, 0.0018289597961595472, 0.002039001879225276, 0.0018275416532189262, 0.0018155148385890893, 0.0018338430577851071, 0.001768942143083835, 0.0018128422659118564, 0.0018326444076184108, 0.001804226591270797, 0.0016279229419115856, 0.0019337502640805074, 0.001770210818254522, 0.0017908071832997458, 0.001869935608868088, 0.0018183551633692517, 0.0018191643258822815, 0.0018160206322767297, 0.0018056273669460599, 0.0017649967708073709, 0.0017646707128733397, 0.0017893164494664085, 0.0018606361628947209, 0.0017743012012571705, 0.0017707914075985247, 0.0018036913468825575, 0.0018125420616825624, 0.0018359393651160051, 0.0018541690446816537, 0.0018390711843587306, 0.001865275365737628, 0.001857523141162736, 0.0018293350621373678, 0.0019074845488886445, 0.0018335015950154286, 0.0017919773663565212, 0.0019305502872305866, 0.001926667798234492, 0.0018297756299832646, 0.0018880304272229575, 0.001830745816268787, 0.0018475821422298951, 0.001760364472105795, 0.0017745069570231195, 0.0018352075071283141, 0.0017667232860563969, 0.0018469986306237324, 0.001761516979040236, 0.001779318753896015, 0.0017754381418000071, 0.0018118742463768137, 0.0018414723240219209, 0.002071204224639401, 0.0022815978962319847, 0.002070755241628812, 0.004795975427200295, 0.002078440140134522, 0.002045547817738689, 0.002027838856780103, 0.0018198167363523829, 0.0017986908336455117, 0.0017275879188079614, 0.0019863684500130464, 0.0019314439007442218, 0.0018869809217142816, 0.0019897005938905843, 0.001895986183793569, 0.0017787588152046107, 0.001818288996701642, 0.0018500249158134874, 0.00176948726614367, 0.001748582571554853, 0.0019600646313736023, 0.0018372950001562737, 0.0017660071417613297, 0.0019095746933349542, 0.004858209936860569, 0.0021378078155827765, 0.0018024632412636159, 0.0019288660806356644, 0.0018241572459893568, 0.0018338120390413975, 0.001845258326098627, 0.0018382479189609994, 0.001818714549346846, 0.0019413539379531024, 0.001835146591029301, 0.0018259489395636686, 0.0020935460022289535, 0.00192055212124726, 0.001867150431688951, 0.0018169770625476934, 0.001768530184897233, 0.0017558194088692569, 0.001776969302635716, 0.0018180720650647975, 0.0018042041444960907, 0.001870018919473704, 0.0018059533298471753, 0.001856654122167704, 0.003442103998279389, 0.0018905039201965745, 0.0018446357131992675, 0.002150569106357134, 0.0019059140444258038, 0.0019511428794690541, 0.0019397284094320268, 0.0017607484098371802, 0.0017513958974836432, 0.0018057289618846712, 0.0018338234667495197, 0.0018092086918804111, 0.0017324423712051036, 0.0020461266299671666, 0.0018532663697794992, 0.0018399876532886102, 0.003833245343471668, 0.001760986776148178, 0.0017873037520948114, 0.0018067937771960789, 0.0017676616125569052, 0.0018236681258267894, 0.0017686818574308132, 0.0017334825112199296, 0.0017439874687365123, 0.0017949247552195983, 0.0018243688937960838, 0.001804388916994236, 0.0019120115938843513, 0.0017146847533936404, 0.0021163268366410416, 0.0018412574688542861, 0.0018893751429812033, 0.0018337391440433506, 0.0018458831437616324, 0.0018309958763800714, 0.001865823182980625, 0.0018627144709913706, 0.0018399395476266438, 0.001854558042915804, 0.0018743208362435807, 0.0018781771436713788, 0.0018796468989885583, 0.0018523396515496531, 0.001886718041662659, 0.0018842829562419532, 0.0018761005509188588, 0.0018663033462908802, 0.0018907882634322255, 0.0018842833696351368, 0.0019181603280713363, 0.0018276098155777674, 0.001839430366015556, 0.0018003045115619898, 0.0018177720604046267, 0.0018074220398022812, 0.0018429837560243144, 0.0017926723705794737, 0.0020774656155963943, 0.0020943455901757187, 0.0020414692663340544, 0.0020376560610852073, 0.002066075245906808, 0.0020257279761515708, 0.0020270106544223974, 0.0020290739378150627, 0.011366930552663244, 0.055604805043727466, 0.0018636182245171191, 0.0018006337530986996, 0.0018393468178276504, 0.0018548341420460113, 0.0017988131220015337, 0.0017921161619300137, 0.0017697546507555004, 0.0017779883073300732, 0.00179171922369575, 0.0020132364695701673, 0.001821491468165602, 0.001804299799877466, 0.0018379329988846974, 0.0018465517763504569, 0.0018203871666776891, 0.001802709452541811, 0.0018195492234461162, 0.0018217543672238077, 0.0018722024100015359, 0.0019347202270386778, 0.0019670151012512495, 0.0019617469183036257, 0.0019626506955876034, 0.001815184775017658, 0.0020595742864724324, 0.0020430844077574356, 0.0018189263254480095, 0.0018376725326691354, 0.0018465833652916612, 0.004446529773805215, 0.001833976712077856, 0.001845539471476662, 0.001828166081246977, 0.0017798896926474205, 0.001772365879685599, 0.001782948692918432, 0.0017937039196187137, 0.0017854697783762704, 0.0019131130823979573, 0.001914032896486472, 0.0019560100425186815, 0.001961849102446315, 0.001891527961635468, 0.0018915005113777457, 0.0019057700220419436, 0.0019298776917691743, 0.001862493182095338, 0.0019098651044222774, 0.0019018457138112613]
[530.4612499321937, 528.4691848704474, 551.1974216540187, 520.3327295304712, 543.3150849035452, 558.2773861607889, 561.5794688430441, 236.42885590463447, 553.1234236292767, 549.3584375027066, 548.4125621530718, 551.7768916547967, 547.5644176410402, 555.7771285239041, 559.5167635122839, 572.4924184239901, 565.0289768304997, 539.7792217921992, 554.1331301507795, 562.268873863786, 562.1342790691236, 501.4081586813006, 554.9219818894372, 549.1909733480031, 558.553270284299, 553.9957691779999, 536.5108023909452, 514.370736452057, 489.9817132184425, 536.15280507218, 558.6825188818768, 566.9813871407702, 552.0195203961337, 563.2026788457496, 564.7837177326485, 574.4683282612061, 552.9578567864478, 567.0797798957606, 562.0208984491152, 570.0763637372394, 556.094090256395, 530.1465560738094, 551.9851062685449, 544.5874156002759, 507.4733265498664, 548.2608336122289, 544.6788499665863, 547.8605559039471, 545.8756630116404, 559.7273840096625, 540.581121576265, 517.2182582436175, 556.3057676234947, 546.7588746892095, 490.43603646895735, 547.1831507854597, 550.8079464539882, 545.3029340513946, 565.3096139462641, 551.6199720205673, 545.6595921407018, 554.2541080140358, 614.279689937751, 517.1298582733469, 564.9044676983888, 558.4074094215981, 534.777772698452, 549.9475680796529, 549.7029519392138, 550.6545367528719, 553.8241268968723, 566.5732745463127, 566.6779602024118, 558.8726355800337, 537.4505881065057, 563.6021659070376, 564.7192524816683, 554.4185826074778, 551.7113346719862, 544.6802977269429, 539.325150998674, 543.7527424196427, 536.113872712058, 538.3513011708912, 546.6467137144439, 524.2506423355454, 545.404488721803, 558.0427625786463, 517.9870250541468, 519.0308370318709, 546.5150937708937, 529.6524810094652, 546.2254733090602, 541.2479245946131, 568.064179802364, 563.5368157009549, 544.8975094727978, 566.0195956505202, 541.418917924319, 567.6925127028017, 562.0128477881715, 563.241250965895, 551.9146828206702, 543.0437302559732, 482.8109116927381, 438.28932418437137, 482.9155951880731, 208.50815755404344, 481.13004588877766, 488.866596677989, 493.1358311122642, 549.5058815671665, 555.9599133405499, 578.8417417794874, 503.4312742902416, 517.7473700451155, 529.9470643781186, 502.58817988521497, 527.4300037351315, 562.189764824845, 549.9675804088293, 540.5332606346456, 565.1354599343057, 571.891780386895, 510.1872581105683, 544.2784092456267, 566.2491256986926, 523.6768184508991, 205.83713198820953, 467.76889517891266, 554.7963348750184, 518.439310037764, 548.1983541707427, 545.3121577949382, 541.9295422523682, 543.9962638800171, 549.8388960263883, 515.104422975218, 544.9155968729004, 547.6604401867675, 477.6584794102071, 520.6836039162386, 535.575486060563, 550.3646802221265, 565.4412961337772, 569.5346542751782, 562.7559229733092, 550.0332023221308, 554.2610036955089, 534.753947987563, 553.7241652222667, 538.6032799865101, 290.51998443390204, 528.9594955698478, 542.1124576763383, 464.993195077515, 524.6826334716845, 512.5201288550024, 515.536090071914, 567.9403112977805, 570.9731314529012, 553.7929673323079, 545.3087596116956, 552.7278331614937, 577.2197774777295, 488.72830515677646, 539.5878413953951, 543.4819077251415, 260.87555332274314, 567.8634351742884, 559.5019866253562, 553.466595148383, 565.7191358890856, 548.3453846881454, 565.3928069645038, 576.8734287929187, 573.3986154868888, 557.1264183036219, 548.1347568469196, 554.2042464247714, 523.009380904667, 583.1975807919428, 472.51680727498797, 543.1070976848475, 529.2755140316508, 545.3338351031892, 541.7461031483014, 546.1508749965222, 535.9564663584653, 536.8509321065089, 543.4961171903228, 539.2120261859064, 533.5265876914378, 532.4311412102713, 532.0148164732972, 539.8577950665839, 530.0209029213269, 530.7058564040814, 533.0204713762433, 535.8185752532755, 528.879948823442, 530.7057399724517, 521.332854905552, 547.1627430956138, 543.6465649776835, 555.4615864026107, 550.1239796684988, 553.2742093315364, 542.5983797910409, 557.8264140238599, 481.35574061615563, 477.47611697460997, 489.843279294495, 490.75995654900845, 484.0094773804312, 493.6496961945383, 493.33731809365, 492.83566328628467, 87.97449719314969, 17.984057298889958, 536.5905885896289, 555.3600215919013, 543.6712588988759, 539.1317624210488, 555.9221176279297, 557.9995433571969, 565.0500760504314, 562.433395021397, 558.1231628119256, 496.712639133496, 549.000650004189, 554.2316194170793, 544.0894747560571, 541.5499380019605, 549.3336902748316, 554.7205616468062, 549.5866707612672, 548.9214232124561, 534.1302813509249, 516.8705976318965, 508.3845057233593, 509.74974940433515, 509.51501571226214, 550.9081024493895, 485.5372328971757, 489.45603823467906, 549.774878734407, 544.166592372987, 541.5406738715279, 224.89447971113623, 545.2631941367574, 541.8469859113201, 546.996255021813, 561.8325698108813, 564.2175870466377, 560.8686351838554, 557.5056111894818, 560.076688001638, 522.7082545202022, 522.4570600827537, 511.24481892349445, 509.7231987684763, 528.6731257915807, 528.6807981202248, 524.7222846587474, 518.1675524127497, 536.9147171185788, 523.5971889765974, 525.8050075976035]
Elapsed: 0.10506940498466856~0.16351574912796032
Time per graph: 0.0021442735711156846~0.0033370561046522523
Speed: 529.8222094140425~65.69409191498251
Total Time: 0.0943
best val loss: 0.21755175292491913 test_score: 0.8980

Testing...
Test loss: 0.4890 score: 0.8980 time: 0.09s
test Score 0.8980
Epoch Time List: [0.3944286396726966, 0.3265792690217495, 0.33373961318284273, 0.33247716003097594, 0.3256023209542036, 0.313202099641785, 0.31486809928901494, 0.42849059915170074, 0.3136003268882632, 0.3209924118127674, 0.31533136311918497, 0.3194416519254446, 0.3189978750888258, 0.32083444483578205, 0.3197097971569747, 0.30616185907274485, 0.3118660179898143, 0.3370774262584746, 0.329371121712029, 0.33165341592393816, 0.32496556430123746, 0.3262148310896009, 0.3068312150426209, 0.32139254128560424, 0.3119534910656512, 0.31794219207949936, 0.3295165719464421, 0.31241299910470843, 0.3244255380704999, 0.33130609313957393, 0.3108803366776556, 0.31296571413986385, 0.3028184117283672, 0.30762363923713565, 0.30715922615490854, 0.3043460950721055, 0.30791961890645325, 0.30012724408879876, 0.29813182004727423, 0.30006941594183445, 0.31553454301320016, 0.3219171760138124, 0.3171825523022562, 0.30888749985024333, 0.31494595808908343, 0.30955043877474964, 0.31979612191207707, 0.3159781419672072, 0.3313588828314096, 0.3271702993661165, 0.3325364328920841, 0.33099319390021265, 0.32596855401061475, 0.31559884105809033, 0.4111499229911715, 0.3202768031042069, 0.31779479375109076, 0.3148329029791057, 0.3130052969790995, 0.3137565969955176, 0.318698224844411, 0.3123018878977746, 0.4069000561721623, 0.3053554801736027, 0.30579477408900857, 0.3017322460655123, 0.3157752288971096, 0.3039442766457796, 0.3040503079537302, 0.30720041994936764, 0.4163598781451583, 0.3038248971570283, 0.30278034997172654, 0.30900313914753497, 0.3202691311016679, 0.3087431329768151, 0.3049145289696753, 0.30580599000677466, 0.4104957371018827, 0.3103020691778511, 0.3310665839817375, 0.3242657820228487, 0.3194034178741276, 0.3251930898986757, 0.31259172898717225, 0.32520848512649536, 0.33863706211559474, 0.31783980457112193, 0.33494991110637784, 0.32845502789132297, 0.32818651874549687, 0.3085983272176236, 0.31436308519914746, 0.3123734160326421, 0.43101109587587416, 0.31729108607396483, 0.31424672715365887, 0.30448735714890063, 0.30527249188162386, 0.31394068989902735, 0.32913225912488997, 0.46229023090563715, 0.32116860104724765, 0.31675202888436615, 0.3439998719841242, 0.3600955680012703, 0.36222268594428897, 0.48898637224920094, 0.3557972810231149, 0.3562939870171249, 0.353372807148844, 0.325491817900911, 0.32348638703115284, 0.30541313788853586, 0.4523427567910403, 0.4364569941535592, 0.32888247119262815, 0.32805662089958787, 0.35421790019609034, 0.3248003839980811, 0.5334707333240658, 0.45019044703803957, 0.3153932101558894, 0.3037351821549237, 0.3105179921258241, 0.3244987758807838, 0.3185082252603024, 0.3347621476277709, 0.5078657518606633, 0.32225874601863325, 0.29561311192810535, 0.3022374650463462, 0.2952413728926331, 0.3051980340387672, 0.30353129212744534, 0.2947505966294557, 0.4015268830116838, 0.31161949620582163, 0.29533365881070495, 0.297713277162984, 0.3220052469987422, 0.3067408341448754, 0.29358503804542124, 0.2884171169716865, 0.40221720305271447, 0.2792886779643595, 0.2883551591075957, 0.29399277712218463, 0.3029782196972519, 0.29762454610317945, 0.3037337251007557, 0.31154682906344533, 0.382427244912833, 0.30484846723265946, 0.3051655348390341, 0.31718229805119336, 0.32802548562176526, 0.31891793268732727, 0.3106306951958686, 0.2979091580491513, 0.4362128262873739, 0.2903813128359616, 0.294480457669124, 0.29116850811988115, 0.28854499431326985, 0.3052022682968527, 0.28794956789352, 0.2959694860037416, 0.3981167692691088, 0.2831952553242445, 0.28646621108055115, 0.2943234408739954, 0.292712606722489, 0.28796572936698794, 0.2914401071611792, 0.29160174797289073, 0.415902613196522, 0.2883592671714723, 0.28691700170747936, 0.28803424583747983, 0.31475574779324234, 0.28077902109362185, 0.3284623371437192, 0.3008840379770845, 0.3045612499117851, 0.3003803489264101, 0.30434202891774476, 0.3017365199048072, 0.30261278175748885, 0.30860042897984385, 0.3092235561925918, 0.3037186509463936, 0.3035402800887823, 0.30693526519462466, 0.31091480678878725, 0.3027298718225211, 0.3086266617756337, 0.3098683129064739, 0.3008405459113419, 0.30733103095553815, 0.3082519769668579, 0.43951705703511834, 0.30002856999635696, 0.2956014322116971, 0.3089313230011612, 0.30184974893927574, 0.30041420087218285, 0.2991257330868393, 0.30366859678179026, 0.2962870318442583, 0.33172489097341895, 0.34698898904025555, 0.34088755887933075, 0.33598769921809435, 0.3303664398845285, 0.33618689980357885, 0.3330566661898047, 0.333525083726272, 0.802665316965431, 6.56699580908753, 1.043296131072566, 0.2973352789413184, 0.3038388581480831, 0.30607850500382483, 0.3056854489259422, 0.29959060112014413, 0.3874842990189791, 0.29683397873304784, 0.2985736532136798, 0.30475084483623505, 0.2915035751648247, 0.2945584338158369, 0.29787558503448963, 0.30436883424408734, 0.3945771201979369, 0.30272734886966646, 0.30882849264889956, 0.29820040659978986, 0.31085559516213834, 0.3218087439890951, 0.3262665180955082, 0.3232443162705749, 0.43047762964852154, 0.3086019952315837, 0.33337396499700844, 0.32751533505506814, 0.3050859719514847, 0.3158601741306484, 0.3068368877284229, 0.4412787100300193, 0.3029911641497165, 0.30518996505998075, 0.3144284419249743, 0.30426712497137487, 0.29495044983923435, 0.2994974278844893, 0.2958617168478668, 0.4003306389786303, 0.31318058795295656, 0.3160593290813267, 0.31740878196433187, 0.32974045211449265, 0.31830316898413, 0.31044878577813506, 0.3135382120963186, 0.3557762918062508, 0.31061762012541294, 0.312261019134894, 0.31052404711954296]
Total Epoch List: [129, 140]
Total Time List: [0.23868092894554138, 0.09425130393356085]
========================k_idx:2========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: SGFormer
SGFormer(
  (trans_conv): SGModule(
    (attns): ModuleList(
      (0-1): 2 x SGFormerAttention(heads=1, head_channels=64)
    )
    (fcs): ModuleList(
      (0): Linear(in_features=14887, out_features=64, bias=True)
    )
    (bns): ModuleList(
      (0-2): 3 x LayerNorm((64,), eps=1e-05, elementwise_affine=True)
    )
  )
  (graph_conv): GraphModule(
    (convs): ModuleList(
      (0-2): 3 x GCNConv(64, 64)
    )
    (fcs): ModuleList(
      (0): Linear(in_features=14887, out_features=64, bias=True)
    )
    (bns): ModuleList(
      (0-3): 4 x BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (classifier): Linear(in_features=64, out_features=2, bias=True)
)
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7f342d377550>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.7356;  Loss pred: 0.7356; Loss self: 0.0000; time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7519 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7400 score: 0.5000 time: 0.08s
Epoch 2/1000, LR 0.000000
Train loss: 0.7574;  Loss pred: 0.7574; Loss self: 0.0000; time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7522 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7404 score: 0.5000 time: 0.08s
     INFO: Early stopping counter 1 of 20
Epoch 3/1000, LR 0.000030
Train loss: 0.7330;  Loss pred: 0.7330; Loss self: 0.0000; time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7497 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7384 score: 0.5000 time: 0.24s
Epoch 4/1000, LR 0.000060
Train loss: 0.7428;  Loss pred: 0.7428; Loss self: 0.0000; time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7445 score: 0.4898 time: 0.09s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7340 score: 0.5000 time: 0.09s
Epoch 5/1000, LR 0.000090
Train loss: 0.7312;  Loss pred: 0.7312; Loss self: 0.0000; time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7364 score: 0.4898 time: 0.09s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7271 score: 0.5000 time: 0.10s
Epoch 6/1000, LR 0.000120
Train loss: 0.7289;  Loss pred: 0.7289; Loss self: 0.0000; time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7260 score: 0.4898 time: 0.09s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7185 score: 0.5000 time: 0.09s
Epoch 7/1000, LR 0.000150
Train loss: 0.6909;  Loss pred: 0.6909; Loss self: 0.0000; time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7137 score: 0.4898 time: 0.09s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7085 score: 0.5000 time: 0.09s
Epoch 8/1000, LR 0.000180
Train loss: 0.6893;  Loss pred: 0.6893; Loss self: 0.0000; time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6995 score: 0.4898 time: 0.09s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6971 score: 0.5000 time: 0.09s
Epoch 9/1000, LR 0.000210
Train loss: 0.6686;  Loss pred: 0.6686; Loss self: 0.0000; time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6842 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6850 score: 0.5000 time: 0.08s
Epoch 10/1000, LR 0.000240
Train loss: 0.6530;  Loss pred: 0.6530; Loss self: 0.0000; time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6688 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6732 score: 0.5000 time: 0.08s
Epoch 11/1000, LR 0.000270
Train loss: 0.6203;  Loss pred: 0.6203; Loss self: 0.0000; time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6558 score: 0.4898 time: 0.12s
Test loss: 0.6639 score: 0.5208 time: 0.08s
Epoch 12/1000, LR 0.000270
Train loss: 0.5883;  Loss pred: 0.5883; Loss self: 0.0000; time: 0.16s
Val loss: 0.6454 score: 0.7347 time: 0.08s
Test loss: 0.6572 score: 0.7917 time: 0.08s
Epoch 13/1000, LR 0.000270
Train loss: 0.5840;  Loss pred: 0.5840; Loss self: 0.0000; time: 0.17s
Val loss: 0.6382 score: 0.9796 time: 0.08s
Test loss: 0.6530 score: 0.8125 time: 0.09s
Epoch 14/1000, LR 0.000270
Train loss: 0.5395;  Loss pred: 0.5395; Loss self: 0.0000; time: 0.16s
Val loss: 0.6329 score: 0.8776 time: 0.09s
Test loss: 0.6506 score: 0.8125 time: 0.09s
Epoch 15/1000, LR 0.000270
Train loss: 0.5118;  Loss pred: 0.5118; Loss self: 0.0000; time: 0.16s
Val loss: 0.6298 score: 0.6939 time: 0.08s
Test loss: 0.6506 score: 0.5833 time: 0.09s
Epoch 16/1000, LR 0.000270
Train loss: 0.5327;  Loss pred: 0.5327; Loss self: 0.0000; time: 0.15s
Val loss: 0.6286 score: 0.5918 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6524 score: 0.5000 time: 0.08s
Epoch 17/1000, LR 0.000270
Train loss: 0.5132;  Loss pred: 0.5132; Loss self: 0.0000; time: 0.16s
Val loss: 0.6283 score: 0.5714 time: 0.09s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6552 score: 0.5000 time: 0.09s
Epoch 18/1000, LR 0.000270
Train loss: 0.4989;  Loss pred: 0.4989; Loss self: 0.0000; time: 0.16s
Val loss: 0.6286 score: 0.5714 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6583 score: 0.5000 time: 0.19s
     INFO: Early stopping counter 1 of 20
Epoch 19/1000, LR 0.000270
Train loss: 0.4984;  Loss pred: 0.4984; Loss self: 0.0000; time: 0.15s
Val loss: 0.6284 score: 0.5510 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6609 score: 0.5000 time: 0.08s
     INFO: Early stopping counter 2 of 20
Epoch 20/1000, LR 0.000270
Train loss: 0.4740;  Loss pred: 0.4740; Loss self: 0.0000; time: 0.15s
Val loss: 0.6282 score: 0.5510 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6636 score: 0.5000 time: 0.08s
Epoch 21/1000, LR 0.000270
Train loss: 0.4722;  Loss pred: 0.4722; Loss self: 0.0000; time: 0.15s
Val loss: 0.6278 score: 0.5306 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6660 score: 0.5000 time: 0.08s
Epoch 22/1000, LR 0.000270
Train loss: 0.4421;  Loss pred: 0.4421; Loss self: 0.0000; time: 0.16s
Val loss: 0.6269 score: 0.5510 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6677 score: 0.5000 time: 0.08s
Epoch 23/1000, LR 0.000270
Train loss: 0.4395;  Loss pred: 0.4395; Loss self: 0.0000; time: 0.15s
Val loss: 0.6255 score: 0.5510 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6689 score: 0.5000 time: 0.09s
Epoch 24/1000, LR 0.000270
Train loss: 0.4169;  Loss pred: 0.4169; Loss self: 0.0000; time: 0.16s
Val loss: 0.6240 score: 0.5510 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6700 score: 0.5000 time: 0.08s
Epoch 25/1000, LR 0.000270
Train loss: 0.4047;  Loss pred: 0.4047; Loss self: 0.0000; time: 0.16s
Val loss: 0.6227 score: 0.5510 time: 0.09s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6712 score: 0.5000 time: 0.08s
Epoch 26/1000, LR 0.000270
Train loss: 0.3960;  Loss pred: 0.3960; Loss self: 0.0000; time: 0.15s
Val loss: 0.6208 score: 0.5510 time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6717 score: 0.5000 time: 0.09s
Epoch 27/1000, LR 0.000270
Train loss: 0.4047;  Loss pred: 0.4047; Loss self: 0.0000; time: 0.16s
Val loss: 0.6184 score: 0.5510 time: 0.09s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6717 score: 0.5000 time: 0.09s
Epoch 28/1000, LR 0.000270
Train loss: 0.4019;  Loss pred: 0.4019; Loss self: 0.0000; time: 0.18s
Val loss: 0.6155 score: 0.5510 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6712 score: 0.5000 time: 0.08s
Epoch 29/1000, LR 0.000270
Train loss: 0.3714;  Loss pred: 0.3714; Loss self: 0.0000; time: 0.15s
Val loss: 0.6131 score: 0.5510 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6715 score: 0.5000 time: 0.08s
Epoch 30/1000, LR 0.000270
Train loss: 0.3434;  Loss pred: 0.3434; Loss self: 0.0000; time: 0.15s
Val loss: 0.6099 score: 0.5714 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6710 score: 0.5000 time: 0.08s
Epoch 31/1000, LR 0.000270
Train loss: 0.3551;  Loss pred: 0.3551; Loss self: 0.0000; time: 0.16s
Val loss: 0.6064 score: 0.5714 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6706 score: 0.5000 time: 0.09s
Epoch 32/1000, LR 0.000270
Train loss: 0.3433;  Loss pred: 0.3433; Loss self: 0.0000; time: 0.32s
Val loss: 0.6022 score: 0.5714 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6697 score: 0.5000 time: 0.08s
Epoch 33/1000, LR 0.000270
Train loss: 0.3461;  Loss pred: 0.3461; Loss self: 0.0000; time: 0.16s
Val loss: 0.5958 score: 0.5714 time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6666 score: 0.5000 time: 0.08s
Epoch 34/1000, LR 0.000270
Train loss: 0.3328;  Loss pred: 0.3328; Loss self: 0.0000; time: 0.15s
Val loss: 0.5898 score: 0.5918 time: 0.08s
Test loss: 0.6639 score: 0.5208 time: 0.08s
Epoch 35/1000, LR 0.000270
Train loss: 0.3303;  Loss pred: 0.3303; Loss self: 0.0000; time: 0.15s
Val loss: 0.5837 score: 0.6122 time: 0.08s
Test loss: 0.6609 score: 0.5208 time: 0.08s
Epoch 36/1000, LR 0.000270
Train loss: 0.3340;  Loss pred: 0.3340; Loss self: 0.0000; time: 0.15s
Val loss: 0.5776 score: 0.6327 time: 0.08s
Test loss: 0.6580 score: 0.5208 time: 0.08s
Epoch 37/1000, LR 0.000270
Train loss: 0.3089;  Loss pred: 0.3089; Loss self: 0.0000; time: 0.16s
Val loss: 0.5714 score: 0.6327 time: 0.08s
Test loss: 0.6548 score: 0.5208 time: 0.09s
Epoch 38/1000, LR 0.000270
Train loss: 0.3055;  Loss pred: 0.3055; Loss self: 0.0000; time: 0.16s
Val loss: 0.5660 score: 0.6327 time: 0.09s
Test loss: 0.6524 score: 0.5208 time: 0.09s
Epoch 39/1000, LR 0.000269
Train loss: 0.2924;  Loss pred: 0.2924; Loss self: 0.0000; time: 0.16s
Val loss: 0.5605 score: 0.6327 time: 0.08s
Test loss: 0.6499 score: 0.5208 time: 0.09s
Epoch 40/1000, LR 0.000269
Train loss: 0.2813;  Loss pred: 0.2813; Loss self: 0.0000; time: 0.16s
Val loss: 0.5555 score: 0.6327 time: 0.19s
Test loss: 0.6477 score: 0.5208 time: 0.08s
Epoch 41/1000, LR 0.000269
Train loss: 0.2877;  Loss pred: 0.2877; Loss self: 0.0000; time: 0.16s
Val loss: 0.5511 score: 0.6327 time: 0.08s
Test loss: 0.6460 score: 0.5208 time: 0.08s
Epoch 42/1000, LR 0.000269
Train loss: 0.2805;  Loss pred: 0.2805; Loss self: 0.0000; time: 0.15s
Val loss: 0.5472 score: 0.6327 time: 0.08s
Test loss: 0.6445 score: 0.5208 time: 0.08s
Epoch 43/1000, LR 0.000269
Train loss: 0.2792;  Loss pred: 0.2792; Loss self: 0.0000; time: 0.15s
Val loss: 0.5433 score: 0.6327 time: 0.08s
Test loss: 0.6431 score: 0.5208 time: 0.08s
Epoch 44/1000, LR 0.000269
Train loss: 0.2646;  Loss pred: 0.2646; Loss self: 0.0000; time: 0.16s
Val loss: 0.5385 score: 0.6327 time: 0.08s
Test loss: 0.6405 score: 0.5208 time: 0.08s
Epoch 45/1000, LR 0.000269
Train loss: 0.2567;  Loss pred: 0.2567; Loss self: 0.0000; time: 0.15s
Val loss: 0.5338 score: 0.6327 time: 0.08s
Test loss: 0.6378 score: 0.5208 time: 0.08s
Epoch 46/1000, LR 0.000269
Train loss: 0.2403;  Loss pred: 0.2403; Loss self: 0.0000; time: 0.16s
Val loss: 0.5292 score: 0.6327 time: 0.08s
Test loss: 0.6350 score: 0.5208 time: 0.09s
Epoch 47/1000, LR 0.000269
Train loss: 0.2336;  Loss pred: 0.2336; Loss self: 0.0000; time: 0.16s
Val loss: 0.5248 score: 0.6531 time: 0.08s
Test loss: 0.6323 score: 0.5208 time: 0.09s
Epoch 48/1000, LR 0.000269
Train loss: 0.2357;  Loss pred: 0.2357; Loss self: 0.0000; time: 0.29s
Val loss: 0.5218 score: 0.6531 time: 0.08s
Test loss: 0.6309 score: 0.5625 time: 0.08s
Epoch 49/1000, LR 0.000269
Train loss: 0.2472;  Loss pred: 0.2472; Loss self: 0.0000; time: 0.16s
Val loss: 0.5192 score: 0.6531 time: 0.09s
Test loss: 0.6297 score: 0.5833 time: 0.08s
Epoch 50/1000, LR 0.000269
Train loss: 0.2461;  Loss pred: 0.2461; Loss self: 0.0000; time: 0.16s
Val loss: 0.5171 score: 0.6531 time: 0.08s
Test loss: 0.6289 score: 0.5833 time: 0.08s
Epoch 51/1000, LR 0.000269
Train loss: 0.2189;  Loss pred: 0.2189; Loss self: 0.0000; time: 0.16s
Val loss: 0.5152 score: 0.6531 time: 0.08s
Test loss: 0.6284 score: 0.5833 time: 0.08s
Epoch 52/1000, LR 0.000269
Train loss: 0.2194;  Loss pred: 0.2194; Loss self: 0.0000; time: 0.15s
Val loss: 0.5128 score: 0.6531 time: 0.08s
Test loss: 0.6277 score: 0.5833 time: 0.08s
Epoch 53/1000, LR 0.000269
Train loss: 0.2140;  Loss pred: 0.2140; Loss self: 0.0000; time: 0.16s
Val loss: 0.5101 score: 0.6531 time: 0.08s
Test loss: 0.6262 score: 0.5833 time: 0.08s
Epoch 54/1000, LR 0.000269
Train loss: 0.2169;  Loss pred: 0.2169; Loss self: 0.0000; time: 0.15s
Val loss: 0.5074 score: 0.6531 time: 0.08s
Test loss: 0.6247 score: 0.5833 time: 0.08s
Epoch 55/1000, LR 0.000269
Train loss: 0.2283;  Loss pred: 0.2283; Loss self: 0.0000; time: 0.16s
Val loss: 0.5046 score: 0.6531 time: 0.21s
Test loss: 0.6229 score: 0.5833 time: 0.08s
Epoch 56/1000, LR 0.000269
Train loss: 0.2017;  Loss pred: 0.2017; Loss self: 0.0000; time: 0.15s
Val loss: 0.5011 score: 0.6531 time: 0.08s
Test loss: 0.6200 score: 0.5833 time: 0.08s
Epoch 57/1000, LR 0.000269
Train loss: 0.2113;  Loss pred: 0.2113; Loss self: 0.0000; time: 0.16s
Val loss: 0.4974 score: 0.6735 time: 0.08s
Test loss: 0.6171 score: 0.5833 time: 0.08s
Epoch 58/1000, LR 0.000269
Train loss: 0.1993;  Loss pred: 0.1993; Loss self: 0.0000; time: 0.16s
Val loss: 0.4928 score: 0.6939 time: 0.08s
Test loss: 0.6132 score: 0.5833 time: 0.08s
Epoch 59/1000, LR 0.000268
Train loss: 0.1807;  Loss pred: 0.1807; Loss self: 0.0000; time: 0.16s
Val loss: 0.4880 score: 0.6939 time: 0.08s
Test loss: 0.6090 score: 0.5833 time: 0.08s
Epoch 60/1000, LR 0.000268
Train loss: 0.1905;  Loss pred: 0.1905; Loss self: 0.0000; time: 0.16s
Val loss: 0.4836 score: 0.6939 time: 0.08s
Test loss: 0.6053 score: 0.5833 time: 0.08s
Epoch 61/1000, LR 0.000268
Train loss: 0.1810;  Loss pred: 0.1810; Loss self: 0.0000; time: 0.15s
Val loss: 0.4782 score: 0.6939 time: 0.08s
Test loss: 0.6005 score: 0.5833 time: 0.08s
Epoch 62/1000, LR 0.000268
Train loss: 0.1840;  Loss pred: 0.1840; Loss self: 0.0000; time: 0.30s
Val loss: 0.4724 score: 0.6939 time: 0.08s
Test loss: 0.5951 score: 0.6042 time: 0.08s
Epoch 63/1000, LR 0.000268
Train loss: 0.1758;  Loss pred: 0.1758; Loss self: 0.0000; time: 0.15s
Val loss: 0.4663 score: 0.7143 time: 0.08s
Test loss: 0.5895 score: 0.6042 time: 0.08s
Epoch 64/1000, LR 0.000268
Train loss: 0.1688;  Loss pred: 0.1688; Loss self: 0.0000; time: 0.15s
Val loss: 0.4588 score: 0.7143 time: 0.08s
Test loss: 0.5818 score: 0.6250 time: 0.08s
Epoch 65/1000, LR 0.000268
Train loss: 0.1750;  Loss pred: 0.1750; Loss self: 0.0000; time: 0.16s
Val loss: 0.4509 score: 0.7143 time: 0.08s
Test loss: 0.5738 score: 0.6458 time: 0.08s
Epoch 66/1000, LR 0.000268
Train loss: 0.1686;  Loss pred: 0.1686; Loss self: 0.0000; time: 0.14s
Val loss: 0.4443 score: 0.7347 time: 0.08s
Test loss: 0.5671 score: 0.6458 time: 0.08s
Epoch 67/1000, LR 0.000268
Train loss: 0.1576;  Loss pred: 0.1576; Loss self: 0.0000; time: 0.15s
Val loss: 0.4380 score: 0.7347 time: 0.09s
Test loss: 0.5608 score: 0.6667 time: 0.09s
Epoch 68/1000, LR 0.000268
Train loss: 0.1697;  Loss pred: 0.1697; Loss self: 0.0000; time: 0.17s
Val loss: 0.4320 score: 0.7347 time: 0.08s
Test loss: 0.5549 score: 0.6667 time: 0.08s
Epoch 69/1000, LR 0.000268
Train loss: 0.1571;  Loss pred: 0.1571; Loss self: 0.0000; time: 0.15s
Val loss: 0.4268 score: 0.7347 time: 0.08s
Test loss: 0.5500 score: 0.6875 time: 0.08s
Epoch 70/1000, LR 0.000268
Train loss: 0.1514;  Loss pred: 0.1514; Loss self: 0.0000; time: 0.17s
Val loss: 0.4213 score: 0.7347 time: 0.09s
Test loss: 0.5448 score: 0.6875 time: 0.08s
Epoch 71/1000, LR 0.000268
Train loss: 0.1504;  Loss pred: 0.1504; Loss self: 0.0000; time: 0.16s
Val loss: 0.4154 score: 0.7551 time: 0.09s
Test loss: 0.5390 score: 0.6875 time: 0.09s
Epoch 72/1000, LR 0.000267
Train loss: 0.1430;  Loss pred: 0.1430; Loss self: 0.0000; time: 0.17s
Val loss: 0.4091 score: 0.7551 time: 0.08s
Test loss: 0.5329 score: 0.7083 time: 0.08s
Epoch 73/1000, LR 0.000267
Train loss: 0.1499;  Loss pred: 0.1499; Loss self: 0.0000; time: 0.15s
Val loss: 0.4029 score: 0.7755 time: 0.08s
Test loss: 0.5266 score: 0.7083 time: 0.08s
Epoch 74/1000, LR 0.000267
Train loss: 0.1373;  Loss pred: 0.1373; Loss self: 0.0000; time: 0.16s
Val loss: 0.3958 score: 0.7755 time: 0.08s
Test loss: 0.5191 score: 0.7083 time: 0.08s
Epoch 75/1000, LR 0.000267
Train loss: 0.1323;  Loss pred: 0.1323; Loss self: 0.0000; time: 0.16s
Val loss: 0.3889 score: 0.7959 time: 0.08s
Test loss: 0.5119 score: 0.7083 time: 0.08s
Epoch 76/1000, LR 0.000267
Train loss: 0.1409;  Loss pred: 0.1409; Loss self: 0.0000; time: 0.16s
Val loss: 0.3824 score: 0.7959 time: 0.08s
Test loss: 0.5053 score: 0.7083 time: 0.08s
Epoch 77/1000, LR 0.000267
Train loss: 0.1376;  Loss pred: 0.1376; Loss self: 0.0000; time: 0.15s
Val loss: 0.3747 score: 0.7959 time: 0.08s
Test loss: 0.4972 score: 0.7083 time: 0.10s
Epoch 78/1000, LR 0.000267
Train loss: 0.1265;  Loss pred: 0.1265; Loss self: 0.0000; time: 0.16s
Val loss: 0.3676 score: 0.7959 time: 0.08s
Test loss: 0.4899 score: 0.7292 time: 0.08s
Epoch 79/1000, LR 0.000267
Train loss: 0.1254;  Loss pred: 0.1254; Loss self: 0.0000; time: 0.16s
Val loss: 0.3600 score: 0.7959 time: 0.08s
Test loss: 0.4818 score: 0.7500 time: 0.09s
Epoch 80/1000, LR 0.000267
Train loss: 0.1326;  Loss pred: 0.1326; Loss self: 0.0000; time: 0.17s
Val loss: 0.3526 score: 0.8163 time: 0.08s
Test loss: 0.4738 score: 0.7708 time: 0.08s
Epoch 81/1000, LR 0.000267
Train loss: 0.1264;  Loss pred: 0.1264; Loss self: 0.0000; time: 0.17s
Val loss: 0.3442 score: 0.8367 time: 0.08s
Test loss: 0.4640 score: 0.7708 time: 0.08s
Epoch 82/1000, LR 0.000267
Train loss: 0.1189;  Loss pred: 0.1189; Loss self: 0.0000; time: 0.15s
Val loss: 0.3354 score: 0.8367 time: 0.08s
Test loss: 0.4536 score: 0.7500 time: 0.08s
Epoch 83/1000, LR 0.000266
Train loss: 0.1263;  Loss pred: 0.1263; Loss self: 0.0000; time: 0.16s
Val loss: 0.3283 score: 0.8367 time: 0.08s
Test loss: 0.4457 score: 0.7500 time: 0.08s
Epoch 84/1000, LR 0.000266
Train loss: 0.1157;  Loss pred: 0.1157; Loss self: 0.0000; time: 0.16s
Val loss: 0.3211 score: 0.8571 time: 0.09s
Test loss: 0.4374 score: 0.7708 time: 0.09s
Epoch 85/1000, LR 0.000266
Train loss: 0.1244;  Loss pred: 0.1244; Loss self: 0.0000; time: 0.16s
Val loss: 0.3136 score: 0.8571 time: 0.08s
Test loss: 0.4284 score: 0.7708 time: 0.08s
Epoch 86/1000, LR 0.000266
Train loss: 0.1044;  Loss pred: 0.1044; Loss self: 0.0000; time: 0.15s
Val loss: 0.3059 score: 0.8776 time: 0.08s
Test loss: 0.4191 score: 0.7708 time: 0.08s
Epoch 87/1000, LR 0.000266
Train loss: 0.1233;  Loss pred: 0.1233; Loss self: 0.0000; time: 0.16s
Val loss: 0.2981 score: 0.8980 time: 0.10s
Test loss: 0.4093 score: 0.7917 time: 0.09s
Epoch 88/1000, LR 0.000266
Train loss: 0.1118;  Loss pred: 0.1118; Loss self: 0.0000; time: 0.16s
Val loss: 0.2907 score: 0.8980 time: 0.09s
Test loss: 0.3999 score: 0.7917 time: 0.09s
Epoch 89/1000, LR 0.000266
Train loss: 0.1085;  Loss pred: 0.1085; Loss self: 0.0000; time: 0.16s
Val loss: 0.2836 score: 0.9184 time: 0.09s
Test loss: 0.3906 score: 0.7917 time: 0.09s
Epoch 90/1000, LR 0.000266
Train loss: 0.1118;  Loss pred: 0.1118; Loss self: 0.0000; time: 0.16s
Val loss: 0.2761 score: 0.9184 time: 0.08s
Test loss: 0.3805 score: 0.8125 time: 0.09s
Epoch 91/1000, LR 0.000266
Train loss: 0.1048;  Loss pred: 0.1048; Loss self: 0.0000; time: 0.16s
Val loss: 0.2688 score: 0.9184 time: 0.09s
Test loss: 0.3704 score: 0.8125 time: 0.09s
Epoch 92/1000, LR 0.000266
Train loss: 0.0995;  Loss pred: 0.0995; Loss self: 0.0000; time: 0.16s
Val loss: 0.2621 score: 0.9184 time: 0.09s
Test loss: 0.3609 score: 0.8333 time: 0.09s
Epoch 93/1000, LR 0.000265
Train loss: 0.1059;  Loss pred: 0.1059; Loss self: 0.0000; time: 0.16s
Val loss: 0.2559 score: 0.9388 time: 0.08s
Test loss: 0.3522 score: 0.8333 time: 0.08s
Epoch 94/1000, LR 0.000265
Train loss: 0.1053;  Loss pred: 0.1053; Loss self: 0.0000; time: 0.16s
Val loss: 0.2509 score: 0.9388 time: 0.09s
Test loss: 0.3454 score: 0.8542 time: 0.09s
Epoch 95/1000, LR 0.000265
Train loss: 0.0953;  Loss pred: 0.0953; Loss self: 0.0000; time: 0.16s
Val loss: 0.2462 score: 0.9388 time: 0.08s
Test loss: 0.3388 score: 0.8542 time: 0.09s
Epoch 96/1000, LR 0.000265
Train loss: 0.0893;  Loss pred: 0.0893; Loss self: 0.0000; time: 0.16s
Val loss: 0.2412 score: 0.9592 time: 0.08s
Test loss: 0.3313 score: 0.8542 time: 0.09s
Epoch 97/1000, LR 0.000265
Train loss: 0.0949;  Loss pred: 0.0949; Loss self: 0.0000; time: 0.16s
Val loss: 0.2363 score: 0.9592 time: 0.09s
Test loss: 0.3241 score: 0.8750 time: 0.08s
Epoch 98/1000, LR 0.000265
Train loss: 0.0890;  Loss pred: 0.0890; Loss self: 0.0000; time: 0.15s
Val loss: 0.2319 score: 0.9592 time: 0.08s
Test loss: 0.3175 score: 0.8750 time: 0.08s
Epoch 99/1000, LR 0.000265
Train loss: 0.0853;  Loss pred: 0.0853; Loss self: 0.0000; time: 0.15s
Val loss: 0.2272 score: 0.9592 time: 0.07s
Test loss: 0.3102 score: 0.8750 time: 0.08s
Epoch 100/1000, LR 0.000265
Train loss: 0.0972;  Loss pred: 0.0972; Loss self: 0.0000; time: 0.15s
Val loss: 0.2229 score: 0.9592 time: 0.07s
Test loss: 0.3032 score: 0.8750 time: 0.08s
Epoch 101/1000, LR 0.000265
Train loss: 0.0954;  Loss pred: 0.0954; Loss self: 0.0000; time: 0.28s
Val loss: 0.2185 score: 0.9592 time: 0.08s
Test loss: 0.2958 score: 0.8542 time: 0.08s
Epoch 102/1000, LR 0.000264
Train loss: 0.0887;  Loss pred: 0.0887; Loss self: 0.0000; time: 0.15s
Val loss: 0.2148 score: 0.9592 time: 0.08s
Test loss: 0.2896 score: 0.8542 time: 0.08s
Epoch 103/1000, LR 0.000264
Train loss: 0.0924;  Loss pred: 0.0924; Loss self: 0.0000; time: 0.16s
Val loss: 0.2114 score: 0.9592 time: 0.08s
Test loss: 0.2836 score: 0.8542 time: 0.08s
Epoch 104/1000, LR 0.000264
Train loss: 0.0817;  Loss pred: 0.0817; Loss self: 0.0000; time: 0.15s
Val loss: 0.2084 score: 0.9592 time: 0.08s
Test loss: 0.2782 score: 0.8750 time: 0.08s
Epoch 105/1000, LR 0.000264
Train loss: 0.0774;  Loss pred: 0.0774; Loss self: 0.0000; time: 0.16s
Val loss: 0.2054 score: 0.9592 time: 0.08s
Test loss: 0.2726 score: 0.8750 time: 0.08s
Epoch 106/1000, LR 0.000264
Train loss: 0.0784;  Loss pred: 0.0784; Loss self: 0.0000; time: 0.16s
Val loss: 0.2027 score: 0.9592 time: 0.09s
Test loss: 0.2677 score: 0.8750 time: 0.09s
Epoch 107/1000, LR 0.000264
Train loss: 0.0760;  Loss pred: 0.0760; Loss self: 0.0000; time: 0.15s
Val loss: 0.2001 score: 0.9592 time: 0.08s
Test loss: 0.2624 score: 0.8750 time: 0.09s
Epoch 108/1000, LR 0.000264
Train loss: 0.0913;  Loss pred: 0.0913; Loss self: 0.0000; time: 0.18s
Val loss: 0.1977 score: 0.9592 time: 0.16s
Test loss: 0.2577 score: 0.8750 time: 0.09s
Epoch 109/1000, LR 0.000264
Train loss: 0.0756;  Loss pred: 0.0756; Loss self: 0.0000; time: 0.16s
Val loss: 0.1956 score: 0.9592 time: 0.09s
Test loss: 0.2531 score: 0.8750 time: 0.08s
Epoch 110/1000, LR 0.000263
Train loss: 0.0778;  Loss pred: 0.0778; Loss self: 0.0000; time: 0.15s
Val loss: 0.1937 score: 0.9592 time: 0.08s
Test loss: 0.2494 score: 0.8750 time: 0.08s
Epoch 111/1000, LR 0.000263
Train loss: 0.0753;  Loss pred: 0.0753; Loss self: 0.0000; time: 0.16s
Val loss: 0.1919 score: 0.9592 time: 0.08s
Test loss: 0.2459 score: 0.8750 time: 0.08s
Epoch 112/1000, LR 0.000263
Train loss: 0.0721;  Loss pred: 0.0721; Loss self: 0.0000; time: 0.16s
Val loss: 0.1903 score: 0.9592 time: 0.08s
Test loss: 0.2426 score: 0.8958 time: 0.08s
Epoch 113/1000, LR 0.000263
Train loss: 0.0697;  Loss pred: 0.0697; Loss self: 0.0000; time: 0.15s
Val loss: 0.1889 score: 0.9592 time: 0.08s
Test loss: 0.2395 score: 0.8958 time: 0.08s
Epoch 114/1000, LR 0.000263
Train loss: 0.0685;  Loss pred: 0.0685; Loss self: 0.0000; time: 0.16s
Val loss: 0.1875 score: 0.9592 time: 0.08s
Test loss: 0.2364 score: 0.8958 time: 0.08s
Epoch 115/1000, LR 0.000263
Train loss: 0.0678;  Loss pred: 0.0678; Loss self: 0.0000; time: 0.18s
Val loss: 0.1864 score: 0.9592 time: 0.11s
Test loss: 0.2337 score: 0.8958 time: 0.18s
Epoch 116/1000, LR 0.000263
Train loss: 0.0649;  Loss pred: 0.0649; Loss self: 0.0000; time: 0.16s
Val loss: 0.1853 score: 0.9592 time: 0.08s
Test loss: 0.2313 score: 0.8958 time: 0.08s
Epoch 117/1000, LR 0.000262
Train loss: 0.0696;  Loss pred: 0.0696; Loss self: 0.0000; time: 0.16s
Val loss: 0.1844 score: 0.9592 time: 0.08s
Test loss: 0.2291 score: 0.8958 time: 0.08s
Epoch 118/1000, LR 0.000262
Train loss: 0.0771;  Loss pred: 0.0771; Loss self: 0.0000; time: 0.16s
Val loss: 0.1836 score: 0.9592 time: 0.08s
Test loss: 0.2273 score: 0.8958 time: 0.09s
Epoch 119/1000, LR 0.000262
Train loss: 0.0693;  Loss pred: 0.0693; Loss self: 0.0000; time: 0.16s
Val loss: 0.1829 score: 0.9592 time: 0.09s
Test loss: 0.2252 score: 0.8958 time: 0.09s
Epoch 120/1000, LR 0.000262
Train loss: 0.0612;  Loss pred: 0.0612; Loss self: 0.0000; time: 0.16s
Val loss: 0.1820 score: 0.9592 time: 0.09s
Test loss: 0.2224 score: 0.8958 time: 0.08s
Epoch 121/1000, LR 0.000262
Train loss: 0.0673;  Loss pred: 0.0673; Loss self: 0.0000; time: 0.16s
Val loss: 0.1812 score: 0.9592 time: 0.08s
Test loss: 0.2199 score: 0.9167 time: 0.09s
Epoch 122/1000, LR 0.000262
Train loss: 0.0706;  Loss pred: 0.0706; Loss self: 0.0000; time: 0.17s
Val loss: 0.1804 score: 0.9592 time: 0.18s
Test loss: 0.2176 score: 0.9167 time: 0.10s
Epoch 123/1000, LR 0.000262
Train loss: 0.0576;  Loss pred: 0.0576; Loss self: 0.0000; time: 0.16s
Val loss: 0.1799 score: 0.9592 time: 0.08s
Test loss: 0.2152 score: 0.9375 time: 0.08s
Epoch 124/1000, LR 0.000261
Train loss: 0.0597;  Loss pred: 0.0597; Loss self: 0.0000; time: 0.15s
Val loss: 0.1794 score: 0.9592 time: 0.09s
Test loss: 0.2128 score: 0.9375 time: 0.09s
Epoch 125/1000, LR 0.000261
Train loss: 0.0769;  Loss pred: 0.0769; Loss self: 0.0000; time: 0.15s
Val loss: 0.1792 score: 0.9592 time: 0.08s
Test loss: 0.2107 score: 0.9375 time: 0.09s
Epoch 126/1000, LR 0.000261
Train loss: 0.0687;  Loss pred: 0.0687; Loss self: 0.0000; time: 0.15s
Val loss: 0.1789 score: 0.9592 time: 0.08s
Test loss: 0.2087 score: 0.9375 time: 0.09s
Epoch 127/1000, LR 0.000261
Train loss: 0.0589;  Loss pred: 0.0589; Loss self: 0.0000; time: 0.16s
Val loss: 0.1787 score: 0.9592 time: 0.09s
Test loss: 0.2066 score: 0.9375 time: 0.09s
Epoch 128/1000, LR 0.000261
Train loss: 0.0592;  Loss pred: 0.0592; Loss self: 0.0000; time: 0.16s
Val loss: 0.1784 score: 0.9592 time: 0.09s
Test loss: 0.2044 score: 0.9375 time: 0.08s
Epoch 129/1000, LR 0.000261
Train loss: 0.0581;  Loss pred: 0.0581; Loss self: 0.0000; time: 0.15s
Val loss: 0.1781 score: 0.9592 time: 0.08s
Test loss: 0.2025 score: 0.9375 time: 0.08s
Epoch 130/1000, LR 0.000260
Train loss: 0.0582;  Loss pred: 0.0582; Loss self: 0.0000; time: 0.14s
Val loss: 0.1780 score: 0.9592 time: 0.08s
Test loss: 0.2006 score: 0.9375 time: 0.08s
Epoch 131/1000, LR 0.000260
Train loss: 0.0525;  Loss pred: 0.0525; Loss self: 0.0000; time: 0.29s
Val loss: 0.1780 score: 0.9592 time: 0.08s
Test loss: 0.1995 score: 0.9375 time: 0.08s
     INFO: Early stopping counter 1 of 20
Epoch 132/1000, LR 0.000260
Train loss: 0.0651;  Loss pred: 0.0651; Loss self: 0.0000; time: 0.15s
Val loss: 0.1779 score: 0.9592 time: 0.08s
Test loss: 0.1976 score: 0.9375 time: 0.08s
Epoch 133/1000, LR 0.000260
Train loss: 0.0530;  Loss pred: 0.0530; Loss self: 0.0000; time: 0.15s
Val loss: 0.1786 score: 0.9796 time: 0.08s
Test loss: 0.1961 score: 0.9167 time: 0.08s
     INFO: Early stopping counter 1 of 20
Epoch 134/1000, LR 0.000260
Train loss: 0.0556;  Loss pred: 0.0556; Loss self: 0.0000; time: 0.16s
Val loss: 0.1797 score: 0.9796 time: 0.09s
Test loss: 0.1955 score: 0.9167 time: 0.08s
     INFO: Early stopping counter 2 of 20
Epoch 135/1000, LR 0.000260
Train loss: 0.0538;  Loss pred: 0.0538; Loss self: 0.0000; time: 0.15s
Val loss: 0.1799 score: 0.9796 time: 0.09s
Test loss: 0.1944 score: 0.9167 time: 0.08s
     INFO: Early stopping counter 3 of 20
Epoch 136/1000, LR 0.000260
Train loss: 0.0515;  Loss pred: 0.0515; Loss self: 0.0000; time: 0.16s
Val loss: 0.1795 score: 0.9796 time: 0.08s
Test loss: 0.1934 score: 0.9167 time: 0.08s
     INFO: Early stopping counter 4 of 20
Epoch 137/1000, LR 0.000259
Train loss: 0.0470;  Loss pred: 0.0470; Loss self: 0.0000; time: 0.15s
Val loss: 0.1789 score: 0.9796 time: 0.08s
Test loss: 0.1923 score: 0.9167 time: 0.09s
     INFO: Early stopping counter 5 of 20
Epoch 138/1000, LR 0.000259
Train loss: 0.0477;  Loss pred: 0.0477; Loss self: 0.0000; time: 0.16s
Val loss: 0.1785 score: 0.9796 time: 0.15s
Test loss: 0.1917 score: 0.9167 time: 0.08s
     INFO: Early stopping counter 6 of 20
Epoch 139/1000, LR 0.000259
Train loss: 0.0486;  Loss pred: 0.0486; Loss self: 0.0000; time: 0.16s
Val loss: 0.1786 score: 0.9592 time: 0.08s
Test loss: 0.1915 score: 0.9167 time: 0.08s
     INFO: Early stopping counter 7 of 20
Epoch 140/1000, LR 0.000259
Train loss: 0.0502;  Loss pred: 0.0502; Loss self: 0.0000; time: 0.16s
Val loss: 0.1796 score: 0.9796 time: 0.08s
Test loss: 0.1913 score: 0.9167 time: 0.08s
     INFO: Early stopping counter 8 of 20
Epoch 141/1000, LR 0.000259
Train loss: 0.0433;  Loss pred: 0.0433; Loss self: 0.0000; time: 0.15s
Val loss: 0.1809 score: 0.9796 time: 0.08s
Test loss: 0.1924 score: 0.9167 time: 0.08s
     INFO: Early stopping counter 9 of 20
Epoch 142/1000, LR 0.000259
Train loss: 0.0474;  Loss pred: 0.0474; Loss self: 0.0000; time: 0.16s
Val loss: 0.1830 score: 0.9796 time: 0.08s
Test loss: 0.1947 score: 0.9167 time: 0.08s
     INFO: Early stopping counter 10 of 20
Epoch 143/1000, LR 0.000258
Train loss: 0.0431;  Loss pred: 0.0431; Loss self: 0.0000; time: 0.15s
Val loss: 0.1845 score: 0.9796 time: 0.08s
Test loss: 0.1965 score: 0.9167 time: 0.08s
     INFO: Early stopping counter 11 of 20
Epoch 144/1000, LR 0.000258
Train loss: 0.0525;  Loss pred: 0.0525; Loss self: 0.0000; time: 0.16s
Val loss: 0.1860 score: 0.9796 time: 0.08s
Test loss: 0.1985 score: 0.9167 time: 0.08s
     INFO: Early stopping counter 12 of 20
Epoch 145/1000, LR 0.000258
Train loss: 0.0437;  Loss pred: 0.0437; Loss self: 0.0000; time: 0.16s
Val loss: 0.1870 score: 0.9796 time: 0.08s
Test loss: 0.2007 score: 0.9167 time: 0.08s
     INFO: Early stopping counter 13 of 20
Epoch 146/1000, LR 0.000258
Train loss: 0.0437;  Loss pred: 0.0437; Loss self: 0.0000; time: 0.18s
Val loss: 0.1884 score: 0.9796 time: 0.10s
Test loss: 0.2004 score: 0.9167 time: 0.09s
     INFO: Early stopping counter 14 of 20
Epoch 147/1000, LR 0.000258
Train loss: 0.0393;  Loss pred: 0.0393; Loss self: 0.0000; time: 0.16s
Val loss: 0.1908 score: 0.9796 time: 0.09s
Test loss: 0.2030 score: 0.8958 time: 0.08s
     INFO: Early stopping counter 15 of 20
Epoch 148/1000, LR 0.000257
Train loss: 0.0430;  Loss pred: 0.0430; Loss self: 0.0000; time: 0.16s
Val loss: 0.1895 score: 0.9796 time: 0.08s
Test loss: 0.1992 score: 0.8958 time: 0.08s
     INFO: Early stopping counter 16 of 20
Epoch 149/1000, LR 0.000257
Train loss: 0.0407;  Loss pred: 0.0407; Loss self: 0.0000; time: 0.15s
Val loss: 0.1862 score: 0.9796 time: 0.08s
Test loss: 0.1933 score: 0.9167 time: 0.08s
     INFO: Early stopping counter 17 of 20
Epoch 150/1000, LR 0.000257
Train loss: 0.0390;  Loss pred: 0.0390; Loss self: 0.0000; time: 0.16s
Val loss: 0.1873 score: 0.9796 time: 0.08s
Test loss: 0.1963 score: 0.9167 time: 0.09s
     INFO: Early stopping counter 18 of 20
Epoch 151/1000, LR 0.000257
Train loss: 0.0383;  Loss pred: 0.0383; Loss self: 0.0000; time: 0.15s
Val loss: 0.1887 score: 0.9796 time: 0.09s
Test loss: 0.1976 score: 0.8958 time: 0.08s
     INFO: Early stopping counter 19 of 20
Epoch 152/1000, LR 0.000257
Train loss: 0.0362;  Loss pred: 0.0362; Loss self: 0.0000; time: 0.16s
Val loss: 0.1891 score: 0.9796 time: 0.08s
Test loss: 0.1995 score: 0.8958 time: 0.08s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 131,   Train_Loss: 0.0651,   Val_Loss: 0.1779,   Val_Precision: 0.9600,   Val_Recall: 0.9600,   Val_accuracy: 0.9600,   Val_Score: 0.9592,   Val_Loss: 0.1779,   Test_Precision: 0.9565,   Test_Recall: 0.9167,   Test_accuracy: 0.9362,   Test_Score: 0.9375,   Test_loss: 0.1976


[0.0923724400345236, 0.09272063802927732, 0.08889736793935299, 0.09417051286436617, 0.09018707810901105, 0.08776998892426491, 0.08725390210747719, 0.20725050591863692, 0.08858782309107482, 0.08919495297595859, 0.08934879209846258, 0.08880400890484452, 0.0894871880300343, 0.08816483709961176, 0.0875755709130317, 0.0855906531214714, 0.08672121609561145, 0.09077785513363779, 0.08842640393413603, 0.08714691898785532, 0.08716778503730893, 0.09772477601654828, 0.08830070099793375, 0.08922215108759701, 0.08772663702256978, 0.08844832889735699, 0.0913308730814606, 0.09526202897541225, 0.10000373213551939, 0.09139185608364642, 0.0877063418738544, 0.08642259007319808, 0.08876497694291174, 0.08700242708437145, 0.0867588750552386, 0.0852962601929903, 0.08861434808932245, 0.08640759508125484, 0.08718537003733218, 0.08595339697785676, 0.0881145850289613, 0.09242727211676538, 0.08877051109448075, 0.0899763721972704, 0.0965567990206182, 0.08937351894564927, 0.08996126800775528, 0.08943881699815392, 0.08976403111591935, 0.0875426169950515, 0.09064319496974349, 0.09473756817169487, 0.08808105695061386, 0.08961903001181781, 0.09991109208203852, 0.08954954100772738, 0.08896022709086537, 0.08985830983147025, 0.08667816501110792, 0.08882927102968097, 0.08979957597330213, 0.08840710297226906, 0.07976822415366769, 0.09475376293994486, 0.08674033009447157, 0.08774955198168755, 0.09162684483453631, 0.08909940300509334, 0.0891390519682318, 0.08898501098155975, 0.08847574098035693, 0.08648484176956117, 0.08646886493079364, 0.08767650602385402, 0.09117117198184133, 0.08694075886160135, 0.08676877897232771, 0.08838087599724531, 0.08881456102244556, 0.08996102889068425, 0.09085428318940103, 0.0901144880335778, 0.09139849292114377, 0.09101863391697407, 0.08963741804473102, 0.09346674289554358, 0.089841578155756, 0.08780689095146954, 0.09459696407429874, 0.0944067221134901, 0.08965900586917996, 0.09251349093392491, 0.08970654499717057, 0.09053152496926486, 0.08625785913318396, 0.08695084089413285, 0.08992516784928739, 0.08656944101676345, 0.09050293290056288, 0.08631433197297156, 0.08718661894090474, 0.08699646894820035, 0.08878183807246387, 0.09023214387707412, 0.10148900700733066, 0.11179829691536725, 0.1014670068398118, 0.23500279593281448, 0.10184356686659157, 0.10023184306919575, 0.09936410398222506, 0.08917102008126676, 0.08813585084863007, 0.08465180802159011, 0.09733205405063927, 0.09464075113646686, 0.0924620651639998, 0.09749532910063863, 0.09290332300588489, 0.08715918194502592, 0.08909616083838046, 0.09065122087486088, 0.08670487604103982, 0.0856805460061878, 0.09604316693730652, 0.09002745500765741, 0.08653434994630516, 0.09356915997341275, 0.23805228690616786, 0.10475258296355605, 0.08832069882191718, 0.09451443795114756, 0.08938370505347848, 0.08985678991302848, 0.09041765797883272, 0.09007414802908897, 0.08911701291799545, 0.09512634295970201, 0.08992218296043575, 0.08947149803861976, 0.10258375410921872, 0.09410705394111574, 0.0914903711527586, 0.08903187606483698, 0.08665797905996442, 0.08603515103459358, 0.08707149582915008, 0.08908553118817508, 0.08840600308030844, 0.0916309270542115, 0.08849171316251159, 0.0909760519862175, 0.16866309591569006, 0.09263469208963215, 0.09038714994676411, 0.10537788621149957, 0.09338978817686439, 0.09560600109398365, 0.09504669206216931, 0.08627667208202183, 0.08581839897669852, 0.0884807191323489, 0.08985734987072647, 0.08865122590214014, 0.08488967618905008, 0.10026020486839116, 0.09081005211919546, 0.0901593950111419, 0.18782902183011174, 0.08628835203126073, 0.08757788385264575, 0.08853289508260787, 0.08661541901528835, 0.08935973816551268, 0.08666541101410985, 0.08494064304977655, 0.0854553859680891, 0.08795131300576031, 0.08939407579600811, 0.08841505693271756, 0.09368856810033321, 0.08401955291628838, 0.10370001499541104, 0.09022161597386003, 0.09257938200607896, 0.08985321805812418, 0.09044827404431999, 0.0897187979426235, 0.09142533596605062, 0.09127300907857716, 0.09015703783370554, 0.0908733441028744, 0.09184172097593546, 0.09203068003989756, 0.09210269805043936, 0.090764642925933, 0.09244918404147029, 0.0923298648558557, 0.09192892699502409, 0.09144886396825314, 0.09264862490817904, 0.0923298851121217, 0.09398985607549548, 0.0895528809633106, 0.09013208793476224, 0.0882149210665375, 0.08907083095982671, 0.08856367995031178, 0.09030620404519141, 0.08784094615839422, 0.10179581516422331, 0.10262293391861022, 0.10003199405036867, 0.09984514699317515, 0.10123768704943359, 0.09926067083142698, 0.09932352206669748, 0.09942462295293808, 0.5569795970804989, 2.7246354471426457, 0.09131729300133884, 0.08823105390183628, 0.09012799407355487, 0.09088687296025455, 0.08814184297807515, 0.08781369193457067, 0.08671797788701952, 0.08712142705917358, 0.08779424196109176, 0.0986485870089382, 0.0892530819401145, 0.08841069019399583, 0.09005871694535017, 0.09048103704117239, 0.08919897116720676, 0.08833276317454875, 0.08915791194885969, 0.08926596399396658, 0.09173791809007525, 0.09480129112489522, 0.09638373996131122, 0.09612559899687767, 0.09616988408379257, 0.08894405397586524, 0.10091914003714919, 0.10011113598011434, 0.08912738994695246, 0.09004595410078764, 0.0904825848992914, 0.2178799589164555, 0.08986485889181495, 0.09043143410235643, 0.08958013798110187, 0.08721459493972361, 0.08684592810459435, 0.08736448595300317, 0.08789149206131697, 0.08748801914043725, 0.0937425410374999, 0.09378761192783713, 0.09584449208341539, 0.09613060601986945, 0.09268487012013793, 0.09268352505750954, 0.09338273108005524, 0.09456400689668953, 0.09126216592267156, 0.09358339011669159, 0.0931904399767518, 0.09033014299347997, 0.08838013489730656, 0.24800803395919502, 0.09147851914167404, 0.10560987493954599, 0.09129463112913072, 0.09046376589685678, 0.09230041201226413, 0.08562249992974102, 0.08651936403475702, 0.08902488299645483, 0.08923604409210384, 0.09062997903674841, 0.09189029596745968, 0.092561966041103, 0.08774903486482799, 0.09525380097329617, 0.20025569712743163, 0.08594889496453106, 0.08517716592177749, 0.08840970904566348, 0.08558635483495891, 0.09341942798346281, 0.0888906188774854, 0.0886868410743773, 0.09091642196290195, 0.09156499290838838, 0.08924712194129825, 0.08722871891222894, 0.0880540159996599, 0.09563803602941334, 0.09027887601405382, 0.08423687401227653, 0.084726857021451, 0.08402736019343138, 0.08892334788106382, 0.09063299698755145, 0.09090058412402868, 0.09120970591902733, 0.0857237009331584, 0.0840877341106534, 0.08718823897652328, 0.08439680794253945, 0.08413986512459815, 0.08556378702633083, 0.09017784288153052, 0.09563718410208821, 0.09008516510948539, 0.08753188978880644, 0.08799616014584899, 0.08933024108409882, 0.08940889313817024, 0.08887579804286361, 0.08743605809286237, 0.08303979900665581, 0.08485453901812434, 0.0864333058707416, 0.08518877299502492, 0.08577394904568791, 0.08789385296404362, 0.0895198080688715, 0.08916491805575788, 0.08519336907193065, 0.0849356590770185, 0.08210204099304974, 0.0817939261905849, 0.09484305582009256, 0.08966837683692575, 0.0894801071844995, 0.08993481402285397, 0.0923745830077678, 0.08792797685600817, 0.08717930503189564, 0.08888060902245343, 0.08649818901903927, 0.08564080600626767, 0.10690922196954489, 0.0874949439894408, 0.09063481912016869, 0.08903653989546001, 0.08910436811856925, 0.08902157586999238, 0.09027429902926087, 0.09121564915403724, 0.08999225404113531, 0.089335469994694, 0.0932247459422797, 0.09112851601094007, 0.09061161801218987, 0.0917471619322896, 0.0902447341941297, 0.0909950309433043, 0.0896555338986218, 0.09108614502474666, 0.09032845892943442, 0.09148923796601593, 0.08765552705153823, 0.08641371200792491, 0.0804567439481616, 0.08239940297789872, 0.08748382097110152, 0.08668996603228152, 0.08714944613166153, 0.08891101786866784, 0.08712116093374789, 0.0901472601108253, 0.09429381089285016, 0.09316179994493723, 0.0873441998846829, 0.08540257881395519, 0.09026556089520454, 0.08470536908134818, 0.08473743894137442, 0.08932403498329222, 0.18332226108759642, 0.08988742297515273, 0.08791261585429311, 0.09143457585014403, 0.09184613195247948, 0.08872426208108664, 0.09052508301101625, 0.10513168200850487, 0.08658223995007575, 0.09121732297353446, 0.09010341879911721, 0.09103922895155847, 0.09324498381465673, 0.08807507110759616, 0.0840310628991574, 0.08577689598314464, 0.08979104203172028, 0.08936674194410443, 0.08911288087256253, 0.08968993206508458, 0.09046956803649664, 0.08989140205085278, 0.09082326898351312, 0.08510791510343552, 0.08691111695952713, 0.08860456990078092, 0.08624452189542353, 0.08913343702442944, 0.0895353858359158, 0.08915909798815846, 0.08919753110967577, 0.10019061714410782, 0.08943281695246696, 0.08760947291739285, 0.08985246787779033, 0.09598351386375725, 0.0895898079033941, 0.08804379892535508]
[0.0018851518374392573, 0.0018922579189648433, 0.0018142319987623058, 0.0019218472013135953, 0.0018405526144696133, 0.0017912242637605083, 0.0017806918797444325, 0.004229602161604835, 0.0018079147569607108, 0.001820305162774665, 0.001823444736703318, 0.0018123267123437657, 0.0018262691434700877, 0.001799282389787995, 0.0017872565492455447, 0.0017467480228871715, 0.0017698207366451317, 0.0018526092884415875, 0.0018046204884517559, 0.0017785085507725574, 0.0017789343885165087, 0.0019943831840111893, 0.0018020551224068111, 0.0018208602262774901, 0.0017903395310728525, 0.001805067936680755, 0.0018638953690093998, 0.0019441230403145356, 0.0020408924925616203, 0.0018651399200744166, 0.0017899253443643755, 0.0017637263280244507, 0.0018115301416920765, 0.0017755597364157438, 0.0017705892868416042, 0.0017407400039385777, 0.0018084560834555601, 0.001763420307780711, 0.0017792932660680037, 0.0017541509587317705, 0.0017982568373257409, 0.001886270859525824, 0.0018116430835608318, 0.0018362524938218448, 0.0019705469187881264, 0.00182394936623774, 0.0018359442450562302, 0.0018252819795541617, 0.001831919002365701, 0.0017865840203071736, 0.0018498611218314997, 0.0019334197586060179, 0.0017975725908288543, 0.0018289597961595472, 0.002039001879225276, 0.0018275416532189262, 0.0018155148385890893, 0.0018338430577851071, 0.001768942143083835, 0.0018128422659118564, 0.0018326444076184108, 0.001804226591270797, 0.0016279229419115856, 0.0019337502640805074, 0.001770210818254522, 0.0017908071832997458, 0.001869935608868088, 0.0018183551633692517, 0.0018191643258822815, 0.0018160206322767297, 0.0018056273669460599, 0.0017649967708073709, 0.0017646707128733397, 0.0017893164494664085, 0.0018606361628947209, 0.0017743012012571705, 0.0017707914075985247, 0.0018036913468825575, 0.0018125420616825624, 0.0018359393651160051, 0.0018541690446816537, 0.0018390711843587306, 0.001865275365737628, 0.001857523141162736, 0.0018293350621373678, 0.0019074845488886445, 0.0018335015950154286, 0.0017919773663565212, 0.0019305502872305866, 0.001926667798234492, 0.0018297756299832646, 0.0018880304272229575, 0.001830745816268787, 0.0018475821422298951, 0.001760364472105795, 0.0017745069570231195, 0.0018352075071283141, 0.0017667232860563969, 0.0018469986306237324, 0.001761516979040236, 0.001779318753896015, 0.0017754381418000071, 0.0018118742463768137, 0.0018414723240219209, 0.002071204224639401, 0.0022815978962319847, 0.002070755241628812, 0.004795975427200295, 0.002078440140134522, 0.002045547817738689, 0.002027838856780103, 0.0018198167363523829, 0.0017986908336455117, 0.0017275879188079614, 0.0019863684500130464, 0.0019314439007442218, 0.0018869809217142816, 0.0019897005938905843, 0.001895986183793569, 0.0017787588152046107, 0.001818288996701642, 0.0018500249158134874, 0.00176948726614367, 0.001748582571554853, 0.0019600646313736023, 0.0018372950001562737, 0.0017660071417613297, 0.0019095746933349542, 0.004858209936860569, 0.0021378078155827765, 0.0018024632412636159, 0.0019288660806356644, 0.0018241572459893568, 0.0018338120390413975, 0.001845258326098627, 0.0018382479189609994, 0.001818714549346846, 0.0019413539379531024, 0.001835146591029301, 0.0018259489395636686, 0.0020935460022289535, 0.00192055212124726, 0.001867150431688951, 0.0018169770625476934, 0.001768530184897233, 0.0017558194088692569, 0.001776969302635716, 0.0018180720650647975, 0.0018042041444960907, 0.001870018919473704, 0.0018059533298471753, 0.001856654122167704, 0.003442103998279389, 0.0018905039201965745, 0.0018446357131992675, 0.002150569106357134, 0.0019059140444258038, 0.0019511428794690541, 0.0019397284094320268, 0.0017607484098371802, 0.0017513958974836432, 0.0018057289618846712, 0.0018338234667495197, 0.0018092086918804111, 0.0017324423712051036, 0.0020461266299671666, 0.0018532663697794992, 0.0018399876532886102, 0.003833245343471668, 0.001760986776148178, 0.0017873037520948114, 0.0018067937771960789, 0.0017676616125569052, 0.0018236681258267894, 0.0017686818574308132, 0.0017334825112199296, 0.0017439874687365123, 0.0017949247552195983, 0.0018243688937960838, 0.001804388916994236, 0.0019120115938843513, 0.0017146847533936404, 0.0021163268366410416, 0.0018412574688542861, 0.0018893751429812033, 0.0018337391440433506, 0.0018458831437616324, 0.0018309958763800714, 0.001865823182980625, 0.0018627144709913706, 0.0018399395476266438, 0.001854558042915804, 0.0018743208362435807, 0.0018781771436713788, 0.0018796468989885583, 0.0018523396515496531, 0.001886718041662659, 0.0018842829562419532, 0.0018761005509188588, 0.0018663033462908802, 0.0018907882634322255, 0.0018842833696351368, 0.0019181603280713363, 0.0018276098155777674, 0.001839430366015556, 0.0018003045115619898, 0.0018177720604046267, 0.0018074220398022812, 0.0018429837560243144, 0.0017926723705794737, 0.0020774656155963943, 0.0020943455901757187, 0.0020414692663340544, 0.0020376560610852073, 0.002066075245906808, 0.0020257279761515708, 0.0020270106544223974, 0.0020290739378150627, 0.011366930552663244, 0.055604805043727466, 0.0018636182245171191, 0.0018006337530986996, 0.0018393468178276504, 0.0018548341420460113, 0.0017988131220015337, 0.0017921161619300137, 0.0017697546507555004, 0.0017779883073300732, 0.00179171922369575, 0.0020132364695701673, 0.001821491468165602, 0.001804299799877466, 0.0018379329988846974, 0.0018465517763504569, 0.0018203871666776891, 0.001802709452541811, 0.0018195492234461162, 0.0018217543672238077, 0.0018722024100015359, 0.0019347202270386778, 0.0019670151012512495, 0.0019617469183036257, 0.0019626506955876034, 0.001815184775017658, 0.0020595742864724324, 0.0020430844077574356, 0.0018189263254480095, 0.0018376725326691354, 0.0018465833652916612, 0.004446529773805215, 0.001833976712077856, 0.001845539471476662, 0.001828166081246977, 0.0017798896926474205, 0.001772365879685599, 0.001782948692918432, 0.0017937039196187137, 0.0017854697783762704, 0.0019131130823979573, 0.001914032896486472, 0.0019560100425186815, 0.001961849102446315, 0.001891527961635468, 0.0018915005113777457, 0.0019057700220419436, 0.0019298776917691743, 0.001862493182095338, 0.0019098651044222774, 0.0019018457138112613, 0.0018818779790308326, 0.0018412528103605534, 0.005166834040816563, 0.0019058024821182091, 0.0022002057279072082, 0.00190197148185689, 0.0018846617895178497, 0.0019229252502555028, 0.0017838020818696048, 0.0018024867507241045, 0.0018546850624261424, 0.00185908425191883, 0.0018881245632655919, 0.0019143811659887433, 0.0019283742925229792, 0.0018281048930172499, 0.0019844541869436703, 0.004171993690154825, 0.0017906019784277305, 0.001774524290037031, 0.0018418689384513225, 0.001783049059061644, 0.0019462380829888086, 0.0018518878932809457, 0.0018476425223828603, 0.001894092124227124, 0.0019076040189247578, 0.0018593150404437135, 0.0018172649773381029, 0.0018344586666595812, 0.001992459083946111, 0.0018808099169594545, 0.001754934875255761, 0.0017651428546135624, 0.0017505700040298204, 0.001852569747522163, 0.0018881874372406553, 0.0018937621692505975, 0.0019002022066464026, 0.0017859104361074667, 0.0017518277939719458, 0.001816421645344235, 0.0017582668321362387, 0.0017529138567624614, 0.0017825788963818923, 0.001878705060031886, 0.001992441335460171, 0.001876774273114279, 0.0018235810372668009, 0.001833253336371854, 0.0018610466892520587, 0.00186268527371188, 0.001851579125892992, 0.0018215845436012994, 0.0017299958126386628, 0.0017678028962109238, 0.0018006938723071169, 0.0017747661040630192, 0.001786957271785165, 0.0018311219367509086, 0.0018649960014348228, 0.0018576024594949558, 0.0017748618556652218, 0.0017694928974378854, 0.0017104591873552029, 0.0017040401289705187, 0.001975896996251928, 0.0018680911841026198, 0.0018641688996770729, 0.0018736419588094577, 0.0019244704793284957, 0.001831832851166837, 0.0018162355214978259, 0.0018516793546344463, 0.0018020456045633182, 0.0017841834584639098, 0.002227275457698852, 0.00182281133311335, 0.0018882253983368476, 0.0018549279144887503, 0.0018563410024701927, 0.0018546161639581744, 0.0018807145631096016, 0.0019003260240424424, 0.0018748386258569856, 0.0018611556248894583, 0.001942182207130827, 0.0018985107502279182, 0.0018877420419206221, 0.0019113992069227, 0.0018800986290443689, 0.0018957298113188397, 0.0018678236228879541, 0.0018976280213488887, 0.001881842894363217, 0.0019060257909586653, 0.0018261568135737132, 0.001800285666831769, 0.0016761821655867, 0.0017166542287062232, 0.0018225796035646151, 0.001806040959005865, 0.001815613461076282, 0.0018523128722639133, 0.0018150241861197476, 0.0018780679189755272, 0.001964454393601045, 0.0019408708321861923, 0.0018196708309308935, 0.0017792203919573997, 0.0018805325186500947, 0.0017646951891947538, 0.0017653633112786338, 0.0018609173954852547, 0.0038192137726582587, 0.0018726546453156818, 0.001831512830297773, 0.0019048869968780007, 0.0019134610823433225, 0.0018484221266893048, 0.0018859392293961719, 0.0021902433751771846, 0.001803796665626578, 0.001900360895281968, 0.0018771545583149418, 0.0018966506031574681, 0.0019426038294720154, 0.0018348973147415866, 0.001750647143732446, 0.0017870186663155134, 0.0018706467089941725, 0.001861807123835509, 0.0018565183515117194, 0.0018685402513559286, 0.0018847826674270134, 0.0018727375427260995, 0.0018921514371565233, 0.0017730815646549065, 0.0018106482699901487, 0.0018459285395996023, 0.0017967608728213236, 0.0018569466046756133, 0.0018653205382482458, 0.0018574812080866347, 0.0018582818981182452, 0.0020873045238355794, 0.0018631836865097284, 0.0018251973524456844, 0.001871926414120632, 0.001999656538828276, 0.0018664543313207105, 0.0018342458109448974]
[530.4612499321937, 528.4691848704474, 551.1974216540187, 520.3327295304712, 543.3150849035452, 558.2773861607889, 561.5794688430441, 236.42885590463447, 553.1234236292767, 549.3584375027066, 548.4125621530718, 551.7768916547967, 547.5644176410402, 555.7771285239041, 559.5167635122839, 572.4924184239901, 565.0289768304997, 539.7792217921992, 554.1331301507795, 562.268873863786, 562.1342790691236, 501.4081586813006, 554.9219818894372, 549.1909733480031, 558.553270284299, 553.9957691779999, 536.5108023909452, 514.370736452057, 489.9817132184425, 536.15280507218, 558.6825188818768, 566.9813871407702, 552.0195203961337, 563.2026788457496, 564.7837177326485, 574.4683282612061, 552.9578567864478, 567.0797798957606, 562.0208984491152, 570.0763637372394, 556.094090256395, 530.1465560738094, 551.9851062685449, 544.5874156002759, 507.4733265498664, 548.2608336122289, 544.6788499665863, 547.8605559039471, 545.8756630116404, 559.7273840096625, 540.581121576265, 517.2182582436175, 556.3057676234947, 546.7588746892095, 490.43603646895735, 547.1831507854597, 550.8079464539882, 545.3029340513946, 565.3096139462641, 551.6199720205673, 545.6595921407018, 554.2541080140358, 614.279689937751, 517.1298582733469, 564.9044676983888, 558.4074094215981, 534.777772698452, 549.9475680796529, 549.7029519392138, 550.6545367528719, 553.8241268968723, 566.5732745463127, 566.6779602024118, 558.8726355800337, 537.4505881065057, 563.6021659070376, 564.7192524816683, 554.4185826074778, 551.7113346719862, 544.6802977269429, 539.325150998674, 543.7527424196427, 536.113872712058, 538.3513011708912, 546.6467137144439, 524.2506423355454, 545.404488721803, 558.0427625786463, 517.9870250541468, 519.0308370318709, 546.5150937708937, 529.6524810094652, 546.2254733090602, 541.2479245946131, 568.064179802364, 563.5368157009549, 544.8975094727978, 566.0195956505202, 541.418917924319, 567.6925127028017, 562.0128477881715, 563.241250965895, 551.9146828206702, 543.0437302559732, 482.8109116927381, 438.28932418437137, 482.9155951880731, 208.50815755404344, 481.13004588877766, 488.866596677989, 493.1358311122642, 549.5058815671665, 555.9599133405499, 578.8417417794874, 503.4312742902416, 517.7473700451155, 529.9470643781186, 502.58817988521497, 527.4300037351315, 562.189764824845, 549.9675804088293, 540.5332606346456, 565.1354599343057, 571.891780386895, 510.1872581105683, 544.2784092456267, 566.2491256986926, 523.6768184508991, 205.83713198820953, 467.76889517891266, 554.7963348750184, 518.439310037764, 548.1983541707427, 545.3121577949382, 541.9295422523682, 543.9962638800171, 549.8388960263883, 515.104422975218, 544.9155968729004, 547.6604401867675, 477.6584794102071, 520.6836039162386, 535.575486060563, 550.3646802221265, 565.4412961337772, 569.5346542751782, 562.7559229733092, 550.0332023221308, 554.2610036955089, 534.753947987563, 553.7241652222667, 538.6032799865101, 290.51998443390204, 528.9594955698478, 542.1124576763383, 464.993195077515, 524.6826334716845, 512.5201288550024, 515.536090071914, 567.9403112977805, 570.9731314529012, 553.7929673323079, 545.3087596116956, 552.7278331614937, 577.2197774777295, 488.72830515677646, 539.5878413953951, 543.4819077251415, 260.87555332274314, 567.8634351742884, 559.5019866253562, 553.466595148383, 565.7191358890856, 548.3453846881454, 565.3928069645038, 576.8734287929187, 573.3986154868888, 557.1264183036219, 548.1347568469196, 554.2042464247714, 523.009380904667, 583.1975807919428, 472.51680727498797, 543.1070976848475, 529.2755140316508, 545.3338351031892, 541.7461031483014, 546.1508749965222, 535.9564663584653, 536.8509321065089, 543.4961171903228, 539.2120261859064, 533.5265876914378, 532.4311412102713, 532.0148164732972, 539.8577950665839, 530.0209029213269, 530.7058564040814, 533.0204713762433, 535.8185752532755, 528.879948823442, 530.7057399724517, 521.332854905552, 547.1627430956138, 543.6465649776835, 555.4615864026107, 550.1239796684988, 553.2742093315364, 542.5983797910409, 557.8264140238599, 481.35574061615563, 477.47611697460997, 489.843279294495, 490.75995654900845, 484.0094773804312, 493.6496961945383, 493.33731809365, 492.83566328628467, 87.97449719314969, 17.984057298889958, 536.5905885896289, 555.3600215919013, 543.6712588988759, 539.1317624210488, 555.9221176279297, 557.9995433571969, 565.0500760504314, 562.433395021397, 558.1231628119256, 496.712639133496, 549.000650004189, 554.2316194170793, 544.0894747560571, 541.5499380019605, 549.3336902748316, 554.7205616468062, 549.5866707612672, 548.9214232124561, 534.1302813509249, 516.8705976318965, 508.3845057233593, 509.74974940433515, 509.51501571226214, 550.9081024493895, 485.5372328971757, 489.45603823467906, 549.774878734407, 544.166592372987, 541.5406738715279, 224.89447971113623, 545.2631941367574, 541.8469859113201, 546.996255021813, 561.8325698108813, 564.2175870466377, 560.8686351838554, 557.5056111894818, 560.076688001638, 522.7082545202022, 522.4570600827537, 511.24481892349445, 509.7231987684763, 528.6731257915807, 528.6807981202248, 524.7222846587474, 518.1675524127497, 536.9147171185788, 523.5971889765974, 525.8050075976035, 531.3840807654278, 543.1084717824167, 193.54211730051247, 524.7133474653403, 454.5029527539591, 525.7702386913302, 530.5991799493258, 520.0410155658043, 560.6003099580975, 554.7890987815997, 539.1750978421558, 537.8992366633533, 529.6260741772552, 522.3620132532583, 518.5715262215276, 547.0145634529321, 503.9168989535285, 239.6935552323161, 558.4714034986533, 563.531311244622, 542.9267952370262, 560.8370644194529, 513.81175239584, 539.9894905238156, 541.2302368481561, 527.9574246728076, 524.217809398232, 537.8324696181431, 550.2774842801307, 545.1199409256404, 501.892364092856, 531.6858396921975, 569.8217148110763, 566.5263847548061, 571.2425082675902, 539.7907427439715, 529.6084383769507, 528.0494120313542, 526.2597825127586, 559.9384939927779, 570.8323634554769, 550.5329682472966, 568.7418892984699, 570.4786896071132, 560.9849875535406, 532.2815279919604, 501.89683490432185, 532.829128321661, 548.3715719586604, 545.4783472419994, 537.3320324391716, 536.8593471548967, 540.0795386034142, 548.9725983417632, 578.0360811826231, 565.6739233448376, 555.3414799589241, 563.4545294225946, 559.6104707086829, 546.1132761996025, 536.1941790924251, 538.3283139449975, 563.424131747537, 565.1336614280493, 584.6383283463488, 586.840640075854, 506.09925613374395, 535.3057754942369, 536.4320798256148, 533.7199005915816, 519.6234552524439, 545.9013355738336, 550.5893856625561, 540.0503048744189, 554.924912814471, 560.4804793229888, 448.9790414308104, 548.603128493834, 529.597791069224, 539.1045076140423, 538.6941292948449, 539.1951280451323, 531.7127966226757, 526.2254935986003, 533.37923926274, 537.3005817605361, 514.8847499109229, 526.7286476412888, 529.7333946022532, 523.1769461754525, 531.8869896247346, 527.501331692574, 535.3824567513715, 526.9736685745034, 531.3939877740871, 524.6518723637179, 547.598099225138, 555.4674007708171, 596.5938670215945, 582.5284925046681, 548.6728799357747, 553.6972985100238, 550.7780270626588, 539.8655999068835, 550.9568454500056, 532.4621063467678, 509.0471956271268, 515.2326385747177, 549.5499422213783, 562.0439179543437, 531.7642689411356, 566.67010037938, 566.4556375512929, 537.3693654678525, 261.83399503819277, 534.0012919634872, 545.99672110264, 524.9655237496723, 522.6131899036842, 541.0019635455742, 530.2397788926495, 456.57026581308696, 554.3862116258175, 526.2158374668218, 532.7211846091492, 527.2452387040818, 514.7729994292208, 544.9896252863788, 571.2173373030286, 559.5912448199584, 534.5744844239936, 537.1125650974521, 538.6426690507657, 535.1771251779768, 530.5651507105257, 533.9776542015193, 528.4989247492655, 563.9898467923156, 552.2883801200334, 541.7327803041111, 556.5570884398057, 538.5184460781456, 536.1008896300026, 538.3634545784105, 538.1314864082955, 479.0867784650917, 536.7157340633889, 547.8859580088936, 534.2090332486528, 500.08588004116075, 535.7752307244485, 545.1831995651977]
Elapsed: 0.10018924801751988~0.13129962988815794
Time per graph: 0.0020587323745420503~0.0026790962379371914
Speed: 531.0581601817871~59.9847974716152
Total Time: 0.0893
best val loss: 0.17794130742549896 test_score: 0.9375

Testing...
Test loss: 0.6530 score: 0.8125 time: 0.08s
test Score 0.8125
Epoch Time List: [0.3944286396726966, 0.3265792690217495, 0.33373961318284273, 0.33247716003097594, 0.3256023209542036, 0.313202099641785, 0.31486809928901494, 0.42849059915170074, 0.3136003268882632, 0.3209924118127674, 0.31533136311918497, 0.3194416519254446, 0.3189978750888258, 0.32083444483578205, 0.3197097971569747, 0.30616185907274485, 0.3118660179898143, 0.3370774262584746, 0.329371121712029, 0.33165341592393816, 0.32496556430123746, 0.3262148310896009, 0.3068312150426209, 0.32139254128560424, 0.3119534910656512, 0.31794219207949936, 0.3295165719464421, 0.31241299910470843, 0.3244255380704999, 0.33130609313957393, 0.3108803366776556, 0.31296571413986385, 0.3028184117283672, 0.30762363923713565, 0.30715922615490854, 0.3043460950721055, 0.30791961890645325, 0.30012724408879876, 0.29813182004727423, 0.30006941594183445, 0.31553454301320016, 0.3219171760138124, 0.3171825523022562, 0.30888749985024333, 0.31494595808908343, 0.30955043877474964, 0.31979612191207707, 0.3159781419672072, 0.3313588828314096, 0.3271702993661165, 0.3325364328920841, 0.33099319390021265, 0.32596855401061475, 0.31559884105809033, 0.4111499229911715, 0.3202768031042069, 0.31779479375109076, 0.3148329029791057, 0.3130052969790995, 0.3137565969955176, 0.318698224844411, 0.3123018878977746, 0.4069000561721623, 0.3053554801736027, 0.30579477408900857, 0.3017322460655123, 0.3157752288971096, 0.3039442766457796, 0.3040503079537302, 0.30720041994936764, 0.4163598781451583, 0.3038248971570283, 0.30278034997172654, 0.30900313914753497, 0.3202691311016679, 0.3087431329768151, 0.3049145289696753, 0.30580599000677466, 0.4104957371018827, 0.3103020691778511, 0.3310665839817375, 0.3242657820228487, 0.3194034178741276, 0.3251930898986757, 0.31259172898717225, 0.32520848512649536, 0.33863706211559474, 0.31783980457112193, 0.33494991110637784, 0.32845502789132297, 0.32818651874549687, 0.3085983272176236, 0.31436308519914746, 0.3123734160326421, 0.43101109587587416, 0.31729108607396483, 0.31424672715365887, 0.30448735714890063, 0.30527249188162386, 0.31394068989902735, 0.32913225912488997, 0.46229023090563715, 0.32116860104724765, 0.31675202888436615, 0.3439998719841242, 0.3600955680012703, 0.36222268594428897, 0.48898637224920094, 0.3557972810231149, 0.3562939870171249, 0.353372807148844, 0.325491817900911, 0.32348638703115284, 0.30541313788853586, 0.4523427567910403, 0.4364569941535592, 0.32888247119262815, 0.32805662089958787, 0.35421790019609034, 0.3248003839980811, 0.5334707333240658, 0.45019044703803957, 0.3153932101558894, 0.3037351821549237, 0.3105179921258241, 0.3244987758807838, 0.3185082252603024, 0.3347621476277709, 0.5078657518606633, 0.32225874601863325, 0.29561311192810535, 0.3022374650463462, 0.2952413728926331, 0.3051980340387672, 0.30353129212744534, 0.2947505966294557, 0.4015268830116838, 0.31161949620582163, 0.29533365881070495, 0.297713277162984, 0.3220052469987422, 0.3067408341448754, 0.29358503804542124, 0.2884171169716865, 0.40221720305271447, 0.2792886779643595, 0.2883551591075957, 0.29399277712218463, 0.3029782196972519, 0.29762454610317945, 0.3037337251007557, 0.31154682906344533, 0.382427244912833, 0.30484846723265946, 0.3051655348390341, 0.31718229805119336, 0.32802548562176526, 0.31891793268732727, 0.3106306951958686, 0.2979091580491513, 0.4362128262873739, 0.2903813128359616, 0.294480457669124, 0.29116850811988115, 0.28854499431326985, 0.3052022682968527, 0.28794956789352, 0.2959694860037416, 0.3981167692691088, 0.2831952553242445, 0.28646621108055115, 0.2943234408739954, 0.292712606722489, 0.28796572936698794, 0.2914401071611792, 0.29160174797289073, 0.415902613196522, 0.2883592671714723, 0.28691700170747936, 0.28803424583747983, 0.31475574779324234, 0.28077902109362185, 0.3284623371437192, 0.3008840379770845, 0.3045612499117851, 0.3003803489264101, 0.30434202891774476, 0.3017365199048072, 0.30261278175748885, 0.30860042897984385, 0.3092235561925918, 0.3037186509463936, 0.3035402800887823, 0.30693526519462466, 0.31091480678878725, 0.3027298718225211, 0.3086266617756337, 0.3098683129064739, 0.3008405459113419, 0.30733103095553815, 0.3082519769668579, 0.43951705703511834, 0.30002856999635696, 0.2956014322116971, 0.3089313230011612, 0.30184974893927574, 0.30041420087218285, 0.2991257330868393, 0.30366859678179026, 0.2962870318442583, 0.33172489097341895, 0.34698898904025555, 0.34088755887933075, 0.33598769921809435, 0.3303664398845285, 0.33618689980357885, 0.3330566661898047, 0.333525083726272, 0.802665316965431, 6.56699580908753, 1.043296131072566, 0.2973352789413184, 0.3038388581480831, 0.30607850500382483, 0.3056854489259422, 0.29959060112014413, 0.3874842990189791, 0.29683397873304784, 0.2985736532136798, 0.30475084483623505, 0.2915035751648247, 0.2945584338158369, 0.29787558503448963, 0.30436883424408734, 0.3945771201979369, 0.30272734886966646, 0.30882849264889956, 0.29820040659978986, 0.31085559516213834, 0.3218087439890951, 0.3262665180955082, 0.3232443162705749, 0.43047762964852154, 0.3086019952315837, 0.33337396499700844, 0.32751533505506814, 0.3050859719514847, 0.3158601741306484, 0.3068368877284229, 0.4412787100300193, 0.3029911641497165, 0.30518996505998075, 0.3144284419249743, 0.30426712497137487, 0.29495044983923435, 0.2994974278844893, 0.2958617168478668, 0.4003306389786303, 0.31318058795295656, 0.3160593290813267, 0.31740878196433187, 0.32974045211449265, 0.31830316898413, 0.31044878577813506, 0.3135382120963186, 0.3557762918062508, 0.31061762012541294, 0.312261019134894, 0.31052404711954296, 0.32878199173137546, 0.3219121468719095, 0.4814364730846137, 0.3376038421411067, 0.34968826780095696, 0.3317931250203401, 0.32856825506314635, 0.33072110801003873, 0.32020726799964905, 0.30790174985304475, 0.3752826051786542, 0.3300399109721184, 0.3346328679472208, 0.3338973808567971, 0.32739287498407066, 0.3129530898295343, 0.3466210237238556, 0.4324302510358393, 0.3145559332333505, 0.30953080393373966, 0.31237371009774506, 0.31824946869164705, 0.3175924210809171, 0.3218413826543838, 0.3289495180360973, 0.43913694424554706, 0.3267879798077047, 0.34773025498725474, 0.3155207831878215, 0.31623868690803647, 0.32730573578737676, 0.4877923522144556, 0.38321830914355814, 0.31162984017282724, 0.31117719784379005, 0.3160724397748709, 0.32398260082118213, 0.3336808839812875, 0.33395171305164695, 0.43304438586346805, 0.31711962493136525, 0.31590140983462334, 0.31207341398112476, 0.3131008171476424, 0.3161306050606072, 0.33001983095891774, 0.3286279740277678, 0.4590804271865636, 0.3338582729920745, 0.32255696691572666, 0.3228948670439422, 0.3147195929195732, 0.32377101364545524, 0.3150816964916885, 0.44567476003430784, 0.3094838459510356, 0.3258279641158879, 0.3149068206548691, 0.3238844929728657, 0.3250331715680659, 0.3191498110536486, 0.47188316588290036, 0.30804888322018087, 0.3059227450285107, 0.3113028728403151, 0.2944397390820086, 0.32945120381191373, 0.33621116005815566, 0.31907938700169325, 0.33718990138731897, 0.3335133120417595, 0.33292438508942723, 0.31318611511960626, 0.32242505508475006, 0.31656608497723937, 0.31697081681340933, 0.3365799479652196, 0.32219202211126685, 0.3336085129994899, 0.34016574709676206, 0.33613800490275025, 0.31134027894586325, 0.3231001850217581, 0.3310013939626515, 0.3285070490092039, 0.3191794930025935, 0.34478348307311535, 0.3338268499355763, 0.3328365199267864, 0.3264553400222212, 0.3344959137029946, 0.3264098570216447, 0.3280935902148485, 0.32737419265322387, 0.3309392100200057, 0.3295841231010854, 0.3245994520839304, 0.3118278346955776, 0.2994597696233541, 0.2992462220136076, 0.4420409530866891, 0.30859500309452415, 0.31846263399347663, 0.3182348469272256, 0.32074711192399263, 0.3347680710721761, 0.32558188075199723, 0.4224821128882468, 0.33005623891949654, 0.3123836989980191, 0.3211477091535926, 0.3175452691502869, 0.30822838190943, 0.3257959960028529, 0.46474600420333445, 0.32596205291338265, 0.3232762690167874, 0.33046286390163004, 0.334203114034608, 0.33158621983602643, 0.32494994695298374, 0.44479114282876253, 0.32062914804555476, 0.3215210170019418, 0.3222082338761538, 0.3200064490083605, 0.330945584224537, 0.3339067318011075, 0.3031144707929343, 0.3027408851776272, 0.4550291928462684, 0.32037148298695683, 0.3173136829864234, 0.3312111501581967, 0.3245817527640611, 0.3276571580208838, 0.3242292320355773, 0.3867278411053121, 0.3243478280492127, 0.32519525405950844, 0.3141166982240975, 0.32583581283688545, 0.3217951348051429, 0.32824711594730616, 0.3236868139356375, 0.36808106722310185, 0.3265154000837356, 0.3211731722112745, 0.31721214414574206, 0.33127979398705065, 0.32031065900810063, 0.3267473678570241]
Total Epoch List: [129, 140, 152]
Total Time List: [0.23868092894554138, 0.09425130393356085, 0.0892531790304929]
========================training times:2========================
========================k_idx:0========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: SGFormer
SGFormer(
  (trans_conv): SGModule(
    (attns): ModuleList(
      (0-1): 2 x SGFormerAttention(heads=1, head_channels=64)
    )
    (fcs): ModuleList(
      (0): Linear(in_features=14887, out_features=64, bias=True)
    )
    (bns): ModuleList(
      (0-2): 3 x LayerNorm((64,), eps=1e-05, elementwise_affine=True)
    )
  )
  (graph_conv): GraphModule(
    (convs): ModuleList(
      (0-2): 3 x GCNConv(64, 64)
    )
    (fcs): ModuleList(
      (0): Linear(in_features=14887, out_features=64, bias=True)
    )
    (bns): ModuleList(
      (0-3): 4 x BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (classifier): Linear(in_features=64, out_features=2, bias=True)
)
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7f342d3771c0>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.7398;  Loss pred: 0.7398; Loss self: 0.0000; time: 0.13s
Val loss: 0.7081 score: 0.3878 time: 0.09s
Test loss: 0.7028 score: 0.4082 time: 0.23s
Epoch 2/1000, LR 0.000000
Train loss: 0.7176;  Loss pred: 0.7176; Loss self: 0.0000; time: 0.14s
Val loss: 0.7071 score: 0.3878 time: 0.09s
Test loss: 0.7024 score: 0.4082 time: 0.09s
Epoch 3/1000, LR 0.000030
Train loss: 0.7365;  Loss pred: 0.7365; Loss self: 0.0000; time: 0.13s
Val loss: 0.7052 score: 0.3878 time: 0.09s
Test loss: 0.7011 score: 0.4082 time: 0.08s
Epoch 4/1000, LR 0.000060
Train loss: 0.7174;  Loss pred: 0.7174; Loss self: 0.0000; time: 0.13s
Val loss: 0.7026 score: 0.4082 time: 0.09s
Test loss: 0.6992 score: 0.4286 time: 0.08s
Epoch 5/1000, LR 0.000090
Train loss: 0.7040;  Loss pred: 0.7040; Loss self: 0.0000; time: 0.13s
Val loss: 0.6990 score: 0.4694 time: 0.09s
Test loss: 0.6965 score: 0.4490 time: 0.08s
Epoch 6/1000, LR 0.000120
Train loss: 0.7082;  Loss pred: 0.7082; Loss self: 0.0000; time: 0.13s
Val loss: 0.6950 score: 0.5306 time: 0.10s
Test loss: 0.6936 score: 0.4694 time: 0.08s
Epoch 7/1000, LR 0.000150
Train loss: 0.6922;  Loss pred: 0.6922; Loss self: 0.0000; time: 0.13s
Val loss: 0.6912 score: 0.5102 time: 0.09s
Test loss: 0.6906 score: 0.4898 time: 0.08s
Epoch 8/1000, LR 0.000180
Train loss: 0.6553;  Loss pred: 0.6553; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6876 score: 0.4898 time: 0.09s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6876 score: 0.5102 time: 0.21s
Epoch 9/1000, LR 0.000210
Train loss: 0.6659;  Loss pred: 0.6659; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6842 score: 0.4898 time: 0.09s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6849 score: 0.5102 time: 0.09s
Epoch 10/1000, LR 0.000240
Train loss: 0.6200;  Loss pred: 0.6200; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6815 score: 0.4898 time: 0.09s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6830 score: 0.5102 time: 0.09s
Epoch 11/1000, LR 0.000270
Train loss: 0.6041;  Loss pred: 0.6041; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6800 score: 0.4898 time: 0.09s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6820 score: 0.5102 time: 0.09s
Epoch 12/1000, LR 0.000270
Train loss: 0.5905;  Loss pred: 0.5905; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6795 score: 0.4898 time: 0.09s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6819 score: 0.5102 time: 0.09s
Epoch 13/1000, LR 0.000270
Train loss: 0.5667;  Loss pred: 0.5667; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6790 score: 0.4898 time: 0.09s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6818 score: 0.5102 time: 0.09s
Epoch 14/1000, LR 0.000270
Train loss: 0.5488;  Loss pred: 0.5488; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6789 score: 0.4898 time: 0.09s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6822 score: 0.5102 time: 0.08s
Epoch 15/1000, LR 0.000270
Train loss: 0.5430;  Loss pred: 0.5430; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6790 score: 0.4898 time: 0.09s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6827 score: 0.5102 time: 0.08s
     INFO: Early stopping counter 1 of 20
Epoch 16/1000, LR 0.000270
Train loss: 0.5475;  Loss pred: 0.5475; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6783 score: 0.4898 time: 0.10s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6827 score: 0.5102 time: 0.22s
Epoch 17/1000, LR 0.000270
Train loss: 0.5082;  Loss pred: 0.5082; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6773 score: 0.4898 time: 0.09s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6826 score: 0.5102 time: 0.09s
Epoch 18/1000, LR 0.000270
Train loss: 0.4877;  Loss pred: 0.4877; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6760 score: 0.4898 time: 0.09s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6822 score: 0.5102 time: 0.09s
Epoch 19/1000, LR 0.000270
Train loss: 0.4788;  Loss pred: 0.4788; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6739 score: 0.4898 time: 0.09s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6812 score: 0.5102 time: 0.09s
Epoch 20/1000, LR 0.000270
Train loss: 0.4687;  Loss pred: 0.4687; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6710 score: 0.4898 time: 0.09s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6795 score: 0.5102 time: 0.09s
Epoch 21/1000, LR 0.000270
Train loss: 0.4655;  Loss pred: 0.4655; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6668 score: 0.4898 time: 0.09s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6765 score: 0.5102 time: 0.09s
Epoch 22/1000, LR 0.000270
Train loss: 0.4339;  Loss pred: 0.4339; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6624 score: 0.4898 time: 0.32s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6733 score: 0.5102 time: 0.09s
Epoch 23/1000, LR 0.000270
Train loss: 0.4429;  Loss pred: 0.4429; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6578 score: 0.4898 time: 0.09s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6702 score: 0.5102 time: 0.20s
Epoch 24/1000, LR 0.000270
Train loss: 0.4094;  Loss pred: 0.4094; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6526 score: 0.4898 time: 0.09s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6668 score: 0.5102 time: 0.08s
Epoch 25/1000, LR 0.000270
Train loss: 0.4065;  Loss pred: 0.4065; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6465 score: 0.4898 time: 0.09s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6627 score: 0.5102 time: 0.08s
Epoch 26/1000, LR 0.000270
Train loss: 0.3850;  Loss pred: 0.3850; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6409 score: 0.4898 time: 0.09s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6590 score: 0.5102 time: 0.08s
Epoch 27/1000, LR 0.000270
Train loss: 0.3747;  Loss pred: 0.3747; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6347 score: 0.4898 time: 0.09s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6547 score: 0.5102 time: 0.09s
Epoch 28/1000, LR 0.000270
Train loss: 0.3791;  Loss pred: 0.3791; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6279 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6498 score: 0.5102 time: 0.08s
Epoch 29/1000, LR 0.000270
Train loss: 0.3580;  Loss pred: 0.3580; Loss self: 0.0000; time: 0.12s
Val loss: 0.6212 score: 0.5102 time: 0.09s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6451 score: 0.5102 time: 0.09s
Epoch 30/1000, LR 0.000270
Train loss: 0.3644;  Loss pred: 0.3644; Loss self: 0.0000; time: 0.13s
Val loss: 0.6152 score: 0.5510 time: 0.09s
Test loss: 0.6408 score: 0.5306 time: 0.09s
Epoch 31/1000, LR 0.000270
Train loss: 0.3286;  Loss pred: 0.3286; Loss self: 0.0000; time: 0.13s
Val loss: 0.6082 score: 0.6122 time: 0.08s
Test loss: 0.6357 score: 0.5510 time: 0.09s
Epoch 32/1000, LR 0.000270
Train loss: 0.3305;  Loss pred: 0.3305; Loss self: 0.0000; time: 0.28s
Val loss: 0.6000 score: 0.6122 time: 0.09s
Test loss: 0.6297 score: 0.5510 time: 0.09s
Epoch 33/1000, LR 0.000270
Train loss: 0.3213;  Loss pred: 0.3213; Loss self: 0.0000; time: 0.13s
Val loss: 0.5920 score: 0.6122 time: 0.09s
Test loss: 0.6239 score: 0.5510 time: 0.09s
Epoch 34/1000, LR 0.000270
Train loss: 0.3063;  Loss pred: 0.3063; Loss self: 0.0000; time: 0.13s
Val loss: 0.5848 score: 0.6122 time: 0.09s
Test loss: 0.6185 score: 0.5510 time: 0.08s
Epoch 35/1000, LR 0.000270
Train loss: 0.3164;  Loss pred: 0.3164; Loss self: 0.0000; time: 0.13s
Val loss: 0.5785 score: 0.6531 time: 0.09s
Test loss: 0.6138 score: 0.5918 time: 0.08s
Epoch 36/1000, LR 0.000270
Train loss: 0.3077;  Loss pred: 0.3077; Loss self: 0.0000; time: 0.13s
Val loss: 0.5735 score: 0.6735 time: 0.09s
Test loss: 0.6103 score: 0.5918 time: 0.09s
Epoch 37/1000, LR 0.000270
Train loss: 0.2813;  Loss pred: 0.2813; Loss self: 0.0000; time: 0.14s
Val loss: 0.5700 score: 0.6735 time: 0.09s
Test loss: 0.6083 score: 0.5918 time: 0.09s
Epoch 38/1000, LR 0.000270
Train loss: 0.2769;  Loss pred: 0.2769; Loss self: 0.0000; time: 0.14s
Val loss: 0.5668 score: 0.6735 time: 0.09s
Test loss: 0.6061 score: 0.5918 time: 0.09s
Epoch 39/1000, LR 0.000269
Train loss: 0.2777;  Loss pred: 0.2777; Loss self: 0.0000; time: 0.14s
Val loss: 0.5635 score: 0.6735 time: 0.09s
Test loss: 0.6039 score: 0.5918 time: 0.09s
Epoch 40/1000, LR 0.000269
Train loss: 0.2720;  Loss pred: 0.2720; Loss self: 0.0000; time: 0.14s
Val loss: 0.5600 score: 0.6735 time: 0.09s
Test loss: 0.6015 score: 0.5918 time: 0.09s
Epoch 41/1000, LR 0.000269
Train loss: 0.2627;  Loss pred: 0.2627; Loss self: 0.0000; time: 0.25s
Val loss: 0.5581 score: 0.6735 time: 0.09s
Test loss: 0.6004 score: 0.5918 time: 0.09s
Epoch 42/1000, LR 0.000269
Train loss: 0.2616;  Loss pred: 0.2616; Loss self: 0.0000; time: 0.14s
Val loss: 0.5569 score: 0.6735 time: 0.09s
Test loss: 0.5999 score: 0.5918 time: 0.09s
Epoch 43/1000, LR 0.000269
Train loss: 0.2439;  Loss pred: 0.2439; Loss self: 0.0000; time: 0.14s
Val loss: 0.5557 score: 0.6735 time: 0.09s
Test loss: 0.5992 score: 0.5918 time: 0.09s
Epoch 44/1000, LR 0.000269
Train loss: 0.2337;  Loss pred: 0.2337; Loss self: 0.0000; time: 0.14s
Val loss: 0.5553 score: 0.6735 time: 0.09s
Test loss: 0.5994 score: 0.5918 time: 0.09s
Epoch 45/1000, LR 0.000269
Train loss: 0.2369;  Loss pred: 0.2369; Loss self: 0.0000; time: 0.14s
Val loss: 0.5540 score: 0.6735 time: 0.09s
Test loss: 0.5987 score: 0.5918 time: 0.09s
Epoch 46/1000, LR 0.000269
Train loss: 0.2340;  Loss pred: 0.2340; Loss self: 0.0000; time: 0.14s
Val loss: 0.5529 score: 0.6735 time: 0.09s
Test loss: 0.5984 score: 0.5918 time: 0.09s
Epoch 47/1000, LR 0.000269
Train loss: 0.2276;  Loss pred: 0.2276; Loss self: 0.0000; time: 0.13s
Val loss: 0.5502 score: 0.6735 time: 0.09s
Test loss: 0.5964 score: 0.5918 time: 0.09s
Epoch 48/1000, LR 0.000269
Train loss: 0.2148;  Loss pred: 0.2148; Loss self: 0.0000; time: 0.14s
Val loss: 0.5481 score: 0.6735 time: 0.09s
Test loss: 0.5949 score: 0.5918 time: 0.20s
Epoch 49/1000, LR 0.000269
Train loss: 0.2159;  Loss pred: 0.2159; Loss self: 0.0000; time: 0.13s
Val loss: 0.5453 score: 0.6735 time: 0.09s
Test loss: 0.5926 score: 0.5918 time: 0.08s
Epoch 50/1000, LR 0.000269
Train loss: 0.2030;  Loss pred: 0.2030; Loss self: 0.0000; time: 0.13s
Val loss: 0.5419 score: 0.6735 time: 0.08s
Test loss: 0.5899 score: 0.6122 time: 0.08s
Epoch 51/1000, LR 0.000269
Train loss: 0.2150;  Loss pred: 0.2150; Loss self: 0.0000; time: 0.13s
Val loss: 0.5381 score: 0.6735 time: 0.09s
Test loss: 0.5867 score: 0.6122 time: 0.08s
Epoch 52/1000, LR 0.000269
Train loss: 0.1929;  Loss pred: 0.1929; Loss self: 0.0000; time: 0.13s
Val loss: 0.5348 score: 0.6735 time: 0.08s
Test loss: 0.5842 score: 0.6327 time: 0.09s
Epoch 53/1000, LR 0.000269
Train loss: 0.2097;  Loss pred: 0.2097; Loss self: 0.0000; time: 0.13s
Val loss: 0.5307 score: 0.6735 time: 0.08s
Test loss: 0.5810 score: 0.6327 time: 0.08s
Epoch 54/1000, LR 0.000269
Train loss: 0.1923;  Loss pred: 0.1923; Loss self: 0.0000; time: 0.13s
Val loss: 0.5273 score: 0.6735 time: 0.08s
Test loss: 0.5788 score: 0.6327 time: 0.09s
Epoch 55/1000, LR 0.000269
Train loss: 0.1911;  Loss pred: 0.1911; Loss self: 0.0000; time: 0.13s
Val loss: 0.5228 score: 0.6735 time: 2.15s
Test loss: 0.5752 score: 0.6327 time: 3.16s
Epoch 56/1000, LR 0.000269
Train loss: 0.1845;  Loss pred: 0.1845; Loss self: 0.0000; time: 1.46s
Val loss: 0.5170 score: 0.6735 time: 0.36s
Test loss: 0.5703 score: 0.6531 time: 0.12s
Epoch 57/1000, LR 0.000269
Train loss: 0.1659;  Loss pred: 0.1659; Loss self: 0.0000; time: 0.13s
Val loss: 0.5129 score: 0.6735 time: 0.09s
Test loss: 0.5671 score: 0.6531 time: 0.09s
Epoch 58/1000, LR 0.000269
Train loss: 0.1762;  Loss pred: 0.1762; Loss self: 0.0000; time: 0.14s
Val loss: 0.5088 score: 0.6735 time: 0.10s
Test loss: 0.5641 score: 0.6735 time: 0.09s
Epoch 59/1000, LR 0.000268
Train loss: 0.1729;  Loss pred: 0.1729; Loss self: 0.0000; time: 0.13s
Val loss: 0.5038 score: 0.6735 time: 0.09s
Test loss: 0.5602 score: 0.6735 time: 0.08s
Epoch 60/1000, LR 0.000268
Train loss: 0.1560;  Loss pred: 0.1560; Loss self: 0.0000; time: 0.13s
Val loss: 0.4995 score: 0.6735 time: 0.09s
Test loss: 0.5570 score: 0.6735 time: 0.09s
Epoch 61/1000, LR 0.000268
Train loss: 0.1494;  Loss pred: 0.1494; Loss self: 0.0000; time: 0.14s
Val loss: 0.4948 score: 0.6735 time: 0.09s
Test loss: 0.5535 score: 0.6531 time: 0.09s
Epoch 62/1000, LR 0.000268
Train loss: 0.1648;  Loss pred: 0.1648; Loss self: 0.0000; time: 0.13s
Val loss: 0.4908 score: 0.6735 time: 0.09s
Test loss: 0.5508 score: 0.6531 time: 0.08s
Epoch 63/1000, LR 0.000268
Train loss: 0.1446;  Loss pred: 0.1446; Loss self: 0.0000; time: 0.12s
Val loss: 0.4865 score: 0.6939 time: 0.09s
Test loss: 0.5478 score: 0.6531 time: 0.08s
Epoch 64/1000, LR 0.000268
Train loss: 0.1525;  Loss pred: 0.1525; Loss self: 0.0000; time: 0.13s
Val loss: 0.4799 score: 0.7143 time: 0.09s
Test loss: 0.5427 score: 0.6531 time: 0.09s
Epoch 65/1000, LR 0.000268
Train loss: 0.1594;  Loss pred: 0.1594; Loss self: 0.0000; time: 0.13s
Val loss: 0.4758 score: 0.7143 time: 0.09s
Test loss: 0.5400 score: 0.6327 time: 0.08s
Epoch 66/1000, LR 0.000268
Train loss: 0.1427;  Loss pred: 0.1427; Loss self: 0.0000; time: 0.13s
Val loss: 0.4709 score: 0.7143 time: 0.09s
Test loss: 0.5369 score: 0.6327 time: 0.08s
Epoch 67/1000, LR 0.000268
Train loss: 0.1394;  Loss pred: 0.1394; Loss self: 0.0000; time: 0.13s
Val loss: 0.4654 score: 0.7347 time: 0.09s
Test loss: 0.5332 score: 0.6327 time: 0.09s
Epoch 68/1000, LR 0.000268
Train loss: 0.1432;  Loss pred: 0.1432; Loss self: 0.0000; time: 0.14s
Val loss: 0.4593 score: 0.7551 time: 0.09s
Test loss: 0.5288 score: 0.6735 time: 0.08s
Epoch 69/1000, LR 0.000268
Train loss: 0.1344;  Loss pred: 0.1344; Loss self: 0.0000; time: 0.12s
Val loss: 0.4550 score: 0.7551 time: 0.09s
Test loss: 0.5258 score: 0.6735 time: 0.08s
Epoch 70/1000, LR 0.000268
Train loss: 0.1346;  Loss pred: 0.1346; Loss self: 0.0000; time: 0.13s
Val loss: 0.4493 score: 0.7551 time: 0.09s
Test loss: 0.5215 score: 0.6735 time: 0.08s
Epoch 71/1000, LR 0.000268
Train loss: 0.1247;  Loss pred: 0.1247; Loss self: 0.0000; time: 0.13s
Val loss: 0.4433 score: 0.7551 time: 0.09s
Test loss: 0.5166 score: 0.6735 time: 0.09s
Epoch 72/1000, LR 0.000267
Train loss: 0.1284;  Loss pred: 0.1284; Loss self: 0.0000; time: 0.13s
Val loss: 0.4368 score: 0.7551 time: 0.09s
Test loss: 0.5113 score: 0.6939 time: 0.08s
Epoch 73/1000, LR 0.000267
Train loss: 0.1218;  Loss pred: 0.1218; Loss self: 0.0000; time: 0.13s
Val loss: 0.4285 score: 0.7551 time: 0.09s
Test loss: 0.5044 score: 0.6939 time: 0.08s
Epoch 74/1000, LR 0.000267
Train loss: 0.1302;  Loss pred: 0.1302; Loss self: 0.0000; time: 0.13s
Val loss: 0.4207 score: 0.7551 time: 0.09s
Test loss: 0.4980 score: 0.7143 time: 0.09s
Epoch 75/1000, LR 0.000267
Train loss: 0.1220;  Loss pred: 0.1220; Loss self: 0.0000; time: 0.13s
Val loss: 0.4120 score: 0.7551 time: 0.09s
Test loss: 0.4907 score: 0.7143 time: 0.09s
Epoch 76/1000, LR 0.000267
Train loss: 0.1195;  Loss pred: 0.1195; Loss self: 0.0000; time: 0.13s
Val loss: 0.4030 score: 0.7755 time: 0.09s
Test loss: 0.4829 score: 0.7347 time: 0.10s
Epoch 77/1000, LR 0.000267
Train loss: 0.1097;  Loss pred: 0.1097; Loss self: 0.0000; time: 0.13s
Val loss: 0.3933 score: 0.7755 time: 0.09s
Test loss: 0.4746 score: 0.7347 time: 0.09s
Epoch 78/1000, LR 0.000267
Train loss: 0.1138;  Loss pred: 0.1138; Loss self: 0.0000; time: 0.14s
Val loss: 0.3830 score: 0.7959 time: 0.09s
Test loss: 0.4658 score: 0.7347 time: 0.08s
Epoch 79/1000, LR 0.000267
Train loss: 0.1138;  Loss pred: 0.1138; Loss self: 0.0000; time: 0.13s
Val loss: 0.3731 score: 0.8163 time: 0.09s
Test loss: 0.4574 score: 0.7347 time: 0.08s
Epoch 80/1000, LR 0.000267
Train loss: 0.1068;  Loss pred: 0.1068; Loss self: 0.0000; time: 0.14s
Val loss: 0.3636 score: 0.8367 time: 0.09s
Test loss: 0.4493 score: 0.7347 time: 0.08s
Epoch 81/1000, LR 0.000267
Train loss: 0.1079;  Loss pred: 0.1079; Loss self: 0.0000; time: 0.13s
Val loss: 0.3536 score: 0.8367 time: 0.09s
Test loss: 0.4405 score: 0.7347 time: 0.09s
Epoch 82/1000, LR 0.000267
Train loss: 0.1045;  Loss pred: 0.1045; Loss self: 0.0000; time: 0.13s
Val loss: 0.3428 score: 0.8571 time: 0.09s
Test loss: 0.4310 score: 0.7347 time: 0.08s
Epoch 83/1000, LR 0.000266
Train loss: 0.1311;  Loss pred: 0.1311; Loss self: 0.0000; time: 0.13s
Val loss: 0.3327 score: 0.8571 time: 0.21s
Test loss: 0.4220 score: 0.7347 time: 0.09s
Epoch 84/1000, LR 0.000266
Train loss: 0.1040;  Loss pred: 0.1040; Loss self: 0.0000; time: 0.14s
Val loss: 0.3221 score: 0.8571 time: 0.09s
Test loss: 0.4126 score: 0.7347 time: 0.09s
Epoch 85/1000, LR 0.000266
Train loss: 0.1006;  Loss pred: 0.1006; Loss self: 0.0000; time: 0.14s
Val loss: 0.3113 score: 0.8571 time: 0.09s
Test loss: 0.4030 score: 0.7347 time: 0.09s
Epoch 86/1000, LR 0.000266
Train loss: 0.0959;  Loss pred: 0.0959; Loss self: 0.0000; time: 0.14s
Val loss: 0.2988 score: 0.8571 time: 0.09s
Test loss: 0.3919 score: 0.7347 time: 0.09s
Epoch 87/1000, LR 0.000266
Train loss: 0.0988;  Loss pred: 0.0988; Loss self: 0.0000; time: 0.14s
Val loss: 0.2886 score: 0.8571 time: 0.11s
Test loss: 0.3829 score: 0.7755 time: 0.21s
Epoch 88/1000, LR 0.000266
Train loss: 0.1007;  Loss pred: 0.1007; Loss self: 0.0000; time: 0.14s
Val loss: 0.2789 score: 0.9184 time: 0.09s
Test loss: 0.3744 score: 0.8163 time: 0.09s
Epoch 89/1000, LR 0.000266
Train loss: 0.0926;  Loss pred: 0.0926; Loss self: 0.0000; time: 0.14s
Val loss: 0.2687 score: 0.9184 time: 0.09s
Test loss: 0.3656 score: 0.8367 time: 0.09s
Epoch 90/1000, LR 0.000266
Train loss: 0.0947;  Loss pred: 0.0947; Loss self: 0.0000; time: 0.13s
Val loss: 0.2589 score: 0.9184 time: 0.09s
Test loss: 0.3573 score: 0.8367 time: 0.09s
Epoch 91/1000, LR 0.000266
Train loss: 0.0988;  Loss pred: 0.0988; Loss self: 0.0000; time: 0.13s
Val loss: 0.2497 score: 0.9184 time: 0.09s
Test loss: 0.3495 score: 0.8367 time: 0.09s
Epoch 92/1000, LR 0.000266
Train loss: 0.0908;  Loss pred: 0.0908; Loss self: 0.0000; time: 0.13s
Val loss: 0.2410 score: 0.9184 time: 0.09s
Test loss: 0.3422 score: 0.8776 time: 0.09s
Epoch 93/1000, LR 0.000265
Train loss: 0.0844;  Loss pred: 0.0844; Loss self: 0.0000; time: 0.13s
Val loss: 0.2329 score: 0.9184 time: 0.09s
Test loss: 0.3353 score: 0.8776 time: 0.09s
Epoch 94/1000, LR 0.000265
Train loss: 0.0838;  Loss pred: 0.0838; Loss self: 0.0000; time: 0.13s
Val loss: 0.2248 score: 0.9184 time: 0.09s
Test loss: 0.3287 score: 0.8776 time: 0.09s
Epoch 95/1000, LR 0.000265
Train loss: 0.0913;  Loss pred: 0.0913; Loss self: 0.0000; time: 0.14s
Val loss: 0.2167 score: 0.9388 time: 0.09s
Test loss: 0.3224 score: 0.8776 time: 0.09s
Epoch 96/1000, LR 0.000265
Train loss: 0.0808;  Loss pred: 0.0808; Loss self: 0.0000; time: 0.14s
Val loss: 0.2098 score: 0.9388 time: 0.10s
Test loss: 0.3169 score: 0.8776 time: 0.08s
Epoch 97/1000, LR 0.000265
Train loss: 0.0866;  Loss pred: 0.0866; Loss self: 0.0000; time: 0.13s
Val loss: 0.2021 score: 0.9592 time: 0.09s
Test loss: 0.3111 score: 0.8776 time: 0.21s
Epoch 98/1000, LR 0.000265
Train loss: 0.0828;  Loss pred: 0.0828; Loss self: 0.0000; time: 0.13s
Val loss: 0.1954 score: 0.9592 time: 0.08s
Test loss: 0.3060 score: 0.8980 time: 0.09s
Epoch 99/1000, LR 0.000265
Train loss: 0.0852;  Loss pred: 0.0852; Loss self: 0.0000; time: 0.13s
Val loss: 0.1894 score: 0.9592 time: 0.09s
Test loss: 0.3016 score: 0.8980 time: 0.08s
Epoch 100/1000, LR 0.000265
Train loss: 0.0760;  Loss pred: 0.0760; Loss self: 0.0000; time: 0.13s
Val loss: 0.1836 score: 0.9592 time: 0.08s
Test loss: 0.2975 score: 0.8980 time: 0.08s
Epoch 101/1000, LR 0.000265
Train loss: 0.0806;  Loss pred: 0.0806; Loss self: 0.0000; time: 0.13s
Val loss: 0.1777 score: 0.9592 time: 0.09s
Test loss: 0.2934 score: 0.8776 time: 0.08s
Epoch 102/1000, LR 0.000264
Train loss: 0.0728;  Loss pred: 0.0728; Loss self: 0.0000; time: 0.13s
Val loss: 0.1720 score: 0.9592 time: 0.08s
Test loss: 0.2897 score: 0.8776 time: 0.08s
Epoch 103/1000, LR 0.000264
Train loss: 0.0754;  Loss pred: 0.0754; Loss self: 0.0000; time: 0.13s
Val loss: 0.1668 score: 0.9592 time: 0.09s
Test loss: 0.2864 score: 0.8776 time: 0.08s
Epoch 104/1000, LR 0.000264
Train loss: 0.0710;  Loss pred: 0.0710; Loss self: 0.0000; time: 0.13s
Val loss: 0.1623 score: 0.9592 time: 0.09s
Test loss: 0.2836 score: 0.8776 time: 0.08s
Epoch 105/1000, LR 0.000264
Train loss: 0.0788;  Loss pred: 0.0788; Loss self: 0.0000; time: 0.14s
Val loss: 0.1576 score: 0.9592 time: 0.13s
Test loss: 0.2809 score: 0.8776 time: 0.09s
Epoch 106/1000, LR 0.000264
Train loss: 0.0757;  Loss pred: 0.0757; Loss self: 0.0000; time: 0.13s
Val loss: 0.1530 score: 0.9592 time: 0.08s
Test loss: 0.2783 score: 0.8776 time: 0.08s
Epoch 107/1000, LR 0.000264
Train loss: 0.0774;  Loss pred: 0.0774; Loss self: 0.0000; time: 0.12s
Val loss: 0.1487 score: 0.9592 time: 0.09s
Test loss: 0.2761 score: 0.8776 time: 0.08s
Epoch 108/1000, LR 0.000264
Train loss: 0.0804;  Loss pred: 0.0804; Loss self: 0.0000; time: 0.12s
Val loss: 0.1459 score: 0.9592 time: 0.09s
Test loss: 0.2747 score: 0.8776 time: 0.08s
Epoch 109/1000, LR 0.000264
Train loss: 0.0709;  Loss pred: 0.0709; Loss self: 0.0000; time: 0.12s
Val loss: 0.1426 score: 0.9592 time: 0.09s
Test loss: 0.2733 score: 0.8776 time: 0.08s
Epoch 110/1000, LR 0.000263
Train loss: 0.0680;  Loss pred: 0.0680; Loss self: 0.0000; time: 0.12s
Val loss: 0.1399 score: 0.9592 time: 0.08s
Test loss: 0.2722 score: 0.8776 time: 0.08s
Epoch 111/1000, LR 0.000263
Train loss: 0.0669;  Loss pred: 0.0669; Loss self: 0.0000; time: 0.13s
Val loss: 0.1380 score: 0.9592 time: 0.09s
Test loss: 0.2711 score: 0.8776 time: 0.08s
Epoch 112/1000, LR 0.000263
Train loss: 0.0674;  Loss pred: 0.0674; Loss self: 0.0000; time: 0.12s
Val loss: 0.1353 score: 0.9592 time: 0.09s
Test loss: 0.2700 score: 0.8776 time: 0.08s
Epoch 113/1000, LR 0.000263
Train loss: 0.0617;  Loss pred: 0.0617; Loss self: 0.0000; time: 0.13s
Val loss: 0.1334 score: 0.9592 time: 0.09s
Test loss: 0.2691 score: 0.8776 time: 0.08s
Epoch 114/1000, LR 0.000263
Train loss: 0.0649;  Loss pred: 0.0649; Loss self: 0.0000; time: 0.13s
Val loss: 0.1313 score: 0.9592 time: 0.10s
Test loss: 0.2683 score: 0.8776 time: 0.18s
Epoch 115/1000, LR 0.000263
Train loss: 0.0637;  Loss pred: 0.0637; Loss self: 0.0000; time: 0.12s
Val loss: 0.1297 score: 0.9592 time: 0.08s
Test loss: 0.2675 score: 0.8776 time: 0.08s
Epoch 116/1000, LR 0.000263
Train loss: 0.0609;  Loss pred: 0.0609; Loss self: 0.0000; time: 0.13s
Val loss: 0.1276 score: 0.9592 time: 0.09s
Test loss: 0.2671 score: 0.8776 time: 0.08s
Epoch 117/1000, LR 0.000262
Train loss: 0.0577;  Loss pred: 0.0577; Loss self: 0.0000; time: 0.12s
Val loss: 0.1258 score: 0.9592 time: 0.09s
Test loss: 0.2668 score: 0.8980 time: 0.08s
Epoch 118/1000, LR 0.000262
Train loss: 0.0678;  Loss pred: 0.0678; Loss self: 0.0000; time: 0.13s
Val loss: 0.1249 score: 0.9592 time: 0.09s
Test loss: 0.2668 score: 0.8980 time: 0.08s
Epoch 119/1000, LR 0.000262
Train loss: 0.0569;  Loss pred: 0.0569; Loss self: 0.0000; time: 0.12s
Val loss: 0.1237 score: 0.9592 time: 0.09s
Test loss: 0.2666 score: 0.8980 time: 0.08s
Epoch 120/1000, LR 0.000262
Train loss: 0.0589;  Loss pred: 0.0589; Loss self: 0.0000; time: 0.13s
Val loss: 0.1227 score: 0.9592 time: 0.08s
Test loss: 0.2666 score: 0.8980 time: 0.08s
Epoch 121/1000, LR 0.000262
Train loss: 0.0583;  Loss pred: 0.0583; Loss self: 0.0000; time: 0.12s
Val loss: 0.1222 score: 0.9592 time: 0.09s
Test loss: 0.2662 score: 0.8980 time: 0.09s
Epoch 122/1000, LR 0.000262
Train loss: 0.0508;  Loss pred: 0.0508; Loss self: 0.0000; time: 0.25s
Val loss: 0.1220 score: 0.9592 time: 0.08s
Test loss: 0.2661 score: 0.8980 time: 0.08s
Epoch 123/1000, LR 0.000262
Train loss: 0.0503;  Loss pred: 0.0503; Loss self: 0.0000; time: 0.15s
Val loss: 0.1211 score: 0.9592 time: 0.10s
Test loss: 0.2658 score: 0.8980 time: 0.08s
Epoch 124/1000, LR 0.000261
Train loss: 0.0555;  Loss pred: 0.0555; Loss self: 0.0000; time: 0.13s
Val loss: 0.1207 score: 0.9592 time: 0.10s
Test loss: 0.2656 score: 0.8980 time: 0.10s
Epoch 125/1000, LR 0.000261
Train loss: 0.0604;  Loss pred: 0.0604; Loss self: 0.0000; time: 0.13s
Val loss: 0.1198 score: 0.9592 time: 0.08s
Test loss: 0.2651 score: 0.8980 time: 0.08s
Epoch 126/1000, LR 0.000261
Train loss: 0.0510;  Loss pred: 0.0510; Loss self: 0.0000; time: 0.13s
Val loss: 0.1191 score: 0.9592 time: 0.10s
Test loss: 0.2649 score: 0.8776 time: 0.12s
Epoch 127/1000, LR 0.000261
Train loss: 0.0579;  Loss pred: 0.0579; Loss self: 0.0000; time: 0.13s
Val loss: 0.1184 score: 0.9592 time: 0.09s
Test loss: 0.2649 score: 0.8776 time: 0.09s
Epoch 128/1000, LR 0.000261
Train loss: 0.0527;  Loss pred: 0.0527; Loss self: 0.0000; time: 0.12s
Val loss: 0.1174 score: 0.9592 time: 0.09s
Test loss: 0.2648 score: 0.8776 time: 0.08s
Epoch 129/1000, LR 0.000261
Train loss: 0.0561;  Loss pred: 0.0561; Loss self: 0.0000; time: 0.13s
Val loss: 0.1163 score: 0.9592 time: 0.09s
Test loss: 0.2649 score: 0.8776 time: 0.08s
Epoch 130/1000, LR 0.000260
Train loss: 0.0563;  Loss pred: 0.0563; Loss self: 0.0000; time: 0.14s
Val loss: 0.1158 score: 0.9592 time: 0.19s
Test loss: 0.2649 score: 0.8776 time: 0.08s
Epoch 131/1000, LR 0.000260
Train loss: 0.0565;  Loss pred: 0.0565; Loss self: 0.0000; time: 0.12s
Val loss: 0.1153 score: 0.9592 time: 0.09s
Test loss: 0.2648 score: 0.8776 time: 0.08s
Epoch 132/1000, LR 0.000260
Train loss: 0.0470;  Loss pred: 0.0470; Loss self: 0.0000; time: 0.13s
Val loss: 0.1145 score: 0.9592 time: 0.09s
Test loss: 0.2647 score: 0.8776 time: 0.08s
Epoch 133/1000, LR 0.000260
Train loss: 0.0480;  Loss pred: 0.0480; Loss self: 0.0000; time: 0.13s
Val loss: 0.1136 score: 0.9592 time: 0.09s
Test loss: 0.2645 score: 0.8776 time: 0.09s
Epoch 134/1000, LR 0.000260
Train loss: 0.0512;  Loss pred: 0.0512; Loss self: 0.0000; time: 0.13s
Val loss: 0.1130 score: 0.9592 time: 0.09s
Test loss: 0.2645 score: 0.8776 time: 0.09s
Epoch 135/1000, LR 0.000260
Train loss: 0.0462;  Loss pred: 0.0462; Loss self: 0.0000; time: 0.13s
Val loss: 0.1125 score: 0.9592 time: 0.09s
Test loss: 0.2643 score: 0.8776 time: 0.09s
Epoch 136/1000, LR 0.000260
Train loss: 0.0538;  Loss pred: 0.0538; Loss self: 0.0000; time: 0.13s
Val loss: 0.1118 score: 0.9592 time: 0.09s
Test loss: 0.2641 score: 0.8776 time: 0.08s
Epoch 137/1000, LR 0.000259
Train loss: 0.0523;  Loss pred: 0.0523; Loss self: 0.0000; time: 0.13s
Val loss: 0.1114 score: 0.9592 time: 0.09s
Test loss: 0.2640 score: 0.8776 time: 0.08s
Epoch 138/1000, LR 0.000259
Train loss: 0.0478;  Loss pred: 0.0478; Loss self: 0.0000; time: 0.12s
Val loss: 0.1112 score: 0.9592 time: 0.19s
Test loss: 0.2641 score: 0.8776 time: 0.08s
Epoch 139/1000, LR 0.000259
Train loss: 0.0488;  Loss pred: 0.0488; Loss self: 0.0000; time: 0.12s
Val loss: 0.1104 score: 0.9592 time: 0.09s
Test loss: 0.2641 score: 0.8776 time: 0.09s
Epoch 140/1000, LR 0.000259
Train loss: 0.0448;  Loss pred: 0.0448; Loss self: 0.0000; time: 0.13s
Val loss: 0.1102 score: 0.9592 time: 0.09s
Test loss: 0.2641 score: 0.8776 time: 0.09s
Epoch 141/1000, LR 0.000259
Train loss: 0.0456;  Loss pred: 0.0456; Loss self: 0.0000; time: 0.13s
Val loss: 0.1095 score: 0.9592 time: 0.09s
Test loss: 0.2642 score: 0.8776 time: 0.10s
Epoch 142/1000, LR 0.000259
Train loss: 0.0563;  Loss pred: 0.0563; Loss self: 0.0000; time: 0.13s
Val loss: 0.1087 score: 0.9592 time: 0.09s
Test loss: 0.2641 score: 0.8776 time: 0.08s
Epoch 143/1000, LR 0.000258
Train loss: 0.0422;  Loss pred: 0.0422; Loss self: 0.0000; time: 0.13s
Val loss: 0.1080 score: 0.9592 time: 0.09s
Test loss: 0.2645 score: 0.8776 time: 0.08s
Epoch 144/1000, LR 0.000258
Train loss: 0.0485;  Loss pred: 0.0485; Loss self: 0.0000; time: 0.13s
Val loss: 0.1076 score: 0.9592 time: 0.09s
Test loss: 0.2646 score: 0.8776 time: 0.08s
Epoch 145/1000, LR 0.000258
Train loss: 0.0408;  Loss pred: 0.0408; Loss self: 0.0000; time: 0.13s
Val loss: 0.1065 score: 0.9592 time: 0.09s
Test loss: 0.2650 score: 0.8980 time: 0.08s
Epoch 146/1000, LR 0.000258
Train loss: 0.0436;  Loss pred: 0.0436; Loss self: 0.0000; time: 0.14s
Val loss: 0.1060 score: 0.9592 time: 0.16s
Test loss: 0.2654 score: 0.8980 time: 0.08s
Epoch 147/1000, LR 0.000258
Train loss: 0.0395;  Loss pred: 0.0395; Loss self: 0.0000; time: 0.12s
Val loss: 0.1053 score: 0.9592 time: 0.09s
Test loss: 0.2657 score: 0.8980 time: 0.09s
Epoch 148/1000, LR 0.000257
Train loss: 0.0398;  Loss pred: 0.0398; Loss self: 0.0000; time: 0.12s
Val loss: 0.1050 score: 0.9592 time: 0.10s
Test loss: 0.2658 score: 0.8980 time: 0.08s
Epoch 149/1000, LR 0.000257
Train loss: 0.0412;  Loss pred: 0.0412; Loss self: 0.0000; time: 0.13s
Val loss: 0.1050 score: 0.9592 time: 0.09s
Test loss: 0.2661 score: 0.8980 time: 0.08s
Epoch 150/1000, LR 0.000257
Train loss: 0.0401;  Loss pred: 0.0401; Loss self: 0.0000; time: 0.13s
Val loss: 0.1051 score: 0.9592 time: 0.09s
Test loss: 0.2663 score: 0.8980 time: 0.08s
     INFO: Early stopping counter 1 of 20
Epoch 151/1000, LR 0.000257
Train loss: 0.0387;  Loss pred: 0.0387; Loss self: 0.0000; time: 0.12s
Val loss: 0.1049 score: 0.9592 time: 0.09s
Test loss: 0.2664 score: 0.8980 time: 0.08s
Epoch 152/1000, LR 0.000257
Train loss: 0.0426;  Loss pred: 0.0426; Loss self: 0.0000; time: 0.13s
Val loss: 0.1047 score: 0.9592 time: 0.09s
Test loss: 0.2664 score: 0.8980 time: 0.09s
Epoch 153/1000, LR 0.000257
Train loss: 0.0397;  Loss pred: 0.0397; Loss self: 0.0000; time: 0.13s
Val loss: 0.1040 score: 0.9592 time: 0.09s
Test loss: 0.2668 score: 0.8980 time: 0.08s
Epoch 154/1000, LR 0.000256
Train loss: 0.0390;  Loss pred: 0.0390; Loss self: 0.0000; time: 0.13s
Val loss: 0.1031 score: 0.9592 time: 0.18s
Test loss: 0.2671 score: 0.8980 time: 0.08s
Epoch 155/1000, LR 0.000256
Train loss: 0.0382;  Loss pred: 0.0382; Loss self: 0.0000; time: 0.12s
Val loss: 0.1021 score: 0.9592 time: 0.09s
Test loss: 0.2674 score: 0.8980 time: 0.08s
Epoch 156/1000, LR 0.000256
Train loss: 0.0386;  Loss pred: 0.0386; Loss self: 0.0000; time: 0.13s
Val loss: 0.1017 score: 0.9592 time: 0.08s
Test loss: 0.2673 score: 0.8980 time: 0.08s
Epoch 157/1000, LR 0.000256
Train loss: 0.0436;  Loss pred: 0.0436; Loss self: 0.0000; time: 0.13s
Val loss: 0.1007 score: 0.9592 time: 0.09s
Test loss: 0.2674 score: 0.8980 time: 0.08s
Epoch 158/1000, LR 0.000256
Train loss: 0.0331;  Loss pred: 0.0331; Loss self: 0.0000; time: 0.13s
Val loss: 0.0999 score: 0.9592 time: 0.09s
Test loss: 0.2672 score: 0.8980 time: 0.09s
Epoch 159/1000, LR 0.000255
Train loss: 0.0436;  Loss pred: 0.0436; Loss self: 0.0000; time: 0.13s
Val loss: 0.0990 score: 0.9592 time: 0.09s
Test loss: 0.2677 score: 0.8980 time: 0.09s
Epoch 160/1000, LR 0.000255
Train loss: 0.0367;  Loss pred: 0.0367; Loss self: 0.0000; time: 0.14s
Val loss: 0.0984 score: 0.9592 time: 0.10s
Test loss: 0.2681 score: 0.9184 time: 0.09s
Epoch 161/1000, LR 0.000255
Train loss: 0.0324;  Loss pred: 0.0324; Loss self: 0.0000; time: 0.13s
Val loss: 0.0980 score: 0.9592 time: 0.22s
Test loss: 0.2687 score: 0.9184 time: 0.08s
Epoch 162/1000, LR 0.000255
Train loss: 0.0369;  Loss pred: 0.0369; Loss self: 0.0000; time: 0.13s
Val loss: 0.0980 score: 0.9592 time: 0.09s
Test loss: 0.2688 score: 0.9184 time: 0.08s
     INFO: Early stopping counter 1 of 20
Epoch 163/1000, LR 0.000255
Train loss: 0.0371;  Loss pred: 0.0371; Loss self: 0.0000; time: 0.12s
Val loss: 0.0977 score: 0.9592 time: 0.09s
Test loss: 0.2689 score: 0.9184 time: 0.08s
Epoch 164/1000, LR 0.000254
Train loss: 0.0363;  Loss pred: 0.0363; Loss self: 0.0000; time: 0.13s
Val loss: 0.0977 score: 0.9592 time: 0.09s
Test loss: 0.2691 score: 0.9184 time: 0.08s
Epoch 165/1000, LR 0.000254
Train loss: 0.0347;  Loss pred: 0.0347; Loss self: 0.0000; time: 0.13s
Val loss: 0.0978 score: 0.9592 time: 0.10s
Test loss: 0.2692 score: 0.9184 time: 0.09s
     INFO: Early stopping counter 1 of 20
Epoch 166/1000, LR 0.000254
Train loss: 0.0382;  Loss pred: 0.0382; Loss self: 0.0000; time: 0.13s
Val loss: 0.0974 score: 0.9592 time: 0.09s
Test loss: 0.2696 score: 0.9184 time: 0.09s
Epoch 167/1000, LR 0.000254
Train loss: 0.0388;  Loss pred: 0.0388; Loss self: 0.0000; time: 0.13s
Val loss: 0.0969 score: 0.9592 time: 0.09s
Test loss: 0.2696 score: 0.9184 time: 0.09s
Epoch 168/1000, LR 0.000254
Train loss: 0.0347;  Loss pred: 0.0347; Loss self: 0.0000; time: 0.14s
Val loss: 0.0961 score: 0.9592 time: 0.09s
Test loss: 0.2699 score: 0.9184 time: 0.09s
Epoch 169/1000, LR 0.000253
Train loss: 0.0295;  Loss pred: 0.0295; Loss self: 0.0000; time: 0.13s
Val loss: 0.0955 score: 0.9592 time: 0.09s
Test loss: 0.2701 score: 0.9184 time: 0.09s
Epoch 170/1000, LR 0.000253
Train loss: 0.0353;  Loss pred: 0.0353; Loss self: 0.0000; time: 0.14s
Val loss: 0.0951 score: 0.9592 time: 0.18s
Test loss: 0.2700 score: 0.9184 time: 0.08s
Epoch 171/1000, LR 0.000253
Train loss: 0.0310;  Loss pred: 0.0310; Loss self: 0.0000; time: 0.12s
Val loss: 0.0945 score: 0.9592 time: 0.09s
Test loss: 0.2702 score: 0.9184 time: 0.09s
Epoch 172/1000, LR 0.000253
Train loss: 0.0324;  Loss pred: 0.0324; Loss self: 0.0000; time: 0.13s
Val loss: 0.0940 score: 0.9592 time: 0.10s
Test loss: 0.2706 score: 0.9184 time: 0.08s
Epoch 173/1000, LR 0.000253
Train loss: 0.0336;  Loss pred: 0.0336; Loss self: 0.0000; time: 0.13s
Val loss: 0.0940 score: 0.9592 time: 0.09s
Test loss: 0.2707 score: 0.9184 time: 0.09s
     INFO: Early stopping counter 1 of 20
Epoch 174/1000, LR 0.000252
Train loss: 0.0301;  Loss pred: 0.0301; Loss self: 0.0000; time: 0.13s
Val loss: 0.0939 score: 0.9592 time: 0.10s
Test loss: 0.2708 score: 0.9184 time: 0.08s
Epoch 175/1000, LR 0.000252
Train loss: 0.0323;  Loss pred: 0.0323; Loss self: 0.0000; time: 0.12s
Val loss: 0.0938 score: 0.9592 time: 0.10s
Test loss: 0.2708 score: 0.9184 time: 0.08s
Epoch 176/1000, LR 0.000252
Train loss: 0.0310;  Loss pred: 0.0310; Loss self: 0.0000; time: 0.13s
Val loss: 0.0936 score: 0.9592 time: 0.09s
Test loss: 0.2709 score: 0.9184 time: 0.08s
Epoch 177/1000, LR 0.000252
Train loss: 0.0334;  Loss pred: 0.0334; Loss self: 0.0000; time: 0.13s
Val loss: 0.0941 score: 0.9592 time: 0.09s
Test loss: 0.2710 score: 0.9184 time: 0.08s
     INFO: Early stopping counter 1 of 20
Epoch 178/1000, LR 0.000251
Train loss: 0.0294;  Loss pred: 0.0294; Loss self: 0.0000; time: 0.24s
Val loss: 0.0938 score: 0.9592 time: 0.10s
Test loss: 0.2718 score: 0.9184 time: 0.08s
     INFO: Early stopping counter 2 of 20
Epoch 179/1000, LR 0.000251
Train loss: 0.0345;  Loss pred: 0.0345; Loss self: 0.0000; time: 0.12s
Val loss: 0.0939 score: 0.9592 time: 0.09s
Test loss: 0.2722 score: 0.9184 time: 0.08s
     INFO: Early stopping counter 3 of 20
Epoch 180/1000, LR 0.000251
Train loss: 0.0281;  Loss pred: 0.0281; Loss self: 0.0000; time: 0.12s
Val loss: 0.0936 score: 0.9592 time: 0.09s
Test loss: 0.2724 score: 0.9184 time: 0.08s
Epoch 181/1000, LR 0.000251
Train loss: 0.0256;  Loss pred: 0.0256; Loss self: 0.0000; time: 0.12s
Val loss: 0.0936 score: 0.9592 time: 0.09s
Test loss: 0.2724 score: 0.8980 time: 0.08s
     INFO: Early stopping counter 1 of 20
Epoch 182/1000, LR 0.000251
Train loss: 0.0276;  Loss pred: 0.0276; Loss self: 0.0000; time: 0.13s
Val loss: 0.0931 score: 0.9592 time: 0.09s
Test loss: 0.2730 score: 0.8980 time: 0.09s
Epoch 183/1000, LR 0.000250
Train loss: 0.0315;  Loss pred: 0.0315; Loss self: 0.0000; time: 0.13s
Val loss: 0.0935 score: 0.9592 time: 0.09s
Test loss: 0.2728 score: 0.8980 time: 0.09s
     INFO: Early stopping counter 1 of 20
Epoch 184/1000, LR 0.000250
Train loss: 0.0272;  Loss pred: 0.0272; Loss self: 0.0000; time: 0.13s
Val loss: 0.0932 score: 0.9592 time: 0.09s
Test loss: 0.2729 score: 0.8980 time: 0.09s
     INFO: Early stopping counter 2 of 20
Epoch 185/1000, LR 0.000250
Train loss: 0.0271;  Loss pred: 0.0271; Loss self: 0.0000; time: 0.13s
Val loss: 0.0934 score: 0.9592 time: 0.09s
Test loss: 0.2726 score: 0.8980 time: 0.09s
     INFO: Early stopping counter 3 of 20
Epoch 186/1000, LR 0.000250
Train loss: 0.0297;  Loss pred: 0.0297; Loss self: 0.0000; time: 0.27s
Val loss: 0.0936 score: 0.9592 time: 0.09s
Test loss: 0.2722 score: 0.8980 time: 0.09s
     INFO: Early stopping counter 4 of 20
Epoch 187/1000, LR 0.000249
Train loss: 0.0261;  Loss pred: 0.0261; Loss self: 0.0000; time: 0.13s
Val loss: 0.0939 score: 0.9592 time: 0.09s
Test loss: 0.2720 score: 0.8980 time: 0.08s
     INFO: Early stopping counter 5 of 20
Epoch 188/1000, LR 0.000249
Train loss: 0.0245;  Loss pred: 0.0245; Loss self: 0.0000; time: 0.13s
Val loss: 0.0937 score: 0.9592 time: 0.09s
Test loss: 0.2725 score: 0.8980 time: 0.09s
     INFO: Early stopping counter 6 of 20
Epoch 189/1000, LR 0.000249
Train loss: 0.0284;  Loss pred: 0.0284; Loss self: 0.0000; time: 0.14s
Val loss: 0.0937 score: 0.9592 time: 0.09s
Test loss: 0.2725 score: 0.8980 time: 0.09s
     INFO: Early stopping counter 7 of 20
Epoch 190/1000, LR 0.000249
Train loss: 0.0260;  Loss pred: 0.0260; Loss self: 0.0000; time: 0.13s
Val loss: 0.0935 score: 0.9592 time: 0.09s
Test loss: 0.2725 score: 0.8980 time: 0.08s
     INFO: Early stopping counter 8 of 20
Epoch 191/1000, LR 0.000249
Train loss: 0.0269;  Loss pred: 0.0269; Loss self: 0.0000; time: 0.13s
Val loss: 0.0934 score: 0.9592 time: 0.09s
Test loss: 0.2727 score: 0.8980 time: 0.09s
     INFO: Early stopping counter 9 of 20
Epoch 192/1000, LR 0.000248
Train loss: 0.0260;  Loss pred: 0.0260; Loss self: 0.0000; time: 0.13s
Val loss: 0.0934 score: 0.9592 time: 0.09s
Test loss: 0.2729 score: 0.8980 time: 0.08s
     INFO: Early stopping counter 10 of 20
Epoch 193/1000, LR 0.000248
Train loss: 0.0267;  Loss pred: 0.0267; Loss self: 0.0000; time: 0.13s
Val loss: 0.0931 score: 0.9592 time: 0.09s
Test loss: 0.2730 score: 0.8980 time: 0.08s
Epoch 194/1000, LR 0.000248
Train loss: 0.0287;  Loss pred: 0.0287; Loss self: 0.0000; time: 0.15s
Val loss: 0.0933 score: 0.9592 time: 0.09s
Test loss: 0.2728 score: 0.8980 time: 0.08s
     INFO: Early stopping counter 1 of 20
Epoch 195/1000, LR 0.000248
Train loss: 0.0223;  Loss pred: 0.0223; Loss self: 0.0000; time: 0.14s
Val loss: 0.0931 score: 0.9592 time: 0.09s
Test loss: 0.2730 score: 0.8980 time: 0.10s
Epoch 196/1000, LR 0.000247
Train loss: 0.0256;  Loss pred: 0.0256; Loss self: 0.0000; time: 0.12s
Val loss: 0.0932 score: 0.9592 time: 0.08s
Test loss: 0.2731 score: 0.8980 time: 0.09s
     INFO: Early stopping counter 1 of 20
Epoch 197/1000, LR 0.000247
Train loss: 0.0242;  Loss pred: 0.0242; Loss self: 0.0000; time: 0.12s
Val loss: 0.0935 score: 0.9592 time: 0.09s
Test loss: 0.2730 score: 0.8980 time: 0.08s
     INFO: Early stopping counter 2 of 20
Epoch 198/1000, LR 0.000247
Train loss: 0.0239;  Loss pred: 0.0239; Loss self: 0.0000; time: 0.13s
Val loss: 0.0938 score: 0.9592 time: 0.09s
Test loss: 0.2728 score: 0.8980 time: 0.08s
     INFO: Early stopping counter 3 of 20
Epoch 199/1000, LR 0.000247
Train loss: 0.0246;  Loss pred: 0.0246; Loss self: 0.0000; time: 0.13s
Val loss: 0.0935 score: 0.9592 time: 0.09s
Test loss: 0.2730 score: 0.8980 time: 0.08s
     INFO: Early stopping counter 4 of 20
Epoch 200/1000, LR 0.000246
Train loss: 0.0224;  Loss pred: 0.0224; Loss self: 0.0000; time: 0.13s
Val loss: 0.0940 score: 0.9592 time: 0.09s
Test loss: 0.2728 score: 0.8980 time: 0.08s
     INFO: Early stopping counter 5 of 20
Epoch 201/1000, LR 0.000246
Train loss: 0.0290;  Loss pred: 0.0290; Loss self: 0.0000; time: 0.12s
Val loss: 0.0934 score: 0.9592 time: 0.09s
Test loss: 0.2728 score: 0.8980 time: 0.08s
     INFO: Early stopping counter 6 of 20
Epoch 202/1000, LR 0.000246
Train loss: 0.0225;  Loss pred: 0.0225; Loss self: 0.0000; time: 0.12s
Val loss: 0.0929 score: 0.9592 time: 0.21s
Test loss: 0.2726 score: 0.8980 time: 0.08s
Epoch 203/1000, LR 0.000246
Train loss: 0.0227;  Loss pred: 0.0227; Loss self: 0.0000; time: 0.13s
Val loss: 0.0926 score: 0.9592 time: 0.09s
Test loss: 0.2727 score: 0.8980 time: 0.08s
Epoch 204/1000, LR 0.000245
Train loss: 0.0228;  Loss pred: 0.0228; Loss self: 0.0000; time: 0.14s
Val loss: 0.0924 score: 0.9592 time: 0.09s
Test loss: 0.2725 score: 0.8776 time: 0.08s
Epoch 205/1000, LR 0.000245
Train loss: 0.0229;  Loss pred: 0.0229; Loss self: 0.0000; time: 0.13s
Val loss: 0.0919 score: 0.9592 time: 0.09s
Test loss: 0.2729 score: 0.8776 time: 0.09s
Epoch 206/1000, LR 0.000245
Train loss: 0.0208;  Loss pred: 0.0208; Loss self: 0.0000; time: 0.13s
Val loss: 0.0914 score: 0.9796 time: 0.09s
Test loss: 0.2733 score: 0.8776 time: 0.08s
Epoch 207/1000, LR 0.000245
Train loss: 0.0237;  Loss pred: 0.0237; Loss self: 0.0000; time: 0.13s
Val loss: 0.0913 score: 0.9796 time: 0.10s
Test loss: 0.2734 score: 0.8776 time: 0.08s
Epoch 208/1000, LR 0.000244
Train loss: 0.0228;  Loss pred: 0.0228; Loss self: 0.0000; time: 0.12s
Val loss: 0.0911 score: 0.9796 time: 0.09s
Test loss: 0.2738 score: 0.8776 time: 0.08s
Epoch 209/1000, LR 0.000244
Train loss: 0.0225;  Loss pred: 0.0225; Loss self: 0.0000; time: 0.13s
Val loss: 0.0909 score: 0.9796 time: 0.09s
Test loss: 0.2738 score: 0.8776 time: 0.09s
Epoch 210/1000, LR 0.000244
Train loss: 0.0228;  Loss pred: 0.0228; Loss self: 0.0000; time: 0.13s
Val loss: 0.0908 score: 0.9796 time: 0.20s
Test loss: 0.2736 score: 0.8776 time: 0.08s
Epoch 211/1000, LR 0.000244
Train loss: 0.0199;  Loss pred: 0.0199; Loss self: 0.0000; time: 0.13s
Val loss: 0.0905 score: 0.9796 time: 0.09s
Test loss: 0.2737 score: 0.8776 time: 0.08s
Epoch 212/1000, LR 0.000243
Train loss: 0.0227;  Loss pred: 0.0227; Loss self: 0.0000; time: 0.13s
Val loss: 0.0897 score: 0.9796 time: 0.08s
Test loss: 0.2743 score: 0.8776 time: 0.08s
Epoch 213/1000, LR 0.000243
Train loss: 0.0215;  Loss pred: 0.0215; Loss self: 0.0000; time: 0.12s
Val loss: 0.0903 score: 0.9796 time: 0.10s
Test loss: 0.2741 score: 0.8776 time: 0.09s
     INFO: Early stopping counter 1 of 20
Epoch 214/1000, LR 0.000243
Train loss: 0.0228;  Loss pred: 0.0228; Loss self: 0.0000; time: 0.13s
Val loss: 0.0909 score: 0.9796 time: 0.09s
Test loss: 0.2741 score: 0.8776 time: 0.09s
     INFO: Early stopping counter 2 of 20
Epoch 215/1000, LR 0.000243
Train loss: 0.0213;  Loss pred: 0.0213; Loss self: 0.0000; time: 0.15s
Val loss: 0.0915 score: 0.9796 time: 0.09s
Test loss: 0.2740 score: 0.8776 time: 0.08s
     INFO: Early stopping counter 3 of 20
Epoch 216/1000, LR 0.000242
Train loss: 0.0211;  Loss pred: 0.0211; Loss self: 0.0000; time: 0.15s
Val loss: 0.0918 score: 0.9592 time: 0.10s
Test loss: 0.2742 score: 0.8776 time: 0.09s
     INFO: Early stopping counter 4 of 20
Epoch 217/1000, LR 0.000242
Train loss: 0.0196;  Loss pred: 0.0196; Loss self: 0.0000; time: 0.13s
Val loss: 0.0921 score: 0.9592 time: 0.09s
Test loss: 0.2745 score: 0.8776 time: 0.09s
     INFO: Early stopping counter 5 of 20
Epoch 218/1000, LR 0.000242
Train loss: 0.0200;  Loss pred: 0.0200; Loss self: 0.0000; time: 0.14s
Val loss: 0.0923 score: 0.9592 time: 0.22s
Test loss: 0.2748 score: 0.8776 time: 0.08s
     INFO: Early stopping counter 6 of 20
Epoch 219/1000, LR 0.000242
Train loss: 0.0182;  Loss pred: 0.0182; Loss self: 0.0000; time: 0.12s
Val loss: 0.0924 score: 0.9592 time: 0.09s
Test loss: 0.2752 score: 0.8776 time: 0.08s
     INFO: Early stopping counter 7 of 20
Epoch 220/1000, LR 0.000241
Train loss: 0.0180;  Loss pred: 0.0180; Loss self: 0.0000; time: 0.12s
Val loss: 0.0928 score: 0.9592 time: 0.09s
Test loss: 0.2753 score: 0.8776 time: 0.08s
     INFO: Early stopping counter 8 of 20
Epoch 221/1000, LR 0.000241
Train loss: 0.0169;  Loss pred: 0.0169; Loss self: 0.0000; time: 0.12s
Val loss: 0.0930 score: 0.9592 time: 0.09s
Test loss: 0.2756 score: 0.8776 time: 0.08s
     INFO: Early stopping counter 9 of 20
Epoch 222/1000, LR 0.000241
Train loss: 0.0199;  Loss pred: 0.0199; Loss self: 0.0000; time: 0.13s
Val loss: 0.0927 score: 0.9592 time: 0.09s
Test loss: 0.2759 score: 0.8776 time: 0.09s
     INFO: Early stopping counter 10 of 20
Epoch 223/1000, LR 0.000241
Train loss: 0.0205;  Loss pred: 0.0205; Loss self: 0.0000; time: 0.14s
Val loss: 0.0931 score: 0.9592 time: 0.09s
Test loss: 0.2760 score: 0.8776 time: 0.09s
     INFO: Early stopping counter 11 of 20
Epoch 224/1000, LR 0.000240
Train loss: 0.0191;  Loss pred: 0.0191; Loss self: 0.0000; time: 0.13s
Val loss: 0.0940 score: 0.9592 time: 0.09s
Test loss: 0.2759 score: 0.8776 time: 0.09s
     INFO: Early stopping counter 12 of 20
Epoch 225/1000, LR 0.000240
Train loss: 0.0213;  Loss pred: 0.0213; Loss self: 0.0000; time: 0.13s
Val loss: 0.0950 score: 0.9592 time: 0.09s
Test loss: 0.2755 score: 0.8776 time: 0.08s
     INFO: Early stopping counter 13 of 20
Epoch 226/1000, LR 0.000240
Train loss: 0.0186;  Loss pred: 0.0186; Loss self: 0.0000; time: 0.13s
Val loss: 0.0956 score: 0.9592 time: 0.23s
Test loss: 0.2756 score: 0.8776 time: 0.08s
     INFO: Early stopping counter 14 of 20
Epoch 227/1000, LR 0.000240
Train loss: 0.0198;  Loss pred: 0.0198; Loss self: 0.0000; time: 0.13s
Val loss: 0.0961 score: 0.9592 time: 0.09s
Test loss: 0.2756 score: 0.8776 time: 0.09s
     INFO: Early stopping counter 15 of 20
Epoch 228/1000, LR 0.000239
Train loss: 0.0202;  Loss pred: 0.0202; Loss self: 0.0000; time: 0.13s
Val loss: 0.0961 score: 0.9592 time: 0.09s
Test loss: 0.2758 score: 0.8776 time: 0.08s
     INFO: Early stopping counter 16 of 20
Epoch 229/1000, LR 0.000239
Train loss: 0.0176;  Loss pred: 0.0176; Loss self: 0.0000; time: 0.13s
Val loss: 0.0965 score: 0.9592 time: 0.09s
Test loss: 0.2759 score: 0.8776 time: 0.08s
     INFO: Early stopping counter 17 of 20
Epoch 230/1000, LR 0.000239
Train loss: 0.0194;  Loss pred: 0.0194; Loss self: 0.0000; time: 0.13s
Val loss: 0.0963 score: 0.9592 time: 0.09s
Test loss: 0.2763 score: 0.8776 time: 0.09s
     INFO: Early stopping counter 18 of 20
Epoch 231/1000, LR 0.000238
Train loss: 0.0174;  Loss pred: 0.0174; Loss self: 0.0000; time: 0.13s
Val loss: 0.0959 score: 0.9592 time: 0.09s
Test loss: 0.2768 score: 0.8776 time: 0.09s
     INFO: Early stopping counter 19 of 20
Epoch 232/1000, LR 0.000238
Train loss: 0.0236;  Loss pred: 0.0236; Loss self: 0.0000; time: 0.14s
Val loss: 0.0962 score: 0.9592 time: 0.09s
Test loss: 0.2767 score: 0.8776 time: 0.08s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 211,   Train_Loss: 0.0227,   Val_Loss: 0.0897,   Val_Precision: 0.9600,   Val_Recall: 1.0000,   Val_accuracy: 0.9796,   Val_Score: 0.9796,   Val_Loss: 0.0897,   Test_Precision: 0.8800,   Test_Recall: 0.8800,   Test_accuracy: 0.8800,   Test_Score: 0.8776,   Test_loss: 0.2743


[0.23724884004332125, 0.09203812386840582, 0.08675278094597161, 0.0865322828758508, 0.0864530960097909, 0.09074261179193854, 0.08487848099321127, 0.21471609501168132, 0.09031237801536918, 0.09099425096064806, 0.09360032598488033, 0.09093293990008533, 0.09124011802487075, 0.08735002810135484, 0.08735446399077773, 0.221397222019732, 0.09102942608296871, 0.09301880304701626, 0.09194997511804104, 0.09293873095884919, 0.09351313603110611, 0.09095243900083005, 0.2078144010156393, 0.08995314105413854, 0.0899076301138848, 0.08996079303324223, 0.09151540393941104, 0.08281276910565794, 0.09025979600846767, 0.09307349193841219, 0.09045870997942984, 0.09102973993867636, 0.09759132517501712, 0.08824983681552112, 0.089608252979815, 0.09178181807510555, 0.09078894392587245, 0.09231420187279582, 0.09019102295860648, 0.09639649698510766, 0.09290689299814403, 0.0906659928150475, 0.0915685030631721, 0.09321372816339135, 0.09336841898038983, 0.09299662313424051, 0.09024207689799368, 0.20793699100613594, 0.08661052794195712, 0.08612522180192173, 0.08854778693057597, 0.09205682598985732, 0.08689293707720935, 0.09591685980558395, 3.166880792938173, 0.1226649780292064, 0.0909090000204742, 0.0954559980891645, 0.0896890670992434, 0.09021354210563004, 0.09139875182881951, 0.08721370995044708, 0.08736699493601918, 0.09109418001025915, 0.0882823090068996, 0.0900058071129024, 0.10015329206362367, 0.08746834681369364, 0.08893413608893752, 0.08806889411062002, 0.09043149184435606, 0.08889440190978348, 0.08851258200593293, 0.09081882913596928, 0.09077718411572278, 0.103669716976583, 0.09068393590860069, 0.08930210187099874, 0.08969759591855109, 0.0891432948410511, 0.09054529410786927, 0.0894688309635967, 0.0926416649017483, 0.0931023359298706, 0.09228604193776846, 0.09270266187377274, 0.21073177503421903, 0.09100939380005002, 0.09088239003904164, 0.09057517908513546, 0.09430147800594568, 0.09089456195943058, 0.09153485600836575, 0.09460940910503268, 0.09295126819051802, 0.08942825999110937, 0.2111837409902364, 0.091310597024858, 0.08802607911638916, 0.08799178898334503, 0.08759210491552949, 0.08796574594452977, 0.08840393694117665, 0.09013702813535929, 0.0973417900968343, 0.08999137883074582, 0.08673636405728757, 0.0855279047973454, 0.08627921296283603, 0.0865545580163598, 0.08720736810937524, 0.08818202302791178, 0.08663987787440419, 0.18052313709631562, 0.08455151505768299, 0.08590411697514355, 0.08767110295593739, 0.08571469387970865, 0.08712288900278509, 0.08486388600431383, 0.09240930690430105, 0.08964980393648148, 0.08566962205804884, 0.10417881095781922, 0.08348598494194448, 0.12219241098500788, 0.09117314498871565, 0.08990018302574754, 0.08971071499399841, 0.08868591790087521, 0.0898641690146178, 0.09003106108866632, 0.09002413391135633, 0.09201176115311682, 0.09167887317016721, 0.0862127491272986, 0.09005230711773038, 0.08924389001913369, 0.09087094711139798, 0.09036758006550372, 0.11068795504979789, 0.08996169501915574, 0.08907402097247541, 0.08819644688628614, 0.08751740702427924, 0.08614207408390939, 0.09043530514463782, 0.0902371488045901, 0.08944934606552124, 0.08820795104838908, 0.08865901711396873, 0.09026679000817239, 0.0898644239641726, 0.08947404217906296, 0.08932587294839323, 0.0869651990942657, 0.08833865216001868, 0.09120633895508945, 0.09710805118083954, 0.09278523898683488, 0.08799143391661346, 0.08813946088775992, 0.08965902891941369, 0.08807822107337415, 0.09459932497702539, 0.09047625097446144, 0.09561633109115064, 0.09816235699690878, 0.09599934192374349, 0.0900929500348866, 0.09149874304421246, 0.08779161609709263, 0.09465950704179704, 0.08728855499066412, 0.08965142094530165, 0.08812940493226051, 0.09016566397622228, 0.08947452693246305, 0.087002047104761, 0.08826597384177148, 0.08979321783408523, 0.09271202119998634, 0.09256926900707185, 0.09127451293170452, 0.09314515301957726, 0.09237598185427487, 0.08938949787989259, 0.09048093995079398, 0.09035971807315946, 0.08949362207204103, 0.09108717599883676, 0.08953577606007457, 0.0888693688903004, 0.08683268004097044, 0.10526102501899004, 0.09269348788075149, 0.08829784905537963, 0.08716912614181638, 0.08861879492178559, 0.08929855492897332, 0.08557918993756175, 0.08716712985187769, 0.08876598393544555, 0.09003228484652936, 0.09108611405827105, 0.08998401206918061, 0.08900089003145695, 0.08912708098068833, 0.09337795106694102, 0.08958687912672758, 0.08880214905366302, 0.08494618209078908, 0.0972064658999443, 0.09853859920985997, 0.08784346119500697, 0.09437941201031208, 0.10119144897907972, 0.08806562214158475, 0.08837099699303508, 0.08758884179405868, 0.08839598600752652, 0.09168236888945103, 0.09450989193283021, 0.09101129812188447, 0.08903977298177779, 0.08608452486805618, 0.0904205550905317, 0.08707480318844318, 0.08984892698936164, 0.09288781392388046, 0.09135720296762884, 0.08818728593178093]
[0.004841813062108597, 0.0018783290585388942, 0.0017704649172647267, 0.0017659649566500162, 0.001764348898158998, 0.0018518900365701743, 0.0017322138978206382, 0.0043819611226873735, 0.0018431097554156976, 0.001857025529809144, 0.0019102107343853128, 0.0018557742836752108, 0.0018620432249973624, 0.0017826536347215272, 0.0017827441630770965, 0.004518310653463919, 0.001857743389448341, 0.0018983429193268626, 0.001876530104449817, 0.0018967087950785548, 0.0019084313475735942, 0.0018561722245067358, 0.004241110224808965, 0.0018357783888599702, 0.0018348495941609144, 0.001835934551698821, 0.0018676613048859397, 0.001690056512360366, 0.0018420366532340342, 0.0018994590191512692, 0.0018460961220291804, 0.0018577497946668643, 0.001991659697449329, 0.001801017077867778, 0.0018287398567309184, 0.0018730983280633785, 0.0018528355903239275, 0.0018839633035264453, 0.0018406331216042138, 0.0019672754486756666, 0.0018960590407784497, 0.0018503263839805613, 0.0018687449604729, 0.0019023209829263541, 0.0019054779383753026, 0.0018978902680457247, 0.001841675038734565, 0.004243612061349713, 0.0017675617947338186, 0.001757657587794321, 0.001807097692460734, 0.001878710734486884, 0.0017733252464736603, 0.0019574869348078358, 0.06463022026404434, 0.0025033668985552327, 0.001855285714703555, 0.0019480815936564182, 0.001830389124474355, 0.001841092696033266, 0.0018652806495677453, 0.0017798716316417772, 0.0017829998966534526, 0.0018590648981685542, 0.0018016797756510122, 0.0018368532063857634, 0.00204394473599232, 0.0017850683023202785, 0.0018149823691619902, 0.0017973243696044903, 0.0018455406498848175, 0.0018141714675466018, 0.001806379224610876, 0.0018534454925708016, 0.001852595594198424, 0.0021157085097261836, 0.0018506925695632793, 0.0018224918749183416, 0.0018305631820112467, 0.0018192509151234919, 0.0018478631450585565, 0.001825894509461157, 0.0018906462224846591, 0.0019000476720381755, 0.0018833886109748666, 0.0018918910586484233, 0.004300648470086103, 0.0018573345673479596, 0.0018547426538579926, 0.001848473042553785, 0.0019245199593050139, 0.0018549910603965424, 0.0018680582858850152, 0.0019308042674496466, 0.0018969646569493475, 0.0018250665304308034, 0.0043098722651068655, 0.0018634815719358775, 0.0017964505942120236, 0.0017957507955784701, 0.0017875939778679488, 0.0017952193049904034, 0.0018041619783913602, 0.0018395311864359038, 0.001986567144833353, 0.0018365587516478738, 0.0017701298787201546, 0.0017454674448437837, 0.001760800264547674, 0.0017664195513542817, 0.0017797422063137805, 0.0017996331230186078, 0.0017681607729470243, 0.0036841456550268494, 0.0017255411236261834, 0.0017531452443906848, 0.0017892061827742324, 0.0017492794669328295, 0.0017780181429139813, 0.0017319160409043639, 0.001885904222536756, 0.0018295878354383974, 0.0017483596338377315, 0.0021260981828126373, 0.0017037956110600913, 0.0024937226731634263, 0.0018606764283411357, 0.0018346976127703578, 0.0018308309182448654, 0.0018099166918545961, 0.0018339626329513838, 0.0018373685936462513, 0.0018372272226807413, 0.0018777910439411597, 0.0018709974116360654, 0.0017594438597407875, 0.0018378021860761301, 0.0018213038779415038, 0.0018545091247224078, 0.0018442363278674228, 0.0022589378581591405, 0.001835952959574607, 0.0018178371627035799, 0.0017999274874752273, 0.0017860695311077395, 0.0017580015119165182, 0.0018456184723395475, 0.001841574465399798, 0.0018254968584800254, 0.0018001622662936546, 0.0018093676962034435, 0.0018421793879218856, 0.0018339678360035224, 0.0018260008607972034, 0.0018229769989468005, 0.0017747999815156265, 0.0018028296359187486, 0.0018613538562263154, 0.0019817969628742765, 0.0018935763058537732, 0.001795743549318642, 0.0017987645079134678, 0.0018297761003961976, 0.001797514715783146, 0.0019305984689188854, 0.0018464541015196212, 0.0019513536957377683, 0.002003313408100179, 0.0019591702433417037, 0.0018386316333650326, 0.0018673212866165809, 0.0017916656346345435, 0.0019318266743223887, 0.001781399081442125, 0.001829620835618401, 0.0017985592843318473, 0.001840115591351475, 0.0018260107537237356, 0.0017755519817298164, 0.001801346404934112, 0.0018325146496752087, 0.0018920820653058436, 0.0018891687552463642, 0.0018627451618715208, 0.0019009214901954544, 0.0018852241194749974, 0.0018242754669365834, 0.001846549794914163, 0.0018440758790440705, 0.001826400450449817, 0.0018589219591599337, 0.001827260735919889, 0.0018136605895979672, 0.0017720955110402132, 0.0021481841840610212, 0.0018917038343010508, 0.0018019969194975434, 0.0017789617579962527, 0.0018085468351384814, 0.0018224194883463942, 0.001746514080358403, 0.001778921017385259, 0.0018115506925601133, 0.0018373935682965176, 0.001858900286903491, 0.0018364084095751144, 0.0018163446945195295, 0.0018189200200140476, 0.0019056724707538985, 0.0018283036556475017, 0.0018122887561972045, 0.0017335955528732464, 0.0019838054265294757, 0.002010991820609387, 0.001792723697857285, 0.0019261104491900426, 0.0020651316118179535, 0.0017972575947262195, 0.0018034897345517362, 0.0017875273835522179, 0.0018039997144393167, 0.0018710687528459393, 0.001928773304751637, 0.0018573734310588666, 0.001817138224117914, 0.0017568270381235955, 0.0018453174508271776, 0.0017770367997641467, 0.0018336515712114622, 0.0018956696719159276, 0.0018644327136250784, 0.0017997405292200191]
[206.53420261634443, 532.3880794230353, 564.8233920076464, 566.2626521745769, 566.7813214514689, 539.9888655657264, 577.2959108907605, 228.20832316894655, 542.5612864679665, 538.4955585951342, 523.5024502790213, 538.8586364175609, 537.044460931575, 560.9614680735287, 560.9329822591902, 221.3216568527354, 538.2874759128875, 526.7752152780658, 532.8984585052482, 527.2290625712966, 523.990554478899, 538.7431116558931, 235.78731676209702, 544.7280598073748, 545.0037993208401, 544.68172575906, 535.4289867139862, 591.6961904447683, 542.8773625347335, 526.4656883446886, 541.6836036147599, 538.2856199853985, 502.09380713014184, 555.2418199076151, 546.8246324480587, 533.8748025224673, 539.7132941650651, 530.7959014531638, 543.2913209387673, 508.31722658521534, 527.4097369823663, 540.4451931603141, 535.1184999299971, 525.6736423427832, 524.802717397319, 526.9008524026573, 542.9839569781599, 235.64830751327972, 565.7510832036243, 568.9390282523099, 553.3735138792053, 532.2799202896562, 563.9123460225621, 510.85909296154193, 15.472637968964634, 399.46202075977345, 539.0005388791473, 513.3255215060408, 546.3319174206613, 543.1557043024254, 536.1123540480287, 561.8382709305764, 560.8525283018353, 537.9048364503808, 555.0375896508378, 544.4093172625504, 489.2500185502851, 560.2026537024796, 550.969539424076, 556.382596770807, 541.8466399330794, 551.2158127767008, 553.5936122247068, 539.5356939323642, 539.7832117984051, 472.65490279160457, 540.3382584693561, 548.6992912079819, 546.2799699168517, 549.67678822474, 541.1656175264599, 547.6767660006337, 528.9196826499962, 526.3025842542692, 530.9578672042553, 528.5716613695532, 232.52307342850068, 538.4059595831861, 539.1583559691858, 540.9870617417473, 519.6100955799501, 539.0861559118401, 535.3152027192973, 517.9188884437656, 527.1579501160427, 547.9252308484074, 232.02543799176948, 536.6299377788588, 556.6532156363752, 556.8701417046385, 559.4111483820801, 557.0350080461872, 554.273957647432, 543.6167689755249, 503.3809215061225, 544.4966021929537, 564.9302980654862, 572.9124326861898, 567.9235857321311, 566.1169223547815, 561.8791285908817, 555.6688122758342, 565.5594306242201, 271.43335080564617, 579.5283498654172, 570.4033953829967, 558.9070782493395, 571.6639444430173, 562.4239572500127, 577.3951949066911, 530.2496214017094, 546.5711897676586, 571.9647037405874, 470.3451647172238, 586.9248597123724, 401.006900551393, 537.4389575577836, 545.0489459622828, 546.2000832707449, 552.5116180763624, 545.2673800614502, 544.2566088579448, 544.2984883170175, 532.5406163942355, 534.4742829577541, 568.3614140137022, 544.128202467257, 549.0571958427015, 539.2262495066909, 542.2298568190266, 442.68592709979305, 544.6762645986864, 550.1042780491676, 555.5779368660612, 559.8886172028192, 568.8277246757492, 541.8237923964738, 543.0136107924934, 547.7960673307518, 555.5054778805553, 552.6792603284994, 542.8352996219733, 545.2658331125058, 547.644867792349, 548.5532733423055, 563.4437741801358, 554.6835818961813, 537.2433600709255, 504.59255853821753, 528.1012425581241, 556.8723888104342, 555.9371421887686, 546.5149532685841, 556.3236791440215, 517.9740977211017, 541.5785852337222, 512.4647582774171, 499.1730180393188, 510.42016557699804, 543.882734231988, 535.5264823290858, 558.1398563822852, 517.6447831950359, 561.3565261246536, 546.5613314695369, 556.000577079389, 543.4441209563084, 547.6419007723401, 563.2051386216012, 555.140309082626, 545.6982295760845, 528.5183017885411, 529.3333362744459, 536.8420868667235, 526.060652771714, 530.4409113323263, 548.1628285443377, 541.5505191109592, 542.277034998353, 547.5250511210255, 537.9461978338835, 547.2672729962508, 551.3710810806499, 564.3036697344851, 465.5094323008916, 528.6239747827552, 554.9399053794349, 562.1256305848631, 552.9301097272548, 548.7210855648653, 572.5691027894774, 562.1385043108022, 552.0132580925922, 544.2492110860707, 537.9524695570276, 544.5411787410447, 550.5562919952955, 549.7767845736707, 524.7491451688927, 546.9550951840358, 551.7884479393553, 576.8358129106922, 504.08169401443155, 497.2670648143024, 557.8104429562842, 519.181026415445, 484.2306389952993, 556.403268476566, 554.4805611264284, 559.4319892391106, 554.3238128010458, 534.4539042079435, 518.4642474760751, 538.3946939684131, 550.3158685055046, 569.2079973154696, 541.9121786073948, 562.7345478341939, 545.3598795431551, 527.5180664726854, 536.3561756303164, 555.63565067537]
Elapsed: 0.10811198402822403~0.2025253788311143
Time per graph: 0.002206367020984164~0.004133170996553352
Speed: 528.6448773172713~69.91353763916338
Total Time: 0.0891
best val loss: 0.08974657207727432 test_score: 0.8776

Testing...
Test loss: 0.2733 score: 0.8776 time: 0.08s
test Score 0.8776
Epoch Time List: [0.4508092768955976, 0.3070920908357948, 0.2974808420985937, 0.3007469158619642, 0.29867733223363757, 0.3189322880934924, 0.29995247977785766, 0.4430777980014682, 0.30277947383001447, 0.3086105757392943, 0.3093104548752308, 0.31197931827045977, 0.31028099078685045, 0.296587459044531, 0.30791683681309223, 0.452049741987139, 0.3068047631531954, 0.32514802436344326, 0.31147579103708267, 0.31698304298333824, 0.3158940391149372, 0.5409636630211025, 0.4301249790005386, 0.2957190468441695, 0.3075818223878741, 0.30983677157200873, 0.31063425401225686, 0.29288325691595674, 0.29816899285651743, 0.30855516088195145, 0.30301867821253836, 0.45043403282761574, 0.314528230112046, 0.30104653909802437, 0.30651373974978924, 0.3106672507710755, 0.3116959538310766, 0.31244988553225994, 0.3102447579149157, 0.31436731899157166, 0.4289464079774916, 0.3146040278952569, 0.31443045986816287, 0.3170021940022707, 0.31418308382853866, 0.31814012792892754, 0.30733542679809034, 0.42816265881992877, 0.30104178097099066, 0.2984959650784731, 0.30101055442355573, 0.3033766867592931, 0.2957819397561252, 0.30025161686353385, 5.439451368991286, 1.939063988160342, 0.3023819960653782, 0.32890042290091515, 0.3005390060134232, 0.3059814600273967, 0.3104550950229168, 0.30322231887839735, 0.2949705522041768, 0.30622234917245805, 0.2989605369511992, 0.29768975428305566, 0.3105018970090896, 0.3123672781512141, 0.2914067867677659, 0.2999130510725081, 0.3014565131161362, 0.2972875910345465, 0.3029139139689505, 0.30675380281172693, 0.30161948478780687, 0.3211245040874928, 0.301776607055217, 0.31338142580352724, 0.3072001291438937, 0.3096798041369766, 0.30190809979103506, 0.3071031130384654, 0.4242815771140158, 0.31498940917663276, 0.3126744271721691, 0.3166290116496384, 0.45220593619160354, 0.3145972737111151, 0.3129913138691336, 0.3074410608969629, 0.3135956502519548, 0.30647686985321343, 0.30268257996067405, 0.31290644500404596, 0.3139831554144621, 0.32472115964628756, 0.4205667939968407, 0.2958643559832126, 0.2971589248627424, 0.29533739504404366, 0.30116873723454773, 0.29957090807147324, 0.298228417057544, 0.2992389991413802, 0.36186988395638764, 0.29497691011056304, 0.2906447390560061, 0.29258750984445214, 0.2897995610255748, 0.28071685205213726, 0.293376584071666, 0.2938604343216866, 0.2941603281069547, 0.3980981446802616, 0.27875960478559136, 0.29291771189309657, 0.2925806201528758, 0.2962420720141381, 0.2855291140731424, 0.29301387909799814, 0.2985730180516839, 0.4171671026851982, 0.3249923412222415, 0.32122206105850637, 0.2928521418944001, 0.338657567743212, 0.3078535217791796, 0.29438111395575106, 0.29877174575813115, 0.405639355070889, 0.2927183071151376, 0.2991818729788065, 0.30522995884530246, 0.30744716012850404, 0.3097085910849273, 0.30140858399681747, 0.2998970369808376, 0.3943131840787828, 0.29842788306996226, 0.3024992817081511, 0.32168451440520585, 0.3042407422326505, 0.31036635604687035, 0.30376975564286113, 0.29622170608490705, 0.3879631529562175, 0.2985584808047861, 0.3091707800049335, 0.29914474091492593, 0.29802547092549503, 0.29335695900954306, 0.3047997171524912, 0.30138200521469116, 0.3929511469323188, 0.29627416119910777, 0.29801060468889773, 0.2986612927634269, 0.3012388236820698, 0.31504758913069963, 0.32207043608650565, 0.4254137398675084, 0.29929423006251454, 0.2973416061140597, 0.2974502413999289, 0.31554697011597455, 0.3012160179205239, 0.3121408987790346, 0.3227209278848022, 0.315779997035861, 0.40940228290855885, 0.29797988338395953, 0.3074407409876585, 0.3101475241128355, 0.30617829505354166, 0.3049566268455237, 0.30471025104634464, 0.2998091799672693, 0.41980571509338915, 0.2919372068718076, 0.2942001251503825, 0.29441848094575107, 0.3035931831691414, 0.30677459575235844, 0.30394078977406025, 0.31018955004401505, 0.44257661513984203, 0.30439548776485026, 0.30221236078068614, 0.31251528509892523, 0.3045650019776076, 0.30921331397257745, 0.30318239401094615, 0.3001609621569514, 0.32323995302431285, 0.3321873371023685, 0.2961687420029193, 0.2912444258108735, 0.29599840589798987, 0.29589992109686136, 0.2980103858280927, 0.29065100103616714, 0.4166499082930386, 0.2987802217248827, 0.30731523176655173, 0.3061769320629537, 0.3079274888150394, 0.30924907396547496, 0.29477318888530135, 0.3057236969470978, 0.41958458675071597, 0.29985752515494823, 0.28943479619920254, 0.3145391969010234, 0.3058680861722678, 0.3175489290151745, 0.3351608859375119, 0.3076159539632499, 0.43851399002596736, 0.2939756189007312, 0.2897114201914519, 0.2941924089100212, 0.3035467907320708, 0.3173790068831295, 0.30917054088786244, 0.3011328207794577, 0.444947024108842, 0.3106291990261525, 0.3013878876809031, 0.3058662568219006, 0.30765261268243194, 0.3112448239699006, 0.30625540809705853]
Total Epoch List: [232]
Total Time List: [0.08906854595988989]
========================k_idx:1========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: SGFormer
SGFormer(
  (trans_conv): SGModule(
    (attns): ModuleList(
      (0-1): 2 x SGFormerAttention(heads=1, head_channels=64)
    )
    (fcs): ModuleList(
      (0): Linear(in_features=14887, out_features=64, bias=True)
    )
    (bns): ModuleList(
      (0-2): 3 x LayerNorm((64,), eps=1e-05, elementwise_affine=True)
    )
  )
  (graph_conv): GraphModule(
    (convs): ModuleList(
      (0-2): 3 x GCNConv(64, 64)
    )
    (fcs): ModuleList(
      (0): Linear(in_features=14887, out_features=64, bias=True)
    )
    (bns): ModuleList(
      (0-3): 4 x BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (classifier): Linear(in_features=64, out_features=2, bias=True)
)
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7f342d3761d0>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.6998;  Loss pred: 0.6998; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7123 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7020 score: 0.5102 time: 0.09s
Epoch 2/1000, LR 0.000000
Train loss: 0.6984;  Loss pred: 0.6984; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7113 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7009 score: 0.5102 time: 0.09s
Epoch 3/1000, LR 0.000030
Train loss: 0.7193;  Loss pred: 0.7193; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7095 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6990 score: 0.5102 time: 0.09s
Epoch 4/1000, LR 0.000060
Train loss: 0.6945;  Loss pred: 0.6945; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7068 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6962 score: 0.5102 time: 0.09s
Epoch 5/1000, LR 0.000090
Train loss: 0.6935;  Loss pred: 0.6935; Loss self: 0.0000; time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7036 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6927 score: 0.5102 time: 0.09s
Epoch 6/1000, LR 0.000120
Train loss: 0.6661;  Loss pred: 0.6661; Loss self: 0.0000; time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6996 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6886 score: 0.5102 time: 0.09s
Epoch 7/1000, LR 0.000150
Train loss: 0.6469;  Loss pred: 0.6469; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6951 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6841 score: 0.5102 time: 0.09s
Epoch 8/1000, LR 0.000180
Train loss: 0.6630;  Loss pred: 0.6630; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6901 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6791 score: 0.5102 time: 0.10s
Epoch 9/1000, LR 0.000210
Train loss: 0.6409;  Loss pred: 0.6409; Loss self: 0.0000; time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6844 score: 0.4898 time: 0.09s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6737 score: 0.5102 time: 0.08s
Epoch 10/1000, LR 0.000240
Train loss: 0.6162;  Loss pred: 0.6162; Loss self: 0.0000; time: 0.13s
Val loss: 0.6785 score: 0.5102 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6680 score: 0.5102 time: 0.09s
Epoch 11/1000, LR 0.000270
Train loss: 0.5821;  Loss pred: 0.5821; Loss self: 0.0000; time: 0.14s
Val loss: 0.6729 score: 0.5510 time: 0.08s
Test loss: 0.6627 score: 0.6122 time: 0.09s
Epoch 12/1000, LR 0.000270
Train loss: 0.5671;  Loss pred: 0.5671; Loss self: 0.0000; time: 0.13s
Val loss: 0.6676 score: 0.6735 time: 0.08s
Test loss: 0.6575 score: 0.7959 time: 0.09s
Epoch 13/1000, LR 0.000270
Train loss: 0.5658;  Loss pred: 0.5658; Loss self: 0.0000; time: 0.13s
Val loss: 0.6624 score: 0.8367 time: 0.08s
Test loss: 0.6521 score: 0.9796 time: 0.09s
Epoch 14/1000, LR 0.000270
Train loss: 0.5385;  Loss pred: 0.5385; Loss self: 0.0000; time: 0.13s
Val loss: 0.6573 score: 0.8367 time: 0.08s
Test loss: 0.6466 score: 0.9796 time: 0.09s
Epoch 15/1000, LR 0.000270
Train loss: 0.5213;  Loss pred: 0.5213; Loss self: 0.0000; time: 0.14s
Val loss: 0.6518 score: 0.8776 time: 0.09s
Test loss: 0.6410 score: 0.9388 time: 0.09s
Epoch 16/1000, LR 0.000270
Train loss: 0.5335;  Loss pred: 0.5335; Loss self: 0.0000; time: 0.14s
Val loss: 0.6468 score: 0.8571 time: 0.08s
Test loss: 0.6358 score: 0.8980 time: 0.09s
Epoch 17/1000, LR 0.000270
Train loss: 0.5061;  Loss pred: 0.5061; Loss self: 0.0000; time: 0.14s
Val loss: 0.6417 score: 0.8367 time: 0.08s
Test loss: 0.6305 score: 0.8163 time: 0.09s
Epoch 18/1000, LR 0.000270
Train loss: 0.5019;  Loss pred: 0.5019; Loss self: 0.0000; time: 0.15s
Val loss: 0.6366 score: 0.8163 time: 0.08s
Test loss: 0.6250 score: 0.7143 time: 0.09s
Epoch 19/1000, LR 0.000270
Train loss: 0.4676;  Loss pred: 0.4676; Loss self: 0.0000; time: 0.14s
Val loss: 0.6315 score: 0.7755 time: 0.08s
Test loss: 0.6195 score: 0.7143 time: 0.09s
Epoch 20/1000, LR 0.000270
Train loss: 0.4587;  Loss pred: 0.4587; Loss self: 0.0000; time: 0.14s
Val loss: 0.6260 score: 0.7143 time: 0.08s
Test loss: 0.6140 score: 0.6735 time: 0.09s
Epoch 21/1000, LR 0.000270
Train loss: 0.4597;  Loss pred: 0.4597; Loss self: 0.0000; time: 0.13s
Val loss: 0.6210 score: 0.6939 time: 0.08s
Test loss: 0.6088 score: 0.6327 time: 0.09s
Epoch 22/1000, LR 0.000270
Train loss: 0.4360;  Loss pred: 0.4360; Loss self: 0.0000; time: 0.14s
Val loss: 0.6162 score: 0.6939 time: 0.08s
Test loss: 0.6040 score: 0.6327 time: 0.09s
Epoch 23/1000, LR 0.000270
Train loss: 0.4159;  Loss pred: 0.4159; Loss self: 0.0000; time: 0.14s
Val loss: 0.6116 score: 0.6735 time: 0.08s
Test loss: 0.5994 score: 0.6122 time: 0.09s
Epoch 24/1000, LR 0.000270
Train loss: 0.4049;  Loss pred: 0.4049; Loss self: 0.0000; time: 0.14s
Val loss: 0.6070 score: 0.6735 time: 0.08s
Test loss: 0.5950 score: 0.6122 time: 0.09s
Epoch 25/1000, LR 0.000270
Train loss: 0.4037;  Loss pred: 0.4037; Loss self: 0.0000; time: 0.15s
Val loss: 0.6024 score: 0.6735 time: 0.08s
Test loss: 0.5905 score: 0.6122 time: 0.09s
Epoch 26/1000, LR 0.000270
Train loss: 0.3847;  Loss pred: 0.3847; Loss self: 0.0000; time: 0.15s
Val loss: 0.5972 score: 0.6735 time: 0.08s
Test loss: 0.5855 score: 0.6122 time: 0.09s
Epoch 27/1000, LR 0.000270
Train loss: 0.4015;  Loss pred: 0.4015; Loss self: 0.0000; time: 0.14s
Val loss: 0.5917 score: 0.6735 time: 0.08s
Test loss: 0.5806 score: 0.6122 time: 0.08s
Epoch 28/1000, LR 0.000270
Train loss: 0.3846;  Loss pred: 0.3846; Loss self: 0.0000; time: 0.14s
Val loss: 0.5855 score: 0.6735 time: 0.09s
Test loss: 0.5748 score: 0.6122 time: 0.10s
Epoch 29/1000, LR 0.000270
Train loss: 0.3695;  Loss pred: 0.3695; Loss self: 0.0000; time: 0.17s
Val loss: 0.5788 score: 0.6735 time: 0.08s
Test loss: 0.5690 score: 0.6327 time: 0.09s
Epoch 30/1000, LR 0.000270
Train loss: 0.3595;  Loss pred: 0.3595; Loss self: 0.0000; time: 0.14s
Val loss: 0.5708 score: 0.6735 time: 0.08s
Test loss: 0.5620 score: 0.6327 time: 0.09s
Epoch 31/1000, LR 0.000270
Train loss: 0.3533;  Loss pred: 0.3533; Loss self: 0.0000; time: 0.15s
Val loss: 0.5623 score: 0.6939 time: 0.08s
Test loss: 0.5547 score: 0.6531 time: 0.09s
Epoch 32/1000, LR 0.000270
Train loss: 0.3336;  Loss pred: 0.3336; Loss self: 0.0000; time: 0.14s
Val loss: 0.5537 score: 0.7143 time: 0.08s
Test loss: 0.5475 score: 0.6531 time: 0.09s
Epoch 33/1000, LR 0.000270
Train loss: 0.3254;  Loss pred: 0.3254; Loss self: 0.0000; time: 0.15s
Val loss: 0.5454 score: 0.7347 time: 0.09s
Test loss: 0.5406 score: 0.6531 time: 0.09s
Epoch 34/1000, LR 0.000270
Train loss: 0.3189;  Loss pred: 0.3189; Loss self: 0.0000; time: 0.15s
Val loss: 0.5376 score: 0.7347 time: 0.08s
Test loss: 0.5340 score: 0.6939 time: 0.09s
Epoch 35/1000, LR 0.000270
Train loss: 0.3209;  Loss pred: 0.3209; Loss self: 0.0000; time: 0.14s
Val loss: 0.5304 score: 0.7347 time: 0.08s
Test loss: 0.5278 score: 0.6939 time: 0.09s
Epoch 36/1000, LR 0.000270
Train loss: 0.3077;  Loss pred: 0.3077; Loss self: 0.0000; time: 0.14s
Val loss: 0.5243 score: 0.7755 time: 0.08s
Test loss: 0.5225 score: 0.6939 time: 0.09s
Epoch 37/1000, LR 0.000270
Train loss: 0.3091;  Loss pred: 0.3091; Loss self: 0.0000; time: 0.15s
Val loss: 0.5179 score: 0.7959 time: 0.08s
Test loss: 0.5166 score: 0.6939 time: 0.09s
Epoch 38/1000, LR 0.000270
Train loss: 0.2902;  Loss pred: 0.2902; Loss self: 0.0000; time: 0.14s
Val loss: 0.5133 score: 0.7959 time: 0.08s
Test loss: 0.5125 score: 0.6939 time: 0.09s
Epoch 39/1000, LR 0.000269
Train loss: 0.2861;  Loss pred: 0.2861; Loss self: 0.0000; time: 0.14s
Val loss: 0.5093 score: 0.7959 time: 0.08s
Test loss: 0.5092 score: 0.6939 time: 0.09s
Epoch 40/1000, LR 0.000269
Train loss: 0.2884;  Loss pred: 0.2884; Loss self: 0.0000; time: 0.14s
Val loss: 0.5047 score: 0.7959 time: 0.08s
Test loss: 0.5052 score: 0.6939 time: 0.09s
Epoch 41/1000, LR 0.000269
Train loss: 0.2778;  Loss pred: 0.2778; Loss self: 0.0000; time: 0.14s
Val loss: 0.4990 score: 0.8163 time: 0.22s
Test loss: 0.5000 score: 0.6939 time: 0.09s
Epoch 42/1000, LR 0.000269
Train loss: 0.2683;  Loss pred: 0.2683; Loss self: 0.0000; time: 0.14s
Val loss: 0.4931 score: 0.8163 time: 0.08s
Test loss: 0.4947 score: 0.7143 time: 0.09s
Epoch 43/1000, LR 0.000269
Train loss: 0.2682;  Loss pred: 0.2682; Loss self: 0.0000; time: 0.13s
Val loss: 0.4871 score: 0.8163 time: 0.08s
Test loss: 0.4887 score: 0.7143 time: 0.08s
Epoch 44/1000, LR 0.000269
Train loss: 0.2568;  Loss pred: 0.2568; Loss self: 0.0000; time: 0.14s
Val loss: 0.4824 score: 0.8367 time: 0.08s
Test loss: 0.4838 score: 0.7143 time: 0.09s
Epoch 45/1000, LR 0.000269
Train loss: 0.2415;  Loss pred: 0.2415; Loss self: 0.0000; time: 0.14s
Val loss: 0.4777 score: 0.8367 time: 0.08s
Test loss: 0.4789 score: 0.7143 time: 0.09s
Epoch 46/1000, LR 0.000269
Train loss: 0.2462;  Loss pred: 0.2462; Loss self: 0.0000; time: 0.15s
Val loss: 0.4732 score: 0.8367 time: 0.08s
Test loss: 0.4739 score: 0.7143 time: 0.09s
Epoch 47/1000, LR 0.000269
Train loss: 0.2519;  Loss pred: 0.2519; Loss self: 0.0000; time: 0.14s
Val loss: 0.4706 score: 0.8367 time: 0.08s
Test loss: 0.4721 score: 0.7143 time: 0.09s
Epoch 48/1000, LR 0.000269
Train loss: 0.2296;  Loss pred: 0.2296; Loss self: 0.0000; time: 0.14s
Val loss: 0.4685 score: 0.8367 time: 0.08s
Test loss: 0.4710 score: 0.7143 time: 0.09s
Epoch 49/1000, LR 0.000269
Train loss: 0.2338;  Loss pred: 0.2338; Loss self: 0.0000; time: 0.14s
Val loss: 0.4671 score: 0.8163 time: 0.22s
Test loss: 0.4722 score: 0.7143 time: 0.09s
Epoch 50/1000, LR 0.000269
Train loss: 0.2272;  Loss pred: 0.2272; Loss self: 0.0000; time: 0.14s
Val loss: 0.4653 score: 0.8163 time: 0.08s
Test loss: 0.4730 score: 0.7143 time: 0.09s
Epoch 51/1000, LR 0.000269
Train loss: 0.2097;  Loss pred: 0.2097; Loss self: 0.0000; time: 0.14s
Val loss: 0.4624 score: 0.8163 time: 0.08s
Test loss: 0.4714 score: 0.7143 time: 0.09s
Epoch 52/1000, LR 0.000269
Train loss: 0.2138;  Loss pred: 0.2138; Loss self: 0.0000; time: 0.13s
Val loss: 0.4600 score: 0.8163 time: 0.08s
Test loss: 0.4702 score: 0.7143 time: 0.09s
Epoch 53/1000, LR 0.000269
Train loss: 0.2098;  Loss pred: 0.2098; Loss self: 0.0000; time: 0.14s
Val loss: 0.4564 score: 0.8163 time: 0.08s
Test loss: 0.4675 score: 0.7143 time: 0.09s
Epoch 54/1000, LR 0.000269
Train loss: 0.2161;  Loss pred: 0.2161; Loss self: 0.0000; time: 0.16s
Val loss: 0.4539 score: 0.8163 time: 0.08s
Test loss: 0.4662 score: 0.7143 time: 0.09s
Epoch 55/1000, LR 0.000269
Train loss: 0.1892;  Loss pred: 0.1892; Loss self: 0.0000; time: 0.14s
Val loss: 0.4498 score: 0.8163 time: 0.08s
Test loss: 0.4623 score: 0.7143 time: 0.09s
Epoch 56/1000, LR 0.000269
Train loss: 0.1912;  Loss pred: 0.1912; Loss self: 0.0000; time: 0.14s
Val loss: 0.4467 score: 0.8163 time: 0.08s
Test loss: 0.4597 score: 0.7143 time: 0.09s
Epoch 57/1000, LR 0.000269
Train loss: 0.1964;  Loss pred: 0.1964; Loss self: 0.0000; time: 0.29s
Val loss: 0.4420 score: 0.8163 time: 0.08s
Test loss: 0.4546 score: 0.7143 time: 0.09s
Epoch 58/1000, LR 0.000269
Train loss: 0.2018;  Loss pred: 0.2018; Loss self: 0.0000; time: 0.15s
Val loss: 0.4382 score: 0.8367 time: 0.08s
Test loss: 0.4500 score: 0.7143 time: 0.09s
Epoch 59/1000, LR 0.000268
Train loss: 0.1931;  Loss pred: 0.1931; Loss self: 0.0000; time: 0.14s
Val loss: 0.4339 score: 0.8367 time: 0.08s
Test loss: 0.4445 score: 0.7143 time: 0.09s
Epoch 60/1000, LR 0.000268
Train loss: 0.1855;  Loss pred: 0.1855; Loss self: 0.0000; time: 0.15s
Val loss: 0.4272 score: 0.8367 time: 0.08s
Test loss: 0.4359 score: 0.7347 time: 0.09s
Epoch 61/1000, LR 0.000268
Train loss: 0.1881;  Loss pred: 0.1881; Loss self: 0.0000; time: 0.15s
Val loss: 0.4189 score: 0.8367 time: 0.08s
Test loss: 0.4239 score: 0.7755 time: 0.11s
Epoch 62/1000, LR 0.000268
Train loss: 0.1688;  Loss pred: 0.1688; Loss self: 0.0000; time: 0.27s
Val loss: 0.4113 score: 0.8367 time: 0.08s
Test loss: 0.4120 score: 0.8163 time: 0.09s
Epoch 63/1000, LR 0.000268
Train loss: 0.1757;  Loss pred: 0.1757; Loss self: 0.0000; time: 0.14s
Val loss: 0.4053 score: 0.8367 time: 0.08s
Test loss: 0.4023 score: 0.8163 time: 0.23s
Epoch 64/1000, LR 0.000268
Train loss: 0.1665;  Loss pred: 0.1665; Loss self: 0.0000; time: 0.14s
Val loss: 0.3998 score: 0.8367 time: 0.08s
Test loss: 0.3932 score: 0.8776 time: 0.09s
Epoch 65/1000, LR 0.000268
Train loss: 0.1733;  Loss pred: 0.1733; Loss self: 0.0000; time: 0.14s
Val loss: 0.3973 score: 0.8367 time: 0.08s
Test loss: 0.3895 score: 0.8776 time: 0.09s
Epoch 66/1000, LR 0.000268
Train loss: 0.1590;  Loss pred: 0.1590; Loss self: 0.0000; time: 0.14s
Val loss: 0.3946 score: 0.8367 time: 0.08s
Test loss: 0.3862 score: 0.8776 time: 0.09s
Epoch 67/1000, LR 0.000268
Train loss: 0.1626;  Loss pred: 0.1626; Loss self: 0.0000; time: 0.14s
Val loss: 0.3921 score: 0.8367 time: 0.10s
Test loss: 0.3840 score: 0.8776 time: 0.09s
Epoch 68/1000, LR 0.000268
Train loss: 0.1635;  Loss pred: 0.1635; Loss self: 0.0000; time: 0.14s
Val loss: 0.3892 score: 0.8367 time: 0.08s
Test loss: 0.3806 score: 0.8776 time: 0.09s
Epoch 69/1000, LR 0.000268
Train loss: 0.1644;  Loss pred: 0.1644; Loss self: 0.0000; time: 0.14s
Val loss: 0.3865 score: 0.8367 time: 0.08s
Test loss: 0.3771 score: 0.8776 time: 0.09s
Epoch 70/1000, LR 0.000268
Train loss: 0.1572;  Loss pred: 0.1572; Loss self: 0.0000; time: 0.14s
Val loss: 0.3837 score: 0.8367 time: 0.08s
Test loss: 0.3734 score: 0.8776 time: 0.08s
Epoch 71/1000, LR 0.000268
Train loss: 0.1410;  Loss pred: 0.1410; Loss self: 0.0000; time: 0.14s
Val loss: 0.3808 score: 0.8367 time: 0.19s
Test loss: 0.3693 score: 0.8776 time: 0.09s
Epoch 72/1000, LR 0.000267
Train loss: 0.1478;  Loss pred: 0.1478; Loss self: 0.0000; time: 0.14s
Val loss: 0.3780 score: 0.8367 time: 0.08s
Test loss: 0.3654 score: 0.8776 time: 0.09s
Epoch 73/1000, LR 0.000267
Train loss: 0.1512;  Loss pred: 0.1512; Loss self: 0.0000; time: 0.13s
Val loss: 0.3735 score: 0.8367 time: 0.08s
Test loss: 0.3593 score: 0.8776 time: 0.08s
Epoch 74/1000, LR 0.000267
Train loss: 0.1443;  Loss pred: 0.1443; Loss self: 0.0000; time: 0.13s
Val loss: 0.3702 score: 0.8367 time: 0.08s
Test loss: 0.3552 score: 0.8776 time: 0.08s
Epoch 75/1000, LR 0.000267
Train loss: 0.1593;  Loss pred: 0.1593; Loss self: 0.0000; time: 0.14s
Val loss: 0.3670 score: 0.8367 time: 0.08s
Test loss: 0.3512 score: 0.8776 time: 0.09s
Epoch 76/1000, LR 0.000267
Train loss: 0.1325;  Loss pred: 0.1325; Loss self: 0.0000; time: 0.14s
Val loss: 0.3637 score: 0.8367 time: 0.08s
Test loss: 0.3470 score: 0.8776 time: 0.08s
Epoch 77/1000, LR 0.000267
Train loss: 0.1305;  Loss pred: 0.1305; Loss self: 0.0000; time: 0.13s
Val loss: 0.3601 score: 0.8367 time: 0.08s
Test loss: 0.3421 score: 0.8776 time: 0.08s
Epoch 78/1000, LR 0.000267
Train loss: 0.1303;  Loss pred: 0.1303; Loss self: 0.0000; time: 0.14s
Val loss: 0.3550 score: 0.8367 time: 0.08s
Test loss: 0.3353 score: 0.8980 time: 0.09s
Epoch 79/1000, LR 0.000267
Train loss: 0.1390;  Loss pred: 0.1390; Loss self: 0.0000; time: 0.14s
Val loss: 0.3500 score: 0.8367 time: 0.08s
Test loss: 0.3285 score: 0.8980 time: 0.09s
Epoch 80/1000, LR 0.000267
Train loss: 0.1268;  Loss pred: 0.1268; Loss self: 0.0000; time: 0.25s
Val loss: 0.3465 score: 0.8367 time: 0.08s
Test loss: 0.3241 score: 0.8980 time: 0.08s
Epoch 81/1000, LR 0.000267
Train loss: 0.1400;  Loss pred: 0.1400; Loss self: 0.0000; time: 0.15s
Val loss: 0.3442 score: 0.8367 time: 0.08s
Test loss: 0.3210 score: 0.8980 time: 0.08s
Epoch 82/1000, LR 0.000267
Train loss: 0.1243;  Loss pred: 0.1243; Loss self: 0.0000; time: 0.13s
Val loss: 0.3405 score: 0.8367 time: 0.08s
Test loss: 0.3160 score: 0.8980 time: 0.09s
Epoch 83/1000, LR 0.000266
Train loss: 0.1159;  Loss pred: 0.1159; Loss self: 0.0000; time: 0.14s
Val loss: 0.3358 score: 0.8571 time: 0.08s
Test loss: 0.3095 score: 0.8980 time: 0.08s
Epoch 84/1000, LR 0.000266
Train loss: 0.1219;  Loss pred: 0.1219; Loss self: 0.0000; time: 0.13s
Val loss: 0.3295 score: 0.8571 time: 0.08s
Test loss: 0.3005 score: 0.9184 time: 0.08s
Epoch 85/1000, LR 0.000266
Train loss: 0.1243;  Loss pred: 0.1243; Loss self: 0.0000; time: 0.14s
Val loss: 0.3240 score: 0.8571 time: 0.08s
Test loss: 0.2924 score: 0.9184 time: 0.08s
Epoch 86/1000, LR 0.000266
Train loss: 0.1273;  Loss pred: 0.1273; Loss self: 0.0000; time: 0.14s
Val loss: 0.3186 score: 0.8571 time: 0.08s
Test loss: 0.2844 score: 0.9184 time: 0.08s
Epoch 87/1000, LR 0.000266
Train loss: 0.1089;  Loss pred: 0.1089; Loss self: 0.0000; time: 0.14s
Val loss: 0.3124 score: 0.8776 time: 0.08s
Test loss: 0.2747 score: 0.9184 time: 0.10s
Epoch 88/1000, LR 0.000266
Train loss: 0.1094;  Loss pred: 0.1094; Loss self: 0.0000; time: 0.21s
Val loss: 0.3064 score: 0.8776 time: 3.16s
Test loss: 0.2652 score: 0.9184 time: 3.39s
Epoch 89/1000, LR 0.000266
Train loss: 0.1057;  Loss pred: 0.1057; Loss self: 0.0000; time: 0.54s
Val loss: 0.2998 score: 0.8776 time: 0.08s
Test loss: 0.2547 score: 0.9184 time: 0.08s
Epoch 90/1000, LR 0.000266
Train loss: 0.1093;  Loss pred: 0.1093; Loss self: 0.0000; time: 0.14s
Val loss: 0.2935 score: 0.8980 time: 0.08s
Test loss: 0.2446 score: 0.9184 time: 0.08s
Epoch 91/1000, LR 0.000266
Train loss: 0.1081;  Loss pred: 0.1081; Loss self: 0.0000; time: 0.13s
Val loss: 0.2871 score: 0.8776 time: 0.08s
Test loss: 0.2339 score: 0.9184 time: 0.09s
Epoch 92/1000, LR 0.000266
Train loss: 0.1028;  Loss pred: 0.1028; Loss self: 0.0000; time: 0.14s
Val loss: 0.2823 score: 0.8776 time: 0.08s
Test loss: 0.2253 score: 0.9184 time: 0.09s
Epoch 93/1000, LR 0.000265
Train loss: 0.1121;  Loss pred: 0.1121; Loss self: 0.0000; time: 0.14s
Val loss: 0.2784 score: 0.8776 time: 0.08s
Test loss: 0.2191 score: 0.9184 time: 0.09s
Epoch 94/1000, LR 0.000265
Train loss: 0.1043;  Loss pred: 0.1043; Loss self: 0.0000; time: 0.27s
Val loss: 0.2752 score: 0.8776 time: 0.08s
Test loss: 0.2142 score: 0.9184 time: 0.08s
Epoch 95/1000, LR 0.000265
Train loss: 0.1005;  Loss pred: 0.1005; Loss self: 0.0000; time: 0.15s
Val loss: 0.2720 score: 0.8776 time: 0.08s
Test loss: 0.2094 score: 0.9184 time: 0.09s
Epoch 96/1000, LR 0.000265
Train loss: 0.0970;  Loss pred: 0.0970; Loss self: 0.0000; time: 0.14s
Val loss: 0.2690 score: 0.8776 time: 0.08s
Test loss: 0.2044 score: 0.9184 time: 0.09s
Epoch 97/1000, LR 0.000265
Train loss: 0.0992;  Loss pred: 0.0992; Loss self: 0.0000; time: 0.14s
Val loss: 0.2665 score: 0.8776 time: 0.08s
Test loss: 0.2006 score: 0.9184 time: 0.09s
Epoch 98/1000, LR 0.000265
Train loss: 0.0927;  Loss pred: 0.0927; Loss self: 0.0000; time: 0.14s
Val loss: 0.2642 score: 0.8776 time: 0.08s
Test loss: 0.1965 score: 0.9184 time: 0.09s
Epoch 99/1000, LR 0.000265
Train loss: 0.0981;  Loss pred: 0.0981; Loss self: 0.0000; time: 0.14s
Val loss: 0.2626 score: 0.8776 time: 0.08s
Test loss: 0.1935 score: 0.9184 time: 0.09s
Epoch 100/1000, LR 0.000265
Train loss: 0.0988;  Loss pred: 0.0988; Loss self: 0.0000; time: 0.14s
Val loss: 0.2611 score: 0.8776 time: 0.08s
Test loss: 0.1903 score: 0.9184 time: 0.09s
Epoch 101/1000, LR 0.000265
Train loss: 0.0834;  Loss pred: 0.0834; Loss self: 0.0000; time: 0.15s
Val loss: 0.2598 score: 0.8776 time: 0.16s
Test loss: 0.1873 score: 0.9184 time: 0.09s
Epoch 102/1000, LR 0.000264
Train loss: 0.0814;  Loss pred: 0.0814; Loss self: 0.0000; time: 0.14s
Val loss: 0.2592 score: 0.8776 time: 0.08s
Test loss: 0.1855 score: 0.9184 time: 0.08s
Epoch 103/1000, LR 0.000264
Train loss: 0.0902;  Loss pred: 0.0902; Loss self: 0.0000; time: 0.14s
Val loss: 0.2586 score: 0.8776 time: 0.08s
Test loss: 0.1830 score: 0.9184 time: 0.09s
Epoch 104/1000, LR 0.000264
Train loss: 0.1081;  Loss pred: 0.1081; Loss self: 0.0000; time: 0.14s
Val loss: 0.2576 score: 0.8776 time: 0.08s
Test loss: 0.1796 score: 0.9184 time: 0.09s
Epoch 105/1000, LR 0.000264
Train loss: 0.0966;  Loss pred: 0.0966; Loss self: 0.0000; time: 0.14s
Val loss: 0.2561 score: 0.8776 time: 0.08s
Test loss: 0.1756 score: 0.9184 time: 0.09s
Epoch 106/1000, LR 0.000264
Train loss: 0.0945;  Loss pred: 0.0945; Loss self: 0.0000; time: 0.14s
Val loss: 0.2540 score: 0.8776 time: 0.08s
Test loss: 0.1709 score: 0.9184 time: 0.09s
Epoch 107/1000, LR 0.000264
Train loss: 0.0890;  Loss pred: 0.0890; Loss self: 0.0000; time: 0.14s
Val loss: 0.2508 score: 0.8776 time: 0.08s
Test loss: 0.1652 score: 0.9184 time: 0.22s
Epoch 108/1000, LR 0.000264
Train loss: 0.0803;  Loss pred: 0.0803; Loss self: 0.0000; time: 0.15s
Val loss: 0.2473 score: 0.8776 time: 0.08s
Test loss: 0.1583 score: 0.9184 time: 0.09s
Epoch 109/1000, LR 0.000264
Train loss: 0.0849;  Loss pred: 0.0849; Loss self: 0.0000; time: 0.14s
Val loss: 0.2449 score: 0.8776 time: 0.08s
Test loss: 0.1525 score: 0.9592 time: 0.09s
Epoch 110/1000, LR 0.000263
Train loss: 0.0840;  Loss pred: 0.0840; Loss self: 0.0000; time: 0.14s
Val loss: 0.2432 score: 0.8776 time: 0.07s
Test loss: 0.1477 score: 0.9592 time: 0.08s
Epoch 111/1000, LR 0.000263
Train loss: 0.0832;  Loss pred: 0.0832; Loss self: 0.0000; time: 0.14s
Val loss: 0.2421 score: 0.8980 time: 0.08s
Test loss: 0.1436 score: 0.9592 time: 0.08s
Epoch 112/1000, LR 0.000263
Train loss: 0.0799;  Loss pred: 0.0799; Loss self: 0.0000; time: 0.14s
Val loss: 0.2413 score: 0.8980 time: 0.08s
Test loss: 0.1401 score: 0.9592 time: 0.08s
Epoch 113/1000, LR 0.000263
Train loss: 0.0808;  Loss pred: 0.0808; Loss self: 0.0000; time: 0.14s
Val loss: 0.2404 score: 0.8980 time: 0.08s
Test loss: 0.1366 score: 0.9592 time: 0.08s
Epoch 114/1000, LR 0.000263
Train loss: 0.0866;  Loss pred: 0.0866; Loss self: 0.0000; time: 0.14s
Val loss: 0.2397 score: 0.8980 time: 0.08s
Test loss: 0.1336 score: 0.9592 time: 0.08s
Epoch 115/1000, LR 0.000263
Train loss: 0.0734;  Loss pred: 0.0734; Loss self: 0.0000; time: 0.15s
Val loss: 0.2390 score: 0.8980 time: 0.17s
Test loss: 0.1305 score: 0.9592 time: 0.09s
Epoch 116/1000, LR 0.000263
Train loss: 0.0725;  Loss pred: 0.0725; Loss self: 0.0000; time: 0.14s
Val loss: 0.2384 score: 0.8980 time: 0.08s
Test loss: 0.1285 score: 0.9592 time: 0.09s
Epoch 117/1000, LR 0.000262
Train loss: 0.0808;  Loss pred: 0.0808; Loss self: 0.0000; time: 0.15s
Val loss: 0.2379 score: 0.8980 time: 0.09s
Test loss: 0.1272 score: 0.9592 time: 0.09s
Epoch 118/1000, LR 0.000262
Train loss: 0.0817;  Loss pred: 0.0817; Loss self: 0.0000; time: 0.15s
Val loss: 0.2372 score: 0.8980 time: 0.08s
Test loss: 0.1259 score: 0.9592 time: 0.09s
Epoch 119/1000, LR 0.000262
Train loss: 0.0782;  Loss pred: 0.0782; Loss self: 0.0000; time: 0.15s
Val loss: 0.2369 score: 0.8980 time: 0.08s
Test loss: 0.1262 score: 0.9592 time: 0.09s
Epoch 120/1000, LR 0.000262
Train loss: 0.0676;  Loss pred: 0.0676; Loss self: 0.0000; time: 0.14s
Val loss: 0.2366 score: 0.8980 time: 0.08s
Test loss: 0.1260 score: 0.9592 time: 0.09s
Epoch 121/1000, LR 0.000262
Train loss: 0.0721;  Loss pred: 0.0721; Loss self: 0.0000; time: 0.14s
Val loss: 0.2362 score: 0.8980 time: 0.08s
Test loss: 0.1254 score: 0.9592 time: 0.09s
Epoch 122/1000, LR 0.000262
Train loss: 0.0644;  Loss pred: 0.0644; Loss self: 0.0000; time: 0.14s
Val loss: 0.2359 score: 0.8980 time: 0.08s
Test loss: 0.1252 score: 0.9592 time: 0.09s
Epoch 123/1000, LR 0.000262
Train loss: 0.0705;  Loss pred: 0.0705; Loss self: 0.0000; time: 0.26s
Val loss: 0.2357 score: 0.8980 time: 0.08s
Test loss: 0.1244 score: 0.9592 time: 0.09s
Epoch 124/1000, LR 0.000261
Train loss: 0.0624;  Loss pred: 0.0624; Loss self: 0.0000; time: 0.14s
Val loss: 0.2351 score: 0.8980 time: 0.08s
Test loss: 0.1228 score: 0.9592 time: 0.09s
Epoch 125/1000, LR 0.000261
Train loss: 0.0690;  Loss pred: 0.0690; Loss self: 0.0000; time: 0.14s
Val loss: 0.2346 score: 0.8980 time: 0.08s
Test loss: 0.1217 score: 0.9592 time: 0.09s
Epoch 126/1000, LR 0.000261
Train loss: 0.0683;  Loss pred: 0.0683; Loss self: 0.0000; time: 0.14s
Val loss: 0.2342 score: 0.8980 time: 0.08s
Test loss: 0.1208 score: 0.9592 time: 0.09s
Epoch 127/1000, LR 0.000261
Train loss: 0.0658;  Loss pred: 0.0658; Loss self: 0.0000; time: 0.15s
Val loss: 0.2340 score: 0.8980 time: 0.08s
Test loss: 0.1206 score: 0.9592 time: 0.09s
Epoch 128/1000, LR 0.000261
Train loss: 0.0634;  Loss pred: 0.0634; Loss self: 0.0000; time: 0.14s
Val loss: 0.2335 score: 0.8980 time: 0.08s
Test loss: 0.1191 score: 0.9592 time: 0.09s
Epoch 129/1000, LR 0.000261
Train loss: 0.0652;  Loss pred: 0.0652; Loss self: 0.0000; time: 0.14s
Val loss: 0.2327 score: 0.8980 time: 0.07s
Test loss: 0.1168 score: 0.9592 time: 0.09s
Epoch 130/1000, LR 0.000260
Train loss: 0.0619;  Loss pred: 0.0619; Loss self: 0.0000; time: 0.13s
Val loss: 0.2322 score: 0.8980 time: 0.08s
Test loss: 0.1149 score: 0.9592 time: 0.09s
Epoch 131/1000, LR 0.000260
Train loss: 0.0623;  Loss pred: 0.0623; Loss self: 0.0000; time: 0.17s
Val loss: 0.2320 score: 0.8980 time: 0.08s
Test loss: 0.1142 score: 0.9592 time: 0.08s
Epoch 132/1000, LR 0.000260
Train loss: 0.0608;  Loss pred: 0.0608; Loss self: 0.0000; time: 0.13s
Val loss: 0.2318 score: 0.8980 time: 0.08s
Test loss: 0.1132 score: 0.9592 time: 0.08s
Epoch 133/1000, LR 0.000260
Train loss: 0.0632;  Loss pred: 0.0632; Loss self: 0.0000; time: 0.14s
Val loss: 0.2315 score: 0.8980 time: 0.08s
Test loss: 0.1114 score: 0.9592 time: 0.09s
Epoch 134/1000, LR 0.000260
Train loss: 0.0611;  Loss pred: 0.0611; Loss self: 0.0000; time: 0.14s
Val loss: 0.2315 score: 0.8980 time: 0.08s
Test loss: 0.1111 score: 0.9592 time: 0.09s
Epoch 135/1000, LR 0.000260
Train loss: 0.0638;  Loss pred: 0.0638; Loss self: 0.0000; time: 0.14s
Val loss: 0.2313 score: 0.8980 time: 0.08s
Test loss: 0.1102 score: 0.9592 time: 0.09s
Epoch 136/1000, LR 0.000260
Train loss: 0.0596;  Loss pred: 0.0596; Loss self: 0.0000; time: 0.13s
Val loss: 0.2311 score: 0.8980 time: 0.08s
Test loss: 0.1094 score: 0.9592 time: 0.09s
Epoch 137/1000, LR 0.000259
Train loss: 0.0608;  Loss pred: 0.0608; Loss self: 0.0000; time: 0.14s
Val loss: 0.2311 score: 0.8980 time: 0.08s
Test loss: 0.1084 score: 0.9592 time: 0.09s
Epoch 138/1000, LR 0.000259
Train loss: 0.0545;  Loss pred: 0.0545; Loss self: 0.0000; time: 0.14s
Val loss: 0.2309 score: 0.8980 time: 0.08s
Test loss: 0.1073 score: 0.9592 time: 0.09s
Epoch 139/1000, LR 0.000259
Train loss: 0.0568;  Loss pred: 0.0568; Loss self: 0.0000; time: 0.15s
Val loss: 0.2308 score: 0.8980 time: 0.14s
Test loss: 0.1063 score: 0.9592 time: 0.08s
Epoch 140/1000, LR 0.000259
Train loss: 0.0578;  Loss pred: 0.0578; Loss self: 0.0000; time: 0.13s
Val loss: 0.2311 score: 0.8980 time: 0.08s
Test loss: 0.1066 score: 0.9592 time: 0.08s
     INFO: Early stopping counter 1 of 20
Epoch 141/1000, LR 0.000259
Train loss: 0.0540;  Loss pred: 0.0540; Loss self: 0.0000; time: 0.14s
Val loss: 0.2313 score: 0.8980 time: 0.08s
Test loss: 0.1066 score: 0.9592 time: 0.08s
     INFO: Early stopping counter 2 of 20
Epoch 142/1000, LR 0.000259
Train loss: 0.0531;  Loss pred: 0.0531; Loss self: 0.0000; time: 0.14s
Val loss: 0.2313 score: 0.8980 time: 0.08s
Test loss: 0.1058 score: 0.9592 time: 0.09s
     INFO: Early stopping counter 3 of 20
Epoch 143/1000, LR 0.000258
Train loss: 0.0556;  Loss pred: 0.0556; Loss self: 0.0000; time: 0.15s
Val loss: 0.2310 score: 0.8980 time: 0.09s
Test loss: 0.1045 score: 0.9592 time: 0.10s
     INFO: Early stopping counter 4 of 20
Epoch 144/1000, LR 0.000258
Train loss: 0.0542;  Loss pred: 0.0542; Loss self: 0.0000; time: 0.15s
Val loss: 0.2307 score: 0.8980 time: 0.09s
Test loss: 0.1028 score: 0.9592 time: 0.09s
Epoch 145/1000, LR 0.000258
Train loss: 0.0617;  Loss pred: 0.0617; Loss self: 0.0000; time: 0.15s
Val loss: 0.2306 score: 0.8980 time: 0.08s
Test loss: 0.1008 score: 0.9592 time: 0.09s
Epoch 146/1000, LR 0.000258
Train loss: 0.0526;  Loss pred: 0.0526; Loss self: 0.0000; time: 0.14s
Val loss: 0.2306 score: 0.8980 time: 0.08s
Test loss: 0.0983 score: 0.9592 time: 0.09s
     INFO: Early stopping counter 1 of 20
Epoch 147/1000, LR 0.000258
Train loss: 0.0466;  Loss pred: 0.0466; Loss self: 0.0000; time: 0.29s
Val loss: 0.2309 score: 0.8980 time: 0.08s
Test loss: 0.0962 score: 0.9592 time: 0.09s
     INFO: Early stopping counter 2 of 20
Epoch 148/1000, LR 0.000257
Train loss: 0.0512;  Loss pred: 0.0512; Loss self: 0.0000; time: 0.13s
Val loss: 0.2310 score: 0.8980 time: 0.08s
Test loss: 0.0951 score: 0.9592 time: 0.08s
     INFO: Early stopping counter 3 of 20
Epoch 149/1000, LR 0.000257
Train loss: 0.0514;  Loss pred: 0.0514; Loss self: 0.0000; time: 0.13s
Val loss: 0.2310 score: 0.8980 time: 0.08s
Test loss: 0.0948 score: 0.9592 time: 0.09s
     INFO: Early stopping counter 4 of 20
Epoch 150/1000, LR 0.000257
Train loss: 0.0496;  Loss pred: 0.0496; Loss self: 0.0000; time: 0.14s
Val loss: 0.2310 score: 0.8980 time: 0.08s
Test loss: 0.0946 score: 0.9592 time: 0.08s
     INFO: Early stopping counter 5 of 20
Epoch 151/1000, LR 0.000257
Train loss: 0.0478;  Loss pred: 0.0478; Loss self: 0.0000; time: 0.14s
Val loss: 0.2308 score: 0.8980 time: 0.08s
Test loss: 0.0949 score: 0.9592 time: 0.08s
     INFO: Early stopping counter 6 of 20
Epoch 152/1000, LR 0.000257
Train loss: 0.0508;  Loss pred: 0.0508; Loss self: 0.0000; time: 0.14s
Val loss: 0.2308 score: 0.8980 time: 0.08s
Test loss: 0.0946 score: 0.9592 time: 0.08s
     INFO: Early stopping counter 7 of 20
Epoch 153/1000, LR 0.000257
Train loss: 0.0511;  Loss pred: 0.0511; Loss self: 0.0000; time: 0.14s
Val loss: 0.2306 score: 0.8980 time: 0.08s
Test loss: 0.0951 score: 0.9592 time: 0.08s
Epoch 154/1000, LR 0.000256
Train loss: 0.0511;  Loss pred: 0.0511; Loss self: 0.0000; time: 0.14s
Val loss: 0.2305 score: 0.8980 time: 0.20s
Test loss: 0.0956 score: 0.9592 time: 0.09s
Epoch 155/1000, LR 0.000256
Train loss: 0.0493;  Loss pred: 0.0493; Loss self: 0.0000; time: 0.15s
Val loss: 0.2305 score: 0.8980 time: 0.08s
Test loss: 0.0958 score: 0.9592 time: 0.09s
Epoch 156/1000, LR 0.000256
Train loss: 0.0462;  Loss pred: 0.0462; Loss self: 0.0000; time: 0.14s
Val loss: 0.2305 score: 0.8980 time: 0.08s
Test loss: 0.0961 score: 0.9592 time: 0.08s
Epoch 157/1000, LR 0.000256
Train loss: 0.0404;  Loss pred: 0.0404; Loss self: 0.0000; time: 0.14s
Val loss: 0.2304 score: 0.8980 time: 0.08s
Test loss: 0.0959 score: 0.9592 time: 0.08s
Epoch 158/1000, LR 0.000256
Train loss: 0.0421;  Loss pred: 0.0421; Loss self: 0.0000; time: 0.14s
Val loss: 0.2303 score: 0.8980 time: 0.08s
Test loss: 0.0958 score: 0.9592 time: 0.08s
Epoch 159/1000, LR 0.000255
Train loss: 0.0402;  Loss pred: 0.0402; Loss self: 0.0000; time: 0.14s
Val loss: 0.2304 score: 0.8980 time: 0.08s
Test loss: 0.0946 score: 0.9592 time: 0.09s
     INFO: Early stopping counter 1 of 20
Epoch 160/1000, LR 0.000255
Train loss: 0.0397;  Loss pred: 0.0397; Loss self: 0.0000; time: 0.15s
Val loss: 0.2307 score: 0.8980 time: 0.09s
Test loss: 0.0924 score: 0.9592 time: 0.08s
     INFO: Early stopping counter 2 of 20
Epoch 161/1000, LR 0.000255
Train loss: 0.0419;  Loss pred: 0.0419; Loss self: 0.0000; time: 0.15s
Val loss: 0.2308 score: 0.8980 time: 0.10s
Test loss: 0.0915 score: 0.9592 time: 0.11s
     INFO: Early stopping counter 3 of 20
Epoch 162/1000, LR 0.000255
Train loss: 0.0438;  Loss pred: 0.0438; Loss self: 0.0000; time: 0.15s
Val loss: 0.2305 score: 0.8980 time: 0.08s
Test loss: 0.0921 score: 0.9592 time: 0.08s
     INFO: Early stopping counter 4 of 20
Epoch 163/1000, LR 0.000255
Train loss: 0.0454;  Loss pred: 0.0454; Loss self: 0.0000; time: 0.14s
Val loss: 0.2304 score: 0.8980 time: 0.08s
Test loss: 0.0922 score: 0.9592 time: 0.09s
     INFO: Early stopping counter 5 of 20
Epoch 164/1000, LR 0.000254
Train loss: 0.0433;  Loss pred: 0.0433; Loss self: 0.0000; time: 0.14s
Val loss: 0.2300 score: 0.8980 time: 0.09s
Test loss: 0.0929 score: 0.9592 time: 0.08s
Epoch 165/1000, LR 0.000254
Train loss: 0.0428;  Loss pred: 0.0428; Loss self: 0.0000; time: 0.14s
Val loss: 0.2297 score: 0.8980 time: 0.08s
Test loss: 0.0934 score: 0.9592 time: 0.08s
Epoch 166/1000, LR 0.000254
Train loss: 0.0376;  Loss pred: 0.0376; Loss self: 0.0000; time: 0.14s
Val loss: 0.2294 score: 0.8980 time: 0.09s
Test loss: 0.0928 score: 0.9592 time: 0.08s
Epoch 167/1000, LR 0.000254
Train loss: 0.0391;  Loss pred: 0.0391; Loss self: 0.0000; time: 0.14s
Val loss: 0.2293 score: 0.8980 time: 0.08s
Test loss: 0.0923 score: 0.9592 time: 0.08s
Epoch 168/1000, LR 0.000254
Train loss: 0.0427;  Loss pred: 0.0427; Loss self: 0.0000; time: 0.14s
Val loss: 0.2293 score: 0.8980 time: 0.08s
Test loss: 0.0914 score: 0.9592 time: 0.08s
Epoch 169/1000, LR 0.000253
Train loss: 0.0407;  Loss pred: 0.0407; Loss self: 0.0000; time: 0.14s
Val loss: 0.2292 score: 0.8980 time: 0.08s
Test loss: 0.0905 score: 0.9592 time: 0.08s
Epoch 170/1000, LR 0.000253
Train loss: 0.0337;  Loss pred: 0.0337; Loss self: 0.0000; time: 0.14s
Val loss: 0.2292 score: 0.8980 time: 0.08s
Test loss: 0.0882 score: 0.9592 time: 0.08s
Epoch 171/1000, LR 0.000253
Train loss: 0.0433;  Loss pred: 0.0433; Loss self: 0.0000; time: 0.14s
Val loss: 0.2291 score: 0.8980 time: 0.08s
Test loss: 0.0869 score: 0.9592 time: 0.09s
Epoch 172/1000, LR 0.000253
Train loss: 0.0403;  Loss pred: 0.0403; Loss self: 0.0000; time: 0.15s
Val loss: 0.2288 score: 0.8980 time: 0.08s
Test loss: 0.0870 score: 0.9592 time: 0.08s
Epoch 173/1000, LR 0.000253
Train loss: 0.0392;  Loss pred: 0.0392; Loss self: 0.0000; time: 0.15s
Val loss: 0.2286 score: 0.8980 time: 0.08s
Test loss: 0.0871 score: 0.9592 time: 0.08s
Epoch 174/1000, LR 0.000252
Train loss: 0.0465;  Loss pred: 0.0465; Loss self: 0.0000; time: 0.14s
Val loss: 0.2286 score: 0.8980 time: 0.09s
Test loss: 0.0880 score: 0.9592 time: 0.09s
Epoch 175/1000, LR 0.000252
Train loss: 0.0368;  Loss pred: 0.0368; Loss self: 0.0000; time: 0.15s
Val loss: 0.2286 score: 0.8980 time: 0.08s
Test loss: 0.0885 score: 0.9592 time: 0.09s
     INFO: Early stopping counter 1 of 20
Epoch 176/1000, LR 0.000252
Train loss: 0.0351;  Loss pred: 0.0351; Loss self: 0.0000; time: 0.14s
Val loss: 0.2285 score: 0.8980 time: 0.08s
Test loss: 0.0882 score: 0.9592 time: 0.09s
Epoch 177/1000, LR 0.000252
Train loss: 0.0400;  Loss pred: 0.0400; Loss self: 0.0000; time: 0.13s
Val loss: 0.2285 score: 0.8980 time: 0.08s
Test loss: 0.0877 score: 0.9592 time: 0.08s
Epoch 178/1000, LR 0.000251
Train loss: 0.0348;  Loss pred: 0.0348; Loss self: 0.0000; time: 0.13s
Val loss: 0.2286 score: 0.8980 time: 0.08s
Test loss: 0.0860 score: 0.9592 time: 0.09s
     INFO: Early stopping counter 1 of 20
Epoch 179/1000, LR 0.000251
Train loss: 0.0383;  Loss pred: 0.0383; Loss self: 0.0000; time: 0.13s
Val loss: 0.2288 score: 0.8980 time: 0.08s
Test loss: 0.0848 score: 0.9592 time: 0.09s
     INFO: Early stopping counter 2 of 20
Epoch 180/1000, LR 0.000251
Train loss: 0.0377;  Loss pred: 0.0377; Loss self: 0.0000; time: 0.13s
Val loss: 0.2288 score: 0.8980 time: 0.08s
Test loss: 0.0850 score: 0.9796 time: 0.09s
     INFO: Early stopping counter 3 of 20
Epoch 181/1000, LR 0.000251
Train loss: 0.0309;  Loss pred: 0.0309; Loss self: 0.0000; time: 0.14s
Val loss: 0.2289 score: 0.8980 time: 0.08s
Test loss: 0.0849 score: 0.9796 time: 0.09s
     INFO: Early stopping counter 4 of 20
Epoch 182/1000, LR 0.000251
Train loss: 0.0347;  Loss pred: 0.0347; Loss self: 0.0000; time: 0.14s
Val loss: 0.2292 score: 0.8980 time: 0.08s
Test loss: 0.0837 score: 0.9796 time: 0.09s
     INFO: Early stopping counter 5 of 20
Epoch 183/1000, LR 0.000250
Train loss: 0.0368;  Loss pred: 0.0368; Loss self: 0.0000; time: 0.14s
Val loss: 0.2295 score: 0.8980 time: 0.08s
Test loss: 0.0831 score: 0.9796 time: 0.09s
     INFO: Early stopping counter 6 of 20
Epoch 184/1000, LR 0.000250
Train loss: 0.0343;  Loss pred: 0.0343; Loss self: 0.0000; time: 0.13s
Val loss: 0.2296 score: 0.8980 time: 0.08s
Test loss: 0.0825 score: 0.9796 time: 0.09s
     INFO: Early stopping counter 7 of 20
Epoch 185/1000, LR 0.000250
Train loss: 0.0380;  Loss pred: 0.0380; Loss self: 0.0000; time: 0.14s
Val loss: 0.2297 score: 0.8980 time: 0.08s
Test loss: 0.0832 score: 0.9796 time: 0.09s
     INFO: Early stopping counter 8 of 20
Epoch 186/1000, LR 0.000250
Train loss: 0.0389;  Loss pred: 0.0389; Loss self: 0.0000; time: 0.14s
Val loss: 0.2298 score: 0.8980 time: 0.08s
Test loss: 0.0846 score: 0.9796 time: 0.09s
     INFO: Early stopping counter 9 of 20
Epoch 187/1000, LR 0.000249
Train loss: 0.0305;  Loss pred: 0.0305; Loss self: 0.0000; time: 0.14s
Val loss: 0.2300 score: 0.8980 time: 0.08s
Test loss: 0.0845 score: 0.9796 time: 0.09s
     INFO: Early stopping counter 10 of 20
Epoch 188/1000, LR 0.000249
Train loss: 0.0330;  Loss pred: 0.0330; Loss self: 0.0000; time: 0.15s
Val loss: 0.2302 score: 0.8980 time: 0.08s
Test loss: 0.0844 score: 0.9796 time: 0.09s
     INFO: Early stopping counter 11 of 20
Epoch 189/1000, LR 0.000249
Train loss: 0.0333;  Loss pred: 0.0333; Loss self: 0.0000; time: 0.14s
Val loss: 0.2302 score: 0.8980 time: 0.08s
Test loss: 0.0837 score: 0.9796 time: 0.09s
     INFO: Early stopping counter 12 of 20
Epoch 190/1000, LR 0.000249
Train loss: 0.0344;  Loss pred: 0.0344; Loss self: 0.0000; time: 0.15s
Val loss: 0.2301 score: 0.8980 time: 0.08s
Test loss: 0.0824 score: 0.9796 time: 0.08s
     INFO: Early stopping counter 13 of 20
Epoch 191/1000, LR 0.000249
Train loss: 0.0308;  Loss pred: 0.0308; Loss self: 0.0000; time: 0.15s
Val loss: 0.2302 score: 0.8980 time: 0.08s
Test loss: 0.0811 score: 0.9796 time: 0.08s
     INFO: Early stopping counter 14 of 20
Epoch 192/1000, LR 0.000248
Train loss: 0.0293;  Loss pred: 0.0293; Loss self: 0.0000; time: 0.15s
Val loss: 0.2307 score: 0.8980 time: 0.08s
Test loss: 0.0799 score: 0.9796 time: 0.08s
     INFO: Early stopping counter 15 of 20
Epoch 193/1000, LR 0.000248
Train loss: 0.0270;  Loss pred: 0.0270; Loss self: 0.0000; time: 0.15s
Val loss: 0.2311 score: 0.8980 time: 0.08s
Test loss: 0.0791 score: 0.9796 time: 0.08s
     INFO: Early stopping counter 16 of 20
Epoch 194/1000, LR 0.000248
Train loss: 0.0340;  Loss pred: 0.0340; Loss self: 0.0000; time: 0.14s
Val loss: 0.2312 score: 0.8980 time: 0.08s
Test loss: 0.0799 score: 0.9796 time: 0.09s
     INFO: Early stopping counter 17 of 20
Epoch 195/1000, LR 0.000248
Train loss: 0.0341;  Loss pred: 0.0341; Loss self: 0.0000; time: 0.15s
Val loss: 0.2313 score: 0.8980 time: 0.08s
Test loss: 0.0814 score: 0.9796 time: 0.09s
     INFO: Early stopping counter 18 of 20
Epoch 196/1000, LR 0.000247
Train loss: 0.0286;  Loss pred: 0.0286; Loss self: 0.0000; time: 0.15s
Val loss: 0.2315 score: 0.8980 time: 0.28s
Test loss: 0.0834 score: 0.9796 time: 0.09s
     INFO: Early stopping counter 19 of 20
Epoch 197/1000, LR 0.000247
Train loss: 0.0333;  Loss pred: 0.0333; Loss self: 0.0000; time: 0.15s
Val loss: 0.2319 score: 0.8980 time: 0.08s
Test loss: 0.0844 score: 0.9796 time: 0.09s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 176,   Train_Loss: 0.0400,   Val_Loss: 0.2285,   Val_Precision: 0.8846,   Val_Recall: 0.9200,   Val_accuracy: 0.9020,   Val_Score: 0.8980,   Val_Loss: 0.2285,   Test_Precision: 0.9583,   Test_Recall: 0.9583,   Test_accuracy: 0.9583,   Test_Score: 0.9592,   Test_loss: 0.0877


[0.23724884004332125, 0.09203812386840582, 0.08675278094597161, 0.0865322828758508, 0.0864530960097909, 0.09074261179193854, 0.08487848099321127, 0.21471609501168132, 0.09031237801536918, 0.09099425096064806, 0.09360032598488033, 0.09093293990008533, 0.09124011802487075, 0.08735002810135484, 0.08735446399077773, 0.221397222019732, 0.09102942608296871, 0.09301880304701626, 0.09194997511804104, 0.09293873095884919, 0.09351313603110611, 0.09095243900083005, 0.2078144010156393, 0.08995314105413854, 0.0899076301138848, 0.08996079303324223, 0.09151540393941104, 0.08281276910565794, 0.09025979600846767, 0.09307349193841219, 0.09045870997942984, 0.09102973993867636, 0.09759132517501712, 0.08824983681552112, 0.089608252979815, 0.09178181807510555, 0.09078894392587245, 0.09231420187279582, 0.09019102295860648, 0.09639649698510766, 0.09290689299814403, 0.0906659928150475, 0.0915685030631721, 0.09321372816339135, 0.09336841898038983, 0.09299662313424051, 0.09024207689799368, 0.20793699100613594, 0.08661052794195712, 0.08612522180192173, 0.08854778693057597, 0.09205682598985732, 0.08689293707720935, 0.09591685980558395, 3.166880792938173, 0.1226649780292064, 0.0909090000204742, 0.0954559980891645, 0.0896890670992434, 0.09021354210563004, 0.09139875182881951, 0.08721370995044708, 0.08736699493601918, 0.09109418001025915, 0.0882823090068996, 0.0900058071129024, 0.10015329206362367, 0.08746834681369364, 0.08893413608893752, 0.08806889411062002, 0.09043149184435606, 0.08889440190978348, 0.08851258200593293, 0.09081882913596928, 0.09077718411572278, 0.103669716976583, 0.09068393590860069, 0.08930210187099874, 0.08969759591855109, 0.0891432948410511, 0.09054529410786927, 0.0894688309635967, 0.0926416649017483, 0.0931023359298706, 0.09228604193776846, 0.09270266187377274, 0.21073177503421903, 0.09100939380005002, 0.09088239003904164, 0.09057517908513546, 0.09430147800594568, 0.09089456195943058, 0.09153485600836575, 0.09460940910503268, 0.09295126819051802, 0.08942825999110937, 0.2111837409902364, 0.091310597024858, 0.08802607911638916, 0.08799178898334503, 0.08759210491552949, 0.08796574594452977, 0.08840393694117665, 0.09013702813535929, 0.0973417900968343, 0.08999137883074582, 0.08673636405728757, 0.0855279047973454, 0.08627921296283603, 0.0865545580163598, 0.08720736810937524, 0.08818202302791178, 0.08663987787440419, 0.18052313709631562, 0.08455151505768299, 0.08590411697514355, 0.08767110295593739, 0.08571469387970865, 0.08712288900278509, 0.08486388600431383, 0.09240930690430105, 0.08964980393648148, 0.08566962205804884, 0.10417881095781922, 0.08348598494194448, 0.12219241098500788, 0.09117314498871565, 0.08990018302574754, 0.08971071499399841, 0.08868591790087521, 0.0898641690146178, 0.09003106108866632, 0.09002413391135633, 0.09201176115311682, 0.09167887317016721, 0.0862127491272986, 0.09005230711773038, 0.08924389001913369, 0.09087094711139798, 0.09036758006550372, 0.11068795504979789, 0.08996169501915574, 0.08907402097247541, 0.08819644688628614, 0.08751740702427924, 0.08614207408390939, 0.09043530514463782, 0.0902371488045901, 0.08944934606552124, 0.08820795104838908, 0.08865901711396873, 0.09026679000817239, 0.0898644239641726, 0.08947404217906296, 0.08932587294839323, 0.0869651990942657, 0.08833865216001868, 0.09120633895508945, 0.09710805118083954, 0.09278523898683488, 0.08799143391661346, 0.08813946088775992, 0.08965902891941369, 0.08807822107337415, 0.09459932497702539, 0.09047625097446144, 0.09561633109115064, 0.09816235699690878, 0.09599934192374349, 0.0900929500348866, 0.09149874304421246, 0.08779161609709263, 0.09465950704179704, 0.08728855499066412, 0.08965142094530165, 0.08812940493226051, 0.09016566397622228, 0.08947452693246305, 0.087002047104761, 0.08826597384177148, 0.08979321783408523, 0.09271202119998634, 0.09256926900707185, 0.09127451293170452, 0.09314515301957726, 0.09237598185427487, 0.08938949787989259, 0.09048093995079398, 0.09035971807315946, 0.08949362207204103, 0.09108717599883676, 0.08953577606007457, 0.0888693688903004, 0.08683268004097044, 0.10526102501899004, 0.09269348788075149, 0.08829784905537963, 0.08716912614181638, 0.08861879492178559, 0.08929855492897332, 0.08557918993756175, 0.08716712985187769, 0.08876598393544555, 0.09003228484652936, 0.09108611405827105, 0.08998401206918061, 0.08900089003145695, 0.08912708098068833, 0.09337795106694102, 0.08958687912672758, 0.08880214905366302, 0.08494618209078908, 0.0972064658999443, 0.09853859920985997, 0.08784346119500697, 0.09437941201031208, 0.10119144897907972, 0.08806562214158475, 0.08837099699303508, 0.08758884179405868, 0.08839598600752652, 0.09168236888945103, 0.09450989193283021, 0.09101129812188447, 0.08903977298177779, 0.08608452486805618, 0.0904205550905317, 0.08707480318844318, 0.08984892698936164, 0.09288781392388046, 0.09135720296762884, 0.08818728593178093, 0.09300387604162097, 0.09208324388600886, 0.09204129409044981, 0.09344320883974433, 0.09433963289484382, 0.09171666391193867, 0.09300048602744937, 0.10359338601119816, 0.08699647802859545, 0.09049344481900334, 0.09161261608824134, 0.0907510700635612, 0.09156771912239492, 0.09101663390174508, 0.09277633298188448, 0.09143076022155583, 0.09082322218455374, 0.09137499309144914, 0.0941697540692985, 0.09281922411173582, 0.0927964688744396, 0.09336535609327257, 0.09338485402986407, 0.0937881120480597, 0.09231041208840907, 0.09032174292951822, 0.08982147299684584, 0.10816891095601022, 0.09426566516049206, 0.09292235411703587, 0.09161873301491141, 0.09421670716255903, 0.09376858687028289, 0.09527072799392045, 0.09213254600763321, 0.09338995115831494, 0.09255682188086212, 0.0928704859688878, 0.09249433688819408, 0.09529501898214221, 0.100762260844931, 0.09247642802074552, 0.08555498300120234, 0.09249743004329503, 0.093189975945279, 0.0943718880880624, 0.09429818205535412, 0.09315057098865509, 0.09153015702031553, 0.09196421690285206, 0.09302573092281818, 0.09040737897157669, 0.09513569995760918, 0.09402604401111603, 0.09084508498199284, 0.09108168794773519, 0.09385902411304414, 0.0928618130274117, 0.0928012109361589, 0.09142708987928927, 0.11230322811752558, 0.09979620389640331, 0.2359329469036311, 0.09155411086976528, 0.09050678694620728, 0.0905352421104908, 0.09168887394480407, 0.09251832589507103, 0.09112994209863245, 0.0898196529597044, 0.09069455810822546, 0.09017646382562816, 0.08809295389801264, 0.08823157986626029, 0.09108641417697072, 0.09017508220858872, 0.08907552994787693, 0.09270358900539577, 0.0956009509973228, 0.08896088809706271, 0.08812479698099196, 0.09276455314829946, 0.0873613238800317, 0.08985301596112549, 0.08924788609147072, 0.08976743603125215, 0.1086375720333308, 3.393257556948811, 0.08570843283087015, 0.08765448210760951, 0.09522066009230912, 0.09001902816817164, 0.09208852192386985, 0.08733002189546824, 0.09106876980513334, 0.09074207814410329, 0.09126814198680222, 0.09010348096489906, 0.09047828614711761, 0.09126323089003563, 0.0912478610407561, 0.0862513359170407, 0.09232616983354092, 0.09107426693663001, 0.09137815306894481, 0.09118254599161446, 0.22852110909298062, 0.09104546112939715, 0.090360022848472, 0.0827697878703475, 0.08681606478057802, 0.08947866596281528, 0.08588871895335615, 0.0872553528752178, 0.09012466296553612, 0.09071361902169883, 0.09438771102577448, 0.09374576900154352, 0.0926698399707675, 0.09519678098149598, 0.09178436803631485, 0.09260917198844254, 0.09298767405562103, 0.09118113503791392, 0.09241812094114721, 0.09203084302134812, 0.09037475078366697, 0.0946703259833157, 0.09121700190007687, 0.09989764681085944, 0.08956941706128418, 0.08936481201089919, 0.09034267906099558, 0.09075240511447191, 0.09110609604977071, 0.09074448491446674, 0.09020645800046623, 0.0914537978824228, 0.08488172898069024, 0.08942045993171632, 0.08711871085688472, 0.09292531106621027, 0.10223325691185892, 0.09397545084357262, 0.09282912104390562, 0.09242053097113967, 0.09117216104641557, 0.08870999002829194, 0.09169841604307294, 0.0890132028143853, 0.08956306101754308, 0.08858383796177804, 0.08919921098276973, 0.09309164201840758, 0.09027717588469386, 0.08782091410830617, 0.08715944597497582, 0.08881414192728698, 0.09146114811301231, 0.08969934890046716, 0.11785641289316118, 0.08939919411204755, 0.0920053340960294, 0.0883459581527859, 0.08840079209767282, 0.08785968204028904, 0.0892675630748272, 0.08832239615730941, 0.08930857316590846, 0.08854057779535651, 0.09029386006295681, 0.08862309297546744, 0.09029036504216492, 0.09447925095446408, 0.09360433393158019, 0.0908304750919342, 0.0887375280726701, 0.09054747200571, 0.09079694608226418, 0.09296097909100354, 0.09143848903477192, 0.09160854085348547, 0.09154151403345168, 0.09228846291080117, 0.09055135794915259, 0.09460052708163857, 0.09142064396291971, 0.09238684782758355, 0.09950208617374301, 0.08732799091376364, 0.08810109016485512, 0.08958469610661268, 0.08597160503268242, 0.09204044309444726, 0.09222970996052027, 0.08996426104567945, 0.093216632027179]
[0.004841813062108597, 0.0018783290585388942, 0.0017704649172647267, 0.0017659649566500162, 0.001764348898158998, 0.0018518900365701743, 0.0017322138978206382, 0.0043819611226873735, 0.0018431097554156976, 0.001857025529809144, 0.0019102107343853128, 0.0018557742836752108, 0.0018620432249973624, 0.0017826536347215272, 0.0017827441630770965, 0.004518310653463919, 0.001857743389448341, 0.0018983429193268626, 0.001876530104449817, 0.0018967087950785548, 0.0019084313475735942, 0.0018561722245067358, 0.004241110224808965, 0.0018357783888599702, 0.0018348495941609144, 0.001835934551698821, 0.0018676613048859397, 0.001690056512360366, 0.0018420366532340342, 0.0018994590191512692, 0.0018460961220291804, 0.0018577497946668643, 0.001991659697449329, 0.001801017077867778, 0.0018287398567309184, 0.0018730983280633785, 0.0018528355903239275, 0.0018839633035264453, 0.0018406331216042138, 0.0019672754486756666, 0.0018960590407784497, 0.0018503263839805613, 0.0018687449604729, 0.0019023209829263541, 0.0019054779383753026, 0.0018978902680457247, 0.001841675038734565, 0.004243612061349713, 0.0017675617947338186, 0.001757657587794321, 0.001807097692460734, 0.001878710734486884, 0.0017733252464736603, 0.0019574869348078358, 0.06463022026404434, 0.0025033668985552327, 0.001855285714703555, 0.0019480815936564182, 0.001830389124474355, 0.001841092696033266, 0.0018652806495677453, 0.0017798716316417772, 0.0017829998966534526, 0.0018590648981685542, 0.0018016797756510122, 0.0018368532063857634, 0.00204394473599232, 0.0017850683023202785, 0.0018149823691619902, 0.0017973243696044903, 0.0018455406498848175, 0.0018141714675466018, 0.001806379224610876, 0.0018534454925708016, 0.001852595594198424, 0.0021157085097261836, 0.0018506925695632793, 0.0018224918749183416, 0.0018305631820112467, 0.0018192509151234919, 0.0018478631450585565, 0.001825894509461157, 0.0018906462224846591, 0.0019000476720381755, 0.0018833886109748666, 0.0018918910586484233, 0.004300648470086103, 0.0018573345673479596, 0.0018547426538579926, 0.001848473042553785, 0.0019245199593050139, 0.0018549910603965424, 0.0018680582858850152, 0.0019308042674496466, 0.0018969646569493475, 0.0018250665304308034, 0.0043098722651068655, 0.0018634815719358775, 0.0017964505942120236, 0.0017957507955784701, 0.0017875939778679488, 0.0017952193049904034, 0.0018041619783913602, 0.0018395311864359038, 0.001986567144833353, 0.0018365587516478738, 0.0017701298787201546, 0.0017454674448437837, 0.001760800264547674, 0.0017664195513542817, 0.0017797422063137805, 0.0017996331230186078, 0.0017681607729470243, 0.0036841456550268494, 0.0017255411236261834, 0.0017531452443906848, 0.0017892061827742324, 0.0017492794669328295, 0.0017780181429139813, 0.0017319160409043639, 0.001885904222536756, 0.0018295878354383974, 0.0017483596338377315, 0.0021260981828126373, 0.0017037956110600913, 0.0024937226731634263, 0.0018606764283411357, 0.0018346976127703578, 0.0018308309182448654, 0.0018099166918545961, 0.0018339626329513838, 0.0018373685936462513, 0.0018372272226807413, 0.0018777910439411597, 0.0018709974116360654, 0.0017594438597407875, 0.0018378021860761301, 0.0018213038779415038, 0.0018545091247224078, 0.0018442363278674228, 0.0022589378581591405, 0.001835952959574607, 0.0018178371627035799, 0.0017999274874752273, 0.0017860695311077395, 0.0017580015119165182, 0.0018456184723395475, 0.001841574465399798, 0.0018254968584800254, 0.0018001622662936546, 0.0018093676962034435, 0.0018421793879218856, 0.0018339678360035224, 0.0018260008607972034, 0.0018229769989468005, 0.0017747999815156265, 0.0018028296359187486, 0.0018613538562263154, 0.0019817969628742765, 0.0018935763058537732, 0.001795743549318642, 0.0017987645079134678, 0.0018297761003961976, 0.001797514715783146, 0.0019305984689188854, 0.0018464541015196212, 0.0019513536957377683, 0.002003313408100179, 0.0019591702433417037, 0.0018386316333650326, 0.0018673212866165809, 0.0017916656346345435, 0.0019318266743223887, 0.001781399081442125, 0.001829620835618401, 0.0017985592843318473, 0.001840115591351475, 0.0018260107537237356, 0.0017755519817298164, 0.001801346404934112, 0.0018325146496752087, 0.0018920820653058436, 0.0018891687552463642, 0.0018627451618715208, 0.0019009214901954544, 0.0018852241194749974, 0.0018242754669365834, 0.001846549794914163, 0.0018440758790440705, 0.001826400450449817, 0.0018589219591599337, 0.001827260735919889, 0.0018136605895979672, 0.0017720955110402132, 0.0021481841840610212, 0.0018917038343010508, 0.0018019969194975434, 0.0017789617579962527, 0.0018085468351384814, 0.0018224194883463942, 0.001746514080358403, 0.001778921017385259, 0.0018115506925601133, 0.0018373935682965176, 0.001858900286903491, 0.0018364084095751144, 0.0018163446945195295, 0.0018189200200140476, 0.0019056724707538985, 0.0018283036556475017, 0.0018122887561972045, 0.0017335955528732464, 0.0019838054265294757, 0.002010991820609387, 0.001792723697857285, 0.0019261104491900426, 0.0020651316118179535, 0.0017972575947262195, 0.0018034897345517362, 0.0017875273835522179, 0.0018039997144393167, 0.0018710687528459393, 0.001928773304751637, 0.0018573734310588666, 0.001817138224117914, 0.0017568270381235955, 0.0018453174508271776, 0.0017770367997641467, 0.0018336515712114622, 0.0018956696719159276, 0.0018644327136250784, 0.0017997405292200191, 0.0018980382865636932, 0.0018792498752246707, 0.0018783937569479554, 0.0019070042620355986, 0.0019252986305070166, 0.0018717686512640544, 0.0018979691026010076, 0.0021141507349224115, 0.0017754383271141928, 0.0018468049963061906, 0.0018696452262906395, 0.001852062654358392, 0.001868728961681529, 0.0018574823245254097, 0.0018933945506507037, 0.001865933882072568, 0.0018535351466235457, 0.001864795777376513, 0.0019218317156999695, 0.0018942698798313433, 0.0018938054872334612, 0.0019054154304749503, 0.0019058133475482464, 0.0019140431030216266, 0.0018838859609879401, 0.0018433008761126169, 0.0018330912856499152, 0.002207528795020617, 0.0019237890849080012, 0.0018963745738170585, 0.0018697700615288044, 0.0019227899420930414, 0.0019136446300057732, 0.0019443005713044989, 0.0018802560409721062, 0.001905917370577856, 0.0018889147322624922, 0.0018953160401813838, 0.0018876395283304915, 0.0019447963057580044, 0.0020563726703047144, 0.0018872740412397043, 0.0017460200612490274, 0.0018877026539447965, 0.001901836243781204, 0.0019259568997563757, 0.001924452695007227, 0.001901032060992961, 0.0018679623881697046, 0.0018768207531194298, 0.0018984843045473099, 0.0018450485504403406, 0.0019415448970940647, 0.0019188988573697148, 0.0018539813261631193, 0.0018588099581170448, 0.001915490288021309, 0.0018951390413757489, 0.0018939022640032427, 0.0018658589771283524, 0.0022919026146433793, 0.002036657222375578, 0.004814958100074104, 0.0018684512422401079, 0.0018470772846164753, 0.0018476580022549142, 0.001871201509077634, 0.0018881290998994087, 0.0018597947367067849, 0.0018330541420347837, 0.0018509093491474585, 0.001840335996441391, 0.0017978153856737272, 0.0018006444870665365, 0.0018589064117749126, 0.00184030780017528, 0.0018178679581199373, 0.0018919099797019545, 0.0019510398162718937, 0.001815528328511484, 0.00179846524451004, 0.0018931541458836623, 0.0017828841608169737, 0.001833735019614806, 0.001821385430438178, 0.0018319884904337172, 0.0022170933068026695, 0.06925015422344512, 0.0017491516904259215, 0.0017888669817879492, 0.0019432787773940637, 0.0018371230238402377, 0.0018793575902830582, 0.0017822453448054741, 0.0018585463225537417, 0.0018518791457980263, 0.0018626151425878005, 0.0018388465503040626, 0.0018464956356554615, 0.001862514916123176, 0.0018622012457297165, 0.0017602313452457286, 0.0018842075476232841, 0.0018586585089108165, 0.0018648602667131595, 0.0018608682855431522, 0.00466369610393838, 0.0018580706352938194, 0.0018440820989484082, 0.0016891793442928061, 0.001771756424093429, 0.0018260952237309242, 0.0017528309990480846, 0.0017807214872493427, 0.0018392788360313494, 0.0018512983473816089, 0.0019262798168525404, 0.0019131789592151738, 0.0018912212238932143, 0.0019427914486019587, 0.0018731503680880582, 0.0018899831018049497, 0.0018977076337881843, 0.0018608394905696719, 0.001886084100839739, 0.001878180469823431, 0.0018443826690544278, 0.0019320474690472593, 0.0018615714673485076, 0.002038727485935907, 0.0018279472869649834, 0.00182377167369182, 0.0018437281441019506, 0.0018520899002953451, 0.001859308082648382, 0.0018519282635605456, 0.0018409481224584945, 0.001866404038416792, 0.0017322801832793926, 0.001824907345545231, 0.0017779328746303003, 0.001896434919718577, 0.0020863929982012025, 0.00191786634374638, 0.0018944718580388902, 0.0018861332851252994, 0.001860656347886032, 0.0018104079597610601, 0.0018713962457769988, 0.0018165959758037816, 0.0018278175717865934, 0.0018078334277913887, 0.0018203920608728516, 0.0018998294289470936, 0.001842391344585589, 0.0017922635532307382, 0.0017787642035709352, 0.0018125335087201425, 0.0018665540431227004, 0.0018305989571523909, 0.002405232916186963, 0.0018244733492254602, 0.001877659879510804, 0.0018029787378119571, 0.0018040977979116902, 0.001793054735516103, 0.0018217870015270856, 0.0018024978807614166, 0.001822623942161397, 0.0018069505672521737, 0.0018427318380195268, 0.0018086345505197437, 0.0018426605110645903, 0.0019281479786625322, 0.0019102925292159222, 0.0018536831651415145, 0.0018109699606667369, 0.0018479075919532655, 0.0018529988996380446, 0.001897162838591909, 0.001866091612954529, 0.0018695620582343973, 0.0018681941639479933, 0.001883438018587779, 0.0018479868969214813, 0.0019306230016660933, 0.001865727427814688, 0.0018854458740323174, 0.002030654819872306, 0.001782203896199258, 0.0017979814319358189, 0.0018282591042165853, 0.0017545225516873964, 0.0018783763896825971, 0.001882238978786128, 0.0018360053274628458, 0.0019023802454526328]
[206.53420261634443, 532.3880794230353, 564.8233920076464, 566.2626521745769, 566.7813214514689, 539.9888655657264, 577.2959108907605, 228.20832316894655, 542.5612864679665, 538.4955585951342, 523.5024502790213, 538.8586364175609, 537.044460931575, 560.9614680735287, 560.9329822591902, 221.3216568527354, 538.2874759128875, 526.7752152780658, 532.8984585052482, 527.2290625712966, 523.990554478899, 538.7431116558931, 235.78731676209702, 544.7280598073748, 545.0037993208401, 544.68172575906, 535.4289867139862, 591.6961904447683, 542.8773625347335, 526.4656883446886, 541.6836036147599, 538.2856199853985, 502.09380713014184, 555.2418199076151, 546.8246324480587, 533.8748025224673, 539.7132941650651, 530.7959014531638, 543.2913209387673, 508.31722658521534, 527.4097369823663, 540.4451931603141, 535.1184999299971, 525.6736423427832, 524.802717397319, 526.9008524026573, 542.9839569781599, 235.64830751327972, 565.7510832036243, 568.9390282523099, 553.3735138792053, 532.2799202896562, 563.9123460225621, 510.85909296154193, 15.472637968964634, 399.46202075977345, 539.0005388791473, 513.3255215060408, 546.3319174206613, 543.1557043024254, 536.1123540480287, 561.8382709305764, 560.8525283018353, 537.9048364503808, 555.0375896508378, 544.4093172625504, 489.2500185502851, 560.2026537024796, 550.969539424076, 556.382596770807, 541.8466399330794, 551.2158127767008, 553.5936122247068, 539.5356939323642, 539.7832117984051, 472.65490279160457, 540.3382584693561, 548.6992912079819, 546.2799699168517, 549.67678822474, 541.1656175264599, 547.6767660006337, 528.9196826499962, 526.3025842542692, 530.9578672042553, 528.5716613695532, 232.52307342850068, 538.4059595831861, 539.1583559691858, 540.9870617417473, 519.6100955799501, 539.0861559118401, 535.3152027192973, 517.9188884437656, 527.1579501160427, 547.9252308484074, 232.02543799176948, 536.6299377788588, 556.6532156363752, 556.8701417046385, 559.4111483820801, 557.0350080461872, 554.273957647432, 543.6167689755249, 503.3809215061225, 544.4966021929537, 564.9302980654862, 572.9124326861898, 567.9235857321311, 566.1169223547815, 561.8791285908817, 555.6688122758342, 565.5594306242201, 271.43335080564617, 579.5283498654172, 570.4033953829967, 558.9070782493395, 571.6639444430173, 562.4239572500127, 577.3951949066911, 530.2496214017094, 546.5711897676586, 571.9647037405874, 470.3451647172238, 586.9248597123724, 401.006900551393, 537.4389575577836, 545.0489459622828, 546.2000832707449, 552.5116180763624, 545.2673800614502, 544.2566088579448, 544.2984883170175, 532.5406163942355, 534.4742829577541, 568.3614140137022, 544.128202467257, 549.0571958427015, 539.2262495066909, 542.2298568190266, 442.68592709979305, 544.6762645986864, 550.1042780491676, 555.5779368660612, 559.8886172028192, 568.8277246757492, 541.8237923964738, 543.0136107924934, 547.7960673307518, 555.5054778805553, 552.6792603284994, 542.8352996219733, 545.2658331125058, 547.644867792349, 548.5532733423055, 563.4437741801358, 554.6835818961813, 537.2433600709255, 504.59255853821753, 528.1012425581241, 556.8723888104342, 555.9371421887686, 546.5149532685841, 556.3236791440215, 517.9740977211017, 541.5785852337222, 512.4647582774171, 499.1730180393188, 510.42016557699804, 543.882734231988, 535.5264823290858, 558.1398563822852, 517.6447831950359, 561.3565261246536, 546.5613314695369, 556.000577079389, 543.4441209563084, 547.6419007723401, 563.2051386216012, 555.140309082626, 545.6982295760845, 528.5183017885411, 529.3333362744459, 536.8420868667235, 526.060652771714, 530.4409113323263, 548.1628285443377, 541.5505191109592, 542.277034998353, 547.5250511210255, 537.9461978338835, 547.2672729962508, 551.3710810806499, 564.3036697344851, 465.5094323008916, 528.6239747827552, 554.9399053794349, 562.1256305848631, 552.9301097272548, 548.7210855648653, 572.5691027894774, 562.1385043108022, 552.0132580925922, 544.2492110860707, 537.9524695570276, 544.5411787410447, 550.5562919952955, 549.7767845736707, 524.7491451688927, 546.9550951840358, 551.7884479393553, 576.8358129106922, 504.08169401443155, 497.2670648143024, 557.8104429562842, 519.181026415445, 484.2306389952993, 556.403268476566, 554.4805611264284, 559.4319892391106, 554.3238128010458, 534.4539042079435, 518.4642474760751, 538.3946939684131, 550.3158685055046, 569.2079973154696, 541.9121786073948, 562.7345478341939, 545.3598795431551, 527.5180664726854, 536.3561756303164, 555.63565067537, 526.8597620390744, 532.1272137269381, 532.3697421273462, 524.3826770122513, 519.3999435488382, 534.2540592955619, 526.8789669070924, 473.003170247792, 563.2411921766978, 541.4756847637449, 534.8608313161056, 539.938536985634, 535.1230812520694, 538.363130995339, 528.1519373002999, 535.9246700045233, 539.5095970106795, 536.2517505304787, 520.3369222345152, 527.9078818953903, 528.0373336867008, 524.81993375625, 524.710355967682, 522.4542741076929, 530.8176931663018, 542.5050315762476, 545.5265691503487, 452.9952235529778, 519.8075027272657, 527.3219825908028, 534.8251213212586, 520.0776112399756, 522.563063340022, 514.3237700789571, 531.8424609251581, 524.6817178106779, 529.4045215064983, 527.6164918143672, 529.762163268769, 514.1926673962082, 486.293177516223, 529.8647563356111, 572.7311055547914, 529.7444477869785, 525.8076257984305, 519.2224188020487, 519.6282572153558, 526.0300552099435, 535.3426848063227, 532.8159326552193, 526.7359849142647, 541.9911577726989, 515.0537602796169, 521.1322087974592, 539.3797585165196, 537.9786113331294, 522.0595511517809, 527.6657691955227, 528.0103514350558, 535.9461847106197, 436.3187133741284, 491.000640173308, 207.686127109727, 535.2026198987609, 541.3958627116345, 541.2257023646056, 534.4159862787451, 529.6248016373858, 537.6937466608499, 545.5376232858833, 540.2749737368861, 543.3790361834325, 556.2306385676259, 555.3567109902514, 537.9506970688129, 543.3873615624273, 550.0949590608401, 528.5663751070952, 512.5472026044196, 550.8038537850194, 556.0296497541893, 528.2190053960095, 560.8889360157693, 545.3350616655943, 549.032611817602, 545.8549577258825, 451.0410080314244, 14.440401053452717, 571.7057048131134, 559.0130569688938, 514.5942062625722, 544.3293601043907, 532.096714946827, 561.0899772663715, 538.0549238212942, 539.9920412025981, 536.8795609653763, 543.819167420275, 541.5664032398453, 536.908451762362, 536.998888972466, 568.1071426781119, 530.7270959939565, 538.0224474833761, 536.2332062350779, 537.3835471155439, 214.42220455906718, 538.1926720142523, 542.2752059521928, 592.003450301875, 564.4116687832411, 547.6165684048415, 570.5056565881555, 561.5701316350638, 543.691353594717, 540.1614501597509, 519.1353775558722, 522.6902560177753, 528.7588714457362, 514.7232868044608, 533.8599703667727, 529.1052597480854, 526.9515610282973, 537.3918626876641, 530.1990508030746, 532.4301983046446, 542.1868339896496, 517.5856266580887, 537.180558221775, 490.5020444853304, 547.0617271794207, 548.314251408304, 542.379310745448, 539.9305939957527, 537.8344822637509, 539.9777192650997, 543.198359476067, 535.7896679479254, 577.2738207435315, 547.973025831198, 562.4509306674122, 527.305202832057, 479.29608700861087, 521.4127685491313, 527.8515992500279, 530.1852249182741, 537.4447576717437, 552.3616898657368, 534.3603751779477, 550.4801361004526, 547.1005506433302, 553.148307043802, 549.3322133697477, 526.3630433150049, 542.7728495027917, 557.9536548614169, 562.1880617973213, 551.7139380811311, 535.7466094724072, 546.2692940432794, 415.7601508236919, 548.1033748311686, 532.5778171606537, 554.6377109325044, 554.2936758514627, 557.7074587810424, 548.9115901923578, 554.7856730780604, 548.6595324837712, 553.4185705592921, 542.6725578664492, 552.90329365467, 542.6935640045023, 518.6323928797489, 523.4800349716329, 539.4665166113529, 552.1902746701742, 541.152601111934, 539.6657279156155, 527.1028820816504, 535.8793711187249, 534.8846247684304, 535.2762680120641, 530.9439387603582, 541.1293779549394, 517.9675157381932, 535.9839733777684, 530.3785241319845, 492.45198652860313, 561.1030265014052, 556.1792698400326, 546.9684235093707, 569.9556264114468, 532.3746643605211, 531.2821651610406, 544.6607289434656, 525.6572666743973]
Elapsed: 0.10896284871361281~0.2180305607127316
Time per graph: 0.0022237316064002617~0.004449603279851665
Speed: 528.7637815012034~62.895572731694315
Total Time: 0.0946
best val loss: 0.22847555577754974 test_score: 0.9592

Testing...
Test loss: 0.2446 score: 0.9184 time: 0.09s
test Score 0.9184
Epoch Time List: [0.4508092768955976, 0.3070920908357948, 0.2974808420985937, 0.3007469158619642, 0.29867733223363757, 0.3189322880934924, 0.29995247977785766, 0.4430777980014682, 0.30277947383001447, 0.3086105757392943, 0.3093104548752308, 0.31197931827045977, 0.31028099078685045, 0.296587459044531, 0.30791683681309223, 0.452049741987139, 0.3068047631531954, 0.32514802436344326, 0.31147579103708267, 0.31698304298333824, 0.3158940391149372, 0.5409636630211025, 0.4301249790005386, 0.2957190468441695, 0.3075818223878741, 0.30983677157200873, 0.31063425401225686, 0.29288325691595674, 0.29816899285651743, 0.30855516088195145, 0.30301867821253836, 0.45043403282761574, 0.314528230112046, 0.30104653909802437, 0.30651373974978924, 0.3106672507710755, 0.3116959538310766, 0.31244988553225994, 0.3102447579149157, 0.31436731899157166, 0.4289464079774916, 0.3146040278952569, 0.31443045986816287, 0.3170021940022707, 0.31418308382853866, 0.31814012792892754, 0.30733542679809034, 0.42816265881992877, 0.30104178097099066, 0.2984959650784731, 0.30101055442355573, 0.3033766867592931, 0.2957819397561252, 0.30025161686353385, 5.439451368991286, 1.939063988160342, 0.3023819960653782, 0.32890042290091515, 0.3005390060134232, 0.3059814600273967, 0.3104550950229168, 0.30322231887839735, 0.2949705522041768, 0.30622234917245805, 0.2989605369511992, 0.29768975428305566, 0.3105018970090896, 0.3123672781512141, 0.2914067867677659, 0.2999130510725081, 0.3014565131161362, 0.2972875910345465, 0.3029139139689505, 0.30675380281172693, 0.30161948478780687, 0.3211245040874928, 0.301776607055217, 0.31338142580352724, 0.3072001291438937, 0.3096798041369766, 0.30190809979103506, 0.3071031130384654, 0.4242815771140158, 0.31498940917663276, 0.3126744271721691, 0.3166290116496384, 0.45220593619160354, 0.3145972737111151, 0.3129913138691336, 0.3074410608969629, 0.3135956502519548, 0.30647686985321343, 0.30268257996067405, 0.31290644500404596, 0.3139831554144621, 0.32472115964628756, 0.4205667939968407, 0.2958643559832126, 0.2971589248627424, 0.29533739504404366, 0.30116873723454773, 0.29957090807147324, 0.298228417057544, 0.2992389991413802, 0.36186988395638764, 0.29497691011056304, 0.2906447390560061, 0.29258750984445214, 0.2897995610255748, 0.28071685205213726, 0.293376584071666, 0.2938604343216866, 0.2941603281069547, 0.3980981446802616, 0.27875960478559136, 0.29291771189309657, 0.2925806201528758, 0.2962420720141381, 0.2855291140731424, 0.29301387909799814, 0.2985730180516839, 0.4171671026851982, 0.3249923412222415, 0.32122206105850637, 0.2928521418944001, 0.338657567743212, 0.3078535217791796, 0.29438111395575106, 0.29877174575813115, 0.405639355070889, 0.2927183071151376, 0.2991818729788065, 0.30522995884530246, 0.30744716012850404, 0.3097085910849273, 0.30140858399681747, 0.2998970369808376, 0.3943131840787828, 0.29842788306996226, 0.3024992817081511, 0.32168451440520585, 0.3042407422326505, 0.31036635604687035, 0.30376975564286113, 0.29622170608490705, 0.3879631529562175, 0.2985584808047861, 0.3091707800049335, 0.29914474091492593, 0.29802547092549503, 0.29335695900954306, 0.3047997171524912, 0.30138200521469116, 0.3929511469323188, 0.29627416119910777, 0.29801060468889773, 0.2986612927634269, 0.3012388236820698, 0.31504758913069963, 0.32207043608650565, 0.4254137398675084, 0.29929423006251454, 0.2973416061140597, 0.2974502413999289, 0.31554697011597455, 0.3012160179205239, 0.3121408987790346, 0.3227209278848022, 0.315779997035861, 0.40940228290855885, 0.29797988338395953, 0.3074407409876585, 0.3101475241128355, 0.30617829505354166, 0.3049566268455237, 0.30471025104634464, 0.2998091799672693, 0.41980571509338915, 0.2919372068718076, 0.2942001251503825, 0.29441848094575107, 0.3035931831691414, 0.30677459575235844, 0.30394078977406025, 0.31018955004401505, 0.44257661513984203, 0.30439548776485026, 0.30221236078068614, 0.31251528509892523, 0.3045650019776076, 0.30921331397257745, 0.30318239401094615, 0.3001609621569514, 0.32323995302431285, 0.3321873371023685, 0.2961687420029193, 0.2912444258108735, 0.29599840589798987, 0.29589992109686136, 0.2980103858280927, 0.29065100103616714, 0.4166499082930386, 0.2987802217248827, 0.30731523176655173, 0.3061769320629537, 0.3079274888150394, 0.30924907396547496, 0.29477318888530135, 0.3057236969470978, 0.41958458675071597, 0.29985752515494823, 0.28943479619920254, 0.3145391969010234, 0.3058680861722678, 0.3175489290151745, 0.3351608859375119, 0.3076159539632499, 0.43851399002596736, 0.2939756189007312, 0.2897114201914519, 0.2941924089100212, 0.3035467907320708, 0.3173790068831295, 0.30917054088786244, 0.3011328207794577, 0.444947024108842, 0.3106291990261525, 0.3013878876809031, 0.3058662568219006, 0.30765261268243194, 0.3112448239699006, 0.30625540809705853, 0.3024098740424961, 0.3106351539026946, 0.3105918269138783, 0.3117974968627095, 0.3271943908184767, 0.3179407122079283, 0.315461064921692, 0.3238676351029426, 0.31917756306938827, 0.29707149020396173, 0.30857134493999183, 0.29919993598014116, 0.3006472473498434, 0.30270066391676664, 0.31617723894305527, 0.310060563031584, 0.30303778289817274, 0.31749725597910583, 0.30765246599912643, 0.3136703490745276, 0.3037617711815983, 0.3100222821813077, 0.31562844989821315, 0.3096456341445446, 0.31777848792262375, 0.3134330289904028, 0.3045031821820885, 0.3280748832039535, 0.3415928443428129, 0.3097542349714786, 0.3143105658236891, 0.3112160803284496, 0.32786929910071194, 0.32178427395410836, 0.30771397124044597, 0.3097055701073259, 0.31573589122854173, 0.31116759800352156, 0.31319672078825533, 0.31741949496790767, 0.46038223640061915, 0.3074845401570201, 0.2887935300823301, 0.30939819128252566, 0.3147296919487417, 0.3186817807145417, 0.3162278130184859, 0.3151253161486238, 0.45402214280329645, 0.3105269188527018, 0.3093418348580599, 0.30045825405977666, 0.3076288546435535, 0.3351254379376769, 0.30465051298961043, 0.3061716209631413, 0.4584758756682277, 0.31564452522434294, 0.313891917001456, 0.31723572383634746, 0.33643239503726363, 0.4509134138934314, 0.45527935307472944, 0.313013015082106, 0.31037823227234185, 0.3058922220952809, 0.3278749817982316, 0.31119953421875834, 0.3096919902600348, 0.3016691089142114, 0.41676156502217054, 0.3014163470361382, 0.2969426529016346, 0.2971285437233746, 0.3040206590667367, 0.3023711310233921, 0.298708101734519, 0.30508949188515544, 0.30606251303106546, 0.4136776099912822, 0.3082936571445316, 0.3004203401505947, 0.3010452161543071, 0.29993810993619263, 0.3051175319124013, 0.3046113320160657, 0.3183940851595253, 6.758743972051889, 0.6954023961443454, 0.2966425712220371, 0.2973094000481069, 0.3124816329218447, 0.31036018300801516, 0.43214224209077656, 0.31405796739272773, 0.3052574328612536, 0.3072932150680572, 0.3058795491233468, 0.3056246000342071, 0.3088012041989714, 0.40156126068904996, 0.29815673595294356, 0.3081369358114898, 0.30772144300863147, 0.30986536014825106, 0.3081375788897276, 0.4443745887838304, 0.3134220577776432, 0.3086851311381906, 0.29057709826156497, 0.2971444148570299, 0.30041584209538996, 0.2965014826040715, 0.3022935150656849, 0.39915241696871817, 0.3050804550293833, 0.3251727903261781, 0.31890177680179477, 0.3163657928816974, 0.30976556218229234, 0.3124186401255429, 0.3074349071830511, 0.43195534287951887, 0.30983217619359493, 0.30627137888222933, 0.30808369582518935, 0.31415241002105176, 0.30810327106155455, 0.2991661049891263, 0.3110919438768178, 0.33230334077961743, 0.2990040418226272, 0.3049936799798161, 0.30824847612529993, 0.3084573079831898, 0.30002233013510704, 0.31023985613137484, 0.29958902206271887, 0.3668442270718515, 0.2963636701460928, 0.298623566981405, 0.30853551416657865, 0.3374571625608951, 0.32411689800210297, 0.3169738189317286, 0.3119571730494499, 0.4586844560690224, 0.2973860341589898, 0.3010305720381439, 0.3074203298892826, 0.3036916602868587, 0.3074770199600607, 0.2994341265875846, 0.4296813008841127, 0.3130462619010359, 0.30407083476893604, 0.30129197472706437, 0.2995390151627362, 0.3038095429074019, 0.325178402941674, 0.3540143179707229, 0.3200021330267191, 0.3136418489739299, 0.31320983404293656, 0.29978585080243647, 0.31475522788241506, 0.305359099060297, 0.3034336492419243, 0.3015881578903645, 0.30590824875980616, 0.3075951593928039, 0.3091509479563683, 0.31231799395754933, 0.31862111622467637, 0.31947552994824946, 0.30433231103233993, 0.2954448792152107, 0.30325259268283844, 0.2995774429291487, 0.304251303197816, 0.30705453269183636, 0.3088464450556785, 0.31107923202216625, 0.30156196490861475, 0.30321738007478416, 0.30799810285679996, 0.3056237199343741, 0.3153164938557893, 0.318908937741071, 0.314295063726604, 0.3048625639639795, 0.3162883804179728, 0.31237227423116565, 0.30995161971077323, 0.31403707014396787, 0.5125116331037134, 0.31447055703029037]
Total Epoch List: [232, 197]
Total Time List: [0.08906854595988989, 0.09458365896716714]
========================k_idx:2========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: SGFormer
SGFormer(
  (trans_conv): SGModule(
    (attns): ModuleList(
      (0-1): 2 x SGFormerAttention(heads=1, head_channels=64)
    )
    (fcs): ModuleList(
      (0): Linear(in_features=14887, out_features=64, bias=True)
    )
    (bns): ModuleList(
      (0-2): 3 x LayerNorm((64,), eps=1e-05, elementwise_affine=True)
    )
  )
  (graph_conv): GraphModule(
    (convs): ModuleList(
      (0-2): 3 x GCNConv(64, 64)
    )
    (fcs): ModuleList(
      (0): Linear(in_features=14887, out_features=64, bias=True)
    )
    (bns): ModuleList(
      (0-3): 4 x BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (classifier): Linear(in_features=64, out_features=2, bias=True)
)
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7f342d3768f0>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.7656;  Loss pred: 0.7656; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7023 score: 0.5102 time: 0.09s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7031 score: 0.5000 time: 0.08s
Epoch 2/1000, LR 0.000000
Train loss: 0.7645;  Loss pred: 0.7645; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7007 score: 0.5102 time: 0.09s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7013 score: 0.5000 time: 0.08s
Epoch 3/1000, LR 0.000030
Train loss: 0.7409;  Loss pred: 0.7409; Loss self: 0.0000; time: 0.14s
Val loss: 0.6980 score: 0.4898 time: 0.09s
Test loss: 0.6981 score: 0.4792 time: 0.08s
Epoch 4/1000, LR 0.000060
Train loss: 0.7094;  Loss pred: 0.7094; Loss self: 0.0000; time: 0.13s
Val loss: 0.6943 score: 0.5102 time: 0.09s
Test loss: 0.6939 score: 0.4583 time: 0.08s
Epoch 5/1000, LR 0.000090
Train loss: 0.7052;  Loss pred: 0.7052; Loss self: 0.0000; time: 0.24s
Val loss: 0.6900 score: 0.6122 time: 0.09s
Test loss: 0.6889 score: 0.5833 time: 0.08s
Epoch 6/1000, LR 0.000120
Train loss: 0.7096;  Loss pred: 0.7096; Loss self: 0.0000; time: 0.13s
Val loss: 0.6858 score: 0.5510 time: 0.09s
Test loss: 0.6837 score: 0.6458 time: 0.08s
Epoch 7/1000, LR 0.000150
Train loss: 0.6932;  Loss pred: 0.6932; Loss self: 0.0000; time: 0.13s
Val loss: 0.6820 score: 0.5102 time: 0.09s
Test loss: 0.6786 score: 0.5625 time: 0.08s
Epoch 8/1000, LR 0.000180
Train loss: 0.6548;  Loss pred: 0.6548; Loss self: 0.0000; time: 0.15s
Val loss: 0.6798 score: 0.5102 time: 0.09s
Test loss: 0.6751 score: 0.5417 time: 0.08s
Epoch 9/1000, LR 0.000210
Train loss: 0.6418;  Loss pred: 0.6418; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6786 score: 0.4898 time: 0.09s
Test loss: 0.6726 score: 0.5208 time: 0.08s
Epoch 10/1000, LR 0.000240
Train loss: 0.6306;  Loss pred: 0.6306; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6778 score: 0.4898 time: 0.09s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6705 score: 0.5000 time: 0.08s
Epoch 11/1000, LR 0.000270
Train loss: 0.5898;  Loss pred: 0.5898; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6777 score: 0.4898 time: 0.09s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6690 score: 0.5000 time: 0.08s
Epoch 12/1000, LR 0.000270
Train loss: 0.5995;  Loss pred: 0.5995; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6771 score: 0.4898 time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6673 score: 0.5000 time: 0.08s
Epoch 13/1000, LR 0.000270
Train loss: 0.5882;  Loss pred: 0.5882; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6747 score: 0.4898 time: 0.09s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6641 score: 0.5000 time: 0.08s
Epoch 14/1000, LR 0.000270
Train loss: 0.5624;  Loss pred: 0.5624; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6703 score: 0.4898 time: 0.10s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6592 score: 0.5000 time: 0.08s
Epoch 15/1000, LR 0.000270
Train loss: 0.5368;  Loss pred: 0.5368; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6641 score: 0.4898 time: 0.09s
Test loss: 0.6525 score: 0.5208 time: 0.08s
Epoch 16/1000, LR 0.000270
Train loss: 0.5257;  Loss pred: 0.5257; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6568 score: 0.4898 time: 0.09s
Test loss: 0.6447 score: 0.5208 time: 0.08s
Epoch 17/1000, LR 0.000270
Train loss: 0.5120;  Loss pred: 0.5120; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6476 score: 0.4898 time: 0.09s
Test loss: 0.6353 score: 0.5625 time: 0.08s
Epoch 18/1000, LR 0.000270
Train loss: 0.4918;  Loss pred: 0.4918; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6374 score: 0.4898 time: 0.10s
Test loss: 0.6250 score: 0.5833 time: 0.08s
Epoch 19/1000, LR 0.000270
Train loss: 0.4810;  Loss pred: 0.4810; Loss self: 0.0000; time: 0.14s
Val loss: 0.6260 score: 0.5102 time: 0.09s
Test loss: 0.6134 score: 0.6042 time: 0.08s
Epoch 20/1000, LR 0.000270
Train loss: 0.4644;  Loss pred: 0.4644; Loss self: 0.0000; time: 0.14s
Val loss: 0.6142 score: 0.5510 time: 0.16s
Test loss: 0.6014 score: 0.6250 time: 0.08s
Epoch 21/1000, LR 0.000270
Train loss: 0.4604;  Loss pred: 0.4604; Loss self: 0.0000; time: 0.13s
Val loss: 0.6019 score: 0.6531 time: 0.09s
Test loss: 0.5891 score: 0.6667 time: 0.08s
Epoch 22/1000, LR 0.000270
Train loss: 0.4443;  Loss pred: 0.4443; Loss self: 0.0000; time: 0.14s
Val loss: 0.5888 score: 0.7143 time: 0.09s
Test loss: 0.5763 score: 0.7292 time: 0.08s
Epoch 23/1000, LR 0.000270
Train loss: 0.4441;  Loss pred: 0.4441; Loss self: 0.0000; time: 0.13s
Val loss: 0.5756 score: 0.8163 time: 0.09s
Test loss: 0.5636 score: 0.7917 time: 0.08s
Epoch 24/1000, LR 0.000270
Train loss: 0.4380;  Loss pred: 0.4380; Loss self: 0.0000; time: 0.13s
Val loss: 0.5631 score: 0.8163 time: 0.09s
Test loss: 0.5520 score: 0.8542 time: 0.08s
Epoch 25/1000, LR 0.000270
Train loss: 0.4173;  Loss pred: 0.4173; Loss self: 0.0000; time: 0.13s
Val loss: 0.5523 score: 0.8980 time: 0.09s
Test loss: 0.5420 score: 0.9375 time: 0.08s
Epoch 26/1000, LR 0.000270
Train loss: 0.4282;  Loss pred: 0.4282; Loss self: 0.0000; time: 0.13s
Val loss: 0.5427 score: 0.9388 time: 0.09s
Test loss: 0.5333 score: 0.9375 time: 0.08s
Epoch 27/1000, LR 0.000270
Train loss: 0.3853;  Loss pred: 0.3853; Loss self: 0.0000; time: 0.13s
Val loss: 0.5337 score: 0.9388 time: 0.09s
Test loss: 0.5250 score: 0.9375 time: 0.08s
Epoch 28/1000, LR 0.000270
Train loss: 0.3856;  Loss pred: 0.3856; Loss self: 0.0000; time: 0.13s
Val loss: 0.5258 score: 0.9388 time: 0.09s
Test loss: 0.5178 score: 0.8958 time: 0.08s
Epoch 29/1000, LR 0.000270
Train loss: 0.3709;  Loss pred: 0.3709; Loss self: 0.0000; time: 0.25s
Val loss: 0.5189 score: 0.9388 time: 0.09s
Test loss: 0.5115 score: 0.8958 time: 0.08s
Epoch 30/1000, LR 0.000270
Train loss: 0.3770;  Loss pred: 0.3770; Loss self: 0.0000; time: 0.13s
Val loss: 0.5127 score: 0.9184 time: 0.09s
Test loss: 0.5057 score: 0.8958 time: 0.08s
Epoch 31/1000, LR 0.000270
Train loss: 0.3625;  Loss pred: 0.3625; Loss self: 0.0000; time: 0.13s
Val loss: 0.5070 score: 0.8980 time: 0.09s
Test loss: 0.5001 score: 0.8958 time: 0.08s
Epoch 32/1000, LR 0.000270
Train loss: 0.3481;  Loss pred: 0.3481; Loss self: 0.0000; time: 0.13s
Val loss: 0.5015 score: 0.8980 time: 0.09s
Test loss: 0.4946 score: 0.8958 time: 0.08s
Epoch 33/1000, LR 0.000270
Train loss: 0.3340;  Loss pred: 0.3340; Loss self: 0.0000; time: 0.13s
Val loss: 0.4960 score: 0.8980 time: 0.09s
Test loss: 0.4891 score: 0.8958 time: 0.08s
Epoch 34/1000, LR 0.000270
Train loss: 0.3253;  Loss pred: 0.3253; Loss self: 0.0000; time: 0.13s
Val loss: 0.4902 score: 0.8980 time: 0.09s
Test loss: 0.4834 score: 0.8958 time: 0.08s
Epoch 35/1000, LR 0.000270
Train loss: 0.3209;  Loss pred: 0.3209; Loss self: 0.0000; time: 0.13s
Val loss: 0.4846 score: 0.8980 time: 0.09s
Test loss: 0.4778 score: 0.8958 time: 0.08s
Epoch 36/1000, LR 0.000270
Train loss: 0.3172;  Loss pred: 0.3172; Loss self: 0.0000; time: 0.14s
Val loss: 0.4793 score: 0.8980 time: 0.09s
Test loss: 0.4723 score: 0.8958 time: 0.22s
Epoch 37/1000, LR 0.000270
Train loss: 0.3075;  Loss pred: 0.3075; Loss self: 0.0000; time: 0.14s
Val loss: 0.4740 score: 0.8980 time: 0.10s
Test loss: 0.4670 score: 0.8958 time: 0.08s
Epoch 38/1000, LR 0.000270
Train loss: 0.3016;  Loss pred: 0.3016; Loss self: 0.0000; time: 0.15s
Val loss: 0.4688 score: 0.9184 time: 0.10s
Test loss: 0.4617 score: 0.9167 time: 0.08s
Epoch 39/1000, LR 0.000269
Train loss: 0.3009;  Loss pred: 0.3009; Loss self: 0.0000; time: 0.13s
Val loss: 0.4637 score: 0.9388 time: 0.09s
Test loss: 0.4564 score: 0.9167 time: 0.08s
Epoch 40/1000, LR 0.000269
Train loss: 0.2998;  Loss pred: 0.2998; Loss self: 0.0000; time: 0.13s
Val loss: 0.4589 score: 0.9388 time: 0.09s
Test loss: 0.4513 score: 0.9375 time: 0.08s
Epoch 41/1000, LR 0.000269
Train loss: 0.2891;  Loss pred: 0.2891; Loss self: 0.0000; time: 0.14s
Val loss: 0.4541 score: 0.9388 time: 0.09s
Test loss: 0.4463 score: 0.9375 time: 0.08s
Epoch 42/1000, LR 0.000269
Train loss: 0.2663;  Loss pred: 0.2663; Loss self: 0.0000; time: 0.14s
Val loss: 0.4486 score: 0.9388 time: 0.09s
Test loss: 0.4408 score: 0.9375 time: 0.08s
Epoch 43/1000, LR 0.000269
Train loss: 0.2712;  Loss pred: 0.2712; Loss self: 0.0000; time: 0.13s
Val loss: 0.4432 score: 0.9388 time: 0.09s
Test loss: 0.4354 score: 0.9375 time: 0.08s
Epoch 44/1000, LR 0.000269
Train loss: 0.2603;  Loss pred: 0.2603; Loss self: 0.0000; time: 0.19s
Val loss: 0.4379 score: 0.9184 time: 0.11s
Test loss: 0.4301 score: 0.9375 time: 0.08s
Epoch 45/1000, LR 0.000269
Train loss: 0.2600;  Loss pred: 0.2600; Loss self: 0.0000; time: 0.13s
Val loss: 0.4332 score: 0.8980 time: 0.09s
Test loss: 0.4252 score: 0.9167 time: 0.07s
Epoch 46/1000, LR 0.000269
Train loss: 0.2518;  Loss pred: 0.2518; Loss self: 0.0000; time: 0.13s
Val loss: 0.4293 score: 0.8980 time: 0.09s
Test loss: 0.4208 score: 0.9167 time: 0.08s
Epoch 47/1000, LR 0.000269
Train loss: 0.2405;  Loss pred: 0.2405; Loss self: 0.0000; time: 0.13s
Val loss: 0.4260 score: 0.8980 time: 0.09s
Test loss: 0.4167 score: 0.9167 time: 0.07s
Epoch 48/1000, LR 0.000269
Train loss: 0.2377;  Loss pred: 0.2377; Loss self: 0.0000; time: 0.13s
Val loss: 0.4236 score: 0.8980 time: 0.09s
Test loss: 0.4135 score: 0.9167 time: 0.08s
Epoch 49/1000, LR 0.000269
Train loss: 0.2298;  Loss pred: 0.2298; Loss self: 0.0000; time: 0.14s
Val loss: 0.4220 score: 0.8776 time: 0.10s
Test loss: 0.4106 score: 0.8958 time: 0.08s
Epoch 50/1000, LR 0.000269
Train loss: 0.2355;  Loss pred: 0.2355; Loss self: 0.0000; time: 0.13s
Val loss: 0.4217 score: 0.8571 time: 0.09s
Test loss: 0.4087 score: 0.8750 time: 0.08s
Epoch 51/1000, LR 0.000269
Train loss: 0.2173;  Loss pred: 0.2173; Loss self: 0.0000; time: 0.13s
Val loss: 0.4217 score: 0.8571 time: 0.09s
Test loss: 0.4072 score: 0.8750 time: 0.22s
     INFO: Early stopping counter 1 of 20
Epoch 52/1000, LR 0.000269
Train loss: 0.2181;  Loss pred: 0.2181; Loss self: 0.0000; time: 0.13s
Val loss: 0.4206 score: 0.8571 time: 0.09s
Test loss: 0.4047 score: 0.8750 time: 0.08s
Epoch 53/1000, LR 0.000269
Train loss: 0.2198;  Loss pred: 0.2198; Loss self: 0.0000; time: 0.13s
Val loss: 0.4181 score: 0.8571 time: 0.09s
Test loss: 0.4014 score: 0.8542 time: 0.08s
Epoch 54/1000, LR 0.000269
Train loss: 0.2078;  Loss pred: 0.2078; Loss self: 0.0000; time: 0.13s
Val loss: 0.4146 score: 0.8571 time: 0.09s
Test loss: 0.3974 score: 0.8542 time: 0.08s
Epoch 55/1000, LR 0.000269
Train loss: 0.2076;  Loss pred: 0.2076; Loss self: 0.0000; time: 0.13s
Val loss: 0.4098 score: 0.8571 time: 0.09s
Test loss: 0.3930 score: 0.8542 time: 0.08s
Epoch 56/1000, LR 0.000269
Train loss: 0.2009;  Loss pred: 0.2009; Loss self: 0.0000; time: 0.14s
Val loss: 0.4060 score: 0.8571 time: 0.12s
Test loss: 0.3892 score: 0.8542 time: 0.08s
Epoch 57/1000, LR 0.000269
Train loss: 0.2018;  Loss pred: 0.2018; Loss self: 0.0000; time: 0.12s
Val loss: 0.4031 score: 0.8571 time: 0.09s
Test loss: 0.3860 score: 0.8750 time: 0.08s
Epoch 58/1000, LR 0.000269
Train loss: 0.1961;  Loss pred: 0.1961; Loss self: 0.0000; time: 0.13s
Val loss: 0.4000 score: 0.8571 time: 0.09s
Test loss: 0.3827 score: 0.8750 time: 0.08s
Epoch 59/1000, LR 0.000268
Train loss: 0.1973;  Loss pred: 0.1973; Loss self: 0.0000; time: 0.14s
Val loss: 0.3976 score: 0.8571 time: 0.09s
Test loss: 0.3800 score: 0.8750 time: 0.20s
Epoch 60/1000, LR 0.000268
Train loss: 0.1867;  Loss pred: 0.1867; Loss self: 0.0000; time: 0.13s
Val loss: 0.3946 score: 0.8571 time: 0.09s
Test loss: 0.3769 score: 0.8750 time: 0.07s
Epoch 61/1000, LR 0.000268
Train loss: 0.1852;  Loss pred: 0.1852; Loss self: 0.0000; time: 0.13s
Val loss: 0.3920 score: 0.8776 time: 0.09s
Test loss: 0.3740 score: 0.8542 time: 0.07s
Epoch 62/1000, LR 0.000268
Train loss: 0.1760;  Loss pred: 0.1760; Loss self: 0.0000; time: 0.13s
Val loss: 0.3867 score: 0.8776 time: 0.09s
Test loss: 0.3696 score: 0.8750 time: 0.08s
Epoch 63/1000, LR 0.000268
Train loss: 0.1934;  Loss pred: 0.1934; Loss self: 0.0000; time: 0.13s
Val loss: 0.3810 score: 0.8776 time: 0.09s
Test loss: 0.3651 score: 0.8750 time: 0.08s
Epoch 64/1000, LR 0.000268
Train loss: 0.1737;  Loss pred: 0.1737; Loss self: 0.0000; time: 0.13s
Val loss: 0.3760 score: 0.8776 time: 0.09s
Test loss: 0.3609 score: 0.8958 time: 0.08s
Epoch 65/1000, LR 0.000268
Train loss: 0.1654;  Loss pred: 0.1654; Loss self: 0.0000; time: 0.14s
Val loss: 0.3717 score: 0.8776 time: 0.09s
Test loss: 0.3568 score: 0.8958 time: 0.08s
Epoch 66/1000, LR 0.000268
Train loss: 0.1683;  Loss pred: 0.1683; Loss self: 0.0000; time: 0.14s
Val loss: 0.3669 score: 0.8571 time: 0.09s
Test loss: 0.3526 score: 0.9167 time: 0.08s
Epoch 67/1000, LR 0.000268
Train loss: 0.1604;  Loss pred: 0.1604; Loss self: 0.0000; time: 0.13s
Val loss: 0.3617 score: 0.8776 time: 0.09s
Test loss: 0.3482 score: 0.9167 time: 0.22s
Epoch 68/1000, LR 0.000268
Train loss: 0.1610;  Loss pred: 0.1610; Loss self: 0.0000; time: 0.13s
Val loss: 0.3563 score: 0.8776 time: 0.09s
Test loss: 0.3436 score: 0.9375 time: 0.08s
Epoch 69/1000, LR 0.000268
Train loss: 0.1485;  Loss pred: 0.1485; Loss self: 0.0000; time: 0.13s
Val loss: 0.3511 score: 0.8776 time: 0.09s
Test loss: 0.3393 score: 0.9375 time: 0.08s
Epoch 70/1000, LR 0.000268
Train loss: 0.1530;  Loss pred: 0.1530; Loss self: 0.0000; time: 0.13s
Val loss: 0.3462 score: 0.8776 time: 0.09s
Test loss: 0.3350 score: 0.9375 time: 0.08s
Epoch 71/1000, LR 0.000268
Train loss: 0.1564;  Loss pred: 0.1564; Loss self: 0.0000; time: 0.13s
Val loss: 0.3416 score: 0.8776 time: 0.09s
Test loss: 0.3308 score: 0.9375 time: 0.08s
Epoch 72/1000, LR 0.000267
Train loss: 0.1473;  Loss pred: 0.1473; Loss self: 0.0000; time: 0.14s
Val loss: 0.3393 score: 0.8776 time: 0.09s
Test loss: 0.3272 score: 0.9375 time: 0.08s
Epoch 73/1000, LR 0.000267
Train loss: 0.1461;  Loss pred: 0.1461; Loss self: 0.0000; time: 0.14s
Val loss: 0.3366 score: 0.8980 time: 0.09s
Test loss: 0.3232 score: 0.9167 time: 0.08s
Epoch 74/1000, LR 0.000267
Train loss: 0.1451;  Loss pred: 0.1451; Loss self: 0.0000; time: 0.14s
Val loss: 0.3340 score: 0.8776 time: 0.09s
Test loss: 0.3193 score: 0.9167 time: 0.08s
Epoch 75/1000, LR 0.000267
Train loss: 0.1434;  Loss pred: 0.1434; Loss self: 0.0000; time: 0.14s
Val loss: 0.3310 score: 0.8776 time: 0.09s
Test loss: 0.3154 score: 0.9167 time: 0.08s
Epoch 76/1000, LR 0.000267
Train loss: 0.1310;  Loss pred: 0.1310; Loss self: 0.0000; time: 0.14s
Val loss: 0.3263 score: 0.8776 time: 0.19s
Test loss: 0.3111 score: 0.9375 time: 0.08s
Epoch 77/1000, LR 0.000267
Train loss: 0.1397;  Loss pred: 0.1397; Loss self: 0.0000; time: 0.13s
Val loss: 0.3194 score: 0.8980 time: 0.09s
Test loss: 0.3060 score: 0.9375 time: 0.08s
Epoch 78/1000, LR 0.000267
Train loss: 0.1258;  Loss pred: 0.1258; Loss self: 0.0000; time: 0.13s
Val loss: 0.3125 score: 0.8980 time: 0.09s
Test loss: 0.3011 score: 0.9375 time: 0.08s
Epoch 79/1000, LR 0.000267
Train loss: 0.1287;  Loss pred: 0.1287; Loss self: 0.0000; time: 0.13s
Val loss: 0.3057 score: 0.9184 time: 0.09s
Test loss: 0.2964 score: 0.9375 time: 0.08s
Epoch 80/1000, LR 0.000267
Train loss: 0.1309;  Loss pred: 0.1309; Loss self: 0.0000; time: 0.17s
Val loss: 0.3011 score: 0.8980 time: 0.09s
Test loss: 0.2919 score: 0.9375 time: 0.09s
Epoch 81/1000, LR 0.000267
Train loss: 0.1192;  Loss pred: 0.1192; Loss self: 0.0000; time: 0.15s
Val loss: 0.2964 score: 0.8980 time: 0.09s
Test loss: 0.2876 score: 0.9375 time: 0.08s
Epoch 82/1000, LR 0.000267
Train loss: 0.1322;  Loss pred: 0.1322; Loss self: 0.0000; time: 0.13s
Val loss: 0.2938 score: 0.9184 time: 0.09s
Test loss: 0.2835 score: 0.9375 time: 0.08s
Epoch 83/1000, LR 0.000266
Train loss: 0.1195;  Loss pred: 0.1195; Loss self: 0.0000; time: 0.12s
Val loss: 0.2913 score: 0.9184 time: 0.10s
Test loss: 0.2798 score: 0.9375 time: 0.08s
Epoch 84/1000, LR 0.000266
Train loss: 0.1174;  Loss pred: 0.1174; Loss self: 0.0000; time: 0.14s
Val loss: 0.2887 score: 0.9184 time: 0.16s
Test loss: 0.2757 score: 0.9375 time: 0.07s
Epoch 85/1000, LR 0.000266
Train loss: 0.1163;  Loss pred: 0.1163; Loss self: 0.0000; time: 0.13s
Val loss: 0.2844 score: 0.9184 time: 0.09s
Test loss: 0.2713 score: 0.9375 time: 0.08s
Epoch 86/1000, LR 0.000266
Train loss: 0.1100;  Loss pred: 0.1100; Loss self: 0.0000; time: 0.13s
Val loss: 0.2794 score: 0.9184 time: 0.09s
Test loss: 0.2668 score: 0.9375 time: 0.08s
Epoch 87/1000, LR 0.000266
Train loss: 0.1113;  Loss pred: 0.1113; Loss self: 0.0000; time: 0.14s
Val loss: 0.2742 score: 0.9184 time: 0.09s
Test loss: 0.2622 score: 0.9375 time: 0.08s
Epoch 88/1000, LR 0.000266
Train loss: 0.1082;  Loss pred: 0.1082; Loss self: 0.0000; time: 0.15s
Val loss: 0.2692 score: 0.9184 time: 0.09s
Test loss: 0.2579 score: 0.9375 time: 0.08s
Epoch 89/1000, LR 0.000266
Train loss: 0.1055;  Loss pred: 0.1055; Loss self: 0.0000; time: 0.14s
Val loss: 0.2649 score: 0.9184 time: 0.09s
Test loss: 0.2539 score: 0.9375 time: 0.08s
Epoch 90/1000, LR 0.000266
Train loss: 0.1050;  Loss pred: 0.1050; Loss self: 0.0000; time: 0.14s
Val loss: 0.2609 score: 0.9184 time: 0.09s
Test loss: 0.2499 score: 0.9375 time: 0.08s
Epoch 91/1000, LR 0.000266
Train loss: 0.1107;  Loss pred: 0.1107; Loss self: 0.0000; time: 0.14s
Val loss: 0.2557 score: 0.9184 time: 0.09s
Test loss: 0.2462 score: 0.9375 time: 0.08s
Epoch 92/1000, LR 0.000266
Train loss: 0.1040;  Loss pred: 0.1040; Loss self: 0.0000; time: 0.14s
Val loss: 0.2511 score: 0.9184 time: 0.23s
Test loss: 0.2427 score: 0.9375 time: 0.08s
Epoch 93/1000, LR 0.000265
Train loss: 0.1025;  Loss pred: 0.1025; Loss self: 0.0000; time: 0.13s
Val loss: 0.2475 score: 0.9184 time: 0.09s
Test loss: 0.2394 score: 0.9375 time: 0.08s
Epoch 94/1000, LR 0.000265
Train loss: 0.0965;  Loss pred: 0.0965; Loss self: 0.0000; time: 0.13s
Val loss: 0.2450 score: 0.9184 time: 0.09s
Test loss: 0.2359 score: 0.9375 time: 0.08s
Epoch 95/1000, LR 0.000265
Train loss: 0.0888;  Loss pred: 0.0888; Loss self: 0.0000; time: 0.15s
Val loss: 0.2420 score: 0.9184 time: 0.09s
Test loss: 0.2323 score: 0.9375 time: 0.08s
Epoch 96/1000, LR 0.000265
Train loss: 0.0953;  Loss pred: 0.0953; Loss self: 0.0000; time: 0.14s
Val loss: 0.2388 score: 0.9388 time: 0.09s
Test loss: 0.2293 score: 0.9375 time: 0.08s
Epoch 97/1000, LR 0.000265
Train loss: 0.0966;  Loss pred: 0.0966; Loss self: 0.0000; time: 0.14s
Val loss: 0.2338 score: 0.9388 time: 0.09s
Test loss: 0.2263 score: 0.9375 time: 0.08s
Epoch 98/1000, LR 0.000265
Train loss: 0.0862;  Loss pred: 0.0862; Loss self: 0.0000; time: 0.14s
Val loss: 0.2273 score: 0.9388 time: 0.09s
Test loss: 0.2242 score: 0.9375 time: 0.08s
Epoch 99/1000, LR 0.000265
Train loss: 0.0830;  Loss pred: 0.0830; Loss self: 0.0000; time: 0.14s
Val loss: 0.2215 score: 0.9592 time: 0.09s
Test loss: 0.2222 score: 0.9375 time: 0.20s
Epoch 100/1000, LR 0.000265
Train loss: 0.0888;  Loss pred: 0.0888; Loss self: 0.0000; time: 0.13s
Val loss: 0.2173 score: 0.9592 time: 0.09s
Test loss: 0.2205 score: 0.9375 time: 0.08s
Epoch 101/1000, LR 0.000265
Train loss: 0.0857;  Loss pred: 0.0857; Loss self: 0.0000; time: 0.14s
Val loss: 0.2158 score: 0.9592 time: 0.09s
Test loss: 0.2180 score: 0.9375 time: 0.08s
Epoch 102/1000, LR 0.000264
Train loss: 0.0871;  Loss pred: 0.0871; Loss self: 0.0000; time: 0.14s
Val loss: 0.2159 score: 0.9388 time: 0.09s
Test loss: 0.2161 score: 0.9375 time: 0.09s
     INFO: Early stopping counter 1 of 20
Epoch 103/1000, LR 0.000264
Train loss: 0.0816;  Loss pred: 0.0816; Loss self: 0.0000; time: 0.14s
Val loss: 0.2150 score: 0.9388 time: 0.09s
Test loss: 0.2145 score: 0.9375 time: 0.08s
Epoch 104/1000, LR 0.000264
Train loss: 0.0898;  Loss pred: 0.0898; Loss self: 0.0000; time: 0.14s
Val loss: 0.2149 score: 0.9388 time: 0.11s
Test loss: 0.2126 score: 0.9375 time: 0.08s
Epoch 105/1000, LR 0.000264
Train loss: 0.0795;  Loss pred: 0.0795; Loss self: 0.0000; time: 0.14s
Val loss: 0.2140 score: 0.9388 time: 0.10s
Test loss: 0.2107 score: 0.9375 time: 0.08s
Epoch 106/1000, LR 0.000264
Train loss: 0.0774;  Loss pred: 0.0774; Loss self: 0.0000; time: 0.14s
Val loss: 0.2117 score: 0.9388 time: 0.10s
Test loss: 0.2089 score: 0.9375 time: 0.08s
Epoch 107/1000, LR 0.000264
Train loss: 0.0802;  Loss pred: 0.0802; Loss self: 0.0000; time: 0.15s
Val loss: 0.2099 score: 0.9388 time: 0.10s
Test loss: 0.2073 score: 0.9375 time: 0.12s
Epoch 108/1000, LR 0.000264
Train loss: 0.0741;  Loss pred: 0.0741; Loss self: 0.0000; time: 0.14s
Val loss: 0.2056 score: 0.9388 time: 0.09s
Test loss: 0.2053 score: 0.9375 time: 0.08s
Epoch 109/1000, LR 0.000264
Train loss: 0.0748;  Loss pred: 0.0748; Loss self: 0.0000; time: 0.13s
Val loss: 0.2022 score: 0.9388 time: 0.09s
Test loss: 0.2035 score: 0.9375 time: 0.07s
Epoch 110/1000, LR 0.000263
Train loss: 0.0848;  Loss pred: 0.0848; Loss self: 0.0000; time: 0.14s
Val loss: 0.2000 score: 0.9388 time: 0.09s
Test loss: 0.2024 score: 0.9375 time: 0.08s
Epoch 111/1000, LR 0.000263
Train loss: 0.0760;  Loss pred: 0.0760; Loss self: 0.0000; time: 0.13s
Val loss: 0.1934 score: 0.9592 time: 0.10s
Test loss: 0.2012 score: 0.9375 time: 0.08s
Epoch 112/1000, LR 0.000263
Train loss: 0.0691;  Loss pred: 0.0691; Loss self: 0.0000; time: 0.13s
Val loss: 0.1895 score: 0.9592 time: 0.09s
Test loss: 0.1996 score: 0.9375 time: 0.08s
Epoch 113/1000, LR 0.000263
Train loss: 0.0737;  Loss pred: 0.0737; Loss self: 0.0000; time: 0.13s
Val loss: 0.1901 score: 0.9592 time: 0.09s
Test loss: 0.1985 score: 0.9375 time: 0.08s
     INFO: Early stopping counter 1 of 20
Epoch 114/1000, LR 0.000263
Train loss: 0.0721;  Loss pred: 0.0721; Loss self: 0.0000; time: 0.14s
Val loss: 0.1902 score: 0.9592 time: 0.09s
Test loss: 0.1975 score: 0.9375 time: 0.08s
     INFO: Early stopping counter 2 of 20
Epoch 115/1000, LR 0.000263
Train loss: 0.0692;  Loss pred: 0.0692; Loss self: 0.0000; time: 0.13s
Val loss: 0.1917 score: 0.9592 time: 0.09s
Test loss: 0.1968 score: 0.9375 time: 0.20s
     INFO: Early stopping counter 3 of 20
Epoch 116/1000, LR 0.000263
Train loss: 0.0665;  Loss pred: 0.0665; Loss self: 0.0000; time: 0.13s
Val loss: 0.1894 score: 0.9592 time: 0.09s
Test loss: 0.1962 score: 0.9375 time: 0.08s
Epoch 117/1000, LR 0.000262
Train loss: 0.0669;  Loss pred: 0.0669; Loss self: 0.0000; time: 0.14s
Val loss: 0.1850 score: 0.9592 time: 0.09s
Test loss: 0.1958 score: 0.9375 time: 0.08s
Epoch 118/1000, LR 0.000262
Train loss: 0.0610;  Loss pred: 0.0610; Loss self: 0.0000; time: 0.13s
Val loss: 0.1822 score: 0.9592 time: 0.09s
Test loss: 0.1955 score: 0.9375 time: 0.08s
Epoch 119/1000, LR 0.000262
Train loss: 0.0598;  Loss pred: 0.0598; Loss self: 0.0000; time: 0.12s
Val loss: 0.1816 score: 0.9592 time: 0.09s
Test loss: 0.1968 score: 0.9375 time: 0.08s
Epoch 120/1000, LR 0.000262
Train loss: 0.0697;  Loss pred: 0.0697; Loss self: 0.0000; time: 0.13s
Val loss: 0.1866 score: 0.9592 time: 0.09s
Test loss: 0.1950 score: 0.9375 time: 0.08s
     INFO: Early stopping counter 1 of 20
Epoch 121/1000, LR 0.000262
Train loss: 0.0634;  Loss pred: 0.0634; Loss self: 0.0000; time: 0.13s
Val loss: 0.1871 score: 0.9592 time: 0.09s
Test loss: 0.1937 score: 0.9375 time: 0.08s
     INFO: Early stopping counter 2 of 20
Epoch 122/1000, LR 0.000262
Train loss: 0.0613;  Loss pred: 0.0613; Loss self: 0.0000; time: 0.13s
Val loss: 0.1823 score: 0.9592 time: 0.09s
Test loss: 0.1918 score: 0.9375 time: 0.08s
     INFO: Early stopping counter 3 of 20
Epoch 123/1000, LR 0.000262
Train loss: 0.0582;  Loss pred: 0.0582; Loss self: 0.0000; time: 0.14s
Val loss: 0.1794 score: 0.9592 time: 0.21s
Test loss: 0.1920 score: 0.9375 time: 0.07s
Epoch 124/1000, LR 0.000261
Train loss: 0.0544;  Loss pred: 0.0544; Loss self: 0.0000; time: 0.13s
Val loss: 0.1788 score: 0.9592 time: 0.09s
Test loss: 0.1916 score: 0.9375 time: 0.07s
Epoch 125/1000, LR 0.000261
Train loss: 0.0580;  Loss pred: 0.0580; Loss self: 0.0000; time: 0.13s
Val loss: 0.1801 score: 0.9592 time: 0.09s
Test loss: 0.1915 score: 0.9375 time: 0.08s
     INFO: Early stopping counter 1 of 20
Epoch 126/1000, LR 0.000261
Train loss: 0.0552;  Loss pred: 0.0552; Loss self: 0.0000; time: 0.13s
Val loss: 0.1838 score: 0.9592 time: 0.09s
Test loss: 0.1914 score: 0.9375 time: 0.08s
     INFO: Early stopping counter 2 of 20
Epoch 127/1000, LR 0.000261
Train loss: 0.0582;  Loss pred: 0.0582; Loss self: 0.0000; time: 0.13s
Val loss: 0.1872 score: 0.9592 time: 0.09s
Test loss: 0.1922 score: 0.9375 time: 0.08s
     INFO: Early stopping counter 3 of 20
Epoch 128/1000, LR 0.000261
Train loss: 0.0547;  Loss pred: 0.0547; Loss self: 0.0000; time: 0.13s
Val loss: 0.1871 score: 0.9592 time: 0.09s
Test loss: 0.1920 score: 0.9375 time: 0.08s
     INFO: Early stopping counter 4 of 20
Epoch 129/1000, LR 0.000261
Train loss: 0.0502;  Loss pred: 0.0502; Loss self: 0.0000; time: 0.14s
Val loss: 0.1823 score: 0.9592 time: 0.09s
Test loss: 0.1904 score: 0.9375 time: 0.08s
     INFO: Early stopping counter 5 of 20
Epoch 130/1000, LR 0.000260
Train loss: 0.0513;  Loss pred: 0.0513; Loss self: 0.0000; time: 0.14s
Val loss: 0.1755 score: 0.9592 time: 0.09s
Test loss: 0.1892 score: 0.9375 time: 0.08s
Epoch 131/1000, LR 0.000260
Train loss: 0.0489;  Loss pred: 0.0489; Loss self: 0.0000; time: 0.13s
Val loss: 0.1725 score: 0.9592 time: 0.22s
Test loss: 0.1880 score: 0.9375 time: 0.08s
Epoch 132/1000, LR 0.000260
Train loss: 0.0545;  Loss pred: 0.0545; Loss self: 0.0000; time: 0.13s
Val loss: 0.1722 score: 0.9592 time: 0.09s
Test loss: 0.1869 score: 0.9375 time: 0.08s
Epoch 133/1000, LR 0.000260
Train loss: 0.0495;  Loss pred: 0.0495; Loss self: 0.0000; time: 0.13s
Val loss: 0.1722 score: 0.9592 time: 0.09s
Test loss: 0.1866 score: 0.9375 time: 0.08s
Epoch 134/1000, LR 0.000260
Train loss: 0.0538;  Loss pred: 0.0538; Loss self: 0.0000; time: 0.13s
Val loss: 0.1733 score: 0.9592 time: 0.09s
Test loss: 0.1847 score: 0.9375 time: 0.08s
     INFO: Early stopping counter 1 of 20
Epoch 135/1000, LR 0.000260
Train loss: 0.0470;  Loss pred: 0.0470; Loss self: 0.0000; time: 0.13s
Val loss: 0.1745 score: 0.9592 time: 0.09s
Test loss: 0.1837 score: 0.9375 time: 0.08s
     INFO: Early stopping counter 2 of 20
Epoch 136/1000, LR 0.000260
Train loss: 0.0414;  Loss pred: 0.0414; Loss self: 0.0000; time: 0.12s
Val loss: 0.1714 score: 0.9592 time: 0.09s
Test loss: 0.1838 score: 0.9375 time: 0.08s
Epoch 137/1000, LR 0.000259
Train loss: 0.0457;  Loss pred: 0.0457; Loss self: 0.0000; time: 0.14s
Val loss: 0.1687 score: 0.9592 time: 0.09s
Test loss: 0.1850 score: 0.9375 time: 0.08s
Epoch 138/1000, LR 0.000259
Train loss: 0.0412;  Loss pred: 0.0412; Loss self: 0.0000; time: 0.15s
Val loss: 0.1681 score: 0.9592 time: 0.10s
Test loss: 0.1859 score: 0.9375 time: 0.08s
Epoch 139/1000, LR 0.000259
Train loss: 0.0457;  Loss pred: 0.0457; Loss self: 0.0000; time: 0.13s
Val loss: 0.1692 score: 0.9592 time: 0.09s
Test loss: 0.1850 score: 0.9375 time: 0.08s
     INFO: Early stopping counter 1 of 20
Epoch 140/1000, LR 0.000259
Train loss: 0.0378;  Loss pred: 0.0378; Loss self: 0.0000; time: 0.14s
Val loss: 0.1678 score: 0.9592 time: 0.09s
Test loss: 0.1858 score: 0.9375 time: 0.08s
Epoch 141/1000, LR 0.000259
Train loss: 0.0392;  Loss pred: 0.0392; Loss self: 0.0000; time: 0.13s
Val loss: 0.1671 score: 0.9592 time: 0.09s
Test loss: 0.1877 score: 0.9375 time: 0.08s
Epoch 142/1000, LR 0.000259
Train loss: 0.0380;  Loss pred: 0.0380; Loss self: 0.0000; time: 0.12s
Val loss: 0.1671 score: 0.9592 time: 0.09s
Test loss: 0.1892 score: 0.9375 time: 0.08s
     INFO: Early stopping counter 1 of 20
Epoch 143/1000, LR 0.000258
Train loss: 0.0414;  Loss pred: 0.0414; Loss self: 0.0000; time: 0.13s
Val loss: 0.1671 score: 0.9592 time: 0.09s
Test loss: 0.1903 score: 0.9375 time: 0.08s
Epoch 144/1000, LR 0.000258
Train loss: 0.0382;  Loss pred: 0.0382; Loss self: 0.0000; time: 0.13s
Val loss: 0.1663 score: 0.9592 time: 0.09s
Test loss: 0.1883 score: 0.9375 time: 0.08s
Epoch 145/1000, LR 0.000258
Train loss: 0.0351;  Loss pred: 0.0351; Loss self: 0.0000; time: 0.13s
Val loss: 0.1661 score: 0.9592 time: 0.09s
Test loss: 0.1874 score: 0.9375 time: 0.08s
Epoch 146/1000, LR 0.000258
Train loss: 0.0339;  Loss pred: 0.0339; Loss self: 0.0000; time: 0.13s
Val loss: 0.1681 score: 0.9592 time: 0.09s
Test loss: 0.1901 score: 0.9167 time: 0.08s
     INFO: Early stopping counter 1 of 20
Epoch 147/1000, LR 0.000258
Train loss: 0.0344;  Loss pred: 0.0344; Loss self: 0.0000; time: 0.13s
Val loss: 0.1671 score: 0.9592 time: 0.09s
Test loss: 0.1914 score: 0.9167 time: 0.08s
     INFO: Early stopping counter 2 of 20
Epoch 148/1000, LR 0.000257
Train loss: 0.0378;  Loss pred: 0.0378; Loss self: 0.0000; time: 0.13s
Val loss: 0.1673 score: 0.9592 time: 0.09s
Test loss: 0.1859 score: 0.9375 time: 0.08s
     INFO: Early stopping counter 3 of 20
Epoch 149/1000, LR 0.000257
Train loss: 0.0295;  Loss pred: 0.0295; Loss self: 0.0000; time: 0.13s
Val loss: 0.1684 score: 0.9592 time: 0.09s
Test loss: 0.1878 score: 0.9375 time: 0.08s
     INFO: Early stopping counter 4 of 20
Epoch 150/1000, LR 0.000257
Train loss: 0.0332;  Loss pred: 0.0332; Loss self: 0.0000; time: 0.15s
Val loss: 0.1697 score: 0.9592 time: 0.09s
Test loss: 0.1929 score: 0.9167 time: 0.08s
     INFO: Early stopping counter 5 of 20
Epoch 151/1000, LR 0.000257
Train loss: 0.0331;  Loss pred: 0.0331; Loss self: 0.0000; time: 0.13s
Val loss: 0.1694 score: 0.9592 time: 0.09s
Test loss: 0.1898 score: 0.9167 time: 0.08s
     INFO: Early stopping counter 6 of 20
Epoch 152/1000, LR 0.000257
Train loss: 0.0356;  Loss pred: 0.0356; Loss self: 0.0000; time: 0.14s
Val loss: 0.1693 score: 0.9592 time: 0.09s
Test loss: 0.1889 score: 0.9167 time: 0.08s
     INFO: Early stopping counter 7 of 20
Epoch 153/1000, LR 0.000257
Train loss: 0.0294;  Loss pred: 0.0294; Loss self: 0.0000; time: 0.13s
Val loss: 0.1701 score: 0.9592 time: 0.09s
Test loss: 0.1912 score: 0.9167 time: 0.08s
     INFO: Early stopping counter 8 of 20
Epoch 154/1000, LR 0.000256
Train loss: 0.0305;  Loss pred: 0.0305; Loss self: 0.0000; time: 0.14s
Val loss: 0.1716 score: 0.9592 time: 0.09s
Test loss: 0.1953 score: 0.9167 time: 0.08s
     INFO: Early stopping counter 9 of 20
Epoch 155/1000, LR 0.000256
Train loss: 0.0300;  Loss pred: 0.0300; Loss self: 0.0000; time: 0.13s
Val loss: 0.1701 score: 0.9592 time: 0.09s
Test loss: 0.1928 score: 0.9167 time: 0.08s
     INFO: Early stopping counter 10 of 20
Epoch 156/1000, LR 0.000256
Train loss: 0.0291;  Loss pred: 0.0291; Loss self: 0.0000; time: 0.13s
Val loss: 0.1690 score: 0.9592 time: 0.09s
Test loss: 0.1876 score: 0.9167 time: 0.08s
     INFO: Early stopping counter 11 of 20
Epoch 157/1000, LR 0.000256
Train loss: 0.0296;  Loss pred: 0.0296; Loss self: 0.0000; time: 0.13s
Val loss: 0.1704 score: 0.9592 time: 0.10s
Test loss: 0.1903 score: 0.9167 time: 0.08s
     INFO: Early stopping counter 12 of 20
Epoch 158/1000, LR 0.000256
Train loss: 0.0257;  Loss pred: 0.0257; Loss self: 0.0000; time: 0.13s
Val loss: 0.1719 score: 0.9592 time: 0.09s
Test loss: 0.1960 score: 0.9167 time: 0.08s
     INFO: Early stopping counter 13 of 20
Epoch 159/1000, LR 0.000255
Train loss: 0.0267;  Loss pred: 0.0267; Loss self: 0.0000; time: 0.13s
Val loss: 0.1726 score: 0.9592 time: 0.09s
Test loss: 0.1982 score: 0.9167 time: 0.08s
     INFO: Early stopping counter 14 of 20
Epoch 160/1000, LR 0.000255
Train loss: 0.0249;  Loss pred: 0.0249; Loss self: 0.0000; time: 0.13s
Val loss: 0.1735 score: 0.9592 time: 0.09s
Test loss: 0.1980 score: 0.9167 time: 0.08s
     INFO: Early stopping counter 15 of 20
Epoch 161/1000, LR 0.000255
Train loss: 0.0260;  Loss pred: 0.0260; Loss self: 0.0000; time: 0.13s
Val loss: 0.1728 score: 0.9592 time: 0.09s
Test loss: 0.1910 score: 0.9167 time: 0.08s
     INFO: Early stopping counter 16 of 20
Epoch 162/1000, LR 0.000255
Train loss: 0.0246;  Loss pred: 0.0246; Loss self: 0.0000; time: 0.13s
Val loss: 0.1741 score: 0.9592 time: 0.09s
Test loss: 0.1946 score: 0.9167 time: 0.08s
     INFO: Early stopping counter 17 of 20
Epoch 163/1000, LR 0.000255
Train loss: 0.0253;  Loss pred: 0.0253; Loss self: 0.0000; time: 0.14s
Val loss: 0.1790 score: 0.9592 time: 0.09s
Test loss: 0.1994 score: 0.9167 time: 0.08s
     INFO: Early stopping counter 18 of 20
Epoch 164/1000, LR 0.000254
Train loss: 0.0259;  Loss pred: 0.0259; Loss self: 0.0000; time: 0.13s
Val loss: 0.1749 score: 0.9592 time: 0.09s
Test loss: 0.1944 score: 0.9167 time: 0.08s
     INFO: Early stopping counter 19 of 20
Epoch 165/1000, LR 0.000254
Train loss: 0.0292;  Loss pred: 0.0292; Loss self: 0.0000; time: 0.13s
Val loss: 0.1710 score: 0.9592 time: 0.09s
Test loss: 0.1823 score: 0.9167 time: 0.07s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 144,   Train_Loss: 0.0351,   Val_Loss: 0.1661,   Val_Precision: 1.0000,   Val_Recall: 0.9200,   Val_accuracy: 0.9583,   Val_Score: 0.9592,   Val_Loss: 0.1661,   Test_Precision: 0.9565,   Test_Recall: 0.9167,   Test_accuracy: 0.9362,   Test_Score: 0.9375,   Test_loss: 0.1874


[0.23724884004332125, 0.09203812386840582, 0.08675278094597161, 0.0865322828758508, 0.0864530960097909, 0.09074261179193854, 0.08487848099321127, 0.21471609501168132, 0.09031237801536918, 0.09099425096064806, 0.09360032598488033, 0.09093293990008533, 0.09124011802487075, 0.08735002810135484, 0.08735446399077773, 0.221397222019732, 0.09102942608296871, 0.09301880304701626, 0.09194997511804104, 0.09293873095884919, 0.09351313603110611, 0.09095243900083005, 0.2078144010156393, 0.08995314105413854, 0.0899076301138848, 0.08996079303324223, 0.09151540393941104, 0.08281276910565794, 0.09025979600846767, 0.09307349193841219, 0.09045870997942984, 0.09102973993867636, 0.09759132517501712, 0.08824983681552112, 0.089608252979815, 0.09178181807510555, 0.09078894392587245, 0.09231420187279582, 0.09019102295860648, 0.09639649698510766, 0.09290689299814403, 0.0906659928150475, 0.0915685030631721, 0.09321372816339135, 0.09336841898038983, 0.09299662313424051, 0.09024207689799368, 0.20793699100613594, 0.08661052794195712, 0.08612522180192173, 0.08854778693057597, 0.09205682598985732, 0.08689293707720935, 0.09591685980558395, 3.166880792938173, 0.1226649780292064, 0.0909090000204742, 0.0954559980891645, 0.0896890670992434, 0.09021354210563004, 0.09139875182881951, 0.08721370995044708, 0.08736699493601918, 0.09109418001025915, 0.0882823090068996, 0.0900058071129024, 0.10015329206362367, 0.08746834681369364, 0.08893413608893752, 0.08806889411062002, 0.09043149184435606, 0.08889440190978348, 0.08851258200593293, 0.09081882913596928, 0.09077718411572278, 0.103669716976583, 0.09068393590860069, 0.08930210187099874, 0.08969759591855109, 0.0891432948410511, 0.09054529410786927, 0.0894688309635967, 0.0926416649017483, 0.0931023359298706, 0.09228604193776846, 0.09270266187377274, 0.21073177503421903, 0.09100939380005002, 0.09088239003904164, 0.09057517908513546, 0.09430147800594568, 0.09089456195943058, 0.09153485600836575, 0.09460940910503268, 0.09295126819051802, 0.08942825999110937, 0.2111837409902364, 0.091310597024858, 0.08802607911638916, 0.08799178898334503, 0.08759210491552949, 0.08796574594452977, 0.08840393694117665, 0.09013702813535929, 0.0973417900968343, 0.08999137883074582, 0.08673636405728757, 0.0855279047973454, 0.08627921296283603, 0.0865545580163598, 0.08720736810937524, 0.08818202302791178, 0.08663987787440419, 0.18052313709631562, 0.08455151505768299, 0.08590411697514355, 0.08767110295593739, 0.08571469387970865, 0.08712288900278509, 0.08486388600431383, 0.09240930690430105, 0.08964980393648148, 0.08566962205804884, 0.10417881095781922, 0.08348598494194448, 0.12219241098500788, 0.09117314498871565, 0.08990018302574754, 0.08971071499399841, 0.08868591790087521, 0.0898641690146178, 0.09003106108866632, 0.09002413391135633, 0.09201176115311682, 0.09167887317016721, 0.0862127491272986, 0.09005230711773038, 0.08924389001913369, 0.09087094711139798, 0.09036758006550372, 0.11068795504979789, 0.08996169501915574, 0.08907402097247541, 0.08819644688628614, 0.08751740702427924, 0.08614207408390939, 0.09043530514463782, 0.0902371488045901, 0.08944934606552124, 0.08820795104838908, 0.08865901711396873, 0.09026679000817239, 0.0898644239641726, 0.08947404217906296, 0.08932587294839323, 0.0869651990942657, 0.08833865216001868, 0.09120633895508945, 0.09710805118083954, 0.09278523898683488, 0.08799143391661346, 0.08813946088775992, 0.08965902891941369, 0.08807822107337415, 0.09459932497702539, 0.09047625097446144, 0.09561633109115064, 0.09816235699690878, 0.09599934192374349, 0.0900929500348866, 0.09149874304421246, 0.08779161609709263, 0.09465950704179704, 0.08728855499066412, 0.08965142094530165, 0.08812940493226051, 0.09016566397622228, 0.08947452693246305, 0.087002047104761, 0.08826597384177148, 0.08979321783408523, 0.09271202119998634, 0.09256926900707185, 0.09127451293170452, 0.09314515301957726, 0.09237598185427487, 0.08938949787989259, 0.09048093995079398, 0.09035971807315946, 0.08949362207204103, 0.09108717599883676, 0.08953577606007457, 0.0888693688903004, 0.08683268004097044, 0.10526102501899004, 0.09269348788075149, 0.08829784905537963, 0.08716912614181638, 0.08861879492178559, 0.08929855492897332, 0.08557918993756175, 0.08716712985187769, 0.08876598393544555, 0.09003228484652936, 0.09108611405827105, 0.08998401206918061, 0.08900089003145695, 0.08912708098068833, 0.09337795106694102, 0.08958687912672758, 0.08880214905366302, 0.08494618209078908, 0.0972064658999443, 0.09853859920985997, 0.08784346119500697, 0.09437941201031208, 0.10119144897907972, 0.08806562214158475, 0.08837099699303508, 0.08758884179405868, 0.08839598600752652, 0.09168236888945103, 0.09450989193283021, 0.09101129812188447, 0.08903977298177779, 0.08608452486805618, 0.0904205550905317, 0.08707480318844318, 0.08984892698936164, 0.09288781392388046, 0.09135720296762884, 0.08818728593178093, 0.09300387604162097, 0.09208324388600886, 0.09204129409044981, 0.09344320883974433, 0.09433963289484382, 0.09171666391193867, 0.09300048602744937, 0.10359338601119816, 0.08699647802859545, 0.09049344481900334, 0.09161261608824134, 0.0907510700635612, 0.09156771912239492, 0.09101663390174508, 0.09277633298188448, 0.09143076022155583, 0.09082322218455374, 0.09137499309144914, 0.0941697540692985, 0.09281922411173582, 0.0927964688744396, 0.09336535609327257, 0.09338485402986407, 0.0937881120480597, 0.09231041208840907, 0.09032174292951822, 0.08982147299684584, 0.10816891095601022, 0.09426566516049206, 0.09292235411703587, 0.09161873301491141, 0.09421670716255903, 0.09376858687028289, 0.09527072799392045, 0.09213254600763321, 0.09338995115831494, 0.09255682188086212, 0.0928704859688878, 0.09249433688819408, 0.09529501898214221, 0.100762260844931, 0.09247642802074552, 0.08555498300120234, 0.09249743004329503, 0.093189975945279, 0.0943718880880624, 0.09429818205535412, 0.09315057098865509, 0.09153015702031553, 0.09196421690285206, 0.09302573092281818, 0.09040737897157669, 0.09513569995760918, 0.09402604401111603, 0.09084508498199284, 0.09108168794773519, 0.09385902411304414, 0.0928618130274117, 0.0928012109361589, 0.09142708987928927, 0.11230322811752558, 0.09979620389640331, 0.2359329469036311, 0.09155411086976528, 0.09050678694620728, 0.0905352421104908, 0.09168887394480407, 0.09251832589507103, 0.09112994209863245, 0.0898196529597044, 0.09069455810822546, 0.09017646382562816, 0.08809295389801264, 0.08823157986626029, 0.09108641417697072, 0.09017508220858872, 0.08907552994787693, 0.09270358900539577, 0.0956009509973228, 0.08896088809706271, 0.08812479698099196, 0.09276455314829946, 0.0873613238800317, 0.08985301596112549, 0.08924788609147072, 0.08976743603125215, 0.1086375720333308, 3.393257556948811, 0.08570843283087015, 0.08765448210760951, 0.09522066009230912, 0.09001902816817164, 0.09208852192386985, 0.08733002189546824, 0.09106876980513334, 0.09074207814410329, 0.09126814198680222, 0.09010348096489906, 0.09047828614711761, 0.09126323089003563, 0.0912478610407561, 0.0862513359170407, 0.09232616983354092, 0.09107426693663001, 0.09137815306894481, 0.09118254599161446, 0.22852110909298062, 0.09104546112939715, 0.090360022848472, 0.0827697878703475, 0.08681606478057802, 0.08947866596281528, 0.08588871895335615, 0.0872553528752178, 0.09012466296553612, 0.09071361902169883, 0.09438771102577448, 0.09374576900154352, 0.0926698399707675, 0.09519678098149598, 0.09178436803631485, 0.09260917198844254, 0.09298767405562103, 0.09118113503791392, 0.09241812094114721, 0.09203084302134812, 0.09037475078366697, 0.0946703259833157, 0.09121700190007687, 0.09989764681085944, 0.08956941706128418, 0.08936481201089919, 0.09034267906099558, 0.09075240511447191, 0.09110609604977071, 0.09074448491446674, 0.09020645800046623, 0.0914537978824228, 0.08488172898069024, 0.08942045993171632, 0.08711871085688472, 0.09292531106621027, 0.10223325691185892, 0.09397545084357262, 0.09282912104390562, 0.09242053097113967, 0.09117216104641557, 0.08870999002829194, 0.09169841604307294, 0.0890132028143853, 0.08956306101754308, 0.08858383796177804, 0.08919921098276973, 0.09309164201840758, 0.09027717588469386, 0.08782091410830617, 0.08715944597497582, 0.08881414192728698, 0.09146114811301231, 0.08969934890046716, 0.11785641289316118, 0.08939919411204755, 0.0920053340960294, 0.0883459581527859, 0.08840079209767282, 0.08785968204028904, 0.0892675630748272, 0.08832239615730941, 0.08930857316590846, 0.08854057779535651, 0.09029386006295681, 0.08862309297546744, 0.09029036504216492, 0.09447925095446408, 0.09360433393158019, 0.0908304750919342, 0.0887375280726701, 0.09054747200571, 0.09079694608226418, 0.09296097909100354, 0.09143848903477192, 0.09160854085348547, 0.09154151403345168, 0.09228846291080117, 0.09055135794915259, 0.09460052708163857, 0.09142064396291971, 0.09238684782758355, 0.09950208617374301, 0.08732799091376364, 0.08810109016485512, 0.08958469610661268, 0.08597160503268242, 0.09204044309444726, 0.09222970996052027, 0.08996426104567945, 0.093216632027179, 0.08327262592501938, 0.08492826716974378, 0.08202912891283631, 0.08482992090284824, 0.08349207788705826, 0.08151329983957112, 0.08844371512532234, 0.08402942400425673, 0.08569036703556776, 0.08324727206490934, 0.08425228903070092, 0.08408848103135824, 0.08457090589217842, 0.08594411588273942, 0.0846601550001651, 0.08551185391843319, 0.08777401316910982, 0.08381317579187453, 0.08347575506195426, 0.08260159380733967, 0.08208346297033131, 0.08385232998989522, 0.08175838296301663, 0.08366227592341602, 0.0849434060510248, 0.08351835887879133, 0.0830868249759078, 0.08480879105627537, 0.08490466885268688, 0.08170598722063005, 0.08087564003653824, 0.08430219697766006, 0.08474003919400275, 0.08325716597028077, 0.08302711113356054, 0.22725879005156457, 0.08561009587720037, 0.08487777109257877, 0.08356883702799678, 0.08507282217033207, 0.08485073200426996, 0.08271498489193618, 0.08281745901331306, 0.0833323560655117, 0.07937901001423597, 0.08040744718164206, 0.07866396196186543, 0.08904380607418716, 0.08558441698551178, 0.08435460994951427, 0.22660386399365962, 0.08247300703078508, 0.08306374214589596, 0.08333548996597528, 0.08404442691244185, 0.08424830506555736, 0.08158356789499521, 0.08180409995839, 0.20507413591258228, 0.0797865220811218, 0.079748400952667, 0.0815889451187104, 0.08208436891436577, 0.08522123005241156, 0.08393759815953672, 0.08294154983013868, 0.22396405693143606, 0.08185383188538253, 0.08488130290061235, 0.08216823800466955, 0.08367859292775393, 0.08379133394919336, 0.0849952211137861, 0.08228699583560228, 0.0845728050917387, 0.0858178399503231, 0.08235049992799759, 0.08148151100613177, 0.0860113340895623, 0.09203709196299314, 0.08558045886456966, 0.08095645904541016, 0.08467315812595189, 0.07952062180265784, 0.08023600396700203, 0.0804440809879452, 0.0841066800057888, 0.08716576406732202, 0.08417148399166763, 0.08421641308814287, 0.08317788993008435, 0.08391762408427894, 0.0843936640303582, 0.08577333693392575, 0.08194837113842368, 0.08417354081757367, 0.08405826590023935, 0.08322749799117446, 0.20701918797567487, 0.08481543301604688, 0.08493121503852308, 0.09196362900547683, 0.08603930100798607, 0.08771969680674374, 0.08765825582668185, 0.08881613006815314, 0.1261187249328941, 0.08551442809402943, 0.07952723396010697, 0.08219773485325277, 0.08957644295878708, 0.08739221189171076, 0.08594018104486167, 0.08505218499340117, 0.20730510400608182, 0.08654375304467976, 0.08713053003884852, 0.08497344306670129, 0.08260070299729705, 0.08224628609605134, 0.08483524387702346, 0.08510736795142293, 0.07932145916856825, 0.07845188095234334, 0.08298764796927571, 0.0864989971742034, 0.0847822839859873, 0.08699730015359819, 0.08627136796712875, 0.08598976489156485, 0.08512334106490016, 0.08146419515833259, 0.0829611208755523, 0.08269378007389605, 0.08360112621448934, 0.08257302502170205, 0.0845519951544702, 0.08371966099366546, 0.08505746396258473, 0.08393096318468451, 0.08683435106649995, 0.08191290800459683, 0.08115351898595691, 0.08118286798708141, 0.08268710295669734, 0.08418301306664944, 0.08486689813435078, 0.08454300300218165, 0.08535646111704409, 0.08588879788294435, 0.08463692385703325, 0.08357522101141512, 0.08486918406561017, 0.08319991314783692, 0.08164475788362324, 0.0821699679363519, 0.08485261397436261, 0.08074521482922137, 0.08242002804763615, 0.08176598395220935, 0.08249250310473144, 0.08306982717476785, 0.08225633692927659, 0.08360700006596744, 0.08034071885049343]
[0.004841813062108597, 0.0018783290585388942, 0.0017704649172647267, 0.0017659649566500162, 0.001764348898158998, 0.0018518900365701743, 0.0017322138978206382, 0.0043819611226873735, 0.0018431097554156976, 0.001857025529809144, 0.0019102107343853128, 0.0018557742836752108, 0.0018620432249973624, 0.0017826536347215272, 0.0017827441630770965, 0.004518310653463919, 0.001857743389448341, 0.0018983429193268626, 0.001876530104449817, 0.0018967087950785548, 0.0019084313475735942, 0.0018561722245067358, 0.004241110224808965, 0.0018357783888599702, 0.0018348495941609144, 0.001835934551698821, 0.0018676613048859397, 0.001690056512360366, 0.0018420366532340342, 0.0018994590191512692, 0.0018460961220291804, 0.0018577497946668643, 0.001991659697449329, 0.001801017077867778, 0.0018287398567309184, 0.0018730983280633785, 0.0018528355903239275, 0.0018839633035264453, 0.0018406331216042138, 0.0019672754486756666, 0.0018960590407784497, 0.0018503263839805613, 0.0018687449604729, 0.0019023209829263541, 0.0019054779383753026, 0.0018978902680457247, 0.001841675038734565, 0.004243612061349713, 0.0017675617947338186, 0.001757657587794321, 0.001807097692460734, 0.001878710734486884, 0.0017733252464736603, 0.0019574869348078358, 0.06463022026404434, 0.0025033668985552327, 0.001855285714703555, 0.0019480815936564182, 0.001830389124474355, 0.001841092696033266, 0.0018652806495677453, 0.0017798716316417772, 0.0017829998966534526, 0.0018590648981685542, 0.0018016797756510122, 0.0018368532063857634, 0.00204394473599232, 0.0017850683023202785, 0.0018149823691619902, 0.0017973243696044903, 0.0018455406498848175, 0.0018141714675466018, 0.001806379224610876, 0.0018534454925708016, 0.001852595594198424, 0.0021157085097261836, 0.0018506925695632793, 0.0018224918749183416, 0.0018305631820112467, 0.0018192509151234919, 0.0018478631450585565, 0.001825894509461157, 0.0018906462224846591, 0.0019000476720381755, 0.0018833886109748666, 0.0018918910586484233, 0.004300648470086103, 0.0018573345673479596, 0.0018547426538579926, 0.001848473042553785, 0.0019245199593050139, 0.0018549910603965424, 0.0018680582858850152, 0.0019308042674496466, 0.0018969646569493475, 0.0018250665304308034, 0.0043098722651068655, 0.0018634815719358775, 0.0017964505942120236, 0.0017957507955784701, 0.0017875939778679488, 0.0017952193049904034, 0.0018041619783913602, 0.0018395311864359038, 0.001986567144833353, 0.0018365587516478738, 0.0017701298787201546, 0.0017454674448437837, 0.001760800264547674, 0.0017664195513542817, 0.0017797422063137805, 0.0017996331230186078, 0.0017681607729470243, 0.0036841456550268494, 0.0017255411236261834, 0.0017531452443906848, 0.0017892061827742324, 0.0017492794669328295, 0.0017780181429139813, 0.0017319160409043639, 0.001885904222536756, 0.0018295878354383974, 0.0017483596338377315, 0.0021260981828126373, 0.0017037956110600913, 0.0024937226731634263, 0.0018606764283411357, 0.0018346976127703578, 0.0018308309182448654, 0.0018099166918545961, 0.0018339626329513838, 0.0018373685936462513, 0.0018372272226807413, 0.0018777910439411597, 0.0018709974116360654, 0.0017594438597407875, 0.0018378021860761301, 0.0018213038779415038, 0.0018545091247224078, 0.0018442363278674228, 0.0022589378581591405, 0.001835952959574607, 0.0018178371627035799, 0.0017999274874752273, 0.0017860695311077395, 0.0017580015119165182, 0.0018456184723395475, 0.001841574465399798, 0.0018254968584800254, 0.0018001622662936546, 0.0018093676962034435, 0.0018421793879218856, 0.0018339678360035224, 0.0018260008607972034, 0.0018229769989468005, 0.0017747999815156265, 0.0018028296359187486, 0.0018613538562263154, 0.0019817969628742765, 0.0018935763058537732, 0.001795743549318642, 0.0017987645079134678, 0.0018297761003961976, 0.001797514715783146, 0.0019305984689188854, 0.0018464541015196212, 0.0019513536957377683, 0.002003313408100179, 0.0019591702433417037, 0.0018386316333650326, 0.0018673212866165809, 0.0017916656346345435, 0.0019318266743223887, 0.001781399081442125, 0.001829620835618401, 0.0017985592843318473, 0.001840115591351475, 0.0018260107537237356, 0.0017755519817298164, 0.001801346404934112, 0.0018325146496752087, 0.0018920820653058436, 0.0018891687552463642, 0.0018627451618715208, 0.0019009214901954544, 0.0018852241194749974, 0.0018242754669365834, 0.001846549794914163, 0.0018440758790440705, 0.001826400450449817, 0.0018589219591599337, 0.001827260735919889, 0.0018136605895979672, 0.0017720955110402132, 0.0021481841840610212, 0.0018917038343010508, 0.0018019969194975434, 0.0017789617579962527, 0.0018085468351384814, 0.0018224194883463942, 0.001746514080358403, 0.001778921017385259, 0.0018115506925601133, 0.0018373935682965176, 0.001858900286903491, 0.0018364084095751144, 0.0018163446945195295, 0.0018189200200140476, 0.0019056724707538985, 0.0018283036556475017, 0.0018122887561972045, 0.0017335955528732464, 0.0019838054265294757, 0.002010991820609387, 0.001792723697857285, 0.0019261104491900426, 0.0020651316118179535, 0.0017972575947262195, 0.0018034897345517362, 0.0017875273835522179, 0.0018039997144393167, 0.0018710687528459393, 0.001928773304751637, 0.0018573734310588666, 0.001817138224117914, 0.0017568270381235955, 0.0018453174508271776, 0.0017770367997641467, 0.0018336515712114622, 0.0018956696719159276, 0.0018644327136250784, 0.0017997405292200191, 0.0018980382865636932, 0.0018792498752246707, 0.0018783937569479554, 0.0019070042620355986, 0.0019252986305070166, 0.0018717686512640544, 0.0018979691026010076, 0.0021141507349224115, 0.0017754383271141928, 0.0018468049963061906, 0.0018696452262906395, 0.001852062654358392, 0.001868728961681529, 0.0018574823245254097, 0.0018933945506507037, 0.001865933882072568, 0.0018535351466235457, 0.001864795777376513, 0.0019218317156999695, 0.0018942698798313433, 0.0018938054872334612, 0.0019054154304749503, 0.0019058133475482464, 0.0019140431030216266, 0.0018838859609879401, 0.0018433008761126169, 0.0018330912856499152, 0.002207528795020617, 0.0019237890849080012, 0.0018963745738170585, 0.0018697700615288044, 0.0019227899420930414, 0.0019136446300057732, 0.0019443005713044989, 0.0018802560409721062, 0.001905917370577856, 0.0018889147322624922, 0.0018953160401813838, 0.0018876395283304915, 0.0019447963057580044, 0.0020563726703047144, 0.0018872740412397043, 0.0017460200612490274, 0.0018877026539447965, 0.001901836243781204, 0.0019259568997563757, 0.001924452695007227, 0.001901032060992961, 0.0018679623881697046, 0.0018768207531194298, 0.0018984843045473099, 0.0018450485504403406, 0.0019415448970940647, 0.0019188988573697148, 0.0018539813261631193, 0.0018588099581170448, 0.001915490288021309, 0.0018951390413757489, 0.0018939022640032427, 0.0018658589771283524, 0.0022919026146433793, 0.002036657222375578, 0.004814958100074104, 0.0018684512422401079, 0.0018470772846164753, 0.0018476580022549142, 0.001871201509077634, 0.0018881290998994087, 0.0018597947367067849, 0.0018330541420347837, 0.0018509093491474585, 0.001840335996441391, 0.0017978153856737272, 0.0018006444870665365, 0.0018589064117749126, 0.00184030780017528, 0.0018178679581199373, 0.0018919099797019545, 0.0019510398162718937, 0.001815528328511484, 0.00179846524451004, 0.0018931541458836623, 0.0017828841608169737, 0.001833735019614806, 0.001821385430438178, 0.0018319884904337172, 0.0022170933068026695, 0.06925015422344512, 0.0017491516904259215, 0.0017888669817879492, 0.0019432787773940637, 0.0018371230238402377, 0.0018793575902830582, 0.0017822453448054741, 0.0018585463225537417, 0.0018518791457980263, 0.0018626151425878005, 0.0018388465503040626, 0.0018464956356554615, 0.001862514916123176, 0.0018622012457297165, 0.0017602313452457286, 0.0018842075476232841, 0.0018586585089108165, 0.0018648602667131595, 0.0018608682855431522, 0.00466369610393838, 0.0018580706352938194, 0.0018440820989484082, 0.0016891793442928061, 0.001771756424093429, 0.0018260952237309242, 0.0017528309990480846, 0.0017807214872493427, 0.0018392788360313494, 0.0018512983473816089, 0.0019262798168525404, 0.0019131789592151738, 0.0018912212238932143, 0.0019427914486019587, 0.0018731503680880582, 0.0018899831018049497, 0.0018977076337881843, 0.0018608394905696719, 0.001886084100839739, 0.001878180469823431, 0.0018443826690544278, 0.0019320474690472593, 0.0018615714673485076, 0.002038727485935907, 0.0018279472869649834, 0.00182377167369182, 0.0018437281441019506, 0.0018520899002953451, 0.001859308082648382, 0.0018519282635605456, 0.0018409481224584945, 0.001866404038416792, 0.0017322801832793926, 0.001824907345545231, 0.0017779328746303003, 0.001896434919718577, 0.0020863929982012025, 0.00191786634374638, 0.0018944718580388902, 0.0018861332851252994, 0.001860656347886032, 0.0018104079597610601, 0.0018713962457769988, 0.0018165959758037816, 0.0018278175717865934, 0.0018078334277913887, 0.0018203920608728516, 0.0018998294289470936, 0.001842391344585589, 0.0017922635532307382, 0.0017787642035709352, 0.0018125335087201425, 0.0018665540431227004, 0.0018305989571523909, 0.002405232916186963, 0.0018244733492254602, 0.001877659879510804, 0.0018029787378119571, 0.0018040977979116902, 0.001793054735516103, 0.0018217870015270856, 0.0018024978807614166, 0.001822623942161397, 0.0018069505672521737, 0.0018427318380195268, 0.0018086345505197437, 0.0018426605110645903, 0.0019281479786625322, 0.0019102925292159222, 0.0018536831651415145, 0.0018109699606667369, 0.0018479075919532655, 0.0018529988996380446, 0.001897162838591909, 0.001866091612954529, 0.0018695620582343973, 0.0018681941639479933, 0.001883438018587779, 0.0018479868969214813, 0.0019306230016660933, 0.001865727427814688, 0.0018854458740323174, 0.002030654819872306, 0.001782203896199258, 0.0017979814319358189, 0.0018282591042165853, 0.0017545225516873964, 0.0018783763896825971, 0.001882238978786128, 0.0018360053274628458, 0.0019023802454526328, 0.0017348463734379038, 0.001769338899369662, 0.0017089401856840898, 0.0017672900188093383, 0.0017394182893137138, 0.0016981937466577317, 0.0018425773984442155, 0.0017506130000886817, 0.0017852159799076617, 0.0017343181680189446, 0.0017552560214729358, 0.0017518433548199634, 0.001761893872753717, 0.001790502414223738, 0.0017637532291701064, 0.0017814969566340249, 0.0018286252743564546, 0.001746107828997386, 0.0017390782304573804, 0.0017208665376529098, 0.0017100721452152357, 0.0017469235414561506, 0.0017032996450628464, 0.0017429640817378338, 0.0017696542927296832, 0.0017399658099748194, 0.0017309755203314126, 0.0017668498136724036, 0.0017688472677643101, 0.0017022080670964594, 0.00168490916742788, 0.0017562957703679178, 0.0017654174832083906, 0.001734524291047516, 0.0017297314819491778, 0.004734558126074262, 0.0017835436641083409, 0.0017682868977620576, 0.0017410174380832661, 0.0017723504618819181, 0.0017677235834222909, 0.0017232288519153371, 0.001725363729444022, 0.0017360907513648272, 0.0016537293752965827, 0.0016751551496175428, 0.0016388325408721964, 0.0018550792932122324, 0.001783008687198162, 0.0017573877072815474, 0.004720913833201242, 0.0017181876464746892, 0.001730494628039499, 0.0017361560409578185, 0.0017509255606758718, 0.0017551730221991118, 0.0016996576644790669, 0.0017042520824664582, 0.004272377831512131, 0.0016622192100233708, 0.0016614250198472291, 0.0016997696899731334, 0.0017100910190492868, 0.001775442292758574, 0.001748699961657015, 0.001727948954794556, 0.004665917852738251, 0.0017052881642788027, 0.0017683604770960908, 0.001711838291763949, 0.0017433040193282068, 0.001745652790608195, 0.001770733773203877, 0.0017143124132417142, 0.001761933439411223, 0.0017878716656317313, 0.0017156354151666164, 0.0016975314792944118, 0.0017919027935325478, 0.0019174394158956904, 0.0017829262263452013, 0.0016865928967793782, 0.0017640241276239976, 0.0016566796208887051, 0.0016715834159792091, 0.001675918353915525, 0.0017522225001206, 0.0018159534180692087, 0.0017535725831597422, 0.0017545086060029764, 0.0017328727068767573, 0.0017482838350891445, 0.0017582013339657958, 0.0017869445194567863, 0.0017072577320504934, 0.0017536154336994514, 0.001751213872921653, 0.0017339062081494678, 0.004312899749493226, 0.00176698818783431, 0.0017694003133025642, 0.0019159089376141007, 0.0017924854376663764, 0.001827493683473828, 0.0018262136630558719, 0.0018503360430865239, 0.0026274734361019605, 0.0017815505852922797, 0.0016568173741688952, 0.001712452809442766, 0.001866175894974731, 0.0018206710810773075, 0.0017904204384346183, 0.0017719205206958577, 0.004318856333460038, 0.001802994855097495, 0.0018152193758093442, 0.0017702800638896103, 0.0017208479791103553, 0.0017134642936677362, 0.0017674009141046554, 0.0017730701656546444, 0.0016525303993451719, 0.001634414186507153, 0.001728909332693244, 0.0018020624411292374, 0.0017662975830414023, 0.0018124437531999622, 0.001797320165981849, 0.0017914534352409344, 0.0017734029388520867, 0.0016971707324652623, 0.0017283566849073395, 0.0017227870848728344, 0.0017416901294685279, 0.0017202713546187927, 0.0017614998990514625, 0.001744159604034697, 0.0017720304992205154, 0.0017485617330142607, 0.0018090489805520822, 0.001706518916762434, 0.0016906983122074355, 0.0016913097497308627, 0.001722647978264528, 0.0017538127722218633, 0.0017680603777989745, 0.001761312562545451, 0.0017782596066050853, 0.001789349955894674, 0.001763269247021526, 0.001741150437737815, 0.0017681080013668786, 0.0017333315239132692, 0.0017009324559088175, 0.001711874332007331, 0.0017677627911325544, 0.0016821919756087784, 0.0017170839176590864, 0.0017034579990043615, 0.001718593814681905, 0.0017306213994743302, 0.0017136736860265955, 0.0017418125013743218, 0.0016737649760519464]
[206.53420261634443, 532.3880794230353, 564.8233920076464, 566.2626521745769, 566.7813214514689, 539.9888655657264, 577.2959108907605, 228.20832316894655, 542.5612864679665, 538.4955585951342, 523.5024502790213, 538.8586364175609, 537.044460931575, 560.9614680735287, 560.9329822591902, 221.3216568527354, 538.2874759128875, 526.7752152780658, 532.8984585052482, 527.2290625712966, 523.990554478899, 538.7431116558931, 235.78731676209702, 544.7280598073748, 545.0037993208401, 544.68172575906, 535.4289867139862, 591.6961904447683, 542.8773625347335, 526.4656883446886, 541.6836036147599, 538.2856199853985, 502.09380713014184, 555.2418199076151, 546.8246324480587, 533.8748025224673, 539.7132941650651, 530.7959014531638, 543.2913209387673, 508.31722658521534, 527.4097369823663, 540.4451931603141, 535.1184999299971, 525.6736423427832, 524.802717397319, 526.9008524026573, 542.9839569781599, 235.64830751327972, 565.7510832036243, 568.9390282523099, 553.3735138792053, 532.2799202896562, 563.9123460225621, 510.85909296154193, 15.472637968964634, 399.46202075977345, 539.0005388791473, 513.3255215060408, 546.3319174206613, 543.1557043024254, 536.1123540480287, 561.8382709305764, 560.8525283018353, 537.9048364503808, 555.0375896508378, 544.4093172625504, 489.2500185502851, 560.2026537024796, 550.969539424076, 556.382596770807, 541.8466399330794, 551.2158127767008, 553.5936122247068, 539.5356939323642, 539.7832117984051, 472.65490279160457, 540.3382584693561, 548.6992912079819, 546.2799699168517, 549.67678822474, 541.1656175264599, 547.6767660006337, 528.9196826499962, 526.3025842542692, 530.9578672042553, 528.5716613695532, 232.52307342850068, 538.4059595831861, 539.1583559691858, 540.9870617417473, 519.6100955799501, 539.0861559118401, 535.3152027192973, 517.9188884437656, 527.1579501160427, 547.9252308484074, 232.02543799176948, 536.6299377788588, 556.6532156363752, 556.8701417046385, 559.4111483820801, 557.0350080461872, 554.273957647432, 543.6167689755249, 503.3809215061225, 544.4966021929537, 564.9302980654862, 572.9124326861898, 567.9235857321311, 566.1169223547815, 561.8791285908817, 555.6688122758342, 565.5594306242201, 271.43335080564617, 579.5283498654172, 570.4033953829967, 558.9070782493395, 571.6639444430173, 562.4239572500127, 577.3951949066911, 530.2496214017094, 546.5711897676586, 571.9647037405874, 470.3451647172238, 586.9248597123724, 401.006900551393, 537.4389575577836, 545.0489459622828, 546.2000832707449, 552.5116180763624, 545.2673800614502, 544.2566088579448, 544.2984883170175, 532.5406163942355, 534.4742829577541, 568.3614140137022, 544.128202467257, 549.0571958427015, 539.2262495066909, 542.2298568190266, 442.68592709979305, 544.6762645986864, 550.1042780491676, 555.5779368660612, 559.8886172028192, 568.8277246757492, 541.8237923964738, 543.0136107924934, 547.7960673307518, 555.5054778805553, 552.6792603284994, 542.8352996219733, 545.2658331125058, 547.644867792349, 548.5532733423055, 563.4437741801358, 554.6835818961813, 537.2433600709255, 504.59255853821753, 528.1012425581241, 556.8723888104342, 555.9371421887686, 546.5149532685841, 556.3236791440215, 517.9740977211017, 541.5785852337222, 512.4647582774171, 499.1730180393188, 510.42016557699804, 543.882734231988, 535.5264823290858, 558.1398563822852, 517.6447831950359, 561.3565261246536, 546.5613314695369, 556.000577079389, 543.4441209563084, 547.6419007723401, 563.2051386216012, 555.140309082626, 545.6982295760845, 528.5183017885411, 529.3333362744459, 536.8420868667235, 526.060652771714, 530.4409113323263, 548.1628285443377, 541.5505191109592, 542.277034998353, 547.5250511210255, 537.9461978338835, 547.2672729962508, 551.3710810806499, 564.3036697344851, 465.5094323008916, 528.6239747827552, 554.9399053794349, 562.1256305848631, 552.9301097272548, 548.7210855648653, 572.5691027894774, 562.1385043108022, 552.0132580925922, 544.2492110860707, 537.9524695570276, 544.5411787410447, 550.5562919952955, 549.7767845736707, 524.7491451688927, 546.9550951840358, 551.7884479393553, 576.8358129106922, 504.08169401443155, 497.2670648143024, 557.8104429562842, 519.181026415445, 484.2306389952993, 556.403268476566, 554.4805611264284, 559.4319892391106, 554.3238128010458, 534.4539042079435, 518.4642474760751, 538.3946939684131, 550.3158685055046, 569.2079973154696, 541.9121786073948, 562.7345478341939, 545.3598795431551, 527.5180664726854, 536.3561756303164, 555.63565067537, 526.8597620390744, 532.1272137269381, 532.3697421273462, 524.3826770122513, 519.3999435488382, 534.2540592955619, 526.8789669070924, 473.003170247792, 563.2411921766978, 541.4756847637449, 534.8608313161056, 539.938536985634, 535.1230812520694, 538.363130995339, 528.1519373002999, 535.9246700045233, 539.5095970106795, 536.2517505304787, 520.3369222345152, 527.9078818953903, 528.0373336867008, 524.81993375625, 524.710355967682, 522.4542741076929, 530.8176931663018, 542.5050315762476, 545.5265691503487, 452.9952235529778, 519.8075027272657, 527.3219825908028, 534.8251213212586, 520.0776112399756, 522.563063340022, 514.3237700789571, 531.8424609251581, 524.6817178106779, 529.4045215064983, 527.6164918143672, 529.762163268769, 514.1926673962082, 486.293177516223, 529.8647563356111, 572.7311055547914, 529.7444477869785, 525.8076257984305, 519.2224188020487, 519.6282572153558, 526.0300552099435, 535.3426848063227, 532.8159326552193, 526.7359849142647, 541.9911577726989, 515.0537602796169, 521.1322087974592, 539.3797585165196, 537.9786113331294, 522.0595511517809, 527.6657691955227, 528.0103514350558, 535.9461847106197, 436.3187133741284, 491.000640173308, 207.686127109727, 535.2026198987609, 541.3958627116345, 541.2257023646056, 534.4159862787451, 529.6248016373858, 537.6937466608499, 545.5376232858833, 540.2749737368861, 543.3790361834325, 556.2306385676259, 555.3567109902514, 537.9506970688129, 543.3873615624273, 550.0949590608401, 528.5663751070952, 512.5472026044196, 550.8038537850194, 556.0296497541893, 528.2190053960095, 560.8889360157693, 545.3350616655943, 549.032611817602, 545.8549577258825, 451.0410080314244, 14.440401053452717, 571.7057048131134, 559.0130569688938, 514.5942062625722, 544.3293601043907, 532.096714946827, 561.0899772663715, 538.0549238212942, 539.9920412025981, 536.8795609653763, 543.819167420275, 541.5664032398453, 536.908451762362, 536.998888972466, 568.1071426781119, 530.7270959939565, 538.0224474833761, 536.2332062350779, 537.3835471155439, 214.42220455906718, 538.1926720142523, 542.2752059521928, 592.003450301875, 564.4116687832411, 547.6165684048415, 570.5056565881555, 561.5701316350638, 543.691353594717, 540.1614501597509, 519.1353775558722, 522.6902560177753, 528.7588714457362, 514.7232868044608, 533.8599703667727, 529.1052597480854, 526.9515610282973, 537.3918626876641, 530.1990508030746, 532.4301983046446, 542.1868339896496, 517.5856266580887, 537.180558221775, 490.5020444853304, 547.0617271794207, 548.314251408304, 542.379310745448, 539.9305939957527, 537.8344822637509, 539.9777192650997, 543.198359476067, 535.7896679479254, 577.2738207435315, 547.973025831198, 562.4509306674122, 527.305202832057, 479.29608700861087, 521.4127685491313, 527.8515992500279, 530.1852249182741, 537.4447576717437, 552.3616898657368, 534.3603751779477, 550.4801361004526, 547.1005506433302, 553.148307043802, 549.3322133697477, 526.3630433150049, 542.7728495027917, 557.9536548614169, 562.1880617973213, 551.7139380811311, 535.7466094724072, 546.2692940432794, 415.7601508236919, 548.1033748311686, 532.5778171606537, 554.6377109325044, 554.2936758514627, 557.7074587810424, 548.9115901923578, 554.7856730780604, 548.6595324837712, 553.4185705592921, 542.6725578664492, 552.90329365467, 542.6935640045023, 518.6323928797489, 523.4800349716329, 539.4665166113529, 552.1902746701742, 541.152601111934, 539.6657279156155, 527.1028820816504, 535.8793711187249, 534.8846247684304, 535.2762680120641, 530.9439387603582, 541.1293779549394, 517.9675157381932, 535.9839733777684, 530.3785241319845, 492.45198652860313, 561.1030265014052, 556.1792698400326, 546.9684235093707, 569.9556264114468, 532.3746643605211, 531.2821651610406, 544.6607289434656, 525.6572666743973, 576.4199155100539, 565.1828490043689, 585.1579876095542, 565.8380850663784, 574.9048438455533, 588.8609600454197, 542.7180431304282, 571.2284782241092, 560.1563123201058, 576.5954704506541, 569.7174587447606, 570.8272930046134, 567.5710753435278, 558.5024583357203, 566.9727394180461, 561.3256852761671, 546.8588966932706, 572.7023173443987, 575.0172605731476, 581.1025887945385, 584.7706500558995, 572.4348984194515, 587.0957602196313, 573.735288338784, 565.0821203374727, 574.7239366815328, 577.7089209260106, 565.9790618657602, 565.3399353489264, 587.4722481522189, 593.5038038439561, 569.3801789379216, 566.4382558298023, 576.5269504505346, 578.1244143588881, 211.2129523751706, 560.6815353746535, 565.5190915374644, 574.3767857379576, 564.2224952158612, 565.6993035438337, 580.3059755461489, 579.5879343784734, 576.0067549543999, 604.6938603969941, 596.9596310098867, 610.1904709969904, 539.0605154502115, 560.8497632007673, 569.0263997276227, 211.823395921188, 582.008607762814, 577.869462173895, 575.9850937409467, 571.1265072936577, 569.7443997555685, 588.3537731737838, 586.7676561983495, 234.0616957199378, 601.6053682750664, 601.8929461480914, 588.3149969663278, 584.764196092874, 563.2399341159442, 571.8533893329708, 578.7207991447264, 214.32010411695046, 586.4111538139468, 565.4955609741673, 584.1673274930419, 573.6234121604081, 572.8516033544073, 564.7376331398763, 583.3242484134088, 567.5583297483504, 559.3242620390527, 582.8744214299654, 589.090695635085, 558.0659863968431, 521.5288638117787, 560.8757026643149, 592.9113077077125, 566.8856702923456, 603.6170104293083, 598.2351765641338, 596.6877787713548, 570.7037775917003, 550.6749182273841, 570.2643903100444, 569.960156694896, 577.0764326955958, 571.9895018928721, 568.7630766064782, 559.6144643058029, 585.7346440592511, 570.2504555918433, 571.0324795061389, 576.7324641321067, 231.86256534655178, 565.9347396236074, 565.1632321311802, 521.9454747391644, 557.8845880622007, 547.1975137550845, 547.5810526609807, 540.4423719336471, 380.59376215181436, 561.3087881172573, 603.5668237132214, 583.9576976870977, 535.8551692221598, 549.248027495604, 558.5280298041669, 564.359398923427, 231.54277956702794, 554.6327529292512, 550.8976013183719, 564.8823710994224, 581.1088557148322, 583.6129785111901, 565.8025816437853, 563.9934726614639, 605.1325896311848, 611.8400147621476, 578.3993301963553, 554.9197281828724, 566.1560144797864, 551.7412599615568, 556.3838980539758, 558.2059685885773, 563.8876411512514, 589.2159114407001, 578.5842753017222, 580.454780965469, 574.1549447174896, 581.303639867675, 567.698017205952, 573.3420254010806, 564.3243727689124, 571.8985959255488, 552.7766305668638, 585.9882302958443, 591.4715788024681, 591.2577516679778, 580.5016536271353, 570.1862911701368, 565.5915445856444, 567.7583986313019, 562.3475876557317, 558.862170424343, 567.1283620973808, 574.3329113475382, 565.5763105120988, 576.923679171508, 587.9128218913869, 584.1550289660616, 565.686756739194, 594.4624718817269, 582.3827185821574, 587.0411836302866, 581.8710572894098, 577.8271320947183, 583.5416673279539, 574.114607175561, 597.4554458409006]
Elapsed: 0.10339852258592831~0.18597887084956494
Time per graph: 0.002120676962573503~0.0037951124294221166
Speed: 537.1283631948503~65.96081176825334
Total Time: 0.0824
best val loss: 0.16612091660499573 test_score: 0.9375

Testing...
Test loss: 0.2222 score: 0.9375 time: 0.08s
test Score 0.9375
Epoch Time List: [0.4508092768955976, 0.3070920908357948, 0.2974808420985937, 0.3007469158619642, 0.29867733223363757, 0.3189322880934924, 0.29995247977785766, 0.4430777980014682, 0.30277947383001447, 0.3086105757392943, 0.3093104548752308, 0.31197931827045977, 0.31028099078685045, 0.296587459044531, 0.30791683681309223, 0.452049741987139, 0.3068047631531954, 0.32514802436344326, 0.31147579103708267, 0.31698304298333824, 0.3158940391149372, 0.5409636630211025, 0.4301249790005386, 0.2957190468441695, 0.3075818223878741, 0.30983677157200873, 0.31063425401225686, 0.29288325691595674, 0.29816899285651743, 0.30855516088195145, 0.30301867821253836, 0.45043403282761574, 0.314528230112046, 0.30104653909802437, 0.30651373974978924, 0.3106672507710755, 0.3116959538310766, 0.31244988553225994, 0.3102447579149157, 0.31436731899157166, 0.4289464079774916, 0.3146040278952569, 0.31443045986816287, 0.3170021940022707, 0.31418308382853866, 0.31814012792892754, 0.30733542679809034, 0.42816265881992877, 0.30104178097099066, 0.2984959650784731, 0.30101055442355573, 0.3033766867592931, 0.2957819397561252, 0.30025161686353385, 5.439451368991286, 1.939063988160342, 0.3023819960653782, 0.32890042290091515, 0.3005390060134232, 0.3059814600273967, 0.3104550950229168, 0.30322231887839735, 0.2949705522041768, 0.30622234917245805, 0.2989605369511992, 0.29768975428305566, 0.3105018970090896, 0.3123672781512141, 0.2914067867677659, 0.2999130510725081, 0.3014565131161362, 0.2972875910345465, 0.3029139139689505, 0.30675380281172693, 0.30161948478780687, 0.3211245040874928, 0.301776607055217, 0.31338142580352724, 0.3072001291438937, 0.3096798041369766, 0.30190809979103506, 0.3071031130384654, 0.4242815771140158, 0.31498940917663276, 0.3126744271721691, 0.3166290116496384, 0.45220593619160354, 0.3145972737111151, 0.3129913138691336, 0.3074410608969629, 0.3135956502519548, 0.30647686985321343, 0.30268257996067405, 0.31290644500404596, 0.3139831554144621, 0.32472115964628756, 0.4205667939968407, 0.2958643559832126, 0.2971589248627424, 0.29533739504404366, 0.30116873723454773, 0.29957090807147324, 0.298228417057544, 0.2992389991413802, 0.36186988395638764, 0.29497691011056304, 0.2906447390560061, 0.29258750984445214, 0.2897995610255748, 0.28071685205213726, 0.293376584071666, 0.2938604343216866, 0.2941603281069547, 0.3980981446802616, 0.27875960478559136, 0.29291771189309657, 0.2925806201528758, 0.2962420720141381, 0.2855291140731424, 0.29301387909799814, 0.2985730180516839, 0.4171671026851982, 0.3249923412222415, 0.32122206105850637, 0.2928521418944001, 0.338657567743212, 0.3078535217791796, 0.29438111395575106, 0.29877174575813115, 0.405639355070889, 0.2927183071151376, 0.2991818729788065, 0.30522995884530246, 0.30744716012850404, 0.3097085910849273, 0.30140858399681747, 0.2998970369808376, 0.3943131840787828, 0.29842788306996226, 0.3024992817081511, 0.32168451440520585, 0.3042407422326505, 0.31036635604687035, 0.30376975564286113, 0.29622170608490705, 0.3879631529562175, 0.2985584808047861, 0.3091707800049335, 0.29914474091492593, 0.29802547092549503, 0.29335695900954306, 0.3047997171524912, 0.30138200521469116, 0.3929511469323188, 0.29627416119910777, 0.29801060468889773, 0.2986612927634269, 0.3012388236820698, 0.31504758913069963, 0.32207043608650565, 0.4254137398675084, 0.29929423006251454, 0.2973416061140597, 0.2974502413999289, 0.31554697011597455, 0.3012160179205239, 0.3121408987790346, 0.3227209278848022, 0.315779997035861, 0.40940228290855885, 0.29797988338395953, 0.3074407409876585, 0.3101475241128355, 0.30617829505354166, 0.3049566268455237, 0.30471025104634464, 0.2998091799672693, 0.41980571509338915, 0.2919372068718076, 0.2942001251503825, 0.29441848094575107, 0.3035931831691414, 0.30677459575235844, 0.30394078977406025, 0.31018955004401505, 0.44257661513984203, 0.30439548776485026, 0.30221236078068614, 0.31251528509892523, 0.3045650019776076, 0.30921331397257745, 0.30318239401094615, 0.3001609621569514, 0.32323995302431285, 0.3321873371023685, 0.2961687420029193, 0.2912444258108735, 0.29599840589798987, 0.29589992109686136, 0.2980103858280927, 0.29065100103616714, 0.4166499082930386, 0.2987802217248827, 0.30731523176655173, 0.3061769320629537, 0.3079274888150394, 0.30924907396547496, 0.29477318888530135, 0.3057236969470978, 0.41958458675071597, 0.29985752515494823, 0.28943479619920254, 0.3145391969010234, 0.3058680861722678, 0.3175489290151745, 0.3351608859375119, 0.3076159539632499, 0.43851399002596736, 0.2939756189007312, 0.2897114201914519, 0.2941924089100212, 0.3035467907320708, 0.3173790068831295, 0.30917054088786244, 0.3011328207794577, 0.444947024108842, 0.3106291990261525, 0.3013878876809031, 0.3058662568219006, 0.30765261268243194, 0.3112448239699006, 0.30625540809705853, 0.3024098740424961, 0.3106351539026946, 0.3105918269138783, 0.3117974968627095, 0.3271943908184767, 0.3179407122079283, 0.315461064921692, 0.3238676351029426, 0.31917756306938827, 0.29707149020396173, 0.30857134493999183, 0.29919993598014116, 0.3006472473498434, 0.30270066391676664, 0.31617723894305527, 0.310060563031584, 0.30303778289817274, 0.31749725597910583, 0.30765246599912643, 0.3136703490745276, 0.3037617711815983, 0.3100222821813077, 0.31562844989821315, 0.3096456341445446, 0.31777848792262375, 0.3134330289904028, 0.3045031821820885, 0.3280748832039535, 0.3415928443428129, 0.3097542349714786, 0.3143105658236891, 0.3112160803284496, 0.32786929910071194, 0.32178427395410836, 0.30771397124044597, 0.3097055701073259, 0.31573589122854173, 0.31116759800352156, 0.31319672078825533, 0.31741949496790767, 0.46038223640061915, 0.3074845401570201, 0.2887935300823301, 0.30939819128252566, 0.3147296919487417, 0.3186817807145417, 0.3162278130184859, 0.3151253161486238, 0.45402214280329645, 0.3105269188527018, 0.3093418348580599, 0.30045825405977666, 0.3076288546435535, 0.3351254379376769, 0.30465051298961043, 0.3061716209631413, 0.4584758756682277, 0.31564452522434294, 0.313891917001456, 0.31723572383634746, 0.33643239503726363, 0.4509134138934314, 0.45527935307472944, 0.313013015082106, 0.31037823227234185, 0.3058922220952809, 0.3278749817982316, 0.31119953421875834, 0.3096919902600348, 0.3016691089142114, 0.41676156502217054, 0.3014163470361382, 0.2969426529016346, 0.2971285437233746, 0.3040206590667367, 0.3023711310233921, 0.298708101734519, 0.30508949188515544, 0.30606251303106546, 0.4136776099912822, 0.3082936571445316, 0.3004203401505947, 0.3010452161543071, 0.29993810993619263, 0.3051175319124013, 0.3046113320160657, 0.3183940851595253, 6.758743972051889, 0.6954023961443454, 0.2966425712220371, 0.2973094000481069, 0.3124816329218447, 0.31036018300801516, 0.43214224209077656, 0.31405796739272773, 0.3052574328612536, 0.3072932150680572, 0.3058795491233468, 0.3056246000342071, 0.3088012041989714, 0.40156126068904996, 0.29815673595294356, 0.3081369358114898, 0.30772144300863147, 0.30986536014825106, 0.3081375788897276, 0.4443745887838304, 0.3134220577776432, 0.3086851311381906, 0.29057709826156497, 0.2971444148570299, 0.30041584209538996, 0.2965014826040715, 0.3022935150656849, 0.39915241696871817, 0.3050804550293833, 0.3251727903261781, 0.31890177680179477, 0.3163657928816974, 0.30976556218229234, 0.3124186401255429, 0.3074349071830511, 0.43195534287951887, 0.30983217619359493, 0.30627137888222933, 0.30808369582518935, 0.31415241002105176, 0.30810327106155455, 0.2991661049891263, 0.3110919438768178, 0.33230334077961743, 0.2990040418226272, 0.3049936799798161, 0.30824847612529993, 0.3084573079831898, 0.30002233013510704, 0.31023985613137484, 0.29958902206271887, 0.3668442270718515, 0.2963636701460928, 0.298623566981405, 0.30853551416657865, 0.3374571625608951, 0.32411689800210297, 0.3169738189317286, 0.3119571730494499, 0.4586844560690224, 0.2973860341589898, 0.3010305720381439, 0.3074203298892826, 0.3036916602868587, 0.3074770199600607, 0.2994341265875846, 0.4296813008841127, 0.3130462619010359, 0.30407083476893604, 0.30129197472706437, 0.2995390151627362, 0.3038095429074019, 0.325178402941674, 0.3540143179707229, 0.3200021330267191, 0.3136418489739299, 0.31320983404293656, 0.29978585080243647, 0.31475522788241506, 0.305359099060297, 0.3034336492419243, 0.3015881578903645, 0.30590824875980616, 0.3075951593928039, 0.3091509479563683, 0.31231799395754933, 0.31862111622467637, 0.31947552994824946, 0.30433231103233993, 0.2954448792152107, 0.30325259268283844, 0.2995774429291487, 0.304251303197816, 0.30705453269183636, 0.3088464450556785, 0.31107923202216625, 0.30156196490861475, 0.30321738007478416, 0.30799810285679996, 0.3056237199343741, 0.3153164938557893, 0.318908937741071, 0.314295063726604, 0.3048625639639795, 0.3162883804179728, 0.31237227423116565, 0.30995161971077323, 0.31403707014396787, 0.5125116331037134, 0.31447055703029037, 0.31144271697849035, 0.3072610858362168, 0.3058781949803233, 0.3009570990689099, 0.4053906199987978, 0.2999680850189179, 0.30273655289784074, 0.32306609814986587, 0.30667017214000225, 0.3040347550995648, 0.29914583591744304, 0.42323159403167665, 0.3056946098804474, 0.3098309230990708, 0.3104163568932563, 0.3096331038977951, 0.3113494662102312, 0.32484807819128036, 0.31346002407372, 0.3792717319447547, 0.3018138911575079, 0.3041241569444537, 0.3006313310470432, 0.30151955503970385, 0.3008026049938053, 0.30611956981010735, 0.3038868212606758, 0.30405130703002214, 0.41947038983926177, 0.29769312031567097, 0.29411424696445465, 0.3065895407926291, 0.30139240762218833, 0.3067016389686614, 0.30010775034315884, 0.4524318231269717, 0.3203816069290042, 0.3222690930124372, 0.29998306394554675, 0.3067252791952342, 0.30758151202462614, 0.3048174208961427, 0.2994631710462272, 0.3765429649502039, 0.29317467869259417, 0.29325653915293515, 0.29283260367810726, 0.3014675108715892, 0.3197287148796022, 0.3044843249954283, 0.4440810938831419, 0.294030605815351, 0.2946288539096713, 0.2999113129917532, 0.29849136294797063, 0.3385368469171226, 0.2871841141022742, 0.29402318992652, 0.4264579978771508, 0.28779601608403027, 0.28930016071535647, 0.2950808391906321, 0.29407479823566973, 0.3017500308342278, 0.3068782230839133, 0.30625627213157713, 0.44385598530061543, 0.29528877092525363, 0.3036484660115093, 0.2981852067168802, 0.2978830309584737, 0.30664369999431074, 0.309329534182325, 0.3055870751850307, 0.3070284300483763, 0.416415074840188, 0.3021420605946332, 0.30070832767523825, 0.3053934960626066, 0.34665308985859156, 0.31982062896713614, 0.29815493267960846, 0.2971422777045518, 0.37023873534053564, 0.29446703591383994, 0.29608320794068277, 0.3053469047881663, 0.32496635988354683, 0.31289136316627264, 0.306919566122815, 0.3090074569918215, 0.4468082869425416, 0.3034041547216475, 0.30222903308458626, 0.3173388249706477, 0.3101598396897316, 0.312599488068372, 0.30644494690932333, 0.4364069199655205, 0.30265385494567454, 0.30824642512016, 0.31732203997671604, 0.3056768460664898, 0.3302264092490077, 0.3140198839828372, 0.32321065687574446, 0.3693547882139683, 0.31615048088133335, 0.3006942558567971, 0.30133492196910083, 0.3076498901937157, 0.3049267618916929, 0.3095224406570196, 0.31068087578751147, 0.42545053409412503, 0.3083020681515336, 0.3098048500251025, 0.3037999761290848, 0.2862646367866546, 0.2907912319060415, 0.30468921386636794, 0.30351114622317255, 0.41661058901809156, 0.296249286737293, 0.2974370731972158, 0.3001738239545375, 0.30619139480404556, 0.3102976121008396, 0.314124115742743, 0.3101808479987085, 0.4275380598846823, 0.29993051826022565, 0.2953069540672004, 0.29541356419213116, 0.29936128412373364, 0.291670047910884, 0.30569545016624033, 0.3255047220736742, 0.30004778783768415, 0.31089474679902196, 0.3044663609471172, 0.28844696120359004, 0.29203654220327735, 0.29917831905186176, 0.2950763569679111, 0.29395318380557, 0.2975988192483783, 0.2987362090498209, 0.3038416928611696, 0.3228966819588095, 0.30007629794999957, 0.30603433889336884, 0.3008339221123606, 0.3113195518963039, 0.29726828285492957, 0.30136222997680306, 0.31681504007428885, 0.29915599059313536, 0.2999251561705023, 0.2956221327185631, 0.2973615580704063, 0.2988267519976944, 0.3098403988406062, 0.30240577063523233, 0.2901714821346104]
Total Epoch List: [232, 197, 165]
Total Time List: [0.08906854595988989, 0.09458365896716714, 0.0824199509806931]
T-times Epoch Time: 0.3442157329615871 ~ 0.0021645325305132785
T-times Total Epoch: 170.55555555555557 ~ 23.624114066607845
T-times Total Time: 0.10596075221999651 ~ 0.024584665128101462
T-times Inference Elapsed: 0.10085635710048348 ~ 0.0018639989624858885
T-times Time Per Graph: 0.002069715726875585 ~ 3.792937617308178e-05
T-times Speed: 550.2117566493866 ~ 22.92930411191269
T-times cross validation test micro f1 score:0.9288899726814357 ~ 0.0031739563552987578
T-times cross validation test precision:0.9360283999414434 ~ 0.005848304286714559
T-times cross validation test recall:0.9224074074074075 ~ 0.006158625610487308
T-times cross validation test f1_score:0.9288899726814357 ~ 0.0028920021971317123
