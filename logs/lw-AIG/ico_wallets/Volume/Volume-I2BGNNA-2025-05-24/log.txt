Namespace(seed=15, model='I2BGNNA', dataset='ico_wallets/Volume', num_heads=8, num_layers=2, dim_hidden=64, dropout=0.1, epochs=1000, lr=0.0003, weight_decay=0.0001, batch_size=64, abs_pe=None, abs_pe_dim=20, num_class=2, outdir='./outdir/ico_wallets/Volume/seed15/khopgnn_gat_1_0.1_0.0003_0.0001_2_8_64_BN', warmup=10, layer_norm=False, use_edge_attr=False, edge_dim=128, gnn_type='gat', k_hop=1, global_pool='mean', se='khopgnn', ie='mamba', aggr='add', not_extract_node_feature=False, profile=False, k_ford=3, early_stop=1, early_stop_mindelta=-0.0, patience=20, training_times=3, Lambda=0.01, order='sd', use_cuda=True, batch_norm=True, save_logs=True)
Data(edge_index=[2, 334], edge_attr=[334, 2], x=[97, 14887], y=[1, 1], num_nodes=97)
Data(edge_index=[2, 334], edge_attr=[334, 2], x=[97, 14887], y=[1, 1], num_nodes=97)
Data(edge_index=[2, 276], edge_attr=[276, 2], x=[88, 14887], y=[1, 1], num_nodes=97)
========================training times:0========================
========================k_idx:0========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: I2BGNNA
I2BGNN(
  (gcs): ModuleList(
    (0): GCNConv(14887, 64)
    (1): GCNConv(64, 64)
  )
  (bns): ModuleList(
    (0-1): 2 x BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  )
  (drops): ModuleList(
    (0-1): 2 x Dropout(p=0.2, inplace=False)
  )
  (lin1): Linear(in_features=64, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=64, bias=True)
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=64, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=64, out_features=2, bias=True)
  )
)
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7d1ecd6a59c0>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.6912;  Loss pred: 0.6912; Loss self: 0.0000; time: 0.29s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6930 score: 0.5102 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6944 score: 0.4898 time: 0.07s
Epoch 2/1000, LR 0.000000
Train loss: 0.6913;  Loss pred: 0.6913; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6930 score: 0.5102 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6944 score: 0.4898 time: 0.07s
Epoch 3/1000, LR 0.000030
Train loss: 0.6927;  Loss pred: 0.6927; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6930 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6943 score: 0.4898 time: 0.07s
Epoch 4/1000, LR 0.000060
Train loss: 0.6904;  Loss pred: 0.6904; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6930 score: 0.5102 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6942 score: 0.4898 time: 0.07s
Epoch 5/1000, LR 0.000090
Train loss: 0.6869;  Loss pred: 0.6869; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6929 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6941 score: 0.4898 time: 0.07s
Epoch 6/1000, LR 0.000120
Train loss: 0.6877;  Loss pred: 0.6877; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6929 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6940 score: 0.4898 time: 0.07s
Epoch 7/1000, LR 0.000150
Train loss: 0.6832;  Loss pred: 0.6832; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6929 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6939 score: 0.4898 time: 0.07s
Epoch 8/1000, LR 0.000180
Train loss: 0.6827;  Loss pred: 0.6827; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6929 score: 0.5102 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6939 score: 0.4898 time: 0.07s
Epoch 9/1000, LR 0.000210
Train loss: 0.6795;  Loss pred: 0.6795; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6928 score: 0.5102 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6938 score: 0.4898 time: 0.07s
Epoch 10/1000, LR 0.000240
Train loss: 0.6743;  Loss pred: 0.6743; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6928 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6936 score: 0.4898 time: 0.07s
Epoch 11/1000, LR 0.000270
Train loss: 0.6673;  Loss pred: 0.6673; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6928 score: 0.5102 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6935 score: 0.4898 time: 0.07s
Epoch 12/1000, LR 0.000270
Train loss: 0.6635;  Loss pred: 0.6635; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6928 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6934 score: 0.4898 time: 0.07s
Epoch 13/1000, LR 0.000270
Train loss: 0.6593;  Loss pred: 0.6593; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6927 score: 0.5102 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6932 score: 0.4898 time: 0.07s
Epoch 14/1000, LR 0.000270
Train loss: 0.6531;  Loss pred: 0.6531; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6927 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6931 score: 0.4898 time: 0.07s
Epoch 15/1000, LR 0.000270
Train loss: 0.6522;  Loss pred: 0.6522; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6927 score: 0.5102 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6929 score: 0.4898 time: 0.07s
Epoch 16/1000, LR 0.000270
Train loss: 0.6449;  Loss pred: 0.6449; Loss self: 0.0000; time: 0.12s
Val loss: 0.6926 score: 0.5306 time: 0.08s
Test loss: 0.6928 score: 0.5102 time: 0.07s
Epoch 17/1000, LR 0.000270
Train loss: 0.6384;  Loss pred: 0.6384; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6926 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6926 score: 0.5102 time: 0.07s
Epoch 18/1000, LR 0.000270
Train loss: 0.6340;  Loss pred: 0.6340; Loss self: 0.0000; time: 0.11s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6925 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6924 score: 0.5102 time: 0.07s
Epoch 19/1000, LR 0.000270
Train loss: 0.6224;  Loss pred: 0.6224; Loss self: 0.0000; time: 0.11s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6925 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6922 score: 0.5102 time: 0.07s
Epoch 20/1000, LR 0.000270
Train loss: 0.6206;  Loss pred: 0.6206; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6925 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6920 score: 0.5102 time: 0.07s
Epoch 21/1000, LR 0.000270
Train loss: 0.6122;  Loss pred: 0.6122; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6924 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6919 score: 0.5102 time: 0.07s
Epoch 22/1000, LR 0.000270
Train loss: 0.6027;  Loss pred: 0.6027; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6924 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6917 score: 0.5102 time: 0.07s
Epoch 23/1000, LR 0.000270
Train loss: 0.5910;  Loss pred: 0.5910; Loss self: 0.0000; time: 0.11s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6924 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6915 score: 0.5102 time: 0.07s
Epoch 24/1000, LR 0.000270
Train loss: 0.5866;  Loss pred: 0.5866; Loss self: 0.0000; time: 0.11s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6923 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6913 score: 0.5102 time: 0.07s
Epoch 25/1000, LR 0.000270
Train loss: 0.5718;  Loss pred: 0.5718; Loss self: 0.0000; time: 0.11s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6923 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6911 score: 0.5102 time: 0.07s
Epoch 26/1000, LR 0.000270
Train loss: 0.5623;  Loss pred: 0.5623; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6923 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6910 score: 0.5102 time: 0.07s
     INFO: Early stopping counter 1 of 20
Epoch 27/1000, LR 0.000270
Train loss: 0.5547;  Loss pred: 0.5547; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6924 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6909 score: 0.5102 time: 0.07s
     INFO: Early stopping counter 2 of 20
Epoch 28/1000, LR 0.000270
Train loss: 0.5372;  Loss pred: 0.5372; Loss self: 0.0000; time: 0.11s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6922 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6906 score: 0.5102 time: 0.07s
Epoch 29/1000, LR 0.000270
Train loss: 0.5249;  Loss pred: 0.5249; Loss self: 0.0000; time: 0.11s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6920 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6904 score: 0.5102 time: 0.07s
Epoch 30/1000, LR 0.000270
Train loss: 0.5140;  Loss pred: 0.5140; Loss self: 0.0000; time: 0.11s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6919 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6902 score: 0.5102 time: 0.06s
Epoch 31/1000, LR 0.000270
Train loss: 0.4999;  Loss pred: 0.4999; Loss self: 0.0000; time: 0.11s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6918 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6899 score: 0.5102 time: 0.06s
Epoch 32/1000, LR 0.000270
Train loss: 0.4804;  Loss pred: 0.4804; Loss self: 0.0000; time: 0.11s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6915 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6896 score: 0.5102 time: 0.07s
Epoch 33/1000, LR 0.000270
Train loss: 0.4748;  Loss pred: 0.4748; Loss self: 0.0000; time: 0.11s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6912 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6893 score: 0.5102 time: 0.07s
Epoch 34/1000, LR 0.000270
Train loss: 0.4576;  Loss pred: 0.4576; Loss self: 0.0000; time: 0.11s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6909 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6890 score: 0.5102 time: 0.07s
Epoch 35/1000, LR 0.000270
Train loss: 0.4473;  Loss pred: 0.4473; Loss self: 0.0000; time: 0.11s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6905 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6886 score: 0.5102 time: 0.07s
Epoch 36/1000, LR 0.000270
Train loss: 0.4189;  Loss pred: 0.4189; Loss self: 0.0000; time: 0.11s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6901 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6882 score: 0.5102 time: 0.07s
Epoch 37/1000, LR 0.000270
Train loss: 0.4051;  Loss pred: 0.4051; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6896 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6878 score: 0.5102 time: 0.06s
Epoch 38/1000, LR 0.000270
Train loss: 0.3891;  Loss pred: 0.3891; Loss self: 0.0000; time: 0.11s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6890 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6873 score: 0.5102 time: 0.06s
Epoch 39/1000, LR 0.000269
Train loss: 0.3681;  Loss pred: 0.3681; Loss self: 0.0000; time: 0.11s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6883 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6868 score: 0.5102 time: 0.07s
Epoch 40/1000, LR 0.000269
Train loss: 0.3610;  Loss pred: 0.3610; Loss self: 0.0000; time: 0.11s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6873 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6861 score: 0.5102 time: 0.06s
Epoch 41/1000, LR 0.000269
Train loss: 0.3410;  Loss pred: 0.3410; Loss self: 0.0000; time: 0.11s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6863 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6853 score: 0.5102 time: 0.07s
Epoch 42/1000, LR 0.000269
Train loss: 0.3192;  Loss pred: 0.3192; Loss self: 0.0000; time: 0.11s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6850 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6844 score: 0.5102 time: 0.06s
Epoch 43/1000, LR 0.000269
Train loss: 0.3089;  Loss pred: 0.3089; Loss self: 0.0000; time: 0.11s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6835 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6832 score: 0.5102 time: 0.07s
Epoch 44/1000, LR 0.000269
Train loss: 0.2880;  Loss pred: 0.2880; Loss self: 0.0000; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6816 score: 0.4898 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6819 score: 0.5102 time: 0.06s
Epoch 45/1000, LR 0.000269
Train loss: 0.2704;  Loss pred: 0.2704; Loss self: 0.0000; time: 0.11s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6794 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6802 score: 0.5102 time: 0.06s
Epoch 46/1000, LR 0.000269
Train loss: 0.2478;  Loss pred: 0.2478; Loss self: 0.0000; time: 0.11s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6770 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6785 score: 0.5102 time: 0.06s
Epoch 47/1000, LR 0.000269
Train loss: 0.2373;  Loss pred: 0.2373; Loss self: 0.0000; time: 0.11s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6741 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6764 score: 0.5102 time: 0.06s
Epoch 48/1000, LR 0.000269
Train loss: 0.2243;  Loss pred: 0.2243; Loss self: 0.0000; time: 0.11s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6709 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6741 score: 0.5102 time: 0.06s
Epoch 49/1000, LR 0.000269
Train loss: 0.2099;  Loss pred: 0.2099; Loss self: 0.0000; time: 0.11s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6670 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6713 score: 0.5102 time: 0.06s
Epoch 50/1000, LR 0.000269
Train loss: 0.2017;  Loss pred: 0.2017; Loss self: 0.0000; time: 0.11s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6626 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6681 score: 0.5102 time: 0.06s
Epoch 51/1000, LR 0.000269
Train loss: 0.1819;  Loss pred: 0.1819; Loss self: 0.0000; time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6576 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6644 score: 0.5102 time: 0.07s
Epoch 52/1000, LR 0.000269
Train loss: 0.1693;  Loss pred: 0.1693; Loss self: 0.0000; time: 0.11s
Val loss: 0.6523 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6604 score: 0.5102 time: 0.07s
Epoch 53/1000, LR 0.000269
Train loss: 0.1573;  Loss pred: 0.1573; Loss self: 0.0000; time: 0.11s
Val loss: 0.6462 score: 0.5102 time: 0.08s
Test loss: 0.6559 score: 0.5306 time: 0.07s
Epoch 54/1000, LR 0.000269
Train loss: 0.1412;  Loss pred: 0.1412; Loss self: 0.0000; time: 0.11s
Val loss: 0.6392 score: 0.5306 time: 0.08s
Test loss: 0.6507 score: 0.5510 time: 0.06s
Epoch 55/1000, LR 0.000269
Train loss: 0.1306;  Loss pred: 0.1306; Loss self: 0.0000; time: 0.11s
Val loss: 0.6312 score: 0.5510 time: 0.08s
Test loss: 0.6447 score: 0.5510 time: 0.07s
Epoch 56/1000, LR 0.000269
Train loss: 0.1224;  Loss pred: 0.1224; Loss self: 0.0000; time: 0.11s
Val loss: 0.6223 score: 0.5714 time: 0.07s
Test loss: 0.6380 score: 0.5510 time: 0.07s
Epoch 57/1000, LR 0.000269
Train loss: 0.1143;  Loss pred: 0.1143; Loss self: 0.0000; time: 0.11s
Val loss: 0.6124 score: 0.5714 time: 0.07s
Test loss: 0.6306 score: 0.5510 time: 0.07s
Epoch 58/1000, LR 0.000269
Train loss: 0.1005;  Loss pred: 0.1005; Loss self: 0.0000; time: 0.11s
Val loss: 0.6016 score: 0.5714 time: 0.08s
Test loss: 0.6225 score: 0.5918 time: 0.17s
Epoch 59/1000, LR 0.000268
Train loss: 0.0947;  Loss pred: 0.0947; Loss self: 0.0000; time: 0.11s
Val loss: 0.5899 score: 0.5714 time: 0.07s
Test loss: 0.6136 score: 0.5918 time: 0.07s
Epoch 60/1000, LR 0.000268
Train loss: 0.0826;  Loss pred: 0.0826; Loss self: 0.0000; time: 0.11s
Val loss: 0.5771 score: 0.5918 time: 0.07s
Test loss: 0.6039 score: 0.5918 time: 0.07s
Epoch 61/1000, LR 0.000268
Train loss: 0.0811;  Loss pred: 0.0811; Loss self: 0.0000; time: 0.11s
Val loss: 0.5634 score: 0.5918 time: 0.07s
Test loss: 0.5937 score: 0.5918 time: 0.07s
Epoch 62/1000, LR 0.000268
Train loss: 0.0715;  Loss pred: 0.0715; Loss self: 0.0000; time: 0.11s
Val loss: 0.5487 score: 0.6531 time: 0.07s
Test loss: 0.5829 score: 0.6327 time: 0.07s
Epoch 63/1000, LR 0.000268
Train loss: 0.0644;  Loss pred: 0.0644; Loss self: 0.0000; time: 0.11s
Val loss: 0.5329 score: 0.6531 time: 0.07s
Test loss: 0.5712 score: 0.6327 time: 0.07s
Epoch 64/1000, LR 0.000268
Train loss: 0.0582;  Loss pred: 0.0582; Loss self: 0.0000; time: 0.12s
Val loss: 0.5161 score: 0.6531 time: 0.08s
Test loss: 0.5588 score: 0.6735 time: 0.06s
Epoch 65/1000, LR 0.000268
Train loss: 0.0530;  Loss pred: 0.0530; Loss self: 0.0000; time: 0.11s
Val loss: 0.4982 score: 0.7143 time: 0.07s
Test loss: 0.5455 score: 0.6735 time: 0.06s
Epoch 66/1000, LR 0.000268
Train loss: 0.0456;  Loss pred: 0.0456; Loss self: 0.0000; time: 0.12s
Val loss: 0.4796 score: 0.7347 time: 0.19s
Test loss: 0.5318 score: 0.7143 time: 0.06s
Epoch 67/1000, LR 0.000268
Train loss: 0.0416;  Loss pred: 0.0416; Loss self: 0.0000; time: 0.11s
Val loss: 0.4605 score: 0.7551 time: 0.07s
Test loss: 0.5178 score: 0.7347 time: 0.06s
Epoch 68/1000, LR 0.000268
Train loss: 0.0399;  Loss pred: 0.0399; Loss self: 0.0000; time: 0.11s
Val loss: 0.4413 score: 0.8163 time: 0.07s
Test loss: 0.5036 score: 0.7347 time: 0.06s
Epoch 69/1000, LR 0.000268
Train loss: 0.0370;  Loss pred: 0.0370; Loss self: 0.0000; time: 0.11s
Val loss: 0.4219 score: 0.8367 time: 0.07s
Test loss: 0.4896 score: 0.7755 time: 0.07s
Epoch 70/1000, LR 0.000268
Train loss: 0.0323;  Loss pred: 0.0323; Loss self: 0.0000; time: 0.12s
Val loss: 0.4021 score: 0.8367 time: 0.07s
Test loss: 0.4754 score: 0.8163 time: 0.06s
Epoch 71/1000, LR 0.000268
Train loss: 0.0280;  Loss pred: 0.0280; Loss self: 0.0000; time: 0.11s
Val loss: 0.3822 score: 0.9184 time: 0.07s
Test loss: 0.4614 score: 0.8367 time: 0.06s
Epoch 72/1000, LR 0.000267
Train loss: 0.0276;  Loss pred: 0.0276; Loss self: 0.0000; time: 0.11s
Val loss: 0.3625 score: 0.9388 time: 0.08s
Test loss: 0.4475 score: 0.8367 time: 0.19s
Epoch 73/1000, LR 0.000267
Train loss: 0.0242;  Loss pred: 0.0242; Loss self: 0.0000; time: 0.11s
Val loss: 0.3429 score: 0.9592 time: 0.08s
Test loss: 0.4341 score: 0.8163 time: 0.06s
Epoch 74/1000, LR 0.000267
Train loss: 0.0221;  Loss pred: 0.0221; Loss self: 0.0000; time: 0.11s
Val loss: 0.3236 score: 0.9796 time: 0.07s
Test loss: 0.4210 score: 0.8163 time: 0.06s
Epoch 75/1000, LR 0.000267
Train loss: 0.0211;  Loss pred: 0.0211; Loss self: 0.0000; time: 0.11s
Val loss: 0.3047 score: 0.9796 time: 0.07s
Test loss: 0.4084 score: 0.8367 time: 0.06s
Epoch 76/1000, LR 0.000267
Train loss: 0.0168;  Loss pred: 0.0168; Loss self: 0.0000; time: 0.11s
Val loss: 0.2867 score: 0.9796 time: 0.07s
Test loss: 0.3966 score: 0.8367 time: 0.06s
Epoch 77/1000, LR 0.000267
Train loss: 0.0170;  Loss pred: 0.0170; Loss self: 0.0000; time: 0.11s
Val loss: 0.2690 score: 0.9796 time: 0.08s
Test loss: 0.3854 score: 0.8571 time: 0.06s
Epoch 78/1000, LR 0.000267
Train loss: 0.0162;  Loss pred: 0.0162; Loss self: 0.0000; time: 0.11s
Val loss: 0.2523 score: 0.9796 time: 0.08s
Test loss: 0.3750 score: 0.8571 time: 0.06s
Epoch 79/1000, LR 0.000267
Train loss: 0.0165;  Loss pred: 0.0165; Loss self: 0.0000; time: 0.11s
Val loss: 0.2363 score: 0.9796 time: 0.07s
Test loss: 0.3653 score: 0.8571 time: 0.06s
Epoch 80/1000, LR 0.000267
Train loss: 0.0124;  Loss pred: 0.0124; Loss self: 0.0000; time: 0.11s
Val loss: 0.2213 score: 0.9796 time: 0.07s
Test loss: 0.3567 score: 0.8571 time: 0.18s
Epoch 81/1000, LR 0.000267
Train loss: 0.0119;  Loss pred: 0.0119; Loss self: 0.0000; time: 0.11s
Val loss: 0.2071 score: 0.9796 time: 0.07s
Test loss: 0.3488 score: 0.8571 time: 0.06s
Epoch 82/1000, LR 0.000267
Train loss: 0.0121;  Loss pred: 0.0121; Loss self: 0.0000; time: 0.11s
Val loss: 0.1939 score: 0.9796 time: 0.08s
Test loss: 0.3419 score: 0.8571 time: 0.07s
Epoch 83/1000, LR 0.000266
Train loss: 0.0107;  Loss pred: 0.0107; Loss self: 0.0000; time: 0.11s
Val loss: 0.1816 score: 0.9796 time: 0.07s
Test loss: 0.3358 score: 0.8571 time: 0.06s
Epoch 84/1000, LR 0.000266
Train loss: 0.0099;  Loss pred: 0.0099; Loss self: 0.0000; time: 0.12s
Val loss: 0.1704 score: 0.9796 time: 0.08s
Test loss: 0.3307 score: 0.8571 time: 0.07s
Epoch 85/1000, LR 0.000266
Train loss: 0.0096;  Loss pred: 0.0096; Loss self: 0.0000; time: 0.11s
Val loss: 0.1598 score: 0.9796 time: 0.07s
Test loss: 0.3265 score: 0.8571 time: 0.07s
Epoch 86/1000, LR 0.000266
Train loss: 0.0086;  Loss pred: 0.0086; Loss self: 0.0000; time: 0.11s
Val loss: 0.1503 score: 0.9796 time: 0.07s
Test loss: 0.3232 score: 0.8571 time: 0.07s
Epoch 87/1000, LR 0.000266
Train loss: 0.0081;  Loss pred: 0.0081; Loss self: 0.0000; time: 0.11s
Val loss: 0.1418 score: 0.9796 time: 0.08s
Test loss: 0.3208 score: 0.8571 time: 0.07s
Epoch 88/1000, LR 0.000266
Train loss: 0.0083;  Loss pred: 0.0083; Loss self: 0.0000; time: 0.23s
Val loss: 0.1342 score: 0.9592 time: 0.07s
Test loss: 0.3192 score: 0.8571 time: 0.06s
Epoch 89/1000, LR 0.000266
Train loss: 0.0073;  Loss pred: 0.0073; Loss self: 0.0000; time: 0.11s
Val loss: 0.1275 score: 0.9592 time: 0.07s
Test loss: 0.3184 score: 0.8571 time: 0.07s
Epoch 90/1000, LR 0.000266
Train loss: 0.0077;  Loss pred: 0.0077; Loss self: 0.0000; time: 0.11s
Val loss: 0.1215 score: 0.9592 time: 0.07s
Test loss: 0.3183 score: 0.8571 time: 0.07s
Epoch 91/1000, LR 0.000266
Train loss: 0.0064;  Loss pred: 0.0064; Loss self: 0.0000; time: 0.11s
Val loss: 0.1163 score: 0.9592 time: 0.07s
Test loss: 0.3189 score: 0.8571 time: 0.07s
Epoch 92/1000, LR 0.000266
Train loss: 0.0058;  Loss pred: 0.0058; Loss self: 0.0000; time: 0.11s
Val loss: 0.1118 score: 0.9592 time: 0.08s
Test loss: 0.3201 score: 0.8571 time: 0.07s
Epoch 93/1000, LR 0.000265
Train loss: 0.0058;  Loss pred: 0.0058; Loss self: 0.0000; time: 0.11s
Val loss: 0.1078 score: 0.9388 time: 0.08s
Test loss: 0.3219 score: 0.8571 time: 0.07s
Epoch 94/1000, LR 0.000265
Train loss: 0.0058;  Loss pred: 0.0058; Loss self: 0.0000; time: 0.11s
Val loss: 0.1042 score: 0.9388 time: 0.07s
Test loss: 0.3242 score: 0.8571 time: 0.06s
Epoch 95/1000, LR 0.000265
Train loss: 0.0056;  Loss pred: 0.0056; Loss self: 0.0000; time: 0.12s
Val loss: 0.1013 score: 0.9388 time: 0.19s
Test loss: 0.3269 score: 0.8776 time: 0.07s
Epoch 96/1000, LR 0.000265
Train loss: 0.0053;  Loss pred: 0.0053; Loss self: 0.0000; time: 0.11s
Val loss: 0.0987 score: 0.9388 time: 0.07s
Test loss: 0.3301 score: 0.8776 time: 0.06s
Epoch 97/1000, LR 0.000265
Train loss: 0.0054;  Loss pred: 0.0054; Loss self: 0.0000; time: 0.11s
Val loss: 0.0965 score: 0.9388 time: 0.07s
Test loss: 0.3336 score: 0.8776 time: 0.06s
Epoch 98/1000, LR 0.000265
Train loss: 0.0053;  Loss pred: 0.0053; Loss self: 0.0000; time: 0.11s
Val loss: 0.0947 score: 0.9388 time: 0.07s
Test loss: 0.3374 score: 0.8776 time: 0.06s
Epoch 99/1000, LR 0.000265
Train loss: 0.0049;  Loss pred: 0.0049; Loss self: 0.0000; time: 0.11s
Val loss: 0.0931 score: 0.9388 time: 0.07s
Test loss: 0.3413 score: 0.8776 time: 0.06s
Epoch 100/1000, LR 0.000265
Train loss: 0.0042;  Loss pred: 0.0042; Loss self: 0.0000; time: 0.12s
Val loss: 0.0918 score: 0.9388 time: 0.07s
Test loss: 0.3454 score: 0.8776 time: 0.06s
Epoch 101/1000, LR 0.000265
Train loss: 0.0042;  Loss pred: 0.0042; Loss self: 0.0000; time: 0.11s
Val loss: 0.0907 score: 0.9388 time: 0.07s
Test loss: 0.3497 score: 0.8776 time: 0.07s
Epoch 102/1000, LR 0.000264
Train loss: 0.0041;  Loss pred: 0.0041; Loss self: 0.0000; time: 0.13s
Val loss: 0.0898 score: 0.9388 time: 0.12s
Test loss: 0.3541 score: 0.8776 time: 0.06s
Epoch 103/1000, LR 0.000264
Train loss: 0.0042;  Loss pred: 0.0042; Loss self: 0.0000; time: 0.11s
Val loss: 0.0890 score: 0.9388 time: 0.07s
Test loss: 0.3585 score: 0.8776 time: 0.06s
Epoch 104/1000, LR 0.000264
Train loss: 0.0042;  Loss pred: 0.0042; Loss self: 0.0000; time: 0.11s
Val loss: 0.0886 score: 0.9388 time: 0.07s
Test loss: 0.3630 score: 0.8776 time: 0.06s
Epoch 105/1000, LR 0.000264
Train loss: 0.0034;  Loss pred: 0.0034; Loss self: 0.0000; time: 0.11s
Val loss: 0.0883 score: 0.9388 time: 0.07s
Test loss: 0.3672 score: 0.8776 time: 0.06s
Epoch 106/1000, LR 0.000264
Train loss: 0.0034;  Loss pred: 0.0034; Loss self: 0.0000; time: 0.11s
Val loss: 0.0880 score: 0.9388 time: 0.07s
Test loss: 0.3715 score: 0.8776 time: 0.06s
Epoch 107/1000, LR 0.000264
Train loss: 0.0033;  Loss pred: 0.0033; Loss self: 0.0000; time: 0.11s
Val loss: 0.0879 score: 0.9388 time: 0.07s
Test loss: 0.3756 score: 0.8776 time: 0.06s
Epoch 108/1000, LR 0.000264
Train loss: 0.0033;  Loss pred: 0.0033; Loss self: 0.0000; time: 0.11s
Val loss: 0.0877 score: 0.9388 time: 0.07s
Test loss: 0.3796 score: 0.8980 time: 0.06s
Epoch 109/1000, LR 0.000264
Train loss: 0.0029;  Loss pred: 0.0029; Loss self: 0.0000; time: 0.11s
Val loss: 0.0875 score: 0.9388 time: 0.07s
Test loss: 0.3837 score: 0.8980 time: 0.06s
Epoch 110/1000, LR 0.000263
Train loss: 0.0036;  Loss pred: 0.0036; Loss self: 0.0000; time: 0.23s
Val loss: 0.0877 score: 0.9388 time: 0.07s
Test loss: 0.3875 score: 0.8980 time: 0.06s
     INFO: Early stopping counter 1 of 20
Epoch 111/1000, LR 0.000263
Train loss: 0.0033;  Loss pred: 0.0033; Loss self: 0.0000; time: 0.11s
Val loss: 0.0879 score: 0.9388 time: 0.07s
Test loss: 0.3914 score: 0.8980 time: 0.06s
     INFO: Early stopping counter 2 of 20
Epoch 112/1000, LR 0.000263
Train loss: 0.0027;  Loss pred: 0.0027; Loss self: 0.0000; time: 0.11s
Val loss: 0.0882 score: 0.9388 time: 0.07s
Test loss: 0.3952 score: 0.8980 time: 0.07s
     INFO: Early stopping counter 3 of 20
Epoch 113/1000, LR 0.000263
Train loss: 0.0027;  Loss pred: 0.0027; Loss self: 0.0000; time: 0.11s
Val loss: 0.0886 score: 0.9388 time: 0.07s
Test loss: 0.3985 score: 0.8980 time: 0.07s
     INFO: Early stopping counter 4 of 20
Epoch 114/1000, LR 0.000263
Train loss: 0.0032;  Loss pred: 0.0032; Loss self: 0.0000; time: 0.11s
Val loss: 0.0888 score: 0.9388 time: 0.08s
Test loss: 0.4017 score: 0.8980 time: 0.06s
     INFO: Early stopping counter 5 of 20
Epoch 115/1000, LR 0.000263
Train loss: 0.0025;  Loss pred: 0.0025; Loss self: 0.0000; time: 0.12s
Val loss: 0.0890 score: 0.9388 time: 0.08s
Test loss: 0.4049 score: 0.8980 time: 0.07s
     INFO: Early stopping counter 6 of 20
Epoch 116/1000, LR 0.000263
Train loss: 0.0027;  Loss pred: 0.0027; Loss self: 0.0000; time: 0.11s
Val loss: 0.0890 score: 0.9388 time: 0.07s
Test loss: 0.4081 score: 0.8980 time: 0.18s
     INFO: Early stopping counter 7 of 20
Epoch 117/1000, LR 0.000262
Train loss: 0.0027;  Loss pred: 0.0027; Loss self: 0.0000; time: 0.11s
Val loss: 0.0893 score: 0.9388 time: 0.07s
Test loss: 0.4110 score: 0.8980 time: 0.06s
     INFO: Early stopping counter 8 of 20
Epoch 118/1000, LR 0.000262
Train loss: 0.0025;  Loss pred: 0.0025; Loss self: 0.0000; time: 0.11s
Val loss: 0.0896 score: 0.9388 time: 0.07s
Test loss: 0.4137 score: 0.8980 time: 0.06s
     INFO: Early stopping counter 9 of 20
Epoch 119/1000, LR 0.000262
Train loss: 0.0027;  Loss pred: 0.0027; Loss self: 0.0000; time: 0.11s
Val loss: 0.0898 score: 0.9388 time: 0.07s
Test loss: 0.4163 score: 0.8980 time: 0.06s
     INFO: Early stopping counter 10 of 20
Epoch 120/1000, LR 0.000262
Train loss: 0.0026;  Loss pred: 0.0026; Loss self: 0.0000; time: 0.11s
Val loss: 0.0897 score: 0.9388 time: 0.07s
Test loss: 0.4187 score: 0.8980 time: 0.06s
     INFO: Early stopping counter 11 of 20
Epoch 121/1000, LR 0.000262
Train loss: 0.0026;  Loss pred: 0.0026; Loss self: 0.0000; time: 0.11s
Val loss: 0.0897 score: 0.9388 time: 0.07s
Test loss: 0.4214 score: 0.8980 time: 0.06s
     INFO: Early stopping counter 12 of 20
Epoch 122/1000, LR 0.000262
Train loss: 0.0021;  Loss pred: 0.0021; Loss self: 0.0000; time: 0.11s
Val loss: 0.0896 score: 0.9388 time: 0.07s
Test loss: 0.4236 score: 0.8980 time: 0.06s
     INFO: Early stopping counter 13 of 20
Epoch 123/1000, LR 0.000262
Train loss: 0.0021;  Loss pred: 0.0021; Loss self: 0.0000; time: 0.11s
Val loss: 0.0895 score: 0.9388 time: 0.07s
Test loss: 0.4257 score: 0.8980 time: 0.07s
     INFO: Early stopping counter 14 of 20
Epoch 124/1000, LR 0.000261
Train loss: 0.0021;  Loss pred: 0.0021; Loss self: 0.0000; time: 0.23s
Val loss: 0.0895 score: 0.9388 time: 0.07s
Test loss: 0.4279 score: 0.8980 time: 0.07s
     INFO: Early stopping counter 15 of 20
Epoch 125/1000, LR 0.000261
Train loss: 0.0022;  Loss pred: 0.0022; Loss self: 0.0000; time: 0.11s
Val loss: 0.0894 score: 0.9388 time: 0.07s
Test loss: 0.4301 score: 0.8980 time: 0.07s
     INFO: Early stopping counter 16 of 20
Epoch 126/1000, LR 0.000261
Train loss: 0.0022;  Loss pred: 0.0022; Loss self: 0.0000; time: 0.11s
Val loss: 0.0893 score: 0.9388 time: 0.07s
Test loss: 0.4319 score: 0.8980 time: 0.07s
     INFO: Early stopping counter 17 of 20
Epoch 127/1000, LR 0.000261
Train loss: 0.0022;  Loss pred: 0.0022; Loss self: 0.0000; time: 0.12s
Val loss: 0.0892 score: 0.9388 time: 0.07s
Test loss: 0.4337 score: 0.8980 time: 0.07s
     INFO: Early stopping counter 18 of 20
Epoch 128/1000, LR 0.000261
Train loss: 0.0022;  Loss pred: 0.0022; Loss self: 0.0000; time: 0.11s
Val loss: 0.0891 score: 0.9388 time: 0.07s
Test loss: 0.4354 score: 0.8980 time: 0.07s
     INFO: Early stopping counter 19 of 20
Epoch 129/1000, LR 0.000261
Train loss: 0.0019;  Loss pred: 0.0019; Loss self: 0.0000; time: 0.10s
Val loss: 0.0891 score: 0.9388 time: 0.07s
Test loss: 0.4370 score: 0.8980 time: 0.06s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 108,   Train_Loss: 0.0029,   Val_Loss: 0.0875,   Val_Precision: 0.9565,   Val_Recall: 0.9167,   Val_accuracy: 0.9362,   Val_Score: 0.9388,   Val_Loss: 0.0875,   Test_Precision: 0.9167,   Test_Recall: 0.8800,   Test_accuracy: 0.8980,   Test_Score: 0.8980,   Test_loss: 0.3837


[0.07078393292613328, 0.07182950200513005, 0.07565474603325129, 0.07194918906316161, 0.07211259403266013, 0.07183831394650042, 0.07171307306271046, 0.07754708500579, 0.07189430296421051, 0.07233359490055591, 0.07208557205740362, 0.07247356593143195, 0.07235056802164763, 0.07162254396826029, 0.07129303400870413, 0.07121002895291895, 0.07144724403042346, 0.07110963901504874, 0.07078702899161726, 0.07133766694460064, 0.07188453897833824, 0.07135094807017595, 0.07108811009675264, 0.07125854701735079, 0.07144543598406017, 0.0712470100261271, 0.07794450398068875, 0.07134187500923872, 0.07038487808313221, 0.07021258503664285, 0.06894871802069247, 0.06994629197288305, 0.07000614900607616, 0.07075591094326228, 0.07042106206063181, 0.07219593797344714, 0.06987993896473199, 0.06980699102859944, 0.07032186491414905, 0.06991312198806554, 0.07057835895102471, 0.06988244294188917, 0.07170787593349814, 0.06543997407425195, 0.06565393705386668, 0.06502827198710293, 0.0654224120080471, 0.06565646606031805, 0.065755404997617, 0.06710893998388201, 0.07122395897749811, 0.0705567040713504, 0.0703179620904848, 0.0694255989510566, 0.07010203192476183, 0.0698585860664025, 0.06987044401466846, 0.17841056000906974, 0.0767566820140928, 0.07033698994200677, 0.06991267600096762, 0.07096321997232735, 0.07042018102947623, 0.06967529293615371, 0.06858130893670022, 0.06912373204249889, 0.06948012509383261, 0.06913573492784053, 0.0707393690245226, 0.06919512699823827, 0.06900859496090561, 0.19735902093816549, 0.06876668892800808, 0.06972259900067002, 0.06890193605795503, 0.06926834501791745, 0.06936872692313045, 0.06947234307881445, 0.06907203898299485, 0.1828952939249575, 0.06973210093565285, 0.06988967803772539, 0.06952537305187434, 0.07037475309334695, 0.0705721570411697, 0.07015506504103541, 0.07141511095687747, 0.06892503902781755, 0.07028862601146102, 0.06987591995857656, 0.07757178705651313, 0.07075237692333758, 0.0708232200704515, 0.06944266706705093, 0.0797262160340324, 0.06569136108737439, 0.0661277249455452, 0.06685079296585172, 0.06828834500629455, 0.06611818401142955, 0.07231886603403836, 0.06593887007329613, 0.06615694495849311, 0.0678873200668022, 0.06609812309034169, 0.06723849603440613, 0.06685232697054744, 0.06520597694907337, 0.06793095404282212, 0.06609208206646144, 0.06636362907011062, 0.07037095003761351, 0.07143615407403558, 0.06979516707360744, 0.07024105591699481, 0.18975004798267037, 0.06593798601534218, 0.06600498605985194, 0.06832788500469178, 0.06692010897677392, 0.06735822302289307, 0.06627176003530622, 0.07662019901908934, 0.07063216099049896, 0.07641606102697551, 0.07008099695667624, 0.07711331208702177, 0.0702624199911952, 0.06638188497163355]
[0.0014445700597170057, 0.0014659082041863277, 0.001543974408841863, 0.0014683507972073797, 0.001471685592503268, 0.0014660880397244984, 0.0014635321033206216, 0.0015825935715467346, 0.00146723067273899, 0.0014761958142970593, 0.0014711341236204822, 0.0014790523659475908, 0.0014765422045234212, 0.0014616845707808221, 0.0014549598777286556, 0.0014532658969983459, 0.0014581070210290502, 0.0014512171227560968, 0.001444633244726883, 0.0014558707539714417, 0.0014670314077211886, 0.0014561417973505296, 0.0014507777570765845, 0.0014542560615785876, 0.001458070122123677, 0.0014540206127781042, 0.001590704162871199, 0.0014559566328416066, 0.0014364260833292287, 0.0014329098987069968, 0.0014071166942998463, 0.0014274753463853682, 0.0014286969184913502, 0.0014439981825155566, 0.001437164531849629, 0.0014733864892540233, 0.0014261212033618773, 0.0014246324699714171, 0.0014351401002887561, 0.001426798407919705, 0.001440374672469892, 0.0014261723049365136, 0.001463426039459146, 0.0013355096749847336, 0.0013398762664054425, 0.001327107591573529, 0.001335151265470349, 0.001339927878782001, 0.001341947040767694, 0.001369570203752694, 0.0014535501832142472, 0.001439932736150008, 0.0014350604508262203, 0.0014168489581848286, 0.0014306537127502415, 0.0014256854299265816, 0.001425927428870785, 0.0036410318369197907, 0.0015664628982467919, 0.0014354487743266687, 0.0014267893061421964, 0.0014482289790270888, 0.0014371465516219639, 0.0014219447537990554, 0.001399618549728576, 0.0014106884090305896, 0.0014179617366088288, 0.0014109333658742966, 0.001443660592337196, 0.001412145448943638, 0.0014083386726715431, 0.004027735121187051, 0.0014034018148573078, 0.001422910183687143, 0.0014061619603664291, 0.0014136396942432134, 0.0014156883045536826, 0.001417802919975805, 0.001409633448632548, 0.003732557018876684, 0.0014231041007276093, 0.0014263199599535794, 0.001418885164323966, 0.0014362194508846318, 0.0014402481028810143, 0.0014317360212456208, 0.0014574512440179075, 0.001406633449547297, 0.0014344617553359391, 0.001426039182828093, 0.0015830976950308802, 0.0014439260596599505, 0.0014453718381724795, 0.0014171972870826721, 0.0016270656333476, 0.001340640022191314, 0.0013495454070519429, 0.0013643018972622802, 0.0013936396940060112, 0.001349350694110807, 0.0014758952251844564, 0.0013456912259856354, 0.0013501417338467982, 0.0013854555115673918, 0.0013489412875579937, 0.0013722142047837985, 0.00136433320348056, 0.001330734223450477, 0.0013863460008739208, 0.001348818001356356, 0.001354359776941033, 0.0014361418375023166, 0.0014578806953884813, 0.0014243911647674988, 0.0014334909370815267, 0.0038724499588300076, 0.001345673183986575, 0.0013470405318337132, 0.0013944466327488118, 0.0013657165097300799, 0.0013746576127121035, 0.0013524848986797187, 0.0015636775310018233, 0.001441472673275489, 0.0015595114495301125, 0.0014302244276872703, 0.0015737410630004443, 0.0014339269385958205, 0.0013547323463598685]
[692.2474914064758, 682.1709552782424, 647.6791287946937, 681.0361678570783, 679.4929603809243, 682.0872777789772, 683.2784861576255, 631.8741703358862, 681.5560897000775, 677.4169052065656, 679.7476749019904, 676.1085834573042, 677.2579862170394, 684.1421329814042, 687.3041760856694, 688.1053233723121, 685.8207151998048, 689.0767648198897, 692.217214057715, 686.874159173896, 681.6486646004047, 686.746305764668, 689.2854505951813, 687.636810614016, 685.8380710411249, 687.748159284595, 628.6524064883403, 686.8336442468682, 696.1722650442849, 697.8805861431774, 710.6731119394333, 700.5374926664654, 699.9385153402319, 692.5216472626873, 695.8145555630999, 678.7085447663503, 701.2026731266898, 701.9354262086019, 696.7960826951988, 700.8698597148115, 694.2638044900104, 701.1775481396094, 683.328007727388, 748.7778027601568, 746.3375724108863, 753.5184082658413, 748.9788055196267, 746.3088244040443, 745.1858900690487, 730.1560717807292, 687.9707433208064, 694.4768841589993, 696.8347566294235, 705.7915342515638, 698.9811658040106, 701.4170019619945, 701.2979621213375, 274.6474199593848, 638.3809033199666, 696.6462460278833, 700.8743307053762, 690.4985430355038, 695.8232609412031, 703.2622029289591, 714.4803848119383, 708.8737623407494, 705.2376479435768, 708.7506924044862, 692.6835887243155, 708.1423522966807, 710.0564795987981, 248.27849148761322, 712.5543015644995, 702.7850467755674, 711.1556336934416, 707.3938317325945, 706.3701782259657, 705.3166458544641, 709.4042788003337, 267.9128530234619, 702.6892828772799, 701.104961072371, 704.7786707083193, 696.2724250698982, 694.3248166754327, 698.4527770209992, 686.1292987360564, 710.9172615807156, 697.1255917281728, 701.2430037278635, 631.6729555850271, 692.5562381189403, 691.8634870210248, 705.6180597540651, 614.6033568065436, 745.9123877008175, 740.9902584785804, 732.9755987341819, 717.5455781727229, 741.0971842712679, 677.5548717389614, 743.1125214237479, 740.6629799901222, 721.784273584275, 741.322108844569, 728.7491971106339, 732.9587797532839, 751.4648547980436, 721.3206510998141, 741.3898680136323, 738.3562455307166, 696.3100537055324, 685.9271840028927, 702.0543406440102, 697.5977134783428, 258.234453803538, 743.1224846418404, 742.3681592109999, 717.1303487095405, 732.2163808341453, 727.4538697873064, 739.3797897308793, 639.5180465113648, 693.7349687855537, 641.2264560810402, 699.1909665653241, 635.4285488957327, 697.3856010957257, 738.1531877399804]
Elapsed: 0.07367469030053377~0.02051735912061977
Time per graph: 0.0015035651081741585~0.000418721614706526
Speed: 687.0561906684796~80.66728339127927
Total Time: 0.0670
best val loss: 0.08752463012933731 test_score: 0.8980

Testing...
Test loss: 0.4210 score: 0.8163 time: 0.07s
test Score 0.8163
Epoch Time List: [0.4344634370645508, 0.2635430139489472, 0.2607021938310936, 0.2612041379325092, 0.25651010393630713, 0.2584334989078343, 0.2577688579913229, 0.26812492311000824, 0.26772345695644617, 0.26062874705530703, 0.26006776408758014, 0.25733603595290333, 0.2611486701061949, 0.2559775010449812, 0.26052997494116426, 0.2592564520891756, 0.26425005099736154, 0.2575646300101653, 0.25313281978014857, 0.25991963408887386, 0.2557188560022041, 0.2585642950143665, 0.2577043240889907, 0.2574328950140625, 0.2569704339839518, 0.2612698870943859, 0.2687719070818275, 0.2582835239591077, 0.25436484697274864, 0.2482264790451154, 0.24814224790316075, 0.24753734900150448, 0.25341243483126163, 0.25052620586939156, 0.2548666420625523, 0.2540812570368871, 0.3867917510215193, 0.2527400868711993, 0.2487003490095958, 0.25279088702518493, 0.24963115295395255, 0.2505265739746392, 0.25388738699257374, 0.36178546003066003, 0.23628104489762336, 0.23987887997645885, 0.24056445399764925, 0.23666120308917016, 0.2413169019855559, 0.24418613000307232, 0.36085116281174123, 0.25077103497460485, 0.25516509101726115, 0.25512102397624403, 0.2560048389714211, 0.25191611202899367, 0.2509110620012507, 0.36683513585012406, 0.2576390679460019, 0.2500923709012568, 0.2514547168975696, 0.25222785701043904, 0.2509378530085087, 0.2563305289950222, 0.2496927878819406, 0.371875366079621, 0.2491267540026456, 0.24852963001467288, 0.2515180470654741, 0.2539589679799974, 0.2493374350015074, 0.38316226098686457, 0.25281407788861543, 0.24813696695491672, 0.2481856788508594, 0.24904972407966852, 0.2562789289513603, 0.25462263193912804, 0.24897942191455513, 0.36303250899072737, 0.24885577789973468, 0.25247344095259905, 0.24862695415504277, 0.2607372240163386, 0.25188332702964544, 0.25121795292943716, 0.2558600160991773, 0.3614651779644191, 0.24957317113876343, 0.24941887706518173, 0.2584806131199002, 0.2567017601104453, 0.258104367996566, 0.2510665850713849, 0.3807046910515055, 0.24510983505751938, 0.24535710492637008, 0.2484397990629077, 0.24521608091890812, 0.2507089850259945, 0.24745711602736264, 0.30859925400000066, 0.2457962220069021, 0.25176771311089396, 0.24575713800732046, 0.24402898899279535, 0.24620702001266181, 0.24044316005893052, 0.23992235015612096, 0.36147376406006515, 0.23914747592061758, 0.24348997103516012, 0.24643740500323474, 0.2566716820001602, 0.2594536499818787, 0.36758032999932766, 0.24151650886051357, 0.2487489429768175, 0.24128121905960143, 0.25018550804816186, 0.247679433086887, 0.2431446418631822, 0.2489456810289994, 0.3647894309833646, 0.2576233949512243, 0.25503387907519937, 0.26108589000068605, 0.2509082049364224, 0.23513870302122086]
Total Epoch List: [129]
Total Time List: [0.06697597703896463]
========================k_idx:1========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: I2BGNNA
I2BGNN(
  (gcs): ModuleList(
    (0): GCNConv(14887, 64)
    (1): GCNConv(64, 64)
  )
  (bns): ModuleList(
    (0-1): 2 x BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  )
  (drops): ModuleList(
    (0-1): 2 x Dropout(p=0.2, inplace=False)
  )
  (lin1): Linear(in_features=64, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=64, bias=True)
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=64, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=64, out_features=2, bias=True)
  )
)
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7d1ecd824640>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.6956;  Loss pred: 0.6956; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6933 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6951 score: 0.4898 time: 0.07s
Epoch 2/1000, LR 0.000000
Train loss: 0.6944;  Loss pred: 0.6944; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6933 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6951 score: 0.4898 time: 0.08s
Epoch 3/1000, LR 0.000030
Train loss: 0.6932;  Loss pred: 0.6932; Loss self: 0.0000; time: 0.11s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6932 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6951 score: 0.4898 time: 0.08s
Epoch 4/1000, LR 0.000060
Train loss: 0.6918;  Loss pred: 0.6918; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6932 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6951 score: 0.4898 time: 0.08s
Epoch 5/1000, LR 0.000090
Train loss: 0.6913;  Loss pred: 0.6913; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6932 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6951 score: 0.4898 time: 0.08s
Epoch 6/1000, LR 0.000120
Train loss: 0.6904;  Loss pred: 0.6904; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6932 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6950 score: 0.4898 time: 0.08s
Epoch 7/1000, LR 0.000150
Train loss: 0.6869;  Loss pred: 0.6869; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6932 score: 0.5102 time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6950 score: 0.4898 time: 0.08s
Epoch 8/1000, LR 0.000180
Train loss: 0.6864;  Loss pred: 0.6864; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6932 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6949 score: 0.4898 time: 0.08s
Epoch 9/1000, LR 0.000210
Train loss: 0.6815;  Loss pred: 0.6815; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6931 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6948 score: 0.4898 time: 0.08s
Epoch 10/1000, LR 0.000240
Train loss: 0.6808;  Loss pred: 0.6808; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6931 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6947 score: 0.4898 time: 0.08s
Epoch 11/1000, LR 0.000270
Train loss: 0.6761;  Loss pred: 0.6761; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6930 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6945 score: 0.4898 time: 0.08s
Epoch 12/1000, LR 0.000270
Train loss: 0.6706;  Loss pred: 0.6706; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6929 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6943 score: 0.4898 time: 0.08s
Epoch 13/1000, LR 0.000270
Train loss: 0.6662;  Loss pred: 0.6662; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6929 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6941 score: 0.4898 time: 0.08s
Epoch 14/1000, LR 0.000270
Train loss: 0.6609;  Loss pred: 0.6609; Loss self: 0.0000; time: 0.11s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6928 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6938 score: 0.4898 time: 0.21s
Epoch 15/1000, LR 0.000270
Train loss: 0.6551;  Loss pred: 0.6551; Loss self: 0.0000; time: 0.11s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6927 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6936 score: 0.4898 time: 0.08s
Epoch 16/1000, LR 0.000270
Train loss: 0.6515;  Loss pred: 0.6515; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6926 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6934 score: 0.4898 time: 0.08s
Epoch 17/1000, LR 0.000270
Train loss: 0.6456;  Loss pred: 0.6456; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6925 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6932 score: 0.4898 time: 0.08s
Epoch 18/1000, LR 0.000270
Train loss: 0.6408;  Loss pred: 0.6408; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6924 score: 0.5102 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6929 score: 0.4898 time: 0.08s
Epoch 19/1000, LR 0.000270
Train loss: 0.6335;  Loss pred: 0.6335; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6923 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6927 score: 0.4898 time: 0.08s
Epoch 20/1000, LR 0.000270
Train loss: 0.6273;  Loss pred: 0.6273; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6922 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6924 score: 0.4898 time: 0.08s
Epoch 21/1000, LR 0.000270
Train loss: 0.6205;  Loss pred: 0.6205; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6920 score: 0.5102 time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6920 score: 0.4898 time: 0.08s
Epoch 22/1000, LR 0.000270
Train loss: 0.6140;  Loss pred: 0.6140; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6918 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6916 score: 0.4898 time: 0.08s
Epoch 23/1000, LR 0.000270
Train loss: 0.6022;  Loss pred: 0.6022; Loss self: 0.0000; time: 0.13s
Val loss: 0.6916 score: 0.5306 time: 0.07s
Test loss: 0.6912 score: 0.5306 time: 0.08s
Epoch 24/1000, LR 0.000270
Train loss: 0.5922;  Loss pred: 0.5922; Loss self: 0.0000; time: 0.13s
Val loss: 0.6913 score: 0.5714 time: 0.07s
Test loss: 0.6908 score: 0.5714 time: 0.08s
Epoch 25/1000, LR 0.000270
Train loss: 0.5863;  Loss pred: 0.5863; Loss self: 0.0000; time: 0.13s
Val loss: 0.6910 score: 0.5918 time: 0.07s
Test loss: 0.6903 score: 0.6122 time: 0.08s
Epoch 26/1000, LR 0.000270
Train loss: 0.5736;  Loss pred: 0.5736; Loss self: 0.0000; time: 0.13s
Val loss: 0.6906 score: 0.5714 time: 0.07s
Test loss: 0.6897 score: 0.6531 time: 0.08s
Epoch 27/1000, LR 0.000270
Train loss: 0.5649;  Loss pred: 0.5649; Loss self: 0.0000; time: 0.13s
Val loss: 0.6902 score: 0.6531 time: 0.07s
Test loss: 0.6891 score: 0.7143 time: 0.08s
Epoch 28/1000, LR 0.000270
Train loss: 0.5530;  Loss pred: 0.5530; Loss self: 0.0000; time: 0.13s
Val loss: 0.6897 score: 0.6939 time: 0.07s
Test loss: 0.6884 score: 0.7551 time: 0.08s
Epoch 29/1000, LR 0.000270
Train loss: 0.5448;  Loss pred: 0.5448; Loss self: 0.0000; time: 0.13s
Val loss: 0.6892 score: 0.7755 time: 0.07s
Test loss: 0.6876 score: 0.7755 time: 0.08s
Epoch 30/1000, LR 0.000270
Train loss: 0.5273;  Loss pred: 0.5273; Loss self: 0.0000; time: 0.13s
Val loss: 0.6886 score: 0.7959 time: 0.07s
Test loss: 0.6868 score: 0.8163 time: 0.08s
Epoch 31/1000, LR 0.000270
Train loss: 0.5206;  Loss pred: 0.5206; Loss self: 0.0000; time: 0.13s
Val loss: 0.6879 score: 0.7755 time: 0.07s
Test loss: 0.6858 score: 0.8367 time: 0.08s
Epoch 32/1000, LR 0.000270
Train loss: 0.5048;  Loss pred: 0.5048; Loss self: 0.0000; time: 0.13s
Val loss: 0.6871 score: 0.7959 time: 0.07s
Test loss: 0.6847 score: 0.8571 time: 0.08s
Epoch 33/1000, LR 0.000270
Train loss: 0.4892;  Loss pred: 0.4892; Loss self: 0.0000; time: 0.13s
Val loss: 0.6862 score: 0.7959 time: 0.07s
Test loss: 0.6835 score: 0.8776 time: 0.08s
Epoch 34/1000, LR 0.000270
Train loss: 0.4746;  Loss pred: 0.4746; Loss self: 0.0000; time: 0.13s
Val loss: 0.6852 score: 0.7959 time: 0.08s
Test loss: 0.6821 score: 0.8776 time: 0.09s
Epoch 35/1000, LR 0.000270
Train loss: 0.4672;  Loss pred: 0.4672; Loss self: 0.0000; time: 0.13s
Val loss: 0.6841 score: 0.7959 time: 0.08s
Test loss: 0.6805 score: 0.8776 time: 0.08s
Epoch 36/1000, LR 0.000270
Train loss: 0.4585;  Loss pred: 0.4585; Loss self: 0.0000; time: 0.14s
Val loss: 0.6828 score: 0.7959 time: 0.08s
Test loss: 0.6788 score: 0.8571 time: 0.08s
Epoch 37/1000, LR 0.000270
Train loss: 0.4406;  Loss pred: 0.4406; Loss self: 0.0000; time: 0.14s
Val loss: 0.6814 score: 0.8163 time: 0.08s
Test loss: 0.6769 score: 0.8571 time: 0.08s
Epoch 38/1000, LR 0.000270
Train loss: 0.4230;  Loss pred: 0.4230; Loss self: 0.0000; time: 0.13s
Val loss: 0.6797 score: 0.8163 time: 0.08s
Test loss: 0.6747 score: 0.8571 time: 0.08s
Epoch 39/1000, LR 0.000269
Train loss: 0.4142;  Loss pred: 0.4142; Loss self: 0.0000; time: 0.13s
Val loss: 0.6778 score: 0.8163 time: 0.07s
Test loss: 0.6722 score: 0.8571 time: 0.08s
Epoch 40/1000, LR 0.000269
Train loss: 0.4026;  Loss pred: 0.4026; Loss self: 0.0000; time: 0.13s
Val loss: 0.6756 score: 0.8163 time: 0.07s
Test loss: 0.6693 score: 0.8571 time: 0.08s
Epoch 41/1000, LR 0.000269
Train loss: 0.3870;  Loss pred: 0.3870; Loss self: 0.0000; time: 0.13s
Val loss: 0.6733 score: 0.8163 time: 0.07s
Test loss: 0.6662 score: 0.8571 time: 0.08s
Epoch 42/1000, LR 0.000269
Train loss: 0.3682;  Loss pred: 0.3682; Loss self: 0.0000; time: 0.13s
Val loss: 0.6708 score: 0.8163 time: 0.07s
Test loss: 0.6629 score: 0.8980 time: 0.08s
Epoch 43/1000, LR 0.000269
Train loss: 0.3607;  Loss pred: 0.3607; Loss self: 0.0000; time: 0.13s
Val loss: 0.6681 score: 0.8163 time: 0.07s
Test loss: 0.6592 score: 0.8980 time: 0.08s
Epoch 44/1000, LR 0.000269
Train loss: 0.3460;  Loss pred: 0.3460; Loss self: 0.0000; time: 0.13s
Val loss: 0.6650 score: 0.7959 time: 0.07s
Test loss: 0.6551 score: 0.8980 time: 0.08s
Epoch 45/1000, LR 0.000269
Train loss: 0.3391;  Loss pred: 0.3391; Loss self: 0.0000; time: 0.13s
Val loss: 0.6617 score: 0.8367 time: 0.07s
Test loss: 0.6505 score: 0.9184 time: 0.08s
Epoch 46/1000, LR 0.000269
Train loss: 0.3191;  Loss pred: 0.3191; Loss self: 0.0000; time: 0.13s
Val loss: 0.6579 score: 0.8367 time: 0.07s
Test loss: 0.6455 score: 0.9388 time: 0.08s
Epoch 47/1000, LR 0.000269
Train loss: 0.3069;  Loss pred: 0.3069; Loss self: 0.0000; time: 0.13s
Val loss: 0.6539 score: 0.8367 time: 0.07s
Test loss: 0.6400 score: 0.9388 time: 0.08s
Epoch 48/1000, LR 0.000269
Train loss: 0.2942;  Loss pred: 0.2942; Loss self: 0.0000; time: 0.13s
Val loss: 0.6496 score: 0.8571 time: 0.07s
Test loss: 0.6341 score: 0.9388 time: 0.08s
Epoch 49/1000, LR 0.000269
Train loss: 0.2846;  Loss pred: 0.2846; Loss self: 0.0000; time: 0.13s
Val loss: 0.6450 score: 0.8571 time: 0.07s
Test loss: 0.6278 score: 0.9388 time: 0.08s
Epoch 50/1000, LR 0.000269
Train loss: 0.2676;  Loss pred: 0.2676; Loss self: 0.0000; time: 0.13s
Val loss: 0.6399 score: 0.8776 time: 0.07s
Test loss: 0.6208 score: 0.9388 time: 0.08s
Epoch 51/1000, LR 0.000269
Train loss: 0.2535;  Loss pred: 0.2535; Loss self: 0.0000; time: 0.13s
Val loss: 0.6346 score: 0.8776 time: 0.07s
Test loss: 0.6134 score: 0.9592 time: 0.08s
Epoch 52/1000, LR 0.000269
Train loss: 0.2438;  Loss pred: 0.2438; Loss self: 0.0000; time: 0.13s
Val loss: 0.6290 score: 0.8776 time: 0.07s
Test loss: 0.6054 score: 0.9592 time: 0.08s
Epoch 53/1000, LR 0.000269
Train loss: 0.2350;  Loss pred: 0.2350; Loss self: 0.0000; time: 0.13s
Val loss: 0.6231 score: 0.8980 time: 0.07s
Test loss: 0.5970 score: 0.9592 time: 0.08s
Epoch 54/1000, LR 0.000269
Train loss: 0.2189;  Loss pred: 0.2189; Loss self: 0.0000; time: 0.13s
Val loss: 0.6169 score: 0.8980 time: 0.07s
Test loss: 0.5881 score: 0.9796 time: 0.08s
Epoch 55/1000, LR 0.000269
Train loss: 0.2056;  Loss pred: 0.2056; Loss self: 0.0000; time: 0.13s
Val loss: 0.6103 score: 0.8980 time: 0.07s
Test loss: 0.5788 score: 0.9796 time: 0.08s
Epoch 56/1000, LR 0.000269
Train loss: 0.1940;  Loss pred: 0.1940; Loss self: 0.0000; time: 0.13s
Val loss: 0.6037 score: 0.9184 time: 0.07s
Test loss: 0.5691 score: 0.9796 time: 0.08s
Epoch 57/1000, LR 0.000269
Train loss: 0.1829;  Loss pred: 0.1829; Loss self: 0.0000; time: 0.13s
Val loss: 0.5966 score: 0.9184 time: 0.07s
Test loss: 0.5589 score: 0.9796 time: 0.08s
Epoch 58/1000, LR 0.000269
Train loss: 0.1730;  Loss pred: 0.1730; Loss self: 0.0000; time: 0.13s
Val loss: 0.5894 score: 0.8980 time: 0.07s
Test loss: 0.5484 score: 0.9796 time: 0.08s
Epoch 59/1000, LR 0.000268
Train loss: 0.1612;  Loss pred: 0.1612; Loss self: 0.0000; time: 0.13s
Val loss: 0.5818 score: 0.8980 time: 0.07s
Test loss: 0.5373 score: 0.9592 time: 0.08s
Epoch 60/1000, LR 0.000268
Train loss: 0.1514;  Loss pred: 0.1514; Loss self: 0.0000; time: 0.13s
Val loss: 0.5740 score: 0.8980 time: 0.07s
Test loss: 0.5259 score: 0.9592 time: 0.08s
Epoch 61/1000, LR 0.000268
Train loss: 0.1430;  Loss pred: 0.1430; Loss self: 0.0000; time: 0.13s
Val loss: 0.5659 score: 0.8980 time: 0.07s
Test loss: 0.5140 score: 0.9592 time: 0.08s
Epoch 62/1000, LR 0.000268
Train loss: 0.1324;  Loss pred: 0.1324; Loss self: 0.0000; time: 0.13s
Val loss: 0.5578 score: 0.8980 time: 0.07s
Test loss: 0.5022 score: 0.9592 time: 0.08s
Epoch 63/1000, LR 0.000268
Train loss: 0.1228;  Loss pred: 0.1228; Loss self: 0.0000; time: 0.13s
Val loss: 0.5496 score: 0.8980 time: 0.07s
Test loss: 0.4899 score: 0.9592 time: 0.08s
Epoch 64/1000, LR 0.000268
Train loss: 0.1166;  Loss pred: 0.1166; Loss self: 0.0000; time: 0.13s
Val loss: 0.5408 score: 0.8980 time: 0.07s
Test loss: 0.4769 score: 0.9592 time: 0.08s
Epoch 65/1000, LR 0.000268
Train loss: 0.1053;  Loss pred: 0.1053; Loss self: 0.0000; time: 0.13s
Val loss: 0.5320 score: 0.8776 time: 0.07s
Test loss: 0.4634 score: 0.9592 time: 0.08s
Epoch 66/1000, LR 0.000268
Train loss: 0.0991;  Loss pred: 0.0991; Loss self: 0.0000; time: 0.13s
Val loss: 0.5233 score: 0.8776 time: 0.07s
Test loss: 0.4498 score: 0.9592 time: 0.08s
Epoch 67/1000, LR 0.000268
Train loss: 0.0919;  Loss pred: 0.0919; Loss self: 0.0000; time: 0.13s
Val loss: 0.5143 score: 0.8571 time: 0.07s
Test loss: 0.4358 score: 0.9592 time: 0.08s
Epoch 68/1000, LR 0.000268
Train loss: 0.0833;  Loss pred: 0.0833; Loss self: 0.0000; time: 0.13s
Val loss: 0.5052 score: 0.8571 time: 0.07s
Test loss: 0.4214 score: 0.9592 time: 0.08s
Epoch 69/1000, LR 0.000268
Train loss: 0.0778;  Loss pred: 0.0778; Loss self: 0.0000; time: 0.13s
Val loss: 0.4962 score: 0.8571 time: 0.07s
Test loss: 0.4065 score: 0.9592 time: 0.08s
Epoch 70/1000, LR 0.000268
Train loss: 0.0721;  Loss pred: 0.0721; Loss self: 0.0000; time: 0.13s
Val loss: 0.4871 score: 0.8571 time: 0.07s
Test loss: 0.3914 score: 0.9592 time: 0.08s
Epoch 71/1000, LR 0.000268
Train loss: 0.0675;  Loss pred: 0.0675; Loss self: 0.0000; time: 0.13s
Val loss: 0.4779 score: 0.8571 time: 0.07s
Test loss: 0.3760 score: 0.9592 time: 0.08s
Epoch 72/1000, LR 0.000267
Train loss: 0.0603;  Loss pred: 0.0603; Loss self: 0.0000; time: 0.13s
Val loss: 0.4693 score: 0.8571 time: 0.07s
Test loss: 0.3607 score: 0.9592 time: 0.08s
Epoch 73/1000, LR 0.000267
Train loss: 0.0546;  Loss pred: 0.0546; Loss self: 0.0000; time: 0.13s
Val loss: 0.4609 score: 0.8571 time: 0.07s
Test loss: 0.3452 score: 0.9592 time: 0.08s
Epoch 74/1000, LR 0.000267
Train loss: 0.0513;  Loss pred: 0.0513; Loss self: 0.0000; time: 0.13s
Val loss: 0.4525 score: 0.8571 time: 0.07s
Test loss: 0.3297 score: 0.9592 time: 0.08s
Epoch 75/1000, LR 0.000267
Train loss: 0.0466;  Loss pred: 0.0466; Loss self: 0.0000; time: 0.13s
Val loss: 0.4445 score: 0.8571 time: 0.07s
Test loss: 0.3146 score: 0.9592 time: 0.08s
Epoch 76/1000, LR 0.000267
Train loss: 0.0429;  Loss pred: 0.0429; Loss self: 0.0000; time: 0.13s
Val loss: 0.4371 score: 0.8571 time: 0.07s
Test loss: 0.2997 score: 0.9592 time: 0.08s
Epoch 77/1000, LR 0.000267
Train loss: 0.0391;  Loss pred: 0.0391; Loss self: 0.0000; time: 0.13s
Val loss: 0.4303 score: 0.8571 time: 0.08s
Test loss: 0.2852 score: 0.9592 time: 0.08s
Epoch 78/1000, LR 0.000267
Train loss: 0.0360;  Loss pred: 0.0360; Loss self: 0.0000; time: 0.13s
Val loss: 0.4237 score: 0.8571 time: 0.07s
Test loss: 0.2710 score: 0.9592 time: 0.08s
Epoch 79/1000, LR 0.000267
Train loss: 0.0312;  Loss pred: 0.0312; Loss self: 0.0000; time: 0.12s
Val loss: 0.4181 score: 0.8571 time: 0.08s
Test loss: 0.2574 score: 0.9592 time: 0.08s
Epoch 80/1000, LR 0.000267
Train loss: 0.0309;  Loss pred: 0.0309; Loss self: 0.0000; time: 0.13s
Val loss: 0.4128 score: 0.8571 time: 0.07s
Test loss: 0.2443 score: 0.9592 time: 0.08s
Epoch 81/1000, LR 0.000267
Train loss: 0.0269;  Loss pred: 0.0269; Loss self: 0.0000; time: 0.13s
Val loss: 0.4088 score: 0.8571 time: 0.07s
Test loss: 0.2318 score: 0.9592 time: 0.08s
Epoch 82/1000, LR 0.000267
Train loss: 0.0239;  Loss pred: 0.0239; Loss self: 0.0000; time: 0.14s
Val loss: 0.4060 score: 0.8571 time: 0.07s
Test loss: 0.2201 score: 0.9592 time: 0.08s
Epoch 83/1000, LR 0.000266
Train loss: 0.0231;  Loss pred: 0.0231; Loss self: 0.0000; time: 0.13s
Val loss: 0.4039 score: 0.8571 time: 0.07s
Test loss: 0.2089 score: 0.9592 time: 0.08s
Epoch 84/1000, LR 0.000266
Train loss: 0.0223;  Loss pred: 0.0223; Loss self: 0.0000; time: 0.13s
Val loss: 0.4019 score: 0.8571 time: 0.07s
Test loss: 0.1980 score: 0.9592 time: 0.08s
Epoch 85/1000, LR 0.000266
Train loss: 0.0188;  Loss pred: 0.0188; Loss self: 0.0000; time: 0.13s
Val loss: 0.4013 score: 0.8571 time: 0.07s
Test loss: 0.1882 score: 0.9592 time: 0.08s
Epoch 86/1000, LR 0.000266
Train loss: 0.0183;  Loss pred: 0.0183; Loss self: 0.0000; time: 0.13s
Val loss: 0.4018 score: 0.8571 time: 0.07s
Test loss: 0.1790 score: 0.9592 time: 0.08s
     INFO: Early stopping counter 1 of 20
Epoch 87/1000, LR 0.000266
Train loss: 0.0169;  Loss pred: 0.0169; Loss self: 0.0000; time: 0.13s
Val loss: 0.4034 score: 0.8571 time: 0.07s
Test loss: 0.1708 score: 0.9592 time: 0.08s
     INFO: Early stopping counter 2 of 20
Epoch 88/1000, LR 0.000266
Train loss: 0.0155;  Loss pred: 0.0155; Loss self: 0.0000; time: 0.13s
Val loss: 0.4065 score: 0.8571 time: 0.07s
Test loss: 0.1636 score: 0.9592 time: 0.08s
     INFO: Early stopping counter 3 of 20
Epoch 89/1000, LR 0.000266
Train loss: 0.0134;  Loss pred: 0.0134; Loss self: 0.0000; time: 0.13s
Val loss: 0.4105 score: 0.8571 time: 0.07s
Test loss: 0.1573 score: 0.9592 time: 0.08s
     INFO: Early stopping counter 4 of 20
Epoch 90/1000, LR 0.000266
Train loss: 0.0128;  Loss pred: 0.0128; Loss self: 0.0000; time: 0.13s
Val loss: 0.4159 score: 0.8367 time: 0.07s
Test loss: 0.1519 score: 0.9592 time: 0.08s
     INFO: Early stopping counter 5 of 20
Epoch 91/1000, LR 0.000266
Train loss: 0.0125;  Loss pred: 0.0125; Loss self: 0.0000; time: 0.13s
Val loss: 0.4204 score: 0.8367 time: 0.07s
Test loss: 0.1467 score: 0.9592 time: 0.08s
     INFO: Early stopping counter 6 of 20
Epoch 92/1000, LR 0.000266
Train loss: 0.0107;  Loss pred: 0.0107; Loss self: 0.0000; time: 0.13s
Val loss: 0.4264 score: 0.8367 time: 0.07s
Test loss: 0.1426 score: 0.9592 time: 0.08s
     INFO: Early stopping counter 7 of 20
Epoch 93/1000, LR 0.000265
Train loss: 0.0105;  Loss pred: 0.0105; Loss self: 0.0000; time: 0.13s
Val loss: 0.4334 score: 0.8367 time: 0.07s
Test loss: 0.1392 score: 0.9592 time: 0.08s
     INFO: Early stopping counter 8 of 20
Epoch 94/1000, LR 0.000265
Train loss: 0.0107;  Loss pred: 0.0107; Loss self: 0.0000; time: 0.13s
Val loss: 0.4395 score: 0.8367 time: 0.07s
Test loss: 0.1358 score: 0.9592 time: 0.08s
     INFO: Early stopping counter 9 of 20
Epoch 95/1000, LR 0.000265
Train loss: 0.0088;  Loss pred: 0.0088; Loss self: 0.0000; time: 0.13s
Val loss: 0.4455 score: 0.8367 time: 0.07s
Test loss: 0.1329 score: 0.9592 time: 0.08s
     INFO: Early stopping counter 10 of 20
Epoch 96/1000, LR 0.000265
Train loss: 0.0086;  Loss pred: 0.0086; Loss self: 0.0000; time: 0.13s
Val loss: 0.4521 score: 0.8367 time: 0.07s
Test loss: 0.1305 score: 0.9592 time: 0.08s
     INFO: Early stopping counter 11 of 20
Epoch 97/1000, LR 0.000265
Train loss: 0.0095;  Loss pred: 0.0095; Loss self: 0.0000; time: 0.13s
Val loss: 0.4580 score: 0.8367 time: 0.07s
Test loss: 0.1283 score: 0.9592 time: 0.08s
     INFO: Early stopping counter 12 of 20
Epoch 98/1000, LR 0.000265
Train loss: 0.0074;  Loss pred: 0.0074; Loss self: 0.0000; time: 0.13s
Val loss: 0.4637 score: 0.8367 time: 0.07s
Test loss: 0.1264 score: 0.9592 time: 0.08s
     INFO: Early stopping counter 13 of 20
Epoch 99/1000, LR 0.000265
Train loss: 0.0071;  Loss pred: 0.0071; Loss self: 0.0000; time: 0.13s
Val loss: 0.4693 score: 0.8367 time: 0.07s
Test loss: 0.1248 score: 0.9592 time: 0.08s
     INFO: Early stopping counter 14 of 20
Epoch 100/1000, LR 0.000265
Train loss: 0.0066;  Loss pred: 0.0066; Loss self: 0.0000; time: 0.13s
Val loss: 0.4753 score: 0.8367 time: 0.07s
Test loss: 0.1235 score: 0.9592 time: 0.08s
     INFO: Early stopping counter 15 of 20
Epoch 101/1000, LR 0.000265
Train loss: 0.0064;  Loss pred: 0.0064; Loss self: 0.0000; time: 0.13s
Val loss: 0.4805 score: 0.8367 time: 0.07s
Test loss: 0.1222 score: 0.9592 time: 0.09s
     INFO: Early stopping counter 16 of 20
Epoch 102/1000, LR 0.000264
Train loss: 0.0059;  Loss pred: 0.0059; Loss self: 0.0000; time: 0.13s
Val loss: 0.4875 score: 0.8367 time: 0.07s
Test loss: 0.1218 score: 0.9592 time: 0.08s
     INFO: Early stopping counter 17 of 20
Epoch 103/1000, LR 0.000264
Train loss: 0.0053;  Loss pred: 0.0053; Loss self: 0.0000; time: 0.13s
Val loss: 0.4947 score: 0.8367 time: 0.07s
Test loss: 0.1216 score: 0.9592 time: 0.08s
     INFO: Early stopping counter 18 of 20
Epoch 104/1000, LR 0.000264
Train loss: 0.0054;  Loss pred: 0.0054; Loss self: 0.0000; time: 0.13s
Val loss: 0.5009 score: 0.8367 time: 0.08s
Test loss: 0.1212 score: 0.9592 time: 0.08s
     INFO: Early stopping counter 19 of 20
Epoch 105/1000, LR 0.000264
Train loss: 0.0061;  Loss pred: 0.0061; Loss self: 0.0000; time: 0.13s
Val loss: 0.5058 score: 0.8367 time: 0.08s
Test loss: 0.1207 score: 0.9592 time: 0.09s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 084,   Train_Loss: 0.0188,   Val_Loss: 0.4013,   Val_Precision: 1.0000,   Val_Recall: 0.7200,   Val_accuracy: 0.8372,   Val_Score: 0.8571,   Val_Loss: 0.4013,   Test_Precision: 1.0000,   Test_Recall: 0.9167,   Test_accuracy: 0.9565,   Test_Score: 0.9592,   Test_loss: 0.1882


[0.07078393292613328, 0.07182950200513005, 0.07565474603325129, 0.07194918906316161, 0.07211259403266013, 0.07183831394650042, 0.07171307306271046, 0.07754708500579, 0.07189430296421051, 0.07233359490055591, 0.07208557205740362, 0.07247356593143195, 0.07235056802164763, 0.07162254396826029, 0.07129303400870413, 0.07121002895291895, 0.07144724403042346, 0.07110963901504874, 0.07078702899161726, 0.07133766694460064, 0.07188453897833824, 0.07135094807017595, 0.07108811009675264, 0.07125854701735079, 0.07144543598406017, 0.0712470100261271, 0.07794450398068875, 0.07134187500923872, 0.07038487808313221, 0.07021258503664285, 0.06894871802069247, 0.06994629197288305, 0.07000614900607616, 0.07075591094326228, 0.07042106206063181, 0.07219593797344714, 0.06987993896473199, 0.06980699102859944, 0.07032186491414905, 0.06991312198806554, 0.07057835895102471, 0.06988244294188917, 0.07170787593349814, 0.06543997407425195, 0.06565393705386668, 0.06502827198710293, 0.0654224120080471, 0.06565646606031805, 0.065755404997617, 0.06710893998388201, 0.07122395897749811, 0.0705567040713504, 0.0703179620904848, 0.0694255989510566, 0.07010203192476183, 0.0698585860664025, 0.06987044401466846, 0.17841056000906974, 0.0767566820140928, 0.07033698994200677, 0.06991267600096762, 0.07096321997232735, 0.07042018102947623, 0.06967529293615371, 0.06858130893670022, 0.06912373204249889, 0.06948012509383261, 0.06913573492784053, 0.0707393690245226, 0.06919512699823827, 0.06900859496090561, 0.19735902093816549, 0.06876668892800808, 0.06972259900067002, 0.06890193605795503, 0.06926834501791745, 0.06936872692313045, 0.06947234307881445, 0.06907203898299485, 0.1828952939249575, 0.06973210093565285, 0.06988967803772539, 0.06952537305187434, 0.07037475309334695, 0.0705721570411697, 0.07015506504103541, 0.07141511095687747, 0.06892503902781755, 0.07028862601146102, 0.06987591995857656, 0.07757178705651313, 0.07075237692333758, 0.0708232200704515, 0.06944266706705093, 0.0797262160340324, 0.06569136108737439, 0.0661277249455452, 0.06685079296585172, 0.06828834500629455, 0.06611818401142955, 0.07231886603403836, 0.06593887007329613, 0.06615694495849311, 0.0678873200668022, 0.06609812309034169, 0.06723849603440613, 0.06685232697054744, 0.06520597694907337, 0.06793095404282212, 0.06609208206646144, 0.06636362907011062, 0.07037095003761351, 0.07143615407403558, 0.06979516707360744, 0.07024105591699481, 0.18975004798267037, 0.06593798601534218, 0.06600498605985194, 0.06832788500469178, 0.06692010897677392, 0.06735822302289307, 0.06627176003530622, 0.07662019901908934, 0.07063216099049896, 0.07641606102697551, 0.07008099695667624, 0.07711331208702177, 0.0702624199911952, 0.06638188497163355, 0.0777484430000186, 0.08225686301011592, 0.0813671430805698, 0.0879807110177353, 0.08443449100013822, 0.08210603694897145, 0.08145065093412995, 0.08110962202772498, 0.08104204991832376, 0.08090463594999164, 0.08126533695030957, 0.08129644498694688, 0.08076734794303775, 0.21748492994811386, 0.0811425750143826, 0.08154368703253567, 0.08197619009297341, 0.08513267792295665, 0.0860007320297882, 0.08099732000846416, 0.08315175794996321, 0.08085632999427617, 0.08149965608026832, 0.08122440008446574, 0.08486869197804481, 0.08310657297261059, 0.08250652498099953, 0.0838920189999044, 0.0831593960756436, 0.0827195419697091, 0.08316555700730532, 0.08253008895553648, 0.08256676397286355, 0.09579560102429241, 0.08272142196074128, 0.08412286709062755, 0.08697611594107002, 0.08682677301112562, 0.08263353200163692, 0.08273282798472792, 0.08338134596124291, 0.08294459502212703, 0.08309202606324106, 0.0825072550214827, 0.0838011100422591, 0.0837472160346806, 0.08309929596725851, 0.08265027706511319, 0.08342060504946858, 0.08349338895641267, 0.08235226501710713, 0.0834610490128398, 0.08300752495415509, 0.08201168896630406, 0.08364183502271771, 0.08214734401553869, 0.08157297607976943, 0.0822970219887793, 0.08389099105261266, 0.08248105796519667, 0.08351936598774046, 0.08229000400751829, 0.08270961989182979, 0.08263171196449548, 0.08279521705117077, 0.08394973096437752, 0.08404935896396637, 0.08357891789637506, 0.08375328197143972, 0.08373975800350308, 0.08359813794959337, 0.0835980080300942, 0.08364281198009849, 0.08373944903723896, 0.08351292600855231, 0.08289329800754786, 0.08680400101002306, 0.0880004899809137, 0.08450109092518687, 0.08262421598192304, 0.08464426198042929, 0.0894971260568127, 0.08552029402926564, 0.08547102892771363, 0.08451848896220326, 0.08673169603571296, 0.08958920696750283, 0.0813677409896627, 0.08142791700083762, 0.08181442599743605, 0.08158537093549967, 0.08165834494866431, 0.08195646700914949, 0.08221615699585527, 0.0826783679658547, 0.08207469596527517, 0.08245036902371794, 0.08166807901579887, 0.08111916994675994, 0.08121175295673311, 0.09077251690905541, 0.0851200979668647, 0.08270767296198756, 0.08812898199539632, 0.09063484705984592]
[0.0014445700597170057, 0.0014659082041863277, 0.001543974408841863, 0.0014683507972073797, 0.001471685592503268, 0.0014660880397244984, 0.0014635321033206216, 0.0015825935715467346, 0.00146723067273899, 0.0014761958142970593, 0.0014711341236204822, 0.0014790523659475908, 0.0014765422045234212, 0.0014616845707808221, 0.0014549598777286556, 0.0014532658969983459, 0.0014581070210290502, 0.0014512171227560968, 0.001444633244726883, 0.0014558707539714417, 0.0014670314077211886, 0.0014561417973505296, 0.0014507777570765845, 0.0014542560615785876, 0.001458070122123677, 0.0014540206127781042, 0.001590704162871199, 0.0014559566328416066, 0.0014364260833292287, 0.0014329098987069968, 0.0014071166942998463, 0.0014274753463853682, 0.0014286969184913502, 0.0014439981825155566, 0.001437164531849629, 0.0014733864892540233, 0.0014261212033618773, 0.0014246324699714171, 0.0014351401002887561, 0.001426798407919705, 0.001440374672469892, 0.0014261723049365136, 0.001463426039459146, 0.0013355096749847336, 0.0013398762664054425, 0.001327107591573529, 0.001335151265470349, 0.001339927878782001, 0.001341947040767694, 0.001369570203752694, 0.0014535501832142472, 0.001439932736150008, 0.0014350604508262203, 0.0014168489581848286, 0.0014306537127502415, 0.0014256854299265816, 0.001425927428870785, 0.0036410318369197907, 0.0015664628982467919, 0.0014354487743266687, 0.0014267893061421964, 0.0014482289790270888, 0.0014371465516219639, 0.0014219447537990554, 0.001399618549728576, 0.0014106884090305896, 0.0014179617366088288, 0.0014109333658742966, 0.001443660592337196, 0.001412145448943638, 0.0014083386726715431, 0.004027735121187051, 0.0014034018148573078, 0.001422910183687143, 0.0014061619603664291, 0.0014136396942432134, 0.0014156883045536826, 0.001417802919975805, 0.001409633448632548, 0.003732557018876684, 0.0014231041007276093, 0.0014263199599535794, 0.001418885164323966, 0.0014362194508846318, 0.0014402481028810143, 0.0014317360212456208, 0.0014574512440179075, 0.001406633449547297, 0.0014344617553359391, 0.001426039182828093, 0.0015830976950308802, 0.0014439260596599505, 0.0014453718381724795, 0.0014171972870826721, 0.0016270656333476, 0.001340640022191314, 0.0013495454070519429, 0.0013643018972622802, 0.0013936396940060112, 0.001349350694110807, 0.0014758952251844564, 0.0013456912259856354, 0.0013501417338467982, 0.0013854555115673918, 0.0013489412875579937, 0.0013722142047837985, 0.00136433320348056, 0.001330734223450477, 0.0013863460008739208, 0.001348818001356356, 0.001354359776941033, 0.0014361418375023166, 0.0014578806953884813, 0.0014243911647674988, 0.0014334909370815267, 0.0038724499588300076, 0.001345673183986575, 0.0013470405318337132, 0.0013944466327488118, 0.0013657165097300799, 0.0013746576127121035, 0.0013524848986797187, 0.0015636775310018233, 0.001441472673275489, 0.0015595114495301125, 0.0014302244276872703, 0.0015737410630004443, 0.0014339269385958205, 0.0013547323463598685, 0.0015867029183677264, 0.0016787114900023658, 0.001660553940419792, 0.0017955247146476591, 0.0017231528775538414, 0.0016756334071218663, 0.0016622581823291828, 0.0016552984087290813, 0.00165391938608824, 0.0016511150193875845, 0.0016584762642920321, 0.0016591111221825894, 0.001648313223327301, 0.004438467958124773, 0.0016559709186608695, 0.0016641568782150137, 0.0016729834712851718, 0.0017374015902644213, 0.0017551169801997592, 0.0016530065307849829, 0.0016969746520400656, 0.0016501291835566564, 0.0016632582873524148, 0.0016576408180503212, 0.0017320141220009144, 0.001696052509645114, 0.0016838066322652965, 0.001712082020406212, 0.0016971305321559918, 0.0016881539177491653, 0.0016972562654552106, 0.0016842875297048262, 0.001685035999446195, 0.001955012265801886, 0.0016881922849130873, 0.0017167932059311746, 0.0017750227743075515, 0.001771974959410727, 0.0016863986122783046, 0.0016884250609128146, 0.0017016601216580188, 0.001692746837186266, 0.0016957556339436952, 0.0016838215310506675, 0.0017102267355563082, 0.0017091268578506246, 0.0016959039993318064, 0.001686740348267616, 0.001702461327540175, 0.001703946713396177, 0.0016806584697368803, 0.0017032867145477509, 0.0016940311215133692, 0.001673707938087838, 0.0017069762249534227, 0.0016764764084803816, 0.0016647546138728456, 0.0016795310609954961, 0.0017120610418900543, 0.0016832868972489117, 0.0017044768568926624, 0.0016793878368881283, 0.0016879514263638733, 0.001686361468663173, 0.0016896983071667503, 0.0017132598155995412, 0.0017152930400809463, 0.001705692201966838, 0.0017092506524783615, 0.001708974653132716, 0.001706084447950885, 0.0017060817965325347, 0.0017069961628591527, 0.0017089683476987543, 0.0017043454287459655, 0.0016916999593377113, 0.0017715102246943482, 0.0017959283669574224, 0.0017245120596976913, 0.001686208489427001, 0.0017274339179679447, 0.0018264719603431163, 0.0017453121230462376, 0.0017443067128104822, 0.0017248671216776176, 0.0017700346129737338, 0.0018283511626020986, 0.0016605661426461777, 0.0016617942245068904, 0.0016696821632129805, 0.001665007570112238, 0.0016664968356870267, 0.0016725809593703977, 0.0016778807550174545, 0.0016873136319562184, 0.0016749937952096974, 0.0016826605923207743, 0.0016666954901183443, 0.0016554932642195905, 0.0016573827134027165, 0.0018525003450827636, 0.0017371448564666267, 0.0016879116931017867, 0.0017985506529672717, 0.0018496907563233863]
[692.2474914064758, 682.1709552782424, 647.6791287946937, 681.0361678570783, 679.4929603809243, 682.0872777789772, 683.2784861576255, 631.8741703358862, 681.5560897000775, 677.4169052065656, 679.7476749019904, 676.1085834573042, 677.2579862170394, 684.1421329814042, 687.3041760856694, 688.1053233723121, 685.8207151998048, 689.0767648198897, 692.217214057715, 686.874159173896, 681.6486646004047, 686.746305764668, 689.2854505951813, 687.636810614016, 685.8380710411249, 687.748159284595, 628.6524064883403, 686.8336442468682, 696.1722650442849, 697.8805861431774, 710.6731119394333, 700.5374926664654, 699.9385153402319, 692.5216472626873, 695.8145555630999, 678.7085447663503, 701.2026731266898, 701.9354262086019, 696.7960826951988, 700.8698597148115, 694.2638044900104, 701.1775481396094, 683.328007727388, 748.7778027601568, 746.3375724108863, 753.5184082658413, 748.9788055196267, 746.3088244040443, 745.1858900690487, 730.1560717807292, 687.9707433208064, 694.4768841589993, 696.8347566294235, 705.7915342515638, 698.9811658040106, 701.4170019619945, 701.2979621213375, 274.6474199593848, 638.3809033199666, 696.6462460278833, 700.8743307053762, 690.4985430355038, 695.8232609412031, 703.2622029289591, 714.4803848119383, 708.8737623407494, 705.2376479435768, 708.7506924044862, 692.6835887243155, 708.1423522966807, 710.0564795987981, 248.27849148761322, 712.5543015644995, 702.7850467755674, 711.1556336934416, 707.3938317325945, 706.3701782259657, 705.3166458544641, 709.4042788003337, 267.9128530234619, 702.6892828772799, 701.104961072371, 704.7786707083193, 696.2724250698982, 694.3248166754327, 698.4527770209992, 686.1292987360564, 710.9172615807156, 697.1255917281728, 701.2430037278635, 631.6729555850271, 692.5562381189403, 691.8634870210248, 705.6180597540651, 614.6033568065436, 745.9123877008175, 740.9902584785804, 732.9755987341819, 717.5455781727229, 741.0971842712679, 677.5548717389614, 743.1125214237479, 740.6629799901222, 721.784273584275, 741.322108844569, 728.7491971106339, 732.9587797532839, 751.4648547980436, 721.3206510998141, 741.3898680136323, 738.3562455307166, 696.3100537055324, 685.9271840028927, 702.0543406440102, 697.5977134783428, 258.234453803538, 743.1224846418404, 742.3681592109999, 717.1303487095405, 732.2163808341453, 727.4538697873064, 739.3797897308793, 639.5180465113648, 693.7349687855537, 641.2264560810402, 699.1909665653241, 635.4285488957327, 697.3856010957257, 738.1531877399804, 630.2377013516307, 595.6949755544896, 602.2086820902654, 556.9402592134371, 580.331561422213, 596.7892474271202, 601.5912633973527, 604.1206798282301, 604.6243900466911, 605.6513254727162, 602.9631062744685, 602.7323827981351, 606.6808091130825, 225.30296702254313, 603.8753390721788, 600.9048864867879, 597.7345366310216, 575.5721680027968, 569.7625920559351, 604.9582874455542, 589.2839936046316, 606.0131594331417, 601.2295309779015, 603.2669979592906, 577.3624979712908, 589.6043868413262, 593.8924225845682, 584.0841665767496, 589.229868329353, 592.3630478749895, 589.1862179880044, 593.7228545385309, 593.459131038542, 511.50574218511656, 592.349585374087, 582.48133586806, 563.3730532782076, 564.3420606420711, 592.9796150917201, 592.2679206498921, 587.6614179720251, 590.75579291126, 589.7076087987825, 593.8871677071513, 584.7177916293753, 585.0940761984086, 589.6560184975119, 592.8594765798188, 587.3848549880801, 586.87281247597, 595.0048852915118, 587.1002171619213, 590.307927227834, 597.4758064077001, 585.8312408699696, 596.4891572237726, 600.6891295970785, 595.4042906519855, 584.091323575727, 594.0757939923105, 586.690277404555, 595.4550688261382, 592.4341094069073, 592.992675996522, 591.8216262385789, 583.6826328936328, 582.9907640462466, 586.2722470366562, 585.051700024236, 585.1461858529634, 586.1374571470146, 586.138368062079, 585.8243982956814, 585.1483448166668, 586.7355191815702, 591.1213714230347, 564.4901090946497, 556.8150814913365, 579.8741704220387, 593.046474543498, 578.8933455563633, 547.5036144612603, 572.9634182879663, 573.2936717240332, 579.7548039685475, 564.9607034067866, 546.9408833786645, 602.2042569207515, 601.7592222025764, 598.9163818314339, 600.5978699139428, 600.0611453832986, 597.8783833438027, 595.9899098965452, 592.6580459381647, 597.0171369350101, 594.296915589359, 599.9896237368439, 604.0495733888752, 603.3609448881808, 539.8109655711419, 575.6572321976718, 592.4480552429567, 556.0032453632525, 540.6309117247745]
Elapsed: 0.07863502872264029~0.018468467317076836
Time per graph: 0.0016047965045436793~0.00037690749626687417
Speed: 640.9038417887282~82.96197955445687
Total Time: 0.0915
best val loss: 0.40128669142723083 test_score: 0.9592

Testing...
Test loss: 0.5691 score: 0.9796 time: 0.08s
test Score 0.9796
Epoch Time List: [0.4344634370645508, 0.2635430139489472, 0.2607021938310936, 0.2612041379325092, 0.25651010393630713, 0.2584334989078343, 0.2577688579913229, 0.26812492311000824, 0.26772345695644617, 0.26062874705530703, 0.26006776408758014, 0.25733603595290333, 0.2611486701061949, 0.2559775010449812, 0.26052997494116426, 0.2592564520891756, 0.26425005099736154, 0.2575646300101653, 0.25313281978014857, 0.25991963408887386, 0.2557188560022041, 0.2585642950143665, 0.2577043240889907, 0.2574328950140625, 0.2569704339839518, 0.2612698870943859, 0.2687719070818275, 0.2582835239591077, 0.25436484697274864, 0.2482264790451154, 0.24814224790316075, 0.24753734900150448, 0.25341243483126163, 0.25052620586939156, 0.2548666420625523, 0.2540812570368871, 0.3867917510215193, 0.2527400868711993, 0.2487003490095958, 0.25279088702518493, 0.24963115295395255, 0.2505265739746392, 0.25388738699257374, 0.36178546003066003, 0.23628104489762336, 0.23987887997645885, 0.24056445399764925, 0.23666120308917016, 0.2413169019855559, 0.24418613000307232, 0.36085116281174123, 0.25077103497460485, 0.25516509101726115, 0.25512102397624403, 0.2560048389714211, 0.25191611202899367, 0.2509110620012507, 0.36683513585012406, 0.2576390679460019, 0.2500923709012568, 0.2514547168975696, 0.25222785701043904, 0.2509378530085087, 0.2563305289950222, 0.2496927878819406, 0.371875366079621, 0.2491267540026456, 0.24852963001467288, 0.2515180470654741, 0.2539589679799974, 0.2493374350015074, 0.38316226098686457, 0.25281407788861543, 0.24813696695491672, 0.2481856788508594, 0.24904972407966852, 0.2562789289513603, 0.25462263193912804, 0.24897942191455513, 0.36303250899072737, 0.24885577789973468, 0.25247344095259905, 0.24862695415504277, 0.2607372240163386, 0.25188332702964544, 0.25121795292943716, 0.2558600160991773, 0.3614651779644191, 0.24957317113876343, 0.24941887706518173, 0.2584806131199002, 0.2567017601104453, 0.258104367996566, 0.2510665850713849, 0.3807046910515055, 0.24510983505751938, 0.24535710492637008, 0.2484397990629077, 0.24521608091890812, 0.2507089850259945, 0.24745711602736264, 0.30859925400000066, 0.2457962220069021, 0.25176771311089396, 0.24575713800732046, 0.24402898899279535, 0.24620702001266181, 0.24044316005893052, 0.23992235015612096, 0.36147376406006515, 0.23914747592061758, 0.24348997103516012, 0.24643740500323474, 0.2566716820001602, 0.2594536499818787, 0.36758032999932766, 0.24151650886051357, 0.2487489429768175, 0.24128121905960143, 0.25018550804816186, 0.247679433086887, 0.2431446418631822, 0.2489456810289994, 0.3647894309833646, 0.2576233949512243, 0.25503387907519937, 0.26108589000068605, 0.2509082049364224, 0.23513870302122086, 0.25978443503845483, 0.2639840889023617, 0.2617574839387089, 0.26879993500187993, 0.2671267680125311, 0.26280294090975076, 0.33579108491539955, 0.262219715048559, 0.2603959811385721, 0.2602888628607616, 0.26134088495746255, 0.26211766875348985, 0.2593358941376209, 0.3956867150263861, 0.2586636820342392, 0.2620860901661217, 0.262939588050358, 0.272687510936521, 0.28846510499715805, 0.25992246996611357, 0.4005793429678306, 0.27014768321532756, 0.2703485789243132, 0.27078946703113616, 0.2748897769488394, 0.27495166496373713, 0.2753325029043481, 0.27589497389271855, 0.27494268701411784, 0.2741439709207043, 0.2743687150068581, 0.27334992913529277, 0.27402060106396675, 0.29521891102194786, 0.2836101178545505, 0.29597970889881253, 0.2949746469967067, 0.2864448019536212, 0.2751444400055334, 0.27420735906343907, 0.27508999698329717, 0.2750572159420699, 0.2746710239443928, 0.2740678610280156, 0.27663135796319693, 0.27627473790198565, 0.27580651186872274, 0.2737393800634891, 0.2741827151039615, 0.2763041788712144, 0.2721818630816415, 0.2726752811577171, 0.27324509993195534, 0.27119620493613183, 0.27345486998092383, 0.2749486080138013, 0.2719658430432901, 0.27207272208761424, 0.27498776698485017, 0.27317474293522537, 0.27306724491063505, 0.27283802500460297, 0.2740346089703962, 0.2749903139192611, 0.2737198161194101, 0.2757420300040394, 0.2797987200319767, 0.27475354401394725, 0.27759530406910926, 0.2754404480801895, 0.27677215496078134, 0.2777787080267444, 0.2779627490090206, 0.2778215939179063, 0.27714634500443935, 0.27587385301012546, 0.29194517491851, 0.27961763506755233, 0.28267076204065233, 0.27234432904515415, 0.2785541098564863, 0.29498133703600615, 0.2846967341611162, 0.27916241507045925, 0.2780737349530682, 0.2835931369336322, 0.2789733640383929, 0.27193559205625206, 0.2703952870797366, 0.2721247678855434, 0.26972794090397656, 0.27297015907242894, 0.27304073877166957, 0.27168729004915804, 0.27623095689341426, 0.2729206170188263, 0.27423350000754, 0.27183183608576655, 0.2704140320420265, 0.27185989706777036, 0.28160026809200644, 0.28049816691782326, 0.2787392019527033, 0.2902920679189265, 0.3002983769401908]
Total Epoch List: [129, 105]
Total Time List: [0.06697597703896463, 0.09154098795261234]
========================k_idx:2========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: I2BGNNA
I2BGNN(
  (gcs): ModuleList(
    (0): GCNConv(14887, 64)
    (1): GCNConv(64, 64)
  )
  (bns): ModuleList(
    (0-1): 2 x BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  )
  (drops): ModuleList(
    (0-1): 2 x Dropout(p=0.2, inplace=False)
  )
  (lin1): Linear(in_features=64, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=64, bias=True)
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=64, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=64, out_features=2, bias=True)
  )
)
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7d1ecd6a4520>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.6980;  Loss pred: 0.6980; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6952 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6942 score: 0.5000 time: 0.07s
Epoch 2/1000, LR 0.000000
Train loss: 0.6954;  Loss pred: 0.6954; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6951 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6942 score: 0.5000 time: 0.07s
Epoch 3/1000, LR 0.000030
Train loss: 0.6927;  Loss pred: 0.6927; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6950 score: 0.4898 time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6941 score: 0.5000 time: 0.07s
Epoch 4/1000, LR 0.000060
Train loss: 0.6932;  Loss pred: 0.6932; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6949 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6941 score: 0.5000 time: 0.07s
Epoch 5/1000, LR 0.000090
Train loss: 0.6904;  Loss pred: 0.6904; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6949 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6940 score: 0.5000 time: 0.07s
Epoch 6/1000, LR 0.000120
Train loss: 0.6889;  Loss pred: 0.6889; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6948 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6940 score: 0.5000 time: 0.07s
Epoch 7/1000, LR 0.000150
Train loss: 0.6842;  Loss pred: 0.6842; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6947 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6939 score: 0.5000 time: 0.08s
Epoch 8/1000, LR 0.000180
Train loss: 0.6836;  Loss pred: 0.6836; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6947 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6939 score: 0.5000 time: 0.07s
Epoch 9/1000, LR 0.000210
Train loss: 0.6791;  Loss pred: 0.6791; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6946 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6938 score: 0.5000 time: 0.07s
Epoch 10/1000, LR 0.000240
Train loss: 0.6756;  Loss pred: 0.6756; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6945 score: 0.4898 time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6937 score: 0.5000 time: 0.07s
Epoch 11/1000, LR 0.000270
Train loss: 0.6676;  Loss pred: 0.6676; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6945 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6937 score: 0.5000 time: 0.07s
Epoch 12/1000, LR 0.000270
Train loss: 0.6605;  Loss pred: 0.6605; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6944 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6936 score: 0.5000 time: 0.07s
Epoch 13/1000, LR 0.000270
Train loss: 0.6554;  Loss pred: 0.6554; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6943 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6935 score: 0.5000 time: 0.07s
Epoch 14/1000, LR 0.000270
Train loss: 0.6508;  Loss pred: 0.6508; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6942 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6935 score: 0.5000 time: 0.07s
Epoch 15/1000, LR 0.000270
Train loss: 0.6439;  Loss pred: 0.6439; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6941 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6934 score: 0.5000 time: 0.07s
Epoch 16/1000, LR 0.000270
Train loss: 0.6383;  Loss pred: 0.6383; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6940 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6932 score: 0.5000 time: 0.07s
Epoch 17/1000, LR 0.000270
Train loss: 0.6282;  Loss pred: 0.6282; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6939 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6931 score: 0.5000 time: 0.20s
Epoch 18/1000, LR 0.000270
Train loss: 0.6203;  Loss pred: 0.6203; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6937 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6929 score: 0.5000 time: 0.07s
Epoch 19/1000, LR 0.000270
Train loss: 0.6148;  Loss pred: 0.6148; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6935 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6928 score: 0.5000 time: 0.07s
Epoch 20/1000, LR 0.000270
Train loss: 0.6027;  Loss pred: 0.6027; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6933 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6926 score: 0.5000 time: 0.07s
Epoch 21/1000, LR 0.000270
Train loss: 0.5935;  Loss pred: 0.5935; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6931 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6923 score: 0.5000 time: 0.07s
Epoch 22/1000, LR 0.000270
Train loss: 0.5833;  Loss pred: 0.5833; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6928 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6920 score: 0.5000 time: 0.07s
Epoch 23/1000, LR 0.000270
Train loss: 0.5752;  Loss pred: 0.5752; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6924 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6916 score: 0.5000 time: 0.08s
Epoch 24/1000, LR 0.000270
Train loss: 0.5621;  Loss pred: 0.5621; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6920 score: 0.4898 time: 0.09s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6912 score: 0.5000 time: 0.08s
Epoch 25/1000, LR 0.000270
Train loss: 0.5507;  Loss pred: 0.5507; Loss self: 0.0000; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6915 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6906 score: 0.5000 time: 0.07s
Epoch 26/1000, LR 0.000270
Train loss: 0.5397;  Loss pred: 0.5397; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6909 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6900 score: 0.5000 time: 0.07s
Epoch 27/1000, LR 0.000270
Train loss: 0.5284;  Loss pred: 0.5284; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6903 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6894 score: 0.5000 time: 0.07s
Epoch 28/1000, LR 0.000270
Train loss: 0.5170;  Loss pred: 0.5170; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6897 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6887 score: 0.5000 time: 0.07s
Epoch 29/1000, LR 0.000270
Train loss: 0.5017;  Loss pred: 0.5017; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6890 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6879 score: 0.5000 time: 0.07s
Epoch 30/1000, LR 0.000270
Train loss: 0.4875;  Loss pred: 0.4875; Loss self: 0.0000; time: 0.12s
Val loss: 0.6882 score: 0.5306 time: 0.07s
Test loss: 0.6871 score: 0.5417 time: 0.07s
Epoch 31/1000, LR 0.000270
Train loss: 0.4730;  Loss pred: 0.4730; Loss self: 0.0000; time: 0.12s
Val loss: 0.6873 score: 0.7959 time: 0.07s
Test loss: 0.6861 score: 0.8333 time: 0.07s
Epoch 32/1000, LR 0.000270
Train loss: 0.4643;  Loss pred: 0.4643; Loss self: 0.0000; time: 0.19s
Val loss: 0.6864 score: 0.8571 time: 0.07s
Test loss: 0.6850 score: 0.9583 time: 0.07s
Epoch 33/1000, LR 0.000270
Train loss: 0.4459;  Loss pred: 0.4459; Loss self: 0.0000; time: 0.12s
Val loss: 0.6852 score: 0.8776 time: 0.07s
Test loss: 0.6838 score: 0.9167 time: 0.07s
Epoch 34/1000, LR 0.000270
Train loss: 0.4333;  Loss pred: 0.4333; Loss self: 0.0000; time: 0.12s
Val loss: 0.6839 score: 0.8163 time: 0.07s
Test loss: 0.6823 score: 0.8750 time: 0.07s
Epoch 35/1000, LR 0.000270
Train loss: 0.4209;  Loss pred: 0.4209; Loss self: 0.0000; time: 0.12s
Val loss: 0.6825 score: 0.7755 time: 0.07s
Test loss: 0.6807 score: 0.8750 time: 0.07s
Epoch 36/1000, LR 0.000270
Train loss: 0.4069;  Loss pred: 0.4069; Loss self: 0.0000; time: 0.11s
Val loss: 0.6810 score: 0.7143 time: 0.07s
Test loss: 0.6789 score: 0.8542 time: 0.07s
Epoch 37/1000, LR 0.000270
Train loss: 0.3944;  Loss pred: 0.3944; Loss self: 0.0000; time: 0.11s
Val loss: 0.6793 score: 0.7143 time: 0.07s
Test loss: 0.6770 score: 0.8542 time: 0.07s
Epoch 38/1000, LR 0.000270
Train loss: 0.3736;  Loss pred: 0.3736; Loss self: 0.0000; time: 0.11s
Val loss: 0.6774 score: 0.7143 time: 0.07s
Test loss: 0.6748 score: 0.8542 time: 0.07s
Epoch 39/1000, LR 0.000269
Train loss: 0.3648;  Loss pred: 0.3648; Loss self: 0.0000; time: 0.12s
Val loss: 0.6754 score: 0.7143 time: 0.17s
Test loss: 0.6725 score: 0.8542 time: 0.07s
Epoch 40/1000, LR 0.000269
Train loss: 0.3498;  Loss pred: 0.3498; Loss self: 0.0000; time: 0.12s
Val loss: 0.6732 score: 0.7143 time: 0.07s
Test loss: 0.6700 score: 0.8542 time: 0.07s
Epoch 41/1000, LR 0.000269
Train loss: 0.3363;  Loss pred: 0.3363; Loss self: 0.0000; time: 0.12s
Val loss: 0.6709 score: 0.7143 time: 0.07s
Test loss: 0.6672 score: 0.8542 time: 0.07s
Epoch 42/1000, LR 0.000269
Train loss: 0.3190;  Loss pred: 0.3190; Loss self: 0.0000; time: 0.12s
Val loss: 0.6682 score: 0.7143 time: 0.07s
Test loss: 0.6641 score: 0.8542 time: 0.07s
Epoch 43/1000, LR 0.000269
Train loss: 0.3072;  Loss pred: 0.3072; Loss self: 0.0000; time: 0.12s
Val loss: 0.6653 score: 0.7551 time: 0.07s
Test loss: 0.6607 score: 0.8542 time: 0.07s
Epoch 44/1000, LR 0.000269
Train loss: 0.2978;  Loss pred: 0.2978; Loss self: 0.0000; time: 0.12s
Val loss: 0.6621 score: 0.7755 time: 0.07s
Test loss: 0.6569 score: 0.8542 time: 0.07s
Epoch 45/1000, LR 0.000269
Train loss: 0.2813;  Loss pred: 0.2813; Loss self: 0.0000; time: 0.12s
Val loss: 0.6586 score: 0.7755 time: 0.07s
Test loss: 0.6528 score: 0.8750 time: 0.07s
Epoch 46/1000, LR 0.000269
Train loss: 0.2735;  Loss pred: 0.2735; Loss self: 0.0000; time: 0.13s
Val loss: 0.6547 score: 0.7755 time: 0.15s
Test loss: 0.6482 score: 0.8750 time: 0.07s
Epoch 47/1000, LR 0.000269
Train loss: 0.2558;  Loss pred: 0.2558; Loss self: 0.0000; time: 0.12s
Val loss: 0.6504 score: 0.8367 time: 0.07s
Test loss: 0.6431 score: 0.8750 time: 0.07s
Epoch 48/1000, LR 0.000269
Train loss: 0.2404;  Loss pred: 0.2404; Loss self: 0.0000; time: 0.12s
Val loss: 0.6458 score: 0.8367 time: 0.07s
Test loss: 0.6376 score: 0.8750 time: 0.07s
Epoch 49/1000, LR 0.000269
Train loss: 0.2281;  Loss pred: 0.2281; Loss self: 0.0000; time: 0.12s
Val loss: 0.6408 score: 0.8367 time: 0.07s
Test loss: 0.6317 score: 0.8750 time: 0.07s
Epoch 50/1000, LR 0.000269
Train loss: 0.2217;  Loss pred: 0.2217; Loss self: 0.0000; time: 0.12s
Val loss: 0.6353 score: 0.8367 time: 0.07s
Test loss: 0.6251 score: 0.8958 time: 0.07s
Epoch 51/1000, LR 0.000269
Train loss: 0.2039;  Loss pred: 0.2039; Loss self: 0.0000; time: 0.12s
Val loss: 0.6295 score: 0.8163 time: 0.07s
Test loss: 0.6181 score: 0.9167 time: 0.07s
Epoch 52/1000, LR 0.000269
Train loss: 0.1927;  Loss pred: 0.1927; Loss self: 0.0000; time: 0.12s
Val loss: 0.6231 score: 0.8367 time: 0.07s
Test loss: 0.6106 score: 0.9167 time: 0.07s
Epoch 53/1000, LR 0.000269
Train loss: 0.1852;  Loss pred: 0.1852; Loss self: 0.0000; time: 0.12s
Val loss: 0.6162 score: 0.8571 time: 0.07s
Test loss: 0.6024 score: 0.9167 time: 0.21s
Epoch 54/1000, LR 0.000269
Train loss: 0.1724;  Loss pred: 0.1724; Loss self: 0.0000; time: 0.12s
Val loss: 0.6089 score: 0.8571 time: 0.07s
Test loss: 0.5937 score: 0.9375 time: 0.07s
Epoch 55/1000, LR 0.000269
Train loss: 0.1647;  Loss pred: 0.1647; Loss self: 0.0000; time: 0.12s
Val loss: 0.6012 score: 0.8776 time: 0.07s
Test loss: 0.5845 score: 0.9167 time: 0.07s
Epoch 56/1000, LR 0.000269
Train loss: 0.1501;  Loss pred: 0.1501; Loss self: 0.0000; time: 0.12s
Val loss: 0.5930 score: 0.8776 time: 0.07s
Test loss: 0.5747 score: 0.9167 time: 0.07s
Epoch 57/1000, LR 0.000269
Train loss: 0.1431;  Loss pred: 0.1431; Loss self: 0.0000; time: 0.12s
Val loss: 0.5844 score: 0.8776 time: 0.07s
Test loss: 0.5644 score: 0.9375 time: 0.07s
Epoch 58/1000, LR 0.000269
Train loss: 0.1298;  Loss pred: 0.1298; Loss self: 0.0000; time: 0.12s
Val loss: 0.5755 score: 0.8776 time: 0.07s
Test loss: 0.5537 score: 0.9375 time: 0.07s
Epoch 59/1000, LR 0.000268
Train loss: 0.1236;  Loss pred: 0.1236; Loss self: 0.0000; time: 0.12s
Val loss: 0.5661 score: 0.8776 time: 0.07s
Test loss: 0.5423 score: 0.9375 time: 0.07s
Epoch 60/1000, LR 0.000268
Train loss: 0.1175;  Loss pred: 0.1175; Loss self: 0.0000; time: 0.12s
Val loss: 0.5565 score: 0.8776 time: 0.07s
Test loss: 0.5306 score: 0.9375 time: 0.07s
Epoch 61/1000, LR 0.000268
Train loss: 0.1041;  Loss pred: 0.1041; Loss self: 0.0000; time: 0.12s
Val loss: 0.5467 score: 0.8776 time: 0.20s
Test loss: 0.5187 score: 0.9375 time: 0.07s
Epoch 62/1000, LR 0.000268
Train loss: 0.0996;  Loss pred: 0.0996; Loss self: 0.0000; time: 0.12s
Val loss: 0.5367 score: 0.8571 time: 0.07s
Test loss: 0.5064 score: 0.9375 time: 0.07s
Epoch 63/1000, LR 0.000268
Train loss: 0.0924;  Loss pred: 0.0924; Loss self: 0.0000; time: 0.11s
Val loss: 0.5265 score: 0.8571 time: 0.07s
Test loss: 0.4937 score: 0.9583 time: 0.07s
Epoch 64/1000, LR 0.000268
Train loss: 0.0856;  Loss pred: 0.0856; Loss self: 0.0000; time: 0.12s
Val loss: 0.5161 score: 0.8776 time: 0.07s
Test loss: 0.4809 score: 0.9583 time: 0.07s
Epoch 65/1000, LR 0.000268
Train loss: 0.0772;  Loss pred: 0.0772; Loss self: 0.0000; time: 0.12s
Val loss: 0.5055 score: 0.8776 time: 0.07s
Test loss: 0.4678 score: 0.9583 time: 0.07s
Epoch 66/1000, LR 0.000268
Train loss: 0.0737;  Loss pred: 0.0737; Loss self: 0.0000; time: 0.12s
Val loss: 0.4948 score: 0.8980 time: 0.07s
Test loss: 0.4545 score: 0.9583 time: 0.07s
Epoch 67/1000, LR 0.000268
Train loss: 0.0670;  Loss pred: 0.0670; Loss self: 0.0000; time: 0.12s
Val loss: 0.4841 score: 0.8980 time: 0.07s
Test loss: 0.4410 score: 0.9583 time: 0.07s
Epoch 68/1000, LR 0.000268
Train loss: 0.0617;  Loss pred: 0.0617; Loss self: 0.0000; time: 0.12s
Val loss: 0.4736 score: 0.8980 time: 0.07s
Test loss: 0.4277 score: 0.9583 time: 0.20s
Epoch 69/1000, LR 0.000268
Train loss: 0.0570;  Loss pred: 0.0570; Loss self: 0.0000; time: 0.12s
Val loss: 0.4632 score: 0.8980 time: 0.07s
Test loss: 0.4143 score: 0.9583 time: 0.07s
Epoch 70/1000, LR 0.000268
Train loss: 0.0517;  Loss pred: 0.0517; Loss self: 0.0000; time: 0.12s
Val loss: 0.4530 score: 0.8980 time: 0.07s
Test loss: 0.4010 score: 0.9583 time: 0.07s
Epoch 71/1000, LR 0.000268
Train loss: 0.0481;  Loss pred: 0.0481; Loss self: 0.0000; time: 0.12s
Val loss: 0.4431 score: 0.8980 time: 0.07s
Test loss: 0.3879 score: 0.9583 time: 0.07s
Epoch 72/1000, LR 0.000267
Train loss: 0.0442;  Loss pred: 0.0442; Loss self: 0.0000; time: 0.12s
Val loss: 0.4336 score: 0.8980 time: 0.07s
Test loss: 0.3751 score: 0.9583 time: 0.07s
Epoch 73/1000, LR 0.000267
Train loss: 0.0400;  Loss pred: 0.0400; Loss self: 0.0000; time: 0.12s
Val loss: 0.4243 score: 0.8980 time: 0.07s
Test loss: 0.3624 score: 0.9167 time: 0.07s
Epoch 74/1000, LR 0.000267
Train loss: 0.0374;  Loss pred: 0.0374; Loss self: 0.0000; time: 0.12s
Val loss: 0.4157 score: 0.8980 time: 0.07s
Test loss: 0.3503 score: 0.9167 time: 0.07s
Epoch 75/1000, LR 0.000267
Train loss: 0.0335;  Loss pred: 0.0335; Loss self: 0.0000; time: 0.23s
Val loss: 0.4077 score: 0.8980 time: 0.07s
Test loss: 0.3388 score: 0.9167 time: 0.07s
Epoch 76/1000, LR 0.000267
Train loss: 0.0317;  Loss pred: 0.0317; Loss self: 0.0000; time: 0.12s
Val loss: 0.4004 score: 0.8980 time: 0.07s
Test loss: 0.3279 score: 0.9167 time: 0.07s
Epoch 77/1000, LR 0.000267
Train loss: 0.0292;  Loss pred: 0.0292; Loss self: 0.0000; time: 0.12s
Val loss: 0.3937 score: 0.8980 time: 0.07s
Test loss: 0.3176 score: 0.9167 time: 0.07s
Epoch 78/1000, LR 0.000267
Train loss: 0.0259;  Loss pred: 0.0259; Loss self: 0.0000; time: 0.12s
Val loss: 0.3877 score: 0.8980 time: 0.07s
Test loss: 0.3081 score: 0.9167 time: 0.07s
Epoch 79/1000, LR 0.000267
Train loss: 0.0243;  Loss pred: 0.0243; Loss self: 0.0000; time: 0.12s
Val loss: 0.3826 score: 0.8980 time: 0.07s
Test loss: 0.2994 score: 0.9167 time: 0.07s
Epoch 80/1000, LR 0.000267
Train loss: 0.0220;  Loss pred: 0.0220; Loss self: 0.0000; time: 0.12s
Val loss: 0.3784 score: 0.8980 time: 0.07s
Test loss: 0.2914 score: 0.9167 time: 0.07s
Epoch 81/1000, LR 0.000267
Train loss: 0.0199;  Loss pred: 0.0199; Loss self: 0.0000; time: 0.12s
Val loss: 0.3751 score: 0.8980 time: 0.07s
Test loss: 0.2843 score: 0.9167 time: 0.07s
Epoch 82/1000, LR 0.000267
Train loss: 0.0181;  Loss pred: 0.0181; Loss self: 0.0000; time: 0.26s
Val loss: 0.3724 score: 0.8980 time: 0.07s
Test loss: 0.2777 score: 0.9167 time: 0.07s
Epoch 83/1000, LR 0.000266
Train loss: 0.0170;  Loss pred: 0.0170; Loss self: 0.0000; time: 0.12s
Val loss: 0.3705 score: 0.8980 time: 0.07s
Test loss: 0.2720 score: 0.9167 time: 0.07s
Epoch 84/1000, LR 0.000266
Train loss: 0.0157;  Loss pred: 0.0157; Loss self: 0.0000; time: 0.12s
Val loss: 0.3690 score: 0.8980 time: 0.07s
Test loss: 0.2669 score: 0.9167 time: 0.07s
Epoch 85/1000, LR 0.000266
Train loss: 0.0143;  Loss pred: 0.0143; Loss self: 0.0000; time: 0.12s
Val loss: 0.3681 score: 0.8980 time: 0.07s
Test loss: 0.2625 score: 0.9167 time: 0.07s
Epoch 86/1000, LR 0.000266
Train loss: 0.0135;  Loss pred: 0.0135; Loss self: 0.0000; time: 0.12s
Val loss: 0.3679 score: 0.8980 time: 0.07s
Test loss: 0.2585 score: 0.9167 time: 0.07s
Epoch 87/1000, LR 0.000266
Train loss: 0.0131;  Loss pred: 0.0131; Loss self: 0.0000; time: 0.12s
Val loss: 0.3685 score: 0.8980 time: 0.07s
Test loss: 0.2554 score: 0.9167 time: 0.07s
     INFO: Early stopping counter 1 of 20
Epoch 88/1000, LR 0.000266
Train loss: 0.0121;  Loss pred: 0.0121; Loss self: 0.0000; time: 0.13s
Val loss: 0.3698 score: 0.8980 time: 0.15s
Test loss: 0.2529 score: 0.9167 time: 0.07s
     INFO: Early stopping counter 2 of 20
Epoch 89/1000, LR 0.000266
Train loss: 0.0110;  Loss pred: 0.0110; Loss self: 0.0000; time: 0.12s
Val loss: 0.3722 score: 0.8980 time: 0.07s
Test loss: 0.2515 score: 0.9167 time: 0.07s
     INFO: Early stopping counter 3 of 20
Epoch 90/1000, LR 0.000266
Train loss: 0.0103;  Loss pred: 0.0103; Loss self: 0.0000; time: 0.12s
Val loss: 0.3752 score: 0.8980 time: 0.07s
Test loss: 0.2508 score: 0.9167 time: 0.07s
     INFO: Early stopping counter 4 of 20
Epoch 91/1000, LR 0.000266
Train loss: 0.0100;  Loss pred: 0.0100; Loss self: 0.0000; time: 0.12s
Val loss: 0.3788 score: 0.8980 time: 0.07s
Test loss: 0.2506 score: 0.9167 time: 0.07s
     INFO: Early stopping counter 5 of 20
Epoch 92/1000, LR 0.000266
Train loss: 0.0095;  Loss pred: 0.0095; Loss self: 0.0000; time: 0.12s
Val loss: 0.3829 score: 0.8980 time: 0.07s
Test loss: 0.2509 score: 0.9167 time: 0.07s
     INFO: Early stopping counter 6 of 20
Epoch 93/1000, LR 0.000265
Train loss: 0.0084;  Loss pred: 0.0084; Loss self: 0.0000; time: 0.12s
Val loss: 0.3878 score: 0.8980 time: 0.07s
Test loss: 0.2518 score: 0.9167 time: 0.07s
     INFO: Early stopping counter 7 of 20
Epoch 94/1000, LR 0.000265
Train loss: 0.0077;  Loss pred: 0.0077; Loss self: 0.0000; time: 0.12s
Val loss: 0.3930 score: 0.8980 time: 0.07s
Test loss: 0.2532 score: 0.9167 time: 0.07s
     INFO: Early stopping counter 8 of 20
Epoch 95/1000, LR 0.000265
Train loss: 0.0075;  Loss pred: 0.0075; Loss self: 0.0000; time: 0.12s
Val loss: 0.3985 score: 0.8980 time: 0.07s
Test loss: 0.2549 score: 0.9167 time: 0.07s
     INFO: Early stopping counter 9 of 20
Epoch 96/1000, LR 0.000265
Train loss: 0.0074;  Loss pred: 0.0074; Loss self: 0.0000; time: 0.17s
Val loss: 0.4044 score: 0.8980 time: 0.07s
Test loss: 0.2571 score: 0.9167 time: 0.07s
     INFO: Early stopping counter 10 of 20
Epoch 97/1000, LR 0.000265
Train loss: 0.0068;  Loss pred: 0.0068; Loss self: 0.0000; time: 0.12s
Val loss: 0.4107 score: 0.8980 time: 0.07s
Test loss: 0.2596 score: 0.9167 time: 0.07s
     INFO: Early stopping counter 11 of 20
Epoch 98/1000, LR 0.000265
Train loss: 0.0066;  Loss pred: 0.0066; Loss self: 0.0000; time: 0.12s
Val loss: 0.4174 score: 0.8980 time: 0.07s
Test loss: 0.2624 score: 0.9167 time: 0.07s
     INFO: Early stopping counter 12 of 20
Epoch 99/1000, LR 0.000265
Train loss: 0.0062;  Loss pred: 0.0062; Loss self: 0.0000; time: 0.12s
Val loss: 0.4244 score: 0.8980 time: 0.07s
Test loss: 0.2657 score: 0.9167 time: 0.07s
     INFO: Early stopping counter 13 of 20
Epoch 100/1000, LR 0.000265
Train loss: 0.0057;  Loss pred: 0.0057; Loss self: 0.0000; time: 0.13s
Val loss: 0.4314 score: 0.8980 time: 0.07s
Test loss: 0.2690 score: 0.9167 time: 0.07s
     INFO: Early stopping counter 14 of 20
Epoch 101/1000, LR 0.000265
Train loss: 0.0053;  Loss pred: 0.0053; Loss self: 0.0000; time: 0.12s
Val loss: 0.4387 score: 0.8980 time: 0.07s
Test loss: 0.2727 score: 0.9167 time: 0.07s
     INFO: Early stopping counter 15 of 20
Epoch 102/1000, LR 0.000264
Train loss: 0.0053;  Loss pred: 0.0053; Loss self: 0.0000; time: 0.12s
Val loss: 0.4457 score: 0.8980 time: 0.07s
Test loss: 0.2763 score: 0.9167 time: 0.18s
     INFO: Early stopping counter 16 of 20
Epoch 103/1000, LR 0.000264
Train loss: 0.0050;  Loss pred: 0.0050; Loss self: 0.0000; time: 0.12s
Val loss: 0.4530 score: 0.8980 time: 0.07s
Test loss: 0.2802 score: 0.9167 time: 0.07s
     INFO: Early stopping counter 17 of 20
Epoch 104/1000, LR 0.000264
Train loss: 0.0048;  Loss pred: 0.0048; Loss self: 0.0000; time: 0.12s
Val loss: 0.4599 score: 0.8980 time: 0.07s
Test loss: 0.2839 score: 0.9167 time: 0.07s
     INFO: Early stopping counter 18 of 20
Epoch 105/1000, LR 0.000264
Train loss: 0.0047;  Loss pred: 0.0047; Loss self: 0.0000; time: 0.12s
Val loss: 0.4668 score: 0.8980 time: 0.07s
Test loss: 0.2875 score: 0.9167 time: 0.07s
     INFO: Early stopping counter 19 of 20
Epoch 106/1000, LR 0.000264
Train loss: 0.0044;  Loss pred: 0.0044; Loss self: 0.0000; time: 0.12s
Val loss: 0.4733 score: 0.8980 time: 0.07s
Test loss: 0.2912 score: 0.9167 time: 0.07s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 085,   Train_Loss: 0.0135,   Val_Loss: 0.3679,   Val_Precision: 0.9545,   Val_Recall: 0.8400,   Val_accuracy: 0.8936,   Val_Score: 0.8980,   Val_Loss: 0.3679,   Test_Precision: 1.0000,   Test_Recall: 0.8333,   Test_accuracy: 0.9091,   Test_Score: 0.9167,   Test_loss: 0.2585


[0.07078393292613328, 0.07182950200513005, 0.07565474603325129, 0.07194918906316161, 0.07211259403266013, 0.07183831394650042, 0.07171307306271046, 0.07754708500579, 0.07189430296421051, 0.07233359490055591, 0.07208557205740362, 0.07247356593143195, 0.07235056802164763, 0.07162254396826029, 0.07129303400870413, 0.07121002895291895, 0.07144724403042346, 0.07110963901504874, 0.07078702899161726, 0.07133766694460064, 0.07188453897833824, 0.07135094807017595, 0.07108811009675264, 0.07125854701735079, 0.07144543598406017, 0.0712470100261271, 0.07794450398068875, 0.07134187500923872, 0.07038487808313221, 0.07021258503664285, 0.06894871802069247, 0.06994629197288305, 0.07000614900607616, 0.07075591094326228, 0.07042106206063181, 0.07219593797344714, 0.06987993896473199, 0.06980699102859944, 0.07032186491414905, 0.06991312198806554, 0.07057835895102471, 0.06988244294188917, 0.07170787593349814, 0.06543997407425195, 0.06565393705386668, 0.06502827198710293, 0.0654224120080471, 0.06565646606031805, 0.065755404997617, 0.06710893998388201, 0.07122395897749811, 0.0705567040713504, 0.0703179620904848, 0.0694255989510566, 0.07010203192476183, 0.0698585860664025, 0.06987044401466846, 0.17841056000906974, 0.0767566820140928, 0.07033698994200677, 0.06991267600096762, 0.07096321997232735, 0.07042018102947623, 0.06967529293615371, 0.06858130893670022, 0.06912373204249889, 0.06948012509383261, 0.06913573492784053, 0.0707393690245226, 0.06919512699823827, 0.06900859496090561, 0.19735902093816549, 0.06876668892800808, 0.06972259900067002, 0.06890193605795503, 0.06926834501791745, 0.06936872692313045, 0.06947234307881445, 0.06907203898299485, 0.1828952939249575, 0.06973210093565285, 0.06988967803772539, 0.06952537305187434, 0.07037475309334695, 0.0705721570411697, 0.07015506504103541, 0.07141511095687747, 0.06892503902781755, 0.07028862601146102, 0.06987591995857656, 0.07757178705651313, 0.07075237692333758, 0.0708232200704515, 0.06944266706705093, 0.0797262160340324, 0.06569136108737439, 0.0661277249455452, 0.06685079296585172, 0.06828834500629455, 0.06611818401142955, 0.07231886603403836, 0.06593887007329613, 0.06615694495849311, 0.0678873200668022, 0.06609812309034169, 0.06723849603440613, 0.06685232697054744, 0.06520597694907337, 0.06793095404282212, 0.06609208206646144, 0.06636362907011062, 0.07037095003761351, 0.07143615407403558, 0.06979516707360744, 0.07024105591699481, 0.18975004798267037, 0.06593798601534218, 0.06600498605985194, 0.06832788500469178, 0.06692010897677392, 0.06735822302289307, 0.06627176003530622, 0.07662019901908934, 0.07063216099049896, 0.07641606102697551, 0.07008099695667624, 0.07711331208702177, 0.0702624199911952, 0.06638188497163355, 0.0777484430000186, 0.08225686301011592, 0.0813671430805698, 0.0879807110177353, 0.08443449100013822, 0.08210603694897145, 0.08145065093412995, 0.08110962202772498, 0.08104204991832376, 0.08090463594999164, 0.08126533695030957, 0.08129644498694688, 0.08076734794303775, 0.21748492994811386, 0.0811425750143826, 0.08154368703253567, 0.08197619009297341, 0.08513267792295665, 0.0860007320297882, 0.08099732000846416, 0.08315175794996321, 0.08085632999427617, 0.08149965608026832, 0.08122440008446574, 0.08486869197804481, 0.08310657297261059, 0.08250652498099953, 0.0838920189999044, 0.0831593960756436, 0.0827195419697091, 0.08316555700730532, 0.08253008895553648, 0.08256676397286355, 0.09579560102429241, 0.08272142196074128, 0.08412286709062755, 0.08697611594107002, 0.08682677301112562, 0.08263353200163692, 0.08273282798472792, 0.08338134596124291, 0.08294459502212703, 0.08309202606324106, 0.0825072550214827, 0.0838011100422591, 0.0837472160346806, 0.08309929596725851, 0.08265027706511319, 0.08342060504946858, 0.08349338895641267, 0.08235226501710713, 0.0834610490128398, 0.08300752495415509, 0.08201168896630406, 0.08364183502271771, 0.08214734401553869, 0.08157297607976943, 0.0822970219887793, 0.08389099105261266, 0.08248105796519667, 0.08351936598774046, 0.08229000400751829, 0.08270961989182979, 0.08263171196449548, 0.08279521705117077, 0.08394973096437752, 0.08404935896396637, 0.08357891789637506, 0.08375328197143972, 0.08373975800350308, 0.08359813794959337, 0.0835980080300942, 0.08364281198009849, 0.08373944903723896, 0.08351292600855231, 0.08289329800754786, 0.08680400101002306, 0.0880004899809137, 0.08450109092518687, 0.08262421598192304, 0.08464426198042929, 0.0894971260568127, 0.08552029402926564, 0.08547102892771363, 0.08451848896220326, 0.08673169603571296, 0.08958920696750283, 0.0813677409896627, 0.08142791700083762, 0.08181442599743605, 0.08158537093549967, 0.08165834494866431, 0.08195646700914949, 0.08221615699585527, 0.0826783679658547, 0.08207469596527517, 0.08245036902371794, 0.08166807901579887, 0.08111916994675994, 0.08121175295673311, 0.09077251690905541, 0.0851200979668647, 0.08270767296198756, 0.08812898199539632, 0.09063484705984592, 0.0767729040235281, 0.07663196593057364, 0.07722712703980505, 0.07676879304926842, 0.07692500692792237, 0.07737822190392762, 0.08224122505635023, 0.07981608610134572, 0.07674915005918592, 0.07782734395004809, 0.0777765060774982, 0.07723511208314449, 0.07532805402297527, 0.07608880300540477, 0.07660218002274632, 0.07507529901340604, 0.20140741299837828, 0.07525479199830443, 0.07531967700924724, 0.07550361205358058, 0.07561774295754731, 0.07940166303887963, 0.08695645397529006, 0.08851716306526214, 0.07528115599416196, 0.07573885004967451, 0.07598564797081053, 0.07611872989218682, 0.07726531592197716, 0.07580264599528164, 0.07792907999828458, 0.07568248698953539, 0.07565310597419739, 0.07647843507584184, 0.07557681400794536, 0.07206912292167544, 0.07163031701929867, 0.07146296498831362, 0.07697046303655952, 0.07660577702336013, 0.07635516300797462, 0.07733559794723988, 0.07685847603715956, 0.07779900392051786, 0.07686019001994282, 0.07590583502314985, 0.07733682205434889, 0.07771282002795488, 0.07742796302773058, 0.0775284399278462, 0.07856924994848669, 0.07829773693811148, 0.210224136011675, 0.07249861303716898, 0.07269818498753011, 0.07221626304090023, 0.07262233400251716, 0.07354240503627807, 0.07339941803365946, 0.07317637105006725, 0.07280661503318697, 0.07301287294831127, 0.07284702104516327, 0.07291392399929464, 0.07326549594290555, 0.07399501709733158, 0.0729276790516451, 0.20789861597586423, 0.07285559689626098, 0.0730288049671799, 0.07319160702172667, 0.07892791600897908, 0.0734489900059998, 0.07416012103203684, 0.07694979500956833, 0.07692076195962727, 0.07647807605098933, 0.07701174099929631, 0.0774744589580223, 0.07657919696066529, 0.07806390302721411, 0.07642477401532233, 0.07670351606793702, 0.07672254403587431, 0.07742137997411191, 0.07802466000430286, 0.07676079496741295, 0.07290225801989436, 0.0770462469663471, 0.07719807198736817, 0.07778320705983788, 0.07754526694770902, 0.07854533800855279, 0.07734329404775053, 0.07925096899271011, 0.07617811695672572, 0.07650342595297843, 0.07656443293672055, 0.07653361500706524, 0.07795923296362162, 0.07737576798535883, 0.1812565049622208, 0.0779525509569794, 0.07780165900476277, 0.07815747102722526, 0.07821796694770455]
[0.0014445700597170057, 0.0014659082041863277, 0.001543974408841863, 0.0014683507972073797, 0.001471685592503268, 0.0014660880397244984, 0.0014635321033206216, 0.0015825935715467346, 0.00146723067273899, 0.0014761958142970593, 0.0014711341236204822, 0.0014790523659475908, 0.0014765422045234212, 0.0014616845707808221, 0.0014549598777286556, 0.0014532658969983459, 0.0014581070210290502, 0.0014512171227560968, 0.001444633244726883, 0.0014558707539714417, 0.0014670314077211886, 0.0014561417973505296, 0.0014507777570765845, 0.0014542560615785876, 0.001458070122123677, 0.0014540206127781042, 0.001590704162871199, 0.0014559566328416066, 0.0014364260833292287, 0.0014329098987069968, 0.0014071166942998463, 0.0014274753463853682, 0.0014286969184913502, 0.0014439981825155566, 0.001437164531849629, 0.0014733864892540233, 0.0014261212033618773, 0.0014246324699714171, 0.0014351401002887561, 0.001426798407919705, 0.001440374672469892, 0.0014261723049365136, 0.001463426039459146, 0.0013355096749847336, 0.0013398762664054425, 0.001327107591573529, 0.001335151265470349, 0.001339927878782001, 0.001341947040767694, 0.001369570203752694, 0.0014535501832142472, 0.001439932736150008, 0.0014350604508262203, 0.0014168489581848286, 0.0014306537127502415, 0.0014256854299265816, 0.001425927428870785, 0.0036410318369197907, 0.0015664628982467919, 0.0014354487743266687, 0.0014267893061421964, 0.0014482289790270888, 0.0014371465516219639, 0.0014219447537990554, 0.001399618549728576, 0.0014106884090305896, 0.0014179617366088288, 0.0014109333658742966, 0.001443660592337196, 0.001412145448943638, 0.0014083386726715431, 0.004027735121187051, 0.0014034018148573078, 0.001422910183687143, 0.0014061619603664291, 0.0014136396942432134, 0.0014156883045536826, 0.001417802919975805, 0.001409633448632548, 0.003732557018876684, 0.0014231041007276093, 0.0014263199599535794, 0.001418885164323966, 0.0014362194508846318, 0.0014402481028810143, 0.0014317360212456208, 0.0014574512440179075, 0.001406633449547297, 0.0014344617553359391, 0.001426039182828093, 0.0015830976950308802, 0.0014439260596599505, 0.0014453718381724795, 0.0014171972870826721, 0.0016270656333476, 0.001340640022191314, 0.0013495454070519429, 0.0013643018972622802, 0.0013936396940060112, 0.001349350694110807, 0.0014758952251844564, 0.0013456912259856354, 0.0013501417338467982, 0.0013854555115673918, 0.0013489412875579937, 0.0013722142047837985, 0.00136433320348056, 0.001330734223450477, 0.0013863460008739208, 0.001348818001356356, 0.001354359776941033, 0.0014361418375023166, 0.0014578806953884813, 0.0014243911647674988, 0.0014334909370815267, 0.0038724499588300076, 0.001345673183986575, 0.0013470405318337132, 0.0013944466327488118, 0.0013657165097300799, 0.0013746576127121035, 0.0013524848986797187, 0.0015636775310018233, 0.001441472673275489, 0.0015595114495301125, 0.0014302244276872703, 0.0015737410630004443, 0.0014339269385958205, 0.0013547323463598685, 0.0015867029183677264, 0.0016787114900023658, 0.001660553940419792, 0.0017955247146476591, 0.0017231528775538414, 0.0016756334071218663, 0.0016622581823291828, 0.0016552984087290813, 0.00165391938608824, 0.0016511150193875845, 0.0016584762642920321, 0.0016591111221825894, 0.001648313223327301, 0.004438467958124773, 0.0016559709186608695, 0.0016641568782150137, 0.0016729834712851718, 0.0017374015902644213, 0.0017551169801997592, 0.0016530065307849829, 0.0016969746520400656, 0.0016501291835566564, 0.0016632582873524148, 0.0016576408180503212, 0.0017320141220009144, 0.001696052509645114, 0.0016838066322652965, 0.001712082020406212, 0.0016971305321559918, 0.0016881539177491653, 0.0016972562654552106, 0.0016842875297048262, 0.001685035999446195, 0.001955012265801886, 0.0016881922849130873, 0.0017167932059311746, 0.0017750227743075515, 0.001771974959410727, 0.0016863986122783046, 0.0016884250609128146, 0.0017016601216580188, 0.001692746837186266, 0.0016957556339436952, 0.0016838215310506675, 0.0017102267355563082, 0.0017091268578506246, 0.0016959039993318064, 0.001686740348267616, 0.001702461327540175, 0.001703946713396177, 0.0016806584697368803, 0.0017032867145477509, 0.0016940311215133692, 0.001673707938087838, 0.0017069762249534227, 0.0016764764084803816, 0.0016647546138728456, 0.0016795310609954961, 0.0017120610418900543, 0.0016832868972489117, 0.0017044768568926624, 0.0016793878368881283, 0.0016879514263638733, 0.001686361468663173, 0.0016896983071667503, 0.0017132598155995412, 0.0017152930400809463, 0.001705692201966838, 0.0017092506524783615, 0.001708974653132716, 0.001706084447950885, 0.0017060817965325347, 0.0017069961628591527, 0.0017089683476987543, 0.0017043454287459655, 0.0016916999593377113, 0.0017715102246943482, 0.0017959283669574224, 0.0017245120596976913, 0.001686208489427001, 0.0017274339179679447, 0.0018264719603431163, 0.0017453121230462376, 0.0017443067128104822, 0.0017248671216776176, 0.0017700346129737338, 0.0018283511626020986, 0.0016605661426461777, 0.0016617942245068904, 0.0016696821632129805, 0.001665007570112238, 0.0016664968356870267, 0.0016725809593703977, 0.0016778807550174545, 0.0016873136319562184, 0.0016749937952096974, 0.0016826605923207743, 0.0016666954901183443, 0.0016554932642195905, 0.0016573827134027165, 0.0018525003450827636, 0.0017371448564666267, 0.0016879116931017867, 0.0017985506529672717, 0.0018496907563233863, 0.0015994355004901688, 0.0015964992902202841, 0.0016088984799959387, 0.0015993498551930922, 0.0016026043109983827, 0.0016120462896651588, 0.0017133588553406298, 0.001662835127111369, 0.0015989406262330401, 0.0016214029989593353, 0.0016203438766145457, 0.0016090648350655101, 0.0015693344588119846, 0.0015851833959459327, 0.0015958787504738818, 0.0015640687294459592, 0.0041959877707995474, 0.0015678081666313422, 0.0015691599376926508, 0.0015729919177829288, 0.0015753696449489023, 0.0016542013133099924, 0.0018115927911518763, 0.0018441075638596278, 0.001568357416545041, 0.001577892709368219, 0.0015830343327252194, 0.001585806872753892, 0.0016096940817078575, 0.0015792217915683675, 0.0016235224999642621, 0.001576718478948654, 0.0015761063744624455, 0.001593300730746705, 0.0015745169584988616, 0.0015014400608682383, 0.001492298271235389, 0.001488811770589867, 0.0016035513132616568, 0.0015959536879866694, 0.001590732562666138, 0.0016111582905674975, 0.0016012182507741575, 0.0016208125816774555, 0.0016012539587488088, 0.0015813715629822884, 0.0016111837927989352, 0.0016190170839157265, 0.0016130825630777206, 0.001615175831830129, 0.001636859373926806, 0.0016312028528773226, 0.004379669500243229, 0.001510387771607687, 0.001514545520573544, 0.0015045054800187547, 0.0015129652917191077, 0.0015321334382557932, 0.0015291545423679054, 0.0015245077302097343, 0.001516804479858062, 0.0015211015197564848, 0.001517646271774235, 0.0015190400833186384, 0.0015263644988105323, 0.001541562856194408, 0.001519326646909273, 0.0043312211661638384, 0.0015178249353387703, 0.0015214334368162479, 0.0015248251462859723, 0.0016443315835203975, 0.0015301872917916626, 0.0015450025215007674, 0.001603120729366007, 0.0016025158741589014, 0.0015932932510622777, 0.001604411270818673, 0.001614051228292131, 0.0015953999366805267, 0.0016263313130669606, 0.0015921827919858818, 0.0015979899180820212, 0.0015983863340807147, 0.0016129454161273316, 0.0016255137500896428, 0.0015991832284877698, 0.0015187970420811325, 0.0016051301451322313, 0.0016082931664035034, 0.001620483480413289, 0.001615526394743938, 0.0016363612085115165, 0.0016113186259948027, 0.001651061854014794, 0.001587044103265119, 0.001593821374020384, 0.0015950923528483447, 0.0015944503126471925, 0.001624150686742117, 0.0016119951663616423, 0.0037761771867129332, 0.001624011478270404, 0.0016208678959325578, 0.0016282806464005262, 0.0016295409780771781]
[692.2474914064758, 682.1709552782424, 647.6791287946937, 681.0361678570783, 679.4929603809243, 682.0872777789772, 683.2784861576255, 631.8741703358862, 681.5560897000775, 677.4169052065656, 679.7476749019904, 676.1085834573042, 677.2579862170394, 684.1421329814042, 687.3041760856694, 688.1053233723121, 685.8207151998048, 689.0767648198897, 692.217214057715, 686.874159173896, 681.6486646004047, 686.746305764668, 689.2854505951813, 687.636810614016, 685.8380710411249, 687.748159284595, 628.6524064883403, 686.8336442468682, 696.1722650442849, 697.8805861431774, 710.6731119394333, 700.5374926664654, 699.9385153402319, 692.5216472626873, 695.8145555630999, 678.7085447663503, 701.2026731266898, 701.9354262086019, 696.7960826951988, 700.8698597148115, 694.2638044900104, 701.1775481396094, 683.328007727388, 748.7778027601568, 746.3375724108863, 753.5184082658413, 748.9788055196267, 746.3088244040443, 745.1858900690487, 730.1560717807292, 687.9707433208064, 694.4768841589993, 696.8347566294235, 705.7915342515638, 698.9811658040106, 701.4170019619945, 701.2979621213375, 274.6474199593848, 638.3809033199666, 696.6462460278833, 700.8743307053762, 690.4985430355038, 695.8232609412031, 703.2622029289591, 714.4803848119383, 708.8737623407494, 705.2376479435768, 708.7506924044862, 692.6835887243155, 708.1423522966807, 710.0564795987981, 248.27849148761322, 712.5543015644995, 702.7850467755674, 711.1556336934416, 707.3938317325945, 706.3701782259657, 705.3166458544641, 709.4042788003337, 267.9128530234619, 702.6892828772799, 701.104961072371, 704.7786707083193, 696.2724250698982, 694.3248166754327, 698.4527770209992, 686.1292987360564, 710.9172615807156, 697.1255917281728, 701.2430037278635, 631.6729555850271, 692.5562381189403, 691.8634870210248, 705.6180597540651, 614.6033568065436, 745.9123877008175, 740.9902584785804, 732.9755987341819, 717.5455781727229, 741.0971842712679, 677.5548717389614, 743.1125214237479, 740.6629799901222, 721.784273584275, 741.322108844569, 728.7491971106339, 732.9587797532839, 751.4648547980436, 721.3206510998141, 741.3898680136323, 738.3562455307166, 696.3100537055324, 685.9271840028927, 702.0543406440102, 697.5977134783428, 258.234453803538, 743.1224846418404, 742.3681592109999, 717.1303487095405, 732.2163808341453, 727.4538697873064, 739.3797897308793, 639.5180465113648, 693.7349687855537, 641.2264560810402, 699.1909665653241, 635.4285488957327, 697.3856010957257, 738.1531877399804, 630.2377013516307, 595.6949755544896, 602.2086820902654, 556.9402592134371, 580.331561422213, 596.7892474271202, 601.5912633973527, 604.1206798282301, 604.6243900466911, 605.6513254727162, 602.9631062744685, 602.7323827981351, 606.6808091130825, 225.30296702254313, 603.8753390721788, 600.9048864867879, 597.7345366310216, 575.5721680027968, 569.7625920559351, 604.9582874455542, 589.2839936046316, 606.0131594331417, 601.2295309779015, 603.2669979592906, 577.3624979712908, 589.6043868413262, 593.8924225845682, 584.0841665767496, 589.229868329353, 592.3630478749895, 589.1862179880044, 593.7228545385309, 593.459131038542, 511.50574218511656, 592.349585374087, 582.48133586806, 563.3730532782076, 564.3420606420711, 592.9796150917201, 592.2679206498921, 587.6614179720251, 590.75579291126, 589.7076087987825, 593.8871677071513, 584.7177916293753, 585.0940761984086, 589.6560184975119, 592.8594765798188, 587.3848549880801, 586.87281247597, 595.0048852915118, 587.1002171619213, 590.307927227834, 597.4758064077001, 585.8312408699696, 596.4891572237726, 600.6891295970785, 595.4042906519855, 584.091323575727, 594.0757939923105, 586.690277404555, 595.4550688261382, 592.4341094069073, 592.992675996522, 591.8216262385789, 583.6826328936328, 582.9907640462466, 586.2722470366562, 585.051700024236, 585.1461858529634, 586.1374571470146, 586.138368062079, 585.8243982956814, 585.1483448166668, 586.7355191815702, 591.1213714230347, 564.4901090946497, 556.8150814913365, 579.8741704220387, 593.046474543498, 578.8933455563633, 547.5036144612603, 572.9634182879663, 573.2936717240332, 579.7548039685475, 564.9607034067866, 546.9408833786645, 602.2042569207515, 601.7592222025764, 598.9163818314339, 600.5978699139428, 600.0611453832986, 597.8783833438027, 595.9899098965452, 592.6580459381647, 597.0171369350101, 594.296915589359, 599.9896237368439, 604.0495733888752, 603.3609448881808, 539.8109655711419, 575.6572321976718, 592.4480552429567, 556.0032453632525, 540.6309117247745, 625.220585446263, 626.370463254024, 621.5432561055837, 625.2540660525262, 623.9843441934989, 620.3295813594235, 583.6488934486475, 601.38253257686, 625.4140920516292, 616.7498152167166, 617.1529478602672, 621.4789971215093, 637.2127970458372, 630.8418335427151, 626.6140204592981, 639.3580928852343, 238.3229062198743, 637.8331362749828, 637.283667508384, 635.7311748997803, 634.77165705604, 604.5213432934828, 552.000430165194, 542.2677177827135, 637.6097625775352, 633.7566515535746, 631.6982388363515, 630.593811378439, 621.2360543309057, 633.2232782875122, 615.9446512272004, 634.2286294930683, 634.4749416682391, 627.6279052049057, 635.1154203847992, 666.0272534767253, 670.1073232311371, 671.6765811193178, 623.6158404971645, 626.5845979913878, 628.6411829804728, 620.6714795526208, 624.5244828533023, 616.9744801493661, 624.5105559528996, 632.3624525751005, 620.6616554048178, 617.6587078262431, 619.9310704171431, 619.12763941429, 610.9260306222959, 613.0445384129099, 228.32773110949674, 662.0816314843306, 660.2640768573991, 664.6702277133177, 660.953695020822, 652.684665076181, 653.9561386983776, 655.9494453087652, 659.2807532409036, 657.418316273914, 658.9150703944536, 658.3104757942303, 655.1515059340555, 648.6923293343084, 658.1863103857714, 230.88176789773584, 658.8375093316051, 657.2748934009235, 655.8128992269752, 608.1498464312598, 653.5147725799775, 647.2481346040983, 623.7833381366569, 624.0187795486652, 627.6308515919977, 623.2815850824435, 619.5590217158879, 626.8020807877508, 614.8808621990955, 628.0685892558416, 625.786175922965, 625.6309746136145, 619.983782464872, 615.1901206279261, 625.3192143251944, 658.4158200820232, 623.0024419095434, 621.7771864542703, 617.0997804587057, 618.9932911362309, 611.1120178103159, 620.6097191873616, 605.6708278786506, 630.1022119943871, 627.4228820746199, 626.9229478871912, 627.1753920884157, 615.7064169987204, 620.3492546798713, 264.8180820324469, 615.7591946733127, 616.9534250813546, 614.1447435431957, 613.6697471578638]
Elapsed: 0.07937813596526051~0.020326703595470744
Time per graph: 0.0016307012160616024~0.00041972070924556706
Speed: 632.7516044214792~82.01418202301353
Total Time: 0.0793
best val loss: 0.367889404296875 test_score: 0.9167

Testing...
Test loss: 0.4545 score: 0.9583 time: 0.07s
test Score 0.9583
Epoch Time List: [0.4344634370645508, 0.2635430139489472, 0.2607021938310936, 0.2612041379325092, 0.25651010393630713, 0.2584334989078343, 0.2577688579913229, 0.26812492311000824, 0.26772345695644617, 0.26062874705530703, 0.26006776408758014, 0.25733603595290333, 0.2611486701061949, 0.2559775010449812, 0.26052997494116426, 0.2592564520891756, 0.26425005099736154, 0.2575646300101653, 0.25313281978014857, 0.25991963408887386, 0.2557188560022041, 0.2585642950143665, 0.2577043240889907, 0.2574328950140625, 0.2569704339839518, 0.2612698870943859, 0.2687719070818275, 0.2582835239591077, 0.25436484697274864, 0.2482264790451154, 0.24814224790316075, 0.24753734900150448, 0.25341243483126163, 0.25052620586939156, 0.2548666420625523, 0.2540812570368871, 0.3867917510215193, 0.2527400868711993, 0.2487003490095958, 0.25279088702518493, 0.24963115295395255, 0.2505265739746392, 0.25388738699257374, 0.36178546003066003, 0.23628104489762336, 0.23987887997645885, 0.24056445399764925, 0.23666120308917016, 0.2413169019855559, 0.24418613000307232, 0.36085116281174123, 0.25077103497460485, 0.25516509101726115, 0.25512102397624403, 0.2560048389714211, 0.25191611202899367, 0.2509110620012507, 0.36683513585012406, 0.2576390679460019, 0.2500923709012568, 0.2514547168975696, 0.25222785701043904, 0.2509378530085087, 0.2563305289950222, 0.2496927878819406, 0.371875366079621, 0.2491267540026456, 0.24852963001467288, 0.2515180470654741, 0.2539589679799974, 0.2493374350015074, 0.38316226098686457, 0.25281407788861543, 0.24813696695491672, 0.2481856788508594, 0.24904972407966852, 0.2562789289513603, 0.25462263193912804, 0.24897942191455513, 0.36303250899072737, 0.24885577789973468, 0.25247344095259905, 0.24862695415504277, 0.2607372240163386, 0.25188332702964544, 0.25121795292943716, 0.2558600160991773, 0.3614651779644191, 0.24957317113876343, 0.24941887706518173, 0.2584806131199002, 0.2567017601104453, 0.258104367996566, 0.2510665850713849, 0.3807046910515055, 0.24510983505751938, 0.24535710492637008, 0.2484397990629077, 0.24521608091890812, 0.2507089850259945, 0.24745711602736264, 0.30859925400000066, 0.2457962220069021, 0.25176771311089396, 0.24575713800732046, 0.24402898899279535, 0.24620702001266181, 0.24044316005893052, 0.23992235015612096, 0.36147376406006515, 0.23914747592061758, 0.24348997103516012, 0.24643740500323474, 0.2566716820001602, 0.2594536499818787, 0.36758032999932766, 0.24151650886051357, 0.2487489429768175, 0.24128121905960143, 0.25018550804816186, 0.247679433086887, 0.2431446418631822, 0.2489456810289994, 0.3647894309833646, 0.2576233949512243, 0.25503387907519937, 0.26108589000068605, 0.2509082049364224, 0.23513870302122086, 0.25978443503845483, 0.2639840889023617, 0.2617574839387089, 0.26879993500187993, 0.2671267680125311, 0.26280294090975076, 0.33579108491539955, 0.262219715048559, 0.2603959811385721, 0.2602888628607616, 0.26134088495746255, 0.26211766875348985, 0.2593358941376209, 0.3956867150263861, 0.2586636820342392, 0.2620860901661217, 0.262939588050358, 0.272687510936521, 0.28846510499715805, 0.25992246996611357, 0.4005793429678306, 0.27014768321532756, 0.2703485789243132, 0.27078946703113616, 0.2748897769488394, 0.27495166496373713, 0.2753325029043481, 0.27589497389271855, 0.27494268701411784, 0.2741439709207043, 0.2743687150068581, 0.27334992913529277, 0.27402060106396675, 0.29521891102194786, 0.2836101178545505, 0.29597970889881253, 0.2949746469967067, 0.2864448019536212, 0.2751444400055334, 0.27420735906343907, 0.27508999698329717, 0.2750572159420699, 0.2746710239443928, 0.2740678610280156, 0.27663135796319693, 0.27627473790198565, 0.27580651186872274, 0.2737393800634891, 0.2741827151039615, 0.2763041788712144, 0.2721818630816415, 0.2726752811577171, 0.27324509993195534, 0.27119620493613183, 0.27345486998092383, 0.2749486080138013, 0.2719658430432901, 0.27207272208761424, 0.27498776698485017, 0.27317474293522537, 0.27306724491063505, 0.27283802500460297, 0.2740346089703962, 0.2749903139192611, 0.2737198161194101, 0.2757420300040394, 0.2797987200319767, 0.27475354401394725, 0.27759530406910926, 0.2754404480801895, 0.27677215496078134, 0.2777787080267444, 0.2779627490090206, 0.2778215939179063, 0.27714634500443935, 0.27587385301012546, 0.29194517491851, 0.27961763506755233, 0.28267076204065233, 0.27234432904515415, 0.2785541098564863, 0.29498133703600615, 0.2846967341611162, 0.27916241507045925, 0.2780737349530682, 0.2835931369336322, 0.2789733640383929, 0.27193559205625206, 0.2703952870797366, 0.2721247678855434, 0.26972794090397656, 0.27297015907242894, 0.27304073877166957, 0.27168729004915804, 0.27623095689341426, 0.2729206170188263, 0.27423350000754, 0.27183183608576655, 0.2704140320420265, 0.27185989706777036, 0.28160026809200644, 0.28049816691782326, 0.2787392019527033, 0.2902920679189265, 0.3002983769401908, 0.2674473749939352, 0.2613354990025982, 0.351541121955961, 0.2643237031297758, 0.26341996411792934, 0.2626610070001334, 0.2822433190885931, 0.28808816499076784, 0.2658032509498298, 0.34996530995704234, 0.26496127317659557, 0.2631253959843889, 0.2589987930841744, 0.2607712069293484, 0.26539593492634594, 0.2594100668793544, 0.384569710935466, 0.25729406997561455, 0.25619497091975063, 0.256367618101649, 0.258158708922565, 0.2668965191114694, 0.2701851309975609, 0.2891349559649825, 0.3872246149694547, 0.25668250513263047, 0.2601270350860432, 0.2605306978803128, 0.2632042800541967, 0.26254246896132827, 0.2621546669397503, 0.3274901551194489, 0.2587050449801609, 0.26099897315725684, 0.2621513759950176, 0.2502007329603657, 0.24654620792716742, 0.24735234293621033, 0.36275312677025795, 0.26116340595763177, 0.261852525989525, 0.2627510608872399, 0.263045750092715, 0.26740870298817754, 0.26290258881635964, 0.3518847528612241, 0.26480893697589636, 0.2658978729741648, 0.26531956694088876, 0.2685601201374084, 0.2685393219580874, 0.26824188604950905, 0.39960657386109233, 0.25027541315648705, 0.25095885805785656, 0.24992091511376202, 0.24993380799423903, 0.25226225994993, 0.259157816064544, 0.253215086995624, 0.3844387091230601, 0.25362810597289354, 0.2501237280666828, 0.2513337481068447, 0.2529282959876582, 0.2587669800268486, 0.25304576905909926, 0.38764237705618143, 0.25601339200511575, 0.25190808391198516, 0.2522510359995067, 0.25987602095119655, 0.25637353502679616, 0.25314423895906657, 0.36989023187197745, 0.2631380980601534, 0.2643081590067595, 0.2638303200947121, 0.2673810259439051, 0.26595771906431764, 0.2644870770163834, 0.39883156202267855, 0.26103597204200923, 0.26028598903212696, 0.26182517188135535, 0.26800885098055005, 0.26190612791106105, 0.349059529020451, 0.26444149599410594, 0.26456929300911725, 0.26278977293986827, 0.2643151719821617, 0.2690669520525262, 0.26681000308599323, 0.2678128400584683, 0.31589425005950034, 0.2611665119184181, 0.26064480107743293, 0.2624257429270074, 0.2726506100734696, 0.26565670722629875, 0.37164066184777766, 0.2667910821037367, 0.26634243805892766, 0.2657745579490438, 0.2666675361106172]
Total Epoch List: [129, 105, 106]
Total Time List: [0.06697597703896463, 0.09154098795261234, 0.07930420897901058]
========================training times:1========================
========================k_idx:0========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: I2BGNNA
I2BGNN(
  (gcs): ModuleList(
    (0): GCNConv(14887, 64)
    (1): GCNConv(64, 64)
  )
  (bns): ModuleList(
    (0-1): 2 x BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  )
  (drops): ModuleList(
    (0-1): 2 x Dropout(p=0.2, inplace=False)
  )
  (lin1): Linear(in_features=64, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=64, bias=True)
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=64, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=64, out_features=2, bias=True)
  )
)
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7d1ecd7792d0>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.6922;  Loss pred: 0.6922; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6934 score: 0.4898 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6930 score: 0.5102 time: 0.07s
Epoch 2/1000, LR 0.000000
Train loss: 0.6935;  Loss pred: 0.6935; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6934 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6930 score: 0.5102 time: 0.07s
     INFO: Early stopping counter 1 of 20
Epoch 3/1000, LR 0.000030
Train loss: 0.6931;  Loss pred: 0.6931; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6934 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6930 score: 0.5102 time: 0.07s
     INFO: Early stopping counter 2 of 20
Epoch 4/1000, LR 0.000060
Train loss: 0.6921;  Loss pred: 0.6921; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6934 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6930 score: 0.5102 time: 0.07s
     INFO: Early stopping counter 3 of 20
Epoch 5/1000, LR 0.000090
Train loss: 0.6921;  Loss pred: 0.6921; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6934 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6930 score: 0.5102 time: 0.07s
     INFO: Early stopping counter 4 of 20
Epoch 6/1000, LR 0.000120
Train loss: 0.6869;  Loss pred: 0.6869; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6934 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6930 score: 0.5102 time: 0.07s
     INFO: Early stopping counter 5 of 20
Epoch 7/1000, LR 0.000150
Train loss: 0.6895;  Loss pred: 0.6895; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6934 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6930 score: 0.5102 time: 0.07s
     INFO: Early stopping counter 6 of 20
Epoch 8/1000, LR 0.000180
Train loss: 0.6852;  Loss pred: 0.6852; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6934 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6930 score: 0.5102 time: 0.07s
     INFO: Early stopping counter 7 of 20
Epoch 9/1000, LR 0.000210
Train loss: 0.6811;  Loss pred: 0.6811; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6934 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6929 score: 0.5102 time: 0.07s
     INFO: Early stopping counter 8 of 20
Epoch 10/1000, LR 0.000240
Train loss: 0.6794;  Loss pred: 0.6794; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6934 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6929 score: 0.5102 time: 0.07s
     INFO: Early stopping counter 9 of 20
Epoch 11/1000, LR 0.000270
Train loss: 0.6771;  Loss pred: 0.6771; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6934 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6929 score: 0.5102 time: 0.07s
Epoch 12/1000, LR 0.000270
Train loss: 0.6718;  Loss pred: 0.6718; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6933 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6929 score: 0.5102 time: 0.07s
Epoch 13/1000, LR 0.000270
Train loss: 0.6652;  Loss pred: 0.6652; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6933 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6929 score: 0.5102 time: 0.07s
Epoch 14/1000, LR 0.000270
Train loss: 0.6615;  Loss pred: 0.6615; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6933 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6929 score: 0.5102 time: 0.07s
Epoch 15/1000, LR 0.000270
Train loss: 0.6613;  Loss pred: 0.6613; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6932 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6929 score: 0.5102 time: 0.08s
Epoch 16/1000, LR 0.000270
Train loss: 0.6531;  Loss pred: 0.6531; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6932 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6929 score: 0.5102 time: 0.07s
Epoch 17/1000, LR 0.000270
Train loss: 0.6479;  Loss pred: 0.6479; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6931 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6928 score: 0.5102 time: 0.07s
Epoch 18/1000, LR 0.000270
Train loss: 0.6438;  Loss pred: 0.6438; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6931 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6928 score: 0.5102 time: 0.07s
Epoch 19/1000, LR 0.000270
Train loss: 0.6371;  Loss pred: 0.6371; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6930 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6928 score: 0.5102 time: 0.07s
Epoch 20/1000, LR 0.000270
Train loss: 0.6290;  Loss pred: 0.6290; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6930 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6927 score: 0.5102 time: 0.07s
Epoch 21/1000, LR 0.000270
Train loss: 0.6258;  Loss pred: 0.6258; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6929 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6926 score: 0.5102 time: 0.07s
Epoch 22/1000, LR 0.000270
Train loss: 0.6168;  Loss pred: 0.6168; Loss self: 0.0000; time: 0.13s
Val loss: 0.6928 score: 0.5714 time: 0.07s
Test loss: 0.6926 score: 0.6735 time: 0.07s
Epoch 23/1000, LR 0.000270
Train loss: 0.6064;  Loss pred: 0.6064; Loss self: 0.0000; time: 0.13s
Val loss: 0.6927 score: 0.8367 time: 0.07s
Test loss: 0.6925 score: 0.9388 time: 0.07s
Epoch 24/1000, LR 0.000270
Train loss: 0.5983;  Loss pred: 0.5983; Loss self: 0.0000; time: 0.13s
Val loss: 0.6926 score: 0.5714 time: 0.07s
Test loss: 0.6924 score: 0.5918 time: 0.07s
Epoch 25/1000, LR 0.000270
Train loss: 0.5862;  Loss pred: 0.5862; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6925 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6922 score: 0.4898 time: 0.07s
Epoch 26/1000, LR 0.000270
Train loss: 0.5803;  Loss pred: 0.5803; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6923 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6921 score: 0.4898 time: 0.07s
Epoch 27/1000, LR 0.000270
Train loss: 0.5707;  Loss pred: 0.5707; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6922 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6919 score: 0.4898 time: 0.07s
Epoch 28/1000, LR 0.000270
Train loss: 0.5567;  Loss pred: 0.5567; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6920 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6916 score: 0.4898 time: 0.07s
Epoch 29/1000, LR 0.000270
Train loss: 0.5455;  Loss pred: 0.5455; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6918 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6914 score: 0.4898 time: 0.07s
Epoch 30/1000, LR 0.000270
Train loss: 0.5357;  Loss pred: 0.5357; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6916 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6911 score: 0.4898 time: 0.07s
Epoch 31/1000, LR 0.000270
Train loss: 0.5218;  Loss pred: 0.5218; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6913 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6908 score: 0.4898 time: 0.07s
Epoch 32/1000, LR 0.000270
Train loss: 0.5105;  Loss pred: 0.5105; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6910 score: 0.5102 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6904 score: 0.4898 time: 0.08s
Epoch 33/1000, LR 0.000270
Train loss: 0.4980;  Loss pred: 0.4980; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6907 score: 0.5102 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6900 score: 0.4898 time: 0.07s
Epoch 34/1000, LR 0.000270
Train loss: 0.4823;  Loss pred: 0.4823; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6904 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6895 score: 0.4898 time: 0.07s
Epoch 35/1000, LR 0.000270
Train loss: 0.4692;  Loss pred: 0.4692; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6900 score: 0.5102 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6888 score: 0.4898 time: 0.08s
Epoch 36/1000, LR 0.000270
Train loss: 0.4504;  Loss pred: 0.4504; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6895 score: 0.5102 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6881 score: 0.4898 time: 0.08s
Epoch 37/1000, LR 0.000270
Train loss: 0.4435;  Loss pred: 0.4435; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6889 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6872 score: 0.4898 time: 0.07s
Epoch 38/1000, LR 0.000270
Train loss: 0.4166;  Loss pred: 0.4166; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6881 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6860 score: 0.4898 time: 0.07s
Epoch 39/1000, LR 0.000269
Train loss: 0.4059;  Loss pred: 0.4059; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6872 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6847 score: 0.4898 time: 0.07s
Epoch 40/1000, LR 0.000269
Train loss: 0.3945;  Loss pred: 0.3945; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6860 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6832 score: 0.4898 time: 0.07s
Epoch 41/1000, LR 0.000269
Train loss: 0.3794;  Loss pred: 0.3794; Loss self: 0.0000; time: 0.13s
Val loss: 0.6847 score: 0.5306 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6813 score: 0.4898 time: 0.07s
Epoch 42/1000, LR 0.000269
Train loss: 0.3605;  Loss pred: 0.3605; Loss self: 0.0000; time: 0.13s
Val loss: 0.6832 score: 0.5714 time: 0.07s
Test loss: 0.6791 score: 0.5510 time: 0.07s
Epoch 43/1000, LR 0.000269
Train loss: 0.3466;  Loss pred: 0.3466; Loss self: 0.0000; time: 0.13s
Val loss: 0.6813 score: 0.5918 time: 0.07s
Test loss: 0.6766 score: 0.5510 time: 0.07s
Epoch 44/1000, LR 0.000269
Train loss: 0.3292;  Loss pred: 0.3292; Loss self: 0.0000; time: 0.13s
Val loss: 0.6792 score: 0.6122 time: 0.07s
Test loss: 0.6738 score: 0.5510 time: 0.07s
Epoch 45/1000, LR 0.000269
Train loss: 0.3141;  Loss pred: 0.3141; Loss self: 0.0000; time: 0.13s
Val loss: 0.6771 score: 0.6122 time: 0.07s
Test loss: 0.6709 score: 0.6531 time: 0.07s
Epoch 46/1000, LR 0.000269
Train loss: 0.3002;  Loss pred: 0.3002; Loss self: 0.0000; time: 0.13s
Val loss: 0.6748 score: 0.6327 time: 0.07s
Test loss: 0.6676 score: 0.6531 time: 0.07s
Epoch 47/1000, LR 0.000269
Train loss: 0.2811;  Loss pred: 0.2811; Loss self: 0.0000; time: 0.13s
Val loss: 0.6723 score: 0.6531 time: 0.07s
Test loss: 0.6641 score: 0.6735 time: 0.07s
Epoch 48/1000, LR 0.000269
Train loss: 0.2666;  Loss pred: 0.2666; Loss self: 0.0000; time: 0.13s
Val loss: 0.6696 score: 0.6531 time: 0.07s
Test loss: 0.6604 score: 0.6735 time: 0.07s
Epoch 49/1000, LR 0.000269
Train loss: 0.2492;  Loss pred: 0.2492; Loss self: 0.0000; time: 0.13s
Val loss: 0.6666 score: 0.6531 time: 0.07s
Test loss: 0.6564 score: 0.6939 time: 0.07s
Epoch 50/1000, LR 0.000269
Train loss: 0.2322;  Loss pred: 0.2322; Loss self: 0.0000; time: 0.13s
Val loss: 0.6634 score: 0.6735 time: 0.07s
Test loss: 0.6521 score: 0.6939 time: 0.07s
Epoch 51/1000, LR 0.000269
Train loss: 0.2191;  Loss pred: 0.2191; Loss self: 0.0000; time: 0.13s
Val loss: 0.6600 score: 0.6531 time: 0.07s
Test loss: 0.6474 score: 0.6939 time: 0.07s
Epoch 52/1000, LR 0.000269
Train loss: 0.2049;  Loss pred: 0.2049; Loss self: 0.0000; time: 0.12s
Val loss: 0.6563 score: 0.6531 time: 0.07s
Test loss: 0.6423 score: 0.6531 time: 0.07s
Epoch 53/1000, LR 0.000269
Train loss: 0.1963;  Loss pred: 0.1963; Loss self: 0.0000; time: 0.13s
Val loss: 0.6523 score: 0.6531 time: 0.07s
Test loss: 0.6368 score: 0.6531 time: 0.07s
Epoch 54/1000, LR 0.000269
Train loss: 0.1827;  Loss pred: 0.1827; Loss self: 0.0000; time: 0.13s
Val loss: 0.6479 score: 0.6735 time: 0.07s
Test loss: 0.6309 score: 0.6735 time: 0.07s
Epoch 55/1000, LR 0.000269
Train loss: 0.1659;  Loss pred: 0.1659; Loss self: 0.0000; time: 0.13s
Val loss: 0.6433 score: 0.6939 time: 0.07s
Test loss: 0.6246 score: 0.6735 time: 0.07s
Epoch 56/1000, LR 0.000269
Train loss: 0.1525;  Loss pred: 0.1525; Loss self: 0.0000; time: 0.13s
Val loss: 0.6383 score: 0.6939 time: 0.07s
Test loss: 0.6180 score: 0.6735 time: 0.07s
Epoch 57/1000, LR 0.000269
Train loss: 0.1427;  Loss pred: 0.1427; Loss self: 0.0000; time: 0.12s
Val loss: 0.6331 score: 0.7143 time: 0.07s
Test loss: 0.6109 score: 0.6735 time: 0.07s
Epoch 58/1000, LR 0.000269
Train loss: 0.1303;  Loss pred: 0.1303; Loss self: 0.0000; time: 0.13s
Val loss: 0.6277 score: 0.7143 time: 0.07s
Test loss: 0.6035 score: 0.6939 time: 0.07s
Epoch 59/1000, LR 0.000268
Train loss: 0.1219;  Loss pred: 0.1219; Loss self: 0.0000; time: 0.13s
Val loss: 0.6221 score: 0.7143 time: 0.07s
Test loss: 0.5957 score: 0.6939 time: 0.07s
Epoch 60/1000, LR 0.000268
Train loss: 0.1099;  Loss pred: 0.1099; Loss self: 0.0000; time: 0.13s
Val loss: 0.6161 score: 0.7143 time: 0.07s
Test loss: 0.5876 score: 0.6939 time: 0.07s
Epoch 61/1000, LR 0.000268
Train loss: 0.1013;  Loss pred: 0.1013; Loss self: 0.0000; time: 0.13s
Val loss: 0.6099 score: 0.7143 time: 0.07s
Test loss: 0.5790 score: 0.6939 time: 0.07s
Epoch 62/1000, LR 0.000268
Train loss: 0.0934;  Loss pred: 0.0934; Loss self: 0.0000; time: 0.13s
Val loss: 0.6033 score: 0.7143 time: 0.07s
Test loss: 0.5699 score: 0.7347 time: 0.07s
Epoch 63/1000, LR 0.000268
Train loss: 0.0854;  Loss pred: 0.0854; Loss self: 0.0000; time: 0.13s
Val loss: 0.5966 score: 0.7143 time: 0.07s
Test loss: 0.5605 score: 0.7347 time: 0.07s
Epoch 64/1000, LR 0.000268
Train loss: 0.0771;  Loss pred: 0.0771; Loss self: 0.0000; time: 0.13s
Val loss: 0.5894 score: 0.7551 time: 0.07s
Test loss: 0.5505 score: 0.7347 time: 0.07s
Epoch 65/1000, LR 0.000268
Train loss: 0.0703;  Loss pred: 0.0703; Loss self: 0.0000; time: 0.13s
Val loss: 0.5820 score: 0.7551 time: 0.07s
Test loss: 0.5402 score: 0.7551 time: 0.07s
Epoch 66/1000, LR 0.000268
Train loss: 0.0645;  Loss pred: 0.0645; Loss self: 0.0000; time: 0.13s
Val loss: 0.5745 score: 0.7551 time: 0.07s
Test loss: 0.5295 score: 0.7755 time: 0.07s
Epoch 67/1000, LR 0.000268
Train loss: 0.0597;  Loss pred: 0.0597; Loss self: 0.0000; time: 0.13s
Val loss: 0.5667 score: 0.7551 time: 0.07s
Test loss: 0.5184 score: 0.7959 time: 0.07s
Epoch 68/1000, LR 0.000268
Train loss: 0.0524;  Loss pred: 0.0524; Loss self: 0.0000; time: 0.13s
Val loss: 0.5583 score: 0.7551 time: 0.07s
Test loss: 0.5064 score: 0.8163 time: 0.07s
Epoch 69/1000, LR 0.000268
Train loss: 0.0487;  Loss pred: 0.0487; Loss self: 0.0000; time: 0.13s
Val loss: 0.5498 score: 0.7551 time: 0.07s
Test loss: 0.4942 score: 0.8163 time: 0.07s
Epoch 70/1000, LR 0.000268
Train loss: 0.0443;  Loss pred: 0.0443; Loss self: 0.0000; time: 0.13s
Val loss: 0.5414 score: 0.7551 time: 0.07s
Test loss: 0.4819 score: 0.8163 time: 0.07s
Epoch 71/1000, LR 0.000268
Train loss: 0.0383;  Loss pred: 0.0383; Loss self: 0.0000; time: 0.13s
Val loss: 0.5327 score: 0.7551 time: 0.07s
Test loss: 0.4691 score: 0.8163 time: 0.07s
Epoch 72/1000, LR 0.000267
Train loss: 0.0358;  Loss pred: 0.0358; Loss self: 0.0000; time: 0.13s
Val loss: 0.5240 score: 0.7755 time: 0.07s
Test loss: 0.4563 score: 0.8367 time: 0.07s
Epoch 73/1000, LR 0.000267
Train loss: 0.0335;  Loss pred: 0.0335; Loss self: 0.0000; time: 0.13s
Val loss: 0.5153 score: 0.7755 time: 0.07s
Test loss: 0.4429 score: 0.8367 time: 0.07s
Epoch 74/1000, LR 0.000267
Train loss: 0.0294;  Loss pred: 0.0294; Loss self: 0.0000; time: 0.13s
Val loss: 0.5068 score: 0.7755 time: 0.07s
Test loss: 0.4297 score: 0.8571 time: 0.07s
Epoch 75/1000, LR 0.000267
Train loss: 0.0262;  Loss pred: 0.0262; Loss self: 0.0000; time: 0.13s
Val loss: 0.4983 score: 0.7755 time: 0.07s
Test loss: 0.4163 score: 0.8571 time: 0.07s
Epoch 76/1000, LR 0.000267
Train loss: 0.0254;  Loss pred: 0.0254; Loss self: 0.0000; time: 0.13s
Val loss: 0.4896 score: 0.7755 time: 0.07s
Test loss: 0.4024 score: 0.8571 time: 0.07s
Epoch 77/1000, LR 0.000267
Train loss: 0.0223;  Loss pred: 0.0223; Loss self: 0.0000; time: 0.13s
Val loss: 0.4814 score: 0.7755 time: 0.07s
Test loss: 0.3887 score: 0.8776 time: 0.07s
Epoch 78/1000, LR 0.000267
Train loss: 0.0205;  Loss pred: 0.0205; Loss self: 0.0000; time: 0.13s
Val loss: 0.4737 score: 0.7959 time: 0.07s
Test loss: 0.3752 score: 0.8776 time: 0.07s
Epoch 79/1000, LR 0.000267
Train loss: 0.0188;  Loss pred: 0.0188; Loss self: 0.0000; time: 0.13s
Val loss: 0.4663 score: 0.7959 time: 0.07s
Test loss: 0.3620 score: 0.8776 time: 0.07s
Epoch 80/1000, LR 0.000267
Train loss: 0.0176;  Loss pred: 0.0176; Loss self: 0.0000; time: 0.13s
Val loss: 0.4594 score: 0.7959 time: 0.07s
Test loss: 0.3491 score: 0.8776 time: 0.07s
Epoch 81/1000, LR 0.000267
Train loss: 0.0162;  Loss pred: 0.0162; Loss self: 0.0000; time: 0.13s
Val loss: 0.4531 score: 0.7959 time: 0.07s
Test loss: 0.3365 score: 0.8776 time: 0.07s
Epoch 82/1000, LR 0.000267
Train loss: 0.0146;  Loss pred: 0.0146; Loss self: 0.0000; time: 0.13s
Val loss: 0.4473 score: 0.7959 time: 0.07s
Test loss: 0.3242 score: 0.8776 time: 0.07s
Epoch 83/1000, LR 0.000266
Train loss: 0.0137;  Loss pred: 0.0137; Loss self: 0.0000; time: 0.13s
Val loss: 0.4422 score: 0.7755 time: 0.07s
Test loss: 0.3125 score: 0.8980 time: 0.07s
Epoch 84/1000, LR 0.000266
Train loss: 0.0130;  Loss pred: 0.0130; Loss self: 0.0000; time: 0.13s
Val loss: 0.4382 score: 0.7755 time: 0.07s
Test loss: 0.3017 score: 0.8980 time: 0.07s
Epoch 85/1000, LR 0.000266
Train loss: 0.0119;  Loss pred: 0.0119; Loss self: 0.0000; time: 0.13s
Val loss: 0.4348 score: 0.7959 time: 0.07s
Test loss: 0.2911 score: 0.8980 time: 0.07s
Epoch 86/1000, LR 0.000266
Train loss: 0.0111;  Loss pred: 0.0111; Loss self: 0.0000; time: 0.13s
Val loss: 0.4325 score: 0.7959 time: 0.07s
Test loss: 0.2817 score: 0.8980 time: 0.07s
Epoch 87/1000, LR 0.000266
Train loss: 0.0108;  Loss pred: 0.0108; Loss self: 0.0000; time: 0.13s
Val loss: 0.4308 score: 0.7959 time: 0.07s
Test loss: 0.2727 score: 0.8980 time: 0.08s
Epoch 88/1000, LR 0.000266
Train loss: 0.0092;  Loss pred: 0.0092; Loss self: 0.0000; time: 0.13s
Val loss: 0.4299 score: 0.7959 time: 0.07s
Test loss: 0.2644 score: 0.8980 time: 0.07s
Epoch 89/1000, LR 0.000266
Train loss: 0.0090;  Loss pred: 0.0090; Loss self: 0.0000; time: 0.13s
Val loss: 0.4298 score: 0.7959 time: 0.07s
Test loss: 0.2569 score: 0.8980 time: 0.07s
Epoch 90/1000, LR 0.000266
Train loss: 0.0082;  Loss pred: 0.0082; Loss self: 0.0000; time: 0.13s
Val loss: 0.4308 score: 0.7959 time: 0.17s
Test loss: 0.2504 score: 0.8980 time: 0.07s
     INFO: Early stopping counter 1 of 20
Epoch 91/1000, LR 0.000266
Train loss: 0.0081;  Loss pred: 0.0081; Loss self: 0.0000; time: 0.13s
Val loss: 0.4322 score: 0.7959 time: 0.07s
Test loss: 0.2445 score: 0.8980 time: 0.07s
     INFO: Early stopping counter 2 of 20
Epoch 92/1000, LR 0.000266
Train loss: 0.0074;  Loss pred: 0.0074; Loss self: 0.0000; time: 0.12s
Val loss: 0.4342 score: 0.7959 time: 0.07s
Test loss: 0.2393 score: 0.8980 time: 0.07s
     INFO: Early stopping counter 3 of 20
Epoch 93/1000, LR 0.000265
Train loss: 0.0068;  Loss pred: 0.0068; Loss self: 0.0000; time: 0.12s
Val loss: 0.4367 score: 0.8163 time: 0.07s
Test loss: 0.2347 score: 0.8980 time: 0.07s
     INFO: Early stopping counter 4 of 20
Epoch 94/1000, LR 0.000265
Train loss: 0.0069;  Loss pred: 0.0069; Loss self: 0.0000; time: 0.13s
Val loss: 0.4399 score: 0.8163 time: 0.07s
Test loss: 0.2310 score: 0.8980 time: 0.08s
     INFO: Early stopping counter 5 of 20
Epoch 95/1000, LR 0.000265
Train loss: 0.0063;  Loss pred: 0.0063; Loss self: 0.0000; time: 0.13s
Val loss: 0.4437 score: 0.8163 time: 0.07s
Test loss: 0.2279 score: 0.8980 time: 0.07s
     INFO: Early stopping counter 6 of 20
Epoch 96/1000, LR 0.000265
Train loss: 0.0060;  Loss pred: 0.0060; Loss self: 0.0000; time: 0.12s
Val loss: 0.4480 score: 0.8163 time: 0.07s
Test loss: 0.2255 score: 0.8980 time: 0.17s
     INFO: Early stopping counter 7 of 20
Epoch 97/1000, LR 0.000265
Train loss: 0.0055;  Loss pred: 0.0055; Loss self: 0.0000; time: 0.12s
Val loss: 0.4527 score: 0.8163 time: 0.07s
Test loss: 0.2236 score: 0.8980 time: 0.07s
     INFO: Early stopping counter 8 of 20
Epoch 98/1000, LR 0.000265
Train loss: 0.0054;  Loss pred: 0.0054; Loss self: 0.0000; time: 0.12s
Val loss: 0.4577 score: 0.8163 time: 0.07s
Test loss: 0.2220 score: 0.8980 time: 0.07s
     INFO: Early stopping counter 9 of 20
Epoch 99/1000, LR 0.000265
Train loss: 0.0053;  Loss pred: 0.0053; Loss self: 0.0000; time: 0.12s
Val loss: 0.4630 score: 0.8163 time: 0.07s
Test loss: 0.2208 score: 0.8980 time: 0.07s
     INFO: Early stopping counter 10 of 20
Epoch 100/1000, LR 0.000265
Train loss: 0.0048;  Loss pred: 0.0048; Loss self: 0.0000; time: 0.12s
Val loss: 0.4687 score: 0.8163 time: 0.07s
Test loss: 0.2203 score: 0.8980 time: 0.08s
     INFO: Early stopping counter 11 of 20
Epoch 101/1000, LR 0.000265
Train loss: 0.0049;  Loss pred: 0.0049; Loss self: 0.0000; time: 0.13s
Val loss: 0.4744 score: 0.8163 time: 0.07s
Test loss: 0.2200 score: 0.8980 time: 0.07s
     INFO: Early stopping counter 12 of 20
Epoch 102/1000, LR 0.000264
Train loss: 0.0044;  Loss pred: 0.0044; Loss self: 0.0000; time: 0.13s
Val loss: 0.4802 score: 0.8163 time: 0.07s
Test loss: 0.2200 score: 0.8980 time: 0.07s
     INFO: Early stopping counter 13 of 20
Epoch 103/1000, LR 0.000264
Train loss: 0.0043;  Loss pred: 0.0043; Loss self: 0.0000; time: 0.12s
Val loss: 0.4859 score: 0.8163 time: 0.07s
Test loss: 0.2202 score: 0.8980 time: 0.19s
     INFO: Early stopping counter 14 of 20
Epoch 104/1000, LR 0.000264
Train loss: 0.0042;  Loss pred: 0.0042; Loss self: 0.0000; time: 0.12s
Val loss: 0.4919 score: 0.8163 time: 0.07s
Test loss: 0.2208 score: 0.8980 time: 0.07s
     INFO: Early stopping counter 15 of 20
Epoch 105/1000, LR 0.000264
Train loss: 0.0042;  Loss pred: 0.0042; Loss self: 0.0000; time: 0.12s
Val loss: 0.4977 score: 0.8163 time: 0.07s
Test loss: 0.2214 score: 0.8980 time: 0.07s
     INFO: Early stopping counter 16 of 20
Epoch 106/1000, LR 0.000264
Train loss: 0.0037;  Loss pred: 0.0037; Loss self: 0.0000; time: 0.12s
Val loss: 0.5035 score: 0.8163 time: 0.07s
Test loss: 0.2221 score: 0.8980 time: 0.07s
     INFO: Early stopping counter 17 of 20
Epoch 107/1000, LR 0.000264
Train loss: 0.0038;  Loss pred: 0.0038; Loss self: 0.0000; time: 0.12s
Val loss: 0.5090 score: 0.7959 time: 0.07s
Test loss: 0.2229 score: 0.8980 time: 0.08s
     INFO: Early stopping counter 18 of 20
Epoch 108/1000, LR 0.000264
Train loss: 0.0034;  Loss pred: 0.0034; Loss self: 0.0000; time: 0.13s
Val loss: 0.5145 score: 0.7959 time: 0.07s
Test loss: 0.2240 score: 0.8980 time: 0.07s
     INFO: Early stopping counter 19 of 20
Epoch 109/1000, LR 0.000264
Train loss: 0.0035;  Loss pred: 0.0035; Loss self: 0.0000; time: 0.13s
Val loss: 0.5198 score: 0.7959 time: 0.07s
Test loss: 0.2249 score: 0.8980 time: 0.07s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 088,   Train_Loss: 0.0090,   Val_Loss: 0.4298,   Val_Precision: 0.8889,   Val_Recall: 0.6667,   Val_accuracy: 0.7619,   Val_Score: 0.7959,   Val_Loss: 0.4298,   Test_Precision: 0.9545,   Test_Recall: 0.8400,   Test_accuracy: 0.8936,   Test_Score: 0.8980,   Test_loss: 0.2569


[0.07760225399397314, 0.0784596879966557, 0.07772735494654626, 0.07700689788907766, 0.07777378102764487, 0.0779603230766952, 0.07779551693238318, 0.0779548849677667, 0.07823769003152847, 0.07846640201751143, 0.07808523194398731, 0.07817429502028972, 0.0779530800646171, 0.07782815105747432, 0.08294490992557257, 0.0780403739772737, 0.07782823697198182, 0.07824796705972403, 0.07813641999382526, 0.07772619696334004, 0.07746112893801183, 0.07731407403480262, 0.07768628909252584, 0.07809496496338397, 0.07791794196236879, 0.07864076097030193, 0.0780710430117324, 0.07759764301590621, 0.07792678603436798, 0.0779912460129708, 0.07968814601190388, 0.08715718297753483, 0.07851228700019419, 0.07883094600401819, 0.08171550906263292, 0.08350296202115715, 0.07892378407996148, 0.0793465820606798, 0.07869246893096715, 0.0789916239446029, 0.07830746693070978, 0.07748390594497323, 0.07829390501137823, 0.07840610202401876, 0.07973149698227644, 0.07821212895214558, 0.07919408800080419, 0.0779214350041002, 0.07819727703463286, 0.07833036791998893, 0.07855450606439263, 0.07801385899074376, 0.07828468293882906, 0.07781508995685726, 0.07814385404344648, 0.07788394193630666, 0.07812630396801978, 0.07792741106823087, 0.07844556507188827, 0.07882702199276537, 0.07806731609161943, 0.07788434904068708, 0.07920547400135547, 0.07782680203672498, 0.07912243809551, 0.07811837096232921, 0.07786378904711455, 0.07791512506082654, 0.07818781700916588, 0.07798479101620615, 0.078213767032139, 0.07759025297127664, 0.07841203198768198, 0.07816310203634202, 0.07835239393170923, 0.07835706591140479, 0.07831627898849547, 0.07810097199399024, 0.07807606796268374, 0.07817795709706843, 0.07871710497420281, 0.07871371903456748, 0.07815778499934822, 0.07884351501706988, 0.07814542402047664, 0.07991967699490488, 0.08033463195897639, 0.07836304209195077, 0.07799785397946835, 0.0780955640366301, 0.0776848669629544, 0.07648236502427608, 0.07630958897061646, 0.08025400503538549, 0.0776042869547382, 0.17882641800679266, 0.07661001000087708, 0.07661293796263635, 0.07668755610939115, 0.08144137298222631, 0.07944651797879487, 0.07810192997567356, 0.19924425997305661, 0.07648934598546475, 0.07657171599566936, 0.07730690704192966, 0.08107621397357434, 0.07939167006406933, 0.07867822598200291]
[0.0015837194692647578, 0.0016012181223807286, 0.0015862725499295155, 0.0015715693446750544, 0.0015872200209723444, 0.0015910270015652083, 0.0015876636108649628, 0.0015909160197503408, 0.0015966875516638464, 0.0016013551432145189, 0.00159357616212219, 0.00159539377592428, 0.0015908791849921858, 0.001588329613417843, 0.0016927532637871954, 0.001592660693413749, 0.0015883313667751392, 0.0015968972869331436, 0.0015946208162005155, 0.0015862489176191846, 0.001580839366081874, 0.0015778382456082167, 0.0015854344712760374, 0.0015937747951711015, 0.0015901620808646691, 0.0016049134891898353, 0.0015932865920761715, 0.0015836253676715555, 0.0015903425721299587, 0.0015916580818973634, 0.0016262886941204875, 0.0017787180199496904, 0.0016022915714325346, 0.0016087948164085345, 0.0016676634502578148, 0.0017041420820644315, 0.001610689471019622, 0.0016193180012383632, 0.0016059687536932072, 0.0016120739580531205, 0.0015981115700144852, 0.0015813042029586373, 0.001597834796150576, 0.0016001245311024238, 0.00162717340780156, 0.0015961658969825627, 0.0016162058775674325, 0.0015902333674306165, 0.0015958627966251603, 0.0015985789371426313, 0.0016031531849876046, 0.0015921195712396685, 0.0015976465905883483, 0.0015880630603440258, 0.0015947725314989078, 0.0015894682027817685, 0.0015944143666942812, 0.001590355327923079, 0.001600929899426291, 0.001608714734546232, 0.0015932105324820292, 0.0015894765110344303, 0.0016164382449256219, 0.0015883020823821425, 0.001614743634602245, 0.0015942524686189635, 0.0015890569193288684, 0.0015901045930780927, 0.0015956697348809363, 0.001591526347269513, 0.0015961993271865103, 0.0015834745504342172, 0.0016002455507690202, 0.0015951653476804495, 0.0015990284475859028, 0.0015991237941103019, 0.001598291407928479, 0.0015938973876324538, 0.0015933891420955866, 0.00159546851218507, 0.0016064715300857717, 0.0016064024292768873, 0.0015950568367213923, 0.001609051326878977, 0.0015948045718464621, 0.0016310138162225485, 0.0016394822848770692, 0.0015992457569785872, 0.0015917929383564967, 0.0015937870211557162, 0.0015854054482235592, 0.0015608645923321648, 0.001557338550420744, 0.0016378368374568466, 0.0015837609582599632, 0.0036495187348325035, 0.0015634695918546344, 0.001563529346176252, 0.0015650521654977786, 0.0016620688363719657, 0.001621357509771324, 0.0015939169382790522, 0.004066209387205237, 0.001561007060927852, 0.0015626880815442726, 0.001577691980447544, 0.0016546166117055987, 0.0016202381645728434, 0.0016056780812653657]
[631.4249584014082, 624.5245329307019, 630.4086898839888, 636.3066341223174, 630.0323753397412, 628.5248452830956, 629.8563456116486, 628.5686909840332, 626.2966094762489, 624.471095145469, 627.519426914798, 626.804501240239, 628.5832446823496, 629.592240522515, 590.7535500848487, 627.8801279741355, 629.5915455163145, 626.2143521581839, 627.1083318620463, 630.4180818612688, 632.5753403260129, 633.7785275413485, 630.7419310715186, 627.4412188157637, 628.8667124147737, 623.0865443749262, 627.6334747140031, 631.462478698687, 628.795341031896, 628.2756399590123, 614.8969759276409, 562.2026587599782, 624.1061351311649, 621.5833055904512, 599.6413723916559, 586.8055313724664, 620.8521369218149, 617.5439285151257, 622.6771210213925, 620.3189345032819, 625.7385396383407, 632.3893898017783, 625.8469288622016, 624.9513588239528, 614.5626490732043, 626.5012940637488, 618.7330549156956, 628.8385217420807, 626.6202847229366, 625.555596139308, 623.7707097264894, 628.0935289435405, 625.9206547248605, 629.697916267486, 627.0486732425169, 629.1412424922214, 627.1895317108264, 628.7902976411867, 624.6369690255393, 621.6142480239473, 627.6634378270905, 629.137953947618, 618.6441103699657, 629.6031536395114, 619.2933531806905, 627.2532234911699, 629.3040783097599, 628.8894481237992, 626.6961001642466, 628.3276439096596, 626.4881728540871, 631.5226220249527, 624.9040964490957, 626.8942598672376, 625.3797432495511, 625.3424554640975, 625.6681322563604, 627.3929600232182, 627.5930804227927, 626.7751399433464, 622.4822421512871, 622.5090187706851, 626.9369071860036, 621.4842145151867, 627.0360755501232, 613.1155910843321, 609.9486461209195, 625.2947651330797, 628.2224125409713, 627.4364056967045, 630.7534776801077, 640.6705648347437, 642.1211365568722, 610.5614290326693, 631.4084172769822, 274.0087317419664, 639.6031014672758, 639.5786573789533, 638.9563377153893, 601.6597977872219, 616.767118894734, 627.3852645544362, 245.9292930527894, 640.6120926868881, 639.9229710716065, 633.8372840789428, 604.369612226477, 617.1932138530005, 622.789842912935]
Elapsed: 0.08046016614849962~0.014973446578642713
Time per graph: 0.0016420442071122371~0.00030558054242127983
Speed: 618.2493088782722~50.14314938570983
Total Time: 0.0794
best val loss: 0.4298323690891266 test_score: 0.8980

Testing...
Test loss: 0.6925 score: 0.9388 time: 0.08s
test Score 0.9388
Epoch Time List: [0.39266364404466003, 0.2826130830217153, 0.2661858790088445, 0.26607032399624586, 0.27117240393999964, 0.2708440598798916, 0.26778734801337123, 0.2690558281028643, 0.27098136802669615, 0.26983992592431605, 0.27142235601786524, 0.2692768289707601, 0.270211192779243, 0.269793672952801, 0.28485040203668177, 0.2721725081792101, 0.2691799160093069, 0.268274859059602, 0.26955623901449144, 0.27032471098937094, 0.2674195229774341, 0.2685833720024675, 0.2702162259956822, 0.2714883011067286, 0.2693555069854483, 0.2729854299686849, 0.27150433498900384, 0.26968639495316893, 0.2683232070412487, 0.2692598729627207, 0.2724960200721398, 0.28927054489031434, 0.28893998311832547, 0.2735071318456903, 0.2851600961294025, 0.2897133269580081, 0.271541042951867, 0.2723916560644284, 0.2727885290514678, 0.27088151313364506, 0.26983908796682954, 0.27201939292717725, 0.2694371759425849, 0.2699380030389875, 0.2715802089078352, 0.27095535991247743, 0.2712447840021923, 0.27038088790141046, 0.27134644286707044, 0.27174408989958465, 0.2704614420654252, 0.26812433009035885, 0.26874385494738817, 0.2687817109981552, 0.27093324495945126, 0.2700783619657159, 0.2684284739661962, 0.2693769570905715, 0.2700551029993221, 0.2709773681126535, 0.27163065888453275, 0.2696436190744862, 0.27138554700650275, 0.27006561285816133, 0.27060042798984796, 0.27011186501476914, 0.2702617389149964, 0.2691751678939909, 0.2712113920133561, 0.27252779400441796, 0.27254804503172636, 0.27129908406641334, 0.27133847202640027, 0.272697635111399, 0.27234503091312945, 0.27006233180873096, 0.27116715896409005, 0.2720064801396802, 0.2701811820734292, 0.2722073720069602, 0.2738324700621888, 0.272273194976151, 0.2707813649903983, 0.2720926101319492, 0.2736746690934524, 0.2736646729754284, 0.2762101950356737, 0.27332384802866727, 0.26844331598840654, 0.3799686769489199, 0.2684880818706006, 0.2653484789188951, 0.2653702151728794, 0.2757150470279157, 0.2720365321729332, 0.369407067890279, 0.26495877804700285, 0.2633576529333368, 0.26344164297915995, 0.270703605026938, 0.27427524991799146, 0.2706836609868333, 0.3907290620263666, 0.26552566105965525, 0.2639659789856523, 0.2651111129671335, 0.2699091621907428, 0.27730480197351426, 0.26941350707784295]
Total Epoch List: [109]
Total Time List: [0.07938399794511497]
========================k_idx:1========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: I2BGNNA
I2BGNN(
  (gcs): ModuleList(
    (0): GCNConv(14887, 64)
    (1): GCNConv(64, 64)
  )
  (bns): ModuleList(
    (0-1): 2 x BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  )
  (drops): ModuleList(
    (0-1): 2 x Dropout(p=0.2, inplace=False)
  )
  (lin1): Linear(in_features=64, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=64, bias=True)
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=64, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=64, out_features=2, bias=True)
  )
)
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7d1ecd779480>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.6991;  Loss pred: 0.6991; Loss self: 0.0000; time: 0.11s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6930 score: 0.5102 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6939 score: 0.4898 time: 0.08s
Epoch 2/1000, LR 0.000000
Train loss: 0.6983;  Loss pred: 0.6983; Loss self: 0.0000; time: 0.11s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6930 score: 0.5102 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6939 score: 0.4898 time: 0.07s
     INFO: Early stopping counter 1 of 20
Epoch 3/1000, LR 0.000030
Train loss: 0.6962;  Loss pred: 0.6962; Loss self: 0.0000; time: 0.11s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6930 score: 0.5102 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6940 score: 0.4898 time: 0.07s
     INFO: Early stopping counter 2 of 20
Epoch 4/1000, LR 0.000060
Train loss: 0.6960;  Loss pred: 0.6960; Loss self: 0.0000; time: 0.11s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6930 score: 0.5102 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6939 score: 0.4898 time: 0.08s
Epoch 5/1000, LR 0.000090
Train loss: 0.6949;  Loss pred: 0.6949; Loss self: 0.0000; time: 0.11s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6929 score: 0.5102 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6939 score: 0.4898 time: 0.07s
Epoch 6/1000, LR 0.000120
Train loss: 0.6927;  Loss pred: 0.6927; Loss self: 0.0000; time: 0.11s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6929 score: 0.5102 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6938 score: 0.4898 time: 0.07s
Epoch 7/1000, LR 0.000150
Train loss: 0.6899;  Loss pred: 0.6899; Loss self: 0.0000; time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6929 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6937 score: 0.4898 time: 0.07s
Epoch 8/1000, LR 0.000180
Train loss: 0.6869;  Loss pred: 0.6869; Loss self: 0.0000; time: 0.11s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6929 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6936 score: 0.4898 time: 0.07s
Epoch 9/1000, LR 0.000210
Train loss: 0.6826;  Loss pred: 0.6826; Loss self: 0.0000; time: 0.10s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6929 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6935 score: 0.4898 time: 0.07s
Epoch 10/1000, LR 0.000240
Train loss: 0.6770;  Loss pred: 0.6770; Loss self: 0.0000; time: 0.10s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6929 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6934 score: 0.4898 time: 0.08s
     INFO: Early stopping counter 1 of 20
Epoch 11/1000, LR 0.000270
Train loss: 0.6749;  Loss pred: 0.6749; Loss self: 0.0000; time: 0.11s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6929 score: 0.5102 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6932 score: 0.4898 time: 0.07s
     INFO: Early stopping counter 2 of 20
Epoch 12/1000, LR 0.000270
Train loss: 0.6721;  Loss pred: 0.6721; Loss self: 0.0000; time: 0.11s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6929 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6931 score: 0.4898 time: 0.07s
     INFO: Early stopping counter 3 of 20
Epoch 13/1000, LR 0.000270
Train loss: 0.6638;  Loss pred: 0.6638; Loss self: 0.0000; time: 0.11s
Val loss: 0.6930 score: 0.5306 time: 0.19s
Test loss: 0.6930 score: 0.5714 time: 0.07s
     INFO: Early stopping counter 4 of 20
Epoch 14/1000, LR 0.000270
Train loss: 0.6602;  Loss pred: 0.6602; Loss self: 0.0000; time: 0.11s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6930 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6929 score: 0.5102 time: 0.07s
     INFO: Early stopping counter 5 of 20
Epoch 15/1000, LR 0.000270
Train loss: 0.6544;  Loss pred: 0.6544; Loss self: 0.0000; time: 0.11s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6931 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6928 score: 0.5102 time: 0.07s
     INFO: Early stopping counter 6 of 20
Epoch 16/1000, LR 0.000270
Train loss: 0.6520;  Loss pred: 0.6520; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6932 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6927 score: 0.5102 time: 0.07s
     INFO: Early stopping counter 7 of 20
Epoch 17/1000, LR 0.000270
Train loss: 0.6440;  Loss pred: 0.6440; Loss self: 0.0000; time: 0.11s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6933 score: 0.4898 time: 0.09s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6925 score: 0.5102 time: 0.08s
     INFO: Early stopping counter 8 of 20
Epoch 18/1000, LR 0.000270
Train loss: 0.6387;  Loss pred: 0.6387; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6934 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6925 score: 0.5102 time: 0.07s
     INFO: Early stopping counter 9 of 20
Epoch 19/1000, LR 0.000270
Train loss: 0.6323;  Loss pred: 0.6323; Loss self: 0.0000; time: 0.11s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6936 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6924 score: 0.5102 time: 0.08s
     INFO: Early stopping counter 10 of 20
Epoch 20/1000, LR 0.000270
Train loss: 0.6297;  Loss pred: 0.6297; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6937 score: 0.4898 time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6923 score: 0.5102 time: 0.07s
     INFO: Early stopping counter 11 of 20
Epoch 21/1000, LR 0.000270
Train loss: 0.6193;  Loss pred: 0.6193; Loss self: 0.0000; time: 0.11s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6939 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6923 score: 0.5102 time: 0.07s
     INFO: Early stopping counter 12 of 20
Epoch 22/1000, LR 0.000270
Train loss: 0.6136;  Loss pred: 0.6136; Loss self: 0.0000; time: 0.11s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6942 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6923 score: 0.5102 time: 0.07s
     INFO: Early stopping counter 13 of 20
Epoch 23/1000, LR 0.000270
Train loss: 0.6048;  Loss pred: 0.6048; Loss self: 0.0000; time: 0.11s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6945 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6924 score: 0.5102 time: 0.07s
     INFO: Early stopping counter 14 of 20
Epoch 24/1000, LR 0.000270
Train loss: 0.5962;  Loss pred: 0.5962; Loss self: 0.0000; time: 0.11s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6947 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6924 score: 0.5102 time: 0.08s
     INFO: Early stopping counter 15 of 20
Epoch 25/1000, LR 0.000270
Train loss: 0.5914;  Loss pred: 0.5914; Loss self: 0.0000; time: 0.11s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6950 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6924 score: 0.5102 time: 0.07s
     INFO: Early stopping counter 16 of 20
Epoch 26/1000, LR 0.000270
Train loss: 0.5828;  Loss pred: 0.5828; Loss self: 0.0000; time: 0.11s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6953 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6925 score: 0.5102 time: 0.07s
     INFO: Early stopping counter 17 of 20
Epoch 27/1000, LR 0.000270
Train loss: 0.5751;  Loss pred: 0.5751; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6956 score: 0.4898 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6925 score: 0.5102 time: 0.07s
     INFO: Early stopping counter 18 of 20
Epoch 28/1000, LR 0.000270
Train loss: 0.5653;  Loss pred: 0.5653; Loss self: 0.0000; time: 0.11s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6958 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6926 score: 0.5102 time: 0.07s
     INFO: Early stopping counter 19 of 20
Epoch 29/1000, LR 0.000270
Train loss: 0.5512;  Loss pred: 0.5512; Loss self: 0.0000; time: 0.11s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6961 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6926 score: 0.5102 time: 0.07s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 008,   Train_Loss: 0.6826,   Val_Loss: 0.6929,   Val_Precision: 0.5102,   Val_Recall: 1.0000,   Val_accuracy: 0.6757,   Val_Score: 0.5102,   Val_Loss: 0.6929,   Test_Precision: 0.4898,   Test_Recall: 1.0000,   Test_accuracy: 0.6575,   Test_Score: 0.4898,   Test_loss: 0.6935


[0.07760225399397314, 0.0784596879966557, 0.07772735494654626, 0.07700689788907766, 0.07777378102764487, 0.0779603230766952, 0.07779551693238318, 0.0779548849677667, 0.07823769003152847, 0.07846640201751143, 0.07808523194398731, 0.07817429502028972, 0.0779530800646171, 0.07782815105747432, 0.08294490992557257, 0.0780403739772737, 0.07782823697198182, 0.07824796705972403, 0.07813641999382526, 0.07772619696334004, 0.07746112893801183, 0.07731407403480262, 0.07768628909252584, 0.07809496496338397, 0.07791794196236879, 0.07864076097030193, 0.0780710430117324, 0.07759764301590621, 0.07792678603436798, 0.0779912460129708, 0.07968814601190388, 0.08715718297753483, 0.07851228700019419, 0.07883094600401819, 0.08171550906263292, 0.08350296202115715, 0.07892378407996148, 0.0793465820606798, 0.07869246893096715, 0.0789916239446029, 0.07830746693070978, 0.07748390594497323, 0.07829390501137823, 0.07840610202401876, 0.07973149698227644, 0.07821212895214558, 0.07919408800080419, 0.0779214350041002, 0.07819727703463286, 0.07833036791998893, 0.07855450606439263, 0.07801385899074376, 0.07828468293882906, 0.07781508995685726, 0.07814385404344648, 0.07788394193630666, 0.07812630396801978, 0.07792741106823087, 0.07844556507188827, 0.07882702199276537, 0.07806731609161943, 0.07788434904068708, 0.07920547400135547, 0.07782680203672498, 0.07912243809551, 0.07811837096232921, 0.07786378904711455, 0.07791512506082654, 0.07818781700916588, 0.07798479101620615, 0.078213767032139, 0.07759025297127664, 0.07841203198768198, 0.07816310203634202, 0.07835239393170923, 0.07835706591140479, 0.07831627898849547, 0.07810097199399024, 0.07807606796268374, 0.07817795709706843, 0.07871710497420281, 0.07871371903456748, 0.07815778499934822, 0.07884351501706988, 0.07814542402047664, 0.07991967699490488, 0.08033463195897639, 0.07836304209195077, 0.07799785397946835, 0.0780955640366301, 0.0776848669629544, 0.07648236502427608, 0.07630958897061646, 0.08025400503538549, 0.0776042869547382, 0.17882641800679266, 0.07661001000087708, 0.07661293796263635, 0.07668755610939115, 0.08144137298222631, 0.07944651797879487, 0.07810192997567356, 0.19924425997305661, 0.07648934598546475, 0.07657171599566936, 0.07730690704192966, 0.08107621397357434, 0.07939167006406933, 0.07867822598200291, 0.08029841806273907, 0.0792598850093782, 0.0788534430321306, 0.08111755806021392, 0.07852976804133505, 0.07857903291005641, 0.07727572694420815, 0.07720982294995338, 0.07716157496906817, 0.0809061749605462, 0.0787048100028187, 0.07752932806033641, 0.07847286004107445, 0.07767901394981891, 0.07844037096947432, 0.07992484292481095, 0.0865763520123437, 0.07937121402937919, 0.08062870800495148, 0.07830569101497531, 0.07897658401634544, 0.07983776892069727, 0.07792506704572588, 0.08169297990389168, 0.07891623408067971, 0.07816857402212918, 0.07908851804677397, 0.07893224095460027, 0.07783203001599759]
[0.0015837194692647578, 0.0016012181223807286, 0.0015862725499295155, 0.0015715693446750544, 0.0015872200209723444, 0.0015910270015652083, 0.0015876636108649628, 0.0015909160197503408, 0.0015966875516638464, 0.0016013551432145189, 0.00159357616212219, 0.00159539377592428, 0.0015908791849921858, 0.001588329613417843, 0.0016927532637871954, 0.001592660693413749, 0.0015883313667751392, 0.0015968972869331436, 0.0015946208162005155, 0.0015862489176191846, 0.001580839366081874, 0.0015778382456082167, 0.0015854344712760374, 0.0015937747951711015, 0.0015901620808646691, 0.0016049134891898353, 0.0015932865920761715, 0.0015836253676715555, 0.0015903425721299587, 0.0015916580818973634, 0.0016262886941204875, 0.0017787180199496904, 0.0016022915714325346, 0.0016087948164085345, 0.0016676634502578148, 0.0017041420820644315, 0.001610689471019622, 0.0016193180012383632, 0.0016059687536932072, 0.0016120739580531205, 0.0015981115700144852, 0.0015813042029586373, 0.001597834796150576, 0.0016001245311024238, 0.00162717340780156, 0.0015961658969825627, 0.0016162058775674325, 0.0015902333674306165, 0.0015958627966251603, 0.0015985789371426313, 0.0016031531849876046, 0.0015921195712396685, 0.0015976465905883483, 0.0015880630603440258, 0.0015947725314989078, 0.0015894682027817685, 0.0015944143666942812, 0.001590355327923079, 0.001600929899426291, 0.001608714734546232, 0.0015932105324820292, 0.0015894765110344303, 0.0016164382449256219, 0.0015883020823821425, 0.001614743634602245, 0.0015942524686189635, 0.0015890569193288684, 0.0015901045930780927, 0.0015956697348809363, 0.001591526347269513, 0.0015961993271865103, 0.0015834745504342172, 0.0016002455507690202, 0.0015951653476804495, 0.0015990284475859028, 0.0015991237941103019, 0.001598291407928479, 0.0015938973876324538, 0.0015933891420955866, 0.00159546851218507, 0.0016064715300857717, 0.0016064024292768873, 0.0015950568367213923, 0.001609051326878977, 0.0015948045718464621, 0.0016310138162225485, 0.0016394822848770692, 0.0015992457569785872, 0.0015917929383564967, 0.0015937870211557162, 0.0015854054482235592, 0.0015608645923321648, 0.001557338550420744, 0.0016378368374568466, 0.0015837609582599632, 0.0036495187348325035, 0.0015634695918546344, 0.001563529346176252, 0.0015650521654977786, 0.0016620688363719657, 0.001621357509771324, 0.0015939169382790522, 0.004066209387205237, 0.001561007060927852, 0.0015626880815442726, 0.001577691980447544, 0.0016546166117055987, 0.0016202381645728434, 0.0016056780812653657, 0.0016387432257701851, 0.0016175486736607794, 0.0016092539394312367, 0.0016554603685757943, 0.0016026483273741845, 0.001603653732858294, 0.0015770556519226152, 0.0015757106724480282, 0.0015747260197769015, 0.001651146427766249, 0.0016062206123024225, 0.0015822311849048247, 0.0016014869396137645, 0.001585285998975896, 0.0016008238973362105, 0.0016311192433634888, 0.0017668643267825246, 0.0016198206944771263, 0.0016454838368357445, 0.0015980753268362308, 0.0016117670207417437, 0.0016293422228713728, 0.0015903074907290997, 0.0016672036715079934, 0.0016105353894016268, 0.0015952770208597792, 0.0016140513887096728, 0.0016108620602979648, 0.0015884087758366854]
[631.4249584014082, 624.5245329307019, 630.4086898839888, 636.3066341223174, 630.0323753397412, 628.5248452830956, 629.8563456116486, 628.5686909840332, 626.2966094762489, 624.471095145469, 627.519426914798, 626.804501240239, 628.5832446823496, 629.592240522515, 590.7535500848487, 627.8801279741355, 629.5915455163145, 626.2143521581839, 627.1083318620463, 630.4180818612688, 632.5753403260129, 633.7785275413485, 630.7419310715186, 627.4412188157637, 628.8667124147737, 623.0865443749262, 627.6334747140031, 631.462478698687, 628.795341031896, 628.2756399590123, 614.8969759276409, 562.2026587599782, 624.1061351311649, 621.5833055904512, 599.6413723916559, 586.8055313724664, 620.8521369218149, 617.5439285151257, 622.6771210213925, 620.3189345032819, 625.7385396383407, 632.3893898017783, 625.8469288622016, 624.9513588239528, 614.5626490732043, 626.5012940637488, 618.7330549156956, 628.8385217420807, 626.6202847229366, 625.555596139308, 623.7707097264894, 628.0935289435405, 625.9206547248605, 629.697916267486, 627.0486732425169, 629.1412424922214, 627.1895317108264, 628.7902976411867, 624.6369690255393, 621.6142480239473, 627.6634378270905, 629.137953947618, 618.6441103699657, 629.6031536395114, 619.2933531806905, 627.2532234911699, 629.3040783097599, 628.8894481237992, 626.6961001642466, 628.3276439096596, 626.4881728540871, 631.5226220249527, 624.9040964490957, 626.8942598672376, 625.3797432495511, 625.3424554640975, 625.6681322563604, 627.3929600232182, 627.5930804227927, 626.7751399433464, 622.4822421512871, 622.5090187706851, 626.9369071860036, 621.4842145151867, 627.0360755501232, 613.1155910843321, 609.9486461209195, 625.2947651330797, 628.2224125409713, 627.4364056967045, 630.7534776801077, 640.6705648347437, 642.1211365568722, 610.5614290326693, 631.4084172769822, 274.0087317419664, 639.6031014672758, 639.5786573789533, 638.9563377153893, 601.6597977872219, 616.767118894734, 627.3852645544362, 245.9292930527894, 640.6120926868881, 639.9229710716065, 633.8372840789428, 604.369612226477, 617.1932138530005, 622.789842912935, 610.223727716717, 618.2194182366303, 621.4059667633518, 604.0615764545955, 623.9672066038485, 623.5760123961651, 634.0930320251432, 634.6342748611314, 635.0311021987651, 605.6398046736828, 622.5794840016148, 632.0188917652717, 624.4197035045275, 630.8010041380583, 624.6783307420708, 613.0759624525831, 565.9744128860243, 617.3522806626429, 607.7239882969583, 625.7527309302354, 620.4370651161448, 613.7446056223292, 628.8092119477695, 599.8067405258863, 620.9115345000502, 626.8503757805319, 619.5589601390782, 620.7856182391109, 629.5608631809879]
Elapsed: 0.08019096161697761~0.013343851407617659
Time per graph: 0.001636550237081176~0.0002723234981146461
Speed: 618.441076478939~44.99765245747052
Total Time: 0.0785
best val loss: 0.6928841471672058 test_score: 0.4898

Testing...
Test loss: 0.6930 score: 0.5714 time: 0.07s
test Score 0.5714
Epoch Time List: [0.39266364404466003, 0.2826130830217153, 0.2661858790088445, 0.26607032399624586, 0.27117240393999964, 0.2708440598798916, 0.26778734801337123, 0.2690558281028643, 0.27098136802669615, 0.26983992592431605, 0.27142235601786524, 0.2692768289707601, 0.270211192779243, 0.269793672952801, 0.28485040203668177, 0.2721725081792101, 0.2691799160093069, 0.268274859059602, 0.26955623901449144, 0.27032471098937094, 0.2674195229774341, 0.2685833720024675, 0.2702162259956822, 0.2714883011067286, 0.2693555069854483, 0.2729854299686849, 0.27150433498900384, 0.26968639495316893, 0.2683232070412487, 0.2692598729627207, 0.2724960200721398, 0.28927054489031434, 0.28893998311832547, 0.2735071318456903, 0.2851600961294025, 0.2897133269580081, 0.271541042951867, 0.2723916560644284, 0.2727885290514678, 0.27088151313364506, 0.26983908796682954, 0.27201939292717725, 0.2694371759425849, 0.2699380030389875, 0.2715802089078352, 0.27095535991247743, 0.2712447840021923, 0.27038088790141046, 0.27134644286707044, 0.27174408989958465, 0.2704614420654252, 0.26812433009035885, 0.26874385494738817, 0.2687817109981552, 0.27093324495945126, 0.2700783619657159, 0.2684284739661962, 0.2693769570905715, 0.2700551029993221, 0.2709773681126535, 0.27163065888453275, 0.2696436190744862, 0.27138554700650275, 0.27006561285816133, 0.27060042798984796, 0.27011186501476914, 0.2702617389149964, 0.2691751678939909, 0.2712113920133561, 0.27252779400441796, 0.27254804503172636, 0.27129908406641334, 0.27133847202640027, 0.272697635111399, 0.27234503091312945, 0.27006233180873096, 0.27116715896409005, 0.2720064801396802, 0.2701811820734292, 0.2722073720069602, 0.2738324700621888, 0.272273194976151, 0.2707813649903983, 0.2720926101319492, 0.2736746690934524, 0.2736646729754284, 0.2762101950356737, 0.27332384802866727, 0.26844331598840654, 0.3799686769489199, 0.2684880818706006, 0.2653484789188951, 0.2653702151728794, 0.2757150470279157, 0.2720365321729332, 0.369407067890279, 0.26495877804700285, 0.2633576529333368, 0.26344164297915995, 0.270703605026938, 0.27427524991799146, 0.2706836609868333, 0.3907290620263666, 0.26552566105965525, 0.2639659789856523, 0.2651111129671335, 0.2699091621907428, 0.27730480197351426, 0.26941350707784295, 0.26380831410642713, 0.2594936670502648, 0.25834672595374286, 0.26552850496955216, 0.2592404600000009, 0.2554450350580737, 0.3793773438083008, 0.2527334709884599, 0.25001311395317316, 0.2535846650134772, 0.2608203259296715, 0.2520572489593178, 0.36974470608402044, 0.2542982498416677, 0.25602538394741714, 0.2715981110231951, 0.27865822706371546, 0.2780263789463788, 0.2581089740851894, 0.3133282830240205, 0.2530697430483997, 0.2567962569883093, 0.2549674389883876, 0.26071096991654485, 0.26189130707643926, 0.2579105868935585, 0.3486081169685349, 0.25563551497180015, 0.2527317369822413]
Total Epoch List: [109, 29]
Total Time List: [0.07938399794511497, 0.0784644220257178]
========================k_idx:2========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: I2BGNNA
I2BGNN(
  (gcs): ModuleList(
    (0): GCNConv(14887, 64)
    (1): GCNConv(64, 64)
  )
  (bns): ModuleList(
    (0-1): 2 x BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  )
  (drops): ModuleList(
    (0-1): 2 x Dropout(p=0.2, inplace=False)
  )
  (lin1): Linear(in_features=64, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=64, bias=True)
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=64, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=64, out_features=2, bias=True)
  )
)
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7d1ecd77b940>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.6972;  Loss pred: 0.6972; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6930 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6934 score: 0.5000 time: 0.07s
Epoch 2/1000, LR 0.000000
Train loss: 0.6966;  Loss pred: 0.6966; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6930 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6934 score: 0.5000 time: 0.07s
Epoch 3/1000, LR 0.000030
Train loss: 0.6954;  Loss pred: 0.6954; Loss self: 0.0000; time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6930 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6933 score: 0.5000 time: 0.07s
     INFO: Early stopping counter 1 of 20
Epoch 4/1000, LR 0.000060
Train loss: 0.6955;  Loss pred: 0.6955; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6929 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6933 score: 0.5000 time: 0.07s
Epoch 5/1000, LR 0.000090
Train loss: 0.6956;  Loss pred: 0.6956; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6929 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6933 score: 0.5000 time: 0.07s
Epoch 6/1000, LR 0.000120
Train loss: 0.6924;  Loss pred: 0.6924; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6929 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6932 score: 0.5000 time: 0.07s
Epoch 7/1000, LR 0.000150
Train loss: 0.6912;  Loss pred: 0.6912; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6929 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6932 score: 0.5000 time: 0.07s
Epoch 8/1000, LR 0.000180
Train loss: 0.6887;  Loss pred: 0.6887; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6929 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6932 score: 0.5000 time: 0.07s
     INFO: Early stopping counter 1 of 20
Epoch 9/1000, LR 0.000210
Train loss: 0.6867;  Loss pred: 0.6867; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6929 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6931 score: 0.5000 time: 0.07s
     INFO: Early stopping counter 2 of 20
Epoch 10/1000, LR 0.000240
Train loss: 0.6836;  Loss pred: 0.6836; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6929 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6931 score: 0.5000 time: 0.07s
     INFO: Early stopping counter 3 of 20
Epoch 11/1000, LR 0.000270
Train loss: 0.6795;  Loss pred: 0.6795; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6929 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6930 score: 0.5000 time: 0.07s
     INFO: Early stopping counter 4 of 20
Epoch 12/1000, LR 0.000270
Train loss: 0.6801;  Loss pred: 0.6801; Loss self: 0.0000; time: 0.13s
Val loss: 0.6930 score: 0.6122 time: 0.07s
Test loss: 0.6930 score: 0.7083 time: 0.07s
     INFO: Early stopping counter 5 of 20
Epoch 13/1000, LR 0.000270
Train loss: 0.6754;  Loss pred: 0.6754; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6930 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6930 score: 0.5000 time: 0.07s
     INFO: Early stopping counter 6 of 20
Epoch 14/1000, LR 0.000270
Train loss: 0.6700;  Loss pred: 0.6700; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6931 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6930 score: 0.5000 time: 0.07s
     INFO: Early stopping counter 7 of 20
Epoch 15/1000, LR 0.000270
Train loss: 0.6670;  Loss pred: 0.6670; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6932 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6930 score: 0.5000 time: 0.07s
     INFO: Early stopping counter 8 of 20
Epoch 16/1000, LR 0.000270
Train loss: 0.6647;  Loss pred: 0.6647; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6933 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6930 score: 0.5000 time: 0.07s
     INFO: Early stopping counter 9 of 20
Epoch 17/1000, LR 0.000270
Train loss: 0.6616;  Loss pred: 0.6616; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6935 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6930 score: 0.5000 time: 0.08s
     INFO: Early stopping counter 10 of 20
Epoch 18/1000, LR 0.000270
Train loss: 0.6543;  Loss pred: 0.6543; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6936 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6931 score: 0.5000 time: 0.07s
     INFO: Early stopping counter 11 of 20
Epoch 19/1000, LR 0.000270
Train loss: 0.6537;  Loss pred: 0.6537; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6938 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6932 score: 0.5000 time: 0.07s
     INFO: Early stopping counter 12 of 20
Epoch 20/1000, LR 0.000270
Train loss: 0.6455;  Loss pred: 0.6455; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6940 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6933 score: 0.5000 time: 0.07s
     INFO: Early stopping counter 13 of 20
Epoch 21/1000, LR 0.000270
Train loss: 0.6388;  Loss pred: 0.6388; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6944 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6935 score: 0.5000 time: 0.07s
     INFO: Early stopping counter 14 of 20
Epoch 22/1000, LR 0.000270
Train loss: 0.6317;  Loss pred: 0.6317; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6947 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6938 score: 0.5000 time: 0.07s
     INFO: Early stopping counter 15 of 20
Epoch 23/1000, LR 0.000270
Train loss: 0.6258;  Loss pred: 0.6258; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6951 score: 0.4898 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6940 score: 0.5000 time: 0.07s
     INFO: Early stopping counter 16 of 20
Epoch 24/1000, LR 0.000270
Train loss: 0.6207;  Loss pred: 0.6207; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6955 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6943 score: 0.5000 time: 0.07s
     INFO: Early stopping counter 17 of 20
Epoch 25/1000, LR 0.000270
Train loss: 0.6111;  Loss pred: 0.6111; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6958 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6946 score: 0.5000 time: 0.07s
     INFO: Early stopping counter 18 of 20
Epoch 26/1000, LR 0.000270
Train loss: 0.5995;  Loss pred: 0.5995; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6963 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6950 score: 0.5000 time: 0.07s
     INFO: Early stopping counter 19 of 20
Epoch 27/1000, LR 0.000270
Train loss: 0.5975;  Loss pred: 0.5975; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6969 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6954 score: 0.5000 time: 0.07s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 006,   Train_Loss: 0.6912,   Val_Loss: 0.6929,   Val_Precision: 0.5102,   Val_Recall: 1.0000,   Val_accuracy: 0.6757,   Val_Score: 0.5102,   Val_Loss: 0.6929,   Test_Precision: 0.5000,   Test_Recall: 1.0000,   Test_accuracy: 0.6667,   Test_Score: 0.5000,   Test_loss: 0.6932


[0.07760225399397314, 0.0784596879966557, 0.07772735494654626, 0.07700689788907766, 0.07777378102764487, 0.0779603230766952, 0.07779551693238318, 0.0779548849677667, 0.07823769003152847, 0.07846640201751143, 0.07808523194398731, 0.07817429502028972, 0.0779530800646171, 0.07782815105747432, 0.08294490992557257, 0.0780403739772737, 0.07782823697198182, 0.07824796705972403, 0.07813641999382526, 0.07772619696334004, 0.07746112893801183, 0.07731407403480262, 0.07768628909252584, 0.07809496496338397, 0.07791794196236879, 0.07864076097030193, 0.0780710430117324, 0.07759764301590621, 0.07792678603436798, 0.0779912460129708, 0.07968814601190388, 0.08715718297753483, 0.07851228700019419, 0.07883094600401819, 0.08171550906263292, 0.08350296202115715, 0.07892378407996148, 0.0793465820606798, 0.07869246893096715, 0.0789916239446029, 0.07830746693070978, 0.07748390594497323, 0.07829390501137823, 0.07840610202401876, 0.07973149698227644, 0.07821212895214558, 0.07919408800080419, 0.0779214350041002, 0.07819727703463286, 0.07833036791998893, 0.07855450606439263, 0.07801385899074376, 0.07828468293882906, 0.07781508995685726, 0.07814385404344648, 0.07788394193630666, 0.07812630396801978, 0.07792741106823087, 0.07844556507188827, 0.07882702199276537, 0.07806731609161943, 0.07788434904068708, 0.07920547400135547, 0.07782680203672498, 0.07912243809551, 0.07811837096232921, 0.07786378904711455, 0.07791512506082654, 0.07818781700916588, 0.07798479101620615, 0.078213767032139, 0.07759025297127664, 0.07841203198768198, 0.07816310203634202, 0.07835239393170923, 0.07835706591140479, 0.07831627898849547, 0.07810097199399024, 0.07807606796268374, 0.07817795709706843, 0.07871710497420281, 0.07871371903456748, 0.07815778499934822, 0.07884351501706988, 0.07814542402047664, 0.07991967699490488, 0.08033463195897639, 0.07836304209195077, 0.07799785397946835, 0.0780955640366301, 0.0776848669629544, 0.07648236502427608, 0.07630958897061646, 0.08025400503538549, 0.0776042869547382, 0.17882641800679266, 0.07661001000087708, 0.07661293796263635, 0.07668755610939115, 0.08144137298222631, 0.07944651797879487, 0.07810192997567356, 0.19924425997305661, 0.07648934598546475, 0.07657171599566936, 0.07730690704192966, 0.08107621397357434, 0.07939167006406933, 0.07867822598200291, 0.08029841806273907, 0.0792598850093782, 0.0788534430321306, 0.08111755806021392, 0.07852976804133505, 0.07857903291005641, 0.07727572694420815, 0.07720982294995338, 0.07716157496906817, 0.0809061749605462, 0.0787048100028187, 0.07752932806033641, 0.07847286004107445, 0.07767901394981891, 0.07844037096947432, 0.07992484292481095, 0.0865763520123437, 0.07937121402937919, 0.08062870800495148, 0.07830569101497531, 0.07897658401634544, 0.07983776892069727, 0.07792506704572588, 0.08169297990389168, 0.07891623408067971, 0.07816857402212918, 0.07908851804677397, 0.07893224095460027, 0.07783203001599759, 0.07305579097010195, 0.07374738005455583, 0.07239451794885099, 0.07421133702155203, 0.07352768292184919, 0.07425701897591352, 0.07729822502005845, 0.07669624104164541, 0.07679956103675067, 0.07457444199826568, 0.07503627997357398, 0.07438196404837072, 0.07497552898712456, 0.07709251495543867, 0.07604696706403047, 0.07964564394205809, 0.08354186697397381, 0.07486660999711603, 0.07516769308131188, 0.07519983302336186, 0.0762272709980607, 0.07446444209199399, 0.07461387396324426, 0.07517331000417471, 0.0754366850014776, 0.07447832403704524, 0.07632561295758933]
[0.0015837194692647578, 0.0016012181223807286, 0.0015862725499295155, 0.0015715693446750544, 0.0015872200209723444, 0.0015910270015652083, 0.0015876636108649628, 0.0015909160197503408, 0.0015966875516638464, 0.0016013551432145189, 0.00159357616212219, 0.00159539377592428, 0.0015908791849921858, 0.001588329613417843, 0.0016927532637871954, 0.001592660693413749, 0.0015883313667751392, 0.0015968972869331436, 0.0015946208162005155, 0.0015862489176191846, 0.001580839366081874, 0.0015778382456082167, 0.0015854344712760374, 0.0015937747951711015, 0.0015901620808646691, 0.0016049134891898353, 0.0015932865920761715, 0.0015836253676715555, 0.0015903425721299587, 0.0015916580818973634, 0.0016262886941204875, 0.0017787180199496904, 0.0016022915714325346, 0.0016087948164085345, 0.0016676634502578148, 0.0017041420820644315, 0.001610689471019622, 0.0016193180012383632, 0.0016059687536932072, 0.0016120739580531205, 0.0015981115700144852, 0.0015813042029586373, 0.001597834796150576, 0.0016001245311024238, 0.00162717340780156, 0.0015961658969825627, 0.0016162058775674325, 0.0015902333674306165, 0.0015958627966251603, 0.0015985789371426313, 0.0016031531849876046, 0.0015921195712396685, 0.0015976465905883483, 0.0015880630603440258, 0.0015947725314989078, 0.0015894682027817685, 0.0015944143666942812, 0.001590355327923079, 0.001600929899426291, 0.001608714734546232, 0.0015932105324820292, 0.0015894765110344303, 0.0016164382449256219, 0.0015883020823821425, 0.001614743634602245, 0.0015942524686189635, 0.0015890569193288684, 0.0015901045930780927, 0.0015956697348809363, 0.001591526347269513, 0.0015961993271865103, 0.0015834745504342172, 0.0016002455507690202, 0.0015951653476804495, 0.0015990284475859028, 0.0015991237941103019, 0.001598291407928479, 0.0015938973876324538, 0.0015933891420955866, 0.00159546851218507, 0.0016064715300857717, 0.0016064024292768873, 0.0015950568367213923, 0.001609051326878977, 0.0015948045718464621, 0.0016310138162225485, 0.0016394822848770692, 0.0015992457569785872, 0.0015917929383564967, 0.0015937870211557162, 0.0015854054482235592, 0.0015608645923321648, 0.001557338550420744, 0.0016378368374568466, 0.0015837609582599632, 0.0036495187348325035, 0.0015634695918546344, 0.001563529346176252, 0.0015650521654977786, 0.0016620688363719657, 0.001621357509771324, 0.0015939169382790522, 0.004066209387205237, 0.001561007060927852, 0.0015626880815442726, 0.001577691980447544, 0.0016546166117055987, 0.0016202381645728434, 0.0016056780812653657, 0.0016387432257701851, 0.0016175486736607794, 0.0016092539394312367, 0.0016554603685757943, 0.0016026483273741845, 0.001603653732858294, 0.0015770556519226152, 0.0015757106724480282, 0.0015747260197769015, 0.001651146427766249, 0.0016062206123024225, 0.0015822311849048247, 0.0016014869396137645, 0.001585285998975896, 0.0016008238973362105, 0.0016311192433634888, 0.0017668643267825246, 0.0016198206944771263, 0.0016454838368357445, 0.0015980753268362308, 0.0016117670207417437, 0.0016293422228713728, 0.0015903074907290997, 0.0016672036715079934, 0.0016105353894016268, 0.0015952770208597792, 0.0016140513887096728, 0.0016108620602979648, 0.0015884087758366854, 0.0015219956452104573, 0.0015364037511365798, 0.0015082191239343956, 0.0015460695212823339, 0.0015318267275385249, 0.001547021228664865, 0.0016103796879178844, 0.0015978383550342794, 0.0015999908549323056, 0.0015536342082972017, 0.0015632558327827912, 0.0015496242510077234, 0.0015619901872317616, 0.001606094061571639, 0.001584311813833968, 0.0016592842487928767, 0.0017404555619577877, 0.0015597210416065839, 0.0015659936058606643, 0.0015666631879867055, 0.0015880681457929313, 0.001551342543583208, 0.0015544557075675887, 0.0015661106250869732, 0.00157159760419745, 0.0015516317507717758, 0.0015901169366164443]
[631.4249584014082, 624.5245329307019, 630.4086898839888, 636.3066341223174, 630.0323753397412, 628.5248452830956, 629.8563456116486, 628.5686909840332, 626.2966094762489, 624.471095145469, 627.519426914798, 626.804501240239, 628.5832446823496, 629.592240522515, 590.7535500848487, 627.8801279741355, 629.5915455163145, 626.2143521581839, 627.1083318620463, 630.4180818612688, 632.5753403260129, 633.7785275413485, 630.7419310715186, 627.4412188157637, 628.8667124147737, 623.0865443749262, 627.6334747140031, 631.462478698687, 628.795341031896, 628.2756399590123, 614.8969759276409, 562.2026587599782, 624.1061351311649, 621.5833055904512, 599.6413723916559, 586.8055313724664, 620.8521369218149, 617.5439285151257, 622.6771210213925, 620.3189345032819, 625.7385396383407, 632.3893898017783, 625.8469288622016, 624.9513588239528, 614.5626490732043, 626.5012940637488, 618.7330549156956, 628.8385217420807, 626.6202847229366, 625.555596139308, 623.7707097264894, 628.0935289435405, 625.9206547248605, 629.697916267486, 627.0486732425169, 629.1412424922214, 627.1895317108264, 628.7902976411867, 624.6369690255393, 621.6142480239473, 627.6634378270905, 629.137953947618, 618.6441103699657, 629.6031536395114, 619.2933531806905, 627.2532234911699, 629.3040783097599, 628.8894481237992, 626.6961001642466, 628.3276439096596, 626.4881728540871, 631.5226220249527, 624.9040964490957, 626.8942598672376, 625.3797432495511, 625.3424554640975, 625.6681322563604, 627.3929600232182, 627.5930804227927, 626.7751399433464, 622.4822421512871, 622.5090187706851, 626.9369071860036, 621.4842145151867, 627.0360755501232, 613.1155910843321, 609.9486461209195, 625.2947651330797, 628.2224125409713, 627.4364056967045, 630.7534776801077, 640.6705648347437, 642.1211365568722, 610.5614290326693, 631.4084172769822, 274.0087317419664, 639.6031014672758, 639.5786573789533, 638.9563377153893, 601.6597977872219, 616.767118894734, 627.3852645544362, 245.9292930527894, 640.6120926868881, 639.9229710716065, 633.8372840789428, 604.369612226477, 617.1932138530005, 622.789842912935, 610.223727716717, 618.2194182366303, 621.4059667633518, 604.0615764545955, 623.9672066038485, 623.5760123961651, 634.0930320251432, 634.6342748611314, 635.0311021987651, 605.6398046736828, 622.5794840016148, 632.0188917652717, 624.4197035045275, 630.8010041380583, 624.6783307420708, 613.0759624525831, 565.9744128860243, 617.3522806626429, 607.7239882969583, 625.7527309302354, 620.4370651161448, 613.7446056223292, 628.8092119477695, 599.8067405258863, 620.9115345000502, 626.8503757805319, 619.5589601390782, 620.7856182391109, 629.5608631809879, 657.0321033091542, 650.8705795987764, 663.0336296170038, 646.8014447180775, 652.8153491660828, 646.4035408635186, 620.9715680734489, 625.8455349061554, 625.0035723124862, 643.6521509757498, 639.6905605782226, 645.317727410176, 640.2088874657086, 622.6285395896755, 631.1888804136618, 602.6694948303742, 574.562213398387, 641.1402894006958, 638.5722114429732, 638.2992896418818, 629.6958997944603, 644.6029628570964, 643.3119934724929, 638.5244975555066, 636.2951924393259, 644.4828159146678, 628.8845662683563]
Elapsed: 0.07942781406807516~0.012355207688803034
Time per graph: 0.0016262304783478784~0.0002507905760460674
Speed: 621.3174184855012~42.23660910820637
Total Time: 0.0770
best val loss: 0.6929255127906799 test_score: 0.5000

Testing...
Test loss: 0.6930 score: 0.7083 time: 0.07s
test Score 0.7083
Epoch Time List: [0.39266364404466003, 0.2826130830217153, 0.2661858790088445, 0.26607032399624586, 0.27117240393999964, 0.2708440598798916, 0.26778734801337123, 0.2690558281028643, 0.27098136802669615, 0.26983992592431605, 0.27142235601786524, 0.2692768289707601, 0.270211192779243, 0.269793672952801, 0.28485040203668177, 0.2721725081792101, 0.2691799160093069, 0.268274859059602, 0.26955623901449144, 0.27032471098937094, 0.2674195229774341, 0.2685833720024675, 0.2702162259956822, 0.2714883011067286, 0.2693555069854483, 0.2729854299686849, 0.27150433498900384, 0.26968639495316893, 0.2683232070412487, 0.2692598729627207, 0.2724960200721398, 0.28927054489031434, 0.28893998311832547, 0.2735071318456903, 0.2851600961294025, 0.2897133269580081, 0.271541042951867, 0.2723916560644284, 0.2727885290514678, 0.27088151313364506, 0.26983908796682954, 0.27201939292717725, 0.2694371759425849, 0.2699380030389875, 0.2715802089078352, 0.27095535991247743, 0.2712447840021923, 0.27038088790141046, 0.27134644286707044, 0.27174408989958465, 0.2704614420654252, 0.26812433009035885, 0.26874385494738817, 0.2687817109981552, 0.27093324495945126, 0.2700783619657159, 0.2684284739661962, 0.2693769570905715, 0.2700551029993221, 0.2709773681126535, 0.27163065888453275, 0.2696436190744862, 0.27138554700650275, 0.27006561285816133, 0.27060042798984796, 0.27011186501476914, 0.2702617389149964, 0.2691751678939909, 0.2712113920133561, 0.27252779400441796, 0.27254804503172636, 0.27129908406641334, 0.27133847202640027, 0.272697635111399, 0.27234503091312945, 0.27006233180873096, 0.27116715896409005, 0.2720064801396802, 0.2701811820734292, 0.2722073720069602, 0.2738324700621888, 0.272273194976151, 0.2707813649903983, 0.2720926101319492, 0.2736746690934524, 0.2736646729754284, 0.2762101950356737, 0.27332384802866727, 0.26844331598840654, 0.3799686769489199, 0.2684880818706006, 0.2653484789188951, 0.2653702151728794, 0.2757150470279157, 0.2720365321729332, 0.369407067890279, 0.26495877804700285, 0.2633576529333368, 0.26344164297915995, 0.270703605026938, 0.27427524991799146, 0.2706836609868333, 0.3907290620263666, 0.26552566105965525, 0.2639659789856523, 0.2651111129671335, 0.2699091621907428, 0.27730480197351426, 0.26941350707784295, 0.26380831410642713, 0.2594936670502648, 0.25834672595374286, 0.26552850496955216, 0.2592404600000009, 0.2554450350580737, 0.3793773438083008, 0.2527334709884599, 0.25001311395317316, 0.2535846650134772, 0.2608203259296715, 0.2520572489593178, 0.36974470608402044, 0.2542982498416677, 0.25602538394741714, 0.2715981110231951, 0.27865822706371546, 0.2780263789463788, 0.2581089740851894, 0.3133282830240205, 0.2530697430483997, 0.2567962569883093, 0.2549674389883876, 0.26071096991654485, 0.26189130707643926, 0.2579105868935585, 0.3486081169685349, 0.25563551497180015, 0.2527317369822413, 0.2636263738386333, 0.25673753512091935, 0.3622170719318092, 0.2597331671277061, 0.2627285950584337, 0.26334483409300447, 0.2690338809043169, 0.27170132007449865, 0.2703885301016271, 0.39186953695025295, 0.2678931698901579, 0.26703167997766286, 0.26720899902284145, 0.27373510296456516, 0.2745720148086548, 0.280379141215235, 0.4149282070575282, 0.2700115549378097, 0.2648221709532663, 0.26622229104395956, 0.27621282008476555, 0.2672014581039548, 0.38773719500750303, 0.2684308900497854, 0.2697395969880745, 0.2681211340241134, 0.2785479761660099]
Total Epoch List: [109, 29, 27]
Total Time List: [0.07938399794511497, 0.0784644220257178, 0.0770034589804709]
========================training times:2========================
========================k_idx:0========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: I2BGNNA
I2BGNN(
  (gcs): ModuleList(
    (0): GCNConv(14887, 64)
    (1): GCNConv(64, 64)
  )
  (bns): ModuleList(
    (0-1): 2 x BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  )
  (drops): ModuleList(
    (0-1): 2 x Dropout(p=0.2, inplace=False)
  )
  (lin1): Linear(in_features=64, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=64, bias=True)
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=64, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=64, out_features=2, bias=True)
  )
)
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7d1ecd68fdc0>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.7056;  Loss pred: 0.7056; Loss self: 0.0000; time: 0.10s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7030 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6980 score: 0.5102 time: 0.07s
Epoch 2/1000, LR 0.000000
Train loss: 0.7025;  Loss pred: 0.7025; Loss self: 0.0000; time: 0.10s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7031 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6981 score: 0.5102 time: 0.07s
     INFO: Early stopping counter 1 of 20
Epoch 3/1000, LR 0.000030
Train loss: 0.7035;  Loss pred: 0.7035; Loss self: 0.0000; time: 0.11s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7032 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6982 score: 0.5102 time: 0.07s
     INFO: Early stopping counter 2 of 20
Epoch 4/1000, LR 0.000060
Train loss: 0.7029;  Loss pred: 0.7029; Loss self: 0.0000; time: 0.11s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7034 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6984 score: 0.5102 time: 0.07s
     INFO: Early stopping counter 3 of 20
Epoch 5/1000, LR 0.000090
Train loss: 0.7015;  Loss pred: 0.7015; Loss self: 0.0000; time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7036 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6985 score: 0.5102 time: 0.07s
     INFO: Early stopping counter 4 of 20
Epoch 6/1000, LR 0.000120
Train loss: 0.7010;  Loss pred: 0.7010; Loss self: 0.0000; time: 0.11s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7037 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6985 score: 0.5102 time: 0.07s
     INFO: Early stopping counter 5 of 20
Epoch 7/1000, LR 0.000150
Train loss: 0.6969;  Loss pred: 0.6969; Loss self: 0.0000; time: 0.11s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7037 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6985 score: 0.5102 time: 0.07s
     INFO: Early stopping counter 6 of 20
Epoch 8/1000, LR 0.000180
Train loss: 0.6953;  Loss pred: 0.6953; Loss self: 0.0000; time: 0.10s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7036 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6984 score: 0.5102 time: 0.07s
     INFO: Early stopping counter 7 of 20
Epoch 9/1000, LR 0.000210
Train loss: 0.6941;  Loss pred: 0.6941; Loss self: 0.0000; time: 0.11s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7034 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6983 score: 0.5102 time: 0.08s
     INFO: Early stopping counter 8 of 20
Epoch 10/1000, LR 0.000240
Train loss: 0.6921;  Loss pred: 0.6921; Loss self: 0.0000; time: 0.11s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7032 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6982 score: 0.5102 time: 0.07s
     INFO: Early stopping counter 9 of 20
Epoch 11/1000, LR 0.000270
Train loss: 0.6897;  Loss pred: 0.6897; Loss self: 0.0000; time: 0.11s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7030 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6980 score: 0.5102 time: 0.07s
     INFO: Early stopping counter 10 of 20
Epoch 12/1000, LR 0.000270
Train loss: 0.6873;  Loss pred: 0.6873; Loss self: 0.0000; time: 0.11s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7028 score: 0.4898 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6979 score: 0.5102 time: 0.08s
Epoch 13/1000, LR 0.000270
Train loss: 0.6846;  Loss pred: 0.6846; Loss self: 0.0000; time: 0.11s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7026 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6978 score: 0.5102 time: 0.08s
Epoch 14/1000, LR 0.000270
Train loss: 0.6822;  Loss pred: 0.6822; Loss self: 0.0000; time: 0.11s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7025 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6977 score: 0.5102 time: 0.08s
Epoch 15/1000, LR 0.000270
Train loss: 0.6781;  Loss pred: 0.6781; Loss self: 0.0000; time: 0.11s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7024 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6976 score: 0.5102 time: 0.08s
Epoch 16/1000, LR 0.000270
Train loss: 0.6741;  Loss pred: 0.6741; Loss self: 0.0000; time: 0.11s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7024 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6976 score: 0.5102 time: 0.08s
Epoch 17/1000, LR 0.000270
Train loss: 0.6709;  Loss pred: 0.6709; Loss self: 0.0000; time: 0.11s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7024 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6976 score: 0.5102 time: 0.08s
Epoch 18/1000, LR 0.000270
Train loss: 0.6693;  Loss pred: 0.6693; Loss self: 0.0000; time: 0.11s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7024 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6976 score: 0.5102 time: 0.08s
     INFO: Early stopping counter 1 of 20
Epoch 19/1000, LR 0.000270
Train loss: 0.6632;  Loss pred: 0.6632; Loss self: 0.0000; time: 0.11s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7024 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6976 score: 0.5102 time: 0.08s
     INFO: Early stopping counter 2 of 20
Epoch 20/1000, LR 0.000270
Train loss: 0.6611;  Loss pred: 0.6611; Loss self: 0.0000; time: 0.11s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7024 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6976 score: 0.5102 time: 0.08s
     INFO: Early stopping counter 3 of 20
Epoch 21/1000, LR 0.000270
Train loss: 0.6522;  Loss pred: 0.6522; Loss self: 0.0000; time: 0.11s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7024 score: 0.4898 time: 0.09s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6976 score: 0.5102 time: 0.08s
     INFO: Early stopping counter 4 of 20
Epoch 22/1000, LR 0.000270
Train loss: 0.6473;  Loss pred: 0.6473; Loss self: 0.0000; time: 0.11s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7024 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6976 score: 0.5102 time: 0.08s
     INFO: Early stopping counter 5 of 20
Epoch 23/1000, LR 0.000270
Train loss: 0.6434;  Loss pred: 0.6434; Loss self: 0.0000; time: 0.11s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7025 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6976 score: 0.5102 time: 0.08s
     INFO: Early stopping counter 6 of 20
Epoch 24/1000, LR 0.000270
Train loss: 0.6376;  Loss pred: 0.6376; Loss self: 0.0000; time: 0.11s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7025 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6977 score: 0.5102 time: 0.08s
     INFO: Early stopping counter 7 of 20
Epoch 25/1000, LR 0.000270
Train loss: 0.6293;  Loss pred: 0.6293; Loss self: 0.0000; time: 0.11s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7026 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6977 score: 0.5102 time: 0.08s
     INFO: Early stopping counter 8 of 20
Epoch 26/1000, LR 0.000270
Train loss: 0.6207;  Loss pred: 0.6207; Loss self: 0.0000; time: 0.11s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7026 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6977 score: 0.5102 time: 0.08s
     INFO: Early stopping counter 9 of 20
Epoch 27/1000, LR 0.000270
Train loss: 0.6181;  Loss pred: 0.6181; Loss self: 0.0000; time: 0.11s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7026 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6977 score: 0.5102 time: 0.08s
     INFO: Early stopping counter 10 of 20
Epoch 28/1000, LR 0.000270
Train loss: 0.6055;  Loss pred: 0.6055; Loss self: 0.0000; time: 0.11s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7027 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6977 score: 0.5102 time: 0.08s
     INFO: Early stopping counter 11 of 20
Epoch 29/1000, LR 0.000270
Train loss: 0.5958;  Loss pred: 0.5958; Loss self: 0.0000; time: 0.11s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7028 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6978 score: 0.5102 time: 0.08s
     INFO: Early stopping counter 12 of 20
Epoch 30/1000, LR 0.000270
Train loss: 0.5831;  Loss pred: 0.5831; Loss self: 0.0000; time: 0.11s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7028 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6978 score: 0.5102 time: 0.08s
     INFO: Early stopping counter 13 of 20
Epoch 31/1000, LR 0.000270
Train loss: 0.5774;  Loss pred: 0.5774; Loss self: 0.0000; time: 0.11s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7028 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6978 score: 0.5102 time: 0.08s
     INFO: Early stopping counter 14 of 20
Epoch 32/1000, LR 0.000270
Train loss: 0.5643;  Loss pred: 0.5643; Loss self: 0.0000; time: 0.11s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7026 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6977 score: 0.5102 time: 0.08s
     INFO: Early stopping counter 15 of 20
Epoch 33/1000, LR 0.000270
Train loss: 0.5602;  Loss pred: 0.5602; Loss self: 0.0000; time: 0.11s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7025 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6976 score: 0.5102 time: 0.08s
     INFO: Early stopping counter 16 of 20
Epoch 34/1000, LR 0.000270
Train loss: 0.5447;  Loss pred: 0.5447; Loss self: 0.0000; time: 0.11s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7023 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6975 score: 0.5102 time: 0.08s
Epoch 35/1000, LR 0.000270
Train loss: 0.5342;  Loss pred: 0.5342; Loss self: 0.0000; time: 0.11s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7021 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6973 score: 0.5102 time: 0.08s
Epoch 36/1000, LR 0.000270
Train loss: 0.5199;  Loss pred: 0.5199; Loss self: 0.0000; time: 0.11s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7018 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6971 score: 0.5102 time: 0.08s
Epoch 37/1000, LR 0.000270
Train loss: 0.5082;  Loss pred: 0.5082; Loss self: 0.0000; time: 0.11s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7014 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6968 score: 0.5102 time: 0.08s
Epoch 38/1000, LR 0.000270
Train loss: 0.4906;  Loss pred: 0.4906; Loss self: 0.0000; time: 0.11s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7008 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6964 score: 0.5102 time: 0.08s
Epoch 39/1000, LR 0.000269
Train loss: 0.4810;  Loss pred: 0.4810; Loss self: 0.0000; time: 0.11s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7001 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6958 score: 0.5102 time: 0.08s
Epoch 40/1000, LR 0.000269
Train loss: 0.4656;  Loss pred: 0.4656; Loss self: 0.0000; time: 0.11s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6993 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6952 score: 0.5102 time: 0.08s
Epoch 41/1000, LR 0.000269
Train loss: 0.4536;  Loss pred: 0.4536; Loss self: 0.0000; time: 0.11s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6982 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6943 score: 0.5102 time: 0.08s
Epoch 42/1000, LR 0.000269
Train loss: 0.4331;  Loss pred: 0.4331; Loss self: 0.0000; time: 0.11s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6970 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6934 score: 0.5102 time: 0.08s
Epoch 43/1000, LR 0.000269
Train loss: 0.4193;  Loss pred: 0.4193; Loss self: 0.0000; time: 0.11s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6956 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6923 score: 0.5102 time: 0.08s
Epoch 44/1000, LR 0.000269
Train loss: 0.4008;  Loss pred: 0.4008; Loss self: 0.0000; time: 0.11s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6939 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6911 score: 0.5102 time: 0.08s
Epoch 45/1000, LR 0.000269
Train loss: 0.3904;  Loss pred: 0.3904; Loss self: 0.0000; time: 0.11s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6920 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6896 score: 0.5102 time: 0.08s
Epoch 46/1000, LR 0.000269
Train loss: 0.3781;  Loss pred: 0.3781; Loss self: 0.0000; time: 0.11s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6896 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6878 score: 0.5102 time: 0.08s
Epoch 47/1000, LR 0.000269
Train loss: 0.3624;  Loss pred: 0.3624; Loss self: 0.0000; time: 0.11s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6870 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6858 score: 0.5102 time: 0.08s
Epoch 48/1000, LR 0.000269
Train loss: 0.3498;  Loss pred: 0.3498; Loss self: 0.0000; time: 0.11s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6840 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6835 score: 0.5102 time: 0.08s
Epoch 49/1000, LR 0.000269
Train loss: 0.3293;  Loss pred: 0.3293; Loss self: 0.0000; time: 0.11s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6805 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6808 score: 0.5102 time: 0.08s
Epoch 50/1000, LR 0.000269
Train loss: 0.3160;  Loss pred: 0.3160; Loss self: 0.0000; time: 0.11s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6768 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6778 score: 0.5102 time: 0.08s
Epoch 51/1000, LR 0.000269
Train loss: 0.3057;  Loss pred: 0.3057; Loss self: 0.0000; time: 0.11s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6725 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6743 score: 0.5102 time: 0.08s
Epoch 52/1000, LR 0.000269
Train loss: 0.2880;  Loss pred: 0.2880; Loss self: 0.0000; time: 0.11s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6676 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6703 score: 0.5102 time: 0.08s
Epoch 53/1000, LR 0.000269
Train loss: 0.2745;  Loss pred: 0.2745; Loss self: 0.0000; time: 0.11s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6621 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6657 score: 0.5102 time: 0.08s
Epoch 54/1000, LR 0.000269
Train loss: 0.2647;  Loss pred: 0.2647; Loss self: 0.0000; time: 0.11s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6561 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6608 score: 0.5102 time: 0.08s
Epoch 55/1000, LR 0.000269
Train loss: 0.2414;  Loss pred: 0.2414; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6498 score: 0.4898 time: 0.09s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6555 score: 0.5102 time: 0.08s
Epoch 56/1000, LR 0.000269
Train loss: 0.2277;  Loss pred: 0.2277; Loss self: 0.0000; time: 0.11s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6432 score: 0.4898 time: 0.09s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6501 score: 0.5102 time: 0.08s
Epoch 57/1000, LR 0.000269
Train loss: 0.2135;  Loss pred: 0.2135; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6363 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6443 score: 0.5102 time: 0.08s
Epoch 58/1000, LR 0.000269
Train loss: 0.2054;  Loss pred: 0.2054; Loss self: 0.0000; time: 0.11s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6291 score: 0.4898 time: 0.08s
Test loss: 0.6382 score: 0.5306 time: 0.08s
Epoch 59/1000, LR 0.000268
Train loss: 0.1855;  Loss pred: 0.1855; Loss self: 0.0000; time: 0.12s
Val loss: 0.6216 score: 0.5510 time: 0.08s
Test loss: 0.6318 score: 0.5510 time: 0.09s
Epoch 60/1000, LR 0.000268
Train loss: 0.1786;  Loss pred: 0.1786; Loss self: 0.0000; time: 0.11s
Val loss: 0.6137 score: 0.5510 time: 0.08s
Test loss: 0.6250 score: 0.5510 time: 0.09s
Epoch 61/1000, LR 0.000268
Train loss: 0.1617;  Loss pred: 0.1617; Loss self: 0.0000; time: 0.11s
Val loss: 0.6052 score: 0.5918 time: 0.09s
Test loss: 0.6177 score: 0.5510 time: 0.09s
Epoch 62/1000, LR 0.000268
Train loss: 0.1496;  Loss pred: 0.1496; Loss self: 0.0000; time: 0.11s
Val loss: 0.5960 score: 0.6122 time: 0.09s
Test loss: 0.6097 score: 0.5510 time: 0.08s
Epoch 63/1000, LR 0.000268
Train loss: 0.1463;  Loss pred: 0.1463; Loss self: 0.0000; time: 0.11s
Val loss: 0.5864 score: 0.6122 time: 0.08s
Test loss: 0.6012 score: 0.5918 time: 0.08s
Epoch 64/1000, LR 0.000268
Train loss: 0.1346;  Loss pred: 0.1346; Loss self: 0.0000; time: 0.11s
Val loss: 0.5760 score: 0.6122 time: 0.08s
Test loss: 0.5920 score: 0.6122 time: 0.08s
Epoch 65/1000, LR 0.000268
Train loss: 0.1223;  Loss pred: 0.1223; Loss self: 0.0000; time: 0.11s
Val loss: 0.5646 score: 0.6735 time: 0.08s
Test loss: 0.5819 score: 0.6327 time: 0.08s
Epoch 66/1000, LR 0.000268
Train loss: 0.1126;  Loss pred: 0.1126; Loss self: 0.0000; time: 0.11s
Val loss: 0.5525 score: 0.7143 time: 0.08s
Test loss: 0.5712 score: 0.6531 time: 0.08s
Epoch 67/1000, LR 0.000268
Train loss: 0.1063;  Loss pred: 0.1063; Loss self: 0.0000; time: 0.11s
Val loss: 0.5399 score: 0.7143 time: 0.08s
Test loss: 0.5599 score: 0.6735 time: 0.08s
Epoch 68/1000, LR 0.000268
Train loss: 0.0966;  Loss pred: 0.0966; Loss self: 0.0000; time: 0.11s
Val loss: 0.5265 score: 0.7143 time: 0.08s
Test loss: 0.5477 score: 0.6939 time: 0.08s
Epoch 69/1000, LR 0.000268
Train loss: 0.0906;  Loss pred: 0.0906; Loss self: 0.0000; time: 0.11s
Val loss: 0.5131 score: 0.7347 time: 0.08s
Test loss: 0.5356 score: 0.6939 time: 0.08s
Epoch 70/1000, LR 0.000268
Train loss: 0.0835;  Loss pred: 0.0835; Loss self: 0.0000; time: 0.11s
Val loss: 0.4998 score: 0.7551 time: 0.08s
Test loss: 0.5233 score: 0.7347 time: 0.08s
Epoch 71/1000, LR 0.000268
Train loss: 0.0731;  Loss pred: 0.0731; Loss self: 0.0000; time: 0.11s
Val loss: 0.4863 score: 0.7959 time: 0.08s
Test loss: 0.5106 score: 0.7347 time: 0.08s
Epoch 72/1000, LR 0.000267
Train loss: 0.0721;  Loss pred: 0.0721; Loss self: 0.0000; time: 0.11s
Val loss: 0.4728 score: 0.7959 time: 0.08s
Test loss: 0.4979 score: 0.7551 time: 0.08s
Epoch 73/1000, LR 0.000267
Train loss: 0.0665;  Loss pred: 0.0665; Loss self: 0.0000; time: 0.11s
Val loss: 0.4587 score: 0.7959 time: 0.08s
Test loss: 0.4846 score: 0.7551 time: 0.08s
Epoch 74/1000, LR 0.000267
Train loss: 0.0588;  Loss pred: 0.0588; Loss self: 0.0000; time: 0.11s
Val loss: 0.4456 score: 0.8163 time: 0.08s
Test loss: 0.4719 score: 0.7347 time: 0.08s
Epoch 75/1000, LR 0.000267
Train loss: 0.0538;  Loss pred: 0.0538; Loss self: 0.0000; time: 0.11s
Val loss: 0.4319 score: 0.8163 time: 0.08s
Test loss: 0.4589 score: 0.7551 time: 0.08s
Epoch 76/1000, LR 0.000267
Train loss: 0.0490;  Loss pred: 0.0490; Loss self: 0.0000; time: 0.11s
Val loss: 0.4184 score: 0.8367 time: 0.08s
Test loss: 0.4459 score: 0.7551 time: 0.08s
Epoch 77/1000, LR 0.000267
Train loss: 0.0460;  Loss pred: 0.0460; Loss self: 0.0000; time: 0.11s
Val loss: 0.4054 score: 0.8571 time: 0.08s
Test loss: 0.4335 score: 0.7551 time: 0.08s
Epoch 78/1000, LR 0.000267
Train loss: 0.0415;  Loss pred: 0.0415; Loss self: 0.0000; time: 0.11s
Val loss: 0.3927 score: 0.8571 time: 0.08s
Test loss: 0.4214 score: 0.7755 time: 0.08s
Epoch 79/1000, LR 0.000267
Train loss: 0.0369;  Loss pred: 0.0369; Loss self: 0.0000; time: 0.11s
Val loss: 0.3791 score: 0.8776 time: 0.08s
Test loss: 0.4088 score: 0.7959 time: 0.08s
Epoch 80/1000, LR 0.000267
Train loss: 0.0348;  Loss pred: 0.0348; Loss self: 0.0000; time: 0.11s
Val loss: 0.3648 score: 0.8776 time: 0.08s
Test loss: 0.3960 score: 0.7959 time: 0.08s
Epoch 81/1000, LR 0.000267
Train loss: 0.0313;  Loss pred: 0.0313; Loss self: 0.0000; time: 0.11s
Val loss: 0.3500 score: 0.8776 time: 0.08s
Test loss: 0.3831 score: 0.7959 time: 0.08s
Epoch 82/1000, LR 0.000267
Train loss: 0.0310;  Loss pred: 0.0310; Loss self: 0.0000; time: 0.11s
Val loss: 0.3371 score: 0.8776 time: 0.08s
Test loss: 0.3717 score: 0.8367 time: 0.08s
Epoch 83/1000, LR 0.000266
Train loss: 0.0279;  Loss pred: 0.0279; Loss self: 0.0000; time: 0.11s
Val loss: 0.3253 score: 0.8776 time: 0.08s
Test loss: 0.3614 score: 0.8571 time: 0.08s
Epoch 84/1000, LR 0.000266
Train loss: 0.0275;  Loss pred: 0.0275; Loss self: 0.0000; time: 0.11s
Val loss: 0.3136 score: 0.8776 time: 0.08s
Test loss: 0.3516 score: 0.8571 time: 0.08s
Epoch 85/1000, LR 0.000266
Train loss: 0.0236;  Loss pred: 0.0236; Loss self: 0.0000; time: 0.11s
Val loss: 0.3029 score: 0.8776 time: 0.08s
Test loss: 0.3428 score: 0.8571 time: 0.08s
Epoch 86/1000, LR 0.000266
Train loss: 0.0212;  Loss pred: 0.0212; Loss self: 0.0000; time: 0.11s
Val loss: 0.2922 score: 0.8776 time: 0.08s
Test loss: 0.3342 score: 0.8571 time: 0.08s
Epoch 87/1000, LR 0.000266
Train loss: 0.0198;  Loss pred: 0.0198; Loss self: 0.0000; time: 0.11s
Val loss: 0.2826 score: 0.8776 time: 0.08s
Test loss: 0.3266 score: 0.8571 time: 0.08s
Epoch 88/1000, LR 0.000266
Train loss: 0.0195;  Loss pred: 0.0195; Loss self: 0.0000; time: 0.11s
Val loss: 0.2733 score: 0.8776 time: 0.08s
Test loss: 0.3197 score: 0.8980 time: 0.08s
Epoch 89/1000, LR 0.000266
Train loss: 0.0177;  Loss pred: 0.0177; Loss self: 0.0000; time: 0.11s
Val loss: 0.2658 score: 0.8980 time: 0.07s
Test loss: 0.3139 score: 0.8980 time: 0.07s
Epoch 90/1000, LR 0.000266
Train loss: 0.0164;  Loss pred: 0.0164; Loss self: 0.0000; time: 0.10s
Val loss: 0.2597 score: 0.8980 time: 0.07s
Test loss: 0.3093 score: 0.8980 time: 0.07s
Epoch 91/1000, LR 0.000266
Train loss: 0.0150;  Loss pred: 0.0150; Loss self: 0.0000; time: 0.11s
Val loss: 0.2544 score: 0.9184 time: 0.08s
Test loss: 0.3055 score: 0.8980 time: 0.07s
Epoch 92/1000, LR 0.000266
Train loss: 0.0132;  Loss pred: 0.0132; Loss self: 0.0000; time: 0.11s
Val loss: 0.2482 score: 0.9184 time: 0.08s
Test loss: 0.3019 score: 0.8980 time: 0.07s
Epoch 93/1000, LR 0.000265
Train loss: 0.0132;  Loss pred: 0.0132; Loss self: 0.0000; time: 0.10s
Val loss: 0.2429 score: 0.9184 time: 0.07s
Test loss: 0.2989 score: 0.8980 time: 0.07s
Epoch 94/1000, LR 0.000265
Train loss: 0.0125;  Loss pred: 0.0125; Loss self: 0.0000; time: 0.10s
Val loss: 0.2376 score: 0.9184 time: 0.07s
Test loss: 0.2966 score: 0.8980 time: 0.07s
Epoch 95/1000, LR 0.000265
Train loss: 0.0115;  Loss pred: 0.0115; Loss self: 0.0000; time: 0.11s
Val loss: 0.2319 score: 0.9184 time: 0.07s
Test loss: 0.2946 score: 0.9184 time: 0.07s
Epoch 96/1000, LR 0.000265
Train loss: 0.0115;  Loss pred: 0.0115; Loss self: 0.0000; time: 0.11s
Val loss: 0.2246 score: 0.9184 time: 0.07s
Test loss: 0.2930 score: 0.9184 time: 0.07s
Epoch 97/1000, LR 0.000265
Train loss: 0.0101;  Loss pred: 0.0101; Loss self: 0.0000; time: 0.10s
Val loss: 0.2180 score: 0.9184 time: 0.07s
Test loss: 0.2922 score: 0.9184 time: 0.07s
Epoch 98/1000, LR 0.000265
Train loss: 0.0101;  Loss pred: 0.0101; Loss self: 0.0000; time: 0.10s
Val loss: 0.2119 score: 0.9184 time: 0.07s
Test loss: 0.2920 score: 0.9184 time: 0.07s
Epoch 99/1000, LR 0.000265
Train loss: 0.0088;  Loss pred: 0.0088; Loss self: 0.0000; time: 0.11s
Val loss: 0.2056 score: 0.9184 time: 0.07s
Test loss: 0.2924 score: 0.9184 time: 0.08s
Epoch 100/1000, LR 0.000265
Train loss: 0.0092;  Loss pred: 0.0092; Loss self: 0.0000; time: 0.12s
Val loss: 0.2010 score: 0.9184 time: 0.09s
Test loss: 0.2937 score: 0.9184 time: 0.08s
Epoch 101/1000, LR 0.000265
Train loss: 0.0081;  Loss pred: 0.0081; Loss self: 0.0000; time: 0.11s
Val loss: 0.1969 score: 0.8980 time: 0.08s
Test loss: 0.2951 score: 0.9184 time: 0.07s
Epoch 102/1000, LR 0.000264
Train loss: 0.0075;  Loss pred: 0.0075; Loss self: 0.0000; time: 0.11s
Val loss: 0.1927 score: 0.8980 time: 0.08s
Test loss: 0.2970 score: 0.9184 time: 0.07s
Epoch 103/1000, LR 0.000264
Train loss: 0.0078;  Loss pred: 0.0078; Loss self: 0.0000; time: 0.11s
Val loss: 0.1890 score: 0.9184 time: 0.08s
Test loss: 0.2992 score: 0.9184 time: 0.08s
Epoch 104/1000, LR 0.000264
Train loss: 0.0079;  Loss pred: 0.0079; Loss self: 0.0000; time: 0.11s
Val loss: 0.1882 score: 0.9184 time: 0.08s
Test loss: 0.3017 score: 0.9184 time: 0.07s
Epoch 105/1000, LR 0.000264
Train loss: 0.0066;  Loss pred: 0.0066; Loss self: 0.0000; time: 0.10s
Val loss: 0.1877 score: 0.9184 time: 0.08s
Test loss: 0.3042 score: 0.9184 time: 0.22s
Epoch 106/1000, LR 0.000264
Train loss: 0.0062;  Loss pred: 0.0062; Loss self: 0.0000; time: 0.13s
Val loss: 0.1868 score: 0.9184 time: 0.07s
Test loss: 0.3068 score: 0.9184 time: 0.07s
Epoch 107/1000, LR 0.000264
Train loss: 0.0069;  Loss pred: 0.0069; Loss self: 0.0000; time: 0.10s
Val loss: 0.1870 score: 0.9184 time: 0.07s
Test loss: 0.3094 score: 0.9184 time: 0.07s
     INFO: Early stopping counter 1 of 20
Epoch 108/1000, LR 0.000264
Train loss: 0.0061;  Loss pred: 0.0061; Loss self: 0.0000; time: 0.10s
Val loss: 0.1872 score: 0.9184 time: 0.07s
Test loss: 0.3121 score: 0.9184 time: 0.07s
     INFO: Early stopping counter 2 of 20
Epoch 109/1000, LR 0.000264
Train loss: 0.0057;  Loss pred: 0.0057; Loss self: 0.0000; time: 0.10s
Val loss: 0.1864 score: 0.9184 time: 0.07s
Test loss: 0.3146 score: 0.9184 time: 0.07s
Epoch 110/1000, LR 0.000263
Train loss: 0.0054;  Loss pred: 0.0054; Loss self: 0.0000; time: 0.10s
Val loss: 0.1845 score: 0.9184 time: 0.08s
Test loss: 0.3174 score: 0.9184 time: 0.07s
Epoch 111/1000, LR 0.000263
Train loss: 0.0057;  Loss pred: 0.0057; Loss self: 0.0000; time: 0.11s
Val loss: 0.1821 score: 0.9184 time: 0.08s
Test loss: 0.3202 score: 0.9184 time: 0.07s
Epoch 112/1000, LR 0.000263
Train loss: 0.0055;  Loss pred: 0.0055; Loss self: 0.0000; time: 0.10s
Val loss: 0.1779 score: 0.9184 time: 0.07s
Test loss: 0.3231 score: 0.9184 time: 0.07s
Epoch 113/1000, LR 0.000263
Train loss: 0.0051;  Loss pred: 0.0051; Loss self: 0.0000; time: 0.10s
Val loss: 0.1734 score: 0.9184 time: 0.07s
Test loss: 0.3262 score: 0.9184 time: 0.20s
Epoch 114/1000, LR 0.000263
Train loss: 0.0049;  Loss pred: 0.0049; Loss self: 0.0000; time: 0.10s
Val loss: 0.1699 score: 0.9184 time: 0.07s
Test loss: 0.3289 score: 0.9184 time: 0.07s
Epoch 115/1000, LR 0.000263
Train loss: 0.0041;  Loss pred: 0.0041; Loss self: 0.0000; time: 0.10s
Val loss: 0.1668 score: 0.9184 time: 0.07s
Test loss: 0.3316 score: 0.9184 time: 0.07s
Epoch 116/1000, LR 0.000263
Train loss: 0.0039;  Loss pred: 0.0039; Loss self: 0.0000; time: 0.10s
Val loss: 0.1642 score: 0.9184 time: 0.07s
Test loss: 0.3340 score: 0.9184 time: 0.07s
Epoch 117/1000, LR 0.000262
Train loss: 0.0043;  Loss pred: 0.0043; Loss self: 0.0000; time: 0.10s
Val loss: 0.1620 score: 0.9184 time: 0.07s
Test loss: 0.3365 score: 0.9184 time: 0.07s
Epoch 118/1000, LR 0.000262
Train loss: 0.0042;  Loss pred: 0.0042; Loss self: 0.0000; time: 0.10s
Val loss: 0.1608 score: 0.9184 time: 0.07s
Test loss: 0.3387 score: 0.9184 time: 0.07s
Epoch 119/1000, LR 0.000262
Train loss: 0.0035;  Loss pred: 0.0035; Loss self: 0.0000; time: 0.11s
Val loss: 0.1597 score: 0.9184 time: 0.07s
Test loss: 0.3409 score: 0.9184 time: 0.07s
Epoch 120/1000, LR 0.000262
Train loss: 0.0043;  Loss pred: 0.0043; Loss self: 0.0000; time: 0.10s
Val loss: 0.1589 score: 0.9184 time: 0.07s
Test loss: 0.3430 score: 0.9184 time: 0.08s
Epoch 121/1000, LR 0.000262
Train loss: 0.0040;  Loss pred: 0.0040; Loss self: 0.0000; time: 0.18s
Val loss: 0.1590 score: 0.9184 time: 0.08s
Test loss: 0.3445 score: 0.9184 time: 0.08s
     INFO: Early stopping counter 1 of 20
Epoch 122/1000, LR 0.000262
Train loss: 0.0040;  Loss pred: 0.0040; Loss self: 0.0000; time: 0.11s
Val loss: 0.1594 score: 0.9184 time: 0.08s
Test loss: 0.3461 score: 0.9184 time: 0.08s
     INFO: Early stopping counter 2 of 20
Epoch 123/1000, LR 0.000262
Train loss: 0.0036;  Loss pred: 0.0036; Loss self: 0.0000; time: 0.11s
Val loss: 0.1602 score: 0.9184 time: 0.08s
Test loss: 0.3473 score: 0.9184 time: 0.08s
     INFO: Early stopping counter 3 of 20
Epoch 124/1000, LR 0.000261
Train loss: 0.0040;  Loss pred: 0.0040; Loss self: 0.0000; time: 0.11s
Val loss: 0.1614 score: 0.9184 time: 0.08s
Test loss: 0.3485 score: 0.9184 time: 0.08s
     INFO: Early stopping counter 4 of 20
Epoch 125/1000, LR 0.000261
Train loss: 0.0033;  Loss pred: 0.0033; Loss self: 0.0000; time: 0.11s
Val loss: 0.1612 score: 0.9184 time: 0.08s
Test loss: 0.3501 score: 0.9184 time: 0.08s
     INFO: Early stopping counter 5 of 20
Epoch 126/1000, LR 0.000261
Train loss: 0.0031;  Loss pred: 0.0031; Loss self: 0.0000; time: 0.11s
Val loss: 0.1603 score: 0.9184 time: 0.08s
Test loss: 0.3516 score: 0.9184 time: 0.08s
     INFO: Early stopping counter 6 of 20
Epoch 127/1000, LR 0.000261
Train loss: 0.0032;  Loss pred: 0.0032; Loss self: 0.0000; time: 0.11s
Val loss: 0.1594 score: 0.9184 time: 0.08s
Test loss: 0.3530 score: 0.9184 time: 0.12s
     INFO: Early stopping counter 7 of 20
Epoch 128/1000, LR 0.000261
Train loss: 0.0033;  Loss pred: 0.0033; Loss self: 0.0000; time: 0.13s
Val loss: 0.1592 score: 0.9184 time: 0.07s
Test loss: 0.3541 score: 0.9184 time: 0.07s
     INFO: Early stopping counter 8 of 20
Epoch 129/1000, LR 0.000261
Train loss: 0.0028;  Loss pred: 0.0028; Loss self: 0.0000; time: 0.10s
Val loss: 0.1580 score: 0.9184 time: 0.07s
Test loss: 0.3559 score: 0.9184 time: 0.07s
Epoch 130/1000, LR 0.000260
Train loss: 0.0028;  Loss pred: 0.0028; Loss self: 0.0000; time: 0.10s
Val loss: 0.1567 score: 0.9184 time: 0.07s
Test loss: 0.3572 score: 0.9184 time: 0.07s
Epoch 131/1000, LR 0.000260
Train loss: 0.0028;  Loss pred: 0.0028; Loss self: 0.0000; time: 0.10s
Val loss: 0.1556 score: 0.9184 time: 0.07s
Test loss: 0.3584 score: 0.9184 time: 0.07s
Epoch 132/1000, LR 0.000260
Train loss: 0.0030;  Loss pred: 0.0030; Loss self: 0.0000; time: 0.11s
Val loss: 0.1547 score: 0.9184 time: 0.08s
Test loss: 0.3597 score: 0.9184 time: 0.07s
Epoch 133/1000, LR 0.000260
Train loss: 0.0025;  Loss pred: 0.0025; Loss self: 0.0000; time: 0.11s
Val loss: 0.1533 score: 0.9184 time: 0.08s
Test loss: 0.3610 score: 0.9184 time: 0.07s
Epoch 134/1000, LR 0.000260
Train loss: 0.0027;  Loss pred: 0.0027; Loss self: 0.0000; time: 0.11s
Val loss: 0.1524 score: 0.9184 time: 0.08s
Test loss: 0.3623 score: 0.9184 time: 0.07s
Epoch 135/1000, LR 0.000260
Train loss: 0.0029;  Loss pred: 0.0029; Loss self: 0.0000; time: 0.21s
Val loss: 0.1504 score: 0.9184 time: 0.08s
Test loss: 0.3639 score: 0.9184 time: 0.08s
Epoch 136/1000, LR 0.000260
Train loss: 0.0028;  Loss pred: 0.0028; Loss self: 0.0000; time: 0.11s
Val loss: 0.1490 score: 0.9184 time: 0.08s
Test loss: 0.3652 score: 0.9184 time: 0.08s
Epoch 137/1000, LR 0.000259
Train loss: 0.0026;  Loss pred: 0.0026; Loss self: 0.0000; time: 0.11s
Val loss: 0.1477 score: 0.9184 time: 0.08s
Test loss: 0.3665 score: 0.9184 time: 0.08s
Epoch 138/1000, LR 0.000259
Train loss: 0.0024;  Loss pred: 0.0024; Loss self: 0.0000; time: 0.11s
Val loss: 0.1459 score: 0.9184 time: 0.08s
Test loss: 0.3675 score: 0.9184 time: 0.08s
Epoch 139/1000, LR 0.000259
Train loss: 0.0022;  Loss pred: 0.0022; Loss self: 0.0000; time: 0.11s
Val loss: 0.1451 score: 0.9184 time: 0.08s
Test loss: 0.3687 score: 0.9184 time: 0.08s
Epoch 140/1000, LR 0.000259
Train loss: 0.0023;  Loss pred: 0.0023; Loss self: 0.0000; time: 0.11s
Val loss: 0.1436 score: 0.9184 time: 0.08s
Test loss: 0.3698 score: 0.9184 time: 0.08s
Epoch 141/1000, LR 0.000259
Train loss: 0.0021;  Loss pred: 0.0021; Loss self: 0.0000; time: 0.11s
Val loss: 0.1423 score: 0.9184 time: 0.08s
Test loss: 0.3709 score: 0.9184 time: 0.08s
Epoch 142/1000, LR 0.000259
Train loss: 0.0025;  Loss pred: 0.0025; Loss self: 0.0000; time: 0.23s
Val loss: 0.1421 score: 0.9184 time: 0.08s
Test loss: 0.3718 score: 0.9184 time: 0.08s
Epoch 143/1000, LR 0.000258
Train loss: 0.0023;  Loss pred: 0.0023; Loss self: 0.0000; time: 0.11s
Val loss: 0.1413 score: 0.9184 time: 0.08s
Test loss: 0.3730 score: 0.9184 time: 0.08s
Epoch 144/1000, LR 0.000258
Train loss: 0.0020;  Loss pred: 0.0020; Loss self: 0.0000; time: 0.11s
Val loss: 0.1402 score: 0.9184 time: 0.08s
Test loss: 0.3740 score: 0.9184 time: 0.08s
Epoch 145/1000, LR 0.000258
Train loss: 0.0022;  Loss pred: 0.0022; Loss self: 0.0000; time: 0.11s
Val loss: 0.1392 score: 0.9184 time: 0.08s
Test loss: 0.3749 score: 0.9184 time: 0.08s
Epoch 146/1000, LR 0.000258
Train loss: 0.0020;  Loss pred: 0.0020; Loss self: 0.0000; time: 0.11s
Val loss: 0.1384 score: 0.9184 time: 0.08s
Test loss: 0.3757 score: 0.9184 time: 0.08s
Epoch 147/1000, LR 0.000258
Train loss: 0.0023;  Loss pred: 0.0023; Loss self: 0.0000; time: 0.11s
Val loss: 0.1379 score: 0.9184 time: 0.08s
Test loss: 0.3763 score: 0.9184 time: 0.08s
Epoch 148/1000, LR 0.000257
Train loss: 0.0019;  Loss pred: 0.0019; Loss self: 0.0000; time: 0.11s
Val loss: 0.1368 score: 0.9184 time: 0.18s
Test loss: 0.3776 score: 0.9184 time: 0.08s
Epoch 149/1000, LR 0.000257
Train loss: 0.0019;  Loss pred: 0.0019; Loss self: 0.0000; time: 0.11s
Val loss: 0.1363 score: 0.9184 time: 0.08s
Test loss: 0.3784 score: 0.9184 time: 0.08s
Epoch 150/1000, LR 0.000257
Train loss: 0.0020;  Loss pred: 0.0020; Loss self: 0.0000; time: 0.11s
Val loss: 0.1353 score: 0.9184 time: 0.08s
Test loss: 0.3791 score: 0.9184 time: 0.08s
Epoch 151/1000, LR 0.000257
Train loss: 0.0018;  Loss pred: 0.0018; Loss self: 0.0000; time: 0.11s
Val loss: 0.1347 score: 0.9184 time: 0.08s
Test loss: 0.3795 score: 0.9184 time: 0.08s
Epoch 152/1000, LR 0.000257
Train loss: 0.0019;  Loss pred: 0.0019; Loss self: 0.0000; time: 0.11s
Val loss: 0.1329 score: 0.9184 time: 0.08s
Test loss: 0.3810 score: 0.9184 time: 0.08s
Epoch 153/1000, LR 0.000257
Train loss: 0.0017;  Loss pred: 0.0017; Loss self: 0.0000; time: 0.11s
Val loss: 0.1310 score: 0.9184 time: 0.08s
Test loss: 0.3821 score: 0.9184 time: 0.08s
Epoch 154/1000, LR 0.000256
Train loss: 0.0018;  Loss pred: 0.0018; Loss self: 0.0000; time: 0.11s
Val loss: 0.1300 score: 0.9184 time: 0.08s
Test loss: 0.3831 score: 0.9184 time: 0.12s
Epoch 155/1000, LR 0.000256
Train loss: 0.0017;  Loss pred: 0.0017; Loss self: 0.0000; time: 0.12s
Val loss: 0.1294 score: 0.9184 time: 0.07s
Test loss: 0.3839 score: 0.9184 time: 0.07s
Epoch 156/1000, LR 0.000256
Train loss: 0.0018;  Loss pred: 0.0018; Loss self: 0.0000; time: 0.11s
Val loss: 0.1290 score: 0.9592 time: 0.08s
Test loss: 0.3844 score: 0.9184 time: 0.07s
Epoch 157/1000, LR 0.000256
Train loss: 0.0016;  Loss pred: 0.0016; Loss self: 0.0000; time: 0.11s
Val loss: 0.1283 score: 0.9592 time: 0.09s
Test loss: 0.3853 score: 0.9184 time: 0.07s
Epoch 158/1000, LR 0.000256
Train loss: 0.0016;  Loss pred: 0.0016; Loss self: 0.0000; time: 0.11s
Val loss: 0.1276 score: 0.9592 time: 0.07s
Test loss: 0.3864 score: 0.9184 time: 0.07s
Epoch 159/1000, LR 0.000255
Train loss: 0.0016;  Loss pred: 0.0016; Loss self: 0.0000; time: 0.11s
Val loss: 0.1275 score: 0.9592 time: 0.08s
Test loss: 0.3867 score: 0.9184 time: 0.07s
Epoch 160/1000, LR 0.000255
Train loss: 0.0017;  Loss pred: 0.0017; Loss self: 0.0000; time: 0.10s
Val loss: 0.1279 score: 0.9592 time: 0.08s
Test loss: 0.3869 score: 0.9184 time: 0.07s
     INFO: Early stopping counter 1 of 20
Epoch 161/1000, LR 0.000255
Train loss: 0.0015;  Loss pred: 0.0015; Loss self: 0.0000; time: 0.11s
Val loss: 0.1281 score: 0.9592 time: 0.08s
Test loss: 0.3874 score: 0.9184 time: 0.08s
     INFO: Early stopping counter 2 of 20
Epoch 162/1000, LR 0.000255
Train loss: 0.0016;  Loss pred: 0.0016; Loss self: 0.0000; time: 0.20s
Val loss: 0.1281 score: 0.9592 time: 0.08s
Test loss: 0.3879 score: 0.9184 time: 0.07s
     INFO: Early stopping counter 3 of 20
Epoch 163/1000, LR 0.000255
Train loss: 0.0018;  Loss pred: 0.0018; Loss self: 0.0000; time: 0.10s
Val loss: 0.1288 score: 0.9592 time: 0.07s
Test loss: 0.3883 score: 0.9184 time: 0.07s
     INFO: Early stopping counter 4 of 20
Epoch 164/1000, LR 0.000254
Train loss: 0.0014;  Loss pred: 0.0014; Loss self: 0.0000; time: 0.10s
Val loss: 0.1288 score: 0.9592 time: 0.07s
Test loss: 0.3890 score: 0.9184 time: 0.07s
     INFO: Early stopping counter 5 of 20
Epoch 165/1000, LR 0.000254
Train loss: 0.0014;  Loss pred: 0.0014; Loss self: 0.0000; time: 0.10s
Val loss: 0.1293 score: 0.9592 time: 0.07s
Test loss: 0.3893 score: 0.9184 time: 0.07s
     INFO: Early stopping counter 6 of 20
Epoch 166/1000, LR 0.000254
Train loss: 0.0014;  Loss pred: 0.0014; Loss self: 0.0000; time: 0.11s
Val loss: 0.1295 score: 0.9592 time: 0.07s
Test loss: 0.3895 score: 0.9184 time: 0.07s
     INFO: Early stopping counter 7 of 20
Epoch 167/1000, LR 0.000254
Train loss: 0.0013;  Loss pred: 0.0013; Loss self: 0.0000; time: 0.11s
Val loss: 0.1298 score: 0.9592 time: 0.07s
Test loss: 0.3898 score: 0.9184 time: 0.07s
     INFO: Early stopping counter 8 of 20
Epoch 168/1000, LR 0.000254
Train loss: 0.0016;  Loss pred: 0.0016; Loss self: 0.0000; time: 0.10s
Val loss: 0.1305 score: 0.9592 time: 0.07s
Test loss: 0.3903 score: 0.9184 time: 0.07s
     INFO: Early stopping counter 9 of 20
Epoch 169/1000, LR 0.000253
Train loss: 0.0015;  Loss pred: 0.0015; Loss self: 0.0000; time: 0.21s
Val loss: 0.1303 score: 0.9592 time: 0.07s
Test loss: 0.3910 score: 0.9184 time: 0.07s
     INFO: Early stopping counter 10 of 20
Epoch 170/1000, LR 0.000253
Train loss: 0.0014;  Loss pred: 0.0014; Loss self: 0.0000; time: 0.10s
Val loss: 0.1307 score: 0.9592 time: 0.07s
Test loss: 0.3914 score: 0.9184 time: 0.07s
     INFO: Early stopping counter 11 of 20
Epoch 171/1000, LR 0.000253
Train loss: 0.0014;  Loss pred: 0.0014; Loss self: 0.0000; time: 0.10s
Val loss: 0.1314 score: 0.9592 time: 0.07s
Test loss: 0.3914 score: 0.9184 time: 0.07s
     INFO: Early stopping counter 12 of 20
Epoch 172/1000, LR 0.000253
Train loss: 0.0013;  Loss pred: 0.0013; Loss self: 0.0000; time: 0.10s
Val loss: 0.1315 score: 0.9592 time: 0.07s
Test loss: 0.3921 score: 0.9184 time: 0.07s
     INFO: Early stopping counter 13 of 20
Epoch 173/1000, LR 0.000253
Train loss: 0.0013;  Loss pred: 0.0013; Loss self: 0.0000; time: 0.10s
Val loss: 0.1319 score: 0.9592 time: 0.08s
Test loss: 0.3925 score: 0.9184 time: 0.08s
     INFO: Early stopping counter 14 of 20
Epoch 174/1000, LR 0.000252
Train loss: 0.0017;  Loss pred: 0.0017; Loss self: 0.0000; time: 0.11s
Val loss: 0.1330 score: 0.9592 time: 0.07s
Test loss: 0.3925 score: 0.9184 time: 0.07s
     INFO: Early stopping counter 15 of 20
Epoch 175/1000, LR 0.000252
Train loss: 0.0013;  Loss pred: 0.0013; Loss self: 0.0000; time: 0.11s
Val loss: 0.1340 score: 0.9592 time: 0.07s
Test loss: 0.3927 score: 0.9184 time: 0.07s
     INFO: Early stopping counter 16 of 20
Epoch 176/1000, LR 0.000252
Train loss: 0.0013;  Loss pred: 0.0013; Loss self: 0.0000; time: 0.23s
Val loss: 0.1340 score: 0.9592 time: 0.07s
Test loss: 0.3931 score: 0.9184 time: 0.07s
     INFO: Early stopping counter 17 of 20
Epoch 177/1000, LR 0.000252
Train loss: 0.0012;  Loss pred: 0.0012; Loss self: 0.0000; time: 0.11s
Val loss: 0.1334 score: 0.9592 time: 0.07s
Test loss: 0.3937 score: 0.9184 time: 0.07s
     INFO: Early stopping counter 18 of 20
Epoch 178/1000, LR 0.000251
Train loss: 0.0014;  Loss pred: 0.0014; Loss self: 0.0000; time: 0.11s
Val loss: 0.1335 score: 0.9592 time: 0.07s
Test loss: 0.3946 score: 0.9184 time: 0.07s
     INFO: Early stopping counter 19 of 20
Epoch 179/1000, LR 0.000251
Train loss: 0.0013;  Loss pred: 0.0013; Loss self: 0.0000; time: 0.11s
Val loss: 0.1347 score: 0.9592 time: 0.07s
Test loss: 0.3947 score: 0.9184 time: 0.07s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 158,   Train_Loss: 0.0016,   Val_Loss: 0.1275,   Val_Precision: 0.9583,   Val_Recall: 0.9583,   Val_accuracy: 0.9583,   Val_Score: 0.9592,   Val_Loss: 0.1275,   Test_Precision: 0.9565,   Test_Recall: 0.8800,   Test_accuracy: 0.9167,   Test_Score: 0.9184,   Test_loss: 0.3867


[0.07801941898651421, 0.07876479800324887, 0.0792875699698925, 0.07997004501521587, 0.07804729999043047, 0.07773569202981889, 0.07801035698503256, 0.0789991159690544, 0.0811175829730928, 0.07884221896529198, 0.07714625902008265, 0.08093058597296476, 0.08169240492861718, 0.08145885495468974, 0.08275256596971303, 0.0844363080104813, 0.08344505704008043, 0.08322757505811751, 0.0837428520899266, 0.08364762598648667, 0.08518351393286139, 0.0836860800627619, 0.08406434499192983, 0.08365772699471563, 0.0836440889397636, 0.08409553894307464, 0.08444887702353299, 0.08416446100454777, 0.08483666297979653, 0.0842295119073242, 0.08430745895020664, 0.08427638793364167, 0.08481852803379297, 0.08476802497170866, 0.08436364599037915, 0.08456845791079104, 0.0850465000839904, 0.08445937000215054, 0.08492995193228126, 0.08476895303465426, 0.08531851891893893, 0.08514710399322212, 0.08423514803871512, 0.08422081090975553, 0.0847218680428341, 0.08419094700366259, 0.08500061999075115, 0.08462696603965014, 0.08487939403858036, 0.08481463592033833, 0.08448044001124799, 0.08464586199261248, 0.08449819392990321, 0.08456077193841338, 0.08865917392540723, 0.08662363304756582, 0.0892601840896532, 0.08436056808568537, 0.0912496040109545, 0.09211663692258298, 0.09026035503484309, 0.08474923600442708, 0.08428427705075592, 0.08486541791353375, 0.08434419997502118, 0.08437740209046751, 0.0842743490356952, 0.0841362290084362, 0.08436939492821693, 0.0843358199344948, 0.08455520903225988, 0.08435399003792554, 0.0842872029170394, 0.08414290600921959, 0.08449918800033629, 0.08411446993704885, 0.08443783002439886, 0.08439066202845424, 0.08392996795009822, 0.08477803005371243, 0.08435705804731697, 0.08462367998436093, 0.08433452702593058, 0.08434388507157564, 0.08459920308087021, 0.08431766601279378, 0.08411748998332769, 0.08408475597389042, 0.07551496592350304, 0.07655417500063777, 0.07741969695780426, 0.07682091393508017, 0.0768038589740172, 0.07697760791052133, 0.07694949500728399, 0.0770650370977819, 0.07673422596417367, 0.07672605710104108, 0.08996502298396081, 0.0909528749762103, 0.07942402304615825, 0.07880136894527823, 0.08673327194992453, 0.07782763498835266, 0.22846841800492257, 0.07355149404611439, 0.07465934799984097, 0.07441282295621932, 0.07515920698642731, 0.07727231294848025, 0.07549014000687748, 0.07560773193836212, 0.20366581808775663, 0.07533438899554312, 0.07514048099983484, 0.07509200705680996, 0.0753509639762342, 0.07677237410098314, 0.07497388799674809, 0.08799926203209907, 0.08029275294393301, 0.08045941591262817, 0.08081883809063584, 0.08141504600644112, 0.08231477299705148, 0.0812631999142468, 0.12467141903471202, 0.07695177395362407, 0.0762657430022955, 0.0761341960169375, 0.07676301302853972, 0.07934123591985554, 0.07765807199757546, 0.07912473997566849, 0.08486885402817279, 0.08405006397515535, 0.08448795403819531, 0.0847657909616828, 0.08470635500270873, 0.08394304802641273, 0.08567331300582737, 0.08231392700690776, 0.08241687493864447, 0.08297595207113773, 0.08381504891440272, 0.08069976698607206, 0.08043371606618166, 0.0808918010443449, 0.08157633990049362, 0.08101981098297983, 0.08177325304131955, 0.0827539679594338, 0.08125166001264006, 0.12302837299648672, 0.07663839706219733, 0.07720412407070398, 0.07966816099360585, 0.07563217706046999, 0.0761972600594163, 0.07925507996696979, 0.09092552599031478, 0.07780151604674757, 0.07496272900607437, 0.07477613899391145, 0.07661219593137503, 0.07717890199273825, 0.07540250092279166, 0.07813696505036205, 0.07502132700756192, 0.07467535592149943, 0.07495276804547757, 0.07541854900773615, 0.08227258501574397, 0.076304174028337, 0.07798249204643071, 0.07583905104547739, 0.07563114596996456, 0.07674461009446532, 0.07676534098573029]
[0.0015922330405411063, 0.0016074448572091606, 0.001618113672854949, 0.0016320417350044055, 0.0015928020406210301, 0.0015864426944861, 0.0015920481017353584, 0.0016122268565113144, 0.0016554608770018937, 0.0016090248768426934, 0.0015744134493894419, 0.0016516446116931584, 0.001667191937318718, 0.0016624256113201988, 0.001688827876932919, 0.0017231899593975774, 0.0017029603477567434, 0.0016985219399615818, 0.0017090377977536041, 0.001707094407887483, 0.001738439059854314, 0.0017078791849543245, 0.0017155988773863231, 0.001707300550912564, 0.0017070222232604818, 0.0017162354886341765, 0.0017234464698680201, 0.0017176420613173016, 0.0017313604689754394, 0.0017189696307617183, 0.001720560386738911, 0.001719926284360034, 0.0017309903680365912, 0.0017299596933001767, 0.001721707061028146, 0.0017258868961385926, 0.0017356428588569468, 0.0017236606122887864, 0.001733264325148597, 0.001729978633360291, 0.001741194263651815, 0.001737695999861676, 0.001719084653851329, 0.0017187920593827659, 0.0017290177151598797, 0.0017181825919114814, 0.0017347065304234928, 0.0017270809395846967, 0.0017322325313995992, 0.0017309109371497619, 0.001724090612474449, 0.0017274665712778056, 0.0017244529373449634, 0.0017257300395594568, 0.0018093708964368822, 0.0017678292458686903, 0.0018216364099929224, 0.0017216442466466402, 0.0018622368165500918, 0.0018799313657669996, 0.0018420480619355732, 0.0017295762449883077, 0.0017200872867501207, 0.0017319473043578316, 0.001721310203571861, 0.001721987797764643, 0.001719884674197861, 0.001717065898131351, 0.0017218243862901414, 0.0017211391823366284, 0.0017256165108624467, 0.0017215100007739906, 0.0017201469983069264, 0.001717202163453461, 0.001724473224496659, 0.0017166218354499765, 0.0017232210209060992, 0.0017222584087439642, 0.0017128564887775146, 0.0017301638786471924, 0.0017215726132105505, 0.0017270138772318558, 0.001721112796447563, 0.0017213037769709314, 0.001726514348589188, 0.0017207686941386486, 0.0017166834690475038, 0.00171601542803858, 0.0015411217535408785, 0.001562330102053832, 0.0015799938154653932, 0.0015677737537771463, 0.0015674256933472898, 0.0015709715900106393, 0.00157039785729151, 0.0015727558591384062, 0.0015660046115137485, 0.0015658379000212466, 0.00183602087722369, 0.0018561811219634755, 0.0016208984295134337, 0.0016081912029648618, 0.0017700667744882557, 0.0015883190813949521, 0.004662620775610664, 0.0015010508989002935, 0.0015236601632620608, 0.001518629039922843, 0.001533861367069945, 0.0015769859785404131, 0.0015406151021811732, 0.0015430149375175943, 0.004156445267097074, 0.001537436510113125, 0.001533479204078262, 0.0015324899399348972, 0.0015377747750251877, 0.0015667831449180233, 0.00153007934687241, 0.001795903306777532, 0.0016386276111006737, 0.0016420288961760852, 0.0016493640426660375, 0.0016615315511518596, 0.0016798933264704384, 0.0016584326513111591, 0.0025443146741777963, 0.0015704443664004912, 0.0015564437347407244, 0.0015537591023864795, 0.0015665921026232596, 0.0016192088963235824, 0.0015848586121954176, 0.0016147906117483365, 0.0017320174291463836, 0.001715307428064395, 0.0017242439599631696, 0.0017299141012588326, 0.0017287011225042598, 0.0017131234291104637, 0.0017484349593025993, 0.0016798760613654644, 0.0016819770395641728, 0.0016933867769619944, 0.0017105112023347495, 0.0016469340201239197, 0.0016415044095139115, 0.001650853082537651, 0.00166482326327538, 0.0016534655302648945, 0.0016688418988024397, 0.0016888564889680367, 0.0016581971431151032, 0.00251078312237728, 0.0015640489196366801, 0.0015755943687898771, 0.001625880836604201, 0.001543513817560612, 0.001555046123661557, 0.001617450611570812, 0.0018556229793941792, 0.0015877860417703585, 0.0015298516123688646, 0.0015260436529369683, 0.001563514202681123, 0.0015750796325048621, 0.0015388265494447276, 0.0015946319398033072, 0.0015310474899502434, 0.0015239868555408046, 0.001529648327458726, 0.0015391540613823704, 0.001679032347260081, 0.0015572280413946326, 0.001591479429518994, 0.0015477357356219875, 0.001543492774897236, 0.0015662165325401084, 0.0015666396119536794]
[628.0487683261232, 622.1053216943293, 618.0035536289804, 612.7294287589408, 627.8244091212377, 630.341079117221, 628.1217250345537, 620.2601054319939, 604.0613909348557, 621.4944308146736, 635.1571757646001, 605.4571261397845, 599.8109621428846, 601.5306749309883, 592.1266540294802, 580.3190730925552, 587.2127330018393, 588.7471786338059, 585.1245661824574, 585.7906835026732, 575.2286767439537, 585.5215104262448, 582.8868351344913, 585.7199539152571, 585.8154547572084, 582.6706221975547, 580.2327008604909, 582.1934747179372, 577.5804738061082, 581.743843581969, 581.2059882974316, 581.4202673064498, 577.7039655825866, 578.0481498342536, 580.81889923994, 579.4122443581597, 576.1553967724536, 580.1606144913507, 576.9460465380937, 578.0418212782266, 574.3184553702212, 575.47465153836, 581.7049193939711, 581.8039445441174, 578.3630735718237, 582.0103199203631, 576.4663834843988, 579.0116589674514, 577.2897009341037, 577.7304762119473, 580.0159184004722, 578.8824030674589, 579.8940512343815, 579.4649088076832, 552.678282804956, 565.6654919228989, 548.9569677649808, 580.8400904819714, 536.9886316889392, 531.9343132465938, 542.8740002306062, 578.1763035296315, 581.3658456190144, 577.3847723217989, 580.9528102052248, 580.724207975298, 581.4343339424146, 582.3888303228666, 580.7793221901155, 581.0105366623484, 579.5030319339085, 580.8853852434202, 581.3456646346277, 582.3426159613628, 579.8872292098829, 582.5394850216798, 580.3086126898469, 580.632961304161, 583.8200728151554, 577.9799314628483, 580.8642588331526, 579.0341427961482, 581.0194439690617, 580.9549792307738, 579.20167348574, 581.1356304924888, 582.5185702725073, 582.745343462913, 648.877999225176, 640.0695977664417, 632.9138697960323, 637.8471368019511, 637.9887762745975, 636.5487487862384, 636.7813069515491, 635.8265932945399, 638.567723650168, 638.635710622684, 544.6561160634155, 538.7405292335891, 616.941803256718, 621.8166087194107, 564.9504382619182, 629.5964153007235, 214.47165620477247, 666.1999274858863, 656.3143305256881, 658.4886589886407, 651.9494013401273, 634.121047116446, 649.0913912139504, 648.0818660179675, 240.59020045713618, 650.433363213431, 652.1118756227779, 652.5328316624915, 650.2902871349419, 638.2504198130888, 653.5608771166479, 556.8228513339863, 610.2667825353532, 609.0026809691196, 606.2942892726076, 601.854354981552, 595.2758929646224, 602.9789628233614, 393.03314568319, 636.7624485113294, 642.4902986721727, 643.6004130009992, 638.3282529801468, 617.5855396240117, 630.971111432303, 619.2753368297691, 577.3613955448735, 582.9858739249036, 579.96433406173, 578.0633843451042, 578.4689944270779, 583.729101480591, 571.9400625567858, 595.2820109759547, 594.5384368975191, 590.5325431878246, 584.6205500642483, 607.18886596608, 609.1972669729981, 605.7474226978602, 600.6643600309836, 604.7903519584072, 599.2179371320912, 592.1166224200865, 603.0646019093915, 398.2821101064165, 639.3661908172887, 634.6811208572941, 615.051224841662, 647.8723990825117, 643.0677423544009, 618.2568993737834, 538.9025740166671, 629.8077786885029, 653.6581665273877, 655.2892494755548, 639.5848520500768, 634.888534752806, 649.845819440041, 627.1039573704682, 653.1476042147448, 656.1736384826877, 653.7450354104236, 649.707540713542, 595.5811403108726, 642.1667048227653, 628.3461673785116, 646.1051308595202, 647.8812316219484, 638.4813205733361, 638.3088952748674]
Elapsed: 0.08345590953912857~0.015398523952903262
Time per graph: 0.0017031818273291542~0.00031425559087557676
Speed: 596.4449476804832~54.29233744780896
Total Time: 0.0775
best val loss: 0.12753410637378693 test_score: 0.9184

Testing...
Test loss: 0.3844 score: 0.9184 time: 0.07s
test Score 0.9184
Epoch Time List: [0.2533633179264143, 0.25523712602443993, 0.26129988906905055, 0.26074120204430073, 0.3130132920341566, 0.2535937699722126, 0.2532034239266068, 0.2544539209920913, 0.2583321820711717, 0.26281752600334585, 0.25422287394758314, 0.3531752530252561, 0.2634344029938802, 0.26432531292084605, 0.26687420601956546, 0.2699742551194504, 0.27065658592619, 0.27145074389409274, 0.27259264304302633, 0.2720676128519699, 0.2784268979448825, 0.2739965708460659, 0.27244734996929765, 0.27218683902174234, 0.2729487620526925, 0.2739525850629434, 0.2734550080494955, 0.2740610180189833, 0.2742085180943832, 0.2738697270397097, 0.2744393680477515, 0.2732720379717648, 0.27533826883882284, 0.27456349006388336, 0.2739639839855954, 0.2739821680588648, 0.27602097601629794, 0.27511241601314396, 0.2749424851499498, 0.2762853589374572, 0.2760853199288249, 0.27596198697574437, 0.27508058201055974, 0.2745959711028263, 0.2746261080028489, 0.2744255799334496, 0.2752882060594857, 0.2762459581717849, 0.2755902099888772, 0.27550380607135594, 0.27614147611893713, 0.27519897499587387, 0.2752912261057645, 0.274683310999535, 0.2919426260050386, 0.2905679289251566, 0.2884237340185791, 0.27751078794244677, 0.2840493740513921, 0.2845868718577549, 0.2880165280075744, 0.27850325300823897, 0.27432679291814566, 0.27452952007297426, 0.27518069406505674, 0.274304919061251, 0.2739726809086278, 0.2753040270181373, 0.27545380184892565, 0.2750368268461898, 0.2746617979137227, 0.27501191198825836, 0.2739022789755836, 0.2740118959918618, 0.27437075006309897, 0.27350182295776904, 0.2742271829629317, 0.27366328006610274, 0.2736710680183023, 0.2749212079215795, 0.2741992159280926, 0.27417869213968515, 0.2746027240063995, 0.27421689219772816, 0.27472517208661884, 0.2745129718678072, 0.27457469410728663, 0.27247924904804677, 0.2553659479599446, 0.24471239582635462, 0.2540698539232835, 0.25235631701070815, 0.250837322906591, 0.2502095610834658, 0.2524581920588389, 0.2521049020579085, 0.2507076249457896, 0.2514217549469322, 0.2652400430524722, 0.2910372900078073, 0.2653393360087648, 0.25863847497384995, 0.2705163008067757, 0.26604870497249067, 0.40086335001979023, 0.2740782320033759, 0.2424132350133732, 0.24386269506067038, 0.2456055210204795, 0.2523443379905075, 0.25241483200807124, 0.24689164804294705, 0.37482072599232197, 0.24601255706511438, 0.24393544101621956, 0.2449627669993788, 0.24492574797477573, 0.2505801870720461, 0.24906050297431648, 0.2560853749746457, 0.3367396420799196, 0.26060391683131456, 0.2623578259954229, 0.2659405739977956, 0.2693553731078282, 0.2660223300335929, 0.30324602185282856, 0.27554754295852035, 0.24948814895469695, 0.24877116410061717, 0.24947944295126945, 0.2567473850212991, 0.2574212980689481, 0.2558888590428978, 0.369314456009306, 0.27027786895632744, 0.27141569508239627, 0.2737348999362439, 0.2760472389636561, 0.2725921889068559, 0.2693867600755766, 0.38798508199397475, 0.2684834299143404, 0.2683445338625461, 0.2714744119439274, 0.26594715483952314, 0.2604426150210202, 0.36730664991773665, 0.2648060499923304, 0.26343250495847315, 0.26581590401474386, 0.27098937006667256, 0.26782627613283694, 0.3042041639564559, 0.2633575400104746, 0.25693897204473615, 0.26996849407441914, 0.2530972521053627, 0.26057517202571034, 0.25471366092097014, 0.2783042420633137, 0.3457916658371687, 0.2453843040857464, 0.24575288011692464, 0.24972877418622375, 0.2550875339657068, 0.2517511359183118, 0.24981783714611083, 0.3541054190136492, 0.24490787996910512, 0.2447138640563935, 0.24628784705419093, 0.25712918920908123, 0.2531092829303816, 0.25211683998350054, 0.37304728606250137, 0.2512670719297603, 0.25146548193879426, 0.25644653697963804]
Total Epoch List: [179]
Total Time List: [0.07753726898226887]
========================k_idx:1========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: I2BGNNA
I2BGNN(
  (gcs): ModuleList(
    (0): GCNConv(14887, 64)
    (1): GCNConv(64, 64)
  )
  (bns): ModuleList(
    (0-1): 2 x BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  )
  (drops): ModuleList(
    (0-1): 2 x Dropout(p=0.2, inplace=False)
  )
  (lin1): Linear(in_features=64, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=64, bias=True)
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=64, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=64, out_features=2, bias=True)
  )
)
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7d1ecd7f0b80>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.6985;  Loss pred: 0.6985; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6943 score: 0.5102 time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6973 score: 0.4898 time: 0.08s
Epoch 2/1000, LR 0.000000
Train loss: 0.6988;  Loss pred: 0.6988; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6943 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6972 score: 0.4898 time: 0.08s
Epoch 3/1000, LR 0.000030
Train loss: 0.6960;  Loss pred: 0.6960; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6942 score: 0.5102 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6970 score: 0.4898 time: 0.08s
Epoch 4/1000, LR 0.000060
Train loss: 0.6955;  Loss pred: 0.6955; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6941 score: 0.5102 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6968 score: 0.4898 time: 0.08s
Epoch 5/1000, LR 0.000090
Train loss: 0.6953;  Loss pred: 0.6953; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6940 score: 0.5102 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6966 score: 0.4898 time: 0.08s
Epoch 6/1000, LR 0.000120
Train loss: 0.6945;  Loss pred: 0.6945; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6938 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6964 score: 0.4898 time: 0.07s
Epoch 7/1000, LR 0.000150
Train loss: 0.6927;  Loss pred: 0.6927; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6937 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6962 score: 0.4898 time: 0.07s
Epoch 8/1000, LR 0.000180
Train loss: 0.6917;  Loss pred: 0.6917; Loss self: 0.0000; time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6936 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6960 score: 0.4898 time: 0.07s
Epoch 9/1000, LR 0.000210
Train loss: 0.6888;  Loss pred: 0.6888; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6935 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6958 score: 0.4898 time: 0.07s
Epoch 10/1000, LR 0.000240
Train loss: 0.6890;  Loss pred: 0.6890; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6934 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6956 score: 0.4898 time: 0.07s
Epoch 11/1000, LR 0.000270
Train loss: 0.6847;  Loss pred: 0.6847; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6933 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6954 score: 0.4898 time: 0.07s
Epoch 12/1000, LR 0.000270
Train loss: 0.6820;  Loss pred: 0.6820; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6932 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6952 score: 0.4898 time: 0.08s
Epoch 13/1000, LR 0.000270
Train loss: 0.6819;  Loss pred: 0.6819; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6932 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6950 score: 0.4898 time: 0.07s
Epoch 14/1000, LR 0.000270
Train loss: 0.6793;  Loss pred: 0.6793; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6931 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6948 score: 0.4898 time: 0.08s
Epoch 15/1000, LR 0.000270
Train loss: 0.6772;  Loss pred: 0.6772; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6930 score: 0.5102 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6946 score: 0.4898 time: 0.08s
Epoch 16/1000, LR 0.000270
Train loss: 0.6731;  Loss pred: 0.6731; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6929 score: 0.5102 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6945 score: 0.4898 time: 0.08s
Epoch 17/1000, LR 0.000270
Train loss: 0.6702;  Loss pred: 0.6702; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6929 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6943 score: 0.4898 time: 0.08s
Epoch 18/1000, LR 0.000270
Train loss: 0.6643;  Loss pred: 0.6643; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6928 score: 0.5102 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6941 score: 0.4898 time: 0.08s
Epoch 19/1000, LR 0.000270
Train loss: 0.6620;  Loss pred: 0.6620; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6927 score: 0.5102 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6938 score: 0.4898 time: 0.08s
Epoch 20/1000, LR 0.000270
Train loss: 0.6565;  Loss pred: 0.6565; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6926 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6936 score: 0.4898 time: 0.08s
Epoch 21/1000, LR 0.000270
Train loss: 0.6500;  Loss pred: 0.6500; Loss self: 0.0000; time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6925 score: 0.5102 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6934 score: 0.4898 time: 0.08s
Epoch 22/1000, LR 0.000270
Train loss: 0.6444;  Loss pred: 0.6444; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6924 score: 0.5102 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6932 score: 0.4898 time: 0.08s
Epoch 23/1000, LR 0.000270
Train loss: 0.6370;  Loss pred: 0.6370; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6924 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6930 score: 0.4898 time: 0.08s
Epoch 24/1000, LR 0.000270
Train loss: 0.6303;  Loss pred: 0.6303; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6922 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6927 score: 0.4898 time: 0.08s
Epoch 25/1000, LR 0.000270
Train loss: 0.6195;  Loss pred: 0.6195; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6921 score: 0.5102 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6925 score: 0.4898 time: 0.08s
Epoch 26/1000, LR 0.000270
Train loss: 0.6123;  Loss pred: 0.6123; Loss self: 0.0000; time: 0.12s
Val loss: 0.6920 score: 0.6939 time: 0.08s
Test loss: 0.6922 score: 0.5918 time: 0.08s
Epoch 27/1000, LR 0.000270
Train loss: 0.6050;  Loss pred: 0.6050; Loss self: 0.0000; time: 0.12s
Val loss: 0.6918 score: 0.6327 time: 0.07s
Test loss: 0.6919 score: 0.5918 time: 0.08s
Epoch 28/1000, LR 0.000270
Train loss: 0.5976;  Loss pred: 0.5976; Loss self: 0.0000; time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6917 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6916 score: 0.5102 time: 0.08s
Epoch 29/1000, LR 0.000270
Train loss: 0.5883;  Loss pred: 0.5883; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6916 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6914 score: 0.5102 time: 0.08s
Epoch 30/1000, LR 0.000270
Train loss: 0.5804;  Loss pred: 0.5804; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6914 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6910 score: 0.5102 time: 0.08s
Epoch 31/1000, LR 0.000270
Train loss: 0.5618;  Loss pred: 0.5618; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6910 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6906 score: 0.5102 time: 0.08s
Epoch 32/1000, LR 0.000270
Train loss: 0.5551;  Loss pred: 0.5551; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6908 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6902 score: 0.5102 time: 0.08s
Epoch 33/1000, LR 0.000270
Train loss: 0.5437;  Loss pred: 0.5437; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6906 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6899 score: 0.5102 time: 0.08s
Epoch 34/1000, LR 0.000270
Train loss: 0.5314;  Loss pred: 0.5314; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6903 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6895 score: 0.5102 time: 0.08s
Epoch 35/1000, LR 0.000270
Train loss: 0.5166;  Loss pred: 0.5166; Loss self: 0.0000; time: 0.29s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6899 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6890 score: 0.5102 time: 0.08s
Epoch 36/1000, LR 0.000270
Train loss: 0.5030;  Loss pred: 0.5030; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6895 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6884 score: 0.5102 time: 0.08s
Epoch 37/1000, LR 0.000270
Train loss: 0.4904;  Loss pred: 0.4904; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6890 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6878 score: 0.5102 time: 0.08s
Epoch 38/1000, LR 0.000270
Train loss: 0.4687;  Loss pred: 0.4687; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6883 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6870 score: 0.5102 time: 0.08s
Epoch 39/1000, LR 0.000269
Train loss: 0.4622;  Loss pred: 0.4622; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6874 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6860 score: 0.5102 time: 0.08s
Epoch 40/1000, LR 0.000269
Train loss: 0.4471;  Loss pred: 0.4471; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6865 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6850 score: 0.5102 time: 0.08s
Epoch 41/1000, LR 0.000269
Train loss: 0.4294;  Loss pred: 0.4294; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6856 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6839 score: 0.5102 time: 0.08s
Epoch 42/1000, LR 0.000269
Train loss: 0.4095;  Loss pred: 0.4095; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6845 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6827 score: 0.5102 time: 0.08s
Epoch 43/1000, LR 0.000269
Train loss: 0.3981;  Loss pred: 0.3981; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6833 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6814 score: 0.5102 time: 0.07s
Epoch 44/1000, LR 0.000269
Train loss: 0.3826;  Loss pred: 0.3826; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6818 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6797 score: 0.5102 time: 0.07s
Epoch 45/1000, LR 0.000269
Train loss: 0.3648;  Loss pred: 0.3648; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6799 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6778 score: 0.5102 time: 0.07s
Epoch 46/1000, LR 0.000269
Train loss: 0.3484;  Loss pred: 0.3484; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6778 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6755 score: 0.5102 time: 0.07s
Epoch 47/1000, LR 0.000269
Train loss: 0.3274;  Loss pred: 0.3274; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6753 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6730 score: 0.5102 time: 0.07s
Epoch 48/1000, LR 0.000269
Train loss: 0.3170;  Loss pred: 0.3170; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6723 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6699 score: 0.5102 time: 0.07s
Epoch 49/1000, LR 0.000269
Train loss: 0.2950;  Loss pred: 0.2950; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6688 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6663 score: 0.5102 time: 0.07s
Epoch 50/1000, LR 0.000269
Train loss: 0.2816;  Loss pred: 0.2816; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6647 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6622 score: 0.5102 time: 0.07s
Epoch 51/1000, LR 0.000269
Train loss: 0.2677;  Loss pred: 0.2677; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6602 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6575 score: 0.5102 time: 0.07s
Epoch 52/1000, LR 0.000269
Train loss: 0.2526;  Loss pred: 0.2526; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6551 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6524 score: 0.5102 time: 0.07s
Epoch 53/1000, LR 0.000269
Train loss: 0.2342;  Loss pred: 0.2342; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6496 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6469 score: 0.5102 time: 0.07s
Epoch 54/1000, LR 0.000269
Train loss: 0.2159;  Loss pred: 0.2159; Loss self: 0.0000; time: 0.12s
Val loss: 0.6436 score: 0.5510 time: 0.07s
Test loss: 0.6408 score: 0.5510 time: 0.07s
Epoch 55/1000, LR 0.000269
Train loss: 0.2136;  Loss pred: 0.2136; Loss self: 0.0000; time: 0.12s
Val loss: 0.6372 score: 0.5714 time: 0.07s
Test loss: 0.6342 score: 0.5714 time: 0.07s
Epoch 56/1000, LR 0.000269
Train loss: 0.2009;  Loss pred: 0.2009; Loss self: 0.0000; time: 0.12s
Val loss: 0.6301 score: 0.6327 time: 0.07s
Test loss: 0.6268 score: 0.6122 time: 0.07s
Epoch 57/1000, LR 0.000269
Train loss: 0.1842;  Loss pred: 0.1842; Loss self: 0.0000; time: 0.12s
Val loss: 0.6225 score: 0.6939 time: 0.07s
Test loss: 0.6188 score: 0.6531 time: 0.07s
Epoch 58/1000, LR 0.000269
Train loss: 0.1750;  Loss pred: 0.1750; Loss self: 0.0000; time: 0.12s
Val loss: 0.6143 score: 0.7347 time: 0.07s
Test loss: 0.6101 score: 0.6939 time: 0.07s
Epoch 59/1000, LR 0.000268
Train loss: 0.1601;  Loss pred: 0.1601; Loss self: 0.0000; time: 0.12s
Val loss: 0.6054 score: 0.7551 time: 0.07s
Test loss: 0.6007 score: 0.7551 time: 0.07s
Epoch 60/1000, LR 0.000268
Train loss: 0.1545;  Loss pred: 0.1545; Loss self: 0.0000; time: 0.12s
Val loss: 0.5959 score: 0.7551 time: 0.07s
Test loss: 0.5905 score: 0.8163 time: 0.07s
Epoch 61/1000, LR 0.000268
Train loss: 0.1384;  Loss pred: 0.1384; Loss self: 0.0000; time: 0.12s
Val loss: 0.5858 score: 0.8367 time: 0.07s
Test loss: 0.5795 score: 0.8571 time: 0.07s
Epoch 62/1000, LR 0.000268
Train loss: 0.1354;  Loss pred: 0.1354; Loss self: 0.0000; time: 0.12s
Val loss: 0.5750 score: 0.8367 time: 0.07s
Test loss: 0.5675 score: 0.8571 time: 0.07s
Epoch 63/1000, LR 0.000268
Train loss: 0.1215;  Loss pred: 0.1215; Loss self: 0.0000; time: 0.12s
Val loss: 0.5635 score: 0.8776 time: 0.07s
Test loss: 0.5546 score: 0.8980 time: 0.07s
Epoch 64/1000, LR 0.000268
Train loss: 0.1090;  Loss pred: 0.1090; Loss self: 0.0000; time: 0.12s
Val loss: 0.5516 score: 0.8776 time: 0.07s
Test loss: 0.5410 score: 0.9592 time: 0.07s
Epoch 65/1000, LR 0.000268
Train loss: 0.1016;  Loss pred: 0.1016; Loss self: 0.0000; time: 0.12s
Val loss: 0.5390 score: 0.8980 time: 0.07s
Test loss: 0.5265 score: 0.9592 time: 0.07s
Epoch 66/1000, LR 0.000268
Train loss: 0.0965;  Loss pred: 0.0965; Loss self: 0.0000; time: 0.12s
Val loss: 0.5259 score: 0.8980 time: 0.07s
Test loss: 0.5113 score: 0.9796 time: 0.07s
Epoch 67/1000, LR 0.000268
Train loss: 0.0873;  Loss pred: 0.0873; Loss self: 0.0000; time: 0.12s
Val loss: 0.5122 score: 0.9184 time: 0.07s
Test loss: 0.4951 score: 0.9796 time: 0.07s
Epoch 68/1000, LR 0.000268
Train loss: 0.0841;  Loss pred: 0.0841; Loss self: 0.0000; time: 0.12s
Val loss: 0.4982 score: 0.9184 time: 0.07s
Test loss: 0.4785 score: 0.9796 time: 0.07s
Epoch 69/1000, LR 0.000268
Train loss: 0.0763;  Loss pred: 0.0763; Loss self: 0.0000; time: 0.12s
Val loss: 0.4840 score: 0.8980 time: 0.07s
Test loss: 0.4612 score: 0.9796 time: 0.07s
Epoch 70/1000, LR 0.000268
Train loss: 0.0702;  Loss pred: 0.0702; Loss self: 0.0000; time: 0.12s
Val loss: 0.4695 score: 0.8776 time: 0.07s
Test loss: 0.4433 score: 0.9796 time: 0.07s
Epoch 71/1000, LR 0.000268
Train loss: 0.0693;  Loss pred: 0.0693; Loss self: 0.0000; time: 0.12s
Val loss: 0.4552 score: 0.8776 time: 0.07s
Test loss: 0.4252 score: 1.0000 time: 0.07s
Epoch 72/1000, LR 0.000267
Train loss: 0.0637;  Loss pred: 0.0637; Loss self: 0.0000; time: 0.12s
Val loss: 0.4409 score: 0.8776 time: 0.07s
Test loss: 0.4070 score: 0.9796 time: 0.07s
Epoch 73/1000, LR 0.000267
Train loss: 0.0560;  Loss pred: 0.0560; Loss self: 0.0000; time: 0.12s
Val loss: 0.4268 score: 0.8776 time: 0.07s
Test loss: 0.3887 score: 0.9796 time: 0.07s
Epoch 74/1000, LR 0.000267
Train loss: 0.0554;  Loss pred: 0.0554; Loss self: 0.0000; time: 0.12s
Val loss: 0.4127 score: 0.8776 time: 0.07s
Test loss: 0.3700 score: 0.9796 time: 0.07s
Epoch 75/1000, LR 0.000267
Train loss: 0.0559;  Loss pred: 0.0559; Loss self: 0.0000; time: 0.12s
Val loss: 0.3991 score: 0.8776 time: 0.07s
Test loss: 0.3516 score: 0.9796 time: 0.07s
Epoch 76/1000, LR 0.000267
Train loss: 0.0469;  Loss pred: 0.0469; Loss self: 0.0000; time: 0.12s
Val loss: 0.3859 score: 0.8776 time: 0.07s
Test loss: 0.3332 score: 0.9796 time: 0.07s
Epoch 77/1000, LR 0.000267
Train loss: 0.0473;  Loss pred: 0.0473; Loss self: 0.0000; time: 0.12s
Val loss: 0.3731 score: 0.8776 time: 0.07s
Test loss: 0.3152 score: 0.9796 time: 0.07s
Epoch 78/1000, LR 0.000267
Train loss: 0.0454;  Loss pred: 0.0454; Loss self: 0.0000; time: 0.12s
Val loss: 0.3610 score: 0.8776 time: 0.07s
Test loss: 0.2975 score: 0.9796 time: 0.07s
Epoch 79/1000, LR 0.000267
Train loss: 0.0398;  Loss pred: 0.0398; Loss self: 0.0000; time: 0.12s
Val loss: 0.3496 score: 0.8776 time: 0.07s
Test loss: 0.2805 score: 0.9796 time: 0.07s
Epoch 80/1000, LR 0.000267
Train loss: 0.0361;  Loss pred: 0.0361; Loss self: 0.0000; time: 0.12s
Val loss: 0.3387 score: 0.8980 time: 0.07s
Test loss: 0.2640 score: 0.9796 time: 0.07s
Epoch 81/1000, LR 0.000267
Train loss: 0.0332;  Loss pred: 0.0332; Loss self: 0.0000; time: 0.12s
Val loss: 0.3285 score: 0.8980 time: 0.07s
Test loss: 0.2481 score: 0.9796 time: 0.07s
Epoch 82/1000, LR 0.000267
Train loss: 0.0326;  Loss pred: 0.0326; Loss self: 0.0000; time: 0.12s
Val loss: 0.3190 score: 0.8980 time: 0.07s
Test loss: 0.2329 score: 0.9796 time: 0.07s
Epoch 83/1000, LR 0.000266
Train loss: 0.0286;  Loss pred: 0.0286; Loss self: 0.0000; time: 0.12s
Val loss: 0.3103 score: 0.8980 time: 0.07s
Test loss: 0.2182 score: 0.9796 time: 0.07s
Epoch 84/1000, LR 0.000266
Train loss: 0.0283;  Loss pred: 0.0283; Loss self: 0.0000; time: 0.12s
Val loss: 0.3024 score: 0.8980 time: 0.07s
Test loss: 0.2042 score: 0.9796 time: 0.07s
Epoch 85/1000, LR 0.000266
Train loss: 0.0266;  Loss pred: 0.0266; Loss self: 0.0000; time: 0.12s
Val loss: 0.2951 score: 0.8980 time: 0.07s
Test loss: 0.1909 score: 0.9796 time: 0.07s
Epoch 86/1000, LR 0.000266
Train loss: 0.0265;  Loss pred: 0.0265; Loss self: 0.0000; time: 0.12s
Val loss: 0.2885 score: 0.8980 time: 0.07s
Test loss: 0.1783 score: 0.9796 time: 0.07s
Epoch 87/1000, LR 0.000266
Train loss: 0.0207;  Loss pred: 0.0207; Loss self: 0.0000; time: 0.12s
Val loss: 0.2827 score: 0.8980 time: 0.07s
Test loss: 0.1664 score: 0.9796 time: 0.07s
Epoch 88/1000, LR 0.000266
Train loss: 0.0180;  Loss pred: 0.0180; Loss self: 0.0000; time: 0.12s
Val loss: 0.2773 score: 0.8980 time: 0.07s
Test loss: 0.1549 score: 0.9796 time: 0.07s
Epoch 89/1000, LR 0.000266
Train loss: 0.0218;  Loss pred: 0.0218; Loss self: 0.0000; time: 0.12s
Val loss: 0.2726 score: 0.8980 time: 0.07s
Test loss: 0.1441 score: 1.0000 time: 0.08s
Epoch 90/1000, LR 0.000266
Train loss: 0.0170;  Loss pred: 0.0170; Loss self: 0.0000; time: 0.12s
Val loss: 0.2685 score: 0.8980 time: 0.07s
Test loss: 0.1339 score: 1.0000 time: 0.07s
Epoch 91/1000, LR 0.000266
Train loss: 0.0184;  Loss pred: 0.0184; Loss self: 0.0000; time: 0.12s
Val loss: 0.2650 score: 0.8980 time: 0.07s
Test loss: 0.1242 score: 1.0000 time: 0.07s
Epoch 92/1000, LR 0.000266
Train loss: 0.0167;  Loss pred: 0.0167; Loss self: 0.0000; time: 0.12s
Val loss: 0.2622 score: 0.8980 time: 0.07s
Test loss: 0.1154 score: 1.0000 time: 0.07s
Epoch 93/1000, LR 0.000265
Train loss: 0.0190;  Loss pred: 0.0190; Loss self: 0.0000; time: 0.12s
Val loss: 0.2600 score: 0.8980 time: 0.07s
Test loss: 0.1074 score: 1.0000 time: 0.07s
Epoch 94/1000, LR 0.000265
Train loss: 0.0156;  Loss pred: 0.0156; Loss self: 0.0000; time: 0.12s
Val loss: 0.2584 score: 0.8980 time: 0.07s
Test loss: 0.0999 score: 1.0000 time: 0.07s
Epoch 95/1000, LR 0.000265
Train loss: 0.0138;  Loss pred: 0.0138; Loss self: 0.0000; time: 0.12s
Val loss: 0.2574 score: 0.8980 time: 0.07s
Test loss: 0.0931 score: 1.0000 time: 0.07s
Epoch 96/1000, LR 0.000265
Train loss: 0.0116;  Loss pred: 0.0116; Loss self: 0.0000; time: 0.12s
Val loss: 0.2568 score: 0.8980 time: 0.07s
Test loss: 0.0869 score: 1.0000 time: 0.07s
Epoch 97/1000, LR 0.000265
Train loss: 0.0119;  Loss pred: 0.0119; Loss self: 0.0000; time: 0.12s
Val loss: 0.2567 score: 0.8980 time: 0.07s
Test loss: 0.0811 score: 1.0000 time: 0.07s
Epoch 98/1000, LR 0.000265
Train loss: 0.0104;  Loss pred: 0.0104; Loss self: 0.0000; time: 0.12s
Val loss: 0.2568 score: 0.8980 time: 0.07s
Test loss: 0.0760 score: 1.0000 time: 0.07s
     INFO: Early stopping counter 1 of 20
Epoch 99/1000, LR 0.000265
Train loss: 0.0105;  Loss pred: 0.0105; Loss self: 0.0000; time: 0.12s
Val loss: 0.2574 score: 0.8980 time: 0.07s
Test loss: 0.0712 score: 1.0000 time: 0.07s
     INFO: Early stopping counter 2 of 20
Epoch 100/1000, LR 0.000265
Train loss: 0.0105;  Loss pred: 0.0105; Loss self: 0.0000; time: 0.12s
Val loss: 0.2582 score: 0.8980 time: 0.07s
Test loss: 0.0668 score: 1.0000 time: 0.08s
     INFO: Early stopping counter 3 of 20
Epoch 101/1000, LR 0.000265
Train loss: 0.0108;  Loss pred: 0.0108; Loss self: 0.0000; time: 0.12s
Val loss: 0.2595 score: 0.9184 time: 0.07s
Test loss: 0.0629 score: 1.0000 time: 0.07s
     INFO: Early stopping counter 4 of 20
Epoch 102/1000, LR 0.000264
Train loss: 0.0108;  Loss pred: 0.0108; Loss self: 0.0000; time: 0.12s
Val loss: 0.2610 score: 0.9184 time: 0.07s
Test loss: 0.0595 score: 1.0000 time: 0.07s
     INFO: Early stopping counter 5 of 20
Epoch 103/1000, LR 0.000264
Train loss: 0.0090;  Loss pred: 0.0090; Loss self: 0.0000; time: 0.12s
Val loss: 0.2625 score: 0.9184 time: 0.07s
Test loss: 0.0563 score: 1.0000 time: 0.07s
     INFO: Early stopping counter 6 of 20
Epoch 104/1000, LR 0.000264
Train loss: 0.0082;  Loss pred: 0.0082; Loss self: 0.0000; time: 0.12s
Val loss: 0.2641 score: 0.9184 time: 0.07s
Test loss: 0.0535 score: 1.0000 time: 0.07s
     INFO: Early stopping counter 7 of 20
Epoch 105/1000, LR 0.000264
Train loss: 0.0077;  Loss pred: 0.0077; Loss self: 0.0000; time: 0.12s
Val loss: 0.2658 score: 0.9184 time: 0.07s
Test loss: 0.0509 score: 1.0000 time: 0.07s
     INFO: Early stopping counter 8 of 20
Epoch 106/1000, LR 0.000264
Train loss: 0.0085;  Loss pred: 0.0085; Loss self: 0.0000; time: 0.12s
Val loss: 0.2677 score: 0.9184 time: 0.07s
Test loss: 0.0486 score: 1.0000 time: 0.08s
     INFO: Early stopping counter 9 of 20
Epoch 107/1000, LR 0.000264
Train loss: 0.0071;  Loss pred: 0.0071; Loss self: 0.0000; time: 0.12s
Val loss: 0.2695 score: 0.9184 time: 0.07s
Test loss: 0.0464 score: 1.0000 time: 0.07s
     INFO: Early stopping counter 10 of 20
Epoch 108/1000, LR 0.000264
Train loss: 0.0056;  Loss pred: 0.0056; Loss self: 0.0000; time: 0.12s
Val loss: 0.2715 score: 0.9184 time: 0.07s
Test loss: 0.0444 score: 1.0000 time: 0.07s
     INFO: Early stopping counter 11 of 20
Epoch 109/1000, LR 0.000264
Train loss: 0.0072;  Loss pred: 0.0072; Loss self: 0.0000; time: 0.12s
Val loss: 0.2736 score: 0.9184 time: 0.07s
Test loss: 0.0426 score: 1.0000 time: 0.07s
     INFO: Early stopping counter 12 of 20
Epoch 110/1000, LR 0.000263
Train loss: 0.0074;  Loss pred: 0.0074; Loss self: 0.0000; time: 0.12s
Val loss: 0.2756 score: 0.9184 time: 0.07s
Test loss: 0.0410 score: 1.0000 time: 0.07s
     INFO: Early stopping counter 13 of 20
Epoch 111/1000, LR 0.000263
Train loss: 0.0057;  Loss pred: 0.0057; Loss self: 0.0000; time: 0.12s
Val loss: 0.2779 score: 0.9184 time: 0.07s
Test loss: 0.0395 score: 1.0000 time: 0.07s
     INFO: Early stopping counter 14 of 20
Epoch 112/1000, LR 0.000263
Train loss: 0.0060;  Loss pred: 0.0060; Loss self: 0.0000; time: 0.12s
Val loss: 0.2801 score: 0.9184 time: 0.07s
Test loss: 0.0381 score: 1.0000 time: 0.07s
     INFO: Early stopping counter 15 of 20
Epoch 113/1000, LR 0.000263
Train loss: 0.0063;  Loss pred: 0.0063; Loss self: 0.0000; time: 0.12s
Val loss: 0.2823 score: 0.9184 time: 0.07s
Test loss: 0.0368 score: 1.0000 time: 0.07s
     INFO: Early stopping counter 16 of 20
Epoch 114/1000, LR 0.000263
Train loss: 0.0061;  Loss pred: 0.0061; Loss self: 0.0000; time: 0.12s
Val loss: 0.2844 score: 0.9184 time: 0.07s
Test loss: 0.0357 score: 1.0000 time: 0.07s
     INFO: Early stopping counter 17 of 20
Epoch 115/1000, LR 0.000263
Train loss: 0.0045;  Loss pred: 0.0045; Loss self: 0.0000; time: 0.12s
Val loss: 0.2862 score: 0.9184 time: 0.07s
Test loss: 0.0346 score: 1.0000 time: 0.07s
     INFO: Early stopping counter 18 of 20
Epoch 116/1000, LR 0.000263
Train loss: 0.0050;  Loss pred: 0.0050; Loss self: 0.0000; time: 0.12s
Val loss: 0.2880 score: 0.9184 time: 0.07s
Test loss: 0.0336 score: 1.0000 time: 0.08s
     INFO: Early stopping counter 19 of 20
Epoch 117/1000, LR 0.000262
Train loss: 0.0063;  Loss pred: 0.0063; Loss self: 0.0000; time: 0.12s
Val loss: 0.2897 score: 0.9184 time: 0.07s
Test loss: 0.0326 score: 1.0000 time: 0.08s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 096,   Train_Loss: 0.0119,   Val_Loss: 0.2567,   Val_Precision: 0.9167,   Val_Recall: 0.8800,   Val_accuracy: 0.8980,   Val_Score: 0.8980,   Val_Loss: 0.2567,   Test_Precision: 1.0000,   Test_Recall: 1.0000,   Test_accuracy: 1.0000,   Test_Score: 1.0000,   Test_loss: 0.0811


[0.07801941898651421, 0.07876479800324887, 0.0792875699698925, 0.07997004501521587, 0.07804729999043047, 0.07773569202981889, 0.07801035698503256, 0.0789991159690544, 0.0811175829730928, 0.07884221896529198, 0.07714625902008265, 0.08093058597296476, 0.08169240492861718, 0.08145885495468974, 0.08275256596971303, 0.0844363080104813, 0.08344505704008043, 0.08322757505811751, 0.0837428520899266, 0.08364762598648667, 0.08518351393286139, 0.0836860800627619, 0.08406434499192983, 0.08365772699471563, 0.0836440889397636, 0.08409553894307464, 0.08444887702353299, 0.08416446100454777, 0.08483666297979653, 0.0842295119073242, 0.08430745895020664, 0.08427638793364167, 0.08481852803379297, 0.08476802497170866, 0.08436364599037915, 0.08456845791079104, 0.0850465000839904, 0.08445937000215054, 0.08492995193228126, 0.08476895303465426, 0.08531851891893893, 0.08514710399322212, 0.08423514803871512, 0.08422081090975553, 0.0847218680428341, 0.08419094700366259, 0.08500061999075115, 0.08462696603965014, 0.08487939403858036, 0.08481463592033833, 0.08448044001124799, 0.08464586199261248, 0.08449819392990321, 0.08456077193841338, 0.08865917392540723, 0.08662363304756582, 0.0892601840896532, 0.08436056808568537, 0.0912496040109545, 0.09211663692258298, 0.09026035503484309, 0.08474923600442708, 0.08428427705075592, 0.08486541791353375, 0.08434419997502118, 0.08437740209046751, 0.0842743490356952, 0.0841362290084362, 0.08436939492821693, 0.0843358199344948, 0.08455520903225988, 0.08435399003792554, 0.0842872029170394, 0.08414290600921959, 0.08449918800033629, 0.08411446993704885, 0.08443783002439886, 0.08439066202845424, 0.08392996795009822, 0.08477803005371243, 0.08435705804731697, 0.08462367998436093, 0.08433452702593058, 0.08434388507157564, 0.08459920308087021, 0.08431766601279378, 0.08411748998332769, 0.08408475597389042, 0.07551496592350304, 0.07655417500063777, 0.07741969695780426, 0.07682091393508017, 0.0768038589740172, 0.07697760791052133, 0.07694949500728399, 0.0770650370977819, 0.07673422596417367, 0.07672605710104108, 0.08996502298396081, 0.0909528749762103, 0.07942402304615825, 0.07880136894527823, 0.08673327194992453, 0.07782763498835266, 0.22846841800492257, 0.07355149404611439, 0.07465934799984097, 0.07441282295621932, 0.07515920698642731, 0.07727231294848025, 0.07549014000687748, 0.07560773193836212, 0.20366581808775663, 0.07533438899554312, 0.07514048099983484, 0.07509200705680996, 0.0753509639762342, 0.07677237410098314, 0.07497388799674809, 0.08799926203209907, 0.08029275294393301, 0.08045941591262817, 0.08081883809063584, 0.08141504600644112, 0.08231477299705148, 0.0812631999142468, 0.12467141903471202, 0.07695177395362407, 0.0762657430022955, 0.0761341960169375, 0.07676301302853972, 0.07934123591985554, 0.07765807199757546, 0.07912473997566849, 0.08486885402817279, 0.08405006397515535, 0.08448795403819531, 0.0847657909616828, 0.08470635500270873, 0.08394304802641273, 0.08567331300582737, 0.08231392700690776, 0.08241687493864447, 0.08297595207113773, 0.08381504891440272, 0.08069976698607206, 0.08043371606618166, 0.0808918010443449, 0.08157633990049362, 0.08101981098297983, 0.08177325304131955, 0.0827539679594338, 0.08125166001264006, 0.12302837299648672, 0.07663839706219733, 0.07720412407070398, 0.07966816099360585, 0.07563217706046999, 0.0761972600594163, 0.07925507996696979, 0.09092552599031478, 0.07780151604674757, 0.07496272900607437, 0.07477613899391145, 0.07661219593137503, 0.07717890199273825, 0.07540250092279166, 0.07813696505036205, 0.07502132700756192, 0.07467535592149943, 0.07495276804547757, 0.07541854900773615, 0.08227258501574397, 0.076304174028337, 0.07798249204643071, 0.07583905104547739, 0.07563114596996456, 0.07674461009446532, 0.07676534098573029, 0.08396009600255638, 0.08790779509581625, 0.0845335409976542, 0.08474978990852833, 0.0893391550052911, 0.07838173001073301, 0.0793283119564876, 0.07891488308086991, 0.07872293307445943, 0.07846235705073923, 0.07931496202945709, 0.0827414580853656, 0.07896149402949959, 0.0804694639518857, 0.08333390695042908, 0.08425783296115696, 0.0841124519938603, 0.0903596410062164, 0.08411801292095333, 0.08474186505191028, 0.08395729400217533, 0.08373286400455981, 0.08303212199825794, 0.08369862800464034, 0.08454098307993263, 0.08309822995215654, 0.08367035805713385, 0.08280770992860198, 0.08327588299289346, 0.08283119997940958, 0.08351805503480136, 0.08530838100705296, 0.08344159799162298, 0.0840265080332756, 0.08181396604049951, 0.08218845108058304, 0.08176995906978846, 0.0826227740617469, 0.08343421202152967, 0.08336823794525117, 0.083636544062756, 0.08365220297127962, 0.07712895702570677, 0.07813909102696925, 0.0780814909376204, 0.07789134699851274, 0.07887230499181896, 0.07848073204513639, 0.07849251700099558, 0.07894597994163632, 0.07909049105364829, 0.07830600894521922, 0.07852199696935713, 0.07887522992677987, 0.07865155988838524, 0.07907225901726633, 0.07864222105126828, 0.07884307904168963, 0.07869765209034085, 0.07878692005760968, 0.07850201893597841, 0.07901775697246194, 0.07922055304516107, 0.07839239109307528, 0.07873693690635264, 0.0791115090250969, 0.07822932302951813, 0.07868247607257217, 0.0790631890995428, 0.07877244695555419, 0.0788738209521398, 0.07905667903833091, 0.07884599699173123, 0.07927952602040023, 0.0794161829398945, 0.07935342902783304, 0.07996232295408845, 0.07937067304737866, 0.07942029600962996, 0.07938722299877554, 0.07921013201121241, 0.07963982806541026, 0.07967446895781904, 0.07962270907592028, 0.07970187591854483, 0.0796090840594843, 0.07892815198283643, 0.07944741693791002, 0.07994274504017085, 0.07951062498614192, 0.07923309202305973, 0.07950102107133716, 0.07960100006312132, 0.0792611779179424, 0.07946933491621166, 0.07925508101470768, 0.07925260195042938, 0.07958988496102393, 0.07893906999379396, 0.08007856889162213, 0.07859265198931098, 0.07860578002873808, 0.0789862610399723, 0.07921145600266755, 0.07901623204816133, 0.07996221899520606, 0.0796833880012855, 0.07910997502040118, 0.07921559491660446, 0.07952169689815491, 0.07916975393891335, 0.07886246603447944, 0.07938702893443406, 0.07972815097309649, 0.07974343898240477, 0.08026333909947425, 0.07999981404282153]
[0.0015922330405411063, 0.0016074448572091606, 0.001618113672854949, 0.0016320417350044055, 0.0015928020406210301, 0.0015864426944861, 0.0015920481017353584, 0.0016122268565113144, 0.0016554608770018937, 0.0016090248768426934, 0.0015744134493894419, 0.0016516446116931584, 0.001667191937318718, 0.0016624256113201988, 0.001688827876932919, 0.0017231899593975774, 0.0017029603477567434, 0.0016985219399615818, 0.0017090377977536041, 0.001707094407887483, 0.001738439059854314, 0.0017078791849543245, 0.0017155988773863231, 0.001707300550912564, 0.0017070222232604818, 0.0017162354886341765, 0.0017234464698680201, 0.0017176420613173016, 0.0017313604689754394, 0.0017189696307617183, 0.001720560386738911, 0.001719926284360034, 0.0017309903680365912, 0.0017299596933001767, 0.001721707061028146, 0.0017258868961385926, 0.0017356428588569468, 0.0017236606122887864, 0.001733264325148597, 0.001729978633360291, 0.001741194263651815, 0.001737695999861676, 0.001719084653851329, 0.0017187920593827659, 0.0017290177151598797, 0.0017181825919114814, 0.0017347065304234928, 0.0017270809395846967, 0.0017322325313995992, 0.0017309109371497619, 0.001724090612474449, 0.0017274665712778056, 0.0017244529373449634, 0.0017257300395594568, 0.0018093708964368822, 0.0017678292458686903, 0.0018216364099929224, 0.0017216442466466402, 0.0018622368165500918, 0.0018799313657669996, 0.0018420480619355732, 0.0017295762449883077, 0.0017200872867501207, 0.0017319473043578316, 0.001721310203571861, 0.001721987797764643, 0.001719884674197861, 0.001717065898131351, 0.0017218243862901414, 0.0017211391823366284, 0.0017256165108624467, 0.0017215100007739906, 0.0017201469983069264, 0.001717202163453461, 0.001724473224496659, 0.0017166218354499765, 0.0017232210209060992, 0.0017222584087439642, 0.0017128564887775146, 0.0017301638786471924, 0.0017215726132105505, 0.0017270138772318558, 0.001721112796447563, 0.0017213037769709314, 0.001726514348589188, 0.0017207686941386486, 0.0017166834690475038, 0.00171601542803858, 0.0015411217535408785, 0.001562330102053832, 0.0015799938154653932, 0.0015677737537771463, 0.0015674256933472898, 0.0015709715900106393, 0.00157039785729151, 0.0015727558591384062, 0.0015660046115137485, 0.0015658379000212466, 0.00183602087722369, 0.0018561811219634755, 0.0016208984295134337, 0.0016081912029648618, 0.0017700667744882557, 0.0015883190813949521, 0.004662620775610664, 0.0015010508989002935, 0.0015236601632620608, 0.001518629039922843, 0.001533861367069945, 0.0015769859785404131, 0.0015406151021811732, 0.0015430149375175943, 0.004156445267097074, 0.001537436510113125, 0.001533479204078262, 0.0015324899399348972, 0.0015377747750251877, 0.0015667831449180233, 0.00153007934687241, 0.001795903306777532, 0.0016386276111006737, 0.0016420288961760852, 0.0016493640426660375, 0.0016615315511518596, 0.0016798933264704384, 0.0016584326513111591, 0.0025443146741777963, 0.0015704443664004912, 0.0015564437347407244, 0.0015537591023864795, 0.0015665921026232596, 0.0016192088963235824, 0.0015848586121954176, 0.0016147906117483365, 0.0017320174291463836, 0.001715307428064395, 0.0017242439599631696, 0.0017299141012588326, 0.0017287011225042598, 0.0017131234291104637, 0.0017484349593025993, 0.0016798760613654644, 0.0016819770395641728, 0.0016933867769619944, 0.0017105112023347495, 0.0016469340201239197, 0.0016415044095139115, 0.001650853082537651, 0.00166482326327538, 0.0016534655302648945, 0.0016688418988024397, 0.0016888564889680367, 0.0016581971431151032, 0.00251078312237728, 0.0015640489196366801, 0.0015755943687898771, 0.001625880836604201, 0.001543513817560612, 0.001555046123661557, 0.001617450611570812, 0.0018556229793941792, 0.0015877860417703585, 0.0015298516123688646, 0.0015260436529369683, 0.001563514202681123, 0.0015750796325048621, 0.0015388265494447276, 0.0015946319398033072, 0.0015310474899502434, 0.0015239868555408046, 0.001529648327458726, 0.0015391540613823704, 0.001679032347260081, 0.0015572280413946326, 0.001591479429518994, 0.0015477357356219875, 0.001543492774897236, 0.0015662165325401084, 0.0015666396119536794, 0.0017134713469909467, 0.001794036634608495, 0.0017251743060745755, 0.0017295875491536393, 0.0018232480613324716, 0.0015996271430761839, 0.0016189451419691347, 0.001610507817976937, 0.0016065904709073354, 0.0016012725928722291, 0.001618672694478716, 0.0016886011854156243, 0.0016114590618265222, 0.001642233958201749, 0.0017006919785801853, 0.001719547611452183, 0.0017165806529359246, 0.0018440743062493143, 0.0017166941412439455, 0.001729425817385924, 0.0017134141633097007, 0.0017088339592767308, 0.001694533102005264, 0.0017081352654008232, 0.0017253261853047476, 0.001695882243921562, 0.0017075583276966093, 0.00168995326384902, 0.0016995078161814992, 0.0016904326526410117, 0.0017044501027510483, 0.0017409873674908768, 0.0017028897549310814, 0.001714826694556645, 0.0016696727763367246, 0.001677315328175164, 0.0016687746748936419, 0.0016861790624846304, 0.0017027390208475444, 0.001701392611127575, 0.001706868246178694, 0.0017071878157404003, 0.0015740603474634035, 0.0015946753270810051, 0.0015934998150534776, 0.00158961932650026, 0.0016096388773840604, 0.0016016475927578856, 0.0016018881020611342, 0.0016111424477884965, 0.0016140916541560876, 0.0015980818152085555, 0.001602489734068513, 0.001609698569934283, 0.001605133875273168, 0.0016137195717809455, 0.001604943286760577, 0.0016090424294222373, 0.0016060745324559358, 0.00160789632770632, 0.0016020820191016002, 0.0016126072851522844, 0.001616745980513491, 0.0015998447161852097, 0.0016068762633949518, 0.0016145205923489162, 0.001596516796520778, 0.0016057648178075953, 0.0016135344714192407, 0.001607600958276616, 0.0016096698153497918, 0.0016134016130271616, 0.0016091019794230862, 0.0016179495106204127, 0.0016207384273447857, 0.0016194577352618988, 0.0016318841419201723, 0.001619809654028136, 0.0016208223675434686, 0.0016201474081382764, 0.0016165333063512736, 0.0016253026135798012, 0.0016260095705677355, 0.0016249532464473527, 0.0016265688962968333, 0.0016246751848874347, 0.001610778611894621, 0.0016213758558757147, 0.0016314845926565479, 0.0016226658160437126, 0.001617001878021627, 0.0016224698177823911, 0.0016245102053698227, 0.001617575059549845, 0.0016218231615553402, 0.0016174506329532181, 0.0016174000398046812, 0.0016242833665515088, 0.0016110014284447748, 0.0016342565079922884, 0.0016039316732512445, 0.001604199592423226, 0.001611964511019843, 0.001616560326585052, 0.0016125761642481905, 0.0016318820203103277, 0.0016261915918629691, 0.0016144892861306363, 0.0016166447942164174, 0.001622891773431733, 0.001615709264059456, 0.001609438082336315, 0.0016201434476415114, 0.0016271051218999283, 0.0016274171220898932, 0.001638027328560699, 0.001632649266180031]
[628.0487683261232, 622.1053216943293, 618.0035536289804, 612.7294287589408, 627.8244091212377, 630.341079117221, 628.1217250345537, 620.2601054319939, 604.0613909348557, 621.4944308146736, 635.1571757646001, 605.4571261397845, 599.8109621428846, 601.5306749309883, 592.1266540294802, 580.3190730925552, 587.2127330018393, 588.7471786338059, 585.1245661824574, 585.7906835026732, 575.2286767439537, 585.5215104262448, 582.8868351344913, 585.7199539152571, 585.8154547572084, 582.6706221975547, 580.2327008604909, 582.1934747179372, 577.5804738061082, 581.743843581969, 581.2059882974316, 581.4202673064498, 577.7039655825866, 578.0481498342536, 580.81889923994, 579.4122443581597, 576.1553967724536, 580.1606144913507, 576.9460465380937, 578.0418212782266, 574.3184553702212, 575.47465153836, 581.7049193939711, 581.8039445441174, 578.3630735718237, 582.0103199203631, 576.4663834843988, 579.0116589674514, 577.2897009341037, 577.7304762119473, 580.0159184004722, 578.8824030674589, 579.8940512343815, 579.4649088076832, 552.678282804956, 565.6654919228989, 548.9569677649808, 580.8400904819714, 536.9886316889392, 531.9343132465938, 542.8740002306062, 578.1763035296315, 581.3658456190144, 577.3847723217989, 580.9528102052248, 580.724207975298, 581.4343339424146, 582.3888303228666, 580.7793221901155, 581.0105366623484, 579.5030319339085, 580.8853852434202, 581.3456646346277, 582.3426159613628, 579.8872292098829, 582.5394850216798, 580.3086126898469, 580.632961304161, 583.8200728151554, 577.9799314628483, 580.8642588331526, 579.0341427961482, 581.0194439690617, 580.9549792307738, 579.20167348574, 581.1356304924888, 582.5185702725073, 582.745343462913, 648.877999225176, 640.0695977664417, 632.9138697960323, 637.8471368019511, 637.9887762745975, 636.5487487862384, 636.7813069515491, 635.8265932945399, 638.567723650168, 638.635710622684, 544.6561160634155, 538.7405292335891, 616.941803256718, 621.8166087194107, 564.9504382619182, 629.5964153007235, 214.47165620477247, 666.1999274858863, 656.3143305256881, 658.4886589886407, 651.9494013401273, 634.121047116446, 649.0913912139504, 648.0818660179675, 240.59020045713618, 650.433363213431, 652.1118756227779, 652.5328316624915, 650.2902871349419, 638.2504198130888, 653.5608771166479, 556.8228513339863, 610.2667825353532, 609.0026809691196, 606.2942892726076, 601.854354981552, 595.2758929646224, 602.9789628233614, 393.03314568319, 636.7624485113294, 642.4902986721727, 643.6004130009992, 638.3282529801468, 617.5855396240117, 630.971111432303, 619.2753368297691, 577.3613955448735, 582.9858739249036, 579.96433406173, 578.0633843451042, 578.4689944270779, 583.729101480591, 571.9400625567858, 595.2820109759547, 594.5384368975191, 590.5325431878246, 584.6205500642483, 607.18886596608, 609.1972669729981, 605.7474226978602, 600.6643600309836, 604.7903519584072, 599.2179371320912, 592.1166224200865, 603.0646019093915, 398.2821101064165, 639.3661908172887, 634.6811208572941, 615.051224841662, 647.8723990825117, 643.0677423544009, 618.2568993737834, 538.9025740166671, 629.8077786885029, 653.6581665273877, 655.2892494755548, 639.5848520500768, 634.888534752806, 649.845819440041, 627.1039573704682, 653.1476042147448, 656.1736384826877, 653.7450354104236, 649.707540713542, 595.5811403108726, 642.1667048227653, 628.3461673785116, 646.1051308595202, 647.8812316219484, 638.4813205733361, 638.3088952748674, 583.6105761302139, 557.4022183879348, 579.6515728751946, 578.1725247093403, 548.4717198981564, 625.1456811847647, 617.6861550624827, 620.9221643246443, 622.4361578811317, 624.5032884789987, 617.7901211350476, 592.206145913527, 620.5556341385063, 608.9266361870893, 587.9959525856324, 581.5483056938943, 582.5534607358337, 542.2774975016665, 582.5149489211766, 578.2265940215511, 583.6300536166686, 585.1943628409942, 590.1330571923484, 585.4337301357364, 579.6005465617901, 589.6635828249475, 585.6315323347919, 591.7323404094681, 588.4056492583996, 591.5645313864892, 586.6994864713032, 574.3867064591095, 587.2370757439148, 583.1493078421795, 598.9197489306905, 596.1908194614489, 599.2420756647294, 593.0568243010164, 587.2890605996957, 587.7538161737187, 585.8683013400607, 585.7586322839962, 635.2996577364387, 627.0868953810575, 627.5494923521156, 629.0814305847812, 621.2573603000766, 624.3570711320426, 624.2633294506086, 620.6775827752728, 619.54350449996, 625.7501903114368, 624.0289586511922, 621.2343221755025, 623.0009941256869, 619.686355353782, 623.0749760749511, 621.4876511112714, 622.6361104617266, 621.9306448858615, 624.1877682147448, 620.1137804642661, 618.5263560589723, 625.0606636277022, 622.3254539134426, 619.3789071126871, 626.3635949081513, 622.7562024713766, 619.7574441160931, 622.0449141010853, 621.245419690431, 619.808479132322, 621.4646509592459, 618.0662582088509, 617.0027088444336, 617.4906440755491, 612.7886008030816, 617.35648846962, 616.9707551084749, 617.2277874080036, 618.6077305497221, 615.2700374962516, 615.0025301824277, 615.4023213814351, 614.7910502141494, 615.5076468834505, 620.8177788155417, 616.7601400847888, 612.9386722382092, 616.2698382579727, 618.4284716004672, 616.3442851385738, 615.5701556656876, 618.209333839686, 616.590035032545, 618.2568912005386, 618.2762306106787, 615.6561229356703, 620.7319139160404, 611.8990471260335, 623.4679548243805, 623.3638287424375, 620.3610520974368, 618.5973907404235, 620.1257479619368, 612.7893974895529, 614.9336923175192, 619.3909173573079, 618.5650698146693, 616.1840341857306, 618.9232321955674, 621.3348689676623, 617.2292962427051, 614.588440869958, 614.4706150785867, 610.4904250154846, 612.5014237379571]
Elapsed: 0.0823023221483045~0.012160728186999552
Time per graph: 0.0016796392275164186~0.000248178126265297
Speed: 601.3963353319874~44.1477051952619
Total Time: 0.0810
best val loss: 0.2566855847835541 test_score: 1.0000

Testing...
Test loss: 0.4951 score: 0.9796 time: 0.07s
test Score 0.9796
Epoch Time List: [0.2533633179264143, 0.25523712602443993, 0.26129988906905055, 0.26074120204430073, 0.3130132920341566, 0.2535937699722126, 0.2532034239266068, 0.2544539209920913, 0.2583321820711717, 0.26281752600334585, 0.25422287394758314, 0.3531752530252561, 0.2634344029938802, 0.26432531292084605, 0.26687420601956546, 0.2699742551194504, 0.27065658592619, 0.27145074389409274, 0.27259264304302633, 0.2720676128519699, 0.2784268979448825, 0.2739965708460659, 0.27244734996929765, 0.27218683902174234, 0.2729487620526925, 0.2739525850629434, 0.2734550080494955, 0.2740610180189833, 0.2742085180943832, 0.2738697270397097, 0.2744393680477515, 0.2732720379717648, 0.27533826883882284, 0.27456349006388336, 0.2739639839855954, 0.2739821680588648, 0.27602097601629794, 0.27511241601314396, 0.2749424851499498, 0.2762853589374572, 0.2760853199288249, 0.27596198697574437, 0.27508058201055974, 0.2745959711028263, 0.2746261080028489, 0.2744255799334496, 0.2752882060594857, 0.2762459581717849, 0.2755902099888772, 0.27550380607135594, 0.27614147611893713, 0.27519897499587387, 0.2752912261057645, 0.274683310999535, 0.2919426260050386, 0.2905679289251566, 0.2884237340185791, 0.27751078794244677, 0.2840493740513921, 0.2845868718577549, 0.2880165280075744, 0.27850325300823897, 0.27432679291814566, 0.27452952007297426, 0.27518069406505674, 0.274304919061251, 0.2739726809086278, 0.2753040270181373, 0.27545380184892565, 0.2750368268461898, 0.2746617979137227, 0.27501191198825836, 0.2739022789755836, 0.2740118959918618, 0.27437075006309897, 0.27350182295776904, 0.2742271829629317, 0.27366328006610274, 0.2736710680183023, 0.2749212079215795, 0.2741992159280926, 0.27417869213968515, 0.2746027240063995, 0.27421689219772816, 0.27472517208661884, 0.2745129718678072, 0.27457469410728663, 0.27247924904804677, 0.2553659479599446, 0.24471239582635462, 0.2540698539232835, 0.25235631701070815, 0.250837322906591, 0.2502095610834658, 0.2524581920588389, 0.2521049020579085, 0.2507076249457896, 0.2514217549469322, 0.2652400430524722, 0.2910372900078073, 0.2653393360087648, 0.25863847497384995, 0.2705163008067757, 0.26604870497249067, 0.40086335001979023, 0.2740782320033759, 0.2424132350133732, 0.24386269506067038, 0.2456055210204795, 0.2523443379905075, 0.25241483200807124, 0.24689164804294705, 0.37482072599232197, 0.24601255706511438, 0.24393544101621956, 0.2449627669993788, 0.24492574797477573, 0.2505801870720461, 0.24906050297431648, 0.2560853749746457, 0.3367396420799196, 0.26060391683131456, 0.2623578259954229, 0.2659405739977956, 0.2693553731078282, 0.2660223300335929, 0.30324602185282856, 0.27554754295852035, 0.24948814895469695, 0.24877116410061717, 0.24947944295126945, 0.2567473850212991, 0.2574212980689481, 0.2558888590428978, 0.369314456009306, 0.27027786895632744, 0.27141569508239627, 0.2737348999362439, 0.2760472389636561, 0.2725921889068559, 0.2693867600755766, 0.38798508199397475, 0.2684834299143404, 0.2683445338625461, 0.2714744119439274, 0.26594715483952314, 0.2604426150210202, 0.36730664991773665, 0.2648060499923304, 0.26343250495847315, 0.26581590401474386, 0.27098937006667256, 0.26782627613283694, 0.3042041639564559, 0.2633575400104746, 0.25693897204473615, 0.26996849407441914, 0.2530972521053627, 0.26057517202571034, 0.25471366092097014, 0.2783042420633137, 0.3457916658371687, 0.2453843040857464, 0.24575288011692464, 0.24972877418622375, 0.2550875339657068, 0.2517511359183118, 0.24981783714611083, 0.3541054190136492, 0.24490787996910512, 0.2447138640563935, 0.24628784705419093, 0.25712918920908123, 0.2531092829303816, 0.25211683998350054, 0.37304728606250137, 0.2512670719297603, 0.25146548193879426, 0.25644653697963804, 0.40245565806981176, 0.27777256595436484, 0.276489723008126, 0.2796953141223639, 0.2823282000608742, 0.26624212600290775, 0.262658377061598, 0.30511817999649793, 0.26070356788113713, 0.25957097904756665, 0.26249052211642265, 0.26947486703284085, 0.268695673905313, 0.2651385428616777, 0.40229404298588634, 0.2776308039901778, 0.275746684987098, 0.28134668210987, 0.2817262300522998, 0.2764266439480707, 0.3733475070912391, 0.27564912696834654, 0.27250772004481405, 0.27355330204591155, 0.27735739294439554, 0.27549874200485647, 0.2722647249465808, 0.30829350906424224, 0.2714091259986162, 0.27192109706811607, 0.2731122502591461, 0.2773015989223495, 0.2775354728801176, 0.27329083206132054, 0.4377159499563277, 0.26967914099805057, 0.2701415040064603, 0.2731400449993089, 0.27783556398935616, 0.27656780986581, 0.27613881416618824, 0.276896839030087, 0.27048101800028235, 0.259494244819507, 0.2599889950361103, 0.2596941269002855, 0.2614677289966494, 0.2607659350614995, 0.26129814295563847, 0.2630451020086184, 0.2632685829885304, 0.2609434899641201, 0.2618703090120107, 0.26275259198155254, 0.2620370660442859, 0.2630243750754744, 0.2618304869392887, 0.26281037798617035, 0.26284039893653244, 0.26297758205328137, 0.26271117804571986, 0.2635667601134628, 0.2640060220146552, 0.26211909810081124, 0.2623854901175946, 0.26229330303613096, 0.2607792018679902, 0.2630462859524414, 0.2630237099947408, 0.26336234307382256, 0.2629885938949883, 0.26320027408655733, 0.2624362020287663, 0.26335077197290957, 0.26323687098920345, 0.26311350998003036, 0.2650446891784668, 0.2645211289636791, 0.2642756149871275, 0.26458317099604756, 0.26539823703933507, 0.26626135711558163, 0.264345690025948, 0.26496948790736496, 0.26495749794412404, 0.2648210789775476, 0.2638228030409664, 0.26572270004544407, 0.2645027458202094, 0.2641341519774869, 0.26460808294359595, 0.26515296404249966, 0.2651341331657022, 0.26391656801570207, 0.2655705668730661, 0.26572301995474845, 0.26521291700191796, 0.2639635318191722, 0.26567452191375196, 0.2653548950329423, 0.2647440171567723, 0.26247683411929756, 0.2637744549429044, 0.26467955706175417, 0.2644629339920357, 0.26527828606776893, 0.2650955179706216, 0.26486295694485307, 0.2628489030757919, 0.26447852910496294, 0.26577018504031, 0.2655347768450156, 0.2653968700906262, 0.2668423771392554, 0.26583185384515673, 0.2655799000058323, 0.2651821420295164]
Total Epoch List: [179, 117]
Total Time List: [0.07753726898226887, 0.08098952600266784]
========================k_idx:2========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: I2BGNNA
I2BGNN(
  (gcs): ModuleList(
    (0): GCNConv(14887, 64)
    (1): GCNConv(64, 64)
  )
  (bns): ModuleList(
    (0-1): 2 x BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  )
  (drops): ModuleList(
    (0-1): 2 x Dropout(p=0.2, inplace=False)
  )
  (lin1): Linear(in_features=64, out_features=64, bias=True)
  (lin2): Linear(in_features=64, out_features=64, bias=True)
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=64, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=64, out_features=2, bias=True)
  )
)
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7d1ecd6603a0>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.7011;  Loss pred: 0.7011; Loss self: 0.0000; time: 0.11s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7043 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7016 score: 0.5000 time: 0.07s
Epoch 2/1000, LR 0.000000
Train loss: 0.7033;  Loss pred: 0.7033; Loss self: 0.0000; time: 0.11s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7042 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7016 score: 0.5000 time: 0.07s
Epoch 3/1000, LR 0.000030
Train loss: 0.7026;  Loss pred: 0.7026; Loss self: 0.0000; time: 0.11s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7042 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7015 score: 0.5000 time: 0.07s
Epoch 4/1000, LR 0.000060
Train loss: 0.7007;  Loss pred: 0.7007; Loss self: 0.0000; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7042 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7016 score: 0.5000 time: 0.07s
     INFO: Early stopping counter 1 of 20
Epoch 5/1000, LR 0.000090
Train loss: 0.6998;  Loss pred: 0.6998; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7042 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7016 score: 0.5000 time: 0.07s
     INFO: Early stopping counter 2 of 20
Epoch 6/1000, LR 0.000120
Train loss: 0.6970;  Loss pred: 0.6970; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7043 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7016 score: 0.5000 time: 0.07s
     INFO: Early stopping counter 3 of 20
Epoch 7/1000, LR 0.000150
Train loss: 0.6936;  Loss pred: 0.6936; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7043 score: 0.4898 time: 0.09s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7017 score: 0.5000 time: 0.07s
     INFO: Early stopping counter 4 of 20
Epoch 8/1000, LR 0.000180
Train loss: 0.6920;  Loss pred: 0.6920; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7044 score: 0.4898 time: 0.09s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7017 score: 0.5000 time: 0.08s
     INFO: Early stopping counter 5 of 20
Epoch 9/1000, LR 0.000210
Train loss: 0.6887;  Loss pred: 0.6887; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7045 score: 0.4898 time: 0.09s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7018 score: 0.5000 time: 0.07s
     INFO: Early stopping counter 6 of 20
Epoch 10/1000, LR 0.000240
Train loss: 0.6864;  Loss pred: 0.6864; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7045 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7018 score: 0.5000 time: 0.08s
     INFO: Early stopping counter 7 of 20
Epoch 11/1000, LR 0.000270
Train loss: 0.6824;  Loss pred: 0.6824; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7046 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7019 score: 0.5000 time: 0.07s
     INFO: Early stopping counter 8 of 20
Epoch 12/1000, LR 0.000270
Train loss: 0.6777;  Loss pred: 0.6777; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7047 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7020 score: 0.5000 time: 0.07s
     INFO: Early stopping counter 9 of 20
Epoch 13/1000, LR 0.000270
Train loss: 0.6766;  Loss pred: 0.6766; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7049 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7022 score: 0.5000 time: 0.07s
     INFO: Early stopping counter 10 of 20
Epoch 14/1000, LR 0.000270
Train loss: 0.6709;  Loss pred: 0.6709; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7051 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7023 score: 0.5000 time: 0.07s
     INFO: Early stopping counter 11 of 20
Epoch 15/1000, LR 0.000270
Train loss: 0.6661;  Loss pred: 0.6661; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7053 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7025 score: 0.5000 time: 0.07s
     INFO: Early stopping counter 12 of 20
Epoch 16/1000, LR 0.000270
Train loss: 0.6612;  Loss pred: 0.6612; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7055 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7027 score: 0.5000 time: 0.07s
     INFO: Early stopping counter 13 of 20
Epoch 17/1000, LR 0.000270
Train loss: 0.6572;  Loss pred: 0.6572; Loss self: 0.0000; time: 0.11s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7058 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7029 score: 0.5000 time: 0.07s
     INFO: Early stopping counter 14 of 20
Epoch 18/1000, LR 0.000270
Train loss: 0.6545;  Loss pred: 0.6545; Loss self: 0.0000; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7061 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7032 score: 0.5000 time: 0.07s
     INFO: Early stopping counter 15 of 20
Epoch 19/1000, LR 0.000270
Train loss: 0.6474;  Loss pred: 0.6474; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7064 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7034 score: 0.5000 time: 0.07s
     INFO: Early stopping counter 16 of 20
Epoch 20/1000, LR 0.000270
Train loss: 0.6422;  Loss pred: 0.6422; Loss self: 0.0000; time: 0.11s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7068 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7038 score: 0.5000 time: 0.07s
     INFO: Early stopping counter 17 of 20
Epoch 21/1000, LR 0.000270
Train loss: 0.6387;  Loss pred: 0.6387; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7072 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7041 score: 0.5000 time: 0.07s
     INFO: Early stopping counter 18 of 20
Epoch 22/1000, LR 0.000270
Train loss: 0.6283;  Loss pred: 0.6283; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7076 score: 0.4898 time: 0.09s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7045 score: 0.5000 time: 0.07s
     INFO: Early stopping counter 19 of 20
Epoch 23/1000, LR 0.000270
Train loss: 0.6236;  Loss pred: 0.6236; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7081 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7048 score: 0.5000 time: 0.07s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 002,   Train_Loss: 0.7026,   Val_Loss: 0.7042,   Val_Precision: 0.0000,   Val_Recall: 0.0000,   Val_accuracy: 0.0000,   Val_Score: 0.4898,   Val_Loss: 0.7042,   Test_Precision: 0.0000,   Test_Recall: 0.0000,   Test_accuracy: 0.0000,   Test_Score: 0.5000,   Test_loss: 0.7015


[0.07801941898651421, 0.07876479800324887, 0.0792875699698925, 0.07997004501521587, 0.07804729999043047, 0.07773569202981889, 0.07801035698503256, 0.0789991159690544, 0.0811175829730928, 0.07884221896529198, 0.07714625902008265, 0.08093058597296476, 0.08169240492861718, 0.08145885495468974, 0.08275256596971303, 0.0844363080104813, 0.08344505704008043, 0.08322757505811751, 0.0837428520899266, 0.08364762598648667, 0.08518351393286139, 0.0836860800627619, 0.08406434499192983, 0.08365772699471563, 0.0836440889397636, 0.08409553894307464, 0.08444887702353299, 0.08416446100454777, 0.08483666297979653, 0.0842295119073242, 0.08430745895020664, 0.08427638793364167, 0.08481852803379297, 0.08476802497170866, 0.08436364599037915, 0.08456845791079104, 0.0850465000839904, 0.08445937000215054, 0.08492995193228126, 0.08476895303465426, 0.08531851891893893, 0.08514710399322212, 0.08423514803871512, 0.08422081090975553, 0.0847218680428341, 0.08419094700366259, 0.08500061999075115, 0.08462696603965014, 0.08487939403858036, 0.08481463592033833, 0.08448044001124799, 0.08464586199261248, 0.08449819392990321, 0.08456077193841338, 0.08865917392540723, 0.08662363304756582, 0.0892601840896532, 0.08436056808568537, 0.0912496040109545, 0.09211663692258298, 0.09026035503484309, 0.08474923600442708, 0.08428427705075592, 0.08486541791353375, 0.08434419997502118, 0.08437740209046751, 0.0842743490356952, 0.0841362290084362, 0.08436939492821693, 0.0843358199344948, 0.08455520903225988, 0.08435399003792554, 0.0842872029170394, 0.08414290600921959, 0.08449918800033629, 0.08411446993704885, 0.08443783002439886, 0.08439066202845424, 0.08392996795009822, 0.08477803005371243, 0.08435705804731697, 0.08462367998436093, 0.08433452702593058, 0.08434388507157564, 0.08459920308087021, 0.08431766601279378, 0.08411748998332769, 0.08408475597389042, 0.07551496592350304, 0.07655417500063777, 0.07741969695780426, 0.07682091393508017, 0.0768038589740172, 0.07697760791052133, 0.07694949500728399, 0.0770650370977819, 0.07673422596417367, 0.07672605710104108, 0.08996502298396081, 0.0909528749762103, 0.07942402304615825, 0.07880136894527823, 0.08673327194992453, 0.07782763498835266, 0.22846841800492257, 0.07355149404611439, 0.07465934799984097, 0.07441282295621932, 0.07515920698642731, 0.07727231294848025, 0.07549014000687748, 0.07560773193836212, 0.20366581808775663, 0.07533438899554312, 0.07514048099983484, 0.07509200705680996, 0.0753509639762342, 0.07677237410098314, 0.07497388799674809, 0.08799926203209907, 0.08029275294393301, 0.08045941591262817, 0.08081883809063584, 0.08141504600644112, 0.08231477299705148, 0.0812631999142468, 0.12467141903471202, 0.07695177395362407, 0.0762657430022955, 0.0761341960169375, 0.07676301302853972, 0.07934123591985554, 0.07765807199757546, 0.07912473997566849, 0.08486885402817279, 0.08405006397515535, 0.08448795403819531, 0.0847657909616828, 0.08470635500270873, 0.08394304802641273, 0.08567331300582737, 0.08231392700690776, 0.08241687493864447, 0.08297595207113773, 0.08381504891440272, 0.08069976698607206, 0.08043371606618166, 0.0808918010443449, 0.08157633990049362, 0.08101981098297983, 0.08177325304131955, 0.0827539679594338, 0.08125166001264006, 0.12302837299648672, 0.07663839706219733, 0.07720412407070398, 0.07966816099360585, 0.07563217706046999, 0.0761972600594163, 0.07925507996696979, 0.09092552599031478, 0.07780151604674757, 0.07496272900607437, 0.07477613899391145, 0.07661219593137503, 0.07717890199273825, 0.07540250092279166, 0.07813696505036205, 0.07502132700756192, 0.07467535592149943, 0.07495276804547757, 0.07541854900773615, 0.08227258501574397, 0.076304174028337, 0.07798249204643071, 0.07583905104547739, 0.07563114596996456, 0.07674461009446532, 0.07676534098573029, 0.08396009600255638, 0.08790779509581625, 0.0845335409976542, 0.08474978990852833, 0.0893391550052911, 0.07838173001073301, 0.0793283119564876, 0.07891488308086991, 0.07872293307445943, 0.07846235705073923, 0.07931496202945709, 0.0827414580853656, 0.07896149402949959, 0.0804694639518857, 0.08333390695042908, 0.08425783296115696, 0.0841124519938603, 0.0903596410062164, 0.08411801292095333, 0.08474186505191028, 0.08395729400217533, 0.08373286400455981, 0.08303212199825794, 0.08369862800464034, 0.08454098307993263, 0.08309822995215654, 0.08367035805713385, 0.08280770992860198, 0.08327588299289346, 0.08283119997940958, 0.08351805503480136, 0.08530838100705296, 0.08344159799162298, 0.0840265080332756, 0.08181396604049951, 0.08218845108058304, 0.08176995906978846, 0.0826227740617469, 0.08343421202152967, 0.08336823794525117, 0.083636544062756, 0.08365220297127962, 0.07712895702570677, 0.07813909102696925, 0.0780814909376204, 0.07789134699851274, 0.07887230499181896, 0.07848073204513639, 0.07849251700099558, 0.07894597994163632, 0.07909049105364829, 0.07830600894521922, 0.07852199696935713, 0.07887522992677987, 0.07865155988838524, 0.07907225901726633, 0.07864222105126828, 0.07884307904168963, 0.07869765209034085, 0.07878692005760968, 0.07850201893597841, 0.07901775697246194, 0.07922055304516107, 0.07839239109307528, 0.07873693690635264, 0.0791115090250969, 0.07822932302951813, 0.07868247607257217, 0.0790631890995428, 0.07877244695555419, 0.0788738209521398, 0.07905667903833091, 0.07884599699173123, 0.07927952602040023, 0.0794161829398945, 0.07935342902783304, 0.07996232295408845, 0.07937067304737866, 0.07942029600962996, 0.07938722299877554, 0.07921013201121241, 0.07963982806541026, 0.07967446895781904, 0.07962270907592028, 0.07970187591854483, 0.0796090840594843, 0.07892815198283643, 0.07944741693791002, 0.07994274504017085, 0.07951062498614192, 0.07923309202305973, 0.07950102107133716, 0.07960100006312132, 0.0792611779179424, 0.07946933491621166, 0.07925508101470768, 0.07925260195042938, 0.07958988496102393, 0.07893906999379396, 0.08007856889162213, 0.07859265198931098, 0.07860578002873808, 0.0789862610399723, 0.07921145600266755, 0.07901623204816133, 0.07996221899520606, 0.0796833880012855, 0.07910997502040118, 0.07921559491660446, 0.07952169689815491, 0.07916975393891335, 0.07886246603447944, 0.07938702893443406, 0.07972815097309649, 0.07974343898240477, 0.08026333909947425, 0.07999981404282153, 0.07416809001006186, 0.07223497796803713, 0.07447715103626251, 0.07869507500436157, 0.07862731302157044, 0.07866669492796063, 0.07891339901834726, 0.08412044798023999, 0.07886769203469157, 0.08518230298068374, 0.0769322180422023, 0.07705250394064933, 0.07625037396792322, 0.07634563196916133, 0.07931009295862168, 0.07568598305806518, 0.08144589606672525, 0.07737465191166848, 0.07757627591490746, 0.07786974392365664, 0.07846020895522088, 0.07736268802545965, 0.07963413302786648]
[0.0015922330405411063, 0.0016074448572091606, 0.001618113672854949, 0.0016320417350044055, 0.0015928020406210301, 0.0015864426944861, 0.0015920481017353584, 0.0016122268565113144, 0.0016554608770018937, 0.0016090248768426934, 0.0015744134493894419, 0.0016516446116931584, 0.001667191937318718, 0.0016624256113201988, 0.001688827876932919, 0.0017231899593975774, 0.0017029603477567434, 0.0016985219399615818, 0.0017090377977536041, 0.001707094407887483, 0.001738439059854314, 0.0017078791849543245, 0.0017155988773863231, 0.001707300550912564, 0.0017070222232604818, 0.0017162354886341765, 0.0017234464698680201, 0.0017176420613173016, 0.0017313604689754394, 0.0017189696307617183, 0.001720560386738911, 0.001719926284360034, 0.0017309903680365912, 0.0017299596933001767, 0.001721707061028146, 0.0017258868961385926, 0.0017356428588569468, 0.0017236606122887864, 0.001733264325148597, 0.001729978633360291, 0.001741194263651815, 0.001737695999861676, 0.001719084653851329, 0.0017187920593827659, 0.0017290177151598797, 0.0017181825919114814, 0.0017347065304234928, 0.0017270809395846967, 0.0017322325313995992, 0.0017309109371497619, 0.001724090612474449, 0.0017274665712778056, 0.0017244529373449634, 0.0017257300395594568, 0.0018093708964368822, 0.0017678292458686903, 0.0018216364099929224, 0.0017216442466466402, 0.0018622368165500918, 0.0018799313657669996, 0.0018420480619355732, 0.0017295762449883077, 0.0017200872867501207, 0.0017319473043578316, 0.001721310203571861, 0.001721987797764643, 0.001719884674197861, 0.001717065898131351, 0.0017218243862901414, 0.0017211391823366284, 0.0017256165108624467, 0.0017215100007739906, 0.0017201469983069264, 0.001717202163453461, 0.001724473224496659, 0.0017166218354499765, 0.0017232210209060992, 0.0017222584087439642, 0.0017128564887775146, 0.0017301638786471924, 0.0017215726132105505, 0.0017270138772318558, 0.001721112796447563, 0.0017213037769709314, 0.001726514348589188, 0.0017207686941386486, 0.0017166834690475038, 0.00171601542803858, 0.0015411217535408785, 0.001562330102053832, 0.0015799938154653932, 0.0015677737537771463, 0.0015674256933472898, 0.0015709715900106393, 0.00157039785729151, 0.0015727558591384062, 0.0015660046115137485, 0.0015658379000212466, 0.00183602087722369, 0.0018561811219634755, 0.0016208984295134337, 0.0016081912029648618, 0.0017700667744882557, 0.0015883190813949521, 0.004662620775610664, 0.0015010508989002935, 0.0015236601632620608, 0.001518629039922843, 0.001533861367069945, 0.0015769859785404131, 0.0015406151021811732, 0.0015430149375175943, 0.004156445267097074, 0.001537436510113125, 0.001533479204078262, 0.0015324899399348972, 0.0015377747750251877, 0.0015667831449180233, 0.00153007934687241, 0.001795903306777532, 0.0016386276111006737, 0.0016420288961760852, 0.0016493640426660375, 0.0016615315511518596, 0.0016798933264704384, 0.0016584326513111591, 0.0025443146741777963, 0.0015704443664004912, 0.0015564437347407244, 0.0015537591023864795, 0.0015665921026232596, 0.0016192088963235824, 0.0015848586121954176, 0.0016147906117483365, 0.0017320174291463836, 0.001715307428064395, 0.0017242439599631696, 0.0017299141012588326, 0.0017287011225042598, 0.0017131234291104637, 0.0017484349593025993, 0.0016798760613654644, 0.0016819770395641728, 0.0016933867769619944, 0.0017105112023347495, 0.0016469340201239197, 0.0016415044095139115, 0.001650853082537651, 0.00166482326327538, 0.0016534655302648945, 0.0016688418988024397, 0.0016888564889680367, 0.0016581971431151032, 0.00251078312237728, 0.0015640489196366801, 0.0015755943687898771, 0.001625880836604201, 0.001543513817560612, 0.001555046123661557, 0.001617450611570812, 0.0018556229793941792, 0.0015877860417703585, 0.0015298516123688646, 0.0015260436529369683, 0.001563514202681123, 0.0015750796325048621, 0.0015388265494447276, 0.0015946319398033072, 0.0015310474899502434, 0.0015239868555408046, 0.001529648327458726, 0.0015391540613823704, 0.001679032347260081, 0.0015572280413946326, 0.001591479429518994, 0.0015477357356219875, 0.001543492774897236, 0.0015662165325401084, 0.0015666396119536794, 0.0017134713469909467, 0.001794036634608495, 0.0017251743060745755, 0.0017295875491536393, 0.0018232480613324716, 0.0015996271430761839, 0.0016189451419691347, 0.001610507817976937, 0.0016065904709073354, 0.0016012725928722291, 0.001618672694478716, 0.0016886011854156243, 0.0016114590618265222, 0.001642233958201749, 0.0017006919785801853, 0.001719547611452183, 0.0017165806529359246, 0.0018440743062493143, 0.0017166941412439455, 0.001729425817385924, 0.0017134141633097007, 0.0017088339592767308, 0.001694533102005264, 0.0017081352654008232, 0.0017253261853047476, 0.001695882243921562, 0.0017075583276966093, 0.00168995326384902, 0.0016995078161814992, 0.0016904326526410117, 0.0017044501027510483, 0.0017409873674908768, 0.0017028897549310814, 0.001714826694556645, 0.0016696727763367246, 0.001677315328175164, 0.0016687746748936419, 0.0016861790624846304, 0.0017027390208475444, 0.001701392611127575, 0.001706868246178694, 0.0017071878157404003, 0.0015740603474634035, 0.0015946753270810051, 0.0015934998150534776, 0.00158961932650026, 0.0016096388773840604, 0.0016016475927578856, 0.0016018881020611342, 0.0016111424477884965, 0.0016140916541560876, 0.0015980818152085555, 0.001602489734068513, 0.001609698569934283, 0.001605133875273168, 0.0016137195717809455, 0.001604943286760577, 0.0016090424294222373, 0.0016060745324559358, 0.00160789632770632, 0.0016020820191016002, 0.0016126072851522844, 0.001616745980513491, 0.0015998447161852097, 0.0016068762633949518, 0.0016145205923489162, 0.001596516796520778, 0.0016057648178075953, 0.0016135344714192407, 0.001607600958276616, 0.0016096698153497918, 0.0016134016130271616, 0.0016091019794230862, 0.0016179495106204127, 0.0016207384273447857, 0.0016194577352618988, 0.0016318841419201723, 0.001619809654028136, 0.0016208223675434686, 0.0016201474081382764, 0.0016165333063512736, 0.0016253026135798012, 0.0016260095705677355, 0.0016249532464473527, 0.0016265688962968333, 0.0016246751848874347, 0.001610778611894621, 0.0016213758558757147, 0.0016314845926565479, 0.0016226658160437126, 0.001617001878021627, 0.0016224698177823911, 0.0016245102053698227, 0.001617575059549845, 0.0016218231615553402, 0.0016174506329532181, 0.0016174000398046812, 0.0016242833665515088, 0.0016110014284447748, 0.0016342565079922884, 0.0016039316732512445, 0.001604199592423226, 0.001611964511019843, 0.001616560326585052, 0.0016125761642481905, 0.0016318820203103277, 0.0016261915918629691, 0.0016144892861306363, 0.0016166447942164174, 0.001622891773431733, 0.001615709264059456, 0.001609438082336315, 0.0016201434476415114, 0.0016271051218999283, 0.0016274171220898932, 0.001638027328560699, 0.001632649266180031, 0.0015451685418762888, 0.001504895374334107, 0.001551607313255469, 0.0016394807292575326, 0.0016380690212827176, 0.0016388894776658465, 0.001644029146215568, 0.0017525093329216663, 0.0016430769173894078, 0.001774631312097578, 0.0016027545425458811, 0.0016052604987635277, 0.0015885494576650672, 0.0015905339993575278, 0.0016522936033046183, 0.0015767913137096912, 0.0016967895013901095, 0.0016119719148264267, 0.0016161724148939054, 0.0016222863317428466, 0.0016345876865671016, 0.001611722667197076, 0.0016590444380805518]
[628.0487683261232, 622.1053216943293, 618.0035536289804, 612.7294287589408, 627.8244091212377, 630.341079117221, 628.1217250345537, 620.2601054319939, 604.0613909348557, 621.4944308146736, 635.1571757646001, 605.4571261397845, 599.8109621428846, 601.5306749309883, 592.1266540294802, 580.3190730925552, 587.2127330018393, 588.7471786338059, 585.1245661824574, 585.7906835026732, 575.2286767439537, 585.5215104262448, 582.8868351344913, 585.7199539152571, 585.8154547572084, 582.6706221975547, 580.2327008604909, 582.1934747179372, 577.5804738061082, 581.743843581969, 581.2059882974316, 581.4202673064498, 577.7039655825866, 578.0481498342536, 580.81889923994, 579.4122443581597, 576.1553967724536, 580.1606144913507, 576.9460465380937, 578.0418212782266, 574.3184553702212, 575.47465153836, 581.7049193939711, 581.8039445441174, 578.3630735718237, 582.0103199203631, 576.4663834843988, 579.0116589674514, 577.2897009341037, 577.7304762119473, 580.0159184004722, 578.8824030674589, 579.8940512343815, 579.4649088076832, 552.678282804956, 565.6654919228989, 548.9569677649808, 580.8400904819714, 536.9886316889392, 531.9343132465938, 542.8740002306062, 578.1763035296315, 581.3658456190144, 577.3847723217989, 580.9528102052248, 580.724207975298, 581.4343339424146, 582.3888303228666, 580.7793221901155, 581.0105366623484, 579.5030319339085, 580.8853852434202, 581.3456646346277, 582.3426159613628, 579.8872292098829, 582.5394850216798, 580.3086126898469, 580.632961304161, 583.8200728151554, 577.9799314628483, 580.8642588331526, 579.0341427961482, 581.0194439690617, 580.9549792307738, 579.20167348574, 581.1356304924888, 582.5185702725073, 582.745343462913, 648.877999225176, 640.0695977664417, 632.9138697960323, 637.8471368019511, 637.9887762745975, 636.5487487862384, 636.7813069515491, 635.8265932945399, 638.567723650168, 638.635710622684, 544.6561160634155, 538.7405292335891, 616.941803256718, 621.8166087194107, 564.9504382619182, 629.5964153007235, 214.47165620477247, 666.1999274858863, 656.3143305256881, 658.4886589886407, 651.9494013401273, 634.121047116446, 649.0913912139504, 648.0818660179675, 240.59020045713618, 650.433363213431, 652.1118756227779, 652.5328316624915, 650.2902871349419, 638.2504198130888, 653.5608771166479, 556.8228513339863, 610.2667825353532, 609.0026809691196, 606.2942892726076, 601.854354981552, 595.2758929646224, 602.9789628233614, 393.03314568319, 636.7624485113294, 642.4902986721727, 643.6004130009992, 638.3282529801468, 617.5855396240117, 630.971111432303, 619.2753368297691, 577.3613955448735, 582.9858739249036, 579.96433406173, 578.0633843451042, 578.4689944270779, 583.729101480591, 571.9400625567858, 595.2820109759547, 594.5384368975191, 590.5325431878246, 584.6205500642483, 607.18886596608, 609.1972669729981, 605.7474226978602, 600.6643600309836, 604.7903519584072, 599.2179371320912, 592.1166224200865, 603.0646019093915, 398.2821101064165, 639.3661908172887, 634.6811208572941, 615.051224841662, 647.8723990825117, 643.0677423544009, 618.2568993737834, 538.9025740166671, 629.8077786885029, 653.6581665273877, 655.2892494755548, 639.5848520500768, 634.888534752806, 649.845819440041, 627.1039573704682, 653.1476042147448, 656.1736384826877, 653.7450354104236, 649.707540713542, 595.5811403108726, 642.1667048227653, 628.3461673785116, 646.1051308595202, 647.8812316219484, 638.4813205733361, 638.3088952748674, 583.6105761302139, 557.4022183879348, 579.6515728751946, 578.1725247093403, 548.4717198981564, 625.1456811847647, 617.6861550624827, 620.9221643246443, 622.4361578811317, 624.5032884789987, 617.7901211350476, 592.206145913527, 620.5556341385063, 608.9266361870893, 587.9959525856324, 581.5483056938943, 582.5534607358337, 542.2774975016665, 582.5149489211766, 578.2265940215511, 583.6300536166686, 585.1943628409942, 590.1330571923484, 585.4337301357364, 579.6005465617901, 589.6635828249475, 585.6315323347919, 591.7323404094681, 588.4056492583996, 591.5645313864892, 586.6994864713032, 574.3867064591095, 587.2370757439148, 583.1493078421795, 598.9197489306905, 596.1908194614489, 599.2420756647294, 593.0568243010164, 587.2890605996957, 587.7538161737187, 585.8683013400607, 585.7586322839962, 635.2996577364387, 627.0868953810575, 627.5494923521156, 629.0814305847812, 621.2573603000766, 624.3570711320426, 624.2633294506086, 620.6775827752728, 619.54350449996, 625.7501903114368, 624.0289586511922, 621.2343221755025, 623.0009941256869, 619.686355353782, 623.0749760749511, 621.4876511112714, 622.6361104617266, 621.9306448858615, 624.1877682147448, 620.1137804642661, 618.5263560589723, 625.0606636277022, 622.3254539134426, 619.3789071126871, 626.3635949081513, 622.7562024713766, 619.7574441160931, 622.0449141010853, 621.245419690431, 619.808479132322, 621.4646509592459, 618.0662582088509, 617.0027088444336, 617.4906440755491, 612.7886008030816, 617.35648846962, 616.9707551084749, 617.2277874080036, 618.6077305497221, 615.2700374962516, 615.0025301824277, 615.4023213814351, 614.7910502141494, 615.5076468834505, 620.8177788155417, 616.7601400847888, 612.9386722382092, 616.2698382579727, 618.4284716004672, 616.3442851385738, 615.5701556656876, 618.209333839686, 616.590035032545, 618.2568912005386, 618.2762306106787, 615.6561229356703, 620.7319139160404, 611.8990471260335, 623.4679548243805, 623.3638287424375, 620.3610520974368, 618.5973907404235, 620.1257479619368, 612.7893974895529, 614.9336923175192, 619.3909173573079, 618.5650698146693, 616.1840341857306, 618.9232321955674, 621.3348689676623, 617.2292962427051, 614.588440869958, 614.4706150785867, 610.4904250154846, 612.5014237379571, 647.1785911365411, 664.4980222910743, 644.4929663948755, 609.94922487004, 610.4748865935655, 610.1692723198325, 608.2617223070072, 570.6103706351548, 608.6142343164577, 563.4973265618876, 623.9258560525175, 622.9518515968359, 629.5051093151686, 628.7196629584372, 605.2193133229961, 634.1993333583988, 589.348295225036, 620.3582027715896, 618.7458657160942, 616.4139957498655, 611.775072220299, 620.4541391349206, 602.7566091942421]
Elapsed: 0.08199605298320527~0.011789882061852741
Time per graph: 0.0016757815889692803~0.00023998308832650473
Speed: 602.4308312925114~43.09074342635958
Total Time: 0.0815
best val loss: 0.7041817307472229 test_score: 0.5000

Testing...
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7016 score: 0.5000 time: 0.12s
test Score 0.5000
Epoch Time List: [0.2533633179264143, 0.25523712602443993, 0.26129988906905055, 0.26074120204430073, 0.3130132920341566, 0.2535937699722126, 0.2532034239266068, 0.2544539209920913, 0.2583321820711717, 0.26281752600334585, 0.25422287394758314, 0.3531752530252561, 0.2634344029938802, 0.26432531292084605, 0.26687420601956546, 0.2699742551194504, 0.27065658592619, 0.27145074389409274, 0.27259264304302633, 0.2720676128519699, 0.2784268979448825, 0.2739965708460659, 0.27244734996929765, 0.27218683902174234, 0.2729487620526925, 0.2739525850629434, 0.2734550080494955, 0.2740610180189833, 0.2742085180943832, 0.2738697270397097, 0.2744393680477515, 0.2732720379717648, 0.27533826883882284, 0.27456349006388336, 0.2739639839855954, 0.2739821680588648, 0.27602097601629794, 0.27511241601314396, 0.2749424851499498, 0.2762853589374572, 0.2760853199288249, 0.27596198697574437, 0.27508058201055974, 0.2745959711028263, 0.2746261080028489, 0.2744255799334496, 0.2752882060594857, 0.2762459581717849, 0.2755902099888772, 0.27550380607135594, 0.27614147611893713, 0.27519897499587387, 0.2752912261057645, 0.274683310999535, 0.2919426260050386, 0.2905679289251566, 0.2884237340185791, 0.27751078794244677, 0.2840493740513921, 0.2845868718577549, 0.2880165280075744, 0.27850325300823897, 0.27432679291814566, 0.27452952007297426, 0.27518069406505674, 0.274304919061251, 0.2739726809086278, 0.2753040270181373, 0.27545380184892565, 0.2750368268461898, 0.2746617979137227, 0.27501191198825836, 0.2739022789755836, 0.2740118959918618, 0.27437075006309897, 0.27350182295776904, 0.2742271829629317, 0.27366328006610274, 0.2736710680183023, 0.2749212079215795, 0.2741992159280926, 0.27417869213968515, 0.2746027240063995, 0.27421689219772816, 0.27472517208661884, 0.2745129718678072, 0.27457469410728663, 0.27247924904804677, 0.2553659479599446, 0.24471239582635462, 0.2540698539232835, 0.25235631701070815, 0.250837322906591, 0.2502095610834658, 0.2524581920588389, 0.2521049020579085, 0.2507076249457896, 0.2514217549469322, 0.2652400430524722, 0.2910372900078073, 0.2653393360087648, 0.25863847497384995, 0.2705163008067757, 0.26604870497249067, 0.40086335001979023, 0.2740782320033759, 0.2424132350133732, 0.24386269506067038, 0.2456055210204795, 0.2523443379905075, 0.25241483200807124, 0.24689164804294705, 0.37482072599232197, 0.24601255706511438, 0.24393544101621956, 0.2449627669993788, 0.24492574797477573, 0.2505801870720461, 0.24906050297431648, 0.2560853749746457, 0.3367396420799196, 0.26060391683131456, 0.2623578259954229, 0.2659405739977956, 0.2693553731078282, 0.2660223300335929, 0.30324602185282856, 0.27554754295852035, 0.24948814895469695, 0.24877116410061717, 0.24947944295126945, 0.2567473850212991, 0.2574212980689481, 0.2558888590428978, 0.369314456009306, 0.27027786895632744, 0.27141569508239627, 0.2737348999362439, 0.2760472389636561, 0.2725921889068559, 0.2693867600755766, 0.38798508199397475, 0.2684834299143404, 0.2683445338625461, 0.2714744119439274, 0.26594715483952314, 0.2604426150210202, 0.36730664991773665, 0.2648060499923304, 0.26343250495847315, 0.26581590401474386, 0.27098937006667256, 0.26782627613283694, 0.3042041639564559, 0.2633575400104746, 0.25693897204473615, 0.26996849407441914, 0.2530972521053627, 0.26057517202571034, 0.25471366092097014, 0.2783042420633137, 0.3457916658371687, 0.2453843040857464, 0.24575288011692464, 0.24972877418622375, 0.2550875339657068, 0.2517511359183118, 0.24981783714611083, 0.3541054190136492, 0.24490787996910512, 0.2447138640563935, 0.24628784705419093, 0.25712918920908123, 0.2531092829303816, 0.25211683998350054, 0.37304728606250137, 0.2512670719297603, 0.25146548193879426, 0.25644653697963804, 0.40245565806981176, 0.27777256595436484, 0.276489723008126, 0.2796953141223639, 0.2823282000608742, 0.26624212600290775, 0.262658377061598, 0.30511817999649793, 0.26070356788113713, 0.25957097904756665, 0.26249052211642265, 0.26947486703284085, 0.268695673905313, 0.2651385428616777, 0.40229404298588634, 0.2776308039901778, 0.275746684987098, 0.28134668210987, 0.2817262300522998, 0.2764266439480707, 0.3733475070912391, 0.27564912696834654, 0.27250772004481405, 0.27355330204591155, 0.27735739294439554, 0.27549874200485647, 0.2722647249465808, 0.30829350906424224, 0.2714091259986162, 0.27192109706811607, 0.2731122502591461, 0.2773015989223495, 0.2775354728801176, 0.27329083206132054, 0.4377159499563277, 0.26967914099805057, 0.2701415040064603, 0.2731400449993089, 0.27783556398935616, 0.27656780986581, 0.27613881416618824, 0.276896839030087, 0.27048101800028235, 0.259494244819507, 0.2599889950361103, 0.2596941269002855, 0.2614677289966494, 0.2607659350614995, 0.26129814295563847, 0.2630451020086184, 0.2632685829885304, 0.2609434899641201, 0.2618703090120107, 0.26275259198155254, 0.2620370660442859, 0.2630243750754744, 0.2618304869392887, 0.26281037798617035, 0.26284039893653244, 0.26297758205328137, 0.26271117804571986, 0.2635667601134628, 0.2640060220146552, 0.26211909810081124, 0.2623854901175946, 0.26229330303613096, 0.2607792018679902, 0.2630462859524414, 0.2630237099947408, 0.26336234307382256, 0.2629885938949883, 0.26320027408655733, 0.2624362020287663, 0.26335077197290957, 0.26323687098920345, 0.26311350998003036, 0.2650446891784668, 0.2645211289636791, 0.2642756149871275, 0.26458317099604756, 0.26539823703933507, 0.26626135711558163, 0.264345690025948, 0.26496948790736496, 0.26495749794412404, 0.2648210789775476, 0.2638228030409664, 0.26572270004544407, 0.2645027458202094, 0.2641341519774869, 0.26460808294359595, 0.26515296404249966, 0.2651341331657022, 0.26391656801570207, 0.2655705668730661, 0.26572301995474845, 0.26521291700191796, 0.2639635318191722, 0.26567452191375196, 0.2653548950329423, 0.2647440171567723, 0.26247683411929756, 0.2637744549429044, 0.26467955706175417, 0.2644629339920357, 0.26527828606776893, 0.2650955179706216, 0.26486295694485307, 0.2628489030757919, 0.26447852910496294, 0.26577018504031, 0.2655347768450156, 0.2653968700906262, 0.2668423771392554, 0.26583185384515673, 0.2655799000058323, 0.2651821420295164, 0.26351929001975805, 0.2590735269477591, 0.25700847909320146, 0.3982121979352087, 0.27604980405885726, 0.2763951011002064, 0.27658975799568, 0.2832831181585789, 0.2809234970482066, 0.28046639694366604, 0.40377108100801706, 0.27172539907041937, 0.2707185969920829, 0.2709852011175826, 0.2773083900101483, 0.27099778200499713, 0.26831282407511026, 0.3946245778352022, 0.27102677896618843, 0.2707970351912081, 0.27533770503941923, 0.2780949061270803, 0.2740328658837825]
Total Epoch List: [179, 117, 23]
Total Time List: [0.07753726898226887, 0.08098952600266784, 0.08154639601707458]
T-times Epoch Time: 0.2741286107018328 ~ 0.001797863567141606
T-times Total Epoch: 91.55555555555554 ~ 26.00617210691187
T-times Total Time: 0.07919402710265583 ~ 0.0007127619133267982
T-times Inference Elapsed: 0.08026733433884697 ~ 0.001222556908556485
T-times Time Per Graph: 0.0016442377611262538 ~ 2.2379405416127624e-05
T-times Speed: 618.833284733164 ~ 12.502413397932704
T-times cross validation test micro f1 score:0.766450714786676 ~ 0.12135962952073152
T-times cross validation test precision:0.7575033087455448 ~ 0.1518382472704965
T-times cross validation test recall:0.8166666666666665 ~ 0.1373559851869101
T-times cross validation test f1_score:0.766450714786676 ~ 0.11684049280754277
