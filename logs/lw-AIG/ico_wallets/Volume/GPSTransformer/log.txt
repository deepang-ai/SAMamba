Namespace(seed=60, model='GPSTransformer', dataset='ico_wallets/Volume', num_heads=1, num_layers=2, dim_hidden=64, dropout=0.1, epochs=1000, lr=0.0003, weight_decay=0.0001, batch_size=64, abs_pe=None, abs_pe_dim=20, num_class=2, outdir='./outdir/ico_wallets/Volume/seed60/edge_attr/khopgnn_gat_1_0.1_0.0003_0.0001_2_1_64_BN', warmup=10, layer_norm=False, use_edge_attr=True, edge_dim=128, gnn_type='gat', k_hop=1, global_pool='mean', se='khopgnn', ie='mamba', aggr='add', not_extract_node_feature=False, k_ford=3, early_stop=1, early_stop_mindelta=-0.0, patience=20, training_times=3, Lambda=0.01, order='sd', use_cuda=True, batch_norm=True, save_logs=True)
Data(edge_index=[2, 334], edge_attr=[334, 2], x=[97, 14887], y=[1, 1], num_nodes=97)
Data(edge_index=[2, 334], edge_attr=[334, 2], x=[97, 14887], y=[1, 1], num_nodes=97)
Data(edge_index=[2, 276], edge_attr=[276, 2], x=[85, 14887], y=[1, 1], num_nodes=97)
========================training times:0========================
========================k_idx:0========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: GPSTransformer
GPS(
  (x_lin): Linear(in_features=14887, out_features=64, bias=True)
  (pe_lin): Linear(in_features=20, out_features=8, bias=True)
  (pe_norm): BatchNorm1d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (convs): ModuleList(
    (0-1): 2 x GPSConv(64, conv=GINEConv(nn=Sequential(
      (0): Linear(in_features=64, out_features=64, bias=True)
      (1): ReLU()
      (2): Linear(in_features=64, out_features=64, bias=True)
    )), heads=1, attn_type=multihead)
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=32, bias=True)
    (1): ReLU()
    (2): Linear(in_features=32, out_features=16, bias=True)
    (3): ReLU()
    (4): Linear(in_features=16, out_features=2, bias=True)
  )
)
Total number of parameters: 1039906
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7d29a7d8b4c0>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 1.1001;  Loss pred: 1.1001; Loss self: 0.0000; time: 0.40s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 3.0975 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 2.1025 score: 0.4898 time: 0.06s
Epoch 2/1000, LR 0.000000
Train loss: 1.1064;  Loss pred: 1.1064; Loss self: 0.0000; time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 2.4982 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 1.7182 score: 0.4898 time: 0.06s
Epoch 3/1000, LR 0.000030
Train loss: 1.0039;  Loss pred: 1.0039; Loss self: 0.0000; time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 2.0903 score: 0.5102 time: 0.10s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 1.4654 score: 0.4898 time: 0.06s
Epoch 4/1000, LR 0.000060
Train loss: 0.9160;  Loss pred: 0.9160; Loss self: 0.0000; time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 1.7915 score: 0.5102 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 1.2847 score: 0.4898 time: 0.06s
Epoch 5/1000, LR 0.000090
Train loss: 0.7964;  Loss pred: 0.7964; Loss self: 0.0000; time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 1.5618 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 1.1502 score: 0.4898 time: 0.07s
Epoch 6/1000, LR 0.000120
Train loss: 0.6820;  Loss pred: 0.6820; Loss self: 0.0000; time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 1.3867 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 1.0488 score: 0.4898 time: 0.06s
Epoch 7/1000, LR 0.000150
Train loss: 0.5638;  Loss pred: 0.5638; Loss self: 0.0000; time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 1.2416 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.9696 score: 0.4898 time: 0.06s
Epoch 8/1000, LR 0.000180
Train loss: 0.4760;  Loss pred: 0.4760; Loss self: 0.0000; time: 0.30s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 1.1326 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.9160 score: 0.4898 time: 0.06s
Epoch 9/1000, LR 0.000210
Train loss: 0.4088;  Loss pred: 0.4088; Loss self: 0.0000; time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 1.0508 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.8727 score: 0.4898 time: 0.06s
Epoch 10/1000, LR 0.000240
Train loss: 0.3607;  Loss pred: 0.3607; Loss self: 0.0000; time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.9856 score: 0.5102 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.8361 score: 0.4898 time: 0.06s
Epoch 11/1000, LR 0.000270
Train loss: 0.3039;  Loss pred: 0.3039; Loss self: 0.0000; time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.9437 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.8110 score: 0.4898 time: 0.07s
Epoch 12/1000, LR 0.000270
Train loss: 0.2579;  Loss pred: 0.2579; Loss self: 0.0000; time: 0.21s
Val loss: 0.9099 score: 0.4898 time: 0.07s
Test loss: 0.7906 score: 0.5102 time: 0.06s
Epoch 13/1000, LR 0.000270
Train loss: 0.2200;  Loss pred: 0.2200; Loss self: 0.0000; time: 0.21s
Val loss: 0.8826 score: 0.4694 time: 0.10s
Test loss: 0.7730 score: 0.5918 time: 0.07s
Epoch 14/1000, LR 0.000270
Train loss: 0.1989;  Loss pred: 0.1989; Loss self: 0.0000; time: 0.20s
Val loss: 0.8580 score: 0.4694 time: 0.07s
Test loss: 0.7566 score: 0.5918 time: 0.06s
Epoch 15/1000, LR 0.000270
Train loss: 0.1680;  Loss pred: 0.1680; Loss self: 0.0000; time: 0.20s
Val loss: 0.8349 score: 0.4898 time: 0.07s
Test loss: 0.7408 score: 0.5714 time: 0.06s
Epoch 16/1000, LR 0.000270
Train loss: 0.1498;  Loss pred: 0.1498; Loss self: 0.0000; time: 0.20s
Val loss: 0.8159 score: 0.5102 time: 0.07s
Test loss: 0.7272 score: 0.5510 time: 0.06s
Epoch 17/1000, LR 0.000270
Train loss: 0.1326;  Loss pred: 0.1326; Loss self: 0.0000; time: 0.21s
Val loss: 0.8000 score: 0.5102 time: 0.07s
Test loss: 0.7166 score: 0.5510 time: 0.07s
Epoch 18/1000, LR 0.000270
Train loss: 0.1145;  Loss pred: 0.1145; Loss self: 0.0000; time: 0.20s
Val loss: 0.7883 score: 0.5510 time: 0.07s
Test loss: 0.7080 score: 0.5510 time: 0.06s
Epoch 19/1000, LR 0.000270
Train loss: 0.1038;  Loss pred: 0.1038; Loss self: 0.0000; time: 0.20s
Val loss: 0.7774 score: 0.6122 time: 0.07s
Test loss: 0.7009 score: 0.6122 time: 0.06s
Epoch 20/1000, LR 0.000270
Train loss: 0.0948;  Loss pred: 0.0948; Loss self: 0.0000; time: 0.19s
Val loss: 0.7675 score: 0.5714 time: 0.07s
Test loss: 0.6943 score: 0.5918 time: 0.06s
Epoch 21/1000, LR 0.000270
Train loss: 0.0827;  Loss pred: 0.0827; Loss self: 0.0000; time: 0.20s
Val loss: 0.7591 score: 0.5306 time: 0.07s
Test loss: 0.6893 score: 0.6327 time: 0.06s
Epoch 22/1000, LR 0.000270
Train loss: 0.0746;  Loss pred: 0.0746; Loss self: 0.0000; time: 0.19s
Val loss: 0.7489 score: 0.4898 time: 0.07s
Test loss: 0.6835 score: 0.6122 time: 0.06s
Epoch 23/1000, LR 0.000270
Train loss: 0.0660;  Loss pred: 0.0660; Loss self: 0.0000; time: 0.19s
Val loss: 0.7391 score: 0.5102 time: 0.07s
Test loss: 0.6778 score: 0.6122 time: 0.06s
Epoch 24/1000, LR 0.000270
Train loss: 0.0633;  Loss pred: 0.0633; Loss self: 0.0000; time: 0.19s
Val loss: 0.7304 score: 0.4898 time: 0.07s
Test loss: 0.6717 score: 0.6327 time: 0.06s
Epoch 25/1000, LR 0.000270
Train loss: 0.0546;  Loss pred: 0.0546; Loss self: 0.0000; time: 0.20s
Val loss: 0.7222 score: 0.4694 time: 0.08s
Test loss: 0.6660 score: 0.6122 time: 0.06s
Epoch 26/1000, LR 0.000270
Train loss: 0.0534;  Loss pred: 0.0534; Loss self: 0.0000; time: 0.21s
Val loss: 0.7148 score: 0.4898 time: 0.09s
Test loss: 0.6607 score: 0.6122 time: 0.07s
Epoch 27/1000, LR 0.000270
Train loss: 0.0510;  Loss pred: 0.0510; Loss self: 0.0000; time: 0.20s
Val loss: 0.7078 score: 0.4898 time: 0.07s
Test loss: 0.6560 score: 0.6122 time: 0.06s
Epoch 28/1000, LR 0.000270
Train loss: 0.0426;  Loss pred: 0.0426; Loss self: 0.0000; time: 0.19s
Val loss: 0.7013 score: 0.4898 time: 0.07s
Test loss: 0.6515 score: 0.6327 time: 0.06s
Epoch 29/1000, LR 0.000270
Train loss: 0.0406;  Loss pred: 0.0406; Loss self: 0.0000; time: 0.19s
Val loss: 0.6939 score: 0.4898 time: 0.07s
Test loss: 0.6464 score: 0.6327 time: 0.06s
Epoch 30/1000, LR 0.000270
Train loss: 0.0362;  Loss pred: 0.0362; Loss self: 0.0000; time: 0.19s
Val loss: 0.6870 score: 0.4898 time: 0.07s
Test loss: 0.6418 score: 0.6327 time: 0.06s
Epoch 31/1000, LR 0.000270
Train loss: 0.0322;  Loss pred: 0.0322; Loss self: 0.0000; time: 0.20s
Val loss: 0.6803 score: 0.5102 time: 0.07s
Test loss: 0.6378 score: 0.6327 time: 0.06s
Epoch 32/1000, LR 0.000270
Train loss: 0.0323;  Loss pred: 0.0323; Loss self: 0.0000; time: 0.19s
Val loss: 0.6724 score: 0.5102 time: 0.08s
Test loss: 0.6332 score: 0.6327 time: 0.07s
Epoch 33/1000, LR 0.000270
Train loss: 0.0283;  Loss pred: 0.0283; Loss self: 0.0000; time: 0.20s
Val loss: 0.6644 score: 0.5306 time: 0.07s
Test loss: 0.6283 score: 0.6735 time: 0.06s
Epoch 34/1000, LR 0.000270
Train loss: 0.0245;  Loss pred: 0.0245; Loss self: 0.0000; time: 0.20s
Val loss: 0.6574 score: 0.5510 time: 0.07s
Test loss: 0.6236 score: 0.7143 time: 0.06s
Epoch 35/1000, LR 0.000270
Train loss: 0.0213;  Loss pred: 0.0213; Loss self: 0.0000; time: 0.20s
Val loss: 0.6519 score: 0.5306 time: 0.07s
Test loss: 0.6195 score: 0.7143 time: 0.06s
Epoch 36/1000, LR 0.000270
Train loss: 0.0212;  Loss pred: 0.0212; Loss self: 0.0000; time: 0.21s
Val loss: 0.6453 score: 0.5510 time: 0.07s
Test loss: 0.6150 score: 0.7143 time: 0.06s
Epoch 37/1000, LR 0.000270
Train loss: 0.0225;  Loss pred: 0.0225; Loss self: 0.0000; time: 0.19s
Val loss: 0.6398 score: 0.5714 time: 0.07s
Test loss: 0.6107 score: 0.7143 time: 0.06s
Epoch 38/1000, LR 0.000270
Train loss: 0.0165;  Loss pred: 0.0165; Loss self: 0.0000; time: 0.19s
Val loss: 0.6358 score: 0.5714 time: 0.07s
Test loss: 0.6071 score: 0.7143 time: 0.06s
Epoch 39/1000, LR 0.000269
Train loss: 0.0160;  Loss pred: 0.0160; Loss self: 0.0000; time: 0.19s
Val loss: 0.6321 score: 0.5714 time: 0.07s
Test loss: 0.6035 score: 0.7143 time: 0.06s
Epoch 40/1000, LR 0.000269
Train loss: 0.0150;  Loss pred: 0.0150; Loss self: 0.0000; time: 0.19s
Val loss: 0.6273 score: 0.5714 time: 0.07s
Test loss: 0.5996 score: 0.7143 time: 0.06s
Epoch 41/1000, LR 0.000269
Train loss: 0.0129;  Loss pred: 0.0129; Loss self: 0.0000; time: 0.19s
Val loss: 0.6232 score: 0.5714 time: 0.07s
Test loss: 0.5960 score: 0.7143 time: 0.06s
Epoch 42/1000, LR 0.000269
Train loss: 0.0118;  Loss pred: 0.0118; Loss self: 0.0000; time: 0.19s
Val loss: 0.6199 score: 0.5918 time: 0.07s
Test loss: 0.5927 score: 0.7143 time: 0.06s
Epoch 43/1000, LR 0.000269
Train loss: 0.0124;  Loss pred: 0.0124; Loss self: 0.0000; time: 0.19s
Val loss: 0.6171 score: 0.5918 time: 0.07s
Test loss: 0.5896 score: 0.7143 time: 0.06s
Epoch 44/1000, LR 0.000269
Train loss: 0.0118;  Loss pred: 0.0118; Loss self: 0.0000; time: 0.19s
Val loss: 0.6132 score: 0.5714 time: 0.07s
Test loss: 0.5864 score: 0.7347 time: 0.06s
Epoch 45/1000, LR 0.000269
Train loss: 0.0124;  Loss pred: 0.0124; Loss self: 0.0000; time: 0.19s
Val loss: 0.6076 score: 0.5918 time: 0.07s
Test loss: 0.5830 score: 0.7551 time: 0.06s
Epoch 46/1000, LR 0.000269
Train loss: 0.0106;  Loss pred: 0.0106; Loss self: 0.0000; time: 0.19s
Val loss: 0.6023 score: 0.6327 time: 0.07s
Test loss: 0.5800 score: 0.7551 time: 0.06s
Epoch 47/1000, LR 0.000269
Train loss: 0.0111;  Loss pred: 0.0111; Loss self: 0.0000; time: 0.19s
Val loss: 0.5968 score: 0.6735 time: 0.07s
Test loss: 0.5772 score: 0.7551 time: 0.06s
Epoch 48/1000, LR 0.000269
Train loss: 0.0089;  Loss pred: 0.0089; Loss self: 0.0000; time: 0.19s
Val loss: 0.5912 score: 0.6735 time: 0.08s
Test loss: 0.5742 score: 0.7551 time: 0.06s
Epoch 49/1000, LR 0.000269
Train loss: 0.0115;  Loss pred: 0.0115; Loss self: 0.0000; time: 0.19s
Val loss: 0.5838 score: 0.6939 time: 0.07s
Test loss: 0.5700 score: 0.7347 time: 0.06s
Epoch 50/1000, LR 0.000269
Train loss: 0.0086;  Loss pred: 0.0086; Loss self: 0.0000; time: 0.19s
Val loss: 0.5771 score: 0.7347 time: 0.07s
Test loss: 0.5661 score: 0.7347 time: 0.06s
Epoch 51/1000, LR 0.000269
Train loss: 0.0081;  Loss pred: 0.0081; Loss self: 0.0000; time: 0.20s
Val loss: 0.5703 score: 0.7755 time: 0.07s
Test loss: 0.5623 score: 0.7347 time: 0.06s
Epoch 52/1000, LR 0.000269
Train loss: 0.0072;  Loss pred: 0.0072; Loss self: 0.0000; time: 0.19s
Val loss: 0.5629 score: 0.7755 time: 0.07s
Test loss: 0.5580 score: 0.7551 time: 0.06s
Epoch 53/1000, LR 0.000269
Train loss: 0.0069;  Loss pred: 0.0069; Loss self: 0.0000; time: 0.19s
Val loss: 0.5555 score: 0.7755 time: 0.07s
Test loss: 0.5540 score: 0.7551 time: 0.06s
Epoch 54/1000, LR 0.000269
Train loss: 0.0067;  Loss pred: 0.0067; Loss self: 0.0000; time: 0.19s
Val loss: 0.5480 score: 0.7959 time: 0.07s
Test loss: 0.5501 score: 0.7551 time: 0.06s
Epoch 55/1000, LR 0.000269
Train loss: 0.0072;  Loss pred: 0.0072; Loss self: 0.0000; time: 0.19s
Val loss: 0.5396 score: 0.7959 time: 0.08s
Test loss: 0.5456 score: 0.7755 time: 0.06s
Epoch 56/1000, LR 0.000269
Train loss: 0.0078;  Loss pred: 0.0078; Loss self: 0.0000; time: 0.19s
Val loss: 0.5311 score: 0.8367 time: 0.07s
Test loss: 0.5413 score: 0.7755 time: 0.06s
Epoch 57/1000, LR 0.000269
Train loss: 0.0060;  Loss pred: 0.0060; Loss self: 0.0000; time: 0.19s
Val loss: 0.5216 score: 0.8776 time: 0.07s
Test loss: 0.5362 score: 0.7755 time: 0.06s
Epoch 58/1000, LR 0.000269
Train loss: 0.0054;  Loss pred: 0.0054; Loss self: 0.0000; time: 0.19s
Val loss: 0.5119 score: 0.8776 time: 0.07s
Test loss: 0.5306 score: 0.7959 time: 0.06s
Epoch 59/1000, LR 0.000268
Train loss: 0.0058;  Loss pred: 0.0058; Loss self: 0.0000; time: 0.19s
Val loss: 0.5014 score: 0.8776 time: 0.07s
Test loss: 0.5236 score: 0.7959 time: 0.06s
Epoch 60/1000, LR 0.000268
Train loss: 0.0066;  Loss pred: 0.0066; Loss self: 0.0000; time: 0.20s
Val loss: 0.4898 score: 0.8980 time: 0.07s
Test loss: 0.5148 score: 0.7959 time: 0.06s
Epoch 61/1000, LR 0.000268
Train loss: 0.0054;  Loss pred: 0.0054; Loss self: 0.0000; time: 0.20s
Val loss: 0.4774 score: 0.9184 time: 0.07s
Test loss: 0.5053 score: 0.7959 time: 0.06s
Epoch 62/1000, LR 0.000268
Train loss: 0.0043;  Loss pred: 0.0043; Loss self: 0.0000; time: 0.20s
Val loss: 0.4650 score: 0.9184 time: 0.07s
Test loss: 0.4954 score: 0.7959 time: 0.06s
Epoch 63/1000, LR 0.000268
Train loss: 0.0045;  Loss pred: 0.0045; Loss self: 0.0000; time: 0.19s
Val loss: 0.4530 score: 0.9184 time: 0.07s
Test loss: 0.4858 score: 0.7959 time: 0.06s
Epoch 64/1000, LR 0.000268
Train loss: 0.0041;  Loss pred: 0.0041; Loss self: 0.0000; time: 0.19s
Val loss: 0.4416 score: 0.9184 time: 0.07s
Test loss: 0.4764 score: 0.7959 time: 0.06s
Epoch 65/1000, LR 0.000268
Train loss: 0.0041;  Loss pred: 0.0041; Loss self: 0.0000; time: 0.19s
Val loss: 0.4305 score: 0.9184 time: 0.07s
Test loss: 0.4677 score: 0.8163 time: 0.06s
Epoch 66/1000, LR 0.000268
Train loss: 0.0042;  Loss pred: 0.0042; Loss self: 0.0000; time: 0.19s
Val loss: 0.4197 score: 0.9184 time: 0.07s
Test loss: 0.4591 score: 0.8163 time: 0.06s
Epoch 67/1000, LR 0.000268
Train loss: 0.0035;  Loss pred: 0.0035; Loss self: 0.0000; time: 0.19s
Val loss: 0.4095 score: 0.9184 time: 0.07s
Test loss: 0.4516 score: 0.8163 time: 0.06s
Epoch 68/1000, LR 0.000268
Train loss: 0.0042;  Loss pred: 0.0042; Loss self: 0.0000; time: 0.20s
Val loss: 0.3992 score: 0.9388 time: 0.08s
Test loss: 0.4435 score: 0.8163 time: 0.06s
Epoch 69/1000, LR 0.000268
Train loss: 0.0032;  Loss pred: 0.0032; Loss self: 0.0000; time: 0.20s
Val loss: 0.3894 score: 0.9388 time: 0.07s
Test loss: 0.4360 score: 0.8367 time: 0.06s
Epoch 70/1000, LR 0.000268
Train loss: 0.0039;  Loss pred: 0.0039; Loss self: 0.0000; time: 0.20s
Val loss: 0.3799 score: 0.9592 time: 0.07s
Test loss: 0.4289 score: 0.8367 time: 0.06s
Epoch 71/1000, LR 0.000268
Train loss: 0.0037;  Loss pred: 0.0037; Loss self: 0.0000; time: 0.19s
Val loss: 0.3699 score: 0.9592 time: 0.07s
Test loss: 0.4205 score: 0.8163 time: 0.06s
Epoch 72/1000, LR 0.000267
Train loss: 0.0029;  Loss pred: 0.0029; Loss self: 0.0000; time: 0.19s
Val loss: 0.3603 score: 0.9592 time: 0.07s
Test loss: 0.4129 score: 0.8163 time: 0.06s
Epoch 73/1000, LR 0.000267
Train loss: 0.0032;  Loss pred: 0.0032; Loss self: 0.0000; time: 0.19s
Val loss: 0.3504 score: 0.9592 time: 0.07s
Test loss: 0.4058 score: 0.8163 time: 0.06s
Epoch 74/1000, LR 0.000267
Train loss: 0.0034;  Loss pred: 0.0034; Loss self: 0.0000; time: 0.19s
Val loss: 0.3392 score: 0.9592 time: 0.07s
Test loss: 0.3986 score: 0.8367 time: 0.06s
Epoch 75/1000, LR 0.000267
Train loss: 0.0036;  Loss pred: 0.0036; Loss self: 0.0000; time: 0.19s
Val loss: 0.3259 score: 0.9592 time: 0.07s
Test loss: 0.3909 score: 0.8367 time: 0.06s
Epoch 76/1000, LR 0.000267
Train loss: 0.0036;  Loss pred: 0.0036; Loss self: 0.0000; time: 0.19s
Val loss: 0.3136 score: 0.9592 time: 0.07s
Test loss: 0.3827 score: 0.8571 time: 0.06s
Epoch 77/1000, LR 0.000267
Train loss: 0.0026;  Loss pred: 0.0026; Loss self: 0.0000; time: 0.19s
Val loss: 0.3011 score: 0.9592 time: 0.07s
Test loss: 0.3740 score: 0.8571 time: 0.06s
Epoch 78/1000, LR 0.000267
Train loss: 0.0027;  Loss pred: 0.0027; Loss self: 0.0000; time: 0.19s
Val loss: 0.2878 score: 0.9592 time: 0.07s
Test loss: 0.3653 score: 0.8571 time: 0.06s
Epoch 79/1000, LR 0.000267
Train loss: 0.0023;  Loss pred: 0.0023; Loss self: 0.0000; time: 0.19s
Val loss: 0.2754 score: 0.9592 time: 0.07s
Test loss: 0.3573 score: 0.8776 time: 0.06s
Epoch 80/1000, LR 0.000267
Train loss: 0.0036;  Loss pred: 0.0036; Loss self: 0.0000; time: 0.19s
Val loss: 0.2614 score: 0.9388 time: 0.07s
Test loss: 0.3474 score: 0.8980 time: 0.06s
Epoch 81/1000, LR 0.000267
Train loss: 0.0037;  Loss pred: 0.0037; Loss self: 0.0000; time: 0.19s
Val loss: 0.2481 score: 0.9184 time: 0.07s
Test loss: 0.3356 score: 0.8980 time: 0.06s
Epoch 82/1000, LR 0.000267
Train loss: 0.0032;  Loss pred: 0.0032; Loss self: 0.0000; time: 0.21s
Val loss: 0.2371 score: 0.9184 time: 0.07s
Test loss: 0.3269 score: 0.8980 time: 0.06s
Epoch 83/1000, LR 0.000266
Train loss: 0.0027;  Loss pred: 0.0027; Loss self: 0.0000; time: 0.19s
Val loss: 0.2272 score: 0.9184 time: 0.07s
Test loss: 0.3204 score: 0.8980 time: 0.06s
Epoch 84/1000, LR 0.000266
Train loss: 0.0026;  Loss pred: 0.0026; Loss self: 0.0000; time: 0.19s
Val loss: 0.2202 score: 0.9184 time: 0.07s
Test loss: 0.3172 score: 0.8980 time: 0.06s
Epoch 85/1000, LR 0.000266
Train loss: 0.0022;  Loss pred: 0.0022; Loss self: 0.0000; time: 0.19s
Val loss: 0.2153 score: 0.9184 time: 0.07s
Test loss: 0.3158 score: 0.8776 time: 0.06s
Epoch 86/1000, LR 0.000266
Train loss: 0.0020;  Loss pred: 0.0020; Loss self: 0.0000; time: 0.20s
Val loss: 0.2120 score: 0.9184 time: 0.07s
Test loss: 0.3158 score: 0.8776 time: 0.06s
Epoch 87/1000, LR 0.000266
Train loss: 0.0021;  Loss pred: 0.0021; Loss self: 0.0000; time: 0.20s
Val loss: 0.2112 score: 0.9184 time: 0.07s
Test loss: 0.3168 score: 0.8980 time: 0.06s
Epoch 88/1000, LR 0.000266
Train loss: 0.0024;  Loss pred: 0.0024; Loss self: 0.0000; time: 0.19s
Val loss: 0.2112 score: 0.9184 time: 0.07s
Test loss: 0.3186 score: 0.8980 time: 0.06s
Epoch 89/1000, LR 0.000266
Train loss: 0.0021;  Loss pred: 0.0021; Loss self: 0.0000; time: 0.19s
Val loss: 0.2122 score: 0.9184 time: 0.07s
Test loss: 0.3215 score: 0.8980 time: 0.06s
     INFO: Early stopping counter 1 of 20
Epoch 90/1000, LR 0.000266
Train loss: 0.0027;  Loss pred: 0.0027; Loss self: 0.0000; time: 0.19s
Val loss: 0.2098 score: 0.9184 time: 0.07s
Test loss: 0.3226 score: 0.8980 time: 0.06s
Epoch 91/1000, LR 0.000266
Train loss: 0.0024;  Loss pred: 0.0024; Loss self: 0.0000; time: 0.19s
Val loss: 0.2054 score: 0.9184 time: 0.07s
Test loss: 0.3221 score: 0.8980 time: 0.06s
Epoch 92/1000, LR 0.000266
Train loss: 0.0020;  Loss pred: 0.0020; Loss self: 0.0000; time: 0.19s
Val loss: 0.1984 score: 0.9184 time: 0.07s
Test loss: 0.3217 score: 0.8980 time: 0.06s
Epoch 93/1000, LR 0.000265
Train loss: 0.0024;  Loss pred: 0.0024; Loss self: 0.0000; time: 0.19s
Val loss: 0.1928 score: 0.9184 time: 0.07s
Test loss: 0.3234 score: 0.8980 time: 0.06s
Epoch 94/1000, LR 0.000265
Train loss: 0.0020;  Loss pred: 0.0020; Loss self: 0.0000; time: 0.20s
Val loss: 0.1892 score: 0.9184 time: 0.08s
Test loss: 0.3260 score: 0.9184 time: 0.06s
Epoch 95/1000, LR 0.000265
Train loss: 0.0019;  Loss pred: 0.0019; Loss self: 0.0000; time: 0.19s
Val loss: 0.1869 score: 0.9184 time: 0.07s
Test loss: 0.3298 score: 0.8980 time: 0.06s
Epoch 96/1000, LR 0.000265
Train loss: 0.0017;  Loss pred: 0.0017; Loss self: 0.0000; time: 0.19s
Val loss: 0.1850 score: 0.9184 time: 0.07s
Test loss: 0.3340 score: 0.8980 time: 0.06s
Epoch 97/1000, LR 0.000265
Train loss: 0.0017;  Loss pred: 0.0017; Loss self: 0.0000; time: 0.20s
Val loss: 0.1820 score: 0.8980 time: 0.07s
Test loss: 0.3383 score: 0.8980 time: 0.06s
Epoch 98/1000, LR 0.000265
Train loss: 0.0014;  Loss pred: 0.0014; Loss self: 0.0000; time: 0.19s
Val loss: 0.1793 score: 0.8980 time: 0.07s
Test loss: 0.3423 score: 0.8980 time: 0.06s
Epoch 99/1000, LR 0.000265
Train loss: 0.0020;  Loss pred: 0.0020; Loss self: 0.0000; time: 0.19s
Val loss: 0.1760 score: 0.8980 time: 0.07s
Test loss: 0.3441 score: 0.8980 time: 0.06s
Epoch 100/1000, LR 0.000265
Train loss: 0.0016;  Loss pred: 0.0016; Loss self: 0.0000; time: 0.19s
Val loss: 0.1735 score: 0.8980 time: 0.07s
Test loss: 0.3461 score: 0.8980 time: 0.06s
Epoch 101/1000, LR 0.000265
Train loss: 0.0017;  Loss pred: 0.0017; Loss self: 0.0000; time: 0.19s
Val loss: 0.1726 score: 0.8980 time: 0.07s
Test loss: 0.3492 score: 0.8980 time: 0.06s
Epoch 102/1000, LR 0.000264
Train loss: 0.0016;  Loss pred: 0.0016; Loss self: 0.0000; time: 0.20s
Val loss: 0.1732 score: 0.8980 time: 0.07s
Test loss: 0.3533 score: 0.8980 time: 0.06s
     INFO: Early stopping counter 1 of 20
Epoch 103/1000, LR 0.000264
Train loss: 0.0015;  Loss pred: 0.0015; Loss self: 0.0000; time: 0.19s
Val loss: 0.1762 score: 0.8776 time: 0.07s
Test loss: 0.3591 score: 0.8980 time: 0.06s
     INFO: Early stopping counter 2 of 20
Epoch 104/1000, LR 0.000264
Train loss: 0.0021;  Loss pred: 0.0021; Loss self: 0.0000; time: 0.19s
Val loss: 0.1798 score: 0.8776 time: 0.07s
Test loss: 0.3630 score: 0.8571 time: 0.06s
     INFO: Early stopping counter 3 of 20
Epoch 105/1000, LR 0.000264
Train loss: 0.0017;  Loss pred: 0.0017; Loss self: 0.0000; time: 0.19s
Val loss: 0.1815 score: 0.8776 time: 0.07s
Test loss: 0.3676 score: 0.8571 time: 0.06s
     INFO: Early stopping counter 4 of 20
Epoch 106/1000, LR 0.000264
Train loss: 0.0015;  Loss pred: 0.0015; Loss self: 0.0000; time: 0.19s
Val loss: 0.1839 score: 0.8776 time: 0.07s
Test loss: 0.3737 score: 0.8571 time: 0.06s
     INFO: Early stopping counter 5 of 20
Epoch 107/1000, LR 0.000264
Train loss: 0.0014;  Loss pred: 0.0014; Loss self: 0.0000; time: 0.19s
Val loss: 0.1855 score: 0.8776 time: 0.07s
Test loss: 0.3798 score: 0.8571 time: 0.06s
     INFO: Early stopping counter 6 of 20
Epoch 108/1000, LR 0.000264
Train loss: 0.0016;  Loss pred: 0.0016; Loss self: 0.0000; time: 0.19s
Val loss: 0.1886 score: 0.8776 time: 0.08s
Test loss: 0.3871 score: 0.8571 time: 0.06s
     INFO: Early stopping counter 7 of 20
Epoch 109/1000, LR 0.000264
Train loss: 0.0019;  Loss pred: 0.0019; Loss self: 0.0000; time: 0.20s
Val loss: 0.1930 score: 0.8776 time: 0.07s
Test loss: 0.3937 score: 0.8571 time: 0.06s
     INFO: Early stopping counter 8 of 20
Epoch 110/1000, LR 0.000263
Train loss: 0.0013;  Loss pred: 0.0013; Loss self: 0.0000; time: 0.19s
Val loss: 0.1970 score: 0.8776 time: 0.07s
Test loss: 0.4003 score: 0.8571 time: 0.06s
     INFO: Early stopping counter 9 of 20
Epoch 111/1000, LR 0.000263
Train loss: 0.0014;  Loss pred: 0.0014; Loss self: 0.0000; time: 0.20s
Val loss: 0.1980 score: 0.8776 time: 0.07s
Test loss: 0.4053 score: 0.8571 time: 0.06s
     INFO: Early stopping counter 10 of 20
Epoch 112/1000, LR 0.000263
Train loss: 0.0019;  Loss pred: 0.0019; Loss self: 0.0000; time: 0.20s
Val loss: 0.2006 score: 0.8776 time: 0.07s
Test loss: 0.4104 score: 0.8776 time: 0.06s
     INFO: Early stopping counter 11 of 20
Epoch 113/1000, LR 0.000263
Train loss: 0.0013;  Loss pred: 0.0013; Loss self: 0.0000; time: 0.19s
Val loss: 0.2020 score: 0.8776 time: 0.07s
Test loss: 0.4158 score: 0.8776 time: 0.06s
     INFO: Early stopping counter 12 of 20
Epoch 114/1000, LR 0.000263
Train loss: 0.0021;  Loss pred: 0.0021; Loss self: 0.0000; time: 0.19s
Val loss: 0.2051 score: 0.8980 time: 0.07s
Test loss: 0.4228 score: 0.8776 time: 0.06s
     INFO: Early stopping counter 13 of 20
Epoch 115/1000, LR 0.000263
Train loss: 0.0014;  Loss pred: 0.0014; Loss self: 0.0000; time: 0.19s
Val loss: 0.2026 score: 0.8980 time: 0.07s
Test loss: 0.4289 score: 0.8776 time: 0.06s
     INFO: Early stopping counter 14 of 20
Epoch 116/1000, LR 0.000263
Train loss: 0.0014;  Loss pred: 0.0014; Loss self: 0.0000; time: 0.19s
Val loss: 0.2016 score: 0.8980 time: 0.07s
Test loss: 0.4355 score: 0.8776 time: 0.06s
     INFO: Early stopping counter 15 of 20
Epoch 117/1000, LR 0.000262
Train loss: 0.0011;  Loss pred: 0.0011; Loss self: 0.0000; time: 0.19s
Val loss: 0.2021 score: 0.8980 time: 0.07s
Test loss: 0.4414 score: 0.8776 time: 0.06s
     INFO: Early stopping counter 16 of 20
Epoch 118/1000, LR 0.000262
Train loss: 0.0011;  Loss pred: 0.0011; Loss self: 0.0000; time: 0.19s
Val loss: 0.2035 score: 0.8980 time: 0.07s
Test loss: 0.4463 score: 0.8776 time: 0.06s
     INFO: Early stopping counter 17 of 20
Epoch 119/1000, LR 0.000262
Train loss: 0.0011;  Loss pred: 0.0011; Loss self: 0.0000; time: 0.19s
Val loss: 0.2040 score: 0.8980 time: 0.07s
Test loss: 0.4498 score: 0.8776 time: 0.06s
     INFO: Early stopping counter 18 of 20
Epoch 120/1000, LR 0.000262
Train loss: 0.0012;  Loss pred: 0.0012; Loss self: 0.0000; time: 0.19s
Val loss: 0.2047 score: 0.8980 time: 0.07s
Test loss: 0.4523 score: 0.8776 time: 0.06s
     INFO: Early stopping counter 19 of 20
Epoch 121/1000, LR 0.000262
Train loss: 0.0017;  Loss pred: 0.0017; Loss self: 0.0000; time: 0.19s
Val loss: 0.2073 score: 0.8980 time: 0.07s
Test loss: 0.4548 score: 0.8776 time: 0.06s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 100,   Train_Loss: 0.0017,   Val_Loss: 0.1726,   Val_Precision: 0.9524,   Val_Recall: 0.8333,   Val_accuracy: 0.8889,   Val_Score: 0.8980,   Val_Loss: 0.1726,   Test_Precision: 0.9545,   Test_Recall: 0.8400,   Test_accuracy: 0.8936,   Test_Score: 0.8980,   Test_loss: 0.3492


[0.06526003195904195, 0.06605039292480797, 0.06869216298218817, 0.06916715996339917, 0.06997330498415977, 0.06864993600174785, 0.06959557591471821, 0.06890147796366364, 0.06849274109117687, 0.06924468197394162, 0.07177121995482594, 0.06843187590129673, 0.07378164597321302, 0.06819455907680094, 0.06814175203908235, 0.06809376692399383, 0.06959260499570519, 0.06475473300088197, 0.06540697696618736, 0.06569140905048698, 0.06628980406094342, 0.0652649860130623, 0.06563278404064476, 0.06523761700373143, 0.06926205905620009, 0.07682545599527657, 0.06806571804918349, 0.06397051399108022, 0.06326988607179374, 0.06474276189692318, 0.06400009198114276, 0.07105629006400704, 0.06415317999199033, 0.06412936002016068, 0.06479831109754741, 0.06483861699234694, 0.06390105898026377, 0.06378448498435318, 0.06396384094841778, 0.06887792097404599, 0.06454267096705735, 0.06392281199805439, 0.0642304290086031, 0.06378615810535848, 0.06412614800501615, 0.06470431899651885, 0.06407640699762851, 0.06557881704065949, 0.06420085509307683, 0.06468669592868537, 0.06552444503176957, 0.06367902201600373, 0.06478969496674836, 0.0642328760586679, 0.06441274599637836, 0.06385345000308007, 0.06435642403084785, 0.06400526396464556, 0.06377975596114993, 0.06449360703118145, 0.06475518306251615, 0.06409755500499159, 0.06425729801412672, 0.06400337698869407, 0.0641075229505077, 0.06623253307770938, 0.06440940708853304, 0.06819145393092185, 0.06651760498061776, 0.06423292600084096, 0.0644967999542132, 0.06389826792292297, 0.06446874991524965, 0.06402617401909083, 0.06370177504140884, 0.06433311803266406, 0.06377266894560307, 0.06394982698839158, 0.06410082196816802, 0.06389617698732764, 0.06494292104616761, 0.06404958805069327, 0.06479791598394513, 0.06343605602160096, 0.06361948896665126, 0.06418972101528198, 0.06360908201895654, 0.06366354902274907, 0.06340911402367055, 0.06411579600535333, 0.06364514108281583, 0.06383657606784254, 0.0672508260468021, 0.06586409802548587, 0.06398899399209768, 0.06436603097245097, 0.06646244204603136, 0.06402403302490711, 0.06363673298619688, 0.06418204098008573, 0.06447851797565818, 0.06456425797659904, 0.06447947199922055, 0.06363391096238047, 0.06393369496800005, 0.06380634603556246, 0.06382692803163081, 0.0666977729415521, 0.06616594200022519, 0.06395189603790641, 0.06397919589653611, 0.0646214570151642, 0.06451341696083546, 0.0637610909761861, 0.06393027305603027, 0.0640441729919985, 0.0645714009879157, 0.06389702798333019, 0.06400664593093097, 0.06411983806174248, 0.06452648702543229]
[0.0013318373869192234, 0.0013479672025471013, 0.0014018808771875135, 0.0014115746931305953, 0.0014280266323297912, 0.0014010191020764867, 0.0014203178758105757, 0.0014061526115033396, 0.001397811042677079, 0.0014131567749784005, 0.0014647187745882844, 0.0013965688959448313, 0.0015057478770043474, 0.0013917256954449173, 0.001390648000797599, 0.001389668712734568, 0.00142025724481031, 0.0013215251632833055, 0.0013348362646160685, 0.0013406410010303467, 0.0013528531441008862, 0.001331938490062496, 0.0013394445722580564, 0.001331379938851662, 0.0014135114093102058, 0.0015678664488831954, 0.0013890962867180304, 0.0013055206936955148, 0.0012912221647304843, 0.0013212808550392486, 0.0013061243261457706, 0.001450128368653205, 0.0013092485712651088, 0.0013087624493910341, 0.0013224145121948452, 0.0013232370814764683, 0.001304103244495179, 0.0013017241833541467, 0.0013053845091513833, 0.0014056718566131834, 0.0013171973666746396, 0.001304547183633763, 0.0013108250818082265, 0.0013017583286807854, 0.001308696898061554, 0.0013204963060514051, 0.0013076817754618063, 0.0013383432049114182, 0.001310221532511772, 0.0013201366516058237, 0.00133723357207693, 0.0012995718778776272, 0.0013222386727907828, 0.0013108750216054674, 0.0013145458366607828, 0.0013031316327159197, 0.0013133964087928133, 0.0013062298768295013, 0.0013016276726765291, 0.001316196061860846, 0.0013215343482146154, 0.001308113367448808, 0.001311373428859729, 0.0013061913671162054, 0.0013083167949083205, 0.0013516843485246813, 0.001314477695684348, 0.0013916623251208542, 0.001357502142461587, 0.001310876040833489, 0.0013162612235553715, 0.001304046284141285, 0.001315688773780605, 0.0013066566126345067, 0.0013000362253348743, 0.0013129207761768177, 0.0013014830397061852, 0.001305098509967175, 0.0013081800401666943, 0.0013040036119862783, 0.0013253657356360738, 0.0013071344500141485, 0.0013224064486519415, 0.001294613388195938, 0.0012983569176867604, 0.001309994306434326, 0.0012981445309991132, 0.001299256102505083, 0.0012940635515034807, 0.0013084856327623129, 0.0012988804302615474, 0.001302787266690664, 0.001372465837689839, 0.0013441652658262423, 0.001305897836573422, 0.00131359246882553, 0.0013563763682863542, 0.0013066129188756554, 0.0012987088364529976, 0.0013098375710221578, 0.0013158881219522077, 0.0013176379178897763, 0.0013159075918208276, 0.0012986512441302137, 0.0013047692850612256, 0.0013021703272563766, 0.0013025903679924657, 0.0013611790396235123, 0.001350325346943371, 0.0013051407354674777, 0.0013056978754395125, 0.0013188052452074326, 0.001316600346139499, 0.0013012467546160429, 0.0013046994501230667, 0.0013070239386122142, 0.0013177836936309325, 0.0013040209792516365, 0.001306258080223081, 0.0013085681237090304, 0.0013168670821516793]
[750.8424150137259, 741.8578123491528, 713.3273705867389, 708.4286824257216, 700.2670520006507, 713.7661424586389, 704.0677421801087, 711.1603618407283, 715.404278166816, 707.6355700274547, 682.7249143994133, 716.0405783801037, 664.1218063607556, 718.5323970614141, 719.0892299319851, 719.5959661725534, 704.0977989403324, 756.7014445003211, 749.1555530128066, 745.911843089576, 739.1785312105001, 750.7854209942375, 746.5781120858062, 751.1003965273183, 707.4580321130909, 637.8094261231935, 719.8925010178131, 765.9779004875958, 774.4600637402542, 756.8413605525942, 765.6238996412311, 689.5941225732602, 763.7968999528591, 764.0806018428317, 756.1925483865679, 755.7224733183866, 766.810453252957, 768.2118937233729, 766.0578113111587, 711.4035863315877, 759.1876702005353, 766.5495066376525, 762.8782923657085, 768.1917434040232, 764.1188738822589, 757.2910241530591, 764.712041388549, 747.1924961625876, 763.2297097750645, 757.4973384638573, 747.8125144935194, 769.4841793846234, 756.2931115071307, 762.8492293455027, 760.7190043218317, 767.3821852638566, 761.3847527717345, 765.5620329456967, 768.2688536758795, 759.7652272156125, 756.6961852720618, 764.4597363531906, 762.5592969879863, 765.5846035851459, 764.3408720974758, 739.817695670936, 760.7584390995518, 718.5651159401463, 736.6470878540782, 762.8486362174825, 759.727614932609, 766.8439473055209, 760.0581687160865, 765.3120110751825, 769.2093347186626, 761.6605800937718, 768.3542308978178, 766.2256851593152, 764.4207748901102, 766.8690414720437, 754.5087164337147, 765.0322428493687, 756.1971593676044, 772.4313753571745, 770.2042376619111, 763.3620963757471, 770.3302491521132, 769.6711972889022, 772.7595749359998, 764.2422468857559, 769.893807545192, 767.585027554189, 728.6155855676666, 743.956138001611, 765.7566863147006, 761.2711124128856, 737.2584950469168, 765.3376034736464, 769.9955308929568, 763.4534404289764, 759.943025031971, 758.9338363922618, 759.9317810882869, 770.0296784990657, 766.4190224657806, 767.9486923242691, 767.7010551991001, 734.6572132616657, 740.5622669111737, 766.2008952941142, 765.873958141648, 758.2620736716221, 759.5319285249876, 768.49375143692, 766.4600455726982, 765.0969278051562, 758.8498816863239, 766.8588281255177, 765.5455037102785, 764.194069748223, 759.3780826885443]
Elapsed: 0.06534697919937758~0.0022831446383004177
Time per graph: 0.001333611820395461~4.659478853674322e-05
Speed: 750.6972634785961~24.47413390296918
Total Time: 0.0651
best val loss: 0.17262998223304749 test_score: 0.8980

Testing...
Test loss: 0.4289 score: 0.8367 time: 0.06s
test Score 0.8367
Epoch Time List: [0.5299562689615414, 0.3249913399340585, 0.37782898696605116, 0.3470770039130002, 0.34465145389549434, 0.34934161708224565, 0.34411121904850006, 0.43708249798510224, 0.3410435769474134, 0.3544995959382504, 0.34550271602347493, 0.34416408312972635, 0.38285754807293415, 0.3390548031311482, 0.3352823421591893, 0.33720499195624143, 0.3436560840345919, 0.33218849590048194, 0.32713768805842847, 0.3251423460897058, 0.33710755105130374, 0.32861237903125584, 0.32182293292135, 0.3277770889690146, 0.33789514005184174, 0.37218618392944336, 0.33123205008450896, 0.32162401091773063, 0.31841617391910404, 0.3250441069249064, 0.32241208001505584, 0.34077863907441497, 0.3368464719969779, 0.3228285030927509, 0.3244704679818824, 0.342187233036384, 0.3232299428200349, 0.31645994482096285, 0.32015796413179487, 0.3257771269418299, 0.31765615998301655, 0.31771656998898834, 0.3176950308261439, 0.3218672600341961, 0.3152982610045001, 0.31674742489121854, 0.31809334398712963, 0.32901491585653275, 0.3227883198997006, 0.3204461318673566, 0.32826824102085084, 0.32055000599939376, 0.32247453404124826, 0.3249742879997939, 0.3287981868488714, 0.3178873991128057, 0.31931880803313106, 0.31712431099731475, 0.3266589851118624, 0.3313340420136228, 0.3325974551262334, 0.3239172729663551, 0.3170671248808503, 0.3164568069623783, 0.3161967920605093, 0.32265342690516263, 0.31798643979709595, 0.3396836328320205, 0.3280622478341684, 0.32233347196597606, 0.32231633295305073, 0.3174902191385627, 0.3215484629618004, 0.3219075670931488, 0.3165254320483655, 0.31825117801781744, 0.3177497610449791, 0.31757571490015835, 0.31837307405658066, 0.31745706300716847, 0.3253294990863651, 0.33180368575267494, 0.3160199917620048, 0.31606739899143577, 0.31501499889418483, 0.3220859010471031, 0.32210135902278125, 0.3156146449036896, 0.31666590610984713, 0.3157113790512085, 0.31477047596126795, 0.3217880839947611, 0.3187277818797156, 0.3349200049415231, 0.32071113493293524, 0.3166853829752654, 0.3267035789322108, 0.32088508002925664, 0.315594902029261, 0.3227737529668957, 0.3202934470027685, 0.327745946822688, 0.3219149369979277, 0.31667311710771173, 0.3163306890055537, 0.3169822500785813, 0.31869700900278986, 0.328618983970955, 0.33202553703449667, 0.32429751206655055, 0.32168181403540075, 0.3251975899329409, 0.31993843999225646, 0.3155595859279856, 0.3199309560004622, 0.3150963799562305, 0.3194340330082923, 0.31632508779875934, 0.31520721409469843, 0.3188969721086323, 0.31701952894218266]
Total Epoch List: [121]
Total Time List: [0.06507592007983476]
========================k_idx:1========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: GPSTransformer
GPS(
  (x_lin): Linear(in_features=14887, out_features=64, bias=True)
  (pe_lin): Linear(in_features=20, out_features=8, bias=True)
  (pe_norm): BatchNorm1d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (convs): ModuleList(
    (0-1): 2 x GPSConv(64, conv=GINEConv(nn=Sequential(
      (0): Linear(in_features=64, out_features=64, bias=True)
      (1): ReLU()
      (2): Linear(in_features=64, out_features=64, bias=True)
    )), heads=1, attn_type=multihead)
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=32, bias=True)
    (1): ReLU()
    (2): Linear(in_features=32, out_features=16, bias=True)
    (3): ReLU()
    (4): Linear(in_features=16, out_features=2, bias=True)
  )
)
Total number of parameters: 1039906
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7d29a7d8b340>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 1.7508;  Loss pred: 1.7508; Loss self: 0.0000; time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 2.7734 score: 0.4898 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 4.0300 score: 0.5102 time: 0.07s
Epoch 2/1000, LR 0.000000
Train loss: 1.7771;  Loss pred: 1.7771; Loss self: 0.0000; time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 2.2430 score: 0.4898 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 3.2174 score: 0.5102 time: 0.07s
Epoch 3/1000, LR 0.000030
Train loss: 1.6900;  Loss pred: 1.6900; Loss self: 0.0000; time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 1.8786 score: 0.4898 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 2.5797 score: 0.5102 time: 0.07s
Epoch 4/1000, LR 0.000060
Train loss: 1.4925;  Loss pred: 1.4925; Loss self: 0.0000; time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 1.6382 score: 0.4898 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 2.1606 score: 0.5102 time: 0.07s
Epoch 5/1000, LR 0.000090
Train loss: 1.1740;  Loss pred: 1.1740; Loss self: 0.0000; time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 1.4961 score: 0.4898 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 1.9222 score: 0.5102 time: 0.07s
Epoch 6/1000, LR 0.000120
Train loss: 0.8408;  Loss pred: 0.8408; Loss self: 0.0000; time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 1.4337 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 1.7565 score: 0.5102 time: 0.07s
Epoch 7/1000, LR 0.000150
Train loss: 0.6326;  Loss pred: 0.6326; Loss self: 0.0000; time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 1.4241 score: 0.4898 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 1.6694 score: 0.5102 time: 0.07s
Epoch 8/1000, LR 0.000180
Train loss: 0.4557;  Loss pred: 0.4557; Loss self: 0.0000; time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 1.4386 score: 0.4898 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 1.6598 score: 0.5102 time: 0.07s
     INFO: Early stopping counter 1 of 20
Epoch 9/1000, LR 0.000210
Train loss: 0.3403;  Loss pred: 0.3403; Loss self: 0.0000; time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 1.4666 score: 0.4898 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 1.6820 score: 0.5102 time: 0.07s
     INFO: Early stopping counter 2 of 20
Epoch 10/1000, LR 0.000240
Train loss: 0.2691;  Loss pred: 0.2691; Loss self: 0.0000; time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 1.4922 score: 0.4898 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 1.7117 score: 0.5102 time: 0.07s
     INFO: Early stopping counter 3 of 20
Epoch 11/1000, LR 0.000270
Train loss: 0.2274;  Loss pred: 0.2274; Loss self: 0.0000; time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 1.5006 score: 0.4898 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 1.7136 score: 0.5102 time: 0.07s
     INFO: Early stopping counter 4 of 20
Epoch 12/1000, LR 0.000270
Train loss: 0.1958;  Loss pred: 0.1958; Loss self: 0.0000; time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 1.5080 score: 0.4898 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 1.7170 score: 0.5102 time: 0.07s
     INFO: Early stopping counter 5 of 20
Epoch 13/1000, LR 0.000270
Train loss: 0.1737;  Loss pred: 0.1737; Loss self: 0.0000; time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 1.5063 score: 0.4898 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 1.7116 score: 0.5102 time: 0.07s
     INFO: Early stopping counter 6 of 20
Epoch 14/1000, LR 0.000270
Train loss: 0.1549;  Loss pred: 0.1549; Loss self: 0.0000; time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 1.4937 score: 0.4898 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 1.6954 score: 0.5102 time: 0.07s
     INFO: Early stopping counter 7 of 20
Epoch 15/1000, LR 0.000270
Train loss: 0.1377;  Loss pred: 0.1377; Loss self: 0.0000; time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 1.4707 score: 0.4898 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 1.6684 score: 0.5102 time: 0.07s
     INFO: Early stopping counter 8 of 20
Epoch 16/1000, LR 0.000270
Train loss: 0.1184;  Loss pred: 0.1184; Loss self: 0.0000; time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 1.4451 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 1.6381 score: 0.5102 time: 0.07s
     INFO: Early stopping counter 9 of 20
Epoch 17/1000, LR 0.000270
Train loss: 0.1119;  Loss pred: 0.1119; Loss self: 0.0000; time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 1.4119 score: 0.4898 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 1.5938 score: 0.5102 time: 0.07s
Epoch 18/1000, LR 0.000270
Train loss: 0.1056;  Loss pred: 0.1056; Loss self: 0.0000; time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 1.3884 score: 0.4898 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 1.5606 score: 0.5102 time: 0.07s
Epoch 19/1000, LR 0.000270
Train loss: 0.0917;  Loss pred: 0.0917; Loss self: 0.0000; time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 1.3711 score: 0.4898 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 1.5375 score: 0.5102 time: 0.07s
Epoch 20/1000, LR 0.000270
Train loss: 0.0860;  Loss pred: 0.0860; Loss self: 0.0000; time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 1.3603 score: 0.4898 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 1.5280 score: 0.5102 time: 0.07s
Epoch 21/1000, LR 0.000270
Train loss: 0.0764;  Loss pred: 0.0764; Loss self: 0.0000; time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 1.3498 score: 0.4898 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 1.5249 score: 0.5102 time: 0.07s
Epoch 22/1000, LR 0.000270
Train loss: 0.0703;  Loss pred: 0.0703; Loss self: 0.0000; time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 1.3356 score: 0.4898 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 1.5223 score: 0.5102 time: 0.07s
Epoch 23/1000, LR 0.000270
Train loss: 0.0637;  Loss pred: 0.0637; Loss self: 0.0000; time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 1.3118 score: 0.4898 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 1.5077 score: 0.5102 time: 0.07s
Epoch 24/1000, LR 0.000270
Train loss: 0.0602;  Loss pred: 0.0602; Loss self: 0.0000; time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 1.2826 score: 0.4898 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 1.4814 score: 0.5102 time: 0.07s
Epoch 25/1000, LR 0.000270
Train loss: 0.0544;  Loss pred: 0.0544; Loss self: 0.0000; time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 1.2480 score: 0.4898 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 1.4460 score: 0.5102 time: 0.07s
Epoch 26/1000, LR 0.000270
Train loss: 0.0519;  Loss pred: 0.0519; Loss self: 0.0000; time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 1.2069 score: 0.4898 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 1.4005 score: 0.5102 time: 0.07s
Epoch 27/1000, LR 0.000270
Train loss: 0.0469;  Loss pred: 0.0469; Loss self: 0.0000; time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 1.1570 score: 0.4898 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 1.3468 score: 0.5102 time: 0.07s
Epoch 28/1000, LR 0.000270
Train loss: 0.0439;  Loss pred: 0.0439; Loss self: 0.0000; time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 1.1095 score: 0.4898 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 1.2951 score: 0.5102 time: 0.07s
Epoch 29/1000, LR 0.000270
Train loss: 0.0419;  Loss pred: 0.0419; Loss self: 0.0000; time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 1.0651 score: 0.4898 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 1.2493 score: 0.5102 time: 0.07s
Epoch 30/1000, LR 0.000270
Train loss: 0.0381;  Loss pred: 0.0381; Loss self: 0.0000; time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 1.0226 score: 0.4898 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 1.2040 score: 0.5102 time: 0.07s
Epoch 31/1000, LR 0.000270
Train loss: 0.0348;  Loss pred: 0.0348; Loss self: 0.0000; time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.9809 score: 0.4898 time: 0.06s
Test loss: 1.1601 score: 0.5306 time: 0.07s
Epoch 32/1000, LR 0.000270
Train loss: 0.0315;  Loss pred: 0.0315; Loss self: 0.0000; time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.9442 score: 0.4898 time: 0.06s
Test loss: 1.1194 score: 0.5306 time: 0.07s
Epoch 33/1000, LR 0.000270
Train loss: 0.0293;  Loss pred: 0.0293; Loss self: 0.0000; time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.9134 score: 0.4898 time: 0.06s
Test loss: 1.0833 score: 0.5306 time: 0.07s
Epoch 34/1000, LR 0.000270
Train loss: 0.0270;  Loss pred: 0.0270; Loss self: 0.0000; time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.8819 score: 0.4898 time: 0.06s
Test loss: 1.0445 score: 0.5306 time: 0.07s
Epoch 35/1000, LR 0.000270
Train loss: 0.0258;  Loss pred: 0.0258; Loss self: 0.0000; time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.8524 score: 0.4898 time: 0.06s
Test loss: 1.0086 score: 0.5306 time: 0.07s
Epoch 36/1000, LR 0.000270
Train loss: 0.0234;  Loss pred: 0.0234; Loss self: 0.0000; time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.8285 score: 0.4898 time: 0.06s
Test loss: 0.9792 score: 0.5306 time: 0.07s
Epoch 37/1000, LR 0.000270
Train loss: 0.0219;  Loss pred: 0.0219; Loss self: 0.0000; time: 0.20s
Val loss: 0.8083 score: 0.4694 time: 0.06s
Test loss: 0.9545 score: 0.5306 time: 0.08s
Epoch 38/1000, LR 0.000270
Train loss: 0.0196;  Loss pred: 0.0196; Loss self: 0.0000; time: 0.19s
Val loss: 0.7914 score: 0.4694 time: 0.06s
Test loss: 0.9342 score: 0.5306 time: 0.07s
Epoch 39/1000, LR 0.000269
Train loss: 0.0194;  Loss pred: 0.0194; Loss self: 0.0000; time: 0.19s
Val loss: 0.7790 score: 0.5102 time: 0.06s
Test loss: 0.9188 score: 0.5306 time: 0.07s
Epoch 40/1000, LR 0.000269
Train loss: 0.0178;  Loss pred: 0.0178; Loss self: 0.0000; time: 0.19s
Val loss: 0.7683 score: 0.5102 time: 0.06s
Test loss: 0.9014 score: 0.5306 time: 0.07s
Epoch 41/1000, LR 0.000269
Train loss: 0.0182;  Loss pred: 0.0182; Loss self: 0.0000; time: 0.19s
Val loss: 0.7515 score: 0.5102 time: 0.06s
Test loss: 0.8741 score: 0.5306 time: 0.07s
Epoch 42/1000, LR 0.000269
Train loss: 0.0165;  Loss pred: 0.0165; Loss self: 0.0000; time: 0.20s
Val loss: 0.7284 score: 0.5510 time: 0.07s
Test loss: 0.8385 score: 0.5306 time: 0.07s
Epoch 43/1000, LR 0.000269
Train loss: 0.0150;  Loss pred: 0.0150; Loss self: 0.0000; time: 0.20s
Val loss: 0.7037 score: 0.5510 time: 0.07s
Test loss: 0.8007 score: 0.5306 time: 0.08s
Epoch 44/1000, LR 0.000269
Train loss: 0.0138;  Loss pred: 0.0138; Loss self: 0.0000; time: 0.20s
Val loss: 0.6790 score: 0.5510 time: 0.06s
Test loss: 0.7591 score: 0.5510 time: 0.07s
Epoch 45/1000, LR 0.000269
Train loss: 0.0130;  Loss pred: 0.0130; Loss self: 0.0000; time: 0.20s
Val loss: 0.6587 score: 0.5918 time: 0.06s
Test loss: 0.7221 score: 0.5510 time: 0.07s
Epoch 46/1000, LR 0.000269
Train loss: 0.0125;  Loss pred: 0.0125; Loss self: 0.0000; time: 0.20s
Val loss: 0.6416 score: 0.5918 time: 0.07s
Test loss: 0.6896 score: 0.6122 time: 0.08s
Epoch 47/1000, LR 0.000269
Train loss: 0.0116;  Loss pred: 0.0116; Loss self: 0.0000; time: 0.20s
Val loss: 0.6289 score: 0.6122 time: 0.07s
Test loss: 0.6658 score: 0.6327 time: 0.07s
Epoch 48/1000, LR 0.000269
Train loss: 0.0111;  Loss pred: 0.0111; Loss self: 0.0000; time: 0.19s
Val loss: 0.6210 score: 0.6327 time: 0.06s
Test loss: 0.6513 score: 0.6327 time: 0.07s
Epoch 49/1000, LR 0.000269
Train loss: 0.0102;  Loss pred: 0.0102; Loss self: 0.0000; time: 0.28s
Val loss: 0.6160 score: 0.6327 time: 0.06s
Test loss: 0.6425 score: 0.6531 time: 0.07s
Epoch 50/1000, LR 0.000269
Train loss: 0.0098;  Loss pred: 0.0098; Loss self: 0.0000; time: 0.20s
Val loss: 0.6135 score: 0.6531 time: 0.06s
Test loss: 0.6404 score: 0.6531 time: 0.07s
Epoch 51/1000, LR 0.000269
Train loss: 0.0095;  Loss pred: 0.0095; Loss self: 0.0000; time: 0.19s
Val loss: 0.6147 score: 0.6531 time: 0.06s
Test loss: 0.6446 score: 0.6531 time: 0.07s
     INFO: Early stopping counter 1 of 20
Epoch 52/1000, LR 0.000269
Train loss: 0.0084;  Loss pred: 0.0084; Loss self: 0.0000; time: 0.19s
Val loss: 0.6186 score: 0.6531 time: 0.06s
Test loss: 0.6498 score: 0.6531 time: 0.07s
     INFO: Early stopping counter 2 of 20
Epoch 53/1000, LR 0.000269
Train loss: 0.0077;  Loss pred: 0.0077; Loss self: 0.0000; time: 0.20s
Val loss: 0.6214 score: 0.6531 time: 0.06s
Test loss: 0.6545 score: 0.6531 time: 0.07s
     INFO: Early stopping counter 3 of 20
Epoch 54/1000, LR 0.000269
Train loss: 0.0075;  Loss pred: 0.0075; Loss self: 0.0000; time: 0.21s
Val loss: 0.6193 score: 0.6531 time: 0.10s
Test loss: 0.6535 score: 0.6531 time: 0.07s
     INFO: Early stopping counter 4 of 20
Epoch 55/1000, LR 0.000269
Train loss: 0.0075;  Loss pred: 0.0075; Loss self: 0.0000; time: 0.19s
Val loss: 0.6049 score: 0.6531 time: 0.06s
Test loss: 0.6336 score: 0.6531 time: 0.07s
Epoch 56/1000, LR 0.000269
Train loss: 0.0072;  Loss pred: 0.0072; Loss self: 0.0000; time: 0.19s
Val loss: 0.5854 score: 0.6531 time: 0.06s
Test loss: 0.6039 score: 0.6531 time: 0.07s
Epoch 57/1000, LR 0.000269
Train loss: 0.0069;  Loss pred: 0.0069; Loss self: 0.0000; time: 0.20s
Val loss: 0.5646 score: 0.6735 time: 0.06s
Test loss: 0.5704 score: 0.6531 time: 0.07s
Epoch 58/1000, LR 0.000269
Train loss: 0.0066;  Loss pred: 0.0066; Loss self: 0.0000; time: 0.20s
Val loss: 0.5466 score: 0.6735 time: 0.06s
Test loss: 0.5396 score: 0.6939 time: 0.07s
Epoch 59/1000, LR 0.000268
Train loss: 0.0062;  Loss pred: 0.0062; Loss self: 0.0000; time: 0.21s
Val loss: 0.5302 score: 0.7143 time: 0.08s
Test loss: 0.5098 score: 0.7143 time: 0.07s
Epoch 60/1000, LR 0.000268
Train loss: 0.0059;  Loss pred: 0.0059; Loss self: 0.0000; time: 0.20s
Val loss: 0.5164 score: 0.7143 time: 0.06s
Test loss: 0.4826 score: 0.7755 time: 0.08s
Epoch 61/1000, LR 0.000268
Train loss: 0.0060;  Loss pred: 0.0060; Loss self: 0.0000; time: 0.20s
Val loss: 0.5048 score: 0.7143 time: 0.07s
Test loss: 0.4585 score: 0.7959 time: 0.07s
Epoch 62/1000, LR 0.000268
Train loss: 0.0059;  Loss pred: 0.0059; Loss self: 0.0000; time: 0.20s
Val loss: 0.4956 score: 0.7347 time: 0.06s
Test loss: 0.4395 score: 0.8367 time: 0.07s
Epoch 63/1000, LR 0.000268
Train loss: 0.0055;  Loss pred: 0.0055; Loss self: 0.0000; time: 0.20s
Val loss: 0.4878 score: 0.7755 time: 0.06s
Test loss: 0.4223 score: 0.8367 time: 0.07s
Epoch 64/1000, LR 0.000268
Train loss: 0.0050;  Loss pred: 0.0050; Loss self: 0.0000; time: 0.21s
Val loss: 0.4805 score: 0.7551 time: 0.11s
Test loss: 0.4069 score: 0.8571 time: 0.07s
Epoch 65/1000, LR 0.000268
Train loss: 0.0049;  Loss pred: 0.0049; Loss self: 0.0000; time: 0.20s
Val loss: 0.4743 score: 0.7755 time: 0.06s
Test loss: 0.3945 score: 0.8571 time: 0.07s
Epoch 66/1000, LR 0.000268
Train loss: 0.0046;  Loss pred: 0.0046; Loss self: 0.0000; time: 0.20s
Val loss: 0.4689 score: 0.7755 time: 0.06s
Test loss: 0.3837 score: 0.8571 time: 0.07s
Epoch 67/1000, LR 0.000268
Train loss: 0.0043;  Loss pred: 0.0043; Loss self: 0.0000; time: 0.20s
Val loss: 0.4629 score: 0.7755 time: 0.07s
Test loss: 0.3734 score: 0.8571 time: 0.07s
Epoch 68/1000, LR 0.000268
Train loss: 0.0045;  Loss pred: 0.0045; Loss self: 0.0000; time: 0.20s
Val loss: 0.4562 score: 0.7959 time: 0.06s
Test loss: 0.3622 score: 0.8571 time: 0.07s
Epoch 69/1000, LR 0.000268
Train loss: 0.0041;  Loss pred: 0.0041; Loss self: 0.0000; time: 0.19s
Val loss: 0.4492 score: 0.7959 time: 0.06s
Test loss: 0.3491 score: 0.8367 time: 0.07s
Epoch 70/1000, LR 0.000268
Train loss: 0.0042;  Loss pred: 0.0042; Loss self: 0.0000; time: 0.19s
Val loss: 0.4399 score: 0.7959 time: 0.06s
Test loss: 0.3329 score: 0.8571 time: 0.07s
Epoch 71/1000, LR 0.000268
Train loss: 0.0038;  Loss pred: 0.0038; Loss self: 0.0000; time: 0.20s
Val loss: 0.4299 score: 0.7959 time: 0.07s
Test loss: 0.3163 score: 0.8571 time: 0.07s
Epoch 72/1000, LR 0.000267
Train loss: 0.0036;  Loss pred: 0.0036; Loss self: 0.0000; time: 0.20s
Val loss: 0.4216 score: 0.7959 time: 0.07s
Test loss: 0.3014 score: 0.8776 time: 0.07s
Epoch 73/1000, LR 0.000267
Train loss: 0.0036;  Loss pred: 0.0036; Loss self: 0.0000; time: 0.20s
Val loss: 0.4167 score: 0.8163 time: 0.07s
Test loss: 0.2904 score: 0.8776 time: 0.07s
Epoch 74/1000, LR 0.000267
Train loss: 0.0037;  Loss pred: 0.0037; Loss self: 0.0000; time: 0.20s
Val loss: 0.4175 score: 0.8163 time: 0.06s
Test loss: 0.2866 score: 0.8571 time: 0.07s
     INFO: Early stopping counter 1 of 20
Epoch 75/1000, LR 0.000267
Train loss: 0.0035;  Loss pred: 0.0035; Loss self: 0.0000; time: 0.19s
Val loss: 0.4202 score: 0.8163 time: 0.06s
Test loss: 0.2855 score: 0.8571 time: 0.07s
     INFO: Early stopping counter 2 of 20
Epoch 76/1000, LR 0.000267
Train loss: 0.0035;  Loss pred: 0.0035; Loss self: 0.0000; time: 0.20s
Val loss: 0.4249 score: 0.7959 time: 0.06s
Test loss: 0.2860 score: 0.8571 time: 0.07s
     INFO: Early stopping counter 3 of 20
Epoch 77/1000, LR 0.000267
Train loss: 0.0035;  Loss pred: 0.0035; Loss self: 0.0000; time: 0.20s
Val loss: 0.4325 score: 0.7755 time: 0.06s
Test loss: 0.2925 score: 0.8571 time: 0.07s
     INFO: Early stopping counter 4 of 20
Epoch 78/1000, LR 0.000267
Train loss: 0.0033;  Loss pred: 0.0033; Loss self: 0.0000; time: 0.20s
Val loss: 0.4400 score: 0.7755 time: 0.06s
Test loss: 0.2952 score: 0.8571 time: 0.07s
     INFO: Early stopping counter 5 of 20
Epoch 79/1000, LR 0.000267
Train loss: 0.0031;  Loss pred: 0.0031; Loss self: 0.0000; time: 0.20s
Val loss: 0.4472 score: 0.7755 time: 0.06s
Test loss: 0.2958 score: 0.8571 time: 0.07s
     INFO: Early stopping counter 6 of 20
Epoch 80/1000, LR 0.000267
Train loss: 0.0029;  Loss pred: 0.0029; Loss self: 0.0000; time: 0.20s
Val loss: 0.4542 score: 0.7755 time: 0.06s
Test loss: 0.2937 score: 0.8571 time: 0.07s
     INFO: Early stopping counter 7 of 20
Epoch 81/1000, LR 0.000267
Train loss: 0.0028;  Loss pred: 0.0028; Loss self: 0.0000; time: 0.20s
Val loss: 0.4632 score: 0.7755 time: 0.06s
Test loss: 0.2902 score: 0.8571 time: 0.07s
     INFO: Early stopping counter 8 of 20
Epoch 82/1000, LR 0.000267
Train loss: 0.0030;  Loss pred: 0.0030; Loss self: 0.0000; time: 0.20s
Val loss: 0.4736 score: 0.7551 time: 0.06s
Test loss: 0.2859 score: 0.8571 time: 0.07s
     INFO: Early stopping counter 9 of 20
Epoch 83/1000, LR 0.000266
Train loss: 0.0028;  Loss pred: 0.0028; Loss self: 0.0000; time: 0.20s
Val loss: 0.4826 score: 0.7551 time: 0.06s
Test loss: 0.2798 score: 0.8571 time: 0.07s
     INFO: Early stopping counter 10 of 20
Epoch 84/1000, LR 0.000266
Train loss: 0.0025;  Loss pred: 0.0025; Loss self: 0.0000; time: 0.20s
Val loss: 0.4890 score: 0.7551 time: 0.07s
Test loss: 0.2722 score: 0.8571 time: 0.07s
     INFO: Early stopping counter 11 of 20
Epoch 85/1000, LR 0.000266
Train loss: 0.0026;  Loss pred: 0.0026; Loss self: 0.0000; time: 0.20s
Val loss: 0.4932 score: 0.7551 time: 0.06s
Test loss: 0.2648 score: 0.8571 time: 0.07s
     INFO: Early stopping counter 12 of 20
Epoch 86/1000, LR 0.000266
Train loss: 0.0026;  Loss pred: 0.0026; Loss self: 0.0000; time: 0.20s
Val loss: 0.4940 score: 0.7551 time: 0.06s
Test loss: 0.2546 score: 0.8571 time: 0.07s
     INFO: Early stopping counter 13 of 20
Epoch 87/1000, LR 0.000266
Train loss: 0.0025;  Loss pred: 0.0025; Loss self: 0.0000; time: 0.20s
Val loss: 0.4910 score: 0.7959 time: 0.06s
Test loss: 0.2423 score: 0.8571 time: 0.07s
     INFO: Early stopping counter 14 of 20
Epoch 88/1000, LR 0.000266
Train loss: 0.0025;  Loss pred: 0.0025; Loss self: 0.0000; time: 0.20s
Val loss: 0.4867 score: 0.7959 time: 0.06s
Test loss: 0.2305 score: 0.8571 time: 0.07s
     INFO: Early stopping counter 15 of 20
Epoch 89/1000, LR 0.000266
Train loss: 0.0023;  Loss pred: 0.0023; Loss self: 0.0000; time: 0.20s
Val loss: 0.4827 score: 0.8163 time: 0.06s
Test loss: 0.2208 score: 0.8776 time: 0.07s
     INFO: Early stopping counter 16 of 20
Epoch 90/1000, LR 0.000266
Train loss: 0.0024;  Loss pred: 0.0024; Loss self: 0.0000; time: 0.20s
Val loss: 0.4792 score: 0.8163 time: 0.06s
Test loss: 0.2119 score: 0.8776 time: 0.07s
     INFO: Early stopping counter 17 of 20
Epoch 91/1000, LR 0.000266
Train loss: 0.0025;  Loss pred: 0.0025; Loss self: 0.0000; time: 0.21s
Val loss: 0.4790 score: 0.8163 time: 0.06s
Test loss: 0.2044 score: 0.8776 time: 0.07s
     INFO: Early stopping counter 18 of 20
Epoch 92/1000, LR 0.000266
Train loss: 0.0021;  Loss pred: 0.0021; Loss self: 0.0000; time: 0.20s
Val loss: 0.4852 score: 0.8163 time: 0.06s
Test loss: 0.2007 score: 0.8776 time: 0.07s
     INFO: Early stopping counter 19 of 20
Epoch 93/1000, LR 0.000265
Train loss: 0.0022;  Loss pred: 0.0022; Loss self: 0.0000; time: 0.20s
Val loss: 0.5040 score: 0.8163 time: 0.06s
Test loss: 0.2070 score: 0.8776 time: 0.07s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 072,   Train_Loss: 0.0036,   Val_Loss: 0.4167,   Val_Precision: 1.0000,   Val_Recall: 0.6400,   Val_accuracy: 0.7805,   Val_Score: 0.8163,   Val_Loss: 0.4167,   Test_Precision: 1.0000,   Test_Recall: 0.7500,   Test_accuracy: 0.8571,   Test_Score: 0.8776,   Test_loss: 0.2904


[0.06526003195904195, 0.06605039292480797, 0.06869216298218817, 0.06916715996339917, 0.06997330498415977, 0.06864993600174785, 0.06959557591471821, 0.06890147796366364, 0.06849274109117687, 0.06924468197394162, 0.07177121995482594, 0.06843187590129673, 0.07378164597321302, 0.06819455907680094, 0.06814175203908235, 0.06809376692399383, 0.06959260499570519, 0.06475473300088197, 0.06540697696618736, 0.06569140905048698, 0.06628980406094342, 0.0652649860130623, 0.06563278404064476, 0.06523761700373143, 0.06926205905620009, 0.07682545599527657, 0.06806571804918349, 0.06397051399108022, 0.06326988607179374, 0.06474276189692318, 0.06400009198114276, 0.07105629006400704, 0.06415317999199033, 0.06412936002016068, 0.06479831109754741, 0.06483861699234694, 0.06390105898026377, 0.06378448498435318, 0.06396384094841778, 0.06887792097404599, 0.06454267096705735, 0.06392281199805439, 0.0642304290086031, 0.06378615810535848, 0.06412614800501615, 0.06470431899651885, 0.06407640699762851, 0.06557881704065949, 0.06420085509307683, 0.06468669592868537, 0.06552444503176957, 0.06367902201600373, 0.06478969496674836, 0.0642328760586679, 0.06441274599637836, 0.06385345000308007, 0.06435642403084785, 0.06400526396464556, 0.06377975596114993, 0.06449360703118145, 0.06475518306251615, 0.06409755500499159, 0.06425729801412672, 0.06400337698869407, 0.0641075229505077, 0.06623253307770938, 0.06440940708853304, 0.06819145393092185, 0.06651760498061776, 0.06423292600084096, 0.0644967999542132, 0.06389826792292297, 0.06446874991524965, 0.06402617401909083, 0.06370177504140884, 0.06433311803266406, 0.06377266894560307, 0.06394982698839158, 0.06410082196816802, 0.06389617698732764, 0.06494292104616761, 0.06404958805069327, 0.06479791598394513, 0.06343605602160096, 0.06361948896665126, 0.06418972101528198, 0.06360908201895654, 0.06366354902274907, 0.06340911402367055, 0.06411579600535333, 0.06364514108281583, 0.06383657606784254, 0.0672508260468021, 0.06586409802548587, 0.06398899399209768, 0.06436603097245097, 0.06646244204603136, 0.06402403302490711, 0.06363673298619688, 0.06418204098008573, 0.06447851797565818, 0.06456425797659904, 0.06447947199922055, 0.06363391096238047, 0.06393369496800005, 0.06380634603556246, 0.06382692803163081, 0.0666977729415521, 0.06616594200022519, 0.06395189603790641, 0.06397919589653611, 0.0646214570151642, 0.06451341696083546, 0.0637610909761861, 0.06393027305603027, 0.0640441729919985, 0.0645714009879157, 0.06389702798333019, 0.06400664593093097, 0.06411983806174248, 0.06452648702543229, 0.07800177403260022, 0.0763482719194144, 0.07226999395061284, 0.07147625298239291, 0.07272468495648354, 0.07717971596866846, 0.07627643703017384, 0.07227165997028351, 0.07669151795562357, 0.07653785997536033, 0.07200285303406417, 0.07761058595497161, 0.07801096909679472, 0.07241131807677448, 0.07319266197737306, 0.07794940297026187, 0.07799758890178055, 0.07668416399974376, 0.07224032597150654, 0.07620836200658232, 0.07662852690555155, 0.07682703004684299, 0.07237323804292828, 0.0774499389808625, 0.07666033098939806, 0.07738262903876603, 0.07641497906297445, 0.07679780200123787, 0.07771749305538833, 0.07734368706587702, 0.07813117606565356, 0.07650941202882677, 0.07639452000148594, 0.0725528069306165, 0.07267519598826766, 0.07242659898474813, 0.0816383350174874, 0.07643764000386, 0.07680968998465687, 0.07716069789603353, 0.0782269990304485, 0.07735941605642438, 0.08023183804471046, 0.07790170097723603, 0.07316735899075866, 0.08244828297756612, 0.07833534700330347, 0.07874312298372388, 0.07236551097594202, 0.07620961999054998, 0.0767533250618726, 0.07430440606549382, 0.07772669405676425, 0.07762148696929216, 0.07727172307204455, 0.073572758003138, 0.07719579606782645, 0.07346608699299395, 0.07814056403003633, 0.08256699808407575, 0.0777674299897626, 0.0774228140944615, 0.07810108596459031, 0.07250015600584447, 0.07213486602995545, 0.07759541308041662, 0.07722828292753547, 0.07671896810643375, 0.07678411202505231, 0.07328244100790471, 0.07357378699816763, 0.0771035939687863, 0.07258240389637649, 0.0770446399692446, 0.07806718302890658, 0.07265768700744957, 0.07680892699863762, 0.07315849396400154, 0.07624523900449276, 0.07637520704884082, 0.0768418509978801, 0.07257383491378278, 0.0714568339753896, 0.07220235106069595, 0.07611995097249746, 0.07615725195501, 0.07850574294570833, 0.07815781107638031, 0.07874803396407515, 0.07774263701867312, 0.0767221839632839, 0.07595663494430482, 0.07221010199282318]
[0.0013318373869192234, 0.0013479672025471013, 0.0014018808771875135, 0.0014115746931305953, 0.0014280266323297912, 0.0014010191020764867, 0.0014203178758105757, 0.0014061526115033396, 0.001397811042677079, 0.0014131567749784005, 0.0014647187745882844, 0.0013965688959448313, 0.0015057478770043474, 0.0013917256954449173, 0.001390648000797599, 0.001389668712734568, 0.00142025724481031, 0.0013215251632833055, 0.0013348362646160685, 0.0013406410010303467, 0.0013528531441008862, 0.001331938490062496, 0.0013394445722580564, 0.001331379938851662, 0.0014135114093102058, 0.0015678664488831954, 0.0013890962867180304, 0.0013055206936955148, 0.0012912221647304843, 0.0013212808550392486, 0.0013061243261457706, 0.001450128368653205, 0.0013092485712651088, 0.0013087624493910341, 0.0013224145121948452, 0.0013232370814764683, 0.001304103244495179, 0.0013017241833541467, 0.0013053845091513833, 0.0014056718566131834, 0.0013171973666746396, 0.001304547183633763, 0.0013108250818082265, 0.0013017583286807854, 0.001308696898061554, 0.0013204963060514051, 0.0013076817754618063, 0.0013383432049114182, 0.001310221532511772, 0.0013201366516058237, 0.00133723357207693, 0.0012995718778776272, 0.0013222386727907828, 0.0013108750216054674, 0.0013145458366607828, 0.0013031316327159197, 0.0013133964087928133, 0.0013062298768295013, 0.0013016276726765291, 0.001316196061860846, 0.0013215343482146154, 0.001308113367448808, 0.001311373428859729, 0.0013061913671162054, 0.0013083167949083205, 0.0013516843485246813, 0.001314477695684348, 0.0013916623251208542, 0.001357502142461587, 0.001310876040833489, 0.0013162612235553715, 0.001304046284141285, 0.001315688773780605, 0.0013066566126345067, 0.0013000362253348743, 0.0013129207761768177, 0.0013014830397061852, 0.001305098509967175, 0.0013081800401666943, 0.0013040036119862783, 0.0013253657356360738, 0.0013071344500141485, 0.0013224064486519415, 0.001294613388195938, 0.0012983569176867604, 0.001309994306434326, 0.0012981445309991132, 0.001299256102505083, 0.0012940635515034807, 0.0013084856327623129, 0.0012988804302615474, 0.001302787266690664, 0.001372465837689839, 0.0013441652658262423, 0.001305897836573422, 0.00131359246882553, 0.0013563763682863542, 0.0013066129188756554, 0.0012987088364529976, 0.0013098375710221578, 0.0013158881219522077, 0.0013176379178897763, 0.0013159075918208276, 0.0012986512441302137, 0.0013047692850612256, 0.0013021703272563766, 0.0013025903679924657, 0.0013611790396235123, 0.001350325346943371, 0.0013051407354674777, 0.0013056978754395125, 0.0013188052452074326, 0.001316600346139499, 0.0013012467546160429, 0.0013046994501230667, 0.0013070239386122142, 0.0013177836936309325, 0.0013040209792516365, 0.001306258080223081, 0.0013085681237090304, 0.0013168670821516793, 0.001591872939440821, 0.001558127998355396, 0.0014748978357267926, 0.0014586990404569981, 0.0014841772440098682, 0.00157509624425854, 0.0015566619802076295, 0.0014749318361282349, 0.0015651330195025218, 0.0015619971423542925, 0.0014694459802870239, 0.001583889509285135, 0.0015920605938121372, 0.001477782001566826, 0.0014937277954565932, 0.0015908041422502423, 0.0015917875286077662, 0.0015649829387702808, 0.0014742923667654395, 0.001555272694011884, 0.001563847487868399, 0.0015678985723845509, 0.0014770048580189444, 0.0015806109996094387, 0.001564496550804042, 0.0015792373273217557, 0.0015594893686321316, 0.0015673020816579157, 0.0015860712868446599, 0.0015784425931811637, 0.0015945137972582359, 0.0015614165720168728, 0.0015590718367650192, 0.001480669529196255, 0.001483167265066687, 0.0014780938568315944, 0.001666088469744641, 0.0015599518368134694, 0.0015675446935644259, 0.0015747081203272147, 0.0015964693679683367, 0.0015787635929882526, 0.0016373844498920502, 0.0015898306321884906, 0.0014932114079746666, 0.001682618019950329, 0.0015986805510878259, 0.001607002509871916, 0.0014768471627743269, 0.0015552983671540813, 0.0015663943890178082, 0.0015164164503162004, 0.0015862590623829439, 0.0015841119789651462, 0.0015769739402458072, 0.001501484857206898, 0.0015754244095474786, 0.0014993078978162032, 0.0015947053883680884, 0.0016850407772260358, 0.001587090407954339, 0.0015800574304992143, 0.0015938997135630675, 0.0014795950205274383, 0.0014721401230603152, 0.0015835798587840125, 0.0015760874066843974, 0.0015656932266619131, 0.0015670226943888226, 0.0014955600205694838, 0.001501505857105462, 0.0015735427340568633, 0.0014812735489056427, 0.0015723395912090735, 0.0015932078169164608, 0.0014828099389275421, 0.0015675291224211759, 0.001493030489061256, 0.0015560252858059747, 0.0015586776948743025, 0.0015682010407730633, 0.0014810986717098526, 0.0014583027341916245, 0.0014735173685856316, 0.0015534683871938257, 0.001554229631734898, 0.0016021580193001702, 0.0015950573689057206, 0.0016071027339607173, 0.0015865844289525127, 0.001565758856393549, 0.001550135407026629, 0.0014736755508739426]
[750.8424150137259, 741.8578123491528, 713.3273705867389, 708.4286824257216, 700.2670520006507, 713.7661424586389, 704.0677421801087, 711.1603618407283, 715.404278166816, 707.6355700274547, 682.7249143994133, 716.0405783801037, 664.1218063607556, 718.5323970614141, 719.0892299319851, 719.5959661725534, 704.0977989403324, 756.7014445003211, 749.1555530128066, 745.911843089576, 739.1785312105001, 750.7854209942375, 746.5781120858062, 751.1003965273183, 707.4580321130909, 637.8094261231935, 719.8925010178131, 765.9779004875958, 774.4600637402542, 756.8413605525942, 765.6238996412311, 689.5941225732602, 763.7968999528591, 764.0806018428317, 756.1925483865679, 755.7224733183866, 766.810453252957, 768.2118937233729, 766.0578113111587, 711.4035863315877, 759.1876702005353, 766.5495066376525, 762.8782923657085, 768.1917434040232, 764.1188738822589, 757.2910241530591, 764.712041388549, 747.1924961625876, 763.2297097750645, 757.4973384638573, 747.8125144935194, 769.4841793846234, 756.2931115071307, 762.8492293455027, 760.7190043218317, 767.3821852638566, 761.3847527717345, 765.5620329456967, 768.2688536758795, 759.7652272156125, 756.6961852720618, 764.4597363531906, 762.5592969879863, 765.5846035851459, 764.3408720974758, 739.817695670936, 760.7584390995518, 718.5651159401463, 736.6470878540782, 762.8486362174825, 759.727614932609, 766.8439473055209, 760.0581687160865, 765.3120110751825, 769.2093347186626, 761.6605800937718, 768.3542308978178, 766.2256851593152, 764.4207748901102, 766.8690414720437, 754.5087164337147, 765.0322428493687, 756.1971593676044, 772.4313753571745, 770.2042376619111, 763.3620963757471, 770.3302491521132, 769.6711972889022, 772.7595749359998, 764.2422468857559, 769.893807545192, 767.585027554189, 728.6155855676666, 743.956138001611, 765.7566863147006, 761.2711124128856, 737.2584950469168, 765.3376034736464, 769.9955308929568, 763.4534404289764, 759.943025031971, 758.9338363922618, 759.9317810882869, 770.0296784990657, 766.4190224657806, 767.9486923242691, 767.7010551991001, 734.6572132616657, 740.5622669111737, 766.2008952941142, 765.873958141648, 758.2620736716221, 759.5319285249876, 768.49375143692, 766.4600455726982, 765.0969278051562, 758.8498816863239, 766.8588281255177, 765.5455037102785, 764.194069748223, 759.3780826885443, 628.190840627815, 641.7957966582335, 678.013063533465, 685.5423718430009, 673.7739741233703, 634.8818389004156, 642.4002209308271, 677.9974338509411, 638.9233295441243, 640.2060368002772, 680.5285892882378, 631.3571711522575, 628.1167964879606, 676.6897952064274, 669.4660185354095, 628.6128967362812, 628.2245475780524, 638.9846018294434, 678.2915129608756, 642.9740609799189, 639.4485445400109, 637.7963585228234, 677.0458435331523, 632.666734729225, 639.1832564194979, 633.2170489510338, 641.2355352426197, 638.0390938689911, 630.4886850258832, 633.535869039506, 627.1504214761255, 640.4440800242729, 641.4072632310132, 675.3701486264969, 674.2327878676836, 676.5470239782846, 600.2082231283124, 641.0454325581685, 637.940343331525, 635.038320493455, 626.3822031691079, 633.4070562820743, 610.7301190419441, 628.9978188578768, 669.6975355662203, 594.3119520552383, 625.515835117621, 622.276563886452, 677.1181373443227, 642.9634474765262, 638.4088241193458, 659.4494538696687, 630.414050084454, 631.2685045493253, 634.1258878660527, 666.0073827585758, 634.749591246487, 666.9744096302945, 627.0750743642568, 593.4574483391611, 630.083828235682, 632.888387913883, 627.3920444872656, 675.8606146454354, 679.2831635627044, 631.4806256552622, 634.4825773994934, 638.6947219105101, 638.1528509962164, 668.6458492111985, 665.998068051334, 635.5086381555259, 675.0947525788163, 635.9949247547951, 627.6645076569032, 674.395263848353, 637.9466803496568, 669.7786865884774, 642.6630782429926, 641.5694554996784, 637.6733428942491, 675.1744627827876, 685.7286738575076, 678.6482611737781, 643.7208560171558, 643.405568637729, 624.158159153867, 626.936698011084, 622.2377567210605, 630.2847688100754, 638.6679506340618, 645.1049343606287, 678.5754160113222]
Elapsed: 0.06998859625364105~0.005805747705858765
Time per graph: 0.0014283386990538989~0.00011848464705834214
Speed: 704.8533068107478~57.20332807138924
Total Time: 0.0730
best val loss: 0.4167346954345703 test_score: 0.8776

Testing...
Test loss: 0.2904 score: 0.8776 time: 0.07s
test Score 0.8776
Epoch Time List: [0.5299562689615414, 0.3249913399340585, 0.37782898696605116, 0.3470770039130002, 0.34465145389549434, 0.34934161708224565, 0.34411121904850006, 0.43708249798510224, 0.3410435769474134, 0.3544995959382504, 0.34550271602347493, 0.34416408312972635, 0.38285754807293415, 0.3390548031311482, 0.3352823421591893, 0.33720499195624143, 0.3436560840345919, 0.33218849590048194, 0.32713768805842847, 0.3251423460897058, 0.33710755105130374, 0.32861237903125584, 0.32182293292135, 0.3277770889690146, 0.33789514005184174, 0.37218618392944336, 0.33123205008450896, 0.32162401091773063, 0.31841617391910404, 0.3250441069249064, 0.32241208001505584, 0.34077863907441497, 0.3368464719969779, 0.3228285030927509, 0.3244704679818824, 0.342187233036384, 0.3232299428200349, 0.31645994482096285, 0.32015796413179487, 0.3257771269418299, 0.31765615998301655, 0.31771656998898834, 0.3176950308261439, 0.3218672600341961, 0.3152982610045001, 0.31674742489121854, 0.31809334398712963, 0.32901491585653275, 0.3227883198997006, 0.3204461318673566, 0.32826824102085084, 0.32055000599939376, 0.32247453404124826, 0.3249742879997939, 0.3287981868488714, 0.3178873991128057, 0.31931880803313106, 0.31712431099731475, 0.3266589851118624, 0.3313340420136228, 0.3325974551262334, 0.3239172729663551, 0.3170671248808503, 0.3164568069623783, 0.3161967920605093, 0.32265342690516263, 0.31798643979709595, 0.3396836328320205, 0.3280622478341684, 0.32233347196597606, 0.32231633295305073, 0.3174902191385627, 0.3215484629618004, 0.3219075670931488, 0.3165254320483655, 0.31825117801781744, 0.3177497610449791, 0.31757571490015835, 0.31837307405658066, 0.31745706300716847, 0.3253294990863651, 0.33180368575267494, 0.3160199917620048, 0.31606739899143577, 0.31501499889418483, 0.3220859010471031, 0.32210135902278125, 0.3156146449036896, 0.31666590610984713, 0.3157113790512085, 0.31477047596126795, 0.3217880839947611, 0.3187277818797156, 0.3349200049415231, 0.32071113493293524, 0.3166853829752654, 0.3267035789322108, 0.32088508002925664, 0.315594902029261, 0.3227737529668957, 0.3202934470027685, 0.327745946822688, 0.3219149369979277, 0.31667311710771173, 0.3163306890055537, 0.3169822500785813, 0.31869700900278986, 0.328618983970955, 0.33202553703449667, 0.32429751206655055, 0.32168181403540075, 0.3251975899329409, 0.31993843999225646, 0.3155595859279856, 0.3199309560004622, 0.3150963799562305, 0.3194340330082923, 0.31632508779875934, 0.31520721409469843, 0.3188969721086323, 0.31701952894218266, 0.33168514596764, 0.3315408378839493, 0.3212377140298486, 0.31937649205792695, 0.31982773391064256, 0.3306478480808437, 0.3261004379019141, 0.32031427894253284, 0.3284944898914546, 0.32606255589053035, 0.31997540302108973, 0.32923831895459443, 0.3306626809062436, 0.362007844960317, 0.32826848793774843, 0.3388450079364702, 0.3397542161401361, 0.3241267391713336, 0.3239083919906989, 0.32954388088546693, 0.3247436349047348, 0.32483208808116615, 0.3226708780275658, 0.3292608450865373, 0.3254926410736516, 0.32675802090670913, 0.32497825298924, 0.324847248964943, 0.32632702589035034, 0.3327916549751535, 0.33523043885361403, 0.33550408901646733, 0.3265922499122098, 0.32209420995786786, 0.3260112040443346, 0.3281013040104881, 0.33622342301532626, 0.32652741204947233, 0.3263934721471742, 0.3267580800456926, 0.32819292799104005, 0.33308150083757937, 0.33715132297948003, 0.3329106420278549, 0.32930638594552875, 0.3531855558976531, 0.34135908901225775, 0.328482081880793, 0.41124358610250056, 0.333962582051754, 0.3266004570759833, 0.3268399579683319, 0.33834079082589597, 0.381393603165634, 0.328421808895655, 0.3254147480474785, 0.3336581760086119, 0.32612580293789506, 0.3608776959590614, 0.34147049207240343, 0.3396816609892994, 0.33152906026225537, 0.3343586709816009, 0.3849322630558163, 0.3309821120928973, 0.33595656300894916, 0.3341317649465054, 0.3320712459972128, 0.3266744740540162, 0.32786396087612957, 0.3371122688986361, 0.3341060639359057, 0.32867328892461956, 0.33673972194083035, 0.3292207058984786, 0.3263541730120778, 0.33074993710033596, 0.3247034989763051, 0.33020359301008284, 0.3283648800570518, 0.3288353179814294, 0.3293006969615817, 0.32864251104183495, 0.32824850804172456, 0.3280089688487351, 0.3337572200689465, 0.33023450686596334, 0.33157219213899225, 0.33626633894164115, 0.333142188959755, 0.3470884069101885, 0.3271048270398751, 0.3233879440231249]
Total Epoch List: [121, 93]
Total Time List: [0.06507592007983476, 0.07295708905439824]
========================k_idx:2========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: GPSTransformer
GPS(
  (x_lin): Linear(in_features=14887, out_features=64, bias=True)
  (pe_lin): Linear(in_features=20, out_features=8, bias=True)
  (pe_norm): BatchNorm1d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (convs): ModuleList(
    (0-1): 2 x GPSConv(64, conv=GINEConv(nn=Sequential(
      (0): Linear(in_features=64, out_features=64, bias=True)
      (1): ReLU()
      (2): Linear(in_features=64, out_features=64, bias=True)
    )), heads=1, attn_type=multihead)
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=32, bias=True)
    (1): ReLU()
    (2): Linear(in_features=32, out_features=16, bias=True)
    (3): ReLU()
    (4): Linear(in_features=16, out_features=2, bias=True)
  )
)
Total number of parameters: 1039906
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7d29a7d73c40>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 1.1451;  Loss pred: 1.1451; Loss self: 0.0000; time: 0.22s
Val loss: 2.0159 score: 0.3878 time: 0.06s
Test loss: 2.1642 score: 0.3333 time: 0.07s
Epoch 2/1000, LR 0.000000
Train loss: 1.1246;  Loss pred: 1.1246; Loss self: 0.0000; time: 0.20s
Val loss: 1.5351 score: 0.3878 time: 0.07s
Test loss: 1.6268 score: 0.2917 time: 0.06s
Epoch 3/1000, LR 0.000030
Train loss: 1.0564;  Loss pred: 1.0564; Loss self: 0.0000; time: 0.21s
Val loss: 1.2004 score: 0.3061 time: 0.08s
Test loss: 1.2426 score: 0.2708 time: 0.07s
Epoch 4/1000, LR 0.000060
Train loss: 0.9189;  Loss pred: 0.9189; Loss self: 0.0000; time: 0.20s
Val loss: 0.9680 score: 0.3061 time: 0.06s
Test loss: 0.9943 score: 0.3125 time: 0.07s
Epoch 5/1000, LR 0.000090
Train loss: 0.8030;  Loss pred: 0.8030; Loss self: 0.0000; time: 0.20s
Val loss: 0.8255 score: 0.3878 time: 0.07s
Test loss: 0.8547 score: 0.3750 time: 0.06s
Epoch 6/1000, LR 0.000120
Train loss: 0.6599;  Loss pred: 0.6599; Loss self: 0.0000; time: 0.20s
Val loss: 0.7528 score: 0.4490 time: 0.07s
Test loss: 0.7763 score: 0.4583 time: 0.07s
Epoch 7/1000, LR 0.000150
Train loss: 0.5465;  Loss pred: 0.5465; Loss self: 0.0000; time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7253 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7371 score: 0.5000 time: 0.07s
Epoch 8/1000, LR 0.000180
Train loss: 0.4574;  Loss pred: 0.4574; Loss self: 0.0000; time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7187 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7279 score: 0.5000 time: 0.07s
Epoch 9/1000, LR 0.000210
Train loss: 0.3854;  Loss pred: 0.3854; Loss self: 0.0000; time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7179 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7281 score: 0.5000 time: 0.06s
Epoch 10/1000, LR 0.000240
Train loss: 0.3358;  Loss pred: 0.3358; Loss self: 0.0000; time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7179 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7306 score: 0.5000 time: 0.07s
     INFO: Early stopping counter 1 of 20
Epoch 11/1000, LR 0.000270
Train loss: 0.2782;  Loss pred: 0.2782; Loss self: 0.0000; time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7169 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7321 score: 0.5000 time: 0.07s
Epoch 12/1000, LR 0.000270
Train loss: 0.2453;  Loss pred: 0.2453; Loss self: 0.0000; time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7131 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7290 score: 0.5000 time: 0.07s
Epoch 13/1000, LR 0.000270
Train loss: 0.2100;  Loss pred: 0.2100; Loss self: 0.0000; time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7059 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7220 score: 0.5000 time: 0.07s
Epoch 14/1000, LR 0.000270
Train loss: 0.1782;  Loss pred: 0.1782; Loss self: 0.0000; time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6982 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7135 score: 0.5000 time: 0.07s
Epoch 15/1000, LR 0.000270
Train loss: 0.1546;  Loss pred: 0.1546; Loss self: 0.0000; time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6913 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7033 score: 0.5000 time: 0.07s
Epoch 16/1000, LR 0.000270
Train loss: 0.1323;  Loss pred: 0.1323; Loss self: 0.0000; time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6843 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6943 score: 0.5000 time: 0.06s
Epoch 17/1000, LR 0.000270
Train loss: 0.1076;  Loss pred: 0.1076; Loss self: 0.0000; time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6773 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6862 score: 0.5000 time: 0.07s
Epoch 18/1000, LR 0.000270
Train loss: 0.0870;  Loss pred: 0.0870; Loss self: 0.0000; time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6711 score: 0.5102 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6805 score: 0.5000 time: 0.08s
Epoch 19/1000, LR 0.000270
Train loss: 0.0770;  Loss pred: 0.0770; Loss self: 0.0000; time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6669 score: 0.5102 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6769 score: 0.5000 time: 0.08s
Epoch 20/1000, LR 0.000270
Train loss: 0.0672;  Loss pred: 0.0672; Loss self: 0.0000; time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6628 score: 0.5102 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6747 score: 0.5000 time: 0.08s
Epoch 21/1000, LR 0.000270
Train loss: 0.0568;  Loss pred: 0.0568; Loss self: 0.0000; time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6585 score: 0.5102 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6721 score: 0.5000 time: 0.08s
Epoch 22/1000, LR 0.000270
Train loss: 0.0433;  Loss pred: 0.0433; Loss self: 0.0000; time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6545 score: 0.5102 time: 0.07s
Test loss: 0.6687 score: 0.4792 time: 0.08s
Epoch 23/1000, LR 0.000270
Train loss: 0.0363;  Loss pred: 0.0363; Loss self: 0.0000; time: 0.22s
Val loss: 0.6497 score: 0.5306 time: 0.08s
Test loss: 0.6641 score: 0.4792 time: 0.07s
Epoch 24/1000, LR 0.000270
Train loss: 0.0335;  Loss pred: 0.0335; Loss self: 0.0000; time: 0.23s
Val loss: 0.6452 score: 0.5102 time: 0.08s
Test loss: 0.6592 score: 0.4792 time: 0.07s
Epoch 25/1000, LR 0.000270
Train loss: 0.0300;  Loss pred: 0.0300; Loss self: 0.0000; time: 0.22s
Val loss: 0.6403 score: 0.5102 time: 0.07s
Test loss: 0.6543 score: 0.4792 time: 0.07s
Epoch 26/1000, LR 0.000270
Train loss: 0.0242;  Loss pred: 0.0242; Loss self: 0.0000; time: 0.22s
Val loss: 0.6359 score: 0.5102 time: 0.08s
Test loss: 0.6496 score: 0.4583 time: 0.07s
Epoch 27/1000, LR 0.000270
Train loss: 0.0237;  Loss pred: 0.0237; Loss self: 0.0000; time: 0.21s
Val loss: 0.6316 score: 0.5306 time: 0.07s
Test loss: 0.6452 score: 0.4792 time: 0.07s
Epoch 28/1000, LR 0.000270
Train loss: 0.0189;  Loss pred: 0.0189; Loss self: 0.0000; time: 0.20s
Val loss: 0.6278 score: 0.5510 time: 0.07s
Test loss: 0.6418 score: 0.4792 time: 0.06s
Epoch 29/1000, LR 0.000270
Train loss: 0.0186;  Loss pred: 0.0186; Loss self: 0.0000; time: 0.22s
Val loss: 0.6244 score: 0.5714 time: 0.06s
Test loss: 0.6381 score: 0.4792 time: 0.07s
Epoch 30/1000, LR 0.000270
Train loss: 0.0142;  Loss pred: 0.0142; Loss self: 0.0000; time: 0.20s
Val loss: 0.6210 score: 0.5714 time: 0.07s
Test loss: 0.6343 score: 0.5208 time: 0.07s
Epoch 31/1000, LR 0.000270
Train loss: 0.0134;  Loss pred: 0.0134; Loss self: 0.0000; time: 0.22s
Val loss: 0.6176 score: 0.5714 time: 0.06s
Test loss: 0.6295 score: 0.5208 time: 0.07s
Epoch 32/1000, LR 0.000270
Train loss: 0.0119;  Loss pred: 0.0119; Loss self: 0.0000; time: 0.19s
Val loss: 0.6143 score: 0.5306 time: 0.07s
Test loss: 0.6246 score: 0.5208 time: 0.07s
Epoch 33/1000, LR 0.000270
Train loss: 0.0114;  Loss pred: 0.0114; Loss self: 0.0000; time: 0.21s
Val loss: 0.6109 score: 0.5510 time: 0.06s
Test loss: 0.6192 score: 0.5208 time: 0.07s
Epoch 34/1000, LR 0.000270
Train loss: 0.0101;  Loss pred: 0.0101; Loss self: 0.0000; time: 0.20s
Val loss: 0.6073 score: 0.5510 time: 0.07s
Test loss: 0.6135 score: 0.5417 time: 0.07s
Epoch 35/1000, LR 0.000270
Train loss: 0.0098;  Loss pred: 0.0098; Loss self: 0.0000; time: 0.21s
Val loss: 0.6033 score: 0.5510 time: 0.07s
Test loss: 0.6073 score: 0.5417 time: 0.07s
Epoch 36/1000, LR 0.000270
Train loss: 0.0096;  Loss pred: 0.0096; Loss self: 0.0000; time: 0.20s
Val loss: 0.5985 score: 0.5918 time: 0.06s
Test loss: 0.6008 score: 0.5625 time: 0.07s
Epoch 37/1000, LR 0.000270
Train loss: 0.0078;  Loss pred: 0.0078; Loss self: 0.0000; time: 0.21s
Val loss: 0.5924 score: 0.6327 time: 0.07s
Test loss: 0.5947 score: 0.5833 time: 0.07s
Epoch 38/1000, LR 0.000270
Train loss: 0.0069;  Loss pred: 0.0069; Loss self: 0.0000; time: 0.20s
Val loss: 0.5862 score: 0.6327 time: 0.06s
Test loss: 0.5884 score: 0.5833 time: 0.07s
Epoch 39/1000, LR 0.000269
Train loss: 0.0070;  Loss pred: 0.0070; Loss self: 0.0000; time: 0.20s
Val loss: 0.5800 score: 0.6327 time: 0.07s
Test loss: 0.5817 score: 0.6042 time: 0.07s
Epoch 40/1000, LR 0.000269
Train loss: 0.0063;  Loss pred: 0.0063; Loss self: 0.0000; time: 0.21s
Val loss: 0.5730 score: 0.6122 time: 0.07s
Test loss: 0.5753 score: 0.6042 time: 0.07s
Epoch 41/1000, LR 0.000269
Train loss: 0.0060;  Loss pred: 0.0060; Loss self: 0.0000; time: 0.20s
Val loss: 0.5653 score: 0.6122 time: 0.07s
Test loss: 0.5689 score: 0.5833 time: 0.07s
Epoch 42/1000, LR 0.000269
Train loss: 0.0060;  Loss pred: 0.0060; Loss self: 0.0000; time: 0.23s
Val loss: 0.5575 score: 0.6327 time: 0.08s
Test loss: 0.5627 score: 0.5833 time: 0.08s
Epoch 43/1000, LR 0.000269
Train loss: 0.0053;  Loss pred: 0.0053; Loss self: 0.0000; time: 0.22s
Val loss: 0.5497 score: 0.6531 time: 0.08s
Test loss: 0.5561 score: 0.6250 time: 0.07s
Epoch 44/1000, LR 0.000269
Train loss: 0.0055;  Loss pred: 0.0055; Loss self: 0.0000; time: 0.23s
Val loss: 0.5419 score: 0.6939 time: 0.07s
Test loss: 0.5488 score: 0.6250 time: 0.07s
Epoch 45/1000, LR 0.000269
Train loss: 0.0043;  Loss pred: 0.0043; Loss self: 0.0000; time: 0.23s
Val loss: 0.5339 score: 0.6939 time: 0.07s
Test loss: 0.5409 score: 0.6667 time: 0.08s
Epoch 46/1000, LR 0.000269
Train loss: 0.0039;  Loss pred: 0.0039; Loss self: 0.0000; time: 0.22s
Val loss: 0.5258 score: 0.7755 time: 0.08s
Test loss: 0.5332 score: 0.6875 time: 0.08s
Epoch 47/1000, LR 0.000269
Train loss: 0.0037;  Loss pred: 0.0037; Loss self: 0.0000; time: 0.22s
Val loss: 0.5179 score: 0.8367 time: 0.08s
Test loss: 0.5256 score: 0.8125 time: 0.08s
Epoch 48/1000, LR 0.000269
Train loss: 0.0036;  Loss pred: 0.0036; Loss self: 0.0000; time: 0.23s
Val loss: 0.5104 score: 0.8571 time: 0.07s
Test loss: 0.5183 score: 0.8125 time: 0.07s
Epoch 49/1000, LR 0.000269
Train loss: 0.0034;  Loss pred: 0.0034; Loss self: 0.0000; time: 0.24s
Val loss: 0.5031 score: 0.8571 time: 0.07s
Test loss: 0.5113 score: 0.8542 time: 0.07s
Epoch 50/1000, LR 0.000269
Train loss: 0.0030;  Loss pred: 0.0030; Loss self: 0.0000; time: 0.23s
Val loss: 0.4952 score: 0.8367 time: 0.08s
Test loss: 0.5040 score: 0.8333 time: 0.08s
Epoch 51/1000, LR 0.000269
Train loss: 0.0033;  Loss pred: 0.0033; Loss self: 0.0000; time: 0.23s
Val loss: 0.4876 score: 0.8367 time: 0.07s
Test loss: 0.4967 score: 0.8542 time: 0.08s
Epoch 52/1000, LR 0.000269
Train loss: 0.0029;  Loss pred: 0.0029; Loss self: 0.0000; time: 0.22s
Val loss: 0.4804 score: 0.8163 time: 0.08s
Test loss: 0.4895 score: 0.8542 time: 0.07s
Epoch 53/1000, LR 0.000269
Train loss: 0.0030;  Loss pred: 0.0030; Loss self: 0.0000; time: 0.23s
Val loss: 0.4740 score: 0.8163 time: 0.07s
Test loss: 0.4824 score: 0.8542 time: 0.08s
Epoch 54/1000, LR 0.000269
Train loss: 0.0023;  Loss pred: 0.0023; Loss self: 0.0000; time: 0.23s
Val loss: 0.4680 score: 0.8776 time: 0.08s
Test loss: 0.4752 score: 0.8542 time: 0.07s
Epoch 55/1000, LR 0.000269
Train loss: 0.0027;  Loss pred: 0.0027; Loss self: 0.0000; time: 0.24s
Val loss: 0.4624 score: 0.8776 time: 0.08s
Test loss: 0.4678 score: 0.8750 time: 0.07s
Epoch 56/1000, LR 0.000269
Train loss: 0.0027;  Loss pred: 0.0027; Loss self: 0.0000; time: 0.21s
Val loss: 0.4570 score: 0.8571 time: 0.07s
Test loss: 0.4605 score: 0.8958 time: 0.07s
Epoch 57/1000, LR 0.000269
Train loss: 0.0024;  Loss pred: 0.0024; Loss self: 0.0000; time: 0.21s
Val loss: 0.4514 score: 0.8367 time: 0.07s
Test loss: 0.4527 score: 0.8750 time: 0.07s
Epoch 58/1000, LR 0.000269
Train loss: 0.0023;  Loss pred: 0.0023; Loss self: 0.0000; time: 0.21s
Val loss: 0.4455 score: 0.8367 time: 0.07s
Test loss: 0.4447 score: 0.8750 time: 0.07s
Epoch 59/1000, LR 0.000268
Train loss: 0.0023;  Loss pred: 0.0023; Loss self: 0.0000; time: 0.21s
Val loss: 0.4395 score: 0.8367 time: 0.07s
Test loss: 0.4363 score: 0.8750 time: 0.07s
Epoch 60/1000, LR 0.000268
Train loss: 0.0023;  Loss pred: 0.0023; Loss self: 0.0000; time: 0.21s
Val loss: 0.4327 score: 0.8367 time: 0.07s
Test loss: 0.4277 score: 0.8750 time: 0.07s
Epoch 61/1000, LR 0.000268
Train loss: 0.0018;  Loss pred: 0.0018; Loss self: 0.0000; time: 0.21s
Val loss: 0.4259 score: 0.8367 time: 0.07s
Test loss: 0.4190 score: 0.8750 time: 0.07s
Epoch 62/1000, LR 0.000268
Train loss: 0.0019;  Loss pred: 0.0019; Loss self: 0.0000; time: 0.21s
Val loss: 0.4194 score: 0.8367 time: 0.07s
Test loss: 0.4101 score: 0.8542 time: 0.07s
Epoch 63/1000, LR 0.000268
Train loss: 0.0020;  Loss pred: 0.0020; Loss self: 0.0000; time: 0.21s
Val loss: 0.4129 score: 0.8367 time: 0.07s
Test loss: 0.4015 score: 0.8542 time: 0.07s
Epoch 64/1000, LR 0.000268
Train loss: 0.0021;  Loss pred: 0.0021; Loss self: 0.0000; time: 0.21s
Val loss: 0.4069 score: 0.8163 time: 0.07s
Test loss: 0.3936 score: 0.8542 time: 0.07s
Epoch 65/1000, LR 0.000268
Train loss: 0.0023;  Loss pred: 0.0023; Loss self: 0.0000; time: 0.21s
Val loss: 0.4008 score: 0.8367 time: 0.07s
Test loss: 0.3855 score: 0.8542 time: 0.07s
Epoch 66/1000, LR 0.000268
Train loss: 0.0016;  Loss pred: 0.0016; Loss self: 0.0000; time: 0.21s
Val loss: 0.3949 score: 0.8367 time: 0.07s
Test loss: 0.3773 score: 0.8542 time: 0.07s
Epoch 67/1000, LR 0.000268
Train loss: 0.0017;  Loss pred: 0.0017; Loss self: 0.0000; time: 0.20s
Val loss: 0.3893 score: 0.8367 time: 0.07s
Test loss: 0.3685 score: 0.8542 time: 0.07s
Epoch 68/1000, LR 0.000268
Train loss: 0.0018;  Loss pred: 0.0018; Loss self: 0.0000; time: 0.20s
Val loss: 0.3838 score: 0.8571 time: 0.07s
Test loss: 0.3601 score: 0.8542 time: 0.07s
Epoch 69/1000, LR 0.000268
Train loss: 0.0015;  Loss pred: 0.0015; Loss self: 0.0000; time: 0.20s
Val loss: 0.3785 score: 0.8571 time: 0.07s
Test loss: 0.3521 score: 0.8750 time: 0.07s
Epoch 70/1000, LR 0.000268
Train loss: 0.0019;  Loss pred: 0.0019; Loss self: 0.0000; time: 0.20s
Val loss: 0.3742 score: 0.8571 time: 0.07s
Test loss: 0.3449 score: 0.8750 time: 0.07s
Epoch 71/1000, LR 0.000268
Train loss: 0.0016;  Loss pred: 0.0016; Loss self: 0.0000; time: 0.21s
Val loss: 0.3703 score: 0.8776 time: 0.07s
Test loss: 0.3385 score: 0.8750 time: 0.07s
Epoch 72/1000, LR 0.000267
Train loss: 0.0016;  Loss pred: 0.0016; Loss self: 0.0000; time: 0.21s
Val loss: 0.3667 score: 0.8571 time: 0.07s
Test loss: 0.3331 score: 0.8542 time: 0.07s
Epoch 73/1000, LR 0.000267
Train loss: 0.0015;  Loss pred: 0.0015; Loss self: 0.0000; time: 0.21s
Val loss: 0.3627 score: 0.8571 time: 0.07s
Test loss: 0.3271 score: 0.8750 time: 0.07s
Epoch 74/1000, LR 0.000267
Train loss: 0.0013;  Loss pred: 0.0013; Loss self: 0.0000; time: 0.21s
Val loss: 0.3593 score: 0.8571 time: 0.07s
Test loss: 0.3205 score: 0.8750 time: 0.07s
Epoch 75/1000, LR 0.000267
Train loss: 0.0012;  Loss pred: 0.0012; Loss self: 0.0000; time: 0.21s
Val loss: 0.3580 score: 0.8571 time: 0.07s
Test loss: 0.3150 score: 0.8750 time: 0.07s
Epoch 76/1000, LR 0.000267
Train loss: 0.0013;  Loss pred: 0.0013; Loss self: 0.0000; time: 0.21s
Val loss: 0.3576 score: 0.8571 time: 0.07s
Test loss: 0.3110 score: 0.8750 time: 0.07s
Epoch 77/1000, LR 0.000267
Train loss: 0.0012;  Loss pred: 0.0012; Loss self: 0.0000; time: 0.21s
Val loss: 0.3579 score: 0.8367 time: 0.07s
Test loss: 0.3074 score: 0.8750 time: 0.07s
     INFO: Early stopping counter 1 of 20
Epoch 78/1000, LR 0.000267
Train loss: 0.0012;  Loss pred: 0.0012; Loss self: 0.0000; time: 0.21s
Val loss: 0.3586 score: 0.8367 time: 0.07s
Test loss: 0.3040 score: 0.8750 time: 0.07s
     INFO: Early stopping counter 2 of 20
Epoch 79/1000, LR 0.000267
Train loss: 0.0015;  Loss pred: 0.0015; Loss self: 0.0000; time: 0.21s
Val loss: 0.3606 score: 0.8367 time: 0.07s
Test loss: 0.3018 score: 0.8750 time: 0.07s
     INFO: Early stopping counter 3 of 20
Epoch 80/1000, LR 0.000267
Train loss: 0.0012;  Loss pred: 0.0012; Loss self: 0.0000; time: 0.21s
Val loss: 0.3630 score: 0.8367 time: 0.07s
Test loss: 0.3002 score: 0.8542 time: 0.07s
     INFO: Early stopping counter 4 of 20
Epoch 81/1000, LR 0.000267
Train loss: 0.0011;  Loss pred: 0.0011; Loss self: 0.0000; time: 0.21s
Val loss: 0.3658 score: 0.8367 time: 0.07s
Test loss: 0.2994 score: 0.8542 time: 0.07s
     INFO: Early stopping counter 5 of 20
Epoch 82/1000, LR 0.000267
Train loss: 0.0009;  Loss pred: 0.0009; Loss self: 0.0000; time: 0.21s
Val loss: 0.3691 score: 0.8367 time: 0.07s
Test loss: 0.2995 score: 0.8542 time: 0.07s
     INFO: Early stopping counter 6 of 20
Epoch 83/1000, LR 0.000266
Train loss: 0.0010;  Loss pred: 0.0010; Loss self: 0.0000; time: 0.21s
Val loss: 0.3735 score: 0.8367 time: 0.07s
Test loss: 0.3005 score: 0.8542 time: 0.07s
     INFO: Early stopping counter 7 of 20
Epoch 84/1000, LR 0.000266
Train loss: 0.0009;  Loss pred: 0.0009; Loss self: 0.0000; time: 0.21s
Val loss: 0.3788 score: 0.8367 time: 0.07s
Test loss: 0.3031 score: 0.8542 time: 0.07s
     INFO: Early stopping counter 8 of 20
Epoch 85/1000, LR 0.000266
Train loss: 0.0010;  Loss pred: 0.0010; Loss self: 0.0000; time: 0.21s
Val loss: 0.3850 score: 0.8367 time: 0.07s
Test loss: 0.3062 score: 0.8542 time: 0.07s
     INFO: Early stopping counter 9 of 20
Epoch 86/1000, LR 0.000266
Train loss: 0.0012;  Loss pred: 0.0012; Loss self: 0.0000; time: 0.21s
Val loss: 0.3926 score: 0.8367 time: 0.07s
Test loss: 0.3098 score: 0.8542 time: 0.07s
     INFO: Early stopping counter 10 of 20
Epoch 87/1000, LR 0.000266
Train loss: 0.0013;  Loss pred: 0.0013; Loss self: 0.0000; time: 0.21s
Val loss: 0.4004 score: 0.8367 time: 0.07s
Test loss: 0.3128 score: 0.8542 time: 0.07s
     INFO: Early stopping counter 11 of 20
Epoch 88/1000, LR 0.000266
Train loss: 0.0010;  Loss pred: 0.0010; Loss self: 0.0000; time: 0.21s
Val loss: 0.4087 score: 0.8367 time: 0.07s
Test loss: 0.3153 score: 0.8542 time: 0.07s
     INFO: Early stopping counter 12 of 20
Epoch 89/1000, LR 0.000266
Train loss: 0.0009;  Loss pred: 0.0009; Loss self: 0.0000; time: 0.21s
Val loss: 0.4168 score: 0.8367 time: 0.07s
Test loss: 0.3180 score: 0.8542 time: 0.07s
     INFO: Early stopping counter 13 of 20
Epoch 90/1000, LR 0.000266
Train loss: 0.0010;  Loss pred: 0.0010; Loss self: 0.0000; time: 0.21s
Val loss: 0.4260 score: 0.8367 time: 0.07s
Test loss: 0.3219 score: 0.8542 time: 0.07s
     INFO: Early stopping counter 14 of 20
Epoch 91/1000, LR 0.000266
Train loss: 0.0011;  Loss pred: 0.0011; Loss self: 0.0000; time: 0.21s
Val loss: 0.4351 score: 0.8367 time: 0.07s
Test loss: 0.3259 score: 0.8542 time: 0.07s
     INFO: Early stopping counter 15 of 20
Epoch 92/1000, LR 0.000266
Train loss: 0.0010;  Loss pred: 0.0010; Loss self: 0.0000; time: 0.21s
Val loss: 0.4438 score: 0.8367 time: 0.07s
Test loss: 0.3302 score: 0.8542 time: 0.07s
     INFO: Early stopping counter 16 of 20
Epoch 93/1000, LR 0.000265
Train loss: 0.0010;  Loss pred: 0.0010; Loss self: 0.0000; time: 0.21s
Val loss: 0.4520 score: 0.8367 time: 0.07s
Test loss: 0.3343 score: 0.8542 time: 0.07s
     INFO: Early stopping counter 17 of 20
Epoch 94/1000, LR 0.000265
Train loss: 0.0007;  Loss pred: 0.0007; Loss self: 0.0000; time: 0.21s
Val loss: 0.4597 score: 0.8367 time: 0.07s
Test loss: 0.3385 score: 0.8542 time: 0.07s
     INFO: Early stopping counter 18 of 20
Epoch 95/1000, LR 0.000265
Train loss: 0.0009;  Loss pred: 0.0009; Loss self: 0.0000; time: 0.21s
Val loss: 0.4699 score: 0.8367 time: 0.07s
Test loss: 0.3445 score: 0.8542 time: 0.07s
     INFO: Early stopping counter 19 of 20
Epoch 96/1000, LR 0.000265
Train loss: 0.0009;  Loss pred: 0.0009; Loss self: 0.0000; time: 0.22s
Val loss: 0.4789 score: 0.8367 time: 0.07s
Test loss: 0.3497 score: 0.8542 time: 0.07s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 075,   Train_Loss: 0.0013,   Val_Loss: 0.3576,   Val_Precision: 1.0000,   Val_Recall: 0.7200,   Val_accuracy: 0.8372,   Val_Score: 0.8571,   Val_Loss: 0.3576,   Test_Precision: 1.0000,   Test_Recall: 0.7500,   Test_accuracy: 0.8571,   Test_Score: 0.8750,   Test_loss: 0.3110


[0.06526003195904195, 0.06605039292480797, 0.06869216298218817, 0.06916715996339917, 0.06997330498415977, 0.06864993600174785, 0.06959557591471821, 0.06890147796366364, 0.06849274109117687, 0.06924468197394162, 0.07177121995482594, 0.06843187590129673, 0.07378164597321302, 0.06819455907680094, 0.06814175203908235, 0.06809376692399383, 0.06959260499570519, 0.06475473300088197, 0.06540697696618736, 0.06569140905048698, 0.06628980406094342, 0.0652649860130623, 0.06563278404064476, 0.06523761700373143, 0.06926205905620009, 0.07682545599527657, 0.06806571804918349, 0.06397051399108022, 0.06326988607179374, 0.06474276189692318, 0.06400009198114276, 0.07105629006400704, 0.06415317999199033, 0.06412936002016068, 0.06479831109754741, 0.06483861699234694, 0.06390105898026377, 0.06378448498435318, 0.06396384094841778, 0.06887792097404599, 0.06454267096705735, 0.06392281199805439, 0.0642304290086031, 0.06378615810535848, 0.06412614800501615, 0.06470431899651885, 0.06407640699762851, 0.06557881704065949, 0.06420085509307683, 0.06468669592868537, 0.06552444503176957, 0.06367902201600373, 0.06478969496674836, 0.0642328760586679, 0.06441274599637836, 0.06385345000308007, 0.06435642403084785, 0.06400526396464556, 0.06377975596114993, 0.06449360703118145, 0.06475518306251615, 0.06409755500499159, 0.06425729801412672, 0.06400337698869407, 0.0641075229505077, 0.06623253307770938, 0.06440940708853304, 0.06819145393092185, 0.06651760498061776, 0.06423292600084096, 0.0644967999542132, 0.06389826792292297, 0.06446874991524965, 0.06402617401909083, 0.06370177504140884, 0.06433311803266406, 0.06377266894560307, 0.06394982698839158, 0.06410082196816802, 0.06389617698732764, 0.06494292104616761, 0.06404958805069327, 0.06479791598394513, 0.06343605602160096, 0.06361948896665126, 0.06418972101528198, 0.06360908201895654, 0.06366354902274907, 0.06340911402367055, 0.06411579600535333, 0.06364514108281583, 0.06383657606784254, 0.0672508260468021, 0.06586409802548587, 0.06398899399209768, 0.06436603097245097, 0.06646244204603136, 0.06402403302490711, 0.06363673298619688, 0.06418204098008573, 0.06447851797565818, 0.06456425797659904, 0.06447947199922055, 0.06363391096238047, 0.06393369496800005, 0.06380634603556246, 0.06382692803163081, 0.0666977729415521, 0.06616594200022519, 0.06395189603790641, 0.06397919589653611, 0.0646214570151642, 0.06451341696083546, 0.0637610909761861, 0.06393027305603027, 0.0640441729919985, 0.0645714009879157, 0.06389702798333019, 0.06400664593093097, 0.06411983806174248, 0.06452648702543229, 0.07800177403260022, 0.0763482719194144, 0.07226999395061284, 0.07147625298239291, 0.07272468495648354, 0.07717971596866846, 0.07627643703017384, 0.07227165997028351, 0.07669151795562357, 0.07653785997536033, 0.07200285303406417, 0.07761058595497161, 0.07801096909679472, 0.07241131807677448, 0.07319266197737306, 0.07794940297026187, 0.07799758890178055, 0.07668416399974376, 0.07224032597150654, 0.07620836200658232, 0.07662852690555155, 0.07682703004684299, 0.07237323804292828, 0.0774499389808625, 0.07666033098939806, 0.07738262903876603, 0.07641497906297445, 0.07679780200123787, 0.07771749305538833, 0.07734368706587702, 0.07813117606565356, 0.07650941202882677, 0.07639452000148594, 0.0725528069306165, 0.07267519598826766, 0.07242659898474813, 0.0816383350174874, 0.07643764000386, 0.07680968998465687, 0.07716069789603353, 0.0782269990304485, 0.07735941605642438, 0.08023183804471046, 0.07790170097723603, 0.07316735899075866, 0.08244828297756612, 0.07833534700330347, 0.07874312298372388, 0.07236551097594202, 0.07620961999054998, 0.0767533250618726, 0.07430440606549382, 0.07772669405676425, 0.07762148696929216, 0.07727172307204455, 0.073572758003138, 0.07719579606782645, 0.07346608699299395, 0.07814056403003633, 0.08256699808407575, 0.0777674299897626, 0.0774228140944615, 0.07810108596459031, 0.07250015600584447, 0.07213486602995545, 0.07759541308041662, 0.07722828292753547, 0.07671896810643375, 0.07678411202505231, 0.07328244100790471, 0.07357378699816763, 0.0771035939687863, 0.07258240389637649, 0.0770446399692446, 0.07806718302890658, 0.07265768700744957, 0.07680892699863762, 0.07315849396400154, 0.07624523900449276, 0.07637520704884082, 0.0768418509978801, 0.07257383491378278, 0.0714568339753896, 0.07220235106069595, 0.07611995097249746, 0.07615725195501, 0.07850574294570833, 0.07815781107638031, 0.07874803396407515, 0.07774263701867312, 0.0767221839632839, 0.07595663494430482, 0.07221010199282318, 0.06976968399249017, 0.06908694200683385, 0.07433104095980525, 0.07210290594957769, 0.06864385795779526, 0.06988712807651609, 0.07771805697120726, 0.07260492409113795, 0.07399731990881264, 0.0702790420036763, 0.0739086609100923, 0.06965949200093746, 0.07771425705868751, 0.07654804096091539, 0.07652887504082173, 0.06936993997078389, 0.07951278705149889, 0.08815943403169513, 0.08090718800667673, 0.08831148396711797, 0.08038784295786172, 0.08021289797034115, 0.0785033650463447, 0.08117886004038155, 0.07453330501448363, 0.07529460592195392, 0.07371019292622805, 0.06833018001634628, 0.07658139406703413, 0.0745702349813655, 0.07823564403224736, 0.07357403100468218, 0.0791304879821837, 0.0697787330718711, 0.07383451401256025, 0.07423553906846792, 0.07381184096448123, 0.0798222569283098, 0.07384639407973737, 0.07345377095043659, 0.07046215410809964, 0.08116477495059371, 0.07721338304691017, 0.07698222598992288, 0.08735510194674134, 0.08029419300146401, 0.0905024129897356, 0.07719652797095478, 0.07703195093199611, 0.08181571494787931, 0.0806335189845413, 0.0770185220753774, 0.08232897496782243, 0.07940030191093683, 0.07918720599263906, 0.07335946103557944, 0.07240637706127018, 0.07292513502761722, 0.07271995197515935, 0.07181985292118043, 0.07311647804453969, 0.07585875503718853, 0.07295566808898002, 0.0720253869658336, 0.07288812007755041, 0.07146465696860105, 0.0714407239574939, 0.07164823508355767, 0.07098113792017102, 0.07166593300644308, 0.07156286900863051, 0.0715880359057337, 0.07077860797289759, 0.07157986506354064, 0.07290363102219999, 0.07319643499795347, 0.07420742197427899, 0.07196578499861062, 0.0726332199992612, 0.07316374103538692, 0.07238724897615612, 0.07296378607861698, 0.07277067808900028, 0.0724337879801169, 0.07293907692655921, 0.07264428702183068, 0.0727913910523057, 0.07282827096059918, 0.07250261097215116, 0.07295878999866545, 0.07544289692305028, 0.07378436508588493, 0.07252158701885492, 0.07269927905872464, 0.07253204204607755, 0.07600833394099027]
[0.0013318373869192234, 0.0013479672025471013, 0.0014018808771875135, 0.0014115746931305953, 0.0014280266323297912, 0.0014010191020764867, 0.0014203178758105757, 0.0014061526115033396, 0.001397811042677079, 0.0014131567749784005, 0.0014647187745882844, 0.0013965688959448313, 0.0015057478770043474, 0.0013917256954449173, 0.001390648000797599, 0.001389668712734568, 0.00142025724481031, 0.0013215251632833055, 0.0013348362646160685, 0.0013406410010303467, 0.0013528531441008862, 0.001331938490062496, 0.0013394445722580564, 0.001331379938851662, 0.0014135114093102058, 0.0015678664488831954, 0.0013890962867180304, 0.0013055206936955148, 0.0012912221647304843, 0.0013212808550392486, 0.0013061243261457706, 0.001450128368653205, 0.0013092485712651088, 0.0013087624493910341, 0.0013224145121948452, 0.0013232370814764683, 0.001304103244495179, 0.0013017241833541467, 0.0013053845091513833, 0.0014056718566131834, 0.0013171973666746396, 0.001304547183633763, 0.0013108250818082265, 0.0013017583286807854, 0.001308696898061554, 0.0013204963060514051, 0.0013076817754618063, 0.0013383432049114182, 0.001310221532511772, 0.0013201366516058237, 0.00133723357207693, 0.0012995718778776272, 0.0013222386727907828, 0.0013108750216054674, 0.0013145458366607828, 0.0013031316327159197, 0.0013133964087928133, 0.0013062298768295013, 0.0013016276726765291, 0.001316196061860846, 0.0013215343482146154, 0.001308113367448808, 0.001311373428859729, 0.0013061913671162054, 0.0013083167949083205, 0.0013516843485246813, 0.001314477695684348, 0.0013916623251208542, 0.001357502142461587, 0.001310876040833489, 0.0013162612235553715, 0.001304046284141285, 0.001315688773780605, 0.0013066566126345067, 0.0013000362253348743, 0.0013129207761768177, 0.0013014830397061852, 0.001305098509967175, 0.0013081800401666943, 0.0013040036119862783, 0.0013253657356360738, 0.0013071344500141485, 0.0013224064486519415, 0.001294613388195938, 0.0012983569176867604, 0.001309994306434326, 0.0012981445309991132, 0.001299256102505083, 0.0012940635515034807, 0.0013084856327623129, 0.0012988804302615474, 0.001302787266690664, 0.001372465837689839, 0.0013441652658262423, 0.001305897836573422, 0.00131359246882553, 0.0013563763682863542, 0.0013066129188756554, 0.0012987088364529976, 0.0013098375710221578, 0.0013158881219522077, 0.0013176379178897763, 0.0013159075918208276, 0.0012986512441302137, 0.0013047692850612256, 0.0013021703272563766, 0.0013025903679924657, 0.0013611790396235123, 0.001350325346943371, 0.0013051407354674777, 0.0013056978754395125, 0.0013188052452074326, 0.001316600346139499, 0.0013012467546160429, 0.0013046994501230667, 0.0013070239386122142, 0.0013177836936309325, 0.0013040209792516365, 0.001306258080223081, 0.0013085681237090304, 0.0013168670821516793, 0.001591872939440821, 0.001558127998355396, 0.0014748978357267926, 0.0014586990404569981, 0.0014841772440098682, 0.00157509624425854, 0.0015566619802076295, 0.0014749318361282349, 0.0015651330195025218, 0.0015619971423542925, 0.0014694459802870239, 0.001583889509285135, 0.0015920605938121372, 0.001477782001566826, 0.0014937277954565932, 0.0015908041422502423, 0.0015917875286077662, 0.0015649829387702808, 0.0014742923667654395, 0.001555272694011884, 0.001563847487868399, 0.0015678985723845509, 0.0014770048580189444, 0.0015806109996094387, 0.001564496550804042, 0.0015792373273217557, 0.0015594893686321316, 0.0015673020816579157, 0.0015860712868446599, 0.0015784425931811637, 0.0015945137972582359, 0.0015614165720168728, 0.0015590718367650192, 0.001480669529196255, 0.001483167265066687, 0.0014780938568315944, 0.001666088469744641, 0.0015599518368134694, 0.0015675446935644259, 0.0015747081203272147, 0.0015964693679683367, 0.0015787635929882526, 0.0016373844498920502, 0.0015898306321884906, 0.0014932114079746666, 0.001682618019950329, 0.0015986805510878259, 0.001607002509871916, 0.0014768471627743269, 0.0015552983671540813, 0.0015663943890178082, 0.0015164164503162004, 0.0015862590623829439, 0.0015841119789651462, 0.0015769739402458072, 0.001501484857206898, 0.0015754244095474786, 0.0014993078978162032, 0.0015947053883680884, 0.0016850407772260358, 0.001587090407954339, 0.0015800574304992143, 0.0015938997135630675, 0.0014795950205274383, 0.0014721401230603152, 0.0015835798587840125, 0.0015760874066843974, 0.0015656932266619131, 0.0015670226943888226, 0.0014955600205694838, 0.001501505857105462, 0.0015735427340568633, 0.0014812735489056427, 0.0015723395912090735, 0.0015932078169164608, 0.0014828099389275421, 0.0015675291224211759, 0.001493030489061256, 0.0015560252858059747, 0.0015586776948743025, 0.0015682010407730633, 0.0014810986717098526, 0.0014583027341916245, 0.0014735173685856316, 0.0015534683871938257, 0.001554229631734898, 0.0016021580193001702, 0.0015950573689057206, 0.0016071027339607173, 0.0015865844289525127, 0.001565758856393549, 0.001550135407026629, 0.0014736755508739426, 0.0014535350831768785, 0.0014393112918090385, 0.001548563353329276, 0.0015021438739495352, 0.0014300803741207346, 0.0014559818349274185, 0.0016191261869001512, 0.0015126025852320406, 0.0015416108314335968, 0.0014641467084099229, 0.0015397637689602561, 0.001451239416686197, 0.0016190470220559898, 0.001594750853352404, 0.0015943515633504528, 0.0014452070827246644, 0.0016565163969062269, 0.0018366548756603152, 0.0016855664168057654, 0.001839822582648291, 0.0016747467282887858, 0.0016711020410487738, 0.001635486771798848, 0.0016912262508412823, 0.0015527771878017422, 0.00156863762337404, 0.0015356290192964177, 0.0014235454170072142, 0.0015954457097298775, 0.0015535465621117812, 0.00162990925067182, 0.0015327923125975456, 0.0016485518329621602, 0.0014537236056639813, 0.0015382190419283386, 0.0015465737305930816, 0.0015377466867600258, 0.001662963686006454, 0.0015384665433278617, 0.0015302868948007624, 0.0014679615439187426, 0.0016909328114707023, 0.0016086121468106285, 0.00160379637479006, 0.001819897957223778, 0.0016727956875305001, 0.0018854669372861583, 0.001608260999394891, 0.0016048323110832523, 0.0017044940614141524, 0.0016798649788446103, 0.0016045525432370293, 0.0017151869784963007, 0.0016541729564778507, 0.0016497334581799805, 0.001528322104907905, 0.001508466188776462, 0.001519273646408692, 0.0015149989994824864, 0.0014962469358579256, 0.0015232599592612435, 0.0015803907299414277, 0.0015199097518537503, 0.0015005288951215334, 0.0015185025016156335, 0.0014888470201791886, 0.0014883484157811229, 0.0014926715642407846, 0.0014787737066702296, 0.0014930402709675643, 0.001490893104346469, 0.0014914174147027854, 0.0014745543327686998, 0.0014912471888237633, 0.0015188256462958332, 0.0015249257291240308, 0.0015459879577974789, 0.0014992871874710545, 0.0015131920833179417, 0.0015242446049038942, 0.0015080676870032523, 0.0015200788766378537, 0.0015160557935208392, 0.001509037249585769, 0.0015195641026366502, 0.001513422646288139, 0.0015164873135897021, 0.001517255645012483, 0.0015104710619198158, 0.0015199747916388635, 0.0015717270192302142, 0.0015371742726226028, 0.0015108663962261442, 0.00151456831372343, 0.0015110842092932824, 0.001583506957103964]
[750.8424150137259, 741.8578123491528, 713.3273705867389, 708.4286824257216, 700.2670520006507, 713.7661424586389, 704.0677421801087, 711.1603618407283, 715.404278166816, 707.6355700274547, 682.7249143994133, 716.0405783801037, 664.1218063607556, 718.5323970614141, 719.0892299319851, 719.5959661725534, 704.0977989403324, 756.7014445003211, 749.1555530128066, 745.911843089576, 739.1785312105001, 750.7854209942375, 746.5781120858062, 751.1003965273183, 707.4580321130909, 637.8094261231935, 719.8925010178131, 765.9779004875958, 774.4600637402542, 756.8413605525942, 765.6238996412311, 689.5941225732602, 763.7968999528591, 764.0806018428317, 756.1925483865679, 755.7224733183866, 766.810453252957, 768.2118937233729, 766.0578113111587, 711.4035863315877, 759.1876702005353, 766.5495066376525, 762.8782923657085, 768.1917434040232, 764.1188738822589, 757.2910241530591, 764.712041388549, 747.1924961625876, 763.2297097750645, 757.4973384638573, 747.8125144935194, 769.4841793846234, 756.2931115071307, 762.8492293455027, 760.7190043218317, 767.3821852638566, 761.3847527717345, 765.5620329456967, 768.2688536758795, 759.7652272156125, 756.6961852720618, 764.4597363531906, 762.5592969879863, 765.5846035851459, 764.3408720974758, 739.817695670936, 760.7584390995518, 718.5651159401463, 736.6470878540782, 762.8486362174825, 759.727614932609, 766.8439473055209, 760.0581687160865, 765.3120110751825, 769.2093347186626, 761.6605800937718, 768.3542308978178, 766.2256851593152, 764.4207748901102, 766.8690414720437, 754.5087164337147, 765.0322428493687, 756.1971593676044, 772.4313753571745, 770.2042376619111, 763.3620963757471, 770.3302491521132, 769.6711972889022, 772.7595749359998, 764.2422468857559, 769.893807545192, 767.585027554189, 728.6155855676666, 743.956138001611, 765.7566863147006, 761.2711124128856, 737.2584950469168, 765.3376034736464, 769.9955308929568, 763.4534404289764, 759.943025031971, 758.9338363922618, 759.9317810882869, 770.0296784990657, 766.4190224657806, 767.9486923242691, 767.7010551991001, 734.6572132616657, 740.5622669111737, 766.2008952941142, 765.873958141648, 758.2620736716221, 759.5319285249876, 768.49375143692, 766.4600455726982, 765.0969278051562, 758.8498816863239, 766.8588281255177, 765.5455037102785, 764.194069748223, 759.3780826885443, 628.190840627815, 641.7957966582335, 678.013063533465, 685.5423718430009, 673.7739741233703, 634.8818389004156, 642.4002209308271, 677.9974338509411, 638.9233295441243, 640.2060368002772, 680.5285892882378, 631.3571711522575, 628.1167964879606, 676.6897952064274, 669.4660185354095, 628.6128967362812, 628.2245475780524, 638.9846018294434, 678.2915129608756, 642.9740609799189, 639.4485445400109, 637.7963585228234, 677.0458435331523, 632.666734729225, 639.1832564194979, 633.2170489510338, 641.2355352426197, 638.0390938689911, 630.4886850258832, 633.535869039506, 627.1504214761255, 640.4440800242729, 641.4072632310132, 675.3701486264969, 674.2327878676836, 676.5470239782846, 600.2082231283124, 641.0454325581685, 637.940343331525, 635.038320493455, 626.3822031691079, 633.4070562820743, 610.7301190419441, 628.9978188578768, 669.6975355662203, 594.3119520552383, 625.515835117621, 622.276563886452, 677.1181373443227, 642.9634474765262, 638.4088241193458, 659.4494538696687, 630.414050084454, 631.2685045493253, 634.1258878660527, 666.0073827585758, 634.749591246487, 666.9744096302945, 627.0750743642568, 593.4574483391611, 630.083828235682, 632.888387913883, 627.3920444872656, 675.8606146454354, 679.2831635627044, 631.4806256552622, 634.4825773994934, 638.6947219105101, 638.1528509962164, 668.6458492111985, 665.998068051334, 635.5086381555259, 675.0947525788163, 635.9949247547951, 627.6645076569032, 674.395263848353, 637.9466803496568, 669.7786865884774, 642.6630782429926, 641.5694554996784, 637.6733428942491, 675.1744627827876, 685.7286738575076, 678.6482611737781, 643.7208560171558, 643.405568637729, 624.158159153867, 626.936698011084, 622.2377567210605, 630.2847688100754, 638.6679506340618, 645.1049343606287, 678.5754160113222, 687.977890299268, 694.776735019651, 645.7598249694385, 665.7151936922888, 699.2613968392066, 686.8217556092316, 617.6170875937221, 661.1121848946167, 648.6721418985269, 682.9916662422507, 649.450272930673, 689.0661792272907, 617.64728656869, 627.0571969896432, 627.2142374286311, 691.9423603395918, 603.6764875178043, 544.4681051689035, 593.272380150437, 543.5306694412744, 597.1052118559891, 598.4075032141101, 611.4387577101065, 591.2869431293186, 644.0074003248941, 637.4958659024533, 651.1989467730768, 702.4714407091743, 626.7840979492235, 643.6884637951699, 613.5310905118291, 652.4041070543672, 606.5929987795254, 687.8886716180514, 650.1024709370275, 646.5905764586594, 650.3021652460603, 601.3360414390426, 649.9978854508573, 653.4722367404161, 681.2167553997954, 591.3895532787267, 621.653890891403, 623.5205514359028, 549.4813574742851, 597.8016367774543, 530.3725990758275, 621.7896226895077, 623.1180623008556, 586.6843555737231, 595.2859382114075, 623.2267084146692, 583.0268142991017, 604.531706363554, 606.1585252100181, 654.3123316666671, 662.9250343430726, 658.209271492224, 660.066442513555, 668.338879121323, 656.4867630899874, 632.7549137402634, 657.9338008591333, 666.431685021971, 658.5435314963491, 671.6606786637126, 671.8856884563381, 669.9397402325598, 676.2359889747503, 669.7742984199283, 670.738899445342, 670.5031000320479, 678.1710092176448, 670.5796379665, 658.4034200626228, 655.7696423513256, 646.8355687742024, 666.9836228553152, 660.8546337404323, 656.0626796924444, 663.1002100357604, 657.8605987945991, 659.606331293146, 662.6741654485337, 658.0834584502649, 660.7539555805027, 659.4186387440878, 659.0847121163769, 662.045123015462, 657.9056478441872, 636.2428002858732, 650.5443252663088, 661.8718918481536, 660.2541403639892, 661.7764872731277, 631.5096978347824]
Elapsed: 0.07151379881073691~0.0058621143128609
Time per graph: 0.0014693288407077222~0.00012649835018584577
Speed: 685.6294616443278~58.85404645808709
Total Time: 0.0770
best val loss: 0.3576158881187439 test_score: 0.8750

Testing...
Test loss: 0.4752 score: 0.8542 time: 0.07s
test Score 0.8542
Epoch Time List: [0.5299562689615414, 0.3249913399340585, 0.37782898696605116, 0.3470770039130002, 0.34465145389549434, 0.34934161708224565, 0.34411121904850006, 0.43708249798510224, 0.3410435769474134, 0.3544995959382504, 0.34550271602347493, 0.34416408312972635, 0.38285754807293415, 0.3390548031311482, 0.3352823421591893, 0.33720499195624143, 0.3436560840345919, 0.33218849590048194, 0.32713768805842847, 0.3251423460897058, 0.33710755105130374, 0.32861237903125584, 0.32182293292135, 0.3277770889690146, 0.33789514005184174, 0.37218618392944336, 0.33123205008450896, 0.32162401091773063, 0.31841617391910404, 0.3250441069249064, 0.32241208001505584, 0.34077863907441497, 0.3368464719969779, 0.3228285030927509, 0.3244704679818824, 0.342187233036384, 0.3232299428200349, 0.31645994482096285, 0.32015796413179487, 0.3257771269418299, 0.31765615998301655, 0.31771656998898834, 0.3176950308261439, 0.3218672600341961, 0.3152982610045001, 0.31674742489121854, 0.31809334398712963, 0.32901491585653275, 0.3227883198997006, 0.3204461318673566, 0.32826824102085084, 0.32055000599939376, 0.32247453404124826, 0.3249742879997939, 0.3287981868488714, 0.3178873991128057, 0.31931880803313106, 0.31712431099731475, 0.3266589851118624, 0.3313340420136228, 0.3325974551262334, 0.3239172729663551, 0.3170671248808503, 0.3164568069623783, 0.3161967920605093, 0.32265342690516263, 0.31798643979709595, 0.3396836328320205, 0.3280622478341684, 0.32233347196597606, 0.32231633295305073, 0.3174902191385627, 0.3215484629618004, 0.3219075670931488, 0.3165254320483655, 0.31825117801781744, 0.3177497610449791, 0.31757571490015835, 0.31837307405658066, 0.31745706300716847, 0.3253294990863651, 0.33180368575267494, 0.3160199917620048, 0.31606739899143577, 0.31501499889418483, 0.3220859010471031, 0.32210135902278125, 0.3156146449036896, 0.31666590610984713, 0.3157113790512085, 0.31477047596126795, 0.3217880839947611, 0.3187277818797156, 0.3349200049415231, 0.32071113493293524, 0.3166853829752654, 0.3267035789322108, 0.32088508002925664, 0.315594902029261, 0.3227737529668957, 0.3202934470027685, 0.327745946822688, 0.3219149369979277, 0.31667311710771173, 0.3163306890055537, 0.3169822500785813, 0.31869700900278986, 0.328618983970955, 0.33202553703449667, 0.32429751206655055, 0.32168181403540075, 0.3251975899329409, 0.31993843999225646, 0.3155595859279856, 0.3199309560004622, 0.3150963799562305, 0.3194340330082923, 0.31632508779875934, 0.31520721409469843, 0.3188969721086323, 0.31701952894218266, 0.33168514596764, 0.3315408378839493, 0.3212377140298486, 0.31937649205792695, 0.31982773391064256, 0.3306478480808437, 0.3261004379019141, 0.32031427894253284, 0.3284944898914546, 0.32606255589053035, 0.31997540302108973, 0.32923831895459443, 0.3306626809062436, 0.362007844960317, 0.32826848793774843, 0.3388450079364702, 0.3397542161401361, 0.3241267391713336, 0.3239083919906989, 0.32954388088546693, 0.3247436349047348, 0.32483208808116615, 0.3226708780275658, 0.3292608450865373, 0.3254926410736516, 0.32675802090670913, 0.32497825298924, 0.324847248964943, 0.32632702589035034, 0.3327916549751535, 0.33523043885361403, 0.33550408901646733, 0.3265922499122098, 0.32209420995786786, 0.3260112040443346, 0.3281013040104881, 0.33622342301532626, 0.32652741204947233, 0.3263934721471742, 0.3267580800456926, 0.32819292799104005, 0.33308150083757937, 0.33715132297948003, 0.3329106420278549, 0.32930638594552875, 0.3531855558976531, 0.34135908901225775, 0.328482081880793, 0.41124358610250056, 0.333962582051754, 0.3266004570759833, 0.3268399579683319, 0.33834079082589597, 0.381393603165634, 0.328421808895655, 0.3254147480474785, 0.3336581760086119, 0.32612580293789506, 0.3608776959590614, 0.34147049207240343, 0.3396816609892994, 0.33152906026225537, 0.3343586709816009, 0.3849322630558163, 0.3309821120928973, 0.33595656300894916, 0.3341317649465054, 0.3320712459972128, 0.3266744740540162, 0.32786396087612957, 0.3371122688986361, 0.3341060639359057, 0.32867328892461956, 0.33673972194083035, 0.3292207058984786, 0.3263541730120778, 0.33074993710033596, 0.3247034989763051, 0.33020359301008284, 0.3283648800570518, 0.3288353179814294, 0.3293006969615817, 0.32864251104183495, 0.32824850804172456, 0.3280089688487351, 0.3337572200689465, 0.33023450686596334, 0.33157219213899225, 0.33626633894164115, 0.333142188959755, 0.3470884069101885, 0.3271048270398751, 0.3233879440231249, 0.3459167949622497, 0.33424295391887426, 0.35211306798737496, 0.3286297209560871, 0.33724190504290164, 0.3393295629648492, 0.358697900082916, 0.3407281670952216, 0.34432563302107155, 0.3390057160286233, 0.3462225840194151, 0.34061199391726404, 0.3450644159456715, 0.36183585808612406, 0.3593006319133565, 0.3417081399820745, 0.3694595340639353, 0.3764054630883038, 0.37166635401081294, 0.38750511908438057, 0.37659421004354954, 0.3661738029913977, 0.3682402060367167, 0.3764494371134788, 0.3621905440231785, 0.3762880970025435, 0.3510511399945244, 0.3282387520885095, 0.35177746694535017, 0.3360944300657138, 0.3571278810268268, 0.3382005759049207, 0.3492130000377074, 0.33879957790486515, 0.3550158290890977, 0.33797329489607364, 0.35078735603019595, 0.33281351113691926, 0.33948344900272787, 0.34987108781933784, 0.33686249796301126, 0.385166377061978, 0.37174407590646297, 0.3730261350283399, 0.3816929468885064, 0.37519945087842643, 0.3788433250738308, 0.37902738410048187, 0.3794173498172313, 0.38318822195287794, 0.37314716703258455, 0.371636922005564, 0.37591037806123495, 0.3798825910780579, 0.38723277393728495, 0.3426564511610195, 0.3409580289153382, 0.3422668760176748, 0.3422765930881724, 0.34148733888287097, 0.3404053229605779, 0.3508487300714478, 0.3420139020308852, 0.34333540801890194, 0.3461010539904237, 0.3409536110702902, 0.3371246890164912, 0.3371287470217794, 0.33447065600194037, 0.3372036281507462, 0.3386817940045148, 0.3387900691013783, 0.3383709950139746, 0.3384924510028213, 0.3422763239359483, 0.3432968700071797, 0.3524882330093533, 0.34726301208138466, 0.34287867904640734, 0.34512110298965126, 0.34460061113350093, 0.3421923719579354, 0.342733831028454, 0.3434996160212904, 0.3425416029058397, 0.34327398613095284, 0.34518439904786646, 0.342892070999369, 0.34443200286477804, 0.3446449760813266, 0.34672050504013896, 0.34696094796527177, 0.33966486691497266, 0.34023621201049536, 0.34007966006174684, 0.3561698099365458]
Total Epoch List: [121, 93, 96]
Total Time List: [0.06507592007983476, 0.07295708905439824, 0.07700601196847856]
========================training times:1========================
========================k_idx:0========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: GPSTransformer
GPS(
  (x_lin): Linear(in_features=14887, out_features=64, bias=True)
  (pe_lin): Linear(in_features=20, out_features=8, bias=True)
  (pe_norm): BatchNorm1d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (convs): ModuleList(
    (0-1): 2 x GPSConv(64, conv=GINEConv(nn=Sequential(
      (0): Linear(in_features=64, out_features=64, bias=True)
      (1): ReLU()
      (2): Linear(in_features=64, out_features=64, bias=True)
    )), heads=1, attn_type=multihead)
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=32, bias=True)
    (1): ReLU()
    (2): Linear(in_features=32, out_features=16, bias=True)
    (3): ReLU()
    (4): Linear(in_features=16, out_features=2, bias=True)
  )
)
Total number of parameters: 1039906
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7d29a7d73460>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.7376;  Loss pred: 0.7376; Loss self: 0.0000; time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.8514 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.8803 score: 0.5102 time: 0.06s
Epoch 2/1000, LR 0.000000
Train loss: 0.7167;  Loss pred: 0.7167; Loss self: 0.0000; time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7495 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7796 score: 0.5102 time: 0.06s
Epoch 3/1000, LR 0.000030
Train loss: 0.6977;  Loss pred: 0.6977; Loss self: 0.0000; time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6946 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7235 score: 0.5102 time: 0.07s
Epoch 4/1000, LR 0.000060
Train loss: 0.6450;  Loss pred: 0.6450; Loss self: 0.0000; time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6660 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6934 score: 0.5102 time: 0.07s
Epoch 5/1000, LR 0.000090
Train loss: 0.5873;  Loss pred: 0.5873; Loss self: 0.0000; time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6502 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6775 score: 0.5102 time: 0.06s
Epoch 6/1000, LR 0.000120
Train loss: 0.5428;  Loss pred: 0.5428; Loss self: 0.0000; time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6409 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6687 score: 0.5102 time: 0.08s
Epoch 7/1000, LR 0.000150
Train loss: 0.5054;  Loss pred: 0.5054; Loss self: 0.0000; time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6361 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6626 score: 0.5102 time: 0.06s
Epoch 8/1000, LR 0.000180
Train loss: 0.4774;  Loss pred: 0.4774; Loss self: 0.0000; time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6333 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6593 score: 0.5102 time: 0.06s
Epoch 9/1000, LR 0.000210
Train loss: 0.4556;  Loss pred: 0.4556; Loss self: 0.0000; time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6307 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6556 score: 0.5102 time: 0.06s
Epoch 10/1000, LR 0.000240
Train loss: 0.4403;  Loss pred: 0.4403; Loss self: 0.0000; time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6278 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6531 score: 0.5102 time: 0.06s
Epoch 11/1000, LR 0.000270
Train loss: 0.4214;  Loss pred: 0.4214; Loss self: 0.0000; time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6262 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6521 score: 0.5102 time: 0.06s
Epoch 12/1000, LR 0.000270
Train loss: 0.3983;  Loss pred: 0.3983; Loss self: 0.0000; time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6269 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6506 score: 0.5102 time: 0.06s
     INFO: Early stopping counter 1 of 20
Epoch 13/1000, LR 0.000270
Train loss: 0.3688;  Loss pred: 0.3688; Loss self: 0.0000; time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6280 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6493 score: 0.5102 time: 0.06s
     INFO: Early stopping counter 2 of 20
Epoch 14/1000, LR 0.000270
Train loss: 0.3397;  Loss pred: 0.3397; Loss self: 0.0000; time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6294 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6490 score: 0.5102 time: 0.06s
     INFO: Early stopping counter 3 of 20
Epoch 15/1000, LR 0.000270
Train loss: 0.3130;  Loss pred: 0.3130; Loss self: 0.0000; time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6323 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6500 score: 0.5102 time: 0.06s
     INFO: Early stopping counter 4 of 20
Epoch 16/1000, LR 0.000270
Train loss: 0.2880;  Loss pred: 0.2880; Loss self: 0.0000; time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6340 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6497 score: 0.5102 time: 0.06s
     INFO: Early stopping counter 5 of 20
Epoch 17/1000, LR 0.000270
Train loss: 0.2644;  Loss pred: 0.2644; Loss self: 0.0000; time: 0.32s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6350 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6489 score: 0.5102 time: 0.07s
     INFO: Early stopping counter 6 of 20
Epoch 18/1000, LR 0.000270
Train loss: 0.2399;  Loss pred: 0.2399; Loss self: 0.0000; time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6363 score: 0.4898 time: 0.09s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6484 score: 0.5102 time: 0.07s
     INFO: Early stopping counter 7 of 20
Epoch 19/1000, LR 0.000270
Train loss: 0.2198;  Loss pred: 0.2198; Loss self: 0.0000; time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6375 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6481 score: 0.5102 time: 0.07s
     INFO: Early stopping counter 8 of 20
Epoch 20/1000, LR 0.000270
Train loss: 0.2000;  Loss pred: 0.2000; Loss self: 0.0000; time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6387 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6476 score: 0.5102 time: 0.07s
     INFO: Early stopping counter 9 of 20
Epoch 21/1000, LR 0.000270
Train loss: 0.1830;  Loss pred: 0.1830; Loss self: 0.0000; time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6395 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6465 score: 0.5102 time: 0.15s
     INFO: Early stopping counter 10 of 20
Epoch 22/1000, LR 0.000270
Train loss: 0.1678;  Loss pred: 0.1678; Loss self: 0.0000; time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6393 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6449 score: 0.5102 time: 0.07s
     INFO: Early stopping counter 11 of 20
Epoch 23/1000, LR 0.000270
Train loss: 0.1560;  Loss pred: 0.1560; Loss self: 0.0000; time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6378 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6431 score: 0.5102 time: 0.07s
     INFO: Early stopping counter 12 of 20
Epoch 24/1000, LR 0.000270
Train loss: 0.1402;  Loss pred: 0.1402; Loss self: 0.0000; time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6358 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6398 score: 0.5102 time: 0.07s
     INFO: Early stopping counter 13 of 20
Epoch 25/1000, LR 0.000270
Train loss: 0.1265;  Loss pred: 0.1265; Loss self: 0.0000; time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6335 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6352 score: 0.5102 time: 0.07s
     INFO: Early stopping counter 14 of 20
Epoch 26/1000, LR 0.000270
Train loss: 0.1173;  Loss pred: 0.1173; Loss self: 0.0000; time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6304 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6299 score: 0.5102 time: 0.06s
     INFO: Early stopping counter 15 of 20
Epoch 27/1000, LR 0.000270
Train loss: 0.1059;  Loss pred: 0.1059; Loss self: 0.0000; time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6263 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6244 score: 0.5102 time: 0.06s
     INFO: Early stopping counter 16 of 20
Epoch 28/1000, LR 0.000270
Train loss: 0.0977;  Loss pred: 0.0977; Loss self: 0.0000; time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6216 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6191 score: 0.5102 time: 0.06s
Epoch 29/1000, LR 0.000270
Train loss: 0.0904;  Loss pred: 0.0904; Loss self: 0.0000; time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6167 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6139 score: 0.5102 time: 0.06s
Epoch 30/1000, LR 0.000270
Train loss: 0.0810;  Loss pred: 0.0810; Loss self: 0.0000; time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6123 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6081 score: 0.5102 time: 0.07s
Epoch 31/1000, LR 0.000270
Train loss: 0.0747;  Loss pred: 0.0747; Loss self: 0.0000; time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6083 score: 0.4898 time: 0.08s
Test loss: 0.6022 score: 0.5306 time: 0.06s
Epoch 32/1000, LR 0.000270
Train loss: 0.0674;  Loss pred: 0.0674; Loss self: 0.0000; time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6044 score: 0.4898 time: 0.10s
Test loss: 0.5965 score: 0.5306 time: 0.06s
Epoch 33/1000, LR 0.000270
Train loss: 0.0691;  Loss pred: 0.0691; Loss self: 0.0000; time: 0.20s
Val loss: 0.6000 score: 0.5102 time: 0.07s
Test loss: 0.5913 score: 0.5306 time: 0.06s
Epoch 34/1000, LR 0.000270
Train loss: 0.0611;  Loss pred: 0.0611; Loss self: 0.0000; time: 0.20s
Val loss: 0.5940 score: 0.5306 time: 0.07s
Test loss: 0.5863 score: 0.6122 time: 0.06s
Epoch 35/1000, LR 0.000270
Train loss: 0.0522;  Loss pred: 0.0522; Loss self: 0.0000; time: 0.20s
Val loss: 0.5883 score: 0.5510 time: 0.07s
Test loss: 0.5810 score: 0.6327 time: 0.07s
Epoch 36/1000, LR 0.000270
Train loss: 0.0516;  Loss pred: 0.0516; Loss self: 0.0000; time: 0.22s
Val loss: 0.5827 score: 0.6122 time: 0.08s
Test loss: 0.5757 score: 0.6735 time: 0.07s
Epoch 37/1000, LR 0.000270
Train loss: 0.0473;  Loss pred: 0.0473; Loss self: 0.0000; time: 0.22s
Val loss: 0.5776 score: 0.6327 time: 0.08s
Test loss: 0.5706 score: 0.6939 time: 0.07s
Epoch 38/1000, LR 0.000270
Train loss: 0.0423;  Loss pred: 0.0423; Loss self: 0.0000; time: 0.22s
Val loss: 0.5733 score: 0.6531 time: 0.08s
Test loss: 0.5654 score: 0.7143 time: 0.07s
Epoch 39/1000, LR 0.000269
Train loss: 0.0457;  Loss pred: 0.0457; Loss self: 0.0000; time: 0.22s
Val loss: 0.5697 score: 0.7347 time: 0.08s
Test loss: 0.5611 score: 0.7347 time: 0.07s
Epoch 40/1000, LR 0.000269
Train loss: 0.0347;  Loss pred: 0.0347; Loss self: 0.0000; time: 0.20s
Val loss: 0.5667 score: 0.7347 time: 0.07s
Test loss: 0.5568 score: 0.7551 time: 0.06s
Epoch 41/1000, LR 0.000269
Train loss: 0.0345;  Loss pred: 0.0345; Loss self: 0.0000; time: 0.20s
Val loss: 0.5646 score: 0.7959 time: 0.07s
Test loss: 0.5536 score: 0.7551 time: 0.06s
Epoch 42/1000, LR 0.000269
Train loss: 0.0374;  Loss pred: 0.0374; Loss self: 0.0000; time: 0.20s
Val loss: 0.5611 score: 0.7755 time: 0.07s
Test loss: 0.5504 score: 0.7551 time: 0.06s
Epoch 43/1000, LR 0.000269
Train loss: 0.0286;  Loss pred: 0.0286; Loss self: 0.0000; time: 0.20s
Val loss: 0.5567 score: 0.7755 time: 0.07s
Test loss: 0.5472 score: 0.7551 time: 0.06s
Epoch 44/1000, LR 0.000269
Train loss: 0.0256;  Loss pred: 0.0256; Loss self: 0.0000; time: 0.20s
Val loss: 0.5514 score: 0.7755 time: 0.07s
Test loss: 0.5465 score: 0.7551 time: 0.06s
Epoch 45/1000, LR 0.000269
Train loss: 0.0234;  Loss pred: 0.0234; Loss self: 0.0000; time: 0.20s
Val loss: 0.5461 score: 0.7959 time: 0.07s
Test loss: 0.5455 score: 0.7551 time: 0.06s
Epoch 46/1000, LR 0.000269
Train loss: 0.0241;  Loss pred: 0.0241; Loss self: 0.0000; time: 0.20s
Val loss: 0.5409 score: 0.7959 time: 0.08s
Test loss: 0.5414 score: 0.7551 time: 0.06s
Epoch 47/1000, LR 0.000269
Train loss: 0.0190;  Loss pred: 0.0190; Loss self: 0.0000; time: 0.20s
Val loss: 0.5342 score: 0.7959 time: 0.07s
Test loss: 0.5358 score: 0.7551 time: 0.06s
Epoch 48/1000, LR 0.000269
Train loss: 0.0222;  Loss pred: 0.0222; Loss self: 0.0000; time: 0.20s
Val loss: 0.5260 score: 0.8163 time: 0.07s
Test loss: 0.5287 score: 0.7551 time: 0.06s
Epoch 49/1000, LR 0.000269
Train loss: 0.0170;  Loss pred: 0.0170; Loss self: 0.0000; time: 0.20s
Val loss: 0.5172 score: 0.8163 time: 0.07s
Test loss: 0.5208 score: 0.7755 time: 0.06s
Epoch 50/1000, LR 0.000269
Train loss: 0.0160;  Loss pred: 0.0160; Loss self: 0.0000; time: 0.20s
Val loss: 0.5082 score: 0.8163 time: 0.07s
Test loss: 0.5132 score: 0.7959 time: 0.06s
Epoch 51/1000, LR 0.000269
Train loss: 0.0172;  Loss pred: 0.0172; Loss self: 0.0000; time: 0.20s
Val loss: 0.4994 score: 0.8367 time: 0.07s
Test loss: 0.5043 score: 0.8571 time: 0.06s
Epoch 52/1000, LR 0.000269
Train loss: 0.0136;  Loss pred: 0.0136; Loss self: 0.0000; time: 0.20s
Val loss: 0.4908 score: 0.8571 time: 0.07s
Test loss: 0.4953 score: 0.8571 time: 0.06s
Epoch 53/1000, LR 0.000269
Train loss: 0.0133;  Loss pred: 0.0133; Loss self: 0.0000; time: 0.20s
Val loss: 0.4820 score: 0.8980 time: 0.07s
Test loss: 0.4866 score: 0.8571 time: 0.06s
Epoch 54/1000, LR 0.000269
Train loss: 0.0121;  Loss pred: 0.0121; Loss self: 0.0000; time: 0.20s
Val loss: 0.4736 score: 0.8980 time: 0.07s
Test loss: 0.4776 score: 0.8571 time: 0.06s
Epoch 55/1000, LR 0.000269
Train loss: 0.0112;  Loss pred: 0.0112; Loss self: 0.0000; time: 0.20s
Val loss: 0.4656 score: 0.8980 time: 0.07s
Test loss: 0.4688 score: 0.8367 time: 0.06s
Epoch 56/1000, LR 0.000269
Train loss: 0.0089;  Loss pred: 0.0089; Loss self: 0.0000; time: 0.20s
Val loss: 0.4575 score: 0.9388 time: 0.07s
Test loss: 0.4606 score: 0.8367 time: 0.06s
Epoch 57/1000, LR 0.000269
Train loss: 0.0107;  Loss pred: 0.0107; Loss self: 0.0000; time: 0.20s
Val loss: 0.4487 score: 0.9388 time: 0.07s
Test loss: 0.4530 score: 0.8367 time: 0.06s
Epoch 58/1000, LR 0.000269
Train loss: 0.0092;  Loss pred: 0.0092; Loss self: 0.0000; time: 0.20s
Val loss: 0.4398 score: 0.9388 time: 0.07s
Test loss: 0.4460 score: 0.8367 time: 0.06s
Epoch 59/1000, LR 0.000268
Train loss: 0.0077;  Loss pred: 0.0077; Loss self: 0.0000; time: 0.20s
Val loss: 0.4307 score: 0.9388 time: 0.07s
Test loss: 0.4393 score: 0.8571 time: 0.06s
Epoch 60/1000, LR 0.000268
Train loss: 0.0072;  Loss pred: 0.0072; Loss self: 0.0000; time: 0.20s
Val loss: 0.4209 score: 0.9388 time: 0.07s
Test loss: 0.4326 score: 0.8571 time: 0.06s
Epoch 61/1000, LR 0.000268
Train loss: 0.0068;  Loss pred: 0.0068; Loss self: 0.0000; time: 0.20s
Val loss: 0.4110 score: 0.9592 time: 0.07s
Test loss: 0.4258 score: 0.8980 time: 0.06s
Epoch 62/1000, LR 0.000268
Train loss: 0.0081;  Loss pred: 0.0081; Loss self: 0.0000; time: 0.20s
Val loss: 0.4011 score: 0.9592 time: 0.07s
Test loss: 0.4186 score: 0.8980 time: 0.07s
Epoch 63/1000, LR 0.000268
Train loss: 0.0083;  Loss pred: 0.0083; Loss self: 0.0000; time: 0.20s
Val loss: 0.3909 score: 0.9388 time: 0.07s
Test loss: 0.4124 score: 0.8980 time: 0.06s
Epoch 64/1000, LR 0.000268
Train loss: 0.0064;  Loss pred: 0.0064; Loss self: 0.0000; time: 0.20s
Val loss: 0.3813 score: 0.9388 time: 0.07s
Test loss: 0.4059 score: 0.8980 time: 0.06s
Epoch 65/1000, LR 0.000268
Train loss: 0.0050;  Loss pred: 0.0050; Loss self: 0.0000; time: 0.20s
Val loss: 0.3721 score: 0.9388 time: 0.07s
Test loss: 0.3992 score: 0.8980 time: 0.06s
Epoch 66/1000, LR 0.000268
Train loss: 0.0061;  Loss pred: 0.0061; Loss self: 0.0000; time: 0.20s
Val loss: 0.3615 score: 0.9388 time: 0.07s
Test loss: 0.3918 score: 0.8980 time: 0.06s
Epoch 67/1000, LR 0.000268
Train loss: 0.0066;  Loss pred: 0.0066; Loss self: 0.0000; time: 0.20s
Val loss: 0.3500 score: 0.9388 time: 0.07s
Test loss: 0.3844 score: 0.8980 time: 0.06s
Epoch 68/1000, LR 0.000268
Train loss: 0.0052;  Loss pred: 0.0052; Loss self: 0.0000; time: 0.20s
Val loss: 0.3378 score: 0.9388 time: 0.07s
Test loss: 0.3758 score: 0.8980 time: 0.06s
Epoch 69/1000, LR 0.000268
Train loss: 0.0054;  Loss pred: 0.0054; Loss self: 0.0000; time: 0.20s
Val loss: 0.3255 score: 0.9184 time: 0.07s
Test loss: 0.3670 score: 0.8980 time: 0.06s
Epoch 70/1000, LR 0.000268
Train loss: 0.0055;  Loss pred: 0.0055; Loss self: 0.0000; time: 0.20s
Val loss: 0.3124 score: 0.8980 time: 0.07s
Test loss: 0.3588 score: 0.8980 time: 0.06s
Epoch 71/1000, LR 0.000268
Train loss: 0.0038;  Loss pred: 0.0038; Loss self: 0.0000; time: 0.20s
Val loss: 0.2990 score: 0.9184 time: 0.07s
Test loss: 0.3507 score: 0.8980 time: 0.06s
Epoch 72/1000, LR 0.000267
Train loss: 0.0042;  Loss pred: 0.0042; Loss self: 0.0000; time: 0.20s
Val loss: 0.2852 score: 0.9184 time: 0.07s
Test loss: 0.3432 score: 0.8980 time: 0.06s
Epoch 73/1000, LR 0.000267
Train loss: 0.0042;  Loss pred: 0.0042; Loss self: 0.0000; time: 0.20s
Val loss: 0.2716 score: 0.9184 time: 0.07s
Test loss: 0.3368 score: 0.8980 time: 0.06s
Epoch 74/1000, LR 0.000267
Train loss: 0.0056;  Loss pred: 0.0056; Loss self: 0.0000; time: 0.20s
Val loss: 0.2591 score: 0.9184 time: 0.07s
Test loss: 0.3309 score: 0.8980 time: 0.06s
Epoch 75/1000, LR 0.000267
Train loss: 0.0030;  Loss pred: 0.0030; Loss self: 0.0000; time: 0.20s
Val loss: 0.2482 score: 0.9184 time: 0.07s
Test loss: 0.3256 score: 0.9184 time: 0.06s
Epoch 76/1000, LR 0.000267
Train loss: 0.0042;  Loss pred: 0.0042; Loss self: 0.0000; time: 0.20s
Val loss: 0.2388 score: 0.9184 time: 0.07s
Test loss: 0.3203 score: 0.9184 time: 0.06s
Epoch 77/1000, LR 0.000267
Train loss: 0.0030;  Loss pred: 0.0030; Loss self: 0.0000; time: 0.20s
Val loss: 0.2300 score: 0.9184 time: 0.07s
Test loss: 0.3158 score: 0.9184 time: 0.06s
Epoch 78/1000, LR 0.000267
Train loss: 0.0036;  Loss pred: 0.0036; Loss self: 0.0000; time: 0.20s
Val loss: 0.2221 score: 0.9184 time: 0.07s
Test loss: 0.3117 score: 0.9184 time: 0.06s
Epoch 79/1000, LR 0.000267
Train loss: 0.0026;  Loss pred: 0.0026; Loss self: 0.0000; time: 0.20s
Val loss: 0.2148 score: 0.9184 time: 0.07s
Test loss: 0.3083 score: 0.9184 time: 0.06s
Epoch 80/1000, LR 0.000267
Train loss: 0.0034;  Loss pred: 0.0034; Loss self: 0.0000; time: 0.20s
Val loss: 0.2078 score: 0.9184 time: 0.07s
Test loss: 0.3057 score: 0.9184 time: 0.06s
Epoch 81/1000, LR 0.000267
Train loss: 0.0029;  Loss pred: 0.0029; Loss self: 0.0000; time: 0.20s
Val loss: 0.2013 score: 0.9184 time: 0.07s
Test loss: 0.3041 score: 0.9184 time: 0.06s
Epoch 82/1000, LR 0.000267
Train loss: 0.0029;  Loss pred: 0.0029; Loss self: 0.0000; time: 0.20s
Val loss: 0.1955 score: 0.9184 time: 0.07s
Test loss: 0.3031 score: 0.9184 time: 0.06s
Epoch 83/1000, LR 0.000266
Train loss: 0.0026;  Loss pred: 0.0026; Loss self: 0.0000; time: 0.20s
Val loss: 0.1905 score: 0.9184 time: 0.07s
Test loss: 0.3027 score: 0.9184 time: 0.06s
Epoch 84/1000, LR 0.000266
Train loss: 0.0037;  Loss pred: 0.0037; Loss self: 0.0000; time: 0.20s
Val loss: 0.1858 score: 0.9184 time: 0.08s
Test loss: 0.3030 score: 0.9184 time: 0.06s
Epoch 85/1000, LR 0.000266
Train loss: 0.0025;  Loss pred: 0.0025; Loss self: 0.0000; time: 0.20s
Val loss: 0.1813 score: 0.9184 time: 0.07s
Test loss: 0.3034 score: 0.9184 time: 0.06s
Epoch 86/1000, LR 0.000266
Train loss: 0.0019;  Loss pred: 0.0019; Loss self: 0.0000; time: 0.20s
Val loss: 0.1775 score: 0.9184 time: 0.08s
Test loss: 0.3046 score: 0.9184 time: 0.07s
Epoch 87/1000, LR 0.000266
Train loss: 0.0022;  Loss pred: 0.0022; Loss self: 0.0000; time: 0.22s
Val loss: 0.1741 score: 0.9184 time: 0.08s
Test loss: 0.3067 score: 0.9184 time: 0.07s
Epoch 88/1000, LR 0.000266
Train loss: 0.0024;  Loss pred: 0.0024; Loss self: 0.0000; time: 0.22s
Val loss: 0.1721 score: 0.9184 time: 0.08s
Test loss: 0.3094 score: 0.9184 time: 0.07s
Epoch 89/1000, LR 0.000266
Train loss: 0.0023;  Loss pred: 0.0023; Loss self: 0.0000; time: 0.22s
Val loss: 0.1709 score: 0.9184 time: 0.07s
Test loss: 0.3122 score: 0.9184 time: 0.06s
Epoch 90/1000, LR 0.000266
Train loss: 0.0034;  Loss pred: 0.0034; Loss self: 0.0000; time: 0.21s
Val loss: 0.1698 score: 0.9184 time: 0.08s
Test loss: 0.3155 score: 0.8980 time: 0.07s
Epoch 91/1000, LR 0.000266
Train loss: 0.0021;  Loss pred: 0.0021; Loss self: 0.0000; time: 0.22s
Val loss: 0.1691 score: 0.9184 time: 0.08s
Test loss: 0.3191 score: 0.8980 time: 0.07s
Epoch 92/1000, LR 0.000266
Train loss: 0.0027;  Loss pred: 0.0027; Loss self: 0.0000; time: 0.22s
Val loss: 0.1691 score: 0.9184 time: 0.08s
Test loss: 0.3233 score: 0.8980 time: 0.07s
Epoch 93/1000, LR 0.000265
Train loss: 0.0021;  Loss pred: 0.0021; Loss self: 0.0000; time: 0.22s
Val loss: 0.1693 score: 0.9184 time: 0.08s
Test loss: 0.3284 score: 0.8980 time: 0.07s
     INFO: Early stopping counter 1 of 20
Epoch 94/1000, LR 0.000265
Train loss: 0.0024;  Loss pred: 0.0024; Loss self: 0.0000; time: 0.22s
Val loss: 0.1690 score: 0.9184 time: 0.08s
Test loss: 0.3336 score: 0.8980 time: 0.07s
Epoch 95/1000, LR 0.000265
Train loss: 0.0029;  Loss pred: 0.0029; Loss self: 0.0000; time: 0.22s
Val loss: 0.1688 score: 0.9184 time: 0.08s
Test loss: 0.3393 score: 0.8980 time: 0.07s
Epoch 96/1000, LR 0.000265
Train loss: 0.0016;  Loss pred: 0.0016; Loss self: 0.0000; time: 0.22s
Val loss: 0.1692 score: 0.9184 time: 0.07s
Test loss: 0.3446 score: 0.8980 time: 0.07s
     INFO: Early stopping counter 1 of 20
Epoch 97/1000, LR 0.000265
Train loss: 0.0025;  Loss pred: 0.0025; Loss self: 0.0000; time: 0.22s
Val loss: 0.1702 score: 0.9184 time: 0.08s
Test loss: 0.3509 score: 0.8980 time: 0.07s
     INFO: Early stopping counter 2 of 20
Epoch 98/1000, LR 0.000265
Train loss: 0.0019;  Loss pred: 0.0019; Loss self: 0.0000; time: 0.24s
Val loss: 0.1712 score: 0.9184 time: 0.08s
Test loss: 0.3576 score: 0.8980 time: 0.08s
     INFO: Early stopping counter 3 of 20
Epoch 99/1000, LR 0.000265
Train loss: 0.0016;  Loss pred: 0.0016; Loss self: 0.0000; time: 0.22s
Val loss: 0.1724 score: 0.9184 time: 0.08s
Test loss: 0.3639 score: 0.8980 time: 0.07s
     INFO: Early stopping counter 4 of 20
Epoch 100/1000, LR 0.000265
Train loss: 0.0015;  Loss pred: 0.0015; Loss self: 0.0000; time: 0.22s
Val loss: 0.1737 score: 0.9184 time: 0.07s
Test loss: 0.3706 score: 0.8980 time: 0.07s
     INFO: Early stopping counter 5 of 20
Epoch 101/1000, LR 0.000265
Train loss: 0.0014;  Loss pred: 0.0014; Loss self: 0.0000; time: 0.22s
Val loss: 0.1752 score: 0.9184 time: 0.08s
Test loss: 0.3762 score: 0.8980 time: 0.07s
     INFO: Early stopping counter 6 of 20
Epoch 102/1000, LR 0.000264
Train loss: 0.0015;  Loss pred: 0.0015; Loss self: 0.0000; time: 0.22s
Val loss: 0.1773 score: 0.9184 time: 0.08s
Test loss: 0.3822 score: 0.8980 time: 0.07s
     INFO: Early stopping counter 7 of 20
Epoch 103/1000, LR 0.000264
Train loss: 0.0015;  Loss pred: 0.0015; Loss self: 0.0000; time: 0.22s
Val loss: 0.1797 score: 0.9184 time: 0.08s
Test loss: 0.3883 score: 0.8980 time: 0.07s
     INFO: Early stopping counter 8 of 20
Epoch 104/1000, LR 0.000264
Train loss: 0.0018;  Loss pred: 0.0018; Loss self: 0.0000; time: 0.22s
Val loss: 0.1833 score: 0.9184 time: 0.07s
Test loss: 0.3935 score: 0.8980 time: 0.07s
     INFO: Early stopping counter 9 of 20
Epoch 105/1000, LR 0.000264
Train loss: 0.0016;  Loss pred: 0.0016; Loss self: 0.0000; time: 0.22s
Val loss: 0.1874 score: 0.9184 time: 0.07s
Test loss: 0.3982 score: 0.8980 time: 0.06s
     INFO: Early stopping counter 10 of 20
Epoch 106/1000, LR 0.000264
Train loss: 0.0013;  Loss pred: 0.0013; Loss self: 0.0000; time: 0.21s
Val loss: 0.1913 score: 0.9184 time: 0.07s
Test loss: 0.4012 score: 0.9184 time: 0.06s
     INFO: Early stopping counter 11 of 20
Epoch 107/1000, LR 0.000264
Train loss: 0.0021;  Loss pred: 0.0021; Loss self: 0.0000; time: 0.20s
Val loss: 0.1941 score: 0.9184 time: 0.07s
Test loss: 0.4050 score: 0.9184 time: 0.06s
     INFO: Early stopping counter 12 of 20
Epoch 108/1000, LR 0.000264
Train loss: 0.0020;  Loss pred: 0.0020; Loss self: 0.0000; time: 0.21s
Val loss: 0.1986 score: 0.9184 time: 0.08s
Test loss: 0.4090 score: 0.9184 time: 0.06s
     INFO: Early stopping counter 13 of 20
Epoch 109/1000, LR 0.000264
Train loss: 0.0012;  Loss pred: 0.0012; Loss self: 0.0000; time: 0.21s
Val loss: 0.2020 score: 0.9184 time: 0.08s
Test loss: 0.4126 score: 0.9184 time: 0.07s
     INFO: Early stopping counter 14 of 20
Epoch 110/1000, LR 0.000263
Train loss: 0.0016;  Loss pred: 0.0016; Loss self: 0.0000; time: 0.21s
Val loss: 0.2048 score: 0.9184 time: 0.08s
Test loss: 0.4174 score: 0.9184 time: 0.06s
     INFO: Early stopping counter 15 of 20
Epoch 111/1000, LR 0.000263
Train loss: 0.0013;  Loss pred: 0.0013; Loss self: 0.0000; time: 0.21s
Val loss: 0.2057 score: 0.9184 time: 0.07s
Test loss: 0.4211 score: 0.9184 time: 0.06s
     INFO: Early stopping counter 16 of 20
Epoch 112/1000, LR 0.000263
Train loss: 0.0012;  Loss pred: 0.0012; Loss self: 0.0000; time: 0.21s
Val loss: 0.2057 score: 0.9184 time: 0.08s
Test loss: 0.4247 score: 0.9184 time: 0.07s
     INFO: Early stopping counter 17 of 20
Epoch 113/1000, LR 0.000263
Train loss: 0.0015;  Loss pred: 0.0015; Loss self: 0.0000; time: 0.21s
Val loss: 0.2055 score: 0.9184 time: 0.08s
Test loss: 0.4291 score: 0.9184 time: 0.07s
     INFO: Early stopping counter 18 of 20
Epoch 114/1000, LR 0.000263
Train loss: 0.0016;  Loss pred: 0.0016; Loss self: 0.0000; time: 0.21s
Val loss: 0.2080 score: 0.9184 time: 0.08s
Test loss: 0.4346 score: 0.9184 time: 0.06s
     INFO: Early stopping counter 19 of 20
Epoch 115/1000, LR 0.000263
Train loss: 0.0014;  Loss pred: 0.0014; Loss self: 0.0000; time: 0.21s
Val loss: 0.2099 score: 0.9184 time: 0.08s
Test loss: 0.4400 score: 0.9184 time: 0.07s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 094,   Train_Loss: 0.0029,   Val_Loss: 0.1688,   Val_Precision: 0.9545,   Val_Recall: 0.8750,   Val_accuracy: 0.9130,   Val_Score: 0.9184,   Val_Loss: 0.1688,   Test_Precision: 0.9167,   Test_Recall: 0.8800,   Test_accuracy: 0.8980,   Test_Score: 0.8980,   Test_loss: 0.3393


[0.07385961001273245, 0.06988044595345855, 0.07486074708867818, 0.07056520704645663, 0.06866268196608871, 0.0835285879438743, 0.0680314430501312, 0.06785395694896579, 0.06836192205082625, 0.06834138301201165, 0.06880831497255713, 0.06822949310299009, 0.06862526491750032, 0.06832172896247357, 0.06837562308646739, 0.06954414793290198, 0.07314913405571133, 0.07183689705561846, 0.07230460306163877, 0.077232999028638, 0.15815942699555308, 0.07086550106760114, 0.07117223902605474, 0.07139206794090569, 0.0719195919809863, 0.0698810740141198, 0.06776586803607643, 0.06836782593745738, 0.06832266796845943, 0.07168257108423859, 0.06889959098771214, 0.06742595706600696, 0.06709854991640896, 0.06745969003532082, 0.07222679106052965, 0.07295524189248681, 0.07373176503460854, 0.07327133207581937, 0.0741697649937123, 0.0662904370110482, 0.065831467974931, 0.06588210503105074, 0.06569775799289346, 0.06569412199314684, 0.06638999795541167, 0.06524747505318373, 0.06569578091148287, 0.06571508490014821, 0.0650941519998014, 0.06605454010423273, 0.06539932801388204, 0.06541467003989965, 0.06595631700474769, 0.065739153069444, 0.06562256207689643, 0.06593827297911048, 0.06570292997639626, 0.06554786011110991, 0.06608974002301693, 0.06682264804840088, 0.06542659900151193, 0.0705081099877134, 0.06539200292900205, 0.06601132196374238, 0.06579770997632295, 0.06613241194281727, 0.06529561092611402, 0.0661644620122388, 0.06614125007763505, 0.06571885093580931, 0.06595708604436368, 0.06588019605260342, 0.06622669997159392, 0.06596964399795979, 0.06744388304650784, 0.06648259505163878, 0.06630128994584084, 0.06601263210177422, 0.06630428705830127, 0.0664217930752784, 0.06561373395379633, 0.06604262301698327, 0.06592145003378391, 0.06562773790210485, 0.06485740898642689, 0.0722377859055996, 0.07304575596936047, 0.07259543694090098, 0.06966155092231929, 0.07555584702640772, 0.07218242296949029, 0.072530637960881, 0.072319405968301, 0.0720555930165574, 0.07491113303694874, 0.077607142040506, 0.07211882702540606, 0.08022016496397555, 0.07258385408204049, 0.07139675505459309, 0.07238205801695585, 0.07203010306693614, 0.07366830098908395, 0.07280125899706036, 0.06892957002855837, 0.06789642805233598, 0.06836763303726912, 0.06880976399406791, 0.07034298800863326, 0.0698372459737584, 0.06945636298041791, 0.07039346091914922, 0.07046763901598752, 0.06967521598562598, 0.07037738105282187]
[0.0015073389798516826, 0.0014261315500705826, 0.0015277703487485343, 0.001440106266254217, 0.001401279223797729, 0.0017046650600790673, 0.001388396796941453, 0.0013847746316115468, 0.0013951412663433928, 0.001394722102285952, 0.0014042513259705535, 0.0013924386347548999, 0.0014005156105612309, 0.0013943209992341545, 0.001395420879315661, 0.0014192683251612649, 0.001492839470524721, 0.0014660591235840504, 0.001475604144115077, 0.0015761836536456735, 0.003227743408072512, 0.0014462347156653295, 0.0014524946740011172, 0.0014569809783858303, 0.0014677467751221694, 0.001426144367635098, 0.0013829768986954372, 0.0013952617538256608, 0.001394340162621621, 0.0014629096139640529, 0.0014061141017900438, 0.001376039940122591, 0.0013693581615593664, 0.0013767283680677718, 0.0014740161440924418, 0.0014888824876017716, 0.0015047298986654804, 0.001495333307669783, 0.0015136686733410675, 0.0013528660614499633, 0.0013434993464271634, 0.0013445327557357295, 0.00134077057128354, 0.0013406963672070783, 0.001354897917457381, 0.0013315811235343619, 0.0013407302226833239, 0.0013411241816356778, 0.0013284520816285999, 0.0013480518388618923, 0.001334680163548613, 0.001334993266120401, 0.0013460472858111774, 0.0013416153687641633, 0.0013392359607529883, 0.0013456790403900097, 0.0013408761219672707, 0.0013377114308389779, 0.0013487702045513659, 0.0013637275111918546, 0.00133523671431657, 0.0014389410201574163, 0.00133453067202045, 0.0013471698359947425, 0.0013428104076800601, 0.0013496410600574954, 0.0013325634882880412, 0.0013502951431069144, 0.0013498214301558174, 0.0013412010395063125, 0.001346062980497218, 0.0013444937969919067, 0.001351565305542733, 0.0013463192652644856, 0.0013764057764593437, 0.0013567876541150771, 0.001353087549915119, 0.0013471965735055963, 0.0013531487154755362, 0.0013555467974546613, 0.0013390557949754353, 0.0013478086329996586, 0.001345335714975182, 0.0013393415898388745, 0.0013236205915597324, 0.001474240528685706, 0.0014907297136604178, 0.0014815395294061424, 0.0014216643045371284, 0.0015419560617634229, 0.0014731106728467407, 0.0014802171012424693, 0.0014759062442510408, 0.001470522306460355, 0.0015287986334071172, 0.001583819225316449, 0.0014718127964368584, 0.001637146223754603, 0.0014813031445314385, 0.0014570766337672059, 0.0014771848574888949, 0.0014700021034068599, 0.0015034347140629376, 0.0014857399795318441, 0.001406725918950171, 0.0013856413888231832, 0.0013952578170871247, 0.0014042808978381206, 0.0014355711838496582, 0.001425249917831804, 0.001417476795518733, 0.0014366012432479433, 0.0014381150819589288, 0.001421943183380122, 0.0014362730827106505]
[663.420778847235, 701.1975858401756, 654.5486373780884, 694.3932009969279, 713.6336448989895, 586.6255039882245, 720.2551908812627, 722.1391677548563, 716.7732932314148, 716.9887093357152, 712.123237134098, 718.1645029376984, 714.0227445228321, 717.1949648246426, 716.6296669506748, 704.5884011301208, 669.8643891352284, 682.1007310778276, 677.6885277722653, 634.4438338051691, 309.81397018704246, 691.4506954979001, 688.4706828186489, 686.3507587503898, 681.3164347894848, 701.1912837816333, 723.0778771093721, 716.7113964516732, 717.1851079149958, 683.5692311094296, 711.1798386254408, 726.723091998994, 730.2691348925417, 726.3596967958848, 678.4186211309811, 671.6446786950643, 664.5710973689585, 668.7472250306027, 660.646558663816, 739.1714734333926, 744.3248875850601, 743.752798683435, 745.8397591786975, 745.8810394803913, 738.0629840192041, 750.9869149734869, 745.8622048502868, 745.6431057565214, 752.7557928729066, 741.8112354227127, 749.2431724925212, 749.0674487865256, 742.915951423923, 745.3701137317439, 746.6944058444695, 743.1192505682308, 745.7810483886065, 747.5453800770963, 741.4161408856334, 733.2843194796529, 748.9308744119141, 694.9555165858033, 749.3271012542725, 742.2969051720237, 744.7067689381964, 740.9377423338021, 750.4332880114483, 740.5788320463665, 740.8387344128656, 745.6003764865061, 742.907289249284, 743.7743500470904, 739.8828572315573, 742.765869731166, 726.5299355052061, 737.035008364827, 739.0504776004562, 742.2821729703917, 739.0170707501065, 737.7096842969354, 746.7948712460819, 741.9450918446917, 743.308892248094, 746.6355167245288, 755.5035078606753, 678.3153634308953, 670.8124154475638, 674.9735529505839, 703.4009342490907, 648.5269099408532, 678.8356220836627, 675.5765753284547, 677.5498131369836, 680.0304868595067, 654.1083816718073, 631.3851884202244, 679.4342340418023, 610.8189882432231, 675.0812645552826, 686.3057006236832, 676.9633434368709, 680.2711354510388, 665.1436145820811, 673.0652831426798, 710.8705303064953, 721.6874496288639, 716.7134186624353, 712.1082409790606, 696.5868437943834, 701.6313332059505, 705.4789208270919, 696.087383120418, 695.3546434113254, 703.2629796240418, 696.246425584137]
Elapsed: 0.06990501933589416~0.008980862789654608
Time per graph: 0.0014266330476713092~0.00018328291407458387
Speed: 707.0730368178745~50.65380114474719
Total Time: 0.0709
best val loss: 0.16880886256694794 test_score: 0.8980

Testing...
Test loss: 0.4258 score: 0.8980 time: 0.06s
test Score 0.8980
Epoch Time List: [0.35025557794142514, 0.34973484894726425, 0.3509860330959782, 0.35968710109591484, 0.34449524781666696, 0.3676899520214647, 0.35729762609116733, 0.34548684291075915, 0.34303716907743365, 0.34589120000600815, 0.3405627228785306, 0.33856255398131907, 0.34320018510334194, 0.3405399650800973, 0.34791362890973687, 0.34923763188999146, 0.46951979387085885, 0.37474683893378824, 0.36433169711381197, 0.36527635413222015, 0.44757716706953943, 0.35487512208055705, 0.3583817781182006, 0.35686489602085203, 0.3596197438891977, 0.35168124607298523, 0.4126796911004931, 0.3389126032125205, 0.33973319618962705, 0.3391916450345889, 0.33862650906667113, 0.38200218009296805, 0.3376869950443506, 0.33880763698834926, 0.3453123898943886, 0.36738421383779496, 0.3704198020277545, 0.3685165380593389, 0.3702639879193157, 0.33290875295642763, 0.3312104409560561, 0.3336681700311601, 0.33160880906507373, 0.33210192702244967, 0.33295236295089126, 0.34240920504089445, 0.33209972886834294, 0.3321028130594641, 0.3247404210269451, 0.3298833610024303, 0.3320108629995957, 0.3299944648751989, 0.3319164681015536, 0.33229390799533576, 0.3303412910318002, 0.33308176207356155, 0.33237248903606087, 0.33186021400615573, 0.3339804249117151, 0.33207037893589586, 0.33402457600459456, 0.33237653295509517, 0.33120850985869765, 0.3295228348579258, 0.33061267191078514, 0.33291260595433414, 0.331081647076644, 0.33208924299106, 0.3310389081016183, 0.33216009207535535, 0.33293708390556276, 0.33392276300583035, 0.3306152910226956, 0.3316058620112017, 0.3302575189154595, 0.3380327691556886, 0.3325191739713773, 0.32823134190402925, 0.3252913129981607, 0.3320072559872642, 0.32583682297263294, 0.3335519441170618, 0.3327176780439913, 0.337231794022955, 0.3267574032070115, 0.3439234639517963, 0.3686596070183441, 0.36387394391931593, 0.3562169590732083, 0.3582324089948088, 0.36600983201060444, 0.37091391102876514, 0.3631249559111893, 0.365326048922725, 0.3691650229739025, 0.3682702179066837, 0.370808805921115, 0.3948854600312188, 0.36801513913087547, 0.3590038319816813, 0.3626213880488649, 0.3626661271555349, 0.36670656490605325, 0.3613232650095597, 0.3508384379092604, 0.34125748788937926, 0.33875163993798196, 0.3503663861192763, 0.35709787893574685, 0.3576204718556255, 0.3471138260792941, 0.3523693950846791, 0.3515868061222136, 0.3489933309610933, 0.3526856548851356]
Total Epoch List: [115]
Total Time List: [0.07093218003865331]
========================k_idx:1========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: GPSTransformer
GPS(
  (x_lin): Linear(in_features=14887, out_features=64, bias=True)
  (pe_lin): Linear(in_features=20, out_features=8, bias=True)
  (pe_norm): BatchNorm1d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (convs): ModuleList(
    (0-1): 2 x GPSConv(64, conv=GINEConv(nn=Sequential(
      (0): Linear(in_features=64, out_features=64, bias=True)
      (1): ReLU()
      (2): Linear(in_features=64, out_features=64, bias=True)
    )), heads=1, attn_type=multihead)
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=32, bias=True)
    (1): ReLU()
    (2): Linear(in_features=32, out_features=16, bias=True)
    (3): ReLU()
    (4): Linear(in_features=16, out_features=2, bias=True)
  )
)
Total number of parameters: 1039906
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7d29a7d734c0>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 1.8261;  Loss pred: 1.8261; Loss self: 0.0000; time: 0.21s
Val loss: 4.5870 score: 0.4694 time: 0.07s
Test loss: 6.8113 score: 0.4694 time: 0.08s
Epoch 2/1000, LR 0.000000
Train loss: 1.8266;  Loss pred: 1.8266; Loss self: 0.0000; time: 0.22s
Val loss: 3.6719 score: 0.4898 time: 0.07s
Test loss: 5.4705 score: 0.4694 time: 0.08s
Epoch 3/1000, LR 0.000030
Train loss: 1.7112;  Loss pred: 1.7112; Loss self: 0.0000; time: 0.20s
Val loss: 3.0305 score: 0.4694 time: 0.07s
Test loss: 4.5263 score: 0.4490 time: 0.08s
Epoch 4/1000, LR 0.000060
Train loss: 1.5474;  Loss pred: 1.5474; Loss self: 0.0000; time: 0.22s
Val loss: 2.5430 score: 0.4490 time: 0.07s
Test loss: 3.7946 score: 0.4082 time: 0.09s
Epoch 5/1000, LR 0.000090
Train loss: 1.3531;  Loss pred: 1.3531; Loss self: 0.0000; time: 0.21s
Val loss: 2.1670 score: 0.4490 time: 0.07s
Test loss: 3.2006 score: 0.3878 time: 0.07s
Epoch 6/1000, LR 0.000120
Train loss: 1.1541;  Loss pred: 1.1541; Loss self: 0.0000; time: 0.22s
Val loss: 1.8751 score: 0.4490 time: 0.06s
Test loss: 2.7371 score: 0.3878 time: 0.08s
Epoch 7/1000, LR 0.000150
Train loss: 0.9549;  Loss pred: 0.9549; Loss self: 0.0000; time: 0.22s
Val loss: 1.6434 score: 0.4286 time: 0.06s
Test loss: 2.3687 score: 0.3878 time: 0.08s
Epoch 8/1000, LR 0.000180
Train loss: 0.7724;  Loss pred: 0.7724; Loss self: 0.0000; time: 0.21s
Val loss: 1.4600 score: 0.4286 time: 0.06s
Test loss: 2.0659 score: 0.3878 time: 0.08s
Epoch 9/1000, LR 0.000210
Train loss: 0.6340;  Loss pred: 0.6340; Loss self: 0.0000; time: 0.21s
Val loss: 1.3101 score: 0.4286 time: 0.06s
Test loss: 1.8051 score: 0.3878 time: 0.08s
Epoch 10/1000, LR 0.000240
Train loss: 0.5303;  Loss pred: 0.5303; Loss self: 0.0000; time: 0.21s
Val loss: 1.1981 score: 0.4286 time: 0.07s
Test loss: 1.6103 score: 0.3878 time: 0.08s
Epoch 11/1000, LR 0.000270
Train loss: 0.4668;  Loss pred: 0.4668; Loss self: 0.0000; time: 0.22s
Val loss: 1.1161 score: 0.4286 time: 0.07s
Test loss: 1.4784 score: 0.3878 time: 0.09s
Epoch 12/1000, LR 0.000270
Train loss: 0.4137;  Loss pred: 0.4137; Loss self: 0.0000; time: 0.21s
Val loss: 1.0685 score: 0.4286 time: 0.07s
Test loss: 1.4039 score: 0.4082 time: 0.07s
Epoch 13/1000, LR 0.000270
Train loss: 0.3663;  Loss pred: 0.3663; Loss self: 0.0000; time: 0.20s
Val loss: 1.0381 score: 0.4286 time: 0.06s
Test loss: 1.3584 score: 0.4082 time: 0.08s
Epoch 14/1000, LR 0.000270
Train loss: 0.3401;  Loss pred: 0.3401; Loss self: 0.0000; time: 0.22s
Val loss: 1.0159 score: 0.4490 time: 0.07s
Test loss: 1.3246 score: 0.4082 time: 0.07s
Epoch 15/1000, LR 0.000270
Train loss: 0.3122;  Loss pred: 0.3122; Loss self: 0.0000; time: 0.21s
Val loss: 0.9945 score: 0.4490 time: 0.07s
Test loss: 1.2931 score: 0.4082 time: 0.08s
Epoch 16/1000, LR 0.000270
Train loss: 0.2960;  Loss pred: 0.2960; Loss self: 0.0000; time: 0.20s
Val loss: 0.9712 score: 0.4490 time: 0.07s
Test loss: 1.2586 score: 0.3878 time: 0.08s
Epoch 17/1000, LR 0.000270
Train loss: 0.2753;  Loss pred: 0.2753; Loss self: 0.0000; time: 0.20s
Val loss: 0.9468 score: 0.4490 time: 0.06s
Test loss: 1.2208 score: 0.3878 time: 0.08s
Epoch 18/1000, LR 0.000270
Train loss: 0.2598;  Loss pred: 0.2598; Loss self: 0.0000; time: 0.21s
Val loss: 0.9163 score: 0.4490 time: 0.06s
Test loss: 1.1739 score: 0.3878 time: 0.08s
Epoch 19/1000, LR 0.000270
Train loss: 0.2448;  Loss pred: 0.2448; Loss self: 0.0000; time: 0.21s
Val loss: 0.8838 score: 0.4490 time: 0.06s
Test loss: 1.1278 score: 0.3878 time: 0.08s
Epoch 20/1000, LR 0.000270
Train loss: 0.2294;  Loss pred: 0.2294; Loss self: 0.0000; time: 0.21s
Val loss: 0.8559 score: 0.4082 time: 0.06s
Test loss: 1.0859 score: 0.3878 time: 0.07s
Epoch 21/1000, LR 0.000270
Train loss: 0.2146;  Loss pred: 0.2146; Loss self: 0.0000; time: 0.20s
Val loss: 0.8344 score: 0.4286 time: 0.07s
Test loss: 1.0551 score: 0.3878 time: 0.07s
Epoch 22/1000, LR 0.000270
Train loss: 0.2031;  Loss pred: 0.2031; Loss self: 0.0000; time: 0.21s
Val loss: 0.8207 score: 0.4490 time: 0.07s
Test loss: 1.0400 score: 0.3673 time: 0.08s
Epoch 23/1000, LR 0.000270
Train loss: 0.1923;  Loss pred: 0.1923; Loss self: 0.0000; time: 0.22s
Val loss: 0.8103 score: 0.4490 time: 0.06s
Test loss: 1.0266 score: 0.3878 time: 0.08s
Epoch 24/1000, LR 0.000270
Train loss: 0.1810;  Loss pred: 0.1810; Loss self: 0.0000; time: 0.21s
Val loss: 0.8022 score: 0.4694 time: 0.07s
Test loss: 1.0176 score: 0.3878 time: 0.08s
Epoch 25/1000, LR 0.000270
Train loss: 0.1717;  Loss pred: 0.1717; Loss self: 0.0000; time: 0.23s
Val loss: 0.7921 score: 0.4490 time: 0.07s
Test loss: 1.0065 score: 0.4082 time: 0.08s
Epoch 26/1000, LR 0.000270
Train loss: 0.1603;  Loss pred: 0.1603; Loss self: 0.0000; time: 0.21s
Val loss: 0.7797 score: 0.4490 time: 0.07s
Test loss: 0.9871 score: 0.3878 time: 0.08s
Epoch 27/1000, LR 0.000270
Train loss: 0.1505;  Loss pred: 0.1505; Loss self: 0.0000; time: 0.20s
Val loss: 0.7665 score: 0.4694 time: 0.07s
Test loss: 0.9623 score: 0.4082 time: 0.08s
Epoch 28/1000, LR 0.000270
Train loss: 0.1442;  Loss pred: 0.1442; Loss self: 0.0000; time: 0.21s
Val loss: 0.7526 score: 0.5306 time: 0.07s
Test loss: 0.9344 score: 0.3878 time: 0.08s
Epoch 29/1000, LR 0.000270
Train loss: 0.1373;  Loss pred: 0.1373; Loss self: 0.0000; time: 0.22s
Val loss: 0.7363 score: 0.5510 time: 0.07s
Test loss: 0.9005 score: 0.4082 time: 0.08s
Epoch 30/1000, LR 0.000270
Train loss: 0.1302;  Loss pred: 0.1302; Loss self: 0.0000; time: 0.21s
Val loss: 0.7210 score: 0.5510 time: 0.07s
Test loss: 0.8665 score: 0.4082 time: 0.08s
Epoch 31/1000, LR 0.000270
Train loss: 0.1221;  Loss pred: 0.1221; Loss self: 0.0000; time: 0.21s
Val loss: 0.7058 score: 0.5714 time: 0.07s
Test loss: 0.8318 score: 0.4082 time: 0.07s
Epoch 32/1000, LR 0.000270
Train loss: 0.1162;  Loss pred: 0.1162; Loss self: 0.0000; time: 0.21s
Val loss: 0.6925 score: 0.5714 time: 0.07s
Test loss: 0.8034 score: 0.4490 time: 0.08s
Epoch 33/1000, LR 0.000270
Train loss: 0.1098;  Loss pred: 0.1098; Loss self: 0.0000; time: 0.21s
Val loss: 0.6813 score: 0.5714 time: 0.06s
Test loss: 0.7821 score: 0.4898 time: 0.08s
Epoch 34/1000, LR 0.000270
Train loss: 0.1031;  Loss pred: 0.1031; Loss self: 0.0000; time: 0.22s
Val loss: 0.6724 score: 0.5714 time: 0.06s
Test loss: 0.7681 score: 0.4898 time: 0.08s
Epoch 35/1000, LR 0.000270
Train loss: 0.0976;  Loss pred: 0.0976; Loss self: 0.0000; time: 0.20s
Val loss: 0.6655 score: 0.6122 time: 0.07s
Test loss: 0.7583 score: 0.5102 time: 0.08s
Epoch 36/1000, LR 0.000270
Train loss: 0.0930;  Loss pred: 0.0930; Loss self: 0.0000; time: 0.21s
Val loss: 0.6609 score: 0.5918 time: 0.06s
Test loss: 0.7501 score: 0.5102 time: 0.07s
Epoch 37/1000, LR 0.000270
Train loss: 0.0891;  Loss pred: 0.0891; Loss self: 0.0000; time: 0.21s
Val loss: 0.6565 score: 0.6531 time: 0.07s
Test loss: 0.7410 score: 0.5306 time: 0.08s
Epoch 38/1000, LR 0.000270
Train loss: 0.0846;  Loss pred: 0.0846; Loss self: 0.0000; time: 0.21s
Val loss: 0.6518 score: 0.7143 time: 0.07s
Test loss: 0.7298 score: 0.5306 time: 0.08s
Epoch 39/1000, LR 0.000269
Train loss: 0.0802;  Loss pred: 0.0802; Loss self: 0.0000; time: 0.21s
Val loss: 0.6468 score: 0.6939 time: 0.07s
Test loss: 0.7167 score: 0.5918 time: 0.08s
Epoch 40/1000, LR 0.000269
Train loss: 0.0772;  Loss pred: 0.0772; Loss self: 0.0000; time: 0.21s
Val loss: 0.6408 score: 0.7143 time: 0.07s
Test loss: 0.7020 score: 0.6531 time: 0.07s
Epoch 41/1000, LR 0.000269
Train loss: 0.0718;  Loss pred: 0.0718; Loss self: 0.0000; time: 0.21s
Val loss: 0.6346 score: 0.7347 time: 0.06s
Test loss: 0.6867 score: 0.6735 time: 0.08s
Epoch 42/1000, LR 0.000269
Train loss: 0.0683;  Loss pred: 0.0683; Loss self: 0.0000; time: 0.21s
Val loss: 0.6294 score: 0.7143 time: 0.06s
Test loss: 0.6733 score: 0.6735 time: 0.08s
Epoch 43/1000, LR 0.000269
Train loss: 0.0651;  Loss pred: 0.0651; Loss self: 0.0000; time: 0.21s
Val loss: 0.6235 score: 0.7143 time: 0.07s
Test loss: 0.6595 score: 0.6939 time: 0.08s
Epoch 44/1000, LR 0.000269
Train loss: 0.0633;  Loss pred: 0.0633; Loss self: 0.0000; time: 0.21s
Val loss: 0.6174 score: 0.7143 time: 0.07s
Test loss: 0.6466 score: 0.6939 time: 0.08s
Epoch 45/1000, LR 0.000269
Train loss: 0.0584;  Loss pred: 0.0584; Loss self: 0.0000; time: 0.21s
Val loss: 0.6127 score: 0.7143 time: 0.07s
Test loss: 0.6373 score: 0.7143 time: 0.08s
Epoch 46/1000, LR 0.000269
Train loss: 0.0559;  Loss pred: 0.0559; Loss self: 0.0000; time: 0.21s
Val loss: 0.6077 score: 0.6939 time: 0.06s
Test loss: 0.6279 score: 0.7347 time: 0.08s
Epoch 47/1000, LR 0.000269
Train loss: 0.0542;  Loss pred: 0.0542; Loss self: 0.0000; time: 0.21s
Val loss: 0.6030 score: 0.7143 time: 0.07s
Test loss: 0.6201 score: 0.7755 time: 0.08s
Epoch 48/1000, LR 0.000269
Train loss: 0.0512;  Loss pred: 0.0512; Loss self: 0.0000; time: 0.21s
Val loss: 0.5975 score: 0.6939 time: 0.07s
Test loss: 0.6106 score: 0.7755 time: 0.08s
Epoch 49/1000, LR 0.000269
Train loss: 0.0490;  Loss pred: 0.0490; Loss self: 0.0000; time: 0.21s
Val loss: 0.5916 score: 0.6735 time: 0.07s
Test loss: 0.6000 score: 0.7755 time: 0.08s
Epoch 50/1000, LR 0.000269
Train loss: 0.0471;  Loss pred: 0.0471; Loss self: 0.0000; time: 0.21s
Val loss: 0.5854 score: 0.6735 time: 0.07s
Test loss: 0.5895 score: 0.7755 time: 0.08s
Epoch 51/1000, LR 0.000269
Train loss: 0.0439;  Loss pred: 0.0439; Loss self: 0.0000; time: 0.22s
Val loss: 0.5793 score: 0.6939 time: 0.06s
Test loss: 0.5793 score: 0.7959 time: 0.07s
Epoch 52/1000, LR 0.000269
Train loss: 0.0425;  Loss pred: 0.0425; Loss self: 0.0000; time: 0.22s
Val loss: 0.5742 score: 0.6735 time: 0.06s
Test loss: 0.5713 score: 0.7959 time: 0.08s
Epoch 53/1000, LR 0.000269
Train loss: 0.0391;  Loss pred: 0.0391; Loss self: 0.0000; time: 0.21s
Val loss: 0.5686 score: 0.6735 time: 0.06s
Test loss: 0.5621 score: 0.7959 time: 0.07s
Epoch 54/1000, LR 0.000269
Train loss: 0.0379;  Loss pred: 0.0379; Loss self: 0.0000; time: 0.21s
Val loss: 0.5629 score: 0.6735 time: 0.07s
Test loss: 0.5526 score: 0.7959 time: 0.07s
Epoch 55/1000, LR 0.000269
Train loss: 0.0355;  Loss pred: 0.0355; Loss self: 0.0000; time: 0.23s
Val loss: 0.5567 score: 0.6735 time: 0.06s
Test loss: 0.5429 score: 0.7959 time: 0.08s
Epoch 56/1000, LR 0.000269
Train loss: 0.0345;  Loss pred: 0.0345; Loss self: 0.0000; time: 0.21s
Val loss: 0.5501 score: 0.6939 time: 0.07s
Test loss: 0.5323 score: 0.8163 time: 0.08s
Epoch 57/1000, LR 0.000269
Train loss: 0.0329;  Loss pred: 0.0329; Loss self: 0.0000; time: 0.20s
Val loss: 0.5444 score: 0.7143 time: 0.06s
Test loss: 0.5225 score: 0.8163 time: 0.07s
Epoch 58/1000, LR 0.000269
Train loss: 0.0314;  Loss pred: 0.0314; Loss self: 0.0000; time: 0.21s
Val loss: 0.5392 score: 0.7143 time: 0.06s
Test loss: 0.5129 score: 0.8163 time: 0.08s
Epoch 59/1000, LR 0.000268
Train loss: 0.0300;  Loss pred: 0.0300; Loss self: 0.0000; time: 0.22s
Val loss: 0.5344 score: 0.7755 time: 0.07s
Test loss: 0.5035 score: 0.8163 time: 0.08s
Epoch 60/1000, LR 0.000268
Train loss: 0.0290;  Loss pred: 0.0290; Loss self: 0.0000; time: 0.21s
Val loss: 0.5302 score: 0.7755 time: 0.07s
Test loss: 0.4945 score: 0.8163 time: 0.08s
Epoch 61/1000, LR 0.000268
Train loss: 0.0271;  Loss pred: 0.0271; Loss self: 0.0000; time: 0.22s
Val loss: 0.5262 score: 0.7755 time: 0.07s
Test loss: 0.4860 score: 0.8163 time: 0.08s
Epoch 62/1000, LR 0.000268
Train loss: 0.0266;  Loss pred: 0.0266; Loss self: 0.0000; time: 0.23s
Val loss: 0.5218 score: 0.7755 time: 0.10s
Test loss: 0.4755 score: 0.8163 time: 0.08s
Epoch 63/1000, LR 0.000268
Train loss: 0.0246;  Loss pred: 0.0246; Loss self: 0.0000; time: 0.24s
Val loss: 0.5171 score: 0.7755 time: 0.07s
Test loss: 0.4635 score: 0.8163 time: 0.08s
Epoch 64/1000, LR 0.000268
Train loss: 0.0245;  Loss pred: 0.0245; Loss self: 0.0000; time: 0.21s
Val loss: 0.5123 score: 0.7755 time: 0.07s
Test loss: 0.4521 score: 0.8163 time: 0.08s
Epoch 65/1000, LR 0.000268
Train loss: 0.0223;  Loss pred: 0.0223; Loss self: 0.0000; time: 0.21s
Val loss: 0.5078 score: 0.7551 time: 0.07s
Test loss: 0.4416 score: 0.8163 time: 0.08s
Epoch 66/1000, LR 0.000268
Train loss: 0.0215;  Loss pred: 0.0215; Loss self: 0.0000; time: 0.21s
Val loss: 0.5028 score: 0.7959 time: 0.07s
Test loss: 0.4296 score: 0.8367 time: 0.08s
Epoch 67/1000, LR 0.000268
Train loss: 0.0207;  Loss pred: 0.0207; Loss self: 0.0000; time: 0.31s
Val loss: 0.4967 score: 0.8163 time: 0.07s
Test loss: 0.4151 score: 0.8367 time: 0.08s
Epoch 68/1000, LR 0.000268
Train loss: 0.0202;  Loss pred: 0.0202; Loss self: 0.0000; time: 0.22s
Val loss: 0.4905 score: 0.8163 time: 0.07s
Test loss: 0.4004 score: 0.8367 time: 0.08s
Epoch 69/1000, LR 0.000268
Train loss: 0.0183;  Loss pred: 0.0183; Loss self: 0.0000; time: 0.23s
Val loss: 0.4849 score: 0.8163 time: 0.07s
Test loss: 0.3859 score: 0.8367 time: 0.09s
Epoch 70/1000, LR 0.000268
Train loss: 0.0177;  Loss pred: 0.0177; Loss self: 0.0000; time: 0.22s
Val loss: 0.4794 score: 0.8163 time: 0.07s
Test loss: 0.3726 score: 0.8571 time: 0.08s
Epoch 71/1000, LR 0.000268
Train loss: 0.0179;  Loss pred: 0.0179; Loss self: 0.0000; time: 0.24s
Val loss: 0.4731 score: 0.8163 time: 0.11s
Test loss: 0.3577 score: 0.8776 time: 0.08s
Epoch 72/1000, LR 0.000267
Train loss: 0.0169;  Loss pred: 0.0169; Loss self: 0.0000; time: 0.21s
Val loss: 0.4675 score: 0.8163 time: 0.07s
Test loss: 0.3443 score: 0.8776 time: 0.07s
Epoch 73/1000, LR 0.000267
Train loss: 0.0158;  Loss pred: 0.0158; Loss self: 0.0000; time: 0.20s
Val loss: 0.4623 score: 0.8163 time: 0.07s
Test loss: 0.3315 score: 0.8776 time: 0.08s
Epoch 74/1000, LR 0.000267
Train loss: 0.0158;  Loss pred: 0.0158; Loss self: 0.0000; time: 0.20s
Val loss: 0.4576 score: 0.8163 time: 0.07s
Test loss: 0.3199 score: 0.8980 time: 0.08s
Epoch 75/1000, LR 0.000267
Train loss: 0.0153;  Loss pred: 0.0153; Loss self: 0.0000; time: 0.21s
Val loss: 0.4547 score: 0.8163 time: 0.07s
Test loss: 0.3110 score: 0.8980 time: 0.08s
Epoch 76/1000, LR 0.000267
Train loss: 0.0146;  Loss pred: 0.0146; Loss self: 0.0000; time: 0.22s
Val loss: 0.4543 score: 0.8163 time: 0.10s
Test loss: 0.3055 score: 0.8980 time: 0.07s
Epoch 77/1000, LR 0.000267
Train loss: 0.0143;  Loss pred: 0.0143; Loss self: 0.0000; time: 0.21s
Val loss: 0.4555 score: 0.8163 time: 0.06s
Test loss: 0.3025 score: 0.8980 time: 0.08s
     INFO: Early stopping counter 1 of 20
Epoch 78/1000, LR 0.000267
Train loss: 0.0132;  Loss pred: 0.0132; Loss self: 0.0000; time: 0.20s
Val loss: 0.4580 score: 0.8163 time: 0.07s
Test loss: 0.2998 score: 0.8776 time: 0.07s
     INFO: Early stopping counter 2 of 20
Epoch 79/1000, LR 0.000267
Train loss: 0.0125;  Loss pred: 0.0125; Loss self: 0.0000; time: 0.20s
Val loss: 0.4624 score: 0.8163 time: 0.06s
Test loss: 0.2982 score: 0.8776 time: 0.07s
     INFO: Early stopping counter 3 of 20
Epoch 80/1000, LR 0.000267
Train loss: 0.0123;  Loss pred: 0.0123; Loss self: 0.0000; time: 0.22s
Val loss: 0.4676 score: 0.8163 time: 0.06s
Test loss: 0.2967 score: 0.8776 time: 0.08s
     INFO: Early stopping counter 4 of 20
Epoch 81/1000, LR 0.000267
Train loss: 0.0123;  Loss pred: 0.0123; Loss self: 0.0000; time: 0.20s
Val loss: 0.4743 score: 0.8163 time: 0.06s
Test loss: 0.2973 score: 0.8571 time: 0.07s
     INFO: Early stopping counter 5 of 20
Epoch 82/1000, LR 0.000267
Train loss: 0.0118;  Loss pred: 0.0118; Loss self: 0.0000; time: 0.20s
Val loss: 0.4813 score: 0.8163 time: 0.06s
Test loss: 0.2984 score: 0.8571 time: 0.07s
     INFO: Early stopping counter 6 of 20
Epoch 83/1000, LR 0.000266
Train loss: 0.0117;  Loss pred: 0.0117; Loss self: 0.0000; time: 0.22s
Val loss: 0.4881 score: 0.8163 time: 0.06s
Test loss: 0.2989 score: 0.8571 time: 0.07s
     INFO: Early stopping counter 7 of 20
Epoch 84/1000, LR 0.000266
Train loss: 0.0106;  Loss pred: 0.0106; Loss self: 0.0000; time: 0.21s
Val loss: 0.4945 score: 0.8163 time: 0.06s
Test loss: 0.2983 score: 0.8571 time: 0.07s
     INFO: Early stopping counter 8 of 20
Epoch 85/1000, LR 0.000266
Train loss: 0.0106;  Loss pred: 0.0106; Loss self: 0.0000; time: 0.20s
Val loss: 0.4990 score: 0.8163 time: 0.07s
Test loss: 0.2950 score: 0.8776 time: 0.07s
     INFO: Early stopping counter 9 of 20
Epoch 86/1000, LR 0.000266
Train loss: 0.0101;  Loss pred: 0.0101; Loss self: 0.0000; time: 0.21s
Val loss: 0.5032 score: 0.8163 time: 0.07s
Test loss: 0.2908 score: 0.8776 time: 0.07s
     INFO: Early stopping counter 10 of 20
Epoch 87/1000, LR 0.000266
Train loss: 0.0097;  Loss pred: 0.0097; Loss self: 0.0000; time: 0.20s
Val loss: 0.5060 score: 0.8163 time: 0.07s
Test loss: 0.2839 score: 0.8776 time: 0.07s
     INFO: Early stopping counter 11 of 20
Epoch 88/1000, LR 0.000266
Train loss: 0.0093;  Loss pred: 0.0093; Loss self: 0.0000; time: 0.20s
Val loss: 0.5070 score: 0.8163 time: 0.07s
Test loss: 0.2738 score: 0.8776 time: 0.07s
     INFO: Early stopping counter 12 of 20
Epoch 89/1000, LR 0.000266
Train loss: 0.0091;  Loss pred: 0.0091; Loss self: 0.0000; time: 0.20s
Val loss: 0.5070 score: 0.8163 time: 0.07s
Test loss: 0.2626 score: 0.8776 time: 0.07s
     INFO: Early stopping counter 13 of 20
Epoch 90/1000, LR 0.000266
Train loss: 0.0086;  Loss pred: 0.0086; Loss self: 0.0000; time: 0.21s
Val loss: 0.5076 score: 0.8163 time: 0.07s
Test loss: 0.2528 score: 0.8776 time: 0.08s
     INFO: Early stopping counter 14 of 20
Epoch 91/1000, LR 0.000266
Train loss: 0.0084;  Loss pred: 0.0084; Loss self: 0.0000; time: 0.22s
Val loss: 0.5123 score: 0.8163 time: 0.06s
Test loss: 0.2474 score: 0.8776 time: 0.07s
     INFO: Early stopping counter 15 of 20
Epoch 92/1000, LR 0.000266
Train loss: 0.0084;  Loss pred: 0.0084; Loss self: 0.0000; time: 0.21s
Val loss: 0.5186 score: 0.8163 time: 0.06s
Test loss: 0.2432 score: 0.8980 time: 0.08s
     INFO: Early stopping counter 16 of 20
Epoch 93/1000, LR 0.000265
Train loss: 0.0087;  Loss pred: 0.0087; Loss self: 0.0000; time: 0.20s
Val loss: 0.5247 score: 0.8163 time: 0.06s
Test loss: 0.2386 score: 0.8980 time: 0.07s
     INFO: Early stopping counter 17 of 20
Epoch 94/1000, LR 0.000265
Train loss: 0.0082;  Loss pred: 0.0082; Loss self: 0.0000; time: 0.21s
Val loss: 0.5315 score: 0.8163 time: 0.07s
Test loss: 0.2347 score: 0.8980 time: 0.07s
     INFO: Early stopping counter 18 of 20
Epoch 95/1000, LR 0.000265
Train loss: 0.0084;  Loss pred: 0.0084; Loss self: 0.0000; time: 0.22s
Val loss: 0.5406 score: 0.8163 time: 0.07s
Test loss: 0.2332 score: 0.8980 time: 0.08s
     INFO: Early stopping counter 19 of 20
Epoch 96/1000, LR 0.000265
Train loss: 0.0077;  Loss pred: 0.0077; Loss self: 0.0000; time: 0.21s
Val loss: 0.5528 score: 0.8163 time: 0.07s
Test loss: 0.2350 score: 0.8980 time: 0.08s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 075,   Train_Loss: 0.0146,   Val_Loss: 0.4543,   Val_Precision: 1.0000,   Val_Recall: 0.6400,   Val_accuracy: 0.7805,   Val_Score: 0.8163,   Val_Loss: 0.4543,   Test_Precision: 1.0000,   Test_Recall: 0.7917,   Test_accuracy: 0.8837,   Test_Score: 0.8980,   Test_loss: 0.3055


[0.07385961001273245, 0.06988044595345855, 0.07486074708867818, 0.07056520704645663, 0.06866268196608871, 0.0835285879438743, 0.0680314430501312, 0.06785395694896579, 0.06836192205082625, 0.06834138301201165, 0.06880831497255713, 0.06822949310299009, 0.06862526491750032, 0.06832172896247357, 0.06837562308646739, 0.06954414793290198, 0.07314913405571133, 0.07183689705561846, 0.07230460306163877, 0.077232999028638, 0.15815942699555308, 0.07086550106760114, 0.07117223902605474, 0.07139206794090569, 0.0719195919809863, 0.0698810740141198, 0.06776586803607643, 0.06836782593745738, 0.06832266796845943, 0.07168257108423859, 0.06889959098771214, 0.06742595706600696, 0.06709854991640896, 0.06745969003532082, 0.07222679106052965, 0.07295524189248681, 0.07373176503460854, 0.07327133207581937, 0.0741697649937123, 0.0662904370110482, 0.065831467974931, 0.06588210503105074, 0.06569775799289346, 0.06569412199314684, 0.06638999795541167, 0.06524747505318373, 0.06569578091148287, 0.06571508490014821, 0.0650941519998014, 0.06605454010423273, 0.06539932801388204, 0.06541467003989965, 0.06595631700474769, 0.065739153069444, 0.06562256207689643, 0.06593827297911048, 0.06570292997639626, 0.06554786011110991, 0.06608974002301693, 0.06682264804840088, 0.06542659900151193, 0.0705081099877134, 0.06539200292900205, 0.06601132196374238, 0.06579770997632295, 0.06613241194281727, 0.06529561092611402, 0.0661644620122388, 0.06614125007763505, 0.06571885093580931, 0.06595708604436368, 0.06588019605260342, 0.06622669997159392, 0.06596964399795979, 0.06744388304650784, 0.06648259505163878, 0.06630128994584084, 0.06601263210177422, 0.06630428705830127, 0.0664217930752784, 0.06561373395379633, 0.06604262301698327, 0.06592145003378391, 0.06562773790210485, 0.06485740898642689, 0.0722377859055996, 0.07304575596936047, 0.07259543694090098, 0.06966155092231929, 0.07555584702640772, 0.07218242296949029, 0.072530637960881, 0.072319405968301, 0.0720555930165574, 0.07491113303694874, 0.077607142040506, 0.07211882702540606, 0.08022016496397555, 0.07258385408204049, 0.07139675505459309, 0.07238205801695585, 0.07203010306693614, 0.07366830098908395, 0.07280125899706036, 0.06892957002855837, 0.06789642805233598, 0.06836763303726912, 0.06880976399406791, 0.07034298800863326, 0.0698372459737584, 0.06945636298041791, 0.07039346091914922, 0.07046763901598752, 0.06967521598562598, 0.07037738105282187, 0.08211295597720891, 0.08283945906441659, 0.0839074709219858, 0.09228648501448333, 0.0796258479822427, 0.08225117402616888, 0.08686447807122022, 0.08273724501486868, 0.08185865299310535, 0.08091320400126278, 0.09917472302913666, 0.07748400303535163, 0.0810869459528476, 0.0762548689963296, 0.08093155594542623, 0.0806769470218569, 0.07988527696579695, 0.08005447301547974, 0.08002156089060009, 0.07939654297661036, 0.07959328091237694, 0.0817467599408701, 0.08273795899003744, 0.08411703095771372, 0.08604803390335292, 0.08125750790350139, 0.08122231706511229, 0.081664287019521, 0.08224959101062268, 0.0833019750425592, 0.07655717700254172, 0.0815196669427678, 0.08105747995432466, 0.08162288391031325, 0.08186291693709791, 0.07674414396751672, 0.08116415096446872, 0.08146133203990757, 0.08388635900337249, 0.07723342208191752, 0.08610909001436085, 0.08317331201396883, 0.08135405101347715, 0.08135642693378031, 0.08233752602245659, 0.08234677102882415, 0.08228368696291, 0.08307302300818264, 0.0819864699151367, 0.08197423606179655, 0.07676309288945049, 0.08271457394585013, 0.07675431401003152, 0.07646099803969264, 0.08056469797156751, 0.08051179302856326, 0.0762351299636066, 0.0875893629854545, 0.0808275219751522, 0.08249794004950672, 0.0842265640385449, 0.08416549989487976, 0.08955766703002155, 0.08246412698645145, 0.08391455293167382, 0.08739720704033971, 0.08428194094449282, 0.08436655497644097, 0.08987386198714375, 0.08378155599348247, 0.07988261000718921, 0.07942835602443665, 0.08010760403703898, 0.08108371892012656, 0.07982606999576092, 0.0776638489915058, 0.08061006700154394, 0.07770334009546787, 0.07847123790998012, 0.0850272880634293, 0.0782962740631774, 0.07699633506126702, 0.07861776393838227, 0.07982065505348146, 0.07426340004894882, 0.07845385302789509, 0.07923388993367553, 0.07953391096089035, 0.07874732604250312, 0.08072333491872996, 0.07974270894192159, 0.08330576994922012, 0.078463398036547, 0.07501607097219676, 0.08599129295907915, 0.08533507399260998]
[0.0015073389798516826, 0.0014261315500705826, 0.0015277703487485343, 0.001440106266254217, 0.001401279223797729, 0.0017046650600790673, 0.001388396796941453, 0.0013847746316115468, 0.0013951412663433928, 0.001394722102285952, 0.0014042513259705535, 0.0013924386347548999, 0.0014005156105612309, 0.0013943209992341545, 0.001395420879315661, 0.0014192683251612649, 0.001492839470524721, 0.0014660591235840504, 0.001475604144115077, 0.0015761836536456735, 0.003227743408072512, 0.0014462347156653295, 0.0014524946740011172, 0.0014569809783858303, 0.0014677467751221694, 0.001426144367635098, 0.0013829768986954372, 0.0013952617538256608, 0.001394340162621621, 0.0014629096139640529, 0.0014061141017900438, 0.001376039940122591, 0.0013693581615593664, 0.0013767283680677718, 0.0014740161440924418, 0.0014888824876017716, 0.0015047298986654804, 0.001495333307669783, 0.0015136686733410675, 0.0013528660614499633, 0.0013434993464271634, 0.0013445327557357295, 0.00134077057128354, 0.0013406963672070783, 0.001354897917457381, 0.0013315811235343619, 0.0013407302226833239, 0.0013411241816356778, 0.0013284520816285999, 0.0013480518388618923, 0.001334680163548613, 0.001334993266120401, 0.0013460472858111774, 0.0013416153687641633, 0.0013392359607529883, 0.0013456790403900097, 0.0013408761219672707, 0.0013377114308389779, 0.0013487702045513659, 0.0013637275111918546, 0.00133523671431657, 0.0014389410201574163, 0.00133453067202045, 0.0013471698359947425, 0.0013428104076800601, 0.0013496410600574954, 0.0013325634882880412, 0.0013502951431069144, 0.0013498214301558174, 0.0013412010395063125, 0.001346062980497218, 0.0013444937969919067, 0.001351565305542733, 0.0013463192652644856, 0.0013764057764593437, 0.0013567876541150771, 0.001353087549915119, 0.0013471965735055963, 0.0013531487154755362, 0.0013555467974546613, 0.0013390557949754353, 0.0013478086329996586, 0.001345335714975182, 0.0013393415898388745, 0.0013236205915597324, 0.001474240528685706, 0.0014907297136604178, 0.0014815395294061424, 0.0014216643045371284, 0.0015419560617634229, 0.0014731106728467407, 0.0014802171012424693, 0.0014759062442510408, 0.001470522306460355, 0.0015287986334071172, 0.001583819225316449, 0.0014718127964368584, 0.001637146223754603, 0.0014813031445314385, 0.0014570766337672059, 0.0014771848574888949, 0.0014700021034068599, 0.0015034347140629376, 0.0014857399795318441, 0.001406725918950171, 0.0013856413888231832, 0.0013952578170871247, 0.0014042808978381206, 0.0014355711838496582, 0.001425249917831804, 0.001417476795518733, 0.0014366012432479433, 0.0014381150819589288, 0.001421943183380122, 0.0014362730827106505, 0.0016757746117797736, 0.001690601205396257, 0.0017123973657548123, 0.0018833976533568027, 0.0016250173057600552, 0.001678595388289161, 0.0017727444504330658, 0.001688515204385075, 0.0016705847549613338, 0.0016512898775767914, 0.002023973939370136, 0.0015813061843949314, 0.0016548356316907673, 0.0015562218162516247, 0.0016516644070495147, 0.0016464683065685083, 0.0016303117748121827, 0.0016337647554179539, 0.0016330930794000017, 0.0016203376117675584, 0.001624352671681162, 0.0016683012232830633, 0.0016885297753068866, 0.0017166741011778312, 0.0017560823245582227, 0.001658316487826559, 0.0016575983074512712, 0.001666618102439204, 0.0016785630818494425, 0.0017000403069910041, 0.0015623913673988106, 0.0016636666723013837, 0.001654234284782136, 0.001665773141026801, 0.001670671774226488, 0.001566207019745239, 0.001656411244172831, 0.0016624761640797465, 0.001711966510272908, 0.0015761922873860719, 0.0017573283676400172, 0.001697414530897323, 0.0016602867553770846, 0.001660335243546537, 0.0016803576739276855, 0.0016805463475270234, 0.0016792589176104081, 0.0016953678164935233, 0.0016731932635742183, 0.0016729435930978886, 0.0015665937324377652, 0.0016880525295071456, 0.0015664145716332964, 0.0015604285314222987, 0.0016441775096238268, 0.0016430978169094544, 0.0015558189788491142, 0.0017875380201113162, 0.0016495412647990243, 0.0016836314295817698, 0.0017189094701743856, 0.0017176632631608114, 0.001827707490408603, 0.0016829413670704377, 0.001712541896564772, 0.0017836164702110145, 0.0017200396111120982, 0.001721766428090632, 0.0018341604487172195, 0.001709827673336377, 0.0016302573470854942, 0.0016209868576415644, 0.0016348490619803875, 0.0016547697738801337, 0.0016291034693012433, 0.0015849765100307306, 0.0016451034081947742, 0.0015857824509279157, 0.0016014538348975535, 0.0017352507768046794, 0.0015978831441464778, 0.0015713537767605514, 0.0016044441620078013, 0.0016289929602751319, 0.0015155795928356902, 0.001601099041385614, 0.0016170181619117455, 0.0016231410400181705, 0.0016070882865816963, 0.0016474149983414278, 0.0016274022233045222, 0.0017001177540657167, 0.0016012938374805512, 0.0015309402239223828, 0.001754924346103656, 0.0017415321222981628]
[663.420778847235, 701.1975858401756, 654.5486373780884, 694.3932009969279, 713.6336448989895, 586.6255039882245, 720.2551908812627, 722.1391677548563, 716.7732932314148, 716.9887093357152, 712.123237134098, 718.1645029376984, 714.0227445228321, 717.1949648246426, 716.6296669506748, 704.5884011301208, 669.8643891352284, 682.1007310778276, 677.6885277722653, 634.4438338051691, 309.81397018704246, 691.4506954979001, 688.4706828186489, 686.3507587503898, 681.3164347894848, 701.1912837816333, 723.0778771093721, 716.7113964516732, 717.1851079149958, 683.5692311094296, 711.1798386254408, 726.723091998994, 730.2691348925417, 726.3596967958848, 678.4186211309811, 671.6446786950643, 664.5710973689585, 668.7472250306027, 660.646558663816, 739.1714734333926, 744.3248875850601, 743.752798683435, 745.8397591786975, 745.8810394803913, 738.0629840192041, 750.9869149734869, 745.8622048502868, 745.6431057565214, 752.7557928729066, 741.8112354227127, 749.2431724925212, 749.0674487865256, 742.915951423923, 745.3701137317439, 746.6944058444695, 743.1192505682308, 745.7810483886065, 747.5453800770963, 741.4161408856334, 733.2843194796529, 748.9308744119141, 694.9555165858033, 749.3271012542725, 742.2969051720237, 744.7067689381964, 740.9377423338021, 750.4332880114483, 740.5788320463665, 740.8387344128656, 745.6003764865061, 742.907289249284, 743.7743500470904, 739.8828572315573, 742.765869731166, 726.5299355052061, 737.035008364827, 739.0504776004562, 742.2821729703917, 739.0170707501065, 737.7096842969354, 746.7948712460819, 741.9450918446917, 743.308892248094, 746.6355167245288, 755.5035078606753, 678.3153634308953, 670.8124154475638, 674.9735529505839, 703.4009342490907, 648.5269099408532, 678.8356220836627, 675.5765753284547, 677.5498131369836, 680.0304868595067, 654.1083816718073, 631.3851884202244, 679.4342340418023, 610.8189882432231, 675.0812645552826, 686.3057006236832, 676.9633434368709, 680.2711354510388, 665.1436145820811, 673.0652831426798, 710.8705303064953, 721.6874496288639, 716.7134186624353, 712.1082409790606, 696.5868437943834, 701.6313332059505, 705.4789208270919, 696.087383120418, 695.3546434113254, 703.2629796240418, 696.246425584137, 596.7389605801103, 591.5055524674206, 583.9766049623694, 530.9553180220267, 615.3780617937965, 595.736177387696, 564.0970980085194, 592.2363016945299, 598.5927963428263, 605.5871919153674, 494.0775078908386, 632.3885973940199, 604.2896229991657, 642.5819183081743, 605.4498696780485, 607.3606130227632, 613.3796096241796, 612.0832247627826, 612.3349689090593, 617.1553340103868, 615.6298551625655, 599.4121361561384, 592.2311910776061, 582.5217490692542, 569.4493851542921, 603.0212009232514, 603.2824692838902, 600.017483631334, 595.7476432152904, 588.221347392613, 640.0444990072347, 601.0819454696895, 604.5092942392382, 600.3218417746782, 598.5616178037093, 638.4851985675948, 603.7148102670446, 601.5123835195219, 584.1235760158579, 634.4403585798415, 569.0456140208661, 589.1312827817958, 602.3055937544234, 602.2880041165441, 595.1113953391778, 595.0445826570218, 595.5007828232967, 589.842505131582, 597.659590060645, 597.7487849116543, 638.3275888917973, 592.3986265356127, 638.4005984809645, 640.8495998778749, 608.2068354217977, 608.6064929968236, 642.7482975813355, 559.4286603972354, 606.2291506977471, 593.9542244399711, 581.764204195436, 582.1862884578546, 547.1335020772058, 594.1977656302663, 583.9273199715134, 560.6586487069683, 581.3819597755927, 580.7988724167185, 545.2085725103183, 584.8542608090473, 613.4000878988572, 616.9081478272675, 611.6772632139154, 604.3136729861714, 613.8345530802418, 630.9241769019096, 607.8645239069394, 630.6035228318066, 624.4326112990768, 576.2855797946492, 625.8279922804732, 636.393926555206, 623.2688077773926, 613.8761949168296, 659.8135820296795, 624.5709816518194, 618.4222438279445, 616.0894064934772, 622.2433505050409, 607.0115914974506, 614.4762405261125, 588.1945515883048, 624.4950030991084, 653.1933673007334, 569.8251336134431, 574.2070371233685]
Elapsed: 0.0752122185719354~0.009155772923377709
Time per graph: 0.0015349432361619468~0.00018685250864036143
Speed: 659.2072403134438~66.67505441543729
Total Time: 0.0861
best val loss: 0.4542965888977051 test_score: 0.8980

Testing...
Test loss: 0.4151 score: 0.8367 time: 0.08s
test Score 0.8367
Epoch Time List: [0.35025557794142514, 0.34973484894726425, 0.3509860330959782, 0.35968710109591484, 0.34449524781666696, 0.3676899520214647, 0.35729762609116733, 0.34548684291075915, 0.34303716907743365, 0.34589120000600815, 0.3405627228785306, 0.33856255398131907, 0.34320018510334194, 0.3405399650800973, 0.34791362890973687, 0.34923763188999146, 0.46951979387085885, 0.37474683893378824, 0.36433169711381197, 0.36527635413222015, 0.44757716706953943, 0.35487512208055705, 0.3583817781182006, 0.35686489602085203, 0.3596197438891977, 0.35168124607298523, 0.4126796911004931, 0.3389126032125205, 0.33973319618962705, 0.3391916450345889, 0.33862650906667113, 0.38200218009296805, 0.3376869950443506, 0.33880763698834926, 0.3453123898943886, 0.36738421383779496, 0.3704198020277545, 0.3685165380593389, 0.3702639879193157, 0.33290875295642763, 0.3312104409560561, 0.3336681700311601, 0.33160880906507373, 0.33210192702244967, 0.33295236295089126, 0.34240920504089445, 0.33209972886834294, 0.3321028130594641, 0.3247404210269451, 0.3298833610024303, 0.3320108629995957, 0.3299944648751989, 0.3319164681015536, 0.33229390799533576, 0.3303412910318002, 0.33308176207356155, 0.33237248903606087, 0.33186021400615573, 0.3339804249117151, 0.33207037893589586, 0.33402457600459456, 0.33237653295509517, 0.33120850985869765, 0.3295228348579258, 0.33061267191078514, 0.33291260595433414, 0.331081647076644, 0.33208924299106, 0.3310389081016183, 0.33216009207535535, 0.33293708390556276, 0.33392276300583035, 0.3306152910226956, 0.3316058620112017, 0.3302575189154595, 0.3380327691556886, 0.3325191739713773, 0.32823134190402925, 0.3252913129981607, 0.3320072559872642, 0.32583682297263294, 0.3335519441170618, 0.3327176780439913, 0.337231794022955, 0.3267574032070115, 0.3439234639517963, 0.3686596070183441, 0.36387394391931593, 0.3562169590732083, 0.3582324089948088, 0.36600983201060444, 0.37091391102876514, 0.3631249559111893, 0.365326048922725, 0.3691650229739025, 0.3682702179066837, 0.370808805921115, 0.3948854600312188, 0.36801513913087547, 0.3590038319816813, 0.3626213880488649, 0.3626661271555349, 0.36670656490605325, 0.3613232650095597, 0.3508384379092604, 0.34125748788937926, 0.33875163993798196, 0.3503663861192763, 0.35709787893574685, 0.3576204718556255, 0.3471138260792941, 0.3523693950846791, 0.3515868061222136, 0.3489933309610933, 0.3526856548851356, 0.35632392298430204, 0.3636693370062858, 0.3563139820471406, 0.37439253088086843, 0.3595090879825875, 0.36081476998515427, 0.3643719400279224, 0.3543579428223893, 0.348201586981304, 0.35683933598920703, 0.3896325237583369, 0.3517434620298445, 0.34390557499136776, 0.3585659860400483, 0.35304692503996193, 0.3472635210491717, 0.341657868004404, 0.34492898115422577, 0.35277283005416393, 0.34552374109625816, 0.34351079096086323, 0.35086370608769357, 0.35836184909567237, 0.3555593640776351, 0.37681058375164866, 0.3506662539439276, 0.34888345294166356, 0.3517694000620395, 0.36150756792631, 0.3576047510141507, 0.3466960929799825, 0.35756059689447284, 0.3515194790670648, 0.3631125210085884, 0.34951904497575015, 0.3475298259872943, 0.35722416499629617, 0.35558921203482896, 0.3618680468061939, 0.35439931706059724, 0.35670437605585903, 0.3488640961004421, 0.3506678829435259, 0.35195486003067344, 0.35116865497548133, 0.3535140729509294, 0.35832802997902036, 0.3529176899464801, 0.3555203400319442, 0.3579562020022422, 0.35260871693026274, 0.3606704161502421, 0.3492697310866788, 0.34583273995667696, 0.37364827701821923, 0.3561411671107635, 0.33851583604700863, 0.3532671829452738, 0.3635844470700249, 0.3527621611719951, 0.3616379218874499, 0.41016537393443286, 0.3940253622131422, 0.3628124389797449, 0.3638340618927032, 0.3543827789835632, 0.46072867698967457, 0.36796426691580564, 0.3792441750410944, 0.367052294081077, 0.4194147839443758, 0.3518460700288415, 0.3456734480569139, 0.3450055589200929, 0.35238625411875546, 0.38978956383652985, 0.3507106531178579, 0.34314719296526164, 0.33668963296804577, 0.3606446160702035, 0.3393983888672665, 0.3342654468724504, 0.35228717303834856, 0.3503878200426698, 0.33706693397834897, 0.3533831579843536, 0.34239452704787254, 0.34919881192035973, 0.34232583292759955, 0.3512397148879245, 0.3523500320734456, 0.34815561899449676, 0.3399247588822618, 0.3463847889797762, 0.36853678594343364, 0.36201356595847756]
Total Epoch List: [115, 96]
Total Time List: [0.07093218003865331, 0.08607852808199823]
========================k_idx:2========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: GPSTransformer
GPS(
  (x_lin): Linear(in_features=14887, out_features=64, bias=True)
  (pe_lin): Linear(in_features=20, out_features=8, bias=True)
  (pe_norm): BatchNorm1d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (convs): ModuleList(
    (0-1): 2 x GPSConv(64, conv=GINEConv(nn=Sequential(
      (0): Linear(in_features=64, out_features=64, bias=True)
      (1): ReLU()
      (2): Linear(in_features=64, out_features=64, bias=True)
    )), heads=1, attn_type=multihead)
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=32, bias=True)
    (1): ReLU()
    (2): Linear(in_features=32, out_features=16, bias=True)
    (3): ReLU()
    (4): Linear(in_features=16, out_features=2, bias=True)
  )
)
Total number of parameters: 1039906
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7d29a7d73fa0>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.7129;  Loss pred: 0.7129; Loss self: 0.0000; time: 0.24s
Val loss: 0.7207 score: 0.4490 time: 0.07s
Test loss: 0.7289 score: 0.4375 time: 0.07s
Epoch 2/1000, LR 0.000000
Train loss: 0.7102;  Loss pred: 0.7102; Loss self: 0.0000; time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6894 score: 0.5102 time: 0.07s
Test loss: 0.6917 score: 0.4792 time: 0.07s
Epoch 3/1000, LR 0.000030
Train loss: 0.6558;  Loss pred: 0.6558; Loss self: 0.0000; time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6833 score: 0.5102 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6834 score: 0.5000 time: 0.07s
Epoch 4/1000, LR 0.000060
Train loss: 0.5668;  Loss pred: 0.5668; Loss self: 0.0000; time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6869 score: 0.5102 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6852 score: 0.5000 time: 0.07s
     INFO: Early stopping counter 1 of 20
Epoch 5/1000, LR 0.000090
Train loss: 0.4819;  Loss pred: 0.4819; Loss self: 0.0000; time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6971 score: 0.5102 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6917 score: 0.5000 time: 0.07s
     INFO: Early stopping counter 2 of 20
Epoch 6/1000, LR 0.000120
Train loss: 0.3977;  Loss pred: 0.3977; Loss self: 0.0000; time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7021 score: 0.5102 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6928 score: 0.5000 time: 0.07s
     INFO: Early stopping counter 3 of 20
Epoch 7/1000, LR 0.000150
Train loss: 0.3164;  Loss pred: 0.3164; Loss self: 0.0000; time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7087 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6978 score: 0.5000 time: 0.07s
     INFO: Early stopping counter 4 of 20
Epoch 8/1000, LR 0.000180
Train loss: 0.2641;  Loss pred: 0.2641; Loss self: 0.0000; time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7128 score: 0.5102 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7028 score: 0.5000 time: 0.07s
     INFO: Early stopping counter 5 of 20
Epoch 9/1000, LR 0.000210
Train loss: 0.2135;  Loss pred: 0.2135; Loss self: 0.0000; time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7120 score: 0.5102 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7021 score: 0.5000 time: 0.07s
     INFO: Early stopping counter 6 of 20
Epoch 10/1000, LR 0.000240
Train loss: 0.1736;  Loss pred: 0.1736; Loss self: 0.0000; time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7131 score: 0.5102 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6987 score: 0.5000 time: 0.07s
     INFO: Early stopping counter 7 of 20
Epoch 11/1000, LR 0.000270
Train loss: 0.1399;  Loss pred: 0.1399; Loss self: 0.0000; time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7118 score: 0.5102 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6945 score: 0.5000 time: 0.07s
     INFO: Early stopping counter 8 of 20
Epoch 12/1000, LR 0.000270
Train loss: 0.1191;  Loss pred: 0.1191; Loss self: 0.0000; time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7104 score: 0.5102 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6903 score: 0.5000 time: 0.07s
     INFO: Early stopping counter 9 of 20
Epoch 13/1000, LR 0.000270
Train loss: 0.0941;  Loss pred: 0.0941; Loss self: 0.0000; time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7079 score: 0.5102 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6867 score: 0.5000 time: 0.06s
     INFO: Early stopping counter 10 of 20
Epoch 14/1000, LR 0.000270
Train loss: 0.0759;  Loss pred: 0.0759; Loss self: 0.0000; time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7036 score: 0.5102 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6754 score: 0.5000 time: 0.06s
     INFO: Early stopping counter 11 of 20
Epoch 15/1000, LR 0.000270
Train loss: 0.0655;  Loss pred: 0.0655; Loss self: 0.0000; time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6951 score: 0.5102 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6610 score: 0.5000 time: 0.07s
     INFO: Early stopping counter 12 of 20
Epoch 16/1000, LR 0.000270
Train loss: 0.0548;  Loss pred: 0.0548; Loss self: 0.0000; time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6822 score: 0.5102 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6458 score: 0.5000 time: 0.08s
Epoch 17/1000, LR 0.000270
Train loss: 0.0456;  Loss pred: 0.0456; Loss self: 0.0000; time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6716 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6334 score: 0.5000 time: 0.07s
Epoch 18/1000, LR 0.000270
Train loss: 0.0381;  Loss pred: 0.0381; Loss self: 0.0000; time: 0.22s
Val loss: 0.6635 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6279 score: 0.5000 time: 0.07s
Epoch 19/1000, LR 0.000270
Train loss: 0.0327;  Loss pred: 0.0327; Loss self: 0.0000; time: 0.21s
Val loss: 0.6552 score: 0.5306 time: 0.06s
Test loss: 0.6215 score: 0.5000 time: 0.07s
Epoch 20/1000, LR 0.000270
Train loss: 0.0284;  Loss pred: 0.0284; Loss self: 0.0000; time: 0.21s
Val loss: 0.6501 score: 0.5102 time: 0.07s
Test loss: 0.6199 score: 0.5208 time: 0.08s
Epoch 21/1000, LR 0.000270
Train loss: 0.0237;  Loss pred: 0.0237; Loss self: 0.0000; time: 0.20s
Val loss: 0.6482 score: 0.5306 time: 0.06s
Test loss: 0.6155 score: 0.5208 time: 0.07s
Epoch 22/1000, LR 0.000270
Train loss: 0.0200;  Loss pred: 0.0200; Loss self: 0.0000; time: 0.20s
Val loss: 0.6467 score: 0.5306 time: 0.07s
Test loss: 0.6112 score: 0.5833 time: 0.07s
Epoch 23/1000, LR 0.000270
Train loss: 0.0170;  Loss pred: 0.0170; Loss self: 0.0000; time: 0.20s
Val loss: 0.6473 score: 0.5306 time: 0.06s
Test loss: 0.6079 score: 0.6250 time: 0.07s
     INFO: Early stopping counter 1 of 20
Epoch 24/1000, LR 0.000270
Train loss: 0.0148;  Loss pred: 0.0148; Loss self: 0.0000; time: 0.19s
Val loss: 0.6512 score: 0.5306 time: 0.06s
Test loss: 0.6063 score: 0.6458 time: 0.06s
     INFO: Early stopping counter 2 of 20
Epoch 25/1000, LR 0.000270
Train loss: 0.0134;  Loss pred: 0.0134; Loss self: 0.0000; time: 0.20s
Val loss: 0.6566 score: 0.5102 time: 0.06s
Test loss: 0.6058 score: 0.6667 time: 0.06s
     INFO: Early stopping counter 3 of 20
Epoch 26/1000, LR 0.000270
Train loss: 0.0113;  Loss pred: 0.0113; Loss self: 0.0000; time: 0.20s
Val loss: 0.6642 score: 0.4898 time: 0.06s
Test loss: 0.6081 score: 0.6875 time: 0.06s
     INFO: Early stopping counter 4 of 20
Epoch 27/1000, LR 0.000270
Train loss: 0.0100;  Loss pred: 0.0100; Loss self: 0.0000; time: 0.20s
Val loss: 0.6740 score: 0.4898 time: 0.06s
Test loss: 0.6135 score: 0.6875 time: 0.06s
     INFO: Early stopping counter 5 of 20
Epoch 28/1000, LR 0.000270
Train loss: 0.0083;  Loss pred: 0.0083; Loss self: 0.0000; time: 0.20s
Val loss: 0.6841 score: 0.5306 time: 0.06s
Test loss: 0.6213 score: 0.6875 time: 0.06s
     INFO: Early stopping counter 6 of 20
Epoch 29/1000, LR 0.000270
Train loss: 0.0075;  Loss pred: 0.0075; Loss self: 0.0000; time: 0.20s
Val loss: 0.6915 score: 0.5714 time: 0.06s
Test loss: 0.6281 score: 0.7083 time: 0.06s
     INFO: Early stopping counter 7 of 20
Epoch 30/1000, LR 0.000270
Train loss: 0.0063;  Loss pred: 0.0063; Loss self: 0.0000; time: 0.20s
Val loss: 0.6966 score: 0.6122 time: 0.06s
Test loss: 0.6321 score: 0.7292 time: 0.06s
     INFO: Early stopping counter 8 of 20
Epoch 31/1000, LR 0.000270
Train loss: 0.0052;  Loss pred: 0.0052; Loss self: 0.0000; time: 0.20s
Val loss: 0.6987 score: 0.6122 time: 0.07s
Test loss: 0.6329 score: 0.7292 time: 0.07s
     INFO: Early stopping counter 9 of 20
Epoch 32/1000, LR 0.000270
Train loss: 0.0053;  Loss pred: 0.0053; Loss self: 0.0000; time: 0.20s
Val loss: 0.6984 score: 0.6531 time: 0.06s
Test loss: 0.6301 score: 0.7292 time: 0.06s
     INFO: Early stopping counter 10 of 20
Epoch 33/1000, LR 0.000270
Train loss: 0.0047;  Loss pred: 0.0047; Loss self: 0.0000; time: 0.20s
Val loss: 0.6967 score: 0.6531 time: 0.06s
Test loss: 0.6252 score: 0.7500 time: 0.07s
     INFO: Early stopping counter 11 of 20
Epoch 34/1000, LR 0.000270
Train loss: 0.0041;  Loss pred: 0.0041; Loss self: 0.0000; time: 0.20s
Val loss: 0.6936 score: 0.6531 time: 0.06s
Test loss: 0.6202 score: 0.7500 time: 0.06s
     INFO: Early stopping counter 12 of 20
Epoch 35/1000, LR 0.000270
Train loss: 0.0033;  Loss pred: 0.0033; Loss self: 0.0000; time: 0.20s
Val loss: 0.6883 score: 0.6531 time: 0.06s
Test loss: 0.6144 score: 0.7500 time: 0.07s
     INFO: Early stopping counter 13 of 20
Epoch 36/1000, LR 0.000270
Train loss: 0.0032;  Loss pred: 0.0032; Loss self: 0.0000; time: 0.20s
Val loss: 0.6795 score: 0.6531 time: 0.06s
Test loss: 0.6062 score: 0.7500 time: 0.07s
     INFO: Early stopping counter 14 of 20
Epoch 37/1000, LR 0.000270
Train loss: 0.0029;  Loss pred: 0.0029; Loss self: 0.0000; time: 0.20s
Val loss: 0.6690 score: 0.6531 time: 0.06s
Test loss: 0.5963 score: 0.7708 time: 0.07s
     INFO: Early stopping counter 15 of 20
Epoch 38/1000, LR 0.000270
Train loss: 0.0028;  Loss pred: 0.0028; Loss self: 0.0000; time: 0.20s
Val loss: 0.6581 score: 0.6531 time: 0.06s
Test loss: 0.5865 score: 0.7708 time: 0.07s
     INFO: Early stopping counter 16 of 20
Epoch 39/1000, LR 0.000269
Train loss: 0.0027;  Loss pred: 0.0027; Loss self: 0.0000; time: 0.20s
Val loss: 0.6461 score: 0.6531 time: 0.06s
Test loss: 0.5750 score: 0.7500 time: 0.07s
Epoch 40/1000, LR 0.000269
Train loss: 0.0024;  Loss pred: 0.0024; Loss self: 0.0000; time: 0.20s
Val loss: 0.6343 score: 0.6531 time: 0.06s
Test loss: 0.5641 score: 0.7708 time: 0.06s
Epoch 41/1000, LR 0.000269
Train loss: 0.0021;  Loss pred: 0.0021; Loss self: 0.0000; time: 0.20s
Val loss: 0.6225 score: 0.6531 time: 0.06s
Test loss: 0.5534 score: 0.7708 time: 0.06s
Epoch 42/1000, LR 0.000269
Train loss: 0.0021;  Loss pred: 0.0021; Loss self: 0.0000; time: 0.20s
Val loss: 0.6113 score: 0.6735 time: 0.06s
Test loss: 0.5437 score: 0.7708 time: 0.07s
Epoch 43/1000, LR 0.000269
Train loss: 0.0018;  Loss pred: 0.0018; Loss self: 0.0000; time: 0.20s
Val loss: 0.6014 score: 0.6735 time: 0.06s
Test loss: 0.5347 score: 0.7708 time: 0.06s
Epoch 44/1000, LR 0.000269
Train loss: 0.0018;  Loss pred: 0.0018; Loss self: 0.0000; time: 0.20s
Val loss: 0.5925 score: 0.6735 time: 0.06s
Test loss: 0.5262 score: 0.7708 time: 0.06s
Epoch 45/1000, LR 0.000269
Train loss: 0.0016;  Loss pred: 0.0016; Loss self: 0.0000; time: 0.20s
Val loss: 0.5838 score: 0.6735 time: 0.06s
Test loss: 0.5178 score: 0.7708 time: 0.06s
Epoch 46/1000, LR 0.000269
Train loss: 0.0015;  Loss pred: 0.0015; Loss self: 0.0000; time: 0.20s
Val loss: 0.5751 score: 0.6735 time: 0.06s
Test loss: 0.5094 score: 0.7708 time: 0.06s
Epoch 47/1000, LR 0.000269
Train loss: 0.0014;  Loss pred: 0.0014; Loss self: 0.0000; time: 0.19s
Val loss: 0.5674 score: 0.6939 time: 0.06s
Test loss: 0.5016 score: 0.7708 time: 0.06s
Epoch 48/1000, LR 0.000269
Train loss: 0.0013;  Loss pred: 0.0013; Loss self: 0.0000; time: 0.19s
Val loss: 0.5603 score: 0.6939 time: 0.06s
Test loss: 0.4933 score: 0.7708 time: 0.06s
Epoch 49/1000, LR 0.000269
Train loss: 0.0013;  Loss pred: 0.0013; Loss self: 0.0000; time: 0.19s
Val loss: 0.5544 score: 0.6939 time: 0.06s
Test loss: 0.4854 score: 0.7917 time: 0.06s
Epoch 50/1000, LR 0.000269
Train loss: 0.0012;  Loss pred: 0.0012; Loss self: 0.0000; time: 0.20s
Val loss: 0.5481 score: 0.6939 time: 0.06s
Test loss: 0.4778 score: 0.7917 time: 0.06s
Epoch 51/1000, LR 0.000269
Train loss: 0.0012;  Loss pred: 0.0012; Loss self: 0.0000; time: 0.19s
Val loss: 0.5418 score: 0.6939 time: 0.06s
Test loss: 0.4706 score: 0.8125 time: 0.06s
Epoch 52/1000, LR 0.000269
Train loss: 0.0011;  Loss pred: 0.0011; Loss self: 0.0000; time: 0.20s
Val loss: 0.5353 score: 0.7143 time: 0.07s
Test loss: 0.4638 score: 0.8125 time: 0.06s
Epoch 53/1000, LR 0.000269
Train loss: 0.0010;  Loss pred: 0.0010; Loss self: 0.0000; time: 0.20s
Val loss: 0.5287 score: 0.7347 time: 0.06s
Test loss: 0.4575 score: 0.8333 time: 0.07s
Epoch 54/1000, LR 0.000269
Train loss: 0.0010;  Loss pred: 0.0010; Loss self: 0.0000; time: 0.20s
Val loss: 0.5223 score: 0.7347 time: 0.06s
Test loss: 0.4505 score: 0.8333 time: 0.06s
Epoch 55/1000, LR 0.000269
Train loss: 0.0011;  Loss pred: 0.0011; Loss self: 0.0000; time: 0.20s
Val loss: 0.5162 score: 0.7551 time: 0.06s
Test loss: 0.4440 score: 0.8333 time: 0.06s
Epoch 56/1000, LR 0.000269
Train loss: 0.0009;  Loss pred: 0.0009; Loss self: 0.0000; time: 0.20s
Val loss: 0.5099 score: 0.7551 time: 0.06s
Test loss: 0.4375 score: 0.8542 time: 0.07s
Epoch 57/1000, LR 0.000269
Train loss: 0.0009;  Loss pred: 0.0009; Loss self: 0.0000; time: 0.20s
Val loss: 0.5042 score: 0.7551 time: 0.06s
Test loss: 0.4310 score: 0.8542 time: 0.06s
Epoch 58/1000, LR 0.000269
Train loss: 0.0009;  Loss pred: 0.0009; Loss self: 0.0000; time: 0.20s
Val loss: 0.4991 score: 0.7551 time: 0.06s
Test loss: 0.4253 score: 0.8542 time: 0.06s
Epoch 59/1000, LR 0.000268
Train loss: 0.0008;  Loss pred: 0.0008; Loss self: 0.0000; time: 0.20s
Val loss: 0.4957 score: 0.7755 time: 0.06s
Test loss: 0.4209 score: 0.8333 time: 0.06s
Epoch 60/1000, LR 0.000268
Train loss: 0.0007;  Loss pred: 0.0007; Loss self: 0.0000; time: 0.20s
Val loss: 0.4929 score: 0.7959 time: 0.06s
Test loss: 0.4172 score: 0.8333 time: 0.06s
Epoch 61/1000, LR 0.000268
Train loss: 0.0008;  Loss pred: 0.0008; Loss self: 0.0000; time: 0.20s
Val loss: 0.4907 score: 0.7959 time: 0.06s
Test loss: 0.4154 score: 0.8125 time: 0.06s
Epoch 62/1000, LR 0.000268
Train loss: 0.0007;  Loss pred: 0.0007; Loss self: 0.0000; time: 0.20s
Val loss: 0.4891 score: 0.7959 time: 0.06s
Test loss: 0.4145 score: 0.8125 time: 0.06s
Epoch 63/1000, LR 0.000268
Train loss: 0.0007;  Loss pred: 0.0007; Loss self: 0.0000; time: 0.20s
Val loss: 0.4874 score: 0.7959 time: 0.06s
Test loss: 0.4138 score: 0.8125 time: 0.06s
Epoch 64/1000, LR 0.000268
Train loss: 0.0007;  Loss pred: 0.0007; Loss self: 0.0000; time: 0.20s
Val loss: 0.4852 score: 0.7959 time: 0.06s
Test loss: 0.4126 score: 0.8125 time: 0.06s
Epoch 65/1000, LR 0.000268
Train loss: 0.0007;  Loss pred: 0.0007; Loss self: 0.0000; time: 0.20s
Val loss: 0.4823 score: 0.7959 time: 0.06s
Test loss: 0.4105 score: 0.8125 time: 0.06s
Epoch 66/1000, LR 0.000268
Train loss: 0.0006;  Loss pred: 0.0006; Loss self: 0.0000; time: 0.20s
Val loss: 0.4788 score: 0.7959 time: 0.06s
Test loss: 0.4076 score: 0.8333 time: 0.06s
Epoch 67/1000, LR 0.000268
Train loss: 0.0006;  Loss pred: 0.0006; Loss self: 0.0000; time: 0.20s
Val loss: 0.4750 score: 0.7959 time: 0.06s
Test loss: 0.4031 score: 0.8333 time: 0.07s
Epoch 68/1000, LR 0.000268
Train loss: 0.0006;  Loss pred: 0.0006; Loss self: 0.0000; time: 0.20s
Val loss: 0.4704 score: 0.8163 time: 0.06s
Test loss: 0.3970 score: 0.8333 time: 0.06s
Epoch 69/1000, LR 0.000268
Train loss: 0.0006;  Loss pred: 0.0006; Loss self: 0.0000; time: 0.20s
Val loss: 0.4659 score: 0.8163 time: 0.06s
Test loss: 0.3906 score: 0.8333 time: 0.06s
Epoch 70/1000, LR 0.000268
Train loss: 0.0006;  Loss pred: 0.0006; Loss self: 0.0000; time: 0.20s
Val loss: 0.4606 score: 0.8163 time: 0.06s
Test loss: 0.3841 score: 0.8333 time: 0.06s
Epoch 71/1000, LR 0.000268
Train loss: 0.0006;  Loss pred: 0.0006; Loss self: 0.0000; time: 0.20s
Val loss: 0.4563 score: 0.8163 time: 0.06s
Test loss: 0.3780 score: 0.8333 time: 0.06s
Epoch 72/1000, LR 0.000267
Train loss: 0.0006;  Loss pred: 0.0006; Loss self: 0.0000; time: 0.20s
Val loss: 0.4521 score: 0.8163 time: 0.06s
Test loss: 0.3715 score: 0.8542 time: 0.06s
Epoch 73/1000, LR 0.000267
Train loss: 0.0006;  Loss pred: 0.0006; Loss self: 0.0000; time: 0.20s
Val loss: 0.4489 score: 0.8163 time: 0.06s
Test loss: 0.3660 score: 0.8542 time: 0.06s
Epoch 74/1000, LR 0.000267
Train loss: 0.0006;  Loss pred: 0.0006; Loss self: 0.0000; time: 0.20s
Val loss: 0.4468 score: 0.8163 time: 0.06s
Test loss: 0.3612 score: 0.8750 time: 0.06s
Epoch 75/1000, LR 0.000267
Train loss: 0.0005;  Loss pred: 0.0005; Loss self: 0.0000; time: 0.20s
Val loss: 0.4462 score: 0.8163 time: 0.06s
Test loss: 0.3580 score: 0.8958 time: 0.06s
Epoch 76/1000, LR 0.000267
Train loss: 0.0005;  Loss pred: 0.0005; Loss self: 0.0000; time: 0.19s
Val loss: 0.4471 score: 0.8163 time: 0.06s
Test loss: 0.3562 score: 0.8958 time: 0.06s
     INFO: Early stopping counter 1 of 20
Epoch 77/1000, LR 0.000267
Train loss: 0.0005;  Loss pred: 0.0005; Loss self: 0.0000; time: 0.20s
Val loss: 0.4506 score: 0.8163 time: 0.06s
Test loss: 0.3572 score: 0.8958 time: 0.06s
     INFO: Early stopping counter 2 of 20
Epoch 78/1000, LR 0.000267
Train loss: 0.0005;  Loss pred: 0.0005; Loss self: 0.0000; time: 0.21s
Val loss: 0.4564 score: 0.8163 time: 0.07s
Test loss: 0.3606 score: 0.8958 time: 0.06s
     INFO: Early stopping counter 3 of 20
Epoch 79/1000, LR 0.000267
Train loss: 0.0005;  Loss pred: 0.0005; Loss self: 0.0000; time: 0.20s
Val loss: 0.4640 score: 0.8163 time: 0.06s
Test loss: 0.3661 score: 0.8958 time: 0.06s
     INFO: Early stopping counter 4 of 20
Epoch 80/1000, LR 0.000267
Train loss: 0.0005;  Loss pred: 0.0005; Loss self: 0.0000; time: 0.20s
Val loss: 0.4729 score: 0.8163 time: 0.06s
Test loss: 0.3734 score: 0.8958 time: 0.06s
     INFO: Early stopping counter 5 of 20
Epoch 81/1000, LR 0.000267
Train loss: 0.0005;  Loss pred: 0.0005; Loss self: 0.0000; time: 0.20s
Val loss: 0.4830 score: 0.8163 time: 0.06s
Test loss: 0.3830 score: 0.8958 time: 0.06s
     INFO: Early stopping counter 6 of 20
Epoch 82/1000, LR 0.000267
Train loss: 0.0005;  Loss pred: 0.0005; Loss self: 0.0000; time: 0.19s
Val loss: 0.4926 score: 0.8163 time: 0.06s
Test loss: 0.3928 score: 0.8958 time: 0.07s
     INFO: Early stopping counter 7 of 20
Epoch 83/1000, LR 0.000266
Train loss: 0.0004;  Loss pred: 0.0004; Loss self: 0.0000; time: 0.20s
Val loss: 0.5008 score: 0.8367 time: 0.06s
Test loss: 0.4016 score: 0.8750 time: 0.06s
     INFO: Early stopping counter 8 of 20
Epoch 84/1000, LR 0.000266
Train loss: 0.0005;  Loss pred: 0.0005; Loss self: 0.0000; time: 0.20s
Val loss: 0.5097 score: 0.8367 time: 0.06s
Test loss: 0.4109 score: 0.8750 time: 0.08s
     INFO: Early stopping counter 9 of 20
Epoch 85/1000, LR 0.000266
Train loss: 0.0004;  Loss pred: 0.0004; Loss self: 0.0000; time: 0.20s
Val loss: 0.5179 score: 0.8367 time: 0.06s
Test loss: 0.4191 score: 0.8750 time: 0.06s
     INFO: Early stopping counter 10 of 20
Epoch 86/1000, LR 0.000266
Train loss: 0.0005;  Loss pred: 0.0005; Loss self: 0.0000; time: 0.21s
Val loss: 0.5260 score: 0.8367 time: 0.07s
Test loss: 0.4268 score: 0.8750 time: 0.07s
     INFO: Early stopping counter 11 of 20
Epoch 87/1000, LR 0.000266
Train loss: 0.0004;  Loss pred: 0.0004; Loss self: 0.0000; time: 0.21s
Val loss: 0.5335 score: 0.8367 time: 0.07s
Test loss: 0.4341 score: 0.8750 time: 0.07s
     INFO: Early stopping counter 12 of 20
Epoch 88/1000, LR 0.000266
Train loss: 0.0004;  Loss pred: 0.0004; Loss self: 0.0000; time: 0.20s
Val loss: 0.5410 score: 0.8367 time: 0.07s
Test loss: 0.4420 score: 0.8750 time: 0.07s
     INFO: Early stopping counter 13 of 20
Epoch 89/1000, LR 0.000266
Train loss: 0.0004;  Loss pred: 0.0004; Loss self: 0.0000; time: 0.20s
Val loss: 0.5481 score: 0.8367 time: 0.06s
Test loss: 0.4497 score: 0.8750 time: 0.06s
     INFO: Early stopping counter 14 of 20
Epoch 90/1000, LR 0.000266
Train loss: 0.0004;  Loss pred: 0.0004; Loss self: 0.0000; time: 0.20s
Val loss: 0.5547 score: 0.8367 time: 0.06s
Test loss: 0.4570 score: 0.8750 time: 0.06s
     INFO: Early stopping counter 15 of 20
Epoch 91/1000, LR 0.000266
Train loss: 0.0004;  Loss pred: 0.0004; Loss self: 0.0000; time: 0.19s
Val loss: 0.5620 score: 0.8367 time: 0.06s
Test loss: 0.4620 score: 0.8750 time: 0.07s
     INFO: Early stopping counter 16 of 20
Epoch 92/1000, LR 0.000266
Train loss: 0.0005;  Loss pred: 0.0005; Loss self: 0.0000; time: 0.20s
Val loss: 0.5712 score: 0.8367 time: 0.06s
Test loss: 0.4677 score: 0.8958 time: 0.07s
     INFO: Early stopping counter 17 of 20
Epoch 93/1000, LR 0.000265
Train loss: 0.0004;  Loss pred: 0.0004; Loss self: 0.0000; time: 0.20s
Val loss: 0.5799 score: 0.8367 time: 0.06s
Test loss: 0.4732 score: 0.8958 time: 0.06s
     INFO: Early stopping counter 18 of 20
Epoch 94/1000, LR 0.000265
Train loss: 0.0004;  Loss pred: 0.0004; Loss self: 0.0000; time: 0.19s
Val loss: 0.5891 score: 0.8367 time: 0.06s
Test loss: 0.4799 score: 0.8958 time: 0.06s
     INFO: Early stopping counter 19 of 20
Epoch 95/1000, LR 0.000265
Train loss: 0.0004;  Loss pred: 0.0004; Loss self: 0.0000; time: 0.20s
Val loss: 0.5988 score: 0.8367 time: 0.06s
Test loss: 0.4872 score: 0.8958 time: 0.06s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 074,   Train_Loss: 0.0005,   Val_Loss: 0.4462,   Val_Precision: 1.0000,   Val_Recall: 0.6400,   Val_accuracy: 0.7805,   Val_Score: 0.8163,   Val_Loss: 0.4462,   Test_Precision: 1.0000,   Test_Recall: 0.7917,   Test_accuracy: 0.8837,   Test_Score: 0.8958,   Test_loss: 0.3580


[0.07385961001273245, 0.06988044595345855, 0.07486074708867818, 0.07056520704645663, 0.06866268196608871, 0.0835285879438743, 0.0680314430501312, 0.06785395694896579, 0.06836192205082625, 0.06834138301201165, 0.06880831497255713, 0.06822949310299009, 0.06862526491750032, 0.06832172896247357, 0.06837562308646739, 0.06954414793290198, 0.07314913405571133, 0.07183689705561846, 0.07230460306163877, 0.077232999028638, 0.15815942699555308, 0.07086550106760114, 0.07117223902605474, 0.07139206794090569, 0.0719195919809863, 0.0698810740141198, 0.06776586803607643, 0.06836782593745738, 0.06832266796845943, 0.07168257108423859, 0.06889959098771214, 0.06742595706600696, 0.06709854991640896, 0.06745969003532082, 0.07222679106052965, 0.07295524189248681, 0.07373176503460854, 0.07327133207581937, 0.0741697649937123, 0.0662904370110482, 0.065831467974931, 0.06588210503105074, 0.06569775799289346, 0.06569412199314684, 0.06638999795541167, 0.06524747505318373, 0.06569578091148287, 0.06571508490014821, 0.0650941519998014, 0.06605454010423273, 0.06539932801388204, 0.06541467003989965, 0.06595631700474769, 0.065739153069444, 0.06562256207689643, 0.06593827297911048, 0.06570292997639626, 0.06554786011110991, 0.06608974002301693, 0.06682264804840088, 0.06542659900151193, 0.0705081099877134, 0.06539200292900205, 0.06601132196374238, 0.06579770997632295, 0.06613241194281727, 0.06529561092611402, 0.0661644620122388, 0.06614125007763505, 0.06571885093580931, 0.06595708604436368, 0.06588019605260342, 0.06622669997159392, 0.06596964399795979, 0.06744388304650784, 0.06648259505163878, 0.06630128994584084, 0.06601263210177422, 0.06630428705830127, 0.0664217930752784, 0.06561373395379633, 0.06604262301698327, 0.06592145003378391, 0.06562773790210485, 0.06485740898642689, 0.0722377859055996, 0.07304575596936047, 0.07259543694090098, 0.06966155092231929, 0.07555584702640772, 0.07218242296949029, 0.072530637960881, 0.072319405968301, 0.0720555930165574, 0.07491113303694874, 0.077607142040506, 0.07211882702540606, 0.08022016496397555, 0.07258385408204049, 0.07139675505459309, 0.07238205801695585, 0.07203010306693614, 0.07366830098908395, 0.07280125899706036, 0.06892957002855837, 0.06789642805233598, 0.06836763303726912, 0.06880976399406791, 0.07034298800863326, 0.0698372459737584, 0.06945636298041791, 0.07039346091914922, 0.07046763901598752, 0.06967521598562598, 0.07037738105282187, 0.08211295597720891, 0.08283945906441659, 0.0839074709219858, 0.09228648501448333, 0.0796258479822427, 0.08225117402616888, 0.08686447807122022, 0.08273724501486868, 0.08185865299310535, 0.08091320400126278, 0.09917472302913666, 0.07748400303535163, 0.0810869459528476, 0.0762548689963296, 0.08093155594542623, 0.0806769470218569, 0.07988527696579695, 0.08005447301547974, 0.08002156089060009, 0.07939654297661036, 0.07959328091237694, 0.0817467599408701, 0.08273795899003744, 0.08411703095771372, 0.08604803390335292, 0.08125750790350139, 0.08122231706511229, 0.081664287019521, 0.08224959101062268, 0.0833019750425592, 0.07655717700254172, 0.0815196669427678, 0.08105747995432466, 0.08162288391031325, 0.08186291693709791, 0.07674414396751672, 0.08116415096446872, 0.08146133203990757, 0.08388635900337249, 0.07723342208191752, 0.08610909001436085, 0.08317331201396883, 0.08135405101347715, 0.08135642693378031, 0.08233752602245659, 0.08234677102882415, 0.08228368696291, 0.08307302300818264, 0.0819864699151367, 0.08197423606179655, 0.07676309288945049, 0.08271457394585013, 0.07675431401003152, 0.07646099803969264, 0.08056469797156751, 0.08051179302856326, 0.0762351299636066, 0.0875893629854545, 0.0808275219751522, 0.08249794004950672, 0.0842265640385449, 0.08416549989487976, 0.08955766703002155, 0.08246412698645145, 0.08391455293167382, 0.08739720704033971, 0.08428194094449282, 0.08436655497644097, 0.08987386198714375, 0.08378155599348247, 0.07988261000718921, 0.07942835602443665, 0.08010760403703898, 0.08108371892012656, 0.07982606999576092, 0.0776638489915058, 0.08061006700154394, 0.07770334009546787, 0.07847123790998012, 0.0850272880634293, 0.0782962740631774, 0.07699633506126702, 0.07861776393838227, 0.07982065505348146, 0.07426340004894882, 0.07845385302789509, 0.07923388993367553, 0.07953391096089035, 0.07874732604250312, 0.08072333491872996, 0.07974270894192159, 0.08330576994922012, 0.078463398036547, 0.07501607097219676, 0.08599129295907915, 0.08533507399260998, 0.07034170592669398, 0.07106003491207957, 0.0700000359211117, 0.0707795680500567, 0.07014743704348803, 0.06975805410183966, 0.0772673450410366, 0.06982789596077055, 0.06983063998632133, 0.06961014098487794, 0.06986840406898409, 0.06970381492283195, 0.0694882160751149, 0.06948410591576248, 0.06967917096335441, 0.08257008099462837, 0.07060268695931882, 0.07153116690460593, 0.07764777902048081, 0.08356979407835752, 0.06977347400970757, 0.07484221097547561, 0.06956277997232974, 0.06882717693224549, 0.06882724305614829, 0.06891609099693596, 0.06942140904720873, 0.06941871100571007, 0.06937626306898892, 0.06927319301757962, 0.06968249497003853, 0.06938759400509298, 0.0696751339128241, 0.06944154296070337, 0.06965698103886098, 0.06988401303533465, 0.07149852497968823, 0.06948222394566983, 0.06959622295107692, 0.06944153399672359, 0.06923473800998181, 0.06994359800592065, 0.06924523296765983, 0.06922973808832467, 0.06945257494226098, 0.06909439503215253, 0.06866361200809479, 0.06910937500651926, 0.06907922704704106, 0.06909445801284164, 0.06914576597046107, 0.06933884194586426, 0.06945728603750467, 0.06935463997069746, 0.069415861973539, 0.0699626689311117, 0.06929485907312483, 0.06912831601221114, 0.0692521040327847, 0.06871087499894202, 0.06881014897953719, 0.06857581506483257, 0.0686741400277242, 0.068556537036784, 0.0686974689597264, 0.06927496299613267, 0.06945650989655405, 0.06906090793199837, 0.06890431197825819, 0.06858094106428325, 0.06818440300412476, 0.06890107202343643, 0.06879515293985605, 0.06835908896755427, 0.06870970397721976, 0.06909038405865431, 0.06867744296323508, 0.06901944905985147, 0.06900342798326164, 0.0683065599296242, 0.06865654198918492, 0.0699645469430834, 0.0692221790086478, 0.08017910702619702, 0.06829869106877595, 0.07133277493994683, 0.07844800699967891, 0.07259149302262813, 0.06861689197830856, 0.06858057202771306, 0.06956048298161477, 0.06957545399200171, 0.06909481808543205, 0.06864664203021675, 0.06793235393706709]
[0.0015073389798516826, 0.0014261315500705826, 0.0015277703487485343, 0.001440106266254217, 0.001401279223797729, 0.0017046650600790673, 0.001388396796941453, 0.0013847746316115468, 0.0013951412663433928, 0.001394722102285952, 0.0014042513259705535, 0.0013924386347548999, 0.0014005156105612309, 0.0013943209992341545, 0.001395420879315661, 0.0014192683251612649, 0.001492839470524721, 0.0014660591235840504, 0.001475604144115077, 0.0015761836536456735, 0.003227743408072512, 0.0014462347156653295, 0.0014524946740011172, 0.0014569809783858303, 0.0014677467751221694, 0.001426144367635098, 0.0013829768986954372, 0.0013952617538256608, 0.001394340162621621, 0.0014629096139640529, 0.0014061141017900438, 0.001376039940122591, 0.0013693581615593664, 0.0013767283680677718, 0.0014740161440924418, 0.0014888824876017716, 0.0015047298986654804, 0.001495333307669783, 0.0015136686733410675, 0.0013528660614499633, 0.0013434993464271634, 0.0013445327557357295, 0.00134077057128354, 0.0013406963672070783, 0.001354897917457381, 0.0013315811235343619, 0.0013407302226833239, 0.0013411241816356778, 0.0013284520816285999, 0.0013480518388618923, 0.001334680163548613, 0.001334993266120401, 0.0013460472858111774, 0.0013416153687641633, 0.0013392359607529883, 0.0013456790403900097, 0.0013408761219672707, 0.0013377114308389779, 0.0013487702045513659, 0.0013637275111918546, 0.00133523671431657, 0.0014389410201574163, 0.00133453067202045, 0.0013471698359947425, 0.0013428104076800601, 0.0013496410600574954, 0.0013325634882880412, 0.0013502951431069144, 0.0013498214301558174, 0.0013412010395063125, 0.001346062980497218, 0.0013444937969919067, 0.001351565305542733, 0.0013463192652644856, 0.0013764057764593437, 0.0013567876541150771, 0.001353087549915119, 0.0013471965735055963, 0.0013531487154755362, 0.0013555467974546613, 0.0013390557949754353, 0.0013478086329996586, 0.001345335714975182, 0.0013393415898388745, 0.0013236205915597324, 0.001474240528685706, 0.0014907297136604178, 0.0014815395294061424, 0.0014216643045371284, 0.0015419560617634229, 0.0014731106728467407, 0.0014802171012424693, 0.0014759062442510408, 0.001470522306460355, 0.0015287986334071172, 0.001583819225316449, 0.0014718127964368584, 0.001637146223754603, 0.0014813031445314385, 0.0014570766337672059, 0.0014771848574888949, 0.0014700021034068599, 0.0015034347140629376, 0.0014857399795318441, 0.001406725918950171, 0.0013856413888231832, 0.0013952578170871247, 0.0014042808978381206, 0.0014355711838496582, 0.001425249917831804, 0.001417476795518733, 0.0014366012432479433, 0.0014381150819589288, 0.001421943183380122, 0.0014362730827106505, 0.0016757746117797736, 0.001690601205396257, 0.0017123973657548123, 0.0018833976533568027, 0.0016250173057600552, 0.001678595388289161, 0.0017727444504330658, 0.001688515204385075, 0.0016705847549613338, 0.0016512898775767914, 0.002023973939370136, 0.0015813061843949314, 0.0016548356316907673, 0.0015562218162516247, 0.0016516644070495147, 0.0016464683065685083, 0.0016303117748121827, 0.0016337647554179539, 0.0016330930794000017, 0.0016203376117675584, 0.001624352671681162, 0.0016683012232830633, 0.0016885297753068866, 0.0017166741011778312, 0.0017560823245582227, 0.001658316487826559, 0.0016575983074512712, 0.001666618102439204, 0.0016785630818494425, 0.0017000403069910041, 0.0015623913673988106, 0.0016636666723013837, 0.001654234284782136, 0.001665773141026801, 0.001670671774226488, 0.001566207019745239, 0.001656411244172831, 0.0016624761640797465, 0.001711966510272908, 0.0015761922873860719, 0.0017573283676400172, 0.001697414530897323, 0.0016602867553770846, 0.001660335243546537, 0.0016803576739276855, 0.0016805463475270234, 0.0016792589176104081, 0.0016953678164935233, 0.0016731932635742183, 0.0016729435930978886, 0.0015665937324377652, 0.0016880525295071456, 0.0015664145716332964, 0.0015604285314222987, 0.0016441775096238268, 0.0016430978169094544, 0.0015558189788491142, 0.0017875380201113162, 0.0016495412647990243, 0.0016836314295817698, 0.0017189094701743856, 0.0017176632631608114, 0.001827707490408603, 0.0016829413670704377, 0.001712541896564772, 0.0017836164702110145, 0.0017200396111120982, 0.001721766428090632, 0.0018341604487172195, 0.001709827673336377, 0.0016302573470854942, 0.0016209868576415644, 0.0016348490619803875, 0.0016547697738801337, 0.0016291034693012433, 0.0015849765100307306, 0.0016451034081947742, 0.0015857824509279157, 0.0016014538348975535, 0.0017352507768046794, 0.0015978831441464778, 0.0015713537767605514, 0.0016044441620078013, 0.0016289929602751319, 0.0015155795928356902, 0.001601099041385614, 0.0016170181619117455, 0.0016231410400181705, 0.0016070882865816963, 0.0016474149983414278, 0.0016274022233045222, 0.0017001177540657167, 0.0016012938374805512, 0.0015309402239223828, 0.001754924346103656, 0.0017415321222981628, 0.0014654522068061244, 0.0014804173940016578, 0.0014583340816898271, 0.0014745743343761812, 0.0014614049384060006, 0.0014532927937883262, 0.001609736355021596, 0.001454747832516053, 0.0014548049997150276, 0.0014502112705182906, 0.0014555917514371686, 0.0014521628108923323, 0.0014476711682315606, 0.0014475855399117183, 0.0014516493950698834, 0.0017202100207214244, 0.0014708893116524753, 0.0014902326438459568, 0.0016176620629266836, 0.0017410373766324483, 0.0014536140418689076, 0.0015592127286557418, 0.0014492245827568695, 0.0014338995194217812, 0.0014339008970030893, 0.0014357518957694992, 0.0014462793551501818, 0.001446223145952293, 0.0014453388139372692, 0.0014431915211995754, 0.0014517186452091362, 0.0014455748751061037, 0.001451565289850502, 0.0014466988116813202, 0.0014511871049762703, 0.0014559169382361385, 0.0014895526037435047, 0.0014475463322014548, 0.0014499213114807692, 0.0014466986249317415, 0.0014423903752079543, 0.0014571582917900134, 0.0014426090201595798, 0.0014422862101734306, 0.001446928644630437, 0.0014394665631698444, 0.001430491916835308, 0.0014397786459691513, 0.0014391505634800221, 0.0014394678752675343, 0.0014405367910512723, 0.0014445592072055053, 0.001447026792448014, 0.0014448883327228639, 0.0014461637911153957, 0.0014575556027314935, 0.0014436428973567672, 0.001440173250254399, 0.0014427521673496813, 0.0014314765624779586, 0.0014335447704070248, 0.0014286628138506785, 0.0014307112505775876, 0.0014282611882663332, 0.0014311972699942999, 0.001443228395752764, 0.001447010622844876, 0.001438768915249966, 0.0014355064995470457, 0.0014287696055059012, 0.001420508395919266, 0.001435439000488259, 0.0014332323529136677, 0.0014241476868240472, 0.0014314521661920783, 0.0014393830012219648, 0.0014307800617340642, 0.0014379051887469056, 0.001437571416317951, 0.0014230533318671708, 0.0014303446247746858, 0.0014575947279809043, 0.0014421287293468292, 0.001670398063045771, 0.0014228893972661656, 0.001486099477915559, 0.0016343334791599773, 0.0015123227713047527, 0.0014295185828814283, 0.001428761917244022, 0.001449176728783641, 0.001449488624833369, 0.0014394753767798345, 0.0014301383756295156, 0.0014152573736888978]
[663.420778847235, 701.1975858401756, 654.5486373780884, 694.3932009969279, 713.6336448989895, 586.6255039882245, 720.2551908812627, 722.1391677548563, 716.7732932314148, 716.9887093357152, 712.123237134098, 718.1645029376984, 714.0227445228321, 717.1949648246426, 716.6296669506748, 704.5884011301208, 669.8643891352284, 682.1007310778276, 677.6885277722653, 634.4438338051691, 309.81397018704246, 691.4506954979001, 688.4706828186489, 686.3507587503898, 681.3164347894848, 701.1912837816333, 723.0778771093721, 716.7113964516732, 717.1851079149958, 683.5692311094296, 711.1798386254408, 726.723091998994, 730.2691348925417, 726.3596967958848, 678.4186211309811, 671.6446786950643, 664.5710973689585, 668.7472250306027, 660.646558663816, 739.1714734333926, 744.3248875850601, 743.752798683435, 745.8397591786975, 745.8810394803913, 738.0629840192041, 750.9869149734869, 745.8622048502868, 745.6431057565214, 752.7557928729066, 741.8112354227127, 749.2431724925212, 749.0674487865256, 742.915951423923, 745.3701137317439, 746.6944058444695, 743.1192505682308, 745.7810483886065, 747.5453800770963, 741.4161408856334, 733.2843194796529, 748.9308744119141, 694.9555165858033, 749.3271012542725, 742.2969051720237, 744.7067689381964, 740.9377423338021, 750.4332880114483, 740.5788320463665, 740.8387344128656, 745.6003764865061, 742.907289249284, 743.7743500470904, 739.8828572315573, 742.765869731166, 726.5299355052061, 737.035008364827, 739.0504776004562, 742.2821729703917, 739.0170707501065, 737.7096842969354, 746.7948712460819, 741.9450918446917, 743.308892248094, 746.6355167245288, 755.5035078606753, 678.3153634308953, 670.8124154475638, 674.9735529505839, 703.4009342490907, 648.5269099408532, 678.8356220836627, 675.5765753284547, 677.5498131369836, 680.0304868595067, 654.1083816718073, 631.3851884202244, 679.4342340418023, 610.8189882432231, 675.0812645552826, 686.3057006236832, 676.9633434368709, 680.2711354510388, 665.1436145820811, 673.0652831426798, 710.8705303064953, 721.6874496288639, 716.7134186624353, 712.1082409790606, 696.5868437943834, 701.6313332059505, 705.4789208270919, 696.087383120418, 695.3546434113254, 703.2629796240418, 696.246425584137, 596.7389605801103, 591.5055524674206, 583.9766049623694, 530.9553180220267, 615.3780617937965, 595.736177387696, 564.0970980085194, 592.2363016945299, 598.5927963428263, 605.5871919153674, 494.0775078908386, 632.3885973940199, 604.2896229991657, 642.5819183081743, 605.4498696780485, 607.3606130227632, 613.3796096241796, 612.0832247627826, 612.3349689090593, 617.1553340103868, 615.6298551625655, 599.4121361561384, 592.2311910776061, 582.5217490692542, 569.4493851542921, 603.0212009232514, 603.2824692838902, 600.017483631334, 595.7476432152904, 588.221347392613, 640.0444990072347, 601.0819454696895, 604.5092942392382, 600.3218417746782, 598.5616178037093, 638.4851985675948, 603.7148102670446, 601.5123835195219, 584.1235760158579, 634.4403585798415, 569.0456140208661, 589.1312827817958, 602.3055937544234, 602.2880041165441, 595.1113953391778, 595.0445826570218, 595.5007828232967, 589.842505131582, 597.659590060645, 597.7487849116543, 638.3275888917973, 592.3986265356127, 638.4005984809645, 640.8495998778749, 608.2068354217977, 608.6064929968236, 642.7482975813355, 559.4286603972354, 606.2291506977471, 593.9542244399711, 581.764204195436, 582.1862884578546, 547.1335020772058, 594.1977656302663, 583.9273199715134, 560.6586487069683, 581.3819597755927, 580.7988724167185, 545.2085725103183, 584.8542608090473, 613.4000878988572, 616.9081478272675, 611.6772632139154, 604.3136729861714, 613.8345530802418, 630.9241769019096, 607.8645239069394, 630.6035228318066, 624.4326112990768, 576.2855797946492, 625.8279922804732, 636.393926555206, 623.2688077773926, 613.8761949168296, 659.8135820296795, 624.5709816518194, 618.4222438279445, 616.0894064934772, 622.2433505050409, 607.0115914974506, 614.4762405261125, 588.1945515883048, 624.4950030991084, 653.1933673007334, 569.8251336134431, 574.2070371233685, 682.3832229776002, 675.485173338135, 685.7139338341884, 678.1618102847627, 684.2730400861591, 688.0925882755399, 621.2197400403398, 687.4043580944569, 687.3773462394503, 689.5547016695094, 687.0058167151998, 688.6280191857516, 690.7646031395205, 690.8054636004346, 688.8715714663728, 581.3243661844374, 679.8608107883705, 671.0361661513627, 618.1760844355799, 574.3702079126075, 687.9405201082831, 641.349305083046, 690.0241769965655, 697.3989365748928, 697.3982665678222, 696.4991674024881, 691.4293538374956, 691.4562270689777, 691.8792952608012, 692.9087271582664, 688.8387107929849, 691.7663119501859, 688.911485409651, 691.2288804867566, 689.0910183606902, 686.8523703086472, 671.3425208930696, 690.8241745044401, 689.6926006134253, 691.2289697152247, 693.293589022893, 686.267240583432, 693.1885119430219, 693.3436601877744, 691.1190843522294, 694.7017913343561, 699.0602241306684, 694.5512095207349, 694.8543296136379, 694.7011581027077, 694.1856717663017, 692.2526920405681, 691.0722076598496, 692.095006480895, 691.484606476505, 686.080172945702, 692.6920790667459, 694.3609040255091, 693.1197350664794, 698.5793733632279, 697.5715168742662, 699.9552240774697, 698.9530554095338, 700.1520507700908, 698.7156983634986, 692.891023307795, 691.079930038083, 695.0386468603014, 696.618232181837, 699.9029067712554, 703.973311859844, 696.6509894602655, 697.7235742460497, 702.1743666417578, 698.5912792742358, 694.7421215555898, 698.9194403422346, 695.4561453884677, 695.6176149921638, 702.7143520249605, 699.1322109925253, 686.0617569502493, 693.4193734930454, 598.6596980222903, 702.7953134806725, 672.9024637049368, 611.8702289045592, 661.2345056057395, 699.5362018899654, 699.9066729948453, 690.0469626222502, 689.8984806555197, 694.6975378189803, 699.2330371946153, 706.5852604558273]
Elapsed: 0.07362770642917847~0.00811068813582902
Time per graph: 0.0015118603720382926~0.00016217685918348105
Speed: 667.4043208580363~58.2704206010883
Total Time: 0.0689
best val loss: 0.4461866617202759 test_score: 0.8958

Testing...
Test loss: 0.4016 score: 0.8750 time: 0.06s
test Score 0.8750
Epoch Time List: [0.35025557794142514, 0.34973484894726425, 0.3509860330959782, 0.35968710109591484, 0.34449524781666696, 0.3676899520214647, 0.35729762609116733, 0.34548684291075915, 0.34303716907743365, 0.34589120000600815, 0.3405627228785306, 0.33856255398131907, 0.34320018510334194, 0.3405399650800973, 0.34791362890973687, 0.34923763188999146, 0.46951979387085885, 0.37474683893378824, 0.36433169711381197, 0.36527635413222015, 0.44757716706953943, 0.35487512208055705, 0.3583817781182006, 0.35686489602085203, 0.3596197438891977, 0.35168124607298523, 0.4126796911004931, 0.3389126032125205, 0.33973319618962705, 0.3391916450345889, 0.33862650906667113, 0.38200218009296805, 0.3376869950443506, 0.33880763698834926, 0.3453123898943886, 0.36738421383779496, 0.3704198020277545, 0.3685165380593389, 0.3702639879193157, 0.33290875295642763, 0.3312104409560561, 0.3336681700311601, 0.33160880906507373, 0.33210192702244967, 0.33295236295089126, 0.34240920504089445, 0.33209972886834294, 0.3321028130594641, 0.3247404210269451, 0.3298833610024303, 0.3320108629995957, 0.3299944648751989, 0.3319164681015536, 0.33229390799533576, 0.3303412910318002, 0.33308176207356155, 0.33237248903606087, 0.33186021400615573, 0.3339804249117151, 0.33207037893589586, 0.33402457600459456, 0.33237653295509517, 0.33120850985869765, 0.3295228348579258, 0.33061267191078514, 0.33291260595433414, 0.331081647076644, 0.33208924299106, 0.3310389081016183, 0.33216009207535535, 0.33293708390556276, 0.33392276300583035, 0.3306152910226956, 0.3316058620112017, 0.3302575189154595, 0.3380327691556886, 0.3325191739713773, 0.32823134190402925, 0.3252913129981607, 0.3320072559872642, 0.32583682297263294, 0.3335519441170618, 0.3327176780439913, 0.337231794022955, 0.3267574032070115, 0.3439234639517963, 0.3686596070183441, 0.36387394391931593, 0.3562169590732083, 0.3582324089948088, 0.36600983201060444, 0.37091391102876514, 0.3631249559111893, 0.365326048922725, 0.3691650229739025, 0.3682702179066837, 0.370808805921115, 0.3948854600312188, 0.36801513913087547, 0.3590038319816813, 0.3626213880488649, 0.3626661271555349, 0.36670656490605325, 0.3613232650095597, 0.3508384379092604, 0.34125748788937926, 0.33875163993798196, 0.3503663861192763, 0.35709787893574685, 0.3576204718556255, 0.3471138260792941, 0.3523693950846791, 0.3515868061222136, 0.3489933309610933, 0.3526856548851356, 0.35632392298430204, 0.3636693370062858, 0.3563139820471406, 0.37439253088086843, 0.3595090879825875, 0.36081476998515427, 0.3643719400279224, 0.3543579428223893, 0.348201586981304, 0.35683933598920703, 0.3896325237583369, 0.3517434620298445, 0.34390557499136776, 0.3585659860400483, 0.35304692503996193, 0.3472635210491717, 0.341657868004404, 0.34492898115422577, 0.35277283005416393, 0.34552374109625816, 0.34351079096086323, 0.35086370608769357, 0.35836184909567237, 0.3555593640776351, 0.37681058375164866, 0.3506662539439276, 0.34888345294166356, 0.3517694000620395, 0.36150756792631, 0.3576047510141507, 0.3466960929799825, 0.35756059689447284, 0.3515194790670648, 0.3631125210085884, 0.34951904497575015, 0.3475298259872943, 0.35722416499629617, 0.35558921203482896, 0.3618680468061939, 0.35439931706059724, 0.35670437605585903, 0.3488640961004421, 0.3506678829435259, 0.35195486003067344, 0.35116865497548133, 0.3535140729509294, 0.35832802997902036, 0.3529176899464801, 0.3555203400319442, 0.3579562020022422, 0.35260871693026274, 0.3606704161502421, 0.3492697310866788, 0.34583273995667696, 0.37364827701821923, 0.3561411671107635, 0.33851583604700863, 0.3532671829452738, 0.3635844470700249, 0.3527621611719951, 0.3616379218874499, 0.41016537393443286, 0.3940253622131422, 0.3628124389797449, 0.3638340618927032, 0.3543827789835632, 0.46072867698967457, 0.36796426691580564, 0.3792441750410944, 0.367052294081077, 0.4194147839443758, 0.3518460700288415, 0.3456734480569139, 0.3450055589200929, 0.35238625411875546, 0.38978956383652985, 0.3507106531178579, 0.34314719296526164, 0.33668963296804577, 0.3606446160702035, 0.3393983888672665, 0.3342654468724504, 0.35228717303834856, 0.3503878200426698, 0.33706693397834897, 0.3533831579843536, 0.34239452704787254, 0.34919881192035973, 0.34232583292759955, 0.3512397148879245, 0.3523500320734456, 0.34815561899449676, 0.3399247588822618, 0.3463847889797762, 0.36853678594343364, 0.36201356595847756, 0.3724797269096598, 0.33034039102494717, 0.33173019101377577, 0.3370135931763798, 0.33062064298428595, 0.3295739410677925, 0.3420502089429647, 0.33082871802616864, 0.3287555129500106, 0.3290264140814543, 0.3313256959663704, 0.327787796035409, 0.3286796680185944, 0.3272470311494544, 0.32399552210699767, 0.33610244293231517, 0.3317138741258532, 0.34993158909492195, 0.35105555003974587, 0.36401487595867366, 0.32522399397566915, 0.3383639908861369, 0.3249075381318107, 0.32244688901118934, 0.32299357804004103, 0.32712205999996513, 0.3288861640030518, 0.32639416796155274, 0.3262485498562455, 0.3280857299687341, 0.32917514094151556, 0.3276326529448852, 0.3277668800437823, 0.32992731302510947, 0.326001078938134, 0.3265554449753836, 0.33120744209736586, 0.3308914810186252, 0.32502553099766374, 0.32508107693865895, 0.32432273298036307, 0.32514299103058875, 0.32460058899596334, 0.3256667199311778, 0.32580932206474245, 0.3243442348903045, 0.32233340409584343, 0.32213103608228266, 0.32148633408360183, 0.32347760791890323, 0.3223949382081628, 0.33073712105397135, 0.33228265307843685, 0.32651563885156065, 0.3280917720403522, 0.3296536708949134, 0.32336897403001785, 0.326379295787774, 0.3256684369407594, 0.3269156039459631, 0.3262272879946977, 0.32536464009899646, 0.32550598215311766, 0.3243164799641818, 0.32554346800316125, 0.3295274939155206, 0.32898248301353306, 0.32844754692632705, 0.3264855060260743, 0.32662727194838226, 0.3237440949305892, 0.32450573693495244, 0.32321297691669315, 0.3234572830842808, 0.3234558269614354, 0.321848883992061, 0.32504498900379986, 0.3406499249394983, 0.3247920071007684, 0.3224068710114807, 0.3226665600668639, 0.32311203295830637, 0.3307447439292446, 0.33544388914015144, 0.3296207470120862, 0.34083995409309864, 0.35013991699088365, 0.3391427588649094, 0.32537626300472766, 0.32680281810462475, 0.32335573504678905, 0.32762587000615895, 0.32741914107464254, 0.3222990079084411, 0.3224899871274829]
Total Epoch List: [115, 96, 95]
Total Time List: [0.07093218003865331, 0.08607852808199823, 0.06888744502793998]
========================training times:2========================
========================k_idx:0========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: GPSTransformer
GPS(
  (x_lin): Linear(in_features=14887, out_features=64, bias=True)
  (pe_lin): Linear(in_features=20, out_features=8, bias=True)
  (pe_norm): BatchNorm1d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (convs): ModuleList(
    (0-1): 2 x GPSConv(64, conv=GINEConv(nn=Sequential(
      (0): Linear(in_features=64, out_features=64, bias=True)
      (1): ReLU()
      (2): Linear(in_features=64, out_features=64, bias=True)
    )), heads=1, attn_type=multihead)
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=32, bias=True)
    (1): ReLU()
    (2): Linear(in_features=32, out_features=16, bias=True)
    (3): ReLU()
    (4): Linear(in_features=16, out_features=2, bias=True)
  )
)
Total number of parameters: 1039906
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7d29a7d894e0>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.6084;  Loss pred: 0.6084; Loss self: 0.0000; time: 0.20s
Val loss: 0.6264 score: 0.5918 time: 0.07s
Test loss: 0.6606 score: 0.5510 time: 0.06s
Epoch 2/1000, LR 0.000000
Train loss: 0.6210;  Loss pred: 0.6210; Loss self: 0.0000; time: 0.20s
Val loss: 0.6153 score: 0.6122 time: 0.07s
Test loss: 0.6501 score: 0.5510 time: 0.06s
Epoch 3/1000, LR 0.000030
Train loss: 0.5925;  Loss pred: 0.5925; Loss self: 0.0000; time: 0.19s
Val loss: 0.5963 score: 0.6122 time: 0.07s
Test loss: 0.6352 score: 0.5510 time: 0.06s
Epoch 4/1000, LR 0.000060
Train loss: 0.5263;  Loss pred: 0.5263; Loss self: 0.0000; time: 0.20s
Val loss: 0.5824 score: 0.5918 time: 0.07s
Test loss: 0.6247 score: 0.5510 time: 0.06s
Epoch 5/1000, LR 0.000090
Train loss: 0.4546;  Loss pred: 0.4546; Loss self: 0.0000; time: 0.19s
Val loss: 0.5741 score: 0.5918 time: 0.07s
Test loss: 0.6158 score: 0.5510 time: 0.06s
Epoch 6/1000, LR 0.000120
Train loss: 0.4013;  Loss pred: 0.4013; Loss self: 0.0000; time: 0.20s
Val loss: 0.5676 score: 0.5918 time: 0.07s
Test loss: 0.6104 score: 0.5510 time: 0.06s
Epoch 7/1000, LR 0.000150
Train loss: 0.3495;  Loss pred: 0.3495; Loss self: 0.0000; time: 0.20s
Val loss: 0.5608 score: 0.5918 time: 0.07s
Test loss: 0.6037 score: 0.5510 time: 0.06s
Epoch 8/1000, LR 0.000180
Train loss: 0.3104;  Loss pred: 0.3104; Loss self: 0.0000; time: 0.20s
Val loss: 0.5543 score: 0.5918 time: 0.07s
Test loss: 0.5976 score: 0.5510 time: 0.06s
Epoch 9/1000, LR 0.000210
Train loss: 0.2775;  Loss pred: 0.2775; Loss self: 0.0000; time: 0.20s
Val loss: 0.5472 score: 0.5918 time: 0.07s
Test loss: 0.5919 score: 0.5714 time: 0.06s
Epoch 10/1000, LR 0.000240
Train loss: 0.2497;  Loss pred: 0.2497; Loss self: 0.0000; time: 0.20s
Val loss: 0.5405 score: 0.6531 time: 0.07s
Test loss: 0.5872 score: 0.6122 time: 0.06s
Epoch 11/1000, LR 0.000270
Train loss: 0.2166;  Loss pred: 0.2166; Loss self: 0.0000; time: 0.19s
Val loss: 0.5351 score: 0.6939 time: 0.07s
Test loss: 0.5866 score: 0.6327 time: 0.07s
Epoch 12/1000, LR 0.000270
Train loss: 0.1872;  Loss pred: 0.1872; Loss self: 0.0000; time: 0.21s
Val loss: 0.5356 score: 0.7347 time: 0.07s
Test loss: 0.5924 score: 0.6531 time: 0.06s
     INFO: Early stopping counter 1 of 20
Epoch 13/1000, LR 0.000270
Train loss: 0.1544;  Loss pred: 0.1544; Loss self: 0.0000; time: 0.20s
Val loss: 0.5484 score: 0.7755 time: 0.07s
Test loss: 0.6067 score: 0.6327 time: 0.06s
     INFO: Early stopping counter 2 of 20
Epoch 14/1000, LR 0.000270
Train loss: 0.1355;  Loss pred: 0.1355; Loss self: 0.0000; time: 0.19s
Val loss: 0.5794 score: 0.8163 time: 0.07s
Test loss: 0.6309 score: 0.7347 time: 0.06s
     INFO: Early stopping counter 3 of 20
Epoch 15/1000, LR 0.000270
Train loss: 0.1194;  Loss pred: 0.1194; Loss self: 0.0000; time: 0.20s
Val loss: 0.6263 score: 0.6735 time: 0.07s
Test loss: 0.6610 score: 0.5510 time: 0.06s
     INFO: Early stopping counter 4 of 20
Epoch 16/1000, LR 0.000270
Train loss: 0.1030;  Loss pred: 0.1030; Loss self: 0.0000; time: 0.20s
Val loss: 0.6798 score: 0.6531 time: 0.07s
Test loss: 0.6959 score: 0.5714 time: 0.06s
     INFO: Early stopping counter 5 of 20
Epoch 17/1000, LR 0.000270
Train loss: 0.0906;  Loss pred: 0.0906; Loss self: 0.0000; time: 0.19s
Val loss: 0.7288 score: 0.6122 time: 0.07s
Test loss: 0.7289 score: 0.5714 time: 0.06s
     INFO: Early stopping counter 6 of 20
Epoch 18/1000, LR 0.000270
Train loss: 0.0864;  Loss pred: 0.0864; Loss self: 0.0000; time: 0.20s
Val loss: 0.7860 score: 0.5918 time: 0.07s
Test loss: 0.7597 score: 0.5714 time: 0.06s
     INFO: Early stopping counter 7 of 20
Epoch 19/1000, LR 0.000270
Train loss: 0.0748;  Loss pred: 0.0748; Loss self: 0.0000; time: 0.19s
Val loss: 0.8445 score: 0.5510 time: 0.07s
Test loss: 0.7913 score: 0.5510 time: 0.06s
     INFO: Early stopping counter 8 of 20
Epoch 20/1000, LR 0.000270
Train loss: 0.0653;  Loss pred: 0.0653; Loss self: 0.0000; time: 0.19s
Val loss: 0.8893 score: 0.5510 time: 0.07s
Test loss: 0.8108 score: 0.5510 time: 0.06s
     INFO: Early stopping counter 9 of 20
Epoch 21/1000, LR 0.000270
Train loss: 0.0597;  Loss pred: 0.0597; Loss self: 0.0000; time: 0.19s
Val loss: 0.9199 score: 0.5510 time: 0.07s
Test loss: 0.8182 score: 0.5510 time: 0.06s
     INFO: Early stopping counter 10 of 20
Epoch 22/1000, LR 0.000270
Train loss: 0.0429;  Loss pred: 0.0429; Loss self: 0.0000; time: 0.19s
Val loss: 0.9275 score: 0.5510 time: 0.07s
Test loss: 0.8155 score: 0.5510 time: 0.06s
     INFO: Early stopping counter 11 of 20
Epoch 23/1000, LR 0.000270
Train loss: 0.0465;  Loss pred: 0.0465; Loss self: 0.0000; time: 0.20s
Val loss: 0.9350 score: 0.5510 time: 0.07s
Test loss: 0.8137 score: 0.5510 time: 0.06s
     INFO: Early stopping counter 12 of 20
Epoch 24/1000, LR 0.000270
Train loss: 0.0416;  Loss pred: 0.0416; Loss self: 0.0000; time: 0.19s
Val loss: 0.9371 score: 0.5510 time: 0.07s
Test loss: 0.8099 score: 0.5510 time: 0.06s
     INFO: Early stopping counter 13 of 20
Epoch 25/1000, LR 0.000270
Train loss: 0.0360;  Loss pred: 0.0360; Loss self: 0.0000; time: 0.19s
Val loss: 0.9277 score: 0.5714 time: 0.07s
Test loss: 0.8025 score: 0.5510 time: 0.06s
     INFO: Early stopping counter 14 of 20
Epoch 26/1000, LR 0.000270
Train loss: 0.0350;  Loss pred: 0.0350; Loss self: 0.0000; time: 0.20s
Val loss: 0.9164 score: 0.5714 time: 0.07s
Test loss: 0.7965 score: 0.5510 time: 0.06s
     INFO: Early stopping counter 15 of 20
Epoch 27/1000, LR 0.000270
Train loss: 0.0265;  Loss pred: 0.0265; Loss self: 0.0000; time: 0.20s
Val loss: 0.9036 score: 0.5714 time: 0.07s
Test loss: 0.7896 score: 0.5510 time: 0.06s
     INFO: Early stopping counter 16 of 20
Epoch 28/1000, LR 0.000270
Train loss: 0.0328;  Loss pred: 0.0328; Loss self: 0.0000; time: 0.20s
Val loss: 0.8934 score: 0.5714 time: 0.07s
Test loss: 0.7843 score: 0.5510 time: 0.06s
     INFO: Early stopping counter 17 of 20
Epoch 29/1000, LR 0.000270
Train loss: 0.0255;  Loss pred: 0.0255; Loss self: 0.0000; time: 0.19s
Val loss: 0.8820 score: 0.5714 time: 0.07s
Test loss: 0.7786 score: 0.5510 time: 0.06s
     INFO: Early stopping counter 18 of 20
Epoch 30/1000, LR 0.000270
Train loss: 0.0180;  Loss pred: 0.0180; Loss self: 0.0000; time: 0.19s
Val loss: 0.8690 score: 0.5714 time: 0.07s
Test loss: 0.7734 score: 0.5510 time: 0.06s
     INFO: Early stopping counter 19 of 20
Epoch 31/1000, LR 0.000270
Train loss: 0.0134;  Loss pred: 0.0134; Loss self: 0.0000; time: 0.20s
Val loss: 0.8564 score: 0.5714 time: 0.07s
Test loss: 0.7677 score: 0.5510 time: 0.06s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 010,   Train_Loss: 0.2166,   Val_Loss: 0.5351,   Val_Precision: 0.6286,   Val_Recall: 0.9167,   Val_accuracy: 0.7458,   Val_Score: 0.6939,   Val_Loss: 0.5351,   Test_Precision: 0.5946,   Test_Recall: 0.8800,   Test_accuracy: 0.7097,   Test_Score: 0.6327,   Test_loss: 0.5866


[0.06425203999970108, 0.06446822395082563, 0.0644293170189485, 0.06476985302288085, 0.06434080202598125, 0.06430459895636886, 0.06523097504395992, 0.06436507194302976, 0.06427436193916947, 0.06485680502373725, 0.07058735901955515, 0.06834620400331914, 0.06551392399705946, 0.06486350495833904, 0.06500770209822804, 0.06453821307513863, 0.06561226700432599, 0.0654647700721398, 0.06502443796489388, 0.0654919610824436, 0.06563644600100815, 0.0650284409057349, 0.06561196490656585, 0.06491087400354445, 0.06469065300188959, 0.06533244298771024, 0.06540759303607047, 0.06554030301049352, 0.06473047402687371, 0.06534642807673663, 0.06550086999777704]
[0.0013112661224428791, 0.001315678039812768, 0.0013148840207948672, 0.0013218337351608338, 0.0013130775923669642, 0.0013123387542116095, 0.001331244388652243, 0.0013135728967965258, 0.0013117216722279483, 0.0013236082657905562, 0.0014405583473378603, 0.001394820489863656, 0.0013370188570828462, 0.0013237449991497763, 0.001326687797923021, 0.0013171063892885434, 0.0013390258572311426, 0.0013360157157579552, 0.001327029346222324, 0.0013365706343355837, 0.0013395193061430234, 0.0013271110388925489, 0.0013390196919707315, 0.00132471171435805, 0.0013202174082018283, 0.0013333151630144945, 0.0013348488374708258, 0.001337557204295786, 0.001321030082181096, 0.0013336005729946252, 0.0013367524489342254]
[762.621700419597, 760.0643696555948, 760.5233497289631, 756.5247983917775, 761.5696176776514, 761.9983764030136, 751.1768752035107, 761.2824552324038, 762.3568483865242, 755.5105432971337, 694.1752840820309, 716.938134524931, 747.9326074591307, 755.4325044795536, 753.7568383198648, 759.2401100872089, 746.8115679765996, 748.4941892563554, 753.5628378126801, 748.1834287771146, 746.5364593208989, 753.5164509176885, 746.8150065278189, 754.8812237118293, 757.4510029844455, 750.0102209436351, 749.1484967651671, 747.6315755231512, 756.985032732141, 749.8497078135481, 748.0816667269145]
Elapsed: 0.0652735123275629~0.0012242812358865746
Time per graph: 0.0013321124964808754~2.4985331344623985e-05
Speed: 750.9375251980282~13.344189418335278
Total Time: 0.0659
best val loss: 0.535085141658783 test_score: 0.6327

Testing...
Test loss: 0.6309 score: 0.7347 time: 0.06s
test Score 0.7347
Epoch Time List: [0.32221966504585, 0.3220785789890215, 0.3261531739262864, 0.32230526697821915, 0.32145366503391415, 0.3223731560865417, 0.323572478024289, 0.324162490083836, 0.32205680094193667, 0.3226844510063529, 0.32808659691363573, 0.34115630912128836, 0.32881911005824804, 0.3231145889731124, 0.3240199339343235, 0.3236857949523255, 0.3277616339037195, 0.3239758238196373, 0.3210588910151273, 0.32347817707341164, 0.3228687949012965, 0.3221863938961178, 0.324742745840922, 0.32348771090619266, 0.32302012119907886, 0.32448436703998595, 0.32573012297507375, 0.33252478716894984, 0.3226655040634796, 0.32310725282877684, 0.3241481491131708]
Total Epoch List: [31]
Total Time List: [0.06592245097272098]
========================k_idx:1========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: GPSTransformer
GPS(
  (x_lin): Linear(in_features=14887, out_features=64, bias=True)
  (pe_lin): Linear(in_features=20, out_features=8, bias=True)
  (pe_norm): BatchNorm1d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (convs): ModuleList(
    (0-1): 2 x GPSConv(64, conv=GINEConv(nn=Sequential(
      (0): Linear(in_features=64, out_features=64, bias=True)
      (1): ReLU()
      (2): Linear(in_features=64, out_features=64, bias=True)
    )), heads=1, attn_type=multihead)
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=32, bias=True)
    (1): ReLU()
    (2): Linear(in_features=32, out_features=16, bias=True)
    (3): ReLU()
    (4): Linear(in_features=16, out_features=2, bias=True)
  )
)
Total number of parameters: 1039906
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7d29a7d8b820>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.5765;  Loss pred: 0.5765; Loss self: 0.0000; time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6558 score: 0.5102 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6100 score: 0.4898 time: 0.15s
Epoch 2/1000, LR 0.000000
Train loss: 0.5749;  Loss pred: 0.5749; Loss self: 0.0000; time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6590 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6276 score: 0.4898 time: 0.07s
     INFO: Early stopping counter 1 of 20
Epoch 3/1000, LR 0.000030
Train loss: 0.5463;  Loss pred: 0.5463; Loss self: 0.0000; time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6703 score: 0.5102 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6468 score: 0.4898 time: 0.07s
     INFO: Early stopping counter 2 of 20
Epoch 4/1000, LR 0.000060
Train loss: 0.4715;  Loss pred: 0.4715; Loss self: 0.0000; time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6776 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6598 score: 0.4898 time: 0.07s
     INFO: Early stopping counter 3 of 20
Epoch 5/1000, LR 0.000090
Train loss: 0.4056;  Loss pred: 0.4056; Loss self: 0.0000; time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6875 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6694 score: 0.4898 time: 0.08s
     INFO: Early stopping counter 4 of 20
Epoch 6/1000, LR 0.000120
Train loss: 0.3344;  Loss pred: 0.3344; Loss self: 0.0000; time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6975 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6810 score: 0.4898 time: 0.07s
     INFO: Early stopping counter 5 of 20
Epoch 7/1000, LR 0.000150
Train loss: 0.2801;  Loss pred: 0.2801; Loss self: 0.0000; time: 0.30s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7065 score: 0.5102 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6900 score: 0.4898 time: 0.07s
     INFO: Early stopping counter 6 of 20
Epoch 8/1000, LR 0.000180
Train loss: 0.2240;  Loss pred: 0.2240; Loss self: 0.0000; time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7122 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6922 score: 0.4898 time: 0.07s
     INFO: Early stopping counter 7 of 20
Epoch 9/1000, LR 0.000210
Train loss: 0.1812;  Loss pred: 0.1812; Loss self: 0.0000; time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7112 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6924 score: 0.4898 time: 0.07s
     INFO: Early stopping counter 8 of 20
Epoch 10/1000, LR 0.000240
Train loss: 0.1467;  Loss pred: 0.1467; Loss self: 0.0000; time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7075 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6947 score: 0.4898 time: 0.07s
     INFO: Early stopping counter 9 of 20
Epoch 11/1000, LR 0.000270
Train loss: 0.1159;  Loss pred: 0.1159; Loss self: 0.0000; time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7049 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6972 score: 0.4898 time: 0.08s
     INFO: Early stopping counter 10 of 20
Epoch 12/1000, LR 0.000270
Train loss: 0.0947;  Loss pred: 0.0947; Loss self: 0.0000; time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7056 score: 0.5102 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7002 score: 0.4898 time: 0.07s
     INFO: Early stopping counter 11 of 20
Epoch 13/1000, LR 0.000270
Train loss: 0.0775;  Loss pred: 0.0775; Loss self: 0.0000; time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7103 score: 0.5102 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7041 score: 0.4898 time: 0.07s
     INFO: Early stopping counter 12 of 20
Epoch 14/1000, LR 0.000270
Train loss: 0.0625;  Loss pred: 0.0625; Loss self: 0.0000; time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7161 score: 0.5102 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7072 score: 0.4898 time: 0.07s
     INFO: Early stopping counter 13 of 20
Epoch 15/1000, LR 0.000270
Train loss: 0.0479;  Loss pred: 0.0479; Loss self: 0.0000; time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7206 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7107 score: 0.4898 time: 0.07s
     INFO: Early stopping counter 14 of 20
Epoch 16/1000, LR 0.000270
Train loss: 0.0391;  Loss pred: 0.0391; Loss self: 0.0000; time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7243 score: 0.5102 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7141 score: 0.4898 time: 0.07s
     INFO: Early stopping counter 15 of 20
Epoch 17/1000, LR 0.000270
Train loss: 0.0317;  Loss pred: 0.0317; Loss self: 0.0000; time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7270 score: 0.5102 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7181 score: 0.4898 time: 0.07s
     INFO: Early stopping counter 16 of 20
Epoch 18/1000, LR 0.000270
Train loss: 0.0262;  Loss pred: 0.0262; Loss self: 0.0000; time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7294 score: 0.5102 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7217 score: 0.4898 time: 0.07s
     INFO: Early stopping counter 17 of 20
Epoch 19/1000, LR 0.000270
Train loss: 0.0206;  Loss pred: 0.0206; Loss self: 0.0000; time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7302 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7248 score: 0.4898 time: 0.08s
     INFO: Early stopping counter 18 of 20
Epoch 20/1000, LR 0.000270
Train loss: 0.0171;  Loss pred: 0.0171; Loss self: 0.0000; time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7289 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7256 score: 0.4898 time: 0.07s
     INFO: Early stopping counter 19 of 20
Epoch 21/1000, LR 0.000270
Train loss: 0.0146;  Loss pred: 0.0146; Loss self: 0.0000; time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7260 score: 0.5102 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7242 score: 0.4898 time: 0.07s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 000,   Train_Loss: 0.5765,   Val_Loss: 0.6558,   Val_Precision: 0.5102,   Val_Recall: 1.0000,   Val_accuracy: 0.6757,   Val_Score: 0.5102,   Val_Loss: 0.6558,   Test_Precision: 0.4898,   Test_Recall: 1.0000,   Test_accuracy: 0.6575,   Test_Score: 0.4898,   Test_loss: 0.6100


[0.06425203999970108, 0.06446822395082563, 0.0644293170189485, 0.06476985302288085, 0.06434080202598125, 0.06430459895636886, 0.06523097504395992, 0.06436507194302976, 0.06427436193916947, 0.06485680502373725, 0.07058735901955515, 0.06834620400331914, 0.06551392399705946, 0.06486350495833904, 0.06500770209822804, 0.06453821307513863, 0.06561226700432599, 0.0654647700721398, 0.06502443796489388, 0.0654919610824436, 0.06563644600100815, 0.0650284409057349, 0.06561196490656585, 0.06491087400354445, 0.06469065300188959, 0.06533244298771024, 0.06540759303607047, 0.06554030301049352, 0.06473047402687371, 0.06534642807673663, 0.06550086999777704, 0.1535849729552865, 0.07732475700322539, 0.07773252495098859, 0.07733356894459575, 0.08748963905964047, 0.07925277797039598, 0.07633526297286153, 0.07637642102781683, 0.07689351798035204, 0.07759404403623194, 0.08165187295526266, 0.07745094399433583, 0.07689007604494691, 0.07708709593862295, 0.07675384904723614, 0.0766594719607383, 0.0765633259434253, 0.07664633309468627, 0.08394460508134216, 0.0771046910667792, 0.07573227898683399]
[0.0013112661224428791, 0.001315678039812768, 0.0013148840207948672, 0.0013218337351608338, 0.0013130775923669642, 0.0013123387542116095, 0.001331244388652243, 0.0013135728967965258, 0.0013117216722279483, 0.0013236082657905562, 0.0014405583473378603, 0.001394820489863656, 0.0013370188570828462, 0.0013237449991497763, 0.001326687797923021, 0.0013171063892885434, 0.0013390258572311426, 0.0013360157157579552, 0.001327029346222324, 0.0013365706343355837, 0.0013395193061430234, 0.0013271110388925489, 0.0013390196919707315, 0.00132471171435805, 0.0013202174082018283, 0.0013333151630144945, 0.0013348488374708258, 0.001337557204295786, 0.001321030082181096, 0.0013336005729946252, 0.0013367524489342254, 0.0031343872031691124, 0.0015780562653719466, 0.001586378060224257, 0.0015782361009101175, 0.0017855028379518464, 0.0016174036320488975, 0.0015578625096502353, 0.0015587024699554456, 0.0015692554689867764, 0.001583551919106774, 0.001666364754189034, 0.0015806315100884863, 0.0015691852254070798, 0.0015732060395637338, 0.001566405082596656, 0.0015644790196069041, 0.0015625168559882715, 0.0015642108794833934, 0.0017131552057416768, 0.0015735651238118202, 0.0015455567140170202]
[762.621700419597, 760.0643696555948, 760.5233497289631, 756.5247983917775, 761.5696176776514, 761.9983764030136, 751.1768752035107, 761.2824552324038, 762.3568483865242, 755.5105432971337, 694.1752840820309, 716.938134524931, 747.9326074591307, 755.4325044795536, 753.7568383198648, 759.2401100872089, 746.8115679765996, 748.4941892563554, 753.5628378126801, 748.1834287771146, 746.5364593208989, 753.5164509176885, 746.8150065278189, 754.8812237118293, 757.4510029844455, 750.0102209436351, 749.1484967651671, 747.6315755231512, 756.985032732141, 749.8497078135481, 748.0816667269145, 319.04162925018363, 633.6909665032132, 630.3667612867994, 633.6187592105722, 560.0663178710497, 618.2748574227066, 641.9051705817838, 641.5592579567699, 637.2448717006362, 631.4917672949206, 600.1087081841621, 632.6585251637925, 637.27339756247, 635.6446484767566, 638.4044658117957, 639.1904189621304, 639.9930958616848, 639.2999902482884, 583.7182741227872, 635.4995957063345, 647.0160499001835]
Elapsed: 0.07192078679173182~0.013172150345303736
Time per graph: 0.0014677711590149352~0.00026881939480211705
Speed: 695.2909771195752~81.23090323348003
Total Time: 0.0762
best val loss: 0.6557992696762085 test_score: 0.4898

Testing...
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6100 score: 0.4898 time: 0.07s
test Score 0.4898
Epoch Time List: [0.32221966504585, 0.3220785789890215, 0.3261531739262864, 0.32230526697821915, 0.32145366503391415, 0.3223731560865417, 0.323572478024289, 0.324162490083836, 0.32205680094193667, 0.3226844510063529, 0.32808659691363573, 0.34115630912128836, 0.32881911005824804, 0.3231145889731124, 0.3240199339343235, 0.3236857949523255, 0.3277616339037195, 0.3239758238196373, 0.3210588910151273, 0.32347817707341164, 0.3228687949012965, 0.3221863938961178, 0.324742745840922, 0.32348771090619266, 0.32302012119907886, 0.32448436703998595, 0.32573012297507375, 0.33252478716894984, 0.3226655040634796, 0.32310725282877684, 0.3241481491131708, 0.4124186368426308, 0.34764637192711234, 0.3458498640684411, 0.3495588480727747, 0.3630177341401577, 0.34837410401087254, 0.4355285500641912, 0.34368113800883293, 0.3461113600060344, 0.34767066198401153, 0.34906239807605743, 0.40352145000360906, 0.3441899560857564, 0.3441683860728517, 0.3459396950202063, 0.3434602312045172, 0.34170704893767834, 0.3410823120502755, 0.3535632330458611, 0.34584158193320036, 0.3304478778736666]
Total Epoch List: [31, 21]
Total Time List: [0.06592245097272098, 0.07622005802113563]
========================k_idx:2========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: GPSTransformer
GPS(
  (x_lin): Linear(in_features=14887, out_features=64, bias=True)
  (pe_lin): Linear(in_features=20, out_features=8, bias=True)
  (pe_norm): BatchNorm1d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (convs): ModuleList(
    (0-1): 2 x GPSConv(64, conv=GINEConv(nn=Sequential(
      (0): Linear(in_features=64, out_features=64, bias=True)
      (1): ReLU()
      (2): Linear(in_features=64, out_features=64, bias=True)
    )), heads=1, attn_type=multihead)
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=32, bias=True)
    (1): ReLU()
    (2): Linear(in_features=32, out_features=16, bias=True)
    (3): ReLU()
    (4): Linear(in_features=16, out_features=2, bias=True)
  )
)
Total number of parameters: 1039906
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7d29a7d89240>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.6782;  Loss pred: 0.6782; Loss self: 0.0000; time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.9227 score: 0.4898 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.9613 score: 0.5000 time: 0.06s
Epoch 2/1000, LR 0.000000
Train loss: 0.6828;  Loss pred: 0.6828; Loss self: 0.0000; time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.9719 score: 0.4898 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 1.0012 score: 0.5000 time: 0.07s
     INFO: Early stopping counter 1 of 20
Epoch 3/1000, LR 0.000030
Train loss: 0.6101;  Loss pred: 0.6101; Loss self: 0.0000; time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.9422 score: 0.4898 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.9642 score: 0.5000 time: 0.06s
     INFO: Early stopping counter 2 of 20
Epoch 4/1000, LR 0.000060
Train loss: 0.5162;  Loss pred: 0.5162; Loss self: 0.0000; time: 0.20s
Val loss: 0.8440 score: 0.5102 time: 0.06s
Test loss: 0.8579 score: 0.5000 time: 0.06s
Epoch 5/1000, LR 0.000090
Train loss: 0.4311;  Loss pred: 0.4311; Loss self: 0.0000; time: 0.20s
Val loss: 0.7347 score: 0.4898 time: 0.06s
Test loss: 0.7561 score: 0.4375 time: 0.06s
Epoch 6/1000, LR 0.000120
Train loss: 0.3542;  Loss pred: 0.3542; Loss self: 0.0000; time: 0.21s
Val loss: 0.6516 score: 0.5102 time: 0.07s
Test loss: 0.6811 score: 0.5417 time: 0.06s
Epoch 7/1000, LR 0.000150
Train loss: 0.2912;  Loss pred: 0.2912; Loss self: 0.0000; time: 0.20s
Val loss: 0.6202 score: 0.5102 time: 0.07s
Test loss: 0.6301 score: 0.4792 time: 0.06s
Epoch 8/1000, LR 0.000180
Train loss: 0.2112;  Loss pred: 0.2112; Loss self: 0.0000; time: 0.20s
Val loss: 0.6127 score: 0.5306 time: 0.06s
Test loss: 0.6146 score: 0.5208 time: 0.06s
Epoch 9/1000, LR 0.000210
Train loss: 0.1725;  Loss pred: 0.1725; Loss self: 0.0000; time: 0.21s
Val loss: 0.6165 score: 0.5306 time: 0.07s
Test loss: 0.6130 score: 0.5208 time: 0.06s
     INFO: Early stopping counter 1 of 20
Epoch 10/1000, LR 0.000240
Train loss: 0.1361;  Loss pred: 0.1361; Loss self: 0.0000; time: 0.20s
Val loss: 0.6211 score: 0.5306 time: 0.06s
Test loss: 0.6154 score: 0.5208 time: 0.06s
     INFO: Early stopping counter 2 of 20
Epoch 11/1000, LR 0.000270
Train loss: 0.1034;  Loss pred: 0.1034; Loss self: 0.0000; time: 0.20s
Val loss: 0.6255 score: 0.5306 time: 0.06s
Test loss: 0.6222 score: 0.5208 time: 0.06s
     INFO: Early stopping counter 3 of 20
Epoch 12/1000, LR 0.000270
Train loss: 0.0777;  Loss pred: 0.0777; Loss self: 0.0000; time: 0.20s
Val loss: 0.6301 score: 0.5306 time: 0.07s
Test loss: 0.6284 score: 0.5208 time: 0.06s
     INFO: Early stopping counter 4 of 20
Epoch 13/1000, LR 0.000270
Train loss: 0.0596;  Loss pred: 0.0596; Loss self: 0.0000; time: 0.20s
Val loss: 0.6345 score: 0.5306 time: 0.06s
Test loss: 0.6337 score: 0.5208 time: 0.06s
     INFO: Early stopping counter 5 of 20
Epoch 14/1000, LR 0.000270
Train loss: 0.0470;  Loss pred: 0.0470; Loss self: 0.0000; time: 0.20s
Val loss: 0.6373 score: 0.5306 time: 0.06s
Test loss: 0.6376 score: 0.5208 time: 0.06s
     INFO: Early stopping counter 6 of 20
Epoch 15/1000, LR 0.000270
Train loss: 0.0362;  Loss pred: 0.0362; Loss self: 0.0000; time: 0.20s
Val loss: 0.6366 score: 0.5306 time: 0.06s
Test loss: 0.6372 score: 0.5208 time: 0.07s
     INFO: Early stopping counter 7 of 20
Epoch 16/1000, LR 0.000270
Train loss: 0.0315;  Loss pred: 0.0315; Loss self: 0.0000; time: 0.20s
Val loss: 0.6327 score: 0.5102 time: 0.06s
Test loss: 0.6334 score: 0.5417 time: 0.06s
     INFO: Early stopping counter 8 of 20
Epoch 17/1000, LR 0.000270
Train loss: 0.0273;  Loss pred: 0.0273; Loss self: 0.0000; time: 0.20s
Val loss: 0.6261 score: 0.5306 time: 0.06s
Test loss: 0.6267 score: 0.5833 time: 0.06s
     INFO: Early stopping counter 9 of 20
Epoch 18/1000, LR 0.000270
Train loss: 0.0213;  Loss pred: 0.0213; Loss self: 0.0000; time: 0.20s
Val loss: 0.6165 score: 0.5510 time: 0.06s
Test loss: 0.6192 score: 0.6042 time: 0.07s
     INFO: Early stopping counter 10 of 20
Epoch 19/1000, LR 0.000270
Train loss: 0.0190;  Loss pred: 0.0190; Loss self: 0.0000; time: 0.20s
Val loss: 0.6101 score: 0.5714 time: 0.06s
Test loss: 0.6162 score: 0.6042 time: 0.06s
Epoch 20/1000, LR 0.000270
Train loss: 0.0178;  Loss pred: 0.0178; Loss self: 0.0000; time: 0.20s
Val loss: 0.6037 score: 0.5714 time: 0.06s
Test loss: 0.6121 score: 0.6042 time: 0.06s
Epoch 21/1000, LR 0.000270
Train loss: 0.0156;  Loss pred: 0.0156; Loss self: 0.0000; time: 0.20s
Val loss: 0.5966 score: 0.6122 time: 0.06s
Test loss: 0.6084 score: 0.6042 time: 0.06s
Epoch 22/1000, LR 0.000270
Train loss: 0.0131;  Loss pred: 0.0131; Loss self: 0.0000; time: 0.20s
Val loss: 0.5895 score: 0.6531 time: 0.07s
Test loss: 0.6060 score: 0.6458 time: 0.07s
Epoch 23/1000, LR 0.000270
Train loss: 0.0109;  Loss pred: 0.0109; Loss self: 0.0000; time: 0.21s
Val loss: 0.5850 score: 0.6531 time: 0.06s
Test loss: 0.6041 score: 0.6667 time: 0.06s
Epoch 24/1000, LR 0.000270
Train loss: 0.0089;  Loss pred: 0.0089; Loss self: 0.0000; time: 0.21s
Val loss: 0.5818 score: 0.6531 time: 0.06s
Test loss: 0.6025 score: 0.6875 time: 0.07s
Epoch 25/1000, LR 0.000270
Train loss: 0.0087;  Loss pred: 0.0087; Loss self: 0.0000; time: 0.22s
Val loss: 0.5787 score: 0.6531 time: 0.07s
Test loss: 0.5997 score: 0.7083 time: 0.07s
Epoch 26/1000, LR 0.000270
Train loss: 0.0069;  Loss pred: 0.0069; Loss self: 0.0000; time: 0.21s
Val loss: 0.5756 score: 0.6735 time: 0.07s
Test loss: 0.5944 score: 0.7083 time: 0.06s
Epoch 27/1000, LR 0.000270
Train loss: 0.0069;  Loss pred: 0.0069; Loss self: 0.0000; time: 0.20s
Val loss: 0.5707 score: 0.6939 time: 0.06s
Test loss: 0.5865 score: 0.7292 time: 0.06s
Epoch 28/1000, LR 0.000270
Train loss: 0.0057;  Loss pred: 0.0057; Loss self: 0.0000; time: 0.20s
Val loss: 0.5646 score: 0.6939 time: 0.06s
Test loss: 0.5780 score: 0.7500 time: 0.06s
Epoch 29/1000, LR 0.000270
Train loss: 0.0054;  Loss pred: 0.0054; Loss self: 0.0000; time: 0.20s
Val loss: 0.5583 score: 0.6531 time: 0.06s
Test loss: 0.5694 score: 0.7500 time: 0.06s
Epoch 30/1000, LR 0.000270
Train loss: 0.0046;  Loss pred: 0.0046; Loss self: 0.0000; time: 0.20s
Val loss: 0.5524 score: 0.6531 time: 0.06s
Test loss: 0.5605 score: 0.7708 time: 0.06s
Epoch 31/1000, LR 0.000270
Train loss: 0.0044;  Loss pred: 0.0044; Loss self: 0.0000; time: 0.20s
Val loss: 0.5463 score: 0.6531 time: 0.06s
Test loss: 0.5488 score: 0.7708 time: 0.07s
Epoch 32/1000, LR 0.000270
Train loss: 0.0041;  Loss pred: 0.0041; Loss self: 0.0000; time: 0.20s
Val loss: 0.5397 score: 0.6531 time: 0.06s
Test loss: 0.5351 score: 0.7708 time: 0.06s
Epoch 33/1000, LR 0.000270
Train loss: 0.0040;  Loss pred: 0.0040; Loss self: 0.0000; time: 0.20s
Val loss: 0.5328 score: 0.6531 time: 0.06s
Test loss: 0.5221 score: 0.7708 time: 0.06s
Epoch 34/1000, LR 0.000270
Train loss: 0.0042;  Loss pred: 0.0042; Loss self: 0.0000; time: 0.20s
Val loss: 0.5260 score: 0.6735 time: 0.06s
Test loss: 0.5091 score: 0.7500 time: 0.07s
Epoch 35/1000, LR 0.000270
Train loss: 0.0035;  Loss pred: 0.0035; Loss self: 0.0000; time: 0.20s
Val loss: 0.5195 score: 0.6735 time: 0.06s
Test loss: 0.4965 score: 0.7500 time: 0.06s
Epoch 36/1000, LR 0.000270
Train loss: 0.0031;  Loss pred: 0.0031; Loss self: 0.0000; time: 0.20s
Val loss: 0.5135 score: 0.6735 time: 0.06s
Test loss: 0.4845 score: 0.7500 time: 0.07s
Epoch 37/1000, LR 0.000270
Train loss: 0.0027;  Loss pred: 0.0027; Loss self: 0.0000; time: 0.20s
Val loss: 0.5064 score: 0.7347 time: 0.07s
Test loss: 0.4734 score: 0.7708 time: 0.07s
Epoch 38/1000, LR 0.000270
Train loss: 0.0023;  Loss pred: 0.0023; Loss self: 0.0000; time: 0.21s
Val loss: 0.4996 score: 0.7551 time: 0.06s
Test loss: 0.4635 score: 0.7708 time: 0.07s
Epoch 39/1000, LR 0.000269
Train loss: 0.0024;  Loss pred: 0.0024; Loss self: 0.0000; time: 0.20s
Val loss: 0.4927 score: 0.7755 time: 0.06s
Test loss: 0.4556 score: 0.7500 time: 0.07s
Epoch 40/1000, LR 0.000269
Train loss: 0.0022;  Loss pred: 0.0022; Loss self: 0.0000; time: 0.20s
Val loss: 0.4862 score: 0.7755 time: 0.07s
Test loss: 0.4483 score: 0.7500 time: 0.06s
Epoch 41/1000, LR 0.000269
Train loss: 0.0018;  Loss pred: 0.0018; Loss self: 0.0000; time: 0.20s
Val loss: 0.4811 score: 0.7551 time: 0.06s
Test loss: 0.4417 score: 0.7917 time: 0.06s
Epoch 42/1000, LR 0.000269
Train loss: 0.0018;  Loss pred: 0.0018; Loss self: 0.0000; time: 0.20s
Val loss: 0.4768 score: 0.7959 time: 0.06s
Test loss: 0.4364 score: 0.7708 time: 0.06s
Epoch 43/1000, LR 0.000269
Train loss: 0.0016;  Loss pred: 0.0016; Loss self: 0.0000; time: 0.20s
Val loss: 0.4729 score: 0.7959 time: 0.06s
Test loss: 0.4317 score: 0.7917 time: 0.06s
Epoch 44/1000, LR 0.000269
Train loss: 0.0017;  Loss pred: 0.0017; Loss self: 0.0000; time: 0.20s
Val loss: 0.4690 score: 0.8367 time: 0.06s
Test loss: 0.4269 score: 0.8125 time: 0.07s
Epoch 45/1000, LR 0.000269
Train loss: 0.0015;  Loss pred: 0.0015; Loss self: 0.0000; time: 0.20s
Val loss: 0.4645 score: 0.8367 time: 0.06s
Test loss: 0.4222 score: 0.8125 time: 0.07s
Epoch 46/1000, LR 0.000269
Train loss: 0.0015;  Loss pred: 0.0015; Loss self: 0.0000; time: 0.20s
Val loss: 0.4593 score: 0.8367 time: 0.06s
Test loss: 0.4168 score: 0.8125 time: 0.07s
Epoch 47/1000, LR 0.000269
Train loss: 0.0014;  Loss pred: 0.0014; Loss self: 0.0000; time: 0.20s
Val loss: 0.4534 score: 0.8163 time: 0.06s
Test loss: 0.4111 score: 0.8542 time: 0.07s
Epoch 48/1000, LR 0.000269
Train loss: 0.0014;  Loss pred: 0.0014; Loss self: 0.0000; time: 0.21s
Val loss: 0.4476 score: 0.8163 time: 0.06s
Test loss: 0.4050 score: 0.8542 time: 0.07s
Epoch 49/1000, LR 0.000269
Train loss: 0.0012;  Loss pred: 0.0012; Loss self: 0.0000; time: 0.20s
Val loss: 0.4422 score: 0.8163 time: 0.06s
Test loss: 0.3991 score: 0.8542 time: 0.06s
Epoch 50/1000, LR 0.000269
Train loss: 0.0013;  Loss pred: 0.0013; Loss self: 0.0000; time: 0.20s
Val loss: 0.4368 score: 0.8163 time: 0.06s
Test loss: 0.3931 score: 0.8542 time: 0.06s
Epoch 51/1000, LR 0.000269
Train loss: 0.0012;  Loss pred: 0.0012; Loss self: 0.0000; time: 0.20s
Val loss: 0.4314 score: 0.8367 time: 0.06s
Test loss: 0.3873 score: 0.8542 time: 0.07s
Epoch 52/1000, LR 0.000269
Train loss: 0.0012;  Loss pred: 0.0012; Loss self: 0.0000; time: 0.20s
Val loss: 0.4259 score: 0.8367 time: 0.07s
Test loss: 0.3817 score: 0.8542 time: 0.07s
Epoch 53/1000, LR 0.000269
Train loss: 0.0012;  Loss pred: 0.0012; Loss self: 0.0000; time: 0.21s
Val loss: 0.4208 score: 0.8367 time: 0.06s
Test loss: 0.3770 score: 0.8542 time: 0.07s
Epoch 54/1000, LR 0.000269
Train loss: 0.0010;  Loss pred: 0.0010; Loss self: 0.0000; time: 0.20s
Val loss: 0.4160 score: 0.8571 time: 0.07s
Test loss: 0.3728 score: 0.8542 time: 0.07s
Epoch 55/1000, LR 0.000269
Train loss: 0.0010;  Loss pred: 0.0010; Loss self: 0.0000; time: 0.20s
Val loss: 0.4116 score: 0.8571 time: 0.06s
Test loss: 0.3690 score: 0.8542 time: 0.06s
Epoch 56/1000, LR 0.000269
Train loss: 0.0010;  Loss pred: 0.0010; Loss self: 0.0000; time: 0.20s
Val loss: 0.4080 score: 0.8571 time: 0.06s
Test loss: 0.3659 score: 0.8542 time: 0.07s
Epoch 57/1000, LR 0.000269
Train loss: 0.0010;  Loss pred: 0.0010; Loss self: 0.0000; time: 0.20s
Val loss: 0.4049 score: 0.8571 time: 0.06s
Test loss: 0.3636 score: 0.8542 time: 0.07s
Epoch 58/1000, LR 0.000269
Train loss: 0.0009;  Loss pred: 0.0009; Loss self: 0.0000; time: 0.20s
Val loss: 0.4021 score: 0.8571 time: 0.07s
Test loss: 0.3621 score: 0.8333 time: 0.07s
Epoch 59/1000, LR 0.000268
Train loss: 0.0009;  Loss pred: 0.0009; Loss self: 0.0000; time: 0.20s
Val loss: 0.4001 score: 0.8571 time: 0.06s
Test loss: 0.3613 score: 0.8333 time: 0.06s
Epoch 60/1000, LR 0.000268
Train loss: 0.0008;  Loss pred: 0.0008; Loss self: 0.0000; time: 0.20s
Val loss: 0.3992 score: 0.8367 time: 0.06s
Test loss: 0.3615 score: 0.8333 time: 0.07s
Epoch 61/1000, LR 0.000268
Train loss: 0.0009;  Loss pred: 0.0009; Loss self: 0.0000; time: 0.20s
Val loss: 0.3991 score: 0.8163 time: 0.06s
Test loss: 0.3625 score: 0.8333 time: 0.06s
Epoch 62/1000, LR 0.000268
Train loss: 0.0009;  Loss pred: 0.0009; Loss self: 0.0000; time: 0.20s
Val loss: 0.3999 score: 0.8163 time: 0.06s
Test loss: 0.3643 score: 0.8542 time: 0.06s
     INFO: Early stopping counter 1 of 20
Epoch 63/1000, LR 0.000268
Train loss: 0.0008;  Loss pred: 0.0008; Loss self: 0.0000; time: 0.20s
Val loss: 0.4005 score: 0.8163 time: 0.06s
Test loss: 0.3651 score: 0.8542 time: 0.06s
     INFO: Early stopping counter 2 of 20
Epoch 64/1000, LR 0.000268
Train loss: 0.0008;  Loss pred: 0.0008; Loss self: 0.0000; time: 0.20s
Val loss: 0.4010 score: 0.8163 time: 0.06s
Test loss: 0.3657 score: 0.8542 time: 0.06s
     INFO: Early stopping counter 3 of 20
Epoch 65/1000, LR 0.000268
Train loss: 0.0008;  Loss pred: 0.0008; Loss self: 0.0000; time: 0.20s
Val loss: 0.4007 score: 0.7959 time: 0.06s
Test loss: 0.3653 score: 0.8542 time: 0.06s
     INFO: Early stopping counter 4 of 20
Epoch 66/1000, LR 0.000268
Train loss: 0.0008;  Loss pred: 0.0008; Loss self: 0.0000; time: 0.20s
Val loss: 0.4007 score: 0.7959 time: 0.06s
Test loss: 0.3639 score: 0.8542 time: 0.06s
     INFO: Early stopping counter 5 of 20
Epoch 67/1000, LR 0.000268
Train loss: 0.0007;  Loss pred: 0.0007; Loss self: 0.0000; time: 0.20s
Val loss: 0.4003 score: 0.7959 time: 0.07s
Test loss: 0.3612 score: 0.8542 time: 0.07s
     INFO: Early stopping counter 6 of 20
Epoch 68/1000, LR 0.000268
Train loss: 0.0007;  Loss pred: 0.0007; Loss self: 0.0000; time: 0.21s
Val loss: 0.4003 score: 0.8163 time: 0.07s
Test loss: 0.3585 score: 0.8542 time: 0.07s
     INFO: Early stopping counter 7 of 20
Epoch 69/1000, LR 0.000268
Train loss: 0.0007;  Loss pred: 0.0007; Loss self: 0.0000; time: 0.20s
Val loss: 0.4008 score: 0.8367 time: 0.06s
Test loss: 0.3560 score: 0.8542 time: 0.06s
     INFO: Early stopping counter 8 of 20
Epoch 70/1000, LR 0.000268
Train loss: 0.0008;  Loss pred: 0.0008; Loss self: 0.0000; time: 0.20s
Val loss: 0.4013 score: 0.8367 time: 0.06s
Test loss: 0.3534 score: 0.8750 time: 0.07s
     INFO: Early stopping counter 9 of 20
Epoch 71/1000, LR 0.000268
Train loss: 0.0008;  Loss pred: 0.0008; Loss self: 0.0000; time: 0.20s
Val loss: 0.4023 score: 0.8367 time: 0.06s
Test loss: 0.3513 score: 0.8542 time: 0.06s
     INFO: Early stopping counter 10 of 20
Epoch 72/1000, LR 0.000267
Train loss: 0.0006;  Loss pred: 0.0006; Loss self: 0.0000; time: 0.20s
Val loss: 0.4040 score: 0.8367 time: 0.06s
Test loss: 0.3499 score: 0.8542 time: 0.06s
     INFO: Early stopping counter 11 of 20
Epoch 73/1000, LR 0.000267
Train loss: 0.0007;  Loss pred: 0.0007; Loss self: 0.0000; time: 0.20s
Val loss: 0.4061 score: 0.8367 time: 0.06s
Test loss: 0.3486 score: 0.8542 time: 0.06s
     INFO: Early stopping counter 12 of 20
Epoch 74/1000, LR 0.000267
Train loss: 0.0006;  Loss pred: 0.0006; Loss self: 0.0000; time: 0.20s
Val loss: 0.4097 score: 0.8367 time: 0.06s
Test loss: 0.3489 score: 0.8542 time: 0.06s
     INFO: Early stopping counter 13 of 20
Epoch 75/1000, LR 0.000267
Train loss: 0.0007;  Loss pred: 0.0007; Loss self: 0.0000; time: 0.19s
Val loss: 0.4142 score: 0.8367 time: 0.06s
Test loss: 0.3503 score: 0.8542 time: 0.06s
     INFO: Early stopping counter 14 of 20
Epoch 76/1000, LR 0.000267
Train loss: 0.0006;  Loss pred: 0.0006; Loss self: 0.0000; time: 0.20s
Val loss: 0.4208 score: 0.8367 time: 0.06s
Test loss: 0.3543 score: 0.8542 time: 0.06s
     INFO: Early stopping counter 15 of 20
Epoch 77/1000, LR 0.000267
Train loss: 0.0006;  Loss pred: 0.0006; Loss self: 0.0000; time: 0.20s
Val loss: 0.4288 score: 0.8367 time: 0.06s
Test loss: 0.3619 score: 0.8542 time: 0.06s
     INFO: Early stopping counter 16 of 20
Epoch 78/1000, LR 0.000267
Train loss: 0.0006;  Loss pred: 0.0006; Loss self: 0.0000; time: 0.20s
Val loss: 0.4390 score: 0.8367 time: 0.06s
Test loss: 0.3750 score: 0.8542 time: 0.06s
     INFO: Early stopping counter 17 of 20
Epoch 79/1000, LR 0.000267
Train loss: 0.0005;  Loss pred: 0.0005; Loss self: 0.0000; time: 0.20s
Val loss: 0.4500 score: 0.8163 time: 0.06s
Test loss: 0.3869 score: 0.8542 time: 0.06s
     INFO: Early stopping counter 18 of 20
Epoch 80/1000, LR 0.000267
Train loss: 0.0005;  Loss pred: 0.0005; Loss self: 0.0000; time: 0.20s
Val loss: 0.4610 score: 0.8163 time: 0.06s
Test loss: 0.3955 score: 0.8333 time: 0.06s
     INFO: Early stopping counter 19 of 20
Epoch 81/1000, LR 0.000267
Train loss: 0.0005;  Loss pred: 0.0005; Loss self: 0.0000; time: 0.20s
Val loss: 0.4715 score: 0.8163 time: 0.06s
Test loss: 0.4015 score: 0.8333 time: 0.06s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 060,   Train_Loss: 0.0009,   Val_Loss: 0.3991,   Val_Precision: 1.0000,   Val_Recall: 0.6400,   Val_accuracy: 0.7805,   Val_Score: 0.8163,   Val_Loss: 0.3991,   Test_Precision: 0.9000,   Test_Recall: 0.7500,   Test_accuracy: 0.8182,   Test_Score: 0.8333,   Test_loss: 0.3625


[0.06425203999970108, 0.06446822395082563, 0.0644293170189485, 0.06476985302288085, 0.06434080202598125, 0.06430459895636886, 0.06523097504395992, 0.06436507194302976, 0.06427436193916947, 0.06485680502373725, 0.07058735901955515, 0.06834620400331914, 0.06551392399705946, 0.06486350495833904, 0.06500770209822804, 0.06453821307513863, 0.06561226700432599, 0.0654647700721398, 0.06502443796489388, 0.0654919610824436, 0.06563644600100815, 0.0650284409057349, 0.06561196490656585, 0.06491087400354445, 0.06469065300188959, 0.06533244298771024, 0.06540759303607047, 0.06554030301049352, 0.06473047402687371, 0.06534642807673663, 0.06550086999777704, 0.1535849729552865, 0.07732475700322539, 0.07773252495098859, 0.07733356894459575, 0.08748963905964047, 0.07925277797039598, 0.07633526297286153, 0.07637642102781683, 0.07689351798035204, 0.07759404403623194, 0.08165187295526266, 0.07745094399433583, 0.07689007604494691, 0.07708709593862295, 0.07675384904723614, 0.0766594719607383, 0.0765633259434253, 0.07664633309468627, 0.08394460508134216, 0.0771046910667792, 0.07573227898683399, 0.06901705998461694, 0.06977928907144815, 0.06898743100464344, 0.06897105101961643, 0.06868463300634176, 0.06929407501593232, 0.06935571506619453, 0.06901303492486477, 0.06882913003209978, 0.06863083806820214, 0.06921407300978899, 0.06897705898154527, 0.06877671310212463, 0.06926170608494431, 0.06954786204732955, 0.06871014391072094, 0.0692577799782157, 0.0694252549437806, 0.06919200601987541, 0.06913036899641156, 0.06908034498337656, 0.069974772981368, 0.06928285700269043, 0.07046021206770092, 0.07705437601543963, 0.06876475503668189, 0.06900851102545857, 0.06916940794326365, 0.0693420369643718, 0.06906018208246678, 0.07028906606137753, 0.06886731903068721, 0.0688724679639563, 0.06947832100559026, 0.06931909604463726, 0.0696491829585284, 0.06973451899830252, 0.06996471795719117, 0.06939325202256441, 0.06936285400297493, 0.0691484350245446, 0.06926506292074919, 0.0689641950884834, 0.07045045506674796, 0.06975378002971411, 0.06947690097149462, 0.07014729001093656, 0.0699311540229246, 0.0692664620000869, 0.06909635104238987, 0.06946653698105365, 0.06981708807870746, 0.06970217707566917, 0.06966624408960342, 0.06887119892053306, 0.06960547901690006, 0.07009885797742754, 0.06954061100259423, 0.0689595949370414, 0.06976446998305619, 0.06887308601289988, 0.06825238896999508, 0.06936262897215784, 0.06857294798828661, 0.06838340603280813, 0.06925643107388169, 0.07000754296313971, 0.06962063896935433, 0.06895111501216888, 0.06954776600468904, 0.06853557599242777, 0.06862653302960098, 0.06906908703967929, 0.0689376019872725, 0.06893807591404766, 0.06924250104930252, 0.06906685000285506, 0.06921269302256405, 0.06878978200256824, 0.06932190794032067, 0.06885918695479631]
[0.0013112661224428791, 0.001315678039812768, 0.0013148840207948672, 0.0013218337351608338, 0.0013130775923669642, 0.0013123387542116095, 0.001331244388652243, 0.0013135728967965258, 0.0013117216722279483, 0.0013236082657905562, 0.0014405583473378603, 0.001394820489863656, 0.0013370188570828462, 0.0013237449991497763, 0.001326687797923021, 0.0013171063892885434, 0.0013390258572311426, 0.0013360157157579552, 0.001327029346222324, 0.0013365706343355837, 0.0013395193061430234, 0.0013271110388925489, 0.0013390196919707315, 0.00132471171435805, 0.0013202174082018283, 0.0013333151630144945, 0.0013348488374708258, 0.001337557204295786, 0.001321030082181096, 0.0013336005729946252, 0.0013367524489342254, 0.0031343872031691124, 0.0015780562653719466, 0.001586378060224257, 0.0015782361009101175, 0.0017855028379518464, 0.0016174036320488975, 0.0015578625096502353, 0.0015587024699554456, 0.0015692554689867764, 0.001583551919106774, 0.001666364754189034, 0.0015806315100884863, 0.0015691852254070798, 0.0015732060395637338, 0.001566405082596656, 0.0015644790196069041, 0.0015625168559882715, 0.0015642108794833934, 0.0017131552057416768, 0.0015735651238118202, 0.0015455567140170202, 0.001437855416346186, 0.001453735188988503, 0.0014372381459300716, 0.0014368968962420088, 0.0014309298542987865, 0.0014436265628319234, 0.0014449107305457194, 0.0014377715609346826, 0.0014339402090020787, 0.0014298091264208779, 0.001441959854370604, 0.0014370220621155265, 0.0014328481896275964, 0.0014429522101030063, 0.001448913792652699, 0.001431461331473353, 0.001442870416212827, 0.0014463594779954292, 0.001441500125414071, 0.0014402160207585741, 0.0014391738538203451, 0.0014578077704451668, 0.0014433928542227175, 0.0014679210847437691, 0.001605299500321659, 0.001432599063264206, 0.001437677313030387, 0.0014410293321513261, 0.0014446257700910792, 0.0014387537933847245, 0.0014643555429453652, 0.0014347358131393169, 0.0014348430825824228, 0.001447465020949797, 0.0014441478342632763, 0.0014510246449693416, 0.0014528024791313026, 0.001457598290774816, 0.0014456927504700918, 0.001445059458395311, 0.0014405923963446792, 0.0014430221441822748, 0.001436754064343404, 0.0014677178138905826, 0.0014532037506190438, 0.0014474354369061377, 0.001461401875227845, 0.0014568990421442625, 0.0014430512916684772, 0.0014395073133831222, 0.0014472195204386178, 0.0014545226683064054, 0.001452128689076441, 0.0014513800852000713, 0.001434816644177772, 0.001450114146185418, 0.0014603928745297405, 0.0014487627292207133, 0.0014366582278550293, 0.0014534264579803373, 0.0014348559586020808, 0.001421924770208231, 0.0014450547702532883, 0.0014286030830893044, 0.0014246542923501693, 0.0014428423140392017, 0.001458490478398744, 0.0014504299785282153, 0.0014364815627535184, 0.001448911791764355, 0.0014278244998422451, 0.0014297194381166871, 0.001438939313326652, 0.0014362000414015104, 0.0014362099148759928, 0.0014425521051938024, 0.0014388927083928138, 0.0014419311046367511, 0.0014331204583868384, 0.0014442064154233474, 0.0014345663948915899]
[762.621700419597, 760.0643696555948, 760.5233497289631, 756.5247983917775, 761.5696176776514, 761.9983764030136, 751.1768752035107, 761.2824552324038, 762.3568483865242, 755.5105432971337, 694.1752840820309, 716.938134524931, 747.9326074591307, 755.4325044795536, 753.7568383198648, 759.2401100872089, 746.8115679765996, 748.4941892563554, 753.5628378126801, 748.1834287771146, 746.5364593208989, 753.5164509176885, 746.8150065278189, 754.8812237118293, 757.4510029844455, 750.0102209436351, 749.1484967651671, 747.6315755231512, 756.985032732141, 749.8497078135481, 748.0816667269145, 319.04162925018363, 633.6909665032132, 630.3667612867994, 633.6187592105722, 560.0663178710497, 618.2748574227066, 641.9051705817838, 641.5592579567699, 637.2448717006362, 631.4917672949206, 600.1087081841621, 632.6585251637925, 637.27339756247, 635.6446484767566, 638.4044658117957, 639.1904189621304, 639.9930958616848, 639.2999902482884, 583.7182741227872, 635.4995957063345, 647.0160499001835, 695.4802191037784, 687.8831905388435, 695.7789165503088, 695.9441575908139, 698.8462760741269, 692.6999168249765, 692.0842781908859, 695.5207817227298, 697.3791471374733, 699.394053039245, 693.5005832298199, 695.883540248394, 697.9106420617417, 693.0236448569659, 690.1721862756107, 698.5868063727122, 693.0629311984574, 691.3910512661364, 693.7217571956505, 694.3402833925506, 694.8430846943611, 685.9614966208012, 692.8120761263647, 681.2355312510233, 622.9367166685263, 698.0320074490902, 695.5663770558949, 693.948400416729, 692.2207956576555, 695.0459519883947, 682.8942634987587, 696.9924294368312, 696.9403220038566, 690.862981506676, 692.4498837822543, 689.1681705524228, 688.3248165971922, 686.0600800158936, 691.7099083984704, 692.0130477610006, 694.1588769574055, 692.9900584212278, 696.0133434228353, 681.3298786292099, 688.1347502537166, 690.8771020125627, 684.2744743598277, 686.389359229862, 692.9760610544794, 694.6821254070641, 690.9801767301507, 687.510770227022, 688.6442004227625, 688.9993945742689, 696.9531640560626, 689.6008859926918, 684.7472467448247, 690.2441509783305, 696.059773028292, 688.0293079221822, 696.934067844871, 703.2720865067686, 692.0152928353855, 699.9844896299221, 701.9246741961225, 693.0764299534053, 685.6404034244267, 689.4507248221131, 696.1453776567455, 690.1731393753719, 700.3661865379717, 699.4379270084349, 694.9563409231778, 696.2818348230611, 696.2770481126663, 693.2158612500538, 694.9788501721997, 693.5144104904501, 697.7780507896933, 692.4217960262039, 697.074742278185]
Elapsed: 0.07036454496814176~0.008364745675405688
Time per graph: 0.001453972490678897~0.00016919336241304518
Speed: 693.355401125213~51.296748479174695
Total Time: 0.0695
best val loss: 0.39912670850753784 test_score: 0.8333

Testing...
Test loss: 0.3728 score: 0.8542 time: 0.06s
test Score 0.8542
Epoch Time List: [0.32221966504585, 0.3220785789890215, 0.3261531739262864, 0.32230526697821915, 0.32145366503391415, 0.3223731560865417, 0.323572478024289, 0.324162490083836, 0.32205680094193667, 0.3226844510063529, 0.32808659691363573, 0.34115630912128836, 0.32881911005824804, 0.3231145889731124, 0.3240199339343235, 0.3236857949523255, 0.3277616339037195, 0.3239758238196373, 0.3210588910151273, 0.32347817707341164, 0.3228687949012965, 0.3221863938961178, 0.324742745840922, 0.32348771090619266, 0.32302012119907886, 0.32448436703998595, 0.32573012297507375, 0.33252478716894984, 0.3226655040634796, 0.32310725282877684, 0.3241481491131708, 0.4124186368426308, 0.34764637192711234, 0.3458498640684411, 0.3495588480727747, 0.3630177341401577, 0.34837410401087254, 0.4355285500641912, 0.34368113800883293, 0.3461113600060344, 0.34767066198401153, 0.34906239807605743, 0.40352145000360906, 0.3441899560857564, 0.3441683860728517, 0.3459396950202063, 0.3434602312045172, 0.34170704893767834, 0.3410823120502755, 0.3535632330458611, 0.34584158193320036, 0.3304478778736666, 0.33134588203392923, 0.3287030690116808, 0.3338750269031152, 0.32682247913908213, 0.32872488093562424, 0.3368417341262102, 0.33085653197485954, 0.33103631203994155, 0.3398200029041618, 0.3300033569103107, 0.3281587508972734, 0.3320822680834681, 0.3265655229333788, 0.32831684802658856, 0.3280883899424225, 0.32767582300584763, 0.3284368831664324, 0.3303085849620402, 0.32917318190447986, 0.3291078959591687, 0.3293139139423147, 0.3339325259439647, 0.3362855539890006, 0.3414824490901083, 0.35954194294754416, 0.3392198591027409, 0.32759386906400323, 0.32856295304372907, 0.3289245170308277, 0.3305568539071828, 0.33006245095748454, 0.3289132989011705, 0.32937820605002344, 0.3278583580395207, 0.3298803379293531, 0.3298618099652231, 0.3368873781291768, 0.3416181809734553, 0.33091958798468113, 0.33074729808140546, 0.32469105906784534, 0.32539779494982213, 0.3249885430559516, 0.330339370877482, 0.3276834050193429, 0.32658971298951656, 0.32687417487613857, 0.33562604105100036, 0.32804749405477196, 0.3255110300378874, 0.3320063741412014, 0.3326887080911547, 0.3385872779181227, 0.33256311109289527, 0.32681288302410394, 0.3261646858882159, 0.331841284991242, 0.3323128930060193, 0.32398480002302676, 0.3328409430105239, 0.3299194979481399, 0.32613464596215636, 0.330494575900957, 0.32938052411191165, 0.3283508811146021, 0.3282130380393937, 0.33078881702385843, 0.34388820093590766, 0.3278474301332608, 0.32930350804235786, 0.3242420250317082, 0.3232389020267874, 0.3265627889195457, 0.3255103682167828, 0.32315577287226915, 0.3283303079660982, 0.3237965349107981, 0.3291385821066797, 0.32678024691995233, 0.32491294003557414, 0.3271847050637007]
Total Epoch List: [31, 21, 81]
Total Time List: [0.06592245097272098, 0.07622005802113563, 0.06946407095529139]
T-times Epoch Time: 0.3384000400186667 ~ 0.005074404636855091
T-times Total Epoch: 83.22222222222221 ~ 27.50398399873133
T-times Total Time: 0.07250486157782789 ~ 0.0020304821101707034
T-times Inference Elapsed: 0.07183535006935238 ~ 0.0013514441878071776
T-times Time Per Graph: 0.0014783872344749707 ~ 2.4485268704560003e-05
T-times Speed: 682.129727875859 ~ 10.879667399817661
T-times cross validation test micro f1 score:0.828744140423655 ~ 0.11254621237436228
T-times cross validation test precision:0.8728447371304515 ~ 0.14955795936719943
T-times cross validation test recall:0.8259259259259258 ~ 0.039610587325333714
T-times cross validation test f1_score:0.828744140423655 ~ 0.07133881778266676
