Namespace(seed=55, model='SAMamba', dataset='ico_wallets/Volume', num_heads=8, num_layers=2, dim_hidden=128, dropout=0.6, epochs=1000, lr=0.0005, weight_decay=0.0001, batch_size=64, abs_pe=None, abs_pe_dim=20, num_class=2, outdir='./outdir/ico_wallets/Volume/seed55/khopgnn_gat_1_0.6_0.0005_0.0001_2_8_128_BN', warmup=10, layer_norm=False, use_edge_attr=False, edge_dim=128, gnn_type='gat', k_hop=1, global_pool='mean', se='khopgnn', ie='mamba', aggr='add', not_extract_node_feature=False, profile=False, k_ford=3, early_stop=1, early_stop_mindelta=-0.0, patience=20, training_times=3, Lambda=0.01, order='sd', use_cuda=True, batch_norm=True, save_logs=True)
Data(edge_index=[2, 334], edge_attr=[334, 2], x=[97, 14887], y=[1, 1], num_nodes=97)
Data(edge_index=[2, 334], edge_attr=[334, 2], x=[97, 14887], y=[1, 1], num_nodes=97)
Data(edge_index=[2, 334], edge_attr=[334, 2], x=[97, 14887], y=[1, 1], num_nodes=97)
========================training times:0========================
========================k_idx:0========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: SAMamba
GraphEncoder(
  (subgraph_conv): GATConv(128, 128, heads=1)
  (embedding): Linear(in_features=14887, out_features=128, bias=False)
  (encoder): GraphMambaEncoder(
    (layers): ModuleList(
      (0-1): 2 x MambaEncoderLayer(
        (self_selen): Selection(
          (khop_structure_extractor): KHopStructureExtractor(
            (structure_extractor): StructureExtractor(
              (gcn): ModuleList(
                (0): GATConv(128, 128, heads=1)
              )
              (relu): ReLU()
              (bn): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (out_proj): Linear(in_features=128, out_features=128, bias=True)
            )
            (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (out_proj): Linear(in_features=256, out_features=128, bias=True)
          )
          (out_proj): Mamba(
            (in_proj): Linear(in_features=128, out_features=512, bias=False)
            (conv1d): Conv1d(256, 256, kernel_size=(4,), stride=(1,), padding=(3,), groups=256)
            (act): SiLU()
            (x_proj): Linear(in_features=256, out_features=40, bias=False)
            (dt_proj): Linear(in_features=8, out_features=256, bias=True)
            (out_proj): Linear(in_features=256, out_features=128, bias=False)
          )
        )
        (norm1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (linear1): Linear(in_features=128, out_features=256, bias=True)
        (dropout): Dropout(p=0.6, inplace=False)
        (linear2): Linear(in_features=256, out_features=128, bias=True)
        (dropout1): Dropout(p=0.6, inplace=False)
        (dropout2): Dropout(p=0.6, inplace=False)
        (activation): ReLU()
      )
    )
  )
  (classifier): Sequential(
    (0): Linear(in_features=128, out_features=128, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=128, out_features=2, bias=True)
  )
)
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7c7a550cfd90>
Training...
Epoch 1/1000, LR 0.000500
Train loss: 0.7189;  Loss pred: 0.7069; Loss self: 1.1909; time: 0.45s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6939 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6930 score: 0.5102 time: 0.06s
Epoch 2/1000, LR 0.000000
Train loss: 0.7145;  Loss pred: 0.7029; Loss self: 1.1609; time: 0.12s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6938 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6930 score: 0.5102 time: 0.06s
Epoch 3/1000, LR 0.000050
Train loss: 0.6949;  Loss pred: 0.6829; Loss self: 1.1981; time: 0.12s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6938 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6929 score: 0.5102 time: 0.06s
     INFO: Early stopping counter 1 of 20
Epoch 4/1000, LR 0.000100
Train loss: 0.6668;  Loss pred: 0.6548; Loss self: 1.2015; time: 0.13s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6937 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6928 score: 0.5102 time: 0.06s
Epoch 5/1000, LR 0.000150
Train loss: 0.6351;  Loss pred: 0.6232; Loss self: 1.1821; time: 0.13s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6935 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6927 score: 0.5102 time: 0.07s
Epoch 6/1000, LR 0.000200
Train loss: 0.5856;  Loss pred: 0.5738; Loss self: 1.1858; time: 0.12s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6932 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6926 score: 0.5102 time: 0.06s
Epoch 7/1000, LR 0.000250
Train loss: 0.5460;  Loss pred: 0.5326; Loss self: 1.3365; time: 0.12s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6929 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6924 score: 0.5102 time: 0.06s
Epoch 8/1000, LR 0.000300
Train loss: 0.4811;  Loss pred: 0.4673; Loss self: 1.3791; time: 0.13s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6923 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6922 score: 0.5102 time: 0.07s
Epoch 9/1000, LR 0.000350
Train loss: 0.4235;  Loss pred: 0.4091; Loss self: 1.4451; time: 0.13s
Val loss: 0.6914 score: 0.5102 time: 0.07s
Test loss: 0.6918 score: 0.5306 time: 0.06s
Epoch 10/1000, LR 0.000400
Train loss: 0.3674;  Loss pred: 0.3521; Loss self: 1.5286; time: 0.12s
Val loss: 0.6900 score: 0.7755 time: 0.07s
Test loss: 0.6911 score: 0.6531 time: 0.06s
Epoch 11/1000, LR 0.000450
Train loss: 0.3111;  Loss pred: 0.2950; Loss self: 1.6103; time: 0.13s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6877 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6898 score: 0.4898 time: 0.06s
Epoch 12/1000, LR 0.000450
Train loss: 0.2535;  Loss pred: 0.2362; Loss self: 1.7291; time: 0.12s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6844 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6878 score: 0.4898 time: 0.07s
Epoch 13/1000, LR 0.000450
Train loss: 0.2252;  Loss pred: 0.2072; Loss self: 1.8079; time: 0.12s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6797 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6847 score: 0.4898 time: 0.07s
Epoch 14/1000, LR 0.000450
Train loss: 0.1934;  Loss pred: 0.1747; Loss self: 1.8637; time: 0.12s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6728 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6800 score: 0.4898 time: 0.06s
Epoch 15/1000, LR 0.000450
Train loss: 0.1722;  Loss pred: 0.1528; Loss self: 1.9402; time: 0.13s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6627 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6725 score: 0.4898 time: 0.06s
Epoch 16/1000, LR 0.000450
Train loss: 0.1453;  Loss pred: 0.1256; Loss self: 1.9746; time: 0.13s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6485 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6616 score: 0.4898 time: 0.07s
Epoch 17/1000, LR 0.000450
Train loss: 0.1270;  Loss pred: 0.1062; Loss self: 2.0755; time: 0.14s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6294 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6467 score: 0.4898 time: 0.06s
Epoch 18/1000, LR 0.000450
Train loss: 0.1159;  Loss pred: 0.0948; Loss self: 2.1147; time: 0.15s
Val loss: 0.6039 score: 0.5510 time: 0.07s
Test loss: 0.6262 score: 0.5510 time: 0.07s
Epoch 19/1000, LR 0.000450
Train loss: 0.1041;  Loss pred: 0.0827; Loss self: 2.1464; time: 0.15s
Val loss: 0.5705 score: 0.7347 time: 0.07s
Test loss: 0.5994 score: 0.6939 time: 0.06s
Epoch 20/1000, LR 0.000450
Train loss: 0.0913;  Loss pred: 0.0690; Loss self: 2.2295; time: 0.15s
Val loss: 0.5273 score: 0.8776 time: 0.07s
Test loss: 0.5653 score: 0.7755 time: 0.06s
Epoch 21/1000, LR 0.000450
Train loss: 0.0827;  Loss pred: 0.0603; Loss self: 2.2382; time: 0.14s
Val loss: 0.4761 score: 0.9388 time: 0.08s
Test loss: 0.5257 score: 0.8163 time: 0.07s
Epoch 22/1000, LR 0.000450
Train loss: 0.0731;  Loss pred: 0.0503; Loss self: 2.2825; time: 0.14s
Val loss: 0.4187 score: 0.9388 time: 0.07s
Test loss: 0.4821 score: 0.8776 time: 0.06s
Epoch 23/1000, LR 0.000450
Train loss: 0.0666;  Loss pred: 0.0436; Loss self: 2.3015; time: 0.14s
Val loss: 0.3582 score: 0.9388 time: 0.07s
Test loss: 0.4372 score: 0.8776 time: 0.06s
Epoch 24/1000, LR 0.000450
Train loss: 0.0613;  Loss pred: 0.0382; Loss self: 2.3093; time: 0.15s
Val loss: 0.3001 score: 0.9388 time: 0.07s
Test loss: 0.3961 score: 0.8776 time: 0.06s
Epoch 25/1000, LR 0.000450
Train loss: 0.0557;  Loss pred: 0.0318; Loss self: 2.3905; time: 0.15s
Val loss: 0.2474 score: 0.9388 time: 0.07s
Test loss: 0.3610 score: 0.8776 time: 0.06s
Epoch 26/1000, LR 0.000450
Train loss: 0.0505;  Loss pred: 0.0267; Loss self: 2.3823; time: 0.13s
Val loss: 0.2056 score: 0.9388 time: 0.16s
Test loss: 0.3382 score: 0.8776 time: 0.06s
Epoch 27/1000, LR 0.000450
Train loss: 0.0457;  Loss pred: 0.0214; Loss self: 2.4270; time: 0.13s
Val loss: 0.1723 score: 0.9388 time: 0.07s
Test loss: 0.3234 score: 0.8980 time: 0.06s
Epoch 28/1000, LR 0.000450
Train loss: 0.0430;  Loss pred: 0.0187; Loss self: 2.4304; time: 0.13s
Val loss: 0.1490 score: 0.9388 time: 0.08s
Test loss: 0.3184 score: 0.8980 time: 0.07s
Epoch 29/1000, LR 0.000450
Train loss: 0.0395;  Loss pred: 0.0152; Loss self: 2.4369; time: 0.13s
Val loss: 0.1330 score: 0.9388 time: 0.07s
Test loss: 0.3209 score: 0.8980 time: 0.07s
Epoch 30/1000, LR 0.000450
Train loss: 0.0378;  Loss pred: 0.0133; Loss self: 2.4521; time: 0.13s
Val loss: 0.1220 score: 0.9388 time: 0.08s
Test loss: 0.3285 score: 0.8980 time: 0.06s
Epoch 31/1000, LR 0.000450
Train loss: 0.0363;  Loss pred: 0.0118; Loss self: 2.4532; time: 0.13s
Val loss: 0.1144 score: 0.9388 time: 0.07s
Test loss: 0.3383 score: 0.8980 time: 0.06s
Epoch 32/1000, LR 0.000450
Train loss: 0.0344;  Loss pred: 0.0094; Loss self: 2.4947; time: 0.22s
Val loss: 0.1095 score: 0.9388 time: 0.07s
Test loss: 0.3500 score: 0.8980 time: 0.07s
Epoch 33/1000, LR 0.000449
Train loss: 0.0329;  Loss pred: 0.0080; Loss self: 2.4958; time: 0.13s
Val loss: 0.1058 score: 0.9388 time: 0.07s
Test loss: 0.3614 score: 0.8980 time: 0.06s
Epoch 34/1000, LR 0.000449
Train loss: 0.0318;  Loss pred: 0.0068; Loss self: 2.4980; time: 0.13s
Val loss: 0.1041 score: 0.9388 time: 0.07s
Test loss: 0.3739 score: 0.8980 time: 0.06s
Epoch 35/1000, LR 0.000449
Train loss: 0.0311;  Loss pred: 0.0058; Loss self: 2.5300; time: 0.13s
Val loss: 0.1024 score: 0.9388 time: 0.07s
Test loss: 0.3848 score: 0.8980 time: 0.06s
Epoch 36/1000, LR 0.000449
Train loss: 0.0300;  Loss pred: 0.0049; Loss self: 2.5053; time: 0.13s
Val loss: 0.1007 score: 0.9388 time: 0.07s
Test loss: 0.3930 score: 0.8980 time: 0.06s
Epoch 37/1000, LR 0.000449
Train loss: 0.0296;  Loss pred: 0.0043; Loss self: 2.5299; time: 0.13s
Val loss: 0.0990 score: 0.9388 time: 0.15s
Test loss: 0.3996 score: 0.8980 time: 0.06s
Epoch 38/1000, LR 0.000449
Train loss: 0.0292;  Loss pred: 0.0040; Loss self: 2.5206; time: 0.13s
Val loss: 0.0971 score: 0.9388 time: 0.08s
Test loss: 0.4042 score: 0.8980 time: 0.06s
Epoch 39/1000, LR 0.000449
Train loss: 0.0287;  Loss pred: 0.0033; Loss self: 2.5375; time: 0.13s
Val loss: 0.0953 score: 0.9388 time: 0.07s
Test loss: 0.4071 score: 0.8980 time: 0.06s
Epoch 40/1000, LR 0.000449
Train loss: 0.0281;  Loss pred: 0.0030; Loss self: 2.5168; time: 0.13s
Val loss: 0.0931 score: 0.9388 time: 0.07s
Test loss: 0.4069 score: 0.8980 time: 0.07s
Epoch 41/1000, LR 0.000449
Train loss: 0.0279;  Loss pred: 0.0026; Loss self: 2.5293; time: 0.13s
Val loss: 0.0904 score: 0.9388 time: 0.07s
Test loss: 0.4037 score: 0.8980 time: 0.06s
Epoch 42/1000, LR 0.000449
Train loss: 0.0277;  Loss pred: 0.0024; Loss self: 2.5288; time: 0.13s
Val loss: 0.0878 score: 0.9388 time: 0.07s
Test loss: 0.3984 score: 0.8980 time: 0.16s
Epoch 43/1000, LR 0.000449
Train loss: 0.0274;  Loss pred: 0.0023; Loss self: 2.5145; time: 0.13s
Val loss: 0.0864 score: 0.9388 time: 0.07s
Test loss: 0.3960 score: 0.8980 time: 0.06s
Epoch 44/1000, LR 0.000449
Train loss: 0.0270;  Loss pred: 0.0019; Loss self: 2.5113; time: 0.13s
Val loss: 0.0851 score: 0.9388 time: 0.07s
Test loss: 0.3924 score: 0.8980 time: 0.06s
Epoch 45/1000, LR 0.000449
Train loss: 0.0269;  Loss pred: 0.0018; Loss self: 2.5049; time: 0.13s
Val loss: 0.0842 score: 0.9388 time: 0.07s
Test loss: 0.3888 score: 0.8980 time: 0.07s
Epoch 46/1000, LR 0.000449
Train loss: 0.0264;  Loss pred: 0.0016; Loss self: 2.4812; time: 0.13s
Val loss: 0.0835 score: 0.9388 time: 0.07s
Test loss: 0.3855 score: 0.8980 time: 0.06s
Epoch 47/1000, LR 0.000449
Train loss: 0.0265;  Loss pred: 0.0017; Loss self: 2.4736; time: 0.13s
Val loss: 0.0837 score: 0.9388 time: 0.07s
Test loss: 0.3845 score: 0.8980 time: 0.06s
     INFO: Early stopping counter 1 of 20
Epoch 48/1000, LR 0.000448
Train loss: 0.0258;  Loss pred: 0.0016; Loss self: 2.4271; time: 0.14s
Val loss: 0.0843 score: 0.9388 time: 0.12s
Test loss: 0.3844 score: 0.8980 time: 0.06s
     INFO: Early stopping counter 2 of 20
Epoch 49/1000, LR 0.000448
Train loss: 0.0257;  Loss pred: 0.0014; Loss self: 2.4241; time: 0.13s
Val loss: 0.0851 score: 0.9388 time: 0.07s
Test loss: 0.3846 score: 0.8980 time: 0.06s
     INFO: Early stopping counter 3 of 20
Epoch 50/1000, LR 0.000448
Train loss: 0.0257;  Loss pred: 0.0015; Loss self: 2.4216; time: 0.13s
Val loss: 0.0865 score: 0.9388 time: 0.07s
Test loss: 0.3863 score: 0.8980 time: 0.07s
     INFO: Early stopping counter 4 of 20
Epoch 51/1000, LR 0.000448
Train loss: 0.0252;  Loss pred: 0.0014; Loss self: 2.3839; time: 0.13s
Val loss: 0.0886 score: 0.9388 time: 0.07s
Test loss: 0.3904 score: 0.8980 time: 0.06s
     INFO: Early stopping counter 5 of 20
Epoch 52/1000, LR 0.000448
Train loss: 0.0252;  Loss pred: 0.0014; Loss self: 2.3793; time: 0.13s
Val loss: 0.0907 score: 0.9388 time: 0.07s
Test loss: 0.3939 score: 0.8980 time: 0.06s
     INFO: Early stopping counter 6 of 20
Epoch 53/1000, LR 0.000448
Train loss: 0.0249;  Loss pred: 0.0013; Loss self: 2.3597; time: 0.13s
Val loss: 0.0939 score: 0.9388 time: 0.07s
Test loss: 0.4000 score: 0.8980 time: 0.15s
     INFO: Early stopping counter 7 of 20
Epoch 54/1000, LR 0.000448
Train loss: 0.0248;  Loss pred: 0.0013; Loss self: 2.3497; time: 0.13s
Val loss: 0.0960 score: 0.9388 time: 0.07s
Test loss: 0.4042 score: 0.8980 time: 0.06s
     INFO: Early stopping counter 8 of 20
Epoch 55/1000, LR 0.000448
Train loss: 0.0244;  Loss pred: 0.0013; Loss self: 2.3066; time: 0.13s
Val loss: 0.0991 score: 0.9388 time: 0.07s
Test loss: 0.4103 score: 0.8980 time: 0.06s
     INFO: Early stopping counter 9 of 20
Epoch 56/1000, LR 0.000448
Train loss: 0.0243;  Loss pred: 0.0013; Loss self: 2.3010; time: 0.13s
Val loss: 0.1015 score: 0.9388 time: 0.08s
Test loss: 0.4149 score: 0.8980 time: 0.07s
     INFO: Early stopping counter 10 of 20
Epoch 57/1000, LR 0.000448
Train loss: 0.0243;  Loss pred: 0.0014; Loss self: 2.2878; time: 0.13s
Val loss: 0.1047 score: 0.9388 time: 0.07s
Test loss: 0.4221 score: 0.8980 time: 0.06s
     INFO: Early stopping counter 11 of 20
Epoch 58/1000, LR 0.000448
Train loss: 0.0240;  Loss pred: 0.0015; Loss self: 2.2541; time: 0.13s
Val loss: 0.1068 score: 0.9388 time: 0.07s
Test loss: 0.4256 score: 0.8980 time: 0.07s
     INFO: Early stopping counter 12 of 20
Epoch 59/1000, LR 0.000447
Train loss: 0.0237;  Loss pred: 0.0013; Loss self: 2.2443; time: 0.22s
Val loss: 0.1099 score: 0.9388 time: 0.07s
Test loss: 0.4327 score: 0.8980 time: 0.06s
     INFO: Early stopping counter 13 of 20
Epoch 60/1000, LR 0.000447
Train loss: 0.0238;  Loss pred: 0.0015; Loss self: 2.2298; time: 0.13s
Val loss: 0.1126 score: 0.9388 time: 0.07s
Test loss: 0.4390 score: 0.8776 time: 0.06s
     INFO: Early stopping counter 14 of 20
Epoch 61/1000, LR 0.000447
Train loss: 0.0239;  Loss pred: 0.0015; Loss self: 2.2376; time: 0.13s
Val loss: 0.1153 score: 0.9388 time: 0.07s
Test loss: 0.4455 score: 0.8776 time: 0.06s
     INFO: Early stopping counter 15 of 20
Epoch 62/1000, LR 0.000447
Train loss: 0.0235;  Loss pred: 0.0014; Loss self: 2.2034; time: 0.13s
Val loss: 0.1174 score: 0.9388 time: 0.08s
Test loss: 0.4511 score: 0.8776 time: 0.07s
     INFO: Early stopping counter 16 of 20
Epoch 63/1000, LR 0.000447
Train loss: 0.0235;  Loss pred: 0.0014; Loss self: 2.2049; time: 0.15s
Val loss: 0.1190 score: 0.9388 time: 0.08s
Test loss: 0.4552 score: 0.8776 time: 0.07s
     INFO: Early stopping counter 17 of 20
Epoch 64/1000, LR 0.000447
Train loss: 0.0233;  Loss pred: 0.0014; Loss self: 2.1866; time: 0.15s
Val loss: 0.1209 score: 0.9388 time: 0.10s
Test loss: 0.4605 score: 0.8776 time: 0.07s
     INFO: Early stopping counter 18 of 20
Epoch 65/1000, LR 0.000447
Train loss: 0.0227;  Loss pred: 0.0013; Loss self: 2.1394; time: 0.13s
Val loss: 0.1226 score: 0.9388 time: 0.07s
Test loss: 0.4653 score: 0.8776 time: 0.07s
     INFO: Early stopping counter 19 of 20
Epoch 66/1000, LR 0.000447
Train loss: 0.0230;  Loss pred: 0.0014; Loss self: 2.1598; time: 0.15s
Val loss: 0.1242 score: 0.9388 time: 0.07s
Test loss: 0.4693 score: 0.8776 time: 0.07s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 045,   Train_Loss: 0.0264,   Val_Loss: 0.0835,   Val_Precision: 1.0000,   Val_Recall: 0.8750,   Val_accuracy: 0.9333,   Val_Score: 0.9388,   Val_Loss: 0.0835,   Test_Precision: 1.0000,   Test_Recall: 0.8000,   Test_accuracy: 0.8889,   Test_Score: 0.8980,   Test_loss: 0.3855


[0.07028893707320094, 0.06375131313689053, 0.06626683403737843, 0.06637981696985662, 0.07203501579351723, 0.06976736290380359, 0.06557220313698053, 0.07044689287431538, 0.06588829192332923, 0.06534151406958699, 0.06640201108530164, 0.07176870806142688, 0.07159258611500263, 0.06637484510429204, 0.06725530815310776, 0.07072933088056743, 0.06857617595233023, 0.07443305803462863, 0.06895881798118353, 0.06720388191752136, 0.07554078404791653, 0.0681827338412404, 0.06893877498805523, 0.06995600881054997, 0.06963628204539418, 0.06869278405793011, 0.06879995786584914, 0.07112252805382013, 0.07484309794381261, 0.06940691499039531, 0.07172186602838337, 0.07340120407752693, 0.06423657503910363, 0.06582075101323426, 0.06590597401373088, 0.06632891180925071, 0.0656651551835239, 0.06757640279829502, 0.06628363882191479, 0.07127058389596641, 0.06654219911433756, 0.16738361190073192, 0.06631779996678233, 0.06679502502083778, 0.07146188104525208, 0.06510528200305998, 0.06631078105419874, 0.06831185892224312, 0.06860164715908468, 0.07292108610272408, 0.06991384411230683, 0.06962025701068342, 0.15700826607644558, 0.06808782601729035, 0.06872897199355066, 0.07030226313509047, 0.069984134985134, 0.07482315716333687, 0.06873672199435532, 0.06873896298930049, 0.0693358511198312, 0.07091508898884058, 0.07027307199314237, 0.0707796011120081, 0.07071904186159372, 0.07148044789209962]
[0.001434468103534713, 0.0013010472068753168, 0.001352384368109764, 0.0013546901422419719, 0.0014701023631330048, 0.0014238237327306854, 0.001338208227285317, 0.001437691691312559, 0.0013446590188434537, 0.0013335002871344285, 0.0013551430833735028, 0.0014646675114576913, 0.0014610731860204619, 0.0013545886755977966, 0.0013725573092470973, 0.0014434557322564783, 0.001399513794945515, 0.0015190420007067068, 0.001407322815942521, 0.0013715077942351298, 0.0015416486540391129, 0.001391484364106947, 0.0014069137752664332, 0.0014276736491948975, 0.0014211486131713098, 0.0014018935522026553, 0.0014040807727724314, 0.001451480164363676, 0.0015274101621186246, 0.0014164676528652103, 0.0014637115515996606, 0.001497983756684223, 0.001310950511002115, 0.0013432806329231482, 0.0013450198778312426, 0.00135365126141328, 0.0013401052078270183, 0.0013791102611896942, 0.0013527273228962201, 0.0014545017121625797, 0.0013580040635579095, 0.003415992079606774, 0.0013534244891180067, 0.0013631637759354649, 0.0014584057356173896, 0.0013286792245522446, 0.001353281246004056, 0.0013941195698416963, 0.001400033615491524, 0.0014881854306678384, 0.0014268131451491192, 0.0014208215716466003, 0.003204250328090726, 0.0013895474697406195, 0.0014026320815010338, 0.001434740063981438, 0.0014282476527578368, 0.0015270032074150381, 0.0014027902447827617, 0.0014028359793734793, 0.0014150173697924735, 0.001447246714057971, 0.0014341443263906606, 0.001444481655347104, 0.001443245752277423, 0.0014587846508591759]
[697.1225066182176, 768.6116189447634, 739.4347521168899, 738.1761842195366, 680.224741540346, 702.3341281733985, 747.2678613167664, 695.5594207316016, 743.6829605025844, 749.9061002445749, 737.9294572426942, 682.7488096631316, 684.4284116415201, 738.2314779493396, 728.5670283221472, 692.7818967033736, 714.533864268863, 658.3096448516684, 710.5690241583085, 729.1245475988604, 648.6562274614286, 718.6570153390109, 710.7756122514514, 700.4401885290284, 703.656176934577, 713.320921141837, 712.2097384934966, 688.9518882529107, 654.7029899375098, 705.9815294597194, 683.1947175023114, 667.5639809429531, 762.8053016551949, 744.4460788687721, 743.4834358079787, 738.7427090755634, 746.2100692985896, 725.105184220257, 739.247284411301, 687.5206757324346, 736.3748215745725, 292.7407255918208, 738.8664887035369, 733.5875686058005, 685.6802435549037, 752.6271063183011, 738.9446967899553, 717.2985887527216, 714.2685639365308, 671.9592729457376, 700.8626205889686, 703.8181429361977, 312.0854794749624, 719.6587534981194, 712.945335550749, 696.9903643904499, 700.1586861137679, 654.8774718638825, 712.8649516342065, 712.841711150444, 706.7051057801926, 690.9671932825297, 697.2798912901045, 692.2898579557963, 692.8826905757478, 685.5021400252828]
Elapsed: 0.0718418562918845~0.016215299805824042
Time per graph: 0.0014661603324874388~0.0003309244858331437
Speed: 698.6408278031847~75.0869128768714
Total Time: 0.0719
best val loss: 0.08346817642450333 test_score: 0.8980

Testing...
Test loss: 0.5257 score: 0.8163 time: 0.07s
test Score 0.8163
Epoch Time List: [0.582448846893385, 0.24827579292468727, 0.2544785728678107, 0.2573301689699292, 0.26222014147788286, 0.25516734970733523, 0.25364446197636425, 0.2589114988222718, 0.2580662239342928, 0.2553634038195014, 0.2605855534784496, 0.25912645692005754, 0.2636082940734923, 0.25551903806626797, 0.2584085329435766, 0.2701052550692111, 0.2739567949902266, 0.28677604300901294, 0.2833844148553908, 0.2801041412167251, 0.29415815719403327, 0.2816388832870871, 0.2832590139005333, 0.28675498301163316, 0.28504492272622883, 0.35087940213270485, 0.2651636532973498, 0.2695063357241452, 0.2695294751320034, 0.27075346116907895, 0.26558763487264514, 0.3594349001068622, 0.2583077410236001, 0.2540302600245923, 0.2636483199894428, 0.2594525336753577, 0.34502996294759214, 0.2676971131004393, 0.25526751577854156, 0.2612034201156348, 0.262584769167006, 0.3687146620359272, 0.26139028323814273, 0.26185751101002097, 0.2739704982377589, 0.26126042287796736, 0.260204351041466, 0.3234363498631865, 0.2675503119826317, 0.26990691595710814, 0.26860713004134595, 0.26857447694055736, 0.3559137340635061, 0.2616505769547075, 0.26765377586707473, 0.27059708698652685, 0.27125046495348215, 0.27139799599535763, 0.35970020899549127, 0.263164825970307, 0.2649897560477257, 0.2722318859305233, 0.2868110209237784, 0.3061440088786185, 0.2695090030319989, 0.28638756182044744]
Total Epoch List: [66]
Total Time List: [0.07191835390403867]
========================k_idx:1========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: SAMamba
GraphEncoder(
  (subgraph_conv): GATConv(128, 128, heads=1)
  (embedding): Linear(in_features=14887, out_features=128, bias=False)
  (encoder): GraphMambaEncoder(
    (layers): ModuleList(
      (0-1): 2 x MambaEncoderLayer(
        (self_selen): Selection(
          (khop_structure_extractor): KHopStructureExtractor(
            (structure_extractor): StructureExtractor(
              (gcn): ModuleList(
                (0): GATConv(128, 128, heads=1)
              )
              (relu): ReLU()
              (bn): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (out_proj): Linear(in_features=128, out_features=128, bias=True)
            )
            (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (out_proj): Linear(in_features=256, out_features=128, bias=True)
          )
          (out_proj): Mamba(
            (in_proj): Linear(in_features=128, out_features=512, bias=False)
            (conv1d): Conv1d(256, 256, kernel_size=(4,), stride=(1,), padding=(3,), groups=256)
            (act): SiLU()
            (x_proj): Linear(in_features=256, out_features=40, bias=False)
            (dt_proj): Linear(in_features=8, out_features=256, bias=True)
            (out_proj): Linear(in_features=256, out_features=128, bias=False)
          )
        )
        (norm1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (linear1): Linear(in_features=128, out_features=256, bias=True)
        (dropout): Dropout(p=0.6, inplace=False)
        (linear2): Linear(in_features=256, out_features=128, bias=True)
        (dropout1): Dropout(p=0.6, inplace=False)
        (dropout2): Dropout(p=0.6, inplace=False)
        (activation): ReLU()
      )
    )
  )
  (classifier): Sequential(
    (0): Linear(in_features=128, out_features=128, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=128, out_features=2, bias=True)
  )
)
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7c7a550cd750>
Training...
Epoch 1/1000, LR 0.000500
Train loss: 0.7239;  Loss pred: 0.7120; Loss self: 1.1827; time: 0.16s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6936 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6930 score: 0.5102 time: 0.08s
Epoch 2/1000, LR 0.000000
Train loss: 0.7243;  Loss pred: 0.7127; Loss self: 1.1618; time: 0.15s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6933 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6929 score: 0.5102 time: 0.08s
Epoch 3/1000, LR 0.000050
Train loss: 0.7049;  Loss pred: 0.6932; Loss self: 1.1673; time: 0.14s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6930 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6927 score: 0.5102 time: 0.08s
Epoch 4/1000, LR 0.000100
Train loss: 0.6713;  Loss pred: 0.6595; Loss self: 1.1775; time: 0.13s
Val loss: 0.6929 score: 0.5102 time: 0.07s
Test loss: 0.6926 score: 0.5306 time: 0.08s
Epoch 5/1000, LR 0.000150
Train loss: 0.6374;  Loss pred: 0.6262; Loss self: 1.1156; time: 0.13s
Val loss: 0.6927 score: 0.5102 time: 0.07s
Test loss: 0.6923 score: 0.5306 time: 0.08s
Epoch 6/1000, LR 0.000200
Train loss: 0.5949;  Loss pred: 0.5836; Loss self: 1.1297; time: 0.14s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6924 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6918 score: 0.5102 time: 0.08s
Epoch 7/1000, LR 0.000250
Train loss: 0.5538;  Loss pred: 0.5425; Loss self: 1.1328; time: 0.13s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6921 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6910 score: 0.5102 time: 0.17s
Epoch 8/1000, LR 0.000300
Train loss: 0.4865;  Loss pred: 0.4743; Loss self: 1.2156; time: 0.15s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6916 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6900 score: 0.5102 time: 0.08s
Epoch 9/1000, LR 0.000350
Train loss: 0.4250;  Loss pred: 0.4121; Loss self: 1.2933; time: 0.15s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6908 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6886 score: 0.5102 time: 0.08s
Epoch 10/1000, LR 0.000400
Train loss: 0.3769;  Loss pred: 0.3624; Loss self: 1.4443; time: 0.15s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6897 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6866 score: 0.5102 time: 0.08s
Epoch 11/1000, LR 0.000450
Train loss: 0.3195;  Loss pred: 0.3031; Loss self: 1.6428; time: 0.16s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6879 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6839 score: 0.5102 time: 0.08s
Epoch 12/1000, LR 0.000450
Train loss: 0.2775;  Loss pred: 0.2603; Loss self: 1.7227; time: 0.13s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6851 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6797 score: 0.5102 time: 0.08s
Epoch 13/1000, LR 0.000450
Train loss: 0.2434;  Loss pred: 0.2253; Loss self: 1.8149; time: 0.13s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6808 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6737 score: 0.5102 time: 0.08s
Epoch 14/1000, LR 0.000450
Train loss: 0.2134;  Loss pred: 0.1942; Loss self: 1.9207; time: 0.13s
Val loss: 0.6743 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6650 score: 0.5102 time: 0.08s
Epoch 15/1000, LR 0.000450
Train loss: 0.1952;  Loss pred: 0.1752; Loss self: 2.0096; time: 0.13s
Val loss: 0.6652 score: 0.5102 time: 0.07s
Test loss: 0.6530 score: 0.5714 time: 0.08s
Epoch 16/1000, LR 0.000450
Train loss: 0.1745;  Loss pred: 0.1535; Loss self: 2.1014; time: 0.14s
Val loss: 0.6520 score: 0.6327 time: 0.07s
Test loss: 0.6358 score: 0.7959 time: 0.08s
Epoch 17/1000, LR 0.000450
Train loss: 0.1552;  Loss pred: 0.1337; Loss self: 2.1531; time: 0.13s
Val loss: 0.6330 score: 0.7347 time: 0.07s
Test loss: 0.6114 score: 0.8776 time: 0.08s
Epoch 18/1000, LR 0.000450
Train loss: 0.1390;  Loss pred: 0.1168; Loss self: 2.2147; time: 0.13s
Val loss: 0.6081 score: 0.7551 time: 0.07s
Test loss: 0.5791 score: 0.9388 time: 0.08s
Epoch 19/1000, LR 0.000450
Train loss: 0.1249;  Loss pred: 0.1022; Loss self: 2.2662; time: 0.13s
Val loss: 0.5770 score: 0.7755 time: 0.07s
Test loss: 0.5378 score: 1.0000 time: 0.08s
Epoch 20/1000, LR 0.000450
Train loss: 0.1118;  Loss pred: 0.0887; Loss self: 2.3072; time: 0.13s
Val loss: 0.5408 score: 0.8163 time: 0.07s
Test loss: 0.4866 score: 1.0000 time: 0.08s
Epoch 21/1000, LR 0.000450
Train loss: 0.0996;  Loss pred: 0.0760; Loss self: 2.3593; time: 0.13s
Val loss: 0.5025 score: 0.8163 time: 0.07s
Test loss: 0.4285 score: 1.0000 time: 0.08s
Epoch 22/1000, LR 0.000450
Train loss: 0.0887;  Loss pred: 0.0647; Loss self: 2.4047; time: 0.13s
Val loss: 0.4661 score: 0.8163 time: 0.07s
Test loss: 0.3671 score: 1.0000 time: 0.08s
Epoch 23/1000, LR 0.000450
Train loss: 0.0797;  Loss pred: 0.0552; Loss self: 2.4504; time: 0.13s
Val loss: 0.4344 score: 0.8163 time: 0.07s
Test loss: 0.3057 score: 1.0000 time: 0.08s
Epoch 24/1000, LR 0.000450
Train loss: 0.0701;  Loss pred: 0.0454; Loss self: 2.4684; time: 0.13s
Val loss: 0.4139 score: 0.8163 time: 0.07s
Test loss: 0.2482 score: 1.0000 time: 0.08s
Epoch 25/1000, LR 0.000450
Train loss: 0.0630;  Loss pred: 0.0377; Loss self: 2.5325; time: 0.14s
Val loss: 0.4062 score: 0.8163 time: 0.07s
Test loss: 0.1982 score: 1.0000 time: 0.08s
Epoch 26/1000, LR 0.000450
Train loss: 0.0569;  Loss pred: 0.0314; Loss self: 2.5530; time: 0.14s
Val loss: 0.4121 score: 0.8163 time: 0.07s
Test loss: 0.1576 score: 1.0000 time: 0.08s
     INFO: Early stopping counter 1 of 20
Epoch 27/1000, LR 0.000450
Train loss: 0.0515;  Loss pred: 0.0254; Loss self: 2.6092; time: 0.13s
Val loss: 0.4306 score: 0.8163 time: 0.07s
Test loss: 0.1251 score: 1.0000 time: 0.08s
     INFO: Early stopping counter 2 of 20
Epoch 28/1000, LR 0.000450
Train loss: 0.0469;  Loss pred: 0.0206; Loss self: 2.6321; time: 0.14s
Val loss: 0.4587 score: 0.8163 time: 0.07s
Test loss: 0.1000 score: 1.0000 time: 0.08s
     INFO: Early stopping counter 3 of 20
Epoch 29/1000, LR 0.000450
Train loss: 0.0439;  Loss pred: 0.0173; Loss self: 2.6639; time: 0.14s
Val loss: 0.4881 score: 0.8163 time: 0.07s
Test loss: 0.0805 score: 1.0000 time: 0.08s
     INFO: Early stopping counter 4 of 20
Epoch 30/1000, LR 0.000450
Train loss: 0.0411;  Loss pred: 0.0143; Loss self: 2.6847; time: 0.13s
Val loss: 0.5251 score: 0.8163 time: 0.07s
Test loss: 0.0666 score: 1.0000 time: 0.08s
     INFO: Early stopping counter 5 of 20
Epoch 31/1000, LR 0.000450
Train loss: 0.0389;  Loss pred: 0.0118; Loss self: 2.7146; time: 0.14s
Val loss: 0.5607 score: 0.8163 time: 0.07s
Test loss: 0.0557 score: 1.0000 time: 0.08s
     INFO: Early stopping counter 6 of 20
Epoch 32/1000, LR 0.000450
Train loss: 0.0374;  Loss pred: 0.0100; Loss self: 2.7441; time: 0.14s
Val loss: 0.5968 score: 0.8163 time: 0.07s
Test loss: 0.0478 score: 1.0000 time: 0.08s
     INFO: Early stopping counter 7 of 20
Epoch 33/1000, LR 0.000449
Train loss: 0.0361;  Loss pred: 0.0087; Loss self: 2.7390; time: 0.13s
Val loss: 0.6314 score: 0.8163 time: 0.07s
Test loss: 0.0420 score: 1.0000 time: 0.08s
     INFO: Early stopping counter 8 of 20
Epoch 34/1000, LR 0.000449
Train loss: 0.0347;  Loss pred: 0.0072; Loss self: 2.7506; time: 0.13s
Val loss: 0.6658 score: 0.8163 time: 0.07s
Test loss: 0.0375 score: 1.0000 time: 0.08s
     INFO: Early stopping counter 9 of 20
Epoch 35/1000, LR 0.000449
Train loss: 0.0337;  Loss pred: 0.0061; Loss self: 2.7601; time: 0.13s
Val loss: 0.6999 score: 0.8163 time: 0.07s
Test loss: 0.0344 score: 1.0000 time: 0.08s
     INFO: Early stopping counter 10 of 20
Epoch 36/1000, LR 0.000449
Train loss: 0.0332;  Loss pred: 0.0053; Loss self: 2.7890; time: 0.13s
Val loss: 0.7301 score: 0.8163 time: 0.07s
Test loss: 0.0325 score: 1.0000 time: 0.08s
     INFO: Early stopping counter 11 of 20
Epoch 37/1000, LR 0.000449
Train loss: 0.0326;  Loss pred: 0.0047; Loss self: 2.7887; time: 0.13s
Val loss: 0.7581 score: 0.8163 time: 0.07s
Test loss: 0.0313 score: 1.0000 time: 0.08s
     INFO: Early stopping counter 12 of 20
Epoch 38/1000, LR 0.000449
Train loss: 0.0318;  Loss pred: 0.0042; Loss self: 2.7673; time: 0.13s
Val loss: 0.7838 score: 0.8163 time: 0.07s
Test loss: 0.0304 score: 1.0000 time: 0.08s
     INFO: Early stopping counter 13 of 20
Epoch 39/1000, LR 0.000449
Train loss: 0.0312;  Loss pred: 0.0035; Loss self: 2.7698; time: 0.13s
Val loss: 0.8081 score: 0.8163 time: 0.07s
Test loss: 0.0301 score: 1.0000 time: 0.08s
     INFO: Early stopping counter 14 of 20
Epoch 40/1000, LR 0.000449
Train loss: 0.0309;  Loss pred: 0.0033; Loss self: 2.7670; time: 0.13s
Val loss: 0.8326 score: 0.8163 time: 0.07s
Test loss: 0.0303 score: 1.0000 time: 0.08s
     INFO: Early stopping counter 15 of 20
Epoch 41/1000, LR 0.000449
Train loss: 0.0307;  Loss pred: 0.0030; Loss self: 2.7691; time: 0.15s
Val loss: 0.8565 score: 0.8163 time: 0.07s
Test loss: 0.0309 score: 1.0000 time: 0.08s
     INFO: Early stopping counter 16 of 20
Epoch 42/1000, LR 0.000449
Train loss: 0.0301;  Loss pred: 0.0027; Loss self: 2.7432; time: 0.14s
Val loss: 0.8768 score: 0.8163 time: 0.07s
Test loss: 0.0316 score: 1.0000 time: 0.08s
     INFO: Early stopping counter 17 of 20
Epoch 43/1000, LR 0.000449
Train loss: 0.0302;  Loss pred: 0.0026; Loss self: 2.7583; time: 0.14s
Val loss: 0.8939 score: 0.8163 time: 0.07s
Test loss: 0.0325 score: 1.0000 time: 0.08s
     INFO: Early stopping counter 18 of 20
Epoch 44/1000, LR 0.000449
Train loss: 0.0297;  Loss pred: 0.0022; Loss self: 2.7568; time: 0.14s
Val loss: 0.9099 score: 0.8163 time: 0.07s
Test loss: 0.0333 score: 1.0000 time: 0.08s
     INFO: Early stopping counter 19 of 20
Epoch 45/1000, LR 0.000449
Train loss: 0.0295;  Loss pred: 0.0022; Loss self: 2.7351; time: 0.14s
Val loss: 0.9258 score: 0.8163 time: 0.07s
Test loss: 0.0345 score: 0.9796 time: 0.08s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 024,   Train_Loss: 0.0630,   Val_Loss: 0.4062,   Val_Precision: 1.0000,   Val_Recall: 0.6400,   Val_accuracy: 0.7805,   Val_Score: 0.8163,   Val_Loss: 0.4062,   Test_Precision: 1.0000,   Test_Recall: 1.0000,   Test_accuracy: 1.0000,   Test_Score: 1.0000,   Test_loss: 0.1982


[0.07028893707320094, 0.06375131313689053, 0.06626683403737843, 0.06637981696985662, 0.07203501579351723, 0.06976736290380359, 0.06557220313698053, 0.07044689287431538, 0.06588829192332923, 0.06534151406958699, 0.06640201108530164, 0.07176870806142688, 0.07159258611500263, 0.06637484510429204, 0.06725530815310776, 0.07072933088056743, 0.06857617595233023, 0.07443305803462863, 0.06895881798118353, 0.06720388191752136, 0.07554078404791653, 0.0681827338412404, 0.06893877498805523, 0.06995600881054997, 0.06963628204539418, 0.06869278405793011, 0.06879995786584914, 0.07112252805382013, 0.07484309794381261, 0.06940691499039531, 0.07172186602838337, 0.07340120407752693, 0.06423657503910363, 0.06582075101323426, 0.06590597401373088, 0.06632891180925071, 0.0656651551835239, 0.06757640279829502, 0.06628363882191479, 0.07127058389596641, 0.06654219911433756, 0.16738361190073192, 0.06631779996678233, 0.06679502502083778, 0.07146188104525208, 0.06510528200305998, 0.06631078105419874, 0.06831185892224312, 0.06860164715908468, 0.07292108610272408, 0.06991384411230683, 0.06962025701068342, 0.15700826607644558, 0.06808782601729035, 0.06872897199355066, 0.07030226313509047, 0.069984134985134, 0.07482315716333687, 0.06873672199435532, 0.06873896298930049, 0.0693358511198312, 0.07091508898884058, 0.07027307199314237, 0.0707796011120081, 0.07071904186159372, 0.07148044789209962, 0.08607298508286476, 0.08550435793586075, 0.08510670508258045, 0.0854763900861144, 0.08762010606005788, 0.08591598714701831, 0.17195059289224446, 0.08397224987857044, 0.08165558008477092, 0.08359282906167209, 0.08078921400010586, 0.08309496799483895, 0.08218066906556487, 0.08215719019062817, 0.08200619299896061, 0.08215760416351259, 0.08195297210477293, 0.08234275202266872, 0.0832705341745168, 0.0829338300973177, 0.08286570711061358, 0.08267843001522124, 0.083209996111691, 0.08312629698775709, 0.08316955505870283, 0.08513408107683063, 0.08499840390868485, 0.08362879580818117, 0.0842702619265765, 0.08329250500537455, 0.08339400077238679, 0.08254834986291826, 0.08235553302802145, 0.0825678410474211, 0.08262822008691728, 0.08244814304634929, 0.08242443087510765, 0.08285321784205735, 0.08526852796785533, 0.08642933494411409, 0.08592651784420013, 0.08471894985996187, 0.08507576701231301, 0.08489972213283181, 0.08510020095854998]
[0.001434468103534713, 0.0013010472068753168, 0.001352384368109764, 0.0013546901422419719, 0.0014701023631330048, 0.0014238237327306854, 0.001338208227285317, 0.001437691691312559, 0.0013446590188434537, 0.0013335002871344285, 0.0013551430833735028, 0.0014646675114576913, 0.0014610731860204619, 0.0013545886755977966, 0.0013725573092470973, 0.0014434557322564783, 0.001399513794945515, 0.0015190420007067068, 0.001407322815942521, 0.0013715077942351298, 0.0015416486540391129, 0.001391484364106947, 0.0014069137752664332, 0.0014276736491948975, 0.0014211486131713098, 0.0014018935522026553, 0.0014040807727724314, 0.001451480164363676, 0.0015274101621186246, 0.0014164676528652103, 0.0014637115515996606, 0.001497983756684223, 0.001310950511002115, 0.0013432806329231482, 0.0013450198778312426, 0.00135365126141328, 0.0013401052078270183, 0.0013791102611896942, 0.0013527273228962201, 0.0014545017121625797, 0.0013580040635579095, 0.003415992079606774, 0.0013534244891180067, 0.0013631637759354649, 0.0014584057356173896, 0.0013286792245522446, 0.001353281246004056, 0.0013941195698416963, 0.001400033615491524, 0.0014881854306678384, 0.0014268131451491192, 0.0014208215716466003, 0.003204250328090726, 0.0013895474697406195, 0.0014026320815010338, 0.001434740063981438, 0.0014282476527578368, 0.0015270032074150381, 0.0014027902447827617, 0.0014028359793734793, 0.0014150173697924735, 0.001447246714057971, 0.0014341443263906606, 0.001444481655347104, 0.001443245752277423, 0.0014587846508591759, 0.0017565915323033625, 0.0017449868966502194, 0.00173687153229756, 0.0017444161242064164, 0.0017881654297970996, 0.0017533874927962922, 0.0035091957733111114, 0.0017137193852769478, 0.001666440409893284, 0.0017059761032994305, 0.0016487594693899155, 0.0016958156733640603, 0.0016771565115421402, 0.0016766773508291464, 0.0016735957754889922, 0.001676685799255359, 0.0016725096347912842, 0.0016804643269932391, 0.0016993986566227917, 0.0016925271448432182, 0.0016911368798084405, 0.0016873148982698212, 0.0016981631859528776, 0.0016964550405664711, 0.001697337858340874, 0.0017374302260577679, 0.0017346613042588746, 0.0017067101185343095, 0.0017198012638076836, 0.0016998470409260113, 0.0017019183831099346, 0.001684660201284046, 0.0016807251638371724, 0.0016850579805596142, 0.0016862902058554547, 0.0016826151642112099, 0.0016821312423491357, 0.0016908819967766805, 0.001740174040160313, 0.0017638639784513079, 0.0017536024049836763, 0.0017289581604073851, 0.0017362401431084288, 0.0017326473904659553, 0.0017367387950724485]
[697.1225066182176, 768.6116189447634, 739.4347521168899, 738.1761842195366, 680.224741540346, 702.3341281733985, 747.2678613167664, 695.5594207316016, 743.6829605025844, 749.9061002445749, 737.9294572426942, 682.7488096631316, 684.4284116415201, 738.2314779493396, 728.5670283221472, 692.7818967033736, 714.533864268863, 658.3096448516684, 710.5690241583085, 729.1245475988604, 648.6562274614286, 718.6570153390109, 710.7756122514514, 700.4401885290284, 703.656176934577, 713.320921141837, 712.2097384934966, 688.9518882529107, 654.7029899375098, 705.9815294597194, 683.1947175023114, 667.5639809429531, 762.8053016551949, 744.4460788687721, 743.4834358079787, 738.7427090755634, 746.2100692985896, 725.105184220257, 739.247284411301, 687.5206757324346, 736.3748215745725, 292.7407255918208, 738.8664887035369, 733.5875686058005, 685.6802435549037, 752.6271063183011, 738.9446967899553, 717.2985887527216, 714.2685639365308, 671.9592729457376, 700.8626205889686, 703.8181429361977, 312.0854794749624, 719.6587534981194, 712.945335550749, 696.9903643904499, 700.1586861137679, 654.8774718638825, 712.8649516342065, 712.841711150444, 706.7051057801926, 690.9671932825297, 697.2798912901045, 692.2898579557963, 692.8826905757478, 685.5021400252828, 569.284310899946, 573.0702058105189, 575.7478209555227, 573.2577142136472, 559.2323748890886, 570.3245883231469, 284.9655774708879, 583.5261061940977, 600.0814634974186, 586.1746820872563, 606.5166075255515, 589.6867305255283, 596.2472751457782, 596.4176706421676, 597.5158485972034, 596.4146654335087, 597.9038800125011, 595.0736257455962, 588.4434450403156, 590.8324738228241, 591.3181907033289, 592.6576011540013, 588.8715573814995, 589.4644868785236, 589.1578951626561, 575.5626815984442, 576.4814131409042, 585.9225823649426, 581.4625335173755, 588.2882258954538, 587.5722419618553, 593.5915143230671, 594.9812744618841, 593.45138952898, 593.0177359315802, 594.3129607231301, 594.4839349178703, 591.4073258253945, 574.6551648982623, 566.9371404012746, 570.2546923738444, 578.3829955517116, 575.957193461544, 577.1514766954823, 575.7918247909495]
Elapsed: 0.07744440554666358~0.01648899867211751
Time per graph: 0.0015804980723808895~0.00033651017698198987
Speed: 650.1094390584383~87.44492201405026
Total Time: 0.0858
best val loss: 0.40619438886642456 test_score: 1.0000

Testing...
Test loss: 0.4866 score: 1.0000 time: 0.08s
test Score 1.0000
Epoch Time List: [0.582448846893385, 0.24827579292468727, 0.2544785728678107, 0.2573301689699292, 0.26222014147788286, 0.25516734970733523, 0.25364446197636425, 0.2589114988222718, 0.2580662239342928, 0.2553634038195014, 0.2605855534784496, 0.25912645692005754, 0.2636082940734923, 0.25551903806626797, 0.2584085329435766, 0.2701052550692111, 0.2739567949902266, 0.28677604300901294, 0.2833844148553908, 0.2801041412167251, 0.29415815719403327, 0.2816388832870871, 0.2832590139005333, 0.28675498301163316, 0.28504492272622883, 0.35087940213270485, 0.2651636532973498, 0.2695063357241452, 0.2695294751320034, 0.27075346116907895, 0.26558763487264514, 0.3594349001068622, 0.2583077410236001, 0.2540302600245923, 0.2636483199894428, 0.2594525336753577, 0.34502996294759214, 0.2676971131004393, 0.25526751577854156, 0.2612034201156348, 0.262584769167006, 0.3687146620359272, 0.26139028323814273, 0.26185751101002097, 0.2739704982377589, 0.26126042287796736, 0.260204351041466, 0.3234363498631865, 0.2675503119826317, 0.26990691595710814, 0.26860713004134595, 0.26857447694055736, 0.3559137340635061, 0.2616505769547075, 0.26765377586707473, 0.27059708698652685, 0.27125046495348215, 0.27139799599535763, 0.35970020899549127, 0.263164825970307, 0.2649897560477257, 0.2722318859305233, 0.2868110209237784, 0.3061440088786185, 0.2695090030319989, 0.28638756182044744, 0.31293426896445453, 0.31347174500115216, 0.29087948496453464, 0.2830333039164543, 0.2851865249685943, 0.28917236416600645, 0.3725859336555004, 0.2956863809376955, 0.2950315703637898, 0.29681343282572925, 0.30017270683310926, 0.2799987809266895, 0.27952251001261175, 0.28003472718410194, 0.27965890103951097, 0.280824659159407, 0.2796506038866937, 0.280311182141304, 0.28107091411948204, 0.28095692582428455, 0.2813631270546466, 0.2811830169521272, 0.2806749439332634, 0.28190638800151646, 0.2822571557480842, 0.2838162670377642, 0.28362959693185985, 0.29465879895724356, 0.28271584236063063, 0.2815387058071792, 0.2832195011433214, 0.2818011720664799, 0.2780900509096682, 0.2770794138778001, 0.2779774477239698, 0.2771994329523295, 0.2774483801331371, 0.27791416994296014, 0.28112308494746685, 0.2887979759834707, 0.29697841708548367, 0.2918874812312424, 0.2917783809825778, 0.2921966859139502, 0.29343159100972116]
Total Epoch List: [66, 45]
Total Time List: [0.07191835390403867, 0.08575359615497291]
========================k_idx:2========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: SAMamba
GraphEncoder(
  (subgraph_conv): GATConv(128, 128, heads=1)
  (embedding): Linear(in_features=14887, out_features=128, bias=False)
  (encoder): GraphMambaEncoder(
    (layers): ModuleList(
      (0-1): 2 x MambaEncoderLayer(
        (self_selen): Selection(
          (khop_structure_extractor): KHopStructureExtractor(
            (structure_extractor): StructureExtractor(
              (gcn): ModuleList(
                (0): GATConv(128, 128, heads=1)
              )
              (relu): ReLU()
              (bn): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (out_proj): Linear(in_features=128, out_features=128, bias=True)
            )
            (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (out_proj): Linear(in_features=256, out_features=128, bias=True)
          )
          (out_proj): Mamba(
            (in_proj): Linear(in_features=128, out_features=512, bias=False)
            (conv1d): Conv1d(256, 256, kernel_size=(4,), stride=(1,), padding=(3,), groups=256)
            (act): SiLU()
            (x_proj): Linear(in_features=256, out_features=40, bias=False)
            (dt_proj): Linear(in_features=8, out_features=256, bias=True)
            (out_proj): Linear(in_features=256, out_features=128, bias=False)
          )
        )
        (norm1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (linear1): Linear(in_features=128, out_features=256, bias=True)
        (dropout): Dropout(p=0.6, inplace=False)
        (linear2): Linear(in_features=256, out_features=128, bias=True)
        (dropout1): Dropout(p=0.6, inplace=False)
        (dropout2): Dropout(p=0.6, inplace=False)
        (activation): ReLU()
      )
    )
  )
  (classifier): Sequential(
    (0): Linear(in_features=128, out_features=128, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=128, out_features=2, bias=True)
  )
)
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7c7a550cd600>
Training...
Epoch 1/1000, LR 0.000500
Train loss: 0.7094;  Loss pred: 0.6961; Loss self: 1.3302; time: 0.14s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6981 score: 0.4898 time: 0.06s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6968 score: 0.5000 time: 0.06s
Epoch 2/1000, LR 0.000000
Train loss: 0.7138;  Loss pred: 0.6997; Loss self: 1.4042; time: 0.14s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6963 score: 0.4898 time: 0.06s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6953 score: 0.5000 time: 0.06s
Epoch 3/1000, LR 0.000050
Train loss: 0.6865;  Loss pred: 0.6737; Loss self: 1.2771; time: 0.14s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6956 score: 0.4898 time: 0.06s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6948 score: 0.5000 time: 0.06s
Epoch 4/1000, LR 0.000100
Train loss: 0.6489;  Loss pred: 0.6359; Loss self: 1.2998; time: 0.14s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6954 score: 0.4898 time: 0.06s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6945 score: 0.5000 time: 0.06s
Epoch 5/1000, LR 0.000150
Train loss: 0.6090;  Loss pred: 0.5966; Loss self: 1.2411; time: 0.14s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6955 score: 0.4898 time: 0.06s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6945 score: 0.5000 time: 0.06s
     INFO: Early stopping counter 1 of 20
Epoch 6/1000, LR 0.000200
Train loss: 0.5613;  Loss pred: 0.5482; Loss self: 1.3075; time: 0.14s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6955 score: 0.4898 time: 0.06s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6944 score: 0.5000 time: 0.06s
     INFO: Early stopping counter 2 of 20
Epoch 7/1000, LR 0.000250
Train loss: 0.4927;  Loss pred: 0.4796; Loss self: 1.3048; time: 0.14s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6955 score: 0.4898 time: 0.06s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6943 score: 0.5000 time: 0.06s
     INFO: Early stopping counter 3 of 20
Epoch 8/1000, LR 0.000300
Train loss: 0.4565;  Loss pred: 0.4428; Loss self: 1.3697; time: 0.14s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6957 score: 0.4898 time: 0.06s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6943 score: 0.5000 time: 0.06s
     INFO: Early stopping counter 4 of 20
Epoch 9/1000, LR 0.000350
Train loss: 0.3891;  Loss pred: 0.3749; Loss self: 1.4242; time: 0.14s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6956 score: 0.4898 time: 0.06s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6939 score: 0.5000 time: 0.06s
     INFO: Early stopping counter 5 of 20
Epoch 10/1000, LR 0.000400
Train loss: 0.3294;  Loss pred: 0.3140; Loss self: 1.5386; time: 0.14s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6953 score: 0.4898 time: 0.06s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6932 score: 0.5000 time: 0.06s
Epoch 11/1000, LR 0.000450
Train loss: 0.2799;  Loss pred: 0.2636; Loss self: 1.6325; time: 0.14s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6943 score: 0.4898 time: 0.06s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6919 score: 0.5000 time: 0.07s
Epoch 12/1000, LR 0.000450
Train loss: 0.2394;  Loss pred: 0.2227; Loss self: 1.6698; time: 0.14s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6926 score: 0.4898 time: 0.06s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6897 score: 0.5000 time: 0.06s
Epoch 13/1000, LR 0.000450
Train loss: 0.2112;  Loss pred: 0.1932; Loss self: 1.8065; time: 0.14s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6893 score: 0.4898 time: 0.06s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6857 score: 0.5000 time: 0.06s
Epoch 14/1000, LR 0.000450
Train loss: 0.1866;  Loss pred: 0.1671; Loss self: 1.9490; time: 0.14s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6836 score: 0.4898 time: 0.06s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6788 score: 0.5000 time: 0.07s
Epoch 15/1000, LR 0.000450
Train loss: 0.1620;  Loss pred: 0.1419; Loss self: 2.0142; time: 0.14s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6749 score: 0.4898 time: 0.06s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6681 score: 0.5000 time: 0.06s
Epoch 16/1000, LR 0.000450
Train loss: 0.1443;  Loss pred: 0.1233; Loss self: 2.0977; time: 0.14s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6616 score: 0.4898 time: 0.06s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6520 score: 0.5000 time: 0.06s
Epoch 17/1000, LR 0.000450
Train loss: 0.1271;  Loss pred: 0.1052; Loss self: 2.1881; time: 0.14s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6435 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6296 score: 0.5000 time: 0.07s
Epoch 18/1000, LR 0.000450
Train loss: 0.1115;  Loss pred: 0.0894; Loss self: 2.2115; time: 0.14s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6199 score: 0.4898 time: 0.06s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6004 score: 0.5000 time: 0.07s
Epoch 19/1000, LR 0.000450
Train loss: 0.0990;  Loss pred: 0.0761; Loss self: 2.2941; time: 0.14s
Val loss: 0.5919 score: 0.5306 time: 0.07s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.5651 score: 0.5000 time: 0.07s
Epoch 20/1000, LR 0.000450
Train loss: 0.0886;  Loss pred: 0.0653; Loss self: 2.3313; time: 0.14s
Val loss: 0.5590 score: 0.6939 time: 0.06s
Test loss: 0.5220 score: 0.6667 time: 0.07s
Epoch 21/1000, LR 0.000450
Train loss: 0.0805;  Loss pred: 0.0567; Loss self: 2.3804; time: 0.14s
Val loss: 0.5246 score: 0.7143 time: 0.06s
Test loss: 0.4746 score: 0.7500 time: 0.07s
Epoch 22/1000, LR 0.000450
Train loss: 0.0718;  Loss pred: 0.0475; Loss self: 2.4273; time: 0.14s
Val loss: 0.4919 score: 0.7551 time: 0.07s
Test loss: 0.4255 score: 0.8542 time: 0.07s
Epoch 23/1000, LR 0.000450
Train loss: 0.0655;  Loss pred: 0.0408; Loss self: 2.4764; time: 0.14s
Val loss: 0.4666 score: 0.7959 time: 0.06s
Test loss: 0.3805 score: 0.8750 time: 0.06s
Epoch 24/1000, LR 0.000450
Train loss: 0.0578;  Loss pred: 0.0332; Loss self: 2.4619; time: 0.14s
Val loss: 0.4511 score: 0.7959 time: 0.07s
Test loss: 0.3423 score: 0.8958 time: 0.07s
Epoch 25/1000, LR 0.000450
Train loss: 0.0537;  Loss pred: 0.0285; Loss self: 2.5225; time: 0.14s
Val loss: 0.4463 score: 0.8571 time: 0.06s
Test loss: 0.3121 score: 0.8958 time: 0.07s
Epoch 26/1000, LR 0.000450
Train loss: 0.0488;  Loss pred: 0.0232; Loss self: 2.5565; time: 0.14s
Val loss: 0.4511 score: 0.8571 time: 0.06s
Test loss: 0.2900 score: 0.8958 time: 0.07s
     INFO: Early stopping counter 1 of 20
Epoch 27/1000, LR 0.000450
Train loss: 0.0447;  Loss pred: 0.0193; Loss self: 2.5413; time: 0.14s
Val loss: 0.4670 score: 0.8571 time: 0.06s
Test loss: 0.2775 score: 0.8958 time: 0.07s
     INFO: Early stopping counter 2 of 20
Epoch 28/1000, LR 0.000450
Train loss: 0.0423;  Loss pred: 0.0165; Loss self: 2.5761; time: 0.16s
Val loss: 0.4911 score: 0.8571 time: 0.07s
Test loss: 0.2724 score: 0.8958 time: 0.07s
     INFO: Early stopping counter 3 of 20
Epoch 29/1000, LR 0.000450
Train loss: 0.0396;  Loss pred: 0.0137; Loss self: 2.5902; time: 0.16s
Val loss: 0.5167 score: 0.8571 time: 0.07s
Test loss: 0.2707 score: 0.8958 time: 0.07s
     INFO: Early stopping counter 4 of 20
Epoch 30/1000, LR 0.000450
Train loss: 0.0380;  Loss pred: 0.0115; Loss self: 2.6449; time: 0.15s
Val loss: 0.5444 score: 0.8776 time: 0.07s
Test loss: 0.2729 score: 0.8958 time: 0.07s
     INFO: Early stopping counter 5 of 20
Epoch 31/1000, LR 0.000450
Train loss: 0.0361;  Loss pred: 0.0096; Loss self: 2.6532; time: 0.14s
Val loss: 0.5745 score: 0.8776 time: 0.07s
Test loss: 0.2790 score: 0.8958 time: 0.07s
     INFO: Early stopping counter 6 of 20
Epoch 32/1000, LR 0.000450
Train loss: 0.0346;  Loss pred: 0.0081; Loss self: 2.6496; time: 0.14s
Val loss: 0.5980 score: 0.8776 time: 0.06s
Test loss: 0.2839 score: 0.8958 time: 0.06s
     INFO: Early stopping counter 7 of 20
Epoch 33/1000, LR 0.000449
Train loss: 0.0336;  Loss pred: 0.0069; Loss self: 2.6667; time: 0.14s
Val loss: 0.6207 score: 0.8776 time: 0.06s
Test loss: 0.2910 score: 0.8958 time: 0.07s
     INFO: Early stopping counter 8 of 20
Epoch 34/1000, LR 0.000449
Train loss: 0.0327;  Loss pred: 0.0059; Loss self: 2.6774; time: 0.15s
Val loss: 0.6424 score: 0.8776 time: 0.06s
Test loss: 0.2994 score: 0.8958 time: 0.06s
     INFO: Early stopping counter 9 of 20
Epoch 35/1000, LR 0.000449
Train loss: 0.0317;  Loss pred: 0.0051; Loss self: 2.6687; time: 0.14s
Val loss: 0.6551 score: 0.8776 time: 0.06s
Test loss: 0.3052 score: 0.8958 time: 0.07s
     INFO: Early stopping counter 10 of 20
Epoch 36/1000, LR 0.000449
Train loss: 0.0311;  Loss pred: 0.0043; Loss self: 2.6750; time: 0.15s
Val loss: 0.6695 score: 0.8776 time: 0.06s
Test loss: 0.3134 score: 0.8958 time: 0.07s
     INFO: Early stopping counter 11 of 20
Epoch 37/1000, LR 0.000449
Train loss: 0.0307;  Loss pred: 0.0039; Loss self: 2.6787; time: 0.14s
Val loss: 0.6816 score: 0.8776 time: 0.06s
Test loss: 0.3216 score: 0.8958 time: 0.06s
     INFO: Early stopping counter 12 of 20
Epoch 38/1000, LR 0.000449
Train loss: 0.0302;  Loss pred: 0.0033; Loss self: 2.6831; time: 0.14s
Val loss: 0.6903 score: 0.8776 time: 0.06s
Test loss: 0.3281 score: 0.8958 time: 0.07s
     INFO: Early stopping counter 13 of 20
Epoch 39/1000, LR 0.000449
Train loss: 0.0297;  Loss pred: 0.0030; Loss self: 2.6738; time: 0.14s
Val loss: 0.6971 score: 0.8776 time: 0.06s
Test loss: 0.3342 score: 0.8958 time: 0.07s
     INFO: Early stopping counter 14 of 20
Epoch 40/1000, LR 0.000449
Train loss: 0.0292;  Loss pred: 0.0027; Loss self: 2.6543; time: 0.14s
Val loss: 0.7055 score: 0.8776 time: 0.06s
Test loss: 0.3412 score: 0.8958 time: 0.07s
     INFO: Early stopping counter 15 of 20
Epoch 41/1000, LR 0.000449
Train loss: 0.0290;  Loss pred: 0.0023; Loss self: 2.6660; time: 0.14s
Val loss: 0.7112 score: 0.8776 time: 0.06s
Test loss: 0.3467 score: 0.8958 time: 0.07s
     INFO: Early stopping counter 16 of 20
Epoch 42/1000, LR 0.000449
Train loss: 0.0286;  Loss pred: 0.0022; Loss self: 2.6434; time: 0.14s
Val loss: 0.7185 score: 0.8776 time: 0.06s
Test loss: 0.3530 score: 0.8958 time: 0.06s
     INFO: Early stopping counter 17 of 20
Epoch 43/1000, LR 0.000449
Train loss: 0.0284;  Loss pred: 0.0020; Loss self: 2.6325; time: 0.14s
Val loss: 0.7284 score: 0.8776 time: 0.06s
Test loss: 0.3608 score: 0.8958 time: 0.06s
     INFO: Early stopping counter 18 of 20
Epoch 44/1000, LR 0.000449
Train loss: 0.0284;  Loss pred: 0.0019; Loss self: 2.6491; time: 0.14s
Val loss: 0.7349 score: 0.8776 time: 0.06s
Test loss: 0.3666 score: 0.8958 time: 0.06s
     INFO: Early stopping counter 19 of 20
Epoch 45/1000, LR 0.000449
Train loss: 0.0280;  Loss pred: 0.0018; Loss self: 2.6222; time: 0.14s
Val loss: 0.7443 score: 0.8776 time: 0.06s
Test loss: 0.3737 score: 0.8958 time: 0.06s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 024,   Train_Loss: 0.0537,   Val_Loss: 0.4463,   Val_Precision: 1.0000,   Val_Recall: 0.7200,   Val_accuracy: 0.8372,   Val_Score: 0.8571,   Val_Loss: 0.4463,   Test_Precision: 1.0000,   Test_Recall: 0.7917,   Test_accuracy: 0.8837,   Test_Score: 0.8958,   Test_loss: 0.3121


[0.07028893707320094, 0.06375131313689053, 0.06626683403737843, 0.06637981696985662, 0.07203501579351723, 0.06976736290380359, 0.06557220313698053, 0.07044689287431538, 0.06588829192332923, 0.06534151406958699, 0.06640201108530164, 0.07176870806142688, 0.07159258611500263, 0.06637484510429204, 0.06725530815310776, 0.07072933088056743, 0.06857617595233023, 0.07443305803462863, 0.06895881798118353, 0.06720388191752136, 0.07554078404791653, 0.0681827338412404, 0.06893877498805523, 0.06995600881054997, 0.06963628204539418, 0.06869278405793011, 0.06879995786584914, 0.07112252805382013, 0.07484309794381261, 0.06940691499039531, 0.07172186602838337, 0.07340120407752693, 0.06423657503910363, 0.06582075101323426, 0.06590597401373088, 0.06632891180925071, 0.0656651551835239, 0.06757640279829502, 0.06628363882191479, 0.07127058389596641, 0.06654219911433756, 0.16738361190073192, 0.06631779996678233, 0.06679502502083778, 0.07146188104525208, 0.06510528200305998, 0.06631078105419874, 0.06831185892224312, 0.06860164715908468, 0.07292108610272408, 0.06991384411230683, 0.06962025701068342, 0.15700826607644558, 0.06808782601729035, 0.06872897199355066, 0.07030226313509047, 0.069984134985134, 0.07482315716333687, 0.06873672199435532, 0.06873896298930049, 0.0693358511198312, 0.07091508898884058, 0.07027307199314237, 0.0707796011120081, 0.07071904186159372, 0.07148044789209962, 0.08607298508286476, 0.08550435793586075, 0.08510670508258045, 0.0854763900861144, 0.08762010606005788, 0.08591598714701831, 0.17195059289224446, 0.08397224987857044, 0.08165558008477092, 0.08359282906167209, 0.08078921400010586, 0.08309496799483895, 0.08218066906556487, 0.08215719019062817, 0.08200619299896061, 0.08215760416351259, 0.08195297210477293, 0.08234275202266872, 0.0832705341745168, 0.0829338300973177, 0.08286570711061358, 0.08267843001522124, 0.083209996111691, 0.08312629698775709, 0.08316955505870283, 0.08513408107683063, 0.08499840390868485, 0.08362879580818117, 0.0842702619265765, 0.08329250500537455, 0.08339400077238679, 0.08254834986291826, 0.08235553302802145, 0.0825678410474211, 0.08262822008691728, 0.08244814304634929, 0.08242443087510765, 0.08285321784205735, 0.08526852796785533, 0.08642933494411409, 0.08592651784420013, 0.08471894985996187, 0.08507576701231301, 0.08489972213283181, 0.08510020095854998, 0.0702627410646528, 0.07046025106683373, 0.07046843296848238, 0.07009098399430513, 0.06987265101633966, 0.07020888896659017, 0.0701136589050293, 0.07045123400166631, 0.06996862008236349, 0.07024591183289886, 0.07063672901131213, 0.07019292912445962, 0.07018243311904371, 0.07046170206740499, 0.07033142494037747, 0.0701316639315337, 0.07139416108839214, 0.07066388288512826, 0.07139834994450212, 0.07225796394050121, 0.07519555906765163, 0.07119199191220105, 0.07006587297655642, 0.07130932295694947, 0.07140990602783859, 0.07058332301676273, 0.07098597194999456, 0.07187018217518926, 0.07358736894093454, 0.07187662995420396, 0.07642074208706617, 0.07022903091274202, 0.07154885004274547, 0.06995781208388507, 0.07049798686057329, 0.07048408780246973, 0.07021009991876781, 0.07040751981548965, 0.0705507870297879, 0.070499821100384, 0.07059118687175214, 0.07003563083708286, 0.07017871900461614, 0.07027672603726387, 0.07025936106219888]
[0.001434468103534713, 0.0013010472068753168, 0.001352384368109764, 0.0013546901422419719, 0.0014701023631330048, 0.0014238237327306854, 0.001338208227285317, 0.001437691691312559, 0.0013446590188434537, 0.0013335002871344285, 0.0013551430833735028, 0.0014646675114576913, 0.0014610731860204619, 0.0013545886755977966, 0.0013725573092470973, 0.0014434557322564783, 0.001399513794945515, 0.0015190420007067068, 0.001407322815942521, 0.0013715077942351298, 0.0015416486540391129, 0.001391484364106947, 0.0014069137752664332, 0.0014276736491948975, 0.0014211486131713098, 0.0014018935522026553, 0.0014040807727724314, 0.001451480164363676, 0.0015274101621186246, 0.0014164676528652103, 0.0014637115515996606, 0.001497983756684223, 0.001310950511002115, 0.0013432806329231482, 0.0013450198778312426, 0.00135365126141328, 0.0013401052078270183, 0.0013791102611896942, 0.0013527273228962201, 0.0014545017121625797, 0.0013580040635579095, 0.003415992079606774, 0.0013534244891180067, 0.0013631637759354649, 0.0014584057356173896, 0.0013286792245522446, 0.001353281246004056, 0.0013941195698416963, 0.001400033615491524, 0.0014881854306678384, 0.0014268131451491192, 0.0014208215716466003, 0.003204250328090726, 0.0013895474697406195, 0.0014026320815010338, 0.001434740063981438, 0.0014282476527578368, 0.0015270032074150381, 0.0014027902447827617, 0.0014028359793734793, 0.0014150173697924735, 0.001447246714057971, 0.0014341443263906606, 0.001444481655347104, 0.001443245752277423, 0.0014587846508591759, 0.0017565915323033625, 0.0017449868966502194, 0.00173687153229756, 0.0017444161242064164, 0.0017881654297970996, 0.0017533874927962922, 0.0035091957733111114, 0.0017137193852769478, 0.001666440409893284, 0.0017059761032994305, 0.0016487594693899155, 0.0016958156733640603, 0.0016771565115421402, 0.0016766773508291464, 0.0016735957754889922, 0.001676685799255359, 0.0016725096347912842, 0.0016804643269932391, 0.0016993986566227917, 0.0016925271448432182, 0.0016911368798084405, 0.0016873148982698212, 0.0016981631859528776, 0.0016964550405664711, 0.001697337858340874, 0.0017374302260577679, 0.0017346613042588746, 0.0017067101185343095, 0.0017198012638076836, 0.0016998470409260113, 0.0017019183831099346, 0.001684660201284046, 0.0016807251638371724, 0.0016850579805596142, 0.0016862902058554547, 0.0016826151642112099, 0.0016821312423491357, 0.0016908819967766805, 0.001740174040160313, 0.0017638639784513079, 0.0017536024049836763, 0.0017289581604073851, 0.0017362401431084288, 0.0017326473904659553, 0.0017367387950724485, 0.0014638071055136, 0.0014679218972257029, 0.0014680923535100494, 0.0014602288332146902, 0.0014556802295070763, 0.0014626851868039619, 0.0014607012271881104, 0.0014677340417013813, 0.0014576795850492392, 0.0014634564965187262, 0.0014715985210690026, 0.001462352690092909, 0.0014621340233134106, 0.0014679521264042705, 0.0014652380195911974, 0.001461076331906952, 0.0014873783560081695, 0.0014721642267735053, 0.001487465623843794, 0.001505374248760442, 0.0015665741472427424, 0.0014831664981708552, 0.001459705687011592, 0.0014856108949364473, 0.0014877063755799707, 0.0014704858961825569, 0.0014788744156248868, 0.0014972954619831096, 0.0015330701862694696, 0.0014974297907125826, 0.0015920987934805453, 0.0014631048106821254, 0.0014906010425571974, 0.0014574544184142724, 0.0014687080595952768, 0.0014684184958847861, 0.0014627104149743293, 0.0014668233294893678, 0.0014698080631205812, 0.0014687462729246665, 0.0014706497264948364, 0.0014590756424392264, 0.001462056645929503, 0.001464098459109664, 0.0014637366887958099]
[697.1225066182176, 768.6116189447634, 739.4347521168899, 738.1761842195366, 680.224741540346, 702.3341281733985, 747.2678613167664, 695.5594207316016, 743.6829605025844, 749.9061002445749, 737.9294572426942, 682.7488096631316, 684.4284116415201, 738.2314779493396, 728.5670283221472, 692.7818967033736, 714.533864268863, 658.3096448516684, 710.5690241583085, 729.1245475988604, 648.6562274614286, 718.6570153390109, 710.7756122514514, 700.4401885290284, 703.656176934577, 713.320921141837, 712.2097384934966, 688.9518882529107, 654.7029899375098, 705.9815294597194, 683.1947175023114, 667.5639809429531, 762.8053016551949, 744.4460788687721, 743.4834358079787, 738.7427090755634, 746.2100692985896, 725.105184220257, 739.247284411301, 687.5206757324346, 736.3748215745725, 292.7407255918208, 738.8664887035369, 733.5875686058005, 685.6802435549037, 752.6271063183011, 738.9446967899553, 717.2985887527216, 714.2685639365308, 671.9592729457376, 700.8626205889686, 703.8181429361977, 312.0854794749624, 719.6587534981194, 712.945335550749, 696.9903643904499, 700.1586861137679, 654.8774718638825, 712.8649516342065, 712.841711150444, 706.7051057801926, 690.9671932825297, 697.2798912901045, 692.2898579557963, 692.8826905757478, 685.5021400252828, 569.284310899946, 573.0702058105189, 575.7478209555227, 573.2577142136472, 559.2323748890886, 570.3245883231469, 284.9655774708879, 583.5261061940977, 600.0814634974186, 586.1746820872563, 606.5166075255515, 589.6867305255283, 596.2472751457782, 596.4176706421676, 597.5158485972034, 596.4146654335087, 597.9038800125011, 595.0736257455962, 588.4434450403156, 590.8324738228241, 591.3181907033289, 592.6576011540013, 588.8715573814995, 589.4644868785236, 589.1578951626561, 575.5626815984442, 576.4814131409042, 585.9225823649426, 581.4625335173755, 588.2882258954538, 587.5722419618553, 593.5915143230671, 594.9812744618841, 593.45138952898, 593.0177359315802, 594.3129607231301, 594.4839349178703, 591.4073258253945, 574.6551648982623, 566.9371404012746, 570.2546923738444, 578.3829955517116, 575.957193461544, 577.1514766954823, 575.7918247909495, 683.1501201445078, 681.2351541931139, 681.1560577978004, 684.824170879096, 686.9640596400907, 683.6741145817225, 684.6026972435884, 681.3223455938999, 686.0218186881041, 683.3137864902732, 679.5331645709845, 683.8295623037875, 683.9318311831996, 681.2211256844505, 682.4829731615897, 684.4269379785454, 672.3238885120011, 679.272041674092, 672.2844440706314, 664.2866389028654, 638.3355692164687, 674.2331364909268, 685.0696060842699, 673.1237657238498, 672.175649990179, 680.0473248985536, 676.1899383981553, 667.8708547446868, 652.2858568095778, 667.810942591258, 628.101725907262, 683.4780343137429, 670.8703210648855, 686.1278043178954, 680.8705062022768, 681.004769963386, 683.6623228785514, 681.7453608050542, 680.3609430995217, 680.8527915503967, 679.9715676576582, 685.3654265163652, 683.9680273565938, 683.0141742025411, 683.182984791262]
Elapsed: 0.0755535135902345~0.014239164172798852
Time per graph: 0.0015506026754864184~0.0002880715149934378
Speed: 657.9597697074123~75.0490830926244
Total Time: 0.0710
best val loss: 0.44627687335014343 test_score: 0.8958

Testing...
Test loss: 0.2729 score: 0.8958 time: 0.07s
test Score 0.8958
Epoch Time List: [0.582448846893385, 0.24827579292468727, 0.2544785728678107, 0.2573301689699292, 0.26222014147788286, 0.25516734970733523, 0.25364446197636425, 0.2589114988222718, 0.2580662239342928, 0.2553634038195014, 0.2605855534784496, 0.25912645692005754, 0.2636082940734923, 0.25551903806626797, 0.2584085329435766, 0.2701052550692111, 0.2739567949902266, 0.28677604300901294, 0.2833844148553908, 0.2801041412167251, 0.29415815719403327, 0.2816388832870871, 0.2832590139005333, 0.28675498301163316, 0.28504492272622883, 0.35087940213270485, 0.2651636532973498, 0.2695063357241452, 0.2695294751320034, 0.27075346116907895, 0.26558763487264514, 0.3594349001068622, 0.2583077410236001, 0.2540302600245923, 0.2636483199894428, 0.2594525336753577, 0.34502996294759214, 0.2676971131004393, 0.25526751577854156, 0.2612034201156348, 0.262584769167006, 0.3687146620359272, 0.26139028323814273, 0.26185751101002097, 0.2739704982377589, 0.26126042287796736, 0.260204351041466, 0.3234363498631865, 0.2675503119826317, 0.26990691595710814, 0.26860713004134595, 0.26857447694055736, 0.3559137340635061, 0.2616505769547075, 0.26765377586707473, 0.27059708698652685, 0.27125046495348215, 0.27139799599535763, 0.35970020899549127, 0.263164825970307, 0.2649897560477257, 0.2722318859305233, 0.2868110209237784, 0.3061440088786185, 0.2695090030319989, 0.28638756182044744, 0.31293426896445453, 0.31347174500115216, 0.29087948496453464, 0.2830333039164543, 0.2851865249685943, 0.28917236416600645, 0.3725859336555004, 0.2956863809376955, 0.2950315703637898, 0.29681343282572925, 0.30017270683310926, 0.2799987809266895, 0.27952251001261175, 0.28003472718410194, 0.27965890103951097, 0.280824659159407, 0.2796506038866937, 0.280311182141304, 0.28107091411948204, 0.28095692582428455, 0.2813631270546466, 0.2811830169521272, 0.2806749439332634, 0.28190638800151646, 0.2822571557480842, 0.2838162670377642, 0.28362959693185985, 0.29465879895724356, 0.28271584236063063, 0.2815387058071792, 0.2832195011433214, 0.2818011720664799, 0.2780900509096682, 0.2770794138778001, 0.2779774477239698, 0.2771994329523295, 0.2774483801331371, 0.27791416994296014, 0.28112308494746685, 0.2887979759834707, 0.29697841708548367, 0.2918874812312424, 0.2917783809825778, 0.2921966859139502, 0.29343159100972116, 0.2673219081480056, 0.26516011683270335, 0.2653473319951445, 0.2654320700094104, 0.2666743230074644, 0.2663180169183761, 0.26672959816642106, 0.2677864180877805, 0.26724042696878314, 0.2663398622535169, 0.2691979461815208, 0.2691401217598468, 0.2682311988901347, 0.2677325748372823, 0.2693821091670543, 0.26832938217557967, 0.27477323496714234, 0.27133350097574294, 0.272917757043615, 0.2731162249110639, 0.27369051589630544, 0.27741738385520875, 0.2687349880579859, 0.2747023766860366, 0.2705443110316992, 0.2694772419054061, 0.2709810449741781, 0.2921513500623405, 0.2952157522086054, 0.2819430469535291, 0.2774848253466189, 0.26954963291063905, 0.2681349799968302, 0.2743529307190329, 0.26792538817971945, 0.2789898868650198, 0.2664356119930744, 0.26729205227456987, 0.2672264410648495, 0.26753975031897426, 0.2686517760157585, 0.266619514208287, 0.2675010145176202, 0.2676408200059086, 0.2674883892759681]
Total Epoch List: [66, 45, 45]
Total Time List: [0.07191835390403867, 0.08575359615497291, 0.07102832919918001]
========================training times:1========================
========================k_idx:0========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: SAMamba
GraphEncoder(
  (subgraph_conv): GATConv(128, 128, heads=1)
  (embedding): Linear(in_features=14887, out_features=128, bias=False)
  (encoder): GraphMambaEncoder(
    (layers): ModuleList(
      (0-1): 2 x MambaEncoderLayer(
        (self_selen): Selection(
          (khop_structure_extractor): KHopStructureExtractor(
            (structure_extractor): StructureExtractor(
              (gcn): ModuleList(
                (0): GATConv(128, 128, heads=1)
              )
              (relu): ReLU()
              (bn): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (out_proj): Linear(in_features=128, out_features=128, bias=True)
            )
            (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (out_proj): Linear(in_features=256, out_features=128, bias=True)
          )
          (out_proj): Mamba(
            (in_proj): Linear(in_features=128, out_features=512, bias=False)
            (conv1d): Conv1d(256, 256, kernel_size=(4,), stride=(1,), padding=(3,), groups=256)
            (act): SiLU()
            (x_proj): Linear(in_features=256, out_features=40, bias=False)
            (dt_proj): Linear(in_features=8, out_features=256, bias=True)
            (out_proj): Linear(in_features=256, out_features=128, bias=False)
          )
        )
        (norm1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (linear1): Linear(in_features=128, out_features=256, bias=True)
        (dropout): Dropout(p=0.6, inplace=False)
        (linear2): Linear(in_features=256, out_features=128, bias=True)
        (dropout1): Dropout(p=0.6, inplace=False)
        (dropout2): Dropout(p=0.6, inplace=False)
        (activation): ReLU()
      )
    )
  )
  (classifier): Sequential(
    (0): Linear(in_features=128, out_features=128, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=128, out_features=2, bias=True)
  )
)
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7c7a550ccf40>
Training...
Epoch 1/1000, LR 0.000500
Train loss: 0.7313;  Loss pred: 0.7195; Loss self: 1.1804; time: 0.15s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6938 score: 0.5102 time: 0.08s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6959 score: 0.4898 time: 0.08s
Epoch 2/1000, LR 0.000000
Train loss: 0.7219;  Loss pred: 0.7094; Loss self: 1.2573; time: 0.15s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6938 score: 0.5102 time: 0.08s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6958 score: 0.4898 time: 0.08s
Epoch 3/1000, LR 0.000050
Train loss: 0.7150;  Loss pred: 0.7026; Loss self: 1.2442; time: 0.15s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6938 score: 0.5102 time: 0.08s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6961 score: 0.4898 time: 0.08s
     INFO: Early stopping counter 1 of 20
Epoch 4/1000, LR 0.000100
Train loss: 0.6712;  Loss pred: 0.6590; Loss self: 1.2246; time: 0.15s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6938 score: 0.5102 time: 0.08s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6962 score: 0.4898 time: 0.08s
     INFO: Early stopping counter 2 of 20
Epoch 5/1000, LR 0.000150
Train loss: 0.6382;  Loss pred: 0.6261; Loss self: 1.2080; time: 0.15s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6939 score: 0.5102 time: 0.08s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6966 score: 0.4898 time: 0.08s
     INFO: Early stopping counter 3 of 20
Epoch 6/1000, LR 0.000200
Train loss: 0.5872;  Loss pred: 0.5749; Loss self: 1.2371; time: 0.15s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6940 score: 0.5102 time: 0.08s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6968 score: 0.4898 time: 0.08s
     INFO: Early stopping counter 4 of 20
Epoch 7/1000, LR 0.000250
Train loss: 0.5388;  Loss pred: 0.5270; Loss self: 1.1804; time: 0.15s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6940 score: 0.5102 time: 0.08s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6972 score: 0.4898 time: 0.08s
     INFO: Early stopping counter 5 of 20
Epoch 8/1000, LR 0.000300
Train loss: 0.4766;  Loss pred: 0.4649; Loss self: 1.1774; time: 0.15s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6940 score: 0.5102 time: 0.08s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6972 score: 0.4898 time: 0.08s
     INFO: Early stopping counter 6 of 20
Epoch 9/1000, LR 0.000350
Train loss: 0.4157;  Loss pred: 0.4028; Loss self: 1.2889; time: 0.15s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6938 score: 0.5102 time: 0.08s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6972 score: 0.4898 time: 0.08s
Epoch 10/1000, LR 0.000400
Train loss: 0.3551;  Loss pred: 0.3400; Loss self: 1.5139; time: 0.16s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6935 score: 0.5102 time: 0.08s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6970 score: 0.4898 time: 0.08s
Epoch 11/1000, LR 0.000450
Train loss: 0.3033;  Loss pred: 0.2867; Loss self: 1.6583; time: 0.17s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6926 score: 0.5102 time: 0.08s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6960 score: 0.4898 time: 0.08s
Epoch 12/1000, LR 0.000450
Train loss: 0.2667;  Loss pred: 0.2489; Loss self: 1.7755; time: 0.17s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6910 score: 0.5102 time: 0.08s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6940 score: 0.4898 time: 0.08s
Epoch 13/1000, LR 0.000450
Train loss: 0.2367;  Loss pred: 0.2173; Loss self: 1.9378; time: 0.17s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6881 score: 0.5102 time: 0.08s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6902 score: 0.4898 time: 0.08s
Epoch 14/1000, LR 0.000450
Train loss: 0.2078;  Loss pred: 0.1873; Loss self: 2.0452; time: 0.17s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6833 score: 0.5102 time: 0.08s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6841 score: 0.4898 time: 0.08s
Epoch 15/1000, LR 0.000450
Train loss: 0.1838;  Loss pred: 0.1624; Loss self: 2.1357; time: 0.17s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6758 score: 0.5102 time: 0.08s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6742 score: 0.4898 time: 0.08s
Epoch 16/1000, LR 0.000450
Train loss: 0.1639;  Loss pred: 0.1416; Loss self: 2.2318; time: 0.17s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6644 score: 0.5102 time: 0.08s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6592 score: 0.4898 time: 0.08s
Epoch 17/1000, LR 0.000450
Train loss: 0.1468;  Loss pred: 0.1240; Loss self: 2.2782; time: 0.17s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6476 score: 0.5102 time: 0.08s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6368 score: 0.4898 time: 0.08s
Epoch 18/1000, LR 0.000450
Train loss: 0.1339;  Loss pred: 0.1104; Loss self: 2.3517; time: 0.17s
Val loss: 0.6241 score: 0.5510 time: 0.08s
Test loss: 0.6060 score: 0.5102 time: 0.08s
Epoch 19/1000, LR 0.000450
Train loss: 0.1217;  Loss pred: 0.0975; Loss self: 2.4154; time: 0.17s
Val loss: 0.5930 score: 0.6531 time: 0.08s
Test loss: 0.5661 score: 0.7143 time: 0.08s
Epoch 20/1000, LR 0.000450
Train loss: 0.1092;  Loss pred: 0.0846; Loss self: 2.4565; time: 0.17s
Val loss: 0.5546 score: 0.7347 time: 0.08s
Test loss: 0.5161 score: 0.8367 time: 0.08s
Epoch 21/1000, LR 0.000450
Train loss: 0.0965;  Loss pred: 0.0713; Loss self: 2.5201; time: 0.17s
Val loss: 0.5100 score: 0.7347 time: 0.08s
Test loss: 0.4588 score: 0.8776 time: 0.08s
Epoch 22/1000, LR 0.000450
Train loss: 0.0857;  Loss pred: 0.0600; Loss self: 2.5771; time: 0.17s
Val loss: 0.4638 score: 0.7959 time: 0.08s
Test loss: 0.3997 score: 0.8776 time: 0.08s
Epoch 23/1000, LR 0.000450
Train loss: 0.0783;  Loss pred: 0.0521; Loss self: 2.6172; time: 0.17s
Val loss: 0.4216 score: 0.8163 time: 0.08s
Test loss: 0.3465 score: 0.8776 time: 0.08s
Epoch 24/1000, LR 0.000450
Train loss: 0.0702;  Loss pred: 0.0435; Loss self: 2.6705; time: 0.17s
Val loss: 0.3875 score: 0.8367 time: 0.08s
Test loss: 0.3034 score: 0.8980 time: 0.08s
Epoch 25/1000, LR 0.000450
Train loss: 0.0638;  Loss pred: 0.0367; Loss self: 2.7087; time: 0.17s
Val loss: 0.3648 score: 0.8571 time: 0.08s
Test loss: 0.2731 score: 0.9184 time: 0.08s
Epoch 26/1000, LR 0.000450
Train loss: 0.0579;  Loss pred: 0.0304; Loss self: 2.7495; time: 0.17s
Val loss: 0.3543 score: 0.8571 time: 0.08s
Test loss: 0.2569 score: 0.9184 time: 0.08s
Epoch 27/1000, LR 0.000450
Train loss: 0.0538;  Loss pred: 0.0260; Loss self: 2.7832; time: 0.17s
Val loss: 0.3532 score: 0.8571 time: 0.08s
Test loss: 0.2516 score: 0.9184 time: 0.08s
Epoch 28/1000, LR 0.000450
Train loss: 0.0496;  Loss pred: 0.0217; Loss self: 2.7947; time: 0.17s
Val loss: 0.3577 score: 0.8571 time: 0.08s
Test loss: 0.2538 score: 0.9184 time: 0.08s
     INFO: Early stopping counter 1 of 20
Epoch 29/1000, LR 0.000450
Train loss: 0.0463;  Loss pred: 0.0183; Loss self: 2.8043; time: 0.17s
Val loss: 0.3677 score: 0.8571 time: 0.08s
Test loss: 0.2622 score: 0.9184 time: 0.08s
     INFO: Early stopping counter 2 of 20
Epoch 30/1000, LR 0.000450
Train loss: 0.0439;  Loss pred: 0.0157; Loss self: 2.8155; time: 0.17s
Val loss: 0.3798 score: 0.8571 time: 0.08s
Test loss: 0.2738 score: 0.9388 time: 0.08s
     INFO: Early stopping counter 3 of 20
Epoch 31/1000, LR 0.000450
Train loss: 0.0417;  Loss pred: 0.0135; Loss self: 2.8278; time: 0.17s
Val loss: 0.3932 score: 0.8571 time: 0.08s
Test loss: 0.2871 score: 0.9388 time: 0.08s
     INFO: Early stopping counter 4 of 20
Epoch 32/1000, LR 0.000450
Train loss: 0.0398;  Loss pred: 0.0114; Loss self: 2.8436; time: 0.17s
Val loss: 0.4069 score: 0.8571 time: 0.08s
Test loss: 0.2999 score: 0.9388 time: 0.08s
     INFO: Early stopping counter 5 of 20
Epoch 33/1000, LR 0.000449
Train loss: 0.0390;  Loss pred: 0.0103; Loss self: 2.8643; time: 0.17s
Val loss: 0.4193 score: 0.8571 time: 0.08s
Test loss: 0.3103 score: 0.9388 time: 0.08s
     INFO: Early stopping counter 6 of 20
Epoch 34/1000, LR 0.000449
Train loss: 0.0370;  Loss pred: 0.0085; Loss self: 2.8501; time: 0.17s
Val loss: 0.4337 score: 0.8571 time: 0.08s
Test loss: 0.3210 score: 0.9388 time: 0.08s
     INFO: Early stopping counter 7 of 20
Epoch 35/1000, LR 0.000449
Train loss: 0.0360;  Loss pred: 0.0076; Loss self: 2.8457; time: 0.17s
Val loss: 0.4485 score: 0.8571 time: 0.08s
Test loss: 0.3304 score: 0.9388 time: 0.08s
     INFO: Early stopping counter 8 of 20
Epoch 36/1000, LR 0.000449
Train loss: 0.0348;  Loss pred: 0.0061; Loss self: 2.8699; time: 0.17s
Val loss: 0.4624 score: 0.8571 time: 0.08s
Test loss: 0.3381 score: 0.9388 time: 0.08s
     INFO: Early stopping counter 9 of 20
Epoch 37/1000, LR 0.000449
Train loss: 0.0344;  Loss pred: 0.0057; Loss self: 2.8623; time: 0.17s
Val loss: 0.4771 score: 0.8571 time: 0.08s
Test loss: 0.3458 score: 0.9388 time: 0.08s
     INFO: Early stopping counter 10 of 20
Epoch 38/1000, LR 0.000449
Train loss: 0.0333;  Loss pred: 0.0048; Loss self: 2.8452; time: 0.17s
Val loss: 0.4920 score: 0.8571 time: 0.08s
Test loss: 0.3531 score: 0.9388 time: 0.08s
     INFO: Early stopping counter 11 of 20
Epoch 39/1000, LR 0.000449
Train loss: 0.0330;  Loss pred: 0.0043; Loss self: 2.8685; time: 0.17s
Val loss: 0.5072 score: 0.8367 time: 0.08s
Test loss: 0.3605 score: 0.9184 time: 0.08s
     INFO: Early stopping counter 12 of 20
Epoch 40/1000, LR 0.000449
Train loss: 0.0325;  Loss pred: 0.0039; Loss self: 2.8593; time: 0.17s
Val loss: 0.5232 score: 0.8571 time: 0.08s
Test loss: 0.3681 score: 0.9184 time: 0.08s
     INFO: Early stopping counter 13 of 20
Epoch 41/1000, LR 0.000449
Train loss: 0.0322;  Loss pred: 0.0035; Loss self: 2.8635; time: 0.17s
Val loss: 0.5348 score: 0.8571 time: 0.08s
Test loss: 0.3728 score: 0.9184 time: 0.08s
     INFO: Early stopping counter 14 of 20
Epoch 42/1000, LR 0.000449
Train loss: 0.0316;  Loss pred: 0.0033; Loss self: 2.8281; time: 0.17s
Val loss: 0.5478 score: 0.8367 time: 0.08s
Test loss: 0.3783 score: 0.9184 time: 0.08s
     INFO: Early stopping counter 15 of 20
Epoch 43/1000, LR 0.000449
Train loss: 0.0312;  Loss pred: 0.0029; Loss self: 2.8289; time: 0.17s
Val loss: 0.5594 score: 0.8367 time: 0.08s
Test loss: 0.3829 score: 0.9184 time: 0.08s
     INFO: Early stopping counter 16 of 20
Epoch 44/1000, LR 0.000449
Train loss: 0.0310;  Loss pred: 0.0027; Loss self: 2.8361; time: 0.17s
Val loss: 0.5732 score: 0.8367 time: 0.08s
Test loss: 0.3887 score: 0.8980 time: 0.08s
     INFO: Early stopping counter 17 of 20
Epoch 45/1000, LR 0.000449
Train loss: 0.0305;  Loss pred: 0.0024; Loss self: 2.8061; time: 0.17s
Val loss: 0.5825 score: 0.8163 time: 0.08s
Test loss: 0.3915 score: 0.8980 time: 0.08s
     INFO: Early stopping counter 18 of 20
Epoch 46/1000, LR 0.000449
Train loss: 0.0304;  Loss pred: 0.0023; Loss self: 2.8119; time: 0.17s
Val loss: 0.5924 score: 0.7959 time: 0.08s
Test loss: 0.3948 score: 0.8980 time: 0.08s
     INFO: Early stopping counter 19 of 20
Epoch 47/1000, LR 0.000449
Train loss: 0.0301;  Loss pred: 0.0021; Loss self: 2.7986; time: 0.17s
Val loss: 0.6037 score: 0.7959 time: 0.08s
Test loss: 0.3989 score: 0.8980 time: 0.08s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 026,   Train_Loss: 0.0538,   Val_Loss: 0.3532,   Val_Precision: 0.9048,   Val_Recall: 0.7917,   Val_accuracy: 0.8444,   Val_Score: 0.8571,   Val_Loss: 0.3532,   Test_Precision: 0.9565,   Test_Recall: 0.8800,   Test_accuracy: 0.9167,   Test_Score: 0.9184,   Test_loss: 0.2516


[0.08421817002817988, 0.08413788001053035, 0.08421590086072683, 0.08465691306628287, 0.08425439288839698, 0.08410245808772743, 0.08397454489022493, 0.08410602016374469, 0.08402830408886075, 0.08446345012634993, 0.08417584002017975, 0.08870415203273296, 0.08439197903499007, 0.08585282997228205, 0.08465454494580626, 0.08383490797132254, 0.08480131812393665, 0.08887438289821148, 0.08371175499632955, 0.08424213202670217, 0.08432307001203299, 0.0839334330521524, 0.08413614286109805, 0.08377454499714077, 0.08354397490620613, 0.08401303295977414, 0.08481663884595037, 0.08404924301430583, 0.08746011322364211, 0.08680622093379498, 0.08330337191000581, 0.08295292500406504, 0.08305783709511161, 0.08354193391278386, 0.08340115402825177, 0.083300672005862, 0.08365586609579623, 0.08320683892816305, 0.08319863094948232, 0.0834349300712347, 0.08353417995385826, 0.08351884502917528, 0.08362787589430809, 0.08340057893656194, 0.08350446308031678, 0.0834355209954083, 0.08495125221088529]
[0.0017187381638404057, 0.0017170995920516398, 0.0017186918543005477, 0.001727692103393528, 0.0017194774058856526, 0.0017163766956679067, 0.0017137662222494884, 0.0017164493910968304, 0.0017148633487522602, 0.0017237438801295903, 0.0017178742861261173, 0.0018102888169945503, 0.001722285286428369, 0.001752098570862899, 0.0017276437744042094, 0.001710916489210664, 0.0017306391453864624, 0.0018137629162900302, 0.001708403163190399, 0.0017192271842184116, 0.001720878979837408, 0.0017129272051459672, 0.0017170641400224092, 0.001709684591778383, 0.0017049790797184926, 0.001714551693056615, 0.0017309518131826604, 0.0017152906737613435, 0.001784900269870247, 0.0017715555292611219, 0.0017000688144899144, 0.0016929168368176538, 0.0016950578999002368, 0.0017049374267915074, 0.0017020643679235056, 0.001700013714405347, 0.0017072625733835964, 0.0016980987536359805, 0.0016979312438669862, 0.0017027536749231573, 0.001704779182731801, 0.0017044662250852098, 0.0017066913447817977, 0.001702052631358407, 0.0017041727159248323, 0.0017027657346001693, 0.0017336990247119445]
[581.822188532526, 582.3774023527495, 581.8378655241651, 578.8068360304494, 581.5720500758364, 582.6226856400322, 583.5101585135694, 582.5980102803898, 583.1368433686586, 580.132589027565, 582.1147729354773, 552.3980431256293, 580.6239000472299, 570.7441445531831, 578.8230275334724, 584.4820634473824, 577.8212070758944, 551.3399744909631, 585.3419272137882, 581.6566938793585, 581.0983873453333, 583.7959704275845, 582.3894266331537, 584.9032065966145, 586.5174604752975, 583.2428407085534, 577.7168332383116, 582.9915683078766, 560.2553917887495, 564.4756732051626, 588.2114838392811, 590.6964702884051, 589.9503492233836, 586.5317895460146, 587.5218463212328, 588.2305486869517, 585.7329830748389, 588.8939014051999, 588.9519988586409, 587.2840063288241, 586.5862336479045, 586.6939369537862, 585.929027564062, 587.5258975992421, 586.7949830761802, 587.2798469454827, 576.801385791949]
Elapsed: 0.08428266313065716~0.0012748969298457338
Time per graph: 0.0017200543496052486~2.601830469072925e-05
Speed: 581.5056559899222~8.508516688832938
Total Time: 0.0854
best val loss: 0.3532158136367798 test_score: 0.9184

Testing...
Test loss: 0.2731 score: 0.9184 time: 0.08s
test Score 0.9184
Epoch Time List: [0.307001403067261, 0.30254742805846035, 0.30269402591511607, 0.30303596379235387, 0.30270572006702423, 0.3023567069321871, 0.30244705895893276, 0.30277024884708226, 0.30252559180371463, 0.31822171923704445, 0.3212687310297042, 0.3255991991609335, 0.3243439821526408, 0.3273403700441122, 0.3223474738188088, 0.3235234091989696, 0.32650245004333556, 0.32774593005888164, 0.32379225525073707, 0.32832981226965785, 0.3251078261528164, 0.32126226578839123, 0.3224751891102642, 0.32318580499850214, 0.3234090602491051, 0.3236947536934167, 0.32903204718604684, 0.3237466199789196, 0.3273947418201715, 0.33094747900031507, 0.3204464379232377, 0.31984001281671226, 0.32112473296001554, 0.32230760995298624, 0.322973309783265, 0.32252215570770204, 0.32239950099028647, 0.32148613478057086, 0.32272136211395264, 0.3229021760635078, 0.32177545316517353, 0.3211888992227614, 0.32238872698508203, 0.32085162471048534, 0.32060786290094256, 0.3220606381073594, 0.3239360502921045]
Total Epoch List: [47]
Total Time List: [0.0854341711383313]
========================k_idx:1========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: SAMamba
GraphEncoder(
  (subgraph_conv): GATConv(128, 128, heads=1)
  (embedding): Linear(in_features=14887, out_features=128, bias=False)
  (encoder): GraphMambaEncoder(
    (layers): ModuleList(
      (0-1): 2 x MambaEncoderLayer(
        (self_selen): Selection(
          (khop_structure_extractor): KHopStructureExtractor(
            (structure_extractor): StructureExtractor(
              (gcn): ModuleList(
                (0): GATConv(128, 128, heads=1)
              )
              (relu): ReLU()
              (bn): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (out_proj): Linear(in_features=128, out_features=128, bias=True)
            )
            (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (out_proj): Linear(in_features=256, out_features=128, bias=True)
          )
          (out_proj): Mamba(
            (in_proj): Linear(in_features=128, out_features=512, bias=False)
            (conv1d): Conv1d(256, 256, kernel_size=(4,), stride=(1,), padding=(3,), groups=256)
            (act): SiLU()
            (x_proj): Linear(in_features=256, out_features=40, bias=False)
            (dt_proj): Linear(in_features=8, out_features=256, bias=True)
            (out_proj): Linear(in_features=256, out_features=128, bias=False)
          )
        )
        (norm1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (linear1): Linear(in_features=128, out_features=256, bias=True)
        (dropout): Dropout(p=0.6, inplace=False)
        (linear2): Linear(in_features=256, out_features=128, bias=True)
        (dropout1): Dropout(p=0.6, inplace=False)
        (dropout2): Dropout(p=0.6, inplace=False)
        (activation): ReLU()
      )
    )
  )
  (classifier): Sequential(
    (0): Linear(in_features=128, out_features=128, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=128, out_features=2, bias=True)
  )
)
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7c7a5504c400>
Training...
Epoch 1/1000, LR 0.000500
Train loss: 0.6843;  Loss pred: 0.6709; Loss self: 1.3414; time: 0.16s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6930 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6931 score: 0.5102 time: 0.08s
Epoch 2/1000, LR 0.000000
Train loss: 0.6879;  Loss pred: 0.6744; Loss self: 1.3447; time: 0.14s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6933 score: 0.4898 time: 0.10s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6932 score: 0.5102 time: 0.08s
     INFO: Early stopping counter 1 of 20
Epoch 3/1000, LR 0.000050
Train loss: 0.6894;  Loss pred: 0.6760; Loss self: 1.3404; time: 0.13s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6934 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6932 score: 0.5102 time: 0.08s
     INFO: Early stopping counter 2 of 20
Epoch 4/1000, LR 0.000100
Train loss: 0.6337;  Loss pred: 0.6208; Loss self: 1.2927; time: 0.13s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6934 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6931 score: 0.5102 time: 0.08s
     INFO: Early stopping counter 3 of 20
Epoch 5/1000, LR 0.000150
Train loss: 0.5924;  Loss pred: 0.5791; Loss self: 1.3339; time: 0.13s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6935 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6931 score: 0.5102 time: 0.08s
     INFO: Early stopping counter 4 of 20
Epoch 6/1000, LR 0.000200
Train loss: 0.5550;  Loss pred: 0.5421; Loss self: 1.2977; time: 0.15s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6934 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6929 score: 0.5102 time: 0.08s
     INFO: Early stopping counter 5 of 20
Epoch 7/1000, LR 0.000250
Train loss: 0.4937;  Loss pred: 0.4809; Loss self: 1.2832; time: 0.13s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6931 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6926 score: 0.5102 time: 0.08s
     INFO: Early stopping counter 6 of 20
Epoch 8/1000, LR 0.000300
Train loss: 0.4364;  Loss pred: 0.4227; Loss self: 1.3635; time: 0.15s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6926 score: 0.4898 time: 0.10s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6921 score: 0.5102 time: 0.10s
Epoch 9/1000, LR 0.000350
Train loss: 0.3958;  Loss pred: 0.3813; Loss self: 1.4552; time: 0.13s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6918 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6913 score: 0.5102 time: 0.08s
Epoch 10/1000, LR 0.000400
Train loss: 0.3631;  Loss pred: 0.3477; Loss self: 1.5402; time: 0.13s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6904 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6902 score: 0.5102 time: 0.08s
Epoch 11/1000, LR 0.000450
Train loss: 0.3235;  Loss pred: 0.3071; Loss self: 1.6404; time: 0.13s
Val loss: 0.6884 score: 0.5102 time: 0.08s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6884 score: 0.5102 time: 0.08s
Epoch 12/1000, LR 0.000450
Train loss: 0.2925;  Loss pred: 0.2754; Loss self: 1.7041; time: 0.13s
Val loss: 0.6857 score: 0.5918 time: 0.08s
Test loss: 0.6860 score: 0.5714 time: 0.08s
Epoch 13/1000, LR 0.000450
Train loss: 0.2577;  Loss pred: 0.2397; Loss self: 1.7990; time: 0.13s
Val loss: 0.6817 score: 0.9184 time: 0.08s
Test loss: 0.6825 score: 0.8367 time: 0.16s
Epoch 14/1000, LR 0.000450
Train loss: 0.2292;  Loss pred: 0.2109; Loss self: 1.8275; time: 0.13s
Val loss: 0.6762 score: 0.9388 time: 0.08s
Test loss: 0.6778 score: 0.9388 time: 0.08s
Epoch 15/1000, LR 0.000450
Train loss: 0.2058;  Loss pred: 0.1873; Loss self: 1.8445; time: 0.13s
Val loss: 0.6688 score: 0.8980 time: 0.08s
Test loss: 0.6713 score: 0.8776 time: 0.08s
Epoch 16/1000, LR 0.000450
Train loss: 0.1861;  Loss pred: 0.1673; Loss self: 1.8802; time: 0.13s
Val loss: 0.6585 score: 0.8571 time: 0.08s
Test loss: 0.6620 score: 0.8571 time: 0.08s
Epoch 17/1000, LR 0.000450
Train loss: 0.1626;  Loss pred: 0.1434; Loss self: 1.9149; time: 0.13s
Val loss: 0.6453 score: 0.8571 time: 0.08s
Test loss: 0.6500 score: 0.7959 time: 0.09s
Epoch 18/1000, LR 0.000450
Train loss: 0.1498;  Loss pred: 0.1305; Loss self: 1.9356; time: 0.13s
Val loss: 0.6267 score: 0.8571 time: 0.08s
Test loss: 0.6329 score: 0.8163 time: 0.08s
Epoch 19/1000, LR 0.000450
Train loss: 0.1362;  Loss pred: 0.1164; Loss self: 1.9839; time: 0.13s
Val loss: 0.6027 score: 0.8571 time: 0.08s
Test loss: 0.6109 score: 0.8367 time: 0.16s
Epoch 20/1000, LR 0.000450
Train loss: 0.1220;  Loss pred: 0.1018; Loss self: 2.0187; time: 0.13s
Val loss: 0.5694 score: 0.8571 time: 0.08s
Test loss: 0.5798 score: 0.8980 time: 0.08s
Epoch 21/1000, LR 0.000450
Train loss: 0.1095;  Loss pred: 0.0888; Loss self: 2.0685; time: 0.13s
Val loss: 0.5273 score: 0.8980 time: 0.08s
Test loss: 0.5406 score: 0.8980 time: 0.08s
Epoch 22/1000, LR 0.000450
Train loss: 0.1012;  Loss pred: 0.0804; Loss self: 2.0798; time: 0.13s
Val loss: 0.4773 score: 0.9184 time: 0.08s
Test loss: 0.4946 score: 0.8776 time: 0.08s
Epoch 23/1000, LR 0.000450
Train loss: 0.0919;  Loss pred: 0.0709; Loss self: 2.1008; time: 0.14s
Val loss: 0.4209 score: 0.9184 time: 0.08s
Test loss: 0.4435 score: 0.8776 time: 0.08s
Epoch 24/1000, LR 0.000450
Train loss: 0.0837;  Loss pred: 0.0620; Loss self: 2.1629; time: 0.13s
Val loss: 0.3645 score: 0.9388 time: 0.08s
Test loss: 0.3947 score: 0.8776 time: 0.08s
Epoch 25/1000, LR 0.000450
Train loss: 0.0747;  Loss pred: 0.0527; Loss self: 2.1906; time: 0.15s
Val loss: 0.3127 score: 0.9388 time: 0.10s
Test loss: 0.3528 score: 0.8980 time: 0.08s
Epoch 26/1000, LR 0.000450
Train loss: 0.0677;  Loss pred: 0.0453; Loss self: 2.2404; time: 0.13s
Val loss: 0.2680 score: 0.9388 time: 0.08s
Test loss: 0.3209 score: 0.8980 time: 0.08s
Epoch 27/1000, LR 0.000450
Train loss: 0.0630;  Loss pred: 0.0405; Loss self: 2.2560; time: 0.13s
Val loss: 0.2339 score: 0.9592 time: 0.08s
Test loss: 0.3009 score: 0.8980 time: 0.08s
Epoch 28/1000, LR 0.000450
Train loss: 0.0559;  Loss pred: 0.0331; Loss self: 2.2815; time: 0.13s
Val loss: 0.2090 score: 0.9592 time: 0.08s
Test loss: 0.2918 score: 0.8980 time: 0.08s
Epoch 29/1000, LR 0.000450
Train loss: 0.0524;  Loss pred: 0.0295; Loss self: 2.2925; time: 0.14s
Val loss: 0.1915 score: 0.9592 time: 0.08s
Test loss: 0.2918 score: 0.8980 time: 0.08s
Epoch 30/1000, LR 0.000450
Train loss: 0.0485;  Loss pred: 0.0251; Loss self: 2.3348; time: 0.13s
Val loss: 0.1804 score: 0.9388 time: 0.08s
Test loss: 0.2989 score: 0.8980 time: 0.12s
Epoch 31/1000, LR 0.000450
Train loss: 0.0457;  Loss pred: 0.0218; Loss self: 2.3903; time: 0.15s
Val loss: 0.1742 score: 0.9388 time: 0.09s
Test loss: 0.3109 score: 0.8980 time: 0.09s
Epoch 32/1000, LR 0.000450
Train loss: 0.0424;  Loss pred: 0.0184; Loss self: 2.4044; time: 0.13s
Val loss: 0.1712 score: 0.9388 time: 0.09s
Test loss: 0.3255 score: 0.8980 time: 0.08s
Epoch 33/1000, LR 0.000449
Train loss: 0.0398;  Loss pred: 0.0156; Loss self: 2.4167; time: 0.14s
Val loss: 0.1710 score: 0.9388 time: 0.09s
Test loss: 0.3408 score: 0.8980 time: 0.09s
Epoch 34/1000, LR 0.000449
Train loss: 0.0379;  Loss pred: 0.0137; Loss self: 2.4145; time: 0.14s
Val loss: 0.1720 score: 0.9388 time: 0.09s
Test loss: 0.3562 score: 0.8980 time: 0.09s
     INFO: Early stopping counter 1 of 20
Epoch 35/1000, LR 0.000449
Train loss: 0.0362;  Loss pred: 0.0120; Loss self: 2.4197; time: 0.13s
Val loss: 0.1734 score: 0.9388 time: 0.09s
Test loss: 0.3723 score: 0.8980 time: 0.17s
     INFO: Early stopping counter 2 of 20
Epoch 36/1000, LR 0.000449
Train loss: 0.0351;  Loss pred: 0.0107; Loss self: 2.4453; time: 0.14s
Val loss: 0.1747 score: 0.9388 time: 0.09s
Test loss: 0.3887 score: 0.8980 time: 0.08s
     INFO: Early stopping counter 3 of 20
Epoch 37/1000, LR 0.000449
Train loss: 0.0337;  Loss pred: 0.0091; Loss self: 2.4622; time: 0.14s
Val loss: 0.1759 score: 0.9388 time: 0.09s
Test loss: 0.4050 score: 0.8980 time: 0.08s
     INFO: Early stopping counter 4 of 20
Epoch 38/1000, LR 0.000449
Train loss: 0.0326;  Loss pred: 0.0082; Loss self: 2.4428; time: 0.15s
Val loss: 0.1775 score: 0.9388 time: 0.08s
Test loss: 0.4209 score: 0.8980 time: 0.08s
     INFO: Early stopping counter 5 of 20
Epoch 39/1000, LR 0.000449
Train loss: 0.0314;  Loss pred: 0.0070; Loss self: 2.4459; time: 0.14s
Val loss: 0.1788 score: 0.9388 time: 0.09s
Test loss: 0.4358 score: 0.8980 time: 0.08s
     INFO: Early stopping counter 6 of 20
Epoch 40/1000, LR 0.000449
Train loss: 0.0308;  Loss pred: 0.0062; Loss self: 2.4545; time: 0.13s
Val loss: 0.1804 score: 0.9388 time: 0.09s
Test loss: 0.4501 score: 0.8980 time: 0.15s
     INFO: Early stopping counter 7 of 20
Epoch 41/1000, LR 0.000449
Train loss: 0.0302;  Loss pred: 0.0059; Loss self: 2.4295; time: 0.13s
Val loss: 0.1820 score: 0.9388 time: 0.09s
Test loss: 0.4630 score: 0.8980 time: 0.08s
     INFO: Early stopping counter 8 of 20
Epoch 42/1000, LR 0.000449
Train loss: 0.0290;  Loss pred: 0.0050; Loss self: 2.4012; time: 0.13s
Val loss: 0.1834 score: 0.9388 time: 0.09s
Test loss: 0.4758 score: 0.8980 time: 0.08s
     INFO: Early stopping counter 9 of 20
Epoch 43/1000, LR 0.000449
Train loss: 0.0286;  Loss pred: 0.0045; Loss self: 2.4065; time: 0.13s
Val loss: 0.1851 score: 0.9184 time: 0.09s
Test loss: 0.4883 score: 0.8980 time: 0.08s
     INFO: Early stopping counter 10 of 20
Epoch 44/1000, LR 0.000449
Train loss: 0.0282;  Loss pred: 0.0043; Loss self: 2.3902; time: 0.14s
Val loss: 0.1866 score: 0.9184 time: 0.09s
Test loss: 0.4992 score: 0.8980 time: 0.09s
     INFO: Early stopping counter 11 of 20
Epoch 45/1000, LR 0.000449
Train loss: 0.0278;  Loss pred: 0.0039; Loss self: 2.3956; time: 0.13s
Val loss: 0.1881 score: 0.9184 time: 0.09s
Test loss: 0.5094 score: 0.8980 time: 0.08s
     INFO: Early stopping counter 12 of 20
Epoch 46/1000, LR 0.000449
Train loss: 0.0275;  Loss pred: 0.0037; Loss self: 2.3809; time: 0.13s
Val loss: 0.1898 score: 0.9184 time: 0.08s
Test loss: 0.5188 score: 0.8980 time: 0.16s
     INFO: Early stopping counter 13 of 20
Epoch 47/1000, LR 0.000449
Train loss: 0.0273;  Loss pred: 0.0035; Loss self: 2.3828; time: 0.15s
Val loss: 0.1911 score: 0.9184 time: 0.08s
Test loss: 0.5273 score: 0.8980 time: 0.08s
     INFO: Early stopping counter 14 of 20
Epoch 48/1000, LR 0.000448
Train loss: 0.0269;  Loss pred: 0.0033; Loss self: 2.3613; time: 0.15s
Val loss: 0.1927 score: 0.9184 time: 0.08s
Test loss: 0.5362 score: 0.8980 time: 0.08s
     INFO: Early stopping counter 15 of 20
Epoch 49/1000, LR 0.000448
Train loss: 0.0265;  Loss pred: 0.0031; Loss self: 2.3400; time: 0.15s
Val loss: 0.1940 score: 0.9184 time: 0.08s
Test loss: 0.5441 score: 0.8980 time: 0.08s
     INFO: Early stopping counter 16 of 20
Epoch 50/1000, LR 0.000448
Train loss: 0.0259;  Loss pred: 0.0028; Loss self: 2.3133; time: 0.15s
Val loss: 0.1954 score: 0.9184 time: 0.08s
Test loss: 0.5505 score: 0.8980 time: 0.08s
     INFO: Early stopping counter 17 of 20
Epoch 51/1000, LR 0.000448
Train loss: 0.0260;  Loss pred: 0.0027; Loss self: 2.3263; time: 0.15s
Val loss: 0.1966 score: 0.9184 time: 0.08s
Test loss: 0.5567 score: 0.8980 time: 0.08s
     INFO: Early stopping counter 18 of 20
Epoch 52/1000, LR 0.000448
Train loss: 0.0257;  Loss pred: 0.0025; Loss self: 2.3236; time: 0.13s
Val loss: 0.1975 score: 0.9184 time: 0.12s
Test loss: 0.5622 score: 0.8980 time: 0.08s
     INFO: Early stopping counter 19 of 20
Epoch 53/1000, LR 0.000448
Train loss: 0.0251;  Loss pred: 0.0024; Loss self: 2.2731; time: 0.15s
Val loss: 0.1982 score: 0.9184 time: 0.08s
Test loss: 0.5673 score: 0.8776 time: 0.08s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 032,   Train_Loss: 0.0398,   Val_Loss: 0.1710,   Val_Precision: 0.9231,   Val_Recall: 0.9600,   Val_accuracy: 0.9412,   Val_Score: 0.9388,   Val_Loss: 0.1710,   Test_Precision: 0.8800,   Test_Recall: 0.9167,   Test_accuracy: 0.8980,   Test_Score: 0.8980,   Test_loss: 0.3408


[0.08421817002817988, 0.08413788001053035, 0.08421590086072683, 0.08465691306628287, 0.08425439288839698, 0.08410245808772743, 0.08397454489022493, 0.08410602016374469, 0.08402830408886075, 0.08446345012634993, 0.08417584002017975, 0.08870415203273296, 0.08439197903499007, 0.08585282997228205, 0.08465454494580626, 0.08383490797132254, 0.08480131812393665, 0.08887438289821148, 0.08371175499632955, 0.08424213202670217, 0.08432307001203299, 0.0839334330521524, 0.08413614286109805, 0.08377454499714077, 0.08354397490620613, 0.08401303295977414, 0.08481663884595037, 0.08404924301430583, 0.08746011322364211, 0.08680622093379498, 0.08330337191000581, 0.08295292500406504, 0.08305783709511161, 0.08354193391278386, 0.08340115402825177, 0.083300672005862, 0.08365586609579623, 0.08320683892816305, 0.08319863094948232, 0.0834349300712347, 0.08353417995385826, 0.08351884502917528, 0.08362787589430809, 0.08340057893656194, 0.08350446308031678, 0.0834355209954083, 0.08495125221088529, 0.08402819489128888, 0.08451757486909628, 0.08417199808172882, 0.08475079480558634, 0.08526912401430309, 0.08495980687439442, 0.08479839307256043, 0.11114742909558117, 0.08480706601403654, 0.08641512109898031, 0.08722605602815747, 0.08531306311488152, 0.1692618529777974, 0.08482343889772892, 0.08490814594551921, 0.08371508307754993, 0.09225645614787936, 0.08397343708202243, 0.1681078861001879, 0.08382177399471402, 0.08396520302630961, 0.08431480708532035, 0.08570109005086124, 0.0850512208417058, 0.08491995395161211, 0.08416196308098733, 0.08496773801743984, 0.08550867205485702, 0.08588509703986347, 0.12412229203619063, 0.09140944993123412, 0.09018395305611193, 0.09247995214536786, 0.090353551087901, 0.1710139277856797, 0.08857877505943179, 0.08609673311002553, 0.08793412405066192, 0.0888410818297416, 0.15048668207600713, 0.08919924986548722, 0.08836724795401096, 0.08945947303436697, 0.09064853796735406, 0.08820825396105647, 0.16821326385252178, 0.08492336003109813, 0.08463653107173741, 0.08559681312181056, 0.08666634210385382, 0.08494016691111028, 0.08486681594513357, 0.08396001695655286]
[0.0017187381638404057, 0.0017170995920516398, 0.0017186918543005477, 0.001727692103393528, 0.0017194774058856526, 0.0017163766956679067, 0.0017137662222494884, 0.0017164493910968304, 0.0017148633487522602, 0.0017237438801295903, 0.0017178742861261173, 0.0018102888169945503, 0.001722285286428369, 0.001752098570862899, 0.0017276437744042094, 0.001710916489210664, 0.0017306391453864624, 0.0018137629162900302, 0.001708403163190399, 0.0017192271842184116, 0.001720878979837408, 0.0017129272051459672, 0.0017170641400224092, 0.001709684591778383, 0.0017049790797184926, 0.001714551693056615, 0.0017309518131826604, 0.0017152906737613435, 0.001784900269870247, 0.0017715555292611219, 0.0017000688144899144, 0.0016929168368176538, 0.0016950578999002368, 0.0017049374267915074, 0.0017020643679235056, 0.001700013714405347, 0.0017072625733835964, 0.0016980987536359805, 0.0016979312438669862, 0.0017027536749231573, 0.001704779182731801, 0.0017044662250852098, 0.0017066913447817977, 0.001702052631358407, 0.0017041727159248323, 0.0017027657346001693, 0.0017336990247119445, 0.0017148611202303851, 0.0017248484667162507, 0.0017177958792189555, 0.001729608057256864, 0.0017401862043735323, 0.0017338736096815187, 0.001730579450460417, 0.0022683148795016564, 0.0017307564492660518, 0.00176357389997919, 0.0017801235924113769, 0.0017410829207118676, 0.0034543235301591306, 0.0017310905897495697, 0.001732819305010596, 0.0017084710832153047, 0.0018827848193444768, 0.001713743613918825, 0.0034307731857181204, 0.0017106484488717147, 0.0017135755719655023, 0.0017207103486800073, 0.0017490018377726783, 0.0017357392008511387, 0.0017330602847267778, 0.0017175910832854559, 0.0017340354697436703, 0.0017450749398950412, 0.0017527570824461933, 0.002533108000738584, 0.0018654989781884514, 0.0018404888378798353, 0.0018873459621503645, 0.0018439500222020612, 0.0034900801588914226, 0.00180773010325371, 0.0017570761859188884, 0.0017945739602175902, 0.0018130833026477877, 0.00307115677706137, 0.0018203928543976983, 0.0018034132235512442, 0.0018257035313136115, 0.0018499701625990625, 0.0018001684481848258, 0.0034329237520922813, 0.0017331297965530231, 0.0017272761443211716, 0.0017468737371798073, 0.001768700859262323, 0.0017334727941042914, 0.0017319758356149708, 0.0017134697338072012]
[581.822188532526, 582.3774023527495, 581.8378655241651, 578.8068360304494, 581.5720500758364, 582.6226856400322, 583.5101585135694, 582.5980102803898, 583.1368433686586, 580.132589027565, 582.1147729354773, 552.3980431256293, 580.6239000472299, 570.7441445531831, 578.8230275334724, 584.4820634473824, 577.8212070758944, 551.3399744909631, 585.3419272137882, 581.6566938793585, 581.0983873453333, 583.7959704275845, 582.3894266331537, 584.9032065966145, 586.5174604752975, 583.2428407085534, 577.7168332383116, 582.9915683078766, 560.2553917887495, 564.4756732051626, 588.2114838392811, 590.6964702884051, 589.9503492233836, 586.5317895460146, 587.5218463212328, 588.2305486869517, 585.7329830748389, 588.8939014051999, 588.9519988586409, 587.2840063288241, 586.5862336479045, 586.6939369537862, 585.929027564062, 587.5258975992421, 586.7949830761802, 587.2798469454827, 576.801385791949, 583.1376011753382, 579.761074260506, 582.1413429252598, 578.165669270752, 574.6511479557444, 576.7433072492994, 577.8411385469486, 440.85590102009894, 577.7820446222009, 567.0303920985675, 561.7587476863828, 574.3551832621132, 289.49228156226957, 577.6705193369841, 577.0942169840871, 585.3186570287289, 531.1281404681002, 583.5178563923547, 291.47948461381094, 584.5736455433411, 583.5750791270804, 581.1553355084549, 571.7546879616099, 576.1234173369128, 577.01397280456, 582.2107541960291, 576.6894723023299, 573.041293034754, 570.5297157346951, 394.77195591677406, 536.0496101536759, 543.3339118491788, 529.8445648304146, 542.3140475389845, 286.5263703048118, 553.1799233746857, 569.12728543813, 557.2353227942475, 551.5466380058884, 325.61020898348534, 549.3319739111279, 554.5040853314919, 547.7340558576288, 540.549258694572, 555.5035702399604, 291.2968863321025, 576.9908301091322, 578.9462230968315, 572.4512188353251, 565.3867327327883, 576.8766625014806, 577.3752609226971, 583.6111255832194]
Elapsed: 0.08995220203418285~0.018120649776975667
Time per graph: 0.0018357592251874047~0.0003698091791219524
Speed: 557.7745563487429~65.37666862745124
Total Time: 0.0845
best val loss: 0.1710403561592102 test_score: 0.8980

Testing...
Test loss: 0.3009 score: 0.8980 time: 0.08s
test Score 0.8980
Epoch Time List: [0.307001403067261, 0.30254742805846035, 0.30269402591511607, 0.30303596379235387, 0.30270572006702423, 0.3023567069321871, 0.30244705895893276, 0.30277024884708226, 0.30252559180371463, 0.31822171923704445, 0.3212687310297042, 0.3255991991609335, 0.3243439821526408, 0.3273403700441122, 0.3223474738188088, 0.3235234091989696, 0.32650245004333556, 0.32774593005888164, 0.32379225525073707, 0.32832981226965785, 0.3251078261528164, 0.32126226578839123, 0.3224751891102642, 0.32318580499850214, 0.3234090602491051, 0.3236947536934167, 0.32903204718604684, 0.3237466199789196, 0.3273947418201715, 0.33094747900031507, 0.3204464379232377, 0.31984001281671226, 0.32112473296001554, 0.32230760995298624, 0.322973309783265, 0.32252215570770204, 0.32239950099028647, 0.32148613478057086, 0.32272136211395264, 0.3229021760635078, 0.32177545316517353, 0.3211888992227614, 0.32238872698508203, 0.32085162471048534, 0.32060786290094256, 0.3220606381073594, 0.3239360502921045, 0.3159878929145634, 0.31931208190508187, 0.28954331181012094, 0.28982838499359787, 0.2910849100444466, 0.30838077375665307, 0.291307375067845, 0.3533948364201933, 0.2960813909303397, 0.29638481908477843, 0.29965495504438877, 0.2987888299394399, 0.37907376303337514, 0.29243211983703077, 0.29323520697653294, 0.28956449404358864, 0.29912427836097777, 0.2929738776292652, 0.3746386170387268, 0.29141208389773965, 0.28827819507569075, 0.29160045296885073, 0.30714633292518556, 0.2944671611767262, 0.32156652421690524, 0.2895432189106941, 0.2918630850035697, 0.2942724348977208, 0.3015187687706202, 0.33553775306791067, 0.32505576801486313, 0.3060982378665358, 0.31650961679406464, 0.3111391051206738, 0.38633208465762436, 0.31111345696263015, 0.3111188253387809, 0.31943189399316907, 0.31050544208846986, 0.36567095085047185, 0.30335404979996383, 0.30310519807972014, 0.30475291330367327, 0.30746367014944553, 0.30375371989794075, 0.3744762500282377, 0.3069356237538159, 0.30840179487131536, 0.3089081197977066, 0.31786272185854614, 0.31297365785576403, 0.32587346574291587, 0.3068519914522767]
Total Epoch List: [47, 53]
Total Time List: [0.0854341711383313, 0.08446112694218755]
========================k_idx:2========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: SAMamba
GraphEncoder(
  (subgraph_conv): GATConv(128, 128, heads=1)
  (embedding): Linear(in_features=14887, out_features=128, bias=False)
  (encoder): GraphMambaEncoder(
    (layers): ModuleList(
      (0-1): 2 x MambaEncoderLayer(
        (self_selen): Selection(
          (khop_structure_extractor): KHopStructureExtractor(
            (structure_extractor): StructureExtractor(
              (gcn): ModuleList(
                (0): GATConv(128, 128, heads=1)
              )
              (relu): ReLU()
              (bn): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (out_proj): Linear(in_features=128, out_features=128, bias=True)
            )
            (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (out_proj): Linear(in_features=256, out_features=128, bias=True)
          )
          (out_proj): Mamba(
            (in_proj): Linear(in_features=128, out_features=512, bias=False)
            (conv1d): Conv1d(256, 256, kernel_size=(4,), stride=(1,), padding=(3,), groups=256)
            (act): SiLU()
            (x_proj): Linear(in_features=256, out_features=40, bias=False)
            (dt_proj): Linear(in_features=8, out_features=256, bias=True)
            (out_proj): Linear(in_features=256, out_features=128, bias=False)
          )
        )
        (norm1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (linear1): Linear(in_features=128, out_features=256, bias=True)
        (dropout): Dropout(p=0.6, inplace=False)
        (linear2): Linear(in_features=256, out_features=128, bias=True)
        (dropout1): Dropout(p=0.6, inplace=False)
        (dropout2): Dropout(p=0.6, inplace=False)
        (activation): ReLU()
      )
    )
  )
  (classifier): Sequential(
    (0): Linear(in_features=128, out_features=128, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=128, out_features=2, bias=True)
  )
)
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7c7a550d0820>
Training...
Epoch 1/1000, LR 0.000500
Train loss: 0.7180;  Loss pred: 0.7042; Loss self: 1.3807; time: 0.16s
Val loss: 0.6928 score: 0.6122 time: 0.08s
Test loss: 0.6931 score: 0.5417 time: 0.08s
Epoch 2/1000, LR 0.000000
Train loss: 0.7095;  Loss pred: 0.6952; Loss self: 1.4274; time: 0.15s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6927 score: 0.5102 time: 0.08s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6931 score: 0.5000 time: 0.08s
Epoch 3/1000, LR 0.000050
Train loss: 0.7002;  Loss pred: 0.6873; Loss self: 1.2895; time: 0.16s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6927 score: 0.5102 time: 0.08s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6932 score: 0.5000 time: 0.07s
Epoch 4/1000, LR 0.000100
Train loss: 0.6642;  Loss pred: 0.6512; Loss self: 1.2967; time: 0.15s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6926 score: 0.5102 time: 0.08s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6932 score: 0.5000 time: 0.08s
Epoch 5/1000, LR 0.000150
Train loss: 0.6237;  Loss pred: 0.6107; Loss self: 1.2986; time: 0.15s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6925 score: 0.5102 time: 0.08s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6932 score: 0.5000 time: 0.08s
Epoch 6/1000, LR 0.000200
Train loss: 0.5659;  Loss pred: 0.5524; Loss self: 1.3425; time: 0.15s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6922 score: 0.5102 time: 0.08s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6930 score: 0.5000 time: 0.08s
Epoch 7/1000, LR 0.000250
Train loss: 0.5194;  Loss pred: 0.5053; Loss self: 1.4119; time: 0.16s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6917 score: 0.5102 time: 0.08s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6926 score: 0.5000 time: 0.08s
Epoch 8/1000, LR 0.000300
Train loss: 0.4645;  Loss pred: 0.4497; Loss self: 1.4726; time: 0.17s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6910 score: 0.5102 time: 0.08s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6921 score: 0.5000 time: 0.08s
Epoch 9/1000, LR 0.000350
Train loss: 0.3988;  Loss pred: 0.3836; Loss self: 1.5200; time: 0.17s
Val loss: 0.6900 score: 0.8776 time: 0.08s
Test loss: 0.6913 score: 0.7083 time: 0.08s
Epoch 10/1000, LR 0.000400
Train loss: 0.3533;  Loss pred: 0.3365; Loss self: 1.6777; time: 0.17s
Val loss: 0.6882 score: 0.6939 time: 0.08s
Test loss: 0.6899 score: 0.6458 time: 0.08s
Epoch 11/1000, LR 0.000450
Train loss: 0.3204;  Loss pred: 0.3021; Loss self: 1.8271; time: 0.17s
Val loss: 0.6861 score: 0.5102 time: 0.08s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6881 score: 0.5000 time: 0.08s
Epoch 12/1000, LR 0.000450
Train loss: 0.2711;  Loss pred: 0.2518; Loss self: 1.9256; time: 0.17s
Val loss: 0.6831 score: 0.5102 time: 0.08s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6855 score: 0.5000 time: 0.08s
Epoch 13/1000, LR 0.000450
Train loss: 0.2359;  Loss pred: 0.2158; Loss self: 2.0090; time: 0.17s
Val loss: 0.6784 score: 0.5102 time: 0.08s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6815 score: 0.5000 time: 0.08s
Epoch 14/1000, LR 0.000450
Train loss: 0.2035;  Loss pred: 0.1825; Loss self: 2.0992; time: 0.17s
Val loss: 0.6713 score: 0.5510 time: 0.08s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6754 score: 0.5000 time: 0.08s
Epoch 15/1000, LR 0.000450
Train loss: 0.1764;  Loss pred: 0.1544; Loss self: 2.1973; time: 0.17s
Val loss: 0.6611 score: 0.6939 time: 0.08s
Test loss: 0.6663 score: 0.7083 time: 0.08s
Epoch 16/1000, LR 0.000450
Train loss: 0.1555;  Loss pred: 0.1331; Loss self: 2.2414; time: 0.17s
Val loss: 0.6464 score: 0.8571 time: 0.08s
Test loss: 0.6527 score: 0.8125 time: 0.08s
Epoch 17/1000, LR 0.000450
Train loss: 0.1337;  Loss pred: 0.1109; Loss self: 2.2840; time: 0.17s
Val loss: 0.6251 score: 0.8980 time: 0.08s
Test loss: 0.6326 score: 0.8750 time: 0.08s
Epoch 18/1000, LR 0.000450
Train loss: 0.1159;  Loss pred: 0.0926; Loss self: 2.3283; time: 0.17s
Val loss: 0.5960 score: 0.9184 time: 0.08s
Test loss: 0.6046 score: 0.9375 time: 0.08s
Epoch 19/1000, LR 0.000450
Train loss: 0.1032;  Loss pred: 0.0793; Loss self: 2.3838; time: 0.17s
Val loss: 0.5583 score: 0.9592 time: 0.08s
Test loss: 0.5671 score: 0.9583 time: 0.08s
Epoch 20/1000, LR 0.000450
Train loss: 0.0934;  Loss pred: 0.0695; Loss self: 2.3929; time: 0.17s
Val loss: 0.5108 score: 0.9592 time: 0.08s
Test loss: 0.5191 score: 0.9583 time: 0.08s
Epoch 21/1000, LR 0.000450
Train loss: 0.0844;  Loss pred: 0.0598; Loss self: 2.4585; time: 0.17s
Val loss: 0.4560 score: 0.9592 time: 0.08s
Test loss: 0.4633 score: 0.9583 time: 0.08s
Epoch 22/1000, LR 0.000450
Train loss: 0.0746;  Loss pred: 0.0495; Loss self: 2.5072; time: 0.18s
Val loss: 0.3971 score: 0.9796 time: 0.08s
Test loss: 0.4017 score: 0.9583 time: 0.08s
Epoch 23/1000, LR 0.000450
Train loss: 0.0666;  Loss pred: 0.0410; Loss self: 2.5503; time: 0.17s
Val loss: 0.3402 score: 0.9796 time: 0.08s
Test loss: 0.3394 score: 0.9583 time: 0.08s
Epoch 24/1000, LR 0.000450
Train loss: 0.0582;  Loss pred: 0.0326; Loss self: 2.5593; time: 0.17s
Val loss: 0.2885 score: 0.9796 time: 0.08s
Test loss: 0.2797 score: 0.9583 time: 0.08s
Epoch 25/1000, LR 0.000450
Train loss: 0.0546;  Loss pred: 0.0284; Loss self: 2.6209; time: 0.17s
Val loss: 0.2489 score: 0.9796 time: 0.08s
Test loss: 0.2293 score: 0.9583 time: 0.08s
Epoch 26/1000, LR 0.000450
Train loss: 0.0482;  Loss pred: 0.0218; Loss self: 2.6457; time: 0.18s
Val loss: 0.2213 score: 0.9592 time: 0.08s
Test loss: 0.1888 score: 0.9583 time: 0.08s
Epoch 27/1000, LR 0.000450
Train loss: 0.0448;  Loss pred: 0.0182; Loss self: 2.6606; time: 0.17s
Val loss: 0.2064 score: 0.9592 time: 0.08s
Test loss: 0.1599 score: 0.9583 time: 0.08s
Epoch 28/1000, LR 0.000450
Train loss: 0.0424;  Loss pred: 0.0154; Loss self: 2.7061; time: 0.17s
Val loss: 0.2016 score: 0.9592 time: 0.08s
Test loss: 0.1398 score: 0.9375 time: 0.08s
Epoch 29/1000, LR 0.000450
Train loss: 0.0401;  Loss pred: 0.0128; Loss self: 2.7283; time: 0.17s
Val loss: 0.2038 score: 0.9592 time: 0.08s
Test loss: 0.1272 score: 0.9375 time: 0.08s
     INFO: Early stopping counter 1 of 20
Epoch 30/1000, LR 0.000450
Train loss: 0.0385;  Loss pred: 0.0108; Loss self: 2.7770; time: 0.18s
Val loss: 0.2103 score: 0.9592 time: 0.08s
Test loss: 0.1197 score: 0.9375 time: 0.08s
     INFO: Early stopping counter 2 of 20
Epoch 31/1000, LR 0.000450
Train loss: 0.0368;  Loss pred: 0.0089; Loss self: 2.7961; time: 0.17s
Val loss: 0.2200 score: 0.9388 time: 0.08s
Test loss: 0.1161 score: 0.9375 time: 0.08s
     INFO: Early stopping counter 3 of 20
Epoch 32/1000, LR 0.000450
Train loss: 0.0357;  Loss pred: 0.0076; Loss self: 2.8101; time: 0.17s
Val loss: 0.2315 score: 0.9388 time: 0.08s
Test loss: 0.1157 score: 0.9375 time: 0.08s
     INFO: Early stopping counter 4 of 20
Epoch 33/1000, LR 0.000449
Train loss: 0.0343;  Loss pred: 0.0063; Loss self: 2.8018; time: 0.17s
Val loss: 0.2434 score: 0.9388 time: 0.08s
Test loss: 0.1175 score: 0.9167 time: 0.08s
     INFO: Early stopping counter 5 of 20
Epoch 34/1000, LR 0.000449
Train loss: 0.0335;  Loss pred: 0.0054; Loss self: 2.8126; time: 0.17s
Val loss: 0.2561 score: 0.9388 time: 0.08s
Test loss: 0.1214 score: 0.9167 time: 0.08s
     INFO: Early stopping counter 6 of 20
Epoch 35/1000, LR 0.000449
Train loss: 0.0326;  Loss pred: 0.0048; Loss self: 2.7799; time: 0.18s
Val loss: 0.2692 score: 0.9388 time: 0.08s
Test loss: 0.1272 score: 0.9167 time: 0.08s
     INFO: Early stopping counter 7 of 20
Epoch 36/1000, LR 0.000449
Train loss: 0.0323;  Loss pred: 0.0041; Loss self: 2.8193; time: 0.18s
Val loss: 0.2819 score: 0.9388 time: 0.08s
Test loss: 0.1340 score: 0.9167 time: 0.08s
     INFO: Early stopping counter 8 of 20
Epoch 37/1000, LR 0.000449
Train loss: 0.0316;  Loss pred: 0.0036; Loss self: 2.7998; time: 0.18s
Val loss: 0.2915 score: 0.9388 time: 0.08s
Test loss: 0.1405 score: 0.9167 time: 0.08s
     INFO: Early stopping counter 9 of 20
Epoch 38/1000, LR 0.000449
Train loss: 0.0313;  Loss pred: 0.0033; Loss self: 2.7977; time: 0.17s
Val loss: 0.2994 score: 0.9388 time: 0.08s
Test loss: 0.1472 score: 0.9167 time: 0.08s
     INFO: Early stopping counter 10 of 20
Epoch 39/1000, LR 0.000449
Train loss: 0.0307;  Loss pred: 0.0029; Loss self: 2.7808; time: 0.17s
Val loss: 0.3043 score: 0.9388 time: 0.08s
Test loss: 0.1530 score: 0.9167 time: 0.08s
     INFO: Early stopping counter 11 of 20
Epoch 40/1000, LR 0.000449
Train loss: 0.0303;  Loss pred: 0.0026; Loss self: 2.7715; time: 0.17s
Val loss: 0.3092 score: 0.9388 time: 0.08s
Test loss: 0.1595 score: 0.9167 time: 0.08s
     INFO: Early stopping counter 12 of 20
Epoch 41/1000, LR 0.000449
Train loss: 0.0300;  Loss pred: 0.0025; Loss self: 2.7574; time: 0.17s
Val loss: 0.3134 score: 0.9388 time: 0.08s
Test loss: 0.1659 score: 0.9167 time: 0.08s
     INFO: Early stopping counter 13 of 20
Epoch 42/1000, LR 0.000449
Train loss: 0.0298;  Loss pred: 0.0023; Loss self: 2.7475; time: 0.17s
Val loss: 0.3172 score: 0.9388 time: 0.08s
Test loss: 0.1725 score: 0.9167 time: 0.08s
     INFO: Early stopping counter 14 of 20
Epoch 43/1000, LR 0.000449
Train loss: 0.0296;  Loss pred: 0.0021; Loss self: 2.7506; time: 0.17s
Val loss: 0.3201 score: 0.9388 time: 0.08s
Test loss: 0.1790 score: 0.9167 time: 0.08s
     INFO: Early stopping counter 15 of 20
Epoch 44/1000, LR 0.000449
Train loss: 0.0290;  Loss pred: 0.0020; Loss self: 2.7022; time: 0.17s
Val loss: 0.3234 score: 0.9388 time: 0.08s
Test loss: 0.1855 score: 0.9167 time: 0.08s
     INFO: Early stopping counter 16 of 20
Epoch 45/1000, LR 0.000449
Train loss: 0.0290;  Loss pred: 0.0020; Loss self: 2.7059; time: 0.17s
Val loss: 0.3264 score: 0.9388 time: 0.08s
Test loss: 0.1914 score: 0.9167 time: 0.08s
     INFO: Early stopping counter 17 of 20
Epoch 46/1000, LR 0.000449
Train loss: 0.0291;  Loss pred: 0.0018; Loss self: 2.7309; time: 0.17s
Val loss: 0.3296 score: 0.9388 time: 0.08s
Test loss: 0.1974 score: 0.9167 time: 0.08s
     INFO: Early stopping counter 18 of 20
Epoch 47/1000, LR 0.000449
Train loss: 0.0289;  Loss pred: 0.0017; Loss self: 2.7151; time: 0.18s
Val loss: 0.3337 score: 0.9388 time: 0.08s
Test loss: 0.2040 score: 0.9167 time: 0.08s
     INFO: Early stopping counter 19 of 20
Epoch 48/1000, LR 0.000448
Train loss: 0.0286;  Loss pred: 0.0017; Loss self: 2.6908; time: 0.18s
Val loss: 0.3375 score: 0.9388 time: 0.08s
Test loss: 0.2097 score: 0.9167 time: 0.08s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 027,   Train_Loss: 0.0424,   Val_Loss: 0.2016,   Val_Precision: 1.0000,   Val_Recall: 0.9200,   Val_accuracy: 0.9583,   Val_Score: 0.9592,   Val_Loss: 0.2016,   Test_Precision: 1.0000,   Test_Recall: 0.8750,   Test_accuracy: 0.9333,   Test_Score: 0.9375,   Test_loss: 0.1398


[0.08421817002817988, 0.08413788001053035, 0.08421590086072683, 0.08465691306628287, 0.08425439288839698, 0.08410245808772743, 0.08397454489022493, 0.08410602016374469, 0.08402830408886075, 0.08446345012634993, 0.08417584002017975, 0.08870415203273296, 0.08439197903499007, 0.08585282997228205, 0.08465454494580626, 0.08383490797132254, 0.08480131812393665, 0.08887438289821148, 0.08371175499632955, 0.08424213202670217, 0.08432307001203299, 0.0839334330521524, 0.08413614286109805, 0.08377454499714077, 0.08354397490620613, 0.08401303295977414, 0.08481663884595037, 0.08404924301430583, 0.08746011322364211, 0.08680622093379498, 0.08330337191000581, 0.08295292500406504, 0.08305783709511161, 0.08354193391278386, 0.08340115402825177, 0.083300672005862, 0.08365586609579623, 0.08320683892816305, 0.08319863094948232, 0.0834349300712347, 0.08353417995385826, 0.08351884502917528, 0.08362787589430809, 0.08340057893656194, 0.08350446308031678, 0.0834355209954083, 0.08495125221088529, 0.08402819489128888, 0.08451757486909628, 0.08417199808172882, 0.08475079480558634, 0.08526912401430309, 0.08495980687439442, 0.08479839307256043, 0.11114742909558117, 0.08480706601403654, 0.08641512109898031, 0.08722605602815747, 0.08531306311488152, 0.1692618529777974, 0.08482343889772892, 0.08490814594551921, 0.08371508307754993, 0.09225645614787936, 0.08397343708202243, 0.1681078861001879, 0.08382177399471402, 0.08396520302630961, 0.08431480708532035, 0.08570109005086124, 0.0850512208417058, 0.08491995395161211, 0.08416196308098733, 0.08496773801743984, 0.08550867205485702, 0.08588509703986347, 0.12412229203619063, 0.09140944993123412, 0.09018395305611193, 0.09247995214536786, 0.090353551087901, 0.1710139277856797, 0.08857877505943179, 0.08609673311002553, 0.08793412405066192, 0.0888410818297416, 0.15048668207600713, 0.08919924986548722, 0.08836724795401096, 0.08945947303436697, 0.09064853796735406, 0.08820825396105647, 0.16821326385252178, 0.08492336003109813, 0.08463653107173741, 0.08559681312181056, 0.08666634210385382, 0.08494016691111028, 0.08486681594513357, 0.08396001695655286, 0.08040621993131936, 0.08079117396846414, 0.08040144084952772, 0.08056061086244881, 0.08105031587183475, 0.08069256017915905, 0.08070654002949595, 0.08056474919430912, 0.08044399111531675, 0.08073269296437502, 0.08089440106414258, 0.08052213303744793, 0.08087991899810731, 0.08097038394771516, 0.08100313087925315, 0.08083459711633623, 0.08040866302326322, 0.08058149088174105, 0.08103064890019596, 0.08077485393732786, 0.08081533620133996, 0.08139194408431649, 0.08077199384570122, 0.08093740697950125, 0.08131425804458559, 0.08116388902999461, 0.08081445889547467, 0.0806378279812634, 0.08079889416694641, 0.08100372785702348, 0.08074542204849422, 0.0805508061312139, 0.0808243399951607, 0.08049721084535122, 0.08063208684325218, 0.08115997817367315, 0.08051777281798422, 0.08088058000430465, 0.08096358110196888, 0.08099063695408404, 0.08118839212693274, 0.08066096506081522, 0.08046694588847458, 0.08058812282979488, 0.08068547700531781, 0.08052249299362302, 0.08087383210659027, 0.08057192200794816]
[0.0017187381638404057, 0.0017170995920516398, 0.0017186918543005477, 0.001727692103393528, 0.0017194774058856526, 0.0017163766956679067, 0.0017137662222494884, 0.0017164493910968304, 0.0017148633487522602, 0.0017237438801295903, 0.0017178742861261173, 0.0018102888169945503, 0.001722285286428369, 0.001752098570862899, 0.0017276437744042094, 0.001710916489210664, 0.0017306391453864624, 0.0018137629162900302, 0.001708403163190399, 0.0017192271842184116, 0.001720878979837408, 0.0017129272051459672, 0.0017170641400224092, 0.001709684591778383, 0.0017049790797184926, 0.001714551693056615, 0.0017309518131826604, 0.0017152906737613435, 0.001784900269870247, 0.0017715555292611219, 0.0017000688144899144, 0.0016929168368176538, 0.0016950578999002368, 0.0017049374267915074, 0.0017020643679235056, 0.001700013714405347, 0.0017072625733835964, 0.0016980987536359805, 0.0016979312438669862, 0.0017027536749231573, 0.001704779182731801, 0.0017044662250852098, 0.0017066913447817977, 0.001702052631358407, 0.0017041727159248323, 0.0017027657346001693, 0.0017336990247119445, 0.0017148611202303851, 0.0017248484667162507, 0.0017177958792189555, 0.001729608057256864, 0.0017401862043735323, 0.0017338736096815187, 0.001730579450460417, 0.0022683148795016564, 0.0017307564492660518, 0.00176357389997919, 0.0017801235924113769, 0.0017410829207118676, 0.0034543235301591306, 0.0017310905897495697, 0.001732819305010596, 0.0017084710832153047, 0.0018827848193444768, 0.001713743613918825, 0.0034307731857181204, 0.0017106484488717147, 0.0017135755719655023, 0.0017207103486800073, 0.0017490018377726783, 0.0017357392008511387, 0.0017330602847267778, 0.0017175910832854559, 0.0017340354697436703, 0.0017450749398950412, 0.0017527570824461933, 0.002533108000738584, 0.0018654989781884514, 0.0018404888378798353, 0.0018873459621503645, 0.0018439500222020612, 0.0034900801588914226, 0.00180773010325371, 0.0017570761859188884, 0.0017945739602175902, 0.0018130833026477877, 0.00307115677706137, 0.0018203928543976983, 0.0018034132235512442, 0.0018257035313136115, 0.0018499701625990625, 0.0018001684481848258, 0.0034329237520922813, 0.0017331297965530231, 0.0017272761443211716, 0.0017468737371798073, 0.001768700859262323, 0.0017334727941042914, 0.0017319758356149708, 0.0017134697338072012, 0.0016751295819024865, 0.0016831494576763362, 0.001675030017698494, 0.0016783460596343502, 0.0016885482473298907, 0.0016810950037324801, 0.001681386250614499, 0.00167843227488144, 0.001675916481569099, 0.0016819311034244795, 0.0016853000221696373, 0.0016775444382801652, 0.001684998312460569, 0.0016868829989107326, 0.0016875652266511072, 0.001684054106590338, 0.001675180479651317, 0.0016787810600362718, 0.0016881385187540825, 0.0016828094570276637, 0.0016836528375279158, 0.0016956655017565936, 0.001682749871785442, 0.0016861959787396092, 0.001694047042595533, 0.0016909143547915544, 0.001683634560322389, 0.001679954749609654, 0.001683310295144717, 0.0016875776636879891, 0.001682196292676963, 0.0016781417944002897, 0.0016838404165658478, 0.001677025225944817, 0.0016798351425677538, 0.0016908328786181908, 0.0016774536003746714, 0.0016850120834230136, 0.0016867412729576852, 0.0016873049365434174, 0.0016914248359777655, 0.001680436772100317, 0.0016763947060098872, 0.0016789192256207268, 0.0016809474376107876, 0.0016775519373671461, 0.0016848715022206306, 0.00167858170849892]
[581.822188532526, 582.3774023527495, 581.8378655241651, 578.8068360304494, 581.5720500758364, 582.6226856400322, 583.5101585135694, 582.5980102803898, 583.1368433686586, 580.132589027565, 582.1147729354773, 552.3980431256293, 580.6239000472299, 570.7441445531831, 578.8230275334724, 584.4820634473824, 577.8212070758944, 551.3399744909631, 585.3419272137882, 581.6566938793585, 581.0983873453333, 583.7959704275845, 582.3894266331537, 584.9032065966145, 586.5174604752975, 583.2428407085534, 577.7168332383116, 582.9915683078766, 560.2553917887495, 564.4756732051626, 588.2114838392811, 590.6964702884051, 589.9503492233836, 586.5317895460146, 587.5218463212328, 588.2305486869517, 585.7329830748389, 588.8939014051999, 588.9519988586409, 587.2840063288241, 586.5862336479045, 586.6939369537862, 585.929027564062, 587.5258975992421, 586.7949830761802, 587.2798469454827, 576.801385791949, 583.1376011753382, 579.761074260506, 582.1413429252598, 578.165669270752, 574.6511479557444, 576.7433072492994, 577.8411385469486, 440.85590102009894, 577.7820446222009, 567.0303920985675, 561.7587476863828, 574.3551832621132, 289.49228156226957, 577.6705193369841, 577.0942169840871, 585.3186570287289, 531.1281404681002, 583.5178563923547, 291.47948461381094, 584.5736455433411, 583.5750791270804, 581.1553355084549, 571.7546879616099, 576.1234173369128, 577.01397280456, 582.2107541960291, 576.6894723023299, 573.041293034754, 570.5297157346951, 394.77195591677406, 536.0496101536759, 543.3339118491788, 529.8445648304146, 542.3140475389845, 286.5263703048118, 553.1799233746857, 569.12728543813, 557.2353227942475, 551.5466380058884, 325.61020898348534, 549.3319739111279, 554.5040853314919, 547.7340558576288, 540.549258694572, 555.5035702399604, 291.2968863321025, 576.9908301091322, 578.9462230968315, 572.4512188353251, 565.3867327327883, 576.8766625014806, 577.3752609226971, 583.6111255832194, 596.9687424803727, 594.1243039584525, 597.0042264520183, 595.8246776697906, 592.2247123120733, 594.8503789373788, 594.7473399610163, 595.7940722217329, 596.6884453954035, 594.5546746617383, 593.3661584556385, 596.1093948874521, 593.472404455836, 592.8093416352692, 592.5696880970061, 593.8051491853042, 596.9506045152499, 595.6702894768148, 592.368451338959, 594.2443428897137, 593.9466722060624, 589.7389543893346, 594.2653847532156, 593.0508746364559, 590.302379364773, 591.3960084177498, 593.9531199742752, 595.2541282628922, 594.0675363801707, 592.565321002546, 594.4609462957799, 595.8972020939183, 593.8805068234886, 596.2939522490553, 595.2965113418363, 591.4245060205097, 596.1416755590991, 593.4675542317492, 592.8591515677501, 592.6611001616471, 591.2175218959275, 595.0833834409231, 596.5182283235522, 595.6212691711133, 594.9025993467997, 596.1067301257223, 593.5170715879625, 595.7410324066112]
Elapsed: 0.08697595285264322~0.015502796938209957
Time per graph: 0.001786157812453893~0.0003123114820610341
Speed: 569.602934837111~56.39516007635464
Total Time: 0.0813
best val loss: 0.20164312422275543 test_score: 0.9375

Testing...
Test loss: 0.4017 score: 0.9583 time: 0.07s
test Score 0.9583
Epoch Time List: [0.307001403067261, 0.30254742805846035, 0.30269402591511607, 0.30303596379235387, 0.30270572006702423, 0.3023567069321871, 0.30244705895893276, 0.30277024884708226, 0.30252559180371463, 0.31822171923704445, 0.3212687310297042, 0.3255991991609335, 0.3243439821526408, 0.3273403700441122, 0.3223474738188088, 0.3235234091989696, 0.32650245004333556, 0.32774593005888164, 0.32379225525073707, 0.32832981226965785, 0.3251078261528164, 0.32126226578839123, 0.3224751891102642, 0.32318580499850214, 0.3234090602491051, 0.3236947536934167, 0.32903204718604684, 0.3237466199789196, 0.3273947418201715, 0.33094747900031507, 0.3204464379232377, 0.31984001281671226, 0.32112473296001554, 0.32230760995298624, 0.322973309783265, 0.32252215570770204, 0.32239950099028647, 0.32148613478057086, 0.32272136211395264, 0.3229021760635078, 0.32177545316517353, 0.3211888992227614, 0.32238872698508203, 0.32085162471048534, 0.32060786290094256, 0.3220606381073594, 0.3239360502921045, 0.3159878929145634, 0.31931208190508187, 0.28954331181012094, 0.28982838499359787, 0.2910849100444466, 0.30838077375665307, 0.291307375067845, 0.3533948364201933, 0.2960813909303397, 0.29638481908477843, 0.29965495504438877, 0.2987888299394399, 0.37907376303337514, 0.29243211983703077, 0.29323520697653294, 0.28956449404358864, 0.29912427836097777, 0.2929738776292652, 0.3746386170387268, 0.29141208389773965, 0.28827819507569075, 0.29160045296885073, 0.30714633292518556, 0.2944671611767262, 0.32156652421690524, 0.2895432189106941, 0.2918630850035697, 0.2942724348977208, 0.3015187687706202, 0.33553775306791067, 0.32505576801486313, 0.3060982378665358, 0.31650961679406464, 0.3111391051206738, 0.38633208465762436, 0.31111345696263015, 0.3111188253387809, 0.31943189399316907, 0.31050544208846986, 0.36567095085047185, 0.30335404979996383, 0.30310519807972014, 0.30475291330367327, 0.30746367014944553, 0.30375371989794075, 0.3744762500282377, 0.3069356237538159, 0.30840179487131536, 0.3089081197977066, 0.31786272185854614, 0.31297365785576403, 0.32587346574291587, 0.3068519914522767, 0.3096346380189061, 0.30443333415314555, 0.30806382023729384, 0.303671051049605, 0.3042245109099895, 0.30515549913980067, 0.3137227101251483, 0.32051860517822206, 0.32355238194577396, 0.32291892101056874, 0.32306016399525106, 0.3235311880707741, 0.32522827200591564, 0.3254400617443025, 0.32537915580905974, 0.3262416049838066, 0.325468844268471, 0.325411768630147, 0.32504407688975334, 0.3253713580779731, 0.3254313657525927, 0.32665294874459505, 0.32573689916171134, 0.32621289789676666, 0.3265406619757414, 0.32823019311763346, 0.32632331061176956, 0.3260966008529067, 0.3262019429821521, 0.3283634837716818, 0.3259900368284434, 0.32574446382932365, 0.3251580309588462, 0.32410099683329463, 0.3270471547730267, 0.328392222058028, 0.32668319600634277, 0.3257971580605954, 0.32609899900853634, 0.32672305405139923, 0.32580753113143146, 0.32500524283386767, 0.32422584271989763, 0.3245427357032895, 0.3244560109451413, 0.32462558010593057, 0.32720244699157774, 0.32669323799200356]
Total Epoch List: [47, 53, 48]
Total Time List: [0.0854341711383313, 0.08446112694218755, 0.08125631092116237]
========================training times:2========================
========================k_idx:0========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: SAMamba
GraphEncoder(
  (subgraph_conv): GATConv(128, 128, heads=1)
  (embedding): Linear(in_features=14887, out_features=128, bias=False)
  (encoder): GraphMambaEncoder(
    (layers): ModuleList(
      (0-1): 2 x MambaEncoderLayer(
        (self_selen): Selection(
          (khop_structure_extractor): KHopStructureExtractor(
            (structure_extractor): StructureExtractor(
              (gcn): ModuleList(
                (0): GATConv(128, 128, heads=1)
              )
              (relu): ReLU()
              (bn): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (out_proj): Linear(in_features=128, out_features=128, bias=True)
            )
            (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (out_proj): Linear(in_features=256, out_features=128, bias=True)
          )
          (out_proj): Mamba(
            (in_proj): Linear(in_features=128, out_features=512, bias=False)
            (conv1d): Conv1d(256, 256, kernel_size=(4,), stride=(1,), padding=(3,), groups=256)
            (act): SiLU()
            (x_proj): Linear(in_features=256, out_features=40, bias=False)
            (dt_proj): Linear(in_features=8, out_features=256, bias=True)
            (out_proj): Linear(in_features=256, out_features=128, bias=False)
          )
        )
        (norm1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (linear1): Linear(in_features=128, out_features=256, bias=True)
        (dropout): Dropout(p=0.6, inplace=False)
        (linear2): Linear(in_features=256, out_features=128, bias=True)
        (dropout1): Dropout(p=0.6, inplace=False)
        (dropout2): Dropout(p=0.6, inplace=False)
        (activation): ReLU()
      )
    )
  )
  (classifier): Sequential(
    (0): Linear(in_features=128, out_features=128, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=128, out_features=2, bias=True)
  )
)
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7c7a550d1de0>
Training...
Epoch 1/1000, LR 0.000500
Train loss: 0.7198;  Loss pred: 0.7071; Loss self: 1.2696; time: 0.15s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6929 score: 0.5102 time: 0.08s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6939 score: 0.4898 time: 0.08s
Epoch 2/1000, LR 0.000000
Train loss: 0.7060;  Loss pred: 0.6931; Loss self: 1.2910; time: 0.15s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6929 score: 0.5102 time: 0.08s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6933 score: 0.4898 time: 0.08s
Epoch 3/1000, LR 0.000050
Train loss: 0.7114;  Loss pred: 0.6987; Loss self: 1.2779; time: 0.15s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6929 score: 0.5102 time: 0.08s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6930 score: 0.4898 time: 0.08s
Epoch 4/1000, LR 0.000100
Train loss: 0.6807;  Loss pred: 0.6672; Loss self: 1.3444; time: 0.15s
Val loss: 0.6928 score: 0.6735 time: 0.08s
Test loss: 0.6928 score: 0.6939 time: 0.08s
Epoch 5/1000, LR 0.000150
Train loss: 0.6510;  Loss pred: 0.6378; Loss self: 1.3225; time: 0.15s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6926 score: 0.4898 time: 0.08s
Test loss: 0.6926 score: 0.4898 time: 0.08s
Epoch 6/1000, LR 0.000200
Train loss: 0.6202;  Loss pred: 0.6070; Loss self: 1.3125; time: 0.15s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6925 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6923 score: 0.5102 time: 0.08s
Epoch 7/1000, LR 0.000250
Train loss: 0.5672;  Loss pred: 0.5537; Loss self: 1.3467; time: 0.15s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6922 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6921 score: 0.5102 time: 0.08s
Epoch 8/1000, LR 0.000300
Train loss: 0.5143;  Loss pred: 0.5009; Loss self: 1.3410; time: 0.15s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6918 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6918 score: 0.5102 time: 0.08s
Epoch 9/1000, LR 0.000350
Train loss: 0.4541;  Loss pred: 0.4396; Loss self: 1.4418; time: 0.15s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6911 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6912 score: 0.5102 time: 0.08s
Epoch 10/1000, LR 0.000400
Train loss: 0.4071;  Loss pred: 0.3914; Loss self: 1.5698; time: 0.15s
Val loss: 0.6898 score: 0.5510 time: 0.08s
Test loss: 0.6902 score: 0.5510 time: 0.08s
Epoch 11/1000, LR 0.000450
Train loss: 0.3490;  Loss pred: 0.3324; Loss self: 1.6600; time: 0.15s
Val loss: 0.6877 score: 0.7143 time: 0.08s
Test loss: 0.6883 score: 0.6939 time: 0.08s
Epoch 12/1000, LR 0.000450
Train loss: 0.2994;  Loss pred: 0.2824; Loss self: 1.7012; time: 0.15s
Val loss: 0.6847 score: 0.8367 time: 0.08s
Test loss: 0.6858 score: 0.7755 time: 0.08s
Epoch 13/1000, LR 0.000450
Train loss: 0.2571;  Loss pred: 0.2392; Loss self: 1.7862; time: 0.15s
Val loss: 0.6803 score: 0.8571 time: 0.08s
Test loss: 0.6821 score: 0.8776 time: 0.08s
Epoch 14/1000, LR 0.000450
Train loss: 0.2171;  Loss pred: 0.1981; Loss self: 1.9017; time: 0.15s
Val loss: 0.6743 score: 0.8571 time: 0.08s
Test loss: 0.6769 score: 0.8367 time: 0.08s
Epoch 15/1000, LR 0.000450
Train loss: 0.1852;  Loss pred: 0.1651; Loss self: 2.0079; time: 0.16s
Val loss: 0.6663 score: 0.8571 time: 0.08s
Test loss: 0.6698 score: 0.7551 time: 0.08s
Epoch 16/1000, LR 0.000450
Train loss: 0.1620;  Loss pred: 0.1409; Loss self: 2.1031; time: 0.15s
Val loss: 0.6548 score: 0.8571 time: 0.08s
Test loss: 0.6597 score: 0.7551 time: 0.08s
Epoch 17/1000, LR 0.000450
Train loss: 0.1398;  Loss pred: 0.1177; Loss self: 2.2035; time: 0.15s
Val loss: 0.6384 score: 0.8571 time: 0.08s
Test loss: 0.6454 score: 0.7755 time: 0.08s
Epoch 18/1000, LR 0.000450
Train loss: 0.1245;  Loss pred: 0.1017; Loss self: 2.2805; time: 0.16s
Val loss: 0.6157 score: 0.8571 time: 0.09s
Test loss: 0.6256 score: 0.7959 time: 0.08s
Epoch 19/1000, LR 0.000450
Train loss: 0.1081;  Loss pred: 0.0849; Loss self: 2.3178; time: 0.15s
Val loss: 0.5860 score: 0.8571 time: 0.08s
Test loss: 0.5995 score: 0.7959 time: 0.08s
Epoch 20/1000, LR 0.000450
Train loss: 0.0964;  Loss pred: 0.0721; Loss self: 2.4223; time: 0.15s
Val loss: 0.5457 score: 0.8776 time: 0.08s
Test loss: 0.5648 score: 0.7959 time: 0.08s
Epoch 21/1000, LR 0.000450
Train loss: 0.0824;  Loss pred: 0.0578; Loss self: 2.4582; time: 0.15s
Val loss: 0.4956 score: 0.8980 time: 0.08s
Test loss: 0.5217 score: 0.8163 time: 0.08s
Epoch 22/1000, LR 0.000450
Train loss: 0.0738;  Loss pred: 0.0485; Loss self: 2.5267; time: 0.15s
Val loss: 0.4385 score: 0.8980 time: 0.09s
Test loss: 0.4719 score: 0.8367 time: 0.08s
Epoch 23/1000, LR 0.000450
Train loss: 0.0647;  Loss pred: 0.0395; Loss self: 2.5298; time: 0.15s
Val loss: 0.3776 score: 0.8980 time: 0.08s
Test loss: 0.4177 score: 0.9388 time: 0.08s
Epoch 24/1000, LR 0.000450
Train loss: 0.0581;  Loss pred: 0.0324; Loss self: 2.5775; time: 0.15s
Val loss: 0.3214 score: 0.9184 time: 0.08s
Test loss: 0.3668 score: 0.9388 time: 0.09s
Epoch 25/1000, LR 0.000450
Train loss: 0.0540;  Loss pred: 0.0276; Loss self: 2.6380; time: 0.15s
Val loss: 0.2718 score: 0.9184 time: 0.09s
Test loss: 0.3216 score: 0.9388 time: 0.08s
Epoch 26/1000, LR 0.000450
Train loss: 0.0494;  Loss pred: 0.0227; Loss self: 2.6633; time: 0.15s
Val loss: 0.2308 score: 0.9184 time: 0.08s
Test loss: 0.2856 score: 0.9388 time: 0.08s
Epoch 27/1000, LR 0.000450
Train loss: 0.0456;  Loss pred: 0.0189; Loss self: 2.6705; time: 0.15s
Val loss: 0.1992 score: 0.9184 time: 0.08s
Test loss: 0.2604 score: 0.9592 time: 0.08s
Epoch 28/1000, LR 0.000450
Train loss: 0.0428;  Loss pred: 0.0160; Loss self: 2.6846; time: 0.15s
Val loss: 0.1761 score: 0.9184 time: 0.08s
Test loss: 0.2448 score: 0.9388 time: 0.08s
Epoch 29/1000, LR 0.000450
Train loss: 0.0407;  Loss pred: 0.0134; Loss self: 2.7259; time: 0.16s
Val loss: 0.1598 score: 0.9184 time: 0.09s
Test loss: 0.2368 score: 0.9388 time: 0.08s
Epoch 30/1000, LR 0.000450
Train loss: 0.0385;  Loss pred: 0.0110; Loss self: 2.7516; time: 0.16s
Val loss: 0.1499 score: 0.9184 time: 0.08s
Test loss: 0.2344 score: 0.9388 time: 0.08s
Epoch 31/1000, LR 0.000450
Train loss: 0.0368;  Loss pred: 0.0092; Loss self: 2.7615; time: 0.15s
Val loss: 0.1439 score: 0.9184 time: 0.09s
Test loss: 0.2353 score: 0.9388 time: 0.08s
Epoch 32/1000, LR 0.000450
Train loss: 0.0354;  Loss pred: 0.0076; Loss self: 2.7795; time: 0.15s
Val loss: 0.1405 score: 0.9184 time: 0.09s
Test loss: 0.2379 score: 0.9388 time: 0.08s
Epoch 33/1000, LR 0.000449
Train loss: 0.0340;  Loss pred: 0.0064; Loss self: 2.7655; time: 0.15s
Val loss: 0.1386 score: 0.9388 time: 0.08s
Test loss: 0.2416 score: 0.9388 time: 0.08s
Epoch 34/1000, LR 0.000449
Train loss: 0.0336;  Loss pred: 0.0058; Loss self: 2.7805; time: 0.13s
Val loss: 0.1374 score: 0.9388 time: 0.08s
Test loss: 0.2462 score: 0.9388 time: 0.08s
Epoch 35/1000, LR 0.000449
Train loss: 0.0328;  Loss pred: 0.0050; Loss self: 2.7817; time: 0.15s
Val loss: 0.1366 score: 0.9388 time: 0.08s
Test loss: 0.2511 score: 0.9388 time: 0.08s
Epoch 36/1000, LR 0.000449
Train loss: 0.0322;  Loss pred: 0.0041; Loss self: 2.8064; time: 0.14s
Val loss: 0.1362 score: 0.9388 time: 0.09s
Test loss: 0.2557 score: 0.9388 time: 0.08s
Epoch 37/1000, LR 0.000449
Train loss: 0.0319;  Loss pred: 0.0040; Loss self: 2.7980; time: 0.15s
Val loss: 0.1347 score: 0.9388 time: 0.08s
Test loss: 0.2596 score: 0.9388 time: 0.08s
Epoch 38/1000, LR 0.000449
Train loss: 0.0316;  Loss pred: 0.0037; Loss self: 2.7875; time: 0.15s
Val loss: 0.1327 score: 0.9592 time: 0.09s
Test loss: 0.2615 score: 0.9388 time: 0.08s
Epoch 39/1000, LR 0.000449
Train loss: 0.0309;  Loss pred: 0.0029; Loss self: 2.8038; time: 0.16s
Val loss: 0.1314 score: 0.9592 time: 0.08s
Test loss: 0.2618 score: 0.9388 time: 0.08s
Epoch 40/1000, LR 0.000449
Train loss: 0.0307;  Loss pred: 0.0028; Loss self: 2.7897; time: 0.15s
Val loss: 0.1282 score: 0.9388 time: 0.08s
Test loss: 0.2659 score: 0.9388 time: 0.08s
Epoch 41/1000, LR 0.000449
Train loss: 0.0304;  Loss pred: 0.0025; Loss self: 2.7903; time: 0.15s
Val loss: 0.1250 score: 0.9388 time: 0.08s
Test loss: 0.2692 score: 0.9388 time: 0.08s
Epoch 42/1000, LR 0.000449
Train loss: 0.0299;  Loss pred: 0.0023; Loss self: 2.7664; time: 0.16s
Val loss: 0.1233 score: 0.9388 time: 0.08s
Test loss: 0.2722 score: 0.9388 time: 0.08s
Epoch 43/1000, LR 0.000449
Train loss: 0.0297;  Loss pred: 0.0024; Loss self: 2.7303; time: 0.15s
Val loss: 0.1190 score: 0.9388 time: 0.08s
Test loss: 0.2764 score: 0.9388 time: 0.08s
Epoch 44/1000, LR 0.000449
Train loss: 0.0293;  Loss pred: 0.0020; Loss self: 2.7361; time: 0.15s
Val loss: 0.1151 score: 0.9388 time: 0.09s
Test loss: 0.2790 score: 0.9388 time: 0.08s
Epoch 45/1000, LR 0.000449
Train loss: 0.0294;  Loss pred: 0.0020; Loss self: 2.7395; time: 0.15s
Val loss: 0.1106 score: 0.9388 time: 0.09s
Test loss: 0.2809 score: 0.9388 time: 0.08s
Epoch 46/1000, LR 0.000449
Train loss: 0.0288;  Loss pred: 0.0016; Loss self: 2.7112; time: 0.15s
Val loss: 0.1063 score: 0.9388 time: 0.08s
Test loss: 0.2833 score: 0.9388 time: 0.08s
Epoch 47/1000, LR 0.000449
Train loss: 0.0287;  Loss pred: 0.0017; Loss self: 2.6990; time: 0.15s
Val loss: 0.1020 score: 0.9388 time: 0.09s
Test loss: 0.2863 score: 0.9388 time: 0.08s
Epoch 48/1000, LR 0.000448
Train loss: 0.0288;  Loss pred: 0.0018; Loss self: 2.7066; time: 0.15s
Val loss: 0.1004 score: 0.9388 time: 0.08s
Test loss: 0.2879 score: 0.9388 time: 0.08s
Epoch 49/1000, LR 0.000448
Train loss: 0.0284;  Loss pred: 0.0014; Loss self: 2.6916; time: 0.15s
Val loss: 0.0982 score: 0.9388 time: 0.08s
Test loss: 0.2890 score: 0.9388 time: 0.08s
Epoch 50/1000, LR 0.000448
Train loss: 0.0284;  Loss pred: 0.0015; Loss self: 2.6906; time: 0.15s
Val loss: 0.0969 score: 0.9388 time: 0.08s
Test loss: 0.2907 score: 0.9388 time: 0.08s
Epoch 51/1000, LR 0.000448
Train loss: 0.0280;  Loss pred: 0.0014; Loss self: 2.6618; time: 0.15s
Val loss: 0.0955 score: 0.9388 time: 0.09s
Test loss: 0.2920 score: 0.9388 time: 0.08s
Epoch 52/1000, LR 0.000448
Train loss: 0.0278;  Loss pred: 0.0015; Loss self: 2.6327; time: 0.15s
Val loss: 0.0942 score: 0.9388 time: 0.09s
Test loss: 0.2948 score: 0.9388 time: 0.08s
Epoch 53/1000, LR 0.000448
Train loss: 0.0276;  Loss pred: 0.0013; Loss self: 2.6318; time: 0.15s
Val loss: 0.0936 score: 0.9388 time: 0.08s
Test loss: 0.2970 score: 0.9388 time: 0.08s
Epoch 54/1000, LR 0.000448
Train loss: 0.0275;  Loss pred: 0.0013; Loss self: 2.6265; time: 0.15s
Val loss: 0.0934 score: 0.9388 time: 0.08s
Test loss: 0.2983 score: 0.9388 time: 0.08s
Epoch 55/1000, LR 0.000448
Train loss: 0.0274;  Loss pred: 0.0014; Loss self: 2.5967; time: 0.15s
Val loss: 0.0932 score: 0.9388 time: 0.08s
Test loss: 0.3004 score: 0.9388 time: 0.08s
Epoch 56/1000, LR 0.000448
Train loss: 0.0271;  Loss pred: 0.0014; Loss self: 2.5756; time: 0.15s
Val loss: 0.0946 score: 0.9388 time: 0.08s
Test loss: 0.3000 score: 0.9388 time: 0.08s
     INFO: Early stopping counter 1 of 20
Epoch 57/1000, LR 0.000448
Train loss: 0.0269;  Loss pred: 0.0014; Loss self: 2.5460; time: 0.15s
Val loss: 0.0956 score: 0.9388 time: 0.08s
Test loss: 0.3001 score: 0.9388 time: 0.08s
     INFO: Early stopping counter 2 of 20
Epoch 58/1000, LR 0.000448
Train loss: 0.0267;  Loss pred: 0.0013; Loss self: 2.5371; time: 0.15s
Val loss: 0.0970 score: 0.9388 time: 0.08s
Test loss: 0.2995 score: 0.9388 time: 0.08s
     INFO: Early stopping counter 3 of 20
Epoch 59/1000, LR 0.000447
Train loss: 0.0263;  Loss pred: 0.0013; Loss self: 2.5011; time: 0.15s
Val loss: 0.0981 score: 0.9388 time: 0.08s
Test loss: 0.2993 score: 0.9388 time: 0.08s
     INFO: Early stopping counter 4 of 20
Epoch 60/1000, LR 0.000447
Train loss: 0.0265;  Loss pred: 0.0014; Loss self: 2.5124; time: 0.15s
Val loss: 0.1002 score: 0.9388 time: 0.08s
Test loss: 0.3008 score: 0.9388 time: 0.08s
     INFO: Early stopping counter 5 of 20
Epoch 61/1000, LR 0.000447
Train loss: 0.0261;  Loss pred: 0.0014; Loss self: 2.4755; time: 0.15s
Val loss: 0.1024 score: 0.9388 time: 0.08s
Test loss: 0.3018 score: 0.9388 time: 0.08s
     INFO: Early stopping counter 6 of 20
Epoch 62/1000, LR 0.000447
Train loss: 0.0258;  Loss pred: 0.0013; Loss self: 2.4516; time: 0.15s
Val loss: 0.1050 score: 0.9388 time: 0.08s
Test loss: 0.3021 score: 0.9184 time: 0.08s
     INFO: Early stopping counter 7 of 20
Epoch 63/1000, LR 0.000447
Train loss: 0.0257;  Loss pred: 0.0013; Loss self: 2.4447; time: 0.15s
Val loss: 0.1080 score: 0.9388 time: 0.08s
Test loss: 0.3017 score: 0.9184 time: 0.08s
     INFO: Early stopping counter 8 of 20
Epoch 64/1000, LR 0.000447
Train loss: 0.0256;  Loss pred: 0.0013; Loss self: 2.4284; time: 0.15s
Val loss: 0.1116 score: 0.9388 time: 0.08s
Test loss: 0.3024 score: 0.9184 time: 0.08s
     INFO: Early stopping counter 9 of 20
Epoch 65/1000, LR 0.000447
Train loss: 0.0256;  Loss pred: 0.0015; Loss self: 2.4061; time: 0.15s
Val loss: 0.1166 score: 0.9388 time: 0.09s
Test loss: 0.3023 score: 0.9184 time: 0.08s
     INFO: Early stopping counter 10 of 20
Epoch 66/1000, LR 0.000447
Train loss: 0.0252;  Loss pred: 0.0014; Loss self: 2.3844; time: 0.15s
Val loss: 0.1207 score: 0.9388 time: 0.08s
Test loss: 0.3023 score: 0.9184 time: 0.08s
     INFO: Early stopping counter 11 of 20
Epoch 67/1000, LR 0.000446
Train loss: 0.0252;  Loss pred: 0.0015; Loss self: 2.3690; time: 0.23s
Val loss: 0.1264 score: 0.9388 time: 0.08s
Test loss: 0.3013 score: 0.9184 time: 0.08s
     INFO: Early stopping counter 12 of 20
Epoch 68/1000, LR 0.000446
Train loss: 0.0250;  Loss pred: 0.0014; Loss self: 2.3562; time: 0.15s
Val loss: 0.1317 score: 0.9388 time: 0.08s
Test loss: 0.3012 score: 0.9184 time: 0.08s
     INFO: Early stopping counter 13 of 20
Epoch 69/1000, LR 0.000446
Train loss: 0.0246;  Loss pred: 0.0013; Loss self: 2.3303; time: 0.14s
Val loss: 0.1364 score: 0.9388 time: 0.08s
Test loss: 0.3011 score: 0.9184 time: 0.08s
     INFO: Early stopping counter 14 of 20
Epoch 70/1000, LR 0.000446
Train loss: 0.0246;  Loss pred: 0.0014; Loss self: 2.3185; time: 0.15s
Val loss: 0.1415 score: 0.9388 time: 0.09s
Test loss: 0.3005 score: 0.9184 time: 0.08s
     INFO: Early stopping counter 15 of 20
Epoch 71/1000, LR 0.000446
Train loss: 0.0247;  Loss pred: 0.0017; Loss self: 2.2991; time: 0.15s
Val loss: 0.1480 score: 0.9388 time: 0.08s
Test loss: 0.3002 score: 0.9184 time: 0.08s
     INFO: Early stopping counter 16 of 20
Epoch 72/1000, LR 0.000446
Train loss: 0.0244;  Loss pred: 0.0017; Loss self: 2.2770; time: 0.14s
Val loss: 0.1554 score: 0.9388 time: 0.10s
Test loss: 0.3003 score: 0.9184 time: 0.08s
     INFO: Early stopping counter 17 of 20
Epoch 73/1000, LR 0.000446
Train loss: 0.0239;  Loss pred: 0.0013; Loss self: 2.2588; time: 0.14s
Val loss: 0.1625 score: 0.9388 time: 0.08s
Test loss: 0.2998 score: 0.9184 time: 0.08s
     INFO: Early stopping counter 18 of 20
Epoch 74/1000, LR 0.000446
Train loss: 0.0241;  Loss pred: 0.0016; Loss self: 2.2431; time: 0.15s
Val loss: 0.1713 score: 0.9388 time: 0.08s
Test loss: 0.2991 score: 0.9184 time: 0.08s
     INFO: Early stopping counter 19 of 20
Epoch 75/1000, LR 0.000445
Train loss: 0.0240;  Loss pred: 0.0016; Loss self: 2.2439; time: 0.15s
Val loss: 0.1797 score: 0.9388 time: 0.09s
Test loss: 0.3001 score: 0.9184 time: 0.08s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 054,   Train_Loss: 0.0274,   Val_Loss: 0.0932,   Val_Precision: 0.9200,   Val_Recall: 0.9583,   Val_accuracy: 0.9388,   Val_Score: 0.9388,   Val_Loss: 0.0932,   Test_Precision: 1.0000,   Test_Recall: 0.8800,   Test_accuracy: 0.9362,   Test_Score: 0.9388,   Test_loss: 0.3004


[0.0849194920156151, 0.08526470907963812, 0.0856269400101155, 0.08529319404624403, 0.08524638693779707, 0.08547334093600512, 0.08551853196695447, 0.08584413910284638, 0.08577385684475303, 0.08532639499753714, 0.08542936202138662, 0.08569990703836083, 0.08579863491468132, 0.0852697379887104, 0.08591991802677512, 0.08567794901318848, 0.0853079641237855, 0.08702576695941389, 0.08576511312276125, 0.08565473696216941, 0.08541352697648108, 0.08585179387591779, 0.08786247903481126, 0.0905315619893372, 0.08528041490353644, 0.08522506197914481, 0.08746798313222826, 0.08525330410338938, 0.08777933102101088, 0.08591035404242575, 0.08657759288325906, 0.08694390300661325, 0.08535592490807176, 0.08553929417394102, 0.08528749807737768, 0.08678138093091547, 0.08509742794558406, 0.08635102608241141, 0.0855020759627223, 0.08485903707332909, 0.08566657616756856, 0.08513618912547827, 0.08497368101961911, 0.0852526999078691, 0.08639178494922817, 0.08964263694360852, 0.0849990900605917, 0.08625442604534328, 0.08466539112851024, 0.08514185482636094, 0.0880430000834167, 0.08729011192917824, 0.08520060894079506, 0.08539186301641166, 0.08502369001507759, 0.08433164493180811, 0.08516438980586827, 0.08514932077378035, 0.08500435296446085, 0.08528564497828484, 0.0853451369330287, 0.08535423898138106, 0.08521799114532769, 0.08564380998723209, 0.0857116081751883, 0.08772572502493858, 0.08451230893842876, 0.08436052082106471, 0.08506609802134335, 0.08590669091790915, 0.08417939115315676, 0.08494078204967082, 0.08532176585868001, 0.08573446795344353, 0.0853939529042691]
[0.0017330508574615329, 0.0017400961036660842, 0.0017474885716350103, 0.0017406774295151842, 0.0017397221824040218, 0.0017443538966531657, 0.0017452761625909076, 0.0017519212061805384, 0.0017504868743827148, 0.0017413549999497375, 0.0017434563677834005, 0.0017489776946604252, 0.0017509925492792105, 0.0017401987344634775, 0.0017534677148321454, 0.001748529571697724, 0.0017409788596690918, 0.0017760360603962019, 0.0017503084310767601, 0.0017480558563708042, 0.0017431332036016546, 0.0017520774260391385, 0.0017931118170369644, 0.0018475828977415757, 0.001740416630684417, 0.0017392869791662206, 0.0017850608802495562, 0.001739863349048763, 0.0017914149187961404, 0.0017532725314780766, 0.0017668896506787563, 0.0017743653674819032, 0.0017419576511851379, 0.0017456998811008371, 0.0017405611852526056, 0.0017710485904268464, 0.0017366822029711033, 0.0017622658384165593, 0.001744940325769843, 0.001731817083129165, 0.0017482974728075217, 0.0017374732474587401, 0.0017341567555024307, 0.0017398510185279408, 0.0017630976520250647, 0.001829441570277725, 0.0017346753073590143, 0.0017602944090886383, 0.0017278651250716374, 0.0017375888740073662, 0.0017967959200697286, 0.001781430855697515, 0.0017387879375672462, 0.0017426910819675848, 0.0017351773472464814, 0.0017210539782001656, 0.001738048771548332, 0.0017377412402812317, 0.0017347827135604254, 0.0017405233669037722, 0.001741737488429157, 0.0017419232445179808, 0.0017391426764352589, 0.0017478328568822875, 0.0017492164933711899, 0.0017903209188762977, 0.001724740998743444, 0.0017216432820625451, 0.0017360428167621093, 0.0017531977738348805, 0.001717946758227689, 0.0017334853479524656, 0.0017412605277281634, 0.0017496830194580312, 0.0017427337327401858]
[577.0171115836375, 574.6809029071276, 572.2498082287117, 574.4889794305665, 574.8044199897237, 573.2781644359364, 572.975223884038, 570.8019267488382, 571.2696362562766, 574.265442732162, 573.573287223346, 571.7625805366009, 571.1046574193855, 574.6470102498442, 570.2984956844371, 571.9091150566336, 574.389513374143, 563.0516307067087, 571.3278769872672, 572.0640998715754, 573.6796235272234, 570.7510325389362, 557.6897048464353, 541.2477032680737, 574.5750657454652, 574.9482471715967, 560.2049829584508, 574.7577822975184, 558.2179703359934, 570.3619842586373, 565.9663010736674, 563.5817844095737, 574.0667686838723, 572.8361506041924, 574.5273469687716, 564.6372467731032, 575.810587734018, 567.4512767599952, 573.0855005364233, 577.4281878506082, 571.985040048213, 575.5484301485609, 576.6491390279616, 574.7618556708858, 567.1835583533428, 546.6148885248034, 576.476759516723, 568.0867898215574, 578.7488765701778, 575.5101307098728, 556.5462325633468, 561.3465135633638, 575.1132604468771, 573.8251663461491, 576.3099671551615, 581.0393007230224, 575.3578474723439, 575.4596696100523, 576.4410679119718, 574.5398303838381, 574.1393330758947, 574.0781077163458, 574.9959526320818, 572.1370874007706, 571.6845249227802, 558.5590770104245, 579.797198958306, 580.840415908916, 576.0226593172964, 570.3863049133565, 582.090216248404, 576.8724847782338, 574.2965995471728, 571.532094030239, 573.8111228429893]
Elapsed: 0.08574840659586092~0.0010806647999294175
Time per graph: 0.001749967481548182~2.205438367202894e-05
Speed: 571.5276351402794~7.019561422611843
Total Time: 0.0859
best val loss: 0.09316444396972656 test_score: 0.9388

Testing...
Test loss: 0.2615 score: 0.9388 time: 0.08s
test Score 0.9388
Epoch Time List: [0.31416981597431004, 0.3127801800146699, 0.31458094506524503, 0.31312357005663216, 0.31439768290147185, 0.312895200913772, 0.3126364592462778, 0.31407822598703206, 0.31565557001158595, 0.3162085807416588, 0.31594288162887096, 0.31597695173695683, 0.31374574778601527, 0.3127603989560157, 0.3205900960601866, 0.3144879259634763, 0.314925221959129, 0.33229523804038763, 0.3179192061070353, 0.31619290891103446, 0.3157863288652152, 0.32038561813533306, 0.31909928284585476, 0.3202613510657102, 0.3194185032043606, 0.31596908881329, 0.3185563071165234, 0.31427413295023143, 0.3285321651492268, 0.32338652899488807, 0.3189319570083171, 0.32141481596045196, 0.31353110494092107, 0.29431808670051396, 0.31116671301424503, 0.3111510418821126, 0.3161834829952568, 0.3218512150924653, 0.3188653700053692, 0.3162733349017799, 0.3183668910060078, 0.3212709079962224, 0.3140096771530807, 0.3180060258600861, 0.3217237973585725, 0.3190002138726413, 0.3186973640695214, 0.3187457798048854, 0.31367077585309744, 0.313710134010762, 0.3223672320600599, 0.31678846827708185, 0.31262060487642884, 0.3133541429415345, 0.3142203367315233, 0.3123230419587344, 0.3129633499775082, 0.31193255609832704, 0.31330577307380736, 0.3137015071697533, 0.312862535007298, 0.31369421910494566, 0.31349496776238084, 0.3140276938211173, 0.3189663579687476, 0.3144888214301318, 0.39378454512916505, 0.3088456951081753, 0.3080141041427851, 0.31471727578900754, 0.31525475694797933, 0.32389900321140885, 0.30732024484314024, 0.31067324290052056, 0.3158710931893438]
Total Epoch List: [75]
Total Time List: [0.08587030903436244]
========================k_idx:1========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: SAMamba
GraphEncoder(
  (subgraph_conv): GATConv(128, 128, heads=1)
  (embedding): Linear(in_features=14887, out_features=128, bias=False)
  (encoder): GraphMambaEncoder(
    (layers): ModuleList(
      (0-1): 2 x MambaEncoderLayer(
        (self_selen): Selection(
          (khop_structure_extractor): KHopStructureExtractor(
            (structure_extractor): StructureExtractor(
              (gcn): ModuleList(
                (0): GATConv(128, 128, heads=1)
              )
              (relu): ReLU()
              (bn): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (out_proj): Linear(in_features=128, out_features=128, bias=True)
            )
            (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (out_proj): Linear(in_features=256, out_features=128, bias=True)
          )
          (out_proj): Mamba(
            (in_proj): Linear(in_features=128, out_features=512, bias=False)
            (conv1d): Conv1d(256, 256, kernel_size=(4,), stride=(1,), padding=(3,), groups=256)
            (act): SiLU()
            (x_proj): Linear(in_features=256, out_features=40, bias=False)
            (dt_proj): Linear(in_features=8, out_features=256, bias=True)
            (out_proj): Linear(in_features=256, out_features=128, bias=False)
          )
        )
        (norm1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (linear1): Linear(in_features=128, out_features=256, bias=True)
        (dropout): Dropout(p=0.6, inplace=False)
        (linear2): Linear(in_features=256, out_features=128, bias=True)
        (dropout1): Dropout(p=0.6, inplace=False)
        (dropout2): Dropout(p=0.6, inplace=False)
        (activation): ReLU()
      )
    )
  )
  (classifier): Sequential(
    (0): Linear(in_features=128, out_features=128, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=128, out_features=2, bias=True)
  )
)
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7c7a550d1d50>
Training...
Epoch 1/1000, LR 0.000500
Train loss: 0.7085;  Loss pred: 0.6959; Loss self: 1.2565; time: 0.17s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6929 score: 0.5102 time: 0.08s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6933 score: 0.4898 time: 0.08s
Epoch 2/1000, LR 0.000000
Train loss: 0.7027;  Loss pred: 0.6899; Loss self: 1.2715; time: 0.15s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6929 score: 0.5102 time: 0.15s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6934 score: 0.4898 time: 0.08s
Epoch 3/1000, LR 0.000050
Train loss: 0.6879;  Loss pred: 0.6741; Loss self: 1.3801; time: 0.16s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6925 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6933 score: 0.4898 time: 0.08s
Epoch 4/1000, LR 0.000100
Train loss: 0.6583;  Loss pred: 0.6458; Loss self: 1.2555; time: 0.16s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6924 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6933 score: 0.4898 time: 0.08s
Epoch 5/1000, LR 0.000150
Train loss: 0.6209;  Loss pred: 0.6082; Loss self: 1.2691; time: 0.16s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6921 score: 0.5102 time: 0.08s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6930 score: 0.4898 time: 0.09s
Epoch 6/1000, LR 0.000200
Train loss: 0.5553;  Loss pred: 0.5420; Loss self: 1.3303; time: 0.17s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6917 score: 0.5102 time: 0.08s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6926 score: 0.4898 time: 0.08s
Epoch 7/1000, LR 0.000250
Train loss: 0.4972;  Loss pred: 0.4831; Loss self: 1.4040; time: 0.16s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6911 score: 0.5102 time: 0.08s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6919 score: 0.4898 time: 0.17s
Epoch 8/1000, LR 0.000300
Train loss: 0.4433;  Loss pred: 0.4284; Loss self: 1.4916; time: 0.16s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6903 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6909 score: 0.4898 time: 0.08s
Epoch 9/1000, LR 0.000350
Train loss: 0.3896;  Loss pred: 0.3741; Loss self: 1.5523; time: 0.16s
Val loss: 0.6891 score: 0.6735 time: 0.08s
Test loss: 0.6894 score: 0.6531 time: 0.08s
Epoch 10/1000, LR 0.000400
Train loss: 0.3421;  Loss pred: 0.3247; Loss self: 1.7422; time: 0.16s
Val loss: 0.6873 score: 0.8571 time: 0.08s
Test loss: 0.6872 score: 0.9184 time: 0.08s
Epoch 11/1000, LR 0.000450
Train loss: 0.2918;  Loss pred: 0.2733; Loss self: 1.8592; time: 0.17s
Val loss: 0.6847 score: 0.8776 time: 0.08s
Test loss: 0.6841 score: 0.8980 time: 0.08s
Epoch 12/1000, LR 0.000450
Train loss: 0.2496;  Loss pred: 0.2301; Loss self: 1.9462; time: 0.16s
Val loss: 0.6808 score: 0.9184 time: 0.17s
Test loss: 0.6796 score: 0.9184 time: 0.08s
Epoch 13/1000, LR 0.000450
Train loss: 0.2194;  Loss pred: 0.1991; Loss self: 2.0219; time: 0.15s
Val loss: 0.6750 score: 0.8776 time: 0.08s
Test loss: 0.6727 score: 0.9388 time: 0.08s
Epoch 14/1000, LR 0.000450
Train loss: 0.1929;  Loss pred: 0.1725; Loss self: 2.0386; time: 0.15s
Val loss: 0.6669 score: 0.8571 time: 0.08s
Test loss: 0.6631 score: 0.9592 time: 0.08s
Epoch 15/1000, LR 0.000450
Train loss: 0.1679;  Loss pred: 0.1464; Loss self: 2.1500; time: 0.16s
Val loss: 0.6558 score: 0.8776 time: 0.08s
Test loss: 0.6498 score: 0.9592 time: 0.08s
Epoch 16/1000, LR 0.000450
Train loss: 0.1436;  Loss pred: 0.1217; Loss self: 2.1854; time: 0.17s
Val loss: 0.6409 score: 0.8776 time: 0.08s
Test loss: 0.6315 score: 0.9592 time: 0.08s
Epoch 17/1000, LR 0.000450
Train loss: 0.1320;  Loss pred: 0.1098; Loss self: 2.2169; time: 0.16s
Val loss: 0.6214 score: 0.8776 time: 0.08s
Test loss: 0.6071 score: 0.9388 time: 0.08s
Epoch 18/1000, LR 0.000450
Train loss: 0.1123;  Loss pred: 0.0897; Loss self: 2.2657; time: 0.20s
Val loss: 0.5971 score: 0.8776 time: 0.08s
Test loss: 0.5759 score: 0.9388 time: 0.08s
Epoch 19/1000, LR 0.000450
Train loss: 0.1012;  Loss pred: 0.0787; Loss self: 2.2492; time: 0.16s
Val loss: 0.5677 score: 0.8776 time: 0.08s
Test loss: 0.5369 score: 0.9388 time: 0.08s
Epoch 20/1000, LR 0.000450
Train loss: 0.0921;  Loss pred: 0.0694; Loss self: 2.2702; time: 0.16s
Val loss: 0.5338 score: 0.8776 time: 0.08s
Test loss: 0.4904 score: 0.9388 time: 0.08s
Epoch 21/1000, LR 0.000450
Train loss: 0.0811;  Loss pred: 0.0578; Loss self: 2.3314; time: 0.16s
Val loss: 0.4962 score: 0.8571 time: 0.08s
Test loss: 0.4391 score: 0.9388 time: 0.08s
Epoch 22/1000, LR 0.000450
Train loss: 0.0739;  Loss pred: 0.0502; Loss self: 2.3714; time: 0.17s
Val loss: 0.4550 score: 0.8776 time: 0.08s
Test loss: 0.3839 score: 0.9388 time: 0.08s
Epoch 23/1000, LR 0.000450
Train loss: 0.0657;  Loss pred: 0.0414; Loss self: 2.4308; time: 0.16s
Val loss: 0.4119 score: 0.8776 time: 0.08s
Test loss: 0.3269 score: 0.9388 time: 0.18s
Epoch 24/1000, LR 0.000450
Train loss: 0.0598;  Loss pred: 0.0351; Loss self: 2.4686; time: 0.17s
Val loss: 0.3691 score: 0.8776 time: 0.08s
Test loss: 0.2697 score: 0.9388 time: 0.08s
Epoch 25/1000, LR 0.000450
Train loss: 0.0533;  Loss pred: 0.0284; Loss self: 2.4862; time: 0.17s
Val loss: 0.3322 score: 0.8776 time: 0.08s
Test loss: 0.2186 score: 0.9388 time: 0.09s
Epoch 26/1000, LR 0.000450
Train loss: 0.0498;  Loss pred: 0.0247; Loss self: 2.5032; time: 0.17s
Val loss: 0.3029 score: 0.8776 time: 0.08s
Test loss: 0.1757 score: 0.9388 time: 0.09s
Epoch 27/1000, LR 0.000450
Train loss: 0.0471;  Loss pred: 0.0213; Loss self: 2.5847; time: 0.18s
Val loss: 0.2826 score: 0.8776 time: 0.08s
Test loss: 0.1410 score: 0.9592 time: 0.08s
Epoch 28/1000, LR 0.000450
Train loss: 0.0427;  Loss pred: 0.0174; Loss self: 2.5231; time: 0.16s
Val loss: 0.2708 score: 0.8776 time: 0.11s
Test loss: 0.1142 score: 0.9592 time: 0.08s
Epoch 29/1000, LR 0.000450
Train loss: 0.0391;  Loss pred: 0.0135; Loss self: 2.5633; time: 0.15s
Val loss: 0.2655 score: 0.8776 time: 0.08s
Test loss: 0.0940 score: 0.9592 time: 0.08s
Epoch 30/1000, LR 0.000450
Train loss: 0.0384;  Loss pred: 0.0126; Loss self: 2.5800; time: 0.15s
Val loss: 0.2653 score: 0.8776 time: 0.08s
Test loss: 0.0785 score: 0.9796 time: 0.08s
Epoch 31/1000, LR 0.000450
Train loss: 0.0372;  Loss pred: 0.0109; Loss self: 2.6240; time: 0.16s
Val loss: 0.2686 score: 0.8776 time: 0.08s
Test loss: 0.0666 score: 0.9796 time: 0.08s
     INFO: Early stopping counter 1 of 20
Epoch 32/1000, LR 0.000450
Train loss: 0.0348;  Loss pred: 0.0086; Loss self: 2.6221; time: 0.16s
Val loss: 0.2745 score: 0.8980 time: 0.08s
Test loss: 0.0568 score: 0.9796 time: 0.08s
     INFO: Early stopping counter 2 of 20
Epoch 33/1000, LR 0.000449
Train loss: 0.0335;  Loss pred: 0.0073; Loss self: 2.6152; time: 0.15s
Val loss: 0.2822 score: 0.8980 time: 0.16s
Test loss: 0.0491 score: 1.0000 time: 0.08s
     INFO: Early stopping counter 3 of 20
Epoch 34/1000, LR 0.000449
Train loss: 0.0325;  Loss pred: 0.0063; Loss self: 2.6194; time: 0.15s
Val loss: 0.2897 score: 0.8980 time: 0.08s
Test loss: 0.0434 score: 1.0000 time: 0.08s
     INFO: Early stopping counter 4 of 20
Epoch 35/1000, LR 0.000449
Train loss: 0.0321;  Loss pred: 0.0056; Loss self: 2.6470; time: 0.16s
Val loss: 0.2989 score: 0.9184 time: 0.08s
Test loss: 0.0383 score: 1.0000 time: 0.08s
     INFO: Early stopping counter 5 of 20
Epoch 36/1000, LR 0.000449
Train loss: 0.0309;  Loss pred: 0.0049; Loss self: 2.6022; time: 0.16s
Val loss: 0.3065 score: 0.9184 time: 0.08s
Test loss: 0.0348 score: 1.0000 time: 0.08s
     INFO: Early stopping counter 6 of 20
Epoch 37/1000, LR 0.000449
Train loss: 0.0305;  Loss pred: 0.0042; Loss self: 2.6255; time: 0.17s
Val loss: 0.3132 score: 0.9388 time: 0.08s
Test loss: 0.0322 score: 1.0000 time: 0.08s
     INFO: Early stopping counter 7 of 20
Epoch 38/1000, LR 0.000449
Train loss: 0.0298;  Loss pred: 0.0038; Loss self: 2.6060; time: 0.16s
Val loss: 0.3193 score: 0.9388 time: 0.09s
Test loss: 0.0301 score: 1.0000 time: 0.08s
     INFO: Early stopping counter 8 of 20
Epoch 39/1000, LR 0.000449
Train loss: 0.0293;  Loss pred: 0.0034; Loss self: 2.5856; time: 0.16s
Val loss: 0.3246 score: 0.9388 time: 0.08s
Test loss: 0.0284 score: 1.0000 time: 0.08s
     INFO: Early stopping counter 9 of 20
Epoch 40/1000, LR 0.000449
Train loss: 0.0290;  Loss pred: 0.0029; Loss self: 2.6104; time: 0.16s
Val loss: 0.3285 score: 0.9388 time: 0.08s
Test loss: 0.0272 score: 1.0000 time: 0.08s
     INFO: Early stopping counter 10 of 20
Epoch 41/1000, LR 0.000449
Train loss: 0.0289;  Loss pred: 0.0027; Loss self: 2.6130; time: 0.16s
Val loss: 0.3332 score: 0.9388 time: 0.08s
Test loss: 0.0260 score: 1.0000 time: 0.08s
     INFO: Early stopping counter 11 of 20
Epoch 42/1000, LR 0.000449
Train loss: 0.0284;  Loss pred: 0.0027; Loss self: 2.5751; time: 0.16s
Val loss: 0.3368 score: 0.9388 time: 0.08s
Test loss: 0.0251 score: 1.0000 time: 0.08s
     INFO: Early stopping counter 12 of 20
Epoch 43/1000, LR 0.000449
Train loss: 0.0276;  Loss pred: 0.0024; Loss self: 2.5197; time: 0.16s
Val loss: 0.3400 score: 0.9388 time: 0.08s
Test loss: 0.0245 score: 1.0000 time: 0.08s
     INFO: Early stopping counter 13 of 20
Epoch 44/1000, LR 0.000449
Train loss: 0.0277;  Loss pred: 0.0022; Loss self: 2.5472; time: 0.14s
Val loss: 0.3432 score: 0.9388 time: 0.08s
Test loss: 0.0239 score: 1.0000 time: 0.08s
     INFO: Early stopping counter 14 of 20
Epoch 45/1000, LR 0.000449
Train loss: 0.0273;  Loss pred: 0.0021; Loss self: 2.5245; time: 0.16s
Val loss: 0.3462 score: 0.9388 time: 0.08s
Test loss: 0.0234 score: 1.0000 time: 0.08s
     INFO: Early stopping counter 15 of 20
Epoch 46/1000, LR 0.000449
Train loss: 0.0272;  Loss pred: 0.0020; Loss self: 2.5230; time: 0.16s
Val loss: 0.3483 score: 0.9388 time: 0.08s
Test loss: 0.0232 score: 1.0000 time: 0.08s
     INFO: Early stopping counter 16 of 20
Epoch 47/1000, LR 0.000449
Train loss: 0.0269;  Loss pred: 0.0019; Loss self: 2.5030; time: 0.16s
Val loss: 0.3492 score: 0.9388 time: 0.07s
Test loss: 0.0232 score: 1.0000 time: 0.08s
     INFO: Early stopping counter 17 of 20
Epoch 48/1000, LR 0.000448
Train loss: 0.0264;  Loss pred: 0.0016; Loss self: 2.4721; time: 0.16s
Val loss: 0.3501 score: 0.9388 time: 0.07s
Test loss: 0.0232 score: 1.0000 time: 0.08s
     INFO: Early stopping counter 18 of 20
Epoch 49/1000, LR 0.000448
Train loss: 0.0264;  Loss pred: 0.0017; Loss self: 2.4742; time: 0.16s
Val loss: 0.3510 score: 0.9388 time: 0.07s
Test loss: 0.0232 score: 1.0000 time: 0.08s
     INFO: Early stopping counter 19 of 20
Epoch 50/1000, LR 0.000448
Train loss: 0.0262;  Loss pred: 0.0015; Loss self: 2.4643; time: 0.16s
Val loss: 0.3509 score: 0.9388 time: 0.07s
Test loss: 0.0234 score: 1.0000 time: 0.08s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 029,   Train_Loss: 0.0384,   Val_Loss: 0.2653,   Val_Precision: 0.8800,   Val_Recall: 0.8800,   Val_accuracy: 0.8800,   Val_Score: 0.8776,   Val_Loss: 0.2653,   Test_Precision: 0.9600,   Test_Recall: 1.0000,   Test_accuracy: 0.9796,   Test_Score: 0.9796,   Test_loss: 0.0785


[0.0849194920156151, 0.08526470907963812, 0.0856269400101155, 0.08529319404624403, 0.08524638693779707, 0.08547334093600512, 0.08551853196695447, 0.08584413910284638, 0.08577385684475303, 0.08532639499753714, 0.08542936202138662, 0.08569990703836083, 0.08579863491468132, 0.0852697379887104, 0.08591991802677512, 0.08567794901318848, 0.0853079641237855, 0.08702576695941389, 0.08576511312276125, 0.08565473696216941, 0.08541352697648108, 0.08585179387591779, 0.08786247903481126, 0.0905315619893372, 0.08528041490353644, 0.08522506197914481, 0.08746798313222826, 0.08525330410338938, 0.08777933102101088, 0.08591035404242575, 0.08657759288325906, 0.08694390300661325, 0.08535592490807176, 0.08553929417394102, 0.08528749807737768, 0.08678138093091547, 0.08509742794558406, 0.08635102608241141, 0.0855020759627223, 0.08485903707332909, 0.08566657616756856, 0.08513618912547827, 0.08497368101961911, 0.0852526999078691, 0.08639178494922817, 0.08964263694360852, 0.0849990900605917, 0.08625442604534328, 0.08466539112851024, 0.08514185482636094, 0.0880430000834167, 0.08729011192917824, 0.08520060894079506, 0.08539186301641166, 0.08502369001507759, 0.08433164493180811, 0.08516438980586827, 0.08514932077378035, 0.08500435296446085, 0.08528564497828484, 0.0853451369330287, 0.08535423898138106, 0.08521799114532769, 0.08564380998723209, 0.0857116081751883, 0.08772572502493858, 0.08451230893842876, 0.08436052082106471, 0.08506609802134335, 0.08590669091790915, 0.08417939115315676, 0.08494078204967082, 0.08532176585868001, 0.08573446795344353, 0.0853939529042691, 0.08681753510609269, 0.08639626414515078, 0.08576329099014401, 0.08663000795058906, 0.09167452086694539, 0.0858221931848675, 0.17160369316115975, 0.08586888783611357, 0.08636182290501893, 0.08835758781060576, 0.08609490701928735, 0.0867333309724927, 0.08591751102358103, 0.08637367188930511, 0.08674066606909037, 0.08769455994479358, 0.08728356612846255, 0.08673829911276698, 0.08621935592964292, 0.08723111590370536, 0.08979905908927321, 0.08673857594840229, 0.1804037659894675, 0.0900876100640744, 0.09063601098023355, 0.09206129191443324, 0.09019814687781036, 0.08660764200612903, 0.08574862102977931, 0.08655487699434161, 0.08722677291370928, 0.08556427108123899, 0.08602451882325113, 0.0859195040538907, 0.08659322396852076, 0.08956247102469206, 0.08701439714059234, 0.08985058893449605, 0.08651184989139438, 0.08616035804152489, 0.0871271810028702, 0.08708525798283517, 0.08685965603217483, 0.08668906288221478, 0.0865157910156995, 0.08609500201418996, 0.08559504081495106, 0.08531604195013642, 0.08565362100489438, 0.08555139903910458]
[0.0017330508574615329, 0.0017400961036660842, 0.0017474885716350103, 0.0017406774295151842, 0.0017397221824040218, 0.0017443538966531657, 0.0017452761625909076, 0.0017519212061805384, 0.0017504868743827148, 0.0017413549999497375, 0.0017434563677834005, 0.0017489776946604252, 0.0017509925492792105, 0.0017401987344634775, 0.0017534677148321454, 0.001748529571697724, 0.0017409788596690918, 0.0017760360603962019, 0.0017503084310767601, 0.0017480558563708042, 0.0017431332036016546, 0.0017520774260391385, 0.0017931118170369644, 0.0018475828977415757, 0.001740416630684417, 0.0017392869791662206, 0.0017850608802495562, 0.001739863349048763, 0.0017914149187961404, 0.0017532725314780766, 0.0017668896506787563, 0.0017743653674819032, 0.0017419576511851379, 0.0017456998811008371, 0.0017405611852526056, 0.0017710485904268464, 0.0017366822029711033, 0.0017622658384165593, 0.001744940325769843, 0.001731817083129165, 0.0017482974728075217, 0.0017374732474587401, 0.0017341567555024307, 0.0017398510185279408, 0.0017630976520250647, 0.001829441570277725, 0.0017346753073590143, 0.0017602944090886383, 0.0017278651250716374, 0.0017375888740073662, 0.0017967959200697286, 0.001781430855697515, 0.0017387879375672462, 0.0017426910819675848, 0.0017351773472464814, 0.0017210539782001656, 0.001738048771548332, 0.0017377412402812317, 0.0017347827135604254, 0.0017405233669037722, 0.001741737488429157, 0.0017419232445179808, 0.0017391426764352589, 0.0017478328568822875, 0.0017492164933711899, 0.0017903209188762977, 0.001724740998743444, 0.0017216432820625451, 0.0017360428167621093, 0.0017531977738348805, 0.001717946758227689, 0.0017334853479524656, 0.0017412605277281634, 0.0017496830194580312, 0.0017427337327401858, 0.0017717864307365855, 0.0017631890641867506, 0.0017502712446968167, 0.001767959345930389, 0.0018709085891213343, 0.0017514733303034184, 0.003502116186962444, 0.0017524262823696648, 0.00176248618173508, 0.0018032160777674646, 0.0017570389187609662, 0.0017700679790304632, 0.0017534185923179801, 0.0017627279977409207, 0.0017702176748793951, 0.001789684896832522, 0.0017812972679278072, 0.0017701693696483057, 0.0017595786924416922, 0.0017802268551776604, 0.0018326338589647595, 0.0017701750193551487, 0.0036817095099891325, 0.0018385226543688653, 0.0018497145098006847, 0.00187880187580476, 0.0018407785077104155, 0.0017675028980842658, 0.0017499718577505983, 0.0017664260611090126, 0.001780138222728761, 0.0017462096139028364, 0.0017556024249643087, 0.0017534592664059327, 0.001767208652418791, 0.0018278055311161646, 0.0017758040232773945, 0.0018336854884591029, 0.0017655479569672321, 0.0017583746539086712, 0.0017781057347524532, 0.0017772501629150035, 0.0017726460414729556, 0.0017691645486166282, 0.0017656283880755001, 0.001757040857432448, 0.0017468375676520625, 0.0017411437132680903, 0.0017480330817325382, 0.0017459469191653996]
[577.0171115836375, 574.6809029071276, 572.2498082287117, 574.4889794305665, 574.8044199897237, 573.2781644359364, 572.975223884038, 570.8019267488382, 571.2696362562766, 574.265442732162, 573.573287223346, 571.7625805366009, 571.1046574193855, 574.6470102498442, 570.2984956844371, 571.9091150566336, 574.389513374143, 563.0516307067087, 571.3278769872672, 572.0640998715754, 573.6796235272234, 570.7510325389362, 557.6897048464353, 541.2477032680737, 574.5750657454652, 574.9482471715967, 560.2049829584508, 574.7577822975184, 558.2179703359934, 570.3619842586373, 565.9663010736674, 563.5817844095737, 574.0667686838723, 572.8361506041924, 574.5273469687716, 564.6372467731032, 575.810587734018, 567.4512767599952, 573.0855005364233, 577.4281878506082, 571.985040048213, 575.5484301485609, 576.6491390279616, 574.7618556708858, 567.1835583533428, 546.6148885248034, 576.476759516723, 568.0867898215574, 578.7488765701778, 575.5101307098728, 556.5462325633468, 561.3465135633638, 575.1132604468771, 573.8251663461491, 576.3099671551615, 581.0393007230224, 575.3578474723439, 575.4596696100523, 576.4410679119718, 574.5398303838381, 574.1393330758947, 574.0781077163458, 574.9959526320818, 572.1370874007706, 571.6845249227802, 558.5590770104245, 579.797198958306, 580.840415908916, 576.0226593172964, 570.3863049133565, 582.090216248404, 576.8724847782338, 574.2965995471728, 571.532094030239, 573.8111228429893, 564.4021100129261, 567.1541528425017, 571.3400154575588, 565.6238659004626, 534.4996574469982, 570.9478886708278, 285.5416401439693, 570.6374128604033, 567.3803348719306, 554.5647093154168, 569.139356745258, 564.9500538096507, 570.3144727569145, 567.3025000349353, 564.9022796409091, 558.757578929035, 561.3886115501123, 564.9176949653571, 568.3178617106023, 561.726162646953, 545.6627329612322, 564.9158919688556, 271.6129551467388, 543.9149730484467, 540.6239691052402, 532.2540992097228, 543.2484113712372, 565.7699351349663, 571.4377608822767, 566.1148360617889, 561.7541307927804, 572.6689350684347, 569.6050459831929, 570.3012434669788, 565.8641375659251, 547.1041546686544, 563.1252023826461, 545.3497921501947, 566.3963961181483, 568.7070146155208, 562.3962515025685, 562.6669902000874, 564.1284140228378, 565.2385476421258, 566.3705946017215, 569.1387287722456, 572.4630718493806, 574.33512947821, 572.0715531360906, 572.7550986933908]
Elapsed: 0.08772163914516568~0.011367688241851897
Time per graph: 0.00179023753357481~0.00023199363758881426
Speed: 563.4110159474825~37.540602426371294
Total Time: 0.0862
best val loss: 0.2653070092201233 test_score: 0.9796

Testing...
Test loss: 0.0322 score: 1.0000 time: 0.08s
test Score 1.0000
Epoch Time List: [0.31416981597431004, 0.3127801800146699, 0.31458094506524503, 0.31312357005663216, 0.31439768290147185, 0.312895200913772, 0.3126364592462778, 0.31407822598703206, 0.31565557001158595, 0.3162085807416588, 0.31594288162887096, 0.31597695173695683, 0.31374574778601527, 0.3127603989560157, 0.3205900960601866, 0.3144879259634763, 0.314925221959129, 0.33229523804038763, 0.3179192061070353, 0.31619290891103446, 0.3157863288652152, 0.32038561813533306, 0.31909928284585476, 0.3202613510657102, 0.3194185032043606, 0.31596908881329, 0.3185563071165234, 0.31427413295023143, 0.3285321651492268, 0.32338652899488807, 0.3189319570083171, 0.32141481596045196, 0.31353110494092107, 0.29431808670051396, 0.31116671301424503, 0.3111510418821126, 0.3161834829952568, 0.3218512150924653, 0.3188653700053692, 0.3162733349017799, 0.3183668910060078, 0.3212709079962224, 0.3140096771530807, 0.3180060258600861, 0.3217237973585725, 0.3190002138726413, 0.3186973640695214, 0.3187457798048854, 0.31367077585309744, 0.313710134010762, 0.3223672320600599, 0.31678846827708185, 0.31262060487642884, 0.3133541429415345, 0.3142203367315233, 0.3123230419587344, 0.3129633499775082, 0.31193255609832704, 0.31330577307380736, 0.3137015071697533, 0.312862535007298, 0.31369421910494566, 0.31349496776238084, 0.3140276938211173, 0.3189663579687476, 0.3144888214301318, 0.39378454512916505, 0.3088456951081753, 0.3080141041427851, 0.31471727578900754, 0.31525475694797933, 0.32389900321140885, 0.30732024484314024, 0.31067324290052056, 0.3158710931893438, 0.32445686403661966, 0.3765084049664438, 0.31218810100108385, 0.31227964884601533, 0.32106332411058247, 0.3235407250467688, 0.40631074784323573, 0.3164408428128809, 0.31867440277710557, 0.3223777231760323, 0.32655323296785355, 0.40911062783561647, 0.30282210814766586, 0.30235252901911736, 0.3210618360899389, 0.32854065485298634, 0.32065978716127574, 0.35313030309043825, 0.31187721830792725, 0.31651929789222777, 0.32130556902848184, 0.3252414441667497, 0.415032816817984, 0.33074082690291107, 0.33225325704552233, 0.33936856500804424, 0.33914260007441044, 0.3461439381353557, 0.3086288261692971, 0.30999927897937596, 0.3151470604352653, 0.3165932248812169, 0.39520349516533315, 0.3092905681114644, 0.3123139350209385, 0.3173269892577082, 0.3269022121094167, 0.3322978049982339, 0.3170971421059221, 0.3142550189513713, 0.3170661630574614, 0.31961122108623385, 0.31994221406057477, 0.30082654394209385, 0.31469543278217316, 0.3158775807823986, 0.31302067171782255, 0.31387167586945, 0.3166290756780654, 0.31732033286243677]
Total Epoch List: [75, 50]
Total Time List: [0.08587030903436244, 0.08617483894340694]
========================k_idx:2========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: SAMamba
GraphEncoder(
  (subgraph_conv): GATConv(128, 128, heads=1)
  (embedding): Linear(in_features=14887, out_features=128, bias=False)
  (encoder): GraphMambaEncoder(
    (layers): ModuleList(
      (0-1): 2 x MambaEncoderLayer(
        (self_selen): Selection(
          (khop_structure_extractor): KHopStructureExtractor(
            (structure_extractor): StructureExtractor(
              (gcn): ModuleList(
                (0): GATConv(128, 128, heads=1)
              )
              (relu): ReLU()
              (bn): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (out_proj): Linear(in_features=128, out_features=128, bias=True)
            )
            (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (out_proj): Linear(in_features=256, out_features=128, bias=True)
          )
          (out_proj): Mamba(
            (in_proj): Linear(in_features=128, out_features=512, bias=False)
            (conv1d): Conv1d(256, 256, kernel_size=(4,), stride=(1,), padding=(3,), groups=256)
            (act): SiLU()
            (x_proj): Linear(in_features=256, out_features=40, bias=False)
            (dt_proj): Linear(in_features=8, out_features=256, bias=True)
            (out_proj): Linear(in_features=256, out_features=128, bias=False)
          )
        )
        (norm1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (linear1): Linear(in_features=128, out_features=256, bias=True)
        (dropout): Dropout(p=0.6, inplace=False)
        (linear2): Linear(in_features=256, out_features=128, bias=True)
        (dropout1): Dropout(p=0.6, inplace=False)
        (dropout2): Dropout(p=0.6, inplace=False)
        (activation): ReLU()
      )
    )
  )
  (classifier): Sequential(
    (0): Linear(in_features=128, out_features=128, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=128, out_features=2, bias=True)
  )
)
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7c7a54425cc0>
Training...
Epoch 1/1000, LR 0.000500
Train loss: 0.7053;  Loss pred: 0.6939; Loss self: 1.1372; time: 0.16s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6944 score: 0.4898 time: 0.09s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6936 score: 0.5000 time: 0.07s
Epoch 2/1000, LR 0.000000
Train loss: 0.7096;  Loss pred: 0.6972; Loss self: 1.2389; time: 0.15s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6945 score: 0.4898 time: 0.09s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6936 score: 0.5000 time: 0.07s
     INFO: Early stopping counter 1 of 20
Epoch 3/1000, LR 0.000050
Train loss: 0.6986;  Loss pred: 0.6868; Loss self: 1.1825; time: 0.15s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6947 score: 0.4898 time: 0.09s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6936 score: 0.5000 time: 0.07s
     INFO: Early stopping counter 2 of 20
Epoch 4/1000, LR 0.000100
Train loss: 0.6505;  Loss pred: 0.6387; Loss self: 1.1758; time: 0.15s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6948 score: 0.4898 time: 0.09s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6935 score: 0.5000 time: 0.07s
     INFO: Early stopping counter 3 of 20
Epoch 5/1000, LR 0.000150
Train loss: 0.6181;  Loss pred: 0.6059; Loss self: 1.2221; time: 0.15s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6948 score: 0.4898 time: 0.09s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6934 score: 0.5000 time: 0.07s
     INFO: Early stopping counter 4 of 20
Epoch 6/1000, LR 0.000200
Train loss: 0.5710;  Loss pred: 0.5590; Loss self: 1.1976; time: 0.15s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6946 score: 0.4898 time: 0.09s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6931 score: 0.5000 time: 0.07s
     INFO: Early stopping counter 5 of 20
Epoch 7/1000, LR 0.000250
Train loss: 0.5216;  Loss pred: 0.5093; Loss self: 1.2240; time: 0.16s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6945 score: 0.4898 time: 0.09s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6929 score: 0.5000 time: 0.07s
     INFO: Early stopping counter 6 of 20
Epoch 8/1000, LR 0.000300
Train loss: 0.4615;  Loss pred: 0.4485; Loss self: 1.3005; time: 0.16s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6941 score: 0.4898 time: 0.09s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6924 score: 0.5000 time: 0.07s
Epoch 9/1000, LR 0.000350
Train loss: 0.4058;  Loss pred: 0.3923; Loss self: 1.3523; time: 0.15s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6933 score: 0.4898 time: 0.09s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6914 score: 0.5000 time: 0.07s
Epoch 10/1000, LR 0.000400
Train loss: 0.3518;  Loss pred: 0.3371; Loss self: 1.4703; time: 0.16s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6920 score: 0.4898 time: 0.09s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6899 score: 0.5000 time: 0.07s
Epoch 11/1000, LR 0.000450
Train loss: 0.3084;  Loss pred: 0.2925; Loss self: 1.5944; time: 0.15s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6900 score: 0.4898 time: 0.09s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6876 score: 0.5000 time: 0.07s
Epoch 12/1000, LR 0.000450
Train loss: 0.2634;  Loss pred: 0.2461; Loss self: 1.7303; time: 0.16s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6868 score: 0.4898 time: 0.09s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6841 score: 0.5000 time: 0.07s
Epoch 13/1000, LR 0.000450
Train loss: 0.2274;  Loss pred: 0.2091; Loss self: 1.8282; time: 0.16s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6821 score: 0.4898 time: 0.09s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6791 score: 0.5000 time: 0.07s
Epoch 14/1000, LR 0.000450
Train loss: 0.1946;  Loss pred: 0.1755; Loss self: 1.9133; time: 0.15s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6752 score: 0.4898 time: 0.09s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6718 score: 0.5000 time: 0.07s
Epoch 15/1000, LR 0.000450
Train loss: 0.1732;  Loss pred: 0.1531; Loss self: 2.0022; time: 0.15s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6655 score: 0.4898 time: 0.09s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6616 score: 0.5000 time: 0.07s
Epoch 16/1000, LR 0.000450
Train loss: 0.1511;  Loss pred: 0.1300; Loss self: 2.1117; time: 0.16s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6512 score: 0.4898 time: 0.09s
Test loss: 0.6468 score: 0.5208 time: 0.07s
Epoch 17/1000, LR 0.000450
Train loss: 0.1324;  Loss pred: 0.1114; Loss self: 2.1000; time: 0.16s
Val loss: 0.6320 score: 0.5510 time: 0.09s
Test loss: 0.6269 score: 0.6250 time: 0.07s
Epoch 18/1000, LR 0.000450
Train loss: 0.1128;  Loss pred: 0.0908; Loss self: 2.2025; time: 0.16s
Val loss: 0.6051 score: 0.7755 time: 0.09s
Test loss: 0.5992 score: 0.7917 time: 0.07s
Epoch 19/1000, LR 0.000450
Train loss: 0.0982;  Loss pred: 0.0755; Loss self: 2.2731; time: 0.16s
Val loss: 0.5697 score: 0.8980 time: 0.09s
Test loss: 0.5633 score: 0.8750 time: 0.07s
Epoch 20/1000, LR 0.000450
Train loss: 0.0890;  Loss pred: 0.0658; Loss self: 2.3219; time: 0.16s
Val loss: 0.5253 score: 0.9592 time: 0.09s
Test loss: 0.5182 score: 0.9167 time: 0.07s
Epoch 21/1000, LR 0.000450
Train loss: 0.0776;  Loss pred: 0.0538; Loss self: 2.3852; time: 0.16s
Val loss: 0.4718 score: 0.9592 time: 0.09s
Test loss: 0.4634 score: 0.9583 time: 0.07s
Epoch 22/1000, LR 0.000450
Train loss: 0.0682;  Loss pred: 0.0439; Loss self: 2.4335; time: 0.16s
Val loss: 0.4123 score: 0.9592 time: 0.09s
Test loss: 0.4017 score: 0.9792 time: 0.07s
Epoch 23/1000, LR 0.000450
Train loss: 0.0607;  Loss pred: 0.0357; Loss self: 2.5028; time: 0.16s
Val loss: 0.3523 score: 0.9592 time: 0.09s
Test loss: 0.3393 score: 0.9792 time: 0.07s
Epoch 24/1000, LR 0.000450
Train loss: 0.0546;  Loss pred: 0.0294; Loss self: 2.5259; time: 0.16s
Val loss: 0.2975 score: 0.9592 time: 0.09s
Test loss: 0.2810 score: 0.9792 time: 0.07s
Epoch 25/1000, LR 0.000450
Train loss: 0.0497;  Loss pred: 0.0239; Loss self: 2.5744; time: 0.16s
Val loss: 0.2511 score: 0.9592 time: 0.09s
Test loss: 0.2301 score: 0.9792 time: 0.07s
Epoch 26/1000, LR 0.000450
Train loss: 0.0454;  Loss pred: 0.0194; Loss self: 2.6014; time: 0.16s
Val loss: 0.2159 score: 0.9592 time: 0.09s
Test loss: 0.1895 score: 0.9792 time: 0.07s
Epoch 27/1000, LR 0.000450
Train loss: 0.0423;  Loss pred: 0.0157; Loss self: 2.6559; time: 0.16s
Val loss: 0.1926 score: 0.9592 time: 0.09s
Test loss: 0.1598 score: 0.9792 time: 0.07s
Epoch 28/1000, LR 0.000450
Train loss: 0.0393;  Loss pred: 0.0123; Loss self: 2.6981; time: 0.16s
Val loss: 0.1794 score: 0.9592 time: 0.09s
Test loss: 0.1395 score: 0.9583 time: 0.07s
Epoch 29/1000, LR 0.000450
Train loss: 0.0369;  Loss pred: 0.0099; Loss self: 2.6998; time: 0.16s
Val loss: 0.1748 score: 0.9592 time: 0.09s
Test loss: 0.1272 score: 0.9583 time: 0.07s
Epoch 30/1000, LR 0.000450
Train loss: 0.0351;  Loss pred: 0.0081; Loss self: 2.6994; time: 0.16s
Val loss: 0.1763 score: 0.9592 time: 0.09s
Test loss: 0.1210 score: 0.9583 time: 0.07s
     INFO: Early stopping counter 1 of 20
Epoch 31/1000, LR 0.000450
Train loss: 0.0343;  Loss pred: 0.0068; Loss self: 2.7464; time: 0.16s
Val loss: 0.1822 score: 0.9592 time: 0.09s
Test loss: 0.1192 score: 0.9583 time: 0.07s
     INFO: Early stopping counter 2 of 20
Epoch 32/1000, LR 0.000450
Train loss: 0.0334;  Loss pred: 0.0059; Loss self: 2.7488; time: 0.14s
Val loss: 0.1907 score: 0.9592 time: 0.09s
Test loss: 0.1207 score: 0.9583 time: 0.07s
     INFO: Early stopping counter 3 of 20
Epoch 33/1000, LR 0.000449
Train loss: 0.0324;  Loss pred: 0.0049; Loss self: 2.7491; time: 0.16s
Val loss: 0.2015 score: 0.9592 time: 0.09s
Test loss: 0.1246 score: 0.9583 time: 0.07s
     INFO: Early stopping counter 4 of 20
Epoch 34/1000, LR 0.000449
Train loss: 0.0314;  Loss pred: 0.0040; Loss self: 2.7384; time: 0.15s
Val loss: 0.2147 score: 0.9592 time: 0.09s
Test loss: 0.1303 score: 0.9583 time: 0.07s
     INFO: Early stopping counter 5 of 20
Epoch 35/1000, LR 0.000449
Train loss: 0.0311;  Loss pred: 0.0035; Loss self: 2.7653; time: 0.16s
Val loss: 0.2273 score: 0.9388 time: 0.09s
Test loss: 0.1362 score: 0.9583 time: 0.07s
     INFO: Early stopping counter 6 of 20
Epoch 36/1000, LR 0.000449
Train loss: 0.0308;  Loss pred: 0.0030; Loss self: 2.7758; time: 0.14s
Val loss: 0.2386 score: 0.9388 time: 0.09s
Test loss: 0.1416 score: 0.9583 time: 0.07s
     INFO: Early stopping counter 7 of 20
Epoch 37/1000, LR 0.000449
Train loss: 0.0303;  Loss pred: 0.0026; Loss self: 2.7714; time: 0.15s
Val loss: 0.2502 score: 0.9388 time: 0.09s
Test loss: 0.1474 score: 0.9583 time: 0.07s
     INFO: Early stopping counter 8 of 20
Epoch 38/1000, LR 0.000449
Train loss: 0.0300;  Loss pred: 0.0023; Loss self: 2.7685; time: 0.16s
Val loss: 0.2631 score: 0.9388 time: 0.09s
Test loss: 0.1539 score: 0.9583 time: 0.07s
     INFO: Early stopping counter 9 of 20
Epoch 39/1000, LR 0.000449
Train loss: 0.0298;  Loss pred: 0.0021; Loss self: 2.7660; time: 0.16s
Val loss: 0.2745 score: 0.9388 time: 0.09s
Test loss: 0.1596 score: 0.9583 time: 0.07s
     INFO: Early stopping counter 10 of 20
Epoch 40/1000, LR 0.000449
Train loss: 0.0295;  Loss pred: 0.0019; Loss self: 2.7673; time: 0.16s
Val loss: 0.2850 score: 0.9184 time: 0.09s
Test loss: 0.1650 score: 0.9583 time: 0.07s
     INFO: Early stopping counter 11 of 20
Epoch 41/1000, LR 0.000449
Train loss: 0.0293;  Loss pred: 0.0017; Loss self: 2.7606; time: 0.16s
Val loss: 0.2947 score: 0.9184 time: 0.09s
Test loss: 0.1700 score: 0.9583 time: 0.07s
     INFO: Early stopping counter 12 of 20
Epoch 42/1000, LR 0.000449
Train loss: 0.0290;  Loss pred: 0.0016; Loss self: 2.7329; time: 0.16s
Val loss: 0.3038 score: 0.9184 time: 0.09s
Test loss: 0.1746 score: 0.9583 time: 0.07s
     INFO: Early stopping counter 13 of 20
Epoch 43/1000, LR 0.000449
Train loss: 0.0289;  Loss pred: 0.0014; Loss self: 2.7466; time: 0.16s
Val loss: 0.3140 score: 0.9184 time: 0.09s
Test loss: 0.1797 score: 0.9583 time: 0.07s
     INFO: Early stopping counter 14 of 20
Epoch 44/1000, LR 0.000449
Train loss: 0.0284;  Loss pred: 0.0013; Loss self: 2.7111; time: 0.16s
Val loss: 0.3207 score: 0.9184 time: 0.09s
Test loss: 0.1829 score: 0.9583 time: 0.07s
     INFO: Early stopping counter 15 of 20
Epoch 45/1000, LR 0.000449
Train loss: 0.0283;  Loss pred: 0.0013; Loss self: 2.7085; time: 0.16s
Val loss: 0.3284 score: 0.9184 time: 0.09s
Test loss: 0.1868 score: 0.9583 time: 0.07s
     INFO: Early stopping counter 16 of 20
Epoch 46/1000, LR 0.000449
Train loss: 0.0283;  Loss pred: 0.0013; Loss self: 2.7086; time: 0.16s
Val loss: 0.3347 score: 0.9184 time: 0.09s
Test loss: 0.1898 score: 0.9583 time: 0.07s
     INFO: Early stopping counter 17 of 20
Epoch 47/1000, LR 0.000449
Train loss: 0.0281;  Loss pred: 0.0010; Loss self: 2.7079; time: 0.16s
Val loss: 0.3410 score: 0.9184 time: 0.09s
Test loss: 0.1928 score: 0.9583 time: 0.07s
     INFO: Early stopping counter 18 of 20
Epoch 48/1000, LR 0.000448
Train loss: 0.0280;  Loss pred: 0.0011; Loss self: 2.6962; time: 0.16s
Val loss: 0.3472 score: 0.9184 time: 0.09s
Test loss: 0.1960 score: 0.9583 time: 0.07s
     INFO: Early stopping counter 19 of 20
Epoch 49/1000, LR 0.000448
Train loss: 0.0280;  Loss pred: 0.0011; Loss self: 2.6900; time: 0.16s
Val loss: 0.3511 score: 0.9184 time: 0.09s
Test loss: 0.1979 score: 0.9583 time: 0.07s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 028,   Train_Loss: 0.0369,   Val_Loss: 0.1748,   Val_Precision: 1.0000,   Val_Recall: 0.9200,   Val_accuracy: 0.9583,   Val_Score: 0.9592,   Val_Loss: 0.1748,   Test_Precision: 1.0000,   Test_Recall: 0.9167,   Test_accuracy: 0.9565,   Test_Score: 0.9583,   Test_loss: 0.1272


[0.0849194920156151, 0.08526470907963812, 0.0856269400101155, 0.08529319404624403, 0.08524638693779707, 0.08547334093600512, 0.08551853196695447, 0.08584413910284638, 0.08577385684475303, 0.08532639499753714, 0.08542936202138662, 0.08569990703836083, 0.08579863491468132, 0.0852697379887104, 0.08591991802677512, 0.08567794901318848, 0.0853079641237855, 0.08702576695941389, 0.08576511312276125, 0.08565473696216941, 0.08541352697648108, 0.08585179387591779, 0.08786247903481126, 0.0905315619893372, 0.08528041490353644, 0.08522506197914481, 0.08746798313222826, 0.08525330410338938, 0.08777933102101088, 0.08591035404242575, 0.08657759288325906, 0.08694390300661325, 0.08535592490807176, 0.08553929417394102, 0.08528749807737768, 0.08678138093091547, 0.08509742794558406, 0.08635102608241141, 0.0855020759627223, 0.08485903707332909, 0.08566657616756856, 0.08513618912547827, 0.08497368101961911, 0.0852526999078691, 0.08639178494922817, 0.08964263694360852, 0.0849990900605917, 0.08625442604534328, 0.08466539112851024, 0.08514185482636094, 0.0880430000834167, 0.08729011192917824, 0.08520060894079506, 0.08539186301641166, 0.08502369001507759, 0.08433164493180811, 0.08516438980586827, 0.08514932077378035, 0.08500435296446085, 0.08528564497828484, 0.0853451369330287, 0.08535423898138106, 0.08521799114532769, 0.08564380998723209, 0.0857116081751883, 0.08772572502493858, 0.08451230893842876, 0.08436052082106471, 0.08506609802134335, 0.08590669091790915, 0.08417939115315676, 0.08494078204967082, 0.08532176585868001, 0.08573446795344353, 0.0853939529042691, 0.08681753510609269, 0.08639626414515078, 0.08576329099014401, 0.08663000795058906, 0.09167452086694539, 0.0858221931848675, 0.17160369316115975, 0.08586888783611357, 0.08636182290501893, 0.08835758781060576, 0.08609490701928735, 0.0867333309724927, 0.08591751102358103, 0.08637367188930511, 0.08674066606909037, 0.08769455994479358, 0.08728356612846255, 0.08673829911276698, 0.08621935592964292, 0.08723111590370536, 0.08979905908927321, 0.08673857594840229, 0.1804037659894675, 0.0900876100640744, 0.09063601098023355, 0.09206129191443324, 0.09019814687781036, 0.08660764200612903, 0.08574862102977931, 0.08655487699434161, 0.08722677291370928, 0.08556427108123899, 0.08602451882325113, 0.0859195040538907, 0.08659322396852076, 0.08956247102469206, 0.08701439714059234, 0.08985058893449605, 0.08651184989139438, 0.08616035804152489, 0.0871271810028702, 0.08708525798283517, 0.08685965603217483, 0.08668906288221478, 0.0865157910156995, 0.08609500201418996, 0.08559504081495106, 0.08531604195013642, 0.08565362100489438, 0.08555139903910458, 0.07705207797698677, 0.07700989092700183, 0.07714522606693208, 0.07739341491833329, 0.07724059489555657, 0.07708874787203968, 0.07712626410648227, 0.07717264699749649, 0.0771040830295533, 0.07744979416020215, 0.07715646107681096, 0.07715198793448508, 0.07734868512488902, 0.07743678195402026, 0.07739328010939062, 0.07768951682373881, 0.07697155699133873, 0.07658479688689113, 0.07719816593453288, 0.07694707508198917, 0.07732956716790795, 0.07732162997126579, 0.07729477295652032, 0.07706304988823831, 0.0772061669267714, 0.07719067600555718, 0.07728915591724217, 0.07714118086732924, 0.07699649408459663, 0.07698290795087814, 0.07717775809578598, 0.07709305197931826, 0.07724381098523736, 0.07734259404242039, 0.07705751108005643, 0.07706474489532411, 0.07776914886198938, 0.07733407500199974, 0.07704016496427357, 0.0771846182178706, 0.07709641009569168, 0.07777531701140106, 0.07715724082663655, 0.07744266698136926, 0.07734240405261517, 0.07741241599433124, 0.07699948595836759, 0.07724566990509629, 0.07742534810677171]
[0.0017330508574615329, 0.0017400961036660842, 0.0017474885716350103, 0.0017406774295151842, 0.0017397221824040218, 0.0017443538966531657, 0.0017452761625909076, 0.0017519212061805384, 0.0017504868743827148, 0.0017413549999497375, 0.0017434563677834005, 0.0017489776946604252, 0.0017509925492792105, 0.0017401987344634775, 0.0017534677148321454, 0.001748529571697724, 0.0017409788596690918, 0.0017760360603962019, 0.0017503084310767601, 0.0017480558563708042, 0.0017431332036016546, 0.0017520774260391385, 0.0017931118170369644, 0.0018475828977415757, 0.001740416630684417, 0.0017392869791662206, 0.0017850608802495562, 0.001739863349048763, 0.0017914149187961404, 0.0017532725314780766, 0.0017668896506787563, 0.0017743653674819032, 0.0017419576511851379, 0.0017456998811008371, 0.0017405611852526056, 0.0017710485904268464, 0.0017366822029711033, 0.0017622658384165593, 0.001744940325769843, 0.001731817083129165, 0.0017482974728075217, 0.0017374732474587401, 0.0017341567555024307, 0.0017398510185279408, 0.0017630976520250647, 0.001829441570277725, 0.0017346753073590143, 0.0017602944090886383, 0.0017278651250716374, 0.0017375888740073662, 0.0017967959200697286, 0.001781430855697515, 0.0017387879375672462, 0.0017426910819675848, 0.0017351773472464814, 0.0017210539782001656, 0.001738048771548332, 0.0017377412402812317, 0.0017347827135604254, 0.0017405233669037722, 0.001741737488429157, 0.0017419232445179808, 0.0017391426764352589, 0.0017478328568822875, 0.0017492164933711899, 0.0017903209188762977, 0.001724740998743444, 0.0017216432820625451, 0.0017360428167621093, 0.0017531977738348805, 0.001717946758227689, 0.0017334853479524656, 0.0017412605277281634, 0.0017496830194580312, 0.0017427337327401858, 0.0017717864307365855, 0.0017631890641867506, 0.0017502712446968167, 0.001767959345930389, 0.0018709085891213343, 0.0017514733303034184, 0.003502116186962444, 0.0017524262823696648, 0.00176248618173508, 0.0018032160777674646, 0.0017570389187609662, 0.0017700679790304632, 0.0017534185923179801, 0.0017627279977409207, 0.0017702176748793951, 0.001789684896832522, 0.0017812972679278072, 0.0017701693696483057, 0.0017595786924416922, 0.0017802268551776604, 0.0018326338589647595, 0.0017701750193551487, 0.0036817095099891325, 0.0018385226543688653, 0.0018497145098006847, 0.00187880187580476, 0.0018407785077104155, 0.0017675028980842658, 0.0017499718577505983, 0.0017664260611090126, 0.001780138222728761, 0.0017462096139028364, 0.0017556024249643087, 0.0017534592664059327, 0.001767208652418791, 0.0018278055311161646, 0.0017758040232773945, 0.0018336854884591029, 0.0017655479569672321, 0.0017583746539086712, 0.0017781057347524532, 0.0017772501629150035, 0.0017726460414729556, 0.0017691645486166282, 0.0017656283880755001, 0.001757040857432448, 0.0017468375676520625, 0.0017411437132680903, 0.0017480330817325382, 0.0017459469191653996, 0.0016052516245205577, 0.0016043727276458715, 0.0016071922097277518, 0.0016123628107986103, 0.0016091790603240952, 0.0016060155806674932, 0.0016067971688850473, 0.0016077634791145101, 0.0016063350631156936, 0.0016135373783375446, 0.0016074262724335615, 0.001607333081968439, 0.0016114309401018545, 0.0016132662907087554, 0.0016123600022789712, 0.0016185316004945587, 0.0016035741039862235, 0.0015955166018102318, 0.0016082951236361016, 0.0016030640642081078, 0.0016110326493314158, 0.0016108672910680373, 0.0016103077699275066, 0.0016054802060049649, 0.0016084618109744042, 0.0016081390834491078, 0.0016101907482758786, 0.0016071079347360258, 0.00160409362676243, 0.0016038105823099613, 0.0016078699603288744, 0.0016061052495691304, 0.001609246062192445, 0.0016113040425504248, 0.0016053648141678423, 0.0016055155186525856, 0.0016201906012914453, 0.001611126562541661, 0.0016050034367556993, 0.0016080128795389708, 0.00160617521032691, 0.0016203191044041887, 0.001607442517221595, 0.001613388895445193, 0.0016113000844294827, 0.0016127586665485676, 0.0016041559574659914, 0.0016092847896895062, 0.001613028085557744]
[577.0171115836375, 574.6809029071276, 572.2498082287117, 574.4889794305665, 574.8044199897237, 573.2781644359364, 572.975223884038, 570.8019267488382, 571.2696362562766, 574.265442732162, 573.573287223346, 571.7625805366009, 571.1046574193855, 574.6470102498442, 570.2984956844371, 571.9091150566336, 574.389513374143, 563.0516307067087, 571.3278769872672, 572.0640998715754, 573.6796235272234, 570.7510325389362, 557.6897048464353, 541.2477032680737, 574.5750657454652, 574.9482471715967, 560.2049829584508, 574.7577822975184, 558.2179703359934, 570.3619842586373, 565.9663010736674, 563.5817844095737, 574.0667686838723, 572.8361506041924, 574.5273469687716, 564.6372467731032, 575.810587734018, 567.4512767599952, 573.0855005364233, 577.4281878506082, 571.985040048213, 575.5484301485609, 576.6491390279616, 574.7618556708858, 567.1835583533428, 546.6148885248034, 576.476759516723, 568.0867898215574, 578.7488765701778, 575.5101307098728, 556.5462325633468, 561.3465135633638, 575.1132604468771, 573.8251663461491, 576.3099671551615, 581.0393007230224, 575.3578474723439, 575.4596696100523, 576.4410679119718, 574.5398303838381, 574.1393330758947, 574.0781077163458, 574.9959526320818, 572.1370874007706, 571.6845249227802, 558.5590770104245, 579.797198958306, 580.840415908916, 576.0226593172964, 570.3863049133565, 582.090216248404, 576.8724847782338, 574.2965995471728, 571.532094030239, 573.8111228429893, 564.4021100129261, 567.1541528425017, 571.3400154575588, 565.6238659004626, 534.4996574469982, 570.9478886708278, 285.5416401439693, 570.6374128604033, 567.3803348719306, 554.5647093154168, 569.139356745258, 564.9500538096507, 570.3144727569145, 567.3025000349353, 564.9022796409091, 558.757578929035, 561.3886115501123, 564.9176949653571, 568.3178617106023, 561.726162646953, 545.6627329612322, 564.9158919688556, 271.6129551467388, 543.9149730484467, 540.6239691052402, 532.2540992097228, 543.2484113712372, 565.7699351349663, 571.4377608822767, 566.1148360617889, 561.7541307927804, 572.6689350684347, 569.6050459831929, 570.3012434669788, 565.8641375659251, 547.1041546686544, 563.1252023826461, 545.3497921501947, 566.3963961181483, 568.7070146155208, 562.3962515025685, 562.6669902000874, 564.1284140228378, 565.2385476421258, 566.3705946017215, 569.1387287722456, 572.4630718493806, 574.33512947821, 572.0715531360906, 572.7550986933908, 622.9552954345529, 623.296558691396, 622.2031154378192, 620.2078051556496, 621.4348823297489, 622.6589654780182, 622.3560878526426, 621.9820346651728, 622.5351254304138, 619.7563275728494, 622.1125143649984, 622.1485834008582, 620.5664637026223, 619.8604692599574, 620.2088854762967, 617.843976413213, 623.6069773851817, 626.756248644123, 621.7764297756233, 623.8053876492994, 620.7198844883768, 620.7836024387707, 620.9993012981726, 622.866601693193, 621.7119941406636, 621.8367616905485, 621.0444328231025, 622.2357430922984, 623.4050078599947, 623.5150279153941, 621.9408438947759, 622.6242024103152, 621.4090085375725, 620.6153361454783, 622.9113726516802, 622.8529020007486, 617.2113325450136, 620.6837024786137, 623.0516253730689, 621.8855661695368, 622.5970825414911, 617.1623831885339, 622.1062273060085, 619.8133647895621, 620.616860672525, 620.0555735596076, 623.3807849828095, 621.3940543071553, 619.9520076268391]
Elapsed: 0.08476371253337497~0.010731536473618155
Time per graph: 0.0017391171323168574~0.0002129238749820466
Speed: 579.8036305527448~41.21582751958387
Total Time: 0.0782
best val loss: 0.17483769357204437 test_score: 0.9583

Testing...
Test loss: 0.5182 score: 0.9167 time: 0.07s
test Score 0.9167
Epoch Time List: [0.31416981597431004, 0.3127801800146699, 0.31458094506524503, 0.31312357005663216, 0.31439768290147185, 0.312895200913772, 0.3126364592462778, 0.31407822598703206, 0.31565557001158595, 0.3162085807416588, 0.31594288162887096, 0.31597695173695683, 0.31374574778601527, 0.3127603989560157, 0.3205900960601866, 0.3144879259634763, 0.314925221959129, 0.33229523804038763, 0.3179192061070353, 0.31619290891103446, 0.3157863288652152, 0.32038561813533306, 0.31909928284585476, 0.3202613510657102, 0.3194185032043606, 0.31596908881329, 0.3185563071165234, 0.31427413295023143, 0.3285321651492268, 0.32338652899488807, 0.3189319570083171, 0.32141481596045196, 0.31353110494092107, 0.29431808670051396, 0.31116671301424503, 0.3111510418821126, 0.3161834829952568, 0.3218512150924653, 0.3188653700053692, 0.3162733349017799, 0.3183668910060078, 0.3212709079962224, 0.3140096771530807, 0.3180060258600861, 0.3217237973585725, 0.3190002138726413, 0.3186973640695214, 0.3187457798048854, 0.31367077585309744, 0.313710134010762, 0.3223672320600599, 0.31678846827708185, 0.31262060487642884, 0.3133541429415345, 0.3142203367315233, 0.3123230419587344, 0.3129633499775082, 0.31193255609832704, 0.31330577307380736, 0.3137015071697533, 0.312862535007298, 0.31369421910494566, 0.31349496776238084, 0.3140276938211173, 0.3189663579687476, 0.3144888214301318, 0.39378454512916505, 0.3088456951081753, 0.3080141041427851, 0.31471727578900754, 0.31525475694797933, 0.32389900321140885, 0.30732024484314024, 0.31067324290052056, 0.3158710931893438, 0.32445686403661966, 0.3765084049664438, 0.31218810100108385, 0.31227964884601533, 0.32106332411058247, 0.3235407250467688, 0.40631074784323573, 0.3164408428128809, 0.31867440277710557, 0.3223777231760323, 0.32655323296785355, 0.40911062783561647, 0.30282210814766586, 0.30235252901911736, 0.3210618360899389, 0.32854065485298634, 0.32065978716127574, 0.35313030309043825, 0.31187721830792725, 0.31651929789222777, 0.32130556902848184, 0.3252414441667497, 0.415032816817984, 0.33074082690291107, 0.33225325704552233, 0.33936856500804424, 0.33914260007441044, 0.3461439381353557, 0.3086288261692971, 0.30999927897937596, 0.3151470604352653, 0.3165932248812169, 0.39520349516533315, 0.3092905681114644, 0.3123139350209385, 0.3173269892577082, 0.3269022121094167, 0.3322978049982339, 0.3170971421059221, 0.3142550189513713, 0.3170661630574614, 0.31961122108623385, 0.31994221406057477, 0.30082654394209385, 0.31469543278217316, 0.3158775807823986, 0.31302067171782255, 0.31387167586945, 0.3166290756780654, 0.31732033286243677, 0.31470178184099495, 0.3102275400888175, 0.31207305821590126, 0.312410790938884, 0.3132577631622553, 0.313166718930006, 0.31516761193051934, 0.31449285009875894, 0.3133571012876928, 0.3138064709492028, 0.31317446171306074, 0.31524665490724146, 0.3162104308139533, 0.3131027799099684, 0.31328257475979626, 0.3150651198811829, 0.3138741059228778, 0.31709791300818324, 0.3221056628972292, 0.3186104311607778, 0.32098071090877056, 0.3222670517861843, 0.3185237697325647, 0.31996651203371584, 0.32145276037044823, 0.3217638828791678, 0.32179638277739286, 0.3217303438577801, 0.3156234580092132, 0.32039305311627686, 0.3162478001322597, 0.2939897873438895, 0.3147753549274057, 0.308469565352425, 0.3154040831141174, 0.295231208903715, 0.3143108852673322, 0.31514392886310816, 0.31521638692356646, 0.31604121229611337, 0.31578482198528945, 0.3146115872077644, 0.3148888887371868, 0.315369987860322, 0.3157227498013526, 0.3144952717702836, 0.31495381006971, 0.3160569060128182, 0.3169164774008095]
Total Epoch List: [75, 50, 49]
Total Time List: [0.08587030903436244, 0.08617483894340694, 0.07816563709639013]
T-times Epoch Time: 0.3058857383212734 ~ 0.018121623348312174
T-times Total Epoch: 53.111111111111114 ~ 3.624334762288908
T-times Total Time: 0.08111807481489247 ~ 0.0034563400634523515
T-times Inference Elapsed: 0.0824310596587509 ~ 0.004946310527451951
T-times Time Per Graph: 0.0016919592067523896 ~ 0.00010178231049843368
T-times Speed: 602.4554450324227 ~ 39.46780199513466
T-times cross validation test micro f1 score:0.9325391990473336 ~ 0.01705803465846154
T-times cross validation test precision:0.9773913043478261 ~ 0.023193236211860042
T-times cross validation test recall:0.8955555555555557 ~ 0.028120112744097214
T-times cross validation test f1_score:0.9325391990473336 ~ 0.017915839403352805
