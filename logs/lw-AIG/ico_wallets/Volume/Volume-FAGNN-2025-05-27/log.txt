Namespace(seed=15, model='FAGNN', dataset='ico_wallets/Volume', num_heads=8, num_layers=2, dim_hidden=64, dropout=0.1, epochs=1000, lr=0.0003, weight_decay=0.0001, batch_size=64, abs_pe=None, abs_pe_dim=20, num_class=2, outdir='./outdir/ico_wallets/Volume/seed15/khopgnn_gat_1_0.1_0.0003_0.0001_2_8_64_BN', warmup=10, layer_norm=False, use_edge_attr=False, edge_dim=128, gnn_type='gat', k_hop=1, global_pool='mean', se='khopgnn', ie='mamba', aggr='add', not_extract_node_feature=False, profile=False, k_ford=3, early_stop=1, early_stop_mindelta=-0.0, patience=20, training_times=3, Lambda=0.01, order='sd', use_cuda=True, batch_norm=True, save_logs=True)
Data(edge_index=[2, 334], edge_attr=[334, 2], x=[97, 14887], y=[1, 1], num_nodes=97)
Data(edge_index=[2, 334], edge_attr=[334, 2], x=[97, 14887], y=[1, 1], num_nodes=97)
Data(edge_index=[2, 276], edge_attr=[276, 2], x=[88, 14887], y=[1, 1], num_nodes=97)
========================training times:0========================
========================k_idx:0========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: FAGNN
FAGNN(
  (conv1): NeighborFilterConv()
  (conv2): NeighborFilterConv()
  (ffn): Sequential(
    (0): Linear(in_features=14951, out_features=64, bias=True)
    (1): ReLU()
    (2): Linear(in_features=64, out_features=64, bias=True)
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=64, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=64, out_features=2, bias=True)
  )
)
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7f904e36b5e0>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.6953;  Loss pred: 0.6953; Loss self: 0.0000; time: 1.43s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6940 score: 0.5102 time: 0.65s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6967 score: 0.4898 time: 0.63s
Epoch 2/1000, LR 0.000000
Train loss: 0.6953;  Loss pred: 0.6953; Loss self: 0.0000; time: 0.64s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6939 score: 0.5102 time: 0.62s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6967 score: 0.4898 time: 0.47s
Epoch 3/1000, LR 0.000030
Train loss: 0.6953;  Loss pred: 0.6953; Loss self: 0.0000; time: 0.83s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6938 score: 0.5102 time: 2.94s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6966 score: 0.4898 time: 3.71s
Epoch 4/1000, LR 0.000060
Train loss: 0.6952;  Loss pred: 0.6952; Loss self: 0.0000; time: 0.63s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6937 score: 0.5102 time: 0.50s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6964 score: 0.4898 time: 0.40s
Epoch 5/1000, LR 0.000090
Train loss: 0.6951;  Loss pred: 0.6951; Loss self: 0.0000; time: 0.60s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6935 score: 0.5102 time: 0.47s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6962 score: 0.4898 time: 0.38s
Epoch 6/1000, LR 0.000120
Train loss: 0.6949;  Loss pred: 0.6949; Loss self: 0.0000; time: 0.51s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6933 score: 0.5102 time: 0.54s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6960 score: 0.4898 time: 0.38s
Epoch 7/1000, LR 0.000150
Train loss: 0.6947;  Loss pred: 0.6947; Loss self: 0.0000; time: 0.60s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6931 score: 0.5102 time: 0.49s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6957 score: 0.4898 time: 0.37s
Epoch 8/1000, LR 0.000180
Train loss: 0.6945;  Loss pred: 0.6945; Loss self: 0.0000; time: 0.51s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6929 score: 0.5102 time: 0.59s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6954 score: 0.4898 time: 0.37s
Epoch 9/1000, LR 0.000210
Train loss: 0.6942;  Loss pred: 0.6942; Loss self: 0.0000; time: 0.49s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6926 score: 0.5102 time: 0.59s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6951 score: 0.4898 time: 0.36s
Epoch 10/1000, LR 0.000240
Train loss: 0.6939;  Loss pred: 0.6939; Loss self: 0.0000; time: 0.52s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6922 score: 0.5102 time: 0.48s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6947 score: 0.4898 time: 0.37s
Epoch 11/1000, LR 0.000270
Train loss: 0.6935;  Loss pred: 0.6935; Loss self: 0.0000; time: 0.50s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6919 score: 0.5102 time: 0.58s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6943 score: 0.4898 time: 0.42s
Epoch 12/1000, LR 0.000270
Train loss: 0.6932;  Loss pred: 0.6932; Loss self: 0.0000; time: 0.50s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6916 score: 0.5102 time: 0.49s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6940 score: 0.4898 time: 0.37s
Epoch 13/1000, LR 0.000270
Train loss: 0.6929;  Loss pred: 0.6929; Loss self: 0.0000; time: 0.60s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6913 score: 0.5102 time: 0.54s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6936 score: 0.4898 time: 0.41s
Epoch 14/1000, LR 0.000270
Train loss: 0.6926;  Loss pred: 0.6926; Loss self: 0.0000; time: 0.63s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6910 score: 0.5102 time: 0.62s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6933 score: 0.4898 time: 0.47s
Epoch 15/1000, LR 0.000270
Train loss: 0.6922;  Loss pred: 0.6922; Loss self: 0.0000; time: 0.77s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6907 score: 0.5102 time: 0.60s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6929 score: 0.4898 time: 0.38s
Epoch 16/1000, LR 0.000270
Train loss: 0.6920;  Loss pred: 0.6920; Loss self: 0.0000; time: 0.51s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6903 score: 0.5102 time: 0.49s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6926 score: 0.4898 time: 0.62s
Epoch 17/1000, LR 0.000270
Train loss: 0.6916;  Loss pred: 0.6916; Loss self: 0.0000; time: 0.63s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6899 score: 0.5102 time: 0.64s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6922 score: 0.4898 time: 0.39s
Epoch 18/1000, LR 0.000270
Train loss: 0.6912;  Loss pred: 0.6912; Loss self: 0.0000; time: 0.51s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6895 score: 0.5102 time: 0.63s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6918 score: 0.4898 time: 0.40s
Epoch 19/1000, LR 0.000270
Train loss: 0.6908;  Loss pred: 0.6908; Loss self: 0.0000; time: 0.50s
Val loss: 0.6890 score: 0.5306 time: 0.59s
Test loss: 0.6914 score: 0.5102 time: 0.37s
Epoch 20/1000, LR 0.000270
Train loss: 0.6903;  Loss pred: 0.6903; Loss self: 0.0000; time: 0.49s
Val loss: 0.6885 score: 0.6122 time: 0.56s
Test loss: 0.6910 score: 0.5714 time: 0.47s
Epoch 21/1000, LR 0.000270
Train loss: 0.6899;  Loss pred: 0.6899; Loss self: 0.0000; time: 0.74s
Val loss: 0.6880 score: 0.7347 time: 0.67s
Test loss: 0.6906 score: 0.5918 time: 0.50s
Epoch 22/1000, LR 0.000270
Train loss: 0.6894;  Loss pred: 0.6894; Loss self: 0.0000; time: 0.63s
Val loss: 0.6874 score: 0.8163 time: 0.51s
Test loss: 0.6901 score: 0.6122 time: 0.47s
Epoch 23/1000, LR 0.000270
Train loss: 0.6889;  Loss pred: 0.6889; Loss self: 0.0000; time: 0.79s
Val loss: 0.6867 score: 0.7755 time: 0.62s
Test loss: 0.6897 score: 0.6939 time: 0.48s
Epoch 24/1000, LR 0.000270
Train loss: 0.6882;  Loss pred: 0.6882; Loss self: 0.0000; time: 0.63s
Val loss: 0.6860 score: 0.7959 time: 0.63s
Test loss: 0.6891 score: 0.6939 time: 0.47s
Epoch 25/1000, LR 0.000270
Train loss: 0.6876;  Loss pred: 0.6876; Loss self: 0.0000; time: 0.64s
Val loss: 0.6852 score: 0.7755 time: 0.78s
Test loss: 0.6886 score: 0.6327 time: 0.47s
Epoch 26/1000, LR 0.000270
Train loss: 0.6868;  Loss pred: 0.6868; Loss self: 0.0000; time: 0.63s
Val loss: 0.6844 score: 0.7143 time: 0.63s
Test loss: 0.6881 score: 0.6122 time: 0.47s
Epoch 27/1000, LR 0.000270
Train loss: 0.6863;  Loss pred: 0.6863; Loss self: 0.0000; time: 0.69s
Val loss: 0.6835 score: 0.6531 time: 0.72s
Test loss: 0.6875 score: 0.6122 time: 0.48s
Epoch 28/1000, LR 0.000270
Train loss: 0.6855;  Loss pred: 0.6855; Loss self: 0.0000; time: 1.25s
Val loss: 0.6825 score: 0.6122 time: 0.65s
Test loss: 0.6869 score: 0.5918 time: 0.48s
Epoch 29/1000, LR 0.000270
Train loss: 0.6843;  Loss pred: 0.6843; Loss self: 0.0000; time: 0.79s
Val loss: 0.6814 score: 0.6122 time: 0.63s
Test loss: 0.6861 score: 0.5918 time: 0.49s
Epoch 30/1000, LR 0.000270
Train loss: 0.6834;  Loss pred: 0.6834; Loss self: 0.0000; time: 0.51s
Val loss: 0.6803 score: 0.6122 time: 0.50s
Test loss: 0.6854 score: 0.5918 time: 0.38s
Epoch 31/1000, LR 0.000270
Train loss: 0.6823;  Loss pred: 0.6823; Loss self: 0.0000; time: 0.66s
Val loss: 0.6790 score: 0.6122 time: 0.62s
Test loss: 0.6845 score: 0.5918 time: 0.46s
Epoch 32/1000, LR 0.000270
Train loss: 0.6810;  Loss pred: 0.6810; Loss self: 0.0000; time: 0.51s
Val loss: 0.6776 score: 0.6122 time: 0.52s
Test loss: 0.6835 score: 0.5918 time: 0.38s
Epoch 33/1000, LR 0.000270
Train loss: 0.6797;  Loss pred: 0.6797; Loss self: 0.0000; time: 0.54s
Val loss: 0.6760 score: 0.6122 time: 0.64s
Test loss: 0.6825 score: 0.5918 time: 0.38s
Epoch 34/1000, LR 0.000270
Train loss: 0.6791;  Loss pred: 0.6791; Loss self: 0.0000; time: 0.53s
Val loss: 0.6742 score: 0.6122 time: 0.52s
Test loss: 0.6813 score: 0.5918 time: 0.39s
Epoch 35/1000, LR 0.000270
Train loss: 0.6774;  Loss pred: 0.6774; Loss self: 0.0000; time: 0.54s
Val loss: 0.6723 score: 0.6122 time: 0.65s
Test loss: 0.6801 score: 0.6122 time: 0.39s
Epoch 36/1000, LR 0.000270
Train loss: 0.6758;  Loss pred: 0.6758; Loss self: 0.0000; time: 0.52s
Val loss: 0.6701 score: 0.6122 time: 0.66s
Test loss: 0.6786 score: 0.6327 time: 0.48s
Epoch 37/1000, LR 0.000270
Train loss: 0.6731;  Loss pred: 0.6731; Loss self: 0.0000; time: 0.78s
Val loss: 0.6678 score: 0.6531 time: 0.63s
Test loss: 0.6771 score: 0.6327 time: 0.49s
Epoch 38/1000, LR 0.000270
Train loss: 0.6708;  Loss pred: 0.6708; Loss self: 0.0000; time: 0.51s
Val loss: 0.6652 score: 0.6531 time: 0.50s
Test loss: 0.6754 score: 0.6122 time: 0.47s
Epoch 39/1000, LR 0.000269
Train loss: 0.6681;  Loss pred: 0.6681; Loss self: 0.0000; time: 0.78s
Val loss: 0.6624 score: 0.6531 time: 0.66s
Test loss: 0.6736 score: 0.6122 time: 0.48s
Epoch 40/1000, LR 0.000269
Train loss: 0.6648;  Loss pred: 0.6648; Loss self: 0.0000; time: 0.61s
Val loss: 0.6593 score: 0.6531 time: 0.55s
Test loss: 0.6716 score: 0.6122 time: 0.41s
Epoch 41/1000, LR 0.000269
Train loss: 0.6632;  Loss pred: 0.6632; Loss self: 0.0000; time: 0.55s
Val loss: 0.6560 score: 0.6735 time: 0.65s
Test loss: 0.6694 score: 0.6122 time: 0.39s
Epoch 42/1000, LR 0.000269
Train loss: 0.6612;  Loss pred: 0.6612; Loss self: 0.0000; time: 0.57s
Val loss: 0.6525 score: 0.6939 time: 0.64s
Test loss: 0.6670 score: 0.6122 time: 0.47s
Epoch 43/1000, LR 0.000269
Train loss: 0.6567;  Loss pred: 0.6567; Loss self: 0.0000; time: 0.63s
Val loss: 0.6486 score: 0.7143 time: 0.67s
Test loss: 0.6644 score: 0.6327 time: 0.38s
Epoch 44/1000, LR 0.000269
Train loss: 0.6541;  Loss pred: 0.6541; Loss self: 0.0000; time: 0.55s
Val loss: 0.6444 score: 0.7347 time: 0.59s
Test loss: 0.6616 score: 0.6327 time: 0.48s
Epoch 45/1000, LR 0.000269
Train loss: 0.6491;  Loss pred: 0.6491; Loss self: 0.0000; time: 0.77s
Val loss: 0.6400 score: 0.7347 time: 0.65s
Test loss: 0.6586 score: 0.6327 time: 0.48s
Epoch 46/1000, LR 0.000269
Train loss: 0.6452;  Loss pred: 0.6452; Loss self: 0.0000; time: 0.61s
Val loss: 0.6351 score: 0.7551 time: 0.51s
Test loss: 0.6553 score: 0.6327 time: 0.39s
Epoch 47/1000, LR 0.000269
Train loss: 0.6409;  Loss pred: 0.6409; Loss self: 0.0000; time: 0.69s
Val loss: 0.6299 score: 0.7755 time: 0.49s
Test loss: 0.6517 score: 0.6122 time: 0.38s
Epoch 48/1000, LR 0.000269
Train loss: 0.6373;  Loss pred: 0.6373; Loss self: 0.0000; time: 0.52s
Val loss: 0.6244 score: 0.7755 time: 0.49s
Test loss: 0.6479 score: 0.6122 time: 0.38s
Epoch 49/1000, LR 0.000269
Train loss: 0.6307;  Loss pred: 0.6307; Loss self: 0.0000; time: 0.55s
Val loss: 0.6183 score: 0.7755 time: 0.63s
Test loss: 0.6437 score: 0.6327 time: 0.39s
Epoch 50/1000, LR 0.000269
Train loss: 0.6253;  Loss pred: 0.6253; Loss self: 0.0000; time: 0.53s
Val loss: 0.6118 score: 0.8163 time: 0.53s
Test loss: 0.6392 score: 0.6939 time: 0.38s
Epoch 51/1000, LR 0.000269
Train loss: 0.6184;  Loss pred: 0.6184; Loss self: 0.0000; time: 0.51s
Val loss: 0.6049 score: 0.8367 time: 0.64s
Test loss: 0.6344 score: 0.6939 time: 0.39s
Epoch 52/1000, LR 0.000269
Train loss: 0.6128;  Loss pred: 0.6128; Loss self: 0.0000; time: 0.87s
Val loss: 0.5974 score: 0.8776 time: 0.63s
Test loss: 0.6292 score: 0.6939 time: 0.39s
Epoch 53/1000, LR 0.000269
Train loss: 0.6043;  Loss pred: 0.6043; Loss self: 0.0000; time: 0.66s
Val loss: 0.5895 score: 0.8980 time: 0.51s
Test loss: 0.6237 score: 0.7143 time: 0.39s
Epoch 54/1000, LR 0.000269
Train loss: 0.5975;  Loss pred: 0.5975; Loss self: 0.0000; time: 0.52s
Val loss: 0.5810 score: 0.9184 time: 0.52s
Test loss: 0.6178 score: 0.7143 time: 0.39s
Epoch 55/1000, LR 0.000269
Train loss: 0.5909;  Loss pred: 0.5909; Loss self: 0.0000; time: 0.67s
Val loss: 0.5720 score: 0.9592 time: 0.49s
Test loss: 0.6115 score: 0.7347 time: 0.39s
Epoch 56/1000, LR 0.000269
Train loss: 0.5774;  Loss pred: 0.5774; Loss self: 0.0000; time: 0.52s
Val loss: 0.5623 score: 0.9592 time: 0.50s
Test loss: 0.6048 score: 0.7347 time: 0.38s
Epoch 57/1000, LR 0.000269
Train loss: 0.5670;  Loss pred: 0.5670; Loss self: 0.0000; time: 0.52s
Val loss: 0.5520 score: 0.9592 time: 0.63s
Test loss: 0.5976 score: 0.7551 time: 0.38s
Epoch 58/1000, LR 0.000269
Train loss: 0.5602;  Loss pred: 0.5602; Loss self: 0.0000; time: 0.51s
Val loss: 0.5410 score: 0.9592 time: 0.49s
Test loss: 0.5900 score: 0.7755 time: 0.38s
Epoch 59/1000, LR 0.000268
Train loss: 0.5475;  Loss pred: 0.5475; Loss self: 0.0000; time: 0.52s
Val loss: 0.5294 score: 0.9592 time: 0.64s
Test loss: 0.5818 score: 0.7959 time: 0.39s
Epoch 60/1000, LR 0.000268
Train loss: 0.5386;  Loss pred: 0.5386; Loss self: 0.0000; time: 0.51s
Val loss: 0.5172 score: 0.9796 time: 0.49s
Test loss: 0.5733 score: 0.8163 time: 0.38s
Epoch 61/1000, LR 0.000268
Train loss: 0.5269;  Loss pred: 0.5269; Loss self: 0.0000; time: 0.64s
Val loss: 0.5045 score: 0.9796 time: 0.50s
Test loss: 0.5643 score: 0.8367 time: 0.39s
Epoch 62/1000, LR 0.000268
Train loss: 0.5117;  Loss pred: 0.5117; Loss self: 0.0000; time: 0.54s
Val loss: 0.4913 score: 0.9796 time: 0.64s
Test loss: 0.5549 score: 0.8367 time: 0.49s
Epoch 63/1000, LR 0.000268
Train loss: 0.5020;  Loss pred: 0.5020; Loss self: 0.0000; time: 0.84s
Val loss: 0.4777 score: 0.9796 time: 0.67s
Test loss: 0.5452 score: 0.8367 time: 0.48s
Epoch 64/1000, LR 0.000268
Train loss: 0.4799;  Loss pred: 0.4799; Loss self: 0.0000; time: 0.63s
Val loss: 0.4638 score: 0.9796 time: 0.64s
Test loss: 0.5352 score: 0.8367 time: 0.50s
Epoch 65/1000, LR 0.000268
Train loss: 0.4695;  Loss pred: 0.4695; Loss self: 0.0000; time: 0.68s
Val loss: 0.4498 score: 0.9796 time: 0.79s
Test loss: 0.5250 score: 0.8571 time: 0.47s
Epoch 66/1000, LR 0.000268
Train loss: 0.4541;  Loss pred: 0.4541; Loss self: 0.0000; time: 0.64s
Val loss: 0.4357 score: 0.9796 time: 0.65s
Test loss: 0.5145 score: 0.8571 time: 0.48s
Epoch 67/1000, LR 0.000268
Train loss: 0.4419;  Loss pred: 0.4419; Loss self: 0.0000; time: 0.63s
Val loss: 0.4215 score: 0.9796 time: 0.73s
Test loss: 0.5040 score: 0.8776 time: 0.47s
Epoch 68/1000, LR 0.000268
Train loss: 0.4286;  Loss pred: 0.4286; Loss self: 0.0000; time: 0.54s
Val loss: 0.4074 score: 0.9796 time: 0.53s
Test loss: 0.4934 score: 0.8776 time: 0.39s
Epoch 69/1000, LR 0.000268
Train loss: 0.4198;  Loss pred: 0.4198; Loss self: 0.0000; time: 0.79s
Val loss: 0.3935 score: 0.9796 time: 0.81s
Test loss: 0.4829 score: 0.8776 time: 0.50s
Epoch 70/1000, LR 0.000268
Train loss: 0.3924;  Loss pred: 0.3924; Loss self: 0.0000; time: 0.65s
Val loss: 0.3798 score: 0.9796 time: 0.56s
Test loss: 0.4725 score: 0.8776 time: 0.38s
Epoch 71/1000, LR 0.000268
Train loss: 0.3768;  Loss pred: 0.3768; Loss self: 0.0000; time: 0.68s
Val loss: 0.3666 score: 0.9796 time: 0.49s
Test loss: 0.4623 score: 0.8776 time: 0.38s
Epoch 72/1000, LR 0.000267
Train loss: 0.3672;  Loss pred: 0.3672; Loss self: 0.0000; time: 0.60s
Val loss: 0.3537 score: 0.9796 time: 0.64s
Test loss: 0.4525 score: 0.8776 time: 0.49s
Epoch 73/1000, LR 0.000267
Train loss: 0.3507;  Loss pred: 0.3507; Loss self: 0.0000; time: 0.63s
Val loss: 0.3413 score: 0.9796 time: 0.77s
Test loss: 0.4430 score: 0.8776 time: 0.50s
Epoch 74/1000, LR 0.000267
Train loss: 0.3407;  Loss pred: 0.3407; Loss self: 0.0000; time: 0.55s
Val loss: 0.3294 score: 0.9796 time: 0.62s
Test loss: 0.4338 score: 0.8776 time: 0.38s
Epoch 75/1000, LR 0.000267
Train loss: 0.3215;  Loss pred: 0.3215; Loss self: 0.0000; time: 0.59s
Val loss: 0.3178 score: 0.9796 time: 0.81s
Test loss: 0.4252 score: 0.8776 time: 0.47s
Epoch 76/1000, LR 0.000267
Train loss: 0.3199;  Loss pred: 0.3199; Loss self: 0.0000; time: 0.63s
Val loss: 0.3068 score: 0.9796 time: 0.62s
Test loss: 0.4168 score: 0.8776 time: 0.44s
Epoch 77/1000, LR 0.000267
Train loss: 0.3033;  Loss pred: 0.3033; Loss self: 0.0000; time: 0.78s
Val loss: 0.2964 score: 0.9796 time: 0.63s
Test loss: 0.4087 score: 0.8776 time: 0.48s
Epoch 78/1000, LR 0.000267
Train loss: 0.2918;  Loss pred: 0.2918; Loss self: 0.0000; time: 0.64s
Val loss: 0.2866 score: 0.9796 time: 0.65s
Test loss: 0.4007 score: 0.8776 time: 0.48s
Epoch 79/1000, LR 0.000267
Train loss: 0.2817;  Loss pred: 0.2817; Loss self: 0.0000; time: 0.78s
Val loss: 0.2774 score: 0.9796 time: 0.55s
Test loss: 0.3929 score: 0.8776 time: 0.41s
Epoch 80/1000, LR 0.000267
Train loss: 0.2642;  Loss pred: 0.2642; Loss self: 0.0000; time: 0.51s
Val loss: 0.2686 score: 0.9796 time: 0.48s
Test loss: 0.3852 score: 0.8776 time: 0.38s
Epoch 81/1000, LR 0.000267
Train loss: 0.2540;  Loss pred: 0.2540; Loss self: 0.0000; time: 0.71s
Val loss: 0.2603 score: 0.9796 time: 3.65s
Test loss: 0.3777 score: 0.8776 time: 3.66s
Epoch 82/1000, LR 0.000267
Train loss: 0.2420;  Loss pred: 0.2420; Loss self: 0.0000; time: 0.51s
Val loss: 0.2523 score: 0.9796 time: 0.50s
Test loss: 0.3705 score: 0.8776 time: 0.37s
Epoch 83/1000, LR 0.000266
Train loss: 0.2359;  Loss pred: 0.2359; Loss self: 0.0000; time: 0.55s
Val loss: 0.2448 score: 0.9796 time: 0.62s
Test loss: 0.3636 score: 0.8776 time: 0.39s
Epoch 84/1000, LR 0.000266
Train loss: 0.2232;  Loss pred: 0.2232; Loss self: 0.0000; time: 0.51s
Val loss: 0.2374 score: 0.9796 time: 0.53s
Test loss: 0.3572 score: 0.8776 time: 0.38s
Epoch 85/1000, LR 0.000266
Train loss: 0.2057;  Loss pred: 0.2057; Loss self: 0.0000; time: 0.63s
Val loss: 0.2301 score: 0.9592 time: 0.52s
Test loss: 0.3514 score: 0.8776 time: 0.38s
Epoch 86/1000, LR 0.000266
Train loss: 0.2005;  Loss pred: 0.2005; Loss self: 0.0000; time: 0.54s
Val loss: 0.2232 score: 0.9592 time: 0.49s
Test loss: 0.3457 score: 0.8776 time: 0.38s
Epoch 87/1000, LR 0.000266
Train loss: 0.1899;  Loss pred: 0.1899; Loss self: 0.0000; time: 0.62s
Val loss: 0.2163 score: 0.9592 time: 0.49s
Test loss: 0.3409 score: 0.8776 time: 0.38s
Epoch 88/1000, LR 0.000266
Train loss: 0.1822;  Loss pred: 0.1822; Loss self: 0.0000; time: 0.51s
Val loss: 0.2099 score: 0.9592 time: 0.49s
Test loss: 0.3363 score: 0.8776 time: 0.38s
Epoch 89/1000, LR 0.000266
Train loss: 0.1719;  Loss pred: 0.1719; Loss self: 0.0000; time: 0.50s
Val loss: 0.2036 score: 0.9592 time: 0.61s
Test loss: 0.3324 score: 0.8776 time: 0.37s
Epoch 90/1000, LR 0.000266
Train loss: 0.1628;  Loss pred: 0.1628; Loss self: 0.0000; time: 0.50s
Val loss: 0.1975 score: 0.9592 time: 0.50s
Test loss: 0.3291 score: 0.8776 time: 0.39s
Epoch 91/1000, LR 0.000266
Train loss: 0.1576;  Loss pred: 0.1576; Loss self: 0.0000; time: 0.51s
Val loss: 0.1916 score: 0.9592 time: 0.60s
Test loss: 0.3266 score: 0.8776 time: 0.38s
Epoch 92/1000, LR 0.000266
Train loss: 0.1548;  Loss pred: 0.1548; Loss self: 0.0000; time: 0.50s
Val loss: 0.1858 score: 0.9592 time: 0.48s
Test loss: 0.3251 score: 0.8776 time: 0.38s
Epoch 93/1000, LR 0.000265
Train loss: 0.1425;  Loss pred: 0.1425; Loss self: 0.0000; time: 0.64s
Val loss: 0.1806 score: 0.9592 time: 0.50s
Test loss: 0.3237 score: 0.8776 time: 0.39s
Epoch 94/1000, LR 0.000265
Train loss: 0.1329;  Loss pred: 0.1329; Loss self: 0.0000; time: 0.52s
Val loss: 0.1758 score: 0.9592 time: 0.49s
Test loss: 0.3227 score: 0.8776 time: 0.38s
Epoch 95/1000, LR 0.000265
Train loss: 0.1304;  Loss pred: 0.1304; Loss self: 0.0000; time: 0.62s
Val loss: 0.1720 score: 0.9592 time: 0.50s
Test loss: 0.3210 score: 0.8776 time: 0.37s
Epoch 96/1000, LR 0.000265
Train loss: 0.1214;  Loss pred: 0.1214; Loss self: 0.0000; time: 0.53s
Val loss: 0.1688 score: 0.9592 time: 0.54s
Test loss: 0.3197 score: 0.8776 time: 0.37s
Epoch 97/1000, LR 0.000265
Train loss: 0.1112;  Loss pred: 0.1112; Loss self: 0.0000; time: 0.51s
Val loss: 0.1659 score: 0.9388 time: 0.62s
Test loss: 0.3194 score: 0.8980 time: 0.39s
Epoch 98/1000, LR 0.000265
Train loss: 0.1026;  Loss pred: 0.1026; Loss self: 0.0000; time: 0.51s
Val loss: 0.1632 score: 0.9388 time: 0.51s
Test loss: 0.3200 score: 0.8980 time: 0.39s
Epoch 99/1000, LR 0.000265
Train loss: 0.0981;  Loss pred: 0.0981; Loss self: 0.0000; time: 0.54s
Val loss: 0.1608 score: 0.9388 time: 0.64s
Test loss: 0.3212 score: 0.8980 time: 0.38s
Epoch 100/1000, LR 0.000265
Train loss: 0.0947;  Loss pred: 0.0947; Loss self: 0.0000; time: 0.54s
Val loss: 0.1586 score: 0.9388 time: 0.52s
Test loss: 0.3234 score: 0.8980 time: 0.40s
Epoch 101/1000, LR 0.000265
Train loss: 0.0847;  Loss pred: 0.0847; Loss self: 0.0000; time: 0.63s
Val loss: 0.1566 score: 0.9388 time: 0.48s
Test loss: 0.3261 score: 0.8980 time: 0.44s
Epoch 102/1000, LR 0.000264
Train loss: 0.0814;  Loss pred: 0.0814; Loss self: 0.0000; time: 0.51s
Val loss: 0.1549 score: 0.9388 time: 0.49s
Test loss: 0.3294 score: 0.8980 time: 0.39s
Epoch 103/1000, LR 0.000264
Train loss: 0.0838;  Loss pred: 0.0838; Loss self: 0.0000; time: 0.63s
Val loss: 0.1535 score: 0.9592 time: 0.54s
Test loss: 0.3334 score: 0.8980 time: 0.37s
Epoch 104/1000, LR 0.000264
Train loss: 0.0743;  Loss pred: 0.0743; Loss self: 0.0000; time: 0.51s
Val loss: 0.1523 score: 0.9592 time: 0.50s
Test loss: 0.3379 score: 0.8980 time: 0.38s
Epoch 105/1000, LR 0.000264
Train loss: 0.0754;  Loss pred: 0.0754; Loss self: 0.0000; time: 0.52s
Val loss: 0.1511 score: 0.9592 time: 0.62s
Test loss: 0.3432 score: 0.8776 time: 0.38s
Epoch 106/1000, LR 0.000264
Train loss: 0.0699;  Loss pred: 0.0699; Loss self: 0.0000; time: 0.51s
Val loss: 0.1505 score: 0.9592 time: 0.51s
Test loss: 0.3480 score: 0.8776 time: 0.38s
Epoch 107/1000, LR 0.000264
Train loss: 0.0634;  Loss pred: 0.0634; Loss self: 0.0000; time: 0.53s
Val loss: 0.1506 score: 0.9592 time: 0.63s
Test loss: 0.3521 score: 0.8776 time: 0.37s
     INFO: Early stopping counter 1 of 20
Epoch 108/1000, LR 0.000264
Train loss: 0.0632;  Loss pred: 0.0632; Loss self: 0.0000; time: 0.52s
Val loss: 0.1509 score: 0.9592 time: 0.50s
Test loss: 0.3565 score: 0.8776 time: 0.38s
     INFO: Early stopping counter 2 of 20
Epoch 109/1000, LR 0.000264
Train loss: 0.0589;  Loss pred: 0.0589; Loss self: 0.0000; time: 0.67s
Val loss: 0.1514 score: 0.9592 time: 0.50s
Test loss: 0.3610 score: 0.8776 time: 0.38s
     INFO: Early stopping counter 3 of 20
Epoch 110/1000, LR 0.000263
Train loss: 0.0594;  Loss pred: 0.0594; Loss self: 0.0000; time: 0.52s
Val loss: 0.1522 score: 0.9592 time: 0.49s
Test loss: 0.3653 score: 0.8980 time: 0.41s
     INFO: Early stopping counter 4 of 20
Epoch 111/1000, LR 0.000263
Train loss: 0.0563;  Loss pred: 0.0563; Loss self: 0.0000; time: 0.64s
Val loss: 0.1531 score: 0.9592 time: 0.49s
Test loss: 0.3697 score: 0.8980 time: 0.39s
     INFO: Early stopping counter 5 of 20
Epoch 112/1000, LR 0.000263
Train loss: 0.0529;  Loss pred: 0.0529; Loss self: 0.0000; time: 0.51s
Val loss: 0.1541 score: 0.9592 time: 0.54s
Test loss: 0.3741 score: 0.8980 time: 0.47s
     INFO: Early stopping counter 6 of 20
Epoch 113/1000, LR 0.000263
Train loss: 0.0518;  Loss pred: 0.0518; Loss self: 0.0000; time: 0.63s
Val loss: 0.1551 score: 0.9592 time: 0.76s
Test loss: 0.3788 score: 0.8980 time: 0.51s
     INFO: Early stopping counter 7 of 20
Epoch 114/1000, LR 0.000263
Train loss: 0.0540;  Loss pred: 0.0540; Loss self: 0.0000; time: 0.63s
Val loss: 0.1561 score: 0.9592 time: 0.62s
Test loss: 0.3838 score: 0.8980 time: 0.47s
     INFO: Early stopping counter 8 of 20
Epoch 115/1000, LR 0.000263
Train loss: 0.0479;  Loss pred: 0.0479; Loss self: 0.0000; time: 0.66s
Val loss: 0.1568 score: 0.9592 time: 0.75s
Test loss: 0.3892 score: 0.8980 time: 0.44s
     INFO: Early stopping counter 9 of 20
Epoch 116/1000, LR 0.000263
Train loss: 0.0497;  Loss pred: 0.0497; Loss self: 0.0000; time: 0.52s
Val loss: 0.1575 score: 0.9592 time: 0.50s
Test loss: 0.3945 score: 0.8980 time: 0.42s
     INFO: Early stopping counter 10 of 20
Epoch 117/1000, LR 0.000262
Train loss: 0.0493;  Loss pred: 0.0493; Loss self: 0.0000; time: 0.63s
Val loss: 0.1584 score: 0.9592 time: 0.58s
Test loss: 0.3993 score: 0.8776 time: 0.47s
     INFO: Early stopping counter 11 of 20
Epoch 118/1000, LR 0.000262
Train loss: 0.0428;  Loss pred: 0.0428; Loss self: 0.0000; time: 0.63s
Val loss: 0.1591 score: 0.9592 time: 0.65s
Test loss: 0.4043 score: 0.8776 time: 0.48s
     INFO: Early stopping counter 12 of 20
Epoch 119/1000, LR 0.000262
Train loss: 0.0415;  Loss pred: 0.0415; Loss self: 0.0000; time: 0.79s
Val loss: 0.1597 score: 0.9592 time: 0.56s
Test loss: 0.4093 score: 0.8776 time: 0.39s
     INFO: Early stopping counter 13 of 20
Epoch 120/1000, LR 0.000262
Train loss: 0.0475;  Loss pred: 0.0475; Loss self: 0.0000; time: 0.54s
Val loss: 0.1613 score: 0.9592 time: 0.50s
Test loss: 0.4127 score: 0.8776 time: 0.38s
     INFO: Early stopping counter 14 of 20
Epoch 121/1000, LR 0.000262
Train loss: 0.0385;  Loss pred: 0.0385; Loss self: 0.0000; time: 0.58s
Val loss: 0.1627 score: 0.9592 time: 0.69s
Test loss: 0.4163 score: 0.8776 time: 0.40s
     INFO: Early stopping counter 15 of 20
Epoch 122/1000, LR 0.000262
Train loss: 0.0400;  Loss pred: 0.0400; Loss self: 0.0000; time: 0.54s
Val loss: 0.1638 score: 0.9592 time: 0.51s
Test loss: 0.4202 score: 0.8776 time: 0.40s
     INFO: Early stopping counter 16 of 20
Epoch 123/1000, LR 0.000262
Train loss: 0.0397;  Loss pred: 0.0397; Loss self: 0.0000; time: 0.66s
Val loss: 0.1646 score: 0.9592 time: 0.69s
Test loss: 0.4243 score: 0.8776 time: 0.41s
     INFO: Early stopping counter 17 of 20
Epoch 124/1000, LR 0.000261
Train loss: 0.0370;  Loss pred: 0.0370; Loss self: 0.0000; time: 0.53s
Val loss: 0.1649 score: 0.9592 time: 0.50s
Test loss: 0.4291 score: 0.8776 time: 0.39s
     INFO: Early stopping counter 18 of 20
Epoch 125/1000, LR 0.000261
Train loss: 0.0358;  Loss pred: 0.0358; Loss self: 0.0000; time: 1.06s
Val loss: 0.1651 score: 0.9592 time: 0.63s
Test loss: 0.4342 score: 0.8776 time: 0.49s
     INFO: Early stopping counter 19 of 20
Epoch 126/1000, LR 0.000261
Train loss: 0.0378;  Loss pred: 0.0378; Loss self: 0.0000; time: 0.62s
Val loss: 0.1651 score: 0.9592 time: 0.62s
Test loss: 0.4392 score: 0.8776 time: 0.48s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 105,   Train_Loss: 0.0699,   Val_Loss: 0.1505,   Val_Precision: 0.9583,   Val_Recall: 0.9583,   Val_accuracy: 0.9583,   Val_Score: 0.9592,   Val_Loss: 0.1505,   Test_Precision: 0.8800,   Test_Recall: 0.8800,   Test_accuracy: 0.8800,   Test_Score: 0.8776,   Test_loss: 0.3480


[0.6371192489750683, 0.4808532171882689, 3.714370231842622, 0.4096544119529426, 0.38975901319645345, 0.3850430881138891, 0.3710247480776161, 0.3783402000553906, 0.3670420781709254, 0.3768683730158955, 0.421046560164541, 0.37572586303576827, 0.411221066955477, 0.4810259588994086, 0.38594911084510386, 0.6231586679350585, 0.397189409006387, 0.4039589809253812, 0.37478494900278747, 0.477442956995219, 0.5091569651849568, 0.47591616678982973, 0.48965000407770276, 0.4802797739394009, 0.47649064287543297, 0.480065189069137, 0.491782542783767, 0.49172582500614226, 0.49699680716730654, 0.3862174970563501, 0.46144315507262945, 0.3836425740737468, 0.3893774710595608, 0.3984540731180459, 0.3995904908515513, 0.48709436203353107, 0.49183662002906203, 0.4799299337901175, 0.4831981249153614, 0.4137717669364065, 0.39104513195343316, 0.47490956890396774, 0.386658368865028, 0.4852432010229677, 0.48349783802405, 0.39194428897462785, 0.38774389401078224, 0.38950822385959327, 0.3942769749555737, 0.3878774519544095, 0.3933185050264001, 0.3981288420036435, 0.39727973588742316, 0.3970378169324249, 0.3948308229446411, 0.388742578914389, 0.38538955710828304, 0.3846820299513638, 0.39339937595650554, 0.39598918915726244, 0.4065545490011573, 0.4934380538761616, 0.4902941449545324, 0.5092109639663249, 0.47928488184697926, 0.4903763961046934, 0.47615545615553856, 0.39838811685331166, 0.5027259481139481, 0.38288986403495073, 0.38689173106104136, 0.49285208201035857, 0.5074475901201367, 0.38492341386154294, 0.480933859013021, 0.45336840697564185, 0.4839022180531174, 0.48431572620756924, 0.4099678809288889, 0.38217580784112215, 3.666195085970685, 0.37993584712967277, 0.39763258304446936, 0.3872853219509125, 0.38092547096312046, 0.3859282808844, 0.3849974009208381, 0.38504506601020694, 0.3795857548248023, 0.39390418608672917, 0.38431647093966603, 0.3882745981682092, 0.3975116810761392, 0.38622688106261194, 0.37890634615905583, 0.3788105109706521, 0.3907696311362088, 0.3938351417891681, 0.3859603509772569, 0.4023447809740901, 0.4425085550174117, 0.39381645806133747, 0.37678954703733325, 0.38936703302897513, 0.3894375520758331, 0.38302729604765773, 0.373388267820701, 0.3864289380144328, 0.38789789704605937, 0.4186170280445367, 0.3961745931301266, 0.48205641913227737, 0.5134763810783625, 0.4818108689505607, 0.4498666359577328, 0.4215683280490339, 0.47833468206226826, 0.4901617751456797, 0.3961050510406494, 0.3882514392025769, 0.4078892439138144, 0.403796074911952, 0.4152284348383546, 0.39409011509269476, 0.49658835283480585, 0.4896594111341983]
[0.013002433652552416, 0.009813330963025895, 0.07580347411923719, 0.008360294121488625, 0.007954265575437826, 0.0078580222064059, 0.007571933634237063, 0.007721228572558992, 0.007490654656549497, 0.007691191286038683, 0.008592786942133489, 0.007667874755832005, 0.008392266672560754, 0.009816856304069563, 0.00787651246622661, 0.012717523835409356, 0.008105906306252795, 0.00824406083521186, 0.007648672428628316, 0.009743733816228959, 0.010390958473162383, 0.009712574832445505, 0.009992857226075567, 0.00980162803957961, 0.00972429883419251, 0.009797248756513, 0.01003637842415851, 0.0100352209184927, 0.010142791983006256, 0.00788198973584388, 0.009417207246380193, 0.007829440287219323, 0.007946479001215525, 0.008131715777919305, 0.008154907976562271, 0.00994070126599043, 0.01003748204140943, 0.009794488444696275, 0.009861186222762478, 0.008444321774212378, 0.007980512897008839, 0.00969203201844832, 0.00789098711969445, 0.009902922469856483, 0.009867302816817346, 0.007998863040298527, 0.007913140694097596, 0.007949147425705985, 0.00804646887664436, 0.00791586636641652, 0.0080269082658449, 0.008125078408237623, 0.008107749711988228, 0.00810281259045765, 0.008057771896829411, 0.007933522018661, 0.007865093002209857, 0.007850653672476813, 0.008028558692989908, 0.0080814120236176, 0.008297031612268515, 0.010070164364819624, 0.010006002958255763, 0.010392060489108672, 0.009781324119326107, 0.010007681553157009, 0.009717458288888542, 0.008130369731700239, 0.010259713226815268, 0.007814078857856137, 0.00789574961349064, 0.010058205755313441, 0.010356073267757893, 0.007855579874725366, 0.009814976714551449, 0.00925241646889065, 0.009875555470471784, 0.009883994412399372, 0.008366691447528345, 0.00779950628247188, 0.07482030787695275, 0.007753792798564751, 0.008114950674376925, 0.007903782080630866, 0.007773989203328989, 0.007876087364987756, 0.007857089814710982, 0.007858062571636876, 0.007746648057649026, 0.008038860940545494, 0.00784319328448298, 0.007923971391187943, 0.008112483287268147, 0.007882181246175754, 0.007732782574674609, 0.007730826754503104, 0.0079748904313512, 0.008037451873248329, 0.007876741856678712, 0.008211117979063064, 0.009030786837090035, 0.008037070572680357, 0.007689582592598638, 0.007946265980183167, 0.007947705144404757, 0.007816883592809342, 0.007620168731034715, 0.007886304857437404, 0.007916283613184884, 0.008543204653970137, 0.00808519577816585, 0.009837886104740354, 0.010479109817925764, 0.009832874876542054, 0.009180951754239445, 0.008603435266306815, 0.009761932286985067, 0.0100033015335853, 0.008083776551849988, 0.007923498759236263, 0.008324270283955396, 0.008240736222692899, 0.008474049690578665, 0.008042655410054994, 0.01013445618030216, 0.009993049206820374]
[76.90867930740774, 101.90219852644759, 13.192007511780028, 119.6130166556797, 125.71870910218598, 127.25848486210626, 132.06666200538595, 129.5130678495867, 133.49968004807752, 130.01887000460314, 116.37667810622004, 130.41423234507366, 119.15731935325493, 101.86560432645342, 126.95974319698738, 78.63165919262549, 123.36683428336846, 121.29944453209525, 130.74164298853836, 102.63006141797726, 96.23751288995962, 102.95930968371366, 100.0714787949316, 102.02386745976638, 102.83517784169767, 102.06947122121622, 99.63753435132594, 99.64902697430612, 98.59218267272466, 126.87151766417972, 106.18859432921393, 127.72305085874237, 125.84189800879558, 122.97527696619447, 122.62554070187721, 100.59652465578506, 99.62657924313291, 102.09823674267551, 101.40767828638202, 118.42277292817545, 125.305229489048, 103.17753780595727, 126.72685746808334, 100.98029173144606, 101.34481717695428, 125.01776752045485, 126.3720738272604, 125.79965453479903, 124.2781169392943, 126.32856009830493, 124.58096777498697, 123.07573536596871, 123.33878517751806, 123.41393668386935, 124.10378610909576, 126.04742227321346, 127.14407823518802, 127.37792822346076, 124.55535772232, 123.74075187325437, 120.52503193085786, 99.3032450883846, 99.9400064313312, 96.22730747651471, 102.23564701472094, 99.92324342939764, 102.90756803591873, 122.99563648391155, 97.4686112460096, 127.97413721959276, 126.65041939670995, 99.42131075134657, 96.56169613181017, 127.29805004178134, 101.88511181258576, 108.07987333496007, 101.26012688501734, 101.17367111676126, 119.52155834495558, 128.2132437340729, 13.365355320972084, 128.9691414226471, 123.22933806086024, 126.52170692441227, 128.63408654745476, 126.96659567863423, 127.27358647825058, 127.2578311617713, 129.08808978518158, 124.3957331014788, 127.49908917563027, 126.19934507992745, 123.26681788909383, 126.8684351156193, 129.31955481007165, 129.35227133598787, 125.39357231401715, 124.41754125189566, 126.95604581126359, 121.78609569973635, 110.73232244757835, 124.42344395969398, 130.046070506157, 125.84527153934374, 125.82248357615624, 127.92821949144646, 131.23068993568253, 126.8020978236622, 126.32190164769492, 117.05209467682448, 123.68284299317895, 101.64785293846339, 95.42795307759644, 101.69965676932033, 108.9211692609358, 116.2326406890336, 102.43873554964455, 99.96699556067348, 123.70455734221748, 126.20687279521817, 120.1306500015321, 121.34838113690056, 118.00733256400261, 124.33704405012698, 98.67327681022002, 100.06955627892718]
Elapsed: 0.47857560616888345~0.4112071571199699
Time per graph: 0.009766849105487415~0.008391982798366731
Speed: 114.74066131916592~18.10363792375218
Total Time: 0.4907
best val loss: 0.15045033395290375 test_score: 0.8776

Testing...
Test loss: 0.5733 score: 0.8163 time: 0.65s
test Score 0.8163
Epoch Time List: [2.7088044830597937, 1.738426759140566, 7.474583562929183, 1.5353863039053977, 1.4546205820515752, 1.4298343886621296, 1.4624888447578996, 1.4687181131448597, 1.4454236452002078, 1.3796417000703514, 1.4999911459162831, 1.3589029130525887, 1.5452265101484954, 1.7252872539684176, 1.7501556316856295, 1.6144894750323147, 1.662863583303988, 1.534979219082743, 1.4679671998601407, 1.5259636691771448, 1.9096654658205807, 1.6105603110045195, 1.8833848431240767, 1.7332990190479904, 1.8836184528190643, 1.7255799788981676, 1.8868012658786029, 2.3925593679305166, 1.9037973310332745, 1.3994987569749355, 1.7374480001162738, 1.4134088295977563, 1.5611583220306784, 1.443603910971433, 1.5870439251884818, 1.6611745317932218, 1.896301138913259, 1.4806042881682515, 1.9136209392454475, 1.5742368979845196, 1.5895431661047041, 1.6771789949852973, 1.6792539421003312, 1.6134485118091106, 1.8920924609992653, 1.503411561017856, 1.5691396549809724, 1.3924752529710531, 1.5682836980558932, 1.4408432990312576, 1.5393485859967768, 1.8936714618466794, 1.5690535609610379, 1.4288713107816875, 1.5503902130294591, 1.407643283950165, 1.537811562186107, 1.3856749790720642, 1.5463570600841194, 1.3784321460407227, 1.5295783670153469, 1.6596226710826159, 1.9918200219981372, 1.7688441299833357, 1.9405640591867268, 1.7801649570465088, 1.8388492208905518, 1.4632135997526348, 2.1014072860125452, 1.5883632390759885, 1.5533731789328158, 1.7329174829646945, 1.8928957870230079, 1.5571074231993407, 1.8734960029833019, 1.6907654968090355, 1.8866758819203824, 1.7662545728962868, 1.7354071189183742, 1.369154016021639, 8.016880513634533, 1.3835173768457025, 1.5556483820546418, 1.4264417900703847, 1.52826969884336, 1.4193599591962993, 1.489703335100785, 1.379754519322887, 1.480973294004798, 1.3895606866572052, 1.4888803260400891, 1.3649536620359868, 1.5319862517062575, 1.3899507329333574, 1.4884703299030662, 1.438352993922308, 1.5109781629871577, 1.4062876971438527, 1.5571328320074826, 1.454477691091597, 1.5536109588574618, 1.393193638883531, 1.5410100501030684, 1.3987079970538616, 1.5207197337877005, 1.402119278209284, 1.5313753422815353, 1.4011815688572824, 1.5470361560583115, 1.4261475170496851, 1.5244699383620173, 1.5219927160069346, 1.893162488238886, 1.7234807992354035, 1.8577102990821004, 1.437492361990735, 1.6839783329050988, 1.765696751885116, 1.737082432024181, 1.4160718810744584, 1.664219204802066, 1.4466578271239996, 1.7679729531519115, 1.418462417786941, 2.1824116942007095, 1.7231047337409109]
Total Epoch List: [126]
Total Time List: [0.49073306820355356]
========================k_idx:1========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: FAGNN
FAGNN(
  (conv1): NeighborFilterConv()
  (conv2): NeighborFilterConv()
  (ffn): Sequential(
    (0): Linear(in_features=14951, out_features=64, bias=True)
    (1): ReLU()
    (2): Linear(in_features=64, out_features=64, bias=True)
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=64, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=64, out_features=2, bias=True)
  )
)
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7f904e47e6e0>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.6932;  Loss pred: 0.6932; Loss self: 0.0000; time: 0.63s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6934 score: 0.4898 time: 0.47s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6930 score: 0.5102 time: 0.62s
Epoch 2/1000, LR 0.000000
Train loss: 0.6932;  Loss pred: 0.6932; Loss self: 0.0000; time: 0.63s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6934 score: 0.4898 time: 0.49s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6930 score: 0.5102 time: 0.82s
Epoch 3/1000, LR 0.000030
Train loss: 0.6932;  Loss pred: 0.6932; Loss self: 0.0000; time: 0.63s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6934 score: 0.4898 time: 0.51s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6930 score: 0.5102 time: 0.60s
Epoch 4/1000, LR 0.000060
Train loss: 0.6932;  Loss pred: 0.6932; Loss self: 0.0000; time: 0.63s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6933 score: 0.4898 time: 0.50s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6929 score: 0.5102 time: 0.85s
Epoch 5/1000, LR 0.000090
Train loss: 0.6931;  Loss pred: 0.6931; Loss self: 0.0000; time: 0.60s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6932 score: 0.4898 time: 0.41s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6929 score: 0.5102 time: 0.54s
Epoch 6/1000, LR 0.000120
Train loss: 0.6930;  Loss pred: 0.6930; Loss self: 0.0000; time: 0.70s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6931 score: 0.4898 time: 0.51s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6928 score: 0.5102 time: 0.84s
Epoch 7/1000, LR 0.000150
Train loss: 0.6930;  Loss pred: 0.6930; Loss self: 0.0000; time: 0.64s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6930 score: 0.4898 time: 0.48s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6928 score: 0.5102 time: 0.64s
Epoch 8/1000, LR 0.000180
Train loss: 0.6929;  Loss pred: 0.6929; Loss self: 0.0000; time: 0.67s
Val loss: 0.6929 score: 0.6122 time: 0.48s
Test loss: 0.6927 score: 0.7347 time: 0.65s
Epoch 9/1000, LR 0.000210
Train loss: 0.6928;  Loss pred: 0.6928; Loss self: 0.0000; time: 0.80s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6928 score: 0.5102 time: 0.43s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6926 score: 0.4898 time: 0.56s
Epoch 10/1000, LR 0.000240
Train loss: 0.6926;  Loss pred: 0.6926; Loss self: 0.0000; time: 0.56s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6926 score: 0.5102 time: 0.40s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6924 score: 0.4898 time: 0.55s
Epoch 11/1000, LR 0.000270
Train loss: 0.6925;  Loss pred: 0.6925; Loss self: 0.0000; time: 0.69s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6925 score: 0.5102 time: 0.50s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6922 score: 0.4898 time: 0.63s
Epoch 12/1000, LR 0.000270
Train loss: 0.6923;  Loss pred: 0.6923; Loss self: 0.0000; time: 0.64s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6923 score: 0.5102 time: 0.49s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6920 score: 0.4898 time: 0.64s
Epoch 13/1000, LR 0.000270
Train loss: 0.6921;  Loss pred: 0.6921; Loss self: 0.0000; time: 0.70s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6921 score: 0.5102 time: 0.39s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6918 score: 0.4898 time: 0.55s
Epoch 14/1000, LR 0.000270
Train loss: 0.6918;  Loss pred: 0.6918; Loss self: 0.0000; time: 0.54s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6920 score: 0.5102 time: 0.51s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6915 score: 0.4898 time: 0.52s
Epoch 15/1000, LR 0.000270
Train loss: 0.6915;  Loss pred: 0.6915; Loss self: 0.0000; time: 0.86s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6917 score: 0.5102 time: 0.53s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6911 score: 0.4898 time: 0.55s
Epoch 16/1000, LR 0.000270
Train loss: 0.6912;  Loss pred: 0.6912; Loss self: 0.0000; time: 0.53s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6915 score: 0.5102 time: 0.42s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6907 score: 0.4898 time: 0.50s
Epoch 17/1000, LR 0.000270
Train loss: 0.6908;  Loss pred: 0.6908; Loss self: 0.0000; time: 0.67s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6912 score: 0.5102 time: 0.40s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6902 score: 0.4898 time: 0.52s
Epoch 18/1000, LR 0.000270
Train loss: 0.6904;  Loss pred: 0.6904; Loss self: 0.0000; time: 0.63s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6909 score: 0.5102 time: 0.51s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6897 score: 0.4898 time: 0.66s
Epoch 19/1000, LR 0.000270
Train loss: 0.6900;  Loss pred: 0.6900; Loss self: 0.0000; time: 0.64s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6906 score: 0.5102 time: 0.66s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6891 score: 0.4898 time: 0.63s
Epoch 20/1000, LR 0.000270
Train loss: 0.6894;  Loss pred: 0.6894; Loss self: 0.0000; time: 0.54s
Val loss: 0.6902 score: 0.5306 time: 0.44s
Test loss: 0.6884 score: 0.5306 time: 0.55s
Epoch 21/1000, LR 0.000270
Train loss: 0.6889;  Loss pred: 0.6889; Loss self: 0.0000; time: 0.59s
Val loss: 0.6897 score: 0.5510 time: 0.49s
Test loss: 0.6877 score: 0.5714 time: 0.51s
Epoch 22/1000, LR 0.000270
Train loss: 0.6881;  Loss pred: 0.6881; Loss self: 0.0000; time: 0.57s
Val loss: 0.6892 score: 0.5306 time: 0.42s
Test loss: 0.6868 score: 0.5714 time: 0.54s
Epoch 23/1000, LR 0.000270
Train loss: 0.6871;  Loss pred: 0.6871; Loss self: 0.0000; time: 0.65s
Val loss: 0.6886 score: 0.5510 time: 0.40s
Test loss: 0.6859 score: 0.5918 time: 0.50s
Epoch 24/1000, LR 0.000270
Train loss: 0.6863;  Loss pred: 0.6863; Loss self: 0.0000; time: 0.51s
Val loss: 0.6879 score: 0.5510 time: 0.41s
Test loss: 0.6848 score: 0.5918 time: 0.53s
Epoch 25/1000, LR 0.000270
Train loss: 0.6852;  Loss pred: 0.6852; Loss self: 0.0000; time: 0.75s
Val loss: 0.6872 score: 0.5510 time: 0.51s
Test loss: 0.6836 score: 0.5918 time: 0.66s
Epoch 26/1000, LR 0.000270
Train loss: 0.6841;  Loss pred: 0.6841; Loss self: 0.0000; time: 0.71s
Val loss: 0.6864 score: 0.5918 time: 0.51s
Test loss: 0.6823 score: 0.5918 time: 0.51s
Epoch 27/1000, LR 0.000270
Train loss: 0.6829;  Loss pred: 0.6829; Loss self: 0.0000; time: 0.69s
Val loss: 0.6854 score: 0.5918 time: 0.59s
Test loss: 0.6808 score: 0.6122 time: 0.62s
Epoch 28/1000, LR 0.000270
Train loss: 0.6814;  Loss pred: 0.6814; Loss self: 0.0000; time: 0.63s
Val loss: 0.6843 score: 0.5918 time: 0.47s
Test loss: 0.6791 score: 0.6122 time: 0.55s
Epoch 29/1000, LR 0.000270
Train loss: 0.6798;  Loss pred: 0.6798; Loss self: 0.0000; time: 0.66s
Val loss: 0.6832 score: 0.5918 time: 0.41s
Test loss: 0.6773 score: 0.6122 time: 0.53s
Epoch 30/1000, LR 0.000270
Train loss: 0.6784;  Loss pred: 0.6784; Loss self: 0.0000; time: 0.56s
Val loss: 0.6820 score: 0.5918 time: 0.44s
Test loss: 0.6754 score: 0.6122 time: 0.54s
Epoch 31/1000, LR 0.000270
Train loss: 0.6764;  Loss pred: 0.6764; Loss self: 0.0000; time: 0.51s
Val loss: 0.6806 score: 0.6122 time: 0.55s
Test loss: 0.6733 score: 0.6122 time: 0.50s
Epoch 32/1000, LR 0.000270
Train loss: 0.6746;  Loss pred: 0.6746; Loss self: 0.0000; time: 0.51s
Val loss: 0.6791 score: 0.6327 time: 0.37s
Test loss: 0.6710 score: 0.6122 time: 0.49s
Epoch 33/1000, LR 0.000270
Train loss: 0.6720;  Loss pred: 0.6720; Loss self: 0.0000; time: 0.50s
Val loss: 0.6775 score: 0.6327 time: 0.49s
Test loss: 0.6685 score: 0.6122 time: 0.49s
Epoch 34/1000, LR 0.000270
Train loss: 0.6702;  Loss pred: 0.6702; Loss self: 0.0000; time: 0.49s
Val loss: 0.6757 score: 0.6531 time: 0.37s
Test loss: 0.6658 score: 0.6122 time: 0.48s
Epoch 35/1000, LR 0.000270
Train loss: 0.6658;  Loss pred: 0.6658; Loss self: 0.0000; time: 0.53s
Val loss: 0.6737 score: 0.6531 time: 3.58s
Test loss: 0.6629 score: 0.6531 time: 1.60s
Epoch 36/1000, LR 0.000270
Train loss: 0.6638;  Loss pred: 0.6638; Loss self: 0.0000; time: 0.51s
Val loss: 0.6716 score: 0.6327 time: 0.37s
Test loss: 0.6598 score: 0.6531 time: 0.51s
Epoch 37/1000, LR 0.000270
Train loss: 0.6616;  Loss pred: 0.6616; Loss self: 0.0000; time: 0.49s
Val loss: 0.6693 score: 0.6327 time: 0.48s
Test loss: 0.6565 score: 0.6531 time: 0.48s
Epoch 38/1000, LR 0.000270
Train loss: 0.6570;  Loss pred: 0.6570; Loss self: 0.0000; time: 0.50s
Val loss: 0.6669 score: 0.6327 time: 0.37s
Test loss: 0.6530 score: 0.6531 time: 0.48s
Epoch 39/1000, LR 0.000269
Train loss: 0.6525;  Loss pred: 0.6525; Loss self: 0.0000; time: 0.52s
Val loss: 0.6642 score: 0.6327 time: 0.46s
Test loss: 0.6492 score: 0.6735 time: 0.49s
Epoch 40/1000, LR 0.000269
Train loss: 0.6493;  Loss pred: 0.6493; Loss self: 0.0000; time: 0.50s
Val loss: 0.6613 score: 0.6531 time: 0.37s
Test loss: 0.6451 score: 0.6939 time: 0.53s
Epoch 41/1000, LR 0.000269
Train loss: 0.6450;  Loss pred: 0.6450; Loss self: 0.0000; time: 0.60s
Val loss: 0.6582 score: 0.6531 time: 0.38s
Test loss: 0.6407 score: 0.7143 time: 0.49s
Epoch 42/1000, LR 0.000269
Train loss: 0.6384;  Loss pred: 0.6384; Loss self: 0.0000; time: 0.53s
Val loss: 0.6549 score: 0.6531 time: 0.37s
Test loss: 0.6359 score: 0.7347 time: 0.49s
Epoch 43/1000, LR 0.000269
Train loss: 0.6327;  Loss pred: 0.6327; Loss self: 0.0000; time: 0.49s
Val loss: 0.6512 score: 0.6531 time: 0.49s
Test loss: 0.6307 score: 0.7551 time: 0.51s
Epoch 44/1000, LR 0.000269
Train loss: 0.6294;  Loss pred: 0.6294; Loss self: 0.0000; time: 0.54s
Val loss: 0.6471 score: 0.6531 time: 0.37s
Test loss: 0.6251 score: 0.7755 time: 0.52s
Epoch 45/1000, LR 0.000269
Train loss: 0.6246;  Loss pred: 0.6246; Loss self: 0.0000; time: 0.52s
Val loss: 0.6427 score: 0.6735 time: 0.57s
Test loss: 0.6190 score: 0.7755 time: 0.57s
Epoch 46/1000, LR 0.000269
Train loss: 0.6144;  Loss pred: 0.6144; Loss self: 0.0000; time: 0.55s
Val loss: 0.6379 score: 0.6735 time: 0.38s
Test loss: 0.6124 score: 0.7755 time: 0.53s
Epoch 47/1000, LR 0.000269
Train loss: 0.6118;  Loss pred: 0.6118; Loss self: 0.0000; time: 0.51s
Val loss: 0.6327 score: 0.6735 time: 0.46s
Test loss: 0.6052 score: 0.7959 time: 0.61s
Epoch 48/1000, LR 0.000269
Train loss: 0.6015;  Loss pred: 0.6015; Loss self: 0.0000; time: 0.50s
Val loss: 0.6270 score: 0.6939 time: 0.38s
Test loss: 0.5976 score: 0.7959 time: 0.48s
Epoch 49/1000, LR 0.000269
Train loss: 0.5942;  Loss pred: 0.5942; Loss self: 0.0000; time: 0.54s
Val loss: 0.6210 score: 0.7347 time: 0.56s
Test loss: 0.5895 score: 0.8163 time: 0.52s
Epoch 50/1000, LR 0.000269
Train loss: 0.5873;  Loss pred: 0.5873; Loss self: 0.0000; time: 0.52s
Val loss: 0.6145 score: 0.7551 time: 0.40s
Test loss: 0.5808 score: 0.8571 time: 0.55s
Epoch 51/1000, LR 0.000269
Train loss: 0.5731;  Loss pred: 0.5731; Loss self: 0.0000; time: 0.49s
Val loss: 0.6077 score: 0.7755 time: 0.40s
Test loss: 0.5717 score: 0.8776 time: 0.70s
Epoch 52/1000, LR 0.000269
Train loss: 0.5684;  Loss pred: 0.5684; Loss self: 0.0000; time: 0.50s
Val loss: 0.6003 score: 0.7755 time: 0.48s
Test loss: 0.5621 score: 0.9184 time: 0.49s
Epoch 53/1000, LR 0.000269
Train loss: 0.5553;  Loss pred: 0.5553; Loss self: 0.0000; time: 0.50s
Val loss: 0.5926 score: 0.7755 time: 0.38s
Test loss: 0.5519 score: 0.9388 time: 0.66s
Epoch 54/1000, LR 0.000269
Train loss: 0.5423;  Loss pred: 0.5423; Loss self: 0.0000; time: 0.53s
Val loss: 0.5843 score: 0.8163 time: 0.37s
Test loss: 0.5412 score: 0.9388 time: 0.55s
Epoch 55/1000, LR 0.000269
Train loss: 0.5319;  Loss pred: 0.5319; Loss self: 0.0000; time: 0.52s
Val loss: 0.5755 score: 0.8367 time: 0.50s
Test loss: 0.5299 score: 0.9388 time: 0.53s
Epoch 56/1000, LR 0.000269
Train loss: 0.5174;  Loss pred: 0.5174; Loss self: 0.0000; time: 0.50s
Val loss: 0.5662 score: 0.8571 time: 0.36s
Test loss: 0.5181 score: 0.9388 time: 0.48s
Epoch 57/1000, LR 0.000269
Train loss: 0.5070;  Loss pred: 0.5070; Loss self: 0.0000; time: 0.52s
Val loss: 0.5565 score: 0.8571 time: 0.50s
Test loss: 0.5057 score: 0.9388 time: 0.79s
Epoch 58/1000, LR 0.000269
Train loss: 0.4885;  Loss pred: 0.4885; Loss self: 0.0000; time: 0.63s
Val loss: 0.5464 score: 0.8776 time: 0.47s
Test loss: 0.4928 score: 0.9388 time: 0.65s
Epoch 59/1000, LR 0.000268
Train loss: 0.4750;  Loss pred: 0.4750; Loss self: 0.0000; time: 0.62s
Val loss: 0.5360 score: 0.8776 time: 0.39s
Test loss: 0.4795 score: 0.9388 time: 0.65s
Epoch 60/1000, LR 0.000268
Train loss: 0.4622;  Loss pred: 0.4622; Loss self: 0.0000; time: 0.52s
Val loss: 0.5254 score: 0.8571 time: 0.50s
Test loss: 0.4658 score: 0.9592 time: 0.63s
Epoch 61/1000, LR 0.000268
Train loss: 0.4498;  Loss pred: 0.4498; Loss self: 0.0000; time: 0.63s
Val loss: 0.5144 score: 0.8571 time: 0.47s
Test loss: 0.4517 score: 0.9592 time: 0.79s
Epoch 62/1000, LR 0.000268
Train loss: 0.4310;  Loss pred: 0.4310; Loss self: 0.0000; time: 0.63s
Val loss: 0.5032 score: 0.8571 time: 0.44s
Test loss: 0.4374 score: 0.9592 time: 0.51s
Epoch 63/1000, LR 0.000268
Train loss: 0.4238;  Loss pred: 0.4238; Loss self: 0.0000; time: 0.50s
Val loss: 0.4919 score: 0.8571 time: 0.45s
Test loss: 0.4230 score: 0.9592 time: 0.61s
Epoch 64/1000, LR 0.000268
Train loss: 0.4034;  Loss pred: 0.4034; Loss self: 0.0000; time: 0.51s
Val loss: 0.4807 score: 0.8571 time: 0.38s
Test loss: 0.4086 score: 0.9592 time: 0.50s
Epoch 65/1000, LR 0.000268
Train loss: 0.3820;  Loss pred: 0.3820; Loss self: 0.0000; time: 0.53s
Val loss: 0.4693 score: 0.8571 time: 0.39s
Test loss: 0.3942 score: 0.9592 time: 0.69s
Epoch 66/1000, LR 0.000268
Train loss: 0.3623;  Loss pred: 0.3623; Loss self: 0.0000; time: 0.63s
Val loss: 0.4579 score: 0.8571 time: 0.39s
Test loss: 0.3798 score: 0.9796 time: 0.51s
Epoch 67/1000, LR 0.000268
Train loss: 0.3508;  Loss pred: 0.3508; Loss self: 0.0000; time: 0.58s
Val loss: 0.4465 score: 0.8571 time: 0.39s
Test loss: 0.3655 score: 0.9796 time: 0.64s
Epoch 68/1000, LR 0.000268
Train loss: 0.3371;  Loss pred: 0.3371; Loss self: 0.0000; time: 0.52s
Val loss: 0.4355 score: 0.8571 time: 0.39s
Test loss: 0.3516 score: 0.9796 time: 0.52s
Epoch 69/1000, LR 0.000268
Train loss: 0.3228;  Loss pred: 0.3228; Loss self: 0.0000; time: 0.51s
Val loss: 0.4245 score: 0.8571 time: 0.37s
Test loss: 0.3378 score: 0.9796 time: 0.65s
Epoch 70/1000, LR 0.000268
Train loss: 0.3080;  Loss pred: 0.3080; Loss self: 0.0000; time: 0.51s
Val loss: 0.4141 score: 0.8571 time: 0.39s
Test loss: 0.3245 score: 0.9796 time: 0.50s
Epoch 71/1000, LR 0.000268
Train loss: 0.2905;  Loss pred: 0.2905; Loss self: 0.0000; time: 0.51s
Val loss: 0.4040 score: 0.8571 time: 0.39s
Test loss: 0.3117 score: 0.9592 time: 0.50s
Epoch 72/1000, LR 0.000267
Train loss: 0.2762;  Loss pred: 0.2762; Loss self: 0.0000; time: 0.73s
Val loss: 0.3942 score: 0.8571 time: 0.47s
Test loss: 0.2991 score: 0.9592 time: 0.63s
Epoch 73/1000, LR 0.000267
Train loss: 0.2582;  Loss pred: 0.2582; Loss self: 0.0000; time: 0.62s
Val loss: 0.3849 score: 0.8571 time: 0.47s
Test loss: 0.2871 score: 0.9592 time: 0.56s
Epoch 74/1000, LR 0.000267
Train loss: 0.2415;  Loss pred: 0.2415; Loss self: 0.0000; time: 0.66s
Val loss: 0.3764 score: 0.8571 time: 0.47s
Test loss: 0.2760 score: 0.9592 time: 0.51s
Epoch 75/1000, LR 0.000267
Train loss: 0.2321;  Loss pred: 0.2321; Loss self: 0.0000; time: 0.50s
Val loss: 0.3682 score: 0.8571 time: 0.38s
Test loss: 0.2652 score: 0.9592 time: 0.52s
Epoch 76/1000, LR 0.000267
Train loss: 0.2154;  Loss pred: 0.2154; Loss self: 0.0000; time: 0.63s
Val loss: 0.3606 score: 0.8571 time: 0.46s
Test loss: 0.2552 score: 0.9592 time: 0.50s
Epoch 77/1000, LR 0.000267
Train loss: 0.1917;  Loss pred: 0.1917; Loss self: 0.0000; time: 0.52s
Val loss: 0.3534 score: 0.8776 time: 0.37s
Test loss: 0.2457 score: 0.9592 time: 0.54s
Epoch 78/1000, LR 0.000267
Train loss: 0.1907;  Loss pred: 0.1907; Loss self: 0.0000; time: 0.68s
Val loss: 0.3468 score: 0.8776 time: 0.44s
Test loss: 0.2365 score: 0.9592 time: 0.65s
Epoch 79/1000, LR 0.000267
Train loss: 0.1857;  Loss pred: 0.1857; Loss self: 0.0000; time: 0.60s
Val loss: 0.3402 score: 0.8776 time: 0.46s
Test loss: 0.2268 score: 0.9592 time: 0.62s
Epoch 80/1000, LR 0.000267
Train loss: 0.1621;  Loss pred: 0.1621; Loss self: 0.0000; time: 0.73s
Val loss: 0.3343 score: 0.8571 time: 0.47s
Test loss: 0.2177 score: 0.9592 time: 0.64s
Epoch 81/1000, LR 0.000267
Train loss: 0.1497;  Loss pred: 0.1497; Loss self: 0.0000; time: 0.73s
Val loss: 0.3291 score: 0.8571 time: 0.38s
Test loss: 0.2092 score: 0.9592 time: 0.50s
Epoch 82/1000, LR 0.000267
Train loss: 0.1399;  Loss pred: 0.1399; Loss self: 0.0000; time: 0.61s
Val loss: 0.3253 score: 0.8571 time: 0.37s
Test loss: 0.2023 score: 0.9388 time: 0.55s
Epoch 83/1000, LR 0.000266
Train loss: 0.1331;  Loss pred: 0.1331; Loss self: 0.0000; time: 0.49s
Val loss: 0.3228 score: 0.8776 time: 0.37s
Test loss: 0.1974 score: 0.9388 time: 0.49s
Epoch 84/1000, LR 0.000266
Train loss: 0.1304;  Loss pred: 0.1304; Loss self: 0.0000; time: 0.49s
Val loss: 0.3222 score: 0.8776 time: 0.51s
Test loss: 0.1953 score: 0.9388 time: 0.49s
Epoch 85/1000, LR 0.000266
Train loss: 0.1059;  Loss pred: 0.1059; Loss self: 0.0000; time: 0.52s
Val loss: 0.3227 score: 0.8776 time: 0.40s
Test loss: 0.1947 score: 0.9388 time: 0.50s
     INFO: Early stopping counter 1 of 20
Epoch 86/1000, LR 0.000266
Train loss: 0.1015;  Loss pred: 0.1015; Loss self: 0.0000; time: 0.52s
Val loss: 0.3235 score: 0.8776 time: 0.59s
Test loss: 0.1937 score: 0.9592 time: 0.52s
     INFO: Early stopping counter 2 of 20
Epoch 87/1000, LR 0.000266
Train loss: 0.0873;  Loss pred: 0.0873; Loss self: 0.0000; time: 0.50s
Val loss: 0.3251 score: 0.8776 time: 0.38s
Test loss: 0.1936 score: 0.9592 time: 0.51s
     INFO: Early stopping counter 3 of 20
Epoch 88/1000, LR 0.000266
Train loss: 0.1036;  Loss pred: 0.1036; Loss self: 0.0000; time: 0.60s
Val loss: 0.3250 score: 0.8776 time: 0.49s
Test loss: 0.1898 score: 0.9592 time: 0.49s
     INFO: Early stopping counter 4 of 20
Epoch 89/1000, LR 0.000266
Train loss: 0.0748;  Loss pred: 0.0748; Loss self: 0.0000; time: 0.52s
Val loss: 0.3252 score: 0.8776 time: 0.41s
Test loss: 0.1860 score: 0.9592 time: 0.49s
     INFO: Early stopping counter 5 of 20
Epoch 90/1000, LR 0.000266
Train loss: 0.0783;  Loss pred: 0.0783; Loss self: 0.0000; time: 0.51s
Val loss: 0.3265 score: 0.8776 time: 0.48s
Test loss: 0.1837 score: 0.9592 time: 0.60s
     INFO: Early stopping counter 6 of 20
Epoch 91/1000, LR 0.000266
Train loss: 0.0754;  Loss pred: 0.0754; Loss self: 0.0000; time: 0.64s
Val loss: 0.3278 score: 0.8776 time: 0.50s
Test loss: 0.1805 score: 0.9592 time: 0.49s
     INFO: Early stopping counter 7 of 20
Epoch 92/1000, LR 0.000266
Train loss: 0.0633;  Loss pred: 0.0633; Loss self: 0.0000; time: 0.59s
Val loss: 0.3295 score: 0.8776 time: 0.50s
Test loss: 0.1775 score: 0.9388 time: 0.54s
     INFO: Early stopping counter 8 of 20
Epoch 93/1000, LR 0.000265
Train loss: 0.0547;  Loss pred: 0.0547; Loss self: 0.0000; time: 0.53s
Val loss: 0.3318 score: 0.8776 time: 0.38s
Test loss: 0.1749 score: 0.9388 time: 0.50s
     INFO: Early stopping counter 9 of 20
Epoch 94/1000, LR 0.000265
Train loss: 0.0576;  Loss pred: 0.0576; Loss self: 0.0000; time: 0.66s
Val loss: 0.3338 score: 0.8776 time: 0.40s
Test loss: 0.1706 score: 0.9388 time: 0.55s
     INFO: Early stopping counter 10 of 20
Epoch 95/1000, LR 0.000265
Train loss: 0.0483;  Loss pred: 0.0483; Loss self: 0.0000; time: 0.65s
Val loss: 0.3358 score: 0.8776 time: 0.48s
Test loss: 0.1656 score: 0.9388 time: 0.65s
     INFO: Early stopping counter 11 of 20
Epoch 96/1000, LR 0.000265
Train loss: 0.0416;  Loss pred: 0.0416; Loss self: 0.0000; time: 0.65s
Val loss: 0.3396 score: 0.8776 time: 0.48s
Test loss: 0.1644 score: 0.9388 time: 0.50s
     INFO: Early stopping counter 12 of 20
Epoch 97/1000, LR 0.000265
Train loss: 0.0411;  Loss pred: 0.0411; Loss self: 0.0000; time: 0.54s
Val loss: 0.3449 score: 0.8776 time: 0.38s
Test loss: 0.1672 score: 0.9388 time: 0.53s
     INFO: Early stopping counter 13 of 20
Epoch 98/1000, LR 0.000265
Train loss: 0.0349;  Loss pred: 0.0349; Loss self: 0.0000; time: 0.50s
Val loss: 0.3509 score: 0.8776 time: 0.49s
Test loss: 0.1714 score: 0.9388 time: 0.49s
     INFO: Early stopping counter 14 of 20
Epoch 99/1000, LR 0.000265
Train loss: 0.0298;  Loss pred: 0.0298; Loss self: 0.0000; time: 0.51s
Val loss: 0.3574 score: 0.8776 time: 0.41s
Test loss: 0.1770 score: 0.9388 time: 0.48s
     INFO: Early stopping counter 15 of 20
Epoch 100/1000, LR 0.000265
Train loss: 0.0259;  Loss pred: 0.0259; Loss self: 0.0000; time: 0.51s
Val loss: 0.3650 score: 0.8776 time: 0.40s
Test loss: 0.1851 score: 0.9592 time: 0.62s
     INFO: Early stopping counter 16 of 20
Epoch 101/1000, LR 0.000265
Train loss: 0.0413;  Loss pred: 0.0413; Loss self: 0.0000; time: 0.51s
Val loss: 0.3749 score: 0.8776 time: 0.39s
Test loss: 0.1979 score: 0.9592 time: 0.50s
     INFO: Early stopping counter 17 of 20
Epoch 102/1000, LR 0.000264
Train loss: 0.0290;  Loss pred: 0.0290; Loss self: 0.0000; time: 0.60s
Val loss: 0.3839 score: 0.8776 time: 0.49s
Test loss: 0.2084 score: 0.9592 time: 0.49s
     INFO: Early stopping counter 18 of 20
Epoch 103/1000, LR 0.000264
Train loss: 0.0256;  Loss pred: 0.0256; Loss self: 0.0000; time: 0.56s
Val loss: 0.3946 score: 0.8776 time: 0.38s
Test loss: 0.2219 score: 0.9592 time: 0.50s
     INFO: Early stopping counter 19 of 20
Epoch 104/1000, LR 0.000264
Train loss: 0.0326;  Loss pred: 0.0326; Loss self: 0.0000; time: 0.51s
Val loss: 0.4018 score: 0.8776 time: 0.45s
Test loss: 0.2283 score: 0.9592 time: 0.67s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 083,   Train_Loss: 0.1304,   Val_Loss: 0.3222,   Val_Precision: 0.9524,   Val_Recall: 0.8000,   Val_accuracy: 0.8696,   Val_Score: 0.8776,   Val_Loss: 0.3222,   Test_Precision: 0.9565,   Test_Recall: 0.9167,   Test_accuracy: 0.9362,   Test_Score: 0.9388,   Test_loss: 0.1953


[0.6371192489750683, 0.4808532171882689, 3.714370231842622, 0.4096544119529426, 0.38975901319645345, 0.3850430881138891, 0.3710247480776161, 0.3783402000553906, 0.3670420781709254, 0.3768683730158955, 0.421046560164541, 0.37572586303576827, 0.411221066955477, 0.4810259588994086, 0.38594911084510386, 0.6231586679350585, 0.397189409006387, 0.4039589809253812, 0.37478494900278747, 0.477442956995219, 0.5091569651849568, 0.47591616678982973, 0.48965000407770276, 0.4802797739394009, 0.47649064287543297, 0.480065189069137, 0.491782542783767, 0.49172582500614226, 0.49699680716730654, 0.3862174970563501, 0.46144315507262945, 0.3836425740737468, 0.3893774710595608, 0.3984540731180459, 0.3995904908515513, 0.48709436203353107, 0.49183662002906203, 0.4799299337901175, 0.4831981249153614, 0.4137717669364065, 0.39104513195343316, 0.47490956890396774, 0.386658368865028, 0.4852432010229677, 0.48349783802405, 0.39194428897462785, 0.38774389401078224, 0.38950822385959327, 0.3942769749555737, 0.3878774519544095, 0.3933185050264001, 0.3981288420036435, 0.39727973588742316, 0.3970378169324249, 0.3948308229446411, 0.388742578914389, 0.38538955710828304, 0.3846820299513638, 0.39339937595650554, 0.39598918915726244, 0.4065545490011573, 0.4934380538761616, 0.4902941449545324, 0.5092109639663249, 0.47928488184697926, 0.4903763961046934, 0.47615545615553856, 0.39838811685331166, 0.5027259481139481, 0.38288986403495073, 0.38689173106104136, 0.49285208201035857, 0.5074475901201367, 0.38492341386154294, 0.480933859013021, 0.45336840697564185, 0.4839022180531174, 0.48431572620756924, 0.4099678809288889, 0.38217580784112215, 3.666195085970685, 0.37993584712967277, 0.39763258304446936, 0.3872853219509125, 0.38092547096312046, 0.3859282808844, 0.3849974009208381, 0.38504506601020694, 0.3795857548248023, 0.39390418608672917, 0.38431647093966603, 0.3882745981682092, 0.3975116810761392, 0.38622688106261194, 0.37890634615905583, 0.3788105109706521, 0.3907696311362088, 0.3938351417891681, 0.3859603509772569, 0.4023447809740901, 0.4425085550174117, 0.39381645806133747, 0.37678954703733325, 0.38936703302897513, 0.3894375520758331, 0.38302729604765773, 0.373388267820701, 0.3864289380144328, 0.38789789704605937, 0.4186170280445367, 0.3961745931301266, 0.48205641913227737, 0.5134763810783625, 0.4818108689505607, 0.4498666359577328, 0.4215683280490339, 0.47833468206226826, 0.4901617751456797, 0.3961050510406494, 0.3882514392025769, 0.4078892439138144, 0.403796074911952, 0.4152284348383546, 0.39409011509269476, 0.49658835283480585, 0.4896594111341983, 0.6317537249997258, 0.8276711041107774, 0.6036023320630193, 0.8585950520355254, 0.5430245050229132, 0.8507707100361586, 0.6500315980520099, 0.6551386131905019, 0.5700289011001587, 0.556545059196651, 0.6411825919058174, 0.6504921531304717, 0.5518445330671966, 0.5282963290810585, 0.5557929889764637, 0.5067791258916259, 0.5233393979724497, 0.6626467879395932, 0.641011840198189, 0.5547323119826615, 0.5147897580172867, 0.5486243460327387, 0.5065754160750657, 0.5354274909477681, 0.6709368419833481, 0.5141659309156239, 0.6252219888847321, 0.5530714879278094, 0.5364106488414109, 0.5419466679450125, 0.5038494791369885, 0.5000426268670708, 0.49477281607687473, 0.4882541170809418, 1.6021921711508185, 0.510606074007228, 0.4865745771676302, 0.4864989130292088, 0.4940014169551432, 0.5328630078583956, 0.49043655185960233, 0.4954397601541132, 0.5142481699585915, 0.5223727200645953, 0.5706010709982365, 0.5363614200614393, 0.6119856350123882, 0.48994006402790546, 0.5277655080426484, 0.5539650360587984, 0.7012706070672721, 0.4973736950196326, 0.6633224040269852, 0.5583489709533751, 0.5358506839256734, 0.48750436515547335, 0.7921932120807469, 0.6626256529707462, 0.6567739909514785, 0.6362896871287376, 0.7947081569582224, 0.5131718870252371, 0.6164134899154305, 0.5107136180158705, 0.6953773919958621, 0.5211696911137551, 0.6448548841290176, 0.523309743963182, 0.6542391888797283, 0.5014395818579942, 0.5060953239444643, 0.632935045985505, 0.5613882199395448, 0.5133487300481647, 0.5254671170841902, 0.5051137499976903, 0.5400211671367288, 0.6555883679538965, 0.6250994068104774, 0.6465018449816853, 0.5038698690477759, 0.5567754039075226, 0.49654958909377456, 0.4962187649216503, 0.5003732540644705, 0.5303392850328237, 0.5217939310241491, 0.49603612814098597, 0.49842006084509194, 0.6106066219508648, 0.4917803481221199, 0.5479277528356761, 0.505306676030159, 0.5544896759092808, 0.6611836128868163, 0.5007127940189093, 0.5386823229491711, 0.4900146001018584, 0.4847532350104302, 0.6223258988466114, 0.4999153472017497, 0.494204199872911, 0.505624552955851, 0.6728047681972384]
[0.013002433652552416, 0.009813330963025895, 0.07580347411923719, 0.008360294121488625, 0.007954265575437826, 0.0078580222064059, 0.007571933634237063, 0.007721228572558992, 0.007490654656549497, 0.007691191286038683, 0.008592786942133489, 0.007667874755832005, 0.008392266672560754, 0.009816856304069563, 0.00787651246622661, 0.012717523835409356, 0.008105906306252795, 0.00824406083521186, 0.007648672428628316, 0.009743733816228959, 0.010390958473162383, 0.009712574832445505, 0.009992857226075567, 0.00980162803957961, 0.00972429883419251, 0.009797248756513, 0.01003637842415851, 0.0100352209184927, 0.010142791983006256, 0.00788198973584388, 0.009417207246380193, 0.007829440287219323, 0.007946479001215525, 0.008131715777919305, 0.008154907976562271, 0.00994070126599043, 0.01003748204140943, 0.009794488444696275, 0.009861186222762478, 0.008444321774212378, 0.007980512897008839, 0.00969203201844832, 0.00789098711969445, 0.009902922469856483, 0.009867302816817346, 0.007998863040298527, 0.007913140694097596, 0.007949147425705985, 0.00804646887664436, 0.00791586636641652, 0.0080269082658449, 0.008125078408237623, 0.008107749711988228, 0.00810281259045765, 0.008057771896829411, 0.007933522018661, 0.007865093002209857, 0.007850653672476813, 0.008028558692989908, 0.0080814120236176, 0.008297031612268515, 0.010070164364819624, 0.010006002958255763, 0.010392060489108672, 0.009781324119326107, 0.010007681553157009, 0.009717458288888542, 0.008130369731700239, 0.010259713226815268, 0.007814078857856137, 0.00789574961349064, 0.010058205755313441, 0.010356073267757893, 0.007855579874725366, 0.009814976714551449, 0.00925241646889065, 0.009875555470471784, 0.009883994412399372, 0.008366691447528345, 0.00779950628247188, 0.07482030787695275, 0.007753792798564751, 0.008114950674376925, 0.007903782080630866, 0.007773989203328989, 0.007876087364987756, 0.007857089814710982, 0.007858062571636876, 0.007746648057649026, 0.008038860940545494, 0.00784319328448298, 0.007923971391187943, 0.008112483287268147, 0.007882181246175754, 0.007732782574674609, 0.007730826754503104, 0.0079748904313512, 0.008037451873248329, 0.007876741856678712, 0.008211117979063064, 0.009030786837090035, 0.008037070572680357, 0.007689582592598638, 0.007946265980183167, 0.007947705144404757, 0.007816883592809342, 0.007620168731034715, 0.007886304857437404, 0.007916283613184884, 0.008543204653970137, 0.00808519577816585, 0.009837886104740354, 0.010479109817925764, 0.009832874876542054, 0.009180951754239445, 0.008603435266306815, 0.009761932286985067, 0.0100033015335853, 0.008083776551849988, 0.007923498759236263, 0.008324270283955396, 0.008240736222692899, 0.008474049690578665, 0.008042655410054994, 0.01013445618030216, 0.009993049206820374, 0.01289293316325971, 0.016891247022668927, 0.012318414940061619, 0.01752234800072501, 0.011082132755569657, 0.01736266755175834, 0.013265950980653264, 0.013370175779397999, 0.011633242879595076, 0.011358062432584713, 0.01308535901848607, 0.013275350063887179, 0.011262133327901972, 0.010781557736348132, 0.011342714060744157, 0.010342431140645427, 0.010680395876988769, 0.013523403835501902, 0.01308187428975896, 0.011321067591482888, 0.010505913428924218, 0.011196415225157932, 0.010338273797450321, 0.010927091651995267, 0.013692588611905063, 0.010493182263584162, 0.012759632426219024, 0.011287173223016518, 0.010947156098804303, 0.011060136080510458, 0.010282642431367112, 0.010204951568715731, 0.010097404409732138, 0.00996436973634575, 0.032697799411241193, 0.01042053212259649, 0.00993009341158429, 0.009928549245494057, 0.010081661570513127, 0.010874755262416236, 0.010008909221624538, 0.010111015513349248, 0.010494860611399827, 0.010660667756420314, 0.011644919816290542, 0.010946151429825291, 0.012489502755354862, 0.00999877681689603, 0.0107707246539316, 0.011305408899159151, 0.014311645042189226, 0.010150483571829237, 0.013537191918918065, 0.011394876958232145, 0.010935728243381088, 0.009949068676642314, 0.01616720840981116, 0.013522972509607064, 0.013403550835744458, 0.012985503818953827, 0.016218533815473925, 0.010472895653576267, 0.012579867141131235, 0.01042272689828307, 0.014191375346854329, 0.010636116145178676, 0.013160303757735053, 0.010679790693126162, 0.013351820181218945, 0.010233460854244779, 0.010328475998866618, 0.012917041754806223, 0.011456902447745812, 0.010476504694860505, 0.010723818716003882, 0.010308443877503884, 0.011020840145647526, 0.013379354448038705, 0.012757130751234233, 0.013193915203707864, 0.010283058551995426, 0.01136276334505148, 0.01013366508354642, 0.010126913569829598, 0.010211699062540215, 0.010823250714955586, 0.010648855735186716, 0.010123186288591551, 0.010171837976430448, 0.012461359631650301, 0.010036333635145304, 0.011182199037462776, 0.010312381143472632, 0.01131611583488328, 0.013493543120139107, 0.010218628449365497, 0.010993516794881043, 0.010000297961262415, 0.009892923163478168, 0.012700528547890028, 0.010202354024525503, 0.010085799997406346, 0.010318868427670427, 0.013730709555045682]
[76.90867930740774, 101.90219852644759, 13.192007511780028, 119.6130166556797, 125.71870910218598, 127.25848486210626, 132.06666200538595, 129.5130678495867, 133.49968004807752, 130.01887000460314, 116.37667810622004, 130.41423234507366, 119.15731935325493, 101.86560432645342, 126.95974319698738, 78.63165919262549, 123.36683428336846, 121.29944453209525, 130.74164298853836, 102.63006141797726, 96.23751288995962, 102.95930968371366, 100.0714787949316, 102.02386745976638, 102.83517784169767, 102.06947122121622, 99.63753435132594, 99.64902697430612, 98.59218267272466, 126.87151766417972, 106.18859432921393, 127.72305085874237, 125.84189800879558, 122.97527696619447, 122.62554070187721, 100.59652465578506, 99.62657924313291, 102.09823674267551, 101.40767828638202, 118.42277292817545, 125.305229489048, 103.17753780595727, 126.72685746808334, 100.98029173144606, 101.34481717695428, 125.01776752045485, 126.3720738272604, 125.79965453479903, 124.2781169392943, 126.32856009830493, 124.58096777498697, 123.07573536596871, 123.33878517751806, 123.41393668386935, 124.10378610909576, 126.04742227321346, 127.14407823518802, 127.37792822346076, 124.55535772232, 123.74075187325437, 120.52503193085786, 99.3032450883846, 99.9400064313312, 96.22730747651471, 102.23564701472094, 99.92324342939764, 102.90756803591873, 122.99563648391155, 97.4686112460096, 127.97413721959276, 126.65041939670995, 99.42131075134657, 96.56169613181017, 127.29805004178134, 101.88511181258576, 108.07987333496007, 101.26012688501734, 101.17367111676126, 119.52155834495558, 128.2132437340729, 13.365355320972084, 128.9691414226471, 123.22933806086024, 126.52170692441227, 128.63408654745476, 126.96659567863423, 127.27358647825058, 127.2578311617713, 129.08808978518158, 124.3957331014788, 127.49908917563027, 126.19934507992745, 123.26681788909383, 126.8684351156193, 129.31955481007165, 129.35227133598787, 125.39357231401715, 124.41754125189566, 126.95604581126359, 121.78609569973635, 110.73232244757835, 124.42344395969398, 130.046070506157, 125.84527153934374, 125.82248357615624, 127.92821949144646, 131.23068993568253, 126.8020978236622, 126.32190164769492, 117.05209467682448, 123.68284299317895, 101.64785293846339, 95.42795307759644, 101.69965676932033, 108.9211692609358, 116.2326406890336, 102.43873554964455, 99.96699556067348, 123.70455734221748, 126.20687279521817, 120.1306500015321, 121.34838113690056, 118.00733256400261, 124.33704405012698, 98.67327681022002, 100.06955627892718, 77.5618695402568, 59.20226012075653, 81.17927548842562, 57.069977149102606, 90.23533845481323, 57.59483656638515, 75.38095093660269, 74.79333230165099, 85.96055376390521, 88.04318570490844, 76.42128875388676, 75.32758045456681, 88.79312390331029, 92.75097573597137, 88.16232117327954, 96.68906530786865, 93.62948822473237, 73.9458802061934, 76.44164573442217, 88.33089211059249, 95.1844888847897, 89.31430104101864, 96.7279470047142, 91.5156595961563, 73.03220949255328, 95.29997429573186, 78.37216360129297, 88.59614185426209, 91.34792552279623, 90.41480075115378, 97.25126655668863, 97.99164584627692, 99.03535199958658, 100.35757669172276, 30.58309788444691, 95.96438917275074, 100.70398721863138, 100.71964949499919, 99.18999889113546, 91.95609242408021, 99.91098708733126, 98.90203399250373, 95.2847338356996, 93.80275446608464, 85.87435686770984, 91.35630969577777, 80.06723883152601, 100.01223332739964, 92.84426369909785, 88.45323587317357, 69.87316951001125, 98.51747386452725, 73.87056385028507, 87.75873611145553, 91.44338426709311, 100.51192051249238, 61.85359739614319, 73.94823876847893, 74.6070957058043, 77.00894889733779, 61.65785461111848, 95.48457590700063, 79.49209548727202, 95.9441813797049, 70.46533373677984, 94.01928169553672, 75.98608804240126, 93.63479385823828, 74.89615546250606, 97.7186520027783, 96.81970506682048, 77.41710671701715, 87.28362701532417, 95.45168251493014, 93.25036411774028, 97.00785219215287, 90.73718398818545, 74.74202166357819, 78.38753239267784, 75.79251378839935, 97.2473311265888, 88.00676117535292, 98.68097985828004, 98.74676949739452, 97.9268967755151, 92.39368340772134, 93.90680321602329, 98.78312731703478, 98.31064968957803, 80.24806518384435, 99.63797900243102, 89.42784837309588, 96.97081460502109, 88.36954433758802, 74.10951972336312, 97.86049125429282, 90.96270271453392, 99.9970204761541, 101.0823579113313, 78.73688061321926, 98.01659475804247, 99.14929903995309, 96.90985082419144, 72.82944817899273]
Elapsed: 0.5238389840015494~0.3207475170227938
Time per graph: 0.0106905915102357~0.006545867694342731
Speed: 102.29911797100418~20.891105963530872
Total Time: 0.6736
best val loss: 0.32219836115837097 test_score: 0.9388

Testing...
Test loss: 0.4928 score: 0.9388 time: 0.61s
test Score 0.9388
Epoch Time List: [2.7088044830597937, 1.738426759140566, 7.474583562929183, 1.5353863039053977, 1.4546205820515752, 1.4298343886621296, 1.4624888447578996, 1.4687181131448597, 1.4454236452002078, 1.3796417000703514, 1.4999911459162831, 1.3589029130525887, 1.5452265101484954, 1.7252872539684176, 1.7501556316856295, 1.6144894750323147, 1.662863583303988, 1.534979219082743, 1.4679671998601407, 1.5259636691771448, 1.9096654658205807, 1.6105603110045195, 1.8833848431240767, 1.7332990190479904, 1.8836184528190643, 1.7255799788981676, 1.8868012658786029, 2.3925593679305166, 1.9037973310332745, 1.3994987569749355, 1.7374480001162738, 1.4134088295977563, 1.5611583220306784, 1.443603910971433, 1.5870439251884818, 1.6611745317932218, 1.896301138913259, 1.4806042881682515, 1.9136209392454475, 1.5742368979845196, 1.5895431661047041, 1.6771789949852973, 1.6792539421003312, 1.6134485118091106, 1.8920924609992653, 1.503411561017856, 1.5691396549809724, 1.3924752529710531, 1.5682836980558932, 1.4408432990312576, 1.5393485859967768, 1.8936714618466794, 1.5690535609610379, 1.4288713107816875, 1.5503902130294591, 1.407643283950165, 1.537811562186107, 1.3856749790720642, 1.5463570600841194, 1.3784321460407227, 1.5295783670153469, 1.6596226710826159, 1.9918200219981372, 1.7688441299833357, 1.9405640591867268, 1.7801649570465088, 1.8388492208905518, 1.4632135997526348, 2.1014072860125452, 1.5883632390759885, 1.5533731789328158, 1.7329174829646945, 1.8928957870230079, 1.5571074231993407, 1.8734960029833019, 1.6907654968090355, 1.8866758819203824, 1.7662545728962868, 1.7354071189183742, 1.369154016021639, 8.016880513634533, 1.3835173768457025, 1.5556483820546418, 1.4264417900703847, 1.52826969884336, 1.4193599591962993, 1.489703335100785, 1.379754519322887, 1.480973294004798, 1.3895606866572052, 1.4888803260400891, 1.3649536620359868, 1.5319862517062575, 1.3899507329333574, 1.4884703299030662, 1.438352993922308, 1.5109781629871577, 1.4062876971438527, 1.5571328320074826, 1.454477691091597, 1.5536109588574618, 1.393193638883531, 1.5410100501030684, 1.3987079970538616, 1.5207197337877005, 1.402119278209284, 1.5313753422815353, 1.4011815688572824, 1.5470361560583115, 1.4261475170496851, 1.5244699383620173, 1.5219927160069346, 1.893162488238886, 1.7234807992354035, 1.8577102990821004, 1.437492361990735, 1.6839783329050988, 1.765696751885116, 1.737082432024181, 1.4160718810744584, 1.664219204802066, 1.4466578271239996, 1.7679729531519115, 1.418462417786941, 2.1824116942007095, 1.7231047337409109, 1.7240356898400933, 1.9455809141509235, 1.7459864497650415, 1.98028554324992, 1.5500401051249355, 2.050754375755787, 1.7652922340203077, 1.8048878810368478, 1.7928729162085801, 1.5143815090414137, 1.828795877750963, 1.773335280129686, 1.639010834041983, 1.570684360805899, 1.9412850821390748, 1.4533647631760687, 1.5866315679159015, 1.7936383178457618, 1.9322451367042959, 1.536686196923256, 1.5886462659109384, 1.534346983069554, 1.543641144875437, 1.4402961109299213, 1.9169901767745614, 1.7319807226303965, 1.8990065020043403, 1.6449530748650432, 1.6014150569681078, 1.5411282959394157, 1.5630108413752168, 1.380134806735441, 1.4723029190208763, 1.347123626852408, 5.710967884864658, 1.380260558333248, 1.451181999174878, 1.3545447380747646, 1.4718847221229225, 1.4022083790041506, 1.4578524071257561, 1.3926289682276547, 1.4873745350632817, 1.424463848117739, 1.6637338320724666, 1.4615868178661913, 1.579402007162571, 1.3613396470900625, 1.6241482140030712, 1.4640355999581516, 1.5818648329004645, 1.4706695682834834, 1.5343637147452682, 1.4545578139368445, 1.5505541921593249, 1.3430369540583342, 1.8105067887809128, 1.750833615893498, 1.659543865825981, 1.651672838954255, 1.891125890193507, 1.5800970161799341, 1.5623019649647176, 1.3962854980491102, 1.6122239991091192, 1.5375167068559676, 1.605185099877417, 1.4231404920574278, 1.5343338798265904, 1.3886529486626387, 1.3990401406772435, 1.828561957925558, 1.640755858272314, 1.641359289875254, 1.400807507103309, 1.5893288308288902, 1.4308559279888868, 1.774485332891345, 1.6781039452180266, 1.8384615308605134, 1.611357379006222, 1.5312842880375683, 1.353061077883467, 1.4981378628872335, 1.4231001660227776, 1.637610631994903, 1.396809961879626, 1.580499337054789, 1.4241394160781056, 1.6009300642181188, 1.6293147718533874, 1.6286158368457109, 1.4106427400838584, 1.6083785460796207, 1.7869313729461282, 1.629164393991232, 1.4529513171873987, 1.4805306009948254, 1.403573620133102, 1.5310088500846177, 1.3972469987347722, 1.5817218497395515, 1.4352610670030117, 1.6300013458821923]
Total Epoch List: [126, 104]
Total Time List: [0.49073306820355356, 0.6735953751485795]
========================k_idx:2========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: FAGNN
FAGNN(
  (conv1): NeighborFilterConv()
  (conv2): NeighborFilterConv()
  (ffn): Sequential(
    (0): Linear(in_features=14951, out_features=64, bias=True)
    (1): ReLU()
    (2): Linear(in_features=64, out_features=64, bias=True)
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=64, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=64, out_features=2, bias=True)
  )
)
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7f904e368a60>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.6945;  Loss pred: 0.6945; Loss self: 0.0000; time: 0.66s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6931 score: 0.5102 time: 0.55s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6938 score: 0.5000 time: 0.59s
Epoch 2/1000, LR 0.000000
Train loss: 0.6945;  Loss pred: 0.6945; Loss self: 0.0000; time: 0.66s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6931 score: 0.5102 time: 0.71s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6938 score: 0.5000 time: 0.58s
Epoch 3/1000, LR 0.000030
Train loss: 0.6945;  Loss pred: 0.6945; Loss self: 0.0000; time: 0.61s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6930 score: 0.5102 time: 0.49s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6937 score: 0.5000 time: 0.57s
Epoch 4/1000, LR 0.000060
Train loss: 0.6945;  Loss pred: 0.6945; Loss self: 0.0000; time: 0.80s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6930 score: 0.5102 time: 0.58s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6937 score: 0.5000 time: 0.60s
Epoch 5/1000, LR 0.000090
Train loss: 0.6944;  Loss pred: 0.6944; Loss self: 0.0000; time: 0.66s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6929 score: 0.5102 time: 0.54s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6936 score: 0.5000 time: 0.45s
Epoch 6/1000, LR 0.000120
Train loss: 0.6943;  Loss pred: 0.6943; Loss self: 0.0000; time: 0.63s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6929 score: 0.5102 time: 0.69s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6935 score: 0.5000 time: 0.50s
Epoch 7/1000, LR 0.000150
Train loss: 0.6941;  Loss pred: 0.6941; Loss self: 0.0000; time: 0.61s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6928 score: 0.5102 time: 0.57s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6934 score: 0.5000 time: 0.57s
Epoch 8/1000, LR 0.000180
Train loss: 0.6940;  Loss pred: 0.6940; Loss self: 0.0000; time: 2.00s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6927 score: 0.5102 time: 3.72s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6933 score: 0.5000 time: 0.45s
Epoch 9/1000, LR 0.000210
Train loss: 0.6938;  Loss pred: 0.6938; Loss self: 0.0000; time: 0.54s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6927 score: 0.5102 time: 0.42s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6932 score: 0.5000 time: 0.44s
Epoch 10/1000, LR 0.000240
Train loss: 0.6936;  Loss pred: 0.6936; Loss self: 0.0000; time: 0.49s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6925 score: 0.5102 time: 0.52s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6930 score: 0.5000 time: 0.47s
Epoch 11/1000, LR 0.000270
Train loss: 0.6934;  Loss pred: 0.6934; Loss self: 0.0000; time: 0.51s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6924 score: 0.5102 time: 0.42s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6929 score: 0.5000 time: 0.45s
Epoch 12/1000, LR 0.000270
Train loss: 0.6932;  Loss pred: 0.6932; Loss self: 0.0000; time: 0.53s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6923 score: 0.5102 time: 0.54s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6927 score: 0.5000 time: 0.43s
Epoch 13/1000, LR 0.000270
Train loss: 0.6930;  Loss pred: 0.6930; Loss self: 0.0000; time: 0.49s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6922 score: 0.5102 time: 0.43s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6925 score: 0.5000 time: 0.43s
Epoch 14/1000, LR 0.000270
Train loss: 0.6927;  Loss pred: 0.6927; Loss self: 0.0000; time: 0.51s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6921 score: 0.5102 time: 0.41s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6923 score: 0.5000 time: 0.55s
Epoch 15/1000, LR 0.000270
Train loss: 0.6924;  Loss pred: 0.6924; Loss self: 0.0000; time: 0.50s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6919 score: 0.5102 time: 0.42s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6921 score: 0.5000 time: 0.48s
Epoch 16/1000, LR 0.000270
Train loss: 0.6922;  Loss pred: 0.6922; Loss self: 0.0000; time: 0.50s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6917 score: 0.5102 time: 0.42s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6919 score: 0.5000 time: 0.55s
Epoch 17/1000, LR 0.000270
Train loss: 0.6918;  Loss pred: 0.6918; Loss self: 0.0000; time: 0.50s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6915 score: 0.5102 time: 0.45s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6916 score: 0.5000 time: 0.46s
Epoch 18/1000, LR 0.000270
Train loss: 0.6915;  Loss pred: 0.6915; Loss self: 0.0000; time: 0.51s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6912 score: 0.5102 time: 0.44s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6913 score: 0.5000 time: 0.60s
Epoch 19/1000, LR 0.000270
Train loss: 0.6911;  Loss pred: 0.6911; Loss self: 0.0000; time: 0.56s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6910 score: 0.5102 time: 0.42s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6910 score: 0.5000 time: 0.44s
Epoch 20/1000, LR 0.000270
Train loss: 0.6907;  Loss pred: 0.6907; Loss self: 0.0000; time: 0.52s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6907 score: 0.5102 time: 0.46s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6906 score: 0.5000 time: 0.65s
Epoch 21/1000, LR 0.000270
Train loss: 0.6902;  Loss pred: 0.6902; Loss self: 0.0000; time: 0.53s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6904 score: 0.5102 time: 0.44s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6902 score: 0.5000 time: 0.51s
Epoch 22/1000, LR 0.000270
Train loss: 0.6897;  Loss pred: 0.6897; Loss self: 0.0000; time: 0.54s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6900 score: 0.5102 time: 0.47s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6898 score: 0.5000 time: 0.48s
Epoch 23/1000, LR 0.000270
Train loss: 0.6892;  Loss pred: 0.6892; Loss self: 0.0000; time: 0.67s
Val loss: 0.6896 score: 0.5306 time: 0.44s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6893 score: 0.5000 time: 0.47s
Epoch 24/1000, LR 0.000270
Train loss: 0.6886;  Loss pred: 0.6886; Loss self: 0.0000; time: 0.61s
Val loss: 0.6892 score: 0.5306 time: 0.53s
Test loss: 0.6888 score: 0.5417 time: 0.70s
Epoch 25/1000, LR 0.000270
Train loss: 0.6880;  Loss pred: 0.6880; Loss self: 0.0000; time: 0.55s
Val loss: 0.6887 score: 0.5510 time: 0.53s
Test loss: 0.6882 score: 0.5625 time: 0.49s
Epoch 26/1000, LR 0.000270
Train loss: 0.6873;  Loss pred: 0.6873; Loss self: 0.0000; time: 0.61s
Val loss: 0.6882 score: 0.5510 time: 0.54s
Test loss: 0.6876 score: 0.5833 time: 0.59s
Epoch 27/1000, LR 0.000270
Train loss: 0.6864;  Loss pred: 0.6864; Loss self: 0.0000; time: 0.65s
Val loss: 0.6876 score: 0.5714 time: 0.43s
Test loss: 0.6868 score: 0.6042 time: 0.45s
Epoch 28/1000, LR 0.000270
Train loss: 0.6858;  Loss pred: 0.6858; Loss self: 0.0000; time: 0.51s
Val loss: 0.6869 score: 0.5918 time: 0.42s
Test loss: 0.6861 score: 0.6250 time: 0.54s
Epoch 29/1000, LR 0.000270
Train loss: 0.6847;  Loss pred: 0.6847; Loss self: 0.0000; time: 0.61s
Val loss: 0.6862 score: 0.6122 time: 0.43s
Test loss: 0.6852 score: 0.6250 time: 0.46s
Epoch 30/1000, LR 0.000270
Train loss: 0.6837;  Loss pred: 0.6837; Loss self: 0.0000; time: 0.55s
Val loss: 0.6854 score: 0.6122 time: 0.43s
Test loss: 0.6842 score: 0.6667 time: 0.46s
Epoch 31/1000, LR 0.000270
Train loss: 0.6827;  Loss pred: 0.6827; Loss self: 0.0000; time: 0.62s
Val loss: 0.6844 score: 0.6735 time: 0.47s
Test loss: 0.6831 score: 0.7292 time: 0.49s
Epoch 32/1000, LR 0.000270
Train loss: 0.6815;  Loss pred: 0.6815; Loss self: 0.0000; time: 0.53s
Val loss: 0.6834 score: 0.6735 time: 0.55s
Test loss: 0.6819 score: 0.7500 time: 0.59s
Epoch 33/1000, LR 0.000270
Train loss: 0.6798;  Loss pred: 0.6798; Loss self: 0.0000; time: 0.78s
Val loss: 0.6822 score: 0.6735 time: 0.54s
Test loss: 0.6806 score: 0.7500 time: 0.58s
Epoch 34/1000, LR 0.000270
Train loss: 0.6783;  Loss pred: 0.6783; Loss self: 0.0000; time: 0.63s
Val loss: 0.6809 score: 0.6531 time: 0.53s
Test loss: 0.6790 score: 0.7708 time: 0.56s
Epoch 35/1000, LR 0.000270
Train loss: 0.6770;  Loss pred: 0.6770; Loss self: 0.0000; time: 0.75s
Val loss: 0.6794 score: 0.6531 time: 0.54s
Test loss: 0.6773 score: 0.7500 time: 0.56s
Epoch 36/1000, LR 0.000270
Train loss: 0.6748;  Loss pred: 0.6748; Loss self: 0.0000; time: 0.64s
Val loss: 0.6777 score: 0.6939 time: 0.56s
Test loss: 0.6753 score: 0.7500 time: 0.55s
Epoch 37/1000, LR 0.000270
Train loss: 0.6729;  Loss pred: 0.6729; Loss self: 0.0000; time: 0.75s
Val loss: 0.6759 score: 0.6939 time: 0.53s
Test loss: 0.6732 score: 0.7500 time: 0.53s
Epoch 38/1000, LR 0.000270
Train loss: 0.6697;  Loss pred: 0.6697; Loss self: 0.0000; time: 0.64s
Val loss: 0.6738 score: 0.6939 time: 0.53s
Test loss: 0.6708 score: 0.7500 time: 0.55s
Epoch 39/1000, LR 0.000269
Train loss: 0.6674;  Loss pred: 0.6674; Loss self: 0.0000; time: 0.70s
Val loss: 0.6716 score: 0.7143 time: 0.42s
Test loss: 0.6683 score: 0.7500 time: 0.44s
Epoch 40/1000, LR 0.000269
Train loss: 0.6649;  Loss pred: 0.6649; Loss self: 0.0000; time: 0.51s
Val loss: 0.6693 score: 0.7143 time: 0.43s
Test loss: 0.6655 score: 0.7708 time: 0.44s
Epoch 41/1000, LR 0.000269
Train loss: 0.6606;  Loss pred: 0.6606; Loss self: 0.0000; time: 0.60s
Val loss: 0.6668 score: 0.7143 time: 0.42s
Test loss: 0.6626 score: 0.7917 time: 0.44s
Epoch 42/1000, LR 0.000269
Train loss: 0.6578;  Loss pred: 0.6578; Loss self: 0.0000; time: 0.51s
Val loss: 0.6641 score: 0.7143 time: 0.42s
Test loss: 0.6595 score: 0.8125 time: 0.43s
Epoch 43/1000, LR 0.000269
Train loss: 0.6554;  Loss pred: 0.6554; Loss self: 0.0000; time: 0.62s
Val loss: 0.6613 score: 0.7143 time: 0.74s
Test loss: 0.6563 score: 0.8125 time: 0.60s
Epoch 44/1000, LR 0.000269
Train loss: 0.6517;  Loss pred: 0.6517; Loss self: 0.0000; time: 0.64s
Val loss: 0.6583 score: 0.7143 time: 0.53s
Test loss: 0.6527 score: 0.8125 time: 0.58s
Epoch 45/1000, LR 0.000269
Train loss: 0.6468;  Loss pred: 0.6468; Loss self: 0.0000; time: 0.53s
Val loss: 0.6551 score: 0.7143 time: 0.57s
Test loss: 0.6489 score: 0.8125 time: 0.45s
Epoch 46/1000, LR 0.000269
Train loss: 0.6421;  Loss pred: 0.6421; Loss self: 0.0000; time: 0.53s
Val loss: 0.6516 score: 0.7347 time: 0.55s
Test loss: 0.6448 score: 0.8125 time: 0.72s
Epoch 47/1000, LR 0.000269
Train loss: 0.6375;  Loss pred: 0.6375; Loss self: 0.0000; time: 0.71s
Val loss: 0.6478 score: 0.7755 time: 0.76s
Test loss: 0.6404 score: 0.8125 time: 0.64s
Epoch 48/1000, LR 0.000269
Train loss: 0.6330;  Loss pred: 0.6330; Loss self: 0.0000; time: 0.68s
Val loss: 0.6437 score: 0.7755 time: 0.54s
Test loss: 0.6356 score: 0.8333 time: 0.59s
Epoch 49/1000, LR 0.000269
Train loss: 0.6283;  Loss pred: 0.6283; Loss self: 0.0000; time: 0.58s
Val loss: 0.6392 score: 0.7755 time: 0.56s
Test loss: 0.6305 score: 0.8333 time: 0.44s
Epoch 50/1000, LR 0.000269
Train loss: 0.6219;  Loss pred: 0.6219; Loss self: 0.0000; time: 0.51s
Val loss: 0.6345 score: 0.7959 time: 0.42s
Test loss: 0.6250 score: 0.8542 time: 0.44s
Epoch 51/1000, LR 0.000269
Train loss: 0.6142;  Loss pred: 0.6142; Loss self: 0.0000; time: 0.57s
Val loss: 0.6294 score: 0.7959 time: 0.64s
Test loss: 0.6190 score: 0.8542 time: 0.56s
Epoch 52/1000, LR 0.000269
Train loss: 0.6107;  Loss pred: 0.6107; Loss self: 0.0000; time: 0.64s
Val loss: 0.6238 score: 0.8163 time: 0.56s
Test loss: 0.6126 score: 0.8542 time: 0.56s
Epoch 53/1000, LR 0.000269
Train loss: 0.6007;  Loss pred: 0.6007; Loss self: 0.0000; time: 0.51s
Val loss: 0.6179 score: 0.8163 time: 0.55s
Test loss: 0.6058 score: 0.8542 time: 0.46s
Epoch 54/1000, LR 0.000269
Train loss: 0.5908;  Loss pred: 0.5908; Loss self: 0.0000; time: 0.51s
Val loss: 0.6116 score: 0.8163 time: 0.46s
Test loss: 0.5985 score: 0.8750 time: 0.53s
Epoch 55/1000, LR 0.000269
Train loss: 0.5878;  Loss pred: 0.5878; Loss self: 0.0000; time: 0.62s
Val loss: 0.6048 score: 0.8163 time: 0.65s
Test loss: 0.5907 score: 0.8958 time: 0.54s
Epoch 56/1000, LR 0.000269
Train loss: 0.5790;  Loss pred: 0.5790; Loss self: 0.0000; time: 0.53s
Val loss: 0.5976 score: 0.8163 time: 0.42s
Test loss: 0.5824 score: 0.8958 time: 0.45s
Epoch 57/1000, LR 0.000269
Train loss: 0.5677;  Loss pred: 0.5677; Loss self: 0.0000; time: 0.52s
Val loss: 0.5899 score: 0.8367 time: 0.52s
Test loss: 0.5736 score: 0.9167 time: 0.49s
Epoch 58/1000, LR 0.000269
Train loss: 0.5612;  Loss pred: 0.5612; Loss self: 0.0000; time: 0.53s
Val loss: 0.5818 score: 0.8571 time: 0.41s
Test loss: 0.5643 score: 0.9375 time: 0.49s
Epoch 59/1000, LR 0.000268
Train loss: 0.5453;  Loss pred: 0.5453; Loss self: 0.0000; time: 0.50s
Val loss: 0.5732 score: 0.8776 time: 0.55s
Test loss: 0.5545 score: 0.9375 time: 0.45s
Epoch 60/1000, LR 0.000268
Train loss: 0.5353;  Loss pred: 0.5353; Loss self: 0.0000; time: 0.63s
Val loss: 0.5642 score: 0.8776 time: 0.53s
Test loss: 0.5443 score: 0.9375 time: 0.56s
Epoch 61/1000, LR 0.000268
Train loss: 0.5255;  Loss pred: 0.5255; Loss self: 0.0000; time: 0.76s
Val loss: 0.5548 score: 0.8776 time: 0.45s
Test loss: 0.5337 score: 0.9375 time: 0.54s
Epoch 62/1000, LR 0.000268
Train loss: 0.5107;  Loss pred: 0.5107; Loss self: 0.0000; time: 0.57s
Val loss: 0.5450 score: 0.8980 time: 0.42s
Test loss: 0.5226 score: 0.9375 time: 0.48s
Epoch 63/1000, LR 0.000268
Train loss: 0.4988;  Loss pred: 0.4988; Loss self: 0.0000; time: 0.51s
Val loss: 0.5347 score: 0.8980 time: 0.60s
Test loss: 0.5111 score: 0.9583 time: 0.45s
Epoch 64/1000, LR 0.000268
Train loss: 0.4878;  Loss pred: 0.4878; Loss self: 0.0000; time: 0.51s
Val loss: 0.5240 score: 0.8980 time: 0.48s
Test loss: 0.4992 score: 0.9583 time: 0.56s
Epoch 65/1000, LR 0.000268
Train loss: 0.4759;  Loss pred: 0.4759; Loss self: 0.0000; time: 0.70s
Val loss: 0.5131 score: 0.8980 time: 0.66s
Test loss: 0.4869 score: 0.9583 time: 0.61s
Epoch 66/1000, LR 0.000268
Train loss: 0.4602;  Loss pred: 0.4602; Loss self: 0.0000; time: 0.64s
Val loss: 0.5018 score: 0.8980 time: 0.48s
Test loss: 0.4743 score: 0.9583 time: 0.44s
Epoch 67/1000, LR 0.000268
Train loss: 0.4386;  Loss pred: 0.4386; Loss self: 0.0000; time: 0.50s
Val loss: 0.4901 score: 0.8980 time: 0.54s
Test loss: 0.4613 score: 0.9583 time: 0.60s
Epoch 68/1000, LR 0.000268
Train loss: 0.4353;  Loss pred: 0.4353; Loss self: 0.0000; time: 0.62s
Val loss: 0.4783 score: 0.8980 time: 0.53s
Test loss: 0.4479 score: 0.9583 time: 0.59s
Epoch 69/1000, LR 0.000268
Train loss: 0.4128;  Loss pred: 0.4128; Loss self: 0.0000; time: 0.63s
Val loss: 0.4663 score: 0.9184 time: 0.53s
Test loss: 0.4345 score: 0.9583 time: 0.67s
Epoch 70/1000, LR 0.000268
Train loss: 0.3980;  Loss pred: 0.3980; Loss self: 0.0000; time: 0.63s
Val loss: 0.4542 score: 0.9184 time: 0.59s
Test loss: 0.4210 score: 0.9583 time: 0.44s
Epoch 71/1000, LR 0.000268
Train loss: 0.3907;  Loss pred: 0.3907; Loss self: 0.0000; time: 0.56s
Val loss: 0.4422 score: 0.9184 time: 0.44s
Test loss: 0.4076 score: 0.9583 time: 0.56s
Epoch 72/1000, LR 0.000267
Train loss: 0.3611;  Loss pred: 0.3611; Loss self: 0.0000; time: 0.61s
Val loss: 0.4301 score: 0.9184 time: 0.41s
Test loss: 0.3941 score: 0.9583 time: 0.49s
Epoch 73/1000, LR 0.000267
Train loss: 0.3505;  Loss pred: 0.3505; Loss self: 0.0000; time: 0.63s
Val loss: 0.4182 score: 0.9184 time: 0.53s
Test loss: 0.3807 score: 0.9583 time: 0.67s
Epoch 74/1000, LR 0.000267
Train loss: 0.3273;  Loss pred: 0.3273; Loss self: 0.0000; time: 0.63s
Val loss: 0.4062 score: 0.9184 time: 0.57s
Test loss: 0.3672 score: 0.9583 time: 0.55s
Epoch 75/1000, LR 0.000267
Train loss: 0.3231;  Loss pred: 0.3231; Loss self: 0.0000; time: 0.65s
Val loss: 0.3948 score: 0.9184 time: 0.54s
Test loss: 0.3542 score: 0.9583 time: 0.46s
Epoch 76/1000, LR 0.000267
Train loss: 0.3004;  Loss pred: 0.3004; Loss self: 0.0000; time: 0.61s
Val loss: 0.3836 score: 0.9184 time: 0.43s
Test loss: 0.3415 score: 0.9375 time: 0.53s
Epoch 77/1000, LR 0.000267
Train loss: 0.2880;  Loss pred: 0.2880; Loss self: 0.0000; time: 0.63s
Val loss: 0.3728 score: 0.9184 time: 0.53s
Test loss: 0.3291 score: 0.9375 time: 0.67s
Epoch 78/1000, LR 0.000267
Train loss: 0.2694;  Loss pred: 0.2694; Loss self: 0.0000; time: 0.63s
Val loss: 0.3623 score: 0.9184 time: 0.58s
Test loss: 0.3169 score: 0.9375 time: 0.56s
Epoch 79/1000, LR 0.000267
Train loss: 0.2545;  Loss pred: 0.2545; Loss self: 0.0000; time: 0.52s
Val loss: 0.3520 score: 0.9184 time: 0.45s
Test loss: 0.3048 score: 0.9375 time: 0.43s
Epoch 80/1000, LR 0.000267
Train loss: 0.2404;  Loss pred: 0.2404; Loss self: 0.0000; time: 0.60s
Val loss: 0.3419 score: 0.9184 time: 0.53s
Test loss: 0.2925 score: 0.9375 time: 0.49s
Epoch 81/1000, LR 0.000267
Train loss: 0.2205;  Loss pred: 0.2205; Loss self: 0.0000; time: 0.54s
Val loss: 0.3325 score: 0.9184 time: 0.45s
Test loss: 0.2808 score: 0.9375 time: 0.46s
Epoch 82/1000, LR 0.000267
Train loss: 0.2104;  Loss pred: 0.2104; Loss self: 0.0000; time: 0.68s
Val loss: 0.3242 score: 0.9184 time: 0.44s
Test loss: 0.2707 score: 0.9167 time: 0.59s
Epoch 83/1000, LR 0.000266
Train loss: 0.1934;  Loss pred: 0.1934; Loss self: 0.0000; time: 0.63s
Val loss: 0.3167 score: 0.9184 time: 0.53s
Test loss: 0.2615 score: 0.9167 time: 0.60s
Epoch 84/1000, LR 0.000266
Train loss: 0.1827;  Loss pred: 0.1827; Loss self: 0.0000; time: 0.86s
Val loss: 0.3099 score: 0.9184 time: 0.54s
Test loss: 0.2529 score: 0.9167 time: 0.57s
Epoch 85/1000, LR 0.000266
Train loss: 0.1603;  Loss pred: 0.1603; Loss self: 0.0000; time: 0.64s
Val loss: 0.3044 score: 0.9184 time: 0.56s
Test loss: 0.2459 score: 0.9167 time: 0.57s
Epoch 86/1000, LR 0.000266
Train loss: 0.1539;  Loss pred: 0.1539; Loss self: 0.0000; time: 0.80s
Val loss: 0.3003 score: 0.9184 time: 0.53s
Test loss: 0.2409 score: 0.9167 time: 0.63s
Epoch 87/1000, LR 0.000266
Train loss: 0.1329;  Loss pred: 0.1329; Loss self: 0.0000; time: 1.65s
Val loss: 0.2969 score: 0.9184 time: 2.80s
Test loss: 0.2361 score: 0.9167 time: 1.18s
Epoch 88/1000, LR 0.000266
Train loss: 0.1201;  Loss pred: 0.1201; Loss self: 0.0000; time: 0.65s
Val loss: 0.2942 score: 0.9184 time: 0.46s
Test loss: 0.2319 score: 0.9167 time: 0.44s
Epoch 89/1000, LR 0.000266
Train loss: 0.1142;  Loss pred: 0.1142; Loss self: 0.0000; time: 0.50s
Val loss: 0.2917 score: 0.9184 time: 0.43s
Test loss: 0.2273 score: 0.9167 time: 0.44s
Epoch 90/1000, LR 0.000266
Train loss: 0.1012;  Loss pred: 0.1012; Loss self: 0.0000; time: 0.68s
Val loss: 0.2900 score: 0.9184 time: 0.42s
Test loss: 0.2233 score: 0.9167 time: 0.44s
Epoch 91/1000, LR 0.000266
Train loss: 0.1055;  Loss pred: 0.1055; Loss self: 0.0000; time: 0.51s
Val loss: 0.2890 score: 0.9184 time: 0.44s
Test loss: 0.2199 score: 0.9167 time: 0.49s
Epoch 92/1000, LR 0.000266
Train loss: 0.0905;  Loss pred: 0.0905; Loss self: 0.0000; time: 0.67s
Val loss: 0.2898 score: 0.9184 time: 0.42s
Test loss: 0.2188 score: 0.9167 time: 0.45s
     INFO: Early stopping counter 1 of 20
Epoch 93/1000, LR 0.000265
Train loss: 0.0837;  Loss pred: 0.0837; Loss self: 0.0000; time: 0.51s
Val loss: 0.2900 score: 0.9184 time: 0.48s
Test loss: 0.2162 score: 0.9167 time: 0.47s
     INFO: Early stopping counter 2 of 20
Epoch 94/1000, LR 0.000265
Train loss: 0.0671;  Loss pred: 0.0671; Loss self: 0.0000; time: 0.68s
Val loss: 0.2909 score: 0.9184 time: 0.43s
Test loss: 0.2141 score: 0.9167 time: 0.44s
     INFO: Early stopping counter 3 of 20
Epoch 95/1000, LR 0.000265
Train loss: 0.0618;  Loss pred: 0.0618; Loss self: 0.0000; time: 0.55s
Val loss: 0.2932 score: 0.9184 time: 0.42s
Test loss: 0.2138 score: 0.9167 time: 0.45s
     INFO: Early stopping counter 4 of 20
Epoch 96/1000, LR 0.000265
Train loss: 0.0557;  Loss pred: 0.0557; Loss self: 0.0000; time: 0.53s
Val loss: 0.2959 score: 0.9184 time: 0.57s
Test loss: 0.2139 score: 0.9167 time: 0.48s
     INFO: Early stopping counter 5 of 20
Epoch 97/1000, LR 0.000265
Train loss: 0.0537;  Loss pred: 0.0537; Loss self: 0.0000; time: 0.52s
Val loss: 0.2990 score: 0.9184 time: 0.43s
Test loss: 0.2141 score: 0.9167 time: 0.45s
     INFO: Early stopping counter 6 of 20
Epoch 98/1000, LR 0.000265
Train loss: 0.0670;  Loss pred: 0.0670; Loss self: 0.0000; time: 0.50s
Val loss: 0.3045 score: 0.9184 time: 0.52s
Test loss: 0.2179 score: 0.9167 time: 0.47s
     INFO: Early stopping counter 7 of 20
Epoch 99/1000, LR 0.000265
Train loss: 0.0453;  Loss pred: 0.0453; Loss self: 0.0000; time: 0.49s
Val loss: 0.3106 score: 0.9184 time: 0.42s
Test loss: 0.2223 score: 0.9167 time: 0.58s
     INFO: Early stopping counter 8 of 20
Epoch 100/1000, LR 0.000265
Train loss: 0.0412;  Loss pred: 0.0412; Loss self: 0.0000; time: 0.67s
Val loss: 0.3168 score: 0.8980 time: 0.59s
Test loss: 0.2264 score: 0.9167 time: 0.44s
     INFO: Early stopping counter 9 of 20
Epoch 101/1000, LR 0.000265
Train loss: 0.0340;  Loss pred: 0.0340; Loss self: 0.0000; time: 0.50s
Val loss: 0.3237 score: 0.8980 time: 0.43s
Test loss: 0.2312 score: 0.9167 time: 0.44s
     INFO: Early stopping counter 10 of 20
Epoch 102/1000, LR 0.000264
Train loss: 0.0323;  Loss pred: 0.0323; Loss self: 0.0000; time: 0.63s
Val loss: 0.3303 score: 0.8980 time: 0.53s
Test loss: 0.2356 score: 0.9167 time: 0.45s
     INFO: Early stopping counter 11 of 20
Epoch 103/1000, LR 0.000264
Train loss: 0.0312;  Loss pred: 0.0312; Loss self: 0.0000; time: 0.53s
Val loss: 0.3369 score: 0.8980 time: 0.53s
Test loss: 0.2396 score: 0.9167 time: 0.45s
     INFO: Early stopping counter 12 of 20
Epoch 104/1000, LR 0.000264
Train loss: 0.0271;  Loss pred: 0.0271; Loss self: 0.0000; time: 0.52s
Val loss: 0.3439 score: 0.8980 time: 0.53s
Test loss: 0.2440 score: 0.9167 time: 0.45s
     INFO: Early stopping counter 13 of 20
Epoch 105/1000, LR 0.000264
Train loss: 0.0238;  Loss pred: 0.0238; Loss self: 0.0000; time: 0.51s
Val loss: 0.3508 score: 0.8980 time: 0.63s
Test loss: 0.2482 score: 0.9167 time: 0.45s
     INFO: Early stopping counter 14 of 20
Epoch 106/1000, LR 0.000264
Train loss: 0.0205;  Loss pred: 0.0205; Loss self: 0.0000; time: 0.52s
Val loss: 0.3576 score: 0.8980 time: 0.55s
Test loss: 0.2523 score: 0.9167 time: 0.44s
     INFO: Early stopping counter 15 of 20
Epoch 107/1000, LR 0.000264
Train loss: 0.0191;  Loss pred: 0.0191; Loss self: 0.0000; time: 0.52s
Val loss: 0.3646 score: 0.8980 time: 0.42s
Test loss: 0.2566 score: 0.9167 time: 0.44s
     INFO: Early stopping counter 16 of 20
Epoch 108/1000, LR 0.000264
Train loss: 0.0264;  Loss pred: 0.0264; Loss self: 0.0000; time: 0.51s
Val loss: 0.3758 score: 0.8980 time: 0.52s
Test loss: 0.2655 score: 0.9167 time: 0.47s
     INFO: Early stopping counter 17 of 20
Epoch 109/1000, LR 0.000264
Train loss: 0.0180;  Loss pred: 0.0180; Loss self: 0.0000; time: 0.65s
Val loss: 0.3859 score: 0.8980 time: 0.53s
Test loss: 0.2730 score: 0.9167 time: 0.56s
     INFO: Early stopping counter 18 of 20
Epoch 110/1000, LR 0.000263
Train loss: 0.0146;  Loss pred: 0.0146; Loss self: 0.0000; time: 0.59s
Val loss: 0.3959 score: 0.8980 time: 0.59s
Test loss: 0.2804 score: 0.9167 time: 0.49s
     INFO: Early stopping counter 19 of 20
Epoch 111/1000, LR 0.000263
Train loss: 0.0145;  Loss pred: 0.0145; Loss self: 0.0000; time: 0.51s
Val loss: 0.4060 score: 0.8980 time: 0.43s
Test loss: 0.2877 score: 0.9167 time: 0.44s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 090,   Train_Loss: 0.1055,   Val_Loss: 0.2890,   Val_Precision: 0.9565,   Val_Recall: 0.8800,   Val_accuracy: 0.9167,   Val_Score: 0.9184,   Val_Loss: 0.2890,   Test_Precision: 1.0000,   Test_Recall: 0.8333,   Test_accuracy: 0.9091,   Test_Score: 0.9167,   Test_loss: 0.2199


[0.6371192489750683, 0.4808532171882689, 3.714370231842622, 0.4096544119529426, 0.38975901319645345, 0.3850430881138891, 0.3710247480776161, 0.3783402000553906, 0.3670420781709254, 0.3768683730158955, 0.421046560164541, 0.37572586303576827, 0.411221066955477, 0.4810259588994086, 0.38594911084510386, 0.6231586679350585, 0.397189409006387, 0.4039589809253812, 0.37478494900278747, 0.477442956995219, 0.5091569651849568, 0.47591616678982973, 0.48965000407770276, 0.4802797739394009, 0.47649064287543297, 0.480065189069137, 0.491782542783767, 0.49172582500614226, 0.49699680716730654, 0.3862174970563501, 0.46144315507262945, 0.3836425740737468, 0.3893774710595608, 0.3984540731180459, 0.3995904908515513, 0.48709436203353107, 0.49183662002906203, 0.4799299337901175, 0.4831981249153614, 0.4137717669364065, 0.39104513195343316, 0.47490956890396774, 0.386658368865028, 0.4852432010229677, 0.48349783802405, 0.39194428897462785, 0.38774389401078224, 0.38950822385959327, 0.3942769749555737, 0.3878774519544095, 0.3933185050264001, 0.3981288420036435, 0.39727973588742316, 0.3970378169324249, 0.3948308229446411, 0.388742578914389, 0.38538955710828304, 0.3846820299513638, 0.39339937595650554, 0.39598918915726244, 0.4065545490011573, 0.4934380538761616, 0.4902941449545324, 0.5092109639663249, 0.47928488184697926, 0.4903763961046934, 0.47615545615553856, 0.39838811685331166, 0.5027259481139481, 0.38288986403495073, 0.38689173106104136, 0.49285208201035857, 0.5074475901201367, 0.38492341386154294, 0.480933859013021, 0.45336840697564185, 0.4839022180531174, 0.48431572620756924, 0.4099678809288889, 0.38217580784112215, 3.666195085970685, 0.37993584712967277, 0.39763258304446936, 0.3872853219509125, 0.38092547096312046, 0.3859282808844, 0.3849974009208381, 0.38504506601020694, 0.3795857548248023, 0.39390418608672917, 0.38431647093966603, 0.3882745981682092, 0.3975116810761392, 0.38622688106261194, 0.37890634615905583, 0.3788105109706521, 0.3907696311362088, 0.3938351417891681, 0.3859603509772569, 0.4023447809740901, 0.4425085550174117, 0.39381645806133747, 0.37678954703733325, 0.38936703302897513, 0.3894375520758331, 0.38302729604765773, 0.373388267820701, 0.3864289380144328, 0.38789789704605937, 0.4186170280445367, 0.3961745931301266, 0.48205641913227737, 0.5134763810783625, 0.4818108689505607, 0.4498666359577328, 0.4215683280490339, 0.47833468206226826, 0.4901617751456797, 0.3961050510406494, 0.3882514392025769, 0.4078892439138144, 0.403796074911952, 0.4152284348383546, 0.39409011509269476, 0.49658835283480585, 0.4896594111341983, 0.6317537249997258, 0.8276711041107774, 0.6036023320630193, 0.8585950520355254, 0.5430245050229132, 0.8507707100361586, 0.6500315980520099, 0.6551386131905019, 0.5700289011001587, 0.556545059196651, 0.6411825919058174, 0.6504921531304717, 0.5518445330671966, 0.5282963290810585, 0.5557929889764637, 0.5067791258916259, 0.5233393979724497, 0.6626467879395932, 0.641011840198189, 0.5547323119826615, 0.5147897580172867, 0.5486243460327387, 0.5065754160750657, 0.5354274909477681, 0.6709368419833481, 0.5141659309156239, 0.6252219888847321, 0.5530714879278094, 0.5364106488414109, 0.5419466679450125, 0.5038494791369885, 0.5000426268670708, 0.49477281607687473, 0.4882541170809418, 1.6021921711508185, 0.510606074007228, 0.4865745771676302, 0.4864989130292088, 0.4940014169551432, 0.5328630078583956, 0.49043655185960233, 0.4954397601541132, 0.5142481699585915, 0.5223727200645953, 0.5706010709982365, 0.5363614200614393, 0.6119856350123882, 0.48994006402790546, 0.5277655080426484, 0.5539650360587984, 0.7012706070672721, 0.4973736950196326, 0.6633224040269852, 0.5583489709533751, 0.5358506839256734, 0.48750436515547335, 0.7921932120807469, 0.6626256529707462, 0.6567739909514785, 0.6362896871287376, 0.7947081569582224, 0.5131718870252371, 0.6164134899154305, 0.5107136180158705, 0.6953773919958621, 0.5211696911137551, 0.6448548841290176, 0.523309743963182, 0.6542391888797283, 0.5014395818579942, 0.5060953239444643, 0.632935045985505, 0.5613882199395448, 0.5133487300481647, 0.5254671170841902, 0.5051137499976903, 0.5400211671367288, 0.6555883679538965, 0.6250994068104774, 0.6465018449816853, 0.5038698690477759, 0.5567754039075226, 0.49654958909377456, 0.4962187649216503, 0.5003732540644705, 0.5303392850328237, 0.5217939310241491, 0.49603612814098597, 0.49842006084509194, 0.6106066219508648, 0.4917803481221199, 0.5479277528356761, 0.505306676030159, 0.5544896759092808, 0.6611836128868163, 0.5007127940189093, 0.5386823229491711, 0.4900146001018584, 0.4847532350104302, 0.6223258988466114, 0.4999153472017497, 0.494204199872911, 0.505624552955851, 0.6728047681972384, 0.5952602371107787, 0.5874279828276485, 0.5801800778135657, 0.6065082550048828, 0.45833918801508844, 0.501087135868147, 0.5793646781239659, 0.45940249087288976, 0.4405074641108513, 0.4784809141419828, 0.4515137420967221, 0.43596204300411046, 0.4342644640710205, 0.5509594229515642, 0.48688866081647575, 0.5512627172283828, 0.463998232036829, 0.605827041901648, 0.4437068151310086, 0.6565046429168433, 0.5110358721576631, 0.48133302200585604, 0.47188983182422817, 0.7085870790760964, 0.4996143691241741, 0.5934463569428772, 0.4509402750991285, 0.5424179139081389, 0.4669559421017766, 0.46092414506711066, 0.4973657908849418, 0.5930173830129206, 0.5901350569911301, 0.5664566578343511, 0.5678617279045284, 0.5617684689350426, 0.5364492109511048, 0.560443283058703, 0.44458569190464914, 0.44963584188371897, 0.44703520205803216, 0.43949033692479134, 0.6067852971609682, 0.5883034488651901, 0.45645294710993767, 0.7263076680246741, 0.6486848019994795, 0.5989197220187634, 0.4450638780836016, 0.44165611802600324, 0.5683991881087422, 0.5649077440612018, 0.46674464317038655, 0.541118212044239, 0.5427418581675738, 0.4501812639646232, 0.4962533798534423, 0.49844484496861696, 0.461202333914116, 0.5654159090481699, 0.5489215331617743, 0.48425397695973516, 0.4569800649769604, 0.5630479138344526, 0.6194405348505825, 0.4474321831949055, 0.6051442269235849, 0.5947557769250125, 0.6805843731854111, 0.44484559493139386, 0.5605027391575277, 0.5006124461069703, 0.6809231650549918, 0.5620463099330664, 0.460511531913653, 0.54184041894041, 0.6748177991248667, 0.5647291929926723, 0.4399537949357182, 0.49008940905332565, 0.46153720188885927, 0.6017187109682709, 0.6058550351299345, 0.5750153758563101, 0.5735350660979748, 0.6347437489312142, 1.1897740429267287, 0.4459320188034326, 0.44629419199191034, 0.4491557548753917, 0.49036392010748386, 0.4582497398369014, 0.4720017910003662, 0.4492897849995643, 0.450363997137174, 0.48531160899437964, 0.45531369489617646, 0.4716254100203514, 0.5850409469567239, 0.44231153395958245, 0.4425425659865141, 0.4531070070806891, 0.4534013089723885, 0.45288721891120076, 0.4574547840747982, 0.44538878509774804, 0.44878472085110843, 0.474390554940328, 0.5623027242254466, 0.49685879121534526, 0.44274428207427263]
[0.013002433652552416, 0.009813330963025895, 0.07580347411923719, 0.008360294121488625, 0.007954265575437826, 0.0078580222064059, 0.007571933634237063, 0.007721228572558992, 0.007490654656549497, 0.007691191286038683, 0.008592786942133489, 0.007667874755832005, 0.008392266672560754, 0.009816856304069563, 0.00787651246622661, 0.012717523835409356, 0.008105906306252795, 0.00824406083521186, 0.007648672428628316, 0.009743733816228959, 0.010390958473162383, 0.009712574832445505, 0.009992857226075567, 0.00980162803957961, 0.00972429883419251, 0.009797248756513, 0.01003637842415851, 0.0100352209184927, 0.010142791983006256, 0.00788198973584388, 0.009417207246380193, 0.007829440287219323, 0.007946479001215525, 0.008131715777919305, 0.008154907976562271, 0.00994070126599043, 0.01003748204140943, 0.009794488444696275, 0.009861186222762478, 0.008444321774212378, 0.007980512897008839, 0.00969203201844832, 0.00789098711969445, 0.009902922469856483, 0.009867302816817346, 0.007998863040298527, 0.007913140694097596, 0.007949147425705985, 0.00804646887664436, 0.00791586636641652, 0.0080269082658449, 0.008125078408237623, 0.008107749711988228, 0.00810281259045765, 0.008057771896829411, 0.007933522018661, 0.007865093002209857, 0.007850653672476813, 0.008028558692989908, 0.0080814120236176, 0.008297031612268515, 0.010070164364819624, 0.010006002958255763, 0.010392060489108672, 0.009781324119326107, 0.010007681553157009, 0.009717458288888542, 0.008130369731700239, 0.010259713226815268, 0.007814078857856137, 0.00789574961349064, 0.010058205755313441, 0.010356073267757893, 0.007855579874725366, 0.009814976714551449, 0.00925241646889065, 0.009875555470471784, 0.009883994412399372, 0.008366691447528345, 0.00779950628247188, 0.07482030787695275, 0.007753792798564751, 0.008114950674376925, 0.007903782080630866, 0.007773989203328989, 0.007876087364987756, 0.007857089814710982, 0.007858062571636876, 0.007746648057649026, 0.008038860940545494, 0.00784319328448298, 0.007923971391187943, 0.008112483287268147, 0.007882181246175754, 0.007732782574674609, 0.007730826754503104, 0.0079748904313512, 0.008037451873248329, 0.007876741856678712, 0.008211117979063064, 0.009030786837090035, 0.008037070572680357, 0.007689582592598638, 0.007946265980183167, 0.007947705144404757, 0.007816883592809342, 0.007620168731034715, 0.007886304857437404, 0.007916283613184884, 0.008543204653970137, 0.00808519577816585, 0.009837886104740354, 0.010479109817925764, 0.009832874876542054, 0.009180951754239445, 0.008603435266306815, 0.009761932286985067, 0.0100033015335853, 0.008083776551849988, 0.007923498759236263, 0.008324270283955396, 0.008240736222692899, 0.008474049690578665, 0.008042655410054994, 0.01013445618030216, 0.009993049206820374, 0.01289293316325971, 0.016891247022668927, 0.012318414940061619, 0.01752234800072501, 0.011082132755569657, 0.01736266755175834, 0.013265950980653264, 0.013370175779397999, 0.011633242879595076, 0.011358062432584713, 0.01308535901848607, 0.013275350063887179, 0.011262133327901972, 0.010781557736348132, 0.011342714060744157, 0.010342431140645427, 0.010680395876988769, 0.013523403835501902, 0.01308187428975896, 0.011321067591482888, 0.010505913428924218, 0.011196415225157932, 0.010338273797450321, 0.010927091651995267, 0.013692588611905063, 0.010493182263584162, 0.012759632426219024, 0.011287173223016518, 0.010947156098804303, 0.011060136080510458, 0.010282642431367112, 0.010204951568715731, 0.010097404409732138, 0.00996436973634575, 0.032697799411241193, 0.01042053212259649, 0.00993009341158429, 0.009928549245494057, 0.010081661570513127, 0.010874755262416236, 0.010008909221624538, 0.010111015513349248, 0.010494860611399827, 0.010660667756420314, 0.011644919816290542, 0.010946151429825291, 0.012489502755354862, 0.00999877681689603, 0.0107707246539316, 0.011305408899159151, 0.014311645042189226, 0.010150483571829237, 0.013537191918918065, 0.011394876958232145, 0.010935728243381088, 0.009949068676642314, 0.01616720840981116, 0.013522972509607064, 0.013403550835744458, 0.012985503818953827, 0.016218533815473925, 0.010472895653576267, 0.012579867141131235, 0.01042272689828307, 0.014191375346854329, 0.010636116145178676, 0.013160303757735053, 0.010679790693126162, 0.013351820181218945, 0.010233460854244779, 0.010328475998866618, 0.012917041754806223, 0.011456902447745812, 0.010476504694860505, 0.010723818716003882, 0.010308443877503884, 0.011020840145647526, 0.013379354448038705, 0.012757130751234233, 0.013193915203707864, 0.010283058551995426, 0.01136276334505148, 0.01013366508354642, 0.010126913569829598, 0.010211699062540215, 0.010823250714955586, 0.010648855735186716, 0.010123186288591551, 0.010171837976430448, 0.012461359631650301, 0.010036333635145304, 0.011182199037462776, 0.010312381143472632, 0.01131611583488328, 0.013493543120139107, 0.010218628449365497, 0.010993516794881043, 0.010000297961262415, 0.009892923163478168, 0.012700528547890028, 0.010202354024525503, 0.010085799997406346, 0.010318868427670427, 0.013730709555045682, 0.01240125493980789, 0.012238082975576011, 0.012087084954449287, 0.012635588645935059, 0.009548733083647676, 0.010439315330586396, 0.012070097460915955, 0.009570885226518536, 0.009177238835642735, 0.009968352377957975, 0.009406536293681711, 0.009082542562585635, 0.009047176334812926, 0.011478321311490921, 0.010143513767009912, 0.011484639942257976, 0.009666629834100604, 0.012621396706284335, 0.009243891981896013, 0.013677180060767569, 0.010646580669951314, 0.010027771291788667, 0.009831038163004754, 0.014762230814085342, 0.01040863269008696, 0.012363465769643275, 0.009394589064565176, 0.01130037320641956, 0.009728248793787012, 0.009602586355564805, 0.010361787310102955, 0.01235452881276918, 0.012294480353981877, 0.01180118037154898, 0.011830452664677674, 0.011703509769480055, 0.011176025228148015, 0.011675901730389645, 0.00926220191468019, 0.009367413372577479, 0.009313233376209004, 0.009156048685933152, 0.01264136035752017, 0.012256321851358129, 0.009509436398123702, 0.015131409750514043, 0.01351426670832249, 0.012477494208724238, 0.0092721641267417, 0.009201169125541734, 0.011841649752265463, 0.01176891133460837, 0.009723846732716387, 0.01127329608425498, 0.011307122045157788, 0.009378776332596317, 0.010338612080280049, 0.010384267603512853, 0.009608381956544084, 0.011779498105170205, 0.01143586527420363, 0.010088624519994482, 0.009520418020353342, 0.011730164871551096, 0.012905011142720468, 0.009321503816560531, 0.012607171394241353, 0.012390745352604426, 0.014178841108029397, 0.009267616561070705, 0.01167714039911516, 0.010429425960561881, 0.014185899271978997, 0.01170929812360555, 0.009593990248201104, 0.011288342061258541, 0.014058704148434723, 0.011765191520680673, 0.009165704061160795, 0.010210196021944284, 0.009615358372684568, 0.012535806478505643, 0.012621979898540303, 0.011979486997006461, 0.011948647210374475, 0.013223828102733629, 0.024786959227640182, 0.009290250391738178, 0.009297795666498132, 0.009357411559903994, 0.010215915002239248, 0.009546869579935446, 0.009833370645840963, 0.00936020385415759, 0.009382583273691125, 0.010110658520716242, 0.009485701977003677, 0.009825529375423988, 0.012188353061598415, 0.009214823624157967, 0.00921963679138571, 0.009439729314181022, 0.009445860603591427, 0.00943515039398335, 0.009530308001558296, 0.00927893302286975, 0.009349681684398092, 0.009883136561256833, 0.011714640088030137, 0.01035122481698636, 0.00922383920988068]
[76.90867930740774, 101.90219852644759, 13.192007511780028, 119.6130166556797, 125.71870910218598, 127.25848486210626, 132.06666200538595, 129.5130678495867, 133.49968004807752, 130.01887000460314, 116.37667810622004, 130.41423234507366, 119.15731935325493, 101.86560432645342, 126.95974319698738, 78.63165919262549, 123.36683428336846, 121.29944453209525, 130.74164298853836, 102.63006141797726, 96.23751288995962, 102.95930968371366, 100.0714787949316, 102.02386745976638, 102.83517784169767, 102.06947122121622, 99.63753435132594, 99.64902697430612, 98.59218267272466, 126.87151766417972, 106.18859432921393, 127.72305085874237, 125.84189800879558, 122.97527696619447, 122.62554070187721, 100.59652465578506, 99.62657924313291, 102.09823674267551, 101.40767828638202, 118.42277292817545, 125.305229489048, 103.17753780595727, 126.72685746808334, 100.98029173144606, 101.34481717695428, 125.01776752045485, 126.3720738272604, 125.79965453479903, 124.2781169392943, 126.32856009830493, 124.58096777498697, 123.07573536596871, 123.33878517751806, 123.41393668386935, 124.10378610909576, 126.04742227321346, 127.14407823518802, 127.37792822346076, 124.55535772232, 123.74075187325437, 120.52503193085786, 99.3032450883846, 99.9400064313312, 96.22730747651471, 102.23564701472094, 99.92324342939764, 102.90756803591873, 122.99563648391155, 97.4686112460096, 127.97413721959276, 126.65041939670995, 99.42131075134657, 96.56169613181017, 127.29805004178134, 101.88511181258576, 108.07987333496007, 101.26012688501734, 101.17367111676126, 119.52155834495558, 128.2132437340729, 13.365355320972084, 128.9691414226471, 123.22933806086024, 126.52170692441227, 128.63408654745476, 126.96659567863423, 127.27358647825058, 127.2578311617713, 129.08808978518158, 124.3957331014788, 127.49908917563027, 126.19934507992745, 123.26681788909383, 126.8684351156193, 129.31955481007165, 129.35227133598787, 125.39357231401715, 124.41754125189566, 126.95604581126359, 121.78609569973635, 110.73232244757835, 124.42344395969398, 130.046070506157, 125.84527153934374, 125.82248357615624, 127.92821949144646, 131.23068993568253, 126.8020978236622, 126.32190164769492, 117.05209467682448, 123.68284299317895, 101.64785293846339, 95.42795307759644, 101.69965676932033, 108.9211692609358, 116.2326406890336, 102.43873554964455, 99.96699556067348, 123.70455734221748, 126.20687279521817, 120.1306500015321, 121.34838113690056, 118.00733256400261, 124.33704405012698, 98.67327681022002, 100.06955627892718, 77.5618695402568, 59.20226012075653, 81.17927548842562, 57.069977149102606, 90.23533845481323, 57.59483656638515, 75.38095093660269, 74.79333230165099, 85.96055376390521, 88.04318570490844, 76.42128875388676, 75.32758045456681, 88.79312390331029, 92.75097573597137, 88.16232117327954, 96.68906530786865, 93.62948822473237, 73.9458802061934, 76.44164573442217, 88.33089211059249, 95.1844888847897, 89.31430104101864, 96.7279470047142, 91.5156595961563, 73.03220949255328, 95.29997429573186, 78.37216360129297, 88.59614185426209, 91.34792552279623, 90.41480075115378, 97.25126655668863, 97.99164584627692, 99.03535199958658, 100.35757669172276, 30.58309788444691, 95.96438917275074, 100.70398721863138, 100.71964949499919, 99.18999889113546, 91.95609242408021, 99.91098708733126, 98.90203399250373, 95.2847338356996, 93.80275446608464, 85.87435686770984, 91.35630969577777, 80.06723883152601, 100.01223332739964, 92.84426369909785, 88.45323587317357, 69.87316951001125, 98.51747386452725, 73.87056385028507, 87.75873611145553, 91.44338426709311, 100.51192051249238, 61.85359739614319, 73.94823876847893, 74.6070957058043, 77.00894889733779, 61.65785461111848, 95.48457590700063, 79.49209548727202, 95.9441813797049, 70.46533373677984, 94.01928169553672, 75.98608804240126, 93.63479385823828, 74.89615546250606, 97.7186520027783, 96.81970506682048, 77.41710671701715, 87.28362701532417, 95.45168251493014, 93.25036411774028, 97.00785219215287, 90.73718398818545, 74.74202166357819, 78.38753239267784, 75.79251378839935, 97.2473311265888, 88.00676117535292, 98.68097985828004, 98.74676949739452, 97.9268967755151, 92.39368340772134, 93.90680321602329, 98.78312731703478, 98.31064968957803, 80.24806518384435, 99.63797900243102, 89.42784837309588, 96.97081460502109, 88.36954433758802, 74.10951972336312, 97.86049125429282, 90.96270271453392, 99.9970204761541, 101.0823579113313, 78.73688061321926, 98.01659475804247, 99.14929903995309, 96.90985082419144, 72.82944817899273, 80.63700043694863, 81.71214413202922, 82.73293385200354, 79.14154441247229, 104.72593497377285, 95.79172276462197, 82.84937244608741, 104.48354319715895, 108.96523648444028, 100.31748097220162, 106.30905667920416, 110.1013282469351, 110.53172426318996, 87.120753363029, 98.58516712939587, 87.07282117922381, 103.44867002896272, 79.23053393148548, 108.17954190274844, 73.11448672584629, 93.92687013797575, 99.72305619084663, 101.71865711630606, 67.74043927330094, 96.07409827732573, 80.88346897480456, 106.44425137996011, 88.49265256406896, 102.79342368779201, 104.13861047138502, 96.50844686080167, 80.94197805151721, 81.33731326644683, 84.73728631509287, 84.52761938566493, 85.44445381741467, 89.47724970067112, 85.64648993209951, 107.9656877718292, 106.75305553691459, 107.37409443153618, 109.21741837571754, 79.10541047151732, 81.59054666871282, 105.15870322212882, 66.08769549486479, 73.99587573509778, 80.1442968653756, 107.84968712060608, 108.68184100910365, 84.44769275570634, 84.96962646488292, 102.83995907046211, 88.70520143586623, 88.4398342926036, 106.62371769379546, 96.72478203408058, 96.29952137035778, 104.07579595843599, 84.89326039800324, 87.44419211161421, 99.12154010867548, 105.03740464569285, 85.2502936617095, 77.48927830752673, 107.27882750242568, 79.31993376855141, 80.70539515928384, 70.52762580389631, 107.90260833627627, 85.63740486290422, 95.88255420590045, 70.49253493398699, 85.40221535431178, 104.23191749517385, 88.58696826985677, 71.13031111842115, 84.9964914079142, 109.102366095088, 97.94131257135005, 104.00028384181839, 79.77149309975684, 79.22687312436992, 83.47602866883103, 83.69148259158115, 75.62106768412092, 40.34379492926628, 107.6397252854778, 107.55237433354289, 106.86716017546415, 97.88648395966568, 104.74637698012438, 101.6945293751285, 106.83528004102419, 106.58045559840772, 98.90552607933985, 105.4218235428769, 101.77568676363032, 82.0455392903475, 108.52079657589518, 108.46414263676218, 105.93524101350343, 105.86647865836473, 105.98665185429208, 104.92840313623552, 107.77101176776509, 106.95551289930117, 101.18245293909311, 85.36327129860238, 96.60692504320842, 108.41472593416295]
Elapsed: 0.5242056145149766~0.2690428746025442
Time per graph: 0.010770728158739787~0.005496648415584365
Speed: 99.5011934747106~19.162383361217838
Total Time: 0.4438
best val loss: 0.2890024483203888 test_score: 0.9167

Testing...
Test loss: 0.4345 score: 0.9583 time: 0.43s
test Score 0.9583
Epoch Time List: [2.7088044830597937, 1.738426759140566, 7.474583562929183, 1.5353863039053977, 1.4546205820515752, 1.4298343886621296, 1.4624888447578996, 1.4687181131448597, 1.4454236452002078, 1.3796417000703514, 1.4999911459162831, 1.3589029130525887, 1.5452265101484954, 1.7252872539684176, 1.7501556316856295, 1.6144894750323147, 1.662863583303988, 1.534979219082743, 1.4679671998601407, 1.5259636691771448, 1.9096654658205807, 1.6105603110045195, 1.8833848431240767, 1.7332990190479904, 1.8836184528190643, 1.7255799788981676, 1.8868012658786029, 2.3925593679305166, 1.9037973310332745, 1.3994987569749355, 1.7374480001162738, 1.4134088295977563, 1.5611583220306784, 1.443603910971433, 1.5870439251884818, 1.6611745317932218, 1.896301138913259, 1.4806042881682515, 1.9136209392454475, 1.5742368979845196, 1.5895431661047041, 1.6771789949852973, 1.6792539421003312, 1.6134485118091106, 1.8920924609992653, 1.503411561017856, 1.5691396549809724, 1.3924752529710531, 1.5682836980558932, 1.4408432990312576, 1.5393485859967768, 1.8936714618466794, 1.5690535609610379, 1.4288713107816875, 1.5503902130294591, 1.407643283950165, 1.537811562186107, 1.3856749790720642, 1.5463570600841194, 1.3784321460407227, 1.5295783670153469, 1.6596226710826159, 1.9918200219981372, 1.7688441299833357, 1.9405640591867268, 1.7801649570465088, 1.8388492208905518, 1.4632135997526348, 2.1014072860125452, 1.5883632390759885, 1.5533731789328158, 1.7329174829646945, 1.8928957870230079, 1.5571074231993407, 1.8734960029833019, 1.6907654968090355, 1.8866758819203824, 1.7662545728962868, 1.7354071189183742, 1.369154016021639, 8.016880513634533, 1.3835173768457025, 1.5556483820546418, 1.4264417900703847, 1.52826969884336, 1.4193599591962993, 1.489703335100785, 1.379754519322887, 1.480973294004798, 1.3895606866572052, 1.4888803260400891, 1.3649536620359868, 1.5319862517062575, 1.3899507329333574, 1.4884703299030662, 1.438352993922308, 1.5109781629871577, 1.4062876971438527, 1.5571328320074826, 1.454477691091597, 1.5536109588574618, 1.393193638883531, 1.5410100501030684, 1.3987079970538616, 1.5207197337877005, 1.402119278209284, 1.5313753422815353, 1.4011815688572824, 1.5470361560583115, 1.4261475170496851, 1.5244699383620173, 1.5219927160069346, 1.893162488238886, 1.7234807992354035, 1.8577102990821004, 1.437492361990735, 1.6839783329050988, 1.765696751885116, 1.737082432024181, 1.4160718810744584, 1.664219204802066, 1.4466578271239996, 1.7679729531519115, 1.418462417786941, 2.1824116942007095, 1.7231047337409109, 1.7240356898400933, 1.9455809141509235, 1.7459864497650415, 1.98028554324992, 1.5500401051249355, 2.050754375755787, 1.7652922340203077, 1.8048878810368478, 1.7928729162085801, 1.5143815090414137, 1.828795877750963, 1.773335280129686, 1.639010834041983, 1.570684360805899, 1.9412850821390748, 1.4533647631760687, 1.5866315679159015, 1.7936383178457618, 1.9322451367042959, 1.536686196923256, 1.5886462659109384, 1.534346983069554, 1.543641144875437, 1.4402961109299213, 1.9169901767745614, 1.7319807226303965, 1.8990065020043403, 1.6449530748650432, 1.6014150569681078, 1.5411282959394157, 1.5630108413752168, 1.380134806735441, 1.4723029190208763, 1.347123626852408, 5.710967884864658, 1.380260558333248, 1.451181999174878, 1.3545447380747646, 1.4718847221229225, 1.4022083790041506, 1.4578524071257561, 1.3926289682276547, 1.4873745350632817, 1.424463848117739, 1.6637338320724666, 1.4615868178661913, 1.579402007162571, 1.3613396470900625, 1.6241482140030712, 1.4640355999581516, 1.5818648329004645, 1.4706695682834834, 1.5343637147452682, 1.4545578139368445, 1.5505541921593249, 1.3430369540583342, 1.8105067887809128, 1.750833615893498, 1.659543865825981, 1.651672838954255, 1.891125890193507, 1.5800970161799341, 1.5623019649647176, 1.3962854980491102, 1.6122239991091192, 1.5375167068559676, 1.605185099877417, 1.4231404920574278, 1.5343338798265904, 1.3886529486626387, 1.3990401406772435, 1.828561957925558, 1.640755858272314, 1.641359289875254, 1.400807507103309, 1.5893288308288902, 1.4308559279888868, 1.774485332891345, 1.6781039452180266, 1.8384615308605134, 1.611357379006222, 1.5312842880375683, 1.353061077883467, 1.4981378628872335, 1.4231001660227776, 1.637610631994903, 1.396809961879626, 1.580499337054789, 1.4241394160781056, 1.6009300642181188, 1.6293147718533874, 1.6286158368457109, 1.4106427400838584, 1.6083785460796207, 1.7869313729461282, 1.629164393991232, 1.4529513171873987, 1.4805306009948254, 1.403573620133102, 1.5310088500846177, 1.3972469987347722, 1.5817218497395515, 1.4352610670030117, 1.6300013458821923, 1.8024631149601191, 1.9423435821663588, 1.6679969888646156, 1.9790679158177227, 1.6596751790493727, 1.8124548287596554, 1.7566340120974928, 6.1763766661752015, 1.3958410937339067, 1.486083379946649, 1.3772621548268944, 1.5037281361874193, 1.3530215309001505, 1.4706884908955544, 1.4005404289346188, 1.465234347153455, 1.4096408654004335, 1.545073009096086, 1.421138840727508, 1.6305688743013889, 1.4780107683036476, 1.4879741619806737, 1.5742477141320705, 1.8408986600115895, 1.5688553790096194, 1.7383017227984965, 1.5279303900897503, 1.4678359439130872, 1.498108257073909, 1.4330386379733682, 1.5862290537916124, 1.665599131025374, 1.9102294049225748, 1.7178534890990704, 1.845998490927741, 1.7501535918563604, 1.8078499929979444, 1.7231391433160752, 1.5649063119199127, 1.3841386761050671, 1.4545017906930298, 1.3644379209727049, 1.9583398692775518, 1.7529981988482177, 1.5509414179250598, 1.7965847293380648, 2.1135177619289607, 1.8176075753290206, 1.580172843998298, 1.3659689449705184, 1.7753095221705735, 1.7575093570630997, 1.5209407827351242, 1.5032488072756678, 1.8122655949555337, 1.394547502975911, 1.5293019695673138, 1.4322185670025647, 1.5015479039866477, 1.7247284250333905, 1.7591594748664647, 1.470403387909755, 1.559300523949787, 1.5473517088685185, 1.9756198457907885, 1.5616970031987876, 1.6484510218724608, 1.7416526388842613, 1.8337766202166677, 1.6579922009259462, 1.5528987848665565, 1.5122539079748094, 1.835267733084038, 1.749430974945426, 1.6431169649586082, 1.5667787827551365, 1.8349550953134894, 1.7678753528743982, 1.403951139189303, 1.619570170994848, 1.4508834907319397, 1.7181967420037836, 1.7510770859662443, 1.9644454452209175, 1.7734408299438655, 1.953715827781707, 5.6336227231658995, 1.554670823039487, 1.3710412250366062, 1.5447967208456248, 1.429884891025722, 1.5461361373309046, 1.4593131388537586, 1.553330120863393, 1.4137642330024391, 1.5795664601027966, 1.402397886151448, 1.4863589617889374, 1.4887705470900983, 1.7015573550015688, 1.3641537921503186, 1.6051051020622253, 1.5112709060776979, 1.4942152281291783, 1.5909836592618376, 1.5040708230808377, 1.381703739054501, 1.5015380347613245, 1.7363128308206797, 1.6787448478862643, 1.3717424899805337]
Total Epoch List: [126, 104, 111]
Total Time List: [0.49073306820355356, 0.6735953751485795, 0.443779184948653]
========================training times:1========================
========================k_idx:0========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: FAGNN
FAGNN(
  (conv1): NeighborFilterConv()
  (conv2): NeighborFilterConv()
  (ffn): Sequential(
    (0): Linear(in_features=14951, out_features=64, bias=True)
    (1): ReLU()
    (2): Linear(in_features=64, out_features=64, bias=True)
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=64, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=64, out_features=2, bias=True)
  )
)
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7f904e438f40>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.6937;  Loss pred: 0.6937; Loss self: 0.0000; time: 0.63s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6930 score: 0.5102 time: 0.43s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6945 score: 0.4898 time: 0.55s
Epoch 2/1000, LR 0.000000
Train loss: 0.6937;  Loss pred: 0.6937; Loss self: 0.0000; time: 0.58s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6930 score: 0.5102 time: 0.43s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6944 score: 0.4898 time: 0.43s
Epoch 3/1000, LR 0.000030
Train loss: 0.6937;  Loss pred: 0.6937; Loss self: 0.0000; time: 0.52s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6929 score: 0.5102 time: 0.56s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6943 score: 0.4898 time: 0.43s
Epoch 4/1000, LR 0.000060
Train loss: 0.6936;  Loss pred: 0.6936; Loss self: 0.0000; time: 0.55s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6929 score: 0.5102 time: 0.43s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6942 score: 0.4898 time: 0.43s
Epoch 5/1000, LR 0.000090
Train loss: 0.6935;  Loss pred: 0.6935; Loss self: 0.0000; time: 0.51s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6928 score: 0.5102 time: 0.53s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6940 score: 0.4898 time: 0.44s
Epoch 6/1000, LR 0.000120
Train loss: 0.6934;  Loss pred: 0.6934; Loss self: 0.0000; time: 0.56s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6927 score: 0.5102 time: 0.43s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6938 score: 0.4898 time: 0.43s
Epoch 7/1000, LR 0.000150
Train loss: 0.6932;  Loss pred: 0.6932; Loss self: 0.0000; time: 0.66s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6926 score: 0.5102 time: 0.43s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6936 score: 0.4898 time: 0.47s
Epoch 8/1000, LR 0.000180
Train loss: 0.6931;  Loss pred: 0.6931; Loss self: 0.0000; time: 0.63s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6924 score: 0.5102 time: 0.54s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6934 score: 0.4898 time: 0.54s
Epoch 9/1000, LR 0.000210
Train loss: 0.6929;  Loss pred: 0.6929; Loss self: 0.0000; time: 0.59s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6923 score: 0.5102 time: 0.64s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6931 score: 0.4898 time: 0.44s
Epoch 10/1000, LR 0.000240
Train loss: 0.6927;  Loss pred: 0.6927; Loss self: 0.0000; time: 0.52s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6921 score: 0.5102 time: 0.52s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6927 score: 0.4898 time: 0.55s
Epoch 11/1000, LR 0.000270
Train loss: 0.6924;  Loss pred: 0.6924; Loss self: 0.0000; time: 0.72s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6919 score: 0.5102 time: 0.67s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6923 score: 0.4898 time: 0.48s
Epoch 12/1000, LR 0.000270
Train loss: 0.6921;  Loss pred: 0.6921; Loss self: 0.0000; time: 0.52s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6917 score: 0.5102 time: 0.47s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6919 score: 0.4898 time: 0.42s
Epoch 13/1000, LR 0.000270
Train loss: 0.6917;  Loss pred: 0.6917; Loss self: 0.0000; time: 0.52s
Val loss: 0.6915 score: 0.6122 time: 0.53s
Test loss: 0.6915 score: 0.5102 time: 0.44s
Epoch 14/1000, LR 0.000270
Train loss: 0.6914;  Loss pred: 0.6914; Loss self: 0.0000; time: 0.56s
Val loss: 0.6912 score: 0.6939 time: 0.45s
Test loss: 0.6911 score: 0.6122 time: 0.44s
Epoch 15/1000, LR 0.000270
Train loss: 0.6910;  Loss pred: 0.6910; Loss self: 0.0000; time: 0.64s
Val loss: 0.6910 score: 0.6122 time: 0.44s
Test loss: 0.6906 score: 0.7347 time: 0.53s
Epoch 16/1000, LR 0.000270
Train loss: 0.6906;  Loss pred: 0.6906; Loss self: 0.0000; time: 0.52s
Val loss: 0.6907 score: 0.6122 time: 0.50s
Test loss: 0.6902 score: 0.7347 time: 0.55s
Epoch 17/1000, LR 0.000270
Train loss: 0.6902;  Loss pred: 0.6902; Loss self: 0.0000; time: 0.74s
Val loss: 0.6904 score: 0.5510 time: 0.77s
Test loss: 0.6896 score: 0.6531 time: 0.58s
Epoch 18/1000, LR 0.000270
Train loss: 0.6898;  Loss pred: 0.6898; Loss self: 0.0000; time: 0.68s
Val loss: 0.6900 score: 0.5102 time: 0.56s
Test loss: 0.6891 score: 0.6531 time: 0.55s
Epoch 19/1000, LR 0.000270
Train loss: 0.6893;  Loss pred: 0.6893; Loss self: 0.0000; time: 0.65s
Val loss: 0.6896 score: 0.5102 time: 0.71s
Test loss: 0.6885 score: 0.6122 time: 0.49s
Epoch 20/1000, LR 0.000270
Train loss: 0.6886;  Loss pred: 0.6886; Loss self: 0.0000; time: 0.60s
Val loss: 0.6891 score: 0.5510 time: 0.55s
Test loss: 0.6878 score: 0.5714 time: 0.49s
Epoch 21/1000, LR 0.000270
Train loss: 0.6881;  Loss pred: 0.6881; Loss self: 0.0000; time: 0.56s
Val loss: 0.6886 score: 0.5510 time: 0.76s
Test loss: 0.6871 score: 0.5714 time: 0.55s
Epoch 22/1000, LR 0.000270
Train loss: 0.6874;  Loss pred: 0.6874; Loss self: 0.0000; time: 0.64s
Val loss: 0.6879 score: 0.5510 time: 0.55s
Test loss: 0.6862 score: 0.5714 time: 0.56s
Epoch 23/1000, LR 0.000270
Train loss: 0.6865;  Loss pred: 0.6865; Loss self: 0.0000; time: 0.65s
Val loss: 0.6872 score: 0.5510 time: 0.45s
Test loss: 0.6853 score: 0.5714 time: 0.53s
Epoch 24/1000, LR 0.000270
Train loss: 0.6857;  Loss pred: 0.6857; Loss self: 0.0000; time: 0.52s
Val loss: 0.6864 score: 0.5510 time: 0.47s
Test loss: 0.6844 score: 0.5714 time: 0.46s
Epoch 25/1000, LR 0.000270
Train loss: 0.6846;  Loss pred: 0.6846; Loss self: 0.0000; time: 0.54s
Val loss: 0.6856 score: 0.5510 time: 0.63s
Test loss: 0.6833 score: 0.5714 time: 0.55s
Epoch 26/1000, LR 0.000270
Train loss: 0.6832;  Loss pred: 0.6832; Loss self: 0.0000; time: 0.67s
Val loss: 0.6846 score: 0.5510 time: 0.45s
Test loss: 0.6821 score: 0.5714 time: 0.43s
Epoch 27/1000, LR 0.000270
Train loss: 0.6822;  Loss pred: 0.6822; Loss self: 0.0000; time: 0.52s
Val loss: 0.6836 score: 0.5102 time: 0.54s
Test loss: 0.6808 score: 0.5714 time: 0.46s
Epoch 28/1000, LR 0.000270
Train loss: 0.6812;  Loss pred: 0.6812; Loss self: 0.0000; time: 0.54s
Val loss: 0.6825 score: 0.5102 time: 0.45s
Test loss: 0.6793 score: 0.6327 time: 0.54s
Epoch 29/1000, LR 0.000270
Train loss: 0.6790;  Loss pred: 0.6790; Loss self: 0.0000; time: 0.52s
Val loss: 0.6813 score: 0.5102 time: 0.54s
Test loss: 0.6778 score: 0.6531 time: 0.49s
Epoch 30/1000, LR 0.000270
Train loss: 0.6773;  Loss pred: 0.6773; Loss self: 0.0000; time: 0.64s
Val loss: 0.6800 score: 0.5102 time: 0.54s
Test loss: 0.6760 score: 0.6531 time: 0.53s
Epoch 31/1000, LR 0.000270
Train loss: 0.6755;  Loss pred: 0.6755; Loss self: 0.0000; time: 0.64s
Val loss: 0.6784 score: 0.5102 time: 0.50s
Test loss: 0.6741 score: 0.6531 time: 0.54s
Epoch 32/1000, LR 0.000270
Train loss: 0.6742;  Loss pred: 0.6742; Loss self: 0.0000; time: 0.63s
Val loss: 0.6768 score: 0.5102 time: 0.55s
Test loss: 0.6720 score: 0.6531 time: 0.55s
Epoch 33/1000, LR 0.000270
Train loss: 0.6722;  Loss pred: 0.6722; Loss self: 0.0000; time: 0.65s
Val loss: 0.6749 score: 0.5306 time: 0.63s
Test loss: 0.6697 score: 0.6531 time: 0.48s
Epoch 34/1000, LR 0.000270
Train loss: 0.6695;  Loss pred: 0.6695; Loss self: 0.0000; time: 0.62s
Val loss: 0.6729 score: 0.5306 time: 0.45s
Test loss: 0.6672 score: 0.6531 time: 0.43s
Epoch 35/1000, LR 0.000270
Train loss: 0.6664;  Loss pred: 0.6664; Loss self: 0.0000; time: 0.54s
Val loss: 0.6707 score: 0.5510 time: 0.57s
Test loss: 0.6644 score: 0.6735 time: 0.49s
Epoch 36/1000, LR 0.000270
Train loss: 0.6643;  Loss pred: 0.6643; Loss self: 0.0000; time: 0.64s
Val loss: 0.6683 score: 0.5510 time: 0.54s
Test loss: 0.6614 score: 0.6735 time: 0.54s
Epoch 37/1000, LR 0.000270
Train loss: 0.6591;  Loss pred: 0.6591; Loss self: 0.0000; time: 0.64s
Val loss: 0.6657 score: 0.5714 time: 0.66s
Test loss: 0.6580 score: 0.6735 time: 0.55s
Epoch 38/1000, LR 0.000270
Train loss: 0.6557;  Loss pred: 0.6557; Loss self: 0.0000; time: 0.53s
Val loss: 0.6629 score: 0.5714 time: 0.44s
Test loss: 0.6544 score: 0.6735 time: 0.52s
Epoch 39/1000, LR 0.000269
Train loss: 0.6525;  Loss pred: 0.6525; Loss self: 0.0000; time: 0.62s
Val loss: 0.6599 score: 0.5714 time: 0.45s
Test loss: 0.6505 score: 0.6735 time: 0.45s
Epoch 40/1000, LR 0.000269
Train loss: 0.6483;  Loss pred: 0.6483; Loss self: 0.0000; time: 0.52s
Val loss: 0.6566 score: 0.5714 time: 0.45s
Test loss: 0.6463 score: 0.6735 time: 0.43s
Epoch 41/1000, LR 0.000269
Train loss: 0.6432;  Loss pred: 0.6432; Loss self: 0.0000; time: 0.53s
Val loss: 0.6530 score: 0.6122 time: 0.59s
Test loss: 0.6418 score: 0.6939 time: 0.57s
Epoch 42/1000, LR 0.000269
Train loss: 0.6393;  Loss pred: 0.6393; Loss self: 0.0000; time: 0.55s
Val loss: 0.6492 score: 0.6327 time: 0.42s
Test loss: 0.6369 score: 0.6939 time: 0.44s
Epoch 43/1000, LR 0.000269
Train loss: 0.6333;  Loss pred: 0.6333; Loss self: 0.0000; time: 0.62s
Val loss: 0.6451 score: 0.6327 time: 0.59s
Test loss: 0.6317 score: 0.7347 time: 0.49s
Epoch 44/1000, LR 0.000269
Train loss: 0.6281;  Loss pred: 0.6281; Loss self: 0.0000; time: 0.51s
Val loss: 0.6408 score: 0.6531 time: 0.43s
Test loss: 0.6262 score: 0.7551 time: 0.44s
Epoch 45/1000, LR 0.000269
Train loss: 0.6201;  Loss pred: 0.6201; Loss self: 0.0000; time: 0.63s
Val loss: 0.6362 score: 0.6531 time: 0.67s
Test loss: 0.6203 score: 0.7347 time: 0.55s
Epoch 46/1000, LR 0.000269
Train loss: 0.6174;  Loss pred: 0.6174; Loss self: 0.0000; time: 0.63s
Val loss: 0.6312 score: 0.6939 time: 0.54s
Test loss: 0.6140 score: 0.7347 time: 0.55s
Epoch 47/1000, LR 0.000269
Train loss: 0.6104;  Loss pred: 0.6104; Loss self: 0.0000; time: 0.70s
Val loss: 0.6259 score: 0.6939 time: 0.43s
Test loss: 0.6072 score: 0.7347 time: 0.45s
Epoch 48/1000, LR 0.000269
Train loss: 0.5985;  Loss pred: 0.5985; Loss self: 0.0000; time: 0.54s
Val loss: 0.6203 score: 0.7143 time: 0.49s
Test loss: 0.5999 score: 0.7755 time: 0.44s
Epoch 49/1000, LR 0.000269
Train loss: 0.5911;  Loss pred: 0.5911; Loss self: 0.0000; time: 0.61s
Val loss: 0.6144 score: 0.7143 time: 0.63s
Test loss: 0.5921 score: 0.8367 time: 0.56s
Epoch 50/1000, LR 0.000269
Train loss: 0.5822;  Loss pred: 0.5822; Loss self: 0.0000; time: 0.67s
Val loss: 0.6080 score: 0.7347 time: 0.54s
Test loss: 0.5838 score: 0.8367 time: 0.44s
Epoch 51/1000, LR 0.000269
Train loss: 0.5736;  Loss pred: 0.5736; Loss self: 0.0000; time: 0.52s
Val loss: 0.6014 score: 0.7347 time: 0.64s
Test loss: 0.5751 score: 0.8571 time: 0.61s
Epoch 52/1000, LR 0.000269
Train loss: 0.5619;  Loss pred: 0.5619; Loss self: 0.0000; time: 0.65s
Val loss: 0.5944 score: 0.7347 time: 0.57s
Test loss: 0.5659 score: 0.9184 time: 0.52s
Epoch 53/1000, LR 0.000269
Train loss: 0.5528;  Loss pred: 0.5528; Loss self: 0.0000; time: 0.51s
Val loss: 0.5871 score: 0.7347 time: 0.53s
Test loss: 0.5561 score: 0.9184 time: 0.43s
Epoch 54/1000, LR 0.000269
Train loss: 0.5423;  Loss pred: 0.5423; Loss self: 0.0000; time: 0.58s
Val loss: 0.5794 score: 0.7347 time: 0.56s
Test loss: 0.5459 score: 0.9184 time: 0.50s
Epoch 55/1000, LR 0.000269
Train loss: 0.5272;  Loss pred: 0.5272; Loss self: 0.0000; time: 0.64s
Val loss: 0.5715 score: 0.7347 time: 0.54s
Test loss: 0.5351 score: 0.9388 time: 0.43s
Epoch 56/1000, LR 0.000269
Train loss: 0.5249;  Loss pred: 0.5249; Loss self: 0.0000; time: 0.54s
Val loss: 0.5633 score: 0.7755 time: 0.57s
Test loss: 0.5240 score: 0.9388 time: 0.51s
Epoch 57/1000, LR 0.000269
Train loss: 0.5019;  Loss pred: 0.5019; Loss self: 0.0000; time: 0.64s
Val loss: 0.5550 score: 0.7959 time: 0.67s
Test loss: 0.5124 score: 0.9388 time: 0.50s
Epoch 58/1000, LR 0.000269
Train loss: 0.4925;  Loss pred: 0.4925; Loss self: 0.0000; time: 0.61s
Val loss: 0.5465 score: 0.8163 time: 3.58s
Test loss: 0.5004 score: 0.9388 time: 2.25s
Epoch 59/1000, LR 0.000268
Train loss: 0.4746;  Loss pred: 0.4746; Loss self: 0.0000; time: 0.51s
Val loss: 0.5378 score: 0.8163 time: 0.53s
Test loss: 0.4880 score: 0.9388 time: 0.42s
Epoch 60/1000, LR 0.000268
Train loss: 0.4682;  Loss pred: 0.4682; Loss self: 0.0000; time: 0.53s
Val loss: 0.5292 score: 0.8367 time: 0.42s
Test loss: 0.4755 score: 0.9388 time: 0.43s
Epoch 61/1000, LR 0.000268
Train loss: 0.4444;  Loss pred: 0.4444; Loss self: 0.0000; time: 0.50s
Val loss: 0.5206 score: 0.8367 time: 0.53s
Test loss: 0.4626 score: 0.9388 time: 0.46s
Epoch 62/1000, LR 0.000268
Train loss: 0.4316;  Loss pred: 0.4316; Loss self: 0.0000; time: 0.51s
Val loss: 0.5122 score: 0.8367 time: 0.42s
Test loss: 0.4497 score: 0.9388 time: 0.42s
Epoch 63/1000, LR 0.000268
Train loss: 0.4151;  Loss pred: 0.4151; Loss self: 0.0000; time: 0.60s
Val loss: 0.5041 score: 0.8367 time: 0.43s
Test loss: 0.4365 score: 0.9388 time: 0.49s
Epoch 64/1000, LR 0.000268
Train loss: 0.4101;  Loss pred: 0.4101; Loss self: 0.0000; time: 0.50s
Val loss: 0.4963 score: 0.8367 time: 0.43s
Test loss: 0.4235 score: 0.9388 time: 0.42s
Epoch 65/1000, LR 0.000268
Train loss: 0.3942;  Loss pred: 0.3942; Loss self: 0.0000; time: 0.51s
Val loss: 0.4887 score: 0.8367 time: 0.52s
Test loss: 0.4109 score: 0.9388 time: 0.42s
Epoch 66/1000, LR 0.000268
Train loss: 0.3849;  Loss pred: 0.3849; Loss self: 0.0000; time: 0.51s
Val loss: 0.4813 score: 0.8367 time: 0.42s
Test loss: 0.3986 score: 0.9388 time: 0.43s
Epoch 67/1000, LR 0.000268
Train loss: 0.3624;  Loss pred: 0.3624; Loss self: 0.0000; time: 0.53s
Val loss: 0.4738 score: 0.8367 time: 0.55s
Test loss: 0.3868 score: 0.9592 time: 0.44s
Epoch 68/1000, LR 0.000268
Train loss: 0.3623;  Loss pred: 0.3623; Loss self: 0.0000; time: 0.52s
Val loss: 0.4668 score: 0.8367 time: 0.42s
Test loss: 0.3754 score: 0.9592 time: 0.42s
Epoch 69/1000, LR 0.000268
Train loss: 0.3376;  Loss pred: 0.3376; Loss self: 0.0000; time: 0.53s
Val loss: 0.4599 score: 0.8367 time: 0.52s
Test loss: 0.3644 score: 0.9592 time: 0.43s
Epoch 70/1000, LR 0.000268
Train loss: 0.3230;  Loss pred: 0.3230; Loss self: 0.0000; time: 0.52s
Val loss: 0.4532 score: 0.8367 time: 0.44s
Test loss: 0.3538 score: 0.9592 time: 0.43s
Epoch 71/1000, LR 0.000268
Train loss: 0.3125;  Loss pred: 0.3125; Loss self: 0.0000; time: 0.65s
Val loss: 0.4467 score: 0.8367 time: 0.44s
Test loss: 0.3436 score: 0.9592 time: 0.44s
Epoch 72/1000, LR 0.000267
Train loss: 0.3113;  Loss pred: 0.3113; Loss self: 0.0000; time: 0.52s
Val loss: 0.4404 score: 0.8367 time: 0.49s
Test loss: 0.3338 score: 0.9592 time: 0.44s
Epoch 73/1000, LR 0.000267
Train loss: 0.2951;  Loss pred: 0.2951; Loss self: 0.0000; time: 0.64s
Val loss: 0.4346 score: 0.8571 time: 0.66s
Test loss: 0.3244 score: 0.9592 time: 0.58s
Epoch 74/1000, LR 0.000267
Train loss: 0.2848;  Loss pred: 0.2848; Loss self: 0.0000; time: 0.60s
Val loss: 0.4290 score: 0.8571 time: 0.43s
Test loss: 0.3153 score: 0.9592 time: 0.43s
Epoch 75/1000, LR 0.000267
Train loss: 0.2634;  Loss pred: 0.2634; Loss self: 0.0000; time: 0.57s
Val loss: 0.4234 score: 0.8571 time: 0.68s
Test loss: 0.3066 score: 0.9592 time: 0.45s
Epoch 76/1000, LR 0.000267
Train loss: 0.2509;  Loss pred: 0.2509; Loss self: 0.0000; time: 0.55s
Val loss: 0.4182 score: 0.8571 time: 0.44s
Test loss: 0.2982 score: 0.9592 time: 0.43s
Epoch 77/1000, LR 0.000267
Train loss: 0.2440;  Loss pred: 0.2440; Loss self: 0.0000; time: 0.56s
Val loss: 0.4135 score: 0.8571 time: 0.65s
Test loss: 0.2900 score: 0.9592 time: 0.47s
Epoch 78/1000, LR 0.000267
Train loss: 0.2297;  Loss pred: 0.2297; Loss self: 0.0000; time: 0.54s
Val loss: 0.4092 score: 0.8367 time: 0.42s
Test loss: 0.2820 score: 0.9592 time: 0.47s
Epoch 79/1000, LR 0.000267
Train loss: 0.2202;  Loss pred: 0.2202; Loss self: 0.0000; time: 0.62s
Val loss: 0.4052 score: 0.8367 time: 0.44s
Test loss: 0.2741 score: 0.9592 time: 0.54s
Epoch 80/1000, LR 0.000267
Train loss: 0.2171;  Loss pred: 0.2171; Loss self: 0.0000; time: 0.67s
Val loss: 0.4013 score: 0.8367 time: 0.47s
Test loss: 0.2667 score: 0.9592 time: 0.49s
Epoch 81/1000, LR 0.000267
Train loss: 0.2008;  Loss pred: 0.2008; Loss self: 0.0000; time: 0.63s
Val loss: 0.3976 score: 0.8367 time: 0.56s
Test loss: 0.2595 score: 0.9592 time: 0.44s
Epoch 82/1000, LR 0.000267
Train loss: 0.1903;  Loss pred: 0.1903; Loss self: 0.0000; time: 0.73s
Val loss: 0.3942 score: 0.8367 time: 0.54s
Test loss: 0.2524 score: 0.9592 time: 0.57s
Epoch 83/1000, LR 0.000266
Train loss: 0.1794;  Loss pred: 0.1794; Loss self: 0.0000; time: 0.66s
Val loss: 0.3910 score: 0.8367 time: 0.60s
Test loss: 0.2456 score: 0.9592 time: 0.43s
Epoch 84/1000, LR 0.000266
Train loss: 0.1719;  Loss pred: 0.1719; Loss self: 0.0000; time: 0.52s
Val loss: 0.3878 score: 0.8367 time: 0.69s
Test loss: 0.2391 score: 0.9592 time: 0.52s
Epoch 85/1000, LR 0.000266
Train loss: 0.1578;  Loss pred: 0.1578; Loss self: 0.0000; time: 0.52s
Val loss: 0.3849 score: 0.8367 time: 0.60s
Test loss: 0.2329 score: 0.9592 time: 0.55s
Epoch 86/1000, LR 0.000266
Train loss: 0.1458;  Loss pred: 0.1458; Loss self: 0.0000; time: 0.65s
Val loss: 0.3821 score: 0.8163 time: 0.67s
Test loss: 0.2269 score: 0.9592 time: 0.55s
Epoch 87/1000, LR 0.000266
Train loss: 0.1405;  Loss pred: 0.1405; Loss self: 0.0000; time: 0.79s
Val loss: 0.3796 score: 0.8163 time: 0.42s
Test loss: 0.2213 score: 0.9592 time: 0.47s
Epoch 88/1000, LR 0.000266
Train loss: 0.1305;  Loss pred: 0.1305; Loss self: 0.0000; time: 0.51s
Val loss: 0.3775 score: 0.8163 time: 0.42s
Test loss: 0.2160 score: 0.9388 time: 0.44s
Epoch 89/1000, LR 0.000266
Train loss: 0.1255;  Loss pred: 0.1255; Loss self: 0.0000; time: 0.65s
Val loss: 0.3758 score: 0.8163 time: 0.68s
Test loss: 0.2112 score: 0.9388 time: 0.43s
Epoch 90/1000, LR 0.000266
Train loss: 0.1121;  Loss pred: 0.1121; Loss self: 0.0000; time: 0.55s
Val loss: 0.3745 score: 0.8163 time: 0.52s
Test loss: 0.2067 score: 0.9388 time: 0.57s
Epoch 91/1000, LR 0.000266
Train loss: 0.1051;  Loss pred: 0.1051; Loss self: 0.0000; time: 0.67s
Val loss: 0.3739 score: 0.8367 time: 0.66s
Test loss: 0.2028 score: 0.9388 time: 0.50s
Epoch 92/1000, LR 0.000266
Train loss: 0.0934;  Loss pred: 0.0934; Loss self: 0.0000; time: 0.53s
Val loss: 0.3739 score: 0.8571 time: 0.47s
Test loss: 0.1994 score: 0.9388 time: 0.55s
Epoch 93/1000, LR 0.000265
Train loss: 0.0889;  Loss pred: 0.0889; Loss self: 0.0000; time: 0.65s
Val loss: 0.3745 score: 0.8571 time: 0.70s
Test loss: 0.1967 score: 0.9388 time: 0.55s
     INFO: Early stopping counter 1 of 20
Epoch 94/1000, LR 0.000265
Train loss: 0.0796;  Loss pred: 0.0796; Loss self: 0.0000; time: 0.67s
Val loss: 0.3756 score: 0.8571 time: 0.54s
Test loss: 0.1948 score: 0.9388 time: 0.51s
     INFO: Early stopping counter 2 of 20
Epoch 95/1000, LR 0.000265
Train loss: 0.0689;  Loss pred: 0.0689; Loss self: 0.0000; time: 0.69s
Val loss: 0.3776 score: 0.8571 time: 0.55s
Test loss: 0.1934 score: 0.9388 time: 0.56s
     INFO: Early stopping counter 3 of 20
Epoch 96/1000, LR 0.000265
Train loss: 0.0670;  Loss pred: 0.0670; Loss self: 0.0000; time: 0.65s
Val loss: 0.3804 score: 0.8571 time: 0.56s
Test loss: 0.1928 score: 0.9388 time: 0.54s
     INFO: Early stopping counter 4 of 20
Epoch 97/1000, LR 0.000265
Train loss: 0.0635;  Loss pred: 0.0635; Loss self: 0.0000; time: 0.62s
Val loss: 0.3839 score: 0.8571 time: 0.54s
Test loss: 0.1928 score: 0.9388 time: 0.42s
     INFO: Early stopping counter 5 of 20
Epoch 98/1000, LR 0.000265
Train loss: 0.0582;  Loss pred: 0.0582; Loss self: 0.0000; time: 0.58s
Val loss: 0.3882 score: 0.8367 time: 0.57s
Test loss: 0.1934 score: 0.9388 time: 0.57s
     INFO: Early stopping counter 6 of 20
Epoch 99/1000, LR 0.000265
Train loss: 0.0647;  Loss pred: 0.0647; Loss self: 0.0000; time: 0.64s
Val loss: 0.3941 score: 0.8367 time: 0.54s
Test loss: 0.1953 score: 0.9388 time: 0.52s
     INFO: Early stopping counter 7 of 20
Epoch 100/1000, LR 0.000265
Train loss: 0.0501;  Loss pred: 0.0501; Loss self: 0.0000; time: 0.65s
Val loss: 0.4008 score: 0.8367 time: 0.54s
Test loss: 0.1975 score: 0.9388 time: 0.55s
     INFO: Early stopping counter 8 of 20
Epoch 101/1000, LR 0.000265
Train loss: 0.0399;  Loss pred: 0.0399; Loss self: 0.0000; time: 0.63s
Val loss: 0.4083 score: 0.8367 time: 0.66s
Test loss: 0.2001 score: 0.9388 time: 0.47s
     INFO: Early stopping counter 9 of 20
Epoch 102/1000, LR 0.000264
Train loss: 0.0385;  Loss pred: 0.0385; Loss self: 0.0000; time: 0.52s
Val loss: 0.4170 score: 0.8367 time: 0.43s
Test loss: 0.2033 score: 0.9388 time: 0.43s
     INFO: Early stopping counter 10 of 20
Epoch 103/1000, LR 0.000264
Train loss: 0.0320;  Loss pred: 0.0320; Loss self: 0.0000; time: 0.65s
Val loss: 0.4261 score: 0.8367 time: 0.43s
Test loss: 0.2067 score: 0.9388 time: 0.44s
     INFO: Early stopping counter 11 of 20
Epoch 104/1000, LR 0.000264
Train loss: 0.0335;  Loss pred: 0.0335; Loss self: 0.0000; time: 0.51s
Val loss: 0.4351 score: 0.8367 time: 0.42s
Test loss: 0.2101 score: 0.9388 time: 0.43s
     INFO: Early stopping counter 12 of 20
Epoch 105/1000, LR 0.000264
Train loss: 0.0273;  Loss pred: 0.0273; Loss self: 0.0000; time: 0.54s
Val loss: 0.4445 score: 0.8367 time: 0.54s
Test loss: 0.2137 score: 0.9388 time: 0.43s
     INFO: Early stopping counter 13 of 20
Epoch 106/1000, LR 0.000264
Train loss: 0.0259;  Loss pred: 0.0259; Loss self: 0.0000; time: 0.51s
Val loss: 0.4543 score: 0.8367 time: 0.50s
Test loss: 0.2175 score: 0.9388 time: 0.54s
     INFO: Early stopping counter 14 of 20
Epoch 107/1000, LR 0.000264
Train loss: 0.0242;  Loss pred: 0.0242; Loss self: 0.0000; time: 0.56s
Val loss: 0.4641 score: 0.8367 time: 0.52s
Test loss: 0.2215 score: 0.9388 time: 0.45s
     INFO: Early stopping counter 15 of 20
Epoch 108/1000, LR 0.000264
Train loss: 0.0239;  Loss pred: 0.0239; Loss self: 0.0000; time: 0.50s
Val loss: 0.4738 score: 0.8367 time: 0.42s
Test loss: 0.2254 score: 0.9388 time: 0.56s
     INFO: Early stopping counter 16 of 20
Epoch 109/1000, LR 0.000264
Train loss: 0.0205;  Loss pred: 0.0205; Loss self: 0.0000; time: 0.64s
Val loss: 0.4833 score: 0.8367 time: 0.54s
Test loss: 0.2294 score: 0.9388 time: 0.44s
     INFO: Early stopping counter 17 of 20
Epoch 110/1000, LR 0.000263
Train loss: 0.0181;  Loss pred: 0.0181; Loss self: 0.0000; time: 0.52s
Val loss: 0.4928 score: 0.8367 time: 0.56s
Test loss: 0.2334 score: 0.9388 time: 0.43s
     INFO: Early stopping counter 18 of 20
Epoch 111/1000, LR 0.000263
Train loss: 0.0192;  Loss pred: 0.0192; Loss self: 0.0000; time: 0.61s
Val loss: 0.5017 score: 0.8367 time: 0.44s
Test loss: 0.2373 score: 0.9388 time: 0.45s
     INFO: Early stopping counter 19 of 20
Epoch 112/1000, LR 0.000263
Train loss: 0.0187;  Loss pred: 0.0187; Loss self: 0.0000; time: 0.57s
Val loss: 0.5098 score: 0.8367 time: 0.49s
Test loss: 0.2410 score: 0.9388 time: 0.55s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 091,   Train_Loss: 0.0934,   Val_Loss: 0.3739,   Val_Precision: 0.9048,   Val_Recall: 0.7917,   Val_accuracy: 0.8444,   Val_Score: 0.8571,   Val_Loss: 0.3739,   Test_Precision: 0.9583,   Test_Recall: 0.9200,   Test_accuracy: 0.9388,   Test_Score: 0.9388,   Test_loss: 0.1994


[0.5557976951822639, 0.43661666894331574, 0.4353777680080384, 0.4317563269287348, 0.4438797659240663, 0.4320737069938332, 0.4727512230165303, 0.5485460001509637, 0.4482768240850419, 0.5608840240165591, 0.48588921898044646, 0.4267306460533291, 0.4454654648434371, 0.4404641829896718, 0.537623907905072, 0.5616491201799363, 0.582398243015632, 0.5550781718920916, 0.49320246395654976, 0.49269940704107285, 0.5611588661558926, 0.5706078989896923, 0.5410676030442119, 0.4632785180583596, 0.5534162679687142, 0.43911505909636617, 0.4633998649660498, 0.5480853549670428, 0.5008167149499059, 0.5400644200854003, 0.5459566009230912, 0.5562938149087131, 0.48659669305197895, 0.4323171761352569, 0.4993087421171367, 0.5483128689229488, 0.5571511599700898, 0.5228511129971594, 0.4557801310438663, 0.4305635979399085, 0.5801630278583616, 0.44886294286698103, 0.4920732250902802, 0.44439376913942397, 0.5563390129245818, 0.5530433000531048, 0.45926043996587396, 0.4479618698824197, 0.5708832328673452, 0.44464431912638247, 0.6204402258154005, 0.5249992969911546, 0.4398067800793797, 0.5094195900019258, 0.4375690561719239, 0.5176349140238017, 0.5065021889749914, 2.2598491290118545, 0.4280350140761584, 0.434902855893597, 0.4613353000022471, 0.4287063810043037, 0.49012007378041744, 0.42398875020444393, 0.42902415501885116, 0.43311849096789956, 0.4422965890262276, 0.4257883110549301, 0.4370779939927161, 0.43648961186408997, 0.44039573799818754, 0.44880396709777415, 0.5922640089411288, 0.43510395591147244, 0.450994161888957, 0.43270232900977135, 0.479936117073521, 0.478334496030584, 0.5486893369816244, 0.4990654019638896, 0.4422760361339897, 0.5806497561279684, 0.4370005859527737, 0.5204990520142019, 0.5533135170117021, 0.5520911908242851, 0.4769732530694455, 0.44074963801540434, 0.4388426698278636, 0.5728680200409144, 0.5015784751158208, 0.5607310289051384, 0.5564900110475719, 0.5168376499786973, 0.562678060028702, 0.5504499061498791, 0.42627913504838943, 0.5746080910321325, 0.5293126830365509, 0.5534459128975868, 0.4745249031111598, 0.4360520930495113, 0.444812930887565, 0.4381598262116313, 0.4385390488896519, 0.5460027479566634, 0.4506459750700742, 0.5650667939335108, 0.4491341558750719, 0.4365639640018344, 0.4555418479721993, 0.5529362678062171]
[0.011342810105760487, 0.0089105442641493, 0.008885260571592621, 0.008811353610790506, 0.009058770733144211, 0.008817830754976188, 0.009647984143194496, 0.011194816329611503, 0.009148506613980447, 0.011446612735031819, 0.009916106509805029, 0.0087087886949659, 0.009091131935580348, 0.008989064958972894, 0.010971916487858611, 0.01146222694244768, 0.01188567842889045, 0.011328125956981462, 0.010065356407276526, 0.010055089939613732, 0.011452221758283523, 0.011645059163054946, 0.01104219598049412, 0.009454663633844075, 0.011294209550381924, 0.008961531818293187, 0.009457140101347955, 0.01118541540749067, 0.010220749284691957, 0.01102172285888572, 0.011141971447410024, 0.011352934998137002, 0.009930544756162835, 0.008822799512964427, 0.010189974328921157, 0.011190058549447936, 0.011370431836124281, 0.010670430877493049, 0.009301635327425842, 0.008787012202855276, 0.011840061793027788, 0.009160468221775122, 0.010042310716128168, 0.009069260594682122, 0.011353857406624118, 0.011286597960267444, 0.009372662040119877, 0.009142078977192238, 0.011650678221782555, 0.00907437385972209, 0.012662045424804091, 0.01071427136716642, 0.008975648573048564, 0.01039631816330461, 0.008929980738202527, 0.010563977837220443, 0.01033677936683656, 0.04611936997983376, 0.008735408450533845, 0.008875568487624429, 0.009415006122494839, 0.008749109816414361, 0.010002450485314642, 0.008652831636825386, 0.008755595000384718, 0.00883915287689591, 0.009026461000535257, 0.008689557368467962, 0.008919959061075839, 0.008907951262532448, 0.008987668122411991, 0.009159264634648452, 0.012087020590635282, 0.008879672569621886, 0.009203962487529735, 0.00883065977570962, 0.00979461463415349, 0.00976192849042008, 0.01119774157105356, 0.010185008203344685, 0.009026041553754891, 0.011849995023019763, 0.008918379305158647, 0.010622429632942895, 0.011292112592075552, 0.011267167159679289, 0.009734148021825418, 0.008994890571742947, 0.00895597285362987, 0.01169118408246764, 0.010236295410526954, 0.01144349038581915, 0.011356939000970855, 0.010547707142422393, 0.011483225714871469, 0.011233671554079165, 0.008699574184661009, 0.011726695735349643, 0.01080229965380716, 0.011294814548930344, 0.009684181696146118, 0.008899022307132885, 0.009077814916072756, 0.008942037269625128, 0.00894977650795208, 0.011142913223605375, 0.009196856634083147, 0.011531975386398179, 0.009166003181123915, 0.00890946865309866, 0.009296772407595905, 0.011284413628698307]
[88.161574660599, 112.22659024582839, 112.54593964268591, 113.48994083898599, 110.3902537616061, 113.40657671793798, 103.6485948938236, 89.3270573233874, 109.30745772996796, 87.36208895576131, 100.84603256441443, 114.82653156782277, 109.99730364557328, 111.24627584338445, 91.14178011713703, 87.24308156007046, 84.13486920269573, 88.27585461156578, 99.35067965174807, 99.45211887765723, 87.31930110213668, 85.87332928050678, 90.56169640228136, 105.76790869855836, 88.54094618478051, 111.58806555355845, 105.7402120813952, 89.40213336469544, 97.8401849165542, 90.72991698333253, 89.75072362373108, 88.08294948963403, 100.6994102090327, 113.34270925351719, 98.13567411664647, 89.36503733033061, 87.94740731156429, 93.71692778679466, 107.50797733937205, 113.80432585208625, 84.45901866735748, 109.1647256220948, 99.5786754928802, 110.2625720763127, 88.07579346703575, 88.60065748069796, 106.69327409005884, 109.38430990312064, 85.83191304093882, 110.20044087434432, 78.97618168712833, 93.333458312851, 111.41256165072332, 96.18789885919927, 111.98232440994997, 94.66131180971092, 96.74193136097064, 21.68286341372967, 114.47661613795371, 112.66883934187892, 106.213420043429, 114.29734235634832, 99.97550115026074, 115.56910407734308, 114.21268342768943, 113.13301330196872, 110.78538974917205, 115.08066033705329, 112.10813784602614, 112.25925810866069, 111.26356540762359, 109.17907057921596, 82.73337440781516, 112.61676510697983, 108.64885655008699, 113.24182172103232, 102.09692135441794, 102.4387753896533, 89.30372197417245, 98.18352425789966, 110.79053802760231, 84.38822109692056, 112.12799610592603, 94.14042121764139, 88.55738834040393, 88.7534538032419, 102.73112734240841, 111.17422630370359, 111.65732817007128, 85.53453550522929, 97.69159250441376, 87.38592564723143, 88.05189496170706, 94.80733457018786, 87.08354471382894, 89.01809129686372, 114.94815479166655, 85.27551345819786, 92.57288096498603, 88.53620355322286, 103.26117697667283, 112.37189496631156, 110.15866805451681, 111.83133886020164, 111.73463372090659, 89.74313807645738, 108.73280293334615, 86.71541227702302, 109.09880568876063, 112.24013899551753, 107.56421219723013, 88.61780796981944]
Elapsed: 0.5083033739647362~0.17484958798091968
Time per graph: 0.010373538244178292~0.003568358938386116
Speed: 99.97419747527827~13.085780478873826
Total Time: 0.5537
best val loss: 0.37389275431632996 test_score: 0.9388

Testing...
Test loss: 0.3244 score: 0.9592 time: 0.56s
test Score 0.9592
Epoch Time List: [1.6028206122573465, 1.4421065340284258, 1.5036961131263524, 1.4041564629878849, 1.481176831992343, 1.4181753173470497, 1.5599925923161209, 1.711775926174596, 1.6711710870731622, 1.5867823497392237, 1.8755753978621215, 1.41058943211101, 1.4897221280261874, 1.443383929086849, 1.61078364495188, 1.5685488330200315, 2.087083663325757, 1.7892308440059423, 1.8472083339001983, 1.6363531311508268, 1.879508753074333, 1.756759799318388, 1.631346508860588, 1.448570615844801, 1.7236217390745878, 1.5495978959370404, 1.518880944000557, 1.5375331128016114, 1.5536270702723414, 1.7132020378485322, 1.6745930849574506, 1.7307669878937304, 1.7611988170538098, 1.4935704660601914, 1.5957276469562203, 1.7189970405306667, 1.842836296884343, 1.4914667068514973, 1.5188193556386977, 1.3973014219664037, 1.691152285085991, 1.4148280690424144, 1.7046428879257292, 1.3771537870634347, 1.8507115901447833, 1.7203573752194643, 1.5826043707784265, 1.47596654901281, 1.8044049739837646, 1.6498118760064244, 1.7736913117114455, 1.7448075069114566, 1.4794687230605632, 1.6423211907967925, 1.6149483770132065, 1.6136173019185662, 1.8117161279078573, 6.446931465761736, 1.4552888758480549, 1.3730466139968485, 1.4801644121762365, 1.3462149628903717, 1.5201655379496515, 1.349505285732448, 1.44922531908378, 1.3516054640058428, 1.5199323750566691, 1.3593243041541427, 1.4834350140299648, 1.3830295228399336, 1.5212320808786899, 1.458814735757187, 1.8807612750679255, 1.4623267049901187, 1.6973867861088365, 1.4261465107556432, 1.6798400881234556, 1.4343095410149544, 1.6069537398871034, 1.6282320590689778, 1.6281786900945008, 1.839561881031841, 1.6992059478070587, 1.730601780116558, 1.6635039430111647, 1.8655502400361001, 1.68389707012102, 1.365640388801694, 1.76356745720841, 1.6321223301347345, 1.8192384410649538, 1.556187563110143, 1.8947374518029392, 1.7223244130145758, 1.795619642129168, 1.7466508860234171, 1.5814802600070834, 1.7184820857364684, 1.6986074249725789, 1.7347999811172485, 1.7606946609448642, 1.3787837231066078, 1.523784255143255, 1.36867905408144, 1.5136261191219091, 1.5559182572178543, 1.5266851398628205, 1.4793970668688416, 1.6243218621239066, 1.5119699200149626, 1.4957748050801456, 1.6066255299374461]
Total Epoch List: [112]
Total Time List: [0.5536897419951856]
========================k_idx:1========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: FAGNN
FAGNN(
  (conv1): NeighborFilterConv()
  (conv2): NeighborFilterConv()
  (ffn): Sequential(
    (0): Linear(in_features=14951, out_features=64, bias=True)
    (1): ReLU()
    (2): Linear(in_features=64, out_features=64, bias=True)
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=64, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=64, out_features=2, bias=True)
  )
)
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7f904e45feb0>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.6946;  Loss pred: 0.6946; Loss self: 0.0000; time: 0.56s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6958 score: 0.4898 time: 0.62s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6935 score: 0.5102 time: 0.48s
Epoch 2/1000, LR 0.000000
Train loss: 0.6946;  Loss pred: 0.6946; Loss self: 0.0000; time: 0.42s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6957 score: 0.4898 time: 0.50s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6935 score: 0.5102 time: 0.75s
Epoch 3/1000, LR 0.000030
Train loss: 0.6946;  Loss pred: 0.6946; Loss self: 0.0000; time: 0.51s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6956 score: 0.4898 time: 0.62s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6935 score: 0.5102 time: 0.59s
Epoch 4/1000, LR 0.000060
Train loss: 0.6946;  Loss pred: 0.6946; Loss self: 0.0000; time: 0.50s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6955 score: 0.4898 time: 0.51s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6934 score: 0.5102 time: 0.47s
Epoch 5/1000, LR 0.000090
Train loss: 0.6945;  Loss pred: 0.6945; Loss self: 0.0000; time: 0.56s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6953 score: 0.4898 time: 0.53s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6933 score: 0.5102 time: 0.49s
Epoch 6/1000, LR 0.000120
Train loss: 0.6944;  Loss pred: 0.6944; Loss self: 0.0000; time: 0.46s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6951 score: 0.4898 time: 0.51s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6932 score: 0.5102 time: 0.59s
Epoch 7/1000, LR 0.000150
Train loss: 0.6942;  Loss pred: 0.6942; Loss self: 0.0000; time: 0.66s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6949 score: 0.4898 time: 0.63s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6931 score: 0.5102 time: 0.60s
Epoch 8/1000, LR 0.000180
Train loss: 0.6941;  Loss pred: 0.6941; Loss self: 0.0000; time: 0.50s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6947 score: 0.4898 time: 0.61s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6930 score: 0.5102 time: 0.71s
Epoch 9/1000, LR 0.000210
Train loss: 0.6939;  Loss pred: 0.6939; Loss self: 0.0000; time: 0.52s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6944 score: 0.4898 time: 0.49s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6928 score: 0.5102 time: 0.47s
Epoch 10/1000, LR 0.000240
Train loss: 0.6937;  Loss pred: 0.6937; Loss self: 0.0000; time: 0.48s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6940 score: 0.4898 time: 0.76s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6926 score: 0.5102 time: 0.63s
Epoch 11/1000, LR 0.000270
Train loss: 0.6935;  Loss pred: 0.6935; Loss self: 0.0000; time: 0.52s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6937 score: 0.4898 time: 0.63s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6925 score: 0.5102 time: 0.63s
Epoch 12/1000, LR 0.000270
Train loss: 0.6933;  Loss pred: 0.6933; Loss self: 0.0000; time: 0.52s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6934 score: 0.4898 time: 0.60s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6923 score: 0.5102 time: 0.47s
Epoch 13/1000, LR 0.000270
Train loss: 0.6930;  Loss pred: 0.6930; Loss self: 0.0000; time: 0.42s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6930 score: 0.4898 time: 0.48s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6921 score: 0.5102 time: 0.52s
Epoch 14/1000, LR 0.000270
Train loss: 0.6928;  Loss pred: 0.6928; Loss self: 0.0000; time: 0.41s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6926 score: 0.4898 time: 0.60s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6919 score: 0.5102 time: 0.48s
Epoch 15/1000, LR 0.000270
Train loss: 0.6925;  Loss pred: 0.6925; Loss self: 0.0000; time: 0.42s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6922 score: 0.4898 time: 0.57s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6917 score: 0.5102 time: 0.57s
Epoch 16/1000, LR 0.000270
Train loss: 0.6922;  Loss pred: 0.6922; Loss self: 0.0000; time: 0.52s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6919 score: 0.4898 time: 0.63s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6915 score: 0.5102 time: 0.76s
Epoch 17/1000, LR 0.000270
Train loss: 0.6920;  Loss pred: 0.6920; Loss self: 0.0000; time: 0.52s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6914 score: 0.4898 time: 0.56s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6913 score: 0.5102 time: 0.46s
Epoch 18/1000, LR 0.000270
Train loss: 0.6917;  Loss pred: 0.6917; Loss self: 0.0000; time: 0.41s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6910 score: 0.4898 time: 0.49s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6911 score: 0.5102 time: 0.46s
Epoch 19/1000, LR 0.000270
Train loss: 0.6914;  Loss pred: 0.6914; Loss self: 0.0000; time: 0.51s
Val loss: 0.6905 score: 0.5102 time: 0.74s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6908 score: 0.5102 time: 0.59s
Epoch 20/1000, LR 0.000270
Train loss: 0.6911;  Loss pred: 0.6911; Loss self: 0.0000; time: 0.54s
Val loss: 0.6900 score: 0.5714 time: 0.64s
Test loss: 0.6906 score: 0.6122 time: 0.47s
Epoch 21/1000, LR 0.000270
Train loss: 0.6907;  Loss pred: 0.6907; Loss self: 0.0000; time: 0.42s
Val loss: 0.6895 score: 0.6735 time: 0.66s
Test loss: 0.6903 score: 0.7347 time: 0.49s
Epoch 22/1000, LR 0.000270
Train loss: 0.6902;  Loss pred: 0.6902; Loss self: 0.0000; time: 0.51s
Val loss: 0.6889 score: 0.8163 time: 0.62s
Test loss: 0.6899 score: 0.6531 time: 0.59s
Epoch 23/1000, LR 0.000270
Train loss: 0.6899;  Loss pred: 0.6899; Loss self: 0.0000; time: 0.52s
Val loss: 0.6883 score: 0.7959 time: 0.79s
Test loss: 0.6895 score: 0.6531 time: 0.68s
Epoch 24/1000, LR 0.000270
Train loss: 0.6895;  Loss pred: 0.6895; Loss self: 0.0000; time: 0.52s
Val loss: 0.6876 score: 0.7959 time: 0.50s
Test loss: 0.6891 score: 0.6531 time: 0.48s
Epoch 25/1000, LR 0.000270
Train loss: 0.6889;  Loss pred: 0.6889; Loss self: 0.0000; time: 0.42s
Val loss: 0.6869 score: 0.7143 time: 0.63s
Test loss: 0.6885 score: 0.6531 time: 0.47s
Epoch 26/1000, LR 0.000270
Train loss: 0.6886;  Loss pred: 0.6886; Loss self: 0.0000; time: 0.43s
Val loss: 0.6861 score: 0.6531 time: 0.49s
Test loss: 0.6880 score: 0.5714 time: 0.47s
Epoch 27/1000, LR 0.000270
Train loss: 0.6878;  Loss pred: 0.6878; Loss self: 0.0000; time: 0.41s
Val loss: 0.6852 score: 0.6327 time: 0.62s
Test loss: 0.6873 score: 0.5510 time: 0.47s
Epoch 28/1000, LR 0.000270
Train loss: 0.6872;  Loss pred: 0.6872; Loss self: 0.0000; time: 0.41s
Val loss: 0.6842 score: 0.6122 time: 0.49s
Test loss: 0.6866 score: 0.5510 time: 0.46s
Epoch 29/1000, LR 0.000270
Train loss: 0.6862;  Loss pred: 0.6862; Loss self: 0.0000; time: 0.41s
Val loss: 0.6831 score: 0.5918 time: 0.64s
Test loss: 0.6859 score: 0.5306 time: 3.30s
Epoch 30/1000, LR 0.000270
Train loss: 0.6858;  Loss pred: 0.6858; Loss self: 0.0000; time: 1.77s
Val loss: 0.6820 score: 0.5918 time: 0.54s
Test loss: 0.6850 score: 0.5306 time: 0.49s
Epoch 31/1000, LR 0.000270
Train loss: 0.6851;  Loss pred: 0.6851; Loss self: 0.0000; time: 0.41s
Val loss: 0.6807 score: 0.5918 time: 0.63s
Test loss: 0.6841 score: 0.5306 time: 0.48s
Epoch 32/1000, LR 0.000270
Train loss: 0.6838;  Loss pred: 0.6838; Loss self: 0.0000; time: 0.41s
Val loss: 0.6793 score: 0.5918 time: 0.53s
Test loss: 0.6831 score: 0.5306 time: 0.46s
Epoch 33/1000, LR 0.000270
Train loss: 0.6826;  Loss pred: 0.6826; Loss self: 0.0000; time: 0.41s
Val loss: 0.6777 score: 0.5510 time: 0.63s
Test loss: 0.6820 score: 0.5306 time: 0.50s
Epoch 34/1000, LR 0.000270
Train loss: 0.6815;  Loss pred: 0.6815; Loss self: 0.0000; time: 0.41s
Val loss: 0.6760 score: 0.5306 time: 0.49s
Test loss: 0.6807 score: 0.5306 time: 0.49s
Epoch 35/1000, LR 0.000270
Train loss: 0.6801;  Loss pred: 0.6801; Loss self: 0.0000; time: 0.56s
Val loss: 0.6742 score: 0.5306 time: 0.64s
Test loss: 0.6793 score: 0.5306 time: 0.47s
Epoch 36/1000, LR 0.000270
Train loss: 0.6786;  Loss pred: 0.6786; Loss self: 0.0000; time: 0.42s
Val loss: 0.6723 score: 0.5306 time: 0.50s
Test loss: 0.6778 score: 0.5306 time: 0.62s
Epoch 37/1000, LR 0.000270
Train loss: 0.6765;  Loss pred: 0.6765; Loss self: 0.0000; time: 0.53s
Val loss: 0.6701 score: 0.5306 time: 0.49s
Test loss: 0.6761 score: 0.5306 time: 0.47s
Epoch 38/1000, LR 0.000270
Train loss: 0.6755;  Loss pred: 0.6755; Loss self: 0.0000; time: 0.43s
Val loss: 0.6678 score: 0.5306 time: 0.49s
Test loss: 0.6743 score: 0.5306 time: 0.64s
Epoch 39/1000, LR 0.000269
Train loss: 0.6723;  Loss pred: 0.6723; Loss self: 0.0000; time: 0.42s
Val loss: 0.6653 score: 0.5510 time: 0.51s
Test loss: 0.6723 score: 0.5306 time: 0.49s
Epoch 40/1000, LR 0.000269
Train loss: 0.6707;  Loss pred: 0.6707; Loss self: 0.0000; time: 0.43s
Val loss: 0.6626 score: 0.5510 time: 0.52s
Test loss: 0.6702 score: 0.5306 time: 0.47s
Epoch 41/1000, LR 0.000269
Train loss: 0.6693;  Loss pred: 0.6693; Loss self: 0.0000; time: 0.42s
Val loss: 0.6597 score: 0.5510 time: 0.64s
Test loss: 0.6678 score: 0.5306 time: 0.47s
Epoch 42/1000, LR 0.000269
Train loss: 0.6656;  Loss pred: 0.6656; Loss self: 0.0000; time: 0.44s
Val loss: 0.6565 score: 0.5918 time: 0.50s
Test loss: 0.6651 score: 0.5306 time: 0.47s
Epoch 43/1000, LR 0.000269
Train loss: 0.6624;  Loss pred: 0.6624; Loss self: 0.0000; time: 0.42s
Val loss: 0.6531 score: 0.5918 time: 0.50s
Test loss: 0.6622 score: 0.5306 time: 0.65s
Epoch 44/1000, LR 0.000269
Train loss: 0.6592;  Loss pred: 0.6592; Loss self: 0.0000; time: 0.42s
Val loss: 0.6493 score: 0.5918 time: 0.50s
Test loss: 0.6590 score: 0.5306 time: 0.59s
Epoch 45/1000, LR 0.000269
Train loss: 0.6569;  Loss pred: 0.6569; Loss self: 0.0000; time: 0.51s
Val loss: 0.6453 score: 0.5918 time: 0.65s
Test loss: 0.6555 score: 0.5306 time: 0.59s
Epoch 46/1000, LR 0.000269
Train loss: 0.6533;  Loss pred: 0.6533; Loss self: 0.0000; time: 0.66s
Val loss: 0.6410 score: 0.5918 time: 0.62s
Test loss: 0.6519 score: 0.5306 time: 0.63s
Epoch 47/1000, LR 0.000269
Train loss: 0.6488;  Loss pred: 0.6488; Loss self: 0.0000; time: 0.52s
Val loss: 0.6364 score: 0.5918 time: 0.57s
Test loss: 0.6479 score: 0.5306 time: 0.64s
Epoch 48/1000, LR 0.000269
Train loss: 0.6454;  Loss pred: 0.6454; Loss self: 0.0000; time: 0.46s
Val loss: 0.6315 score: 0.6122 time: 0.50s
Test loss: 0.6437 score: 0.5306 time: 0.49s
Epoch 49/1000, LR 0.000269
Train loss: 0.6374;  Loss pred: 0.6374; Loss self: 0.0000; time: 0.43s
Val loss: 0.6262 score: 0.6327 time: 0.52s
Test loss: 0.6391 score: 0.5306 time: 0.63s
Epoch 50/1000, LR 0.000269
Train loss: 0.6347;  Loss pred: 0.6347; Loss self: 0.0000; time: 0.44s
Val loss: 0.6207 score: 0.6327 time: 0.50s
Test loss: 0.6343 score: 0.5306 time: 0.48s
Epoch 51/1000, LR 0.000269
Train loss: 0.6318;  Loss pred: 0.6318; Loss self: 0.0000; time: 0.51s
Val loss: 0.6148 score: 0.6327 time: 0.86s
Test loss: 0.6293 score: 0.5510 time: 0.64s
Epoch 52/1000, LR 0.000269
Train loss: 0.6256;  Loss pred: 0.6256; Loss self: 0.0000; time: 0.56s
Val loss: 0.6086 score: 0.6327 time: 0.51s
Test loss: 0.6239 score: 0.5510 time: 0.48s
Epoch 53/1000, LR 0.000269
Train loss: 0.6191;  Loss pred: 0.6191; Loss self: 0.0000; time: 0.52s
Val loss: 0.6021 score: 0.6327 time: 0.63s
Test loss: 0.6182 score: 0.5510 time: 0.74s
Epoch 54/1000, LR 0.000269
Train loss: 0.6088;  Loss pred: 0.6088; Loss self: 0.0000; time: 0.52s
Val loss: 0.5952 score: 0.6327 time: 0.61s
Test loss: 0.6121 score: 0.5510 time: 0.62s
Epoch 55/1000, LR 0.000269
Train loss: 0.6039;  Loss pred: 0.6039; Loss self: 0.0000; time: 0.44s
Val loss: 0.5880 score: 0.6327 time: 0.70s
Test loss: 0.6058 score: 0.5714 time: 0.63s
Epoch 56/1000, LR 0.000269
Train loss: 0.5977;  Loss pred: 0.5977; Loss self: 0.0000; time: 0.55s
Val loss: 0.5805 score: 0.6327 time: 0.63s
Test loss: 0.5991 score: 0.5714 time: 0.62s
Epoch 57/1000, LR 0.000269
Train loss: 0.5877;  Loss pred: 0.5877; Loss self: 0.0000; time: 0.54s
Val loss: 0.5727 score: 0.6327 time: 0.78s
Test loss: 0.5921 score: 0.6122 time: 0.59s
Epoch 58/1000, LR 0.000269
Train loss: 0.5818;  Loss pred: 0.5818; Loss self: 0.0000; time: 0.52s
Val loss: 0.5645 score: 0.6531 time: 0.59s
Test loss: 0.5846 score: 0.6122 time: 0.48s
Epoch 59/1000, LR 0.000268
Train loss: 0.5729;  Loss pred: 0.5729; Loss self: 0.0000; time: 0.57s
Val loss: 0.5560 score: 0.6735 time: 0.50s
Test loss: 0.5768 score: 0.6327 time: 0.48s
Epoch 60/1000, LR 0.000268
Train loss: 0.5605;  Loss pred: 0.5605; Loss self: 0.0000; time: 0.43s
Val loss: 0.5471 score: 0.6939 time: 0.49s
Test loss: 0.5687 score: 0.6531 time: 0.62s
Epoch 61/1000, LR 0.000268
Train loss: 0.5520;  Loss pred: 0.5520; Loss self: 0.0000; time: 0.42s
Val loss: 0.5379 score: 0.7143 time: 0.49s
Test loss: 0.5600 score: 0.6531 time: 0.47s
Epoch 62/1000, LR 0.000268
Train loss: 0.5463;  Loss pred: 0.5463; Loss self: 0.0000; time: 0.42s
Val loss: 0.5283 score: 0.7551 time: 0.49s
Test loss: 0.5510 score: 0.6939 time: 0.61s
Epoch 63/1000, LR 0.000268
Train loss: 0.5295;  Loss pred: 0.5295; Loss self: 0.0000; time: 0.43s
Val loss: 0.5184 score: 0.7551 time: 0.49s
Test loss: 0.5415 score: 0.7143 time: 0.48s
Epoch 64/1000, LR 0.000268
Train loss: 0.5237;  Loss pred: 0.5237; Loss self: 0.0000; time: 0.44s
Val loss: 0.5081 score: 0.7755 time: 0.52s
Test loss: 0.5317 score: 0.7347 time: 0.51s
Epoch 65/1000, LR 0.000268
Train loss: 0.5104;  Loss pred: 0.5104; Loss self: 0.0000; time: 0.44s
Val loss: 0.4976 score: 0.8367 time: 0.68s
Test loss: 0.5216 score: 0.7551 time: 0.50s
Epoch 66/1000, LR 0.000268
Train loss: 0.4942;  Loss pred: 0.4942; Loss self: 0.0000; time: 0.42s
Val loss: 0.4871 score: 0.8367 time: 0.51s
Test loss: 0.5112 score: 0.7551 time: 0.48s
Epoch 67/1000, LR 0.000268
Train loss: 0.4844;  Loss pred: 0.4844; Loss self: 0.0000; time: 0.44s
Val loss: 0.4766 score: 0.8367 time: 0.53s
Test loss: 0.5008 score: 0.8163 time: 0.71s
Epoch 68/1000, LR 0.000268
Train loss: 0.4704;  Loss pred: 0.4704; Loss self: 0.0000; time: 0.51s
Val loss: 0.4662 score: 0.8367 time: 0.65s
Test loss: 0.4903 score: 0.8163 time: 0.53s
Epoch 69/1000, LR 0.000268
Train loss: 0.4526;  Loss pred: 0.4526; Loss self: 0.0000; time: 0.42s
Val loss: 0.4562 score: 0.8367 time: 0.52s
Test loss: 0.4804 score: 0.8163 time: 0.47s
Epoch 70/1000, LR 0.000268
Train loss: 0.4507;  Loss pred: 0.4507; Loss self: 0.0000; time: 0.55s
Val loss: 0.4468 score: 0.8571 time: 0.49s
Test loss: 0.4713 score: 0.8163 time: 0.55s
Epoch 71/1000, LR 0.000268
Train loss: 0.4298;  Loss pred: 0.4298; Loss self: 0.0000; time: 0.51s
Val loss: 0.4381 score: 0.8571 time: 0.62s
Test loss: 0.4628 score: 0.8163 time: 0.73s
Epoch 72/1000, LR 0.000267
Train loss: 0.4256;  Loss pred: 0.4256; Loss self: 0.0000; time: 0.42s
Val loss: 0.4301 score: 0.8571 time: 0.50s
Test loss: 0.4550 score: 0.8163 time: 0.49s
Epoch 73/1000, LR 0.000267
Train loss: 0.4067;  Loss pred: 0.4067; Loss self: 0.0000; time: 0.43s
Val loss: 0.4231 score: 0.8571 time: 0.53s
Test loss: 0.4481 score: 0.8163 time: 0.63s
Epoch 74/1000, LR 0.000267
Train loss: 0.3909;  Loss pred: 0.3909; Loss self: 0.0000; time: 0.42s
Val loss: 0.4171 score: 0.8776 time: 0.51s
Test loss: 0.4421 score: 0.8163 time: 0.47s
Epoch 75/1000, LR 0.000267
Train loss: 0.3884;  Loss pred: 0.3884; Loss self: 0.0000; time: 0.46s
Val loss: 0.4122 score: 0.8776 time: 0.64s
Test loss: 0.4371 score: 0.8163 time: 0.48s
Epoch 76/1000, LR 0.000267
Train loss: 0.3725;  Loss pred: 0.3725; Loss self: 0.0000; time: 0.45s
Val loss: 0.4079 score: 0.8776 time: 0.61s
Test loss: 0.4323 score: 0.8163 time: 0.64s
Epoch 77/1000, LR 0.000267
Train loss: 0.3593;  Loss pred: 0.3593; Loss self: 0.0000; time: 0.51s
Val loss: 0.4045 score: 0.8776 time: 0.64s
Test loss: 0.4282 score: 0.8163 time: 0.74s
Epoch 78/1000, LR 0.000267
Train loss: 0.3496;  Loss pred: 0.3496; Loss self: 0.0000; time: 0.45s
Val loss: 0.4015 score: 0.8980 time: 0.49s
Test loss: 0.4241 score: 0.8367 time: 0.48s
Epoch 79/1000, LR 0.000267
Train loss: 0.3479;  Loss pred: 0.3479; Loss self: 0.0000; time: 0.42s
Val loss: 0.3987 score: 0.8980 time: 0.65s
Test loss: 0.4198 score: 0.8367 time: 0.50s
Epoch 80/1000, LR 0.000267
Train loss: 0.3370;  Loss pred: 0.3370; Loss self: 0.0000; time: 0.42s
Val loss: 0.3960 score: 0.8980 time: 0.50s
Test loss: 0.4155 score: 0.8367 time: 0.47s
Epoch 81/1000, LR 0.000267
Train loss: 0.3241;  Loss pred: 0.3241; Loss self: 0.0000; time: 0.43s
Val loss: 0.3930 score: 0.8980 time: 0.66s
Test loss: 0.4107 score: 0.8367 time: 0.50s
Epoch 82/1000, LR 0.000267
Train loss: 0.3223;  Loss pred: 0.3223; Loss self: 0.0000; time: 0.43s
Val loss: 0.3884 score: 0.9184 time: 0.49s
Test loss: 0.4040 score: 0.8571 time: 0.51s
Epoch 83/1000, LR 0.000266
Train loss: 0.3099;  Loss pred: 0.3099; Loss self: 0.0000; time: 0.68s
Val loss: 0.3841 score: 0.9184 time: 0.63s
Test loss: 0.3972 score: 0.8571 time: 0.59s
Epoch 84/1000, LR 0.000266
Train loss: 0.3007;  Loss pred: 0.3007; Loss self: 0.0000; time: 0.51s
Val loss: 0.3804 score: 0.9184 time: 0.58s
Test loss: 0.3908 score: 0.8571 time: 0.62s
Epoch 85/1000, LR 0.000266
Train loss: 0.2865;  Loss pred: 0.2865; Loss self: 0.0000; time: 0.52s
Val loss: 0.3774 score: 0.9184 time: 0.62s
Test loss: 0.3847 score: 0.8776 time: 0.59s
Epoch 86/1000, LR 0.000266
Train loss: 0.2847;  Loss pred: 0.2847; Loss self: 0.0000; time: 0.54s
Val loss: 0.3760 score: 0.9184 time: 0.63s
Test loss: 0.3800 score: 0.8776 time: 0.71s
Epoch 87/1000, LR 0.000266
Train loss: 0.2879;  Loss pred: 0.2879; Loss self: 0.0000; time: 0.51s
Val loss: 0.3737 score: 0.9184 time: 0.52s
Test loss: 0.3741 score: 0.8776 time: 0.55s
Epoch 88/1000, LR 0.000266
Train loss: 0.2673;  Loss pred: 0.2673; Loss self: 0.0000; time: 0.46s
Val loss: 0.3710 score: 0.9184 time: 0.55s
Test loss: 0.3678 score: 0.8776 time: 0.59s
Epoch 89/1000, LR 0.000266
Train loss: 0.2542;  Loss pred: 0.2542; Loss self: 0.0000; time: 0.53s
Val loss: 0.3695 score: 0.9184 time: 0.59s
Test loss: 0.3626 score: 0.8776 time: 0.47s
Epoch 90/1000, LR 0.000266
Train loss: 0.2595;  Loss pred: 0.2595; Loss self: 0.0000; time: 0.60s
Val loss: 0.3676 score: 0.9184 time: 0.57s
Test loss: 0.3570 score: 0.8776 time: 0.51s
Epoch 91/1000, LR 0.000266
Train loss: 0.2616;  Loss pred: 0.2616; Loss self: 0.0000; time: 0.43s
Val loss: 0.3639 score: 0.9184 time: 0.50s
Test loss: 0.3496 score: 0.8776 time: 0.62s
Epoch 92/1000, LR 0.000266
Train loss: 0.2358;  Loss pred: 0.2358; Loss self: 0.0000; time: 0.42s
Val loss: 0.3611 score: 0.9388 time: 0.53s
Test loss: 0.3430 score: 0.8776 time: 0.50s
Epoch 93/1000, LR 0.000265
Train loss: 0.2276;  Loss pred: 0.2276; Loss self: 0.0000; time: 0.52s
Val loss: 0.3595 score: 0.9388 time: 0.62s
Test loss: 0.3374 score: 0.8776 time: 0.59s
Epoch 94/1000, LR 0.000265
Train loss: 0.2199;  Loss pred: 0.2199; Loss self: 0.0000; time: 0.71s
Val loss: 0.3583 score: 0.9388 time: 0.62s
Test loss: 0.3322 score: 0.8776 time: 0.51s
Epoch 95/1000, LR 0.000265
Train loss: 0.2143;  Loss pred: 0.2143; Loss self: 0.0000; time: 0.42s
Val loss: 0.3573 score: 0.9388 time: 0.75s
Test loss: 0.3272 score: 0.8776 time: 0.80s
Epoch 96/1000, LR 0.000265
Train loss: 0.2047;  Loss pred: 0.2047; Loss self: 0.0000; time: 0.53s
Val loss: 0.3564 score: 0.9388 time: 0.63s
Test loss: 0.3223 score: 0.8776 time: 0.60s
Epoch 97/1000, LR 0.000265
Train loss: 0.2043;  Loss pred: 0.2043; Loss self: 0.0000; time: 0.46s
Val loss: 0.3563 score: 0.9388 time: 0.53s
Test loss: 0.3181 score: 0.8776 time: 0.61s
Epoch 98/1000, LR 0.000265
Train loss: 0.1984;  Loss pred: 0.1984; Loss self: 0.0000; time: 0.42s
Val loss: 0.3570 score: 0.9388 time: 0.49s
Test loss: 0.3146 score: 0.8776 time: 0.53s
     INFO: Early stopping counter 1 of 20
Epoch 99/1000, LR 0.000265
Train loss: 0.1899;  Loss pred: 0.1899; Loss self: 0.0000; time: 0.42s
Val loss: 0.3590 score: 0.9388 time: 0.63s
Test loss: 0.3124 score: 0.8776 time: 0.47s
     INFO: Early stopping counter 2 of 20
Epoch 100/1000, LR 0.000265
Train loss: 0.1946;  Loss pred: 0.1946; Loss self: 0.0000; time: 0.42s
Val loss: 0.3582 score: 0.9388 time: 0.49s
Test loss: 0.3076 score: 0.8776 time: 0.48s
     INFO: Early stopping counter 3 of 20
Epoch 101/1000, LR 0.000265
Train loss: 0.1808;  Loss pred: 0.1808; Loss self: 0.0000; time: 0.42s
Val loss: 0.3591 score: 0.9388 time: 0.50s
Test loss: 0.3042 score: 0.8776 time: 0.61s
     INFO: Early stopping counter 4 of 20
Epoch 102/1000, LR 0.000264
Train loss: 0.1649;  Loss pred: 0.1649; Loss self: 0.0000; time: 0.43s
Val loss: 0.3601 score: 0.9388 time: 0.65s
Test loss: 0.3011 score: 0.8776 time: 0.64s
     INFO: Early stopping counter 5 of 20
Epoch 103/1000, LR 0.000264
Train loss: 0.1624;  Loss pred: 0.1624; Loss self: 0.0000; time: 0.48s
Val loss: 0.3611 score: 0.9388 time: 0.67s
Test loss: 0.2978 score: 0.8980 time: 0.59s
     INFO: Early stopping counter 6 of 20
Epoch 104/1000, LR 0.000264
Train loss: 0.1614;  Loss pred: 0.1614; Loss self: 0.0000; time: 0.51s
Val loss: 0.3629 score: 0.9388 time: 0.51s
Test loss: 0.2954 score: 0.8980 time: 0.47s
     INFO: Early stopping counter 7 of 20
Epoch 105/1000, LR 0.000264
Train loss: 0.1593;  Loss pred: 0.1593; Loss self: 0.0000; time: 0.45s
Val loss: 0.3643 score: 0.9388 time: 0.67s
Test loss: 0.2926 score: 0.8980 time: 0.46s
     INFO: Early stopping counter 8 of 20
Epoch 106/1000, LR 0.000264
Train loss: 0.1574;  Loss pred: 0.1574; Loss self: 0.0000; time: 0.43s
Val loss: 0.3680 score: 0.9388 time: 0.49s
Test loss: 0.2923 score: 0.8980 time: 0.51s
     INFO: Early stopping counter 9 of 20
Epoch 107/1000, LR 0.000264
Train loss: 0.1488;  Loss pred: 0.1488; Loss self: 0.0000; time: 0.55s
Val loss: 0.3723 score: 0.9388 time: 0.49s
Test loss: 0.2927 score: 0.8980 time: 0.57s
     INFO: Early stopping counter 10 of 20
Epoch 108/1000, LR 0.000264
Train loss: 0.1482;  Loss pred: 0.1482; Loss self: 0.0000; time: 0.53s
Val loss: 0.3794 score: 0.9388 time: 0.66s
Test loss: 0.2962 score: 0.8980 time: 0.77s
     INFO: Early stopping counter 11 of 20
Epoch 109/1000, LR 0.000264
Train loss: 0.1344;  Loss pred: 0.1344; Loss self: 0.0000; time: 0.52s
Val loss: 0.3882 score: 0.9388 time: 0.61s
Test loss: 0.3014 score: 0.8980 time: 0.61s
     INFO: Early stopping counter 12 of 20
Epoch 110/1000, LR 0.000263
Train loss: 0.1417;  Loss pred: 0.1417; Loss self: 0.0000; time: 0.48s
Val loss: 0.3985 score: 0.9388 time: 0.53s
Test loss: 0.3081 score: 0.8776 time: 0.62s
     INFO: Early stopping counter 13 of 20
Epoch 111/1000, LR 0.000263
Train loss: 0.1869;  Loss pred: 0.1869; Loss self: 0.0000; time: 0.42s
Val loss: 0.4001 score: 0.9388 time: 0.52s
Test loss: 0.3065 score: 0.8980 time: 0.47s
     INFO: Early stopping counter 14 of 20
Epoch 112/1000, LR 0.000263
Train loss: 0.1557;  Loss pred: 0.1557; Loss self: 0.0000; time: 0.41s
Val loss: 0.3971 score: 0.9388 time: 0.49s
Test loss: 0.3003 score: 0.8980 time: 0.47s
     INFO: Early stopping counter 15 of 20
Epoch 113/1000, LR 0.000263
Train loss: 0.1316;  Loss pred: 0.1316; Loss self: 0.0000; time: 0.42s
Val loss: 0.3960 score: 0.9388 time: 0.71s
Test loss: 0.2962 score: 0.8980 time: 0.47s
     INFO: Early stopping counter 16 of 20
Epoch 114/1000, LR 0.000263
Train loss: 0.1269;  Loss pred: 0.1269; Loss self: 0.0000; time: 0.52s
Val loss: 0.3972 score: 0.9388 time: 2.51s
Test loss: 0.2943 score: 0.8980 time: 3.30s
     INFO: Early stopping counter 17 of 20
Epoch 115/1000, LR 0.000263
Train loss: 0.1243;  Loss pred: 0.1243; Loss self: 0.0000; time: 0.42s
Val loss: 0.4003 score: 0.9388 time: 0.50s
Test loss: 0.2946 score: 0.8980 time: 0.64s
     INFO: Early stopping counter 18 of 20
Epoch 116/1000, LR 0.000263
Train loss: 0.1313;  Loss pred: 0.1313; Loss self: 0.0000; time: 0.42s
Val loss: 0.4035 score: 0.9388 time: 0.50s
Test loss: 0.2951 score: 0.8980 time: 0.47s
     INFO: Early stopping counter 19 of 20
Epoch 117/1000, LR 0.000262
Train loss: 0.1305;  Loss pred: 0.1305; Loss self: 0.0000; time: 0.44s
Val loss: 0.4077 score: 0.9388 time: 0.51s
Test loss: 0.2967 score: 0.8980 time: 0.47s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 096,   Train_Loss: 0.2043,   Val_Loss: 0.3563,   Val_Precision: 0.8929,   Val_Recall: 1.0000,   Val_accuracy: 0.9434,   Val_Score: 0.9388,   Val_Loss: 0.3563,   Test_Precision: 0.8462,   Test_Recall: 0.9167,   Test_accuracy: 0.8800,   Test_Score: 0.8776,   Test_loss: 0.3181


[0.5557976951822639, 0.43661666894331574, 0.4353777680080384, 0.4317563269287348, 0.4438797659240663, 0.4320737069938332, 0.4727512230165303, 0.5485460001509637, 0.4482768240850419, 0.5608840240165591, 0.48588921898044646, 0.4267306460533291, 0.4454654648434371, 0.4404641829896718, 0.537623907905072, 0.5616491201799363, 0.582398243015632, 0.5550781718920916, 0.49320246395654976, 0.49269940704107285, 0.5611588661558926, 0.5706078989896923, 0.5410676030442119, 0.4632785180583596, 0.5534162679687142, 0.43911505909636617, 0.4633998649660498, 0.5480853549670428, 0.5008167149499059, 0.5400644200854003, 0.5459566009230912, 0.5562938149087131, 0.48659669305197895, 0.4323171761352569, 0.4993087421171367, 0.5483128689229488, 0.5571511599700898, 0.5228511129971594, 0.4557801310438663, 0.4305635979399085, 0.5801630278583616, 0.44886294286698103, 0.4920732250902802, 0.44439376913942397, 0.5563390129245818, 0.5530433000531048, 0.45926043996587396, 0.4479618698824197, 0.5708832328673452, 0.44464431912638247, 0.6204402258154005, 0.5249992969911546, 0.4398067800793797, 0.5094195900019258, 0.4375690561719239, 0.5176349140238017, 0.5065021889749914, 2.2598491290118545, 0.4280350140761584, 0.434902855893597, 0.4613353000022471, 0.4287063810043037, 0.49012007378041744, 0.42398875020444393, 0.42902415501885116, 0.43311849096789956, 0.4422965890262276, 0.4257883110549301, 0.4370779939927161, 0.43648961186408997, 0.44039573799818754, 0.44880396709777415, 0.5922640089411288, 0.43510395591147244, 0.450994161888957, 0.43270232900977135, 0.479936117073521, 0.478334496030584, 0.5486893369816244, 0.4990654019638896, 0.4422760361339897, 0.5806497561279684, 0.4370005859527737, 0.5204990520142019, 0.5533135170117021, 0.5520911908242851, 0.4769732530694455, 0.44074963801540434, 0.4388426698278636, 0.5728680200409144, 0.5015784751158208, 0.5607310289051384, 0.5564900110475719, 0.5168376499786973, 0.562678060028702, 0.5504499061498791, 0.42627913504838943, 0.5746080910321325, 0.5293126830365509, 0.5534459128975868, 0.4745249031111598, 0.4360520930495113, 0.444812930887565, 0.4381598262116313, 0.4385390488896519, 0.5460027479566634, 0.4506459750700742, 0.5650667939335108, 0.4491341558750719, 0.4365639640018344, 0.4555418479721993, 0.5529362678062171, 0.4822837598621845, 0.7583583300001919, 0.5989137401338667, 0.47944922489114106, 0.4910283461213112, 0.6020567349623889, 0.6074364220257849, 0.718606858048588, 0.4780277528334409, 0.637462595012039, 0.6365801920183003, 0.4789469849783927, 0.5278028161264956, 0.4825568380765617, 0.5786859919317067, 0.7683905391022563, 0.4691070099361241, 0.46668914798647165, 0.6012188559398055, 0.4720959479454905, 0.4950640699826181, 0.6020180720370263, 0.6835721919778734, 0.48742848192341626, 0.47215264895930886, 0.47643985715694726, 0.475682889809832, 0.46711748698726296, 3.3091608139220625, 0.4986285651102662, 0.486366885015741, 0.4688519879709929, 0.5051073029171675, 0.49168322305195034, 0.4790174029767513, 0.6262954080011696, 0.4771258740220219, 0.6408773090224713, 0.4951875729020685, 0.4724078020080924, 0.47556717600673437, 0.4785402228590101, 0.6581305279396474, 0.5957893370650709, 0.5967928320169449, 0.6392666189931333, 0.6430167679209262, 0.49071962386369705, 0.6424030761700124, 0.4808319378644228, 0.6456010669935495, 0.48804523004218936, 0.750218243105337, 0.6310687300283462, 0.6423485151026398, 0.6254836220759898, 0.5995722799561918, 0.48207449703477323, 0.48554276092909276, 0.6262974929995835, 0.4745055618695915, 0.6141032457817346, 0.48354335804469883, 0.5117538359481841, 0.5026454669423401, 0.49052343890070915, 0.7166361431591213, 0.5386644429527223, 0.48001237912103534, 0.5564263160340488, 0.7301839389838278, 0.49888141197152436, 0.6333513229619712, 0.4762881558854133, 0.4847057741135359, 0.6499581870157272, 0.7469557460863143, 0.4859416000545025, 0.5073359259404242, 0.4799491078592837, 0.5101077749859542, 0.5129916539881378, 0.5968837970867753, 0.6296080281026661, 0.5960862149950117, 0.7158494079485536, 0.5619503308553249, 0.5937542859464884, 0.4737936691381037, 0.5127384918741882, 0.6283517039846629, 0.5086855289991945, 0.5981908249668777, 0.5133480390068144, 0.8083920211065561, 0.6074237790890038, 0.6182883800938725, 0.5329005608800799, 0.47965471795760095, 0.4845176939852536, 0.6191656931769103, 0.6463701720349491, 0.5951154849026352, 0.47987548308447003, 0.4699512810911983, 0.5166563270613551, 0.5769857580307871, 0.7790642951149493, 0.6211484530940652, 0.6271306800190359, 0.47251242212951183, 0.4760751340072602, 0.4753374429419637, 3.3032918989192694, 0.6492060769814998, 0.47541322093456984, 0.4715743060223758]
[0.011342810105760487, 0.0089105442641493, 0.008885260571592621, 0.008811353610790506, 0.009058770733144211, 0.008817830754976188, 0.009647984143194496, 0.011194816329611503, 0.009148506613980447, 0.011446612735031819, 0.009916106509805029, 0.0087087886949659, 0.009091131935580348, 0.008989064958972894, 0.010971916487858611, 0.01146222694244768, 0.01188567842889045, 0.011328125956981462, 0.010065356407276526, 0.010055089939613732, 0.011452221758283523, 0.011645059163054946, 0.01104219598049412, 0.009454663633844075, 0.011294209550381924, 0.008961531818293187, 0.009457140101347955, 0.01118541540749067, 0.010220749284691957, 0.01102172285888572, 0.011141971447410024, 0.011352934998137002, 0.009930544756162835, 0.008822799512964427, 0.010189974328921157, 0.011190058549447936, 0.011370431836124281, 0.010670430877493049, 0.009301635327425842, 0.008787012202855276, 0.011840061793027788, 0.009160468221775122, 0.010042310716128168, 0.009069260594682122, 0.011353857406624118, 0.011286597960267444, 0.009372662040119877, 0.009142078977192238, 0.011650678221782555, 0.00907437385972209, 0.012662045424804091, 0.01071427136716642, 0.008975648573048564, 0.01039631816330461, 0.008929980738202527, 0.010563977837220443, 0.01033677936683656, 0.04611936997983376, 0.008735408450533845, 0.008875568487624429, 0.009415006122494839, 0.008749109816414361, 0.010002450485314642, 0.008652831636825386, 0.008755595000384718, 0.00883915287689591, 0.009026461000535257, 0.008689557368467962, 0.008919959061075839, 0.008907951262532448, 0.008987668122411991, 0.009159264634648452, 0.012087020590635282, 0.008879672569621886, 0.009203962487529735, 0.00883065977570962, 0.00979461463415349, 0.00976192849042008, 0.01119774157105356, 0.010185008203344685, 0.009026041553754891, 0.011849995023019763, 0.008918379305158647, 0.010622429632942895, 0.011292112592075552, 0.011267167159679289, 0.009734148021825418, 0.008994890571742947, 0.00895597285362987, 0.01169118408246764, 0.010236295410526954, 0.01144349038581915, 0.011356939000970855, 0.010547707142422393, 0.011483225714871469, 0.011233671554079165, 0.008699574184661009, 0.011726695735349643, 0.01080229965380716, 0.011294814548930344, 0.009684181696146118, 0.008899022307132885, 0.009077814916072756, 0.008942037269625128, 0.00894977650795208, 0.011142913223605375, 0.009196856634083147, 0.011531975386398179, 0.009166003181123915, 0.00890946865309866, 0.009296772407595905, 0.011284413628698307, 0.009842525711473154, 0.015476700612248815, 0.012222729390487075, 0.009784678059002879, 0.010020986655536962, 0.012286872142089568, 0.012396661673995609, 0.014665446082624246, 0.009755668425172264, 0.013009440714531407, 0.012991432490169394, 0.009774428264865158, 0.010771486043397869, 0.009848098736256361, 0.011809918202687892, 0.015681439573515435, 0.009573612447676003, 0.009524268326254524, 0.012269772570200113, 0.00963461118256103, 0.010103348366992206, 0.012286083102796455, 0.01395045289750762, 0.009947520039253394, 0.009635768346108345, 0.009723262390958108, 0.009707814077751674, 0.009533009938515571, 0.06753389416167474, 0.010176093165515637, 0.009925854796239612, 0.009568407917775365, 0.01030831230443199, 0.010034351490856129, 0.009775865366872477, 0.012781538938799379, 0.009737262735143304, 0.013079128755560639, 0.010105868834736092, 0.009640975551185559, 0.009705452571566008, 0.009766126997122655, 0.013431235264074437, 0.012158966062552467, 0.012179445551366222, 0.013046257530472108, 0.013122791182059718, 0.01001468620129994, 0.013110266860612497, 0.00981289669111067, 0.013175531979460193, 0.009960106735554884, 0.015310576389904837, 0.012878953674047882, 0.013109153369441628, 0.012764971879101833, 0.012236168978697792, 0.009838255041525985, 0.009909035937328423, 0.012781581489787417, 0.009683786976930439, 0.012532719301668053, 0.00986823179683059, 0.010443955835677227, 0.010258070753925309, 0.010010682426545084, 0.014625227411410638, 0.010993151896994333, 0.009796171002470109, 0.01135563910273569, 0.014901713040486281, 0.010181253305541314, 0.012925537203305535, 0.009720166446641088, 0.00989195457374563, 0.01326445279623933, 0.015243994818088047, 0.009917175511316377, 0.010353794406947434, 0.00979487975223028, 0.010410362754815392, 0.010469217428329344, 0.01218130198136276, 0.012849143430666655, 0.012165024795816565, 0.014609171590786807, 0.011468374099088262, 0.01211743440707119, 0.009669258553838852, 0.010464050854575269, 0.012823504162952304, 0.010381337326514174, 0.012207976019732197, 0.010476490591975803, 0.01649779634911339, 0.012396403654877628, 0.0126181302059974, 0.010875521650613874, 0.00978887179505308, 0.009888116203780686, 0.012636034554630821, 0.013191228000713246, 0.012145213977604801, 0.00979337720580551, 0.009590842471248945, 0.010544006674721534, 0.011775219551648716, 0.015899271328876516, 0.012676499042736024, 0.012798585306510938, 0.009643110655704322, 0.009715819061372657, 0.009700764141672728, 0.06741412038610754, 0.013249103611867343, 0.009702310631317752, 0.009623965429028079]
[88.161574660599, 112.22659024582839, 112.54593964268591, 113.48994083898599, 110.3902537616061, 113.40657671793798, 103.6485948938236, 89.3270573233874, 109.30745772996796, 87.36208895576131, 100.84603256441443, 114.82653156782277, 109.99730364557328, 111.24627584338445, 91.14178011713703, 87.24308156007046, 84.13486920269573, 88.27585461156578, 99.35067965174807, 99.45211887765723, 87.31930110213668, 85.87332928050678, 90.56169640228136, 105.76790869855836, 88.54094618478051, 111.58806555355845, 105.7402120813952, 89.40213336469544, 97.8401849165542, 90.72991698333253, 89.75072362373108, 88.08294948963403, 100.6994102090327, 113.34270925351719, 98.13567411664647, 89.36503733033061, 87.94740731156429, 93.71692778679466, 107.50797733937205, 113.80432585208625, 84.45901866735748, 109.1647256220948, 99.5786754928802, 110.2625720763127, 88.07579346703575, 88.60065748069796, 106.69327409005884, 109.38430990312064, 85.83191304093882, 110.20044087434432, 78.97618168712833, 93.333458312851, 111.41256165072332, 96.18789885919927, 111.98232440994997, 94.66131180971092, 96.74193136097064, 21.68286341372967, 114.47661613795371, 112.66883934187892, 106.213420043429, 114.29734235634832, 99.97550115026074, 115.56910407734308, 114.21268342768943, 113.13301330196872, 110.78538974917205, 115.08066033705329, 112.10813784602614, 112.25925810866069, 111.26356540762359, 109.17907057921596, 82.73337440781516, 112.61676510697983, 108.64885655008699, 113.24182172103232, 102.09692135441794, 102.4387753896533, 89.30372197417245, 98.18352425789966, 110.79053802760231, 84.38822109692056, 112.12799610592603, 94.14042121764139, 88.55738834040393, 88.7534538032419, 102.73112734240841, 111.17422630370359, 111.65732817007128, 85.53453550522929, 97.69159250441376, 87.38592564723143, 88.05189496170706, 94.80733457018786, 87.08354471382894, 89.01809129686372, 114.94815479166655, 85.27551345819786, 92.57288096498603, 88.53620355322286, 103.26117697667283, 112.37189496631156, 110.15866805451681, 111.83133886020164, 111.73463372090659, 89.74313807645738, 108.73280293334615, 86.71541227702302, 109.09880568876063, 112.24013899551753, 107.56421219723013, 88.61780796981944, 101.59993779181377, 64.61325479208173, 81.81478686571413, 102.20060322576484, 99.79057296194115, 81.38767852677718, 80.66687841434707, 68.18749285675048, 102.50450880636016, 76.86725524510909, 76.9738056797585, 102.30777421474026, 92.83770094219513, 101.54244253446004, 84.67459154563863, 63.76965554163226, 104.45377912104124, 104.99494194671183, 81.50110316052016, 103.79246043784659, 98.97708795897955, 81.39290542259057, 71.68226059375172, 100.5275682837483, 103.77999595682228, 102.84613947371447, 103.0098013817339, 104.89866332350795, 14.807379500521925, 98.26954055302505, 100.74699061473756, 104.5105945099065, 97.00909037942677, 99.65766107667812, 102.29273445077354, 78.2378401214599, 102.6982661555227, 76.45769215130989, 98.95240244587187, 103.72394315189702, 103.0348654662115, 102.39473644922137, 74.45331574786552, 82.24383511356518, 82.10554378543328, 76.65033421763312, 76.20330051179269, 99.85335335521513, 76.27609801020337, 101.90670823080023, 75.89826365712865, 100.40053049133205, 65.3143274644682, 77.6460592458749, 76.28257690012776, 78.33938135321311, 81.72492564796396, 101.64404112102514, 100.91799104622181, 78.23757966093693, 103.26538598817665, 79.79114316131728, 101.33527673328196, 95.74916015863792, 97.48421745066872, 99.89328972701443, 68.37500517905093, 90.96572205769418, 102.08070068885583, 88.0619744034565, 67.10637879572047, 98.21973483910214, 77.36622349005837, 102.87889672358041, 101.092255584565, 75.3894650130999, 65.59960246204173, 100.83516207401102, 96.58294927403585, 102.09415789635412, 96.0581320317049, 95.51812318789314, 82.09303090342785, 77.82619949695096, 82.20287395911353, 68.45015090593121, 87.19631844582922, 82.52572008282915, 103.42054609792018, 95.5652847924342, 77.98180491796043, 96.3267032510327, 81.91366024832155, 95.4518110068198, 60.6141559053577, 80.6685574171771, 79.2510446218648, 91.94961236121988, 102.15681857283712, 101.13149758673484, 79.1387516136162, 75.8079535844525, 82.33696020868406, 102.10982166674846, 104.26612708922715, 94.84060764087197, 84.92410656241091, 62.89596418068441, 78.88613383148773, 78.13363555824226, 103.7009773820708, 102.92493033096062, 103.08466275395573, 14.83368757572748, 75.47680426502899, 103.0682316820629, 103.90727266992997]
Elapsed: 0.5583140994361134~0.2932173656257037
Time per graph: 0.01139416529461456~0.0059840278699123195
Speed: 94.02796088609094~15.764718793562922
Total Time: 0.4724
best val loss: 0.35631129145622253 test_score: 0.8776

Testing...
Test loss: 0.3430 score: 0.8776 time: 0.58s
test Score 0.8776
Epoch Time List: [1.6028206122573465, 1.4421065340284258, 1.5036961131263524, 1.4041564629878849, 1.481176831992343, 1.4181753173470497, 1.5599925923161209, 1.711775926174596, 1.6711710870731622, 1.5867823497392237, 1.8755753978621215, 1.41058943211101, 1.4897221280261874, 1.443383929086849, 1.61078364495188, 1.5685488330200315, 2.087083663325757, 1.7892308440059423, 1.8472083339001983, 1.6363531311508268, 1.879508753074333, 1.756759799318388, 1.631346508860588, 1.448570615844801, 1.7236217390745878, 1.5495978959370404, 1.518880944000557, 1.5375331128016114, 1.5536270702723414, 1.7132020378485322, 1.6745930849574506, 1.7307669878937304, 1.7611988170538098, 1.4935704660601914, 1.5957276469562203, 1.7189970405306667, 1.842836296884343, 1.4914667068514973, 1.5188193556386977, 1.3973014219664037, 1.691152285085991, 1.4148280690424144, 1.7046428879257292, 1.3771537870634347, 1.8507115901447833, 1.7203573752194643, 1.5826043707784265, 1.47596654901281, 1.8044049739837646, 1.6498118760064244, 1.7736913117114455, 1.7448075069114566, 1.4794687230605632, 1.6423211907967925, 1.6149483770132065, 1.6136173019185662, 1.8117161279078573, 6.446931465761736, 1.4552888758480549, 1.3730466139968485, 1.4801644121762365, 1.3462149628903717, 1.5201655379496515, 1.349505285732448, 1.44922531908378, 1.3516054640058428, 1.5199323750566691, 1.3593243041541427, 1.4834350140299648, 1.3830295228399336, 1.5212320808786899, 1.458814735757187, 1.8807612750679255, 1.4623267049901187, 1.6973867861088365, 1.4261465107556432, 1.6798400881234556, 1.4343095410149544, 1.6069537398871034, 1.6282320590689778, 1.6281786900945008, 1.839561881031841, 1.6992059478070587, 1.730601780116558, 1.6635039430111647, 1.8655502400361001, 1.68389707012102, 1.365640388801694, 1.76356745720841, 1.6321223301347345, 1.8192384410649538, 1.556187563110143, 1.8947374518029392, 1.7223244130145758, 1.795619642129168, 1.7466508860234171, 1.5814802600070834, 1.7184820857364684, 1.6986074249725789, 1.7347999811172485, 1.7606946609448642, 1.3787837231066078, 1.523784255143255, 1.36867905408144, 1.5136261191219091, 1.5559182572178543, 1.5266851398628205, 1.4793970668688416, 1.6243218621239066, 1.5119699200149626, 1.4957748050801456, 1.6066255299374461, 1.655571105889976, 1.6764030877966434, 1.7210699920542538, 1.483016877900809, 1.5797856270801276, 1.5585791480261832, 1.8919943417422473, 1.8257219889201224, 1.4881428147200495, 1.8636465456802398, 1.7818499237764627, 1.5874954052269459, 1.4227195852436125, 1.4848647378385067, 1.5600573839619756, 1.9101961837150156, 1.5357904338743538, 1.369337699841708, 1.8359982850961387, 1.650372747797519, 1.566627173917368, 1.7183844377286732, 1.984061607858166, 1.5001060673967004, 1.51505356002599, 1.384667298058048, 1.5014154689852148, 1.3662329611834139, 4.355055981781334, 2.8040557811036706, 1.5247811991721392, 1.4011137187480927, 1.5419637288432568, 1.3898222823627293, 1.6728034701664, 1.5366867147386074, 1.4888076111674309, 1.5565199400298297, 1.4222408987116069, 1.415564491879195, 1.5354857759084553, 1.4118694067001343, 1.5669996486976743, 1.5019995279144496, 1.7513428379315883, 1.9176865417975932, 1.7275092487689108, 1.4455655626952648, 1.5787773409392685, 1.4069974680896848, 2.0127810032572597, 1.5526619150768965, 1.8989973049610853, 1.7540958430618048, 1.7772723489906639, 1.7934927640017122, 1.9207790920045227, 1.5840537210460752, 1.549518883228302, 1.5398509437218308, 1.3782975424546748, 1.5219204479362816, 1.4029071035329252, 1.465921690221876, 1.610029520932585, 1.421015297062695, 1.6852256597485393, 1.6938869680743665, 1.4079098079819232, 1.5855358282569796, 1.8570712429936975, 1.4228207869455218, 1.5902975113131106, 1.4018000229261816, 1.5887742238119245, 1.7020128699950874, 1.890114823821932, 1.4244753438979387, 1.5755665102042258, 1.3943247359711677, 1.5962893939577043, 1.4249788881279528, 1.8934271920006722, 1.7157250251621008, 1.7192245402839035, 1.8748165885917842, 1.5874394229613245, 1.591572833713144, 1.5912828091531992, 1.6727789607830346, 1.554305198835209, 1.4531746548600495, 1.7262946779374033, 1.8306577939074486, 1.9730343318078667, 1.7618031718302518, 1.6057678540237248, 1.4420557592529804, 1.5207135120872408, 1.3848215229809284, 1.528287329012528, 1.712386490777135, 1.7401887660380453, 1.4969566131476313, 1.5780978370457888, 1.4358082422986627, 1.6092263502068818, 1.9695124770514667, 1.7476195918861777, 1.6303725587204099, 1.4075885978527367, 1.3650667238980532, 1.6012107692658901, 6.330675031989813, 1.5554369310848415, 1.3854835610836744, 1.41578315384686]
Total Epoch List: [112, 117]
Total Time List: [0.5536897419951856, 0.4723813950549811]
========================k_idx:2========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: FAGNN
FAGNN(
  (conv1): NeighborFilterConv()
  (conv2): NeighborFilterConv()
  (ffn): Sequential(
    (0): Linear(in_features=14951, out_features=64, bias=True)
    (1): ReLU()
    (2): Linear(in_features=64, out_features=64, bias=True)
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=64, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=64, out_features=2, bias=True)
  )
)
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7f904e4aca60>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.6997;  Loss pred: 0.6997; Loss self: 0.0000; time: 0.64s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6958 score: 0.5102 time: 0.41s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6978 score: 0.5000 time: 0.41s
Epoch 2/1000, LR 0.000000
Train loss: 0.6997;  Loss pred: 0.6997; Loss self: 0.0000; time: 0.55s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6958 score: 0.5102 time: 0.53s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6977 score: 0.5000 time: 0.52s
Epoch 3/1000, LR 0.000030
Train loss: 0.6997;  Loss pred: 0.6997; Loss self: 0.0000; time: 0.53s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6957 score: 0.5102 time: 0.41s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6977 score: 0.5000 time: 0.42s
Epoch 4/1000, LR 0.000060
Train loss: 0.6996;  Loss pred: 0.6996; Loss self: 0.0000; time: 0.54s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6956 score: 0.5102 time: 0.54s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6976 score: 0.5000 time: 0.42s
Epoch 5/1000, LR 0.000090
Train loss: 0.6995;  Loss pred: 0.6995; Loss self: 0.0000; time: 0.54s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6955 score: 0.5102 time: 0.41s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6974 score: 0.5000 time: 0.42s
Epoch 6/1000, LR 0.000120
Train loss: 0.6993;  Loss pred: 0.6993; Loss self: 0.0000; time: 0.66s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6954 score: 0.5102 time: 0.42s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6973 score: 0.5000 time: 0.41s
Epoch 7/1000, LR 0.000150
Train loss: 0.6991;  Loss pred: 0.6991; Loss self: 0.0000; time: 0.54s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6953 score: 0.5102 time: 0.41s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6971 score: 0.5000 time: 0.45s
Epoch 8/1000, LR 0.000180
Train loss: 0.6988;  Loss pred: 0.6988; Loss self: 0.0000; time: 0.65s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6951 score: 0.5102 time: 0.40s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6969 score: 0.5000 time: 0.41s
Epoch 9/1000, LR 0.000210
Train loss: 0.6986;  Loss pred: 0.6986; Loss self: 0.0000; time: 0.55s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6949 score: 0.5102 time: 0.43s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6966 score: 0.5000 time: 0.50s
Epoch 10/1000, LR 0.000240
Train loss: 0.6983;  Loss pred: 0.6983; Loss self: 0.0000; time: 0.64s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6947 score: 0.5102 time: 0.43s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6964 score: 0.5000 time: 0.42s
Epoch 11/1000, LR 0.000270
Train loss: 0.6979;  Loss pred: 0.6979; Loss self: 0.0000; time: 0.54s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6945 score: 0.5102 time: 0.45s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6961 score: 0.5000 time: 0.41s
Epoch 12/1000, LR 0.000270
Train loss: 0.6976;  Loss pred: 0.6976; Loss self: 0.0000; time: 0.55s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6943 score: 0.5102 time: 0.53s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6959 score: 0.5000 time: 0.42s
Epoch 13/1000, LR 0.000270
Train loss: 0.6973;  Loss pred: 0.6973; Loss self: 0.0000; time: 0.54s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6941 score: 0.5102 time: 0.40s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6956 score: 0.5000 time: 0.42s
Epoch 14/1000, LR 0.000270
Train loss: 0.6969;  Loss pred: 0.6969; Loss self: 0.0000; time: 0.66s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6939 score: 0.5102 time: 0.46s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6954 score: 0.5000 time: 0.53s
Epoch 15/1000, LR 0.000270
Train loss: 0.6966;  Loss pred: 0.6966; Loss self: 0.0000; time: 0.70s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6937 score: 0.5102 time: 0.53s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6952 score: 0.5000 time: 0.50s
Epoch 16/1000, LR 0.000270
Train loss: 0.6963;  Loss pred: 0.6963; Loss self: 0.0000; time: 0.54s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6935 score: 0.5102 time: 0.57s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6949 score: 0.5000 time: 0.53s
Epoch 17/1000, LR 0.000270
Train loss: 0.6959;  Loss pred: 0.6959; Loss self: 0.0000; time: 0.67s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6933 score: 0.5102 time: 0.41s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6947 score: 0.5000 time: 0.41s
Epoch 18/1000, LR 0.000270
Train loss: 0.6956;  Loss pred: 0.6956; Loss self: 0.0000; time: 0.69s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6931 score: 0.5102 time: 0.42s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6944 score: 0.5000 time: 0.42s
Epoch 19/1000, LR 0.000270
Train loss: 0.6953;  Loss pred: 0.6953; Loss self: 0.0000; time: 0.54s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6929 score: 0.5102 time: 0.55s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6941 score: 0.5000 time: 0.52s
Epoch 20/1000, LR 0.000270
Train loss: 0.6949;  Loss pred: 0.6949; Loss self: 0.0000; time: 0.84s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6926 score: 0.5102 time: 0.53s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6939 score: 0.5000 time: 0.53s
Epoch 21/1000, LR 0.000270
Train loss: 0.6945;  Loss pred: 0.6945; Loss self: 0.0000; time: 0.68s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6924 score: 0.5102 time: 0.45s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6936 score: 0.5000 time: 0.42s
Epoch 22/1000, LR 0.000270
Train loss: 0.6941;  Loss pred: 0.6941; Loss self: 0.0000; time: 0.55s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6921 score: 0.5102 time: 0.53s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6933 score: 0.5000 time: 0.42s
Epoch 23/1000, LR 0.000270
Train loss: 0.6937;  Loss pred: 0.6937; Loss self: 0.0000; time: 0.60s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6918 score: 0.5102 time: 0.54s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6930 score: 0.5000 time: 0.51s
Epoch 24/1000, LR 0.000270
Train loss: 0.6932;  Loss pred: 0.6932; Loss self: 0.0000; time: 0.55s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6914 score: 0.5102 time: 0.53s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6926 score: 0.5000 time: 0.51s
Epoch 25/1000, LR 0.000270
Train loss: 0.6927;  Loss pred: 0.6927; Loss self: 0.0000; time: 0.71s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6910 score: 0.5102 time: 0.53s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6922 score: 0.5000 time: 0.53s
Epoch 26/1000, LR 0.000270
Train loss: 0.6922;  Loss pred: 0.6922; Loss self: 0.0000; time: 0.59s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6906 score: 0.5102 time: 0.59s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6918 score: 0.5000 time: 0.44s
Epoch 27/1000, LR 0.000270
Train loss: 0.6916;  Loss pred: 0.6916; Loss self: 0.0000; time: 0.56s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6902 score: 0.5102 time: 0.46s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6914 score: 0.5000 time: 0.54s
Epoch 28/1000, LR 0.000270
Train loss: 0.6912;  Loss pred: 0.6912; Loss self: 0.0000; time: 0.72s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6897 score: 0.5102 time: 0.69s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6908 score: 0.5000 time: 0.53s
Epoch 29/1000, LR 0.000270
Train loss: 0.6903;  Loss pred: 0.6903; Loss self: 0.0000; time: 0.69s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6891 score: 0.5102 time: 0.44s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6903 score: 0.5000 time: 0.44s
Epoch 30/1000, LR 0.000270
Train loss: 0.6894;  Loss pred: 0.6894; Loss self: 0.0000; time: 0.73s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6884 score: 0.5102 time: 0.50s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6897 score: 0.5000 time: 0.42s
Epoch 31/1000, LR 0.000270
Train loss: 0.6885;  Loss pred: 0.6885; Loss self: 0.0000; time: 0.72s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6877 score: 0.5102 time: 0.58s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6890 score: 0.5000 time: 0.55s
Epoch 32/1000, LR 0.000270
Train loss: 0.6875;  Loss pred: 0.6875; Loss self: 0.0000; time: 0.83s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6869 score: 0.5102 time: 0.55s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6882 score: 0.5000 time: 0.53s
Epoch 33/1000, LR 0.000270
Train loss: 0.6865;  Loss pred: 0.6865; Loss self: 0.0000; time: 0.67s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6859 score: 0.5102 time: 0.52s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6873 score: 0.5000 time: 0.45s
Epoch 34/1000, LR 0.000270
Train loss: 0.6850;  Loss pred: 0.6850; Loss self: 0.0000; time: 0.71s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6848 score: 0.5102 time: 0.42s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6863 score: 0.5000 time: 0.42s
Epoch 35/1000, LR 0.000270
Train loss: 0.6837;  Loss pred: 0.6837; Loss self: 0.0000; time: 0.60s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6836 score: 0.5102 time: 0.41s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6852 score: 0.5000 time: 0.46s
Epoch 36/1000, LR 0.000270
Train loss: 0.6822;  Loss pred: 0.6822; Loss self: 0.0000; time: 0.58s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6821 score: 0.5102 time: 0.67s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6839 score: 0.5000 time: 0.45s
Epoch 37/1000, LR 0.000270
Train loss: 0.6801;  Loss pred: 0.6801; Loss self: 0.0000; time: 0.58s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6806 score: 0.5102 time: 0.44s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6825 score: 0.5000 time: 0.41s
Epoch 38/1000, LR 0.000270
Train loss: 0.6787;  Loss pred: 0.6787; Loss self: 0.0000; time: 0.66s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6788 score: 0.5102 time: 0.46s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6810 score: 0.5000 time: 0.53s
Epoch 39/1000, LR 0.000269
Train loss: 0.6757;  Loss pred: 0.6757; Loss self: 0.0000; time: 0.72s
Val loss: 0.6769 score: 0.5306 time: 0.52s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6792 score: 0.5000 time: 0.53s
Epoch 40/1000, LR 0.000269
Train loss: 0.6733;  Loss pred: 0.6733; Loss self: 0.0000; time: 0.71s
Val loss: 0.6746 score: 0.6327 time: 0.69s
Test loss: 0.6773 score: 0.5625 time: 0.56s
Epoch 41/1000, LR 0.000269
Train loss: 0.6714;  Loss pred: 0.6714; Loss self: 0.0000; time: 0.69s
Val loss: 0.6721 score: 0.7959 time: 0.54s
Test loss: 0.6750 score: 0.7083 time: 0.54s
Epoch 42/1000, LR 0.000269
Train loss: 0.6678;  Loss pred: 0.6678; Loss self: 0.0000; time: 0.89s
Val loss: 0.6693 score: 0.8163 time: 0.53s
Test loss: 0.6725 score: 0.7917 time: 0.53s
Epoch 43/1000, LR 0.000269
Train loss: 0.6636;  Loss pred: 0.6636; Loss self: 0.0000; time: 0.70s
Val loss: 0.6664 score: 0.8571 time: 0.54s
Test loss: 0.6698 score: 0.7708 time: 0.53s
Epoch 44/1000, LR 0.000269
Train loss: 0.6591;  Loss pred: 0.6591; Loss self: 0.0000; time: 0.75s
Val loss: 0.6630 score: 0.8980 time: 0.42s
Test loss: 0.6669 score: 0.7917 time: 0.45s
Epoch 45/1000, LR 0.000269
Train loss: 0.6551;  Loss pred: 0.6551; Loss self: 0.0000; time: 0.62s
Val loss: 0.6592 score: 0.8980 time: 0.53s
Test loss: 0.6637 score: 0.7917 time: 0.53s
Epoch 46/1000, LR 0.000269
Train loss: 0.6490;  Loss pred: 0.6490; Loss self: 0.0000; time: 0.73s
Val loss: 0.6550 score: 0.8980 time: 0.66s
Test loss: 0.6600 score: 0.8125 time: 0.48s
Epoch 47/1000, LR 0.000269
Train loss: 0.6452;  Loss pred: 0.6452; Loss self: 0.0000; time: 0.54s
Val loss: 0.6503 score: 0.8980 time: 0.42s
Test loss: 0.6559 score: 0.8333 time: 0.42s
Epoch 48/1000, LR 0.000269
Train loss: 0.6394;  Loss pred: 0.6394; Loss self: 0.0000; time: 0.54s
Val loss: 0.6451 score: 0.8980 time: 0.53s
Test loss: 0.6515 score: 0.8958 time: 0.42s
Epoch 49/1000, LR 0.000269
Train loss: 0.6326;  Loss pred: 0.6326; Loss self: 0.0000; time: 0.54s
Val loss: 0.6394 score: 0.9184 time: 0.41s
Test loss: 0.6466 score: 0.8958 time: 0.49s
Epoch 50/1000, LR 0.000269
Train loss: 0.6261;  Loss pred: 0.6261; Loss self: 0.0000; time: 0.68s
Val loss: 0.6334 score: 0.9184 time: 0.66s
Test loss: 0.6413 score: 0.8958 time: 0.53s
Epoch 51/1000, LR 0.000269
Train loss: 0.6178;  Loss pred: 0.6178; Loss self: 0.0000; time: 0.69s
Val loss: 0.6267 score: 0.9184 time: 0.52s
Test loss: 0.6356 score: 0.8750 time: 0.52s
Epoch 52/1000, LR 0.000269
Train loss: 0.6098;  Loss pred: 0.6098; Loss self: 0.0000; time: 0.68s
Val loss: 0.6195 score: 0.9184 time: 0.54s
Test loss: 0.6294 score: 0.8750 time: 0.43s
Epoch 53/1000, LR 0.000269
Train loss: 0.6019;  Loss pred: 0.6019; Loss self: 0.0000; time: 0.64s
Val loss: 0.6118 score: 0.9388 time: 0.53s
Test loss: 0.6228 score: 0.8750 time: 0.52s
Epoch 54/1000, LR 0.000269
Train loss: 0.5919;  Loss pred: 0.5919; Loss self: 0.0000; time: 0.80s
Val loss: 0.6034 score: 0.9388 time: 0.45s
Test loss: 0.6157 score: 0.8750 time: 0.52s
Epoch 55/1000, LR 0.000269
Train loss: 0.5824;  Loss pred: 0.5824; Loss self: 0.0000; time: 0.68s
Val loss: 0.5946 score: 0.9388 time: 0.52s
Test loss: 0.6080 score: 0.8750 time: 0.53s
Epoch 56/1000, LR 0.000269
Train loss: 0.5663;  Loss pred: 0.5663; Loss self: 0.0000; time: 0.79s
Val loss: 0.5850 score: 0.9388 time: 0.48s
Test loss: 0.5999 score: 0.8750 time: 0.55s
Epoch 57/1000, LR 0.000269
Train loss: 0.5595;  Loss pred: 0.5595; Loss self: 0.0000; time: 0.76s
Val loss: 0.5748 score: 0.9388 time: 0.52s
Test loss: 0.5912 score: 0.8750 time: 0.53s
Epoch 58/1000, LR 0.000269
Train loss: 0.5430;  Loss pred: 0.5430; Loss self: 0.0000; time: 0.82s
Val loss: 0.5640 score: 0.9388 time: 0.53s
Test loss: 0.5819 score: 0.8750 time: 0.53s
Epoch 59/1000, LR 0.000268
Train loss: 0.5311;  Loss pred: 0.5311; Loss self: 0.0000; time: 0.69s
Val loss: 0.5527 score: 0.9388 time: 0.46s
Test loss: 0.5722 score: 0.8750 time: 0.78s
Epoch 60/1000, LR 0.000268
Train loss: 0.5188;  Loss pred: 0.5188; Loss self: 0.0000; time: 0.72s
Val loss: 0.5408 score: 0.9388 time: 0.67s
Test loss: 0.5622 score: 0.8750 time: 0.53s
Epoch 61/1000, LR 0.000268
Train loss: 0.5039;  Loss pred: 0.5039; Loss self: 0.0000; time: 0.71s
Val loss: 0.5283 score: 0.9388 time: 0.52s
Test loss: 0.5517 score: 0.8750 time: 0.52s
Epoch 62/1000, LR 0.000268
Train loss: 0.4890;  Loss pred: 0.4890; Loss self: 0.0000; time: 0.77s
Val loss: 0.5153 score: 0.9388 time: 0.54s
Test loss: 0.5409 score: 0.8750 time: 0.53s
Epoch 63/1000, LR 0.000268
Train loss: 0.4753;  Loss pred: 0.4753; Loss self: 0.0000; time: 0.73s
Val loss: 0.5016 score: 0.9388 time: 0.56s
Test loss: 0.5297 score: 0.8958 time: 0.54s
Epoch 64/1000, LR 0.000268
Train loss: 0.4570;  Loss pred: 0.4570; Loss self: 0.0000; time: 0.62s
Val loss: 0.4875 score: 0.9388 time: 0.52s
Test loss: 0.5183 score: 0.8958 time: 0.42s
Epoch 65/1000, LR 0.000268
Train loss: 0.4452;  Loss pred: 0.4452; Loss self: 0.0000; time: 0.55s
Val loss: 0.4729 score: 0.9388 time: 0.43s
Test loss: 0.5066 score: 0.8958 time: 0.58s
Epoch 66/1000, LR 0.000268
Train loss: 0.4193;  Loss pred: 0.4193; Loss self: 0.0000; time: 0.82s
Val loss: 0.4578 score: 0.9388 time: 0.52s
Test loss: 0.4948 score: 0.8958 time: 0.56s
Epoch 67/1000, LR 0.000268
Train loss: 0.4090;  Loss pred: 0.4090; Loss self: 0.0000; time: 0.68s
Val loss: 0.4424 score: 0.9388 time: 0.52s
Test loss: 0.4830 score: 0.8958 time: 0.48s
Epoch 68/1000, LR 0.000268
Train loss: 0.3914;  Loss pred: 0.3914; Loss self: 0.0000; time: 0.66s
Val loss: 0.4268 score: 0.9388 time: 0.45s
Test loss: 0.4711 score: 0.8958 time: 0.43s
Epoch 69/1000, LR 0.000268
Train loss: 0.3732;  Loss pred: 0.3732; Loss self: 0.0000; time: 0.58s
Val loss: 0.4109 score: 0.9388 time: 0.42s
Test loss: 0.4593 score: 0.9167 time: 0.42s
Epoch 70/1000, LR 0.000268
Train loss: 0.3508;  Loss pred: 0.3508; Loss self: 0.0000; time: 0.59s
Val loss: 0.3948 score: 0.9388 time: 0.66s
Test loss: 0.4476 score: 0.9167 time: 0.53s
Epoch 71/1000, LR 0.000268
Train loss: 0.3328;  Loss pred: 0.3328; Loss self: 0.0000; time: 0.69s
Val loss: 0.3786 score: 0.9388 time: 0.44s
Test loss: 0.4361 score: 0.9167 time: 0.46s
Epoch 72/1000, LR 0.000267
Train loss: 0.3141;  Loss pred: 0.3141; Loss self: 0.0000; time: 0.68s
Val loss: 0.3625 score: 0.9388 time: 0.65s
Test loss: 0.4248 score: 0.9167 time: 0.52s
Epoch 73/1000, LR 0.000267
Train loss: 0.3004;  Loss pred: 0.3004; Loss self: 0.0000; time: 0.71s
Val loss: 0.3465 score: 0.9388 time: 0.42s
Test loss: 0.4140 score: 0.9167 time: 0.41s
Epoch 74/1000, LR 0.000267
Train loss: 0.2859;  Loss pred: 0.2859; Loss self: 0.0000; time: 0.57s
Val loss: 0.3308 score: 0.9388 time: 0.60s
Test loss: 0.4038 score: 0.8958 time: 0.44s
Epoch 75/1000, LR 0.000267
Train loss: 0.2648;  Loss pred: 0.2648; Loss self: 0.0000; time: 0.74s
Val loss: 0.3155 score: 0.9388 time: 3.49s
Test loss: 0.3945 score: 0.8958 time: 1.71s
Epoch 76/1000, LR 0.000267
Train loss: 0.2525;  Loss pred: 0.2525; Loss self: 0.0000; time: 0.54s
Val loss: 0.3006 score: 0.9388 time: 0.54s
Test loss: 0.3860 score: 0.8958 time: 0.41s
Epoch 77/1000, LR 0.000267
Train loss: 0.2293;  Loss pred: 0.2293; Loss self: 0.0000; time: 0.57s
Val loss: 0.2863 score: 0.9592 time: 0.41s
Test loss: 0.3788 score: 0.8958 time: 0.41s
Epoch 78/1000, LR 0.000267
Train loss: 0.2228;  Loss pred: 0.2228; Loss self: 0.0000; time: 0.65s
Val loss: 0.2727 score: 0.9592 time: 0.41s
Test loss: 0.3728 score: 0.8958 time: 0.41s
Epoch 79/1000, LR 0.000267
Train loss: 0.2024;  Loss pred: 0.2024; Loss self: 0.0000; time: 0.58s
Val loss: 0.2599 score: 0.9592 time: 0.42s
Test loss: 0.3684 score: 0.8958 time: 0.41s
Epoch 80/1000, LR 0.000267
Train loss: 0.1977;  Loss pred: 0.1977; Loss self: 0.0000; time: 0.65s
Val loss: 0.2479 score: 0.9388 time: 0.41s
Test loss: 0.3653 score: 0.8958 time: 0.45s
Epoch 81/1000, LR 0.000267
Train loss: 0.1788;  Loss pred: 0.1788; Loss self: 0.0000; time: 0.54s
Val loss: 0.2367 score: 0.9388 time: 0.41s
Test loss: 0.3630 score: 0.8958 time: 0.42s
Epoch 82/1000, LR 0.000267
Train loss: 0.1690;  Loss pred: 0.1690; Loss self: 0.0000; time: 0.66s
Val loss: 0.2264 score: 0.9388 time: 0.42s
Test loss: 0.3614 score: 0.8958 time: 0.42s
Epoch 83/1000, LR 0.000266
Train loss: 0.1577;  Loss pred: 0.1577; Loss self: 0.0000; time: 0.54s
Val loss: 0.2170 score: 0.9388 time: 0.44s
Test loss: 0.3601 score: 0.8958 time: 0.41s
Epoch 84/1000, LR 0.000266
Train loss: 0.1430;  Loss pred: 0.1430; Loss self: 0.0000; time: 0.57s
Val loss: 0.2086 score: 0.9592 time: 0.53s
Test loss: 0.3600 score: 0.8958 time: 0.44s
Epoch 85/1000, LR 0.000266
Train loss: 0.1356;  Loss pred: 0.1356; Loss self: 0.0000; time: 0.53s
Val loss: 0.2012 score: 0.9592 time: 0.40s
Test loss: 0.3602 score: 0.8958 time: 0.41s
Epoch 86/1000, LR 0.000266
Train loss: 0.1301;  Loss pred: 0.1301; Loss self: 0.0000; time: 0.63s
Val loss: 0.1948 score: 0.9592 time: 0.41s
Test loss: 0.3614 score: 0.8958 time: 0.41s
Epoch 87/1000, LR 0.000266
Train loss: 0.1280;  Loss pred: 0.1280; Loss self: 0.0000; time: 0.54s
Val loss: 0.1893 score: 0.9592 time: 0.51s
Test loss: 0.3629 score: 0.8958 time: 0.44s
Epoch 88/1000, LR 0.000266
Train loss: 0.1172;  Loss pred: 0.1172; Loss self: 0.0000; time: 0.53s
Val loss: 0.1849 score: 0.9592 time: 0.52s
Test loss: 0.3647 score: 0.8958 time: 0.41s
Epoch 89/1000, LR 0.000266
Train loss: 0.1130;  Loss pred: 0.1130; Loss self: 0.0000; time: 0.53s
Val loss: 0.1810 score: 0.9592 time: 0.41s
Test loss: 0.3681 score: 0.8958 time: 0.40s
Epoch 90/1000, LR 0.000266
Train loss: 0.1093;  Loss pred: 0.1093; Loss self: 0.0000; time: 0.72s
Val loss: 0.1780 score: 0.9592 time: 0.46s
Test loss: 0.3718 score: 0.8958 time: 0.41s
Epoch 91/1000, LR 0.000266
Train loss: 0.1003;  Loss pred: 0.1003; Loss self: 0.0000; time: 0.55s
Val loss: 0.1758 score: 0.9592 time: 0.43s
Test loss: 0.3755 score: 0.8958 time: 0.44s
Epoch 92/1000, LR 0.000266
Train loss: 0.0964;  Loss pred: 0.0964; Loss self: 0.0000; time: 0.63s
Val loss: 0.1742 score: 0.9592 time: 0.44s
Test loss: 0.3791 score: 0.8958 time: 0.43s
Epoch 93/1000, LR 0.000265
Train loss: 0.0964;  Loss pred: 0.0964; Loss self: 0.0000; time: 0.58s
Val loss: 0.1733 score: 0.9388 time: 0.44s
Test loss: 0.3830 score: 0.8958 time: 0.43s
Epoch 94/1000, LR 0.000265
Train loss: 0.0944;  Loss pred: 0.0944; Loss self: 0.0000; time: 0.56s
Val loss: 0.1726 score: 0.9388 time: 0.52s
Test loss: 0.3879 score: 0.8958 time: 0.42s
Epoch 95/1000, LR 0.000265
Train loss: 0.0890;  Loss pred: 0.0890; Loss self: 0.0000; time: 0.58s
Val loss: 0.1728 score: 0.9388 time: 0.41s
Test loss: 0.3916 score: 0.8958 time: 0.49s
     INFO: Early stopping counter 1 of 20
Epoch 96/1000, LR 0.000265
Train loss: 0.0909;  Loss pred: 0.0909; Loss self: 0.0000; time: 0.55s
Val loss: 0.1729 score: 0.9388 time: 0.52s
Test loss: 0.3967 score: 0.8958 time: 0.46s
     INFO: Early stopping counter 2 of 20
Epoch 97/1000, LR 0.000265
Train loss: 0.0837;  Loss pred: 0.0837; Loss self: 0.0000; time: 0.55s
Val loss: 0.1732 score: 0.9388 time: 0.42s
Test loss: 0.4020 score: 0.8958 time: 0.42s
     INFO: Early stopping counter 3 of 20
Epoch 98/1000, LR 0.000265
Train loss: 0.0816;  Loss pred: 0.0816; Loss self: 0.0000; time: 0.57s
Val loss: 0.1747 score: 0.9388 time: 0.54s
Test loss: 0.4050 score: 0.8958 time: 0.43s
     INFO: Early stopping counter 4 of 20
Epoch 99/1000, LR 0.000265
Train loss: 0.0813;  Loss pred: 0.0813; Loss self: 0.0000; time: 0.56s
Val loss: 0.1768 score: 0.9388 time: 0.42s
Test loss: 0.4072 score: 0.8958 time: 0.44s
     INFO: Early stopping counter 5 of 20
Epoch 100/1000, LR 0.000265
Train loss: 0.0789;  Loss pred: 0.0789; Loss self: 0.0000; time: 0.56s
Val loss: 0.1794 score: 0.9388 time: 0.67s
Test loss: 0.4089 score: 0.8958 time: 0.43s
     INFO: Early stopping counter 6 of 20
Epoch 101/1000, LR 0.000265
Train loss: 0.0761;  Loss pred: 0.0761; Loss self: 0.0000; time: 0.55s
Val loss: 0.1823 score: 0.9388 time: 0.42s
Test loss: 0.4105 score: 0.8958 time: 0.41s
     INFO: Early stopping counter 7 of 20
Epoch 102/1000, LR 0.000264
Train loss: 0.0716;  Loss pred: 0.0716; Loss self: 0.0000; time: 0.70s
Val loss: 0.1860 score: 0.9388 time: 0.41s
Test loss: 0.4107 score: 0.8958 time: 0.41s
     INFO: Early stopping counter 8 of 20
Epoch 103/1000, LR 0.000264
Train loss: 0.0743;  Loss pred: 0.0743; Loss self: 0.0000; time: 0.55s
Val loss: 0.1900 score: 0.9388 time: 0.43s
Test loss: 0.4109 score: 0.8958 time: 0.45s
     INFO: Early stopping counter 9 of 20
Epoch 104/1000, LR 0.000264
Train loss: 0.0707;  Loss pred: 0.0707; Loss self: 0.0000; time: 0.64s
Val loss: 0.1947 score: 0.9388 time: 0.42s
Test loss: 0.4110 score: 0.8958 time: 0.41s
     INFO: Early stopping counter 10 of 20
Epoch 105/1000, LR 0.000264
Train loss: 0.0658;  Loss pred: 0.0658; Loss self: 0.0000; time: 0.73s
Val loss: 0.2004 score: 0.9388 time: 0.47s
Test loss: 0.4108 score: 0.8958 time: 0.43s
     INFO: Early stopping counter 11 of 20
Epoch 106/1000, LR 0.000264
Train loss: 0.0698;  Loss pred: 0.0698; Loss self: 0.0000; time: 0.83s
Val loss: 0.2055 score: 0.9388 time: 0.53s
Test loss: 0.4117 score: 0.8958 time: 0.56s
     INFO: Early stopping counter 12 of 20
Epoch 107/1000, LR 0.000264
Train loss: 0.0667;  Loss pred: 0.0667; Loss self: 0.0000; time: 0.65s
Val loss: 0.2111 score: 0.9388 time: 0.57s
Test loss: 0.4126 score: 0.8958 time: 0.42s
     INFO: Early stopping counter 13 of 20
Epoch 108/1000, LR 0.000264
Train loss: 0.0658;  Loss pred: 0.0658; Loss self: 0.0000; time: 0.63s
Val loss: 0.2166 score: 0.9388 time: 0.55s
Test loss: 0.4137 score: 0.8958 time: 0.42s
     INFO: Early stopping counter 14 of 20
Epoch 109/1000, LR 0.000264
Train loss: 0.0632;  Loss pred: 0.0632; Loss self: 0.0000; time: 0.53s
Val loss: 0.2235 score: 0.9388 time: 0.42s
Test loss: 0.4145 score: 0.8958 time: 0.50s
     INFO: Early stopping counter 15 of 20
Epoch 110/1000, LR 0.000263
Train loss: 0.0716;  Loss pred: 0.0716; Loss self: 0.0000; time: 0.83s
Val loss: 0.2277 score: 0.9388 time: 0.52s
Test loss: 0.4168 score: 0.8958 time: 0.55s
     INFO: Early stopping counter 16 of 20
Epoch 111/1000, LR 0.000263
Train loss: 0.0535;  Loss pred: 0.0535; Loss self: 0.0000; time: 0.68s
Val loss: 0.2321 score: 0.9388 time: 0.55s
Test loss: 0.4192 score: 0.8958 time: 0.44s
     INFO: Early stopping counter 17 of 20
Epoch 112/1000, LR 0.000263
Train loss: 0.0581;  Loss pred: 0.0581; Loss self: 0.0000; time: 0.55s
Val loss: 0.2359 score: 0.9388 time: 0.53s
Test loss: 0.4218 score: 0.8750 time: 0.53s
     INFO: Early stopping counter 18 of 20
Epoch 113/1000, LR 0.000263
Train loss: 0.0634;  Loss pred: 0.0634; Loss self: 0.0000; time: 0.72s
Val loss: 0.2404 score: 0.9388 time: 0.53s
Test loss: 0.4241 score: 0.8542 time: 0.54s
     INFO: Early stopping counter 19 of 20
Epoch 114/1000, LR 0.000263
Train loss: 0.0584;  Loss pred: 0.0584; Loss self: 0.0000; time: 0.72s
Val loss: 0.2444 score: 0.9388 time: 0.45s
Test loss: 0.4267 score: 0.8542 time: 0.53s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 093,   Train_Loss: 0.0944,   Val_Loss: 0.1726,   Val_Precision: 0.9231,   Val_Recall: 0.9600,   Val_accuracy: 0.9412,   Val_Score: 0.9388,   Val_Loss: 0.1726,   Test_Precision: 0.9524,   Test_Recall: 0.8333,   Test_accuracy: 0.8889,   Test_Score: 0.8958,   Test_loss: 0.3879


[0.5557976951822639, 0.43661666894331574, 0.4353777680080384, 0.4317563269287348, 0.4438797659240663, 0.4320737069938332, 0.4727512230165303, 0.5485460001509637, 0.4482768240850419, 0.5608840240165591, 0.48588921898044646, 0.4267306460533291, 0.4454654648434371, 0.4404641829896718, 0.537623907905072, 0.5616491201799363, 0.582398243015632, 0.5550781718920916, 0.49320246395654976, 0.49269940704107285, 0.5611588661558926, 0.5706078989896923, 0.5410676030442119, 0.4632785180583596, 0.5534162679687142, 0.43911505909636617, 0.4633998649660498, 0.5480853549670428, 0.5008167149499059, 0.5400644200854003, 0.5459566009230912, 0.5562938149087131, 0.48659669305197895, 0.4323171761352569, 0.4993087421171367, 0.5483128689229488, 0.5571511599700898, 0.5228511129971594, 0.4557801310438663, 0.4305635979399085, 0.5801630278583616, 0.44886294286698103, 0.4920732250902802, 0.44439376913942397, 0.5563390129245818, 0.5530433000531048, 0.45926043996587396, 0.4479618698824197, 0.5708832328673452, 0.44464431912638247, 0.6204402258154005, 0.5249992969911546, 0.4398067800793797, 0.5094195900019258, 0.4375690561719239, 0.5176349140238017, 0.5065021889749914, 2.2598491290118545, 0.4280350140761584, 0.434902855893597, 0.4613353000022471, 0.4287063810043037, 0.49012007378041744, 0.42398875020444393, 0.42902415501885116, 0.43311849096789956, 0.4422965890262276, 0.4257883110549301, 0.4370779939927161, 0.43648961186408997, 0.44039573799818754, 0.44880396709777415, 0.5922640089411288, 0.43510395591147244, 0.450994161888957, 0.43270232900977135, 0.479936117073521, 0.478334496030584, 0.5486893369816244, 0.4990654019638896, 0.4422760361339897, 0.5806497561279684, 0.4370005859527737, 0.5204990520142019, 0.5533135170117021, 0.5520911908242851, 0.4769732530694455, 0.44074963801540434, 0.4388426698278636, 0.5728680200409144, 0.5015784751158208, 0.5607310289051384, 0.5564900110475719, 0.5168376499786973, 0.562678060028702, 0.5504499061498791, 0.42627913504838943, 0.5746080910321325, 0.5293126830365509, 0.5534459128975868, 0.4745249031111598, 0.4360520930495113, 0.444812930887565, 0.4381598262116313, 0.4385390488896519, 0.5460027479566634, 0.4506459750700742, 0.5650667939335108, 0.4491341558750719, 0.4365639640018344, 0.4555418479721993, 0.5529362678062171, 0.4822837598621845, 0.7583583300001919, 0.5989137401338667, 0.47944922489114106, 0.4910283461213112, 0.6020567349623889, 0.6074364220257849, 0.718606858048588, 0.4780277528334409, 0.637462595012039, 0.6365801920183003, 0.4789469849783927, 0.5278028161264956, 0.4825568380765617, 0.5786859919317067, 0.7683905391022563, 0.4691070099361241, 0.46668914798647165, 0.6012188559398055, 0.4720959479454905, 0.4950640699826181, 0.6020180720370263, 0.6835721919778734, 0.48742848192341626, 0.47215264895930886, 0.47643985715694726, 0.475682889809832, 0.46711748698726296, 3.3091608139220625, 0.4986285651102662, 0.486366885015741, 0.4688519879709929, 0.5051073029171675, 0.49168322305195034, 0.4790174029767513, 0.6262954080011696, 0.4771258740220219, 0.6408773090224713, 0.4951875729020685, 0.4724078020080924, 0.47556717600673437, 0.4785402228590101, 0.6581305279396474, 0.5957893370650709, 0.5967928320169449, 0.6392666189931333, 0.6430167679209262, 0.49071962386369705, 0.6424030761700124, 0.4808319378644228, 0.6456010669935495, 0.48804523004218936, 0.750218243105337, 0.6310687300283462, 0.6423485151026398, 0.6254836220759898, 0.5995722799561918, 0.48207449703477323, 0.48554276092909276, 0.6262974929995835, 0.4745055618695915, 0.6141032457817346, 0.48354335804469883, 0.5117538359481841, 0.5026454669423401, 0.49052343890070915, 0.7166361431591213, 0.5386644429527223, 0.48001237912103534, 0.5564263160340488, 0.7301839389838278, 0.49888141197152436, 0.6333513229619712, 0.4762881558854133, 0.4847057741135359, 0.6499581870157272, 0.7469557460863143, 0.4859416000545025, 0.5073359259404242, 0.4799491078592837, 0.5101077749859542, 0.5129916539881378, 0.5968837970867753, 0.6296080281026661, 0.5960862149950117, 0.7158494079485536, 0.5619503308553249, 0.5937542859464884, 0.4737936691381037, 0.5127384918741882, 0.6283517039846629, 0.5086855289991945, 0.5981908249668777, 0.5133480390068144, 0.8083920211065561, 0.6074237790890038, 0.6182883800938725, 0.5329005608800799, 0.47965471795760095, 0.4845176939852536, 0.6191656931769103, 0.6463701720349491, 0.5951154849026352, 0.47987548308447003, 0.4699512810911983, 0.5166563270613551, 0.5769857580307871, 0.7790642951149493, 0.6211484530940652, 0.6271306800190359, 0.47251242212951183, 0.4760751340072602, 0.4753374429419637, 3.3032918989192694, 0.6492060769814998, 0.47541322093456984, 0.4715743060223758, 0.4115197700448334, 0.5293153568636626, 0.4211966269649565, 0.4260684170294553, 0.421270159073174, 0.4193603179883212, 0.45351545698940754, 0.41119520692154765, 0.5010980879887938, 0.42885407619178295, 0.4138127558398992, 0.42169347894378006, 0.4237152221612632, 0.5350918029434979, 0.5060861818492413, 0.533825783059001, 0.417946649948135, 0.4264239261392504, 0.5287859130185097, 0.5367899779230356, 0.4207750139757991, 0.427564999088645, 0.5190976101439446, 0.5149158760905266, 0.5389143119100481, 0.441240242915228, 0.5494249409530312, 0.5358545929193497, 0.44469753303565085, 0.42063189996406436, 0.5528715630061924, 0.5378601311240345, 0.45055541885085404, 0.4234605359379202, 0.4673547809943557, 0.4518741490319371, 0.41694884304888546, 0.5378225948661566, 0.5340285850688815, 0.563338756095618, 0.5504611418582499, 0.5406684060581028, 0.5373880169354379, 0.4617957449518144, 0.5345748108811677, 0.485295157879591, 0.42397331283427775, 0.42397117405198514, 0.49268828984349966, 0.5390862820204347, 0.5317556639201939, 0.4316715269815177, 0.530900718877092, 0.5314086570870131, 0.5324041179846972, 0.5631647298578173, 0.5404330019373447, 0.5393659290857613, 0.7864899460691959, 0.534148502163589, 0.5320907440036535, 0.539114051964134, 0.544962293934077, 0.4208128908649087, 0.5917057890910655, 0.5658474878873676, 0.4849503650330007, 0.43833646597340703, 0.4246535759884864, 0.537514132913202, 0.4651651040185243, 0.5296892900951207, 0.4177459590137005, 0.44124582200311124, 1.714258013991639, 0.41307122982107103, 0.41527882497757673, 0.4171509628649801, 0.41945718391798437, 0.45166574511677027, 0.4219237440265715, 0.428465394070372, 0.418350568972528, 0.44460124685429037, 0.4169112020172179, 0.4143123710528016, 0.44610302802175283, 0.4134580339305103, 0.40932903508655727, 0.41298173600807786, 0.44404771900735795, 0.433782747015357, 0.43029488204047084, 0.4218755450565368, 0.49442431307397783, 0.4615215230733156, 0.4211583810392767, 0.432107871863991, 0.4407337261363864, 0.4336634420324117, 0.41129621490836143, 0.41939321300014853, 0.453841753071174, 0.4179842558223754, 0.4321288429200649, 0.5714588919654489, 0.42396568483673036, 0.4238523789681494, 0.5024095010012388, 0.5590561619028449, 0.4421518468298018, 0.5353097000624985, 0.5422730108257383, 0.538707252824679]
[0.011342810105760487, 0.0089105442641493, 0.008885260571592621, 0.008811353610790506, 0.009058770733144211, 0.008817830754976188, 0.009647984143194496, 0.011194816329611503, 0.009148506613980447, 0.011446612735031819, 0.009916106509805029, 0.0087087886949659, 0.009091131935580348, 0.008989064958972894, 0.010971916487858611, 0.01146222694244768, 0.01188567842889045, 0.011328125956981462, 0.010065356407276526, 0.010055089939613732, 0.011452221758283523, 0.011645059163054946, 0.01104219598049412, 0.009454663633844075, 0.011294209550381924, 0.008961531818293187, 0.009457140101347955, 0.01118541540749067, 0.010220749284691957, 0.01102172285888572, 0.011141971447410024, 0.011352934998137002, 0.009930544756162835, 0.008822799512964427, 0.010189974328921157, 0.011190058549447936, 0.011370431836124281, 0.010670430877493049, 0.009301635327425842, 0.008787012202855276, 0.011840061793027788, 0.009160468221775122, 0.010042310716128168, 0.009069260594682122, 0.011353857406624118, 0.011286597960267444, 0.009372662040119877, 0.009142078977192238, 0.011650678221782555, 0.00907437385972209, 0.012662045424804091, 0.01071427136716642, 0.008975648573048564, 0.01039631816330461, 0.008929980738202527, 0.010563977837220443, 0.01033677936683656, 0.04611936997983376, 0.008735408450533845, 0.008875568487624429, 0.009415006122494839, 0.008749109816414361, 0.010002450485314642, 0.008652831636825386, 0.008755595000384718, 0.00883915287689591, 0.009026461000535257, 0.008689557368467962, 0.008919959061075839, 0.008907951262532448, 0.008987668122411991, 0.009159264634648452, 0.012087020590635282, 0.008879672569621886, 0.009203962487529735, 0.00883065977570962, 0.00979461463415349, 0.00976192849042008, 0.01119774157105356, 0.010185008203344685, 0.009026041553754891, 0.011849995023019763, 0.008918379305158647, 0.010622429632942895, 0.011292112592075552, 0.011267167159679289, 0.009734148021825418, 0.008994890571742947, 0.00895597285362987, 0.01169118408246764, 0.010236295410526954, 0.01144349038581915, 0.011356939000970855, 0.010547707142422393, 0.011483225714871469, 0.011233671554079165, 0.008699574184661009, 0.011726695735349643, 0.01080229965380716, 0.011294814548930344, 0.009684181696146118, 0.008899022307132885, 0.009077814916072756, 0.008942037269625128, 0.00894977650795208, 0.011142913223605375, 0.009196856634083147, 0.011531975386398179, 0.009166003181123915, 0.00890946865309866, 0.009296772407595905, 0.011284413628698307, 0.009842525711473154, 0.015476700612248815, 0.012222729390487075, 0.009784678059002879, 0.010020986655536962, 0.012286872142089568, 0.012396661673995609, 0.014665446082624246, 0.009755668425172264, 0.013009440714531407, 0.012991432490169394, 0.009774428264865158, 0.010771486043397869, 0.009848098736256361, 0.011809918202687892, 0.015681439573515435, 0.009573612447676003, 0.009524268326254524, 0.012269772570200113, 0.00963461118256103, 0.010103348366992206, 0.012286083102796455, 0.01395045289750762, 0.009947520039253394, 0.009635768346108345, 0.009723262390958108, 0.009707814077751674, 0.009533009938515571, 0.06753389416167474, 0.010176093165515637, 0.009925854796239612, 0.009568407917775365, 0.01030831230443199, 0.010034351490856129, 0.009775865366872477, 0.012781538938799379, 0.009737262735143304, 0.013079128755560639, 0.010105868834736092, 0.009640975551185559, 0.009705452571566008, 0.009766126997122655, 0.013431235264074437, 0.012158966062552467, 0.012179445551366222, 0.013046257530472108, 0.013122791182059718, 0.01001468620129994, 0.013110266860612497, 0.00981289669111067, 0.013175531979460193, 0.009960106735554884, 0.015310576389904837, 0.012878953674047882, 0.013109153369441628, 0.012764971879101833, 0.012236168978697792, 0.009838255041525985, 0.009909035937328423, 0.012781581489787417, 0.009683786976930439, 0.012532719301668053, 0.00986823179683059, 0.010443955835677227, 0.010258070753925309, 0.010010682426545084, 0.014625227411410638, 0.010993151896994333, 0.009796171002470109, 0.01135563910273569, 0.014901713040486281, 0.010181253305541314, 0.012925537203305535, 0.009720166446641088, 0.00989195457374563, 0.01326445279623933, 0.015243994818088047, 0.009917175511316377, 0.010353794406947434, 0.00979487975223028, 0.010410362754815392, 0.010469217428329344, 0.01218130198136276, 0.012849143430666655, 0.012165024795816565, 0.014609171590786807, 0.011468374099088262, 0.01211743440707119, 0.009669258553838852, 0.010464050854575269, 0.012823504162952304, 0.010381337326514174, 0.012207976019732197, 0.010476490591975803, 0.01649779634911339, 0.012396403654877628, 0.0126181302059974, 0.010875521650613874, 0.00978887179505308, 0.009888116203780686, 0.012636034554630821, 0.013191228000713246, 0.012145213977604801, 0.00979337720580551, 0.009590842471248945, 0.010544006674721534, 0.011775219551648716, 0.015899271328876516, 0.012676499042736024, 0.012798585306510938, 0.009643110655704322, 0.009715819061372657, 0.009700764141672728, 0.06741412038610754, 0.013249103611867343, 0.009702310631317752, 0.009623965429028079, 0.008573328542600697, 0.011027403267992971, 0.008774929728436595, 0.008876425354780318, 0.008776461647357792, 0.008736673291423358, 0.009448238687279323, 0.008566566810865575, 0.010439543499766538, 0.008934459920662144, 0.008621099079997899, 0.008785280811328752, 0.008827400461692983, 0.011147745894656206, 0.010543462121859193, 0.011121370480395854, 0.00870722187391948, 0.008883831794567717, 0.01101637318788562, 0.011183124540063242, 0.008766146124495814, 0.008907604147680104, 0.010814533544665514, 0.010727414085219303, 0.011227381498126002, 0.009192505060733916, 0.011446352936521484, 0.011163637352486452, 0.009264531938242726, 0.008763164582584674, 0.011518157562629009, 0.011205419398417385, 0.00938657122605946, 0.00882209449870667, 0.00973655793738241, 0.00941404477149869, 0.008686434230185114, 0.011204637393044928, 0.011125595522268364, 0.011736224085325375, 0.011467940455380207, 0.011263925126210475, 0.011195583686154956, 0.009620744686496133, 0.011136975226690993, 0.010110315789158145, 0.00883277735071412, 0.00883273279274969, 0.010264339371739576, 0.011230964208759056, 0.011078242998337373, 0.008993156812114952, 0.01106043164327275, 0.011071013689312773, 0.011091752458014525, 0.011732598538704527, 0.01125902087369468, 0.011236790189286694, 0.016385207209774915, 0.01112809379507477, 0.011085223833409449, 0.01123154274925279, 0.011353381123626605, 0.008766935226352265, 0.012327203939397199, 0.011788489330986826, 0.010103132604854181, 0.009132009707779313, 0.008846949499760134, 0.011198211102358377, 0.00969093966705259, 0.011035193543648347, 0.008703040812785426, 0.009192621291731484, 0.03571370862482581, 0.008605650621272313, 0.008651642187032849, 0.008690645059687085, 0.008738691331624674, 0.009409703023266047, 0.008790078000553573, 0.008926362376466082, 0.008715636853594333, 0.00926252597613105, 0.008685650042025372, 0.0086315077302667, 0.009293813083786517, 0.008613709040218964, 0.008527688230969943, 0.008603786166834956, 0.009250994145986624, 0.009037140562819937, 0.008964476709176475, 0.008789073855344517, 0.010300506522374539, 0.009615031730694076, 0.008774132938318266, 0.009002247330499813, 0.009181952627841383, 0.00903465504234191, 0.008568671143924197, 0.008737358604169762, 0.009455036522316126, 0.00870800532963282, 0.009002684227501353, 0.011905393582613518, 0.00883261843409855, 0.00883025789516978, 0.010466864604192475, 0.011647003372975936, 0.009211496808954204, 0.011152285417968718, 0.011297354392202882, 0.011223067767180813]
[88.161574660599, 112.22659024582839, 112.54593964268591, 113.48994083898599, 110.3902537616061, 113.40657671793798, 103.6485948938236, 89.3270573233874, 109.30745772996796, 87.36208895576131, 100.84603256441443, 114.82653156782277, 109.99730364557328, 111.24627584338445, 91.14178011713703, 87.24308156007046, 84.13486920269573, 88.27585461156578, 99.35067965174807, 99.45211887765723, 87.31930110213668, 85.87332928050678, 90.56169640228136, 105.76790869855836, 88.54094618478051, 111.58806555355845, 105.7402120813952, 89.40213336469544, 97.8401849165542, 90.72991698333253, 89.75072362373108, 88.08294948963403, 100.6994102090327, 113.34270925351719, 98.13567411664647, 89.36503733033061, 87.94740731156429, 93.71692778679466, 107.50797733937205, 113.80432585208625, 84.45901866735748, 109.1647256220948, 99.5786754928802, 110.2625720763127, 88.07579346703575, 88.60065748069796, 106.69327409005884, 109.38430990312064, 85.83191304093882, 110.20044087434432, 78.97618168712833, 93.333458312851, 111.41256165072332, 96.18789885919927, 111.98232440994997, 94.66131180971092, 96.74193136097064, 21.68286341372967, 114.47661613795371, 112.66883934187892, 106.213420043429, 114.29734235634832, 99.97550115026074, 115.56910407734308, 114.21268342768943, 113.13301330196872, 110.78538974917205, 115.08066033705329, 112.10813784602614, 112.25925810866069, 111.26356540762359, 109.17907057921596, 82.73337440781516, 112.61676510697983, 108.64885655008699, 113.24182172103232, 102.09692135441794, 102.4387753896533, 89.30372197417245, 98.18352425789966, 110.79053802760231, 84.38822109692056, 112.12799610592603, 94.14042121764139, 88.55738834040393, 88.7534538032419, 102.73112734240841, 111.17422630370359, 111.65732817007128, 85.53453550522929, 97.69159250441376, 87.38592564723143, 88.05189496170706, 94.80733457018786, 87.08354471382894, 89.01809129686372, 114.94815479166655, 85.27551345819786, 92.57288096498603, 88.53620355322286, 103.26117697667283, 112.37189496631156, 110.15866805451681, 111.83133886020164, 111.73463372090659, 89.74313807645738, 108.73280293334615, 86.71541227702302, 109.09880568876063, 112.24013899551753, 107.56421219723013, 88.61780796981944, 101.59993779181377, 64.61325479208173, 81.81478686571413, 102.20060322576484, 99.79057296194115, 81.38767852677718, 80.66687841434707, 68.18749285675048, 102.50450880636016, 76.86725524510909, 76.9738056797585, 102.30777421474026, 92.83770094219513, 101.54244253446004, 84.67459154563863, 63.76965554163226, 104.45377912104124, 104.99494194671183, 81.50110316052016, 103.79246043784659, 98.97708795897955, 81.39290542259057, 71.68226059375172, 100.5275682837483, 103.77999595682228, 102.84613947371447, 103.0098013817339, 104.89866332350795, 14.807379500521925, 98.26954055302505, 100.74699061473756, 104.5105945099065, 97.00909037942677, 99.65766107667812, 102.29273445077354, 78.2378401214599, 102.6982661555227, 76.45769215130989, 98.95240244587187, 103.72394315189702, 103.0348654662115, 102.39473644922137, 74.45331574786552, 82.24383511356518, 82.10554378543328, 76.65033421763312, 76.20330051179269, 99.85335335521513, 76.27609801020337, 101.90670823080023, 75.89826365712865, 100.40053049133205, 65.3143274644682, 77.6460592458749, 76.28257690012776, 78.33938135321311, 81.72492564796396, 101.64404112102514, 100.91799104622181, 78.23757966093693, 103.26538598817665, 79.79114316131728, 101.33527673328196, 95.74916015863792, 97.48421745066872, 99.89328972701443, 68.37500517905093, 90.96572205769418, 102.08070068885583, 88.0619744034565, 67.10637879572047, 98.21973483910214, 77.36622349005837, 102.87889672358041, 101.092255584565, 75.3894650130999, 65.59960246204173, 100.83516207401102, 96.58294927403585, 102.09415789635412, 96.0581320317049, 95.51812318789314, 82.09303090342785, 77.82619949695096, 82.20287395911353, 68.45015090593121, 87.19631844582922, 82.52572008282915, 103.42054609792018, 95.5652847924342, 77.98180491796043, 96.3267032510327, 81.91366024832155, 95.4518110068198, 60.6141559053577, 80.6685574171771, 79.2510446218648, 91.94961236121988, 102.15681857283712, 101.13149758673484, 79.1387516136162, 75.8079535844525, 82.33696020868406, 102.10982166674846, 104.26612708922715, 94.84060764087197, 84.92410656241091, 62.89596418068441, 78.88613383148773, 78.13363555824226, 103.7009773820708, 102.92493033096062, 103.08466275395573, 14.83368757572748, 75.47680426502899, 103.0682316820629, 103.90727266992997, 116.64081167903693, 90.68318040952572, 113.96102657772136, 112.65796309112868, 113.94113484231498, 114.4600429298052, 105.83983249135674, 116.73287818541613, 95.78962911762983, 111.92618343805707, 115.99449104118679, 113.82675425815474, 113.28363365178208, 89.70423343425516, 94.84550600573162, 89.91697576865599, 114.84719402812907, 112.56404028399994, 90.77397642081247, 89.42044742661382, 114.07521455815512, 112.2636326694468, 92.46815832322885, 93.21911059421518, 89.06796301229393, 108.78427516690009, 87.36407181796173, 89.57653929677967, 107.93853447383955, 114.11402702481851, 86.81944091861779, 89.24253206812023, 106.53517412447167, 113.35176699213561, 102.70570014898318, 106.2242664308896, 115.12203667242747, 89.24876057307588, 89.88282901337338, 85.2062803785734, 87.19961564945588, 88.7789992205346, 89.32093475721612, 103.94205777060269, 89.79098719761802, 98.90887889697329, 113.21467306307129, 113.21524419043284, 97.42468207484086, 89.03955006998397, 90.26702159810725, 111.19565919865536, 90.41238463855312, 90.32596545024003, 90.15707876508132, 85.2326103804807, 88.81766995710765, 88.99338540230228, 61.03065937447715, 89.86265019105016, 90.21017663045538, 89.0349636132158, 88.0794883137483, 114.06494677799493, 81.12139662134122, 84.82851126407128, 98.97920171012474, 109.50492082242607, 113.03331165472493, 89.2999775463598, 103.18916785745927, 90.6191627763141, 114.90236820800888, 108.78289970451335, 28.000452445447404, 116.20271888892391, 115.5849928119783, 115.06625723775744, 114.43361048594019, 106.27327956338695, 113.76463325320013, 112.02771720723001, 114.73630863676927, 107.96191045260629, 115.1324305217821, 115.85461442541065, 107.59846265302514, 116.0940072773319, 117.26507500219195, 116.22790020685352, 108.09649041166368, 110.65447007808422, 111.55140812361654, 113.77763077868705, 97.08260441637715, 104.00381694089464, 113.9713755227955, 111.08337321637208, 108.90929636990442, 110.68491218683938, 116.7042103966228, 114.45106528222, 105.76373741547832, 114.83686127258785, 111.07798238054437, 83.99554311756539, 113.21671002332383, 113.24697555515426, 95.53959450277523, 85.85899462519767, 108.55998984094873, 89.66771944240111, 88.51629906291795, 89.10219743342027]
Elapsed: 0.5345800658945896~0.2534289817937212
Time per graph: 0.010978601721900965~0.0051696463655930055
Speed: 96.58537062119015~15.584948331799904
Total Time: 0.5398
best val loss: 0.17257532477378845 test_score: 0.8958

Testing...
Test loss: 0.3788 score: 0.8958 time: 0.42s
test Score 0.8958
Epoch Time List: [1.6028206122573465, 1.4421065340284258, 1.5036961131263524, 1.4041564629878849, 1.481176831992343, 1.4181753173470497, 1.5599925923161209, 1.711775926174596, 1.6711710870731622, 1.5867823497392237, 1.8755753978621215, 1.41058943211101, 1.4897221280261874, 1.443383929086849, 1.61078364495188, 1.5685488330200315, 2.087083663325757, 1.7892308440059423, 1.8472083339001983, 1.6363531311508268, 1.879508753074333, 1.756759799318388, 1.631346508860588, 1.448570615844801, 1.7236217390745878, 1.5495978959370404, 1.518880944000557, 1.5375331128016114, 1.5536270702723414, 1.7132020378485322, 1.6745930849574506, 1.7307669878937304, 1.7611988170538098, 1.4935704660601914, 1.5957276469562203, 1.7189970405306667, 1.842836296884343, 1.4914667068514973, 1.5188193556386977, 1.3973014219664037, 1.691152285085991, 1.4148280690424144, 1.7046428879257292, 1.3771537870634347, 1.8507115901447833, 1.7203573752194643, 1.5826043707784265, 1.47596654901281, 1.8044049739837646, 1.6498118760064244, 1.7736913117114455, 1.7448075069114566, 1.4794687230605632, 1.6423211907967925, 1.6149483770132065, 1.6136173019185662, 1.8117161279078573, 6.446931465761736, 1.4552888758480549, 1.3730466139968485, 1.4801644121762365, 1.3462149628903717, 1.5201655379496515, 1.349505285732448, 1.44922531908378, 1.3516054640058428, 1.5199323750566691, 1.3593243041541427, 1.4834350140299648, 1.3830295228399336, 1.5212320808786899, 1.458814735757187, 1.8807612750679255, 1.4623267049901187, 1.6973867861088365, 1.4261465107556432, 1.6798400881234556, 1.4343095410149544, 1.6069537398871034, 1.6282320590689778, 1.6281786900945008, 1.839561881031841, 1.6992059478070587, 1.730601780116558, 1.6635039430111647, 1.8655502400361001, 1.68389707012102, 1.365640388801694, 1.76356745720841, 1.6321223301347345, 1.8192384410649538, 1.556187563110143, 1.8947374518029392, 1.7223244130145758, 1.795619642129168, 1.7466508860234171, 1.5814802600070834, 1.7184820857364684, 1.6986074249725789, 1.7347999811172485, 1.7606946609448642, 1.3787837231066078, 1.523784255143255, 1.36867905408144, 1.5136261191219091, 1.5559182572178543, 1.5266851398628205, 1.4793970668688416, 1.6243218621239066, 1.5119699200149626, 1.4957748050801456, 1.6066255299374461, 1.655571105889976, 1.6764030877966434, 1.7210699920542538, 1.483016877900809, 1.5797856270801276, 1.5585791480261832, 1.8919943417422473, 1.8257219889201224, 1.4881428147200495, 1.8636465456802398, 1.7818499237764627, 1.5874954052269459, 1.4227195852436125, 1.4848647378385067, 1.5600573839619756, 1.9101961837150156, 1.5357904338743538, 1.369337699841708, 1.8359982850961387, 1.650372747797519, 1.566627173917368, 1.7183844377286732, 1.984061607858166, 1.5001060673967004, 1.51505356002599, 1.384667298058048, 1.5014154689852148, 1.3662329611834139, 4.355055981781334, 2.8040557811036706, 1.5247811991721392, 1.4011137187480927, 1.5419637288432568, 1.3898222823627293, 1.6728034701664, 1.5366867147386074, 1.4888076111674309, 1.5565199400298297, 1.4222408987116069, 1.415564491879195, 1.5354857759084553, 1.4118694067001343, 1.5669996486976743, 1.5019995279144496, 1.7513428379315883, 1.9176865417975932, 1.7275092487689108, 1.4455655626952648, 1.5787773409392685, 1.4069974680896848, 2.0127810032572597, 1.5526619150768965, 1.8989973049610853, 1.7540958430618048, 1.7772723489906639, 1.7934927640017122, 1.9207790920045227, 1.5840537210460752, 1.549518883228302, 1.5398509437218308, 1.3782975424546748, 1.5219204479362816, 1.4029071035329252, 1.465921690221876, 1.610029520932585, 1.421015297062695, 1.6852256597485393, 1.6938869680743665, 1.4079098079819232, 1.5855358282569796, 1.8570712429936975, 1.4228207869455218, 1.5902975113131106, 1.4018000229261816, 1.5887742238119245, 1.7020128699950874, 1.890114823821932, 1.4244753438979387, 1.5755665102042258, 1.3943247359711677, 1.5962893939577043, 1.4249788881279528, 1.8934271920006722, 1.7157250251621008, 1.7192245402839035, 1.8748165885917842, 1.5874394229613245, 1.591572833713144, 1.5912828091531992, 1.6727789607830346, 1.554305198835209, 1.4531746548600495, 1.7262946779374033, 1.8306577939074486, 1.9730343318078667, 1.7618031718302518, 1.6057678540237248, 1.4420557592529804, 1.5207135120872408, 1.3848215229809284, 1.528287329012528, 1.712386490777135, 1.7401887660380453, 1.4969566131476313, 1.5780978370457888, 1.4358082422986627, 1.6092263502068818, 1.9695124770514667, 1.7476195918861777, 1.6303725587204099, 1.4075885978527367, 1.3650667238980532, 1.6012107692658901, 6.330675031989813, 1.5554369310848415, 1.3854835610836744, 1.41578315384686, 1.4638965209014714, 1.600968032842502, 1.3588151729200035, 1.501193739939481, 1.3718362266663462, 1.5001430287957191, 1.4043838842771947, 1.460716038243845, 1.4722410440444946, 1.4905717400833964, 1.4017815708648413, 1.495308611774817, 1.3632982328999788, 1.6396604438778013, 1.730367663083598, 1.6395403440110385, 1.495247018756345, 1.5280865121167153, 1.6060638288035989, 1.9059182782657444, 1.546654445817694, 1.498501073103398, 1.657280107960105, 1.5899345718789846, 1.7651583191473037, 1.616889129858464, 1.5587735588196665, 1.9332536098081619, 1.5738060490693897, 1.6426242906600237, 1.8373441430740058, 1.911720547126606, 1.6334859901107848, 1.5440984170418233, 1.471494110999629, 1.699099461082369, 1.434983444865793, 1.6451025258284062, 1.7663604840636253, 1.9541793630924076, 1.769685396924615, 1.9472369279246777, 1.7748389490880072, 1.6296773622743785, 1.679178831866011, 1.8746232222765684, 1.3795088580809534, 1.496038533281535, 1.4434562071692199, 1.8729001798201352, 1.7382749151438475, 1.6488531799986959, 1.6964210346341133, 1.772991952020675, 1.725168933160603, 1.81931950035505, 1.8175550072919577, 1.882352659245953, 1.927483667852357, 1.911489378893748, 1.7553918261546642, 1.8426924080122262, 1.8194562399294227, 1.5605602781288326, 1.5713722370564938, 1.8959161411039531, 1.6838573450222611, 1.5457898217719048, 1.4214885858818889, 1.7760226968675852, 1.58380705001764, 1.8547586181666702, 1.5434205657802522, 1.6118317316286266, 5.940418380778283, 1.4883665170054883, 1.3874150386545807, 1.4701950917951763, 1.408484592102468, 1.5094065798912197, 1.3670887569896877, 1.505214060889557, 1.3907749699428678, 1.5367370850872248, 1.3442934420891106, 1.4518207868095487, 1.484306578990072, 1.4558940413407981, 1.3475275721866637, 1.5875171108637005, 1.4165397309698164, 1.5005410250741988, 1.4483422939665616, 1.5011875401251018, 1.4842311169486493, 1.5235571372322738, 1.3805119460448623, 1.5388252020347863, 1.414756717858836, 1.6568773901090026, 1.3739241531584412, 1.5198975671082735, 1.4260504769627005, 1.4722158920485526, 1.6288754777051508, 1.9264548050705343, 1.6393816829659045, 1.6005014278925955, 1.4471338889561594, 1.8988810549490154, 1.6709656447637826, 1.609690162120387, 1.7776280990801752, 1.6982807049062103]
Total Epoch List: [112, 117, 114]
Total Time List: [0.5536897419951856, 0.4723813950549811, 0.539802361978218]
========================training times:2========================
========================k_idx:0========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: FAGNN
FAGNN(
  (conv1): NeighborFilterConv()
  (conv2): NeighborFilterConv()
  (ffn): Sequential(
    (0): Linear(in_features=14951, out_features=64, bias=True)
    (1): ReLU()
    (2): Linear(in_features=64, out_features=64, bias=True)
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=64, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=64, out_features=2, bias=True)
  )
)
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7f904e47f820>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.6931;  Loss pred: 0.6931; Loss self: 0.0000; time: 0.54s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6930 score: 0.5102 time: 0.57s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6932 score: 0.4898 time: 0.61s
Epoch 2/1000, LR 0.000000
Train loss: 0.6931;  Loss pred: 0.6931; Loss self: 0.0000; time: 0.45s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6930 score: 0.5102 time: 0.50s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6932 score: 0.4898 time: 0.45s
Epoch 3/1000, LR 0.000030
Train loss: 0.6931;  Loss pred: 0.6931; Loss self: 0.0000; time: 0.45s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6930 score: 0.5102 time: 0.52s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6931 score: 0.4898 time: 0.71s
Epoch 4/1000, LR 0.000060
Train loss: 0.6931;  Loss pred: 0.6931; Loss self: 0.0000; time: 0.44s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6929 score: 0.5102 time: 0.51s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6931 score: 0.4898 time: 0.46s
Epoch 5/1000, LR 0.000090
Train loss: 0.6930;  Loss pred: 0.6930; Loss self: 0.0000; time: 0.45s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6929 score: 0.5102 time: 0.53s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6930 score: 0.4898 time: 0.59s
Epoch 6/1000, LR 0.000120
Train loss: 0.6930;  Loss pred: 0.6930; Loss self: 0.0000; time: 0.44s
Val loss: 0.6928 score: 0.6327 time: 0.51s
Test loss: 0.6929 score: 0.5510 time: 0.45s
Epoch 7/1000, LR 0.000150
Train loss: 0.6929;  Loss pred: 0.6929; Loss self: 0.0000; time: 0.44s
Val loss: 0.6927 score: 0.6327 time: 0.53s
Test loss: 0.6928 score: 0.6735 time: 0.58s
Epoch 8/1000, LR 0.000180
Train loss: 0.6928;  Loss pred: 0.6928; Loss self: 0.0000; time: 0.44s
Val loss: 0.6926 score: 0.5918 time: 0.50s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6926 score: 0.5102 time: 0.45s
Epoch 9/1000, LR 0.000210
Train loss: 0.6927;  Loss pred: 0.6927; Loss self: 0.0000; time: 0.44s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6924 score: 0.4898 time: 0.50s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6925 score: 0.5102 time: 0.44s
Epoch 10/1000, LR 0.000240
Train loss: 0.6926;  Loss pred: 0.6926; Loss self: 0.0000; time: 0.57s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6923 score: 0.4898 time: 0.50s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6923 score: 0.5102 time: 0.44s
Epoch 11/1000, LR 0.000270
Train loss: 0.6924;  Loss pred: 0.6924; Loss self: 0.0000; time: 0.43s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6920 score: 0.4898 time: 0.51s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6921 score: 0.5102 time: 0.49s
Epoch 12/1000, LR 0.000270
Train loss: 0.6922;  Loss pred: 0.6922; Loss self: 0.0000; time: 0.70s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6918 score: 0.4898 time: 0.65s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6918 score: 0.5102 time: 0.56s
Epoch 13/1000, LR 0.000270
Train loss: 0.6920;  Loss pred: 0.6920; Loss self: 0.0000; time: 0.56s
Val loss: 0.6915 score: 0.5510 time: 0.64s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6916 score: 0.5102 time: 0.69s
Epoch 14/1000, LR 0.000270
Train loss: 0.6918;  Loss pred: 0.6918; Loss self: 0.0000; time: 0.56s
Val loss: 0.6912 score: 0.5918 time: 0.64s
Test loss: 0.6913 score: 0.5306 time: 0.57s
Epoch 15/1000, LR 0.000270
Train loss: 0.6915;  Loss pred: 0.6915; Loss self: 0.0000; time: 0.56s
Val loss: 0.6908 score: 0.5918 time: 0.65s
Test loss: 0.6910 score: 0.5714 time: 0.68s
Epoch 16/1000, LR 0.000270
Train loss: 0.6912;  Loss pred: 0.6912; Loss self: 0.0000; time: 0.56s
Val loss: 0.6904 score: 0.6122 time: 0.66s
Test loss: 0.6906 score: 0.5918 time: 0.57s
Epoch 17/1000, LR 0.000270
Train loss: 0.6908;  Loss pred: 0.6908; Loss self: 0.0000; time: 0.55s
Val loss: 0.6899 score: 0.6122 time: 0.66s
Test loss: 0.6903 score: 0.5918 time: 0.71s
Epoch 18/1000, LR 0.000270
Train loss: 0.6905;  Loss pred: 0.6905; Loss self: 0.0000; time: 0.53s
Val loss: 0.6894 score: 0.6122 time: 0.63s
Test loss: 0.6898 score: 0.6327 time: 0.58s
Epoch 19/1000, LR 0.000270
Train loss: 0.6900;  Loss pred: 0.6900; Loss self: 0.0000; time: 0.54s
Val loss: 0.6889 score: 0.6122 time: 0.63s
Test loss: 0.6894 score: 0.6327 time: 0.69s
Epoch 20/1000, LR 0.000270
Train loss: 0.6896;  Loss pred: 0.6896; Loss self: 0.0000; time: 0.53s
Val loss: 0.6883 score: 0.6122 time: 0.53s
Test loss: 0.6889 score: 0.6327 time: 0.46s
Epoch 21/1000, LR 0.000270
Train loss: 0.6892;  Loss pred: 0.6892; Loss self: 0.0000; time: 0.46s
Val loss: 0.6876 score: 0.6327 time: 0.62s
Test loss: 0.6883 score: 0.6735 time: 0.46s
Epoch 22/1000, LR 0.000270
Train loss: 0.6884;  Loss pred: 0.6884; Loss self: 0.0000; time: 0.49s
Val loss: 0.6868 score: 0.6327 time: 0.64s
Test loss: 0.6877 score: 0.6735 time: 0.54s
Epoch 23/1000, LR 0.000270
Train loss: 0.6876;  Loss pred: 0.6876; Loss self: 0.0000; time: 0.44s
Val loss: 0.6859 score: 0.6531 time: 0.71s
Test loss: 0.6870 score: 0.7143 time: 0.55s
Epoch 24/1000, LR 0.000270
Train loss: 0.6868;  Loss pred: 0.6868; Loss self: 0.0000; time: 0.54s
Val loss: 0.6850 score: 0.6735 time: 0.64s
Test loss: 0.6862 score: 0.7347 time: 0.55s
Epoch 25/1000, LR 0.000270
Train loss: 0.6858;  Loss pred: 0.6858; Loss self: 0.0000; time: 0.57s
Val loss: 0.6840 score: 0.6939 time: 0.65s
Test loss: 0.6854 score: 0.7755 time: 0.70s
Epoch 26/1000, LR 0.000270
Train loss: 0.6851;  Loss pred: 0.6851; Loss self: 0.0000; time: 0.53s
Val loss: 0.6829 score: 0.7143 time: 0.50s
Test loss: 0.6845 score: 0.7551 time: 0.46s
Epoch 27/1000, LR 0.000270
Train loss: 0.6839;  Loss pred: 0.6839; Loss self: 0.0000; time: 0.42s
Val loss: 0.6818 score: 0.7143 time: 0.59s
Test loss: 0.6835 score: 0.7551 time: 0.45s
Epoch 28/1000, LR 0.000270
Train loss: 0.6832;  Loss pred: 0.6832; Loss self: 0.0000; time: 0.43s
Val loss: 0.6805 score: 0.7347 time: 0.51s
Test loss: 0.6824 score: 0.7551 time: 0.58s
Epoch 29/1000, LR 0.000270
Train loss: 0.6818;  Loss pred: 0.6818; Loss self: 0.0000; time: 0.44s
Val loss: 0.6791 score: 0.7347 time: 0.61s
Test loss: 0.6813 score: 0.7347 time: 0.55s
Epoch 30/1000, LR 0.000270
Train loss: 0.6805;  Loss pred: 0.6805; Loss self: 0.0000; time: 0.55s
Val loss: 0.6777 score: 0.7551 time: 0.58s
Test loss: 0.6800 score: 0.7347 time: 0.55s
Epoch 31/1000, LR 0.000270
Train loss: 0.6793;  Loss pred: 0.6793; Loss self: 0.0000; time: 0.53s
Val loss: 0.6760 score: 0.7959 time: 0.77s
Test loss: 0.6786 score: 0.7347 time: 0.45s
Epoch 32/1000, LR 0.000270
Train loss: 0.6779;  Loss pred: 0.6779; Loss self: 0.0000; time: 0.43s
Val loss: 0.6743 score: 0.7959 time: 0.51s
Test loss: 0.6772 score: 0.7551 time: 0.49s
Epoch 33/1000, LR 0.000270
Train loss: 0.6759;  Loss pred: 0.6759; Loss self: 0.0000; time: 0.59s
Val loss: 0.6725 score: 0.8367 time: 0.67s
Test loss: 0.6757 score: 0.7551 time: 0.57s
Epoch 34/1000, LR 0.000270
Train loss: 0.6743;  Loss pred: 0.6743; Loss self: 0.0000; time: 0.43s
Val loss: 0.6705 score: 0.8571 time: 0.51s
Test loss: 0.6740 score: 0.7551 time: 0.53s
Epoch 35/1000, LR 0.000270
Train loss: 0.6721;  Loss pred: 0.6721; Loss self: 0.0000; time: 0.43s
Val loss: 0.6683 score: 0.8776 time: 0.50s
Test loss: 0.6723 score: 0.7959 time: 0.53s
Epoch 36/1000, LR 0.000270
Train loss: 0.6700;  Loss pred: 0.6700; Loss self: 0.0000; time: 0.45s
Val loss: 0.6660 score: 0.8980 time: 0.54s
Test loss: 0.6703 score: 0.7959 time: 0.60s
Epoch 37/1000, LR 0.000270
Train loss: 0.6681;  Loss pred: 0.6681; Loss self: 0.0000; time: 0.49s
Val loss: 0.6635 score: 0.8776 time: 0.53s
Test loss: 0.6683 score: 0.8163 time: 0.66s
Epoch 38/1000, LR 0.000270
Train loss: 0.6649;  Loss pred: 0.6649; Loss self: 0.0000; time: 0.58s
Val loss: 0.6607 score: 0.8980 time: 0.52s
Test loss: 0.6660 score: 0.8367 time: 0.44s
Epoch 39/1000, LR 0.000269
Train loss: 0.6636;  Loss pred: 0.6636; Loss self: 0.0000; time: 0.51s
Val loss: 0.6577 score: 0.9184 time: 0.74s
Test loss: 0.6636 score: 0.8571 time: 0.55s
Epoch 40/1000, LR 0.000269
Train loss: 0.6600;  Loss pred: 0.6600; Loss self: 0.0000; time: 0.53s
Val loss: 0.6544 score: 0.9184 time: 0.63s
Test loss: 0.6609 score: 0.8776 time: 0.52s
Epoch 41/1000, LR 0.000269
Train loss: 0.6556;  Loss pred: 0.6556; Loss self: 0.0000; time: 0.43s
Val loss: 0.6508 score: 0.9184 time: 0.60s
Test loss: 0.6580 score: 0.8776 time: 0.44s
Epoch 42/1000, LR 0.000269
Train loss: 0.6537;  Loss pred: 0.6537; Loss self: 0.0000; time: 0.46s
Val loss: 0.6468 score: 0.9184 time: 3.49s
Test loss: 0.6549 score: 0.8776 time: 1.51s
Epoch 43/1000, LR 0.000269
Train loss: 0.6494;  Loss pred: 0.6494; Loss self: 0.0000; time: 0.45s
Val loss: 0.6425 score: 0.9184 time: 0.61s
Test loss: 0.6515 score: 0.8776 time: 0.43s
Epoch 44/1000, LR 0.000269
Train loss: 0.6457;  Loss pred: 0.6457; Loss self: 0.0000; time: 0.43s
Val loss: 0.6379 score: 0.9388 time: 0.49s
Test loss: 0.6478 score: 0.8776 time: 0.44s
Epoch 45/1000, LR 0.000269
Train loss: 0.6401;  Loss pred: 0.6401; Loss self: 0.0000; time: 0.53s
Val loss: 0.6328 score: 0.9388 time: 0.49s
Test loss: 0.6438 score: 0.8776 time: 0.43s
Epoch 46/1000, LR 0.000269
Train loss: 0.6354;  Loss pred: 0.6354; Loss self: 0.0000; time: 0.42s
Val loss: 0.6273 score: 0.9388 time: 0.50s
Test loss: 0.6394 score: 0.8776 time: 0.43s
Epoch 47/1000, LR 0.000269
Train loss: 0.6293;  Loss pred: 0.6293; Loss self: 0.0000; time: 0.53s
Val loss: 0.6213 score: 0.9388 time: 0.50s
Test loss: 0.6347 score: 0.8776 time: 0.47s
Epoch 48/1000, LR 0.000269
Train loss: 0.6228;  Loss pred: 0.6228; Loss self: 0.0000; time: 0.42s
Val loss: 0.6147 score: 0.9388 time: 0.49s
Test loss: 0.6296 score: 0.8980 time: 0.43s
Epoch 49/1000, LR 0.000269
Train loss: 0.6174;  Loss pred: 0.6174; Loss self: 0.0000; time: 0.53s
Val loss: 0.6077 score: 0.9388 time: 0.54s
Test loss: 0.6241 score: 0.9184 time: 0.44s
Epoch 50/1000, LR 0.000269
Train loss: 0.6107;  Loss pred: 0.6107; Loss self: 0.0000; time: 0.43s
Val loss: 0.6000 score: 0.9388 time: 0.50s
Test loss: 0.6181 score: 0.9184 time: 0.53s
Epoch 51/1000, LR 0.000269
Train loss: 0.6003;  Loss pred: 0.6003; Loss self: 0.0000; time: 0.43s
Val loss: 0.5917 score: 0.9388 time: 0.58s
Test loss: 0.6116 score: 0.9388 time: 0.55s
Epoch 52/1000, LR 0.000269
Train loss: 0.5913;  Loss pred: 0.5913; Loss self: 0.0000; time: 0.44s
Val loss: 0.5828 score: 0.9388 time: 0.51s
Test loss: 0.6047 score: 0.9388 time: 0.58s
Epoch 53/1000, LR 0.000269
Train loss: 0.5860;  Loss pred: 0.5860; Loss self: 0.0000; time: 0.47s
Val loss: 0.5734 score: 0.9388 time: 0.50s
Test loss: 0.5973 score: 0.9388 time: 0.43s
Epoch 54/1000, LR 0.000269
Train loss: 0.5735;  Loss pred: 0.5735; Loss self: 0.0000; time: 0.43s
Val loss: 0.5634 score: 0.9592 time: 0.51s
Test loss: 0.5894 score: 0.9388 time: 0.48s
Epoch 55/1000, LR 0.000269
Train loss: 0.5670;  Loss pred: 0.5670; Loss self: 0.0000; time: 0.52s
Val loss: 0.5528 score: 0.9592 time: 0.73s
Test loss: 0.5810 score: 0.9388 time: 0.54s
Epoch 56/1000, LR 0.000269
Train loss: 0.5510;  Loss pred: 0.5510; Loss self: 0.0000; time: 0.53s
Val loss: 0.5417 score: 0.9592 time: 0.58s
Test loss: 0.5722 score: 0.9184 time: 0.42s
Epoch 57/1000, LR 0.000269
Train loss: 0.5413;  Loss pred: 0.5413; Loss self: 0.0000; time: 0.48s
Val loss: 0.5300 score: 0.9592 time: 0.53s
Test loss: 0.5629 score: 0.9184 time: 0.65s
Epoch 58/1000, LR 0.000269
Train loss: 0.5328;  Loss pred: 0.5328; Loss self: 0.0000; time: 0.44s
Val loss: 0.5179 score: 0.9592 time: 0.50s
Test loss: 0.5530 score: 0.9184 time: 0.52s
Epoch 59/1000, LR 0.000268
Train loss: 0.5144;  Loss pred: 0.5144; Loss self: 0.0000; time: 0.54s
Val loss: 0.5052 score: 0.9592 time: 0.76s
Test loss: 0.5428 score: 0.9184 time: 0.45s
Epoch 60/1000, LR 0.000268
Train loss: 0.5044;  Loss pred: 0.5044; Loss self: 0.0000; time: 0.45s
Val loss: 0.4920 score: 0.9592 time: 0.52s
Test loss: 0.5321 score: 0.9184 time: 0.49s
Epoch 61/1000, LR 0.000268
Train loss: 0.4864;  Loss pred: 0.4864; Loss self: 0.0000; time: 0.45s
Val loss: 0.4784 score: 0.9592 time: 0.66s
Test loss: 0.5209 score: 0.9184 time: 0.45s
Epoch 62/1000, LR 0.000268
Train loss: 0.4785;  Loss pred: 0.4785; Loss self: 0.0000; time: 0.54s
Val loss: 0.4644 score: 0.9592 time: 0.68s
Test loss: 0.5093 score: 0.9184 time: 0.54s
Epoch 63/1000, LR 0.000268
Train loss: 0.4576;  Loss pred: 0.4576; Loss self: 0.0000; time: 0.45s
Val loss: 0.4501 score: 0.9592 time: 0.66s
Test loss: 0.4973 score: 0.9184 time: 0.48s
Epoch 64/1000, LR 0.000268
Train loss: 0.4398;  Loss pred: 0.4398; Loss self: 0.0000; time: 0.44s
Val loss: 0.4356 score: 0.9592 time: 0.51s
Test loss: 0.4850 score: 0.9184 time: 0.44s
Epoch 65/1000, LR 0.000268
Train loss: 0.4333;  Loss pred: 0.4333; Loss self: 0.0000; time: 0.44s
Val loss: 0.4209 score: 0.9592 time: 0.64s
Test loss: 0.4725 score: 0.9184 time: 0.48s
Epoch 66/1000, LR 0.000268
Train loss: 0.4184;  Loss pred: 0.4184; Loss self: 0.0000; time: 0.44s
Val loss: 0.4060 score: 0.9592 time: 0.50s
Test loss: 0.4599 score: 0.9184 time: 0.45s
Epoch 67/1000, LR 0.000268
Train loss: 0.4018;  Loss pred: 0.4018; Loss self: 0.0000; time: 0.44s
Val loss: 0.3911 score: 0.9592 time: 0.66s
Test loss: 0.4471 score: 0.9184 time: 0.49s
Epoch 68/1000, LR 0.000268
Train loss: 0.3842;  Loss pred: 0.3842; Loss self: 0.0000; time: 0.46s
Val loss: 0.3761 score: 0.9592 time: 0.52s
Test loss: 0.4345 score: 0.9184 time: 0.45s
Epoch 69/1000, LR 0.000268
Train loss: 0.3712;  Loss pred: 0.3712; Loss self: 0.0000; time: 0.62s
Val loss: 0.3610 score: 0.9592 time: 0.51s
Test loss: 0.4220 score: 0.9184 time: 0.54s
Epoch 70/1000, LR 0.000268
Train loss: 0.3536;  Loss pred: 0.3536; Loss self: 0.0000; time: 0.53s
Val loss: 0.3461 score: 0.9592 time: 0.64s
Test loss: 0.4095 score: 0.9184 time: 0.55s
Epoch 71/1000, LR 0.000268
Train loss: 0.3419;  Loss pred: 0.3419; Loss self: 0.0000; time: 0.69s
Val loss: 0.3312 score: 0.9592 time: 0.61s
Test loss: 0.3973 score: 0.9184 time: 0.48s
Epoch 72/1000, LR 0.000267
Train loss: 0.3291;  Loss pred: 0.3291; Loss self: 0.0000; time: 0.57s
Val loss: 0.3169 score: 0.9592 time: 0.61s
Test loss: 0.3853 score: 0.9184 time: 0.43s
Epoch 73/1000, LR 0.000267
Train loss: 0.3099;  Loss pred: 0.3099; Loss self: 0.0000; time: 0.70s
Val loss: 0.3029 score: 0.9592 time: 0.66s
Test loss: 0.3737 score: 0.9184 time: 0.55s
Epoch 74/1000, LR 0.000267
Train loss: 0.2920;  Loss pred: 0.2920; Loss self: 0.0000; time: 0.55s
Val loss: 0.2894 score: 0.9592 time: 0.65s
Test loss: 0.3625 score: 0.9184 time: 0.64s
Epoch 75/1000, LR 0.000267
Train loss: 0.2835;  Loss pred: 0.2835; Loss self: 0.0000; time: 0.45s
Val loss: 0.2762 score: 0.9592 time: 0.50s
Test loss: 0.3518 score: 0.9184 time: 0.44s
Epoch 76/1000, LR 0.000267
Train loss: 0.2662;  Loss pred: 0.2662; Loss self: 0.0000; time: 0.55s
Val loss: 0.2635 score: 0.9592 time: 0.62s
Test loss: 0.3417 score: 0.9184 time: 0.62s
Epoch 77/1000, LR 0.000267
Train loss: 0.2485;  Loss pred: 0.2485; Loss self: 0.0000; time: 0.46s
Val loss: 0.2514 score: 0.9592 time: 0.52s
Test loss: 0.3323 score: 0.9184 time: 0.45s
Epoch 78/1000, LR 0.000267
Train loss: 0.2391;  Loss pred: 0.2391; Loss self: 0.0000; time: 0.45s
Val loss: 0.2401 score: 0.9592 time: 0.53s
Test loss: 0.3236 score: 0.9184 time: 0.45s
Epoch 79/1000, LR 0.000267
Train loss: 0.2261;  Loss pred: 0.2261; Loss self: 0.0000; time: 0.47s
Val loss: 0.2289 score: 0.9592 time: 0.79s
Test loss: 0.3156 score: 0.9184 time: 0.55s
Epoch 80/1000, LR 0.000267
Train loss: 0.2129;  Loss pred: 0.2129; Loss self: 0.0000; time: 0.54s
Val loss: 0.2184 score: 0.9592 time: 0.63s
Test loss: 0.3083 score: 0.9184 time: 0.55s
Epoch 81/1000, LR 0.000267
Train loss: 0.2101;  Loss pred: 0.2101; Loss self: 0.0000; time: 0.44s
Val loss: 0.2087 score: 0.9592 time: 0.51s
Test loss: 0.3019 score: 0.9184 time: 0.58s
Epoch 82/1000, LR 0.000267
Train loss: 0.1956;  Loss pred: 0.1956; Loss self: 0.0000; time: 0.44s
Val loss: 0.1997 score: 0.9592 time: 0.50s
Test loss: 0.2962 score: 0.9184 time: 0.44s
Epoch 83/1000, LR 0.000266
Train loss: 0.1888;  Loss pred: 0.1888; Loss self: 0.0000; time: 0.44s
Val loss: 0.1914 score: 0.9592 time: 0.64s
Test loss: 0.2913 score: 0.9184 time: 0.44s
Epoch 84/1000, LR 0.000266
Train loss: 0.1747;  Loss pred: 0.1747; Loss self: 0.0000; time: 0.44s
Val loss: 0.1836 score: 0.9592 time: 0.51s
Test loss: 0.2873 score: 0.9388 time: 0.44s
Epoch 85/1000, LR 0.000266
Train loss: 0.1643;  Loss pred: 0.1643; Loss self: 0.0000; time: 0.50s
Val loss: 0.1770 score: 0.9592 time: 0.75s
Test loss: 0.2839 score: 0.9388 time: 0.58s
Epoch 86/1000, LR 0.000266
Train loss: 0.1602;  Loss pred: 0.1602; Loss self: 0.0000; time: 0.49s
Val loss: 0.1721 score: 0.9592 time: 0.64s
Test loss: 0.2813 score: 0.9184 time: 0.55s
Epoch 87/1000, LR 0.000266
Train loss: 0.1522;  Loss pred: 0.1522; Loss self: 0.0000; time: 0.54s
Val loss: 0.1681 score: 0.9592 time: 0.77s
Test loss: 0.2794 score: 0.9184 time: 0.55s
Epoch 88/1000, LR 0.000266
Train loss: 0.1421;  Loss pred: 0.1421; Loss self: 0.0000; time: 0.50s
Val loss: 0.1654 score: 0.9592 time: 0.51s
Test loss: 0.2784 score: 0.9184 time: 0.43s
Epoch 89/1000, LR 0.000266
Train loss: 0.1349;  Loss pred: 0.1349; Loss self: 0.0000; time: 0.48s
Val loss: 0.1635 score: 0.9592 time: 0.81s
Test loss: 0.2782 score: 0.9184 time: 0.52s
Epoch 90/1000, LR 0.000266
Train loss: 0.1357;  Loss pred: 0.1357; Loss self: 0.0000; time: 0.54s
Val loss: 0.1633 score: 0.9592 time: 0.67s
Test loss: 0.2790 score: 0.9184 time: 0.56s
Epoch 91/1000, LR 0.000266
Train loss: 0.1302;  Loss pred: 0.1302; Loss self: 0.0000; time: 0.54s
Val loss: 0.1626 score: 0.9592 time: 0.69s
Test loss: 0.2802 score: 0.9184 time: 0.44s
Epoch 92/1000, LR 0.000266
Train loss: 0.1188;  Loss pred: 0.1188; Loss self: 0.0000; time: 0.47s
Val loss: 0.1628 score: 0.9592 time: 0.50s
Test loss: 0.2820 score: 0.9184 time: 0.44s
     INFO: Early stopping counter 1 of 20
Epoch 93/1000, LR 0.000265
Train loss: 0.1136;  Loss pred: 0.1136; Loss self: 0.0000; time: 0.55s
Val loss: 0.1639 score: 0.9592 time: 0.63s
Test loss: 0.2844 score: 0.9184 time: 0.56s
     INFO: Early stopping counter 2 of 20
Epoch 94/1000, LR 0.000265
Train loss: 0.1112;  Loss pred: 0.1112; Loss self: 0.0000; time: 0.54s
Val loss: 0.1666 score: 0.9592 time: 0.64s
Test loss: 0.2881 score: 0.9184 time: 0.56s
     INFO: Early stopping counter 3 of 20
Epoch 95/1000, LR 0.000265
Train loss: 0.1077;  Loss pred: 0.1077; Loss self: 0.0000; time: 0.66s
Val loss: 0.1706 score: 0.9592 time: 0.52s
Test loss: 0.2928 score: 0.9184 time: 0.43s
     INFO: Early stopping counter 4 of 20
Epoch 96/1000, LR 0.000265
Train loss: 0.1098;  Loss pred: 0.1098; Loss self: 0.0000; time: 0.44s
Val loss: 0.1745 score: 0.9592 time: 0.54s
Test loss: 0.2978 score: 0.8980 time: 0.47s
     INFO: Early stopping counter 5 of 20
Epoch 97/1000, LR 0.000265
Train loss: 0.0950;  Loss pred: 0.0950; Loss self: 0.0000; time: 0.58s
Val loss: 0.1784 score: 0.9388 time: 0.51s
Test loss: 0.3030 score: 0.8980 time: 0.45s
     INFO: Early stopping counter 6 of 20
Epoch 98/1000, LR 0.000265
Train loss: 0.1065;  Loss pred: 0.1065; Loss self: 0.0000; time: 0.48s
Val loss: 0.1824 score: 0.9388 time: 0.51s
Test loss: 0.3085 score: 0.8980 time: 0.59s
     INFO: Early stopping counter 7 of 20
Epoch 99/1000, LR 0.000265
Train loss: 0.0886;  Loss pred: 0.0886; Loss self: 0.0000; time: 0.45s
Val loss: 0.1872 score: 0.9388 time: 0.52s
Test loss: 0.3148 score: 0.8776 time: 0.44s
     INFO: Early stopping counter 8 of 20
Epoch 100/1000, LR 0.000265
Train loss: 0.0828;  Loss pred: 0.0828; Loss self: 0.0000; time: 0.59s
Val loss: 0.1924 score: 0.9388 time: 0.64s
Test loss: 0.3216 score: 0.8776 time: 0.71s
     INFO: Early stopping counter 9 of 20
Epoch 101/1000, LR 0.000265
Train loss: 0.0802;  Loss pred: 0.0802; Loss self: 0.0000; time: 0.54s
Val loss: 0.1979 score: 0.9388 time: 0.72s
Test loss: 0.3289 score: 0.8776 time: 0.55s
     INFO: Early stopping counter 10 of 20
Epoch 102/1000, LR 0.000264
Train loss: 0.0773;  Loss pred: 0.0773; Loss self: 0.0000; time: 0.55s
Val loss: 0.2037 score: 0.9388 time: 0.64s
Test loss: 0.3366 score: 0.8776 time: 0.59s
     INFO: Early stopping counter 11 of 20
Epoch 103/1000, LR 0.000264
Train loss: 0.0881;  Loss pred: 0.0881; Loss self: 0.0000; time: 0.54s
Val loss: 0.2099 score: 0.9388 time: 0.80s
Test loss: 0.3448 score: 0.8776 time: 0.56s
     INFO: Early stopping counter 12 of 20
Epoch 104/1000, LR 0.000264
Train loss: 0.0837;  Loss pred: 0.0837; Loss self: 0.0000; time: 0.52s
Val loss: 0.2141 score: 0.9388 time: 0.51s
Test loss: 0.3511 score: 0.8776 time: 0.46s
     INFO: Early stopping counter 13 of 20
Epoch 105/1000, LR 0.000264
Train loss: 0.0771;  Loss pred: 0.0771; Loss self: 0.0000; time: 0.46s
Val loss: 0.2164 score: 0.9388 time: 0.53s
Test loss: 0.3556 score: 0.8776 time: 0.61s
     INFO: Early stopping counter 14 of 20
Epoch 106/1000, LR 0.000264
Train loss: 0.0691;  Loss pred: 0.0691; Loss self: 0.0000; time: 0.45s
Val loss: 0.2189 score: 0.9388 time: 0.51s
Test loss: 0.3603 score: 0.8776 time: 0.45s
     INFO: Early stopping counter 15 of 20
Epoch 107/1000, LR 0.000264
Train loss: 0.0717;  Loss pred: 0.0717; Loss self: 0.0000; time: 0.47s
Val loss: 0.2188 score: 0.9388 time: 0.66s
Test loss: 0.3626 score: 0.8776 time: 0.66s
     INFO: Early stopping counter 16 of 20
Epoch 108/1000, LR 0.000264
Train loss: 0.0788;  Loss pred: 0.0788; Loss self: 0.0000; time: 0.55s
Val loss: 0.2147 score: 0.9388 time: 0.58s
Test loss: 0.3611 score: 0.8776 time: 0.43s
     INFO: Early stopping counter 17 of 20
Epoch 109/1000, LR 0.000264
Train loss: 0.0563;  Loss pred: 0.0563; Loss self: 0.0000; time: 0.46s
Val loss: 0.2115 score: 0.9388 time: 0.64s
Test loss: 0.3606 score: 0.8776 time: 0.44s
     INFO: Early stopping counter 18 of 20
Epoch 110/1000, LR 0.000263
Train loss: 0.0699;  Loss pred: 0.0699; Loss self: 0.0000; time: 0.45s
Val loss: 0.2065 score: 0.9388 time: 0.50s
Test loss: 0.3586 score: 0.8776 time: 0.47s
     INFO: Early stopping counter 19 of 20
Epoch 111/1000, LR 0.000263
Train loss: 0.0605;  Loss pred: 0.0605; Loss self: 0.0000; time: 0.44s
Val loss: 0.1999 score: 0.9388 time: 0.66s
Test loss: 0.3556 score: 0.8776 time: 0.46s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 090,   Train_Loss: 0.1302,   Val_Loss: 0.1626,   Val_Precision: 0.9231,   Val_Recall: 1.0000,   Val_accuracy: 0.9600,   Val_Score: 0.9592,   Val_Loss: 0.1626,   Test_Precision: 0.9200,   Test_Recall: 0.9200,   Test_accuracy: 0.9200,   Test_Score: 0.9184,   Test_loss: 0.2802


[0.6227087839506567, 0.4557098669465631, 0.7132796109654009, 0.46749154408462346, 0.594128652010113, 0.4530330009292811, 0.5804642019793391, 0.4509469398763031, 0.4495153359603137, 0.4430166510865092, 0.5019226549193263, 0.5697651370428503, 0.7001396778505296, 0.580072927987203, 0.6830095530021936, 0.581694952910766, 0.7145566779654473, 0.5911042669322342, 0.7009357118513435, 0.469384070020169, 0.46560450620017946, 0.5480443921405822, 0.5570869690272957, 0.5561958570033312, 0.7029278031550348, 0.4620986790396273, 0.45321387983858585, 0.5818643730599433, 0.5562601159326732, 0.5559133400674909, 0.4538283220026642, 0.5003790690097958, 0.5769332370255142, 0.5302128691691905, 0.5397878438234329, 0.6027016357984394, 0.6666738078929484, 0.4413969258312136, 0.55328563391231, 0.5266274919267744, 0.4431350198574364, 1.5140820879023522, 0.4329633731395006, 0.44185982597991824, 0.42989791999571025, 0.42984618502669036, 0.47912067314609885, 0.4370650190394372, 0.4474087511189282, 0.5384855889715254, 0.5509043079800904, 0.5833360799588263, 0.4381748731248081, 0.4823975651524961, 0.5490636611357331, 0.42604362103156745, 0.6519483160227537, 0.5290174090769142, 0.45138902706094086, 0.4965191911906004, 0.45546926092356443, 0.5475415450055152, 0.48401307198219, 0.4461718969978392, 0.4887299248948693, 0.45831945119425654, 0.49927506293170154, 0.4562228568829596, 0.5492180048022419, 0.5551030188798904, 0.48647005786187947, 0.43406992591917515, 0.5619298999663442, 0.6450574991758913, 0.44510894804261625, 0.6258882849942893, 0.45456719398498535, 0.4534809950273484, 0.557323485147208, 0.5562262239400297, 0.5869244011119008, 0.44932498782873154, 0.44583860388956964, 0.44897839589975774, 0.5881211729720235, 0.5557046618778259, 0.5570933748967946, 0.4364910169970244, 0.5264528181869537, 0.563808973878622, 0.4453510718885809, 0.4492754810489714, 0.5629205820150673, 0.5639159770216793, 0.43987377313897014, 0.4735481219831854, 0.456806463887915, 0.5943967460189015, 0.4473464419133961, 0.7159024169668555, 0.5613610420841724, 0.5945262049790472, 0.5648923490662128, 0.46442935382947326, 0.6181377992033958, 0.45040287892334163, 0.6706126190256327, 0.4311196890193969, 0.4454271518625319, 0.47128826519474387, 0.4702799858059734]
[0.012708342529605237, 0.00930020136625639, 0.014556726754395937, 0.00954064375682905, 0.012125074530818633, 0.00924557144753635, 0.011846208203659982, 0.009202998772985777, 0.009173782366537014, 0.009041156144622637, 0.010243319488149516, 0.011627859939650004, 0.01428856485409244, 0.011838223020147, 0.013938970469432523, 0.011871325569607469, 0.014582789346233619, 0.012063352386372126, 0.014304810445945787, 0.00957926673510549, 0.0095021327795955, 0.011184579431440453, 0.011369121816883586, 0.01135093585721084, 0.014345465370510914, 0.009430585286523007, 0.00924926285384869, 0.011874783123672312, 0.011352247263932106, 0.011345170205458999, 0.00926180248985029, 0.010211817734893791, 0.01177414769439825, 0.010820670799371235, 0.011016078445376182, 0.01230003338364162, 0.01360558791618262, 0.009008100527167624, 0.011291543549230816, 0.010747499835240294, 0.009043571833825233, 0.03089963444698678, 0.008835987206928584, 0.009017547468977923, 0.008773426938687965, 0.00877237112299368, 0.009777972921348957, 0.008919694266110962, 0.0091307908391618, 0.010989501815745416, 0.01124294506081817, 0.011904817958343394, 0.008942344349485879, 0.009844848268418287, 0.011205380839504758, 0.008694767776154438, 0.01330506767393375, 0.010796273654630902, 0.009212020960427364, 0.010133044718175518, 0.009295291039256416, 0.011174317245010515, 0.009877817795554899, 0.009105548918323249, 0.009974080099895293, 0.009353458187637888, 0.010189286998606153, 0.00931067054863183, 0.011208530710249836, 0.01132863303836511, 0.009927960364528152, 0.00885856991671786, 0.01146795714217029, 0.013164438758691659, 0.009083856082502373, 0.012773230306005903, 0.00927688150989766, 0.0092547141842316, 0.011373948676473632, 0.011351555590612851, 0.01197804900228369, 0.00916989771079044, 0.009098747018154482, 0.009162824406117504, 0.012002472917796398, 0.011340911466894405, 0.011369252548914174, 0.008907979938714785, 0.01074393506503987, 0.011506305589359634, 0.009088797385481243, 0.009168887368346356, 0.01148817514316464, 0.011508489326973046, 0.00897701577834633, 0.009664247387411947, 0.009322580895671735, 0.012130545837120438, 0.00912951922272237, 0.014610253407486848, 0.011456347797636171, 0.01213318785671525, 0.011528415287065566, 0.009478150078152515, 0.012615057126599915, 0.009191895488231462, 0.013685971816849649, 0.008798361000395855, 0.009090350038010855, 0.009618127861117222, 0.009597550730734152]
[78.68846764795717, 107.52455356808365, 68.69676245712405, 104.81473006307515, 82.47371984875414, 108.15989100018996, 84.41519706626816, 108.66023398105537, 109.0062920663647, 110.60532347898503, 97.6246031530012, 86.00034788775584, 69.98603500151987, 84.47213727078294, 71.7413098903503, 84.23659128346738, 68.57398651639137, 82.89569664977142, 69.90655372741526, 104.39212391228895, 105.2395312921074, 89.40881560454086, 87.95754114578676, 88.09846276813695, 69.70843915985034, 106.03795730781131, 108.11672408941132, 84.21206430343184, 88.08828567160961, 88.14323468843386, 107.9703439039936, 97.9257587591873, 84.93183761197143, 92.41571234734428, 90.776405138957, 81.30059234879361, 73.499212688236, 111.01119453364109, 88.56185123318417, 93.04489558781574, 110.57577894828552, 32.36284240564912, 113.17354547728041, 110.89489724785925, 113.98054682490427, 113.99426517408186, 102.27068616815532, 112.11146595005499, 109.519538626492, 90.99593564534754, 88.9446665967456, 83.99960448779129, 111.82749857507923, 101.57596874376856, 89.24283916120756, 115.01169735004522, 75.15933210615096, 92.62455102470145, 108.55381292506394, 98.68702130627256, 107.58135444890767, 89.49092620817738, 101.23693519129382, 109.82314289561205, 100.25987258819968, 106.91232910215626, 98.14229397373882, 107.40364990649859, 89.21775974486403, 88.2719032926072, 100.72562372155755, 112.88503781099065, 87.19948876707711, 75.96222051925778, 110.08540766362795, 78.2887316710954, 107.79484452109077, 108.05303979066404, 87.92021385400159, 88.09365306962405, 83.48605017472744, 109.05247054427618, 109.90524277735464, 109.13665434124832, 83.31616383131116, 88.17633423196452, 87.95652974526504, 112.25889672853, 93.07576730000359, 86.90886855332107, 110.02555757238403, 109.06448730652843, 87.04602667857053, 86.89237758219473, 111.39559344566638, 103.47417237089132, 107.26643310376396, 82.43652127671949, 109.53479319164036, 68.44508251222817, 87.28785278379324, 82.41857060232846, 86.74219093425279, 105.5058204137363, 79.27035049975441, 108.791489337571, 73.06751857904881, 113.65753234665048, 110.00676495608516, 103.9703375167901, 104.19324972127617]
Elapsed: 0.5325884224956033~0.1220443840262173
Time per graph: 0.010869151479502109~0.0024907017148207613
Speed: 94.9904868567631~14.584198411497225
Total Time: 0.4709
best val loss: 0.16264930367469788 test_score: 0.9184

Testing...
Test loss: 0.5894 score: 0.9388 time: 0.44s
test Score 0.9388
Epoch Time List: [1.7259658421389759, 1.4056893221568316, 1.6713143668603152, 1.4089211618993431, 1.566130296792835, 1.3976833377964795, 1.5425519261043519, 1.3914607306942344, 1.3805462021846324, 1.5168517308775336, 1.4328091552015394, 1.9106276859529316, 1.8901375560089946, 1.7713006306439638, 1.8837954432237893, 1.7917112549766898, 1.909385637845844, 1.746612090151757, 1.861304609104991, 1.5197515147738159, 1.539622592041269, 1.6670328222680837, 1.6998512798454612, 1.721070768777281, 1.9162404600065202, 1.4924935901071876, 1.465068151243031, 1.519483664771542, 1.5989199820905924, 1.679855754133314, 1.7558292739558965, 1.4268978098407388, 1.8321853750385344, 1.4673961570952088, 1.4623532930854708, 1.5891130201052874, 1.681268696906045, 1.5301661321427673, 1.803276140941307, 1.6815261179581285, 1.4635201448109, 5.460340854944661, 1.4879453028552234, 1.3535601641051471, 1.4416017311159521, 1.340865921229124, 1.497775553027168, 1.345064983703196, 1.5138882105238736, 1.4637773700524122, 1.5534620368853211, 1.518580679083243, 1.4034949950873852, 1.4168247098568827, 1.7984646640252322, 1.531472935108468, 1.6558873252943158, 1.460636424832046, 1.7493402201216668, 1.4563838699832559, 1.5554202641360462, 1.7542230961844325, 1.5812625428661704, 1.3945982051081955, 1.571562142809853, 1.3909015899989754, 1.5915916934609413, 1.428738476941362, 1.6757907888386399, 1.7170023971702904, 1.7820384909864515, 1.610651802038774, 1.9151470472570509, 1.8366654620040208, 1.3928228067234159, 1.787750987103209, 1.427228454966098, 1.42708878708072, 1.8080893768928945, 1.7196042120922357, 1.5332433399744332, 1.3811422321014106, 1.5239019168075174, 1.3892480537761003, 1.8344153619837016, 1.6783148089889437, 1.865312703885138, 1.4439739568624645, 1.8089915469754487, 1.771590385120362, 1.674103077268228, 1.4105792418122292, 1.7397592950146645, 1.7367665963247418, 1.6112890320364386, 1.4411988072097301, 1.5469975117594004, 1.5781767279841006, 1.4070728258229792, 1.938510209089145, 1.8181990499142557, 1.7692862029653043, 1.8972247869241983, 1.494204621994868, 1.594682231079787, 1.4001057259738445, 1.7931100081186742, 1.558000888908282, 1.5381759577430785, 1.4077624422498047, 1.563934648875147]
Total Epoch List: [111]
Total Time List: [0.47090889886021614]
========================k_idx:1========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: FAGNN
FAGNN(
  (conv1): NeighborFilterConv()
  (conv2): NeighborFilterConv()
  (ffn): Sequential(
    (0): Linear(in_features=14951, out_features=64, bias=True)
    (1): ReLU()
    (2): Linear(in_features=64, out_features=64, bias=True)
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=64, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=64, out_features=2, bias=True)
  )
)
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7f904e47f2b0>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.6990;  Loss pred: 0.6990; Loss self: 0.0000; time: 0.53s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6968 score: 0.5102 time: 0.42s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7012 score: 0.4898 time: 0.48s
Epoch 2/1000, LR 0.000000
Train loss: 0.6990;  Loss pred: 0.6990; Loss self: 0.0000; time: 0.63s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6967 score: 0.5102 time: 0.43s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7012 score: 0.4898 time: 0.48s
Epoch 3/1000, LR 0.000030
Train loss: 0.6989;  Loss pred: 0.6989; Loss self: 0.0000; time: 0.49s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6966 score: 0.5102 time: 0.42s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7011 score: 0.4898 time: 0.47s
Epoch 4/1000, LR 0.000060
Train loss: 0.6988;  Loss pred: 0.6988; Loss self: 0.0000; time: 0.62s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6965 score: 0.5102 time: 0.42s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7009 score: 0.4898 time: 0.48s
Epoch 5/1000, LR 0.000090
Train loss: 0.6987;  Loss pred: 0.6987; Loss self: 0.0000; time: 0.47s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6963 score: 0.5102 time: 0.43s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7007 score: 0.4898 time: 0.48s
Epoch 6/1000, LR 0.000120
Train loss: 0.6985;  Loss pred: 0.6985; Loss self: 0.0000; time: 0.48s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6961 score: 0.5102 time: 0.57s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7005 score: 0.4898 time: 0.47s
Epoch 7/1000, LR 0.000150
Train loss: 0.6982;  Loss pred: 0.6982; Loss self: 0.0000; time: 0.48s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6959 score: 0.5102 time: 0.42s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7002 score: 0.4898 time: 0.47s
Epoch 8/1000, LR 0.000180
Train loss: 0.6980;  Loss pred: 0.6980; Loss self: 0.0000; time: 0.72s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6956 score: 0.5102 time: 0.55s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6999 score: 0.4898 time: 0.61s
Epoch 9/1000, LR 0.000210
Train loss: 0.6976;  Loss pred: 0.6976; Loss self: 0.0000; time: 0.51s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6953 score: 0.5102 time: 0.54s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6995 score: 0.4898 time: 0.63s
Epoch 10/1000, LR 0.000240
Train loss: 0.6973;  Loss pred: 0.6973; Loss self: 0.0000; time: 0.50s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6950 score: 0.5102 time: 0.59s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6991 score: 0.4898 time: 0.49s
Epoch 11/1000, LR 0.000270
Train loss: 0.6968;  Loss pred: 0.6968; Loss self: 0.0000; time: 0.57s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6946 score: 0.5102 time: 0.57s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6987 score: 0.4898 time: 0.61s
Epoch 12/1000, LR 0.000270
Train loss: 0.6964;  Loss pred: 0.6964; Loss self: 0.0000; time: 0.60s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6943 score: 0.5102 time: 0.58s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6983 score: 0.4898 time: 0.47s
Epoch 13/1000, LR 0.000270
Train loss: 0.6961;  Loss pred: 0.6961; Loss self: 0.0000; time: 0.49s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6939 score: 0.5102 time: 0.42s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6979 score: 0.4898 time: 0.48s
Epoch 14/1000, LR 0.000270
Train loss: 0.6956;  Loss pred: 0.6956; Loss self: 0.0000; time: 0.47s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6935 score: 0.5102 time: 0.52s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6974 score: 0.4898 time: 0.50s
Epoch 15/1000, LR 0.000270
Train loss: 0.6951;  Loss pred: 0.6951; Loss self: 0.0000; time: 0.47s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6931 score: 0.5102 time: 0.42s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6970 score: 0.4898 time: 0.46s
Epoch 16/1000, LR 0.000270
Train loss: 0.6946;  Loss pred: 0.6946; Loss self: 0.0000; time: 0.47s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6927 score: 0.5102 time: 0.56s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6965 score: 0.4898 time: 0.51s
Epoch 17/1000, LR 0.000270
Train loss: 0.6942;  Loss pred: 0.6942; Loss self: 0.0000; time: 1.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6922 score: 0.5102 time: 3.48s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6961 score: 0.4898 time: 1.12s
Epoch 18/1000, LR 0.000270
Train loss: 0.6937;  Loss pred: 0.6937; Loss self: 0.0000; time: 0.49s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6917 score: 0.5102 time: 0.42s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6955 score: 0.4898 time: 0.59s
Epoch 19/1000, LR 0.000270
Train loss: 0.6930;  Loss pred: 0.6930; Loss self: 0.0000; time: 0.50s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6912 score: 0.5102 time: 0.43s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6950 score: 0.4898 time: 0.48s
Epoch 20/1000, LR 0.000270
Train loss: 0.6924;  Loss pred: 0.6924; Loss self: 0.0000; time: 0.47s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6906 score: 0.5102 time: 0.43s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6944 score: 0.4898 time: 0.60s
Epoch 21/1000, LR 0.000270
Train loss: 0.6917;  Loss pred: 0.6917; Loss self: 0.0000; time: 0.54s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6900 score: 0.5102 time: 0.41s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6938 score: 0.4898 time: 0.47s
Epoch 22/1000, LR 0.000270
Train loss: 0.6911;  Loss pred: 0.6911; Loss self: 0.0000; time: 0.48s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6893 score: 0.5102 time: 0.42s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6931 score: 0.4898 time: 0.70s
Epoch 23/1000, LR 0.000270
Train loss: 0.6901;  Loss pred: 0.6901; Loss self: 0.0000; time: 0.47s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6885 score: 0.5102 time: 0.43s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6924 score: 0.4898 time: 0.47s
Epoch 24/1000, LR 0.000270
Train loss: 0.6892;  Loss pred: 0.6892; Loss self: 0.0000; time: 0.50s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6876 score: 0.5102 time: 0.42s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6915 score: 0.4898 time: 0.57s
Epoch 25/1000, LR 0.000270
Train loss: 0.6884;  Loss pred: 0.6884; Loss self: 0.0000; time: 0.47s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6866 score: 0.5102 time: 0.46s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6906 score: 0.4898 time: 0.55s
Epoch 26/1000, LR 0.000270
Train loss: 0.6871;  Loss pred: 0.6871; Loss self: 0.0000; time: 0.46s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6855 score: 0.5102 time: 0.41s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6896 score: 0.4898 time: 0.61s
Epoch 27/1000, LR 0.000270
Train loss: 0.6858;  Loss pred: 0.6858; Loss self: 0.0000; time: 0.47s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6844 score: 0.5102 time: 0.46s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6885 score: 0.4898 time: 0.58s
Epoch 28/1000, LR 0.000270
Train loss: 0.6848;  Loss pred: 0.6848; Loss self: 0.0000; time: 0.58s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6831 score: 0.5102 time: 0.43s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6873 score: 0.4898 time: 0.66s
Epoch 29/1000, LR 0.000270
Train loss: 0.6832;  Loss pred: 0.6832; Loss self: 0.0000; time: 0.49s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6817 score: 0.5102 time: 0.43s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6860 score: 0.4898 time: 0.47s
Epoch 30/1000, LR 0.000270
Train loss: 0.6812;  Loss pred: 0.6812; Loss self: 0.0000; time: 0.48s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6802 score: 0.5102 time: 0.42s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6846 score: 0.4898 time: 0.58s
Epoch 31/1000, LR 0.000270
Train loss: 0.6797;  Loss pred: 0.6797; Loss self: 0.0000; time: 0.60s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6785 score: 0.5102 time: 0.41s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6830 score: 0.4898 time: 0.46s
Epoch 32/1000, LR 0.000270
Train loss: 0.6786;  Loss pred: 0.6786; Loss self: 0.0000; time: 0.48s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6767 score: 0.5102 time: 0.42s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6813 score: 0.4898 time: 0.67s
Epoch 33/1000, LR 0.000270
Train loss: 0.6759;  Loss pred: 0.6759; Loss self: 0.0000; time: 0.48s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6748 score: 0.5102 time: 0.43s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6794 score: 0.4898 time: 0.52s
Epoch 34/1000, LR 0.000270
Train loss: 0.6732;  Loss pred: 0.6732; Loss self: 0.0000; time: 0.64s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6727 score: 0.5102 time: 0.54s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6773 score: 0.4898 time: 0.76s
Epoch 35/1000, LR 0.000270
Train loss: 0.6709;  Loss pred: 0.6709; Loss self: 0.0000; time: 0.59s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6703 score: 0.5102 time: 0.52s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6751 score: 0.4898 time: 0.71s
Epoch 36/1000, LR 0.000270
Train loss: 0.6683;  Loss pred: 0.6683; Loss self: 0.0000; time: 0.59s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6677 score: 0.5102 time: 0.44s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6725 score: 0.4898 time: 0.64s
Epoch 37/1000, LR 0.000270
Train loss: 0.6648;  Loss pred: 0.6648; Loss self: 0.0000; time: 0.50s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6648 score: 0.5102 time: 0.44s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6697 score: 0.4898 time: 0.50s
Epoch 38/1000, LR 0.000270
Train loss: 0.6615;  Loss pred: 0.6615; Loss self: 0.0000; time: 0.50s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6616 score: 0.5102 time: 0.44s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6666 score: 0.4898 time: 0.63s
Epoch 39/1000, LR 0.000269
Train loss: 0.6583;  Loss pred: 0.6583; Loss self: 0.0000; time: 0.48s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6582 score: 0.5102 time: 0.44s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6632 score: 0.4898 time: 0.50s
Epoch 40/1000, LR 0.000269
Train loss: 0.6541;  Loss pred: 0.6541; Loss self: 0.0000; time: 0.48s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6545 score: 0.5102 time: 0.43s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6594 score: 0.4898 time: 0.80s
Epoch 41/1000, LR 0.000269
Train loss: 0.6493;  Loss pred: 0.6493; Loss self: 0.0000; time: 0.61s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6504 score: 0.5102 time: 0.54s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6553 score: 0.4898 time: 0.61s
Epoch 42/1000, LR 0.000269
Train loss: 0.6449;  Loss pred: 0.6449; Loss self: 0.0000; time: 0.61s
Val loss: 0.6460 score: 0.5306 time: 0.47s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6507 score: 0.4898 time: 0.62s
Epoch 43/1000, LR 0.000269
Train loss: 0.6394;  Loss pred: 0.6394; Loss self: 0.0000; time: 0.49s
Val loss: 0.6413 score: 0.5306 time: 0.42s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6457 score: 0.4898 time: 0.47s
Epoch 44/1000, LR 0.000269
Train loss: 0.6318;  Loss pred: 0.6318; Loss self: 0.0000; time: 0.49s
Val loss: 0.6363 score: 0.5510 time: 0.43s
Test loss: 0.6404 score: 0.5510 time: 0.64s
Epoch 45/1000, LR 0.000269
Train loss: 0.6283;  Loss pred: 0.6283; Loss self: 0.0000; time: 0.49s
Val loss: 0.6309 score: 0.5714 time: 0.42s
Test loss: 0.6346 score: 0.5510 time: 0.52s
Epoch 46/1000, LR 0.000269
Train loss: 0.6220;  Loss pred: 0.6220; Loss self: 0.0000; time: 0.48s
Val loss: 0.6252 score: 0.5918 time: 0.44s
Test loss: 0.6282 score: 0.5510 time: 0.48s
Epoch 47/1000, LR 0.000269
Train loss: 0.6111;  Loss pred: 0.6111; Loss self: 0.0000; time: 0.64s
Val loss: 0.6192 score: 0.5918 time: 0.47s
Test loss: 0.6214 score: 0.5918 time: 0.49s
Epoch 48/1000, LR 0.000269
Train loss: 0.6060;  Loss pred: 0.6060; Loss self: 0.0000; time: 0.50s
Val loss: 0.6127 score: 0.6122 time: 0.46s
Test loss: 0.6141 score: 0.5714 time: 0.60s
Epoch 49/1000, LR 0.000269
Train loss: 0.5992;  Loss pred: 0.5992; Loss self: 0.0000; time: 0.75s
Val loss: 0.6058 score: 0.6122 time: 0.54s
Test loss: 0.6061 score: 0.6122 time: 0.62s
Epoch 50/1000, LR 0.000269
Train loss: 0.5870;  Loss pred: 0.5870; Loss self: 0.0000; time: 0.59s
Val loss: 0.5985 score: 0.6327 time: 0.54s
Test loss: 0.5977 score: 0.6122 time: 0.63s
Epoch 51/1000, LR 0.000269
Train loss: 0.5805;  Loss pred: 0.5805; Loss self: 0.0000; time: 0.71s
Val loss: 0.5907 score: 0.6939 time: 0.42s
Test loss: 0.5886 score: 0.6327 time: 0.48s
Epoch 52/1000, LR 0.000269
Train loss: 0.5689;  Loss pred: 0.5689; Loss self: 0.0000; time: 0.54s
Val loss: 0.5826 score: 0.7551 time: 0.51s
Test loss: 0.5789 score: 0.6735 time: 0.49s
Epoch 53/1000, LR 0.000269
Train loss: 0.5560;  Loss pred: 0.5560; Loss self: 0.0000; time: 0.48s
Val loss: 0.5739 score: 0.8367 time: 0.61s
Test loss: 0.5686 score: 0.7143 time: 0.47s
Epoch 54/1000, LR 0.000269
Train loss: 0.5432;  Loss pred: 0.5432; Loss self: 0.0000; time: 0.61s
Val loss: 0.5649 score: 0.8571 time: 0.42s
Test loss: 0.5577 score: 0.7755 time: 0.57s
Epoch 55/1000, LR 0.000269
Train loss: 0.5404;  Loss pred: 0.5404; Loss self: 0.0000; time: 0.68s
Val loss: 0.5555 score: 0.8571 time: 0.56s
Test loss: 0.5463 score: 0.7959 time: 0.60s
Epoch 56/1000, LR 0.000269
Train loss: 0.5259;  Loss pred: 0.5259; Loss self: 0.0000; time: 0.59s
Val loss: 0.5459 score: 0.8571 time: 0.54s
Test loss: 0.5345 score: 0.8367 time: 0.63s
Epoch 57/1000, LR 0.000269
Train loss: 0.5109;  Loss pred: 0.5109; Loss self: 0.0000; time: 0.72s
Val loss: 0.5361 score: 0.8776 time: 0.43s
Test loss: 0.5224 score: 0.8571 time: 0.49s
Epoch 58/1000, LR 0.000269
Train loss: 0.4980;  Loss pred: 0.4980; Loss self: 0.0000; time: 0.50s
Val loss: 0.5260 score: 0.8776 time: 0.46s
Test loss: 0.5099 score: 0.8980 time: 0.49s
Epoch 59/1000, LR 0.000268
Train loss: 0.4854;  Loss pred: 0.4854; Loss self: 0.0000; time: 0.63s
Val loss: 0.5158 score: 0.8980 time: 0.68s
Test loss: 0.4972 score: 0.8980 time: 0.62s
Epoch 60/1000, LR 0.000268
Train loss: 0.4686;  Loss pred: 0.4686; Loss self: 0.0000; time: 0.52s
Val loss: 0.5055 score: 0.8980 time: 0.54s
Test loss: 0.4844 score: 0.8980 time: 0.61s
Epoch 61/1000, LR 0.000268
Train loss: 0.4523;  Loss pred: 0.4523; Loss self: 0.0000; time: 0.76s
Val loss: 0.4954 score: 0.8980 time: 0.56s
Test loss: 0.4715 score: 0.8980 time: 0.48s
Epoch 62/1000, LR 0.000268
Train loss: 0.4399;  Loss pred: 0.4399; Loss self: 0.0000; time: 0.62s
Val loss: 0.4854 score: 0.8980 time: 0.55s
Test loss: 0.4587 score: 0.8980 time: 0.64s
Epoch 63/1000, LR 0.000268
Train loss: 0.4295;  Loss pred: 0.4295; Loss self: 0.0000; time: 0.66s
Val loss: 0.4756 score: 0.8980 time: 0.71s
Test loss: 0.4456 score: 0.9184 time: 0.63s
Epoch 64/1000, LR 0.000268
Train loss: 0.4147;  Loss pred: 0.4147; Loss self: 0.0000; time: 0.64s
Val loss: 0.4659 score: 0.8980 time: 0.54s
Test loss: 0.4324 score: 0.9184 time: 0.61s
Epoch 65/1000, LR 0.000268
Train loss: 0.4023;  Loss pred: 0.4023; Loss self: 0.0000; time: 0.61s
Val loss: 0.4564 score: 0.8980 time: 0.76s
Test loss: 0.4191 score: 0.9184 time: 0.66s
Epoch 66/1000, LR 0.000268
Train loss: 0.3836;  Loss pred: 0.3836; Loss self: 0.0000; time: 0.59s
Val loss: 0.4472 score: 0.8980 time: 0.53s
Test loss: 0.4059 score: 0.9184 time: 0.60s
Epoch 67/1000, LR 0.000268
Train loss: 0.3759;  Loss pred: 0.3759; Loss self: 0.0000; time: 0.60s
Val loss: 0.4382 score: 0.8980 time: 0.71s
Test loss: 0.3924 score: 0.9184 time: 0.63s
Epoch 68/1000, LR 0.000268
Train loss: 0.3576;  Loss pred: 0.3576; Loss self: 0.0000; time: 0.60s
Val loss: 0.4293 score: 0.8980 time: 0.56s
Test loss: 0.3791 score: 0.9388 time: 0.50s
Epoch 69/1000, LR 0.000268
Train loss: 0.3507;  Loss pred: 0.3507; Loss self: 0.0000; time: 0.49s
Val loss: 0.4205 score: 0.8980 time: 0.58s
Test loss: 0.3656 score: 0.9388 time: 0.52s
Epoch 70/1000, LR 0.000268
Train loss: 0.3394;  Loss pred: 0.3394; Loss self: 0.0000; time: 0.49s
Val loss: 0.4121 score: 0.8980 time: 0.43s
Test loss: 0.3522 score: 0.9388 time: 0.48s
Epoch 71/1000, LR 0.000268
Train loss: 0.3232;  Loss pred: 0.3232; Loss self: 0.0000; time: 0.48s
Val loss: 0.4040 score: 0.8980 time: 0.55s
Test loss: 0.3390 score: 0.9388 time: 0.64s
Epoch 72/1000, LR 0.000267
Train loss: 0.3136;  Loss pred: 0.3136; Loss self: 0.0000; time: 0.48s
Val loss: 0.3962 score: 0.8980 time: 0.42s
Test loss: 0.3257 score: 0.9592 time: 0.48s
Epoch 73/1000, LR 0.000267
Train loss: 0.3062;  Loss pred: 0.3062; Loss self: 0.0000; time: 0.47s
Val loss: 0.3887 score: 0.8776 time: 0.42s
Test loss: 0.3126 score: 0.9592 time: 0.62s
Epoch 74/1000, LR 0.000267
Train loss: 0.2804;  Loss pred: 0.2804; Loss self: 0.0000; time: 0.48s
Val loss: 0.3817 score: 0.8776 time: 0.43s
Test loss: 0.2997 score: 0.9592 time: 0.47s
Epoch 75/1000, LR 0.000267
Train loss: 0.2705;  Loss pred: 0.2705; Loss self: 0.0000; time: 0.48s
Val loss: 0.3751 score: 0.8571 time: 0.42s
Test loss: 0.2870 score: 0.9592 time: 0.61s
Epoch 76/1000, LR 0.000267
Train loss: 0.2646;  Loss pred: 0.2646; Loss self: 0.0000; time: 0.48s
Val loss: 0.3689 score: 0.8571 time: 0.43s
Test loss: 0.2745 score: 0.9592 time: 0.49s
Epoch 77/1000, LR 0.000267
Train loss: 0.2499;  Loss pred: 0.2499; Loss self: 0.0000; time: 0.48s
Val loss: 0.3632 score: 0.8571 time: 0.44s
Test loss: 0.2622 score: 0.9388 time: 0.62s
Epoch 78/1000, LR 0.000267
Train loss: 0.2462;  Loss pred: 0.2462; Loss self: 0.0000; time: 0.48s
Val loss: 0.3578 score: 0.8571 time: 0.43s
Test loss: 0.2501 score: 0.9388 time: 0.47s
Epoch 79/1000, LR 0.000267
Train loss: 0.2223;  Loss pred: 0.2223; Loss self: 0.0000; time: 0.48s
Val loss: 0.3531 score: 0.8571 time: 0.44s
Test loss: 0.2383 score: 0.9388 time: 0.62s
Epoch 80/1000, LR 0.000267
Train loss: 0.2230;  Loss pred: 0.2230; Loss self: 0.0000; time: 0.48s
Val loss: 0.3489 score: 0.8571 time: 0.42s
Test loss: 0.2270 score: 0.9388 time: 0.49s
Epoch 81/1000, LR 0.000267
Train loss: 0.2031;  Loss pred: 0.2031; Loss self: 0.0000; time: 0.49s
Val loss: 0.3454 score: 0.8571 time: 0.42s
Test loss: 0.2161 score: 0.9388 time: 0.63s
Epoch 82/1000, LR 0.000267
Train loss: 0.1993;  Loss pred: 0.1993; Loss self: 0.0000; time: 0.50s
Val loss: 0.3426 score: 0.8776 time: 0.51s
Test loss: 0.2057 score: 0.9388 time: 0.47s
Epoch 83/1000, LR 0.000266
Train loss: 0.1854;  Loss pred: 0.1854; Loss self: 0.0000; time: 0.49s
Val loss: 0.3406 score: 0.8776 time: 0.44s
Test loss: 0.1956 score: 0.9592 time: 0.65s
Epoch 84/1000, LR 0.000266
Train loss: 0.1686;  Loss pred: 0.1686; Loss self: 0.0000; time: 0.49s
Val loss: 0.3395 score: 0.8776 time: 0.44s
Test loss: 0.1860 score: 0.9592 time: 0.62s
Epoch 85/1000, LR 0.000266
Train loss: 0.1635;  Loss pred: 0.1635; Loss self: 0.0000; time: 0.61s
Val loss: 0.3392 score: 0.8776 time: 0.56s
Test loss: 0.1770 score: 0.9592 time: 0.65s
Epoch 86/1000, LR 0.000266
Train loss: 0.1524;  Loss pred: 0.1524; Loss self: 0.0000; time: 0.49s
Val loss: 0.3398 score: 0.8776 time: 0.44s
Test loss: 0.1684 score: 0.9592 time: 0.49s
     INFO: Early stopping counter 1 of 20
Epoch 87/1000, LR 0.000266
Train loss: 0.1493;  Loss pred: 0.1493; Loss self: 0.0000; time: 0.61s
Val loss: 0.3415 score: 0.8776 time: 0.43s
Test loss: 0.1604 score: 0.9592 time: 0.65s
     INFO: Early stopping counter 2 of 20
Epoch 88/1000, LR 0.000266
Train loss: 0.1378;  Loss pred: 0.1378; Loss self: 0.0000; time: 0.51s
Val loss: 0.3441 score: 0.8776 time: 0.43s
Test loss: 0.1531 score: 0.9388 time: 0.51s
     INFO: Early stopping counter 3 of 20
Epoch 89/1000, LR 0.000266
Train loss: 0.1327;  Loss pred: 0.1327; Loss self: 0.0000; time: 0.60s
Val loss: 0.3475 score: 0.8776 time: 0.55s
Test loss: 0.1464 score: 0.9388 time: 0.76s
     INFO: Early stopping counter 4 of 20
Epoch 90/1000, LR 0.000266
Train loss: 0.1408;  Loss pred: 0.1408; Loss self: 0.0000; time: 0.61s
Val loss: 0.3512 score: 0.8776 time: 0.54s
Test loss: 0.1401 score: 0.9388 time: 0.61s
     INFO: Early stopping counter 5 of 20
Epoch 91/1000, LR 0.000266
Train loss: 0.1176;  Loss pred: 0.1176; Loss self: 0.0000; time: 0.58s
Val loss: 0.3556 score: 0.8776 time: 0.57s
Test loss: 0.1345 score: 0.9388 time: 0.76s
     INFO: Early stopping counter 6 of 20
Epoch 92/1000, LR 0.000266
Train loss: 0.1169;  Loss pred: 0.1169; Loss self: 0.0000; time: 0.61s
Val loss: 0.3606 score: 0.8980 time: 0.55s
Test loss: 0.1293 score: 0.9388 time: 0.65s
     INFO: Early stopping counter 7 of 20
Epoch 93/1000, LR 0.000265
Train loss: 0.1137;  Loss pred: 0.1137; Loss self: 0.0000; time: 0.54s
Val loss: 0.3665 score: 0.8776 time: 0.43s
Test loss: 0.1247 score: 0.9388 time: 0.64s
     INFO: Early stopping counter 8 of 20
Epoch 94/1000, LR 0.000265
Train loss: 0.1080;  Loss pred: 0.1080; Loss self: 0.0000; time: 0.50s
Val loss: 0.3731 score: 0.8776 time: 0.44s
Test loss: 0.1206 score: 0.9388 time: 0.73s
     INFO: Early stopping counter 9 of 20
Epoch 95/1000, LR 0.000265
Train loss: 0.0971;  Loss pred: 0.0971; Loss self: 0.0000; time: 0.59s
Val loss: 0.3803 score: 0.8776 time: 0.53s
Test loss: 0.1170 score: 0.9388 time: 0.76s
     INFO: Early stopping counter 10 of 20
Epoch 96/1000, LR 0.000265
Train loss: 0.0973;  Loss pred: 0.0973; Loss self: 0.0000; time: 0.59s
Val loss: 0.3880 score: 0.8776 time: 0.53s
Test loss: 0.1141 score: 0.9388 time: 0.60s
     INFO: Early stopping counter 11 of 20
Epoch 97/1000, LR 0.000265
Train loss: 0.1001;  Loss pred: 0.1001; Loss self: 0.0000; time: 0.49s
Val loss: 0.3961 score: 0.8571 time: 0.53s
Test loss: 0.1117 score: 0.9388 time: 0.72s
     INFO: Early stopping counter 12 of 20
Epoch 98/1000, LR 0.000265
Train loss: 0.0924;  Loss pred: 0.0924; Loss self: 0.0000; time: 0.59s
Val loss: 0.4046 score: 0.8571 time: 0.54s
Test loss: 0.1099 score: 0.9388 time: 0.60s
     INFO: Early stopping counter 13 of 20
Epoch 99/1000, LR 0.000265
Train loss: 0.0850;  Loss pred: 0.0850; Loss self: 0.0000; time: 0.63s
Val loss: 0.4133 score: 0.8571 time: 0.54s
Test loss: 0.1088 score: 0.9388 time: 0.61s
     INFO: Early stopping counter 14 of 20
Epoch 100/1000, LR 0.000265
Train loss: 0.0844;  Loss pred: 0.0844; Loss self: 0.0000; time: 0.79s
Val loss: 0.4225 score: 0.8571 time: 0.56s
Test loss: 0.1085 score: 0.9388 time: 0.61s
     INFO: Early stopping counter 15 of 20
Epoch 101/1000, LR 0.000265
Train loss: 0.0773;  Loss pred: 0.0773; Loss self: 0.0000; time: 0.52s
Val loss: 0.4321 score: 0.8571 time: 0.46s
Test loss: 0.1093 score: 0.9388 time: 0.54s
     INFO: Early stopping counter 16 of 20
Epoch 102/1000, LR 0.000264
Train loss: 0.0844;  Loss pred: 0.0844; Loss self: 0.0000; time: 0.67s
Val loss: 0.4407 score: 0.8571 time: 0.41s
Test loss: 0.1083 score: 0.9388 time: 0.47s
     INFO: Early stopping counter 17 of 20
Epoch 103/1000, LR 0.000264
Train loss: 0.0751;  Loss pred: 0.0751; Loss self: 0.0000; time: 0.46s
Val loss: 0.4496 score: 0.8571 time: 0.43s
Test loss: 0.1078 score: 0.9388 time: 0.60s
     INFO: Early stopping counter 18 of 20
Epoch 104/1000, LR 0.000264
Train loss: 0.0795;  Loss pred: 0.0795; Loss self: 0.0000; time: 0.73s
Val loss: 0.4586 score: 0.8571 time: 0.53s
Test loss: 0.1072 score: 0.9388 time: 0.60s
     INFO: Early stopping counter 19 of 20
Epoch 105/1000, LR 0.000264
Train loss: 0.0774;  Loss pred: 0.0774; Loss self: 0.0000; time: 0.61s
Val loss: 0.4678 score: 0.8571 time: 0.54s
Test loss: 0.1074 score: 0.9388 time: 0.59s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 084,   Train_Loss: 0.1635,   Val_Loss: 0.3392,   Val_Precision: 0.8800,   Val_Recall: 0.8800,   Val_accuracy: 0.8800,   Val_Score: 0.8776,   Val_Loss: 0.3392,   Test_Precision: 1.0000,   Test_Recall: 0.9167,   Test_accuracy: 0.9565,   Test_Score: 0.9592,   Test_loss: 0.1770


[0.6227087839506567, 0.4557098669465631, 0.7132796109654009, 0.46749154408462346, 0.594128652010113, 0.4530330009292811, 0.5804642019793391, 0.4509469398763031, 0.4495153359603137, 0.4430166510865092, 0.5019226549193263, 0.5697651370428503, 0.7001396778505296, 0.580072927987203, 0.6830095530021936, 0.581694952910766, 0.7145566779654473, 0.5911042669322342, 0.7009357118513435, 0.469384070020169, 0.46560450620017946, 0.5480443921405822, 0.5570869690272957, 0.5561958570033312, 0.7029278031550348, 0.4620986790396273, 0.45321387983858585, 0.5818643730599433, 0.5562601159326732, 0.5559133400674909, 0.4538283220026642, 0.5003790690097958, 0.5769332370255142, 0.5302128691691905, 0.5397878438234329, 0.6027016357984394, 0.6666738078929484, 0.4413969258312136, 0.55328563391231, 0.5266274919267744, 0.4431350198574364, 1.5140820879023522, 0.4329633731395006, 0.44185982597991824, 0.42989791999571025, 0.42984618502669036, 0.47912067314609885, 0.4370650190394372, 0.4474087511189282, 0.5384855889715254, 0.5509043079800904, 0.5833360799588263, 0.4381748731248081, 0.4823975651524961, 0.5490636611357331, 0.42604362103156745, 0.6519483160227537, 0.5290174090769142, 0.45138902706094086, 0.4965191911906004, 0.45546926092356443, 0.5475415450055152, 0.48401307198219, 0.4461718969978392, 0.4887299248948693, 0.45831945119425654, 0.49927506293170154, 0.4562228568829596, 0.5492180048022419, 0.5551030188798904, 0.48647005786187947, 0.43406992591917515, 0.5619298999663442, 0.6450574991758913, 0.44510894804261625, 0.6258882849942893, 0.45456719398498535, 0.4534809950273484, 0.557323485147208, 0.5562262239400297, 0.5869244011119008, 0.44932498782873154, 0.44583860388956964, 0.44897839589975774, 0.5881211729720235, 0.5557046618778259, 0.5570933748967946, 0.4364910169970244, 0.5264528181869537, 0.563808973878622, 0.4453510718885809, 0.4492754810489714, 0.5629205820150673, 0.5639159770216793, 0.43987377313897014, 0.4735481219831854, 0.456806463887915, 0.5943967460189015, 0.4473464419133961, 0.7159024169668555, 0.5613610420841724, 0.5945262049790472, 0.5648923490662128, 0.46442935382947326, 0.6181377992033958, 0.45040287892334163, 0.6706126190256327, 0.4311196890193969, 0.4454271518625319, 0.47128826519474387, 0.4702799858059734, 0.4838031630497426, 0.49008012004196644, 0.47754836012609303, 0.4861374159809202, 0.4897858831100166, 0.477929265005514, 0.4722796392161399, 0.6133575800340623, 0.638564552878961, 0.49466609419323504, 0.6175617049448192, 0.47465863707475364, 0.48376587219536304, 0.5068948101252317, 0.46861471701413393, 0.5163122890517116, 1.1293568490073085, 0.5935565619729459, 0.4867815428879112, 0.6026110390666872, 0.472439874894917, 0.7044080849736929, 0.4724572179839015, 0.573514404008165, 0.5565581559203565, 0.6128846530336887, 0.5874184619169682, 0.6623768249992281, 0.4719974279869348, 0.5888507629279047, 0.4647821611724794, 0.6741535090841353, 0.5224992469884455, 0.7640027590095997, 0.7171591138467193, 0.6490168969612569, 0.5016352899838239, 0.6387041271664202, 0.5056630140170455, 0.8092530120629817, 0.6146644770633429, 0.6275771921500564, 0.4788820478133857, 0.6447817890439183, 0.5241320501081645, 0.48838517093099654, 0.4914611750282347, 0.6110377840232104, 0.6273848321288824, 0.6343714199028909, 0.4899973841384053, 0.4965809527784586, 0.4784010660368949, 0.5759847369045019, 0.6048508821986616, 0.6357171258423477, 0.4929681411013007, 0.4986348270904273, 0.6241722339764237, 0.6209014360792935, 0.48372736806049943, 0.6490627839230001, 0.6326934339012951, 0.6139711651485413, 0.6664100920315832, 0.6055992799811065, 0.6376063989009708, 0.5085851410403848, 0.5259205270558596, 0.4896927459631115, 0.6420552409254014, 0.48120128409937024, 0.6297723259776831, 0.4745284749660641, 0.6151331141591072, 0.495209455024451, 0.6239615499507636, 0.4716190667822957, 0.624315895838663, 0.4913617230486125, 0.6356583461165428, 0.47924294299446046, 0.6579037820920348, 0.6232137538027018, 0.6504862490110099, 0.4978836739901453, 0.6530008350964636, 0.5101751899346709, 0.7648375248536468, 0.6126242259051651, 0.7683067838661373, 0.655318435980007, 0.6440257809590548, 0.7323015369474888, 0.7662546532228589, 0.6021760110743344, 0.7277417909353971, 0.6038949550129473, 0.6180769610218704, 0.6193905789405107, 0.5503127900883555, 0.47143502393737435, 0.6049219300039113, 0.6071856829803437, 0.5971738710068166]
[0.012708342529605237, 0.00930020136625639, 0.014556726754395937, 0.00954064375682905, 0.012125074530818633, 0.00924557144753635, 0.011846208203659982, 0.009202998772985777, 0.009173782366537014, 0.009041156144622637, 0.010243319488149516, 0.011627859939650004, 0.01428856485409244, 0.011838223020147, 0.013938970469432523, 0.011871325569607469, 0.014582789346233619, 0.012063352386372126, 0.014304810445945787, 0.00957926673510549, 0.0095021327795955, 0.011184579431440453, 0.011369121816883586, 0.01135093585721084, 0.014345465370510914, 0.009430585286523007, 0.00924926285384869, 0.011874783123672312, 0.011352247263932106, 0.011345170205458999, 0.00926180248985029, 0.010211817734893791, 0.01177414769439825, 0.010820670799371235, 0.011016078445376182, 0.01230003338364162, 0.01360558791618262, 0.009008100527167624, 0.011291543549230816, 0.010747499835240294, 0.009043571833825233, 0.03089963444698678, 0.008835987206928584, 0.009017547468977923, 0.008773426938687965, 0.00877237112299368, 0.009777972921348957, 0.008919694266110962, 0.0091307908391618, 0.010989501815745416, 0.01124294506081817, 0.011904817958343394, 0.008942344349485879, 0.009844848268418287, 0.011205380839504758, 0.008694767776154438, 0.01330506767393375, 0.010796273654630902, 0.009212020960427364, 0.010133044718175518, 0.009295291039256416, 0.011174317245010515, 0.009877817795554899, 0.009105548918323249, 0.009974080099895293, 0.009353458187637888, 0.010189286998606153, 0.00931067054863183, 0.011208530710249836, 0.01132863303836511, 0.009927960364528152, 0.00885856991671786, 0.01146795714217029, 0.013164438758691659, 0.009083856082502373, 0.012773230306005903, 0.00927688150989766, 0.0092547141842316, 0.011373948676473632, 0.011351555590612851, 0.01197804900228369, 0.00916989771079044, 0.009098747018154482, 0.009162824406117504, 0.012002472917796398, 0.011340911466894405, 0.011369252548914174, 0.008907979938714785, 0.01074393506503987, 0.011506305589359634, 0.009088797385481243, 0.009168887368346356, 0.01148817514316464, 0.011508489326973046, 0.00897701577834633, 0.009664247387411947, 0.009322580895671735, 0.012130545837120438, 0.00912951922272237, 0.014610253407486848, 0.011456347797636171, 0.01213318785671525, 0.011528415287065566, 0.009478150078152515, 0.012615057126599915, 0.009191895488231462, 0.013685971816849649, 0.008798361000395855, 0.009090350038010855, 0.009618127861117222, 0.009597550730734152, 0.009873533939790666, 0.010001635102897274, 0.009745884900532511, 0.009921171754712656, 0.00999563026755136, 0.009753658469500286, 0.009638359984002856, 0.01251750163334821, 0.01303192965059104, 0.010095226412106837, 0.012603300100914677, 0.009686910960709259, 0.009872772901946185, 0.010344792043372077, 0.009563565653349673, 0.010536985490851256, 0.023048098959332824, 0.012113399223937673, 0.009934317201794106, 0.01229818447074872, 0.009641630099896265, 0.014375675203544753, 0.009641984040487786, 0.011704375592003368, 0.011358329712660337, 0.012507850061912013, 0.011988131875856494, 0.01351789438773935, 0.009632600571161934, 0.012017362508732749, 0.009485350228009783, 0.013758234879268067, 0.010663249938539704, 0.015591893041012238, 0.014635900282586108, 0.01324524279512769, 0.01023745489762906, 0.013034778105437147, 0.010319653347286642, 0.016515367593122075, 0.012544173001292713, 0.012807697798980743, 0.00977310301659971, 0.013158812021304454, 0.010696572451187032, 0.009967044304714215, 0.010029819898535403, 0.012470158857616539, 0.012803772084262907, 0.012946355508222264, 0.009999946615069496, 0.010134305158744052, 0.009763287061977446, 0.011754790549071468, 0.012343895555074726, 0.012973818894741791, 0.01006057430818981, 0.010176220961029128, 0.01273820885666171, 0.012671457879169255, 0.009871987103275498, 0.013246179263734696, 0.012912110895944797, 0.01253002377854166, 0.013600205959828228, 0.012359168979206256, 0.013012375487774914, 0.010379288592660914, 0.010733071980731828, 0.009993729509451255, 0.01310316818215105, 0.009820434369374903, 0.012852496448524145, 0.009684254591144165, 0.012553737023655248, 0.010106315408662265, 0.012733909182668644, 0.009624878913924401, 0.012741140731401285, 0.010027790266298214, 0.012972619308500874, 0.009780468224376743, 0.01342660779779663, 0.012718648036789834, 0.013275229571653264, 0.010160891305921333, 0.013326547655029868, 0.010411738570095325, 0.015608929078645853, 0.012502535222554388, 0.015679730282982394, 0.013373845632245042, 0.01314338328487867, 0.014944929325458954, 0.015637850065772632, 0.012289306348455804, 0.01485187328439586, 0.012324386836998925, 0.01261381553105858, 0.012640624060010423, 0.011230873267109297, 0.009621122937497437, 0.012345345510283903, 0.01239154455061926, 0.012187221857281973]
[78.68846764795717, 107.52455356808365, 68.69676245712405, 104.81473006307515, 82.47371984875414, 108.15989100018996, 84.41519706626816, 108.66023398105537, 109.0062920663647, 110.60532347898503, 97.6246031530012, 86.00034788775584, 69.98603500151987, 84.47213727078294, 71.7413098903503, 84.23659128346738, 68.57398651639137, 82.89569664977142, 69.90655372741526, 104.39212391228895, 105.2395312921074, 89.40881560454086, 87.95754114578676, 88.09846276813695, 69.70843915985034, 106.03795730781131, 108.11672408941132, 84.21206430343184, 88.08828567160961, 88.14323468843386, 107.9703439039936, 97.9257587591873, 84.93183761197143, 92.41571234734428, 90.776405138957, 81.30059234879361, 73.499212688236, 111.01119453364109, 88.56185123318417, 93.04489558781574, 110.57577894828552, 32.36284240564912, 113.17354547728041, 110.89489724785925, 113.98054682490427, 113.99426517408186, 102.27068616815532, 112.11146595005499, 109.519538626492, 90.99593564534754, 88.9446665967456, 83.99960448779129, 111.82749857507923, 101.57596874376856, 89.24283916120756, 115.01169735004522, 75.15933210615096, 92.62455102470145, 108.55381292506394, 98.68702130627256, 107.58135444890767, 89.49092620817738, 101.23693519129382, 109.82314289561205, 100.25987258819968, 106.91232910215626, 98.14229397373882, 107.40364990649859, 89.21775974486403, 88.2719032926072, 100.72562372155755, 112.88503781099065, 87.19948876707711, 75.96222051925778, 110.08540766362795, 78.2887316710954, 107.79484452109077, 108.05303979066404, 87.92021385400159, 88.09365306962405, 83.48605017472744, 109.05247054427618, 109.90524277735464, 109.13665434124832, 83.31616383131116, 88.17633423196452, 87.95652974526504, 112.25889672853, 93.07576730000359, 86.90886855332107, 110.02555757238403, 109.06448730652843, 87.04602667857053, 86.89237758219473, 111.39559344566638, 103.47417237089132, 107.26643310376396, 82.43652127671949, 109.53479319164036, 68.44508251222817, 87.28785278379324, 82.41857060232846, 86.74219093425279, 105.5058204137363, 79.27035049975441, 108.791489337571, 73.06751857904881, 113.65753234665048, 110.00676495608516, 103.9703375167901, 104.19324972127617, 101.28085912278756, 99.98365164415166, 102.60740919948279, 100.7945457173433, 100.04371642739554, 102.52563211301712, 103.75209077682689, 79.88814615656797, 76.73460698543956, 99.05671841106371, 79.34429808010566, 103.23208338097307, 101.28866630801095, 96.66699879585317, 104.56351074975332, 94.90380345196931, 43.38752631028043, 82.5532108298609, 100.66117073646522, 81.31281510522989, 103.71690156530263, 69.56195001911424, 103.71309429686737, 85.43813312717147, 88.04111390474691, 79.9497911351789, 83.41583245459208, 73.97601810730184, 103.81412502391083, 83.21293455808812, 105.4257329420529, 72.68374241138116, 93.78003945924075, 64.13589404247729, 68.32514438416929, 75.49880477599501, 97.68052802182257, 76.7178383790725, 96.90247979724349, 60.54966650675435, 79.71828831577396, 78.07804460217524, 102.32164731114473, 75.99470213427887, 93.48789105701115, 100.33064662178934, 99.7026875972144, 80.191440335118, 78.10198380749829, 77.2418152247478, 100.000533852155, 98.67474724077978, 102.4245209274284, 85.07169871087085, 81.01170295376389, 77.07830732902352, 99.39790407252895, 98.26830645969673, 78.50397267407257, 78.91751758445338, 101.29672876782858, 75.49346721720681, 77.4466706535228, 79.80830824220412, 73.52829824443556, 80.91158893307914, 76.84991882838739, 96.34571686416828, 93.16997051684876, 100.06274424920962, 76.31742080225955, 101.82848969680072, 77.80589584328034, 103.26039971258676, 79.65755520572723, 98.94802997568094, 78.53047996926503, 103.89741096413074, 78.48590805809417, 99.7228674956275, 77.08543480842812, 102.24459372073923, 74.47897600495226, 78.62470893977175, 75.3282641631532, 98.4165630644275, 75.03818887576392, 96.0454388349903, 64.06589426868962, 79.98377786578955, 63.776607247213, 74.77280862199783, 76.08391068915185, 66.91232713268721, 63.947409381341465, 81.37155764903311, 67.33157365748947, 81.1399393110503, 79.27815319145371, 79.11001824376504, 89.04027106499342, 103.93797132584103, 81.0021881661377, 80.70019002998505, 82.05315466563786]
Elapsed: 0.557167653212134~0.11486247265880564
Time per graph: 0.011370768432900698~0.0023441320950776663
Speed: 90.71630285320424~14.552501497886608
Total Time: 0.5982
best val loss: 0.3392273783683777 test_score: 0.9592

Testing...
Test loss: 0.4972 score: 0.8980 time: 0.74s
test Score 0.8980
Epoch Time List: [1.7259658421389759, 1.4056893221568316, 1.6713143668603152, 1.4089211618993431, 1.566130296792835, 1.3976833377964795, 1.5425519261043519, 1.3914607306942344, 1.3805462021846324, 1.5168517308775336, 1.4328091552015394, 1.9106276859529316, 1.8901375560089946, 1.7713006306439638, 1.8837954432237893, 1.7917112549766898, 1.909385637845844, 1.746612090151757, 1.861304609104991, 1.5197515147738159, 1.539622592041269, 1.6670328222680837, 1.6998512798454612, 1.721070768777281, 1.9162404600065202, 1.4924935901071876, 1.465068151243031, 1.519483664771542, 1.5989199820905924, 1.679855754133314, 1.7558292739558965, 1.4268978098407388, 1.8321853750385344, 1.4673961570952088, 1.4623532930854708, 1.5891130201052874, 1.681268696906045, 1.5301661321427673, 1.803276140941307, 1.6815261179581285, 1.4635201448109, 5.460340854944661, 1.4879453028552234, 1.3535601641051471, 1.4416017311159521, 1.340865921229124, 1.497775553027168, 1.345064983703196, 1.5138882105238736, 1.4637773700524122, 1.5534620368853211, 1.518580679083243, 1.4034949950873852, 1.4168247098568827, 1.7984646640252322, 1.531472935108468, 1.6558873252943158, 1.460636424832046, 1.7493402201216668, 1.4563838699832559, 1.5554202641360462, 1.7542230961844325, 1.5812625428661704, 1.3945982051081955, 1.571562142809853, 1.3909015899989754, 1.5915916934609413, 1.428738476941362, 1.6757907888386399, 1.7170023971702904, 1.7820384909864515, 1.610651802038774, 1.9151470472570509, 1.8366654620040208, 1.3928228067234159, 1.787750987103209, 1.427228454966098, 1.42708878708072, 1.8080893768928945, 1.7196042120922357, 1.5332433399744332, 1.3811422321014106, 1.5239019168075174, 1.3892480537761003, 1.8344153619837016, 1.6783148089889437, 1.865312703885138, 1.4439739568624645, 1.8089915469754487, 1.771590385120362, 1.674103077268228, 1.4105792418122292, 1.7397592950146645, 1.7367665963247418, 1.6112890320364386, 1.4411988072097301, 1.5469975117594004, 1.5781767279841006, 1.4070728258229792, 1.938510209089145, 1.8181990499142557, 1.7692862029653043, 1.8972247869241983, 1.494204621994868, 1.594682231079787, 1.4001057259738445, 1.7931100081186742, 1.558000888908282, 1.5381759577430785, 1.4077624422498047, 1.563934648875147, 1.4272179640829563, 1.5458439090289176, 1.383167140884325, 1.5137511168140918, 1.3871108379680663, 1.5221678751986474, 1.3727104449644685, 1.8689556610770524, 1.6811258469242603, 1.583333780290559, 1.7572556077502668, 1.6448135029058903, 1.3912315459456295, 1.4864970110356808, 1.3496326999738812, 1.547614831943065, 5.770477800164372, 1.502276714425534, 1.4074702342040837, 1.4913860317319632, 1.4242889089509845, 1.5941078725736588, 1.3667625249363482, 1.4827726478688419, 1.4760952880606055, 1.47637375514023, 1.5148988731671125, 1.669751709094271, 1.3860951089300215, 1.476971887750551, 1.47719318815507, 1.5673923501744866, 1.4291048229206353, 1.9399623679928482, 1.8144840830937028, 1.682489317143336, 1.4370736628770828, 1.5761086540296674, 1.422530840151012, 1.7054048681166023, 1.7581660780124366, 1.7028204801026732, 1.384106561774388, 1.5547821868676692, 1.4332193688023835, 1.4043407759163529, 1.5965744608547539, 1.564194915117696, 1.9084489468950778, 1.7515611762646586, 1.6164230958092958, 1.542991860769689, 1.566185190808028, 1.6012222510762513, 1.8435022088233382, 1.7634059782139957, 1.6321536062750965, 1.455297561129555, 1.9279546868056059, 1.6810071968939155, 1.7982310520019382, 1.8044091330375522, 1.9967233932111412, 1.7839215623680502, 2.021853039972484, 1.725106640951708, 1.9331603650934994, 1.669701413018629, 1.5907036098651588, 1.4093908951617777, 1.6660247028339654, 1.3804213958792388, 1.514792773872614, 1.374052200699225, 1.5097465009894222, 1.4014294447842985, 1.5379891628399491, 1.3768980149179697, 1.5359509440604597, 1.389196892734617, 1.5412516039796174, 1.484690902987495, 1.5830440840218216, 1.5509305712766945, 1.8113931121770293, 1.4269965540152043, 1.6876306573394686, 1.447018755832687, 1.908394165802747, 1.7545633912086487, 1.908106692833826, 1.8000692271161824, 1.6122832261025906, 1.6621765848249197, 1.8825210079085082, 1.7127701845020056, 1.7343780873343349, 1.7317351570818573, 1.7715662932023406, 1.9618180259130895, 1.5164197809062898, 1.5445916769094765, 1.4899151809513569, 1.8568665059283376, 1.7377586748916656]
Total Epoch List: [111, 105]
Total Time List: [0.47090889886021614, 0.5981951949652284]
========================k_idx:2========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: FAGNN
FAGNN(
  (conv1): NeighborFilterConv()
  (conv2): NeighborFilterConv()
  (ffn): Sequential(
    (0): Linear(in_features=14951, out_features=64, bias=True)
    (1): ReLU()
    (2): Linear(in_features=64, out_features=64, bias=True)
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=64, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=64, out_features=2, bias=True)
  )
)
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7f904e3687f0>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.6951;  Loss pred: 0.6951; Loss self: 0.0000; time: 0.43s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6932 score: 0.5102 time: 0.52s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6941 score: 0.5000 time: 0.40s
Epoch 2/1000, LR 0.000000
Train loss: 0.6951;  Loss pred: 0.6951; Loss self: 0.0000; time: 0.43s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6932 score: 0.5102 time: 0.50s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6941 score: 0.5000 time: 0.39s
Epoch 3/1000, LR 0.000030
Train loss: 0.6951;  Loss pred: 0.6951; Loss self: 0.0000; time: 0.54s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6931 score: 0.5102 time: 0.50s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6940 score: 0.5000 time: 0.40s
Epoch 4/1000, LR 0.000060
Train loss: 0.6950;  Loss pred: 0.6950; Loss self: 0.0000; time: 0.43s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6931 score: 0.5102 time: 0.50s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6939 score: 0.5000 time: 0.50s
Epoch 5/1000, LR 0.000090
Train loss: 0.6949;  Loss pred: 0.6949; Loss self: 0.0000; time: 0.43s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6930 score: 0.5102 time: 0.51s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6938 score: 0.5000 time: 0.39s
Epoch 6/1000, LR 0.000120
Train loss: 0.6948;  Loss pred: 0.6948; Loss self: 0.0000; time: 0.43s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6929 score: 0.5102 time: 0.51s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6937 score: 0.5000 time: 0.39s
Epoch 7/1000, LR 0.000150
Train loss: 0.6947;  Loss pred: 0.6947; Loss self: 0.0000; time: 0.54s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6928 score: 0.5102 time: 0.49s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6935 score: 0.5000 time: 0.40s
Epoch 8/1000, LR 0.000180
Train loss: 0.6945;  Loss pred: 0.6945; Loss self: 0.0000; time: 0.41s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6926 score: 0.5102 time: 0.50s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6934 score: 0.5000 time: 0.55s
Epoch 9/1000, LR 0.000210
Train loss: 0.6943;  Loss pred: 0.6943; Loss self: 0.0000; time: 0.45s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6925 score: 0.5102 time: 0.52s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6931 score: 0.5000 time: 0.41s
Epoch 10/1000, LR 0.000240
Train loss: 0.6941;  Loss pred: 0.6941; Loss self: 0.0000; time: 0.44s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6923 score: 0.5102 time: 0.54s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6929 score: 0.5000 time: 0.55s
Epoch 11/1000, LR 0.000270
Train loss: 0.6938;  Loss pred: 0.6938; Loss self: 0.0000; time: 0.43s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6921 score: 0.5102 time: 0.51s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6926 score: 0.5000 time: 0.41s
Epoch 12/1000, LR 0.000270
Train loss: 0.6936;  Loss pred: 0.6936; Loss self: 0.0000; time: 0.44s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6919 score: 0.5102 time: 0.65s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6924 score: 0.5000 time: 0.41s
Epoch 13/1000, LR 0.000270
Train loss: 0.6933;  Loss pred: 0.6933; Loss self: 0.0000; time: 0.43s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6917 score: 0.5102 time: 0.52s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6921 score: 0.5000 time: 0.40s
Epoch 14/1000, LR 0.000270
Train loss: 0.6930;  Loss pred: 0.6930; Loss self: 0.0000; time: 0.47s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6915 score: 0.5102 time: 0.62s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6918 score: 0.5000 time: 0.42s
Epoch 15/1000, LR 0.000270
Train loss: 0.6926;  Loss pred: 0.6926; Loss self: 0.0000; time: 0.44s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6912 score: 0.5102 time: 0.54s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6914 score: 0.5000 time: 0.46s
Epoch 16/1000, LR 0.000270
Train loss: 0.6924;  Loss pred: 0.6924; Loss self: 0.0000; time: 0.53s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6909 score: 0.5102 time: 0.50s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6910 score: 0.5000 time: 0.40s
Epoch 17/1000, LR 0.000270
Train loss: 0.6920;  Loss pred: 0.6920; Loss self: 0.0000; time: 0.43s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6905 score: 0.5102 time: 0.53s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6905 score: 0.5000 time: 0.56s
Epoch 18/1000, LR 0.000270
Train loss: 0.6916;  Loss pred: 0.6916; Loss self: 0.0000; time: 0.54s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6901 score: 0.5102 time: 0.62s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6900 score: 0.5000 time: 0.42s
Epoch 19/1000, LR 0.000270
Train loss: 0.6911;  Loss pred: 0.6911; Loss self: 0.0000; time: 0.45s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6897 score: 0.5102 time: 0.61s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6895 score: 0.5000 time: 0.41s
Epoch 20/1000, LR 0.000270
Train loss: 0.6907;  Loss pred: 0.6907; Loss self: 0.0000; time: 0.43s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6892 score: 0.5102 time: 0.63s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6889 score: 0.5000 time: 0.46s
Epoch 21/1000, LR 0.000270
Train loss: 0.6901;  Loss pred: 0.6901; Loss self: 0.0000; time: 0.60s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6886 score: 0.5102 time: 0.65s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6883 score: 0.5000 time: 0.51s
Epoch 22/1000, LR 0.000270
Train loss: 0.6897;  Loss pred: 0.6897; Loss self: 0.0000; time: 0.55s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6880 score: 0.5102 time: 0.78s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6876 score: 0.5000 time: 0.41s
Epoch 23/1000, LR 0.000270
Train loss: 0.6889;  Loss pred: 0.6889; Loss self: 0.0000; time: 0.45s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6873 score: 0.5102 time: 0.52s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6868 score: 0.5000 time: 0.42s
Epoch 24/1000, LR 0.000270
Train loss: 0.6882;  Loss pred: 0.6882; Loss self: 0.0000; time: 0.49s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6865 score: 0.5102 time: 0.67s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6859 score: 0.5000 time: 0.51s
Epoch 25/1000, LR 0.000270
Train loss: 0.6875;  Loss pred: 0.6875; Loss self: 0.0000; time: 0.55s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6856 score: 0.5102 time: 0.68s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6850 score: 0.5000 time: 0.59s
Epoch 26/1000, LR 0.000270
Train loss: 0.6868;  Loss pred: 0.6868; Loss self: 0.0000; time: 0.55s
Val loss: 0.6846 score: 0.5714 time: 0.72s
Test loss: 0.6839 score: 0.5625 time: 0.42s
Epoch 27/1000, LR 0.000270
Train loss: 0.6856;  Loss pred: 0.6856; Loss self: 0.0000; time: 0.48s
Val loss: 0.6835 score: 0.5918 time: 0.54s
Test loss: 0.6827 score: 0.5625 time: 0.42s
Epoch 28/1000, LR 0.000270
Train loss: 0.6844;  Loss pred: 0.6844; Loss self: 0.0000; time: 0.45s
Val loss: 0.6822 score: 0.5714 time: 0.83s
Test loss: 0.6814 score: 0.5625 time: 0.63s
Epoch 29/1000, LR 0.000270
Train loss: 0.6834;  Loss pred: 0.6834; Loss self: 0.0000; time: 0.55s
Val loss: 0.6808 score: 0.5918 time: 0.68s
Test loss: 0.6799 score: 0.5625 time: 0.52s
Epoch 30/1000, LR 0.000270
Train loss: 0.6821;  Loss pred: 0.6821; Loss self: 0.0000; time: 0.51s
Val loss: 0.6793 score: 0.5918 time: 0.66s
Test loss: 0.6782 score: 0.5625 time: 0.42s
Epoch 31/1000, LR 0.000270
Train loss: 0.6809;  Loss pred: 0.6809; Loss self: 0.0000; time: 0.58s
Val loss: 0.6776 score: 0.5918 time: 0.69s
Test loss: 0.6764 score: 0.5625 time: 0.55s
Epoch 32/1000, LR 0.000270
Train loss: 0.6786;  Loss pred: 0.6786; Loss self: 0.0000; time: 0.59s
Val loss: 0.6757 score: 0.5918 time: 0.84s
Test loss: 0.6745 score: 0.5625 time: 0.44s
Epoch 33/1000, LR 0.000270
Train loss: 0.6770;  Loss pred: 0.6770; Loss self: 0.0000; time: 0.49s
Val loss: 0.6736 score: 0.6122 time: 0.54s
Test loss: 0.6723 score: 0.5625 time: 0.43s
Epoch 34/1000, LR 0.000270
Train loss: 0.6749;  Loss pred: 0.6749; Loss self: 0.0000; time: 0.47s
Val loss: 0.6713 score: 0.6122 time: 0.71s
Test loss: 0.6699 score: 0.5833 time: 0.46s
Epoch 35/1000, LR 0.000270
Train loss: 0.6723;  Loss pred: 0.6723; Loss self: 0.0000; time: 0.48s
Val loss: 0.6687 score: 0.6122 time: 0.64s
Test loss: 0.6671 score: 0.6042 time: 0.51s
Epoch 36/1000, LR 0.000270
Train loss: 0.6695;  Loss pred: 0.6695; Loss self: 0.0000; time: 0.55s
Val loss: 0.6658 score: 0.6122 time: 0.85s
Test loss: 0.6641 score: 0.6042 time: 0.54s
Epoch 37/1000, LR 0.000270
Train loss: 0.6683;  Loss pred: 0.6683; Loss self: 0.0000; time: 0.56s
Val loss: 0.6627 score: 0.6327 time: 0.66s
Test loss: 0.6609 score: 0.6042 time: 0.51s
Epoch 38/1000, LR 0.000270
Train loss: 0.6645;  Loss pred: 0.6645; Loss self: 0.0000; time: 0.46s
Val loss: 0.6592 score: 0.6327 time: 0.67s
Test loss: 0.6573 score: 0.6042 time: 0.40s
Epoch 39/1000, LR 0.000269
Train loss: 0.6597;  Loss pred: 0.6597; Loss self: 0.0000; time: 0.44s
Val loss: 0.6554 score: 0.6327 time: 0.51s
Test loss: 0.6534 score: 0.6042 time: 0.41s
Epoch 40/1000, LR 0.000269
Train loss: 0.6561;  Loss pred: 0.6561; Loss self: 0.0000; time: 0.60s
Val loss: 0.6512 score: 0.6327 time: 0.52s
Test loss: 0.6492 score: 0.6042 time: 0.41s
Epoch 41/1000, LR 0.000269
Train loss: 0.6514;  Loss pred: 0.6514; Loss self: 0.0000; time: 0.46s
Val loss: 0.6466 score: 0.6327 time: 0.53s
Test loss: 0.6446 score: 0.6042 time: 0.43s
Epoch 42/1000, LR 0.000269
Train loss: 0.6492;  Loss pred: 0.6492; Loss self: 0.0000; time: 0.45s
Val loss: 0.6417 score: 0.6531 time: 0.67s
Test loss: 0.6396 score: 0.6042 time: 0.42s
Epoch 43/1000, LR 0.000269
Train loss: 0.6439;  Loss pred: 0.6439; Loss self: 0.0000; time: 0.45s
Val loss: 0.6364 score: 0.6531 time: 0.53s
Test loss: 0.6342 score: 0.6250 time: 0.42s
Epoch 44/1000, LR 0.000269
Train loss: 0.6367;  Loss pred: 0.6367; Loss self: 0.0000; time: 0.46s
Val loss: 0.6306 score: 0.6531 time: 0.67s
Test loss: 0.6284 score: 0.6458 time: 0.44s
Epoch 45/1000, LR 0.000269
Train loss: 0.6330;  Loss pred: 0.6330; Loss self: 0.0000; time: 0.48s
Val loss: 0.6245 score: 0.6735 time: 0.52s
Test loss: 0.6221 score: 0.6667 time: 0.42s
Epoch 46/1000, LR 0.000269
Train loss: 0.6266;  Loss pred: 0.6266; Loss self: 0.0000; time: 0.46s
Val loss: 0.6179 score: 0.7347 time: 0.67s
Test loss: 0.6155 score: 0.6875 time: 0.44s
Epoch 47/1000, LR 0.000269
Train loss: 0.6197;  Loss pred: 0.6197; Loss self: 0.0000; time: 0.46s
Val loss: 0.6109 score: 0.7347 time: 0.53s
Test loss: 0.6085 score: 0.7083 time: 0.42s
Epoch 48/1000, LR 0.000269
Train loss: 0.6110;  Loss pred: 0.6110; Loss self: 0.0000; time: 0.46s
Val loss: 0.6035 score: 0.7755 time: 0.71s
Test loss: 0.6011 score: 0.7500 time: 0.42s
Epoch 49/1000, LR 0.000269
Train loss: 0.6020;  Loss pred: 0.6020; Loss self: 0.0000; time: 0.44s
Val loss: 0.5957 score: 0.7755 time: 0.53s
Test loss: 0.5932 score: 0.7500 time: 0.43s
Epoch 50/1000, LR 0.000269
Train loss: 0.5979;  Loss pred: 0.5979; Loss self: 0.0000; time: 0.49s
Val loss: 0.5873 score: 0.7959 time: 0.66s
Test loss: 0.5850 score: 0.7917 time: 0.42s
Epoch 51/1000, LR 0.000269
Train loss: 0.5861;  Loss pred: 0.5861; Loss self: 0.0000; time: 0.46s
Val loss: 0.5785 score: 0.7755 time: 0.57s
Test loss: 0.5763 score: 0.8125 time: 0.59s
Epoch 52/1000, LR 0.000269
Train loss: 0.5789;  Loss pred: 0.5789; Loss self: 0.0000; time: 0.56s
Val loss: 0.5691 score: 0.8163 time: 0.81s
Test loss: 0.5671 score: 0.8333 time: 0.52s
Epoch 53/1000, LR 0.000269
Train loss: 0.5687;  Loss pred: 0.5687; Loss self: 0.0000; time: 0.56s
Val loss: 0.5594 score: 0.8367 time: 0.68s
Test loss: 0.5576 score: 0.8542 time: 0.42s
Epoch 54/1000, LR 0.000269
Train loss: 0.5552;  Loss pred: 0.5552; Loss self: 0.0000; time: 0.45s
Val loss: 0.5491 score: 0.8776 time: 0.68s
Test loss: 0.5476 score: 0.8542 time: 0.45s
Epoch 55/1000, LR 0.000269
Train loss: 0.5409;  Loss pred: 0.5409; Loss self: 0.0000; time: 0.51s
Val loss: 0.5382 score: 0.8980 time: 0.71s
Test loss: 0.5373 score: 0.8542 time: 0.54s
Epoch 56/1000, LR 0.000269
Train loss: 0.5329;  Loss pred: 0.5329; Loss self: 0.0000; time: 0.63s
Val loss: 0.5269 score: 0.8980 time: 0.84s
Test loss: 0.5266 score: 0.8958 time: 0.53s
Epoch 57/1000, LR 0.000269
Train loss: 0.5242;  Loss pred: 0.5242; Loss self: 0.0000; time: 0.57s
Val loss: 0.5152 score: 0.8980 time: 0.58s
Test loss: 0.5158 score: 0.8958 time: 0.45s
Epoch 58/1000, LR 0.000269
Train loss: 0.5079;  Loss pred: 0.5079; Loss self: 0.0000; time: 0.45s
Val loss: 0.5032 score: 0.8980 time: 0.66s
Test loss: 0.5049 score: 0.8958 time: 0.43s
Epoch 59/1000, LR 0.000268
Train loss: 0.4944;  Loss pred: 0.4944; Loss self: 0.0000; time: 0.46s
Val loss: 0.4910 score: 0.9184 time: 0.64s
Test loss: 0.4940 score: 0.8958 time: 0.51s
Epoch 60/1000, LR 0.000268
Train loss: 0.4809;  Loss pred: 0.4809; Loss self: 0.0000; time: 0.48s
Val loss: 0.4784 score: 0.9388 time: 0.79s
Test loss: 0.4830 score: 0.8958 time: 0.49s
Epoch 61/1000, LR 0.000268
Train loss: 0.4694;  Loss pred: 0.4694; Loss self: 0.0000; time: 0.47s
Val loss: 0.4659 score: 0.9388 time: 0.52s
Test loss: 0.4721 score: 0.8958 time: 0.43s
Epoch 62/1000, LR 0.000268
Train loss: 0.4559;  Loss pred: 0.4559; Loss self: 0.0000; time: 0.46s
Val loss: 0.4532 score: 0.9388 time: 0.70s
Test loss: 0.4613 score: 0.8958 time: 0.45s
Epoch 63/1000, LR 0.000268
Train loss: 0.4509;  Loss pred: 0.4509; Loss self: 0.0000; time: 0.45s
Val loss: 0.4409 score: 0.9388 time: 0.52s
Test loss: 0.4509 score: 0.8958 time: 0.42s
Epoch 64/1000, LR 0.000268
Train loss: 0.4310;  Loss pred: 0.4310; Loss self: 0.0000; time: 0.60s
Val loss: 0.4289 score: 0.9388 time: 0.56s
Test loss: 0.4411 score: 0.8958 time: 0.42s
Epoch 65/1000, LR 0.000268
Train loss: 0.4237;  Loss pred: 0.4237; Loss self: 0.0000; time: 0.45s
Val loss: 0.4172 score: 0.9388 time: 0.54s
Test loss: 0.4317 score: 0.8958 time: 0.42s
Epoch 66/1000, LR 0.000268
Train loss: 0.4077;  Loss pred: 0.4077; Loss self: 0.0000; time: 0.50s
Val loss: 0.4058 score: 0.9388 time: 0.82s
Test loss: 0.4226 score: 0.8958 time: 0.53s
Epoch 67/1000, LR 0.000268
Train loss: 0.3942;  Loss pred: 0.3942; Loss self: 0.0000; time: 0.47s
Val loss: 0.3945 score: 0.9388 time: 0.53s
Test loss: 0.4137 score: 0.8958 time: 0.50s
Epoch 68/1000, LR 0.000268
Train loss: 0.3851;  Loss pred: 0.3851; Loss self: 0.0000; time: 0.44s
Val loss: 0.3833 score: 0.9388 time: 0.67s
Test loss: 0.4049 score: 0.8958 time: 0.51s
Epoch 69/1000, LR 0.000268
Train loss: 0.3786;  Loss pred: 0.3786; Loss self: 0.0000; time: 0.50s
Val loss: 0.3730 score: 0.9388 time: 0.68s
Test loss: 0.3967 score: 0.8958 time: 0.43s
Epoch 70/1000, LR 0.000268
Train loss: 0.3640;  Loss pred: 0.3640; Loss self: 0.0000; time: 0.45s
Val loss: 0.3631 score: 0.9388 time: 0.67s
Test loss: 0.3887 score: 0.8958 time: 0.44s
Epoch 71/1000, LR 0.000268
Train loss: 0.3496;  Loss pred: 0.3496; Loss self: 0.0000; time: 0.56s
Val loss: 0.3535 score: 0.9592 time: 0.52s
Test loss: 0.3809 score: 0.9167 time: 0.41s
Epoch 72/1000, LR 0.000267
Train loss: 0.3410;  Loss pred: 0.3410; Loss self: 0.0000; time: 0.45s
Val loss: 0.3444 score: 0.9592 time: 0.70s
Test loss: 0.3735 score: 0.9167 time: 0.45s
Epoch 73/1000, LR 0.000267
Train loss: 0.3284;  Loss pred: 0.3284; Loss self: 0.0000; time: 0.44s
Val loss: 0.3353 score: 0.9592 time: 0.53s
Test loss: 0.3661 score: 0.9167 time: 0.42s
Epoch 74/1000, LR 0.000267
Train loss: 0.3229;  Loss pred: 0.3229; Loss self: 0.0000; time: 0.45s
Val loss: 0.3267 score: 0.9592 time: 0.68s
Test loss: 0.3591 score: 0.9167 time: 0.41s
Epoch 75/1000, LR 0.000267
Train loss: 0.3060;  Loss pred: 0.3060; Loss self: 0.0000; time: 0.44s
Val loss: 0.3183 score: 0.9592 time: 0.53s
Test loss: 0.3521 score: 0.9167 time: 0.42s
Epoch 76/1000, LR 0.000267
Train loss: 0.2933;  Loss pred: 0.2933; Loss self: 0.0000; time: 0.45s
Val loss: 0.3095 score: 0.9592 time: 0.77s
Test loss: 0.3450 score: 0.9167 time: 0.42s
Epoch 77/1000, LR 0.000267
Train loss: 0.2931;  Loss pred: 0.2931; Loss self: 0.0000; time: 0.46s
Val loss: 0.3009 score: 0.9592 time: 0.52s
Test loss: 0.3382 score: 0.9167 time: 0.42s
Epoch 78/1000, LR 0.000267
Train loss: 0.2770;  Loss pred: 0.2770; Loss self: 0.0000; time: 0.46s
Val loss: 0.2920 score: 0.9592 time: 0.66s
Test loss: 0.3314 score: 0.9167 time: 0.42s
Epoch 79/1000, LR 0.000267
Train loss: 0.2693;  Loss pred: 0.2693; Loss self: 0.0000; time: 0.45s
Val loss: 0.2835 score: 0.9592 time: 0.53s
Test loss: 0.3248 score: 0.9167 time: 0.43s
Epoch 80/1000, LR 0.000267
Train loss: 0.2547;  Loss pred: 0.2547; Loss self: 0.0000; time: 0.46s
Val loss: 0.2751 score: 0.9592 time: 0.67s
Test loss: 0.3184 score: 0.9167 time: 0.42s
Epoch 81/1000, LR 0.000267
Train loss: 0.2552;  Loss pred: 0.2552; Loss self: 0.0000; time: 0.46s
Val loss: 0.2668 score: 0.9592 time: 0.53s
Test loss: 0.3121 score: 0.9167 time: 0.42s
Epoch 82/1000, LR 0.000267
Train loss: 0.2429;  Loss pred: 0.2429; Loss self: 0.0000; time: 0.45s
Val loss: 0.2589 score: 0.9592 time: 0.67s
Test loss: 0.3058 score: 0.9167 time: 0.41s
Epoch 83/1000, LR 0.000266
Train loss: 0.2324;  Loss pred: 0.2324; Loss self: 0.0000; time: 0.45s
Val loss: 0.2510 score: 0.9592 time: 0.53s
Test loss: 0.2997 score: 0.9167 time: 0.41s
Epoch 84/1000, LR 0.000266
Train loss: 0.2227;  Loss pred: 0.2227; Loss self: 0.0000; time: 0.44s
Val loss: 0.2434 score: 0.9592 time: 0.67s
Test loss: 0.2934 score: 0.9167 time: 0.55s
Epoch 85/1000, LR 0.000266
Train loss: 0.2123;  Loss pred: 0.2123; Loss self: 0.0000; time: 0.44s
Val loss: 0.2360 score: 0.9592 time: 0.51s
Test loss: 0.2873 score: 0.9167 time: 0.46s
Epoch 86/1000, LR 0.000266
Train loss: 0.2006;  Loss pred: 0.2006; Loss self: 0.0000; time: 0.59s
Val loss: 0.2288 score: 0.9592 time: 3.87s
Test loss: 0.2812 score: 0.9167 time: 1.67s
Epoch 87/1000, LR 0.000266
Train loss: 0.1981;  Loss pred: 0.1981; Loss self: 0.0000; time: 0.47s
Val loss: 0.2218 score: 0.9592 time: 0.52s
Test loss: 0.2753 score: 0.9167 time: 0.41s
Epoch 88/1000, LR 0.000266
Train loss: 0.1916;  Loss pred: 0.1916; Loss self: 0.0000; time: 0.60s
Val loss: 0.2148 score: 0.9592 time: 0.52s
Test loss: 0.2699 score: 0.9167 time: 0.44s
Epoch 89/1000, LR 0.000266
Train loss: 0.1774;  Loss pred: 0.1774; Loss self: 0.0000; time: 0.44s
Val loss: 0.2082 score: 0.9592 time: 0.52s
Test loss: 0.2646 score: 0.9167 time: 0.40s
Epoch 90/1000, LR 0.000266
Train loss: 0.1788;  Loss pred: 0.1788; Loss self: 0.0000; time: 0.47s
Val loss: 0.2017 score: 0.9592 time: 0.61s
Test loss: 0.2598 score: 0.9167 time: 0.40s
Epoch 91/1000, LR 0.000266
Train loss: 0.1659;  Loss pred: 0.1659; Loss self: 0.0000; time: 0.44s
Val loss: 0.1959 score: 0.9592 time: 0.50s
Test loss: 0.2547 score: 0.9375 time: 0.43s
Epoch 92/1000, LR 0.000266
Train loss: 0.1575;  Loss pred: 0.1575; Loss self: 0.0000; time: 0.44s
Val loss: 0.1906 score: 0.9592 time: 0.62s
Test loss: 0.2499 score: 0.9375 time: 0.40s
Epoch 93/1000, LR 0.000265
Train loss: 0.1455;  Loss pred: 0.1455; Loss self: 0.0000; time: 0.44s
Val loss: 0.1860 score: 0.9592 time: 0.50s
Test loss: 0.2448 score: 0.9375 time: 0.53s
Epoch 94/1000, LR 0.000265
Train loss: 0.1485;  Loss pred: 0.1485; Loss self: 0.0000; time: 0.42s
Val loss: 0.1817 score: 0.9592 time: 0.61s
Test loss: 0.2405 score: 0.9375 time: 0.41s
Epoch 95/1000, LR 0.000265
Train loss: 0.1347;  Loss pred: 0.1347; Loss self: 0.0000; time: 0.43s
Val loss: 0.1779 score: 0.9592 time: 0.56s
Test loss: 0.2364 score: 0.9375 time: 0.41s
Epoch 96/1000, LR 0.000265
Train loss: 0.1332;  Loss pred: 0.1332; Loss self: 0.0000; time: 0.46s
Val loss: 0.1745 score: 0.9592 time: 0.67s
Test loss: 0.2326 score: 0.9375 time: 0.42s
Epoch 97/1000, LR 0.000265
Train loss: 0.1291;  Loss pred: 0.1291; Loss self: 0.0000; time: 0.47s
Val loss: 0.1715 score: 0.9592 time: 0.53s
Test loss: 0.2293 score: 0.9375 time: 0.41s
Epoch 98/1000, LR 0.000265
Train loss: 0.1198;  Loss pred: 0.1198; Loss self: 0.0000; time: 0.44s
Val loss: 0.1686 score: 0.9388 time: 0.67s
Test loss: 0.2266 score: 0.9375 time: 0.45s
Epoch 99/1000, LR 0.000265
Train loss: 0.1233;  Loss pred: 0.1233; Loss self: 0.0000; time: 0.46s
Val loss: 0.1662 score: 0.9388 time: 0.52s
Test loss: 0.2242 score: 0.9375 time: 0.42s
Epoch 100/1000, LR 0.000265
Train loss: 0.1115;  Loss pred: 0.1115; Loss self: 0.0000; time: 0.45s
Val loss: 0.1638 score: 0.9388 time: 0.67s
Test loss: 0.2223 score: 0.9375 time: 0.41s
Epoch 101/1000, LR 0.000265
Train loss: 0.0993;  Loss pred: 0.0993; Loss self: 0.0000; time: 0.44s
Val loss: 0.1622 score: 0.9388 time: 0.53s
Test loss: 0.2205 score: 0.9375 time: 0.41s
Epoch 102/1000, LR 0.000264
Train loss: 0.1252;  Loss pred: 0.1252; Loss self: 0.0000; time: 0.45s
Val loss: 0.1628 score: 0.9388 time: 0.69s
Test loss: 0.2183 score: 0.9375 time: 0.41s
     INFO: Early stopping counter 1 of 20
Epoch 103/1000, LR 0.000264
Train loss: 0.1068;  Loss pred: 0.1068; Loss self: 0.0000; time: 0.44s
Val loss: 0.1635 score: 0.9388 time: 0.53s
Test loss: 0.2167 score: 0.9375 time: 0.45s
     INFO: Early stopping counter 2 of 20
Epoch 104/1000, LR 0.000264
Train loss: 0.1053;  Loss pred: 0.1053; Loss self: 0.0000; time: 0.45s
Val loss: 0.1637 score: 0.9388 time: 0.69s
Test loss: 0.2157 score: 0.9375 time: 0.43s
     INFO: Early stopping counter 3 of 20
Epoch 105/1000, LR 0.000264
Train loss: 0.1219;  Loss pred: 0.1219; Loss self: 0.0000; time: 0.45s
Val loss: 0.1620 score: 0.9388 time: 0.54s
Test loss: 0.2154 score: 0.9375 time: 0.41s
Epoch 106/1000, LR 0.000264
Train loss: 0.1068;  Loss pred: 0.1068; Loss self: 0.0000; time: 0.45s
Val loss: 0.1599 score: 0.9388 time: 0.65s
Test loss: 0.2157 score: 0.9375 time: 0.42s
Epoch 107/1000, LR 0.000264
Train loss: 0.1022;  Loss pred: 0.1022; Loss self: 0.0000; time: 0.44s
Val loss: 0.1580 score: 0.9388 time: 0.55s
Test loss: 0.2162 score: 0.9375 time: 0.41s
Epoch 108/1000, LR 0.000264
Train loss: 0.1053;  Loss pred: 0.1053; Loss self: 0.0000; time: 0.44s
Val loss: 0.1575 score: 0.9388 time: 0.67s
Test loss: 0.2164 score: 0.9375 time: 0.42s
Epoch 109/1000, LR 0.000264
Train loss: 0.1110;  Loss pred: 0.1110; Loss self: 0.0000; time: 0.48s
Val loss: 0.1545 score: 0.9388 time: 0.52s
Test loss: 0.2180 score: 0.9375 time: 0.41s
Epoch 110/1000, LR 0.000263
Train loss: 0.0958;  Loss pred: 0.0958; Loss self: 0.0000; time: 0.51s
Val loss: 0.1517 score: 0.9592 time: 0.65s
Test loss: 0.2201 score: 0.9375 time: 0.44s
Epoch 111/1000, LR 0.000263
Train loss: 0.1068;  Loss pred: 0.1068; Loss self: 0.0000; time: 0.45s
Val loss: 0.1496 score: 0.9592 time: 0.55s
Test loss: 0.2221 score: 0.9375 time: 0.43s
Epoch 112/1000, LR 0.000263
Train loss: 0.0985;  Loss pred: 0.0985; Loss self: 0.0000; time: 0.61s
Val loss: 0.1481 score: 0.9592 time: 0.55s
Test loss: 0.2240 score: 0.9375 time: 0.43s
Epoch 113/1000, LR 0.000263
Train loss: 0.1064;  Loss pred: 0.1064; Loss self: 0.0000; time: 0.48s
Val loss: 0.1470 score: 0.9592 time: 0.53s
Test loss: 0.2257 score: 0.9375 time: 0.43s
Epoch 114/1000, LR 0.000263
Train loss: 0.1111;  Loss pred: 0.1111; Loss self: 0.0000; time: 0.46s
Val loss: 0.1450 score: 0.9592 time: 0.71s
Test loss: 0.2290 score: 0.9375 time: 0.42s
Epoch 115/1000, LR 0.000263
Train loss: 0.0985;  Loss pred: 0.0985; Loss self: 0.0000; time: 0.46s
Val loss: 0.1436 score: 0.9592 time: 0.53s
Test loss: 0.2321 score: 0.9375 time: 0.42s
Epoch 116/1000, LR 0.000263
Train loss: 0.0969;  Loss pred: 0.0969; Loss self: 0.0000; time: 0.47s
Val loss: 0.1428 score: 0.9592 time: 0.66s
Test loss: 0.2349 score: 0.9375 time: 0.42s
Epoch 117/1000, LR 0.000262
Train loss: 0.0980;  Loss pred: 0.0980; Loss self: 0.0000; time: 0.45s
Val loss: 0.1423 score: 0.9592 time: 0.50s
Test loss: 0.2373 score: 0.9375 time: 0.44s
Epoch 118/1000, LR 0.000262
Train loss: 0.0945;  Loss pred: 0.0945; Loss self: 0.0000; time: 0.55s
Val loss: 0.1419 score: 0.9592 time: 0.71s
Test loss: 0.2395 score: 0.9375 time: 0.51s
Epoch 119/1000, LR 0.000262
Train loss: 0.1031;  Loss pred: 0.1031; Loss self: 0.0000; time: 0.55s
Val loss: 0.1418 score: 0.9592 time: 0.65s
Test loss: 0.2412 score: 0.9375 time: 0.52s
Epoch 120/1000, LR 0.000262
Train loss: 0.0895;  Loss pred: 0.0895; Loss self: 0.0000; time: 0.51s
Val loss: 0.1417 score: 0.9592 time: 0.62s
Test loss: 0.2427 score: 0.9375 time: 0.41s
Epoch 121/1000, LR 0.000262
Train loss: 0.0943;  Loss pred: 0.0943; Loss self: 0.0000; time: 0.44s
Val loss: 0.1417 score: 0.9592 time: 0.51s
Test loss: 0.2439 score: 0.9375 time: 0.47s
Epoch 122/1000, LR 0.000262
Train loss: 0.0997;  Loss pred: 0.0997; Loss self: 0.0000; time: 0.55s
Val loss: 0.1417 score: 0.9592 time: 0.82s
Test loss: 0.2451 score: 0.9375 time: 0.52s
     INFO: Early stopping counter 1 of 20
Epoch 123/1000, LR 0.000262
Train loss: 0.0961;  Loss pred: 0.0961; Loss self: 0.0000; time: 0.52s
Val loss: 0.1417 score: 0.9592 time: 0.53s
Test loss: 0.2458 score: 0.9375 time: 0.43s
     INFO: Early stopping counter 2 of 20
Epoch 124/1000, LR 0.000261
Train loss: 0.0909;  Loss pred: 0.0909; Loss self: 0.0000; time: 0.46s
Val loss: 0.1418 score: 0.9592 time: 0.67s
Test loss: 0.2466 score: 0.9375 time: 0.44s
     INFO: Early stopping counter 3 of 20
Epoch 125/1000, LR 0.000261
Train loss: 0.1099;  Loss pred: 0.1099; Loss self: 0.0000; time: 0.44s
Val loss: 0.1422 score: 0.9592 time: 0.51s
Test loss: 0.2436 score: 0.9375 time: 0.41s
     INFO: Early stopping counter 4 of 20
Epoch 126/1000, LR 0.000261
Train loss: 0.0949;  Loss pred: 0.0949; Loss self: 0.0000; time: 0.56s
Val loss: 0.1432 score: 0.9592 time: 0.89s
Test loss: 0.2405 score: 0.9375 time: 0.57s
     INFO: Early stopping counter 5 of 20
Epoch 127/1000, LR 0.000261
Train loss: 0.0957;  Loss pred: 0.0957; Loss self: 0.0000; time: 0.57s
Val loss: 0.1446 score: 0.9592 time: 0.56s
Test loss: 0.2381 score: 0.9375 time: 0.54s
     INFO: Early stopping counter 6 of 20
Epoch 128/1000, LR 0.000261
Train loss: 0.0934;  Loss pred: 0.0934; Loss self: 0.0000; time: 0.49s
Val loss: 0.1464 score: 0.9592 time: 0.75s
Test loss: 0.2361 score: 0.9375 time: 0.52s
     INFO: Early stopping counter 7 of 20
Epoch 129/1000, LR 0.000261
Train loss: 0.0959;  Loss pred: 0.0959; Loss self: 0.0000; time: 0.54s
Val loss: 0.1481 score: 0.9592 time: 0.53s
Test loss: 0.2348 score: 0.9375 time: 0.43s
     INFO: Early stopping counter 8 of 20
Epoch 130/1000, LR 0.000260
Train loss: 0.0922;  Loss pred: 0.0922; Loss self: 0.0000; time: 0.56s
Val loss: 0.1497 score: 0.9592 time: 0.82s
Test loss: 0.2338 score: 0.9375 time: 0.55s
     INFO: Early stopping counter 9 of 20
Epoch 131/1000, LR 0.000260
Train loss: 0.0961;  Loss pred: 0.0961; Loss self: 0.0000; time: 0.60s
Val loss: 0.1508 score: 0.9592 time: 0.65s
Test loss: 0.2334 score: 0.9375 time: 0.52s
     INFO: Early stopping counter 10 of 20
Epoch 132/1000, LR 0.000260
Train loss: 0.0925;  Loss pred: 0.0925; Loss self: 0.0000; time: 0.56s
Val loss: 0.1513 score: 0.9592 time: 0.84s
Test loss: 0.2335 score: 0.9375 time: 0.51s
     INFO: Early stopping counter 11 of 20
Epoch 133/1000, LR 0.000260
Train loss: 0.0828;  Loss pred: 0.0828; Loss self: 0.0000; time: 0.55s
Val loss: 0.1515 score: 0.9592 time: 0.66s
Test loss: 0.2338 score: 0.9375 time: 0.57s
     INFO: Early stopping counter 12 of 20
Epoch 134/1000, LR 0.000260
Train loss: 0.0888;  Loss pred: 0.0888; Loss self: 0.0000; time: 0.59s
Val loss: 0.1513 score: 0.9592 time: 0.81s
Test loss: 0.2343 score: 0.9375 time: 0.44s
     INFO: Early stopping counter 13 of 20
Epoch 135/1000, LR 0.000260
Train loss: 0.0909;  Loss pred: 0.0909; Loss self: 0.0000; time: 0.50s
Val loss: 0.1509 score: 0.9592 time: 0.52s
Test loss: 0.2350 score: 0.9375 time: 0.43s
     INFO: Early stopping counter 14 of 20
Epoch 136/1000, LR 0.000260
Train loss: 0.0809;  Loss pred: 0.0809; Loss self: 0.0000; time: 0.71s
Val loss: 0.1503 score: 0.9592 time: 0.65s
Test loss: 0.2358 score: 0.9375 time: 0.54s
     INFO: Early stopping counter 15 of 20
Epoch 137/1000, LR 0.000259
Train loss: 0.0897;  Loss pred: 0.0897; Loss self: 0.0000; time: 0.55s
Val loss: 0.1496 score: 0.9592 time: 0.66s
Test loss: 0.2367 score: 0.9375 time: 0.42s
     INFO: Early stopping counter 16 of 20
Epoch 138/1000, LR 0.000259
Train loss: 0.0824;  Loss pred: 0.0824; Loss self: 0.0000; time: 0.46s
Val loss: 0.1488 score: 0.9592 time: 0.94s
Test loss: 0.2377 score: 0.9375 time: 0.52s
     INFO: Early stopping counter 17 of 20
Epoch 139/1000, LR 0.000259
Train loss: 0.0875;  Loss pred: 0.0875; Loss self: 0.0000; time: 0.56s
Val loss: 0.1478 score: 0.9592 time: 0.66s
Test loss: 0.2389 score: 0.9375 time: 0.51s
     INFO: Early stopping counter 18 of 20
Epoch 140/1000, LR 0.000259
Train loss: 0.0976;  Loss pred: 0.0976; Loss self: 0.0000; time: 0.45s
Val loss: 0.1481 score: 0.9592 time: 0.67s
Test loss: 0.2389 score: 0.9375 time: 0.42s
     INFO: Early stopping counter 19 of 20
Epoch 141/1000, LR 0.000259
Train loss: 0.0827;  Loss pred: 0.0827; Loss self: 0.0000; time: 0.49s
Val loss: 0.1481 score: 0.9592 time: 0.53s
Test loss: 0.2391 score: 0.9375 time: 0.41s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 120,   Train_Loss: 0.0943,   Val_Loss: 0.1417,   Val_Precision: 1.0000,   Val_Recall: 0.9200,   Val_accuracy: 0.9583,   Val_Score: 0.9592,   Val_Loss: 0.1417,   Test_Precision: 0.9565,   Test_Recall: 0.9167,   Test_accuracy: 0.9362,   Test_Score: 0.9375,   Test_loss: 0.2439


[0.6227087839506567, 0.4557098669465631, 0.7132796109654009, 0.46749154408462346, 0.594128652010113, 0.4530330009292811, 0.5804642019793391, 0.4509469398763031, 0.4495153359603137, 0.4430166510865092, 0.5019226549193263, 0.5697651370428503, 0.7001396778505296, 0.580072927987203, 0.6830095530021936, 0.581694952910766, 0.7145566779654473, 0.5911042669322342, 0.7009357118513435, 0.469384070020169, 0.46560450620017946, 0.5480443921405822, 0.5570869690272957, 0.5561958570033312, 0.7029278031550348, 0.4620986790396273, 0.45321387983858585, 0.5818643730599433, 0.5562601159326732, 0.5559133400674909, 0.4538283220026642, 0.5003790690097958, 0.5769332370255142, 0.5302128691691905, 0.5397878438234329, 0.6027016357984394, 0.6666738078929484, 0.4413969258312136, 0.55328563391231, 0.5266274919267744, 0.4431350198574364, 1.5140820879023522, 0.4329633731395006, 0.44185982597991824, 0.42989791999571025, 0.42984618502669036, 0.47912067314609885, 0.4370650190394372, 0.4474087511189282, 0.5384855889715254, 0.5509043079800904, 0.5833360799588263, 0.4381748731248081, 0.4823975651524961, 0.5490636611357331, 0.42604362103156745, 0.6519483160227537, 0.5290174090769142, 0.45138902706094086, 0.4965191911906004, 0.45546926092356443, 0.5475415450055152, 0.48401307198219, 0.4461718969978392, 0.4887299248948693, 0.45831945119425654, 0.49927506293170154, 0.4562228568829596, 0.5492180048022419, 0.5551030188798904, 0.48647005786187947, 0.43406992591917515, 0.5619298999663442, 0.6450574991758913, 0.44510894804261625, 0.6258882849942893, 0.45456719398498535, 0.4534809950273484, 0.557323485147208, 0.5562262239400297, 0.5869244011119008, 0.44932498782873154, 0.44583860388956964, 0.44897839589975774, 0.5881211729720235, 0.5557046618778259, 0.5570933748967946, 0.4364910169970244, 0.5264528181869537, 0.563808973878622, 0.4453510718885809, 0.4492754810489714, 0.5629205820150673, 0.5639159770216793, 0.43987377313897014, 0.4735481219831854, 0.456806463887915, 0.5943967460189015, 0.4473464419133961, 0.7159024169668555, 0.5613610420841724, 0.5945262049790472, 0.5648923490662128, 0.46442935382947326, 0.6181377992033958, 0.45040287892334163, 0.6706126190256327, 0.4311196890193969, 0.4454271518625319, 0.47128826519474387, 0.4702799858059734, 0.4838031630497426, 0.49008012004196644, 0.47754836012609303, 0.4861374159809202, 0.4897858831100166, 0.477929265005514, 0.4722796392161399, 0.6133575800340623, 0.638564552878961, 0.49466609419323504, 0.6175617049448192, 0.47465863707475364, 0.48376587219536304, 0.5068948101252317, 0.46861471701413393, 0.5163122890517116, 1.1293568490073085, 0.5935565619729459, 0.4867815428879112, 0.6026110390666872, 0.472439874894917, 0.7044080849736929, 0.4724572179839015, 0.573514404008165, 0.5565581559203565, 0.6128846530336887, 0.5874184619169682, 0.6623768249992281, 0.4719974279869348, 0.5888507629279047, 0.4647821611724794, 0.6741535090841353, 0.5224992469884455, 0.7640027590095997, 0.7171591138467193, 0.6490168969612569, 0.5016352899838239, 0.6387041271664202, 0.5056630140170455, 0.8092530120629817, 0.6146644770633429, 0.6275771921500564, 0.4788820478133857, 0.6447817890439183, 0.5241320501081645, 0.48838517093099654, 0.4914611750282347, 0.6110377840232104, 0.6273848321288824, 0.6343714199028909, 0.4899973841384053, 0.4965809527784586, 0.4784010660368949, 0.5759847369045019, 0.6048508821986616, 0.6357171258423477, 0.4929681411013007, 0.4986348270904273, 0.6241722339764237, 0.6209014360792935, 0.48372736806049943, 0.6490627839230001, 0.6326934339012951, 0.6139711651485413, 0.6664100920315832, 0.6055992799811065, 0.6376063989009708, 0.5085851410403848, 0.5259205270558596, 0.4896927459631115, 0.6420552409254014, 0.48120128409937024, 0.6297723259776831, 0.4745284749660641, 0.6151331141591072, 0.495209455024451, 0.6239615499507636, 0.4716190667822957, 0.624315895838663, 0.4913617230486125, 0.6356583461165428, 0.47924294299446046, 0.6579037820920348, 0.6232137538027018, 0.6504862490110099, 0.4978836739901453, 0.6530008350964636, 0.5101751899346709, 0.7648375248536468, 0.6126242259051651, 0.7683067838661373, 0.655318435980007, 0.6440257809590548, 0.7323015369474888, 0.7662546532228589, 0.6021760110743344, 0.7277417909353971, 0.6038949550129473, 0.6180769610218704, 0.6193905789405107, 0.5503127900883555, 0.47143502393737435, 0.6049219300039113, 0.6071856829803437, 0.5971738710068166, 0.40128984302282333, 0.39785500592552125, 0.40339123201556504, 0.5094888370949775, 0.39706824510358274, 0.39268289506435394, 0.4096765648573637, 0.5547728349920362, 0.41168832196854055, 0.5555723879951984, 0.415700068930164, 0.4155371249653399, 0.40285252896137536, 0.4205881110392511, 0.4617943479679525, 0.40366311301477253, 0.5645897171925753, 0.4252380470279604, 0.41509460913948715, 0.4684851311612874, 0.5203581999521703, 0.41604701103642583, 0.421517833834514, 0.5140800399240106, 0.5962184208910912, 0.4278120258823037, 0.42565708607435226, 0.633595764869824, 0.5248204600065947, 0.4268172439187765, 0.5538967589382082, 0.4463070039637387, 0.4311553060542792, 0.46999787096865475, 0.5166953159496188, 0.5429646170232445, 0.5219793450087309, 0.40835346397943795, 0.4109480679035187, 0.4165934259071946, 0.43291023396886885, 0.4255446600727737, 0.42459080391563475, 0.44751180009916425, 0.42900944291613996, 0.4490349539555609, 0.424928184133023, 0.42886594985611737, 0.4342029031831771, 0.42083383607678115, 0.5927153038792312, 0.5233628009445965, 0.4218659568578005, 0.45589906396344304, 0.5435749117750674, 0.5368059549946338, 0.45252687693573534, 0.43468776997178793, 0.5217324169352651, 0.49679686804302037, 0.43087801709771156, 0.4545824700035155, 0.42731292312964797, 0.4227271999698132, 0.42736425111070275, 0.5337885390035808, 0.5030300819780678, 0.5124543639831245, 0.43607235490344465, 0.4522274921182543, 0.41749377991072834, 0.4551775110885501, 0.4284748639911413, 0.4163301531225443, 0.4214809611439705, 0.4208006230182946, 0.43006464885547757, 0.42581490613520145, 0.43432944687083364, 0.4204307869076729, 0.42723516398109496, 0.419254703912884, 0.4154567779041827, 0.5588069420773536, 0.46373709407635033, 1.67520154081285, 0.41827569412998855, 0.4435402969829738, 0.40589670184999704, 0.40584160992875695, 0.4317551071289927, 0.4011459150351584, 0.5368540689814836, 0.4110734388232231, 0.4175072661601007, 0.4209122678730637, 0.41962462989613414, 0.45442814682610333, 0.4237469721119851, 0.4152107199188322, 0.41818650788627565, 0.41373824002221227, 0.45152927399612963, 0.4333313370589167, 0.41315051913261414, 0.4286748240701854, 0.4203736581839621, 0.4212802241090685, 0.4168642829172313, 0.4407104570418596, 0.4385397240985185, 0.43465123185887933, 0.4306764679495245, 0.42945014615543187, 0.4222999659832567, 0.42908653197810054, 0.4451320879161358, 0.5184716559015214, 0.5222922139801085, 0.41012239502742887, 0.472003746079281, 0.5265340500045568, 0.4310701792128384, 0.4453129889443517, 0.413743152981624, 0.5768091599456966, 0.5531741068698466, 0.5275826358702034, 0.43495722487568855, 0.5524176748003811, 0.5245201208163053, 0.5165700791403651, 0.5740900749806315, 0.44863007101230323, 0.4357253620401025, 0.5485635611694306, 0.42771338508464396, 0.528681947151199, 0.5157892419956625, 0.42204954102635384, 0.4138075599912554]
[0.012708342529605237, 0.00930020136625639, 0.014556726754395937, 0.00954064375682905, 0.012125074530818633, 0.00924557144753635, 0.011846208203659982, 0.009202998772985777, 0.009173782366537014, 0.009041156144622637, 0.010243319488149516, 0.011627859939650004, 0.01428856485409244, 0.011838223020147, 0.013938970469432523, 0.011871325569607469, 0.014582789346233619, 0.012063352386372126, 0.014304810445945787, 0.00957926673510549, 0.0095021327795955, 0.011184579431440453, 0.011369121816883586, 0.01135093585721084, 0.014345465370510914, 0.009430585286523007, 0.00924926285384869, 0.011874783123672312, 0.011352247263932106, 0.011345170205458999, 0.00926180248985029, 0.010211817734893791, 0.01177414769439825, 0.010820670799371235, 0.011016078445376182, 0.01230003338364162, 0.01360558791618262, 0.009008100527167624, 0.011291543549230816, 0.010747499835240294, 0.009043571833825233, 0.03089963444698678, 0.008835987206928584, 0.009017547468977923, 0.008773426938687965, 0.00877237112299368, 0.009777972921348957, 0.008919694266110962, 0.0091307908391618, 0.010989501815745416, 0.01124294506081817, 0.011904817958343394, 0.008942344349485879, 0.009844848268418287, 0.011205380839504758, 0.008694767776154438, 0.01330506767393375, 0.010796273654630902, 0.009212020960427364, 0.010133044718175518, 0.009295291039256416, 0.011174317245010515, 0.009877817795554899, 0.009105548918323249, 0.009974080099895293, 0.009353458187637888, 0.010189286998606153, 0.00931067054863183, 0.011208530710249836, 0.01132863303836511, 0.009927960364528152, 0.00885856991671786, 0.01146795714217029, 0.013164438758691659, 0.009083856082502373, 0.012773230306005903, 0.00927688150989766, 0.0092547141842316, 0.011373948676473632, 0.011351555590612851, 0.01197804900228369, 0.00916989771079044, 0.009098747018154482, 0.009162824406117504, 0.012002472917796398, 0.011340911466894405, 0.011369252548914174, 0.008907979938714785, 0.01074393506503987, 0.011506305589359634, 0.009088797385481243, 0.009168887368346356, 0.01148817514316464, 0.011508489326973046, 0.00897701577834633, 0.009664247387411947, 0.009322580895671735, 0.012130545837120438, 0.00912951922272237, 0.014610253407486848, 0.011456347797636171, 0.01213318785671525, 0.011528415287065566, 0.009478150078152515, 0.012615057126599915, 0.009191895488231462, 0.013685971816849649, 0.008798361000395855, 0.009090350038010855, 0.009618127861117222, 0.009597550730734152, 0.009873533939790666, 0.010001635102897274, 0.009745884900532511, 0.009921171754712656, 0.00999563026755136, 0.009753658469500286, 0.009638359984002856, 0.01251750163334821, 0.01303192965059104, 0.010095226412106837, 0.012603300100914677, 0.009686910960709259, 0.009872772901946185, 0.010344792043372077, 0.009563565653349673, 0.010536985490851256, 0.023048098959332824, 0.012113399223937673, 0.009934317201794106, 0.01229818447074872, 0.009641630099896265, 0.014375675203544753, 0.009641984040487786, 0.011704375592003368, 0.011358329712660337, 0.012507850061912013, 0.011988131875856494, 0.01351789438773935, 0.009632600571161934, 0.012017362508732749, 0.009485350228009783, 0.013758234879268067, 0.010663249938539704, 0.015591893041012238, 0.014635900282586108, 0.01324524279512769, 0.01023745489762906, 0.013034778105437147, 0.010319653347286642, 0.016515367593122075, 0.012544173001292713, 0.012807697798980743, 0.00977310301659971, 0.013158812021304454, 0.010696572451187032, 0.009967044304714215, 0.010029819898535403, 0.012470158857616539, 0.012803772084262907, 0.012946355508222264, 0.009999946615069496, 0.010134305158744052, 0.009763287061977446, 0.011754790549071468, 0.012343895555074726, 0.012973818894741791, 0.01006057430818981, 0.010176220961029128, 0.01273820885666171, 0.012671457879169255, 0.009871987103275498, 0.013246179263734696, 0.012912110895944797, 0.01253002377854166, 0.013600205959828228, 0.012359168979206256, 0.013012375487774914, 0.010379288592660914, 0.010733071980731828, 0.009993729509451255, 0.01310316818215105, 0.009820434369374903, 0.012852496448524145, 0.009684254591144165, 0.012553737023655248, 0.010106315408662265, 0.012733909182668644, 0.009624878913924401, 0.012741140731401285, 0.010027790266298214, 0.012972619308500874, 0.009780468224376743, 0.01342660779779663, 0.012718648036789834, 0.013275229571653264, 0.010160891305921333, 0.013326547655029868, 0.010411738570095325, 0.015608929078645853, 0.012502535222554388, 0.015679730282982394, 0.013373845632245042, 0.01314338328487867, 0.014944929325458954, 0.015637850065772632, 0.012289306348455804, 0.01485187328439586, 0.012324386836998925, 0.01261381553105858, 0.012640624060010423, 0.011230873267109297, 0.009621122937497437, 0.012345345510283903, 0.01239154455061926, 0.012187221857281973, 0.008360205062975487, 0.008288645956781693, 0.008403984000324272, 0.010614350772812031, 0.008272255106324641, 0.00818089364717404, 0.00853492843452841, 0.01155776739566742, 0.00857684004101126, 0.011574424749899967, 0.00866041810271175, 0.008657023436777914, 0.008392761020028653, 0.008762252313317731, 0.009620715582665676, 0.008409648187807761, 0.011762285774845319, 0.008859125979749175, 0.00864780435707265, 0.009760106899193488, 0.01084079583233688, 0.008667646063258871, 0.008781621538219042, 0.010710000831750222, 0.012421217101897733, 0.008912750539214661, 0.00886785595988234, 0.013199911768121334, 0.010933759583470723, 0.00889202591497451, 0.011539515811212672, 0.00929806258257789, 0.00898240220946415, 0.009791622311846973, 0.010764485748950392, 0.011311762854650928, 0.010874569687681893, 0.008507363832904957, 0.008561418081323305, 0.008679029706399888, 0.009018963207684768, 0.008865513751516119, 0.008845641748242391, 0.009323162502065921, 0.008937696727419583, 0.009354894874074185, 0.008852670502771312, 0.008934707288669111, 0.00904589381631619, 0.00876737158493294, 0.012348235497483984, 0.010903391686345762, 0.008788874101204177, 0.009497897165905064, 0.011324477328647239, 0.011183457395721538, 0.009427643269494487, 0.009055995207745582, 0.010869425352818022, 0.010349934750896258, 0.008976625356202325, 0.00947046812507324, 0.008902352565201, 0.008806816666037776, 0.00890342189813964, 0.0111205945625746, 0.010479793374543078, 0.01067613258298176, 0.009084840727155097, 0.009421406085796965, 0.008697787081473507, 0.009482864814344794, 0.008926559666482111, 0.008673544856719673, 0.008780853357166052, 0.00876667964621447, 0.008959680184489116, 0.008871143877816698, 0.009048530143142367, 0.008758974727243185, 0.008900732582939478, 0.008734472998185083, 0.008655349539670473, 0.0116418112932782, 0.009661189459923966, 0.03490003210026771, 0.008714076961041428, 0.009240422853811955, 0.008456181288541606, 0.008455033540182436, 0.008994898065187348, 0.008357206563232467, 0.011184459770447575, 0.008564029975483814, 0.008698068045002097, 0.008769005580688827, 0.008742179789502794, 0.009467253058877153, 0.008828061918999689, 0.008650223331642337, 0.00871221891429741, 0.008619546667129422, 0.009406859874919368, 0.009027736188727431, 0.00860730248192946, 0.008930725501462197, 0.008757784545499211, 0.008776671335605593, 0.008684672560775653, 0.009181467855038742, 0.00913624425205247, 0.009055233997059986, 0.008972426415615095, 0.00894687804490483, 0.008797915957984515, 0.008939302749543762, 0.009273585164919496, 0.010801492831281697, 0.010881087791252261, 0.008544216563071435, 0.009833411376651688, 0.010969459375094933, 0.0089806287336008, 0.00927735393634066, 0.0086196490204505, 0.01201685749886868, 0.011524460559788471, 0.010991304913962571, 0.009061608851576844, 0.011508701558341272, 0.01092750251700636, 0.010761876648757607, 0.011960209895429822, 0.009346459812756317, 0.009077611709168801, 0.011428407524363138, 0.008910695522596749, 0.011014207232316645, 0.010745609208242968, 0.008792698771382371, 0.008620990833151154]
[78.68846764795717, 107.52455356808365, 68.69676245712405, 104.81473006307515, 82.47371984875414, 108.15989100018996, 84.41519706626816, 108.66023398105537, 109.0062920663647, 110.60532347898503, 97.6246031530012, 86.00034788775584, 69.98603500151987, 84.47213727078294, 71.7413098903503, 84.23659128346738, 68.57398651639137, 82.89569664977142, 69.90655372741526, 104.39212391228895, 105.2395312921074, 89.40881560454086, 87.95754114578676, 88.09846276813695, 69.70843915985034, 106.03795730781131, 108.11672408941132, 84.21206430343184, 88.08828567160961, 88.14323468843386, 107.9703439039936, 97.9257587591873, 84.93183761197143, 92.41571234734428, 90.776405138957, 81.30059234879361, 73.499212688236, 111.01119453364109, 88.56185123318417, 93.04489558781574, 110.57577894828552, 32.36284240564912, 113.17354547728041, 110.89489724785925, 113.98054682490427, 113.99426517408186, 102.27068616815532, 112.11146595005499, 109.519538626492, 90.99593564534754, 88.9446665967456, 83.99960448779129, 111.82749857507923, 101.57596874376856, 89.24283916120756, 115.01169735004522, 75.15933210615096, 92.62455102470145, 108.55381292506394, 98.68702130627256, 107.58135444890767, 89.49092620817738, 101.23693519129382, 109.82314289561205, 100.25987258819968, 106.91232910215626, 98.14229397373882, 107.40364990649859, 89.21775974486403, 88.2719032926072, 100.72562372155755, 112.88503781099065, 87.19948876707711, 75.96222051925778, 110.08540766362795, 78.2887316710954, 107.79484452109077, 108.05303979066404, 87.92021385400159, 88.09365306962405, 83.48605017472744, 109.05247054427618, 109.90524277735464, 109.13665434124832, 83.31616383131116, 88.17633423196452, 87.95652974526504, 112.25889672853, 93.07576730000359, 86.90886855332107, 110.02555757238403, 109.06448730652843, 87.04602667857053, 86.89237758219473, 111.39559344566638, 103.47417237089132, 107.26643310376396, 82.43652127671949, 109.53479319164036, 68.44508251222817, 87.28785278379324, 82.41857060232846, 86.74219093425279, 105.5058204137363, 79.27035049975441, 108.791489337571, 73.06751857904881, 113.65753234665048, 110.00676495608516, 103.9703375167901, 104.19324972127617, 101.28085912278756, 99.98365164415166, 102.60740919948279, 100.7945457173433, 100.04371642739554, 102.52563211301712, 103.75209077682689, 79.88814615656797, 76.73460698543956, 99.05671841106371, 79.34429808010566, 103.23208338097307, 101.28866630801095, 96.66699879585317, 104.56351074975332, 94.90380345196931, 43.38752631028043, 82.5532108298609, 100.66117073646522, 81.31281510522989, 103.71690156530263, 69.56195001911424, 103.71309429686737, 85.43813312717147, 88.04111390474691, 79.9497911351789, 83.41583245459208, 73.97601810730184, 103.81412502391083, 83.21293455808812, 105.4257329420529, 72.68374241138116, 93.78003945924075, 64.13589404247729, 68.32514438416929, 75.49880477599501, 97.68052802182257, 76.7178383790725, 96.90247979724349, 60.54966650675435, 79.71828831577396, 78.07804460217524, 102.32164731114473, 75.99470213427887, 93.48789105701115, 100.33064662178934, 99.7026875972144, 80.191440335118, 78.10198380749829, 77.2418152247478, 100.000533852155, 98.67474724077978, 102.4245209274284, 85.07169871087085, 81.01170295376389, 77.07830732902352, 99.39790407252895, 98.26830645969673, 78.50397267407257, 78.91751758445338, 101.29672876782858, 75.49346721720681, 77.4466706535228, 79.80830824220412, 73.52829824443556, 80.91158893307914, 76.84991882838739, 96.34571686416828, 93.16997051684876, 100.06274424920962, 76.31742080225955, 101.82848969680072, 77.80589584328034, 103.26039971258676, 79.65755520572723, 98.94802997568094, 78.53047996926503, 103.89741096413074, 78.48590805809417, 99.7228674956275, 77.08543480842812, 102.24459372073923, 74.47897600495226, 78.62470893977175, 75.3282641631532, 98.4165630644275, 75.03818887576392, 96.0454388349903, 64.06589426868962, 79.98377786578955, 63.776607247213, 74.77280862199783, 76.08391068915185, 66.91232713268721, 63.947409381341465, 81.37155764903311, 67.33157365748947, 81.1399393110503, 79.27815319145371, 79.11001824376504, 89.04027106499342, 103.93797132584103, 81.0021881661377, 80.70019002998505, 82.05315466563786, 119.61429085378073, 120.64696757638795, 118.99118322469612, 94.21207395570862, 120.88602045595033, 122.23603473264001, 117.16559871251621, 86.52190044721465, 116.59305702547461, 86.3973822982989, 115.46786634780139, 115.51314459328681, 119.15030079059561, 114.12590784222247, 103.94237220792294, 118.91103856755765, 85.01748887436383, 112.87795232688548, 115.63628855481046, 102.45789419403117, 92.24415028803622, 115.371577554243, 113.8741854961339, 93.37067435470783, 80.50740855718708, 112.19880951454455, 112.76682938062385, 75.75808214226602, 91.45984895367238, 112.46031102045731, 86.65874863036488, 107.54928686689371, 111.32879342080314, 102.12812219994339, 92.8980745873073, 88.40355060916447, 91.95766165651081, 117.54522548244374, 116.80307987545841, 115.22025316523612, 110.8774896817333, 112.79662160909591, 113.05002265083795, 107.25974150707013, 111.88564912167385, 106.89590994457495, 112.96026432780388, 111.9230846284341, 110.54739534928962, 114.05926968106813, 80.98323037358294, 91.71458100072593, 113.78021672457324, 105.28646315415325, 88.30429617006038, 89.41778598652066, 106.07104781273935, 110.42408670277356, 92.00118382898123, 96.61896659912803, 111.40043839626864, 105.59140126901241, 112.32985805448402, 113.54840664009238, 112.31636683519949, 89.9232495504704, 95.421728679226, 93.66687723549306, 110.07347624828954, 106.14126924297724, 114.97177277770156, 105.4533645241144, 112.02524123093576, 115.29311446694923, 113.88414762488891, 114.0682721800846, 111.61112667070273, 112.72503453591972, 110.5151869066682, 114.16861346678891, 112.35030270617891, 114.48887645628852, 115.53548420160881, 85.8972864967657, 103.50692367106008, 28.65326877428085, 114.75684739425219, 108.22015570288211, 118.2566889093345, 118.27274194094122, 111.17413368699157, 119.6572075171027, 89.40977217713059, 116.7674567770889, 114.96805897886705, 114.03801614657515, 114.38794718003327, 105.62725996452906, 113.27514568603188, 115.60395167394358, 114.78132147929897, 116.01538208656525, 106.30539981425754, 110.76974106184666, 116.18041797641511, 111.97298582699396, 114.18412896603157, 113.93841261243942, 115.14538895991343, 108.91504667755332, 109.45416654938366, 110.43336928948227, 111.4525718772859, 111.77083167792718, 113.66328171076172, 111.86554790876026, 107.83316076967101, 92.57979573933956, 91.90257621153832, 117.0382319570473, 101.6941081478993, 91.16219549255095, 111.3507783991253, 107.78935533362198, 116.01400447134861, 83.21643159154917, 86.77195733475223, 90.98100797200806, 110.35567925953747, 86.89077520436877, 91.51221868342837, 92.92059671725042, 83.6105727861946, 106.99238214614385, 110.16113401170865, 87.50125490959216, 112.22468520712967, 90.79182721983955, 93.06126629217997, 113.73072432035356, 115.99594749069915]
Elapsed: 0.5209050589855447~0.12348682197261088
Time per graph: 0.010708859715005422~0.002505067786902019
Speed: 96.69287197536984~15.762585421920397
Total Time: 0.4149
best val loss: 0.1416889727115631 test_score: 0.9375

Testing...
Test loss: 0.3809 score: 0.9167 time: 0.42s
test Score 0.9167
Epoch Time List: [1.7259658421389759, 1.4056893221568316, 1.6713143668603152, 1.4089211618993431, 1.566130296792835, 1.3976833377964795, 1.5425519261043519, 1.3914607306942344, 1.3805462021846324, 1.5168517308775336, 1.4328091552015394, 1.9106276859529316, 1.8901375560089946, 1.7713006306439638, 1.8837954432237893, 1.7917112549766898, 1.909385637845844, 1.746612090151757, 1.861304609104991, 1.5197515147738159, 1.539622592041269, 1.6670328222680837, 1.6998512798454612, 1.721070768777281, 1.9162404600065202, 1.4924935901071876, 1.465068151243031, 1.519483664771542, 1.5989199820905924, 1.679855754133314, 1.7558292739558965, 1.4268978098407388, 1.8321853750385344, 1.4673961570952088, 1.4623532930854708, 1.5891130201052874, 1.681268696906045, 1.5301661321427673, 1.803276140941307, 1.6815261179581285, 1.4635201448109, 5.460340854944661, 1.4879453028552234, 1.3535601641051471, 1.4416017311159521, 1.340865921229124, 1.497775553027168, 1.345064983703196, 1.5138882105238736, 1.4637773700524122, 1.5534620368853211, 1.518580679083243, 1.4034949950873852, 1.4168247098568827, 1.7984646640252322, 1.531472935108468, 1.6558873252943158, 1.460636424832046, 1.7493402201216668, 1.4563838699832559, 1.5554202641360462, 1.7542230961844325, 1.5812625428661704, 1.3945982051081955, 1.571562142809853, 1.3909015899989754, 1.5915916934609413, 1.428738476941362, 1.6757907888386399, 1.7170023971702904, 1.7820384909864515, 1.610651802038774, 1.9151470472570509, 1.8366654620040208, 1.3928228067234159, 1.787750987103209, 1.427228454966098, 1.42708878708072, 1.8080893768928945, 1.7196042120922357, 1.5332433399744332, 1.3811422321014106, 1.5239019168075174, 1.3892480537761003, 1.8344153619837016, 1.6783148089889437, 1.865312703885138, 1.4439739568624645, 1.8089915469754487, 1.771590385120362, 1.674103077268228, 1.4105792418122292, 1.7397592950146645, 1.7367665963247418, 1.6112890320364386, 1.4411988072097301, 1.5469975117594004, 1.5781767279841006, 1.4070728258229792, 1.938510209089145, 1.8181990499142557, 1.7692862029653043, 1.8972247869241983, 1.494204621994868, 1.594682231079787, 1.4001057259738445, 1.7931100081186742, 1.558000888908282, 1.5381759577430785, 1.4077624422498047, 1.563934648875147, 1.4272179640829563, 1.5458439090289176, 1.383167140884325, 1.5137511168140918, 1.3871108379680663, 1.5221678751986474, 1.3727104449644685, 1.8689556610770524, 1.6811258469242603, 1.583333780290559, 1.7572556077502668, 1.6448135029058903, 1.3912315459456295, 1.4864970110356808, 1.3496326999738812, 1.547614831943065, 5.770477800164372, 1.502276714425534, 1.4074702342040837, 1.4913860317319632, 1.4242889089509845, 1.5941078725736588, 1.3667625249363482, 1.4827726478688419, 1.4760952880606055, 1.47637375514023, 1.5148988731671125, 1.669751709094271, 1.3860951089300215, 1.476971887750551, 1.47719318815507, 1.5673923501744866, 1.4291048229206353, 1.9399623679928482, 1.8144840830937028, 1.682489317143336, 1.4370736628770828, 1.5761086540296674, 1.422530840151012, 1.7054048681166023, 1.7581660780124366, 1.7028204801026732, 1.384106561774388, 1.5547821868676692, 1.4332193688023835, 1.4043407759163529, 1.5965744608547539, 1.564194915117696, 1.9084489468950778, 1.7515611762646586, 1.6164230958092958, 1.542991860769689, 1.566185190808028, 1.6012222510762513, 1.8435022088233382, 1.7634059782139957, 1.6321536062750965, 1.455297561129555, 1.9279546868056059, 1.6810071968939155, 1.7982310520019382, 1.8044091330375522, 1.9967233932111412, 1.7839215623680502, 2.021853039972484, 1.725106640951708, 1.9331603650934994, 1.669701413018629, 1.5907036098651588, 1.4093908951617777, 1.6660247028339654, 1.3804213958792388, 1.514792773872614, 1.374052200699225, 1.5097465009894222, 1.4014294447842985, 1.5379891628399491, 1.3768980149179697, 1.5359509440604597, 1.389196892734617, 1.5412516039796174, 1.484690902987495, 1.5830440840218216, 1.5509305712766945, 1.8113931121770293, 1.4269965540152043, 1.6876306573394686, 1.447018755832687, 1.908394165802747, 1.7545633912086487, 1.908106692833826, 1.8000692271161824, 1.6122832261025906, 1.6621765848249197, 1.8825210079085082, 1.7127701845020056, 1.7343780873343349, 1.7317351570818573, 1.7715662932023406, 1.9618180259130895, 1.5164197809062898, 1.5445916769094765, 1.4899151809513569, 1.8568665059283376, 1.7377586748916656, 1.3472754198592156, 1.3229492648970336, 1.4318431890569627, 1.4370605337899178, 1.3311925209127367, 1.3249624441377819, 1.4365885048173368, 1.467289854073897, 1.3740719980560243, 1.5246795071288943, 1.353339499561116, 1.504559134831652, 1.3462329420726746, 1.5035949191078544, 1.434052862925455, 1.4295519411098212, 1.5193708217702806, 1.577227802714333, 1.473938853945583, 1.518531711306423, 1.7575312717817724, 1.7317434749566019, 1.3848107720259577, 1.6640590119641274, 1.815406114095822, 1.690871300874278, 1.4447801059577614, 1.90634014015086, 1.7490961221046746, 1.5922373307403177, 1.8142441702075303, 1.8654211566317827, 1.4584403021726757, 1.6406820251140743, 1.6242358940653503, 1.9334612761158496, 1.7325565689243376, 1.5354765090160072, 1.362308070063591, 1.5298019358888268, 1.4146595771890134, 1.546654922887683, 1.4058004401158541, 1.5670240179169923, 1.4243681938387454, 1.5722226849757135, 1.403508177259937, 1.592030507978052, 1.4052681331522763, 1.5642676199786365, 1.6149410381913185, 1.8899296121671796, 1.6490775940474123, 1.5758495330810547, 1.7551957762334496, 1.999989579198882, 1.6014757617376745, 1.5454663820564747, 1.6114923770073801, 1.7671314280014485, 1.4218901328276843, 1.6137501536868513, 1.396930726012215, 1.5803978720214218, 1.4037004967685789, 1.8528786320239305, 1.5000488909427077, 1.622998405015096, 1.6134910378605127, 1.5699590460862964, 1.4920803990680724, 1.5963881921488792, 1.393467356916517, 1.5354486892465502, 1.3815074698068202, 1.6317453358788043, 1.4008728240150958, 1.54497826914303, 1.406592802843079, 1.5424205742310733, 1.411531776888296, 1.5348364477977157, 1.384401086019352, 1.661954046227038, 1.4104746640659869, 6.132195575162768, 1.4041715520434082, 1.5494195180945098, 1.368372461758554, 1.4826240201946348, 1.3633370089810342, 1.4510093762073666, 1.4659951536450535, 1.4408401299733669, 1.4048480389174074, 1.5475171660073102, 1.4178013999480754, 1.561799886636436, 1.3993650020565838, 1.5374919448513538, 1.386307206004858, 1.5459435279481113, 1.4155344851315022, 1.562678448855877, 1.4041546368971467, 1.522652506828308, 1.4128861192148179, 1.5251518958248198, 1.4149361681193113, 1.5931544748600572, 1.440602907212451, 1.5827843437436968, 1.4325863420963287, 1.5973039122764021, 1.4092853190377355, 1.560958666028455, 1.383989600930363, 1.7794106227811426, 1.710819839965552, 1.5360388087574393, 1.4220886947587132, 1.895157913910225, 1.4835610350128263, 1.567112882854417, 1.3579083220101893, 2.019675675081089, 1.6740608331747353, 1.761649559950456, 1.4994428509380668, 1.92836138070561, 1.7750216338317841, 1.9010759780649096, 1.7762673329561949, 1.8407457650173455, 1.447789679048583, 1.90260921115987, 1.6328571976628155, 1.9179129840340465, 1.7256960389204323, 1.5408951069694012, 1.4247454097494483]
Total Epoch List: [111, 105, 141]
Total Time List: [0.47090889886021614, 0.5981951949652284, 0.4149263200815767]
T-times Epoch Time: 1.6508373222872892 ~ 0.02334253614471553
T-times Total Epoch: 115.66666666666667 ~ 2.3726840560069573
T-times Total Time: 0.5175568379151324 ~ 0.017169153861280054
T-times Inference Elapsed: 0.5265635797983704 ~ 0.005826459971479185
T-times Time Per Graph: 0.01081939653188206 ~ 0.00011537372745197902
T-times Speed: 97.5931453570902 ~ 1.3499073671067696
T-times cross validation test micro f1 score:0.9111797192051365 ~ 0.014804947535022964
T-times cross validation test precision:0.9411012900143335 ~ 0.016578166096427042
T-times cross validation test recall:0.8948148148148148 ~ 0.017125374846958395
T-times cross validation test f1_score:0.9111797192051365 ~ 0.015309394171313905
