Namespace(seed=15, model='AEtransGAT', dataset='ico_wallets/Volume', num_heads=1, num_layers=2, dim_hidden=64, dropout=0.1, epochs=1000, lr=0.0003, weight_decay=0.0001, batch_size=64, abs_pe=None, abs_pe_dim=20, num_class=2, outdir='./outdir/ico_wallets/Volume/seed15/edge_attr/khopgnn_gat_1_0.1_0.0003_0.0001_2_1_64_BN', warmup=10, layer_norm=False, use_edge_attr=True, edge_dim=128, gnn_type='gat', k_hop=1, global_pool='mean', se='khopgnn', ie='mamba', aggr='add', not_extract_node_feature=False, k_ford=3, early_stop=1, early_stop_mindelta=-0.0, patience=2, training_times=3, Lambda=0.01, order='sd', use_cuda=True, batch_norm=True, save_logs=True)
Data(edge_index=[2, 334], edge_attr=[334, 2], x=[97, 14887], y=[1, 1], num_nodes=97)
Data(edge_index=[2, 334], edge_attr=[334, 2], x=[97, 14887], y=[1, 1], num_nodes=97)
Data(edge_index=[2, 276], edge_attr=[276, 2], x=[88, 14887], y=[1, 1], num_nodes=97)
========================training times:0========================
========================k_idx:0========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: AEtransGAT
AEtransGAT(
  (gae): GAE(
    (encoder): transGAT(
      (encoder): EdgeFeatureGAT()
      (classifier): Sequential(
        (0): Linear(in_features=64, out_features=64, bias=True)
        (1): ReLU(inplace=True)
        (2): Linear(in_features=64, out_features=2, bias=True)
      )
    )
    (decoder): InnerProductDecoder()
  )
)
Total number of parameters: 1914242
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x75feb28ef7f0>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 2.5791;  Loss pred: 2.5791; Loss self: 0.0000; time: 0.40s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7009 score: 0.4898 time: 0.46s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6966 score: 0.5102 time: 1.12s
Epoch 2/1000, LR 0.000000
Train loss: 2.6163;  Loss pred: 2.6163; Loss self: 0.0000; time: 3.91s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7008 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6965 score: 0.5102 time: 0.07s
Epoch 3/1000, LR 0.000030
Train loss: 2.6338;  Loss pred: 2.6338; Loss self: 0.0000; time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7006 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6964 score: 0.5102 time: 0.06s
Epoch 4/1000, LR 0.000060
Train loss: 2.5906;  Loss pred: 2.5906; Loss self: 0.0000; time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7003 score: 0.4898 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6962 score: 0.5102 time: 0.07s
Epoch 5/1000, LR 0.000090
Train loss: 2.5365;  Loss pred: 2.5365; Loss self: 0.0000; time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6999 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6959 score: 0.5102 time: 0.06s
Epoch 6/1000, LR 0.000120
Train loss: 2.5485;  Loss pred: 2.5485; Loss self: 0.0000; time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6994 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6956 score: 0.5102 time: 0.06s
Epoch 7/1000, LR 0.000150
Train loss: 2.5328;  Loss pred: 2.5328; Loss self: 0.0000; time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6989 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6953 score: 0.5102 time: 0.06s
Epoch 8/1000, LR 0.000180
Train loss: 2.5283;  Loss pred: 2.5283; Loss self: 0.0000; time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6983 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6949 score: 0.5102 time: 0.06s
Epoch 9/1000, LR 0.000210
Train loss: 2.4412;  Loss pred: 2.4412; Loss self: 0.0000; time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6977 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6946 score: 0.5102 time: 0.06s
Epoch 10/1000, LR 0.000240
Train loss: 2.4508;  Loss pred: 2.4508; Loss self: 0.0000; time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6971 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6942 score: 0.5102 time: 0.06s
Epoch 11/1000, LR 0.000270
Train loss: 2.3948;  Loss pred: 2.3948; Loss self: 0.0000; time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6965 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6939 score: 0.5102 time: 0.06s
Epoch 12/1000, LR 0.000270
Train loss: 2.3694;  Loss pred: 2.3694; Loss self: 0.0000; time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6960 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6937 score: 0.5102 time: 0.06s
Epoch 13/1000, LR 0.000270
Train loss: 2.3160;  Loss pred: 2.3160; Loss self: 0.0000; time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6956 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6935 score: 0.5102 time: 0.06s
Epoch 14/1000, LR 0.000270
Train loss: 2.2732;  Loss pred: 2.2732; Loss self: 0.0000; time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6951 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6933 score: 0.5102 time: 0.06s
Epoch 15/1000, LR 0.000270
Train loss: 2.2523;  Loss pred: 2.2523; Loss self: 0.0000; time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6948 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6931 score: 0.5102 time: 0.07s
Epoch 16/1000, LR 0.000270
Train loss: 2.1912;  Loss pred: 2.1912; Loss self: 0.0000; time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6944 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6930 score: 0.5102 time: 0.06s
Epoch 17/1000, LR 0.000270
Train loss: 2.1789;  Loss pred: 2.1789; Loss self: 0.0000; time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6941 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6929 score: 0.5102 time: 0.07s
Epoch 18/1000, LR 0.000270
Train loss: 2.1025;  Loss pred: 2.1025; Loss self: 0.0000; time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6939 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6928 score: 0.5102 time: 0.07s
Epoch 19/1000, LR 0.000270
Train loss: 2.0800;  Loss pred: 2.0800; Loss self: 0.0000; time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6936 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6928 score: 0.5102 time: 0.07s
Epoch 20/1000, LR 0.000270
Train loss: 2.0671;  Loss pred: 2.0671; Loss self: 0.0000; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6934 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6927 score: 0.5102 time: 0.08s
Epoch 21/1000, LR 0.000270
Train loss: 2.0333;  Loss pred: 2.0333; Loss self: 0.0000; time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6932 score: 0.4898 time: 1.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6927 score: 0.5102 time: 1.51s
Epoch 22/1000, LR 0.000270
Train loss: 1.9904;  Loss pred: 1.9904; Loss self: 0.0000; time: 2.37s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6931 score: 0.4898 time: 0.36s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6927 score: 0.5102 time: 0.36s
Epoch 23/1000, LR 0.000270
Train loss: 1.9414;  Loss pred: 1.9414; Loss self: 0.0000; time: 1.56s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6929 score: 0.4898 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6926 score: 0.5102 time: 0.19s
Epoch 24/1000, LR 0.000270
Train loss: 1.9253;  Loss pred: 1.9253; Loss self: 0.0000; time: 0.43s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6928 score: 0.4898 time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6926 score: 0.5102 time: 0.09s
Epoch 25/1000, LR 0.000270
Train loss: 1.9235;  Loss pred: 1.9235; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6926 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6926 score: 0.5102 time: 0.06s
Epoch 26/1000, LR 0.000270
Train loss: 1.8694;  Loss pred: 1.8694; Loss self: 0.0000; time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6925 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6925 score: 0.5102 time: 0.06s
Epoch 27/1000, LR 0.000270
Train loss: 1.8400;  Loss pred: 1.8400; Loss self: 0.0000; time: 0.20s
Val loss: 0.6924 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6925 score: 0.5102 time: 0.06s
Epoch 28/1000, LR 0.000270
Train loss: 1.8156;  Loss pred: 1.8156; Loss self: 0.0000; time: 0.21s
Val loss: 0.6923 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6925 score: 0.5102 time: 0.06s
Epoch 29/1000, LR 0.000270
Train loss: 1.7705;  Loss pred: 1.7705; Loss self: 0.0000; time: 0.20s
Val loss: 0.6921 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6924 score: 0.5102 time: 0.06s
Epoch 30/1000, LR 0.000270
Train loss: 1.7614;  Loss pred: 1.7614; Loss self: 0.0000; time: 0.21s
Val loss: 0.6920 score: 0.5102 time: 0.07s
Test loss: 0.6924 score: 0.4898 time: 0.06s
Epoch 31/1000, LR 0.000270
Train loss: 1.7369;  Loss pred: 1.7369; Loss self: 0.0000; time: 0.20s
Val loss: 0.6919 score: 0.5510 time: 0.07s
Test loss: 0.6924 score: 0.5306 time: 0.06s
Epoch 32/1000, LR 0.000270
Train loss: 1.7061;  Loss pred: 1.7061; Loss self: 0.0000; time: 0.21s
Val loss: 0.6918 score: 0.5714 time: 0.07s
Test loss: 0.6923 score: 0.5306 time: 0.06s
Epoch 33/1000, LR 0.000270
Train loss: 1.6818;  Loss pred: 1.6818; Loss self: 0.0000; time: 0.21s
Val loss: 0.6917 score: 0.5918 time: 0.07s
Test loss: 0.6923 score: 0.5510 time: 0.07s
Epoch 34/1000, LR 0.000270
Train loss: 1.6412;  Loss pred: 1.6412; Loss self: 0.0000; time: 0.22s
Val loss: 0.6916 score: 0.6122 time: 0.07s
Test loss: 0.6922 score: 0.5714 time: 0.06s
Epoch 35/1000, LR 0.000270
Train loss: 1.6280;  Loss pred: 1.6280; Loss self: 0.0000; time: 0.24s
Val loss: 0.6915 score: 0.6531 time: 0.70s
Test loss: 0.6921 score: 0.5918 time: 0.70s
Epoch 36/1000, LR 0.000270
Train loss: 1.6055;  Loss pred: 1.6055; Loss self: 0.0000; time: 2.45s
Val loss: 0.6914 score: 0.7551 time: 0.62s
Test loss: 0.6921 score: 0.6327 time: 0.29s
Epoch 37/1000, LR 0.000270
Train loss: 1.5875;  Loss pred: 1.5875; Loss self: 0.0000; time: 1.48s
Val loss: 0.6913 score: 0.7755 time: 0.54s
Test loss: 0.6920 score: 0.6735 time: 0.29s
Epoch 38/1000, LR 0.000270
Train loss: 1.5803;  Loss pred: 1.5803; Loss self: 0.0000; time: 0.65s
Val loss: 0.6912 score: 0.7959 time: 0.26s
Test loss: 0.6919 score: 0.7143 time: 0.09s
Epoch 39/1000, LR 0.000269
Train loss: 1.5481;  Loss pred: 1.5481; Loss self: 0.0000; time: 0.24s
Val loss: 0.6911 score: 0.8163 time: 0.08s
Test loss: 0.6919 score: 0.7347 time: 0.06s
Epoch 40/1000, LR 0.000269
Train loss: 1.5423;  Loss pred: 1.5423; Loss self: 0.0000; time: 0.21s
Val loss: 0.6910 score: 0.8367 time: 0.07s
Test loss: 0.6918 score: 0.7551 time: 0.06s
Epoch 41/1000, LR 0.000269
Train loss: 1.5297;  Loss pred: 1.5297; Loss self: 0.0000; time: 0.21s
Val loss: 0.6908 score: 0.8571 time: 0.07s
Test loss: 0.6917 score: 0.7551 time: 0.06s
Epoch 42/1000, LR 0.000269
Train loss: 1.5123;  Loss pred: 1.5123; Loss self: 0.0000; time: 0.21s
Val loss: 0.6907 score: 0.8571 time: 0.07s
Test loss: 0.6916 score: 0.7755 time: 0.06s
Epoch 43/1000, LR 0.000269
Train loss: 1.4789;  Loss pred: 1.4789; Loss self: 0.0000; time: 0.21s
Val loss: 0.6906 score: 0.8571 time: 0.07s
Test loss: 0.6915 score: 0.7755 time: 0.06s
Epoch 44/1000, LR 0.000269
Train loss: 1.4809;  Loss pred: 1.4809; Loss self: 0.0000; time: 0.21s
Val loss: 0.6905 score: 0.8571 time: 0.07s
Test loss: 0.6914 score: 0.7755 time: 0.06s
Epoch 45/1000, LR 0.000269
Train loss: 1.4447;  Loss pred: 1.4447; Loss self: 0.0000; time: 0.21s
Val loss: 0.6903 score: 0.8571 time: 0.07s
Test loss: 0.6913 score: 0.7755 time: 0.06s
Epoch 46/1000, LR 0.000269
Train loss: 1.4472;  Loss pred: 1.4472; Loss self: 0.0000; time: 0.33s
Val loss: 0.6902 score: 0.8776 time: 0.85s
Test loss: 0.6912 score: 0.7755 time: 0.94s
Epoch 47/1000, LR 0.000269
Train loss: 1.4239;  Loss pred: 1.4239; Loss self: 0.0000; time: 2.72s
Val loss: 0.6900 score: 0.8776 time: 0.24s
Test loss: 0.6911 score: 0.7755 time: 0.12s
Epoch 48/1000, LR 0.000269
Train loss: 1.3974;  Loss pred: 1.3974; Loss self: 0.0000; time: 0.31s
Val loss: 0.6899 score: 0.8776 time: 0.08s
Test loss: 0.6910 score: 0.7551 time: 0.07s
Epoch 49/1000, LR 0.000269
Train loss: 1.3974;  Loss pred: 1.3974; Loss self: 0.0000; time: 0.22s
Val loss: 0.6897 score: 0.8367 time: 0.07s
Test loss: 0.6909 score: 0.7347 time: 0.06s
Epoch 50/1000, LR 0.000269
Train loss: 1.3857;  Loss pred: 1.3857; Loss self: 0.0000; time: 0.22s
Val loss: 0.6896 score: 0.8163 time: 0.07s
Test loss: 0.6908 score: 0.7347 time: 0.06s
Epoch 51/1000, LR 0.000269
Train loss: 1.3682;  Loss pred: 1.3682; Loss self: 0.0000; time: 0.21s
Val loss: 0.6893 score: 0.7959 time: 0.07s
Test loss: 0.6906 score: 0.7347 time: 0.06s
Epoch 52/1000, LR 0.000269
Train loss: 1.3621;  Loss pred: 1.3621; Loss self: 0.0000; time: 0.20s
Val loss: 0.6891 score: 0.7959 time: 0.07s
Test loss: 0.6905 score: 0.7347 time: 0.06s
Epoch 53/1000, LR 0.000269
Train loss: 1.3404;  Loss pred: 1.3404; Loss self: 0.0000; time: 0.20s
Val loss: 0.6889 score: 0.7755 time: 0.07s
Test loss: 0.6903 score: 0.7143 time: 0.06s
Epoch 54/1000, LR 0.000269
Train loss: 1.3306;  Loss pred: 1.3306; Loss self: 0.0000; time: 0.20s
Val loss: 0.6886 score: 0.7551 time: 0.07s
Test loss: 0.6901 score: 0.6531 time: 0.06s
Epoch 55/1000, LR 0.000269
Train loss: 1.3141;  Loss pred: 1.3141; Loss self: 0.0000; time: 0.20s
Val loss: 0.6884 score: 0.7143 time: 0.07s
Test loss: 0.6900 score: 0.6327 time: 0.06s
Epoch 56/1000, LR 0.000269
Train loss: 1.3026;  Loss pred: 1.3026; Loss self: 0.0000; time: 0.20s
Val loss: 0.6882 score: 0.6939 time: 0.08s
Test loss: 0.6898 score: 0.6122 time: 0.06s
Epoch 57/1000, LR 0.000269
Train loss: 1.3011;  Loss pred: 1.3011; Loss self: 0.0000; time: 0.22s
Val loss: 0.6880 score: 0.6939 time: 0.08s
Test loss: 0.6897 score: 0.5918 time: 0.14s
Epoch 58/1000, LR 0.000269
Train loss: 1.2923;  Loss pred: 1.2923; Loss self: 0.0000; time: 0.21s
Val loss: 0.6878 score: 0.6939 time: 0.07s
Test loss: 0.6895 score: 0.5918 time: 0.06s
Epoch 59/1000, LR 0.000268
Train loss: 1.2775;  Loss pred: 1.2775; Loss self: 0.0000; time: 0.22s
Val loss: 0.6876 score: 0.6735 time: 0.07s
Test loss: 0.6893 score: 0.5918 time: 0.06s
Epoch 60/1000, LR 0.000268
Train loss: 1.2560;  Loss pred: 1.2560; Loss self: 0.0000; time: 0.22s
Val loss: 0.6873 score: 0.6735 time: 1.51s
Test loss: 0.6891 score: 0.6122 time: 2.16s
Epoch 61/1000, LR 0.000268
Train loss: 1.2567;  Loss pred: 1.2567; Loss self: 0.0000; time: 2.99s
Val loss: 0.6870 score: 0.6939 time: 0.09s
Test loss: 0.6889 score: 0.6122 time: 0.07s
Epoch 62/1000, LR 0.000268
Train loss: 1.2505;  Loss pred: 1.2505; Loss self: 0.0000; time: 0.20s
Val loss: 0.6868 score: 0.6939 time: 0.08s
Test loss: 0.6887 score: 0.6327 time: 0.07s
Epoch 63/1000, LR 0.000268
Train loss: 1.2438;  Loss pred: 1.2438; Loss self: 0.0000; time: 0.20s
Val loss: 0.6865 score: 0.6939 time: 0.07s
Test loss: 0.6885 score: 0.6327 time: 0.06s
Epoch 64/1000, LR 0.000268
Train loss: 1.2290;  Loss pred: 1.2290; Loss self: 0.0000; time: 0.20s
Val loss: 0.6862 score: 0.6939 time: 0.07s
Test loss: 0.6883 score: 0.6531 time: 0.06s
Epoch 65/1000, LR 0.000268
Train loss: 1.2214;  Loss pred: 1.2214; Loss self: 0.0000; time: 0.21s
Val loss: 0.6859 score: 0.6939 time: 0.07s
Test loss: 0.6881 score: 0.6735 time: 0.06s
Epoch 66/1000, LR 0.000268
Train loss: 1.2153;  Loss pred: 1.2153; Loss self: 0.0000; time: 0.20s
Val loss: 0.6856 score: 0.7143 time: 0.07s
Test loss: 0.6879 score: 0.6735 time: 0.06s
Epoch 67/1000, LR 0.000268
Train loss: 1.2028;  Loss pred: 1.2028; Loss self: 0.0000; time: 0.20s
Val loss: 0.6853 score: 0.7551 time: 0.07s
Test loss: 0.6877 score: 0.7143 time: 0.06s
Epoch 68/1000, LR 0.000268
Train loss: 1.2021;  Loss pred: 1.2021; Loss self: 0.0000; time: 0.20s
Val loss: 0.6850 score: 0.8571 time: 0.07s
Test loss: 0.6875 score: 0.7551 time: 0.06s
Epoch 69/1000, LR 0.000268
Train loss: 1.1941;  Loss pred: 1.1941; Loss self: 0.0000; time: 2.77s
Val loss: 0.6846 score: 0.9184 time: 0.57s
Test loss: 0.6873 score: 0.7347 time: 1.41s
Epoch 70/1000, LR 0.000268
Train loss: 1.1854;  Loss pred: 1.1854; Loss self: 0.0000; time: 1.52s
Val loss: 0.6843 score: 0.9592 time: 0.10s
Test loss: 0.6870 score: 0.7755 time: 0.07s
Epoch 71/1000, LR 0.000268
Train loss: 1.1821;  Loss pred: 1.1821; Loss self: 0.0000; time: 0.22s
Val loss: 0.6839 score: 0.9592 time: 0.07s
Test loss: 0.6868 score: 0.8163 time: 0.07s
Epoch 72/1000, LR 0.000267
Train loss: 1.1688;  Loss pred: 1.1688; Loss self: 0.0000; time: 0.21s
Val loss: 0.6835 score: 0.9592 time: 0.06s
Test loss: 0.6865 score: 0.8163 time: 0.06s
Epoch 73/1000, LR 0.000267
Train loss: 1.1702;  Loss pred: 1.1702; Loss self: 0.0000; time: 0.19s
Val loss: 0.6831 score: 0.9796 time: 0.06s
Test loss: 0.6862 score: 0.8163 time: 0.06s
Epoch 74/1000, LR 0.000267
Train loss: 1.1560;  Loss pred: 1.1560; Loss self: 0.0000; time: 0.20s
Val loss: 0.6827 score: 0.9796 time: 0.06s
Test loss: 0.6859 score: 0.8163 time: 0.06s
Epoch 75/1000, LR 0.000267
Train loss: 1.1550;  Loss pred: 1.1550; Loss self: 0.0000; time: 0.19s
Val loss: 0.6823 score: 0.9796 time: 0.06s
Test loss: 0.6856 score: 0.8367 time: 0.06s
Epoch 76/1000, LR 0.000267
Train loss: 1.1526;  Loss pred: 1.1526; Loss self: 0.0000; time: 0.19s
Val loss: 0.6819 score: 0.9796 time: 0.07s
Test loss: 0.6853 score: 0.8367 time: 0.06s
Epoch 77/1000, LR 0.000267
Train loss: 1.1443;  Loss pred: 1.1443; Loss self: 0.0000; time: 0.19s
Val loss: 0.6814 score: 0.9796 time: 0.07s
Test loss: 0.6850 score: 0.8367 time: 0.06s
Epoch 78/1000, LR 0.000267
Train loss: 1.1388;  Loss pred: 1.1388; Loss self: 0.0000; time: 0.20s
Val loss: 0.6810 score: 0.9796 time: 0.07s
Test loss: 0.6847 score: 0.8367 time: 0.06s
Epoch 79/1000, LR 0.000267
Train loss: 1.1302;  Loss pred: 1.1302; Loss self: 0.0000; time: 0.20s
Val loss: 0.6805 score: 0.9796 time: 0.07s
Test loss: 0.6844 score: 0.8367 time: 0.06s
Epoch 80/1000, LR 0.000267
Train loss: 1.1306;  Loss pred: 1.1306; Loss self: 0.0000; time: 0.20s
Val loss: 0.6801 score: 0.9796 time: 0.07s
Test loss: 0.6840 score: 0.8367 time: 0.07s
Epoch 81/1000, LR 0.000267
Train loss: 1.1202;  Loss pred: 1.1202; Loss self: 0.0000; time: 0.22s
Val loss: 0.6796 score: 0.9796 time: 0.67s
Test loss: 0.6837 score: 0.8367 time: 1.30s
Epoch 82/1000, LR 0.000267
Train loss: 1.1172;  Loss pred: 1.1172; Loss self: 0.0000; time: 4.38s
Val loss: 0.6790 score: 0.9796 time: 0.70s
Test loss: 0.6833 score: 0.8367 time: 0.60s
Epoch 83/1000, LR 0.000266
Train loss: 1.1132;  Loss pred: 1.1132; Loss self: 0.0000; time: 0.49s
Val loss: 0.6785 score: 0.9796 time: 0.10s
Test loss: 0.6829 score: 0.8367 time: 0.34s
Epoch 84/1000, LR 0.000266
Train loss: 1.1083;  Loss pred: 1.1083; Loss self: 0.0000; time: 0.47s
Val loss: 0.6780 score: 0.9796 time: 0.07s
Test loss: 0.6825 score: 0.8367 time: 0.06s
Epoch 85/1000, LR 0.000266
Train loss: 1.1064;  Loss pred: 1.1064; Loss self: 0.0000; time: 0.21s
Val loss: 0.6774 score: 0.9796 time: 0.07s
Test loss: 0.6821 score: 0.8367 time: 0.06s
Epoch 86/1000, LR 0.000266
Train loss: 1.1020;  Loss pred: 1.1020; Loss self: 0.0000; time: 0.21s
Val loss: 0.6768 score: 0.9796 time: 0.07s
Test loss: 0.6817 score: 0.8367 time: 0.06s
Epoch 87/1000, LR 0.000266
Train loss: 1.0979;  Loss pred: 1.0979; Loss self: 0.0000; time: 0.20s
Val loss: 0.6762 score: 0.9796 time: 0.07s
Test loss: 0.6812 score: 0.8367 time: 0.06s
Epoch 88/1000, LR 0.000266
Train loss: 1.0916;  Loss pred: 1.0916; Loss self: 0.0000; time: 0.20s
Val loss: 0.6756 score: 0.9796 time: 0.07s
Test loss: 0.6808 score: 0.8367 time: 0.06s
Epoch 89/1000, LR 0.000266
Train loss: 1.0906;  Loss pred: 1.0906; Loss self: 0.0000; time: 0.20s
Val loss: 0.6750 score: 0.9796 time: 0.08s
Test loss: 0.6803 score: 0.8367 time: 0.07s
Epoch 90/1000, LR 0.000266
Train loss: 1.0888;  Loss pred: 1.0888; Loss self: 0.0000; time: 0.22s
Val loss: 0.6743 score: 0.9796 time: 0.06s
Test loss: 0.6798 score: 0.8367 time: 0.06s
Epoch 91/1000, LR 0.000266
Train loss: 1.0853;  Loss pred: 1.0853; Loss self: 0.0000; time: 0.19s
Val loss: 0.6736 score: 0.9796 time: 0.06s
Test loss: 0.6793 score: 0.8367 time: 0.06s
Epoch 92/1000, LR 0.000266
Train loss: 1.0818;  Loss pred: 1.0818; Loss self: 0.0000; time: 0.19s
Val loss: 0.6729 score: 0.9796 time: 0.07s
Test loss: 0.6788 score: 0.8571 time: 0.80s
Epoch 93/1000, LR 0.000265
Train loss: 1.0758;  Loss pred: 1.0758; Loss self: 0.0000; time: 4.10s
Val loss: 0.6722 score: 0.9796 time: 1.46s
Test loss: 0.6783 score: 0.8571 time: 0.46s
Epoch 94/1000, LR 0.000265
Train loss: 1.0741;  Loss pred: 1.0741; Loss self: 0.0000; time: 0.34s
Val loss: 0.6715 score: 0.9796 time: 0.11s
Test loss: 0.6777 score: 0.8571 time: 0.15s
Epoch 95/1000, LR 0.000265
Train loss: 1.0697;  Loss pred: 1.0697; Loss self: 0.0000; time: 0.22s
Val loss: 0.6707 score: 0.9796 time: 0.07s
Test loss: 0.6772 score: 0.8571 time: 0.06s
Epoch 96/1000, LR 0.000265
Train loss: 1.0661;  Loss pred: 1.0661; Loss self: 0.0000; time: 0.20s
Val loss: 0.6699 score: 0.9796 time: 0.07s
Test loss: 0.6766 score: 0.8571 time: 0.06s
Epoch 97/1000, LR 0.000265
Train loss: 1.0631;  Loss pred: 1.0631; Loss self: 0.0000; time: 0.20s
Val loss: 0.6691 score: 0.9796 time: 0.06s
Test loss: 0.6760 score: 0.8571 time: 0.08s
Epoch 98/1000, LR 0.000265
Train loss: 1.0612;  Loss pred: 1.0612; Loss self: 0.0000; time: 0.20s
Val loss: 0.6683 score: 0.9796 time: 0.06s
Test loss: 0.6754 score: 0.8571 time: 0.06s
Epoch 99/1000, LR 0.000265
Train loss: 1.0535;  Loss pred: 1.0535; Loss self: 0.0000; time: 0.20s
Val loss: 0.6674 score: 0.9796 time: 0.06s
Test loss: 0.6747 score: 0.8571 time: 0.06s
Epoch 100/1000, LR 0.000265
Train loss: 1.0528;  Loss pred: 1.0528; Loss self: 0.0000; time: 0.20s
Val loss: 0.6665 score: 0.9796 time: 0.06s
Test loss: 0.6741 score: 0.8571 time: 0.06s
Epoch 101/1000, LR 0.000265
Train loss: 1.0476;  Loss pred: 1.0476; Loss self: 0.0000; time: 0.21s
Val loss: 0.6656 score: 0.9796 time: 0.07s
Test loss: 0.6734 score: 0.8571 time: 0.06s
Epoch 102/1000, LR 0.000264
Train loss: 1.0523;  Loss pred: 1.0523; Loss self: 0.0000; time: 0.20s
Val loss: 0.6647 score: 0.9796 time: 0.07s
Test loss: 0.6727 score: 0.8571 time: 0.07s
Epoch 103/1000, LR 0.000264
Train loss: 1.0444;  Loss pred: 1.0444; Loss self: 0.0000; time: 0.20s
Val loss: 0.6637 score: 0.9796 time: 0.07s
Test loss: 0.6720 score: 0.8980 time: 0.07s
Epoch 104/1000, LR 0.000264
Train loss: 1.0442;  Loss pred: 1.0442; Loss self: 0.0000; time: 0.20s
Val loss: 0.6627 score: 0.9796 time: 0.07s
Test loss: 0.6713 score: 0.8571 time: 0.06s
Epoch 105/1000, LR 0.000264
Train loss: 1.0425;  Loss pred: 1.0425; Loss self: 0.0000; time: 2.20s
Val loss: 0.6616 score: 0.9796 time: 0.36s
Test loss: 0.6705 score: 0.8571 time: 0.31s
Epoch 106/1000, LR 0.000264
Train loss: 1.0383;  Loss pred: 1.0383; Loss self: 0.0000; time: 1.62s
Val loss: 0.6606 score: 0.9796 time: 0.21s
Test loss: 0.6697 score: 0.8980 time: 0.29s
Epoch 107/1000, LR 0.000264
Train loss: 1.0341;  Loss pred: 1.0341; Loss self: 0.0000; time: 0.59s
Val loss: 0.6595 score: 0.9796 time: 0.07s
Test loss: 0.6689 score: 0.8980 time: 0.06s
Epoch 108/1000, LR 0.000264
Train loss: 1.0308;  Loss pred: 1.0308; Loss self: 0.0000; time: 0.20s
Val loss: 0.6584 score: 0.9796 time: 0.06s
Test loss: 0.6681 score: 0.8980 time: 0.06s
Epoch 109/1000, LR 0.000264
Train loss: 1.0310;  Loss pred: 1.0310; Loss self: 0.0000; time: 0.19s
Val loss: 0.6572 score: 0.9796 time: 0.06s
Test loss: 0.6672 score: 0.8980 time: 0.06s
Epoch 110/1000, LR 0.000263
Train loss: 1.0272;  Loss pred: 1.0272; Loss self: 0.0000; time: 0.19s
Val loss: 0.6560 score: 0.9796 time: 0.06s
Test loss: 0.6663 score: 0.8980 time: 0.06s
Epoch 111/1000, LR 0.000263
Train loss: 1.0265;  Loss pred: 1.0265; Loss self: 0.0000; time: 0.19s
Val loss: 0.6548 score: 0.9796 time: 0.07s
Test loss: 0.6654 score: 0.8980 time: 0.06s
Epoch 112/1000, LR 0.000263
Train loss: 1.0257;  Loss pred: 1.0257; Loss self: 0.0000; time: 0.19s
Val loss: 0.6535 score: 0.9796 time: 0.06s
Test loss: 0.6644 score: 0.8980 time: 0.06s
Epoch 113/1000, LR 0.000263
Train loss: 1.0252;  Loss pred: 1.0252; Loss self: 0.0000; time: 0.19s
Val loss: 0.6521 score: 0.9796 time: 0.06s
Test loss: 0.6634 score: 0.8980 time: 0.06s
Epoch 114/1000, LR 0.000263
Train loss: 1.0157;  Loss pred: 1.0157; Loss self: 0.0000; time: 0.20s
Val loss: 0.6506 score: 0.9796 time: 0.07s
Test loss: 0.6624 score: 0.8980 time: 0.06s
Epoch 115/1000, LR 0.000263
Train loss: 1.0167;  Loss pred: 1.0167; Loss self: 0.0000; time: 0.19s
Val loss: 0.6491 score: 0.9796 time: 0.06s
Test loss: 0.6613 score: 0.8980 time: 0.06s
Epoch 116/1000, LR 0.000263
Train loss: 1.0124;  Loss pred: 1.0124; Loss self: 0.0000; time: 0.20s
Val loss: 0.6475 score: 0.9796 time: 0.07s
Test loss: 0.6601 score: 0.8776 time: 0.06s
Epoch 117/1000, LR 0.000262
Train loss: 1.0098;  Loss pred: 1.0098; Loss self: 0.0000; time: 0.20s
Val loss: 0.6459 score: 0.9796 time: 0.07s
Test loss: 0.6589 score: 0.8571 time: 0.06s
Epoch 118/1000, LR 0.000262
Train loss: 1.0110;  Loss pred: 1.0110; Loss self: 0.0000; time: 0.69s
Val loss: 0.6443 score: 0.9796 time: 0.98s
Test loss: 0.6577 score: 0.8571 time: 1.08s
Epoch 119/1000, LR 0.000262
Train loss: 1.0088;  Loss pred: 1.0088; Loss self: 0.0000; time: 2.42s
Val loss: 0.6426 score: 0.9796 time: 0.51s
Test loss: 0.6564 score: 0.8571 time: 0.63s
Epoch 120/1000, LR 0.000262
Train loss: 1.0009;  Loss pred: 1.0009; Loss self: 0.0000; time: 2.28s
Val loss: 0.6409 score: 0.9796 time: 0.07s
Test loss: 0.6551 score: 0.8571 time: 0.06s
Epoch 121/1000, LR 0.000262
Train loss: 1.0002;  Loss pred: 1.0002; Loss self: 0.0000; time: 0.21s
Val loss: 0.6392 score: 0.9796 time: 0.07s
Test loss: 0.6538 score: 0.8571 time: 0.06s
Epoch 122/1000, LR 0.000262
Train loss: 0.9971;  Loss pred: 0.9971; Loss self: 0.0000; time: 0.20s
Val loss: 0.6374 score: 0.9796 time: 0.06s
Test loss: 0.6525 score: 0.8571 time: 0.06s
Epoch 123/1000, LR 0.000262
Train loss: 0.9955;  Loss pred: 0.9955; Loss self: 0.0000; time: 0.20s
Val loss: 0.6355 score: 0.9796 time: 0.06s
Test loss: 0.6511 score: 0.8571 time: 0.06s
Epoch 124/1000, LR 0.000261
Train loss: 0.9937;  Loss pred: 0.9937; Loss self: 0.0000; time: 0.22s
Val loss: 0.6336 score: 0.9796 time: 0.07s
Test loss: 0.6497 score: 0.8571 time: 0.06s
Epoch 125/1000, LR 0.000261
Train loss: 0.9914;  Loss pred: 0.9914; Loss self: 0.0000; time: 0.20s
Val loss: 0.6317 score: 0.9796 time: 0.07s
Test loss: 0.6482 score: 0.8571 time: 0.06s
Epoch 126/1000, LR 0.000261
Train loss: 0.9866;  Loss pred: 0.9866; Loss self: 0.0000; time: 0.21s
Val loss: 0.6296 score: 0.9796 time: 0.08s
Test loss: 0.6468 score: 0.8571 time: 2.05s
Epoch 127/1000, LR 0.000261
Train loss: 0.9876;  Loss pred: 0.9876; Loss self: 0.0000; time: 6.00s
Val loss: 0.6276 score: 0.9796 time: 0.18s
Test loss: 0.6452 score: 0.8571 time: 0.08s
Epoch 128/1000, LR 0.000261
Train loss: 0.9832;  Loss pred: 0.9832; Loss self: 0.0000; time: 0.24s
Val loss: 0.6254 score: 0.9796 time: 0.07s
Test loss: 0.6436 score: 0.8571 time: 0.06s
Epoch 129/1000, LR 0.000261
Train loss: 0.9827;  Loss pred: 0.9827; Loss self: 0.0000; time: 0.20s
Val loss: 0.6232 score: 0.9796 time: 0.06s
Test loss: 0.6420 score: 0.8571 time: 0.06s
Epoch 130/1000, LR 0.000260
Train loss: 0.9791;  Loss pred: 0.9791; Loss self: 0.0000; time: 0.20s
Val loss: 0.6209 score: 0.9796 time: 0.07s
Test loss: 0.6403 score: 0.8571 time: 0.06s
Epoch 131/1000, LR 0.000260
Train loss: 0.9782;  Loss pred: 0.9782; Loss self: 0.0000; time: 0.20s
Val loss: 0.6185 score: 0.9796 time: 0.07s
Test loss: 0.6386 score: 0.8571 time: 0.06s
Epoch 132/1000, LR 0.000260
Train loss: 0.9743;  Loss pred: 0.9743; Loss self: 0.0000; time: 0.20s
Val loss: 0.6161 score: 0.9796 time: 0.07s
Test loss: 0.6368 score: 0.8776 time: 0.06s
Epoch 133/1000, LR 0.000260
Train loss: 0.9721;  Loss pred: 0.9721; Loss self: 0.0000; time: 0.20s
Val loss: 0.6137 score: 0.9796 time: 0.06s
Test loss: 0.6350 score: 0.8571 time: 0.06s
Epoch 134/1000, LR 0.000260
Train loss: 0.9705;  Loss pred: 0.9705; Loss self: 0.0000; time: 0.20s
Val loss: 0.6111 score: 0.9796 time: 0.07s
Test loss: 0.6332 score: 0.8571 time: 0.06s
Epoch 135/1000, LR 0.000260
Train loss: 0.9676;  Loss pred: 0.9676; Loss self: 0.0000; time: 5.45s
Val loss: 0.6086 score: 0.9796 time: 0.48s
Test loss: 0.6313 score: 0.8776 time: 0.43s
Epoch 136/1000, LR 0.000260
Train loss: 0.9646;  Loss pred: 0.9646; Loss self: 0.0000; time: 1.82s
Val loss: 0.6060 score: 0.9796 time: 0.66s
Test loss: 0.6294 score: 0.8980 time: 0.85s
Epoch 137/1000, LR 0.000259
Train loss: 0.9611;  Loss pred: 0.9611; Loss self: 0.0000; time: 0.89s
Val loss: 0.6034 score: 0.9796 time: 0.09s
Test loss: 0.6275 score: 0.8980 time: 0.07s
Epoch 138/1000, LR 0.000259
Train loss: 0.9598;  Loss pred: 0.9598; Loss self: 0.0000; time: 0.21s
Val loss: 0.6008 score: 0.9796 time: 0.07s
Test loss: 0.6255 score: 0.8980 time: 0.06s
Epoch 139/1000, LR 0.000259
Train loss: 0.9555;  Loss pred: 0.9555; Loss self: 0.0000; time: 0.21s
Val loss: 0.5981 score: 0.9796 time: 0.07s
Test loss: 0.6236 score: 0.8980 time: 0.06s
Epoch 140/1000, LR 0.000259
Train loss: 0.9546;  Loss pred: 0.9546; Loss self: 0.0000; time: 0.21s
Val loss: 0.5953 score: 0.9796 time: 0.07s
Test loss: 0.6215 score: 0.8980 time: 0.06s
Epoch 141/1000, LR 0.000259
Train loss: 0.9511;  Loss pred: 0.9511; Loss self: 0.0000; time: 0.21s
Val loss: 0.5926 score: 0.9796 time: 0.07s
Test loss: 0.6195 score: 0.8980 time: 0.06s
Epoch 142/1000, LR 0.000259
Train loss: 0.9481;  Loss pred: 0.9481; Loss self: 0.0000; time: 0.21s
Val loss: 0.5897 score: 0.9796 time: 0.07s
Test loss: 0.6174 score: 0.8980 time: 0.06s
Epoch 143/1000, LR 0.000258
Train loss: 0.9497;  Loss pred: 0.9497; Loss self: 0.0000; time: 0.21s
Val loss: 0.5868 score: 0.9796 time: 0.07s
Test loss: 0.6153 score: 0.8980 time: 0.06s
Epoch 144/1000, LR 0.000258
Train loss: 0.9440;  Loss pred: 0.9440; Loss self: 0.0000; time: 1.42s
Val loss: 0.5838 score: 0.9796 time: 0.60s
Test loss: 0.6131 score: 0.8980 time: 0.69s
Epoch 145/1000, LR 0.000258
Train loss: 0.9397;  Loss pred: 0.9397; Loss self: 0.0000; time: 1.88s
Val loss: 0.5808 score: 0.9796 time: 0.27s
Test loss: 0.6109 score: 0.8980 time: 0.93s
Epoch 146/1000, LR 0.000258
Train loss: 0.9370;  Loss pred: 0.9370; Loss self: 0.0000; time: 2.23s
Val loss: 0.5777 score: 0.9796 time: 0.08s
Test loss: 0.6087 score: 0.8980 time: 0.30s
Epoch 147/1000, LR 0.000258
Train loss: 0.9373;  Loss pred: 0.9373; Loss self: 0.0000; time: 0.86s
Val loss: 0.5746 score: 0.9796 time: 0.07s
Test loss: 0.6064 score: 0.8980 time: 0.06s
Epoch 148/1000, LR 0.000257
Train loss: 0.9349;  Loss pred: 0.9349; Loss self: 0.0000; time: 0.22s
Val loss: 0.5714 score: 0.9796 time: 0.08s
Test loss: 0.6041 score: 0.8980 time: 0.06s
Epoch 149/1000, LR 0.000257
Train loss: 0.9313;  Loss pred: 0.9313; Loss self: 0.0000; time: 0.21s
Val loss: 0.5681 score: 0.9796 time: 0.07s
Test loss: 0.6017 score: 0.8980 time: 0.07s
Epoch 150/1000, LR 0.000257
Train loss: 0.9275;  Loss pred: 0.9275; Loss self: 0.0000; time: 0.21s
Val loss: 0.5649 score: 0.9796 time: 0.07s
Test loss: 0.5993 score: 0.8980 time: 0.06s
Epoch 151/1000, LR 0.000257
Train loss: 0.9235;  Loss pred: 0.9235; Loss self: 0.0000; time: 0.22s
Val loss: 0.5615 score: 0.9796 time: 0.08s
Test loss: 0.5969 score: 0.8980 time: 0.06s
Epoch 152/1000, LR 0.000257
Train loss: 0.9230;  Loss pred: 0.9230; Loss self: 0.0000; time: 0.22s
Val loss: 0.5582 score: 0.9796 time: 0.08s
Test loss: 0.5944 score: 0.8980 time: 0.07s
Epoch 153/1000, LR 0.000257
Train loss: 0.9207;  Loss pred: 0.9207; Loss self: 0.0000; time: 0.22s
Val loss: 0.5548 score: 0.9796 time: 0.07s
Test loss: 0.5919 score: 0.8980 time: 0.06s
Epoch 154/1000, LR 0.000256
Train loss: 0.9153;  Loss pred: 0.9153; Loss self: 0.0000; time: 0.20s
Val loss: 0.5514 score: 0.9796 time: 0.07s
Test loss: 0.5894 score: 0.8980 time: 0.06s
Epoch 155/1000, LR 0.000256
Train loss: 0.9142;  Loss pred: 0.9142; Loss self: 0.0000; time: 1.61s
Val loss: 0.5479 score: 0.9796 time: 0.37s
Test loss: 0.5868 score: 0.8980 time: 0.67s
Epoch 156/1000, LR 0.000256
Train loss: 0.9117;  Loss pred: 0.9117; Loss self: 0.0000; time: 1.63s
Val loss: 0.5443 score: 0.9796 time: 0.55s
Test loss: 0.5842 score: 0.8980 time: 0.47s
Epoch 157/1000, LR 0.000256
Train loss: 0.9056;  Loss pred: 0.9056; Loss self: 0.0000; time: 1.70s
Val loss: 0.5407 score: 0.9796 time: 0.07s
Test loss: 0.5816 score: 0.8980 time: 0.06s
Epoch 158/1000, LR 0.000256
Train loss: 0.9057;  Loss pred: 0.9057; Loss self: 0.0000; time: 0.19s
Val loss: 0.5371 score: 0.9796 time: 0.07s
Test loss: 0.5790 score: 0.8980 time: 0.06s
Epoch 159/1000, LR 0.000255
Train loss: 0.9023;  Loss pred: 0.9023; Loss self: 0.0000; time: 0.19s
Val loss: 0.5334 score: 0.9796 time: 0.06s
Test loss: 0.5763 score: 0.8980 time: 0.06s
Epoch 160/1000, LR 0.000255
Train loss: 0.8978;  Loss pred: 0.8978; Loss self: 0.0000; time: 0.19s
Val loss: 0.5297 score: 0.9796 time: 0.07s
Test loss: 0.5736 score: 0.8980 time: 0.06s
Epoch 161/1000, LR 0.000255
Train loss: 0.8949;  Loss pred: 0.8949; Loss self: 0.0000; time: 0.19s
Val loss: 0.5260 score: 0.9796 time: 0.07s
Test loss: 0.5709 score: 0.8980 time: 0.06s
Epoch 162/1000, LR 0.000255
Train loss: 0.8920;  Loss pred: 0.8920; Loss self: 0.0000; time: 0.19s
Val loss: 0.5222 score: 0.9796 time: 0.07s
Test loss: 0.5681 score: 0.8980 time: 0.06s
Epoch 163/1000, LR 0.000255
Train loss: 0.8899;  Loss pred: 0.8899; Loss self: 0.0000; time: 0.19s
Val loss: 0.5183 score: 0.9796 time: 0.06s
Test loss: 0.5654 score: 0.8980 time: 0.06s
Epoch 164/1000, LR 0.000254
Train loss: 0.8889;  Loss pred: 0.8889; Loss self: 0.0000; time: 0.19s
Val loss: 0.5145 score: 0.9796 time: 0.06s
Test loss: 0.5626 score: 0.8980 time: 0.06s
Epoch 165/1000, LR 0.000254
Train loss: 0.8819;  Loss pred: 0.8819; Loss self: 0.0000; time: 0.19s
Val loss: 0.5106 score: 0.9796 time: 0.06s
Test loss: 0.5598 score: 0.8980 time: 0.06s
Epoch 166/1000, LR 0.000254
Train loss: 0.8797;  Loss pred: 0.8797; Loss self: 0.0000; time: 0.19s
Val loss: 0.5066 score: 0.9796 time: 0.06s
Test loss: 0.5569 score: 0.8980 time: 0.06s
Epoch 167/1000, LR 0.000254
Train loss: 0.8779;  Loss pred: 0.8779; Loss self: 0.0000; time: 0.19s
Val loss: 0.5026 score: 0.9796 time: 0.06s
Test loss: 0.5540 score: 0.8980 time: 0.06s
Epoch 168/1000, LR 0.000254
Train loss: 0.8758;  Loss pred: 0.8758; Loss self: 0.0000; time: 0.19s
Val loss: 0.4986 score: 0.9796 time: 0.06s
Test loss: 0.5511 score: 0.8980 time: 0.06s
Epoch 169/1000, LR 0.000253
Train loss: 0.8697;  Loss pred: 0.8697; Loss self: 0.0000; time: 0.19s
Val loss: 0.4945 score: 0.9796 time: 0.06s
Test loss: 0.5482 score: 0.8980 time: 0.06s
Epoch 170/1000, LR 0.000253
Train loss: 0.8681;  Loss pred: 0.8681; Loss self: 0.0000; time: 0.19s
Val loss: 0.4905 score: 0.9796 time: 0.06s
Test loss: 0.5452 score: 0.8980 time: 0.06s
Epoch 171/1000, LR 0.000253
Train loss: 0.8653;  Loss pred: 0.8653; Loss self: 0.0000; time: 0.19s
Val loss: 0.4863 score: 0.9796 time: 0.06s
Test loss: 0.5423 score: 0.8980 time: 0.06s
Epoch 172/1000, LR 0.000253
Train loss: 0.8625;  Loss pred: 0.8625; Loss self: 0.0000; time: 0.19s
Val loss: 0.4822 score: 0.9796 time: 0.07s
Test loss: 0.5393 score: 0.8980 time: 0.06s
Epoch 173/1000, LR 0.000253
Train loss: 0.8600;  Loss pred: 0.8600; Loss self: 0.0000; time: 0.19s
Val loss: 0.4780 score: 0.9796 time: 0.06s
Test loss: 0.5363 score: 0.8980 time: 0.06s
Epoch 174/1000, LR 0.000252
Train loss: 0.8565;  Loss pred: 0.8565; Loss self: 0.0000; time: 0.19s
Val loss: 0.4738 score: 0.9796 time: 0.06s
Test loss: 0.5333 score: 0.8980 time: 0.06s
Epoch 175/1000, LR 0.000252
Train loss: 0.8549;  Loss pred: 0.8549; Loss self: 0.0000; time: 0.19s
Val loss: 0.4696 score: 0.9796 time: 0.06s
Test loss: 0.5302 score: 0.8980 time: 0.06s
Epoch 176/1000, LR 0.000252
Train loss: 0.8499;  Loss pred: 0.8499; Loss self: 0.0000; time: 0.19s
Val loss: 0.4654 score: 0.9796 time: 0.06s
Test loss: 0.5272 score: 0.8980 time: 0.05s
Epoch 177/1000, LR 0.000252
Train loss: 0.8466;  Loss pred: 0.8466; Loss self: 0.0000; time: 0.19s
Val loss: 0.4611 score: 0.9796 time: 0.06s
Test loss: 0.5241 score: 0.8980 time: 0.06s
Epoch 178/1000, LR 0.000251
Train loss: 0.8428;  Loss pred: 0.8428; Loss self: 0.0000; time: 0.19s
Val loss: 0.4568 score: 0.9796 time: 0.06s
Test loss: 0.5211 score: 0.8980 time: 0.06s
Epoch 179/1000, LR 0.000251
Train loss: 0.8395;  Loss pred: 0.8395; Loss self: 0.0000; time: 0.19s
Val loss: 0.4525 score: 0.9796 time: 0.06s
Test loss: 0.5180 score: 0.8980 time: 0.06s
Epoch 180/1000, LR 0.000251
Train loss: 0.8365;  Loss pred: 0.8365; Loss self: 0.0000; time: 0.19s
Val loss: 0.4482 score: 0.9796 time: 0.06s
Test loss: 0.5149 score: 0.8980 time: 0.06s
Epoch 181/1000, LR 0.000251
Train loss: 0.8354;  Loss pred: 0.8354; Loss self: 0.0000; time: 0.19s
Val loss: 0.4439 score: 0.9796 time: 0.06s
Test loss: 0.5118 score: 0.8980 time: 0.06s
Epoch 182/1000, LR 0.000251
Train loss: 0.8309;  Loss pred: 0.8309; Loss self: 0.0000; time: 0.19s
Val loss: 0.4396 score: 0.9796 time: 0.07s
Test loss: 0.5087 score: 0.8980 time: 0.06s
Epoch 183/1000, LR 0.000250
Train loss: 0.8296;  Loss pred: 0.8296; Loss self: 0.0000; time: 0.19s
Val loss: 0.4352 score: 0.9796 time: 0.06s
Test loss: 0.5056 score: 0.8980 time: 0.06s
Epoch 184/1000, LR 0.000250
Train loss: 0.8244;  Loss pred: 0.8244; Loss self: 0.0000; time: 0.19s
Val loss: 0.4309 score: 0.9796 time: 0.06s
Test loss: 0.5025 score: 0.8980 time: 0.06s
Epoch 185/1000, LR 0.000250
Train loss: 0.8216;  Loss pred: 0.8216; Loss self: 0.0000; time: 0.19s
Val loss: 0.4265 score: 0.9796 time: 0.06s
Test loss: 0.4994 score: 0.8980 time: 0.06s
Epoch 186/1000, LR 0.000250
Train loss: 0.8184;  Loss pred: 0.8184; Loss self: 0.0000; time: 0.19s
Val loss: 0.4222 score: 0.9796 time: 0.06s
Test loss: 0.4963 score: 0.8980 time: 0.06s
Epoch 187/1000, LR 0.000249
Train loss: 0.8191;  Loss pred: 0.8191; Loss self: 0.0000; time: 0.19s
Val loss: 0.4178 score: 0.9796 time: 0.06s
Test loss: 0.4932 score: 0.8980 time: 0.06s
Epoch 188/1000, LR 0.000249
Train loss: 0.8135;  Loss pred: 0.8135; Loss self: 0.0000; time: 0.19s
Val loss: 0.4134 score: 0.9796 time: 0.06s
Test loss: 0.4901 score: 0.8980 time: 0.06s
Epoch 189/1000, LR 0.000249
Train loss: 0.8096;  Loss pred: 0.8096; Loss self: 0.0000; time: 0.19s
Val loss: 0.4091 score: 0.9796 time: 0.06s
Test loss: 0.4870 score: 0.8980 time: 0.06s
Epoch 190/1000, LR 0.000249
Train loss: 0.8059;  Loss pred: 0.8059; Loss self: 0.0000; time: 0.19s
Val loss: 0.4047 score: 0.9796 time: 0.07s
Test loss: 0.4839 score: 0.8980 time: 0.06s
Epoch 191/1000, LR 0.000249
Train loss: 0.8029;  Loss pred: 0.8029; Loss self: 0.0000; time: 0.19s
Val loss: 0.4003 score: 0.9796 time: 0.06s
Test loss: 0.4808 score: 0.8980 time: 0.06s
Epoch 192/1000, LR 0.000248
Train loss: 0.8019;  Loss pred: 0.8019; Loss self: 0.0000; time: 0.19s
Val loss: 0.3960 score: 0.9796 time: 0.06s
Test loss: 0.4776 score: 0.8980 time: 0.06s
Epoch 193/1000, LR 0.000248
Train loss: 0.7984;  Loss pred: 0.7984; Loss self: 0.0000; time: 0.19s
Val loss: 0.3916 score: 0.9796 time: 0.06s
Test loss: 0.4745 score: 0.8980 time: 0.06s
Epoch 194/1000, LR 0.000248
Train loss: 0.7953;  Loss pred: 0.7953; Loss self: 0.0000; time: 0.19s
Val loss: 0.3873 score: 0.9796 time: 0.06s
Test loss: 0.4715 score: 0.8980 time: 0.06s
Epoch 195/1000, LR 0.000248
Train loss: 0.7906;  Loss pred: 0.7906; Loss self: 0.0000; time: 0.19s
Val loss: 0.3829 score: 0.9796 time: 0.06s
Test loss: 0.4684 score: 0.8980 time: 0.06s
Epoch 196/1000, LR 0.000247
Train loss: 0.7900;  Loss pred: 0.7900; Loss self: 0.0000; time: 0.19s
Val loss: 0.3786 score: 0.9796 time: 0.06s
Test loss: 0.4653 score: 0.8980 time: 0.06s
Epoch 197/1000, LR 0.000247
Train loss: 0.7853;  Loss pred: 0.7853; Loss self: 0.0000; time: 0.19s
Val loss: 0.3743 score: 0.9796 time: 0.08s
Test loss: 0.4623 score: 0.8980 time: 0.06s
Epoch 198/1000, LR 0.000247
Train loss: 0.7818;  Loss pred: 0.7818; Loss self: 0.0000; time: 0.19s
Val loss: 0.3700 score: 0.9796 time: 0.07s
Test loss: 0.4593 score: 0.8980 time: 0.06s
Epoch 199/1000, LR 0.000247
Train loss: 0.7798;  Loss pred: 0.7798; Loss self: 0.0000; time: 0.21s
Val loss: 0.3657 score: 0.9796 time: 0.07s
Test loss: 0.4563 score: 0.8980 time: 0.06s
Epoch 200/1000, LR 0.000246
Train loss: 0.7756;  Loss pred: 0.7756; Loss self: 0.0000; time: 0.21s
Val loss: 0.3614 score: 0.9796 time: 0.07s
Test loss: 0.4532 score: 0.8980 time: 0.06s
Epoch 201/1000, LR 0.000246
Train loss: 0.7721;  Loss pred: 0.7721; Loss self: 0.0000; time: 0.20s
Val loss: 0.3572 score: 0.9796 time: 0.07s
Test loss: 0.4502 score: 0.8980 time: 0.06s
Epoch 202/1000, LR 0.000246
Train loss: 0.7714;  Loss pred: 0.7714; Loss self: 0.0000; time: 2.96s
Val loss: 0.3530 score: 0.9796 time: 1.57s
Test loss: 0.4471 score: 0.8980 time: 0.69s
Epoch 203/1000, LR 0.000246
Train loss: 0.7675;  Loss pred: 0.7675; Loss self: 0.0000; time: 3.30s
Val loss: 0.3487 score: 0.9796 time: 0.07s
Test loss: 0.4441 score: 0.8980 time: 0.06s
Epoch 204/1000, LR 0.000245
Train loss: 0.7645;  Loss pred: 0.7645; Loss self: 0.0000; time: 0.20s
Val loss: 0.3446 score: 0.9796 time: 0.06s
Test loss: 0.4411 score: 0.8980 time: 0.06s
Epoch 205/1000, LR 0.000245
Train loss: 0.7619;  Loss pred: 0.7619; Loss self: 0.0000; time: 0.22s
Val loss: 0.3404 score: 0.9796 time: 0.08s
Test loss: 0.4382 score: 0.8980 time: 0.06s
Epoch 206/1000, LR 0.000245
Train loss: 0.7586;  Loss pred: 0.7586; Loss self: 0.0000; time: 0.19s
Val loss: 0.3363 score: 0.9796 time: 0.06s
Test loss: 0.4353 score: 0.8980 time: 0.06s
Epoch 207/1000, LR 0.000245
Train loss: 0.7560;  Loss pred: 0.7560; Loss self: 0.0000; time: 0.20s
Val loss: 0.3323 score: 0.9796 time: 0.07s
Test loss: 0.4325 score: 0.8980 time: 0.06s
Epoch 208/1000, LR 0.000244
Train loss: 0.7524;  Loss pred: 0.7524; Loss self: 0.0000; time: 0.20s
Val loss: 0.3282 score: 0.9796 time: 0.06s
Test loss: 0.4297 score: 0.8980 time: 0.06s
Epoch 209/1000, LR 0.000244
Train loss: 0.7485;  Loss pred: 0.7485; Loss self: 0.0000; time: 0.19s
Val loss: 0.3242 score: 0.9796 time: 0.06s
Test loss: 0.4270 score: 0.8980 time: 0.06s
Epoch 210/1000, LR 0.000244
Train loss: 0.7477;  Loss pred: 0.7477; Loss self: 0.0000; time: 0.19s
Val loss: 0.3203 score: 0.9796 time: 0.07s
Test loss: 0.4243 score: 0.8980 time: 0.06s
Epoch 211/1000, LR 0.000244
Train loss: 0.7443;  Loss pred: 0.7443; Loss self: 0.0000; time: 0.19s
Val loss: 0.3163 score: 0.9796 time: 0.06s
Test loss: 0.4216 score: 0.8980 time: 0.06s
Epoch 212/1000, LR 0.000243
Train loss: 0.7418;  Loss pred: 0.7418; Loss self: 0.0000; time: 0.19s
Val loss: 0.3124 score: 0.9796 time: 0.06s
Test loss: 0.4189 score: 0.8980 time: 0.06s
Epoch 213/1000, LR 0.000243
Train loss: 0.7403;  Loss pred: 0.7403; Loss self: 0.0000; time: 0.19s
Val loss: 0.3085 score: 0.9796 time: 0.08s
Test loss: 0.4162 score: 0.8980 time: 2.17s
Epoch 214/1000, LR 0.000243
Train loss: 0.7363;  Loss pred: 0.7363; Loss self: 0.0000; time: 2.91s
Val loss: 0.3046 score: 0.9796 time: 0.27s
Test loss: 0.4135 score: 0.8980 time: 0.49s
Epoch 215/1000, LR 0.000243
Train loss: 0.7331;  Loss pred: 0.7331; Loss self: 0.0000; time: 1.29s
Val loss: 0.3008 score: 0.9796 time: 0.21s
Test loss: 0.4108 score: 0.8980 time: 0.23s
Epoch 216/1000, LR 0.000242
Train loss: 0.7308;  Loss pred: 0.7308; Loss self: 0.0000; time: 0.75s
Val loss: 0.2970 score: 0.9796 time: 0.07s
Test loss: 0.4082 score: 0.8980 time: 0.06s
Epoch 217/1000, LR 0.000242
Train loss: 0.7267;  Loss pred: 0.7267; Loss self: 0.0000; time: 0.21s
Val loss: 0.2931 score: 0.9796 time: 0.07s
Test loss: 0.4055 score: 0.8980 time: 0.06s
Epoch 218/1000, LR 0.000242
Train loss: 0.7260;  Loss pred: 0.7260; Loss self: 0.0000; time: 0.21s
Val loss: 0.2894 score: 0.9796 time: 0.07s
Test loss: 0.4028 score: 0.8980 time: 0.06s
Epoch 219/1000, LR 0.000242
Train loss: 0.7219;  Loss pred: 0.7219; Loss self: 0.0000; time: 0.20s
Val loss: 0.2856 score: 0.9796 time: 0.07s
Test loss: 0.4002 score: 0.8980 time: 0.06s
Epoch 220/1000, LR 0.000241
Train loss: 0.7195;  Loss pred: 0.7195; Loss self: 0.0000; time: 0.21s
Val loss: 0.2819 score: 0.9796 time: 0.07s
Test loss: 0.3976 score: 0.8980 time: 0.06s
Epoch 221/1000, LR 0.000241
Train loss: 0.7191;  Loss pred: 0.7191; Loss self: 0.0000; time: 0.20s
Val loss: 0.2781 score: 0.9796 time: 0.07s
Test loss: 0.3949 score: 0.8980 time: 0.06s
Epoch 222/1000, LR 0.000241
Train loss: 0.7160;  Loss pred: 0.7160; Loss self: 0.0000; time: 0.20s
Val loss: 0.2745 score: 0.9796 time: 0.07s
Test loss: 0.3924 score: 0.8980 time: 0.06s
Epoch 223/1000, LR 0.000241
Train loss: 0.7145;  Loss pred: 0.7145; Loss self: 0.0000; time: 0.21s
Val loss: 0.2710 score: 0.9796 time: 0.07s
Test loss: 0.3899 score: 0.8980 time: 0.06s
Epoch 224/1000, LR 0.000240
Train loss: 0.7096;  Loss pred: 0.7096; Loss self: 0.0000; time: 0.20s
Val loss: 0.2675 score: 0.9796 time: 0.07s
Test loss: 0.3876 score: 0.8980 time: 0.07s
Epoch 225/1000, LR 0.000240
Train loss: 0.7038;  Loss pred: 0.7038; Loss self: 0.0000; time: 0.20s
Val loss: 0.2641 score: 0.9796 time: 0.07s
Test loss: 0.3853 score: 0.8980 time: 0.06s
Epoch 226/1000, LR 0.000240
Train loss: 0.7064;  Loss pred: 0.7064; Loss self: 0.0000; time: 0.21s
Val loss: 0.2607 score: 0.9796 time: 1.48s
Test loss: 0.3831 score: 0.8980 time: 0.49s
Epoch 227/1000, LR 0.000240
Train loss: 0.7019;  Loss pred: 0.7019; Loss self: 0.0000; time: 2.62s
Val loss: 0.2574 score: 0.9796 time: 0.11s
Test loss: 0.3809 score: 0.8980 time: 0.27s
Epoch 228/1000, LR 0.000239
Train loss: 0.7020;  Loss pred: 0.7020; Loss self: 0.0000; time: 0.48s
Val loss: 0.2541 score: 0.9796 time: 0.11s
Test loss: 0.3787 score: 0.8980 time: 0.06s
Epoch 229/1000, LR 0.000239
Train loss: 0.6983;  Loss pred: 0.6983; Loss self: 0.0000; time: 0.21s
Val loss: 0.2509 score: 0.9796 time: 0.07s
Test loss: 0.3765 score: 0.8980 time: 0.06s
Epoch 230/1000, LR 0.000239
Train loss: 0.6954;  Loss pred: 0.6954; Loss self: 0.0000; time: 0.21s
Val loss: 0.2476 score: 0.9796 time: 0.07s
Test loss: 0.3742 score: 0.8980 time: 0.06s
Epoch 231/1000, LR 0.000238
Train loss: 0.6958;  Loss pred: 0.6958; Loss self: 0.0000; time: 0.21s
Val loss: 0.2444 score: 0.9796 time: 0.07s
Test loss: 0.3719 score: 0.8980 time: 0.06s
Epoch 232/1000, LR 0.000238
Train loss: 0.6904;  Loss pred: 0.6904; Loss self: 0.0000; time: 0.21s
Val loss: 0.2412 score: 0.9796 time: 0.07s
Test loss: 0.3697 score: 0.8980 time: 0.06s
Epoch 233/1000, LR 0.000238
Train loss: 0.6882;  Loss pred: 0.6882; Loss self: 0.0000; time: 0.20s
Val loss: 0.2381 score: 0.9796 time: 0.07s
Test loss: 0.3675 score: 0.8980 time: 0.06s
Epoch 234/1000, LR 0.000238
Train loss: 0.6884;  Loss pred: 0.6884; Loss self: 0.0000; time: 0.20s
Val loss: 0.2351 score: 0.9796 time: 0.07s
Test loss: 0.3653 score: 0.8980 time: 0.06s
Epoch 235/1000, LR 0.000237
Train loss: 0.6849;  Loss pred: 0.6849; Loss self: 0.0000; time: 0.20s
Val loss: 0.2320 score: 0.9796 time: 0.07s
Test loss: 0.3631 score: 0.8980 time: 0.06s
Epoch 236/1000, LR 0.000237
Train loss: 0.6852;  Loss pred: 0.6852; Loss self: 0.0000; time: 0.20s
Val loss: 0.2290 score: 0.9796 time: 0.07s
Test loss: 0.3609 score: 0.8980 time: 0.06s
Epoch 237/1000, LR 0.000237
Train loss: 0.6837;  Loss pred: 0.6837; Loss self: 0.0000; time: 0.21s
Val loss: 0.2261 score: 0.9796 time: 0.07s
Test loss: 0.3588 score: 0.8980 time: 0.06s
Epoch 238/1000, LR 0.000236
Train loss: 0.6792;  Loss pred: 0.6792; Loss self: 0.0000; time: 0.20s
Val loss: 0.2232 score: 0.9796 time: 0.07s
Test loss: 0.3567 score: 0.8980 time: 0.06s
Epoch 239/1000, LR 0.000236
Train loss: 0.6774;  Loss pred: 0.6774; Loss self: 0.0000; time: 0.21s
Val loss: 0.2204 score: 0.9796 time: 0.07s
Test loss: 0.3546 score: 0.8980 time: 0.06s
Epoch 240/1000, LR 0.000236
Train loss: 0.6758;  Loss pred: 0.6758; Loss self: 0.0000; time: 0.21s
Val loss: 0.2176 score: 0.9796 time: 0.07s
Test loss: 0.3526 score: 0.8980 time: 0.07s
Epoch 241/1000, LR 0.000236
Train loss: 0.6735;  Loss pred: 0.6735; Loss self: 0.0000; time: 5.23s
Val loss: 0.2149 score: 0.9796 time: 0.35s
Test loss: 0.3507 score: 0.8980 time: 0.20s
Epoch 242/1000, LR 0.000235
Train loss: 0.6702;  Loss pred: 0.6702; Loss self: 0.0000; time: 0.67s
Val loss: 0.2122 score: 0.9796 time: 0.42s
Test loss: 0.3489 score: 0.8980 time: 0.39s
Epoch 243/1000, LR 0.000235
Train loss: 0.6680;  Loss pred: 0.6680; Loss self: 0.0000; time: 0.27s
Val loss: 0.2096 score: 0.9796 time: 0.08s
Test loss: 0.3471 score: 0.8980 time: 0.13s
Epoch 244/1000, LR 0.000235
Train loss: 0.6684;  Loss pred: 0.6684; Loss self: 0.0000; time: 0.41s
Val loss: 0.2071 score: 0.9796 time: 0.07s
Test loss: 0.3454 score: 0.8980 time: 0.06s
Epoch 245/1000, LR 0.000234
Train loss: 0.6652;  Loss pred: 0.6652; Loss self: 0.0000; time: 0.21s
Val loss: 0.2045 score: 0.9796 time: 0.07s
Test loss: 0.3436 score: 0.8980 time: 0.06s
Epoch 246/1000, LR 0.000234
Train loss: 0.6632;  Loss pred: 0.6632; Loss self: 0.0000; time: 0.20s
Val loss: 0.2020 score: 0.9796 time: 0.07s
Test loss: 0.3419 score: 0.8980 time: 0.06s
Epoch 247/1000, LR 0.000234
Train loss: 0.6593;  Loss pred: 0.6593; Loss self: 0.0000; time: 0.20s
Val loss: 0.1996 score: 0.9796 time: 0.07s
Test loss: 0.3401 score: 0.8980 time: 0.06s
Epoch 248/1000, LR 0.000234
Train loss: 0.6626;  Loss pred: 0.6626; Loss self: 0.0000; time: 0.20s
Val loss: 0.1972 score: 0.9796 time: 0.07s
Test loss: 0.3385 score: 0.8980 time: 0.06s
Epoch 249/1000, LR 0.000233
Train loss: 0.6541;  Loss pred: 0.6541; Loss self: 0.0000; time: 0.20s
Val loss: 0.1948 score: 0.9796 time: 0.07s
Test loss: 0.3368 score: 0.8980 time: 0.06s
Epoch 250/1000, LR 0.000233
Train loss: 0.6560;  Loss pred: 0.6560; Loss self: 0.0000; time: 0.20s
Val loss: 0.1925 score: 0.9796 time: 0.07s
Test loss: 0.3352 score: 0.8980 time: 0.06s
Epoch 251/1000, LR 0.000233
Train loss: 0.6542;  Loss pred: 0.6542; Loss self: 0.0000; time: 0.19s
Val loss: 0.1902 score: 0.9796 time: 0.06s
Test loss: 0.3337 score: 0.8980 time: 0.06s
Epoch 252/1000, LR 0.000232
Train loss: 0.6527;  Loss pred: 0.6527; Loss self: 0.0000; time: 0.19s
Val loss: 0.1880 score: 0.9796 time: 0.07s
Test loss: 0.3322 score: 0.8980 time: 0.06s
Epoch 253/1000, LR 0.000232
Train loss: 0.6525;  Loss pred: 0.6525; Loss self: 0.0000; time: 0.20s
Val loss: 0.1858 score: 0.9796 time: 0.07s
Test loss: 0.3306 score: 0.8980 time: 0.06s
Epoch 254/1000, LR 0.000232
Train loss: 0.6501;  Loss pred: 0.6501; Loss self: 0.0000; time: 0.20s
Val loss: 0.1836 score: 0.9796 time: 0.07s
Test loss: 0.3290 score: 0.8980 time: 0.06s
Epoch 255/1000, LR 0.000232
Train loss: 0.6481;  Loss pred: 0.6481; Loss self: 0.0000; time: 0.20s
Val loss: 0.1815 score: 0.9796 time: 0.07s
Test loss: 0.3274 score: 0.8980 time: 0.06s
Epoch 256/1000, LR 0.000231
Train loss: 0.6469;  Loss pred: 0.6469; Loss self: 0.0000; time: 0.21s
Val loss: 0.1793 score: 0.9796 time: 0.07s
Test loss: 0.3258 score: 0.8980 time: 0.06s
Epoch 257/1000, LR 0.000231
Train loss: 0.6432;  Loss pred: 0.6432; Loss self: 0.0000; time: 0.21s
Val loss: 0.1773 score: 0.9796 time: 0.07s
Test loss: 0.3243 score: 0.8980 time: 0.06s
Epoch 258/1000, LR 0.000231
Train loss: 0.6435;  Loss pred: 0.6435; Loss self: 0.0000; time: 4.25s
Val loss: 0.1752 score: 0.9796 time: 0.14s
Test loss: 0.3228 score: 0.8980 time: 0.37s
Epoch 259/1000, LR 0.000230
Train loss: 0.6409;  Loss pred: 0.6409; Loss self: 0.0000; time: 1.79s
Val loss: 0.1732 score: 0.9796 time: 0.11s
Test loss: 0.3214 score: 0.8980 time: 0.18s
Epoch 260/1000, LR 0.000230
Train loss: 0.6372;  Loss pred: 0.6372; Loss self: 0.0000; time: 0.57s
Val loss: 0.1713 score: 0.9796 time: 0.10s
Test loss: 0.3201 score: 0.8980 time: 0.07s
Epoch 261/1000, LR 0.000230
Train loss: 0.6373;  Loss pred: 0.6373; Loss self: 0.0000; time: 0.22s
Val loss: 0.1694 score: 0.9796 time: 0.07s
Test loss: 0.3189 score: 0.8980 time: 0.07s
Epoch 262/1000, LR 0.000229
Train loss: 0.6376;  Loss pred: 0.6376; Loss self: 0.0000; time: 0.22s
Val loss: 0.1675 score: 0.9796 time: 0.07s
Test loss: 0.3176 score: 0.8980 time: 0.06s
Epoch 263/1000, LR 0.000229
Train loss: 0.6323;  Loss pred: 0.6323; Loss self: 0.0000; time: 0.20s
Val loss: 0.1657 score: 0.9796 time: 0.07s
Test loss: 0.3163 score: 0.8980 time: 0.06s
Epoch 264/1000, LR 0.000229
Train loss: 0.6349;  Loss pred: 0.6349; Loss self: 0.0000; time: 0.20s
Val loss: 0.1638 score: 0.9796 time: 0.08s
Test loss: 0.3151 score: 0.8980 time: 0.07s
Epoch 265/1000, LR 0.000228
Train loss: 0.6324;  Loss pred: 0.6324; Loss self: 0.0000; time: 0.20s
Val loss: 0.1620 score: 0.9796 time: 0.07s
Test loss: 0.3137 score: 0.8980 time: 0.06s
Epoch 266/1000, LR 0.000228
Train loss: 0.6296;  Loss pred: 0.6296; Loss self: 0.0000; time: 0.20s
Val loss: 0.1602 score: 0.9796 time: 0.07s
Test loss: 0.3124 score: 0.8980 time: 0.06s
Epoch 267/1000, LR 0.000228
Train loss: 0.6301;  Loss pred: 0.6301; Loss self: 0.0000; time: 0.19s
Val loss: 0.1584 score: 0.9796 time: 0.18s
Test loss: 0.3110 score: 0.8980 time: 0.56s
Epoch 268/1000, LR 0.000228
Train loss: 0.6263;  Loss pred: 0.6263; Loss self: 0.0000; time: 4.43s
Val loss: 0.1567 score: 0.9796 time: 0.09s
Test loss: 0.3099 score: 0.8980 time: 0.07s
Epoch 269/1000, LR 0.000227
Train loss: 0.6262;  Loss pred: 0.6262; Loss self: 0.0000; time: 0.21s
Val loss: 0.1550 score: 0.9796 time: 0.07s
Test loss: 0.3087 score: 0.8980 time: 0.06s
Epoch 270/1000, LR 0.000227
Train loss: 0.6242;  Loss pred: 0.6242; Loss self: 0.0000; time: 0.20s
Val loss: 0.1533 score: 0.9796 time: 0.07s
Test loss: 0.3076 score: 0.8980 time: 0.06s
Epoch 271/1000, LR 0.000227
Train loss: 0.6235;  Loss pred: 0.6235; Loss self: 0.0000; time: 0.29s
Val loss: 0.1516 score: 0.9796 time: 0.06s
Test loss: 0.3063 score: 0.8980 time: 0.06s
Epoch 272/1000, LR 0.000226
Train loss: 0.6228;  Loss pred: 0.6228; Loss self: 0.0000; time: 0.19s
Val loss: 0.1500 score: 0.9796 time: 0.06s
Test loss: 0.3051 score: 0.8980 time: 0.06s
Epoch 273/1000, LR 0.000226
Train loss: 0.6226;  Loss pred: 0.6226; Loss self: 0.0000; time: 0.19s
Val loss: 0.1484 score: 0.9796 time: 0.06s
Test loss: 0.3038 score: 0.8980 time: 0.06s
Epoch 274/1000, LR 0.000226
Train loss: 0.6211;  Loss pred: 0.6211; Loss self: 0.0000; time: 0.21s
Val loss: 0.1468 score: 0.9796 time: 1.67s
Test loss: 0.3025 score: 0.8980 time: 0.78s
Epoch 275/1000, LR 0.000225
Train loss: 0.6224;  Loss pred: 0.6224; Loss self: 0.0000; time: 4.26s
Val loss: 0.1453 score: 0.9796 time: 0.46s
Test loss: 0.3014 score: 0.8980 time: 0.54s
Epoch 276/1000, LR 0.000225
Train loss: 0.6175;  Loss pred: 0.6175; Loss self: 0.0000; time: 0.97s
Val loss: 0.1437 score: 0.9796 time: 0.33s
Test loss: 0.3003 score: 0.8980 time: 0.16s
Epoch 277/1000, LR 0.000225
Train loss: 0.6133;  Loss pred: 0.6133; Loss self: 0.0000; time: 0.77s
Val loss: 0.1423 score: 0.9796 time: 0.07s
Test loss: 0.2993 score: 0.8980 time: 0.06s
Epoch 278/1000, LR 0.000224
Train loss: 0.6198;  Loss pred: 0.6198; Loss self: 0.0000; time: 0.22s
Val loss: 0.1408 score: 0.9796 time: 0.07s
Test loss: 0.2983 score: 0.8980 time: 0.06s
Epoch 279/1000, LR 0.000224
Train loss: 0.6137;  Loss pred: 0.6137; Loss self: 0.0000; time: 0.21s
Val loss: 0.1394 score: 0.9796 time: 0.07s
Test loss: 0.2974 score: 0.8980 time: 0.06s
Epoch 280/1000, LR 0.000224
Train loss: 0.6115;  Loss pred: 0.6115; Loss self: 0.0000; time: 0.20s
Val loss: 0.1381 score: 0.9796 time: 0.07s
Test loss: 0.2966 score: 0.8980 time: 0.06s
Epoch 281/1000, LR 0.000223
Train loss: 0.6120;  Loss pred: 0.6120; Loss self: 0.0000; time: 0.21s
Val loss: 0.1367 score: 0.9796 time: 0.07s
Test loss: 0.2958 score: 0.8980 time: 0.06s
Epoch 282/1000, LR 0.000223
Train loss: 0.6095;  Loss pred: 0.6095; Loss self: 0.0000; time: 0.20s
Val loss: 0.1354 score: 0.9796 time: 0.07s
Test loss: 0.2950 score: 0.8980 time: 0.06s
Epoch 283/1000, LR 0.000223
Train loss: 0.6087;  Loss pred: 0.6087; Loss self: 0.0000; time: 0.20s
Val loss: 0.1341 score: 0.9796 time: 0.07s
Test loss: 0.2942 score: 0.8980 time: 0.06s
Epoch 284/1000, LR 0.000222
Train loss: 0.6088;  Loss pred: 0.6088; Loss self: 0.0000; time: 0.20s
Val loss: 0.1328 score: 0.9796 time: 0.07s
Test loss: 0.2934 score: 0.8980 time: 0.06s
Epoch 285/1000, LR 0.000222
Train loss: 0.6078;  Loss pred: 0.6078; Loss self: 0.0000; time: 0.20s
Val loss: 0.1315 score: 0.9796 time: 0.07s
Test loss: 0.2924 score: 0.8980 time: 0.06s
Epoch 286/1000, LR 0.000222
Train loss: 0.6059;  Loss pred: 0.6059; Loss self: 0.0000; time: 0.20s
Val loss: 0.1302 score: 0.9796 time: 0.08s
Test loss: 0.2913 score: 0.8980 time: 0.06s
Epoch 287/1000, LR 0.000221
Train loss: 0.6059;  Loss pred: 0.6059; Loss self: 0.0000; time: 0.20s
Val loss: 0.1289 score: 0.9796 time: 0.07s
Test loss: 0.2901 score: 0.8980 time: 0.06s
Epoch 288/1000, LR 0.000221
Train loss: 0.6061;  Loss pred: 0.6061; Loss self: 0.0000; time: 0.20s
Val loss: 0.1277 score: 0.9796 time: 0.07s
Test loss: 0.2890 score: 0.8980 time: 0.06s
Epoch 289/1000, LR 0.000221
Train loss: 0.6071;  Loss pred: 0.6071; Loss self: 0.0000; time: 0.20s
Val loss: 0.1264 score: 0.9796 time: 0.07s
Test loss: 0.2879 score: 0.8980 time: 0.06s
Epoch 290/1000, LR 0.000220
Train loss: 0.6031;  Loss pred: 0.6031; Loss self: 0.0000; time: 0.20s
Val loss: 0.1252 score: 0.9796 time: 0.07s
Test loss: 0.2870 score: 0.8980 time: 0.06s
Epoch 291/1000, LR 0.000220
Train loss: 0.6020;  Loss pred: 0.6020; Loss self: 0.0000; time: 0.20s
Val loss: 0.1241 score: 0.9796 time: 0.07s
Test loss: 0.2861 score: 0.8980 time: 0.06s
Epoch 292/1000, LR 0.000220
Train loss: 0.6028;  Loss pred: 0.6028; Loss self: 0.0000; time: 0.20s
Val loss: 0.1230 score: 0.9796 time: 0.07s
Test loss: 0.2853 score: 0.8980 time: 0.06s
Epoch 293/1000, LR 0.000219
Train loss: 0.6011;  Loss pred: 0.6011; Loss self: 0.0000; time: 0.20s
Val loss: 0.1218 score: 0.9796 time: 0.07s
Test loss: 0.2845 score: 0.8980 time: 0.06s
Epoch 294/1000, LR 0.000219
Train loss: 0.6008;  Loss pred: 0.6008; Loss self: 0.0000; time: 0.20s
Val loss: 0.1208 score: 0.9796 time: 0.07s
Test loss: 0.2838 score: 0.8980 time: 0.06s
Epoch 295/1000, LR 0.000219
Train loss: 0.5974;  Loss pred: 0.5974; Loss self: 0.0000; time: 3.58s
Val loss: 0.1197 score: 0.9796 time: 0.40s
Test loss: 0.2831 score: 0.8980 time: 0.70s
Epoch 296/1000, LR 0.000218
Train loss: 0.6011;  Loss pred: 0.6011; Loss self: 0.0000; time: 2.21s
Val loss: 0.1187 score: 0.9796 time: 0.35s
Test loss: 0.2825 score: 0.8980 time: 0.15s
Epoch 297/1000, LR 0.000218
Train loss: 0.5980;  Loss pred: 0.5980; Loss self: 0.0000; time: 0.26s
Val loss: 0.1177 score: 0.9796 time: 0.07s
Test loss: 0.2818 score: 0.8980 time: 0.06s
Epoch 298/1000, LR 0.000218
Train loss: 0.5986;  Loss pred: 0.5986; Loss self: 0.0000; time: 0.21s
Val loss: 0.1166 score: 0.9796 time: 0.07s
Test loss: 0.2810 score: 0.8980 time: 0.06s
Epoch 299/1000, LR 0.000217
Train loss: 0.5944;  Loss pred: 0.5944; Loss self: 0.0000; time: 0.21s
Val loss: 0.1156 score: 0.9796 time: 0.08s
Test loss: 0.2801 score: 0.8980 time: 0.07s
Epoch 300/1000, LR 0.000217
Train loss: 0.5956;  Loss pred: 0.5956; Loss self: 0.0000; time: 0.21s
Val loss: 0.1146 score: 0.9796 time: 0.07s
Test loss: 0.2793 score: 0.8980 time: 0.06s
Epoch 301/1000, LR 0.000217
Train loss: 0.5936;  Loss pred: 0.5936; Loss self: 0.0000; time: 0.20s
Val loss: 0.1136 score: 0.9796 time: 0.07s
Test loss: 0.2785 score: 0.8980 time: 0.06s
Epoch 302/1000, LR 0.000216
Train loss: 0.5915;  Loss pred: 0.5915; Loss self: 0.0000; time: 0.20s
Val loss: 0.1127 score: 0.9796 time: 0.07s
Test loss: 0.2779 score: 0.8980 time: 0.06s
Epoch 303/1000, LR 0.000216
Train loss: 0.5923;  Loss pred: 0.5923; Loss self: 0.0000; time: 0.20s
Val loss: 0.1118 score: 0.9796 time: 0.07s
Test loss: 0.2773 score: 0.8980 time: 0.06s
Epoch 304/1000, LR 0.000216
Train loss: 0.5904;  Loss pred: 0.5904; Loss self: 0.0000; time: 0.21s
Val loss: 0.1109 score: 0.9796 time: 0.07s
Test loss: 0.2765 score: 0.8980 time: 0.06s
Epoch 305/1000, LR 0.000215
Train loss: 0.5887;  Loss pred: 0.5887; Loss self: 0.0000; time: 0.20s
Val loss: 0.1100 score: 0.9796 time: 0.07s
Test loss: 0.2758 score: 0.8980 time: 0.06s
Epoch 306/1000, LR 0.000215
Train loss: 0.5924;  Loss pred: 0.5924; Loss self: 0.0000; time: 0.20s
Val loss: 0.1091 score: 0.9796 time: 0.06s
Test loss: 0.2752 score: 0.8980 time: 0.06s
Epoch 307/1000, LR 0.000215
Train loss: 0.5889;  Loss pred: 0.5889; Loss self: 0.0000; time: 0.20s
Val loss: 0.1082 score: 0.9796 time: 0.07s
Test loss: 0.2745 score: 0.8980 time: 0.06s
Epoch 308/1000, LR 0.000214
Train loss: 0.5842;  Loss pred: 0.5842; Loss self: 0.0000; time: 0.20s
Val loss: 0.1073 score: 0.9796 time: 0.07s
Test loss: 0.2739 score: 0.8980 time: 0.06s
Epoch 309/1000, LR 0.000214
Train loss: 0.5874;  Loss pred: 0.5874; Loss self: 0.0000; time: 0.20s
Val loss: 0.1065 score: 0.9796 time: 0.07s
Test loss: 0.2733 score: 0.8980 time: 0.06s
Epoch 310/1000, LR 0.000214
Train loss: 0.5831;  Loss pred: 0.5831; Loss self: 0.0000; time: 0.20s
Val loss: 0.1057 score: 0.9796 time: 0.07s
Test loss: 0.2729 score: 0.8980 time: 0.06s
Epoch 311/1000, LR 0.000213
Train loss: 0.5851;  Loss pred: 0.5851; Loss self: 0.0000; time: 0.20s
Val loss: 0.1049 score: 0.9796 time: 0.06s
Test loss: 0.2724 score: 0.8980 time: 0.06s
Epoch 312/1000, LR 0.000213
Train loss: 0.5816;  Loss pred: 0.5816; Loss self: 0.0000; time: 0.20s
Val loss: 0.1041 score: 0.9796 time: 0.07s
Test loss: 0.2720 score: 0.8980 time: 0.06s
Epoch 313/1000, LR 0.000213
Train loss: 0.5826;  Loss pred: 0.5826; Loss self: 0.0000; time: 0.20s
Val loss: 0.1034 score: 0.9796 time: 0.07s
Test loss: 0.2714 score: 0.8980 time: 0.06s
Epoch 314/1000, LR 0.000212
Train loss: 0.5817;  Loss pred: 0.5817; Loss self: 0.0000; time: 0.20s
Val loss: 0.1026 score: 0.9796 time: 0.07s
Test loss: 0.2708 score: 0.8980 time: 0.06s
Epoch 315/1000, LR 0.000212
Train loss: 0.5817;  Loss pred: 0.5817; Loss self: 0.0000; time: 0.20s
Val loss: 0.1018 score: 0.9796 time: 0.07s
Test loss: 0.2702 score: 0.8980 time: 0.06s
Epoch 316/1000, LR 0.000212
Train loss: 0.5816;  Loss pred: 0.5816; Loss self: 0.0000; time: 0.20s
Val loss: 0.1010 score: 0.9796 time: 0.07s
Test loss: 0.2695 score: 0.8980 time: 0.06s
Epoch 317/1000, LR 0.000211
Train loss: 0.5805;  Loss pred: 0.5805; Loss self: 0.0000; time: 0.20s
Val loss: 0.1002 score: 0.9796 time: 0.07s
Test loss: 0.2687 score: 0.8980 time: 0.07s
Epoch 318/1000, LR 0.000211
Train loss: 0.5801;  Loss pred: 0.5801; Loss self: 0.0000; time: 0.23s
Val loss: 0.0994 score: 0.9796 time: 1.32s
Test loss: 0.2681 score: 0.8980 time: 0.93s
Epoch 319/1000, LR 0.000210
Train loss: 0.5783;  Loss pred: 0.5783; Loss self: 0.0000; time: 3.35s
Val loss: 0.0986 score: 0.9796 time: 0.72s
Test loss: 0.2674 score: 0.8980 time: 0.52s
Epoch 320/1000, LR 0.000210
Train loss: 0.5790;  Loss pred: 0.5790; Loss self: 0.0000; time: 0.60s
Val loss: 0.0979 score: 0.9796 time: 0.16s
Test loss: 0.2667 score: 0.8980 time: 0.24s
Epoch 321/1000, LR 0.000210
Train loss: 0.5770;  Loss pred: 0.5770; Loss self: 0.0000; time: 1.23s
Val loss: 0.0972 score: 0.9796 time: 0.15s
Test loss: 0.2663 score: 0.8980 time: 0.14s
Epoch 322/1000, LR 0.000209
Train loss: 0.5778;  Loss pred: 0.5778; Loss self: 0.0000; time: 0.39s
Val loss: 0.0965 score: 0.9796 time: 0.06s
Test loss: 0.2658 score: 0.8980 time: 0.06s
Epoch 323/1000, LR 0.000209
Train loss: 0.5760;  Loss pred: 0.5760; Loss self: 0.0000; time: 0.19s
Val loss: 0.0959 score: 0.9796 time: 0.06s
Test loss: 0.2655 score: 0.8980 time: 0.06s
Epoch 324/1000, LR 0.000209
Train loss: 0.5740;  Loss pred: 0.5740; Loss self: 0.0000; time: 0.19s
Val loss: 0.0952 score: 0.9796 time: 0.06s
Test loss: 0.2652 score: 0.8980 time: 0.06s
Epoch 325/1000, LR 0.000208
Train loss: 0.5744;  Loss pred: 0.5744; Loss self: 0.0000; time: 0.19s
Val loss: 0.0946 score: 0.9796 time: 0.06s
Test loss: 0.2649 score: 0.8980 time: 0.06s
Epoch 326/1000, LR 0.000208
Train loss: 0.5742;  Loss pred: 0.5742; Loss self: 0.0000; time: 0.19s
Val loss: 0.0940 score: 0.9796 time: 0.06s
Test loss: 0.2647 score: 0.8980 time: 0.06s
Epoch 327/1000, LR 0.000208
Train loss: 0.5736;  Loss pred: 0.5736; Loss self: 0.0000; time: 0.19s
Val loss: 0.0933 score: 0.9796 time: 0.06s
Test loss: 0.2643 score: 0.8980 time: 0.06s
Epoch 328/1000, LR 0.000207
Train loss: 0.5725;  Loss pred: 0.5725; Loss self: 0.0000; time: 0.20s
Val loss: 0.0927 score: 0.9796 time: 0.07s
Test loss: 0.2640 score: 0.8980 time: 0.06s
Epoch 329/1000, LR 0.000207
Train loss: 0.5709;  Loss pred: 0.5709; Loss self: 0.0000; time: 0.19s
Val loss: 0.0921 score: 0.9796 time: 0.06s
Test loss: 0.2635 score: 0.8980 time: 0.07s
Epoch 330/1000, LR 0.000207
Train loss: 0.5722;  Loss pred: 0.5722; Loss self: 0.0000; time: 0.26s
Val loss: 0.0915 score: 0.9796 time: 0.07s
Test loss: 0.2630 score: 0.8980 time: 0.06s
Epoch 331/1000, LR 0.000206
Train loss: 0.5721;  Loss pred: 0.5721; Loss self: 0.0000; time: 0.19s
Val loss: 0.0908 score: 0.9796 time: 1.92s
Test loss: 0.2624 score: 0.8980 time: 1.37s
Epoch 332/1000, LR 0.000206
Train loss: 0.5721;  Loss pred: 0.5721; Loss self: 0.0000; time: 2.74s
Val loss: 0.0902 score: 0.9796 time: 1.04s
Test loss: 0.2618 score: 0.8980 time: 0.51s
Epoch 333/1000, LR 0.000205
Train loss: 0.5711;  Loss pred: 0.5711; Loss self: 0.0000; time: 0.78s
Val loss: 0.0895 score: 0.9796 time: 0.16s
Test loss: 0.2611 score: 0.8980 time: 0.21s
Epoch 334/1000, LR 0.000205
Train loss: 0.5681;  Loss pred: 0.5681; Loss self: 0.0000; time: 0.39s
Val loss: 0.0889 score: 0.9796 time: 0.07s
Test loss: 0.2604 score: 0.8980 time: 0.06s
Epoch 335/1000, LR 0.000205
Train loss: 0.5700;  Loss pred: 0.5700; Loss self: 0.0000; time: 0.20s
Val loss: 0.0883 score: 0.9796 time: 0.07s
Test loss: 0.2598 score: 0.8980 time: 0.06s
Epoch 336/1000, LR 0.000204
Train loss: 0.5686;  Loss pred: 0.5686; Loss self: 0.0000; time: 0.20s
Val loss: 0.0877 score: 0.9796 time: 0.07s
Test loss: 0.2593 score: 0.8980 time: 0.06s
Epoch 337/1000, LR 0.000204
Train loss: 0.5674;  Loss pred: 0.5674; Loss self: 0.0000; time: 0.19s
Val loss: 0.0872 score: 0.9796 time: 0.08s
Test loss: 0.2589 score: 0.8980 time: 0.06s
Epoch 338/1000, LR 0.000204
Train loss: 0.5667;  Loss pred: 0.5667; Loss self: 0.0000; time: 0.21s
Val loss: 0.0866 score: 0.9796 time: 0.07s
Test loss: 0.2586 score: 0.8980 time: 0.07s
Epoch 339/1000, LR 0.000203
Train loss: 0.5672;  Loss pred: 0.5672; Loss self: 0.0000; time: 0.21s
Val loss: 0.0861 score: 0.9796 time: 0.07s
Test loss: 0.2585 score: 0.8980 time: 0.06s
Epoch 340/1000, LR 0.000203
Train loss: 0.5665;  Loss pred: 0.5665; Loss self: 0.0000; time: 0.21s
Val loss: 0.0856 score: 0.9796 time: 0.07s
Test loss: 0.2583 score: 0.8980 time: 0.06s
Epoch 341/1000, LR 0.000203
Train loss: 0.5637;  Loss pred: 0.5637; Loss self: 0.0000; time: 0.21s
Val loss: 0.0851 score: 0.9796 time: 0.07s
Test loss: 0.2581 score: 0.8980 time: 0.06s
Epoch 342/1000, LR 0.000202
Train loss: 0.5659;  Loss pred: 0.5659; Loss self: 0.0000; time: 0.20s
Val loss: 0.0846 score: 0.9796 time: 0.07s
Test loss: 0.2578 score: 0.8776 time: 0.06s
Epoch 343/1000, LR 0.000202
Train loss: 0.5676;  Loss pred: 0.5676; Loss self: 0.0000; time: 0.21s
Val loss: 0.0841 score: 0.9796 time: 0.07s
Test loss: 0.2575 score: 0.8776 time: 0.06s
Epoch 344/1000, LR 0.000201
Train loss: 0.5663;  Loss pred: 0.5663; Loss self: 0.0000; time: 5.17s
Val loss: 0.0836 score: 0.9796 time: 0.21s
Test loss: 0.2571 score: 0.8776 time: 0.48s
Epoch 345/1000, LR 0.000201
Train loss: 0.5649;  Loss pred: 0.5649; Loss self: 0.0000; time: 1.25s
Val loss: 0.0831 score: 0.9796 time: 0.64s
Test loss: 0.2566 score: 0.8776 time: 0.07s
Epoch 346/1000, LR 0.000201
Train loss: 0.5621;  Loss pred: 0.5621; Loss self: 0.0000; time: 0.24s
Val loss: 0.0826 score: 0.9796 time: 0.07s
Test loss: 0.2564 score: 0.8980 time: 0.06s
Epoch 347/1000, LR 0.000200
Train loss: 0.5624;  Loss pred: 0.5624; Loss self: 0.0000; time: 0.20s
Val loss: 0.0821 score: 0.9796 time: 0.07s
Test loss: 0.2560 score: 0.8980 time: 0.06s
Epoch 348/1000, LR 0.000200
Train loss: 0.5637;  Loss pred: 0.5637; Loss self: 0.0000; time: 0.20s
Val loss: 0.0816 score: 0.9796 time: 0.06s
Test loss: 0.2556 score: 0.8980 time: 0.06s
Epoch 349/1000, LR 0.000200
Train loss: 0.5608;  Loss pred: 0.5608; Loss self: 0.0000; time: 0.20s
Val loss: 0.0812 score: 0.9796 time: 0.07s
Test loss: 0.2552 score: 0.8980 time: 0.06s
Epoch 350/1000, LR 0.000199
Train loss: 0.5626;  Loss pred: 0.5626; Loss self: 0.0000; time: 0.20s
Val loss: 0.0807 score: 0.9796 time: 0.07s
Test loss: 0.2548 score: 0.8776 time: 0.06s
Epoch 351/1000, LR 0.000199
Train loss: 0.5622;  Loss pred: 0.5622; Loss self: 0.0000; time: 0.20s
Val loss: 0.0802 score: 0.9796 time: 0.06s
Test loss: 0.2543 score: 0.8776 time: 0.06s
Epoch 352/1000, LR 0.000198
Train loss: 0.5615;  Loss pred: 0.5615; Loss self: 0.0000; time: 0.19s
Val loss: 0.0797 score: 0.9796 time: 0.06s
Test loss: 0.2539 score: 0.8980 time: 0.06s
Epoch 353/1000, LR 0.000198
Train loss: 0.5604;  Loss pred: 0.5604; Loss self: 0.0000; time: 0.20s
Val loss: 0.0792 score: 0.9796 time: 0.07s
Test loss: 0.2534 score: 0.8980 time: 0.07s
Epoch 354/1000, LR 0.000198
Train loss: 0.5606;  Loss pred: 0.5606; Loss self: 0.0000; time: 0.20s
Val loss: 0.0788 score: 0.9796 time: 0.07s
Test loss: 0.2530 score: 0.8980 time: 0.06s
Epoch 355/1000, LR 0.000197
Train loss: 0.5606;  Loss pred: 0.5606; Loss self: 0.0000; time: 0.19s
Val loss: 0.0784 score: 0.9796 time: 0.06s
Test loss: 0.2528 score: 0.8980 time: 0.06s
Epoch 356/1000, LR 0.000197
Train loss: 0.5600;  Loss pred: 0.5600; Loss self: 0.0000; time: 0.19s
Val loss: 0.0780 score: 0.9796 time: 0.06s
Test loss: 0.2526 score: 0.8776 time: 0.06s
Epoch 357/1000, LR 0.000196
Train loss: 0.5575;  Loss pred: 0.5575; Loss self: 0.0000; time: 0.19s
Val loss: 0.0776 score: 0.9796 time: 0.06s
Test loss: 0.2525 score: 0.8980 time: 0.06s
Epoch 358/1000, LR 0.000196
Train loss: 0.5575;  Loss pred: 0.5575; Loss self: 0.0000; time: 0.19s
Val loss: 0.0772 score: 0.9796 time: 0.06s
Test loss: 0.2525 score: 0.8980 time: 0.06s
Epoch 359/1000, LR 0.000196
Train loss: 0.5579;  Loss pred: 0.5579; Loss self: 0.0000; time: 0.19s
Val loss: 0.0768 score: 0.9796 time: 0.08s
Test loss: 0.2522 score: 0.8980 time: 0.85s
Epoch 360/1000, LR 0.000195
Train loss: 0.5572;  Loss pred: 0.5572; Loss self: 0.0000; time: 3.65s
Val loss: 0.0764 score: 0.9796 time: 0.23s
Test loss: 0.2519 score: 0.8980 time: 0.36s
Epoch 361/1000, LR 0.000195
Train loss: 0.5585;  Loss pred: 0.5585; Loss self: 0.0000; time: 1.76s
Val loss: 0.0760 score: 0.9796 time: 0.25s
Test loss: 0.2517 score: 0.8980 time: 0.35s
Epoch 362/1000, LR 0.000195
Train loss: 0.5574;  Loss pred: 0.5574; Loss self: 0.0000; time: 0.87s
Val loss: 0.0756 score: 0.9796 time: 0.37s
Test loss: 0.2514 score: 0.8980 time: 0.53s
Epoch 363/1000, LR 0.000194
Train loss: 0.5562;  Loss pred: 0.5562; Loss self: 0.0000; time: 1.29s
Val loss: 0.0752 score: 0.9796 time: 0.35s
Test loss: 0.2511 score: 0.8980 time: 0.27s
Epoch 364/1000, LR 0.000194
Train loss: 0.5556;  Loss pred: 0.5556; Loss self: 0.0000; time: 0.46s
Val loss: 0.0748 score: 0.9796 time: 0.07s
Test loss: 0.2508 score: 0.8980 time: 0.06s
Epoch 365/1000, LR 0.000193
Train loss: 0.5560;  Loss pred: 0.5560; Loss self: 0.0000; time: 0.22s
Val loss: 0.0744 score: 0.9796 time: 0.07s
Test loss: 0.2505 score: 0.8980 time: 0.06s
Epoch 366/1000, LR 0.000193
Train loss: 0.5558;  Loss pred: 0.5558; Loss self: 0.0000; time: 0.21s
Val loss: 0.0740 score: 0.9796 time: 0.07s
Test loss: 0.2503 score: 0.8980 time: 0.06s
Epoch 367/1000, LR 0.000193
Train loss: 0.5551;  Loss pred: 0.5551; Loss self: 0.0000; time: 0.21s
Val loss: 0.0737 score: 0.9796 time: 0.06s
Test loss: 0.2500 score: 0.8980 time: 0.06s
Epoch 368/1000, LR 0.000192
Train loss: 0.5533;  Loss pred: 0.5533; Loss self: 0.0000; time: 0.21s
Val loss: 0.0733 score: 0.9796 time: 0.07s
Test loss: 0.2499 score: 0.8980 time: 0.06s
Epoch 369/1000, LR 0.000192
Train loss: 0.5553;  Loss pred: 0.5553; Loss self: 0.0000; time: 0.20s
Val loss: 0.0730 score: 0.9796 time: 0.07s
Test loss: 0.2497 score: 0.8980 time: 0.06s
Epoch 370/1000, LR 0.000191
Train loss: 0.5550;  Loss pred: 0.5550; Loss self: 0.0000; time: 0.20s
Val loss: 0.0726 score: 0.9796 time: 0.07s
Test loss: 0.2494 score: 0.8980 time: 0.06s
Epoch 371/1000, LR 0.000191
Train loss: 0.5534;  Loss pred: 0.5534; Loss self: 0.0000; time: 0.20s
Val loss: 0.0722 score: 0.9796 time: 0.07s
Test loss: 0.2491 score: 0.8980 time: 0.07s
Epoch 372/1000, LR 0.000191
Train loss: 0.5537;  Loss pred: 0.5537; Loss self: 0.0000; time: 3.26s
Val loss: 0.0719 score: 0.9796 time: 0.85s
Test loss: 0.2487 score: 0.8980 time: 0.70s
Epoch 373/1000, LR 0.000190
Train loss: 0.5524;  Loss pred: 0.5524; Loss self: 0.0000; time: 1.85s
Val loss: 0.0715 score: 0.9796 time: 0.56s
Test loss: 0.2483 score: 0.8980 time: 0.76s
Epoch 374/1000, LR 0.000190
Train loss: 0.5515;  Loss pred: 0.5515; Loss self: 0.0000; time: 1.75s
Val loss: 0.0711 score: 0.9796 time: 0.10s
Test loss: 0.2479 score: 0.8980 time: 0.10s
Epoch 375/1000, LR 0.000190
Train loss: 0.5517;  Loss pred: 0.5517; Loss self: 0.0000; time: 0.36s
Val loss: 0.0708 score: 0.9796 time: 0.07s
Test loss: 0.2476 score: 0.8980 time: 0.06s
Epoch 376/1000, LR 0.000189
Train loss: 0.5525;  Loss pred: 0.5525; Loss self: 0.0000; time: 0.20s
Val loss: 0.0704 score: 0.9796 time: 0.07s
Test loss: 0.2472 score: 0.8980 time: 0.06s
Epoch 377/1000, LR 0.000189
Train loss: 0.5520;  Loss pred: 0.5520; Loss self: 0.0000; time: 0.20s
Val loss: 0.0701 score: 0.9796 time: 0.07s
Test loss: 0.2470 score: 0.8980 time: 0.06s
Epoch 378/1000, LR 0.000188
Train loss: 0.5529;  Loss pred: 0.5529; Loss self: 0.0000; time: 0.20s
Val loss: 0.0698 score: 0.9796 time: 0.07s
Test loss: 0.2467 score: 0.8980 time: 0.06s
Epoch 379/1000, LR 0.000188
Train loss: 0.5505;  Loss pred: 0.5505; Loss self: 0.0000; time: 0.21s
Val loss: 0.0695 score: 0.9796 time: 0.07s
Test loss: 0.2465 score: 0.8980 time: 0.06s
Epoch 380/1000, LR 0.000188
Train loss: 0.5510;  Loss pred: 0.5510; Loss self: 0.0000; time: 0.20s
Val loss: 0.0691 score: 0.9796 time: 0.07s
Test loss: 0.2464 score: 0.8980 time: 0.06s
Epoch 381/1000, LR 0.000187
Train loss: 0.5506;  Loss pred: 0.5506; Loss self: 0.0000; time: 0.20s
Val loss: 0.0689 score: 0.9796 time: 0.07s
Test loss: 0.2463 score: 0.8980 time: 0.06s
Epoch 382/1000, LR 0.000187
Train loss: 0.5506;  Loss pred: 0.5506; Loss self: 0.0000; time: 0.20s
Val loss: 0.0686 score: 0.9796 time: 0.07s
Test loss: 0.2463 score: 0.8980 time: 0.06s
Epoch 383/1000, LR 0.000186
Train loss: 0.5507;  Loss pred: 0.5507; Loss self: 0.0000; time: 0.20s
Val loss: 0.0683 score: 0.9796 time: 0.07s
Test loss: 0.2462 score: 0.8980 time: 0.06s
Epoch 384/1000, LR 0.000186
Train loss: 0.5495;  Loss pred: 0.5495; Loss self: 0.0000; time: 0.20s
Val loss: 0.0680 score: 0.9796 time: 0.07s
Test loss: 0.2460 score: 0.8980 time: 0.06s
Epoch 385/1000, LR 0.000186
Train loss: 0.5498;  Loss pred: 0.5498; Loss self: 0.0000; time: 0.21s
Val loss: 0.0677 score: 0.9796 time: 0.07s
Test loss: 0.2458 score: 0.8980 time: 0.06s
Epoch 386/1000, LR 0.000185
Train loss: 0.5489;  Loss pred: 0.5489; Loss self: 0.0000; time: 0.21s
Val loss: 0.0674 score: 0.9796 time: 0.07s
Test loss: 0.2456 score: 0.8980 time: 0.06s
Epoch 387/1000, LR 0.000185
Train loss: 0.5480;  Loss pred: 0.5480; Loss self: 0.0000; time: 0.20s
Val loss: 0.0671 score: 0.9796 time: 0.07s
Test loss: 0.2454 score: 0.8980 time: 0.06s
Epoch 388/1000, LR 0.000184
Train loss: 0.5479;  Loss pred: 0.5479; Loss self: 0.0000; time: 0.21s
Val loss: 0.0668 score: 0.9796 time: 0.07s
Test loss: 0.2455 score: 0.8980 time: 0.06s
Epoch 389/1000, LR 0.000184
Train loss: 0.5463;  Loss pred: 0.5463; Loss self: 0.0000; time: 0.21s
Val loss: 0.0666 score: 0.9796 time: 0.07s
Test loss: 0.2455 score: 0.8980 time: 0.06s
Epoch 390/1000, LR 0.000184
Train loss: 0.5483;  Loss pred: 0.5483; Loss self: 0.0000; time: 5.60s
Val loss: 0.0663 score: 0.9796 time: 1.33s
Test loss: 0.2454 score: 0.8980 time: 0.51s
Epoch 391/1000, LR 0.000183
Train loss: 0.5487;  Loss pred: 0.5487; Loss self: 0.0000; time: 2.59s
Val loss: 0.0661 score: 0.9796 time: 0.10s
Test loss: 0.2454 score: 0.8980 time: 0.17s
Epoch 392/1000, LR 0.000183
Train loss: 0.5443;  Loss pred: 0.5443; Loss self: 0.0000; time: 0.57s
Val loss: 0.0658 score: 0.9796 time: 0.08s
Test loss: 0.2453 score: 0.8980 time: 0.09s
Epoch 393/1000, LR 0.000182
Train loss: 0.5458;  Loss pred: 0.5458; Loss self: 0.0000; time: 0.22s
Val loss: 0.0655 score: 0.9796 time: 0.07s
Test loss: 0.2452 score: 0.8980 time: 0.06s
Epoch 394/1000, LR 0.000182
Train loss: 0.5465;  Loss pred: 0.5465; Loss self: 0.0000; time: 0.21s
Val loss: 0.0652 score: 0.9796 time: 0.08s
Test loss: 0.2449 score: 0.8980 time: 0.06s
Epoch 395/1000, LR 0.000182
Train loss: 0.5452;  Loss pred: 0.5452; Loss self: 0.0000; time: 0.21s
Val loss: 0.0649 score: 0.9796 time: 0.07s
Test loss: 0.2446 score: 0.8980 time: 0.06s
Epoch 396/1000, LR 0.000181
Train loss: 0.5439;  Loss pred: 0.5439; Loss self: 0.0000; time: 0.19s
Val loss: 0.0646 score: 0.9796 time: 0.06s
Test loss: 0.2442 score: 0.8980 time: 0.06s
Epoch 397/1000, LR 0.000181
Train loss: 0.5467;  Loss pred: 0.5467; Loss self: 0.0000; time: 0.20s
Val loss: 0.0643 score: 0.9796 time: 0.06s
Test loss: 0.2438 score: 0.8980 time: 0.06s
Epoch 398/1000, LR 0.000180
Train loss: 0.5444;  Loss pred: 0.5444; Loss self: 0.0000; time: 0.20s
Val loss: 0.0641 score: 0.9796 time: 0.07s
Test loss: 0.2436 score: 0.8980 time: 0.06s
Epoch 399/1000, LR 0.000180
Train loss: 0.5445;  Loss pred: 0.5445; Loss self: 0.0000; time: 0.19s
Val loss: 0.0638 score: 0.9796 time: 0.06s
Test loss: 0.2434 score: 0.8980 time: 0.06s
Epoch 400/1000, LR 0.000180
Train loss: 0.5461;  Loss pred: 0.5461; Loss self: 0.0000; time: 0.20s
Val loss: 0.0635 score: 0.9796 time: 0.07s
Test loss: 0.2431 score: 0.8980 time: 0.79s
Epoch 401/1000, LR 0.000179
Train loss: 0.5467;  Loss pred: 0.5467; Loss self: 0.0000; time: 4.70s
Val loss: 0.0633 score: 0.9796 time: 1.83s
Test loss: 0.2428 score: 0.8980 time: 0.18s
Epoch 402/1000, LR 0.000179
Train loss: 0.5449;  Loss pred: 0.5449; Loss self: 0.0000; time: 0.32s
Val loss: 0.0630 score: 0.9796 time: 0.07s
Test loss: 0.2427 score: 0.8980 time: 0.06s
Epoch 403/1000, LR 0.000178
Train loss: 0.5444;  Loss pred: 0.5444; Loss self: 0.0000; time: 0.20s
Val loss: 0.0628 score: 0.9796 time: 0.07s
Test loss: 0.2426 score: 0.8980 time: 0.06s
Epoch 404/1000, LR 0.000178
Train loss: 0.5430;  Loss pred: 0.5430; Loss self: 0.0000; time: 0.20s
Val loss: 0.0625 score: 0.9796 time: 0.07s
Test loss: 0.2424 score: 0.8980 time: 0.06s
Epoch 405/1000, LR 0.000178
Train loss: 0.5460;  Loss pred: 0.5460; Loss self: 0.0000; time: 0.20s
Val loss: 0.0623 score: 0.9796 time: 0.07s
Test loss: 0.2423 score: 0.8980 time: 0.06s
Epoch 406/1000, LR 0.000177
Train loss: 0.5417;  Loss pred: 0.5417; Loss self: 0.0000; time: 0.20s
Val loss: 0.0621 score: 0.9796 time: 0.06s
Test loss: 0.2423 score: 0.8980 time: 0.14s
Epoch 407/1000, LR 0.000177
Train loss: 0.5436;  Loss pred: 0.5436; Loss self: 0.0000; time: 0.21s
Val loss: 0.0619 score: 0.9796 time: 0.07s
Test loss: 0.2423 score: 0.8980 time: 0.06s
Epoch 408/1000, LR 0.000176
Train loss: 0.5425;  Loss pred: 0.5425; Loss self: 0.0000; time: 0.21s
Val loss: 0.0617 score: 0.9796 time: 2.23s
Test loss: 0.2422 score: 0.8980 time: 2.00s
Epoch 409/1000, LR 0.000176
Train loss: 0.5422;  Loss pred: 0.5422; Loss self: 0.0000; time: 0.95s
Val loss: 0.0615 score: 0.9796 time: 0.12s
Test loss: 0.2421 score: 0.8980 time: 0.08s
Epoch 410/1000, LR 0.000175
Train loss: 0.5404;  Loss pred: 0.5404; Loss self: 0.0000; time: 0.75s
Val loss: 0.0613 score: 0.9796 time: 0.08s
Test loss: 0.2421 score: 0.8980 time: 0.06s
Epoch 411/1000, LR 0.000175
Train loss: 0.5413;  Loss pred: 0.5413; Loss self: 0.0000; time: 0.21s
Val loss: 0.0611 score: 0.9796 time: 0.07s
Test loss: 0.2422 score: 0.8980 time: 0.06s
Epoch 412/1000, LR 0.000175
Train loss: 0.5409;  Loss pred: 0.5409; Loss self: 0.0000; time: 0.21s
Val loss: 0.0609 score: 0.9796 time: 0.07s
Test loss: 0.2423 score: 0.8980 time: 0.07s
Epoch 413/1000, LR 0.000174
Train loss: 0.5406;  Loss pred: 0.5406; Loss self: 0.0000; time: 0.21s
Val loss: 0.0607 score: 0.9796 time: 0.07s
Test loss: 0.2424 score: 0.8980 time: 0.06s
Epoch 414/1000, LR 0.000174
Train loss: 0.5420;  Loss pred: 0.5420; Loss self: 0.0000; time: 0.21s
Val loss: 0.0605 score: 0.9796 time: 0.08s
Test loss: 0.2423 score: 0.8980 time: 0.06s
Epoch 415/1000, LR 0.000173
Train loss: 0.5424;  Loss pred: 0.5424; Loss self: 0.0000; time: 0.21s
Val loss: 0.0603 score: 0.9796 time: 0.07s
Test loss: 0.2420 score: 0.8980 time: 0.06s
Epoch 416/1000, LR 0.000173
Train loss: 0.5409;  Loss pred: 0.5409; Loss self: 0.0000; time: 0.21s
Val loss: 0.0600 score: 0.9796 time: 0.07s
Test loss: 0.2417 score: 0.8980 time: 0.06s
Epoch 417/1000, LR 0.000173
Train loss: 0.5378;  Loss pred: 0.5378; Loss self: 0.0000; time: 0.21s
Val loss: 0.0598 score: 0.9796 time: 0.07s
Test loss: 0.2415 score: 0.8980 time: 0.06s
Epoch 418/1000, LR 0.000172
Train loss: 0.5411;  Loss pred: 0.5411; Loss self: 0.0000; time: 0.21s
Val loss: 0.0596 score: 0.9796 time: 0.07s
Test loss: 0.2412 score: 0.8980 time: 0.06s
Epoch 419/1000, LR 0.000172
Train loss: 0.5409;  Loss pred: 0.5409; Loss self: 0.0000; time: 0.20s
Val loss: 0.0594 score: 0.9796 time: 0.07s
Test loss: 0.2409 score: 0.8980 time: 0.06s
Epoch 420/1000, LR 0.000171
Train loss: 0.5420;  Loss pred: 0.5420; Loss self: 0.0000; time: 0.21s
Val loss: 0.0591 score: 0.9796 time: 0.07s
Test loss: 0.2406 score: 0.8980 time: 0.06s
Epoch 421/1000, LR 0.000171
Train loss: 0.5387;  Loss pred: 0.5387; Loss self: 0.0000; time: 0.20s
Val loss: 0.0589 score: 0.9796 time: 1.29s
Test loss: 0.2402 score: 0.8980 time: 1.01s
Epoch 422/1000, LR 0.000171
Train loss: 0.5401;  Loss pred: 0.5401; Loss self: 0.0000; time: 2.31s
Val loss: 0.0587 score: 0.9796 time: 0.38s
Test loss: 0.2400 score: 0.8980 time: 0.43s
Epoch 423/1000, LR 0.000170
Train loss: 0.5382;  Loss pred: 0.5382; Loss self: 0.0000; time: 0.80s
Val loss: 0.0585 score: 0.9796 time: 0.07s
Test loss: 0.2397 score: 0.8980 time: 0.06s
Epoch 424/1000, LR 0.000170
Train loss: 0.5398;  Loss pred: 0.5398; Loss self: 0.0000; time: 0.20s
Val loss: 0.0583 score: 0.9796 time: 0.07s
Test loss: 0.2395 score: 0.8980 time: 0.06s
Epoch 425/1000, LR 0.000169
Train loss: 0.5394;  Loss pred: 0.5394; Loss self: 0.0000; time: 0.20s
Val loss: 0.0581 score: 0.9796 time: 0.07s
Test loss: 0.2393 score: 0.8980 time: 0.06s
Epoch 426/1000, LR 0.000169
Train loss: 0.5405;  Loss pred: 0.5405; Loss self: 0.0000; time: 0.19s
Val loss: 0.0579 score: 0.9796 time: 0.06s
Test loss: 0.2390 score: 0.8980 time: 0.06s
Epoch 427/1000, LR 0.000168
Train loss: 0.5380;  Loss pred: 0.5380; Loss self: 0.0000; time: 0.19s
Val loss: 0.0577 score: 0.9796 time: 0.06s
Test loss: 0.2389 score: 0.8980 time: 0.06s
Epoch 428/1000, LR 0.000168
Train loss: 0.5405;  Loss pred: 0.5405; Loss self: 0.0000; time: 0.19s
Val loss: 0.0575 score: 0.9796 time: 0.06s
Test loss: 0.2389 score: 0.8980 time: 0.06s
Epoch 429/1000, LR 0.000168
Train loss: 0.5390;  Loss pred: 0.5390; Loss self: 0.0000; time: 0.19s
Val loss: 0.0573 score: 0.9796 time: 0.06s
Test loss: 0.2388 score: 0.8980 time: 0.05s
Epoch 430/1000, LR 0.000167
Train loss: 0.5370;  Loss pred: 0.5370; Loss self: 0.0000; time: 0.19s
Val loss: 0.0572 score: 0.9796 time: 0.06s
Test loss: 0.2389 score: 0.8980 time: 0.05s
Epoch 431/1000, LR 0.000167
Train loss: 0.5368;  Loss pred: 0.5368; Loss self: 0.0000; time: 0.20s
Val loss: 0.0570 score: 0.9796 time: 0.06s
Test loss: 0.2390 score: 0.8980 time: 0.06s
Epoch 432/1000, LR 0.000166
Train loss: 0.5384;  Loss pred: 0.5384; Loss self: 0.0000; time: 0.20s
Val loss: 0.0569 score: 0.9796 time: 0.06s
Test loss: 0.2391 score: 0.8980 time: 0.06s
Epoch 433/1000, LR 0.000166
Train loss: 0.5360;  Loss pred: 0.5360; Loss self: 0.0000; time: 0.20s
Val loss: 0.0567 score: 0.9796 time: 0.06s
Test loss: 0.2391 score: 0.8980 time: 0.06s
Epoch 434/1000, LR 0.000166
Train loss: 0.5371;  Loss pred: 0.5371; Loss self: 0.0000; time: 0.20s
Val loss: 0.0565 score: 0.9796 time: 0.06s
Test loss: 0.2391 score: 0.8980 time: 0.06s
Epoch 435/1000, LR 0.000165
Train loss: 0.5364;  Loss pred: 0.5364; Loss self: 0.0000; time: 0.20s
Val loss: 0.0564 score: 0.9796 time: 0.07s
Test loss: 0.2391 score: 0.8980 time: 0.06s
Epoch 436/1000, LR 0.000165
Train loss: 0.5362;  Loss pred: 0.5362; Loss self: 0.0000; time: 0.20s
Val loss: 0.0562 score: 0.9796 time: 0.07s
Test loss: 0.2392 score: 0.8980 time: 0.06s
Epoch 437/1000, LR 0.000164
Train loss: 0.5352;  Loss pred: 0.5352; Loss self: 0.0000; time: 0.21s
Val loss: 0.0561 score: 0.9796 time: 0.07s
Test loss: 0.2392 score: 0.8980 time: 0.06s
Epoch 438/1000, LR 0.000164
Train loss: 0.5358;  Loss pred: 0.5358; Loss self: 0.0000; time: 0.20s
Val loss: 0.0559 score: 0.9796 time: 0.07s
Test loss: 0.2392 score: 0.8980 time: 0.06s
Epoch 439/1000, LR 0.000163
Train loss: 0.5377;  Loss pred: 0.5377; Loss self: 0.0000; time: 0.20s
Val loss: 0.0558 score: 0.9796 time: 0.07s
Test loss: 0.2391 score: 0.8980 time: 0.06s
Epoch 440/1000, LR 0.000163
Train loss: 0.5363;  Loss pred: 0.5363; Loss self: 0.0000; time: 0.20s
Val loss: 0.0556 score: 0.9796 time: 0.07s
Test loss: 0.2390 score: 0.8980 time: 0.06s
Epoch 441/1000, LR 0.000163
Train loss: 0.5355;  Loss pred: 0.5355; Loss self: 0.0000; time: 0.20s
Val loss: 0.0554 score: 0.9796 time: 0.07s
Test loss: 0.2389 score: 0.8980 time: 0.24s
Epoch 442/1000, LR 0.000162
Train loss: 0.5356;  Loss pred: 0.5356; Loss self: 0.0000; time: 3.68s
Val loss: 0.0552 score: 0.9796 time: 0.11s
Test loss: 0.2385 score: 0.8980 time: 0.18s
Epoch 443/1000, LR 0.000162
Train loss: 0.5339;  Loss pred: 0.5339; Loss self: 0.0000; time: 0.46s
Val loss: 0.0550 score: 0.9796 time: 0.16s
Test loss: 0.2381 score: 0.8980 time: 0.10s
Epoch 444/1000, LR 0.000161
Train loss: 0.5352;  Loss pred: 0.5352; Loss self: 0.0000; time: 0.22s
Val loss: 0.0548 score: 0.9796 time: 0.07s
Test loss: 0.2377 score: 0.8980 time: 0.06s
Epoch 445/1000, LR 0.000161
Train loss: 0.5360;  Loss pred: 0.5360; Loss self: 0.0000; time: 0.20s
Val loss: 0.0546 score: 0.9796 time: 0.06s
Test loss: 0.2374 score: 0.8980 time: 0.06s
Epoch 446/1000, LR 0.000161
Train loss: 0.5325;  Loss pred: 0.5325; Loss self: 0.0000; time: 0.19s
Val loss: 0.0545 score: 0.9796 time: 0.06s
Test loss: 0.2371 score: 0.8980 time: 0.06s
Epoch 447/1000, LR 0.000160
Train loss: 0.5344;  Loss pred: 0.5344; Loss self: 0.0000; time: 0.19s
Val loss: 0.0543 score: 0.9796 time: 0.07s
Test loss: 0.2369 score: 0.8980 time: 0.06s
Epoch 448/1000, LR 0.000160
Train loss: 0.5323;  Loss pred: 0.5323; Loss self: 0.0000; time: 0.19s
Val loss: 0.0541 score: 0.9796 time: 0.07s
Test loss: 0.2367 score: 0.8980 time: 0.06s
Epoch 449/1000, LR 0.000159
Train loss: 0.5339;  Loss pred: 0.5339; Loss self: 0.0000; time: 0.20s
Val loss: 0.0540 score: 0.9796 time: 0.07s
Test loss: 0.2364 score: 0.8980 time: 0.06s
Epoch 450/1000, LR 0.000159
Train loss: 0.5346;  Loss pred: 0.5346; Loss self: 0.0000; time: 0.20s
Val loss: 0.0538 score: 0.9796 time: 0.07s
Test loss: 0.2360 score: 0.8980 time: 0.06s
Epoch 451/1000, LR 0.000158
Train loss: 0.5320;  Loss pred: 0.5320; Loss self: 0.0000; time: 0.20s
Val loss: 0.0536 score: 0.9796 time: 0.07s
Test loss: 0.2357 score: 0.8980 time: 0.06s
Epoch 452/1000, LR 0.000158
Train loss: 0.5340;  Loss pred: 0.5340; Loss self: 0.0000; time: 0.20s
Val loss: 0.0535 score: 0.9796 time: 0.07s
Test loss: 0.2355 score: 0.8980 time: 0.06s
Epoch 453/1000, LR 0.000158
Train loss: 0.5327;  Loss pred: 0.5327; Loss self: 0.0000; time: 0.20s
Val loss: 0.0533 score: 0.9796 time: 0.07s
Test loss: 0.2355 score: 0.8980 time: 0.06s
Epoch 454/1000, LR 0.000157
Train loss: 0.5352;  Loss pred: 0.5352; Loss self: 0.0000; time: 0.20s
Val loss: 0.0532 score: 0.9796 time: 0.07s
Test loss: 0.2354 score: 0.9184 time: 0.06s
Epoch 455/1000, LR 0.000157
Train loss: 0.5325;  Loss pred: 0.5325; Loss self: 0.0000; time: 0.23s
Val loss: 0.0530 score: 0.9796 time: 0.96s
Test loss: 0.2353 score: 0.9184 time: 1.41s
Epoch 456/1000, LR 0.000156
Train loss: 0.5330;  Loss pred: 0.5330; Loss self: 0.0000; time: 4.82s
Val loss: 0.0529 score: 0.9796 time: 0.15s
Test loss: 0.2354 score: 0.8980 time: 0.29s
Epoch 457/1000, LR 0.000156
Train loss: 0.5331;  Loss pred: 0.5331; Loss self: 0.0000; time: 0.34s
Val loss: 0.0528 score: 0.9796 time: 0.07s
Test loss: 0.2357 score: 0.8980 time: 0.06s
Epoch 458/1000, LR 0.000155
Train loss: 0.5318;  Loss pred: 0.5318; Loss self: 0.0000; time: 0.21s
Val loss: 0.0526 score: 0.9796 time: 0.07s
Test loss: 0.2359 score: 0.8980 time: 0.06s
Epoch 459/1000, LR 0.000155
Train loss: 0.5310;  Loss pred: 0.5310; Loss self: 0.0000; time: 0.21s
Val loss: 0.0525 score: 0.9796 time: 0.07s
Test loss: 0.2360 score: 0.8980 time: 0.06s
Epoch 460/1000, LR 0.000155
Train loss: 0.5318;  Loss pred: 0.5318; Loss self: 0.0000; time: 0.21s
Val loss: 0.0524 score: 0.9796 time: 0.07s
Test loss: 0.2362 score: 0.8980 time: 0.06s
Epoch 461/1000, LR 0.000154
Train loss: 0.5300;  Loss pred: 0.5300; Loss self: 0.0000; time: 0.20s
Val loss: 0.0523 score: 0.9796 time: 0.07s
Test loss: 0.2364 score: 0.8980 time: 0.06s
Epoch 462/1000, LR 0.000154
Train loss: 0.5332;  Loss pred: 0.5332; Loss self: 0.0000; time: 0.20s
Val loss: 0.0522 score: 0.9796 time: 0.07s
Test loss: 0.2365 score: 0.8980 time: 0.06s
Epoch 463/1000, LR 0.000153
Train loss: 0.5304;  Loss pred: 0.5304; Loss self: 0.0000; time: 0.20s
Val loss: 0.0521 score: 0.9796 time: 0.07s
Test loss: 0.2365 score: 0.8980 time: 0.06s
Epoch 464/1000, LR 0.000153
Train loss: 0.5302;  Loss pred: 0.5302; Loss self: 0.0000; time: 0.20s
Val loss: 0.0519 score: 0.9796 time: 0.07s
Test loss: 0.2366 score: 0.8980 time: 0.06s
Epoch 465/1000, LR 0.000153
Train loss: 0.5299;  Loss pred: 0.5299; Loss self: 0.0000; time: 0.20s
Val loss: 0.0518 score: 0.9796 time: 0.07s
Test loss: 0.2367 score: 0.8980 time: 0.06s
Epoch 466/1000, LR 0.000152
Train loss: 0.5323;  Loss pred: 0.5323; Loss self: 0.0000; time: 0.20s
Val loss: 0.0517 score: 0.9796 time: 0.07s
Test loss: 0.2367 score: 0.8980 time: 0.06s
Epoch 467/1000, LR 0.000152
Train loss: 0.5300;  Loss pred: 0.5300; Loss self: 0.0000; time: 0.20s
Val loss: 0.0516 score: 0.9796 time: 0.07s
Test loss: 0.2368 score: 0.8980 time: 0.06s
Epoch 468/1000, LR 0.000151
Train loss: 0.5292;  Loss pred: 0.5292; Loss self: 0.0000; time: 0.20s
Val loss: 0.0515 score: 0.9796 time: 0.07s
Test loss: 0.2369 score: 0.8980 time: 0.06s
Epoch 469/1000, LR 0.000151
Train loss: 0.5302;  Loss pred: 0.5302; Loss self: 0.0000; time: 0.20s
Val loss: 0.0514 score: 0.9796 time: 0.07s
Test loss: 0.2369 score: 0.8980 time: 0.06s
Epoch 470/1000, LR 0.000150
Train loss: 0.5344;  Loss pred: 0.5344; Loss self: 0.0000; time: 3.36s
Val loss: 0.0512 score: 0.9796 time: 1.29s
Test loss: 0.2367 score: 0.8980 time: 1.36s
Epoch 471/1000, LR 0.000150
Train loss: 0.5314;  Loss pred: 0.5314; Loss self: 0.0000; time: 2.41s
Val loss: 0.0511 score: 0.9796 time: 0.16s
Test loss: 0.2364 score: 0.8980 time: 0.20s
Epoch 472/1000, LR 0.000150
Train loss: 0.5303;  Loss pred: 0.5303; Loss self: 0.0000; time: 0.67s
Val loss: 0.0509 score: 0.9796 time: 0.21s
Test loss: 0.2362 score: 0.8980 time: 0.19s
Epoch 473/1000, LR 0.000149
Train loss: 0.5301;  Loss pred: 0.5301; Loss self: 0.0000; time: 0.41s
Val loss: 0.0508 score: 0.9796 time: 0.13s
Test loss: 0.2361 score: 0.8980 time: 0.19s
Epoch 474/1000, LR 0.000149
Train loss: 0.5297;  Loss pred: 0.5297; Loss self: 0.0000; time: 1.07s
Val loss: 0.0507 score: 0.9796 time: 1.55s
Test loss: 0.2359 score: 0.8980 time: 0.27s
Epoch 475/1000, LR 0.000148
Train loss: 0.5297;  Loss pred: 0.5297; Loss self: 0.0000; time: 0.50s
Val loss: 0.0505 score: 0.9796 time: 0.07s
Test loss: 0.2357 score: 0.8980 time: 0.15s
Epoch 476/1000, LR 0.000148
Train loss: 0.5283;  Loss pred: 0.5283; Loss self: 0.0000; time: 0.21s
Val loss: 0.0504 score: 0.9796 time: 0.07s
Test loss: 0.2355 score: 0.8980 time: 0.06s
Epoch 477/1000, LR 0.000147
Train loss: 0.5301;  Loss pred: 0.5301; Loss self: 0.0000; time: 0.21s
Val loss: 0.0502 score: 0.9796 time: 0.07s
Test loss: 0.2353 score: 0.8980 time: 0.06s
Epoch 478/1000, LR 0.000147
Train loss: 0.5290;  Loss pred: 0.5290; Loss self: 0.0000; time: 0.21s
Val loss: 0.0501 score: 0.9796 time: 0.07s
Test loss: 0.2350 score: 0.9184 time: 0.06s
Epoch 479/1000, LR 0.000147
Train loss: 0.5289;  Loss pred: 0.5289; Loss self: 0.0000; time: 0.19s
Val loss: 0.0500 score: 0.9796 time: 0.06s
Test loss: 0.2347 score: 0.9184 time: 0.06s
Epoch 480/1000, LR 0.000146
Train loss: 0.5300;  Loss pred: 0.5300; Loss self: 0.0000; time: 0.22s
Val loss: 0.0498 score: 0.9796 time: 0.08s
Test loss: 0.2343 score: 0.9184 time: 0.07s
Epoch 481/1000, LR 0.000146
Train loss: 0.5276;  Loss pred: 0.5276; Loss self: 0.0000; time: 0.20s
Val loss: 0.0497 score: 0.9796 time: 0.07s
Test loss: 0.2341 score: 0.9184 time: 0.06s
Epoch 482/1000, LR 0.000145
Train loss: 0.5297;  Loss pred: 0.5297; Loss self: 0.0000; time: 0.20s
Val loss: 0.0496 score: 0.9796 time: 0.07s
Test loss: 0.2338 score: 0.9184 time: 0.06s
Epoch 483/1000, LR 0.000145
Train loss: 0.5299;  Loss pred: 0.5299; Loss self: 0.0000; time: 0.20s
Val loss: 0.0495 score: 0.9796 time: 0.38s
Test loss: 0.2336 score: 0.9184 time: 2.71s
Epoch 484/1000, LR 0.000144
Train loss: 0.5279;  Loss pred: 0.5279; Loss self: 0.0000; time: 2.10s
Val loss: 0.0493 score: 0.9796 time: 0.14s
Test loss: 0.2335 score: 0.9184 time: 0.43s
Epoch 485/1000, LR 0.000144
Train loss: 0.5276;  Loss pred: 0.5276; Loss self: 0.0000; time: 0.78s
Val loss: 0.0492 score: 0.9796 time: 0.27s
Test loss: 0.2335 score: 0.9184 time: 0.19s
Epoch 486/1000, LR 0.000144
Train loss: 0.5278;  Loss pred: 0.5278; Loss self: 0.0000; time: 0.41s
Val loss: 0.0491 score: 0.9796 time: 0.13s
Test loss: 0.2334 score: 0.9184 time: 0.11s
Epoch 487/1000, LR 0.000143
Train loss: 0.5279;  Loss pred: 0.5279; Loss self: 0.0000; time: 0.36s
Val loss: 0.0490 score: 0.9796 time: 0.11s
Test loss: 0.2334 score: 0.9184 time: 0.07s
Epoch 488/1000, LR 0.000143
Train loss: 0.5284;  Loss pred: 0.5284; Loss self: 0.0000; time: 0.20s
Val loss: 0.0489 score: 0.9796 time: 0.07s
Test loss: 0.2335 score: 0.9184 time: 0.06s
Epoch 489/1000, LR 0.000142
Train loss: 0.5271;  Loss pred: 0.5271; Loss self: 0.0000; time: 0.21s
Val loss: 0.0488 score: 0.9796 time: 0.07s
Test loss: 0.2336 score: 0.9184 time: 0.06s
Epoch 490/1000, LR 0.000142
Train loss: 0.5270;  Loss pred: 0.5270; Loss self: 0.0000; time: 0.20s
Val loss: 0.0487 score: 0.9796 time: 0.07s
Test loss: 0.2337 score: 0.9184 time: 0.06s
Epoch 491/1000, LR 0.000141
Train loss: 0.5287;  Loss pred: 0.5287; Loss self: 0.0000; time: 0.20s
Val loss: 0.0487 score: 0.9796 time: 0.07s
Test loss: 0.2339 score: 0.9184 time: 0.06s
Epoch 492/1000, LR 0.000141
Train loss: 0.5264;  Loss pred: 0.5264; Loss self: 0.0000; time: 0.20s
Val loss: 0.0486 score: 0.9796 time: 0.07s
Test loss: 0.2340 score: 0.9184 time: 0.06s
Epoch 493/1000, LR 0.000141
Train loss: 0.5255;  Loss pred: 0.5255; Loss self: 0.0000; time: 0.20s
Val loss: 0.0485 score: 0.9796 time: 0.07s
Test loss: 0.2342 score: 0.9184 time: 0.06s
Epoch 494/1000, LR 0.000140
Train loss: 0.5279;  Loss pred: 0.5279; Loss self: 0.0000; time: 0.20s
Val loss: 0.0484 score: 0.9796 time: 0.07s
Test loss: 0.2343 score: 0.9184 time: 0.06s
Epoch 495/1000, LR 0.000140
Train loss: 0.5266;  Loss pred: 0.5266; Loss self: 0.0000; time: 0.20s
Val loss: 0.0483 score: 0.9796 time: 0.07s
Test loss: 0.2344 score: 0.9184 time: 0.06s
Epoch 496/1000, LR 0.000139
Train loss: 0.5269;  Loss pred: 0.5269; Loss self: 0.0000; time: 3.39s
Val loss: 0.0482 score: 0.9796 time: 0.25s
Test loss: 0.2345 score: 0.9184 time: 0.15s
Epoch 497/1000, LR 0.000139
Train loss: 0.5266;  Loss pred: 0.5266; Loss self: 0.0000; time: 1.68s
Val loss: 0.0481 score: 0.9796 time: 0.17s
Test loss: 0.2346 score: 0.9184 time: 0.13s
Epoch 498/1000, LR 0.000138
Train loss: 0.5282;  Loss pred: 0.5282; Loss self: 0.0000; time: 0.92s
Val loss: 0.0480 score: 0.9796 time: 0.09s
Test loss: 0.2344 score: 0.9184 time: 0.07s
Epoch 499/1000, LR 0.000138
Train loss: 0.5265;  Loss pred: 0.5265; Loss self: 0.0000; time: 0.20s
Val loss: 0.0479 score: 0.9796 time: 0.06s
Test loss: 0.2344 score: 0.9184 time: 0.06s
Epoch 500/1000, LR 0.000138
Train loss: 0.5242;  Loss pred: 0.5242; Loss self: 0.0000; time: 0.20s
Val loss: 0.0478 score: 0.9796 time: 0.07s
Test loss: 0.2343 score: 0.9184 time: 0.06s
Epoch 501/1000, LR 0.000137
Train loss: 0.5247;  Loss pred: 0.5247; Loss self: 0.0000; time: 0.20s
Val loss: 0.0478 score: 0.9796 time: 0.07s
Test loss: 0.2344 score: 0.9184 time: 0.06s
Epoch 502/1000, LR 0.000137
Train loss: 0.5252;  Loss pred: 0.5252; Loss self: 0.0000; time: 0.20s
Val loss: 0.0477 score: 0.9796 time: 0.07s
Test loss: 0.2344 score: 0.9184 time: 0.06s
Epoch 503/1000, LR 0.000136
Train loss: 0.5259;  Loss pred: 0.5259; Loss self: 0.0000; time: 0.19s
Val loss: 0.0476 score: 0.9796 time: 0.06s
Test loss: 0.2344 score: 0.9184 time: 0.06s
Epoch 504/1000, LR 0.000136
Train loss: 0.5252;  Loss pred: 0.5252; Loss self: 0.0000; time: 0.20s
Val loss: 0.0475 score: 0.9796 time: 0.06s
Test loss: 0.2343 score: 0.9184 time: 0.06s
Epoch 505/1000, LR 0.000135
Train loss: 0.5256;  Loss pred: 0.5256; Loss self: 0.0000; time: 0.20s
Val loss: 0.0474 score: 0.9796 time: 0.06s
Test loss: 0.2342 score: 0.9184 time: 0.06s
Epoch 506/1000, LR 0.000135
Train loss: 0.5247;  Loss pred: 0.5247; Loss self: 0.0000; time: 0.19s
Val loss: 0.0473 score: 0.9796 time: 0.07s
Test loss: 0.2340 score: 0.9184 time: 0.06s
Epoch 507/1000, LR 0.000135
Train loss: 0.5250;  Loss pred: 0.5250; Loss self: 0.0000; time: 0.19s
Val loss: 0.0472 score: 0.9796 time: 0.06s
Test loss: 0.2340 score: 0.9184 time: 0.06s
Epoch 508/1000, LR 0.000134
Train loss: 0.5255;  Loss pred: 0.5255; Loss self: 0.0000; time: 0.19s
Val loss: 0.0471 score: 0.9796 time: 0.06s
Test loss: 0.2338 score: 0.9184 time: 0.06s
Epoch 509/1000, LR 0.000134
Train loss: 0.5257;  Loss pred: 0.5257; Loss self: 0.0000; time: 0.19s
Val loss: 0.0470 score: 0.9796 time: 0.06s
Test loss: 0.2338 score: 0.9184 time: 0.06s
Epoch 510/1000, LR 0.000133
Train loss: 0.5233;  Loss pred: 0.5233; Loss self: 0.0000; time: 0.19s
Val loss: 0.0469 score: 0.9796 time: 0.06s
Test loss: 0.2337 score: 0.9184 time: 0.06s
Epoch 511/1000, LR 0.000133
Train loss: 0.5227;  Loss pred: 0.5227; Loss self: 0.0000; time: 0.19s
Val loss: 0.0468 score: 0.9796 time: 0.06s
Test loss: 0.2337 score: 0.9184 time: 0.06s
Epoch 512/1000, LR 0.000132
Train loss: 0.5252;  Loss pred: 0.5252; Loss self: 0.0000; time: 0.19s
Val loss: 0.0467 score: 0.9796 time: 0.06s
Test loss: 0.2337 score: 0.9184 time: 0.06s
Epoch 513/1000, LR 0.000132
Train loss: 0.5239;  Loss pred: 0.5239; Loss self: 0.0000; time: 0.19s
Val loss: 0.0466 score: 0.9796 time: 0.07s
Test loss: 0.2336 score: 0.9184 time: 0.06s
Epoch 514/1000, LR 0.000132
Train loss: 0.5231;  Loss pred: 0.5231; Loss self: 0.0000; time: 0.19s
Val loss: 0.0465 score: 0.9796 time: 0.06s
Test loss: 0.2336 score: 0.9184 time: 0.06s
Epoch 515/1000, LR 0.000131
Train loss: 0.5230;  Loss pred: 0.5230; Loss self: 0.0000; time: 0.19s
Val loss: 0.0464 score: 0.9796 time: 0.06s
Test loss: 0.2336 score: 0.9184 time: 0.05s
Epoch 516/1000, LR 0.000131
Train loss: 0.5243;  Loss pred: 0.5243; Loss self: 0.0000; time: 0.19s
Val loss: 0.0464 score: 0.9796 time: 0.06s
Test loss: 0.2336 score: 0.9184 time: 0.06s
Epoch 517/1000, LR 0.000130
Train loss: 0.5237;  Loss pred: 0.5237; Loss self: 0.0000; time: 0.19s
Val loss: 0.0463 score: 0.9796 time: 0.06s
Test loss: 0.2336 score: 0.9184 time: 0.06s
Epoch 518/1000, LR 0.000130
Train loss: 0.5243;  Loss pred: 0.5243; Loss self: 0.0000; time: 0.19s
Val loss: 0.0462 score: 0.9796 time: 0.06s
Test loss: 0.2336 score: 0.9184 time: 0.06s
Epoch 519/1000, LR 0.000129
Train loss: 0.5251;  Loss pred: 0.5251; Loss self: 0.0000; time: 0.19s
Val loss: 0.0461 score: 0.9796 time: 0.07s
Test loss: 0.2338 score: 0.9184 time: 0.06s
Epoch 520/1000, LR 0.000129
Train loss: 0.5242;  Loss pred: 0.5242; Loss self: 0.0000; time: 0.19s
Val loss: 0.0461 score: 0.9796 time: 0.07s
Test loss: 0.2339 score: 0.9184 time: 0.06s
Epoch 521/1000, LR 0.000129
Train loss: 0.5254;  Loss pred: 0.5254; Loss self: 0.0000; time: 0.20s
Val loss: 0.0460 score: 0.9796 time: 0.06s
Test loss: 0.2339 score: 0.9184 time: 0.06s
Epoch 522/1000, LR 0.000128
Train loss: 0.5231;  Loss pred: 0.5231; Loss self: 0.0000; time: 0.19s
Val loss: 0.0459 score: 0.9796 time: 0.07s
Test loss: 0.2340 score: 0.9184 time: 2.16s
Epoch 523/1000, LR 0.000128
Train loss: 0.5236;  Loss pred: 0.5236; Loss self: 0.0000; time: 5.34s
Val loss: 0.0459 score: 0.9796 time: 1.11s
Test loss: 0.2340 score: 0.9184 time: 0.54s
Epoch 524/1000, LR 0.000127
Train loss: 0.5236;  Loss pred: 0.5236; Loss self: 0.0000; time: 0.44s
Val loss: 0.0458 score: 0.9796 time: 0.13s
Test loss: 0.2339 score: 0.9184 time: 0.10s
Epoch 525/1000, LR 0.000127
Train loss: 0.5230;  Loss pred: 0.5230; Loss self: 0.0000; time: 0.33s
Val loss: 0.0457 score: 0.9796 time: 0.07s
Test loss: 0.2338 score: 0.9184 time: 0.06s
Epoch 526/1000, LR 0.000126
Train loss: 0.5230;  Loss pred: 0.5230; Loss self: 0.0000; time: 0.21s
Val loss: 0.0456 score: 0.9796 time: 0.07s
Test loss: 0.2337 score: 0.9184 time: 0.06s
Epoch 527/1000, LR 0.000126
Train loss: 0.5222;  Loss pred: 0.5222; Loss self: 0.0000; time: 0.20s
Val loss: 0.0455 score: 0.9796 time: 0.06s
Test loss: 0.2337 score: 0.9184 time: 0.06s
Epoch 528/1000, LR 0.000126
Train loss: 0.5198;  Loss pred: 0.5198; Loss self: 0.0000; time: 0.19s
Val loss: 0.0455 score: 0.9796 time: 0.06s
Test loss: 0.2338 score: 0.9184 time: 0.06s
Epoch 529/1000, LR 0.000125
Train loss: 0.5217;  Loss pred: 0.5217; Loss self: 0.0000; time: 0.20s
Val loss: 0.0454 score: 0.9796 time: 0.07s
Test loss: 0.2338 score: 0.9184 time: 0.06s
Epoch 530/1000, LR 0.000125
Train loss: 0.5228;  Loss pred: 0.5228; Loss self: 0.0000; time: 0.19s
Val loss: 0.0453 score: 0.9796 time: 0.06s
Test loss: 0.2339 score: 0.9184 time: 0.06s
Epoch 531/1000, LR 0.000124
Train loss: 0.5211;  Loss pred: 0.5211; Loss self: 0.0000; time: 0.19s
Val loss: 0.0453 score: 0.9796 time: 0.06s
Test loss: 0.2341 score: 0.9184 time: 0.06s
Epoch 532/1000, LR 0.000124
Train loss: 0.5232;  Loss pred: 0.5232; Loss self: 0.0000; time: 0.19s
Val loss: 0.0452 score: 0.9796 time: 0.07s
Test loss: 0.2342 score: 0.9184 time: 0.06s
Epoch 533/1000, LR 0.000123
Train loss: 0.5230;  Loss pred: 0.5230; Loss self: 0.0000; time: 3.46s
Val loss: 0.0452 score: 0.9796 time: 0.09s
Test loss: 0.2342 score: 0.9184 time: 0.18s
Epoch 534/1000, LR 0.000123
Train loss: 0.5213;  Loss pred: 0.5213; Loss self: 0.0000; time: 0.51s
Val loss: 0.0451 score: 0.9796 time: 0.08s
Test loss: 0.2341 score: 0.9184 time: 0.07s
Epoch 535/1000, LR 0.000123
Train loss: 0.5214;  Loss pred: 0.5214; Loss self: 0.0000; time: 0.20s
Val loss: 0.0450 score: 0.9796 time: 0.07s
Test loss: 0.2341 score: 0.9184 time: 0.06s
Epoch 536/1000, LR 0.000122
Train loss: 0.5207;  Loss pred: 0.5207; Loss self: 0.0000; time: 0.19s
Val loss: 0.0449 score: 0.9796 time: 0.06s
Test loss: 0.2340 score: 0.9184 time: 0.06s
Epoch 537/1000, LR 0.000122
Train loss: 0.5219;  Loss pred: 0.5219; Loss self: 0.0000; time: 0.19s
Val loss: 0.0448 score: 0.9796 time: 0.06s
Test loss: 0.2339 score: 0.9184 time: 0.06s
Epoch 538/1000, LR 0.000121
Train loss: 0.5184;  Loss pred: 0.5184; Loss self: 0.0000; time: 0.19s
Val loss: 0.0448 score: 0.9796 time: 0.07s
Test loss: 0.2337 score: 0.9184 time: 0.06s
Epoch 539/1000, LR 0.000121
Train loss: 0.5225;  Loss pred: 0.5225; Loss self: 0.0000; time: 0.20s
Val loss: 0.0447 score: 0.9796 time: 0.07s
Test loss: 0.2335 score: 0.9184 time: 0.06s
Epoch 540/1000, LR 0.000120
Train loss: 0.5229;  Loss pred: 0.5229; Loss self: 0.0000; time: 0.20s
Val loss: 0.0446 score: 0.9796 time: 0.07s
Test loss: 0.2334 score: 0.9184 time: 0.06s
Epoch 541/1000, LR 0.000120
Train loss: 0.5231;  Loss pred: 0.5231; Loss self: 0.0000; time: 0.21s
Val loss: 0.0445 score: 0.9796 time: 0.07s
Test loss: 0.2332 score: 0.9184 time: 0.07s
Epoch 542/1000, LR 0.000120
Train loss: 0.5219;  Loss pred: 0.5219; Loss self: 0.0000; time: 0.21s
Val loss: 0.0444 score: 0.9796 time: 0.07s
Test loss: 0.2331 score: 0.9184 time: 0.06s
Epoch 543/1000, LR 0.000119
Train loss: 0.5216;  Loss pred: 0.5216; Loss self: 0.0000; time: 0.21s
Val loss: 0.0443 score: 0.9796 time: 0.08s
Test loss: 0.2330 score: 0.9184 time: 0.06s
Epoch 544/1000, LR 0.000119
Train loss: 0.5198;  Loss pred: 0.5198; Loss self: 0.0000; time: 0.21s
Val loss: 0.0443 score: 0.9796 time: 0.07s
Test loss: 0.2329 score: 0.9184 time: 0.06s
Epoch 545/1000, LR 0.000118
Train loss: 0.5208;  Loss pred: 0.5208; Loss self: 0.0000; time: 0.21s
Val loss: 0.0442 score: 0.9796 time: 0.08s
Test loss: 0.2328 score: 0.9184 time: 0.06s
Epoch 546/1000, LR 0.000118
Train loss: 0.5202;  Loss pred: 0.5202; Loss self: 0.0000; time: 0.21s
Val loss: 0.0441 score: 0.9796 time: 0.07s
Test loss: 0.2327 score: 0.9184 time: 0.06s
Epoch 547/1000, LR 0.000117
Train loss: 0.5219;  Loss pred: 0.5219; Loss self: 0.0000; time: 0.21s
Val loss: 0.0441 score: 0.9796 time: 0.08s
Test loss: 0.2326 score: 0.9184 time: 0.28s
Epoch 548/1000, LR 0.000117
Train loss: 0.5213;  Loss pred: 0.5213; Loss self: 0.0000; time: 3.39s
Val loss: 0.0440 score: 0.9796 time: 0.08s
Test loss: 0.2326 score: 0.9184 time: 0.07s
Epoch 549/1000, LR 0.000117
Train loss: 0.5219;  Loss pred: 0.5219; Loss self: 0.0000; time: 0.21s
Val loss: 0.0439 score: 0.9796 time: 0.07s
Test loss: 0.2325 score: 0.9184 time: 0.06s
Epoch 550/1000, LR 0.000116
Train loss: 0.5200;  Loss pred: 0.5200; Loss self: 0.0000; time: 0.21s
Val loss: 0.0439 score: 0.9796 time: 0.07s
Test loss: 0.2324 score: 0.9184 time: 0.06s
Epoch 551/1000, LR 0.000116
Train loss: 0.5209;  Loss pred: 0.5209; Loss self: 0.0000; time: 0.20s
Val loss: 0.0438 score: 0.9796 time: 0.07s
Test loss: 0.2324 score: 0.9184 time: 0.06s
Epoch 552/1000, LR 0.000115
Train loss: 0.5226;  Loss pred: 0.5226; Loss self: 0.0000; time: 0.20s
Val loss: 0.0437 score: 0.9796 time: 0.07s
Test loss: 0.2324 score: 0.9184 time: 0.06s
Epoch 553/1000, LR 0.000115
Train loss: 0.5201;  Loss pred: 0.5201; Loss self: 0.0000; time: 0.20s
Val loss: 0.0437 score: 0.9796 time: 0.07s
Test loss: 0.2325 score: 0.9184 time: 0.06s
Epoch 554/1000, LR 0.000115
Train loss: 0.5197;  Loss pred: 0.5197; Loss self: 0.0000; time: 0.20s
Val loss: 0.0436 score: 0.9796 time: 0.07s
Test loss: 0.2325 score: 0.9184 time: 0.06s
Epoch 555/1000, LR 0.000114
Train loss: 0.5213;  Loss pred: 0.5213; Loss self: 0.0000; time: 0.20s
Val loss: 0.0436 score: 0.9796 time: 0.08s
Test loss: 0.2326 score: 0.9184 time: 0.06s
Epoch 556/1000, LR 0.000114
Train loss: 0.5192;  Loss pred: 0.5192; Loss self: 0.0000; time: 0.20s
Val loss: 0.0435 score: 0.9796 time: 0.07s
Test loss: 0.2326 score: 0.9184 time: 0.06s
Epoch 557/1000, LR 0.000113
Train loss: 0.5196;  Loss pred: 0.5196; Loss self: 0.0000; time: 0.20s
Val loss: 0.0435 score: 0.9796 time: 0.07s
Test loss: 0.2327 score: 0.9184 time: 0.06s
Epoch 558/1000, LR 0.000113
Train loss: 0.5219;  Loss pred: 0.5219; Loss self: 0.0000; time: 0.20s
Val loss: 0.0434 score: 0.9796 time: 0.07s
Test loss: 0.2328 score: 0.9184 time: 0.06s
Epoch 559/1000, LR 0.000112
Train loss: 0.5194;  Loss pred: 0.5194; Loss self: 0.0000; time: 0.20s
Val loss: 0.0434 score: 0.9796 time: 0.06s
Test loss: 0.2329 score: 0.9184 time: 0.06s
Epoch 560/1000, LR 0.000112
Train loss: 0.5181;  Loss pred: 0.5181; Loss self: 0.0000; time: 0.19s
Val loss: 0.0433 score: 0.9796 time: 0.06s
Test loss: 0.2330 score: 0.9184 time: 0.06s
Epoch 561/1000, LR 0.000112
Train loss: 0.5190;  Loss pred: 0.5190; Loss self: 0.0000; time: 0.21s
Val loss: 0.0433 score: 0.9796 time: 0.08s
Test loss: 0.2332 score: 0.9184 time: 0.06s
Epoch 562/1000, LR 0.000111
Train loss: 0.5200;  Loss pred: 0.5200; Loss self: 0.0000; time: 0.19s
Val loss: 0.0432 score: 0.9796 time: 0.06s
Test loss: 0.2333 score: 0.9184 time: 0.06s
Epoch 563/1000, LR 0.000111
Train loss: 0.5200;  Loss pred: 0.5200; Loss self: 0.0000; time: 0.19s
Val loss: 0.0432 score: 0.9796 time: 0.06s
Test loss: 0.2335 score: 0.9184 time: 0.06s
Epoch 564/1000, LR 0.000110
Train loss: 0.5199;  Loss pred: 0.5199; Loss self: 0.0000; time: 0.19s
Val loss: 0.0432 score: 0.9796 time: 0.07s
Test loss: 0.2337 score: 0.9184 time: 0.06s
Epoch 565/1000, LR 0.000110
Train loss: 0.5206;  Loss pred: 0.5206; Loss self: 0.0000; time: 0.20s
Val loss: 0.0431 score: 0.9796 time: 0.08s
Test loss: 0.2339 score: 0.9184 time: 2.11s
Epoch 566/1000, LR 0.000109
Train loss: 0.5191;  Loss pred: 0.5191; Loss self: 0.0000; time: 4.98s
Val loss: 0.0431 score: 0.9796 time: 0.67s
Test loss: 0.2340 score: 0.9184 time: 0.36s
Epoch 567/1000, LR 0.000109
Train loss: 0.5188;  Loss pred: 0.5188; Loss self: 0.0000; time: 0.46s
Val loss: 0.0431 score: 0.9796 time: 0.13s
Test loss: 0.2341 score: 0.9184 time: 0.09s
Epoch 568/1000, LR 0.000109
Train loss: 0.5184;  Loss pred: 0.5184; Loss self: 0.0000; time: 0.19s
Val loss: 0.0430 score: 0.9796 time: 0.06s
Test loss: 0.2341 score: 0.9184 time: 0.06s
Epoch 569/1000, LR 0.000108
Train loss: 0.5187;  Loss pred: 0.5187; Loss self: 0.0000; time: 0.19s
Val loss: 0.0429 score: 0.9796 time: 0.06s
Test loss: 0.2341 score: 0.9184 time: 0.06s
Epoch 570/1000, LR 0.000108
Train loss: 0.5168;  Loss pred: 0.5168; Loss self: 0.0000; time: 0.19s
Val loss: 0.0429 score: 0.9796 time: 0.09s
Test loss: 0.2341 score: 0.9184 time: 0.06s
Epoch 571/1000, LR 0.000107
Train loss: 0.5171;  Loss pred: 0.5171; Loss self: 0.0000; time: 0.20s
Val loss: 0.0428 score: 0.9796 time: 0.07s
Test loss: 0.2341 score: 0.9184 time: 0.06s
Epoch 572/1000, LR 0.000107
Train loss: 0.5194;  Loss pred: 0.5194; Loss self: 0.0000; time: 0.19s
Val loss: 0.0428 score: 0.9796 time: 0.06s
Test loss: 0.2341 score: 0.9184 time: 0.06s
Epoch 573/1000, LR 0.000107
Train loss: 0.5182;  Loss pred: 0.5182; Loss self: 0.0000; time: 0.19s
Val loss: 0.0427 score: 0.9796 time: 0.06s
Test loss: 0.2340 score: 0.9184 time: 0.06s
Epoch 574/1000, LR 0.000106
Train loss: 0.5176;  Loss pred: 0.5176; Loss self: 0.0000; time: 0.19s
Val loss: 0.0427 score: 0.9796 time: 0.06s
Test loss: 0.2339 score: 0.9184 time: 0.06s
Epoch 575/1000, LR 0.000106
Train loss: 0.5171;  Loss pred: 0.5171; Loss self: 0.0000; time: 0.19s
Val loss: 0.0426 score: 0.9796 time: 0.06s
Test loss: 0.2340 score: 0.9184 time: 0.06s
Epoch 576/1000, LR 0.000105
Train loss: 0.5180;  Loss pred: 0.5180; Loss self: 0.0000; time: 0.19s
Val loss: 0.0426 score: 0.9796 time: 0.07s
Test loss: 0.2340 score: 0.9184 time: 0.06s
Epoch 577/1000, LR 0.000105
Train loss: 0.5204;  Loss pred: 0.5204; Loss self: 0.0000; time: 3.90s
Val loss: 0.0425 score: 0.9796 time: 0.08s
Test loss: 0.2340 score: 0.9184 time: 0.07s
Epoch 578/1000, LR 0.000104
Train loss: 0.5195;  Loss pred: 0.5195; Loss self: 0.0000; time: 0.21s
Val loss: 0.0425 score: 0.9796 time: 0.07s
Test loss: 0.2339 score: 0.9184 time: 0.06s
Epoch 579/1000, LR 0.000104
Train loss: 0.5168;  Loss pred: 0.5168; Loss self: 0.0000; time: 0.21s
Val loss: 0.0424 score: 0.9796 time: 0.07s
Test loss: 0.2338 score: 0.9184 time: 0.06s
Epoch 580/1000, LR 0.000104
Train loss: 0.5180;  Loss pred: 0.5180; Loss self: 0.0000; time: 0.21s
Val loss: 0.0423 score: 0.9796 time: 0.07s
Test loss: 0.2338 score: 0.9184 time: 0.06s
Epoch 581/1000, LR 0.000103
Train loss: 0.5181;  Loss pred: 0.5181; Loss self: 0.0000; time: 0.21s
Val loss: 0.0423 score: 0.9796 time: 0.07s
Test loss: 0.2337 score: 0.9184 time: 0.06s
Epoch 582/1000, LR 0.000103
Train loss: 0.5172;  Loss pred: 0.5172; Loss self: 0.0000; time: 0.21s
Val loss: 0.0422 score: 0.9796 time: 0.07s
Test loss: 0.2337 score: 0.9184 time: 0.06s
Epoch 583/1000, LR 0.000102
Train loss: 0.5178;  Loss pred: 0.5178; Loss self: 0.0000; time: 0.21s
Val loss: 0.0422 score: 0.9796 time: 0.07s
Test loss: 0.2336 score: 0.9184 time: 0.06s
Epoch 584/1000, LR 0.000102
Train loss: 0.5165;  Loss pred: 0.5165; Loss self: 0.0000; time: 4.65s
Val loss: 0.0421 score: 0.9796 time: 0.38s
Test loss: 0.2335 score: 0.9184 time: 0.25s
Epoch 585/1000, LR 0.000102
Train loss: 0.5183;  Loss pred: 0.5183; Loss self: 0.0000; time: 0.82s
Val loss: 0.0421 score: 0.9796 time: 0.24s
Test loss: 0.2334 score: 0.9184 time: 0.33s
Epoch 586/1000, LR 0.000101
Train loss: 0.5170;  Loss pred: 0.5170; Loss self: 0.0000; time: 0.71s
Val loss: 0.0420 score: 0.9796 time: 0.29s
Test loss: 0.2333 score: 0.9184 time: 0.37s
Epoch 587/1000, LR 0.000101
Train loss: 0.5178;  Loss pred: 0.5178; Loss self: 0.0000; time: 0.29s
Val loss: 0.0419 score: 0.9796 time: 0.06s
Test loss: 0.2332 score: 0.9184 time: 0.06s
Epoch 588/1000, LR 0.000100
Train loss: 0.5164;  Loss pred: 0.5164; Loss self: 0.0000; time: 0.19s
Val loss: 0.0419 score: 0.9796 time: 0.06s
Test loss: 0.2330 score: 0.9184 time: 0.06s
Epoch 589/1000, LR 0.000100
Train loss: 0.5180;  Loss pred: 0.5180; Loss self: 0.0000; time: 0.20s
Val loss: 0.0418 score: 0.9796 time: 0.06s
Test loss: 0.2328 score: 0.9184 time: 0.06s
Epoch 590/1000, LR 0.000099
Train loss: 0.5164;  Loss pred: 0.5164; Loss self: 0.0000; time: 0.19s
Val loss: 0.0417 score: 0.9796 time: 0.07s
Test loss: 0.2325 score: 0.9184 time: 0.06s
Epoch 591/1000, LR 0.000099
Train loss: 0.5149;  Loss pred: 0.5149; Loss self: 0.0000; time: 0.20s
Val loss: 0.0417 score: 0.9796 time: 0.07s
Test loss: 0.2323 score: 0.9184 time: 0.06s
Epoch 592/1000, LR 0.000099
Train loss: 0.5156;  Loss pred: 0.5156; Loss self: 0.0000; time: 0.20s
Val loss: 0.0416 score: 0.9796 time: 0.07s
Test loss: 0.2321 score: 0.9184 time: 0.07s
Epoch 593/1000, LR 0.000098
Train loss: 0.5168;  Loss pred: 0.5168; Loss self: 0.0000; time: 0.20s
Val loss: 0.0416 score: 0.9796 time: 0.07s
Test loss: 0.2320 score: 0.9184 time: 0.06s
Epoch 594/1000, LR 0.000098
Train loss: 0.5195;  Loss pred: 0.5195; Loss self: 0.0000; time: 0.20s
Val loss: 0.0415 score: 0.9796 time: 0.07s
Test loss: 0.2319 score: 0.9184 time: 0.06s
Epoch 595/1000, LR 0.000097
Train loss: 0.5165;  Loss pred: 0.5165; Loss self: 0.0000; time: 0.19s
Val loss: 0.0415 score: 0.9796 time: 0.06s
Test loss: 0.2319 score: 0.9184 time: 0.06s
Epoch 596/1000, LR 0.000097
Train loss: 0.5180;  Loss pred: 0.5180; Loss self: 0.0000; time: 0.19s
Val loss: 0.0414 score: 0.9796 time: 0.06s
Test loss: 0.2318 score: 0.9184 time: 0.07s
Epoch 597/1000, LR 0.000097
Train loss: 0.5180;  Loss pred: 0.5180; Loss self: 0.0000; time: 0.20s
Val loss: 0.0413 score: 0.9796 time: 0.07s
Test loss: 0.2316 score: 0.9184 time: 0.06s
Epoch 598/1000, LR 0.000096
Train loss: 0.5141;  Loss pred: 0.5141; Loss self: 0.0000; time: 0.21s
Val loss: 0.0413 score: 0.9796 time: 0.08s
Test loss: 0.2316 score: 0.9184 time: 0.06s
Epoch 599/1000, LR 0.000096
Train loss: 0.5161;  Loss pred: 0.5161; Loss self: 0.0000; time: 0.21s
Val loss: 0.0412 score: 0.9796 time: 0.07s
Test loss: 0.2314 score: 0.9184 time: 0.06s
Epoch 600/1000, LR 0.000095
Train loss: 0.5158;  Loss pred: 0.5158; Loss self: 0.0000; time: 0.21s
Val loss: 0.0412 score: 0.9796 time: 0.08s
Test loss: 0.2313 score: 0.9184 time: 0.06s
Epoch 601/1000, LR 0.000095
Train loss: 0.5162;  Loss pred: 0.5162; Loss self: 0.0000; time: 0.21s
Val loss: 0.0411 score: 0.9796 time: 0.07s
Test loss: 0.2313 score: 0.9184 time: 0.06s
Epoch 602/1000, LR 0.000095
Train loss: 0.5156;  Loss pred: 0.5156; Loss self: 0.0000; time: 0.21s
Val loss: 0.0411 score: 0.9796 time: 0.08s
Test loss: 0.2313 score: 0.9184 time: 0.06s
Epoch 603/1000, LR 0.000094
Train loss: 0.5176;  Loss pred: 0.5176; Loss self: 0.0000; time: 0.21s
Val loss: 0.0411 score: 0.9796 time: 0.07s
Test loss: 0.2313 score: 0.9184 time: 0.06s
Epoch 604/1000, LR 0.000094
Train loss: 0.5157;  Loss pred: 0.5157; Loss self: 0.0000; time: 0.21s
Val loss: 0.0410 score: 0.9796 time: 0.07s
Test loss: 0.2314 score: 0.9184 time: 0.06s
Epoch 605/1000, LR 0.000093
Train loss: 0.5152;  Loss pred: 0.5152; Loss self: 0.0000; time: 0.21s
Val loss: 0.0410 score: 0.9796 time: 0.07s
Test loss: 0.2315 score: 0.9184 time: 0.06s
Epoch 606/1000, LR 0.000093
Train loss: 0.5165;  Loss pred: 0.5165; Loss self: 0.0000; time: 0.21s
Val loss: 0.0409 score: 0.9796 time: 0.07s
Test loss: 0.2318 score: 0.9184 time: 0.06s
Epoch 607/1000, LR 0.000092
Train loss: 0.5157;  Loss pred: 0.5157; Loss self: 0.0000; time: 0.21s
Val loss: 0.0409 score: 0.9796 time: 0.08s
Test loss: 0.2320 score: 0.9184 time: 0.06s
Epoch 608/1000, LR 0.000092
Train loss: 0.5166;  Loss pred: 0.5166; Loss self: 0.0000; time: 0.21s
Val loss: 0.0409 score: 0.9796 time: 0.07s
Test loss: 0.2323 score: 0.9184 time: 0.06s
Epoch 609/1000, LR 0.000092
Train loss: 0.5153;  Loss pred: 0.5153; Loss self: 0.0000; time: 0.21s
Val loss: 0.0409 score: 0.9796 time: 0.07s
Test loss: 0.2326 score: 0.9184 time: 0.06s
Epoch 610/1000, LR 0.000091
Train loss: 0.5133;  Loss pred: 0.5133; Loss self: 0.0000; time: 0.21s
Val loss: 0.0409 score: 0.9796 time: 0.07s
Test loss: 0.2328 score: 0.9184 time: 0.06s
Epoch 611/1000, LR 0.000091
Train loss: 0.5214;  Loss pred: 0.5214; Loss self: 0.0000; time: 0.21s
Val loss: 0.0408 score: 0.9796 time: 0.07s
Test loss: 0.2328 score: 0.9184 time: 0.06s
Epoch 612/1000, LR 0.000090
Train loss: 0.5157;  Loss pred: 0.5157; Loss self: 0.0000; time: 0.21s
Val loss: 0.0408 score: 0.9796 time: 0.07s
Test loss: 0.2328 score: 0.9184 time: 0.06s
Epoch 613/1000, LR 0.000090
Train loss: 0.5151;  Loss pred: 0.5151; Loss self: 0.0000; time: 0.21s
Val loss: 0.0408 score: 0.9796 time: 0.07s
Test loss: 0.2329 score: 0.9184 time: 0.06s
Epoch 614/1000, LR 0.000090
Train loss: 0.5136;  Loss pred: 0.5136; Loss self: 0.0000; time: 0.21s
Val loss: 0.0407 score: 0.9796 time: 0.07s
Test loss: 0.2329 score: 0.9184 time: 0.06s
Epoch 615/1000, LR 0.000089
Train loss: 0.5144;  Loss pred: 0.5144; Loss self: 0.0000; time: 2.23s
Val loss: 0.0407 score: 0.9796 time: 1.16s
Test loss: 0.2330 score: 0.9184 time: 1.21s
Epoch 616/1000, LR 0.000089
Train loss: 0.5165;  Loss pred: 0.5165; Loss self: 0.0000; time: 2.66s
Val loss: 0.0406 score: 0.9796 time: 0.51s
Test loss: 0.2330 score: 0.9184 time: 0.23s
Epoch 617/1000, LR 0.000088
Train loss: 0.5154;  Loss pred: 0.5154; Loss self: 0.0000; time: 0.35s
Val loss: 0.0406 score: 0.9796 time: 0.07s
Test loss: 0.2329 score: 0.9184 time: 0.06s
Epoch 618/1000, LR 0.000088
Train loss: 0.5150;  Loss pred: 0.5150; Loss self: 0.0000; time: 0.22s
Val loss: 0.0406 score: 0.9796 time: 0.07s
Test loss: 0.2329 score: 0.9184 time: 0.06s
Epoch 619/1000, LR 0.000088
Train loss: 0.5144;  Loss pred: 0.5144; Loss self: 0.0000; time: 0.22s
Val loss: 0.0405 score: 0.9796 time: 0.08s
Test loss: 0.2328 score: 0.9184 time: 0.06s
Epoch 620/1000, LR 0.000087
Train loss: 0.5162;  Loss pred: 0.5162; Loss self: 0.0000; time: 0.21s
Val loss: 0.0405 score: 0.9796 time: 0.07s
Test loss: 0.2328 score: 0.9184 time: 0.06s
Epoch 621/1000, LR 0.000087
Train loss: 0.5160;  Loss pred: 0.5160; Loss self: 0.0000; time: 0.21s
Val loss: 0.0404 score: 0.9796 time: 0.06s
Test loss: 0.2328 score: 0.9184 time: 0.06s
Epoch 622/1000, LR 0.000086
Train loss: 0.5128;  Loss pred: 0.5128; Loss self: 0.0000; time: 0.20s
Val loss: 0.0404 score: 0.9796 time: 0.07s
Test loss: 0.2328 score: 0.9184 time: 0.06s
Epoch 623/1000, LR 0.000086
Train loss: 0.5147;  Loss pred: 0.5147; Loss self: 0.0000; time: 0.21s
Val loss: 0.0404 score: 0.9796 time: 1.66s
Test loss: 0.2329 score: 0.9184 time: 0.93s
Epoch 624/1000, LR 0.000086
Train loss: 0.5133;  Loss pred: 0.5133; Loss self: 0.0000; time: 5.12s
Val loss: 0.0403 score: 0.9796 time: 0.19s
Test loss: 0.2330 score: 0.9184 time: 0.18s
Epoch 625/1000, LR 0.000085
Train loss: 0.5132;  Loss pred: 0.5132; Loss self: 0.0000; time: 0.45s
Val loss: 0.0403 score: 1.0000 time: 0.08s
Test loss: 0.2331 score: 0.9184 time: 0.07s
Epoch 626/1000, LR 0.000085
Train loss: 0.5139;  Loss pred: 0.5139; Loss self: 0.0000; time: 0.20s
Val loss: 0.0403 score: 1.0000 time: 0.06s
Test loss: 0.2331 score: 0.9184 time: 0.06s
Epoch 627/1000, LR 0.000084
Train loss: 0.5162;  Loss pred: 0.5162; Loss self: 0.0000; time: 0.19s
Val loss: 0.0402 score: 1.0000 time: 0.06s
Test loss: 0.2331 score: 0.9184 time: 0.06s
Epoch 628/1000, LR 0.000084
Train loss: 0.5149;  Loss pred: 0.5149; Loss self: 0.0000; time: 0.19s
Val loss: 0.0402 score: 1.0000 time: 0.07s
Test loss: 0.2331 score: 0.9184 time: 0.06s
Epoch 629/1000, LR 0.000084
Train loss: 0.5146;  Loss pred: 0.5146; Loss self: 0.0000; time: 0.19s
Val loss: 0.0402 score: 1.0000 time: 0.06s
Test loss: 0.2332 score: 0.9184 time: 0.06s
Epoch 630/1000, LR 0.000083
Train loss: 0.5118;  Loss pred: 0.5118; Loss self: 0.0000; time: 0.19s
Val loss: 0.0401 score: 1.0000 time: 0.06s
Test loss: 0.2333 score: 0.9184 time: 0.06s
Epoch 631/1000, LR 0.000083
Train loss: 0.5137;  Loss pred: 0.5137; Loss self: 0.0000; time: 0.19s
Val loss: 0.0401 score: 1.0000 time: 0.07s
Test loss: 0.2333 score: 0.9184 time: 0.06s
Epoch 632/1000, LR 0.000082
Train loss: 0.5135;  Loss pred: 0.5135; Loss self: 0.0000; time: 0.19s
Val loss: 0.0401 score: 1.0000 time: 0.07s
Test loss: 0.2334 score: 0.9184 time: 0.06s
Epoch 633/1000, LR 0.000082
Train loss: 0.5132;  Loss pred: 0.5132; Loss self: 0.0000; time: 0.19s
Val loss: 0.0401 score: 1.0000 time: 0.06s
Test loss: 0.2335 score: 0.9184 time: 0.06s
Epoch 634/1000, LR 0.000082
Train loss: 0.5138;  Loss pred: 0.5138; Loss self: 0.0000; time: 0.19s
Val loss: 0.0400 score: 1.0000 time: 0.06s
Test loss: 0.2337 score: 0.9184 time: 0.06s
Epoch 635/1000, LR 0.000081
Train loss: 0.5151;  Loss pred: 0.5151; Loss self: 0.0000; time: 0.20s
Val loss: 0.0400 score: 1.0000 time: 0.07s
Test loss: 0.2337 score: 0.9184 time: 0.06s
Epoch 636/1000, LR 0.000081
Train loss: 0.5139;  Loss pred: 0.5139; Loss self: 0.0000; time: 0.20s
Val loss: 0.0400 score: 1.0000 time: 0.07s
Test loss: 0.2337 score: 0.9184 time: 0.09s
Epoch 637/1000, LR 0.000080
Train loss: 0.5145;  Loss pred: 0.5145; Loss self: 0.0000; time: 0.24s
Val loss: 0.0399 score: 1.0000 time: 0.08s
Test loss: 0.2337 score: 0.9184 time: 0.07s
Epoch 638/1000, LR 0.000080
Train loss: 0.5123;  Loss pred: 0.5123; Loss self: 0.0000; time: 0.21s
Val loss: 0.0399 score: 1.0000 time: 0.07s
Test loss: 0.2337 score: 0.9184 time: 0.06s
Epoch 639/1000, LR 0.000080
Train loss: 0.5128;  Loss pred: 0.5128; Loss self: 0.0000; time: 0.20s
Val loss: 0.0399 score: 1.0000 time: 0.07s
Test loss: 0.2337 score: 0.9184 time: 0.06s
Epoch 640/1000, LR 0.000079
Train loss: 0.5130;  Loss pred: 0.5130; Loss self: 0.0000; time: 0.21s
Val loss: 0.0398 score: 1.0000 time: 0.07s
Test loss: 0.2338 score: 0.9184 time: 0.06s
Epoch 641/1000, LR 0.000079
Train loss: 0.5138;  Loss pred: 0.5138; Loss self: 0.0000; time: 0.51s
Val loss: 0.0398 score: 1.0000 time: 1.84s
Test loss: 0.2339 score: 0.9184 time: 0.89s
Epoch 642/1000, LR 0.000079
Train loss: 0.5127;  Loss pred: 0.5127; Loss self: 0.0000; time: 3.36s
Val loss: 0.0398 score: 1.0000 time: 0.46s
Test loss: 0.2340 score: 0.9184 time: 0.07s
Epoch 643/1000, LR 0.000078
Train loss: 0.5145;  Loss pred: 0.5145; Loss self: 0.0000; time: 0.21s
Val loss: 0.0398 score: 1.0000 time: 0.06s
Test loss: 0.2340 score: 0.9184 time: 0.06s
Epoch 644/1000, LR 0.000078
Train loss: 0.5147;  Loss pred: 0.5147; Loss self: 0.0000; time: 0.20s
Val loss: 0.0397 score: 1.0000 time: 0.06s
Test loss: 0.2341 score: 0.9184 time: 0.06s
Epoch 645/1000, LR 0.000077
Train loss: 0.5134;  Loss pred: 0.5134; Loss self: 0.0000; time: 0.19s
Val loss: 0.0397 score: 1.0000 time: 0.07s
Test loss: 0.2342 score: 0.9184 time: 0.06s
Epoch 646/1000, LR 0.000077
Train loss: 0.5128;  Loss pred: 0.5128; Loss self: 0.0000; time: 0.20s
Val loss: 0.0397 score: 1.0000 time: 0.06s
Test loss: 0.2343 score: 0.9184 time: 0.06s
Epoch 647/1000, LR 0.000077
Train loss: 0.5127;  Loss pred: 0.5127; Loss self: 0.0000; time: 0.20s
Val loss: 0.0397 score: 1.0000 time: 0.07s
Test loss: 0.2343 score: 0.9184 time: 0.06s
Epoch 648/1000, LR 0.000076
Train loss: 0.5125;  Loss pred: 0.5125; Loss self: 0.0000; time: 0.20s
Val loss: 0.0396 score: 1.0000 time: 0.06s
Test loss: 0.2344 score: 0.9184 time: 0.06s
Epoch 649/1000, LR 0.000076
Train loss: 0.5132;  Loss pred: 0.5132; Loss self: 0.0000; time: 1.87s
Val loss: 0.0396 score: 1.0000 time: 1.35s
Test loss: 0.2345 score: 0.9184 time: 1.03s
Epoch 650/1000, LR 0.000075
Train loss: 0.5141;  Loss pred: 0.5141; Loss self: 0.0000; time: 5.88s
Val loss: 0.0396 score: 1.0000 time: 1.26s
Test loss: 0.2345 score: 0.9184 time: 1.18s
Epoch 651/1000, LR 0.000075
Train loss: 0.5119;  Loss pred: 0.5119; Loss self: 0.0000; time: 4.23s
Val loss: 0.0395 score: 1.0000 time: 0.28s
Test loss: 0.2345 score: 0.9184 time: 0.19s
Epoch 652/1000, LR 0.000075
Train loss: 0.5105;  Loss pred: 0.5105; Loss self: 0.0000; time: 0.20s
Val loss: 0.0395 score: 1.0000 time: 0.07s
Test loss: 0.2344 score: 0.9184 time: 0.06s
Epoch 653/1000, LR 0.000074
Train loss: 0.5116;  Loss pred: 0.5116; Loss self: 0.0000; time: 0.20s
Val loss: 0.0395 score: 1.0000 time: 0.07s
Test loss: 0.2344 score: 0.9184 time: 0.06s
Epoch 654/1000, LR 0.000074
Train loss: 0.5106;  Loss pred: 0.5106; Loss self: 0.0000; time: 0.19s
Val loss: 0.0394 score: 1.0000 time: 0.07s
Test loss: 0.2343 score: 0.9184 time: 0.06s
Epoch 655/1000, LR 0.000074
Train loss: 0.5118;  Loss pred: 0.5118; Loss self: 0.0000; time: 0.19s
Val loss: 0.0394 score: 1.0000 time: 0.08s
Test loss: 0.2343 score: 0.9184 time: 0.06s
Epoch 656/1000, LR 0.000073
Train loss: 0.5134;  Loss pred: 0.5134; Loss self: 0.0000; time: 0.21s
Val loss: 0.0394 score: 1.0000 time: 0.07s
Test loss: 0.2343 score: 0.9184 time: 0.06s
Epoch 657/1000, LR 0.000073
Train loss: 0.5124;  Loss pred: 0.5124; Loss self: 0.0000; time: 0.20s
Val loss: 0.0393 score: 1.0000 time: 0.06s
Test loss: 0.2343 score: 0.9184 time: 0.06s
Epoch 658/1000, LR 0.000072
Train loss: 0.5128;  Loss pred: 0.5128; Loss self: 0.0000; time: 0.19s
Val loss: 0.0393 score: 1.0000 time: 0.06s
Test loss: 0.2343 score: 0.9184 time: 0.06s
Epoch 659/1000, LR 0.000072
Train loss: 0.5124;  Loss pred: 0.5124; Loss self: 0.0000; time: 0.20s
Val loss: 0.0393 score: 1.0000 time: 0.07s
Test loss: 0.2343 score: 0.9184 time: 0.06s
Epoch 660/1000, LR 0.000072
Train loss: 0.5127;  Loss pred: 0.5127; Loss self: 0.0000; time: 0.21s
Val loss: 0.0392 score: 1.0000 time: 0.07s
Test loss: 0.2342 score: 0.9184 time: 0.06s
Epoch 661/1000, LR 0.000071
Train loss: 0.5113;  Loss pred: 0.5113; Loss self: 0.0000; time: 0.21s
Val loss: 0.0392 score: 1.0000 time: 0.07s
Test loss: 0.2341 score: 0.9184 time: 0.06s
Epoch 662/1000, LR 0.000071
Train loss: 0.5117;  Loss pred: 0.5117; Loss self: 0.0000; time: 0.21s
Val loss: 0.0391 score: 1.0000 time: 0.07s
Test loss: 0.2340 score: 0.9184 time: 0.06s
Epoch 663/1000, LR 0.000070
Train loss: 0.5128;  Loss pred: 0.5128; Loss self: 0.0000; time: 0.21s
Val loss: 0.0391 score: 1.0000 time: 0.08s
Test loss: 0.2339 score: 0.9184 time: 0.06s
Epoch 664/1000, LR 0.000070
Train loss: 0.5126;  Loss pred: 0.5126; Loss self: 0.0000; time: 0.21s
Val loss: 0.0391 score: 1.0000 time: 0.07s
Test loss: 0.2338 score: 0.9184 time: 0.06s
Epoch 665/1000, LR 0.000070
Train loss: 0.5111;  Loss pred: 0.5111; Loss self: 0.0000; time: 0.21s
Val loss: 0.0390 score: 1.0000 time: 0.07s
Test loss: 0.2338 score: 0.9184 time: 0.06s
Epoch 666/1000, LR 0.000069
Train loss: 0.5124;  Loss pred: 0.5124; Loss self: 0.0000; time: 0.21s
Val loss: 0.0390 score: 1.0000 time: 0.07s
Test loss: 0.2337 score: 0.9184 time: 0.19s
Epoch 667/1000, LR 0.000069
Train loss: 0.5128;  Loss pred: 0.5128; Loss self: 0.0000; time: 3.35s
Val loss: 0.0390 score: 1.0000 time: 1.56s
Test loss: 0.2337 score: 0.9184 time: 1.57s
Epoch 668/1000, LR 0.000069
Train loss: 0.5116;  Loss pred: 0.5116; Loss self: 0.0000; time: 3.51s
Val loss: 0.0389 score: 1.0000 time: 0.77s
Test loss: 0.2337 score: 0.9184 time: 0.89s
Epoch 669/1000, LR 0.000068
Train loss: 0.5117;  Loss pred: 0.5117; Loss self: 0.0000; time: 0.69s
Val loss: 0.0389 score: 1.0000 time: 0.08s
Test loss: 0.2338 score: 0.9184 time: 0.07s
Epoch 670/1000, LR 0.000068
Train loss: 0.5134;  Loss pred: 0.5134; Loss self: 0.0000; time: 0.22s
Val loss: 0.0389 score: 1.0000 time: 0.07s
Test loss: 0.2338 score: 0.9184 time: 0.07s
Epoch 671/1000, LR 0.000068
Train loss: 0.5136;  Loss pred: 0.5136; Loss self: 0.0000; time: 0.22s
Val loss: 0.0389 score: 1.0000 time: 0.07s
Test loss: 0.2339 score: 0.9184 time: 0.06s
Epoch 672/1000, LR 0.000067
Train loss: 0.5099;  Loss pred: 0.5099; Loss self: 0.0000; time: 0.21s
Val loss: 0.0388 score: 1.0000 time: 0.06s
Test loss: 0.2340 score: 0.9184 time: 0.06s
Epoch 673/1000, LR 0.000067
Train loss: 0.5099;  Loss pred: 0.5099; Loss self: 0.0000; time: 0.19s
Val loss: 0.0388 score: 1.0000 time: 0.06s
Test loss: 0.2341 score: 0.9184 time: 0.06s
Epoch 674/1000, LR 0.000066
Train loss: 0.5114;  Loss pred: 0.5114; Loss self: 0.0000; time: 0.19s
Val loss: 0.0388 score: 1.0000 time: 0.06s
Test loss: 0.2341 score: 0.9184 time: 0.06s
Epoch 675/1000, LR 0.000066
Train loss: 0.5135;  Loss pred: 0.5135; Loss self: 0.0000; time: 0.20s
Val loss: 0.0388 score: 1.0000 time: 0.07s
Test loss: 0.2342 score: 0.9184 time: 0.06s
Epoch 676/1000, LR 0.000066
Train loss: 0.5134;  Loss pred: 0.5134; Loss self: 0.0000; time: 0.21s
Val loss: 0.0388 score: 1.0000 time: 0.07s
Test loss: 0.2343 score: 0.9184 time: 0.06s
Epoch 677/1000, LR 0.000065
Train loss: 0.5099;  Loss pred: 0.5099; Loss self: 0.0000; time: 0.20s
Val loss: 0.0387 score: 1.0000 time: 0.07s
Test loss: 0.2343 score: 0.9184 time: 0.06s
Epoch 678/1000, LR 0.000065
Train loss: 0.5112;  Loss pred: 0.5112; Loss self: 0.0000; time: 0.20s
Val loss: 0.0387 score: 1.0000 time: 0.07s
Test loss: 0.2343 score: 0.9184 time: 0.06s
Epoch 679/1000, LR 0.000065
Train loss: 0.5146;  Loss pred: 0.5146; Loss self: 0.0000; time: 0.20s
Val loss: 0.0387 score: 1.0000 time: 0.07s
Test loss: 0.2344 score: 0.9184 time: 0.06s
Epoch 680/1000, LR 0.000064
Train loss: 0.5119;  Loss pred: 0.5119; Loss self: 0.0000; time: 0.20s
Val loss: 0.0387 score: 1.0000 time: 0.07s
Test loss: 0.2344 score: 0.9184 time: 0.06s
Epoch 681/1000, LR 0.000064
Train loss: 0.5126;  Loss pred: 0.5126; Loss self: 0.0000; time: 0.20s
Val loss: 0.0387 score: 1.0000 time: 0.07s
Test loss: 0.2345 score: 0.9184 time: 0.06s
Epoch 682/1000, LR 0.000063
Train loss: 0.5151;  Loss pred: 0.5151; Loss self: 0.0000; time: 0.59s
Val loss: 0.0386 score: 1.0000 time: 0.96s
Test loss: 0.2345 score: 0.9184 time: 1.00s
Epoch 683/1000, LR 0.000063
Train loss: 0.5097;  Loss pred: 0.5097; Loss self: 0.0000; time: 2.03s
Val loss: 0.0386 score: 1.0000 time: 0.30s
Test loss: 0.2345 score: 0.9184 time: 0.23s
Epoch 684/1000, LR 0.000063
Train loss: 0.5122;  Loss pred: 0.5122; Loss self: 0.0000; time: 0.74s
Val loss: 0.0386 score: 1.0000 time: 0.20s
Test loss: 0.2345 score: 0.9184 time: 0.21s
Epoch 685/1000, LR 0.000062
Train loss: 0.5116;  Loss pred: 0.5116; Loss self: 0.0000; time: 0.96s
Val loss: 0.0386 score: 1.0000 time: 0.07s
Test loss: 0.2344 score: 0.9184 time: 0.06s
Epoch 686/1000, LR 0.000062
Train loss: 0.5108;  Loss pred: 0.5108; Loss self: 0.0000; time: 0.21s
Val loss: 0.0385 score: 1.0000 time: 0.07s
Test loss: 0.2344 score: 0.9184 time: 0.06s
Epoch 687/1000, LR 0.000062
Train loss: 0.5107;  Loss pred: 0.5107; Loss self: 0.0000; time: 0.21s
Val loss: 0.0385 score: 1.0000 time: 0.07s
Test loss: 0.2344 score: 0.9184 time: 0.06s
Epoch 688/1000, LR 0.000061
Train loss: 0.5148;  Loss pred: 0.5148; Loss self: 0.0000; time: 0.21s
Val loss: 0.0385 score: 1.0000 time: 0.07s
Test loss: 0.2343 score: 0.9184 time: 0.06s
Epoch 689/1000, LR 0.000061
Train loss: 0.5132;  Loss pred: 0.5132; Loss self: 0.0000; time: 0.21s
Val loss: 0.0384 score: 1.0000 time: 0.07s
Test loss: 0.2342 score: 0.9184 time: 0.06s
Epoch 690/1000, LR 0.000061
Train loss: 0.5123;  Loss pred: 0.5123; Loss self: 0.0000; time: 0.20s
Val loss: 0.0384 score: 1.0000 time: 0.07s
Test loss: 0.2341 score: 0.9184 time: 0.06s
Epoch 691/1000, LR 0.000060
Train loss: 0.5122;  Loss pred: 0.5122; Loss self: 0.0000; time: 0.19s
Val loss: 0.0384 score: 1.0000 time: 0.07s
Test loss: 0.2340 score: 0.9184 time: 0.06s
Epoch 692/1000, LR 0.000060
Train loss: 0.5114;  Loss pred: 0.5114; Loss self: 0.0000; time: 0.20s
Val loss: 0.0384 score: 1.0000 time: 0.07s
Test loss: 0.2340 score: 0.9184 time: 0.06s
Epoch 693/1000, LR 0.000060
Train loss: 0.5137;  Loss pred: 0.5137; Loss self: 0.0000; time: 0.20s
Val loss: 0.0383 score: 1.0000 time: 0.07s
Test loss: 0.2339 score: 0.9184 time: 0.06s
Epoch 694/1000, LR 0.000059
Train loss: 0.5122;  Loss pred: 0.5122; Loss self: 0.0000; time: 0.29s
Val loss: 0.0383 score: 1.0000 time: 0.07s
Test loss: 0.2338 score: 0.9184 time: 0.06s
Epoch 695/1000, LR 0.000059
Train loss: 0.5120;  Loss pred: 0.5120; Loss self: 0.0000; time: 0.19s
Val loss: 0.0383 score: 1.0000 time: 0.07s
Test loss: 0.2337 score: 0.9184 time: 0.06s
Epoch 696/1000, LR 0.000058
Train loss: 0.5092;  Loss pred: 0.5092; Loss self: 0.0000; time: 0.20s
Val loss: 0.0382 score: 1.0000 time: 0.07s
Test loss: 0.2335 score: 0.9184 time: 0.06s
Epoch 697/1000, LR 0.000058
Train loss: 0.5116;  Loss pred: 0.5116; Loss self: 0.0000; time: 0.20s
Val loss: 0.0382 score: 1.0000 time: 0.07s
Test loss: 0.2333 score: 0.9184 time: 0.06s
Epoch 698/1000, LR 0.000058
Train loss: 0.5115;  Loss pred: 0.5115; Loss self: 0.0000; time: 0.20s
Val loss: 0.0381 score: 1.0000 time: 0.07s
Test loss: 0.2332 score: 0.9184 time: 0.06s
Epoch 699/1000, LR 0.000057
Train loss: 0.5105;  Loss pred: 0.5105; Loss self: 0.0000; time: 0.22s
Val loss: 0.0381 score: 1.0000 time: 0.09s
Test loss: 0.2332 score: 0.9184 time: 0.06s
Epoch 700/1000, LR 0.000057
Train loss: 0.5098;  Loss pred: 0.5098; Loss self: 0.0000; time: 0.20s
Val loss: 0.0381 score: 1.0000 time: 0.07s
Test loss: 0.2331 score: 0.9184 time: 0.06s
Epoch 701/1000, LR 0.000057
Train loss: 0.5118;  Loss pred: 0.5118; Loss self: 0.0000; time: 0.20s
Val loss: 0.0381 score: 1.0000 time: 0.07s
Test loss: 0.2331 score: 0.9184 time: 0.06s
Epoch 702/1000, LR 0.000056
Train loss: 0.5122;  Loss pred: 0.5122; Loss self: 0.0000; time: 0.20s
Val loss: 0.0380 score: 1.0000 time: 0.08s
Test loss: 0.2331 score: 0.9184 time: 2.16s
Epoch 703/1000, LR 0.000056
Train loss: 0.5118;  Loss pred: 0.5118; Loss self: 0.0000; time: 5.24s
Val loss: 0.0380 score: 1.0000 time: 0.07s
Test loss: 0.2331 score: 0.9184 time: 0.06s
Epoch 704/1000, LR 0.000056
Train loss: 0.5096;  Loss pred: 0.5096; Loss self: 0.0000; time: 0.21s
Val loss: 0.0380 score: 1.0000 time: 0.07s
Test loss: 0.2332 score: 0.9184 time: 0.07s
Epoch 705/1000, LR 0.000055
Train loss: 0.5108;  Loss pred: 0.5108; Loss self: 0.0000; time: 0.21s
Val loss: 0.0380 score: 1.0000 time: 0.07s
Test loss: 0.2332 score: 0.9184 time: 0.06s
Epoch 706/1000, LR 0.000055
Train loss: 0.5122;  Loss pred: 0.5122; Loss self: 0.0000; time: 0.21s
Val loss: 0.0380 score: 1.0000 time: 0.07s
Test loss: 0.2332 score: 0.9184 time: 0.06s
Epoch 707/1000, LR 0.000055
Train loss: 0.5115;  Loss pred: 0.5115; Loss self: 0.0000; time: 0.20s
Val loss: 0.0380 score: 1.0000 time: 0.06s
Test loss: 0.2333 score: 0.9184 time: 0.06s
Epoch 708/1000, LR 0.000054
Train loss: 0.5090;  Loss pred: 0.5090; Loss self: 0.0000; time: 0.20s
Val loss: 0.0379 score: 1.0000 time: 0.06s
Test loss: 0.2334 score: 0.9184 time: 0.06s
Epoch 709/1000, LR 0.000054
Train loss: 0.5109;  Loss pred: 0.5109; Loss self: 0.0000; time: 0.19s
Val loss: 0.0379 score: 1.0000 time: 0.06s
Test loss: 0.2335 score: 0.9184 time: 0.06s
Epoch 710/1000, LR 0.000054
Train loss: 0.5093;  Loss pred: 0.5093; Loss self: 0.0000; time: 0.19s
Val loss: 0.0379 score: 1.0000 time: 0.06s
Test loss: 0.2335 score: 0.9184 time: 0.06s
Epoch 711/1000, LR 0.000053
Train loss: 0.5109;  Loss pred: 0.5109; Loss self: 0.0000; time: 0.19s
Val loss: 0.0379 score: 1.0000 time: 0.06s
Test loss: 0.2337 score: 0.9184 time: 0.06s
Epoch 712/1000, LR 0.000053
Train loss: 0.5104;  Loss pred: 0.5104; Loss self: 0.0000; time: 5.07s
Val loss: 0.0379 score: 1.0000 time: 0.50s
Test loss: 0.2338 score: 0.9184 time: 1.37s
Epoch 713/1000, LR 0.000053
Train loss: 0.5106;  Loss pred: 0.5106; Loss self: 0.0000; time: 1.78s
Val loss: 0.0379 score: 1.0000 time: 0.35s
Test loss: 0.2340 score: 0.9184 time: 0.19s
Epoch 714/1000, LR 0.000052
Train loss: 0.5089;  Loss pred: 0.5089; Loss self: 0.0000; time: 1.12s
Val loss: 0.0379 score: 1.0000 time: 1.01s
Test loss: 0.2341 score: 0.9184 time: 1.68s
Epoch 715/1000, LR 0.000052
Train loss: 0.5117;  Loss pred: 0.5117; Loss self: 0.0000; time: 0.47s
Val loss: 0.0379 score: 1.0000 time: 0.07s
Test loss: 0.2343 score: 0.9184 time: 0.06s
     INFO: Early stopping counter 1 of 2
Epoch 716/1000, LR 0.000052
Train loss: 0.5100;  Loss pred: 0.5100; Loss self: 0.0000; time: 0.21s
Val loss: 0.0379 score: 1.0000 time: 0.07s
Test loss: 0.2345 score: 0.9184 time: 0.06s
     INFO: Early stopping counter 2 of 2
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 713,   Train_Loss: 0.5089,   Val_Loss: 0.0379,   Val_Precision: 1.0000,   Val_Recall: 1.0000,   Val_accuracy: 1.0000,   Val_Score: 1.0000,   Val_Loss: 0.0379,   Test_Precision: 0.9565,   Test_Recall: 0.8800,   Test_accuracy: 0.9167,   Test_Score: 0.9184,   Test_loss: 0.2341


[1.1250675929477438, 0.07260138390120119, 0.06517546193208545, 0.06942169705871493, 0.06307711405679584, 0.06370937905739993, 0.06353496201336384, 0.06884501304011792, 0.0664323780220002, 0.06322127894964069, 0.06440213706810027, 0.0631538750603795, 0.06382796703837812, 0.06595662399195135, 0.0696884390199557, 0.06860149395652115, 0.0696936259046197, 0.07671907800249755, 0.06994366110302508, 0.08053676399867982, 1.5170276639983058, 0.3635215579997748, 0.19728804100304842, 0.09975753107573837, 0.06620932999067008, 0.06520154199097306, 0.06493534008041024, 0.06481671601068228, 0.06553994701243937, 0.06519350199960172, 0.06751241302117705, 0.06698870600666851, 0.07034255308099091, 0.06904265598859638, 0.7079291759291664, 0.29737150797154754, 0.29062273900490254, 0.10017506603617221, 0.06606452004052699, 0.06941800401546061, 0.06561793701257557, 0.06805465393699706, 0.06500946800224483, 0.06617417791858315, 0.06593629496637732, 0.9478009770391509, 0.12139186600688845, 0.07250222493894398, 0.06941007892601192, 0.0684408110100776, 0.06417788797989488, 0.06419697601813823, 0.06421515892725438, 0.06461732892785221, 0.06484952592290938, 0.06799174600746483, 0.1487254409585148, 0.06940725794993341, 0.06884129496756941, 2.1710067989770323, 0.074032180942595, 0.07430030300747603, 0.0638693260261789, 0.06624759593978524, 0.06461389001924545, 0.06443172099534422, 0.06254479999188334, 0.06303938594646752, 1.4150545729789883, 0.07174125104211271, 0.07600608200300485, 0.06253405101597309, 0.061866942909546196, 0.06246624398045242, 0.06197469704784453, 0.06184293807018548, 0.061756442999467254, 0.06228293105959892, 0.06297286006156355, 0.07575407798867673, 1.3042752699693665, 0.6072317759972066, 0.3420649969484657, 0.06774329300969839, 0.06546581897418946, 0.06285132595803589, 0.06252083706203848, 0.06223062006756663, 0.07395434298086911, 0.061848468030802906, 0.06118969200178981, 0.8000162750249729, 0.4628510350594297, 0.15736174397170544, 0.06282533193007112, 0.06248521606903523, 0.08036140399053693, 0.06135073699988425, 0.06133868300821632, 0.06053328700363636, 0.06225241406355053, 0.0741469879867509, 0.07334171095862985, 0.06444651598576456, 0.31430238694883883, 0.2978845089673996, 0.06520012801047415, 0.06237223604694009, 0.06152717105578631, 0.06212544091977179, 0.06123218103311956, 0.06145436700899154, 0.06125844200141728, 0.06195212004240602, 0.06192778202239424, 0.062011402915231884, 0.06177784607280046, 1.0857283340301365, 0.6311360759427771, 0.06474093592260033, 0.06514206901192665, 0.06190473202150315, 0.06076760496944189, 0.06565935001708567, 0.06444231909699738, 2.058738171006553, 0.08766032301355153, 0.062314005102962255, 0.06163706898223609, 0.06266388692893088, 0.062070864951238036, 0.06268804601859301, 0.06217115093022585, 0.06176924810279161, 0.43242676998488605, 0.8568806980038062, 0.06983314291574061, 0.0669098700163886, 0.0668419529683888, 0.06791843893006444, 0.0666529139271006, 0.06768140709027648, 0.0675741529557854, 0.6992912340210751, 0.9380899249808863, 0.30850372300483286, 0.06733186496421695, 0.06785435602068901, 0.07248535007238388, 0.06836368003860116, 0.06794471910689026, 0.07666253496427089, 0.06402595504187047, 0.06210166204255074, 0.6722918150480837, 0.47543843195308, 0.061514504021033645, 0.06054721400141716, 0.06924343691207469, 0.06474179995711893, 0.061351225012913346, 0.05998940800782293, 0.06007451505865902, 0.06028251093812287, 0.0598492439603433, 0.060017738956958055, 0.05985578789841384, 0.06021264009177685, 0.05970510595943779, 0.06027904199436307, 0.05982470000162721, 0.06543243792839348, 0.05934077105484903, 0.05994168506003916, 0.05984782602172345, 0.05864717997610569, 0.05976193107198924, 0.06012161704711616, 0.05980157200247049, 0.06084371905308217, 0.059946853085421026, 0.0603357549989596, 0.06042889796663076, 0.06412903207819909, 0.06010557699482888, 0.06000156106892973, 0.05960321601014584, 0.05996228288859129, 0.06015214091166854, 0.06013369397260249, 0.05975861893966794, 0.060398735920898616, 0.05954533303156495, 0.05992923700250685, 0.059819236979819834, 0.06108355394098908, 0.06309165491256863, 0.06721752695739269, 0.06566394097171724, 0.06768737197853625, 0.06523262697737664, 0.6982098269509152, 0.06177571404259652, 0.061719611985608935, 0.0629792510299012, 0.06152171909343451, 0.0623580280225724, 0.06233565404545516, 0.06348260689992458, 0.06242344295606017, 0.061633193981833756, 0.061667119967751205, 2.172633665963076, 0.4913909339811653, 0.23835222399793565, 0.06477198901120573, 0.06548859097529203, 0.06485605600755662, 0.06487756606657058, 0.06627889804076403, 0.06506447598803788, 0.06486578902695328, 0.06469597294926643, 0.06977198901586235, 0.06554349593352526, 0.49097754491958767, 0.2763960459269583, 0.06564669706858695, 0.06570510508026928, 0.06604870303999633, 0.06545242096763104, 0.06647082604467869, 0.06586214096751064, 0.06563952309079468, 0.06572110892739147, 0.0669107869034633, 0.06583865790162235, 0.06611254892777652, 0.06595169100910425, 0.07578426296822727, 0.20499895501416177, 0.3951002210378647, 0.13119681098032743, 0.06408304907381535, 0.06463857798371464, 0.06356241495814174, 0.06383267894852906, 0.06361543294042349, 0.0633556901011616, 0.06642631895374507, 0.061881675967015326, 0.06180676491931081, 0.062185003072954714, 0.06193888501729816, 0.06811047298833728, 0.06703020399436355, 0.06900497199967504, 0.37440110894385725, 0.18271234398707747, 0.07257488591130823, 0.07048596802633256, 0.06741675094235688, 0.062494186917319894, 0.07266014092601836, 0.06643604999408126, 0.06251480907667428, 0.5620149539317936, 0.0721541449893266, 0.0634066549828276, 0.06760266807395965, 0.06208055396564305, 0.06208280904684216, 0.0625737210502848, 0.7811409650603309, 0.5421252980595455, 0.1619326890213415, 0.06937139690853655, 0.0643412439385429, 0.06343211291823536, 0.0634324150159955, 0.06407666602171957, 0.06345105497166514, 0.06350390298757702, 0.06393551302608103, 0.06286177108995616, 0.06437789998017251, 0.0638196860672906, 0.06329401093535125, 0.06362919404637069, 0.06286765704862773, 0.06354229804128408, 0.06349020299967378, 0.06396328704431653, 0.06944924697745591, 0.7021669310051948, 0.15066916204523295, 0.06510162097401917, 0.06531176902353764, 0.07255030306987464, 0.06306795298587531, 0.06395858398173004, 0.06310978205874562, 0.06659926497377455, 0.06357999797910452, 0.06328763801138848, 0.062329409061931074, 0.0630039309617132, 0.06352573295589536, 0.06293555593583733, 0.06423496291972697, 0.06274548301007599, 0.06322590098716319, 0.06331986794248223, 0.0648027160204947, 0.06367957103066146, 0.06413593690376729, 0.07166974898427725, 0.9351862570038065, 0.5278814040357247, 0.2404966619797051, 0.14699518401175737, 0.0611959109082818, 0.060774209909141064, 0.06006977707147598, 0.059800112969242036, 0.05932479491457343, 0.061905408045277, 0.061308822012506425, 0.07259627396706492, 0.05976037296932191, 1.3797877499600872, 0.5188635989325121, 0.21851876704022288, 0.06615567696280777, 0.06384660303592682, 0.06116173800546676, 0.06827467796392739, 0.06987617397680879, 0.06473501399159431, 0.06559327605646104, 0.06530119699891657, 0.0651976199587807, 0.06506231299135834, 0.48478819394949824, 0.08137928193900734, 0.06303953798487782, 0.06348156393505633, 0.061951122013852, 0.06207906699273735, 0.06345623300876468, 0.06073413696140051, 0.060320374090224504, 0.07411619799677283, 0.059882447007112205, 0.061358314007520676, 0.06121002102736384, 0.06101328495424241, 0.05984595697373152, 0.8562058289535344, 0.36308432603254914, 0.3589624180458486, 0.5360252219252288, 0.27372678997926414, 0.06796180410310626, 0.0674912299728021, 0.0708560849307105, 0.06211181101389229, 0.06362960301339626, 0.06330638704821467, 0.06271806207951158, 0.07145854795817286, 0.7057888769777492, 0.7623719859402627, 0.10197966697160155, 0.06613733107224107, 0.06447841401677579, 0.06481045496184379, 0.0653388419887051, 0.06502651295159012, 0.06511953508015722, 0.06516978004947305, 0.06379691895563155, 0.06480425596237183, 0.06441011803690344, 0.06505688501056284, 0.06518991605844349, 0.06481833499856293, 0.06524338200688362, 0.06536815501749516, 0.5175710839685053, 0.17081168701406568, 0.09227630903478712, 0.06718872010242194, 0.06873557693324983, 0.06324115907773376, 0.060615563997998834, 0.06088417989667505, 0.06154175801202655, 0.06130311009474099, 0.7979504340328276, 0.18750561494380236, 0.06384482304565609, 0.06263048795517534, 0.06634805398061872, 0.062326967949047685, 0.14073967293370515, 0.0660147910239175, 2.0099160009995103, 0.0808089089114219, 0.06659966299775988, 0.06647378799971193, 0.07108314393553883, 0.06576379400212318, 0.06600105902180076, 0.06531793007161468, 0.06609735602978617, 0.06549849105067551, 0.06496734102256596, 0.06590594898443669, 0.06562471692450345, 1.0139631449710578, 0.433356367982924, 0.06455590797122568, 0.06520998408086598, 0.06416238297242671, 0.059874388040043414, 0.06007877201773226, 0.06335243303328753, 0.05923111201263964, 0.05906330293510109, 0.06179066596087068, 0.06517562200315297, 0.06110068899579346, 0.06357699900399894, 0.06334260804578662, 0.06338056398089975, 0.06613795296289027, 0.06298181891907007, 0.0622283429838717, 0.06173609895631671, 0.24811943992972374, 0.185759594081901, 0.10409536701627076, 0.06668517796788365, 0.06078002299182117, 0.06097498210147023, 0.06022069102618843, 0.06519732601009309, 0.06403364206198603, 0.06380565010476857, 0.06574902695138007, 0.06479679408948869, 0.064824333996512, 0.06435351702384651, 1.418682603049092, 0.29788276902399957, 0.06759201700333506, 0.0676653190748766, 0.06681511900387704, 0.06284347898326814, 0.06251368997618556, 0.06201293400954455, 0.06202096305787563, 0.0614965099375695, 0.06415769003797323, 0.061816305038519204, 0.06276234099641442, 0.06648268993012607, 0.06636232510209084, 1.3676926760235801, 0.20484785898588598, 0.19766586902551353, 0.19537242501974106, 0.2756413930328563, 0.15000747703015804, 0.06532892491668463, 0.06613829894922674, 0.06598337495233864, 0.061237829038873315, 0.07381295808590949, 0.06317257205955684, 0.06317669292911887, 2.7187610460678115, 0.43240723898634315, 0.19500290707219392, 0.11767496902029961, 0.06955885293427855, 0.06462281802669168, 0.06456879910547286, 0.0645724548958242, 0.06380702101159841, 0.0641079219058156, 0.06351136404555291, 0.06381946289911866, 0.06390934996306896, 0.151724140974693, 0.13299658906180412, 0.0722929600160569, 0.06101640698034316, 0.0691805180395022, 0.06612590607255697, 0.06057285703718662, 0.06066833401564509, 0.060895478003658354, 0.060905662016011775, 0.06040834600571543, 0.06116679997649044, 0.06011232908349484, 0.059701594058424234, 0.0598566880216822, 0.06166074797511101, 0.060140721034258604, 0.060052155051380396, 0.06230014795437455, 0.05920008500106633, 0.059808498015627265, 0.05946708202827722, 0.060787836904637516, 0.06079749995842576, 0.06194696598686278, 0.060422981972806156, 2.1693912680493668, 0.541298964060843, 0.10760771599598229, 0.0654468759894371, 0.0656769770430401, 0.06301131495274603, 0.06112062104512006, 0.06112696696072817, 0.061334936995990574, 0.06106480897869915, 0.06236384902149439, 0.18522180197760463, 0.07031690201256424, 0.06049346795771271, 0.06062412401661277, 0.06451582501176745, 0.06376780406571925, 0.0659716899972409, 0.06707607896532863, 0.07044851500540972, 0.06702344701625407, 0.06640851206611842, 0.06759932497516274, 0.0678577609360218, 0.06712761893868446, 0.28539070300757885, 0.07402405596803874, 0.06562827399466187, 0.06373321905266494, 0.06374375708401203, 0.06382676400244236, 0.06373048201203346, 0.06348897004500031, 0.06534413993358612, 0.06373709707986563, 0.06358121801167727, 0.06411356804892421, 0.06535728392191231, 0.06000717298593372, 0.06120710202958435, 0.060148693970404565, 0.06120062305126339, 0.06083882204256952, 2.1150936159538105, 0.368608062970452, 0.0948590210173279, 0.06047128001227975, 0.06064203893765807, 0.06907965906430036, 0.06617257604375482, 0.06085759797133505, 0.060929831000976264, 0.060708109056577086, 0.06068915606010705, 0.06709183193743229, 0.07865988602861762, 0.06549027608707547, 0.06592220708262175, 0.06652869191020727, 0.06679576099850237, 0.0658269259147346, 0.06665642594452947, 0.25492203002795577, 0.3399289969820529, 0.37802586797624826, 0.06340789201203734, 0.06104602897539735, 0.060528318979777396, 0.0605004410026595, 0.06157185591291636, 0.07089941401500255, 0.06068425299599767, 0.06139027199242264, 0.060750926961191, 0.06946049607358873, 0.06588819902390242, 0.06702332000713795, 0.06582847202662379, 0.06736704497598112, 0.06668515899218619, 0.06701808096840978, 0.06519622600171715, 0.06570287200156599, 0.06566261500120163, 0.06672699400223792, 0.06684203806798905, 0.066849619965069, 0.06642330100294203, 0.06644856301136315, 0.06727767398115247, 0.06696839106734842, 0.06913681700825691, 0.06599097605794668, 1.216842534020543, 0.23141142399981618, 0.06653583107981831, 0.06703355500940233, 0.06766534992493689, 0.06666724500246346, 0.0711883750045672, 0.06133677205070853, 0.9317941470071673, 0.18492432299535722, 0.07374197989702225, 0.06074041791725904, 0.061293284059502184, 0.06127759697847068, 0.06141822796780616, 0.061016611056402326, 0.0608588510658592, 0.0621505540329963, 0.06192007102072239, 0.06163601402658969, 0.060774766956456006, 0.09486502700019628, 0.07470900297630578, 0.06632820900995284, 0.06397118500899523, 0.06298397295176983, 0.8986464090412483, 0.07054201106075197, 0.06135188799817115, 0.06086431606672704, 0.06159884494263679, 0.060601128032431006, 0.0630223749903962, 0.06175465299747884, 1.0376595898997039, 1.1805398190626875, 0.19654598808847368, 0.06169412098824978, 0.06114864500705153, 0.06086521397810429, 0.0612690020352602, 0.061586212017573416, 0.0614949120208621, 0.06183173099998385, 0.06508620001841336, 0.06756043108180165, 0.06710878608282655, 0.0652913780650124, 0.06585067603737116, 0.06622012599837035, 0.06615473004058003, 0.19429152796510607, 1.572006318019703, 0.8984209460904822, 0.0703990840120241, 0.06987040501553565, 0.06868693500291556, 0.06140254100318998, 0.06037580000702292, 0.06282650399953127, 0.06465791095979512, 0.06457310903351754, 0.06530521798413247, 0.06547046301420778, 0.06417581008281559, 0.06619932199828327, 0.06465087702963501, 1.0072673639515415, 0.2331233989680186, 0.21133621397893876, 0.06545217102393508, 0.06545846699737012, 0.06475087604485452, 0.06491228600498289, 0.06490437395405024, 0.06169183901511133, 0.06336515198927373, 0.062162793008610606, 0.06724672100972384, 0.061802469892427325, 0.061814625980332494, 0.06129618105478585, 0.06274635391309857, 0.06070867402013391, 0.06289232207927853, 0.06225197296589613, 0.061824903008528054, 2.161183835938573, 0.06528045202139765, 0.07184788305312395, 0.06509210006333888, 0.06518628902267665, 0.06057037599384785, 0.06496623996645212, 0.06103533296845853, 0.059832219034433365, 0.05989590601529926, 1.370392601005733, 0.19392608909402043, 1.6879422939382493, 0.06515957496594638, 0.06419455690775067]
[0.02296056312138253, 0.0014816608959428814, 0.001330111468001744, 0.001416769327728876, 0.0012872880419754252, 0.0013001914093346925, 0.0012966318778237518, 0.0014050002661248554, 0.0013557628167755141, 0.0012902301826457285, 0.0013143293279204138, 0.0012888545930689695, 0.0013026115722117983, 0.00134605355085615, 0.0014222130412235856, 0.001400030488908595, 0.0014223188960126468, 0.0015656954694387255, 0.001427421655163777, 0.001643607428544486, 0.030959748244863385, 0.007418807306117854, 0.004026286551082621, 0.002035867981137518, 0.0013512108161361242, 0.0013306437141014909, 0.0013252110220491886, 0.0013227901226669854, 0.0013375499390293748, 0.001330479632644933, 0.0013778043473709602, 0.001367116449115684, 0.0014355623077753248, 0.0014090337956856405, 0.014447534202636048, 0.006068806285133624, 0.005931076306222501, 0.0020443891027790246, 0.001348255511031163, 0.0014166939594991961, 0.0013391415716852157, 0.001388870488510144, 0.0013267238367805068, 0.0013504934269098602, 0.0013456386727832106, 0.01934287708243165, 0.002477385020548744, 0.001479637243651918, 0.0014165322229798352, 0.0013967512451036244, 0.001309752815916222, 0.0013101423677171068, 0.0013105134474949874, 0.0013187209985275961, 0.0013234597127124363, 0.001387586653213568, 0.0030352130807860165, 0.0014164746520394574, 0.0014049243870932534, 0.044306261203612904, 0.0015108608355631633, 0.0015163327144382863, 0.0013034556331873244, 0.0013519917538731682, 0.0013186508167192948, 0.0013149330815376372, 0.0012764244896302723, 0.0012865180805401535, 0.028878664754673232, 0.0014641071641247492, 0.0015511445306735684, 0.001276205122774961, 0.0012625906716233917, 0.0012748213057235188, 0.0012647897356702965, 0.0012621007769425608, 0.001260335571417699, 0.0012710802257061005, 0.0012851604094196642, 0.0015460015916056475, 0.026617862652436052, 0.012392485224432787, 0.006980918305070728, 0.0013825161838713958, 0.0013360371219222338, 0.0012826801215925691, 0.0012759354502456834, 0.0012700126544401354, 0.0015092723057320227, 0.001262213633281692, 0.0012487692245263227, 0.016326862755611692, 0.009445939491008769, 0.003211464162687866, 0.001282149631225941, 0.0012752084912048007, 0.0016400286528681005, 0.0012520558571404948, 0.0012518098573105372, 0.001235373204155844, 0.001270457429868378, 0.0015132038364643041, 0.0014967696114006092, 0.0013152350201176442, 0.006414334427527323, 0.006079275693212237, 0.0013306148573566153, 0.0012729027764681652, 0.0012556565521589043, 0.0012678661412198324, 0.0012496363476146848, 0.0012541707552855416, 0.0012501722857432098, 0.0012643289804572658, 0.001263832286171311, 0.0012655388350047323, 0.0012607723688326624, 0.022157721102655847, 0.01288032808046484, 0.0013212435902571495, 0.0013294299798352377, 0.0012633618779898602, 0.0012401552034579978, 0.0013399867350425646, 0.0013151493693264772, 0.042015064714419445, 0.0017889861839500312, 0.0012717143898563726, 0.00125789936698441, 0.0012788548352843036, 0.0012667523459436334, 0.0012793478779304698, 0.0012687989985760376, 0.0012605969000569716, 0.008825036122140532, 0.017487361183751146, 0.00142516618195389, 0.0013655075513548693, 0.0013641214891507917, 0.0013860905904094784, 0.0013602635495326653, 0.0013812532059240099, 0.0013790643460364366, 0.014271249673899492, 0.0191446923465487, 0.006295994347037405, 0.0013741196931472846, 0.0013847827759324288, 0.0014792928586200792, 0.0013951771436449215, 0.0013866269205487808, 0.0015645415298830793, 0.0013066521437116424, 0.0012673808580112396, 0.01372024112343028, 0.009702825141899591, 0.0012553980412455847, 0.0012356574286003501, 0.0014131313655525446, 0.001321261223614672, 0.0012520658165900683, 0.001224273632812713, 0.0012260105114012044, 0.0012302553252678137, 0.0012214131420478225, 0.0012248518154481236, 0.001221546691804364, 0.001228829389628099, 0.0012184715501926079, 0.0012301845304972055, 0.0012209122449311676, 0.001335355876089663, 0.0012110361439765108, 0.00122329969510284, 0.0012213842045249684, 0.001196881224002157, 0.0012196312463671273, 0.0012269717764717583, 0.0012204402449483775, 0.0012417085521037178, 0.0012234051650085924, 0.0012313419387542776, 0.0012332428156455256, 0.0013087557566979406, 0.0012266444284658954, 0.0012245216544679537, 0.001216392163472364, 0.0012237200589508427, 0.0012275947124830314, 0.0012272182443388263, 0.001219563651829958, 0.0012326272636918084, 0.0012152108781952032, 0.0012230456531123848, 0.0012208007546902007, 0.0012466031416528383, 0.0012875847941340537, 0.0013717862644365855, 0.0013400804279942293, 0.0013813749383374744, 0.001331278101579115, 0.014249180141855411, 0.001260728858012174, 0.0012595839180736517, 0.0012852908373449225, 0.0012555452876211125, 0.0012726128167871917, 0.001272156205009289, 0.0012955634061209097, 0.0012739478154297993, 0.001257820285343546, 0.0012585126524030858, 0.044339462570675024, 0.010028386407778884, 0.004864331101998687, 0.0013218773267593008, 0.0013365018566386128, 0.0013235929797460533, 0.0013240319605422567, 0.0013526305722604906, 0.0013278464487354671, 0.0013237916127949649, 0.0013203259785564579, 0.0014239181431808642, 0.0013376223659903115, 0.010019949896318115, 0.005640735631162415, 0.0013397285116038152, 0.00134092051184223, 0.0013479327151019657, 0.00133576369321696, 0.001356547470299565, 0.001344125325867564, 0.001339582103893769, 0.0013412471209671727, 0.0013655262633359858, 0.001343646079624946, 0.0013492356924036024, 0.0013459528777368215, 0.0015466176115964748, 0.0041836521431461585, 0.008063269817099279, 0.002677485938374029, 0.0013078173280370478, 0.0013191546527288702, 0.0012971921420028927, 0.0013027077336434502, 0.0012982741416412958, 0.001292973267370645, 0.0013556391623213279, 0.001262891346265619, 0.00126136254937369, 0.0012690816953664227, 0.0012640588779040441, 0.0013900096528232098, 0.0013679633468237458, 0.0014082647346872457, 0.007640838958037903, 0.0037288233466750507, 0.0014811201206389435, 0.001438489143394542, 0.0013758520600480996, 0.0012753915697412224, 0.0014828600188983338, 0.0013558377549812502, 0.00127581243013621, 0.011469692937383542, 0.001472533571210747, 0.0012940133669964817, 0.0013796462872236663, 0.0012669500809314909, 0.0012669961029967787, 0.0012770147153119348, 0.015941652348170018, 0.011063781593051948, 0.003304748755537582, 0.0014157427940517664, 0.0013130866109906715, 0.0012945329166986809, 0.0012945390819590918, 0.0013076870616677465, 0.001294919489217656, 0.001295998020154633, 0.001304806388287368, 0.0012828932875501259, 0.0013138346934729085, 0.0013024425728018491, 0.0012917145088847195, 0.0012985549805381773, 0.001283013409155668, 0.001296781592679267, 0.001295718428564771, 0.0013053732049860517, 0.001417331570968488, 0.014329937367452954, 0.0030748808580659784, 0.0013286045096738606, 0.0013328932453783192, 0.0014806184299974417, 0.001287101081344394, 0.0013052772241169397, 0.0012879547358927677, 0.0013591686729341745, 0.0012975509791653982, 0.0012915844492120097, 0.0012720287563659403, 0.0012857945094227182, 0.0012964435297121502, 0.001284399100731374, 0.0013109176106066728, 0.0012805200614301221, 0.001290324509942106, 0.0012922422029078007, 0.0013225044085815245, 0.0012995830822583971, 0.001308896671505455, 0.0014626479384546376, 0.019085433816404214, 0.010773089878280096, 0.004908095142442961, 0.0029999017145256606, 0.001248896140985343, 0.001240289998145736, 0.0012259138177852242, 0.0012204104687600415, 0.001210710100297417, 0.001263375674393408, 0.001251200449234825, 0.0014815566115727534, 0.0012195994483535082, 0.028158933672654842, 0.010589053039439023, 0.004459566674290263, 0.0013501158563838322, 0.001302991898692384, 0.001248198734805444, 0.0013933607747740283, 0.0014260443668736487, 0.001321122734522333, 0.0013386382868665518, 0.0013326774897738074, 0.0013305636726281776, 0.0013278023059460887, 0.00989363661121425, 0.0016608016722246396, 0.0012865211833648536, 0.0012955421211235986, 0.0012643086125275918, 0.0012669197345456602, 0.0012950251634441772, 0.0012394721828857247, 0.001231028042657643, 0.0015125754693218944, 0.0012220907552471878, 0.0012522104899494015, 0.001249184102599262, 0.0012451690806988248, 0.0012213460606883984, 0.0174735883459905, 0.007409884204745901, 0.007325763633588747, 0.010939290243372018, 0.005586261019984983, 0.001386975593940944, 0.0013773720402612674, 0.0014460425496063366, 0.0012675879798753529, 0.0012985633268040053, 0.001291967082616626, 0.0012799604506022772, 0.001458337713432099, 0.014403854632198962, 0.015558611957964544, 0.002081217693297991, 0.0013497414504538994, 0.001315886000342363, 0.0013226623461600772, 0.0013334457548715326, 0.0013270716928895942, 0.001328970103676678, 0.0013299955112137357, 0.0013019779378700318, 0.001322535835966772, 0.0013144922048347642, 0.001327691530827813, 0.001330406450172316, 0.0013228231632359782, 0.0013314975919772167, 0.0013340439799488808, 0.01056267518303072, 0.003485952796205422, 0.0018831899803017779, 0.0013711983694371824, 0.001402766876188772, 0.001290635899545587, 0.001237052326489772, 0.0012425342836056132, 0.001255954245143399, 0.00125108387948451, 0.01628470273536383, 0.003826645202934742, 0.0013029555723603284, 0.0012781732235750069, 0.0013540419179718106, 0.0012719789377356671, 0.0028722382231368398, 0.0013472406331411734, 0.04101869389794919, 0.001649161406355549, 0.0013591767958726507, 0.001356607918361468, 0.0014506764068477312, 0.0013421182449412893, 0.0013469603882000154, 0.0013330189810533608, 0.0013489256332609423, 0.0013367038989933779, 0.0013258641025013461, 0.0013450193670293202, 0.0013392799372347643, 0.020693125407572607, 0.008844007509855591, 0.0013174675096168506, 0.001330816001650326, 0.0013094363871923819, 0.0012219262865314983, 0.001226097388116985, 0.0012929067965977046, 0.0012087982043395846, 0.0012053735292877775, 0.0012610339992014425, 0.0013301147347582237, 0.001246952836648846, 0.001297489775591815, 0.0012927062866487065, 0.0012934808975693826, 0.0013497541420998014, 0.0012853432432463278, 0.0012699661833443204, 0.0012599203868636063, 0.005063662039382117, 0.003791012124120429, 0.0021243952452300154, 0.0013609219993445643, 0.0012404086324861463, 0.001244387389825923, 0.0012289936944120089, 0.0013305576736753692, 0.0013068090216731842, 0.0013021561245871137, 0.001341816876558777, 0.0013223835528467078, 0.001322945591765551, 0.0013133370821193165, 0.028952706184675346, 0.006079240184163256, 0.0013794289184354094, 0.0013809248790791144, 0.0013635738572219806, 0.0012825199792503702, 0.0012757895913507258, 0.0012655700818274397, 0.0012657339399566455, 0.0012550308150524388, 0.0013093406130198616, 0.0012615572456840653, 0.0012808641019676412, 0.001356789590410736, 0.0013543331653487925, 0.027912095429052656, 0.004180568550732367, 0.004033997327051297, 0.003987192347341654, 0.005625334551690945, 0.0030613770822481234, 0.001333243365646625, 0.0013497612030454436, 0.0013465994888232375, 0.001249751613038231, 0.0015063868997124384, 0.001289236164480752, 0.0012893202638595688, 0.05548491930750636, 0.008824637530333534, 0.003979651164738651, 0.0024015299800061144, 0.0014195684272301744, 0.0013188330209528913, 0.0013177305939892422, 0.0013178052019555957, 0.0013021841022775186, 0.0013083249368533796, 0.001296150286643937, 0.0013024380183493604, 0.0013042724482258971, 0.003096411040299857, 0.002714216103302125, 0.0014753665309399366, 0.0012452327955172074, 0.0014118473069286163, 0.0013495082871950402, 0.0012361807558609514, 0.0012381292656254098, 0.0012427648572175174, 0.001242972694204322, 0.0012328233878717435, 0.0012483020403365396, 0.0012267822261937723, 0.0012183998787433517, 0.0012215650616669838, 0.0012583826117369594, 0.0012273616537603798, 0.0012255541847220489, 0.001271431590905603, 0.0012081650000217619, 0.0012205815921556584, 0.001213613918944433, 0.0012405681000946431, 0.0012407653052739951, 0.0012642237956502608, 0.0012331220810776766, 0.04427329118468096, 0.011046917633894754, 0.0021960758366527, 0.0013356505303966757, 0.0013403464702661245, 0.001285945203117266, 0.0012473596131657154, 0.0012474891216475137, 0.0012517334080814403, 0.0012462205914020234, 0.0012727316126835589, 0.003780036775053156, 0.0014350388165829437, 0.0012345605705655655, 0.0012372270207471993, 0.0013166494900360703, 0.00130138375644325, 0.001346361020351855, 0.0013688995707209926, 0.0014377247960287698, 0.0013678254493113076, 0.0013552757564513963, 0.001379578060717607, 0.001384852264000445, 0.0013699514069119279, 0.005824300061379161, 0.0015106950197558925, 0.0013393525305033034, 0.0013006779398503049, 0.0013008930017145313, 0.0013025870204580073, 0.001300622081878234, 0.0012956932662244961, 0.001333553876195635, 0.001300757083262564, 0.001297575877789332, 0.0013084401642637594, 0.0013338221208553534, 0.0012246361833864025, 0.001249124531216007, 0.0012275243667429503, 0.0012489923071686406, 0.0012416086131136637, 0.043165175835792054, 0.007522613530009225, 0.0019358983881087328, 0.001234107755352648, 0.001237592631380777, 0.0014097889604959258, 0.001350460735586833, 0.0012419917953333684, 0.001243465938795434, 0.0012389410011546345, 0.0012385542053083072, 0.0013692210599475978, 0.0016053037965024005, 0.0013365362466750096, 0.0013453511649514644, 0.0013577284063307606, 0.0013631787958878036, 0.0013434066513211143, 0.0013603352233577445, 0.005202490408733791, 0.006937326469021488, 0.007714813632168332, 0.0012940386124905578, 0.0012458373260285174, 0.0012352718159138244, 0.001234702877605296, 0.0012565684880187012, 0.001446926816632705, 0.0012384541427754626, 0.0012528626937229111, 0.0012398148359426735, 0.0014175611443589537, 0.001344657122936784, 0.0013678228572885298, 0.0013434382046249751, 0.0013748376525710433, 0.0013609216120854324, 0.0013677159381308118, 0.0013305352245248398, 0.0013408749388074692, 0.001340053367371462, 0.0013617753878007739, 0.0013641232258773275, 0.001364277958470796, 0.0013555775714886127, 0.0013560931226808805, 0.0013730137547173975, 0.0013667018585173146, 0.0014109554491481002, 0.0013467546134274832, 0.024833521102460062, 0.004722682122445228, 0.0013578741036697614, 0.0013680317348857619, 0.0013809255086721815, 0.0013605560204584379, 0.0014528239796850451, 0.0012517708581777252, 0.019016207081778924, 0.0037739657754154534, 0.001504938365245352, 0.0012396003656583478, 0.001250883348153106, 0.0012505632036422588, 0.0012534332238327787, 0.0012452369603347412, 0.0012420173686910041, 0.0012683786537346182, 0.0012636749187902529, 0.0012578778372773407, 0.0012403013664582859, 0.001936020959187679, 0.0015246735301286895, 0.0013536369185704663, 0.0013055343879386783, 0.0012853872030973434, 0.018339722633494863, 0.0014396328787908566, 0.0012520793469014521, 0.00124212889932096, 0.001257119284543608, 0.0012367577149475716, 0.0012861709181713511, 0.0012602990407648744, 0.021176726324483752, 0.024092649368626276, 0.004011142614050483, 0.0012590636936377507, 0.0012479315307561535, 0.0012421472240429447, 0.0012503877966379632, 0.0012568614697463963, 0.0012549982045073898, 0.0012618720612241601, 0.0013282897962941503, 0.0013787843077918704, 0.0013695670629148276, 0.0013324771033676, 0.0013438913477014523, 0.0013514311428238846, 0.0013500965314404089, 0.003965133223777675, 0.032081761592238835, 0.01833512134878535, 0.0014367160002453899, 0.0014259266329701154, 0.0014017741837329706, 0.0012531130816977546, 0.0012321591838167943, 0.0012821735510108422, 0.0013195492032611249, 0.0013178185517044396, 0.001332759550696581, 0.0013361318982491384, 0.0013097104098533793, 0.001351006571393536, 0.0013194056536660207, 0.020556476815337583, 0.00475762038710242, 0.004312983958753853, 0.0013357585923252056, 0.001335887081578982, 0.0013214464498949902, 0.0013247405307139366, 0.0013245790602867396, 0.001259017122757374, 0.0012931663671280353, 0.0012686284287471551, 0.0013723820614229356, 0.0012612748957638229, 0.0012615229791904591, 0.0012509424705058336, 0.0012805378349611954, 0.001238952531023141, 0.0012835167771281333, 0.0012704484278754312, 0.0012617327144597561, 0.044105792570174954, 0.0013322541228856664, 0.0014662833276147746, 0.0013284102053742629, 0.0013303324290342173, 0.0012361301223234255, 0.0013258416319684107, 0.0012456190401726232, 0.0012210656945802728, 0.0012223654288836584, 0.02796719593889251, 0.00395767528763307, 0.034447801917107126, 0.0013297872442029873, 0.0013100929981173606]
[43.55293878087546, 674.9182641846211, 751.8166890947284, 705.8312037309772, 776.8269162707645, 769.1175259431222, 771.2289178624742, 711.7436374287028, 737.5921419488074, 775.0555005227158, 760.8443171410025, 775.8827142935028, 767.6885583797077, 742.9124936106409, 703.12953897516, 714.2701590588631, 703.0772091992992, 638.69380701375, 700.5638427737483, 608.4177904242989, 32.30000425361704, 134.79255609932864, 248.36781667492392, 491.1909854986086, 740.0769650879242, 751.5159688521461, 754.5968025935136, 755.9778251018523, 747.6356364874674, 751.6086495905581, 725.7924551538375, 731.466584757171, 696.5911507872404, 709.7061852326947, 69.2159634976011, 164.77705054610783, 168.6034622334676, 489.143675556017, 741.6991748360718, 705.8687540063358, 746.7470364179421, 720.0095388827151, 753.7363634218325, 740.4700978724169, 743.1415432879026, 51.698617312119474, 403.65142749531066, 675.8413281973648, 705.9493485410359, 715.9470975991937, 763.5028440847153, 763.275827605268, 763.0597014563065, 758.3105153527845, 755.595346344541, 720.6757125286985, 329.46616049145194, 705.9780410190806, 711.782078229114, 22.57017344353254, 661.8743278412248, 659.4858704017622, 767.1914367769557, 739.6494816889327, 758.3508744854272, 760.4949742618344, 783.4384314340905, 777.2918353235605, 34.62763976434115, 683.010113264356, 644.6852502943491, 783.5730966395239, 792.02232558414, 784.4236643287467, 790.6452525644772, 792.3297554910789, 793.4394796737679, 786.7324027045483, 778.1129831501476, 646.8298644902553, 37.56875647971985, 80.69406433734689, 143.24762965262465, 723.3188382646969, 748.4821967830071, 779.61760158753, 783.7387070070422, 787.3937291127496, 662.5709596619034, 792.2589121463142, 800.7884726493924, 61.248753968749504, 105.8655945183496, 311.3844493793262, 779.940168951918, 784.1854935071933, 609.745444539148, 798.6864118697132, 798.8433659952635, 809.4719851749744, 787.1180698306451, 660.8495008422414, 668.105493579767, 760.3203873863948, 155.90082046680755, 164.4932801972678, 751.5322668097882, 785.6059539555963, 796.3961150687718, 788.7267965353861, 800.2328052547506, 797.3395933413599, 799.8897523196285, 790.9333847890865, 791.2442267394736, 790.1772528349638, 793.1645907864327, 45.130994986670316, 77.63777395675709, 756.8627067514274, 752.2020829738882, 791.5388436376628, 806.3506867621417, 746.2760442686285, 760.369904227781, 23.800986784076116, 558.9758093000071, 786.3400838870275, 794.9761532970019, 781.9495789588103, 789.4202866110161, 781.6482266087351, 788.1469020091373, 793.2749953254731, 113.3139837797568, 57.18415657413063, 701.6725576725438, 732.3284290942154, 733.072536393024, 721.4535665411165, 735.1516552388409, 723.9802200719854, 725.1293261798088, 70.07094843480228, 52.23379837599085, 158.83114642098627, 727.7386424101087, 722.1349206388421, 675.9986666418606, 716.7548612411215, 721.1745172264744, 639.1648805095839, 765.3146285433136, 789.0288019413432, 72.88501645151729, 103.06276629491262, 796.5601085435954, 809.285791396663, 707.6482939779578, 756.8526057733126, 798.68005878752, 816.8108609041474, 815.6536919549756, 812.8393996443873, 818.7237926091077, 816.425291115024, 818.6342828393123, 813.7826198172609, 820.7003272599402, 812.886176999664, 819.0596860271296, 748.8640428410072, 825.7391862116016, 817.4611699841323, 818.7431901405085, 835.5047935802507, 819.9199577566292, 815.0146720371749, 819.376453815892, 805.3419607248318, 817.3906965587943, 812.1220990910767, 810.8703227892412, 764.0845091853139, 815.2321706222979, 816.6454193368211, 822.1032903939121, 817.1803613788522, 814.6010974398218, 814.8509889035736, 819.9654019693829, 811.2752568890348, 822.902442648614, 817.6309669677644, 819.1344870635891, 802.1799132273374, 776.647879468424, 728.9765365967678, 746.2238676948323, 723.9164199717779, 751.1578526033255, 70.1794762958052, 793.1919648263844, 793.9129625673157, 778.0340222962615, 796.4666904964492, 785.7849510934334, 786.0669908792358, 771.8649625911663, 784.9615093241665, 795.0261350148857, 794.5887537090192, 22.553272909116725, 99.71693942948923, 205.57811115882177, 756.4998504449642, 748.2219310305064, 755.5192686137256, 755.2687773416363, 739.300161114072, 753.0991259963218, 755.405904021908, 757.3887178174912, 702.2875611136737, 747.5951549745865, 99.80089824276021, 177.28184148100644, 746.4198838336883, 745.7563600292348, 741.876793104138, 748.6354098992389, 737.1655042629441, 743.9782442567632, 746.5014627272907, 745.5747597645543, 732.3183939040442, 744.2436034042026, 741.1603514716878, 742.9680611712569, 646.5722312367591, 239.0256086750051, 124.01916625428578, 373.48468788122756, 764.6327805588396, 758.0612310553196, 770.895819994699, 767.6318902346356, 770.2533447487343, 773.411195139071, 737.6594213224488, 791.8337574780356, 792.7934759887509, 787.9713367950433, 791.1023904662699, 719.4194644396375, 731.0137382861064, 710.0937596240276, 130.8756807324193, 268.18111426267717, 675.1646852036606, 695.1738249759767, 726.8223299858562, 784.0729260919456, 674.3724877975558, 737.5513746582673, 783.8142789479147, 87.18629221020099, 679.1016650152022, 772.7895441459677, 724.8234632750339, 789.2970804854261, 789.2684102458857, 783.076332644868, 62.72875472126278, 90.38500910285501, 302.5948639285683, 706.3429912562459, 761.5643870175021, 772.4793916791247, 772.475712735261, 764.7089501097146, 772.2487832847154, 771.606117022219, 766.3972287203135, 779.4880600783622, 761.1307609457797, 767.7881703826476, 774.1648739886115, 770.0867618139332, 779.4150808276314, 771.1398786390162, 771.7726150639613, 766.0644451566518, 705.5512065653643, 69.783975627923, 325.215852632734, 752.6694307589511, 750.2476312093299, 675.3934570446538, 776.9397559323677, 766.1207761259535, 776.4248013784684, 735.7438557211586, 770.6826290888494, 774.2428306643795, 786.1457494537314, 777.7292504141808, 771.3409624729511, 778.5741981838597, 762.824445952187, 780.9327086083844, 774.9988412177553, 773.8487396169248, 756.1411466843938, 769.4775452618343, 764.0022484355691, 683.6915252870429, 52.39597955276684, 92.8238798059344, 203.74503162183177, 333.34425429938403, 800.7070941952218, 806.2630525885273, 815.7180264161086, 819.3964453746595, 825.9615573987076, 791.5301998197297, 799.232449613931, 674.9657705880339, 819.941335124435, 35.51270838679167, 94.43715092137994, 224.23703311020668, 740.6771761635428, 767.4644800198288, 801.1544733346244, 717.6892145267815, 701.2404545254956, 756.9319442236088, 747.0277892176332, 750.3690935529555, 751.56117709479, 753.1241627777396, 101.07506868268428, 602.1188542401349, 777.2899606553956, 771.8776438798604, 790.9461266745712, 789.3159864295725, 772.1857676807263, 806.795032440189, 812.3291796351926, 661.1240366395152, 818.2698344671903, 798.5877837841841, 800.5225153916323, 803.1037836554465, 818.7687602941635, 57.22922963499141, 134.95487545669303, 136.5045406891076, 91.41360890446141, 179.01061128767097, 720.9932203338965, 726.0202550723438, 691.5425830811375, 788.8998758873795, 770.0818122295024, 774.0135282508097, 781.2741397825663, 685.7122261801538, 69.42586033634075, 64.27308571624181, 480.4879389696881, 740.8826332359533, 759.9442502920642, 756.0508567459993, 749.9367682162236, 753.5387917306703, 752.462374611316, 751.8822368711709, 768.0621698060015, 756.1231785216628, 760.7500419720657, 753.1869992245126, 751.6499937823352, 755.9589428066358, 751.0340281690205, 749.6004742199872, 94.67298602597687, 286.8656170813712, 531.0138703264297, 729.2890819367417, 712.8768272009218, 774.8118585203501, 808.3732422520673, 804.806767261325, 796.2073490072281, 799.3069181037124, 61.40732294906446, 261.3255075837909, 767.4858768887119, 782.3665693786278, 738.5295733664419, 786.1765398255458, 348.1605362482351, 742.2578976618596, 24.379128269854466, 606.3687860667814, 735.739458646332, 737.1326574651101, 689.3336069158006, 745.0908321745859, 742.4123298357205, 750.1768648558875, 741.3307118959282, 748.1088375316799, 754.2251110905122, 743.4837181628485, 746.6698874506537, 48.32522783793945, 113.07091257957654, 759.0320009415813, 751.4186775331182, 763.6873465416235, 818.3799718709321, 815.5958977579908, 773.4509576649366, 827.2679396858805, 829.618351243264, 793.0000306361733, 751.8148426359406, 801.9549501867884, 770.7189827710796, 773.5709266120019, 773.1076677507406, 740.8756667672145, 778.0023003616913, 787.4225417298961, 793.7009436678445, 197.48553363605265, 263.78179949292957, 470.72219835048946, 734.7959695571176, 806.1859405119616, 803.6082719705876, 813.6738248103323, 751.5645655837847, 765.2227551349786, 767.9570683715664, 745.2581775276218, 756.2102521974736, 755.8889845692289, 761.419146397901, 34.539085694493714, 164.49424100812024, 724.9376801047717, 724.1523526369236, 733.3669494348533, 779.7149488341676, 783.828310545521, 790.1577434226592, 790.0554519650887, 796.793184682256, 763.7432078835477, 792.6711240580772, 780.7229498147518, 737.0339565306316, 738.3707536560709, 35.82676200508892, 239.20191425274356, 247.89307451796532, 250.80304958618845, 177.7672049210665, 326.65038416817623, 750.0506102387379, 740.871791057349, 742.6113022468746, 800.1589992502047, 663.8400799893407, 775.6530785829735, 775.6024845266209, 18.02291528005725, 113.31910195321124, 251.27830520936405, 416.40121436146046, 704.4394485098364, 758.246104027236, 758.8804605140434, 758.8374962521173, 767.9405686576891, 764.3361154646151, 771.5154718587878, 767.7908552357417, 766.7109746588791, 322.95453897592347, 368.430501456165, 677.7976719879318, 803.0626912493499, 708.2918918303113, 741.0106403114463, 808.9431867134505, 807.670109869243, 804.6574492289276, 804.52290276589, 811.1461948546638, 801.0881723227835, 815.1406000579343, 820.7486043345575, 818.6219722389329, 794.6708661364042, 814.7557787358021, 815.957395002324, 786.5149860620733, 827.7015142650116, 819.2815674320541, 823.9852760338903, 806.0823101317129, 805.9541927465262, 790.9991913145759, 810.9497148295799, 22.58698129824175, 90.52298868706556, 455.3576808732702, 748.6988379385508, 746.0757514446627, 777.638112087432, 801.6934246107797, 801.6101965517232, 798.8921551057124, 802.4261570537683, 785.7116064646941, 264.54769080545196, 696.8452619150466, 810.0048096804916, 808.2591013863158, 759.5035790220861, 768.4128490531128, 742.7428341164111, 730.5137801112064, 695.5434049424223, 731.0874355375494, 737.8572185326792, 724.859309142562, 722.0986858997392, 729.9528982959674, 171.69445074284272, 661.9469760094835, 746.6294177412864, 768.829830476782, 768.7027285733993, 767.7030281235157, 768.8628495034435, 771.7876028744728, 749.8759651562049, 768.7830517069307, 770.6678407922401, 764.2688044223144, 749.7251577734519, 816.5690460286487, 800.5606927169323, 814.6477797857067, 800.6454437392933, 805.406783939936, 23.166823269854767, 132.9325235186944, 516.5560373119301, 810.3020142793355, 808.0203248174677, 709.3260253990264, 740.4880228268593, 805.1582979512241, 804.2037733407611, 807.1409365482676, 807.3930036441764, 730.3422575447909, 622.935049539394, 748.2026787434809, 743.3003561089395, 736.5243264685638, 733.5794856966841, 744.3762460284039, 735.1129213074986, 192.21563548127426, 144.14774978019022, 129.6207591885715, 772.7744677381461, 802.6730128465504, 809.5384247557078, 809.9114516842288, 795.818142452986, 691.1199574883855, 807.4582380248088, 798.172062278011, 806.5720549630832, 705.4369428644419, 743.6840090624446, 731.0888209474183, 744.358762879721, 727.3586071271249, 734.796178648109, 731.1459727278235, 751.5772461846094, 745.7817064500941, 746.2389367085563, 734.335492444881, 733.0716030854588, 732.9884601528628, 737.6929369684547, 737.4124853779106, 728.3248230865877, 731.6884759964136, 708.7395995414137, 742.5257652951382, 40.2681519013805, 211.7440839914581, 736.445298792739, 730.9771948261898, 724.1520224805917, 734.9936239031532, 688.3146299779469, 798.868254095448, 52.58672224695042, 264.9732561207225, 664.4790398688313, 806.7116045653175, 799.4350564155098, 799.6397120013648, 797.8087551741892, 803.0600053271648, 805.1417195992414, 788.4080964746581, 791.3427615998936, 794.9897600266861, 806.2556625697569, 516.5233337244358, 655.878114389246, 738.7505366328726, 765.9698658561651, 777.9756929198783, 54.52645168000763, 694.6215349290279, 798.6714280326736, 805.069426004559, 795.4694612477017, 808.5658071212371, 777.5016414006449, 793.4624780743314, 47.22165195306117, 41.506435622734436, 249.3055211991558, 794.240994362048, 801.3260145723495, 805.0575492534587, 799.7518871255743, 795.6326326096824, 796.8138889828282, 792.473366142908, 752.8477616781674, 725.2766037071489, 730.1577462528312, 750.4819388435847, 744.1077745685076, 739.9563087694196, 740.6877780310322, 252.19833573391935, 31.170358183882215, 54.540135348831335, 696.0317834764844, 701.2983535604935, 713.3816641828623, 798.0125773207717, 811.5834489033738, 779.9256186587364, 757.8345676906983, 758.8298090860994, 750.3228916854051, 748.4291044247921, 763.5275649308986, 740.1888496874742, 757.9170190922409, 48.64646840911381, 210.18911107555593, 231.85803832410483, 748.6382687303266, 748.5662626649756, 756.7465182410273, 754.8648031936295, 754.9568236293302, 794.2703732336058, 773.2957068941392, 788.2528700602733, 728.6600634834615, 792.8485719954048, 792.6926552235435, 799.3972733179633, 780.9218694661242, 807.1334251798886, 779.1094108154148, 787.1236470986062, 792.5608875316957, 22.67275887648862, 750.6075476306255, 681.9964335451562, 752.7795224354383, 751.6918164025897, 808.9763221046695, 754.2378937938087, 802.8136755692301, 818.956755921096, 818.0859637966556, 35.75617670734564, 252.67358419342702, 29.029428420609612, 751.9999942542339, 763.3045909237187]
Elapsed: 0.15837797975719142~0.31437973178616097
Time per graph: 0.0032322036685141107~0.0064159128935951226
Speed: 661.3030008415935~233.07378678218168
Total Time: 0.0661
best val loss: 0.03790061175823212 test_score: 0.9184

Testing...
Test loss: 0.2331 score: 0.9184 time: 0.06s
test Score 0.9184
Epoch Time List: [1.9869935419410467, 4.054680104018189, 0.3512155970092863, 0.32481396314688027, 0.3286691779503599, 0.3318680408410728, 0.32556348701473325, 0.3353743690531701, 0.34137710800860077, 0.3359849948901683, 0.3318568659015, 0.3417895151069388, 0.33118316100444645, 0.35235156607814133, 0.32939576217904687, 0.3568011570023373, 0.35523426497820765, 0.37604613904841244, 0.34999916900414973, 0.39223748084623367, 2.884609259897843, 3.0806974879233167, 1.9325210060924292, 0.6661368969362229, 0.3800672080833465, 0.34027195000089705, 0.3383539420319721, 0.3329651530366391, 0.33185928815510124, 0.338839522912167, 0.3347344050416723, 0.33801154501270503, 0.3470402820967138, 0.3616360240848735, 1.6447780369780958, 3.369594909832813, 2.303258904023096, 1.012641201959923, 0.37734368396922946, 0.34308771905489266, 0.3445110279135406, 0.34175825805868953, 0.33648258983157575, 0.33841254806611687, 0.33936398394871503, 2.12263453588821, 3.0764662319561467, 0.4508116259239614, 0.35516314895357937, 0.3524733129888773, 0.34151945204939693, 0.3300332190701738, 0.32832221896387637, 0.33335594297386706, 0.33515326492488384, 0.342847750056535, 0.43656122812535614, 0.3508594479644671, 0.35152704280335456, 3.895825411193073, 3.150547464028932, 0.3482093879720196, 0.32508996198885143, 0.3252483159303665, 0.33912791789043695, 0.3310684320749715, 0.33289744216017425, 0.3267461770446971, 4.745232988963835, 1.6867911200970411, 0.35971221898216754, 0.3319794898852706, 0.3139907759614289, 0.31921385193709284, 0.3138297600671649, 0.31555359612684697, 0.31658428895752877, 0.3239901940105483, 0.32125419296789914, 0.34174166107550263, 2.1870635750237852, 5.680947924847715, 0.9235442630015314, 0.6113212950294837, 0.3377975800540298, 0.332190784974955, 0.31913594191428274, 0.32520267192739993, 0.3494842710206285, 0.3426810910459608, 0.3135987528366968, 1.0633706880034879, 6.017131041968241, 0.6037950499448925, 0.34147460595704615, 0.32114708004519343, 0.33681063109543175, 0.3261419930495322, 0.31624499696772546, 0.3169881399953738, 0.3349324209848419, 0.336848090053536, 0.3326789540005848, 0.3272469249786809, 2.8716370910406113, 2.1238537849858403, 0.720138965989463, 0.3208837799029425, 0.3145404190290719, 0.3166577339870855, 0.320043065934442, 0.31203744490630925, 0.3116926901275292, 0.3181670579360798, 0.3166189248440787, 0.32379966508597136, 0.32173960108775645, 2.7447007560404018, 3.5546221969416365, 2.412980506196618, 0.33462593401782215, 0.3249718869337812, 0.3179599040886387, 0.3461743200896308, 0.33239010302349925, 2.340799023048021, 6.266156640951522, 0.37086032691877335, 0.3168076389702037, 0.3224344099871814, 0.32199554692488164, 0.32758491090498865, 0.31873609288595617, 0.31879026303067803, 6.3575911150546744, 3.3295092850457877, 1.0438960848841816, 0.34500911890063435, 0.3446670949924737, 0.3435581000521779, 0.3409214620478451, 0.34457146911881864, 0.3424563800217584, 2.7111008579377085, 3.081267736153677, 2.616064192028716, 1.0002684469800442, 0.35463874705601484, 0.3485278739826754, 0.3483607629314065, 0.3580932910554111, 0.37738353991881013, 0.35160409309901297, 0.32170978107023984, 2.644357603043318, 2.6480825021862984, 1.823957686079666, 0.3170012349728495, 0.31778263789601624, 0.3179837348870933, 0.31744955689646304, 0.3124219449236989, 0.3054600750328973, 0.309437278076075, 0.3094345841091126, 0.31088599981740117, 0.30627513001672924, 0.3067826629849151, 0.30511809804011136, 0.30653616809286177, 0.306431423057802, 0.32225059799384326, 0.31448959303088486, 0.3062993149505928, 0.307303695124574, 0.30671644606627524, 0.3074639029800892, 0.3073584239464253, 0.30717267701402307, 0.30832990794442594, 0.30810924794059247, 0.31474190310109407, 0.3075207289075479, 0.3079937739530578, 0.30856328306254, 0.3055100010242313, 0.3064292680937797, 0.30912443494889885, 0.30856312811374664, 0.3131469418294728, 0.30632207391317934, 0.3079114140709862, 0.3068085570121184, 0.3077125329291448, 0.3081315979361534, 0.3093003421090543, 0.32886429398786277, 0.32525431201793253, 0.3346061479533091, 0.33912500797305256, 0.3331078678602353, 5.217154128011316, 3.422703202930279, 0.31686376314610243, 0.3581425059819594, 0.31432787608355284, 0.3185947729507461, 0.3174780890112743, 0.3150269619654864, 0.3173335319152102, 0.3128476190613583, 0.3151279218727723, 2.441614740062505, 3.667576527921483, 1.7318520640255883, 0.8733556040097028, 0.3348576759453863, 0.33232256409246475, 0.3331166470889002, 0.33542429096996784, 0.33122059295419604, 0.3296388591406867, 0.3331540538929403, 0.33419005188625306, 0.3321467631030828, 2.1675933180376887, 3.001053098938428, 0.6521856199251488, 0.33910576708149165, 0.3388834389625117, 0.335367688909173, 0.33955472195520997, 0.3329029369633645, 0.33393122092820704, 0.3336137079168111, 0.3344751870026812, 0.33609154890291393, 0.3349373679375276, 0.3362290539080277, 0.3444046980002895, 5.7869938230142, 1.4801476938882843, 0.48073305597063154, 0.5365775910904631, 0.33313714095856994, 0.32844940095674247, 0.3278804109431803, 0.32808308699168265, 0.33297866908833385, 0.32822780904825777, 0.3158195719588548, 0.3190710488706827, 0.3240110161714256, 0.32095225690864027, 0.3339336421340704, 0.3453233679756522, 0.34476580400951207, 4.7624331309925765, 2.077444403897971, 0.7340354569023475, 0.3563681810628623, 0.3532409109175205, 0.3306945310905576, 0.34358833893202245, 0.3323914440115914, 0.32121908105909824, 0.9348819570150226, 4.593591123120859, 0.33544867706950754, 0.3255298479925841, 0.41009755386039615, 0.3175909760175273, 0.31667433585971594, 2.6564435800537467, 5.2622016940731555, 1.4584333690581843, 0.9133244019467384, 0.3431696919724345, 0.33031751320231706, 0.33012127003166825, 0.333982671960257, 0.32729973597452044, 0.326534082996659, 0.3232978949090466, 0.321721309912391, 0.33846349793020636, 0.3237025741254911, 0.32664125703740865, 0.32135380001273006, 0.3221655760426074, 0.3299883770523593, 0.321054870961234, 0.32748371292836964, 0.33549811504781246, 4.672492607147433, 2.706216943101026, 0.39688792498782277, 0.3436287899967283, 0.3562678409507498, 0.3320559229468927, 0.3290527840144932, 0.3295950230676681, 0.33415364497341216, 0.33444184192921966, 0.3272205670364201, 0.32332348404452205, 0.32411842595320195, 0.33035187504719943, 0.32682202896103263, 0.3246023061219603, 0.32454351091291755, 0.33072601701132953, 0.3253246389795095, 0.3259539850987494, 0.3231784579111263, 0.32772349496372044, 0.3349113971926272, 2.4756317770807073, 4.588531749206595, 0.9906137288780883, 1.5237406060332432, 0.5118806288810447, 0.3129608938470483, 0.3099137820536271, 0.30796463694423437, 0.30500435910653323, 0.3093572029611096, 0.32794641400687397, 0.31762038299348205, 0.3831625909078866, 3.493624092079699, 4.288732627872378, 1.1601292027626187, 0.5199973558774218, 0.3352043479681015, 0.3213305090321228, 0.3329292320413515, 0.3497304110787809, 0.33737566298805177, 0.3404664139961824, 0.33423883002251387, 0.3357547289924696, 0.337240006076172, 5.858254222897813, 1.9629653780721128, 0.36448379303328693, 0.3269275319762528, 0.32349441619589925, 0.32566077809315175, 0.3232417751569301, 0.31717682315502316, 0.3149834780488163, 0.34059020096901804, 0.3181562020909041, 0.3135046809911728, 0.31593067198991776, 0.31240884494036436, 0.31056611402891576, 1.1183270439505577, 4.235970305046067, 2.367965108016506, 1.7671166659565642, 1.9044755639042705, 0.5932571248849854, 0.35070624004583806, 0.34450543904677033, 0.3331119370413944, 0.3319779341109097, 0.32835725299082696, 0.3239322700537741, 0.3381256520515308, 4.809318824089132, 3.160602799966, 1.9528573889983818, 0.49242012994363904, 0.3296959839062765, 0.330072304001078, 0.3336499738506973, 0.3360977699048817, 0.33594637911301106, 0.3348318039206788, 0.3302526921033859, 0.32744630391243845, 0.3306283139390871, 0.33436515112407506, 0.33723521302454174, 0.33186471182852983, 0.3364420550642535, 0.33469079399947077, 7.439309622975998, 2.8608708491083235, 0.7465043219272047, 0.3498115101829171, 0.34960370894987136, 0.3364332920173183, 0.3106542769819498, 0.3204797930084169, 0.32123335008509457, 0.31515936902724206, 1.063678971142508, 6.705888824886642, 0.44589233794249594, 0.3217141979839653, 0.33172117976937443, 0.32726495305541903, 0.39630834199488163, 0.33969609800260514, 4.445263289031573, 1.1472845169482753, 0.8929339059395716, 0.34471687593031675, 0.3450969198020175, 0.34083500609267503, 0.34301378298550844, 0.3396534320199862, 0.3391988640651107, 0.3402927800780162, 0.3437992340186611, 0.33484994899481535, 0.3353179480182007, 2.504654165939428, 3.1126011739252135, 0.9305133830057457, 0.329311657929793, 0.32601689093280584, 0.3082880770089105, 0.3097740029916167, 0.3139313260326162, 0.3103006340097636, 0.3079869420034811, 0.32295203395187855, 0.3201042120344937, 0.31785473111085594, 0.3196048450190574, 0.3267858889885247, 0.329340577009134, 0.33583879203069955, 0.3267793031409383, 0.32203857507556677, 0.3233777058776468, 0.512633147998713, 3.9699682589853182, 0.7220524821896106, 0.3557085449574515, 0.3172227619215846, 0.3116626428673044, 0.3168121710186824, 0.32263528706971556, 0.32785521796904504, 0.32874286104924977, 0.33167116495314986, 0.3335980469128117, 0.33184916607569903, 0.328518892172724, 2.602584370994009, 5.25912816694472, 0.47897268598899245, 0.34951771213673055, 0.3452140368754044, 0.33653178601525724, 0.3217824990861118, 0.32185517996549606, 0.32580967410467565, 0.3183688768185675, 0.32103352108970284, 0.32364923995919526, 0.32098526996560395, 0.33387082116678357, 0.3258235150715336, 6.013799128937535, 2.7661419381620362, 1.0766604531090707, 0.7213211118942127, 2.8941300489241257, 0.7205591569654644, 0.3413990889675915, 0.34198462101630867, 0.3405541640240699, 0.314467801945284, 0.37756254395935684, 0.3293691851431504, 0.3241169760003686, 3.293628849904053, 2.670094058965333, 1.2376392129808664, 0.6491366070695221, 0.5309542840113863, 0.33328498306218535, 0.333496511913836, 0.3285363840404898, 0.3290910959476605, 0.32980400684755296, 0.3269732450135052, 0.3272924260236323, 0.3266902460018173, 3.7875749769154936, 1.9765082550002262, 1.0788543638773263, 0.3213546648621559, 0.3331515190657228, 0.32780369697138667, 0.3183441059663892, 0.31235550506971776, 0.31917143100872636, 0.3163502630777657, 0.31522432703059167, 0.3134144330397248, 0.3107123860390857, 0.3142133679939434, 0.3085497940191999, 0.3125935409916565, 0.31018819008022547, 0.31448534212540835, 0.30974081705790013, 0.3084546298487112, 0.3075312030268833, 0.30616728495806456, 0.3137599481269717, 0.3147684510331601, 0.31977374106645584, 0.3168855680851266, 2.4231920088641346, 6.984865373116918, 0.6718552729580551, 0.46117410296574235, 0.33809371304232627, 0.32190796185750514, 0.3156543489312753, 0.3213746679248288, 0.3135985300177708, 0.31578860396984965, 0.31871780392248183, 3.7311518740607426, 0.6597326329210773, 0.3183055139379576, 0.31328445894178003, 0.3136707858648151, 0.31891025917138904, 0.32818404713179916, 0.3314750362187624, 0.34897219692356884, 0.34309965604916215, 0.3487658069934696, 0.3461848860606551, 0.34946274186950177, 0.3473400310613215, 0.5689590031979606, 3.542123956955038, 0.33593405003193766, 0.3331609619781375, 0.3248737780377269, 0.326241266913712, 0.32606759294867516, 0.3263019820442423, 0.33794820797629654, 0.3296489219646901, 0.32498147105798125, 0.32949609588831663, 0.3168411578517407, 0.3068724221084267, 0.3462334548821673, 0.3103617139859125, 0.3094456698745489, 0.3128171149874106, 2.3870632760226727, 6.017244859132916, 0.6873137339716777, 0.3129660259000957, 0.3090275120921433, 0.3459525549551472, 0.338432333082892, 0.31213094503618777, 0.31354436790570617, 0.3101903371280059, 0.31519103795289993, 0.3206463659880683, 4.056279702926986, 0.3470296000596136, 0.3386880599427968, 0.3458363770041615, 0.3345639209728688, 0.3390996130183339, 0.33827493200078607, 5.285032681887969, 1.393113799043931, 1.374486296903342, 0.4165407521650195, 0.31530171795748174, 0.31579396198503673, 0.3172663009027019, 0.32391817797906697, 0.33488368312828243, 0.3250761158997193, 0.3189348300220445, 0.3131969489622861, 0.3226904020411894, 0.3274328629486263, 0.3508637360064313, 0.34013893199153244, 0.3500736600253731, 0.34103977389167994, 0.35027549508959055, 0.34353615483269095, 0.34186526003759354, 0.3414436799939722, 0.3468203628435731, 0.34977560001425445, 0.34440607496071607, 0.3423707321053371, 0.3417592791374773, 0.3445435839239508, 0.3435964439995587, 0.3471367888851091, 0.3427453129552305, 4.602383463992737, 3.3885515440488234, 0.4782183449715376, 0.3483280761865899, 0.3546119489474222, 0.3463228930486366, 0.3368508570129052, 0.32513448200188577, 2.7959356530336663, 5.48485448397696, 0.602281698025763, 0.3180504138581455, 0.31243672606069595, 0.3170482990099117, 0.31260539405047894, 0.3116736519150436, 0.3141347218770534, 0.31366028101183474, 0.31630541291087866, 0.3159239920787513, 0.3187343979952857, 0.35333704203367233, 0.39010193198919296, 0.3420872238930315, 0.33026785706169903, 0.33301135897636414, 3.2441820120438933, 3.8937837909907103, 0.3275514249689877, 0.31660895701497793, 0.3223344668513164, 0.31642841279972345, 0.32903717702720314, 0.32151768915355206, 4.2478884210577235, 8.317047677002847, 4.70242375601083, 0.32268776604905725, 0.3207979629514739, 0.31819445092696697, 0.32605824596248567, 0.3328151779714972, 0.3164772081654519, 0.3161002459237352, 0.32605727203190327, 0.34467448899522424, 0.3449656961020082, 0.3398275909712538, 0.3470257659209892, 0.34459223388694227, 0.34254280501045287, 0.46876175014767796, 6.480008686077781, 5.17918871389702, 0.8364596180617809, 0.3582692409399897, 0.35435418994165957, 0.3265419719973579, 0.311670582043007, 0.31393740698695183, 0.3285835748538375, 0.3347061360254884, 0.331788980984129, 0.3323637949069962, 0.3333905440522358, 0.3350552619667724, 0.33177829696796834, 2.5531657451065257, 2.5638864981010556, 1.1462689199252054, 1.0958135117543861, 0.33972309390082955, 0.33735545887611806, 0.3352309539914131, 0.3381730748806149, 0.3218234081286937, 0.31863078312017024, 0.3264771798858419, 0.32353332999628037, 0.4145921579329297, 0.3167453099740669, 0.31775234395172447, 0.32070689590182155, 0.3216950960922986, 0.3636241320054978, 0.3196869910461828, 0.32323254796210676, 2.4282772758742794, 5.367974494001828, 0.34611645992845297, 0.33790738496463746, 0.3388454089872539, 0.3218377260491252, 0.3169944480760023, 0.3148530359612778, 0.3147561280056834, 0.3129591761389747, 6.927313965978101, 2.3207666348898783, 3.8062106219585985, 0.6021206928417087, 0.33448707195930183]
Total Epoch List: [716]
Total Time List: [0.06605594896245748]
========================k_idx:1========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: AEtransGAT
AEtransGAT(
  (gae): GAE(
    (encoder): transGAT(
      (encoder): EdgeFeatureGAT()
      (classifier): Sequential(
        (0): Linear(in_features=64, out_features=64, bias=True)
        (1): ReLU(inplace=True)
        (2): Linear(in_features=64, out_features=2, bias=True)
      )
    )
    (decoder): InnerProductDecoder()
  )
)
Total number of parameters: 1914242
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x75feb28b0820>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 2.8152;  Loss pred: 2.8152; Loss self: 0.0000; time: 0.20s
Val loss: 0.6930 score: 0.5306 time: 0.06s
Test loss: 0.6927 score: 0.5714 time: 0.08s
Epoch 2/1000, LR 0.000000
Train loss: 2.7757;  Loss pred: 2.7757; Loss self: 0.0000; time: 0.21s
Val loss: 0.6930 score: 0.5306 time: 0.06s
Test loss: 0.6926 score: 0.5714 time: 0.07s
Epoch 3/1000, LR 0.000030
Train loss: 2.7864;  Loss pred: 2.7864; Loss self: 0.0000; time: 0.20s
Val loss: 0.6930 score: 0.5102 time: 0.07s
Test loss: 0.6926 score: 0.5510 time: 0.07s
Epoch 4/1000, LR 0.000060
Train loss: 2.7462;  Loss pred: 2.7462; Loss self: 0.0000; time: 0.21s
Val loss: 0.6930 score: 0.5306 time: 0.06s
Test loss: 0.6926 score: 0.5510 time: 0.07s
Epoch 5/1000, LR 0.000090
Train loss: 2.7402;  Loss pred: 2.7402; Loss self: 0.0000; time: 0.44s
Val loss: 0.6929 score: 0.5306 time: 0.32s
Test loss: 0.6926 score: 0.5510 time: 1.85s
Epoch 6/1000, LR 0.000120
Train loss: 2.7589;  Loss pred: 2.7589; Loss self: 0.0000; time: 1.14s
Val loss: 0.6929 score: 0.5306 time: 0.15s
Test loss: 0.6926 score: 0.5714 time: 0.16s
Epoch 7/1000, LR 0.000150
Train loss: 2.7267;  Loss pred: 2.7267; Loss self: 0.0000; time: 0.83s
Val loss: 0.6929 score: 0.5510 time: 0.07s
Test loss: 0.6925 score: 0.6122 time: 0.08s
Epoch 8/1000, LR 0.000180
Train loss: 2.6771;  Loss pred: 2.6771; Loss self: 0.0000; time: 0.22s
Val loss: 0.6929 score: 0.5714 time: 0.06s
Test loss: 0.6924 score: 0.6122 time: 0.07s
Epoch 9/1000, LR 0.000210
Train loss: 2.6421;  Loss pred: 2.6421; Loss self: 0.0000; time: 0.21s
Val loss: 0.6929 score: 0.5714 time: 0.06s
Test loss: 0.6924 score: 0.6735 time: 0.07s
Epoch 10/1000, LR 0.000240
Train loss: 2.6384;  Loss pred: 2.6384; Loss self: 0.0000; time: 0.21s
Val loss: 0.6929 score: 0.5510 time: 0.06s
Test loss: 0.6923 score: 0.6122 time: 0.07s
Epoch 11/1000, LR 0.000270
Train loss: 2.5805;  Loss pred: 2.5805; Loss self: 0.0000; time: 0.22s
Val loss: 0.6929 score: 0.5714 time: 0.06s
Test loss: 0.6922 score: 0.5918 time: 0.07s
Epoch 12/1000, LR 0.000270
Train loss: 2.5315;  Loss pred: 2.5315; Loss self: 0.0000; time: 0.21s
Val loss: 0.6929 score: 0.5510 time: 0.06s
Test loss: 0.6922 score: 0.5714 time: 0.07s
Epoch 13/1000, LR 0.000270
Train loss: 2.4792;  Loss pred: 2.4792; Loss self: 0.0000; time: 0.20s
Val loss: 0.6928 score: 0.5918 time: 0.06s
Test loss: 0.6921 score: 0.6531 time: 0.07s
Epoch 14/1000, LR 0.000270
Train loss: 2.4729;  Loss pred: 2.4729; Loss self: 0.0000; time: 0.20s
Val loss: 0.6928 score: 0.5714 time: 0.06s
Test loss: 0.6921 score: 0.6531 time: 0.07s
Epoch 15/1000, LR 0.000270
Train loss: 2.4250;  Loss pred: 2.4250; Loss self: 0.0000; time: 0.20s
Val loss: 0.6928 score: 0.5714 time: 0.06s
Test loss: 0.6920 score: 0.6939 time: 0.07s
Epoch 16/1000, LR 0.000270
Train loss: 2.3868;  Loss pred: 2.3868; Loss self: 0.0000; time: 0.20s
Val loss: 0.6927 score: 0.5918 time: 0.06s
Test loss: 0.6920 score: 0.6939 time: 0.07s
Epoch 17/1000, LR 0.000270
Train loss: 2.3463;  Loss pred: 2.3463; Loss self: 0.0000; time: 0.20s
Val loss: 0.6927 score: 0.5918 time: 0.06s
Test loss: 0.6919 score: 0.6735 time: 0.07s
Epoch 18/1000, LR 0.000270
Train loss: 2.3059;  Loss pred: 2.3059; Loss self: 0.0000; time: 0.20s
Val loss: 0.6927 score: 0.5918 time: 0.06s
Test loss: 0.6919 score: 0.6735 time: 0.07s
Epoch 19/1000, LR 0.000270
Train loss: 2.2976;  Loss pred: 2.2976; Loss self: 0.0000; time: 0.20s
Val loss: 0.6926 score: 0.5918 time: 0.06s
Test loss: 0.6918 score: 0.6122 time: 0.07s
Epoch 20/1000, LR 0.000270
Train loss: 2.2348;  Loss pred: 2.2348; Loss self: 0.0000; time: 0.20s
Val loss: 0.6926 score: 0.5510 time: 0.06s
Test loss: 0.6918 score: 0.6122 time: 0.07s
Epoch 21/1000, LR 0.000270
Train loss: 2.1966;  Loss pred: 2.1966; Loss self: 0.0000; time: 0.20s
Val loss: 0.6925 score: 0.5510 time: 0.06s
Test loss: 0.6917 score: 0.6122 time: 0.07s
Epoch 22/1000, LR 0.000270
Train loss: 2.1737;  Loss pred: 2.1737; Loss self: 0.0000; time: 0.20s
Val loss: 0.6925 score: 0.5510 time: 0.06s
Test loss: 0.6917 score: 0.6122 time: 0.07s
Epoch 23/1000, LR 0.000270
Train loss: 2.1327;  Loss pred: 2.1327; Loss self: 0.0000; time: 0.20s
Val loss: 0.6924 score: 0.5510 time: 0.06s
Test loss: 0.6916 score: 0.6122 time: 0.07s
Epoch 24/1000, LR 0.000270
Train loss: 2.1210;  Loss pred: 2.1210; Loss self: 0.0000; time: 0.20s
Val loss: 0.6924 score: 0.5510 time: 0.06s
Test loss: 0.6916 score: 0.6122 time: 0.07s
Epoch 25/1000, LR 0.000270
Train loss: 2.1011;  Loss pred: 2.1011; Loss self: 0.0000; time: 0.20s
Val loss: 0.6924 score: 0.5510 time: 0.06s
Test loss: 0.6915 score: 0.6122 time: 0.07s
Epoch 26/1000, LR 0.000270
Train loss: 2.0635;  Loss pred: 2.0635; Loss self: 0.0000; time: 0.20s
Val loss: 0.6923 score: 0.5510 time: 0.06s
Test loss: 0.6914 score: 0.6122 time: 0.07s
Epoch 27/1000, LR 0.000270
Train loss: 2.0182;  Loss pred: 2.0182; Loss self: 0.0000; time: 0.20s
Val loss: 0.6923 score: 0.5510 time: 0.06s
Test loss: 0.6913 score: 0.6122 time: 0.07s
Epoch 28/1000, LR 0.000270
Train loss: 2.0001;  Loss pred: 2.0001; Loss self: 0.0000; time: 0.20s
Val loss: 0.6922 score: 0.5510 time: 0.06s
Test loss: 0.6913 score: 0.6122 time: 0.07s
Epoch 29/1000, LR 0.000270
Train loss: 1.9413;  Loss pred: 1.9413; Loss self: 0.0000; time: 0.20s
Val loss: 0.6922 score: 0.5510 time: 0.06s
Test loss: 0.6912 score: 0.6122 time: 0.07s
Epoch 30/1000, LR 0.000270
Train loss: 1.9292;  Loss pred: 1.9292; Loss self: 0.0000; time: 0.20s
Val loss: 0.6921 score: 0.5510 time: 0.06s
Test loss: 0.6911 score: 0.6122 time: 0.07s
Epoch 31/1000, LR 0.000270
Train loss: 1.9202;  Loss pred: 1.9202; Loss self: 0.0000; time: 0.20s
Val loss: 0.6921 score: 0.5510 time: 0.06s
Test loss: 0.6910 score: 0.6122 time: 0.07s
Epoch 32/1000, LR 0.000270
Train loss: 1.8746;  Loss pred: 1.8746; Loss self: 0.0000; time: 0.20s
Val loss: 0.6921 score: 0.6122 time: 0.06s
Test loss: 0.6908 score: 0.6531 time: 0.07s
Epoch 33/1000, LR 0.000270
Train loss: 1.8449;  Loss pred: 1.8449; Loss self: 0.0000; time: 0.20s
Val loss: 0.6920 score: 0.6122 time: 0.06s
Test loss: 0.6907 score: 0.6735 time: 0.07s
Epoch 34/1000, LR 0.000270
Train loss: 1.8398;  Loss pred: 1.8398; Loss self: 0.0000; time: 0.20s
Val loss: 0.6920 score: 0.6327 time: 0.06s
Test loss: 0.6906 score: 0.6735 time: 0.07s
Epoch 35/1000, LR 0.000270
Train loss: 1.8189;  Loss pred: 1.8189; Loss self: 0.0000; time: 0.21s
Val loss: 0.6919 score: 0.6122 time: 0.06s
Test loss: 0.6905 score: 0.7143 time: 0.07s
Epoch 36/1000, LR 0.000270
Train loss: 1.7932;  Loss pred: 1.7932; Loss self: 0.0000; time: 0.21s
Val loss: 0.6918 score: 0.6122 time: 0.06s
Test loss: 0.6904 score: 0.7143 time: 0.07s
Epoch 37/1000, LR 0.000270
Train loss: 1.7619;  Loss pred: 1.7619; Loss self: 0.0000; time: 0.20s
Val loss: 0.6918 score: 0.6122 time: 0.06s
Test loss: 0.6903 score: 0.7143 time: 0.07s
Epoch 38/1000, LR 0.000270
Train loss: 1.7555;  Loss pred: 1.7555; Loss self: 0.0000; time: 0.21s
Val loss: 0.6917 score: 0.6122 time: 0.06s
Test loss: 0.6902 score: 0.7143 time: 0.07s
Epoch 39/1000, LR 0.000269
Train loss: 1.7441;  Loss pred: 1.7441; Loss self: 0.0000; time: 0.21s
Val loss: 0.6916 score: 0.6122 time: 0.06s
Test loss: 0.6900 score: 0.7143 time: 0.07s
Epoch 40/1000, LR 0.000269
Train loss: 1.7068;  Loss pred: 1.7068; Loss self: 0.0000; time: 0.20s
Val loss: 0.6915 score: 0.6122 time: 0.06s
Test loss: 0.6899 score: 0.7143 time: 0.07s
Epoch 41/1000, LR 0.000269
Train loss: 1.6752;  Loss pred: 1.6752; Loss self: 0.0000; time: 0.20s
Val loss: 0.6914 score: 0.6122 time: 0.06s
Test loss: 0.6898 score: 0.7143 time: 0.07s
Epoch 42/1000, LR 0.000269
Train loss: 1.6694;  Loss pred: 1.6694; Loss self: 0.0000; time: 0.21s
Val loss: 0.6914 score: 0.6122 time: 0.07s
Test loss: 0.6897 score: 0.7143 time: 0.07s
Epoch 43/1000, LR 0.000269
Train loss: 1.6376;  Loss pred: 1.6376; Loss self: 0.0000; time: 0.20s
Val loss: 0.6913 score: 0.6327 time: 0.06s
Test loss: 0.6896 score: 0.6735 time: 0.07s
Epoch 44/1000, LR 0.000269
Train loss: 1.6355;  Loss pred: 1.6355; Loss self: 0.0000; time: 0.20s
Val loss: 0.6912 score: 0.6122 time: 0.07s
Test loss: 0.6894 score: 0.6735 time: 0.07s
Epoch 45/1000, LR 0.000269
Train loss: 1.6138;  Loss pred: 1.6138; Loss self: 0.0000; time: 0.20s
Val loss: 0.6911 score: 0.6122 time: 0.06s
Test loss: 0.6893 score: 0.6735 time: 0.07s
Epoch 46/1000, LR 0.000269
Train loss: 1.5835;  Loss pred: 1.5835; Loss self: 0.0000; time: 4.48s
Val loss: 0.6910 score: 0.6122 time: 0.15s
Test loss: 0.6891 score: 0.6735 time: 0.25s
Epoch 47/1000, LR 0.000269
Train loss: 1.5905;  Loss pred: 1.5905; Loss self: 0.0000; time: 0.62s
Val loss: 0.6909 score: 0.6327 time: 0.06s
Test loss: 0.6890 score: 0.6939 time: 0.08s
Epoch 48/1000, LR 0.000269
Train loss: 1.5599;  Loss pred: 1.5599; Loss self: 0.0000; time: 0.21s
Val loss: 0.6907 score: 0.6327 time: 0.06s
Test loss: 0.6888 score: 0.6939 time: 0.07s
Epoch 49/1000, LR 0.000269
Train loss: 1.5479;  Loss pred: 1.5479; Loss self: 0.0000; time: 0.20s
Val loss: 0.6906 score: 0.6327 time: 0.06s
Test loss: 0.6886 score: 0.7143 time: 0.07s
Epoch 50/1000, LR 0.000269
Train loss: 1.5207;  Loss pred: 1.5207; Loss self: 0.0000; time: 0.21s
Val loss: 0.6905 score: 0.6327 time: 0.06s
Test loss: 0.6884 score: 0.7551 time: 0.07s
Epoch 51/1000, LR 0.000269
Train loss: 1.5063;  Loss pred: 1.5063; Loss self: 0.0000; time: 0.20s
Val loss: 0.6904 score: 0.6735 time: 0.06s
Test loss: 0.6883 score: 0.7551 time: 0.07s
Epoch 52/1000, LR 0.000269
Train loss: 1.4911;  Loss pred: 1.4911; Loss self: 0.0000; time: 0.20s
Val loss: 0.6903 score: 0.7551 time: 0.06s
Test loss: 0.6881 score: 0.7755 time: 0.07s
Epoch 53/1000, LR 0.000269
Train loss: 1.4817;  Loss pred: 1.4817; Loss self: 0.0000; time: 0.22s
Val loss: 0.6901 score: 0.7551 time: 0.07s
Test loss: 0.6879 score: 0.8367 time: 0.07s
Epoch 54/1000, LR 0.000269
Train loss: 1.4734;  Loss pred: 1.4734; Loss self: 0.0000; time: 0.21s
Val loss: 0.6900 score: 0.7959 time: 0.07s
Test loss: 0.6876 score: 0.8776 time: 0.07s
Epoch 55/1000, LR 0.000269
Train loss: 1.4506;  Loss pred: 1.4506; Loss self: 0.0000; time: 0.22s
Val loss: 0.6899 score: 0.8163 time: 0.07s
Test loss: 0.6874 score: 0.8980 time: 0.08s
Epoch 56/1000, LR 0.000269
Train loss: 1.4345;  Loss pred: 1.4345; Loss self: 0.0000; time: 0.22s
Val loss: 0.6897 score: 0.8367 time: 0.06s
Test loss: 0.6872 score: 0.8980 time: 0.08s
Epoch 57/1000, LR 0.000269
Train loss: 1.4262;  Loss pred: 1.4262; Loss self: 0.0000; time: 0.22s
Val loss: 0.6896 score: 0.8367 time: 0.07s
Test loss: 0.6870 score: 0.9184 time: 0.08s
Epoch 58/1000, LR 0.000269
Train loss: 1.4093;  Loss pred: 1.4093; Loss self: 0.0000; time: 0.20s
Val loss: 0.6894 score: 0.8367 time: 0.06s
Test loss: 0.6867 score: 0.9184 time: 0.07s
Epoch 59/1000, LR 0.000268
Train loss: 1.4025;  Loss pred: 1.4025; Loss self: 0.0000; time: 0.20s
Val loss: 0.6891 score: 0.8163 time: 0.06s
Test loss: 0.6864 score: 0.8980 time: 0.07s
Epoch 60/1000, LR 0.000268
Train loss: 1.3859;  Loss pred: 1.3859; Loss self: 0.0000; time: 0.20s
Val loss: 0.6889 score: 0.7755 time: 0.06s
Test loss: 0.6860 score: 0.8980 time: 0.07s
Epoch 61/1000, LR 0.000268
Train loss: 1.3653;  Loss pred: 1.3653; Loss self: 0.0000; time: 0.20s
Val loss: 0.6886 score: 0.7755 time: 0.06s
Test loss: 0.6857 score: 0.8980 time: 0.07s
Epoch 62/1000, LR 0.000268
Train loss: 1.3641;  Loss pred: 1.3641; Loss self: 0.0000; time: 0.21s
Val loss: 0.6884 score: 0.7551 time: 0.06s
Test loss: 0.6854 score: 0.8367 time: 0.07s
Epoch 63/1000, LR 0.000268
Train loss: 1.3497;  Loss pred: 1.3497; Loss self: 0.0000; time: 0.20s
Val loss: 0.6882 score: 0.7551 time: 0.06s
Test loss: 0.6850 score: 0.8163 time: 0.16s
Epoch 64/1000, LR 0.000268
Train loss: 1.3386;  Loss pred: 1.3386; Loss self: 0.0000; time: 0.20s
Val loss: 0.6879 score: 0.7551 time: 0.06s
Test loss: 0.6847 score: 0.7755 time: 0.07s
Epoch 65/1000, LR 0.000268
Train loss: 1.3338;  Loss pred: 1.3338; Loss self: 0.0000; time: 0.20s
Val loss: 0.6877 score: 0.7551 time: 0.06s
Test loss: 0.6844 score: 0.7755 time: 0.07s
Epoch 66/1000, LR 0.000268
Train loss: 1.3174;  Loss pred: 1.3174; Loss self: 0.0000; time: 0.21s
Val loss: 0.6874 score: 0.7551 time: 0.06s
Test loss: 0.6840 score: 0.7755 time: 0.07s
Epoch 67/1000, LR 0.000268
Train loss: 1.3084;  Loss pred: 1.3084; Loss self: 0.0000; time: 0.21s
Val loss: 0.6871 score: 0.7755 time: 0.07s
Test loss: 0.6837 score: 0.7755 time: 0.07s
Epoch 68/1000, LR 0.000268
Train loss: 1.3008;  Loss pred: 1.3008; Loss self: 0.0000; time: 0.21s
Val loss: 0.6868 score: 0.7551 time: 0.06s
Test loss: 0.6833 score: 0.7755 time: 0.17s
Epoch 69/1000, LR 0.000268
Train loss: 1.2946;  Loss pred: 1.2946; Loss self: 0.0000; time: 0.20s
Val loss: 0.6866 score: 0.7347 time: 0.06s
Test loss: 0.6830 score: 0.7755 time: 0.07s
Epoch 70/1000, LR 0.000268
Train loss: 1.2770;  Loss pred: 1.2770; Loss self: 0.0000; time: 0.21s
Val loss: 0.6863 score: 0.7551 time: 0.07s
Test loss: 0.6826 score: 0.7755 time: 0.07s
Epoch 71/1000, LR 0.000268
Train loss: 1.2754;  Loss pred: 1.2754; Loss self: 0.0000; time: 0.21s
Val loss: 0.6860 score: 0.7755 time: 0.08s
Test loss: 0.6822 score: 0.7755 time: 0.08s
Epoch 72/1000, LR 0.000267
Train loss: 1.2537;  Loss pred: 1.2537; Loss self: 0.0000; time: 0.20s
Val loss: 0.6856 score: 0.7755 time: 0.06s
Test loss: 0.6818 score: 0.7755 time: 0.07s
Epoch 73/1000, LR 0.000267
Train loss: 1.2577;  Loss pred: 1.2577; Loss self: 0.0000; time: 0.22s
Val loss: 0.6853 score: 0.7551 time: 0.07s
Test loss: 0.6814 score: 0.7755 time: 0.07s
Epoch 74/1000, LR 0.000267
Train loss: 1.2499;  Loss pred: 1.2499; Loss self: 0.0000; time: 0.20s
Val loss: 0.6850 score: 0.7551 time: 0.06s
Test loss: 0.6810 score: 0.7755 time: 0.07s
Epoch 75/1000, LR 0.000267
Train loss: 1.2403;  Loss pred: 1.2403; Loss self: 0.0000; time: 0.20s
Val loss: 0.6846 score: 0.7959 time: 0.06s
Test loss: 0.6805 score: 0.7959 time: 0.07s
Epoch 76/1000, LR 0.000267
Train loss: 1.2313;  Loss pred: 1.2313; Loss self: 0.0000; time: 0.22s
Val loss: 0.6843 score: 0.7755 time: 0.06s
Test loss: 0.6800 score: 0.7959 time: 0.07s
Epoch 77/1000, LR 0.000267
Train loss: 1.2226;  Loss pred: 1.2226; Loss self: 0.0000; time: 0.88s
Val loss: 0.6839 score: 0.7959 time: 0.62s
Test loss: 0.6795 score: 0.8163 time: 1.84s
Epoch 78/1000, LR 0.000267
Train loss: 1.2134;  Loss pred: 1.2134; Loss self: 0.0000; time: 1.83s
Val loss: 0.6835 score: 0.7959 time: 0.11s
Test loss: 0.6790 score: 0.8776 time: 0.33s
Epoch 79/1000, LR 0.000267
Train loss: 1.2111;  Loss pred: 1.2111; Loss self: 0.0000; time: 0.42s
Val loss: 0.6832 score: 0.8163 time: 0.07s
Test loss: 0.6785 score: 0.8980 time: 0.08s
Epoch 80/1000, LR 0.000267
Train loss: 1.2084;  Loss pred: 1.2084; Loss self: 0.0000; time: 0.22s
Val loss: 0.6828 score: 0.8367 time: 0.07s
Test loss: 0.6779 score: 0.9388 time: 0.07s
Epoch 81/1000, LR 0.000267
Train loss: 1.2018;  Loss pred: 1.2018; Loss self: 0.0000; time: 0.21s
Val loss: 0.6823 score: 0.8571 time: 0.07s
Test loss: 0.6773 score: 0.9388 time: 0.08s
Epoch 82/1000, LR 0.000267
Train loss: 1.1886;  Loss pred: 1.1886; Loss self: 0.0000; time: 0.21s
Val loss: 0.6819 score: 0.8571 time: 0.07s
Test loss: 0.6768 score: 0.9388 time: 0.08s
Epoch 83/1000, LR 0.000266
Train loss: 1.1841;  Loss pred: 1.1841; Loss self: 0.0000; time: 0.21s
Val loss: 0.6814 score: 0.8571 time: 0.07s
Test loss: 0.6761 score: 0.9592 time: 0.07s
Epoch 84/1000, LR 0.000266
Train loss: 1.1840;  Loss pred: 1.1840; Loss self: 0.0000; time: 0.21s
Val loss: 0.6810 score: 0.8571 time: 0.06s
Test loss: 0.6755 score: 0.9592 time: 0.07s
Epoch 85/1000, LR 0.000266
Train loss: 1.1750;  Loss pred: 1.1750; Loss self: 0.0000; time: 0.21s
Val loss: 0.6805 score: 0.8571 time: 0.06s
Test loss: 0.6748 score: 0.9592 time: 0.07s
Epoch 86/1000, LR 0.000266
Train loss: 1.1704;  Loss pred: 1.1704; Loss self: 0.0000; time: 0.19s
Val loss: 0.6800 score: 0.8571 time: 0.06s
Test loss: 0.6742 score: 0.9592 time: 0.07s
Epoch 87/1000, LR 0.000266
Train loss: 1.1593;  Loss pred: 1.1593; Loss self: 0.0000; time: 0.20s
Val loss: 0.6795 score: 0.8571 time: 0.06s
Test loss: 0.6735 score: 0.9592 time: 0.07s
Epoch 88/1000, LR 0.000266
Train loss: 1.1599;  Loss pred: 1.1599; Loss self: 0.0000; time: 0.20s
Val loss: 0.6789 score: 0.8571 time: 0.06s
Test loss: 0.6728 score: 0.9592 time: 0.07s
Epoch 89/1000, LR 0.000266
Train loss: 1.1465;  Loss pred: 1.1465; Loss self: 0.0000; time: 0.21s
Val loss: 0.6783 score: 0.8571 time: 0.06s
Test loss: 0.6721 score: 0.9592 time: 0.07s
Epoch 90/1000, LR 0.000266
Train loss: 1.1451;  Loss pred: 1.1451; Loss self: 0.0000; time: 0.21s
Val loss: 0.6778 score: 0.8571 time: 0.06s
Test loss: 0.6713 score: 0.9592 time: 0.07s
Epoch 91/1000, LR 0.000266
Train loss: 1.1425;  Loss pred: 1.1425; Loss self: 0.0000; time: 0.20s
Val loss: 0.6771 score: 0.8571 time: 0.06s
Test loss: 0.6705 score: 0.9592 time: 0.07s
Epoch 92/1000, LR 0.000266
Train loss: 1.1300;  Loss pred: 1.1300; Loss self: 0.0000; time: 0.20s
Val loss: 0.6765 score: 0.8571 time: 0.06s
Test loss: 0.6698 score: 0.9592 time: 0.07s
Epoch 93/1000, LR 0.000265
Train loss: 1.1323;  Loss pred: 1.1323; Loss self: 0.0000; time: 0.21s
Val loss: 0.6759 score: 0.8571 time: 0.07s
Test loss: 0.6689 score: 0.9592 time: 0.07s
Epoch 94/1000, LR 0.000265
Train loss: 1.1234;  Loss pred: 1.1234; Loss self: 0.0000; time: 0.21s
Val loss: 0.6752 score: 0.8571 time: 0.06s
Test loss: 0.6681 score: 0.9592 time: 0.07s
Epoch 95/1000, LR 0.000265
Train loss: 1.1142;  Loss pred: 1.1142; Loss self: 0.0000; time: 0.20s
Val loss: 0.6745 score: 0.8571 time: 0.06s
Test loss: 0.6672 score: 0.9592 time: 0.07s
Epoch 96/1000, LR 0.000265
Train loss: 1.1157;  Loss pred: 1.1157; Loss self: 0.0000; time: 0.20s
Val loss: 0.6738 score: 0.8571 time: 0.06s
Test loss: 0.6663 score: 0.9592 time: 0.07s
Epoch 97/1000, LR 0.000265
Train loss: 1.1132;  Loss pred: 1.1132; Loss self: 0.0000; time: 0.20s
Val loss: 0.6731 score: 0.8571 time: 0.06s
Test loss: 0.6654 score: 0.9592 time: 0.07s
Epoch 98/1000, LR 0.000265
Train loss: 1.1077;  Loss pred: 1.1077; Loss self: 0.0000; time: 0.20s
Val loss: 0.6723 score: 0.8571 time: 0.06s
Test loss: 0.6645 score: 0.9592 time: 0.07s
Epoch 99/1000, LR 0.000265
Train loss: 1.1005;  Loss pred: 1.1005; Loss self: 0.0000; time: 0.20s
Val loss: 0.6715 score: 0.8571 time: 0.06s
Test loss: 0.6635 score: 0.9592 time: 0.07s
Epoch 100/1000, LR 0.000265
Train loss: 1.0983;  Loss pred: 1.0983; Loss self: 0.0000; time: 4.95s
Val loss: 0.6708 score: 0.8571 time: 0.53s
Test loss: 0.6625 score: 0.9592 time: 1.19s
Epoch 101/1000, LR 0.000265
Train loss: 1.0921;  Loss pred: 1.0921; Loss self: 0.0000; time: 0.85s
Val loss: 0.6699 score: 0.8571 time: 0.07s
Test loss: 0.6614 score: 0.9592 time: 0.07s
Epoch 102/1000, LR 0.000264
Train loss: 1.0929;  Loss pred: 1.0929; Loss self: 0.0000; time: 0.21s
Val loss: 0.6691 score: 0.8571 time: 0.06s
Test loss: 0.6603 score: 0.9592 time: 0.07s
Epoch 103/1000, LR 0.000264
Train loss: 1.0850;  Loss pred: 1.0850; Loss self: 0.0000; time: 0.21s
Val loss: 0.6683 score: 0.8776 time: 0.06s
Test loss: 0.6592 score: 0.9592 time: 0.07s
Epoch 104/1000, LR 0.000264
Train loss: 1.0826;  Loss pred: 1.0826; Loss self: 0.0000; time: 0.20s
Val loss: 0.6674 score: 0.8776 time: 0.06s
Test loss: 0.6581 score: 0.9592 time: 0.08s
Epoch 105/1000, LR 0.000264
Train loss: 1.0797;  Loss pred: 1.0797; Loss self: 0.0000; time: 0.20s
Val loss: 0.6665 score: 0.8776 time: 0.06s
Test loss: 0.6569 score: 0.9592 time: 0.07s
Epoch 106/1000, LR 0.000264
Train loss: 1.0748;  Loss pred: 1.0748; Loss self: 0.0000; time: 0.21s
Val loss: 0.6655 score: 0.8776 time: 0.06s
Test loss: 0.6557 score: 0.9592 time: 0.07s
Epoch 107/1000, LR 0.000264
Train loss: 1.0708;  Loss pred: 1.0708; Loss self: 0.0000; time: 0.20s
Val loss: 0.6646 score: 0.8776 time: 0.06s
Test loss: 0.6545 score: 0.9592 time: 0.07s
Epoch 108/1000, LR 0.000264
Train loss: 1.0724;  Loss pred: 1.0724; Loss self: 0.0000; time: 0.20s
Val loss: 0.6636 score: 0.8776 time: 0.06s
Test loss: 0.6532 score: 0.9592 time: 0.07s
Epoch 109/1000, LR 0.000264
Train loss: 1.0644;  Loss pred: 1.0644; Loss self: 0.0000; time: 0.20s
Val loss: 0.6626 score: 0.8776 time: 0.06s
Test loss: 0.6519 score: 0.9592 time: 0.07s
Epoch 110/1000, LR 0.000263
Train loss: 1.0577;  Loss pred: 1.0577; Loss self: 0.0000; time: 0.20s
Val loss: 0.6615 score: 0.8776 time: 0.06s
Test loss: 0.6506 score: 0.9592 time: 0.07s
Epoch 111/1000, LR 0.000263
Train loss: 1.0579;  Loss pred: 1.0579; Loss self: 0.0000; time: 0.21s
Val loss: 0.6605 score: 0.8776 time: 0.06s
Test loss: 0.6492 score: 0.9592 time: 0.07s
Epoch 112/1000, LR 0.000263
Train loss: 1.0534;  Loss pred: 1.0534; Loss self: 0.0000; time: 3.50s
Val loss: 0.6594 score: 0.8776 time: 0.24s
Test loss: 0.6478 score: 0.9592 time: 0.91s
Epoch 113/1000, LR 0.000263
Train loss: 1.0473;  Loss pred: 1.0473; Loss self: 0.0000; time: 0.77s
Val loss: 0.6582 score: 0.8776 time: 0.06s
Test loss: 0.6464 score: 0.9592 time: 0.07s
Epoch 114/1000, LR 0.000263
Train loss: 1.0449;  Loss pred: 1.0449; Loss self: 0.0000; time: 0.19s
Val loss: 0.6571 score: 0.8980 time: 0.06s
Test loss: 0.6449 score: 0.9592 time: 0.06s
Epoch 115/1000, LR 0.000263
Train loss: 1.0464;  Loss pred: 1.0464; Loss self: 0.0000; time: 0.19s
Val loss: 0.6559 score: 0.8980 time: 0.06s
Test loss: 0.6434 score: 0.9592 time: 0.07s
Epoch 116/1000, LR 0.000263
Train loss: 1.0393;  Loss pred: 1.0393; Loss self: 0.0000; time: 0.20s
Val loss: 0.6547 score: 0.8980 time: 0.06s
Test loss: 0.6419 score: 0.9592 time: 0.07s
Epoch 117/1000, LR 0.000262
Train loss: 1.0368;  Loss pred: 1.0368; Loss self: 0.0000; time: 0.19s
Val loss: 0.6535 score: 0.8980 time: 0.06s
Test loss: 0.6403 score: 0.9592 time: 0.07s
Epoch 118/1000, LR 0.000262
Train loss: 1.0372;  Loss pred: 1.0372; Loss self: 0.0000; time: 0.20s
Val loss: 0.6522 score: 0.8980 time: 0.06s
Test loss: 0.6387 score: 0.9592 time: 0.07s
Epoch 119/1000, LR 0.000262
Train loss: 1.0331;  Loss pred: 1.0331; Loss self: 0.0000; time: 0.20s
Val loss: 0.6509 score: 0.8980 time: 0.06s
Test loss: 0.6370 score: 0.9592 time: 0.07s
Epoch 120/1000, LR 0.000262
Train loss: 1.0301;  Loss pred: 1.0301; Loss self: 0.0000; time: 0.19s
Val loss: 0.6495 score: 0.8980 time: 0.06s
Test loss: 0.6353 score: 0.9592 time: 0.06s
Epoch 121/1000, LR 0.000262
Train loss: 1.0236;  Loss pred: 1.0236; Loss self: 0.0000; time: 0.20s
Val loss: 0.6481 score: 0.8980 time: 0.06s
Test loss: 0.6335 score: 0.9592 time: 0.07s
Epoch 122/1000, LR 0.000262
Train loss: 1.0257;  Loss pred: 1.0257; Loss self: 0.0000; time: 0.20s
Val loss: 0.6467 score: 0.8980 time: 0.06s
Test loss: 0.6317 score: 0.9592 time: 0.07s
Epoch 123/1000, LR 0.000262
Train loss: 1.0179;  Loss pred: 1.0179; Loss self: 0.0000; time: 0.19s
Val loss: 0.6452 score: 0.8980 time: 0.06s
Test loss: 0.6298 score: 0.9592 time: 0.07s
Epoch 124/1000, LR 0.000261
Train loss: 1.0148;  Loss pred: 1.0148; Loss self: 0.0000; time: 0.20s
Val loss: 0.6437 score: 0.8980 time: 0.06s
Test loss: 0.6279 score: 0.9592 time: 0.06s
Epoch 125/1000, LR 0.000261
Train loss: 1.0122;  Loss pred: 1.0122; Loss self: 0.0000; time: 0.20s
Val loss: 0.6422 score: 0.8980 time: 0.06s
Test loss: 0.6259 score: 0.9592 time: 0.07s
Epoch 126/1000, LR 0.000261
Train loss: 1.0080;  Loss pred: 1.0080; Loss self: 0.0000; time: 0.19s
Val loss: 0.6406 score: 0.8980 time: 0.06s
Test loss: 0.6239 score: 0.9592 time: 0.07s
Epoch 127/1000, LR 0.000261
Train loss: 1.0059;  Loss pred: 1.0059; Loss self: 0.0000; time: 0.19s
Val loss: 0.6390 score: 0.9184 time: 0.06s
Test loss: 0.6218 score: 0.9592 time: 0.07s
Epoch 128/1000, LR 0.000261
Train loss: 1.0054;  Loss pred: 1.0054; Loss self: 0.0000; time: 0.21s
Val loss: 0.6373 score: 0.9184 time: 0.06s
Test loss: 0.6197 score: 0.9592 time: 0.07s
Epoch 129/1000, LR 0.000261
Train loss: 1.0020;  Loss pred: 1.0020; Loss self: 0.0000; time: 0.20s
Val loss: 0.6356 score: 0.9184 time: 0.06s
Test loss: 0.6175 score: 0.9592 time: 0.06s
Epoch 130/1000, LR 0.000260
Train loss: 0.9959;  Loss pred: 0.9959; Loss self: 0.0000; time: 0.20s
Val loss: 0.6339 score: 0.9184 time: 0.06s
Test loss: 0.6153 score: 0.9796 time: 0.07s
Epoch 131/1000, LR 0.000260
Train loss: 0.9970;  Loss pred: 0.9970; Loss self: 0.0000; time: 0.19s
Val loss: 0.6321 score: 0.9184 time: 0.06s
Test loss: 0.6131 score: 0.9796 time: 0.07s
Epoch 132/1000, LR 0.000260
Train loss: 0.9940;  Loss pred: 0.9940; Loss self: 0.0000; time: 0.19s
Val loss: 0.6303 score: 0.9184 time: 0.06s
Test loss: 0.6108 score: 0.9796 time: 0.07s
Epoch 133/1000, LR 0.000260
Train loss: 0.9899;  Loss pred: 0.9899; Loss self: 0.0000; time: 0.19s
Val loss: 0.6285 score: 0.9184 time: 0.06s
Test loss: 0.6085 score: 0.9796 time: 0.06s
Epoch 134/1000, LR 0.000260
Train loss: 0.9880;  Loss pred: 0.9880; Loss self: 0.0000; time: 0.20s
Val loss: 0.6267 score: 0.9184 time: 0.07s
Test loss: 0.6062 score: 0.9796 time: 0.08s
Epoch 135/1000, LR 0.000260
Train loss: 0.9852;  Loss pred: 0.9852; Loss self: 0.0000; time: 0.21s
Val loss: 0.6249 score: 0.9184 time: 0.05s
Test loss: 0.6038 score: 0.9796 time: 0.07s
Epoch 136/1000, LR 0.000260
Train loss: 0.9821;  Loss pred: 0.9821; Loss self: 0.0000; time: 0.19s
Val loss: 0.6230 score: 0.9184 time: 0.06s
Test loss: 0.6014 score: 0.9796 time: 0.07s
Epoch 137/1000, LR 0.000259
Train loss: 0.9769;  Loss pred: 0.9769; Loss self: 0.0000; time: 0.21s
Val loss: 0.6211 score: 0.9184 time: 0.06s
Test loss: 0.5989 score: 0.9796 time: 0.07s
Epoch 138/1000, LR 0.000259
Train loss: 0.9750;  Loss pred: 0.9750; Loss self: 0.0000; time: 0.20s
Val loss: 0.6191 score: 0.9184 time: 0.06s
Test loss: 0.5964 score: 0.9796 time: 0.07s
Epoch 139/1000, LR 0.000259
Train loss: 0.9692;  Loss pred: 0.9692; Loss self: 0.0000; time: 0.19s
Val loss: 0.6172 score: 0.9184 time: 0.07s
Test loss: 0.5939 score: 0.9796 time: 0.07s
Epoch 140/1000, LR 0.000259
Train loss: 0.9720;  Loss pred: 0.9720; Loss self: 0.0000; time: 0.21s
Val loss: 0.6152 score: 0.9184 time: 0.06s
Test loss: 0.5913 score: 0.9796 time: 0.08s
Epoch 141/1000, LR 0.000259
Train loss: 0.9644;  Loss pred: 0.9644; Loss self: 0.0000; time: 0.21s
Val loss: 0.6132 score: 0.9184 time: 0.07s
Test loss: 0.5887 score: 0.9796 time: 0.08s
Epoch 142/1000, LR 0.000259
Train loss: 0.9613;  Loss pred: 0.9613; Loss self: 0.0000; time: 0.21s
Val loss: 0.6112 score: 0.9184 time: 0.06s
Test loss: 0.5860 score: 0.9796 time: 0.08s
Epoch 143/1000, LR 0.000258
Train loss: 0.9593;  Loss pred: 0.9593; Loss self: 0.0000; time: 0.21s
Val loss: 0.6091 score: 0.9184 time: 0.07s
Test loss: 0.5833 score: 0.9796 time: 0.07s
Epoch 144/1000, LR 0.000258
Train loss: 0.9591;  Loss pred: 0.9591; Loss self: 0.0000; time: 0.21s
Val loss: 0.6070 score: 0.9184 time: 0.07s
Test loss: 0.5806 score: 0.9796 time: 0.07s
Epoch 145/1000, LR 0.000258
Train loss: 0.9533;  Loss pred: 0.9533; Loss self: 0.0000; time: 0.21s
Val loss: 0.6048 score: 0.9184 time: 0.07s
Test loss: 0.5778 score: 0.9796 time: 0.08s
Epoch 146/1000, LR 0.000258
Train loss: 0.9541;  Loss pred: 0.9541; Loss self: 0.0000; time: 0.21s
Val loss: 0.6027 score: 0.9184 time: 0.06s
Test loss: 0.5750 score: 0.9796 time: 0.07s
Epoch 147/1000, LR 0.000258
Train loss: 0.9481;  Loss pred: 0.9481; Loss self: 0.0000; time: 0.21s
Val loss: 0.6004 score: 0.9184 time: 0.06s
Test loss: 0.5721 score: 0.9796 time: 0.07s
Epoch 148/1000, LR 0.000257
Train loss: 0.9476;  Loss pred: 0.9476; Loss self: 0.0000; time: 0.21s
Val loss: 0.5982 score: 0.9184 time: 0.07s
Test loss: 0.5692 score: 0.9796 time: 0.07s
Epoch 149/1000, LR 0.000257
Train loss: 0.9426;  Loss pred: 0.9426; Loss self: 0.0000; time: 0.22s
Val loss: 0.5959 score: 0.9184 time: 0.06s
Test loss: 0.5663 score: 0.9796 time: 0.07s
Epoch 150/1000, LR 0.000257
Train loss: 0.9407;  Loss pred: 0.9407; Loss self: 0.0000; time: 0.21s
Val loss: 0.5936 score: 0.9184 time: 0.06s
Test loss: 0.5633 score: 0.9796 time: 0.08s
Epoch 151/1000, LR 0.000257
Train loss: 0.9355;  Loss pred: 0.9355; Loss self: 0.0000; time: 0.22s
Val loss: 0.5912 score: 0.9184 time: 0.07s
Test loss: 0.5603 score: 0.9796 time: 0.07s
Epoch 152/1000, LR 0.000257
Train loss: 0.9329;  Loss pred: 0.9329; Loss self: 0.0000; time: 0.22s
Val loss: 0.5888 score: 0.9184 time: 0.07s
Test loss: 0.5572 score: 0.9796 time: 0.08s
Epoch 153/1000, LR 0.000257
Train loss: 0.9310;  Loss pred: 0.9310; Loss self: 0.0000; time: 0.23s
Val loss: 0.5864 score: 0.9184 time: 0.06s
Test loss: 0.5541 score: 0.9796 time: 0.07s
Epoch 154/1000, LR 0.000256
Train loss: 0.9263;  Loss pred: 0.9263; Loss self: 0.0000; time: 0.20s
Val loss: 0.5840 score: 0.9184 time: 0.06s
Test loss: 0.5510 score: 0.9796 time: 0.07s
Epoch 155/1000, LR 0.000256
Train loss: 0.9237;  Loss pred: 0.9237; Loss self: 0.0000; time: 0.21s
Val loss: 0.5815 score: 0.9184 time: 0.06s
Test loss: 0.5478 score: 0.9796 time: 0.07s
Epoch 156/1000, LR 0.000256
Train loss: 0.9218;  Loss pred: 0.9218; Loss self: 0.0000; time: 0.21s
Val loss: 0.5790 score: 0.9184 time: 0.07s
Test loss: 0.5446 score: 0.9796 time: 0.08s
Epoch 157/1000, LR 0.000256
Train loss: 0.9202;  Loss pred: 0.9202; Loss self: 0.0000; time: 0.21s
Val loss: 0.5765 score: 0.9184 time: 0.06s
Test loss: 0.5414 score: 0.9796 time: 0.07s
Epoch 158/1000, LR 0.000256
Train loss: 0.9140;  Loss pred: 0.9140; Loss self: 0.0000; time: 0.20s
Val loss: 0.5740 score: 0.9184 time: 0.06s
Test loss: 0.5381 score: 0.9796 time: 0.07s
Epoch 159/1000, LR 0.000255
Train loss: 0.9154;  Loss pred: 0.9154; Loss self: 0.0000; time: 0.20s
Val loss: 0.5714 score: 0.9184 time: 0.06s
Test loss: 0.5348 score: 0.9796 time: 0.07s
Epoch 160/1000, LR 0.000255
Train loss: 0.9098;  Loss pred: 0.9098; Loss self: 0.0000; time: 0.20s
Val loss: 0.5688 score: 0.9184 time: 0.07s
Test loss: 0.5315 score: 0.9796 time: 0.07s
Epoch 161/1000, LR 0.000255
Train loss: 0.9057;  Loss pred: 0.9057; Loss self: 0.0000; time: 0.22s
Val loss: 0.5662 score: 0.9184 time: 0.06s
Test loss: 0.5281 score: 0.9796 time: 0.08s
Epoch 162/1000, LR 0.000255
Train loss: 0.9033;  Loss pred: 0.9033; Loss self: 0.0000; time: 0.21s
Val loss: 0.5636 score: 0.9184 time: 0.06s
Test loss: 0.5247 score: 0.9796 time: 0.07s
Epoch 163/1000, LR 0.000255
Train loss: 0.9005;  Loss pred: 0.9005; Loss self: 0.0000; time: 0.20s
Val loss: 0.5610 score: 0.9184 time: 0.06s
Test loss: 0.5213 score: 0.9796 time: 0.07s
Epoch 164/1000, LR 0.000254
Train loss: 0.8996;  Loss pred: 0.8996; Loss self: 0.0000; time: 0.22s
Val loss: 0.5584 score: 0.9184 time: 0.06s
Test loss: 0.5178 score: 0.9796 time: 0.07s
Epoch 165/1000, LR 0.000254
Train loss: 0.8939;  Loss pred: 0.8939; Loss self: 0.0000; time: 0.20s
Val loss: 0.5557 score: 0.9184 time: 0.07s
Test loss: 0.5144 score: 0.9796 time: 0.07s
Epoch 166/1000, LR 0.000254
Train loss: 0.8920;  Loss pred: 0.8920; Loss self: 0.0000; time: 0.22s
Val loss: 0.5531 score: 0.9184 time: 0.07s
Test loss: 0.5109 score: 0.9796 time: 0.07s
Epoch 167/1000, LR 0.000254
Train loss: 0.8911;  Loss pred: 0.8911; Loss self: 0.0000; time: 0.21s
Val loss: 0.5504 score: 0.9184 time: 0.06s
Test loss: 0.5073 score: 0.9796 time: 0.07s
Epoch 168/1000, LR 0.000254
Train loss: 0.8857;  Loss pred: 0.8857; Loss self: 0.0000; time: 0.20s
Val loss: 0.5477 score: 0.9184 time: 0.06s
Test loss: 0.5038 score: 0.9796 time: 0.07s
Epoch 169/1000, LR 0.000253
Train loss: 0.8803;  Loss pred: 0.8803; Loss self: 0.0000; time: 0.22s
Val loss: 0.5450 score: 0.9184 time: 0.07s
Test loss: 0.5002 score: 0.9796 time: 0.07s
Epoch 170/1000, LR 0.000253
Train loss: 0.8788;  Loss pred: 0.8788; Loss self: 0.0000; time: 0.20s
Val loss: 0.5423 score: 0.9184 time: 0.06s
Test loss: 0.4966 score: 1.0000 time: 0.07s
Epoch 171/1000, LR 0.000253
Train loss: 0.8771;  Loss pred: 0.8771; Loss self: 0.0000; time: 0.21s
Val loss: 0.5395 score: 0.9184 time: 0.07s
Test loss: 0.4930 score: 1.0000 time: 0.07s
Epoch 172/1000, LR 0.000253
Train loss: 0.8735;  Loss pred: 0.8735; Loss self: 0.0000; time: 0.21s
Val loss: 0.5368 score: 0.9184 time: 0.66s
Test loss: 0.4893 score: 1.0000 time: 2.04s
Epoch 173/1000, LR 0.000253
Train loss: 0.8713;  Loss pred: 0.8713; Loss self: 0.0000; time: 5.56s
Val loss: 0.5340 score: 0.9184 time: 0.46s
Test loss: 0.4857 score: 1.0000 time: 0.09s
Epoch 174/1000, LR 0.000252
Train loss: 0.8684;  Loss pred: 0.8684; Loss self: 0.0000; time: 0.23s
Val loss: 0.5313 score: 0.9184 time: 0.07s
Test loss: 0.4820 score: 1.0000 time: 0.07s
Epoch 175/1000, LR 0.000252
Train loss: 0.8634;  Loss pred: 0.8634; Loss self: 0.0000; time: 0.23s
Val loss: 0.5285 score: 0.9184 time: 0.08s
Test loss: 0.4783 score: 1.0000 time: 0.07s
Epoch 176/1000, LR 0.000252
Train loss: 0.8611;  Loss pred: 0.8611; Loss self: 0.0000; time: 0.20s
Val loss: 0.5257 score: 0.9184 time: 0.06s
Test loss: 0.4746 score: 1.0000 time: 0.07s
Epoch 177/1000, LR 0.000252
Train loss: 0.8562;  Loss pred: 0.8562; Loss self: 0.0000; time: 0.20s
Val loss: 0.5229 score: 0.9184 time: 0.07s
Test loss: 0.4709 score: 1.0000 time: 0.07s
Epoch 178/1000, LR 0.000251
Train loss: 0.8570;  Loss pred: 0.8570; Loss self: 0.0000; time: 0.21s
Val loss: 0.5201 score: 0.9184 time: 0.06s
Test loss: 0.4672 score: 1.0000 time: 0.07s
Epoch 179/1000, LR 0.000251
Train loss: 0.8498;  Loss pred: 0.8498; Loss self: 0.0000; time: 0.20s
Val loss: 0.5173 score: 0.9184 time: 0.06s
Test loss: 0.4634 score: 1.0000 time: 0.08s
Epoch 180/1000, LR 0.000251
Train loss: 0.8491;  Loss pred: 0.8491; Loss self: 0.0000; time: 0.28s
Val loss: 0.5145 score: 0.9184 time: 0.06s
Test loss: 0.4597 score: 1.0000 time: 0.07s
Epoch 181/1000, LR 0.000251
Train loss: 0.8461;  Loss pred: 0.8461; Loss self: 0.0000; time: 0.21s
Val loss: 0.5116 score: 0.9184 time: 0.06s
Test loss: 0.4560 score: 1.0000 time: 0.07s
Epoch 182/1000, LR 0.000251
Train loss: 0.8436;  Loss pred: 0.8436; Loss self: 0.0000; time: 0.21s
Val loss: 0.5087 score: 0.9184 time: 0.06s
Test loss: 0.4522 score: 1.0000 time: 0.07s
Epoch 183/1000, LR 0.000250
Train loss: 0.8395;  Loss pred: 0.8395; Loss self: 0.0000; time: 0.21s
Val loss: 0.5059 score: 0.9184 time: 0.06s
Test loss: 0.4485 score: 1.0000 time: 0.07s
Epoch 184/1000, LR 0.000250
Train loss: 0.8366;  Loss pred: 0.8366; Loss self: 0.0000; time: 0.20s
Val loss: 0.5031 score: 0.9184 time: 0.07s
Test loss: 0.4448 score: 1.0000 time: 0.07s
Epoch 185/1000, LR 0.000250
Train loss: 0.8327;  Loss pred: 0.8327; Loss self: 0.0000; time: 0.21s
Val loss: 0.5003 score: 0.9184 time: 0.06s
Test loss: 0.4410 score: 1.0000 time: 0.07s
Epoch 186/1000, LR 0.000250
Train loss: 0.8284;  Loss pred: 0.8284; Loss self: 0.0000; time: 0.21s
Val loss: 0.4975 score: 0.9184 time: 0.06s
Test loss: 0.4373 score: 1.0000 time: 0.07s
Epoch 187/1000, LR 0.000249
Train loss: 0.8266;  Loss pred: 0.8266; Loss self: 0.0000; time: 0.21s
Val loss: 0.4947 score: 0.9184 time: 0.06s
Test loss: 0.4335 score: 1.0000 time: 0.07s
Epoch 188/1000, LR 0.000249
Train loss: 0.8242;  Loss pred: 0.8242; Loss self: 0.0000; time: 0.21s
Val loss: 0.4919 score: 0.9184 time: 0.06s
Test loss: 0.4298 score: 1.0000 time: 0.07s
Epoch 189/1000, LR 0.000249
Train loss: 0.8209;  Loss pred: 0.8209; Loss self: 0.0000; time: 0.21s
Val loss: 0.4891 score: 0.9184 time: 0.58s
Test loss: 0.4261 score: 1.0000 time: 0.94s
Epoch 190/1000, LR 0.000249
Train loss: 0.8182;  Loss pred: 0.8182; Loss self: 0.0000; time: 2.53s
Val loss: 0.4863 score: 0.9184 time: 2.22s
Test loss: 0.4223 score: 1.0000 time: 1.95s
Epoch 191/1000, LR 0.000249
Train loss: 0.8155;  Loss pred: 0.8155; Loss self: 0.0000; time: 2.59s
Val loss: 0.4836 score: 0.9184 time: 0.09s
Test loss: 0.4186 score: 1.0000 time: 0.09s
Epoch 192/1000, LR 0.000248
Train loss: 0.8150;  Loss pred: 0.8150; Loss self: 0.0000; time: 0.22s
Val loss: 0.4809 score: 0.9184 time: 0.06s
Test loss: 0.4148 score: 1.0000 time: 0.07s
Epoch 193/1000, LR 0.000248
Train loss: 0.8114;  Loss pred: 0.8114; Loss self: 0.0000; time: 0.19s
Val loss: 0.4782 score: 0.9184 time: 0.06s
Test loss: 0.4111 score: 1.0000 time: 0.08s
Epoch 194/1000, LR 0.000248
Train loss: 0.8075;  Loss pred: 0.8075; Loss self: 0.0000; time: 0.21s
Val loss: 0.4755 score: 0.9184 time: 0.07s
Test loss: 0.4074 score: 1.0000 time: 0.08s
Epoch 195/1000, LR 0.000248
Train loss: 0.8046;  Loss pred: 0.8046; Loss self: 0.0000; time: 0.21s
Val loss: 0.4728 score: 0.9184 time: 0.06s
Test loss: 0.4037 score: 1.0000 time: 0.07s
Epoch 196/1000, LR 0.000247
Train loss: 0.7997;  Loss pred: 0.7997; Loss self: 0.0000; time: 0.20s
Val loss: 0.4700 score: 0.9184 time: 0.06s
Test loss: 0.4000 score: 1.0000 time: 0.07s
Epoch 197/1000, LR 0.000247
Train loss: 0.7973;  Loss pred: 0.7973; Loss self: 0.0000; time: 0.22s
Val loss: 0.4673 score: 0.9184 time: 0.06s
Test loss: 0.3963 score: 1.0000 time: 0.07s
Epoch 198/1000, LR 0.000247
Train loss: 0.7955;  Loss pred: 0.7955; Loss self: 0.0000; time: 0.20s
Val loss: 0.4646 score: 0.9184 time: 0.06s
Test loss: 0.3926 score: 1.0000 time: 0.07s
Epoch 199/1000, LR 0.000247
Train loss: 0.7910;  Loss pred: 0.7910; Loss self: 0.0000; time: 0.20s
Val loss: 0.4618 score: 0.9184 time: 0.06s
Test loss: 0.3889 score: 1.0000 time: 0.07s
Epoch 200/1000, LR 0.000246
Train loss: 0.7880;  Loss pred: 0.7880; Loss self: 0.0000; time: 0.20s
Val loss: 0.4590 score: 0.9184 time: 0.06s
Test loss: 0.3853 score: 1.0000 time: 0.07s
Epoch 201/1000, LR 0.000246
Train loss: 0.7864;  Loss pred: 0.7864; Loss self: 0.0000; time: 0.20s
Val loss: 0.4563 score: 0.9184 time: 0.06s
Test loss: 0.3816 score: 1.0000 time: 0.07s
Epoch 202/1000, LR 0.000246
Train loss: 0.7845;  Loss pred: 0.7845; Loss self: 0.0000; time: 0.23s
Val loss: 0.4537 score: 0.9184 time: 0.07s
Test loss: 0.3779 score: 1.0000 time: 0.08s
Epoch 203/1000, LR 0.000246
Train loss: 0.7813;  Loss pred: 0.7813; Loss self: 0.0000; time: 0.21s
Val loss: 0.4511 score: 0.9184 time: 0.06s
Test loss: 0.3743 score: 1.0000 time: 0.07s
Epoch 204/1000, LR 0.000245
Train loss: 0.7791;  Loss pred: 0.7791; Loss self: 0.0000; time: 0.21s
Val loss: 0.4485 score: 0.9184 time: 0.06s
Test loss: 0.3706 score: 1.0000 time: 0.08s
Epoch 205/1000, LR 0.000245
Train loss: 0.7757;  Loss pred: 0.7757; Loss self: 0.0000; time: 0.20s
Val loss: 0.4459 score: 0.9184 time: 0.06s
Test loss: 0.3670 score: 1.0000 time: 0.07s
Epoch 206/1000, LR 0.000245
Train loss: 0.7723;  Loss pred: 0.7723; Loss self: 0.0000; time: 4.25s
Val loss: 0.4435 score: 0.9184 time: 0.32s
Test loss: 0.3634 score: 1.0000 time: 0.34s
Epoch 207/1000, LR 0.000245
Train loss: 0.7703;  Loss pred: 0.7703; Loss self: 0.0000; time: 0.98s
Val loss: 0.4410 score: 0.9184 time: 0.42s
Test loss: 0.3598 score: 1.0000 time: 0.30s
Epoch 208/1000, LR 0.000244
Train loss: 0.7682;  Loss pred: 0.7682; Loss self: 0.0000; time: 0.40s
Val loss: 0.4385 score: 0.9184 time: 0.52s
Test loss: 0.3563 score: 1.0000 time: 0.43s
Epoch 209/1000, LR 0.000244
Train loss: 0.7648;  Loss pred: 0.7648; Loss self: 0.0000; time: 0.31s
Val loss: 0.4360 score: 0.9184 time: 0.07s
Test loss: 0.3527 score: 1.0000 time: 0.07s
Epoch 210/1000, LR 0.000244
Train loss: 0.7603;  Loss pred: 0.7603; Loss self: 0.0000; time: 0.21s
Val loss: 0.4335 score: 0.9184 time: 0.07s
Test loss: 0.3491 score: 1.0000 time: 0.07s
Epoch 211/1000, LR 0.000244
Train loss: 0.7589;  Loss pred: 0.7589; Loss self: 0.0000; time: 0.21s
Val loss: 0.4310 score: 0.9184 time: 0.07s
Test loss: 0.3456 score: 1.0000 time: 0.07s
Epoch 212/1000, LR 0.000243
Train loss: 0.7581;  Loss pred: 0.7581; Loss self: 0.0000; time: 0.22s
Val loss: 0.4285 score: 0.9184 time: 0.06s
Test loss: 0.3421 score: 1.0000 time: 0.07s
Epoch 213/1000, LR 0.000243
Train loss: 0.7525;  Loss pred: 0.7525; Loss self: 0.0000; time: 0.21s
Val loss: 0.4260 score: 0.9184 time: 0.06s
Test loss: 0.3386 score: 1.0000 time: 0.07s
Epoch 214/1000, LR 0.000243
Train loss: 0.7504;  Loss pred: 0.7504; Loss self: 0.0000; time: 0.21s
Val loss: 0.4236 score: 0.9184 time: 0.07s
Test loss: 0.3352 score: 1.0000 time: 0.07s
Epoch 215/1000, LR 0.000243
Train loss: 0.7482;  Loss pred: 0.7482; Loss self: 0.0000; time: 0.20s
Val loss: 0.4212 score: 0.9184 time: 0.06s
Test loss: 0.3317 score: 1.0000 time: 0.07s
Epoch 216/1000, LR 0.000242
Train loss: 0.7461;  Loss pred: 0.7461; Loss self: 0.0000; time: 0.21s
Val loss: 0.4188 score: 0.9184 time: 0.06s
Test loss: 0.3283 score: 1.0000 time: 0.07s
Epoch 217/1000, LR 0.000242
Train loss: 0.7429;  Loss pred: 0.7429; Loss self: 0.0000; time: 0.21s
Val loss: 0.4165 score: 0.9184 time: 0.06s
Test loss: 0.3249 score: 1.0000 time: 0.07s
Epoch 218/1000, LR 0.000242
Train loss: 0.7420;  Loss pred: 0.7420; Loss self: 0.0000; time: 0.20s
Val loss: 0.4142 score: 0.9184 time: 0.06s
Test loss: 0.3215 score: 1.0000 time: 0.07s
Epoch 219/1000, LR 0.000242
Train loss: 0.7382;  Loss pred: 0.7382; Loss self: 0.0000; time: 0.20s
Val loss: 0.4119 score: 0.9184 time: 0.06s
Test loss: 0.3182 score: 1.0000 time: 0.07s
Epoch 220/1000, LR 0.000241
Train loss: 0.7342;  Loss pred: 0.7342; Loss self: 0.0000; time: 0.21s
Val loss: 0.4096 score: 0.9184 time: 0.06s
Test loss: 0.3149 score: 1.0000 time: 0.07s
Epoch 221/1000, LR 0.000241
Train loss: 0.7348;  Loss pred: 0.7348; Loss self: 0.0000; time: 0.20s
Val loss: 0.4074 score: 0.9184 time: 0.06s
Test loss: 0.3116 score: 1.0000 time: 0.07s
Epoch 222/1000, LR 0.000241
Train loss: 0.7321;  Loss pred: 0.7321; Loss self: 0.0000; time: 0.20s
Val loss: 0.4053 score: 0.9184 time: 0.06s
Test loss: 0.3084 score: 1.0000 time: 0.07s
Epoch 223/1000, LR 0.000241
Train loss: 0.7271;  Loss pred: 0.7271; Loss self: 0.0000; time: 0.20s
Val loss: 0.4031 score: 0.9184 time: 0.06s
Test loss: 0.3051 score: 1.0000 time: 0.07s
Epoch 224/1000, LR 0.000240
Train loss: 0.7243;  Loss pred: 0.7243; Loss self: 0.0000; time: 0.21s
Val loss: 0.4009 score: 0.9184 time: 0.07s
Test loss: 0.3019 score: 1.0000 time: 0.07s
Epoch 225/1000, LR 0.000240
Train loss: 0.7240;  Loss pred: 0.7240; Loss self: 0.0000; time: 0.21s
Val loss: 0.3987 score: 0.9184 time: 0.06s
Test loss: 0.2988 score: 1.0000 time: 0.07s
Epoch 226/1000, LR 0.000240
Train loss: 0.7217;  Loss pred: 0.7217; Loss self: 0.0000; time: 0.21s
Val loss: 0.3965 score: 0.9184 time: 0.06s
Test loss: 0.2957 score: 1.0000 time: 0.07s
Epoch 227/1000, LR 0.000240
Train loss: 0.7190;  Loss pred: 0.7190; Loss self: 0.0000; time: 0.20s
Val loss: 0.3943 score: 0.9184 time: 0.06s
Test loss: 0.2926 score: 1.0000 time: 0.07s
Epoch 228/1000, LR 0.000239
Train loss: 0.7159;  Loss pred: 0.7159; Loss self: 0.0000; time: 2.60s
Val loss: 0.3922 score: 0.9184 time: 1.89s
Test loss: 0.2895 score: 1.0000 time: 0.33s
Epoch 229/1000, LR 0.000239
Train loss: 0.7135;  Loss pred: 0.7135; Loss self: 0.0000; time: 0.29s
Val loss: 0.3901 score: 0.9184 time: 0.07s
Test loss: 0.2865 score: 1.0000 time: 0.07s
Epoch 230/1000, LR 0.000239
Train loss: 0.7107;  Loss pred: 0.7107; Loss self: 0.0000; time: 0.20s
Val loss: 0.3882 score: 0.9184 time: 0.06s
Test loss: 0.2835 score: 1.0000 time: 0.07s
Epoch 231/1000, LR 0.000238
Train loss: 0.7104;  Loss pred: 0.7104; Loss self: 0.0000; time: 0.21s
Val loss: 0.3862 score: 0.9184 time: 0.06s
Test loss: 0.2805 score: 1.0000 time: 0.07s
Epoch 232/1000, LR 0.000238
Train loss: 0.7086;  Loss pred: 0.7086; Loss self: 0.0000; time: 0.20s
Val loss: 0.3843 score: 0.9184 time: 0.06s
Test loss: 0.2776 score: 1.0000 time: 0.07s
Epoch 233/1000, LR 0.000238
Train loss: 0.7056;  Loss pred: 0.7056; Loss self: 0.0000; time: 0.20s
Val loss: 0.3825 score: 0.9184 time: 0.06s
Test loss: 0.2747 score: 1.0000 time: 0.07s
Epoch 234/1000, LR 0.000238
Train loss: 0.7023;  Loss pred: 0.7023; Loss self: 0.0000; time: 0.21s
Val loss: 0.3806 score: 0.9184 time: 0.06s
Test loss: 0.2718 score: 1.0000 time: 0.07s
Epoch 235/1000, LR 0.000237
Train loss: 0.7009;  Loss pred: 0.7009; Loss self: 0.0000; time: 0.20s
Val loss: 0.3787 score: 0.9184 time: 0.06s
Test loss: 0.2690 score: 1.0000 time: 0.07s
Epoch 236/1000, LR 0.000237
Train loss: 0.6989;  Loss pred: 0.6989; Loss self: 0.0000; time: 0.20s
Val loss: 0.3770 score: 0.9184 time: 0.06s
Test loss: 0.2662 score: 1.0000 time: 0.07s
Epoch 237/1000, LR 0.000237
Train loss: 0.6962;  Loss pred: 0.6962; Loss self: 0.0000; time: 0.20s
Val loss: 0.3752 score: 0.9184 time: 0.06s
Test loss: 0.2634 score: 1.0000 time: 0.07s
Epoch 238/1000, LR 0.000236
Train loss: 0.6947;  Loss pred: 0.6947; Loss self: 0.0000; time: 0.21s
Val loss: 0.3735 score: 0.9184 time: 0.06s
Test loss: 0.2607 score: 1.0000 time: 0.07s
Epoch 239/1000, LR 0.000236
Train loss: 0.6927;  Loss pred: 0.6927; Loss self: 0.0000; time: 0.20s
Val loss: 0.3719 score: 0.9184 time: 0.06s
Test loss: 0.2580 score: 1.0000 time: 0.07s
Epoch 240/1000, LR 0.000236
Train loss: 0.6899;  Loss pred: 0.6899; Loss self: 0.0000; time: 0.20s
Val loss: 0.3703 score: 0.9184 time: 0.06s
Test loss: 0.2553 score: 1.0000 time: 0.07s
Epoch 241/1000, LR 0.000236
Train loss: 0.6880;  Loss pred: 0.6880; Loss self: 0.0000; time: 0.20s
Val loss: 0.3688 score: 0.9184 time: 0.06s
Test loss: 0.2526 score: 1.0000 time: 0.07s
Epoch 242/1000, LR 0.000235
Train loss: 0.6869;  Loss pred: 0.6869; Loss self: 0.0000; time: 0.20s
Val loss: 0.3673 score: 0.9184 time: 0.06s
Test loss: 0.2500 score: 1.0000 time: 0.07s
Epoch 243/1000, LR 0.000235
Train loss: 0.6853;  Loss pred: 0.6853; Loss self: 0.0000; time: 0.20s
Val loss: 0.3656 score: 0.9184 time: 0.06s
Test loss: 0.2474 score: 1.0000 time: 0.07s
Epoch 244/1000, LR 0.000235
Train loss: 0.6844;  Loss pred: 0.6844; Loss self: 0.0000; time: 0.20s
Val loss: 0.3640 score: 0.9184 time: 0.06s
Test loss: 0.2448 score: 1.0000 time: 0.07s
Epoch 245/1000, LR 0.000234
Train loss: 0.6807;  Loss pred: 0.6807; Loss self: 0.0000; time: 0.21s
Val loss: 0.3624 score: 0.9184 time: 0.06s
Test loss: 0.2422 score: 1.0000 time: 0.07s
Epoch 246/1000, LR 0.000234
Train loss: 0.6793;  Loss pred: 0.6793; Loss self: 0.0000; time: 0.20s
Val loss: 0.3609 score: 0.9184 time: 0.06s
Test loss: 0.2397 score: 1.0000 time: 0.07s
Epoch 247/1000, LR 0.000234
Train loss: 0.6799;  Loss pred: 0.6799; Loss self: 0.0000; time: 0.20s
Val loss: 0.3597 score: 0.9184 time: 0.06s
Test loss: 0.2372 score: 1.0000 time: 0.07s
Epoch 248/1000, LR 0.000234
Train loss: 0.6761;  Loss pred: 0.6761; Loss self: 0.0000; time: 0.20s
Val loss: 0.3585 score: 0.9184 time: 0.06s
Test loss: 0.2348 score: 1.0000 time: 0.07s
Epoch 249/1000, LR 0.000233
Train loss: 0.6747;  Loss pred: 0.6747; Loss self: 0.0000; time: 0.21s
Val loss: 0.3574 score: 0.9184 time: 0.06s
Test loss: 0.2324 score: 1.0000 time: 0.07s
Epoch 250/1000, LR 0.000233
Train loss: 0.6726;  Loss pred: 0.6726; Loss self: 0.0000; time: 0.20s
Val loss: 0.3562 score: 0.9184 time: 0.06s
Test loss: 0.2301 score: 1.0000 time: 0.07s
Epoch 251/1000, LR 0.000233
Train loss: 0.6710;  Loss pred: 0.6710; Loss self: 0.0000; time: 0.21s
Val loss: 0.3548 score: 0.9184 time: 0.06s
Test loss: 0.2277 score: 1.0000 time: 0.07s
Epoch 252/1000, LR 0.000232
Train loss: 0.6694;  Loss pred: 0.6694; Loss self: 0.0000; time: 0.20s
Val loss: 0.3534 score: 0.9184 time: 0.06s
Test loss: 0.2254 score: 1.0000 time: 0.07s
Epoch 253/1000, LR 0.000232
Train loss: 0.6673;  Loss pred: 0.6673; Loss self: 0.0000; time: 0.20s
Val loss: 0.3520 score: 0.9184 time: 0.06s
Test loss: 0.2230 score: 1.0000 time: 0.07s
Epoch 254/1000, LR 0.000232
Train loss: 0.6659;  Loss pred: 0.6659; Loss self: 0.0000; time: 0.20s
Val loss: 0.3505 score: 0.9184 time: 0.06s
Test loss: 0.2207 score: 1.0000 time: 0.07s
Epoch 255/1000, LR 0.000232
Train loss: 0.6640;  Loss pred: 0.6640; Loss self: 0.0000; time: 0.20s
Val loss: 0.3490 score: 0.9184 time: 0.06s
Test loss: 0.2185 score: 1.0000 time: 0.07s
Epoch 256/1000, LR 0.000231
Train loss: 0.6630;  Loss pred: 0.6630; Loss self: 0.0000; time: 0.21s
Val loss: 0.3476 score: 0.9184 time: 0.06s
Test loss: 0.2162 score: 1.0000 time: 0.07s
Epoch 257/1000, LR 0.000231
Train loss: 0.6598;  Loss pred: 0.6598; Loss self: 0.0000; time: 0.20s
Val loss: 0.3462 score: 0.9184 time: 0.06s
Test loss: 0.2140 score: 1.0000 time: 0.07s
Epoch 258/1000, LR 0.000231
Train loss: 0.6624;  Loss pred: 0.6624; Loss self: 0.0000; time: 0.20s
Val loss: 0.3449 score: 0.9184 time: 0.06s
Test loss: 0.2119 score: 1.0000 time: 0.07s
Epoch 259/1000, LR 0.000230
Train loss: 0.6583;  Loss pred: 0.6583; Loss self: 0.0000; time: 0.21s
Val loss: 0.3437 score: 0.9184 time: 0.06s
Test loss: 0.2098 score: 1.0000 time: 0.07s
Epoch 260/1000, LR 0.000230
Train loss: 0.6569;  Loss pred: 0.6569; Loss self: 0.0000; time: 0.22s
Val loss: 0.3425 score: 0.9184 time: 0.06s
Test loss: 0.2077 score: 1.0000 time: 0.07s
Epoch 261/1000, LR 0.000230
Train loss: 0.6551;  Loss pred: 0.6551; Loss self: 0.0000; time: 0.21s
Val loss: 0.3413 score: 0.9184 time: 0.06s
Test loss: 0.2056 score: 1.0000 time: 0.07s
Epoch 262/1000, LR 0.000229
Train loss: 0.6522;  Loss pred: 0.6522; Loss self: 0.0000; time: 0.21s
Val loss: 0.3402 score: 0.9184 time: 0.06s
Test loss: 0.2036 score: 1.0000 time: 0.07s
Epoch 263/1000, LR 0.000229
Train loss: 0.6529;  Loss pred: 0.6529; Loss self: 0.0000; time: 0.20s
Val loss: 0.3392 score: 0.9184 time: 0.07s
Test loss: 0.2016 score: 1.0000 time: 0.07s
Epoch 264/1000, LR 0.000229
Train loss: 0.6509;  Loss pred: 0.6509; Loss self: 0.0000; time: 0.21s
Val loss: 0.3382 score: 0.9184 time: 0.07s
Test loss: 0.1997 score: 1.0000 time: 0.07s
Epoch 265/1000, LR 0.000228
Train loss: 0.6473;  Loss pred: 0.6473; Loss self: 0.0000; time: 0.21s
Val loss: 0.3374 score: 0.9184 time: 0.07s
Test loss: 0.1978 score: 1.0000 time: 0.07s
Epoch 266/1000, LR 0.000228
Train loss: 0.6480;  Loss pred: 0.6480; Loss self: 0.0000; time: 0.22s
Val loss: 0.3365 score: 0.9184 time: 0.07s
Test loss: 0.1959 score: 1.0000 time: 0.08s
Epoch 267/1000, LR 0.000228
Train loss: 0.6458;  Loss pred: 0.6458; Loss self: 0.0000; time: 0.21s
Val loss: 0.3357 score: 0.9184 time: 0.07s
Test loss: 0.1940 score: 1.0000 time: 0.07s
Epoch 268/1000, LR 0.000228
Train loss: 0.6448;  Loss pred: 0.6448; Loss self: 0.0000; time: 0.21s
Val loss: 0.3349 score: 0.9184 time: 0.07s
Test loss: 0.1922 score: 1.0000 time: 0.07s
Epoch 269/1000, LR 0.000227
Train loss: 0.6420;  Loss pred: 0.6420; Loss self: 0.0000; time: 0.21s
Val loss: 0.3340 score: 0.9184 time: 0.07s
Test loss: 0.1904 score: 1.0000 time: 0.07s
Epoch 270/1000, LR 0.000227
Train loss: 0.6420;  Loss pred: 0.6420; Loss self: 0.0000; time: 0.23s
Val loss: 0.3329 score: 0.9184 time: 0.06s
Test loss: 0.1886 score: 1.0000 time: 0.07s
Epoch 271/1000, LR 0.000227
Train loss: 0.6403;  Loss pred: 0.6403; Loss self: 0.0000; time: 2.34s
Val loss: 0.3320 score: 0.9184 time: 1.62s
Test loss: 0.1868 score: 1.0000 time: 0.73s
Epoch 272/1000, LR 0.000226
Train loss: 0.6380;  Loss pred: 0.6380; Loss self: 0.0000; time: 0.74s
Val loss: 0.3310 score: 0.9184 time: 0.49s
Test loss: 0.1850 score: 1.0000 time: 0.27s
Epoch 273/1000, LR 0.000226
Train loss: 0.6390;  Loss pred: 0.6390; Loss self: 0.0000; time: 0.22s
Val loss: 0.3303 score: 0.9184 time: 0.07s
Test loss: 0.1833 score: 1.0000 time: 0.07s
Epoch 274/1000, LR 0.000226
Train loss: 0.6361;  Loss pred: 0.6361; Loss self: 0.0000; time: 0.23s
Val loss: 0.3296 score: 0.9184 time: 0.10s
Test loss: 0.1817 score: 1.0000 time: 0.08s
Epoch 275/1000, LR 0.000225
Train loss: 0.6354;  Loss pred: 0.6354; Loss self: 0.0000; time: 0.22s
Val loss: 0.3288 score: 0.9184 time: 0.07s
Test loss: 0.1800 score: 1.0000 time: 0.07s
Epoch 276/1000, LR 0.000225
Train loss: 0.6342;  Loss pred: 0.6342; Loss self: 0.0000; time: 0.22s
Val loss: 0.3281 score: 0.9184 time: 0.07s
Test loss: 0.1784 score: 1.0000 time: 0.08s
Epoch 277/1000, LR 0.000225
Train loss: 0.6340;  Loss pred: 0.6340; Loss self: 0.0000; time: 0.22s
Val loss: 0.3275 score: 0.9184 time: 0.07s
Test loss: 0.1768 score: 1.0000 time: 0.07s
Epoch 278/1000, LR 0.000224
Train loss: 0.6306;  Loss pred: 0.6306; Loss self: 0.0000; time: 0.22s
Val loss: 0.3268 score: 0.9184 time: 0.07s
Test loss: 0.1752 score: 1.0000 time: 0.07s
Epoch 279/1000, LR 0.000224
Train loss: 0.6302;  Loss pred: 0.6302; Loss self: 0.0000; time: 0.23s
Val loss: 0.3260 score: 0.9184 time: 0.06s
Test loss: 0.1736 score: 1.0000 time: 0.07s
Epoch 280/1000, LR 0.000224
Train loss: 0.6306;  Loss pred: 0.6306; Loss self: 0.0000; time: 0.21s
Val loss: 0.3252 score: 0.9184 time: 0.07s
Test loss: 0.1721 score: 1.0000 time: 0.07s
Epoch 281/1000, LR 0.000223
Train loss: 0.6281;  Loss pred: 0.6281; Loss self: 0.0000; time: 0.21s
Val loss: 0.3243 score: 0.9184 time: 0.07s
Test loss: 0.1705 score: 1.0000 time: 0.08s
Epoch 282/1000, LR 0.000223
Train loss: 0.6271;  Loss pred: 0.6271; Loss self: 0.0000; time: 0.21s
Val loss: 0.3235 score: 0.9184 time: 0.07s
Test loss: 0.1690 score: 1.0000 time: 0.08s
Epoch 283/1000, LR 0.000223
Train loss: 0.6251;  Loss pred: 0.6251; Loss self: 0.0000; time: 0.21s
Val loss: 0.3228 score: 0.9184 time: 0.07s
Test loss: 0.1675 score: 1.0000 time: 0.08s
Epoch 284/1000, LR 0.000222
Train loss: 0.6257;  Loss pred: 0.6257; Loss self: 0.0000; time: 0.22s
Val loss: 0.3223 score: 0.9184 time: 0.07s
Test loss: 0.1661 score: 1.0000 time: 0.08s
Epoch 285/1000, LR 0.000222
Train loss: 0.6225;  Loss pred: 0.6225; Loss self: 0.0000; time: 0.23s
Val loss: 0.3218 score: 0.9184 time: 2.35s
Test loss: 0.1647 score: 1.0000 time: 2.10s
Epoch 286/1000, LR 0.000222
Train loss: 0.6238;  Loss pred: 0.6238; Loss self: 0.0000; time: 3.68s
Val loss: 0.3212 score: 0.9184 time: 0.49s
Test loss: 0.1633 score: 1.0000 time: 0.12s
Epoch 287/1000, LR 0.000221
Train loss: 0.6226;  Loss pred: 0.6226; Loss self: 0.0000; time: 0.22s
Val loss: 0.3208 score: 0.9184 time: 0.07s
Test loss: 0.1619 score: 1.0000 time: 0.08s
Epoch 288/1000, LR 0.000221
Train loss: 0.6217;  Loss pred: 0.6217; Loss self: 0.0000; time: 0.22s
Val loss: 0.3205 score: 0.9184 time: 0.07s
Test loss: 0.1606 score: 1.0000 time: 0.08s
Epoch 289/1000, LR 0.000221
Train loss: 0.6206;  Loss pred: 0.6206; Loss self: 0.0000; time: 0.22s
Val loss: 0.3201 score: 0.9184 time: 0.07s
Test loss: 0.1593 score: 1.0000 time: 0.07s
Epoch 290/1000, LR 0.000220
Train loss: 0.6191;  Loss pred: 0.6191; Loss self: 0.0000; time: 0.22s
Val loss: 0.3197 score: 0.9184 time: 0.06s
Test loss: 0.1580 score: 1.0000 time: 0.08s
Epoch 291/1000, LR 0.000220
Train loss: 0.6188;  Loss pred: 0.6188; Loss self: 0.0000; time: 0.21s
Val loss: 0.3192 score: 0.9184 time: 0.06s
Test loss: 0.1567 score: 1.0000 time: 0.07s
Epoch 292/1000, LR 0.000220
Train loss: 0.6181;  Loss pred: 0.6181; Loss self: 0.0000; time: 0.20s
Val loss: 0.3186 score: 0.9184 time: 0.06s
Test loss: 0.1553 score: 1.0000 time: 0.07s
Epoch 293/1000, LR 0.000219
Train loss: 0.6162;  Loss pred: 0.6162; Loss self: 0.0000; time: 0.20s
Val loss: 0.3181 score: 0.9184 time: 0.06s
Test loss: 0.1541 score: 1.0000 time: 0.07s
Epoch 294/1000, LR 0.000219
Train loss: 0.6139;  Loss pred: 0.6139; Loss self: 0.0000; time: 0.21s
Val loss: 0.3177 score: 0.9184 time: 0.06s
Test loss: 0.1528 score: 1.0000 time: 0.07s
Epoch 295/1000, LR 0.000219
Train loss: 0.6135;  Loss pred: 0.6135; Loss self: 0.0000; time: 0.21s
Val loss: 0.3171 score: 0.9184 time: 0.06s
Test loss: 0.1515 score: 1.0000 time: 0.07s
Epoch 296/1000, LR 0.000218
Train loss: 0.6125;  Loss pred: 0.6125; Loss self: 0.0000; time: 0.21s
Val loss: 0.3165 score: 0.9184 time: 0.06s
Test loss: 0.1503 score: 1.0000 time: 0.07s
Epoch 297/1000, LR 0.000218
Train loss: 0.6118;  Loss pred: 0.6118; Loss self: 0.0000; time: 0.20s
Val loss: 0.3160 score: 0.9184 time: 0.06s
Test loss: 0.1490 score: 1.0000 time: 0.07s
Epoch 298/1000, LR 0.000218
Train loss: 0.6104;  Loss pred: 0.6104; Loss self: 0.0000; time: 0.20s
Val loss: 0.3155 score: 0.9184 time: 0.07s
Test loss: 0.1478 score: 1.0000 time: 0.07s
Epoch 299/1000, LR 0.000217
Train loss: 0.6096;  Loss pred: 0.6096; Loss self: 0.0000; time: 0.22s
Val loss: 0.3151 score: 0.9184 time: 0.07s
Test loss: 0.1466 score: 1.0000 time: 0.07s
Epoch 300/1000, LR 0.000217
Train loss: 0.6092;  Loss pred: 0.6092; Loss self: 0.0000; time: 0.23s
Val loss: 0.3146 score: 0.9184 time: 0.07s
Test loss: 0.1455 score: 1.0000 time: 0.07s
Epoch 301/1000, LR 0.000217
Train loss: 0.6089;  Loss pred: 0.6089; Loss self: 0.0000; time: 0.24s
Val loss: 0.3143 score: 0.9184 time: 0.06s
Test loss: 0.1443 score: 1.0000 time: 0.08s
Epoch 302/1000, LR 0.000216
Train loss: 0.6081;  Loss pred: 0.6081; Loss self: 0.0000; time: 0.21s
Val loss: 0.3140 score: 0.9184 time: 0.07s
Test loss: 0.1432 score: 1.0000 time: 0.08s
Epoch 303/1000, LR 0.000216
Train loss: 0.6071;  Loss pred: 0.6071; Loss self: 0.0000; time: 0.22s
Val loss: 0.3138 score: 0.9184 time: 0.07s
Test loss: 0.1421 score: 1.0000 time: 0.08s
Epoch 304/1000, LR 0.000216
Train loss: 0.6051;  Loss pred: 0.6051; Loss self: 0.0000; time: 0.22s
Val loss: 0.3134 score: 0.9184 time: 0.07s
Test loss: 0.1410 score: 1.0000 time: 0.08s
Epoch 305/1000, LR 0.000215
Train loss: 0.6042;  Loss pred: 0.6042; Loss self: 0.0000; time: 0.21s
Val loss: 0.3132 score: 0.9184 time: 0.06s
Test loss: 0.1400 score: 1.0000 time: 0.07s
Epoch 306/1000, LR 0.000215
Train loss: 0.6052;  Loss pred: 0.6052; Loss self: 0.0000; time: 0.20s
Val loss: 0.3130 score: 0.9184 time: 0.07s
Test loss: 0.1389 score: 1.0000 time: 0.07s
Epoch 307/1000, LR 0.000215
Train loss: 0.6028;  Loss pred: 0.6028; Loss self: 0.0000; time: 0.20s
Val loss: 0.3127 score: 0.9184 time: 0.07s
Test loss: 0.1379 score: 1.0000 time: 0.07s
Epoch 308/1000, LR 0.000214
Train loss: 0.6025;  Loss pred: 0.6025; Loss self: 0.0000; time: 0.21s
Val loss: 0.3124 score: 0.9184 time: 0.06s
Test loss: 0.1368 score: 1.0000 time: 0.07s
Epoch 309/1000, LR 0.000214
Train loss: 0.6025;  Loss pred: 0.6025; Loss self: 0.0000; time: 0.21s
Val loss: 0.3120 score: 0.9184 time: 0.06s
Test loss: 0.1358 score: 1.0000 time: 0.07s
Epoch 310/1000, LR 0.000214
Train loss: 0.6015;  Loss pred: 0.6015; Loss self: 0.0000; time: 0.22s
Val loss: 0.3119 score: 0.9184 time: 0.06s
Test loss: 0.1348 score: 1.0000 time: 0.07s
Epoch 311/1000, LR 0.000213
Train loss: 0.6008;  Loss pred: 0.6008; Loss self: 0.0000; time: 0.20s
Val loss: 0.3116 score: 0.9184 time: 0.07s
Test loss: 0.1338 score: 1.0000 time: 0.07s
Epoch 312/1000, LR 0.000213
Train loss: 0.6010;  Loss pred: 0.6010; Loss self: 0.0000; time: 0.20s
Val loss: 0.3114 score: 0.9184 time: 0.07s
Test loss: 0.1329 score: 1.0000 time: 0.07s
Epoch 313/1000, LR 0.000213
Train loss: 0.5974;  Loss pred: 0.5974; Loss self: 0.0000; time: 0.21s
Val loss: 0.3112 score: 0.9184 time: 0.06s
Test loss: 0.1319 score: 1.0000 time: 0.07s
Epoch 314/1000, LR 0.000212
Train loss: 0.5980;  Loss pred: 0.5980; Loss self: 0.0000; time: 0.21s
Val loss: 0.3110 score: 0.9184 time: 0.06s
Test loss: 0.1309 score: 1.0000 time: 0.07s
Epoch 315/1000, LR 0.000212
Train loss: 0.5981;  Loss pred: 0.5981; Loss self: 0.0000; time: 0.21s
Val loss: 0.3109 score: 0.9184 time: 0.06s
Test loss: 0.1300 score: 1.0000 time: 0.07s
Epoch 316/1000, LR 0.000212
Train loss: 0.5953;  Loss pred: 0.5953; Loss self: 0.0000; time: 0.21s
Val loss: 0.3109 score: 0.9184 time: 0.06s
Test loss: 0.1291 score: 1.0000 time: 0.07s
Epoch 317/1000, LR 0.000211
Train loss: 0.5952;  Loss pred: 0.5952; Loss self: 0.0000; time: 0.21s
Val loss: 0.3109 score: 0.9184 time: 0.06s
Test loss: 0.1283 score: 1.0000 time: 0.07s
     INFO: Early stopping counter 1 of 2
Epoch 318/1000, LR 0.000211
Train loss: 0.5947;  Loss pred: 0.5947; Loss self: 0.0000; time: 0.20s
Val loss: 0.3109 score: 0.9184 time: 0.06s
Test loss: 0.1274 score: 1.0000 time: 0.07s
     INFO: Early stopping counter 2 of 2
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 315,   Train_Loss: 0.5953,   Val_Loss: 0.3109,   Val_Precision: 1.0000,   Val_Recall: 0.8400,   Val_accuracy: 0.9130,   Val_Score: 0.9184,   Val_Loss: 0.3109,   Test_Precision: 1.0000,   Test_Recall: 1.0000,   Test_accuracy: 1.0000,   Test_Score: 1.0000,   Test_loss: 0.1291


[1.1250675929477438, 0.07260138390120119, 0.06517546193208545, 0.06942169705871493, 0.06307711405679584, 0.06370937905739993, 0.06353496201336384, 0.06884501304011792, 0.0664323780220002, 0.06322127894964069, 0.06440213706810027, 0.0631538750603795, 0.06382796703837812, 0.06595662399195135, 0.0696884390199557, 0.06860149395652115, 0.0696936259046197, 0.07671907800249755, 0.06994366110302508, 0.08053676399867982, 1.5170276639983058, 0.3635215579997748, 0.19728804100304842, 0.09975753107573837, 0.06620932999067008, 0.06520154199097306, 0.06493534008041024, 0.06481671601068228, 0.06553994701243937, 0.06519350199960172, 0.06751241302117705, 0.06698870600666851, 0.07034255308099091, 0.06904265598859638, 0.7079291759291664, 0.29737150797154754, 0.29062273900490254, 0.10017506603617221, 0.06606452004052699, 0.06941800401546061, 0.06561793701257557, 0.06805465393699706, 0.06500946800224483, 0.06617417791858315, 0.06593629496637732, 0.9478009770391509, 0.12139186600688845, 0.07250222493894398, 0.06941007892601192, 0.0684408110100776, 0.06417788797989488, 0.06419697601813823, 0.06421515892725438, 0.06461732892785221, 0.06484952592290938, 0.06799174600746483, 0.1487254409585148, 0.06940725794993341, 0.06884129496756941, 2.1710067989770323, 0.074032180942595, 0.07430030300747603, 0.0638693260261789, 0.06624759593978524, 0.06461389001924545, 0.06443172099534422, 0.06254479999188334, 0.06303938594646752, 1.4150545729789883, 0.07174125104211271, 0.07600608200300485, 0.06253405101597309, 0.061866942909546196, 0.06246624398045242, 0.06197469704784453, 0.06184293807018548, 0.061756442999467254, 0.06228293105959892, 0.06297286006156355, 0.07575407798867673, 1.3042752699693665, 0.6072317759972066, 0.3420649969484657, 0.06774329300969839, 0.06546581897418946, 0.06285132595803589, 0.06252083706203848, 0.06223062006756663, 0.07395434298086911, 0.061848468030802906, 0.06118969200178981, 0.8000162750249729, 0.4628510350594297, 0.15736174397170544, 0.06282533193007112, 0.06248521606903523, 0.08036140399053693, 0.06135073699988425, 0.06133868300821632, 0.06053328700363636, 0.06225241406355053, 0.0741469879867509, 0.07334171095862985, 0.06444651598576456, 0.31430238694883883, 0.2978845089673996, 0.06520012801047415, 0.06237223604694009, 0.06152717105578631, 0.06212544091977179, 0.06123218103311956, 0.06145436700899154, 0.06125844200141728, 0.06195212004240602, 0.06192778202239424, 0.062011402915231884, 0.06177784607280046, 1.0857283340301365, 0.6311360759427771, 0.06474093592260033, 0.06514206901192665, 0.06190473202150315, 0.06076760496944189, 0.06565935001708567, 0.06444231909699738, 2.058738171006553, 0.08766032301355153, 0.062314005102962255, 0.06163706898223609, 0.06266388692893088, 0.062070864951238036, 0.06268804601859301, 0.06217115093022585, 0.06176924810279161, 0.43242676998488605, 0.8568806980038062, 0.06983314291574061, 0.0669098700163886, 0.0668419529683888, 0.06791843893006444, 0.0666529139271006, 0.06768140709027648, 0.0675741529557854, 0.6992912340210751, 0.9380899249808863, 0.30850372300483286, 0.06733186496421695, 0.06785435602068901, 0.07248535007238388, 0.06836368003860116, 0.06794471910689026, 0.07666253496427089, 0.06402595504187047, 0.06210166204255074, 0.6722918150480837, 0.47543843195308, 0.061514504021033645, 0.06054721400141716, 0.06924343691207469, 0.06474179995711893, 0.061351225012913346, 0.05998940800782293, 0.06007451505865902, 0.06028251093812287, 0.0598492439603433, 0.060017738956958055, 0.05985578789841384, 0.06021264009177685, 0.05970510595943779, 0.06027904199436307, 0.05982470000162721, 0.06543243792839348, 0.05934077105484903, 0.05994168506003916, 0.05984782602172345, 0.05864717997610569, 0.05976193107198924, 0.06012161704711616, 0.05980157200247049, 0.06084371905308217, 0.059946853085421026, 0.0603357549989596, 0.06042889796663076, 0.06412903207819909, 0.06010557699482888, 0.06000156106892973, 0.05960321601014584, 0.05996228288859129, 0.06015214091166854, 0.06013369397260249, 0.05975861893966794, 0.060398735920898616, 0.05954533303156495, 0.05992923700250685, 0.059819236979819834, 0.06108355394098908, 0.06309165491256863, 0.06721752695739269, 0.06566394097171724, 0.06768737197853625, 0.06523262697737664, 0.6982098269509152, 0.06177571404259652, 0.061719611985608935, 0.0629792510299012, 0.06152171909343451, 0.0623580280225724, 0.06233565404545516, 0.06348260689992458, 0.06242344295606017, 0.061633193981833756, 0.061667119967751205, 2.172633665963076, 0.4913909339811653, 0.23835222399793565, 0.06477198901120573, 0.06548859097529203, 0.06485605600755662, 0.06487756606657058, 0.06627889804076403, 0.06506447598803788, 0.06486578902695328, 0.06469597294926643, 0.06977198901586235, 0.06554349593352526, 0.49097754491958767, 0.2763960459269583, 0.06564669706858695, 0.06570510508026928, 0.06604870303999633, 0.06545242096763104, 0.06647082604467869, 0.06586214096751064, 0.06563952309079468, 0.06572110892739147, 0.0669107869034633, 0.06583865790162235, 0.06611254892777652, 0.06595169100910425, 0.07578426296822727, 0.20499895501416177, 0.3951002210378647, 0.13119681098032743, 0.06408304907381535, 0.06463857798371464, 0.06356241495814174, 0.06383267894852906, 0.06361543294042349, 0.0633556901011616, 0.06642631895374507, 0.061881675967015326, 0.06180676491931081, 0.062185003072954714, 0.06193888501729816, 0.06811047298833728, 0.06703020399436355, 0.06900497199967504, 0.37440110894385725, 0.18271234398707747, 0.07257488591130823, 0.07048596802633256, 0.06741675094235688, 0.062494186917319894, 0.07266014092601836, 0.06643604999408126, 0.06251480907667428, 0.5620149539317936, 0.0721541449893266, 0.0634066549828276, 0.06760266807395965, 0.06208055396564305, 0.06208280904684216, 0.0625737210502848, 0.7811409650603309, 0.5421252980595455, 0.1619326890213415, 0.06937139690853655, 0.0643412439385429, 0.06343211291823536, 0.0634324150159955, 0.06407666602171957, 0.06345105497166514, 0.06350390298757702, 0.06393551302608103, 0.06286177108995616, 0.06437789998017251, 0.0638196860672906, 0.06329401093535125, 0.06362919404637069, 0.06286765704862773, 0.06354229804128408, 0.06349020299967378, 0.06396328704431653, 0.06944924697745591, 0.7021669310051948, 0.15066916204523295, 0.06510162097401917, 0.06531176902353764, 0.07255030306987464, 0.06306795298587531, 0.06395858398173004, 0.06310978205874562, 0.06659926497377455, 0.06357999797910452, 0.06328763801138848, 0.062329409061931074, 0.0630039309617132, 0.06352573295589536, 0.06293555593583733, 0.06423496291972697, 0.06274548301007599, 0.06322590098716319, 0.06331986794248223, 0.0648027160204947, 0.06367957103066146, 0.06413593690376729, 0.07166974898427725, 0.9351862570038065, 0.5278814040357247, 0.2404966619797051, 0.14699518401175737, 0.0611959109082818, 0.060774209909141064, 0.06006977707147598, 0.059800112969242036, 0.05932479491457343, 0.061905408045277, 0.061308822012506425, 0.07259627396706492, 0.05976037296932191, 1.3797877499600872, 0.5188635989325121, 0.21851876704022288, 0.06615567696280777, 0.06384660303592682, 0.06116173800546676, 0.06827467796392739, 0.06987617397680879, 0.06473501399159431, 0.06559327605646104, 0.06530119699891657, 0.0651976199587807, 0.06506231299135834, 0.48478819394949824, 0.08137928193900734, 0.06303953798487782, 0.06348156393505633, 0.061951122013852, 0.06207906699273735, 0.06345623300876468, 0.06073413696140051, 0.060320374090224504, 0.07411619799677283, 0.059882447007112205, 0.061358314007520676, 0.06121002102736384, 0.06101328495424241, 0.05984595697373152, 0.8562058289535344, 0.36308432603254914, 0.3589624180458486, 0.5360252219252288, 0.27372678997926414, 0.06796180410310626, 0.0674912299728021, 0.0708560849307105, 0.06211181101389229, 0.06362960301339626, 0.06330638704821467, 0.06271806207951158, 0.07145854795817286, 0.7057888769777492, 0.7623719859402627, 0.10197966697160155, 0.06613733107224107, 0.06447841401677579, 0.06481045496184379, 0.0653388419887051, 0.06502651295159012, 0.06511953508015722, 0.06516978004947305, 0.06379691895563155, 0.06480425596237183, 0.06441011803690344, 0.06505688501056284, 0.06518991605844349, 0.06481833499856293, 0.06524338200688362, 0.06536815501749516, 0.5175710839685053, 0.17081168701406568, 0.09227630903478712, 0.06718872010242194, 0.06873557693324983, 0.06324115907773376, 0.060615563997998834, 0.06088417989667505, 0.06154175801202655, 0.06130311009474099, 0.7979504340328276, 0.18750561494380236, 0.06384482304565609, 0.06263048795517534, 0.06634805398061872, 0.062326967949047685, 0.14073967293370515, 0.0660147910239175, 2.0099160009995103, 0.0808089089114219, 0.06659966299775988, 0.06647378799971193, 0.07108314393553883, 0.06576379400212318, 0.06600105902180076, 0.06531793007161468, 0.06609735602978617, 0.06549849105067551, 0.06496734102256596, 0.06590594898443669, 0.06562471692450345, 1.0139631449710578, 0.433356367982924, 0.06455590797122568, 0.06520998408086598, 0.06416238297242671, 0.059874388040043414, 0.06007877201773226, 0.06335243303328753, 0.05923111201263964, 0.05906330293510109, 0.06179066596087068, 0.06517562200315297, 0.06110068899579346, 0.06357699900399894, 0.06334260804578662, 0.06338056398089975, 0.06613795296289027, 0.06298181891907007, 0.0622283429838717, 0.06173609895631671, 0.24811943992972374, 0.185759594081901, 0.10409536701627076, 0.06668517796788365, 0.06078002299182117, 0.06097498210147023, 0.06022069102618843, 0.06519732601009309, 0.06403364206198603, 0.06380565010476857, 0.06574902695138007, 0.06479679408948869, 0.064824333996512, 0.06435351702384651, 1.418682603049092, 0.29788276902399957, 0.06759201700333506, 0.0676653190748766, 0.06681511900387704, 0.06284347898326814, 0.06251368997618556, 0.06201293400954455, 0.06202096305787563, 0.0614965099375695, 0.06415769003797323, 0.061816305038519204, 0.06276234099641442, 0.06648268993012607, 0.06636232510209084, 1.3676926760235801, 0.20484785898588598, 0.19766586902551353, 0.19537242501974106, 0.2756413930328563, 0.15000747703015804, 0.06532892491668463, 0.06613829894922674, 0.06598337495233864, 0.061237829038873315, 0.07381295808590949, 0.06317257205955684, 0.06317669292911887, 2.7187610460678115, 0.43240723898634315, 0.19500290707219392, 0.11767496902029961, 0.06955885293427855, 0.06462281802669168, 0.06456879910547286, 0.0645724548958242, 0.06380702101159841, 0.0641079219058156, 0.06351136404555291, 0.06381946289911866, 0.06390934996306896, 0.151724140974693, 0.13299658906180412, 0.0722929600160569, 0.06101640698034316, 0.0691805180395022, 0.06612590607255697, 0.06057285703718662, 0.06066833401564509, 0.060895478003658354, 0.060905662016011775, 0.06040834600571543, 0.06116679997649044, 0.06011232908349484, 0.059701594058424234, 0.0598566880216822, 0.06166074797511101, 0.060140721034258604, 0.060052155051380396, 0.06230014795437455, 0.05920008500106633, 0.059808498015627265, 0.05946708202827722, 0.060787836904637516, 0.06079749995842576, 0.06194696598686278, 0.060422981972806156, 2.1693912680493668, 0.541298964060843, 0.10760771599598229, 0.0654468759894371, 0.0656769770430401, 0.06301131495274603, 0.06112062104512006, 0.06112696696072817, 0.061334936995990574, 0.06106480897869915, 0.06236384902149439, 0.18522180197760463, 0.07031690201256424, 0.06049346795771271, 0.06062412401661277, 0.06451582501176745, 0.06376780406571925, 0.0659716899972409, 0.06707607896532863, 0.07044851500540972, 0.06702344701625407, 0.06640851206611842, 0.06759932497516274, 0.0678577609360218, 0.06712761893868446, 0.28539070300757885, 0.07402405596803874, 0.06562827399466187, 0.06373321905266494, 0.06374375708401203, 0.06382676400244236, 0.06373048201203346, 0.06348897004500031, 0.06534413993358612, 0.06373709707986563, 0.06358121801167727, 0.06411356804892421, 0.06535728392191231, 0.06000717298593372, 0.06120710202958435, 0.060148693970404565, 0.06120062305126339, 0.06083882204256952, 2.1150936159538105, 0.368608062970452, 0.0948590210173279, 0.06047128001227975, 0.06064203893765807, 0.06907965906430036, 0.06617257604375482, 0.06085759797133505, 0.060929831000976264, 0.060708109056577086, 0.06068915606010705, 0.06709183193743229, 0.07865988602861762, 0.06549027608707547, 0.06592220708262175, 0.06652869191020727, 0.06679576099850237, 0.0658269259147346, 0.06665642594452947, 0.25492203002795577, 0.3399289969820529, 0.37802586797624826, 0.06340789201203734, 0.06104602897539735, 0.060528318979777396, 0.0605004410026595, 0.06157185591291636, 0.07089941401500255, 0.06068425299599767, 0.06139027199242264, 0.060750926961191, 0.06946049607358873, 0.06588819902390242, 0.06702332000713795, 0.06582847202662379, 0.06736704497598112, 0.06668515899218619, 0.06701808096840978, 0.06519622600171715, 0.06570287200156599, 0.06566261500120163, 0.06672699400223792, 0.06684203806798905, 0.066849619965069, 0.06642330100294203, 0.06644856301136315, 0.06727767398115247, 0.06696839106734842, 0.06913681700825691, 0.06599097605794668, 1.216842534020543, 0.23141142399981618, 0.06653583107981831, 0.06703355500940233, 0.06766534992493689, 0.06666724500246346, 0.0711883750045672, 0.06133677205070853, 0.9317941470071673, 0.18492432299535722, 0.07374197989702225, 0.06074041791725904, 0.061293284059502184, 0.06127759697847068, 0.06141822796780616, 0.061016611056402326, 0.0608588510658592, 0.0621505540329963, 0.06192007102072239, 0.06163601402658969, 0.060774766956456006, 0.09486502700019628, 0.07470900297630578, 0.06632820900995284, 0.06397118500899523, 0.06298397295176983, 0.8986464090412483, 0.07054201106075197, 0.06135188799817115, 0.06086431606672704, 0.06159884494263679, 0.060601128032431006, 0.0630223749903962, 0.06175465299747884, 1.0376595898997039, 1.1805398190626875, 0.19654598808847368, 0.06169412098824978, 0.06114864500705153, 0.06086521397810429, 0.0612690020352602, 0.061586212017573416, 0.0614949120208621, 0.06183173099998385, 0.06508620001841336, 0.06756043108180165, 0.06710878608282655, 0.0652913780650124, 0.06585067603737116, 0.06622012599837035, 0.06615473004058003, 0.19429152796510607, 1.572006318019703, 0.8984209460904822, 0.0703990840120241, 0.06987040501553565, 0.06868693500291556, 0.06140254100318998, 0.06037580000702292, 0.06282650399953127, 0.06465791095979512, 0.06457310903351754, 0.06530521798413247, 0.06547046301420778, 0.06417581008281559, 0.06619932199828327, 0.06465087702963501, 1.0072673639515415, 0.2331233989680186, 0.21133621397893876, 0.06545217102393508, 0.06545846699737012, 0.06475087604485452, 0.06491228600498289, 0.06490437395405024, 0.06169183901511133, 0.06336515198927373, 0.062162793008610606, 0.06724672100972384, 0.061802469892427325, 0.061814625980332494, 0.06129618105478585, 0.06274635391309857, 0.06070867402013391, 0.06289232207927853, 0.06225197296589613, 0.061824903008528054, 2.161183835938573, 0.06528045202139765, 0.07184788305312395, 0.06509210006333888, 0.06518628902267665, 0.06057037599384785, 0.06496623996645212, 0.06103533296845853, 0.059832219034433365, 0.05989590601529926, 1.370392601005733, 0.19392608909402043, 1.6879422939382493, 0.06515957496594638, 0.06419455690775067, 0.0808924090815708, 0.07738897099625319, 0.07284810999408364, 0.07230598200112581, 1.851730625028722, 0.1635646269423887, 0.08326259604655206, 0.07775719906203449, 0.07674639706965536, 0.07628466503228992, 0.07560261408798397, 0.0718397389864549, 0.0758500489173457, 0.07136786007322371, 0.07193626090884209, 0.07029419403988868, 0.07086422306019813, 0.07056875992566347, 0.07096583896782249, 0.07037114794366062, 0.07071233005262911, 0.07071589701808989, 0.07073681405745447, 0.07108941406477243, 0.0727302220184356, 0.07140186498872936, 0.07102888403460383, 0.07033306895755231, 0.07149533601477742, 0.07055672106798738, 0.07023596903309226, 0.0708840029546991, 0.07629205798730254, 0.07193478802219033, 0.07234971993602812, 0.07744445407297462, 0.0721695669926703, 0.07734764297492802, 0.0781255440087989, 0.07776005810592324, 0.07173149893060327, 0.07296880695503205, 0.07698263903148472, 0.07651836599688977, 0.07254430209286511, 0.2535263010067865, 0.08067652699537575, 0.07384072709828615, 0.07264083600603044, 0.07733111700508744, 0.07280785299371928, 0.07262227800674736, 0.07582833000924438, 0.07577311294153333, 0.0812903509940952, 0.08136789791751653, 0.07999352901242673, 0.07584154093638062, 0.07664036704227328, 0.07729127304628491, 0.07682448206469417, 0.07683227001689374, 0.16571695602033287, 0.07215675304178149, 0.07161684799939394, 0.07635691796895117, 0.07804688694886863, 0.1714566999580711, 0.07592906197533011, 0.07763163396157324, 0.08502235193736851, 0.07103162002749741, 0.07390389498323202, 0.07209276198409498, 0.07837525801733136, 0.07246361998841166, 1.847474624053575, 0.3320683119818568, 0.08705425204243511, 0.07466615200974047, 0.08062597003299743, 0.07978135894518346, 0.07745814591180533, 0.07715159095823765, 0.07570784003473818, 0.07074523309711367, 0.07558532303664833, 0.07035972992889583, 0.07134615792892873, 0.07447384390980005, 0.07487556105479598, 0.0706339799799025, 0.07598125201184303, 0.07468108600005507, 0.07438448094762862, 0.07001612894237041, 0.06943138397764415, 0.0743188849883154, 0.07517238997388631, 1.1998227750882506, 0.0768646050710231, 0.07808936096262187, 0.07706313801463693, 0.08102881792001426, 0.07521470193751156, 0.07696896407287568, 0.07316208991687745, 0.07492113299667835, 0.0714259990490973, 0.0766464569605887, 0.07588602299802005, 0.9150567279430106, 0.07311377697624266, 0.06887387996539474, 0.0696464009815827, 0.07293006998952478, 0.07272930804174393, 0.06940531195141375, 0.07706079503986984, 0.06838166201487184, 0.06958144996315241, 0.0740591250360012, 0.07237799698486924, 0.06862795096822083, 0.07344042102340609, 0.07523183396551758, 0.0730170370079577, 0.07257835299242288, 0.06911216396838427, 0.07380442903377116, 0.07358697894960642, 0.07375087204854935, 0.06908442499116063, 0.07933847000822425, 0.07144441199488938, 0.07514648896176368, 0.06949919403996319, 0.07537172106094658, 0.07743284699972719, 0.08154109306633472, 0.07926194800529629, 0.08026019192766398, 0.07493575604166836, 0.07481708796694875, 0.08057282597292215, 0.07896261301357299, 0.07900650997180492, 0.07550912396982312, 0.07442406995687634, 0.08078389707952738, 0.07645266701001674, 0.08118834591004997, 0.0721505950205028, 0.07653568498790264, 0.07124986895360053, 0.08379588497336954, 0.07665566704235971, 0.07112473901361227, 0.07553150493185967, 0.07207621494308114, 0.08465990494005382, 0.07670904195401818, 0.07226937904488295, 0.07694543094839901, 0.07182092196308076, 0.07282504497561604, 0.07844664098229259, 0.07635864801704884, 0.07808584999293089, 0.076721703982912, 0.07822672894690186, 2.0425386991119012, 0.09699661599006504, 0.07403753197286278, 0.07651441602502018, 0.07682748604565859, 0.07744968496263027, 0.07631680101621896, 0.08120866597164422, 0.07810020993929356, 0.07174994808156043, 0.07779710891190916, 0.07909111003391445, 0.07196949399076402, 0.0723079489544034, 0.07195151306223124, 0.07237535598687828, 0.0769613740267232, 0.9485415789531544, 1.9533805589890108, 0.1020010010106489, 0.07534406497143209, 0.08329197310376912, 0.08191265794448555, 0.07526720699388534, 0.0761861139908433, 0.07382370694540441, 0.07615243992768228, 0.07581292896065861, 0.07569299102760851, 0.07101376890204847, 0.08595684892497957, 0.07286037504673004, 0.08005324099212885, 0.07559390703681856, 0.3444967530667782, 0.30260972189716995, 0.43283338099718094, 0.07887636893428862, 0.07913228706456721, 0.07830523396842182, 0.07489989406894892, 0.07135064003523439, 0.0762308599660173, 0.07571481191553175, 0.07208003103733063, 0.07183596305549145, 0.07615785300731659, 0.07093312393408269, 0.07571388897486031, 0.0711197319906205, 0.07073856191709638, 0.0707201010081917, 0.07762634695973247, 0.07062745408620685, 0.06934012298006564, 0.06948903994634748, 0.33243719197344035, 0.07659773295745254, 0.07627957500517368, 0.0703707590000704, 0.07555597194004804, 0.07575901097152382, 0.07188225502613932, 0.07717403792776167, 0.07511224900372326, 0.07623414695262909, 0.07052704796660691, 0.07568660890683532, 0.0746906369458884, 0.07084863900672644, 0.07552292395848781, 0.07239191408734769, 0.07820837700273842, 0.07480397599283606, 0.07451032404787838, 0.07517205004114658, 0.0760930209653452, 0.07696629909332842, 0.07604320195969194, 0.0706881390651688, 0.07529065001290292, 0.07473651808686554, 0.07424621097743511, 0.07035886798985302, 0.07697977893985808, 0.07690521399490535, 0.07612138707190752, 0.07151998998597264, 0.07581024407409132, 0.07843596301972866, 0.07705363701097667, 0.0736564788967371, 0.07796827598940581, 0.07836799905635417, 0.08088833303190768, 0.0784935459960252, 0.07792850898113102, 0.07332304504234344, 0.07877776201348752, 0.7358003449626267, 0.2706768539501354, 0.0789915710920468, 0.07950189395342022, 0.07607488799840212, 0.08132637909147888, 0.07541230891365558, 0.0748157879570499, 0.07809991296380758, 0.07890280708670616, 0.08170158392749727, 0.08020401594694704, 0.08082753198686987, 0.08028164901770651, 2.100557186990045, 0.11968979798257351, 0.08303979795891792, 0.08192995900753886, 0.0768502299906686, 0.08810560696292669, 0.07730191899463534, 0.07508604705799371, 0.07794433506205678, 0.07702754496131092, 0.07262353494297713, 0.07217580091673881, 0.07798454107251018, 0.07718765502795577, 0.07726088096387684, 0.07755725900642574, 0.08232780802063644, 0.08236625394783914, 0.08200690301600844, 0.0822602219413966, 0.07829041697550565, 0.0773499479983002, 0.07283428206574172, 0.07705801504198462, 0.07226699299644679, 0.07831713499035686, 0.07681963499635458, 0.07291806093417108, 0.0726166139356792, 0.0769526989897713, 0.07250291504897177, 0.07268545695114881, 0.07646225008647889, 0.07687573705334216]
[0.02296056312138253, 0.0014816608959428814, 0.001330111468001744, 0.001416769327728876, 0.0012872880419754252, 0.0013001914093346925, 0.0012966318778237518, 0.0014050002661248554, 0.0013557628167755141, 0.0012902301826457285, 0.0013143293279204138, 0.0012888545930689695, 0.0013026115722117983, 0.00134605355085615, 0.0014222130412235856, 0.001400030488908595, 0.0014223188960126468, 0.0015656954694387255, 0.001427421655163777, 0.001643607428544486, 0.030959748244863385, 0.007418807306117854, 0.004026286551082621, 0.002035867981137518, 0.0013512108161361242, 0.0013306437141014909, 0.0013252110220491886, 0.0013227901226669854, 0.0013375499390293748, 0.001330479632644933, 0.0013778043473709602, 0.001367116449115684, 0.0014355623077753248, 0.0014090337956856405, 0.014447534202636048, 0.006068806285133624, 0.005931076306222501, 0.0020443891027790246, 0.001348255511031163, 0.0014166939594991961, 0.0013391415716852157, 0.001388870488510144, 0.0013267238367805068, 0.0013504934269098602, 0.0013456386727832106, 0.01934287708243165, 0.002477385020548744, 0.001479637243651918, 0.0014165322229798352, 0.0013967512451036244, 0.001309752815916222, 0.0013101423677171068, 0.0013105134474949874, 0.0013187209985275961, 0.0013234597127124363, 0.001387586653213568, 0.0030352130807860165, 0.0014164746520394574, 0.0014049243870932534, 0.044306261203612904, 0.0015108608355631633, 0.0015163327144382863, 0.0013034556331873244, 0.0013519917538731682, 0.0013186508167192948, 0.0013149330815376372, 0.0012764244896302723, 0.0012865180805401535, 0.028878664754673232, 0.0014641071641247492, 0.0015511445306735684, 0.001276205122774961, 0.0012625906716233917, 0.0012748213057235188, 0.0012647897356702965, 0.0012621007769425608, 0.001260335571417699, 0.0012710802257061005, 0.0012851604094196642, 0.0015460015916056475, 0.026617862652436052, 0.012392485224432787, 0.006980918305070728, 0.0013825161838713958, 0.0013360371219222338, 0.0012826801215925691, 0.0012759354502456834, 0.0012700126544401354, 0.0015092723057320227, 0.001262213633281692, 0.0012487692245263227, 0.016326862755611692, 0.009445939491008769, 0.003211464162687866, 0.001282149631225941, 0.0012752084912048007, 0.0016400286528681005, 0.0012520558571404948, 0.0012518098573105372, 0.001235373204155844, 0.001270457429868378, 0.0015132038364643041, 0.0014967696114006092, 0.0013152350201176442, 0.006414334427527323, 0.006079275693212237, 0.0013306148573566153, 0.0012729027764681652, 0.0012556565521589043, 0.0012678661412198324, 0.0012496363476146848, 0.0012541707552855416, 0.0012501722857432098, 0.0012643289804572658, 0.001263832286171311, 0.0012655388350047323, 0.0012607723688326624, 0.022157721102655847, 0.01288032808046484, 0.0013212435902571495, 0.0013294299798352377, 0.0012633618779898602, 0.0012401552034579978, 0.0013399867350425646, 0.0013151493693264772, 0.042015064714419445, 0.0017889861839500312, 0.0012717143898563726, 0.00125789936698441, 0.0012788548352843036, 0.0012667523459436334, 0.0012793478779304698, 0.0012687989985760376, 0.0012605969000569716, 0.008825036122140532, 0.017487361183751146, 0.00142516618195389, 0.0013655075513548693, 0.0013641214891507917, 0.0013860905904094784, 0.0013602635495326653, 0.0013812532059240099, 0.0013790643460364366, 0.014271249673899492, 0.0191446923465487, 0.006295994347037405, 0.0013741196931472846, 0.0013847827759324288, 0.0014792928586200792, 0.0013951771436449215, 0.0013866269205487808, 0.0015645415298830793, 0.0013066521437116424, 0.0012673808580112396, 0.01372024112343028, 0.009702825141899591, 0.0012553980412455847, 0.0012356574286003501, 0.0014131313655525446, 0.001321261223614672, 0.0012520658165900683, 0.001224273632812713, 0.0012260105114012044, 0.0012302553252678137, 0.0012214131420478225, 0.0012248518154481236, 0.001221546691804364, 0.001228829389628099, 0.0012184715501926079, 0.0012301845304972055, 0.0012209122449311676, 0.001335355876089663, 0.0012110361439765108, 0.00122329969510284, 0.0012213842045249684, 0.001196881224002157, 0.0012196312463671273, 0.0012269717764717583, 0.0012204402449483775, 0.0012417085521037178, 0.0012234051650085924, 0.0012313419387542776, 0.0012332428156455256, 0.0013087557566979406, 0.0012266444284658954, 0.0012245216544679537, 0.001216392163472364, 0.0012237200589508427, 0.0012275947124830314, 0.0012272182443388263, 0.001219563651829958, 0.0012326272636918084, 0.0012152108781952032, 0.0012230456531123848, 0.0012208007546902007, 0.0012466031416528383, 0.0012875847941340537, 0.0013717862644365855, 0.0013400804279942293, 0.0013813749383374744, 0.001331278101579115, 0.014249180141855411, 0.001260728858012174, 0.0012595839180736517, 0.0012852908373449225, 0.0012555452876211125, 0.0012726128167871917, 0.001272156205009289, 0.0012955634061209097, 0.0012739478154297993, 0.001257820285343546, 0.0012585126524030858, 0.044339462570675024, 0.010028386407778884, 0.004864331101998687, 0.0013218773267593008, 0.0013365018566386128, 0.0013235929797460533, 0.0013240319605422567, 0.0013526305722604906, 0.0013278464487354671, 0.0013237916127949649, 0.0013203259785564579, 0.0014239181431808642, 0.0013376223659903115, 0.010019949896318115, 0.005640735631162415, 0.0013397285116038152, 0.00134092051184223, 0.0013479327151019657, 0.00133576369321696, 0.001356547470299565, 0.001344125325867564, 0.001339582103893769, 0.0013412471209671727, 0.0013655262633359858, 0.001343646079624946, 0.0013492356924036024, 0.0013459528777368215, 0.0015466176115964748, 0.0041836521431461585, 0.008063269817099279, 0.002677485938374029, 0.0013078173280370478, 0.0013191546527288702, 0.0012971921420028927, 0.0013027077336434502, 0.0012982741416412958, 0.001292973267370645, 0.0013556391623213279, 0.001262891346265619, 0.00126136254937369, 0.0012690816953664227, 0.0012640588779040441, 0.0013900096528232098, 0.0013679633468237458, 0.0014082647346872457, 0.007640838958037903, 0.0037288233466750507, 0.0014811201206389435, 0.001438489143394542, 0.0013758520600480996, 0.0012753915697412224, 0.0014828600188983338, 0.0013558377549812502, 0.00127581243013621, 0.011469692937383542, 0.001472533571210747, 0.0012940133669964817, 0.0013796462872236663, 0.0012669500809314909, 0.0012669961029967787, 0.0012770147153119348, 0.015941652348170018, 0.011063781593051948, 0.003304748755537582, 0.0014157427940517664, 0.0013130866109906715, 0.0012945329166986809, 0.0012945390819590918, 0.0013076870616677465, 0.001294919489217656, 0.001295998020154633, 0.001304806388287368, 0.0012828932875501259, 0.0013138346934729085, 0.0013024425728018491, 0.0012917145088847195, 0.0012985549805381773, 0.001283013409155668, 0.001296781592679267, 0.001295718428564771, 0.0013053732049860517, 0.001417331570968488, 0.014329937367452954, 0.0030748808580659784, 0.0013286045096738606, 0.0013328932453783192, 0.0014806184299974417, 0.001287101081344394, 0.0013052772241169397, 0.0012879547358927677, 0.0013591686729341745, 0.0012975509791653982, 0.0012915844492120097, 0.0012720287563659403, 0.0012857945094227182, 0.0012964435297121502, 0.001284399100731374, 0.0013109176106066728, 0.0012805200614301221, 0.001290324509942106, 0.0012922422029078007, 0.0013225044085815245, 0.0012995830822583971, 0.001308896671505455, 0.0014626479384546376, 0.019085433816404214, 0.010773089878280096, 0.004908095142442961, 0.0029999017145256606, 0.001248896140985343, 0.001240289998145736, 0.0012259138177852242, 0.0012204104687600415, 0.001210710100297417, 0.001263375674393408, 0.001251200449234825, 0.0014815566115727534, 0.0012195994483535082, 0.028158933672654842, 0.010589053039439023, 0.004459566674290263, 0.0013501158563838322, 0.001302991898692384, 0.001248198734805444, 0.0013933607747740283, 0.0014260443668736487, 0.001321122734522333, 0.0013386382868665518, 0.0013326774897738074, 0.0013305636726281776, 0.0013278023059460887, 0.00989363661121425, 0.0016608016722246396, 0.0012865211833648536, 0.0012955421211235986, 0.0012643086125275918, 0.0012669197345456602, 0.0012950251634441772, 0.0012394721828857247, 0.001231028042657643, 0.0015125754693218944, 0.0012220907552471878, 0.0012522104899494015, 0.001249184102599262, 0.0012451690806988248, 0.0012213460606883984, 0.0174735883459905, 0.007409884204745901, 0.007325763633588747, 0.010939290243372018, 0.005586261019984983, 0.001386975593940944, 0.0013773720402612674, 0.0014460425496063366, 0.0012675879798753529, 0.0012985633268040053, 0.001291967082616626, 0.0012799604506022772, 0.001458337713432099, 0.014403854632198962, 0.015558611957964544, 0.002081217693297991, 0.0013497414504538994, 0.001315886000342363, 0.0013226623461600772, 0.0013334457548715326, 0.0013270716928895942, 0.001328970103676678, 0.0013299955112137357, 0.0013019779378700318, 0.001322535835966772, 0.0013144922048347642, 0.001327691530827813, 0.001330406450172316, 0.0013228231632359782, 0.0013314975919772167, 0.0013340439799488808, 0.01056267518303072, 0.003485952796205422, 0.0018831899803017779, 0.0013711983694371824, 0.001402766876188772, 0.001290635899545587, 0.001237052326489772, 0.0012425342836056132, 0.001255954245143399, 0.00125108387948451, 0.01628470273536383, 0.003826645202934742, 0.0013029555723603284, 0.0012781732235750069, 0.0013540419179718106, 0.0012719789377356671, 0.0028722382231368398, 0.0013472406331411734, 0.04101869389794919, 0.001649161406355549, 0.0013591767958726507, 0.001356607918361468, 0.0014506764068477312, 0.0013421182449412893, 0.0013469603882000154, 0.0013330189810533608, 0.0013489256332609423, 0.0013367038989933779, 0.0013258641025013461, 0.0013450193670293202, 0.0013392799372347643, 0.020693125407572607, 0.008844007509855591, 0.0013174675096168506, 0.001330816001650326, 0.0013094363871923819, 0.0012219262865314983, 0.001226097388116985, 0.0012929067965977046, 0.0012087982043395846, 0.0012053735292877775, 0.0012610339992014425, 0.0013301147347582237, 0.001246952836648846, 0.001297489775591815, 0.0012927062866487065, 0.0012934808975693826, 0.0013497541420998014, 0.0012853432432463278, 0.0012699661833443204, 0.0012599203868636063, 0.005063662039382117, 0.003791012124120429, 0.0021243952452300154, 0.0013609219993445643, 0.0012404086324861463, 0.001244387389825923, 0.0012289936944120089, 0.0013305576736753692, 0.0013068090216731842, 0.0013021561245871137, 0.001341816876558777, 0.0013223835528467078, 0.001322945591765551, 0.0013133370821193165, 0.028952706184675346, 0.006079240184163256, 0.0013794289184354094, 0.0013809248790791144, 0.0013635738572219806, 0.0012825199792503702, 0.0012757895913507258, 0.0012655700818274397, 0.0012657339399566455, 0.0012550308150524388, 0.0013093406130198616, 0.0012615572456840653, 0.0012808641019676412, 0.001356789590410736, 0.0013543331653487925, 0.027912095429052656, 0.004180568550732367, 0.004033997327051297, 0.003987192347341654, 0.005625334551690945, 0.0030613770822481234, 0.001333243365646625, 0.0013497612030454436, 0.0013465994888232375, 0.001249751613038231, 0.0015063868997124384, 0.001289236164480752, 0.0012893202638595688, 0.05548491930750636, 0.008824637530333534, 0.003979651164738651, 0.0024015299800061144, 0.0014195684272301744, 0.0013188330209528913, 0.0013177305939892422, 0.0013178052019555957, 0.0013021841022775186, 0.0013083249368533796, 0.001296150286643937, 0.0013024380183493604, 0.0013042724482258971, 0.003096411040299857, 0.002714216103302125, 0.0014753665309399366, 0.0012452327955172074, 0.0014118473069286163, 0.0013495082871950402, 0.0012361807558609514, 0.0012381292656254098, 0.0012427648572175174, 0.001242972694204322, 0.0012328233878717435, 0.0012483020403365396, 0.0012267822261937723, 0.0012183998787433517, 0.0012215650616669838, 0.0012583826117369594, 0.0012273616537603798, 0.0012255541847220489, 0.001271431590905603, 0.0012081650000217619, 0.0012205815921556584, 0.001213613918944433, 0.0012405681000946431, 0.0012407653052739951, 0.0012642237956502608, 0.0012331220810776766, 0.04427329118468096, 0.011046917633894754, 0.0021960758366527, 0.0013356505303966757, 0.0013403464702661245, 0.001285945203117266, 0.0012473596131657154, 0.0012474891216475137, 0.0012517334080814403, 0.0012462205914020234, 0.0012727316126835589, 0.003780036775053156, 0.0014350388165829437, 0.0012345605705655655, 0.0012372270207471993, 0.0013166494900360703, 0.00130138375644325, 0.001346361020351855, 0.0013688995707209926, 0.0014377247960287698, 0.0013678254493113076, 0.0013552757564513963, 0.001379578060717607, 0.001384852264000445, 0.0013699514069119279, 0.005824300061379161, 0.0015106950197558925, 0.0013393525305033034, 0.0013006779398503049, 0.0013008930017145313, 0.0013025870204580073, 0.001300622081878234, 0.0012956932662244961, 0.001333553876195635, 0.001300757083262564, 0.001297575877789332, 0.0013084401642637594, 0.0013338221208553534, 0.0012246361833864025, 0.001249124531216007, 0.0012275243667429503, 0.0012489923071686406, 0.0012416086131136637, 0.043165175835792054, 0.007522613530009225, 0.0019358983881087328, 0.001234107755352648, 0.001237592631380777, 0.0014097889604959258, 0.001350460735586833, 0.0012419917953333684, 0.001243465938795434, 0.0012389410011546345, 0.0012385542053083072, 0.0013692210599475978, 0.0016053037965024005, 0.0013365362466750096, 0.0013453511649514644, 0.0013577284063307606, 0.0013631787958878036, 0.0013434066513211143, 0.0013603352233577445, 0.005202490408733791, 0.006937326469021488, 0.007714813632168332, 0.0012940386124905578, 0.0012458373260285174, 0.0012352718159138244, 0.001234702877605296, 0.0012565684880187012, 0.001446926816632705, 0.0012384541427754626, 0.0012528626937229111, 0.0012398148359426735, 0.0014175611443589537, 0.001344657122936784, 0.0013678228572885298, 0.0013434382046249751, 0.0013748376525710433, 0.0013609216120854324, 0.0013677159381308118, 0.0013305352245248398, 0.0013408749388074692, 0.001340053367371462, 0.0013617753878007739, 0.0013641232258773275, 0.001364277958470796, 0.0013555775714886127, 0.0013560931226808805, 0.0013730137547173975, 0.0013667018585173146, 0.0014109554491481002, 0.0013467546134274832, 0.024833521102460062, 0.004722682122445228, 0.0013578741036697614, 0.0013680317348857619, 0.0013809255086721815, 0.0013605560204584379, 0.0014528239796850451, 0.0012517708581777252, 0.019016207081778924, 0.0037739657754154534, 0.001504938365245352, 0.0012396003656583478, 0.001250883348153106, 0.0012505632036422588, 0.0012534332238327787, 0.0012452369603347412, 0.0012420173686910041, 0.0012683786537346182, 0.0012636749187902529, 0.0012578778372773407, 0.0012403013664582859, 0.001936020959187679, 0.0015246735301286895, 0.0013536369185704663, 0.0013055343879386783, 0.0012853872030973434, 0.018339722633494863, 0.0014396328787908566, 0.0012520793469014521, 0.00124212889932096, 0.001257119284543608, 0.0012367577149475716, 0.0012861709181713511, 0.0012602990407648744, 0.021176726324483752, 0.024092649368626276, 0.004011142614050483, 0.0012590636936377507, 0.0012479315307561535, 0.0012421472240429447, 0.0012503877966379632, 0.0012568614697463963, 0.0012549982045073898, 0.0012618720612241601, 0.0013282897962941503, 0.0013787843077918704, 0.0013695670629148276, 0.0013324771033676, 0.0013438913477014523, 0.0013514311428238846, 0.0013500965314404089, 0.003965133223777675, 0.032081761592238835, 0.01833512134878535, 0.0014367160002453899, 0.0014259266329701154, 0.0014017741837329706, 0.0012531130816977546, 0.0012321591838167943, 0.0012821735510108422, 0.0013195492032611249, 0.0013178185517044396, 0.001332759550696581, 0.0013361318982491384, 0.0013097104098533793, 0.001351006571393536, 0.0013194056536660207, 0.020556476815337583, 0.00475762038710242, 0.004312983958753853, 0.0013357585923252056, 0.001335887081578982, 0.0013214464498949902, 0.0013247405307139366, 0.0013245790602867396, 0.001259017122757374, 0.0012931663671280353, 0.0012686284287471551, 0.0013723820614229356, 0.0012612748957638229, 0.0012615229791904591, 0.0012509424705058336, 0.0012805378349611954, 0.001238952531023141, 0.0012835167771281333, 0.0012704484278754312, 0.0012617327144597561, 0.044105792570174954, 0.0013322541228856664, 0.0014662833276147746, 0.0013284102053742629, 0.0013303324290342173, 0.0012361301223234255, 0.0013258416319684107, 0.0012456190401726232, 0.0012210656945802728, 0.0012223654288836584, 0.02796719593889251, 0.00395767528763307, 0.034447801917107126, 0.0013297872442029873, 0.0013100929981173606, 0.0016508654914606288, 0.0015793667550255753, 0.0014866961223282376, 0.0014756322857372615, 0.03779042091895351, 0.0033380536110691576, 0.0016992366540112666, 0.001586881613510908, 0.0015662530014215379, 0.0015568298986181617, 0.0015429104915915095, 0.001466117122172549, 0.001547960181986647, 0.0014564869402698716, 0.0014680869573233078, 0.0014345753885691567, 0.0014462086338815944, 0.001440178773993132, 0.0014482824279147448, 0.0014361458764012372, 0.0014431087765842676, 0.0014431815717977528, 0.001443608450152132, 0.001450804368668825, 0.0014842902452741958, 0.001457180918137334, 0.0014495690619306906, 0.0014353687542357615, 0.0014590884900974985, 0.0014399330830201507, 0.0014333871231243319, 0.0014466123051979408, 0.0015569807752510722, 0.0014680568984120476, 0.0014765248966536351, 0.0015804990627137677, 0.0014728483059728633, 0.0015785233260189391, 0.0015943988573224265, 0.0015869399613453721, 0.001463908141440883, 0.001489159325612899, 0.0015710742659486678, 0.001561599306058975, 0.0014804959610788797, 0.005174006142995643, 0.0016464597345995052, 0.0015069536142507378, 0.0014824660409393968, 0.001578186061328315, 0.0014858745508922301, 0.0014820873062601503, 0.001547516938964171, 0.0015463900600312924, 0.0016589867549815349, 0.0016605693452554394, 0.0016325210002536069, 0.0015477865497220535, 0.0015640891233116997, 0.001577372919311937, 0.0015678465727488606, 0.001568005510548852, 0.0033819786942925075, 0.0014725867967710507, 0.0014615683265182438, 0.0015583044483459421, 0.0015927936112014006, 0.0034991163256749207, 0.001549572693374084, 0.0015843190604402702, 0.001735150039538133, 0.0014496248985203554, 0.0015082427547598372, 0.001471280856818265, 0.0015994950615781912, 0.0014788493875186055, 0.03770356375619541, 0.006776904326160343, 0.0017766173886211247, 0.0015237990206069484, 0.0016454279598570904, 0.001628190998881295, 0.0015807784879960272, 0.0015745222644538296, 0.001545057959892616, 0.0014437802672880341, 0.0015425576129928231, 0.0014359128556917517, 0.0014560440393658926, 0.0015198743655061235, 0.0015280726745876732, 0.0014415097955082143, 0.0015506377961600618, 0.0015241037959194913, 0.0015180506315842575, 0.0014289005906606208, 0.0014169670199519213, 0.001516711938537049, 0.0015341304076303328, 0.024486179083433687, 0.0015686654096127165, 0.0015936604278086095, 0.0015727171023395292, 0.0016536493453064135, 0.0015349939170920728, 0.0015707951851607282, 0.0014931038758546418, 0.0015290027142179255, 0.0014576734499815776, 0.0015642134073589528, 0.0015486943468983685, 0.018674627100877767, 0.0014921178974743401, 0.0014055893870488722, 0.0014213551220731164, 0.0014883687752964242, 0.0014842715926886517, 0.001416434937783954, 0.001572669286527956, 0.0013955441227524864, 0.001420029591084743, 0.0015114107150204328, 0.0014771019792830457, 0.001400570427922874, 0.0014987841025184915, 0.0015353435503166852, 0.0014901436124073, 0.0014811908773963852, 0.001410452325885393, 0.001506212837423901, 0.0015017750806042127, 0.0015051198377254971, 0.0014098862243094006, 0.0016191524491474337, 0.0014580492243854975, 0.0015336018155461975, 0.001418350898774759, 0.0015381983889989099, 0.0015802621836679019, 0.00166410394012928, 0.0016175907756182917, 0.001637963100564571, 0.0015293011437075175, 0.0015268793462642602, 0.0016443433872024929, 0.0016114818982361835, 0.001612377754526631, 0.0015410025299963902, 0.0015188585705484968, 0.0016486509608066812, 0.0015602585104085049, 0.0016569050185724485, 0.0014724611228674042, 0.001561952754855156, 0.0014540789582367455, 0.0017101201014973375, 0.0015644013682114227, 0.0014515252859920872, 0.0015414592843236668, 0.0014709431621036967, 0.0017277531620419147, 0.0015654906521228198, 0.0014748852866302644, 0.0015703149173142655, 0.0014657331012873625, 0.0014862254076656333, 0.0016009518567814815, 0.0015583397554499762, 0.0015935887753659366, 0.0015657490608757551, 0.0015964638560592216, 0.041684463247181655, 0.00197952277530745, 0.0015109700402625057, 0.001561518694388167, 0.0015679078784828283, 0.001580605815563883, 0.0015574857350248768, 0.0016573197137070249, 0.001593881835495787, 0.001464284654725723, 0.001587696100243044, 0.0016141042864064173, 0.00146876518348498, 0.0014756724276408857, 0.0014683982257598213, 0.0014770480813648627, 0.0015706402862596574, 0.01935799140720723, 0.03986490936712267, 0.0020816530818499775, 0.001537633979008818, 0.0016998361857912066, 0.0016716868968262356, 0.0015360654488548028, 0.001554818652874353, 0.0015066062641919268, 0.0015541314270955567, 0.0015472026318501756, 0.001544754918930786, 0.0014492605898377238, 0.0017542214066322362, 0.0014869464295251028, 0.0016337396120842621, 0.0015427327966697666, 0.007030545980954657, 0.006175708610146325, 0.008833334306064916, 0.001609721814985482, 0.0016149446339707595, 0.0015980659993555472, 0.0015285692667132433, 0.001456135510923151, 0.0015557318360411695, 0.0015452002431741174, 0.001471021041578176, 0.0014660400623569684, 0.0015542418981085019, 0.0014476147741649527, 0.0015451814076502103, 0.001451423101849398, 0.0014436441207570688, 0.0014432673675141164, 0.0015842111624435199, 0.0014413766140042215, 0.0014151045506135846, 0.0014181436723744382, 0.006784432489253885, 0.0015632190399480109, 0.0015567260205137487, 0.001436137938776947, 0.0015419586110213886, 0.001546102264724976, 0.0014669847964518228, 0.0015749803658726873, 0.0015329030408923114, 0.0015557989174005936, 0.0014393275095225901, 0.0015446246715680677, 0.001524298713181396, 0.0014458905919740091, 0.0015412841624181186, 0.0014773860017826058, 0.0015960893265864983, 0.0015266117549558378, 0.001520618858119967, 0.0015341234702274812, 0.0015529187952111267, 0.001570740797823029, 0.0015519020808100396, 0.0014426150829626285, 0.001536543877814345, 0.001525235062997256, 0.0015152287954578595, 0.0014358952650990412, 0.0015710158967317976, 0.0015694941631613337, 0.0015534976953450515, 0.0014595916323667886, 0.0015471478382467615, 0.0016007339391781359, 0.0015725232043056463, 0.0015031934468721856, 0.001591189305906241, 0.001599346919517432, 0.001650782306773626, 0.001601909101959698, 0.0015903777343087963, 0.0014963886743335395, 0.001607709428846684, 0.015016333570665851, 0.005524017427553784, 0.0016120728794295266, 0.0016224876317024535, 0.0015525487346612678, 0.0016597220222750793, 0.0015390267125235833, 0.001526852815449998, 0.0015938757747715833, 0.0016102613691164522, 0.0016673792638264749, 0.001636816651978511, 0.0016495414691197934, 0.0016384010003613575, 0.042868514020205, 0.0024426489384198674, 0.001694689754263631, 0.001672039979745691, 0.0015683720406258898, 0.0017980736114882997, 0.0015775901835639865, 0.0015323683073059941, 0.0015907007155521792, 0.0015719907134961412, 0.0014821129580199414, 0.001472975528913037, 0.0015915212463777587, 0.0015752582658766483, 0.0015767526727321806, 0.0015828012042127702, 0.0016801593473599274, 0.001680943958119166, 0.0016736102656328253, 0.0016787800396203386, 0.0015977636117450132, 0.0015785703673122488, 0.0014864139197090147, 0.0015726125518772372, 0.00147483659176422, 0.0015983088773542217, 0.0015677476529868282, 0.0014881236925341037, 0.0014819717129730449, 0.0015704632446892103, 0.001479651327530036, 0.0014833766724724248, 0.0015604540833975284, 0.0015688925929253502]
[43.55293878087546, 674.9182641846211, 751.8166890947284, 705.8312037309772, 776.8269162707645, 769.1175259431222, 771.2289178624742, 711.7436374287028, 737.5921419488074, 775.0555005227158, 760.8443171410025, 775.8827142935028, 767.6885583797077, 742.9124936106409, 703.12953897516, 714.2701590588631, 703.0772091992992, 638.69380701375, 700.5638427737483, 608.4177904242989, 32.30000425361704, 134.79255609932864, 248.36781667492392, 491.1909854986086, 740.0769650879242, 751.5159688521461, 754.5968025935136, 755.9778251018523, 747.6356364874674, 751.6086495905581, 725.7924551538375, 731.466584757171, 696.5911507872404, 709.7061852326947, 69.2159634976011, 164.77705054610783, 168.6034622334676, 489.143675556017, 741.6991748360718, 705.8687540063358, 746.7470364179421, 720.0095388827151, 753.7363634218325, 740.4700978724169, 743.1415432879026, 51.698617312119474, 403.65142749531066, 675.8413281973648, 705.9493485410359, 715.9470975991937, 763.5028440847153, 763.275827605268, 763.0597014563065, 758.3105153527845, 755.595346344541, 720.6757125286985, 329.46616049145194, 705.9780410190806, 711.782078229114, 22.57017344353254, 661.8743278412248, 659.4858704017622, 767.1914367769557, 739.6494816889327, 758.3508744854272, 760.4949742618344, 783.4384314340905, 777.2918353235605, 34.62763976434115, 683.010113264356, 644.6852502943491, 783.5730966395239, 792.02232558414, 784.4236643287467, 790.6452525644772, 792.3297554910789, 793.4394796737679, 786.7324027045483, 778.1129831501476, 646.8298644902553, 37.56875647971985, 80.69406433734689, 143.24762965262465, 723.3188382646969, 748.4821967830071, 779.61760158753, 783.7387070070422, 787.3937291127496, 662.5709596619034, 792.2589121463142, 800.7884726493924, 61.248753968749504, 105.8655945183496, 311.3844493793262, 779.940168951918, 784.1854935071933, 609.745444539148, 798.6864118697132, 798.8433659952635, 809.4719851749744, 787.1180698306451, 660.8495008422414, 668.105493579767, 760.3203873863948, 155.90082046680755, 164.4932801972678, 751.5322668097882, 785.6059539555963, 796.3961150687718, 788.7267965353861, 800.2328052547506, 797.3395933413599, 799.8897523196285, 790.9333847890865, 791.2442267394736, 790.1772528349638, 793.1645907864327, 45.130994986670316, 77.63777395675709, 756.8627067514274, 752.2020829738882, 791.5388436376628, 806.3506867621417, 746.2760442686285, 760.369904227781, 23.800986784076116, 558.9758093000071, 786.3400838870275, 794.9761532970019, 781.9495789588103, 789.4202866110161, 781.6482266087351, 788.1469020091373, 793.2749953254731, 113.3139837797568, 57.18415657413063, 701.6725576725438, 732.3284290942154, 733.072536393024, 721.4535665411165, 735.1516552388409, 723.9802200719854, 725.1293261798088, 70.07094843480228, 52.23379837599085, 158.83114642098627, 727.7386424101087, 722.1349206388421, 675.9986666418606, 716.7548612411215, 721.1745172264744, 639.1648805095839, 765.3146285433136, 789.0288019413432, 72.88501645151729, 103.06276629491262, 796.5601085435954, 809.285791396663, 707.6482939779578, 756.8526057733126, 798.68005878752, 816.8108609041474, 815.6536919549756, 812.8393996443873, 818.7237926091077, 816.425291115024, 818.6342828393123, 813.7826198172609, 820.7003272599402, 812.886176999664, 819.0596860271296, 748.8640428410072, 825.7391862116016, 817.4611699841323, 818.7431901405085, 835.5047935802507, 819.9199577566292, 815.0146720371749, 819.376453815892, 805.3419607248318, 817.3906965587943, 812.1220990910767, 810.8703227892412, 764.0845091853139, 815.2321706222979, 816.6454193368211, 822.1032903939121, 817.1803613788522, 814.6010974398218, 814.8509889035736, 819.9654019693829, 811.2752568890348, 822.902442648614, 817.6309669677644, 819.1344870635891, 802.1799132273374, 776.647879468424, 728.9765365967678, 746.2238676948323, 723.9164199717779, 751.1578526033255, 70.1794762958052, 793.1919648263844, 793.9129625673157, 778.0340222962615, 796.4666904964492, 785.7849510934334, 786.0669908792358, 771.8649625911663, 784.9615093241665, 795.0261350148857, 794.5887537090192, 22.553272909116725, 99.71693942948923, 205.57811115882177, 756.4998504449642, 748.2219310305064, 755.5192686137256, 755.2687773416363, 739.300161114072, 753.0991259963218, 755.405904021908, 757.3887178174912, 702.2875611136737, 747.5951549745865, 99.80089824276021, 177.28184148100644, 746.4198838336883, 745.7563600292348, 741.876793104138, 748.6354098992389, 737.1655042629441, 743.9782442567632, 746.5014627272907, 745.5747597645543, 732.3183939040442, 744.2436034042026, 741.1603514716878, 742.9680611712569, 646.5722312367591, 239.0256086750051, 124.01916625428578, 373.48468788122756, 764.6327805588396, 758.0612310553196, 770.895819994699, 767.6318902346356, 770.2533447487343, 773.411195139071, 737.6594213224488, 791.8337574780356, 792.7934759887509, 787.9713367950433, 791.1023904662699, 719.4194644396375, 731.0137382861064, 710.0937596240276, 130.8756807324193, 268.18111426267717, 675.1646852036606, 695.1738249759767, 726.8223299858562, 784.0729260919456, 674.3724877975558, 737.5513746582673, 783.8142789479147, 87.18629221020099, 679.1016650152022, 772.7895441459677, 724.8234632750339, 789.2970804854261, 789.2684102458857, 783.076332644868, 62.72875472126278, 90.38500910285501, 302.5948639285683, 706.3429912562459, 761.5643870175021, 772.4793916791247, 772.475712735261, 764.7089501097146, 772.2487832847154, 771.606117022219, 766.3972287203135, 779.4880600783622, 761.1307609457797, 767.7881703826476, 774.1648739886115, 770.0867618139332, 779.4150808276314, 771.1398786390162, 771.7726150639613, 766.0644451566518, 705.5512065653643, 69.783975627923, 325.215852632734, 752.6694307589511, 750.2476312093299, 675.3934570446538, 776.9397559323677, 766.1207761259535, 776.4248013784684, 735.7438557211586, 770.6826290888494, 774.2428306643795, 786.1457494537314, 777.7292504141808, 771.3409624729511, 778.5741981838597, 762.824445952187, 780.9327086083844, 774.9988412177553, 773.8487396169248, 756.1411466843938, 769.4775452618343, 764.0022484355691, 683.6915252870429, 52.39597955276684, 92.8238798059344, 203.74503162183177, 333.34425429938403, 800.7070941952218, 806.2630525885273, 815.7180264161086, 819.3964453746595, 825.9615573987076, 791.5301998197297, 799.232449613931, 674.9657705880339, 819.941335124435, 35.51270838679167, 94.43715092137994, 224.23703311020668, 740.6771761635428, 767.4644800198288, 801.1544733346244, 717.6892145267815, 701.2404545254956, 756.9319442236088, 747.0277892176332, 750.3690935529555, 751.56117709479, 753.1241627777396, 101.07506868268428, 602.1188542401349, 777.2899606553956, 771.8776438798604, 790.9461266745712, 789.3159864295725, 772.1857676807263, 806.795032440189, 812.3291796351926, 661.1240366395152, 818.2698344671903, 798.5877837841841, 800.5225153916323, 803.1037836554465, 818.7687602941635, 57.22922963499141, 134.95487545669303, 136.5045406891076, 91.41360890446141, 179.01061128767097, 720.9932203338965, 726.0202550723438, 691.5425830811375, 788.8998758873795, 770.0818122295024, 774.0135282508097, 781.2741397825663, 685.7122261801538, 69.42586033634075, 64.27308571624181, 480.4879389696881, 740.8826332359533, 759.9442502920642, 756.0508567459993, 749.9367682162236, 753.5387917306703, 752.462374611316, 751.8822368711709, 768.0621698060015, 756.1231785216628, 760.7500419720657, 753.1869992245126, 751.6499937823352, 755.9589428066358, 751.0340281690205, 749.6004742199872, 94.67298602597687, 286.8656170813712, 531.0138703264297, 729.2890819367417, 712.8768272009218, 774.8118585203501, 808.3732422520673, 804.806767261325, 796.2073490072281, 799.3069181037124, 61.40732294906446, 261.3255075837909, 767.4858768887119, 782.3665693786278, 738.5295733664419, 786.1765398255458, 348.1605362482351, 742.2578976618596, 24.379128269854466, 606.3687860667814, 735.739458646332, 737.1326574651101, 689.3336069158006, 745.0908321745859, 742.4123298357205, 750.1768648558875, 741.3307118959282, 748.1088375316799, 754.2251110905122, 743.4837181628485, 746.6698874506537, 48.32522783793945, 113.07091257957654, 759.0320009415813, 751.4186775331182, 763.6873465416235, 818.3799718709321, 815.5958977579908, 773.4509576649366, 827.2679396858805, 829.618351243264, 793.0000306361733, 751.8148426359406, 801.9549501867884, 770.7189827710796, 773.5709266120019, 773.1076677507406, 740.8756667672145, 778.0023003616913, 787.4225417298961, 793.7009436678445, 197.48553363605265, 263.78179949292957, 470.72219835048946, 734.7959695571176, 806.1859405119616, 803.6082719705876, 813.6738248103323, 751.5645655837847, 765.2227551349786, 767.9570683715664, 745.2581775276218, 756.2102521974736, 755.8889845692289, 761.419146397901, 34.539085694493714, 164.49424100812024, 724.9376801047717, 724.1523526369236, 733.3669494348533, 779.7149488341676, 783.828310545521, 790.1577434226592, 790.0554519650887, 796.793184682256, 763.7432078835477, 792.6711240580772, 780.7229498147518, 737.0339565306316, 738.3707536560709, 35.82676200508892, 239.20191425274356, 247.89307451796532, 250.80304958618845, 177.7672049210665, 326.65038416817623, 750.0506102387379, 740.871791057349, 742.6113022468746, 800.1589992502047, 663.8400799893407, 775.6530785829735, 775.6024845266209, 18.02291528005725, 113.31910195321124, 251.27830520936405, 416.40121436146046, 704.4394485098364, 758.246104027236, 758.8804605140434, 758.8374962521173, 767.9405686576891, 764.3361154646151, 771.5154718587878, 767.7908552357417, 766.7109746588791, 322.95453897592347, 368.430501456165, 677.7976719879318, 803.0626912493499, 708.2918918303113, 741.0106403114463, 808.9431867134505, 807.670109869243, 804.6574492289276, 804.52290276589, 811.1461948546638, 801.0881723227835, 815.1406000579343, 820.7486043345575, 818.6219722389329, 794.6708661364042, 814.7557787358021, 815.957395002324, 786.5149860620733, 827.7015142650116, 819.2815674320541, 823.9852760338903, 806.0823101317129, 805.9541927465262, 790.9991913145759, 810.9497148295799, 22.58698129824175, 90.52298868706556, 455.3576808732702, 748.6988379385508, 746.0757514446627, 777.638112087432, 801.6934246107797, 801.6101965517232, 798.8921551057124, 802.4261570537683, 785.7116064646941, 264.54769080545196, 696.8452619150466, 810.0048096804916, 808.2591013863158, 759.5035790220861, 768.4128490531128, 742.7428341164111, 730.5137801112064, 695.5434049424223, 731.0874355375494, 737.8572185326792, 724.859309142562, 722.0986858997392, 729.9528982959674, 171.69445074284272, 661.9469760094835, 746.6294177412864, 768.829830476782, 768.7027285733993, 767.7030281235157, 768.8628495034435, 771.7876028744728, 749.8759651562049, 768.7830517069307, 770.6678407922401, 764.2688044223144, 749.7251577734519, 816.5690460286487, 800.5606927169323, 814.6477797857067, 800.6454437392933, 805.406783939936, 23.166823269854767, 132.9325235186944, 516.5560373119301, 810.3020142793355, 808.0203248174677, 709.3260253990264, 740.4880228268593, 805.1582979512241, 804.2037733407611, 807.1409365482676, 807.3930036441764, 730.3422575447909, 622.935049539394, 748.2026787434809, 743.3003561089395, 736.5243264685638, 733.5794856966841, 744.3762460284039, 735.1129213074986, 192.21563548127426, 144.14774978019022, 129.6207591885715, 772.7744677381461, 802.6730128465504, 809.5384247557078, 809.9114516842288, 795.818142452986, 691.1199574883855, 807.4582380248088, 798.172062278011, 806.5720549630832, 705.4369428644419, 743.6840090624446, 731.0888209474183, 744.358762879721, 727.3586071271249, 734.796178648109, 731.1459727278235, 751.5772461846094, 745.7817064500941, 746.2389367085563, 734.335492444881, 733.0716030854588, 732.9884601528628, 737.6929369684547, 737.4124853779106, 728.3248230865877, 731.6884759964136, 708.7395995414137, 742.5257652951382, 40.2681519013805, 211.7440839914581, 736.445298792739, 730.9771948261898, 724.1520224805917, 734.9936239031532, 688.3146299779469, 798.868254095448, 52.58672224695042, 264.9732561207225, 664.4790398688313, 806.7116045653175, 799.4350564155098, 799.6397120013648, 797.8087551741892, 803.0600053271648, 805.1417195992414, 788.4080964746581, 791.3427615998936, 794.9897600266861, 806.2556625697569, 516.5233337244358, 655.878114389246, 738.7505366328726, 765.9698658561651, 777.9756929198783, 54.52645168000763, 694.6215349290279, 798.6714280326736, 805.069426004559, 795.4694612477017, 808.5658071212371, 777.5016414006449, 793.4624780743314, 47.22165195306117, 41.506435622734436, 249.3055211991558, 794.240994362048, 801.3260145723495, 805.0575492534587, 799.7518871255743, 795.6326326096824, 796.8138889828282, 792.473366142908, 752.8477616781674, 725.2766037071489, 730.1577462528312, 750.4819388435847, 744.1077745685076, 739.9563087694196, 740.6877780310322, 252.19833573391935, 31.170358183882215, 54.540135348831335, 696.0317834764844, 701.2983535604935, 713.3816641828623, 798.0125773207717, 811.5834489033738, 779.9256186587364, 757.8345676906983, 758.8298090860994, 750.3228916854051, 748.4291044247921, 763.5275649308986, 740.1888496874742, 757.9170190922409, 48.64646840911381, 210.18911107555593, 231.85803832410483, 748.6382687303266, 748.5662626649756, 756.7465182410273, 754.8648031936295, 754.9568236293302, 794.2703732336058, 773.2957068941392, 788.2528700602733, 728.6600634834615, 792.8485719954048, 792.6926552235435, 799.3972733179633, 780.9218694661242, 807.1334251798886, 779.1094108154148, 787.1236470986062, 792.5608875316957, 22.67275887648862, 750.6075476306255, 681.9964335451562, 752.7795224354383, 751.6918164025897, 808.9763221046695, 754.2378937938087, 802.8136755692301, 818.956755921096, 818.0859637966556, 35.75617670734564, 252.67358419342702, 29.029428420609612, 751.9999942542339, 763.3045909237187, 605.7428695267199, 633.1651573758792, 672.6324128927921, 677.6756036483546, 26.461732250737047, 299.57577574067375, 588.4995463341562, 630.1667317119786, 638.4664540737645, 642.3309321638783, 648.1257373319834, 682.0737476404076, 646.0114488969627, 686.5835678655042, 681.1585614950573, 697.0703721589692, 691.4631655295962, 694.3582408365427, 690.4730601750189, 696.3080954602242, 692.9484569880633, 692.913504122917, 692.7086079986695, 689.2728072755548, 673.7226786902908, 686.2565845826933, 689.8601979460665, 696.6850832226967, 685.3593916933568, 694.476716864214, 697.648237428222, 691.2702155282507, 642.268688153034, 681.172508423665, 677.2659250557704, 632.7115425699575, 678.95654694695, 633.5034671435714, 627.1956326407324, 630.1435620489526, 683.1029705290993, 671.5198184643044, 636.5071477993856, 640.3691370251123, 675.4493266373191, 193.27383315030633, 607.3637751264206, 663.5904320765727, 674.5517080218096, 633.6388493751668, 673.0043255667346, 674.7240839160594, 646.1964808406873, 646.6673744525777, 602.7775670886115, 602.2030954968482, 612.5495475063741, 646.0839191163515, 639.3497564145614, 633.9654927233113, 637.8175118543192, 637.752860734506, 295.6848905309839, 679.0771193879407, 684.196545489054, 641.7231248113596, 627.8277317082703, 285.78644061143547, 645.3391985261248, 631.1859933832442, 576.3190370938659, 689.833625940551, 663.0232413476659, 679.6798825769822, 625.1973038374493, 676.2013822637629, 26.522691766389883, 147.55999965054542, 562.8673941867269, 656.2545233830688, 607.7446259554581, 614.1785580973514, 632.5997017252636, 635.1132801204816, 647.2249106237423, 692.6261721794956, 648.2740038862021, 696.4210927119595, 686.7924135285772, 657.9491191477504, 654.4191363606673, 693.7171034952579, 644.8959276475529, 656.1232920469831, 658.739556635464, 699.8387477309894, 705.7327276635773, 659.3209788831452, 651.8350689265277, 40.83936479401793, 637.4845737478761, 627.4862464741421, 635.8422621032279, 604.7231251524516, 651.4683796887109, 636.6202350548185, 669.7457666350289, 654.0210757647302, 686.0247060222152, 639.2989570958982, 645.7052045180766, 53.5485926759414, 670.1883287457833, 711.4453262197477, 703.5539426216376, 671.8764976783658, 673.7311452471927, 705.997835357354, 635.8615944028127, 716.5663798774493, 704.2106772127984, 661.6335255943194, 677.0013269397817, 713.9947981645287, 667.2075039491301, 651.3200252762559, 671.0762584718383, 675.1324324639271, 708.9924144527629, 663.9167952587068, 665.8786744534792, 664.3989235509495, 709.2770911282762, 617.6070699992154, 685.8479009317776, 652.0597392771386, 705.044147300819, 650.111199668347, 632.8063851271365, 600.9240023326382, 618.203327487306, 610.5143636357384, 653.8934493802042, 654.9305958238615, 608.1454809152068, 620.5468402062293, 620.2020569885526, 648.9282013069391, 658.3891478710067, 606.5565263800303, 640.9194331125176, 603.5348971672361, 679.1350783188389, 640.2242301449974, 687.7205631340862, 584.7542515431668, 639.2221461320366, 688.9304717254863, 648.7359154859297, 679.835921443648, 578.7863810465655, 638.7773690273976, 678.0188324237366, 636.8149400951472, 682.25245040976, 672.8454478319462, 624.6284020122741, 641.7085853728004, 627.5144601030272, 638.6719462189425, 626.3843657998259, 23.989753546067583, 505.17226296862634, 661.8264911634295, 640.402195371615, 637.7925729715963, 632.6688097394155, 642.0604552015544, 603.3838804482938, 627.3990817449424, 682.9273234358324, 629.8434567212959, 619.5386558487887, 680.8440254740191, 677.657169212459, 681.0141707182675, 677.0260309169844, 636.6830194973622, 51.65825208640639, 25.0847177599435, 480.3874424221033, 650.3498320482063, 588.2919826974618, 598.198144579909, 651.0139270078233, 643.1618235035487, 663.7434237248133, 643.4462250524417, 646.3277526901438, 647.3518794114976, 690.0070332499497, 570.0534700005774, 672.5191843793441, 612.0926447539816, 648.200389697204, 142.2364639544272, 161.9247382166087, 113.20753470333459, 621.2253512940177, 619.216274641714, 625.7563832803341, 654.2065327207695, 686.7492705854188, 642.7843005030186, 647.1653136332811, 679.7999292567264, 682.1095996464717, 643.4004907582216, 690.7915129402044, 647.1732024790027, 688.9789743085967, 692.6914920524767, 692.8723135494984, 631.2289824151848, 693.78120213977, 706.6615675614946, 705.1471719544971, 147.39626366449033, 639.7056167082367, 642.3737939897615, 696.311943998657, 648.525837757476, 646.787746719899, 681.67032297723, 634.9285500114194, 652.3569810507352, 642.7565855816288, 694.7689065789408, 647.4064660541922, 656.0393913295899, 691.6152615909515, 648.8096253653197, 676.8711757072325, 626.5313496824554, 655.0453949759664, 657.6269882884134, 651.8380165657195, 643.9486746401607, 636.642278207806, 644.3705517026141, 693.1855987158741, 650.8112227959599, 655.6366453016685, 659.9663384154657, 696.4296242950733, 636.5307964612653, 637.1479572665392, 643.7087116359625, 685.1231384345897, 646.3506429567886, 624.7134364586719, 635.920663848998, 665.2503721864805, 628.4607345512941, 625.2552137354465, 605.7733935581437, 624.2551457986276, 628.7814387910907, 668.2755738213398, 622.0029453440264, 66.5941519808459, 181.0276692850393, 620.3193495531516, 616.3375180559707, 644.1021641862844, 602.5105328356376, 649.7613016477624, 654.9419759921504, 627.401467434505, 621.0171958287109, 599.7435746592507, 610.9419761775057, 606.2290756070576, 610.3511898365815, 23.327144008972997, 409.39161754733897, 590.0785069857908, 598.0718237084828, 637.6038172683379, 556.1507569049306, 633.8781835855917, 652.5846268369168, 628.6537688850353, 636.136073460624, 674.7124060881096, 678.8979045279435, 628.3296577258781, 634.8165387619719, 634.2148754802556, 631.791280761228, 595.1816424860671, 594.9038307731064, 597.5106752956491, 595.6706515441732, 625.8748119240494, 633.4845887818412, 672.7601152953165, 635.8845341824939, 678.0412186571707, 625.6612937390181, 637.8577560583991, 671.9871506763761, 674.7767121639985, 636.754794091279, 675.8348952852884, 674.1376068246008, 640.8391061547488, 637.3922628670229]
Elapsed: 0.14739550696911086~0.29793444126177027
Time per graph: 0.0030080715707981806~0.006080294719627965
Speed: 647.9126880187132~208.27772080377724
Total Time: 0.0794
best val loss: 0.31090039014816284 test_score: 1.0000

Testing...
Test loss: 0.6218 score: 0.9592 time: 0.07s
test Score 0.9592
Epoch Time List: [1.9869935419410467, 4.054680104018189, 0.3512155970092863, 0.32481396314688027, 0.3286691779503599, 0.3318680408410728, 0.32556348701473325, 0.3353743690531701, 0.34137710800860077, 0.3359849948901683, 0.3318568659015, 0.3417895151069388, 0.33118316100444645, 0.35235156607814133, 0.32939576217904687, 0.3568011570023373, 0.35523426497820765, 0.37604613904841244, 0.34999916900414973, 0.39223748084623367, 2.884609259897843, 3.0806974879233167, 1.9325210060924292, 0.6661368969362229, 0.3800672080833465, 0.34027195000089705, 0.3383539420319721, 0.3329651530366391, 0.33185928815510124, 0.338839522912167, 0.3347344050416723, 0.33801154501270503, 0.3470402820967138, 0.3616360240848735, 1.6447780369780958, 3.369594909832813, 2.303258904023096, 1.012641201959923, 0.37734368396922946, 0.34308771905489266, 0.3445110279135406, 0.34175825805868953, 0.33648258983157575, 0.33841254806611687, 0.33936398394871503, 2.12263453588821, 3.0764662319561467, 0.4508116259239614, 0.35516314895357937, 0.3524733129888773, 0.34151945204939693, 0.3300332190701738, 0.32832221896387637, 0.33335594297386706, 0.33515326492488384, 0.342847750056535, 0.43656122812535614, 0.3508594479644671, 0.35152704280335456, 3.895825411193073, 3.150547464028932, 0.3482093879720196, 0.32508996198885143, 0.3252483159303665, 0.33912791789043695, 0.3310684320749715, 0.33289744216017425, 0.3267461770446971, 4.745232988963835, 1.6867911200970411, 0.35971221898216754, 0.3319794898852706, 0.3139907759614289, 0.31921385193709284, 0.3138297600671649, 0.31555359612684697, 0.31658428895752877, 0.3239901940105483, 0.32125419296789914, 0.34174166107550263, 2.1870635750237852, 5.680947924847715, 0.9235442630015314, 0.6113212950294837, 0.3377975800540298, 0.332190784974955, 0.31913594191428274, 0.32520267192739993, 0.3494842710206285, 0.3426810910459608, 0.3135987528366968, 1.0633706880034879, 6.017131041968241, 0.6037950499448925, 0.34147460595704615, 0.32114708004519343, 0.33681063109543175, 0.3261419930495322, 0.31624499696772546, 0.3169881399953738, 0.3349324209848419, 0.336848090053536, 0.3326789540005848, 0.3272469249786809, 2.8716370910406113, 2.1238537849858403, 0.720138965989463, 0.3208837799029425, 0.3145404190290719, 0.3166577339870855, 0.320043065934442, 0.31203744490630925, 0.3116926901275292, 0.3181670579360798, 0.3166189248440787, 0.32379966508597136, 0.32173960108775645, 2.7447007560404018, 3.5546221969416365, 2.412980506196618, 0.33462593401782215, 0.3249718869337812, 0.3179599040886387, 0.3461743200896308, 0.33239010302349925, 2.340799023048021, 6.266156640951522, 0.37086032691877335, 0.3168076389702037, 0.3224344099871814, 0.32199554692488164, 0.32758491090498865, 0.31873609288595617, 0.31879026303067803, 6.3575911150546744, 3.3295092850457877, 1.0438960848841816, 0.34500911890063435, 0.3446670949924737, 0.3435581000521779, 0.3409214620478451, 0.34457146911881864, 0.3424563800217584, 2.7111008579377085, 3.081267736153677, 2.616064192028716, 1.0002684469800442, 0.35463874705601484, 0.3485278739826754, 0.3483607629314065, 0.3580932910554111, 0.37738353991881013, 0.35160409309901297, 0.32170978107023984, 2.644357603043318, 2.6480825021862984, 1.823957686079666, 0.3170012349728495, 0.31778263789601624, 0.3179837348870933, 0.31744955689646304, 0.3124219449236989, 0.3054600750328973, 0.309437278076075, 0.3094345841091126, 0.31088599981740117, 0.30627513001672924, 0.3067826629849151, 0.30511809804011136, 0.30653616809286177, 0.306431423057802, 0.32225059799384326, 0.31448959303088486, 0.3062993149505928, 0.307303695124574, 0.30671644606627524, 0.3074639029800892, 0.3073584239464253, 0.30717267701402307, 0.30832990794442594, 0.30810924794059247, 0.31474190310109407, 0.3075207289075479, 0.3079937739530578, 0.30856328306254, 0.3055100010242313, 0.3064292680937797, 0.30912443494889885, 0.30856312811374664, 0.3131469418294728, 0.30632207391317934, 0.3079114140709862, 0.3068085570121184, 0.3077125329291448, 0.3081315979361534, 0.3093003421090543, 0.32886429398786277, 0.32525431201793253, 0.3346061479533091, 0.33912500797305256, 0.3331078678602353, 5.217154128011316, 3.422703202930279, 0.31686376314610243, 0.3581425059819594, 0.31432787608355284, 0.3185947729507461, 0.3174780890112743, 0.3150269619654864, 0.3173335319152102, 0.3128476190613583, 0.3151279218727723, 2.441614740062505, 3.667576527921483, 1.7318520640255883, 0.8733556040097028, 0.3348576759453863, 0.33232256409246475, 0.3331166470889002, 0.33542429096996784, 0.33122059295419604, 0.3296388591406867, 0.3331540538929403, 0.33419005188625306, 0.3321467631030828, 2.1675933180376887, 3.001053098938428, 0.6521856199251488, 0.33910576708149165, 0.3388834389625117, 0.335367688909173, 0.33955472195520997, 0.3329029369633645, 0.33393122092820704, 0.3336137079168111, 0.3344751870026812, 0.33609154890291393, 0.3349373679375276, 0.3362290539080277, 0.3444046980002895, 5.7869938230142, 1.4801476938882843, 0.48073305597063154, 0.5365775910904631, 0.33313714095856994, 0.32844940095674247, 0.3278804109431803, 0.32808308699168265, 0.33297866908833385, 0.32822780904825777, 0.3158195719588548, 0.3190710488706827, 0.3240110161714256, 0.32095225690864027, 0.3339336421340704, 0.3453233679756522, 0.34476580400951207, 4.7624331309925765, 2.077444403897971, 0.7340354569023475, 0.3563681810628623, 0.3532409109175205, 0.3306945310905576, 0.34358833893202245, 0.3323914440115914, 0.32121908105909824, 0.9348819570150226, 4.593591123120859, 0.33544867706950754, 0.3255298479925841, 0.41009755386039615, 0.3175909760175273, 0.31667433585971594, 2.6564435800537467, 5.2622016940731555, 1.4584333690581843, 0.9133244019467384, 0.3431696919724345, 0.33031751320231706, 0.33012127003166825, 0.333982671960257, 0.32729973597452044, 0.326534082996659, 0.3232978949090466, 0.321721309912391, 0.33846349793020636, 0.3237025741254911, 0.32664125703740865, 0.32135380001273006, 0.3221655760426074, 0.3299883770523593, 0.321054870961234, 0.32748371292836964, 0.33549811504781246, 4.672492607147433, 2.706216943101026, 0.39688792498782277, 0.3436287899967283, 0.3562678409507498, 0.3320559229468927, 0.3290527840144932, 0.3295950230676681, 0.33415364497341216, 0.33444184192921966, 0.3272205670364201, 0.32332348404452205, 0.32411842595320195, 0.33035187504719943, 0.32682202896103263, 0.3246023061219603, 0.32454351091291755, 0.33072601701132953, 0.3253246389795095, 0.3259539850987494, 0.3231784579111263, 0.32772349496372044, 0.3349113971926272, 2.4756317770807073, 4.588531749206595, 0.9906137288780883, 1.5237406060332432, 0.5118806288810447, 0.3129608938470483, 0.3099137820536271, 0.30796463694423437, 0.30500435910653323, 0.3093572029611096, 0.32794641400687397, 0.31762038299348205, 0.3831625909078866, 3.493624092079699, 4.288732627872378, 1.1601292027626187, 0.5199973558774218, 0.3352043479681015, 0.3213305090321228, 0.3329292320413515, 0.3497304110787809, 0.33737566298805177, 0.3404664139961824, 0.33423883002251387, 0.3357547289924696, 0.337240006076172, 5.858254222897813, 1.9629653780721128, 0.36448379303328693, 0.3269275319762528, 0.32349441619589925, 0.32566077809315175, 0.3232417751569301, 0.31717682315502316, 0.3149834780488163, 0.34059020096901804, 0.3181562020909041, 0.3135046809911728, 0.31593067198991776, 0.31240884494036436, 0.31056611402891576, 1.1183270439505577, 4.235970305046067, 2.367965108016506, 1.7671166659565642, 1.9044755639042705, 0.5932571248849854, 0.35070624004583806, 0.34450543904677033, 0.3331119370413944, 0.3319779341109097, 0.32835725299082696, 0.3239322700537741, 0.3381256520515308, 4.809318824089132, 3.160602799966, 1.9528573889983818, 0.49242012994363904, 0.3296959839062765, 0.330072304001078, 0.3336499738506973, 0.3360977699048817, 0.33594637911301106, 0.3348318039206788, 0.3302526921033859, 0.32744630391243845, 0.3306283139390871, 0.33436515112407506, 0.33723521302454174, 0.33186471182852983, 0.3364420550642535, 0.33469079399947077, 7.439309622975998, 2.8608708491083235, 0.7465043219272047, 0.3498115101829171, 0.34960370894987136, 0.3364332920173183, 0.3106542769819498, 0.3204797930084169, 0.32123335008509457, 0.31515936902724206, 1.063678971142508, 6.705888824886642, 0.44589233794249594, 0.3217141979839653, 0.33172117976937443, 0.32726495305541903, 0.39630834199488163, 0.33969609800260514, 4.445263289031573, 1.1472845169482753, 0.8929339059395716, 0.34471687593031675, 0.3450969198020175, 0.34083500609267503, 0.34301378298550844, 0.3396534320199862, 0.3391988640651107, 0.3402927800780162, 0.3437992340186611, 0.33484994899481535, 0.3353179480182007, 2.504654165939428, 3.1126011739252135, 0.9305133830057457, 0.329311657929793, 0.32601689093280584, 0.3082880770089105, 0.3097740029916167, 0.3139313260326162, 0.3103006340097636, 0.3079869420034811, 0.32295203395187855, 0.3201042120344937, 0.31785473111085594, 0.3196048450190574, 0.3267858889885247, 0.329340577009134, 0.33583879203069955, 0.3267793031409383, 0.32203857507556677, 0.3233777058776468, 0.512633147998713, 3.9699682589853182, 0.7220524821896106, 0.3557085449574515, 0.3172227619215846, 0.3116626428673044, 0.3168121710186824, 0.32263528706971556, 0.32785521796904504, 0.32874286104924977, 0.33167116495314986, 0.3335980469128117, 0.33184916607569903, 0.328518892172724, 2.602584370994009, 5.25912816694472, 0.47897268598899245, 0.34951771213673055, 0.3452140368754044, 0.33653178601525724, 0.3217824990861118, 0.32185517996549606, 0.32580967410467565, 0.3183688768185675, 0.32103352108970284, 0.32364923995919526, 0.32098526996560395, 0.33387082116678357, 0.3258235150715336, 6.013799128937535, 2.7661419381620362, 1.0766604531090707, 0.7213211118942127, 2.8941300489241257, 0.7205591569654644, 0.3413990889675915, 0.34198462101630867, 0.3405541640240699, 0.314467801945284, 0.37756254395935684, 0.3293691851431504, 0.3241169760003686, 3.293628849904053, 2.670094058965333, 1.2376392129808664, 0.6491366070695221, 0.5309542840113863, 0.33328498306218535, 0.333496511913836, 0.3285363840404898, 0.3290910959476605, 0.32980400684755296, 0.3269732450135052, 0.3272924260236323, 0.3266902460018173, 3.7875749769154936, 1.9765082550002262, 1.0788543638773263, 0.3213546648621559, 0.3331515190657228, 0.32780369697138667, 0.3183441059663892, 0.31235550506971776, 0.31917143100872636, 0.3163502630777657, 0.31522432703059167, 0.3134144330397248, 0.3107123860390857, 0.3142133679939434, 0.3085497940191999, 0.3125935409916565, 0.31018819008022547, 0.31448534212540835, 0.30974081705790013, 0.3084546298487112, 0.3075312030268833, 0.30616728495806456, 0.3137599481269717, 0.3147684510331601, 0.31977374106645584, 0.3168855680851266, 2.4231920088641346, 6.984865373116918, 0.6718552729580551, 0.46117410296574235, 0.33809371304232627, 0.32190796185750514, 0.3156543489312753, 0.3213746679248288, 0.3135985300177708, 0.31578860396984965, 0.31871780392248183, 3.7311518740607426, 0.6597326329210773, 0.3183055139379576, 0.31328445894178003, 0.3136707858648151, 0.31891025917138904, 0.32818404713179916, 0.3314750362187624, 0.34897219692356884, 0.34309965604916215, 0.3487658069934696, 0.3461848860606551, 0.34946274186950177, 0.3473400310613215, 0.5689590031979606, 3.542123956955038, 0.33593405003193766, 0.3331609619781375, 0.3248737780377269, 0.326241266913712, 0.32606759294867516, 0.3263019820442423, 0.33794820797629654, 0.3296489219646901, 0.32498147105798125, 0.32949609588831663, 0.3168411578517407, 0.3068724221084267, 0.3462334548821673, 0.3103617139859125, 0.3094456698745489, 0.3128171149874106, 2.3870632760226727, 6.017244859132916, 0.6873137339716777, 0.3129660259000957, 0.3090275120921433, 0.3459525549551472, 0.338432333082892, 0.31213094503618777, 0.31354436790570617, 0.3101903371280059, 0.31519103795289993, 0.3206463659880683, 4.056279702926986, 0.3470296000596136, 0.3386880599427968, 0.3458363770041615, 0.3345639209728688, 0.3390996130183339, 0.33827493200078607, 5.285032681887969, 1.393113799043931, 1.374486296903342, 0.4165407521650195, 0.31530171795748174, 0.31579396198503673, 0.3172663009027019, 0.32391817797906697, 0.33488368312828243, 0.3250761158997193, 0.3189348300220445, 0.3131969489622861, 0.3226904020411894, 0.3274328629486263, 0.3508637360064313, 0.34013893199153244, 0.3500736600253731, 0.34103977389167994, 0.35027549508959055, 0.34353615483269095, 0.34186526003759354, 0.3414436799939722, 0.3468203628435731, 0.34977560001425445, 0.34440607496071607, 0.3423707321053371, 0.3417592791374773, 0.3445435839239508, 0.3435964439995587, 0.3471367888851091, 0.3427453129552305, 4.602383463992737, 3.3885515440488234, 0.4782183449715376, 0.3483280761865899, 0.3546119489474222, 0.3463228930486366, 0.3368508570129052, 0.32513448200188577, 2.7959356530336663, 5.48485448397696, 0.602281698025763, 0.3180504138581455, 0.31243672606069595, 0.3170482990099117, 0.31260539405047894, 0.3116736519150436, 0.3141347218770534, 0.31366028101183474, 0.31630541291087866, 0.3159239920787513, 0.3187343979952857, 0.35333704203367233, 0.39010193198919296, 0.3420872238930315, 0.33026785706169903, 0.33301135897636414, 3.2441820120438933, 3.8937837909907103, 0.3275514249689877, 0.31660895701497793, 0.3223344668513164, 0.31642841279972345, 0.32903717702720314, 0.32151768915355206, 4.2478884210577235, 8.317047677002847, 4.70242375601083, 0.32268776604905725, 0.3207979629514739, 0.31819445092696697, 0.32605824596248567, 0.3328151779714972, 0.3164772081654519, 0.3161002459237352, 0.32605727203190327, 0.34467448899522424, 0.3449656961020082, 0.3398275909712538, 0.3470257659209892, 0.34459223388694227, 0.34254280501045287, 0.46876175014767796, 6.480008686077781, 5.17918871389702, 0.8364596180617809, 0.3582692409399897, 0.35435418994165957, 0.3265419719973579, 0.311670582043007, 0.31393740698695183, 0.3285835748538375, 0.3347061360254884, 0.331788980984129, 0.3323637949069962, 0.3333905440522358, 0.3350552619667724, 0.33177829696796834, 2.5531657451065257, 2.5638864981010556, 1.1462689199252054, 1.0958135117543861, 0.33972309390082955, 0.33735545887611806, 0.3352309539914131, 0.3381730748806149, 0.3218234081286937, 0.31863078312017024, 0.3264771798858419, 0.32353332999628037, 0.4145921579329297, 0.3167453099740669, 0.31775234395172447, 0.32070689590182155, 0.3216950960922986, 0.3636241320054978, 0.3196869910461828, 0.32323254796210676, 2.4282772758742794, 5.367974494001828, 0.34611645992845297, 0.33790738496463746, 0.3388454089872539, 0.3218377260491252, 0.3169944480760023, 0.3148530359612778, 0.3147561280056834, 0.3129591761389747, 6.927313965978101, 2.3207666348898783, 3.8062106219585985, 0.6021206928417087, 0.33448707195930183, 0.3410852230153978, 0.33915351203177124, 0.3346021840116009, 0.33967850101180375, 2.608522122958675, 1.4512584330514073, 0.9798279389506206, 0.35317747201770544, 0.34642065106891096, 0.34903497400227934, 0.3533835189882666, 0.3398019728483632, 0.3272724120179191, 0.32151191704906523, 0.3225836631609127, 0.3205480579053983, 0.3216237031156197, 0.3207086119800806, 0.32068869506474584, 0.32160161598585546, 0.3223118409514427, 0.3207367529394105, 0.32183478178922087, 0.32090383290778846, 0.3278535029385239, 0.32536092784721404, 0.32435960101429373, 0.32034416717942804, 0.32245274097658694, 0.32382250286173075, 0.3192845791345462, 0.3212086741114035, 0.32732841093093157, 0.32858862704597414, 0.33734328707214445, 0.34489868592936546, 0.3329072672640905, 0.3405281319282949, 0.34620153391733766, 0.3387967150192708, 0.32749378494918346, 0.34720788698177785, 0.3408029678976163, 0.33822183101437986, 0.33354181807953864, 4.8824094800511375, 0.7594610379310325, 0.3355705259600654, 0.3299853791249916, 0.3496182329254225, 0.33436374098528177, 0.3287274860776961, 0.35644284810405225, 0.3524631450418383, 0.3624053519451991, 0.3549787359079346, 0.3600848140195012, 0.3399945331038907, 0.33280028693843633, 0.33896659209858626, 0.33316927100531757, 0.3453493259148672, 0.4195129689760506, 0.33409485314041376, 0.33575849409680814, 0.34352057485375553, 0.3487876480212435, 0.4342731209471822, 0.3336065079784021, 0.35087865497916937, 0.37092305696569383, 0.3329477831721306, 0.35584633506368846, 0.33361959201283753, 0.34153740003239363, 0.3442562879063189, 3.341617160127498, 2.270852254005149, 0.5751422819448635, 0.35734265809878707, 0.35307041101623327, 0.35229468590114266, 0.3540709401713684, 0.34282311715651304, 0.33394415699876845, 0.32255979301407933, 0.3352182990638539, 0.32149105495773256, 0.3319831689586863, 0.3417165910359472, 0.3245359880384058, 0.3248132619773969, 0.35288646013941616, 0.3403166518546641, 0.3315340200206265, 0.3234289848478511, 0.322518861037679, 0.3323169680079445, 0.32733960496261716, 6.672770249075256, 0.9946741759777069, 0.3434033340308815, 0.3453695150092244, 0.3342571940738708, 0.33344233606476337, 0.33929745899513364, 0.3254821060691029, 0.3328088790876791, 0.3343915978912264, 0.3358261149842292, 0.33919273992069066, 4.64970430592075, 0.9017557050101459, 0.31408255698625, 0.31674134195782244, 0.3260257469955832, 0.32105696690268815, 0.32350182195659727, 0.32896883809007704, 0.3111276251729578, 0.327983764000237, 0.328047810937278, 0.31935377803165466, 0.32308852893766016, 0.3267613409552723, 0.32461403403431177, 0.3212887200061232, 0.3331846441142261, 0.32324059191159904, 0.3288560489891097, 0.31990717607550323, 0.317048077005893, 0.3150223098928109, 0.33807803387753665, 0.33410556986927986, 0.3191786501556635, 0.3294690758921206, 0.3347118600504473, 0.33125416503753513, 0.3497909320285544, 0.354859406943433, 0.3446288959821686, 0.35197804088238627, 0.35187719704117626, 0.3506633900105953, 0.3485729700187221, 0.3423393451375887, 0.346928600105457, 0.35560927307233214, 0.3523607541574165, 0.3605979969725013, 0.364406322943978, 0.35945345100481063, 0.33923743409104645, 0.33224932313896716, 0.34976407303474844, 0.34286749898456037, 0.3228933709906414, 0.32628911698702723, 0.33360885397996753, 0.35621830890886486, 0.34216801915317774, 0.331882705911994, 0.3528786120004952, 0.33403615991119295, 0.3525119940750301, 0.34672515210695565, 0.3394758690847084, 0.3568784228991717, 0.3352527270326391, 0.34670435404405, 2.9068007910391316, 6.109856416936964, 0.36436453997157514, 0.3827221500687301, 0.34036997403018177, 0.34019888390321285, 0.3414728600764647, 0.3405142780393362, 0.41802075295709074, 0.3380273060174659, 0.35192825889680535, 0.34514522878453135, 0.3359995710197836, 0.338010803097859, 0.34524994995445013, 0.3382305040722713, 0.3477003900334239, 1.7332963309017941, 6.700857111951336, 2.7668007239699364, 0.35138492891564965, 0.3345605139620602, 0.3528542109997943, 0.33898923196829855, 0.33453722309786826, 0.3519293899880722, 0.3329720499459654, 0.33299070002976805, 0.3338554910151288, 0.32664830901194364, 0.38252905511762947, 0.34742082410957664, 0.3435438929591328, 0.3326773529406637, 4.916025866987184, 1.7024786599213257, 1.343027553986758, 0.4585350980050862, 0.35132071806583554, 0.3526739680673927, 0.3496097599854693, 0.33700723689980805, 0.34865130798425525, 0.33393999689724296, 0.33593336306512356, 0.3453863550676033, 0.3367677719797939, 0.3305356941418722, 0.3414442519424483, 0.3329880378441885, 0.3325240359408781, 0.3253429619362578, 0.3488806380191818, 0.3362262628506869, 0.3354366699932143, 0.3304235889809206, 4.824711579014547, 0.42598160612396896, 0.33528572507202625, 0.3377020718762651, 0.3364171989960596, 0.33819224406033754, 0.3318297481164336, 0.33944719401188195, 0.3242221559630707, 0.33291411492973566, 0.3348482829751447, 0.33930316800251603, 0.32880594104062766, 0.3207556229317561, 0.33587818196974695, 0.3280712260166183, 0.34041497204452753, 0.33404559502378106, 0.33557505207136273, 0.3273042062064633, 0.3376535839634016, 0.3416641710791737, 0.3367329438915476, 0.3375199328875169, 0.3363256258890033, 0.33485957398079336, 0.32792222406715155, 0.33095979201607406, 0.342177381971851, 0.3355930781690404, 0.34113942098338157, 0.3399563949787989, 0.35154887405224144, 0.3473416368942708, 0.34809163911268115, 0.336374006816186, 0.35123608505818993, 0.35085445386357605, 0.3636886429740116, 0.35728185705374926, 0.3440345859853551, 0.3412594071123749, 0.3613069709390402, 4.688758150092326, 1.4943051749141887, 0.3616124619729817, 0.40191269305069, 0.3579738310072571, 0.36839943507220596, 0.3622546070255339, 0.3634813450044021, 0.3653947140555829, 0.355837486917153, 0.35342681396286935, 0.357364634051919, 0.3549605649895966, 0.36497532902285457, 4.668545132968575, 4.284861559048295, 0.37366932502482086, 0.3671897890744731, 0.36507744702976197, 0.37016059714369476, 0.3459952468983829, 0.3357493708608672, 0.3393551552435383, 0.3437807329464704, 0.3358190511353314, 0.3401236170902848, 0.3387375519378111, 0.34289564099162817, 0.3640266091097146, 0.3706964689772576, 0.3794508029241115, 0.3622921959031373, 0.36727362300734967, 0.36835754790809005, 0.353191732079722, 0.34045767900533974, 0.3352574669988826, 0.34502725896891207, 0.34106306300964206, 0.35634481196757406, 0.33991391595918685, 0.3380327189806849, 0.34617722092662007, 0.34496963198762387, 0.34535278601106256, 0.34528264007531106, 0.3431900420691818, 0.33179687603842467]
Total Epoch List: [716, 318]
Total Time List: [0.06605594896245748, 0.07941902708262205]
========================k_idx:2========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: AEtransGAT
AEtransGAT(
  (gae): GAE(
    (encoder): transGAT(
      (encoder): EdgeFeatureGAT()
      (classifier): Sequential(
        (0): Linear(in_features=64, out_features=64, bias=True)
        (1): ReLU(inplace=True)
        (2): Linear(in_features=64, out_features=2, bias=True)
      )
    )
    (decoder): InnerProductDecoder()
  )
)
Total number of parameters: 1914242
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x75feb28b02b0>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 2.5623;  Loss pred: 2.5623; Loss self: 0.0000; time: 1.77s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6965 score: 0.4898 time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6953 score: 0.5000 time: 0.46s
Epoch 2/1000, LR 0.000000
Train loss: 2.5620;  Loss pred: 2.5620; Loss self: 0.0000; time: 1.81s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6964 score: 0.4898 time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6952 score: 0.5000 time: 0.28s
Epoch 3/1000, LR 0.000030
Train loss: 2.5371;  Loss pred: 2.5371; Loss self: 0.0000; time: 1.61s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6962 score: 0.4898 time: 0.32s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6951 score: 0.5000 time: 0.32s
Epoch 4/1000, LR 0.000060
Train loss: 2.5474;  Loss pred: 2.5474; Loss self: 0.0000; time: 0.84s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6960 score: 0.4898 time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6950 score: 0.5000 time: 0.54s
Epoch 5/1000, LR 0.000090
Train loss: 2.5393;  Loss pred: 2.5393; Loss self: 0.0000; time: 1.55s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6957 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6948 score: 0.5000 time: 0.07s
Epoch 6/1000, LR 0.000120
Train loss: 2.5215;  Loss pred: 2.5215; Loss self: 0.0000; time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6954 score: 0.4898 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6946 score: 0.5000 time: 0.06s
Epoch 7/1000, LR 0.000150
Train loss: 2.4958;  Loss pred: 2.4958; Loss self: 0.0000; time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6951 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6943 score: 0.5000 time: 0.06s
Epoch 8/1000, LR 0.000180
Train loss: 2.4478;  Loss pred: 2.4478; Loss self: 0.0000; time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6947 score: 0.4898 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6941 score: 0.5000 time: 0.06s
Epoch 9/1000, LR 0.000210
Train loss: 2.3993;  Loss pred: 2.3993; Loss self: 0.0000; time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6944 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6939 score: 0.5000 time: 0.07s
Epoch 10/1000, LR 0.000240
Train loss: 2.3964;  Loss pred: 2.3964; Loss self: 0.0000; time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6941 score: 0.4898 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6937 score: 0.5000 time: 0.06s
Epoch 11/1000, LR 0.000270
Train loss: 2.3688;  Loss pred: 2.3688; Loss self: 0.0000; time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6938 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6935 score: 0.5000 time: 0.06s
Epoch 12/1000, LR 0.000270
Train loss: 2.3173;  Loss pred: 2.3173; Loss self: 0.0000; time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6936 score: 0.4898 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6934 score: 0.5000 time: 0.06s
Epoch 13/1000, LR 0.000270
Train loss: 2.2644;  Loss pred: 2.2644; Loss self: 0.0000; time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6934 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6932 score: 0.5000 time: 0.07s
Epoch 14/1000, LR 0.000270
Train loss: 2.2346;  Loss pred: 2.2346; Loss self: 0.0000; time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6932 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6931 score: 0.5000 time: 0.07s
Epoch 15/1000, LR 0.000270
Train loss: 2.2008;  Loss pred: 2.2008; Loss self: 0.0000; time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6932 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6930 score: 0.5000 time: 0.07s
Epoch 16/1000, LR 0.000270
Train loss: 2.1823;  Loss pred: 2.1823; Loss self: 0.0000; time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6931 score: 0.4898 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6930 score: 0.5000 time: 0.07s
Epoch 17/1000, LR 0.000270
Train loss: 2.1100;  Loss pred: 2.1100; Loss self: 0.0000; time: 3.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6931 score: 0.4898 time: 1.32s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6929 score: 0.5000 time: 0.96s
Epoch 18/1000, LR 0.000270
Train loss: 2.1044;  Loss pred: 2.1044; Loss self: 0.0000; time: 1.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6931 score: 0.4898 time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6929 score: 0.5000 time: 0.25s
Epoch 19/1000, LR 0.000270
Train loss: 2.0692;  Loss pred: 2.0692; Loss self: 0.0000; time: 1.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6930 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6928 score: 0.5000 time: 0.06s
Epoch 20/1000, LR 0.000270
Train loss: 2.0257;  Loss pred: 2.0257; Loss self: 0.0000; time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6930 score: 0.4898 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6927 score: 0.5000 time: 0.07s
Epoch 21/1000, LR 0.000270
Train loss: 1.9888;  Loss pred: 1.9888; Loss self: 0.0000; time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6930 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6927 score: 0.5000 time: 0.07s
Epoch 22/1000, LR 0.000270
Train loss: 1.9617;  Loss pred: 1.9617; Loss self: 0.0000; time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6930 score: 0.4898 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6926 score: 0.5000 time: 0.06s
     INFO: Early stopping counter 1 of 2
Epoch 23/1000, LR 0.000270
Train loss: 1.9267;  Loss pred: 1.9267; Loss self: 0.0000; time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6930 score: 0.4898 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6926 score: 0.5000 time: 0.07s
     INFO: Early stopping counter 2 of 2
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 020,   Train_Loss: 1.9888,   Val_Loss: 0.6930,   Val_Precision: 0.0000,   Val_Recall: 0.0000,   Val_accuracy: 0.0000,   Val_Score: 0.4898,   Val_Loss: 0.6930,   Test_Precision: 0.0000,   Test_Recall: 0.0000,   Test_accuracy: 0.0000,   Test_Score: 0.5000,   Test_loss: 0.6927


[1.1250675929477438, 0.07260138390120119, 0.06517546193208545, 0.06942169705871493, 0.06307711405679584, 0.06370937905739993, 0.06353496201336384, 0.06884501304011792, 0.0664323780220002, 0.06322127894964069, 0.06440213706810027, 0.0631538750603795, 0.06382796703837812, 0.06595662399195135, 0.0696884390199557, 0.06860149395652115, 0.0696936259046197, 0.07671907800249755, 0.06994366110302508, 0.08053676399867982, 1.5170276639983058, 0.3635215579997748, 0.19728804100304842, 0.09975753107573837, 0.06620932999067008, 0.06520154199097306, 0.06493534008041024, 0.06481671601068228, 0.06553994701243937, 0.06519350199960172, 0.06751241302117705, 0.06698870600666851, 0.07034255308099091, 0.06904265598859638, 0.7079291759291664, 0.29737150797154754, 0.29062273900490254, 0.10017506603617221, 0.06606452004052699, 0.06941800401546061, 0.06561793701257557, 0.06805465393699706, 0.06500946800224483, 0.06617417791858315, 0.06593629496637732, 0.9478009770391509, 0.12139186600688845, 0.07250222493894398, 0.06941007892601192, 0.0684408110100776, 0.06417788797989488, 0.06419697601813823, 0.06421515892725438, 0.06461732892785221, 0.06484952592290938, 0.06799174600746483, 0.1487254409585148, 0.06940725794993341, 0.06884129496756941, 2.1710067989770323, 0.074032180942595, 0.07430030300747603, 0.0638693260261789, 0.06624759593978524, 0.06461389001924545, 0.06443172099534422, 0.06254479999188334, 0.06303938594646752, 1.4150545729789883, 0.07174125104211271, 0.07600608200300485, 0.06253405101597309, 0.061866942909546196, 0.06246624398045242, 0.06197469704784453, 0.06184293807018548, 0.061756442999467254, 0.06228293105959892, 0.06297286006156355, 0.07575407798867673, 1.3042752699693665, 0.6072317759972066, 0.3420649969484657, 0.06774329300969839, 0.06546581897418946, 0.06285132595803589, 0.06252083706203848, 0.06223062006756663, 0.07395434298086911, 0.061848468030802906, 0.06118969200178981, 0.8000162750249729, 0.4628510350594297, 0.15736174397170544, 0.06282533193007112, 0.06248521606903523, 0.08036140399053693, 0.06135073699988425, 0.06133868300821632, 0.06053328700363636, 0.06225241406355053, 0.0741469879867509, 0.07334171095862985, 0.06444651598576456, 0.31430238694883883, 0.2978845089673996, 0.06520012801047415, 0.06237223604694009, 0.06152717105578631, 0.06212544091977179, 0.06123218103311956, 0.06145436700899154, 0.06125844200141728, 0.06195212004240602, 0.06192778202239424, 0.062011402915231884, 0.06177784607280046, 1.0857283340301365, 0.6311360759427771, 0.06474093592260033, 0.06514206901192665, 0.06190473202150315, 0.06076760496944189, 0.06565935001708567, 0.06444231909699738, 2.058738171006553, 0.08766032301355153, 0.062314005102962255, 0.06163706898223609, 0.06266388692893088, 0.062070864951238036, 0.06268804601859301, 0.06217115093022585, 0.06176924810279161, 0.43242676998488605, 0.8568806980038062, 0.06983314291574061, 0.0669098700163886, 0.0668419529683888, 0.06791843893006444, 0.0666529139271006, 0.06768140709027648, 0.0675741529557854, 0.6992912340210751, 0.9380899249808863, 0.30850372300483286, 0.06733186496421695, 0.06785435602068901, 0.07248535007238388, 0.06836368003860116, 0.06794471910689026, 0.07666253496427089, 0.06402595504187047, 0.06210166204255074, 0.6722918150480837, 0.47543843195308, 0.061514504021033645, 0.06054721400141716, 0.06924343691207469, 0.06474179995711893, 0.061351225012913346, 0.05998940800782293, 0.06007451505865902, 0.06028251093812287, 0.0598492439603433, 0.060017738956958055, 0.05985578789841384, 0.06021264009177685, 0.05970510595943779, 0.06027904199436307, 0.05982470000162721, 0.06543243792839348, 0.05934077105484903, 0.05994168506003916, 0.05984782602172345, 0.05864717997610569, 0.05976193107198924, 0.06012161704711616, 0.05980157200247049, 0.06084371905308217, 0.059946853085421026, 0.0603357549989596, 0.06042889796663076, 0.06412903207819909, 0.06010557699482888, 0.06000156106892973, 0.05960321601014584, 0.05996228288859129, 0.06015214091166854, 0.06013369397260249, 0.05975861893966794, 0.060398735920898616, 0.05954533303156495, 0.05992923700250685, 0.059819236979819834, 0.06108355394098908, 0.06309165491256863, 0.06721752695739269, 0.06566394097171724, 0.06768737197853625, 0.06523262697737664, 0.6982098269509152, 0.06177571404259652, 0.061719611985608935, 0.0629792510299012, 0.06152171909343451, 0.0623580280225724, 0.06233565404545516, 0.06348260689992458, 0.06242344295606017, 0.061633193981833756, 0.061667119967751205, 2.172633665963076, 0.4913909339811653, 0.23835222399793565, 0.06477198901120573, 0.06548859097529203, 0.06485605600755662, 0.06487756606657058, 0.06627889804076403, 0.06506447598803788, 0.06486578902695328, 0.06469597294926643, 0.06977198901586235, 0.06554349593352526, 0.49097754491958767, 0.2763960459269583, 0.06564669706858695, 0.06570510508026928, 0.06604870303999633, 0.06545242096763104, 0.06647082604467869, 0.06586214096751064, 0.06563952309079468, 0.06572110892739147, 0.0669107869034633, 0.06583865790162235, 0.06611254892777652, 0.06595169100910425, 0.07578426296822727, 0.20499895501416177, 0.3951002210378647, 0.13119681098032743, 0.06408304907381535, 0.06463857798371464, 0.06356241495814174, 0.06383267894852906, 0.06361543294042349, 0.0633556901011616, 0.06642631895374507, 0.061881675967015326, 0.06180676491931081, 0.062185003072954714, 0.06193888501729816, 0.06811047298833728, 0.06703020399436355, 0.06900497199967504, 0.37440110894385725, 0.18271234398707747, 0.07257488591130823, 0.07048596802633256, 0.06741675094235688, 0.062494186917319894, 0.07266014092601836, 0.06643604999408126, 0.06251480907667428, 0.5620149539317936, 0.0721541449893266, 0.0634066549828276, 0.06760266807395965, 0.06208055396564305, 0.06208280904684216, 0.0625737210502848, 0.7811409650603309, 0.5421252980595455, 0.1619326890213415, 0.06937139690853655, 0.0643412439385429, 0.06343211291823536, 0.0634324150159955, 0.06407666602171957, 0.06345105497166514, 0.06350390298757702, 0.06393551302608103, 0.06286177108995616, 0.06437789998017251, 0.0638196860672906, 0.06329401093535125, 0.06362919404637069, 0.06286765704862773, 0.06354229804128408, 0.06349020299967378, 0.06396328704431653, 0.06944924697745591, 0.7021669310051948, 0.15066916204523295, 0.06510162097401917, 0.06531176902353764, 0.07255030306987464, 0.06306795298587531, 0.06395858398173004, 0.06310978205874562, 0.06659926497377455, 0.06357999797910452, 0.06328763801138848, 0.062329409061931074, 0.0630039309617132, 0.06352573295589536, 0.06293555593583733, 0.06423496291972697, 0.06274548301007599, 0.06322590098716319, 0.06331986794248223, 0.0648027160204947, 0.06367957103066146, 0.06413593690376729, 0.07166974898427725, 0.9351862570038065, 0.5278814040357247, 0.2404966619797051, 0.14699518401175737, 0.0611959109082818, 0.060774209909141064, 0.06006977707147598, 0.059800112969242036, 0.05932479491457343, 0.061905408045277, 0.061308822012506425, 0.07259627396706492, 0.05976037296932191, 1.3797877499600872, 0.5188635989325121, 0.21851876704022288, 0.06615567696280777, 0.06384660303592682, 0.06116173800546676, 0.06827467796392739, 0.06987617397680879, 0.06473501399159431, 0.06559327605646104, 0.06530119699891657, 0.0651976199587807, 0.06506231299135834, 0.48478819394949824, 0.08137928193900734, 0.06303953798487782, 0.06348156393505633, 0.061951122013852, 0.06207906699273735, 0.06345623300876468, 0.06073413696140051, 0.060320374090224504, 0.07411619799677283, 0.059882447007112205, 0.061358314007520676, 0.06121002102736384, 0.06101328495424241, 0.05984595697373152, 0.8562058289535344, 0.36308432603254914, 0.3589624180458486, 0.5360252219252288, 0.27372678997926414, 0.06796180410310626, 0.0674912299728021, 0.0708560849307105, 0.06211181101389229, 0.06362960301339626, 0.06330638704821467, 0.06271806207951158, 0.07145854795817286, 0.7057888769777492, 0.7623719859402627, 0.10197966697160155, 0.06613733107224107, 0.06447841401677579, 0.06481045496184379, 0.0653388419887051, 0.06502651295159012, 0.06511953508015722, 0.06516978004947305, 0.06379691895563155, 0.06480425596237183, 0.06441011803690344, 0.06505688501056284, 0.06518991605844349, 0.06481833499856293, 0.06524338200688362, 0.06536815501749516, 0.5175710839685053, 0.17081168701406568, 0.09227630903478712, 0.06718872010242194, 0.06873557693324983, 0.06324115907773376, 0.060615563997998834, 0.06088417989667505, 0.06154175801202655, 0.06130311009474099, 0.7979504340328276, 0.18750561494380236, 0.06384482304565609, 0.06263048795517534, 0.06634805398061872, 0.062326967949047685, 0.14073967293370515, 0.0660147910239175, 2.0099160009995103, 0.0808089089114219, 0.06659966299775988, 0.06647378799971193, 0.07108314393553883, 0.06576379400212318, 0.06600105902180076, 0.06531793007161468, 0.06609735602978617, 0.06549849105067551, 0.06496734102256596, 0.06590594898443669, 0.06562471692450345, 1.0139631449710578, 0.433356367982924, 0.06455590797122568, 0.06520998408086598, 0.06416238297242671, 0.059874388040043414, 0.06007877201773226, 0.06335243303328753, 0.05923111201263964, 0.05906330293510109, 0.06179066596087068, 0.06517562200315297, 0.06110068899579346, 0.06357699900399894, 0.06334260804578662, 0.06338056398089975, 0.06613795296289027, 0.06298181891907007, 0.0622283429838717, 0.06173609895631671, 0.24811943992972374, 0.185759594081901, 0.10409536701627076, 0.06668517796788365, 0.06078002299182117, 0.06097498210147023, 0.06022069102618843, 0.06519732601009309, 0.06403364206198603, 0.06380565010476857, 0.06574902695138007, 0.06479679408948869, 0.064824333996512, 0.06435351702384651, 1.418682603049092, 0.29788276902399957, 0.06759201700333506, 0.0676653190748766, 0.06681511900387704, 0.06284347898326814, 0.06251368997618556, 0.06201293400954455, 0.06202096305787563, 0.0614965099375695, 0.06415769003797323, 0.061816305038519204, 0.06276234099641442, 0.06648268993012607, 0.06636232510209084, 1.3676926760235801, 0.20484785898588598, 0.19766586902551353, 0.19537242501974106, 0.2756413930328563, 0.15000747703015804, 0.06532892491668463, 0.06613829894922674, 0.06598337495233864, 0.061237829038873315, 0.07381295808590949, 0.06317257205955684, 0.06317669292911887, 2.7187610460678115, 0.43240723898634315, 0.19500290707219392, 0.11767496902029961, 0.06955885293427855, 0.06462281802669168, 0.06456879910547286, 0.0645724548958242, 0.06380702101159841, 0.0641079219058156, 0.06351136404555291, 0.06381946289911866, 0.06390934996306896, 0.151724140974693, 0.13299658906180412, 0.0722929600160569, 0.06101640698034316, 0.0691805180395022, 0.06612590607255697, 0.06057285703718662, 0.06066833401564509, 0.060895478003658354, 0.060905662016011775, 0.06040834600571543, 0.06116679997649044, 0.06011232908349484, 0.059701594058424234, 0.0598566880216822, 0.06166074797511101, 0.060140721034258604, 0.060052155051380396, 0.06230014795437455, 0.05920008500106633, 0.059808498015627265, 0.05946708202827722, 0.060787836904637516, 0.06079749995842576, 0.06194696598686278, 0.060422981972806156, 2.1693912680493668, 0.541298964060843, 0.10760771599598229, 0.0654468759894371, 0.0656769770430401, 0.06301131495274603, 0.06112062104512006, 0.06112696696072817, 0.061334936995990574, 0.06106480897869915, 0.06236384902149439, 0.18522180197760463, 0.07031690201256424, 0.06049346795771271, 0.06062412401661277, 0.06451582501176745, 0.06376780406571925, 0.0659716899972409, 0.06707607896532863, 0.07044851500540972, 0.06702344701625407, 0.06640851206611842, 0.06759932497516274, 0.0678577609360218, 0.06712761893868446, 0.28539070300757885, 0.07402405596803874, 0.06562827399466187, 0.06373321905266494, 0.06374375708401203, 0.06382676400244236, 0.06373048201203346, 0.06348897004500031, 0.06534413993358612, 0.06373709707986563, 0.06358121801167727, 0.06411356804892421, 0.06535728392191231, 0.06000717298593372, 0.06120710202958435, 0.060148693970404565, 0.06120062305126339, 0.06083882204256952, 2.1150936159538105, 0.368608062970452, 0.0948590210173279, 0.06047128001227975, 0.06064203893765807, 0.06907965906430036, 0.06617257604375482, 0.06085759797133505, 0.060929831000976264, 0.060708109056577086, 0.06068915606010705, 0.06709183193743229, 0.07865988602861762, 0.06549027608707547, 0.06592220708262175, 0.06652869191020727, 0.06679576099850237, 0.0658269259147346, 0.06665642594452947, 0.25492203002795577, 0.3399289969820529, 0.37802586797624826, 0.06340789201203734, 0.06104602897539735, 0.060528318979777396, 0.0605004410026595, 0.06157185591291636, 0.07089941401500255, 0.06068425299599767, 0.06139027199242264, 0.060750926961191, 0.06946049607358873, 0.06588819902390242, 0.06702332000713795, 0.06582847202662379, 0.06736704497598112, 0.06668515899218619, 0.06701808096840978, 0.06519622600171715, 0.06570287200156599, 0.06566261500120163, 0.06672699400223792, 0.06684203806798905, 0.066849619965069, 0.06642330100294203, 0.06644856301136315, 0.06727767398115247, 0.06696839106734842, 0.06913681700825691, 0.06599097605794668, 1.216842534020543, 0.23141142399981618, 0.06653583107981831, 0.06703355500940233, 0.06766534992493689, 0.06666724500246346, 0.0711883750045672, 0.06133677205070853, 0.9317941470071673, 0.18492432299535722, 0.07374197989702225, 0.06074041791725904, 0.061293284059502184, 0.06127759697847068, 0.06141822796780616, 0.061016611056402326, 0.0608588510658592, 0.0621505540329963, 0.06192007102072239, 0.06163601402658969, 0.060774766956456006, 0.09486502700019628, 0.07470900297630578, 0.06632820900995284, 0.06397118500899523, 0.06298397295176983, 0.8986464090412483, 0.07054201106075197, 0.06135188799817115, 0.06086431606672704, 0.06159884494263679, 0.060601128032431006, 0.0630223749903962, 0.06175465299747884, 1.0376595898997039, 1.1805398190626875, 0.19654598808847368, 0.06169412098824978, 0.06114864500705153, 0.06086521397810429, 0.0612690020352602, 0.061586212017573416, 0.0614949120208621, 0.06183173099998385, 0.06508620001841336, 0.06756043108180165, 0.06710878608282655, 0.0652913780650124, 0.06585067603737116, 0.06622012599837035, 0.06615473004058003, 0.19429152796510607, 1.572006318019703, 0.8984209460904822, 0.0703990840120241, 0.06987040501553565, 0.06868693500291556, 0.06140254100318998, 0.06037580000702292, 0.06282650399953127, 0.06465791095979512, 0.06457310903351754, 0.06530521798413247, 0.06547046301420778, 0.06417581008281559, 0.06619932199828327, 0.06465087702963501, 1.0072673639515415, 0.2331233989680186, 0.21133621397893876, 0.06545217102393508, 0.06545846699737012, 0.06475087604485452, 0.06491228600498289, 0.06490437395405024, 0.06169183901511133, 0.06336515198927373, 0.062162793008610606, 0.06724672100972384, 0.061802469892427325, 0.061814625980332494, 0.06129618105478585, 0.06274635391309857, 0.06070867402013391, 0.06289232207927853, 0.06225197296589613, 0.061824903008528054, 2.161183835938573, 0.06528045202139765, 0.07184788305312395, 0.06509210006333888, 0.06518628902267665, 0.06057037599384785, 0.06496623996645212, 0.06103533296845853, 0.059832219034433365, 0.05989590601529926, 1.370392601005733, 0.19392608909402043, 1.6879422939382493, 0.06515957496594638, 0.06419455690775067, 0.0808924090815708, 0.07738897099625319, 0.07284810999408364, 0.07230598200112581, 1.851730625028722, 0.1635646269423887, 0.08326259604655206, 0.07775719906203449, 0.07674639706965536, 0.07628466503228992, 0.07560261408798397, 0.0718397389864549, 0.0758500489173457, 0.07136786007322371, 0.07193626090884209, 0.07029419403988868, 0.07086422306019813, 0.07056875992566347, 0.07096583896782249, 0.07037114794366062, 0.07071233005262911, 0.07071589701808989, 0.07073681405745447, 0.07108941406477243, 0.0727302220184356, 0.07140186498872936, 0.07102888403460383, 0.07033306895755231, 0.07149533601477742, 0.07055672106798738, 0.07023596903309226, 0.0708840029546991, 0.07629205798730254, 0.07193478802219033, 0.07234971993602812, 0.07744445407297462, 0.0721695669926703, 0.07734764297492802, 0.0781255440087989, 0.07776005810592324, 0.07173149893060327, 0.07296880695503205, 0.07698263903148472, 0.07651836599688977, 0.07254430209286511, 0.2535263010067865, 0.08067652699537575, 0.07384072709828615, 0.07264083600603044, 0.07733111700508744, 0.07280785299371928, 0.07262227800674736, 0.07582833000924438, 0.07577311294153333, 0.0812903509940952, 0.08136789791751653, 0.07999352901242673, 0.07584154093638062, 0.07664036704227328, 0.07729127304628491, 0.07682448206469417, 0.07683227001689374, 0.16571695602033287, 0.07215675304178149, 0.07161684799939394, 0.07635691796895117, 0.07804688694886863, 0.1714566999580711, 0.07592906197533011, 0.07763163396157324, 0.08502235193736851, 0.07103162002749741, 0.07390389498323202, 0.07209276198409498, 0.07837525801733136, 0.07246361998841166, 1.847474624053575, 0.3320683119818568, 0.08705425204243511, 0.07466615200974047, 0.08062597003299743, 0.07978135894518346, 0.07745814591180533, 0.07715159095823765, 0.07570784003473818, 0.07074523309711367, 0.07558532303664833, 0.07035972992889583, 0.07134615792892873, 0.07447384390980005, 0.07487556105479598, 0.0706339799799025, 0.07598125201184303, 0.07468108600005507, 0.07438448094762862, 0.07001612894237041, 0.06943138397764415, 0.0743188849883154, 0.07517238997388631, 1.1998227750882506, 0.0768646050710231, 0.07808936096262187, 0.07706313801463693, 0.08102881792001426, 0.07521470193751156, 0.07696896407287568, 0.07316208991687745, 0.07492113299667835, 0.0714259990490973, 0.0766464569605887, 0.07588602299802005, 0.9150567279430106, 0.07311377697624266, 0.06887387996539474, 0.0696464009815827, 0.07293006998952478, 0.07272930804174393, 0.06940531195141375, 0.07706079503986984, 0.06838166201487184, 0.06958144996315241, 0.0740591250360012, 0.07237799698486924, 0.06862795096822083, 0.07344042102340609, 0.07523183396551758, 0.0730170370079577, 0.07257835299242288, 0.06911216396838427, 0.07380442903377116, 0.07358697894960642, 0.07375087204854935, 0.06908442499116063, 0.07933847000822425, 0.07144441199488938, 0.07514648896176368, 0.06949919403996319, 0.07537172106094658, 0.07743284699972719, 0.08154109306633472, 0.07926194800529629, 0.08026019192766398, 0.07493575604166836, 0.07481708796694875, 0.08057282597292215, 0.07896261301357299, 0.07900650997180492, 0.07550912396982312, 0.07442406995687634, 0.08078389707952738, 0.07645266701001674, 0.08118834591004997, 0.0721505950205028, 0.07653568498790264, 0.07124986895360053, 0.08379588497336954, 0.07665566704235971, 0.07112473901361227, 0.07553150493185967, 0.07207621494308114, 0.08465990494005382, 0.07670904195401818, 0.07226937904488295, 0.07694543094839901, 0.07182092196308076, 0.07282504497561604, 0.07844664098229259, 0.07635864801704884, 0.07808584999293089, 0.076721703982912, 0.07822672894690186, 2.0425386991119012, 0.09699661599006504, 0.07403753197286278, 0.07651441602502018, 0.07682748604565859, 0.07744968496263027, 0.07631680101621896, 0.08120866597164422, 0.07810020993929356, 0.07174994808156043, 0.07779710891190916, 0.07909111003391445, 0.07196949399076402, 0.0723079489544034, 0.07195151306223124, 0.07237535598687828, 0.0769613740267232, 0.9485415789531544, 1.9533805589890108, 0.1020010010106489, 0.07534406497143209, 0.08329197310376912, 0.08191265794448555, 0.07526720699388534, 0.0761861139908433, 0.07382370694540441, 0.07615243992768228, 0.07581292896065861, 0.07569299102760851, 0.07101376890204847, 0.08595684892497957, 0.07286037504673004, 0.08005324099212885, 0.07559390703681856, 0.3444967530667782, 0.30260972189716995, 0.43283338099718094, 0.07887636893428862, 0.07913228706456721, 0.07830523396842182, 0.07489989406894892, 0.07135064003523439, 0.0762308599660173, 0.07571481191553175, 0.07208003103733063, 0.07183596305549145, 0.07615785300731659, 0.07093312393408269, 0.07571388897486031, 0.0711197319906205, 0.07073856191709638, 0.0707201010081917, 0.07762634695973247, 0.07062745408620685, 0.06934012298006564, 0.06948903994634748, 0.33243719197344035, 0.07659773295745254, 0.07627957500517368, 0.0703707590000704, 0.07555597194004804, 0.07575901097152382, 0.07188225502613932, 0.07717403792776167, 0.07511224900372326, 0.07623414695262909, 0.07052704796660691, 0.07568660890683532, 0.0746906369458884, 0.07084863900672644, 0.07552292395848781, 0.07239191408734769, 0.07820837700273842, 0.07480397599283606, 0.07451032404787838, 0.07517205004114658, 0.0760930209653452, 0.07696629909332842, 0.07604320195969194, 0.0706881390651688, 0.07529065001290292, 0.07473651808686554, 0.07424621097743511, 0.07035886798985302, 0.07697977893985808, 0.07690521399490535, 0.07612138707190752, 0.07151998998597264, 0.07581024407409132, 0.07843596301972866, 0.07705363701097667, 0.0736564788967371, 0.07796827598940581, 0.07836799905635417, 0.08088833303190768, 0.0784935459960252, 0.07792850898113102, 0.07332304504234344, 0.07877776201348752, 0.7358003449626267, 0.2706768539501354, 0.0789915710920468, 0.07950189395342022, 0.07607488799840212, 0.08132637909147888, 0.07541230891365558, 0.0748157879570499, 0.07809991296380758, 0.07890280708670616, 0.08170158392749727, 0.08020401594694704, 0.08082753198686987, 0.08028164901770651, 2.100557186990045, 0.11968979798257351, 0.08303979795891792, 0.08192995900753886, 0.0768502299906686, 0.08810560696292669, 0.07730191899463534, 0.07508604705799371, 0.07794433506205678, 0.07702754496131092, 0.07262353494297713, 0.07217580091673881, 0.07798454107251018, 0.07718765502795577, 0.07726088096387684, 0.07755725900642574, 0.08232780802063644, 0.08236625394783914, 0.08200690301600844, 0.0822602219413966, 0.07829041697550565, 0.0773499479983002, 0.07283428206574172, 0.07705801504198462, 0.07226699299644679, 0.07831713499035686, 0.07681963499635458, 0.07291806093417108, 0.0726166139356792, 0.0769526989897713, 0.07250291504897177, 0.07268545695114881, 0.07646225008647889, 0.07687573705334216, 0.46336437202990055, 0.2809878350235522, 0.32867091696243733, 0.5431965050520375, 0.07797538104932755, 0.06542459002230316, 0.06586176401469857, 0.06928379205055535, 0.072987528052181, 0.06504378991667181, 0.06873833702411503, 0.06930833507794887, 0.07045023597311229, 0.06935567897744477, 0.07032281695865095, 0.0715535159688443, 0.9620587199460715, 0.256844190065749, 0.06785856490023434, 0.07657700509298593, 0.07389491610229015, 0.06557606998831034, 0.06951676507014781]
[0.02296056312138253, 0.0014816608959428814, 0.001330111468001744, 0.001416769327728876, 0.0012872880419754252, 0.0013001914093346925, 0.0012966318778237518, 0.0014050002661248554, 0.0013557628167755141, 0.0012902301826457285, 0.0013143293279204138, 0.0012888545930689695, 0.0013026115722117983, 0.00134605355085615, 0.0014222130412235856, 0.001400030488908595, 0.0014223188960126468, 0.0015656954694387255, 0.001427421655163777, 0.001643607428544486, 0.030959748244863385, 0.007418807306117854, 0.004026286551082621, 0.002035867981137518, 0.0013512108161361242, 0.0013306437141014909, 0.0013252110220491886, 0.0013227901226669854, 0.0013375499390293748, 0.001330479632644933, 0.0013778043473709602, 0.001367116449115684, 0.0014355623077753248, 0.0014090337956856405, 0.014447534202636048, 0.006068806285133624, 0.005931076306222501, 0.0020443891027790246, 0.001348255511031163, 0.0014166939594991961, 0.0013391415716852157, 0.001388870488510144, 0.0013267238367805068, 0.0013504934269098602, 0.0013456386727832106, 0.01934287708243165, 0.002477385020548744, 0.001479637243651918, 0.0014165322229798352, 0.0013967512451036244, 0.001309752815916222, 0.0013101423677171068, 0.0013105134474949874, 0.0013187209985275961, 0.0013234597127124363, 0.001387586653213568, 0.0030352130807860165, 0.0014164746520394574, 0.0014049243870932534, 0.044306261203612904, 0.0015108608355631633, 0.0015163327144382863, 0.0013034556331873244, 0.0013519917538731682, 0.0013186508167192948, 0.0013149330815376372, 0.0012764244896302723, 0.0012865180805401535, 0.028878664754673232, 0.0014641071641247492, 0.0015511445306735684, 0.001276205122774961, 0.0012625906716233917, 0.0012748213057235188, 0.0012647897356702965, 0.0012621007769425608, 0.001260335571417699, 0.0012710802257061005, 0.0012851604094196642, 0.0015460015916056475, 0.026617862652436052, 0.012392485224432787, 0.006980918305070728, 0.0013825161838713958, 0.0013360371219222338, 0.0012826801215925691, 0.0012759354502456834, 0.0012700126544401354, 0.0015092723057320227, 0.001262213633281692, 0.0012487692245263227, 0.016326862755611692, 0.009445939491008769, 0.003211464162687866, 0.001282149631225941, 0.0012752084912048007, 0.0016400286528681005, 0.0012520558571404948, 0.0012518098573105372, 0.001235373204155844, 0.001270457429868378, 0.0015132038364643041, 0.0014967696114006092, 0.0013152350201176442, 0.006414334427527323, 0.006079275693212237, 0.0013306148573566153, 0.0012729027764681652, 0.0012556565521589043, 0.0012678661412198324, 0.0012496363476146848, 0.0012541707552855416, 0.0012501722857432098, 0.0012643289804572658, 0.001263832286171311, 0.0012655388350047323, 0.0012607723688326624, 0.022157721102655847, 0.01288032808046484, 0.0013212435902571495, 0.0013294299798352377, 0.0012633618779898602, 0.0012401552034579978, 0.0013399867350425646, 0.0013151493693264772, 0.042015064714419445, 0.0017889861839500312, 0.0012717143898563726, 0.00125789936698441, 0.0012788548352843036, 0.0012667523459436334, 0.0012793478779304698, 0.0012687989985760376, 0.0012605969000569716, 0.008825036122140532, 0.017487361183751146, 0.00142516618195389, 0.0013655075513548693, 0.0013641214891507917, 0.0013860905904094784, 0.0013602635495326653, 0.0013812532059240099, 0.0013790643460364366, 0.014271249673899492, 0.0191446923465487, 0.006295994347037405, 0.0013741196931472846, 0.0013847827759324288, 0.0014792928586200792, 0.0013951771436449215, 0.0013866269205487808, 0.0015645415298830793, 0.0013066521437116424, 0.0012673808580112396, 0.01372024112343028, 0.009702825141899591, 0.0012553980412455847, 0.0012356574286003501, 0.0014131313655525446, 0.001321261223614672, 0.0012520658165900683, 0.001224273632812713, 0.0012260105114012044, 0.0012302553252678137, 0.0012214131420478225, 0.0012248518154481236, 0.001221546691804364, 0.001228829389628099, 0.0012184715501926079, 0.0012301845304972055, 0.0012209122449311676, 0.001335355876089663, 0.0012110361439765108, 0.00122329969510284, 0.0012213842045249684, 0.001196881224002157, 0.0012196312463671273, 0.0012269717764717583, 0.0012204402449483775, 0.0012417085521037178, 0.0012234051650085924, 0.0012313419387542776, 0.0012332428156455256, 0.0013087557566979406, 0.0012266444284658954, 0.0012245216544679537, 0.001216392163472364, 0.0012237200589508427, 0.0012275947124830314, 0.0012272182443388263, 0.001219563651829958, 0.0012326272636918084, 0.0012152108781952032, 0.0012230456531123848, 0.0012208007546902007, 0.0012466031416528383, 0.0012875847941340537, 0.0013717862644365855, 0.0013400804279942293, 0.0013813749383374744, 0.001331278101579115, 0.014249180141855411, 0.001260728858012174, 0.0012595839180736517, 0.0012852908373449225, 0.0012555452876211125, 0.0012726128167871917, 0.001272156205009289, 0.0012955634061209097, 0.0012739478154297993, 0.001257820285343546, 0.0012585126524030858, 0.044339462570675024, 0.010028386407778884, 0.004864331101998687, 0.0013218773267593008, 0.0013365018566386128, 0.0013235929797460533, 0.0013240319605422567, 0.0013526305722604906, 0.0013278464487354671, 0.0013237916127949649, 0.0013203259785564579, 0.0014239181431808642, 0.0013376223659903115, 0.010019949896318115, 0.005640735631162415, 0.0013397285116038152, 0.00134092051184223, 0.0013479327151019657, 0.00133576369321696, 0.001356547470299565, 0.001344125325867564, 0.001339582103893769, 0.0013412471209671727, 0.0013655262633359858, 0.001343646079624946, 0.0013492356924036024, 0.0013459528777368215, 0.0015466176115964748, 0.0041836521431461585, 0.008063269817099279, 0.002677485938374029, 0.0013078173280370478, 0.0013191546527288702, 0.0012971921420028927, 0.0013027077336434502, 0.0012982741416412958, 0.001292973267370645, 0.0013556391623213279, 0.001262891346265619, 0.00126136254937369, 0.0012690816953664227, 0.0012640588779040441, 0.0013900096528232098, 0.0013679633468237458, 0.0014082647346872457, 0.007640838958037903, 0.0037288233466750507, 0.0014811201206389435, 0.001438489143394542, 0.0013758520600480996, 0.0012753915697412224, 0.0014828600188983338, 0.0013558377549812502, 0.00127581243013621, 0.011469692937383542, 0.001472533571210747, 0.0012940133669964817, 0.0013796462872236663, 0.0012669500809314909, 0.0012669961029967787, 0.0012770147153119348, 0.015941652348170018, 0.011063781593051948, 0.003304748755537582, 0.0014157427940517664, 0.0013130866109906715, 0.0012945329166986809, 0.0012945390819590918, 0.0013076870616677465, 0.001294919489217656, 0.001295998020154633, 0.001304806388287368, 0.0012828932875501259, 0.0013138346934729085, 0.0013024425728018491, 0.0012917145088847195, 0.0012985549805381773, 0.001283013409155668, 0.001296781592679267, 0.001295718428564771, 0.0013053732049860517, 0.001417331570968488, 0.014329937367452954, 0.0030748808580659784, 0.0013286045096738606, 0.0013328932453783192, 0.0014806184299974417, 0.001287101081344394, 0.0013052772241169397, 0.0012879547358927677, 0.0013591686729341745, 0.0012975509791653982, 0.0012915844492120097, 0.0012720287563659403, 0.0012857945094227182, 0.0012964435297121502, 0.001284399100731374, 0.0013109176106066728, 0.0012805200614301221, 0.001290324509942106, 0.0012922422029078007, 0.0013225044085815245, 0.0012995830822583971, 0.001308896671505455, 0.0014626479384546376, 0.019085433816404214, 0.010773089878280096, 0.004908095142442961, 0.0029999017145256606, 0.001248896140985343, 0.001240289998145736, 0.0012259138177852242, 0.0012204104687600415, 0.001210710100297417, 0.001263375674393408, 0.001251200449234825, 0.0014815566115727534, 0.0012195994483535082, 0.028158933672654842, 0.010589053039439023, 0.004459566674290263, 0.0013501158563838322, 0.001302991898692384, 0.001248198734805444, 0.0013933607747740283, 0.0014260443668736487, 0.001321122734522333, 0.0013386382868665518, 0.0013326774897738074, 0.0013305636726281776, 0.0013278023059460887, 0.00989363661121425, 0.0016608016722246396, 0.0012865211833648536, 0.0012955421211235986, 0.0012643086125275918, 0.0012669197345456602, 0.0012950251634441772, 0.0012394721828857247, 0.001231028042657643, 0.0015125754693218944, 0.0012220907552471878, 0.0012522104899494015, 0.001249184102599262, 0.0012451690806988248, 0.0012213460606883984, 0.0174735883459905, 0.007409884204745901, 0.007325763633588747, 0.010939290243372018, 0.005586261019984983, 0.001386975593940944, 0.0013773720402612674, 0.0014460425496063366, 0.0012675879798753529, 0.0012985633268040053, 0.001291967082616626, 0.0012799604506022772, 0.001458337713432099, 0.014403854632198962, 0.015558611957964544, 0.002081217693297991, 0.0013497414504538994, 0.001315886000342363, 0.0013226623461600772, 0.0013334457548715326, 0.0013270716928895942, 0.001328970103676678, 0.0013299955112137357, 0.0013019779378700318, 0.001322535835966772, 0.0013144922048347642, 0.001327691530827813, 0.001330406450172316, 0.0013228231632359782, 0.0013314975919772167, 0.0013340439799488808, 0.01056267518303072, 0.003485952796205422, 0.0018831899803017779, 0.0013711983694371824, 0.001402766876188772, 0.001290635899545587, 0.001237052326489772, 0.0012425342836056132, 0.001255954245143399, 0.00125108387948451, 0.01628470273536383, 0.003826645202934742, 0.0013029555723603284, 0.0012781732235750069, 0.0013540419179718106, 0.0012719789377356671, 0.0028722382231368398, 0.0013472406331411734, 0.04101869389794919, 0.001649161406355549, 0.0013591767958726507, 0.001356607918361468, 0.0014506764068477312, 0.0013421182449412893, 0.0013469603882000154, 0.0013330189810533608, 0.0013489256332609423, 0.0013367038989933779, 0.0013258641025013461, 0.0013450193670293202, 0.0013392799372347643, 0.020693125407572607, 0.008844007509855591, 0.0013174675096168506, 0.001330816001650326, 0.0013094363871923819, 0.0012219262865314983, 0.001226097388116985, 0.0012929067965977046, 0.0012087982043395846, 0.0012053735292877775, 0.0012610339992014425, 0.0013301147347582237, 0.001246952836648846, 0.001297489775591815, 0.0012927062866487065, 0.0012934808975693826, 0.0013497541420998014, 0.0012853432432463278, 0.0012699661833443204, 0.0012599203868636063, 0.005063662039382117, 0.003791012124120429, 0.0021243952452300154, 0.0013609219993445643, 0.0012404086324861463, 0.001244387389825923, 0.0012289936944120089, 0.0013305576736753692, 0.0013068090216731842, 0.0013021561245871137, 0.001341816876558777, 0.0013223835528467078, 0.001322945591765551, 0.0013133370821193165, 0.028952706184675346, 0.006079240184163256, 0.0013794289184354094, 0.0013809248790791144, 0.0013635738572219806, 0.0012825199792503702, 0.0012757895913507258, 0.0012655700818274397, 0.0012657339399566455, 0.0012550308150524388, 0.0013093406130198616, 0.0012615572456840653, 0.0012808641019676412, 0.001356789590410736, 0.0013543331653487925, 0.027912095429052656, 0.004180568550732367, 0.004033997327051297, 0.003987192347341654, 0.005625334551690945, 0.0030613770822481234, 0.001333243365646625, 0.0013497612030454436, 0.0013465994888232375, 0.001249751613038231, 0.0015063868997124384, 0.001289236164480752, 0.0012893202638595688, 0.05548491930750636, 0.008824637530333534, 0.003979651164738651, 0.0024015299800061144, 0.0014195684272301744, 0.0013188330209528913, 0.0013177305939892422, 0.0013178052019555957, 0.0013021841022775186, 0.0013083249368533796, 0.001296150286643937, 0.0013024380183493604, 0.0013042724482258971, 0.003096411040299857, 0.002714216103302125, 0.0014753665309399366, 0.0012452327955172074, 0.0014118473069286163, 0.0013495082871950402, 0.0012361807558609514, 0.0012381292656254098, 0.0012427648572175174, 0.001242972694204322, 0.0012328233878717435, 0.0012483020403365396, 0.0012267822261937723, 0.0012183998787433517, 0.0012215650616669838, 0.0012583826117369594, 0.0012273616537603798, 0.0012255541847220489, 0.001271431590905603, 0.0012081650000217619, 0.0012205815921556584, 0.001213613918944433, 0.0012405681000946431, 0.0012407653052739951, 0.0012642237956502608, 0.0012331220810776766, 0.04427329118468096, 0.011046917633894754, 0.0021960758366527, 0.0013356505303966757, 0.0013403464702661245, 0.001285945203117266, 0.0012473596131657154, 0.0012474891216475137, 0.0012517334080814403, 0.0012462205914020234, 0.0012727316126835589, 0.003780036775053156, 0.0014350388165829437, 0.0012345605705655655, 0.0012372270207471993, 0.0013166494900360703, 0.00130138375644325, 0.001346361020351855, 0.0013688995707209926, 0.0014377247960287698, 0.0013678254493113076, 0.0013552757564513963, 0.001379578060717607, 0.001384852264000445, 0.0013699514069119279, 0.005824300061379161, 0.0015106950197558925, 0.0013393525305033034, 0.0013006779398503049, 0.0013008930017145313, 0.0013025870204580073, 0.001300622081878234, 0.0012956932662244961, 0.001333553876195635, 0.001300757083262564, 0.001297575877789332, 0.0013084401642637594, 0.0013338221208553534, 0.0012246361833864025, 0.001249124531216007, 0.0012275243667429503, 0.0012489923071686406, 0.0012416086131136637, 0.043165175835792054, 0.007522613530009225, 0.0019358983881087328, 0.001234107755352648, 0.001237592631380777, 0.0014097889604959258, 0.001350460735586833, 0.0012419917953333684, 0.001243465938795434, 0.0012389410011546345, 0.0012385542053083072, 0.0013692210599475978, 0.0016053037965024005, 0.0013365362466750096, 0.0013453511649514644, 0.0013577284063307606, 0.0013631787958878036, 0.0013434066513211143, 0.0013603352233577445, 0.005202490408733791, 0.006937326469021488, 0.007714813632168332, 0.0012940386124905578, 0.0012458373260285174, 0.0012352718159138244, 0.001234702877605296, 0.0012565684880187012, 0.001446926816632705, 0.0012384541427754626, 0.0012528626937229111, 0.0012398148359426735, 0.0014175611443589537, 0.001344657122936784, 0.0013678228572885298, 0.0013434382046249751, 0.0013748376525710433, 0.0013609216120854324, 0.0013677159381308118, 0.0013305352245248398, 0.0013408749388074692, 0.001340053367371462, 0.0013617753878007739, 0.0013641232258773275, 0.001364277958470796, 0.0013555775714886127, 0.0013560931226808805, 0.0013730137547173975, 0.0013667018585173146, 0.0014109554491481002, 0.0013467546134274832, 0.024833521102460062, 0.004722682122445228, 0.0013578741036697614, 0.0013680317348857619, 0.0013809255086721815, 0.0013605560204584379, 0.0014528239796850451, 0.0012517708581777252, 0.019016207081778924, 0.0037739657754154534, 0.001504938365245352, 0.0012396003656583478, 0.001250883348153106, 0.0012505632036422588, 0.0012534332238327787, 0.0012452369603347412, 0.0012420173686910041, 0.0012683786537346182, 0.0012636749187902529, 0.0012578778372773407, 0.0012403013664582859, 0.001936020959187679, 0.0015246735301286895, 0.0013536369185704663, 0.0013055343879386783, 0.0012853872030973434, 0.018339722633494863, 0.0014396328787908566, 0.0012520793469014521, 0.00124212889932096, 0.001257119284543608, 0.0012367577149475716, 0.0012861709181713511, 0.0012602990407648744, 0.021176726324483752, 0.024092649368626276, 0.004011142614050483, 0.0012590636936377507, 0.0012479315307561535, 0.0012421472240429447, 0.0012503877966379632, 0.0012568614697463963, 0.0012549982045073898, 0.0012618720612241601, 0.0013282897962941503, 0.0013787843077918704, 0.0013695670629148276, 0.0013324771033676, 0.0013438913477014523, 0.0013514311428238846, 0.0013500965314404089, 0.003965133223777675, 0.032081761592238835, 0.01833512134878535, 0.0014367160002453899, 0.0014259266329701154, 0.0014017741837329706, 0.0012531130816977546, 0.0012321591838167943, 0.0012821735510108422, 0.0013195492032611249, 0.0013178185517044396, 0.001332759550696581, 0.0013361318982491384, 0.0013097104098533793, 0.001351006571393536, 0.0013194056536660207, 0.020556476815337583, 0.00475762038710242, 0.004312983958753853, 0.0013357585923252056, 0.001335887081578982, 0.0013214464498949902, 0.0013247405307139366, 0.0013245790602867396, 0.001259017122757374, 0.0012931663671280353, 0.0012686284287471551, 0.0013723820614229356, 0.0012612748957638229, 0.0012615229791904591, 0.0012509424705058336, 0.0012805378349611954, 0.001238952531023141, 0.0012835167771281333, 0.0012704484278754312, 0.0012617327144597561, 0.044105792570174954, 0.0013322541228856664, 0.0014662833276147746, 0.0013284102053742629, 0.0013303324290342173, 0.0012361301223234255, 0.0013258416319684107, 0.0012456190401726232, 0.0012210656945802728, 0.0012223654288836584, 0.02796719593889251, 0.00395767528763307, 0.034447801917107126, 0.0013297872442029873, 0.0013100929981173606, 0.0016508654914606288, 0.0015793667550255753, 0.0014866961223282376, 0.0014756322857372615, 0.03779042091895351, 0.0033380536110691576, 0.0016992366540112666, 0.001586881613510908, 0.0015662530014215379, 0.0015568298986181617, 0.0015429104915915095, 0.001466117122172549, 0.001547960181986647, 0.0014564869402698716, 0.0014680869573233078, 0.0014345753885691567, 0.0014462086338815944, 0.001440178773993132, 0.0014482824279147448, 0.0014361458764012372, 0.0014431087765842676, 0.0014431815717977528, 0.001443608450152132, 0.001450804368668825, 0.0014842902452741958, 0.001457180918137334, 0.0014495690619306906, 0.0014353687542357615, 0.0014590884900974985, 0.0014399330830201507, 0.0014333871231243319, 0.0014466123051979408, 0.0015569807752510722, 0.0014680568984120476, 0.0014765248966536351, 0.0015804990627137677, 0.0014728483059728633, 0.0015785233260189391, 0.0015943988573224265, 0.0015869399613453721, 0.001463908141440883, 0.001489159325612899, 0.0015710742659486678, 0.001561599306058975, 0.0014804959610788797, 0.005174006142995643, 0.0016464597345995052, 0.0015069536142507378, 0.0014824660409393968, 0.001578186061328315, 0.0014858745508922301, 0.0014820873062601503, 0.001547516938964171, 0.0015463900600312924, 0.0016589867549815349, 0.0016605693452554394, 0.0016325210002536069, 0.0015477865497220535, 0.0015640891233116997, 0.001577372919311937, 0.0015678465727488606, 0.001568005510548852, 0.0033819786942925075, 0.0014725867967710507, 0.0014615683265182438, 0.0015583044483459421, 0.0015927936112014006, 0.0034991163256749207, 0.001549572693374084, 0.0015843190604402702, 0.001735150039538133, 0.0014496248985203554, 0.0015082427547598372, 0.001471280856818265, 0.0015994950615781912, 0.0014788493875186055, 0.03770356375619541, 0.006776904326160343, 0.0017766173886211247, 0.0015237990206069484, 0.0016454279598570904, 0.001628190998881295, 0.0015807784879960272, 0.0015745222644538296, 0.001545057959892616, 0.0014437802672880341, 0.0015425576129928231, 0.0014359128556917517, 0.0014560440393658926, 0.0015198743655061235, 0.0015280726745876732, 0.0014415097955082143, 0.0015506377961600618, 0.0015241037959194913, 0.0015180506315842575, 0.0014289005906606208, 0.0014169670199519213, 0.001516711938537049, 0.0015341304076303328, 0.024486179083433687, 0.0015686654096127165, 0.0015936604278086095, 0.0015727171023395292, 0.0016536493453064135, 0.0015349939170920728, 0.0015707951851607282, 0.0014931038758546418, 0.0015290027142179255, 0.0014576734499815776, 0.0015642134073589528, 0.0015486943468983685, 0.018674627100877767, 0.0014921178974743401, 0.0014055893870488722, 0.0014213551220731164, 0.0014883687752964242, 0.0014842715926886517, 0.001416434937783954, 0.001572669286527956, 0.0013955441227524864, 0.001420029591084743, 0.0015114107150204328, 0.0014771019792830457, 0.001400570427922874, 0.0014987841025184915, 0.0015353435503166852, 0.0014901436124073, 0.0014811908773963852, 0.001410452325885393, 0.001506212837423901, 0.0015017750806042127, 0.0015051198377254971, 0.0014098862243094006, 0.0016191524491474337, 0.0014580492243854975, 0.0015336018155461975, 0.001418350898774759, 0.0015381983889989099, 0.0015802621836679019, 0.00166410394012928, 0.0016175907756182917, 0.001637963100564571, 0.0015293011437075175, 0.0015268793462642602, 0.0016443433872024929, 0.0016114818982361835, 0.001612377754526631, 0.0015410025299963902, 0.0015188585705484968, 0.0016486509608066812, 0.0015602585104085049, 0.0016569050185724485, 0.0014724611228674042, 0.001561952754855156, 0.0014540789582367455, 0.0017101201014973375, 0.0015644013682114227, 0.0014515252859920872, 0.0015414592843236668, 0.0014709431621036967, 0.0017277531620419147, 0.0015654906521228198, 0.0014748852866302644, 0.0015703149173142655, 0.0014657331012873625, 0.0014862254076656333, 0.0016009518567814815, 0.0015583397554499762, 0.0015935887753659366, 0.0015657490608757551, 0.0015964638560592216, 0.041684463247181655, 0.00197952277530745, 0.0015109700402625057, 0.001561518694388167, 0.0015679078784828283, 0.001580605815563883, 0.0015574857350248768, 0.0016573197137070249, 0.001593881835495787, 0.001464284654725723, 0.001587696100243044, 0.0016141042864064173, 0.00146876518348498, 0.0014756724276408857, 0.0014683982257598213, 0.0014770480813648627, 0.0015706402862596574, 0.01935799140720723, 0.03986490936712267, 0.0020816530818499775, 0.001537633979008818, 0.0016998361857912066, 0.0016716868968262356, 0.0015360654488548028, 0.001554818652874353, 0.0015066062641919268, 0.0015541314270955567, 0.0015472026318501756, 0.001544754918930786, 0.0014492605898377238, 0.0017542214066322362, 0.0014869464295251028, 0.0016337396120842621, 0.0015427327966697666, 0.007030545980954657, 0.006175708610146325, 0.008833334306064916, 0.001609721814985482, 0.0016149446339707595, 0.0015980659993555472, 0.0015285692667132433, 0.001456135510923151, 0.0015557318360411695, 0.0015452002431741174, 0.001471021041578176, 0.0014660400623569684, 0.0015542418981085019, 0.0014476147741649527, 0.0015451814076502103, 0.001451423101849398, 0.0014436441207570688, 0.0014432673675141164, 0.0015842111624435199, 0.0014413766140042215, 0.0014151045506135846, 0.0014181436723744382, 0.006784432489253885, 0.0015632190399480109, 0.0015567260205137487, 0.001436137938776947, 0.0015419586110213886, 0.001546102264724976, 0.0014669847964518228, 0.0015749803658726873, 0.0015329030408923114, 0.0015557989174005936, 0.0014393275095225901, 0.0015446246715680677, 0.001524298713181396, 0.0014458905919740091, 0.0015412841624181186, 0.0014773860017826058, 0.0015960893265864983, 0.0015266117549558378, 0.001520618858119967, 0.0015341234702274812, 0.0015529187952111267, 0.001570740797823029, 0.0015519020808100396, 0.0014426150829626285, 0.001536543877814345, 0.001525235062997256, 0.0015152287954578595, 0.0014358952650990412, 0.0015710158967317976, 0.0015694941631613337, 0.0015534976953450515, 0.0014595916323667886, 0.0015471478382467615, 0.0016007339391781359, 0.0015725232043056463, 0.0015031934468721856, 0.001591189305906241, 0.001599346919517432, 0.001650782306773626, 0.001601909101959698, 0.0015903777343087963, 0.0014963886743335395, 0.001607709428846684, 0.015016333570665851, 0.005524017427553784, 0.0016120728794295266, 0.0016224876317024535, 0.0015525487346612678, 0.0016597220222750793, 0.0015390267125235833, 0.001526852815449998, 0.0015938757747715833, 0.0016102613691164522, 0.0016673792638264749, 0.001636816651978511, 0.0016495414691197934, 0.0016384010003613575, 0.042868514020205, 0.0024426489384198674, 0.001694689754263631, 0.001672039979745691, 0.0015683720406258898, 0.0017980736114882997, 0.0015775901835639865, 0.0015323683073059941, 0.0015907007155521792, 0.0015719907134961412, 0.0014821129580199414, 0.001472975528913037, 0.0015915212463777587, 0.0015752582658766483, 0.0015767526727321806, 0.0015828012042127702, 0.0016801593473599274, 0.001680943958119166, 0.0016736102656328253, 0.0016787800396203386, 0.0015977636117450132, 0.0015785703673122488, 0.0014864139197090147, 0.0015726125518772372, 0.00147483659176422, 0.0015983088773542217, 0.0015677476529868282, 0.0014881236925341037, 0.0014819717129730449, 0.0015704632446892103, 0.001479651327530036, 0.0014833766724724248, 0.0015604540833975284, 0.0015688925929253502, 0.009653424417289594, 0.005853913229657337, 0.006847310770050778, 0.011316593855250781, 0.001624487105194324, 0.001363012292131316, 0.0013721200836395535, 0.0014434123343865697, 0.0015205735010871042, 0.0013550789565973294, 0.0014320486880023964, 0.001443923647457268, 0.0014677132494398393, 0.001444909978696766, 0.0014650586866385613, 0.0014906982493509229, 0.02004288999887649, 0.005350920626369771, 0.0014137201020882155, 0.00159535427277054, 0.0015394774187977116, 0.0013661681247564654, 0.0014482659389614128]
[43.55293878087546, 674.9182641846211, 751.8166890947284, 705.8312037309772, 776.8269162707645, 769.1175259431222, 771.2289178624742, 711.7436374287028, 737.5921419488074, 775.0555005227158, 760.8443171410025, 775.8827142935028, 767.6885583797077, 742.9124936106409, 703.12953897516, 714.2701590588631, 703.0772091992992, 638.69380701375, 700.5638427737483, 608.4177904242989, 32.30000425361704, 134.79255609932864, 248.36781667492392, 491.1909854986086, 740.0769650879242, 751.5159688521461, 754.5968025935136, 755.9778251018523, 747.6356364874674, 751.6086495905581, 725.7924551538375, 731.466584757171, 696.5911507872404, 709.7061852326947, 69.2159634976011, 164.77705054610783, 168.6034622334676, 489.143675556017, 741.6991748360718, 705.8687540063358, 746.7470364179421, 720.0095388827151, 753.7363634218325, 740.4700978724169, 743.1415432879026, 51.698617312119474, 403.65142749531066, 675.8413281973648, 705.9493485410359, 715.9470975991937, 763.5028440847153, 763.275827605268, 763.0597014563065, 758.3105153527845, 755.595346344541, 720.6757125286985, 329.46616049145194, 705.9780410190806, 711.782078229114, 22.57017344353254, 661.8743278412248, 659.4858704017622, 767.1914367769557, 739.6494816889327, 758.3508744854272, 760.4949742618344, 783.4384314340905, 777.2918353235605, 34.62763976434115, 683.010113264356, 644.6852502943491, 783.5730966395239, 792.02232558414, 784.4236643287467, 790.6452525644772, 792.3297554910789, 793.4394796737679, 786.7324027045483, 778.1129831501476, 646.8298644902553, 37.56875647971985, 80.69406433734689, 143.24762965262465, 723.3188382646969, 748.4821967830071, 779.61760158753, 783.7387070070422, 787.3937291127496, 662.5709596619034, 792.2589121463142, 800.7884726493924, 61.248753968749504, 105.8655945183496, 311.3844493793262, 779.940168951918, 784.1854935071933, 609.745444539148, 798.6864118697132, 798.8433659952635, 809.4719851749744, 787.1180698306451, 660.8495008422414, 668.105493579767, 760.3203873863948, 155.90082046680755, 164.4932801972678, 751.5322668097882, 785.6059539555963, 796.3961150687718, 788.7267965353861, 800.2328052547506, 797.3395933413599, 799.8897523196285, 790.9333847890865, 791.2442267394736, 790.1772528349638, 793.1645907864327, 45.130994986670316, 77.63777395675709, 756.8627067514274, 752.2020829738882, 791.5388436376628, 806.3506867621417, 746.2760442686285, 760.369904227781, 23.800986784076116, 558.9758093000071, 786.3400838870275, 794.9761532970019, 781.9495789588103, 789.4202866110161, 781.6482266087351, 788.1469020091373, 793.2749953254731, 113.3139837797568, 57.18415657413063, 701.6725576725438, 732.3284290942154, 733.072536393024, 721.4535665411165, 735.1516552388409, 723.9802200719854, 725.1293261798088, 70.07094843480228, 52.23379837599085, 158.83114642098627, 727.7386424101087, 722.1349206388421, 675.9986666418606, 716.7548612411215, 721.1745172264744, 639.1648805095839, 765.3146285433136, 789.0288019413432, 72.88501645151729, 103.06276629491262, 796.5601085435954, 809.285791396663, 707.6482939779578, 756.8526057733126, 798.68005878752, 816.8108609041474, 815.6536919549756, 812.8393996443873, 818.7237926091077, 816.425291115024, 818.6342828393123, 813.7826198172609, 820.7003272599402, 812.886176999664, 819.0596860271296, 748.8640428410072, 825.7391862116016, 817.4611699841323, 818.7431901405085, 835.5047935802507, 819.9199577566292, 815.0146720371749, 819.376453815892, 805.3419607248318, 817.3906965587943, 812.1220990910767, 810.8703227892412, 764.0845091853139, 815.2321706222979, 816.6454193368211, 822.1032903939121, 817.1803613788522, 814.6010974398218, 814.8509889035736, 819.9654019693829, 811.2752568890348, 822.902442648614, 817.6309669677644, 819.1344870635891, 802.1799132273374, 776.647879468424, 728.9765365967678, 746.2238676948323, 723.9164199717779, 751.1578526033255, 70.1794762958052, 793.1919648263844, 793.9129625673157, 778.0340222962615, 796.4666904964492, 785.7849510934334, 786.0669908792358, 771.8649625911663, 784.9615093241665, 795.0261350148857, 794.5887537090192, 22.553272909116725, 99.71693942948923, 205.57811115882177, 756.4998504449642, 748.2219310305064, 755.5192686137256, 755.2687773416363, 739.300161114072, 753.0991259963218, 755.405904021908, 757.3887178174912, 702.2875611136737, 747.5951549745865, 99.80089824276021, 177.28184148100644, 746.4198838336883, 745.7563600292348, 741.876793104138, 748.6354098992389, 737.1655042629441, 743.9782442567632, 746.5014627272907, 745.5747597645543, 732.3183939040442, 744.2436034042026, 741.1603514716878, 742.9680611712569, 646.5722312367591, 239.0256086750051, 124.01916625428578, 373.48468788122756, 764.6327805588396, 758.0612310553196, 770.895819994699, 767.6318902346356, 770.2533447487343, 773.411195139071, 737.6594213224488, 791.8337574780356, 792.7934759887509, 787.9713367950433, 791.1023904662699, 719.4194644396375, 731.0137382861064, 710.0937596240276, 130.8756807324193, 268.18111426267717, 675.1646852036606, 695.1738249759767, 726.8223299858562, 784.0729260919456, 674.3724877975558, 737.5513746582673, 783.8142789479147, 87.18629221020099, 679.1016650152022, 772.7895441459677, 724.8234632750339, 789.2970804854261, 789.2684102458857, 783.076332644868, 62.72875472126278, 90.38500910285501, 302.5948639285683, 706.3429912562459, 761.5643870175021, 772.4793916791247, 772.475712735261, 764.7089501097146, 772.2487832847154, 771.606117022219, 766.3972287203135, 779.4880600783622, 761.1307609457797, 767.7881703826476, 774.1648739886115, 770.0867618139332, 779.4150808276314, 771.1398786390162, 771.7726150639613, 766.0644451566518, 705.5512065653643, 69.783975627923, 325.215852632734, 752.6694307589511, 750.2476312093299, 675.3934570446538, 776.9397559323677, 766.1207761259535, 776.4248013784684, 735.7438557211586, 770.6826290888494, 774.2428306643795, 786.1457494537314, 777.7292504141808, 771.3409624729511, 778.5741981838597, 762.824445952187, 780.9327086083844, 774.9988412177553, 773.8487396169248, 756.1411466843938, 769.4775452618343, 764.0022484355691, 683.6915252870429, 52.39597955276684, 92.8238798059344, 203.74503162183177, 333.34425429938403, 800.7070941952218, 806.2630525885273, 815.7180264161086, 819.3964453746595, 825.9615573987076, 791.5301998197297, 799.232449613931, 674.9657705880339, 819.941335124435, 35.51270838679167, 94.43715092137994, 224.23703311020668, 740.6771761635428, 767.4644800198288, 801.1544733346244, 717.6892145267815, 701.2404545254956, 756.9319442236088, 747.0277892176332, 750.3690935529555, 751.56117709479, 753.1241627777396, 101.07506868268428, 602.1188542401349, 777.2899606553956, 771.8776438798604, 790.9461266745712, 789.3159864295725, 772.1857676807263, 806.795032440189, 812.3291796351926, 661.1240366395152, 818.2698344671903, 798.5877837841841, 800.5225153916323, 803.1037836554465, 818.7687602941635, 57.22922963499141, 134.95487545669303, 136.5045406891076, 91.41360890446141, 179.01061128767097, 720.9932203338965, 726.0202550723438, 691.5425830811375, 788.8998758873795, 770.0818122295024, 774.0135282508097, 781.2741397825663, 685.7122261801538, 69.42586033634075, 64.27308571624181, 480.4879389696881, 740.8826332359533, 759.9442502920642, 756.0508567459993, 749.9367682162236, 753.5387917306703, 752.462374611316, 751.8822368711709, 768.0621698060015, 756.1231785216628, 760.7500419720657, 753.1869992245126, 751.6499937823352, 755.9589428066358, 751.0340281690205, 749.6004742199872, 94.67298602597687, 286.8656170813712, 531.0138703264297, 729.2890819367417, 712.8768272009218, 774.8118585203501, 808.3732422520673, 804.806767261325, 796.2073490072281, 799.3069181037124, 61.40732294906446, 261.3255075837909, 767.4858768887119, 782.3665693786278, 738.5295733664419, 786.1765398255458, 348.1605362482351, 742.2578976618596, 24.379128269854466, 606.3687860667814, 735.739458646332, 737.1326574651101, 689.3336069158006, 745.0908321745859, 742.4123298357205, 750.1768648558875, 741.3307118959282, 748.1088375316799, 754.2251110905122, 743.4837181628485, 746.6698874506537, 48.32522783793945, 113.07091257957654, 759.0320009415813, 751.4186775331182, 763.6873465416235, 818.3799718709321, 815.5958977579908, 773.4509576649366, 827.2679396858805, 829.618351243264, 793.0000306361733, 751.8148426359406, 801.9549501867884, 770.7189827710796, 773.5709266120019, 773.1076677507406, 740.8756667672145, 778.0023003616913, 787.4225417298961, 793.7009436678445, 197.48553363605265, 263.78179949292957, 470.72219835048946, 734.7959695571176, 806.1859405119616, 803.6082719705876, 813.6738248103323, 751.5645655837847, 765.2227551349786, 767.9570683715664, 745.2581775276218, 756.2102521974736, 755.8889845692289, 761.419146397901, 34.539085694493714, 164.49424100812024, 724.9376801047717, 724.1523526369236, 733.3669494348533, 779.7149488341676, 783.828310545521, 790.1577434226592, 790.0554519650887, 796.793184682256, 763.7432078835477, 792.6711240580772, 780.7229498147518, 737.0339565306316, 738.3707536560709, 35.82676200508892, 239.20191425274356, 247.89307451796532, 250.80304958618845, 177.7672049210665, 326.65038416817623, 750.0506102387379, 740.871791057349, 742.6113022468746, 800.1589992502047, 663.8400799893407, 775.6530785829735, 775.6024845266209, 18.02291528005725, 113.31910195321124, 251.27830520936405, 416.40121436146046, 704.4394485098364, 758.246104027236, 758.8804605140434, 758.8374962521173, 767.9405686576891, 764.3361154646151, 771.5154718587878, 767.7908552357417, 766.7109746588791, 322.95453897592347, 368.430501456165, 677.7976719879318, 803.0626912493499, 708.2918918303113, 741.0106403114463, 808.9431867134505, 807.670109869243, 804.6574492289276, 804.52290276589, 811.1461948546638, 801.0881723227835, 815.1406000579343, 820.7486043345575, 818.6219722389329, 794.6708661364042, 814.7557787358021, 815.957395002324, 786.5149860620733, 827.7015142650116, 819.2815674320541, 823.9852760338903, 806.0823101317129, 805.9541927465262, 790.9991913145759, 810.9497148295799, 22.58698129824175, 90.52298868706556, 455.3576808732702, 748.6988379385508, 746.0757514446627, 777.638112087432, 801.6934246107797, 801.6101965517232, 798.8921551057124, 802.4261570537683, 785.7116064646941, 264.54769080545196, 696.8452619150466, 810.0048096804916, 808.2591013863158, 759.5035790220861, 768.4128490531128, 742.7428341164111, 730.5137801112064, 695.5434049424223, 731.0874355375494, 737.8572185326792, 724.859309142562, 722.0986858997392, 729.9528982959674, 171.69445074284272, 661.9469760094835, 746.6294177412864, 768.829830476782, 768.7027285733993, 767.7030281235157, 768.8628495034435, 771.7876028744728, 749.8759651562049, 768.7830517069307, 770.6678407922401, 764.2688044223144, 749.7251577734519, 816.5690460286487, 800.5606927169323, 814.6477797857067, 800.6454437392933, 805.406783939936, 23.166823269854767, 132.9325235186944, 516.5560373119301, 810.3020142793355, 808.0203248174677, 709.3260253990264, 740.4880228268593, 805.1582979512241, 804.2037733407611, 807.1409365482676, 807.3930036441764, 730.3422575447909, 622.935049539394, 748.2026787434809, 743.3003561089395, 736.5243264685638, 733.5794856966841, 744.3762460284039, 735.1129213074986, 192.21563548127426, 144.14774978019022, 129.6207591885715, 772.7744677381461, 802.6730128465504, 809.5384247557078, 809.9114516842288, 795.818142452986, 691.1199574883855, 807.4582380248088, 798.172062278011, 806.5720549630832, 705.4369428644419, 743.6840090624446, 731.0888209474183, 744.358762879721, 727.3586071271249, 734.796178648109, 731.1459727278235, 751.5772461846094, 745.7817064500941, 746.2389367085563, 734.335492444881, 733.0716030854588, 732.9884601528628, 737.6929369684547, 737.4124853779106, 728.3248230865877, 731.6884759964136, 708.7395995414137, 742.5257652951382, 40.2681519013805, 211.7440839914581, 736.445298792739, 730.9771948261898, 724.1520224805917, 734.9936239031532, 688.3146299779469, 798.868254095448, 52.58672224695042, 264.9732561207225, 664.4790398688313, 806.7116045653175, 799.4350564155098, 799.6397120013648, 797.8087551741892, 803.0600053271648, 805.1417195992414, 788.4080964746581, 791.3427615998936, 794.9897600266861, 806.2556625697569, 516.5233337244358, 655.878114389246, 738.7505366328726, 765.9698658561651, 777.9756929198783, 54.52645168000763, 694.6215349290279, 798.6714280326736, 805.069426004559, 795.4694612477017, 808.5658071212371, 777.5016414006449, 793.4624780743314, 47.22165195306117, 41.506435622734436, 249.3055211991558, 794.240994362048, 801.3260145723495, 805.0575492534587, 799.7518871255743, 795.6326326096824, 796.8138889828282, 792.473366142908, 752.8477616781674, 725.2766037071489, 730.1577462528312, 750.4819388435847, 744.1077745685076, 739.9563087694196, 740.6877780310322, 252.19833573391935, 31.170358183882215, 54.540135348831335, 696.0317834764844, 701.2983535604935, 713.3816641828623, 798.0125773207717, 811.5834489033738, 779.9256186587364, 757.8345676906983, 758.8298090860994, 750.3228916854051, 748.4291044247921, 763.5275649308986, 740.1888496874742, 757.9170190922409, 48.64646840911381, 210.18911107555593, 231.85803832410483, 748.6382687303266, 748.5662626649756, 756.7465182410273, 754.8648031936295, 754.9568236293302, 794.2703732336058, 773.2957068941392, 788.2528700602733, 728.6600634834615, 792.8485719954048, 792.6926552235435, 799.3972733179633, 780.9218694661242, 807.1334251798886, 779.1094108154148, 787.1236470986062, 792.5608875316957, 22.67275887648862, 750.6075476306255, 681.9964335451562, 752.7795224354383, 751.6918164025897, 808.9763221046695, 754.2378937938087, 802.8136755692301, 818.956755921096, 818.0859637966556, 35.75617670734564, 252.67358419342702, 29.029428420609612, 751.9999942542339, 763.3045909237187, 605.7428695267199, 633.1651573758792, 672.6324128927921, 677.6756036483546, 26.461732250737047, 299.57577574067375, 588.4995463341562, 630.1667317119786, 638.4664540737645, 642.3309321638783, 648.1257373319834, 682.0737476404076, 646.0114488969627, 686.5835678655042, 681.1585614950573, 697.0703721589692, 691.4631655295962, 694.3582408365427, 690.4730601750189, 696.3080954602242, 692.9484569880633, 692.913504122917, 692.7086079986695, 689.2728072755548, 673.7226786902908, 686.2565845826933, 689.8601979460665, 696.6850832226967, 685.3593916933568, 694.476716864214, 697.648237428222, 691.2702155282507, 642.268688153034, 681.172508423665, 677.2659250557704, 632.7115425699575, 678.95654694695, 633.5034671435714, 627.1956326407324, 630.1435620489526, 683.1029705290993, 671.5198184643044, 636.5071477993856, 640.3691370251123, 675.4493266373191, 193.27383315030633, 607.3637751264206, 663.5904320765727, 674.5517080218096, 633.6388493751668, 673.0043255667346, 674.7240839160594, 646.1964808406873, 646.6673744525777, 602.7775670886115, 602.2030954968482, 612.5495475063741, 646.0839191163515, 639.3497564145614, 633.9654927233113, 637.8175118543192, 637.752860734506, 295.6848905309839, 679.0771193879407, 684.196545489054, 641.7231248113596, 627.8277317082703, 285.78644061143547, 645.3391985261248, 631.1859933832442, 576.3190370938659, 689.833625940551, 663.0232413476659, 679.6798825769822, 625.1973038374493, 676.2013822637629, 26.522691766389883, 147.55999965054542, 562.8673941867269, 656.2545233830688, 607.7446259554581, 614.1785580973514, 632.5997017252636, 635.1132801204816, 647.2249106237423, 692.6261721794956, 648.2740038862021, 696.4210927119595, 686.7924135285772, 657.9491191477504, 654.4191363606673, 693.7171034952579, 644.8959276475529, 656.1232920469831, 658.739556635464, 699.8387477309894, 705.7327276635773, 659.3209788831452, 651.8350689265277, 40.83936479401793, 637.4845737478761, 627.4862464741421, 635.8422621032279, 604.7231251524516, 651.4683796887109, 636.6202350548185, 669.7457666350289, 654.0210757647302, 686.0247060222152, 639.2989570958982, 645.7052045180766, 53.5485926759414, 670.1883287457833, 711.4453262197477, 703.5539426216376, 671.8764976783658, 673.7311452471927, 705.997835357354, 635.8615944028127, 716.5663798774493, 704.2106772127984, 661.6335255943194, 677.0013269397817, 713.9947981645287, 667.2075039491301, 651.3200252762559, 671.0762584718383, 675.1324324639271, 708.9924144527629, 663.9167952587068, 665.8786744534792, 664.3989235509495, 709.2770911282762, 617.6070699992154, 685.8479009317776, 652.0597392771386, 705.044147300819, 650.111199668347, 632.8063851271365, 600.9240023326382, 618.203327487306, 610.5143636357384, 653.8934493802042, 654.9305958238615, 608.1454809152068, 620.5468402062293, 620.2020569885526, 648.9282013069391, 658.3891478710067, 606.5565263800303, 640.9194331125176, 603.5348971672361, 679.1350783188389, 640.2242301449974, 687.7205631340862, 584.7542515431668, 639.2221461320366, 688.9304717254863, 648.7359154859297, 679.835921443648, 578.7863810465655, 638.7773690273976, 678.0188324237366, 636.8149400951472, 682.25245040976, 672.8454478319462, 624.6284020122741, 641.7085853728004, 627.5144601030272, 638.6719462189425, 626.3843657998259, 23.989753546067583, 505.17226296862634, 661.8264911634295, 640.402195371615, 637.7925729715963, 632.6688097394155, 642.0604552015544, 603.3838804482938, 627.3990817449424, 682.9273234358324, 629.8434567212959, 619.5386558487887, 680.8440254740191, 677.657169212459, 681.0141707182675, 677.0260309169844, 636.6830194973622, 51.65825208640639, 25.0847177599435, 480.3874424221033, 650.3498320482063, 588.2919826974618, 598.198144579909, 651.0139270078233, 643.1618235035487, 663.7434237248133, 643.4462250524417, 646.3277526901438, 647.3518794114976, 690.0070332499497, 570.0534700005774, 672.5191843793441, 612.0926447539816, 648.200389697204, 142.2364639544272, 161.9247382166087, 113.20753470333459, 621.2253512940177, 619.216274641714, 625.7563832803341, 654.2065327207695, 686.7492705854188, 642.7843005030186, 647.1653136332811, 679.7999292567264, 682.1095996464717, 643.4004907582216, 690.7915129402044, 647.1732024790027, 688.9789743085967, 692.6914920524767, 692.8723135494984, 631.2289824151848, 693.78120213977, 706.6615675614946, 705.1471719544971, 147.39626366449033, 639.7056167082367, 642.3737939897615, 696.311943998657, 648.525837757476, 646.787746719899, 681.67032297723, 634.9285500114194, 652.3569810507352, 642.7565855816288, 694.7689065789408, 647.4064660541922, 656.0393913295899, 691.6152615909515, 648.8096253653197, 676.8711757072325, 626.5313496824554, 655.0453949759664, 657.6269882884134, 651.8380165657195, 643.9486746401607, 636.642278207806, 644.3705517026141, 693.1855987158741, 650.8112227959599, 655.6366453016685, 659.9663384154657, 696.4296242950733, 636.5307964612653, 637.1479572665392, 643.7087116359625, 685.1231384345897, 646.3506429567886, 624.7134364586719, 635.920663848998, 665.2503721864805, 628.4607345512941, 625.2552137354465, 605.7733935581437, 624.2551457986276, 628.7814387910907, 668.2755738213398, 622.0029453440264, 66.5941519808459, 181.0276692850393, 620.3193495531516, 616.3375180559707, 644.1021641862844, 602.5105328356376, 649.7613016477624, 654.9419759921504, 627.401467434505, 621.0171958287109, 599.7435746592507, 610.9419761775057, 606.2290756070576, 610.3511898365815, 23.327144008972997, 409.39161754733897, 590.0785069857908, 598.0718237084828, 637.6038172683379, 556.1507569049306, 633.8781835855917, 652.5846268369168, 628.6537688850353, 636.136073460624, 674.7124060881096, 678.8979045279435, 628.3296577258781, 634.8165387619719, 634.2148754802556, 631.791280761228, 595.1816424860671, 594.9038307731064, 597.5106752956491, 595.6706515441732, 625.8748119240494, 633.4845887818412, 672.7601152953165, 635.8845341824939, 678.0412186571707, 625.6612937390181, 637.8577560583991, 671.9871506763761, 674.7767121639985, 636.754794091279, 675.8348952852884, 674.1376068246008, 640.8391061547488, 637.3922628670229, 103.59018279658022, 170.82590068704104, 146.042736131368, 88.36581154991353, 615.5789090615024, 733.6690987843692, 728.7991859630073, 692.8027259965089, 657.6466045772004, 737.9643784824536, 698.3002801356755, 692.5573950956394, 681.3319975013208, 692.0846383121724, 682.5665136284782, 670.8265743489121, 49.89300445474955, 186.88372895533507, 707.3535974503675, 626.8200217769624, 649.5710737874751, 731.974331620613, 690.4809214232606]
Elapsed: 0.1479960320069822~0.29641106504872833
Time per graph: 0.0030219461492268776~0.006050826034211999
Speed: 645.5796111862538~209.86806931802573
Total Time: 0.0767
best val loss: 0.6930068731307983 test_score: 0.5000

Testing...
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6953 score: 0.5000 time: 0.07s
test Score 0.5000
Epoch Time List: [1.9869935419410467, 4.054680104018189, 0.3512155970092863, 0.32481396314688027, 0.3286691779503599, 0.3318680408410728, 0.32556348701473325, 0.3353743690531701, 0.34137710800860077, 0.3359849948901683, 0.3318568659015, 0.3417895151069388, 0.33118316100444645, 0.35235156607814133, 0.32939576217904687, 0.3568011570023373, 0.35523426497820765, 0.37604613904841244, 0.34999916900414973, 0.39223748084623367, 2.884609259897843, 3.0806974879233167, 1.9325210060924292, 0.6661368969362229, 0.3800672080833465, 0.34027195000089705, 0.3383539420319721, 0.3329651530366391, 0.33185928815510124, 0.338839522912167, 0.3347344050416723, 0.33801154501270503, 0.3470402820967138, 0.3616360240848735, 1.6447780369780958, 3.369594909832813, 2.303258904023096, 1.012641201959923, 0.37734368396922946, 0.34308771905489266, 0.3445110279135406, 0.34175825805868953, 0.33648258983157575, 0.33841254806611687, 0.33936398394871503, 2.12263453588821, 3.0764662319561467, 0.4508116259239614, 0.35516314895357937, 0.3524733129888773, 0.34151945204939693, 0.3300332190701738, 0.32832221896387637, 0.33335594297386706, 0.33515326492488384, 0.342847750056535, 0.43656122812535614, 0.3508594479644671, 0.35152704280335456, 3.895825411193073, 3.150547464028932, 0.3482093879720196, 0.32508996198885143, 0.3252483159303665, 0.33912791789043695, 0.3310684320749715, 0.33289744216017425, 0.3267461770446971, 4.745232988963835, 1.6867911200970411, 0.35971221898216754, 0.3319794898852706, 0.3139907759614289, 0.31921385193709284, 0.3138297600671649, 0.31555359612684697, 0.31658428895752877, 0.3239901940105483, 0.32125419296789914, 0.34174166107550263, 2.1870635750237852, 5.680947924847715, 0.9235442630015314, 0.6113212950294837, 0.3377975800540298, 0.332190784974955, 0.31913594191428274, 0.32520267192739993, 0.3494842710206285, 0.3426810910459608, 0.3135987528366968, 1.0633706880034879, 6.017131041968241, 0.6037950499448925, 0.34147460595704615, 0.32114708004519343, 0.33681063109543175, 0.3261419930495322, 0.31624499696772546, 0.3169881399953738, 0.3349324209848419, 0.336848090053536, 0.3326789540005848, 0.3272469249786809, 2.8716370910406113, 2.1238537849858403, 0.720138965989463, 0.3208837799029425, 0.3145404190290719, 0.3166577339870855, 0.320043065934442, 0.31203744490630925, 0.3116926901275292, 0.3181670579360798, 0.3166189248440787, 0.32379966508597136, 0.32173960108775645, 2.7447007560404018, 3.5546221969416365, 2.412980506196618, 0.33462593401782215, 0.3249718869337812, 0.3179599040886387, 0.3461743200896308, 0.33239010302349925, 2.340799023048021, 6.266156640951522, 0.37086032691877335, 0.3168076389702037, 0.3224344099871814, 0.32199554692488164, 0.32758491090498865, 0.31873609288595617, 0.31879026303067803, 6.3575911150546744, 3.3295092850457877, 1.0438960848841816, 0.34500911890063435, 0.3446670949924737, 0.3435581000521779, 0.3409214620478451, 0.34457146911881864, 0.3424563800217584, 2.7111008579377085, 3.081267736153677, 2.616064192028716, 1.0002684469800442, 0.35463874705601484, 0.3485278739826754, 0.3483607629314065, 0.3580932910554111, 0.37738353991881013, 0.35160409309901297, 0.32170978107023984, 2.644357603043318, 2.6480825021862984, 1.823957686079666, 0.3170012349728495, 0.31778263789601624, 0.3179837348870933, 0.31744955689646304, 0.3124219449236989, 0.3054600750328973, 0.309437278076075, 0.3094345841091126, 0.31088599981740117, 0.30627513001672924, 0.3067826629849151, 0.30511809804011136, 0.30653616809286177, 0.306431423057802, 0.32225059799384326, 0.31448959303088486, 0.3062993149505928, 0.307303695124574, 0.30671644606627524, 0.3074639029800892, 0.3073584239464253, 0.30717267701402307, 0.30832990794442594, 0.30810924794059247, 0.31474190310109407, 0.3075207289075479, 0.3079937739530578, 0.30856328306254, 0.3055100010242313, 0.3064292680937797, 0.30912443494889885, 0.30856312811374664, 0.3131469418294728, 0.30632207391317934, 0.3079114140709862, 0.3068085570121184, 0.3077125329291448, 0.3081315979361534, 0.3093003421090543, 0.32886429398786277, 0.32525431201793253, 0.3346061479533091, 0.33912500797305256, 0.3331078678602353, 5.217154128011316, 3.422703202930279, 0.31686376314610243, 0.3581425059819594, 0.31432787608355284, 0.3185947729507461, 0.3174780890112743, 0.3150269619654864, 0.3173335319152102, 0.3128476190613583, 0.3151279218727723, 2.441614740062505, 3.667576527921483, 1.7318520640255883, 0.8733556040097028, 0.3348576759453863, 0.33232256409246475, 0.3331166470889002, 0.33542429096996784, 0.33122059295419604, 0.3296388591406867, 0.3331540538929403, 0.33419005188625306, 0.3321467631030828, 2.1675933180376887, 3.001053098938428, 0.6521856199251488, 0.33910576708149165, 0.3388834389625117, 0.335367688909173, 0.33955472195520997, 0.3329029369633645, 0.33393122092820704, 0.3336137079168111, 0.3344751870026812, 0.33609154890291393, 0.3349373679375276, 0.3362290539080277, 0.3444046980002895, 5.7869938230142, 1.4801476938882843, 0.48073305597063154, 0.5365775910904631, 0.33313714095856994, 0.32844940095674247, 0.3278804109431803, 0.32808308699168265, 0.33297866908833385, 0.32822780904825777, 0.3158195719588548, 0.3190710488706827, 0.3240110161714256, 0.32095225690864027, 0.3339336421340704, 0.3453233679756522, 0.34476580400951207, 4.7624331309925765, 2.077444403897971, 0.7340354569023475, 0.3563681810628623, 0.3532409109175205, 0.3306945310905576, 0.34358833893202245, 0.3323914440115914, 0.32121908105909824, 0.9348819570150226, 4.593591123120859, 0.33544867706950754, 0.3255298479925841, 0.41009755386039615, 0.3175909760175273, 0.31667433585971594, 2.6564435800537467, 5.2622016940731555, 1.4584333690581843, 0.9133244019467384, 0.3431696919724345, 0.33031751320231706, 0.33012127003166825, 0.333982671960257, 0.32729973597452044, 0.326534082996659, 0.3232978949090466, 0.321721309912391, 0.33846349793020636, 0.3237025741254911, 0.32664125703740865, 0.32135380001273006, 0.3221655760426074, 0.3299883770523593, 0.321054870961234, 0.32748371292836964, 0.33549811504781246, 4.672492607147433, 2.706216943101026, 0.39688792498782277, 0.3436287899967283, 0.3562678409507498, 0.3320559229468927, 0.3290527840144932, 0.3295950230676681, 0.33415364497341216, 0.33444184192921966, 0.3272205670364201, 0.32332348404452205, 0.32411842595320195, 0.33035187504719943, 0.32682202896103263, 0.3246023061219603, 0.32454351091291755, 0.33072601701132953, 0.3253246389795095, 0.3259539850987494, 0.3231784579111263, 0.32772349496372044, 0.3349113971926272, 2.4756317770807073, 4.588531749206595, 0.9906137288780883, 1.5237406060332432, 0.5118806288810447, 0.3129608938470483, 0.3099137820536271, 0.30796463694423437, 0.30500435910653323, 0.3093572029611096, 0.32794641400687397, 0.31762038299348205, 0.3831625909078866, 3.493624092079699, 4.288732627872378, 1.1601292027626187, 0.5199973558774218, 0.3352043479681015, 0.3213305090321228, 0.3329292320413515, 0.3497304110787809, 0.33737566298805177, 0.3404664139961824, 0.33423883002251387, 0.3357547289924696, 0.337240006076172, 5.858254222897813, 1.9629653780721128, 0.36448379303328693, 0.3269275319762528, 0.32349441619589925, 0.32566077809315175, 0.3232417751569301, 0.31717682315502316, 0.3149834780488163, 0.34059020096901804, 0.3181562020909041, 0.3135046809911728, 0.31593067198991776, 0.31240884494036436, 0.31056611402891576, 1.1183270439505577, 4.235970305046067, 2.367965108016506, 1.7671166659565642, 1.9044755639042705, 0.5932571248849854, 0.35070624004583806, 0.34450543904677033, 0.3331119370413944, 0.3319779341109097, 0.32835725299082696, 0.3239322700537741, 0.3381256520515308, 4.809318824089132, 3.160602799966, 1.9528573889983818, 0.49242012994363904, 0.3296959839062765, 0.330072304001078, 0.3336499738506973, 0.3360977699048817, 0.33594637911301106, 0.3348318039206788, 0.3302526921033859, 0.32744630391243845, 0.3306283139390871, 0.33436515112407506, 0.33723521302454174, 0.33186471182852983, 0.3364420550642535, 0.33469079399947077, 7.439309622975998, 2.8608708491083235, 0.7465043219272047, 0.3498115101829171, 0.34960370894987136, 0.3364332920173183, 0.3106542769819498, 0.3204797930084169, 0.32123335008509457, 0.31515936902724206, 1.063678971142508, 6.705888824886642, 0.44589233794249594, 0.3217141979839653, 0.33172117976937443, 0.32726495305541903, 0.39630834199488163, 0.33969609800260514, 4.445263289031573, 1.1472845169482753, 0.8929339059395716, 0.34471687593031675, 0.3450969198020175, 0.34083500609267503, 0.34301378298550844, 0.3396534320199862, 0.3391988640651107, 0.3402927800780162, 0.3437992340186611, 0.33484994899481535, 0.3353179480182007, 2.504654165939428, 3.1126011739252135, 0.9305133830057457, 0.329311657929793, 0.32601689093280584, 0.3082880770089105, 0.3097740029916167, 0.3139313260326162, 0.3103006340097636, 0.3079869420034811, 0.32295203395187855, 0.3201042120344937, 0.31785473111085594, 0.3196048450190574, 0.3267858889885247, 0.329340577009134, 0.33583879203069955, 0.3267793031409383, 0.32203857507556677, 0.3233777058776468, 0.512633147998713, 3.9699682589853182, 0.7220524821896106, 0.3557085449574515, 0.3172227619215846, 0.3116626428673044, 0.3168121710186824, 0.32263528706971556, 0.32785521796904504, 0.32874286104924977, 0.33167116495314986, 0.3335980469128117, 0.33184916607569903, 0.328518892172724, 2.602584370994009, 5.25912816694472, 0.47897268598899245, 0.34951771213673055, 0.3452140368754044, 0.33653178601525724, 0.3217824990861118, 0.32185517996549606, 0.32580967410467565, 0.3183688768185675, 0.32103352108970284, 0.32364923995919526, 0.32098526996560395, 0.33387082116678357, 0.3258235150715336, 6.013799128937535, 2.7661419381620362, 1.0766604531090707, 0.7213211118942127, 2.8941300489241257, 0.7205591569654644, 0.3413990889675915, 0.34198462101630867, 0.3405541640240699, 0.314467801945284, 0.37756254395935684, 0.3293691851431504, 0.3241169760003686, 3.293628849904053, 2.670094058965333, 1.2376392129808664, 0.6491366070695221, 0.5309542840113863, 0.33328498306218535, 0.333496511913836, 0.3285363840404898, 0.3290910959476605, 0.32980400684755296, 0.3269732450135052, 0.3272924260236323, 0.3266902460018173, 3.7875749769154936, 1.9765082550002262, 1.0788543638773263, 0.3213546648621559, 0.3331515190657228, 0.32780369697138667, 0.3183441059663892, 0.31235550506971776, 0.31917143100872636, 0.3163502630777657, 0.31522432703059167, 0.3134144330397248, 0.3107123860390857, 0.3142133679939434, 0.3085497940191999, 0.3125935409916565, 0.31018819008022547, 0.31448534212540835, 0.30974081705790013, 0.3084546298487112, 0.3075312030268833, 0.30616728495806456, 0.3137599481269717, 0.3147684510331601, 0.31977374106645584, 0.3168855680851266, 2.4231920088641346, 6.984865373116918, 0.6718552729580551, 0.46117410296574235, 0.33809371304232627, 0.32190796185750514, 0.3156543489312753, 0.3213746679248288, 0.3135985300177708, 0.31578860396984965, 0.31871780392248183, 3.7311518740607426, 0.6597326329210773, 0.3183055139379576, 0.31328445894178003, 0.3136707858648151, 0.31891025917138904, 0.32818404713179916, 0.3314750362187624, 0.34897219692356884, 0.34309965604916215, 0.3487658069934696, 0.3461848860606551, 0.34946274186950177, 0.3473400310613215, 0.5689590031979606, 3.542123956955038, 0.33593405003193766, 0.3331609619781375, 0.3248737780377269, 0.326241266913712, 0.32606759294867516, 0.3263019820442423, 0.33794820797629654, 0.3296489219646901, 0.32498147105798125, 0.32949609588831663, 0.3168411578517407, 0.3068724221084267, 0.3462334548821673, 0.3103617139859125, 0.3094456698745489, 0.3128171149874106, 2.3870632760226727, 6.017244859132916, 0.6873137339716777, 0.3129660259000957, 0.3090275120921433, 0.3459525549551472, 0.338432333082892, 0.31213094503618777, 0.31354436790570617, 0.3101903371280059, 0.31519103795289993, 0.3206463659880683, 4.056279702926986, 0.3470296000596136, 0.3386880599427968, 0.3458363770041615, 0.3345639209728688, 0.3390996130183339, 0.33827493200078607, 5.285032681887969, 1.393113799043931, 1.374486296903342, 0.4165407521650195, 0.31530171795748174, 0.31579396198503673, 0.3172663009027019, 0.32391817797906697, 0.33488368312828243, 0.3250761158997193, 0.3189348300220445, 0.3131969489622861, 0.3226904020411894, 0.3274328629486263, 0.3508637360064313, 0.34013893199153244, 0.3500736600253731, 0.34103977389167994, 0.35027549508959055, 0.34353615483269095, 0.34186526003759354, 0.3414436799939722, 0.3468203628435731, 0.34977560001425445, 0.34440607496071607, 0.3423707321053371, 0.3417592791374773, 0.3445435839239508, 0.3435964439995587, 0.3471367888851091, 0.3427453129552305, 4.602383463992737, 3.3885515440488234, 0.4782183449715376, 0.3483280761865899, 0.3546119489474222, 0.3463228930486366, 0.3368508570129052, 0.32513448200188577, 2.7959356530336663, 5.48485448397696, 0.602281698025763, 0.3180504138581455, 0.31243672606069595, 0.3170482990099117, 0.31260539405047894, 0.3116736519150436, 0.3141347218770534, 0.31366028101183474, 0.31630541291087866, 0.3159239920787513, 0.3187343979952857, 0.35333704203367233, 0.39010193198919296, 0.3420872238930315, 0.33026785706169903, 0.33301135897636414, 3.2441820120438933, 3.8937837909907103, 0.3275514249689877, 0.31660895701497793, 0.3223344668513164, 0.31642841279972345, 0.32903717702720314, 0.32151768915355206, 4.2478884210577235, 8.317047677002847, 4.70242375601083, 0.32268776604905725, 0.3207979629514739, 0.31819445092696697, 0.32605824596248567, 0.3328151779714972, 0.3164772081654519, 0.3161002459237352, 0.32605727203190327, 0.34467448899522424, 0.3449656961020082, 0.3398275909712538, 0.3470257659209892, 0.34459223388694227, 0.34254280501045287, 0.46876175014767796, 6.480008686077781, 5.17918871389702, 0.8364596180617809, 0.3582692409399897, 0.35435418994165957, 0.3265419719973579, 0.311670582043007, 0.31393740698695183, 0.3285835748538375, 0.3347061360254884, 0.331788980984129, 0.3323637949069962, 0.3333905440522358, 0.3350552619667724, 0.33177829696796834, 2.5531657451065257, 2.5638864981010556, 1.1462689199252054, 1.0958135117543861, 0.33972309390082955, 0.33735545887611806, 0.3352309539914131, 0.3381730748806149, 0.3218234081286937, 0.31863078312017024, 0.3264771798858419, 0.32353332999628037, 0.4145921579329297, 0.3167453099740669, 0.31775234395172447, 0.32070689590182155, 0.3216950960922986, 0.3636241320054978, 0.3196869910461828, 0.32323254796210676, 2.4282772758742794, 5.367974494001828, 0.34611645992845297, 0.33790738496463746, 0.3388454089872539, 0.3218377260491252, 0.3169944480760023, 0.3148530359612778, 0.3147561280056834, 0.3129591761389747, 6.927313965978101, 2.3207666348898783, 3.8062106219585985, 0.6021206928417087, 0.33448707195930183, 0.3410852230153978, 0.33915351203177124, 0.3346021840116009, 0.33967850101180375, 2.608522122958675, 1.4512584330514073, 0.9798279389506206, 0.35317747201770544, 0.34642065106891096, 0.34903497400227934, 0.3533835189882666, 0.3398019728483632, 0.3272724120179191, 0.32151191704906523, 0.3225836631609127, 0.3205480579053983, 0.3216237031156197, 0.3207086119800806, 0.32068869506474584, 0.32160161598585546, 0.3223118409514427, 0.3207367529394105, 0.32183478178922087, 0.32090383290778846, 0.3278535029385239, 0.32536092784721404, 0.32435960101429373, 0.32034416717942804, 0.32245274097658694, 0.32382250286173075, 0.3192845791345462, 0.3212086741114035, 0.32732841093093157, 0.32858862704597414, 0.33734328707214445, 0.34489868592936546, 0.3329072672640905, 0.3405281319282949, 0.34620153391733766, 0.3387967150192708, 0.32749378494918346, 0.34720788698177785, 0.3408029678976163, 0.33822183101437986, 0.33354181807953864, 4.8824094800511375, 0.7594610379310325, 0.3355705259600654, 0.3299853791249916, 0.3496182329254225, 0.33436374098528177, 0.3287274860776961, 0.35644284810405225, 0.3524631450418383, 0.3624053519451991, 0.3549787359079346, 0.3600848140195012, 0.3399945331038907, 0.33280028693843633, 0.33896659209858626, 0.33316927100531757, 0.3453493259148672, 0.4195129689760506, 0.33409485314041376, 0.33575849409680814, 0.34352057485375553, 0.3487876480212435, 0.4342731209471822, 0.3336065079784021, 0.35087865497916937, 0.37092305696569383, 0.3329477831721306, 0.35584633506368846, 0.33361959201283753, 0.34153740003239363, 0.3442562879063189, 3.341617160127498, 2.270852254005149, 0.5751422819448635, 0.35734265809878707, 0.35307041101623327, 0.35229468590114266, 0.3540709401713684, 0.34282311715651304, 0.33394415699876845, 0.32255979301407933, 0.3352182990638539, 0.32149105495773256, 0.3319831689586863, 0.3417165910359472, 0.3245359880384058, 0.3248132619773969, 0.35288646013941616, 0.3403166518546641, 0.3315340200206265, 0.3234289848478511, 0.322518861037679, 0.3323169680079445, 0.32733960496261716, 6.672770249075256, 0.9946741759777069, 0.3434033340308815, 0.3453695150092244, 0.3342571940738708, 0.33344233606476337, 0.33929745899513364, 0.3254821060691029, 0.3328088790876791, 0.3343915978912264, 0.3358261149842292, 0.33919273992069066, 4.64970430592075, 0.9017557050101459, 0.31408255698625, 0.31674134195782244, 0.3260257469955832, 0.32105696690268815, 0.32350182195659727, 0.32896883809007704, 0.3111276251729578, 0.327983764000237, 0.328047810937278, 0.31935377803165466, 0.32308852893766016, 0.3267613409552723, 0.32461403403431177, 0.3212887200061232, 0.3331846441142261, 0.32324059191159904, 0.3288560489891097, 0.31990717607550323, 0.317048077005893, 0.3150223098928109, 0.33807803387753665, 0.33410556986927986, 0.3191786501556635, 0.3294690758921206, 0.3347118600504473, 0.33125416503753513, 0.3497909320285544, 0.354859406943433, 0.3446288959821686, 0.35197804088238627, 0.35187719704117626, 0.3506633900105953, 0.3485729700187221, 0.3423393451375887, 0.346928600105457, 0.35560927307233214, 0.3523607541574165, 0.3605979969725013, 0.364406322943978, 0.35945345100481063, 0.33923743409104645, 0.33224932313896716, 0.34976407303474844, 0.34286749898456037, 0.3228933709906414, 0.32628911698702723, 0.33360885397996753, 0.35621830890886486, 0.34216801915317774, 0.331882705911994, 0.3528786120004952, 0.33403615991119295, 0.3525119940750301, 0.34672515210695565, 0.3394758690847084, 0.3568784228991717, 0.3352527270326391, 0.34670435404405, 2.9068007910391316, 6.109856416936964, 0.36436453997157514, 0.3827221500687301, 0.34036997403018177, 0.34019888390321285, 0.3414728600764647, 0.3405142780393362, 0.41802075295709074, 0.3380273060174659, 0.35192825889680535, 0.34514522878453135, 0.3359995710197836, 0.338010803097859, 0.34524994995445013, 0.3382305040722713, 0.3477003900334239, 1.7332963309017941, 6.700857111951336, 2.7668007239699364, 0.35138492891564965, 0.3345605139620602, 0.3528542109997943, 0.33898923196829855, 0.33453722309786826, 0.3519293899880722, 0.3329720499459654, 0.33299070002976805, 0.3338554910151288, 0.32664830901194364, 0.38252905511762947, 0.34742082410957664, 0.3435438929591328, 0.3326773529406637, 4.916025866987184, 1.7024786599213257, 1.343027553986758, 0.4585350980050862, 0.35132071806583554, 0.3526739680673927, 0.3496097599854693, 0.33700723689980805, 0.34865130798425525, 0.33393999689724296, 0.33593336306512356, 0.3453863550676033, 0.3367677719797939, 0.3305356941418722, 0.3414442519424483, 0.3329880378441885, 0.3325240359408781, 0.3253429619362578, 0.3488806380191818, 0.3362262628506869, 0.3354366699932143, 0.3304235889809206, 4.824711579014547, 0.42598160612396896, 0.33528572507202625, 0.3377020718762651, 0.3364171989960596, 0.33819224406033754, 0.3318297481164336, 0.33944719401188195, 0.3242221559630707, 0.33291411492973566, 0.3348482829751447, 0.33930316800251603, 0.32880594104062766, 0.3207556229317561, 0.33587818196974695, 0.3280712260166183, 0.34041497204452753, 0.33404559502378106, 0.33557505207136273, 0.3273042062064633, 0.3376535839634016, 0.3416641710791737, 0.3367329438915476, 0.3375199328875169, 0.3363256258890033, 0.33485957398079336, 0.32792222406715155, 0.33095979201607406, 0.342177381971851, 0.3355930781690404, 0.34113942098338157, 0.3399563949787989, 0.35154887405224144, 0.3473416368942708, 0.34809163911268115, 0.336374006816186, 0.35123608505818993, 0.35085445386357605, 0.3636886429740116, 0.35728185705374926, 0.3440345859853551, 0.3412594071123749, 0.3613069709390402, 4.688758150092326, 1.4943051749141887, 0.3616124619729817, 0.40191269305069, 0.3579738310072571, 0.36839943507220596, 0.3622546070255339, 0.3634813450044021, 0.3653947140555829, 0.355837486917153, 0.35342681396286935, 0.357364634051919, 0.3549605649895966, 0.36497532902285457, 4.668545132968575, 4.284861559048295, 0.37366932502482086, 0.3671897890744731, 0.36507744702976197, 0.37016059714369476, 0.3459952468983829, 0.3357493708608672, 0.3393551552435383, 0.3437807329464704, 0.3358190511353314, 0.3401236170902848, 0.3387375519378111, 0.34289564099162817, 0.3640266091097146, 0.3706964689772576, 0.3794508029241115, 0.3622921959031373, 0.36727362300734967, 0.36835754790809005, 0.353191732079722, 0.34045767900533974, 0.3352574669988826, 0.34502725896891207, 0.34106306300964206, 0.35634481196757406, 0.33991391595918685, 0.3380327189806849, 0.34617722092662007, 0.34496963198762387, 0.34535278601106256, 0.34528264007531106, 0.3431900420691818, 0.33179687603842467, 2.3476221808232367, 2.3620109900366515, 2.2630485051777214, 1.499155221041292, 1.6870192418573424, 0.3272219281643629, 0.3248136210022494, 0.3300968579715118, 0.3315480889286846, 0.33357097301632166, 0.34154008503537625, 0.3353092110482976, 0.33412054495420307, 0.32717582397162914, 0.32967169396579266, 0.34229346306528896, 5.320094430935569, 1.8020546429324895, 1.2128115468658507, 0.3381723379716277, 0.3517844390589744, 0.3230165680870414, 0.3225112200016156]
Total Epoch List: [716, 318, 23]
Total Time List: [0.06605594896245748, 0.07941902708262205, 0.0767288759816438]
========================training times:1========================
========================k_idx:0========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: AEtransGAT
AEtransGAT(
  (gae): GAE(
    (encoder): transGAT(
      (encoder): EdgeFeatureGAT()
      (classifier): Sequential(
        (0): Linear(in_features=64, out_features=64, bias=True)
        (1): ReLU(inplace=True)
        (2): Linear(in_features=64, out_features=2, bias=True)
      )
    )
    (decoder): InnerProductDecoder()
  )
)
Total number of parameters: 1914242
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x75feb28b1900>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 2.8174;  Loss pred: 2.8174; Loss self: 0.0000; time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6977 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6946 score: 0.5102 time: 0.06s
Epoch 2/1000, LR 0.000000
Train loss: 2.7840;  Loss pred: 2.7840; Loss self: 0.0000; time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6976 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6945 score: 0.5102 time: 0.06s
Epoch 3/1000, LR 0.000030
Train loss: 2.8055;  Loss pred: 2.8055; Loss self: 0.0000; time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6975 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6944 score: 0.5102 time: 0.06s
Epoch 4/1000, LR 0.000060
Train loss: 2.8055;  Loss pred: 2.8055; Loss self: 0.0000; time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6972 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6943 score: 0.5102 time: 0.06s
Epoch 5/1000, LR 0.000090
Train loss: 2.7248;  Loss pred: 2.7248; Loss self: 0.0000; time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6969 score: 0.4898 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6941 score: 0.5102 time: 0.06s
Epoch 6/1000, LR 0.000120
Train loss: 2.7129;  Loss pred: 2.7129; Loss self: 0.0000; time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6965 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6939 score: 0.5102 time: 0.06s
Epoch 7/1000, LR 0.000150
Train loss: 2.7202;  Loss pred: 2.7202; Loss self: 0.0000; time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6961 score: 0.4898 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6936 score: 0.5102 time: 0.06s
Epoch 8/1000, LR 0.000180
Train loss: 2.6624;  Loss pred: 2.6624; Loss self: 0.0000; time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6957 score: 0.4898 time: 0.10s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6934 score: 0.5102 time: 0.06s
Epoch 9/1000, LR 0.000210
Train loss: 2.6362;  Loss pred: 2.6362; Loss self: 0.0000; time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6953 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6933 score: 0.5102 time: 0.06s
Epoch 10/1000, LR 0.000240
Train loss: 2.5965;  Loss pred: 2.5965; Loss self: 0.0000; time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6949 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6931 score: 0.5102 time: 0.06s
Epoch 11/1000, LR 0.000270
Train loss: 2.5343;  Loss pred: 2.5343; Loss self: 0.0000; time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6945 score: 0.4898 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6930 score: 0.5102 time: 1.15s
Epoch 12/1000, LR 0.000270
Train loss: 2.5380;  Loss pred: 2.5380; Loss self: 0.0000; time: 6.46s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6942 score: 0.4898 time: 0.89s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6928 score: 0.5102 time: 0.71s
Epoch 13/1000, LR 0.000270
Train loss: 2.4733;  Loss pred: 2.4733; Loss self: 0.0000; time: 1.87s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6939 score: 0.4898 time: 0.34s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6928 score: 0.5102 time: 0.45s
Epoch 14/1000, LR 0.000270
Train loss: 2.4419;  Loss pred: 2.4419; Loss self: 0.0000; time: 0.70s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6937 score: 0.4898 time: 0.45s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6927 score: 0.5102 time: 0.20s
Epoch 15/1000, LR 0.000270
Train loss: 2.4158;  Loss pred: 2.4158; Loss self: 0.0000; time: 0.66s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6935 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6927 score: 0.5102 time: 0.06s
Epoch 16/1000, LR 0.000270
Train loss: 2.3311;  Loss pred: 2.3311; Loss self: 0.0000; time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6933 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6927 score: 0.5102 time: 0.06s
Epoch 17/1000, LR 0.000270
Train loss: 2.3221;  Loss pred: 2.3221; Loss self: 0.0000; time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6931 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6927 score: 0.5102 time: 0.06s
Epoch 18/1000, LR 0.000270
Train loss: 2.2637;  Loss pred: 2.2637; Loss self: 0.0000; time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6930 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6927 score: 0.5102 time: 0.06s
Epoch 19/1000, LR 0.000270
Train loss: 2.2292;  Loss pred: 2.2292; Loss self: 0.0000; time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6929 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6927 score: 0.5102 time: 0.06s
Epoch 20/1000, LR 0.000270
Train loss: 2.1855;  Loss pred: 2.1855; Loss self: 0.0000; time: 0.20s
Val loss: 0.6928 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6928 score: 0.5102 time: 0.06s
Epoch 21/1000, LR 0.000270
Train loss: 2.1452;  Loss pred: 2.1452; Loss self: 0.0000; time: 0.19s
Val loss: 0.6927 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6928 score: 0.5102 time: 0.06s
Epoch 22/1000, LR 0.000270
Train loss: 2.1095;  Loss pred: 2.1095; Loss self: 0.0000; time: 0.20s
Val loss: 0.6927 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6928 score: 0.5102 time: 0.06s
Epoch 23/1000, LR 0.000270
Train loss: 2.0753;  Loss pred: 2.0753; Loss self: 0.0000; time: 0.20s
Val loss: 0.6926 score: 0.7755 time: 0.07s
Test loss: 0.6928 score: 0.6939 time: 0.06s
Epoch 24/1000, LR 0.000270
Train loss: 2.0301;  Loss pred: 2.0301; Loss self: 0.0000; time: 4.82s
Val loss: 0.6925 score: 0.5306 time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6928 score: 0.4898 time: 0.22s
Epoch 25/1000, LR 0.000270
Train loss: 1.9995;  Loss pred: 1.9995; Loss self: 0.0000; time: 0.79s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6925 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6928 score: 0.4898 time: 0.06s
Epoch 26/1000, LR 0.000270
Train loss: 1.9864;  Loss pred: 1.9864; Loss self: 0.0000; time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6924 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6928 score: 0.4898 time: 0.06s
Epoch 27/1000, LR 0.000270
Train loss: 1.9242;  Loss pred: 1.9242; Loss self: 0.0000; time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6923 score: 0.5102 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6928 score: 0.4898 time: 0.06s
Epoch 28/1000, LR 0.000270
Train loss: 1.9133;  Loss pred: 1.9133; Loss self: 0.0000; time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6923 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6928 score: 0.4898 time: 0.06s
Epoch 29/1000, LR 0.000270
Train loss: 1.8684;  Loss pred: 1.8684; Loss self: 0.0000; time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6922 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6927 score: 0.4898 time: 0.06s
Epoch 30/1000, LR 0.000270
Train loss: 1.8526;  Loss pred: 1.8526; Loss self: 0.0000; time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6921 score: 0.5102 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6927 score: 0.4898 time: 0.06s
Epoch 31/1000, LR 0.000270
Train loss: 1.8192;  Loss pred: 1.8192; Loss self: 0.0000; time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6921 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6927 score: 0.4898 time: 0.06s
Epoch 32/1000, LR 0.000270
Train loss: 1.7806;  Loss pred: 1.7806; Loss self: 0.0000; time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6920 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6926 score: 0.4898 time: 0.06s
Epoch 33/1000, LR 0.000270
Train loss: 1.7812;  Loss pred: 1.7812; Loss self: 0.0000; time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6920 score: 0.5102 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6926 score: 0.4898 time: 0.06s
Epoch 34/1000, LR 0.000270
Train loss: 1.7590;  Loss pred: 1.7590; Loss self: 0.0000; time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6919 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6925 score: 0.4898 time: 0.06s
Epoch 35/1000, LR 0.000270
Train loss: 1.7126;  Loss pred: 1.7126; Loss self: 0.0000; time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6918 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6925 score: 0.4898 time: 0.06s
Epoch 36/1000, LR 0.000270
Train loss: 1.6918;  Loss pred: 1.6918; Loss self: 0.0000; time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6918 score: 0.5102 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6924 score: 0.4898 time: 0.06s
Epoch 37/1000, LR 0.000270
Train loss: 1.6576;  Loss pred: 1.6576; Loss self: 0.0000; time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6917 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6924 score: 0.4898 time: 0.06s
Epoch 38/1000, LR 0.000270
Train loss: 1.6362;  Loss pred: 1.6362; Loss self: 0.0000; time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6917 score: 0.5102 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6923 score: 0.4898 time: 0.06s
Epoch 39/1000, LR 0.000269
Train loss: 1.6252;  Loss pred: 1.6252; Loss self: 0.0000; time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6916 score: 0.5102 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6923 score: 0.4898 time: 0.06s
Epoch 40/1000, LR 0.000269
Train loss: 1.5940;  Loss pred: 1.5940; Loss self: 0.0000; time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6915 score: 0.5102 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6922 score: 0.4898 time: 0.06s
Epoch 41/1000, LR 0.000269
Train loss: 1.5696;  Loss pred: 1.5696; Loss self: 0.0000; time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6915 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6921 score: 0.4898 time: 0.06s
Epoch 42/1000, LR 0.000269
Train loss: 1.5622;  Loss pred: 1.5622; Loss self: 0.0000; time: 0.21s
Val loss: 0.6914 score: 0.5714 time: 0.07s
Test loss: 0.6921 score: 0.5510 time: 0.06s
Epoch 43/1000, LR 0.000269
Train loss: 1.5315;  Loss pred: 1.5315; Loss self: 0.0000; time: 0.21s
Val loss: 0.6913 score: 0.8571 time: 0.07s
Test loss: 0.6920 score: 0.7551 time: 0.06s
Epoch 44/1000, LR 0.000269
Train loss: 1.5348;  Loss pred: 1.5348; Loss self: 0.0000; time: 0.21s
Val loss: 0.6912 score: 0.9184 time: 0.07s
Test loss: 0.6919 score: 0.8571 time: 0.06s
Epoch 45/1000, LR 0.000269
Train loss: 1.4932;  Loss pred: 1.4932; Loss self: 0.0000; time: 0.21s
Val loss: 0.6911 score: 0.9388 time: 0.42s
Test loss: 0.6918 score: 0.7959 time: 0.69s
Epoch 46/1000, LR 0.000269
Train loss: 1.4746;  Loss pred: 1.4746; Loss self: 0.0000; time: 2.24s
Val loss: 0.6911 score: 0.7347 time: 1.96s
Test loss: 0.6917 score: 0.6939 time: 0.34s
Epoch 47/1000, LR 0.000269
Train loss: 1.4569;  Loss pred: 1.4569; Loss self: 0.0000; time: 1.49s
Val loss: 0.6910 score: 0.6531 time: 0.28s
Test loss: 0.6916 score: 0.6735 time: 0.39s
Epoch 48/1000, LR 0.000269
Train loss: 1.4414;  Loss pred: 1.4414; Loss self: 0.0000; time: 0.69s
Val loss: 0.6908 score: 0.6327 time: 0.08s
Test loss: 0.6915 score: 0.6122 time: 0.07s
Epoch 49/1000, LR 0.000269
Train loss: 1.4334;  Loss pred: 1.4334; Loss self: 0.0000; time: 0.21s
Val loss: 0.6907 score: 0.6327 time: 0.07s
Test loss: 0.6914 score: 0.5918 time: 0.06s
Epoch 50/1000, LR 0.000269
Train loss: 1.4152;  Loss pred: 1.4152; Loss self: 0.0000; time: 0.21s
Val loss: 0.6905 score: 0.6327 time: 0.07s
Test loss: 0.6913 score: 0.5918 time: 0.06s
Epoch 51/1000, LR 0.000269
Train loss: 1.4012;  Loss pred: 1.4012; Loss self: 0.0000; time: 0.21s
Val loss: 0.6904 score: 0.6327 time: 0.08s
Test loss: 0.6912 score: 0.5918 time: 0.07s
Epoch 52/1000, LR 0.000269
Train loss: 1.3913;  Loss pred: 1.3913; Loss self: 0.0000; time: 0.21s
Val loss: 0.6903 score: 0.6327 time: 0.07s
Test loss: 0.6911 score: 0.5918 time: 0.06s
Epoch 53/1000, LR 0.000269
Train loss: 1.3557;  Loss pred: 1.3557; Loss self: 0.0000; time: 0.21s
Val loss: 0.6902 score: 0.6327 time: 0.07s
Test loss: 0.6910 score: 0.5918 time: 0.06s
Epoch 54/1000, LR 0.000269
Train loss: 1.3572;  Loss pred: 1.3572; Loss self: 0.0000; time: 0.21s
Val loss: 0.6900 score: 0.6327 time: 0.07s
Test loss: 0.6909 score: 0.6122 time: 0.06s
Epoch 55/1000, LR 0.000269
Train loss: 1.3395;  Loss pred: 1.3395; Loss self: 0.0000; time: 0.21s
Val loss: 0.6899 score: 0.6327 time: 0.07s
Test loss: 0.6908 score: 0.6122 time: 0.08s
Epoch 56/1000, LR 0.000269
Train loss: 1.3294;  Loss pred: 1.3294; Loss self: 0.0000; time: 0.22s
Val loss: 0.6898 score: 0.6327 time: 0.09s
Test loss: 0.6907 score: 0.6122 time: 0.07s
Epoch 57/1000, LR 0.000269
Train loss: 1.3272;  Loss pred: 1.3272; Loss self: 0.0000; time: 0.23s
Val loss: 0.6896 score: 0.6735 time: 0.73s
Test loss: 0.6906 score: 0.6122 time: 0.92s
Epoch 58/1000, LR 0.000269
Train loss: 1.3059;  Loss pred: 1.3059; Loss self: 0.0000; time: 1.14s
Val loss: 0.6895 score: 0.6735 time: 0.30s
Test loss: 0.6905 score: 0.6122 time: 1.24s
Epoch 59/1000, LR 0.000268
Train loss: 1.3012;  Loss pred: 1.3012; Loss self: 0.0000; time: 1.57s
Val loss: 0.6893 score: 0.6735 time: 0.27s
Test loss: 0.6904 score: 0.6122 time: 0.34s
Epoch 60/1000, LR 0.000268
Train loss: 1.2883;  Loss pred: 1.2883; Loss self: 0.0000; time: 0.92s
Val loss: 0.6892 score: 0.6735 time: 0.07s
Test loss: 0.6903 score: 0.6735 time: 0.06s
Epoch 61/1000, LR 0.000268
Train loss: 1.2779;  Loss pred: 1.2779; Loss self: 0.0000; time: 0.20s
Val loss: 0.6890 score: 0.6939 time: 0.08s
Test loss: 0.6902 score: 0.6735 time: 0.06s
Epoch 62/1000, LR 0.000268
Train loss: 1.2699;  Loss pred: 1.2699; Loss self: 0.0000; time: 0.20s
Val loss: 0.6888 score: 0.6939 time: 0.08s
Test loss: 0.6900 score: 0.6735 time: 0.06s
Epoch 63/1000, LR 0.000268
Train loss: 1.2490;  Loss pred: 1.2490; Loss self: 0.0000; time: 0.20s
Val loss: 0.6886 score: 0.6939 time: 0.07s
Test loss: 0.6899 score: 0.6735 time: 0.06s
Epoch 64/1000, LR 0.000268
Train loss: 1.2437;  Loss pred: 1.2437; Loss self: 0.0000; time: 0.20s
Val loss: 0.6884 score: 0.7143 time: 0.07s
Test loss: 0.6898 score: 0.6939 time: 0.06s
Epoch 65/1000, LR 0.000268
Train loss: 1.2409;  Loss pred: 1.2409; Loss self: 0.0000; time: 0.20s
Val loss: 0.6882 score: 0.7347 time: 0.07s
Test loss: 0.6896 score: 0.6939 time: 0.06s
Epoch 66/1000, LR 0.000268
Train loss: 1.2305;  Loss pred: 1.2305; Loss self: 0.0000; time: 0.20s
Val loss: 0.6880 score: 0.7347 time: 0.07s
Test loss: 0.6895 score: 0.7143 time: 0.06s
Epoch 67/1000, LR 0.000268
Train loss: 1.2156;  Loss pred: 1.2156; Loss self: 0.0000; time: 0.19s
Val loss: 0.6878 score: 0.7551 time: 0.06s
Test loss: 0.6893 score: 0.7143 time: 0.06s
Epoch 68/1000, LR 0.000268
Train loss: 1.2093;  Loss pred: 1.2093; Loss self: 0.0000; time: 0.19s
Val loss: 0.6876 score: 0.7551 time: 0.06s
Test loss: 0.6892 score: 0.7143 time: 0.06s
Epoch 69/1000, LR 0.000268
Train loss: 1.2050;  Loss pred: 1.2050; Loss self: 0.0000; time: 1.46s
Val loss: 0.6874 score: 0.7755 time: 1.49s
Test loss: 0.6890 score: 0.7347 time: 0.85s
Epoch 70/1000, LR 0.000268
Train loss: 1.1990;  Loss pred: 1.1990; Loss self: 0.0000; time: 6.26s
Val loss: 0.6872 score: 0.7959 time: 1.42s
Test loss: 0.6888 score: 0.7347 time: 0.44s
Epoch 71/1000, LR 0.000268
Train loss: 1.1840;  Loss pred: 1.1840; Loss self: 0.0000; time: 0.25s
Val loss: 0.6869 score: 0.8367 time: 0.12s
Test loss: 0.6887 score: 0.7347 time: 0.06s
Epoch 72/1000, LR 0.000267
Train loss: 1.1876;  Loss pred: 1.1876; Loss self: 0.0000; time: 0.21s
Val loss: 0.6867 score: 0.8571 time: 0.08s
Test loss: 0.6885 score: 0.7143 time: 0.06s
Epoch 73/1000, LR 0.000267
Train loss: 1.1750;  Loss pred: 1.1750; Loss self: 0.0000; time: 0.21s
Val loss: 0.6864 score: 0.8571 time: 0.08s
Test loss: 0.6883 score: 0.7143 time: 0.06s
Epoch 74/1000, LR 0.000267
Train loss: 1.1756;  Loss pred: 1.1756; Loss self: 0.0000; time: 0.21s
Val loss: 0.6861 score: 0.8776 time: 0.08s
Test loss: 0.6881 score: 0.7755 time: 0.06s
Epoch 75/1000, LR 0.000267
Train loss: 1.1629;  Loss pred: 1.1629; Loss self: 0.0000; time: 0.21s
Val loss: 0.6858 score: 0.8776 time: 0.07s
Test loss: 0.6879 score: 0.7755 time: 0.07s
Epoch 76/1000, LR 0.000267
Train loss: 1.1577;  Loss pred: 1.1577; Loss self: 0.0000; time: 0.22s
Val loss: 0.6856 score: 0.8980 time: 0.07s
Test loss: 0.6877 score: 0.7755 time: 0.07s
Epoch 77/1000, LR 0.000267
Train loss: 1.1508;  Loss pred: 1.1508; Loss self: 0.0000; time: 3.64s
Val loss: 0.6853 score: 0.9184 time: 1.04s
Test loss: 0.6875 score: 0.7755 time: 1.62s
Epoch 78/1000, LR 0.000267
Train loss: 1.1441;  Loss pred: 1.1441; Loss self: 0.0000; time: 0.70s
Val loss: 0.6849 score: 0.9388 time: 0.08s
Test loss: 0.6872 score: 0.8163 time: 0.06s
Epoch 79/1000, LR 0.000267
Train loss: 1.1482;  Loss pred: 1.1482; Loss self: 0.0000; time: 0.21s
Val loss: 0.6846 score: 0.9796 time: 0.06s
Test loss: 0.6870 score: 0.8163 time: 0.06s
Epoch 80/1000, LR 0.000267
Train loss: 1.1369;  Loss pred: 1.1369; Loss self: 0.0000; time: 0.20s
Val loss: 0.6843 score: 0.9796 time: 0.07s
Test loss: 0.6868 score: 0.8163 time: 0.06s
Epoch 81/1000, LR 0.000267
Train loss: 1.1272;  Loss pred: 1.1272; Loss self: 0.0000; time: 0.20s
Val loss: 0.6839 score: 0.9796 time: 0.08s
Test loss: 0.6865 score: 0.8163 time: 0.06s
Epoch 82/1000, LR 0.000267
Train loss: 1.1226;  Loss pred: 1.1226; Loss self: 0.0000; time: 0.20s
Val loss: 0.6835 score: 0.9796 time: 0.07s
Test loss: 0.6863 score: 0.8163 time: 0.06s
Epoch 83/1000, LR 0.000266
Train loss: 1.1206;  Loss pred: 1.1206; Loss self: 0.0000; time: 0.22s
Val loss: 0.6832 score: 0.9796 time: 0.08s
Test loss: 0.6860 score: 0.8367 time: 0.06s
Epoch 84/1000, LR 0.000266
Train loss: 1.1179;  Loss pred: 1.1179; Loss self: 0.0000; time: 0.22s
Val loss: 0.6828 score: 0.9796 time: 0.07s
Test loss: 0.6857 score: 0.8571 time: 0.06s
Epoch 85/1000, LR 0.000266
Train loss: 1.1120;  Loss pred: 1.1120; Loss self: 0.0000; time: 0.21s
Val loss: 0.6824 score: 0.9796 time: 0.07s
Test loss: 0.6854 score: 0.8571 time: 0.06s
Epoch 86/1000, LR 0.000266
Train loss: 1.0985;  Loss pred: 1.0985; Loss self: 0.0000; time: 0.20s
Val loss: 0.6819 score: 0.9796 time: 0.07s
Test loss: 0.6852 score: 0.8776 time: 0.06s
Epoch 87/1000, LR 0.000266
Train loss: 1.1035;  Loss pred: 1.1035; Loss self: 0.0000; time: 0.20s
Val loss: 0.6815 score: 0.9796 time: 0.07s
Test loss: 0.6848 score: 0.8776 time: 0.06s
Epoch 88/1000, LR 0.000266
Train loss: 1.1014;  Loss pred: 1.1014; Loss self: 0.0000; time: 0.20s
Val loss: 0.6811 score: 0.9796 time: 0.07s
Test loss: 0.6845 score: 0.8980 time: 0.06s
Epoch 89/1000, LR 0.000266
Train loss: 1.0943;  Loss pred: 1.0943; Loss self: 0.0000; time: 0.21s
Val loss: 0.6806 score: 0.9796 time: 0.08s
Test loss: 0.6842 score: 0.8980 time: 0.06s
Epoch 90/1000, LR 0.000266
Train loss: 1.0957;  Loss pred: 1.0957; Loss self: 0.0000; time: 0.21s
Val loss: 0.6801 score: 0.9796 time: 0.07s
Test loss: 0.6839 score: 0.8980 time: 0.06s
Epoch 91/1000, LR 0.000266
Train loss: 1.0868;  Loss pred: 1.0868; Loss self: 0.0000; time: 0.21s
Val loss: 0.6796 score: 0.9796 time: 0.08s
Test loss: 0.6835 score: 0.8980 time: 0.06s
Epoch 92/1000, LR 0.000266
Train loss: 1.0849;  Loss pred: 1.0849; Loss self: 0.0000; time: 0.81s
Val loss: 0.6791 score: 0.9796 time: 0.55s
Test loss: 0.6831 score: 0.8980 time: 0.85s
Epoch 93/1000, LR 0.000265
Train loss: 1.0788;  Loss pred: 1.0788; Loss self: 0.0000; time: 1.17s
Val loss: 0.6786 score: 0.9796 time: 0.26s
Test loss: 0.6828 score: 0.8980 time: 0.51s
Epoch 94/1000, LR 0.000265
Train loss: 1.0746;  Loss pred: 1.0746; Loss self: 0.0000; time: 0.22s
Val loss: 0.6780 score: 0.9796 time: 0.08s
Test loss: 0.6824 score: 0.8980 time: 0.06s
Epoch 95/1000, LR 0.000265
Train loss: 1.0745;  Loss pred: 1.0745; Loss self: 0.0000; time: 0.21s
Val loss: 0.6775 score: 0.9796 time: 0.08s
Test loss: 0.6819 score: 0.8980 time: 0.06s
Epoch 96/1000, LR 0.000265
Train loss: 1.0716;  Loss pred: 1.0716; Loss self: 0.0000; time: 0.21s
Val loss: 0.6769 score: 0.9796 time: 0.07s
Test loss: 0.6815 score: 0.8980 time: 0.07s
Epoch 97/1000, LR 0.000265
Train loss: 1.0664;  Loss pred: 1.0664; Loss self: 0.0000; time: 0.21s
Val loss: 0.6763 score: 0.9796 time: 0.08s
Test loss: 0.6811 score: 0.8980 time: 0.07s
Epoch 98/1000, LR 0.000265
Train loss: 1.0649;  Loss pred: 1.0649; Loss self: 0.0000; time: 0.21s
Val loss: 0.6757 score: 0.9796 time: 0.08s
Test loss: 0.6806 score: 0.8980 time: 0.07s
Epoch 99/1000, LR 0.000265
Train loss: 1.0642;  Loss pred: 1.0642; Loss self: 0.0000; time: 0.21s
Val loss: 0.6751 score: 0.9796 time: 0.07s
Test loss: 0.6802 score: 0.8980 time: 0.06s
Epoch 100/1000, LR 0.000265
Train loss: 1.0566;  Loss pred: 1.0566; Loss self: 0.0000; time: 0.21s
Val loss: 0.6744 score: 0.9796 time: 0.07s
Test loss: 0.6797 score: 0.8980 time: 0.06s
Epoch 101/1000, LR 0.000265
Train loss: 1.0546;  Loss pred: 1.0546; Loss self: 0.0000; time: 0.21s
Val loss: 0.6737 score: 0.9796 time: 0.07s
Test loss: 0.6792 score: 0.8980 time: 0.06s
Epoch 102/1000, LR 0.000264
Train loss: 1.0519;  Loss pred: 1.0519; Loss self: 0.0000; time: 0.21s
Val loss: 0.6730 score: 0.9796 time: 0.07s
Test loss: 0.6787 score: 0.8980 time: 0.06s
Epoch 103/1000, LR 0.000264
Train loss: 1.0476;  Loss pred: 1.0476; Loss self: 0.0000; time: 0.21s
Val loss: 0.6723 score: 0.9796 time: 0.07s
Test loss: 0.6781 score: 0.8776 time: 0.06s
Epoch 104/1000, LR 0.000264
Train loss: 1.0454;  Loss pred: 1.0454; Loss self: 0.0000; time: 0.21s
Val loss: 0.6715 score: 0.9796 time: 0.07s
Test loss: 0.6776 score: 0.8776 time: 0.06s
Epoch 105/1000, LR 0.000264
Train loss: 1.0469;  Loss pred: 1.0469; Loss self: 0.0000; time: 0.21s
Val loss: 0.6707 score: 0.9796 time: 0.07s
Test loss: 0.6770 score: 0.8776 time: 0.06s
Epoch 106/1000, LR 0.000264
Train loss: 1.0426;  Loss pred: 1.0426; Loss self: 0.0000; time: 0.21s
Val loss: 0.6699 score: 0.9796 time: 0.07s
Test loss: 0.6764 score: 0.8776 time: 0.06s
Epoch 107/1000, LR 0.000264
Train loss: 1.0428;  Loss pred: 1.0428; Loss self: 0.0000; time: 0.21s
Val loss: 0.6690 score: 0.9796 time: 0.07s
Test loss: 0.6757 score: 0.8776 time: 0.06s
Epoch 108/1000, LR 0.000264
Train loss: 1.0409;  Loss pred: 1.0409; Loss self: 0.0000; time: 0.21s
Val loss: 0.6682 score: 0.9796 time: 0.07s
Test loss: 0.6751 score: 0.8776 time: 0.06s
Epoch 109/1000, LR 0.000264
Train loss: 1.0316;  Loss pred: 1.0316; Loss self: 0.0000; time: 0.21s
Val loss: 0.6673 score: 0.9796 time: 0.07s
Test loss: 0.6744 score: 0.8776 time: 0.07s
Epoch 110/1000, LR 0.000263
Train loss: 1.0341;  Loss pred: 1.0341; Loss self: 0.0000; time: 0.22s
Val loss: 0.6664 score: 0.9796 time: 0.08s
Test loss: 0.6738 score: 0.8776 time: 0.06s
Epoch 111/1000, LR 0.000263
Train loss: 1.0311;  Loss pred: 1.0311; Loss self: 0.0000; time: 3.46s
Val loss: 0.6654 score: 0.9796 time: 0.55s
Test loss: 0.6731 score: 0.8776 time: 0.26s
Epoch 112/1000, LR 0.000263
Train loss: 1.0295;  Loss pred: 1.0295; Loss self: 0.0000; time: 0.46s
Val loss: 0.6645 score: 0.9796 time: 0.08s
Test loss: 0.6723 score: 0.8776 time: 0.06s
Epoch 113/1000, LR 0.000263
Train loss: 1.0251;  Loss pred: 1.0251; Loss self: 0.0000; time: 0.21s
Val loss: 0.6635 score: 0.9796 time: 0.07s
Test loss: 0.6716 score: 0.8776 time: 0.07s
Epoch 114/1000, LR 0.000263
Train loss: 1.0242;  Loss pred: 1.0242; Loss self: 0.0000; time: 0.21s
Val loss: 0.6624 score: 0.9796 time: 0.07s
Test loss: 0.6708 score: 0.8776 time: 0.06s
Epoch 115/1000, LR 0.000263
Train loss: 1.0218;  Loss pred: 1.0218; Loss self: 0.0000; time: 0.21s
Val loss: 0.6614 score: 0.9796 time: 0.07s
Test loss: 0.6700 score: 0.8776 time: 0.06s
Epoch 116/1000, LR 0.000263
Train loss: 1.0209;  Loss pred: 1.0209; Loss self: 0.0000; time: 0.20s
Val loss: 0.6603 score: 0.9796 time: 0.07s
Test loss: 0.6692 score: 0.8776 time: 0.06s
Epoch 117/1000, LR 0.000262
Train loss: 1.0180;  Loss pred: 1.0180; Loss self: 0.0000; time: 0.20s
Val loss: 0.6591 score: 0.9796 time: 0.07s
Test loss: 0.6684 score: 0.8776 time: 0.06s
Epoch 118/1000, LR 0.000262
Train loss: 1.0138;  Loss pred: 1.0138; Loss self: 0.0000; time: 0.20s
Val loss: 0.6579 score: 0.9796 time: 0.07s
Test loss: 0.6675 score: 0.8776 time: 0.06s
Epoch 119/1000, LR 0.000262
Train loss: 1.0125;  Loss pred: 1.0125; Loss self: 0.0000; time: 0.20s
Val loss: 0.6567 score: 0.9796 time: 0.07s
Test loss: 0.6666 score: 0.8776 time: 0.06s
Epoch 120/1000, LR 0.000262
Train loss: 1.0120;  Loss pred: 1.0120; Loss self: 0.0000; time: 0.20s
Val loss: 0.6554 score: 0.9796 time: 0.07s
Test loss: 0.6656 score: 0.8776 time: 0.06s
Epoch 121/1000, LR 0.000262
Train loss: 1.0081;  Loss pred: 1.0081; Loss self: 0.0000; time: 0.20s
Val loss: 0.6541 score: 0.9796 time: 0.08s
Test loss: 0.6646 score: 0.8776 time: 0.06s
Epoch 122/1000, LR 0.000262
Train loss: 1.0060;  Loss pred: 1.0060; Loss self: 0.0000; time: 0.20s
Val loss: 0.6527 score: 0.9796 time: 0.06s
Test loss: 0.6636 score: 0.8776 time: 0.06s
Epoch 123/1000, LR 0.000262
Train loss: 1.0040;  Loss pred: 1.0040; Loss self: 0.0000; time: 0.20s
Val loss: 0.6514 score: 0.9796 time: 0.07s
Test loss: 0.6626 score: 0.8776 time: 0.06s
Epoch 124/1000, LR 0.000261
Train loss: 0.9999;  Loss pred: 0.9999; Loss self: 0.0000; time: 0.20s
Val loss: 0.6500 score: 0.9796 time: 0.07s
Test loss: 0.6615 score: 0.8980 time: 0.06s
Epoch 125/1000, LR 0.000261
Train loss: 0.9997;  Loss pred: 0.9997; Loss self: 0.0000; time: 0.19s
Val loss: 0.6485 score: 0.9796 time: 0.06s
Test loss: 0.6605 score: 0.8980 time: 0.06s
Epoch 126/1000, LR 0.000261
Train loss: 1.0004;  Loss pred: 1.0004; Loss self: 0.0000; time: 0.20s
Val loss: 0.6470 score: 0.9796 time: 0.07s
Test loss: 0.6594 score: 0.8980 time: 0.06s
Epoch 127/1000, LR 0.000261
Train loss: 0.9946;  Loss pred: 0.9946; Loss self: 0.0000; time: 0.21s
Val loss: 0.6454 score: 0.9796 time: 0.07s
Test loss: 0.6582 score: 0.8980 time: 0.06s
Epoch 128/1000, LR 0.000261
Train loss: 0.9921;  Loss pred: 0.9921; Loss self: 0.0000; time: 0.21s
Val loss: 0.6438 score: 0.9796 time: 0.07s
Test loss: 0.6570 score: 0.8980 time: 0.07s
Epoch 129/1000, LR 0.000261
Train loss: 0.9910;  Loss pred: 0.9910; Loss self: 0.0000; time: 0.21s
Val loss: 0.6421 score: 0.9796 time: 0.08s
Test loss: 0.6558 score: 0.8980 time: 0.07s
Epoch 130/1000, LR 0.000260
Train loss: 0.9892;  Loss pred: 0.9892; Loss self: 0.0000; time: 0.21s
Val loss: 0.6404 score: 0.9796 time: 0.07s
Test loss: 0.6545 score: 0.8980 time: 0.06s
Epoch 131/1000, LR 0.000260
Train loss: 0.9859;  Loss pred: 0.9859; Loss self: 0.0000; time: 0.21s
Val loss: 0.6387 score: 0.9796 time: 0.07s
Test loss: 0.6532 score: 0.8980 time: 0.06s
Epoch 132/1000, LR 0.000260
Train loss: 0.9827;  Loss pred: 0.9827; Loss self: 0.0000; time: 0.21s
Val loss: 0.6369 score: 0.9796 time: 0.07s
Test loss: 0.6519 score: 0.8980 time: 0.06s
Epoch 133/1000, LR 0.000260
Train loss: 0.9824;  Loss pred: 0.9824; Loss self: 0.0000; time: 0.21s
Val loss: 0.6351 score: 0.9796 time: 0.07s
Test loss: 0.6505 score: 0.8980 time: 0.07s
Epoch 134/1000, LR 0.000260
Train loss: 0.9838;  Loss pred: 0.9838; Loss self: 0.0000; time: 0.21s
Val loss: 0.6333 score: 0.9796 time: 0.08s
Test loss: 0.6492 score: 0.8980 time: 0.06s
Epoch 135/1000, LR 0.000260
Train loss: 0.9783;  Loss pred: 0.9783; Loss self: 0.0000; time: 0.21s
Val loss: 0.6314 score: 0.9796 time: 0.07s
Test loss: 0.6478 score: 0.8980 time: 0.06s
Epoch 136/1000, LR 0.000260
Train loss: 0.9774;  Loss pred: 0.9774; Loss self: 0.0000; time: 0.21s
Val loss: 0.6295 score: 0.9796 time: 0.07s
Test loss: 0.6464 score: 0.8980 time: 0.07s
Epoch 137/1000, LR 0.000259
Train loss: 0.9748;  Loss pred: 0.9748; Loss self: 0.0000; time: 0.22s
Val loss: 0.6276 score: 0.9796 time: 0.08s
Test loss: 0.6449 score: 0.8980 time: 0.06s
Epoch 138/1000, LR 0.000259
Train loss: 0.9729;  Loss pred: 0.9729; Loss self: 0.0000; time: 0.22s
Val loss: 0.6256 score: 0.9796 time: 0.07s
Test loss: 0.6434 score: 0.8980 time: 0.07s
Epoch 139/1000, LR 0.000259
Train loss: 0.9724;  Loss pred: 0.9724; Loss self: 0.0000; time: 0.21s
Val loss: 0.6235 score: 0.9796 time: 0.07s
Test loss: 0.6419 score: 0.8980 time: 0.06s
Epoch 140/1000, LR 0.000259
Train loss: 0.9673;  Loss pred: 0.9673; Loss self: 0.0000; time: 0.21s
Val loss: 0.6214 score: 0.9796 time: 0.07s
Test loss: 0.6404 score: 0.8980 time: 0.06s
Epoch 141/1000, LR 0.000259
Train loss: 0.9673;  Loss pred: 0.9673; Loss self: 0.0000; time: 0.21s
Val loss: 0.6193 score: 0.9796 time: 0.08s
Test loss: 0.6388 score: 0.8980 time: 0.06s
Epoch 142/1000, LR 0.000259
Train loss: 0.9623;  Loss pred: 0.9623; Loss self: 0.0000; time: 0.21s
Val loss: 0.6171 score: 0.9796 time: 0.07s
Test loss: 0.6372 score: 0.8980 time: 0.06s
Epoch 143/1000, LR 0.000258
Train loss: 0.9615;  Loss pred: 0.9615; Loss self: 0.0000; time: 0.21s
Val loss: 0.6149 score: 0.9796 time: 0.07s
Test loss: 0.6356 score: 0.8980 time: 0.06s
Epoch 144/1000, LR 0.000258
Train loss: 0.9603;  Loss pred: 0.9603; Loss self: 0.0000; time: 0.21s
Val loss: 0.6126 score: 0.9796 time: 0.07s
Test loss: 0.6339 score: 0.8980 time: 0.06s
Epoch 145/1000, LR 0.000258
Train loss: 0.9588;  Loss pred: 0.9588; Loss self: 0.0000; time: 0.21s
Val loss: 0.6103 score: 0.9796 time: 0.08s
Test loss: 0.6322 score: 0.8980 time: 0.06s
Epoch 146/1000, LR 0.000258
Train loss: 0.9538;  Loss pred: 0.9538; Loss self: 0.0000; time: 0.21s
Val loss: 0.6079 score: 0.9796 time: 0.08s
Test loss: 0.6305 score: 0.8980 time: 0.06s
Epoch 147/1000, LR 0.000258
Train loss: 0.9544;  Loss pred: 0.9544; Loss self: 0.0000; time: 0.21s
Val loss: 0.6055 score: 0.9796 time: 0.07s
Test loss: 0.6287 score: 0.8980 time: 0.07s
Epoch 148/1000, LR 0.000257
Train loss: 0.9517;  Loss pred: 0.9517; Loss self: 0.0000; time: 0.21s
Val loss: 0.6031 score: 0.9796 time: 0.07s
Test loss: 0.6269 score: 0.8980 time: 0.07s
Epoch 149/1000, LR 0.000257
Train loss: 0.9475;  Loss pred: 0.9475; Loss self: 0.0000; time: 0.21s
Val loss: 0.6006 score: 0.9796 time: 0.08s
Test loss: 0.6251 score: 0.8980 time: 0.06s
Epoch 150/1000, LR 0.000257
Train loss: 0.9485;  Loss pred: 0.9485; Loss self: 0.0000; time: 0.21s
Val loss: 0.5981 score: 0.9796 time: 0.08s
Test loss: 0.6232 score: 0.8980 time: 0.06s
Epoch 151/1000, LR 0.000257
Train loss: 0.9444;  Loss pred: 0.9444; Loss self: 0.0000; time: 0.21s
Val loss: 0.5955 score: 0.9796 time: 0.08s
Test loss: 0.6213 score: 0.8980 time: 0.07s
Epoch 152/1000, LR 0.000257
Train loss: 0.9416;  Loss pred: 0.9416; Loss self: 0.0000; time: 0.21s
Val loss: 0.5929 score: 0.9796 time: 0.08s
Test loss: 0.6194 score: 0.8980 time: 0.06s
Epoch 153/1000, LR 0.000257
Train loss: 0.9396;  Loss pred: 0.9396; Loss self: 0.0000; time: 0.22s
Val loss: 0.5902 score: 0.9796 time: 0.08s
Test loss: 0.6174 score: 0.8980 time: 0.06s
Epoch 154/1000, LR 0.000256
Train loss: 0.9380;  Loss pred: 0.9380; Loss self: 0.0000; time: 0.22s
Val loss: 0.5875 score: 0.9796 time: 0.08s
Test loss: 0.6154 score: 0.8980 time: 0.06s
Epoch 155/1000, LR 0.000256
Train loss: 0.9349;  Loss pred: 0.9349; Loss self: 0.0000; time: 0.21s
Val loss: 0.5847 score: 0.9796 time: 0.08s
Test loss: 0.6134 score: 0.8980 time: 0.07s
Epoch 156/1000, LR 0.000256
Train loss: 0.9320;  Loss pred: 0.9320; Loss self: 0.0000; time: 0.23s
Val loss: 0.5819 score: 0.9796 time: 2.29s
Test loss: 0.6113 score: 0.8980 time: 1.10s
Epoch 157/1000, LR 0.000256
Train loss: 0.9287;  Loss pred: 0.9287; Loss self: 0.0000; time: 0.35s
Val loss: 0.5791 score: 0.9796 time: 0.09s
Test loss: 0.6092 score: 0.8980 time: 0.07s
Epoch 158/1000, LR 0.000256
Train loss: 0.9288;  Loss pred: 0.9288; Loss self: 0.0000; time: 0.22s
Val loss: 0.5762 score: 0.9796 time: 0.07s
Test loss: 0.6071 score: 0.8980 time: 0.06s
Epoch 159/1000, LR 0.000255
Train loss: 0.9270;  Loss pred: 0.9270; Loss self: 0.0000; time: 0.21s
Val loss: 0.5733 score: 0.9796 time: 0.07s
Test loss: 0.6050 score: 0.8980 time: 0.06s
Epoch 160/1000, LR 0.000255
Train loss: 0.9240;  Loss pred: 0.9240; Loss self: 0.0000; time: 0.21s
Val loss: 0.5703 score: 0.9796 time: 0.07s
Test loss: 0.6028 score: 0.8980 time: 0.07s
Epoch 161/1000, LR 0.000255
Train loss: 0.9228;  Loss pred: 0.9228; Loss self: 0.0000; time: 0.21s
Val loss: 0.5673 score: 0.9796 time: 0.08s
Test loss: 0.6006 score: 0.8980 time: 0.06s
Epoch 162/1000, LR 0.000255
Train loss: 0.9212;  Loss pred: 0.9212; Loss self: 0.0000; time: 0.21s
Val loss: 0.5643 score: 0.9796 time: 0.08s
Test loss: 0.5984 score: 0.8980 time: 0.06s
Epoch 163/1000, LR 0.000255
Train loss: 0.9147;  Loss pred: 0.9147; Loss self: 0.0000; time: 0.21s
Val loss: 0.5612 score: 0.9796 time: 0.08s
Test loss: 0.5961 score: 0.8980 time: 0.06s
Epoch 164/1000, LR 0.000254
Train loss: 0.9151;  Loss pred: 0.9151; Loss self: 0.0000; time: 0.21s
Val loss: 0.5581 score: 0.9796 time: 0.08s
Test loss: 0.5938 score: 0.8980 time: 0.06s
Epoch 165/1000, LR 0.000254
Train loss: 0.9097;  Loss pred: 0.9097; Loss self: 0.0000; time: 0.21s
Val loss: 0.5549 score: 0.9796 time: 0.08s
Test loss: 0.5915 score: 0.8980 time: 0.06s
Epoch 166/1000, LR 0.000254
Train loss: 0.9088;  Loss pred: 0.9088; Loss self: 0.0000; time: 0.21s
Val loss: 0.5517 score: 0.9796 time: 0.07s
Test loss: 0.5892 score: 0.8980 time: 0.07s
Epoch 167/1000, LR 0.000254
Train loss: 0.9061;  Loss pred: 0.9061; Loss self: 0.0000; time: 0.21s
Val loss: 0.5485 score: 0.9796 time: 0.08s
Test loss: 0.5868 score: 0.8980 time: 0.06s
Epoch 168/1000, LR 0.000254
Train loss: 0.9040;  Loss pred: 0.9040; Loss self: 0.0000; time: 0.22s
Val loss: 0.5452 score: 0.9796 time: 2.23s
Test loss: 0.5844 score: 0.8980 time: 1.05s
Epoch 169/1000, LR 0.000253
Train loss: 0.9002;  Loss pred: 0.9002; Loss self: 0.0000; time: 1.59s
Val loss: 0.5419 score: 0.9796 time: 1.05s
Test loss: 0.5820 score: 0.8980 time: 0.30s
Epoch 170/1000, LR 0.000253
Train loss: 0.8982;  Loss pred: 0.8982; Loss self: 0.0000; time: 0.83s
Val loss: 0.5384 score: 0.9796 time: 0.43s
Test loss: 0.5795 score: 0.8980 time: 0.42s
Epoch 171/1000, LR 0.000253
Train loss: 0.8980;  Loss pred: 0.8980; Loss self: 0.0000; time: 0.23s
Val loss: 0.5350 score: 0.9796 time: 0.10s
Test loss: 0.5769 score: 0.8980 time: 0.06s
Epoch 172/1000, LR 0.000253
Train loss: 0.8945;  Loss pred: 0.8945; Loss self: 0.0000; time: 0.21s
Val loss: 0.5315 score: 0.9796 time: 0.08s
Test loss: 0.5743 score: 0.8980 time: 0.06s
Epoch 173/1000, LR 0.000253
Train loss: 0.8902;  Loss pred: 0.8902; Loss self: 0.0000; time: 0.21s
Val loss: 0.5279 score: 0.9796 time: 0.07s
Test loss: 0.5717 score: 0.8980 time: 0.07s
Epoch 174/1000, LR 0.000252
Train loss: 0.8899;  Loss pred: 0.8899; Loss self: 0.0000; time: 0.21s
Val loss: 0.5243 score: 0.9796 time: 0.07s
Test loss: 0.5691 score: 0.8980 time: 0.07s
Epoch 175/1000, LR 0.000252
Train loss: 0.8857;  Loss pred: 0.8857; Loss self: 0.0000; time: 0.20s
Val loss: 0.5207 score: 0.9796 time: 0.07s
Test loss: 0.5664 score: 0.8980 time: 0.06s
Epoch 176/1000, LR 0.000252
Train loss: 0.8842;  Loss pred: 0.8842; Loss self: 0.0000; time: 0.21s
Val loss: 0.5170 score: 0.9796 time: 0.07s
Test loss: 0.5637 score: 0.8980 time: 0.06s
Epoch 177/1000, LR 0.000252
Train loss: 0.8785;  Loss pred: 0.8785; Loss self: 0.0000; time: 0.20s
Val loss: 0.5134 score: 0.9796 time: 0.07s
Test loss: 0.5609 score: 0.8980 time: 0.06s
Epoch 178/1000, LR 0.000251
Train loss: 0.8761;  Loss pred: 0.8761; Loss self: 0.0000; time: 0.21s
Val loss: 0.5096 score: 0.9796 time: 0.07s
Test loss: 0.5582 score: 0.8980 time: 0.06s
Epoch 179/1000, LR 0.000251
Train loss: 0.8755;  Loss pred: 0.8755; Loss self: 0.0000; time: 0.21s
Val loss: 0.5059 score: 0.9796 time: 0.07s
Test loss: 0.5554 score: 0.8980 time: 0.06s
Epoch 180/1000, LR 0.000251
Train loss: 0.8747;  Loss pred: 0.8747; Loss self: 0.0000; time: 0.20s
Val loss: 0.5021 score: 0.9796 time: 0.07s
Test loss: 0.5526 score: 0.8980 time: 0.06s
Epoch 181/1000, LR 0.000251
Train loss: 0.8692;  Loss pred: 0.8692; Loss self: 0.0000; time: 0.20s
Val loss: 0.4984 score: 0.9796 time: 0.08s
Test loss: 0.5499 score: 0.8980 time: 0.06s
Epoch 182/1000, LR 0.000251
Train loss: 0.8666;  Loss pred: 0.8666; Loss self: 0.0000; time: 0.20s
Val loss: 0.4946 score: 0.9796 time: 0.08s
Test loss: 0.5471 score: 0.8980 time: 0.06s
Epoch 183/1000, LR 0.000250
Train loss: 0.8635;  Loss pred: 0.8635; Loss self: 0.0000; time: 0.20s
Val loss: 0.4909 score: 0.9796 time: 0.07s
Test loss: 0.5444 score: 0.8980 time: 0.07s
Epoch 184/1000, LR 0.000250
Train loss: 0.8625;  Loss pred: 0.8625; Loss self: 0.0000; time: 0.20s
Val loss: 0.4871 score: 0.9796 time: 0.07s
Test loss: 0.5416 score: 0.8980 time: 0.07s
Epoch 185/1000, LR 0.000250
Train loss: 0.8590;  Loss pred: 0.8590; Loss self: 0.0000; time: 0.20s
Val loss: 0.4832 score: 0.9796 time: 0.07s
Test loss: 0.5388 score: 0.8980 time: 0.07s
Epoch 186/1000, LR 0.000250
Train loss: 0.8552;  Loss pred: 0.8552; Loss self: 0.0000; time: 0.20s
Val loss: 0.4794 score: 0.9796 time: 0.07s
Test loss: 0.5360 score: 0.8980 time: 0.06s
Epoch 187/1000, LR 0.000249
Train loss: 0.8533;  Loss pred: 0.8533; Loss self: 0.0000; time: 0.20s
Val loss: 0.4755 score: 0.9796 time: 0.07s
Test loss: 0.5332 score: 0.8980 time: 0.06s
Epoch 188/1000, LR 0.000249
Train loss: 0.8497;  Loss pred: 0.8497; Loss self: 0.0000; time: 0.21s
Val loss: 0.4716 score: 0.9796 time: 1.72s
Test loss: 0.5303 score: 0.8980 time: 1.54s
Epoch 189/1000, LR 0.000249
Train loss: 0.8476;  Loss pred: 0.8476; Loss self: 0.0000; time: 3.55s
Val loss: 0.4677 score: 0.9796 time: 0.09s
Test loss: 0.5275 score: 0.8980 time: 0.07s
Epoch 190/1000, LR 0.000249
Train loss: 0.8433;  Loss pred: 0.8433; Loss self: 0.0000; time: 0.22s
Val loss: 0.4638 score: 0.9796 time: 0.08s
Test loss: 0.5246 score: 0.8980 time: 0.06s
Epoch 191/1000, LR 0.000249
Train loss: 0.8405;  Loss pred: 0.8405; Loss self: 0.0000; time: 0.21s
Val loss: 0.4598 score: 0.9796 time: 0.07s
Test loss: 0.5217 score: 0.8980 time: 0.07s
Epoch 192/1000, LR 0.000248
Train loss: 0.8383;  Loss pred: 0.8383; Loss self: 0.0000; time: 0.20s
Val loss: 0.4559 score: 0.9796 time: 0.08s
Test loss: 0.5188 score: 0.8980 time: 0.06s
Epoch 193/1000, LR 0.000248
Train loss: 0.8375;  Loss pred: 0.8375; Loss self: 0.0000; time: 0.20s
Val loss: 0.4519 score: 0.9796 time: 0.08s
Test loss: 0.5159 score: 0.8980 time: 0.06s
Epoch 194/1000, LR 0.000248
Train loss: 0.8335;  Loss pred: 0.8335; Loss self: 0.0000; time: 0.20s
Val loss: 0.4479 score: 0.9796 time: 0.07s
Test loss: 0.5130 score: 0.8980 time: 0.06s
Epoch 195/1000, LR 0.000248
Train loss: 0.8330;  Loss pred: 0.8330; Loss self: 0.0000; time: 0.21s
Val loss: 0.4440 score: 0.9796 time: 0.08s
Test loss: 0.5102 score: 0.8980 time: 0.07s
Epoch 196/1000, LR 0.000247
Train loss: 0.8285;  Loss pred: 0.8285; Loss self: 0.0000; time: 3.16s
Val loss: 0.4400 score: 0.9796 time: 1.58s
Test loss: 0.5073 score: 0.8980 time: 1.13s
Epoch 197/1000, LR 0.000247
Train loss: 0.8237;  Loss pred: 0.8237; Loss self: 0.0000; time: 0.60s
Val loss: 0.4360 score: 0.9796 time: 0.15s
Test loss: 0.5044 score: 0.8980 time: 0.11s
Epoch 198/1000, LR 0.000247
Train loss: 0.8255;  Loss pred: 0.8255; Loss self: 0.0000; time: 0.33s
Val loss: 0.4320 score: 0.9796 time: 0.07s
Test loss: 0.5015 score: 0.8980 time: 0.07s
Epoch 199/1000, LR 0.000247
Train loss: 0.8199;  Loss pred: 0.8199; Loss self: 0.0000; time: 0.21s
Val loss: 0.4281 score: 0.9796 time: 0.07s
Test loss: 0.4986 score: 0.8980 time: 0.06s
Epoch 200/1000, LR 0.000246
Train loss: 0.8170;  Loss pred: 0.8170; Loss self: 0.0000; time: 0.21s
Val loss: 0.4241 score: 0.9796 time: 0.07s
Test loss: 0.4958 score: 0.8980 time: 0.07s
Epoch 201/1000, LR 0.000246
Train loss: 0.8143;  Loss pred: 0.8143; Loss self: 0.0000; time: 0.21s
Val loss: 0.4202 score: 0.9796 time: 0.08s
Test loss: 0.4929 score: 0.8980 time: 0.06s
Epoch 202/1000, LR 0.000246
Train loss: 0.8113;  Loss pred: 0.8113; Loss self: 0.0000; time: 0.21s
Val loss: 0.4162 score: 0.9796 time: 0.07s
Test loss: 0.4901 score: 0.8980 time: 0.06s
Epoch 203/1000, LR 0.000246
Train loss: 0.8071;  Loss pred: 0.8071; Loss self: 0.0000; time: 0.20s
Val loss: 0.4123 score: 0.9796 time: 0.07s
Test loss: 0.4873 score: 0.8980 time: 0.06s
Epoch 204/1000, LR 0.000245
Train loss: 0.8035;  Loss pred: 0.8035; Loss self: 0.0000; time: 0.21s
Val loss: 0.4084 score: 0.9796 time: 0.07s
Test loss: 0.4846 score: 0.8980 time: 0.06s
Epoch 205/1000, LR 0.000245
Train loss: 0.8038;  Loss pred: 0.8038; Loss self: 0.0000; time: 0.21s
Val loss: 0.4045 score: 0.9796 time: 0.07s
Test loss: 0.4818 score: 0.8980 time: 0.07s
Epoch 206/1000, LR 0.000245
Train loss: 0.8005;  Loss pred: 0.8005; Loss self: 0.0000; time: 0.21s
Val loss: 0.4006 score: 0.9796 time: 0.07s
Test loss: 0.4791 score: 0.8980 time: 0.06s
Epoch 207/1000, LR 0.000245
Train loss: 0.7965;  Loss pred: 0.7965; Loss self: 0.0000; time: 0.20s
Val loss: 0.3967 score: 0.9796 time: 0.07s
Test loss: 0.4764 score: 0.8980 time: 0.07s
Epoch 208/1000, LR 0.000244
Train loss: 0.7964;  Loss pred: 0.7964; Loss self: 0.0000; time: 0.22s
Val loss: 0.3928 score: 0.9796 time: 0.07s
Test loss: 0.4736 score: 0.8980 time: 0.06s
Epoch 209/1000, LR 0.000244
Train loss: 0.7918;  Loss pred: 0.7918; Loss self: 0.0000; time: 0.22s
Val loss: 0.3889 score: 0.9796 time: 0.08s
Test loss: 0.4708 score: 0.8980 time: 0.06s
Epoch 210/1000, LR 0.000244
Train loss: 0.7910;  Loss pred: 0.7910; Loss self: 0.0000; time: 0.21s
Val loss: 0.3850 score: 0.9796 time: 0.07s
Test loss: 0.4680 score: 0.8980 time: 0.07s
Epoch 211/1000, LR 0.000244
Train loss: 0.7866;  Loss pred: 0.7866; Loss self: 0.0000; time: 0.21s
Val loss: 0.3811 score: 0.9796 time: 0.08s
Test loss: 0.4652 score: 0.8980 time: 0.06s
Epoch 212/1000, LR 0.000243
Train loss: 0.7834;  Loss pred: 0.7834; Loss self: 0.0000; time: 0.21s
Val loss: 0.3772 score: 0.9796 time: 0.08s
Test loss: 0.4624 score: 0.8980 time: 0.73s
Epoch 213/1000, LR 0.000243
Train loss: 0.7796;  Loss pred: 0.7796; Loss self: 0.0000; time: 2.42s
Val loss: 0.3733 score: 0.9796 time: 0.99s
Test loss: 0.4596 score: 0.8980 time: 1.26s
Epoch 214/1000, LR 0.000243
Train loss: 0.7795;  Loss pred: 0.7795; Loss self: 0.0000; time: 4.94s
Val loss: 0.3694 score: 0.9796 time: 1.52s
Test loss: 0.4568 score: 0.8980 time: 0.13s
Epoch 215/1000, LR 0.000243
Train loss: 0.7753;  Loss pred: 0.7753; Loss self: 0.0000; time: 0.47s
Val loss: 0.3656 score: 0.9796 time: 0.07s
Test loss: 0.4541 score: 0.8980 time: 0.06s
Epoch 216/1000, LR 0.000242
Train loss: 0.7729;  Loss pred: 0.7729; Loss self: 0.0000; time: 0.21s
Val loss: 0.3617 score: 0.9796 time: 0.07s
Test loss: 0.4514 score: 0.8980 time: 0.07s
Epoch 217/1000, LR 0.000242
Train loss: 0.7693;  Loss pred: 0.7693; Loss self: 0.0000; time: 0.21s
Val loss: 0.3580 score: 0.9796 time: 0.08s
Test loss: 0.4487 score: 0.8980 time: 0.06s
Epoch 218/1000, LR 0.000242
Train loss: 0.7672;  Loss pred: 0.7672; Loss self: 0.0000; time: 0.21s
Val loss: 0.3542 score: 0.9796 time: 0.07s
Test loss: 0.4461 score: 0.8980 time: 0.06s
Epoch 219/1000, LR 0.000242
Train loss: 0.7660;  Loss pred: 0.7660; Loss self: 0.0000; time: 0.21s
Val loss: 0.3505 score: 0.9796 time: 0.07s
Test loss: 0.4436 score: 0.8980 time: 0.06s
Epoch 220/1000, LR 0.000241
Train loss: 0.7626;  Loss pred: 0.7626; Loss self: 0.0000; time: 0.20s
Val loss: 0.3467 score: 0.9796 time: 0.07s
Test loss: 0.4410 score: 0.8980 time: 0.06s
Epoch 221/1000, LR 0.000241
Train loss: 0.7596;  Loss pred: 0.7596; Loss self: 0.0000; time: 0.20s
Val loss: 0.3430 score: 0.9796 time: 0.07s
Test loss: 0.4383 score: 0.8980 time: 0.06s
Epoch 222/1000, LR 0.000241
Train loss: 0.7573;  Loss pred: 0.7573; Loss self: 0.0000; time: 0.38s
Val loss: 0.3393 score: 0.9796 time: 0.26s
Test loss: 0.4357 score: 0.8980 time: 0.55s
Epoch 223/1000, LR 0.000241
Train loss: 0.7548;  Loss pred: 0.7548; Loss self: 0.0000; time: 1.69s
Val loss: 0.3356 score: 0.9796 time: 1.12s
Test loss: 0.4331 score: 0.8980 time: 2.13s
Epoch 224/1000, LR 0.000240
Train loss: 0.7511;  Loss pred: 0.7511; Loss self: 0.0000; time: 1.61s
Val loss: 0.3319 score: 0.9796 time: 0.16s
Test loss: 0.4305 score: 0.8980 time: 0.23s
Epoch 225/1000, LR 0.000240
Train loss: 0.7488;  Loss pred: 0.7488; Loss self: 0.0000; time: 0.38s
Val loss: 0.3283 score: 0.9796 time: 0.07s
Test loss: 0.4280 score: 0.8980 time: 0.06s
Epoch 226/1000, LR 0.000240
Train loss: 0.7495;  Loss pred: 0.7495; Loss self: 0.0000; time: 0.20s
Val loss: 0.3247 score: 0.9796 time: 0.07s
Test loss: 0.4255 score: 0.8980 time: 0.06s
Epoch 227/1000, LR 0.000240
Train loss: 0.7446;  Loss pred: 0.7446; Loss self: 0.0000; time: 0.20s
Val loss: 0.3212 score: 0.9796 time: 0.08s
Test loss: 0.4230 score: 0.8980 time: 0.06s
Epoch 228/1000, LR 0.000239
Train loss: 0.7436;  Loss pred: 0.7436; Loss self: 0.0000; time: 0.20s
Val loss: 0.3176 score: 0.9796 time: 0.06s
Test loss: 0.4205 score: 0.8980 time: 0.06s
Epoch 229/1000, LR 0.000239
Train loss: 0.7407;  Loss pred: 0.7407; Loss self: 0.0000; time: 0.20s
Val loss: 0.3141 score: 0.9796 time: 0.07s
Test loss: 0.4181 score: 0.8980 time: 0.06s
Epoch 230/1000, LR 0.000239
Train loss: 0.7392;  Loss pred: 0.7392; Loss self: 0.0000; time: 0.19s
Val loss: 0.3106 score: 0.9796 time: 0.07s
Test loss: 0.4156 score: 0.8980 time: 0.06s
Epoch 231/1000, LR 0.000238
Train loss: 0.7339;  Loss pred: 0.7339; Loss self: 0.0000; time: 0.20s
Val loss: 0.3072 score: 0.9796 time: 0.07s
Test loss: 0.4132 score: 0.8980 time: 0.06s
Epoch 232/1000, LR 0.000238
Train loss: 0.7341;  Loss pred: 0.7341; Loss self: 0.0000; time: 0.20s
Val loss: 0.3037 score: 0.9796 time: 0.15s
Test loss: 0.4107 score: 0.8980 time: 0.06s
Epoch 233/1000, LR 0.000238
Train loss: 0.7319;  Loss pred: 0.7319; Loss self: 0.0000; time: 0.20s
Val loss: 0.3003 score: 0.9796 time: 0.07s
Test loss: 0.4083 score: 0.8980 time: 0.06s
Epoch 234/1000, LR 0.000238
Train loss: 0.7259;  Loss pred: 0.7259; Loss self: 0.0000; time: 0.20s
Val loss: 0.2969 score: 0.9796 time: 0.07s
Test loss: 0.4060 score: 0.8980 time: 0.85s
Epoch 235/1000, LR 0.000237
Train loss: 0.7270;  Loss pred: 0.7270; Loss self: 0.0000; time: 2.34s
Val loss: 0.2935 score: 0.9796 time: 0.24s
Test loss: 0.4036 score: 0.8980 time: 0.46s
Epoch 236/1000, LR 0.000237
Train loss: 0.7255;  Loss pred: 0.7255; Loss self: 0.0000; time: 0.53s
Val loss: 0.2902 score: 0.9796 time: 0.23s
Test loss: 0.4013 score: 0.8980 time: 1.51s
Epoch 237/1000, LR 0.000237
Train loss: 0.7210;  Loss pred: 0.7210; Loss self: 0.0000; time: 5.69s
Val loss: 0.2869 score: 0.9796 time: 0.79s
Test loss: 0.3990 score: 0.8980 time: 0.38s
Epoch 238/1000, LR 0.000236
Train loss: 0.7169;  Loss pred: 0.7169; Loss self: 0.0000; time: 0.90s
Val loss: 0.2836 score: 0.9796 time: 0.07s
Test loss: 0.3968 score: 0.8980 time: 0.06s
Epoch 239/1000, LR 0.000236
Train loss: 0.7160;  Loss pred: 0.7160; Loss self: 0.0000; time: 0.19s
Val loss: 0.2804 score: 0.9796 time: 0.07s
Test loss: 0.3946 score: 0.8980 time: 0.06s
Epoch 240/1000, LR 0.000236
Train loss: 0.7128;  Loss pred: 0.7128; Loss self: 0.0000; time: 0.19s
Val loss: 0.2773 score: 0.9796 time: 0.07s
Test loss: 0.3925 score: 0.8980 time: 0.06s
Epoch 241/1000, LR 0.000236
Train loss: 0.7143;  Loss pred: 0.7143; Loss self: 0.0000; time: 0.19s
Val loss: 0.2741 score: 0.9796 time: 0.07s
Test loss: 0.3904 score: 0.8980 time: 0.06s
Epoch 242/1000, LR 0.000235
Train loss: 0.7102;  Loss pred: 0.7102; Loss self: 0.0000; time: 0.19s
Val loss: 0.2710 score: 0.9796 time: 0.07s
Test loss: 0.3884 score: 0.8980 time: 0.06s
Epoch 243/1000, LR 0.000235
Train loss: 0.7066;  Loss pred: 0.7066; Loss self: 0.0000; time: 0.19s
Val loss: 0.2680 score: 0.9796 time: 0.06s
Test loss: 0.3863 score: 0.8980 time: 0.06s
Epoch 244/1000, LR 0.000235
Train loss: 0.7037;  Loss pred: 0.7037; Loss self: 0.0000; time: 0.19s
Val loss: 0.2649 score: 0.9796 time: 0.07s
Test loss: 0.3842 score: 0.8980 time: 0.06s
Epoch 245/1000, LR 0.000234
Train loss: 0.7041;  Loss pred: 0.7041; Loss self: 0.0000; time: 0.19s
Val loss: 0.2619 score: 0.9796 time: 0.06s
Test loss: 0.3821 score: 0.8980 time: 0.06s
Epoch 246/1000, LR 0.000234
Train loss: 0.7029;  Loss pred: 0.7029; Loss self: 0.0000; time: 0.20s
Val loss: 0.2590 score: 0.9796 time: 0.07s
Test loss: 0.3801 score: 0.8980 time: 0.06s
Epoch 247/1000, LR 0.000234
Train loss: 0.6979;  Loss pred: 0.6979; Loss self: 0.0000; time: 0.19s
Val loss: 0.2560 score: 0.9796 time: 0.06s
Test loss: 0.3782 score: 0.8980 time: 0.06s
Epoch 248/1000, LR 0.000234
Train loss: 0.6980;  Loss pred: 0.6980; Loss self: 0.0000; time: 0.19s
Val loss: 0.2531 score: 0.9796 time: 0.07s
Test loss: 0.3762 score: 0.8980 time: 0.06s
Epoch 249/1000, LR 0.000233
Train loss: 0.6967;  Loss pred: 0.6967; Loss self: 0.0000; time: 0.19s
Val loss: 0.2503 score: 0.9796 time: 0.06s
Test loss: 0.3743 score: 0.8980 time: 0.06s
Epoch 250/1000, LR 0.000233
Train loss: 0.6932;  Loss pred: 0.6932; Loss self: 0.0000; time: 3.77s
Val loss: 0.2475 score: 0.9796 time: 0.82s
Test loss: 0.3723 score: 0.8980 time: 1.91s
Epoch 251/1000, LR 0.000233
Train loss: 0.6918;  Loss pred: 0.6918; Loss self: 0.0000; time: 0.80s
Val loss: 0.2446 score: 0.9796 time: 0.09s
Test loss: 0.3704 score: 0.8980 time: 0.08s
Epoch 252/1000, LR 0.000232
Train loss: 0.6897;  Loss pred: 0.6897; Loss self: 0.0000; time: 0.21s
Val loss: 0.2419 score: 0.9796 time: 0.07s
Test loss: 0.3684 score: 0.8980 time: 0.06s
Epoch 253/1000, LR 0.000232
Train loss: 0.6871;  Loss pred: 0.6871; Loss self: 0.0000; time: 0.19s
Val loss: 0.2391 score: 0.9796 time: 0.07s
Test loss: 0.3663 score: 0.8980 time: 0.06s
Epoch 254/1000, LR 0.000232
Train loss: 0.6869;  Loss pred: 0.6869; Loss self: 0.0000; time: 0.20s
Val loss: 0.2364 score: 0.9796 time: 0.07s
Test loss: 0.3643 score: 0.8980 time: 0.06s
Epoch 255/1000, LR 0.000232
Train loss: 0.6861;  Loss pred: 0.6861; Loss self: 0.0000; time: 0.20s
Val loss: 0.2337 score: 0.9796 time: 0.07s
Test loss: 0.3624 score: 0.8980 time: 0.06s
Epoch 256/1000, LR 0.000231
Train loss: 0.6811;  Loss pred: 0.6811; Loss self: 0.0000; time: 0.19s
Val loss: 0.2311 score: 0.9796 time: 0.07s
Test loss: 0.3605 score: 0.8980 time: 0.06s
Epoch 257/1000, LR 0.000231
Train loss: 0.6789;  Loss pred: 0.6789; Loss self: 0.0000; time: 0.20s
Val loss: 0.2285 score: 0.9796 time: 0.07s
Test loss: 0.3586 score: 0.8980 time: 0.06s
Epoch 258/1000, LR 0.000231
Train loss: 0.6783;  Loss pred: 0.6783; Loss self: 0.0000; time: 0.20s
Val loss: 0.2259 score: 0.9796 time: 0.08s
Test loss: 0.3568 score: 0.8980 time: 0.06s
Epoch 259/1000, LR 0.000230
Train loss: 0.6755;  Loss pred: 0.6755; Loss self: 0.0000; time: 0.20s
Val loss: 0.2234 score: 0.9796 time: 0.07s
Test loss: 0.3551 score: 0.8980 time: 0.06s
Epoch 260/1000, LR 0.000230
Train loss: 0.6762;  Loss pred: 0.6762; Loss self: 0.0000; time: 5.27s
Val loss: 0.2210 score: 0.9796 time: 0.08s
Test loss: 0.3535 score: 0.8980 time: 0.06s
Epoch 261/1000, LR 0.000230
Train loss: 0.6714;  Loss pred: 0.6714; Loss self: 0.0000; time: 0.21s
Val loss: 0.2186 score: 0.9796 time: 0.07s
Test loss: 0.3519 score: 0.8980 time: 0.06s
Epoch 262/1000, LR 0.000229
Train loss: 0.6699;  Loss pred: 0.6699; Loss self: 0.0000; time: 0.20s
Val loss: 0.2162 score: 0.9796 time: 0.07s
Test loss: 0.3503 score: 0.8980 time: 0.06s
Epoch 263/1000, LR 0.000229
Train loss: 0.6681;  Loss pred: 0.6681; Loss self: 0.0000; time: 0.20s
Val loss: 0.2139 score: 0.9796 time: 0.07s
Test loss: 0.3489 score: 0.8980 time: 0.06s
Epoch 264/1000, LR 0.000229
Train loss: 0.6678;  Loss pred: 0.6678; Loss self: 0.0000; time: 0.20s
Val loss: 0.2117 score: 0.9796 time: 0.07s
Test loss: 0.3475 score: 0.8980 time: 0.06s
Epoch 265/1000, LR 0.000228
Train loss: 0.6674;  Loss pred: 0.6674; Loss self: 0.0000; time: 0.21s
Val loss: 0.2094 score: 0.9796 time: 0.07s
Test loss: 0.3462 score: 0.8980 time: 0.06s
Epoch 266/1000, LR 0.000228
Train loss: 0.6657;  Loss pred: 0.6657; Loss self: 0.0000; time: 0.20s
Val loss: 0.2072 score: 0.9796 time: 0.08s
Test loss: 0.3447 score: 0.8980 time: 0.06s
Epoch 267/1000, LR 0.000228
Train loss: 0.6613;  Loss pred: 0.6613; Loss self: 0.0000; time: 0.20s
Val loss: 0.2050 score: 0.9796 time: 0.07s
Test loss: 0.3433 score: 0.8980 time: 0.06s
Epoch 268/1000, LR 0.000228
Train loss: 0.6627;  Loss pred: 0.6627; Loss self: 0.0000; time: 0.20s
Val loss: 0.2028 score: 0.9796 time: 0.07s
Test loss: 0.3418 score: 0.8980 time: 0.06s
Epoch 269/1000, LR 0.000227
Train loss: 0.6592;  Loss pred: 0.6592; Loss self: 0.0000; time: 0.20s
Val loss: 0.2007 score: 0.9796 time: 0.07s
Test loss: 0.3403 score: 0.8980 time: 0.06s
Epoch 270/1000, LR 0.000227
Train loss: 0.6572;  Loss pred: 0.6572; Loss self: 0.0000; time: 0.20s
Val loss: 0.1985 score: 0.9796 time: 0.07s
Test loss: 0.3388 score: 0.8980 time: 1.60s
Epoch 271/1000, LR 0.000227
Train loss: 0.6552;  Loss pred: 0.6552; Loss self: 0.0000; time: 4.46s
Val loss: 0.1964 score: 0.9796 time: 0.54s
Test loss: 0.3374 score: 0.8980 time: 0.23s
Epoch 272/1000, LR 0.000226
Train loss: 0.6555;  Loss pred: 0.6555; Loss self: 0.0000; time: 0.92s
Val loss: 0.1943 score: 0.9796 time: 0.07s
Test loss: 0.3359 score: 0.8980 time: 0.06s
Epoch 273/1000, LR 0.000226
Train loss: 0.6545;  Loss pred: 0.6545; Loss self: 0.0000; time: 0.22s
Val loss: 0.1923 score: 0.9796 time: 0.07s
Test loss: 0.3344 score: 0.8980 time: 0.06s
Epoch 274/1000, LR 0.000226
Train loss: 0.6539;  Loss pred: 0.6539; Loss self: 0.0000; time: 0.21s
Val loss: 0.1902 score: 0.9796 time: 0.08s
Test loss: 0.3329 score: 0.8980 time: 0.07s
Epoch 275/1000, LR 0.000225
Train loss: 0.6521;  Loss pred: 0.6521; Loss self: 0.0000; time: 0.21s
Val loss: 0.1882 score: 0.9796 time: 0.08s
Test loss: 0.3315 score: 0.8980 time: 0.06s
Epoch 276/1000, LR 0.000225
Train loss: 0.6476;  Loss pred: 0.6476; Loss self: 0.0000; time: 0.22s
Val loss: 0.1863 score: 0.9796 time: 0.08s
Test loss: 0.3302 score: 0.8980 time: 0.06s
Epoch 277/1000, LR 0.000225
Train loss: 0.6446;  Loss pred: 0.6446; Loss self: 0.0000; time: 0.21s
Val loss: 0.1844 score: 0.9796 time: 0.08s
Test loss: 0.3290 score: 0.8980 time: 0.06s
Epoch 278/1000, LR 0.000224
Train loss: 0.6435;  Loss pred: 0.6435; Loss self: 0.0000; time: 0.21s
Val loss: 0.1826 score: 0.9796 time: 0.07s
Test loss: 0.3280 score: 0.8980 time: 0.07s
Epoch 279/1000, LR 0.000224
Train loss: 0.6449;  Loss pred: 0.6449; Loss self: 0.0000; time: 0.21s
Val loss: 0.1808 score: 0.9796 time: 0.08s
Test loss: 0.3269 score: 0.8980 time: 0.07s
Epoch 280/1000, LR 0.000224
Train loss: 0.6418;  Loss pred: 0.6418; Loss self: 0.0000; time: 0.21s
Val loss: 0.1790 score: 0.9796 time: 0.07s
Test loss: 0.3259 score: 0.8980 time: 0.06s
Epoch 281/1000, LR 0.000223
Train loss: 0.6408;  Loss pred: 0.6408; Loss self: 0.0000; time: 0.21s
Val loss: 0.1773 score: 0.9796 time: 0.08s
Test loss: 0.3249 score: 0.8980 time: 0.06s
Epoch 282/1000, LR 0.000223
Train loss: 0.6405;  Loss pred: 0.6405; Loss self: 0.0000; time: 0.21s
Val loss: 0.1756 score: 0.9796 time: 0.07s
Test loss: 0.3239 score: 0.9184 time: 0.07s
Epoch 283/1000, LR 0.000223
Train loss: 0.6384;  Loss pred: 0.6384; Loss self: 0.0000; time: 0.22s
Val loss: 0.1739 score: 0.9796 time: 0.07s
Test loss: 0.3229 score: 0.9184 time: 0.07s
Epoch 284/1000, LR 0.000222
Train loss: 0.6366;  Loss pred: 0.6366; Loss self: 0.0000; time: 0.21s
Val loss: 0.1722 score: 0.9796 time: 0.08s
Test loss: 0.3219 score: 0.9184 time: 0.06s
Epoch 285/1000, LR 0.000222
Train loss: 0.6327;  Loss pred: 0.6327; Loss self: 0.0000; time: 0.21s
Val loss: 0.1705 score: 0.9796 time: 0.08s
Test loss: 0.3209 score: 0.9184 time: 0.07s
Epoch 286/1000, LR 0.000222
Train loss: 0.6348;  Loss pred: 0.6348; Loss self: 0.0000; time: 0.21s
Val loss: 0.1689 score: 0.9796 time: 0.07s
Test loss: 0.3199 score: 0.9184 time: 0.06s
Epoch 287/1000, LR 0.000221
Train loss: 0.6338;  Loss pred: 0.6338; Loss self: 0.0000; time: 4.49s
Val loss: 0.1673 score: 0.9796 time: 0.35s
Test loss: 0.3188 score: 0.9184 time: 0.42s
Epoch 288/1000, LR 0.000221
Train loss: 0.6358;  Loss pred: 0.6358; Loss self: 0.0000; time: 0.57s
Val loss: 0.1656 score: 0.9796 time: 0.09s
Test loss: 0.3177 score: 0.9184 time: 0.06s
Epoch 289/1000, LR 0.000221
Train loss: 0.6314;  Loss pred: 0.6314; Loss self: 0.0000; time: 0.21s
Val loss: 0.1639 score: 0.9796 time: 0.07s
Test loss: 0.3164 score: 0.9184 time: 0.06s
Epoch 290/1000, LR 0.000220
Train loss: 0.6297;  Loss pred: 0.6297; Loss self: 0.0000; time: 0.20s
Val loss: 0.1623 score: 0.9796 time: 0.07s
Test loss: 0.3150 score: 0.9184 time: 0.06s
Epoch 291/1000, LR 0.000220
Train loss: 0.6278;  Loss pred: 0.6278; Loss self: 0.0000; time: 0.20s
Val loss: 0.1606 score: 0.9796 time: 0.07s
Test loss: 0.3136 score: 0.8980 time: 0.06s
Epoch 292/1000, LR 0.000220
Train loss: 0.6267;  Loss pred: 0.6267; Loss self: 0.0000; time: 0.20s
Val loss: 0.1590 score: 0.9796 time: 0.07s
Test loss: 0.3122 score: 0.8980 time: 0.06s
Epoch 293/1000, LR 0.000219
Train loss: 0.6293;  Loss pred: 0.6293; Loss self: 0.0000; time: 0.20s
Val loss: 0.1574 score: 0.9796 time: 0.08s
Test loss: 0.3108 score: 0.8980 time: 0.06s
Epoch 294/1000, LR 0.000219
Train loss: 0.6248;  Loss pred: 0.6248; Loss self: 0.0000; time: 0.20s
Val loss: 0.1559 score: 0.9796 time: 0.08s
Test loss: 0.3095 score: 0.8980 time: 0.06s
Epoch 295/1000, LR 0.000219
Train loss: 0.6246;  Loss pred: 0.6246; Loss self: 0.0000; time: 0.20s
Val loss: 0.1544 score: 0.9796 time: 0.07s
Test loss: 0.3084 score: 0.8980 time: 0.07s
Epoch 296/1000, LR 0.000218
Train loss: 0.6245;  Loss pred: 0.6245; Loss self: 0.0000; time: 0.20s
Val loss: 0.1530 score: 0.9796 time: 0.08s
Test loss: 0.3073 score: 0.8980 time: 0.06s
Epoch 297/1000, LR 0.000218
Train loss: 0.6240;  Loss pred: 0.6240; Loss self: 0.0000; time: 0.21s
Val loss: 0.1515 score: 0.9796 time: 0.07s
Test loss: 0.3064 score: 0.8980 time: 0.06s
Epoch 298/1000, LR 0.000218
Train loss: 0.6208;  Loss pred: 0.6208; Loss self: 0.0000; time: 0.21s
Val loss: 0.1502 score: 0.9796 time: 0.07s
Test loss: 0.3055 score: 0.8980 time: 0.06s
Epoch 299/1000, LR 0.000217
Train loss: 0.6194;  Loss pred: 0.6194; Loss self: 0.0000; time: 0.22s
Val loss: 0.1489 score: 0.9796 time: 0.07s
Test loss: 0.3049 score: 0.8980 time: 0.07s
Epoch 300/1000, LR 0.000217
Train loss: 0.6215;  Loss pred: 0.6215; Loss self: 0.0000; time: 0.22s
Val loss: 0.1476 score: 0.9796 time: 0.07s
Test loss: 0.3043 score: 0.8980 time: 0.07s
Epoch 301/1000, LR 0.000217
Train loss: 0.6184;  Loss pred: 0.6184; Loss self: 0.0000; time: 0.21s
Val loss: 0.1464 score: 0.9796 time: 0.07s
Test loss: 0.3037 score: 0.9184 time: 0.07s
Epoch 302/1000, LR 0.000216
Train loss: 0.6159;  Loss pred: 0.6159; Loss self: 0.0000; time: 0.22s
Val loss: 0.1452 score: 0.9796 time: 0.08s
Test loss: 0.3032 score: 0.9184 time: 0.06s
Epoch 303/1000, LR 0.000216
Train loss: 0.6158;  Loss pred: 0.6158; Loss self: 0.0000; time: 0.22s
Val loss: 0.1440 score: 0.9796 time: 0.07s
Test loss: 0.3027 score: 0.8980 time: 0.16s
Epoch 304/1000, LR 0.000216
Train loss: 0.6147;  Loss pred: 0.6147; Loss self: 0.0000; time: 0.22s
Val loss: 0.1428 score: 0.9796 time: 0.07s
Test loss: 0.3021 score: 0.8980 time: 0.07s
Epoch 305/1000, LR 0.000215
Train loss: 0.6135;  Loss pred: 0.6135; Loss self: 0.0000; time: 0.23s
Val loss: 0.1416 score: 0.9796 time: 1.69s
Test loss: 0.3014 score: 0.8980 time: 0.70s
Epoch 306/1000, LR 0.000215
Train loss: 0.6130;  Loss pred: 0.6130; Loss self: 0.0000; time: 3.49s
Val loss: 0.1404 score: 0.9796 time: 0.48s
Test loss: 0.3007 score: 0.8980 time: 0.50s
Epoch 307/1000, LR 0.000215
Train loss: 0.6115;  Loss pred: 0.6115; Loss self: 0.0000; time: 0.62s
Val loss: 0.1392 score: 0.9796 time: 0.40s
Test loss: 0.3000 score: 0.8980 time: 0.18s
Epoch 308/1000, LR 0.000214
Train loss: 0.6080;  Loss pred: 0.6080; Loss self: 0.0000; time: 1.22s
Val loss: 0.1381 score: 0.9796 time: 0.07s
Test loss: 0.2993 score: 0.8980 time: 0.06s
Epoch 309/1000, LR 0.000214
Train loss: 0.6066;  Loss pred: 0.6066; Loss self: 0.0000; time: 0.21s
Val loss: 0.1369 score: 0.9796 time: 0.07s
Test loss: 0.2985 score: 0.8980 time: 0.06s
Epoch 310/1000, LR 0.000214
Train loss: 0.6080;  Loss pred: 0.6080; Loss self: 0.0000; time: 0.21s
Val loss: 0.1358 score: 0.9796 time: 0.07s
Test loss: 0.2977 score: 0.8980 time: 0.06s
Epoch 311/1000, LR 0.000213
Train loss: 0.6102;  Loss pred: 0.6102; Loss self: 0.0000; time: 0.21s
Val loss: 0.1346 score: 0.9796 time: 0.08s
Test loss: 0.2967 score: 0.8980 time: 0.06s
Epoch 312/1000, LR 0.000213
Train loss: 0.6106;  Loss pred: 0.6106; Loss self: 0.0000; time: 0.21s
Val loss: 0.1334 score: 0.9796 time: 0.08s
Test loss: 0.2957 score: 0.8980 time: 0.06s
Epoch 313/1000, LR 0.000213
Train loss: 0.6071;  Loss pred: 0.6071; Loss self: 0.0000; time: 0.21s
Val loss: 0.1322 score: 0.9796 time: 0.07s
Test loss: 0.2946 score: 0.8980 time: 0.06s
Epoch 314/1000, LR 0.000212
Train loss: 0.6026;  Loss pred: 0.6026; Loss self: 0.0000; time: 0.21s
Val loss: 0.1310 score: 0.9796 time: 0.08s
Test loss: 0.2937 score: 0.8980 time: 0.06s
Epoch 315/1000, LR 0.000212
Train loss: 0.6051;  Loss pred: 0.6051; Loss self: 0.0000; time: 0.21s
Val loss: 0.1299 score: 0.9796 time: 0.08s
Test loss: 0.2927 score: 0.8980 time: 0.06s
Epoch 316/1000, LR 0.000212
Train loss: 0.6049;  Loss pred: 0.6049; Loss self: 0.0000; time: 0.22s
Val loss: 0.1288 score: 0.9796 time: 0.08s
Test loss: 0.2918 score: 0.8980 time: 0.06s
Epoch 317/1000, LR 0.000211
Train loss: 0.6041;  Loss pred: 0.6041; Loss self: 0.0000; time: 0.22s
Val loss: 0.1277 score: 0.9796 time: 0.60s
Test loss: 0.2909 score: 0.8980 time: 0.72s
Epoch 318/1000, LR 0.000211
Train loss: 0.6022;  Loss pred: 0.6022; Loss self: 0.0000; time: 3.79s
Val loss: 0.1267 score: 0.9796 time: 0.13s
Test loss: 0.2900 score: 0.8980 time: 0.16s
Epoch 319/1000, LR 0.000210
Train loss: 0.6022;  Loss pred: 0.6022; Loss self: 0.0000; time: 0.51s
Val loss: 0.1256 score: 0.9796 time: 0.07s
Test loss: 0.2891 score: 0.9184 time: 0.07s
Epoch 320/1000, LR 0.000210
Train loss: 0.6021;  Loss pred: 0.6021; Loss self: 0.0000; time: 0.21s
Val loss: 0.1246 score: 0.9796 time: 0.08s
Test loss: 0.2882 score: 0.9184 time: 0.06s
Epoch 321/1000, LR 0.000210
Train loss: 0.6003;  Loss pred: 0.6003; Loss self: 0.0000; time: 0.21s
Val loss: 0.1236 score: 0.9796 time: 0.08s
Test loss: 0.2875 score: 0.9184 time: 0.06s
Epoch 322/1000, LR 0.000209
Train loss: 0.5966;  Loss pred: 0.5966; Loss self: 0.0000; time: 0.21s
Val loss: 0.1226 score: 0.9796 time: 0.07s
Test loss: 0.2868 score: 0.9184 time: 0.07s
Epoch 323/1000, LR 0.000209
Train loss: 0.5996;  Loss pred: 0.5996; Loss self: 0.0000; time: 0.21s
Val loss: 0.1217 score: 0.9796 time: 0.07s
Test loss: 0.2862 score: 0.9184 time: 0.06s
Epoch 324/1000, LR 0.000209
Train loss: 0.5950;  Loss pred: 0.5950; Loss self: 0.0000; time: 0.21s
Val loss: 0.1208 score: 0.9796 time: 0.07s
Test loss: 0.2857 score: 0.8980 time: 0.07s
Epoch 325/1000, LR 0.000208
Train loss: 0.5966;  Loss pred: 0.5966; Loss self: 0.0000; time: 0.21s
Val loss: 0.1199 score: 0.9796 time: 0.07s
Test loss: 0.2852 score: 0.8980 time: 0.06s
Epoch 326/1000, LR 0.000208
Train loss: 0.5957;  Loss pred: 0.5957; Loss self: 0.0000; time: 0.21s
Val loss: 0.1191 score: 0.9796 time: 0.07s
Test loss: 0.2847 score: 0.8980 time: 0.06s
Epoch 327/1000, LR 0.000208
Train loss: 0.5960;  Loss pred: 0.5960; Loss self: 0.0000; time: 0.21s
Val loss: 0.1182 score: 0.9796 time: 0.07s
Test loss: 0.2842 score: 0.8980 time: 0.06s
Epoch 328/1000, LR 0.000207
Train loss: 0.5947;  Loss pred: 0.5947; Loss self: 0.0000; time: 0.21s
Val loss: 0.1174 score: 0.9796 time: 0.07s
Test loss: 0.2839 score: 0.8980 time: 0.06s
Epoch 329/1000, LR 0.000207
Train loss: 0.5941;  Loss pred: 0.5941; Loss self: 0.0000; time: 0.21s
Val loss: 0.1166 score: 0.9796 time: 0.07s
Test loss: 0.2834 score: 0.8980 time: 0.06s
Epoch 330/1000, LR 0.000207
Train loss: 0.5899;  Loss pred: 0.5899; Loss self: 0.0000; time: 0.21s
Val loss: 0.1158 score: 0.9796 time: 0.08s
Test loss: 0.2832 score: 0.8980 time: 0.06s
Epoch 331/1000, LR 0.000206
Train loss: 0.5918;  Loss pred: 0.5918; Loss self: 0.0000; time: 0.21s
Val loss: 0.1150 score: 0.9796 time: 0.08s
Test loss: 0.2827 score: 0.8980 time: 0.06s
Epoch 332/1000, LR 0.000206
Train loss: 0.5916;  Loss pred: 0.5916; Loss self: 0.0000; time: 0.21s
Val loss: 0.1142 score: 0.9796 time: 0.08s
Test loss: 0.2822 score: 0.8980 time: 0.06s
Epoch 333/1000, LR 0.000205
Train loss: 0.5912;  Loss pred: 0.5912; Loss self: 0.0000; time: 0.21s
Val loss: 0.1133 score: 0.9796 time: 0.07s
Test loss: 0.2816 score: 0.8980 time: 0.06s
Epoch 334/1000, LR 0.000205
Train loss: 0.5940;  Loss pred: 0.5940; Loss self: 0.0000; time: 0.21s
Val loss: 0.1125 score: 0.9796 time: 0.08s
Test loss: 0.2810 score: 0.8980 time: 0.06s
Epoch 335/1000, LR 0.000205
Train loss: 0.5892;  Loss pred: 0.5892; Loss self: 0.0000; time: 5.98s
Val loss: 0.1117 score: 0.9796 time: 1.54s
Test loss: 0.2802 score: 0.8980 time: 1.90s
Epoch 336/1000, LR 0.000204
Train loss: 0.5907;  Loss pred: 0.5907; Loss self: 0.0000; time: 4.99s
Val loss: 0.1108 score: 0.9796 time: 0.57s
Test loss: 0.2795 score: 0.8980 time: 0.07s
Epoch 337/1000, LR 0.000204
Train loss: 0.5899;  Loss pred: 0.5899; Loss self: 0.0000; time: 0.20s
Val loss: 0.1100 score: 0.9796 time: 0.08s
Test loss: 0.2788 score: 0.8980 time: 0.06s
Epoch 338/1000, LR 0.000204
Train loss: 0.5904;  Loss pred: 0.5904; Loss self: 0.0000; time: 0.20s
Val loss: 0.1093 score: 0.9796 time: 0.07s
Test loss: 0.2782 score: 0.8980 time: 0.06s
Epoch 339/1000, LR 0.000203
Train loss: 0.5903;  Loss pred: 0.5903; Loss self: 0.0000; time: 0.20s
Val loss: 0.1085 score: 0.9796 time: 0.08s
Test loss: 0.2776 score: 0.8980 time: 0.06s
Epoch 340/1000, LR 0.000203
Train loss: 0.5859;  Loss pred: 0.5859; Loss self: 0.0000; time: 0.20s
Val loss: 0.1077 score: 0.9796 time: 0.07s
Test loss: 0.2770 score: 0.8980 time: 0.06s
Epoch 341/1000, LR 0.000203
Train loss: 0.5872;  Loss pred: 0.5872; Loss self: 0.0000; time: 0.20s
Val loss: 0.1070 score: 0.9796 time: 0.07s
Test loss: 0.2764 score: 0.8980 time: 0.06s
Epoch 342/1000, LR 0.000202
Train loss: 0.5866;  Loss pred: 0.5866; Loss self: 0.0000; time: 0.20s
Val loss: 0.1063 score: 0.9796 time: 0.08s
Test loss: 0.2759 score: 0.8980 time: 0.06s
Epoch 343/1000, LR 0.000202
Train loss: 0.5848;  Loss pred: 0.5848; Loss self: 0.0000; time: 0.20s
Val loss: 0.1056 score: 0.9796 time: 0.07s
Test loss: 0.2754 score: 0.8980 time: 0.06s
Epoch 344/1000, LR 0.000201
Train loss: 0.5851;  Loss pred: 0.5851; Loss self: 0.0000; time: 0.20s
Val loss: 0.1049 score: 0.9796 time: 0.07s
Test loss: 0.2749 score: 0.8980 time: 0.06s
Epoch 345/1000, LR 0.000201
Train loss: 0.5821;  Loss pred: 0.5821; Loss self: 0.0000; time: 0.20s
Val loss: 0.1042 score: 0.9796 time: 0.07s
Test loss: 0.2744 score: 0.8980 time: 0.06s
Epoch 346/1000, LR 0.000201
Train loss: 0.5846;  Loss pred: 0.5846; Loss self: 0.0000; time: 0.20s
Val loss: 0.1035 score: 0.9796 time: 0.07s
Test loss: 0.2739 score: 0.8980 time: 0.06s
Epoch 347/1000, LR 0.000200
Train loss: 0.5790;  Loss pred: 0.5790; Loss self: 0.0000; time: 0.20s
Val loss: 0.1028 score: 0.9796 time: 0.08s
Test loss: 0.2734 score: 0.8980 time: 0.06s
Epoch 348/1000, LR 0.000200
Train loss: 0.5809;  Loss pred: 0.5809; Loss self: 0.0000; time: 0.20s
Val loss: 0.1022 score: 0.9796 time: 0.07s
Test loss: 0.2730 score: 0.8980 time: 0.06s
Epoch 349/1000, LR 0.000200
Train loss: 0.5800;  Loss pred: 0.5800; Loss self: 0.0000; time: 0.20s
Val loss: 0.1015 score: 0.9796 time: 0.07s
Test loss: 0.2726 score: 0.8980 time: 0.06s
Epoch 350/1000, LR 0.000199
Train loss: 0.5794;  Loss pred: 0.5794; Loss self: 0.0000; time: 0.20s
Val loss: 0.1009 score: 0.9796 time: 0.08s
Test loss: 0.2723 score: 0.8980 time: 0.06s
Epoch 351/1000, LR 0.000199
Train loss: 0.5791;  Loss pred: 0.5791; Loss self: 0.0000; time: 0.22s
Val loss: 0.1003 score: 0.9796 time: 0.95s
Test loss: 0.2720 score: 0.8980 time: 1.71s
Epoch 352/1000, LR 0.000198
Train loss: 0.5798;  Loss pred: 0.5798; Loss self: 0.0000; time: 2.49s
Val loss: 0.0997 score: 0.9796 time: 0.25s
Test loss: 0.2717 score: 0.8980 time: 0.27s
Epoch 353/1000, LR 0.000198
Train loss: 0.5781;  Loss pred: 0.5781; Loss self: 0.0000; time: 0.40s
Val loss: 0.0991 score: 0.9796 time: 0.07s
Test loss: 0.2714 score: 0.8980 time: 0.06s
Epoch 354/1000, LR 0.000198
Train loss: 0.5792;  Loss pred: 0.5792; Loss self: 0.0000; time: 0.21s
Val loss: 0.0985 score: 0.9796 time: 0.07s
Test loss: 0.2711 score: 0.8980 time: 0.07s
Epoch 355/1000, LR 0.000197
Train loss: 0.5766;  Loss pred: 0.5766; Loss self: 0.0000; time: 0.21s
Val loss: 0.0979 score: 0.9796 time: 0.07s
Test loss: 0.2708 score: 0.8980 time: 0.06s
Epoch 356/1000, LR 0.000197
Train loss: 0.5797;  Loss pred: 0.5797; Loss self: 0.0000; time: 0.21s
Val loss: 0.0973 score: 0.9796 time: 0.07s
Test loss: 0.2703 score: 0.8980 time: 0.06s
Epoch 357/1000, LR 0.000196
Train loss: 0.5812;  Loss pred: 0.5812; Loss self: 0.0000; time: 0.20s
Val loss: 0.0967 score: 0.9796 time: 0.07s
Test loss: 0.2697 score: 0.8980 time: 0.07s
Epoch 358/1000, LR 0.000196
Train loss: 0.5735;  Loss pred: 0.5735; Loss self: 0.0000; time: 0.21s
Val loss: 0.0961 score: 0.9796 time: 0.07s
Test loss: 0.2692 score: 0.8980 time: 0.06s
Epoch 359/1000, LR 0.000196
Train loss: 0.5755;  Loss pred: 0.5755; Loss self: 0.0000; time: 0.21s
Val loss: 0.0955 score: 0.9796 time: 0.08s
Test loss: 0.2688 score: 0.8980 time: 0.06s
Epoch 360/1000, LR 0.000195
Train loss: 0.5789;  Loss pred: 0.5789; Loss self: 0.0000; time: 0.21s
Val loss: 0.0949 score: 0.9796 time: 0.07s
Test loss: 0.2683 score: 0.8980 time: 0.07s
Epoch 361/1000, LR 0.000195
Train loss: 0.5747;  Loss pred: 0.5747; Loss self: 0.0000; time: 0.21s
Val loss: 0.0944 score: 0.9796 time: 0.07s
Test loss: 0.2678 score: 0.8980 time: 0.07s
Epoch 362/1000, LR 0.000195
Train loss: 0.5753;  Loss pred: 0.5753; Loss self: 0.0000; time: 0.21s
Val loss: 0.0938 score: 0.9796 time: 0.07s
Test loss: 0.2673 score: 0.8980 time: 0.06s
Epoch 363/1000, LR 0.000194
Train loss: 0.5752;  Loss pred: 0.5752; Loss self: 0.0000; time: 0.20s
Val loss: 0.0932 score: 0.9796 time: 0.07s
Test loss: 0.2668 score: 0.8980 time: 0.06s
Epoch 364/1000, LR 0.000194
Train loss: 0.5725;  Loss pred: 0.5725; Loss self: 0.0000; time: 0.21s
Val loss: 0.0927 score: 0.9796 time: 0.08s
Test loss: 0.2664 score: 0.8980 time: 1.19s
Epoch 365/1000, LR 0.000193
Train loss: 0.5758;  Loss pred: 0.5758; Loss self: 0.0000; time: 3.51s
Val loss: 0.0921 score: 0.9796 time: 0.17s
Test loss: 0.2659 score: 0.8980 time: 0.07s
Epoch 366/1000, LR 0.000193
Train loss: 0.5727;  Loss pred: 0.5727; Loss self: 0.0000; time: 0.21s
Val loss: 0.0916 score: 0.9796 time: 0.07s
Test loss: 0.2654 score: 0.8980 time: 0.06s
Epoch 367/1000, LR 0.000193
Train loss: 0.5719;  Loss pred: 0.5719; Loss self: 0.0000; time: 0.21s
Val loss: 0.0910 score: 0.9796 time: 0.08s
Test loss: 0.2650 score: 0.8980 time: 0.15s
Epoch 368/1000, LR 0.000192
Train loss: 0.5687;  Loss pred: 0.5687; Loss self: 0.0000; time: 0.21s
Val loss: 0.0905 score: 0.9796 time: 0.07s
Test loss: 0.2647 score: 0.8980 time: 0.06s
Epoch 369/1000, LR 0.000192
Train loss: 0.5708;  Loss pred: 0.5708; Loss self: 0.0000; time: 0.21s
Val loss: 0.0901 score: 0.9796 time: 0.07s
Test loss: 0.2644 score: 0.8980 time: 0.06s
Epoch 370/1000, LR 0.000191
Train loss: 0.5701;  Loss pred: 0.5701; Loss self: 0.0000; time: 0.20s
Val loss: 0.0896 score: 0.9796 time: 0.08s
Test loss: 0.2641 score: 0.8980 time: 0.06s
Epoch 371/1000, LR 0.000191
Train loss: 0.5705;  Loss pred: 0.5705; Loss self: 0.0000; time: 0.20s
Val loss: 0.0891 score: 0.9796 time: 0.07s
Test loss: 0.2638 score: 0.8980 time: 0.07s
Epoch 372/1000, LR 0.000191
Train loss: 0.5673;  Loss pred: 0.5673; Loss self: 0.0000; time: 0.26s
Val loss: 0.0886 score: 0.9796 time: 0.08s
Test loss: 0.2637 score: 0.8980 time: 0.06s
Epoch 373/1000, LR 0.000190
Train loss: 0.5687;  Loss pred: 0.5687; Loss self: 0.0000; time: 0.21s
Val loss: 0.0882 score: 0.9796 time: 0.08s
Test loss: 0.2635 score: 0.8980 time: 0.06s
Epoch 374/1000, LR 0.000190
Train loss: 0.5691;  Loss pred: 0.5691; Loss self: 0.0000; time: 3.71s
Val loss: 0.0878 score: 0.9796 time: 0.44s
Test loss: 0.2634 score: 0.8980 time: 0.36s
Epoch 375/1000, LR 0.000190
Train loss: 0.5691;  Loss pred: 0.5691; Loss self: 0.0000; time: 1.04s
Val loss: 0.0874 score: 0.9796 time: 0.37s
Test loss: 0.2633 score: 0.8980 time: 0.20s
Epoch 376/1000, LR 0.000189
Train loss: 0.5677;  Loss pred: 0.5677; Loss self: 0.0000; time: 1.53s
Val loss: 0.0870 score: 0.9796 time: 0.27s
Test loss: 0.2633 score: 0.8980 time: 0.19s
Epoch 377/1000, LR 0.000189
Train loss: 0.5670;  Loss pred: 0.5670; Loss self: 0.0000; time: 0.51s
Val loss: 0.0866 score: 0.9796 time: 0.07s
Test loss: 0.2633 score: 0.8980 time: 0.07s
Epoch 378/1000, LR 0.000188
Train loss: 0.5675;  Loss pred: 0.5675; Loss self: 0.0000; time: 0.21s
Val loss: 0.0862 score: 0.9796 time: 0.08s
Test loss: 0.2631 score: 0.8980 time: 0.06s
Epoch 379/1000, LR 0.000188
Train loss: 0.5663;  Loss pred: 0.5663; Loss self: 0.0000; time: 0.21s
Val loss: 0.0857 score: 0.9796 time: 0.08s
Test loss: 0.2630 score: 0.8980 time: 0.06s
Epoch 380/1000, LR 0.000188
Train loss: 0.5654;  Loss pred: 0.5654; Loss self: 0.0000; time: 0.21s
Val loss: 0.0853 score: 0.9796 time: 0.07s
Test loss: 0.2628 score: 0.8980 time: 0.06s
Epoch 381/1000, LR 0.000187
Train loss: 0.5671;  Loss pred: 0.5671; Loss self: 0.0000; time: 0.21s
Val loss: 0.0849 score: 0.9796 time: 0.07s
Test loss: 0.2625 score: 0.8980 time: 0.06s
Epoch 382/1000, LR 0.000187
Train loss: 0.5653;  Loss pred: 0.5653; Loss self: 0.0000; time: 0.20s
Val loss: 0.0845 score: 0.9796 time: 0.08s
Test loss: 0.2622 score: 0.8980 time: 0.06s
Epoch 383/1000, LR 0.000186
Train loss: 0.5665;  Loss pred: 0.5665; Loss self: 0.0000; time: 0.21s
Val loss: 0.0840 score: 0.9796 time: 0.07s
Test loss: 0.2618 score: 0.8980 time: 0.07s
Epoch 384/1000, LR 0.000186
Train loss: 0.5661;  Loss pred: 0.5661; Loss self: 0.0000; time: 0.21s
Val loss: 0.0836 score: 0.9796 time: 0.07s
Test loss: 0.2614 score: 0.8980 time: 0.06s
Epoch 385/1000, LR 0.000186
Train loss: 0.5659;  Loss pred: 0.5659; Loss self: 0.0000; time: 0.21s
Val loss: 0.0831 score: 0.9796 time: 0.07s
Test loss: 0.2608 score: 0.8980 time: 0.07s
Epoch 386/1000, LR 0.000185
Train loss: 0.5649;  Loss pred: 0.5649; Loss self: 0.0000; time: 0.21s
Val loss: 0.0826 score: 0.9796 time: 0.07s
Test loss: 0.2603 score: 0.8980 time: 0.07s
Epoch 387/1000, LR 0.000185
Train loss: 0.5624;  Loss pred: 0.5624; Loss self: 0.0000; time: 0.22s
Val loss: 0.0822 score: 0.9796 time: 0.82s
Test loss: 0.2597 score: 0.8980 time: 2.21s
Epoch 388/1000, LR 0.000184
Train loss: 0.5632;  Loss pred: 0.5632; Loss self: 0.0000; time: 4.00s
Val loss: 0.0817 score: 0.9796 time: 0.07s
Test loss: 0.2592 score: 0.8980 time: 0.07s
Epoch 389/1000, LR 0.000184
Train loss: 0.5624;  Loss pred: 0.5624; Loss self: 0.0000; time: 0.21s
Val loss: 0.0813 score: 0.9796 time: 0.07s
Test loss: 0.2587 score: 0.8980 time: 0.06s
Epoch 390/1000, LR 0.000184
Train loss: 0.5618;  Loss pred: 0.5618; Loss self: 0.0000; time: 0.20s
Val loss: 0.0809 score: 0.9796 time: 0.07s
Test loss: 0.2584 score: 0.8980 time: 0.06s
Epoch 391/1000, LR 0.000183
Train loss: 0.5630;  Loss pred: 0.5630; Loss self: 0.0000; time: 0.20s
Val loss: 0.0805 score: 0.9796 time: 0.06s
Test loss: 0.2581 score: 0.8980 time: 0.06s
Epoch 392/1000, LR 0.000183
Train loss: 0.5611;  Loss pred: 0.5611; Loss self: 0.0000; time: 0.20s
Val loss: 0.0801 score: 0.9796 time: 0.07s
Test loss: 0.2578 score: 0.8980 time: 0.06s
Epoch 393/1000, LR 0.000182
Train loss: 0.5653;  Loss pred: 0.5653; Loss self: 0.0000; time: 0.19s
Val loss: 0.0797 score: 0.9796 time: 0.07s
Test loss: 0.2576 score: 0.8980 time: 0.06s
Epoch 394/1000, LR 0.000182
Train loss: 0.5599;  Loss pred: 0.5599; Loss self: 0.0000; time: 0.20s
Val loss: 0.0793 score: 0.9796 time: 0.07s
Test loss: 0.2574 score: 0.8980 time: 0.06s
Epoch 395/1000, LR 0.000182
Train loss: 0.5607;  Loss pred: 0.5607; Loss self: 0.0000; time: 0.20s
Val loss: 0.0790 score: 0.9796 time: 0.07s
Test loss: 0.2573 score: 0.8980 time: 0.06s
Epoch 396/1000, LR 0.000181
Train loss: 0.5595;  Loss pred: 0.5595; Loss self: 0.0000; time: 0.20s
Val loss: 0.0787 score: 0.9796 time: 0.07s
Test loss: 0.2573 score: 0.8980 time: 0.06s
Epoch 397/1000, LR 0.000181
Train loss: 0.5615;  Loss pred: 0.5615; Loss self: 0.0000; time: 0.20s
Val loss: 0.0784 score: 0.9796 time: 0.08s
Test loss: 0.2572 score: 0.8980 time: 0.06s
Epoch 398/1000, LR 0.000180
Train loss: 0.5587;  Loss pred: 0.5587; Loss self: 0.0000; time: 0.20s
Val loss: 0.0781 score: 0.9796 time: 0.07s
Test loss: 0.2573 score: 0.8980 time: 0.06s
Epoch 399/1000, LR 0.000180
Train loss: 0.5597;  Loss pred: 0.5597; Loss self: 0.0000; time: 0.21s
Val loss: 0.0778 score: 0.9796 time: 0.07s
Test loss: 0.2573 score: 0.8980 time: 0.06s
Epoch 400/1000, LR 0.000180
Train loss: 0.5607;  Loss pred: 0.5607; Loss self: 0.0000; time: 0.20s
Val loss: 0.0775 score: 0.9796 time: 0.08s
Test loss: 0.2573 score: 0.8980 time: 0.06s
Epoch 401/1000, LR 0.000179
Train loss: 0.5593;  Loss pred: 0.5593; Loss self: 0.0000; time: 0.87s
Val loss: 0.0772 score: 0.9796 time: 1.73s
Test loss: 0.2573 score: 0.8980 time: 1.31s
Epoch 402/1000, LR 0.000179
Train loss: 0.5607;  Loss pred: 0.5607; Loss self: 0.0000; time: 3.30s
Val loss: 0.0769 score: 0.9796 time: 0.07s
Test loss: 0.2572 score: 0.8980 time: 0.07s
Epoch 403/1000, LR 0.000178
Train loss: 0.5588;  Loss pred: 0.5588; Loss self: 0.0000; time: 0.21s
Val loss: 0.0766 score: 0.9796 time: 0.07s
Test loss: 0.2570 score: 0.8980 time: 0.06s
Epoch 404/1000, LR 0.000178
Train loss: 0.5577;  Loss pred: 0.5577; Loss self: 0.0000; time: 0.21s
Val loss: 0.0762 score: 0.9796 time: 0.08s
Test loss: 0.2569 score: 0.8980 time: 0.06s
Epoch 405/1000, LR 0.000178
Train loss: 0.5590;  Loss pred: 0.5590; Loss self: 0.0000; time: 0.21s
Val loss: 0.0759 score: 0.9796 time: 0.08s
Test loss: 0.2567 score: 0.8980 time: 0.07s
Epoch 406/1000, LR 0.000177
Train loss: 0.5571;  Loss pred: 0.5571; Loss self: 0.0000; time: 0.21s
Val loss: 0.0756 score: 0.9796 time: 0.07s
Test loss: 0.2565 score: 0.8980 time: 0.06s
Epoch 407/1000, LR 0.000177
Train loss: 0.5570;  Loss pred: 0.5570; Loss self: 0.0000; time: 0.21s
Val loss: 0.0753 score: 0.9796 time: 0.07s
Test loss: 0.2562 score: 0.8980 time: 0.06s
Epoch 408/1000, LR 0.000176
Train loss: 0.5566;  Loss pred: 0.5566; Loss self: 0.0000; time: 0.21s
Val loss: 0.0749 score: 0.9796 time: 0.07s
Test loss: 0.2559 score: 0.8980 time: 1.90s
Epoch 409/1000, LR 0.000176
Train loss: 0.5568;  Loss pred: 0.5568; Loss self: 0.0000; time: 6.05s
Val loss: 0.0746 score: 0.9796 time: 0.21s
Test loss: 0.2555 score: 0.8980 time: 0.34s
Epoch 410/1000, LR 0.000175
Train loss: 0.5541;  Loss pred: 0.5541; Loss self: 0.0000; time: 0.46s
Val loss: 0.0742 score: 0.9796 time: 0.07s
Test loss: 0.2551 score: 0.8980 time: 0.06s
Epoch 411/1000, LR 0.000175
Train loss: 0.5557;  Loss pred: 0.5557; Loss self: 0.0000; time: 0.21s
Val loss: 0.0738 score: 0.9796 time: 0.07s
Test loss: 0.2546 score: 0.8980 time: 0.06s
Epoch 412/1000, LR 0.000175
Train loss: 0.5575;  Loss pred: 0.5575; Loss self: 0.0000; time: 0.21s
Val loss: 0.0735 score: 0.9796 time: 0.07s
Test loss: 0.2542 score: 0.8980 time: 0.06s
Epoch 413/1000, LR 0.000174
Train loss: 0.5566;  Loss pred: 0.5566; Loss self: 0.0000; time: 0.20s
Val loss: 0.0731 score: 0.9796 time: 0.07s
Test loss: 0.2537 score: 0.8980 time: 0.07s
Epoch 414/1000, LR 0.000174
Train loss: 0.5544;  Loss pred: 0.5544; Loss self: 0.0000; time: 0.20s
Val loss: 0.0728 score: 0.9796 time: 0.07s
Test loss: 0.2533 score: 0.8980 time: 0.06s
Epoch 415/1000, LR 0.000173
Train loss: 0.5550;  Loss pred: 0.5550; Loss self: 0.0000; time: 0.20s
Val loss: 0.0724 score: 0.9796 time: 0.07s
Test loss: 0.2529 score: 0.8980 time: 0.06s
Epoch 416/1000, LR 0.000173
Train loss: 0.5539;  Loss pred: 0.5539; Loss self: 0.0000; time: 0.20s
Val loss: 0.0721 score: 0.9796 time: 0.08s
Test loss: 0.2525 score: 0.8980 time: 0.06s
Epoch 417/1000, LR 0.000173
Train loss: 0.5541;  Loss pred: 0.5541; Loss self: 0.0000; time: 0.20s
Val loss: 0.0718 score: 0.9796 time: 0.07s
Test loss: 0.2523 score: 0.8980 time: 0.06s
Epoch 418/1000, LR 0.000172
Train loss: 0.5537;  Loss pred: 0.5537; Loss self: 0.0000; time: 0.20s
Val loss: 0.0715 score: 0.9796 time: 0.07s
Test loss: 0.2521 score: 0.8980 time: 0.06s
Epoch 419/1000, LR 0.000172
Train loss: 0.5529;  Loss pred: 0.5529; Loss self: 0.0000; time: 1.90s
Val loss: 0.0712 score: 0.9796 time: 0.79s
Test loss: 0.2519 score: 0.8980 time: 0.60s
Epoch 420/1000, LR 0.000171
Train loss: 0.5540;  Loss pred: 0.5540; Loss self: 0.0000; time: 1.22s
Val loss: 0.0710 score: 0.9796 time: 0.10s
Test loss: 0.2518 score: 0.8980 time: 0.08s
Epoch 421/1000, LR 0.000171
Train loss: 0.5515;  Loss pred: 0.5515; Loss self: 0.0000; time: 0.22s
Val loss: 0.0707 score: 0.9796 time: 0.08s
Test loss: 0.2517 score: 0.8980 time: 0.07s
Epoch 422/1000, LR 0.000171
Train loss: 0.5501;  Loss pred: 0.5501; Loss self: 0.0000; time: 0.22s
Val loss: 0.0705 score: 0.9796 time: 0.08s
Test loss: 0.2518 score: 0.8980 time: 0.06s
Epoch 423/1000, LR 0.000170
Train loss: 0.5525;  Loss pred: 0.5525; Loss self: 0.0000; time: 0.20s
Val loss: 0.0703 score: 0.9796 time: 0.07s
Test loss: 0.2519 score: 0.8980 time: 0.06s
Epoch 424/1000, LR 0.000170
Train loss: 0.5516;  Loss pred: 0.5516; Loss self: 0.0000; time: 0.20s
Val loss: 0.0700 score: 0.9796 time: 0.08s
Test loss: 0.2520 score: 0.8980 time: 0.06s
Epoch 425/1000, LR 0.000169
Train loss: 0.5524;  Loss pred: 0.5524; Loss self: 0.0000; time: 0.21s
Val loss: 0.0698 score: 0.9796 time: 0.07s
Test loss: 0.2521 score: 0.8980 time: 0.06s
Epoch 426/1000, LR 0.000169
Train loss: 0.5524;  Loss pred: 0.5524; Loss self: 0.0000; time: 0.20s
Val loss: 0.0696 score: 0.9796 time: 0.07s
Test loss: 0.2522 score: 0.8980 time: 0.06s
Epoch 427/1000, LR 0.000168
Train loss: 0.5507;  Loss pred: 0.5507; Loss self: 0.0000; time: 0.21s
Val loss: 0.0694 score: 0.9796 time: 0.07s
Test loss: 0.2522 score: 0.8980 time: 0.06s
Epoch 428/1000, LR 0.000168
Train loss: 0.5511;  Loss pred: 0.5511; Loss self: 0.0000; time: 0.20s
Val loss: 0.0692 score: 0.9796 time: 0.07s
Test loss: 0.2522 score: 0.8980 time: 0.06s
Epoch 429/1000, LR 0.000168
Train loss: 0.5530;  Loss pred: 0.5530; Loss self: 0.0000; time: 0.20s
Val loss: 0.0689 score: 0.9796 time: 0.08s
Test loss: 0.2521 score: 0.8980 time: 0.06s
Epoch 430/1000, LR 0.000167
Train loss: 0.5510;  Loss pred: 0.5510; Loss self: 0.0000; time: 0.21s
Val loss: 0.0686 score: 0.9796 time: 2.33s
Test loss: 0.2519 score: 0.8980 time: 2.23s
Epoch 431/1000, LR 0.000167
Train loss: 0.5500;  Loss pred: 0.5500; Loss self: 0.0000; time: 1.19s
Val loss: 0.0684 score: 0.9796 time: 0.07s
Test loss: 0.2517 score: 0.8980 time: 0.06s
Epoch 432/1000, LR 0.000166
Train loss: 0.5501;  Loss pred: 0.5501; Loss self: 0.0000; time: 0.20s
Val loss: 0.0681 score: 0.9796 time: 0.08s
Test loss: 0.2515 score: 0.8980 time: 0.06s
Epoch 433/1000, LR 0.000166
Train loss: 0.5499;  Loss pred: 0.5499; Loss self: 0.0000; time: 0.21s
Val loss: 0.0678 score: 0.9796 time: 0.07s
Test loss: 0.2513 score: 0.8980 time: 0.06s
Epoch 434/1000, LR 0.000166
Train loss: 0.5496;  Loss pred: 0.5496; Loss self: 0.0000; time: 0.22s
Val loss: 0.0676 score: 0.9796 time: 0.11s
Test loss: 0.2511 score: 0.8980 time: 0.06s
Epoch 435/1000, LR 0.000165
Train loss: 0.5484;  Loss pred: 0.5484; Loss self: 0.0000; time: 0.20s
Val loss: 0.0674 score: 0.9796 time: 0.07s
Test loss: 0.2510 score: 0.8980 time: 0.06s
Epoch 436/1000, LR 0.000165
Train loss: 0.5498;  Loss pred: 0.5498; Loss self: 0.0000; time: 0.20s
Val loss: 0.0671 score: 0.9796 time: 0.07s
Test loss: 0.2509 score: 0.8980 time: 0.06s
Epoch 437/1000, LR 0.000164
Train loss: 0.5499;  Loss pred: 0.5499; Loss self: 0.0000; time: 0.21s
Val loss: 0.0669 score: 0.9796 time: 0.07s
Test loss: 0.2509 score: 0.8980 time: 0.06s
Epoch 438/1000, LR 0.000164
Train loss: 0.5531;  Loss pred: 0.5531; Loss self: 0.0000; time: 0.19s
Val loss: 0.0666 score: 0.9796 time: 0.07s
Test loss: 0.2505 score: 0.8980 time: 0.06s
Epoch 439/1000, LR 0.000163
Train loss: 0.5466;  Loss pred: 0.5466; Loss self: 0.0000; time: 0.27s
Val loss: 0.0664 score: 0.9796 time: 0.07s
Test loss: 0.2502 score: 0.8980 time: 0.06s
Epoch 440/1000, LR 0.000163
Train loss: 0.5494;  Loss pred: 0.5494; Loss self: 0.0000; time: 0.20s
Val loss: 0.0661 score: 0.9796 time: 0.07s
Test loss: 0.2498 score: 0.8980 time: 0.06s
Epoch 441/1000, LR 0.000163
Train loss: 0.5474;  Loss pred: 0.5474; Loss self: 0.0000; time: 0.20s
Val loss: 0.0658 score: 0.9796 time: 0.08s
Test loss: 0.2494 score: 0.8980 time: 0.06s
Epoch 442/1000, LR 0.000162
Train loss: 0.5485;  Loss pred: 0.5485; Loss self: 0.0000; time: 0.20s
Val loss: 0.0655 score: 0.9796 time: 0.07s
Test loss: 0.2490 score: 0.8980 time: 0.06s
Epoch 443/1000, LR 0.000162
Train loss: 0.5477;  Loss pred: 0.5477; Loss self: 0.0000; time: 0.19s
Val loss: 0.0653 score: 0.9796 time: 0.08s
Test loss: 0.2487 score: 0.8980 time: 0.06s
Epoch 444/1000, LR 0.000161
Train loss: 0.5467;  Loss pred: 0.5467; Loss self: 0.0000; time: 0.20s
Val loss: 0.0650 score: 0.9796 time: 0.07s
Test loss: 0.2484 score: 0.8980 time: 0.06s
Epoch 445/1000, LR 0.000161
Train loss: 0.5484;  Loss pred: 0.5484; Loss self: 0.0000; time: 0.20s
Val loss: 0.0647 score: 0.9796 time: 0.07s
Test loss: 0.2480 score: 0.8980 time: 0.07s
Epoch 446/1000, LR 0.000161
Train loss: 0.5462;  Loss pred: 0.5462; Loss self: 0.0000; time: 0.21s
Val loss: 0.0645 score: 0.9796 time: 0.07s
Test loss: 0.2478 score: 0.8980 time: 0.07s
Epoch 447/1000, LR 0.000160
Train loss: 0.5472;  Loss pred: 0.5472; Loss self: 0.0000; time: 2.60s
Val loss: 0.0643 score: 0.9796 time: 0.33s
Test loss: 0.2476 score: 0.9184 time: 1.73s
Epoch 448/1000, LR 0.000160
Train loss: 0.5464;  Loss pred: 0.5464; Loss self: 0.0000; time: 0.58s
Val loss: 0.0641 score: 0.9796 time: 0.08s
Test loss: 0.2475 score: 0.9184 time: 0.06s
Epoch 449/1000, LR 0.000159
Train loss: 0.5474;  Loss pred: 0.5474; Loss self: 0.0000; time: 0.21s
Val loss: 0.0639 score: 0.9796 time: 0.07s
Test loss: 0.2473 score: 0.9184 time: 0.06s
Epoch 450/1000, LR 0.000159
Train loss: 0.5467;  Loss pred: 0.5467; Loss self: 0.0000; time: 0.21s
Val loss: 0.0637 score: 0.9796 time: 0.08s
Test loss: 0.2472 score: 0.9184 time: 0.06s
Epoch 451/1000, LR 0.000158
Train loss: 0.5460;  Loss pred: 0.5460; Loss self: 0.0000; time: 0.21s
Val loss: 0.0635 score: 0.9796 time: 0.07s
Test loss: 0.2471 score: 0.9184 time: 0.06s
Epoch 452/1000, LR 0.000158
Train loss: 0.5456;  Loss pred: 0.5456; Loss self: 0.0000; time: 0.20s
Val loss: 0.0633 score: 0.9796 time: 0.07s
Test loss: 0.2470 score: 0.9184 time: 0.06s
Epoch 453/1000, LR 0.000158
Train loss: 0.5452;  Loss pred: 0.5452; Loss self: 0.0000; time: 0.21s
Val loss: 0.0631 score: 0.9796 time: 0.07s
Test loss: 0.2470 score: 0.9184 time: 0.06s
Epoch 454/1000, LR 0.000157
Train loss: 0.5457;  Loss pred: 0.5457; Loss self: 0.0000; time: 0.20s
Val loss: 0.0629 score: 0.9796 time: 0.07s
Test loss: 0.2470 score: 0.9184 time: 0.06s
Epoch 455/1000, LR 0.000157
Train loss: 0.5451;  Loss pred: 0.5451; Loss self: 0.0000; time: 0.20s
Val loss: 0.0627 score: 0.9796 time: 0.07s
Test loss: 0.2469 score: 0.9184 time: 0.06s
Epoch 456/1000, LR 0.000156
Train loss: 0.5445;  Loss pred: 0.5445; Loss self: 0.0000; time: 0.22s
Val loss: 0.0625 score: 0.9796 time: 0.93s
Test loss: 0.2468 score: 0.9184 time: 0.55s
Epoch 457/1000, LR 0.000156
Train loss: 0.5461;  Loss pred: 0.5461; Loss self: 0.0000; time: 1.38s
Val loss: 0.0623 score: 0.9796 time: 0.56s
Test loss: 0.2467 score: 0.9184 time: 0.48s
Epoch 458/1000, LR 0.000155
Train loss: 0.5445;  Loss pred: 0.5445; Loss self: 0.0000; time: 0.29s
Val loss: 0.0621 score: 0.9796 time: 0.08s
Test loss: 0.2466 score: 0.9184 time: 0.06s
Epoch 459/1000, LR 0.000155
Train loss: 0.5438;  Loss pred: 0.5438; Loss self: 0.0000; time: 0.21s
Val loss: 0.0619 score: 0.9796 time: 0.07s
Test loss: 0.2464 score: 0.9184 time: 0.06s
Epoch 460/1000, LR 0.000155
Train loss: 0.5440;  Loss pred: 0.5440; Loss self: 0.0000; time: 0.21s
Val loss: 0.0618 score: 0.9796 time: 0.07s
Test loss: 0.2464 score: 0.9184 time: 0.07s
Epoch 461/1000, LR 0.000154
Train loss: 0.5447;  Loss pred: 0.5447; Loss self: 0.0000; time: 0.21s
Val loss: 0.0616 score: 0.9796 time: 0.08s
Test loss: 0.2463 score: 0.9184 time: 0.06s
Epoch 462/1000, LR 0.000154
Train loss: 0.5427;  Loss pred: 0.5427; Loss self: 0.0000; time: 0.21s
Val loss: 0.0614 score: 0.9796 time: 0.09s
Test loss: 0.2462 score: 0.9184 time: 0.06s
Epoch 463/1000, LR 0.000153
Train loss: 0.5422;  Loss pred: 0.5422; Loss self: 0.0000; time: 0.21s
Val loss: 0.0612 score: 0.9796 time: 0.07s
Test loss: 0.2462 score: 0.9184 time: 0.06s
Epoch 464/1000, LR 0.000153
Train loss: 0.5434;  Loss pred: 0.5434; Loss self: 0.0000; time: 0.21s
Val loss: 0.0611 score: 0.9796 time: 0.07s
Test loss: 0.2461 score: 0.9184 time: 0.06s
Epoch 465/1000, LR 0.000153
Train loss: 0.5443;  Loss pred: 0.5443; Loss self: 0.0000; time: 0.21s
Val loss: 0.0609 score: 0.9796 time: 0.08s
Test loss: 0.2460 score: 0.9184 time: 0.07s
Epoch 466/1000, LR 0.000152
Train loss: 0.5417;  Loss pred: 0.5417; Loss self: 0.0000; time: 0.22s
Val loss: 0.0607 score: 0.9796 time: 0.08s
Test loss: 0.2460 score: 0.9184 time: 0.07s
Epoch 467/1000, LR 0.000152
Train loss: 0.5432;  Loss pred: 0.5432; Loss self: 0.0000; time: 0.22s
Val loss: 0.0606 score: 0.9796 time: 0.07s
Test loss: 0.2461 score: 0.9184 time: 0.07s
Epoch 468/1000, LR 0.000151
Train loss: 0.5416;  Loss pred: 0.5416; Loss self: 0.0000; time: 0.22s
Val loss: 0.0605 score: 0.9796 time: 0.08s
Test loss: 0.2463 score: 0.9184 time: 0.07s
Epoch 469/1000, LR 0.000151
Train loss: 0.5421;  Loss pred: 0.5421; Loss self: 0.0000; time: 0.22s
Val loss: 0.0603 score: 0.9796 time: 0.08s
Test loss: 0.2464 score: 0.9184 time: 0.06s
Epoch 470/1000, LR 0.000150
Train loss: 0.5430;  Loss pred: 0.5430; Loss self: 0.0000; time: 0.22s
Val loss: 0.0602 score: 0.9796 time: 0.07s
Test loss: 0.2465 score: 0.9184 time: 0.07s
Epoch 471/1000, LR 0.000150
Train loss: 0.5411;  Loss pred: 0.5411; Loss self: 0.0000; time: 4.52s
Val loss: 0.0601 score: 0.9796 time: 0.92s
Test loss: 0.2467 score: 0.9184 time: 0.31s
Epoch 472/1000, LR 0.000150
Train loss: 0.5417;  Loss pred: 0.5417; Loss self: 0.0000; time: 0.81s
Val loss: 0.0600 score: 0.9796 time: 0.18s
Test loss: 0.2468 score: 0.9184 time: 0.13s
Epoch 473/1000, LR 0.000149
Train loss: 0.5422;  Loss pred: 0.5422; Loss self: 0.0000; time: 0.21s
Val loss: 0.0598 score: 0.9796 time: 0.08s
Test loss: 0.2469 score: 0.9184 time: 0.06s
Epoch 474/1000, LR 0.000149
Train loss: 0.5407;  Loss pred: 0.5407; Loss self: 0.0000; time: 0.21s
Val loss: 0.0597 score: 0.9796 time: 0.07s
Test loss: 0.2470 score: 0.9184 time: 0.06s
Epoch 475/1000, LR 0.000148
Train loss: 0.5411;  Loss pred: 0.5411; Loss self: 0.0000; time: 0.21s
Val loss: 0.0596 score: 0.9796 time: 0.08s
Test loss: 0.2470 score: 0.9184 time: 0.06s
Epoch 476/1000, LR 0.000148
Train loss: 0.5424;  Loss pred: 0.5424; Loss self: 0.0000; time: 0.21s
Val loss: 0.0594 score: 0.9796 time: 0.08s
Test loss: 0.2469 score: 0.9184 time: 0.06s
Epoch 477/1000, LR 0.000147
Train loss: 0.5416;  Loss pred: 0.5416; Loss self: 0.0000; time: 0.22s
Val loss: 0.0592 score: 0.9796 time: 0.08s
Test loss: 0.2466 score: 0.9184 time: 0.07s
Epoch 478/1000, LR 0.000147
Train loss: 0.5411;  Loss pred: 0.5411; Loss self: 0.0000; time: 0.21s
Val loss: 0.0590 score: 0.9796 time: 0.08s
Test loss: 0.2462 score: 0.9184 time: 0.06s
Epoch 479/1000, LR 0.000147
Train loss: 0.5411;  Loss pred: 0.5411; Loss self: 0.0000; time: 0.20s
Val loss: 0.0587 score: 0.9796 time: 0.81s
Test loss: 0.2459 score: 0.9184 time: 1.78s
Epoch 480/1000, LR 0.000146
Train loss: 0.5416;  Loss pred: 0.5416; Loss self: 0.0000; time: 4.49s
Val loss: 0.0585 score: 0.9796 time: 0.29s
Test loss: 0.2454 score: 0.9184 time: 0.22s
Epoch 481/1000, LR 0.000146
Train loss: 0.5414;  Loss pred: 0.5414; Loss self: 0.0000; time: 0.85s
Val loss: 0.0583 score: 0.9796 time: 0.19s
Test loss: 0.2450 score: 0.9184 time: 0.07s
Epoch 482/1000, LR 0.000145
Train loss: 0.5395;  Loss pred: 0.5395; Loss self: 0.0000; time: 0.21s
Val loss: 0.0581 score: 0.9796 time: 0.07s
Test loss: 0.2447 score: 0.9184 time: 0.06s
Epoch 483/1000, LR 0.000145
Train loss: 0.5417;  Loss pred: 0.5417; Loss self: 0.0000; time: 0.20s
Val loss: 0.0579 score: 0.9796 time: 0.07s
Test loss: 0.2442 score: 0.9184 time: 0.06s
Epoch 484/1000, LR 0.000144
Train loss: 0.5387;  Loss pred: 0.5387; Loss self: 0.0000; time: 0.20s
Val loss: 0.0577 score: 0.9796 time: 0.07s
Test loss: 0.2439 score: 0.9184 time: 0.06s
Epoch 485/1000, LR 0.000144
Train loss: 0.5401;  Loss pred: 0.5401; Loss self: 0.0000; time: 0.20s
Val loss: 0.0575 score: 0.9796 time: 0.07s
Test loss: 0.2436 score: 0.9184 time: 0.06s
Epoch 486/1000, LR 0.000144
Train loss: 0.5372;  Loss pred: 0.5372; Loss self: 0.0000; time: 0.20s
Val loss: 0.0573 score: 0.9796 time: 0.07s
Test loss: 0.2434 score: 0.9184 time: 0.06s
Epoch 487/1000, LR 0.000143
Train loss: 0.5379;  Loss pred: 0.5379; Loss self: 0.0000; time: 0.20s
Val loss: 0.0571 score: 0.9796 time: 0.08s
Test loss: 0.2432 score: 0.9184 time: 0.06s
Epoch 488/1000, LR 0.000143
Train loss: 0.5403;  Loss pred: 0.5403; Loss self: 0.0000; time: 0.20s
Val loss: 0.0570 score: 0.9796 time: 0.07s
Test loss: 0.2431 score: 0.9184 time: 0.06s
Epoch 489/1000, LR 0.000142
Train loss: 0.5373;  Loss pred: 0.5373; Loss self: 0.0000; time: 0.20s
Val loss: 0.0568 score: 0.9796 time: 0.07s
Test loss: 0.2430 score: 0.9184 time: 0.06s
Epoch 490/1000, LR 0.000142
Train loss: 0.5381;  Loss pred: 0.5381; Loss self: 0.0000; time: 0.20s
Val loss: 0.0567 score: 0.9796 time: 0.07s
Test loss: 0.2429 score: 0.9184 time: 0.06s
Epoch 491/1000, LR 0.000141
Train loss: 0.5368;  Loss pred: 0.5368; Loss self: 0.0000; time: 0.20s
Val loss: 0.0566 score: 0.9796 time: 0.07s
Test loss: 0.2429 score: 0.9184 time: 0.06s
Epoch 492/1000, LR 0.000141
Train loss: 0.5386;  Loss pred: 0.5386; Loss self: 0.0000; time: 0.20s
Val loss: 0.0564 score: 0.9796 time: 0.07s
Test loss: 0.2428 score: 0.9184 time: 0.06s
Epoch 493/1000, LR 0.000141
Train loss: 0.5386;  Loss pred: 0.5386; Loss self: 0.0000; time: 0.20s
Val loss: 0.0563 score: 0.9796 time: 0.07s
Test loss: 0.2428 score: 0.9184 time: 0.06s
Epoch 494/1000, LR 0.000140
Train loss: 0.5408;  Loss pred: 0.5408; Loss self: 0.0000; time: 0.20s
Val loss: 0.0561 score: 0.9796 time: 0.07s
Test loss: 0.2428 score: 0.9184 time: 0.06s
Epoch 495/1000, LR 0.000140
Train loss: 0.5368;  Loss pred: 0.5368; Loss self: 0.0000; time: 0.20s
Val loss: 0.0560 score: 0.9796 time: 0.07s
Test loss: 0.2428 score: 0.9184 time: 0.06s
Epoch 496/1000, LR 0.000139
Train loss: 0.5370;  Loss pred: 0.5370; Loss self: 0.0000; time: 0.20s
Val loss: 0.0559 score: 0.9796 time: 0.08s
Test loss: 0.2429 score: 0.9184 time: 0.06s
Epoch 497/1000, LR 0.000139
Train loss: 0.5377;  Loss pred: 0.5377; Loss self: 0.0000; time: 0.20s
Val loss: 0.0558 score: 0.9796 time: 0.07s
Test loss: 0.2431 score: 0.9184 time: 0.06s
Epoch 498/1000, LR 0.000138
Train loss: 0.5371;  Loss pred: 0.5371; Loss self: 0.0000; time: 0.20s
Val loss: 0.0557 score: 0.9796 time: 0.08s
Test loss: 0.2432 score: 0.9184 time: 0.06s
Epoch 499/1000, LR 0.000138
Train loss: 0.5369;  Loss pred: 0.5369; Loss self: 0.0000; time: 0.21s
Val loss: 0.0556 score: 0.9796 time: 0.08s
Test loss: 0.2433 score: 0.9184 time: 0.07s
Epoch 500/1000, LR 0.000138
Train loss: 0.5374;  Loss pred: 0.5374; Loss self: 0.0000; time: 0.21s
Val loss: 0.0555 score: 0.9796 time: 0.08s
Test loss: 0.2434 score: 0.9184 time: 0.06s
Epoch 501/1000, LR 0.000137
Train loss: 0.5368;  Loss pred: 0.5368; Loss self: 0.0000; time: 0.21s
Val loss: 0.0554 score: 0.9796 time: 0.08s
Test loss: 0.2435 score: 0.9184 time: 0.06s
Epoch 502/1000, LR 0.000137
Train loss: 0.5375;  Loss pred: 0.5375; Loss self: 0.0000; time: 0.21s
Val loss: 0.0553 score: 0.9796 time: 0.08s
Test loss: 0.2435 score: 0.9184 time: 0.07s
Epoch 503/1000, LR 0.000136
Train loss: 0.5354;  Loss pred: 0.5354; Loss self: 0.0000; time: 0.20s
Val loss: 0.0552 score: 0.9796 time: 0.08s
Test loss: 0.2436 score: 0.9184 time: 0.06s
Epoch 504/1000, LR 0.000136
Train loss: 0.5381;  Loss pred: 0.5381; Loss self: 0.0000; time: 0.21s
Val loss: 0.0551 score: 0.9796 time: 0.07s
Test loss: 0.2435 score: 0.9184 time: 0.06s
Epoch 505/1000, LR 0.000135
Train loss: 0.5365;  Loss pred: 0.5365; Loss self: 0.0000; time: 0.20s
Val loss: 0.0549 score: 0.9796 time: 0.07s
Test loss: 0.2434 score: 0.9184 time: 0.06s
Epoch 506/1000, LR 0.000135
Train loss: 0.5368;  Loss pred: 0.5368; Loss self: 0.0000; time: 0.20s
Val loss: 0.0548 score: 0.9796 time: 0.07s
Test loss: 0.2433 score: 0.9184 time: 0.06s
Epoch 507/1000, LR 0.000135
Train loss: 0.5358;  Loss pred: 0.5358; Loss self: 0.0000; time: 0.20s
Val loss: 0.0546 score: 0.9796 time: 0.59s
Test loss: 0.2432 score: 0.9184 time: 0.89s
Epoch 508/1000, LR 0.000134
Train loss: 0.5349;  Loss pred: 0.5349; Loss self: 0.0000; time: 1.98s
Val loss: 0.0545 score: 0.9796 time: 0.52s
Test loss: 0.2431 score: 0.9184 time: 0.87s
Epoch 509/1000, LR 0.000134
Train loss: 0.5351;  Loss pred: 0.5351; Loss self: 0.0000; time: 2.00s
Val loss: 0.0544 score: 0.9796 time: 0.31s
Test loss: 0.2430 score: 0.9184 time: 0.21s
Epoch 510/1000, LR 0.000133
Train loss: 0.5358;  Loss pred: 0.5358; Loss self: 0.0000; time: 0.60s
Val loss: 0.0542 score: 0.9796 time: 0.13s
Test loss: 0.2429 score: 0.9184 time: 0.07s
Epoch 511/1000, LR 0.000133
Train loss: 0.5341;  Loss pred: 0.5341; Loss self: 0.0000; time: 0.22s
Val loss: 0.0541 score: 0.9796 time: 0.08s
Test loss: 0.2429 score: 0.9184 time: 0.06s
Epoch 512/1000, LR 0.000132
Train loss: 0.5361;  Loss pred: 0.5361; Loss self: 0.0000; time: 0.21s
Val loss: 0.0540 score: 0.9796 time: 0.07s
Test loss: 0.2429 score: 0.9184 time: 0.07s
Epoch 513/1000, LR 0.000132
Train loss: 0.5356;  Loss pred: 0.5356; Loss self: 0.0000; time: 0.21s
Val loss: 0.0539 score: 0.9796 time: 0.07s
Test loss: 0.2429 score: 0.9184 time: 0.06s
Epoch 514/1000, LR 0.000132
Train loss: 0.5349;  Loss pred: 0.5349; Loss self: 0.0000; time: 0.20s
Val loss: 0.0538 score: 0.9796 time: 0.07s
Test loss: 0.2429 score: 0.9184 time: 0.17s
Epoch 515/1000, LR 0.000131
Train loss: 0.5346;  Loss pred: 0.5346; Loss self: 0.0000; time: 0.20s
Val loss: 0.0537 score: 1.0000 time: 0.07s
Test loss: 0.2429 score: 0.9184 time: 0.06s
Epoch 516/1000, LR 0.000131
Train loss: 0.5348;  Loss pred: 0.5348; Loss self: 0.0000; time: 0.20s
Val loss: 0.0536 score: 1.0000 time: 0.08s
Test loss: 0.2430 score: 0.9184 time: 0.06s
Epoch 517/1000, LR 0.000130
Train loss: 0.5337;  Loss pred: 0.5337; Loss self: 0.0000; time: 0.21s
Val loss: 0.0535 score: 1.0000 time: 0.07s
Test loss: 0.2431 score: 0.9184 time: 0.06s
Epoch 518/1000, LR 0.000130
Train loss: 0.5343;  Loss pred: 0.5343; Loss self: 0.0000; time: 0.21s
Val loss: 0.0534 score: 1.0000 time: 0.08s
Test loss: 0.2432 score: 0.9184 time: 0.06s
Epoch 519/1000, LR 0.000129
Train loss: 0.5342;  Loss pred: 0.5342; Loss self: 0.0000; time: 0.22s
Val loss: 0.0533 score: 1.0000 time: 0.11s
Test loss: 0.2433 score: 0.9184 time: 0.06s
Epoch 520/1000, LR 0.000129
Train loss: 0.5343;  Loss pred: 0.5343; Loss self: 0.0000; time: 0.20s
Val loss: 0.0532 score: 1.0000 time: 0.73s
Test loss: 0.2433 score: 0.9184 time: 0.51s
Epoch 521/1000, LR 0.000129
Train loss: 0.5326;  Loss pred: 0.5326; Loss self: 0.0000; time: 3.38s
Val loss: 0.0531 score: 1.0000 time: 0.67s
Test loss: 0.2433 score: 0.9184 time: 1.60s
Epoch 522/1000, LR 0.000128
Train loss: 0.5328;  Loss pred: 0.5328; Loss self: 0.0000; time: 1.07s
Val loss: 0.0530 score: 1.0000 time: 0.85s
Test loss: 0.2433 score: 0.9184 time: 0.50s
Epoch 523/1000, LR 0.000128
Train loss: 0.5343;  Loss pred: 0.5343; Loss self: 0.0000; time: 0.57s
Val loss: 0.0528 score: 1.0000 time: 0.44s
Test loss: 0.2431 score: 0.9184 time: 0.11s
Epoch 524/1000, LR 0.000127
Train loss: 0.5326;  Loss pred: 0.5326; Loss self: 0.0000; time: 0.24s
Val loss: 0.0527 score: 1.0000 time: 0.07s
Test loss: 0.2430 score: 0.9184 time: 0.06s
Epoch 525/1000, LR 0.000127
Train loss: 0.5352;  Loss pred: 0.5352; Loss self: 0.0000; time: 0.21s
Val loss: 0.0526 score: 1.0000 time: 0.07s
Test loss: 0.2429 score: 0.9184 time: 0.07s
Epoch 526/1000, LR 0.000126
Train loss: 0.5345;  Loss pred: 0.5345; Loss self: 0.0000; time: 0.20s
Val loss: 0.0525 score: 1.0000 time: 0.07s
Test loss: 0.2428 score: 0.9184 time: 0.06s
Epoch 527/1000, LR 0.000126
Train loss: 0.5328;  Loss pred: 0.5328; Loss self: 0.0000; time: 0.20s
Val loss: 0.0523 score: 1.0000 time: 0.07s
Test loss: 0.2427 score: 0.9184 time: 0.06s
Epoch 528/1000, LR 0.000126
Train loss: 0.5338;  Loss pred: 0.5338; Loss self: 0.0000; time: 0.20s
Val loss: 0.0522 score: 1.0000 time: 0.08s
Test loss: 0.2426 score: 0.9184 time: 0.06s
Epoch 529/1000, LR 0.000125
Train loss: 0.5323;  Loss pred: 0.5323; Loss self: 0.0000; time: 0.20s
Val loss: 0.0521 score: 1.0000 time: 0.07s
Test loss: 0.2425 score: 0.9184 time: 0.06s
Epoch 530/1000, LR 0.000125
Train loss: 0.5316;  Loss pred: 0.5316; Loss self: 0.0000; time: 0.20s
Val loss: 0.0520 score: 1.0000 time: 0.07s
Test loss: 0.2424 score: 0.9184 time: 0.06s
Epoch 531/1000, LR 0.000124
Train loss: 0.5309;  Loss pred: 0.5309; Loss self: 0.0000; time: 0.23s
Val loss: 0.0519 score: 1.0000 time: 0.89s
Test loss: 0.2423 score: 0.9184 time: 1.23s
Epoch 532/1000, LR 0.000124
Train loss: 0.5333;  Loss pred: 0.5333; Loss self: 0.0000; time: 5.67s
Val loss: 0.0518 score: 1.0000 time: 0.83s
Test loss: 0.2422 score: 0.9184 time: 1.38s
Epoch 533/1000, LR 0.000123
Train loss: 0.5322;  Loss pred: 0.5322; Loss self: 0.0000; time: 4.02s
Val loss: 0.0516 score: 1.0000 time: 0.32s
Test loss: 0.2421 score: 0.9184 time: 0.07s
Epoch 534/1000, LR 0.000123
Train loss: 0.5334;  Loss pred: 0.5334; Loss self: 0.0000; time: 0.22s
Val loss: 0.0515 score: 1.0000 time: 0.07s
Test loss: 0.2420 score: 0.9184 time: 0.07s
Epoch 535/1000, LR 0.000123
Train loss: 0.5319;  Loss pred: 0.5319; Loss self: 0.0000; time: 0.22s
Val loss: 0.0514 score: 1.0000 time: 0.08s
Test loss: 0.2418 score: 0.9184 time: 0.07s
Epoch 536/1000, LR 0.000122
Train loss: 0.5333;  Loss pred: 0.5333; Loss self: 0.0000; time: 0.22s
Val loss: 0.0512 score: 1.0000 time: 0.08s
Test loss: 0.2416 score: 0.9184 time: 0.07s
Epoch 537/1000, LR 0.000122
Train loss: 0.5306;  Loss pred: 0.5306; Loss self: 0.0000; time: 0.22s
Val loss: 0.0511 score: 1.0000 time: 0.07s
Test loss: 0.2415 score: 0.9184 time: 0.07s
Epoch 538/1000, LR 0.000121
Train loss: 0.5322;  Loss pred: 0.5322; Loss self: 0.0000; time: 0.23s
Val loss: 0.0510 score: 1.0000 time: 0.08s
Test loss: 0.2414 score: 0.9184 time: 0.07s
Epoch 539/1000, LR 0.000121
Train loss: 0.5316;  Loss pred: 0.5316; Loss self: 0.0000; time: 0.22s
Val loss: 0.0509 score: 1.0000 time: 0.07s
Test loss: 0.2414 score: 0.9184 time: 0.07s
Epoch 540/1000, LR 0.000120
Train loss: 0.5312;  Loss pred: 0.5312; Loss self: 0.0000; time: 0.23s
Val loss: 0.0508 score: 1.0000 time: 1.94s
Test loss: 0.2413 score: 0.9184 time: 0.31s
Epoch 541/1000, LR 0.000120
Train loss: 0.5300;  Loss pred: 0.5300; Loss self: 0.0000; time: 1.15s
Val loss: 0.0507 score: 1.0000 time: 0.17s
Test loss: 0.2412 score: 0.9184 time: 0.31s
Epoch 542/1000, LR 0.000120
Train loss: 0.5318;  Loss pred: 0.5318; Loss self: 0.0000; time: 0.92s
Val loss: 0.0506 score: 1.0000 time: 0.19s
Test loss: 0.2412 score: 0.9184 time: 0.24s
Epoch 543/1000, LR 0.000119
Train loss: 0.5301;  Loss pred: 0.5301; Loss self: 0.0000; time: 0.23s
Val loss: 0.0505 score: 1.0000 time: 0.08s
Test loss: 0.2412 score: 0.9184 time: 0.07s
Epoch 544/1000, LR 0.000119
Train loss: 0.5313;  Loss pred: 0.5313; Loss self: 0.0000; time: 0.22s
Val loss: 0.0504 score: 1.0000 time: 0.08s
Test loss: 0.2413 score: 0.9184 time: 0.07s
Epoch 545/1000, LR 0.000118
Train loss: 0.5324;  Loss pred: 0.5324; Loss self: 0.0000; time: 0.21s
Val loss: 0.0504 score: 1.0000 time: 0.07s
Test loss: 0.2414 score: 0.9184 time: 0.06s
Epoch 546/1000, LR 0.000118
Train loss: 0.5321;  Loss pred: 0.5321; Loss self: 0.0000; time: 0.20s
Val loss: 0.0503 score: 1.0000 time: 0.07s
Test loss: 0.2414 score: 0.9184 time: 0.06s
Epoch 547/1000, LR 0.000117
Train loss: 0.5312;  Loss pred: 0.5312; Loss self: 0.0000; time: 0.20s
Val loss: 0.0502 score: 1.0000 time: 0.07s
Test loss: 0.2415 score: 0.9184 time: 0.06s
Epoch 548/1000, LR 0.000117
Train loss: 0.5286;  Loss pred: 0.5286; Loss self: 0.0000; time: 0.20s
Val loss: 0.0502 score: 1.0000 time: 0.08s
Test loss: 0.2417 score: 0.9184 time: 0.06s
Epoch 549/1000, LR 0.000117
Train loss: 0.5308;  Loss pred: 0.5308; Loss self: 0.0000; time: 0.20s
Val loss: 0.0501 score: 1.0000 time: 0.07s
Test loss: 0.2418 score: 0.9184 time: 0.06s
Epoch 550/1000, LR 0.000116
Train loss: 0.5310;  Loss pred: 0.5310; Loss self: 0.0000; time: 0.20s
Val loss: 0.0500 score: 1.0000 time: 0.08s
Test loss: 0.2418 score: 0.9184 time: 0.06s
Epoch 551/1000, LR 0.000116
Train loss: 0.5290;  Loss pred: 0.5290; Loss self: 0.0000; time: 0.20s
Val loss: 0.0499 score: 1.0000 time: 0.07s
Test loss: 0.2418 score: 0.9184 time: 0.06s
Epoch 552/1000, LR 0.000115
Train loss: 0.5302;  Loss pred: 0.5302; Loss self: 0.0000; time: 0.20s
Val loss: 0.0498 score: 1.0000 time: 0.07s
Test loss: 0.2417 score: 0.9184 time: 0.06s
Epoch 553/1000, LR 0.000115
Train loss: 0.5315;  Loss pred: 0.5315; Loss self: 0.0000; time: 0.20s
Val loss: 0.0497 score: 1.0000 time: 0.08s
Test loss: 0.2416 score: 0.9184 time: 0.06s
Epoch 554/1000, LR 0.000115
Train loss: 0.5292;  Loss pred: 0.5292; Loss self: 0.0000; time: 0.20s
Val loss: 0.0496 score: 1.0000 time: 0.07s
Test loss: 0.2416 score: 0.9184 time: 0.06s
Epoch 555/1000, LR 0.000114
Train loss: 0.5283;  Loss pred: 0.5283; Loss self: 0.0000; time: 0.21s
Val loss: 0.0495 score: 1.0000 time: 0.07s
Test loss: 0.2415 score: 0.9184 time: 0.06s
Epoch 556/1000, LR 0.000114
Train loss: 0.5285;  Loss pred: 0.5285; Loss self: 0.0000; time: 0.20s
Val loss: 0.0494 score: 1.0000 time: 0.07s
Test loss: 0.2414 score: 0.9184 time: 0.06s
Epoch 557/1000, LR 0.000113
Train loss: 0.5316;  Loss pred: 0.5316; Loss self: 0.0000; time: 0.20s
Val loss: 0.0493 score: 1.0000 time: 0.08s
Test loss: 0.2412 score: 0.9184 time: 0.06s
Epoch 558/1000, LR 0.000113
Train loss: 0.5298;  Loss pred: 0.5298; Loss self: 0.0000; time: 0.20s
Val loss: 0.0492 score: 1.0000 time: 0.08s
Test loss: 0.2411 score: 0.9184 time: 0.06s
Epoch 559/1000, LR 0.000112
Train loss: 0.5288;  Loss pred: 0.5288; Loss self: 0.0000; time: 0.20s
Val loss: 0.0491 score: 1.0000 time: 0.07s
Test loss: 0.2411 score: 0.9184 time: 0.06s
Epoch 560/1000, LR 0.000112
Train loss: 0.5291;  Loss pred: 0.5291; Loss self: 0.0000; time: 0.21s
Val loss: 0.0490 score: 1.0000 time: 0.07s
Test loss: 0.2410 score: 0.9184 time: 0.07s
Epoch 561/1000, LR 0.000112
Train loss: 0.5294;  Loss pred: 0.5294; Loss self: 0.0000; time: 0.20s
Val loss: 0.0489 score: 1.0000 time: 0.07s
Test loss: 0.2409 score: 0.9184 time: 0.06s
Epoch 562/1000, LR 0.000111
Train loss: 0.5287;  Loss pred: 0.5287; Loss self: 0.0000; time: 0.20s
Val loss: 0.0489 score: 1.0000 time: 0.08s
Test loss: 0.2408 score: 0.9184 time: 0.06s
Epoch 563/1000, LR 0.000111
Train loss: 0.5289;  Loss pred: 0.5289; Loss self: 0.0000; time: 0.20s
Val loss: 0.0488 score: 1.0000 time: 0.07s
Test loss: 0.2408 score: 0.9184 time: 0.06s
Epoch 564/1000, LR 0.000110
Train loss: 0.5273;  Loss pred: 0.5273; Loss self: 0.0000; time: 0.20s
Val loss: 0.0487 score: 1.0000 time: 0.07s
Test loss: 0.2408 score: 0.9184 time: 0.06s
Epoch 565/1000, LR 0.000110
Train loss: 0.5305;  Loss pred: 0.5305; Loss self: 0.0000; time: 0.21s
Val loss: 0.0486 score: 1.0000 time: 0.08s
Test loss: 0.2408 score: 0.9184 time: 0.06s
Epoch 566/1000, LR 0.000109
Train loss: 0.5278;  Loss pred: 0.5278; Loss self: 0.0000; time: 0.21s
Val loss: 0.0485 score: 1.0000 time: 0.35s
Test loss: 0.2408 score: 0.9184 time: 0.35s
Epoch 567/1000, LR 0.000109
Train loss: 0.5284;  Loss pred: 0.5284; Loss self: 0.0000; time: 3.60s
Val loss: 0.0485 score: 1.0000 time: 0.50s
Test loss: 0.2408 score: 0.9184 time: 0.27s
Epoch 568/1000, LR 0.000109
Train loss: 0.5284;  Loss pred: 0.5284; Loss self: 0.0000; time: 0.64s
Val loss: 0.0484 score: 1.0000 time: 0.09s
Test loss: 0.2408 score: 0.9184 time: 0.07s
Epoch 569/1000, LR 0.000108
Train loss: 0.5275;  Loss pred: 0.5275; Loss self: 0.0000; time: 0.22s
Val loss: 0.0483 score: 1.0000 time: 0.08s
Test loss: 0.2408 score: 0.9184 time: 0.06s
Epoch 570/1000, LR 0.000108
Train loss: 0.5284;  Loss pred: 0.5284; Loss self: 0.0000; time: 0.21s
Val loss: 0.0482 score: 1.0000 time: 0.07s
Test loss: 0.2408 score: 0.9184 time: 0.07s
Epoch 571/1000, LR 0.000107
Train loss: 0.5274;  Loss pred: 0.5274; Loss self: 0.0000; time: 0.21s
Val loss: 0.0482 score: 1.0000 time: 0.07s
Test loss: 0.2409 score: 0.9184 time: 0.07s
Epoch 572/1000, LR 0.000107
Train loss: 0.5267;  Loss pred: 0.5267; Loss self: 0.0000; time: 0.20s
Val loss: 0.0481 score: 1.0000 time: 0.07s
Test loss: 0.2409 score: 0.9184 time: 0.06s
Epoch 573/1000, LR 0.000107
Train loss: 0.5286;  Loss pred: 0.5286; Loss self: 0.0000; time: 0.21s
Val loss: 0.0481 score: 1.0000 time: 0.07s
Test loss: 0.2410 score: 0.9184 time: 0.06s
Epoch 574/1000, LR 0.000106
Train loss: 0.5277;  Loss pred: 0.5277; Loss self: 0.0000; time: 0.21s
Val loss: 0.0480 score: 1.0000 time: 0.07s
Test loss: 0.2411 score: 0.9184 time: 0.06s
Epoch 575/1000, LR 0.000106
Train loss: 0.5286;  Loss pred: 0.5286; Loss self: 0.0000; time: 0.21s
Val loss: 0.0479 score: 1.0000 time: 0.08s
Test loss: 0.2411 score: 0.9184 time: 0.06s
Epoch 576/1000, LR 0.000105
Train loss: 0.5268;  Loss pred: 0.5268; Loss self: 0.0000; time: 0.21s
Val loss: 0.0479 score: 1.0000 time: 0.07s
Test loss: 0.2411 score: 0.9184 time: 0.06s
Epoch 577/1000, LR 0.000105
Train loss: 0.5281;  Loss pred: 0.5281; Loss self: 0.0000; time: 0.21s
Val loss: 0.0478 score: 1.0000 time: 0.07s
Test loss: 0.2411 score: 0.9184 time: 0.07s
Epoch 578/1000, LR 0.000104
Train loss: 0.5275;  Loss pred: 0.5275; Loss self: 0.0000; time: 0.22s
Val loss: 0.0477 score: 1.0000 time: 1.78s
Test loss: 0.2410 score: 0.9184 time: 1.28s
Epoch 579/1000, LR 0.000104
Train loss: 0.5268;  Loss pred: 0.5268; Loss self: 0.0000; time: 2.58s
Val loss: 0.0476 score: 1.0000 time: 2.02s
Test loss: 0.2410 score: 0.9184 time: 0.42s
Epoch 580/1000, LR 0.000104
Train loss: 0.5272;  Loss pred: 0.5272; Loss self: 0.0000; time: 0.59s
Val loss: 0.0475 score: 1.0000 time: 0.07s
Test loss: 0.2409 score: 0.9184 time: 0.07s
Epoch 581/1000, LR 0.000103
Train loss: 0.5265;  Loss pred: 0.5265; Loss self: 0.0000; time: 0.22s
Val loss: 0.0474 score: 1.0000 time: 0.08s
Test loss: 0.2408 score: 0.9184 time: 0.07s
Epoch 582/1000, LR 0.000103
Train loss: 0.5274;  Loss pred: 0.5274; Loss self: 0.0000; time: 0.22s
Val loss: 0.0474 score: 1.0000 time: 0.07s
Test loss: 0.2407 score: 0.9184 time: 0.06s
Epoch 583/1000, LR 0.000102
Train loss: 0.5259;  Loss pred: 0.5259; Loss self: 0.0000; time: 0.29s
Val loss: 0.0473 score: 1.0000 time: 0.08s
Test loss: 0.2405 score: 0.9184 time: 0.06s
Epoch 584/1000, LR 0.000102
Train loss: 0.5278;  Loss pred: 0.5278; Loss self: 0.0000; time: 0.20s
Val loss: 0.0472 score: 1.0000 time: 0.07s
Test loss: 0.2404 score: 0.9184 time: 0.06s
Epoch 585/1000, LR 0.000102
Train loss: 0.5276;  Loss pred: 0.5276; Loss self: 0.0000; time: 0.21s
Val loss: 0.0471 score: 1.0000 time: 0.07s
Test loss: 0.2403 score: 0.9184 time: 0.06s
Epoch 586/1000, LR 0.000101
Train loss: 0.5274;  Loss pred: 0.5274; Loss self: 0.0000; time: 0.21s
Val loss: 0.0470 score: 1.0000 time: 0.08s
Test loss: 0.2401 score: 0.9184 time: 0.06s
Epoch 587/1000, LR 0.000101
Train loss: 0.5268;  Loss pred: 0.5268; Loss self: 0.0000; time: 0.21s
Val loss: 0.0469 score: 1.0000 time: 0.08s
Test loss: 0.2400 score: 0.9184 time: 0.06s
Epoch 588/1000, LR 0.000100
Train loss: 0.5259;  Loss pred: 0.5259; Loss self: 0.0000; time: 0.21s
Val loss: 0.0468 score: 1.0000 time: 0.08s
Test loss: 0.2399 score: 0.9184 time: 0.06s
Epoch 589/1000, LR 0.000100
Train loss: 0.5253;  Loss pred: 0.5253; Loss self: 0.0000; time: 0.22s
Val loss: 0.0467 score: 1.0000 time: 0.07s
Test loss: 0.2399 score: 0.9184 time: 0.06s
Epoch 590/1000, LR 0.000099
Train loss: 0.5269;  Loss pred: 0.5269; Loss self: 0.0000; time: 0.21s
Val loss: 0.0467 score: 1.0000 time: 0.07s
Test loss: 0.2398 score: 0.9184 time: 0.06s
Epoch 591/1000, LR 0.000099
Train loss: 0.5239;  Loss pred: 0.5239; Loss self: 0.0000; time: 0.21s
Val loss: 0.0466 score: 1.0000 time: 0.07s
Test loss: 0.2398 score: 0.9184 time: 0.07s
Epoch 592/1000, LR 0.000099
Train loss: 0.5264;  Loss pred: 0.5264; Loss self: 0.0000; time: 0.21s
Val loss: 0.0465 score: 1.0000 time: 1.29s
Test loss: 0.2397 score: 0.9184 time: 1.66s
Epoch 593/1000, LR 0.000098
Train loss: 0.5268;  Loss pred: 0.5268; Loss self: 0.0000; time: 1.47s
Val loss: 0.0464 score: 1.0000 time: 0.09s
Test loss: 0.2397 score: 0.9184 time: 0.08s
Epoch 594/1000, LR 0.000098
Train loss: 0.5285;  Loss pred: 0.5285; Loss self: 0.0000; time: 0.22s
Val loss: 0.0464 score: 1.0000 time: 0.08s
Test loss: 0.2395 score: 0.9184 time: 0.06s
Epoch 595/1000, LR 0.000097
Train loss: 0.5249;  Loss pred: 0.5249; Loss self: 0.0000; time: 0.22s
Val loss: 0.0463 score: 1.0000 time: 0.08s
Test loss: 0.2395 score: 0.9184 time: 0.07s
Epoch 596/1000, LR 0.000097
Train loss: 0.5249;  Loss pred: 0.5249; Loss self: 0.0000; time: 0.21s
Val loss: 0.0462 score: 1.0000 time: 0.08s
Test loss: 0.2394 score: 0.9184 time: 0.07s
Epoch 597/1000, LR 0.000097
Train loss: 0.5253;  Loss pred: 0.5253; Loss self: 0.0000; time: 0.22s
Val loss: 0.0461 score: 1.0000 time: 0.08s
Test loss: 0.2394 score: 0.9184 time: 0.06s
Epoch 598/1000, LR 0.000096
Train loss: 0.5247;  Loss pred: 0.5247; Loss self: 0.0000; time: 0.22s
Val loss: 0.0461 score: 1.0000 time: 0.08s
Test loss: 0.2393 score: 0.9184 time: 0.06s
Epoch 599/1000, LR 0.000096
Train loss: 0.5242;  Loss pred: 0.5242; Loss self: 0.0000; time: 0.21s
Val loss: 0.0460 score: 1.0000 time: 0.07s
Test loss: 0.2393 score: 0.9184 time: 0.07s
Epoch 600/1000, LR 0.000095
Train loss: 0.5262;  Loss pred: 0.5262; Loss self: 0.0000; time: 0.21s
Val loss: 0.0460 score: 1.0000 time: 0.08s
Test loss: 0.2393 score: 0.9184 time: 0.06s
Epoch 601/1000, LR 0.000095
Train loss: 0.5272;  Loss pred: 0.5272; Loss self: 0.0000; time: 0.21s
Val loss: 0.0459 score: 1.0000 time: 0.07s
Test loss: 0.2393 score: 0.9184 time: 0.06s
Epoch 602/1000, LR 0.000095
Train loss: 0.5254;  Loss pred: 0.5254; Loss self: 0.0000; time: 0.21s
Val loss: 0.0458 score: 1.0000 time: 0.08s
Test loss: 0.2393 score: 0.9184 time: 0.06s
Epoch 603/1000, LR 0.000094
Train loss: 0.5266;  Loss pred: 0.5266; Loss self: 0.0000; time: 0.22s
Val loss: 0.0458 score: 1.0000 time: 0.07s
Test loss: 0.2392 score: 0.9184 time: 0.07s
Epoch 604/1000, LR 0.000094
Train loss: 0.5227;  Loss pred: 0.5227; Loss self: 0.0000; time: 0.21s
Val loss: 0.0457 score: 1.0000 time: 0.08s
Test loss: 0.2392 score: 0.9184 time: 0.06s
Epoch 605/1000, LR 0.000093
Train loss: 0.5256;  Loss pred: 0.5256; Loss self: 0.0000; time: 0.20s
Val loss: 0.0456 score: 1.0000 time: 0.07s
Test loss: 0.2391 score: 0.9184 time: 0.06s
Epoch 606/1000, LR 0.000093
Train loss: 0.5239;  Loss pred: 0.5239; Loss self: 0.0000; time: 0.21s
Val loss: 0.0456 score: 1.0000 time: 0.07s
Test loss: 0.2392 score: 0.9184 time: 0.06s
Epoch 607/1000, LR 0.000092
Train loss: 0.5268;  Loss pred: 0.5268; Loss self: 0.0000; time: 0.21s
Val loss: 0.0455 score: 1.0000 time: 0.07s
Test loss: 0.2391 score: 0.9184 time: 0.06s
Epoch 608/1000, LR 0.000092
Train loss: 0.5275;  Loss pred: 0.5275; Loss self: 0.0000; time: 0.21s
Val loss: 0.0455 score: 1.0000 time: 0.07s
Test loss: 0.2391 score: 0.9184 time: 0.06s
Epoch 609/1000, LR 0.000092
Train loss: 0.5237;  Loss pred: 0.5237; Loss self: 0.0000; time: 0.21s
Val loss: 0.0454 score: 1.0000 time: 0.08s
Test loss: 0.2391 score: 0.9184 time: 0.06s
Epoch 610/1000, LR 0.000091
Train loss: 0.5250;  Loss pred: 0.5250; Loss self: 0.0000; time: 0.21s
Val loss: 0.0453 score: 1.0000 time: 0.07s
Test loss: 0.2391 score: 0.9184 time: 0.06s
Epoch 611/1000, LR 0.000091
Train loss: 0.5247;  Loss pred: 0.5247; Loss self: 0.0000; time: 0.21s
Val loss: 0.0453 score: 1.0000 time: 0.07s
Test loss: 0.2391 score: 0.9184 time: 0.06s
Epoch 612/1000, LR 0.000090
Train loss: 0.5243;  Loss pred: 0.5243; Loss self: 0.0000; time: 0.20s
Val loss: 0.0452 score: 1.0000 time: 0.07s
Test loss: 0.2391 score: 0.9184 time: 0.07s
Epoch 613/1000, LR 0.000090
Train loss: 0.5225;  Loss pred: 0.5225; Loss self: 0.0000; time: 0.20s
Val loss: 0.0452 score: 1.0000 time: 0.07s
Test loss: 0.2392 score: 0.9184 time: 0.06s
Epoch 614/1000, LR 0.000090
Train loss: 0.5247;  Loss pred: 0.5247; Loss self: 0.0000; time: 0.20s
Val loss: 0.0452 score: 1.0000 time: 0.07s
Test loss: 0.2392 score: 0.9184 time: 0.06s
Epoch 615/1000, LR 0.000089
Train loss: 0.5229;  Loss pred: 0.5229; Loss self: 0.0000; time: 0.21s
Val loss: 0.0451 score: 1.0000 time: 0.07s
Test loss: 0.2393 score: 0.9184 time: 0.06s
Epoch 616/1000, LR 0.000089
Train loss: 0.5247;  Loss pred: 0.5247; Loss self: 0.0000; time: 0.20s
Val loss: 0.0451 score: 1.0000 time: 0.07s
Test loss: 0.2393 score: 0.9184 time: 0.06s
Epoch 617/1000, LR 0.000088
Train loss: 0.5248;  Loss pred: 0.5248; Loss self: 0.0000; time: 0.21s
Val loss: 0.0450 score: 1.0000 time: 0.07s
Test loss: 0.2394 score: 0.9184 time: 0.07s
Epoch 618/1000, LR 0.000088
Train loss: 0.5232;  Loss pred: 0.5232; Loss self: 0.0000; time: 0.21s
Val loss: 0.0450 score: 1.0000 time: 0.07s
Test loss: 0.2394 score: 0.9184 time: 0.06s
Epoch 619/1000, LR 0.000088
Train loss: 0.5240;  Loss pred: 0.5240; Loss self: 0.0000; time: 0.20s
Val loss: 0.0449 score: 1.0000 time: 0.07s
Test loss: 0.2394 score: 0.9184 time: 0.06s
Epoch 620/1000, LR 0.000087
Train loss: 0.5240;  Loss pred: 0.5240; Loss self: 0.0000; time: 0.20s
Val loss: 0.0449 score: 1.0000 time: 0.08s
Test loss: 0.2394 score: 0.9184 time: 0.06s
Epoch 621/1000, LR 0.000087
Train loss: 0.5230;  Loss pred: 0.5230; Loss self: 0.0000; time: 0.20s
Val loss: 0.0448 score: 1.0000 time: 0.07s
Test loss: 0.2395 score: 0.9184 time: 0.06s
Epoch 622/1000, LR 0.000086
Train loss: 0.5246;  Loss pred: 0.5246; Loss self: 0.0000; time: 0.20s
Val loss: 0.0448 score: 1.0000 time: 0.07s
Test loss: 0.2396 score: 0.9184 time: 0.06s
Epoch 623/1000, LR 0.000086
Train loss: 0.5249;  Loss pred: 0.5249; Loss self: 0.0000; time: 0.20s
Val loss: 0.0447 score: 1.0000 time: 0.08s
Test loss: 0.2396 score: 0.9184 time: 0.06s
Epoch 624/1000, LR 0.000086
Train loss: 0.5241;  Loss pred: 0.5241; Loss self: 0.0000; time: 0.21s
Val loss: 0.0447 score: 1.0000 time: 0.07s
Test loss: 0.2396 score: 0.9184 time: 0.06s
Epoch 625/1000, LR 0.000085
Train loss: 0.5227;  Loss pred: 0.5227; Loss self: 0.0000; time: 0.20s
Val loss: 0.0447 score: 1.0000 time: 0.07s
Test loss: 0.2396 score: 0.9184 time: 0.06s
Epoch 626/1000, LR 0.000085
Train loss: 0.5222;  Loss pred: 0.5222; Loss self: 0.0000; time: 0.20s
Val loss: 0.0446 score: 1.0000 time: 0.07s
Test loss: 0.2397 score: 0.9184 time: 0.06s
Epoch 627/1000, LR 0.000084
Train loss: 0.5226;  Loss pred: 0.5226; Loss self: 0.0000; time: 0.20s
Val loss: 0.0446 score: 1.0000 time: 0.07s
Test loss: 0.2397 score: 0.9184 time: 0.06s
Epoch 628/1000, LR 0.000084
Train loss: 0.5234;  Loss pred: 0.5234; Loss self: 0.0000; time: 0.20s
Val loss: 0.0445 score: 1.0000 time: 0.07s
Test loss: 0.2398 score: 0.9184 time: 0.06s
Epoch 629/1000, LR 0.000084
Train loss: 0.5230;  Loss pred: 0.5230; Loss self: 0.0000; time: 0.20s
Val loss: 0.0445 score: 1.0000 time: 0.07s
Test loss: 0.2398 score: 0.9184 time: 0.06s
Epoch 630/1000, LR 0.000083
Train loss: 0.5234;  Loss pred: 0.5234; Loss self: 0.0000; time: 0.20s
Val loss: 0.0444 score: 1.0000 time: 0.07s
Test loss: 0.2399 score: 0.9184 time: 0.07s
Epoch 631/1000, LR 0.000083
Train loss: 0.5238;  Loss pred: 0.5238; Loss self: 0.0000; time: 0.20s
Val loss: 0.0444 score: 1.0000 time: 0.07s
Test loss: 0.2399 score: 0.9184 time: 0.07s
Epoch 632/1000, LR 0.000082
Train loss: 0.5221;  Loss pred: 0.5221; Loss self: 0.0000; time: 0.22s
Val loss: 0.0444 score: 1.0000 time: 1.05s
Test loss: 0.2399 score: 0.9184 time: 0.58s
Epoch 633/1000, LR 0.000082
Train loss: 0.5222;  Loss pred: 0.5222; Loss self: 0.0000; time: 0.90s
Val loss: 0.0443 score: 1.0000 time: 0.40s
Test loss: 0.2399 score: 0.9184 time: 0.13s
Epoch 634/1000, LR 0.000082
Train loss: 0.5204;  Loss pred: 0.5204; Loss self: 0.0000; time: 0.48s
Val loss: 0.0443 score: 1.0000 time: 0.08s
Test loss: 0.2400 score: 0.9184 time: 0.06s
Epoch 635/1000, LR 0.000081
Train loss: 0.5240;  Loss pred: 0.5240; Loss self: 0.0000; time: 0.21s
Val loss: 0.0442 score: 1.0000 time: 0.07s
Test loss: 0.2401 score: 0.9184 time: 0.07s
Epoch 636/1000, LR 0.000081
Train loss: 0.5244;  Loss pred: 0.5244; Loss self: 0.0000; time: 0.21s
Val loss: 0.0442 score: 1.0000 time: 0.08s
Test loss: 0.2401 score: 0.9184 time: 0.06s
Epoch 637/1000, LR 0.000080
Train loss: 0.5227;  Loss pred: 0.5227; Loss self: 0.0000; time: 0.21s
Val loss: 0.0442 score: 1.0000 time: 0.07s
Test loss: 0.2402 score: 0.9184 time: 0.06s
Epoch 638/1000, LR 0.000080
Train loss: 0.5242;  Loss pred: 0.5242; Loss self: 0.0000; time: 0.21s
Val loss: 0.0441 score: 1.0000 time: 0.08s
Test loss: 0.2402 score: 0.9184 time: 0.07s
Epoch 639/1000, LR 0.000080
Train loss: 0.5227;  Loss pred: 0.5227; Loss self: 0.0000; time: 0.21s
Val loss: 0.0441 score: 1.0000 time: 0.07s
Test loss: 0.2403 score: 0.9184 time: 0.06s
Epoch 640/1000, LR 0.000079
Train loss: 0.5206;  Loss pred: 0.5206; Loss self: 0.0000; time: 0.21s
Val loss: 0.0441 score: 1.0000 time: 0.08s
Test loss: 0.2403 score: 0.9184 time: 0.06s
Epoch 641/1000, LR 0.000079
Train loss: 0.5234;  Loss pred: 0.5234; Loss self: 0.0000; time: 0.21s
Val loss: 0.0440 score: 1.0000 time: 0.07s
Test loss: 0.2402 score: 0.9184 time: 0.06s
Epoch 642/1000, LR 0.000079
Train loss: 0.5251;  Loss pred: 0.5251; Loss self: 0.0000; time: 0.20s
Val loss: 0.0439 score: 1.0000 time: 0.07s
Test loss: 0.2402 score: 0.9184 time: 0.06s
Epoch 643/1000, LR 0.000078
Train loss: 0.5207;  Loss pred: 0.5207; Loss self: 0.0000; time: 0.20s
Val loss: 0.0439 score: 1.0000 time: 0.08s
Test loss: 0.2401 score: 0.9184 time: 0.06s
Epoch 644/1000, LR 0.000078
Train loss: 0.5265;  Loss pred: 0.5265; Loss self: 0.0000; time: 0.20s
Val loss: 0.0438 score: 1.0000 time: 0.07s
Test loss: 0.2399 score: 0.9184 time: 0.06s
Epoch 645/1000, LR 0.000077
Train loss: 0.5219;  Loss pred: 0.5219; Loss self: 0.0000; time: 0.20s
Val loss: 0.0437 score: 1.0000 time: 0.07s
Test loss: 0.2398 score: 0.9184 time: 0.06s
Epoch 646/1000, LR 0.000077
Train loss: 0.5225;  Loss pred: 0.5225; Loss self: 0.0000; time: 0.20s
Val loss: 0.0437 score: 1.0000 time: 0.07s
Test loss: 0.2397 score: 0.9184 time: 0.06s
Epoch 647/1000, LR 0.000077
Train loss: 0.5235;  Loss pred: 0.5235; Loss self: 0.0000; time: 0.20s
Val loss: 0.0436 score: 1.0000 time: 0.07s
Test loss: 0.2396 score: 0.9184 time: 0.06s
Epoch 648/1000, LR 0.000076
Train loss: 0.5213;  Loss pred: 0.5213; Loss self: 0.0000; time: 0.20s
Val loss: 0.0436 score: 1.0000 time: 0.08s
Test loss: 0.2395 score: 0.9184 time: 0.06s
Epoch 649/1000, LR 0.000076
Train loss: 0.5213;  Loss pred: 0.5213; Loss self: 0.0000; time: 0.20s
Val loss: 0.0435 score: 1.0000 time: 0.08s
Test loss: 0.2394 score: 0.9184 time: 0.06s
Epoch 650/1000, LR 0.000075
Train loss: 0.5225;  Loss pred: 0.5225; Loss self: 0.0000; time: 0.20s
Val loss: 0.0435 score: 1.0000 time: 0.07s
Test loss: 0.2394 score: 0.9184 time: 0.06s
Epoch 651/1000, LR 0.000075
Train loss: 0.5215;  Loss pred: 0.5215; Loss self: 0.0000; time: 0.20s
Val loss: 0.0434 score: 1.0000 time: 0.07s
Test loss: 0.2394 score: 0.9184 time: 0.06s
Epoch 652/1000, LR 0.000075
Train loss: 0.5223;  Loss pred: 0.5223; Loss self: 0.0000; time: 0.20s
Val loss: 0.0434 score: 1.0000 time: 0.07s
Test loss: 0.2394 score: 0.9184 time: 0.06s
Epoch 653/1000, LR 0.000074
Train loss: 0.5214;  Loss pred: 0.5214; Loss self: 0.0000; time: 0.22s
Val loss: 0.0433 score: 1.0000 time: 2.06s
Test loss: 0.2394 score: 0.9184 time: 1.35s
Epoch 654/1000, LR 0.000074
Train loss: 0.5227;  Loss pred: 0.5227; Loss self: 0.0000; time: 5.55s
Val loss: 0.0433 score: 1.0000 time: 0.40s
Test loss: 0.2395 score: 0.9184 time: 0.51s
Epoch 655/1000, LR 0.000074
Train loss: 0.5229;  Loss pred: 0.5229; Loss self: 0.0000; time: 0.64s
Val loss: 0.0433 score: 1.0000 time: 0.08s
Test loss: 0.2395 score: 0.9184 time: 0.06s
Epoch 656/1000, LR 0.000073
Train loss: 0.5204;  Loss pred: 0.5204; Loss self: 0.0000; time: 0.22s
Val loss: 0.0432 score: 1.0000 time: 0.08s
Test loss: 0.2396 score: 0.9184 time: 0.07s
Epoch 657/1000, LR 0.000073
Train loss: 0.5218;  Loss pred: 0.5218; Loss self: 0.0000; time: 0.22s
Val loss: 0.0432 score: 1.0000 time: 0.07s
Test loss: 0.2396 score: 0.9184 time: 0.07s
Epoch 658/1000, LR 0.000072
Train loss: 0.5195;  Loss pred: 0.5195; Loss self: 0.0000; time: 0.22s
Val loss: 0.0432 score: 1.0000 time: 0.07s
Test loss: 0.2396 score: 0.9184 time: 0.07s
Epoch 659/1000, LR 0.000072
Train loss: 0.5212;  Loss pred: 0.5212; Loss self: 0.0000; time: 0.21s
Val loss: 0.0431 score: 1.0000 time: 0.07s
Test loss: 0.2397 score: 0.9184 time: 0.07s
Epoch 660/1000, LR 0.000072
Train loss: 0.5234;  Loss pred: 0.5234; Loss self: 0.0000; time: 0.20s
Val loss: 0.0431 score: 1.0000 time: 0.07s
Test loss: 0.2397 score: 0.9184 time: 0.06s
Epoch 661/1000, LR 0.000071
Train loss: 0.5203;  Loss pred: 0.5203; Loss self: 0.0000; time: 0.20s
Val loss: 0.0431 score: 1.0000 time: 0.07s
Test loss: 0.2397 score: 0.9184 time: 0.06s
Epoch 662/1000, LR 0.000071
Train loss: 0.5223;  Loss pred: 0.5223; Loss self: 0.0000; time: 0.20s
Val loss: 0.0430 score: 1.0000 time: 0.07s
Test loss: 0.2397 score: 0.9184 time: 0.06s
Epoch 663/1000, LR 0.000070
Train loss: 0.5228;  Loss pred: 0.5228; Loss self: 0.0000; time: 0.20s
Val loss: 0.0430 score: 1.0000 time: 0.07s
Test loss: 0.2397 score: 0.9184 time: 0.06s
Epoch 664/1000, LR 0.000070
Train loss: 0.5220;  Loss pred: 0.5220; Loss self: 0.0000; time: 0.21s
Val loss: 0.0429 score: 1.0000 time: 0.07s
Test loss: 0.2397 score: 0.9184 time: 0.06s
Epoch 665/1000, LR 0.000070
Train loss: 0.5217;  Loss pred: 0.5217; Loss self: 0.0000; time: 0.20s
Val loss: 0.0429 score: 1.0000 time: 0.08s
Test loss: 0.2397 score: 0.9184 time: 0.06s
Epoch 666/1000, LR 0.000069
Train loss: 0.5222;  Loss pred: 0.5222; Loss self: 0.0000; time: 0.20s
Val loss: 0.0429 score: 1.0000 time: 0.08s
Test loss: 0.2397 score: 0.9184 time: 0.06s
Epoch 667/1000, LR 0.000069
Train loss: 0.5202;  Loss pred: 0.5202; Loss self: 0.0000; time: 0.21s
Val loss: 0.0428 score: 1.0000 time: 0.63s
Test loss: 0.2397 score: 0.9184 time: 0.35s
Epoch 668/1000, LR 0.000069
Train loss: 0.5209;  Loss pred: 0.5209; Loss self: 0.0000; time: 1.35s
Val loss: 0.0428 score: 1.0000 time: 0.24s
Test loss: 0.2397 score: 0.9184 time: 1.21s
Epoch 669/1000, LR 0.000068
Train loss: 0.5199;  Loss pred: 0.5199; Loss self: 0.0000; time: 6.72s
Val loss: 0.0428 score: 1.0000 time: 0.09s
Test loss: 0.2397 score: 0.9184 time: 0.07s
Epoch 670/1000, LR 0.000068
Train loss: 0.5200;  Loss pred: 0.5200; Loss self: 0.0000; time: 0.22s
Val loss: 0.0427 score: 1.0000 time: 0.07s
Test loss: 0.2397 score: 0.9184 time: 0.06s
Epoch 671/1000, LR 0.000068
Train loss: 0.5202;  Loss pred: 0.5202; Loss self: 0.0000; time: 0.26s
Val loss: 0.0427 score: 1.0000 time: 0.07s
Test loss: 0.2397 score: 0.9184 time: 0.06s
Epoch 672/1000, LR 0.000067
Train loss: 0.5193;  Loss pred: 0.5193; Loss self: 0.0000; time: 0.20s
Val loss: 0.0427 score: 1.0000 time: 0.08s
Test loss: 0.2397 score: 0.9184 time: 0.08s
Epoch 673/1000, LR 0.000067
Train loss: 0.5201;  Loss pred: 0.5201; Loss self: 0.0000; time: 0.25s
Val loss: 0.0426 score: 1.0000 time: 0.08s
Test loss: 0.2398 score: 0.9184 time: 0.06s
Epoch 674/1000, LR 0.000066
Train loss: 0.5211;  Loss pred: 0.5211; Loss self: 0.0000; time: 0.21s
Val loss: 0.0426 score: 1.0000 time: 0.07s
Test loss: 0.2398 score: 0.9184 time: 0.07s
Epoch 675/1000, LR 0.000066
Train loss: 0.5217;  Loss pred: 0.5217; Loss self: 0.0000; time: 0.21s
Val loss: 0.0426 score: 1.0000 time: 0.08s
Test loss: 0.2398 score: 0.9184 time: 0.16s
Epoch 676/1000, LR 0.000066
Train loss: 0.5212;  Loss pred: 0.5212; Loss self: 0.0000; time: 0.21s
Val loss: 0.0425 score: 1.0000 time: 0.08s
Test loss: 0.2398 score: 0.9184 time: 0.06s
Epoch 677/1000, LR 0.000065
Train loss: 0.5208;  Loss pred: 0.5208; Loss self: 0.0000; time: 0.21s
Val loss: 0.0425 score: 1.0000 time: 0.08s
Test loss: 0.2399 score: 0.9184 time: 0.06s
Epoch 678/1000, LR 0.000065
Train loss: 0.5202;  Loss pred: 0.5202; Loss self: 0.0000; time: 0.21s
Val loss: 0.0425 score: 1.0000 time: 0.07s
Test loss: 0.2399 score: 0.9184 time: 0.07s
Epoch 679/1000, LR 0.000065
Train loss: 0.5198;  Loss pred: 0.5198; Loss self: 0.0000; time: 4.66s
Val loss: 0.0424 score: 1.0000 time: 0.68s
Test loss: 0.2399 score: 0.9184 time: 0.07s
Epoch 680/1000, LR 0.000064
Train loss: 0.5198;  Loss pred: 0.5198; Loss self: 0.0000; time: 0.21s
Val loss: 0.0424 score: 1.0000 time: 0.08s
Test loss: 0.2399 score: 0.9184 time: 0.07s
Epoch 681/1000, LR 0.000064
Train loss: 0.5214;  Loss pred: 0.5214; Loss self: 0.0000; time: 0.21s
Val loss: 0.0424 score: 1.0000 time: 0.08s
Test loss: 0.2399 score: 0.9184 time: 0.07s
Epoch 682/1000, LR 0.000063
Train loss: 0.5212;  Loss pred: 0.5212; Loss self: 0.0000; time: 0.21s
Val loss: 0.0423 score: 1.0000 time: 0.07s
Test loss: 0.2398 score: 0.9184 time: 0.07s
Epoch 683/1000, LR 0.000063
Train loss: 0.5191;  Loss pred: 0.5191; Loss self: 0.0000; time: 0.21s
Val loss: 0.0423 score: 1.0000 time: 0.08s
Test loss: 0.2398 score: 0.9184 time: 0.06s
Epoch 684/1000, LR 0.000063
Train loss: 0.5198;  Loss pred: 0.5198; Loss self: 0.0000; time: 0.21s
Val loss: 0.0422 score: 1.0000 time: 0.07s
Test loss: 0.2398 score: 0.9184 time: 0.06s
Epoch 685/1000, LR 0.000062
Train loss: 0.5194;  Loss pred: 0.5194; Loss self: 0.0000; time: 0.21s
Val loss: 0.0422 score: 1.0000 time: 0.08s
Test loss: 0.2398 score: 0.9184 time: 0.06s
Epoch 686/1000, LR 0.000062
Train loss: 0.5193;  Loss pred: 0.5193; Loss self: 0.0000; time: 0.22s
Val loss: 0.0422 score: 1.0000 time: 0.08s
Test loss: 0.2397 score: 0.9184 time: 0.07s
Epoch 687/1000, LR 0.000062
Train loss: 0.5182;  Loss pred: 0.5182; Loss self: 0.0000; time: 0.21s
Val loss: 0.0421 score: 1.0000 time: 0.08s
Test loss: 0.2397 score: 0.9184 time: 0.06s
Epoch 688/1000, LR 0.000061
Train loss: 0.5197;  Loss pred: 0.5197; Loss self: 0.0000; time: 0.21s
Val loss: 0.0421 score: 1.0000 time: 0.08s
Test loss: 0.2397 score: 0.9184 time: 0.06s
Epoch 689/1000, LR 0.000061
Train loss: 0.5212;  Loss pred: 0.5212; Loss self: 0.0000; time: 0.21s
Val loss: 0.0420 score: 1.0000 time: 0.07s
Test loss: 0.2396 score: 0.9184 time: 0.06s
Epoch 690/1000, LR 0.000061
Train loss: 0.5212;  Loss pred: 0.5212; Loss self: 0.0000; time: 0.20s
Val loss: 0.0420 score: 1.0000 time: 0.07s
Test loss: 0.2396 score: 0.9184 time: 0.06s
Epoch 691/1000, LR 0.000060
Train loss: 0.5198;  Loss pred: 0.5198; Loss self: 0.0000; time: 0.20s
Val loss: 0.0420 score: 1.0000 time: 0.07s
Test loss: 0.2396 score: 0.9184 time: 0.08s
Epoch 692/1000, LR 0.000060
Train loss: 0.5201;  Loss pred: 0.5201; Loss self: 0.0000; time: 0.20s
Val loss: 0.0419 score: 1.0000 time: 0.07s
Test loss: 0.2395 score: 0.9184 time: 0.06s
Epoch 693/1000, LR 0.000060
Train loss: 0.5209;  Loss pred: 0.5209; Loss self: 0.0000; time: 0.20s
Val loss: 0.0419 score: 1.0000 time: 0.07s
Test loss: 0.2395 score: 0.9184 time: 0.06s
Epoch 694/1000, LR 0.000059
Train loss: 0.5198;  Loss pred: 0.5198; Loss self: 0.0000; time: 0.20s
Val loss: 0.0419 score: 1.0000 time: 0.08s
Test loss: 0.2395 score: 0.9184 time: 0.06s
Epoch 695/1000, LR 0.000059
Train loss: 0.5193;  Loss pred: 0.5193; Loss self: 0.0000; time: 0.20s
Val loss: 0.0418 score: 1.0000 time: 0.07s
Test loss: 0.2395 score: 0.9184 time: 0.06s
Epoch 696/1000, LR 0.000058
Train loss: 0.5211;  Loss pred: 0.5211; Loss self: 0.0000; time: 0.20s
Val loss: 0.0418 score: 1.0000 time: 0.07s
Test loss: 0.2395 score: 0.9184 time: 0.06s
Epoch 697/1000, LR 0.000058
Train loss: 0.5213;  Loss pred: 0.5213; Loss self: 0.0000; time: 0.20s
Val loss: 0.0418 score: 1.0000 time: 0.07s
Test loss: 0.2394 score: 0.9184 time: 0.06s
Epoch 698/1000, LR 0.000058
Train loss: 0.5185;  Loss pred: 0.5185; Loss self: 0.0000; time: 0.21s
Val loss: 0.0417 score: 1.0000 time: 0.08s
Test loss: 0.2393 score: 0.9184 time: 0.06s
Epoch 699/1000, LR 0.000057
Train loss: 0.5214;  Loss pred: 0.5214; Loss self: 0.0000; time: 0.20s
Val loss: 0.0417 score: 1.0000 time: 0.08s
Test loss: 0.2392 score: 0.9184 time: 0.06s
Epoch 700/1000, LR 0.000057
Train loss: 0.5197;  Loss pred: 0.5197; Loss self: 0.0000; time: 0.20s
Val loss: 0.0416 score: 1.0000 time: 0.07s
Test loss: 0.2392 score: 0.9184 time: 0.06s
Epoch 701/1000, LR 0.000057
Train loss: 0.5202;  Loss pred: 0.5202; Loss self: 0.0000; time: 0.20s
Val loss: 0.0416 score: 1.0000 time: 0.07s
Test loss: 0.2391 score: 0.9184 time: 0.06s
Epoch 702/1000, LR 0.000056
Train loss: 0.5189;  Loss pred: 0.5189; Loss self: 0.0000; time: 0.21s
Val loss: 0.0416 score: 1.0000 time: 0.07s
Test loss: 0.2390 score: 0.9184 time: 0.06s
Epoch 703/1000, LR 0.000056
Train loss: 0.5198;  Loss pred: 0.5198; Loss self: 0.0000; time: 0.20s
Val loss: 0.0415 score: 1.0000 time: 0.07s
Test loss: 0.2390 score: 0.9184 time: 0.07s
Epoch 704/1000, LR 0.000056
Train loss: 0.5177;  Loss pred: 0.5177; Loss self: 0.0000; time: 0.20s
Val loss: 0.0415 score: 1.0000 time: 0.08s
Test loss: 0.2390 score: 0.9184 time: 0.06s
Epoch 705/1000, LR 0.000055
Train loss: 0.5197;  Loss pred: 0.5197; Loss self: 0.0000; time: 0.21s
Val loss: 0.0415 score: 1.0000 time: 0.08s
Test loss: 0.2390 score: 0.9184 time: 0.06s
Epoch 706/1000, LR 0.000055
Train loss: 0.5196;  Loss pred: 0.5196; Loss self: 0.0000; time: 0.21s
Val loss: 0.0414 score: 1.0000 time: 0.09s
Test loss: 0.2390 score: 0.9184 time: 0.08s
Epoch 707/1000, LR 0.000055
Train loss: 0.5242;  Loss pred: 0.5242; Loss self: 0.0000; time: 0.21s
Val loss: 0.0414 score: 1.0000 time: 0.08s
Test loss: 0.2390 score: 0.9184 time: 0.06s
Epoch 708/1000, LR 0.000054
Train loss: 0.5193;  Loss pred: 0.5193; Loss self: 0.0000; time: 0.21s
Val loss: 0.0414 score: 1.0000 time: 0.07s
Test loss: 0.2390 score: 0.9184 time: 0.06s
Epoch 709/1000, LR 0.000054
Train loss: 0.5193;  Loss pred: 0.5193; Loss self: 0.0000; time: 0.21s
Val loss: 0.0413 score: 1.0000 time: 0.08s
Test loss: 0.2390 score: 0.9184 time: 0.07s
Epoch 710/1000, LR 0.000054
Train loss: 0.5179;  Loss pred: 0.5179; Loss self: 0.0000; time: 2.37s
Val loss: 0.0413 score: 1.0000 time: 1.44s
Test loss: 0.2390 score: 0.9184 time: 0.93s
Epoch 711/1000, LR 0.000053
Train loss: 0.5191;  Loss pred: 0.5191; Loss self: 0.0000; time: 0.96s
Val loss: 0.0413 score: 1.0000 time: 0.14s
Test loss: 0.2389 score: 0.9184 time: 0.19s
Epoch 712/1000, LR 0.000053
Train loss: 0.5173;  Loss pred: 0.5173; Loss self: 0.0000; time: 1.09s
Val loss: 0.0413 score: 1.0000 time: 0.20s
Test loss: 0.2390 score: 0.9184 time: 0.22s
Epoch 713/1000, LR 0.000053
Train loss: 0.5203;  Loss pred: 0.5203; Loss self: 0.0000; time: 0.70s
Val loss: 0.0412 score: 1.0000 time: 0.07s
Test loss: 0.2390 score: 0.9184 time: 0.06s
Epoch 714/1000, LR 0.000052
Train loss: 0.5164;  Loss pred: 0.5164; Loss self: 0.0000; time: 0.21s
Val loss: 0.0412 score: 1.0000 time: 0.07s
Test loss: 0.2390 score: 0.9184 time: 0.06s
Epoch 715/1000, LR 0.000052
Train loss: 0.5186;  Loss pred: 0.5186; Loss self: 0.0000; time: 0.20s
Val loss: 0.0412 score: 1.0000 time: 0.07s
Test loss: 0.2391 score: 0.9184 time: 0.06s
Epoch 716/1000, LR 0.000052
Train loss: 0.5192;  Loss pred: 0.5192; Loss self: 0.0000; time: 0.20s
Val loss: 0.0412 score: 1.0000 time: 0.07s
Test loss: 0.2391 score: 0.9184 time: 0.06s
Epoch 717/1000, LR 0.000051
Train loss: 0.5185;  Loss pred: 0.5185; Loss self: 0.0000; time: 0.20s
Val loss: 0.0412 score: 1.0000 time: 0.07s
Test loss: 0.2392 score: 0.9184 time: 0.06s
Epoch 718/1000, LR 0.000051
Train loss: 0.5185;  Loss pred: 0.5185; Loss self: 0.0000; time: 0.20s
Val loss: 0.0412 score: 1.0000 time: 0.07s
Test loss: 0.2392 score: 0.9184 time: 0.06s
Epoch 719/1000, LR 0.000051
Train loss: 0.5175;  Loss pred: 0.5175; Loss self: 0.0000; time: 0.20s
Val loss: 0.0411 score: 1.0000 time: 0.07s
Test loss: 0.2393 score: 0.9184 time: 0.06s
Epoch 720/1000, LR 0.000050
Train loss: 0.5172;  Loss pred: 0.5172; Loss self: 0.0000; time: 0.21s
Val loss: 0.0411 score: 1.0000 time: 0.07s
Test loss: 0.2393 score: 0.9184 time: 0.06s
Epoch 721/1000, LR 0.000050
Train loss: 0.5202;  Loss pred: 0.5202; Loss self: 0.0000; time: 0.20s
Val loss: 0.0411 score: 1.0000 time: 0.08s
Test loss: 0.2394 score: 0.9184 time: 0.06s
Epoch 722/1000, LR 0.000050
Train loss: 0.5172;  Loss pred: 0.5172; Loss self: 0.0000; time: 2.10s
Val loss: 0.0411 score: 1.0000 time: 0.62s
Test loss: 0.2394 score: 0.9184 time: 1.18s
Epoch 723/1000, LR 0.000049
Train loss: 0.5182;  Loss pred: 0.5182; Loss self: 0.0000; time: 4.64s
Val loss: 0.0411 score: 1.0000 time: 0.18s
Test loss: 0.2395 score: 0.9184 time: 0.40s
Epoch 724/1000, LR 0.000049
Train loss: 0.5202;  Loss pred: 0.5202; Loss self: 0.0000; time: 0.47s
Val loss: 0.0410 score: 1.0000 time: 0.07s
Test loss: 0.2395 score: 0.9184 time: 0.07s
Epoch 725/1000, LR 0.000049
Train loss: 0.5191;  Loss pred: 0.5191; Loss self: 0.0000; time: 0.22s
Val loss: 0.0410 score: 1.0000 time: 0.07s
Test loss: 0.2395 score: 0.9184 time: 0.07s
Epoch 726/1000, LR 0.000048
Train loss: 0.5185;  Loss pred: 0.5185; Loss self: 0.0000; time: 0.21s
Val loss: 0.0410 score: 1.0000 time: 0.07s
Test loss: 0.2395 score: 0.9184 time: 0.06s
Epoch 727/1000, LR 0.000048
Train loss: 0.5185;  Loss pred: 0.5185; Loss self: 0.0000; time: 0.21s
Val loss: 0.0410 score: 1.0000 time: 0.08s
Test loss: 0.2396 score: 0.9184 time: 0.07s
Epoch 728/1000, LR 0.000048
Train loss: 0.5180;  Loss pred: 0.5180; Loss self: 0.0000; time: 0.22s
Val loss: 0.0410 score: 1.0000 time: 0.08s
Test loss: 0.2396 score: 0.9184 time: 0.06s
Epoch 729/1000, LR 0.000047
Train loss: 0.5182;  Loss pred: 0.5182; Loss self: 0.0000; time: 0.22s
Val loss: 0.0410 score: 1.0000 time: 0.08s
Test loss: 0.2397 score: 0.9184 time: 0.07s
Epoch 730/1000, LR 0.000047
Train loss: 0.5177;  Loss pred: 0.5177; Loss self: 0.0000; time: 0.21s
Val loss: 0.0409 score: 1.0000 time: 0.08s
Test loss: 0.2397 score: 0.9184 time: 0.06s
Epoch 731/1000, LR 0.000047
Train loss: 0.5182;  Loss pred: 0.5182; Loss self: 0.0000; time: 0.21s
Val loss: 0.0409 score: 1.0000 time: 0.08s
Test loss: 0.2397 score: 0.9184 time: 0.06s
Epoch 732/1000, LR 0.000046
Train loss: 0.5184;  Loss pred: 0.5184; Loss self: 0.0000; time: 0.22s
Val loss: 0.0409 score: 1.0000 time: 0.07s
Test loss: 0.2397 score: 0.9184 time: 0.07s
Epoch 733/1000, LR 0.000046
Train loss: 0.5190;  Loss pred: 0.5190; Loss self: 0.0000; time: 0.21s
Val loss: 0.0409 score: 1.0000 time: 0.07s
Test loss: 0.2397 score: 0.9184 time: 0.06s
Epoch 734/1000, LR 0.000046
Train loss: 0.5187;  Loss pred: 0.5187; Loss self: 0.0000; time: 0.21s
Val loss: 0.0409 score: 1.0000 time: 0.07s
Test loss: 0.2397 score: 0.9184 time: 0.06s
Epoch 735/1000, LR 0.000045
Train loss: 0.5186;  Loss pred: 0.5186; Loss self: 0.0000; time: 0.21s
Val loss: 0.0408 score: 1.0000 time: 0.09s
Test loss: 0.2397 score: 0.9184 time: 1.23s
Epoch 736/1000, LR 0.000045
Train loss: 0.5187;  Loss pred: 0.5187; Loss self: 0.0000; time: 3.51s
Val loss: 0.0408 score: 1.0000 time: 0.17s
Test loss: 0.2397 score: 0.9184 time: 0.40s
Epoch 737/1000, LR 0.000045
Train loss: 0.5173;  Loss pred: 0.5173; Loss self: 0.0000; time: 0.38s
Val loss: 0.0408 score: 1.0000 time: 0.08s
Test loss: 0.2397 score: 0.9184 time: 0.07s
Epoch 738/1000, LR 0.000044
Train loss: 0.5218;  Loss pred: 0.5218; Loss self: 0.0000; time: 0.22s
Val loss: 0.0408 score: 1.0000 time: 0.07s
Test loss: 0.2397 score: 0.9184 time: 0.06s
Epoch 739/1000, LR 0.000044
Train loss: 0.5165;  Loss pred: 0.5165; Loss self: 0.0000; time: 0.20s
Val loss: 0.0407 score: 1.0000 time: 0.07s
Test loss: 0.2396 score: 0.9184 time: 0.06s
Epoch 740/1000, LR 0.000044
Train loss: 0.5188;  Loss pred: 0.5188; Loss self: 0.0000; time: 0.21s
Val loss: 0.0407 score: 1.0000 time: 0.07s
Test loss: 0.2396 score: 0.9184 time: 0.07s
Epoch 741/1000, LR 0.000043
Train loss: 0.5197;  Loss pred: 0.5197; Loss self: 0.0000; time: 0.21s
Val loss: 0.0407 score: 1.0000 time: 0.07s
Test loss: 0.2395 score: 0.9184 time: 0.06s
Epoch 742/1000, LR 0.000043
Train loss: 0.5172;  Loss pred: 0.5172; Loss self: 0.0000; time: 0.21s
Val loss: 0.0406 score: 1.0000 time: 0.07s
Test loss: 0.2395 score: 0.9184 time: 0.06s
Epoch 743/1000, LR 0.000043
Train loss: 0.5189;  Loss pred: 0.5189; Loss self: 0.0000; time: 0.21s
Val loss: 0.0406 score: 1.0000 time: 0.07s
Test loss: 0.2395 score: 0.9184 time: 0.06s
Epoch 744/1000, LR 0.000042
Train loss: 0.5169;  Loss pred: 0.5169; Loss self: 0.0000; time: 0.21s
Val loss: 0.0406 score: 1.0000 time: 0.07s
Test loss: 0.2395 score: 0.9184 time: 0.07s
Epoch 745/1000, LR 0.000042
Train loss: 0.5140;  Loss pred: 0.5140; Loss self: 0.0000; time: 0.20s
Val loss: 0.0406 score: 1.0000 time: 0.08s
Test loss: 0.2395 score: 0.9184 time: 0.06s
Epoch 746/1000, LR 0.000042
Train loss: 0.5157;  Loss pred: 0.5157; Loss self: 0.0000; time: 0.21s
Val loss: 0.0406 score: 1.0000 time: 0.07s
Test loss: 0.2396 score: 0.9184 time: 0.06s
Epoch 747/1000, LR 0.000042
Train loss: 0.5178;  Loss pred: 0.5178; Loss self: 0.0000; time: 0.21s
Val loss: 0.0406 score: 1.0000 time: 0.08s
Test loss: 0.2396 score: 0.9184 time: 0.06s
Epoch 748/1000, LR 0.000041
Train loss: 0.5208;  Loss pred: 0.5208; Loss self: 0.0000; time: 0.20s
Val loss: 0.0405 score: 1.0000 time: 0.07s
Test loss: 0.2396 score: 0.9184 time: 0.06s
Epoch 749/1000, LR 0.000041
Train loss: 0.5196;  Loss pred: 0.5196; Loss self: 0.0000; time: 0.20s
Val loss: 0.0405 score: 1.0000 time: 0.07s
Test loss: 0.2396 score: 0.9184 time: 0.06s
Epoch 750/1000, LR 0.000041
Train loss: 0.5172;  Loss pred: 0.5172; Loss self: 0.0000; time: 0.20s
Val loss: 0.0405 score: 1.0000 time: 0.07s
Test loss: 0.2396 score: 0.9184 time: 0.06s
Epoch 751/1000, LR 0.000040
Train loss: 0.5189;  Loss pred: 0.5189; Loss self: 0.0000; time: 0.21s
Val loss: 0.0405 score: 1.0000 time: 0.07s
Test loss: 0.2395 score: 0.9184 time: 0.06s
Epoch 752/1000, LR 0.000040
Train loss: 0.5182;  Loss pred: 0.5182; Loss self: 0.0000; time: 0.21s
Val loss: 0.0404 score: 1.0000 time: 0.07s
Test loss: 0.2395 score: 0.9184 time: 0.06s
Epoch 753/1000, LR 0.000040
Train loss: 0.5174;  Loss pred: 0.5174; Loss self: 0.0000; time: 0.21s
Val loss: 0.0404 score: 1.0000 time: 0.07s
Test loss: 0.2395 score: 0.9184 time: 0.07s
Epoch 754/1000, LR 0.000039
Train loss: 0.5169;  Loss pred: 0.5169; Loss self: 0.0000; time: 0.21s
Val loss: 0.0404 score: 1.0000 time: 0.07s
Test loss: 0.2394 score: 0.9184 time: 0.06s
Epoch 755/1000, LR 0.000039
Train loss: 0.5167;  Loss pred: 0.5167; Loss self: 0.0000; time: 0.22s
Val loss: 0.0404 score: 1.0000 time: 0.13s
Test loss: 0.2394 score: 0.9184 time: 0.06s
Epoch 756/1000, LR 0.000039
Train loss: 0.5161;  Loss pred: 0.5161; Loss self: 0.0000; time: 0.22s
Val loss: 0.0403 score: 1.0000 time: 0.07s
Test loss: 0.2394 score: 0.9184 time: 0.06s
Epoch 757/1000, LR 0.000038
Train loss: 0.5181;  Loss pred: 0.5181; Loss self: 0.0000; time: 0.20s
Val loss: 0.0403 score: 1.0000 time: 0.07s
Test loss: 0.2394 score: 0.9184 time: 0.06s
Epoch 758/1000, LR 0.000038
Train loss: 0.5189;  Loss pred: 0.5189; Loss self: 0.0000; time: 0.22s
Val loss: 0.0403 score: 1.0000 time: 0.08s
Test loss: 0.2394 score: 0.9184 time: 0.07s
Epoch 759/1000, LR 0.000038
Train loss: 0.5172;  Loss pred: 0.5172; Loss self: 0.0000; time: 0.22s
Val loss: 0.0403 score: 1.0000 time: 0.08s
Test loss: 0.2394 score: 0.9184 time: 0.07s
Epoch 760/1000, LR 0.000038
Train loss: 0.5170;  Loss pred: 0.5170; Loss self: 0.0000; time: 0.24s
Val loss: 0.0403 score: 1.0000 time: 0.10s
Test loss: 0.2394 score: 0.9184 time: 0.06s
Epoch 761/1000, LR 0.000037
Train loss: 0.5161;  Loss pred: 0.5161; Loss self: 0.0000; time: 0.21s
Val loss: 0.0403 score: 1.0000 time: 1.26s
Test loss: 0.2394 score: 0.9184 time: 0.87s
Epoch 762/1000, LR 0.000037
Train loss: 0.5180;  Loss pred: 0.5180; Loss self: 0.0000; time: 1.80s
Val loss: 0.0402 score: 1.0000 time: 0.41s
Test loss: 0.2394 score: 0.9184 time: 0.27s
Epoch 763/1000, LR 0.000037
Train loss: 0.5178;  Loss pred: 0.5178; Loss self: 0.0000; time: 1.12s
Val loss: 0.0402 score: 1.0000 time: 0.68s
Test loss: 0.2395 score: 0.9184 time: 0.07s
Epoch 764/1000, LR 0.000036
Train loss: 0.5163;  Loss pred: 0.5163; Loss self: 0.0000; time: 0.21s
Val loss: 0.0402 score: 1.0000 time: 0.07s
Test loss: 0.2395 score: 0.9184 time: 0.06s
Epoch 765/1000, LR 0.000036
Train loss: 0.5158;  Loss pred: 0.5158; Loss self: 0.0000; time: 0.24s
Val loss: 0.0402 score: 1.0000 time: 0.07s
Test loss: 0.2396 score: 0.9184 time: 0.06s
Epoch 766/1000, LR 0.000036
Train loss: 0.5162;  Loss pred: 0.5162; Loss self: 0.0000; time: 0.20s
Val loss: 0.0402 score: 1.0000 time: 0.07s
Test loss: 0.2396 score: 0.9184 time: 0.06s
Epoch 767/1000, LR 0.000036
Train loss: 0.5181;  Loss pred: 0.5181; Loss self: 0.0000; time: 0.20s
Val loss: 0.0402 score: 1.0000 time: 0.08s
Test loss: 0.2397 score: 0.9184 time: 0.06s
Epoch 768/1000, LR 0.000035
Train loss: 0.5161;  Loss pred: 0.5161; Loss self: 0.0000; time: 0.21s
Val loss: 0.0402 score: 1.0000 time: 0.07s
Test loss: 0.2397 score: 0.9184 time: 0.06s
Epoch 769/1000, LR 0.000035
Train loss: 0.5164;  Loss pred: 0.5164; Loss self: 0.0000; time: 0.20s
Val loss: 0.0402 score: 1.0000 time: 0.07s
Test loss: 0.2398 score: 0.9184 time: 0.06s
Epoch 770/1000, LR 0.000035
Train loss: 0.5160;  Loss pred: 0.5160; Loss self: 0.0000; time: 0.21s
Val loss: 0.0402 score: 1.0000 time: 0.08s
Test loss: 0.2399 score: 0.9184 time: 0.06s
Epoch 771/1000, LR 0.000034
Train loss: 0.5187;  Loss pred: 0.5187; Loss self: 0.0000; time: 0.20s
Val loss: 0.0402 score: 1.0000 time: 0.07s
Test loss: 0.2399 score: 0.9184 time: 0.07s
Epoch 772/1000, LR 0.000034
Train loss: 0.5184;  Loss pred: 0.5184; Loss self: 0.0000; time: 0.20s
Val loss: 0.0402 score: 1.0000 time: 0.07s
Test loss: 0.2399 score: 0.9184 time: 0.07s
Epoch 773/1000, LR 0.000034
Train loss: 0.5165;  Loss pred: 0.5165; Loss self: 0.0000; time: 0.20s
Val loss: 0.0402 score: 1.0000 time: 0.07s
Test loss: 0.2400 score: 0.9184 time: 0.06s
Epoch 774/1000, LR 0.000034
Train loss: 0.5154;  Loss pred: 0.5154; Loss self: 0.0000; time: 0.20s
Val loss: 0.0401 score: 1.0000 time: 0.07s
Test loss: 0.2400 score: 0.9184 time: 0.06s
Epoch 775/1000, LR 0.000033
Train loss: 0.5183;  Loss pred: 0.5183; Loss self: 0.0000; time: 0.21s
Val loss: 0.0401 score: 1.0000 time: 0.08s
Test loss: 0.2400 score: 0.9184 time: 0.06s
Epoch 776/1000, LR 0.000033
Train loss: 0.5192;  Loss pred: 0.5192; Loss self: 0.0000; time: 0.21s
Val loss: 0.0401 score: 1.0000 time: 0.08s
Test loss: 0.2400 score: 0.9184 time: 0.07s
Epoch 777/1000, LR 0.000033
Train loss: 0.5171;  Loss pred: 0.5171; Loss self: 0.0000; time: 0.21s
Val loss: 0.0401 score: 1.0000 time: 0.07s
Test loss: 0.2400 score: 0.9184 time: 0.06s
Epoch 778/1000, LR 0.000032
Train loss: 0.5162;  Loss pred: 0.5162; Loss self: 0.0000; time: 0.20s
Val loss: 0.0401 score: 1.0000 time: 0.07s
Test loss: 0.2400 score: 0.9184 time: 0.06s
Epoch 779/1000, LR 0.000032
Train loss: 0.5158;  Loss pred: 0.5158; Loss self: 0.0000; time: 0.20s
Val loss: 0.0401 score: 1.0000 time: 0.08s
Test loss: 0.2400 score: 0.9184 time: 0.06s
Epoch 780/1000, LR 0.000032
Train loss: 0.5156;  Loss pred: 0.5156; Loss self: 0.0000; time: 0.21s
Val loss: 0.0400 score: 1.0000 time: 0.07s
Test loss: 0.2400 score: 0.9184 time: 0.06s
Epoch 781/1000, LR 0.000032
Train loss: 0.5169;  Loss pred: 0.5169; Loss self: 0.0000; time: 0.21s
Val loss: 0.0400 score: 1.0000 time: 0.07s
Test loss: 0.2400 score: 0.9184 time: 0.06s
Epoch 782/1000, LR 0.000031
Train loss: 0.5174;  Loss pred: 0.5174; Loss self: 0.0000; time: 0.21s
Val loss: 0.0400 score: 1.0000 time: 0.08s
Test loss: 0.2400 score: 0.9184 time: 0.06s
Epoch 783/1000, LR 0.000031
Train loss: 0.5171;  Loss pred: 0.5171; Loss self: 0.0000; time: 0.21s
Val loss: 0.0400 score: 1.0000 time: 0.07s
Test loss: 0.2400 score: 0.9184 time: 0.07s
Epoch 784/1000, LR 0.000031
Train loss: 0.5173;  Loss pred: 0.5173; Loss self: 0.0000; time: 0.20s
Val loss: 0.0400 score: 1.0000 time: 0.08s
Test loss: 0.2399 score: 0.9184 time: 0.06s
Epoch 785/1000, LR 0.000030
Train loss: 0.5163;  Loss pred: 0.5163; Loss self: 0.0000; time: 0.20s
Val loss: 0.0400 score: 1.0000 time: 0.07s
Test loss: 0.2399 score: 0.9184 time: 0.06s
Epoch 786/1000, LR 0.000030
Train loss: 0.5173;  Loss pred: 0.5173; Loss self: 0.0000; time: 0.20s
Val loss: 0.0399 score: 1.0000 time: 0.07s
Test loss: 0.2399 score: 0.9184 time: 0.07s
Epoch 787/1000, LR 0.000030
Train loss: 0.5161;  Loss pred: 0.5161; Loss self: 0.0000; time: 0.20s
Val loss: 0.0399 score: 1.0000 time: 0.07s
Test loss: 0.2399 score: 0.9184 time: 0.06s
Epoch 788/1000, LR 0.000030
Train loss: 0.5160;  Loss pred: 0.5160; Loss self: 0.0000; time: 0.20s
Val loss: 0.0399 score: 1.0000 time: 0.08s
Test loss: 0.2399 score: 0.9184 time: 0.06s
Epoch 789/1000, LR 0.000029
Train loss: 0.5157;  Loss pred: 0.5157; Loss self: 0.0000; time: 0.21s
Val loss: 0.0399 score: 1.0000 time: 0.07s
Test loss: 0.2399 score: 0.9184 time: 0.07s
Epoch 790/1000, LR 0.000029
Train loss: 0.5154;  Loss pred: 0.5154; Loss self: 0.0000; time: 0.21s
Val loss: 0.0399 score: 1.0000 time: 0.07s
Test loss: 0.2399 score: 0.9184 time: 0.06s
Epoch 791/1000, LR 0.000029
Train loss: 0.5178;  Loss pred: 0.5178; Loss self: 0.0000; time: 0.21s
Val loss: 0.0399 score: 1.0000 time: 0.07s
Test loss: 0.2399 score: 0.9184 time: 0.06s
Epoch 792/1000, LR 0.000029
Train loss: 0.5150;  Loss pred: 0.5150; Loss self: 0.0000; time: 0.21s
Val loss: 0.0398 score: 1.0000 time: 0.07s
Test loss: 0.2399 score: 0.9184 time: 0.07s
Epoch 793/1000, LR 0.000028
Train loss: 0.5161;  Loss pred: 0.5161; Loss self: 0.0000; time: 0.21s
Val loss: 0.0398 score: 1.0000 time: 0.08s
Test loss: 0.2399 score: 0.9184 time: 0.06s
Epoch 794/1000, LR 0.000028
Train loss: 0.5180;  Loss pred: 0.5180; Loss self: 0.0000; time: 0.20s
Val loss: 0.0398 score: 1.0000 time: 0.08s
Test loss: 0.2398 score: 0.9184 time: 0.07s
Epoch 795/1000, LR 0.000028
Train loss: 0.5160;  Loss pred: 0.5160; Loss self: 0.0000; time: 0.22s
Val loss: 0.0398 score: 1.0000 time: 0.08s
Test loss: 0.2398 score: 0.9184 time: 0.06s
Epoch 796/1000, LR 0.000028
Train loss: 0.5153;  Loss pred: 0.5153; Loss self: 0.0000; time: 0.21s
Val loss: 0.0398 score: 1.0000 time: 0.08s
Test loss: 0.2398 score: 0.9184 time: 0.06s
Epoch 797/1000, LR 0.000027
Train loss: 0.5157;  Loss pred: 0.5157; Loss self: 0.0000; time: 0.21s
Val loss: 0.0398 score: 1.0000 time: 0.07s
Test loss: 0.2398 score: 0.9184 time: 0.06s
Epoch 798/1000, LR 0.000027
Train loss: 0.5182;  Loss pred: 0.5182; Loss self: 0.0000; time: 0.21s
Val loss: 0.0397 score: 1.0000 time: 0.07s
Test loss: 0.2398 score: 0.9184 time: 0.07s
Epoch 799/1000, LR 0.000027
Train loss: 0.5166;  Loss pred: 0.5166; Loss self: 0.0000; time: 0.21s
Val loss: 0.0397 score: 1.0000 time: 0.45s
Test loss: 0.2398 score: 0.9184 time: 1.95s
Epoch 800/1000, LR 0.000027
Train loss: 0.5145;  Loss pred: 0.5145; Loss self: 0.0000; time: 2.56s
Val loss: 0.0397 score: 1.0000 time: 1.09s
Test loss: 0.2398 score: 0.9184 time: 0.96s
Epoch 801/1000, LR 0.000026
Train loss: 0.5152;  Loss pred: 0.5152; Loss self: 0.0000; time: 4.90s
Val loss: 0.0397 score: 1.0000 time: 0.08s
Test loss: 0.2398 score: 0.9184 time: 0.07s
Epoch 802/1000, LR 0.000026
Train loss: 0.5165;  Loss pred: 0.5165; Loss self: 0.0000; time: 0.22s
Val loss: 0.0397 score: 1.0000 time: 0.07s
Test loss: 0.2398 score: 0.9184 time: 0.06s
Epoch 803/1000, LR 0.000026
Train loss: 0.5170;  Loss pred: 0.5170; Loss self: 0.0000; time: 0.20s
Val loss: 0.0397 score: 1.0000 time: 0.07s
Test loss: 0.2398 score: 0.9184 time: 0.06s
Epoch 804/1000, LR 0.000026
Train loss: 0.5170;  Loss pred: 0.5170; Loss self: 0.0000; time: 0.21s
Val loss: 0.0397 score: 1.0000 time: 0.07s
Test loss: 0.2398 score: 0.9184 time: 0.06s
Epoch 805/1000, LR 0.000025
Train loss: 0.5160;  Loss pred: 0.5160; Loss self: 0.0000; time: 0.21s
Val loss: 0.0397 score: 1.0000 time: 0.07s
Test loss: 0.2398 score: 0.9184 time: 0.06s
Epoch 806/1000, LR 0.000025
Train loss: 0.5155;  Loss pred: 0.5155; Loss self: 0.0000; time: 0.21s
Val loss: 0.0397 score: 1.0000 time: 0.07s
Test loss: 0.2398 score: 0.9184 time: 0.06s
Epoch 807/1000, LR 0.000025
Train loss: 0.5147;  Loss pred: 0.5147; Loss self: 0.0000; time: 0.21s
Val loss: 0.0397 score: 1.0000 time: 0.07s
Test loss: 0.2399 score: 0.9184 time: 0.06s
Epoch 808/1000, LR 0.000025
Train loss: 0.5158;  Loss pred: 0.5158; Loss self: 0.0000; time: 0.21s
Val loss: 0.0396 score: 1.0000 time: 0.08s
Test loss: 0.2399 score: 0.9184 time: 0.06s
Epoch 809/1000, LR 0.000024
Train loss: 0.5151;  Loss pred: 0.5151; Loss self: 0.0000; time: 0.21s
Val loss: 0.0396 score: 1.0000 time: 0.08s
Test loss: 0.2399 score: 0.9184 time: 0.06s
Epoch 810/1000, LR 0.000024
Train loss: 0.5165;  Loss pred: 0.5165; Loss self: 0.0000; time: 0.21s
Val loss: 0.0396 score: 1.0000 time: 0.07s
Test loss: 0.2399 score: 0.9184 time: 0.06s
Epoch 811/1000, LR 0.000024
Train loss: 0.5161;  Loss pred: 0.5161; Loss self: 0.0000; time: 0.20s
Val loss: 0.0396 score: 1.0000 time: 0.07s
Test loss: 0.2399 score: 0.9184 time: 0.06s
Epoch 812/1000, LR 0.000024
Train loss: 0.5145;  Loss pred: 0.5145; Loss self: 0.0000; time: 0.20s
Val loss: 0.0396 score: 1.0000 time: 0.08s
Test loss: 0.2399 score: 0.9184 time: 0.07s
Epoch 813/1000, LR 0.000023
Train loss: 0.5168;  Loss pred: 0.5168; Loss self: 0.0000; time: 0.23s
Val loss: 0.0396 score: 1.0000 time: 0.08s
Test loss: 0.2399 score: 0.9184 time: 0.07s
Epoch 814/1000, LR 0.000023
Train loss: 0.5163;  Loss pred: 0.5163; Loss self: 0.0000; time: 0.23s
Val loss: 0.0396 score: 1.0000 time: 0.08s
Test loss: 0.2399 score: 0.9184 time: 0.07s
Epoch 815/1000, LR 0.000023
Train loss: 0.5146;  Loss pred: 0.5146; Loss self: 0.0000; time: 0.23s
Val loss: 0.0396 score: 1.0000 time: 0.07s
Test loss: 0.2399 score: 0.9184 time: 0.07s
Epoch 816/1000, LR 0.000023
Train loss: 0.5169;  Loss pred: 0.5169; Loss self: 0.0000; time: 0.23s
Val loss: 0.0396 score: 1.0000 time: 0.07s
Test loss: 0.2399 score: 0.9184 time: 0.07s
Epoch 817/1000, LR 0.000022
Train loss: 0.5152;  Loss pred: 0.5152; Loss self: 0.0000; time: 0.21s
Val loss: 0.0395 score: 1.0000 time: 0.07s
Test loss: 0.2399 score: 0.9184 time: 0.07s
Epoch 818/1000, LR 0.000022
Train loss: 0.5168;  Loss pred: 0.5168; Loss self: 0.0000; time: 5.86s
Val loss: 0.0395 score: 1.0000 time: 0.67s
Test loss: 0.2399 score: 0.9184 time: 0.40s
Epoch 819/1000, LR 0.000022
Train loss: 0.5160;  Loss pred: 0.5160; Loss self: 0.0000; time: 0.48s
Val loss: 0.0395 score: 1.0000 time: 0.07s
Test loss: 0.2399 score: 0.9184 time: 0.07s
Epoch 820/1000, LR 0.000022
Train loss: 0.5158;  Loss pred: 0.5158; Loss self: 0.0000; time: 0.20s
Val loss: 0.0395 score: 1.0000 time: 0.07s
Test loss: 0.2399 score: 0.9184 time: 0.06s
Epoch 821/1000, LR 0.000021
Train loss: 0.5179;  Loss pred: 0.5179; Loss self: 0.0000; time: 0.20s
Val loss: 0.0395 score: 1.0000 time: 0.07s
Test loss: 0.2399 score: 0.9184 time: 0.06s
Epoch 822/1000, LR 0.000021
Train loss: 0.5171;  Loss pred: 0.5171; Loss self: 0.0000; time: 0.20s
Val loss: 0.0395 score: 1.0000 time: 0.07s
Test loss: 0.2399 score: 0.9184 time: 0.06s
Epoch 823/1000, LR 0.000021
Train loss: 0.5163;  Loss pred: 0.5163; Loss self: 0.0000; time: 0.20s
Val loss: 0.0395 score: 1.0000 time: 0.07s
Test loss: 0.2399 score: 0.9184 time: 0.06s
Epoch 824/1000, LR 0.000021
Train loss: 0.5149;  Loss pred: 0.5149; Loss self: 0.0000; time: 0.20s
Val loss: 0.0395 score: 1.0000 time: 0.07s
Test loss: 0.2399 score: 0.9184 time: 0.06s
Epoch 825/1000, LR 0.000021
Train loss: 0.5155;  Loss pred: 0.5155; Loss self: 0.0000; time: 0.20s
Val loss: 0.0395 score: 1.0000 time: 0.07s
Test loss: 0.2399 score: 0.9184 time: 0.06s
Epoch 826/1000, LR 0.000020
Train loss: 0.5155;  Loss pred: 0.5155; Loss self: 0.0000; time: 0.20s
Val loss: 0.0394 score: 1.0000 time: 0.07s
Test loss: 0.2398 score: 0.9184 time: 0.07s
Epoch 827/1000, LR 0.000020
Train loss: 0.5146;  Loss pred: 0.5146; Loss self: 0.0000; time: 0.21s
Val loss: 0.0394 score: 1.0000 time: 0.07s
Test loss: 0.2398 score: 0.9184 time: 0.06s
Epoch 828/1000, LR 0.000020
Train loss: 0.5144;  Loss pred: 0.5144; Loss self: 0.0000; time: 0.21s
Val loss: 0.0394 score: 1.0000 time: 0.07s
Test loss: 0.2398 score: 0.9184 time: 0.06s
Epoch 829/1000, LR 0.000020
Train loss: 0.5176;  Loss pred: 0.5176; Loss self: 0.0000; time: 0.21s
Val loss: 0.0394 score: 1.0000 time: 0.08s
Test loss: 0.2398 score: 0.9184 time: 0.07s
Epoch 830/1000, LR 0.000019
Train loss: 0.5152;  Loss pred: 0.5152; Loss self: 0.0000; time: 0.22s
Val loss: 0.0394 score: 1.0000 time: 0.08s
Test loss: 0.2398 score: 0.9184 time: 0.07s
Epoch 831/1000, LR 0.000019
Train loss: 0.5163;  Loss pred: 0.5163; Loss self: 0.0000; time: 0.21s
Val loss: 0.0394 score: 1.0000 time: 0.07s
Test loss: 0.2398 score: 0.9184 time: 0.07s
Epoch 832/1000, LR 0.000019
Train loss: 0.5144;  Loss pred: 0.5144; Loss self: 0.0000; time: 0.22s
Val loss: 0.0394 score: 1.0000 time: 0.08s
Test loss: 0.2398 score: 0.9184 time: 0.07s
Epoch 833/1000, LR 0.000019
Train loss: 0.5166;  Loss pred: 0.5166; Loss self: 0.0000; time: 0.22s
Val loss: 0.0394 score: 1.0000 time: 1.62s
Test loss: 0.2398 score: 0.9184 time: 4.50s
Epoch 834/1000, LR 0.000019
Train loss: 0.5150;  Loss pred: 0.5150; Loss self: 0.0000; time: 3.90s
Val loss: 0.0394 score: 1.0000 time: 0.08s
Test loss: 0.2398 score: 0.9184 time: 0.07s
Epoch 835/1000, LR 0.000018
Train loss: 0.5162;  Loss pred: 0.5162; Loss self: 0.0000; time: 0.21s
Val loss: 0.0394 score: 1.0000 time: 0.07s
Test loss: 0.2398 score: 0.9184 time: 0.07s
Epoch 836/1000, LR 0.000018
Train loss: 0.5152;  Loss pred: 0.5152; Loss self: 0.0000; time: 0.27s
Val loss: 0.0393 score: 1.0000 time: 0.08s
Test loss: 0.2398 score: 0.9184 time: 0.07s
Epoch 837/1000, LR 0.000018
Train loss: 0.5175;  Loss pred: 0.5175; Loss self: 0.0000; time: 0.22s
Val loss: 0.0393 score: 1.0000 time: 0.07s
Test loss: 0.2398 score: 0.9184 time: 0.07s
Epoch 838/1000, LR 0.000018
Train loss: 0.5153;  Loss pred: 0.5153; Loss self: 0.0000; time: 0.21s
Val loss: 0.0393 score: 1.0000 time: 0.08s
Test loss: 0.2398 score: 0.9184 time: 0.07s
Epoch 839/1000, LR 0.000017
Train loss: 0.5142;  Loss pred: 0.5142; Loss self: 0.0000; time: 0.22s
Val loss: 0.0393 score: 1.0000 time: 0.09s
Test loss: 0.2398 score: 0.9184 time: 0.07s
Epoch 840/1000, LR 0.000017
Train loss: 0.5153;  Loss pred: 0.5153; Loss self: 0.0000; time: 0.22s
Val loss: 0.0393 score: 1.0000 time: 0.08s
Test loss: 0.2398 score: 0.9184 time: 0.06s
Epoch 841/1000, LR 0.000017
Train loss: 0.5155;  Loss pred: 0.5155; Loss self: 0.0000; time: 0.32s
Val loss: 0.0393 score: 1.0000 time: 0.08s
Test loss: 0.2399 score: 0.9184 time: 0.07s
Epoch 842/1000, LR 0.000017
Train loss: 0.5164;  Loss pred: 0.5164; Loss self: 0.0000; time: 0.22s
Val loss: 0.0393 score: 1.0000 time: 0.21s
Test loss: 0.2399 score: 0.9184 time: 2.33s
Epoch 843/1000, LR 0.000017
Train loss: 0.5162;  Loss pred: 0.5162; Loss self: 0.0000; time: 3.77s
Val loss: 0.0393 score: 1.0000 time: 0.13s
Test loss: 0.2399 score: 0.9184 time: 0.11s
Epoch 844/1000, LR 0.000016
Train loss: 0.5144;  Loss pred: 0.5144; Loss self: 0.0000; time: 0.41s
Val loss: 0.0393 score: 1.0000 time: 0.08s
Test loss: 0.2399 score: 0.9184 time: 0.07s
Epoch 845/1000, LR 0.000016
Train loss: 0.5154;  Loss pred: 0.5154; Loss self: 0.0000; time: 0.22s
Val loss: 0.0393 score: 1.0000 time: 0.08s
Test loss: 0.2399 score: 0.9184 time: 0.07s
Epoch 846/1000, LR 0.000016
Train loss: 0.5147;  Loss pred: 0.5147; Loss self: 0.0000; time: 0.22s
Val loss: 0.0393 score: 1.0000 time: 0.09s
Test loss: 0.2399 score: 0.9184 time: 0.07s
Epoch 847/1000, LR 0.000016
Train loss: 0.5160;  Loss pred: 0.5160; Loss self: 0.0000; time: 0.21s
Val loss: 0.0393 score: 1.0000 time: 0.08s
Test loss: 0.2399 score: 0.9184 time: 0.09s
Epoch 848/1000, LR 0.000016
Train loss: 0.5159;  Loss pred: 0.5159; Loss self: 0.0000; time: 0.21s
Val loss: 0.0393 score: 1.0000 time: 0.07s
Test loss: 0.2399 score: 0.9184 time: 0.07s
Epoch 849/1000, LR 0.000015
Train loss: 0.5149;  Loss pred: 0.5149; Loss self: 0.0000; time: 0.21s
Val loss: 0.0393 score: 1.0000 time: 0.07s
Test loss: 0.2399 score: 0.9184 time: 0.07s
Epoch 850/1000, LR 0.000015
Train loss: 0.5156;  Loss pred: 0.5156; Loss self: 0.0000; time: 0.21s
Val loss: 0.0393 score: 1.0000 time: 0.07s
Test loss: 0.2400 score: 0.9184 time: 0.06s
Epoch 851/1000, LR 0.000015
Train loss: 0.5169;  Loss pred: 0.5169; Loss self: 0.0000; time: 0.23s
Val loss: 0.0393 score: 1.0000 time: 0.09s
Test loss: 0.2400 score: 0.9184 time: 0.09s
Epoch 852/1000, LR 0.000015
Train loss: 0.5144;  Loss pred: 0.5144; Loss self: 0.0000; time: 0.23s
Val loss: 0.0393 score: 1.0000 time: 0.07s
Test loss: 0.2400 score: 0.9184 time: 0.06s
Epoch 853/1000, LR 0.000015
Train loss: 0.5152;  Loss pred: 0.5152; Loss self: 0.0000; time: 0.21s
Val loss: 0.0393 score: 1.0000 time: 0.08s
Test loss: 0.2400 score: 0.9184 time: 0.06s
Epoch 854/1000, LR 0.000014
Train loss: 0.5145;  Loss pred: 0.5145; Loss self: 0.0000; time: 0.20s
Val loss: 0.0393 score: 1.0000 time: 0.07s
Test loss: 0.2401 score: 0.9184 time: 0.38s
Epoch 855/1000, LR 0.000014
Train loss: 0.5136;  Loss pred: 0.5136; Loss self: 0.0000; time: 2.63s
Val loss: 0.0393 score: 1.0000 time: 0.34s
Test loss: 0.2401 score: 0.9184 time: 0.58s
Epoch 856/1000, LR 0.000014
Train loss: 0.5154;  Loss pred: 0.5154; Loss self: 0.0000; time: 2.51s
Val loss: 0.0392 score: 1.0000 time: 0.63s
Test loss: 0.2401 score: 0.9184 time: 0.44s
Epoch 857/1000, LR 0.000014
Train loss: 0.5176;  Loss pred: 0.5176; Loss self: 0.0000; time: 2.68s
Val loss: 0.0392 score: 1.0000 time: 0.08s
Test loss: 0.2401 score: 0.9184 time: 0.06s
Epoch 858/1000, LR 0.000014
Train loss: 0.5159;  Loss pred: 0.5159; Loss self: 0.0000; time: 0.20s
Val loss: 0.0392 score: 1.0000 time: 0.07s
Test loss: 0.2401 score: 0.9184 time: 0.06s
Epoch 859/1000, LR 0.000013
Train loss: 0.5154;  Loss pred: 0.5154; Loss self: 0.0000; time: 0.20s
Val loss: 0.0392 score: 1.0000 time: 0.07s
Test loss: 0.2401 score: 0.9184 time: 0.06s
Epoch 860/1000, LR 0.000013
Train loss: 0.5141;  Loss pred: 0.5141; Loss self: 0.0000; time: 0.20s
Val loss: 0.0392 score: 1.0000 time: 0.08s
Test loss: 0.2401 score: 0.9184 time: 0.06s
Epoch 861/1000, LR 0.000013
Train loss: 0.5148;  Loss pred: 0.5148; Loss self: 0.0000; time: 0.20s
Val loss: 0.0392 score: 1.0000 time: 0.08s
Test loss: 0.2401 score: 0.9184 time: 0.06s
Epoch 862/1000, LR 0.000013
Train loss: 0.5160;  Loss pred: 0.5160; Loss self: 0.0000; time: 0.20s
Val loss: 0.0392 score: 1.0000 time: 0.07s
Test loss: 0.2401 score: 0.9184 time: 0.06s
Epoch 863/1000, LR 0.000013
Train loss: 0.5153;  Loss pred: 0.5153; Loss self: 0.0000; time: 0.20s
Val loss: 0.0392 score: 1.0000 time: 0.08s
Test loss: 0.2401 score: 0.9184 time: 0.06s
Epoch 864/1000, LR 0.000013
Train loss: 0.5157;  Loss pred: 0.5157; Loss self: 0.0000; time: 0.20s
Val loss: 0.0392 score: 1.0000 time: 0.07s
Test loss: 0.2401 score: 0.9184 time: 0.06s
Epoch 865/1000, LR 0.000012
Train loss: 0.5168;  Loss pred: 0.5168; Loss self: 0.0000; time: 0.20s
Val loss: 0.0392 score: 1.0000 time: 0.07s
Test loss: 0.2401 score: 0.9184 time: 0.06s
Epoch 866/1000, LR 0.000012
Train loss: 0.5168;  Loss pred: 0.5168; Loss self: 0.0000; time: 0.21s
Val loss: 0.0392 score: 1.0000 time: 0.07s
Test loss: 0.2400 score: 0.9184 time: 0.06s
Epoch 867/1000, LR 0.000012
Train loss: 0.5155;  Loss pred: 0.5155; Loss self: 0.0000; time: 0.20s
Val loss: 0.0392 score: 1.0000 time: 0.08s
Test loss: 0.2400 score: 0.9184 time: 0.06s
Epoch 868/1000, LR 0.000012
Train loss: 0.5151;  Loss pred: 0.5151; Loss self: 0.0000; time: 0.20s
Val loss: 0.0392 score: 1.0000 time: 0.08s
Test loss: 0.2400 score: 0.9184 time: 0.06s
Epoch 869/1000, LR 0.000012
Train loss: 0.5149;  Loss pred: 0.5149; Loss self: 0.0000; time: 0.20s
Val loss: 0.0391 score: 1.0000 time: 0.07s
Test loss: 0.2400 score: 0.9184 time: 0.06s
Epoch 870/1000, LR 0.000011
Train loss: 0.5146;  Loss pred: 0.5146; Loss self: 0.0000; time: 0.20s
Val loss: 0.0391 score: 1.0000 time: 0.08s
Test loss: 0.2400 score: 0.9184 time: 0.06s
Epoch 871/1000, LR 0.000011
Train loss: 0.5148;  Loss pred: 0.5148; Loss self: 0.0000; time: 0.20s
Val loss: 0.0391 score: 1.0000 time: 0.07s
Test loss: 0.2400 score: 0.9184 time: 0.07s
Epoch 872/1000, LR 0.000011
Train loss: 0.5164;  Loss pred: 0.5164; Loss self: 0.0000; time: 0.20s
Val loss: 0.0391 score: 1.0000 time: 0.08s
Test loss: 0.2400 score: 0.9184 time: 0.06s
Epoch 873/1000, LR 0.000011
Train loss: 0.5169;  Loss pred: 0.5169; Loss self: 0.0000; time: 0.20s
Val loss: 0.0391 score: 1.0000 time: 0.07s
Test loss: 0.2399 score: 0.9184 time: 0.06s
Epoch 874/1000, LR 0.000011
Train loss: 0.5142;  Loss pred: 0.5142; Loss self: 0.0000; time: 0.20s
Val loss: 0.0391 score: 1.0000 time: 0.08s
Test loss: 0.2399 score: 0.9184 time: 0.06s
Epoch 875/1000, LR 0.000011
Train loss: 0.5139;  Loss pred: 0.5139; Loss self: 0.0000; time: 0.20s
Val loss: 0.0391 score: 1.0000 time: 0.08s
Test loss: 0.2399 score: 0.9184 time: 0.06s
Epoch 876/1000, LR 0.000010
Train loss: 0.5172;  Loss pred: 0.5172; Loss self: 0.0000; time: 0.20s
Val loss: 0.0391 score: 1.0000 time: 0.07s
Test loss: 0.2399 score: 0.9184 time: 0.06s
Epoch 877/1000, LR 0.000010
Train loss: 0.5145;  Loss pred: 0.5145; Loss self: 0.0000; time: 0.24s
Val loss: 0.0391 score: 1.0000 time: 0.88s
Test loss: 0.2399 score: 0.9184 time: 0.66s
Epoch 878/1000, LR 0.000010
Train loss: 0.5157;  Loss pred: 0.5157; Loss self: 0.0000; time: 0.27s
Val loss: 0.0391 score: 1.0000 time: 0.08s
Test loss: 0.2399 score: 0.9184 time: 0.06s
Epoch 879/1000, LR 0.000010
Train loss: 0.5157;  Loss pred: 0.5157; Loss self: 0.0000; time: 0.21s
Val loss: 0.0391 score: 1.0000 time: 0.07s
Test loss: 0.2399 score: 0.9184 time: 0.07s
Epoch 880/1000, LR 0.000010
Train loss: 0.5157;  Loss pred: 0.5157; Loss self: 0.0000; time: 0.21s
Val loss: 0.0391 score: 1.0000 time: 0.07s
Test loss: 0.2399 score: 0.9184 time: 0.07s
Epoch 881/1000, LR 0.000010
Train loss: 0.5142;  Loss pred: 0.5142; Loss self: 0.0000; time: 0.21s
Val loss: 0.0391 score: 1.0000 time: 0.07s
Test loss: 0.2398 score: 0.9184 time: 0.06s
Epoch 882/1000, LR 0.000010
Train loss: 0.5151;  Loss pred: 0.5151; Loss self: 0.0000; time: 0.21s
Val loss: 0.0391 score: 1.0000 time: 0.07s
Test loss: 0.2398 score: 0.9184 time: 0.06s
Epoch 883/1000, LR 0.000009
Train loss: 0.5147;  Loss pred: 0.5147; Loss self: 0.0000; time: 0.21s
Val loss: 0.0391 score: 1.0000 time: 0.07s
Test loss: 0.2399 score: 0.9184 time: 0.07s
Epoch 884/1000, LR 0.000009
Train loss: 0.5152;  Loss pred: 0.5152; Loss self: 0.0000; time: 0.21s
Val loss: 0.0391 score: 1.0000 time: 0.07s
Test loss: 0.2399 score: 0.9184 time: 0.07s
Epoch 885/1000, LR 0.000009
Train loss: 0.5149;  Loss pred: 0.5149; Loss self: 0.0000; time: 0.21s
Val loss: 0.0390 score: 1.0000 time: 0.08s
Test loss: 0.2398 score: 0.9184 time: 0.06s
Epoch 886/1000, LR 0.000009
Train loss: 0.5152;  Loss pred: 0.5152; Loss self: 0.0000; time: 0.21s
Val loss: 0.0390 score: 1.0000 time: 0.08s
Test loss: 0.2398 score: 0.9184 time: 0.06s
Epoch 887/1000, LR 0.000009
Train loss: 0.5143;  Loss pred: 0.5143; Loss self: 0.0000; time: 0.21s
Val loss: 0.0390 score: 1.0000 time: 0.08s
Test loss: 0.2398 score: 0.9184 time: 0.07s
Epoch 888/1000, LR 0.000009
Train loss: 0.5147;  Loss pred: 0.5147; Loss self: 0.0000; time: 0.21s
Val loss: 0.0390 score: 1.0000 time: 0.08s
Test loss: 0.2398 score: 0.9184 time: 0.06s
Epoch 889/1000, LR 0.000008
Train loss: 0.5147;  Loss pred: 0.5147; Loss self: 0.0000; time: 0.21s
Val loss: 0.0390 score: 1.0000 time: 0.07s
Test loss: 0.2398 score: 0.9184 time: 0.06s
Epoch 890/1000, LR 0.000008
Train loss: 0.5139;  Loss pred: 0.5139; Loss self: 0.0000; time: 1.42s
Val loss: 0.0390 score: 1.0000 time: 0.42s
Test loss: 0.2398 score: 0.9184 time: 1.28s
Epoch 891/1000, LR 0.000008
Train loss: 0.5174;  Loss pred: 0.5174; Loss self: 0.0000; time: 2.07s
Val loss: 0.0390 score: 1.0000 time: 0.48s
Test loss: 0.2398 score: 0.9184 time: 0.42s
Epoch 892/1000, LR 0.000008
Train loss: 0.5141;  Loss pred: 0.5141; Loss self: 0.0000; time: 1.29s
Val loss: 0.0390 score: 1.0000 time: 0.08s
Test loss: 0.2398 score: 0.9184 time: 0.07s
Epoch 893/1000, LR 0.000008
Train loss: 0.5152;  Loss pred: 0.5152; Loss self: 0.0000; time: 0.21s
Val loss: 0.0390 score: 1.0000 time: 0.07s
Test loss: 0.2398 score: 0.9184 time: 0.07s
Epoch 894/1000, LR 0.000008
Train loss: 0.5138;  Loss pred: 0.5138; Loss self: 0.0000; time: 0.21s
Val loss: 0.0390 score: 1.0000 time: 0.08s
Test loss: 0.2398 score: 0.9184 time: 0.06s
Epoch 895/1000, LR 0.000008
Train loss: 0.5150;  Loss pred: 0.5150; Loss self: 0.0000; time: 0.21s
Val loss: 0.0390 score: 1.0000 time: 0.07s
Test loss: 0.2398 score: 0.9184 time: 0.06s
Epoch 896/1000, LR 0.000007
Train loss: 0.5158;  Loss pred: 0.5158; Loss self: 0.0000; time: 0.20s
Val loss: 0.0390 score: 1.0000 time: 0.07s
Test loss: 0.2398 score: 0.9184 time: 0.06s
Epoch 897/1000, LR 0.000007
Train loss: 0.5144;  Loss pred: 0.5144; Loss self: 0.0000; time: 0.20s
Val loss: 0.0390 score: 1.0000 time: 0.08s
Test loss: 0.2398 score: 0.9184 time: 0.06s
Epoch 898/1000, LR 0.000007
Train loss: 0.5153;  Loss pred: 0.5153; Loss self: 0.0000; time: 0.20s
Val loss: 0.0390 score: 1.0000 time: 0.07s
Test loss: 0.2398 score: 0.9184 time: 0.06s
Epoch 899/1000, LR 0.000007
Train loss: 0.5153;  Loss pred: 0.5153; Loss self: 0.0000; time: 0.20s
Val loss: 0.0390 score: 1.0000 time: 0.07s
Test loss: 0.2398 score: 0.9184 time: 0.06s
Epoch 900/1000, LR 0.000007
Train loss: 0.5162;  Loss pred: 0.5162; Loss self: 0.0000; time: 0.20s
Val loss: 0.0390 score: 1.0000 time: 0.07s
Test loss: 0.2398 score: 0.9184 time: 0.06s
Epoch 901/1000, LR 0.000007
Train loss: 0.5156;  Loss pred: 0.5156; Loss self: 0.0000; time: 0.20s
Val loss: 0.0390 score: 1.0000 time: 0.07s
Test loss: 0.2398 score: 0.9184 time: 0.06s
Epoch 902/1000, LR 0.000007
Train loss: 0.5161;  Loss pred: 0.5161; Loss self: 0.0000; time: 0.20s
Val loss: 0.0390 score: 1.0000 time: 0.07s
Test loss: 0.2398 score: 0.9184 time: 0.07s
Epoch 903/1000, LR 0.000006
Train loss: 0.5169;  Loss pred: 0.5169; Loss self: 0.0000; time: 0.20s
Val loss: 0.0390 score: 1.0000 time: 0.07s
Test loss: 0.2398 score: 0.9184 time: 0.06s
Epoch 904/1000, LR 0.000006
Train loss: 0.5143;  Loss pred: 0.5143; Loss self: 0.0000; time: 0.20s
Val loss: 0.0390 score: 1.0000 time: 0.07s
Test loss: 0.2398 score: 0.9184 time: 0.06s
Epoch 905/1000, LR 0.000006
Train loss: 0.5153;  Loss pred: 0.5153; Loss self: 0.0000; time: 0.20s
Val loss: 0.0390 score: 1.0000 time: 0.07s
Test loss: 0.2398 score: 0.9184 time: 0.06s
Epoch 906/1000, LR 0.000006
Train loss: 0.5158;  Loss pred: 0.5158; Loss self: 0.0000; time: 0.20s
Val loss: 0.0390 score: 1.0000 time: 0.07s
Test loss: 0.2399 score: 0.9184 time: 0.06s
Epoch 907/1000, LR 0.000006
Train loss: 0.5151;  Loss pred: 0.5151; Loss self: 0.0000; time: 0.20s
Val loss: 0.0390 score: 1.0000 time: 0.07s
Test loss: 0.2399 score: 0.9184 time: 0.06s
Epoch 908/1000, LR 0.000006
Train loss: 0.5154;  Loss pred: 0.5154; Loss self: 0.0000; time: 0.20s
Val loss: 0.0390 score: 1.0000 time: 0.07s
Test loss: 0.2398 score: 0.9184 time: 0.06s
Epoch 909/1000, LR 0.000006
Train loss: 0.5135;  Loss pred: 0.5135; Loss self: 0.0000; time: 0.20s
Val loss: 0.0390 score: 1.0000 time: 0.07s
Test loss: 0.2398 score: 0.9184 time: 0.06s
Epoch 910/1000, LR 0.000006
Train loss: 0.5178;  Loss pred: 0.5178; Loss self: 0.0000; time: 0.20s
Val loss: 0.0390 score: 1.0000 time: 0.07s
Test loss: 0.2398 score: 0.9184 time: 0.06s
Epoch 911/1000, LR 0.000005
Train loss: 0.5147;  Loss pred: 0.5147; Loss self: 0.0000; time: 0.20s
Val loss: 0.0390 score: 1.0000 time: 0.07s
Test loss: 0.2398 score: 0.9184 time: 0.06s
Epoch 912/1000, LR 0.000005
Train loss: 0.5160;  Loss pred: 0.5160; Loss self: 0.0000; time: 0.20s
Val loss: 0.0389 score: 1.0000 time: 0.07s
Test loss: 0.2398 score: 0.9184 time: 0.06s
Epoch 913/1000, LR 0.000005
Train loss: 0.5142;  Loss pred: 0.5142; Loss self: 0.0000; time: 0.20s
Val loss: 0.0389 score: 1.0000 time: 0.07s
Test loss: 0.2398 score: 0.9184 time: 0.06s
Epoch 914/1000, LR 0.000005
Train loss: 0.5163;  Loss pred: 0.5163; Loss self: 0.0000; time: 0.20s
Val loss: 0.0389 score: 1.0000 time: 0.07s
Test loss: 0.2398 score: 0.9184 time: 0.06s
Epoch 915/1000, LR 0.000005
Train loss: 0.5169;  Loss pred: 0.5169; Loss self: 0.0000; time: 0.19s
Val loss: 0.0389 score: 1.0000 time: 0.07s
Test loss: 0.2398 score: 0.9184 time: 0.06s
Epoch 916/1000, LR 0.000005
Train loss: 0.5162;  Loss pred: 0.5162; Loss self: 0.0000; time: 0.20s
Val loss: 0.0389 score: 1.0000 time: 0.07s
Test loss: 0.2398 score: 0.9184 time: 0.06s
Epoch 917/1000, LR 0.000005
Train loss: 0.5143;  Loss pred: 0.5143; Loss self: 0.0000; time: 0.21s
Val loss: 0.0389 score: 1.0000 time: 0.07s
Test loss: 0.2398 score: 0.9184 time: 0.06s
Epoch 918/1000, LR 0.000005
Train loss: 0.5154;  Loss pred: 0.5154; Loss self: 0.0000; time: 0.20s
Val loss: 0.0389 score: 1.0000 time: 0.06s
Test loss: 0.2398 score: 0.9184 time: 0.06s
Epoch 919/1000, LR 0.000005
Train loss: 0.5165;  Loss pred: 0.5165; Loss self: 0.0000; time: 0.20s
Val loss: 0.0389 score: 1.0000 time: 0.07s
Test loss: 0.2398 score: 0.9184 time: 0.06s
Epoch 920/1000, LR 0.000004
Train loss: 0.5146;  Loss pred: 0.5146; Loss self: 0.0000; time: 0.20s
Val loss: 0.0389 score: 1.0000 time: 0.07s
Test loss: 0.2398 score: 0.9184 time: 0.06s
Epoch 921/1000, LR 0.000004
Train loss: 0.5143;  Loss pred: 0.5143; Loss self: 0.0000; time: 0.20s
Val loss: 0.0389 score: 1.0000 time: 0.07s
Test loss: 0.2398 score: 0.9184 time: 0.06s
Epoch 922/1000, LR 0.000004
Train loss: 0.5147;  Loss pred: 0.5147; Loss self: 0.0000; time: 0.20s
Val loss: 0.0389 score: 1.0000 time: 0.07s
Test loss: 0.2398 score: 0.9184 time: 0.06s
Epoch 923/1000, LR 0.000004
Train loss: 0.5155;  Loss pred: 0.5155; Loss self: 0.0000; time: 0.20s
Val loss: 0.0389 score: 1.0000 time: 0.07s
Test loss: 0.2398 score: 0.9184 time: 0.06s
Epoch 924/1000, LR 0.000004
Train loss: 0.5149;  Loss pred: 0.5149; Loss self: 0.0000; time: 0.20s
Val loss: 0.0389 score: 1.0000 time: 0.07s
Test loss: 0.2398 score: 0.9184 time: 0.06s
Epoch 925/1000, LR 0.000004
Train loss: 0.5157;  Loss pred: 0.5157; Loss self: 0.0000; time: 0.20s
Val loss: 0.0389 score: 1.0000 time: 0.07s
Test loss: 0.2398 score: 0.9184 time: 0.06s
Epoch 926/1000, LR 0.000004
Train loss: 0.5159;  Loss pred: 0.5159; Loss self: 0.0000; time: 0.20s
Val loss: 0.0389 score: 1.0000 time: 0.07s
Test loss: 0.2398 score: 0.9184 time: 0.06s
Epoch 927/1000, LR 0.000004
Train loss: 0.5142;  Loss pred: 0.5142; Loss self: 0.0000; time: 0.20s
Val loss: 0.0389 score: 1.0000 time: 0.07s
Test loss: 0.2398 score: 0.9184 time: 0.06s
Epoch 928/1000, LR 0.000004
Train loss: 0.5146;  Loss pred: 0.5146; Loss self: 0.0000; time: 0.20s
Val loss: 0.0389 score: 1.0000 time: 0.07s
Test loss: 0.2398 score: 0.9184 time: 0.06s
Epoch 929/1000, LR 0.000004
Train loss: 0.5145;  Loss pred: 0.5145; Loss self: 0.0000; time: 0.20s
Val loss: 0.0389 score: 1.0000 time: 0.07s
Test loss: 0.2398 score: 0.9184 time: 0.06s
Epoch 930/1000, LR 0.000003
Train loss: 0.5146;  Loss pred: 0.5146; Loss self: 0.0000; time: 0.20s
Val loss: 0.0389 score: 1.0000 time: 0.07s
Test loss: 0.2398 score: 0.9184 time: 0.06s
Epoch 931/1000, LR 0.000003
Train loss: 0.5145;  Loss pred: 0.5145; Loss self: 0.0000; time: 0.20s
Val loss: 0.0389 score: 1.0000 time: 0.07s
Test loss: 0.2398 score: 0.9184 time: 0.06s
Epoch 932/1000, LR 0.000003
Train loss: 0.5142;  Loss pred: 0.5142; Loss self: 0.0000; time: 0.20s
Val loss: 0.0389 score: 1.0000 time: 0.07s
Test loss: 0.2398 score: 0.9184 time: 0.06s
Epoch 933/1000, LR 0.000003
Train loss: 0.5158;  Loss pred: 0.5158; Loss self: 0.0000; time: 0.20s
Val loss: 0.0389 score: 1.0000 time: 0.07s
Test loss: 0.2398 score: 0.9184 time: 0.15s
Epoch 934/1000, LR 0.000003
Train loss: 0.5142;  Loss pred: 0.5142; Loss self: 0.0000; time: 0.19s
Val loss: 0.0389 score: 1.0000 time: 0.07s
Test loss: 0.2398 score: 0.9184 time: 0.06s
Epoch 935/1000, LR 0.000003
Train loss: 0.5154;  Loss pred: 0.5154; Loss self: 0.0000; time: 0.20s
Val loss: 0.0389 score: 1.0000 time: 0.06s
Test loss: 0.2398 score: 0.9184 time: 0.06s
Epoch 936/1000, LR 0.000003
Train loss: 0.5167;  Loss pred: 0.5167; Loss self: 0.0000; time: 0.20s
Val loss: 0.0389 score: 1.0000 time: 0.07s
Test loss: 0.2398 score: 0.9184 time: 0.07s
Epoch 937/1000, LR 0.000003
Train loss: 0.5154;  Loss pred: 0.5154; Loss self: 0.0000; time: 0.20s
Val loss: 0.0389 score: 1.0000 time: 0.07s
Test loss: 0.2398 score: 0.9184 time: 0.06s
Epoch 938/1000, LR 0.000003
Train loss: 0.5152;  Loss pred: 0.5152; Loss self: 0.0000; time: 0.22s
Val loss: 0.0389 score: 1.0000 time: 0.09s
Test loss: 0.2398 score: 0.9184 time: 0.06s
Epoch 939/1000, LR 0.000003
Train loss: 0.5140;  Loss pred: 0.5140; Loss self: 0.0000; time: 0.21s
Val loss: 0.0389 score: 1.0000 time: 0.07s
Test loss: 0.2398 score: 0.9184 time: 0.07s
Epoch 940/1000, LR 0.000003
Train loss: 0.5157;  Loss pred: 0.5157; Loss self: 0.0000; time: 0.21s
Val loss: 0.0389 score: 1.0000 time: 0.07s
Test loss: 0.2398 score: 0.9184 time: 0.06s
Epoch 941/1000, LR 0.000002
Train loss: 0.5138;  Loss pred: 0.5138; Loss self: 0.0000; time: 0.21s
Val loss: 0.0389 score: 1.0000 time: 0.07s
Test loss: 0.2398 score: 0.9184 time: 0.06s
Epoch 942/1000, LR 0.000002
Train loss: 0.5134;  Loss pred: 0.5134; Loss self: 0.0000; time: 0.22s
Val loss: 0.0389 score: 1.0000 time: 0.16s
Test loss: 0.2398 score: 0.9184 time: 0.06s
Epoch 943/1000, LR 0.000002
Train loss: 0.5153;  Loss pred: 0.5153; Loss self: 0.0000; time: 0.21s
Val loss: 0.0389 score: 1.0000 time: 0.08s
Test loss: 0.2398 score: 0.9184 time: 0.06s
Epoch 944/1000, LR 0.000002
Train loss: 0.5155;  Loss pred: 0.5155; Loss self: 0.0000; time: 0.21s
Val loss: 0.0389 score: 1.0000 time: 0.08s
Test loss: 0.2398 score: 0.9184 time: 0.06s
Epoch 945/1000, LR 0.000002
Train loss: 0.5158;  Loss pred: 0.5158; Loss self: 0.0000; time: 0.21s
Val loss: 0.0389 score: 1.0000 time: 0.08s
Test loss: 0.2398 score: 0.9184 time: 0.06s
Epoch 946/1000, LR 0.000002
Train loss: 0.5166;  Loss pred: 0.5166; Loss self: 0.0000; time: 0.21s
Val loss: 0.0389 score: 1.0000 time: 0.08s
Test loss: 0.2398 score: 0.9184 time: 0.14s
Epoch 947/1000, LR 0.000002
Train loss: 0.5154;  Loss pred: 0.5154; Loss self: 0.0000; time: 0.20s
Val loss: 0.0389 score: 1.0000 time: 0.06s
Test loss: 0.2398 score: 0.9184 time: 0.06s
Epoch 948/1000, LR 0.000002
Train loss: 0.5170;  Loss pred: 0.5170; Loss self: 0.0000; time: 0.20s
Val loss: 0.0389 score: 1.0000 time: 0.07s
Test loss: 0.2398 score: 0.9184 time: 0.06s
Epoch 949/1000, LR 0.000002
Train loss: 0.5153;  Loss pred: 0.5153; Loss self: 0.0000; time: 0.20s
Val loss: 0.0389 score: 1.0000 time: 0.07s
Test loss: 0.2398 score: 0.9184 time: 0.06s
Epoch 950/1000, LR 0.000002
Train loss: 0.5151;  Loss pred: 0.5151; Loss self: 0.0000; time: 0.20s
Val loss: 0.0389 score: 1.0000 time: 0.07s
Test loss: 0.2398 score: 0.9184 time: 0.06s
Epoch 951/1000, LR 0.000002
Train loss: 0.5156;  Loss pred: 0.5156; Loss self: 0.0000; time: 0.20s
Val loss: 0.0389 score: 1.0000 time: 0.07s
Test loss: 0.2398 score: 0.9184 time: 0.06s
Epoch 952/1000, LR 0.000002
Train loss: 0.5163;  Loss pred: 0.5163; Loss self: 0.0000; time: 0.19s
Val loss: 0.0389 score: 1.0000 time: 0.07s
Test loss: 0.2398 score: 0.9184 time: 0.06s
Epoch 953/1000, LR 0.000002
Train loss: 0.5142;  Loss pred: 0.5142; Loss self: 0.0000; time: 0.20s
Val loss: 0.0389 score: 1.0000 time: 0.07s
Test loss: 0.2398 score: 0.9184 time: 0.06s
Epoch 954/1000, LR 0.000001
Train loss: 0.5149;  Loss pred: 0.5149; Loss self: 0.0000; time: 0.20s
Val loss: 0.0389 score: 1.0000 time: 0.07s
Test loss: 0.2398 score: 0.9184 time: 0.06s
Epoch 955/1000, LR 0.000001
Train loss: 0.5157;  Loss pred: 0.5157; Loss self: 0.0000; time: 0.20s
Val loss: 0.0389 score: 1.0000 time: 0.07s
Test loss: 0.2398 score: 0.9184 time: 0.06s
Epoch 956/1000, LR 0.000001
Train loss: 0.5148;  Loss pred: 0.5148; Loss self: 0.0000; time: 0.20s
Val loss: 0.0389 score: 1.0000 time: 0.07s
Test loss: 0.2398 score: 0.9184 time: 0.06s
Epoch 957/1000, LR 0.000001
Train loss: 0.5159;  Loss pred: 0.5159; Loss self: 0.0000; time: 0.20s
Val loss: 0.0389 score: 1.0000 time: 0.06s
Test loss: 0.2398 score: 0.9184 time: 0.06s
Epoch 958/1000, LR 0.000001
Train loss: 0.5166;  Loss pred: 0.5166; Loss self: 0.0000; time: 0.20s
Val loss: 0.0389 score: 1.0000 time: 0.08s
Test loss: 0.2398 score: 0.9184 time: 0.06s
Epoch 959/1000, LR 0.000001
Train loss: 0.5147;  Loss pred: 0.5147; Loss self: 0.0000; time: 0.21s
Val loss: 0.0389 score: 1.0000 time: 0.07s
Test loss: 0.2398 score: 0.9184 time: 0.07s
Epoch 960/1000, LR 0.000001
Train loss: 0.5157;  Loss pred: 0.5157; Loss self: 0.0000; time: 0.21s
Val loss: 0.0389 score: 1.0000 time: 0.08s
Test loss: 0.2398 score: 0.9184 time: 0.07s
Epoch 961/1000, LR 0.000001
Train loss: 0.5155;  Loss pred: 0.5155; Loss self: 0.0000; time: 0.21s
Val loss: 0.0389 score: 1.0000 time: 0.07s
Test loss: 0.2398 score: 0.9184 time: 0.07s
Epoch 962/1000, LR 0.000001
Train loss: 0.5143;  Loss pred: 0.5143; Loss self: 0.0000; time: 0.21s
Val loss: 0.0389 score: 1.0000 time: 0.07s
Test loss: 0.2398 score: 0.9184 time: 0.07s
Epoch 963/1000, LR 0.000001
Train loss: 0.5156;  Loss pred: 0.5156; Loss self: 0.0000; time: 0.21s
Val loss: 0.0389 score: 1.0000 time: 0.07s
Test loss: 0.2398 score: 0.9184 time: 0.06s
Epoch 964/1000, LR 0.000001
Train loss: 0.5133;  Loss pred: 0.5133; Loss self: 0.0000; time: 0.21s
Val loss: 0.0389 score: 1.0000 time: 0.07s
Test loss: 0.2398 score: 0.9184 time: 0.06s
Epoch 965/1000, LR 0.000001
Train loss: 0.5148;  Loss pred: 0.5148; Loss self: 0.0000; time: 0.21s
Val loss: 0.0389 score: 1.0000 time: 0.07s
Test loss: 0.2398 score: 0.9184 time: 0.07s
Epoch 966/1000, LR 0.000001
Train loss: 0.5160;  Loss pred: 0.5160; Loss self: 0.0000; time: 0.21s
Val loss: 0.0389 score: 1.0000 time: 0.07s
Test loss: 0.2398 score: 0.9184 time: 0.06s
Epoch 967/1000, LR 0.000001
Train loss: 0.5152;  Loss pred: 0.5152; Loss self: 0.0000; time: 0.21s
Val loss: 0.0389 score: 1.0000 time: 0.07s
Test loss: 0.2398 score: 0.9184 time: 0.06s
Epoch 968/1000, LR 0.000001
Train loss: 0.5139;  Loss pred: 0.5139; Loss self: 0.0000; time: 0.21s
Val loss: 0.0389 score: 1.0000 time: 0.07s
Test loss: 0.2398 score: 0.9184 time: 0.06s
Epoch 969/1000, LR 0.000001
Train loss: 0.5151;  Loss pred: 0.5151; Loss self: 0.0000; time: 0.19s
Val loss: 0.0389 score: 1.0000 time: 0.07s
Test loss: 0.2398 score: 0.9184 time: 0.06s
Epoch 970/1000, LR 0.000001
Train loss: 0.5150;  Loss pred: 0.5150; Loss self: 0.0000; time: 0.20s
Val loss: 0.0389 score: 1.0000 time: 0.07s
Test loss: 0.2398 score: 0.9184 time: 0.06s
Epoch 971/1000, LR 0.000001
Train loss: 0.5161;  Loss pred: 0.5161; Loss self: 0.0000; time: 0.21s
Val loss: 0.0389 score: 1.0000 time: 0.07s
Test loss: 0.2398 score: 0.9184 time: 0.06s
Epoch 972/1000, LR 0.000001
Train loss: 0.5151;  Loss pred: 0.5151; Loss self: 0.0000; time: 0.20s
Val loss: 0.0389 score: 1.0000 time: 0.06s
Test loss: 0.2398 score: 0.9184 time: 0.06s
Epoch 973/1000, LR 0.000001
Train loss: 0.5133;  Loss pred: 0.5133; Loss self: 0.0000; time: 0.19s
Val loss: 0.0389 score: 1.0000 time: 0.07s
Test loss: 0.2398 score: 0.9184 time: 0.06s
Epoch 974/1000, LR 0.000000
Train loss: 0.5137;  Loss pred: 0.5137; Loss self: 0.0000; time: 0.19s
Val loss: 0.0389 score: 1.0000 time: 0.07s
Test loss: 0.2398 score: 0.9184 time: 0.06s
Epoch 975/1000, LR 0.000000
Train loss: 0.5177;  Loss pred: 0.5177; Loss self: 0.0000; time: 0.19s
Val loss: 0.0389 score: 1.0000 time: 0.08s
Test loss: 0.2398 score: 0.9184 time: 0.06s
Epoch 976/1000, LR 0.000000
Train loss: 0.5143;  Loss pred: 0.5143; Loss self: 0.0000; time: 0.21s
Val loss: 0.0389 score: 1.0000 time: 0.08s
Test loss: 0.2398 score: 0.9184 time: 0.07s
Epoch 977/1000, LR 0.000000
Train loss: 0.5154;  Loss pred: 0.5154; Loss self: 0.0000; time: 0.21s
Val loss: 0.0389 score: 1.0000 time: 0.07s
Test loss: 0.2398 score: 0.9184 time: 0.06s
Epoch 978/1000, LR 0.000000
Train loss: 0.5149;  Loss pred: 0.5149; Loss self: 0.0000; time: 0.21s
Val loss: 0.0389 score: 1.0000 time: 0.07s
Test loss: 0.2398 score: 0.9184 time: 0.06s
Epoch 979/1000, LR 0.000000
Train loss: 0.5156;  Loss pred: 0.5156; Loss self: 0.0000; time: 0.20s
Val loss: 0.0389 score: 1.0000 time: 0.08s
Test loss: 0.2398 score: 0.9184 time: 0.06s
Epoch 980/1000, LR 0.000000
Train loss: 0.5140;  Loss pred: 0.5140; Loss self: 0.0000; time: 0.62s
Val loss: 0.0389 score: 1.0000 time: 0.51s
Test loss: 0.2398 score: 0.9184 time: 0.64s
Epoch 981/1000, LR 0.000000
Train loss: 0.5148;  Loss pred: 0.5148; Loss self: 0.0000; time: 1.34s
Val loss: 0.0389 score: 1.0000 time: 0.22s
Test loss: 0.2398 score: 0.9184 time: 0.98s
Epoch 982/1000, LR 0.000000
Train loss: 0.5153;  Loss pred: 0.5153; Loss self: 0.0000; time: 5.34s
Val loss: 0.0389 score: 1.0000 time: 0.08s
Test loss: 0.2398 score: 0.9184 time: 0.06s
Epoch 983/1000, LR 0.000000
Train loss: 0.5140;  Loss pred: 0.5140; Loss self: 0.0000; time: 0.21s
Val loss: 0.0389 score: 1.0000 time: 0.07s
Test loss: 0.2398 score: 0.9184 time: 0.07s
Epoch 984/1000, LR 0.000000
Train loss: 0.5134;  Loss pred: 0.5134; Loss self: 0.0000; time: 0.21s
Val loss: 0.0389 score: 1.0000 time: 0.07s
Test loss: 0.2398 score: 0.9184 time: 0.06s
Epoch 985/1000, LR 0.000000
Train loss: 0.5160;  Loss pred: 0.5160; Loss self: 0.0000; time: 0.20s
Val loss: 0.0389 score: 1.0000 time: 0.06s
Test loss: 0.2398 score: 0.9184 time: 0.06s
Epoch 986/1000, LR 0.000000
Train loss: 0.5140;  Loss pred: 0.5140; Loss self: 0.0000; time: 0.19s
Val loss: 0.0389 score: 1.0000 time: 0.07s
Test loss: 0.2398 score: 0.9184 time: 0.06s
Epoch 987/1000, LR 0.000000
Train loss: 0.5147;  Loss pred: 0.5147; Loss self: 0.0000; time: 0.20s
Val loss: 0.0389 score: 1.0000 time: 0.07s
Test loss: 0.2398 score: 0.9184 time: 0.06s
Epoch 988/1000, LR 0.000000
Train loss: 0.5160;  Loss pred: 0.5160; Loss self: 0.0000; time: 0.20s
Val loss: 0.0389 score: 1.0000 time: 0.07s
Test loss: 0.2398 score: 0.9184 time: 0.06s
Epoch 989/1000, LR 0.000000
Train loss: 0.5152;  Loss pred: 0.5152; Loss self: 0.0000; time: 0.20s
Val loss: 0.0389 score: 1.0000 time: 0.06s
Test loss: 0.2398 score: 0.9184 time: 0.06s
Epoch 990/1000, LR 0.000000
Train loss: 0.5161;  Loss pred: 0.5161; Loss self: 0.0000; time: 0.20s
Val loss: 0.0389 score: 1.0000 time: 0.07s
Test loss: 0.2398 score: 0.9184 time: 0.06s
Epoch 991/1000, LR 0.000000
Train loss: 0.5159;  Loss pred: 0.5159; Loss self: 0.0000; time: 0.20s
Val loss: 0.0389 score: 1.0000 time: 0.07s
Test loss: 0.2398 score: 0.9184 time: 0.06s
Epoch 992/1000, LR 0.000000
Train loss: 0.5159;  Loss pred: 0.5159; Loss self: 0.0000; time: 0.20s
Val loss: 0.0389 score: 1.0000 time: 0.07s
Test loss: 0.2398 score: 0.9184 time: 0.06s
Epoch 993/1000, LR 0.000000
Train loss: 0.5156;  Loss pred: 0.5156; Loss self: 0.0000; time: 0.21s
Val loss: 0.0389 score: 1.0000 time: 0.07s
Test loss: 0.2398 score: 0.9184 time: 0.06s
Epoch 994/1000, LR 0.000000
Train loss: 0.5140;  Loss pred: 0.5140; Loss self: 0.0000; time: 0.20s
Val loss: 0.0389 score: 1.0000 time: 0.06s
Test loss: 0.2398 score: 0.9184 time: 0.06s
Epoch 995/1000, LR 0.000000
Train loss: 0.5160;  Loss pred: 0.5160; Loss self: 0.0000; time: 0.19s
Val loss: 0.0389 score: 1.0000 time: 0.07s
Test loss: 0.2398 score: 0.9184 time: 0.06s
Epoch 996/1000, LR 0.000000
Train loss: 0.5155;  Loss pred: 0.5155; Loss self: 0.0000; time: 0.20s
Val loss: 0.0389 score: 1.0000 time: 0.07s
Test loss: 0.2398 score: 0.9184 time: 0.06s
Epoch 997/1000, LR 0.000000
Train loss: 0.5156;  Loss pred: 0.5156; Loss self: 0.0000; time: 0.20s
Val loss: 0.0389 score: 1.0000 time: 0.08s
Test loss: 0.2398 score: 0.9184 time: 0.06s
Epoch 998/1000, LR 0.000000
Train loss: 0.5162;  Loss pred: 0.5162; Loss self: 0.0000; time: 0.21s
Val loss: 0.0389 score: 1.0000 time: 0.08s
Test loss: 0.2398 score: 0.9184 time: 0.06s
Epoch 999/1000, LR 0.000000
Train loss: 0.5152;  Loss pred: 0.5152; Loss self: 0.0000; time: 0.22s
Val loss: 0.0389 score: 1.0000 time: 0.78s
Test loss: 0.2398 score: 0.9184 time: 0.68s
Epoch 1000/1000, LR 0.000000
Train loss: 0.5148;  Loss pred: 0.5148; Loss self: 0.0000; time: 1.50s
Val loss: 0.0389 score: 1.0000 time: 1.42s
Test loss: 0.2398 score: 0.9184 time: 0.89s
[0.06476087204646319, 0.06579116999637336, 0.06670370895881206, 0.06283232697751373, 0.06635016796644777, 0.06713514402508736, 0.06663857901003212, 0.06579748599324375, 0.06196772202383727, 0.062184436013922095, 1.1575231599854305, 0.7153603859478608, 0.4520734220277518, 0.20855456998106092, 0.0624145430047065, 0.06335032999049872, 0.06274904799647629, 0.06183908192906529, 0.06212091201450676, 0.0656258639646694, 0.06278192496392876, 0.061973148956894875, 0.06682089902460575, 0.22932421998120844, 0.06647005898412317, 0.06625901197548956, 0.06943125301040709, 0.06677279609721154, 0.06148970394860953, 0.06119351997040212, 0.06136334897018969, 0.06794143002480268, 0.06500601605512202, 0.06330907205119729, 0.06279758806340396, 0.06461836700327694, 0.06341015791986138, 0.06493650190532207, 0.06442683702334762, 0.06460916704963893, 0.0647593749454245, 0.0685099350521341, 0.06626345205586404, 0.06469235301483423, 0.6948692150181159, 0.3487890580436215, 0.39466211502440274, 0.07342516200151294, 0.06805292889475822, 0.06669866503216326, 0.07165221392642707, 0.06792366900481284, 0.06723513500764966, 0.06716945499647409, 0.08681857003830373, 0.07082555699162185, 0.9292787079466507, 1.2429560100426897, 0.3411137278890237, 0.06832538999151438, 0.0633906819857657, 0.06216920597944409, 0.06540966406464577, 0.06452242191880941, 0.06129719200544059, 0.06532936100848019, 0.06521882896777242, 0.06099008792079985, 0.8595021119108424, 0.44879164604935795, 0.06850228901021183, 0.0673556929687038, 0.06918730994220823, 0.06795746996067464, 0.07283785194158554, 0.07099246501456946, 1.6247506119543687, 0.06786082498729229, 0.06247847503982484, 0.06238862208556384, 0.06309982598759234, 0.06843031500466168, 0.06853935297112912, 0.06705821596551687, 0.06369954999536276, 0.06352299998980016, 0.06738124799448997, 0.06833131902385503, 0.06453858397435397, 0.06475179898552597, 0.06494150590151548, 0.856712986016646, 0.5195994810201228, 0.06789304199628532, 0.06746559799648821, 0.0731841679662466, 0.07599541801027954, 0.07152079197112471, 0.06579671904910356, 0.06625745096243918, 0.06559836503583938, 0.06600432505365461, 0.06585284392349422, 0.06549552700016648, 0.06582786806393415, 0.06580422702245414, 0.06627796601969749, 0.06606625311542302, 0.07201612694188952, 0.06662704993505031, 0.2685609789332375, 0.06897633103653789, 0.07226494699716568, 0.06502013502176851, 0.06194956705439836, 0.06210083398036659, 0.06174241297412664, 0.06228774704504758, 0.06199062999803573, 0.06189611298032105, 0.06258089502807707, 0.06554542703088373, 0.06167076400015503, 0.06221748807001859, 0.06154343893285841, 0.06750979099888355, 0.06695332902017981, 0.07336356001906097, 0.07371507701463997, 0.06722062802873552, 0.06712983595207334, 0.06650756602175534, 0.07445000298321247, 0.06705248705111444, 0.06685109704267234, 0.07282947294879705, 0.06859242601785809, 0.07314315508119762, 0.06692946702241898, 0.06771053606644273, 0.06733830296434462, 0.06739915790967643, 0.06787370308302343, 0.06783171591814607, 0.06763095105998218, 0.06763171299826354, 0.07347791898064315, 0.07389857294037938, 0.06693518394604325, 0.06785635103005916, 0.0740184240275994, 0.06898828910198063, 0.06841460999567062, 0.06727437896188349, 0.07242574007250369, 1.1049873309675604, 0.06973579700570554, 0.0663173709763214, 0.07052694796584547, 0.07254803495015949, 0.06626497500110418, 0.06647055200301111, 0.06632130197249353, 0.06729779299348593, 0.06681169103831053, 0.07345814001746476, 0.06851096707396209, 1.0557224840158597, 0.30492499307729304, 0.42083856696262956, 0.06481902697123587, 0.06499480898492038, 0.07171523000579327, 0.07228195201605558, 0.0643088249489665, 0.06489149294793606, 0.06518906098790467, 0.06471460603643209, 0.06474307202734053, 0.0641459230100736, 0.06441276497207582, 0.06495361309498549, 0.06971751700621098, 0.07118141697719693, 0.07059967005625367, 0.06424430094193667, 0.06453299208078533, 1.544311658013612, 0.06957882700953633, 0.06305972998961806, 0.06983294000383466, 0.0640381519915536, 0.06435789610259235, 0.06794984103180468, 0.07132203399669379, 1.1363897930132225, 0.1172800490166992, 0.07068418001290411, 0.06473798397928476, 0.07024006103165448, 0.06436633097473532, 0.0642209961079061, 0.06350769102573395, 0.06431071006227285, 0.07012145896442235, 0.06431479507591575, 0.06935204192996025, 0.06759858399163932, 0.06839565804693848, 0.0739662959240377, 0.06828439701348543, 0.7392536730039865, 1.2675416379934177, 0.1325124129652977, 0.06736809702124447, 0.07087960303761065, 0.06607222300954163, 0.0651511310134083, 0.06464820692781359, 0.06091385101899505, 0.06124209298286587, 0.5564304799772799, 2.132388131925836, 0.23668571398593485, 0.06667599105276167, 0.06308440503198653, 0.0653558389749378, 0.06502007006201893, 0.06160800904035568, 0.062220701947808266, 0.06635490793269128, 0.06648201995994896, 0.06563782307785004, 0.8506578800734133, 0.4693251089192927, 1.519177146954462, 0.3862044010311365, 0.061377485981211066, 0.06095018296036869, 0.0615432970225811, 0.0601485799998045, 0.059477505972608924, 0.06490450794808567, 0.0602725800126791, 0.06359793804585934, 0.0606649040710181, 0.06484047800768167, 0.0611747830407694, 0.06498901802115142, 1.912888011080213, 0.08741021400783211, 0.0615475740050897, 0.06136196607258171, 0.06173864600714296, 0.061009470955468714, 0.06185999605804682, 0.06252516596578062, 0.06260105897672474, 0.06590831198263913, 0.06394266500137746, 0.06749054393731058, 0.06257951003499329, 0.06289265502709895, 0.06695004005450755, 0.06419650197494775, 0.06298727996181697, 0.06346747290808707, 0.06320392701309174, 0.06292027502786368, 1.6048958000028506, 0.2362813219660893, 0.06785095599479973, 0.06721629993990064, 0.07240299799013883, 0.06757554796058685, 0.06686734198592603, 0.06738421006593853, 0.07057991600595415, 0.07433232304174453, 0.06764360598754138, 0.06783948605880141, 0.07256339699961245, 0.07391184696462005, 0.06844383501447737, 0.06947421608492732, 0.06753231701441109, 0.4220553449122235, 0.06859597004950047, 0.06921478407457471, 0.0646464090095833, 0.06813114194665104, 0.06510418897960335, 0.06532575993333012, 0.06525081698782742, 0.07011349394451827, 0.06548786209896207, 0.06534931890200824, 0.06434668507426977, 0.07230448699556291, 0.06943024904467165, 0.07413355202879757, 0.06864948908332735, 0.16475897992495447, 0.07491439499426633, 0.7072585810674354, 0.5006445340113714, 0.18565852008759975, 0.0676679010502994, 0.06722866697236896, 0.06886849901638925, 0.0668798639671877, 0.06879457598552108, 0.06783791701309383, 0.06819728610571474, 0.06873145198915154, 0.0683390130288899, 0.7279247969854623, 0.1665846329415217, 0.07137009501457214, 0.06616553501226008, 0.06538884702604264, 0.07008704706095159, 0.06557476299349219, 0.07241675991099328, 0.06500819500070065, 0.06504361901897937, 0.06544885702896863, 0.06500213406980038, 0.06600897200405598, 0.06511185294948518, 0.0655770639423281, 0.06698209897149354, 0.06579338503070176, 0.06996298802550882, 1.899643717915751, 0.07226561894640326, 0.06332338193897158, 0.06254529894795269, 0.06327792804222554, 0.06251364096533507, 0.0631484299665317, 0.06287728797178715, 0.06262192700523883, 0.06349792098626494, 0.06439099693670869, 0.06340990611352026, 0.0650928079849109, 0.06341387098655105, 0.06293345894664526, 0.0645281879696995, 1.7162056119414046, 0.2716270349919796, 0.06714264408219606, 0.07118098007049412, 0.06566129601560533, 0.06418716302141547, 0.07142510497942567, 0.06911094696260989, 0.06466778297908604, 0.06990271992981434, 0.07148966705426574, 0.06545510503929108, 0.06561778602190316, 1.2000648390967399, 0.06945508194621652, 0.065545194898732, 0.15201776998583227, 0.06295793992467225, 0.064309066045098, 0.06580444297287613, 0.06939325598068535, 0.06445921899285167, 0.06418992893304676, 0.3668155289487913, 0.20138085703365505, 0.19358270696830004, 0.07272000994998962, 0.06528760003857315, 0.0653917589224875, 0.06596439203713089, 0.06563992309384048, 0.06641777500044554, 0.07210104598198086, 0.06443729600869119, 0.06992327095940709, 0.0695811070036143, 2.2161726449849084, 0.071116485982202, 0.06742271594703197, 0.06291959306690842, 0.06714440998621285, 0.06201162200886756, 0.061729033943265676, 0.06224128999747336, 0.061859858920797706, 0.06827762501779944, 0.06586357206106186, 0.06834904197603464, 0.06278733804356307, 0.06359967798925936, 1.3186588590033352, 0.06958860799204558, 0.06546626298222691, 0.06502499699126929, 0.07032969291321933, 0.06582687492482364, 0.0655704439850524, 1.9051513120066375, 0.34100844198837876, 0.0672868819674477, 0.0643655169988051, 0.06323832599446177, 0.07136381301097572, 0.06756826408673078, 0.06489188096020371, 0.06471206899732351, 0.06912705302238464, 0.06919988198205829, 0.6070576390484348, 0.0859090369194746, 0.0758327569346875, 0.0649796420475468, 0.06416665902361274, 0.06514160090591758, 0.06459972297307104, 0.06517120101489127, 0.06891537504270673, 0.06532650394365191, 0.06629782903473824, 2.238558550947346, 0.06598101602867246, 0.06924301700200886, 0.067480772966519, 0.0628592330031097, 0.06268713995814323, 0.06307572196237743, 0.06305542297195643, 0.06772002205252647, 0.06268972100224346, 0.062342574005015194, 0.0634767560986802, 0.06228792399633676, 0.06478872790466994, 0.06264090596232563, 0.07228142698295414, 0.07419338892214, 1.7326488689286634, 0.06752698204945773, 0.0669534649932757, 0.06520805996842682, 0.06508939596824348, 0.06680669297929853, 0.06480691605247557, 0.06576545105781406, 0.06891637109220028, 0.5567155629396439, 0.4813709679292515, 0.06928115605842322, 0.06672954501118511, 0.07263791200239211, 0.0672091000014916, 0.06677619100082666, 0.06693172990344465, 0.06674282194580883, 0.07575565692968667, 0.07562583195976913, 0.0763560690684244, 0.07039989798795432, 0.06928758102003485, 0.07593594503123313, 0.3158821549732238, 0.13041383994277567, 0.06614619097672403, 0.0659579400671646, 0.06593002506997436, 0.06693006900604814, 0.07002889807336032, 0.06604031892493367, 1.7890617039520293, 0.22036014206241816, 0.0759566790657118, 0.06264192296657711, 0.06709688995033503, 0.06791546498425305, 0.06306132394820452, 0.06323279696516693, 0.06395158101804554, 0.06762970797717571, 0.06245590595062822, 0.06364724098239094, 0.06304338597692549, 0.06624758802354336, 0.06256500107701868, 0.062405450036749244, 0.06723620695993304, 0.0686056069098413, 0.06282429001294076, 0.0674764149589464, 0.07289072405546904, 0.0673585309414193, 0.06730797293130308, 0.07337011909112334, 0.0664173288969323, 0.06607174105010927, 0.06301894097123295, 0.06300670397467911, 0.896176618989557, 0.8744621520163491, 0.21723663003649563, 0.07425843097735196, 0.06504185998346657, 0.06942025804892182, 0.0680359520483762, 0.1715412779012695, 0.06343274796381593, 0.06362874701153487, 0.06590970896650106, 0.06434416002593935, 0.06861695402767509, 0.517940251971595, 1.6072091270470992, 0.506246354081668, 0.11551346397027373, 0.06630984100047499, 0.06963990395888686, 0.06729125301353633, 0.06486731197219342, 0.06440085300710052, 0.06430957093834877, 0.06387872400227934, 1.2316235409816727, 1.3823849940672517, 0.07851068291347474, 0.07376984693109989, 0.07108216302003711, 0.07271471689455211, 0.07723820197861642, 0.07201258500572294, 0.07108293694909662, 0.31924587104003876, 0.3110714729409665, 0.24418258003424853, 0.07032156898640096, 0.07015903701540083, 0.06441724894102663, 0.06654989102389663, 0.06320760492235422, 0.063731548958458, 0.06386265100445598, 0.06440409598872066, 0.06900439003948122, 0.06583102501463145, 0.06380656594410539, 0.06863827502820641, 0.06819069501943886, 0.06474722002167255, 0.06424930098000914, 0.0643530689412728, 0.06374898494686931, 0.06989809998776764, 0.06339333101641387, 0.06399103906005621, 0.06720307003706694, 0.0658894709777087, 0.06558989104814827, 0.35669670498464257, 0.2774782329797745, 0.06974509102292359, 0.06629608699586242, 0.07045628689229488, 0.07003830000758171, 0.06582778599113226, 0.06648476398549974, 0.06757584598381072, 0.06799790496006608, 0.0674353139474988, 0.07260047493036836, 1.2878672890365124, 0.42416838300414383, 0.0708550380077213, 0.07010504603385925, 0.06911957205738872, 0.06387625099159777, 0.06314541702158749, 0.06569126795511693, 0.06714261998422444, 0.06793443101923913, 0.06717778695747256, 0.06921430805232376, 0.06554288696497679, 0.07067063602153212, 1.6688041030429304, 0.07966629404108971, 0.06865600997116417, 0.07281723199412227, 0.07409592799376696, 0.06876463210210204, 0.06786132592242211, 0.06932685791980475, 0.06493154994677752, 0.06484484695829451, 0.06852187891490757, 0.07018460996914655, 0.065465735970065, 0.06465642701368779, 0.06410289893392473, 0.06439938396215439, 0.06907692807726562, 0.06559018197003752, 0.06878612097352743, 0.0636490338947624, 0.07021142798475921, 0.06324622198008001, 0.06431960500776768, 0.06692157802172005, 0.06340384704526514, 0.07188033603597432, 0.06277055200189352, 0.06325363996438682, 0.0633833589963615, 0.06339651392772794, 0.0673870580503717, 0.06369193003047258, 0.06352362199686468, 0.06936838501133025, 0.06352287204936147, 0.0665385730098933, 0.06500590301584452, 0.06316434603650123, 0.0695675800088793, 0.07912603102158755, 0.5844906029524282, 0.13304443599190563, 0.06720435293391347, 0.07339471008162946, 0.06709514104295522, 0.06664645893033594, 0.07241625699680299, 0.0673919339897111, 0.06662923796102405, 0.06642111495602876, 0.0668294399511069, 0.06493537197820842, 0.06322812498547137, 0.06634204590227455, 0.06255423102993518, 0.06259065808262676, 0.06379132508300245, 0.06288823706563562, 0.0621612899703905, 0.06835659104399383, 0.06396779301576316, 1.3505150770070031, 0.5170005529653281, 0.0689914709655568, 0.06966114405076951, 0.07459674205165356, 0.07465723797213286, 0.07027957204263657, 0.06894670298788697, 0.06392588000744581, 0.06406681705266237, 0.06431151297874749, 0.06458875490352511, 0.06488044699653983, 0.065676145022735, 0.35896303795743734, 1.2143864179961383, 0.07307350402697921, 0.06556992197874933, 0.06835028098430485, 0.08258796902373433, 0.06818107003346086, 0.06971676799003035, 0.16462945798411965, 0.06640488398261368, 0.06783919292502105, 0.07787828089203686, 0.07304806995671242, 0.07368136709555984, 0.07458002492785454, 0.07515960093587637, 0.06915963196661323, 0.06756905501242727, 0.06901379197370261, 0.07452968298457563, 0.06816296197939664, 0.06471523200161755, 0.06693262991029769, 0.06875477300491184, 0.07952850603032857, 0.06697196408640593, 0.06376407400239259, 0.06349858804605901, 0.06372120301239192, 0.06320428301114589, 0.06704925000667572, 0.06351494207046926, 0.06360963697079569, 0.06883960799314082, 0.06846590200439095, 0.06928402802441269, 0.06961149291601032, 0.06880488898605108, 0.06799859402235597, 0.08520903903990984, 0.06564321601763368, 0.06672774895559996, 0.07125349901616573, 0.9326045931084082, 0.19147153396625072, 0.22290814109146595, 0.06835463794413954, 0.067610680940561, 0.06682980805635452, 0.06916595599614084, 0.06328932603355497, 0.063800786039792, 0.06399306305684149, 0.06298427400179207, 0.06481503101531416, 1.189055873081088, 0.4092626499477774, 0.07255974307190627, 0.07367307704407722, 0.06647590606007725, 0.07152761903125793, 0.06630488007795066, 0.071962476009503, 0.06671614805236459, 0.06672616396099329, 0.0729592900024727, 0.06630897999275476, 0.06693426193669438, 1.2392065059393644, 0.40281887899618596, 0.07382444804534316, 0.06446668901480734, 0.06884725007694215, 0.06952448503579944, 0.06382494198624045, 0.06547389400657266, 0.06431331089697778, 0.06947608303744346, 0.06549400591757149, 0.067580278031528, 0.0638267049798742, 0.06398291804362088, 0.06851908203680068, 0.06828400900121778, 0.06414322403725237, 0.06848392600659281, 0.07262359699234366, 0.06490559701342136, 0.06663399399258196, 0.06449548597447574, 0.06905579299200326, 0.07653011404909194, 0.07602321798913181, 0.06532603409141302, 0.8788891969015822, 0.27111051604151726, 0.07314431900158525, 0.0649898829869926, 0.06671972898766398, 0.0664210399845615, 0.06542360107414424, 0.06253501202445477, 0.06445766892284155, 0.06479838397353888, 0.07054607500322163, 0.07039458001963794, 0.065173669019714, 0.06805391400121152, 0.06851681298576295, 0.07044602697715163, 0.06432551401667297, 0.06500284594949335, 0.06457605899777263, 0.0654990510083735, 0.06379286793526262, 0.06456376204732805, 0.06988232699222863, 0.06446634698659182, 0.06401399301830679, 0.07062350900378078, 0.06861829198896885, 0.06755873002111912, 0.07087855401914567, 0.06670955300796777, 0.0673260820331052, 0.07348539703525603, 0.06830362405162305, 0.07730086101219058, 0.06577072897925973, 0.0666669289348647, 0.06769820593763143, 0.07245017006061971, 1.9596740840934217, 0.9620321380207315, 0.07046111591625959, 0.0661841289838776, 0.06658130697906017, 0.06601538206450641, 0.06682723190169781, 0.06865145300980657, 0.06898205203469843, 0.0653947819955647, 0.06540770502761006, 0.06596414197701961, 0.06502940692007542, 0.07118258194532245, 0.07133390300441533, 0.07771919202059507, 0.07355559896677732, 0.07522508397232741, 0.07532250508666039, 0.4084291330073029, 0.06941979797556996, 0.06922365503851324, 0.06821172300260514, 0.0690124100074172, 0.06366311002057046, 0.06415278499480337, 0.06419195095077157, 0.07602789194788784, 0.06731262803077698, 0.06729819905012846, 0.07346002699341625, 0.07078589906450361, 0.07427044294308871, 0.0736660819966346, 4.503635878092609, 0.07090341404546052, 0.07478197803720832, 0.06951119098812342, 0.07665546203497797, 0.07437500089872628, 0.07058266701642424, 0.06871553789824247, 0.0696682749548927, 2.332156667020172, 0.11686899000778794, 0.07871303509455174, 0.07055106398183852, 0.07199013198260218, 0.09376147203147411, 0.07216680399142206, 0.07080496102571487, 0.06907609000336379, 0.0964147619670257, 0.06607787392567843, 0.06552604003809392, 0.3881415129872039, 0.589422435965389, 0.4416610869811848, 0.06642098899465054, 0.06578986090607941, 0.06496899807825685, 0.06450557801872492, 0.06417133600916713, 0.06789846892934293, 0.0642391899600625, 0.06423501507379115, 0.06929359491914511, 0.06399363896343857, 0.06451334792654961, 0.06411885400302708, 0.06508341000881046, 0.06473090499639511, 0.07003381999675184, 0.06497018598020077, 0.06504035601392388, 0.06508302304428071, 0.06520673504564911, 0.06518018001224846, 0.6629909779876471, 0.06755822896957397, 0.07157642999663949, 0.07272328692488372, 0.06755729403812438, 0.06771413504611701, 0.0715579780517146, 0.07116692594718188, 0.06715583894401789, 0.06848079105839133, 0.07245134899858385, 0.06722319894470274, 0.06702481501270086, 1.2813479989999905, 0.4242539450060576, 0.07447791192680597, 0.07344364305026829, 0.0679449460003525, 0.0673694210126996, 0.06451893097255379, 0.06527983793057501, 0.0645159799605608, 0.06451353791635484, 0.06359558505937457, 0.06826719699893147, 0.07225725089665502, 0.06712194008287042, 0.06816358701325953, 0.06355647498276085, 0.06342282507102937, 0.06409452995285392, 0.06353080994449556, 0.06412143202032894, 0.06470184994395822, 0.06406555196736008, 0.06418455101083964, 0.06448719301261008, 0.06435815803706646, 0.06566310208290815, 0.06721896294038743, 0.061438523000106215, 0.0614573530619964, 0.06632577197160572, 0.06660741090308875, 0.06304434209596366, 0.062390729086473584, 0.06757632200606167, 0.06224048207513988, 0.062174338032491505, 0.06284572905860841, 0.06227965105790645, 0.06752009608317167, 0.06740075396373868, 0.06250167998950928, 0.06291136902291328, 0.06730246206279844, 0.15030146902427077, 0.06578605202957988, 0.06729742395691574, 0.06942942296154797, 0.06754914403427392, 0.06394178501795977, 0.07084997103083879, 0.06671703804749995, 0.06626581703312695, 0.0666622529970482, 0.06542167300358415, 0.06645414605736732, 0.0673836680362001, 0.1479212570702657, 0.0664302259683609, 0.06586207693908364, 0.06253713590558618, 0.06324854400008917, 0.06221957900561392, 0.0641926700482145, 0.06554925406817347, 0.06259911507368088, 0.06743779208045453, 0.06788706895895302, 0.06741353799588978, 0.06647619407158345, 0.071465541026555, 0.07118047692347318, 0.0725632639368996, 0.07049167098011822, 0.06552886101417243, 0.06561469100415707, 0.07097887492273003, 0.06530368502717465, 0.06594293203670532, 0.06080109300091863, 0.0667420559329912, 0.062383426004089415, 0.061483904952183366, 0.06124903494492173, 0.06171438191086054, 0.062082702992483974, 0.0662174359895289, 0.07125466503202915, 0.06620554102119058, 0.06591898808255792, 0.0665885719936341, 0.6497554560191929, 0.9856888699578121, 0.0686382990097627, 0.07206263800617307, 0.06703889893833548, 0.0676741610514, 0.06212588702328503, 0.06289674190338701, 0.06663579004816711, 0.06213947606738657, 0.0632768829818815, 0.06185680499766022, 0.06259323400445282, 0.063094393000938, 0.06173845799639821, 0.06352546706330031, 0.06286085909232497, 0.06723922595847398, 0.06744257791433483, 0.6813678949838504, 0.8929920350201428]
[0.0013216504499278202, 0.001342676938701497, 0.0013613001828328992, 0.0012822923872961986, 0.0013540850605397504, 0.0013701049801038236, 0.001359971000204737, 0.0013428058365968112, 0.001264647388241577, 0.001269070122733104, 0.023622921632355725, 0.014599191549956342, 0.009225988204647996, 0.004256215713899203, 0.0012737661837695204, 0.0012928638773571169, 0.0012805928162546183, 0.0012620220801850058, 0.0012677737145817705, 0.001339303346217743, 0.0012812637747740563, 0.0012647581419774465, 0.0013636918168286889, 0.004680086122065478, 0.0013565318160025136, 0.0013522247341936644, 0.0014169643471511652, 0.0013627101244328885, 0.0012548919173185618, 0.0012488473463347371, 0.0012523132442895854, 0.0013865597964245447, 0.0013266533888800411, 0.0012920218785958631, 0.001281583429865387, 0.0013187421837403458, 0.001294084855507375, 0.001325234732761675, 0.0013148334086397473, 0.0013185544295844678, 0.0013216198968453979, 0.0013981619398394714, 0.0013523153480788578, 0.0013202521023435556, 0.014181004388124816, 0.007118144041706561, 0.008054328878049036, 0.0014984726939084275, 0.0013888352835664944, 0.0013611972455543522, 0.0014622900801311647, 0.0013861973266288334, 0.0013721456124010135, 0.0013708052040096752, 0.001771807551802117, 0.0014454195304412622, 0.018964871590747973, 0.025366449184544688, 0.006961504650796402, 0.0013943957141125385, 0.001293687387464606, 0.0012687593057029406, 0.0013348911033601177, 0.0013167841207920288, 0.0012509631021518487, 0.0013332522654791875, 0.001330996509546376, 0.001244695671853058, 0.017540859426751886, 0.009159013184680775, 0.0013980058981675882, 0.001374605978953139, 0.001411985917187923, 0.0013868871420545845, 0.0014864867743180723, 0.0014488258166238666, 0.033158175754170786, 0.0013849147956590264, 0.001275070919180099, 0.0012732371854196703, 0.0012877515507671907, 0.0013965370409114628, 0.0013987623055332474, 0.0013685350197044257, 0.001299990816231893, 0.0012963877548938807, 0.001375127510091632, 0.0013945167147725516, 0.001317113958660285, 0.0013214652854188973, 0.001325336855132969, 0.017483938490135634, 0.010604071041226995, 0.0013855722856384758, 0.0013768489387038412, 0.001493554448290747, 0.0015509268981689702, 0.0014596079994107084, 0.0013427901846755827, 0.001352192876784473, 0.0013387421435885588, 0.0013470270419113186, 0.0013439355902753922, 0.0013366434081666628, 0.001343425878855799, 0.001342943408621513, 0.0013526115514223976, 0.0013482908799065923, 0.0014697168763650923, 0.0013597357129602103, 0.005480836304759949, 0.0014076802252354671, 0.0014747948366768506, 0.0013269415310565003, 0.001264276878661191, 0.0012673639587829917, 0.0012600492443699315, 0.00127117851112342, 0.0012651148979190964, 0.0012631859791902254, 0.001277161123021981, 0.0013376617761404843, 0.0012585870204113272, 0.0012697446544901753, 0.0012559885496501715, 0.001377750836711909, 0.0013663944697995878, 0.001497215510593081, 0.0015043893268293872, 0.0013718495516068473, 0.0013699966520831293, 0.001357297265750109, 0.001519387815983928, 0.0013684181030839682, 0.0013643081029116803, 0.0014863157744652458, 0.0013998454289358793, 0.0014927174506366861, 0.0013659074902534485, 0.0013818476748253619, 0.0013742510809049923, 0.0013754930185648252, 0.0013851776139392536, 0.0013843207330233892, 0.0013802234910200446, 0.0013802390407808886, 0.0014995493669519011, 0.0015081341416403955, 0.001366024162164148, 0.0013848234904093705, 0.001510580082195906, 0.0014079242673873597, 0.00139621653052389, 0.0013729465094261936, 0.0014780763280102794, 0.022550761856480826, 0.0014231795307286844, 0.0013534157342106408, 0.0014393254686907238, 0.0014805721418399895, 0.0013523464285939628, 0.0013565418776124716, 0.0013534959586223169, 0.0013734243468058352, 0.001363503898741031, 0.001499145714642138, 0.0013981830015094305, 0.021545356816650197, 0.006222959042393735, 0.008588542182910807, 0.0013228372851272626, 0.0013264246731616405, 0.0014635761225672097, 0.0014751418778786855, 0.0013124249989585, 0.00132431618261094, 0.0013303889997531564, 0.0013207062456414712, 0.0013212871842314393, 0.0013091004695933387, 0.0013145462239199147, 0.0013255839407139895, 0.00142280646951451, 0.001452681979126468, 0.0014408095929847689, 0.0013111081824885035, 0.0013169998383833741, 0.031516564449257385, 0.0014199760614191086, 0.0012869332650942461, 0.001425162040894585, 0.0013069010610521144, 0.0013134264510733132, 0.001386731449628667, 0.0014555517142182406, 0.023191628428841277, 0.0023934703880959017, 0.001442534285977635, 0.0013211833465160156, 0.001433470633299071, 0.001313598591321129, 0.0013106325736307368, 0.0012960753270557948, 0.0013124634706586295, 0.0014310501829473948, 0.001312546838283995, 0.0014153477944889848, 0.001379562938604884, 0.0013958297560599689, 0.0015095162433477081, 0.0013935591227241925, 0.015086809653142581, 0.02586819669374322, 0.002704334958475463, 0.00137485912288254, 0.0014465225109716459, 0.0013484127144804414, 0.0013296149186409858, 0.001319351161792114, 0.0012431398167141847, 0.001249838632303385, 0.011355724081168977, 0.043518125141343594, 0.004830320693590507, 0.0013607345112808505, 0.0012874368373874802, 0.0013337926321415877, 0.001326940205347325, 0.0012573063069460343, 0.0012698102438328217, 0.0013541817945447198, 0.0013567759175499787, 0.0013395474097520417, 0.017360364899457414, 0.009578063447332504, 0.03100361524396861, 0.007881722470023195, 0.0012526017547185933, 0.0012438812849054836, 0.0012559856535220633, 0.0012275220408123366, 0.001213826652502223, 0.0013245817948588912, 0.0012300526533199816, 0.0012979171029767211, 0.0012380592667554713, 0.0013232750613812584, 0.001248464960015702, 0.00132630649022758, 0.03903853083837169, 0.001783881918527186, 0.0012560729388793816, 0.0012522850218894227, 0.0012599723674927134, 0.0012450912439891575, 0.0012624488991438126, 0.0012760237952200125, 0.001277572632178056, 0.0013450675914824313, 0.001304952346966887, 0.0013773580395369505, 0.001277132857857006, 0.0012835235719816113, 0.0013663273480511745, 0.0013101326933662807, 0.0012854546930983054, 0.001295254549144634, 0.0012898760614916682, 0.0012840872454666058, 0.03275297551026226, 0.004822067795226312, 0.0013847133876489742, 0.0013717612232632783, 0.0014776122038803843, 0.0013790928155221806, 0.0013646396323658374, 0.0013751879605293578, 0.001440406449101105, 0.0015169861845253985, 0.0013804817548477833, 0.0013844793073224779, 0.0014808856530533153, 0.0015084050400942868, 0.0013968129594791301, 0.0014178411445903536, 0.001378210551314512, 0.008613374385963745, 0.0013999177561122545, 0.0014125466137668307, 0.0013193144695833325, 0.0013904314682990008, 0.0013286569179510887, 0.0013331787741495942, 0.0013316493262821923, 0.001430887631520781, 0.0013364869816114707, 0.0013336595694287395, 0.0013131976545769342, 0.0014756017754196512, 0.0014169438580545236, 0.0015129296332407668, 0.001401009981292395, 0.0033624281617337646, 0.001528865203964619, 0.014433848593212967, 0.010217235387987172, 0.003788949389542852, 0.0013809775724550899, 0.001372013611680999, 0.001405479571763046, 0.0013648951830038307, 0.001403970938480022, 0.0013844472859815067, 0.001391781349096219, 0.001402682693656154, 0.0013946737352834673, 0.014855608101744128, 0.0033996863865616675, 0.0014565325513177989, 0.0013503170410665323, 0.0013344662658376048, 0.0014303478992030937, 0.0013382604692549426, 0.0014778930594080261, 0.0013266978571571562, 0.0013274207963057014, 0.00133569095977487, 0.0013265741646898035, 0.0013471218776337955, 0.0013288133254996976, 0.001338307427394451, 0.0013669816116631335, 0.0013427221434837093, 0.0014278160821532412, 0.03876823914113778, 0.0014748085499265973, 0.001292313917121869, 0.0012764346724071977, 0.0012913862865760314, 0.0012757885911292872, 0.0012887434687047284, 0.0012832099586079012, 0.0012779985103109966, 0.001295875938495203, 0.0013141019783001772, 0.0012940797166024543, 0.0013284246527532839, 0.0012941606323785928, 0.0012843563050335767, 0.0013169017952999898, 0.03502460432533479, 0.005543408877387339, 0.0013702580424937972, 0.0014526730626631453, 0.001340026449298068, 0.0013099421024778668, 0.0014576552036617482, 0.001410427489032855, 0.0013197506730425724, 0.0014265861210166191, 0.0014589727970258314, 0.001335818470189614, 0.0013391384902429215, 0.024491119165239588, 0.0014174506519636025, 0.0013376570387496328, 0.0031024034690986177, 0.0012848559168300458, 0.0013124299192877145, 0.0013429478157729823, 0.0014161888975650072, 0.0013154942651602383, 0.0013099985496540154, 0.007486031203036558, 0.004109813408850103, 0.003950667489148981, 0.0014840818357140738, 0.001332400000787207, 0.001334525692295663, 0.0013462120823904264, 0.0013395902672212344, 0.00135546479592746, 0.0014714499179996094, 0.0013150468573202283, 0.0014270055297838182, 0.001420022591910496, 0.04522801316295731, 0.0014513568567796325, 0.0013759737948373873, 0.0012840733278960902, 0.0013702940813512827, 0.0012655433063034195, 0.0012597762029237893, 0.0012702304081117011, 0.001262446100424443, 0.0013934209187306007, 0.0013441545318584054, 0.0013948784076741763, 0.0012813742457870015, 0.0012979526120257012, 0.026911405285782352, 0.0014201756733070528, 0.0013360461833107533, 0.001327040754923863, 0.0014352998553718232, 0.0013434056107106867, 0.0013381723262255593, 0.038880639020543624, 0.006959355958946505, 0.001373201672805055, 0.001313581979567451, 0.001290578081519628, 0.0014564043471627698, 0.001378944165035322, 0.001324324101228647, 0.001320654469333133, 0.0014107561841302989, 0.001412242489429761, 0.01238893140915173, 0.001753245651417849, 0.0015476072843813775, 0.0013261151438274858, 0.0013095236535431171, 0.0013294204266513794, 0.0013183616933279804, 0.001330024510507985, 0.0014064362253613618, 0.0013331939580337126, 0.0013530169190762906, 0.045684868386680524, 0.0013465513475239277, 0.0014131227959593643, 0.0013771586319697754, 0.0012828414898593814, 0.0012793293869008823, 0.0012872596318852536, 0.001286845366774621, 0.0013820412663780913, 0.0012793820612702746, 0.0012722974286737796, 0.0012954440020138817, 0.0012711821223742195, 0.0013222189368299988, 0.0012783858359658293, 0.0014751311629174315, 0.0015141507943293878, 0.035360180998544155, 0.0013781016744787292, 0.0013663972447607287, 0.001330776734049527, 0.001328355019760071, 0.0013634018975367047, 0.0013225901235199096, 0.0013421520624043687, 0.0014064565529020466, 0.011361542100809058, 0.009823897304678602, 0.0014139011440494535, 0.0013618274492078594, 0.0014824063673957574, 0.0013716142857447267, 0.001362779408180136, 0.0013659536714988704, 0.001362098407057323, 0.0015460338148915646, 0.001543384325709574, 0.001558287123845396, 0.0014367326119990678, 0.0014140322657149968, 0.001549713163902717, 0.0064465745912902815, 0.0026615069376076665, 0.0013499222648311027, 0.0013460804095339713, 0.0013455107157137624, 0.0013659197756336356, 0.0014291611851706188, 0.001347761610712932, 0.03651146334595978, 0.0044971457563758805, 0.001550136307463506, 0.001278406591154635, 0.0013693242847007147, 0.0013860298976378174, 0.0012869657948613167, 0.00129046524418708, 0.0013051343064907255, 0.001380198121983178, 0.001274610325523025, 0.0012989232853549172, 0.001286599713814806, 0.0013519915923172114, 0.0012768367566738505, 0.0012735806129948826, 0.0013721674889782254, 0.0014001144267314551, 0.001282128367611036, 0.0013770696930397227, 0.0014875657970503885, 0.0013746638967636594, 0.0013736321006388385, 0.001497349369206599, 0.0013554556917741286, 0.0013484028785736586, 0.0012861008361476113, 0.0012858511015240634, 0.018289318754888918, 0.017846166367680594, 0.004433400612989706, 0.0015154781832112645, 0.0013273848976217667, 0.0014167399601820781, 0.0013884888173138, 0.0035008424061483572, 0.0012945458768125698, 0.0012985458573782627, 0.0013450961013571645, 0.001313146122978354, 0.0014003460005647978, 0.010570209223910101, 0.03280018626626733, 0.010331558246564652, 0.0023574176320464027, 0.0013532620612341836, 0.0014212225297732012, 0.0013732908778272721, 0.0013238226933100698, 0.0013143031225938881, 0.0013124402232316075, 0.0013036474286179458, 0.025135174305748423, 0.028211938654433708, 0.0016022588349688723, 0.0015055070802265284, 0.0014506563881640227, 0.0014839738141745329, 0.0015762898362982943, 0.0014696445919535294, 0.001450672182634625, 0.006515221857959975, 0.006348397406958499, 0.004983317959882623, 0.0014351340609469584, 0.0014318170819469557, 0.0013146377334903394, 0.001358161041304013, 0.0012899511208643718, 0.0013006438562950613, 0.0013033194082542037, 0.0013143693058922583, 0.0014082528579485963, 0.00134349030642105, 0.0013021748151858241, 0.0014007811230246205, 0.0013916468371314053, 0.0013213718371769907, 0.0013112102240818192, 0.0013133279375769958, 0.0013009996927932513, 0.0014264918364850537, 0.0012937414493145688, 0.0013059395726542084, 0.001371491225246264, 0.0013446830811777285, 0.0013385692050642505, 0.007279524591523318, 0.005662821081219887, 0.001423369204549461, 0.0013529813672624985, 0.0014378834059652016, 0.0014293530613792185, 0.0013434242039006583, 0.0013568319180714233, 0.0013790988976287904, 0.0013877123461237975, 0.0013762308968877305, 0.0014816423455177217, 0.026283005898704335, 0.008656497612329466, 0.001446021183831047, 0.001430715225180801, 0.00141060351137528, 0.0013035969590121995, 0.0012886819800323978, 0.0013406381215329986, 0.001370257550698458, 0.0013864169595763087, 0.0013709752440300522, 0.0014125368990270154, 0.0013376099380607508, 0.0014422578779904514, 0.03405722659271287, 0.001625842735532443, 0.0014011430606360035, 0.0014860659590637197, 0.0015121617957911625, 0.001403359838818409, 0.0013849250188249411, 0.0014148338350980561, 0.0013251336723832147, 0.0013233642236386634, 0.001398405692140971, 0.0014323389789621746, 0.00133603542796051, 0.0013195189186466895, 0.0013082224272229538, 0.0013142731420847835, 0.0014097332260666453, 0.0013385751422456636, 0.0014037983872148456, 0.0012989598754033142, 0.0014328862854032492, 0.0012907392240832656, 0.001312645000158524, 0.0013657464902391847, 0.001293956062148268, 0.001466945633387231, 0.001281031673508031, 0.0012908906115180984, 0.001293537938701255, 0.0012938064066883252, 0.001375246082660647, 0.0012998353067443383, 0.0012964004489156057, 0.0014156813267618418, 0.0012963851438645198, 0.0013579300614263937, 0.0013266510819560106, 0.0012890682864592088, 0.0014197465307934551, 0.0016148169596242358, 0.01192837965209037, 0.0027151925712633803, 0.0013715174068145606, 0.0014978512261557032, 0.0013692885927133718, 0.0013601318149048152, 0.0014778827958531221, 0.0013753455916267572, 0.001359780366551511, 0.0013555329582863012, 0.0013638661214511614, 0.0013252116730246618, 0.001290369897662681, 0.001353919304128052, 0.0012766169597945955, 0.0012773603690331991, 0.0013018637772041317, 0.0012834334095027677, 0.0012685977544977653, 0.0013950324702855883, 0.0013054651635870033, 0.02756153218381639, 0.010551031693169961, 0.0014079892033787103, 0.0014216560010361125, 0.0015223824908500727, 0.0015236171014720993, 0.001434276980461971, 0.0014070755711813666, 0.0013046097960703227, 0.001307486062299232, 0.0013124798567091323, 0.001318137855173982, 0.0013240907550314252, 0.001340329490259898, 0.00732577628484566, 0.024783396285635476, 0.001491296000550596, 0.0013381616730357008, 0.0013949036935572417, 0.001685468755586415, 0.00139145040884614, 0.001422791183470007, 0.0033597848568187685, 0.0013552017139308915, 0.0013844733250004296, 0.0015893526712660582, 0.0014907769378920902, 0.0015037013692971394, 0.001522041325058256, 0.0015338694068546199, 0.0014114210605431273, 0.0013789603063760667, 0.0014084447341571962, 0.0015210139384607272, 0.0013910808567223804, 0.0013207190204411745, 0.0013659720389856671, 0.001403158632753303, 0.001623030735312828, 0.0013667747772735904, 0.0013013076327018896, 0.001295889551960388, 0.0013004327145386108, 0.0012898833267580793, 0.0013683520409525658, 0.0012962233075605972, 0.0012981558565468509, 0.00140488995904369, 0.0013972633062120602, 0.001413959755600259, 0.0014206427125716392, 0.0014041814078785935, 0.0013877264086195097, 0.0017389599804063232, 0.001339657469747626, 0.001361790795012244, 0.0014541530411462395, 0.01903274679813078, 0.0039075823258418515, 0.00454914573656053, 0.0013949926111048885, 0.0013798098151134898, 0.0013638736338031534, 0.001411550122370221, 0.001291618898643979, 0.0013020568579549389, 0.001305980878711051, 0.0012853933469753485, 0.0013227557350064115, 0.02426644638940996, 0.008352298978526069, 0.001480811083100128, 0.0015035321845730044, 0.0013566511440832093, 0.0014597473271685292, 0.0013531608179173603, 0.0014686219593776123, 0.0013615540418849916, 0.0013617584481835365, 0.0014889651020912795, 0.0013532444896480562, 0.001366005345646824, 0.02528992869264009, 0.008220793448901755, 0.0015066213886804727, 0.0013156467145879049, 0.0014050459199375948, 0.0014188670415469274, 0.0013025498364538867, 0.001336201918501483, 0.0013125165489179138, 0.0014178792456621115, 0.0013366123656647242, 0.001379189347582204, 0.0013025858159158, 0.001305773837624916, 0.0013983486129959322, 0.0013935512041064854, 0.0013090453885153544, 0.00139763114299169, 0.0014821142243335442, 0.0013246040206820685, 0.0013598774284200401, 0.001316234407642362, 0.0014093018977959849, 0.001561839062226366, 0.0015514942446761594, 0.0013331843692125107, 0.01793651422248127, 0.005532867674316678, 0.0014927412041139845, 0.0013263241425916857, 0.0013616271221972241, 0.0013555314282563571, 0.0013351755321253927, 0.0012762247351929545, 0.001315462631078399, 0.001322415999459977, 0.0014397158163922783, 0.0014366240820334274, 0.0013300748779533468, 0.001388855387779827, 0.001398302305831897, 0.00143767401994187, 0.0013127655921769993, 0.001326588692846803, 0.0013178787550565843, 0.0013367153267014999, 0.0013018952639849515, 0.0013176277968842461, 0.0014261699386169107, 0.0013156397344202412, 0.0013064080207817713, 0.0014412961021179752, 0.0014003733058973234, 0.001378749592267737, 0.0014465011024315441, 0.0013614194491421993, 0.001374001674145004, 0.0014997019803113475, 0.0013939515112576131, 0.001577568592085522, 0.0013422597750869332, 0.0013605495700992796, 0.0013815960395434986, 0.0014785748991963206, 0.039993348654967786, 0.019633308939198603, 0.0014379819574746855, 0.001350696509875053, 0.001358802183246126, 0.0013472526951940084, 0.0013638210592183228, 0.0014010500614246239, 0.001407796980299968, 0.0013345873876645857, 0.0013348511230124502, 0.0013462069791228492, 0.0013271307534709269, 0.0014527057539861724, 0.001455793938865619, 0.0015861059596039811, 0.001501134672791374, 0.0015352057953536206, 0.001537193981360416, 0.008335288428720467, 0.0014167305709299992, 0.0014127276538472091, 0.001392075979645003, 0.0014084165307636164, 0.0012992471432769482, 0.001309240510098028, 0.0013100398153218689, 0.0015515896315895477, 0.001373727102668918, 0.001373432633676091, 0.0014991842243554337, 0.0014446101849898696, 0.0015157233253691573, 0.001503389428502747, 0.09191093628760427, 0.0014470084499073575, 0.0015261628170858842, 0.0014185957344514982, 0.0015643971843873055, 0.0015178571611984956, 0.0014404625921719233, 0.0014023579162906628, 0.0014218015296916877, 0.04759503402081983, 0.002385081428730366, 0.0016063884713173825, 0.0014398176322824188, 0.0014691863669918813, 0.0019134994292137573, 0.001472791918192287, 0.001444999204606426, 0.0014097161225176283, 0.0019676482034086877, 0.0013485280392995598, 0.0013372661232264067, 0.007921255367085794, 0.012029029305416102, 0.009013491571044587, 0.0013555303876459294, 0.0013426502225730493, 0.0013258979199644253, 0.00131644036772908, 0.0013096191022279008, 0.0013856830393743453, 0.0013110038767359695, 0.0013109186749753294, 0.0014141549983499, 0.0013059926319069096, 0.0013165989372765227, 0.0013085480408781037, 0.0013282328573226625, 0.0013210388774774512, 0.0014292616325867723, 0.0013259221628612401, 0.0013273542043657936, 0.0013282249600873614, 0.0013307496948091655, 0.0013302077553520094, 0.013530428122196878, 0.0013787393667259995, 0.0014607434693191732, 0.001484148712752729, 0.0013787202864923344, 0.001381921123390143, 0.0014603668990145837, 0.0014523862438200383, 0.0013705273253881202, 0.001397567164456966, 0.0014785989591547726, 0.001371902019279648, 0.00136785336760614, 0.026149959163265114, 0.00865824377563383, 0.0015199573862613464, 0.0014988498581687407, 0.0013866315510276022, 0.0013748861431163184, 0.0013167128769908936, 0.0013322415904198982, 0.0013166526522563429, 0.0013166028146194865, 0.001297869082844379, 0.0013932081020190095, 0.0014746377734011229, 0.0013698355118953148, 0.0013910936125155007, 0.0012970709180155275, 0.0012943433687965177, 0.0013080516316908964, 0.0012965471417243993, 0.0013086006534761007, 0.0013204459172236373, 0.0013074602442318384, 0.0013098887961395845, 0.0013160651635226546, 0.0013134317966748256, 0.0013400633078144522, 0.0013718155702119883, 0.0012538474081654329, 0.0012542316951427836, 0.0013535871830939942, 0.0013593349163895662, 0.001286619226448238, 0.0012732801854382365, 0.0013791086123686057, 0.0012702139199008138, 0.0012688640414794184, 0.0012825658991552737, 0.00127101328689605, 0.0013779611445545238, 0.0013755255910967077, 0.0012755444895818221, 0.0012839054902635363, 0.001373519633934662, 0.003067376918862669, 0.0013425724903995894, 0.00137341681544726, 0.0014169269992152648, 0.0013785539598831413, 0.001304934388121628, 0.001445917776139567, 0.0013615722050510195, 0.0013523636129209582, 0.001360454142796902, 0.0013351361837466152, 0.0013562070623952516, 0.0013751768986979614, 0.0030188011646993004, 0.0013557188973134877, 0.001344124019164972, 0.0012762680797058406, 0.0012907866122467177, 0.0012697873266451821, 0.0013100544907798876, 0.0013377398789423157, 0.0012775329606873648, 0.0013762814710296843, 0.0013854503869174086, 0.0013757864897120365, 0.00135665702186905, 0.0014584804291133673, 0.0014526627943565954, 0.001480882937487747, 0.001438605530206494, 0.0013373236941667845, 0.0013390753266154503, 0.001448548467810817, 0.0013327282658607072, 0.0013457741231980677, 0.0012408386326718088, 0.0013620827741426776, 0.0012731311429406004, 0.0012547735704527218, 0.0012499803049984028, 0.0012594771818542968, 0.001266993938622122, 0.0013513762446842631, 0.00145417683738835, 0.0013511334902283792, 0.0013452854710726105, 0.0013589504488496756, 0.013260315428963122, 0.020116099386894126, 0.0014007816124441369, 0.0014706660817586342, 0.0013681407946599076, 0.001381105327579592, 0.001267875245373164, 0.0012836069776201431, 0.0013599140826156552, 0.0012681525728038075, 0.0012913649588139082, 0.0012623837754624535, 0.0012774129388663843, 0.0012876406734885306, 0.001259968530538739, 0.0012964381033326595, 0.0012828746753535708, 0.0013722291011933464, 0.0013763791411088742, 0.013905467244568375, 0.018224327245309035]
[756.6297125345158, 744.7807966129961, 734.5918355193161, 779.8533391503388, 738.5060430408937, 729.8710788747167, 735.3097969364455, 744.7093040155279, 790.734246793049, 787.9785222950271, 42.33176639041654, 68.49694358609813, 108.389473064382, 234.95049762970788, 785.0734402766524, 773.4766339393815, 780.8883411705565, 792.3791633292226, 788.7842984107718, 746.6568367980623, 780.4794139101797, 790.6650029044307, 733.3035130514559, 213.67128166407898, 737.1740111093325, 739.5220444597938, 705.7340588777126, 733.8317827616966, 796.8813777498767, 800.7383792221816, 798.5222583565998, 721.209429682479, 753.7763883030507, 773.9807015394932, 780.2847451804495, 758.2983333131135, 772.7468533026974, 754.5833015680823, 760.552624712011, 758.4063103979273, 756.6472042278729, 715.2247329195744, 739.4724916940651, 757.4310983674392, 70.51686697434498, 140.48605846423024, 124.15683729098303, 667.3461612381644, 720.0277900717102, 734.6473872658672, 683.8588414073779, 721.3980151238301, 728.7856266582203, 729.4982518850592, 564.3953819831581, 691.8406586734834, 52.72906780385746, 39.42215139079385, 143.6471065038503, 717.1565358951557, 772.9842693757877, 788.1715590223492, 749.1247769071593, 759.425925791475, 799.3840891708527, 750.0456034406868, 751.3167711768194, 803.4092369833952, 57.00974938975235, 109.1820679625819, 715.3045643875555, 727.4811948377903, 708.2223610215432, 721.0392033187098, 672.7271424656645, 690.2140951147978, 30.158474561864743, 722.066081707315, 784.2701021234363, 785.3996187445556, 776.5473079059701, 716.0569112777279, 714.9177498165222, 730.7083747231975, 769.2362034514707, 771.3741480702721, 727.205290172229, 717.0943090223927, 759.2357467816676, 756.7357319439576, 754.5251579830785, 57.1953510683074, 94.30340442950202, 721.7234426273162, 726.2960894906853, 669.5437191087473, 644.7757152065668, 685.1154559331909, 744.7179845461853, 739.5394674597081, 746.9698364163354, 742.3755937231102, 744.083278421911, 748.1426937732015, 744.3655922809108, 744.6330154942767, 739.3105573794682, 741.6797183032778, 680.4031552479704, 735.437034174054, 182.45390746874318, 710.3886110446191, 678.06041568012, 753.6127075650485, 790.9659797456333, 789.0393229741735, 793.6197767413724, 786.6715738580551, 790.4420394106761, 791.649065516906, 782.9865644781212, 747.5731293490873, 794.5418026583361, 787.5599211729051, 796.1856023914615, 725.8206443093617, 731.8530791087527, 667.9065190848025, 664.7215465876607, 728.942906916214, 729.928791051762, 736.7582807642002, 658.1598124455264, 730.7708059008618, 732.9722647441725, 672.8045393717126, 714.3645857815674, 669.9191461675964, 732.1140026946091, 723.6687648125762, 727.6690656422588, 727.0120505906962, 721.9290796623101, 722.3759466608409, 724.5203450790111, 724.5121826392019, 666.8670082083904, 663.07099109387, 732.0514729517904, 722.1136895247118, 661.9973424688058, 710.2654760370515, 716.2212866973998, 728.360495572357, 676.5550472932312, 44.34439981958356, 702.6520396116072, 738.8712682457729, 694.7698916977032, 675.4145723404225, 739.4554966508855, 737.1685434142371, 738.827473868389, 728.1070867322936, 733.4045769310477, 667.0465654092276, 715.2139590600331, 46.413712639337795, 160.69525657931024, 116.43419554831627, 755.9508726001744, 753.9063621429924, 683.2579355325458, 677.9008954976189, 761.9483024123809, 755.1066830796116, 751.6598530095652, 757.1706451000365, 756.8377351526917, 763.8833101256481, 760.7187802175927, 754.3845163523763, 702.8362756469754, 688.3819131571547, 694.0542351112533, 762.7135680764227, 759.3015358509888, 31.729346693546816, 704.2372242533518, 777.0410689685349, 701.6745965057366, 765.1688638120431, 761.3673374575442, 721.1201565146415, 687.0247138811471, 43.11900749308284, 417.8033724476276, 693.2244243486246, 756.8972184193951, 697.6075943031655, 761.2675642368552, 762.9903453641342, 771.5600930939956, 761.9259677362091, 698.7875141739595, 761.8775733042684, 706.5401196043498, 724.8672547055185, 716.4197465045565, 662.4638882866635, 717.5870644405489, 66.28306600207553, 38.65750720234285, 369.7766790559621, 727.3472484245458, 691.313126767926, 741.612704523712, 752.0974576775291, 757.948322599474, 804.4147460767191, 800.103288659796, 88.06131540817239, 22.97893111782907, 207.02559176390275, 734.8972130196848, 776.7371345605126, 749.7417333865178, 753.6134604786138, 795.3511363742182, 787.5192414431775, 738.4532889368838, 737.0413839639541, 746.5207970392821, 57.602475857592964, 104.40523864753689, 32.254302994375415, 126.87581982280292, 798.3383355747077, 803.9352405531089, 796.1874382846472, 814.6493233948211, 823.8408655293294, 754.9552650363362, 812.9733286627556, 770.4652305656037, 807.7157748842322, 755.7007829922993, 800.9836335233814, 753.973540330343, 25.615718074540897, 560.5752205984705, 796.1321106815343, 798.5402544312316, 793.6681992399195, 803.1539895791832, 792.1112693576712, 783.6844451851148, 782.7343626601962, 743.4570621821881, 766.3115073315201, 726.0276350049016, 783.0038933286648, 779.1052862832244, 731.8890318826773, 763.2814638268284, 777.9348469993292, 772.0490158945084, 775.2682834066685, 778.763283827046, 30.53157719019077, 207.37991303024958, 722.171106973872, 728.9898438892326, 676.7675560433798, 725.1143568762335, 732.7941943664117, 727.1733237215552, 694.2484884207902, 659.2017845652682, 724.3848000803629, 722.2932077865115, 675.2715835541946, 662.9519084194338, 715.9154654270238, 705.2976307080739, 725.5785402645613, 116.09851786189492, 714.3276779181116, 707.9412390740898, 757.969402333487, 719.201214011188, 752.6397420502593, 750.0869496199999, 750.9484518659893, 698.8668977012377, 748.230258699751, 749.8165370855027, 761.4999893692047, 677.6896156251959, 705.7442638362601, 660.969273143228, 713.7707891827624, 297.4041234190625, 654.0799001814041, 69.28159136089431, 97.87383397036555, 263.9254044300267, 724.124721462499, 728.8557427464543, 711.5009140585311, 732.6569926045328, 712.265455496271, 722.3099139459456, 718.5036648532256, 712.9196107734502, 717.0135743588447, 67.31464596744408, 294.1447787515976, 686.5620676278394, 740.5668221517531, 749.3632664984076, 699.1306105019216, 747.2386900561561, 676.6389446341623, 753.7511232156481, 753.3406157136193, 748.6761759385939, 753.8214045000889, 742.3233313948466, 752.5511528295007, 747.2124711635942, 731.5387357576474, 744.7557224352371, 700.370315546477, 25.79431055301347, 678.0541108537586, 773.8057965259048, 783.4321815421417, 774.3616378731958, 783.8289250688722, 775.9496162607641, 779.295697708625, 782.4735255416326, 771.6788083597107, 760.9759489849671, 772.7499219487445, 752.7713355269393, 772.701606725633, 778.6001408494328, 759.3580657031455, 28.55135751745402, 180.39441472181454, 729.7895498427821, 688.3861384245178, 746.2539269458596, 763.3925179658055, 686.0332933933338, 709.0048994193319, 757.7188785928676, 700.9741544992575, 685.413739062535, 748.604711131187, 746.7487547300643, 40.83112712216544, 705.4919327277421, 747.5757769231673, 322.3307380746783, 778.2973848672208, 761.9454458510994, 744.6305718323193, 706.1204912137063, 760.1705507079437, 763.3596237676069, 133.58213088857687, 243.3200489945827, 253.1217832800734, 673.8172895424225, 750.5253673140056, 749.3298973358774, 742.8250073527282, 746.4969136229543, 737.7543135052522, 679.6017912451067, 760.4291774346175, 700.7681323782209, 704.2141482091504, 22.11019078810698, 689.0104217503521, 726.7580267531041, 778.7717245388668, 729.7703563120363, 790.1744610549467, 793.7917843495694, 787.2587474004655, 792.1130253907816, 717.6582370465594, 743.9620789861244, 716.908365989694, 780.4121265023666, 770.4441523788071, 37.15896622196512, 704.1382406384822, 748.4771203956266, 753.556359357911, 696.7185262768273, 744.3768226269216, 747.2879093386978, 25.71974188674274, 143.69145735597323, 728.2251542537728, 761.2771913400408, 774.8465701684019, 686.6225042160208, 725.1925243647445, 755.1021680208387, 757.2003300037685, 708.8397068530156, 708.0936931757255, 80.71721175736738, 570.3707288201745, 646.1587575169165, 754.0823318808958, 763.6364545950328, 752.207488280331, 758.5171846700655, 751.8658431475548, 711.0169533233301, 750.0784068020154, 739.0890578683254, 21.889085715119432, 742.6378517527943, 707.6525853658056, 726.1327611690467, 779.5195337107587, 781.6595243094156, 776.8440610038023, 777.0941449681892, 723.567395799038, 781.6273420366068, 785.9797382773794, 771.936107192138, 786.6693390340279, 756.3044002360804, 782.236451520517, 677.9058195898026, 660.4362020910187, 28.280398226501497, 725.6358645513241, 731.8515928178055, 751.4408498539195, 752.8107961534419, 733.459445675356, 756.0921423930068, 745.0720585330489, 711.0066769831074, 88.01622096077871, 101.79259503494127, 707.263024864646, 734.3074194764357, 674.5788617710588, 729.0679387004516, 733.7944747311718, 732.0892508035746, 734.161346066324, 646.8163829069533, 647.9267563769303, 641.7302592684543, 696.0237358353003, 707.1974411378484, 645.2807030958246, 155.12114004715954, 375.7269935575912, 740.7833962388319, 742.8976700925405, 743.2122154965692, 732.1074179016924, 699.7111385169728, 741.9709776946571, 27.38865847486379, 222.36326198283396, 645.1045596346968, 782.223751753984, 730.2872016313975, 721.4851582237005, 777.0214282251068, 774.9143221830382, 766.2046695323047, 724.5336622854702, 784.5535062566349, 769.868406606292, 777.2425170490444, 739.6495700731952, 783.185473611358, 785.1878316900999, 728.7740075700546, 714.2273380715633, 779.9531039651512, 726.1796589195245, 672.2391721985302, 727.4505443507157, 727.9969647876804, 667.846810213618, 737.7592687601025, 741.6181142076771, 777.5440089094427, 777.6950214645719, 54.676722156897725, 56.0344434427665, 225.56048669954063, 659.8577340658394, 753.3609895605022, 705.8458348781811, 720.2074568627936, 285.64553441301706, 772.4716581402271, 770.092172192501, 743.4413042986504, 761.5298728003662, 714.1092270029498, 94.60550674228593, 30.487631743373033, 96.79082052627541, 424.1929755704467, 738.9551725761038, 703.6195803619719, 728.1778508440486, 755.3881687128451, 760.8594872896715, 761.9394638315113, 767.0785659126749, 39.78488423576596, 35.44598661754296, 624.1188865214948, 664.2280286383864, 689.343119541643, 673.8663381039877, 634.4010961514325, 680.4366208504514, 689.3356141867001, 153.4867149271747, 157.52006938064346, 200.6695153811847, 696.7990149576411, 698.4132349086242, 760.6658279501975, 736.2897105632397, 775.2231722779688, 768.8499777706573, 767.2716247964879, 760.8211752336616, 710.0997483198445, 744.3298959587728, 767.9460455985682, 713.8874043653311, 718.5731130329696, 756.7892487677224, 762.6542118372014, 761.4244480665923, 768.639689570561, 701.0204856580527, 772.9519685172068, 765.7322137559452, 729.1333561543134, 743.669652721561, 747.0663423427559, 137.3716081905205, 176.59042827900535, 702.5584063528548, 739.1084786506045, 695.4666809919364, 699.6172093653859, 744.3665203414383, 737.0109640561686, 725.1111589744511, 720.6104368770891, 726.6222566732401, 674.9267142810877, 38.047398530215176, 115.52016124577891, 691.5528010112747, 698.9511136806619, 708.9164261508475, 767.1082638592146, 775.9866402220196, 745.913445200645, 729.7898117695265, 721.2837329295234, 729.4077733019077, 707.9461079486282, 747.6021009905076, 693.3572804562074, 29.362343914814467, 615.0656383580128, 713.7029958568845, 672.9176413071459, 661.3048965946136, 712.5756148486997, 722.0607515982807, 706.7967807899464, 754.6408493276975, 755.6498673135083, 715.1000640372048, 698.1587561937098, 748.4831457849316, 757.851960944682, 764.3960072774192, 760.876843616949, 709.3540689185153, 747.0630287682974, 712.353005322946, 769.8467203919673, 697.8920868926982, 774.7498343131549, 761.8205987751701, 732.2003074120065, 772.8237683277803, 681.6885215377501, 780.6208235753905, 774.6589765836096, 773.0735760282574, 772.9131613744571, 727.1425911392746, 769.3282332087689, 771.3665949718435, 706.3736598739702, 771.3757016830688, 736.4149512601424, 753.7776990507577, 775.7540934831181, 704.3510783865969, 619.2652325330406, 83.83368312935585, 368.2980023529969, 729.1194373701503, 667.6230472945842, 730.306237356734, 735.2228578448348, 676.6436437354563, 727.0899809386825, 735.412883285014, 737.7172158648397, 733.2097955010381, 754.596431917628, 774.971581258487, 738.5964561927992, 783.3203157201496, 782.8644321859414, 768.1295213141186, 779.1600192077152, 788.2719297385936, 716.8291930834283, 766.0104826177957, 36.2824531426878, 94.77746149197274, 710.2327188307478, 703.405042620151, 656.8651478917212, 656.3328798513832, 697.2154009457133, 710.693882035355, 766.5127174517221, 764.8265085453273, 761.9164552417332, 758.6459914452645, 755.2352406359537, 746.0852031287426, 136.50430495245024, 40.34959488500786, 670.557689171562, 747.2938585450886, 716.8953703533682, 593.3067561683017, 718.674552569394, 702.8438267104854, 297.63810559788516, 737.8975319470375, 722.2963288221459, 629.1869753510482, 670.791165721927, 665.0256629528909, 657.0123843133663, 651.9459841438639, 708.5057945891718, 725.1840356652604, 710.0030095241141, 657.4561709881531, 718.8654744025215, 757.1633212838556, 732.0794067955975, 712.6777946965143, 616.1312772719963, 731.6494397085495, 768.4577995778845, 771.670701787221, 768.974810322883, 775.2639167089197, 730.8060864979301, 771.4720096199559, 770.3235285322689, 711.799521067615, 715.6847213793731, 707.2337073522128, 703.9067537183968, 712.1586957277678, 720.6031345867272, 575.0563620022706, 746.4594663801538, 734.3271842214272, 687.6855266978968, 52.54102366864938, 255.9127144645787, 219.82149131060143, 716.8496750731612, 724.7375609643345, 733.2057569083624, 708.4410139973196, 774.2221804356235, 768.0156161311104, 765.7079948880712, 777.9719743789667, 755.9974782457873, 41.2091652791983, 119.72751485202103, 675.3055885470997, 665.1004948617012, 737.1091708884195, 685.0500640680735, 739.0104611062361, 680.9104232812849, 734.4548723277695, 734.3446272236534, 671.6074128234981, 738.9647677487119, 732.0615568502661, 39.541432170626145, 121.64275945086465, 663.7367606176219, 760.0824666013978, 711.7205109171201, 704.7876726417893, 767.7249438090136, 748.3898849071217, 761.8951553978014, 705.2786780393467, 748.1600692080085, 725.0636047567043, 767.7037380427308, 765.8294041324256, 715.1292536826859, 717.591142006998, 763.9154522626169, 715.4963632675333, 674.7118296160106, 754.9425974753401, 735.3603928567592, 759.7430930188181, 709.5711724818547, 640.2708346751955, 644.539935247216, 750.0838016805455, 55.75219285063872, 180.73810162530268, 669.9084859746665, 753.9635055168064, 734.4154531721758, 737.7180485489125, 748.9651929197315, 783.5610550588556, 760.1888311948574, 756.1916979289126, 694.5815199181856, 696.0763170450125, 751.8373713957745, 720.0173673938531, 715.1529364067425, 695.567970297212, 761.7506171392483, 753.8131490130844, 758.7951442142066, 748.1024418771467, 768.1109438397642, 758.9396659395538, 701.1787115424637, 760.0864992426417, 765.4576396443028, 693.819957280469, 714.0953028658495, 725.2948654405199, 691.3233583569461, 734.5274820556428, 727.8011510591988, 666.799146182626, 717.3850682207784, 633.8868591938782, 745.0122685344077, 734.9971085044918, 723.8005693259051, 676.3269148851032, 25.0041577820162, 50.93384936267489, 695.4190174653872, 740.3587650437518, 735.9422970686128, 742.2512521721079, 733.2340216048228, 713.7503701924642, 710.3296952568572, 749.2952572779178, 749.1472140677615, 742.8278232902729, 753.5052574018335, 688.3706471568904, 686.9104021543174, 630.474902351215, 666.1627488361792, 651.3784686239145, 650.5359844793306, 119.9718532299797, 705.8505128067926, 707.8505168896149, 718.3515947563528, 710.0172272600487, 769.6765047162698, 763.8016027514502, 763.3355782811121, 644.5003109330758, 727.9466191335748, 728.1026935579856, 667.0294309092965, 692.2282636454017, 659.7510134354158, 665.1636502432495, 10.88009806440049, 691.0809678160646, 655.2380839086617, 704.9224636126875, 639.2238556678616, 658.8235214507292, 694.2214295840923, 713.0847185182737, 703.3330455178552, 21.010595339895396, 419.2728969141834, 622.5144277709553, 694.5324029785537, 680.6488424252619, 522.6027166419864, 678.9825416936058, 692.0419034226179, 709.362675241373, 508.2209300766436, 741.5492825194876, 747.7943115670287, 126.24261605744658, 83.13222743166378, 110.94479781979854, 737.718614878595, 744.7956163025127, 754.2058743306804, 759.6242294856438, 763.5807986450548, 721.6657573087663, 762.7742508967391, 762.8238265953601, 707.1360644107931, 765.7011039487085, 759.5327412830593, 764.2057981523919, 752.8800349177583, 756.9799928292185, 699.6619633524576, 754.1920845806479, 753.3784100060899, 752.884511321205, 751.4561182322147, 751.762268695669, 73.90749139411815, 725.3002446536602, 684.5828997380918, 673.7869267462067, 725.3102821487787, 723.6303021020402, 684.7594263296252, 688.522081681123, 729.6461598945571, 715.5291176210172, 676.3159096038055, 728.915028877263, 731.0725138251371, 38.24097750044589, 115.49686355727432, 657.9131816713029, 667.1782330631678, 721.1721089563569, 727.3329540825823, 759.4670162908386, 750.6146086347734, 759.501754913343, 759.5305044893221, 770.4937371714132, 717.7678614923498, 678.132635714049, 730.0146560052253, 718.8588826827478, 770.9678677631324, 772.5925161032048, 764.4958163519256, 771.2793216835962, 764.1750730779863, 757.3199227292814, 764.8416113696229, 763.4235844654388, 759.8407949066468, 761.3642387306816, 746.2334011897758, 728.9609636413944, 797.5452144237792, 797.3008526834896, 738.77768088364, 735.6538759822557, 777.2307295302423, 785.3730949687408, 725.1060511343699, 787.2689665360352, 788.1065010196527, 779.6870325794738, 786.7738365207075, 725.7098677650207, 726.9948348999447, 783.9789267780402, 778.8735289189694, 728.0565747249957, 326.01145097316015, 744.8387384299602, 728.1110794280942, 705.7526609019583, 725.397792977773, 766.3220535090948, 691.6022587881063, 734.445074811533, 739.4461004759725, 735.0486639293412, 748.9872659984638, 737.3505327673637, 727.1791730553468, 331.2573254885467, 737.6160367622037, 743.9789675220916, 783.5344438219313, 774.7213912138582, 787.5334546313604, 763.3270272633398, 747.529482929559, 782.7586690694534, 726.5955555238537, 721.7869433960564, 726.8569705240443, 737.1059773252874, 685.6451276537974, 688.3910043575625, 675.2728218318567, 695.1175836620496, 747.762119494224, 746.7839785589415, 690.3462481385205, 750.3405049747156, 743.0667470582822, 805.9065648582941, 734.169772192751, 785.4650367677451, 796.9565374565551, 800.0126049996267, 793.9802438720843, 789.2697585337445, 739.9863686620013, 687.6742733682682, 740.1193199873775, 743.3366534484976, 735.8620035384513, 75.41298737251789, 49.711426691971496, 713.8871549399925, 679.9640057001873, 730.918925817558, 724.0577384148645, 788.7211329736765, 779.0546619293381, 735.340572454844, 788.5486505689622, 774.3744269772344, 792.1521326853765, 782.832214684964, 776.6141755143209, 793.6706161799285, 771.344191002542, 779.4993690435051, 728.7412860799695, 726.5439951337494, 71.9141602660357, 54.87171002470893]
Elapsed: 0.15103123987361322~0.3250277807418868
Time per graph: 0.003082270201502311~0.006633220015140547
Speed: 661.5487604865953~202.8730371394854
Total Time: 0.8964
best val loss: inf test_score: 0.0000

Testing...
Test loss: 0.2429 score: 0.9184 time: 0.09s
test Score 0.9184
Epoch Time List: [0.32660016184672713, 0.3218054490862414, 0.3938263200689107, 0.3281263851094991, 0.3208367241313681, 0.32987178303301334, 0.3189153008861467, 0.36945496092084795, 0.33579947997350246, 0.327330493950285, 1.4136506370268762, 8.064478895161301, 2.6484901640797034, 1.3466251919744536, 0.791385494871065, 0.3209166090236977, 0.32727601984515786, 0.3246773611754179, 0.32610511395614594, 0.3290600188774988, 0.3227558000944555, 0.3231145069003105, 0.33391408307943493, 5.192906555952504, 0.9185419549467042, 0.34168969094753265, 0.35484900791198015, 0.3255407069809735, 0.3241060159634799, 0.31414199201390147, 0.3251491420669481, 0.3334731929935515, 0.3453267910517752, 0.34065268689300865, 0.33006096887402236, 0.3515259630512446, 0.33186741010285914, 0.3408085670089349, 0.34503403794951737, 0.3449877310777083, 0.33649325696751475, 0.34866378805600107, 0.33900226501282305, 0.3393885479308665, 1.327899423893541, 4.540627957088873, 2.1585350779350847, 0.8389999868813902, 0.3466189770260826, 0.3471061891177669, 0.35166431579273194, 0.3432915919693187, 0.34366404998581856, 0.34301820409018546, 0.35968269198201597, 0.3674976989859715, 1.8839392440859228, 2.6761979589937255, 2.1825801201630384, 1.049961820943281, 0.3390514929778874, 0.32889488304499537, 0.3251790431095287, 0.32377091399393976, 0.3214923780178651, 0.3273049920098856, 0.3188480429816991, 0.3144057639874518, 3.796347885974683, 8.1209654341219, 0.4311568171251565, 0.35554768587462604, 0.3586442330852151, 0.3563088378868997, 0.3527742519509047, 0.352823346038349, 6.302943927934393, 0.8418394098989666, 0.3323720870539546, 0.3244832259370014, 0.33080992894247174, 0.3338668519863859, 0.36303490097634494, 0.3484336379915476, 0.34517451794818044, 0.3315365739399567, 0.3354697360191494, 0.33638126496225595, 0.3450286128791049, 0.3393976829247549, 0.34122289705555886, 2.209568023099564, 1.9420918080722913, 0.3601619660621509, 0.3522231130627915, 0.35023762797936797, 0.36253334302455187, 0.3541899989359081, 0.343826569034718, 0.3397510180948302, 0.3373864550376311, 0.33650068507995456, 0.33678696886636317, 0.34099529299419373, 0.33807966101448983, 0.33647707500495017, 0.33973958890419453, 0.3417512798914686, 0.3496207151329145, 0.35774509399197996, 4.269721527118236, 0.605136388912797, 0.34937198692932725, 0.3399470141157508, 0.3358042559120804, 0.33218001388013363, 0.3267733689863235, 0.33181365788914263, 0.32202182803303003, 0.3284684410318732, 0.3324559648754075, 0.32359825703315437, 0.3307339318562299, 0.3305399409728125, 0.31659840303473175, 0.33688851399347186, 0.3467356018954888, 0.34920039190910757, 0.3585623591206968, 0.3460348730441183, 0.34588989592157304, 0.3451710370136425, 0.35021798603702337, 0.34922195703256875, 0.3406694281147793, 0.34681219107005745, 0.3599408849840984, 0.35392186185345054, 0.351064958027564, 0.35175905807409436, 0.3570487340912223, 0.34816248703282326, 0.34551078104414046, 0.34836666577029973, 0.35718214488588274, 0.35485619690734893, 0.3528207989875227, 0.3526305948616937, 0.3530226150760427, 0.35003633110318333, 0.3575528039364144, 0.3586359379114583, 0.3625550731085241, 0.36115345906000584, 0.35672241600696, 3.611576601048, 0.5035285770427436, 0.3523877590196207, 0.3378527278546244, 0.3441723920404911, 0.347702061990276, 0.34493139397818595, 0.3487901670159772, 0.35009472689125687, 0.3514780189143494, 0.34908232605084777, 0.3529223409714177, 3.495209724875167, 2.932991707115434, 1.6781245010206476, 0.3964976300485432, 0.34539113205391914, 0.3405182260321453, 0.34823861613404006, 0.33536289096809924, 0.3384031009627506, 0.3369375340407714, 0.33931080193724483, 0.3380134040489793, 0.3297421848401427, 0.3396804379299283, 0.3436887818388641, 0.33531151490751654, 0.3429047710960731, 0.33829234493896365, 0.33800665894523263, 0.3318531708791852, 3.467263223021291, 3.7080046200426295, 0.3554401269648224, 0.3376922579482198, 0.339992014109157, 0.34097064984962344, 0.3384602880105376, 0.3579026000807062, 5.871244700974785, 0.8606928100343794, 0.46523384493775666, 0.3476053300546482, 0.3398985269013792, 0.3438338930718601, 0.33651381998788565, 0.32917851605452597, 0.34749383199959993, 0.34002957097254694, 0.3447587580885738, 0.3422429470811039, 0.3497083930997178, 0.3594329620245844, 0.35343203123193234, 0.35511494288221, 1.0240514650940895, 4.670388731057756, 6.5855670620221645, 0.6101160378893837, 0.34159460105001926, 0.34989704797044396, 0.3426584380213171, 0.3434694459429011, 0.3272646788973361, 0.3220152298454195, 1.1918389501515776, 4.934551640995778, 1.994765130104497, 0.5129701402038336, 0.3346327490871772, 0.33745102013926953, 0.3254424158949405, 0.3273548159049824, 0.32490866794250906, 0.32657713105436414, 0.4100297500845045, 0.3253509340574965, 1.11135126685258, 3.045126480050385, 2.2737302610184997, 6.8633634420111775, 1.0273161277873442, 0.3235800957772881, 0.3158875380177051, 0.3148463418474421, 0.3143965151393786, 0.31406658713240176, 0.31330789695493877, 0.3131876199040562, 0.3208164209499955, 0.3182946159504354, 0.32246626110281795, 0.31371166405733675, 6.4942224560072646, 0.9778928949963301, 0.340530876070261, 0.3218637170502916, 0.32595828908961266, 0.3231059020617977, 0.32194921385962516, 0.3294167050626129, 0.33398340397980064, 0.3373849631752819, 5.405792796169408, 0.3388105269987136, 0.32660316803958267, 0.3230359489098191, 0.32794292899779975, 0.33834538410883397, 0.3389446820365265, 0.3294994489988312, 0.3280781051144004, 0.324690421926789, 1.8701919849263504, 5.230003613978624, 1.0592427219962701, 0.34922083909623325, 0.3586196060059592, 0.35877331893425435, 0.36093812994658947, 0.3538528879871592, 0.35039843199774623, 0.3626616479596123, 0.3516011079773307, 0.35463362210430205, 0.3519493010826409, 0.36375516396947205, 0.36140821990557015, 0.35527547379024327, 0.3525257569272071, 5.250566111877561, 0.7177382279187441, 0.3452128949575126, 0.33449813595507294, 0.33041806309483945, 0.3331150838639587, 0.33936096099205315, 0.3396713499678299, 0.33612023014575243, 0.3392387378262356, 0.3418874980416149, 0.34329176996834576, 0.36144488712307066, 0.35467597807291895, 0.35601988492999226, 0.36267009703442454, 0.44991031393874437, 0.3576922760112211, 2.6252626040950418, 4.466619957820512, 1.198562851990573, 1.3507290340494365, 0.3479601979488507, 0.3456183950183913, 0.3525888390140608, 0.35178625595290214, 0.3484665510477498, 0.35804365994408727, 0.3548733899369836, 0.3628928699763492, 1.5401184358634055, 4.082686243928038, 0.6549057350493968, 0.35296264418866485, 0.3482668639626354, 0.34125926182605326, 0.3436299320310354, 0.34560395509470254, 0.3373750610044226, 0.34411184198688716, 0.33962860400788486, 0.34268513903953135, 0.34388854703865945, 0.3472541010705754, 0.35047832096461207, 0.34671594901010394, 0.34505815198644996, 0.34883884817827493, 9.41683678003028, 5.626108175958507, 0.33905997802503407, 0.3289446720154956, 0.33620957797393203, 0.3283659120788798, 0.33347051008604467, 0.3335947401355952, 0.32350848603527993, 0.32812175212893635, 0.3320037960074842, 0.3345562650356442, 0.3361553620779887, 0.330152023001574, 0.326488027931191, 0.3379890250507742, 2.879852144047618, 3.007539666024968, 0.5337783008581027, 0.3416271370369941, 0.34005118592176586, 0.3347165009472519, 0.33862673002295196, 0.3495998550206423, 0.3448487708810717, 0.3449689280241728, 0.34294732694979757, 0.34371254686266184, 0.33803529490251094, 1.482365301111713, 3.750160238938406, 0.35045210900716484, 0.4316035220399499, 0.34232158097438514, 0.3414191521005705, 0.34122967696748674, 0.3342339029768482, 0.4003024658886716, 0.3427941169356927, 4.515039331978187, 1.6124614039435983, 1.9824963500723243, 0.6434021468739957, 0.3441612448077649, 0.34584473608992994, 0.3388585631037131, 0.3385798790259287, 0.3448967100121081, 0.34037905698642135, 0.3423235999653116, 0.3445860930951312, 0.3491730659734458, 3.2536645989166573, 4.138125097961165, 0.33441937912721187, 0.33129694988019764, 0.32547800010070205, 0.3260989061091095, 0.32549515704158694, 0.3244816579390317, 0.3241365069989115, 0.3335193219827488, 0.3428842939902097, 0.33787925401702523, 0.3343506009550765, 0.34120277396868914, 3.913558497093618, 3.4354896809672937, 0.3435143759706989, 0.3483543648617342, 0.35639852890744805, 0.3403969219652936, 0.33924376603681594, 2.1800950500182807, 6.594326621037908, 0.5877257710089907, 0.34171896195039153, 0.33545227197464556, 0.33708227204624563, 0.3323687689844519, 0.33538133394904435, 0.337872851989232, 0.34282239293679595, 0.33430105703882873, 3.295809278031811, 1.3945835910271853, 0.37477053701877594, 0.3519174960674718, 0.32903035497292876, 0.3420789251103997, 0.33964370796456933, 0.33642329508438706, 0.34203627589158714, 0.3356620860286057, 0.34300554590299726, 4.77713236794807, 1.3247730829752982, 0.3503381269983947, 0.3519836670020595, 0.39078084006905556, 0.3283262699842453, 0.3284035489195958, 0.3387288320809603, 0.3257903358899057, 0.4013911240035668, 0.3241902479203418, 0.3314066178863868, 0.3204480019630864, 0.3344777339370921, 0.3267817080486566, 0.3375990450149402, 0.3550875049550086, 4.658120635780506, 0.7218838450498879, 0.34902359591796994, 0.3492825219873339, 0.3383541499497369, 0.3339744480326772, 0.34056374698411673, 0.33934305305592716, 0.3364981299964711, 1.693883687024936, 2.4150753038702533, 0.4321203799918294, 0.34963310800958425, 0.34672893583774567, 0.34956989996135235, 0.3571493459166959, 0.3454527418361977, 0.3458774311002344, 0.36309075192548335, 0.36762196000199765, 0.36443937523290515, 0.35973367013502866, 0.3594632151070982, 0.3636306420667097, 5.750273868092336, 1.1215815920149907, 0.35436634405050427, 0.3405627568718046, 0.3501396661158651, 0.35331501718610525, 0.36196594196371734, 0.34670986502896994, 2.7933471020078287, 4.992419317131862, 1.1086794820148498, 0.34589711914304644, 0.3339163080090657, 0.3327920079464093, 0.321545390994288, 0.32528840110171586, 0.3345257989130914, 0.3300728611648083, 0.32824940083082765, 0.3266681699315086, 0.332338452921249, 0.3277234077686444, 0.3269727770239115, 0.3267676739487797, 0.32745365193113685, 0.3462447130586952, 0.3259412889601663, 0.34047114406712353, 0.3574514329666272, 0.3578396779485047, 0.3541059630224481, 0.35656292201019824, 0.34251524799037725, 0.3397338291397318, 0.32449416595045477, 0.3360156511189416, 1.6813233100110665, 3.36711879784707, 2.523500338080339, 0.8005382849369198, 0.35590857500210404, 0.3384563549188897, 0.3456656220369041, 0.43783363595139235, 0.32992731407284737, 0.33930665196385235, 0.3375177870038897, 0.3423533661989495, 0.3854031590744853, 1.4449709189357236, 5.643933737068437, 2.4211430300492793, 1.1228751568123698, 0.3772408489603549, 0.3388865049928427, 0.3345609491225332, 0.33589228393975645, 0.3371099838986993, 0.3335982918506488, 0.33218543394468725, 2.3483585770009086, 7.871056988835335, 4.408263724995777, 0.3586547620361671, 0.3639823308913037, 0.36920692410785705, 0.3727742818882689, 0.3730593918589875, 0.36522316420450807, 2.4859518710291013, 1.6261422949610278, 1.3411738330032676, 0.3775158920325339, 0.3717413969570771, 0.33918873395305127, 0.3352950259577483, 0.32950668199919164, 0.3346514009172097, 0.32928934693336487, 0.3375539700500667, 0.33261917904019356, 0.33166412892751396, 0.33645523991435766, 0.33543912088498473, 0.3434740810189396, 0.3370042620226741, 0.33687332703266293, 0.3380462370114401, 0.33042639098130167, 0.3441724090371281, 0.33052753598894924, 0.3352270049508661, 0.3390299609163776, 0.33701852580998093, 0.344209388946183, 0.9124419610016048, 4.374978341045789, 0.7887218128889799, 0.3558546550339088, 0.34419020696077496, 0.33939995104447007, 0.33790872478857636, 0.34310714807361364, 0.3434861060231924, 0.3555824711220339, 0.34696317894849926, 0.3492243679938838, 3.2785092500271276, 5.015983817982487, 0.7261748821474612, 0.36315773194655776, 0.35803602694068104, 0.4249617501627654, 0.3289584858575836, 0.3381251699756831, 0.35511683695949614, 0.3550667241215706, 0.3558925752295181, 0.35068015195429325, 0.34320475615095347, 0.3431001699063927, 3.1588786608772352, 1.6356814940227196, 0.36198633385356516, 0.362522306968458, 0.35950047487858683, 0.36462862498592585, 0.35705109708942473, 0.34836762794293463, 0.34490367001853883, 0.33360660495236516, 0.345338886952959, 0.35736432508565485, 0.34724504698533565, 0.33301983296405524, 0.3330832280917093, 0.3349707762245089, 0.34541542804799974, 0.3464208469958976, 0.33790195593610406, 0.339959658915177, 0.3374597759684548, 0.3342865378363058, 0.3352202851092443, 0.3365925248945132, 0.33456708304584026, 0.34202238998841494, 0.3386423789197579, 0.3346880851313472, 0.3370452548842877, 0.3314532870426774, 0.3364005599869415, 0.33989791199564934, 0.3361147380201146, 0.3382872008951381, 0.33357505092862993, 0.32993781415279955, 0.3332745609804988, 0.33032939792610705, 0.3344333480345085, 0.34599867393262684, 1.8479408031562343, 1.4316387920407578, 0.6275829689111561, 0.352065910003148, 0.3590751870069653, 0.34960074187256396, 0.3576796499546617, 0.34992313489783555, 0.3520057121058926, 0.3454498619539663, 0.3318021559389308, 0.33630420884583145, 0.3291895758593455, 0.3264069890137762, 0.32596483000088483, 0.33147607184946537, 0.3344637098489329, 0.33499192993622273, 0.32812792796175927, 0.3308606119826436, 0.33169654896482825, 3.619951055967249, 6.458885362022556, 0.78447407099884, 0.3636629299726337, 0.3625875379657373, 0.36299946205690503, 0.3464137848932296, 0.33379223698284477, 0.3333720969967544, 0.3361208630958572, 0.3341693369438872, 0.3374009720282629, 0.34066808701027185, 0.34248177404515445, 1.1901106350123882, 2.801426236052066, 6.870765182888135, 0.3524238090030849, 0.39535786118358374, 0.3551146329846233, 0.3915222220821306, 0.35016649996396154, 0.44517248100601137, 0.3458946291357279, 0.3526887510670349, 0.3592749967938289, 5.40937428898178, 0.36034324287902564, 0.3593217780580744, 0.3574004218680784, 0.3530946710379794, 0.34627271397039294, 0.35289557196665555, 0.3648307240800932, 0.35217715392354876, 0.35230553697329015, 0.33665574388578534, 0.330019410001114, 0.34545754292048514, 0.32812859711702913, 0.33087377494666725, 0.3344475011108443, 0.32825478992890567, 0.3334016229491681, 0.3307951500173658, 0.3444502209313214, 0.33594696514774114, 0.3377021460328251, 0.3339847750030458, 0.34370385902002454, 0.3403789548901841, 0.3426942570367828, 0.35494314203970134, 0.3766600890085101, 0.3515439569018781, 0.34136017598211765, 0.3534703549230471, 4.736089746118523, 1.2871985129313543, 1.509419519919902, 0.8377186270663515, 0.3369146710028872, 0.3328493299195543, 0.3392879160819575, 0.33097255893517286, 0.33329194493126124, 0.33360575209371746, 0.3356900099897757, 0.340440361876972, 3.8962125399848446, 5.2154819059651345, 0.6071778080658987, 0.3552839020267129, 0.34633178310468793, 0.3566661039367318, 0.35503934195730835, 0.3600612849695608, 0.35501019889488816, 0.35150649305433035, 0.3578076650155708, 0.3503828450338915, 0.35147578001488, 1.5306197711033747, 4.085126259131357, 0.5221671549370512, 0.34495308494661003, 0.3352598921628669, 0.34304238692857325, 0.3413472209358588, 0.3381196389673278, 0.34199069789610803, 0.3418945971643552, 0.340760161052458, 0.33734161395113915, 0.34006080497056246, 0.33455859206151217, 0.33624918409623206, 0.3409390830202028, 0.3371992588508874, 0.34173018392175436, 0.3479028770234436, 0.3401981448987499, 0.4139435888500884, 0.3519299188628793, 0.33714406308718026, 0.36814654886256903, 0.374375231214799, 0.3970428081229329, 2.338598703034222, 2.476679018000141, 1.863900202093646, 0.342684522154741, 0.37047437811270356, 0.3313427969114855, 0.34095676301512867, 0.3340747479815036, 0.33429078792687505, 0.3426034750882536, 0.337986950064078, 0.3380554241593927, 0.3364057190483436, 0.34015684807673097, 0.34754837898071855, 0.3530747409677133, 0.3410447750939056, 0.33432084310334176, 0.3400085490429774, 0.3393994269426912, 0.33210801100358367, 0.34111063193995506, 0.33758974506054074, 0.34046361793298274, 0.33187635499052703, 0.33627984300255775, 0.3362741710152477, 0.3474597688764334, 0.3427647309144959, 0.3411678069969639, 0.3436904269037768, 0.3481792079983279, 0.34840501996222883, 0.355749384034425, 0.36159127892460674, 0.3541086360346526, 0.3429486630484462, 0.34649898495990783, 2.6084452371578664, 4.611171733122319, 5.04526013310533, 0.3472151670139283, 0.3389069240074605, 0.340525271021761, 0.3435116938780993, 0.33771005808375776, 0.3452182658948004, 0.3429450870025903, 0.34764878696296364, 0.3391728119459003, 0.337520121014677, 0.3472339319996536, 0.36960959585849196, 0.38296051998622715, 0.3698887978680432, 0.3696628500474617, 0.3570186300203204, 6.9393599421018735, 0.6216729630250484, 0.3339078798890114, 0.33160613011568785, 0.3301335669821128, 0.32796155486721545, 0.3324834778904915, 0.33189156698063016, 0.3378600630676374, 0.3425153900170699, 0.344251852016896, 0.3535479340935126, 0.3636342550162226, 0.3534197910921648, 0.3670499259606004, 6.331085819052532, 4.0503186479909346, 0.3558078871574253, 0.416547138011083, 0.35996965400408953, 0.3603973010322079, 0.3716552380938083, 0.3688348749419674, 0.4589640611084178, 2.748416441027075, 4.015510409837589, 0.558227182016708, 0.3650359530001879, 0.37319825403392315, 0.3811615880113095, 0.34657199890352786, 0.3489954248070717, 0.33867526112589985, 0.41256226191762835, 0.3637029799865559, 0.34612873708829284, 0.6603664569556713, 3.558765879017301, 3.580690592993051, 2.821221360936761, 0.33673523215111345, 0.3332126218592748, 0.3384901009267196, 0.33675094705540687, 0.3308740290813148, 0.33718436607159674, 0.33137844293378294, 0.33348021807614714, 0.33393924310803413, 0.3389898990280926, 0.34065649297554046, 0.3349554701708257, 0.3398223250405863, 0.33471914706751704, 0.34040823788382113, 0.3351783480029553, 0.34077397792134434, 0.33978297421708703, 0.3343848449876532, 1.7714297788916156, 0.41447726101614535, 0.3482485660351813, 0.35004238900728524, 0.34309615788515657, 0.347758152987808, 0.3503454279853031, 0.34809359500650316, 0.34857768402434886, 0.35455518204253167, 0.35618439596146345, 0.35570996499154717, 0.3454241550061852, 3.1146591000724584, 2.962528361938894, 1.4336332210805267, 0.34882805007509887, 0.3497846870450303, 0.33963355608284473, 0.321091532940045, 0.3384918631054461, 0.3287024849560112, 0.33289707894437015, 0.32703324407339096, 0.3275087069487199, 0.33820486010517925, 0.33632769412361085, 0.33192020596470684, 0.32919907907489687, 0.32126118685118854, 0.3211362389847636, 0.3228060060646385, 0.32285723404493183, 0.32277654798235744, 0.322944491985254, 0.32461522601079196, 0.3244226441020146, 0.32591395208146423, 0.3213537058327347, 0.3381819579517469, 0.338233140995726, 0.32099818089045584, 0.32834325497969985, 0.32821502222213894, 0.3298306619981304, 0.32657057396136224, 0.3258383610518649, 0.32446476398035884, 0.3229225490940735, 0.3300429788650945, 0.3238363090204075, 0.3333056539995596, 0.32906905200798064, 0.32574585010297596, 0.3254339079139754, 0.3266320509137586, 0.41726659110281616, 0.3241199131589383, 0.33102599799167365, 0.3321308981394395, 0.3376188410911709, 0.3637811669614166, 0.34897103998810053, 0.34071024996228516, 0.34281898802146316, 0.43618964485358447, 0.3473713102284819, 0.3545144301606342, 0.3467289009131491, 0.43391212390270084, 0.32289804494939744, 0.3267118231160566, 0.32419503189157695, 0.32918662892188877, 0.32279586512595415, 0.32344905100762844, 0.32486556097865105, 0.3224854131694883, 0.32799412589520216, 0.3265962810255587, 0.32574504299554974, 0.3418417169013992, 0.3452223640633747, 0.3508418219862506, 0.3466917958576232, 0.34177280496805906, 0.34139557590242475, 0.3378810960566625, 0.3465518788434565, 0.3395441729808226, 0.3385437710676342, 0.330855845939368, 0.32614525698591024, 0.329750484903343, 0.34126963187009096, 0.31617336499039084, 0.3175066302064806, 0.32269842305686325, 0.3350868009729311, 0.3507914589717984, 0.33925195992924273, 0.3409003739943728, 0.34591629612259567, 1.7704919900279492, 2.540828000055626, 5.476408444927074, 0.34679094911552966, 0.3423210069304332, 0.3242832910036668, 0.32049344712868333, 0.3284057410201058, 0.32622751186136156, 0.3171079258900136, 0.332369432086125, 0.32552668103016913, 0.3283018060028553, 0.34296228212770075, 0.3176029701717198, 0.3184938010526821, 0.3227984021650627, 0.3466787301003933, 0.3531753468560055, 1.6711639469722286, 3.8078962399158627]
Total Epoch List: [1000]
Total Time List: [0.8964204690419137]
========================k_idx:1========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: AEtransGAT
AEtransGAT(
  (gae): GAE(
    (encoder): transGAT(
      (encoder): EdgeFeatureGAT()
      (classifier): Sequential(
        (0): Linear(in_features=64, out_features=64, bias=True)
        (1): ReLU(inplace=True)
        (2): Linear(in_features=64, out_features=2, bias=True)
      )
    )
    (decoder): InnerProductDecoder()
  )
)
Total number of parameters: 1914242
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x75feb28b18a0>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 1.5316;  Loss pred: 1.5316; Loss self: 0.0000; time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6930 score: 0.5102 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6944 score: 0.4898 time: 0.06s
Epoch 2/1000, LR 0.000000
Train loss: 1.5118;  Loss pred: 1.5118; Loss self: 0.0000; time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6930 score: 0.5102 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6943 score: 0.4898 time: 0.07s
Epoch 3/1000, LR 0.000030
Train loss: 1.5171;  Loss pred: 1.5171; Loss self: 0.0000; time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6929 score: 0.5102 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6943 score: 0.4898 time: 0.07s
Epoch 4/1000, LR 0.000060
Train loss: 1.5264;  Loss pred: 1.5264; Loss self: 0.0000; time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6929 score: 0.5102 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6941 score: 0.4898 time: 0.07s
Epoch 5/1000, LR 0.000090
Train loss: 1.5129;  Loss pred: 1.5129; Loss self: 0.0000; time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6928 score: 0.5102 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6939 score: 0.4898 time: 0.07s
Epoch 6/1000, LR 0.000120
Train loss: 1.4936;  Loss pred: 1.4936; Loss self: 0.0000; time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6928 score: 0.5102 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6937 score: 0.4898 time: 0.07s
Epoch 7/1000, LR 0.000150
Train loss: 1.4921;  Loss pred: 1.4921; Loss self: 0.0000; time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6927 score: 0.5102 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6935 score: 0.4898 time: 0.07s
Epoch 8/1000, LR 0.000180
Train loss: 1.4721;  Loss pred: 1.4721; Loss self: 0.0000; time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6927 score: 0.5102 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6933 score: 0.4898 time: 0.07s
Epoch 9/1000, LR 0.000210
Train loss: 1.4674;  Loss pred: 1.4674; Loss self: 0.0000; time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6926 score: 0.5102 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6931 score: 0.4898 time: 0.07s
Epoch 10/1000, LR 0.000240
Train loss: 1.4580;  Loss pred: 1.4580; Loss self: 0.0000; time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6926 score: 0.5102 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6929 score: 0.4898 time: 0.07s
Epoch 11/1000, LR 0.000270
Train loss: 1.4287;  Loss pred: 1.4287; Loss self: 0.0000; time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6926 score: 0.5102 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6927 score: 0.4898 time: 0.07s
Epoch 12/1000, LR 0.000270
Train loss: 1.4159;  Loss pred: 1.4159; Loss self: 0.0000; time: 0.19s
Val loss: 0.6926 score: 0.5510 time: 0.06s
Test loss: 0.6925 score: 0.5510 time: 0.07s
     INFO: Early stopping counter 1 of 2
Epoch 13/1000, LR 0.000270
Train loss: 1.3981;  Loss pred: 1.3981; Loss self: 0.0000; time: 0.19s
Val loss: 0.6926 score: 0.7347 time: 0.06s
Test loss: 0.6924 score: 0.7551 time: 0.06s
     INFO: Early stopping counter 2 of 2
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 010,   Train_Loss: 1.4287,   Val_Loss: 0.6926,   Val_Precision: 0.5102,   Val_Recall: 1.0000,   Val_accuracy: 0.6757,   Val_Score: 0.5102,   Val_Loss: 0.6926,   Test_Precision: 0.4898,   Test_Recall: 1.0000,   Test_accuracy: 0.6575,   Test_Score: 0.4898,   Test_loss: 0.6927


[0.06476087204646319, 0.06579116999637336, 0.06670370895881206, 0.06283232697751373, 0.06635016796644777, 0.06713514402508736, 0.06663857901003212, 0.06579748599324375, 0.06196772202383727, 0.062184436013922095, 1.1575231599854305, 0.7153603859478608, 0.4520734220277518, 0.20855456998106092, 0.0624145430047065, 0.06335032999049872, 0.06274904799647629, 0.06183908192906529, 0.06212091201450676, 0.0656258639646694, 0.06278192496392876, 0.061973148956894875, 0.06682089902460575, 0.22932421998120844, 0.06647005898412317, 0.06625901197548956, 0.06943125301040709, 0.06677279609721154, 0.06148970394860953, 0.06119351997040212, 0.06136334897018969, 0.06794143002480268, 0.06500601605512202, 0.06330907205119729, 0.06279758806340396, 0.06461836700327694, 0.06341015791986138, 0.06493650190532207, 0.06442683702334762, 0.06460916704963893, 0.0647593749454245, 0.0685099350521341, 0.06626345205586404, 0.06469235301483423, 0.6948692150181159, 0.3487890580436215, 0.39466211502440274, 0.07342516200151294, 0.06805292889475822, 0.06669866503216326, 0.07165221392642707, 0.06792366900481284, 0.06723513500764966, 0.06716945499647409, 0.08681857003830373, 0.07082555699162185, 0.9292787079466507, 1.2429560100426897, 0.3411137278890237, 0.06832538999151438, 0.0633906819857657, 0.06216920597944409, 0.06540966406464577, 0.06452242191880941, 0.06129719200544059, 0.06532936100848019, 0.06521882896777242, 0.06099008792079985, 0.8595021119108424, 0.44879164604935795, 0.06850228901021183, 0.0673556929687038, 0.06918730994220823, 0.06795746996067464, 0.07283785194158554, 0.07099246501456946, 1.6247506119543687, 0.06786082498729229, 0.06247847503982484, 0.06238862208556384, 0.06309982598759234, 0.06843031500466168, 0.06853935297112912, 0.06705821596551687, 0.06369954999536276, 0.06352299998980016, 0.06738124799448997, 0.06833131902385503, 0.06453858397435397, 0.06475179898552597, 0.06494150590151548, 0.856712986016646, 0.5195994810201228, 0.06789304199628532, 0.06746559799648821, 0.0731841679662466, 0.07599541801027954, 0.07152079197112471, 0.06579671904910356, 0.06625745096243918, 0.06559836503583938, 0.06600432505365461, 0.06585284392349422, 0.06549552700016648, 0.06582786806393415, 0.06580422702245414, 0.06627796601969749, 0.06606625311542302, 0.07201612694188952, 0.06662704993505031, 0.2685609789332375, 0.06897633103653789, 0.07226494699716568, 0.06502013502176851, 0.06194956705439836, 0.06210083398036659, 0.06174241297412664, 0.06228774704504758, 0.06199062999803573, 0.06189611298032105, 0.06258089502807707, 0.06554542703088373, 0.06167076400015503, 0.06221748807001859, 0.06154343893285841, 0.06750979099888355, 0.06695332902017981, 0.07336356001906097, 0.07371507701463997, 0.06722062802873552, 0.06712983595207334, 0.06650756602175534, 0.07445000298321247, 0.06705248705111444, 0.06685109704267234, 0.07282947294879705, 0.06859242601785809, 0.07314315508119762, 0.06692946702241898, 0.06771053606644273, 0.06733830296434462, 0.06739915790967643, 0.06787370308302343, 0.06783171591814607, 0.06763095105998218, 0.06763171299826354, 0.07347791898064315, 0.07389857294037938, 0.06693518394604325, 0.06785635103005916, 0.0740184240275994, 0.06898828910198063, 0.06841460999567062, 0.06727437896188349, 0.07242574007250369, 1.1049873309675604, 0.06973579700570554, 0.0663173709763214, 0.07052694796584547, 0.07254803495015949, 0.06626497500110418, 0.06647055200301111, 0.06632130197249353, 0.06729779299348593, 0.06681169103831053, 0.07345814001746476, 0.06851096707396209, 1.0557224840158597, 0.30492499307729304, 0.42083856696262956, 0.06481902697123587, 0.06499480898492038, 0.07171523000579327, 0.07228195201605558, 0.0643088249489665, 0.06489149294793606, 0.06518906098790467, 0.06471460603643209, 0.06474307202734053, 0.0641459230100736, 0.06441276497207582, 0.06495361309498549, 0.06971751700621098, 0.07118141697719693, 0.07059967005625367, 0.06424430094193667, 0.06453299208078533, 1.544311658013612, 0.06957882700953633, 0.06305972998961806, 0.06983294000383466, 0.0640381519915536, 0.06435789610259235, 0.06794984103180468, 0.07132203399669379, 1.1363897930132225, 0.1172800490166992, 0.07068418001290411, 0.06473798397928476, 0.07024006103165448, 0.06436633097473532, 0.0642209961079061, 0.06350769102573395, 0.06431071006227285, 0.07012145896442235, 0.06431479507591575, 0.06935204192996025, 0.06759858399163932, 0.06839565804693848, 0.0739662959240377, 0.06828439701348543, 0.7392536730039865, 1.2675416379934177, 0.1325124129652977, 0.06736809702124447, 0.07087960303761065, 0.06607222300954163, 0.0651511310134083, 0.06464820692781359, 0.06091385101899505, 0.06124209298286587, 0.5564304799772799, 2.132388131925836, 0.23668571398593485, 0.06667599105276167, 0.06308440503198653, 0.0653558389749378, 0.06502007006201893, 0.06160800904035568, 0.062220701947808266, 0.06635490793269128, 0.06648201995994896, 0.06563782307785004, 0.8506578800734133, 0.4693251089192927, 1.519177146954462, 0.3862044010311365, 0.061377485981211066, 0.06095018296036869, 0.0615432970225811, 0.0601485799998045, 0.059477505972608924, 0.06490450794808567, 0.0602725800126791, 0.06359793804585934, 0.0606649040710181, 0.06484047800768167, 0.0611747830407694, 0.06498901802115142, 1.912888011080213, 0.08741021400783211, 0.0615475740050897, 0.06136196607258171, 0.06173864600714296, 0.061009470955468714, 0.06185999605804682, 0.06252516596578062, 0.06260105897672474, 0.06590831198263913, 0.06394266500137746, 0.06749054393731058, 0.06257951003499329, 0.06289265502709895, 0.06695004005450755, 0.06419650197494775, 0.06298727996181697, 0.06346747290808707, 0.06320392701309174, 0.06292027502786368, 1.6048958000028506, 0.2362813219660893, 0.06785095599479973, 0.06721629993990064, 0.07240299799013883, 0.06757554796058685, 0.06686734198592603, 0.06738421006593853, 0.07057991600595415, 0.07433232304174453, 0.06764360598754138, 0.06783948605880141, 0.07256339699961245, 0.07391184696462005, 0.06844383501447737, 0.06947421608492732, 0.06753231701441109, 0.4220553449122235, 0.06859597004950047, 0.06921478407457471, 0.0646464090095833, 0.06813114194665104, 0.06510418897960335, 0.06532575993333012, 0.06525081698782742, 0.07011349394451827, 0.06548786209896207, 0.06534931890200824, 0.06434668507426977, 0.07230448699556291, 0.06943024904467165, 0.07413355202879757, 0.06864948908332735, 0.16475897992495447, 0.07491439499426633, 0.7072585810674354, 0.5006445340113714, 0.18565852008759975, 0.0676679010502994, 0.06722866697236896, 0.06886849901638925, 0.0668798639671877, 0.06879457598552108, 0.06783791701309383, 0.06819728610571474, 0.06873145198915154, 0.0683390130288899, 0.7279247969854623, 0.1665846329415217, 0.07137009501457214, 0.06616553501226008, 0.06538884702604264, 0.07008704706095159, 0.06557476299349219, 0.07241675991099328, 0.06500819500070065, 0.06504361901897937, 0.06544885702896863, 0.06500213406980038, 0.06600897200405598, 0.06511185294948518, 0.0655770639423281, 0.06698209897149354, 0.06579338503070176, 0.06996298802550882, 1.899643717915751, 0.07226561894640326, 0.06332338193897158, 0.06254529894795269, 0.06327792804222554, 0.06251364096533507, 0.0631484299665317, 0.06287728797178715, 0.06262192700523883, 0.06349792098626494, 0.06439099693670869, 0.06340990611352026, 0.0650928079849109, 0.06341387098655105, 0.06293345894664526, 0.0645281879696995, 1.7162056119414046, 0.2716270349919796, 0.06714264408219606, 0.07118098007049412, 0.06566129601560533, 0.06418716302141547, 0.07142510497942567, 0.06911094696260989, 0.06466778297908604, 0.06990271992981434, 0.07148966705426574, 0.06545510503929108, 0.06561778602190316, 1.2000648390967399, 0.06945508194621652, 0.065545194898732, 0.15201776998583227, 0.06295793992467225, 0.064309066045098, 0.06580444297287613, 0.06939325598068535, 0.06445921899285167, 0.06418992893304676, 0.3668155289487913, 0.20138085703365505, 0.19358270696830004, 0.07272000994998962, 0.06528760003857315, 0.0653917589224875, 0.06596439203713089, 0.06563992309384048, 0.06641777500044554, 0.07210104598198086, 0.06443729600869119, 0.06992327095940709, 0.0695811070036143, 2.2161726449849084, 0.071116485982202, 0.06742271594703197, 0.06291959306690842, 0.06714440998621285, 0.06201162200886756, 0.061729033943265676, 0.06224128999747336, 0.061859858920797706, 0.06827762501779944, 0.06586357206106186, 0.06834904197603464, 0.06278733804356307, 0.06359967798925936, 1.3186588590033352, 0.06958860799204558, 0.06546626298222691, 0.06502499699126929, 0.07032969291321933, 0.06582687492482364, 0.0655704439850524, 1.9051513120066375, 0.34100844198837876, 0.0672868819674477, 0.0643655169988051, 0.06323832599446177, 0.07136381301097572, 0.06756826408673078, 0.06489188096020371, 0.06471206899732351, 0.06912705302238464, 0.06919988198205829, 0.6070576390484348, 0.0859090369194746, 0.0758327569346875, 0.0649796420475468, 0.06416665902361274, 0.06514160090591758, 0.06459972297307104, 0.06517120101489127, 0.06891537504270673, 0.06532650394365191, 0.06629782903473824, 2.238558550947346, 0.06598101602867246, 0.06924301700200886, 0.067480772966519, 0.0628592330031097, 0.06268713995814323, 0.06307572196237743, 0.06305542297195643, 0.06772002205252647, 0.06268972100224346, 0.062342574005015194, 0.0634767560986802, 0.06228792399633676, 0.06478872790466994, 0.06264090596232563, 0.07228142698295414, 0.07419338892214, 1.7326488689286634, 0.06752698204945773, 0.0669534649932757, 0.06520805996842682, 0.06508939596824348, 0.06680669297929853, 0.06480691605247557, 0.06576545105781406, 0.06891637109220028, 0.5567155629396439, 0.4813709679292515, 0.06928115605842322, 0.06672954501118511, 0.07263791200239211, 0.0672091000014916, 0.06677619100082666, 0.06693172990344465, 0.06674282194580883, 0.07575565692968667, 0.07562583195976913, 0.0763560690684244, 0.07039989798795432, 0.06928758102003485, 0.07593594503123313, 0.3158821549732238, 0.13041383994277567, 0.06614619097672403, 0.0659579400671646, 0.06593002506997436, 0.06693006900604814, 0.07002889807336032, 0.06604031892493367, 1.7890617039520293, 0.22036014206241816, 0.0759566790657118, 0.06264192296657711, 0.06709688995033503, 0.06791546498425305, 0.06306132394820452, 0.06323279696516693, 0.06395158101804554, 0.06762970797717571, 0.06245590595062822, 0.06364724098239094, 0.06304338597692549, 0.06624758802354336, 0.06256500107701868, 0.062405450036749244, 0.06723620695993304, 0.0686056069098413, 0.06282429001294076, 0.0674764149589464, 0.07289072405546904, 0.0673585309414193, 0.06730797293130308, 0.07337011909112334, 0.0664173288969323, 0.06607174105010927, 0.06301894097123295, 0.06300670397467911, 0.896176618989557, 0.8744621520163491, 0.21723663003649563, 0.07425843097735196, 0.06504185998346657, 0.06942025804892182, 0.0680359520483762, 0.1715412779012695, 0.06343274796381593, 0.06362874701153487, 0.06590970896650106, 0.06434416002593935, 0.06861695402767509, 0.517940251971595, 1.6072091270470992, 0.506246354081668, 0.11551346397027373, 0.06630984100047499, 0.06963990395888686, 0.06729125301353633, 0.06486731197219342, 0.06440085300710052, 0.06430957093834877, 0.06387872400227934, 1.2316235409816727, 1.3823849940672517, 0.07851068291347474, 0.07376984693109989, 0.07108216302003711, 0.07271471689455211, 0.07723820197861642, 0.07201258500572294, 0.07108293694909662, 0.31924587104003876, 0.3110714729409665, 0.24418258003424853, 0.07032156898640096, 0.07015903701540083, 0.06441724894102663, 0.06654989102389663, 0.06320760492235422, 0.063731548958458, 0.06386265100445598, 0.06440409598872066, 0.06900439003948122, 0.06583102501463145, 0.06380656594410539, 0.06863827502820641, 0.06819069501943886, 0.06474722002167255, 0.06424930098000914, 0.0643530689412728, 0.06374898494686931, 0.06989809998776764, 0.06339333101641387, 0.06399103906005621, 0.06720307003706694, 0.0658894709777087, 0.06558989104814827, 0.35669670498464257, 0.2774782329797745, 0.06974509102292359, 0.06629608699586242, 0.07045628689229488, 0.07003830000758171, 0.06582778599113226, 0.06648476398549974, 0.06757584598381072, 0.06799790496006608, 0.0674353139474988, 0.07260047493036836, 1.2878672890365124, 0.42416838300414383, 0.0708550380077213, 0.07010504603385925, 0.06911957205738872, 0.06387625099159777, 0.06314541702158749, 0.06569126795511693, 0.06714261998422444, 0.06793443101923913, 0.06717778695747256, 0.06921430805232376, 0.06554288696497679, 0.07067063602153212, 1.6688041030429304, 0.07966629404108971, 0.06865600997116417, 0.07281723199412227, 0.07409592799376696, 0.06876463210210204, 0.06786132592242211, 0.06932685791980475, 0.06493154994677752, 0.06484484695829451, 0.06852187891490757, 0.07018460996914655, 0.065465735970065, 0.06465642701368779, 0.06410289893392473, 0.06439938396215439, 0.06907692807726562, 0.06559018197003752, 0.06878612097352743, 0.0636490338947624, 0.07021142798475921, 0.06324622198008001, 0.06431960500776768, 0.06692157802172005, 0.06340384704526514, 0.07188033603597432, 0.06277055200189352, 0.06325363996438682, 0.0633833589963615, 0.06339651392772794, 0.0673870580503717, 0.06369193003047258, 0.06352362199686468, 0.06936838501133025, 0.06352287204936147, 0.0665385730098933, 0.06500590301584452, 0.06316434603650123, 0.0695675800088793, 0.07912603102158755, 0.5844906029524282, 0.13304443599190563, 0.06720435293391347, 0.07339471008162946, 0.06709514104295522, 0.06664645893033594, 0.07241625699680299, 0.0673919339897111, 0.06662923796102405, 0.06642111495602876, 0.0668294399511069, 0.06493537197820842, 0.06322812498547137, 0.06634204590227455, 0.06255423102993518, 0.06259065808262676, 0.06379132508300245, 0.06288823706563562, 0.0621612899703905, 0.06835659104399383, 0.06396779301576316, 1.3505150770070031, 0.5170005529653281, 0.0689914709655568, 0.06966114405076951, 0.07459674205165356, 0.07465723797213286, 0.07027957204263657, 0.06894670298788697, 0.06392588000744581, 0.06406681705266237, 0.06431151297874749, 0.06458875490352511, 0.06488044699653983, 0.065676145022735, 0.35896303795743734, 1.2143864179961383, 0.07307350402697921, 0.06556992197874933, 0.06835028098430485, 0.08258796902373433, 0.06818107003346086, 0.06971676799003035, 0.16462945798411965, 0.06640488398261368, 0.06783919292502105, 0.07787828089203686, 0.07304806995671242, 0.07368136709555984, 0.07458002492785454, 0.07515960093587637, 0.06915963196661323, 0.06756905501242727, 0.06901379197370261, 0.07452968298457563, 0.06816296197939664, 0.06471523200161755, 0.06693262991029769, 0.06875477300491184, 0.07952850603032857, 0.06697196408640593, 0.06376407400239259, 0.06349858804605901, 0.06372120301239192, 0.06320428301114589, 0.06704925000667572, 0.06351494207046926, 0.06360963697079569, 0.06883960799314082, 0.06846590200439095, 0.06928402802441269, 0.06961149291601032, 0.06880488898605108, 0.06799859402235597, 0.08520903903990984, 0.06564321601763368, 0.06672774895559996, 0.07125349901616573, 0.9326045931084082, 0.19147153396625072, 0.22290814109146595, 0.06835463794413954, 0.067610680940561, 0.06682980805635452, 0.06916595599614084, 0.06328932603355497, 0.063800786039792, 0.06399306305684149, 0.06298427400179207, 0.06481503101531416, 1.189055873081088, 0.4092626499477774, 0.07255974307190627, 0.07367307704407722, 0.06647590606007725, 0.07152761903125793, 0.06630488007795066, 0.071962476009503, 0.06671614805236459, 0.06672616396099329, 0.0729592900024727, 0.06630897999275476, 0.06693426193669438, 1.2392065059393644, 0.40281887899618596, 0.07382444804534316, 0.06446668901480734, 0.06884725007694215, 0.06952448503579944, 0.06382494198624045, 0.06547389400657266, 0.06431331089697778, 0.06947608303744346, 0.06549400591757149, 0.067580278031528, 0.0638267049798742, 0.06398291804362088, 0.06851908203680068, 0.06828400900121778, 0.06414322403725237, 0.06848392600659281, 0.07262359699234366, 0.06490559701342136, 0.06663399399258196, 0.06449548597447574, 0.06905579299200326, 0.07653011404909194, 0.07602321798913181, 0.06532603409141302, 0.8788891969015822, 0.27111051604151726, 0.07314431900158525, 0.0649898829869926, 0.06671972898766398, 0.0664210399845615, 0.06542360107414424, 0.06253501202445477, 0.06445766892284155, 0.06479838397353888, 0.07054607500322163, 0.07039458001963794, 0.065173669019714, 0.06805391400121152, 0.06851681298576295, 0.07044602697715163, 0.06432551401667297, 0.06500284594949335, 0.06457605899777263, 0.0654990510083735, 0.06379286793526262, 0.06456376204732805, 0.06988232699222863, 0.06446634698659182, 0.06401399301830679, 0.07062350900378078, 0.06861829198896885, 0.06755873002111912, 0.07087855401914567, 0.06670955300796777, 0.0673260820331052, 0.07348539703525603, 0.06830362405162305, 0.07730086101219058, 0.06577072897925973, 0.0666669289348647, 0.06769820593763143, 0.07245017006061971, 1.9596740840934217, 0.9620321380207315, 0.07046111591625959, 0.0661841289838776, 0.06658130697906017, 0.06601538206450641, 0.06682723190169781, 0.06865145300980657, 0.06898205203469843, 0.0653947819955647, 0.06540770502761006, 0.06596414197701961, 0.06502940692007542, 0.07118258194532245, 0.07133390300441533, 0.07771919202059507, 0.07355559896677732, 0.07522508397232741, 0.07532250508666039, 0.4084291330073029, 0.06941979797556996, 0.06922365503851324, 0.06821172300260514, 0.0690124100074172, 0.06366311002057046, 0.06415278499480337, 0.06419195095077157, 0.07602789194788784, 0.06731262803077698, 0.06729819905012846, 0.07346002699341625, 0.07078589906450361, 0.07427044294308871, 0.0736660819966346, 4.503635878092609, 0.07090341404546052, 0.07478197803720832, 0.06951119098812342, 0.07665546203497797, 0.07437500089872628, 0.07058266701642424, 0.06871553789824247, 0.0696682749548927, 2.332156667020172, 0.11686899000778794, 0.07871303509455174, 0.07055106398183852, 0.07199013198260218, 0.09376147203147411, 0.07216680399142206, 0.07080496102571487, 0.06907609000336379, 0.0964147619670257, 0.06607787392567843, 0.06552604003809392, 0.3881415129872039, 0.589422435965389, 0.4416610869811848, 0.06642098899465054, 0.06578986090607941, 0.06496899807825685, 0.06450557801872492, 0.06417133600916713, 0.06789846892934293, 0.0642391899600625, 0.06423501507379115, 0.06929359491914511, 0.06399363896343857, 0.06451334792654961, 0.06411885400302708, 0.06508341000881046, 0.06473090499639511, 0.07003381999675184, 0.06497018598020077, 0.06504035601392388, 0.06508302304428071, 0.06520673504564911, 0.06518018001224846, 0.6629909779876471, 0.06755822896957397, 0.07157642999663949, 0.07272328692488372, 0.06755729403812438, 0.06771413504611701, 0.0715579780517146, 0.07116692594718188, 0.06715583894401789, 0.06848079105839133, 0.07245134899858385, 0.06722319894470274, 0.06702481501270086, 1.2813479989999905, 0.4242539450060576, 0.07447791192680597, 0.07344364305026829, 0.0679449460003525, 0.0673694210126996, 0.06451893097255379, 0.06527983793057501, 0.0645159799605608, 0.06451353791635484, 0.06359558505937457, 0.06826719699893147, 0.07225725089665502, 0.06712194008287042, 0.06816358701325953, 0.06355647498276085, 0.06342282507102937, 0.06409452995285392, 0.06353080994449556, 0.06412143202032894, 0.06470184994395822, 0.06406555196736008, 0.06418455101083964, 0.06448719301261008, 0.06435815803706646, 0.06566310208290815, 0.06721896294038743, 0.061438523000106215, 0.0614573530619964, 0.06632577197160572, 0.06660741090308875, 0.06304434209596366, 0.062390729086473584, 0.06757632200606167, 0.06224048207513988, 0.062174338032491505, 0.06284572905860841, 0.06227965105790645, 0.06752009608317167, 0.06740075396373868, 0.06250167998950928, 0.06291136902291328, 0.06730246206279844, 0.15030146902427077, 0.06578605202957988, 0.06729742395691574, 0.06942942296154797, 0.06754914403427392, 0.06394178501795977, 0.07084997103083879, 0.06671703804749995, 0.06626581703312695, 0.0666622529970482, 0.06542167300358415, 0.06645414605736732, 0.0673836680362001, 0.1479212570702657, 0.0664302259683609, 0.06586207693908364, 0.06253713590558618, 0.06324854400008917, 0.06221957900561392, 0.0641926700482145, 0.06554925406817347, 0.06259911507368088, 0.06743779208045453, 0.06788706895895302, 0.06741353799588978, 0.06647619407158345, 0.071465541026555, 0.07118047692347318, 0.0725632639368996, 0.07049167098011822, 0.06552886101417243, 0.06561469100415707, 0.07097887492273003, 0.06530368502717465, 0.06594293203670532, 0.06080109300091863, 0.0667420559329912, 0.062383426004089415, 0.061483904952183366, 0.06124903494492173, 0.06171438191086054, 0.062082702992483974, 0.0662174359895289, 0.07125466503202915, 0.06620554102119058, 0.06591898808255792, 0.0665885719936341, 0.6497554560191929, 0.9856888699578121, 0.0686382990097627, 0.07206263800617307, 0.06703889893833548, 0.0676741610514, 0.06212588702328503, 0.06289674190338701, 0.06663579004816711, 0.06213947606738657, 0.0632768829818815, 0.06185680499766022, 0.06259323400445282, 0.063094393000938, 0.06173845799639821, 0.06352546706330031, 0.06286085909232497, 0.06723922595847398, 0.06744257791433483, 0.6813678949838504, 0.8929920350201428, 0.06900974805466831, 0.07329760398715734, 0.07288600702304393, 0.07351605198346078, 0.07320944697130471, 0.07470090803690255, 0.07364072802010924, 0.0734892119653523, 0.07299982709810138, 0.07366373401600868, 0.07320796104613692, 0.07386978296563029, 0.06849678105209023]
[0.0013216504499278202, 0.001342676938701497, 0.0013613001828328992, 0.0012822923872961986, 0.0013540850605397504, 0.0013701049801038236, 0.001359971000204737, 0.0013428058365968112, 0.001264647388241577, 0.001269070122733104, 0.023622921632355725, 0.014599191549956342, 0.009225988204647996, 0.004256215713899203, 0.0012737661837695204, 0.0012928638773571169, 0.0012805928162546183, 0.0012620220801850058, 0.0012677737145817705, 0.001339303346217743, 0.0012812637747740563, 0.0012647581419774465, 0.0013636918168286889, 0.004680086122065478, 0.0013565318160025136, 0.0013522247341936644, 0.0014169643471511652, 0.0013627101244328885, 0.0012548919173185618, 0.0012488473463347371, 0.0012523132442895854, 0.0013865597964245447, 0.0013266533888800411, 0.0012920218785958631, 0.001281583429865387, 0.0013187421837403458, 0.001294084855507375, 0.001325234732761675, 0.0013148334086397473, 0.0013185544295844678, 0.0013216198968453979, 0.0013981619398394714, 0.0013523153480788578, 0.0013202521023435556, 0.014181004388124816, 0.007118144041706561, 0.008054328878049036, 0.0014984726939084275, 0.0013888352835664944, 0.0013611972455543522, 0.0014622900801311647, 0.0013861973266288334, 0.0013721456124010135, 0.0013708052040096752, 0.001771807551802117, 0.0014454195304412622, 0.018964871590747973, 0.025366449184544688, 0.006961504650796402, 0.0013943957141125385, 0.001293687387464606, 0.0012687593057029406, 0.0013348911033601177, 0.0013167841207920288, 0.0012509631021518487, 0.0013332522654791875, 0.001330996509546376, 0.001244695671853058, 0.017540859426751886, 0.009159013184680775, 0.0013980058981675882, 0.001374605978953139, 0.001411985917187923, 0.0013868871420545845, 0.0014864867743180723, 0.0014488258166238666, 0.033158175754170786, 0.0013849147956590264, 0.001275070919180099, 0.0012732371854196703, 0.0012877515507671907, 0.0013965370409114628, 0.0013987623055332474, 0.0013685350197044257, 0.001299990816231893, 0.0012963877548938807, 0.001375127510091632, 0.0013945167147725516, 0.001317113958660285, 0.0013214652854188973, 0.001325336855132969, 0.017483938490135634, 0.010604071041226995, 0.0013855722856384758, 0.0013768489387038412, 0.001493554448290747, 0.0015509268981689702, 0.0014596079994107084, 0.0013427901846755827, 0.001352192876784473, 0.0013387421435885588, 0.0013470270419113186, 0.0013439355902753922, 0.0013366434081666628, 0.001343425878855799, 0.001342943408621513, 0.0013526115514223976, 0.0013482908799065923, 0.0014697168763650923, 0.0013597357129602103, 0.005480836304759949, 0.0014076802252354671, 0.0014747948366768506, 0.0013269415310565003, 0.001264276878661191, 0.0012673639587829917, 0.0012600492443699315, 0.00127117851112342, 0.0012651148979190964, 0.0012631859791902254, 0.001277161123021981, 0.0013376617761404843, 0.0012585870204113272, 0.0012697446544901753, 0.0012559885496501715, 0.001377750836711909, 0.0013663944697995878, 0.001497215510593081, 0.0015043893268293872, 0.0013718495516068473, 0.0013699966520831293, 0.001357297265750109, 0.001519387815983928, 0.0013684181030839682, 0.0013643081029116803, 0.0014863157744652458, 0.0013998454289358793, 0.0014927174506366861, 0.0013659074902534485, 0.0013818476748253619, 0.0013742510809049923, 0.0013754930185648252, 0.0013851776139392536, 0.0013843207330233892, 0.0013802234910200446, 0.0013802390407808886, 0.0014995493669519011, 0.0015081341416403955, 0.001366024162164148, 0.0013848234904093705, 0.001510580082195906, 0.0014079242673873597, 0.00139621653052389, 0.0013729465094261936, 0.0014780763280102794, 0.022550761856480826, 0.0014231795307286844, 0.0013534157342106408, 0.0014393254686907238, 0.0014805721418399895, 0.0013523464285939628, 0.0013565418776124716, 0.0013534959586223169, 0.0013734243468058352, 0.001363503898741031, 0.001499145714642138, 0.0013981830015094305, 0.021545356816650197, 0.006222959042393735, 0.008588542182910807, 0.0013228372851272626, 0.0013264246731616405, 0.0014635761225672097, 0.0014751418778786855, 0.0013124249989585, 0.00132431618261094, 0.0013303889997531564, 0.0013207062456414712, 0.0013212871842314393, 0.0013091004695933387, 0.0013145462239199147, 0.0013255839407139895, 0.00142280646951451, 0.001452681979126468, 0.0014408095929847689, 0.0013111081824885035, 0.0013169998383833741, 0.031516564449257385, 0.0014199760614191086, 0.0012869332650942461, 0.001425162040894585, 0.0013069010610521144, 0.0013134264510733132, 0.001386731449628667, 0.0014555517142182406, 0.023191628428841277, 0.0023934703880959017, 0.001442534285977635, 0.0013211833465160156, 0.001433470633299071, 0.001313598591321129, 0.0013106325736307368, 0.0012960753270557948, 0.0013124634706586295, 0.0014310501829473948, 0.001312546838283995, 0.0014153477944889848, 0.001379562938604884, 0.0013958297560599689, 0.0015095162433477081, 0.0013935591227241925, 0.015086809653142581, 0.02586819669374322, 0.002704334958475463, 0.00137485912288254, 0.0014465225109716459, 0.0013484127144804414, 0.0013296149186409858, 0.001319351161792114, 0.0012431398167141847, 0.001249838632303385, 0.011355724081168977, 0.043518125141343594, 0.004830320693590507, 0.0013607345112808505, 0.0012874368373874802, 0.0013337926321415877, 0.001326940205347325, 0.0012573063069460343, 0.0012698102438328217, 0.0013541817945447198, 0.0013567759175499787, 0.0013395474097520417, 0.017360364899457414, 0.009578063447332504, 0.03100361524396861, 0.007881722470023195, 0.0012526017547185933, 0.0012438812849054836, 0.0012559856535220633, 0.0012275220408123366, 0.001213826652502223, 0.0013245817948588912, 0.0012300526533199816, 0.0012979171029767211, 0.0012380592667554713, 0.0013232750613812584, 0.001248464960015702, 0.00132630649022758, 0.03903853083837169, 0.001783881918527186, 0.0012560729388793816, 0.0012522850218894227, 0.0012599723674927134, 0.0012450912439891575, 0.0012624488991438126, 0.0012760237952200125, 0.001277572632178056, 0.0013450675914824313, 0.001304952346966887, 0.0013773580395369505, 0.001277132857857006, 0.0012835235719816113, 0.0013663273480511745, 0.0013101326933662807, 0.0012854546930983054, 0.001295254549144634, 0.0012898760614916682, 0.0012840872454666058, 0.03275297551026226, 0.004822067795226312, 0.0013847133876489742, 0.0013717612232632783, 0.0014776122038803843, 0.0013790928155221806, 0.0013646396323658374, 0.0013751879605293578, 0.001440406449101105, 0.0015169861845253985, 0.0013804817548477833, 0.0013844793073224779, 0.0014808856530533153, 0.0015084050400942868, 0.0013968129594791301, 0.0014178411445903536, 0.001378210551314512, 0.008613374385963745, 0.0013999177561122545, 0.0014125466137668307, 0.0013193144695833325, 0.0013904314682990008, 0.0013286569179510887, 0.0013331787741495942, 0.0013316493262821923, 0.001430887631520781, 0.0013364869816114707, 0.0013336595694287395, 0.0013131976545769342, 0.0014756017754196512, 0.0014169438580545236, 0.0015129296332407668, 0.001401009981292395, 0.0033624281617337646, 0.001528865203964619, 0.014433848593212967, 0.010217235387987172, 0.003788949389542852, 0.0013809775724550899, 0.001372013611680999, 0.001405479571763046, 0.0013648951830038307, 0.001403970938480022, 0.0013844472859815067, 0.001391781349096219, 0.001402682693656154, 0.0013946737352834673, 0.014855608101744128, 0.0033996863865616675, 0.0014565325513177989, 0.0013503170410665323, 0.0013344662658376048, 0.0014303478992030937, 0.0013382604692549426, 0.0014778930594080261, 0.0013266978571571562, 0.0013274207963057014, 0.00133569095977487, 0.0013265741646898035, 0.0013471218776337955, 0.0013288133254996976, 0.001338307427394451, 0.0013669816116631335, 0.0013427221434837093, 0.0014278160821532412, 0.03876823914113778, 0.0014748085499265973, 0.001292313917121869, 0.0012764346724071977, 0.0012913862865760314, 0.0012757885911292872, 0.0012887434687047284, 0.0012832099586079012, 0.0012779985103109966, 0.001295875938495203, 0.0013141019783001772, 0.0012940797166024543, 0.0013284246527532839, 0.0012941606323785928, 0.0012843563050335767, 0.0013169017952999898, 0.03502460432533479, 0.005543408877387339, 0.0013702580424937972, 0.0014526730626631453, 0.001340026449298068, 0.0013099421024778668, 0.0014576552036617482, 0.001410427489032855, 0.0013197506730425724, 0.0014265861210166191, 0.0014589727970258314, 0.001335818470189614, 0.0013391384902429215, 0.024491119165239588, 0.0014174506519636025, 0.0013376570387496328, 0.0031024034690986177, 0.0012848559168300458, 0.0013124299192877145, 0.0013429478157729823, 0.0014161888975650072, 0.0013154942651602383, 0.0013099985496540154, 0.007486031203036558, 0.004109813408850103, 0.003950667489148981, 0.0014840818357140738, 0.001332400000787207, 0.001334525692295663, 0.0013462120823904264, 0.0013395902672212344, 0.00135546479592746, 0.0014714499179996094, 0.0013150468573202283, 0.0014270055297838182, 0.001420022591910496, 0.04522801316295731, 0.0014513568567796325, 0.0013759737948373873, 0.0012840733278960902, 0.0013702940813512827, 0.0012655433063034195, 0.0012597762029237893, 0.0012702304081117011, 0.001262446100424443, 0.0013934209187306007, 0.0013441545318584054, 0.0013948784076741763, 0.0012813742457870015, 0.0012979526120257012, 0.026911405285782352, 0.0014201756733070528, 0.0013360461833107533, 0.001327040754923863, 0.0014352998553718232, 0.0013434056107106867, 0.0013381723262255593, 0.038880639020543624, 0.006959355958946505, 0.001373201672805055, 0.001313581979567451, 0.001290578081519628, 0.0014564043471627698, 0.001378944165035322, 0.001324324101228647, 0.001320654469333133, 0.0014107561841302989, 0.001412242489429761, 0.01238893140915173, 0.001753245651417849, 0.0015476072843813775, 0.0013261151438274858, 0.0013095236535431171, 0.0013294204266513794, 0.0013183616933279804, 0.001330024510507985, 0.0014064362253613618, 0.0013331939580337126, 0.0013530169190762906, 0.045684868386680524, 0.0013465513475239277, 0.0014131227959593643, 0.0013771586319697754, 0.0012828414898593814, 0.0012793293869008823, 0.0012872596318852536, 0.001286845366774621, 0.0013820412663780913, 0.0012793820612702746, 0.0012722974286737796, 0.0012954440020138817, 0.0012711821223742195, 0.0013222189368299988, 0.0012783858359658293, 0.0014751311629174315, 0.0015141507943293878, 0.035360180998544155, 0.0013781016744787292, 0.0013663972447607287, 0.001330776734049527, 0.001328355019760071, 0.0013634018975367047, 0.0013225901235199096, 0.0013421520624043687, 0.0014064565529020466, 0.011361542100809058, 0.009823897304678602, 0.0014139011440494535, 0.0013618274492078594, 0.0014824063673957574, 0.0013716142857447267, 0.001362779408180136, 0.0013659536714988704, 0.001362098407057323, 0.0015460338148915646, 0.001543384325709574, 0.001558287123845396, 0.0014367326119990678, 0.0014140322657149968, 0.001549713163902717, 0.0064465745912902815, 0.0026615069376076665, 0.0013499222648311027, 0.0013460804095339713, 0.0013455107157137624, 0.0013659197756336356, 0.0014291611851706188, 0.001347761610712932, 0.03651146334595978, 0.0044971457563758805, 0.001550136307463506, 0.001278406591154635, 0.0013693242847007147, 0.0013860298976378174, 0.0012869657948613167, 0.00129046524418708, 0.0013051343064907255, 0.001380198121983178, 0.001274610325523025, 0.0012989232853549172, 0.001286599713814806, 0.0013519915923172114, 0.0012768367566738505, 0.0012735806129948826, 0.0013721674889782254, 0.0014001144267314551, 0.001282128367611036, 0.0013770696930397227, 0.0014875657970503885, 0.0013746638967636594, 0.0013736321006388385, 0.001497349369206599, 0.0013554556917741286, 0.0013484028785736586, 0.0012861008361476113, 0.0012858511015240634, 0.018289318754888918, 0.017846166367680594, 0.004433400612989706, 0.0015154781832112645, 0.0013273848976217667, 0.0014167399601820781, 0.0013884888173138, 0.0035008424061483572, 0.0012945458768125698, 0.0012985458573782627, 0.0013450961013571645, 0.001313146122978354, 0.0014003460005647978, 0.010570209223910101, 0.03280018626626733, 0.010331558246564652, 0.0023574176320464027, 0.0013532620612341836, 0.0014212225297732012, 0.0013732908778272721, 0.0013238226933100698, 0.0013143031225938881, 0.0013124402232316075, 0.0013036474286179458, 0.025135174305748423, 0.028211938654433708, 0.0016022588349688723, 0.0015055070802265284, 0.0014506563881640227, 0.0014839738141745329, 0.0015762898362982943, 0.0014696445919535294, 0.001450672182634625, 0.006515221857959975, 0.006348397406958499, 0.004983317959882623, 0.0014351340609469584, 0.0014318170819469557, 0.0013146377334903394, 0.001358161041304013, 0.0012899511208643718, 0.0013006438562950613, 0.0013033194082542037, 0.0013143693058922583, 0.0014082528579485963, 0.00134349030642105, 0.0013021748151858241, 0.0014007811230246205, 0.0013916468371314053, 0.0013213718371769907, 0.0013112102240818192, 0.0013133279375769958, 0.0013009996927932513, 0.0014264918364850537, 0.0012937414493145688, 0.0013059395726542084, 0.001371491225246264, 0.0013446830811777285, 0.0013385692050642505, 0.007279524591523318, 0.005662821081219887, 0.001423369204549461, 0.0013529813672624985, 0.0014378834059652016, 0.0014293530613792185, 0.0013434242039006583, 0.0013568319180714233, 0.0013790988976287904, 0.0013877123461237975, 0.0013762308968877305, 0.0014816423455177217, 0.026283005898704335, 0.008656497612329466, 0.001446021183831047, 0.001430715225180801, 0.00141060351137528, 0.0013035969590121995, 0.0012886819800323978, 0.0013406381215329986, 0.001370257550698458, 0.0013864169595763087, 0.0013709752440300522, 0.0014125368990270154, 0.0013376099380607508, 0.0014422578779904514, 0.03405722659271287, 0.001625842735532443, 0.0014011430606360035, 0.0014860659590637197, 0.0015121617957911625, 0.001403359838818409, 0.0013849250188249411, 0.0014148338350980561, 0.0013251336723832147, 0.0013233642236386634, 0.001398405692140971, 0.0014323389789621746, 0.00133603542796051, 0.0013195189186466895, 0.0013082224272229538, 0.0013142731420847835, 0.0014097332260666453, 0.0013385751422456636, 0.0014037983872148456, 0.0012989598754033142, 0.0014328862854032492, 0.0012907392240832656, 0.001312645000158524, 0.0013657464902391847, 0.001293956062148268, 0.001466945633387231, 0.001281031673508031, 0.0012908906115180984, 0.001293537938701255, 0.0012938064066883252, 0.001375246082660647, 0.0012998353067443383, 0.0012964004489156057, 0.0014156813267618418, 0.0012963851438645198, 0.0013579300614263937, 0.0013266510819560106, 0.0012890682864592088, 0.0014197465307934551, 0.0016148169596242358, 0.01192837965209037, 0.0027151925712633803, 0.0013715174068145606, 0.0014978512261557032, 0.0013692885927133718, 0.0013601318149048152, 0.0014778827958531221, 0.0013753455916267572, 0.001359780366551511, 0.0013555329582863012, 0.0013638661214511614, 0.0013252116730246618, 0.001290369897662681, 0.001353919304128052, 0.0012766169597945955, 0.0012773603690331991, 0.0013018637772041317, 0.0012834334095027677, 0.0012685977544977653, 0.0013950324702855883, 0.0013054651635870033, 0.02756153218381639, 0.010551031693169961, 0.0014079892033787103, 0.0014216560010361125, 0.0015223824908500727, 0.0015236171014720993, 0.001434276980461971, 0.0014070755711813666, 0.0013046097960703227, 0.001307486062299232, 0.0013124798567091323, 0.001318137855173982, 0.0013240907550314252, 0.001340329490259898, 0.00732577628484566, 0.024783396285635476, 0.001491296000550596, 0.0013381616730357008, 0.0013949036935572417, 0.001685468755586415, 0.00139145040884614, 0.001422791183470007, 0.0033597848568187685, 0.0013552017139308915, 0.0013844733250004296, 0.0015893526712660582, 0.0014907769378920902, 0.0015037013692971394, 0.001522041325058256, 0.0015338694068546199, 0.0014114210605431273, 0.0013789603063760667, 0.0014084447341571962, 0.0015210139384607272, 0.0013910808567223804, 0.0013207190204411745, 0.0013659720389856671, 0.001403158632753303, 0.001623030735312828, 0.0013667747772735904, 0.0013013076327018896, 0.001295889551960388, 0.0013004327145386108, 0.0012898833267580793, 0.0013683520409525658, 0.0012962233075605972, 0.0012981558565468509, 0.00140488995904369, 0.0013972633062120602, 0.001413959755600259, 0.0014206427125716392, 0.0014041814078785935, 0.0013877264086195097, 0.0017389599804063232, 0.001339657469747626, 0.001361790795012244, 0.0014541530411462395, 0.01903274679813078, 0.0039075823258418515, 0.00454914573656053, 0.0013949926111048885, 0.0013798098151134898, 0.0013638736338031534, 0.001411550122370221, 0.001291618898643979, 0.0013020568579549389, 0.001305980878711051, 0.0012853933469753485, 0.0013227557350064115, 0.02426644638940996, 0.008352298978526069, 0.001480811083100128, 0.0015035321845730044, 0.0013566511440832093, 0.0014597473271685292, 0.0013531608179173603, 0.0014686219593776123, 0.0013615540418849916, 0.0013617584481835365, 0.0014889651020912795, 0.0013532444896480562, 0.001366005345646824, 0.02528992869264009, 0.008220793448901755, 0.0015066213886804727, 0.0013156467145879049, 0.0014050459199375948, 0.0014188670415469274, 0.0013025498364538867, 0.001336201918501483, 0.0013125165489179138, 0.0014178792456621115, 0.0013366123656647242, 0.001379189347582204, 0.0013025858159158, 0.001305773837624916, 0.0013983486129959322, 0.0013935512041064854, 0.0013090453885153544, 0.00139763114299169, 0.0014821142243335442, 0.0013246040206820685, 0.0013598774284200401, 0.001316234407642362, 0.0014093018977959849, 0.001561839062226366, 0.0015514942446761594, 0.0013331843692125107, 0.01793651422248127, 0.005532867674316678, 0.0014927412041139845, 0.0013263241425916857, 0.0013616271221972241, 0.0013555314282563571, 0.0013351755321253927, 0.0012762247351929545, 0.001315462631078399, 0.001322415999459977, 0.0014397158163922783, 0.0014366240820334274, 0.0013300748779533468, 0.001388855387779827, 0.001398302305831897, 0.00143767401994187, 0.0013127655921769993, 0.001326588692846803, 0.0013178787550565843, 0.0013367153267014999, 0.0013018952639849515, 0.0013176277968842461, 0.0014261699386169107, 0.0013156397344202412, 0.0013064080207817713, 0.0014412961021179752, 0.0014003733058973234, 0.001378749592267737, 0.0014465011024315441, 0.0013614194491421993, 0.001374001674145004, 0.0014997019803113475, 0.0013939515112576131, 0.001577568592085522, 0.0013422597750869332, 0.0013605495700992796, 0.0013815960395434986, 0.0014785748991963206, 0.039993348654967786, 0.019633308939198603, 0.0014379819574746855, 0.001350696509875053, 0.001358802183246126, 0.0013472526951940084, 0.0013638210592183228, 0.0014010500614246239, 0.001407796980299968, 0.0013345873876645857, 0.0013348511230124502, 0.0013462069791228492, 0.0013271307534709269, 0.0014527057539861724, 0.001455793938865619, 0.0015861059596039811, 0.001501134672791374, 0.0015352057953536206, 0.001537193981360416, 0.008335288428720467, 0.0014167305709299992, 0.0014127276538472091, 0.001392075979645003, 0.0014084165307636164, 0.0012992471432769482, 0.001309240510098028, 0.0013100398153218689, 0.0015515896315895477, 0.001373727102668918, 0.001373432633676091, 0.0014991842243554337, 0.0014446101849898696, 0.0015157233253691573, 0.001503389428502747, 0.09191093628760427, 0.0014470084499073575, 0.0015261628170858842, 0.0014185957344514982, 0.0015643971843873055, 0.0015178571611984956, 0.0014404625921719233, 0.0014023579162906628, 0.0014218015296916877, 0.04759503402081983, 0.002385081428730366, 0.0016063884713173825, 0.0014398176322824188, 0.0014691863669918813, 0.0019134994292137573, 0.001472791918192287, 0.001444999204606426, 0.0014097161225176283, 0.0019676482034086877, 0.0013485280392995598, 0.0013372661232264067, 0.007921255367085794, 0.012029029305416102, 0.009013491571044587, 0.0013555303876459294, 0.0013426502225730493, 0.0013258979199644253, 0.00131644036772908, 0.0013096191022279008, 0.0013856830393743453, 0.0013110038767359695, 0.0013109186749753294, 0.0014141549983499, 0.0013059926319069096, 0.0013165989372765227, 0.0013085480408781037, 0.0013282328573226625, 0.0013210388774774512, 0.0014292616325867723, 0.0013259221628612401, 0.0013273542043657936, 0.0013282249600873614, 0.0013307496948091655, 0.0013302077553520094, 0.013530428122196878, 0.0013787393667259995, 0.0014607434693191732, 0.001484148712752729, 0.0013787202864923344, 0.001381921123390143, 0.0014603668990145837, 0.0014523862438200383, 0.0013705273253881202, 0.001397567164456966, 0.0014785989591547726, 0.001371902019279648, 0.00136785336760614, 0.026149959163265114, 0.00865824377563383, 0.0015199573862613464, 0.0014988498581687407, 0.0013866315510276022, 0.0013748861431163184, 0.0013167128769908936, 0.0013322415904198982, 0.0013166526522563429, 0.0013166028146194865, 0.001297869082844379, 0.0013932081020190095, 0.0014746377734011229, 0.0013698355118953148, 0.0013910936125155007, 0.0012970709180155275, 0.0012943433687965177, 0.0013080516316908964, 0.0012965471417243993, 0.0013086006534761007, 0.0013204459172236373, 0.0013074602442318384, 0.0013098887961395845, 0.0013160651635226546, 0.0013134317966748256, 0.0013400633078144522, 0.0013718155702119883, 0.0012538474081654329, 0.0012542316951427836, 0.0013535871830939942, 0.0013593349163895662, 0.001286619226448238, 0.0012732801854382365, 0.0013791086123686057, 0.0012702139199008138, 0.0012688640414794184, 0.0012825658991552737, 0.00127101328689605, 0.0013779611445545238, 0.0013755255910967077, 0.0012755444895818221, 0.0012839054902635363, 0.001373519633934662, 0.003067376918862669, 0.0013425724903995894, 0.00137341681544726, 0.0014169269992152648, 0.0013785539598831413, 0.001304934388121628, 0.001445917776139567, 0.0013615722050510195, 0.0013523636129209582, 0.001360454142796902, 0.0013351361837466152, 0.0013562070623952516, 0.0013751768986979614, 0.0030188011646993004, 0.0013557188973134877, 0.001344124019164972, 0.0012762680797058406, 0.0012907866122467177, 0.0012697873266451821, 0.0013100544907798876, 0.0013377398789423157, 0.0012775329606873648, 0.0013762814710296843, 0.0013854503869174086, 0.0013757864897120365, 0.00135665702186905, 0.0014584804291133673, 0.0014526627943565954, 0.001480882937487747, 0.001438605530206494, 0.0013373236941667845, 0.0013390753266154503, 0.001448548467810817, 0.0013327282658607072, 0.0013457741231980677, 0.0012408386326718088, 0.0013620827741426776, 0.0012731311429406004, 0.0012547735704527218, 0.0012499803049984028, 0.0012594771818542968, 0.001266993938622122, 0.0013513762446842631, 0.00145417683738835, 0.0013511334902283792, 0.0013452854710726105, 0.0013589504488496756, 0.013260315428963122, 0.020116099386894126, 0.0014007816124441369, 0.0014706660817586342, 0.0013681407946599076, 0.001381105327579592, 0.001267875245373164, 0.0012836069776201431, 0.0013599140826156552, 0.0012681525728038075, 0.0012913649588139082, 0.0012623837754624535, 0.0012774129388663843, 0.0012876406734885306, 0.001259968530538739, 0.0012964381033326595, 0.0012828746753535708, 0.0013722291011933464, 0.0013763791411088742, 0.013905467244568375, 0.018224327245309035, 0.0014083622051973123, 0.0014958694691256601, 0.0014874695310825292, 0.0015003275914991997, 0.0014940703463531574, 0.0015245083272837255, 0.0015028720004103926, 0.001499779836027598, 0.001489792389757171, 0.0015033415105307894, 0.001494040021349733, 0.0015075465911353122, 0.0013978934908589842]
[756.6297125345158, 744.7807966129961, 734.5918355193161, 779.8533391503388, 738.5060430408937, 729.8710788747167, 735.3097969364455, 744.7093040155279, 790.734246793049, 787.9785222950271, 42.33176639041654, 68.49694358609813, 108.389473064382, 234.95049762970788, 785.0734402766524, 773.4766339393815, 780.8883411705565, 792.3791633292226, 788.7842984107718, 746.6568367980623, 780.4794139101797, 790.6650029044307, 733.3035130514559, 213.67128166407898, 737.1740111093325, 739.5220444597938, 705.7340588777126, 733.8317827616966, 796.8813777498767, 800.7383792221816, 798.5222583565998, 721.209429682479, 753.7763883030507, 773.9807015394932, 780.2847451804495, 758.2983333131135, 772.7468533026974, 754.5833015680823, 760.552624712011, 758.4063103979273, 756.6472042278729, 715.2247329195744, 739.4724916940651, 757.4310983674392, 70.51686697434498, 140.48605846423024, 124.15683729098303, 667.3461612381644, 720.0277900717102, 734.6473872658672, 683.8588414073779, 721.3980151238301, 728.7856266582203, 729.4982518850592, 564.3953819831581, 691.8406586734834, 52.72906780385746, 39.42215139079385, 143.6471065038503, 717.1565358951557, 772.9842693757877, 788.1715590223492, 749.1247769071593, 759.425925791475, 799.3840891708527, 750.0456034406868, 751.3167711768194, 803.4092369833952, 57.00974938975235, 109.1820679625819, 715.3045643875555, 727.4811948377903, 708.2223610215432, 721.0392033187098, 672.7271424656645, 690.2140951147978, 30.158474561864743, 722.066081707315, 784.2701021234363, 785.3996187445556, 776.5473079059701, 716.0569112777279, 714.9177498165222, 730.7083747231975, 769.2362034514707, 771.3741480702721, 727.205290172229, 717.0943090223927, 759.2357467816676, 756.7357319439576, 754.5251579830785, 57.1953510683074, 94.30340442950202, 721.7234426273162, 726.2960894906853, 669.5437191087473, 644.7757152065668, 685.1154559331909, 744.7179845461853, 739.5394674597081, 746.9698364163354, 742.3755937231102, 744.083278421911, 748.1426937732015, 744.3655922809108, 744.6330154942767, 739.3105573794682, 741.6797183032778, 680.4031552479704, 735.437034174054, 182.45390746874318, 710.3886110446191, 678.06041568012, 753.6127075650485, 790.9659797456333, 789.0393229741735, 793.6197767413724, 786.6715738580551, 790.4420394106761, 791.649065516906, 782.9865644781212, 747.5731293490873, 794.5418026583361, 787.5599211729051, 796.1856023914615, 725.8206443093617, 731.8530791087527, 667.9065190848025, 664.7215465876607, 728.942906916214, 729.928791051762, 736.7582807642002, 658.1598124455264, 730.7708059008618, 732.9722647441725, 672.8045393717126, 714.3645857815674, 669.9191461675964, 732.1140026946091, 723.6687648125762, 727.6690656422588, 727.0120505906962, 721.9290796623101, 722.3759466608409, 724.5203450790111, 724.5121826392019, 666.8670082083904, 663.07099109387, 732.0514729517904, 722.1136895247118, 661.9973424688058, 710.2654760370515, 716.2212866973998, 728.360495572357, 676.5550472932312, 44.34439981958356, 702.6520396116072, 738.8712682457729, 694.7698916977032, 675.4145723404225, 739.4554966508855, 737.1685434142371, 738.827473868389, 728.1070867322936, 733.4045769310477, 667.0465654092276, 715.2139590600331, 46.413712639337795, 160.69525657931024, 116.43419554831627, 755.9508726001744, 753.9063621429924, 683.2579355325458, 677.9008954976189, 761.9483024123809, 755.1066830796116, 751.6598530095652, 757.1706451000365, 756.8377351526917, 763.8833101256481, 760.7187802175927, 754.3845163523763, 702.8362756469754, 688.3819131571547, 694.0542351112533, 762.7135680764227, 759.3015358509888, 31.729346693546816, 704.2372242533518, 777.0410689685349, 701.6745965057366, 765.1688638120431, 761.3673374575442, 721.1201565146415, 687.0247138811471, 43.11900749308284, 417.8033724476276, 693.2244243486246, 756.8972184193951, 697.6075943031655, 761.2675642368552, 762.9903453641342, 771.5600930939956, 761.9259677362091, 698.7875141739595, 761.8775733042684, 706.5401196043498, 724.8672547055185, 716.4197465045565, 662.4638882866635, 717.5870644405489, 66.28306600207553, 38.65750720234285, 369.7766790559621, 727.3472484245458, 691.313126767926, 741.612704523712, 752.0974576775291, 757.948322599474, 804.4147460767191, 800.103288659796, 88.06131540817239, 22.97893111782907, 207.02559176390275, 734.8972130196848, 776.7371345605126, 749.7417333865178, 753.6134604786138, 795.3511363742182, 787.5192414431775, 738.4532889368838, 737.0413839639541, 746.5207970392821, 57.602475857592964, 104.40523864753689, 32.254302994375415, 126.87581982280292, 798.3383355747077, 803.9352405531089, 796.1874382846472, 814.6493233948211, 823.8408655293294, 754.9552650363362, 812.9733286627556, 770.4652305656037, 807.7157748842322, 755.7007829922993, 800.9836335233814, 753.973540330343, 25.615718074540897, 560.5752205984705, 796.1321106815343, 798.5402544312316, 793.6681992399195, 803.1539895791832, 792.1112693576712, 783.6844451851148, 782.7343626601962, 743.4570621821881, 766.3115073315201, 726.0276350049016, 783.0038933286648, 779.1052862832244, 731.8890318826773, 763.2814638268284, 777.9348469993292, 772.0490158945084, 775.2682834066685, 778.763283827046, 30.53157719019077, 207.37991303024958, 722.171106973872, 728.9898438892326, 676.7675560433798, 725.1143568762335, 732.7941943664117, 727.1733237215552, 694.2484884207902, 659.2017845652682, 724.3848000803629, 722.2932077865115, 675.2715835541946, 662.9519084194338, 715.9154654270238, 705.2976307080739, 725.5785402645613, 116.09851786189492, 714.3276779181116, 707.9412390740898, 757.969402333487, 719.201214011188, 752.6397420502593, 750.0869496199999, 750.9484518659893, 698.8668977012377, 748.230258699751, 749.8165370855027, 761.4999893692047, 677.6896156251959, 705.7442638362601, 660.969273143228, 713.7707891827624, 297.4041234190625, 654.0799001814041, 69.28159136089431, 97.87383397036555, 263.9254044300267, 724.124721462499, 728.8557427464543, 711.5009140585311, 732.6569926045328, 712.265455496271, 722.3099139459456, 718.5036648532256, 712.9196107734502, 717.0135743588447, 67.31464596744408, 294.1447787515976, 686.5620676278394, 740.5668221517531, 749.3632664984076, 699.1306105019216, 747.2386900561561, 676.6389446341623, 753.7511232156481, 753.3406157136193, 748.6761759385939, 753.8214045000889, 742.3233313948466, 752.5511528295007, 747.2124711635942, 731.5387357576474, 744.7557224352371, 700.370315546477, 25.79431055301347, 678.0541108537586, 773.8057965259048, 783.4321815421417, 774.3616378731958, 783.8289250688722, 775.9496162607641, 779.295697708625, 782.4735255416326, 771.6788083597107, 760.9759489849671, 772.7499219487445, 752.7713355269393, 772.701606725633, 778.6001408494328, 759.3580657031455, 28.55135751745402, 180.39441472181454, 729.7895498427821, 688.3861384245178, 746.2539269458596, 763.3925179658055, 686.0332933933338, 709.0048994193319, 757.7188785928676, 700.9741544992575, 685.413739062535, 748.604711131187, 746.7487547300643, 40.83112712216544, 705.4919327277421, 747.5757769231673, 322.3307380746783, 778.2973848672208, 761.9454458510994, 744.6305718323193, 706.1204912137063, 760.1705507079437, 763.3596237676069, 133.58213088857687, 243.3200489945827, 253.1217832800734, 673.8172895424225, 750.5253673140056, 749.3298973358774, 742.8250073527282, 746.4969136229543, 737.7543135052522, 679.6017912451067, 760.4291774346175, 700.7681323782209, 704.2141482091504, 22.11019078810698, 689.0104217503521, 726.7580267531041, 778.7717245388668, 729.7703563120363, 790.1744610549467, 793.7917843495694, 787.2587474004655, 792.1130253907816, 717.6582370465594, 743.9620789861244, 716.908365989694, 780.4121265023666, 770.4441523788071, 37.15896622196512, 704.1382406384822, 748.4771203956266, 753.556359357911, 696.7185262768273, 744.3768226269216, 747.2879093386978, 25.71974188674274, 143.69145735597323, 728.2251542537728, 761.2771913400408, 774.8465701684019, 686.6225042160208, 725.1925243647445, 755.1021680208387, 757.2003300037685, 708.8397068530156, 708.0936931757255, 80.71721175736738, 570.3707288201745, 646.1587575169165, 754.0823318808958, 763.6364545950328, 752.207488280331, 758.5171846700655, 751.8658431475548, 711.0169533233301, 750.0784068020154, 739.0890578683254, 21.889085715119432, 742.6378517527943, 707.6525853658056, 726.1327611690467, 779.5195337107587, 781.6595243094156, 776.8440610038023, 777.0941449681892, 723.567395799038, 781.6273420366068, 785.9797382773794, 771.936107192138, 786.6693390340279, 756.3044002360804, 782.236451520517, 677.9058195898026, 660.4362020910187, 28.280398226501497, 725.6358645513241, 731.8515928178055, 751.4408498539195, 752.8107961534419, 733.459445675356, 756.0921423930068, 745.0720585330489, 711.0066769831074, 88.01622096077871, 101.79259503494127, 707.263024864646, 734.3074194764357, 674.5788617710588, 729.0679387004516, 733.7944747311718, 732.0892508035746, 734.161346066324, 646.8163829069533, 647.9267563769303, 641.7302592684543, 696.0237358353003, 707.1974411378484, 645.2807030958246, 155.12114004715954, 375.7269935575912, 740.7833962388319, 742.8976700925405, 743.2122154965692, 732.1074179016924, 699.7111385169728, 741.9709776946571, 27.38865847486379, 222.36326198283396, 645.1045596346968, 782.223751753984, 730.2872016313975, 721.4851582237005, 777.0214282251068, 774.9143221830382, 766.2046695323047, 724.5336622854702, 784.5535062566349, 769.868406606292, 777.2425170490444, 739.6495700731952, 783.185473611358, 785.1878316900999, 728.7740075700546, 714.2273380715633, 779.9531039651512, 726.1796589195245, 672.2391721985302, 727.4505443507157, 727.9969647876804, 667.846810213618, 737.7592687601025, 741.6181142076771, 777.5440089094427, 777.6950214645719, 54.676722156897725, 56.0344434427665, 225.56048669954063, 659.8577340658394, 753.3609895605022, 705.8458348781811, 720.2074568627936, 285.64553441301706, 772.4716581402271, 770.092172192501, 743.4413042986504, 761.5298728003662, 714.1092270029498, 94.60550674228593, 30.487631743373033, 96.79082052627541, 424.1929755704467, 738.9551725761038, 703.6195803619719, 728.1778508440486, 755.3881687128451, 760.8594872896715, 761.9394638315113, 767.0785659126749, 39.78488423576596, 35.44598661754296, 624.1188865214948, 664.2280286383864, 689.343119541643, 673.8663381039877, 634.4010961514325, 680.4366208504514, 689.3356141867001, 153.4867149271747, 157.52006938064346, 200.6695153811847, 696.7990149576411, 698.4132349086242, 760.6658279501975, 736.2897105632397, 775.2231722779688, 768.8499777706573, 767.2716247964879, 760.8211752336616, 710.0997483198445, 744.3298959587728, 767.9460455985682, 713.8874043653311, 718.5731130329696, 756.7892487677224, 762.6542118372014, 761.4244480665923, 768.639689570561, 701.0204856580527, 772.9519685172068, 765.7322137559452, 729.1333561543134, 743.669652721561, 747.0663423427559, 137.3716081905205, 176.59042827900535, 702.5584063528548, 739.1084786506045, 695.4666809919364, 699.6172093653859, 744.3665203414383, 737.0109640561686, 725.1111589744511, 720.6104368770891, 726.6222566732401, 674.9267142810877, 38.047398530215176, 115.52016124577891, 691.5528010112747, 698.9511136806619, 708.9164261508475, 767.1082638592146, 775.9866402220196, 745.913445200645, 729.7898117695265, 721.2837329295234, 729.4077733019077, 707.9461079486282, 747.6021009905076, 693.3572804562074, 29.362343914814467, 615.0656383580128, 713.7029958568845, 672.9176413071459, 661.3048965946136, 712.5756148486997, 722.0607515982807, 706.7967807899464, 754.6408493276975, 755.6498673135083, 715.1000640372048, 698.1587561937098, 748.4831457849316, 757.851960944682, 764.3960072774192, 760.876843616949, 709.3540689185153, 747.0630287682974, 712.353005322946, 769.8467203919673, 697.8920868926982, 774.7498343131549, 761.8205987751701, 732.2003074120065, 772.8237683277803, 681.6885215377501, 780.6208235753905, 774.6589765836096, 773.0735760282574, 772.9131613744571, 727.1425911392746, 769.3282332087689, 771.3665949718435, 706.3736598739702, 771.3757016830688, 736.4149512601424, 753.7776990507577, 775.7540934831181, 704.3510783865969, 619.2652325330406, 83.83368312935585, 368.2980023529969, 729.1194373701503, 667.6230472945842, 730.306237356734, 735.2228578448348, 676.6436437354563, 727.0899809386825, 735.412883285014, 737.7172158648397, 733.2097955010381, 754.596431917628, 774.971581258487, 738.5964561927992, 783.3203157201496, 782.8644321859414, 768.1295213141186, 779.1600192077152, 788.2719297385936, 716.8291930834283, 766.0104826177957, 36.2824531426878, 94.77746149197274, 710.2327188307478, 703.405042620151, 656.8651478917212, 656.3328798513832, 697.2154009457133, 710.693882035355, 766.5127174517221, 764.8265085453273, 761.9164552417332, 758.6459914452645, 755.2352406359537, 746.0852031287426, 136.50430495245024, 40.34959488500786, 670.557689171562, 747.2938585450886, 716.8953703533682, 593.3067561683017, 718.674552569394, 702.8438267104854, 297.63810559788516, 737.8975319470375, 722.2963288221459, 629.1869753510482, 670.791165721927, 665.0256629528909, 657.0123843133663, 651.9459841438639, 708.5057945891718, 725.1840356652604, 710.0030095241141, 657.4561709881531, 718.8654744025215, 757.1633212838556, 732.0794067955975, 712.6777946965143, 616.1312772719963, 731.6494397085495, 768.4577995778845, 771.670701787221, 768.974810322883, 775.2639167089197, 730.8060864979301, 771.4720096199559, 770.3235285322689, 711.799521067615, 715.6847213793731, 707.2337073522128, 703.9067537183968, 712.1586957277678, 720.6031345867272, 575.0563620022706, 746.4594663801538, 734.3271842214272, 687.6855266978968, 52.54102366864938, 255.9127144645787, 219.82149131060143, 716.8496750731612, 724.7375609643345, 733.2057569083624, 708.4410139973196, 774.2221804356235, 768.0156161311104, 765.7079948880712, 777.9719743789667, 755.9974782457873, 41.2091652791983, 119.72751485202103, 675.3055885470997, 665.1004948617012, 737.1091708884195, 685.0500640680735, 739.0104611062361, 680.9104232812849, 734.4548723277695, 734.3446272236534, 671.6074128234981, 738.9647677487119, 732.0615568502661, 39.541432170626145, 121.64275945086465, 663.7367606176219, 760.0824666013978, 711.7205109171201, 704.7876726417893, 767.7249438090136, 748.3898849071217, 761.8951553978014, 705.2786780393467, 748.1600692080085, 725.0636047567043, 767.7037380427308, 765.8294041324256, 715.1292536826859, 717.591142006998, 763.9154522626169, 715.4963632675333, 674.7118296160106, 754.9425974753401, 735.3603928567592, 759.7430930188181, 709.5711724818547, 640.2708346751955, 644.539935247216, 750.0838016805455, 55.75219285063872, 180.73810162530268, 669.9084859746665, 753.9635055168064, 734.4154531721758, 737.7180485489125, 748.9651929197315, 783.5610550588556, 760.1888311948574, 756.1916979289126, 694.5815199181856, 696.0763170450125, 751.8373713957745, 720.0173673938531, 715.1529364067425, 695.567970297212, 761.7506171392483, 753.8131490130844, 758.7951442142066, 748.1024418771467, 768.1109438397642, 758.9396659395538, 701.1787115424637, 760.0864992426417, 765.4576396443028, 693.819957280469, 714.0953028658495, 725.2948654405199, 691.3233583569461, 734.5274820556428, 727.8011510591988, 666.799146182626, 717.3850682207784, 633.8868591938782, 745.0122685344077, 734.9971085044918, 723.8005693259051, 676.3269148851032, 25.0041577820162, 50.93384936267489, 695.4190174653872, 740.3587650437518, 735.9422970686128, 742.2512521721079, 733.2340216048228, 713.7503701924642, 710.3296952568572, 749.2952572779178, 749.1472140677615, 742.8278232902729, 753.5052574018335, 688.3706471568904, 686.9104021543174, 630.474902351215, 666.1627488361792, 651.3784686239145, 650.5359844793306, 119.9718532299797, 705.8505128067926, 707.8505168896149, 718.3515947563528, 710.0172272600487, 769.6765047162698, 763.8016027514502, 763.3355782811121, 644.5003109330758, 727.9466191335748, 728.1026935579856, 667.0294309092965, 692.2282636454017, 659.7510134354158, 665.1636502432495, 10.88009806440049, 691.0809678160646, 655.2380839086617, 704.9224636126875, 639.2238556678616, 658.8235214507292, 694.2214295840923, 713.0847185182737, 703.3330455178552, 21.010595339895396, 419.2728969141834, 622.5144277709553, 694.5324029785537, 680.6488424252619, 522.6027166419864, 678.9825416936058, 692.0419034226179, 709.362675241373, 508.2209300766436, 741.5492825194876, 747.7943115670287, 126.24261605744658, 83.13222743166378, 110.94479781979854, 737.718614878595, 744.7956163025127, 754.2058743306804, 759.6242294856438, 763.5807986450548, 721.6657573087663, 762.7742508967391, 762.8238265953601, 707.1360644107931, 765.7011039487085, 759.5327412830593, 764.2057981523919, 752.8800349177583, 756.9799928292185, 699.6619633524576, 754.1920845806479, 753.3784100060899, 752.884511321205, 751.4561182322147, 751.762268695669, 73.90749139411815, 725.3002446536602, 684.5828997380918, 673.7869267462067, 725.3102821487787, 723.6303021020402, 684.7594263296252, 688.522081681123, 729.6461598945571, 715.5291176210172, 676.3159096038055, 728.915028877263, 731.0725138251371, 38.24097750044589, 115.49686355727432, 657.9131816713029, 667.1782330631678, 721.1721089563569, 727.3329540825823, 759.4670162908386, 750.6146086347734, 759.501754913343, 759.5305044893221, 770.4937371714132, 717.7678614923498, 678.132635714049, 730.0146560052253, 718.8588826827478, 770.9678677631324, 772.5925161032048, 764.4958163519256, 771.2793216835962, 764.1750730779863, 757.3199227292814, 764.8416113696229, 763.4235844654388, 759.8407949066468, 761.3642387306816, 746.2334011897758, 728.9609636413944, 797.5452144237792, 797.3008526834896, 738.77768088364, 735.6538759822557, 777.2307295302423, 785.3730949687408, 725.1060511343699, 787.2689665360352, 788.1065010196527, 779.6870325794738, 786.7738365207075, 725.7098677650207, 726.9948348999447, 783.9789267780402, 778.8735289189694, 728.0565747249957, 326.01145097316015, 744.8387384299602, 728.1110794280942, 705.7526609019583, 725.397792977773, 766.3220535090948, 691.6022587881063, 734.445074811533, 739.4461004759725, 735.0486639293412, 748.9872659984638, 737.3505327673637, 727.1791730553468, 331.2573254885467, 737.6160367622037, 743.9789675220916, 783.5344438219313, 774.7213912138582, 787.5334546313604, 763.3270272633398, 747.529482929559, 782.7586690694534, 726.5955555238537, 721.7869433960564, 726.8569705240443, 737.1059773252874, 685.6451276537974, 688.3910043575625, 675.2728218318567, 695.1175836620496, 747.762119494224, 746.7839785589415, 690.3462481385205, 750.3405049747156, 743.0667470582822, 805.9065648582941, 734.169772192751, 785.4650367677451, 796.9565374565551, 800.0126049996267, 793.9802438720843, 789.2697585337445, 739.9863686620013, 687.6742733682682, 740.1193199873775, 743.3366534484976, 735.8620035384513, 75.41298737251789, 49.711426691971496, 713.8871549399925, 679.9640057001873, 730.918925817558, 724.0577384148645, 788.7211329736765, 779.0546619293381, 735.340572454844, 788.5486505689622, 774.3744269772344, 792.1521326853765, 782.832214684964, 776.6141755143209, 793.6706161799285, 771.344191002542, 779.4993690435051, 728.7412860799695, 726.5439951337494, 71.9141602660357, 54.87171002470893, 710.0446151633978, 668.5075273208851, 672.2826781347477, 666.521102235247, 669.3125276469594, 655.9491884060338, 665.3926613357144, 666.7645316853018, 671.2344665440231, 665.1848518750254, 669.3261128952814, 663.3294160725833, 715.3620834055928]
Elapsed: 0.1500268782486014~0.32305565896555216
Time per graph: 0.0030617730254816605~0.006592972631950043
Speed: 661.7057968897492~201.58107676795558
Total Time: 0.0729
best val loss: 0.6926048398017883 test_score: 0.4898

Testing...
Test loss: 0.6924 score: 0.7551 time: 0.07s
test Score 0.7551
Epoch Time List: [0.32660016184672713, 0.3218054490862414, 0.3938263200689107, 0.3281263851094991, 0.3208367241313681, 0.32987178303301334, 0.3189153008861467, 0.36945496092084795, 0.33579947997350246, 0.327330493950285, 1.4136506370268762, 8.064478895161301, 2.6484901640797034, 1.3466251919744536, 0.791385494871065, 0.3209166090236977, 0.32727601984515786, 0.3246773611754179, 0.32610511395614594, 0.3290600188774988, 0.3227558000944555, 0.3231145069003105, 0.33391408307943493, 5.192906555952504, 0.9185419549467042, 0.34168969094753265, 0.35484900791198015, 0.3255407069809735, 0.3241060159634799, 0.31414199201390147, 0.3251491420669481, 0.3334731929935515, 0.3453267910517752, 0.34065268689300865, 0.33006096887402236, 0.3515259630512446, 0.33186741010285914, 0.3408085670089349, 0.34503403794951737, 0.3449877310777083, 0.33649325696751475, 0.34866378805600107, 0.33900226501282305, 0.3393885479308665, 1.327899423893541, 4.540627957088873, 2.1585350779350847, 0.8389999868813902, 0.3466189770260826, 0.3471061891177669, 0.35166431579273194, 0.3432915919693187, 0.34366404998581856, 0.34301820409018546, 0.35968269198201597, 0.3674976989859715, 1.8839392440859228, 2.6761979589937255, 2.1825801201630384, 1.049961820943281, 0.3390514929778874, 0.32889488304499537, 0.3251790431095287, 0.32377091399393976, 0.3214923780178651, 0.3273049920098856, 0.3188480429816991, 0.3144057639874518, 3.796347885974683, 8.1209654341219, 0.4311568171251565, 0.35554768587462604, 0.3586442330852151, 0.3563088378868997, 0.3527742519509047, 0.352823346038349, 6.302943927934393, 0.8418394098989666, 0.3323720870539546, 0.3244832259370014, 0.33080992894247174, 0.3338668519863859, 0.36303490097634494, 0.3484336379915476, 0.34517451794818044, 0.3315365739399567, 0.3354697360191494, 0.33638126496225595, 0.3450286128791049, 0.3393976829247549, 0.34122289705555886, 2.209568023099564, 1.9420918080722913, 0.3601619660621509, 0.3522231130627915, 0.35023762797936797, 0.36253334302455187, 0.3541899989359081, 0.343826569034718, 0.3397510180948302, 0.3373864550376311, 0.33650068507995456, 0.33678696886636317, 0.34099529299419373, 0.33807966101448983, 0.33647707500495017, 0.33973958890419453, 0.3417512798914686, 0.3496207151329145, 0.35774509399197996, 4.269721527118236, 0.605136388912797, 0.34937198692932725, 0.3399470141157508, 0.3358042559120804, 0.33218001388013363, 0.3267733689863235, 0.33181365788914263, 0.32202182803303003, 0.3284684410318732, 0.3324559648754075, 0.32359825703315437, 0.3307339318562299, 0.3305399409728125, 0.31659840303473175, 0.33688851399347186, 0.3467356018954888, 0.34920039190910757, 0.3585623591206968, 0.3460348730441183, 0.34588989592157304, 0.3451710370136425, 0.35021798603702337, 0.34922195703256875, 0.3406694281147793, 0.34681219107005745, 0.3599408849840984, 0.35392186185345054, 0.351064958027564, 0.35175905807409436, 0.3570487340912223, 0.34816248703282326, 0.34551078104414046, 0.34836666577029973, 0.35718214488588274, 0.35485619690734893, 0.3528207989875227, 0.3526305948616937, 0.3530226150760427, 0.35003633110318333, 0.3575528039364144, 0.3586359379114583, 0.3625550731085241, 0.36115345906000584, 0.35672241600696, 3.611576601048, 0.5035285770427436, 0.3523877590196207, 0.3378527278546244, 0.3441723920404911, 0.347702061990276, 0.34493139397818595, 0.3487901670159772, 0.35009472689125687, 0.3514780189143494, 0.34908232605084777, 0.3529223409714177, 3.495209724875167, 2.932991707115434, 1.6781245010206476, 0.3964976300485432, 0.34539113205391914, 0.3405182260321453, 0.34823861613404006, 0.33536289096809924, 0.3384031009627506, 0.3369375340407714, 0.33931080193724483, 0.3380134040489793, 0.3297421848401427, 0.3396804379299283, 0.3436887818388641, 0.33531151490751654, 0.3429047710960731, 0.33829234493896365, 0.33800665894523263, 0.3318531708791852, 3.467263223021291, 3.7080046200426295, 0.3554401269648224, 0.3376922579482198, 0.339992014109157, 0.34097064984962344, 0.3384602880105376, 0.3579026000807062, 5.871244700974785, 0.8606928100343794, 0.46523384493775666, 0.3476053300546482, 0.3398985269013792, 0.3438338930718601, 0.33651381998788565, 0.32917851605452597, 0.34749383199959993, 0.34002957097254694, 0.3447587580885738, 0.3422429470811039, 0.3497083930997178, 0.3594329620245844, 0.35343203123193234, 0.35511494288221, 1.0240514650940895, 4.670388731057756, 6.5855670620221645, 0.6101160378893837, 0.34159460105001926, 0.34989704797044396, 0.3426584380213171, 0.3434694459429011, 0.3272646788973361, 0.3220152298454195, 1.1918389501515776, 4.934551640995778, 1.994765130104497, 0.5129701402038336, 0.3346327490871772, 0.33745102013926953, 0.3254424158949405, 0.3273548159049824, 0.32490866794250906, 0.32657713105436414, 0.4100297500845045, 0.3253509340574965, 1.11135126685258, 3.045126480050385, 2.2737302610184997, 6.8633634420111775, 1.0273161277873442, 0.3235800957772881, 0.3158875380177051, 0.3148463418474421, 0.3143965151393786, 0.31406658713240176, 0.31330789695493877, 0.3131876199040562, 0.3208164209499955, 0.3182946159504354, 0.32246626110281795, 0.31371166405733675, 6.4942224560072646, 0.9778928949963301, 0.340530876070261, 0.3218637170502916, 0.32595828908961266, 0.3231059020617977, 0.32194921385962516, 0.3294167050626129, 0.33398340397980064, 0.3373849631752819, 5.405792796169408, 0.3388105269987136, 0.32660316803958267, 0.3230359489098191, 0.32794292899779975, 0.33834538410883397, 0.3389446820365265, 0.3294994489988312, 0.3280781051144004, 0.324690421926789, 1.8701919849263504, 5.230003613978624, 1.0592427219962701, 0.34922083909623325, 0.3586196060059592, 0.35877331893425435, 0.36093812994658947, 0.3538528879871592, 0.35039843199774623, 0.3626616479596123, 0.3516011079773307, 0.35463362210430205, 0.3519493010826409, 0.36375516396947205, 0.36140821990557015, 0.35527547379024327, 0.3525257569272071, 5.250566111877561, 0.7177382279187441, 0.3452128949575126, 0.33449813595507294, 0.33041806309483945, 0.3331150838639587, 0.33936096099205315, 0.3396713499678299, 0.33612023014575243, 0.3392387378262356, 0.3418874980416149, 0.34329176996834576, 0.36144488712307066, 0.35467597807291895, 0.35601988492999226, 0.36267009703442454, 0.44991031393874437, 0.3576922760112211, 2.6252626040950418, 4.466619957820512, 1.198562851990573, 1.3507290340494365, 0.3479601979488507, 0.3456183950183913, 0.3525888390140608, 0.35178625595290214, 0.3484665510477498, 0.35804365994408727, 0.3548733899369836, 0.3628928699763492, 1.5401184358634055, 4.082686243928038, 0.6549057350493968, 0.35296264418866485, 0.3482668639626354, 0.34125926182605326, 0.3436299320310354, 0.34560395509470254, 0.3373750610044226, 0.34411184198688716, 0.33962860400788486, 0.34268513903953135, 0.34388854703865945, 0.3472541010705754, 0.35047832096461207, 0.34671594901010394, 0.34505815198644996, 0.34883884817827493, 9.41683678003028, 5.626108175958507, 0.33905997802503407, 0.3289446720154956, 0.33620957797393203, 0.3283659120788798, 0.33347051008604467, 0.3335947401355952, 0.32350848603527993, 0.32812175212893635, 0.3320037960074842, 0.3345562650356442, 0.3361553620779887, 0.330152023001574, 0.326488027931191, 0.3379890250507742, 2.879852144047618, 3.007539666024968, 0.5337783008581027, 0.3416271370369941, 0.34005118592176586, 0.3347165009472519, 0.33862673002295196, 0.3495998550206423, 0.3448487708810717, 0.3449689280241728, 0.34294732694979757, 0.34371254686266184, 0.33803529490251094, 1.482365301111713, 3.750160238938406, 0.35045210900716484, 0.4316035220399499, 0.34232158097438514, 0.3414191521005705, 0.34122967696748674, 0.3342339029768482, 0.4003024658886716, 0.3427941169356927, 4.515039331978187, 1.6124614039435983, 1.9824963500723243, 0.6434021468739957, 0.3441612448077649, 0.34584473608992994, 0.3388585631037131, 0.3385798790259287, 0.3448967100121081, 0.34037905698642135, 0.3423235999653116, 0.3445860930951312, 0.3491730659734458, 3.2536645989166573, 4.138125097961165, 0.33441937912721187, 0.33129694988019764, 0.32547800010070205, 0.3260989061091095, 0.32549515704158694, 0.3244816579390317, 0.3241365069989115, 0.3335193219827488, 0.3428842939902097, 0.33787925401702523, 0.3343506009550765, 0.34120277396868914, 3.913558497093618, 3.4354896809672937, 0.3435143759706989, 0.3483543648617342, 0.35639852890744805, 0.3403969219652936, 0.33924376603681594, 2.1800950500182807, 6.594326621037908, 0.5877257710089907, 0.34171896195039153, 0.33545227197464556, 0.33708227204624563, 0.3323687689844519, 0.33538133394904435, 0.337872851989232, 0.34282239293679595, 0.33430105703882873, 3.295809278031811, 1.3945835910271853, 0.37477053701877594, 0.3519174960674718, 0.32903035497292876, 0.3420789251103997, 0.33964370796456933, 0.33642329508438706, 0.34203627589158714, 0.3356620860286057, 0.34300554590299726, 4.77713236794807, 1.3247730829752982, 0.3503381269983947, 0.3519836670020595, 0.39078084006905556, 0.3283262699842453, 0.3284035489195958, 0.3387288320809603, 0.3257903358899057, 0.4013911240035668, 0.3241902479203418, 0.3314066178863868, 0.3204480019630864, 0.3344777339370921, 0.3267817080486566, 0.3375990450149402, 0.3550875049550086, 4.658120635780506, 0.7218838450498879, 0.34902359591796994, 0.3492825219873339, 0.3383541499497369, 0.3339744480326772, 0.34056374698411673, 0.33934305305592716, 0.3364981299964711, 1.693883687024936, 2.4150753038702533, 0.4321203799918294, 0.34963310800958425, 0.34672893583774567, 0.34956989996135235, 0.3571493459166959, 0.3454527418361977, 0.3458774311002344, 0.36309075192548335, 0.36762196000199765, 0.36443937523290515, 0.35973367013502866, 0.3594632151070982, 0.3636306420667097, 5.750273868092336, 1.1215815920149907, 0.35436634405050427, 0.3405627568718046, 0.3501396661158651, 0.35331501718610525, 0.36196594196371734, 0.34670986502896994, 2.7933471020078287, 4.992419317131862, 1.1086794820148498, 0.34589711914304644, 0.3339163080090657, 0.3327920079464093, 0.321545390994288, 0.32528840110171586, 0.3345257989130914, 0.3300728611648083, 0.32824940083082765, 0.3266681699315086, 0.332338452921249, 0.3277234077686444, 0.3269727770239115, 0.3267676739487797, 0.32745365193113685, 0.3462447130586952, 0.3259412889601663, 0.34047114406712353, 0.3574514329666272, 0.3578396779485047, 0.3541059630224481, 0.35656292201019824, 0.34251524799037725, 0.3397338291397318, 0.32449416595045477, 0.3360156511189416, 1.6813233100110665, 3.36711879784707, 2.523500338080339, 0.8005382849369198, 0.35590857500210404, 0.3384563549188897, 0.3456656220369041, 0.43783363595139235, 0.32992731407284737, 0.33930665196385235, 0.3375177870038897, 0.3423533661989495, 0.3854031590744853, 1.4449709189357236, 5.643933737068437, 2.4211430300492793, 1.1228751568123698, 0.3772408489603549, 0.3388865049928427, 0.3345609491225332, 0.33589228393975645, 0.3371099838986993, 0.3335982918506488, 0.33218543394468725, 2.3483585770009086, 7.871056988835335, 4.408263724995777, 0.3586547620361671, 0.3639823308913037, 0.36920692410785705, 0.3727742818882689, 0.3730593918589875, 0.36522316420450807, 2.4859518710291013, 1.6261422949610278, 1.3411738330032676, 0.3775158920325339, 0.3717413969570771, 0.33918873395305127, 0.3352950259577483, 0.32950668199919164, 0.3346514009172097, 0.32928934693336487, 0.3375539700500667, 0.33261917904019356, 0.33166412892751396, 0.33645523991435766, 0.33543912088498473, 0.3434740810189396, 0.3370042620226741, 0.33687332703266293, 0.3380462370114401, 0.33042639098130167, 0.3441724090371281, 0.33052753598894924, 0.3352270049508661, 0.3390299609163776, 0.33701852580998093, 0.344209388946183, 0.9124419610016048, 4.374978341045789, 0.7887218128889799, 0.3558546550339088, 0.34419020696077496, 0.33939995104447007, 0.33790872478857636, 0.34310714807361364, 0.3434861060231924, 0.3555824711220339, 0.34696317894849926, 0.3492243679938838, 3.2785092500271276, 5.015983817982487, 0.7261748821474612, 0.36315773194655776, 0.35803602694068104, 0.4249617501627654, 0.3289584858575836, 0.3381251699756831, 0.35511683695949614, 0.3550667241215706, 0.3558925752295181, 0.35068015195429325, 0.34320475615095347, 0.3431001699063927, 3.1588786608772352, 1.6356814940227196, 0.36198633385356516, 0.362522306968458, 0.35950047487858683, 0.36462862498592585, 0.35705109708942473, 0.34836762794293463, 0.34490367001853883, 0.33360660495236516, 0.345338886952959, 0.35736432508565485, 0.34724504698533565, 0.33301983296405524, 0.3330832280917093, 0.3349707762245089, 0.34541542804799974, 0.3464208469958976, 0.33790195593610406, 0.339959658915177, 0.3374597759684548, 0.3342865378363058, 0.3352202851092443, 0.3365925248945132, 0.33456708304584026, 0.34202238998841494, 0.3386423789197579, 0.3346880851313472, 0.3370452548842877, 0.3314532870426774, 0.3364005599869415, 0.33989791199564934, 0.3361147380201146, 0.3382872008951381, 0.33357505092862993, 0.32993781415279955, 0.3332745609804988, 0.33032939792610705, 0.3344333480345085, 0.34599867393262684, 1.8479408031562343, 1.4316387920407578, 0.6275829689111561, 0.352065910003148, 0.3590751870069653, 0.34960074187256396, 0.3576796499546617, 0.34992313489783555, 0.3520057121058926, 0.3454498619539663, 0.3318021559389308, 0.33630420884583145, 0.3291895758593455, 0.3264069890137762, 0.32596483000088483, 0.33147607184946537, 0.3344637098489329, 0.33499192993622273, 0.32812792796175927, 0.3308606119826436, 0.33169654896482825, 3.619951055967249, 6.458885362022556, 0.78447407099884, 0.3636629299726337, 0.3625875379657373, 0.36299946205690503, 0.3464137848932296, 0.33379223698284477, 0.3333720969967544, 0.3361208630958572, 0.3341693369438872, 0.3374009720282629, 0.34066808701027185, 0.34248177404515445, 1.1901106350123882, 2.801426236052066, 6.870765182888135, 0.3524238090030849, 0.39535786118358374, 0.3551146329846233, 0.3915222220821306, 0.35016649996396154, 0.44517248100601137, 0.3458946291357279, 0.3526887510670349, 0.3592749967938289, 5.40937428898178, 0.36034324287902564, 0.3593217780580744, 0.3574004218680784, 0.3530946710379794, 0.34627271397039294, 0.35289557196665555, 0.3648307240800932, 0.35217715392354876, 0.35230553697329015, 0.33665574388578534, 0.330019410001114, 0.34545754292048514, 0.32812859711702913, 0.33087377494666725, 0.3344475011108443, 0.32825478992890567, 0.3334016229491681, 0.3307951500173658, 0.3444502209313214, 0.33594696514774114, 0.3377021460328251, 0.3339847750030458, 0.34370385902002454, 0.3403789548901841, 0.3426942570367828, 0.35494314203970134, 0.3766600890085101, 0.3515439569018781, 0.34136017598211765, 0.3534703549230471, 4.736089746118523, 1.2871985129313543, 1.509419519919902, 0.8377186270663515, 0.3369146710028872, 0.3328493299195543, 0.3392879160819575, 0.33097255893517286, 0.33329194493126124, 0.33360575209371746, 0.3356900099897757, 0.340440361876972, 3.8962125399848446, 5.2154819059651345, 0.6071778080658987, 0.3552839020267129, 0.34633178310468793, 0.3566661039367318, 0.35503934195730835, 0.3600612849695608, 0.35501019889488816, 0.35150649305433035, 0.3578076650155708, 0.3503828450338915, 0.35147578001488, 1.5306197711033747, 4.085126259131357, 0.5221671549370512, 0.34495308494661003, 0.3352598921628669, 0.34304238692857325, 0.3413472209358588, 0.3381196389673278, 0.34199069789610803, 0.3418945971643552, 0.340760161052458, 0.33734161395113915, 0.34006080497056246, 0.33455859206151217, 0.33624918409623206, 0.3409390830202028, 0.3371992588508874, 0.34173018392175436, 0.3479028770234436, 0.3401981448987499, 0.4139435888500884, 0.3519299188628793, 0.33714406308718026, 0.36814654886256903, 0.374375231214799, 0.3970428081229329, 2.338598703034222, 2.476679018000141, 1.863900202093646, 0.342684522154741, 0.37047437811270356, 0.3313427969114855, 0.34095676301512867, 0.3340747479815036, 0.33429078792687505, 0.3426034750882536, 0.337986950064078, 0.3380554241593927, 0.3364057190483436, 0.34015684807673097, 0.34754837898071855, 0.3530747409677133, 0.3410447750939056, 0.33432084310334176, 0.3400085490429774, 0.3393994269426912, 0.33210801100358367, 0.34111063193995506, 0.33758974506054074, 0.34046361793298274, 0.33187635499052703, 0.33627984300255775, 0.3362741710152477, 0.3474597688764334, 0.3427647309144959, 0.3411678069969639, 0.3436904269037768, 0.3481792079983279, 0.34840501996222883, 0.355749384034425, 0.36159127892460674, 0.3541086360346526, 0.3429486630484462, 0.34649898495990783, 2.6084452371578664, 4.611171733122319, 5.04526013310533, 0.3472151670139283, 0.3389069240074605, 0.340525271021761, 0.3435116938780993, 0.33771005808375776, 0.3452182658948004, 0.3429450870025903, 0.34764878696296364, 0.3391728119459003, 0.337520121014677, 0.3472339319996536, 0.36960959585849196, 0.38296051998622715, 0.3698887978680432, 0.3696628500474617, 0.3570186300203204, 6.9393599421018735, 0.6216729630250484, 0.3339078798890114, 0.33160613011568785, 0.3301335669821128, 0.32796155486721545, 0.3324834778904915, 0.33189156698063016, 0.3378600630676374, 0.3425153900170699, 0.344251852016896, 0.3535479340935126, 0.3636342550162226, 0.3534197910921648, 0.3670499259606004, 6.331085819052532, 4.0503186479909346, 0.3558078871574253, 0.416547138011083, 0.35996965400408953, 0.3603973010322079, 0.3716552380938083, 0.3688348749419674, 0.4589640611084178, 2.748416441027075, 4.015510409837589, 0.558227182016708, 0.3650359530001879, 0.37319825403392315, 0.3811615880113095, 0.34657199890352786, 0.3489954248070717, 0.33867526112589985, 0.41256226191762835, 0.3637029799865559, 0.34612873708829284, 0.6603664569556713, 3.558765879017301, 3.580690592993051, 2.821221360936761, 0.33673523215111345, 0.3332126218592748, 0.3384901009267196, 0.33675094705540687, 0.3308740290813148, 0.33718436607159674, 0.33137844293378294, 0.33348021807614714, 0.33393924310803413, 0.3389898990280926, 0.34065649297554046, 0.3349554701708257, 0.3398223250405863, 0.33471914706751704, 0.34040823788382113, 0.3351783480029553, 0.34077397792134434, 0.33978297421708703, 0.3343848449876532, 1.7714297788916156, 0.41447726101614535, 0.3482485660351813, 0.35004238900728524, 0.34309615788515657, 0.347758152987808, 0.3503454279853031, 0.34809359500650316, 0.34857768402434886, 0.35455518204253167, 0.35618439596146345, 0.35570996499154717, 0.3454241550061852, 3.1146591000724584, 2.962528361938894, 1.4336332210805267, 0.34882805007509887, 0.3497846870450303, 0.33963355608284473, 0.321091532940045, 0.3384918631054461, 0.3287024849560112, 0.33289707894437015, 0.32703324407339096, 0.3275087069487199, 0.33820486010517925, 0.33632769412361085, 0.33192020596470684, 0.32919907907489687, 0.32126118685118854, 0.3211362389847636, 0.3228060060646385, 0.32285723404493183, 0.32277654798235744, 0.322944491985254, 0.32461522601079196, 0.3244226441020146, 0.32591395208146423, 0.3213537058327347, 0.3381819579517469, 0.338233140995726, 0.32099818089045584, 0.32834325497969985, 0.32821502222213894, 0.3298306619981304, 0.32657057396136224, 0.3258383610518649, 0.32446476398035884, 0.3229225490940735, 0.3300429788650945, 0.3238363090204075, 0.3333056539995596, 0.32906905200798064, 0.32574585010297596, 0.3254339079139754, 0.3266320509137586, 0.41726659110281616, 0.3241199131589383, 0.33102599799167365, 0.3321308981394395, 0.3376188410911709, 0.3637811669614166, 0.34897103998810053, 0.34071024996228516, 0.34281898802146316, 0.43618964485358447, 0.3473713102284819, 0.3545144301606342, 0.3467289009131491, 0.43391212390270084, 0.32289804494939744, 0.3267118231160566, 0.32419503189157695, 0.32918662892188877, 0.32279586512595415, 0.32344905100762844, 0.32486556097865105, 0.3224854131694883, 0.32799412589520216, 0.3265962810255587, 0.32574504299554974, 0.3418417169013992, 0.3452223640633747, 0.3508418219862506, 0.3466917958576232, 0.34177280496805906, 0.34139557590242475, 0.3378810960566625, 0.3465518788434565, 0.3395441729808226, 0.3385437710676342, 0.330855845939368, 0.32614525698591024, 0.329750484903343, 0.34126963187009096, 0.31617336499039084, 0.3175066302064806, 0.32269842305686325, 0.3350868009729311, 0.3507914589717984, 0.33925195992924273, 0.3409003739943728, 0.34591629612259567, 1.7704919900279492, 2.540828000055626, 5.476408444927074, 0.34679094911552966, 0.3423210069304332, 0.3242832910036668, 0.32049344712868333, 0.3284057410201058, 0.32622751186136156, 0.3171079258900136, 0.332369432086125, 0.32552668103016913, 0.3283018060028553, 0.34296228212770075, 0.3176029701717198, 0.3184938010526821, 0.3227984021650627, 0.3466787301003933, 0.3531753468560055, 1.6711639469722286, 3.8078962399158627, 0.30838357808534056, 0.31511987489648163, 0.3147083419607952, 0.3157012171577662, 0.3188389140414074, 0.32754181686323136, 0.32437179319094867, 0.31819928297773004, 0.318741912022233, 0.3181135739432648, 0.31804306304547936, 0.31901786499656737, 0.31237854703795165]
Total Epoch List: [1000, 13]
Total Time List: [0.8964204690419137, 0.07286693400237709]
========================k_idx:2========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: AEtransGAT
AEtransGAT(
  (gae): GAE(
    (encoder): transGAT(
      (encoder): EdgeFeatureGAT()
      (classifier): Sequential(
        (0): Linear(in_features=64, out_features=64, bias=True)
        (1): ReLU(inplace=True)
        (2): Linear(in_features=64, out_features=2, bias=True)
      )
    )
    (decoder): InnerProductDecoder()
  )
)
Total number of parameters: 1914242
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x75feb28b1840>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 2.7197;  Loss pred: 2.7197; Loss self: 0.0000; time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6953 score: 0.4898 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6944 score: 0.5000 time: 0.06s
Epoch 2/1000, LR 0.000000
Train loss: 2.7340;  Loss pred: 2.7340; Loss self: 0.0000; time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6953 score: 0.4898 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6943 score: 0.5000 time: 0.06s
Epoch 3/1000, LR 0.000030
Train loss: 2.7074;  Loss pred: 2.7074; Loss self: 0.0000; time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6951 score: 0.4898 time: 2.36s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6942 score: 0.5000 time: 4.53s
Epoch 4/1000, LR 0.000060
Train loss: 2.6956;  Loss pred: 2.6956; Loss self: 0.0000; time: 4.63s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6950 score: 0.4898 time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6941 score: 0.5000 time: 0.66s
Epoch 5/1000, LR 0.000090
Train loss: 2.6913;  Loss pred: 2.6913; Loss self: 0.0000; time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6947 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6939 score: 0.5000 time: 0.07s
Epoch 6/1000, LR 0.000120
Train loss: 2.6482;  Loss pred: 2.6482; Loss self: 0.0000; time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6945 score: 0.4898 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6938 score: 0.5000 time: 0.07s
Epoch 7/1000, LR 0.000150
Train loss: 2.6334;  Loss pred: 2.6334; Loss self: 0.0000; time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6942 score: 0.4898 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6936 score: 0.5000 time: 0.07s
Epoch 8/1000, LR 0.000180
Train loss: 2.6091;  Loss pred: 2.6091; Loss self: 0.0000; time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6939 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6934 score: 0.5000 time: 0.07s
Epoch 9/1000, LR 0.000210
Train loss: 2.5915;  Loss pred: 2.5915; Loss self: 0.0000; time: 0.29s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6937 score: 0.4898 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6932 score: 0.5000 time: 0.06s
Epoch 10/1000, LR 0.000240
Train loss: 2.5272;  Loss pred: 2.5272; Loss self: 0.0000; time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6935 score: 0.4898 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6931 score: 0.5000 time: 0.06s
Epoch 11/1000, LR 0.000270
Train loss: 2.5224;  Loss pred: 2.5224; Loss self: 0.0000; time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6933 score: 0.4898 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6930 score: 0.5000 time: 0.06s
Epoch 12/1000, LR 0.000270
Train loss: 2.4806;  Loss pred: 2.4806; Loss self: 0.0000; time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6931 score: 0.4898 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6929 score: 0.5000 time: 0.06s
Epoch 13/1000, LR 0.000270
Train loss: 2.4094;  Loss pred: 2.4094; Loss self: 0.0000; time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6930 score: 0.4898 time: 0.11s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6928 score: 0.5000 time: 0.07s
Epoch 14/1000, LR 0.000270
Train loss: 2.4000;  Loss pred: 2.4000; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6930 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6928 score: 0.5000 time: 0.07s
Epoch 15/1000, LR 0.000270
Train loss: 2.3766;  Loss pred: 2.3766; Loss self: 0.0000; time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6929 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6927 score: 0.5000 time: 0.07s
Epoch 16/1000, LR 0.000270
Train loss: 2.3551;  Loss pred: 2.3551; Loss self: 0.0000; time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6928 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6926 score: 0.5000 time: 0.07s
Epoch 17/1000, LR 0.000270
Train loss: 2.3067;  Loss pred: 2.3067; Loss self: 0.0000; time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6928 score: 0.4898 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6926 score: 0.5000 time: 0.07s
Epoch 18/1000, LR 0.000270
Train loss: 2.2643;  Loss pred: 2.2643; Loss self: 0.0000; time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6928 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6925 score: 0.5000 time: 0.07s
Epoch 19/1000, LR 0.000270
Train loss: 2.2432;  Loss pred: 2.2432; Loss self: 0.0000; time: 0.97s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6928 score: 0.4898 time: 3.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6925 score: 0.5000 time: 2.74s
Epoch 20/1000, LR 0.000270
Train loss: 2.2138;  Loss pred: 2.2138; Loss self: 0.0000; time: 8.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6928 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6925 score: 0.5000 time: 0.07s
Epoch 21/1000, LR 0.000270
Train loss: 2.1654;  Loss pred: 2.1654; Loss self: 0.0000; time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6927 score: 0.4898 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6924 score: 0.5000 time: 0.07s
Epoch 22/1000, LR 0.000270
Train loss: 2.1506;  Loss pred: 2.1506; Loss self: 0.0000; time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6927 score: 0.4898 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6923 score: 0.5000 time: 0.06s
Epoch 23/1000, LR 0.000270
Train loss: 2.1193;  Loss pred: 2.1193; Loss self: 0.0000; time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6927 score: 0.4898 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6923 score: 0.5000 time: 0.06s
Epoch 24/1000, LR 0.000270
Train loss: 2.0816;  Loss pred: 2.0816; Loss self: 0.0000; time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6926 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6922 score: 0.5000 time: 0.07s
Epoch 25/1000, LR 0.000270
Train loss: 2.0668;  Loss pred: 2.0668; Loss self: 0.0000; time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6926 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6921 score: 0.5000 time: 0.07s
Epoch 26/1000, LR 0.000270
Train loss: 2.0119;  Loss pred: 2.0119; Loss self: 0.0000; time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6925 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6920 score: 0.5000 time: 0.08s
Epoch 27/1000, LR 0.000270
Train loss: 1.9989;  Loss pred: 1.9989; Loss self: 0.0000; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6924 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6920 score: 0.5000 time: 0.07s
Epoch 28/1000, LR 0.000270
Train loss: 1.9890;  Loss pred: 1.9890; Loss self: 0.0000; time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6923 score: 0.4898 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6919 score: 0.5000 time: 0.59s
Epoch 29/1000, LR 0.000270
Train loss: 1.9655;  Loss pred: 1.9655; Loss self: 0.0000; time: 3.88s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6922 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6917 score: 0.5000 time: 0.07s
Epoch 30/1000, LR 0.000270
Train loss: 1.9200;  Loss pred: 1.9200; Loss self: 0.0000; time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6921 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6916 score: 0.5000 time: 0.07s
Epoch 31/1000, LR 0.000270
Train loss: 1.9169;  Loss pred: 1.9169; Loss self: 0.0000; time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6920 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6915 score: 0.5000 time: 0.07s
Epoch 32/1000, LR 0.000270
Train loss: 1.8909;  Loss pred: 1.8909; Loss self: 0.0000; time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6918 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6914 score: 0.5000 time: 0.07s
Epoch 33/1000, LR 0.000270
Train loss: 1.8612;  Loss pred: 1.8612; Loss self: 0.0000; time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6917 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6912 score: 0.5000 time: 0.07s
Epoch 34/1000, LR 0.000270
Train loss: 1.8314;  Loss pred: 1.8314; Loss self: 0.0000; time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6915 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6911 score: 0.5000 time: 0.07s
Epoch 35/1000, LR 0.000270
Train loss: 1.8049;  Loss pred: 1.8049; Loss self: 0.0000; time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6914 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6909 score: 0.5000 time: 0.07s
Epoch 36/1000, LR 0.000270
Train loss: 1.7841;  Loss pred: 1.7841; Loss self: 0.0000; time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6912 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6908 score: 0.5000 time: 0.07s
Epoch 37/1000, LR 0.000270
Train loss: 1.7644;  Loss pred: 1.7644; Loss self: 0.0000; time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6910 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6905 score: 0.5000 time: 0.07s
Epoch 38/1000, LR 0.000270
Train loss: 1.7583;  Loss pred: 1.7583; Loss self: 0.0000; time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6909 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6904 score: 0.5000 time: 0.07s
Epoch 39/1000, LR 0.000269
Train loss: 1.7250;  Loss pred: 1.7250; Loss self: 0.0000; time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6908 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6903 score: 0.5000 time: 0.07s
Epoch 40/1000, LR 0.000269
Train loss: 1.7081;  Loss pred: 1.7081; Loss self: 0.0000; time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6907 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6901 score: 0.5000 time: 0.07s
Epoch 41/1000, LR 0.000269
Train loss: 1.6943;  Loss pred: 1.6943; Loss self: 0.0000; time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6906 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6899 score: 0.5000 time: 0.07s
Epoch 42/1000, LR 0.000269
Train loss: 1.6557;  Loss pred: 1.6557; Loss self: 0.0000; time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6904 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6898 score: 0.5000 time: 0.07s
Epoch 43/1000, LR 0.000269
Train loss: 1.6353;  Loss pred: 1.6353; Loss self: 0.0000; time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6903 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6896 score: 0.5000 time: 0.16s
Epoch 44/1000, LR 0.000269
Train loss: 1.6240;  Loss pred: 1.6240; Loss self: 0.0000; time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6901 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6894 score: 0.5000 time: 0.07s
Epoch 45/1000, LR 0.000269
Train loss: 1.6232;  Loss pred: 1.6232; Loss self: 0.0000; time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6900 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6892 score: 0.5000 time: 0.07s
Epoch 46/1000, LR 0.000269
Train loss: 1.5918;  Loss pred: 1.5918; Loss self: 0.0000; time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6898 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6890 score: 0.5000 time: 0.07s
Epoch 47/1000, LR 0.000269
Train loss: 1.5930;  Loss pred: 1.5930; Loss self: 0.0000; time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6897 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6888 score: 0.5000 time: 0.07s
Epoch 48/1000, LR 0.000269
Train loss: 1.5578;  Loss pred: 1.5578; Loss self: 0.0000; time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6895 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6886 score: 0.5000 time: 0.07s
Epoch 49/1000, LR 0.000269
Train loss: 1.5430;  Loss pred: 1.5430; Loss self: 0.0000; time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6893 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6884 score: 0.5000 time: 0.07s
Epoch 50/1000, LR 0.000269
Train loss: 1.5319;  Loss pred: 1.5319; Loss self: 0.0000; time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6891 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6882 score: 0.5000 time: 0.07s
Epoch 51/1000, LR 0.000269
Train loss: 1.5212;  Loss pred: 1.5212; Loss self: 0.0000; time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6889 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6879 score: 0.5000 time: 0.07s
Epoch 52/1000, LR 0.000269
Train loss: 1.4962;  Loss pred: 1.4962; Loss self: 0.0000; time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6887 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6876 score: 0.5000 time: 0.07s
Epoch 53/1000, LR 0.000269
Train loss: 1.4902;  Loss pred: 1.4902; Loss self: 0.0000; time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6885 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6874 score: 0.5000 time: 0.07s
Epoch 54/1000, LR 0.000269
Train loss: 1.4643;  Loss pred: 1.4643; Loss self: 0.0000; time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6882 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6871 score: 0.5000 time: 0.07s
Epoch 55/1000, LR 0.000269
Train loss: 1.4574;  Loss pred: 1.4574; Loss self: 0.0000; time: 0.49s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6879 score: 0.4898 time: 1.37s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6868 score: 0.5000 time: 2.37s
Epoch 56/1000, LR 0.000269
Train loss: 1.4381;  Loss pred: 1.4381; Loss self: 0.0000; time: 6.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6876 score: 0.4898 time: 3.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6864 score: 0.5000 time: 0.76s
Epoch 57/1000, LR 0.000269
Train loss: 1.4242;  Loss pred: 1.4242; Loss self: 0.0000; time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6873 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6861 score: 0.5000 time: 0.07s
Epoch 58/1000, LR 0.000269
Train loss: 1.4256;  Loss pred: 1.4256; Loss self: 0.0000; time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6870 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6857 score: 0.5000 time: 0.07s
Epoch 59/1000, LR 0.000268
Train loss: 1.4183;  Loss pred: 1.4183; Loss self: 0.0000; time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6867 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6853 score: 0.5000 time: 0.07s
Epoch 60/1000, LR 0.000268
Train loss: 1.3971;  Loss pred: 1.3971; Loss self: 0.0000; time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6863 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6850 score: 0.5000 time: 0.07s
Epoch 61/1000, LR 0.000268
Train loss: 1.3753;  Loss pred: 1.3753; Loss self: 0.0000; time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6860 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6846 score: 0.5000 time: 0.07s
Epoch 62/1000, LR 0.000268
Train loss: 1.3683;  Loss pred: 1.3683; Loss self: 0.0000; time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6856 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6842 score: 0.5000 time: 0.07s
Epoch 63/1000, LR 0.000268
Train loss: 1.3560;  Loss pred: 1.3560; Loss self: 0.0000; time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6853 score: 0.4898 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6838 score: 0.5000 time: 0.07s
Epoch 64/1000, LR 0.000268
Train loss: 1.3530;  Loss pred: 1.3530; Loss self: 0.0000; time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6849 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6833 score: 0.5000 time: 0.07s
Epoch 65/1000, LR 0.000268
Train loss: 1.3385;  Loss pred: 1.3385; Loss self: 0.0000; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6845 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6829 score: 0.5000 time: 0.07s
Epoch 66/1000, LR 0.000268
Train loss: 1.3288;  Loss pred: 1.3288; Loss self: 0.0000; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6841 score: 0.4898 time: 0.07s
Test loss: 0.6824 score: 0.5208 time: 0.07s
Epoch 67/1000, LR 0.000268
Train loss: 1.3199;  Loss pred: 1.3199; Loss self: 0.0000; time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6837 score: 0.4898 time: 0.07s
Test loss: 0.6820 score: 0.5208 time: 0.07s
Epoch 68/1000, LR 0.000268
Train loss: 1.3102;  Loss pred: 1.3102; Loss self: 0.0000; time: 0.22s
Val loss: 0.6833 score: 0.5102 time: 0.07s
Test loss: 0.6815 score: 0.5208 time: 0.07s
Epoch 69/1000, LR 0.000268
Train loss: 1.3059;  Loss pred: 1.3059; Loss self: 0.0000; time: 0.22s
Val loss: 0.6829 score: 0.5306 time: 0.07s
Test loss: 0.6810 score: 0.5625 time: 0.07s
Epoch 70/1000, LR 0.000268
Train loss: 1.2872;  Loss pred: 1.2872; Loss self: 0.0000; time: 0.22s
Val loss: 0.6825 score: 0.5306 time: 0.07s
Test loss: 0.6805 score: 0.5833 time: 0.07s
Epoch 71/1000, LR 0.000268
Train loss: 1.2845;  Loss pred: 1.2845; Loss self: 0.0000; time: 0.22s
Val loss: 0.6820 score: 0.5306 time: 0.07s
Test loss: 0.6800 score: 0.5833 time: 0.07s
Epoch 72/1000, LR 0.000267
Train loss: 1.2703;  Loss pred: 1.2703; Loss self: 0.0000; time: 0.22s
Val loss: 0.6816 score: 0.5510 time: 0.07s
Test loss: 0.6794 score: 0.6042 time: 0.07s
Epoch 73/1000, LR 0.000267
Train loss: 1.2664;  Loss pred: 1.2664; Loss self: 0.0000; time: 0.22s
Val loss: 0.6811 score: 0.5510 time: 0.07s
Test loss: 0.6789 score: 0.6042 time: 0.07s
Epoch 74/1000, LR 0.000267
Train loss: 1.2606;  Loss pred: 1.2606; Loss self: 0.0000; time: 0.22s
Val loss: 0.6806 score: 0.5714 time: 0.07s
Test loss: 0.6783 score: 0.6250 time: 0.07s
Epoch 75/1000, LR 0.000267
Train loss: 1.2502;  Loss pred: 1.2502; Loss self: 0.0000; time: 0.22s
Val loss: 0.6801 score: 0.5918 time: 0.07s
Test loss: 0.6777 score: 0.6250 time: 0.07s
Epoch 76/1000, LR 0.000267
Train loss: 1.2411;  Loss pred: 1.2411; Loss self: 0.0000; time: 0.22s
Val loss: 0.6796 score: 0.5918 time: 0.07s
Test loss: 0.6771 score: 0.6250 time: 0.07s
Epoch 77/1000, LR 0.000267
Train loss: 1.2421;  Loss pred: 1.2421; Loss self: 0.0000; time: 0.97s
Val loss: 0.6791 score: 0.6122 time: 0.44s
Test loss: 0.6764 score: 0.6458 time: 2.47s
Epoch 78/1000, LR 0.000267
Train loss: 1.2285;  Loss pred: 1.2285; Loss self: 0.0000; time: 9.85s
Val loss: 0.6786 score: 0.6122 time: 0.16s
Test loss: 0.6758 score: 0.6458 time: 0.07s
Epoch 79/1000, LR 0.000267
Train loss: 1.2240;  Loss pred: 1.2240; Loss self: 0.0000; time: 0.22s
Val loss: 0.6780 score: 0.6122 time: 0.07s
Test loss: 0.6751 score: 0.6458 time: 0.07s
Epoch 80/1000, LR 0.000267
Train loss: 1.2130;  Loss pred: 1.2130; Loss self: 0.0000; time: 0.22s
Val loss: 0.6775 score: 0.6327 time: 0.07s
Test loss: 0.6744 score: 0.6458 time: 0.07s
Epoch 81/1000, LR 0.000267
Train loss: 1.2069;  Loss pred: 1.2069; Loss self: 0.0000; time: 0.21s
Val loss: 0.6769 score: 0.6327 time: 0.06s
Test loss: 0.6737 score: 0.6458 time: 0.06s
Epoch 82/1000, LR 0.000267
Train loss: 1.2003;  Loss pred: 1.2003; Loss self: 0.0000; time: 0.20s
Val loss: 0.6763 score: 0.6327 time: 0.06s
Test loss: 0.6729 score: 0.6458 time: 0.06s
Epoch 83/1000, LR 0.000266
Train loss: 1.1915;  Loss pred: 1.1915; Loss self: 0.0000; time: 0.20s
Val loss: 0.6757 score: 0.6327 time: 0.06s
Test loss: 0.6722 score: 0.6667 time: 0.06s
Epoch 84/1000, LR 0.000266
Train loss: 1.1904;  Loss pred: 1.1904; Loss self: 0.0000; time: 0.20s
Val loss: 0.6750 score: 0.6531 time: 0.07s
Test loss: 0.6714 score: 0.6667 time: 0.07s
Epoch 85/1000, LR 0.000266
Train loss: 1.1795;  Loss pred: 1.1795; Loss self: 0.0000; time: 0.22s
Val loss: 0.6744 score: 0.6531 time: 0.07s
Test loss: 0.6705 score: 0.6875 time: 0.07s
Epoch 86/1000, LR 0.000266
Train loss: 1.1718;  Loss pred: 1.1718; Loss self: 0.0000; time: 0.21s
Val loss: 0.6737 score: 0.6531 time: 0.07s
Test loss: 0.6697 score: 0.7292 time: 0.07s
Epoch 87/1000, LR 0.000266
Train loss: 1.1685;  Loss pred: 1.1685; Loss self: 0.0000; time: 0.22s
Val loss: 0.6730 score: 0.6735 time: 0.07s
Test loss: 0.6688 score: 0.7708 time: 0.07s
Epoch 88/1000, LR 0.000266
Train loss: 1.1614;  Loss pred: 1.1614; Loss self: 0.0000; time: 0.22s
Val loss: 0.6722 score: 0.6939 time: 0.07s
Test loss: 0.6679 score: 0.7917 time: 0.06s
Epoch 89/1000, LR 0.000266
Train loss: 1.1501;  Loss pred: 1.1501; Loss self: 0.0000; time: 0.20s
Val loss: 0.6715 score: 0.7143 time: 0.06s
Test loss: 0.6670 score: 0.8125 time: 0.06s
Epoch 90/1000, LR 0.000266
Train loss: 1.1437;  Loss pred: 1.1437; Loss self: 0.0000; time: 0.20s
Val loss: 0.6707 score: 0.7143 time: 0.06s
Test loss: 0.6660 score: 0.8125 time: 0.06s
Epoch 91/1000, LR 0.000266
Train loss: 1.1357;  Loss pred: 1.1357; Loss self: 0.0000; time: 0.20s
Val loss: 0.6699 score: 0.7551 time: 0.06s
Test loss: 0.6650 score: 0.8125 time: 0.06s
Epoch 92/1000, LR 0.000266
Train loss: 1.1412;  Loss pred: 1.1412; Loss self: 0.0000; time: 0.20s
Val loss: 0.6691 score: 0.7755 time: 0.07s
Test loss: 0.6640 score: 0.8125 time: 0.07s
Epoch 93/1000, LR 0.000265
Train loss: 1.1345;  Loss pred: 1.1345; Loss self: 0.0000; time: 0.21s
Val loss: 0.6683 score: 0.7755 time: 0.07s
Test loss: 0.6629 score: 0.8125 time: 0.07s
Epoch 94/1000, LR 0.000265
Train loss: 1.1253;  Loss pred: 1.1253; Loss self: 0.0000; time: 0.22s
Val loss: 0.6674 score: 0.7755 time: 0.07s
Test loss: 0.6618 score: 0.8542 time: 0.07s
Epoch 95/1000, LR 0.000265
Train loss: 1.1211;  Loss pred: 1.1211; Loss self: 0.0000; time: 0.22s
Val loss: 0.6665 score: 0.8571 time: 0.07s
Test loss: 0.6607 score: 0.8542 time: 0.07s
Epoch 96/1000, LR 0.000265
Train loss: 1.1145;  Loss pred: 1.1145; Loss self: 0.0000; time: 0.21s
Val loss: 0.6655 score: 0.8571 time: 0.07s
Test loss: 0.6596 score: 0.8542 time: 0.07s
Epoch 97/1000, LR 0.000265
Train loss: 1.1160;  Loss pred: 1.1160; Loss self: 0.0000; time: 0.22s
Val loss: 0.6646 score: 0.8571 time: 0.07s
Test loss: 0.6584 score: 0.8750 time: 0.07s
Epoch 98/1000, LR 0.000265
Train loss: 1.1099;  Loss pred: 1.1099; Loss self: 0.0000; time: 0.22s
Val loss: 0.6636 score: 0.8571 time: 0.07s
Test loss: 0.6571 score: 0.8750 time: 0.07s
Epoch 99/1000, LR 0.000265
Train loss: 1.1094;  Loss pred: 1.1094; Loss self: 0.0000; time: 0.22s
Val loss: 0.6626 score: 0.8571 time: 0.07s
Test loss: 0.6559 score: 0.8958 time: 0.07s
Epoch 100/1000, LR 0.000265
Train loss: 1.1002;  Loss pred: 1.1002; Loss self: 0.0000; time: 0.36s
Val loss: 0.6615 score: 0.8571 time: 0.33s
Test loss: 0.6546 score: 0.8958 time: 0.52s
Epoch 101/1000, LR 0.000265
Train loss: 1.0940;  Loss pred: 1.0940; Loss self: 0.0000; time: 2.98s
Val loss: 0.6604 score: 0.8571 time: 0.75s
Test loss: 0.6532 score: 0.8958 time: 0.07s
Epoch 102/1000, LR 0.000264
Train loss: 1.0935;  Loss pred: 1.0935; Loss self: 0.0000; time: 0.21s
Val loss: 0.6593 score: 0.8776 time: 0.06s
Test loss: 0.6519 score: 0.8958 time: 0.07s
Epoch 103/1000, LR 0.000264
Train loss: 1.0852;  Loss pred: 1.0852; Loss self: 0.0000; time: 0.21s
Val loss: 0.6582 score: 0.8776 time: 0.06s
Test loss: 0.6504 score: 0.8958 time: 0.06s
Epoch 104/1000, LR 0.000264
Train loss: 1.0836;  Loss pred: 1.0836; Loss self: 0.0000; time: 0.21s
Val loss: 0.6571 score: 0.8980 time: 0.06s
Test loss: 0.6490 score: 0.8958 time: 0.06s
Epoch 105/1000, LR 0.000264
Train loss: 1.0800;  Loss pred: 1.0800; Loss self: 0.0000; time: 0.21s
Val loss: 0.6559 score: 0.8980 time: 0.06s
Test loss: 0.6475 score: 0.8958 time: 0.06s
Epoch 106/1000, LR 0.000264
Train loss: 1.0755;  Loss pred: 1.0755; Loss self: 0.0000; time: 0.21s
Val loss: 0.6547 score: 0.8980 time: 0.06s
Test loss: 0.6460 score: 0.8958 time: 0.07s
Epoch 107/1000, LR 0.000264
Train loss: 1.0698;  Loss pred: 1.0698; Loss self: 0.0000; time: 0.21s
Val loss: 0.6535 score: 0.8980 time: 0.06s
Test loss: 0.6445 score: 0.8958 time: 0.07s
Epoch 108/1000, LR 0.000264
Train loss: 1.0566;  Loss pred: 1.0566; Loss self: 0.0000; time: 0.21s
Val loss: 0.6522 score: 0.8980 time: 0.06s
Test loss: 0.6429 score: 0.8958 time: 0.07s
Epoch 109/1000, LR 0.000264
Train loss: 1.0622;  Loss pred: 1.0622; Loss self: 0.0000; time: 0.21s
Val loss: 0.6509 score: 0.8980 time: 0.06s
Test loss: 0.6412 score: 0.8958 time: 0.07s
Epoch 110/1000, LR 0.000263
Train loss: 1.0560;  Loss pred: 1.0560; Loss self: 0.0000; time: 0.21s
Val loss: 0.6496 score: 0.8980 time: 0.06s
Test loss: 0.6396 score: 0.8958 time: 0.07s
Epoch 111/1000, LR 0.000263
Train loss: 1.0521;  Loss pred: 1.0521; Loss self: 0.0000; time: 0.21s
Val loss: 0.6482 score: 0.8980 time: 0.06s
Test loss: 0.6378 score: 0.8958 time: 0.06s
Epoch 112/1000, LR 0.000263
Train loss: 1.0443;  Loss pred: 1.0443; Loss self: 0.0000; time: 0.21s
Val loss: 0.6468 score: 0.8980 time: 0.06s
Test loss: 0.6361 score: 0.9167 time: 0.07s
Epoch 113/1000, LR 0.000263
Train loss: 1.0422;  Loss pred: 1.0422; Loss self: 0.0000; time: 0.21s
Val loss: 0.6454 score: 0.8980 time: 0.06s
Test loss: 0.6343 score: 0.9167 time: 0.07s
Epoch 114/1000, LR 0.000263
Train loss: 1.0398;  Loss pred: 1.0398; Loss self: 0.0000; time: 0.21s
Val loss: 0.6439 score: 0.8980 time: 0.07s
Test loss: 0.6324 score: 0.9167 time: 0.07s
Epoch 115/1000, LR 0.000263
Train loss: 1.0341;  Loss pred: 1.0341; Loss self: 0.0000; time: 0.21s
Val loss: 0.6424 score: 0.8980 time: 0.07s
Test loss: 0.6305 score: 0.9167 time: 0.07s
Epoch 116/1000, LR 0.000263
Train loss: 1.0321;  Loss pred: 1.0321; Loss self: 0.0000; time: 0.21s
Val loss: 0.6409 score: 0.8980 time: 0.07s
Test loss: 0.6286 score: 0.9167 time: 0.07s
Epoch 117/1000, LR 0.000262
Train loss: 1.0323;  Loss pred: 1.0323; Loss self: 0.0000; time: 0.22s
Val loss: 0.6394 score: 0.8980 time: 0.08s
Test loss: 0.6267 score: 0.9167 time: 1.13s
Epoch 118/1000, LR 0.000262
Train loss: 1.0278;  Loss pred: 1.0278; Loss self: 0.0000; time: 5.22s
Val loss: 0.6378 score: 0.8980 time: 0.10s
Test loss: 0.6247 score: 0.9167 time: 0.08s
Epoch 119/1000, LR 0.000262
Train loss: 1.0210;  Loss pred: 1.0210; Loss self: 0.0000; time: 0.24s
Val loss: 0.6362 score: 0.8980 time: 0.07s
Test loss: 0.6226 score: 0.9167 time: 0.06s
Epoch 120/1000, LR 0.000262
Train loss: 1.0178;  Loss pred: 1.0178; Loss self: 0.0000; time: 0.20s
Val loss: 0.6346 score: 0.8776 time: 0.06s
Test loss: 0.6206 score: 0.9375 time: 0.06s
Epoch 121/1000, LR 0.000262
Train loss: 1.0173;  Loss pred: 1.0173; Loss self: 0.0000; time: 0.20s
Val loss: 0.6329 score: 0.8776 time: 0.06s
Test loss: 0.6184 score: 0.9583 time: 0.06s
Epoch 122/1000, LR 0.000262
Train loss: 1.0102;  Loss pred: 1.0102; Loss self: 0.0000; time: 0.20s
Val loss: 0.6312 score: 0.8776 time: 0.06s
Test loss: 0.6163 score: 0.9583 time: 0.06s
Epoch 123/1000, LR 0.000262
Train loss: 1.0076;  Loss pred: 1.0076; Loss self: 0.0000; time: 0.20s
Val loss: 0.6294 score: 0.8776 time: 0.06s
Test loss: 0.6140 score: 0.9583 time: 0.06s
Epoch 124/1000, LR 0.000261
Train loss: 1.0044;  Loss pred: 1.0044; Loss self: 0.0000; time: 0.20s
Val loss: 0.6276 score: 0.8776 time: 0.06s
Test loss: 0.6118 score: 0.9583 time: 0.06s
Epoch 125/1000, LR 0.000261
Train loss: 0.9998;  Loss pred: 0.9998; Loss self: 0.0000; time: 0.21s
Val loss: 0.6258 score: 0.8980 time: 0.06s
Test loss: 0.6095 score: 0.9583 time: 0.06s
Epoch 126/1000, LR 0.000261
Train loss: 0.9932;  Loss pred: 0.9932; Loss self: 0.0000; time: 0.20s
Val loss: 0.6239 score: 0.8980 time: 0.06s
Test loss: 0.6071 score: 0.9583 time: 0.06s
Epoch 127/1000, LR 0.000261
Train loss: 0.9945;  Loss pred: 0.9945; Loss self: 0.0000; time: 0.20s
Val loss: 0.6220 score: 0.8980 time: 0.06s
Test loss: 0.6047 score: 0.9583 time: 0.06s
Epoch 128/1000, LR 0.000261
Train loss: 0.9894;  Loss pred: 0.9894; Loss self: 0.0000; time: 0.21s
Val loss: 0.6201 score: 0.8980 time: 0.07s
Test loss: 0.6023 score: 0.9583 time: 0.07s
Epoch 129/1000, LR 0.000261
Train loss: 0.9862;  Loss pred: 0.9862; Loss self: 0.0000; time: 0.22s
Val loss: 0.6181 score: 0.8980 time: 0.06s
Test loss: 0.5998 score: 0.9583 time: 0.06s
Epoch 130/1000, LR 0.000260
Train loss: 0.9819;  Loss pred: 0.9819; Loss self: 0.0000; time: 0.21s
Val loss: 0.6161 score: 0.8980 time: 0.06s
Test loss: 0.5973 score: 0.9583 time: 0.07s
Epoch 131/1000, LR 0.000260
Train loss: 0.9791;  Loss pred: 0.9791; Loss self: 0.0000; time: 0.20s
Val loss: 0.6141 score: 0.8980 time: 0.07s
Test loss: 0.5947 score: 0.9583 time: 0.07s
Epoch 132/1000, LR 0.000260
Train loss: 0.9735;  Loss pred: 0.9735; Loss self: 0.0000; time: 1.20s
Val loss: 0.6120 score: 0.8980 time: 0.58s
Test loss: 0.5921 score: 0.9583 time: 0.46s
Epoch 133/1000, LR 0.000260
Train loss: 0.9714;  Loss pred: 0.9714; Loss self: 0.0000; time: 0.93s
Val loss: 0.6099 score: 0.8980 time: 1.88s
Test loss: 0.5894 score: 0.9583 time: 1.40s
Epoch 134/1000, LR 0.000260
Train loss: 0.9678;  Loss pred: 0.9678; Loss self: 0.0000; time: 2.99s
Val loss: 0.6077 score: 0.8980 time: 0.08s
Test loss: 0.5868 score: 0.9583 time: 0.07s
Epoch 135/1000, LR 0.000260
Train loss: 0.9650;  Loss pred: 0.9650; Loss self: 0.0000; time: 0.21s
Val loss: 0.6056 score: 0.8980 time: 0.06s
Test loss: 0.5840 score: 0.9583 time: 0.07s
Epoch 136/1000, LR 0.000260
Train loss: 0.9650;  Loss pred: 0.9650; Loss self: 0.0000; time: 0.21s
Val loss: 0.6034 score: 0.8980 time: 0.06s
Test loss: 0.5813 score: 0.9583 time: 0.06s
Epoch 137/1000, LR 0.000259
Train loss: 0.9571;  Loss pred: 0.9571; Loss self: 0.0000; time: 0.21s
Val loss: 0.6012 score: 0.8980 time: 0.06s
Test loss: 0.5785 score: 0.9583 time: 0.07s
Epoch 138/1000, LR 0.000259
Train loss: 0.9545;  Loss pred: 0.9545; Loss self: 0.0000; time: 0.20s
Val loss: 0.5989 score: 0.8980 time: 0.06s
Test loss: 0.5756 score: 0.9583 time: 0.07s
Epoch 139/1000, LR 0.000259
Train loss: 0.9515;  Loss pred: 0.9515; Loss self: 0.0000; time: 0.20s
Val loss: 0.5967 score: 0.8980 time: 0.06s
Test loss: 0.5728 score: 0.9583 time: 0.07s
Epoch 140/1000, LR 0.000259
Train loss: 0.9507;  Loss pred: 0.9507; Loss self: 0.0000; time: 0.21s
Val loss: 0.5944 score: 0.8980 time: 0.06s
Test loss: 0.5699 score: 0.9583 time: 0.07s
Epoch 141/1000, LR 0.000259
Train loss: 0.9451;  Loss pred: 0.9451; Loss self: 0.0000; time: 0.21s
Val loss: 0.5920 score: 0.8980 time: 0.06s
Test loss: 0.5669 score: 0.9583 time: 0.07s
Epoch 142/1000, LR 0.000259
Train loss: 0.9422;  Loss pred: 0.9422; Loss self: 0.0000; time: 0.23s
Val loss: 0.5896 score: 0.8980 time: 0.07s
Test loss: 0.5639 score: 0.9583 time: 0.07s
Epoch 143/1000, LR 0.000258
Train loss: 0.9375;  Loss pred: 0.9375; Loss self: 0.0000; time: 0.21s
Val loss: 0.5872 score: 0.8980 time: 0.06s
Test loss: 0.5609 score: 0.9583 time: 0.07s
Epoch 144/1000, LR 0.000258
Train loss: 0.9340;  Loss pred: 0.9340; Loss self: 0.0000; time: 0.21s
Val loss: 0.5848 score: 0.8980 time: 0.06s
Test loss: 0.5578 score: 0.9583 time: 0.07s
Epoch 145/1000, LR 0.000258
Train loss: 0.9303;  Loss pred: 0.9303; Loss self: 0.0000; time: 0.21s
Val loss: 0.5823 score: 0.8980 time: 0.06s
Test loss: 0.5547 score: 0.9583 time: 0.07s
Epoch 146/1000, LR 0.000258
Train loss: 0.9270;  Loss pred: 0.9270; Loss self: 0.0000; time: 0.20s
Val loss: 0.5798 score: 0.8980 time: 0.06s
Test loss: 0.5515 score: 0.9583 time: 0.06s
Epoch 147/1000, LR 0.000258
Train loss: 0.9217;  Loss pred: 0.9217; Loss self: 0.0000; time: 0.24s
Val loss: 0.5772 score: 0.8980 time: 0.06s
Test loss: 0.5483 score: 0.9583 time: 0.07s
Epoch 148/1000, LR 0.000257
Train loss: 0.9202;  Loss pred: 0.9202; Loss self: 0.0000; time: 0.22s
Val loss: 0.5747 score: 0.8980 time: 0.07s
Test loss: 0.5451 score: 0.9583 time: 0.07s
Epoch 149/1000, LR 0.000257
Train loss: 0.9186;  Loss pred: 0.9186; Loss self: 0.0000; time: 0.21s
Val loss: 0.5720 score: 0.8980 time: 0.07s
Test loss: 0.5418 score: 0.9583 time: 0.06s
Epoch 150/1000, LR 0.000257
Train loss: 0.9110;  Loss pred: 0.9110; Loss self: 0.0000; time: 0.21s
Val loss: 0.5694 score: 0.8980 time: 2.16s
Test loss: 0.5385 score: 0.9583 time: 1.59s
Epoch 151/1000, LR 0.000257
Train loss: 0.9106;  Loss pred: 0.9106; Loss self: 0.0000; time: 4.97s
Val loss: 0.5668 score: 0.8980 time: 0.13s
Test loss: 0.5351 score: 0.9583 time: 0.35s
Epoch 152/1000, LR 0.000257
Train loss: 0.9077;  Loss pred: 0.9077; Loss self: 0.0000; time: 0.76s
Val loss: 0.5641 score: 0.8980 time: 0.07s
Test loss: 0.5318 score: 0.9583 time: 0.07s
Epoch 153/1000, LR 0.000257
Train loss: 0.9031;  Loss pred: 0.9031; Loss self: 0.0000; time: 0.21s
Val loss: 0.5615 score: 0.8980 time: 0.07s
Test loss: 0.5284 score: 0.9583 time: 0.07s
Epoch 154/1000, LR 0.000256
Train loss: 0.8971;  Loss pred: 0.8971; Loss self: 0.0000; time: 0.21s
Val loss: 0.5588 score: 0.8980 time: 0.06s
Test loss: 0.5250 score: 0.9583 time: 0.06s
Epoch 155/1000, LR 0.000256
Train loss: 0.8950;  Loss pred: 0.8950; Loss self: 0.0000; time: 0.24s
Val loss: 0.5560 score: 0.8980 time: 0.08s
Test loss: 0.5216 score: 0.9583 time: 0.07s
Epoch 156/1000, LR 0.000256
Train loss: 0.8900;  Loss pred: 0.8900; Loss self: 0.0000; time: 0.20s
Val loss: 0.5533 score: 0.8980 time: 0.06s
Test loss: 0.5181 score: 0.9583 time: 0.06s
Epoch 157/1000, LR 0.000256
Train loss: 0.8875;  Loss pred: 0.8875; Loss self: 0.0000; time: 0.20s
Val loss: 0.5505 score: 0.8980 time: 0.06s
Test loss: 0.5146 score: 0.9583 time: 0.07s
Epoch 158/1000, LR 0.000256
Train loss: 0.8834;  Loss pred: 0.8834; Loss self: 0.0000; time: 0.20s
Val loss: 0.5477 score: 0.8980 time: 0.07s
Test loss: 0.5111 score: 0.9583 time: 0.07s
Epoch 159/1000, LR 0.000255
Train loss: 0.8818;  Loss pred: 0.8818; Loss self: 0.0000; time: 0.22s
Val loss: 0.5449 score: 0.8980 time: 0.07s
Test loss: 0.5075 score: 0.9583 time: 0.07s
Epoch 160/1000, LR 0.000255
Train loss: 0.8793;  Loss pred: 0.8793; Loss self: 0.0000; time: 0.22s
Val loss: 0.5421 score: 0.8980 time: 0.07s
Test loss: 0.5039 score: 0.9583 time: 0.07s
Epoch 161/1000, LR 0.000255
Train loss: 0.8756;  Loss pred: 0.8756; Loss self: 0.0000; time: 0.22s
Val loss: 0.5393 score: 0.8980 time: 0.07s
Test loss: 0.5003 score: 0.9583 time: 0.07s
Epoch 162/1000, LR 0.000255
Train loss: 0.8718;  Loss pred: 0.8718; Loss self: 0.0000; time: 0.22s
Val loss: 0.5364 score: 0.8980 time: 0.07s
Test loss: 0.4967 score: 0.9583 time: 0.07s
Epoch 163/1000, LR 0.000255
Train loss: 0.8682;  Loss pred: 0.8682; Loss self: 0.0000; time: 0.22s
Val loss: 0.5336 score: 0.8980 time: 0.07s
Test loss: 0.4931 score: 0.9583 time: 0.07s
Epoch 164/1000, LR 0.000254
Train loss: 0.8648;  Loss pred: 0.8648; Loss self: 0.0000; time: 0.22s
Val loss: 0.5307 score: 0.8980 time: 0.07s
Test loss: 0.4895 score: 0.9583 time: 0.07s
Epoch 165/1000, LR 0.000254
Train loss: 0.8610;  Loss pred: 0.8610; Loss self: 0.0000; time: 5.95s
Val loss: 0.5278 score: 0.8980 time: 0.36s
Test loss: 0.4858 score: 0.9583 time: 0.30s
Epoch 166/1000, LR 0.000254
Train loss: 0.8588;  Loss pred: 0.8588; Loss self: 0.0000; time: 2.78s
Val loss: 0.5249 score: 0.8980 time: 0.08s
Test loss: 0.4821 score: 0.9583 time: 0.07s
Epoch 167/1000, LR 0.000254
Train loss: 0.8543;  Loss pred: 0.8543; Loss self: 0.0000; time: 0.20s
Val loss: 0.5220 score: 0.8980 time: 0.06s
Test loss: 0.4784 score: 0.9583 time: 0.07s
Epoch 168/1000, LR 0.000254
Train loss: 0.8492;  Loss pred: 0.8492; Loss self: 0.0000; time: 0.21s
Val loss: 0.5190 score: 0.8980 time: 0.09s
Test loss: 0.4747 score: 0.9583 time: 0.07s
Epoch 169/1000, LR 0.000253
Train loss: 0.8473;  Loss pred: 0.8473; Loss self: 0.0000; time: 0.20s
Val loss: 0.5161 score: 0.8980 time: 0.06s
Test loss: 0.4710 score: 0.9583 time: 0.06s
Epoch 170/1000, LR 0.000253
Train loss: 0.8454;  Loss pred: 0.8454; Loss self: 0.0000; time: 0.21s
Val loss: 0.5132 score: 0.8980 time: 0.06s
Test loss: 0.4673 score: 0.9583 time: 0.07s
Epoch 171/1000, LR 0.000253
Train loss: 0.8381;  Loss pred: 0.8381; Loss self: 0.0000; time: 0.22s
Val loss: 0.5104 score: 0.8980 time: 0.08s
Test loss: 0.4637 score: 0.9583 time: 0.08s
Epoch 172/1000, LR 0.000253
Train loss: 0.8364;  Loss pred: 0.8364; Loss self: 0.0000; time: 1.37s
Val loss: 0.5076 score: 0.8980 time: 0.54s
Test loss: 0.4601 score: 0.9583 time: 0.38s
Epoch 173/1000, LR 0.000253
Train loss: 0.8332;  Loss pred: 0.8332; Loss self: 0.0000; time: 6.17s
Val loss: 0.5047 score: 0.8980 time: 0.07s
Test loss: 0.4564 score: 0.9583 time: 0.07s
Epoch 174/1000, LR 0.000252
Train loss: 0.8301;  Loss pred: 0.8301; Loss self: 0.0000; time: 0.19s
Val loss: 0.5019 score: 0.8980 time: 0.06s
Test loss: 0.4528 score: 0.9583 time: 0.06s
Epoch 175/1000, LR 0.000252
Train loss: 0.8277;  Loss pred: 0.8277; Loss self: 0.0000; time: 0.21s
Val loss: 0.4991 score: 0.8980 time: 0.07s
Test loss: 0.4492 score: 0.9583 time: 0.07s
Epoch 176/1000, LR 0.000252
Train loss: 0.8212;  Loss pred: 0.8212; Loss self: 0.0000; time: 0.21s
Val loss: 0.4963 score: 0.8980 time: 0.06s
Test loss: 0.4456 score: 0.9583 time: 0.07s
Epoch 177/1000, LR 0.000252
Train loss: 0.8207;  Loss pred: 0.8207; Loss self: 0.0000; time: 0.21s
Val loss: 0.4935 score: 0.8980 time: 0.06s
Test loss: 0.4420 score: 0.9583 time: 0.07s
Epoch 178/1000, LR 0.000251
Train loss: 0.8162;  Loss pred: 0.8162; Loss self: 0.0000; time: 0.21s
Val loss: 0.4907 score: 0.8980 time: 0.06s
Test loss: 0.4384 score: 0.9583 time: 0.99s
Epoch 179/1000, LR 0.000251
Train loss: 0.8120;  Loss pred: 0.8120; Loss self: 0.0000; time: 5.38s
Val loss: 0.4879 score: 0.8980 time: 0.28s
Test loss: 0.4348 score: 0.9583 time: 0.40s
Epoch 180/1000, LR 0.000251
Train loss: 0.8084;  Loss pred: 0.8084; Loss self: 0.0000; time: 0.55s
Val loss: 0.4851 score: 0.8980 time: 0.07s
Test loss: 0.4312 score: 0.9583 time: 0.07s
Epoch 181/1000, LR 0.000251
Train loss: 0.8068;  Loss pred: 0.8068; Loss self: 0.0000; time: 0.21s
Val loss: 0.4823 score: 0.8980 time: 0.06s
Test loss: 0.4276 score: 0.9583 time: 0.07s
Epoch 182/1000, LR 0.000251
Train loss: 0.8040;  Loss pred: 0.8040; Loss self: 0.0000; time: 0.21s
Val loss: 0.4795 score: 0.8980 time: 0.06s
Test loss: 0.4239 score: 0.9583 time: 0.06s
Epoch 183/1000, LR 0.000250
Train loss: 0.7995;  Loss pred: 0.7995; Loss self: 0.0000; time: 0.21s
Val loss: 0.4767 score: 0.8980 time: 0.06s
Test loss: 0.4203 score: 0.9583 time: 0.07s
Epoch 184/1000, LR 0.000250
Train loss: 0.7957;  Loss pred: 0.7957; Loss self: 0.0000; time: 0.21s
Val loss: 0.4739 score: 0.8980 time: 0.06s
Test loss: 0.4168 score: 0.9583 time: 0.07s
Epoch 185/1000, LR 0.000250
Train loss: 0.7954;  Loss pred: 0.7954; Loss self: 0.0000; time: 0.21s
Val loss: 0.4711 score: 0.8980 time: 0.41s
Test loss: 0.4132 score: 0.9583 time: 1.40s
Epoch 186/1000, LR 0.000250
Train loss: 0.7896;  Loss pred: 0.7896; Loss self: 0.0000; time: 5.54s
Val loss: 0.4684 score: 0.8980 time: 0.95s
Test loss: 0.4097 score: 0.9583 time: 0.07s
Epoch 187/1000, LR 0.000249
Train loss: 0.7837;  Loss pred: 0.7837; Loss self: 0.0000; time: 0.21s
Val loss: 0.4657 score: 0.8980 time: 0.07s
Test loss: 0.4063 score: 0.9583 time: 0.07s
Epoch 188/1000, LR 0.000249
Train loss: 0.7832;  Loss pred: 0.7832; Loss self: 0.0000; time: 0.21s
Val loss: 0.4632 score: 0.8980 time: 0.07s
Test loss: 0.4030 score: 0.9583 time: 0.07s
Epoch 189/1000, LR 0.000249
Train loss: 0.7790;  Loss pred: 0.7790; Loss self: 0.0000; time: 0.21s
Val loss: 0.4606 score: 0.8980 time: 0.06s
Test loss: 0.3996 score: 0.9583 time: 0.07s
Epoch 190/1000, LR 0.000249
Train loss: 0.7797;  Loss pred: 0.7797; Loss self: 0.0000; time: 0.22s
Val loss: 0.4580 score: 0.8980 time: 0.06s
Test loss: 0.3963 score: 0.9583 time: 0.07s
Epoch 191/1000, LR 0.000249
Train loss: 0.7761;  Loss pred: 0.7761; Loss self: 0.0000; time: 0.21s
Val loss: 0.4556 score: 0.8980 time: 0.06s
Test loss: 0.3930 score: 0.9583 time: 0.07s
Epoch 192/1000, LR 0.000248
Train loss: 0.7710;  Loss pred: 0.7710; Loss self: 0.0000; time: 0.20s
Val loss: 0.4531 score: 0.8980 time: 0.06s
Test loss: 0.3898 score: 0.9583 time: 0.07s
Epoch 193/1000, LR 0.000248
Train loss: 0.7688;  Loss pred: 0.7688; Loss self: 0.0000; time: 0.21s
Val loss: 0.4506 score: 0.8980 time: 0.06s
Test loss: 0.3866 score: 0.9583 time: 0.06s
Epoch 194/1000, LR 0.000248
Train loss: 0.7654;  Loss pred: 0.7654; Loss self: 0.0000; time: 0.20s
Val loss: 0.4482 score: 0.8980 time: 0.06s
Test loss: 0.3834 score: 0.9583 time: 0.07s
Epoch 195/1000, LR 0.000248
Train loss: 0.7637;  Loss pred: 0.7637; Loss self: 0.0000; time: 0.20s
Val loss: 0.4456 score: 0.8980 time: 0.06s
Test loss: 0.3801 score: 0.9583 time: 0.06s
Epoch 196/1000, LR 0.000247
Train loss: 0.7605;  Loss pred: 0.7605; Loss self: 0.0000; time: 0.20s
Val loss: 0.4431 score: 0.8980 time: 0.06s
Test loss: 0.3767 score: 0.9583 time: 0.06s
Epoch 197/1000, LR 0.000247
Train loss: 0.7599;  Loss pred: 0.7599; Loss self: 0.0000; time: 0.20s
Val loss: 0.4405 score: 0.8980 time: 0.06s
Test loss: 0.3733 score: 0.9583 time: 0.06s
Epoch 198/1000, LR 0.000247
Train loss: 0.7549;  Loss pred: 0.7549; Loss self: 0.0000; time: 0.20s
Val loss: 0.4378 score: 0.8980 time: 0.06s
Test loss: 0.3700 score: 0.9583 time: 0.06s
Epoch 199/1000, LR 0.000247
Train loss: 0.7531;  Loss pred: 0.7531; Loss self: 0.0000; time: 0.20s
Val loss: 0.4353 score: 0.8980 time: 0.06s
Test loss: 0.3666 score: 0.9583 time: 0.06s
Epoch 200/1000, LR 0.000246
Train loss: 0.7493;  Loss pred: 0.7493; Loss self: 0.0000; time: 0.20s
Val loss: 0.4328 score: 0.8980 time: 0.06s
Test loss: 0.3634 score: 0.9583 time: 0.06s
Epoch 201/1000, LR 0.000246
Train loss: 0.7463;  Loss pred: 0.7463; Loss self: 0.0000; time: 0.20s
Val loss: 0.4305 score: 0.8980 time: 0.06s
Test loss: 0.3603 score: 0.9583 time: 0.07s
Epoch 202/1000, LR 0.000246
Train loss: 0.7432;  Loss pred: 0.7432; Loss self: 0.0000; time: 0.20s
Val loss: 0.4283 score: 0.8980 time: 0.06s
Test loss: 0.3574 score: 0.9583 time: 0.06s
Epoch 203/1000, LR 0.000246
Train loss: 0.7402;  Loss pred: 0.7402; Loss self: 0.0000; time: 0.20s
Val loss: 0.4261 score: 0.8980 time: 0.07s
Test loss: 0.3545 score: 0.9583 time: 0.07s
Epoch 204/1000, LR 0.000245
Train loss: 0.7389;  Loss pred: 0.7389; Loss self: 0.0000; time: 0.22s
Val loss: 0.4241 score: 0.8980 time: 0.07s
Test loss: 0.3517 score: 0.9583 time: 0.07s
Epoch 205/1000, LR 0.000245
Train loss: 0.7355;  Loss pred: 0.7355; Loss self: 0.0000; time: 0.21s
Val loss: 0.4221 score: 0.8980 time: 0.07s
Test loss: 0.3490 score: 0.9583 time: 0.15s
Epoch 206/1000, LR 0.000245
Train loss: 0.7315;  Loss pred: 0.7315; Loss self: 0.0000; time: 0.28s
Val loss: 0.4201 score: 0.8980 time: 0.07s
Test loss: 0.3462 score: 0.9583 time: 0.07s
Epoch 207/1000, LR 0.000245
Train loss: 0.7331;  Loss pred: 0.7331; Loss self: 0.0000; time: 0.22s
Val loss: 0.4180 score: 0.8980 time: 0.07s
Test loss: 0.3434 score: 0.9375 time: 0.07s
Epoch 208/1000, LR 0.000244
Train loss: 0.7289;  Loss pred: 0.7289; Loss self: 0.0000; time: 0.22s
Val loss: 0.4159 score: 0.8980 time: 0.07s
Test loss: 0.3406 score: 0.9375 time: 0.07s
Epoch 209/1000, LR 0.000244
Train loss: 0.7233;  Loss pred: 0.7233; Loss self: 0.0000; time: 0.22s
Val loss: 0.4138 score: 0.8980 time: 0.07s
Test loss: 0.3377 score: 0.9375 time: 0.07s
Epoch 210/1000, LR 0.000244
Train loss: 0.7224;  Loss pred: 0.7224; Loss self: 0.0000; time: 0.30s
Val loss: 0.4117 score: 0.8980 time: 0.07s
Test loss: 0.3348 score: 0.9375 time: 0.09s
Epoch 211/1000, LR 0.000244
Train loss: 0.7184;  Loss pred: 0.7184; Loss self: 0.0000; time: 0.21s
Val loss: 0.4095 score: 0.8980 time: 0.06s
Test loss: 0.3319 score: 0.9375 time: 0.07s
Epoch 212/1000, LR 0.000243
Train loss: 0.7173;  Loss pred: 0.7173; Loss self: 0.0000; time: 0.21s
Val loss: 0.4072 score: 0.8980 time: 0.06s
Test loss: 0.3289 score: 0.9375 time: 0.07s
Epoch 213/1000, LR 0.000243
Train loss: 0.7150;  Loss pred: 0.7150; Loss self: 0.0000; time: 0.21s
Val loss: 0.4051 score: 0.8980 time: 0.07s
Test loss: 0.3260 score: 0.9375 time: 0.07s
Epoch 214/1000, LR 0.000243
Train loss: 0.7132;  Loss pred: 0.7132; Loss self: 0.0000; time: 0.22s
Val loss: 0.4030 score: 0.8980 time: 0.07s
Test loss: 0.3232 score: 0.9375 time: 0.07s
Epoch 215/1000, LR 0.000243
Train loss: 0.7098;  Loss pred: 0.7098; Loss self: 0.0000; time: 0.22s
Val loss: 0.4009 score: 0.8980 time: 0.08s
Test loss: 0.3204 score: 0.9375 time: 0.07s
Epoch 216/1000, LR 0.000242
Train loss: 0.7094;  Loss pred: 0.7094; Loss self: 0.0000; time: 0.21s
Val loss: 0.3990 score: 0.8980 time: 0.06s
Test loss: 0.3178 score: 0.9375 time: 0.06s
Epoch 217/1000, LR 0.000242
Train loss: 0.7059;  Loss pred: 0.7059; Loss self: 0.0000; time: 0.23s
Val loss: 0.3972 score: 0.8980 time: 0.07s
Test loss: 0.3153 score: 0.9375 time: 0.07s
Epoch 218/1000, LR 0.000242
Train loss: 0.7047;  Loss pred: 0.7047; Loss self: 0.0000; time: 0.21s
Val loss: 0.3955 score: 0.8980 time: 0.06s
Test loss: 0.3129 score: 0.9375 time: 0.07s
Epoch 219/1000, LR 0.000242
Train loss: 0.7026;  Loss pred: 0.7026; Loss self: 0.0000; time: 0.21s
Val loss: 0.3940 score: 0.8980 time: 0.06s
Test loss: 0.3107 score: 0.9375 time: 0.07s
Epoch 220/1000, LR 0.000241
Train loss: 0.6979;  Loss pred: 0.6979; Loss self: 0.0000; time: 0.21s
Val loss: 0.3925 score: 0.8980 time: 0.06s
Test loss: 0.3085 score: 0.9375 time: 0.07s
Epoch 221/1000, LR 0.000241
Train loss: 0.6979;  Loss pred: 0.6979; Loss self: 0.0000; time: 0.20s
Val loss: 0.3910 score: 0.8980 time: 0.06s
Test loss: 0.3064 score: 0.9375 time: 0.07s
Epoch 222/1000, LR 0.000241
Train loss: 0.6954;  Loss pred: 0.6954; Loss self: 0.0000; time: 0.22s
Val loss: 0.3895 score: 0.8980 time: 0.15s
Test loss: 0.3042 score: 0.9375 time: 0.16s
Epoch 223/1000, LR 0.000241
Train loss: 0.6918;  Loss pred: 0.6918; Loss self: 0.0000; time: 0.21s
Val loss: 0.3880 score: 0.8980 time: 0.09s
Test loss: 0.3021 score: 0.9375 time: 0.10s
Epoch 224/1000, LR 0.000240
Train loss: 0.6900;  Loss pred: 0.6900; Loss self: 0.0000; time: 0.52s
Val loss: 0.3865 score: 0.8980 time: 1.39s
Test loss: 0.2999 score: 0.9375 time: 0.07s
Epoch 225/1000, LR 0.000240
Train loss: 0.6876;  Loss pred: 0.6876; Loss self: 0.0000; time: 0.21s
Val loss: 0.3849 score: 0.8980 time: 0.06s
Test loss: 0.2977 score: 0.9375 time: 0.06s
Epoch 226/1000, LR 0.000240
Train loss: 0.6872;  Loss pred: 0.6872; Loss self: 0.0000; time: 0.20s
Val loss: 0.3833 score: 0.8980 time: 0.06s
Test loss: 0.2954 score: 0.9375 time: 0.06s
Epoch 227/1000, LR 0.000240
Train loss: 0.6840;  Loss pred: 0.6840; Loss self: 0.0000; time: 0.21s
Val loss: 0.3817 score: 0.8980 time: 0.06s
Test loss: 0.2932 score: 0.9375 time: 0.06s
Epoch 228/1000, LR 0.000239
Train loss: 0.6812;  Loss pred: 0.6812; Loss self: 0.0000; time: 0.21s
Val loss: 0.3801 score: 0.8980 time: 0.16s
Test loss: 0.2910 score: 0.9375 time: 0.08s
Epoch 229/1000, LR 0.000239
Train loss: 0.6790;  Loss pred: 0.6790; Loss self: 0.0000; time: 0.27s
Val loss: 0.3784 score: 0.8980 time: 0.07s
Test loss: 0.2887 score: 0.9375 time: 0.07s
Epoch 230/1000, LR 0.000239
Train loss: 0.6754;  Loss pred: 0.6754; Loss self: 0.0000; time: 0.22s
Val loss: 0.3768 score: 0.8980 time: 0.07s
Test loss: 0.2864 score: 0.9375 time: 0.07s
Epoch 231/1000, LR 0.000238
Train loss: 0.6748;  Loss pred: 0.6748; Loss self: 0.0000; time: 0.22s
Val loss: 0.3752 score: 0.8980 time: 0.07s
Test loss: 0.2842 score: 0.9375 time: 0.07s
Epoch 232/1000, LR 0.000238
Train loss: 0.6732;  Loss pred: 0.6732; Loss self: 0.0000; time: 0.21s
Val loss: 0.3738 score: 0.8980 time: 0.07s
Test loss: 0.2822 score: 0.9375 time: 0.07s
Epoch 233/1000, LR 0.000238
Train loss: 0.6706;  Loss pred: 0.6706; Loss self: 0.0000; time: 0.21s
Val loss: 0.3724 score: 0.8980 time: 0.07s
Test loss: 0.2802 score: 0.9375 time: 0.07s
Epoch 234/1000, LR 0.000238
Train loss: 0.6697;  Loss pred: 0.6697; Loss self: 0.0000; time: 0.23s
Val loss: 0.3711 score: 0.8980 time: 0.07s
Test loss: 0.2782 score: 0.9375 time: 0.08s
Epoch 235/1000, LR 0.000237
Train loss: 0.6694;  Loss pred: 0.6694; Loss self: 0.0000; time: 0.22s
Val loss: 0.3698 score: 0.8980 time: 0.07s
Test loss: 0.2764 score: 0.9375 time: 0.07s
Epoch 236/1000, LR 0.000237
Train loss: 0.6665;  Loss pred: 0.6665; Loss self: 0.0000; time: 0.21s
Val loss: 0.3686 score: 0.8980 time: 0.07s
Test loss: 0.2746 score: 0.9375 time: 0.07s
Epoch 237/1000, LR 0.000237
Train loss: 0.6634;  Loss pred: 0.6634; Loss self: 0.0000; time: 0.21s
Val loss: 0.3675 score: 0.8980 time: 0.07s
Test loss: 0.2729 score: 0.9375 time: 0.07s
Epoch 238/1000, LR 0.000236
Train loss: 0.6632;  Loss pred: 0.6632; Loss self: 0.0000; time: 0.21s
Val loss: 0.3664 score: 0.8980 time: 0.07s
Test loss: 0.2712 score: 0.9375 time: 0.07s
Epoch 239/1000, LR 0.000236
Train loss: 0.6593;  Loss pred: 0.6593; Loss self: 0.0000; time: 0.21s
Val loss: 0.3653 score: 0.8980 time: 0.07s
Test loss: 0.2696 score: 0.9375 time: 0.07s
Epoch 240/1000, LR 0.000236
Train loss: 0.6599;  Loss pred: 0.6599; Loss self: 0.0000; time: 0.21s
Val loss: 0.3642 score: 0.8980 time: 0.07s
Test loss: 0.2680 score: 0.9375 time: 0.07s
Epoch 241/1000, LR 0.000236
Train loss: 0.6572;  Loss pred: 0.6572; Loss self: 0.0000; time: 0.21s
Val loss: 0.3632 score: 0.8980 time: 0.07s
Test loss: 0.2664 score: 0.9375 time: 0.07s
Epoch 242/1000, LR 0.000235
Train loss: 0.6552;  Loss pred: 0.6552; Loss self: 0.0000; time: 0.21s
Val loss: 0.3621 score: 0.8980 time: 0.07s
Test loss: 0.2649 score: 0.9375 time: 0.07s
Epoch 243/1000, LR 0.000235
Train loss: 0.6544;  Loss pred: 0.6544; Loss self: 0.0000; time: 0.21s
Val loss: 0.3611 score: 0.8980 time: 0.07s
Test loss: 0.2632 score: 0.9375 time: 0.08s
Epoch 244/1000, LR 0.000235
Train loss: 0.6526;  Loss pred: 0.6526; Loss self: 0.0000; time: 0.23s
Val loss: 0.3599 score: 0.8980 time: 0.07s
Test loss: 0.2616 score: 0.9375 time: 0.07s
Epoch 245/1000, LR 0.000234
Train loss: 0.6501;  Loss pred: 0.6501; Loss self: 0.0000; time: 0.22s
Val loss: 0.3588 score: 0.8980 time: 0.07s
Test loss: 0.2599 score: 0.9375 time: 0.07s
Epoch 246/1000, LR 0.000234
Train loss: 0.6479;  Loss pred: 0.6479; Loss self: 0.0000; time: 0.21s
Val loss: 0.3576 score: 0.8980 time: 0.07s
Test loss: 0.2583 score: 0.9375 time: 0.07s
Epoch 247/1000, LR 0.000234
Train loss: 0.6487;  Loss pred: 0.6487; Loss self: 0.0000; time: 0.22s
Val loss: 0.3564 score: 0.8980 time: 0.12s
Test loss: 0.2565 score: 0.9375 time: 0.14s
Epoch 248/1000, LR 0.000234
Train loss: 0.6472;  Loss pred: 0.6472; Loss self: 0.0000; time: 0.24s
Val loss: 0.3552 score: 0.8980 time: 0.07s
Test loss: 0.2548 score: 0.9375 time: 0.07s
Epoch 249/1000, LR 0.000233
Train loss: 0.6441;  Loss pred: 0.6441; Loss self: 0.0000; time: 0.22s
Val loss: 0.3541 score: 0.8980 time: 0.07s
Test loss: 0.2532 score: 0.9375 time: 0.07s
Epoch 250/1000, LR 0.000233
Train loss: 0.6432;  Loss pred: 0.6432; Loss self: 0.0000; time: 0.22s
Val loss: 0.3531 score: 0.8980 time: 0.07s
Test loss: 0.2517 score: 0.9375 time: 0.07s
Epoch 251/1000, LR 0.000233
Train loss: 0.6418;  Loss pred: 0.6418; Loss self: 0.0000; time: 0.22s
Val loss: 0.3522 score: 0.8980 time: 0.07s
Test loss: 0.2503 score: 0.9375 time: 0.07s
Epoch 252/1000, LR 0.000232
Train loss: 0.6403;  Loss pred: 0.6403; Loss self: 0.0000; time: 0.21s
Val loss: 0.3512 score: 0.8980 time: 0.06s
Test loss: 0.2489 score: 0.9375 time: 0.06s
Epoch 253/1000, LR 0.000232
Train loss: 0.6389;  Loss pred: 0.6389; Loss self: 0.0000; time: 0.21s
Val loss: 0.3504 score: 0.8980 time: 0.07s
Test loss: 0.2476 score: 0.9375 time: 0.07s
Epoch 254/1000, LR 0.000232
Train loss: 0.6373;  Loss pred: 0.6373; Loss self: 0.0000; time: 0.22s
Val loss: 0.3497 score: 0.8980 time: 0.07s
Test loss: 0.2464 score: 0.9375 time: 0.07s
Epoch 255/1000, LR 0.000232
Train loss: 0.6367;  Loss pred: 0.6367; Loss self: 0.0000; time: 0.22s
Val loss: 0.3490 score: 0.8980 time: 0.07s
Test loss: 0.2454 score: 0.9375 time: 0.07s
Epoch 256/1000, LR 0.000231
Train loss: 0.6363;  Loss pred: 0.6363; Loss self: 0.0000; time: 0.21s
Val loss: 0.3484 score: 0.8980 time: 0.07s
Test loss: 0.2443 score: 0.9375 time: 0.07s
Epoch 257/1000, LR 0.000231
Train loss: 0.6333;  Loss pred: 0.6333; Loss self: 0.0000; time: 0.21s
Val loss: 0.3478 score: 0.8980 time: 0.07s
Test loss: 0.2433 score: 0.9375 time: 0.07s
Epoch 258/1000, LR 0.000231
Train loss: 0.6323;  Loss pred: 0.6323; Loss self: 0.0000; time: 0.21s
Val loss: 0.3472 score: 0.8980 time: 0.07s
Test loss: 0.2422 score: 0.9375 time: 0.07s
Epoch 259/1000, LR 0.000230
Train loss: 0.6319;  Loss pred: 0.6319; Loss self: 0.0000; time: 0.21s
Val loss: 0.3464 score: 0.8980 time: 0.06s
Test loss: 0.2410 score: 0.9375 time: 0.06s
Epoch 260/1000, LR 0.000230
Train loss: 0.6291;  Loss pred: 0.6291; Loss self: 0.0000; time: 0.20s
Val loss: 0.3457 score: 0.8980 time: 0.06s
Test loss: 0.2398 score: 0.9375 time: 0.06s
Epoch 261/1000, LR 0.000230
Train loss: 0.6278;  Loss pred: 0.6278; Loss self: 0.0000; time: 0.20s
Val loss: 0.3447 score: 0.8980 time: 0.06s
Test loss: 0.2385 score: 0.9375 time: 0.07s
Epoch 262/1000, LR 0.000229
Train loss: 0.6263;  Loss pred: 0.6263; Loss self: 0.0000; time: 0.21s
Val loss: 0.3438 score: 0.8980 time: 0.06s
Test loss: 0.2371 score: 0.9375 time: 0.06s
Epoch 263/1000, LR 0.000229
Train loss: 0.6266;  Loss pred: 0.6266; Loss self: 0.0000; time: 0.21s
Val loss: 0.3429 score: 0.8980 time: 0.07s
Test loss: 0.2358 score: 0.9375 time: 0.07s
Epoch 264/1000, LR 0.000229
Train loss: 0.6236;  Loss pred: 0.6236; Loss self: 0.0000; time: 0.21s
Val loss: 0.3419 score: 0.8980 time: 0.07s
Test loss: 0.2343 score: 0.9375 time: 0.07s
Epoch 265/1000, LR 0.000228
Train loss: 0.6248;  Loss pred: 0.6248; Loss self: 0.0000; time: 0.21s
Val loss: 0.3410 score: 0.8980 time: 0.07s
Test loss: 0.2330 score: 0.9375 time: 0.07s
Epoch 266/1000, LR 0.000228
Train loss: 0.6224;  Loss pred: 0.6224; Loss self: 0.0000; time: 0.21s
Val loss: 0.3403 score: 0.8980 time: 0.07s
Test loss: 0.2319 score: 0.9375 time: 0.07s
Epoch 267/1000, LR 0.000228
Train loss: 0.6228;  Loss pred: 0.6228; Loss self: 0.0000; time: 0.21s
Val loss: 0.3397 score: 0.8980 time: 0.07s
Test loss: 0.2309 score: 0.9375 time: 0.07s
Epoch 268/1000, LR 0.000228
Train loss: 0.6194;  Loss pred: 0.6194; Loss self: 0.0000; time: 0.21s
Val loss: 0.3392 score: 0.8980 time: 0.07s
Test loss: 0.2301 score: 0.9375 time: 0.07s
Epoch 269/1000, LR 0.000227
Train loss: 0.6182;  Loss pred: 0.6182; Loss self: 0.0000; time: 0.21s
Val loss: 0.3387 score: 0.8980 time: 0.07s
Test loss: 0.2292 score: 0.9375 time: 0.07s
Epoch 270/1000, LR 0.000227
Train loss: 0.6180;  Loss pred: 0.6180; Loss self: 0.0000; time: 0.20s
Val loss: 0.3384 score: 0.8980 time: 0.06s
Test loss: 0.2285 score: 0.9375 time: 0.06s
Epoch 271/1000, LR 0.000227
Train loss: 0.6174;  Loss pred: 0.6174; Loss self: 0.0000; time: 0.20s
Val loss: 0.3380 score: 0.8980 time: 0.06s
Test loss: 0.2279 score: 0.9375 time: 0.06s
Epoch 272/1000, LR 0.000226
Train loss: 0.6147;  Loss pred: 0.6147; Loss self: 0.0000; time: 0.20s
Val loss: 0.3377 score: 0.8980 time: 0.06s
Test loss: 0.2272 score: 0.9375 time: 0.06s
Epoch 273/1000, LR 0.000226
Train loss: 0.6147;  Loss pred: 0.6147; Loss self: 0.0000; time: 0.20s
Val loss: 0.3373 score: 0.8980 time: 0.06s
Test loss: 0.2264 score: 0.9375 time: 0.06s
Epoch 274/1000, LR 0.000226
Train loss: 0.6128;  Loss pred: 0.6128; Loss self: 0.0000; time: 0.20s
Val loss: 0.3369 score: 0.8980 time: 0.06s
Test loss: 0.2257 score: 0.9375 time: 0.06s
Epoch 275/1000, LR 0.000225
Train loss: 0.6121;  Loss pred: 0.6121; Loss self: 0.0000; time: 0.20s
Val loss: 0.3363 score: 0.8980 time: 0.07s
Test loss: 0.2248 score: 0.9375 time: 0.07s
Epoch 276/1000, LR 0.000225
Train loss: 0.6113;  Loss pred: 0.6113; Loss self: 0.0000; time: 0.21s
Val loss: 0.3357 score: 0.8980 time: 0.07s
Test loss: 0.2238 score: 0.9375 time: 0.07s
Epoch 277/1000, LR 0.000225
Train loss: 0.6101;  Loss pred: 0.6101; Loss self: 0.0000; time: 0.21s
Val loss: 0.3350 score: 0.8980 time: 0.07s
Test loss: 0.2228 score: 0.9375 time: 0.07s
Epoch 278/1000, LR 0.000224
Train loss: 0.6089;  Loss pred: 0.6089; Loss self: 0.0000; time: 0.21s
Val loss: 0.3343 score: 0.8980 time: 0.07s
Test loss: 0.2216 score: 0.9375 time: 0.07s
Epoch 279/1000, LR 0.000224
Train loss: 0.6071;  Loss pred: 0.6071; Loss self: 0.0000; time: 0.21s
Val loss: 0.3335 score: 0.8980 time: 0.07s
Test loss: 0.2204 score: 0.9375 time: 0.07s
Epoch 280/1000, LR 0.000224
Train loss: 0.6061;  Loss pred: 0.6061; Loss self: 0.0000; time: 0.21s
Val loss: 0.3326 score: 0.8980 time: 0.07s
Test loss: 0.2191 score: 0.9375 time: 0.07s
Epoch 281/1000, LR 0.000223
Train loss: 0.6061;  Loss pred: 0.6061; Loss self: 0.0000; time: 0.21s
Val loss: 0.3317 score: 0.8980 time: 0.07s
Test loss: 0.2178 score: 0.9375 time: 0.07s
Epoch 282/1000, LR 0.000223
Train loss: 0.6056;  Loss pred: 0.6056; Loss self: 0.0000; time: 0.21s
Val loss: 0.3310 score: 0.8980 time: 0.07s
Test loss: 0.2167 score: 0.9375 time: 0.07s
Epoch 283/1000, LR 0.000223
Train loss: 0.6060;  Loss pred: 0.6060; Loss self: 0.0000; time: 0.21s
Val loss: 0.3305 score: 0.8980 time: 0.07s
Test loss: 0.2159 score: 0.9375 time: 0.07s
Epoch 284/1000, LR 0.000222
Train loss: 0.6047;  Loss pred: 0.6047; Loss self: 0.0000; time: 0.21s
Val loss: 0.3301 score: 0.8980 time: 0.07s
Test loss: 0.2152 score: 0.9375 time: 0.07s
Epoch 285/1000, LR 0.000222
Train loss: 0.6057;  Loss pred: 0.6057; Loss self: 0.0000; time: 0.21s
Val loss: 0.3300 score: 0.8980 time: 0.07s
Test loss: 0.2148 score: 0.9375 time: 0.07s
Epoch 286/1000, LR 0.000222
Train loss: 0.6020;  Loss pred: 0.6020; Loss self: 0.0000; time: 0.22s
Val loss: 0.3300 score: 0.8980 time: 0.07s
Test loss: 0.2145 score: 0.9375 time: 0.07s
     INFO: Early stopping counter 1 of 2
Epoch 287/1000, LR 0.000221
Train loss: 0.5995;  Loss pred: 0.5995; Loss self: 0.0000; time: 0.21s
Val loss: 0.3300 score: 0.8980 time: 0.07s
Test loss: 0.2143 score: 0.9375 time: 0.07s
     INFO: Early stopping counter 2 of 2
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 284,   Train_Loss: 0.6057,   Val_Loss: 0.3300,   Val_Precision: 0.9545,   Val_Recall: 0.8400,   Val_accuracy: 0.8936,   Val_Score: 0.8980,   Val_Loss: 0.3300,   Test_Precision: 1.0000,   Test_Recall: 0.8750,   Test_accuracy: 0.9333,   Test_Score: 0.9375,   Test_loss: 0.2148


[0.06476087204646319, 0.06579116999637336, 0.06670370895881206, 0.06283232697751373, 0.06635016796644777, 0.06713514402508736, 0.06663857901003212, 0.06579748599324375, 0.06196772202383727, 0.062184436013922095, 1.1575231599854305, 0.7153603859478608, 0.4520734220277518, 0.20855456998106092, 0.0624145430047065, 0.06335032999049872, 0.06274904799647629, 0.06183908192906529, 0.06212091201450676, 0.0656258639646694, 0.06278192496392876, 0.061973148956894875, 0.06682089902460575, 0.22932421998120844, 0.06647005898412317, 0.06625901197548956, 0.06943125301040709, 0.06677279609721154, 0.06148970394860953, 0.06119351997040212, 0.06136334897018969, 0.06794143002480268, 0.06500601605512202, 0.06330907205119729, 0.06279758806340396, 0.06461836700327694, 0.06341015791986138, 0.06493650190532207, 0.06442683702334762, 0.06460916704963893, 0.0647593749454245, 0.0685099350521341, 0.06626345205586404, 0.06469235301483423, 0.6948692150181159, 0.3487890580436215, 0.39466211502440274, 0.07342516200151294, 0.06805292889475822, 0.06669866503216326, 0.07165221392642707, 0.06792366900481284, 0.06723513500764966, 0.06716945499647409, 0.08681857003830373, 0.07082555699162185, 0.9292787079466507, 1.2429560100426897, 0.3411137278890237, 0.06832538999151438, 0.0633906819857657, 0.06216920597944409, 0.06540966406464577, 0.06452242191880941, 0.06129719200544059, 0.06532936100848019, 0.06521882896777242, 0.06099008792079985, 0.8595021119108424, 0.44879164604935795, 0.06850228901021183, 0.0673556929687038, 0.06918730994220823, 0.06795746996067464, 0.07283785194158554, 0.07099246501456946, 1.6247506119543687, 0.06786082498729229, 0.06247847503982484, 0.06238862208556384, 0.06309982598759234, 0.06843031500466168, 0.06853935297112912, 0.06705821596551687, 0.06369954999536276, 0.06352299998980016, 0.06738124799448997, 0.06833131902385503, 0.06453858397435397, 0.06475179898552597, 0.06494150590151548, 0.856712986016646, 0.5195994810201228, 0.06789304199628532, 0.06746559799648821, 0.0731841679662466, 0.07599541801027954, 0.07152079197112471, 0.06579671904910356, 0.06625745096243918, 0.06559836503583938, 0.06600432505365461, 0.06585284392349422, 0.06549552700016648, 0.06582786806393415, 0.06580422702245414, 0.06627796601969749, 0.06606625311542302, 0.07201612694188952, 0.06662704993505031, 0.2685609789332375, 0.06897633103653789, 0.07226494699716568, 0.06502013502176851, 0.06194956705439836, 0.06210083398036659, 0.06174241297412664, 0.06228774704504758, 0.06199062999803573, 0.06189611298032105, 0.06258089502807707, 0.06554542703088373, 0.06167076400015503, 0.06221748807001859, 0.06154343893285841, 0.06750979099888355, 0.06695332902017981, 0.07336356001906097, 0.07371507701463997, 0.06722062802873552, 0.06712983595207334, 0.06650756602175534, 0.07445000298321247, 0.06705248705111444, 0.06685109704267234, 0.07282947294879705, 0.06859242601785809, 0.07314315508119762, 0.06692946702241898, 0.06771053606644273, 0.06733830296434462, 0.06739915790967643, 0.06787370308302343, 0.06783171591814607, 0.06763095105998218, 0.06763171299826354, 0.07347791898064315, 0.07389857294037938, 0.06693518394604325, 0.06785635103005916, 0.0740184240275994, 0.06898828910198063, 0.06841460999567062, 0.06727437896188349, 0.07242574007250369, 1.1049873309675604, 0.06973579700570554, 0.0663173709763214, 0.07052694796584547, 0.07254803495015949, 0.06626497500110418, 0.06647055200301111, 0.06632130197249353, 0.06729779299348593, 0.06681169103831053, 0.07345814001746476, 0.06851096707396209, 1.0557224840158597, 0.30492499307729304, 0.42083856696262956, 0.06481902697123587, 0.06499480898492038, 0.07171523000579327, 0.07228195201605558, 0.0643088249489665, 0.06489149294793606, 0.06518906098790467, 0.06471460603643209, 0.06474307202734053, 0.0641459230100736, 0.06441276497207582, 0.06495361309498549, 0.06971751700621098, 0.07118141697719693, 0.07059967005625367, 0.06424430094193667, 0.06453299208078533, 1.544311658013612, 0.06957882700953633, 0.06305972998961806, 0.06983294000383466, 0.0640381519915536, 0.06435789610259235, 0.06794984103180468, 0.07132203399669379, 1.1363897930132225, 0.1172800490166992, 0.07068418001290411, 0.06473798397928476, 0.07024006103165448, 0.06436633097473532, 0.0642209961079061, 0.06350769102573395, 0.06431071006227285, 0.07012145896442235, 0.06431479507591575, 0.06935204192996025, 0.06759858399163932, 0.06839565804693848, 0.0739662959240377, 0.06828439701348543, 0.7392536730039865, 1.2675416379934177, 0.1325124129652977, 0.06736809702124447, 0.07087960303761065, 0.06607222300954163, 0.0651511310134083, 0.06464820692781359, 0.06091385101899505, 0.06124209298286587, 0.5564304799772799, 2.132388131925836, 0.23668571398593485, 0.06667599105276167, 0.06308440503198653, 0.0653558389749378, 0.06502007006201893, 0.06160800904035568, 0.062220701947808266, 0.06635490793269128, 0.06648201995994896, 0.06563782307785004, 0.8506578800734133, 0.4693251089192927, 1.519177146954462, 0.3862044010311365, 0.061377485981211066, 0.06095018296036869, 0.0615432970225811, 0.0601485799998045, 0.059477505972608924, 0.06490450794808567, 0.0602725800126791, 0.06359793804585934, 0.0606649040710181, 0.06484047800768167, 0.0611747830407694, 0.06498901802115142, 1.912888011080213, 0.08741021400783211, 0.0615475740050897, 0.06136196607258171, 0.06173864600714296, 0.061009470955468714, 0.06185999605804682, 0.06252516596578062, 0.06260105897672474, 0.06590831198263913, 0.06394266500137746, 0.06749054393731058, 0.06257951003499329, 0.06289265502709895, 0.06695004005450755, 0.06419650197494775, 0.06298727996181697, 0.06346747290808707, 0.06320392701309174, 0.06292027502786368, 1.6048958000028506, 0.2362813219660893, 0.06785095599479973, 0.06721629993990064, 0.07240299799013883, 0.06757554796058685, 0.06686734198592603, 0.06738421006593853, 0.07057991600595415, 0.07433232304174453, 0.06764360598754138, 0.06783948605880141, 0.07256339699961245, 0.07391184696462005, 0.06844383501447737, 0.06947421608492732, 0.06753231701441109, 0.4220553449122235, 0.06859597004950047, 0.06921478407457471, 0.0646464090095833, 0.06813114194665104, 0.06510418897960335, 0.06532575993333012, 0.06525081698782742, 0.07011349394451827, 0.06548786209896207, 0.06534931890200824, 0.06434668507426977, 0.07230448699556291, 0.06943024904467165, 0.07413355202879757, 0.06864948908332735, 0.16475897992495447, 0.07491439499426633, 0.7072585810674354, 0.5006445340113714, 0.18565852008759975, 0.0676679010502994, 0.06722866697236896, 0.06886849901638925, 0.0668798639671877, 0.06879457598552108, 0.06783791701309383, 0.06819728610571474, 0.06873145198915154, 0.0683390130288899, 0.7279247969854623, 0.1665846329415217, 0.07137009501457214, 0.06616553501226008, 0.06538884702604264, 0.07008704706095159, 0.06557476299349219, 0.07241675991099328, 0.06500819500070065, 0.06504361901897937, 0.06544885702896863, 0.06500213406980038, 0.06600897200405598, 0.06511185294948518, 0.0655770639423281, 0.06698209897149354, 0.06579338503070176, 0.06996298802550882, 1.899643717915751, 0.07226561894640326, 0.06332338193897158, 0.06254529894795269, 0.06327792804222554, 0.06251364096533507, 0.0631484299665317, 0.06287728797178715, 0.06262192700523883, 0.06349792098626494, 0.06439099693670869, 0.06340990611352026, 0.0650928079849109, 0.06341387098655105, 0.06293345894664526, 0.0645281879696995, 1.7162056119414046, 0.2716270349919796, 0.06714264408219606, 0.07118098007049412, 0.06566129601560533, 0.06418716302141547, 0.07142510497942567, 0.06911094696260989, 0.06466778297908604, 0.06990271992981434, 0.07148966705426574, 0.06545510503929108, 0.06561778602190316, 1.2000648390967399, 0.06945508194621652, 0.065545194898732, 0.15201776998583227, 0.06295793992467225, 0.064309066045098, 0.06580444297287613, 0.06939325598068535, 0.06445921899285167, 0.06418992893304676, 0.3668155289487913, 0.20138085703365505, 0.19358270696830004, 0.07272000994998962, 0.06528760003857315, 0.0653917589224875, 0.06596439203713089, 0.06563992309384048, 0.06641777500044554, 0.07210104598198086, 0.06443729600869119, 0.06992327095940709, 0.0695811070036143, 2.2161726449849084, 0.071116485982202, 0.06742271594703197, 0.06291959306690842, 0.06714440998621285, 0.06201162200886756, 0.061729033943265676, 0.06224128999747336, 0.061859858920797706, 0.06827762501779944, 0.06586357206106186, 0.06834904197603464, 0.06278733804356307, 0.06359967798925936, 1.3186588590033352, 0.06958860799204558, 0.06546626298222691, 0.06502499699126929, 0.07032969291321933, 0.06582687492482364, 0.0655704439850524, 1.9051513120066375, 0.34100844198837876, 0.0672868819674477, 0.0643655169988051, 0.06323832599446177, 0.07136381301097572, 0.06756826408673078, 0.06489188096020371, 0.06471206899732351, 0.06912705302238464, 0.06919988198205829, 0.6070576390484348, 0.0859090369194746, 0.0758327569346875, 0.0649796420475468, 0.06416665902361274, 0.06514160090591758, 0.06459972297307104, 0.06517120101489127, 0.06891537504270673, 0.06532650394365191, 0.06629782903473824, 2.238558550947346, 0.06598101602867246, 0.06924301700200886, 0.067480772966519, 0.0628592330031097, 0.06268713995814323, 0.06307572196237743, 0.06305542297195643, 0.06772002205252647, 0.06268972100224346, 0.062342574005015194, 0.0634767560986802, 0.06228792399633676, 0.06478872790466994, 0.06264090596232563, 0.07228142698295414, 0.07419338892214, 1.7326488689286634, 0.06752698204945773, 0.0669534649932757, 0.06520805996842682, 0.06508939596824348, 0.06680669297929853, 0.06480691605247557, 0.06576545105781406, 0.06891637109220028, 0.5567155629396439, 0.4813709679292515, 0.06928115605842322, 0.06672954501118511, 0.07263791200239211, 0.0672091000014916, 0.06677619100082666, 0.06693172990344465, 0.06674282194580883, 0.07575565692968667, 0.07562583195976913, 0.0763560690684244, 0.07039989798795432, 0.06928758102003485, 0.07593594503123313, 0.3158821549732238, 0.13041383994277567, 0.06614619097672403, 0.0659579400671646, 0.06593002506997436, 0.06693006900604814, 0.07002889807336032, 0.06604031892493367, 1.7890617039520293, 0.22036014206241816, 0.0759566790657118, 0.06264192296657711, 0.06709688995033503, 0.06791546498425305, 0.06306132394820452, 0.06323279696516693, 0.06395158101804554, 0.06762970797717571, 0.06245590595062822, 0.06364724098239094, 0.06304338597692549, 0.06624758802354336, 0.06256500107701868, 0.062405450036749244, 0.06723620695993304, 0.0686056069098413, 0.06282429001294076, 0.0674764149589464, 0.07289072405546904, 0.0673585309414193, 0.06730797293130308, 0.07337011909112334, 0.0664173288969323, 0.06607174105010927, 0.06301894097123295, 0.06300670397467911, 0.896176618989557, 0.8744621520163491, 0.21723663003649563, 0.07425843097735196, 0.06504185998346657, 0.06942025804892182, 0.0680359520483762, 0.1715412779012695, 0.06343274796381593, 0.06362874701153487, 0.06590970896650106, 0.06434416002593935, 0.06861695402767509, 0.517940251971595, 1.6072091270470992, 0.506246354081668, 0.11551346397027373, 0.06630984100047499, 0.06963990395888686, 0.06729125301353633, 0.06486731197219342, 0.06440085300710052, 0.06430957093834877, 0.06387872400227934, 1.2316235409816727, 1.3823849940672517, 0.07851068291347474, 0.07376984693109989, 0.07108216302003711, 0.07271471689455211, 0.07723820197861642, 0.07201258500572294, 0.07108293694909662, 0.31924587104003876, 0.3110714729409665, 0.24418258003424853, 0.07032156898640096, 0.07015903701540083, 0.06441724894102663, 0.06654989102389663, 0.06320760492235422, 0.063731548958458, 0.06386265100445598, 0.06440409598872066, 0.06900439003948122, 0.06583102501463145, 0.06380656594410539, 0.06863827502820641, 0.06819069501943886, 0.06474722002167255, 0.06424930098000914, 0.0643530689412728, 0.06374898494686931, 0.06989809998776764, 0.06339333101641387, 0.06399103906005621, 0.06720307003706694, 0.0658894709777087, 0.06558989104814827, 0.35669670498464257, 0.2774782329797745, 0.06974509102292359, 0.06629608699586242, 0.07045628689229488, 0.07003830000758171, 0.06582778599113226, 0.06648476398549974, 0.06757584598381072, 0.06799790496006608, 0.0674353139474988, 0.07260047493036836, 1.2878672890365124, 0.42416838300414383, 0.0708550380077213, 0.07010504603385925, 0.06911957205738872, 0.06387625099159777, 0.06314541702158749, 0.06569126795511693, 0.06714261998422444, 0.06793443101923913, 0.06717778695747256, 0.06921430805232376, 0.06554288696497679, 0.07067063602153212, 1.6688041030429304, 0.07966629404108971, 0.06865600997116417, 0.07281723199412227, 0.07409592799376696, 0.06876463210210204, 0.06786132592242211, 0.06932685791980475, 0.06493154994677752, 0.06484484695829451, 0.06852187891490757, 0.07018460996914655, 0.065465735970065, 0.06465642701368779, 0.06410289893392473, 0.06439938396215439, 0.06907692807726562, 0.06559018197003752, 0.06878612097352743, 0.0636490338947624, 0.07021142798475921, 0.06324622198008001, 0.06431960500776768, 0.06692157802172005, 0.06340384704526514, 0.07188033603597432, 0.06277055200189352, 0.06325363996438682, 0.0633833589963615, 0.06339651392772794, 0.0673870580503717, 0.06369193003047258, 0.06352362199686468, 0.06936838501133025, 0.06352287204936147, 0.0665385730098933, 0.06500590301584452, 0.06316434603650123, 0.0695675800088793, 0.07912603102158755, 0.5844906029524282, 0.13304443599190563, 0.06720435293391347, 0.07339471008162946, 0.06709514104295522, 0.06664645893033594, 0.07241625699680299, 0.0673919339897111, 0.06662923796102405, 0.06642111495602876, 0.0668294399511069, 0.06493537197820842, 0.06322812498547137, 0.06634204590227455, 0.06255423102993518, 0.06259065808262676, 0.06379132508300245, 0.06288823706563562, 0.0621612899703905, 0.06835659104399383, 0.06396779301576316, 1.3505150770070031, 0.5170005529653281, 0.0689914709655568, 0.06966114405076951, 0.07459674205165356, 0.07465723797213286, 0.07027957204263657, 0.06894670298788697, 0.06392588000744581, 0.06406681705266237, 0.06431151297874749, 0.06458875490352511, 0.06488044699653983, 0.065676145022735, 0.35896303795743734, 1.2143864179961383, 0.07307350402697921, 0.06556992197874933, 0.06835028098430485, 0.08258796902373433, 0.06818107003346086, 0.06971676799003035, 0.16462945798411965, 0.06640488398261368, 0.06783919292502105, 0.07787828089203686, 0.07304806995671242, 0.07368136709555984, 0.07458002492785454, 0.07515960093587637, 0.06915963196661323, 0.06756905501242727, 0.06901379197370261, 0.07452968298457563, 0.06816296197939664, 0.06471523200161755, 0.06693262991029769, 0.06875477300491184, 0.07952850603032857, 0.06697196408640593, 0.06376407400239259, 0.06349858804605901, 0.06372120301239192, 0.06320428301114589, 0.06704925000667572, 0.06351494207046926, 0.06360963697079569, 0.06883960799314082, 0.06846590200439095, 0.06928402802441269, 0.06961149291601032, 0.06880488898605108, 0.06799859402235597, 0.08520903903990984, 0.06564321601763368, 0.06672774895559996, 0.07125349901616573, 0.9326045931084082, 0.19147153396625072, 0.22290814109146595, 0.06835463794413954, 0.067610680940561, 0.06682980805635452, 0.06916595599614084, 0.06328932603355497, 0.063800786039792, 0.06399306305684149, 0.06298427400179207, 0.06481503101531416, 1.189055873081088, 0.4092626499477774, 0.07255974307190627, 0.07367307704407722, 0.06647590606007725, 0.07152761903125793, 0.06630488007795066, 0.071962476009503, 0.06671614805236459, 0.06672616396099329, 0.0729592900024727, 0.06630897999275476, 0.06693426193669438, 1.2392065059393644, 0.40281887899618596, 0.07382444804534316, 0.06446668901480734, 0.06884725007694215, 0.06952448503579944, 0.06382494198624045, 0.06547389400657266, 0.06431331089697778, 0.06947608303744346, 0.06549400591757149, 0.067580278031528, 0.0638267049798742, 0.06398291804362088, 0.06851908203680068, 0.06828400900121778, 0.06414322403725237, 0.06848392600659281, 0.07262359699234366, 0.06490559701342136, 0.06663399399258196, 0.06449548597447574, 0.06905579299200326, 0.07653011404909194, 0.07602321798913181, 0.06532603409141302, 0.8788891969015822, 0.27111051604151726, 0.07314431900158525, 0.0649898829869926, 0.06671972898766398, 0.0664210399845615, 0.06542360107414424, 0.06253501202445477, 0.06445766892284155, 0.06479838397353888, 0.07054607500322163, 0.07039458001963794, 0.065173669019714, 0.06805391400121152, 0.06851681298576295, 0.07044602697715163, 0.06432551401667297, 0.06500284594949335, 0.06457605899777263, 0.0654990510083735, 0.06379286793526262, 0.06456376204732805, 0.06988232699222863, 0.06446634698659182, 0.06401399301830679, 0.07062350900378078, 0.06861829198896885, 0.06755873002111912, 0.07087855401914567, 0.06670955300796777, 0.0673260820331052, 0.07348539703525603, 0.06830362405162305, 0.07730086101219058, 0.06577072897925973, 0.0666669289348647, 0.06769820593763143, 0.07245017006061971, 1.9596740840934217, 0.9620321380207315, 0.07046111591625959, 0.0661841289838776, 0.06658130697906017, 0.06601538206450641, 0.06682723190169781, 0.06865145300980657, 0.06898205203469843, 0.0653947819955647, 0.06540770502761006, 0.06596414197701961, 0.06502940692007542, 0.07118258194532245, 0.07133390300441533, 0.07771919202059507, 0.07355559896677732, 0.07522508397232741, 0.07532250508666039, 0.4084291330073029, 0.06941979797556996, 0.06922365503851324, 0.06821172300260514, 0.0690124100074172, 0.06366311002057046, 0.06415278499480337, 0.06419195095077157, 0.07602789194788784, 0.06731262803077698, 0.06729819905012846, 0.07346002699341625, 0.07078589906450361, 0.07427044294308871, 0.0736660819966346, 4.503635878092609, 0.07090341404546052, 0.07478197803720832, 0.06951119098812342, 0.07665546203497797, 0.07437500089872628, 0.07058266701642424, 0.06871553789824247, 0.0696682749548927, 2.332156667020172, 0.11686899000778794, 0.07871303509455174, 0.07055106398183852, 0.07199013198260218, 0.09376147203147411, 0.07216680399142206, 0.07080496102571487, 0.06907609000336379, 0.0964147619670257, 0.06607787392567843, 0.06552604003809392, 0.3881415129872039, 0.589422435965389, 0.4416610869811848, 0.06642098899465054, 0.06578986090607941, 0.06496899807825685, 0.06450557801872492, 0.06417133600916713, 0.06789846892934293, 0.0642391899600625, 0.06423501507379115, 0.06929359491914511, 0.06399363896343857, 0.06451334792654961, 0.06411885400302708, 0.06508341000881046, 0.06473090499639511, 0.07003381999675184, 0.06497018598020077, 0.06504035601392388, 0.06508302304428071, 0.06520673504564911, 0.06518018001224846, 0.6629909779876471, 0.06755822896957397, 0.07157642999663949, 0.07272328692488372, 0.06755729403812438, 0.06771413504611701, 0.0715579780517146, 0.07116692594718188, 0.06715583894401789, 0.06848079105839133, 0.07245134899858385, 0.06722319894470274, 0.06702481501270086, 1.2813479989999905, 0.4242539450060576, 0.07447791192680597, 0.07344364305026829, 0.0679449460003525, 0.0673694210126996, 0.06451893097255379, 0.06527983793057501, 0.0645159799605608, 0.06451353791635484, 0.06359558505937457, 0.06826719699893147, 0.07225725089665502, 0.06712194008287042, 0.06816358701325953, 0.06355647498276085, 0.06342282507102937, 0.06409452995285392, 0.06353080994449556, 0.06412143202032894, 0.06470184994395822, 0.06406555196736008, 0.06418455101083964, 0.06448719301261008, 0.06435815803706646, 0.06566310208290815, 0.06721896294038743, 0.061438523000106215, 0.0614573530619964, 0.06632577197160572, 0.06660741090308875, 0.06304434209596366, 0.062390729086473584, 0.06757632200606167, 0.06224048207513988, 0.062174338032491505, 0.06284572905860841, 0.06227965105790645, 0.06752009608317167, 0.06740075396373868, 0.06250167998950928, 0.06291136902291328, 0.06730246206279844, 0.15030146902427077, 0.06578605202957988, 0.06729742395691574, 0.06942942296154797, 0.06754914403427392, 0.06394178501795977, 0.07084997103083879, 0.06671703804749995, 0.06626581703312695, 0.0666622529970482, 0.06542167300358415, 0.06645414605736732, 0.0673836680362001, 0.1479212570702657, 0.0664302259683609, 0.06586207693908364, 0.06253713590558618, 0.06324854400008917, 0.06221957900561392, 0.0641926700482145, 0.06554925406817347, 0.06259911507368088, 0.06743779208045453, 0.06788706895895302, 0.06741353799588978, 0.06647619407158345, 0.071465541026555, 0.07118047692347318, 0.0725632639368996, 0.07049167098011822, 0.06552886101417243, 0.06561469100415707, 0.07097887492273003, 0.06530368502717465, 0.06594293203670532, 0.06080109300091863, 0.0667420559329912, 0.062383426004089415, 0.061483904952183366, 0.06124903494492173, 0.06171438191086054, 0.062082702992483974, 0.0662174359895289, 0.07125466503202915, 0.06620554102119058, 0.06591898808255792, 0.0665885719936341, 0.6497554560191929, 0.9856888699578121, 0.0686382990097627, 0.07206263800617307, 0.06703889893833548, 0.0676741610514, 0.06212588702328503, 0.06289674190338701, 0.06663579004816711, 0.06213947606738657, 0.0632768829818815, 0.06185680499766022, 0.06259323400445282, 0.063094393000938, 0.06173845799639821, 0.06352546706330031, 0.06286085909232497, 0.06723922595847398, 0.06744257791433483, 0.6813678949838504, 0.8929920350201428, 0.06900974805466831, 0.07329760398715734, 0.07288600702304393, 0.07351605198346078, 0.07320944697130471, 0.07470090803690255, 0.07364072802010924, 0.0734892119653523, 0.07299982709810138, 0.07366373401600868, 0.07320796104613692, 0.07386978296563029, 0.06849678105209023, 0.0664602090837434, 0.06619799800682813, 4.533783770049922, 0.6670357179827988, 0.07097875897306949, 0.07065471401438117, 0.07063882902730256, 0.07213402807246894, 0.0663723050383851, 0.06654895492829382, 0.06758477503899485, 0.06605030107311904, 0.0776737011037767, 0.07322518492583185, 0.0713946350151673, 0.07049686391837895, 0.0697195699904114, 0.071599071030505, 2.749402269953862, 0.07070181495510042, 0.06944836210459471, 0.06782910402398556, 0.06800632202066481, 0.07279606501106173, 0.07337457698304206, 0.08973858400713652, 0.07674665795639157, 0.5972348679788411, 0.07901465403847396, 0.07266889698803425, 0.07231832598336041, 0.07208395993802696, 0.07225321198347956, 0.07184084400068969, 0.07186439400538802, 0.07166514603886753, 0.07213008892722428, 0.07169795804657042, 0.07211136003024876, 0.07255923107732087, 0.07189503405243158, 0.07221680297516286, 0.1612819649744779, 0.07194329507183284, 0.07132423296570778, 0.07228857895825058, 0.07213372900150716, 0.0722948289476335, 0.07250893104355782, 0.07205735507886857, 0.07475457701366395, 0.07244636199902743, 0.07287380704656243, 0.07296977611258626, 2.375354212941602, 0.7659915660042316, 0.07539979100693017, 0.07401058706454933, 0.07321318401955068, 0.07340948795899749, 0.07354960893280804, 0.0706268479116261, 0.07012413803022355, 0.07398431305773556, 0.07550053903833032, 0.07401300210040063, 0.07437927403952926, 0.07371956901624799, 0.07395844289567322, 0.07687062898185104, 0.0748910770053044, 0.07439555402379483, 0.07449377002194524, 0.07441904710140079, 0.07529994705691934, 0.07500013301614672, 2.474249894032255, 0.07690031302627176, 0.07344733597710729, 0.07229233696125448, 0.06780415202956647, 0.06895118998363614, 0.06860720505937934, 0.07235368504188955, 0.07269789290148765, 0.07264930394012481, 0.07302198803517967, 0.06826134701259434, 0.06815036304760724, 0.06762765301391482, 0.06820873101241887, 0.07217615994159132, 0.07223050796892494, 0.07194292801432312, 0.07297717500478029, 0.07372458302415907, 0.07416535797528923, 0.07259585009887815, 0.07265447801910341, 0.5297662040684372, 0.07516886596567929, 0.07038655504584312, 0.06905363197438419, 0.06914906995370984, 0.06918600900098681, 0.069772575981915, 0.07059501600451767, 0.07026206096634269, 0.06930794601794332, 0.06938106403686106, 0.06908585398923606, 0.07030969799961895, 0.07003533898387104, 0.07010024203918874, 0.07092589698731899, 0.07059947797097266, 1.130748801981099, 0.08316732407547534, 0.069075328996405, 0.0680613579461351, 0.06724902894347906, 0.06629594101104885, 0.06719394004903734, 0.06757949595339596, 0.0672314140247181, 0.06680819299072027, 0.06835492898244411, 0.07328807504381984, 0.06923204800114036, 0.07020720595028251, 0.07243974995799363, 0.4614236110355705, 1.4102375690126792, 0.0767256369581446, 0.0694161569699645, 0.06916900700889528, 0.06987093901261687, 0.07010455499403179, 0.07299563707783818, 0.07036085403524339, 0.07287674199324101, 0.07085339003242552, 0.07045898598153144, 0.07021596399135888, 0.07130337599664927, 0.0684060319326818, 0.07643289701081812, 0.07492864702362567, 0.06860564998351038, 1.595143656944856, 0.35706314304843545, 0.07048205099999905, 0.07041943492367864, 0.06920084601733834, 0.06943876098375767, 0.0688324939692393, 0.0697359360055998, 0.07353627006523311, 0.07323377800639719, 0.07339408004190773, 0.07385482208337635, 0.07444279605988413, 0.07351610204204917, 0.07425773597788066, 0.3048237219918519, 0.07429360295645893, 0.07001591403968632, 0.07012995902914554, 0.06728605600073934, 0.07673994300421327, 0.0813313169637695, 0.3824240640969947, 0.06938360002823174, 0.06911408400628716, 0.07088966097217053, 0.07058635097928345, 0.0699123329250142, 0.9985138329211622, 0.40264356101397425, 0.07044462894555181, 0.07045268104411662, 0.06942441803403199, 0.07083008403424174, 0.07009783200919628, 1.4008034009020776, 0.07717719895299524, 0.07212722802069038, 0.07153504702728242, 0.07092201092746109, 0.07071072806138545, 0.06972091901116073, 0.07043316797353327, 0.06905160192400217, 0.06951458193361759, 0.06833317701239139, 0.06809779303148389, 0.06809274898841977, 0.06837076297961175, 0.06825039000250399, 0.06857378105632961, 0.06951758707873523, 0.06861796393059194, 0.07389139605220407, 0.07359663094393909, 0.15369573596399277, 0.0739787279162556, 0.07392572797834873, 0.0740935179637745, 0.07633441593497992, 0.09052005189005286, 0.07017621002160013, 0.07059554697480053, 0.07003963796887547, 0.07433178799692541, 0.07621634099632502, 0.0692754570627585, 0.069682297995314, 0.06990981101989746, 0.07051036600023508, 0.07048956595826894, 0.06960290891584009, 0.16712035203818232, 0.10524884797632694, 0.07389397907536477, 0.06932295591104776, 0.06901977001689374, 0.06839813699480146, 0.08041493000928313, 0.07330466399434954, 0.07209991000127047, 0.07273417094256729, 0.07135215494781733, 0.07187455601524562, 0.08392252097837627, 0.0719911950873211, 0.07163573789875954, 0.07111736491788179, 0.07107891398482025, 0.07138921099249274, 0.0714686029823497, 0.07144602295011282, 0.07261112798005342, 0.08420223300345242, 0.07303680793847889, 0.07214471895713359, 0.07288773800246418, 0.1403668540297076, 0.07346552901435643, 0.07405136700253934, 0.07396935496944934, 0.0723716199863702, 0.06908539088908583, 0.07262928795535117, 0.07211886101868004, 0.07183554896619171, 0.0721025129314512, 0.07351294998079538, 0.07161376706790179, 0.06839614396449178, 0.06831639003939927, 0.06941765104420483, 0.06879331194795668, 0.07159946300089359, 0.07140334800351411, 0.07310205907560885, 0.07146607094909996, 0.07241438305936754, 0.07182194897904992, 0.07194112206343561, 0.06781803700141609, 0.06833355606067926, 0.06821280403528363, 0.06823683099355549, 0.06820648291613907, 0.07191162591334432, 0.07169056206475943, 0.07191903900820762, 0.07121358101721853, 0.07307162194047123, 0.07192127197049558, 0.07234164292458445, 0.0726507599465549, 0.07221248908899724, 0.07167961494997144, 0.07204567606095225, 0.07170251000206918, 0.07219963194802403]
[0.0013216504499278202, 0.001342676938701497, 0.0013613001828328992, 0.0012822923872961986, 0.0013540850605397504, 0.0013701049801038236, 0.001359971000204737, 0.0013428058365968112, 0.001264647388241577, 0.001269070122733104, 0.023622921632355725, 0.014599191549956342, 0.009225988204647996, 0.004256215713899203, 0.0012737661837695204, 0.0012928638773571169, 0.0012805928162546183, 0.0012620220801850058, 0.0012677737145817705, 0.001339303346217743, 0.0012812637747740563, 0.0012647581419774465, 0.0013636918168286889, 0.004680086122065478, 0.0013565318160025136, 0.0013522247341936644, 0.0014169643471511652, 0.0013627101244328885, 0.0012548919173185618, 0.0012488473463347371, 0.0012523132442895854, 0.0013865597964245447, 0.0013266533888800411, 0.0012920218785958631, 0.001281583429865387, 0.0013187421837403458, 0.001294084855507375, 0.001325234732761675, 0.0013148334086397473, 0.0013185544295844678, 0.0013216198968453979, 0.0013981619398394714, 0.0013523153480788578, 0.0013202521023435556, 0.014181004388124816, 0.007118144041706561, 0.008054328878049036, 0.0014984726939084275, 0.0013888352835664944, 0.0013611972455543522, 0.0014622900801311647, 0.0013861973266288334, 0.0013721456124010135, 0.0013708052040096752, 0.001771807551802117, 0.0014454195304412622, 0.018964871590747973, 0.025366449184544688, 0.006961504650796402, 0.0013943957141125385, 0.001293687387464606, 0.0012687593057029406, 0.0013348911033601177, 0.0013167841207920288, 0.0012509631021518487, 0.0013332522654791875, 0.001330996509546376, 0.001244695671853058, 0.017540859426751886, 0.009159013184680775, 0.0013980058981675882, 0.001374605978953139, 0.001411985917187923, 0.0013868871420545845, 0.0014864867743180723, 0.0014488258166238666, 0.033158175754170786, 0.0013849147956590264, 0.001275070919180099, 0.0012732371854196703, 0.0012877515507671907, 0.0013965370409114628, 0.0013987623055332474, 0.0013685350197044257, 0.001299990816231893, 0.0012963877548938807, 0.001375127510091632, 0.0013945167147725516, 0.001317113958660285, 0.0013214652854188973, 0.001325336855132969, 0.017483938490135634, 0.010604071041226995, 0.0013855722856384758, 0.0013768489387038412, 0.001493554448290747, 0.0015509268981689702, 0.0014596079994107084, 0.0013427901846755827, 0.001352192876784473, 0.0013387421435885588, 0.0013470270419113186, 0.0013439355902753922, 0.0013366434081666628, 0.001343425878855799, 0.001342943408621513, 0.0013526115514223976, 0.0013482908799065923, 0.0014697168763650923, 0.0013597357129602103, 0.005480836304759949, 0.0014076802252354671, 0.0014747948366768506, 0.0013269415310565003, 0.001264276878661191, 0.0012673639587829917, 0.0012600492443699315, 0.00127117851112342, 0.0012651148979190964, 0.0012631859791902254, 0.001277161123021981, 0.0013376617761404843, 0.0012585870204113272, 0.0012697446544901753, 0.0012559885496501715, 0.001377750836711909, 0.0013663944697995878, 0.001497215510593081, 0.0015043893268293872, 0.0013718495516068473, 0.0013699966520831293, 0.001357297265750109, 0.001519387815983928, 0.0013684181030839682, 0.0013643081029116803, 0.0014863157744652458, 0.0013998454289358793, 0.0014927174506366861, 0.0013659074902534485, 0.0013818476748253619, 0.0013742510809049923, 0.0013754930185648252, 0.0013851776139392536, 0.0013843207330233892, 0.0013802234910200446, 0.0013802390407808886, 0.0014995493669519011, 0.0015081341416403955, 0.001366024162164148, 0.0013848234904093705, 0.001510580082195906, 0.0014079242673873597, 0.00139621653052389, 0.0013729465094261936, 0.0014780763280102794, 0.022550761856480826, 0.0014231795307286844, 0.0013534157342106408, 0.0014393254686907238, 0.0014805721418399895, 0.0013523464285939628, 0.0013565418776124716, 0.0013534959586223169, 0.0013734243468058352, 0.001363503898741031, 0.001499145714642138, 0.0013981830015094305, 0.021545356816650197, 0.006222959042393735, 0.008588542182910807, 0.0013228372851272626, 0.0013264246731616405, 0.0014635761225672097, 0.0014751418778786855, 0.0013124249989585, 0.00132431618261094, 0.0013303889997531564, 0.0013207062456414712, 0.0013212871842314393, 0.0013091004695933387, 0.0013145462239199147, 0.0013255839407139895, 0.00142280646951451, 0.001452681979126468, 0.0014408095929847689, 0.0013111081824885035, 0.0013169998383833741, 0.031516564449257385, 0.0014199760614191086, 0.0012869332650942461, 0.001425162040894585, 0.0013069010610521144, 0.0013134264510733132, 0.001386731449628667, 0.0014555517142182406, 0.023191628428841277, 0.0023934703880959017, 0.001442534285977635, 0.0013211833465160156, 0.001433470633299071, 0.001313598591321129, 0.0013106325736307368, 0.0012960753270557948, 0.0013124634706586295, 0.0014310501829473948, 0.001312546838283995, 0.0014153477944889848, 0.001379562938604884, 0.0013958297560599689, 0.0015095162433477081, 0.0013935591227241925, 0.015086809653142581, 0.02586819669374322, 0.002704334958475463, 0.00137485912288254, 0.0014465225109716459, 0.0013484127144804414, 0.0013296149186409858, 0.001319351161792114, 0.0012431398167141847, 0.001249838632303385, 0.011355724081168977, 0.043518125141343594, 0.004830320693590507, 0.0013607345112808505, 0.0012874368373874802, 0.0013337926321415877, 0.001326940205347325, 0.0012573063069460343, 0.0012698102438328217, 0.0013541817945447198, 0.0013567759175499787, 0.0013395474097520417, 0.017360364899457414, 0.009578063447332504, 0.03100361524396861, 0.007881722470023195, 0.0012526017547185933, 0.0012438812849054836, 0.0012559856535220633, 0.0012275220408123366, 0.001213826652502223, 0.0013245817948588912, 0.0012300526533199816, 0.0012979171029767211, 0.0012380592667554713, 0.0013232750613812584, 0.001248464960015702, 0.00132630649022758, 0.03903853083837169, 0.001783881918527186, 0.0012560729388793816, 0.0012522850218894227, 0.0012599723674927134, 0.0012450912439891575, 0.0012624488991438126, 0.0012760237952200125, 0.001277572632178056, 0.0013450675914824313, 0.001304952346966887, 0.0013773580395369505, 0.001277132857857006, 0.0012835235719816113, 0.0013663273480511745, 0.0013101326933662807, 0.0012854546930983054, 0.001295254549144634, 0.0012898760614916682, 0.0012840872454666058, 0.03275297551026226, 0.004822067795226312, 0.0013847133876489742, 0.0013717612232632783, 0.0014776122038803843, 0.0013790928155221806, 0.0013646396323658374, 0.0013751879605293578, 0.001440406449101105, 0.0015169861845253985, 0.0013804817548477833, 0.0013844793073224779, 0.0014808856530533153, 0.0015084050400942868, 0.0013968129594791301, 0.0014178411445903536, 0.001378210551314512, 0.008613374385963745, 0.0013999177561122545, 0.0014125466137668307, 0.0013193144695833325, 0.0013904314682990008, 0.0013286569179510887, 0.0013331787741495942, 0.0013316493262821923, 0.001430887631520781, 0.0013364869816114707, 0.0013336595694287395, 0.0013131976545769342, 0.0014756017754196512, 0.0014169438580545236, 0.0015129296332407668, 0.001401009981292395, 0.0033624281617337646, 0.001528865203964619, 0.014433848593212967, 0.010217235387987172, 0.003788949389542852, 0.0013809775724550899, 0.001372013611680999, 0.001405479571763046, 0.0013648951830038307, 0.001403970938480022, 0.0013844472859815067, 0.001391781349096219, 0.001402682693656154, 0.0013946737352834673, 0.014855608101744128, 0.0033996863865616675, 0.0014565325513177989, 0.0013503170410665323, 0.0013344662658376048, 0.0014303478992030937, 0.0013382604692549426, 0.0014778930594080261, 0.0013266978571571562, 0.0013274207963057014, 0.00133569095977487, 0.0013265741646898035, 0.0013471218776337955, 0.0013288133254996976, 0.001338307427394451, 0.0013669816116631335, 0.0013427221434837093, 0.0014278160821532412, 0.03876823914113778, 0.0014748085499265973, 0.001292313917121869, 0.0012764346724071977, 0.0012913862865760314, 0.0012757885911292872, 0.0012887434687047284, 0.0012832099586079012, 0.0012779985103109966, 0.001295875938495203, 0.0013141019783001772, 0.0012940797166024543, 0.0013284246527532839, 0.0012941606323785928, 0.0012843563050335767, 0.0013169017952999898, 0.03502460432533479, 0.005543408877387339, 0.0013702580424937972, 0.0014526730626631453, 0.001340026449298068, 0.0013099421024778668, 0.0014576552036617482, 0.001410427489032855, 0.0013197506730425724, 0.0014265861210166191, 0.0014589727970258314, 0.001335818470189614, 0.0013391384902429215, 0.024491119165239588, 0.0014174506519636025, 0.0013376570387496328, 0.0031024034690986177, 0.0012848559168300458, 0.0013124299192877145, 0.0013429478157729823, 0.0014161888975650072, 0.0013154942651602383, 0.0013099985496540154, 0.007486031203036558, 0.004109813408850103, 0.003950667489148981, 0.0014840818357140738, 0.001332400000787207, 0.001334525692295663, 0.0013462120823904264, 0.0013395902672212344, 0.00135546479592746, 0.0014714499179996094, 0.0013150468573202283, 0.0014270055297838182, 0.001420022591910496, 0.04522801316295731, 0.0014513568567796325, 0.0013759737948373873, 0.0012840733278960902, 0.0013702940813512827, 0.0012655433063034195, 0.0012597762029237893, 0.0012702304081117011, 0.001262446100424443, 0.0013934209187306007, 0.0013441545318584054, 0.0013948784076741763, 0.0012813742457870015, 0.0012979526120257012, 0.026911405285782352, 0.0014201756733070528, 0.0013360461833107533, 0.001327040754923863, 0.0014352998553718232, 0.0013434056107106867, 0.0013381723262255593, 0.038880639020543624, 0.006959355958946505, 0.001373201672805055, 0.001313581979567451, 0.001290578081519628, 0.0014564043471627698, 0.001378944165035322, 0.001324324101228647, 0.001320654469333133, 0.0014107561841302989, 0.001412242489429761, 0.01238893140915173, 0.001753245651417849, 0.0015476072843813775, 0.0013261151438274858, 0.0013095236535431171, 0.0013294204266513794, 0.0013183616933279804, 0.001330024510507985, 0.0014064362253613618, 0.0013331939580337126, 0.0013530169190762906, 0.045684868386680524, 0.0013465513475239277, 0.0014131227959593643, 0.0013771586319697754, 0.0012828414898593814, 0.0012793293869008823, 0.0012872596318852536, 0.001286845366774621, 0.0013820412663780913, 0.0012793820612702746, 0.0012722974286737796, 0.0012954440020138817, 0.0012711821223742195, 0.0013222189368299988, 0.0012783858359658293, 0.0014751311629174315, 0.0015141507943293878, 0.035360180998544155, 0.0013781016744787292, 0.0013663972447607287, 0.001330776734049527, 0.001328355019760071, 0.0013634018975367047, 0.0013225901235199096, 0.0013421520624043687, 0.0014064565529020466, 0.011361542100809058, 0.009823897304678602, 0.0014139011440494535, 0.0013618274492078594, 0.0014824063673957574, 0.0013716142857447267, 0.001362779408180136, 0.0013659536714988704, 0.001362098407057323, 0.0015460338148915646, 0.001543384325709574, 0.001558287123845396, 0.0014367326119990678, 0.0014140322657149968, 0.001549713163902717, 0.0064465745912902815, 0.0026615069376076665, 0.0013499222648311027, 0.0013460804095339713, 0.0013455107157137624, 0.0013659197756336356, 0.0014291611851706188, 0.001347761610712932, 0.03651146334595978, 0.0044971457563758805, 0.001550136307463506, 0.001278406591154635, 0.0013693242847007147, 0.0013860298976378174, 0.0012869657948613167, 0.00129046524418708, 0.0013051343064907255, 0.001380198121983178, 0.001274610325523025, 0.0012989232853549172, 0.001286599713814806, 0.0013519915923172114, 0.0012768367566738505, 0.0012735806129948826, 0.0013721674889782254, 0.0014001144267314551, 0.001282128367611036, 0.0013770696930397227, 0.0014875657970503885, 0.0013746638967636594, 0.0013736321006388385, 0.001497349369206599, 0.0013554556917741286, 0.0013484028785736586, 0.0012861008361476113, 0.0012858511015240634, 0.018289318754888918, 0.017846166367680594, 0.004433400612989706, 0.0015154781832112645, 0.0013273848976217667, 0.0014167399601820781, 0.0013884888173138, 0.0035008424061483572, 0.0012945458768125698, 0.0012985458573782627, 0.0013450961013571645, 0.001313146122978354, 0.0014003460005647978, 0.010570209223910101, 0.03280018626626733, 0.010331558246564652, 0.0023574176320464027, 0.0013532620612341836, 0.0014212225297732012, 0.0013732908778272721, 0.0013238226933100698, 0.0013143031225938881, 0.0013124402232316075, 0.0013036474286179458, 0.025135174305748423, 0.028211938654433708, 0.0016022588349688723, 0.0015055070802265284, 0.0014506563881640227, 0.0014839738141745329, 0.0015762898362982943, 0.0014696445919535294, 0.001450672182634625, 0.006515221857959975, 0.006348397406958499, 0.004983317959882623, 0.0014351340609469584, 0.0014318170819469557, 0.0013146377334903394, 0.001358161041304013, 0.0012899511208643718, 0.0013006438562950613, 0.0013033194082542037, 0.0013143693058922583, 0.0014082528579485963, 0.00134349030642105, 0.0013021748151858241, 0.0014007811230246205, 0.0013916468371314053, 0.0013213718371769907, 0.0013112102240818192, 0.0013133279375769958, 0.0013009996927932513, 0.0014264918364850537, 0.0012937414493145688, 0.0013059395726542084, 0.001371491225246264, 0.0013446830811777285, 0.0013385692050642505, 0.007279524591523318, 0.005662821081219887, 0.001423369204549461, 0.0013529813672624985, 0.0014378834059652016, 0.0014293530613792185, 0.0013434242039006583, 0.0013568319180714233, 0.0013790988976287904, 0.0013877123461237975, 0.0013762308968877305, 0.0014816423455177217, 0.026283005898704335, 0.008656497612329466, 0.001446021183831047, 0.001430715225180801, 0.00141060351137528, 0.0013035969590121995, 0.0012886819800323978, 0.0013406381215329986, 0.001370257550698458, 0.0013864169595763087, 0.0013709752440300522, 0.0014125368990270154, 0.0013376099380607508, 0.0014422578779904514, 0.03405722659271287, 0.001625842735532443, 0.0014011430606360035, 0.0014860659590637197, 0.0015121617957911625, 0.001403359838818409, 0.0013849250188249411, 0.0014148338350980561, 0.0013251336723832147, 0.0013233642236386634, 0.001398405692140971, 0.0014323389789621746, 0.00133603542796051, 0.0013195189186466895, 0.0013082224272229538, 0.0013142731420847835, 0.0014097332260666453, 0.0013385751422456636, 0.0014037983872148456, 0.0012989598754033142, 0.0014328862854032492, 0.0012907392240832656, 0.001312645000158524, 0.0013657464902391847, 0.001293956062148268, 0.001466945633387231, 0.001281031673508031, 0.0012908906115180984, 0.001293537938701255, 0.0012938064066883252, 0.001375246082660647, 0.0012998353067443383, 0.0012964004489156057, 0.0014156813267618418, 0.0012963851438645198, 0.0013579300614263937, 0.0013266510819560106, 0.0012890682864592088, 0.0014197465307934551, 0.0016148169596242358, 0.01192837965209037, 0.0027151925712633803, 0.0013715174068145606, 0.0014978512261557032, 0.0013692885927133718, 0.0013601318149048152, 0.0014778827958531221, 0.0013753455916267572, 0.001359780366551511, 0.0013555329582863012, 0.0013638661214511614, 0.0013252116730246618, 0.001290369897662681, 0.001353919304128052, 0.0012766169597945955, 0.0012773603690331991, 0.0013018637772041317, 0.0012834334095027677, 0.0012685977544977653, 0.0013950324702855883, 0.0013054651635870033, 0.02756153218381639, 0.010551031693169961, 0.0014079892033787103, 0.0014216560010361125, 0.0015223824908500727, 0.0015236171014720993, 0.001434276980461971, 0.0014070755711813666, 0.0013046097960703227, 0.001307486062299232, 0.0013124798567091323, 0.001318137855173982, 0.0013240907550314252, 0.001340329490259898, 0.00732577628484566, 0.024783396285635476, 0.001491296000550596, 0.0013381616730357008, 0.0013949036935572417, 0.001685468755586415, 0.00139145040884614, 0.001422791183470007, 0.0033597848568187685, 0.0013552017139308915, 0.0013844733250004296, 0.0015893526712660582, 0.0014907769378920902, 0.0015037013692971394, 0.001522041325058256, 0.0015338694068546199, 0.0014114210605431273, 0.0013789603063760667, 0.0014084447341571962, 0.0015210139384607272, 0.0013910808567223804, 0.0013207190204411745, 0.0013659720389856671, 0.001403158632753303, 0.001623030735312828, 0.0013667747772735904, 0.0013013076327018896, 0.001295889551960388, 0.0013004327145386108, 0.0012898833267580793, 0.0013683520409525658, 0.0012962233075605972, 0.0012981558565468509, 0.00140488995904369, 0.0013972633062120602, 0.001413959755600259, 0.0014206427125716392, 0.0014041814078785935, 0.0013877264086195097, 0.0017389599804063232, 0.001339657469747626, 0.001361790795012244, 0.0014541530411462395, 0.01903274679813078, 0.0039075823258418515, 0.00454914573656053, 0.0013949926111048885, 0.0013798098151134898, 0.0013638736338031534, 0.001411550122370221, 0.001291618898643979, 0.0013020568579549389, 0.001305980878711051, 0.0012853933469753485, 0.0013227557350064115, 0.02426644638940996, 0.008352298978526069, 0.001480811083100128, 0.0015035321845730044, 0.0013566511440832093, 0.0014597473271685292, 0.0013531608179173603, 0.0014686219593776123, 0.0013615540418849916, 0.0013617584481835365, 0.0014889651020912795, 0.0013532444896480562, 0.001366005345646824, 0.02528992869264009, 0.008220793448901755, 0.0015066213886804727, 0.0013156467145879049, 0.0014050459199375948, 0.0014188670415469274, 0.0013025498364538867, 0.001336201918501483, 0.0013125165489179138, 0.0014178792456621115, 0.0013366123656647242, 0.001379189347582204, 0.0013025858159158, 0.001305773837624916, 0.0013983486129959322, 0.0013935512041064854, 0.0013090453885153544, 0.00139763114299169, 0.0014821142243335442, 0.0013246040206820685, 0.0013598774284200401, 0.001316234407642362, 0.0014093018977959849, 0.001561839062226366, 0.0015514942446761594, 0.0013331843692125107, 0.01793651422248127, 0.005532867674316678, 0.0014927412041139845, 0.0013263241425916857, 0.0013616271221972241, 0.0013555314282563571, 0.0013351755321253927, 0.0012762247351929545, 0.001315462631078399, 0.001322415999459977, 0.0014397158163922783, 0.0014366240820334274, 0.0013300748779533468, 0.001388855387779827, 0.001398302305831897, 0.00143767401994187, 0.0013127655921769993, 0.001326588692846803, 0.0013178787550565843, 0.0013367153267014999, 0.0013018952639849515, 0.0013176277968842461, 0.0014261699386169107, 0.0013156397344202412, 0.0013064080207817713, 0.0014412961021179752, 0.0014003733058973234, 0.001378749592267737, 0.0014465011024315441, 0.0013614194491421993, 0.001374001674145004, 0.0014997019803113475, 0.0013939515112576131, 0.001577568592085522, 0.0013422597750869332, 0.0013605495700992796, 0.0013815960395434986, 0.0014785748991963206, 0.039993348654967786, 0.019633308939198603, 0.0014379819574746855, 0.001350696509875053, 0.001358802183246126, 0.0013472526951940084, 0.0013638210592183228, 0.0014010500614246239, 0.001407796980299968, 0.0013345873876645857, 0.0013348511230124502, 0.0013462069791228492, 0.0013271307534709269, 0.0014527057539861724, 0.001455793938865619, 0.0015861059596039811, 0.001501134672791374, 0.0015352057953536206, 0.001537193981360416, 0.008335288428720467, 0.0014167305709299992, 0.0014127276538472091, 0.001392075979645003, 0.0014084165307636164, 0.0012992471432769482, 0.001309240510098028, 0.0013100398153218689, 0.0015515896315895477, 0.001373727102668918, 0.001373432633676091, 0.0014991842243554337, 0.0014446101849898696, 0.0015157233253691573, 0.001503389428502747, 0.09191093628760427, 0.0014470084499073575, 0.0015261628170858842, 0.0014185957344514982, 0.0015643971843873055, 0.0015178571611984956, 0.0014404625921719233, 0.0014023579162906628, 0.0014218015296916877, 0.04759503402081983, 0.002385081428730366, 0.0016063884713173825, 0.0014398176322824188, 0.0014691863669918813, 0.0019134994292137573, 0.001472791918192287, 0.001444999204606426, 0.0014097161225176283, 0.0019676482034086877, 0.0013485280392995598, 0.0013372661232264067, 0.007921255367085794, 0.012029029305416102, 0.009013491571044587, 0.0013555303876459294, 0.0013426502225730493, 0.0013258979199644253, 0.00131644036772908, 0.0013096191022279008, 0.0013856830393743453, 0.0013110038767359695, 0.0013109186749753294, 0.0014141549983499, 0.0013059926319069096, 0.0013165989372765227, 0.0013085480408781037, 0.0013282328573226625, 0.0013210388774774512, 0.0014292616325867723, 0.0013259221628612401, 0.0013273542043657936, 0.0013282249600873614, 0.0013307496948091655, 0.0013302077553520094, 0.013530428122196878, 0.0013787393667259995, 0.0014607434693191732, 0.001484148712752729, 0.0013787202864923344, 0.001381921123390143, 0.0014603668990145837, 0.0014523862438200383, 0.0013705273253881202, 0.001397567164456966, 0.0014785989591547726, 0.001371902019279648, 0.00136785336760614, 0.026149959163265114, 0.00865824377563383, 0.0015199573862613464, 0.0014988498581687407, 0.0013866315510276022, 0.0013748861431163184, 0.0013167128769908936, 0.0013322415904198982, 0.0013166526522563429, 0.0013166028146194865, 0.001297869082844379, 0.0013932081020190095, 0.0014746377734011229, 0.0013698355118953148, 0.0013910936125155007, 0.0012970709180155275, 0.0012943433687965177, 0.0013080516316908964, 0.0012965471417243993, 0.0013086006534761007, 0.0013204459172236373, 0.0013074602442318384, 0.0013098887961395845, 0.0013160651635226546, 0.0013134317966748256, 0.0013400633078144522, 0.0013718155702119883, 0.0012538474081654329, 0.0012542316951427836, 0.0013535871830939942, 0.0013593349163895662, 0.001286619226448238, 0.0012732801854382365, 0.0013791086123686057, 0.0012702139199008138, 0.0012688640414794184, 0.0012825658991552737, 0.00127101328689605, 0.0013779611445545238, 0.0013755255910967077, 0.0012755444895818221, 0.0012839054902635363, 0.001373519633934662, 0.003067376918862669, 0.0013425724903995894, 0.00137341681544726, 0.0014169269992152648, 0.0013785539598831413, 0.001304934388121628, 0.001445917776139567, 0.0013615722050510195, 0.0013523636129209582, 0.001360454142796902, 0.0013351361837466152, 0.0013562070623952516, 0.0013751768986979614, 0.0030188011646993004, 0.0013557188973134877, 0.001344124019164972, 0.0012762680797058406, 0.0012907866122467177, 0.0012697873266451821, 0.0013100544907798876, 0.0013377398789423157, 0.0012775329606873648, 0.0013762814710296843, 0.0013854503869174086, 0.0013757864897120365, 0.00135665702186905, 0.0014584804291133673, 0.0014526627943565954, 0.001480882937487747, 0.001438605530206494, 0.0013373236941667845, 0.0013390753266154503, 0.001448548467810817, 0.0013327282658607072, 0.0013457741231980677, 0.0012408386326718088, 0.0013620827741426776, 0.0012731311429406004, 0.0012547735704527218, 0.0012499803049984028, 0.0012594771818542968, 0.001266993938622122, 0.0013513762446842631, 0.00145417683738835, 0.0013511334902283792, 0.0013452854710726105, 0.0013589504488496756, 0.013260315428963122, 0.020116099386894126, 0.0014007816124441369, 0.0014706660817586342, 0.0013681407946599076, 0.001381105327579592, 0.001267875245373164, 0.0012836069776201431, 0.0013599140826156552, 0.0012681525728038075, 0.0012913649588139082, 0.0012623837754624535, 0.0012774129388663843, 0.0012876406734885306, 0.001259968530538739, 0.0012964381033326595, 0.0012828746753535708, 0.0013722291011933464, 0.0013763791411088742, 0.013905467244568375, 0.018224327245309035, 0.0014083622051973123, 0.0014958694691256601, 0.0014874695310825292, 0.0015003275914991997, 0.0014940703463531574, 0.0015245083272837255, 0.0015028720004103926, 0.001499779836027598, 0.001489792389757171, 0.0015033415105307894, 0.001494040021349733, 0.0015075465911353122, 0.0013978934908589842, 0.001384587689244654, 0.001379124958475586, 0.09445382854270672, 0.013896577457974976, 0.001478724145272281, 0.0014719732086329411, 0.0014716422714021367, 0.0015027922515097696, 0.0013827563549663562, 0.0013864365610061213, 0.001408016146645726, 0.0013760479390233134, 0.0016182021063286811, 0.0015255246859548304, 0.001487388229482652, 0.0014686846649662282, 0.0014524910414669041, 0.0014916473131355208, 0.05727921395737212, 0.0014729544782312587, 0.0014468408771790564, 0.0014131063338330325, 0.001416798375430517, 0.0015165846877304527, 0.001528637020480043, 0.001869553833482011, 0.0015988887074248244, 0.012442393082892522, 0.001646138625801541, 0.00151393535391738, 0.0015066317913200085, 0.0015017491653755617, 0.0015052752496558242, 0.0014966842500143684, 0.0014971748751122504, 0.0014930238758097403, 0.0015027101859838392, 0.0014937074593035504, 0.0015023200006301825, 0.0015116506474441849, 0.001497813209425658, 0.0015045167286492263, 0.0033600409369682893, 0.0014988186473298508, 0.001485921520118912, 0.0015060120616302204, 0.0015027860208647326, 0.0015061422697423648, 0.0015106027300741214, 0.0015011948974764284, 0.001557387021117999, 0.0015092992083130714, 0.0015182043134700507, 0.0015202036690122138, 0.04948654610295004, 0.01595815762508816, 0.0015708289793110453, 0.0015418872305114444, 0.0015252746670739725, 0.0015293643324791144, 0.001532283519433501, 0.0014713926648255438, 0.001460919542296324, 0.0015413398553694908, 0.0015729278966318816, 0.0015419375437583465, 0.0015495682091568597, 0.0015358243545051664, 0.0015408008936598587, 0.0016014714371218968, 0.0015602307709438417, 0.0015499073754957255, 0.001551953542123859, 0.0015503968146125164, 0.0015687488970191528, 0.0015625027711697232, 0.05154687279233864, 0.001602089854713995, 0.0015301528328564018, 0.0015060903533594683, 0.0014125865006159681, 0.0014364831246590863, 0.001429316772070403, 0.0015073684383726989, 0.0015145394354476593, 0.0015135271654192668, 0.0015212914173995766, 0.0014221113960957155, 0.0014197992301584843, 0.001408909437789892, 0.001421015229425393, 0.0015036699987831526, 0.001504802249352603, 0.0014988110002983983, 0.0015203578125995894, 0.001535928813003314, 0.0015451116244851921, 0.001512413543726628, 0.001513634958731321, 0.011036795918092443, 0.0015660180409516518, 0.001466386563455065, 0.0014386173327996705, 0.0014406056240356218, 0.0014413751875205587, 0.0014535953329565625, 0.0014707295000941183, 0.001463792936798806, 0.0014439155420404859, 0.001445438834101272, 0.0014392886247757513, 0.0014647853749920614, 0.00145906956216398, 0.0014604217091497655, 0.001477622853902479, 0.001470822457728597, 0.02355726670793956, 0.0017326525849057361, 0.001439069354091771, 0.001417944957211148, 0.0014010214363224804, 0.0013811654377301845, 0.0013998737510216113, 0.0014079061656957492, 0.0014006544588482939, 0.001391837353973339, 0.0014240610204675856, 0.0015268348967462468, 0.0014423343333570908, 0.0014626501239642191, 0.0015091614574582006, 0.009612991896574385, 0.029379949354430817, 0.001598450769961346, 0.0014461699368742604, 0.001441020979351985, 0.0014556445627628516, 0.0014605115623756622, 0.0015207424391216289, 0.0014658511257342373, 0.001518265458192521, 0.0014761122923421983, 0.001467895541281905, 0.00146283258315331, 0.001485486999930193, 0.0014251256652642041, 0.001592352021058711, 0.0015610134796588682, 0.001429284374656466, 0.0332321595196845, 0.007438815480175738, 0.0014683760624999802, 0.0014670715609099716, 0.001441684292027882, 0.0014466408538282849, 0.0014340102910258186, 0.0014528320001166624, 0.0015320056263590232, 0.001525703708466608, 0.001529043334206411, 0.0015386421267370072, 0.0015508915845809195, 0.0015315854592093576, 0.001547036166205847, 0.006350494208163582, 0.0015477833949262276, 0.0014586648758267984, 0.0014610408131071988, 0.0014017928333487362, 0.0015987488125877765, 0.0016944024367451978, 0.007967168002020722, 0.001445491667254828, 0.0014398767501309824, 0.0014768679369202193, 0.0014705489787350718, 0.0014565069359377958, 0.02080237151919088, 0.008388407521124464, 0.0014675964363656628, 0.0014677641884190962, 0.0014463420423756663, 0.0014756267507133696, 0.0014603715001915891, 0.02918340418545995, 0.0016078583115207341, 0.0015026505837643829, 0.0014903134797350504, 0.001477541894322106, 0.0014731401679455303, 0.0014525191460658486, 0.0014673576661152765, 0.0014385750400833786, 0.0014482204569503665, 0.0014236078544248205, 0.0014187040214892477, 0.0014185989372587453, 0.0014243908954085782, 0.0014218831250521664, 0.0014286204386735335, 0.0014482830641403173, 0.0014295409152206655, 0.0015394040844209182, 0.0015332631446653977, 0.0032019944992498495, 0.0015412234982553248, 0.0015401193328822653, 0.0015436149575786355, 0.0015903003319787483, 0.0018858344143761012, 0.0014620043754500027, 0.001470740561975011, 0.0014591591243515722, 0.0015485789166026127, 0.0015878404374234378, 0.0014432386888074689, 0.0014517145415690418, 0.0014564543962478638, 0.0014689659583382308, 0.001468532624130603, 0.0014500606024133351, 0.003481674000795465, 0.0021926843328401446, 0.0015394578974034327, 0.0014442282481468283, 0.001437911875351953, 0.0014249611873916972, 0.0016753110418600652, 0.001527180499882282, 0.0015020814583598014, 0.001515295227970152, 0.0014865032280795276, 0.0014973865836509503, 0.0017483858537161723, 0.0014998165643191896, 0.0014924112062241572, 0.0014816117691225372, 0.0014808107080170885, 0.0014872752290102653, 0.001488929228798952, 0.0014884588114606838, 0.0015127318329177797, 0.0017542131875719253, 0.0015216001653849769, 0.0015030149782736164, 0.0015184945417180036, 0.002924309458952242, 0.0015305318544657591, 0.0015427368125529028, 0.0015410282285301946, 0.0015077420830493793, 0.0014392789768559548, 0.0015131101657364827, 0.0015024762712225008, 0.0014965739367956605, 0.0015021356860719, 0.0015315197912665706, 0.0014919534805812873, 0.001424919665926912, 0.0014232581258208181, 0.001446201063420934, 0.0014331939989157643, 0.0014916554791852832, 0.0014875697500732106, 0.0015229595640751843, 0.0014888764781062491, 0.0015086329804034904, 0.0014962906037302066, 0.0014987733763215754, 0.0014128757708628352, 0.0014236157512641512, 0.001421100084068409, 0.0014216006456990726, 0.0014209683940862305, 0.0014981588731946733, 0.0014935533763491549, 0.001498313312670992, 0.0014836162711920526, 0.0015223254570931506, 0.001498359832718658, 0.0015071175609288427, 0.0015135574988865603, 0.0015044268560207759, 0.0014933253114577383, 0.0015009515846031718, 0.0014938022917097744, 0.0015041589989171673]
[756.6297125345158, 744.7807966129961, 734.5918355193161, 779.8533391503388, 738.5060430408937, 729.8710788747167, 735.3097969364455, 744.7093040155279, 790.734246793049, 787.9785222950271, 42.33176639041654, 68.49694358609813, 108.389473064382, 234.95049762970788, 785.0734402766524, 773.4766339393815, 780.8883411705565, 792.3791633292226, 788.7842984107718, 746.6568367980623, 780.4794139101797, 790.6650029044307, 733.3035130514559, 213.67128166407898, 737.1740111093325, 739.5220444597938, 705.7340588777126, 733.8317827616966, 796.8813777498767, 800.7383792221816, 798.5222583565998, 721.209429682479, 753.7763883030507, 773.9807015394932, 780.2847451804495, 758.2983333131135, 772.7468533026974, 754.5833015680823, 760.552624712011, 758.4063103979273, 756.6472042278729, 715.2247329195744, 739.4724916940651, 757.4310983674392, 70.51686697434498, 140.48605846423024, 124.15683729098303, 667.3461612381644, 720.0277900717102, 734.6473872658672, 683.8588414073779, 721.3980151238301, 728.7856266582203, 729.4982518850592, 564.3953819831581, 691.8406586734834, 52.72906780385746, 39.42215139079385, 143.6471065038503, 717.1565358951557, 772.9842693757877, 788.1715590223492, 749.1247769071593, 759.425925791475, 799.3840891708527, 750.0456034406868, 751.3167711768194, 803.4092369833952, 57.00974938975235, 109.1820679625819, 715.3045643875555, 727.4811948377903, 708.2223610215432, 721.0392033187098, 672.7271424656645, 690.2140951147978, 30.158474561864743, 722.066081707315, 784.2701021234363, 785.3996187445556, 776.5473079059701, 716.0569112777279, 714.9177498165222, 730.7083747231975, 769.2362034514707, 771.3741480702721, 727.205290172229, 717.0943090223927, 759.2357467816676, 756.7357319439576, 754.5251579830785, 57.1953510683074, 94.30340442950202, 721.7234426273162, 726.2960894906853, 669.5437191087473, 644.7757152065668, 685.1154559331909, 744.7179845461853, 739.5394674597081, 746.9698364163354, 742.3755937231102, 744.083278421911, 748.1426937732015, 744.3655922809108, 744.6330154942767, 739.3105573794682, 741.6797183032778, 680.4031552479704, 735.437034174054, 182.45390746874318, 710.3886110446191, 678.06041568012, 753.6127075650485, 790.9659797456333, 789.0393229741735, 793.6197767413724, 786.6715738580551, 790.4420394106761, 791.649065516906, 782.9865644781212, 747.5731293490873, 794.5418026583361, 787.5599211729051, 796.1856023914615, 725.8206443093617, 731.8530791087527, 667.9065190848025, 664.7215465876607, 728.942906916214, 729.928791051762, 736.7582807642002, 658.1598124455264, 730.7708059008618, 732.9722647441725, 672.8045393717126, 714.3645857815674, 669.9191461675964, 732.1140026946091, 723.6687648125762, 727.6690656422588, 727.0120505906962, 721.9290796623101, 722.3759466608409, 724.5203450790111, 724.5121826392019, 666.8670082083904, 663.07099109387, 732.0514729517904, 722.1136895247118, 661.9973424688058, 710.2654760370515, 716.2212866973998, 728.360495572357, 676.5550472932312, 44.34439981958356, 702.6520396116072, 738.8712682457729, 694.7698916977032, 675.4145723404225, 739.4554966508855, 737.1685434142371, 738.827473868389, 728.1070867322936, 733.4045769310477, 667.0465654092276, 715.2139590600331, 46.413712639337795, 160.69525657931024, 116.43419554831627, 755.9508726001744, 753.9063621429924, 683.2579355325458, 677.9008954976189, 761.9483024123809, 755.1066830796116, 751.6598530095652, 757.1706451000365, 756.8377351526917, 763.8833101256481, 760.7187802175927, 754.3845163523763, 702.8362756469754, 688.3819131571547, 694.0542351112533, 762.7135680764227, 759.3015358509888, 31.729346693546816, 704.2372242533518, 777.0410689685349, 701.6745965057366, 765.1688638120431, 761.3673374575442, 721.1201565146415, 687.0247138811471, 43.11900749308284, 417.8033724476276, 693.2244243486246, 756.8972184193951, 697.6075943031655, 761.2675642368552, 762.9903453641342, 771.5600930939956, 761.9259677362091, 698.7875141739595, 761.8775733042684, 706.5401196043498, 724.8672547055185, 716.4197465045565, 662.4638882866635, 717.5870644405489, 66.28306600207553, 38.65750720234285, 369.7766790559621, 727.3472484245458, 691.313126767926, 741.612704523712, 752.0974576775291, 757.948322599474, 804.4147460767191, 800.103288659796, 88.06131540817239, 22.97893111782907, 207.02559176390275, 734.8972130196848, 776.7371345605126, 749.7417333865178, 753.6134604786138, 795.3511363742182, 787.5192414431775, 738.4532889368838, 737.0413839639541, 746.5207970392821, 57.602475857592964, 104.40523864753689, 32.254302994375415, 126.87581982280292, 798.3383355747077, 803.9352405531089, 796.1874382846472, 814.6493233948211, 823.8408655293294, 754.9552650363362, 812.9733286627556, 770.4652305656037, 807.7157748842322, 755.7007829922993, 800.9836335233814, 753.973540330343, 25.615718074540897, 560.5752205984705, 796.1321106815343, 798.5402544312316, 793.6681992399195, 803.1539895791832, 792.1112693576712, 783.6844451851148, 782.7343626601962, 743.4570621821881, 766.3115073315201, 726.0276350049016, 783.0038933286648, 779.1052862832244, 731.8890318826773, 763.2814638268284, 777.9348469993292, 772.0490158945084, 775.2682834066685, 778.763283827046, 30.53157719019077, 207.37991303024958, 722.171106973872, 728.9898438892326, 676.7675560433798, 725.1143568762335, 732.7941943664117, 727.1733237215552, 694.2484884207902, 659.2017845652682, 724.3848000803629, 722.2932077865115, 675.2715835541946, 662.9519084194338, 715.9154654270238, 705.2976307080739, 725.5785402645613, 116.09851786189492, 714.3276779181116, 707.9412390740898, 757.969402333487, 719.201214011188, 752.6397420502593, 750.0869496199999, 750.9484518659893, 698.8668977012377, 748.230258699751, 749.8165370855027, 761.4999893692047, 677.6896156251959, 705.7442638362601, 660.969273143228, 713.7707891827624, 297.4041234190625, 654.0799001814041, 69.28159136089431, 97.87383397036555, 263.9254044300267, 724.124721462499, 728.8557427464543, 711.5009140585311, 732.6569926045328, 712.265455496271, 722.3099139459456, 718.5036648532256, 712.9196107734502, 717.0135743588447, 67.31464596744408, 294.1447787515976, 686.5620676278394, 740.5668221517531, 749.3632664984076, 699.1306105019216, 747.2386900561561, 676.6389446341623, 753.7511232156481, 753.3406157136193, 748.6761759385939, 753.8214045000889, 742.3233313948466, 752.5511528295007, 747.2124711635942, 731.5387357576474, 744.7557224352371, 700.370315546477, 25.79431055301347, 678.0541108537586, 773.8057965259048, 783.4321815421417, 774.3616378731958, 783.8289250688722, 775.9496162607641, 779.295697708625, 782.4735255416326, 771.6788083597107, 760.9759489849671, 772.7499219487445, 752.7713355269393, 772.701606725633, 778.6001408494328, 759.3580657031455, 28.55135751745402, 180.39441472181454, 729.7895498427821, 688.3861384245178, 746.2539269458596, 763.3925179658055, 686.0332933933338, 709.0048994193319, 757.7188785928676, 700.9741544992575, 685.413739062535, 748.604711131187, 746.7487547300643, 40.83112712216544, 705.4919327277421, 747.5757769231673, 322.3307380746783, 778.2973848672208, 761.9454458510994, 744.6305718323193, 706.1204912137063, 760.1705507079437, 763.3596237676069, 133.58213088857687, 243.3200489945827, 253.1217832800734, 673.8172895424225, 750.5253673140056, 749.3298973358774, 742.8250073527282, 746.4969136229543, 737.7543135052522, 679.6017912451067, 760.4291774346175, 700.7681323782209, 704.2141482091504, 22.11019078810698, 689.0104217503521, 726.7580267531041, 778.7717245388668, 729.7703563120363, 790.1744610549467, 793.7917843495694, 787.2587474004655, 792.1130253907816, 717.6582370465594, 743.9620789861244, 716.908365989694, 780.4121265023666, 770.4441523788071, 37.15896622196512, 704.1382406384822, 748.4771203956266, 753.556359357911, 696.7185262768273, 744.3768226269216, 747.2879093386978, 25.71974188674274, 143.69145735597323, 728.2251542537728, 761.2771913400408, 774.8465701684019, 686.6225042160208, 725.1925243647445, 755.1021680208387, 757.2003300037685, 708.8397068530156, 708.0936931757255, 80.71721175736738, 570.3707288201745, 646.1587575169165, 754.0823318808958, 763.6364545950328, 752.207488280331, 758.5171846700655, 751.8658431475548, 711.0169533233301, 750.0784068020154, 739.0890578683254, 21.889085715119432, 742.6378517527943, 707.6525853658056, 726.1327611690467, 779.5195337107587, 781.6595243094156, 776.8440610038023, 777.0941449681892, 723.567395799038, 781.6273420366068, 785.9797382773794, 771.936107192138, 786.6693390340279, 756.3044002360804, 782.236451520517, 677.9058195898026, 660.4362020910187, 28.280398226501497, 725.6358645513241, 731.8515928178055, 751.4408498539195, 752.8107961534419, 733.459445675356, 756.0921423930068, 745.0720585330489, 711.0066769831074, 88.01622096077871, 101.79259503494127, 707.263024864646, 734.3074194764357, 674.5788617710588, 729.0679387004516, 733.7944747311718, 732.0892508035746, 734.161346066324, 646.8163829069533, 647.9267563769303, 641.7302592684543, 696.0237358353003, 707.1974411378484, 645.2807030958246, 155.12114004715954, 375.7269935575912, 740.7833962388319, 742.8976700925405, 743.2122154965692, 732.1074179016924, 699.7111385169728, 741.9709776946571, 27.38865847486379, 222.36326198283396, 645.1045596346968, 782.223751753984, 730.2872016313975, 721.4851582237005, 777.0214282251068, 774.9143221830382, 766.2046695323047, 724.5336622854702, 784.5535062566349, 769.868406606292, 777.2425170490444, 739.6495700731952, 783.185473611358, 785.1878316900999, 728.7740075700546, 714.2273380715633, 779.9531039651512, 726.1796589195245, 672.2391721985302, 727.4505443507157, 727.9969647876804, 667.846810213618, 737.7592687601025, 741.6181142076771, 777.5440089094427, 777.6950214645719, 54.676722156897725, 56.0344434427665, 225.56048669954063, 659.8577340658394, 753.3609895605022, 705.8458348781811, 720.2074568627936, 285.64553441301706, 772.4716581402271, 770.092172192501, 743.4413042986504, 761.5298728003662, 714.1092270029498, 94.60550674228593, 30.487631743373033, 96.79082052627541, 424.1929755704467, 738.9551725761038, 703.6195803619719, 728.1778508440486, 755.3881687128451, 760.8594872896715, 761.9394638315113, 767.0785659126749, 39.78488423576596, 35.44598661754296, 624.1188865214948, 664.2280286383864, 689.343119541643, 673.8663381039877, 634.4010961514325, 680.4366208504514, 689.3356141867001, 153.4867149271747, 157.52006938064346, 200.6695153811847, 696.7990149576411, 698.4132349086242, 760.6658279501975, 736.2897105632397, 775.2231722779688, 768.8499777706573, 767.2716247964879, 760.8211752336616, 710.0997483198445, 744.3298959587728, 767.9460455985682, 713.8874043653311, 718.5731130329696, 756.7892487677224, 762.6542118372014, 761.4244480665923, 768.639689570561, 701.0204856580527, 772.9519685172068, 765.7322137559452, 729.1333561543134, 743.669652721561, 747.0663423427559, 137.3716081905205, 176.59042827900535, 702.5584063528548, 739.1084786506045, 695.4666809919364, 699.6172093653859, 744.3665203414383, 737.0109640561686, 725.1111589744511, 720.6104368770891, 726.6222566732401, 674.9267142810877, 38.047398530215176, 115.52016124577891, 691.5528010112747, 698.9511136806619, 708.9164261508475, 767.1082638592146, 775.9866402220196, 745.913445200645, 729.7898117695265, 721.2837329295234, 729.4077733019077, 707.9461079486282, 747.6021009905076, 693.3572804562074, 29.362343914814467, 615.0656383580128, 713.7029958568845, 672.9176413071459, 661.3048965946136, 712.5756148486997, 722.0607515982807, 706.7967807899464, 754.6408493276975, 755.6498673135083, 715.1000640372048, 698.1587561937098, 748.4831457849316, 757.851960944682, 764.3960072774192, 760.876843616949, 709.3540689185153, 747.0630287682974, 712.353005322946, 769.8467203919673, 697.8920868926982, 774.7498343131549, 761.8205987751701, 732.2003074120065, 772.8237683277803, 681.6885215377501, 780.6208235753905, 774.6589765836096, 773.0735760282574, 772.9131613744571, 727.1425911392746, 769.3282332087689, 771.3665949718435, 706.3736598739702, 771.3757016830688, 736.4149512601424, 753.7776990507577, 775.7540934831181, 704.3510783865969, 619.2652325330406, 83.83368312935585, 368.2980023529969, 729.1194373701503, 667.6230472945842, 730.306237356734, 735.2228578448348, 676.6436437354563, 727.0899809386825, 735.412883285014, 737.7172158648397, 733.2097955010381, 754.596431917628, 774.971581258487, 738.5964561927992, 783.3203157201496, 782.8644321859414, 768.1295213141186, 779.1600192077152, 788.2719297385936, 716.8291930834283, 766.0104826177957, 36.2824531426878, 94.77746149197274, 710.2327188307478, 703.405042620151, 656.8651478917212, 656.3328798513832, 697.2154009457133, 710.693882035355, 766.5127174517221, 764.8265085453273, 761.9164552417332, 758.6459914452645, 755.2352406359537, 746.0852031287426, 136.50430495245024, 40.34959488500786, 670.557689171562, 747.2938585450886, 716.8953703533682, 593.3067561683017, 718.674552569394, 702.8438267104854, 297.63810559788516, 737.8975319470375, 722.2963288221459, 629.1869753510482, 670.791165721927, 665.0256629528909, 657.0123843133663, 651.9459841438639, 708.5057945891718, 725.1840356652604, 710.0030095241141, 657.4561709881531, 718.8654744025215, 757.1633212838556, 732.0794067955975, 712.6777946965143, 616.1312772719963, 731.6494397085495, 768.4577995778845, 771.670701787221, 768.974810322883, 775.2639167089197, 730.8060864979301, 771.4720096199559, 770.3235285322689, 711.799521067615, 715.6847213793731, 707.2337073522128, 703.9067537183968, 712.1586957277678, 720.6031345867272, 575.0563620022706, 746.4594663801538, 734.3271842214272, 687.6855266978968, 52.54102366864938, 255.9127144645787, 219.82149131060143, 716.8496750731612, 724.7375609643345, 733.2057569083624, 708.4410139973196, 774.2221804356235, 768.0156161311104, 765.7079948880712, 777.9719743789667, 755.9974782457873, 41.2091652791983, 119.72751485202103, 675.3055885470997, 665.1004948617012, 737.1091708884195, 685.0500640680735, 739.0104611062361, 680.9104232812849, 734.4548723277695, 734.3446272236534, 671.6074128234981, 738.9647677487119, 732.0615568502661, 39.541432170626145, 121.64275945086465, 663.7367606176219, 760.0824666013978, 711.7205109171201, 704.7876726417893, 767.7249438090136, 748.3898849071217, 761.8951553978014, 705.2786780393467, 748.1600692080085, 725.0636047567043, 767.7037380427308, 765.8294041324256, 715.1292536826859, 717.591142006998, 763.9154522626169, 715.4963632675333, 674.7118296160106, 754.9425974753401, 735.3603928567592, 759.7430930188181, 709.5711724818547, 640.2708346751955, 644.539935247216, 750.0838016805455, 55.75219285063872, 180.73810162530268, 669.9084859746665, 753.9635055168064, 734.4154531721758, 737.7180485489125, 748.9651929197315, 783.5610550588556, 760.1888311948574, 756.1916979289126, 694.5815199181856, 696.0763170450125, 751.8373713957745, 720.0173673938531, 715.1529364067425, 695.567970297212, 761.7506171392483, 753.8131490130844, 758.7951442142066, 748.1024418771467, 768.1109438397642, 758.9396659395538, 701.1787115424637, 760.0864992426417, 765.4576396443028, 693.819957280469, 714.0953028658495, 725.2948654405199, 691.3233583569461, 734.5274820556428, 727.8011510591988, 666.799146182626, 717.3850682207784, 633.8868591938782, 745.0122685344077, 734.9971085044918, 723.8005693259051, 676.3269148851032, 25.0041577820162, 50.93384936267489, 695.4190174653872, 740.3587650437518, 735.9422970686128, 742.2512521721079, 733.2340216048228, 713.7503701924642, 710.3296952568572, 749.2952572779178, 749.1472140677615, 742.8278232902729, 753.5052574018335, 688.3706471568904, 686.9104021543174, 630.474902351215, 666.1627488361792, 651.3784686239145, 650.5359844793306, 119.9718532299797, 705.8505128067926, 707.8505168896149, 718.3515947563528, 710.0172272600487, 769.6765047162698, 763.8016027514502, 763.3355782811121, 644.5003109330758, 727.9466191335748, 728.1026935579856, 667.0294309092965, 692.2282636454017, 659.7510134354158, 665.1636502432495, 10.88009806440049, 691.0809678160646, 655.2380839086617, 704.9224636126875, 639.2238556678616, 658.8235214507292, 694.2214295840923, 713.0847185182737, 703.3330455178552, 21.010595339895396, 419.2728969141834, 622.5144277709553, 694.5324029785537, 680.6488424252619, 522.6027166419864, 678.9825416936058, 692.0419034226179, 709.362675241373, 508.2209300766436, 741.5492825194876, 747.7943115670287, 126.24261605744658, 83.13222743166378, 110.94479781979854, 737.718614878595, 744.7956163025127, 754.2058743306804, 759.6242294856438, 763.5807986450548, 721.6657573087663, 762.7742508967391, 762.8238265953601, 707.1360644107931, 765.7011039487085, 759.5327412830593, 764.2057981523919, 752.8800349177583, 756.9799928292185, 699.6619633524576, 754.1920845806479, 753.3784100060899, 752.884511321205, 751.4561182322147, 751.762268695669, 73.90749139411815, 725.3002446536602, 684.5828997380918, 673.7869267462067, 725.3102821487787, 723.6303021020402, 684.7594263296252, 688.522081681123, 729.6461598945571, 715.5291176210172, 676.3159096038055, 728.915028877263, 731.0725138251371, 38.24097750044589, 115.49686355727432, 657.9131816713029, 667.1782330631678, 721.1721089563569, 727.3329540825823, 759.4670162908386, 750.6146086347734, 759.501754913343, 759.5305044893221, 770.4937371714132, 717.7678614923498, 678.132635714049, 730.0146560052253, 718.8588826827478, 770.9678677631324, 772.5925161032048, 764.4958163519256, 771.2793216835962, 764.1750730779863, 757.3199227292814, 764.8416113696229, 763.4235844654388, 759.8407949066468, 761.3642387306816, 746.2334011897758, 728.9609636413944, 797.5452144237792, 797.3008526834896, 738.77768088364, 735.6538759822557, 777.2307295302423, 785.3730949687408, 725.1060511343699, 787.2689665360352, 788.1065010196527, 779.6870325794738, 786.7738365207075, 725.7098677650207, 726.9948348999447, 783.9789267780402, 778.8735289189694, 728.0565747249957, 326.01145097316015, 744.8387384299602, 728.1110794280942, 705.7526609019583, 725.397792977773, 766.3220535090948, 691.6022587881063, 734.445074811533, 739.4461004759725, 735.0486639293412, 748.9872659984638, 737.3505327673637, 727.1791730553468, 331.2573254885467, 737.6160367622037, 743.9789675220916, 783.5344438219313, 774.7213912138582, 787.5334546313604, 763.3270272633398, 747.529482929559, 782.7586690694534, 726.5955555238537, 721.7869433960564, 726.8569705240443, 737.1059773252874, 685.6451276537974, 688.3910043575625, 675.2728218318567, 695.1175836620496, 747.762119494224, 746.7839785589415, 690.3462481385205, 750.3405049747156, 743.0667470582822, 805.9065648582941, 734.169772192751, 785.4650367677451, 796.9565374565551, 800.0126049996267, 793.9802438720843, 789.2697585337445, 739.9863686620013, 687.6742733682682, 740.1193199873775, 743.3366534484976, 735.8620035384513, 75.41298737251789, 49.711426691971496, 713.8871549399925, 679.9640057001873, 730.918925817558, 724.0577384148645, 788.7211329736765, 779.0546619293381, 735.340572454844, 788.5486505689622, 774.3744269772344, 792.1521326853765, 782.832214684964, 776.6141755143209, 793.6706161799285, 771.344191002542, 779.4993690435051, 728.7412860799695, 726.5439951337494, 71.9141602660357, 54.87171002470893, 710.0446151633978, 668.5075273208851, 672.2826781347477, 666.521102235247, 669.3125276469594, 655.9491884060338, 665.3926613357144, 666.7645316853018, 671.2344665440231, 665.1848518750254, 669.3261128952814, 663.3294160725833, 715.3620834055928, 722.2366685533211, 725.0974567999615, 10.587183340565769, 71.96016450986782, 676.2586539194351, 679.3601908887496, 679.5129627849232, 665.4279718273482, 723.1932049405269, 721.2735354254588, 710.2191280847663, 726.7188675924886, 617.9697802203237, 655.512171783767, 672.3194255395064, 680.8813517658637, 688.4724046146797, 670.3997595101403, 17.458340136165486, 678.9076069756154, 691.1610086312506, 707.6608292367588, 705.8167325298732, 659.3762999786618, 654.177536329695, 534.8869778932858, 625.4344003783749, 80.37039123727209, 607.4822523000322, 660.5301853955997, 663.7321778029573, 665.8901653192643, 664.3303277780236, 668.1435980838309, 667.9246470289753, 669.7816533293219, 665.4643119659764, 669.4751330131642, 665.637147598732, 661.5285097060915, 667.6399925618587, 664.665258257256, 297.6154215853941, 667.1921261331399, 672.9830522408574, 664.005306117884, 665.4307307334283, 663.9479019276555, 661.9874174005581, 666.1360238307776, 642.1011517626054, 662.5591496318943, 658.6728750061128, 657.8065955134632, 20.207512521072612, 62.6638753353256, 636.6065390763246, 648.5558607734885, 655.6196215586278, 653.8664324536654, 652.6207371659973, 679.6282351445359, 684.5003924228198, 648.7861820456718, 635.7570503653124, 648.5346984694218, 645.3410660406573, 651.1161234464172, 649.0131230549223, 624.4257479840926, 640.930828069147, 645.1998460102545, 644.3491849836518, 644.9961652236271, 637.4506473917801, 639.9988649308945, 19.399818957564946, 624.1847153938317, 653.5294896871558, 663.9707888503576, 707.9212491156776, 696.1446207294118, 699.6349721353047, 663.4078136063166, 660.2667296704793, 660.7083261191331, 657.3362529773241, 703.1797950184591, 704.3249346517646, 709.7688277031245, 703.7222256965983, 665.0395371386353, 664.5391448811434, 667.1955301908713, 657.7399028786167, 651.0718410475201, 647.20243130213, 661.1948194645049, 660.661273863658, 90.60600625592035, 638.5622475921867, 681.9484199608484, 695.1118808320734, 694.1525031664552, 693.7818887531923, 687.9493744424968, 679.9346854306015, 683.1567326639225, 692.5612827650837, 691.8314192255449, 694.787676902404, 682.6938724763139, 685.368282590226, 684.73372706996, 676.7626782158572, 679.8917127933361, 42.449746500639975, 577.1497464128993, 694.8935415493734, 705.245993445914, 713.7649532507401, 724.0262264623461, 714.3501328389164, 710.2746080423811, 713.9519627290965, 718.4747536378846, 702.217099988913, 654.9496622922652, 693.3205269214247, 683.6905037068613, 662.6196256590373, 104.02588608821699, 34.03681837352075, 625.6057545170329, 691.4816678884858, 693.952422850701, 686.9808918888646, 684.6916010534035, 657.5735471534507, 682.197518181872, 658.6463484392838, 677.4552350710835, 681.2473857143169, 683.6052269524794, 673.1799066885087, 701.692506404065, 628.0018405321755, 640.6094585541518, 699.6508306755636, 30.09133364949296, 134.43000470504685, 681.0244497567281, 681.6300081365738, 693.6331383574928, 691.2565737056802, 697.3450652747063, 688.3108301026547, 652.7391171379755, 655.4352555156592, 654.0037012875179, 649.9237104086682, 644.7903966609111, 652.9181861756671, 646.3972994584414, 157.46805952747687, 646.0852360078868, 685.5584285137335, 684.4435768178836, 713.3721732697868, 625.4891275768167, 590.1785657962783, 125.51511399613624, 691.8061325798764, 694.5038871618923, 677.108612761508, 680.0181527174798, 686.574142097122, 48.0714421948222, 119.21213859504411, 681.3862279990182, 681.3083517707861, 691.3993859692177, 677.6781455856402, 684.7572688653592, 34.26605044582942, 621.9453498077119, 665.4907074237034, 670.999768570691, 676.799760360635, 678.8220304892095, 688.459083454082, 681.4971040069786, 695.1323164497842, 690.5026062853578, 702.440631309968, 704.868658192902, 704.9208720911406, 702.05447340574, 703.2926844555608, 699.9759858738229, 690.472756852672, 699.5252737104338, 649.6020181576773, 652.2037678132731, 312.3053460067705, 648.8351631882116, 649.3003357918664, 647.8299494898861, 628.8120425377383, 530.2692497160915, 683.9924810021184, 679.9295714378964, 685.3262151544881, 645.7533350601686, 629.7862029655091, 692.8860816683679, 688.8406579706643, 686.5989093624988, 680.7509692949257, 680.9518451059387, 689.6263496406291, 287.21815993442465, 456.061998994957, 649.5793107993901, 692.4113285300693, 695.4529113651233, 701.7735001122647, 596.904082294903, 654.8014462449474, 665.7428559779644, 659.9374046334012, 672.7196962040504, 667.8302122634124, 571.9561262032133, 666.7482036071052, 670.0566143094225, 674.9406429136529, 675.3057595991263, 672.3705071491498, 671.623594095639, 671.8358561891681, 661.0556995228858, 570.0561408868091, 657.2028728368284, 665.329364281262, 658.5469835595286, 341.9610728743778, 653.3676493450414, 648.1987023730974, 648.9173796341046, 663.2434096271421, 694.7923342731362, 660.8904114481747, 665.5679155494034, 668.1928472849905, 665.718822388815, 652.9461817617112, 670.262185125494, 701.7939494501247, 702.6132377942914, 691.4667851471065, 697.742246169407, 670.3960894148178, 672.2373858105041, 656.6162514020843, 671.6473896289455, 662.8517425971595, 668.3193742626136, 667.2122789199059, 707.7763102903986, 702.4367348506883, 703.680206067641, 703.4324323257814, 703.7454204905529, 667.4859508508603, 669.5441996484934, 667.4171493659989, 674.0287360130677, 656.8897572727185, 667.3964278564365, 663.518245639508, 660.6950847494358, 664.7049645504269, 669.646454344118, 666.2440083064934, 669.4326321158746, 664.8233336501609]
Elapsed: 0.14982649018585037~0.3417932360635664
Time per graph: 0.0030716804674165476~0.007019868289073177
Speed: 654.0336102090666~192.68380866345
Total Time: 0.0759
best val loss: 0.33000293374061584 test_score: 0.9375

Testing...
Test loss: 0.6490 score: 0.8958 time: 0.07s
test Score 0.8958
Epoch Time List: [0.32660016184672713, 0.3218054490862414, 0.3938263200689107, 0.3281263851094991, 0.3208367241313681, 0.32987178303301334, 0.3189153008861467, 0.36945496092084795, 0.33579947997350246, 0.327330493950285, 1.4136506370268762, 8.064478895161301, 2.6484901640797034, 1.3466251919744536, 0.791385494871065, 0.3209166090236977, 0.32727601984515786, 0.3246773611754179, 0.32610511395614594, 0.3290600188774988, 0.3227558000944555, 0.3231145069003105, 0.33391408307943493, 5.192906555952504, 0.9185419549467042, 0.34168969094753265, 0.35484900791198015, 0.3255407069809735, 0.3241060159634799, 0.31414199201390147, 0.3251491420669481, 0.3334731929935515, 0.3453267910517752, 0.34065268689300865, 0.33006096887402236, 0.3515259630512446, 0.33186741010285914, 0.3408085670089349, 0.34503403794951737, 0.3449877310777083, 0.33649325696751475, 0.34866378805600107, 0.33900226501282305, 0.3393885479308665, 1.327899423893541, 4.540627957088873, 2.1585350779350847, 0.8389999868813902, 0.3466189770260826, 0.3471061891177669, 0.35166431579273194, 0.3432915919693187, 0.34366404998581856, 0.34301820409018546, 0.35968269198201597, 0.3674976989859715, 1.8839392440859228, 2.6761979589937255, 2.1825801201630384, 1.049961820943281, 0.3390514929778874, 0.32889488304499537, 0.3251790431095287, 0.32377091399393976, 0.3214923780178651, 0.3273049920098856, 0.3188480429816991, 0.3144057639874518, 3.796347885974683, 8.1209654341219, 0.4311568171251565, 0.35554768587462604, 0.3586442330852151, 0.3563088378868997, 0.3527742519509047, 0.352823346038349, 6.302943927934393, 0.8418394098989666, 0.3323720870539546, 0.3244832259370014, 0.33080992894247174, 0.3338668519863859, 0.36303490097634494, 0.3484336379915476, 0.34517451794818044, 0.3315365739399567, 0.3354697360191494, 0.33638126496225595, 0.3450286128791049, 0.3393976829247549, 0.34122289705555886, 2.209568023099564, 1.9420918080722913, 0.3601619660621509, 0.3522231130627915, 0.35023762797936797, 0.36253334302455187, 0.3541899989359081, 0.343826569034718, 0.3397510180948302, 0.3373864550376311, 0.33650068507995456, 0.33678696886636317, 0.34099529299419373, 0.33807966101448983, 0.33647707500495017, 0.33973958890419453, 0.3417512798914686, 0.3496207151329145, 0.35774509399197996, 4.269721527118236, 0.605136388912797, 0.34937198692932725, 0.3399470141157508, 0.3358042559120804, 0.33218001388013363, 0.3267733689863235, 0.33181365788914263, 0.32202182803303003, 0.3284684410318732, 0.3324559648754075, 0.32359825703315437, 0.3307339318562299, 0.3305399409728125, 0.31659840303473175, 0.33688851399347186, 0.3467356018954888, 0.34920039190910757, 0.3585623591206968, 0.3460348730441183, 0.34588989592157304, 0.3451710370136425, 0.35021798603702337, 0.34922195703256875, 0.3406694281147793, 0.34681219107005745, 0.3599408849840984, 0.35392186185345054, 0.351064958027564, 0.35175905807409436, 0.3570487340912223, 0.34816248703282326, 0.34551078104414046, 0.34836666577029973, 0.35718214488588274, 0.35485619690734893, 0.3528207989875227, 0.3526305948616937, 0.3530226150760427, 0.35003633110318333, 0.3575528039364144, 0.3586359379114583, 0.3625550731085241, 0.36115345906000584, 0.35672241600696, 3.611576601048, 0.5035285770427436, 0.3523877590196207, 0.3378527278546244, 0.3441723920404911, 0.347702061990276, 0.34493139397818595, 0.3487901670159772, 0.35009472689125687, 0.3514780189143494, 0.34908232605084777, 0.3529223409714177, 3.495209724875167, 2.932991707115434, 1.6781245010206476, 0.3964976300485432, 0.34539113205391914, 0.3405182260321453, 0.34823861613404006, 0.33536289096809924, 0.3384031009627506, 0.3369375340407714, 0.33931080193724483, 0.3380134040489793, 0.3297421848401427, 0.3396804379299283, 0.3436887818388641, 0.33531151490751654, 0.3429047710960731, 0.33829234493896365, 0.33800665894523263, 0.3318531708791852, 3.467263223021291, 3.7080046200426295, 0.3554401269648224, 0.3376922579482198, 0.339992014109157, 0.34097064984962344, 0.3384602880105376, 0.3579026000807062, 5.871244700974785, 0.8606928100343794, 0.46523384493775666, 0.3476053300546482, 0.3398985269013792, 0.3438338930718601, 0.33651381998788565, 0.32917851605452597, 0.34749383199959993, 0.34002957097254694, 0.3447587580885738, 0.3422429470811039, 0.3497083930997178, 0.3594329620245844, 0.35343203123193234, 0.35511494288221, 1.0240514650940895, 4.670388731057756, 6.5855670620221645, 0.6101160378893837, 0.34159460105001926, 0.34989704797044396, 0.3426584380213171, 0.3434694459429011, 0.3272646788973361, 0.3220152298454195, 1.1918389501515776, 4.934551640995778, 1.994765130104497, 0.5129701402038336, 0.3346327490871772, 0.33745102013926953, 0.3254424158949405, 0.3273548159049824, 0.32490866794250906, 0.32657713105436414, 0.4100297500845045, 0.3253509340574965, 1.11135126685258, 3.045126480050385, 2.2737302610184997, 6.8633634420111775, 1.0273161277873442, 0.3235800957772881, 0.3158875380177051, 0.3148463418474421, 0.3143965151393786, 0.31406658713240176, 0.31330789695493877, 0.3131876199040562, 0.3208164209499955, 0.3182946159504354, 0.32246626110281795, 0.31371166405733675, 6.4942224560072646, 0.9778928949963301, 0.340530876070261, 0.3218637170502916, 0.32595828908961266, 0.3231059020617977, 0.32194921385962516, 0.3294167050626129, 0.33398340397980064, 0.3373849631752819, 5.405792796169408, 0.3388105269987136, 0.32660316803958267, 0.3230359489098191, 0.32794292899779975, 0.33834538410883397, 0.3389446820365265, 0.3294994489988312, 0.3280781051144004, 0.324690421926789, 1.8701919849263504, 5.230003613978624, 1.0592427219962701, 0.34922083909623325, 0.3586196060059592, 0.35877331893425435, 0.36093812994658947, 0.3538528879871592, 0.35039843199774623, 0.3626616479596123, 0.3516011079773307, 0.35463362210430205, 0.3519493010826409, 0.36375516396947205, 0.36140821990557015, 0.35527547379024327, 0.3525257569272071, 5.250566111877561, 0.7177382279187441, 0.3452128949575126, 0.33449813595507294, 0.33041806309483945, 0.3331150838639587, 0.33936096099205315, 0.3396713499678299, 0.33612023014575243, 0.3392387378262356, 0.3418874980416149, 0.34329176996834576, 0.36144488712307066, 0.35467597807291895, 0.35601988492999226, 0.36267009703442454, 0.44991031393874437, 0.3576922760112211, 2.6252626040950418, 4.466619957820512, 1.198562851990573, 1.3507290340494365, 0.3479601979488507, 0.3456183950183913, 0.3525888390140608, 0.35178625595290214, 0.3484665510477498, 0.35804365994408727, 0.3548733899369836, 0.3628928699763492, 1.5401184358634055, 4.082686243928038, 0.6549057350493968, 0.35296264418866485, 0.3482668639626354, 0.34125926182605326, 0.3436299320310354, 0.34560395509470254, 0.3373750610044226, 0.34411184198688716, 0.33962860400788486, 0.34268513903953135, 0.34388854703865945, 0.3472541010705754, 0.35047832096461207, 0.34671594901010394, 0.34505815198644996, 0.34883884817827493, 9.41683678003028, 5.626108175958507, 0.33905997802503407, 0.3289446720154956, 0.33620957797393203, 0.3283659120788798, 0.33347051008604467, 0.3335947401355952, 0.32350848603527993, 0.32812175212893635, 0.3320037960074842, 0.3345562650356442, 0.3361553620779887, 0.330152023001574, 0.326488027931191, 0.3379890250507742, 2.879852144047618, 3.007539666024968, 0.5337783008581027, 0.3416271370369941, 0.34005118592176586, 0.3347165009472519, 0.33862673002295196, 0.3495998550206423, 0.3448487708810717, 0.3449689280241728, 0.34294732694979757, 0.34371254686266184, 0.33803529490251094, 1.482365301111713, 3.750160238938406, 0.35045210900716484, 0.4316035220399499, 0.34232158097438514, 0.3414191521005705, 0.34122967696748674, 0.3342339029768482, 0.4003024658886716, 0.3427941169356927, 4.515039331978187, 1.6124614039435983, 1.9824963500723243, 0.6434021468739957, 0.3441612448077649, 0.34584473608992994, 0.3388585631037131, 0.3385798790259287, 0.3448967100121081, 0.34037905698642135, 0.3423235999653116, 0.3445860930951312, 0.3491730659734458, 3.2536645989166573, 4.138125097961165, 0.33441937912721187, 0.33129694988019764, 0.32547800010070205, 0.3260989061091095, 0.32549515704158694, 0.3244816579390317, 0.3241365069989115, 0.3335193219827488, 0.3428842939902097, 0.33787925401702523, 0.3343506009550765, 0.34120277396868914, 3.913558497093618, 3.4354896809672937, 0.3435143759706989, 0.3483543648617342, 0.35639852890744805, 0.3403969219652936, 0.33924376603681594, 2.1800950500182807, 6.594326621037908, 0.5877257710089907, 0.34171896195039153, 0.33545227197464556, 0.33708227204624563, 0.3323687689844519, 0.33538133394904435, 0.337872851989232, 0.34282239293679595, 0.33430105703882873, 3.295809278031811, 1.3945835910271853, 0.37477053701877594, 0.3519174960674718, 0.32903035497292876, 0.3420789251103997, 0.33964370796456933, 0.33642329508438706, 0.34203627589158714, 0.3356620860286057, 0.34300554590299726, 4.77713236794807, 1.3247730829752982, 0.3503381269983947, 0.3519836670020595, 0.39078084006905556, 0.3283262699842453, 0.3284035489195958, 0.3387288320809603, 0.3257903358899057, 0.4013911240035668, 0.3241902479203418, 0.3314066178863868, 0.3204480019630864, 0.3344777339370921, 0.3267817080486566, 0.3375990450149402, 0.3550875049550086, 4.658120635780506, 0.7218838450498879, 0.34902359591796994, 0.3492825219873339, 0.3383541499497369, 0.3339744480326772, 0.34056374698411673, 0.33934305305592716, 0.3364981299964711, 1.693883687024936, 2.4150753038702533, 0.4321203799918294, 0.34963310800958425, 0.34672893583774567, 0.34956989996135235, 0.3571493459166959, 0.3454527418361977, 0.3458774311002344, 0.36309075192548335, 0.36762196000199765, 0.36443937523290515, 0.35973367013502866, 0.3594632151070982, 0.3636306420667097, 5.750273868092336, 1.1215815920149907, 0.35436634405050427, 0.3405627568718046, 0.3501396661158651, 0.35331501718610525, 0.36196594196371734, 0.34670986502896994, 2.7933471020078287, 4.992419317131862, 1.1086794820148498, 0.34589711914304644, 0.3339163080090657, 0.3327920079464093, 0.321545390994288, 0.32528840110171586, 0.3345257989130914, 0.3300728611648083, 0.32824940083082765, 0.3266681699315086, 0.332338452921249, 0.3277234077686444, 0.3269727770239115, 0.3267676739487797, 0.32745365193113685, 0.3462447130586952, 0.3259412889601663, 0.34047114406712353, 0.3574514329666272, 0.3578396779485047, 0.3541059630224481, 0.35656292201019824, 0.34251524799037725, 0.3397338291397318, 0.32449416595045477, 0.3360156511189416, 1.6813233100110665, 3.36711879784707, 2.523500338080339, 0.8005382849369198, 0.35590857500210404, 0.3384563549188897, 0.3456656220369041, 0.43783363595139235, 0.32992731407284737, 0.33930665196385235, 0.3375177870038897, 0.3423533661989495, 0.3854031590744853, 1.4449709189357236, 5.643933737068437, 2.4211430300492793, 1.1228751568123698, 0.3772408489603549, 0.3388865049928427, 0.3345609491225332, 0.33589228393975645, 0.3371099838986993, 0.3335982918506488, 0.33218543394468725, 2.3483585770009086, 7.871056988835335, 4.408263724995777, 0.3586547620361671, 0.3639823308913037, 0.36920692410785705, 0.3727742818882689, 0.3730593918589875, 0.36522316420450807, 2.4859518710291013, 1.6261422949610278, 1.3411738330032676, 0.3775158920325339, 0.3717413969570771, 0.33918873395305127, 0.3352950259577483, 0.32950668199919164, 0.3346514009172097, 0.32928934693336487, 0.3375539700500667, 0.33261917904019356, 0.33166412892751396, 0.33645523991435766, 0.33543912088498473, 0.3434740810189396, 0.3370042620226741, 0.33687332703266293, 0.3380462370114401, 0.33042639098130167, 0.3441724090371281, 0.33052753598894924, 0.3352270049508661, 0.3390299609163776, 0.33701852580998093, 0.344209388946183, 0.9124419610016048, 4.374978341045789, 0.7887218128889799, 0.3558546550339088, 0.34419020696077496, 0.33939995104447007, 0.33790872478857636, 0.34310714807361364, 0.3434861060231924, 0.3555824711220339, 0.34696317894849926, 0.3492243679938838, 3.2785092500271276, 5.015983817982487, 0.7261748821474612, 0.36315773194655776, 0.35803602694068104, 0.4249617501627654, 0.3289584858575836, 0.3381251699756831, 0.35511683695949614, 0.3550667241215706, 0.3558925752295181, 0.35068015195429325, 0.34320475615095347, 0.3431001699063927, 3.1588786608772352, 1.6356814940227196, 0.36198633385356516, 0.362522306968458, 0.35950047487858683, 0.36462862498592585, 0.35705109708942473, 0.34836762794293463, 0.34490367001853883, 0.33360660495236516, 0.345338886952959, 0.35736432508565485, 0.34724504698533565, 0.33301983296405524, 0.3330832280917093, 0.3349707762245089, 0.34541542804799974, 0.3464208469958976, 0.33790195593610406, 0.339959658915177, 0.3374597759684548, 0.3342865378363058, 0.3352202851092443, 0.3365925248945132, 0.33456708304584026, 0.34202238998841494, 0.3386423789197579, 0.3346880851313472, 0.3370452548842877, 0.3314532870426774, 0.3364005599869415, 0.33989791199564934, 0.3361147380201146, 0.3382872008951381, 0.33357505092862993, 0.32993781415279955, 0.3332745609804988, 0.33032939792610705, 0.3344333480345085, 0.34599867393262684, 1.8479408031562343, 1.4316387920407578, 0.6275829689111561, 0.352065910003148, 0.3590751870069653, 0.34960074187256396, 0.3576796499546617, 0.34992313489783555, 0.3520057121058926, 0.3454498619539663, 0.3318021559389308, 0.33630420884583145, 0.3291895758593455, 0.3264069890137762, 0.32596483000088483, 0.33147607184946537, 0.3344637098489329, 0.33499192993622273, 0.32812792796175927, 0.3308606119826436, 0.33169654896482825, 3.619951055967249, 6.458885362022556, 0.78447407099884, 0.3636629299726337, 0.3625875379657373, 0.36299946205690503, 0.3464137848932296, 0.33379223698284477, 0.3333720969967544, 0.3361208630958572, 0.3341693369438872, 0.3374009720282629, 0.34066808701027185, 0.34248177404515445, 1.1901106350123882, 2.801426236052066, 6.870765182888135, 0.3524238090030849, 0.39535786118358374, 0.3551146329846233, 0.3915222220821306, 0.35016649996396154, 0.44517248100601137, 0.3458946291357279, 0.3526887510670349, 0.3592749967938289, 5.40937428898178, 0.36034324287902564, 0.3593217780580744, 0.3574004218680784, 0.3530946710379794, 0.34627271397039294, 0.35289557196665555, 0.3648307240800932, 0.35217715392354876, 0.35230553697329015, 0.33665574388578534, 0.330019410001114, 0.34545754292048514, 0.32812859711702913, 0.33087377494666725, 0.3344475011108443, 0.32825478992890567, 0.3334016229491681, 0.3307951500173658, 0.3444502209313214, 0.33594696514774114, 0.3377021460328251, 0.3339847750030458, 0.34370385902002454, 0.3403789548901841, 0.3426942570367828, 0.35494314203970134, 0.3766600890085101, 0.3515439569018781, 0.34136017598211765, 0.3534703549230471, 4.736089746118523, 1.2871985129313543, 1.509419519919902, 0.8377186270663515, 0.3369146710028872, 0.3328493299195543, 0.3392879160819575, 0.33097255893517286, 0.33329194493126124, 0.33360575209371746, 0.3356900099897757, 0.340440361876972, 3.8962125399848446, 5.2154819059651345, 0.6071778080658987, 0.3552839020267129, 0.34633178310468793, 0.3566661039367318, 0.35503934195730835, 0.3600612849695608, 0.35501019889488816, 0.35150649305433035, 0.3578076650155708, 0.3503828450338915, 0.35147578001488, 1.5306197711033747, 4.085126259131357, 0.5221671549370512, 0.34495308494661003, 0.3352598921628669, 0.34304238692857325, 0.3413472209358588, 0.3381196389673278, 0.34199069789610803, 0.3418945971643552, 0.340760161052458, 0.33734161395113915, 0.34006080497056246, 0.33455859206151217, 0.33624918409623206, 0.3409390830202028, 0.3371992588508874, 0.34173018392175436, 0.3479028770234436, 0.3401981448987499, 0.4139435888500884, 0.3519299188628793, 0.33714406308718026, 0.36814654886256903, 0.374375231214799, 0.3970428081229329, 2.338598703034222, 2.476679018000141, 1.863900202093646, 0.342684522154741, 0.37047437811270356, 0.3313427969114855, 0.34095676301512867, 0.3340747479815036, 0.33429078792687505, 0.3426034750882536, 0.337986950064078, 0.3380554241593927, 0.3364057190483436, 0.34015684807673097, 0.34754837898071855, 0.3530747409677133, 0.3410447750939056, 0.33432084310334176, 0.3400085490429774, 0.3393994269426912, 0.33210801100358367, 0.34111063193995506, 0.33758974506054074, 0.34046361793298274, 0.33187635499052703, 0.33627984300255775, 0.3362741710152477, 0.3474597688764334, 0.3427647309144959, 0.3411678069969639, 0.3436904269037768, 0.3481792079983279, 0.34840501996222883, 0.355749384034425, 0.36159127892460674, 0.3541086360346526, 0.3429486630484462, 0.34649898495990783, 2.6084452371578664, 4.611171733122319, 5.04526013310533, 0.3472151670139283, 0.3389069240074605, 0.340525271021761, 0.3435116938780993, 0.33771005808375776, 0.3452182658948004, 0.3429450870025903, 0.34764878696296364, 0.3391728119459003, 0.337520121014677, 0.3472339319996536, 0.36960959585849196, 0.38296051998622715, 0.3698887978680432, 0.3696628500474617, 0.3570186300203204, 6.9393599421018735, 0.6216729630250484, 0.3339078798890114, 0.33160613011568785, 0.3301335669821128, 0.32796155486721545, 0.3324834778904915, 0.33189156698063016, 0.3378600630676374, 0.3425153900170699, 0.344251852016896, 0.3535479340935126, 0.3636342550162226, 0.3534197910921648, 0.3670499259606004, 6.331085819052532, 4.0503186479909346, 0.3558078871574253, 0.416547138011083, 0.35996965400408953, 0.3603973010322079, 0.3716552380938083, 0.3688348749419674, 0.4589640611084178, 2.748416441027075, 4.015510409837589, 0.558227182016708, 0.3650359530001879, 0.37319825403392315, 0.3811615880113095, 0.34657199890352786, 0.3489954248070717, 0.33867526112589985, 0.41256226191762835, 0.3637029799865559, 0.34612873708829284, 0.6603664569556713, 3.558765879017301, 3.580690592993051, 2.821221360936761, 0.33673523215111345, 0.3332126218592748, 0.3384901009267196, 0.33675094705540687, 0.3308740290813148, 0.33718436607159674, 0.33137844293378294, 0.33348021807614714, 0.33393924310803413, 0.3389898990280926, 0.34065649297554046, 0.3349554701708257, 0.3398223250405863, 0.33471914706751704, 0.34040823788382113, 0.3351783480029553, 0.34077397792134434, 0.33978297421708703, 0.3343848449876532, 1.7714297788916156, 0.41447726101614535, 0.3482485660351813, 0.35004238900728524, 0.34309615788515657, 0.347758152987808, 0.3503454279853031, 0.34809359500650316, 0.34857768402434886, 0.35455518204253167, 0.35618439596146345, 0.35570996499154717, 0.3454241550061852, 3.1146591000724584, 2.962528361938894, 1.4336332210805267, 0.34882805007509887, 0.3497846870450303, 0.33963355608284473, 0.321091532940045, 0.3384918631054461, 0.3287024849560112, 0.33289707894437015, 0.32703324407339096, 0.3275087069487199, 0.33820486010517925, 0.33632769412361085, 0.33192020596470684, 0.32919907907489687, 0.32126118685118854, 0.3211362389847636, 0.3228060060646385, 0.32285723404493183, 0.32277654798235744, 0.322944491985254, 0.32461522601079196, 0.3244226441020146, 0.32591395208146423, 0.3213537058327347, 0.3381819579517469, 0.338233140995726, 0.32099818089045584, 0.32834325497969985, 0.32821502222213894, 0.3298306619981304, 0.32657057396136224, 0.3258383610518649, 0.32446476398035884, 0.3229225490940735, 0.3300429788650945, 0.3238363090204075, 0.3333056539995596, 0.32906905200798064, 0.32574585010297596, 0.3254339079139754, 0.3266320509137586, 0.41726659110281616, 0.3241199131589383, 0.33102599799167365, 0.3321308981394395, 0.3376188410911709, 0.3637811669614166, 0.34897103998810053, 0.34071024996228516, 0.34281898802146316, 0.43618964485358447, 0.3473713102284819, 0.3545144301606342, 0.3467289009131491, 0.43391212390270084, 0.32289804494939744, 0.3267118231160566, 0.32419503189157695, 0.32918662892188877, 0.32279586512595415, 0.32344905100762844, 0.32486556097865105, 0.3224854131694883, 0.32799412589520216, 0.3265962810255587, 0.32574504299554974, 0.3418417169013992, 0.3452223640633747, 0.3508418219862506, 0.3466917958576232, 0.34177280496805906, 0.34139557590242475, 0.3378810960566625, 0.3465518788434565, 0.3395441729808226, 0.3385437710676342, 0.330855845939368, 0.32614525698591024, 0.329750484903343, 0.34126963187009096, 0.31617336499039084, 0.3175066302064806, 0.32269842305686325, 0.3350868009729311, 0.3507914589717984, 0.33925195992924273, 0.3409003739943728, 0.34591629612259567, 1.7704919900279492, 2.540828000055626, 5.476408444927074, 0.34679094911552966, 0.3423210069304332, 0.3242832910036668, 0.32049344712868333, 0.3284057410201058, 0.32622751186136156, 0.3171079258900136, 0.332369432086125, 0.32552668103016913, 0.3283018060028553, 0.34296228212770075, 0.3176029701717198, 0.3184938010526821, 0.3227984021650627, 0.3466787301003933, 0.3531753468560055, 1.6711639469722286, 3.8078962399158627, 0.30838357808534056, 0.31511987489648163, 0.3147083419607952, 0.3157012171577662, 0.3188389140414074, 0.32754181686323136, 0.32437179319094867, 0.31819928297773004, 0.318741912022233, 0.3181135739432648, 0.31804306304547936, 0.31901786499656737, 0.31237854703795165, 0.3236199418315664, 0.3199531501159072, 7.098481801804155, 5.534211765159853, 0.34450965700671077, 0.34081490500830114, 0.3382437920663506, 0.3383410490350798, 0.4132905340520665, 0.3167211170075461, 0.31857726408634335, 0.31707653892226517, 0.3958858671830967, 0.3985055348603055, 0.34926662081852555, 0.3466161220567301, 0.43375037400983274, 0.34051659202668816, 6.796997212921269, 8.36475961015094, 0.3384432189632207, 0.3307164328871295, 0.32359551975969225, 0.33500655309762806, 0.3502969208639115, 0.3710424250457436, 0.38111210498027503, 0.8608919879188761, 4.031270107137971, 0.3515346149215475, 0.3489315500482917, 0.3505762900458649, 0.3455099940765649, 0.34986985998693854, 0.3476907630683854, 0.3471499679144472, 0.3508766589220613, 0.3476307240780443, 0.34715347609017044, 0.3495047769974917, 0.34959386696573347, 0.34878805093467236, 0.4390539510641247, 0.34686556819360703, 0.345662550884299, 0.3550916949752718, 0.348824149928987, 0.34902115096338093, 0.34924155892804265, 0.34818260895553976, 0.3521736549446359, 0.35026907303836197, 0.3507039719261229, 0.34938060608692467, 4.221170858014375, 10.267137862974778, 0.36887303297407925, 0.35885524400509894, 0.3548421129817143, 0.3565919960383326, 0.3557939409511164, 0.350642949109897, 0.3387335648294538, 0.3561697849072516, 0.3767176568508148, 0.3810100288828835, 0.3599027800373733, 0.3607853540452197, 0.3604588860180229, 0.3598630278138444, 0.360033574863337, 0.35883245908189565, 0.35871290205977857, 0.3621768718585372, 0.3594270490575582, 0.3629481280222535, 3.8732781569706276, 10.080521139898337, 0.3543692461680621, 0.35153966094367206, 0.3383827480720356, 0.3280351849971339, 0.3289738950552419, 0.3372406361158937, 0.3505290261236951, 0.34882629406638443, 0.3512219289550558, 0.3449824219569564, 0.3288948079571128, 0.3289590252097696, 0.3284186169039458, 0.33518820896279067, 0.34802293300163, 0.35253955295775086, 0.3535847698803991, 0.3510694799479097, 0.3522938439855352, 0.35278094990644604, 0.3511092889821157, 1.209457210963592, 3.8027232429012656, 0.3409811758901924, 0.33750180003698915, 0.33615397312678397, 0.33433723892085254, 0.33799119002651423, 0.3391552690882236, 0.3404020881280303, 0.3359204479493201, 0.33523876615799963, 0.3364430151414126, 0.3387287190416828, 0.3395313349319622, 0.342860740958713, 0.3467115900712088, 0.3467211070237681, 1.4231907029170543, 5.4045700039714575, 0.3758944800356403, 0.3288155629998073, 0.32102720893453807, 0.32049249904230237, 0.31986844597849995, 0.32752330391667783, 0.334103608969599, 0.3248204829869792, 0.32936118892394006, 0.34725717187393457, 0.34831730683799833, 0.3356222889851779, 0.339905695989728, 2.229053708142601, 4.207712676026858, 3.1477969439001754, 0.33753463299944997, 0.3364031818928197, 0.33684977900702506, 0.33433529385365546, 0.3373828842304647, 0.33738424291368574, 0.3389947541290894, 0.3621213489677757, 0.33643864491023123, 0.3359222539002076, 0.33674559905193746, 0.3292754719732329, 0.37178288004361093, 0.35823855304624885, 0.3471712740138173, 3.9511879361234605, 5.4551364339422435, 0.8912853489164263, 0.3414258520351723, 0.33495871105697006, 0.3824990428984165, 0.3304963349364698, 0.3320066989399493, 0.33763913600705564, 0.3539888671366498, 0.35369543405249715, 0.35630067298188806, 0.35415401100181043, 0.35555159219074994, 0.3558032839791849, 6.609211838920601, 2.9331248679663986, 0.3321559661999345, 0.3585269949398935, 0.3256487869657576, 0.34775578300468624, 0.3721510861068964, 2.2880365169839934, 6.303590519004501, 0.32157935993745923, 0.340501350001432, 0.3374914248706773, 0.33842753793578595, 1.2667000949149951, 6.056750187184662, 0.6830101719824597, 0.3407413871027529, 0.338504810933955, 0.33942241687327623, 0.3401063799392432, 2.0108289800118655, 6.561840276001021, 0.3465179760241881, 0.34499500796664506, 0.34519121306948364, 0.3462858429411426, 0.340414269012399, 0.3322133340407163, 0.33399582596030086, 0.3316077721538022, 0.3289567050524056, 0.32605097512714565, 0.3253899138653651, 0.3277688381494954, 0.3268765901448205, 0.32666134997271, 0.3294261699775234, 0.3286336970049888, 0.33979390491731465, 0.35262990300543606, 0.43449587910436094, 0.4173376358812675, 0.353301948052831, 0.357470044051297, 0.35828013496939093, 0.4597328210948035, 0.34227682009804994, 0.33886786992661655, 0.34410181699786335, 0.3614776040194556, 0.36806903104297817, 0.3369976580142975, 0.36099761503282934, 0.33458259399048984, 0.336783806909807, 0.3379649899434298, 0.3341470139566809, 0.5260863377479836, 0.4042232730425894, 1.9779548001242802, 0.3412582410965115, 0.3268237599404529, 0.3353294529952109, 0.4442200391786173, 0.4048477850155905, 0.3507638309383765, 0.3551647629356012, 0.34513854095712304, 0.347029653028585, 0.3788510710000992, 0.35594775690697134, 0.3470118719851598, 0.34467941289767623, 0.3444804720347747, 0.34432632790412754, 0.3418865230632946, 0.3455822999821976, 0.34629751392640173, 0.35829608200583607, 0.3661242179805413, 0.3503747161012143, 0.3479496049694717, 0.4698107809526846, 0.37375444907229394, 0.3572007849579677, 0.358867438044399, 0.3509821239858866, 0.3397541750455275, 0.35174625797662884, 0.3497889048885554, 0.3497380099724978, 0.3484567990526557, 0.35019104497041553, 0.3489975220290944, 0.34300981322303414, 0.3320312349824235, 0.33324379206169397, 0.3342240699566901, 0.33889134298078716, 0.34662932890933007, 0.34800314891617745, 0.3424348219996318, 0.3461113160010427, 0.3463955500628799, 0.347166694002226, 0.3276145930867642, 0.3302843519486487, 0.32850111520383507, 0.32785365905147046, 0.3302036280510947, 0.33770526095759124, 0.346190300071612, 0.34617397910915315, 0.3473790369462222, 0.34875476104207337, 0.35016288806218654, 0.3472046940587461, 0.34817671205382794, 0.3472225731238723, 0.3467879701638594, 0.34778977488167584, 0.3502882680622861, 0.3477797780651599]
Total Epoch List: [1000, 13, 287]
Total Time List: [0.8964204690419137, 0.07286693400237709, 0.07589435100089759]
========================training times:2========================
========================k_idx:0========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: AEtransGAT
AEtransGAT(
  (gae): GAE(
    (encoder): transGAT(
      (encoder): EdgeFeatureGAT()
      (classifier): Sequential(
        (0): Linear(in_features=64, out_features=64, bias=True)
        (1): ReLU(inplace=True)
        (2): Linear(in_features=64, out_features=2, bias=True)
      )
    )
    (decoder): InnerProductDecoder()
  )
)
Total number of parameters: 1914242
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x75feb28b1720>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 2.0068;  Loss pred: 2.0068; Loss self: 0.0000; time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6938 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6933 score: 0.5102 time: 0.06s
Epoch 2/1000, LR 0.000000
Train loss: 2.0321;  Loss pred: 2.0321; Loss self: 0.0000; time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6937 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6933 score: 0.5102 time: 0.06s
Epoch 3/1000, LR 0.000030
Train loss: 2.0298;  Loss pred: 2.0298; Loss self: 0.0000; time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6937 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6933 score: 0.5102 time: 0.06s
Epoch 4/1000, LR 0.000060
Train loss: 2.0116;  Loss pred: 2.0116; Loss self: 0.0000; time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6936 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6932 score: 0.5102 time: 0.06s
Epoch 5/1000, LR 0.000090
Train loss: 2.0056;  Loss pred: 2.0056; Loss self: 0.0000; time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6935 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6932 score: 0.5102 time: 0.06s
Epoch 6/1000, LR 0.000120
Train loss: 1.9760;  Loss pred: 1.9760; Loss self: 0.0000; time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6934 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6933 score: 0.5102 time: 0.06s
Epoch 7/1000, LR 0.000150
Train loss: 1.9706;  Loss pred: 1.9706; Loss self: 0.0000; time: 0.22s
Val loss: 0.6933 score: 0.4898 time: 0.07s
Test loss: 0.6933 score: 0.4898 time: 0.06s
Epoch 8/1000, LR 0.000180
Train loss: 1.9400;  Loss pred: 1.9400; Loss self: 0.0000; time: 0.22s
Val loss: 0.6932 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6933 score: 0.4898 time: 0.06s
Epoch 9/1000, LR 0.000210
Train loss: 1.9175;  Loss pred: 1.9175; Loss self: 0.0000; time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6931 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6933 score: 0.4898 time: 0.06s
Epoch 10/1000, LR 0.000240
Train loss: 1.9103;  Loss pred: 1.9103; Loss self: 0.0000; time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6930 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6933 score: 0.4898 time: 0.06s
Epoch 11/1000, LR 0.000270
Train loss: 1.8743;  Loss pred: 1.8743; Loss self: 0.0000; time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6929 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6933 score: 0.4898 time: 0.06s
Epoch 12/1000, LR 0.000270
Train loss: 1.8496;  Loss pred: 1.8496; Loss self: 0.0000; time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6929 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6933 score: 0.4898 time: 0.06s
Epoch 13/1000, LR 0.000270
Train loss: 1.8047;  Loss pred: 1.8047; Loss self: 0.0000; time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6928 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6932 score: 0.4898 time: 0.06s
Epoch 14/1000, LR 0.000270
Train loss: 1.7758;  Loss pred: 1.7758; Loss self: 0.0000; time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6927 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6931 score: 0.4898 time: 0.06s
Epoch 15/1000, LR 0.000270
Train loss: 1.7623;  Loss pred: 1.7623; Loss self: 0.0000; time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6926 score: 0.5102 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6931 score: 0.4898 time: 0.06s
Epoch 16/1000, LR 0.000270
Train loss: 1.7307;  Loss pred: 1.7307; Loss self: 0.0000; time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6926 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6930 score: 0.4898 time: 0.06s
Epoch 17/1000, LR 0.000270
Train loss: 1.6978;  Loss pred: 1.6978; Loss self: 0.0000; time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6925 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6929 score: 0.4898 time: 0.06s
Epoch 18/1000, LR 0.000270
Train loss: 1.6829;  Loss pred: 1.6829; Loss self: 0.0000; time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6925 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6929 score: 0.4898 time: 0.06s
Epoch 19/1000, LR 0.000270
Train loss: 1.6566;  Loss pred: 1.6566; Loss self: 0.0000; time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6925 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6928 score: 0.4898 time: 0.06s
Epoch 20/1000, LR 0.000270
Train loss: 1.6302;  Loss pred: 1.6302; Loss self: 0.0000; time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6924 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6927 score: 0.4898 time: 0.06s
Epoch 21/1000, LR 0.000270
Train loss: 1.5914;  Loss pred: 1.5914; Loss self: 0.0000; time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6924 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6926 score: 0.4898 time: 0.06s
Epoch 22/1000, LR 0.000270
Train loss: 1.5814;  Loss pred: 1.5814; Loss self: 0.0000; time: 0.21s
Val loss: 0.6923 score: 0.6122 time: 0.07s
Test loss: 0.6925 score: 0.5510 time: 0.06s
Epoch 23/1000, LR 0.000270
Train loss: 1.5709;  Loss pred: 1.5709; Loss self: 0.0000; time: 0.21s
Val loss: 0.6922 score: 0.8571 time: 0.07s
Test loss: 0.6925 score: 0.7143 time: 0.06s
Epoch 24/1000, LR 0.000270
Train loss: 1.5464;  Loss pred: 1.5464; Loss self: 0.0000; time: 0.21s
Val loss: 0.6922 score: 0.8571 time: 0.07s
Test loss: 0.6924 score: 0.7755 time: 0.06s
Epoch 25/1000, LR 0.000270
Train loss: 1.5179;  Loss pred: 1.5179; Loss self: 0.0000; time: 0.22s
Val loss: 0.6921 score: 0.7755 time: 0.07s
Test loss: 0.6923 score: 0.7959 time: 0.06s
Epoch 26/1000, LR 0.000270
Train loss: 1.4955;  Loss pred: 1.4955; Loss self: 0.0000; time: 0.21s
Val loss: 0.6919 score: 0.7347 time: 0.07s
Test loss: 0.6922 score: 0.7755 time: 0.07s
Epoch 27/1000, LR 0.000270
Train loss: 1.4643;  Loss pred: 1.4643; Loss self: 0.0000; time: 0.22s
Val loss: 0.6918 score: 0.8163 time: 0.07s
Test loss: 0.6921 score: 0.7959 time: 0.06s
Epoch 28/1000, LR 0.000270
Train loss: 1.4502;  Loss pred: 1.4502; Loss self: 0.0000; time: 0.22s
Val loss: 0.6917 score: 0.7959 time: 0.07s
Test loss: 0.6921 score: 0.7959 time: 0.06s
Epoch 29/1000, LR 0.000270
Train loss: 1.4225;  Loss pred: 1.4225; Loss self: 0.0000; time: 0.22s
Val loss: 0.6916 score: 0.7755 time: 0.07s
Test loss: 0.6920 score: 0.7959 time: 0.06s
Epoch 30/1000, LR 0.000270
Train loss: 1.4189;  Loss pred: 1.4189; Loss self: 0.0000; time: 0.22s
Val loss: 0.6915 score: 0.7143 time: 0.07s
Test loss: 0.6919 score: 0.7143 time: 0.06s
Epoch 31/1000, LR 0.000270
Train loss: 1.3927;  Loss pred: 1.3927; Loss self: 0.0000; time: 0.22s
Val loss: 0.6914 score: 0.6122 time: 0.07s
Test loss: 0.6918 score: 0.6939 time: 0.06s
Epoch 32/1000, LR 0.000270
Train loss: 1.3984;  Loss pred: 1.3984; Loss self: 0.0000; time: 0.22s
Val loss: 0.6913 score: 0.6122 time: 0.07s
Test loss: 0.6917 score: 0.6327 time: 0.06s
Epoch 33/1000, LR 0.000270
Train loss: 1.3662;  Loss pred: 1.3662; Loss self: 0.0000; time: 0.23s
Val loss: 0.6912 score: 0.6122 time: 0.07s
Test loss: 0.6915 score: 0.6531 time: 0.06s
Epoch 34/1000, LR 0.000270
Train loss: 1.3543;  Loss pred: 1.3543; Loss self: 0.0000; time: 0.22s
Val loss: 0.6910 score: 0.6122 time: 0.07s
Test loss: 0.6914 score: 0.6327 time: 0.06s
Epoch 35/1000, LR 0.000270
Train loss: 1.3383;  Loss pred: 1.3383; Loss self: 0.0000; time: 0.22s
Val loss: 0.6909 score: 0.6939 time: 0.08s
Test loss: 0.6913 score: 0.7143 time: 0.06s
Epoch 36/1000, LR 0.000270
Train loss: 1.3238;  Loss pred: 1.3238; Loss self: 0.0000; time: 0.22s
Val loss: 0.6907 score: 0.7143 time: 0.08s
Test loss: 0.6912 score: 0.7347 time: 0.07s
Epoch 37/1000, LR 0.000270
Train loss: 1.3146;  Loss pred: 1.3146; Loss self: 0.0000; time: 0.22s
Val loss: 0.6906 score: 0.9184 time: 0.09s
Test loss: 0.6912 score: 0.8367 time: 0.08s
Epoch 38/1000, LR 0.000270
Train loss: 1.3043;  Loss pred: 1.3043; Loss self: 0.0000; time: 0.23s
Val loss: 0.6904 score: 0.8571 time: 0.07s
Test loss: 0.6911 score: 0.8163 time: 0.06s
Epoch 39/1000, LR 0.000269
Train loss: 1.2997;  Loss pred: 1.2997; Loss self: 0.0000; time: 5.64s
Val loss: 0.6902 score: 0.8367 time: 1.69s
Test loss: 0.6910 score: 0.7347 time: 1.50s
Epoch 40/1000, LR 0.000269
Train loss: 1.2794;  Loss pred: 1.2794; Loss self: 0.0000; time: 2.61s
Val loss: 0.6901 score: 0.6939 time: 0.08s
Test loss: 0.6909 score: 0.6735 time: 0.06s
Epoch 41/1000, LR 0.000269
Train loss: 1.2661;  Loss pred: 1.2661; Loss self: 0.0000; time: 0.21s
Val loss: 0.6898 score: 0.5510 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6908 score: 0.4898 time: 0.06s
Epoch 42/1000, LR 0.000269
Train loss: 1.2540;  Loss pred: 1.2540; Loss self: 0.0000; time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6896 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6906 score: 0.4898 time: 0.06s
Epoch 43/1000, LR 0.000269
Train loss: 1.2405;  Loss pred: 1.2405; Loss self: 0.0000; time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6894 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6905 score: 0.4898 time: 0.06s
Epoch 44/1000, LR 0.000269
Train loss: 1.2308;  Loss pred: 1.2308; Loss self: 0.0000; time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6892 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6904 score: 0.4898 time: 0.06s
Epoch 45/1000, LR 0.000269
Train loss: 1.2215;  Loss pred: 1.2215; Loss self: 0.0000; time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6890 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6903 score: 0.4898 time: 0.06s
Epoch 46/1000, LR 0.000269
Train loss: 1.2152;  Loss pred: 1.2152; Loss self: 0.0000; time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6888 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6901 score: 0.4898 time: 0.06s
Epoch 47/1000, LR 0.000269
Train loss: 1.2146;  Loss pred: 1.2146; Loss self: 0.0000; time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6885 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6899 score: 0.4898 time: 0.07s
Epoch 48/1000, LR 0.000269
Train loss: 1.1959;  Loss pred: 1.1959; Loss self: 0.0000; time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6883 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6898 score: 0.4898 time: 0.06s
Epoch 49/1000, LR 0.000269
Train loss: 1.1859;  Loss pred: 1.1859; Loss self: 0.0000; time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6880 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6896 score: 0.4898 time: 0.06s
Epoch 50/1000, LR 0.000269
Train loss: 1.1813;  Loss pred: 1.1813; Loss self: 0.0000; time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6878 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6894 score: 0.4898 time: 0.06s
Epoch 51/1000, LR 0.000269
Train loss: 1.1726;  Loss pred: 1.1726; Loss self: 0.0000; time: 0.21s
Val loss: 0.6875 score: 0.5510 time: 0.07s
Test loss: 0.6892 score: 0.5102 time: 0.06s
Epoch 52/1000, LR 0.000269
Train loss: 1.1619;  Loss pred: 1.1619; Loss self: 0.0000; time: 0.21s
Val loss: 0.6872 score: 0.7347 time: 0.07s
Test loss: 0.6889 score: 0.6122 time: 0.06s
Epoch 53/1000, LR 0.000269
Train loss: 1.1568;  Loss pred: 1.1568; Loss self: 0.0000; time: 0.21s
Val loss: 0.6870 score: 0.7959 time: 0.07s
Test loss: 0.6887 score: 0.7143 time: 0.06s
Epoch 54/1000, LR 0.000269
Train loss: 1.1503;  Loss pred: 1.1503; Loss self: 0.0000; time: 0.21s
Val loss: 0.6867 score: 0.8776 time: 0.07s
Test loss: 0.6885 score: 0.8367 time: 0.06s
Epoch 55/1000, LR 0.000269
Train loss: 1.1425;  Loss pred: 1.1425; Loss self: 0.0000; time: 0.21s
Val loss: 0.6864 score: 0.9184 time: 0.07s
Test loss: 0.6882 score: 0.8776 time: 0.06s
Epoch 56/1000, LR 0.000269
Train loss: 1.1401;  Loss pred: 1.1401; Loss self: 0.0000; time: 0.23s
Val loss: 0.6861 score: 0.9184 time: 0.11s
Test loss: 0.6880 score: 0.8776 time: 0.06s
Epoch 57/1000, LR 0.000269
Train loss: 1.1338;  Loss pred: 1.1338; Loss self: 0.0000; time: 0.57s
Val loss: 0.6857 score: 0.9592 time: 1.69s
Test loss: 0.6877 score: 0.8980 time: 1.64s
Epoch 58/1000, LR 0.000269
Train loss: 1.1200;  Loss pred: 1.1200; Loss self: 0.0000; time: 3.56s
Val loss: 0.6854 score: 0.9592 time: 0.08s
Test loss: 0.6874 score: 0.8980 time: 0.06s
Epoch 59/1000, LR 0.000268
Train loss: 1.1159;  Loss pred: 1.1159; Loss self: 0.0000; time: 0.22s
Val loss: 0.6850 score: 0.9796 time: 0.07s
Test loss: 0.6871 score: 0.8776 time: 0.06s
Epoch 60/1000, LR 0.000268
Train loss: 1.1170;  Loss pred: 1.1170; Loss self: 0.0000; time: 0.21s
Val loss: 0.6846 score: 0.9796 time: 0.07s
Test loss: 0.6868 score: 0.8571 time: 0.06s
Epoch 61/1000, LR 0.000268
Train loss: 1.1072;  Loss pred: 1.1072; Loss self: 0.0000; time: 0.21s
Val loss: 0.6842 score: 0.9796 time: 0.07s
Test loss: 0.6865 score: 0.8571 time: 0.06s
Epoch 62/1000, LR 0.000268
Train loss: 1.1045;  Loss pred: 1.1045; Loss self: 0.0000; time: 0.21s
Val loss: 0.6838 score: 0.9796 time: 0.07s
Test loss: 0.6862 score: 0.8571 time: 0.06s
Epoch 63/1000, LR 0.000268
Train loss: 1.1009;  Loss pred: 1.1009; Loss self: 0.0000; time: 0.21s
Val loss: 0.6834 score: 0.9796 time: 0.07s
Test loss: 0.6859 score: 0.8571 time: 0.06s
Epoch 64/1000, LR 0.000268
Train loss: 1.0933;  Loss pred: 1.0933; Loss self: 0.0000; time: 0.21s
Val loss: 0.6829 score: 0.9796 time: 0.07s
Test loss: 0.6855 score: 0.8571 time: 0.06s
Epoch 65/1000, LR 0.000268
Train loss: 1.0884;  Loss pred: 1.0884; Loss self: 0.0000; time: 0.21s
Val loss: 0.6824 score: 0.9796 time: 0.07s
Test loss: 0.6852 score: 0.8571 time: 0.06s
Epoch 66/1000, LR 0.000268
Train loss: 1.0865;  Loss pred: 1.0865; Loss self: 0.0000; time: 0.21s
Val loss: 0.6820 score: 0.9796 time: 0.07s
Test loss: 0.6848 score: 0.8367 time: 0.06s
Epoch 67/1000, LR 0.000268
Train loss: 1.0760;  Loss pred: 1.0760; Loss self: 0.0000; time: 0.21s
Val loss: 0.6814 score: 0.9796 time: 0.07s
Test loss: 0.6844 score: 0.8571 time: 0.06s
Epoch 68/1000, LR 0.000268
Train loss: 1.0768;  Loss pred: 1.0768; Loss self: 0.0000; time: 2.19s
Val loss: 0.6809 score: 0.9796 time: 1.09s
Test loss: 0.6840 score: 0.8571 time: 1.01s
Epoch 69/1000, LR 0.000268
Train loss: 1.0693;  Loss pred: 1.0693; Loss self: 0.0000; time: 3.91s
Val loss: 0.6804 score: 0.9796 time: 0.54s
Test loss: 0.6836 score: 0.8571 time: 0.07s
Epoch 70/1000, LR 0.000268
Train loss: 1.0710;  Loss pred: 1.0710; Loss self: 0.0000; time: 0.21s
Val loss: 0.6798 score: 0.9796 time: 0.07s
Test loss: 0.6832 score: 0.8571 time: 0.06s
Epoch 71/1000, LR 0.000268
Train loss: 1.0665;  Loss pred: 1.0665; Loss self: 0.0000; time: 0.21s
Val loss: 0.6792 score: 0.9796 time: 0.07s
Test loss: 0.6828 score: 0.8571 time: 0.06s
Epoch 72/1000, LR 0.000267
Train loss: 1.0628;  Loss pred: 1.0628; Loss self: 0.0000; time: 0.21s
Val loss: 0.6786 score: 0.9796 time: 0.07s
Test loss: 0.6823 score: 0.8571 time: 0.06s
Epoch 73/1000, LR 0.000267
Train loss: 1.0531;  Loss pred: 1.0531; Loss self: 0.0000; time: 0.20s
Val loss: 0.6780 score: 0.9796 time: 0.07s
Test loss: 0.6818 score: 0.8571 time: 0.06s
Epoch 74/1000, LR 0.000267
Train loss: 1.0583;  Loss pred: 1.0583; Loss self: 0.0000; time: 0.21s
Val loss: 0.6773 score: 0.9796 time: 0.07s
Test loss: 0.6814 score: 0.8571 time: 0.06s
Epoch 75/1000, LR 0.000267
Train loss: 1.0473;  Loss pred: 1.0473; Loss self: 0.0000; time: 0.21s
Val loss: 0.6766 score: 0.9796 time: 0.07s
Test loss: 0.6809 score: 0.8571 time: 0.06s
Epoch 76/1000, LR 0.000267
Train loss: 1.0494;  Loss pred: 1.0494; Loss self: 0.0000; time: 0.20s
Val loss: 0.6759 score: 0.9796 time: 0.07s
Test loss: 0.6803 score: 0.8571 time: 0.06s
Epoch 77/1000, LR 0.000267
Train loss: 1.0420;  Loss pred: 1.0420; Loss self: 0.0000; time: 0.21s
Val loss: 0.6752 score: 0.9796 time: 0.07s
Test loss: 0.6798 score: 0.8571 time: 0.06s
Epoch 78/1000, LR 0.000267
Train loss: 1.0422;  Loss pred: 1.0422; Loss self: 0.0000; time: 0.22s
Val loss: 0.6744 score: 0.9796 time: 0.08s
Test loss: 0.6792 score: 0.8571 time: 0.07s
Epoch 79/1000, LR 0.000267
Train loss: 1.0401;  Loss pred: 1.0401; Loss self: 0.0000; time: 0.21s
Val loss: 0.6736 score: 0.9796 time: 0.07s
Test loss: 0.6786 score: 0.8776 time: 0.06s
Epoch 80/1000, LR 0.000267
Train loss: 1.0345;  Loss pred: 1.0345; Loss self: 0.0000; time: 0.20s
Val loss: 0.6728 score: 0.9796 time: 0.07s
Test loss: 0.6780 score: 0.8776 time: 0.07s
Epoch 81/1000, LR 0.000267
Train loss: 1.0330;  Loss pred: 1.0330; Loss self: 0.0000; time: 0.20s
Val loss: 0.6719 score: 0.9796 time: 0.07s
Test loss: 0.6774 score: 0.8776 time: 0.06s
Epoch 82/1000, LR 0.000267
Train loss: 1.0301;  Loss pred: 1.0301; Loss self: 0.0000; time: 0.22s
Val loss: 0.6710 score: 0.9796 time: 0.07s
Test loss: 0.6767 score: 0.8776 time: 0.06s
Epoch 83/1000, LR 0.000266
Train loss: 1.0298;  Loss pred: 1.0298; Loss self: 0.0000; time: 0.22s
Val loss: 0.6700 score: 0.9796 time: 0.07s
Test loss: 0.6759 score: 0.8776 time: 0.06s
Epoch 84/1000, LR 0.000266
Train loss: 1.0256;  Loss pred: 1.0256; Loss self: 0.0000; time: 0.22s
Val loss: 0.6689 score: 0.9796 time: 0.07s
Test loss: 0.6751 score: 0.8776 time: 0.06s
Epoch 85/1000, LR 0.000266
Train loss: 1.0218;  Loss pred: 1.0218; Loss self: 0.0000; time: 0.22s
Val loss: 0.6678 score: 0.9796 time: 0.07s
Test loss: 0.6743 score: 0.8776 time: 0.06s
Epoch 86/1000, LR 0.000266
Train loss: 1.0178;  Loss pred: 1.0178; Loss self: 0.0000; time: 0.22s
Val loss: 0.6666 score: 0.9796 time: 0.07s
Test loss: 0.6734 score: 0.8571 time: 0.06s
Epoch 87/1000, LR 0.000266
Train loss: 1.0171;  Loss pred: 1.0171; Loss self: 0.0000; time: 0.22s
Val loss: 0.6655 score: 0.9796 time: 0.63s
Test loss: 0.6725 score: 0.8571 time: 2.31s
Epoch 88/1000, LR 0.000266
Train loss: 1.0130;  Loss pred: 1.0130; Loss self: 0.0000; time: 7.49s
Val loss: 0.6643 score: 0.9796 time: 2.47s
Test loss: 0.6717 score: 0.8571 time: 2.32s
Epoch 89/1000, LR 0.000266
Train loss: 1.0129;  Loss pred: 1.0129; Loss self: 0.0000; time: 0.46s
Val loss: 0.6631 score: 0.9796 time: 0.07s
Test loss: 0.6708 score: 0.8571 time: 0.07s
Epoch 90/1000, LR 0.000266
Train loss: 1.0105;  Loss pred: 1.0105; Loss self: 0.0000; time: 0.22s
Val loss: 0.6619 score: 0.9796 time: 0.07s
Test loss: 0.6699 score: 0.8571 time: 0.06s
Epoch 91/1000, LR 0.000266
Train loss: 1.0081;  Loss pred: 1.0081; Loss self: 0.0000; time: 0.22s
Val loss: 0.6607 score: 0.9796 time: 0.07s
Test loss: 0.6690 score: 0.8571 time: 0.06s
Epoch 92/1000, LR 0.000266
Train loss: 1.0047;  Loss pred: 1.0047; Loss self: 0.0000; time: 0.22s
Val loss: 0.6594 score: 0.9796 time: 0.07s
Test loss: 0.6680 score: 0.8571 time: 0.06s
Epoch 93/1000, LR 0.000265
Train loss: 1.0021;  Loss pred: 1.0021; Loss self: 0.0000; time: 0.22s
Val loss: 0.6581 score: 0.9796 time: 0.07s
Test loss: 0.6670 score: 0.8571 time: 0.06s
Epoch 94/1000, LR 0.000265
Train loss: 0.9996;  Loss pred: 0.9996; Loss self: 0.0000; time: 0.22s
Val loss: 0.6567 score: 0.9796 time: 0.07s
Test loss: 0.6660 score: 0.8571 time: 0.06s
Epoch 95/1000, LR 0.000265
Train loss: 0.9999;  Loss pred: 0.9999; Loss self: 0.0000; time: 0.22s
Val loss: 0.6553 score: 0.9796 time: 0.07s
Test loss: 0.6650 score: 0.8571 time: 0.06s
Epoch 96/1000, LR 0.000265
Train loss: 0.9946;  Loss pred: 0.9946; Loss self: 0.0000; time: 0.22s
Val loss: 0.6539 score: 0.9796 time: 0.07s
Test loss: 0.6639 score: 0.8571 time: 0.06s
Epoch 97/1000, LR 0.000265
Train loss: 0.9946;  Loss pred: 0.9946; Loss self: 0.0000; time: 0.22s
Val loss: 0.6524 score: 0.9796 time: 0.07s
Test loss: 0.6628 score: 0.8571 time: 0.06s
Epoch 98/1000, LR 0.000265
Train loss: 0.9934;  Loss pred: 0.9934; Loss self: 0.0000; time: 0.22s
Val loss: 0.6508 score: 0.9796 time: 0.07s
Test loss: 0.6616 score: 0.8776 time: 0.06s
Epoch 99/1000, LR 0.000265
Train loss: 0.9897;  Loss pred: 0.9897; Loss self: 0.0000; time: 0.21s
Val loss: 0.6493 score: 0.9796 time: 0.07s
Test loss: 0.6605 score: 0.8776 time: 2.34s
Epoch 100/1000, LR 0.000265
Train loss: 0.9869;  Loss pred: 0.9869; Loss self: 0.0000; time: 4.64s
Val loss: 0.6476 score: 0.9796 time: 0.14s
Test loss: 0.6592 score: 0.8776 time: 0.62s
Epoch 101/1000, LR 0.000265
Train loss: 0.9848;  Loss pred: 0.9848; Loss self: 0.0000; time: 0.31s
Val loss: 0.6460 score: 0.9796 time: 0.07s
Test loss: 0.6580 score: 0.8776 time: 0.06s
Epoch 102/1000, LR 0.000264
Train loss: 0.9836;  Loss pred: 0.9836; Loss self: 0.0000; time: 0.22s
Val loss: 0.6443 score: 0.9796 time: 0.07s
Test loss: 0.6567 score: 0.8776 time: 0.06s
Epoch 103/1000, LR 0.000264
Train loss: 0.9819;  Loss pred: 0.9819; Loss self: 0.0000; time: 0.21s
Val loss: 0.6425 score: 0.9796 time: 0.07s
Test loss: 0.6554 score: 0.8980 time: 0.06s
Epoch 104/1000, LR 0.000264
Train loss: 0.9800;  Loss pred: 0.9800; Loss self: 0.0000; time: 0.21s
Val loss: 0.6407 score: 0.9796 time: 0.07s
Test loss: 0.6540 score: 0.8980 time: 0.07s
Epoch 105/1000, LR 0.000264
Train loss: 0.9764;  Loss pred: 0.9764; Loss self: 0.0000; time: 0.23s
Val loss: 0.6388 score: 0.9796 time: 0.07s
Test loss: 0.6526 score: 0.8980 time: 0.07s
Epoch 106/1000, LR 0.000264
Train loss: 0.9752;  Loss pred: 0.9752; Loss self: 0.0000; time: 0.23s
Val loss: 0.6369 score: 0.9796 time: 0.07s
Test loss: 0.6512 score: 0.8980 time: 0.07s
Epoch 107/1000, LR 0.000264
Train loss: 0.9719;  Loss pred: 0.9719; Loss self: 0.0000; time: 0.22s
Val loss: 0.6349 score: 0.9796 time: 0.07s
Test loss: 0.6497 score: 0.8980 time: 0.07s
Epoch 108/1000, LR 0.000264
Train loss: 0.9712;  Loss pred: 0.9712; Loss self: 0.0000; time: 0.22s
Val loss: 0.6329 score: 0.9796 time: 0.07s
Test loss: 0.6482 score: 0.8776 time: 0.06s
Epoch 109/1000, LR 0.000264
Train loss: 0.9664;  Loss pred: 0.9664; Loss self: 0.0000; time: 0.22s
Val loss: 0.6309 score: 0.9796 time: 0.07s
Test loss: 0.6466 score: 0.8776 time: 0.06s
Epoch 110/1000, LR 0.000263
Train loss: 0.9671;  Loss pred: 0.9671; Loss self: 0.0000; time: 0.21s
Val loss: 0.6288 score: 0.9796 time: 0.07s
Test loss: 0.6450 score: 0.8776 time: 0.06s
Epoch 111/1000, LR 0.000263
Train loss: 0.9646;  Loss pred: 0.9646; Loss self: 0.0000; time: 0.21s
Val loss: 0.6266 score: 0.9796 time: 0.07s
Test loss: 0.6434 score: 0.8571 time: 0.06s
Epoch 112/1000, LR 0.000263
Train loss: 0.9598;  Loss pred: 0.9598; Loss self: 0.0000; time: 0.21s
Val loss: 0.6244 score: 0.9796 time: 0.07s
Test loss: 0.6417 score: 0.8571 time: 0.06s
Epoch 113/1000, LR 0.000263
Train loss: 0.9615;  Loss pred: 0.9615; Loss self: 0.0000; time: 0.21s
Val loss: 0.6222 score: 0.9796 time: 0.07s
Test loss: 0.6400 score: 0.8571 time: 0.06s
Epoch 114/1000, LR 0.000263
Train loss: 0.9587;  Loss pred: 0.9587; Loss self: 0.0000; time: 0.21s
Val loss: 0.6199 score: 0.9796 time: 0.07s
Test loss: 0.6382 score: 0.8571 time: 0.06s
Epoch 115/1000, LR 0.000263
Train loss: 0.9557;  Loss pred: 0.9557; Loss self: 0.0000; time: 0.21s
Val loss: 0.6175 score: 0.9796 time: 0.07s
Test loss: 0.6364 score: 0.8571 time: 0.06s
Epoch 116/1000, LR 0.000263
Train loss: 0.9527;  Loss pred: 0.9527; Loss self: 0.0000; time: 0.21s
Val loss: 0.6151 score: 0.9796 time: 0.07s
Test loss: 0.6346 score: 0.8571 time: 0.06s
Epoch 117/1000, LR 0.000262
Train loss: 0.9523;  Loss pred: 0.9523; Loss self: 0.0000; time: 0.21s
Val loss: 0.6127 score: 0.9796 time: 0.07s
Test loss: 0.6328 score: 0.8571 time: 0.06s
Epoch 118/1000, LR 0.000262
Train loss: 0.9470;  Loss pred: 0.9470; Loss self: 0.0000; time: 0.21s
Val loss: 0.6102 score: 0.9796 time: 0.07s
Test loss: 0.6309 score: 0.8571 time: 0.06s
Epoch 119/1000, LR 0.000262
Train loss: 0.9448;  Loss pred: 0.9448; Loss self: 0.0000; time: 0.21s
Val loss: 0.6076 score: 0.9796 time: 0.07s
Test loss: 0.6290 score: 0.8571 time: 0.16s
Epoch 120/1000, LR 0.000262
Train loss: 0.9427;  Loss pred: 0.9427; Loss self: 0.0000; time: 0.21s
Val loss: 0.6050 score: 0.9796 time: 0.07s
Test loss: 0.6270 score: 0.8571 time: 0.06s
Epoch 121/1000, LR 0.000262
Train loss: 0.9422;  Loss pred: 0.9422; Loss self: 0.0000; time: 0.21s
Val loss: 0.6023 score: 0.9796 time: 0.07s
Test loss: 0.6251 score: 0.8571 time: 0.06s
Epoch 122/1000, LR 0.000262
Train loss: 0.9405;  Loss pred: 0.9405; Loss self: 0.0000; time: 0.21s
Val loss: 0.5996 score: 0.9796 time: 0.07s
Test loss: 0.6230 score: 0.8571 time: 0.06s
Epoch 123/1000, LR 0.000262
Train loss: 0.9367;  Loss pred: 0.9367; Loss self: 0.0000; time: 0.21s
Val loss: 0.5968 score: 0.9796 time: 0.07s
Test loss: 0.6210 score: 0.8571 time: 0.06s
Epoch 124/1000, LR 0.000261
Train loss: 0.9344;  Loss pred: 0.9344; Loss self: 0.0000; time: 0.21s
Val loss: 0.5940 score: 0.9796 time: 0.07s
Test loss: 0.6189 score: 0.8571 time: 0.15s
Epoch 125/1000, LR 0.000261
Train loss: 0.9327;  Loss pred: 0.9327; Loss self: 0.0000; time: 0.22s
Val loss: 0.5911 score: 0.9796 time: 0.07s
Test loss: 0.6167 score: 0.8776 time: 0.06s
Epoch 126/1000, LR 0.000261
Train loss: 0.9302;  Loss pred: 0.9302; Loss self: 0.0000; time: 0.22s
Val loss: 0.5881 score: 0.9796 time: 0.07s
Test loss: 0.6145 score: 0.8776 time: 0.07s
Epoch 127/1000, LR 0.000261
Train loss: 0.9274;  Loss pred: 0.9274; Loss self: 0.0000; time: 0.22s
Val loss: 0.5851 score: 0.9796 time: 0.07s
Test loss: 0.6123 score: 0.8776 time: 0.07s
Epoch 128/1000, LR 0.000261
Train loss: 0.9238;  Loss pred: 0.9238; Loss self: 0.0000; time: 0.22s
Val loss: 0.5821 score: 0.9796 time: 0.07s
Test loss: 0.6100 score: 0.8776 time: 0.07s
Epoch 129/1000, LR 0.000261
Train loss: 0.9208;  Loss pred: 0.9208; Loss self: 0.0000; time: 0.24s
Val loss: 0.5790 score: 0.9796 time: 0.07s
Test loss: 0.6077 score: 0.8776 time: 0.07s
Epoch 130/1000, LR 0.000260
Train loss: 0.9186;  Loss pred: 0.9186; Loss self: 0.0000; time: 0.22s
Val loss: 0.5758 score: 0.9796 time: 0.07s
Test loss: 0.6054 score: 0.8776 time: 0.07s
Epoch 131/1000, LR 0.000260
Train loss: 0.9155;  Loss pred: 0.9155; Loss self: 0.0000; time: 0.22s
Val loss: 0.5727 score: 0.9796 time: 0.07s
Test loss: 0.6030 score: 0.8776 time: 0.07s
Epoch 132/1000, LR 0.000260
Train loss: 0.9134;  Loss pred: 0.9134; Loss self: 0.0000; time: 0.22s
Val loss: 0.5694 score: 0.9796 time: 0.07s
Test loss: 0.6006 score: 0.8776 time: 0.07s
Epoch 133/1000, LR 0.000260
Train loss: 0.9110;  Loss pred: 0.9110; Loss self: 0.0000; time: 0.23s
Val loss: 0.5661 score: 0.9796 time: 0.10s
Test loss: 0.5982 score: 0.8776 time: 0.06s
Epoch 134/1000, LR 0.000260
Train loss: 0.9089;  Loss pred: 0.9089; Loss self: 0.0000; time: 0.21s
Val loss: 0.5628 score: 0.9796 time: 0.07s
Test loss: 0.5957 score: 0.8776 time: 0.06s
Epoch 135/1000, LR 0.000260
Train loss: 0.9073;  Loss pred: 0.9073; Loss self: 0.0000; time: 0.21s
Val loss: 0.5593 score: 0.9796 time: 0.07s
Test loss: 0.5931 score: 0.8776 time: 0.06s
Epoch 136/1000, LR 0.000260
Train loss: 0.9041;  Loss pred: 0.9041; Loss self: 0.0000; time: 0.21s
Val loss: 0.5559 score: 0.9796 time: 0.07s
Test loss: 0.5906 score: 0.8776 time: 0.06s
Epoch 137/1000, LR 0.000259
Train loss: 0.8987;  Loss pred: 0.8987; Loss self: 0.0000; time: 0.21s
Val loss: 0.5524 score: 0.9796 time: 0.07s
Test loss: 0.5880 score: 0.8776 time: 0.06s
Epoch 138/1000, LR 0.000259
Train loss: 0.8987;  Loss pred: 0.8987; Loss self: 0.0000; time: 0.21s
Val loss: 0.5489 score: 0.9796 time: 0.07s
Test loss: 0.5853 score: 0.8776 time: 0.06s
Epoch 139/1000, LR 0.000259
Train loss: 0.8952;  Loss pred: 0.8952; Loss self: 0.0000; time: 0.21s
Val loss: 0.5453 score: 0.9796 time: 0.07s
Test loss: 0.5827 score: 0.8776 time: 0.06s
Epoch 140/1000, LR 0.000259
Train loss: 0.8938;  Loss pred: 0.8938; Loss self: 0.0000; time: 0.21s
Val loss: 0.5417 score: 0.9796 time: 0.07s
Test loss: 0.5800 score: 0.8776 time: 0.06s
Epoch 141/1000, LR 0.000259
Train loss: 0.8904;  Loss pred: 0.8904; Loss self: 0.0000; time: 0.21s
Val loss: 0.5381 score: 0.9796 time: 0.07s
Test loss: 0.5772 score: 0.8776 time: 0.06s
Epoch 142/1000, LR 0.000259
Train loss: 0.8894;  Loss pred: 0.8894; Loss self: 0.0000; time: 0.21s
Val loss: 0.5343 score: 0.9796 time: 0.07s
Test loss: 0.5745 score: 0.8776 time: 0.06s
Epoch 143/1000, LR 0.000258
Train loss: 0.8880;  Loss pred: 0.8880; Loss self: 0.0000; time: 0.21s
Val loss: 0.5305 score: 0.9796 time: 0.07s
Test loss: 0.5717 score: 0.8776 time: 0.06s
Epoch 144/1000, LR 0.000258
Train loss: 0.8823;  Loss pred: 0.8823; Loss self: 0.0000; time: 0.21s
Val loss: 0.5267 score: 0.9796 time: 0.07s
Test loss: 0.5688 score: 0.8776 time: 0.06s
Epoch 145/1000, LR 0.000258
Train loss: 0.8780;  Loss pred: 0.8780; Loss self: 0.0000; time: 0.21s
Val loss: 0.5228 score: 0.9796 time: 0.07s
Test loss: 0.5659 score: 0.8980 time: 0.06s
Epoch 146/1000, LR 0.000258
Train loss: 0.8754;  Loss pred: 0.8754; Loss self: 0.0000; time: 0.21s
Val loss: 0.5188 score: 0.9796 time: 0.07s
Test loss: 0.5630 score: 0.8980 time: 0.06s
Epoch 147/1000, LR 0.000258
Train loss: 0.8748;  Loss pred: 0.8748; Loss self: 0.0000; time: 0.21s
Val loss: 0.5148 score: 0.9796 time: 0.07s
Test loss: 0.5601 score: 0.8980 time: 0.06s
Epoch 148/1000, LR 0.000257
Train loss: 0.8710;  Loss pred: 0.8710; Loss self: 0.0000; time: 0.21s
Val loss: 0.5108 score: 0.9796 time: 0.07s
Test loss: 0.5571 score: 0.8980 time: 0.06s
Epoch 149/1000, LR 0.000257
Train loss: 0.8688;  Loss pred: 0.8688; Loss self: 0.0000; time: 0.21s
Val loss: 0.5067 score: 0.9796 time: 0.07s
Test loss: 0.5542 score: 0.8980 time: 0.06s
Epoch 150/1000, LR 0.000257
Train loss: 0.8656;  Loss pred: 0.8656; Loss self: 0.0000; time: 0.21s
Val loss: 0.5027 score: 0.9796 time: 0.07s
Test loss: 0.5512 score: 0.8980 time: 0.07s
Epoch 151/1000, LR 0.000257
Train loss: 0.8630;  Loss pred: 0.8630; Loss self: 0.0000; time: 0.23s
Val loss: 0.4986 score: 0.9796 time: 0.08s
Test loss: 0.5482 score: 0.8980 time: 0.07s
Epoch 152/1000, LR 0.000257
Train loss: 0.8580;  Loss pred: 0.8580; Loss self: 0.0000; time: 0.24s
Val loss: 0.4944 score: 0.9796 time: 0.08s
Test loss: 0.5452 score: 0.8980 time: 0.07s
Epoch 153/1000, LR 0.000257
Train loss: 0.8556;  Loss pred: 0.8556; Loss self: 0.0000; time: 0.22s
Val loss: 0.4903 score: 0.9796 time: 0.08s
Test loss: 0.5422 score: 0.8980 time: 0.07s
Epoch 154/1000, LR 0.000256
Train loss: 0.8531;  Loss pred: 0.8531; Loss self: 0.0000; time: 0.22s
Val loss: 0.4861 score: 0.9796 time: 0.08s
Test loss: 0.5391 score: 0.8980 time: 0.07s
Epoch 155/1000, LR 0.000256
Train loss: 0.8482;  Loss pred: 0.8482; Loss self: 0.0000; time: 0.22s
Val loss: 0.4819 score: 0.9796 time: 0.07s
Test loss: 0.5361 score: 0.8980 time: 0.07s
Epoch 156/1000, LR 0.000256
Train loss: 0.8483;  Loss pred: 0.8483; Loss self: 0.0000; time: 0.22s
Val loss: 0.4776 score: 0.9796 time: 0.07s
Test loss: 0.5329 score: 0.8980 time: 0.07s
Epoch 157/1000, LR 0.000256
Train loss: 0.8441;  Loss pred: 0.8441; Loss self: 0.0000; time: 0.22s
Val loss: 0.4733 score: 0.9796 time: 0.07s
Test loss: 0.5298 score: 0.8980 time: 0.07s
Epoch 158/1000, LR 0.000256
Train loss: 0.8415;  Loss pred: 0.8415; Loss self: 0.0000; time: 0.22s
Val loss: 0.4690 score: 0.9796 time: 0.07s
Test loss: 0.5266 score: 0.8980 time: 0.07s
Epoch 159/1000, LR 0.000255
Train loss: 0.8386;  Loss pred: 0.8386; Loss self: 0.0000; time: 0.22s
Val loss: 0.4647 score: 0.9796 time: 0.07s
Test loss: 0.5235 score: 0.8980 time: 0.07s
Epoch 160/1000, LR 0.000255
Train loss: 0.8343;  Loss pred: 0.8343; Loss self: 0.0000; time: 0.22s
Val loss: 0.4603 score: 0.9796 time: 0.07s
Test loss: 0.5203 score: 0.8980 time: 0.07s
Epoch 161/1000, LR 0.000255
Train loss: 0.8330;  Loss pred: 0.8330; Loss self: 0.0000; time: 0.22s
Val loss: 0.4560 score: 0.9796 time: 0.07s
Test loss: 0.5171 score: 0.8980 time: 0.07s
Epoch 162/1000, LR 0.000255
Train loss: 0.8294;  Loss pred: 0.8294; Loss self: 0.0000; time: 0.22s
Val loss: 0.4516 score: 0.9796 time: 0.07s
Test loss: 0.5140 score: 0.8980 time: 0.07s
Epoch 163/1000, LR 0.000255
Train loss: 0.8248;  Loss pred: 0.8248; Loss self: 0.0000; time: 0.22s
Val loss: 0.4472 score: 0.9796 time: 0.08s
Test loss: 0.5108 score: 0.8980 time: 0.07s
Epoch 164/1000, LR 0.000254
Train loss: 0.8210;  Loss pred: 0.8210; Loss self: 0.0000; time: 0.22s
Val loss: 0.4428 score: 0.9796 time: 0.07s
Test loss: 0.5076 score: 0.8980 time: 0.09s
Epoch 165/1000, LR 0.000254
Train loss: 0.8206;  Loss pred: 0.8206; Loss self: 0.0000; time: 0.22s
Val loss: 0.4383 score: 0.9796 time: 0.07s
Test loss: 0.5043 score: 0.8980 time: 0.06s
Epoch 166/1000, LR 0.000254
Train loss: 0.8154;  Loss pred: 0.8154; Loss self: 0.0000; time: 0.22s
Val loss: 0.4339 score: 0.9796 time: 0.07s
Test loss: 0.5011 score: 0.8980 time: 0.06s
Epoch 167/1000, LR 0.000254
Train loss: 0.8147;  Loss pred: 0.8147; Loss self: 0.0000; time: 0.21s
Val loss: 0.4295 score: 0.9796 time: 0.07s
Test loss: 0.4978 score: 0.8980 time: 0.07s
Epoch 168/1000, LR 0.000254
Train loss: 0.8105;  Loss pred: 0.8105; Loss self: 0.0000; time: 0.22s
Val loss: 0.4250 score: 0.9796 time: 0.07s
Test loss: 0.4946 score: 0.8980 time: 0.07s
Epoch 169/1000, LR 0.000253
Train loss: 0.8080;  Loss pred: 0.8080; Loss self: 0.0000; time: 0.21s
Val loss: 0.4206 score: 0.9796 time: 0.07s
Test loss: 0.4914 score: 0.8980 time: 0.06s
Epoch 170/1000, LR 0.000253
Train loss: 0.8050;  Loss pred: 0.8050; Loss self: 0.0000; time: 0.21s
Val loss: 0.4161 score: 0.9796 time: 0.07s
Test loss: 0.4882 score: 0.8980 time: 0.06s
Epoch 171/1000, LR 0.000253
Train loss: 0.7995;  Loss pred: 0.7995; Loss self: 0.0000; time: 0.21s
Val loss: 0.4117 score: 0.9796 time: 0.07s
Test loss: 0.4850 score: 0.8980 time: 0.06s
Epoch 172/1000, LR 0.000253
Train loss: 0.7979;  Loss pred: 0.7979; Loss self: 0.0000; time: 0.21s
Val loss: 0.4072 score: 0.9796 time: 0.07s
Test loss: 0.4819 score: 0.8980 time: 0.06s
Epoch 173/1000, LR 0.000253
Train loss: 0.7949;  Loss pred: 0.7949; Loss self: 0.0000; time: 0.21s
Val loss: 0.4028 score: 0.9796 time: 0.07s
Test loss: 0.4788 score: 0.8980 time: 0.08s
Epoch 174/1000, LR 0.000252
Train loss: 0.7917;  Loss pred: 0.7917; Loss self: 0.0000; time: 0.23s
Val loss: 0.3984 score: 0.9796 time: 0.07s
Test loss: 0.4756 score: 0.8980 time: 0.07s
Epoch 175/1000, LR 0.000252
Train loss: 0.7874;  Loss pred: 0.7874; Loss self: 0.0000; time: 0.20s
Val loss: 0.3940 score: 0.9796 time: 0.07s
Test loss: 0.4725 score: 0.8980 time: 0.06s
Epoch 176/1000, LR 0.000252
Train loss: 0.7868;  Loss pred: 0.7868; Loss self: 0.0000; time: 0.20s
Val loss: 0.3896 score: 0.9796 time: 0.06s
Test loss: 0.4693 score: 0.8980 time: 0.06s
Epoch 177/1000, LR 0.000252
Train loss: 0.7822;  Loss pred: 0.7822; Loss self: 0.0000; time: 0.22s
Val loss: 0.3852 score: 0.9796 time: 0.07s
Test loss: 0.4661 score: 0.8980 time: 0.06s
Epoch 178/1000, LR 0.000251
Train loss: 0.7812;  Loss pred: 0.7812; Loss self: 0.0000; time: 0.19s
Val loss: 0.3808 score: 0.9796 time: 0.06s
Test loss: 0.4630 score: 0.8980 time: 0.06s
Epoch 179/1000, LR 0.000251
Train loss: 0.7766;  Loss pred: 0.7766; Loss self: 0.0000; time: 0.20s
Val loss: 0.3765 score: 0.9796 time: 0.07s
Test loss: 0.4598 score: 0.8980 time: 0.07s
Epoch 180/1000, LR 0.000251
Train loss: 0.7742;  Loss pred: 0.7742; Loss self: 0.0000; time: 0.20s
Val loss: 0.3721 score: 0.9796 time: 0.07s
Test loss: 0.4567 score: 0.8980 time: 0.06s
Epoch 181/1000, LR 0.000251
Train loss: 0.7734;  Loss pred: 0.7734; Loss self: 0.0000; time: 0.19s
Val loss: 0.3678 score: 0.9796 time: 0.06s
Test loss: 0.4536 score: 0.8980 time: 0.06s
Epoch 182/1000, LR 0.000251
Train loss: 0.7684;  Loss pred: 0.7684; Loss self: 0.0000; time: 0.20s
Val loss: 0.3635 score: 0.9796 time: 0.07s
Test loss: 0.4506 score: 0.8980 time: 0.06s
Epoch 183/1000, LR 0.000250
Train loss: 0.7679;  Loss pred: 0.7679; Loss self: 0.0000; time: 0.19s
Val loss: 0.3593 score: 0.9796 time: 0.06s
Test loss: 0.4475 score: 0.8980 time: 0.06s
Epoch 184/1000, LR 0.000250
Train loss: 0.7622;  Loss pred: 0.7622; Loss self: 0.0000; time: 0.19s
Val loss: 0.3550 score: 0.9796 time: 0.06s
Test loss: 0.4445 score: 0.8980 time: 0.06s
Epoch 185/1000, LR 0.000250
Train loss: 0.7618;  Loss pred: 0.7618; Loss self: 0.0000; time: 0.19s
Val loss: 0.3508 score: 0.9796 time: 0.07s
Test loss: 0.4416 score: 0.8980 time: 0.06s
Epoch 186/1000, LR 0.000250
Train loss: 0.7566;  Loss pred: 0.7566; Loss self: 0.0000; time: 0.20s
Val loss: 0.3466 score: 0.9796 time: 0.07s
Test loss: 0.4386 score: 0.8980 time: 0.06s
Epoch 187/1000, LR 0.000249
Train loss: 0.7533;  Loss pred: 0.7533; Loss self: 0.0000; time: 0.20s
Val loss: 0.3425 score: 0.9796 time: 0.07s
Test loss: 0.4356 score: 0.8980 time: 0.06s
Epoch 188/1000, LR 0.000249
Train loss: 0.7503;  Loss pred: 0.7503; Loss self: 0.0000; time: 0.20s
Val loss: 0.3383 score: 0.9796 time: 0.07s
Test loss: 0.4327 score: 0.8980 time: 0.06s
Epoch 189/1000, LR 0.000249
Train loss: 0.7483;  Loss pred: 0.7483; Loss self: 0.0000; time: 0.20s
Val loss: 0.3342 score: 0.9796 time: 0.07s
Test loss: 0.4298 score: 0.8980 time: 0.06s
Epoch 190/1000, LR 0.000249
Train loss: 0.7451;  Loss pred: 0.7451; Loss self: 0.0000; time: 0.20s
Val loss: 0.3301 score: 0.9796 time: 0.07s
Test loss: 0.4269 score: 0.8980 time: 0.06s
Epoch 191/1000, LR 0.000249
Train loss: 0.7421;  Loss pred: 0.7421; Loss self: 0.0000; time: 0.20s
Val loss: 0.3261 score: 0.9796 time: 0.07s
Test loss: 0.4240 score: 0.8980 time: 0.06s
Epoch 192/1000, LR 0.000248
Train loss: 0.7398;  Loss pred: 0.7398; Loss self: 0.0000; time: 0.21s
Val loss: 0.3220 score: 0.9796 time: 0.07s
Test loss: 0.4211 score: 0.8980 time: 0.06s
Epoch 193/1000, LR 0.000248
Train loss: 0.7382;  Loss pred: 0.7382; Loss self: 0.0000; time: 0.21s
Val loss: 0.3180 score: 0.9796 time: 0.07s
Test loss: 0.4183 score: 0.8980 time: 0.06s
Epoch 194/1000, LR 0.000248
Train loss: 0.7356;  Loss pred: 0.7356; Loss self: 0.0000; time: 0.23s
Val loss: 0.3141 score: 0.9796 time: 0.06s
Test loss: 0.4154 score: 0.8980 time: 0.06s
Epoch 195/1000, LR 0.000248
Train loss: 0.7360;  Loss pred: 0.7360; Loss self: 0.0000; time: 0.19s
Val loss: 0.3101 score: 0.9796 time: 0.06s
Test loss: 0.4126 score: 0.8980 time: 0.31s
Epoch 196/1000, LR 0.000247
Train loss: 0.7304;  Loss pred: 0.7304; Loss self: 0.0000; time: 6.54s
Val loss: 0.3062 score: 0.9796 time: 1.41s
Test loss: 0.4098 score: 0.8980 time: 2.01s
Epoch 197/1000, LR 0.000247
Train loss: 0.7288;  Loss pred: 0.7288; Loss self: 0.0000; time: 3.91s
Val loss: 0.3024 score: 0.9796 time: 0.08s
Test loss: 0.4070 score: 0.8980 time: 0.07s
Epoch 198/1000, LR 0.000247
Train loss: 0.7227;  Loss pred: 0.7227; Loss self: 0.0000; time: 0.22s
Val loss: 0.2986 score: 0.9796 time: 0.07s
Test loss: 0.4044 score: 0.8980 time: 0.06s
Epoch 199/1000, LR 0.000247
Train loss: 0.7207;  Loss pred: 0.7207; Loss self: 0.0000; time: 0.19s
Val loss: 0.2949 score: 0.9796 time: 0.07s
Test loss: 0.4018 score: 0.8980 time: 0.06s
Epoch 200/1000, LR 0.000246
Train loss: 0.7173;  Loss pred: 0.7173; Loss self: 0.0000; time: 0.20s
Val loss: 0.2912 score: 0.9796 time: 0.06s
Test loss: 0.3992 score: 0.8980 time: 0.06s
Epoch 201/1000, LR 0.000246
Train loss: 0.7165;  Loss pred: 0.7165; Loss self: 0.0000; time: 0.20s
Val loss: 0.2875 score: 0.9796 time: 0.06s
Test loss: 0.3967 score: 0.8980 time: 0.06s
Epoch 202/1000, LR 0.000246
Train loss: 0.7141;  Loss pred: 0.7141; Loss self: 0.0000; time: 0.19s
Val loss: 0.2839 score: 0.9796 time: 0.06s
Test loss: 0.3942 score: 0.8980 time: 0.06s
Epoch 203/1000, LR 0.000246
Train loss: 0.7115;  Loss pred: 0.7115; Loss self: 0.0000; time: 0.19s
Val loss: 0.2803 score: 0.9796 time: 0.06s
Test loss: 0.3917 score: 0.8980 time: 0.06s
Epoch 204/1000, LR 0.000245
Train loss: 0.7084;  Loss pred: 0.7084; Loss self: 0.0000; time: 0.19s
Val loss: 0.2767 score: 0.9796 time: 0.06s
Test loss: 0.3893 score: 0.8980 time: 0.06s
Epoch 205/1000, LR 0.000245
Train loss: 0.7078;  Loss pred: 0.7078; Loss self: 0.0000; time: 0.19s
Val loss: 0.2731 score: 0.9796 time: 0.06s
Test loss: 0.3869 score: 0.8980 time: 0.06s
Epoch 206/1000, LR 0.000245
Train loss: 0.7036;  Loss pred: 0.7036; Loss self: 0.0000; time: 0.19s
Val loss: 0.2695 score: 0.9796 time: 0.06s
Test loss: 0.3844 score: 0.8980 time: 0.06s
Epoch 207/1000, LR 0.000245
Train loss: 0.7017;  Loss pred: 0.7017; Loss self: 0.0000; time: 0.19s
Val loss: 0.2660 score: 0.9796 time: 0.06s
Test loss: 0.3819 score: 0.8980 time: 0.06s
Epoch 208/1000, LR 0.000244
Train loss: 0.6991;  Loss pred: 0.6991; Loss self: 0.0000; time: 0.19s
Val loss: 0.2625 score: 0.9796 time: 0.06s
Test loss: 0.3794 score: 0.8980 time: 0.06s
Epoch 209/1000, LR 0.000244
Train loss: 0.6971;  Loss pred: 0.6971; Loss self: 0.0000; time: 0.19s
Val loss: 0.2590 score: 0.9796 time: 0.06s
Test loss: 0.3770 score: 0.8980 time: 0.06s
Epoch 210/1000, LR 0.000244
Train loss: 0.6951;  Loss pred: 0.6951; Loss self: 0.0000; time: 0.19s
Val loss: 0.2556 score: 0.9796 time: 0.06s
Test loss: 0.3746 score: 0.8980 time: 0.06s
Epoch 211/1000, LR 0.000244
Train loss: 0.6936;  Loss pred: 0.6936; Loss self: 0.0000; time: 1.87s
Val loss: 0.2523 score: 0.9796 time: 1.66s
Test loss: 0.3722 score: 0.8980 time: 1.43s
Epoch 212/1000, LR 0.000243
Train loss: 0.6908;  Loss pred: 0.6908; Loss self: 0.0000; time: 0.32s
Val loss: 0.2489 score: 0.9796 time: 0.07s
Test loss: 0.3699 score: 0.8980 time: 0.06s
Epoch 213/1000, LR 0.000243
Train loss: 0.6896;  Loss pred: 0.6896; Loss self: 0.0000; time: 0.22s
Val loss: 0.2457 score: 0.9796 time: 0.07s
Test loss: 0.3676 score: 0.8980 time: 0.06s
Epoch 214/1000, LR 0.000243
Train loss: 0.6843;  Loss pred: 0.6843; Loss self: 0.0000; time: 0.21s
Val loss: 0.2424 score: 0.9796 time: 0.07s
Test loss: 0.3653 score: 0.8980 time: 0.06s
Epoch 215/1000, LR 0.000243
Train loss: 0.6826;  Loss pred: 0.6826; Loss self: 0.0000; time: 0.21s
Val loss: 0.2393 score: 0.9796 time: 0.07s
Test loss: 0.3630 score: 0.8980 time: 0.06s
Epoch 216/1000, LR 0.000242
Train loss: 0.6823;  Loss pred: 0.6823; Loss self: 0.0000; time: 0.21s
Val loss: 0.2361 score: 0.9796 time: 0.07s
Test loss: 0.3608 score: 0.8980 time: 0.06s
Epoch 217/1000, LR 0.000242
Train loss: 0.6795;  Loss pred: 0.6795; Loss self: 0.0000; time: 0.21s
Val loss: 0.2331 score: 0.9796 time: 0.07s
Test loss: 0.3588 score: 0.8980 time: 0.06s
Epoch 218/1000, LR 0.000242
Train loss: 0.6774;  Loss pred: 0.6774; Loss self: 0.0000; time: 0.21s
Val loss: 0.2301 score: 0.9796 time: 0.07s
Test loss: 0.3567 score: 0.8980 time: 0.06s
Epoch 219/1000, LR 0.000242
Train loss: 0.6768;  Loss pred: 0.6768; Loss self: 0.0000; time: 0.21s
Val loss: 0.2271 score: 0.9796 time: 0.07s
Test loss: 0.3546 score: 0.8980 time: 0.06s
Epoch 220/1000, LR 0.000241
Train loss: 0.6716;  Loss pred: 0.6716; Loss self: 0.0000; time: 0.21s
Val loss: 0.2242 score: 0.9796 time: 0.07s
Test loss: 0.3527 score: 0.8980 time: 0.06s
Epoch 221/1000, LR 0.000241
Train loss: 0.6704;  Loss pred: 0.6704; Loss self: 0.0000; time: 0.21s
Val loss: 0.2213 score: 0.9796 time: 0.07s
Test loss: 0.3507 score: 0.8980 time: 0.06s
Epoch 222/1000, LR 0.000241
Train loss: 0.6695;  Loss pred: 0.6695; Loss self: 0.0000; time: 0.20s
Val loss: 0.2185 score: 0.9796 time: 0.07s
Test loss: 0.3488 score: 0.8980 time: 0.06s
Epoch 223/1000, LR 0.000241
Train loss: 0.6660;  Loss pred: 0.6660; Loss self: 0.0000; time: 0.21s
Val loss: 0.2157 score: 0.9796 time: 0.07s
Test loss: 0.3469 score: 0.8980 time: 0.06s
Epoch 224/1000, LR 0.000240
Train loss: 0.6647;  Loss pred: 0.6647; Loss self: 0.0000; time: 0.21s
Val loss: 0.2130 score: 0.9796 time: 0.07s
Test loss: 0.3451 score: 0.8980 time: 0.06s
Epoch 225/1000, LR 0.000240
Train loss: 0.6615;  Loss pred: 0.6615; Loss self: 0.0000; time: 0.21s
Val loss: 0.2103 score: 0.9796 time: 0.06s
Test loss: 0.3432 score: 0.8980 time: 0.06s
Epoch 226/1000, LR 0.000240
Train loss: 0.6616;  Loss pred: 0.6616; Loss self: 0.0000; time: 4.23s
Val loss: 0.2076 score: 0.9796 time: 2.49s
Test loss: 0.3414 score: 0.8980 time: 1.40s
Epoch 227/1000, LR 0.000240
Train loss: 0.6581;  Loss pred: 0.6581; Loss self: 0.0000; time: 1.48s
Val loss: 0.2050 score: 0.9796 time: 0.15s
Test loss: 0.3396 score: 0.8980 time: 0.32s
Epoch 228/1000, LR 0.000239
Train loss: 0.6557;  Loss pred: 0.6557; Loss self: 0.0000; time: 0.33s
Val loss: 0.2025 score: 0.9796 time: 0.07s
Test loss: 0.3379 score: 0.8980 time: 0.06s
Epoch 229/1000, LR 0.000239
Train loss: 0.6573;  Loss pred: 0.6573; Loss self: 0.0000; time: 0.20s
Val loss: 0.2000 score: 0.9796 time: 0.06s
Test loss: 0.3361 score: 0.8980 time: 0.06s
Epoch 230/1000, LR 0.000239
Train loss: 0.6537;  Loss pred: 0.6537; Loss self: 0.0000; time: 0.22s
Val loss: 0.1975 score: 0.9796 time: 0.12s
Test loss: 0.3344 score: 0.8980 time: 0.06s
Epoch 231/1000, LR 0.000238
Train loss: 0.6509;  Loss pred: 0.6509; Loss self: 0.0000; time: 0.20s
Val loss: 0.1950 score: 0.9796 time: 0.06s
Test loss: 0.3326 score: 0.8980 time: 0.06s
Epoch 232/1000, LR 0.000238
Train loss: 0.6493;  Loss pred: 0.6493; Loss self: 0.0000; time: 0.20s
Val loss: 0.1926 score: 0.9796 time: 0.07s
Test loss: 0.3309 score: 0.8980 time: 0.06s
Epoch 233/1000, LR 0.000238
Train loss: 0.6493;  Loss pred: 0.6493; Loss self: 0.0000; time: 0.20s
Val loss: 0.1903 score: 0.9796 time: 0.07s
Test loss: 0.3292 score: 0.8980 time: 0.06s
Epoch 234/1000, LR 0.000238
Train loss: 0.6449;  Loss pred: 0.6449; Loss self: 0.0000; time: 0.20s
Val loss: 0.1879 score: 0.9796 time: 0.07s
Test loss: 0.3275 score: 0.8980 time: 0.06s
Epoch 235/1000, LR 0.000237
Train loss: 0.6467;  Loss pred: 0.6467; Loss self: 0.0000; time: 0.20s
Val loss: 0.1856 score: 0.9796 time: 0.07s
Test loss: 0.3259 score: 0.8980 time: 0.06s
Epoch 236/1000, LR 0.000237
Train loss: 0.6425;  Loss pred: 0.6425; Loss self: 0.0000; time: 0.20s
Val loss: 0.1834 score: 0.9796 time: 0.07s
Test loss: 0.3243 score: 0.8980 time: 0.06s
Epoch 237/1000, LR 0.000237
Train loss: 0.6401;  Loss pred: 0.6401; Loss self: 0.0000; time: 0.20s
Val loss: 0.1812 score: 0.9796 time: 0.07s
Test loss: 0.3227 score: 0.8980 time: 0.25s
Epoch 238/1000, LR 0.000236
Train loss: 0.6396;  Loss pred: 0.6396; Loss self: 0.0000; time: 6.47s
Val loss: 0.1790 score: 0.9796 time: 0.93s
Test loss: 0.3213 score: 0.8980 time: 0.07s
Epoch 239/1000, LR 0.000236
Train loss: 0.6384;  Loss pred: 0.6384; Loss self: 0.0000; time: 0.20s
Val loss: 0.1769 score: 0.9796 time: 0.07s
Test loss: 0.3198 score: 0.8980 time: 0.06s
Epoch 240/1000, LR 0.000236
Train loss: 0.6365;  Loss pred: 0.6365; Loss self: 0.0000; time: 0.20s
Val loss: 0.1748 score: 0.9796 time: 0.06s
Test loss: 0.3183 score: 0.8980 time: 0.06s
Epoch 241/1000, LR 0.000236
Train loss: 0.6366;  Loss pred: 0.6366; Loss self: 0.0000; time: 0.19s
Val loss: 0.1727 score: 0.9796 time: 0.06s
Test loss: 0.3169 score: 0.8980 time: 0.06s
Epoch 242/1000, LR 0.000235
Train loss: 0.6370;  Loss pred: 0.6370; Loss self: 0.0000; time: 0.19s
Val loss: 0.1707 score: 0.9796 time: 0.06s
Test loss: 0.3155 score: 0.8980 time: 0.06s
Epoch 243/1000, LR 0.000235
Train loss: 0.6329;  Loss pred: 0.6329; Loss self: 0.0000; time: 0.22s
Val loss: 0.1687 score: 0.9796 time: 0.07s
Test loss: 0.3140 score: 0.8980 time: 0.07s
Epoch 244/1000, LR 0.000235
Train loss: 0.6335;  Loss pred: 0.6335; Loss self: 0.0000; time: 0.20s
Val loss: 0.1667 score: 0.9796 time: 0.08s
Test loss: 0.3126 score: 0.8980 time: 0.06s
Epoch 245/1000, LR 0.000234
Train loss: 0.6281;  Loss pred: 0.6281; Loss self: 0.0000; time: 0.20s
Val loss: 0.1648 score: 0.9796 time: 0.07s
Test loss: 0.3113 score: 0.8980 time: 0.06s
Epoch 246/1000, LR 0.000234
Train loss: 0.6268;  Loss pred: 0.6268; Loss self: 0.0000; time: 0.21s
Val loss: 0.1629 score: 0.9796 time: 0.07s
Test loss: 0.3100 score: 0.8980 time: 0.07s
Epoch 247/1000, LR 0.000234
Train loss: 0.6262;  Loss pred: 0.6262; Loss self: 0.0000; time: 0.21s
Val loss: 0.1610 score: 0.9796 time: 0.07s
Test loss: 0.3087 score: 0.8980 time: 0.06s
Epoch 248/1000, LR 0.000234
Train loss: 0.6234;  Loss pred: 0.6234; Loss self: 0.0000; time: 3.85s
Val loss: 0.1592 score: 0.9796 time: 0.86s
Test loss: 0.3073 score: 0.8980 time: 0.23s
Epoch 249/1000, LR 0.000233
Train loss: 0.6214;  Loss pred: 0.6214; Loss self: 0.0000; time: 7.26s
Val loss: 0.1573 score: 0.9796 time: 2.62s
Test loss: 0.3060 score: 0.8980 time: 0.10s
Epoch 250/1000, LR 0.000233
Train loss: 0.6213;  Loss pred: 0.6213; Loss self: 0.0000; time: 0.20s
Val loss: 0.1555 score: 0.9796 time: 0.07s
Test loss: 0.3047 score: 0.8980 time: 0.06s
Epoch 251/1000, LR 0.000233
Train loss: 0.6201;  Loss pred: 0.6201; Loss self: 0.0000; time: 0.20s
Val loss: 0.1538 score: 0.9796 time: 0.07s
Test loss: 0.3034 score: 0.8980 time: 0.06s
Epoch 252/1000, LR 0.000232
Train loss: 0.6201;  Loss pred: 0.6201; Loss self: 0.0000; time: 0.20s
Val loss: 0.1521 score: 0.9796 time: 0.07s
Test loss: 0.3021 score: 0.8980 time: 0.06s
Epoch 253/1000, LR 0.000232
Train loss: 0.6163;  Loss pred: 0.6163; Loss self: 0.0000; time: 0.21s
Val loss: 0.1504 score: 0.9796 time: 0.07s
Test loss: 0.3010 score: 0.8980 time: 0.06s
Epoch 254/1000, LR 0.000232
Train loss: 0.6164;  Loss pred: 0.6164; Loss self: 0.0000; time: 0.21s
Val loss: 0.1488 score: 0.9796 time: 0.07s
Test loss: 0.3000 score: 0.8980 time: 0.06s
Epoch 255/1000, LR 0.000232
Train loss: 0.6170;  Loss pred: 0.6170; Loss self: 0.0000; time: 0.21s
Val loss: 0.1472 score: 0.9796 time: 0.07s
Test loss: 0.2990 score: 0.8980 time: 0.06s
Epoch 256/1000, LR 0.000231
Train loss: 0.6146;  Loss pred: 0.6146; Loss self: 0.0000; time: 0.21s
Val loss: 0.1456 score: 0.9796 time: 0.07s
Test loss: 0.2980 score: 0.8980 time: 1.24s
Epoch 257/1000, LR 0.000231
Train loss: 0.6142;  Loss pred: 0.6142; Loss self: 0.0000; time: 5.87s
Val loss: 0.1440 score: 0.9796 time: 0.48s
Test loss: 0.2969 score: 0.8980 time: 0.07s
Epoch 258/1000, LR 0.000231
Train loss: 0.6122;  Loss pred: 0.6122; Loss self: 0.0000; time: 0.22s
Val loss: 0.1425 score: 0.9796 time: 0.07s
Test loss: 0.2958 score: 0.8980 time: 0.06s
Epoch 259/1000, LR 0.000230
Train loss: 0.6095;  Loss pred: 0.6095; Loss self: 0.0000; time: 0.21s
Val loss: 0.1410 score: 0.9796 time: 0.07s
Test loss: 0.2947 score: 0.8980 time: 0.06s
Epoch 260/1000, LR 0.000230
Train loss: 0.6103;  Loss pred: 0.6103; Loss self: 0.0000; time: 0.22s
Val loss: 0.1395 score: 0.9796 time: 0.07s
Test loss: 0.2936 score: 0.8980 time: 0.06s
Epoch 261/1000, LR 0.000230
Train loss: 0.6100;  Loss pred: 0.6100; Loss self: 0.0000; time: 0.22s
Val loss: 0.1380 score: 0.9796 time: 0.07s
Test loss: 0.2925 score: 0.8980 time: 0.06s
Epoch 262/1000, LR 0.000229
Train loss: 0.6106;  Loss pred: 0.6106; Loss self: 0.0000; time: 0.21s
Val loss: 0.1365 score: 0.9796 time: 0.07s
Test loss: 0.2913 score: 0.8980 time: 0.06s
Epoch 263/1000, LR 0.000229
Train loss: 0.6069;  Loss pred: 0.6069; Loss self: 0.0000; time: 0.21s
Val loss: 0.1351 score: 0.9796 time: 0.07s
Test loss: 0.2902 score: 0.8980 time: 0.06s
Epoch 264/1000, LR 0.000229
Train loss: 0.6047;  Loss pred: 0.6047; Loss self: 0.0000; time: 0.21s
Val loss: 0.1337 score: 0.9796 time: 0.07s
Test loss: 0.2891 score: 0.8980 time: 0.07s
Epoch 265/1000, LR 0.000228
Train loss: 0.6049;  Loss pred: 0.6049; Loss self: 0.0000; time: 0.23s
Val loss: 0.1323 score: 0.9796 time: 0.08s
Test loss: 0.2880 score: 0.8980 time: 0.07s
Epoch 266/1000, LR 0.000228
Train loss: 0.6034;  Loss pred: 0.6034; Loss self: 0.0000; time: 0.23s
Val loss: 0.1310 score: 0.9796 time: 0.08s
Test loss: 0.2870 score: 0.8980 time: 0.07s
Epoch 267/1000, LR 0.000228
Train loss: 0.6024;  Loss pred: 0.6024; Loss self: 0.0000; time: 0.23s
Val loss: 0.1297 score: 0.9796 time: 0.08s
Test loss: 0.2862 score: 0.8980 time: 0.07s
Epoch 268/1000, LR 0.000228
Train loss: 0.6017;  Loss pred: 0.6017; Loss self: 0.0000; time: 0.23s
Val loss: 0.1284 score: 0.9796 time: 0.08s
Test loss: 0.2853 score: 0.8980 time: 0.07s
Epoch 269/1000, LR 0.000227
Train loss: 0.5980;  Loss pred: 0.5980; Loss self: 0.0000; time: 0.23s
Val loss: 0.1271 score: 0.9796 time: 0.20s
Test loss: 0.2846 score: 0.8980 time: 2.26s
Epoch 270/1000, LR 0.000227
Train loss: 0.5981;  Loss pred: 0.5981; Loss self: 0.0000; time: 6.28s
Val loss: 0.1259 score: 0.9796 time: 0.10s
Test loss: 0.2838 score: 0.8980 time: 0.06s
Epoch 271/1000, LR 0.000227
Train loss: 0.5965;  Loss pred: 0.5965; Loss self: 0.0000; time: 0.21s
Val loss: 0.1247 score: 0.9796 time: 0.07s
Test loss: 0.2830 score: 0.8980 time: 0.07s
Epoch 272/1000, LR 0.000226
Train loss: 0.6001;  Loss pred: 0.6001; Loss self: 0.0000; time: 0.21s
Val loss: 0.1235 score: 0.9796 time: 0.07s
Test loss: 0.2821 score: 0.8980 time: 0.06s
Epoch 273/1000, LR 0.000226
Train loss: 0.5976;  Loss pred: 0.5976; Loss self: 0.0000; time: 0.21s
Val loss: 0.1223 score: 0.9796 time: 0.07s
Test loss: 0.2812 score: 0.8980 time: 0.06s
Epoch 274/1000, LR 0.000226
Train loss: 0.5941;  Loss pred: 0.5941; Loss self: 0.0000; time: 0.21s
Val loss: 0.1211 score: 0.9796 time: 0.07s
Test loss: 0.2804 score: 0.8980 time: 0.06s
Epoch 275/1000, LR 0.000225
Train loss: 0.5983;  Loss pred: 0.5983; Loss self: 0.0000; time: 0.21s
Val loss: 0.1200 score: 0.9796 time: 0.07s
Test loss: 0.2795 score: 0.8980 time: 0.06s
Epoch 276/1000, LR 0.000225
Train loss: 0.5929;  Loss pred: 0.5929; Loss self: 0.0000; time: 0.32s
Val loss: 0.1188 score: 0.9796 time: 0.07s
Test loss: 0.2785 score: 0.8980 time: 0.07s
Epoch 277/1000, LR 0.000225
Train loss: 0.5951;  Loss pred: 0.5951; Loss self: 0.0000; time: 0.22s
Val loss: 0.1177 score: 0.9796 time: 0.07s
Test loss: 0.2776 score: 0.8980 time: 0.07s
Epoch 278/1000, LR 0.000224
Train loss: 0.5926;  Loss pred: 0.5926; Loss self: 0.0000; time: 0.22s
Val loss: 0.1166 score: 0.9796 time: 0.08s
Test loss: 0.2768 score: 0.8980 time: 0.07s
Epoch 279/1000, LR 0.000224
Train loss: 0.5921;  Loss pred: 0.5921; Loss self: 0.0000; time: 0.22s
Val loss: 0.1155 score: 0.9796 time: 0.07s
Test loss: 0.2760 score: 0.8980 time: 0.07s
Epoch 280/1000, LR 0.000224
Train loss: 0.5898;  Loss pred: 0.5898; Loss self: 0.0000; time: 0.24s
Val loss: 0.1145 score: 0.9796 time: 0.09s
Test loss: 0.2752 score: 0.8980 time: 0.07s
Epoch 281/1000, LR 0.000223
Train loss: 0.5900;  Loss pred: 0.5900; Loss self: 0.0000; time: 2.37s
Val loss: 0.1134 score: 0.9796 time: 1.22s
Test loss: 0.2744 score: 0.8980 time: 1.40s
Epoch 282/1000, LR 0.000223
Train loss: 0.5878;  Loss pred: 0.5878; Loss self: 0.0000; time: 3.07s
Val loss: 0.1124 score: 0.9796 time: 0.08s
Test loss: 0.2737 score: 0.8980 time: 0.07s
Epoch 283/1000, LR 0.000223
Train loss: 0.5857;  Loss pred: 0.5857; Loss self: 0.0000; time: 0.22s
Val loss: 0.1114 score: 0.9796 time: 0.07s
Test loss: 0.2730 score: 0.8980 time: 0.06s
Epoch 284/1000, LR 0.000222
Train loss: 0.5878;  Loss pred: 0.5878; Loss self: 0.0000; time: 0.21s
Val loss: 0.1105 score: 0.9796 time: 0.07s
Test loss: 0.2724 score: 0.8980 time: 0.06s
Epoch 285/1000, LR 0.000222
Train loss: 0.5872;  Loss pred: 0.5872; Loss self: 0.0000; time: 0.21s
Val loss: 0.1095 score: 0.9796 time: 0.07s
Test loss: 0.2717 score: 0.8980 time: 0.06s
Epoch 286/1000, LR 0.000222
Train loss: 0.5845;  Loss pred: 0.5845; Loss self: 0.0000; time: 0.21s
Val loss: 0.1085 score: 0.9796 time: 0.07s
Test loss: 0.2710 score: 0.8980 time: 0.06s
Epoch 287/1000, LR 0.000221
Train loss: 0.5839;  Loss pred: 0.5839; Loss self: 0.0000; time: 0.21s
Val loss: 0.1076 score: 0.9796 time: 0.07s
Test loss: 0.2703 score: 0.8980 time: 0.06s
Epoch 288/1000, LR 0.000221
Train loss: 0.5838;  Loss pred: 0.5838; Loss self: 0.0000; time: 0.21s
Val loss: 0.1067 score: 0.9796 time: 0.07s
Test loss: 0.2697 score: 0.8980 time: 0.06s
Epoch 289/1000, LR 0.000221
Train loss: 0.5830;  Loss pred: 0.5830; Loss self: 0.0000; time: 0.21s
Val loss: 0.1058 score: 0.9796 time: 0.07s
Test loss: 0.2691 score: 0.8980 time: 0.06s
Epoch 290/1000, LR 0.000220
Train loss: 0.5804;  Loss pred: 0.5804; Loss self: 0.0000; time: 0.21s
Val loss: 0.1049 score: 0.9796 time: 0.07s
Test loss: 0.2686 score: 0.8776 time: 0.06s
Epoch 291/1000, LR 0.000220
Train loss: 0.5841;  Loss pred: 0.5841; Loss self: 0.0000; time: 0.21s
Val loss: 0.1041 score: 0.9796 time: 0.07s
Test loss: 0.2682 score: 0.8776 time: 0.06s
Epoch 292/1000, LR 0.000220
Train loss: 0.5779;  Loss pred: 0.5779; Loss self: 0.0000; time: 0.21s
Val loss: 0.1033 score: 0.9796 time: 0.07s
Test loss: 0.2678 score: 0.8776 time: 0.06s
Epoch 293/1000, LR 0.000219
Train loss: 0.5806;  Loss pred: 0.5806; Loss self: 0.0000; time: 0.21s
Val loss: 0.1025 score: 0.9796 time: 0.07s
Test loss: 0.2673 score: 0.8776 time: 0.06s
Epoch 294/1000, LR 0.000219
Train loss: 0.5778;  Loss pred: 0.5778; Loss self: 0.0000; time: 0.20s
Val loss: 0.1017 score: 0.9796 time: 0.07s
Test loss: 0.2668 score: 0.8776 time: 0.06s
Epoch 295/1000, LR 0.000219
Train loss: 0.5803;  Loss pred: 0.5803; Loss self: 0.0000; time: 0.21s
Val loss: 0.1008 score: 0.9796 time: 0.07s
Test loss: 0.2662 score: 0.8776 time: 0.06s
Epoch 296/1000, LR 0.000218
Train loss: 0.5792;  Loss pred: 0.5792; Loss self: 0.0000; time: 5.06s
Val loss: 0.1000 score: 0.9796 time: 0.97s
Test loss: 0.2655 score: 0.8776 time: 0.07s
Epoch 297/1000, LR 0.000218
Train loss: 0.5779;  Loss pred: 0.5779; Loss self: 0.0000; time: 0.23s
Val loss: 0.0992 score: 0.9796 time: 0.08s
Test loss: 0.2648 score: 0.8776 time: 0.07s
Epoch 298/1000, LR 0.000218
Train loss: 0.5787;  Loss pred: 0.5787; Loss self: 0.0000; time: 0.23s
Val loss: 0.0984 score: 0.9796 time: 0.08s
Test loss: 0.2640 score: 0.8776 time: 0.06s
Epoch 299/1000, LR 0.000217
Train loss: 0.5753;  Loss pred: 0.5753; Loss self: 0.0000; time: 0.21s
Val loss: 0.0976 score: 0.9796 time: 0.07s
Test loss: 0.2633 score: 0.8776 time: 0.06s
Epoch 300/1000, LR 0.000217
Train loss: 0.5743;  Loss pred: 0.5743; Loss self: 0.0000; time: 0.21s
Val loss: 0.0968 score: 0.9796 time: 0.07s
Test loss: 0.2626 score: 0.8776 time: 0.06s
Epoch 301/1000, LR 0.000217
Train loss: 0.5738;  Loss pred: 0.5738; Loss self: 0.0000; time: 0.21s
Val loss: 0.0960 score: 0.9796 time: 0.07s
Test loss: 0.2620 score: 0.8776 time: 0.06s
Epoch 302/1000, LR 0.000216
Train loss: 0.5737;  Loss pred: 0.5737; Loss self: 0.0000; time: 0.21s
Val loss: 0.0953 score: 0.9796 time: 0.07s
Test loss: 0.2613 score: 0.8980 time: 0.06s
Epoch 303/1000, LR 0.000216
Train loss: 0.5723;  Loss pred: 0.5723; Loss self: 0.0000; time: 0.21s
Val loss: 0.0945 score: 0.9796 time: 0.07s
Test loss: 0.2607 score: 0.8980 time: 0.06s
Epoch 304/1000, LR 0.000216
Train loss: 0.5729;  Loss pred: 0.5729; Loss self: 0.0000; time: 0.21s
Val loss: 0.0938 score: 0.9796 time: 0.07s
Test loss: 0.2602 score: 0.8980 time: 0.06s
Epoch 305/1000, LR 0.000215
Train loss: 0.5716;  Loss pred: 0.5716; Loss self: 0.0000; time: 0.21s
Val loss: 0.0931 score: 0.9796 time: 0.07s
Test loss: 0.2597 score: 0.8980 time: 0.06s
Epoch 306/1000, LR 0.000215
Train loss: 0.5741;  Loss pred: 0.5741; Loss self: 0.0000; time: 0.22s
Val loss: 0.0924 score: 0.9796 time: 2.35s
Test loss: 0.2593 score: 0.8776 time: 1.27s
Epoch 307/1000, LR 0.000215
Train loss: 0.5711;  Loss pred: 0.5711; Loss self: 0.0000; time: 2.23s
Val loss: 0.0918 score: 0.9796 time: 0.23s
Test loss: 0.2589 score: 0.8776 time: 0.87s
Epoch 308/1000, LR 0.000214
Train loss: 0.5691;  Loss pred: 0.5691; Loss self: 0.0000; time: 1.17s
Val loss: 0.0911 score: 0.9796 time: 0.18s
Test loss: 0.2585 score: 0.8776 time: 0.07s
Epoch 309/1000, LR 0.000214
Train loss: 0.5702;  Loss pred: 0.5702; Loss self: 0.0000; time: 0.22s
Val loss: 0.0905 score: 0.9796 time: 0.07s
Test loss: 0.2582 score: 0.8776 time: 0.07s
Epoch 310/1000, LR 0.000214
Train loss: 0.5694;  Loss pred: 0.5694; Loss self: 0.0000; time: 0.22s
Val loss: 0.0898 score: 0.9796 time: 0.07s
Test loss: 0.2578 score: 0.8776 time: 0.07s
Epoch 311/1000, LR 0.000213
Train loss: 0.5689;  Loss pred: 0.5689; Loss self: 0.0000; time: 0.22s
Val loss: 0.0892 score: 0.9796 time: 0.08s
Test loss: 0.2574 score: 0.8776 time: 0.07s
Epoch 312/1000, LR 0.000213
Train loss: 0.5691;  Loss pred: 0.5691; Loss self: 0.0000; time: 0.22s
Val loss: 0.0886 score: 0.9796 time: 0.07s
Test loss: 0.2570 score: 0.8776 time: 0.07s
Epoch 313/1000, LR 0.000213
Train loss: 0.5666;  Loss pred: 0.5666; Loss self: 0.0000; time: 0.22s
Val loss: 0.0880 score: 0.9796 time: 0.07s
Test loss: 0.2565 score: 0.8776 time: 0.07s
Epoch 314/1000, LR 0.000212
Train loss: 0.5663;  Loss pred: 0.5663; Loss self: 0.0000; time: 0.22s
Val loss: 0.0874 score: 0.9796 time: 0.07s
Test loss: 0.2561 score: 0.8776 time: 0.07s
Epoch 315/1000, LR 0.000212
Train loss: 0.5688;  Loss pred: 0.5688; Loss self: 0.0000; time: 0.22s
Val loss: 0.0868 score: 0.9796 time: 0.07s
Test loss: 0.2556 score: 0.8776 time: 0.07s
Epoch 316/1000, LR 0.000212
Train loss: 0.5643;  Loss pred: 0.5643; Loss self: 0.0000; time: 0.22s
Val loss: 0.0862 score: 0.9796 time: 0.07s
Test loss: 0.2552 score: 0.8776 time: 0.07s
Epoch 317/1000, LR 0.000211
Train loss: 0.5633;  Loss pred: 0.5633; Loss self: 0.0000; time: 0.22s
Val loss: 0.0856 score: 0.9796 time: 0.08s
Test loss: 0.2547 score: 0.8776 time: 0.07s
Epoch 318/1000, LR 0.000211
Train loss: 0.5634;  Loss pred: 0.5634; Loss self: 0.0000; time: 0.33s
Val loss: 0.0850 score: 0.9796 time: 3.05s
Test loss: 0.2544 score: 0.8776 time: 2.10s
Epoch 319/1000, LR 0.000210
Train loss: 0.5640;  Loss pred: 0.5640; Loss self: 0.0000; time: 3.31s
Val loss: 0.0844 score: 0.9796 time: 0.16s
Test loss: 0.2541 score: 0.8776 time: 0.08s
Epoch 320/1000, LR 0.000210
Train loss: 0.5633;  Loss pred: 0.5633; Loss self: 0.0000; time: 0.22s
Val loss: 0.0839 score: 0.9796 time: 0.07s
Test loss: 0.2538 score: 0.8776 time: 0.06s
Epoch 321/1000, LR 0.000210
Train loss: 0.5634;  Loss pred: 0.5634; Loss self: 0.0000; time: 0.22s
Val loss: 0.0834 score: 0.9796 time: 0.08s
Test loss: 0.2534 score: 0.8776 time: 0.08s
Epoch 322/1000, LR 0.000209
Train loss: 0.5628;  Loss pred: 0.5628; Loss self: 0.0000; time: 0.23s
Val loss: 0.0828 score: 0.9796 time: 0.07s
Test loss: 0.2531 score: 0.8776 time: 0.06s
Epoch 323/1000, LR 0.000209
Train loss: 0.5627;  Loss pred: 0.5627; Loss self: 0.0000; time: 0.21s
Val loss: 0.0823 score: 0.9796 time: 0.07s
Test loss: 0.2527 score: 0.8776 time: 0.06s
Epoch 324/1000, LR 0.000209
Train loss: 0.5616;  Loss pred: 0.5616; Loss self: 0.0000; time: 0.21s
Val loss: 0.0817 score: 0.9796 time: 0.07s
Test loss: 0.2523 score: 0.8776 time: 0.06s
Epoch 325/1000, LR 0.000208
Train loss: 0.5611;  Loss pred: 0.5611; Loss self: 0.0000; time: 0.21s
Val loss: 0.0812 score: 0.9796 time: 0.07s
Test loss: 0.2519 score: 0.8776 time: 0.06s
Epoch 326/1000, LR 0.000208
Train loss: 0.5610;  Loss pred: 0.5610; Loss self: 0.0000; time: 0.21s
Val loss: 0.0807 score: 0.9796 time: 0.07s
Test loss: 0.2514 score: 0.8776 time: 0.06s
Epoch 327/1000, LR 0.000208
Train loss: 0.5617;  Loss pred: 0.5617; Loss self: 0.0000; time: 0.21s
Val loss: 0.0802 score: 0.9796 time: 0.07s
Test loss: 0.2510 score: 0.8776 time: 0.06s
Epoch 328/1000, LR 0.000207
Train loss: 0.5620;  Loss pred: 0.5620; Loss self: 0.0000; time: 0.21s
Val loss: 0.0797 score: 0.9796 time: 0.07s
Test loss: 0.2505 score: 0.8776 time: 0.06s
Epoch 329/1000, LR 0.000207
Train loss: 0.5646;  Loss pred: 0.5646; Loss self: 0.0000; time: 0.21s
Val loss: 0.0791 score: 0.9796 time: 0.07s
Test loss: 0.2499 score: 0.8776 time: 0.06s
Epoch 330/1000, LR 0.000207
Train loss: 0.5589;  Loss pred: 0.5589; Loss self: 0.0000; time: 0.21s
Val loss: 0.0787 score: 0.9796 time: 0.07s
Test loss: 0.2494 score: 0.8776 time: 0.06s
Epoch 331/1000, LR 0.000206
Train loss: 0.5592;  Loss pred: 0.5592; Loss self: 0.0000; time: 0.21s
Val loss: 0.0782 score: 0.9796 time: 0.07s
Test loss: 0.2489 score: 0.8776 time: 0.06s
Epoch 332/1000, LR 0.000206
Train loss: 0.5592;  Loss pred: 0.5592; Loss self: 0.0000; time: 0.21s
Val loss: 0.0777 score: 0.9796 time: 0.07s
Test loss: 0.2485 score: 0.8980 time: 0.06s
Epoch 333/1000, LR 0.000205
Train loss: 0.5606;  Loss pred: 0.5606; Loss self: 0.0000; time: 0.21s
Val loss: 0.0773 score: 0.9796 time: 0.07s
Test loss: 0.2480 score: 0.8980 time: 0.06s
Epoch 334/1000, LR 0.000205
Train loss: 0.5582;  Loss pred: 0.5582; Loss self: 0.0000; time: 0.22s
Val loss: 0.0768 score: 0.9796 time: 0.07s
Test loss: 0.2476 score: 0.8980 time: 0.06s
Epoch 335/1000, LR 0.000205
Train loss: 0.5561;  Loss pred: 0.5561; Loss self: 0.0000; time: 0.21s
Val loss: 0.0764 score: 0.9796 time: 0.07s
Test loss: 0.2473 score: 0.8980 time: 0.06s
Epoch 336/1000, LR 0.000204
Train loss: 0.5554;  Loss pred: 0.5554; Loss self: 0.0000; time: 0.21s
Val loss: 0.0759 score: 0.9796 time: 0.07s
Test loss: 0.2471 score: 0.8980 time: 0.07s
Epoch 337/1000, LR 0.000204
Train loss: 0.5553;  Loss pred: 0.5553; Loss self: 0.0000; time: 0.23s
Val loss: 0.0755 score: 0.9796 time: 0.08s
Test loss: 0.2469 score: 0.8980 time: 0.07s
Epoch 338/1000, LR 0.000204
Train loss: 0.5570;  Loss pred: 0.5570; Loss self: 0.0000; time: 4.05s
Val loss: 0.0751 score: 0.9796 time: 2.19s
Test loss: 0.2468 score: 0.8980 time: 0.57s
Epoch 339/1000, LR 0.000203
Train loss: 0.5564;  Loss pred: 0.5564; Loss self: 0.0000; time: 2.97s
Val loss: 0.0747 score: 0.9796 time: 0.12s
Test loss: 0.2467 score: 0.8980 time: 0.11s
Epoch 340/1000, LR 0.000203
Train loss: 0.5550;  Loss pred: 0.5550; Loss self: 0.0000; time: 0.23s
Val loss: 0.0743 score: 0.9796 time: 0.07s
Test loss: 0.2467 score: 0.8980 time: 0.06s
Epoch 341/1000, LR 0.000203
Train loss: 0.5544;  Loss pred: 0.5544; Loss self: 0.0000; time: 0.24s
Val loss: 0.0740 score: 0.9796 time: 0.08s
Test loss: 0.2468 score: 0.8776 time: 0.07s
Epoch 342/1000, LR 0.000202
Train loss: 0.5537;  Loss pred: 0.5537; Loss self: 0.0000; time: 0.22s
Val loss: 0.0736 score: 0.9796 time: 0.07s
Test loss: 0.2468 score: 0.8776 time: 0.06s
Epoch 343/1000, LR 0.000202
Train loss: 0.5537;  Loss pred: 0.5537; Loss self: 0.0000; time: 0.22s
Val loss: 0.0733 score: 0.9796 time: 0.07s
Test loss: 0.2468 score: 0.8776 time: 0.06s
Epoch 344/1000, LR 0.000201
Train loss: 0.5544;  Loss pred: 0.5544; Loss self: 0.0000; time: 0.22s
Val loss: 0.0729 score: 0.9796 time: 0.07s
Test loss: 0.2468 score: 0.8980 time: 0.06s
Epoch 345/1000, LR 0.000201
Train loss: 0.5555;  Loss pred: 0.5555; Loss self: 0.0000; time: 0.22s
Val loss: 0.0726 score: 0.9796 time: 0.08s
Test loss: 0.2468 score: 0.8980 time: 0.06s
Epoch 346/1000, LR 0.000201
Train loss: 0.5522;  Loss pred: 0.5522; Loss self: 0.0000; time: 0.23s
Val loss: 0.0722 score: 0.9796 time: 0.15s
Test loss: 0.2466 score: 0.8980 time: 0.07s
Epoch 347/1000, LR 0.000200
Train loss: 0.5530;  Loss pred: 0.5530; Loss self: 0.0000; time: 0.21s
Val loss: 0.0718 score: 0.9796 time: 0.07s
Test loss: 0.2463 score: 0.8980 time: 0.06s
Epoch 348/1000, LR 0.000200
Train loss: 0.5528;  Loss pred: 0.5528; Loss self: 0.0000; time: 0.20s
Val loss: 0.0714 score: 0.9796 time: 0.92s
Test loss: 0.2458 score: 0.8980 time: 1.28s
Epoch 349/1000, LR 0.000200
Train loss: 0.5533;  Loss pred: 0.5533; Loss self: 0.0000; time: 3.98s
Val loss: 0.0710 score: 0.9796 time: 1.92s
Test loss: 0.2453 score: 0.9184 time: 0.45s
Epoch 350/1000, LR 0.000199
Train loss: 0.5550;  Loss pred: 0.5550; Loss self: 0.0000; time: 0.24s
Val loss: 0.0705 score: 0.9796 time: 0.08s
Test loss: 0.2446 score: 0.8980 time: 0.07s
Epoch 351/1000, LR 0.000199
Train loss: 0.5548;  Loss pred: 0.5548; Loss self: 0.0000; time: 0.23s
Val loss: 0.0701 score: 0.9796 time: 0.08s
Test loss: 0.2440 score: 0.8980 time: 0.07s
Epoch 352/1000, LR 0.000198
Train loss: 0.5484;  Loss pred: 0.5484; Loss self: 0.0000; time: 0.23s
Val loss: 0.0697 score: 0.9796 time: 0.08s
Test loss: 0.2435 score: 0.8980 time: 0.07s
Epoch 353/1000, LR 0.000198
Train loss: 0.5492;  Loss pred: 0.5492; Loss self: 0.0000; time: 0.23s
Val loss: 0.0693 score: 0.9796 time: 0.08s
Test loss: 0.2431 score: 0.8980 time: 0.07s
Epoch 354/1000, LR 0.000198
Train loss: 0.5515;  Loss pred: 0.5515; Loss self: 0.0000; time: 0.23s
Val loss: 0.0690 score: 0.9796 time: 0.08s
Test loss: 0.2426 score: 0.8980 time: 0.07s
Epoch 355/1000, LR 0.000197
Train loss: 0.5494;  Loss pred: 0.5494; Loss self: 0.0000; time: 0.24s
Val loss: 0.0686 score: 0.9796 time: 0.41s
Test loss: 0.2422 score: 0.8980 time: 1.92s
Epoch 356/1000, LR 0.000197
Train loss: 0.5517;  Loss pred: 0.5517; Loss self: 0.0000; time: 7.59s
Val loss: 0.0682 score: 0.9796 time: 3.49s
Test loss: 0.2419 score: 0.8980 time: 0.06s
Epoch 357/1000, LR 0.000196
Train loss: 0.5484;  Loss pred: 0.5484; Loss self: 0.0000; time: 0.21s
Val loss: 0.0679 score: 0.9796 time: 0.07s
Test loss: 0.2416 score: 0.8980 time: 0.06s
Epoch 358/1000, LR 0.000196
Train loss: 0.5469;  Loss pred: 0.5469; Loss self: 0.0000; time: 0.21s
Val loss: 0.0676 score: 0.9796 time: 0.07s
Test loss: 0.2414 score: 0.8980 time: 0.06s
Epoch 359/1000, LR 0.000196
Train loss: 0.5466;  Loss pred: 0.5466; Loss self: 0.0000; time: 0.20s
Val loss: 0.0673 score: 0.9796 time: 0.07s
Test loss: 0.2411 score: 0.8980 time: 0.06s
Epoch 360/1000, LR 0.000195
Train loss: 0.5502;  Loss pred: 0.5502; Loss self: 0.0000; time: 0.21s
Val loss: 0.0669 score: 0.9796 time: 0.07s
Test loss: 0.2409 score: 0.8980 time: 0.06s
Epoch 361/1000, LR 0.000195
Train loss: 0.5481;  Loss pred: 0.5481; Loss self: 0.0000; time: 0.21s
Val loss: 0.0666 score: 0.9796 time: 0.07s
Test loss: 0.2408 score: 0.8980 time: 0.06s
Epoch 362/1000, LR 0.000195
Train loss: 0.5481;  Loss pred: 0.5481; Loss self: 0.0000; time: 0.21s
Val loss: 0.0663 score: 0.9796 time: 0.07s
Test loss: 0.2407 score: 0.8980 time: 0.06s
Epoch 363/1000, LR 0.000194
Train loss: 0.5471;  Loss pred: 0.5471; Loss self: 0.0000; time: 0.21s
Val loss: 0.0660 score: 0.9796 time: 0.07s
Test loss: 0.2407 score: 0.8980 time: 0.07s
Epoch 364/1000, LR 0.000194
Train loss: 0.5477;  Loss pred: 0.5477; Loss self: 0.0000; time: 0.23s
Val loss: 0.0657 score: 0.9796 time: 0.07s
Test loss: 0.2406 score: 0.8980 time: 0.06s
Epoch 365/1000, LR 0.000193
Train loss: 0.5482;  Loss pred: 0.5482; Loss self: 0.0000; time: 0.23s
Val loss: 0.0654 score: 0.9796 time: 0.07s
Test loss: 0.2405 score: 0.8980 time: 0.06s
Epoch 366/1000, LR 0.000193
Train loss: 0.5487;  Loss pred: 0.5487; Loss self: 0.0000; time: 0.22s
Val loss: 0.0651 score: 0.9796 time: 1.79s
Test loss: 0.2404 score: 0.8980 time: 1.81s
Epoch 367/1000, LR 0.000193
Train loss: 0.5454;  Loss pred: 0.5454; Loss self: 0.0000; time: 1.40s
Val loss: 0.0649 score: 0.9796 time: 0.53s
Test loss: 0.2403 score: 0.8980 time: 1.03s
Epoch 368/1000, LR 0.000192
Train loss: 0.5450;  Loss pred: 0.5450; Loss self: 0.0000; time: 0.36s
Val loss: 0.0646 score: 0.9796 time: 0.71s
Test loss: 0.2402 score: 0.8980 time: 0.46s
Epoch 369/1000, LR 0.000192
Train loss: 0.5451;  Loss pred: 0.5451; Loss self: 0.0000; time: 0.22s
Val loss: 0.0643 score: 0.9796 time: 0.07s
Test loss: 0.2401 score: 0.9184 time: 0.06s
Epoch 370/1000, LR 0.000191
Train loss: 0.5452;  Loss pred: 0.5452; Loss self: 0.0000; time: 0.22s
Val loss: 0.0640 score: 0.9796 time: 0.07s
Test loss: 0.2400 score: 0.9184 time: 0.07s
Epoch 371/1000, LR 0.000191
Train loss: 0.5442;  Loss pred: 0.5442; Loss self: 0.0000; time: 0.22s
Val loss: 0.0637 score: 0.9796 time: 0.07s
Test loss: 0.2398 score: 0.9184 time: 0.06s
Epoch 372/1000, LR 0.000191
Train loss: 0.5432;  Loss pred: 0.5432; Loss self: 0.0000; time: 0.22s
Val loss: 0.0635 score: 0.9796 time: 0.07s
Test loss: 0.2397 score: 0.9184 time: 0.07s
Epoch 373/1000, LR 0.000190
Train loss: 0.5448;  Loss pred: 0.5448; Loss self: 0.0000; time: 0.22s
Val loss: 0.0632 score: 0.9796 time: 0.07s
Test loss: 0.2395 score: 0.9184 time: 0.07s
Epoch 374/1000, LR 0.000190
Train loss: 0.5448;  Loss pred: 0.5448; Loss self: 0.0000; time: 0.22s
Val loss: 0.0629 score: 0.9796 time: 0.07s
Test loss: 0.2393 score: 0.9184 time: 0.07s
Epoch 375/1000, LR 0.000190
Train loss: 0.5428;  Loss pred: 0.5428; Loss self: 0.0000; time: 0.22s
Val loss: 0.0626 score: 0.9796 time: 0.07s
Test loss: 0.2390 score: 0.9184 time: 0.07s
Epoch 376/1000, LR 0.000189
Train loss: 0.5419;  Loss pred: 0.5419; Loss self: 0.0000; time: 0.22s
Val loss: 0.0623 score: 0.9796 time: 0.07s
Test loss: 0.2387 score: 0.9184 time: 0.07s
Epoch 377/1000, LR 0.000189
Train loss: 0.5437;  Loss pred: 0.5437; Loss self: 0.0000; time: 0.22s
Val loss: 0.0621 score: 0.9796 time: 2.30s
Test loss: 0.2384 score: 0.8980 time: 1.69s
Epoch 378/1000, LR 0.000188
Train loss: 0.5427;  Loss pred: 0.5427; Loss self: 0.0000; time: 1.43s
Val loss: 0.0618 score: 0.9796 time: 0.56s
Test loss: 0.2381 score: 0.8980 time: 0.24s
Epoch 379/1000, LR 0.000188
Train loss: 0.5441;  Loss pred: 0.5441; Loss self: 0.0000; time: 1.30s
Val loss: 0.0615 score: 0.9796 time: 0.24s
Test loss: 0.2378 score: 0.8980 time: 0.66s
Epoch 380/1000, LR 0.000188
Train loss: 0.5420;  Loss pred: 0.5420; Loss self: 0.0000; time: 0.22s
Val loss: 0.0612 score: 0.9796 time: 0.07s
Test loss: 0.2375 score: 0.8980 time: 0.06s
Epoch 381/1000, LR 0.000187
Train loss: 0.5421;  Loss pred: 0.5421; Loss self: 0.0000; time: 0.21s
Val loss: 0.0610 score: 0.9796 time: 0.07s
Test loss: 0.2373 score: 0.8980 time: 0.06s
Epoch 382/1000, LR 0.000187
Train loss: 0.5404;  Loss pred: 0.5404; Loss self: 0.0000; time: 0.21s
Val loss: 0.0607 score: 0.9796 time: 0.07s
Test loss: 0.2372 score: 0.8980 time: 0.06s
Epoch 383/1000, LR 0.000186
Train loss: 0.5397;  Loss pred: 0.5397; Loss self: 0.0000; time: 0.22s
Val loss: 0.0605 score: 0.9796 time: 0.07s
Test loss: 0.2371 score: 0.8980 time: 0.07s
Epoch 384/1000, LR 0.000186
Train loss: 0.5418;  Loss pred: 0.5418; Loss self: 0.0000; time: 0.22s
Val loss: 0.0603 score: 0.9796 time: 0.07s
Test loss: 0.2370 score: 0.8980 time: 0.06s
Epoch 385/1000, LR 0.000186
Train loss: 0.5426;  Loss pred: 0.5426; Loss self: 0.0000; time: 0.22s
Val loss: 0.0600 score: 0.9796 time: 0.07s
Test loss: 0.2368 score: 0.8980 time: 0.07s
Epoch 386/1000, LR 0.000185
Train loss: 0.5408;  Loss pred: 0.5408; Loss self: 0.0000; time: 0.22s
Val loss: 0.0598 score: 0.9796 time: 0.07s
Test loss: 0.2368 score: 0.8980 time: 0.07s
Epoch 387/1000, LR 0.000185
Train loss: 0.5411;  Loss pred: 0.5411; Loss self: 0.0000; time: 0.22s
Val loss: 0.0596 score: 0.9796 time: 0.07s
Test loss: 0.2366 score: 0.8980 time: 0.07s
Epoch 388/1000, LR 0.000184
Train loss: 0.5426;  Loss pred: 0.5426; Loss self: 0.0000; time: 0.22s
Val loss: 0.0593 score: 0.9796 time: 0.07s
Test loss: 0.2365 score: 0.8980 time: 0.07s
Epoch 389/1000, LR 0.000184
Train loss: 0.5385;  Loss pred: 0.5385; Loss self: 0.0000; time: 0.22s
Val loss: 0.0591 score: 0.9796 time: 0.07s
Test loss: 0.2364 score: 0.8980 time: 0.24s
Epoch 390/1000, LR 0.000184
Train loss: 0.5414;  Loss pred: 0.5414; Loss self: 0.0000; time: 7.08s
Val loss: 0.0589 score: 0.9796 time: 1.27s
Test loss: 0.2362 score: 0.8980 time: 0.55s
Epoch 391/1000, LR 0.000183
Train loss: 0.5385;  Loss pred: 0.5385; Loss self: 0.0000; time: 0.35s
Val loss: 0.0587 score: 0.9796 time: 2.20s
Test loss: 0.2361 score: 0.9184 time: 0.50s
Epoch 392/1000, LR 0.000183
Train loss: 0.5400;  Loss pred: 0.5400; Loss self: 0.0000; time: 0.21s
Val loss: 0.0584 score: 0.9796 time: 0.07s
Test loss: 0.2360 score: 0.9184 time: 0.06s
Epoch 393/1000, LR 0.000182
Train loss: 0.5409;  Loss pred: 0.5409; Loss self: 0.0000; time: 0.21s
Val loss: 0.0582 score: 0.9796 time: 0.07s
Test loss: 0.2358 score: 0.9184 time: 0.06s
Epoch 394/1000, LR 0.000182
Train loss: 0.5382;  Loss pred: 0.5382; Loss self: 0.0000; time: 0.21s
Val loss: 0.0580 score: 0.9796 time: 0.07s
Test loss: 0.2357 score: 0.9184 time: 0.06s
Epoch 395/1000, LR 0.000182
Train loss: 0.5397;  Loss pred: 0.5397; Loss self: 0.0000; time: 0.21s
Val loss: 0.0578 score: 0.9796 time: 0.07s
Test loss: 0.2355 score: 0.9184 time: 0.06s
Epoch 396/1000, LR 0.000181
Train loss: 0.5387;  Loss pred: 0.5387; Loss self: 0.0000; time: 0.21s
Val loss: 0.0576 score: 0.9796 time: 0.07s
Test loss: 0.2354 score: 0.9184 time: 0.06s
Epoch 397/1000, LR 0.000181
Train loss: 0.5379;  Loss pred: 0.5379; Loss self: 0.0000; time: 0.21s
Val loss: 0.0574 score: 0.9796 time: 0.07s
Test loss: 0.2352 score: 0.9184 time: 0.06s
Epoch 398/1000, LR 0.000180
Train loss: 0.5394;  Loss pred: 0.5394; Loss self: 0.0000; time: 0.21s
Val loss: 0.0572 score: 0.9796 time: 0.07s
Test loss: 0.2351 score: 0.9184 time: 0.06s
Epoch 399/1000, LR 0.000180
Train loss: 0.5378;  Loss pred: 0.5378; Loss self: 0.0000; time: 0.21s
Val loss: 0.0570 score: 0.9796 time: 0.07s
Test loss: 0.2351 score: 0.9184 time: 0.06s
Epoch 400/1000, LR 0.000180
Train loss: 0.5371;  Loss pred: 0.5371; Loss self: 0.0000; time: 0.21s
Val loss: 0.0568 score: 0.9796 time: 0.07s
Test loss: 0.2350 score: 0.9184 time: 0.06s
Epoch 401/1000, LR 0.000179
Train loss: 0.5366;  Loss pred: 0.5366; Loss self: 0.0000; time: 0.20s
Val loss: 0.0566 score: 0.9796 time: 0.07s
Test loss: 0.2350 score: 0.9184 time: 0.06s
Epoch 402/1000, LR 0.000179
Train loss: 0.5364;  Loss pred: 0.5364; Loss self: 0.0000; time: 0.21s
Val loss: 0.0564 score: 0.9796 time: 0.07s
Test loss: 0.2352 score: 0.9184 time: 0.06s
Epoch 403/1000, LR 0.000178
Train loss: 0.5366;  Loss pred: 0.5366; Loss self: 0.0000; time: 0.21s
Val loss: 0.0562 score: 0.9796 time: 0.07s
Test loss: 0.2353 score: 0.9184 time: 0.06s
Epoch 404/1000, LR 0.000178
Train loss: 0.5341;  Loss pred: 0.5341; Loss self: 0.0000; time: 0.21s
Val loss: 0.0561 score: 0.9796 time: 0.07s
Test loss: 0.2354 score: 0.9184 time: 0.06s
Epoch 405/1000, LR 0.000178
Train loss: 0.5357;  Loss pred: 0.5357; Loss self: 0.0000; time: 0.21s
Val loss: 0.0559 score: 0.9796 time: 0.07s
Test loss: 0.2355 score: 0.9184 time: 0.06s
Epoch 406/1000, LR 0.000177
Train loss: 0.5364;  Loss pred: 0.5364; Loss self: 0.0000; time: 0.21s
Val loss: 0.0557 score: 0.9796 time: 0.07s
Test loss: 0.2355 score: 0.9184 time: 0.06s
Epoch 407/1000, LR 0.000177
Train loss: 0.5353;  Loss pred: 0.5353; Loss self: 0.0000; time: 0.20s
Val loss: 0.0556 score: 0.9796 time: 0.07s
Test loss: 0.2354 score: 0.9184 time: 0.06s
Epoch 408/1000, LR 0.000176
Train loss: 0.5370;  Loss pred: 0.5370; Loss self: 0.0000; time: 0.20s
Val loss: 0.0554 score: 0.9796 time: 0.07s
Test loss: 0.2353 score: 0.9184 time: 0.06s
Epoch 409/1000, LR 0.000176
Train loss: 0.5355;  Loss pred: 0.5355; Loss self: 0.0000; time: 0.20s
Val loss: 0.0552 score: 0.9796 time: 0.07s
Test loss: 0.2352 score: 0.9184 time: 0.06s
Epoch 410/1000, LR 0.000175
Train loss: 0.5349;  Loss pred: 0.5349; Loss self: 0.0000; time: 0.20s
Val loss: 0.0550 score: 0.9796 time: 0.07s
Test loss: 0.2350 score: 0.9184 time: 0.06s
Epoch 411/1000, LR 0.000175
Train loss: 0.5346;  Loss pred: 0.5346; Loss self: 0.0000; time: 0.20s
Val loss: 0.0548 score: 0.9796 time: 0.07s
Test loss: 0.2348 score: 0.9184 time: 0.06s
Epoch 412/1000, LR 0.000175
Train loss: 0.5355;  Loss pred: 0.5355; Loss self: 0.0000; time: 2.31s
Val loss: 0.0546 score: 0.9796 time: 0.51s
Test loss: 0.2346 score: 0.9184 time: 0.15s
Epoch 413/1000, LR 0.000174
Train loss: 0.5348;  Loss pred: 0.5348; Loss self: 0.0000; time: 0.23s
Val loss: 0.0544 score: 0.9796 time: 0.08s
Test loss: 0.2344 score: 0.9184 time: 0.07s
Epoch 414/1000, LR 0.000174
Train loss: 0.5333;  Loss pred: 0.5333; Loss self: 0.0000; time: 0.23s
Val loss: 0.0542 score: 0.9796 time: 0.08s
Test loss: 0.2342 score: 0.9184 time: 0.07s
Epoch 415/1000, LR 0.000173
Train loss: 0.5341;  Loss pred: 0.5341; Loss self: 0.0000; time: 0.22s
Val loss: 0.0540 score: 0.9796 time: 0.07s
Test loss: 0.2340 score: 0.9184 time: 0.06s
Epoch 416/1000, LR 0.000173
Train loss: 0.5335;  Loss pred: 0.5335; Loss self: 0.0000; time: 0.22s
Val loss: 0.0538 score: 0.9796 time: 0.07s
Test loss: 0.2338 score: 0.9184 time: 0.06s
Epoch 417/1000, LR 0.000173
Train loss: 0.5344;  Loss pred: 0.5344; Loss self: 0.0000; time: 0.22s
Val loss: 0.0536 score: 0.9796 time: 0.07s
Test loss: 0.2337 score: 0.9184 time: 0.07s
Epoch 418/1000, LR 0.000172
Train loss: 0.5352;  Loss pred: 0.5352; Loss self: 0.0000; time: 0.22s
Val loss: 0.0535 score: 0.9796 time: 0.07s
Test loss: 0.2335 score: 0.9184 time: 0.06s
Epoch 419/1000, LR 0.000172
Train loss: 0.5340;  Loss pred: 0.5340; Loss self: 0.0000; time: 0.22s
Val loss: 0.0533 score: 0.9796 time: 0.07s
Test loss: 0.2334 score: 0.9184 time: 0.06s
Epoch 420/1000, LR 0.000171
Train loss: 0.5330;  Loss pred: 0.5330; Loss self: 0.0000; time: 0.22s
Val loss: 0.0531 score: 0.9796 time: 0.07s
Test loss: 0.2332 score: 0.9184 time: 0.06s
Epoch 421/1000, LR 0.000171
Train loss: 0.5327;  Loss pred: 0.5327; Loss self: 0.0000; time: 0.22s
Val loss: 0.0529 score: 0.9796 time: 0.07s
Test loss: 0.2331 score: 0.9184 time: 0.06s
Epoch 422/1000, LR 0.000171
Train loss: 0.5314;  Loss pred: 0.5314; Loss self: 0.0000; time: 0.22s
Val loss: 0.0528 score: 0.9796 time: 0.07s
Test loss: 0.2331 score: 0.9184 time: 0.06s
Epoch 423/1000, LR 0.000170
Train loss: 0.5332;  Loss pred: 0.5332; Loss self: 0.0000; time: 0.22s
Val loss: 0.0526 score: 0.9796 time: 0.08s
Test loss: 0.2330 score: 0.9184 time: 0.07s
Epoch 424/1000, LR 0.000170
Train loss: 0.5323;  Loss pred: 0.5323; Loss self: 0.0000; time: 0.23s
Val loss: 0.0525 score: 0.9796 time: 0.08s
Test loss: 0.2330 score: 0.9184 time: 0.07s
Epoch 425/1000, LR 0.000169
Train loss: 0.5347;  Loss pred: 0.5347; Loss self: 0.0000; time: 0.23s
Val loss: 0.0523 score: 0.9796 time: 0.08s
Test loss: 0.2329 score: 0.9184 time: 0.07s
Epoch 426/1000, LR 0.000169
Train loss: 0.5309;  Loss pred: 0.5309; Loss self: 0.0000; time: 0.23s
Val loss: 0.0522 score: 0.9796 time: 0.08s
Test loss: 0.2329 score: 0.9184 time: 0.07s
Epoch 427/1000, LR 0.000168
Train loss: 0.5313;  Loss pred: 0.5313; Loss self: 0.0000; time: 0.23s
Val loss: 0.0520 score: 0.9796 time: 0.08s
Test loss: 0.2329 score: 0.9184 time: 0.07s
Epoch 428/1000, LR 0.000168
Train loss: 0.5312;  Loss pred: 0.5312; Loss self: 0.0000; time: 0.23s
Val loss: 0.0519 score: 0.9796 time: 0.08s
Test loss: 0.2329 score: 0.9184 time: 0.07s
Epoch 429/1000, LR 0.000168
Train loss: 0.5311;  Loss pred: 0.5311; Loss self: 0.0000; time: 0.23s
Val loss: 0.0517 score: 0.9796 time: 0.08s
Test loss: 0.2329 score: 0.9184 time: 0.07s
Epoch 430/1000, LR 0.000167
Train loss: 0.5311;  Loss pred: 0.5311; Loss self: 0.0000; time: 0.22s
Val loss: 0.0516 score: 0.9796 time: 0.07s
Test loss: 0.2329 score: 0.9184 time: 0.06s
Epoch 431/1000, LR 0.000167
Train loss: 0.5307;  Loss pred: 0.5307; Loss self: 0.0000; time: 0.21s
Val loss: 0.0515 score: 0.9796 time: 0.07s
Test loss: 0.2330 score: 0.9184 time: 0.06s
Epoch 432/1000, LR 0.000166
Train loss: 0.5301;  Loss pred: 0.5301; Loss self: 0.0000; time: 0.21s
Val loss: 0.0513 score: 0.9796 time: 0.07s
Test loss: 0.2331 score: 0.9184 time: 0.06s
Epoch 433/1000, LR 0.000166
Train loss: 0.5311;  Loss pred: 0.5311; Loss self: 0.0000; time: 0.21s
Val loss: 0.0512 score: 0.9796 time: 0.08s
Test loss: 0.2330 score: 0.9184 time: 0.07s
Epoch 434/1000, LR 0.000166
Train loss: 0.5311;  Loss pred: 0.5311; Loss self: 0.0000; time: 0.23s
Val loss: 0.0510 score: 0.9796 time: 0.08s
Test loss: 0.2329 score: 0.9184 time: 0.07s
Epoch 435/1000, LR 0.000165
Train loss: 0.5315;  Loss pred: 0.5315; Loss self: 0.0000; time: 0.23s
Val loss: 0.0509 score: 0.9796 time: 0.08s
Test loss: 0.2327 score: 0.9184 time: 0.07s
Epoch 436/1000, LR 0.000165
Train loss: 0.5308;  Loss pred: 0.5308; Loss self: 0.0000; time: 0.23s
Val loss: 0.0507 score: 0.9796 time: 0.08s
Test loss: 0.2326 score: 0.9184 time: 0.07s
Epoch 437/1000, LR 0.000164
Train loss: 0.5300;  Loss pred: 0.5300; Loss self: 0.0000; time: 0.23s
Val loss: 0.0506 score: 0.9796 time: 0.08s
Test loss: 0.2326 score: 0.9184 time: 0.07s
Epoch 438/1000, LR 0.000164
Train loss: 0.5305;  Loss pred: 0.5305; Loss self: 0.0000; time: 0.23s
Val loss: 0.0504 score: 0.9796 time: 0.08s
Test loss: 0.2326 score: 0.9184 time: 0.07s
Epoch 439/1000, LR 0.000163
Train loss: 0.5302;  Loss pred: 0.5302; Loss self: 0.0000; time: 0.25s
Val loss: 0.0503 score: 0.9796 time: 0.07s
Test loss: 0.2326 score: 0.9184 time: 0.06s
Epoch 440/1000, LR 0.000163
Train loss: 0.5300;  Loss pred: 0.5300; Loss self: 0.0000; time: 0.21s
Val loss: 0.0502 score: 0.9796 time: 0.24s
Test loss: 0.2325 score: 0.9184 time: 2.42s
Epoch 441/1000, LR 0.000163
Train loss: 0.5298;  Loss pred: 0.5298; Loss self: 0.0000; time: 2.26s
Val loss: 0.0500 score: 0.9796 time: 0.14s
Test loss: 0.2325 score: 0.9184 time: 0.14s
Epoch 442/1000, LR 0.000162
Train loss: 0.5284;  Loss pred: 0.5284; Loss self: 0.0000; time: 0.23s
Val loss: 0.0499 score: 0.9796 time: 0.08s
Test loss: 0.2325 score: 0.9184 time: 0.07s
Epoch 443/1000, LR 0.000162
Train loss: 0.5310;  Loss pred: 0.5310; Loss self: 0.0000; time: 0.22s
Val loss: 0.0498 score: 0.9796 time: 0.07s
Test loss: 0.2325 score: 0.9184 time: 0.06s
Epoch 444/1000, LR 0.000161
Train loss: 0.5280;  Loss pred: 0.5280; Loss self: 0.0000; time: 0.22s
Val loss: 0.0496 score: 0.9796 time: 0.08s
Test loss: 0.2325 score: 0.9184 time: 0.07s
Epoch 445/1000, LR 0.000161
Train loss: 0.5275;  Loss pred: 0.5275; Loss self: 0.0000; time: 0.22s
Val loss: 0.0495 score: 0.9796 time: 0.08s
Test loss: 0.2325 score: 0.9184 time: 0.07s
Epoch 446/1000, LR 0.000161
Train loss: 0.5298;  Loss pred: 0.5298; Loss self: 0.0000; time: 0.22s
Val loss: 0.0494 score: 0.9796 time: 0.08s
Test loss: 0.2324 score: 0.9184 time: 0.07s
Epoch 447/1000, LR 0.000160
Train loss: 0.5293;  Loss pred: 0.5293; Loss self: 0.0000; time: 0.23s
Val loss: 0.0492 score: 0.9796 time: 0.08s
Test loss: 0.2323 score: 0.9184 time: 0.07s
Epoch 448/1000, LR 0.000160
Train loss: 0.5257;  Loss pred: 0.5257; Loss self: 0.0000; time: 0.22s
Val loss: 0.0491 score: 0.9796 time: 0.07s
Test loss: 0.2323 score: 0.9184 time: 0.07s
Epoch 449/1000, LR 0.000159
Train loss: 0.5272;  Loss pred: 0.5272; Loss self: 0.0000; time: 0.22s
Val loss: 0.0490 score: 0.9796 time: 0.07s
Test loss: 0.2321 score: 0.9184 time: 0.07s
Epoch 450/1000, LR 0.000159
Train loss: 0.5270;  Loss pred: 0.5270; Loss self: 0.0000; time: 0.22s
Val loss: 0.0488 score: 0.9796 time: 0.07s
Test loss: 0.2320 score: 0.9184 time: 0.07s
Epoch 451/1000, LR 0.000158
Train loss: 0.5275;  Loss pred: 0.5275; Loss self: 0.0000; time: 1.02s
Val loss: 0.0487 score: 0.9796 time: 1.49s
Test loss: 0.2318 score: 0.9184 time: 1.22s
Epoch 452/1000, LR 0.000158
Train loss: 0.5271;  Loss pred: 0.5271; Loss self: 0.0000; time: 6.33s
Val loss: 0.0486 score: 0.9796 time: 2.01s
Test loss: 0.2317 score: 0.9184 time: 1.24s
Epoch 453/1000, LR 0.000158
Train loss: 0.5289;  Loss pred: 0.5289; Loss self: 0.0000; time: 0.21s
Val loss: 0.0484 score: 0.9796 time: 0.07s
Test loss: 0.2315 score: 0.9184 time: 0.06s
Epoch 454/1000, LR 0.000157
Train loss: 0.5279;  Loss pred: 0.5279; Loss self: 0.0000; time: 0.21s
Val loss: 0.0483 score: 0.9796 time: 0.07s
Test loss: 0.2314 score: 0.9184 time: 0.06s
Epoch 455/1000, LR 0.000157
Train loss: 0.5258;  Loss pred: 0.5258; Loss self: 0.0000; time: 0.21s
Val loss: 0.0482 score: 0.9796 time: 0.07s
Test loss: 0.2312 score: 0.9184 time: 0.06s
Epoch 456/1000, LR 0.000156
Train loss: 0.5278;  Loss pred: 0.5278; Loss self: 0.0000; time: 0.21s
Val loss: 0.0480 score: 0.9796 time: 0.07s
Test loss: 0.2311 score: 0.9184 time: 0.06s
Epoch 457/1000, LR 0.000156
Train loss: 0.5287;  Loss pred: 0.5287; Loss self: 0.0000; time: 0.21s
Val loss: 0.0479 score: 0.9796 time: 0.07s
Test loss: 0.2310 score: 0.9184 time: 0.06s
Epoch 458/1000, LR 0.000155
Train loss: 0.5268;  Loss pred: 0.5268; Loss self: 0.0000; time: 0.21s
Val loss: 0.0478 score: 0.9796 time: 0.07s
Test loss: 0.2309 score: 0.9184 time: 0.06s
Epoch 459/1000, LR 0.000155
Train loss: 0.5273;  Loss pred: 0.5273; Loss self: 0.0000; time: 0.21s
Val loss: 0.0477 score: 0.9796 time: 0.07s
Test loss: 0.2309 score: 0.9184 time: 0.06s
Epoch 460/1000, LR 0.000155
Train loss: 0.5283;  Loss pred: 0.5283; Loss self: 0.0000; time: 0.21s
Val loss: 0.0476 score: 0.9796 time: 0.07s
Test loss: 0.2308 score: 0.9184 time: 0.06s
Epoch 461/1000, LR 0.000154
Train loss: 0.5255;  Loss pred: 0.5255; Loss self: 0.0000; time: 0.21s
Val loss: 0.0474 score: 0.9796 time: 0.07s
Test loss: 0.2308 score: 0.9184 time: 0.07s
Epoch 462/1000, LR 0.000154
Train loss: 0.5245;  Loss pred: 0.5245; Loss self: 0.0000; time: 5.86s
Val loss: 0.0473 score: 0.9796 time: 0.08s
Test loss: 0.2308 score: 0.9184 time: 0.07s
Epoch 463/1000, LR 0.000153
Train loss: 0.5256;  Loss pred: 0.5256; Loss self: 0.0000; time: 0.21s
Val loss: 0.0472 score: 0.9796 time: 0.07s
Test loss: 0.2308 score: 0.9184 time: 0.06s
Epoch 464/1000, LR 0.000153
Train loss: 0.5254;  Loss pred: 0.5254; Loss self: 0.0000; time: 0.21s
Val loss: 0.0471 score: 0.9796 time: 0.07s
Test loss: 0.2308 score: 0.9184 time: 0.07s
Epoch 465/1000, LR 0.000153
Train loss: 0.5285;  Loss pred: 0.5285; Loss self: 0.0000; time: 0.26s
Val loss: 0.0470 score: 0.9796 time: 0.07s
Test loss: 0.2309 score: 0.9184 time: 0.07s
Epoch 466/1000, LR 0.000152
Train loss: 0.5285;  Loss pred: 0.5285; Loss self: 0.0000; time: 0.22s
Val loss: 0.0469 score: 0.9796 time: 0.07s
Test loss: 0.2308 score: 0.9184 time: 0.06s
Epoch 467/1000, LR 0.000152
Train loss: 0.5243;  Loss pred: 0.5243; Loss self: 0.0000; time: 0.21s
Val loss: 0.0468 score: 0.9796 time: 0.07s
Test loss: 0.2306 score: 0.9184 time: 0.72s
Epoch 468/1000, LR 0.000151
Train loss: 0.5267;  Loss pred: 0.5267; Loss self: 0.0000; time: 5.71s
Val loss: 0.0467 score: 0.9796 time: 4.91s
Test loss: 0.2305 score: 0.9184 time: 0.41s
Epoch 469/1000, LR 0.000151
Train loss: 0.5237;  Loss pred: 0.5237; Loss self: 0.0000; time: 0.21s
Val loss: 0.0466 score: 0.9796 time: 0.08s
Test loss: 0.2304 score: 0.9184 time: 0.06s
Epoch 470/1000, LR 0.000150
Train loss: 0.5269;  Loss pred: 0.5269; Loss self: 0.0000; time: 0.21s
Val loss: 0.0465 score: 0.9796 time: 0.07s
Test loss: 0.2303 score: 0.9184 time: 0.06s
Epoch 471/1000, LR 0.000150
Train loss: 0.5256;  Loss pred: 0.5256; Loss self: 0.0000; time: 0.20s
Val loss: 0.0464 score: 0.9796 time: 0.07s
Test loss: 0.2302 score: 0.9184 time: 0.06s
Epoch 472/1000, LR 0.000150
Train loss: 0.5254;  Loss pred: 0.5254; Loss self: 0.0000; time: 0.20s
Val loss: 0.0463 score: 0.9796 time: 0.07s
Test loss: 0.2301 score: 0.9184 time: 0.06s
Epoch 473/1000, LR 0.000149
Train loss: 0.5245;  Loss pred: 0.5245; Loss self: 0.0000; time: 0.20s
Val loss: 0.0461 score: 0.9796 time: 0.07s
Test loss: 0.2301 score: 0.9184 time: 0.06s
Epoch 474/1000, LR 0.000149
Train loss: 0.5240;  Loss pred: 0.5240; Loss self: 0.0000; time: 0.20s
Val loss: 0.0461 score: 0.9796 time: 0.07s
Test loss: 0.2301 score: 0.9184 time: 0.06s
Epoch 475/1000, LR 0.000148
Train loss: 0.5236;  Loss pred: 0.5236; Loss self: 0.0000; time: 0.23s
Val loss: 0.0460 score: 0.9796 time: 0.07s
Test loss: 0.2301 score: 0.9184 time: 0.07s
Epoch 476/1000, LR 0.000148
Train loss: 0.5260;  Loss pred: 0.5260; Loss self: 0.0000; time: 0.21s
Val loss: 0.0459 score: 0.9796 time: 0.10s
Test loss: 0.2302 score: 0.9184 time: 0.07s
Epoch 477/1000, LR 0.000147
Train loss: 0.5229;  Loss pred: 0.5229; Loss self: 0.0000; time: 0.23s
Val loss: 0.0458 score: 0.9796 time: 0.07s
Test loss: 0.2302 score: 0.9184 time: 0.07s
Epoch 478/1000, LR 0.000147
Train loss: 0.5241;  Loss pred: 0.5241; Loss self: 0.0000; time: 0.50s
Val loss: 0.0457 score: 0.9796 time: 0.95s
Test loss: 0.2302 score: 0.9184 time: 0.79s
Epoch 479/1000, LR 0.000147
Train loss: 0.5238;  Loss pred: 0.5238; Loss self: 0.0000; time: 5.30s
Val loss: 0.0456 score: 0.9796 time: 1.52s
Test loss: 0.2303 score: 0.9184 time: 1.31s
Epoch 480/1000, LR 0.000146
Train loss: 0.5226;  Loss pred: 0.5226; Loss self: 0.0000; time: 0.23s
Val loss: 0.0455 score: 0.9796 time: 0.08s
Test loss: 0.2303 score: 0.9184 time: 0.06s
Epoch 481/1000, LR 0.000146
Train loss: 0.5239;  Loss pred: 0.5239; Loss self: 0.0000; time: 0.22s
Val loss: 0.0454 score: 0.9796 time: 0.07s
Test loss: 0.2303 score: 0.9184 time: 0.06s
Epoch 482/1000, LR 0.000145
Train loss: 0.5237;  Loss pred: 0.5237; Loss self: 0.0000; time: 0.21s
Val loss: 0.0453 score: 0.9796 time: 0.07s
Test loss: 0.2303 score: 0.9184 time: 0.06s
Epoch 483/1000, LR 0.000145
Train loss: 0.5231;  Loss pred: 0.5231; Loss self: 0.0000; time: 0.20s
Val loss: 0.0452 score: 0.9796 time: 0.07s
Test loss: 0.2304 score: 0.9184 time: 0.06s
Epoch 484/1000, LR 0.000144
Train loss: 0.5247;  Loss pred: 0.5247; Loss self: 0.0000; time: 0.20s
Val loss: 0.0452 score: 0.9796 time: 0.08s
Test loss: 0.2304 score: 0.9184 time: 0.06s
Epoch 485/1000, LR 0.000144
Train loss: 0.5231;  Loss pred: 0.5231; Loss self: 0.0000; time: 0.21s
Val loss: 0.0451 score: 0.9796 time: 0.07s
Test loss: 0.2304 score: 0.9184 time: 0.07s
Epoch 486/1000, LR 0.000144
Train loss: 0.5226;  Loss pred: 0.5226; Loss self: 0.0000; time: 0.21s
Val loss: 0.0450 score: 0.9796 time: 0.07s
Test loss: 0.2305 score: 0.9184 time: 0.06s
Epoch 487/1000, LR 0.000143
Train loss: 0.5229;  Loss pred: 0.5229; Loss self: 0.0000; time: 0.20s
Val loss: 0.0449 score: 0.9796 time: 0.07s
Test loss: 0.2305 score: 0.9184 time: 0.06s
Epoch 488/1000, LR 0.000143
Train loss: 0.5250;  Loss pred: 0.5250; Loss self: 0.0000; time: 0.20s
Val loss: 0.0448 score: 0.9796 time: 0.07s
Test loss: 0.2304 score: 0.9184 time: 0.06s
Epoch 489/1000, LR 0.000142
Train loss: 0.5227;  Loss pred: 0.5227; Loss self: 0.0000; time: 0.20s
Val loss: 0.0447 score: 0.9796 time: 0.06s
Test loss: 0.2303 score: 0.9184 time: 0.06s
Epoch 490/1000, LR 0.000142
Train loss: 0.5220;  Loss pred: 0.5220; Loss self: 0.0000; time: 0.21s
Val loss: 0.0446 score: 0.9796 time: 0.08s
Test loss: 0.2303 score: 0.9184 time: 0.06s
Epoch 491/1000, LR 0.000141
Train loss: 0.5222;  Loss pred: 0.5222; Loss self: 0.0000; time: 0.21s
Val loss: 0.0445 score: 0.9796 time: 0.07s
Test loss: 0.2303 score: 0.9184 time: 0.06s
Epoch 492/1000, LR 0.000141
Train loss: 0.5221;  Loss pred: 0.5221; Loss self: 0.0000; time: 0.20s
Val loss: 0.0444 score: 0.9796 time: 0.06s
Test loss: 0.2303 score: 0.9184 time: 0.06s
Epoch 493/1000, LR 0.000141
Train loss: 0.5246;  Loss pred: 0.5246; Loss self: 0.0000; time: 0.20s
Val loss: 0.0443 score: 0.9796 time: 0.06s
Test loss: 0.2301 score: 0.9184 time: 0.06s
Epoch 494/1000, LR 0.000140
Train loss: 0.5227;  Loss pred: 0.5227; Loss self: 0.0000; time: 0.22s
Val loss: 0.0442 score: 0.9796 time: 2.33s
Test loss: 0.2300 score: 0.9184 time: 2.49s
Epoch 495/1000, LR 0.000140
Train loss: 0.5218;  Loss pred: 0.5218; Loss self: 0.0000; time: 4.79s
Val loss: 0.0441 score: 0.9796 time: 0.09s
Test loss: 0.2300 score: 0.9184 time: 0.07s
Epoch 496/1000, LR 0.000139
Train loss: 0.5221;  Loss pred: 0.5221; Loss self: 0.0000; time: 0.22s
Val loss: 0.0440 score: 0.9796 time: 0.07s
Test loss: 0.2300 score: 0.9184 time: 0.06s
Epoch 497/1000, LR 0.000139
Train loss: 0.5247;  Loss pred: 0.5247; Loss self: 0.0000; time: 0.21s
Val loss: 0.0439 score: 0.9796 time: 0.08s
Test loss: 0.2298 score: 0.9184 time: 0.07s
Epoch 498/1000, LR 0.000138
Train loss: 0.5219;  Loss pred: 0.5219; Loss self: 0.0000; time: 0.21s
Val loss: 0.0438 score: 0.9796 time: 0.07s
Test loss: 0.2297 score: 0.9184 time: 0.06s
Epoch 499/1000, LR 0.000138
Train loss: 0.5204;  Loss pred: 0.5204; Loss self: 0.0000; time: 0.22s
Val loss: 0.0438 score: 0.9796 time: 0.07s
Test loss: 0.2296 score: 0.9184 time: 0.07s
Epoch 500/1000, LR 0.000138
Train loss: 0.5207;  Loss pred: 0.5207; Loss self: 0.0000; time: 0.21s
Val loss: 0.0437 score: 0.9796 time: 0.07s
Test loss: 0.2295 score: 0.9184 time: 0.06s
Epoch 501/1000, LR 0.000137
Train loss: 0.5196;  Loss pred: 0.5196; Loss self: 0.0000; time: 0.21s
Val loss: 0.0436 score: 0.9796 time: 0.07s
Test loss: 0.2295 score: 0.9184 time: 0.06s
Epoch 502/1000, LR 0.000137
Train loss: 0.5217;  Loss pred: 0.5217; Loss self: 0.0000; time: 0.21s
Val loss: 0.0435 score: 0.9796 time: 0.07s
Test loss: 0.2295 score: 0.9184 time: 0.06s
Epoch 503/1000, LR 0.000136
Train loss: 0.5216;  Loss pred: 0.5216; Loss self: 0.0000; time: 0.21s
Val loss: 0.0434 score: 0.9796 time: 0.07s
Test loss: 0.2296 score: 0.9184 time: 0.06s
Epoch 504/1000, LR 0.000136
Train loss: 0.5203;  Loss pred: 0.5203; Loss self: 0.0000; time: 0.21s
Val loss: 0.0434 score: 0.9796 time: 0.07s
Test loss: 0.2297 score: 0.9184 time: 0.06s
Epoch 505/1000, LR 0.000135
Train loss: 0.5208;  Loss pred: 0.5208; Loss self: 0.0000; time: 0.21s
Val loss: 0.0433 score: 0.9796 time: 0.07s
Test loss: 0.2298 score: 0.9184 time: 0.06s
Epoch 506/1000, LR 0.000135
Train loss: 0.5194;  Loss pred: 0.5194; Loss self: 0.0000; time: 0.21s
Val loss: 0.0432 score: 0.9796 time: 0.07s
Test loss: 0.2298 score: 0.9184 time: 0.06s
Epoch 507/1000, LR 0.000135
Train loss: 0.5198;  Loss pred: 0.5198; Loss self: 0.0000; time: 0.21s
Val loss: 0.0431 score: 0.9796 time: 0.07s
Test loss: 0.2299 score: 0.9184 time: 0.06s
Epoch 508/1000, LR 0.000134
Train loss: 0.5215;  Loss pred: 0.5215; Loss self: 0.0000; time: 0.42s
Val loss: 0.0431 score: 0.9796 time: 2.25s
Test loss: 0.2300 score: 0.9184 time: 1.29s
Epoch 509/1000, LR 0.000134
Train loss: 0.5228;  Loss pred: 0.5228; Loss self: 0.0000; time: 1.43s
Val loss: 0.0430 score: 0.9796 time: 0.12s
Test loss: 0.2300 score: 0.9184 time: 0.50s
Epoch 510/1000, LR 0.000133
Train loss: 0.5196;  Loss pred: 0.5196; Loss self: 0.0000; time: 0.97s
Val loss: 0.0429 score: 0.9796 time: 0.31s
Test loss: 0.2300 score: 0.9184 time: 0.14s
Epoch 511/1000, LR 0.000133
Train loss: 0.5204;  Loss pred: 0.5204; Loss self: 0.0000; time: 0.97s
Val loss: 0.0428 score: 0.9796 time: 0.08s
Test loss: 0.2300 score: 0.9184 time: 0.07s
Epoch 512/1000, LR 0.000132
Train loss: 0.5199;  Loss pred: 0.5199; Loss self: 0.0000; time: 0.23s
Val loss: 0.0428 score: 0.9796 time: 0.08s
Test loss: 0.2301 score: 0.9184 time: 0.07s
Epoch 513/1000, LR 0.000132
Train loss: 0.5198;  Loss pred: 0.5198; Loss self: 0.0000; time: 0.23s
Val loss: 0.0427 score: 0.9796 time: 0.07s
Test loss: 0.2302 score: 0.9184 time: 0.07s
Epoch 514/1000, LR 0.000132
Train loss: 0.5204;  Loss pred: 0.5204; Loss self: 0.0000; time: 0.23s
Val loss: 0.0426 score: 0.9796 time: 0.08s
Test loss: 0.2303 score: 0.9184 time: 0.07s
Epoch 515/1000, LR 0.000131
Train loss: 0.5202;  Loss pred: 0.5202; Loss self: 0.0000; time: 0.22s
Val loss: 0.0426 score: 0.9796 time: 0.07s
Test loss: 0.2303 score: 0.9184 time: 0.07s
Epoch 516/1000, LR 0.000131
Train loss: 0.5177;  Loss pred: 0.5177; Loss self: 0.0000; time: 0.22s
Val loss: 0.0425 score: 0.9796 time: 0.07s
Test loss: 0.2304 score: 0.9184 time: 0.07s
Epoch 517/1000, LR 0.000130
Train loss: 0.5199;  Loss pred: 0.5199; Loss self: 0.0000; time: 0.22s
Val loss: 0.0424 score: 0.9796 time: 0.08s
Test loss: 0.2304 score: 0.9184 time: 0.07s
Epoch 518/1000, LR 0.000130
Train loss: 0.5200;  Loss pred: 0.5200; Loss self: 0.0000; time: 0.22s
Val loss: 0.0423 score: 0.9796 time: 0.07s
Test loss: 0.2303 score: 0.9184 time: 0.07s
Epoch 519/1000, LR 0.000129
Train loss: 0.5195;  Loss pred: 0.5195; Loss self: 0.0000; time: 0.22s
Val loss: 0.0423 score: 0.9796 time: 0.07s
Test loss: 0.2303 score: 0.9184 time: 0.06s
Epoch 520/1000, LR 0.000129
Train loss: 0.5207;  Loss pred: 0.5207; Loss self: 0.0000; time: 0.22s
Val loss: 0.0422 score: 0.9796 time: 0.07s
Test loss: 0.2301 score: 0.9184 time: 0.06s
Epoch 521/1000, LR 0.000129
Train loss: 0.5187;  Loss pred: 0.5187; Loss self: 0.0000; time: 0.21s
Val loss: 0.0421 score: 0.9796 time: 0.07s
Test loss: 0.2301 score: 0.9184 time: 0.06s
Epoch 522/1000, LR 0.000128
Train loss: 0.5193;  Loss pred: 0.5193; Loss self: 0.0000; time: 0.21s
Val loss: 0.0420 score: 0.9796 time: 0.07s
Test loss: 0.2300 score: 0.9184 time: 0.06s
Epoch 523/1000, LR 0.000128
Train loss: 0.5185;  Loss pred: 0.5185; Loss self: 0.0000; time: 0.22s
Val loss: 0.0419 score: 0.9796 time: 0.07s
Test loss: 0.2300 score: 0.9184 time: 0.06s
Epoch 524/1000, LR 0.000127
Train loss: 0.5174;  Loss pred: 0.5174; Loss self: 0.0000; time: 0.21s
Val loss: 0.0419 score: 0.9796 time: 0.07s
Test loss: 0.2300 score: 0.9184 time: 0.06s
Epoch 525/1000, LR 0.000127
Train loss: 0.5201;  Loss pred: 0.5201; Loss self: 0.0000; time: 0.21s
Val loss: 0.0418 score: 0.9796 time: 0.07s
Test loss: 0.2300 score: 0.9184 time: 0.06s
Epoch 526/1000, LR 0.000126
Train loss: 0.5169;  Loss pred: 0.5169; Loss self: 0.0000; time: 0.22s
Val loss: 0.0417 score: 0.9796 time: 0.07s
Test loss: 0.2300 score: 0.9184 time: 0.06s
Epoch 527/1000, LR 0.000126
Train loss: 0.5170;  Loss pred: 0.5170; Loss self: 0.0000; time: 0.21s
Val loss: 0.0417 score: 0.9796 time: 0.07s
Test loss: 0.2300 score: 0.9184 time: 0.06s
Epoch 528/1000, LR 0.000126
Train loss: 0.5187;  Loss pred: 0.5187; Loss self: 0.0000; time: 0.23s
Val loss: 0.0416 score: 0.9796 time: 0.09s
Test loss: 0.2300 score: 0.9184 time: 0.06s
Epoch 529/1000, LR 0.000125
Train loss: 0.5173;  Loss pred: 0.5173; Loss self: 0.0000; time: 0.66s
Val loss: 0.0415 score: 0.9796 time: 0.30s
Test loss: 0.2301 score: 0.9184 time: 1.53s
Epoch 530/1000, LR 0.000125
Train loss: 0.5177;  Loss pred: 0.5177; Loss self: 0.0000; time: 6.15s
Val loss: 0.0415 score: 0.9796 time: 0.08s
Test loss: 0.2302 score: 0.9184 time: 0.06s
Epoch 531/1000, LR 0.000124
Train loss: 0.5189;  Loss pred: 0.5189; Loss self: 0.0000; time: 0.20s
Val loss: 0.0414 score: 0.9796 time: 0.07s
Test loss: 0.2303 score: 0.9184 time: 0.06s
Epoch 532/1000, LR 0.000124
Train loss: 0.5191;  Loss pred: 0.5191; Loss self: 0.0000; time: 0.20s
Val loss: 0.0414 score: 0.9796 time: 0.07s
Test loss: 0.2305 score: 0.9184 time: 0.06s
Epoch 533/1000, LR 0.000123
Train loss: 0.5165;  Loss pred: 0.5165; Loss self: 0.0000; time: 0.20s
Val loss: 0.0413 score: 1.0000 time: 0.07s
Test loss: 0.2306 score: 0.9184 time: 0.06s
Epoch 534/1000, LR 0.000123
Train loss: 0.5183;  Loss pred: 0.5183; Loss self: 0.0000; time: 0.20s
Val loss: 0.0413 score: 1.0000 time: 0.07s
Test loss: 0.2307 score: 0.9184 time: 0.06s
Epoch 535/1000, LR 0.000123
Train loss: 0.5184;  Loss pred: 0.5184; Loss self: 0.0000; time: 0.20s
Val loss: 0.0412 score: 1.0000 time: 0.07s
Test loss: 0.2307 score: 0.9184 time: 0.06s
Epoch 536/1000, LR 0.000122
Train loss: 0.5163;  Loss pred: 0.5163; Loss self: 0.0000; time: 0.20s
Val loss: 0.0411 score: 1.0000 time: 0.07s
Test loss: 0.2307 score: 0.9184 time: 0.06s
Epoch 537/1000, LR 0.000122
Train loss: 0.5173;  Loss pred: 0.5173; Loss self: 0.0000; time: 0.20s
Val loss: 0.0411 score: 1.0000 time: 0.07s
Test loss: 0.2307 score: 0.9184 time: 0.06s
Epoch 538/1000, LR 0.000121
Train loss: 0.5183;  Loss pred: 0.5183; Loss self: 0.0000; time: 0.21s
Val loss: 0.0410 score: 1.0000 time: 0.07s
Test loss: 0.2306 score: 0.9184 time: 0.06s
Epoch 539/1000, LR 0.000121
Train loss: 0.5193;  Loss pred: 0.5193; Loss self: 0.0000; time: 0.20s
Val loss: 0.0409 score: 1.0000 time: 0.07s
Test loss: 0.2305 score: 0.9184 time: 0.06s
Epoch 540/1000, LR 0.000120
Train loss: 0.5165;  Loss pred: 0.5165; Loss self: 0.0000; time: 0.20s
Val loss: 0.0408 score: 1.0000 time: 0.07s
Test loss: 0.2304 score: 0.9184 time: 0.06s
Epoch 541/1000, LR 0.000120
Train loss: 0.5159;  Loss pred: 0.5159; Loss self: 0.0000; time: 5.38s
Val loss: 0.0408 score: 0.9796 time: 0.86s
Test loss: 0.2303 score: 0.9184 time: 0.69s
Epoch 542/1000, LR 0.000120
Train loss: 0.5172;  Loss pred: 0.5172; Loss self: 0.0000; time: 0.29s
Val loss: 0.0407 score: 0.9796 time: 0.07s
Test loss: 0.2302 score: 0.9184 time: 0.06s
Epoch 543/1000, LR 0.000119
Train loss: 0.5169;  Loss pred: 0.5169; Loss self: 0.0000; time: 0.21s
Val loss: 0.0406 score: 0.9796 time: 0.07s
Test loss: 0.2301 score: 0.9184 time: 0.06s
Epoch 544/1000, LR 0.000119
Train loss: 0.5163;  Loss pred: 0.5163; Loss self: 0.0000; time: 0.21s
Val loss: 0.0405 score: 0.9796 time: 0.07s
Test loss: 0.2299 score: 0.9184 time: 0.06s
Epoch 545/1000, LR 0.000118
Train loss: 0.5183;  Loss pred: 0.5183; Loss self: 0.0000; time: 0.21s
Val loss: 0.0404 score: 0.9796 time: 0.07s
Test loss: 0.2298 score: 0.9184 time: 0.06s
Epoch 546/1000, LR 0.000118
Train loss: 0.5151;  Loss pred: 0.5151; Loss self: 0.0000; time: 0.21s
Val loss: 0.0404 score: 0.9796 time: 0.07s
Test loss: 0.2297 score: 0.9184 time: 0.06s
Epoch 547/1000, LR 0.000117
Train loss: 0.5174;  Loss pred: 0.5174; Loss self: 0.0000; time: 0.21s
Val loss: 0.0403 score: 0.9796 time: 0.07s
Test loss: 0.2297 score: 0.9184 time: 0.06s
Epoch 548/1000, LR 0.000117
Train loss: 0.5177;  Loss pred: 0.5177; Loss self: 0.0000; time: 0.22s
Val loss: 0.0403 score: 0.9796 time: 2.90s
Test loss: 0.2297 score: 0.9184 time: 2.87s
Epoch 549/1000, LR 0.000117
Train loss: 0.5152;  Loss pred: 0.5152; Loss self: 0.0000; time: 6.56s
Val loss: 0.0402 score: 0.9796 time: 1.78s
Test loss: 0.2298 score: 0.9184 time: 0.24s
Epoch 550/1000, LR 0.000116
Train loss: 0.5162;  Loss pred: 0.5162; Loss self: 0.0000; time: 0.22s
Val loss: 0.0401 score: 0.9796 time: 0.07s
Test loss: 0.2298 score: 0.9184 time: 0.06s
Epoch 551/1000, LR 0.000116
Train loss: 0.5166;  Loss pred: 0.5166; Loss self: 0.0000; time: 0.21s
Val loss: 0.0401 score: 0.9796 time: 0.07s
Test loss: 0.2299 score: 0.9184 time: 0.06s
Epoch 552/1000, LR 0.000115
Train loss: 0.5166;  Loss pred: 0.5166; Loss self: 0.0000; time: 0.21s
Val loss: 0.0400 score: 0.9796 time: 0.07s
Test loss: 0.2299 score: 0.9184 time: 0.06s
Epoch 553/1000, LR 0.000115
Train loss: 0.5163;  Loss pred: 0.5163; Loss self: 0.0000; time: 0.22s
Val loss: 0.0400 score: 0.9796 time: 0.07s
Test loss: 0.2300 score: 0.9184 time: 0.06s
Epoch 554/1000, LR 0.000115
Train loss: 0.5166;  Loss pred: 0.5166; Loss self: 0.0000; time: 0.21s
Val loss: 0.0399 score: 1.0000 time: 0.07s
Test loss: 0.2300 score: 0.9184 time: 0.06s
Epoch 555/1000, LR 0.000114
Train loss: 0.5152;  Loss pred: 0.5152; Loss self: 0.0000; time: 0.21s
Val loss: 0.0399 score: 1.0000 time: 0.07s
Test loss: 0.2301 score: 0.9184 time: 0.06s
Epoch 556/1000, LR 0.000114
Train loss: 0.5150;  Loss pred: 0.5150; Loss self: 0.0000; time: 0.22s
Val loss: 0.0398 score: 1.0000 time: 0.07s
Test loss: 0.2302 score: 0.9184 time: 0.07s
Epoch 557/1000, LR 0.000113
Train loss: 0.5150;  Loss pred: 0.5150; Loss self: 0.0000; time: 0.22s
Val loss: 0.0398 score: 1.0000 time: 0.31s
Test loss: 0.2303 score: 0.9184 time: 2.56s
Epoch 558/1000, LR 0.000113
Train loss: 0.5159;  Loss pred: 0.5159; Loss self: 0.0000; time: 4.86s
Val loss: 0.0397 score: 1.0000 time: 0.08s
Test loss: 0.2303 score: 0.9184 time: 0.07s
Epoch 559/1000, LR 0.000112
Train loss: 0.5164;  Loss pred: 0.5164; Loss self: 0.0000; time: 0.21s
Val loss: 0.0397 score: 1.0000 time: 0.07s
Test loss: 0.2304 score: 0.9184 time: 0.07s
Epoch 560/1000, LR 0.000112
Train loss: 0.5154;  Loss pred: 0.5154; Loss self: 0.0000; time: 0.21s
Val loss: 0.0396 score: 1.0000 time: 0.08s
Test loss: 0.2305 score: 0.9184 time: 0.07s
Epoch 561/1000, LR 0.000112
Train loss: 0.5150;  Loss pred: 0.5150; Loss self: 0.0000; time: 0.21s
Val loss: 0.0396 score: 1.0000 time: 0.07s
Test loss: 0.2306 score: 0.9184 time: 0.06s
Epoch 562/1000, LR 0.000111
Train loss: 0.5151;  Loss pred: 0.5151; Loss self: 0.0000; time: 0.21s
Val loss: 0.0395 score: 1.0000 time: 0.07s
Test loss: 0.2307 score: 0.9184 time: 0.06s
Epoch 563/1000, LR 0.000111
Train loss: 0.5120;  Loss pred: 0.5120; Loss self: 0.0000; time: 0.21s
Val loss: 0.0395 score: 1.0000 time: 0.07s
Test loss: 0.2307 score: 0.9184 time: 0.06s
Epoch 564/1000, LR 0.000110
Train loss: 0.5164;  Loss pred: 0.5164; Loss self: 0.0000; time: 0.21s
Val loss: 0.0394 score: 1.0000 time: 0.07s
Test loss: 0.2308 score: 0.9184 time: 0.06s
Epoch 565/1000, LR 0.000110
Train loss: 0.5140;  Loss pred: 0.5140; Loss self: 0.0000; time: 0.21s
Val loss: 0.0394 score: 1.0000 time: 0.07s
Test loss: 0.2307 score: 0.9184 time: 0.06s
Epoch 566/1000, LR 0.000109
Train loss: 0.5135;  Loss pred: 0.5135; Loss self: 0.0000; time: 0.21s
Val loss: 0.0393 score: 1.0000 time: 0.07s
Test loss: 0.2307 score: 0.9184 time: 0.06s
Epoch 567/1000, LR 0.000109
Train loss: 0.5144;  Loss pred: 0.5144; Loss self: 0.0000; time: 0.21s
Val loss: 0.0393 score: 1.0000 time: 0.07s
Test loss: 0.2307 score: 0.9184 time: 0.06s
Epoch 568/1000, LR 0.000109
Train loss: 0.5156;  Loss pred: 0.5156; Loss self: 0.0000; time: 0.21s
Val loss: 0.0392 score: 1.0000 time: 0.07s
Test loss: 0.2306 score: 0.9184 time: 0.06s
Epoch 569/1000, LR 0.000108
Train loss: 0.5157;  Loss pred: 0.5157; Loss self: 0.0000; time: 0.22s
Val loss: 0.0391 score: 1.0000 time: 0.08s
Test loss: 0.2306 score: 0.9184 time: 0.06s
Epoch 570/1000, LR 0.000108
Train loss: 0.5122;  Loss pred: 0.5122; Loss self: 0.0000; time: 0.22s
Val loss: 0.0391 score: 1.0000 time: 0.08s
Test loss: 0.2306 score: 0.9184 time: 0.07s
Epoch 571/1000, LR 0.000107
Train loss: 0.5129;  Loss pred: 0.5129; Loss self: 0.0000; time: 0.21s
Val loss: 0.0390 score: 1.0000 time: 0.07s
Test loss: 0.2306 score: 0.9184 time: 0.06s
Epoch 572/1000, LR 0.000107
Train loss: 0.5149;  Loss pred: 0.5149; Loss self: 0.0000; time: 0.21s
Val loss: 0.0390 score: 1.0000 time: 0.07s
Test loss: 0.2305 score: 0.9184 time: 0.06s
Epoch 573/1000, LR 0.000107
Train loss: 0.5143;  Loss pred: 0.5143; Loss self: 0.0000; time: 0.21s
Val loss: 0.0389 score: 1.0000 time: 0.07s
Test loss: 0.2305 score: 0.9184 time: 0.06s
Epoch 574/1000, LR 0.000106
Train loss: 0.5144;  Loss pred: 0.5144; Loss self: 0.0000; time: 0.21s
Val loss: 0.0389 score: 1.0000 time: 0.07s
Test loss: 0.2305 score: 0.9184 time: 0.06s
Epoch 575/1000, LR 0.000106
Train loss: 0.5155;  Loss pred: 0.5155; Loss self: 0.0000; time: 2.66s
Val loss: 0.0388 score: 1.0000 time: 1.70s
Test loss: 0.2304 score: 0.9184 time: 1.70s
Epoch 576/1000, LR 0.000105
Train loss: 0.5133;  Loss pred: 0.5133; Loss self: 0.0000; time: 1.37s
Val loss: 0.0387 score: 1.0000 time: 0.16s
Test loss: 0.2304 score: 0.9184 time: 0.06s
Epoch 577/1000, LR 0.000105
Train loss: 0.5125;  Loss pred: 0.5125; Loss self: 0.0000; time: 0.20s
Val loss: 0.0387 score: 1.0000 time: 0.07s
Test loss: 0.2303 score: 0.9184 time: 0.06s
Epoch 578/1000, LR 0.000104
Train loss: 0.5122;  Loss pred: 0.5122; Loss self: 0.0000; time: 0.20s
Val loss: 0.0386 score: 1.0000 time: 0.07s
Test loss: 0.2302 score: 0.9184 time: 0.06s
Epoch 579/1000, LR 0.000104
Train loss: 0.5158;  Loss pred: 0.5158; Loss self: 0.0000; time: 0.20s
Val loss: 0.0386 score: 1.0000 time: 0.07s
Test loss: 0.2301 score: 0.9184 time: 0.06s
Epoch 580/1000, LR 0.000104
Train loss: 0.5128;  Loss pred: 0.5128; Loss self: 0.0000; time: 0.20s
Val loss: 0.0385 score: 1.0000 time: 0.07s
Test loss: 0.2300 score: 0.9184 time: 0.06s
Epoch 581/1000, LR 0.000103
Train loss: 0.5159;  Loss pred: 0.5159; Loss self: 0.0000; time: 0.21s
Val loss: 0.0384 score: 1.0000 time: 0.07s
Test loss: 0.2299 score: 0.9184 time: 0.06s
Epoch 582/1000, LR 0.000103
Train loss: 0.5137;  Loss pred: 0.5137; Loss self: 0.0000; time: 0.31s
Val loss: 0.0384 score: 1.0000 time: 0.07s
Test loss: 0.2298 score: 0.9184 time: 0.06s
Epoch 583/1000, LR 0.000102
Train loss: 0.5112;  Loss pred: 0.5112; Loss self: 0.0000; time: 0.20s
Val loss: 0.0383 score: 1.0000 time: 0.07s
Test loss: 0.2299 score: 0.9184 time: 0.06s
Epoch 584/1000, LR 0.000102
Train loss: 0.5135;  Loss pred: 0.5135; Loss self: 0.0000; time: 0.21s
Val loss: 0.0383 score: 1.0000 time: 0.07s
Test loss: 0.2299 score: 0.9184 time: 0.06s
Epoch 585/1000, LR 0.000102
Train loss: 0.5126;  Loss pred: 0.5126; Loss self: 0.0000; time: 0.21s
Val loss: 0.0382 score: 1.0000 time: 0.07s
Test loss: 0.2299 score: 0.9184 time: 0.06s
Epoch 586/1000, LR 0.000101
Train loss: 0.5131;  Loss pred: 0.5131; Loss self: 0.0000; time: 0.23s
Val loss: 0.0382 score: 1.0000 time: 0.10s
Test loss: 0.2299 score: 0.9184 time: 0.06s
Epoch 587/1000, LR 0.000101
Train loss: 0.5125;  Loss pred: 0.5125; Loss self: 0.0000; time: 0.20s
Val loss: 0.0381 score: 1.0000 time: 0.07s
Test loss: 0.2299 score: 0.9184 time: 0.06s
Epoch 588/1000, LR 0.000100
Train loss: 0.5130;  Loss pred: 0.5130; Loss self: 0.0000; time: 0.20s
Val loss: 0.0381 score: 1.0000 time: 0.07s
Test loss: 0.2300 score: 0.9184 time: 0.06s
Epoch 589/1000, LR 0.000100
Train loss: 0.5134;  Loss pred: 0.5134; Loss self: 0.0000; time: 0.20s
Val loss: 0.0381 score: 1.0000 time: 0.07s
Test loss: 0.2300 score: 0.9184 time: 0.06s
Epoch 590/1000, LR 0.000099
Train loss: 0.5120;  Loss pred: 0.5120; Loss self: 0.0000; time: 0.21s
Val loss: 0.0380 score: 1.0000 time: 0.07s
Test loss: 0.2299 score: 0.9184 time: 0.06s
Epoch 591/1000, LR 0.000099
Train loss: 0.5114;  Loss pred: 0.5114; Loss self: 0.0000; time: 2.29s
Val loss: 0.0380 score: 1.0000 time: 0.48s
Test loss: 0.2299 score: 0.9184 time: 0.38s
Epoch 592/1000, LR 0.000099
Train loss: 0.5128;  Loss pred: 0.5128; Loss self: 0.0000; time: 2.94s
Val loss: 0.0379 score: 1.0000 time: 0.68s
Test loss: 0.2299 score: 0.9184 time: 0.74s
Epoch 593/1000, LR 0.000098
Train loss: 0.5124;  Loss pred: 0.5124; Loss self: 0.0000; time: 1.59s
Val loss: 0.0379 score: 1.0000 time: 0.14s
Test loss: 0.2299 score: 0.9184 time: 0.07s
Epoch 594/1000, LR 0.000098
Train loss: 0.5103;  Loss pred: 0.5103; Loss self: 0.0000; time: 0.22s
Val loss: 0.0378 score: 1.0000 time: 0.07s
Test loss: 0.2300 score: 0.9184 time: 0.06s
Epoch 595/1000, LR 0.000097
Train loss: 0.5110;  Loss pred: 0.5110; Loss self: 0.0000; time: 0.22s
Val loss: 0.0378 score: 1.0000 time: 0.07s
Test loss: 0.2300 score: 0.9184 time: 0.06s
Epoch 596/1000, LR 0.000097
Train loss: 0.5142;  Loss pred: 0.5142; Loss self: 0.0000; time: 0.22s
Val loss: 0.0378 score: 1.0000 time: 0.07s
Test loss: 0.2301 score: 0.9184 time: 0.07s
Epoch 597/1000, LR 0.000097
Train loss: 0.5135;  Loss pred: 0.5135; Loss self: 0.0000; time: 0.22s
Val loss: 0.0377 score: 1.0000 time: 0.07s
Test loss: 0.2302 score: 0.9184 time: 0.06s
Epoch 598/1000, LR 0.000096
Train loss: 0.5134;  Loss pred: 0.5134; Loss self: 0.0000; time: 0.22s
Val loss: 0.0377 score: 1.0000 time: 0.07s
Test loss: 0.2304 score: 0.9184 time: 0.06s
Epoch 599/1000, LR 0.000096
Train loss: 0.5126;  Loss pred: 0.5126; Loss self: 0.0000; time: 0.22s
Val loss: 0.0377 score: 1.0000 time: 0.07s
Test loss: 0.2305 score: 0.9184 time: 0.06s
Epoch 600/1000, LR 0.000095
Train loss: 0.5118;  Loss pred: 0.5118; Loss self: 0.0000; time: 0.22s
Val loss: 0.0377 score: 1.0000 time: 0.07s
Test loss: 0.2306 score: 0.9184 time: 0.06s
Epoch 601/1000, LR 0.000095
Train loss: 0.5126;  Loss pred: 0.5126; Loss self: 0.0000; time: 0.22s
Val loss: 0.0376 score: 1.0000 time: 0.07s
Test loss: 0.2307 score: 0.9184 time: 2.11s
Epoch 602/1000, LR 0.000095
Train loss: 0.5096;  Loss pred: 0.5096; Loss self: 0.0000; time: 3.98s
Val loss: 0.0376 score: 1.0000 time: 0.15s
Test loss: 0.2307 score: 0.9184 time: 0.19s
Epoch 603/1000, LR 0.000094
Train loss: 0.5123;  Loss pred: 0.5123; Loss self: 0.0000; time: 0.27s
Val loss: 0.0376 score: 1.0000 time: 0.10s
Test loss: 0.2308 score: 0.9184 time: 0.07s
Epoch 604/1000, LR 0.000094
Train loss: 0.5127;  Loss pred: 0.5127; Loss self: 0.0000; time: 0.23s
Val loss: 0.0375 score: 1.0000 time: 0.08s
Test loss: 0.2308 score: 0.9184 time: 0.07s
Epoch 605/1000, LR 0.000093
Train loss: 0.5120;  Loss pred: 0.5120; Loss self: 0.0000; time: 0.23s
Val loss: 0.0375 score: 1.0000 time: 0.08s
Test loss: 0.2309 score: 0.9184 time: 0.07s
Epoch 606/1000, LR 0.000093
Train loss: 0.5128;  Loss pred: 0.5128; Loss self: 0.0000; time: 0.23s
Val loss: 0.0375 score: 1.0000 time: 0.08s
Test loss: 0.2309 score: 0.9184 time: 0.07s
Epoch 607/1000, LR 0.000092
Train loss: 0.5132;  Loss pred: 0.5132; Loss self: 0.0000; time: 0.23s
Val loss: 0.0374 score: 1.0000 time: 0.07s
Test loss: 0.2310 score: 0.9184 time: 0.06s
Epoch 608/1000, LR 0.000092
Train loss: 0.5126;  Loss pred: 0.5126; Loss self: 0.0000; time: 0.22s
Val loss: 0.0374 score: 1.0000 time: 0.07s
Test loss: 0.2311 score: 0.9184 time: 0.06s
Epoch 609/1000, LR 0.000092
Train loss: 0.5123;  Loss pred: 0.5123; Loss self: 0.0000; time: 0.22s
Val loss: 0.0374 score: 1.0000 time: 0.07s
Test loss: 0.2311 score: 0.9184 time: 0.07s
Epoch 610/1000, LR 0.000091
Train loss: 0.5100;  Loss pred: 0.5100; Loss self: 0.0000; time: 0.22s
Val loss: 0.0373 score: 1.0000 time: 0.07s
Test loss: 0.2311 score: 0.9184 time: 0.06s
Epoch 611/1000, LR 0.000091
Train loss: 0.5114;  Loss pred: 0.5114; Loss self: 0.0000; time: 0.22s
Val loss: 0.0373 score: 1.0000 time: 0.07s
Test loss: 0.2310 score: 0.9184 time: 0.07s
Epoch 612/1000, LR 0.000090
Train loss: 0.5125;  Loss pred: 0.5125; Loss self: 0.0000; time: 0.22s
Val loss: 0.0373 score: 1.0000 time: 0.07s
Test loss: 0.2310 score: 0.9184 time: 0.06s
Epoch 613/1000, LR 0.000090
Train loss: 0.5122;  Loss pred: 0.5122; Loss self: 0.0000; time: 0.22s
Val loss: 0.0372 score: 1.0000 time: 0.07s
Test loss: 0.2309 score: 0.9184 time: 0.06s
Epoch 614/1000, LR 0.000090
Train loss: 0.5115;  Loss pred: 0.5115; Loss self: 0.0000; time: 0.22s
Val loss: 0.0372 score: 1.0000 time: 0.07s
Test loss: 0.2308 score: 0.9184 time: 0.06s
Epoch 615/1000, LR 0.000089
Train loss: 0.5099;  Loss pred: 0.5099; Loss self: 0.0000; time: 3.15s
Val loss: 0.0371 score: 1.0000 time: 2.33s
Test loss: 0.2307 score: 0.9184 time: 2.23s
Epoch 616/1000, LR 0.000089
Train loss: 0.5145;  Loss pred: 0.5145; Loss self: 0.0000; time: 3.84s
Val loss: 0.0370 score: 1.0000 time: 0.16s
Test loss: 0.2305 score: 0.9184 time: 0.11s
Epoch 617/1000, LR 0.000088
Train loss: 0.5114;  Loss pred: 0.5114; Loss self: 0.0000; time: 0.45s
Val loss: 0.0370 score: 1.0000 time: 0.09s
Test loss: 0.2303 score: 0.9184 time: 0.07s
Epoch 618/1000, LR 0.000088
Train loss: 0.5121;  Loss pred: 0.5121; Loss self: 0.0000; time: 0.23s
Val loss: 0.0369 score: 1.0000 time: 0.08s
Test loss: 0.2301 score: 0.9184 time: 0.07s
Epoch 619/1000, LR 0.000088
Train loss: 0.5122;  Loss pred: 0.5122; Loss self: 0.0000; time: 0.23s
Val loss: 0.0369 score: 1.0000 time: 0.08s
Test loss: 0.2299 score: 0.9184 time: 0.07s
Epoch 620/1000, LR 0.000087
Train loss: 0.5097;  Loss pred: 0.5097; Loss self: 0.0000; time: 0.23s
Val loss: 0.0368 score: 1.0000 time: 0.07s
Test loss: 0.2297 score: 0.9184 time: 0.07s
Epoch 621/1000, LR 0.000087
Train loss: 0.5106;  Loss pred: 0.5106; Loss self: 0.0000; time: 0.23s
Val loss: 0.0368 score: 1.0000 time: 0.08s
Test loss: 0.2296 score: 0.9184 time: 0.07s
Epoch 622/1000, LR 0.000086
Train loss: 0.5095;  Loss pred: 0.5095; Loss self: 0.0000; time: 0.23s
Val loss: 0.0367 score: 1.0000 time: 0.07s
Test loss: 0.2294 score: 0.9184 time: 0.07s
Epoch 623/1000, LR 0.000086
Train loss: 0.5091;  Loss pred: 0.5091; Loss self: 0.0000; time: 0.22s
Val loss: 0.0367 score: 1.0000 time: 0.07s
Test loss: 0.2294 score: 0.9184 time: 0.07s
Epoch 624/1000, LR 0.000086
Train loss: 0.5126;  Loss pred: 0.5126; Loss self: 0.0000; time: 0.23s
Val loss: 0.0366 score: 1.0000 time: 0.09s
Test loss: 0.2293 score: 0.9184 time: 0.07s
Epoch 625/1000, LR 0.000085
Train loss: 0.5124;  Loss pred: 0.5124; Loss self: 0.0000; time: 0.25s
Val loss: 0.0366 score: 1.0000 time: 0.08s
Test loss: 0.2292 score: 0.9184 time: 0.07s
Epoch 626/1000, LR 0.000085
Train loss: 0.5113;  Loss pred: 0.5113; Loss self: 0.0000; time: 0.47s
Val loss: 0.0365 score: 1.0000 time: 0.57s
Test loss: 0.2292 score: 0.9184 time: 2.30s
Epoch 627/1000, LR 0.000084
Train loss: 0.5099;  Loss pred: 0.5099; Loss self: 0.0000; time: 2.47s
Val loss: 0.0365 score: 1.0000 time: 0.07s
Test loss: 0.2292 score: 0.9184 time: 0.07s
Epoch 628/1000, LR 0.000084
Train loss: 0.5093;  Loss pred: 0.5093; Loss self: 0.0000; time: 0.23s
Val loss: 0.0365 score: 1.0000 time: 0.07s
Test loss: 0.2292 score: 0.9184 time: 0.07s
Epoch 629/1000, LR 0.000084
Train loss: 0.5106;  Loss pred: 0.5106; Loss self: 0.0000; time: 0.22s
Val loss: 0.0364 score: 1.0000 time: 0.07s
Test loss: 0.2293 score: 0.9184 time: 0.07s
Epoch 630/1000, LR 0.000083
Train loss: 0.5103;  Loss pred: 0.5103; Loss self: 0.0000; time: 0.22s
Val loss: 0.0364 score: 1.0000 time: 0.07s
Test loss: 0.2294 score: 0.9184 time: 0.07s
Epoch 631/1000, LR 0.000083
Train loss: 0.5100;  Loss pred: 0.5100; Loss self: 0.0000; time: 0.22s
Val loss: 0.0364 score: 1.0000 time: 0.07s
Test loss: 0.2296 score: 0.9184 time: 0.06s
Epoch 632/1000, LR 0.000082
Train loss: 0.5112;  Loss pred: 0.5112; Loss self: 0.0000; time: 0.21s
Val loss: 0.0364 score: 1.0000 time: 0.07s
Test loss: 0.2297 score: 0.9184 time: 0.06s
Epoch 633/1000, LR 0.000082
Train loss: 0.5100;  Loss pred: 0.5100; Loss self: 0.0000; time: 0.21s
Val loss: 0.0364 score: 1.0000 time: 0.07s
Test loss: 0.2299 score: 0.9184 time: 0.06s
Epoch 634/1000, LR 0.000082
Train loss: 0.5107;  Loss pred: 0.5107; Loss self: 0.0000; time: 0.21s
Val loss: 0.0364 score: 1.0000 time: 0.07s
Test loss: 0.2300 score: 0.9184 time: 0.06s
Epoch 635/1000, LR 0.000081
Train loss: 0.5108;  Loss pred: 0.5108; Loss self: 0.0000; time: 0.21s
Val loss: 0.0363 score: 1.0000 time: 0.07s
Test loss: 0.2302 score: 0.9184 time: 0.06s
Epoch 636/1000, LR 0.000081
Train loss: 0.5105;  Loss pred: 0.5105; Loss self: 0.0000; time: 0.21s
Val loss: 0.0363 score: 1.0000 time: 0.07s
Test loss: 0.2304 score: 0.9184 time: 0.06s
Epoch 637/1000, LR 0.000080
Train loss: 0.5108;  Loss pred: 0.5108; Loss self: 0.0000; time: 0.23s
Val loss: 0.0363 score: 1.0000 time: 0.07s
Test loss: 0.2305 score: 0.9184 time: 0.07s
Epoch 638/1000, LR 0.000080
Train loss: 0.5098;  Loss pred: 0.5098; Loss self: 0.0000; time: 0.23s
Val loss: 0.0363 score: 1.0000 time: 0.07s
Test loss: 0.2306 score: 0.9184 time: 0.06s
Epoch 639/1000, LR 0.000080
Train loss: 0.5090;  Loss pred: 0.5090; Loss self: 0.0000; time: 0.21s
Val loss: 0.0363 score: 1.0000 time: 0.07s
Test loss: 0.2307 score: 0.9184 time: 0.06s
Epoch 640/1000, LR 0.000079
Train loss: 0.5102;  Loss pred: 0.5102; Loss self: 0.0000; time: 0.21s
Val loss: 0.0362 score: 1.0000 time: 0.07s
Test loss: 0.2307 score: 0.9184 time: 0.06s
Epoch 641/1000, LR 0.000079
Train loss: 0.5095;  Loss pred: 0.5095; Loss self: 0.0000; time: 0.21s
Val loss: 0.0362 score: 1.0000 time: 0.23s
Test loss: 0.2308 score: 0.9184 time: 2.52s
Epoch 642/1000, LR 0.000079
Train loss: 0.5092;  Loss pred: 0.5092; Loss self: 0.0000; time: 4.00s
Val loss: 0.0362 score: 1.0000 time: 0.11s
Test loss: 0.2308 score: 0.9184 time: 0.13s
Epoch 643/1000, LR 0.000078
Train loss: 0.5098;  Loss pred: 0.5098; Loss self: 0.0000; time: 0.23s
Val loss: 0.0362 score: 1.0000 time: 0.08s
Test loss: 0.2308 score: 0.9184 time: 0.07s
Epoch 644/1000, LR 0.000078
Train loss: 0.5103;  Loss pred: 0.5103; Loss self: 0.0000; time: 0.24s
Val loss: 0.0361 score: 1.0000 time: 0.08s
Test loss: 0.2308 score: 0.9184 time: 0.07s
Epoch 645/1000, LR 0.000077
Train loss: 0.5094;  Loss pred: 0.5094; Loss self: 0.0000; time: 0.22s
Val loss: 0.0361 score: 1.0000 time: 0.07s
Test loss: 0.2308 score: 0.9184 time: 0.07s
Epoch 646/1000, LR 0.000077
Train loss: 0.5106;  Loss pred: 0.5106; Loss self: 0.0000; time: 0.22s
Val loss: 0.0361 score: 1.0000 time: 0.07s
Test loss: 0.2308 score: 0.9184 time: 0.07s
Epoch 647/1000, LR 0.000077
Train loss: 0.5092;  Loss pred: 0.5092; Loss self: 0.0000; time: 0.22s
Val loss: 0.0360 score: 1.0000 time: 0.07s
Test loss: 0.2309 score: 0.9184 time: 0.07s
Epoch 648/1000, LR 0.000076
Train loss: 0.5080;  Loss pred: 0.5080; Loss self: 0.0000; time: 0.23s
Val loss: 0.0360 score: 1.0000 time: 0.08s
Test loss: 0.2309 score: 0.9184 time: 0.07s
Epoch 649/1000, LR 0.000076
Train loss: 0.5098;  Loss pred: 0.5098; Loss self: 0.0000; time: 0.22s
Val loss: 0.0360 score: 1.0000 time: 0.07s
Test loss: 0.2310 score: 0.9184 time: 0.06s
Epoch 650/1000, LR 0.000075
Train loss: 0.5102;  Loss pred: 0.5102; Loss self: 0.0000; time: 0.21s
Val loss: 0.0360 score: 1.0000 time: 0.07s
Test loss: 0.2310 score: 0.9184 time: 0.06s
Epoch 651/1000, LR 0.000075
Train loss: 0.5092;  Loss pred: 0.5092; Loss self: 0.0000; time: 0.21s
Val loss: 0.0359 score: 1.0000 time: 0.07s
Test loss: 0.2311 score: 0.9184 time: 0.06s
Epoch 652/1000, LR 0.000075
Train loss: 0.5105;  Loss pred: 0.5105; Loss self: 0.0000; time: 0.21s
Val loss: 0.0359 score: 1.0000 time: 1.11s
Test loss: 0.2312 score: 0.9184 time: 1.53s
Epoch 653/1000, LR 0.000074
Train loss: 0.5095;  Loss pred: 0.5095; Loss self: 0.0000; time: 5.30s
Val loss: 0.0359 score: 1.0000 time: 0.07s
Test loss: 0.2312 score: 0.9184 time: 0.06s
Epoch 654/1000, LR 0.000074
Train loss: 0.5089;  Loss pred: 0.5089; Loss self: 0.0000; time: 0.21s
Val loss: 0.0359 score: 1.0000 time: 0.07s
Test loss: 0.2313 score: 0.9184 time: 0.06s
Epoch 655/1000, LR 0.000074
Train loss: 0.5090;  Loss pred: 0.5090; Loss self: 0.0000; time: 0.20s
Val loss: 0.0359 score: 1.0000 time: 0.07s
Test loss: 0.2313 score: 0.9184 time: 0.06s
Epoch 656/1000, LR 0.000073
Train loss: 0.5069;  Loss pred: 0.5069; Loss self: 0.0000; time: 0.20s
Val loss: 0.0358 score: 1.0000 time: 0.07s
Test loss: 0.2313 score: 0.9184 time: 0.06s
Epoch 657/1000, LR 0.000073
Train loss: 0.5083;  Loss pred: 0.5083; Loss self: 0.0000; time: 0.24s
Val loss: 0.0358 score: 1.0000 time: 0.07s
Test loss: 0.2314 score: 0.9184 time: 0.06s
Epoch 658/1000, LR 0.000072
Train loss: 0.5091;  Loss pred: 0.5091; Loss self: 0.0000; time: 0.20s
Val loss: 0.0358 score: 1.0000 time: 0.07s
Test loss: 0.2314 score: 0.9184 time: 0.06s
Epoch 659/1000, LR 0.000072
Train loss: 0.5091;  Loss pred: 0.5091; Loss self: 0.0000; time: 0.20s
Val loss: 0.0358 score: 1.0000 time: 0.07s
Test loss: 0.2313 score: 0.9184 time: 0.06s
Epoch 660/1000, LR 0.000072
Train loss: 0.5092;  Loss pred: 0.5092; Loss self: 0.0000; time: 0.20s
Val loss: 0.0357 score: 1.0000 time: 0.07s
Test loss: 0.2313 score: 0.9184 time: 0.06s
Epoch 661/1000, LR 0.000071
Train loss: 0.5098;  Loss pred: 0.5098; Loss self: 0.0000; time: 0.20s
Val loss: 0.0357 score: 1.0000 time: 0.07s
Test loss: 0.2313 score: 0.9184 time: 0.06s
Epoch 662/1000, LR 0.000071
Train loss: 0.5075;  Loss pred: 0.5075; Loss self: 0.0000; time: 0.20s
Val loss: 0.0357 score: 1.0000 time: 0.07s
Test loss: 0.2312 score: 0.9184 time: 0.06s
Epoch 663/1000, LR 0.000070
Train loss: 0.5073;  Loss pred: 0.5073; Loss self: 0.0000; time: 1.53s
Val loss: 0.0356 score: 1.0000 time: 1.66s
Test loss: 0.2312 score: 0.9184 time: 1.25s
Epoch 664/1000, LR 0.000070
Train loss: 0.5094;  Loss pred: 0.5094; Loss self: 0.0000; time: 0.95s
Val loss: 0.0356 score: 1.0000 time: 0.07s
Test loss: 0.2311 score: 0.9184 time: 0.07s
Epoch 665/1000, LR 0.000070
Train loss: 0.5091;  Loss pred: 0.5091; Loss self: 0.0000; time: 0.22s
Val loss: 0.0355 score: 1.0000 time: 0.07s
Test loss: 0.2310 score: 0.9184 time: 0.06s
Epoch 666/1000, LR 0.000069
Train loss: 0.5086;  Loss pred: 0.5086; Loss self: 0.0000; time: 0.22s
Val loss: 0.0355 score: 1.0000 time: 0.07s
Test loss: 0.2310 score: 0.9184 time: 0.06s
Epoch 667/1000, LR 0.000069
Train loss: 0.5091;  Loss pred: 0.5091; Loss self: 0.0000; time: 0.21s
Val loss: 0.0355 score: 1.0000 time: 0.07s
Test loss: 0.2310 score: 0.9184 time: 0.06s
Epoch 668/1000, LR 0.000069
Train loss: 0.5104;  Loss pred: 0.5104; Loss self: 0.0000; time: 0.21s
Val loss: 0.0355 score: 1.0000 time: 0.07s
Test loss: 0.2310 score: 0.9184 time: 0.06s
Epoch 669/1000, LR 0.000068
Train loss: 0.5077;  Loss pred: 0.5077; Loss self: 0.0000; time: 0.21s
Val loss: 0.0354 score: 1.0000 time: 0.07s
Test loss: 0.2310 score: 0.9184 time: 0.06s
Epoch 670/1000, LR 0.000068
Train loss: 0.5086;  Loss pred: 0.5086; Loss self: 0.0000; time: 0.20s
Val loss: 0.0354 score: 1.0000 time: 0.07s
Test loss: 0.2309 score: 0.9184 time: 0.06s
Epoch 671/1000, LR 0.000068
Train loss: 0.5100;  Loss pred: 0.5100; Loss self: 0.0000; time: 0.21s
Val loss: 0.0354 score: 1.0000 time: 0.07s
Test loss: 0.2310 score: 0.9184 time: 0.06s
Epoch 672/1000, LR 0.000067
Train loss: 0.5073;  Loss pred: 0.5073; Loss self: 0.0000; time: 0.21s
Val loss: 0.0354 score: 1.0000 time: 0.07s
Test loss: 0.2310 score: 0.9184 time: 0.07s
Epoch 673/1000, LR 0.000067
Train loss: 0.5085;  Loss pred: 0.5085; Loss self: 0.0000; time: 0.22s
Val loss: 0.0353 score: 1.0000 time: 0.08s
Test loss: 0.2309 score: 0.9184 time: 0.07s
Epoch 674/1000, LR 0.000066
Train loss: 0.5080;  Loss pred: 0.5080; Loss self: 0.0000; time: 0.22s
Val loss: 0.0353 score: 1.0000 time: 0.07s
Test loss: 0.2309 score: 0.9184 time: 0.07s
Epoch 675/1000, LR 0.000066
Train loss: 0.5085;  Loss pred: 0.5085; Loss self: 0.0000; time: 0.22s
Val loss: 0.0353 score: 1.0000 time: 0.07s
Test loss: 0.2308 score: 0.9184 time: 0.07s
Epoch 676/1000, LR 0.000066
Train loss: 0.5095;  Loss pred: 0.5095; Loss self: 0.0000; time: 0.22s
Val loss: 0.0352 score: 1.0000 time: 0.07s
Test loss: 0.2308 score: 0.9184 time: 0.07s
Epoch 677/1000, LR 0.000065
Train loss: 0.5083;  Loss pred: 0.5083; Loss self: 0.0000; time: 3.27s
Val loss: 0.0352 score: 1.0000 time: 1.34s
Test loss: 0.2308 score: 0.9184 time: 2.76s
Epoch 678/1000, LR 0.000065
Train loss: 0.5075;  Loss pred: 0.5075; Loss self: 0.0000; time: 6.38s
Val loss: 0.0352 score: 1.0000 time: 0.09s
Test loss: 0.2308 score: 0.9184 time: 1.04s
Epoch 679/1000, LR 0.000065
Train loss: 0.5083;  Loss pred: 0.5083; Loss self: 0.0000; time: 0.98s
Val loss: 0.0352 score: 1.0000 time: 0.07s
Test loss: 0.2308 score: 0.9184 time: 0.06s
Epoch 680/1000, LR 0.000064
Train loss: 0.5070;  Loss pred: 0.5070; Loss self: 0.0000; time: 0.21s
Val loss: 0.0351 score: 1.0000 time: 0.07s
Test loss: 0.2308 score: 0.9184 time: 0.06s
Epoch 681/1000, LR 0.000064
Train loss: 0.5065;  Loss pred: 0.5065; Loss self: 0.0000; time: 0.21s
Val loss: 0.0351 score: 1.0000 time: 0.07s
Test loss: 0.2309 score: 0.9184 time: 0.06s
Epoch 682/1000, LR 0.000063
Train loss: 0.5087;  Loss pred: 0.5087; Loss self: 0.0000; time: 0.21s
Val loss: 0.0351 score: 1.0000 time: 0.07s
Test loss: 0.2309 score: 0.9184 time: 0.06s
Epoch 683/1000, LR 0.000063
Train loss: 0.5074;  Loss pred: 0.5074; Loss self: 0.0000; time: 0.21s
Val loss: 0.0351 score: 1.0000 time: 0.07s
Test loss: 0.2309 score: 0.9184 time: 0.06s
Epoch 684/1000, LR 0.000063
Train loss: 0.5096;  Loss pred: 0.5096; Loss self: 0.0000; time: 0.21s
Val loss: 0.0350 score: 1.0000 time: 0.07s
Test loss: 0.2309 score: 0.9184 time: 0.06s
Epoch 685/1000, LR 0.000062
Train loss: 0.5099;  Loss pred: 0.5099; Loss self: 0.0000; time: 0.21s
Val loss: 0.0350 score: 1.0000 time: 0.07s
Test loss: 0.2309 score: 0.9184 time: 0.06s
Epoch 686/1000, LR 0.000062
Train loss: 0.5083;  Loss pred: 0.5083; Loss self: 0.0000; time: 0.21s
Val loss: 0.0350 score: 1.0000 time: 0.07s
Test loss: 0.2309 score: 0.9184 time: 0.06s
Epoch 687/1000, LR 0.000062
Train loss: 0.5078;  Loss pred: 0.5078; Loss self: 0.0000; time: 0.21s
Val loss: 0.0350 score: 1.0000 time: 0.07s
Test loss: 0.2309 score: 0.9184 time: 0.06s
Epoch 688/1000, LR 0.000061
Train loss: 0.5091;  Loss pred: 0.5091; Loss self: 0.0000; time: 0.38s
Val loss: 0.0350 score: 1.0000 time: 2.47s
Test loss: 0.2310 score: 0.9184 time: 2.56s
Epoch 689/1000, LR 0.000061
Train loss: 0.5087;  Loss pred: 0.5087; Loss self: 0.0000; time: 5.16s
Val loss: 0.0350 score: 1.0000 time: 0.07s
Test loss: 0.2311 score: 0.9184 time: 0.06s
Epoch 690/1000, LR 0.000061
Train loss: 0.5082;  Loss pred: 0.5082; Loss self: 0.0000; time: 0.22s
Val loss: 0.0350 score: 1.0000 time: 0.07s
Test loss: 0.2311 score: 0.9184 time: 0.06s
Epoch 691/1000, LR 0.000060
Train loss: 0.5069;  Loss pred: 0.5069; Loss self: 0.0000; time: 0.21s
Val loss: 0.0349 score: 1.0000 time: 0.07s
Test loss: 0.2312 score: 0.9184 time: 0.06s
Epoch 692/1000, LR 0.000060
Train loss: 0.5069;  Loss pred: 0.5069; Loss self: 0.0000; time: 0.21s
Val loss: 0.0349 score: 1.0000 time: 0.07s
Test loss: 0.2313 score: 0.9184 time: 0.06s
Epoch 693/1000, LR 0.000060
Train loss: 0.5080;  Loss pred: 0.5080; Loss self: 0.0000; time: 0.21s
Val loss: 0.0349 score: 1.0000 time: 0.07s
Test loss: 0.2313 score: 0.9184 time: 0.06s
Epoch 694/1000, LR 0.000059
Train loss: 0.5068;  Loss pred: 0.5068; Loss self: 0.0000; time: 0.21s
Val loss: 0.0349 score: 1.0000 time: 0.07s
Test loss: 0.2314 score: 0.9184 time: 0.06s
Epoch 695/1000, LR 0.000059
Train loss: 0.5072;  Loss pred: 0.5072; Loss self: 0.0000; time: 0.21s
Val loss: 0.0349 score: 1.0000 time: 0.07s
Test loss: 0.2314 score: 0.9184 time: 0.06s
Epoch 696/1000, LR 0.000058
Train loss: 0.5080;  Loss pred: 0.5080; Loss self: 0.0000; time: 0.23s
Val loss: 0.0349 score: 1.0000 time: 2.33s
Test loss: 0.2315 score: 0.9184 time: 0.34s
Epoch 697/1000, LR 0.000058
Train loss: 0.5080;  Loss pred: 0.5080; Loss self: 0.0000; time: 2.61s
Val loss: 0.0348 score: 1.0000 time: 0.07s
Test loss: 0.2315 score: 0.9184 time: 0.06s
Epoch 698/1000, LR 0.000058
Train loss: 0.5069;  Loss pred: 0.5069; Loss self: 0.0000; time: 0.20s
Val loss: 0.0348 score: 1.0000 time: 0.07s
Test loss: 0.2316 score: 0.9184 time: 0.06s
Epoch 699/1000, LR 0.000057
Train loss: 0.5067;  Loss pred: 0.5067; Loss self: 0.0000; time: 0.21s
Val loss: 0.0348 score: 1.0000 time: 0.09s
Test loss: 0.2316 score: 0.9184 time: 0.06s
Epoch 700/1000, LR 0.000057
Train loss: 0.5096;  Loss pred: 0.5096; Loss self: 0.0000; time: 0.21s
Val loss: 0.0348 score: 1.0000 time: 0.07s
Test loss: 0.2316 score: 0.9184 time: 0.06s
Epoch 701/1000, LR 0.000057
Train loss: 0.5075;  Loss pred: 0.5075; Loss self: 0.0000; time: 0.20s
Val loss: 0.0348 score: 1.0000 time: 0.07s
Test loss: 0.2316 score: 0.9184 time: 0.06s
Epoch 702/1000, LR 0.000056
Train loss: 0.5081;  Loss pred: 0.5081; Loss self: 0.0000; time: 0.21s
Val loss: 0.0348 score: 1.0000 time: 0.07s
Test loss: 0.2316 score: 0.9184 time: 0.06s
Epoch 703/1000, LR 0.000056
Train loss: 0.5070;  Loss pred: 0.5070; Loss self: 0.0000; time: 0.21s
Val loss: 0.0347 score: 1.0000 time: 0.07s
Test loss: 0.2315 score: 0.9184 time: 0.06s
Epoch 704/1000, LR 0.000056
Train loss: 0.5068;  Loss pred: 0.5068; Loss self: 0.0000; time: 0.21s
Val loss: 0.0347 score: 1.0000 time: 0.08s
Test loss: 0.2315 score: 0.9184 time: 0.13s
Epoch 705/1000, LR 0.000055
Train loss: 0.5064;  Loss pred: 0.5064; Loss self: 0.0000; time: 0.20s
Val loss: 0.0347 score: 1.0000 time: 0.07s
Test loss: 0.2315 score: 0.9184 time: 0.06s
Epoch 706/1000, LR 0.000055
Train loss: 0.5068;  Loss pred: 0.5068; Loss self: 0.0000; time: 0.20s
Val loss: 0.0347 score: 1.0000 time: 0.07s
Test loss: 0.2315 score: 0.9184 time: 0.66s
Epoch 707/1000, LR 0.000055
Train loss: 0.5063;  Loss pred: 0.5063; Loss self: 0.0000; time: 6.37s
Val loss: 0.0346 score: 1.0000 time: 0.12s
Test loss: 0.2315 score: 0.9184 time: 0.07s
Epoch 708/1000, LR 0.000054
Train loss: 0.5083;  Loss pred: 0.5083; Loss self: 0.0000; time: 0.22s
Val loss: 0.0346 score: 1.0000 time: 0.07s
Test loss: 0.2315 score: 0.9184 time: 0.06s
Epoch 709/1000, LR 0.000054
Train loss: 0.5059;  Loss pred: 0.5059; Loss self: 0.0000; time: 0.22s
Val loss: 0.0346 score: 1.0000 time: 0.07s
Test loss: 0.2315 score: 0.9184 time: 0.06s
Epoch 710/1000, LR 0.000054
Train loss: 0.5069;  Loss pred: 0.5069; Loss self: 0.0000; time: 0.22s
Val loss: 0.0346 score: 1.0000 time: 0.07s
Test loss: 0.2315 score: 0.9184 time: 0.06s
Epoch 711/1000, LR 0.000053
Train loss: 0.5082;  Loss pred: 0.5082; Loss self: 0.0000; time: 0.21s
Val loss: 0.0346 score: 1.0000 time: 0.07s
Test loss: 0.2315 score: 0.9184 time: 0.06s
Epoch 712/1000, LR 0.000053
Train loss: 0.5056;  Loss pred: 0.5056; Loss self: 0.0000; time: 0.20s
Val loss: 0.0345 score: 1.0000 time: 0.07s
Test loss: 0.2314 score: 0.9184 time: 0.06s
Epoch 713/1000, LR 0.000053
Train loss: 0.5087;  Loss pred: 0.5087; Loss self: 0.0000; time: 0.20s
Val loss: 0.0345 score: 1.0000 time: 0.07s
Test loss: 0.2314 score: 0.9184 time: 0.06s
Epoch 714/1000, LR 0.000052
Train loss: 0.5068;  Loss pred: 0.5068; Loss self: 0.0000; time: 0.20s
Val loss: 0.0345 score: 1.0000 time: 0.07s
Test loss: 0.2313 score: 0.9184 time: 0.06s
Epoch 715/1000, LR 0.000052
Train loss: 0.5089;  Loss pred: 0.5089; Loss self: 0.0000; time: 0.21s
Val loss: 0.0345 score: 1.0000 time: 0.08s
Test loss: 0.2312 score: 0.9184 time: 0.06s
Epoch 716/1000, LR 0.000052
Train loss: 0.5065;  Loss pred: 0.5065; Loss self: 0.0000; time: 0.24s
Val loss: 0.0344 score: 1.0000 time: 0.07s
Test loss: 0.2312 score: 0.9184 time: 0.07s
Epoch 717/1000, LR 0.000051
Train loss: 0.5058;  Loss pred: 0.5058; Loss self: 0.0000; time: 0.41s
Val loss: 0.0344 score: 1.0000 time: 2.58s
Test loss: 0.2311 score: 0.9184 time: 1.14s
Epoch 718/1000, LR 0.000051
Train loss: 0.5069;  Loss pred: 0.5069; Loss self: 0.0000; time: 2.78s
Val loss: 0.0344 score: 1.0000 time: 0.07s
Test loss: 0.2311 score: 0.9184 time: 0.06s
Epoch 719/1000, LR 0.000051
Train loss: 0.5067;  Loss pred: 0.5067; Loss self: 0.0000; time: 0.20s
Val loss: 0.0344 score: 1.0000 time: 0.07s
Test loss: 0.2311 score: 0.9184 time: 0.06s
Epoch 720/1000, LR 0.000050
Train loss: 0.5070;  Loss pred: 0.5070; Loss self: 0.0000; time: 0.20s
Val loss: 0.0343 score: 1.0000 time: 0.07s
Test loss: 0.2311 score: 0.9184 time: 0.06s
Epoch 721/1000, LR 0.000050
Train loss: 0.5071;  Loss pred: 0.5071; Loss self: 0.0000; time: 0.20s
Val loss: 0.0343 score: 1.0000 time: 0.07s
Test loss: 0.2310 score: 0.9184 time: 0.06s
Epoch 722/1000, LR 0.000050
Train loss: 0.5074;  Loss pred: 0.5074; Loss self: 0.0000; time: 0.21s
Val loss: 0.0343 score: 1.0000 time: 0.07s
Test loss: 0.2310 score: 0.9184 time: 0.06s
Epoch 723/1000, LR 0.000049
Train loss: 0.5071;  Loss pred: 0.5071; Loss self: 0.0000; time: 0.21s
Val loss: 0.0343 score: 1.0000 time: 0.07s
Test loss: 0.2310 score: 0.9184 time: 0.06s
Epoch 724/1000, LR 0.000049
Train loss: 0.5079;  Loss pred: 0.5079; Loss self: 0.0000; time: 0.21s
Val loss: 0.0343 score: 1.0000 time: 0.07s
Test loss: 0.2310 score: 0.9184 time: 0.06s
Epoch 725/1000, LR 0.000049
Train loss: 0.5072;  Loss pred: 0.5072; Loss self: 0.0000; time: 0.86s
Val loss: 0.0343 score: 1.0000 time: 2.76s
Test loss: 0.2310 score: 0.9184 time: 2.72s
Epoch 726/1000, LR 0.000048
Train loss: 0.5063;  Loss pred: 0.5063; Loss self: 0.0000; time: 4.92s
Val loss: 0.0342 score: 1.0000 time: 0.29s
Test loss: 0.2311 score: 0.9184 time: 0.06s
Epoch 727/1000, LR 0.000048
Train loss: 0.5073;  Loss pred: 0.5073; Loss self: 0.0000; time: 0.20s
Val loss: 0.0342 score: 1.0000 time: 0.07s
Test loss: 0.2311 score: 0.9184 time: 0.06s
Epoch 728/1000, LR 0.000048
Train loss: 0.5078;  Loss pred: 0.5078; Loss self: 0.0000; time: 0.21s
Val loss: 0.0342 score: 1.0000 time: 0.07s
Test loss: 0.2311 score: 0.9184 time: 0.06s
Epoch 729/1000, LR 0.000047
Train loss: 0.5045;  Loss pred: 0.5045; Loss self: 0.0000; time: 0.21s
Val loss: 0.0342 score: 1.0000 time: 0.07s
Test loss: 0.2312 score: 0.9184 time: 0.06s
Epoch 730/1000, LR 0.000047
Train loss: 0.5076;  Loss pred: 0.5076; Loss self: 0.0000; time: 0.21s
Val loss: 0.0342 score: 1.0000 time: 0.07s
Test loss: 0.2313 score: 0.9184 time: 0.06s
Epoch 731/1000, LR 0.000047
Train loss: 0.5062;  Loss pred: 0.5062; Loss self: 0.0000; time: 0.21s
Val loss: 0.0342 score: 1.0000 time: 0.07s
Test loss: 0.2313 score: 0.9184 time: 0.06s
Epoch 732/1000, LR 0.000046
Train loss: 0.5046;  Loss pred: 0.5046; Loss self: 0.0000; time: 0.21s
Val loss: 0.0342 score: 1.0000 time: 0.07s
Test loss: 0.2314 score: 0.9184 time: 0.06s
Epoch 733/1000, LR 0.000046
Train loss: 0.5086;  Loss pred: 0.5086; Loss self: 0.0000; time: 0.21s
Val loss: 0.0342 score: 1.0000 time: 0.07s
Test loss: 0.2314 score: 0.9184 time: 0.83s
Epoch 734/1000, LR 0.000046
Train loss: 0.5067;  Loss pred: 0.5067; Loss self: 0.0000; time: 7.71s
Val loss: 0.0341 score: 1.0000 time: 1.73s
Test loss: 0.2313 score: 0.9184 time: 0.70s
Epoch 735/1000, LR 0.000045
Train loss: 0.5065;  Loss pred: 0.5065; Loss self: 0.0000; time: 0.26s
Val loss: 0.0341 score: 1.0000 time: 0.07s
Test loss: 0.2313 score: 0.9184 time: 0.06s
Epoch 736/1000, LR 0.000045
Train loss: 0.5056;  Loss pred: 0.5056; Loss self: 0.0000; time: 0.21s
Val loss: 0.0341 score: 1.0000 time: 0.07s
Test loss: 0.2313 score: 0.9184 time: 0.06s
Epoch 737/1000, LR 0.000045
Train loss: 0.5073;  Loss pred: 0.5073; Loss self: 0.0000; time: 0.21s
Val loss: 0.0341 score: 1.0000 time: 0.07s
Test loss: 0.2313 score: 0.9184 time: 0.06s
Epoch 738/1000, LR 0.000044
Train loss: 0.5067;  Loss pred: 0.5067; Loss self: 0.0000; time: 0.21s
Val loss: 0.0341 score: 1.0000 time: 0.07s
Test loss: 0.2313 score: 0.9184 time: 0.06s
Epoch 739/1000, LR 0.000044
Train loss: 0.5068;  Loss pred: 0.5068; Loss self: 0.0000; time: 0.21s
Val loss: 0.0341 score: 1.0000 time: 0.07s
Test loss: 0.2314 score: 0.9184 time: 0.06s
Epoch 740/1000, LR 0.000044
Train loss: 0.5042;  Loss pred: 0.5042; Loss self: 0.0000; time: 0.21s
Val loss: 0.0341 score: 1.0000 time: 0.07s
Test loss: 0.2314 score: 0.9184 time: 0.06s
Epoch 741/1000, LR 0.000043
Train loss: 0.5062;  Loss pred: 0.5062; Loss self: 0.0000; time: 0.21s
Val loss: 0.0340 score: 1.0000 time: 0.07s
Test loss: 0.2313 score: 0.9184 time: 0.06s
Epoch 742/1000, LR 0.000043
Train loss: 0.5061;  Loss pred: 0.5061; Loss self: 0.0000; time: 0.21s
Val loss: 0.0340 score: 1.0000 time: 0.07s
Test loss: 0.2313 score: 0.9184 time: 0.06s
Epoch 743/1000, LR 0.000043
Train loss: 0.5066;  Loss pred: 0.5066; Loss self: 0.0000; time: 0.21s
Val loss: 0.0340 score: 1.0000 time: 0.07s
Test loss: 0.2313 score: 0.9184 time: 0.06s
Epoch 744/1000, LR 0.000042
Train loss: 0.5061;  Loss pred: 0.5061; Loss self: 0.0000; time: 0.21s
Val loss: 0.0340 score: 1.0000 time: 0.07s
Test loss: 0.2313 score: 0.9184 time: 0.06s
Epoch 745/1000, LR 0.000042
Train loss: 0.5061;  Loss pred: 0.5061; Loss self: 0.0000; time: 0.21s
Val loss: 0.0340 score: 1.0000 time: 0.07s
Test loss: 0.2313 score: 0.9184 time: 0.06s
Epoch 746/1000, LR 0.000042
Train loss: 0.5070;  Loss pred: 0.5070; Loss self: 0.0000; time: 0.21s
Val loss: 0.0340 score: 1.0000 time: 0.07s
Test loss: 0.2314 score: 0.9184 time: 0.06s
Epoch 747/1000, LR 0.000042
Train loss: 0.5065;  Loss pred: 0.5065; Loss self: 0.0000; time: 0.21s
Val loss: 0.0340 score: 1.0000 time: 0.07s
Test loss: 0.2314 score: 0.9184 time: 0.06s
Epoch 748/1000, LR 0.000041
Train loss: 0.5049;  Loss pred: 0.5049; Loss self: 0.0000; time: 0.21s
Val loss: 0.0339 score: 1.0000 time: 0.07s
Test loss: 0.2314 score: 0.9184 time: 0.06s
Epoch 749/1000, LR 0.000041
Train loss: 0.5042;  Loss pred: 0.5042; Loss self: 0.0000; time: 0.21s
Val loss: 0.0339 score: 1.0000 time: 0.07s
Test loss: 0.2314 score: 0.9184 time: 0.06s
Epoch 750/1000, LR 0.000041
Train loss: 0.5073;  Loss pred: 0.5073; Loss self: 0.0000; time: 0.21s
Val loss: 0.0339 score: 1.0000 time: 0.07s
Test loss: 0.2315 score: 0.9184 time: 0.06s
Epoch 751/1000, LR 0.000040
Train loss: 0.5064;  Loss pred: 0.5064; Loss self: 0.0000; time: 0.21s
Val loss: 0.0339 score: 1.0000 time: 0.07s
Test loss: 0.2315 score: 0.9184 time: 0.06s
Epoch 752/1000, LR 0.000040
Train loss: 0.5054;  Loss pred: 0.5054; Loss self: 0.0000; time: 0.21s
Val loss: 0.0339 score: 1.0000 time: 0.07s
Test loss: 0.2316 score: 0.9184 time: 0.06s
Epoch 753/1000, LR 0.000040
Train loss: 0.5071;  Loss pred: 0.5071; Loss self: 0.0000; time: 0.21s
Val loss: 0.0339 score: 1.0000 time: 0.07s
Test loss: 0.2316 score: 0.9184 time: 0.06s
Epoch 754/1000, LR 0.000039
Train loss: 0.5065;  Loss pred: 0.5065; Loss self: 0.0000; time: 0.21s
Val loss: 0.0339 score: 1.0000 time: 0.07s
Test loss: 0.2316 score: 0.9184 time: 0.06s
Epoch 755/1000, LR 0.000039
Train loss: 0.5061;  Loss pred: 0.5061; Loss self: 0.0000; time: 0.21s
Val loss: 0.0339 score: 1.0000 time: 0.07s
Test loss: 0.2317 score: 0.9184 time: 0.06s
Epoch 756/1000, LR 0.000039
Train loss: 0.5039;  Loss pred: 0.5039; Loss self: 0.0000; time: 0.21s
Val loss: 0.0339 score: 1.0000 time: 0.07s
Test loss: 0.2317 score: 0.9184 time: 0.06s
Epoch 757/1000, LR 0.000038
Train loss: 0.5058;  Loss pred: 0.5058; Loss self: 0.0000; time: 0.21s
Val loss: 0.0339 score: 1.0000 time: 0.07s
Test loss: 0.2317 score: 0.9184 time: 0.07s
Epoch 758/1000, LR 0.000038
Train loss: 0.5049;  Loss pred: 0.5049; Loss self: 0.0000; time: 0.21s
Val loss: 0.0339 score: 1.0000 time: 0.07s
Test loss: 0.2318 score: 0.9184 time: 0.06s
Epoch 759/1000, LR 0.000038
Train loss: 0.5066;  Loss pred: 0.5066; Loss self: 0.0000; time: 0.23s
Val loss: 0.0338 score: 1.0000 time: 0.10s
Test loss: 0.2318 score: 0.9184 time: 0.06s
Epoch 760/1000, LR 0.000038
Train loss: 0.5052;  Loss pred: 0.5052; Loss self: 0.0000; time: 0.21s
Val loss: 0.0338 score: 1.0000 time: 0.15s
Test loss: 0.2318 score: 0.9184 time: 3.10s
Epoch 761/1000, LR 0.000037
Train loss: 0.5064;  Loss pred: 0.5064; Loss self: 0.0000; time: 5.25s
Val loss: 0.0338 score: 1.0000 time: 0.07s
Test loss: 0.2318 score: 0.9184 time: 0.07s
Epoch 762/1000, LR 0.000037
Train loss: 0.5087;  Loss pred: 0.5087; Loss self: 0.0000; time: 0.21s
Val loss: 0.0338 score: 1.0000 time: 0.07s
Test loss: 0.2318 score: 0.9184 time: 0.06s
Epoch 763/1000, LR 0.000037
Train loss: 0.5074;  Loss pred: 0.5074; Loss self: 0.0000; time: 0.20s
Val loss: 0.0338 score: 1.0000 time: 0.07s
Test loss: 0.2318 score: 0.9184 time: 0.06s
Epoch 764/1000, LR 0.000036
Train loss: 0.5065;  Loss pred: 0.5065; Loss self: 0.0000; time: 0.20s
Val loss: 0.0338 score: 1.0000 time: 0.07s
Test loss: 0.2318 score: 0.9184 time: 0.06s
Epoch 765/1000, LR 0.000036
Train loss: 0.5042;  Loss pred: 0.5042; Loss self: 0.0000; time: 0.20s
Val loss: 0.0338 score: 1.0000 time: 0.07s
Test loss: 0.2318 score: 0.9184 time: 0.06s
Epoch 766/1000, LR 0.000036
Train loss: 0.5068;  Loss pred: 0.5068; Loss self: 0.0000; time: 0.20s
Val loss: 0.0338 score: 1.0000 time: 0.07s
Test loss: 0.2318 score: 0.9184 time: 0.06s
Epoch 767/1000, LR 0.000036
Train loss: 0.5045;  Loss pred: 0.5045; Loss self: 0.0000; time: 0.20s
Val loss: 0.0337 score: 1.0000 time: 0.07s
Test loss: 0.2318 score: 0.9184 time: 0.06s
Epoch 768/1000, LR 0.000035
Train loss: 0.5075;  Loss pred: 0.5075; Loss self: 0.0000; time: 0.20s
Val loss: 0.0337 score: 1.0000 time: 0.07s
Test loss: 0.2318 score: 0.9184 time: 0.06s
Epoch 769/1000, LR 0.000035
Train loss: 0.5041;  Loss pred: 0.5041; Loss self: 0.0000; time: 0.67s
Val loss: 0.0337 score: 1.0000 time: 1.93s
Test loss: 0.2318 score: 0.9184 time: 1.75s
Epoch 770/1000, LR 0.000035
Train loss: 0.5050;  Loss pred: 0.5050; Loss self: 0.0000; time: 0.80s
Val loss: 0.0337 score: 1.0000 time: 0.82s
Test loss: 0.2318 score: 0.9184 time: 0.07s
Epoch 771/1000, LR 0.000034
Train loss: 0.5046;  Loss pred: 0.5046; Loss self: 0.0000; time: 0.23s
Val loss: 0.0337 score: 1.0000 time: 0.07s
Test loss: 0.2318 score: 0.9184 time: 0.06s
Epoch 772/1000, LR 0.000034
Train loss: 0.5054;  Loss pred: 0.5054; Loss self: 0.0000; time: 0.22s
Val loss: 0.0337 score: 1.0000 time: 0.07s
Test loss: 0.2318 score: 0.9184 time: 0.06s
Epoch 773/1000, LR 0.000034
Train loss: 0.5054;  Loss pred: 0.5054; Loss self: 0.0000; time: 0.22s
Val loss: 0.0337 score: 1.0000 time: 0.08s
Test loss: 0.2318 score: 0.9184 time: 0.06s
Epoch 774/1000, LR 0.000034
Train loss: 0.5065;  Loss pred: 0.5065; Loss self: 0.0000; time: 0.22s
Val loss: 0.0337 score: 1.0000 time: 0.07s
Test loss: 0.2318 score: 0.9184 time: 0.06s
Epoch 775/1000, LR 0.000033
Train loss: 0.5075;  Loss pred: 0.5075; Loss self: 0.0000; time: 0.23s
Val loss: 0.0336 score: 1.0000 time: 0.07s
Test loss: 0.2318 score: 0.9184 time: 0.07s
Epoch 776/1000, LR 0.000033
Train loss: 0.5032;  Loss pred: 0.5032; Loss self: 0.0000; time: 0.22s
Val loss: 0.0336 score: 1.0000 time: 0.07s
Test loss: 0.2318 score: 0.9184 time: 0.06s
Epoch 777/1000, LR 0.000033
Train loss: 0.5042;  Loss pred: 0.5042; Loss self: 0.0000; time: 0.22s
Val loss: 0.0336 score: 1.0000 time: 0.07s
Test loss: 0.2318 score: 0.9184 time: 0.07s
Epoch 778/1000, LR 0.000032
Train loss: 0.5062;  Loss pred: 0.5062; Loss self: 0.0000; time: 0.23s
Val loss: 0.0336 score: 1.0000 time: 0.09s
Test loss: 0.2319 score: 0.9184 time: 0.07s
Epoch 779/1000, LR 0.000032
Train loss: 0.5065;  Loss pred: 0.5065; Loss self: 0.0000; time: 0.22s
Val loss: 0.0336 score: 1.0000 time: 0.23s
Test loss: 0.2319 score: 0.9184 time: 2.25s
Epoch 780/1000, LR 0.000032
Train loss: 0.5063;  Loss pred: 0.5063; Loss self: 0.0000; time: 3.17s
Val loss: 0.0336 score: 1.0000 time: 0.08s
Test loss: 0.2319 score: 0.9184 time: 0.07s
Epoch 781/1000, LR 0.000032
Train loss: 0.5047;  Loss pred: 0.5047; Loss self: 0.0000; time: 0.23s
Val loss: 0.0336 score: 1.0000 time: 0.07s
Test loss: 0.2319 score: 0.9184 time: 0.06s
Epoch 782/1000, LR 0.000031
Train loss: 0.5052;  Loss pred: 0.5052; Loss self: 0.0000; time: 0.22s
Val loss: 0.0336 score: 1.0000 time: 0.07s
Test loss: 0.2319 score: 0.9184 time: 0.06s
Epoch 783/1000, LR 0.000031
Train loss: 0.5061;  Loss pred: 0.5061; Loss self: 0.0000; time: 0.22s
Val loss: 0.0336 score: 1.0000 time: 0.07s
Test loss: 0.2320 score: 0.9184 time: 0.06s
Epoch 784/1000, LR 0.000031
Train loss: 0.5074;  Loss pred: 0.5074; Loss self: 0.0000; time: 0.22s
Val loss: 0.0336 score: 1.0000 time: 0.07s
Test loss: 0.2320 score: 0.9184 time: 0.06s
Epoch 785/1000, LR 0.000030
Train loss: 0.5052;  Loss pred: 0.5052; Loss self: 0.0000; time: 0.22s
Val loss: 0.0336 score: 1.0000 time: 0.07s
Test loss: 0.2320 score: 0.9184 time: 0.07s
Epoch 786/1000, LR 0.000030
Train loss: 0.5064;  Loss pred: 0.5064; Loss self: 0.0000; time: 0.22s
Val loss: 0.0335 score: 1.0000 time: 0.07s
Test loss: 0.2319 score: 0.9184 time: 0.06s
Epoch 787/1000, LR 0.000030
Train loss: 0.5028;  Loss pred: 0.5028; Loss self: 0.0000; time: 0.22s
Val loss: 0.0335 score: 1.0000 time: 0.07s
Test loss: 0.2319 score: 0.9184 time: 0.06s
Epoch 788/1000, LR 0.000030
Train loss: 0.5073;  Loss pred: 0.5073; Loss self: 0.0000; time: 0.22s
Val loss: 0.0335 score: 1.0000 time: 0.07s
Test loss: 0.2319 score: 0.9184 time: 0.06s
Epoch 789/1000, LR 0.000029
Train loss: 0.5054;  Loss pred: 0.5054; Loss self: 0.0000; time: 0.22s
Val loss: 0.0335 score: 1.0000 time: 0.36s
Test loss: 0.2319 score: 0.9184 time: 2.34s
Epoch 790/1000, LR 0.000029
Train loss: 0.5035;  Loss pred: 0.5035; Loss self: 0.0000; time: 5.58s
Val loss: 0.0335 score: 1.0000 time: 0.08s
Test loss: 0.2320 score: 0.9184 time: 0.07s
Epoch 791/1000, LR 0.000029
Train loss: 0.5058;  Loss pred: 0.5058; Loss self: 0.0000; time: 0.24s
Val loss: 0.0335 score: 1.0000 time: 0.08s
Test loss: 0.2320 score: 0.9184 time: 0.07s
Epoch 792/1000, LR 0.000029
Train loss: 0.5059;  Loss pred: 0.5059; Loss self: 0.0000; time: 0.23s
Val loss: 0.0335 score: 1.0000 time: 0.08s
Test loss: 0.2320 score: 0.9184 time: 0.07s
Epoch 793/1000, LR 0.000028
Train loss: 0.5059;  Loss pred: 0.5059; Loss self: 0.0000; time: 0.23s
Val loss: 0.0335 score: 1.0000 time: 0.08s
Test loss: 0.2320 score: 0.9184 time: 0.07s
Epoch 794/1000, LR 0.000028
Train loss: 0.5031;  Loss pred: 0.5031; Loss self: 0.0000; time: 0.23s
Val loss: 0.0335 score: 1.0000 time: 0.08s
Test loss: 0.2320 score: 0.9184 time: 0.07s
Epoch 795/1000, LR 0.000028
Train loss: 0.5071;  Loss pred: 0.5071; Loss self: 0.0000; time: 0.22s
Val loss: 0.0335 score: 1.0000 time: 0.07s
Test loss: 0.2320 score: 0.9184 time: 0.06s
Epoch 796/1000, LR 0.000028
Train loss: 0.5052;  Loss pred: 0.5052; Loss self: 0.0000; time: 0.22s
Val loss: 0.0335 score: 1.0000 time: 0.07s
Test loss: 0.2320 score: 0.9184 time: 0.06s
Epoch 797/1000, LR 0.000027
Train loss: 0.5044;  Loss pred: 0.5044; Loss self: 0.0000; time: 0.22s
Val loss: 0.0334 score: 1.0000 time: 0.07s
Test loss: 0.2320 score: 0.9184 time: 0.06s
Epoch 798/1000, LR 0.000027
Train loss: 0.5047;  Loss pred: 0.5047; Loss self: 0.0000; time: 0.21s
Val loss: 0.0334 score: 1.0000 time: 0.07s
Test loss: 0.2320 score: 0.9184 time: 0.06s
Epoch 799/1000, LR 0.000027
Train loss: 0.5073;  Loss pred: 0.5073; Loss self: 0.0000; time: 0.21s
Val loss: 0.0334 score: 1.0000 time: 0.07s
Test loss: 0.2320 score: 0.9184 time: 0.06s
Epoch 800/1000, LR 0.000027
Train loss: 0.5037;  Loss pred: 0.5037; Loss self: 0.0000; time: 0.22s
Val loss: 0.0334 score: 1.0000 time: 0.07s
Test loss: 0.2320 score: 0.9184 time: 0.06s
Epoch 801/1000, LR 0.000026
Train loss: 0.5056;  Loss pred: 0.5056; Loss self: 0.0000; time: 0.22s
Val loss: 0.0334 score: 1.0000 time: 0.97s
Test loss: 0.2321 score: 0.9184 time: 0.89s
Epoch 802/1000, LR 0.000026
Train loss: 0.5040;  Loss pred: 0.5040; Loss self: 0.0000; time: 7.09s
Val loss: 0.0334 score: 1.0000 time: 0.87s
Test loss: 0.2321 score: 0.9184 time: 1.00s
Epoch 803/1000, LR 0.000026
Train loss: 0.5047;  Loss pred: 0.5047; Loss self: 0.0000; time: 1.76s
Val loss: 0.0334 score: 1.0000 time: 0.07s
Test loss: 0.2321 score: 0.9184 time: 0.06s
Epoch 804/1000, LR 0.000026
Train loss: 0.5053;  Loss pred: 0.5053; Loss self: 0.0000; time: 0.23s
Val loss: 0.0334 score: 1.0000 time: 0.09s
Test loss: 0.2321 score: 0.9184 time: 0.06s
Epoch 805/1000, LR 0.000025
Train loss: 0.5053;  Loss pred: 0.5053; Loss self: 0.0000; time: 0.23s
Val loss: 0.0334 score: 1.0000 time: 0.08s
Test loss: 0.2321 score: 0.9184 time: 0.07s
Epoch 806/1000, LR 0.000025
Train loss: 0.5052;  Loss pred: 0.5052; Loss self: 0.0000; time: 0.21s
Val loss: 0.0334 score: 1.0000 time: 0.07s
Test loss: 0.2321 score: 0.9184 time: 0.06s
Epoch 807/1000, LR 0.000025
Train loss: 0.5031;  Loss pred: 0.5031; Loss self: 0.0000; time: 0.21s
Val loss: 0.0334 score: 1.0000 time: 0.07s
Test loss: 0.2321 score: 0.9184 time: 0.06s
Epoch 808/1000, LR 0.000025
Train loss: 0.5045;  Loss pred: 0.5045; Loss self: 0.0000; time: 0.21s
Val loss: 0.0334 score: 1.0000 time: 0.07s
Test loss: 0.2321 score: 0.9184 time: 0.06s
Epoch 809/1000, LR 0.000024
Train loss: 0.5046;  Loss pred: 0.5046; Loss self: 0.0000; time: 6.38s
Val loss: 0.0333 score: 1.0000 time: 0.87s
Test loss: 0.2321 score: 0.9184 time: 0.07s
Epoch 810/1000, LR 0.000024
Train loss: 0.5043;  Loss pred: 0.5043; Loss self: 0.0000; time: 0.21s
Val loss: 0.0333 score: 1.0000 time: 0.08s
Test loss: 0.2321 score: 0.9184 time: 0.15s
Epoch 811/1000, LR 0.000024
Train loss: 0.5065;  Loss pred: 0.5065; Loss self: 0.0000; time: 0.22s
Val loss: 0.0333 score: 1.0000 time: 0.07s
Test loss: 0.2321 score: 0.9184 time: 0.06s
Epoch 812/1000, LR 0.000024
Train loss: 0.5022;  Loss pred: 0.5022; Loss self: 0.0000; time: 0.22s
Val loss: 0.0333 score: 1.0000 time: 0.09s
Test loss: 0.2321 score: 0.9184 time: 0.08s
Epoch 813/1000, LR 0.000023
Train loss: 0.5044;  Loss pred: 0.5044; Loss self: 0.0000; time: 0.28s
Val loss: 0.0333 score: 1.0000 time: 0.09s
Test loss: 0.2321 score: 0.9184 time: 0.07s
Epoch 814/1000, LR 0.000023
Train loss: 0.5054;  Loss pred: 0.5054; Loss self: 0.0000; time: 0.21s
Val loss: 0.0333 score: 1.0000 time: 0.07s
Test loss: 0.2321 score: 0.9184 time: 0.07s
Epoch 815/1000, LR 0.000023
Train loss: 0.5056;  Loss pred: 0.5056; Loss self: 0.0000; time: 4.85s
Val loss: 0.0333 score: 1.0000 time: 0.08s
Test loss: 0.2321 score: 0.9184 time: 0.07s
Epoch 816/1000, LR 0.000023
Train loss: 0.5037;  Loss pred: 0.5037; Loss self: 0.0000; time: 0.23s
Val loss: 0.0333 score: 1.0000 time: 0.08s
Test loss: 0.2321 score: 0.9184 time: 0.07s
Epoch 817/1000, LR 0.000022
Train loss: 0.5037;  Loss pred: 0.5037; Loss self: 0.0000; time: 0.22s
Val loss: 0.0333 score: 1.0000 time: 0.07s
Test loss: 0.2321 score: 0.9184 time: 0.06s
Epoch 818/1000, LR 0.000022
Train loss: 0.5035;  Loss pred: 0.5035; Loss self: 0.0000; time: 0.22s
Val loss: 0.0333 score: 1.0000 time: 0.07s
Test loss: 0.2321 score: 0.9184 time: 0.06s
Epoch 819/1000, LR 0.000022
Train loss: 0.5038;  Loss pred: 0.5038; Loss self: 0.0000; time: 0.22s
Val loss: 0.0333 score: 1.0000 time: 0.07s
Test loss: 0.2321 score: 0.9184 time: 0.06s
Epoch 820/1000, LR 0.000022
Train loss: 0.5053;  Loss pred: 0.5053; Loss self: 0.0000; time: 0.21s
Val loss: 0.0333 score: 1.0000 time: 0.07s
Test loss: 0.2321 score: 0.9184 time: 0.06s
Epoch 821/1000, LR 0.000021
Train loss: 0.5066;  Loss pred: 0.5066; Loss self: 0.0000; time: 0.21s
Val loss: 0.0333 score: 1.0000 time: 0.07s
Test loss: 0.2321 score: 0.9184 time: 0.06s
Epoch 822/1000, LR 0.000021
Train loss: 0.5044;  Loss pred: 0.5044; Loss self: 0.0000; time: 0.82s
Val loss: 0.0333 score: 1.0000 time: 2.16s
Test loss: 0.2321 score: 0.9184 time: 0.98s
Epoch 823/1000, LR 0.000021
Train loss: 0.5060;  Loss pred: 0.5060; Loss self: 0.0000; time: 0.31s
Val loss: 0.0332 score: 1.0000 time: 0.07s
Test loss: 0.2321 score: 0.9184 time: 0.06s
Epoch 824/1000, LR 0.000021
Train loss: 0.5026;  Loss pred: 0.5026; Loss self: 0.0000; time: 0.21s
Val loss: 0.0332 score: 1.0000 time: 0.07s
Test loss: 0.2321 score: 0.9184 time: 0.06s
Epoch 825/1000, LR 0.000021
Train loss: 0.5052;  Loss pred: 0.5052; Loss self: 0.0000; time: 0.21s
Val loss: 0.0332 score: 1.0000 time: 0.07s
Test loss: 0.2321 score: 0.9184 time: 0.06s
Epoch 826/1000, LR 0.000020
Train loss: 0.5048;  Loss pred: 0.5048; Loss self: 0.0000; time: 0.21s
Val loss: 0.0332 score: 1.0000 time: 0.07s
Test loss: 0.2321 score: 0.9184 time: 0.06s
Epoch 827/1000, LR 0.000020
Train loss: 0.5046;  Loss pred: 0.5046; Loss self: 0.0000; time: 0.21s
Val loss: 0.0332 score: 1.0000 time: 0.07s
Test loss: 0.2321 score: 0.9184 time: 0.06s
Epoch 828/1000, LR 0.000020
Train loss: 0.5062;  Loss pred: 0.5062; Loss self: 0.0000; time: 0.21s
Val loss: 0.0332 score: 1.0000 time: 0.07s
Test loss: 0.2321 score: 0.9184 time: 0.06s
Epoch 829/1000, LR 0.000020
Train loss: 0.5031;  Loss pred: 0.5031; Loss self: 0.0000; time: 0.21s
Val loss: 0.0332 score: 1.0000 time: 0.07s
Test loss: 0.2321 score: 0.9184 time: 0.06s
Epoch 830/1000, LR 0.000019
Train loss: 0.5049;  Loss pred: 0.5049; Loss self: 0.0000; time: 0.21s
Val loss: 0.0332 score: 1.0000 time: 0.07s
Test loss: 0.2321 score: 0.9184 time: 0.06s
Epoch 831/1000, LR 0.000019
Train loss: 0.5031;  Loss pred: 0.5031; Loss self: 0.0000; time: 0.44s
Val loss: 0.0332 score: 1.0000 time: 2.69s
Test loss: 0.2321 score: 0.9184 time: 2.09s
Epoch 832/1000, LR 0.000019
Train loss: 0.5042;  Loss pred: 0.5042; Loss self: 0.0000; time: 5.16s
Val loss: 0.0332 score: 1.0000 time: 0.08s
Test loss: 0.2321 score: 0.9184 time: 0.07s
Epoch 833/1000, LR 0.000019
Train loss: 0.5041;  Loss pred: 0.5041; Loss self: 0.0000; time: 0.22s
Val loss: 0.0332 score: 1.0000 time: 0.07s
Test loss: 0.2321 score: 0.9184 time: 0.06s
Epoch 834/1000, LR 0.000019
Train loss: 0.5041;  Loss pred: 0.5041; Loss self: 0.0000; time: 0.21s
Val loss: 0.0332 score: 1.0000 time: 0.07s
Test loss: 0.2322 score: 0.9184 time: 0.06s
Epoch 835/1000, LR 0.000018
Train loss: 0.5042;  Loss pred: 0.5042; Loss self: 0.0000; time: 0.21s
Val loss: 0.0332 score: 1.0000 time: 0.07s
Test loss: 0.2322 score: 0.9184 time: 0.06s
Epoch 836/1000, LR 0.000018
Train loss: 0.5045;  Loss pred: 0.5045; Loss self: 0.0000; time: 0.21s
Val loss: 0.0332 score: 1.0000 time: 0.07s
Test loss: 0.2322 score: 0.9184 time: 0.06s
Epoch 837/1000, LR 0.000018
Train loss: 0.5036;  Loss pred: 0.5036; Loss self: 0.0000; time: 0.21s
Val loss: 0.0332 score: 1.0000 time: 0.07s
Test loss: 0.2323 score: 0.9184 time: 0.06s
Epoch 838/1000, LR 0.000018
Train loss: 0.5033;  Loss pred: 0.5033; Loss self: 0.0000; time: 0.21s
Val loss: 0.0332 score: 1.0000 time: 0.07s
Test loss: 0.2323 score: 0.9184 time: 0.06s
Epoch 839/1000, LR 0.000017
Train loss: 0.5049;  Loss pred: 0.5049; Loss self: 0.0000; time: 0.21s
Val loss: 0.0332 score: 1.0000 time: 0.07s
Test loss: 0.2323 score: 0.9184 time: 0.06s
Epoch 840/1000, LR 0.000017
Train loss: 0.5056;  Loss pred: 0.5056; Loss self: 0.0000; time: 0.21s
Val loss: 0.0332 score: 1.0000 time: 0.07s
Test loss: 0.2324 score: 0.9184 time: 0.06s
Epoch 841/1000, LR 0.000017
Train loss: 0.5033;  Loss pred: 0.5033; Loss self: 0.0000; time: 0.21s
Val loss: 0.0332 score: 1.0000 time: 0.07s
Test loss: 0.2324 score: 0.9184 time: 0.06s
Epoch 842/1000, LR 0.000017
Train loss: 0.5043;  Loss pred: 0.5043; Loss self: 0.0000; time: 0.21s
Val loss: 0.0332 score: 1.0000 time: 0.07s
Test loss: 0.2324 score: 0.9184 time: 0.06s
Epoch 843/1000, LR 0.000017
Train loss: 0.5027;  Loss pred: 0.5027; Loss self: 0.0000; time: 0.21s
Val loss: 0.0332 score: 1.0000 time: 0.07s
Test loss: 0.2325 score: 0.9184 time: 2.63s
Epoch 844/1000, LR 0.000016
Train loss: 0.5046;  Loss pred: 0.5046; Loss self: 0.0000; time: 4.51s
Val loss: 0.0332 score: 1.0000 time: 0.08s
Test loss: 0.2325 score: 0.9184 time: 0.06s
Epoch 845/1000, LR 0.000016
Train loss: 0.5050;  Loss pred: 0.5050; Loss self: 0.0000; time: 0.22s
Val loss: 0.0332 score: 1.0000 time: 0.07s
Test loss: 0.2325 score: 0.9184 time: 0.07s
     INFO: Early stopping counter 1 of 2
Epoch 846/1000, LR 0.000016
Train loss: 0.5046;  Loss pred: 0.5046; Loss self: 0.0000; time: 0.22s
Val loss: 0.0332 score: 1.0000 time: 0.07s
Test loss: 0.2326 score: 0.9184 time: 0.06s
     INFO: Early stopping counter 2 of 2
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 843,   Train_Loss: 0.5046,   Val_Loss: 0.0332,   Val_Precision: 1.0000,   Val_Recall: 1.0000,   Val_accuracy: 1.0000,   Val_Score: 1.0000,   Val_Loss: 0.0332,   Test_Precision: 0.9565,   Test_Recall: 0.8800,   Test_accuracy: 0.9167,   Test_Score: 0.9184,   Test_loss: 0.2325


[0.06833410204853863, 0.0679529559565708, 0.06700492603704333, 0.06759673892520368, 0.06693420500960201, 0.06688495411071926, 0.06407436996232718, 0.06819558294955641, 0.06436006794683635, 0.0635923370718956, 0.06472922000102699, 0.06660582998301834, 0.06671573990024626, 0.0672321260208264, 0.06456550001166761, 0.06372885906603187, 0.06462040601763874, 0.06521135393995792, 0.06485772202722728, 0.06542657501995564, 0.0653282740386203, 0.06485900201369077, 0.06518519599922001, 0.06499172688927501, 0.06779441307298839, 0.07171953504439443, 0.06770034204237163, 0.06795574899297208, 0.06900283496361226, 0.06733783206436783, 0.06744095589965582, 0.0675262650474906, 0.06708796403836459, 0.06968736799899489, 0.06876102194655687, 0.07160585897509009, 0.08581699000205845, 0.06630075396969914, 1.5063912719488144, 0.06452985003124923, 0.06432656198740005, 0.06390337203629315, 0.06379966402892023, 0.06354285508859903, 0.06323131208773702, 0.06361076305620372, 0.07259423902723938, 0.06666238897014409, 0.0668840779690072, 0.06706631905399263, 0.06660780101083219, 0.0665643879910931, 0.06631771894171834, 0.06647277902811766, 0.0665992849972099, 0.06377321295440197, 1.6493149939924479, 0.06546144897583872, 0.06604680197779089, 0.06500831095036119, 0.06556718703359365, 0.065528545062989, 0.0653838359285146, 0.06549742701463401, 0.06476026494055986, 0.06490289408247918, 0.06503039796371013, 1.0126681280089542, 0.06985096004791558, 0.06452557700686157, 0.06373154500033706, 0.06308141292538494, 0.06338339007925242, 0.06385672697797418, 0.06331021699588746, 0.06280869897454977, 0.06342368293553591, 0.07260816299822181, 0.06360519398003817, 0.07251774694304913, 0.06800691201351583, 0.06818427098914981, 0.06826049101073295, 0.06778019201010466, 0.06869762891437858, 0.06810694606974721, 2.3145632080268115, 2.3269002290908247, 0.06928620510734618, 0.06763326097279787, 0.0679139920976013, 0.06733823393005878, 0.06739226100035012, 0.06748012395109981, 0.06688652408774942, 0.06777872098609805, 0.06764911999925971, 0.06769327609799802, 2.340762081905268, 0.6299631940200925, 0.06874702800996602, 0.06714840105269104, 0.06626055401284248, 0.07058287796098739, 0.06962641701102257, 0.06981276790611446, 0.06932449294254184, 0.06921096902806312, 0.0661068509798497, 0.06564042903482914, 0.06535239203367382, 0.06607088400050998, 0.06513929495122284, 0.06550110504031181, 0.06556485104374588, 0.06548797292634845, 0.06551938899792731, 0.06560793099924922, 0.16484043491072953, 0.0662523980718106, 0.06535482499748468, 0.0660698750289157, 0.06517630896996707, 0.1523956989403814, 0.06925167900044471, 0.06931907392572612, 0.06995846098288894, 0.06953941204119474, 0.06994888500776142, 0.06974644993897527, 0.07015235908329487, 0.0694645990151912, 0.06548912299331278, 0.06475556991063058, 0.06462515296880156, 0.06616493093315512, 0.06549291696865112, 0.06514728302136064, 0.06534865009598434, 0.06467923091258854, 0.06593856704421341, 0.06637915293686092, 0.06614321900997311, 0.06629478395916522, 0.06608096603304148, 0.06625307898502797, 0.065597637090832, 0.06561642000451684, 0.06555224198382348, 0.07105369807686657, 0.0721471490105614, 0.07141565799247473, 0.07103964302223176, 0.07080650003626943, 0.06982119695749134, 0.07055863796267658, 0.0711759909754619, 0.0702351339859888, 0.0703665919136256, 0.07002281001769006, 0.07067594002000988, 0.07012129202485085, 0.07027707097586244, 0.09085514000616968, 0.0662083049537614, 0.0672642879653722, 0.07163944293279201, 0.07125823595561087, 0.06571922299917787, 0.06715138698928058, 0.06657942302990705, 0.06814709503669292, 0.08836912701372057, 0.07095531595405191, 0.06184779293835163, 0.06154624302871525, 0.06172040500678122, 0.06151789997238666, 0.07091481902170926, 0.06680259690620005, 0.06049473595339805, 0.06504169199615717, 0.06011264200787991, 0.060236489051021636, 0.06490972707979381, 0.06565647490788251, 0.06556651403661817, 0.06524249899666756, 0.06564106501173228, 0.06525126099586487, 0.06583019602112472, 0.06570325000211596, 0.062045417027547956, 0.06070071004796773, 0.3128449459327385, 2.015300514991395, 0.08074439002666622, 0.06174625095445663, 0.0615318400086835, 0.06143924698699266, 0.061508151004090905, 0.05942303000483662, 0.05972827400546521, 0.05999056599102914, 0.059661387000232935, 0.05985585192684084, 0.06016465602442622, 0.06051682902034372, 0.06061661895364523, 0.06149158091284335, 1.4328284329967573, 0.06771844800096005, 0.06721360702067614, 0.06570260203443468, 0.06553275091573596, 0.06666369608137757, 0.06529315700754523, 0.06490483798552305, 0.06532537692692131, 0.06567327596712857, 0.06547572906129062, 0.06494116398971528, 0.06512447993736714, 0.06536812102422118, 0.061968120047822595, 1.402061619097367, 0.32461915793828666, 0.0641754069365561, 0.06370740802958608, 0.062294562929309905, 0.06238514801952988, 0.062286115949973464, 0.06443443894386292, 0.06227063899859786, 0.06324823899194598, 0.0624067970784381, 0.2563899619271979, 0.0745847950456664, 0.06112186796963215, 0.06046964193228632, 0.06063993205316365, 0.06037247506901622, 0.06987287499941885, 0.06119576597120613, 0.06349778105504811, 0.07167099602520466, 0.06848965701647103, 0.23333637102041394, 0.10375957598444074, 0.06251100800000131, 0.06258093705400825, 0.06758909206837416, 0.06465884600766003, 0.06445940304547548, 0.06579946400597692, 1.2442387669580057, 0.07085756293963641, 0.06728277006186545, 0.06618339999113232, 0.06635018100496382, 0.06625051004812121, 0.06832368206232786, 0.06599224708043039, 0.07110289693810046, 0.07139032997656614, 0.07155806303489953, 0.07256866502575576, 0.07184460398275405, 2.265620109043084, 0.0678205710137263, 0.07242972194217145, 0.0653404260519892, 0.0653053930727765, 0.06609777896665037, 0.06644903100095689, 0.06986327306367457, 0.0698251329595223, 0.07148983504157513, 0.07059536699671298, 0.0703550970647484, 1.4021176469977945, 0.07274275191593915, 0.06749198900070041, 0.06680228898767382, 0.06707307102624327, 0.06741432601120323, 0.06712762801907957, 0.0669220780255273, 0.0668640190269798, 0.0664813769981265, 0.06849959003739059, 0.06705894903279841, 0.0650606449926272, 0.06466939393430948, 0.0645461370004341, 0.07686687994282693, 0.07175156904850155, 0.06629737198818475, 0.06635211198590696, 0.06650314503349364, 0.06686897296458483, 0.06924016692209989, 0.0673291030107066, 0.06701235403306782, 0.06623583904001862, 1.2732593569671735, 0.8736019259085879, 0.07585632603149861, 0.06997894390951842, 0.0700540830148384, 0.07072442304342985, 0.07023579906672239, 0.07038604095578194, 0.07093961897771806, 0.07033697003498673, 0.070343590923585, 0.07045630004722625, 2.1002839050488546, 0.0830539830494672, 0.0673690470866859, 0.08197509695310146, 0.0655857619130984, 0.06469692999962717, 0.06500291696283966, 0.06500466202851385, 0.06589812401216477, 0.06497354805469513, 0.06499310804065317, 0.06563610897865146, 0.06509727600496262, 0.06529394199606031, 0.06565617804881185, 0.06547182297799736, 0.06528270698618144, 0.06510074995458126, 0.07037426601164043, 0.07180111994966865, 0.5760427849600092, 0.11270860594231635, 0.06893811793997884, 0.06926876294892281, 0.06818613596260548, 0.06848851696122438, 0.06885175406932831, 0.06875480699818581, 0.07773124298546463, 0.06444465997628868, 1.2882507800823078, 0.45440423500258476, 0.07219154201447964, 0.0727267850888893, 0.07235967507585883, 0.07245374796912074, 0.07200081297196448, 1.92329349310603, 0.06519285100512207, 0.0661633750423789, 0.06586349499411881, 0.06484770402312279, 0.06537698395550251, 0.06488197494763881, 0.066857949947007, 0.0731791720027104, 0.06572514493018389, 0.06376156001351774, 1.8158081059809774, 1.0302194351097569, 0.468877510051243, 0.06878308509476483, 0.07203998800832778, 0.06923187500797212, 0.06946918601170182, 0.07010159501805902, 0.06970352702774107, 0.07017065701074898, 0.07037222699727863, 1.693216944928281, 0.24316679302137345, 0.6655788409989327, 0.06838043592870235, 0.06912983593065292, 0.06880844198167324, 0.06953353097196668, 0.06816839589737356, 0.06971694203093648, 0.06966838601510972, 0.06967123004142195, 0.06974727695342153, 0.2403919470962137, 0.5589719930430874, 0.5014324019430205, 0.06596852699294686, 0.0662359029520303, 0.06565063400194049, 0.06573414790909737, 0.0653356050606817, 0.06567288597580045, 0.06622929300647229, 0.06534743995871395, 0.06533283297903836, 0.06561904097907245, 0.06572871294338256, 0.0657412389991805, 0.06731882505118847, 0.06782381201628596, 0.06251050694845617, 0.06326822703704238, 0.06350664596538991, 0.06345620006322861, 0.06347168399952352, 0.06744990090373904, 0.15337021194864064, 0.0727692210348323, 0.07163084601052105, 0.0684711099602282, 0.06885250401683152, 0.07047579099889845, 0.06914142810273916, 0.0686131149996072, 0.06826126296073198, 0.0681215530494228, 0.06889093795325607, 0.07076907309237868, 0.07189328991807997, 0.07463656307663769, 0.07306559500284493, 0.07282038894481957, 0.07219581701792777, 0.07189016893971711, 0.06766075093764812, 0.0672525119734928, 0.0668437429703772, 0.07279205194208771, 0.0725372910965234, 0.07185946602839977, 0.07333700195886195, 0.07295700896065682, 0.07281744305510074, 0.06642825505696237, 2.4225288700545207, 0.13961805088911206, 0.06979086494538933, 0.06902847799938172, 0.07304321101401001, 0.07386704604141414, 0.07584767905063927, 0.0745291979983449, 0.07025225600227714, 0.07109058101195842, 0.07212393707595766, 1.2217237900476903, 1.2488049590028822, 0.06526651093736291, 0.0644889889517799, 0.06496750400401652, 0.06540324701927602, 0.06527512497268617, 0.06576831999700516, 0.0660531249595806, 0.06636769406031817, 0.07429223402868956, 0.07363686198368669, 0.06689672800712287, 0.07645931001752615, 0.06942467694170773, 0.06549953203648329, 0.729588724905625, 0.41384778800420463, 0.06498586595989764, 0.06527975900098681, 0.06356052006594837, 0.06295447598677129, 0.06315730896312743, 0.06336738204117864, 0.07392470689956099, 0.07444600795861334, 0.06971489905845374, 0.7970731960376725, 1.3182900779647753, 0.06679402699228376, 0.06629594997502863, 0.06210175296291709, 0.06383077404461801, 0.06307642301544547, 0.07330795598682016, 0.06649823696352541, 0.06315581500530243, 0.06270428199786693, 0.06249454303178936, 0.06844849395565689, 0.06302750995382667, 0.06292722898069769, 0.06276307802181691, 2.4983681560261175, 0.0710204589413479, 0.06753073295112699, 0.06941689306404442, 0.06538577598985285, 0.06964696198701859, 0.06678891705814749, 0.0663457919145003, 0.06647827406413853, 0.0659459320595488, 0.06645173893775791, 0.06703659100458026, 0.06662942096590996, 0.06731070799287409, 1.2949593610828742, 0.5036123500904068, 0.14941497100517154, 0.07236599700991064, 0.07064762397203594, 0.07102145301178098, 0.07070406596176326, 0.06945660803467035, 0.07045480096712708, 0.06950244109611958, 0.06959108798764646, 0.06931652803905308, 0.06608578702434897, 0.06581244908738881, 0.06796893000137061, 0.06665494397748262, 0.06668262800667435, 0.06655587092973292, 0.06779859506059438, 0.06699139205738902, 0.06612062104977667, 1.531634487095289, 0.06533439201302826, 0.06385084800422192, 0.06318357202690095, 0.0632892600260675, 0.06269951199647039, 0.0630759340710938, 0.06260741397272795, 0.0628781239502132, 0.0664830730529502, 0.06410802400205284, 0.06270285707432777, 0.6920245820656419, 0.06917736504692584, 0.06516069895587862, 0.06615241500549018, 0.06510327209252864, 0.0646005499875173, 0.0642999280244112, 2.8721112209605053, 0.247337396023795, 0.06643839599564672, 0.06593338795937598, 0.06578172906301916, 0.06743584503419697, 0.06724082108121365, 0.06670819502323866, 0.07898264401592314, 2.5611360020702705, 0.07544202695135027, 0.07226991897914559, 0.0737697280710563, 0.06557862705085427, 0.06583746091928333, 0.06656396901234984, 0.06603222398553044, 0.06593453197274357, 0.06788228696677834, 0.06620909401681274, 0.06570077396463603, 0.0667634840356186, 0.06999096798244864, 0.06586119090206921, 0.06636128691025078, 0.066522695007734, 0.06612040696199983, 1.708958485047333, 0.06650950200855732, 0.06264269701205194, 0.06209814001340419, 0.062402363982982934, 0.0662438569124788, 0.06518777494784445, 0.06556053599342704, 0.06607605097815394, 0.06584477506112307, 0.06527977797668427, 0.06310638890136033, 0.0671999859623611, 0.06187313294503838, 0.06188829196617007, 0.06632111093495041, 0.3897425369359553, 0.7472167069790885, 0.07326272095087916, 0.06883379200007766, 0.06847804202698171, 0.06976828596089035, 0.06898722797632217, 0.06849935895297676, 0.0685441920068115, 0.06899655505549163, 2.1129244449548423, 0.199518078006804, 0.07371175102889538, 0.07388344698119909, 0.07182340393774211, 0.07238225196488202, 0.06725078902672976, 0.0692501679295674, 0.07001635408960283, 0.06860856804996729, 0.06938363297376782, 0.06917601905297488, 0.06848281202837825, 0.0689194219885394, 2.234944660915062, 0.11834099108818918, 0.07193912100046873, 0.07166711997706443, 0.07129964290652424, 0.07074364298023283, 0.07082774897571653, 0.07072763796895742, 0.07241883291862905, 0.07448148704133928, 0.07080242095980793, 2.300391045981087, 0.07106660399585962, 0.07000330800656229, 0.06961656897328794, 0.06966210796963423, 0.06401895103044808, 0.06524920091032982, 0.06420207896735519, 0.06450140406377614, 0.06379041692707688, 0.06410493899602443, 0.06980041495990008, 0.06639075302518904, 0.06698641297407448, 0.0665336330421269, 2.522448188974522, 0.1357430520001799, 0.07205045490991324, 0.07219371607061476, 0.07027437200304121, 0.07084097992628813, 0.06985153397545218, 0.0706522969994694, 0.06531525496393442, 0.06611769401933998, 0.0654041770612821, 1.5397327980026603, 0.0652296410407871, 0.0658202659105882, 0.0658100500004366, 0.06263148109428585, 0.06290411204099655, 0.0629685310414061, 0.06389153306372464, 0.06270741403568536, 0.06418588105589151, 0.06402752199210227, 1.249610314029269, 0.06990733800921589, 0.06817667803261429, 0.06720805598888546, 0.06625157606322318, 0.06521115591749549, 0.06376190902665257, 0.06697794597130269, 0.06474320497363806, 0.07003175595309585, 0.07061265490483493, 0.07176483399234712, 0.07151708798483014, 0.0723292879993096, 2.7606780850328505, 1.0483905160799623, 0.0664652599953115, 0.06778712500818074, 0.0667248439276591, 0.06600387301295996, 0.06658391002565622, 0.06692401797045022, 0.06619850895367563, 0.06594663602299988, 0.06600934197194874, 2.568068869994022, 0.06750849902164191, 0.06852564797736704, 0.06694990501273423, 0.06760197004768997, 0.0669242909643799, 0.06736958201508969, 0.0667896979721263, 0.3402234490495175, 0.0636064960854128, 0.06307879299856722, 0.06532700196839869, 0.06399208097718656, 0.063645651913248, 0.06474513793364167, 0.06480207201093435, 0.12935510103125125, 0.06351081700995564, 0.667491719010286, 0.07009400299284607, 0.0683326970320195, 0.06707954104058444, 0.06709584698546678, 0.06412996305152774, 0.06311180104967207, 0.06418108206707984, 0.06400849809870124, 0.06526242906693369, 0.0702777560800314, 1.1465609719743952, 0.06305826699826866, 0.062881043064408, 0.06295838998630643, 0.0653729330515489, 0.06523699907120317, 0.06503566401079297, 0.06274685100652277, 2.722664223983884, 0.06745130300987512, 0.06692251702770591, 0.06749324803240597, 0.06721879495307803, 0.0671981800114736, 0.06820860202424228, 0.06757341499906033, 0.8377736669499427, 0.7041068279650062, 0.06551835406571627, 0.06648875505197793, 0.06621273199561983, 0.06538430706132203, 0.0658200269099325, 0.06587668298743665, 0.06527598097454756, 0.06611480994615704, 0.06550094997510314, 0.06603329803328961, 0.06596081703901291, 0.06547801499255002, 0.06702075805515051, 0.06570249097421765, 0.06586165504995733, 0.06561115093063563, 0.06592854298651218, 0.06609763798769563, 0.0656171990558505, 0.06545319000724703, 0.06629886594600976, 0.06744604895357043, 0.069379627937451, 0.06626592599786818, 0.06651346490252763, 3.1093335640616715, 0.0718208720209077, 0.06583731202408671, 0.06386744300834835, 0.063692950992845, 0.06387679697945714, 0.0639349949778989, 0.06397205998655409, 0.06374301901087165, 1.7588728979462758, 0.07489047292619944, 0.06830032705329359, 0.06815781304612756, 0.06904220709111542, 0.06871085800230503, 0.06953292503021657, 0.06897666293662041, 0.06944878899957985, 0.07040882401634008, 2.2517256480641663, 0.0711284950375557, 0.06859283801168203, 0.06872334494255483, 0.06780333293136209, 0.06790623802226037, 0.06985305994749069, 0.0679105039453134, 0.0686023929156363, 0.06866192992310971, 2.345023786998354, 0.07346284599043429, 0.07358296099118888, 0.07198657002300024, 0.07126102200709283, 0.07139869499951601, 0.06716126995161176, 0.06643893907312304, 0.0663716959534213, 0.06600652798078954, 0.06707210803870112, 0.06732858705800027, 0.8957044970011339, 1.0051321990322322, 0.06955601205118, 0.06669463193975389, 0.0750485040480271, 0.0682867580326274, 0.06363444600719959, 0.06444533204194158, 0.07300386403221637, 0.1496193630155176, 0.06883140502031893, 0.08850706298835576, 0.07841148192528635, 0.0793342050164938, 0.07188963901717216, 0.07164364599157125, 0.06907351000700146, 0.06741318397689611, 0.06646751496009529, 0.06592960492707789, 0.06587137700989842, 0.9884094059234485, 0.06736113096121699, 0.06512280006427318, 0.06538029492367059, 0.06691529892850667, 0.0648028280120343, 0.06672544602770358, 0.06518740998581052, 0.0649827818851918, 2.100203071953729, 0.07103174598887563, 0.0662989099510014, 0.06649934395682067, 0.06727804301772267, 0.06505839701276273, 0.06559474300593138, 0.06482297403272241, 0.06439234700519592, 0.06592091510538012, 0.06543139705900103, 0.06522042700089514, 2.6324793100357056, 0.06905429100152105, 0.06946117605548352, 0.06865761196240783]
[0.0013945735111946659, 0.0013867950195218532, 0.0013674474701437416, 0.0013795252841878303, 0.0013660041838694289, 0.0013649990634840665, 0.0013076402033127996, 0.0013917465908072737, 0.0013134707744252316, 0.0012978027973856246, 0.0013210044898168774, 0.00135930265271466, 0.0013615457122499238, 0.0013720842045066611, 0.001317663265544237, 0.0013005889605312627, 0.0013187837962783417, 0.0013308439579583248, 0.0013236269801474956, 0.0013352362248970537, 0.0013332300824208223, 0.0013236531023202197, 0.0013303101224330614, 0.0013263617732505106, 0.0013835594504691508, 0.0014636639804978455, 0.0013816396335177884, 0.0013868520202647363, 0.0014082211217063727, 0.0013742414707013843, 0.0013763460387684861, 0.0013780870417855224, 0.0013691421232319304, 0.001422191183652957, 0.00140328616217463, 0.0014613440607161242, 0.001751367142899152, 0.0013530766116265133, 0.030742679019363558, 0.0013169357149234535, 0.001312786979334695, 0.0013041504497202684, 0.0013020339597738823, 0.001296792960991817, 0.0012904349405660617, 0.0012981788378817086, 0.0014815150821885588, 0.0013604569177580426, 0.0013649811830409632, 0.0013687003888569924, 0.0013593428777720854, 0.0013584568977774102, 0.0013534228355452723, 0.0013565873271044419, 0.0013591690815757125, 0.0013014941419265708, 0.033659489673315265, 0.0013359479382824228, 0.0013478939179141, 0.0013267002234767591, 0.001338105857828442, 0.0013373172461834488, 0.0013343639985411142, 0.0013366821839721228, 0.0013216380600114257, 0.0013245488588261058, 0.001327150978851227, 0.020666696489978657, 0.0014255297968962363, 0.0013168485103441136, 0.001300643775517083, 0.0012873757739874478, 0.0012935385730459678, 0.0013031985097545751, 0.0012920452448140299, 0.001281810183154077, 0.0012943608762354267, 0.0014817992448616698, 0.001298065183266085, 0.0014799540192459006, 0.0013878961635411394, 0.0013915157344724452, 0.001393071245116999, 0.0013832692246960134, 0.0014019924268240528, 0.0013899376748928002, 0.047235983837281864, 0.04748775977736377, 0.0014140041858642077, 0.0013802706320979157, 0.0013859998387265571, 0.001374249672042016, 0.0013753522653132677, 0.001377145386757139, 0.0013650311038316209, 0.0013832392037979194, 0.0013805942856991778, 0.0013814954305713882, 0.04777065473276058, 0.012856391714695764, 0.0014030005716319596, 0.0013703755316875723, 0.001352256204343724, 0.001440466897163008, 0.0014209472859392361, 0.0014247503654309073, 0.001414785570255956, 0.0014124687556747574, 0.0013491194077520346, 0.0013396005925475334, 0.0013337222864015065, 0.0013483853877655097, 0.0013293733663514865, 0.0013367572457206492, 0.0013380581845662423, 0.0013364892433948663, 0.0013371303877128021, 0.0013389373673316166, 0.0033640905083822353, 0.0013520897565675633, 0.0013337719387241773, 0.0013483647965084836, 0.0013301287544891238, 0.003110116304905743, 0.0014132995714376472, 0.0014146749780760432, 0.0014277236935283457, 0.0014191716743100966, 0.0014275282654645188, 0.0014233969375301078, 0.0014316807976182625, 0.001417644877861045, 0.0013365127141492404, 0.0013215422430740936, 0.0013188806728326849, 0.001350304712921533, 0.0013365901422173697, 0.0013295363881910334, 0.001333645920326211, 0.0013199843043385415, 0.001345685041718641, 0.001354676590548182, 0.0013498616124484309, 0.001352954774676841, 0.0013485911435314587, 0.0013521036527556728, 0.0013387272875679999, 0.0013391106123370783, 0.001337800856812724, 0.0014500754709564606, 0.0014723907961339063, 0.0014574624080096884, 0.0014497886331067706, 0.0014450306129850903, 0.0014249223868875783, 0.0014399722033199302, 0.0014525712443971817, 0.0014333700813467102, 0.0014360528961964408, 0.0014290369391365318, 0.0014423661228573444, 0.0014310467760173641, 0.001434225938282907, 0.0018541865307381566, 0.0013511898970155387, 0.0013727405707218818, 0.0014620294476080003, 0.0014542497133798137, 0.0013412086326362832, 0.0013704364691689915, 0.0013587637353042255, 0.0013907570415651615, 0.001803451571708583, 0.0014480676725316718, 0.0012621998558847271, 0.0012560457760962297, 0.0012596001021792085, 0.0012554673463752379, 0.0014472412045246788, 0.0013633183042081644, 0.0012345864480285316, 0.00132738146930933, 0.0012267886124057124, 0.0012293161030820742, 0.001324688307750894, 0.001339928059344541, 0.0013380921231962893, 0.0013314795713605626, 0.0013396135716680058, 0.0013316583876707116, 0.0013434733881862188, 0.0013408826531044074, 0.0012662330005622031, 0.0012387900009789333, 0.006384590733321194, 0.041128581938599904, 0.0016478446944217598, 0.0012601275704991147, 0.0012557518369119083, 0.0012538621834080135, 0.00125526838783859, 0.0012127148980578901, 0.0012189443674584736, 0.0012242972651230438, 0.0012175793265353661, 0.0012215479985069558, 0.001227850122947474, 0.0012350373269457901, 0.0012370738561968415, 0.0012549302227110887, 0.02924139659177056, 0.0013820091428767358, 0.0013717062657280844, 0.0013408694292741771, 0.0013374030799129788, 0.0013604835934975014, 0.0013325134083172496, 0.0013245885303167968, 0.0013331709576922717, 0.0013402709381046649, 0.001336239368597768, 0.0013253298773411282, 0.0013290710191299416, 0.0013340432862085955, 0.0012646555111800529, 0.028613502430558508, 0.006624880774250748, 0.001309702182378696, 0.0013001511842772669, 0.0012713176108022429, 0.0012731662861128546, 0.0012711452234688463, 0.0013149885498747534, 0.0012708293673183238, 0.0012907803875907343, 0.0012736081036415939, 0.005232448202595875, 0.001522138674401355, 0.0012473850606047378, 0.0012340743251487004, 0.0012375496337380335, 0.0012320913279391065, 0.0014259770408044665, 0.0012488931830858393, 0.0012958730827560838, 0.0014626733882694828, 0.0013977481023769599, 0.004761966755518651, 0.002117542367029403, 0.001275734857142884, 0.001277161980694046, 0.001379369225885187, 0.001319568285870613, 0.0013154980213362344, 0.0013428462042036106, 0.025392627897102157, 0.0014460727130538042, 0.001373117756364601, 0.0013506816324720882, 0.0013540853266319145, 0.0013520512254718616, 0.0013943608584148543, 0.0013467805526618446, 0.001451079529348989, 0.0014569455097258395, 0.0014603686333652965, 0.0014809931637909338, 0.001466216407811307, 0.04623714508251192, 0.0013840932859944142, 0.0014781575906565602, 0.0013334780826936572, 0.0013327631239342143, 0.0013489342646255177, 0.0013561026734889162, 0.0014257810829321341, 0.0014250027134596389, 0.001458976225338268, 0.001440721775443122, 0.001435818307443845, 0.02861464585709785, 0.0014845459574681459, 0.0013773875306265391, 0.0013633120201566086, 0.0013688381842090463, 0.0013758025716572087, 0.0013699515922261135, 0.0013657566943985162, 0.0013645718168771388, 0.0013567627958801327, 0.001397950817089604, 0.001368549980261192, 0.0013277682651556572, 0.0013197835496797853, 0.0013172681020496755, 0.0015687118355678965, 0.001464317735683705, 0.0013530075915956072, 0.0013541247344062645, 0.0013572070414998702, 0.0013646729176445883, 0.001413064631063263, 0.0013740633267491143, 0.0013675990618993432, 0.0013517518171432372, 0.025984884836064766, 0.017828610732828324, 0.0015480882863571144, 0.0014281417124391515, 0.0014296751635681307, 0.001443355572314895, 0.001433383654422906, 0.0014364498154241213, 0.0014477473260758786, 0.0014354483680609539, 0.0014355834882364285, 0.0014378836744331888, 0.04286293683773173, 0.001694979245907494, 0.0013748785119731815, 0.001672961162308193, 0.0013384849370020081, 0.0013203455101964729, 0.0013265901420987686, 0.001326625755683956, 0.0013448596737176484, 0.0013259907766264311, 0.00132638996001333, 0.0013395124281357443, 0.001328515836835972, 0.0013325294284910268, 0.0013399220009961603, 0.001336159652612191, 0.0013323001425751314, 0.0013285867337669646, 0.0014362095104416414, 0.0014653289785646663, 0.011755975203265493, 0.002300175631475844, 0.001406900366122017, 0.0014136482234474044, 0.0013915537951552138, 0.0013977248359433546, 0.0014051378381495572, 0.001403159326493588, 0.0015863518976625434, 0.0013151971423732384, 0.02629083224657771, 0.009273555816379281, 0.0014732967758057068, 0.0014842201038548837, 0.0014767280627726292, 0.001478647917737158, 0.001469404346366622, 0.03925088761440877, 0.0013304663470433075, 0.001350272960048549, 0.0013441529590636492, 0.0013234225310841386, 0.001334224162357194, 0.0013241219377069144, 0.0013644479581021837, 0.0014934524898512326, 0.0013413294883710997, 0.0013012563268064844, 0.03705730828532607, 0.021024886430811365, 0.00956892877655598, 0.0014037364305054046, 0.0014702038369046487, 0.0014128954083259618, 0.001417738490034731, 0.0014306447962869186, 0.0014225209597498178, 0.0014320542247091628, 0.0014361678979036454, 0.034555447855679204, 0.004962587612681091, 0.013583241653039443, 0.0013955191005857624, 0.00141081297817659, 0.0014042539179933314, 0.001419051652489116, 0.0013911917530076237, 0.0014227947353252343, 0.001421803796226729, 0.0014218618375800398, 0.0014234138153759496, 0.004905958104004361, 0.011407591694756886, 0.010233314325367766, 0.0013462964692438136, 0.0013517531214700062, 0.001339808857182459, 0.0013415132226346402, 0.001333379695115953, 0.0013402629790979686, 0.0013516182246218836, 0.0013336212236472235, 0.0013333231220211909, 0.0013391641016137234, 0.0013414023049669911, 0.0013416579387587856, 0.0013738535724732341, 0.0013841594289037951, 0.0012757246316011464, 0.0012911883068784159, 0.0012960539992936716, 0.001295024491086298, 0.0012953404897861943, 0.0013765285898722252, 0.003130004325482462, 0.001485086143568006, 0.0014618540002147155, 0.0013973695910250653, 0.0014051531432006433, 0.001438281448957111, 0.0014110495531171256, 0.0014002676530532083, 0.0013930869991986118, 0.0013902357765188326, 0.001405937509250124, 0.0014442667978036465, 0.0014672099983281627, 0.0015231951648293405, 0.0014911345918947945, 0.0014861303866289708, 0.001473384020774036, 0.001467146304892186, 0.001380831651788737, 0.001372500244356996, 0.0013641580198036165, 0.0014855520804507695, 0.0014803528795208857, 0.0014665197148653014, 0.0014966735093645295, 0.0014889185502174863, 0.0014860702664306273, 0.0013556786746318852, 0.04943936469499022, 0.0028493479773288176, 0.0014243033662324352, 0.0014087444489669738, 0.0014906777757961228, 0.0015074907355390641, 0.0015479118173599852, 0.0015210040407825488, 0.0014337195102505538, 0.0014508281839175187, 0.0014719170831828093, 0.02493313857240184, 0.02548581548985474, 0.00133196961096659, 0.001316101815342447, 0.0013258674286533983, 0.0013347601432505312, 0.0013321454076058402, 0.0013422106121837789, 0.0013480229583587877, 0.0013544427359248607, 0.0015161680414018277, 0.0015027931017078916, 0.0013652393470841404, 0.0015603940819903296, 0.0014168301416675048, 0.0013367251436016997, 0.01488956581440051, 0.008445873224575604, 0.0013262421624468906, 0.0013322399796119758, 0.0012971534707336401, 0.001284785224219822, 0.0012889246727168864, 0.001293211878391401, 0.0015086674877461425, 0.00151930628486966, 0.00142275304200926, 0.016266799919136172, 0.026903879142138272, 0.001363143408005791, 0.0013529785709189518, 0.0012673827135289203, 0.0013026688580534288, 0.0012872739390907238, 0.0014960807344249012, 0.001357106876806641, 0.0012888941837816822, 0.0012796792244462638, 0.0012753988373834565, 0.001396908039911365, 0.0012862757133434014, 0.0012842291628713815, 0.001280879143302386, 0.050987105225022804, 0.0014493971212519979, 0.0013781782234923877, 0.0014166712870213147, 0.0013344035916296498, 0.0014213665711636447, 0.001363039123635663, 0.001353995753357149, 0.0013566994706967048, 0.001345835348154057, 0.0013561579375052635, 0.0013680936939710257, 0.001359784101345101, 0.0013736879182219201, 0.0264277420629158, 0.010277803063069527, 0.0030492851225545213, 0.001476857081834911, 0.0014417882443272642, 0.0014494174084036934, 0.001442940121668638, 0.0014174817966259255, 0.0014378530809617772, 0.0014184171652269302, 0.0014202262854621727, 0.0014146230212051648, 0.0013486895311091626, 0.0013431112058650777, 0.0013871210204361348, 0.0013603049791322984, 0.0013608699593198846, 0.001358283080198631, 0.0013836447971549873, 0.0013671712664773269, 0.0013494004295872791, 0.03125784667541406, 0.0013333549390413932, 0.0013030785306984065, 0.0012894606536102233, 0.0012916175515523978, 0.0012795818774789876, 0.0012872639606345672, 0.0012777023259740398, 0.001283227019392106, 0.0013567974092438817, 0.001308327020450058, 0.001279650144374036, 0.014122950654400855, 0.0014117829601413437, 0.0013298101827730329, 0.0013500492858263304, 0.0013286382059699722, 0.0013183785711738225, 0.0013122434290696163, 0.0586145147134797, 0.005047701959669286, 0.0013558856325642187, 0.0013455793461097138, 0.0013424842665922276, 0.0013762417353917749, 0.0013722616547186459, 0.001361391735168136, 0.0016118906942025131, 0.05226808167490348, 0.0015396332030887811, 0.0014748963056968488, 0.0015055046545113533, 0.0013383393275684544, 0.0013436216514139455, 0.0013584483471908132, 0.001347596407867968, 0.0013456026933212973, 0.0013853527952403743, 0.0013512060003431172, 0.0013408321217272658, 0.0013625200823595633, 0.0014283871016826254, 0.0013441059367769227, 0.00135431197776022, 0.001357606020566, 0.0013493960604489762, 0.034876703776476184, 0.0013573367756848432, 0.00127842238800106, 0.0012673089798653917, 0.0012735176323057742, 0.001351915447193445, 0.0013303627540376417, 0.0013379701223148375, 0.001348490836288856, 0.0013437709196147565, 0.0013322403668711077, 0.001287885487782864, 0.0013714282849461449, 0.0012627169988783343, 0.0012630263666565322, 0.0013534920598969472, 0.007953929325223577, 0.015249320550593642, 0.0014951575704261052, 0.0014047712653077074, 0.001397511061775137, 0.0014238425706304153, 0.001407902611761677, 0.0013979461010811584, 0.0013988610613635, 0.0014080929603161557, 0.04312090703989474, 0.004071797510342938, 0.0015043214495692933, 0.0015078254485958998, 0.0014657837538314717, 0.001477188815609837, 0.0013724650821781584, 0.0014132687332564775, 0.0014289051855020986, 0.0014001748581625977, 0.0014159925096687309, 0.0014117554908770385, 0.0013976084087424132, 0.0014065188160926408, 0.04561111552887881, 0.0024151222671059016, 0.0014681453265401783, 0.001462594285246213, 0.0014550947531943722, 0.001443747815923119, 0.0014454642648105414, 0.0014434211830399474, 0.001477935365686307, 0.0015200303477824343, 0.0014449473665266925, 0.046946756040430346, 0.0014503388570583596, 0.0014286389389094344, 0.0014207463055773049, 0.0014216756728496782, 0.001306509204703022, 0.0013316163451087717, 0.001310246509537861, 0.001316355184975023, 0.001301845243409732, 0.0013082640611433557, 0.0014244982644877567, 0.0013549133270446742, 0.0013670696525321323, 0.0013578292457576916, 0.0514785344688678, 0.00277026636735061, 0.0014704174471410866, 0.0014733411442982604, 0.0014341708572049225, 0.0014457342842099617, 0.001425541509703106, 0.0014418836122340694, 0.0013329643870190699, 0.0013493406942722444, 0.0013347791236996347, 0.031423118326584905, 0.001331217164097696, 0.001343270732869147, 0.0013430622449068694, 0.0012781934917201192, 0.0012837573885917664, 0.0012850720620695122, 0.0013039088380351967, 0.0012797431435854155, 0.0013099159399161534, 0.0013066841222878014, 0.025502251306719775, 0.001426680367535018, 0.001391360776175802, 0.0013715929793650095, 0.0013520729808821057, 0.0013308399166835814, 0.001301263449523522, 0.0013668968565571978, 0.001321289897421185, 0.0014292195092468541, 0.0014410745898945903, 0.0014645884488234107, 0.0014595324078536763, 0.001476107918353257, 0.05634036908230307, 0.021395724817958414, 0.0013564338774553367, 0.001383410714452668, 0.0013617315087277367, 0.0013470178165910195, 0.0013588553066460453, 0.001365796285111229, 0.0013509899786464414, 0.0013458497147550996, 0.0013471294279989538, 0.0524095687753882, 0.0013777244698294268, 0.001398482611783001, 0.001366324592096617, 0.0013796320417895913, 0.0013658018564159165, 0.0013748894288793815, 0.0013630550606556389, 0.0069433356948881125, 0.0012980917568451592, 0.0012873223060932085, 0.0013332041218040548, 0.0013059608362691135, 0.0012988908553724081, 0.0013213293455845239, 0.0013224912655292725, 0.0026399000210459438, 0.001296139122652156, 0.013622279979801756, 0.0014304898569968585, 0.001394544837388153, 0.00136897022531805, 0.0013693029997034036, 0.0013087747561536273, 0.001287995939789226, 0.0013098180013689765, 0.0013062958795653314, 0.0013318863074884427, 0.0014342399200006407, 0.023399203509681533, 0.0012869034081279319, 0.0012832865931511838, 0.0012848651017613557, 0.0013341414908479368, 0.0013313673279837382, 0.0013272584491998566, 0.0012805479797249545, 0.0555645759996711, 0.0013765572042831657, 0.0013657656536266512, 0.0013774132251511424, 0.0013718121418995516, 0.0013713914288055835, 0.001392012286209026, 0.0013790492856951089, 0.017097421774488628, 0.014369527101326657, 0.0013371092666472708, 0.001356913368407713, 0.001351280244808568, 0.001334373613496368, 0.0013432658553047447, 0.0013444221017844215, 0.001332162877031583, 0.001349281835635858, 0.0013367540811245538, 0.001347618327209992, 0.0013461391232451614, 0.0013362860202561228, 0.001367770572554092, 0.0013408671627391357, 0.0013441154091828028, 0.0013390030802170538, 0.0013454804691124935, 0.0013489313875039925, 0.001339126511343888, 0.0013357793879030006, 0.0013530380805308114, 0.0013764499786442944, 0.001415910774233694, 0.0013523658366911874, 0.0013574176510719924, 0.06345578702166677, 0.0014657320820593408, 0.0013436186127364635, 0.001303417204252007, 0.0012998561427111225, 0.0013036081016215744, 0.0013047958158754877, 0.0013055522446235527, 0.0013008779389973805, 0.03589536526420971, 0.001528376998493866, 0.0013938842255774202, 0.0013909757764515827, 0.0014090246345125595, 0.0014022624082103067, 0.0014190392863309505, 0.0014076869987065391, 0.0014173222244812213, 0.001436914775843675, 0.045953584654370744, 0.001451601939541953, 0.0013998538369731028, 0.0014025172437256087, 0.0013837414883951448, 0.001385841592291028, 0.001425572651989606, 0.0013859286519451713, 0.0014000488350129857, 0.0014012638759818307, 0.047857628306088855, 0.0014992417549068223, 0.0015016930814528344, 0.0014691136739387804, 0.0014543065715733232, 0.0014571162244799186, 0.001370638162277791, 0.0013558967157780212, 0.0013545244072126796, 0.0013470719996079498, 0.0013688185314020636, 0.0013740527971020462, 0.018279683612268036, 0.02051290202106596, 0.0014195104500240817, 0.0013611149375459977, 0.0015316021234291245, 0.0013936073067883144, 0.0012986621634122363, 0.0013152108579988079, 0.001489874776167681, 0.0030534563880717878, 0.00140472255143508, 0.0018062665915990971, 0.001600234325005844, 0.0016190654084998735, 0.0014671354901463706, 0.0014621152243177807, 0.001409663469530642, 0.0013757792648346145, 0.0013564798971448019, 0.0013455021413689365, 0.0013443138165285392, 0.020171620529049968, 0.0013747169583921833, 0.001329036736005575, 0.0013342917331361345, 0.001365618345479728, 0.001322506694123149, 0.0013617437964837467, 0.0013303553058328678, 0.0013261792221467715, 0.042861287182729156, 0.0014496274691607272, 0.0013530389785918655, 0.0013571294685065442, 0.0013730212860759727, 0.0013277223880155658, 0.0013386682246108444, 0.0013229178374024983, 0.001314129530718284, 0.001345324798068982, 0.001335334633857164, 0.0013310291224672478, 0.05372406755174909, 0.001409271244929001, 0.00141757502154048, 0.0014011757543348536]
[717.065104114409, 721.0871007777238, 731.2895170261154, 724.8870401014298, 732.0621794637096, 732.601235232769, 764.7363529100602, 718.5216091817092, 761.3416449540684, 770.5330902464249, 756.9996981150487, 735.6713370660334, 734.4593655599873, 728.8182436001108, 758.9192369167005, 768.8824296890244, 758.2744061779028, 751.4028928937098, 755.4998613646928, 748.9311489261756, 750.0580831361401, 755.4849516441346, 751.704420748943, 753.9421145629848, 722.7734230436648, 683.216922274649, 723.7777317185836, 721.0574635130214, 710.1157514157137, 727.6741543024609, 726.561469159871, 725.6435694398136, 730.3843648016968, 703.140345330688, 712.6130271607113, 684.3015460096065, 570.9825058980123, 739.0564520939542, 32.52806950787018, 759.3385073151612, 761.7382071436969, 766.7826976668937, 768.0291228146345, 771.133118454913, 774.9325196986222, 770.3098916877591, 674.9846910250527, 735.047164630501, 732.6108318740024, 730.6200890576968, 735.6495674137524, 736.1293550322529, 738.8673914291657, 737.1438461941427, 735.7436345157879, 768.3476765555962, 29.7093036675712, 748.5321630764008, 741.8981469606491, 753.7497788154385, 747.3250297423103, 747.7657248898016, 749.4206986199562, 748.1209909062839, 756.6368056859341, 754.9740376404535, 753.4937742091658, 48.38702694864189, 701.4935795640823, 759.3887923666208, 768.85002552097, 776.7739771136553, 773.0731969169221, 767.3428050407493, 773.9667043501521, 780.1467121593287, 772.5820660683455, 674.8552501073469, 770.3773376648792, 675.6966682718578, 720.5149969206312, 718.6408139172946, 717.8383758226333, 722.9250692104131, 713.2706146389891, 719.4567195807003, 21.170301087509724, 21.058058006700815, 707.2114849425445, 724.4956001708659, 721.5008054537656, 727.6698116391665, 727.0864528457568, 726.1397450234141, 732.58403943545, 722.9407590923748, 724.3257562040159, 723.853280923556, 20.933353448769278, 77.78232199139742, 712.7580845079825, 729.7269813103952, 739.5048340601397, 694.2193548282816, 703.7558746164233, 701.8773423494129, 706.8208928785477, 707.9802622057186, 741.2242343072111, 746.4911598003168, 749.7812777036837, 741.6277342319468, 752.2341166985588, 748.078982329284, 747.3516559552075, 748.2289924458072, 747.870222073501, 746.8609244903749, 297.257162822558, 739.5958701281903, 749.7533655990337, 741.639059837104, 751.8069184092485, 321.5313840265876, 707.5640721965071, 706.8761485836126, 700.4156368160366, 704.6363862117886, 700.5115234440555, 702.5447179443914, 698.4797181491818, 705.3952760784555, 748.2158526539359, 756.6916647884512, 758.2187081809345, 740.5735834516861, 748.1725088448019, 752.1418810963117, 749.8242110285158, 757.5847657530373, 743.1159364919821, 738.1835686666299, 740.8166813382904, 739.1230059695486, 741.5145834202735, 739.5882689629133, 746.9781256320329, 746.7643007135559, 747.4954100287199, 689.6192784644556, 679.1675162774212, 686.1240430657834, 689.7557182919053, 692.026861586162, 701.7926093394283, 694.4578497379661, 688.4343909857591, 697.6565319826258, 696.3531793631144, 699.7719741270165, 693.3052462567466, 698.7891777954476, 697.2402139074588, 539.3200648490836, 740.0884229587308, 728.469764300862, 683.9807513016115, 687.6398123372536, 745.5961553381874, 729.6945334550112, 735.9631214885944, 719.0328505362787, 554.4922944909488, 690.5754606424503, 792.2675599571031, 796.1493275412176, 793.9027618923818, 796.5161363114553, 690.9698237402193, 733.5044185303556, 809.9878316312846, 753.3629353137837, 815.1363567346915, 813.4604252664182, 754.894562101056, 746.308723834901, 747.3327005403099, 751.0441928734643, 746.4839272678169, 750.9433419701306, 744.3392692355958, 745.7774158572363, 789.7440672893562, 807.2393216039575, 156.62711076858815, 24.313991702725893, 606.8533056453522, 793.5704474777242, 796.3356856074029, 797.5358163223228, 796.6423831654606, 824.5961203259367, 820.3819851803593, 816.7950942041012, 821.3017240080036, 818.6334071377103, 814.4316487092773, 809.6921268549588, 808.3591735373975, 796.8570538046728, 34.198092996742545, 723.5842144419098, 729.0190509330455, 745.7847708119557, 747.71773373295, 735.0327521622087, 750.4614916129357, 754.9514261314295, 750.0913474225445, 746.117797207588, 748.3689101671858, 754.5291305182045, 752.4052406579721, 749.6008640334603, 790.7291678718876, 34.94853530870184, 150.94611270390698, 763.5323613676727, 769.1413214809198, 786.5855011392215, 785.4433555990023, 786.6921745346184, 760.4628953577165, 786.8877016197525, 774.7251272282798, 785.1708835243169, 191.11512647251607, 656.970364670152, 801.6770695611791, 810.3239647899687, 808.0483988181451, 811.6281458393828, 701.2735628869935, 800.7089906032962, 771.6805089223582, 683.6796293826876, 715.4364926694848, 209.99726611721894, 472.24556900027994, 783.8619399642215, 782.986038667211, 724.9690519652321, 757.8236084540551, 760.1683801730366, 744.6869171388549, 39.381508839977975, 691.528158282033, 728.2696588583566, 740.3669199008413, 738.5058979166039, 739.6169473172166, 717.1744630990474, 742.5114641160728, 689.1421040503818, 686.3674676400045, 684.758613101395, 675.2225631077702, 682.0275606468958, 21.627632895920854, 722.4946541674328, 676.5178532525921, 749.9185873231426, 750.3208800136043, 741.3259683766825, 737.4072919030885, 701.3699451976802, 701.7530496992443, 685.412128472585, 694.096540390271, 696.4669518528966, 34.947138783195896, 673.606630343377, 726.0120901088201, 733.5077995462303, 730.5465405159109, 726.8484741931106, 729.9527995547946, 732.1948368266305, 732.830612234487, 737.0485121176243, 715.3327483164977, 730.7003868496984, 753.1434710730699, 757.7000033396587, 759.1469029303869, 637.4657074210098, 682.9118951653547, 739.0941530643565, 738.4844058981497, 736.8072588946229, 732.776321029357, 707.6817139266647, 727.7684954782194, 731.2084571125577, 739.7807699000385, 38.48391117793552, 56.08961993649184, 645.9579914225377, 700.2106242608658, 699.4595873822392, 692.829971478318, 697.6499256946038, 696.1607633363394, 690.7282658987888, 696.6464431951877, 696.5808733482093, 695.4665511410012, 23.33017925919887, 589.977725340583, 727.3369910806389, 597.742507435316, 747.113375993487, 757.3775138987642, 753.8123254993606, 753.7920892274848, 743.5720020034959, 754.153058699388, 753.9260927381795, 746.5402925687985, 752.719668274054, 750.4524692804816, 746.3120982091148, 748.4135582488221, 750.5816204951789, 752.6795011453133, 696.2772441831938, 682.4406086471654, 85.0631260027009, 434.7494105736516, 710.7823866422054, 707.3895636931104, 718.6211582200889, 715.4484017772199, 711.6739531524621, 712.6774423393106, 630.3771574727393, 760.3422846521142, 38.03607244613455, 107.83350203529965, 678.7498733601223, 673.7545175427517, 677.1727477857033, 676.2935165325532, 680.5478713008354, 25.477130856854963, 751.6161549086137, 740.5909987000296, 743.9629494969162, 755.616574837068, 749.4992432405697, 755.217455071984, 732.89713547661, 669.5894290548292, 745.5289760418168, 768.4880983089463, 26.98523034378024, 47.56268259953732, 104.50490575810483, 712.3844464448056, 680.1777922885776, 707.7664730928867, 705.3486993750889, 698.9855221892884, 702.9773397334493, 698.297580318993, 696.2974186094023, 28.938996947066034, 201.50777740319617, 73.62012879865358, 716.5792281741288, 708.811171621382, 712.1219226712166, 704.695983578843, 718.8081713668123, 702.842072135874, 703.3319243160428, 703.3032138354355, 702.5363876603107, 203.83378308587183, 87.6609214949038, 97.7200512175279, 742.7784465346471, 739.7800560745285, 746.3751225700527, 745.426868052831, 749.9739224040293, 746.1222279474031, 739.8538890519554, 749.8380966562402, 750.0057439070697, 746.7344732396703, 745.4885057951409, 745.3464635890239, 727.8796081592476, 722.460129316147, 783.8682229917536, 774.4803718193558, 771.5727898258743, 772.1861685883451, 771.9977935415711, 726.4651147513209, 319.4883763765595, 673.3616122748555, 684.0628406483282, 715.6302859477801, 711.6662015374427, 695.2742112644877, 708.6923331579086, 714.1491827077158, 717.8302579632576, 719.3024499081819, 711.2691662471996, 692.3928470284989, 681.5656934859134, 656.5146890497388, 670.6302740447417, 672.8884686008787, 678.7096818619318, 681.5952823965198, 724.2012440144998, 728.5973201910001, 733.0529055160043, 673.1504153638049, 675.5146113024407, 681.8865030340552, 668.1483929147571, 671.6284110060487, 672.9156908588764, 737.6379216642435, 20.22679713158474, 350.95748499538183, 702.0976174796232, 709.8519541520079, 670.835787744895, 663.3539937758952, 646.0316335755697, 657.460449273695, 697.4864977775479, 689.261492908006, 679.3860954705707, 40.107265160226824, 39.237512348705295, 750.7678792118352, 759.8196342733576, 754.2232189953096, 749.1982773509461, 750.6688040889028, 745.0395570729384, 741.8271282393408, 738.3110215561569, 659.557498043168, 665.427595364606, 732.4722966238751, 640.8637481657646, 705.8009076677841, 748.096947817993, 67.1611256140757, 118.40101945767115, 754.010110909926, 750.6155162009603, 770.9188022558526, 778.3402090471922, 775.8405290606544, 773.2684927421785, 662.8365813688603, 658.1951315272742, 702.8626686946079, 61.47490624899158, 37.16936114367788, 733.5985297856142, 739.1100062440705, 789.0276467599786, 767.654798698646, 776.8354268916202, 668.4131257023396, 736.8616408112703, 775.8588816546198, 781.4458349378258, 784.0684581864206, 715.8667366990392, 777.4382969578985, 778.6772243702368, 780.7137818028498, 19.612802013110418, 689.9420354417388, 725.5955601053824, 705.8800507650541, 749.3984625586498, 703.5482755031447, 733.6546564655304, 738.5547536028541, 737.0829145282049, 743.0329433474878, 737.3772422403558, 730.9440898725308, 735.4108633942683, 727.9673838104248, 37.8390252795463, 97.29705792799497, 327.94571835980224, 677.1135895949775, 693.583127712765, 689.9323784867076, 693.0294507602885, 705.4764317822846, 695.4813487140855, 705.0112086312856, 704.1131474866191, 706.9021110288919, 741.4604895595205, 744.5399871829042, 720.9176310265867, 735.129265378322, 734.824068347989, 736.2235564723101, 722.7288405638302, 731.4372562675445, 741.0698693091828, 31.991967021405625, 749.9878469861472, 767.4134570109397, 775.5180409733376, 774.2229879100805, 781.5052851250008, 776.8414486699696, 782.6549108280467, 779.2853368016853, 737.0297092159703, 764.3348982091686, 781.4635933082851, 70.80673327201563, 708.324174630839, 751.9870226250752, 740.71369875058, 752.6503419115138, 758.5074741541399, 762.0537301596567, 17.06062064811445, 198.10995339857936, 737.5253310331371, 743.1743084427989, 744.8876868690668, 726.6165342059837, 728.7239984892162, 734.5424348976946, 620.3894616407302, 19.132135099577415, 649.505348412739, 678.0137668915829, 664.2290988629148, 747.1946608763548, 744.257134400641, 736.1339885082402, 742.0619364681297, 743.1614138135677, 721.8377899374641, 740.0796027741632, 745.8055216575477, 733.9341364189187, 700.0903318309233, 743.9889763435864, 738.3823051272233, 736.5907228247925, 741.0722687802098, 28.672434367908505, 736.7368348915845, 782.2140861938432, 789.0735534014884, 785.2266624604518, 739.6912300070128, 751.6746819354397, 747.4008449978603, 741.5697408460499, 744.1744611400642, 750.615298010069, 776.4665488400929, 729.166818984829, 791.9430885054177, 791.7491086486086, 738.8296020562829, 125.72402382666267, 65.57669219964498, 668.8258279794616, 711.8596633459437, 715.5578423327732, 702.3248360647369, 710.276400971174, 715.3351615105971, 714.8672785453607, 710.1803845219654, 23.190606799500223, 245.59178040161854, 664.7515398296773, 663.2067398326569, 682.2288740655362, 676.9615295165661, 728.6159866544357, 707.5795115737, 699.8364973030825, 714.1965120786887, 706.2184250070279, 708.3379568644428, 715.5080019157966, 710.9752024349276, 21.92448021506615, 414.0577119510905, 681.1314805984456, 683.7166055463291, 687.2404685706537, 692.6417404556274, 691.8192475211907, 692.7984788846794, 676.6195756711128, 657.881601810711, 692.0667307098947, 21.300726276780537, 689.4940414326645, 699.9669214975756, 703.8554287098152, 703.3953095613922, 765.3983580064455, 750.9670511879425, 763.2151604454275, 759.6733855832188, 768.1404568340603, 764.371681299608, 702.0015572708301, 738.0545899428059, 731.4916238157774, 736.4696283603636, 19.425572431646454, 360.97611832048, 680.0789816145659, 678.7294333494578, 697.2669922668176, 691.6900366283155, 701.4878158183326, 693.5372532950768, 750.2075897438764, 741.1026764736697, 749.187623813204, 31.8237034786573, 751.1922374271697, 744.4515655187834, 744.5671291797365, 782.3541634954325, 778.9633842707324, 778.1664775978205, 766.9247809585034, 781.4068041797293, 763.407764977659, 765.295898942397, 39.21222436297233, 700.9278481400669, 718.7208502086216, 729.0792640707147, 739.6050465763986, 751.4051746298488, 768.4838918407842, 731.5840951735739, 756.8361810316877, 699.6825844666528, 693.9266065840121, 682.7856663784003, 685.150939176853, 677.4572424999914, 17.749262496651045, 46.73830910185637, 737.2272372583285, 722.8511313038655, 734.359154936716, 742.3806780304965, 735.9135259722542, 732.1736124934334, 740.1979406256598, 743.025011661103, 742.3191708352886, 19.08048517410441, 725.834535060416, 715.0607319493562, 731.8905081445588, 724.8309474625197, 732.1706258506345, 727.3312158746182, 733.6460784782921, 144.02299470213276, 770.3615670670061, 776.8062397946167, 750.0726885293661, 765.7197461271606, 769.8876282513264, 756.8135857586848, 756.1486612917563, 378.80222433719064, 771.5221171272132, 73.40915041261346, 699.0612307446763, 717.0798479831619, 730.4760772044345, 730.2985535097812, 764.0734169865189, 776.3999630027133, 763.4648469900663, 765.5233516718655, 750.8148363546994, 697.2334168467108, 42.73649740198401, 777.0590968087555, 779.2491601930031, 778.2918211640672, 749.5456867655264, 751.1075110386173, 753.4327625511477, 780.915682842893, 17.997077850570104, 726.4500137651339, 732.1900337328019, 725.9985469431448, 728.9627853966197, 729.1864153409156, 718.3844639212034, 725.137245182612, 58.4883506525012, 69.5917125837548, 747.8820354805003, 736.9667240978417, 740.0389399918054, 749.4152985982452, 744.4542687144619, 743.8140139712983, 750.6589601327646, 741.1350049997104, 748.0807533115911, 742.0498666490571, 742.8652675878569, 748.3427835369649, 731.1167677285675, 745.7860314493726, 743.983733218252, 746.8242715602261, 743.2289230178276, 741.3275495430195, 746.7554346276398, 748.6266138376858, 739.0774985488134, 726.5066043191262, 706.2591924559729, 739.4448845636952, 736.6929398702534, 15.7590039763994, 682.2529248285326, 744.2588175846737, 767.2140560503578, 769.3159013075789, 767.1017069900744, 766.4034386322915, 765.9593893068165, 768.7116292945402, 27.85874980347596, 654.2888312147112, 717.4196978846987, 718.9197805809583, 709.7107995886402, 713.1332867122137, 704.7021246223471, 710.3851928154877, 705.5558593008216, 695.9354979232199, 21.761088009156822, 688.8940919406224, 714.3602950450136, 713.0037113437744, 722.6783386828953, 721.5831921647211, 701.4724914962042, 721.5378645909984, 714.2607993318496, 713.6414612125249, 20.89531043210454, 667.0038349233075, 665.9150344040579, 680.6825215362273, 687.6129280762052, 686.28705329043, 729.5871569329086, 737.5193024390463, 738.2665049630116, 742.3508173958325, 730.5570293351542, 727.7740725167589, 54.70554202201127, 48.74980629133013, 704.4682200000959, 734.6918121425773, 652.9110822601151, 717.5622538206867, 770.0232040121188, 760.3343554520035, 671.1973489290438, 327.4977182927722, 711.8843496734561, 553.6281325530662, 624.9084801979535, 617.6402724375037, 681.6003066630429, 683.9406247661484, 709.3891709721026, 726.8607875989557, 737.2022262216038, 743.2169516895626, 743.8739286205725, 49.57459905414438, 727.4224660540756, 752.4246493031516, 749.4612873375065, 732.269014479818, 756.1398399295226, 734.3525284140596, 751.6788903051362, 754.0458961355433, 23.331077196462438, 689.8324026509774, 739.0770079962663, 736.8493745113736, 728.3208280462649, 753.1694946370644, 747.0110828175543, 755.9048428611894, 760.9599941441196, 743.3149239762431, 748.8759556183028, 751.2983623877145, 18.613631572791125, 709.5866062678232, 705.4300370736634, 713.6863429917869]
Elapsed: 0.18466576564964568~0.44247172632256265
Time per graph: 0.003768689094890728~0.009030035231072708
Speed: 665.0338066052637~198.10121825154113
Total Time: 0.0725
best val loss: 0.033160340040922165 test_score: 0.9184

Testing...
Test loss: 0.2306 score: 0.9184 time: 0.06s
test Score 0.9184
Epoch Time List: [0.35466102801728994, 0.34879771899431944, 0.34707444615196437, 0.34521770698484033, 0.34669743082486093, 0.3471206119284034, 0.343686412088573, 0.3656709318747744, 0.33787984191440046, 0.33250794000923634, 0.3370203779777512, 0.3398236819775775, 0.3464504941366613, 0.34517315903212875, 0.3667381718987599, 0.33125817112158984, 0.3323449249146506, 0.3408403500216082, 0.3441269419854507, 0.3411552320467308, 0.3341121730627492, 0.33807753189466894, 0.33720761199947447, 0.3394647949608043, 0.35653441795147955, 0.3442439379869029, 0.35732928093057126, 0.3532685290556401, 0.35629380284808576, 0.3549065310508013, 0.3520958620356396, 0.35185827093664557, 0.36760184390004724, 0.35244769405107945, 0.35784620500635356, 0.37074892804957926, 0.38659041677601635, 0.36300201108679175, 8.828036851948127, 2.7458150800084695, 0.3362964190309867, 0.33267008315306157, 0.331307491986081, 0.3315737619996071, 0.3297655910719186, 0.3312179740751162, 0.3686258470406756, 0.3492577699944377, 0.3438410769449547, 0.34658168791793287, 0.3440385019639507, 0.3426663710270077, 0.3461243938654661, 0.34087420895230025, 0.34185926895588636, 0.4031606251373887, 3.901398337096907, 3.7006525680189952, 0.34928196715191007, 0.34322302299551666, 0.3422585019143298, 0.3405923709506169, 0.3408574990462512, 0.3410162559011951, 0.3380641491385177, 0.33812599815428257, 0.33915214508306235, 4.290065223001875, 4.519831335055642, 0.335048369015567, 0.3334391439566389, 0.33138746803160757, 0.32912706199567765, 0.3328175221104175, 0.3318322158884257, 0.33071281609591097, 0.3308352080639452, 0.3676125450292602, 0.3349223919212818, 0.33907663193531334, 0.33666958496905863, 0.3541397190419957, 0.35542208701372147, 0.3555855309823528, 0.35526176320854574, 0.35685451806057245, 3.1545566669665277, 12.287578499992378, 0.598222409025766, 0.3513539960840717, 0.3538803639821708, 0.35199203493539244, 0.35108029504772276, 0.35105320205911994, 0.35157027596142143, 0.3516533739166334, 0.3516105831367895, 0.35207372694276273, 2.621525263064541, 5.399288546061143, 0.44251861702650785, 0.34884328907355666, 0.34817383415065706, 0.3549481270601973, 0.36596053279936314, 0.3644844340160489, 0.3618638348998502, 0.3594078420428559, 0.34760552807711065, 0.340364454081282, 0.3410788901383057, 0.3439061460085213, 0.34186065604444593, 0.3417764969635755, 0.34174621501006186, 0.34179215086624026, 0.34136102988850325, 0.34787045896518975, 0.44024449004791677, 0.33989620790816844, 0.3385023099835962, 0.3426814441336319, 0.34195284405723214, 0.425064934999682, 0.35814284696243703, 0.35697474202606827, 0.36197494389489293, 0.35930154006928205, 0.3788835140876472, 0.35932089400012046, 0.36132341099437326, 0.3612116539152339, 0.39764694299083203, 0.33915310609154403, 0.33687887189444155, 0.3392181908711791, 0.3422145239310339, 0.3387857541674748, 0.3393261010060087, 0.33617230399977416, 0.33967701508663595, 0.33933186205103993, 0.34125961107201874, 0.3409789049765095, 0.34272445377428085, 0.34047149994876236, 0.3397202448686585, 0.3398545100353658, 0.33974844007752836, 0.34693713998422027, 0.381197799812071, 0.38554771477356553, 0.36673019907902926, 0.36545101401861757, 0.3637897199951112, 0.36320168210659176, 0.36247573094442487, 0.36218756902962923, 0.361196466954425, 0.3631137650227174, 0.36391218984499574, 0.3628383129835129, 0.36706941202282906, 0.3825780398910865, 0.3481142020318657, 0.3546405170345679, 0.3550061739515513, 0.3545771630015224, 0.3357837270013988, 0.3380249289330095, 0.3387682989705354, 0.33956126391422004, 0.3661978240124881, 0.37176543788518757, 0.3203948970185593, 0.31869441200979054, 0.33986712503246963, 0.3138645169092342, 0.3346203329274431, 0.3286459851078689, 0.3113959188340232, 0.3304017239715904, 0.3100027129985392, 0.3093413549941033, 0.31928359193261713, 0.33263275609351695, 0.33337060804478824, 0.33325628796592355, 0.33281554689165205, 0.3331297099357471, 0.3336211609421298, 0.3366746640531346, 0.33326759305782616, 0.3532231819117442, 0.5634481590241194, 9.961061165900901, 4.059816279099323, 0.3497970850439742, 0.3168137149186805, 0.3171656869817525, 0.3212459458736703, 0.3118877599481493, 0.3093031420139596, 0.31181659596040845, 0.3085173259023577, 0.3086395871359855, 0.312263744068332, 0.31117692100815475, 0.31233350303955376, 0.31456467893440276, 4.956505561014637, 0.45903625898063183, 0.3510071139317006, 0.3380434380378574, 0.3384699489688501, 0.3397538240533322, 0.3380662730196491, 0.3388734071049839, 0.33870814100373536, 0.3366962719010189, 0.34067399508785456, 0.3331900999182835, 0.3371780371526256, 0.33880184206645936, 0.3277260819450021, 8.113152488018386, 1.9501210818998516, 0.46185499406419694, 0.3248292059870437, 0.3932163800345734, 0.3201569520169869, 0.3216272038407624, 0.33032001298852265, 0.3208835310069844, 0.3239542560186237, 0.32531793287489563, 0.5144011869560927, 7.467358872992918, 0.3249490469461307, 0.3155611710390076, 0.31305517605505884, 0.31442567717749625, 0.3513366130646318, 0.33148740988690406, 0.3315334591316059, 0.34988717501983047, 0.3477314949268475, 4.9399193349527195, 9.985936373937875, 0.32973977213259786, 0.3256427019368857, 0.3366663680644706, 0.33948356297332793, 0.3397665120428428, 0.3431476909900084, 1.5159702419769019, 6.419942886917852, 0.3516977148829028, 0.3466801369795576, 0.34838328312616795, 0.3479927530279383, 0.3498027571476996, 0.34609429095871747, 0.35503467614762485, 0.36935532419010997, 0.37184933898970485, 0.3728847870370373, 0.3731165879871696, 2.684537003049627, 6.445616625947878, 0.34950807900168, 0.3410238070646301, 0.3374244279693812, 0.34410519106313586, 0.3375959969125688, 0.4561329890275374, 0.36118795396760106, 0.36618597502820194, 0.36344826384447515, 0.4004437478724867, 4.992058176081628, 3.215855870046653, 0.3496838129358366, 0.3464537840336561, 0.3466708419146016, 0.34613776800688356, 0.3470766111277044, 0.3491375989979133, 0.34660141298081726, 0.34589115297421813, 0.34669062204193324, 0.34663602604996413, 0.34190760005731136, 0.3331450499827042, 0.335598828853108, 6.098439085064456, 0.37116509792394936, 0.36576180695556104, 0.34558180207386613, 0.34470887295901775, 0.3459386290051043, 0.3470731768757105, 0.34721967088989913, 0.34128599101677537, 0.343607616960071, 3.844389762962237, 3.3326252789702266, 1.4211816960014403, 0.3643488270463422, 0.3602919310797006, 0.36355860612820834, 0.3613976330962032, 0.3612228808924556, 0.36524520604871213, 0.3621863720472902, 0.3635731649119407, 0.3648919730912894, 5.473455515806563, 3.5531620089896023, 0.3514135121367872, 0.3800099330255762, 0.36015338893048465, 0.33959471993148327, 0.3397431130288169, 0.33975910407025367, 0.3426823350600898, 0.34006670711096376, 0.3412776459008455, 0.3416764280991629, 0.34124018799047917, 0.3422623658552766, 0.34405090694781393, 0.34693699097260833, 0.3537589539773762, 0.34833779197651893, 0.35007382498588413, 0.3688241910422221, 6.809974278905429, 3.1992494580335915, 0.3662064168602228, 0.3826217890018597, 0.3543688071658835, 0.35450176801532507, 0.35801208310294896, 0.3604542230023071, 0.4584904379444197, 0.3355760750127956, 2.4055189180653542, 6.347519895993173, 0.38292378187179565, 0.37354764400515705, 0.3777542740572244, 0.37428380304481834, 0.37423605006188154, 2.5739918801700696, 11.136254275101237, 0.33672231796663254, 0.3385464991442859, 0.3336212339345366, 0.33810531499329954, 0.33597082388587296, 0.33941807597875595, 0.35075804113876075, 0.36173380981199443, 0.35559801710769534, 3.8162197939818725, 2.9574019338469952, 1.5338329090736806, 0.3572758799418807, 0.35936656105332077, 0.3619653369532898, 0.3589975112117827, 0.35732368589378893, 0.3570430971449241, 0.3574551399797201, 0.3608187170466408, 4.208252870943397, 2.2303703060606495, 2.206689973012544, 0.3534953958587721, 0.3496341039426625, 0.35037629294674844, 0.35370420396793634, 0.3519219630397856, 0.35847331990953535, 0.3575371300103143, 0.35610227612778544, 0.35631770407781005, 0.5279968309914693, 8.909550536074676, 3.0472395459655672, 0.346198390936479, 0.33812695217784494, 0.34086659003514796, 0.33640856202691793, 0.33655855990946293, 0.33529733296018094, 0.3380739811109379, 0.3351118649588898, 0.33727531589102, 0.3351021039998159, 0.3372016418725252, 0.33605773001909256, 0.33925605192780495, 0.3465259480290115, 0.3398223698604852, 0.3270635559456423, 0.326839841902256, 0.32577295508235693, 0.3283196260454133, 0.32961911300662905, 2.9753516050986946, 0.3765624179504812, 0.37266999401617795, 0.36045482114423066, 0.3556120719294995, 0.36354933003894985, 0.35505368805024773, 0.35414966009557247, 0.3542448189109564, 0.353305886965245, 0.35694288392551243, 0.3602205158676952, 0.3744082519551739, 0.3751410919940099, 0.3718772779684514, 0.37055704987142235, 0.3731082559097558, 0.37022183591034263, 0.35501885402482003, 0.3477712390013039, 0.34695538599044085, 0.35814001597464085, 0.37700060207862407, 0.3686951401177794, 0.3753421929432079, 0.3740563109749928, 0.3734795100754127, 0.3815944930538535, 2.8708673949586228, 2.5403411649167538, 0.37126023101154715, 0.3583314560819417, 0.3671138419304043, 0.3735612710006535, 0.36994569399394095, 0.3764930759789422, 0.35945552913472056, 0.3610615929355845, 0.36339220905210823, 3.721769012976438, 9.577239054022357, 0.34770897100679576, 0.33579093299340457, 0.33599921909626573, 0.33621648396365345, 0.3403955219546333, 0.3414374788990244, 0.34023231000173837, 0.3410611590370536, 0.34963795309886336, 6.012583700125106, 0.3443752779858187, 0.3494429829297587, 0.394994736998342, 0.34985430887900293, 1.0026647709310055, 11.032310905051418, 0.3469322600867599, 0.3377396789146587, 0.3312435980187729, 0.3304609680781141, 0.32765671086963266, 0.3259010559413582, 0.374714974896051, 0.3754503910895437, 0.3661917260615155, 2.2400190300541, 8.133808589889668, 0.37131143792066723, 0.35023288091178983, 0.3402219822164625, 0.32791094994172454, 0.3389467769302428, 0.34394204698037356, 0.34032701898831874, 0.32275442592799664, 0.32268966105766594, 0.32098556717392057, 0.35164952592458576, 0.3332819779170677, 0.32412342191673815, 0.3190581798553467, 5.037316430127248, 4.943052515969612, 0.35687855596188456, 0.35384499398060143, 0.34075166401453316, 0.3552014500601217, 0.3404121899511665, 0.3447269609896466, 0.3434298379579559, 0.342253998038359, 0.3434770068852231, 0.3445491259917617, 0.3438583791721612, 0.3477933070389554, 3.958188163000159, 2.044408515910618, 1.4214134220965207, 1.1197641220642254, 0.3710366659797728, 0.3671548960264772, 0.3672111810883507, 0.3603573850123212, 0.3614869370358065, 0.36270310601685196, 0.36109838902484626, 0.35993031703401357, 0.3565416340716183, 0.3456794270314276, 0.3466373310657218, 0.3495594068663195, 0.3452714978484437, 0.345770665910095, 0.3506708990316838, 0.3492955759866163, 0.38437382807023823, 2.49600282299798, 6.283647094969638, 0.3318392470246181, 0.3301904440158978, 0.33112334995530546, 0.3262701010098681, 0.32852982403710485, 0.3274373939493671, 0.33014452701900154, 0.3359497310593724, 0.3304560160031542, 0.32703987101558596, 6.9355597399408, 0.4313729549758136, 0.3415411659516394, 0.3376557899173349, 0.340548946056515, 0.3390649009961635, 0.33510708902031183, 5.9864561159629375, 8.58530032786075, 0.35590433108154684, 0.34466857789084315, 0.3422566739609465, 0.3509080717340112, 0.34697955509182066, 0.3467403131071478, 0.3720713489456102, 3.0805517341941595, 5.0109451349126175, 0.3537109608296305, 0.35988326999358833, 0.34610274515580386, 0.343524924130179, 0.344766044174321, 0.34323143505025655, 0.34643908799625933, 0.3471700489753857, 0.3419534619897604, 0.3430945680011064, 0.36317118583247066, 0.3646984670776874, 0.3441412370884791, 0.3436664171749726, 0.3436224309261888, 0.3403622079640627, 6.061206821119413, 1.5923514199675992, 0.3233986860141158, 0.32173564401455224, 0.3205642600078136, 0.32954564807005227, 0.33719104109331965, 0.4361002161167562, 0.33404941693879664, 0.33791214879602194, 0.3353727828944102, 0.3827096540480852, 0.3402253311360255, 0.32627502689138055, 0.3255485659465194, 0.34423125092871487, 3.15313569502905, 4.366754961083643, 1.8016857729526237, 0.3594835699768737, 0.3581790248863399, 0.3599916729144752, 0.36039965506643057, 0.3554761150153354, 0.356321377097629, 0.3567397288279608, 2.4010210600681603, 4.33055788197089, 0.4439016890246421, 0.3779781301273033, 0.37357520207297057, 0.3734155319398269, 0.3589896430494264, 0.3616075289901346, 0.3625420810421929, 0.35882300103548914, 0.35819062287919223, 0.3566190211568028, 0.35603526304475963, 0.3582183290272951, 7.7048700619488955, 4.108974049100652, 0.6073998269857839, 0.37087830307427794, 0.3701708111912012, 0.3668948949780315, 0.36799270287156105, 0.3661339631071314, 0.3662446920061484, 0.38942060514818877, 0.3931120560737327, 3.334202175028622, 2.6087780668167397, 0.36420102301053703, 0.3608195760753006, 0.3610902400687337, 0.35062747297342867, 0.33644134597852826, 0.3369904700666666, 0.3356727068312466, 0.33751439105253667, 0.3366888299351558, 0.37016356410458684, 0.36201099096797407, 0.34478228411171585, 0.3441518390318379, 2.9631890980526805, 4.241927940165624, 0.3751391270197928, 0.3819432408781722, 0.3654171519447118, 0.3645374459447339, 0.36142101406585425, 0.38073789002373815, 0.34637335187289864, 0.3380340279545635, 0.33545792009681463, 2.849907591124065, 5.436297988053411, 0.33644099987577647, 0.3347772470442578, 0.3288615520577878, 0.36177430604584515, 0.32476106099784374, 0.32777081697713584, 0.32440934085752815, 0.32718251703772694, 0.32897788390982896, 4.428870304953307, 1.0887283169431612, 0.3516640510642901, 0.349619589978829, 0.3440272267907858, 0.34179178380873054, 0.3337915779557079, 0.3336407449096441, 0.3360260840272531, 0.34555249521508813, 0.3650512979365885, 0.36459543101955205, 0.36419683403801173, 0.3661816450767219, 7.364501175004989, 7.51874090207275, 1.1129047040594742, 0.3443766199052334, 0.3426252199569717, 0.3417117960052565, 0.34478468098677695, 0.343149948050268, 0.34245666197966784, 0.3414590619504452, 0.3417076071491465, 5.414116673870012, 5.294785809004679, 0.3518875549780205, 0.3487880389438942, 0.34683936601504683, 0.34720263886265457, 0.3459662700770423, 0.34200937394052744, 2.895012697088532, 2.7407504779985175, 0.32338713807985187, 0.3605498820543289, 0.33306918188463897, 0.3304050149163231, 0.334542035125196, 0.33704199199564755, 0.40744128613732755, 0.3253040869021788, 0.9306352860294282, 6.556501811021008, 0.3551014281110838, 0.3525343640940264, 0.349847940960899, 0.34006018901709467, 0.32744138687849045, 0.3277115050004795, 0.327443263027817, 0.3491881008958444, 0.38470455701462924, 4.12980349897407, 2.9083500429987907, 0.3224008629331365, 0.32459921983536333, 0.33225439791567624, 0.33547518588602543, 0.3357807390857488, 0.33270941907539964, 6.334462692029774, 5.275078818900511, 0.3377383011393249, 0.3487271830672398, 0.3473516749218106, 0.3477320648962632, 0.34690067591145635, 0.34715981408953667, 1.121183184091933, 10.139684263966046, 0.38726955000311136, 0.3437836499651894, 0.34207004110794514, 0.34305531601421535, 0.3435706009622663, 0.34174654714297503, 0.34222467988729477, 0.3414522660896182, 0.34076833410654217, 0.34181723196525127, 0.34051654709037393, 0.3398730930639431, 0.34146700403653085, 0.3379866679897532, 0.33830534608568996, 0.33875095611438155, 0.33897834294475615, 0.33842427795752883, 0.3384116970701143, 0.33677690604235977, 0.33980265888385475, 0.3420995681080967, 0.3455652929842472, 0.34499681496527046, 0.3912027641199529, 3.461057608947158, 5.39230818010401, 0.34015338809695095, 0.3293878921540454, 0.3290559609886259, 0.3299276860198006, 0.32981252600438893, 0.32918683090247214, 0.32849559315945953, 4.35589574999176, 1.6964873319957405, 0.36334540508687496, 0.3562976310495287, 0.361099852132611, 0.35956929193343967, 0.3648675160948187, 0.35821378882974386, 0.3585993030574173, 0.38043103099334985, 2.696468200883828, 3.3247096120612696, 0.36731807608157396, 0.35572582797612995, 0.3526992261176929, 0.3513243329944089, 0.3581092129461467, 0.3511536280857399, 0.3543191631324589, 0.3591798839624971, 2.923498790129088, 5.722741577075794, 0.3813949559116736, 0.3765489789657295, 0.37447208003140986, 0.377972193993628, 0.3578819171525538, 0.34961994690820575, 0.35215169994626194, 0.3454073560424149, 0.3457385399378836, 0.350276364129968, 2.076760584837757, 8.956783503876068, 1.896469717961736, 0.3816635940456763, 0.3735977441538125, 0.3499118429608643, 0.3334439860191196, 0.3330630719428882, 7.32278688903898, 0.43420600495301187, 0.35609145904891193, 0.40101046196650714, 0.443050246918574, 0.3463883940130472, 4.9956769129494205, 0.37140874203760177, 0.35919431888032705, 0.35474381409585476, 0.34977425599936396, 0.3421172689413652, 0.33921320200897753, 3.967183263041079, 0.4455843560863286, 0.34073346911463886, 0.33882416086271405, 0.33950615383218974, 0.3444180120714009, 0.34013334894552827, 0.33714351803064346, 0.3396126660518348, 5.229480785899796, 5.30242631805595, 0.3475636028451845, 0.3452491380739957, 0.34518174186814576, 0.34383456595242023, 0.341959115001373, 0.34028134401887655, 0.3373318569501862, 0.3377450780244544, 0.3379881370346993, 0.3400914069497958, 2.9042289000935853, 4.646248901030049, 0.36101389001123607, 0.3593963279854506]
Total Epoch List: [846]
Total Time List: [0.07254559500142932]
========================k_idx:1========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: AEtransGAT
AEtransGAT(
  (gae): GAE(
    (encoder): transGAT(
      (encoder): EdgeFeatureGAT()
      (classifier): Sequential(
        (0): Linear(in_features=64, out_features=64, bias=True)
        (1): ReLU(inplace=True)
        (2): Linear(in_features=64, out_features=2, bias=True)
      )
    )
    (decoder): InnerProductDecoder()
  )
)
Total number of parameters: 1914242
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x75feb28b18d0>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 3.7676;  Loss pred: 3.7676; Loss self: 0.0000; time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6928 score: 0.5102 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6934 score: 0.4898 time: 0.07s
Epoch 2/1000, LR 0.000000
Train loss: 3.7598;  Loss pred: 3.7598; Loss self: 0.0000; time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6928 score: 0.5102 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6933 score: 0.4898 time: 0.07s
Epoch 3/1000, LR 0.000030
Train loss: 3.7698;  Loss pred: 3.7698; Loss self: 0.0000; time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6928 score: 0.5102 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6933 score: 0.4898 time: 0.35s
Epoch 4/1000, LR 0.000060
Train loss: 3.8057;  Loss pred: 3.8057; Loss self: 0.0000; time: 6.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6928 score: 0.5102 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6931 score: 0.4898 time: 0.09s
Epoch 5/1000, LR 0.000090
Train loss: 3.7239;  Loss pred: 3.7239; Loss self: 0.0000; time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6927 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6930 score: 0.4898 time: 0.08s
Epoch 6/1000, LR 0.000120
Train loss: 3.7158;  Loss pred: 3.7158; Loss self: 0.0000; time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6928 score: 0.5102 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6928 score: 0.4898 time: 0.08s
     INFO: Early stopping counter 1 of 2
Epoch 7/1000, LR 0.000150
Train loss: 3.6876;  Loss pred: 3.6876; Loss self: 0.0000; time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6928 score: 0.5102 time: 0.07s
Test loss: 0.6927 score: 0.5306 time: 0.07s
     INFO: Early stopping counter 2 of 2
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 004,   Train_Loss: 3.7239,   Val_Loss: 0.6927,   Val_Precision: 0.5102,   Val_Recall: 1.0000,   Val_accuracy: 0.6757,   Val_Score: 0.5102,   Val_Loss: 0.6927,   Test_Precision: 0.4898,   Test_Recall: 1.0000,   Test_accuracy: 0.6575,   Test_Score: 0.4898,   Test_loss: 0.6930


[0.06833410204853863, 0.0679529559565708, 0.06700492603704333, 0.06759673892520368, 0.06693420500960201, 0.06688495411071926, 0.06407436996232718, 0.06819558294955641, 0.06436006794683635, 0.0635923370718956, 0.06472922000102699, 0.06660582998301834, 0.06671573990024626, 0.0672321260208264, 0.06456550001166761, 0.06372885906603187, 0.06462040601763874, 0.06521135393995792, 0.06485772202722728, 0.06542657501995564, 0.0653282740386203, 0.06485900201369077, 0.06518519599922001, 0.06499172688927501, 0.06779441307298839, 0.07171953504439443, 0.06770034204237163, 0.06795574899297208, 0.06900283496361226, 0.06733783206436783, 0.06744095589965582, 0.0675262650474906, 0.06708796403836459, 0.06968736799899489, 0.06876102194655687, 0.07160585897509009, 0.08581699000205845, 0.06630075396969914, 1.5063912719488144, 0.06452985003124923, 0.06432656198740005, 0.06390337203629315, 0.06379966402892023, 0.06354285508859903, 0.06323131208773702, 0.06361076305620372, 0.07259423902723938, 0.06666238897014409, 0.0668840779690072, 0.06706631905399263, 0.06660780101083219, 0.0665643879910931, 0.06631771894171834, 0.06647277902811766, 0.0665992849972099, 0.06377321295440197, 1.6493149939924479, 0.06546144897583872, 0.06604680197779089, 0.06500831095036119, 0.06556718703359365, 0.065528545062989, 0.0653838359285146, 0.06549742701463401, 0.06476026494055986, 0.06490289408247918, 0.06503039796371013, 1.0126681280089542, 0.06985096004791558, 0.06452557700686157, 0.06373154500033706, 0.06308141292538494, 0.06338339007925242, 0.06385672697797418, 0.06331021699588746, 0.06280869897454977, 0.06342368293553591, 0.07260816299822181, 0.06360519398003817, 0.07251774694304913, 0.06800691201351583, 0.06818427098914981, 0.06826049101073295, 0.06778019201010466, 0.06869762891437858, 0.06810694606974721, 2.3145632080268115, 2.3269002290908247, 0.06928620510734618, 0.06763326097279787, 0.0679139920976013, 0.06733823393005878, 0.06739226100035012, 0.06748012395109981, 0.06688652408774942, 0.06777872098609805, 0.06764911999925971, 0.06769327609799802, 2.340762081905268, 0.6299631940200925, 0.06874702800996602, 0.06714840105269104, 0.06626055401284248, 0.07058287796098739, 0.06962641701102257, 0.06981276790611446, 0.06932449294254184, 0.06921096902806312, 0.0661068509798497, 0.06564042903482914, 0.06535239203367382, 0.06607088400050998, 0.06513929495122284, 0.06550110504031181, 0.06556485104374588, 0.06548797292634845, 0.06551938899792731, 0.06560793099924922, 0.16484043491072953, 0.0662523980718106, 0.06535482499748468, 0.0660698750289157, 0.06517630896996707, 0.1523956989403814, 0.06925167900044471, 0.06931907392572612, 0.06995846098288894, 0.06953941204119474, 0.06994888500776142, 0.06974644993897527, 0.07015235908329487, 0.0694645990151912, 0.06548912299331278, 0.06475556991063058, 0.06462515296880156, 0.06616493093315512, 0.06549291696865112, 0.06514728302136064, 0.06534865009598434, 0.06467923091258854, 0.06593856704421341, 0.06637915293686092, 0.06614321900997311, 0.06629478395916522, 0.06608096603304148, 0.06625307898502797, 0.065597637090832, 0.06561642000451684, 0.06555224198382348, 0.07105369807686657, 0.0721471490105614, 0.07141565799247473, 0.07103964302223176, 0.07080650003626943, 0.06982119695749134, 0.07055863796267658, 0.0711759909754619, 0.0702351339859888, 0.0703665919136256, 0.07002281001769006, 0.07067594002000988, 0.07012129202485085, 0.07027707097586244, 0.09085514000616968, 0.0662083049537614, 0.0672642879653722, 0.07163944293279201, 0.07125823595561087, 0.06571922299917787, 0.06715138698928058, 0.06657942302990705, 0.06814709503669292, 0.08836912701372057, 0.07095531595405191, 0.06184779293835163, 0.06154624302871525, 0.06172040500678122, 0.06151789997238666, 0.07091481902170926, 0.06680259690620005, 0.06049473595339805, 0.06504169199615717, 0.06011264200787991, 0.060236489051021636, 0.06490972707979381, 0.06565647490788251, 0.06556651403661817, 0.06524249899666756, 0.06564106501173228, 0.06525126099586487, 0.06583019602112472, 0.06570325000211596, 0.062045417027547956, 0.06070071004796773, 0.3128449459327385, 2.015300514991395, 0.08074439002666622, 0.06174625095445663, 0.0615318400086835, 0.06143924698699266, 0.061508151004090905, 0.05942303000483662, 0.05972827400546521, 0.05999056599102914, 0.059661387000232935, 0.05985585192684084, 0.06016465602442622, 0.06051682902034372, 0.06061661895364523, 0.06149158091284335, 1.4328284329967573, 0.06771844800096005, 0.06721360702067614, 0.06570260203443468, 0.06553275091573596, 0.06666369608137757, 0.06529315700754523, 0.06490483798552305, 0.06532537692692131, 0.06567327596712857, 0.06547572906129062, 0.06494116398971528, 0.06512447993736714, 0.06536812102422118, 0.061968120047822595, 1.402061619097367, 0.32461915793828666, 0.0641754069365561, 0.06370740802958608, 0.062294562929309905, 0.06238514801952988, 0.062286115949973464, 0.06443443894386292, 0.06227063899859786, 0.06324823899194598, 0.0624067970784381, 0.2563899619271979, 0.0745847950456664, 0.06112186796963215, 0.06046964193228632, 0.06063993205316365, 0.06037247506901622, 0.06987287499941885, 0.06119576597120613, 0.06349778105504811, 0.07167099602520466, 0.06848965701647103, 0.23333637102041394, 0.10375957598444074, 0.06251100800000131, 0.06258093705400825, 0.06758909206837416, 0.06465884600766003, 0.06445940304547548, 0.06579946400597692, 1.2442387669580057, 0.07085756293963641, 0.06728277006186545, 0.06618339999113232, 0.06635018100496382, 0.06625051004812121, 0.06832368206232786, 0.06599224708043039, 0.07110289693810046, 0.07139032997656614, 0.07155806303489953, 0.07256866502575576, 0.07184460398275405, 2.265620109043084, 0.0678205710137263, 0.07242972194217145, 0.0653404260519892, 0.0653053930727765, 0.06609777896665037, 0.06644903100095689, 0.06986327306367457, 0.0698251329595223, 0.07148983504157513, 0.07059536699671298, 0.0703550970647484, 1.4021176469977945, 0.07274275191593915, 0.06749198900070041, 0.06680228898767382, 0.06707307102624327, 0.06741432601120323, 0.06712762801907957, 0.0669220780255273, 0.0668640190269798, 0.0664813769981265, 0.06849959003739059, 0.06705894903279841, 0.0650606449926272, 0.06466939393430948, 0.0645461370004341, 0.07686687994282693, 0.07175156904850155, 0.06629737198818475, 0.06635211198590696, 0.06650314503349364, 0.06686897296458483, 0.06924016692209989, 0.0673291030107066, 0.06701235403306782, 0.06623583904001862, 1.2732593569671735, 0.8736019259085879, 0.07585632603149861, 0.06997894390951842, 0.0700540830148384, 0.07072442304342985, 0.07023579906672239, 0.07038604095578194, 0.07093961897771806, 0.07033697003498673, 0.070343590923585, 0.07045630004722625, 2.1002839050488546, 0.0830539830494672, 0.0673690470866859, 0.08197509695310146, 0.0655857619130984, 0.06469692999962717, 0.06500291696283966, 0.06500466202851385, 0.06589812401216477, 0.06497354805469513, 0.06499310804065317, 0.06563610897865146, 0.06509727600496262, 0.06529394199606031, 0.06565617804881185, 0.06547182297799736, 0.06528270698618144, 0.06510074995458126, 0.07037426601164043, 0.07180111994966865, 0.5760427849600092, 0.11270860594231635, 0.06893811793997884, 0.06926876294892281, 0.06818613596260548, 0.06848851696122438, 0.06885175406932831, 0.06875480699818581, 0.07773124298546463, 0.06444465997628868, 1.2882507800823078, 0.45440423500258476, 0.07219154201447964, 0.0727267850888893, 0.07235967507585883, 0.07245374796912074, 0.07200081297196448, 1.92329349310603, 0.06519285100512207, 0.0661633750423789, 0.06586349499411881, 0.06484770402312279, 0.06537698395550251, 0.06488197494763881, 0.066857949947007, 0.0731791720027104, 0.06572514493018389, 0.06376156001351774, 1.8158081059809774, 1.0302194351097569, 0.468877510051243, 0.06878308509476483, 0.07203998800832778, 0.06923187500797212, 0.06946918601170182, 0.07010159501805902, 0.06970352702774107, 0.07017065701074898, 0.07037222699727863, 1.693216944928281, 0.24316679302137345, 0.6655788409989327, 0.06838043592870235, 0.06912983593065292, 0.06880844198167324, 0.06953353097196668, 0.06816839589737356, 0.06971694203093648, 0.06966838601510972, 0.06967123004142195, 0.06974727695342153, 0.2403919470962137, 0.5589719930430874, 0.5014324019430205, 0.06596852699294686, 0.0662359029520303, 0.06565063400194049, 0.06573414790909737, 0.0653356050606817, 0.06567288597580045, 0.06622929300647229, 0.06534743995871395, 0.06533283297903836, 0.06561904097907245, 0.06572871294338256, 0.0657412389991805, 0.06731882505118847, 0.06782381201628596, 0.06251050694845617, 0.06326822703704238, 0.06350664596538991, 0.06345620006322861, 0.06347168399952352, 0.06744990090373904, 0.15337021194864064, 0.0727692210348323, 0.07163084601052105, 0.0684711099602282, 0.06885250401683152, 0.07047579099889845, 0.06914142810273916, 0.0686131149996072, 0.06826126296073198, 0.0681215530494228, 0.06889093795325607, 0.07076907309237868, 0.07189328991807997, 0.07463656307663769, 0.07306559500284493, 0.07282038894481957, 0.07219581701792777, 0.07189016893971711, 0.06766075093764812, 0.0672525119734928, 0.0668437429703772, 0.07279205194208771, 0.0725372910965234, 0.07185946602839977, 0.07333700195886195, 0.07295700896065682, 0.07281744305510074, 0.06642825505696237, 2.4225288700545207, 0.13961805088911206, 0.06979086494538933, 0.06902847799938172, 0.07304321101401001, 0.07386704604141414, 0.07584767905063927, 0.0745291979983449, 0.07025225600227714, 0.07109058101195842, 0.07212393707595766, 1.2217237900476903, 1.2488049590028822, 0.06526651093736291, 0.0644889889517799, 0.06496750400401652, 0.06540324701927602, 0.06527512497268617, 0.06576831999700516, 0.0660531249595806, 0.06636769406031817, 0.07429223402868956, 0.07363686198368669, 0.06689672800712287, 0.07645931001752615, 0.06942467694170773, 0.06549953203648329, 0.729588724905625, 0.41384778800420463, 0.06498586595989764, 0.06527975900098681, 0.06356052006594837, 0.06295447598677129, 0.06315730896312743, 0.06336738204117864, 0.07392470689956099, 0.07444600795861334, 0.06971489905845374, 0.7970731960376725, 1.3182900779647753, 0.06679402699228376, 0.06629594997502863, 0.06210175296291709, 0.06383077404461801, 0.06307642301544547, 0.07330795598682016, 0.06649823696352541, 0.06315581500530243, 0.06270428199786693, 0.06249454303178936, 0.06844849395565689, 0.06302750995382667, 0.06292722898069769, 0.06276307802181691, 2.4983681560261175, 0.0710204589413479, 0.06753073295112699, 0.06941689306404442, 0.06538577598985285, 0.06964696198701859, 0.06678891705814749, 0.0663457919145003, 0.06647827406413853, 0.0659459320595488, 0.06645173893775791, 0.06703659100458026, 0.06662942096590996, 0.06731070799287409, 1.2949593610828742, 0.5036123500904068, 0.14941497100517154, 0.07236599700991064, 0.07064762397203594, 0.07102145301178098, 0.07070406596176326, 0.06945660803467035, 0.07045480096712708, 0.06950244109611958, 0.06959108798764646, 0.06931652803905308, 0.06608578702434897, 0.06581244908738881, 0.06796893000137061, 0.06665494397748262, 0.06668262800667435, 0.06655587092973292, 0.06779859506059438, 0.06699139205738902, 0.06612062104977667, 1.531634487095289, 0.06533439201302826, 0.06385084800422192, 0.06318357202690095, 0.0632892600260675, 0.06269951199647039, 0.0630759340710938, 0.06260741397272795, 0.0628781239502132, 0.0664830730529502, 0.06410802400205284, 0.06270285707432777, 0.6920245820656419, 0.06917736504692584, 0.06516069895587862, 0.06615241500549018, 0.06510327209252864, 0.0646005499875173, 0.0642999280244112, 2.8721112209605053, 0.247337396023795, 0.06643839599564672, 0.06593338795937598, 0.06578172906301916, 0.06743584503419697, 0.06724082108121365, 0.06670819502323866, 0.07898264401592314, 2.5611360020702705, 0.07544202695135027, 0.07226991897914559, 0.0737697280710563, 0.06557862705085427, 0.06583746091928333, 0.06656396901234984, 0.06603222398553044, 0.06593453197274357, 0.06788228696677834, 0.06620909401681274, 0.06570077396463603, 0.0667634840356186, 0.06999096798244864, 0.06586119090206921, 0.06636128691025078, 0.066522695007734, 0.06612040696199983, 1.708958485047333, 0.06650950200855732, 0.06264269701205194, 0.06209814001340419, 0.062402363982982934, 0.0662438569124788, 0.06518777494784445, 0.06556053599342704, 0.06607605097815394, 0.06584477506112307, 0.06527977797668427, 0.06310638890136033, 0.0671999859623611, 0.06187313294503838, 0.06188829196617007, 0.06632111093495041, 0.3897425369359553, 0.7472167069790885, 0.07326272095087916, 0.06883379200007766, 0.06847804202698171, 0.06976828596089035, 0.06898722797632217, 0.06849935895297676, 0.0685441920068115, 0.06899655505549163, 2.1129244449548423, 0.199518078006804, 0.07371175102889538, 0.07388344698119909, 0.07182340393774211, 0.07238225196488202, 0.06725078902672976, 0.0692501679295674, 0.07001635408960283, 0.06860856804996729, 0.06938363297376782, 0.06917601905297488, 0.06848281202837825, 0.0689194219885394, 2.234944660915062, 0.11834099108818918, 0.07193912100046873, 0.07166711997706443, 0.07129964290652424, 0.07074364298023283, 0.07082774897571653, 0.07072763796895742, 0.07241883291862905, 0.07448148704133928, 0.07080242095980793, 2.300391045981087, 0.07106660399585962, 0.07000330800656229, 0.06961656897328794, 0.06966210796963423, 0.06401895103044808, 0.06524920091032982, 0.06420207896735519, 0.06450140406377614, 0.06379041692707688, 0.06410493899602443, 0.06980041495990008, 0.06639075302518904, 0.06698641297407448, 0.0665336330421269, 2.522448188974522, 0.1357430520001799, 0.07205045490991324, 0.07219371607061476, 0.07027437200304121, 0.07084097992628813, 0.06985153397545218, 0.0706522969994694, 0.06531525496393442, 0.06611769401933998, 0.0654041770612821, 1.5397327980026603, 0.0652296410407871, 0.0658202659105882, 0.0658100500004366, 0.06263148109428585, 0.06290411204099655, 0.0629685310414061, 0.06389153306372464, 0.06270741403568536, 0.06418588105589151, 0.06402752199210227, 1.249610314029269, 0.06990733800921589, 0.06817667803261429, 0.06720805598888546, 0.06625157606322318, 0.06521115591749549, 0.06376190902665257, 0.06697794597130269, 0.06474320497363806, 0.07003175595309585, 0.07061265490483493, 0.07176483399234712, 0.07151708798483014, 0.0723292879993096, 2.7606780850328505, 1.0483905160799623, 0.0664652599953115, 0.06778712500818074, 0.0667248439276591, 0.06600387301295996, 0.06658391002565622, 0.06692401797045022, 0.06619850895367563, 0.06594663602299988, 0.06600934197194874, 2.568068869994022, 0.06750849902164191, 0.06852564797736704, 0.06694990501273423, 0.06760197004768997, 0.0669242909643799, 0.06736958201508969, 0.0667896979721263, 0.3402234490495175, 0.0636064960854128, 0.06307879299856722, 0.06532700196839869, 0.06399208097718656, 0.063645651913248, 0.06474513793364167, 0.06480207201093435, 0.12935510103125125, 0.06351081700995564, 0.667491719010286, 0.07009400299284607, 0.0683326970320195, 0.06707954104058444, 0.06709584698546678, 0.06412996305152774, 0.06311180104967207, 0.06418108206707984, 0.06400849809870124, 0.06526242906693369, 0.0702777560800314, 1.1465609719743952, 0.06305826699826866, 0.062881043064408, 0.06295838998630643, 0.0653729330515489, 0.06523699907120317, 0.06503566401079297, 0.06274685100652277, 2.722664223983884, 0.06745130300987512, 0.06692251702770591, 0.06749324803240597, 0.06721879495307803, 0.0671981800114736, 0.06820860202424228, 0.06757341499906033, 0.8377736669499427, 0.7041068279650062, 0.06551835406571627, 0.06648875505197793, 0.06621273199561983, 0.06538430706132203, 0.0658200269099325, 0.06587668298743665, 0.06527598097454756, 0.06611480994615704, 0.06550094997510314, 0.06603329803328961, 0.06596081703901291, 0.06547801499255002, 0.06702075805515051, 0.06570249097421765, 0.06586165504995733, 0.06561115093063563, 0.06592854298651218, 0.06609763798769563, 0.0656171990558505, 0.06545319000724703, 0.06629886594600976, 0.06744604895357043, 0.069379627937451, 0.06626592599786818, 0.06651346490252763, 3.1093335640616715, 0.0718208720209077, 0.06583731202408671, 0.06386744300834835, 0.063692950992845, 0.06387679697945714, 0.0639349949778989, 0.06397205998655409, 0.06374301901087165, 1.7588728979462758, 0.07489047292619944, 0.06830032705329359, 0.06815781304612756, 0.06904220709111542, 0.06871085800230503, 0.06953292503021657, 0.06897666293662041, 0.06944878899957985, 0.07040882401634008, 2.2517256480641663, 0.0711284950375557, 0.06859283801168203, 0.06872334494255483, 0.06780333293136209, 0.06790623802226037, 0.06985305994749069, 0.0679105039453134, 0.0686023929156363, 0.06866192992310971, 2.345023786998354, 0.07346284599043429, 0.07358296099118888, 0.07198657002300024, 0.07126102200709283, 0.07139869499951601, 0.06716126995161176, 0.06643893907312304, 0.0663716959534213, 0.06600652798078954, 0.06707210803870112, 0.06732858705800027, 0.8957044970011339, 1.0051321990322322, 0.06955601205118, 0.06669463193975389, 0.0750485040480271, 0.0682867580326274, 0.06363444600719959, 0.06444533204194158, 0.07300386403221637, 0.1496193630155176, 0.06883140502031893, 0.08850706298835576, 0.07841148192528635, 0.0793342050164938, 0.07188963901717216, 0.07164364599157125, 0.06907351000700146, 0.06741318397689611, 0.06646751496009529, 0.06592960492707789, 0.06587137700989842, 0.9884094059234485, 0.06736113096121699, 0.06512280006427318, 0.06538029492367059, 0.06691529892850667, 0.0648028280120343, 0.06672544602770358, 0.06518740998581052, 0.0649827818851918, 2.100203071953729, 0.07103174598887563, 0.0662989099510014, 0.06649934395682067, 0.06727804301772267, 0.06505839701276273, 0.06559474300593138, 0.06482297403272241, 0.06439234700519592, 0.06592091510538012, 0.06543139705900103, 0.06522042700089514, 2.6324793100357056, 0.06905429100152105, 0.06946117605548352, 0.06865761196240783, 0.07449805096257478, 0.0754659870872274, 0.35305151401553303, 0.09021393593866378, 0.08009697101078928, 0.08416988595854491, 0.0757960609626025]
[0.0013945735111946659, 0.0013867950195218532, 0.0013674474701437416, 0.0013795252841878303, 0.0013660041838694289, 0.0013649990634840665, 0.0013076402033127996, 0.0013917465908072737, 0.0013134707744252316, 0.0012978027973856246, 0.0013210044898168774, 0.00135930265271466, 0.0013615457122499238, 0.0013720842045066611, 0.001317663265544237, 0.0013005889605312627, 0.0013187837962783417, 0.0013308439579583248, 0.0013236269801474956, 0.0013352362248970537, 0.0013332300824208223, 0.0013236531023202197, 0.0013303101224330614, 0.0013263617732505106, 0.0013835594504691508, 0.0014636639804978455, 0.0013816396335177884, 0.0013868520202647363, 0.0014082211217063727, 0.0013742414707013843, 0.0013763460387684861, 0.0013780870417855224, 0.0013691421232319304, 0.001422191183652957, 0.00140328616217463, 0.0014613440607161242, 0.001751367142899152, 0.0013530766116265133, 0.030742679019363558, 0.0013169357149234535, 0.001312786979334695, 0.0013041504497202684, 0.0013020339597738823, 0.001296792960991817, 0.0012904349405660617, 0.0012981788378817086, 0.0014815150821885588, 0.0013604569177580426, 0.0013649811830409632, 0.0013687003888569924, 0.0013593428777720854, 0.0013584568977774102, 0.0013534228355452723, 0.0013565873271044419, 0.0013591690815757125, 0.0013014941419265708, 0.033659489673315265, 0.0013359479382824228, 0.0013478939179141, 0.0013267002234767591, 0.001338105857828442, 0.0013373172461834488, 0.0013343639985411142, 0.0013366821839721228, 0.0013216380600114257, 0.0013245488588261058, 0.001327150978851227, 0.020666696489978657, 0.0014255297968962363, 0.0013168485103441136, 0.001300643775517083, 0.0012873757739874478, 0.0012935385730459678, 0.0013031985097545751, 0.0012920452448140299, 0.001281810183154077, 0.0012943608762354267, 0.0014817992448616698, 0.001298065183266085, 0.0014799540192459006, 0.0013878961635411394, 0.0013915157344724452, 0.001393071245116999, 0.0013832692246960134, 0.0014019924268240528, 0.0013899376748928002, 0.047235983837281864, 0.04748775977736377, 0.0014140041858642077, 0.0013802706320979157, 0.0013859998387265571, 0.001374249672042016, 0.0013753522653132677, 0.001377145386757139, 0.0013650311038316209, 0.0013832392037979194, 0.0013805942856991778, 0.0013814954305713882, 0.04777065473276058, 0.012856391714695764, 0.0014030005716319596, 0.0013703755316875723, 0.001352256204343724, 0.001440466897163008, 0.0014209472859392361, 0.0014247503654309073, 0.001414785570255956, 0.0014124687556747574, 0.0013491194077520346, 0.0013396005925475334, 0.0013337222864015065, 0.0013483853877655097, 0.0013293733663514865, 0.0013367572457206492, 0.0013380581845662423, 0.0013364892433948663, 0.0013371303877128021, 0.0013389373673316166, 0.0033640905083822353, 0.0013520897565675633, 0.0013337719387241773, 0.0013483647965084836, 0.0013301287544891238, 0.003110116304905743, 0.0014132995714376472, 0.0014146749780760432, 0.0014277236935283457, 0.0014191716743100966, 0.0014275282654645188, 0.0014233969375301078, 0.0014316807976182625, 0.001417644877861045, 0.0013365127141492404, 0.0013215422430740936, 0.0013188806728326849, 0.001350304712921533, 0.0013365901422173697, 0.0013295363881910334, 0.001333645920326211, 0.0013199843043385415, 0.001345685041718641, 0.001354676590548182, 0.0013498616124484309, 0.001352954774676841, 0.0013485911435314587, 0.0013521036527556728, 0.0013387272875679999, 0.0013391106123370783, 0.001337800856812724, 0.0014500754709564606, 0.0014723907961339063, 0.0014574624080096884, 0.0014497886331067706, 0.0014450306129850903, 0.0014249223868875783, 0.0014399722033199302, 0.0014525712443971817, 0.0014333700813467102, 0.0014360528961964408, 0.0014290369391365318, 0.0014423661228573444, 0.0014310467760173641, 0.001434225938282907, 0.0018541865307381566, 0.0013511898970155387, 0.0013727405707218818, 0.0014620294476080003, 0.0014542497133798137, 0.0013412086326362832, 0.0013704364691689915, 0.0013587637353042255, 0.0013907570415651615, 0.001803451571708583, 0.0014480676725316718, 0.0012621998558847271, 0.0012560457760962297, 0.0012596001021792085, 0.0012554673463752379, 0.0014472412045246788, 0.0013633183042081644, 0.0012345864480285316, 0.00132738146930933, 0.0012267886124057124, 0.0012293161030820742, 0.001324688307750894, 0.001339928059344541, 0.0013380921231962893, 0.0013314795713605626, 0.0013396135716680058, 0.0013316583876707116, 0.0013434733881862188, 0.0013408826531044074, 0.0012662330005622031, 0.0012387900009789333, 0.006384590733321194, 0.041128581938599904, 0.0016478446944217598, 0.0012601275704991147, 0.0012557518369119083, 0.0012538621834080135, 0.00125526838783859, 0.0012127148980578901, 0.0012189443674584736, 0.0012242972651230438, 0.0012175793265353661, 0.0012215479985069558, 0.001227850122947474, 0.0012350373269457901, 0.0012370738561968415, 0.0012549302227110887, 0.02924139659177056, 0.0013820091428767358, 0.0013717062657280844, 0.0013408694292741771, 0.0013374030799129788, 0.0013604835934975014, 0.0013325134083172496, 0.0013245885303167968, 0.0013331709576922717, 0.0013402709381046649, 0.001336239368597768, 0.0013253298773411282, 0.0013290710191299416, 0.0013340432862085955, 0.0012646555111800529, 0.028613502430558508, 0.006624880774250748, 0.001309702182378696, 0.0013001511842772669, 0.0012713176108022429, 0.0012731662861128546, 0.0012711452234688463, 0.0013149885498747534, 0.0012708293673183238, 0.0012907803875907343, 0.0012736081036415939, 0.005232448202595875, 0.001522138674401355, 0.0012473850606047378, 0.0012340743251487004, 0.0012375496337380335, 0.0012320913279391065, 0.0014259770408044665, 0.0012488931830858393, 0.0012958730827560838, 0.0014626733882694828, 0.0013977481023769599, 0.004761966755518651, 0.002117542367029403, 0.001275734857142884, 0.001277161980694046, 0.001379369225885187, 0.001319568285870613, 0.0013154980213362344, 0.0013428462042036106, 0.025392627897102157, 0.0014460727130538042, 0.001373117756364601, 0.0013506816324720882, 0.0013540853266319145, 0.0013520512254718616, 0.0013943608584148543, 0.0013467805526618446, 0.001451079529348989, 0.0014569455097258395, 0.0014603686333652965, 0.0014809931637909338, 0.001466216407811307, 0.04623714508251192, 0.0013840932859944142, 0.0014781575906565602, 0.0013334780826936572, 0.0013327631239342143, 0.0013489342646255177, 0.0013561026734889162, 0.0014257810829321341, 0.0014250027134596389, 0.001458976225338268, 0.001440721775443122, 0.001435818307443845, 0.02861464585709785, 0.0014845459574681459, 0.0013773875306265391, 0.0013633120201566086, 0.0013688381842090463, 0.0013758025716572087, 0.0013699515922261135, 0.0013657566943985162, 0.0013645718168771388, 0.0013567627958801327, 0.001397950817089604, 0.001368549980261192, 0.0013277682651556572, 0.0013197835496797853, 0.0013172681020496755, 0.0015687118355678965, 0.001464317735683705, 0.0013530075915956072, 0.0013541247344062645, 0.0013572070414998702, 0.0013646729176445883, 0.001413064631063263, 0.0013740633267491143, 0.0013675990618993432, 0.0013517518171432372, 0.025984884836064766, 0.017828610732828324, 0.0015480882863571144, 0.0014281417124391515, 0.0014296751635681307, 0.001443355572314895, 0.001433383654422906, 0.0014364498154241213, 0.0014477473260758786, 0.0014354483680609539, 0.0014355834882364285, 0.0014378836744331888, 0.04286293683773173, 0.001694979245907494, 0.0013748785119731815, 0.001672961162308193, 0.0013384849370020081, 0.0013203455101964729, 0.0013265901420987686, 0.001326625755683956, 0.0013448596737176484, 0.0013259907766264311, 0.00132638996001333, 0.0013395124281357443, 0.001328515836835972, 0.0013325294284910268, 0.0013399220009961603, 0.001336159652612191, 0.0013323001425751314, 0.0013285867337669646, 0.0014362095104416414, 0.0014653289785646663, 0.011755975203265493, 0.002300175631475844, 0.001406900366122017, 0.0014136482234474044, 0.0013915537951552138, 0.0013977248359433546, 0.0014051378381495572, 0.001403159326493588, 0.0015863518976625434, 0.0013151971423732384, 0.02629083224657771, 0.009273555816379281, 0.0014732967758057068, 0.0014842201038548837, 0.0014767280627726292, 0.001478647917737158, 0.001469404346366622, 0.03925088761440877, 0.0013304663470433075, 0.001350272960048549, 0.0013441529590636492, 0.0013234225310841386, 0.001334224162357194, 0.0013241219377069144, 0.0013644479581021837, 0.0014934524898512326, 0.0013413294883710997, 0.0013012563268064844, 0.03705730828532607, 0.021024886430811365, 0.00956892877655598, 0.0014037364305054046, 0.0014702038369046487, 0.0014128954083259618, 0.001417738490034731, 0.0014306447962869186, 0.0014225209597498178, 0.0014320542247091628, 0.0014361678979036454, 0.034555447855679204, 0.004962587612681091, 0.013583241653039443, 0.0013955191005857624, 0.00141081297817659, 0.0014042539179933314, 0.001419051652489116, 0.0013911917530076237, 0.0014227947353252343, 0.001421803796226729, 0.0014218618375800398, 0.0014234138153759496, 0.004905958104004361, 0.011407591694756886, 0.010233314325367766, 0.0013462964692438136, 0.0013517531214700062, 0.001339808857182459, 0.0013415132226346402, 0.001333379695115953, 0.0013402629790979686, 0.0013516182246218836, 0.0013336212236472235, 0.0013333231220211909, 0.0013391641016137234, 0.0013414023049669911, 0.0013416579387587856, 0.0013738535724732341, 0.0013841594289037951, 0.0012757246316011464, 0.0012911883068784159, 0.0012960539992936716, 0.001295024491086298, 0.0012953404897861943, 0.0013765285898722252, 0.003130004325482462, 0.001485086143568006, 0.0014618540002147155, 0.0013973695910250653, 0.0014051531432006433, 0.001438281448957111, 0.0014110495531171256, 0.0014002676530532083, 0.0013930869991986118, 0.0013902357765188326, 0.001405937509250124, 0.0014442667978036465, 0.0014672099983281627, 0.0015231951648293405, 0.0014911345918947945, 0.0014861303866289708, 0.001473384020774036, 0.001467146304892186, 0.001380831651788737, 0.001372500244356996, 0.0013641580198036165, 0.0014855520804507695, 0.0014803528795208857, 0.0014665197148653014, 0.0014966735093645295, 0.0014889185502174863, 0.0014860702664306273, 0.0013556786746318852, 0.04943936469499022, 0.0028493479773288176, 0.0014243033662324352, 0.0014087444489669738, 0.0014906777757961228, 0.0015074907355390641, 0.0015479118173599852, 0.0015210040407825488, 0.0014337195102505538, 0.0014508281839175187, 0.0014719170831828093, 0.02493313857240184, 0.02548581548985474, 0.00133196961096659, 0.001316101815342447, 0.0013258674286533983, 0.0013347601432505312, 0.0013321454076058402, 0.0013422106121837789, 0.0013480229583587877, 0.0013544427359248607, 0.0015161680414018277, 0.0015027931017078916, 0.0013652393470841404, 0.0015603940819903296, 0.0014168301416675048, 0.0013367251436016997, 0.01488956581440051, 0.008445873224575604, 0.0013262421624468906, 0.0013322399796119758, 0.0012971534707336401, 0.001284785224219822, 0.0012889246727168864, 0.001293211878391401, 0.0015086674877461425, 0.00151930628486966, 0.00142275304200926, 0.016266799919136172, 0.026903879142138272, 0.001363143408005791, 0.0013529785709189518, 0.0012673827135289203, 0.0013026688580534288, 0.0012872739390907238, 0.0014960807344249012, 0.001357106876806641, 0.0012888941837816822, 0.0012796792244462638, 0.0012753988373834565, 0.001396908039911365, 0.0012862757133434014, 0.0012842291628713815, 0.001280879143302386, 0.050987105225022804, 0.0014493971212519979, 0.0013781782234923877, 0.0014166712870213147, 0.0013344035916296498, 0.0014213665711636447, 0.001363039123635663, 0.001353995753357149, 0.0013566994706967048, 0.001345835348154057, 0.0013561579375052635, 0.0013680936939710257, 0.001359784101345101, 0.0013736879182219201, 0.0264277420629158, 0.010277803063069527, 0.0030492851225545213, 0.001476857081834911, 0.0014417882443272642, 0.0014494174084036934, 0.001442940121668638, 0.0014174817966259255, 0.0014378530809617772, 0.0014184171652269302, 0.0014202262854621727, 0.0014146230212051648, 0.0013486895311091626, 0.0013431112058650777, 0.0013871210204361348, 0.0013603049791322984, 0.0013608699593198846, 0.001358283080198631, 0.0013836447971549873, 0.0013671712664773269, 0.0013494004295872791, 0.03125784667541406, 0.0013333549390413932, 0.0013030785306984065, 0.0012894606536102233, 0.0012916175515523978, 0.0012795818774789876, 0.0012872639606345672, 0.0012777023259740398, 0.001283227019392106, 0.0013567974092438817, 0.001308327020450058, 0.001279650144374036, 0.014122950654400855, 0.0014117829601413437, 0.0013298101827730329, 0.0013500492858263304, 0.0013286382059699722, 0.0013183785711738225, 0.0013122434290696163, 0.0586145147134797, 0.005047701959669286, 0.0013558856325642187, 0.0013455793461097138, 0.0013424842665922276, 0.0013762417353917749, 0.0013722616547186459, 0.001361391735168136, 0.0016118906942025131, 0.05226808167490348, 0.0015396332030887811, 0.0014748963056968488, 0.0015055046545113533, 0.0013383393275684544, 0.0013436216514139455, 0.0013584483471908132, 0.001347596407867968, 0.0013456026933212973, 0.0013853527952403743, 0.0013512060003431172, 0.0013408321217272658, 0.0013625200823595633, 0.0014283871016826254, 0.0013441059367769227, 0.00135431197776022, 0.001357606020566, 0.0013493960604489762, 0.034876703776476184, 0.0013573367756848432, 0.00127842238800106, 0.0012673089798653917, 0.0012735176323057742, 0.001351915447193445, 0.0013303627540376417, 0.0013379701223148375, 0.001348490836288856, 0.0013437709196147565, 0.0013322403668711077, 0.001287885487782864, 0.0013714282849461449, 0.0012627169988783343, 0.0012630263666565322, 0.0013534920598969472, 0.007953929325223577, 0.015249320550593642, 0.0014951575704261052, 0.0014047712653077074, 0.001397511061775137, 0.0014238425706304153, 0.001407902611761677, 0.0013979461010811584, 0.0013988610613635, 0.0014080929603161557, 0.04312090703989474, 0.004071797510342938, 0.0015043214495692933, 0.0015078254485958998, 0.0014657837538314717, 0.001477188815609837, 0.0013724650821781584, 0.0014132687332564775, 0.0014289051855020986, 0.0014001748581625977, 0.0014159925096687309, 0.0014117554908770385, 0.0013976084087424132, 0.0014065188160926408, 0.04561111552887881, 0.0024151222671059016, 0.0014681453265401783, 0.001462594285246213, 0.0014550947531943722, 0.001443747815923119, 0.0014454642648105414, 0.0014434211830399474, 0.001477935365686307, 0.0015200303477824343, 0.0014449473665266925, 0.046946756040430346, 0.0014503388570583596, 0.0014286389389094344, 0.0014207463055773049, 0.0014216756728496782, 0.001306509204703022, 0.0013316163451087717, 0.001310246509537861, 0.001316355184975023, 0.001301845243409732, 0.0013082640611433557, 0.0014244982644877567, 0.0013549133270446742, 0.0013670696525321323, 0.0013578292457576916, 0.0514785344688678, 0.00277026636735061, 0.0014704174471410866, 0.0014733411442982604, 0.0014341708572049225, 0.0014457342842099617, 0.001425541509703106, 0.0014418836122340694, 0.0013329643870190699, 0.0013493406942722444, 0.0013347791236996347, 0.031423118326584905, 0.001331217164097696, 0.001343270732869147, 0.0013430622449068694, 0.0012781934917201192, 0.0012837573885917664, 0.0012850720620695122, 0.0013039088380351967, 0.0012797431435854155, 0.0013099159399161534, 0.0013066841222878014, 0.025502251306719775, 0.001426680367535018, 0.001391360776175802, 0.0013715929793650095, 0.0013520729808821057, 0.0013308399166835814, 0.001301263449523522, 0.0013668968565571978, 0.001321289897421185, 0.0014292195092468541, 0.0014410745898945903, 0.0014645884488234107, 0.0014595324078536763, 0.001476107918353257, 0.05634036908230307, 0.021395724817958414, 0.0013564338774553367, 0.001383410714452668, 0.0013617315087277367, 0.0013470178165910195, 0.0013588553066460453, 0.001365796285111229, 0.0013509899786464414, 0.0013458497147550996, 0.0013471294279989538, 0.0524095687753882, 0.0013777244698294268, 0.001398482611783001, 0.001366324592096617, 0.0013796320417895913, 0.0013658018564159165, 0.0013748894288793815, 0.0013630550606556389, 0.0069433356948881125, 0.0012980917568451592, 0.0012873223060932085, 0.0013332041218040548, 0.0013059608362691135, 0.0012988908553724081, 0.0013213293455845239, 0.0013224912655292725, 0.0026399000210459438, 0.001296139122652156, 0.013622279979801756, 0.0014304898569968585, 0.001394544837388153, 0.00136897022531805, 0.0013693029997034036, 0.0013087747561536273, 0.001287995939789226, 0.0013098180013689765, 0.0013062958795653314, 0.0013318863074884427, 0.0014342399200006407, 0.023399203509681533, 0.0012869034081279319, 0.0012832865931511838, 0.0012848651017613557, 0.0013341414908479368, 0.0013313673279837382, 0.0013272584491998566, 0.0012805479797249545, 0.0555645759996711, 0.0013765572042831657, 0.0013657656536266512, 0.0013774132251511424, 0.0013718121418995516, 0.0013713914288055835, 0.001392012286209026, 0.0013790492856951089, 0.017097421774488628, 0.014369527101326657, 0.0013371092666472708, 0.001356913368407713, 0.001351280244808568, 0.001334373613496368, 0.0013432658553047447, 0.0013444221017844215, 0.001332162877031583, 0.001349281835635858, 0.0013367540811245538, 0.001347618327209992, 0.0013461391232451614, 0.0013362860202561228, 0.001367770572554092, 0.0013408671627391357, 0.0013441154091828028, 0.0013390030802170538, 0.0013454804691124935, 0.0013489313875039925, 0.001339126511343888, 0.0013357793879030006, 0.0013530380805308114, 0.0013764499786442944, 0.001415910774233694, 0.0013523658366911874, 0.0013574176510719924, 0.06345578702166677, 0.0014657320820593408, 0.0013436186127364635, 0.001303417204252007, 0.0012998561427111225, 0.0013036081016215744, 0.0013047958158754877, 0.0013055522446235527, 0.0013008779389973805, 0.03589536526420971, 0.001528376998493866, 0.0013938842255774202, 0.0013909757764515827, 0.0014090246345125595, 0.0014022624082103067, 0.0014190392863309505, 0.0014076869987065391, 0.0014173222244812213, 0.001436914775843675, 0.045953584654370744, 0.001451601939541953, 0.0013998538369731028, 0.0014025172437256087, 0.0013837414883951448, 0.001385841592291028, 0.001425572651989606, 0.0013859286519451713, 0.0014000488350129857, 0.0014012638759818307, 0.047857628306088855, 0.0014992417549068223, 0.0015016930814528344, 0.0014691136739387804, 0.0014543065715733232, 0.0014571162244799186, 0.001370638162277791, 0.0013558967157780212, 0.0013545244072126796, 0.0013470719996079498, 0.0013688185314020636, 0.0013740527971020462, 0.018279683612268036, 0.02051290202106596, 0.0014195104500240817, 0.0013611149375459977, 0.0015316021234291245, 0.0013936073067883144, 0.0012986621634122363, 0.0013152108579988079, 0.001489874776167681, 0.0030534563880717878, 0.00140472255143508, 0.0018062665915990971, 0.001600234325005844, 0.0016190654084998735, 0.0014671354901463706, 0.0014621152243177807, 0.001409663469530642, 0.0013757792648346145, 0.0013564798971448019, 0.0013455021413689365, 0.0013443138165285392, 0.020171620529049968, 0.0013747169583921833, 0.001329036736005575, 0.0013342917331361345, 0.001365618345479728, 0.001322506694123149, 0.0013617437964837467, 0.0013303553058328678, 0.0013261792221467715, 0.042861287182729156, 0.0014496274691607272, 0.0013530389785918655, 0.0013571294685065442, 0.0013730212860759727, 0.0013277223880155658, 0.0013386682246108444, 0.0013229178374024983, 0.001314129530718284, 0.001345324798068982, 0.001335334633857164, 0.0013310291224672478, 0.05372406755174909, 0.001409271244929001, 0.00141757502154048, 0.0014011757543348536, 0.001520368386991322, 0.0015401221854536204, 0.007205132939092511, 0.001841100733442118, 0.001634632061444679, 0.0017177527746641819, 0.0015468583869918877]
[717.065104114409, 721.0871007777238, 731.2895170261154, 724.8870401014298, 732.0621794637096, 732.601235232769, 764.7363529100602, 718.5216091817092, 761.3416449540684, 770.5330902464249, 756.9996981150487, 735.6713370660334, 734.4593655599873, 728.8182436001108, 758.9192369167005, 768.8824296890244, 758.2744061779028, 751.4028928937098, 755.4998613646928, 748.9311489261756, 750.0580831361401, 755.4849516441346, 751.704420748943, 753.9421145629848, 722.7734230436648, 683.216922274649, 723.7777317185836, 721.0574635130214, 710.1157514157137, 727.6741543024609, 726.561469159871, 725.6435694398136, 730.3843648016968, 703.140345330688, 712.6130271607113, 684.3015460096065, 570.9825058980123, 739.0564520939542, 32.52806950787018, 759.3385073151612, 761.7382071436969, 766.7826976668937, 768.0291228146345, 771.133118454913, 774.9325196986222, 770.3098916877591, 674.9846910250527, 735.047164630501, 732.6108318740024, 730.6200890576968, 735.6495674137524, 736.1293550322529, 738.8673914291657, 737.1438461941427, 735.7436345157879, 768.3476765555962, 29.7093036675712, 748.5321630764008, 741.8981469606491, 753.7497788154385, 747.3250297423103, 747.7657248898016, 749.4206986199562, 748.1209909062839, 756.6368056859341, 754.9740376404535, 753.4937742091658, 48.38702694864189, 701.4935795640823, 759.3887923666208, 768.85002552097, 776.7739771136553, 773.0731969169221, 767.3428050407493, 773.9667043501521, 780.1467121593287, 772.5820660683455, 674.8552501073469, 770.3773376648792, 675.6966682718578, 720.5149969206312, 718.6408139172946, 717.8383758226333, 722.9250692104131, 713.2706146389891, 719.4567195807003, 21.170301087509724, 21.058058006700815, 707.2114849425445, 724.4956001708659, 721.5008054537656, 727.6698116391665, 727.0864528457568, 726.1397450234141, 732.58403943545, 722.9407590923748, 724.3257562040159, 723.853280923556, 20.933353448769278, 77.78232199139742, 712.7580845079825, 729.7269813103952, 739.5048340601397, 694.2193548282816, 703.7558746164233, 701.8773423494129, 706.8208928785477, 707.9802622057186, 741.2242343072111, 746.4911598003168, 749.7812777036837, 741.6277342319468, 752.2341166985588, 748.078982329284, 747.3516559552075, 748.2289924458072, 747.870222073501, 746.8609244903749, 297.257162822558, 739.5958701281903, 749.7533655990337, 741.639059837104, 751.8069184092485, 321.5313840265876, 707.5640721965071, 706.8761485836126, 700.4156368160366, 704.6363862117886, 700.5115234440555, 702.5447179443914, 698.4797181491818, 705.3952760784555, 748.2158526539359, 756.6916647884512, 758.2187081809345, 740.5735834516861, 748.1725088448019, 752.1418810963117, 749.8242110285158, 757.5847657530373, 743.1159364919821, 738.1835686666299, 740.8166813382904, 739.1230059695486, 741.5145834202735, 739.5882689629133, 746.9781256320329, 746.7643007135559, 747.4954100287199, 689.6192784644556, 679.1675162774212, 686.1240430657834, 689.7557182919053, 692.026861586162, 701.7926093394283, 694.4578497379661, 688.4343909857591, 697.6565319826258, 696.3531793631144, 699.7719741270165, 693.3052462567466, 698.7891777954476, 697.2402139074588, 539.3200648490836, 740.0884229587308, 728.469764300862, 683.9807513016115, 687.6398123372536, 745.5961553381874, 729.6945334550112, 735.9631214885944, 719.0328505362787, 554.4922944909488, 690.5754606424503, 792.2675599571031, 796.1493275412176, 793.9027618923818, 796.5161363114553, 690.9698237402193, 733.5044185303556, 809.9878316312846, 753.3629353137837, 815.1363567346915, 813.4604252664182, 754.894562101056, 746.308723834901, 747.3327005403099, 751.0441928734643, 746.4839272678169, 750.9433419701306, 744.3392692355958, 745.7774158572363, 789.7440672893562, 807.2393216039575, 156.62711076858815, 24.313991702725893, 606.8533056453522, 793.5704474777242, 796.3356856074029, 797.5358163223228, 796.6423831654606, 824.5961203259367, 820.3819851803593, 816.7950942041012, 821.3017240080036, 818.6334071377103, 814.4316487092773, 809.6921268549588, 808.3591735373975, 796.8570538046728, 34.198092996742545, 723.5842144419098, 729.0190509330455, 745.7847708119557, 747.71773373295, 735.0327521622087, 750.4614916129357, 754.9514261314295, 750.0913474225445, 746.117797207588, 748.3689101671858, 754.5291305182045, 752.4052406579721, 749.6008640334603, 790.7291678718876, 34.94853530870184, 150.94611270390698, 763.5323613676727, 769.1413214809198, 786.5855011392215, 785.4433555990023, 786.6921745346184, 760.4628953577165, 786.8877016197525, 774.7251272282798, 785.1708835243169, 191.11512647251607, 656.970364670152, 801.6770695611791, 810.3239647899687, 808.0483988181451, 811.6281458393828, 701.2735628869935, 800.7089906032962, 771.6805089223582, 683.6796293826876, 715.4364926694848, 209.99726611721894, 472.24556900027994, 783.8619399642215, 782.986038667211, 724.9690519652321, 757.8236084540551, 760.1683801730366, 744.6869171388549, 39.381508839977975, 691.528158282033, 728.2696588583566, 740.3669199008413, 738.5058979166039, 739.6169473172166, 717.1744630990474, 742.5114641160728, 689.1421040503818, 686.3674676400045, 684.758613101395, 675.2225631077702, 682.0275606468958, 21.627632895920854, 722.4946541674328, 676.5178532525921, 749.9185873231426, 750.3208800136043, 741.3259683766825, 737.4072919030885, 701.3699451976802, 701.7530496992443, 685.412128472585, 694.096540390271, 696.4669518528966, 34.947138783195896, 673.606630343377, 726.0120901088201, 733.5077995462303, 730.5465405159109, 726.8484741931106, 729.9527995547946, 732.1948368266305, 732.830612234487, 737.0485121176243, 715.3327483164977, 730.7003868496984, 753.1434710730699, 757.7000033396587, 759.1469029303869, 637.4657074210098, 682.9118951653547, 739.0941530643565, 738.4844058981497, 736.8072588946229, 732.776321029357, 707.6817139266647, 727.7684954782194, 731.2084571125577, 739.7807699000385, 38.48391117793552, 56.08961993649184, 645.9579914225377, 700.2106242608658, 699.4595873822392, 692.829971478318, 697.6499256946038, 696.1607633363394, 690.7282658987888, 696.6464431951877, 696.5808733482093, 695.4665511410012, 23.33017925919887, 589.977725340583, 727.3369910806389, 597.742507435316, 747.113375993487, 757.3775138987642, 753.8123254993606, 753.7920892274848, 743.5720020034959, 754.153058699388, 753.9260927381795, 746.5402925687985, 752.719668274054, 750.4524692804816, 746.3120982091148, 748.4135582488221, 750.5816204951789, 752.6795011453133, 696.2772441831938, 682.4406086471654, 85.0631260027009, 434.7494105736516, 710.7823866422054, 707.3895636931104, 718.6211582200889, 715.4484017772199, 711.6739531524621, 712.6774423393106, 630.3771574727393, 760.3422846521142, 38.03607244613455, 107.83350203529965, 678.7498733601223, 673.7545175427517, 677.1727477857033, 676.2935165325532, 680.5478713008354, 25.477130856854963, 751.6161549086137, 740.5909987000296, 743.9629494969162, 755.616574837068, 749.4992432405697, 755.217455071984, 732.89713547661, 669.5894290548292, 745.5289760418168, 768.4880983089463, 26.98523034378024, 47.56268259953732, 104.50490575810483, 712.3844464448056, 680.1777922885776, 707.7664730928867, 705.3486993750889, 698.9855221892884, 702.9773397334493, 698.297580318993, 696.2974186094023, 28.938996947066034, 201.50777740319617, 73.62012879865358, 716.5792281741288, 708.811171621382, 712.1219226712166, 704.695983578843, 718.8081713668123, 702.842072135874, 703.3319243160428, 703.3032138354355, 702.5363876603107, 203.83378308587183, 87.6609214949038, 97.7200512175279, 742.7784465346471, 739.7800560745285, 746.3751225700527, 745.426868052831, 749.9739224040293, 746.1222279474031, 739.8538890519554, 749.8380966562402, 750.0057439070697, 746.7344732396703, 745.4885057951409, 745.3464635890239, 727.8796081592476, 722.460129316147, 783.8682229917536, 774.4803718193558, 771.5727898258743, 772.1861685883451, 771.9977935415711, 726.4651147513209, 319.4883763765595, 673.3616122748555, 684.0628406483282, 715.6302859477801, 711.6662015374427, 695.2742112644877, 708.6923331579086, 714.1491827077158, 717.8302579632576, 719.3024499081819, 711.2691662471996, 692.3928470284989, 681.5656934859134, 656.5146890497388, 670.6302740447417, 672.8884686008787, 678.7096818619318, 681.5952823965198, 724.2012440144998, 728.5973201910001, 733.0529055160043, 673.1504153638049, 675.5146113024407, 681.8865030340552, 668.1483929147571, 671.6284110060487, 672.9156908588764, 737.6379216642435, 20.22679713158474, 350.95748499538183, 702.0976174796232, 709.8519541520079, 670.835787744895, 663.3539937758952, 646.0316335755697, 657.460449273695, 697.4864977775479, 689.261492908006, 679.3860954705707, 40.107265160226824, 39.237512348705295, 750.7678792118352, 759.8196342733576, 754.2232189953096, 749.1982773509461, 750.6688040889028, 745.0395570729384, 741.8271282393408, 738.3110215561569, 659.557498043168, 665.427595364606, 732.4722966238751, 640.8637481657646, 705.8009076677841, 748.096947817993, 67.1611256140757, 118.40101945767115, 754.010110909926, 750.6155162009603, 770.9188022558526, 778.3402090471922, 775.8405290606544, 773.2684927421785, 662.8365813688603, 658.1951315272742, 702.8626686946079, 61.47490624899158, 37.16936114367788, 733.5985297856142, 739.1100062440705, 789.0276467599786, 767.654798698646, 776.8354268916202, 668.4131257023396, 736.8616408112703, 775.8588816546198, 781.4458349378258, 784.0684581864206, 715.8667366990392, 777.4382969578985, 778.6772243702368, 780.7137818028498, 19.612802013110418, 689.9420354417388, 725.5955601053824, 705.8800507650541, 749.3984625586498, 703.5482755031447, 733.6546564655304, 738.5547536028541, 737.0829145282049, 743.0329433474878, 737.3772422403558, 730.9440898725308, 735.4108633942683, 727.9673838104248, 37.8390252795463, 97.29705792799497, 327.94571835980224, 677.1135895949775, 693.583127712765, 689.9323784867076, 693.0294507602885, 705.4764317822846, 695.4813487140855, 705.0112086312856, 704.1131474866191, 706.9021110288919, 741.4604895595205, 744.5399871829042, 720.9176310265867, 735.129265378322, 734.824068347989, 736.2235564723101, 722.7288405638302, 731.4372562675445, 741.0698693091828, 31.991967021405625, 749.9878469861472, 767.4134570109397, 775.5180409733376, 774.2229879100805, 781.5052851250008, 776.8414486699696, 782.6549108280467, 779.2853368016853, 737.0297092159703, 764.3348982091686, 781.4635933082851, 70.80673327201563, 708.324174630839, 751.9870226250752, 740.71369875058, 752.6503419115138, 758.5074741541399, 762.0537301596567, 17.06062064811445, 198.10995339857936, 737.5253310331371, 743.1743084427989, 744.8876868690668, 726.6165342059837, 728.7239984892162, 734.5424348976946, 620.3894616407302, 19.132135099577415, 649.505348412739, 678.0137668915829, 664.2290988629148, 747.1946608763548, 744.257134400641, 736.1339885082402, 742.0619364681297, 743.1614138135677, 721.8377899374641, 740.0796027741632, 745.8055216575477, 733.9341364189187, 700.0903318309233, 743.9889763435864, 738.3823051272233, 736.5907228247925, 741.0722687802098, 28.672434367908505, 736.7368348915845, 782.2140861938432, 789.0735534014884, 785.2266624604518, 739.6912300070128, 751.6746819354397, 747.4008449978603, 741.5697408460499, 744.1744611400642, 750.615298010069, 776.4665488400929, 729.166818984829, 791.9430885054177, 791.7491086486086, 738.8296020562829, 125.72402382666267, 65.57669219964498, 668.8258279794616, 711.8596633459437, 715.5578423327732, 702.3248360647369, 710.276400971174, 715.3351615105971, 714.8672785453607, 710.1803845219654, 23.190606799500223, 245.59178040161854, 664.7515398296773, 663.2067398326569, 682.2288740655362, 676.9615295165661, 728.6159866544357, 707.5795115737, 699.8364973030825, 714.1965120786887, 706.2184250070279, 708.3379568644428, 715.5080019157966, 710.9752024349276, 21.92448021506615, 414.0577119510905, 681.1314805984456, 683.7166055463291, 687.2404685706537, 692.6417404556274, 691.8192475211907, 692.7984788846794, 676.6195756711128, 657.881601810711, 692.0667307098947, 21.300726276780537, 689.4940414326645, 699.9669214975756, 703.8554287098152, 703.3953095613922, 765.3983580064455, 750.9670511879425, 763.2151604454275, 759.6733855832188, 768.1404568340603, 764.371681299608, 702.0015572708301, 738.0545899428059, 731.4916238157774, 736.4696283603636, 19.425572431646454, 360.97611832048, 680.0789816145659, 678.7294333494578, 697.2669922668176, 691.6900366283155, 701.4878158183326, 693.5372532950768, 750.2075897438764, 741.1026764736697, 749.187623813204, 31.8237034786573, 751.1922374271697, 744.4515655187834, 744.5671291797365, 782.3541634954325, 778.9633842707324, 778.1664775978205, 766.9247809585034, 781.4068041797293, 763.407764977659, 765.295898942397, 39.21222436297233, 700.9278481400669, 718.7208502086216, 729.0792640707147, 739.6050465763986, 751.4051746298488, 768.4838918407842, 731.5840951735739, 756.8361810316877, 699.6825844666528, 693.9266065840121, 682.7856663784003, 685.150939176853, 677.4572424999914, 17.749262496651045, 46.73830910185637, 737.2272372583285, 722.8511313038655, 734.359154936716, 742.3806780304965, 735.9135259722542, 732.1736124934334, 740.1979406256598, 743.025011661103, 742.3191708352886, 19.08048517410441, 725.834535060416, 715.0607319493562, 731.8905081445588, 724.8309474625197, 732.1706258506345, 727.3312158746182, 733.6460784782921, 144.02299470213276, 770.3615670670061, 776.8062397946167, 750.0726885293661, 765.7197461271606, 769.8876282513264, 756.8135857586848, 756.1486612917563, 378.80222433719064, 771.5221171272132, 73.40915041261346, 699.0612307446763, 717.0798479831619, 730.4760772044345, 730.2985535097812, 764.0734169865189, 776.3999630027133, 763.4648469900663, 765.5233516718655, 750.8148363546994, 697.2334168467108, 42.73649740198401, 777.0590968087555, 779.2491601930031, 778.2918211640672, 749.5456867655264, 751.1075110386173, 753.4327625511477, 780.915682842893, 17.997077850570104, 726.4500137651339, 732.1900337328019, 725.9985469431448, 728.9627853966197, 729.1864153409156, 718.3844639212034, 725.137245182612, 58.4883506525012, 69.5917125837548, 747.8820354805003, 736.9667240978417, 740.0389399918054, 749.4152985982452, 744.4542687144619, 743.8140139712983, 750.6589601327646, 741.1350049997104, 748.0807533115911, 742.0498666490571, 742.8652675878569, 748.3427835369649, 731.1167677285675, 745.7860314493726, 743.983733218252, 746.8242715602261, 743.2289230178276, 741.3275495430195, 746.7554346276398, 748.6266138376858, 739.0774985488134, 726.5066043191262, 706.2591924559729, 739.4448845636952, 736.6929398702534, 15.7590039763994, 682.2529248285326, 744.2588175846737, 767.2140560503578, 769.3159013075789, 767.1017069900744, 766.4034386322915, 765.9593893068165, 768.7116292945402, 27.85874980347596, 654.2888312147112, 717.4196978846987, 718.9197805809583, 709.7107995886402, 713.1332867122137, 704.7021246223471, 710.3851928154877, 705.5558593008216, 695.9354979232199, 21.761088009156822, 688.8940919406224, 714.3602950450136, 713.0037113437744, 722.6783386828953, 721.5831921647211, 701.4724914962042, 721.5378645909984, 714.2607993318496, 713.6414612125249, 20.89531043210454, 667.0038349233075, 665.9150344040579, 680.6825215362273, 687.6129280762052, 686.28705329043, 729.5871569329086, 737.5193024390463, 738.2665049630116, 742.3508173958325, 730.5570293351542, 727.7740725167589, 54.70554202201127, 48.74980629133013, 704.4682200000959, 734.6918121425773, 652.9110822601151, 717.5622538206867, 770.0232040121188, 760.3343554520035, 671.1973489290438, 327.4977182927722, 711.8843496734561, 553.6281325530662, 624.9084801979535, 617.6402724375037, 681.6003066630429, 683.9406247661484, 709.3891709721026, 726.8607875989557, 737.2022262216038, 743.2169516895626, 743.8739286205725, 49.57459905414438, 727.4224660540756, 752.4246493031516, 749.4612873375065, 732.269014479818, 756.1398399295226, 734.3525284140596, 751.6788903051362, 754.0458961355433, 23.331077196462438, 689.8324026509774, 739.0770079962663, 736.8493745113736, 728.3208280462649, 753.1694946370644, 747.0110828175543, 755.9048428611894, 760.9599941441196, 743.3149239762431, 748.8759556183028, 751.2983623877145, 18.613631572791125, 709.5866062678232, 705.4300370736634, 713.6863429917869, 657.7353281982624, 649.2991331758944, 138.78994439843748, 543.153333131535, 611.7584645416812, 582.1559509315877, 646.4715893900664]
Elapsed: 0.18412723346487242~0.4407774426246037
Time per graph: 0.003757698642140253~0.008995458012747015
Speed: 664.0656085953347~198.1800034677876
Total Time: 0.0781
best val loss: 0.6927490234375 test_score: 0.4898

Testing...
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6934 score: 0.4898 time: 0.07s
test Score 0.4898
Epoch Time List: [0.35466102801728994, 0.34879771899431944, 0.34707444615196437, 0.34521770698484033, 0.34669743082486093, 0.3471206119284034, 0.343686412088573, 0.3656709318747744, 0.33787984191440046, 0.33250794000923634, 0.3370203779777512, 0.3398236819775775, 0.3464504941366613, 0.34517315903212875, 0.3667381718987599, 0.33125817112158984, 0.3323449249146506, 0.3408403500216082, 0.3441269419854507, 0.3411552320467308, 0.3341121730627492, 0.33807753189466894, 0.33720761199947447, 0.3394647949608043, 0.35653441795147955, 0.3442439379869029, 0.35732928093057126, 0.3532685290556401, 0.35629380284808576, 0.3549065310508013, 0.3520958620356396, 0.35185827093664557, 0.36760184390004724, 0.35244769405107945, 0.35784620500635356, 0.37074892804957926, 0.38659041677601635, 0.36300201108679175, 8.828036851948127, 2.7458150800084695, 0.3362964190309867, 0.33267008315306157, 0.331307491986081, 0.3315737619996071, 0.3297655910719186, 0.3312179740751162, 0.3686258470406756, 0.3492577699944377, 0.3438410769449547, 0.34658168791793287, 0.3440385019639507, 0.3426663710270077, 0.3461243938654661, 0.34087420895230025, 0.34185926895588636, 0.4031606251373887, 3.901398337096907, 3.7006525680189952, 0.34928196715191007, 0.34322302299551666, 0.3422585019143298, 0.3405923709506169, 0.3408574990462512, 0.3410162559011951, 0.3380641491385177, 0.33812599815428257, 0.33915214508306235, 4.290065223001875, 4.519831335055642, 0.335048369015567, 0.3334391439566389, 0.33138746803160757, 0.32912706199567765, 0.3328175221104175, 0.3318322158884257, 0.33071281609591097, 0.3308352080639452, 0.3676125450292602, 0.3349223919212818, 0.33907663193531334, 0.33666958496905863, 0.3541397190419957, 0.35542208701372147, 0.3555855309823528, 0.35526176320854574, 0.35685451806057245, 3.1545566669665277, 12.287578499992378, 0.598222409025766, 0.3513539960840717, 0.3538803639821708, 0.35199203493539244, 0.35108029504772276, 0.35105320205911994, 0.35157027596142143, 0.3516533739166334, 0.3516105831367895, 0.35207372694276273, 2.621525263064541, 5.399288546061143, 0.44251861702650785, 0.34884328907355666, 0.34817383415065706, 0.3549481270601973, 0.36596053279936314, 0.3644844340160489, 0.3618638348998502, 0.3594078420428559, 0.34760552807711065, 0.340364454081282, 0.3410788901383057, 0.3439061460085213, 0.34186065604444593, 0.3417764969635755, 0.34174621501006186, 0.34179215086624026, 0.34136102988850325, 0.34787045896518975, 0.44024449004791677, 0.33989620790816844, 0.3385023099835962, 0.3426814441336319, 0.34195284405723214, 0.425064934999682, 0.35814284696243703, 0.35697474202606827, 0.36197494389489293, 0.35930154006928205, 0.3788835140876472, 0.35932089400012046, 0.36132341099437326, 0.3612116539152339, 0.39764694299083203, 0.33915310609154403, 0.33687887189444155, 0.3392181908711791, 0.3422145239310339, 0.3387857541674748, 0.3393261010060087, 0.33617230399977416, 0.33967701508663595, 0.33933186205103993, 0.34125961107201874, 0.3409789049765095, 0.34272445377428085, 0.34047149994876236, 0.3397202448686585, 0.3398545100353658, 0.33974844007752836, 0.34693713998422027, 0.381197799812071, 0.38554771477356553, 0.36673019907902926, 0.36545101401861757, 0.3637897199951112, 0.36320168210659176, 0.36247573094442487, 0.36218756902962923, 0.361196466954425, 0.3631137650227174, 0.36391218984499574, 0.3628383129835129, 0.36706941202282906, 0.3825780398910865, 0.3481142020318657, 0.3546405170345679, 0.3550061739515513, 0.3545771630015224, 0.3357837270013988, 0.3380249289330095, 0.3387682989705354, 0.33956126391422004, 0.3661978240124881, 0.37176543788518757, 0.3203948970185593, 0.31869441200979054, 0.33986712503246963, 0.3138645169092342, 0.3346203329274431, 0.3286459851078689, 0.3113959188340232, 0.3304017239715904, 0.3100027129985392, 0.3093413549941033, 0.31928359193261713, 0.33263275609351695, 0.33337060804478824, 0.33325628796592355, 0.33281554689165205, 0.3331297099357471, 0.3336211609421298, 0.3366746640531346, 0.33326759305782616, 0.3532231819117442, 0.5634481590241194, 9.961061165900901, 4.059816279099323, 0.3497970850439742, 0.3168137149186805, 0.3171656869817525, 0.3212459458736703, 0.3118877599481493, 0.3093031420139596, 0.31181659596040845, 0.3085173259023577, 0.3086395871359855, 0.312263744068332, 0.31117692100815475, 0.31233350303955376, 0.31456467893440276, 4.956505561014637, 0.45903625898063183, 0.3510071139317006, 0.3380434380378574, 0.3384699489688501, 0.3397538240533322, 0.3380662730196491, 0.3388734071049839, 0.33870814100373536, 0.3366962719010189, 0.34067399508785456, 0.3331900999182835, 0.3371780371526256, 0.33880184206645936, 0.3277260819450021, 8.113152488018386, 1.9501210818998516, 0.46185499406419694, 0.3248292059870437, 0.3932163800345734, 0.3201569520169869, 0.3216272038407624, 0.33032001298852265, 0.3208835310069844, 0.3239542560186237, 0.32531793287489563, 0.5144011869560927, 7.467358872992918, 0.3249490469461307, 0.3155611710390076, 0.31305517605505884, 0.31442567717749625, 0.3513366130646318, 0.33148740988690406, 0.3315334591316059, 0.34988717501983047, 0.3477314949268475, 4.9399193349527195, 9.985936373937875, 0.32973977213259786, 0.3256427019368857, 0.3366663680644706, 0.33948356297332793, 0.3397665120428428, 0.3431476909900084, 1.5159702419769019, 6.419942886917852, 0.3516977148829028, 0.3466801369795576, 0.34838328312616795, 0.3479927530279383, 0.3498027571476996, 0.34609429095871747, 0.35503467614762485, 0.36935532419010997, 0.37184933898970485, 0.3728847870370373, 0.3731165879871696, 2.684537003049627, 6.445616625947878, 0.34950807900168, 0.3410238070646301, 0.3374244279693812, 0.34410519106313586, 0.3375959969125688, 0.4561329890275374, 0.36118795396760106, 0.36618597502820194, 0.36344826384447515, 0.4004437478724867, 4.992058176081628, 3.215855870046653, 0.3496838129358366, 0.3464537840336561, 0.3466708419146016, 0.34613776800688356, 0.3470766111277044, 0.3491375989979133, 0.34660141298081726, 0.34589115297421813, 0.34669062204193324, 0.34663602604996413, 0.34190760005731136, 0.3331450499827042, 0.335598828853108, 6.098439085064456, 0.37116509792394936, 0.36576180695556104, 0.34558180207386613, 0.34470887295901775, 0.3459386290051043, 0.3470731768757105, 0.34721967088989913, 0.34128599101677537, 0.343607616960071, 3.844389762962237, 3.3326252789702266, 1.4211816960014403, 0.3643488270463422, 0.3602919310797006, 0.36355860612820834, 0.3613976330962032, 0.3612228808924556, 0.36524520604871213, 0.3621863720472902, 0.3635731649119407, 0.3648919730912894, 5.473455515806563, 3.5531620089896023, 0.3514135121367872, 0.3800099330255762, 0.36015338893048465, 0.33959471993148327, 0.3397431130288169, 0.33975910407025367, 0.3426823350600898, 0.34006670711096376, 0.3412776459008455, 0.3416764280991629, 0.34124018799047917, 0.3422623658552766, 0.34405090694781393, 0.34693699097260833, 0.3537589539773762, 0.34833779197651893, 0.35007382498588413, 0.3688241910422221, 6.809974278905429, 3.1992494580335915, 0.3662064168602228, 0.3826217890018597, 0.3543688071658835, 0.35450176801532507, 0.35801208310294896, 0.3604542230023071, 0.4584904379444197, 0.3355760750127956, 2.4055189180653542, 6.347519895993173, 0.38292378187179565, 0.37354764400515705, 0.3777542740572244, 0.37428380304481834, 0.37423605006188154, 2.5739918801700696, 11.136254275101237, 0.33672231796663254, 0.3385464991442859, 0.3336212339345366, 0.33810531499329954, 0.33597082388587296, 0.33941807597875595, 0.35075804113876075, 0.36173380981199443, 0.35559801710769534, 3.8162197939818725, 2.9574019338469952, 1.5338329090736806, 0.3572758799418807, 0.35936656105332077, 0.3619653369532898, 0.3589975112117827, 0.35732368589378893, 0.3570430971449241, 0.3574551399797201, 0.3608187170466408, 4.208252870943397, 2.2303703060606495, 2.206689973012544, 0.3534953958587721, 0.3496341039426625, 0.35037629294674844, 0.35370420396793634, 0.3519219630397856, 0.35847331990953535, 0.3575371300103143, 0.35610227612778544, 0.35631770407781005, 0.5279968309914693, 8.909550536074676, 3.0472395459655672, 0.346198390936479, 0.33812695217784494, 0.34086659003514796, 0.33640856202691793, 0.33655855990946293, 0.33529733296018094, 0.3380739811109379, 0.3351118649588898, 0.33727531589102, 0.3351021039998159, 0.3372016418725252, 0.33605773001909256, 0.33925605192780495, 0.3465259480290115, 0.3398223698604852, 0.3270635559456423, 0.326839841902256, 0.32577295508235693, 0.3283196260454133, 0.32961911300662905, 2.9753516050986946, 0.3765624179504812, 0.37266999401617795, 0.36045482114423066, 0.3556120719294995, 0.36354933003894985, 0.35505368805024773, 0.35414966009557247, 0.3542448189109564, 0.353305886965245, 0.35694288392551243, 0.3602205158676952, 0.3744082519551739, 0.3751410919940099, 0.3718772779684514, 0.37055704987142235, 0.3731082559097558, 0.37022183591034263, 0.35501885402482003, 0.3477712390013039, 0.34695538599044085, 0.35814001597464085, 0.37700060207862407, 0.3686951401177794, 0.3753421929432079, 0.3740563109749928, 0.3734795100754127, 0.3815944930538535, 2.8708673949586228, 2.5403411649167538, 0.37126023101154715, 0.3583314560819417, 0.3671138419304043, 0.3735612710006535, 0.36994569399394095, 0.3764930759789422, 0.35945552913472056, 0.3610615929355845, 0.36339220905210823, 3.721769012976438, 9.577239054022357, 0.34770897100679576, 0.33579093299340457, 0.33599921909626573, 0.33621648396365345, 0.3403955219546333, 0.3414374788990244, 0.34023231000173837, 0.3410611590370536, 0.34963795309886336, 6.012583700125106, 0.3443752779858187, 0.3494429829297587, 0.394994736998342, 0.34985430887900293, 1.0026647709310055, 11.032310905051418, 0.3469322600867599, 0.3377396789146587, 0.3312435980187729, 0.3304609680781141, 0.32765671086963266, 0.3259010559413582, 0.374714974896051, 0.3754503910895437, 0.3661917260615155, 2.2400190300541, 8.133808589889668, 0.37131143792066723, 0.35023288091178983, 0.3402219822164625, 0.32791094994172454, 0.3389467769302428, 0.34394204698037356, 0.34032701898831874, 0.32275442592799664, 0.32268966105766594, 0.32098556717392057, 0.35164952592458576, 0.3332819779170677, 0.32412342191673815, 0.3190581798553467, 5.037316430127248, 4.943052515969612, 0.35687855596188456, 0.35384499398060143, 0.34075166401453316, 0.3552014500601217, 0.3404121899511665, 0.3447269609896466, 0.3434298379579559, 0.342253998038359, 0.3434770068852231, 0.3445491259917617, 0.3438583791721612, 0.3477933070389554, 3.958188163000159, 2.044408515910618, 1.4214134220965207, 1.1197641220642254, 0.3710366659797728, 0.3671548960264772, 0.3672111810883507, 0.3603573850123212, 0.3614869370358065, 0.36270310601685196, 0.36109838902484626, 0.35993031703401357, 0.3565416340716183, 0.3456794270314276, 0.3466373310657218, 0.3495594068663195, 0.3452714978484437, 0.345770665910095, 0.3506708990316838, 0.3492955759866163, 0.38437382807023823, 2.49600282299798, 6.283647094969638, 0.3318392470246181, 0.3301904440158978, 0.33112334995530546, 0.3262701010098681, 0.32852982403710485, 0.3274373939493671, 0.33014452701900154, 0.3359497310593724, 0.3304560160031542, 0.32703987101558596, 6.9355597399408, 0.4313729549758136, 0.3415411659516394, 0.3376557899173349, 0.340548946056515, 0.3390649009961635, 0.33510708902031183, 5.9864561159629375, 8.58530032786075, 0.35590433108154684, 0.34466857789084315, 0.3422566739609465, 0.3509080717340112, 0.34697955509182066, 0.3467403131071478, 0.3720713489456102, 3.0805517341941595, 5.0109451349126175, 0.3537109608296305, 0.35988326999358833, 0.34610274515580386, 0.343524924130179, 0.344766044174321, 0.34323143505025655, 0.34643908799625933, 0.3471700489753857, 0.3419534619897604, 0.3430945680011064, 0.36317118583247066, 0.3646984670776874, 0.3441412370884791, 0.3436664171749726, 0.3436224309261888, 0.3403622079640627, 6.061206821119413, 1.5923514199675992, 0.3233986860141158, 0.32173564401455224, 0.3205642600078136, 0.32954564807005227, 0.33719104109331965, 0.4361002161167562, 0.33404941693879664, 0.33791214879602194, 0.3353727828944102, 0.3827096540480852, 0.3402253311360255, 0.32627502689138055, 0.3255485659465194, 0.34423125092871487, 3.15313569502905, 4.366754961083643, 1.8016857729526237, 0.3594835699768737, 0.3581790248863399, 0.3599916729144752, 0.36039965506643057, 0.3554761150153354, 0.356321377097629, 0.3567397288279608, 2.4010210600681603, 4.33055788197089, 0.4439016890246421, 0.3779781301273033, 0.37357520207297057, 0.3734155319398269, 0.3589896430494264, 0.3616075289901346, 0.3625420810421929, 0.35882300103548914, 0.35819062287919223, 0.3566190211568028, 0.35603526304475963, 0.3582183290272951, 7.7048700619488955, 4.108974049100652, 0.6073998269857839, 0.37087830307427794, 0.3701708111912012, 0.3668948949780315, 0.36799270287156105, 0.3661339631071314, 0.3662446920061484, 0.38942060514818877, 0.3931120560737327, 3.334202175028622, 2.6087780668167397, 0.36420102301053703, 0.3608195760753006, 0.3610902400687337, 0.35062747297342867, 0.33644134597852826, 0.3369904700666666, 0.3356727068312466, 0.33751439105253667, 0.3366888299351558, 0.37016356410458684, 0.36201099096797407, 0.34478228411171585, 0.3441518390318379, 2.9631890980526805, 4.241927940165624, 0.3751391270197928, 0.3819432408781722, 0.3654171519447118, 0.3645374459447339, 0.36142101406585425, 0.38073789002373815, 0.34637335187289864, 0.3380340279545635, 0.33545792009681463, 2.849907591124065, 5.436297988053411, 0.33644099987577647, 0.3347772470442578, 0.3288615520577878, 0.36177430604584515, 0.32476106099784374, 0.32777081697713584, 0.32440934085752815, 0.32718251703772694, 0.32897788390982896, 4.428870304953307, 1.0887283169431612, 0.3516640510642901, 0.349619589978829, 0.3440272267907858, 0.34179178380873054, 0.3337915779557079, 0.3336407449096441, 0.3360260840272531, 0.34555249521508813, 0.3650512979365885, 0.36459543101955205, 0.36419683403801173, 0.3661816450767219, 7.364501175004989, 7.51874090207275, 1.1129047040594742, 0.3443766199052334, 0.3426252199569717, 0.3417117960052565, 0.34478468098677695, 0.343149948050268, 0.34245666197966784, 0.3414590619504452, 0.3417076071491465, 5.414116673870012, 5.294785809004679, 0.3518875549780205, 0.3487880389438942, 0.34683936601504683, 0.34720263886265457, 0.3459662700770423, 0.34200937394052744, 2.895012697088532, 2.7407504779985175, 0.32338713807985187, 0.3605498820543289, 0.33306918188463897, 0.3304050149163231, 0.334542035125196, 0.33704199199564755, 0.40744128613732755, 0.3253040869021788, 0.9306352860294282, 6.556501811021008, 0.3551014281110838, 0.3525343640940264, 0.349847940960899, 0.34006018901709467, 0.32744138687849045, 0.3277115050004795, 0.327443263027817, 0.3491881008958444, 0.38470455701462924, 4.12980349897407, 2.9083500429987907, 0.3224008629331365, 0.32459921983536333, 0.33225439791567624, 0.33547518588602543, 0.3357807390857488, 0.33270941907539964, 6.334462692029774, 5.275078818900511, 0.3377383011393249, 0.3487271830672398, 0.3473516749218106, 0.3477320648962632, 0.34690067591145635, 0.34715981408953667, 1.121183184091933, 10.139684263966046, 0.38726955000311136, 0.3437836499651894, 0.34207004110794514, 0.34305531601421535, 0.3435706009622663, 0.34174654714297503, 0.34222467988729477, 0.3414522660896182, 0.34076833410654217, 0.34181723196525127, 0.34051654709037393, 0.3398730930639431, 0.34146700403653085, 0.3379866679897532, 0.33830534608568996, 0.33875095611438155, 0.33897834294475615, 0.33842427795752883, 0.3384116970701143, 0.33677690604235977, 0.33980265888385475, 0.3420995681080967, 0.3455652929842472, 0.34499681496527046, 0.3912027641199529, 3.461057608947158, 5.39230818010401, 0.34015338809695095, 0.3293878921540454, 0.3290559609886259, 0.3299276860198006, 0.32981252600438893, 0.32918683090247214, 0.32849559315945953, 4.35589574999176, 1.6964873319957405, 0.36334540508687496, 0.3562976310495287, 0.361099852132611, 0.35956929193343967, 0.3648675160948187, 0.35821378882974386, 0.3585993030574173, 0.38043103099334985, 2.696468200883828, 3.3247096120612696, 0.36731807608157396, 0.35572582797612995, 0.3526992261176929, 0.3513243329944089, 0.3581092129461467, 0.3511536280857399, 0.3543191631324589, 0.3591798839624971, 2.923498790129088, 5.722741577075794, 0.3813949559116736, 0.3765489789657295, 0.37447208003140986, 0.377972193993628, 0.3578819171525538, 0.34961994690820575, 0.35215169994626194, 0.3454073560424149, 0.3457385399378836, 0.350276364129968, 2.076760584837757, 8.956783503876068, 1.896469717961736, 0.3816635940456763, 0.3735977441538125, 0.3499118429608643, 0.3334439860191196, 0.3330630719428882, 7.32278688903898, 0.43420600495301187, 0.35609145904891193, 0.40101046196650714, 0.443050246918574, 0.3463883940130472, 4.9956769129494205, 0.37140874203760177, 0.35919431888032705, 0.35474381409585476, 0.34977425599936396, 0.3421172689413652, 0.33921320200897753, 3.967183263041079, 0.4455843560863286, 0.34073346911463886, 0.33882416086271405, 0.33950615383218974, 0.3444180120714009, 0.34013334894552827, 0.33714351803064346, 0.3396126660518348, 5.229480785899796, 5.30242631805595, 0.3475636028451845, 0.3452491380739957, 0.34518174186814576, 0.34383456595242023, 0.341959115001373, 0.34028134401887655, 0.3373318569501862, 0.3377450780244544, 0.3379881370346993, 0.3400914069497958, 2.9042289000935853, 4.646248901030049, 0.36101389001123607, 0.3593963279854506, 0.33785813907161355, 0.34238847403321415, 0.6202701010042801, 6.2077620279742405, 0.38065517507493496, 0.35905389906838536, 0.3509851590497419]
Total Epoch List: [846, 7]
Total Time List: [0.07254559500142932, 0.07814370095729828]
========================k_idx:2========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: AEtransGAT
AEtransGAT(
  (gae): GAE(
    (encoder): transGAT(
      (encoder): EdgeFeatureGAT()
      (classifier): Sequential(
        (0): Linear(in_features=64, out_features=64, bias=True)
        (1): ReLU(inplace=True)
        (2): Linear(in_features=64, out_features=2, bias=True)
      )
    )
    (decoder): InnerProductDecoder()
  )
)
Total number of parameters: 1914242
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x75feb28b0610>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 2.2605;  Loss pred: 2.2605; Loss self: 0.0000; time: 2.83s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6935 score: 0.5102 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6939 score: 0.5000 time: 0.07s
Epoch 2/1000, LR 0.000000
Train loss: 2.2901;  Loss pred: 2.2901; Loss self: 0.0000; time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6935 score: 0.5102 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6939 score: 0.5000 time: 0.06s
Epoch 3/1000, LR 0.000030
Train loss: 2.2548;  Loss pred: 2.2548; Loss self: 0.0000; time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6935 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6939 score: 0.5000 time: 0.07s
     INFO: Early stopping counter 1 of 2
Epoch 4/1000, LR 0.000060
Train loss: 2.2715;  Loss pred: 2.2715; Loss self: 0.0000; time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6935 score: 0.5102 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6938 score: 0.5000 time: 0.06s
     INFO: Early stopping counter 2 of 2
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 001,   Train_Loss: 2.2901,   Val_Loss: 0.6935,   Val_Precision: 0.5102,   Val_Recall: 1.0000,   Val_accuracy: 0.6757,   Val_Score: 0.5102,   Val_Loss: 0.6935,   Test_Precision: 0.5000,   Test_Recall: 1.0000,   Test_accuracy: 0.6667,   Test_Score: 0.5000,   Test_loss: 0.6939


[0.06833410204853863, 0.0679529559565708, 0.06700492603704333, 0.06759673892520368, 0.06693420500960201, 0.06688495411071926, 0.06407436996232718, 0.06819558294955641, 0.06436006794683635, 0.0635923370718956, 0.06472922000102699, 0.06660582998301834, 0.06671573990024626, 0.0672321260208264, 0.06456550001166761, 0.06372885906603187, 0.06462040601763874, 0.06521135393995792, 0.06485772202722728, 0.06542657501995564, 0.0653282740386203, 0.06485900201369077, 0.06518519599922001, 0.06499172688927501, 0.06779441307298839, 0.07171953504439443, 0.06770034204237163, 0.06795574899297208, 0.06900283496361226, 0.06733783206436783, 0.06744095589965582, 0.0675262650474906, 0.06708796403836459, 0.06968736799899489, 0.06876102194655687, 0.07160585897509009, 0.08581699000205845, 0.06630075396969914, 1.5063912719488144, 0.06452985003124923, 0.06432656198740005, 0.06390337203629315, 0.06379966402892023, 0.06354285508859903, 0.06323131208773702, 0.06361076305620372, 0.07259423902723938, 0.06666238897014409, 0.0668840779690072, 0.06706631905399263, 0.06660780101083219, 0.0665643879910931, 0.06631771894171834, 0.06647277902811766, 0.0665992849972099, 0.06377321295440197, 1.6493149939924479, 0.06546144897583872, 0.06604680197779089, 0.06500831095036119, 0.06556718703359365, 0.065528545062989, 0.0653838359285146, 0.06549742701463401, 0.06476026494055986, 0.06490289408247918, 0.06503039796371013, 1.0126681280089542, 0.06985096004791558, 0.06452557700686157, 0.06373154500033706, 0.06308141292538494, 0.06338339007925242, 0.06385672697797418, 0.06331021699588746, 0.06280869897454977, 0.06342368293553591, 0.07260816299822181, 0.06360519398003817, 0.07251774694304913, 0.06800691201351583, 0.06818427098914981, 0.06826049101073295, 0.06778019201010466, 0.06869762891437858, 0.06810694606974721, 2.3145632080268115, 2.3269002290908247, 0.06928620510734618, 0.06763326097279787, 0.0679139920976013, 0.06733823393005878, 0.06739226100035012, 0.06748012395109981, 0.06688652408774942, 0.06777872098609805, 0.06764911999925971, 0.06769327609799802, 2.340762081905268, 0.6299631940200925, 0.06874702800996602, 0.06714840105269104, 0.06626055401284248, 0.07058287796098739, 0.06962641701102257, 0.06981276790611446, 0.06932449294254184, 0.06921096902806312, 0.0661068509798497, 0.06564042903482914, 0.06535239203367382, 0.06607088400050998, 0.06513929495122284, 0.06550110504031181, 0.06556485104374588, 0.06548797292634845, 0.06551938899792731, 0.06560793099924922, 0.16484043491072953, 0.0662523980718106, 0.06535482499748468, 0.0660698750289157, 0.06517630896996707, 0.1523956989403814, 0.06925167900044471, 0.06931907392572612, 0.06995846098288894, 0.06953941204119474, 0.06994888500776142, 0.06974644993897527, 0.07015235908329487, 0.0694645990151912, 0.06548912299331278, 0.06475556991063058, 0.06462515296880156, 0.06616493093315512, 0.06549291696865112, 0.06514728302136064, 0.06534865009598434, 0.06467923091258854, 0.06593856704421341, 0.06637915293686092, 0.06614321900997311, 0.06629478395916522, 0.06608096603304148, 0.06625307898502797, 0.065597637090832, 0.06561642000451684, 0.06555224198382348, 0.07105369807686657, 0.0721471490105614, 0.07141565799247473, 0.07103964302223176, 0.07080650003626943, 0.06982119695749134, 0.07055863796267658, 0.0711759909754619, 0.0702351339859888, 0.0703665919136256, 0.07002281001769006, 0.07067594002000988, 0.07012129202485085, 0.07027707097586244, 0.09085514000616968, 0.0662083049537614, 0.0672642879653722, 0.07163944293279201, 0.07125823595561087, 0.06571922299917787, 0.06715138698928058, 0.06657942302990705, 0.06814709503669292, 0.08836912701372057, 0.07095531595405191, 0.06184779293835163, 0.06154624302871525, 0.06172040500678122, 0.06151789997238666, 0.07091481902170926, 0.06680259690620005, 0.06049473595339805, 0.06504169199615717, 0.06011264200787991, 0.060236489051021636, 0.06490972707979381, 0.06565647490788251, 0.06556651403661817, 0.06524249899666756, 0.06564106501173228, 0.06525126099586487, 0.06583019602112472, 0.06570325000211596, 0.062045417027547956, 0.06070071004796773, 0.3128449459327385, 2.015300514991395, 0.08074439002666622, 0.06174625095445663, 0.0615318400086835, 0.06143924698699266, 0.061508151004090905, 0.05942303000483662, 0.05972827400546521, 0.05999056599102914, 0.059661387000232935, 0.05985585192684084, 0.06016465602442622, 0.06051682902034372, 0.06061661895364523, 0.06149158091284335, 1.4328284329967573, 0.06771844800096005, 0.06721360702067614, 0.06570260203443468, 0.06553275091573596, 0.06666369608137757, 0.06529315700754523, 0.06490483798552305, 0.06532537692692131, 0.06567327596712857, 0.06547572906129062, 0.06494116398971528, 0.06512447993736714, 0.06536812102422118, 0.061968120047822595, 1.402061619097367, 0.32461915793828666, 0.0641754069365561, 0.06370740802958608, 0.062294562929309905, 0.06238514801952988, 0.062286115949973464, 0.06443443894386292, 0.06227063899859786, 0.06324823899194598, 0.0624067970784381, 0.2563899619271979, 0.0745847950456664, 0.06112186796963215, 0.06046964193228632, 0.06063993205316365, 0.06037247506901622, 0.06987287499941885, 0.06119576597120613, 0.06349778105504811, 0.07167099602520466, 0.06848965701647103, 0.23333637102041394, 0.10375957598444074, 0.06251100800000131, 0.06258093705400825, 0.06758909206837416, 0.06465884600766003, 0.06445940304547548, 0.06579946400597692, 1.2442387669580057, 0.07085756293963641, 0.06728277006186545, 0.06618339999113232, 0.06635018100496382, 0.06625051004812121, 0.06832368206232786, 0.06599224708043039, 0.07110289693810046, 0.07139032997656614, 0.07155806303489953, 0.07256866502575576, 0.07184460398275405, 2.265620109043084, 0.0678205710137263, 0.07242972194217145, 0.0653404260519892, 0.0653053930727765, 0.06609777896665037, 0.06644903100095689, 0.06986327306367457, 0.0698251329595223, 0.07148983504157513, 0.07059536699671298, 0.0703550970647484, 1.4021176469977945, 0.07274275191593915, 0.06749198900070041, 0.06680228898767382, 0.06707307102624327, 0.06741432601120323, 0.06712762801907957, 0.0669220780255273, 0.0668640190269798, 0.0664813769981265, 0.06849959003739059, 0.06705894903279841, 0.0650606449926272, 0.06466939393430948, 0.0645461370004341, 0.07686687994282693, 0.07175156904850155, 0.06629737198818475, 0.06635211198590696, 0.06650314503349364, 0.06686897296458483, 0.06924016692209989, 0.0673291030107066, 0.06701235403306782, 0.06623583904001862, 1.2732593569671735, 0.8736019259085879, 0.07585632603149861, 0.06997894390951842, 0.0700540830148384, 0.07072442304342985, 0.07023579906672239, 0.07038604095578194, 0.07093961897771806, 0.07033697003498673, 0.070343590923585, 0.07045630004722625, 2.1002839050488546, 0.0830539830494672, 0.0673690470866859, 0.08197509695310146, 0.0655857619130984, 0.06469692999962717, 0.06500291696283966, 0.06500466202851385, 0.06589812401216477, 0.06497354805469513, 0.06499310804065317, 0.06563610897865146, 0.06509727600496262, 0.06529394199606031, 0.06565617804881185, 0.06547182297799736, 0.06528270698618144, 0.06510074995458126, 0.07037426601164043, 0.07180111994966865, 0.5760427849600092, 0.11270860594231635, 0.06893811793997884, 0.06926876294892281, 0.06818613596260548, 0.06848851696122438, 0.06885175406932831, 0.06875480699818581, 0.07773124298546463, 0.06444465997628868, 1.2882507800823078, 0.45440423500258476, 0.07219154201447964, 0.0727267850888893, 0.07235967507585883, 0.07245374796912074, 0.07200081297196448, 1.92329349310603, 0.06519285100512207, 0.0661633750423789, 0.06586349499411881, 0.06484770402312279, 0.06537698395550251, 0.06488197494763881, 0.066857949947007, 0.0731791720027104, 0.06572514493018389, 0.06376156001351774, 1.8158081059809774, 1.0302194351097569, 0.468877510051243, 0.06878308509476483, 0.07203998800832778, 0.06923187500797212, 0.06946918601170182, 0.07010159501805902, 0.06970352702774107, 0.07017065701074898, 0.07037222699727863, 1.693216944928281, 0.24316679302137345, 0.6655788409989327, 0.06838043592870235, 0.06912983593065292, 0.06880844198167324, 0.06953353097196668, 0.06816839589737356, 0.06971694203093648, 0.06966838601510972, 0.06967123004142195, 0.06974727695342153, 0.2403919470962137, 0.5589719930430874, 0.5014324019430205, 0.06596852699294686, 0.0662359029520303, 0.06565063400194049, 0.06573414790909737, 0.0653356050606817, 0.06567288597580045, 0.06622929300647229, 0.06534743995871395, 0.06533283297903836, 0.06561904097907245, 0.06572871294338256, 0.0657412389991805, 0.06731882505118847, 0.06782381201628596, 0.06251050694845617, 0.06326822703704238, 0.06350664596538991, 0.06345620006322861, 0.06347168399952352, 0.06744990090373904, 0.15337021194864064, 0.0727692210348323, 0.07163084601052105, 0.0684711099602282, 0.06885250401683152, 0.07047579099889845, 0.06914142810273916, 0.0686131149996072, 0.06826126296073198, 0.0681215530494228, 0.06889093795325607, 0.07076907309237868, 0.07189328991807997, 0.07463656307663769, 0.07306559500284493, 0.07282038894481957, 0.07219581701792777, 0.07189016893971711, 0.06766075093764812, 0.0672525119734928, 0.0668437429703772, 0.07279205194208771, 0.0725372910965234, 0.07185946602839977, 0.07333700195886195, 0.07295700896065682, 0.07281744305510074, 0.06642825505696237, 2.4225288700545207, 0.13961805088911206, 0.06979086494538933, 0.06902847799938172, 0.07304321101401001, 0.07386704604141414, 0.07584767905063927, 0.0745291979983449, 0.07025225600227714, 0.07109058101195842, 0.07212393707595766, 1.2217237900476903, 1.2488049590028822, 0.06526651093736291, 0.0644889889517799, 0.06496750400401652, 0.06540324701927602, 0.06527512497268617, 0.06576831999700516, 0.0660531249595806, 0.06636769406031817, 0.07429223402868956, 0.07363686198368669, 0.06689672800712287, 0.07645931001752615, 0.06942467694170773, 0.06549953203648329, 0.729588724905625, 0.41384778800420463, 0.06498586595989764, 0.06527975900098681, 0.06356052006594837, 0.06295447598677129, 0.06315730896312743, 0.06336738204117864, 0.07392470689956099, 0.07444600795861334, 0.06971489905845374, 0.7970731960376725, 1.3182900779647753, 0.06679402699228376, 0.06629594997502863, 0.06210175296291709, 0.06383077404461801, 0.06307642301544547, 0.07330795598682016, 0.06649823696352541, 0.06315581500530243, 0.06270428199786693, 0.06249454303178936, 0.06844849395565689, 0.06302750995382667, 0.06292722898069769, 0.06276307802181691, 2.4983681560261175, 0.0710204589413479, 0.06753073295112699, 0.06941689306404442, 0.06538577598985285, 0.06964696198701859, 0.06678891705814749, 0.0663457919145003, 0.06647827406413853, 0.0659459320595488, 0.06645173893775791, 0.06703659100458026, 0.06662942096590996, 0.06731070799287409, 1.2949593610828742, 0.5036123500904068, 0.14941497100517154, 0.07236599700991064, 0.07064762397203594, 0.07102145301178098, 0.07070406596176326, 0.06945660803467035, 0.07045480096712708, 0.06950244109611958, 0.06959108798764646, 0.06931652803905308, 0.06608578702434897, 0.06581244908738881, 0.06796893000137061, 0.06665494397748262, 0.06668262800667435, 0.06655587092973292, 0.06779859506059438, 0.06699139205738902, 0.06612062104977667, 1.531634487095289, 0.06533439201302826, 0.06385084800422192, 0.06318357202690095, 0.0632892600260675, 0.06269951199647039, 0.0630759340710938, 0.06260741397272795, 0.0628781239502132, 0.0664830730529502, 0.06410802400205284, 0.06270285707432777, 0.6920245820656419, 0.06917736504692584, 0.06516069895587862, 0.06615241500549018, 0.06510327209252864, 0.0646005499875173, 0.0642999280244112, 2.8721112209605053, 0.247337396023795, 0.06643839599564672, 0.06593338795937598, 0.06578172906301916, 0.06743584503419697, 0.06724082108121365, 0.06670819502323866, 0.07898264401592314, 2.5611360020702705, 0.07544202695135027, 0.07226991897914559, 0.0737697280710563, 0.06557862705085427, 0.06583746091928333, 0.06656396901234984, 0.06603222398553044, 0.06593453197274357, 0.06788228696677834, 0.06620909401681274, 0.06570077396463603, 0.0667634840356186, 0.06999096798244864, 0.06586119090206921, 0.06636128691025078, 0.066522695007734, 0.06612040696199983, 1.708958485047333, 0.06650950200855732, 0.06264269701205194, 0.06209814001340419, 0.062402363982982934, 0.0662438569124788, 0.06518777494784445, 0.06556053599342704, 0.06607605097815394, 0.06584477506112307, 0.06527977797668427, 0.06310638890136033, 0.0671999859623611, 0.06187313294503838, 0.06188829196617007, 0.06632111093495041, 0.3897425369359553, 0.7472167069790885, 0.07326272095087916, 0.06883379200007766, 0.06847804202698171, 0.06976828596089035, 0.06898722797632217, 0.06849935895297676, 0.0685441920068115, 0.06899655505549163, 2.1129244449548423, 0.199518078006804, 0.07371175102889538, 0.07388344698119909, 0.07182340393774211, 0.07238225196488202, 0.06725078902672976, 0.0692501679295674, 0.07001635408960283, 0.06860856804996729, 0.06938363297376782, 0.06917601905297488, 0.06848281202837825, 0.0689194219885394, 2.234944660915062, 0.11834099108818918, 0.07193912100046873, 0.07166711997706443, 0.07129964290652424, 0.07074364298023283, 0.07082774897571653, 0.07072763796895742, 0.07241883291862905, 0.07448148704133928, 0.07080242095980793, 2.300391045981087, 0.07106660399585962, 0.07000330800656229, 0.06961656897328794, 0.06966210796963423, 0.06401895103044808, 0.06524920091032982, 0.06420207896735519, 0.06450140406377614, 0.06379041692707688, 0.06410493899602443, 0.06980041495990008, 0.06639075302518904, 0.06698641297407448, 0.0665336330421269, 2.522448188974522, 0.1357430520001799, 0.07205045490991324, 0.07219371607061476, 0.07027437200304121, 0.07084097992628813, 0.06985153397545218, 0.0706522969994694, 0.06531525496393442, 0.06611769401933998, 0.0654041770612821, 1.5397327980026603, 0.0652296410407871, 0.0658202659105882, 0.0658100500004366, 0.06263148109428585, 0.06290411204099655, 0.0629685310414061, 0.06389153306372464, 0.06270741403568536, 0.06418588105589151, 0.06402752199210227, 1.249610314029269, 0.06990733800921589, 0.06817667803261429, 0.06720805598888546, 0.06625157606322318, 0.06521115591749549, 0.06376190902665257, 0.06697794597130269, 0.06474320497363806, 0.07003175595309585, 0.07061265490483493, 0.07176483399234712, 0.07151708798483014, 0.0723292879993096, 2.7606780850328505, 1.0483905160799623, 0.0664652599953115, 0.06778712500818074, 0.0667248439276591, 0.06600387301295996, 0.06658391002565622, 0.06692401797045022, 0.06619850895367563, 0.06594663602299988, 0.06600934197194874, 2.568068869994022, 0.06750849902164191, 0.06852564797736704, 0.06694990501273423, 0.06760197004768997, 0.0669242909643799, 0.06736958201508969, 0.0667896979721263, 0.3402234490495175, 0.0636064960854128, 0.06307879299856722, 0.06532700196839869, 0.06399208097718656, 0.063645651913248, 0.06474513793364167, 0.06480207201093435, 0.12935510103125125, 0.06351081700995564, 0.667491719010286, 0.07009400299284607, 0.0683326970320195, 0.06707954104058444, 0.06709584698546678, 0.06412996305152774, 0.06311180104967207, 0.06418108206707984, 0.06400849809870124, 0.06526242906693369, 0.0702777560800314, 1.1465609719743952, 0.06305826699826866, 0.062881043064408, 0.06295838998630643, 0.0653729330515489, 0.06523699907120317, 0.06503566401079297, 0.06274685100652277, 2.722664223983884, 0.06745130300987512, 0.06692251702770591, 0.06749324803240597, 0.06721879495307803, 0.0671981800114736, 0.06820860202424228, 0.06757341499906033, 0.8377736669499427, 0.7041068279650062, 0.06551835406571627, 0.06648875505197793, 0.06621273199561983, 0.06538430706132203, 0.0658200269099325, 0.06587668298743665, 0.06527598097454756, 0.06611480994615704, 0.06550094997510314, 0.06603329803328961, 0.06596081703901291, 0.06547801499255002, 0.06702075805515051, 0.06570249097421765, 0.06586165504995733, 0.06561115093063563, 0.06592854298651218, 0.06609763798769563, 0.0656171990558505, 0.06545319000724703, 0.06629886594600976, 0.06744604895357043, 0.069379627937451, 0.06626592599786818, 0.06651346490252763, 3.1093335640616715, 0.0718208720209077, 0.06583731202408671, 0.06386744300834835, 0.063692950992845, 0.06387679697945714, 0.0639349949778989, 0.06397205998655409, 0.06374301901087165, 1.7588728979462758, 0.07489047292619944, 0.06830032705329359, 0.06815781304612756, 0.06904220709111542, 0.06871085800230503, 0.06953292503021657, 0.06897666293662041, 0.06944878899957985, 0.07040882401634008, 2.2517256480641663, 0.0711284950375557, 0.06859283801168203, 0.06872334494255483, 0.06780333293136209, 0.06790623802226037, 0.06985305994749069, 0.0679105039453134, 0.0686023929156363, 0.06866192992310971, 2.345023786998354, 0.07346284599043429, 0.07358296099118888, 0.07198657002300024, 0.07126102200709283, 0.07139869499951601, 0.06716126995161176, 0.06643893907312304, 0.0663716959534213, 0.06600652798078954, 0.06707210803870112, 0.06732858705800027, 0.8957044970011339, 1.0051321990322322, 0.06955601205118, 0.06669463193975389, 0.0750485040480271, 0.0682867580326274, 0.06363444600719959, 0.06444533204194158, 0.07300386403221637, 0.1496193630155176, 0.06883140502031893, 0.08850706298835576, 0.07841148192528635, 0.0793342050164938, 0.07188963901717216, 0.07164364599157125, 0.06907351000700146, 0.06741318397689611, 0.06646751496009529, 0.06592960492707789, 0.06587137700989842, 0.9884094059234485, 0.06736113096121699, 0.06512280006427318, 0.06538029492367059, 0.06691529892850667, 0.0648028280120343, 0.06672544602770358, 0.06518740998581052, 0.0649827818851918, 2.100203071953729, 0.07103174598887563, 0.0662989099510014, 0.06649934395682067, 0.06727804301772267, 0.06505839701276273, 0.06559474300593138, 0.06482297403272241, 0.06439234700519592, 0.06592091510538012, 0.06543139705900103, 0.06522042700089514, 2.6324793100357056, 0.06905429100152105, 0.06946117605548352, 0.06865761196240783, 0.07449805096257478, 0.0754659870872274, 0.35305151401553303, 0.09021393593866378, 0.08009697101078928, 0.08416988595854491, 0.0757960609626025, 0.07599531894084066, 0.0689509540097788, 0.06939386099111289, 0.0691040629753843]
[0.0013945735111946659, 0.0013867950195218532, 0.0013674474701437416, 0.0013795252841878303, 0.0013660041838694289, 0.0013649990634840665, 0.0013076402033127996, 0.0013917465908072737, 0.0013134707744252316, 0.0012978027973856246, 0.0013210044898168774, 0.00135930265271466, 0.0013615457122499238, 0.0013720842045066611, 0.001317663265544237, 0.0013005889605312627, 0.0013187837962783417, 0.0013308439579583248, 0.0013236269801474956, 0.0013352362248970537, 0.0013332300824208223, 0.0013236531023202197, 0.0013303101224330614, 0.0013263617732505106, 0.0013835594504691508, 0.0014636639804978455, 0.0013816396335177884, 0.0013868520202647363, 0.0014082211217063727, 0.0013742414707013843, 0.0013763460387684861, 0.0013780870417855224, 0.0013691421232319304, 0.001422191183652957, 0.00140328616217463, 0.0014613440607161242, 0.001751367142899152, 0.0013530766116265133, 0.030742679019363558, 0.0013169357149234535, 0.001312786979334695, 0.0013041504497202684, 0.0013020339597738823, 0.001296792960991817, 0.0012904349405660617, 0.0012981788378817086, 0.0014815150821885588, 0.0013604569177580426, 0.0013649811830409632, 0.0013687003888569924, 0.0013593428777720854, 0.0013584568977774102, 0.0013534228355452723, 0.0013565873271044419, 0.0013591690815757125, 0.0013014941419265708, 0.033659489673315265, 0.0013359479382824228, 0.0013478939179141, 0.0013267002234767591, 0.001338105857828442, 0.0013373172461834488, 0.0013343639985411142, 0.0013366821839721228, 0.0013216380600114257, 0.0013245488588261058, 0.001327150978851227, 0.020666696489978657, 0.0014255297968962363, 0.0013168485103441136, 0.001300643775517083, 0.0012873757739874478, 0.0012935385730459678, 0.0013031985097545751, 0.0012920452448140299, 0.001281810183154077, 0.0012943608762354267, 0.0014817992448616698, 0.001298065183266085, 0.0014799540192459006, 0.0013878961635411394, 0.0013915157344724452, 0.001393071245116999, 0.0013832692246960134, 0.0014019924268240528, 0.0013899376748928002, 0.047235983837281864, 0.04748775977736377, 0.0014140041858642077, 0.0013802706320979157, 0.0013859998387265571, 0.001374249672042016, 0.0013753522653132677, 0.001377145386757139, 0.0013650311038316209, 0.0013832392037979194, 0.0013805942856991778, 0.0013814954305713882, 0.04777065473276058, 0.012856391714695764, 0.0014030005716319596, 0.0013703755316875723, 0.001352256204343724, 0.001440466897163008, 0.0014209472859392361, 0.0014247503654309073, 0.001414785570255956, 0.0014124687556747574, 0.0013491194077520346, 0.0013396005925475334, 0.0013337222864015065, 0.0013483853877655097, 0.0013293733663514865, 0.0013367572457206492, 0.0013380581845662423, 0.0013364892433948663, 0.0013371303877128021, 0.0013389373673316166, 0.0033640905083822353, 0.0013520897565675633, 0.0013337719387241773, 0.0013483647965084836, 0.0013301287544891238, 0.003110116304905743, 0.0014132995714376472, 0.0014146749780760432, 0.0014277236935283457, 0.0014191716743100966, 0.0014275282654645188, 0.0014233969375301078, 0.0014316807976182625, 0.001417644877861045, 0.0013365127141492404, 0.0013215422430740936, 0.0013188806728326849, 0.001350304712921533, 0.0013365901422173697, 0.0013295363881910334, 0.001333645920326211, 0.0013199843043385415, 0.001345685041718641, 0.001354676590548182, 0.0013498616124484309, 0.001352954774676841, 0.0013485911435314587, 0.0013521036527556728, 0.0013387272875679999, 0.0013391106123370783, 0.001337800856812724, 0.0014500754709564606, 0.0014723907961339063, 0.0014574624080096884, 0.0014497886331067706, 0.0014450306129850903, 0.0014249223868875783, 0.0014399722033199302, 0.0014525712443971817, 0.0014333700813467102, 0.0014360528961964408, 0.0014290369391365318, 0.0014423661228573444, 0.0014310467760173641, 0.001434225938282907, 0.0018541865307381566, 0.0013511898970155387, 0.0013727405707218818, 0.0014620294476080003, 0.0014542497133798137, 0.0013412086326362832, 0.0013704364691689915, 0.0013587637353042255, 0.0013907570415651615, 0.001803451571708583, 0.0014480676725316718, 0.0012621998558847271, 0.0012560457760962297, 0.0012596001021792085, 0.0012554673463752379, 0.0014472412045246788, 0.0013633183042081644, 0.0012345864480285316, 0.00132738146930933, 0.0012267886124057124, 0.0012293161030820742, 0.001324688307750894, 0.001339928059344541, 0.0013380921231962893, 0.0013314795713605626, 0.0013396135716680058, 0.0013316583876707116, 0.0013434733881862188, 0.0013408826531044074, 0.0012662330005622031, 0.0012387900009789333, 0.006384590733321194, 0.041128581938599904, 0.0016478446944217598, 0.0012601275704991147, 0.0012557518369119083, 0.0012538621834080135, 0.00125526838783859, 0.0012127148980578901, 0.0012189443674584736, 0.0012242972651230438, 0.0012175793265353661, 0.0012215479985069558, 0.001227850122947474, 0.0012350373269457901, 0.0012370738561968415, 0.0012549302227110887, 0.02924139659177056, 0.0013820091428767358, 0.0013717062657280844, 0.0013408694292741771, 0.0013374030799129788, 0.0013604835934975014, 0.0013325134083172496, 0.0013245885303167968, 0.0013331709576922717, 0.0013402709381046649, 0.001336239368597768, 0.0013253298773411282, 0.0013290710191299416, 0.0013340432862085955, 0.0012646555111800529, 0.028613502430558508, 0.006624880774250748, 0.001309702182378696, 0.0013001511842772669, 0.0012713176108022429, 0.0012731662861128546, 0.0012711452234688463, 0.0013149885498747534, 0.0012708293673183238, 0.0012907803875907343, 0.0012736081036415939, 0.005232448202595875, 0.001522138674401355, 0.0012473850606047378, 0.0012340743251487004, 0.0012375496337380335, 0.0012320913279391065, 0.0014259770408044665, 0.0012488931830858393, 0.0012958730827560838, 0.0014626733882694828, 0.0013977481023769599, 0.004761966755518651, 0.002117542367029403, 0.001275734857142884, 0.001277161980694046, 0.001379369225885187, 0.001319568285870613, 0.0013154980213362344, 0.0013428462042036106, 0.025392627897102157, 0.0014460727130538042, 0.001373117756364601, 0.0013506816324720882, 0.0013540853266319145, 0.0013520512254718616, 0.0013943608584148543, 0.0013467805526618446, 0.001451079529348989, 0.0014569455097258395, 0.0014603686333652965, 0.0014809931637909338, 0.001466216407811307, 0.04623714508251192, 0.0013840932859944142, 0.0014781575906565602, 0.0013334780826936572, 0.0013327631239342143, 0.0013489342646255177, 0.0013561026734889162, 0.0014257810829321341, 0.0014250027134596389, 0.001458976225338268, 0.001440721775443122, 0.001435818307443845, 0.02861464585709785, 0.0014845459574681459, 0.0013773875306265391, 0.0013633120201566086, 0.0013688381842090463, 0.0013758025716572087, 0.0013699515922261135, 0.0013657566943985162, 0.0013645718168771388, 0.0013567627958801327, 0.001397950817089604, 0.001368549980261192, 0.0013277682651556572, 0.0013197835496797853, 0.0013172681020496755, 0.0015687118355678965, 0.001464317735683705, 0.0013530075915956072, 0.0013541247344062645, 0.0013572070414998702, 0.0013646729176445883, 0.001413064631063263, 0.0013740633267491143, 0.0013675990618993432, 0.0013517518171432372, 0.025984884836064766, 0.017828610732828324, 0.0015480882863571144, 0.0014281417124391515, 0.0014296751635681307, 0.001443355572314895, 0.001433383654422906, 0.0014364498154241213, 0.0014477473260758786, 0.0014354483680609539, 0.0014355834882364285, 0.0014378836744331888, 0.04286293683773173, 0.001694979245907494, 0.0013748785119731815, 0.001672961162308193, 0.0013384849370020081, 0.0013203455101964729, 0.0013265901420987686, 0.001326625755683956, 0.0013448596737176484, 0.0013259907766264311, 0.00132638996001333, 0.0013395124281357443, 0.001328515836835972, 0.0013325294284910268, 0.0013399220009961603, 0.001336159652612191, 0.0013323001425751314, 0.0013285867337669646, 0.0014362095104416414, 0.0014653289785646663, 0.011755975203265493, 0.002300175631475844, 0.001406900366122017, 0.0014136482234474044, 0.0013915537951552138, 0.0013977248359433546, 0.0014051378381495572, 0.001403159326493588, 0.0015863518976625434, 0.0013151971423732384, 0.02629083224657771, 0.009273555816379281, 0.0014732967758057068, 0.0014842201038548837, 0.0014767280627726292, 0.001478647917737158, 0.001469404346366622, 0.03925088761440877, 0.0013304663470433075, 0.001350272960048549, 0.0013441529590636492, 0.0013234225310841386, 0.001334224162357194, 0.0013241219377069144, 0.0013644479581021837, 0.0014934524898512326, 0.0013413294883710997, 0.0013012563268064844, 0.03705730828532607, 0.021024886430811365, 0.00956892877655598, 0.0014037364305054046, 0.0014702038369046487, 0.0014128954083259618, 0.001417738490034731, 0.0014306447962869186, 0.0014225209597498178, 0.0014320542247091628, 0.0014361678979036454, 0.034555447855679204, 0.004962587612681091, 0.013583241653039443, 0.0013955191005857624, 0.00141081297817659, 0.0014042539179933314, 0.001419051652489116, 0.0013911917530076237, 0.0014227947353252343, 0.001421803796226729, 0.0014218618375800398, 0.0014234138153759496, 0.004905958104004361, 0.011407591694756886, 0.010233314325367766, 0.0013462964692438136, 0.0013517531214700062, 0.001339808857182459, 0.0013415132226346402, 0.001333379695115953, 0.0013402629790979686, 0.0013516182246218836, 0.0013336212236472235, 0.0013333231220211909, 0.0013391641016137234, 0.0013414023049669911, 0.0013416579387587856, 0.0013738535724732341, 0.0013841594289037951, 0.0012757246316011464, 0.0012911883068784159, 0.0012960539992936716, 0.001295024491086298, 0.0012953404897861943, 0.0013765285898722252, 0.003130004325482462, 0.001485086143568006, 0.0014618540002147155, 0.0013973695910250653, 0.0014051531432006433, 0.001438281448957111, 0.0014110495531171256, 0.0014002676530532083, 0.0013930869991986118, 0.0013902357765188326, 0.001405937509250124, 0.0014442667978036465, 0.0014672099983281627, 0.0015231951648293405, 0.0014911345918947945, 0.0014861303866289708, 0.001473384020774036, 0.001467146304892186, 0.001380831651788737, 0.001372500244356996, 0.0013641580198036165, 0.0014855520804507695, 0.0014803528795208857, 0.0014665197148653014, 0.0014966735093645295, 0.0014889185502174863, 0.0014860702664306273, 0.0013556786746318852, 0.04943936469499022, 0.0028493479773288176, 0.0014243033662324352, 0.0014087444489669738, 0.0014906777757961228, 0.0015074907355390641, 0.0015479118173599852, 0.0015210040407825488, 0.0014337195102505538, 0.0014508281839175187, 0.0014719170831828093, 0.02493313857240184, 0.02548581548985474, 0.00133196961096659, 0.001316101815342447, 0.0013258674286533983, 0.0013347601432505312, 0.0013321454076058402, 0.0013422106121837789, 0.0013480229583587877, 0.0013544427359248607, 0.0015161680414018277, 0.0015027931017078916, 0.0013652393470841404, 0.0015603940819903296, 0.0014168301416675048, 0.0013367251436016997, 0.01488956581440051, 0.008445873224575604, 0.0013262421624468906, 0.0013322399796119758, 0.0012971534707336401, 0.001284785224219822, 0.0012889246727168864, 0.001293211878391401, 0.0015086674877461425, 0.00151930628486966, 0.00142275304200926, 0.016266799919136172, 0.026903879142138272, 0.001363143408005791, 0.0013529785709189518, 0.0012673827135289203, 0.0013026688580534288, 0.0012872739390907238, 0.0014960807344249012, 0.001357106876806641, 0.0012888941837816822, 0.0012796792244462638, 0.0012753988373834565, 0.001396908039911365, 0.0012862757133434014, 0.0012842291628713815, 0.001280879143302386, 0.050987105225022804, 0.0014493971212519979, 0.0013781782234923877, 0.0014166712870213147, 0.0013344035916296498, 0.0014213665711636447, 0.001363039123635663, 0.001353995753357149, 0.0013566994706967048, 0.001345835348154057, 0.0013561579375052635, 0.0013680936939710257, 0.001359784101345101, 0.0013736879182219201, 0.0264277420629158, 0.010277803063069527, 0.0030492851225545213, 0.001476857081834911, 0.0014417882443272642, 0.0014494174084036934, 0.001442940121668638, 0.0014174817966259255, 0.0014378530809617772, 0.0014184171652269302, 0.0014202262854621727, 0.0014146230212051648, 0.0013486895311091626, 0.0013431112058650777, 0.0013871210204361348, 0.0013603049791322984, 0.0013608699593198846, 0.001358283080198631, 0.0013836447971549873, 0.0013671712664773269, 0.0013494004295872791, 0.03125784667541406, 0.0013333549390413932, 0.0013030785306984065, 0.0012894606536102233, 0.0012916175515523978, 0.0012795818774789876, 0.0012872639606345672, 0.0012777023259740398, 0.001283227019392106, 0.0013567974092438817, 0.001308327020450058, 0.001279650144374036, 0.014122950654400855, 0.0014117829601413437, 0.0013298101827730329, 0.0013500492858263304, 0.0013286382059699722, 0.0013183785711738225, 0.0013122434290696163, 0.0586145147134797, 0.005047701959669286, 0.0013558856325642187, 0.0013455793461097138, 0.0013424842665922276, 0.0013762417353917749, 0.0013722616547186459, 0.001361391735168136, 0.0016118906942025131, 0.05226808167490348, 0.0015396332030887811, 0.0014748963056968488, 0.0015055046545113533, 0.0013383393275684544, 0.0013436216514139455, 0.0013584483471908132, 0.001347596407867968, 0.0013456026933212973, 0.0013853527952403743, 0.0013512060003431172, 0.0013408321217272658, 0.0013625200823595633, 0.0014283871016826254, 0.0013441059367769227, 0.00135431197776022, 0.001357606020566, 0.0013493960604489762, 0.034876703776476184, 0.0013573367756848432, 0.00127842238800106, 0.0012673089798653917, 0.0012735176323057742, 0.001351915447193445, 0.0013303627540376417, 0.0013379701223148375, 0.001348490836288856, 0.0013437709196147565, 0.0013322403668711077, 0.001287885487782864, 0.0013714282849461449, 0.0012627169988783343, 0.0012630263666565322, 0.0013534920598969472, 0.007953929325223577, 0.015249320550593642, 0.0014951575704261052, 0.0014047712653077074, 0.001397511061775137, 0.0014238425706304153, 0.001407902611761677, 0.0013979461010811584, 0.0013988610613635, 0.0014080929603161557, 0.04312090703989474, 0.004071797510342938, 0.0015043214495692933, 0.0015078254485958998, 0.0014657837538314717, 0.001477188815609837, 0.0013724650821781584, 0.0014132687332564775, 0.0014289051855020986, 0.0014001748581625977, 0.0014159925096687309, 0.0014117554908770385, 0.0013976084087424132, 0.0014065188160926408, 0.04561111552887881, 0.0024151222671059016, 0.0014681453265401783, 0.001462594285246213, 0.0014550947531943722, 0.001443747815923119, 0.0014454642648105414, 0.0014434211830399474, 0.001477935365686307, 0.0015200303477824343, 0.0014449473665266925, 0.046946756040430346, 0.0014503388570583596, 0.0014286389389094344, 0.0014207463055773049, 0.0014216756728496782, 0.001306509204703022, 0.0013316163451087717, 0.001310246509537861, 0.001316355184975023, 0.001301845243409732, 0.0013082640611433557, 0.0014244982644877567, 0.0013549133270446742, 0.0013670696525321323, 0.0013578292457576916, 0.0514785344688678, 0.00277026636735061, 0.0014704174471410866, 0.0014733411442982604, 0.0014341708572049225, 0.0014457342842099617, 0.001425541509703106, 0.0014418836122340694, 0.0013329643870190699, 0.0013493406942722444, 0.0013347791236996347, 0.031423118326584905, 0.001331217164097696, 0.001343270732869147, 0.0013430622449068694, 0.0012781934917201192, 0.0012837573885917664, 0.0012850720620695122, 0.0013039088380351967, 0.0012797431435854155, 0.0013099159399161534, 0.0013066841222878014, 0.025502251306719775, 0.001426680367535018, 0.001391360776175802, 0.0013715929793650095, 0.0013520729808821057, 0.0013308399166835814, 0.001301263449523522, 0.0013668968565571978, 0.001321289897421185, 0.0014292195092468541, 0.0014410745898945903, 0.0014645884488234107, 0.0014595324078536763, 0.001476107918353257, 0.05634036908230307, 0.021395724817958414, 0.0013564338774553367, 0.001383410714452668, 0.0013617315087277367, 0.0013470178165910195, 0.0013588553066460453, 0.001365796285111229, 0.0013509899786464414, 0.0013458497147550996, 0.0013471294279989538, 0.0524095687753882, 0.0013777244698294268, 0.001398482611783001, 0.001366324592096617, 0.0013796320417895913, 0.0013658018564159165, 0.0013748894288793815, 0.0013630550606556389, 0.0069433356948881125, 0.0012980917568451592, 0.0012873223060932085, 0.0013332041218040548, 0.0013059608362691135, 0.0012988908553724081, 0.0013213293455845239, 0.0013224912655292725, 0.0026399000210459438, 0.001296139122652156, 0.013622279979801756, 0.0014304898569968585, 0.001394544837388153, 0.00136897022531805, 0.0013693029997034036, 0.0013087747561536273, 0.001287995939789226, 0.0013098180013689765, 0.0013062958795653314, 0.0013318863074884427, 0.0014342399200006407, 0.023399203509681533, 0.0012869034081279319, 0.0012832865931511838, 0.0012848651017613557, 0.0013341414908479368, 0.0013313673279837382, 0.0013272584491998566, 0.0012805479797249545, 0.0555645759996711, 0.0013765572042831657, 0.0013657656536266512, 0.0013774132251511424, 0.0013718121418995516, 0.0013713914288055835, 0.001392012286209026, 0.0013790492856951089, 0.017097421774488628, 0.014369527101326657, 0.0013371092666472708, 0.001356913368407713, 0.001351280244808568, 0.001334373613496368, 0.0013432658553047447, 0.0013444221017844215, 0.001332162877031583, 0.001349281835635858, 0.0013367540811245538, 0.001347618327209992, 0.0013461391232451614, 0.0013362860202561228, 0.001367770572554092, 0.0013408671627391357, 0.0013441154091828028, 0.0013390030802170538, 0.0013454804691124935, 0.0013489313875039925, 0.001339126511343888, 0.0013357793879030006, 0.0013530380805308114, 0.0013764499786442944, 0.001415910774233694, 0.0013523658366911874, 0.0013574176510719924, 0.06345578702166677, 0.0014657320820593408, 0.0013436186127364635, 0.001303417204252007, 0.0012998561427111225, 0.0013036081016215744, 0.0013047958158754877, 0.0013055522446235527, 0.0013008779389973805, 0.03589536526420971, 0.001528376998493866, 0.0013938842255774202, 0.0013909757764515827, 0.0014090246345125595, 0.0014022624082103067, 0.0014190392863309505, 0.0014076869987065391, 0.0014173222244812213, 0.001436914775843675, 0.045953584654370744, 0.001451601939541953, 0.0013998538369731028, 0.0014025172437256087, 0.0013837414883951448, 0.001385841592291028, 0.001425572651989606, 0.0013859286519451713, 0.0014000488350129857, 0.0014012638759818307, 0.047857628306088855, 0.0014992417549068223, 0.0015016930814528344, 0.0014691136739387804, 0.0014543065715733232, 0.0014571162244799186, 0.001370638162277791, 0.0013558967157780212, 0.0013545244072126796, 0.0013470719996079498, 0.0013688185314020636, 0.0013740527971020462, 0.018279683612268036, 0.02051290202106596, 0.0014195104500240817, 0.0013611149375459977, 0.0015316021234291245, 0.0013936073067883144, 0.0012986621634122363, 0.0013152108579988079, 0.001489874776167681, 0.0030534563880717878, 0.00140472255143508, 0.0018062665915990971, 0.001600234325005844, 0.0016190654084998735, 0.0014671354901463706, 0.0014621152243177807, 0.001409663469530642, 0.0013757792648346145, 0.0013564798971448019, 0.0013455021413689365, 0.0013443138165285392, 0.020171620529049968, 0.0013747169583921833, 0.001329036736005575, 0.0013342917331361345, 0.001365618345479728, 0.001322506694123149, 0.0013617437964837467, 0.0013303553058328678, 0.0013261792221467715, 0.042861287182729156, 0.0014496274691607272, 0.0013530389785918655, 0.0013571294685065442, 0.0013730212860759727, 0.0013277223880155658, 0.0013386682246108444, 0.0013229178374024983, 0.001314129530718284, 0.001345324798068982, 0.001335334633857164, 0.0013310291224672478, 0.05372406755174909, 0.001409271244929001, 0.00141757502154048, 0.0014011757543348536, 0.001520368386991322, 0.0015401221854536204, 0.007205132939092511, 0.001841100733442118, 0.001634632061444679, 0.0017177527746641819, 0.0015468583869918877, 0.0015832358112675138, 0.0014364782085370582, 0.001445705437314852, 0.0014396679786538396]
[717.065104114409, 721.0871007777238, 731.2895170261154, 724.8870401014298, 732.0621794637096, 732.601235232769, 764.7363529100602, 718.5216091817092, 761.3416449540684, 770.5330902464249, 756.9996981150487, 735.6713370660334, 734.4593655599873, 728.8182436001108, 758.9192369167005, 768.8824296890244, 758.2744061779028, 751.4028928937098, 755.4998613646928, 748.9311489261756, 750.0580831361401, 755.4849516441346, 751.704420748943, 753.9421145629848, 722.7734230436648, 683.216922274649, 723.7777317185836, 721.0574635130214, 710.1157514157137, 727.6741543024609, 726.561469159871, 725.6435694398136, 730.3843648016968, 703.140345330688, 712.6130271607113, 684.3015460096065, 570.9825058980123, 739.0564520939542, 32.52806950787018, 759.3385073151612, 761.7382071436969, 766.7826976668937, 768.0291228146345, 771.133118454913, 774.9325196986222, 770.3098916877591, 674.9846910250527, 735.047164630501, 732.6108318740024, 730.6200890576968, 735.6495674137524, 736.1293550322529, 738.8673914291657, 737.1438461941427, 735.7436345157879, 768.3476765555962, 29.7093036675712, 748.5321630764008, 741.8981469606491, 753.7497788154385, 747.3250297423103, 747.7657248898016, 749.4206986199562, 748.1209909062839, 756.6368056859341, 754.9740376404535, 753.4937742091658, 48.38702694864189, 701.4935795640823, 759.3887923666208, 768.85002552097, 776.7739771136553, 773.0731969169221, 767.3428050407493, 773.9667043501521, 780.1467121593287, 772.5820660683455, 674.8552501073469, 770.3773376648792, 675.6966682718578, 720.5149969206312, 718.6408139172946, 717.8383758226333, 722.9250692104131, 713.2706146389891, 719.4567195807003, 21.170301087509724, 21.058058006700815, 707.2114849425445, 724.4956001708659, 721.5008054537656, 727.6698116391665, 727.0864528457568, 726.1397450234141, 732.58403943545, 722.9407590923748, 724.3257562040159, 723.853280923556, 20.933353448769278, 77.78232199139742, 712.7580845079825, 729.7269813103952, 739.5048340601397, 694.2193548282816, 703.7558746164233, 701.8773423494129, 706.8208928785477, 707.9802622057186, 741.2242343072111, 746.4911598003168, 749.7812777036837, 741.6277342319468, 752.2341166985588, 748.078982329284, 747.3516559552075, 748.2289924458072, 747.870222073501, 746.8609244903749, 297.257162822558, 739.5958701281903, 749.7533655990337, 741.639059837104, 751.8069184092485, 321.5313840265876, 707.5640721965071, 706.8761485836126, 700.4156368160366, 704.6363862117886, 700.5115234440555, 702.5447179443914, 698.4797181491818, 705.3952760784555, 748.2158526539359, 756.6916647884512, 758.2187081809345, 740.5735834516861, 748.1725088448019, 752.1418810963117, 749.8242110285158, 757.5847657530373, 743.1159364919821, 738.1835686666299, 740.8166813382904, 739.1230059695486, 741.5145834202735, 739.5882689629133, 746.9781256320329, 746.7643007135559, 747.4954100287199, 689.6192784644556, 679.1675162774212, 686.1240430657834, 689.7557182919053, 692.026861586162, 701.7926093394283, 694.4578497379661, 688.4343909857591, 697.6565319826258, 696.3531793631144, 699.7719741270165, 693.3052462567466, 698.7891777954476, 697.2402139074588, 539.3200648490836, 740.0884229587308, 728.469764300862, 683.9807513016115, 687.6398123372536, 745.5961553381874, 729.6945334550112, 735.9631214885944, 719.0328505362787, 554.4922944909488, 690.5754606424503, 792.2675599571031, 796.1493275412176, 793.9027618923818, 796.5161363114553, 690.9698237402193, 733.5044185303556, 809.9878316312846, 753.3629353137837, 815.1363567346915, 813.4604252664182, 754.894562101056, 746.308723834901, 747.3327005403099, 751.0441928734643, 746.4839272678169, 750.9433419701306, 744.3392692355958, 745.7774158572363, 789.7440672893562, 807.2393216039575, 156.62711076858815, 24.313991702725893, 606.8533056453522, 793.5704474777242, 796.3356856074029, 797.5358163223228, 796.6423831654606, 824.5961203259367, 820.3819851803593, 816.7950942041012, 821.3017240080036, 818.6334071377103, 814.4316487092773, 809.6921268549588, 808.3591735373975, 796.8570538046728, 34.198092996742545, 723.5842144419098, 729.0190509330455, 745.7847708119557, 747.71773373295, 735.0327521622087, 750.4614916129357, 754.9514261314295, 750.0913474225445, 746.117797207588, 748.3689101671858, 754.5291305182045, 752.4052406579721, 749.6008640334603, 790.7291678718876, 34.94853530870184, 150.94611270390698, 763.5323613676727, 769.1413214809198, 786.5855011392215, 785.4433555990023, 786.6921745346184, 760.4628953577165, 786.8877016197525, 774.7251272282798, 785.1708835243169, 191.11512647251607, 656.970364670152, 801.6770695611791, 810.3239647899687, 808.0483988181451, 811.6281458393828, 701.2735628869935, 800.7089906032962, 771.6805089223582, 683.6796293826876, 715.4364926694848, 209.99726611721894, 472.24556900027994, 783.8619399642215, 782.986038667211, 724.9690519652321, 757.8236084540551, 760.1683801730366, 744.6869171388549, 39.381508839977975, 691.528158282033, 728.2696588583566, 740.3669199008413, 738.5058979166039, 739.6169473172166, 717.1744630990474, 742.5114641160728, 689.1421040503818, 686.3674676400045, 684.758613101395, 675.2225631077702, 682.0275606468958, 21.627632895920854, 722.4946541674328, 676.5178532525921, 749.9185873231426, 750.3208800136043, 741.3259683766825, 737.4072919030885, 701.3699451976802, 701.7530496992443, 685.412128472585, 694.096540390271, 696.4669518528966, 34.947138783195896, 673.606630343377, 726.0120901088201, 733.5077995462303, 730.5465405159109, 726.8484741931106, 729.9527995547946, 732.1948368266305, 732.830612234487, 737.0485121176243, 715.3327483164977, 730.7003868496984, 753.1434710730699, 757.7000033396587, 759.1469029303869, 637.4657074210098, 682.9118951653547, 739.0941530643565, 738.4844058981497, 736.8072588946229, 732.776321029357, 707.6817139266647, 727.7684954782194, 731.2084571125577, 739.7807699000385, 38.48391117793552, 56.08961993649184, 645.9579914225377, 700.2106242608658, 699.4595873822392, 692.829971478318, 697.6499256946038, 696.1607633363394, 690.7282658987888, 696.6464431951877, 696.5808733482093, 695.4665511410012, 23.33017925919887, 589.977725340583, 727.3369910806389, 597.742507435316, 747.113375993487, 757.3775138987642, 753.8123254993606, 753.7920892274848, 743.5720020034959, 754.153058699388, 753.9260927381795, 746.5402925687985, 752.719668274054, 750.4524692804816, 746.3120982091148, 748.4135582488221, 750.5816204951789, 752.6795011453133, 696.2772441831938, 682.4406086471654, 85.0631260027009, 434.7494105736516, 710.7823866422054, 707.3895636931104, 718.6211582200889, 715.4484017772199, 711.6739531524621, 712.6774423393106, 630.3771574727393, 760.3422846521142, 38.03607244613455, 107.83350203529965, 678.7498733601223, 673.7545175427517, 677.1727477857033, 676.2935165325532, 680.5478713008354, 25.477130856854963, 751.6161549086137, 740.5909987000296, 743.9629494969162, 755.616574837068, 749.4992432405697, 755.217455071984, 732.89713547661, 669.5894290548292, 745.5289760418168, 768.4880983089463, 26.98523034378024, 47.56268259953732, 104.50490575810483, 712.3844464448056, 680.1777922885776, 707.7664730928867, 705.3486993750889, 698.9855221892884, 702.9773397334493, 698.297580318993, 696.2974186094023, 28.938996947066034, 201.50777740319617, 73.62012879865358, 716.5792281741288, 708.811171621382, 712.1219226712166, 704.695983578843, 718.8081713668123, 702.842072135874, 703.3319243160428, 703.3032138354355, 702.5363876603107, 203.83378308587183, 87.6609214949038, 97.7200512175279, 742.7784465346471, 739.7800560745285, 746.3751225700527, 745.426868052831, 749.9739224040293, 746.1222279474031, 739.8538890519554, 749.8380966562402, 750.0057439070697, 746.7344732396703, 745.4885057951409, 745.3464635890239, 727.8796081592476, 722.460129316147, 783.8682229917536, 774.4803718193558, 771.5727898258743, 772.1861685883451, 771.9977935415711, 726.4651147513209, 319.4883763765595, 673.3616122748555, 684.0628406483282, 715.6302859477801, 711.6662015374427, 695.2742112644877, 708.6923331579086, 714.1491827077158, 717.8302579632576, 719.3024499081819, 711.2691662471996, 692.3928470284989, 681.5656934859134, 656.5146890497388, 670.6302740447417, 672.8884686008787, 678.7096818619318, 681.5952823965198, 724.2012440144998, 728.5973201910001, 733.0529055160043, 673.1504153638049, 675.5146113024407, 681.8865030340552, 668.1483929147571, 671.6284110060487, 672.9156908588764, 737.6379216642435, 20.22679713158474, 350.95748499538183, 702.0976174796232, 709.8519541520079, 670.835787744895, 663.3539937758952, 646.0316335755697, 657.460449273695, 697.4864977775479, 689.261492908006, 679.3860954705707, 40.107265160226824, 39.237512348705295, 750.7678792118352, 759.8196342733576, 754.2232189953096, 749.1982773509461, 750.6688040889028, 745.0395570729384, 741.8271282393408, 738.3110215561569, 659.557498043168, 665.427595364606, 732.4722966238751, 640.8637481657646, 705.8009076677841, 748.096947817993, 67.1611256140757, 118.40101945767115, 754.010110909926, 750.6155162009603, 770.9188022558526, 778.3402090471922, 775.8405290606544, 773.2684927421785, 662.8365813688603, 658.1951315272742, 702.8626686946079, 61.47490624899158, 37.16936114367788, 733.5985297856142, 739.1100062440705, 789.0276467599786, 767.654798698646, 776.8354268916202, 668.4131257023396, 736.8616408112703, 775.8588816546198, 781.4458349378258, 784.0684581864206, 715.8667366990392, 777.4382969578985, 778.6772243702368, 780.7137818028498, 19.612802013110418, 689.9420354417388, 725.5955601053824, 705.8800507650541, 749.3984625586498, 703.5482755031447, 733.6546564655304, 738.5547536028541, 737.0829145282049, 743.0329433474878, 737.3772422403558, 730.9440898725308, 735.4108633942683, 727.9673838104248, 37.8390252795463, 97.29705792799497, 327.94571835980224, 677.1135895949775, 693.583127712765, 689.9323784867076, 693.0294507602885, 705.4764317822846, 695.4813487140855, 705.0112086312856, 704.1131474866191, 706.9021110288919, 741.4604895595205, 744.5399871829042, 720.9176310265867, 735.129265378322, 734.824068347989, 736.2235564723101, 722.7288405638302, 731.4372562675445, 741.0698693091828, 31.991967021405625, 749.9878469861472, 767.4134570109397, 775.5180409733376, 774.2229879100805, 781.5052851250008, 776.8414486699696, 782.6549108280467, 779.2853368016853, 737.0297092159703, 764.3348982091686, 781.4635933082851, 70.80673327201563, 708.324174630839, 751.9870226250752, 740.71369875058, 752.6503419115138, 758.5074741541399, 762.0537301596567, 17.06062064811445, 198.10995339857936, 737.5253310331371, 743.1743084427989, 744.8876868690668, 726.6165342059837, 728.7239984892162, 734.5424348976946, 620.3894616407302, 19.132135099577415, 649.505348412739, 678.0137668915829, 664.2290988629148, 747.1946608763548, 744.257134400641, 736.1339885082402, 742.0619364681297, 743.1614138135677, 721.8377899374641, 740.0796027741632, 745.8055216575477, 733.9341364189187, 700.0903318309233, 743.9889763435864, 738.3823051272233, 736.5907228247925, 741.0722687802098, 28.672434367908505, 736.7368348915845, 782.2140861938432, 789.0735534014884, 785.2266624604518, 739.6912300070128, 751.6746819354397, 747.4008449978603, 741.5697408460499, 744.1744611400642, 750.615298010069, 776.4665488400929, 729.166818984829, 791.9430885054177, 791.7491086486086, 738.8296020562829, 125.72402382666267, 65.57669219964498, 668.8258279794616, 711.8596633459437, 715.5578423327732, 702.3248360647369, 710.276400971174, 715.3351615105971, 714.8672785453607, 710.1803845219654, 23.190606799500223, 245.59178040161854, 664.7515398296773, 663.2067398326569, 682.2288740655362, 676.9615295165661, 728.6159866544357, 707.5795115737, 699.8364973030825, 714.1965120786887, 706.2184250070279, 708.3379568644428, 715.5080019157966, 710.9752024349276, 21.92448021506615, 414.0577119510905, 681.1314805984456, 683.7166055463291, 687.2404685706537, 692.6417404556274, 691.8192475211907, 692.7984788846794, 676.6195756711128, 657.881601810711, 692.0667307098947, 21.300726276780537, 689.4940414326645, 699.9669214975756, 703.8554287098152, 703.3953095613922, 765.3983580064455, 750.9670511879425, 763.2151604454275, 759.6733855832188, 768.1404568340603, 764.371681299608, 702.0015572708301, 738.0545899428059, 731.4916238157774, 736.4696283603636, 19.425572431646454, 360.97611832048, 680.0789816145659, 678.7294333494578, 697.2669922668176, 691.6900366283155, 701.4878158183326, 693.5372532950768, 750.2075897438764, 741.1026764736697, 749.187623813204, 31.8237034786573, 751.1922374271697, 744.4515655187834, 744.5671291797365, 782.3541634954325, 778.9633842707324, 778.1664775978205, 766.9247809585034, 781.4068041797293, 763.407764977659, 765.295898942397, 39.21222436297233, 700.9278481400669, 718.7208502086216, 729.0792640707147, 739.6050465763986, 751.4051746298488, 768.4838918407842, 731.5840951735739, 756.8361810316877, 699.6825844666528, 693.9266065840121, 682.7856663784003, 685.150939176853, 677.4572424999914, 17.749262496651045, 46.73830910185637, 737.2272372583285, 722.8511313038655, 734.359154936716, 742.3806780304965, 735.9135259722542, 732.1736124934334, 740.1979406256598, 743.025011661103, 742.3191708352886, 19.08048517410441, 725.834535060416, 715.0607319493562, 731.8905081445588, 724.8309474625197, 732.1706258506345, 727.3312158746182, 733.6460784782921, 144.02299470213276, 770.3615670670061, 776.8062397946167, 750.0726885293661, 765.7197461271606, 769.8876282513264, 756.8135857586848, 756.1486612917563, 378.80222433719064, 771.5221171272132, 73.40915041261346, 699.0612307446763, 717.0798479831619, 730.4760772044345, 730.2985535097812, 764.0734169865189, 776.3999630027133, 763.4648469900663, 765.5233516718655, 750.8148363546994, 697.2334168467108, 42.73649740198401, 777.0590968087555, 779.2491601930031, 778.2918211640672, 749.5456867655264, 751.1075110386173, 753.4327625511477, 780.915682842893, 17.997077850570104, 726.4500137651339, 732.1900337328019, 725.9985469431448, 728.9627853966197, 729.1864153409156, 718.3844639212034, 725.137245182612, 58.4883506525012, 69.5917125837548, 747.8820354805003, 736.9667240978417, 740.0389399918054, 749.4152985982452, 744.4542687144619, 743.8140139712983, 750.6589601327646, 741.1350049997104, 748.0807533115911, 742.0498666490571, 742.8652675878569, 748.3427835369649, 731.1167677285675, 745.7860314493726, 743.983733218252, 746.8242715602261, 743.2289230178276, 741.3275495430195, 746.7554346276398, 748.6266138376858, 739.0774985488134, 726.5066043191262, 706.2591924559729, 739.4448845636952, 736.6929398702534, 15.7590039763994, 682.2529248285326, 744.2588175846737, 767.2140560503578, 769.3159013075789, 767.1017069900744, 766.4034386322915, 765.9593893068165, 768.7116292945402, 27.85874980347596, 654.2888312147112, 717.4196978846987, 718.9197805809583, 709.7107995886402, 713.1332867122137, 704.7021246223471, 710.3851928154877, 705.5558593008216, 695.9354979232199, 21.761088009156822, 688.8940919406224, 714.3602950450136, 713.0037113437744, 722.6783386828953, 721.5831921647211, 701.4724914962042, 721.5378645909984, 714.2607993318496, 713.6414612125249, 20.89531043210454, 667.0038349233075, 665.9150344040579, 680.6825215362273, 687.6129280762052, 686.28705329043, 729.5871569329086, 737.5193024390463, 738.2665049630116, 742.3508173958325, 730.5570293351542, 727.7740725167589, 54.70554202201127, 48.74980629133013, 704.4682200000959, 734.6918121425773, 652.9110822601151, 717.5622538206867, 770.0232040121188, 760.3343554520035, 671.1973489290438, 327.4977182927722, 711.8843496734561, 553.6281325530662, 624.9084801979535, 617.6402724375037, 681.6003066630429, 683.9406247661484, 709.3891709721026, 726.8607875989557, 737.2022262216038, 743.2169516895626, 743.8739286205725, 49.57459905414438, 727.4224660540756, 752.4246493031516, 749.4612873375065, 732.269014479818, 756.1398399295226, 734.3525284140596, 751.6788903051362, 754.0458961355433, 23.331077196462438, 689.8324026509774, 739.0770079962663, 736.8493745113736, 728.3208280462649, 753.1694946370644, 747.0110828175543, 755.9048428611894, 760.9599941441196, 743.3149239762431, 748.8759556183028, 751.2983623877145, 18.613631572791125, 709.5866062678232, 705.4300370736634, 713.6863429917869, 657.7353281982624, 649.2991331758944, 138.78994439843748, 543.153333131535, 611.7584645416812, 582.1559509315877, 646.4715893900664, 631.6178505332164, 696.1470031755111, 691.7038382710432, 694.6045996904434]
Elapsed: 0.18359856982783349~0.43981539532433733
Time per graph: 0.0037470502090798242~0.008975788624174759
Speed: 664.133065838379~197.72810358094404
Total Time: 0.0734
best val loss: 0.693538248538971 test_score: 0.5000

Testing...
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6939 score: 0.5000 time: 0.06s
test Score 0.5000
Epoch Time List: [0.35466102801728994, 0.34879771899431944, 0.34707444615196437, 0.34521770698484033, 0.34669743082486093, 0.3471206119284034, 0.343686412088573, 0.3656709318747744, 0.33787984191440046, 0.33250794000923634, 0.3370203779777512, 0.3398236819775775, 0.3464504941366613, 0.34517315903212875, 0.3667381718987599, 0.33125817112158984, 0.3323449249146506, 0.3408403500216082, 0.3441269419854507, 0.3411552320467308, 0.3341121730627492, 0.33807753189466894, 0.33720761199947447, 0.3394647949608043, 0.35653441795147955, 0.3442439379869029, 0.35732928093057126, 0.3532685290556401, 0.35629380284808576, 0.3549065310508013, 0.3520958620356396, 0.35185827093664557, 0.36760184390004724, 0.35244769405107945, 0.35784620500635356, 0.37074892804957926, 0.38659041677601635, 0.36300201108679175, 8.828036851948127, 2.7458150800084695, 0.3362964190309867, 0.33267008315306157, 0.331307491986081, 0.3315737619996071, 0.3297655910719186, 0.3312179740751162, 0.3686258470406756, 0.3492577699944377, 0.3438410769449547, 0.34658168791793287, 0.3440385019639507, 0.3426663710270077, 0.3461243938654661, 0.34087420895230025, 0.34185926895588636, 0.4031606251373887, 3.901398337096907, 3.7006525680189952, 0.34928196715191007, 0.34322302299551666, 0.3422585019143298, 0.3405923709506169, 0.3408574990462512, 0.3410162559011951, 0.3380641491385177, 0.33812599815428257, 0.33915214508306235, 4.290065223001875, 4.519831335055642, 0.335048369015567, 0.3334391439566389, 0.33138746803160757, 0.32912706199567765, 0.3328175221104175, 0.3318322158884257, 0.33071281609591097, 0.3308352080639452, 0.3676125450292602, 0.3349223919212818, 0.33907663193531334, 0.33666958496905863, 0.3541397190419957, 0.35542208701372147, 0.3555855309823528, 0.35526176320854574, 0.35685451806057245, 3.1545566669665277, 12.287578499992378, 0.598222409025766, 0.3513539960840717, 0.3538803639821708, 0.35199203493539244, 0.35108029504772276, 0.35105320205911994, 0.35157027596142143, 0.3516533739166334, 0.3516105831367895, 0.35207372694276273, 2.621525263064541, 5.399288546061143, 0.44251861702650785, 0.34884328907355666, 0.34817383415065706, 0.3549481270601973, 0.36596053279936314, 0.3644844340160489, 0.3618638348998502, 0.3594078420428559, 0.34760552807711065, 0.340364454081282, 0.3410788901383057, 0.3439061460085213, 0.34186065604444593, 0.3417764969635755, 0.34174621501006186, 0.34179215086624026, 0.34136102988850325, 0.34787045896518975, 0.44024449004791677, 0.33989620790816844, 0.3385023099835962, 0.3426814441336319, 0.34195284405723214, 0.425064934999682, 0.35814284696243703, 0.35697474202606827, 0.36197494389489293, 0.35930154006928205, 0.3788835140876472, 0.35932089400012046, 0.36132341099437326, 0.3612116539152339, 0.39764694299083203, 0.33915310609154403, 0.33687887189444155, 0.3392181908711791, 0.3422145239310339, 0.3387857541674748, 0.3393261010060087, 0.33617230399977416, 0.33967701508663595, 0.33933186205103993, 0.34125961107201874, 0.3409789049765095, 0.34272445377428085, 0.34047149994876236, 0.3397202448686585, 0.3398545100353658, 0.33974844007752836, 0.34693713998422027, 0.381197799812071, 0.38554771477356553, 0.36673019907902926, 0.36545101401861757, 0.3637897199951112, 0.36320168210659176, 0.36247573094442487, 0.36218756902962923, 0.361196466954425, 0.3631137650227174, 0.36391218984499574, 0.3628383129835129, 0.36706941202282906, 0.3825780398910865, 0.3481142020318657, 0.3546405170345679, 0.3550061739515513, 0.3545771630015224, 0.3357837270013988, 0.3380249289330095, 0.3387682989705354, 0.33956126391422004, 0.3661978240124881, 0.37176543788518757, 0.3203948970185593, 0.31869441200979054, 0.33986712503246963, 0.3138645169092342, 0.3346203329274431, 0.3286459851078689, 0.3113959188340232, 0.3304017239715904, 0.3100027129985392, 0.3093413549941033, 0.31928359193261713, 0.33263275609351695, 0.33337060804478824, 0.33325628796592355, 0.33281554689165205, 0.3331297099357471, 0.3336211609421298, 0.3366746640531346, 0.33326759305782616, 0.3532231819117442, 0.5634481590241194, 9.961061165900901, 4.059816279099323, 0.3497970850439742, 0.3168137149186805, 0.3171656869817525, 0.3212459458736703, 0.3118877599481493, 0.3093031420139596, 0.31181659596040845, 0.3085173259023577, 0.3086395871359855, 0.312263744068332, 0.31117692100815475, 0.31233350303955376, 0.31456467893440276, 4.956505561014637, 0.45903625898063183, 0.3510071139317006, 0.3380434380378574, 0.3384699489688501, 0.3397538240533322, 0.3380662730196491, 0.3388734071049839, 0.33870814100373536, 0.3366962719010189, 0.34067399508785456, 0.3331900999182835, 0.3371780371526256, 0.33880184206645936, 0.3277260819450021, 8.113152488018386, 1.9501210818998516, 0.46185499406419694, 0.3248292059870437, 0.3932163800345734, 0.3201569520169869, 0.3216272038407624, 0.33032001298852265, 0.3208835310069844, 0.3239542560186237, 0.32531793287489563, 0.5144011869560927, 7.467358872992918, 0.3249490469461307, 0.3155611710390076, 0.31305517605505884, 0.31442567717749625, 0.3513366130646318, 0.33148740988690406, 0.3315334591316059, 0.34988717501983047, 0.3477314949268475, 4.9399193349527195, 9.985936373937875, 0.32973977213259786, 0.3256427019368857, 0.3366663680644706, 0.33948356297332793, 0.3397665120428428, 0.3431476909900084, 1.5159702419769019, 6.419942886917852, 0.3516977148829028, 0.3466801369795576, 0.34838328312616795, 0.3479927530279383, 0.3498027571476996, 0.34609429095871747, 0.35503467614762485, 0.36935532419010997, 0.37184933898970485, 0.3728847870370373, 0.3731165879871696, 2.684537003049627, 6.445616625947878, 0.34950807900168, 0.3410238070646301, 0.3374244279693812, 0.34410519106313586, 0.3375959969125688, 0.4561329890275374, 0.36118795396760106, 0.36618597502820194, 0.36344826384447515, 0.4004437478724867, 4.992058176081628, 3.215855870046653, 0.3496838129358366, 0.3464537840336561, 0.3466708419146016, 0.34613776800688356, 0.3470766111277044, 0.3491375989979133, 0.34660141298081726, 0.34589115297421813, 0.34669062204193324, 0.34663602604996413, 0.34190760005731136, 0.3331450499827042, 0.335598828853108, 6.098439085064456, 0.37116509792394936, 0.36576180695556104, 0.34558180207386613, 0.34470887295901775, 0.3459386290051043, 0.3470731768757105, 0.34721967088989913, 0.34128599101677537, 0.343607616960071, 3.844389762962237, 3.3326252789702266, 1.4211816960014403, 0.3643488270463422, 0.3602919310797006, 0.36355860612820834, 0.3613976330962032, 0.3612228808924556, 0.36524520604871213, 0.3621863720472902, 0.3635731649119407, 0.3648919730912894, 5.473455515806563, 3.5531620089896023, 0.3514135121367872, 0.3800099330255762, 0.36015338893048465, 0.33959471993148327, 0.3397431130288169, 0.33975910407025367, 0.3426823350600898, 0.34006670711096376, 0.3412776459008455, 0.3416764280991629, 0.34124018799047917, 0.3422623658552766, 0.34405090694781393, 0.34693699097260833, 0.3537589539773762, 0.34833779197651893, 0.35007382498588413, 0.3688241910422221, 6.809974278905429, 3.1992494580335915, 0.3662064168602228, 0.3826217890018597, 0.3543688071658835, 0.35450176801532507, 0.35801208310294896, 0.3604542230023071, 0.4584904379444197, 0.3355760750127956, 2.4055189180653542, 6.347519895993173, 0.38292378187179565, 0.37354764400515705, 0.3777542740572244, 0.37428380304481834, 0.37423605006188154, 2.5739918801700696, 11.136254275101237, 0.33672231796663254, 0.3385464991442859, 0.3336212339345366, 0.33810531499329954, 0.33597082388587296, 0.33941807597875595, 0.35075804113876075, 0.36173380981199443, 0.35559801710769534, 3.8162197939818725, 2.9574019338469952, 1.5338329090736806, 0.3572758799418807, 0.35936656105332077, 0.3619653369532898, 0.3589975112117827, 0.35732368589378893, 0.3570430971449241, 0.3574551399797201, 0.3608187170466408, 4.208252870943397, 2.2303703060606495, 2.206689973012544, 0.3534953958587721, 0.3496341039426625, 0.35037629294674844, 0.35370420396793634, 0.3519219630397856, 0.35847331990953535, 0.3575371300103143, 0.35610227612778544, 0.35631770407781005, 0.5279968309914693, 8.909550536074676, 3.0472395459655672, 0.346198390936479, 0.33812695217784494, 0.34086659003514796, 0.33640856202691793, 0.33655855990946293, 0.33529733296018094, 0.3380739811109379, 0.3351118649588898, 0.33727531589102, 0.3351021039998159, 0.3372016418725252, 0.33605773001909256, 0.33925605192780495, 0.3465259480290115, 0.3398223698604852, 0.3270635559456423, 0.326839841902256, 0.32577295508235693, 0.3283196260454133, 0.32961911300662905, 2.9753516050986946, 0.3765624179504812, 0.37266999401617795, 0.36045482114423066, 0.3556120719294995, 0.36354933003894985, 0.35505368805024773, 0.35414966009557247, 0.3542448189109564, 0.353305886965245, 0.35694288392551243, 0.3602205158676952, 0.3744082519551739, 0.3751410919940099, 0.3718772779684514, 0.37055704987142235, 0.3731082559097558, 0.37022183591034263, 0.35501885402482003, 0.3477712390013039, 0.34695538599044085, 0.35814001597464085, 0.37700060207862407, 0.3686951401177794, 0.3753421929432079, 0.3740563109749928, 0.3734795100754127, 0.3815944930538535, 2.8708673949586228, 2.5403411649167538, 0.37126023101154715, 0.3583314560819417, 0.3671138419304043, 0.3735612710006535, 0.36994569399394095, 0.3764930759789422, 0.35945552913472056, 0.3610615929355845, 0.36339220905210823, 3.721769012976438, 9.577239054022357, 0.34770897100679576, 0.33579093299340457, 0.33599921909626573, 0.33621648396365345, 0.3403955219546333, 0.3414374788990244, 0.34023231000173837, 0.3410611590370536, 0.34963795309886336, 6.012583700125106, 0.3443752779858187, 0.3494429829297587, 0.394994736998342, 0.34985430887900293, 1.0026647709310055, 11.032310905051418, 0.3469322600867599, 0.3377396789146587, 0.3312435980187729, 0.3304609680781141, 0.32765671086963266, 0.3259010559413582, 0.374714974896051, 0.3754503910895437, 0.3661917260615155, 2.2400190300541, 8.133808589889668, 0.37131143792066723, 0.35023288091178983, 0.3402219822164625, 0.32791094994172454, 0.3389467769302428, 0.34394204698037356, 0.34032701898831874, 0.32275442592799664, 0.32268966105766594, 0.32098556717392057, 0.35164952592458576, 0.3332819779170677, 0.32412342191673815, 0.3190581798553467, 5.037316430127248, 4.943052515969612, 0.35687855596188456, 0.35384499398060143, 0.34075166401453316, 0.3552014500601217, 0.3404121899511665, 0.3447269609896466, 0.3434298379579559, 0.342253998038359, 0.3434770068852231, 0.3445491259917617, 0.3438583791721612, 0.3477933070389554, 3.958188163000159, 2.044408515910618, 1.4214134220965207, 1.1197641220642254, 0.3710366659797728, 0.3671548960264772, 0.3672111810883507, 0.3603573850123212, 0.3614869370358065, 0.36270310601685196, 0.36109838902484626, 0.35993031703401357, 0.3565416340716183, 0.3456794270314276, 0.3466373310657218, 0.3495594068663195, 0.3452714978484437, 0.345770665910095, 0.3506708990316838, 0.3492955759866163, 0.38437382807023823, 2.49600282299798, 6.283647094969638, 0.3318392470246181, 0.3301904440158978, 0.33112334995530546, 0.3262701010098681, 0.32852982403710485, 0.3274373939493671, 0.33014452701900154, 0.3359497310593724, 0.3304560160031542, 0.32703987101558596, 6.9355597399408, 0.4313729549758136, 0.3415411659516394, 0.3376557899173349, 0.340548946056515, 0.3390649009961635, 0.33510708902031183, 5.9864561159629375, 8.58530032786075, 0.35590433108154684, 0.34466857789084315, 0.3422566739609465, 0.3509080717340112, 0.34697955509182066, 0.3467403131071478, 0.3720713489456102, 3.0805517341941595, 5.0109451349126175, 0.3537109608296305, 0.35988326999358833, 0.34610274515580386, 0.343524924130179, 0.344766044174321, 0.34323143505025655, 0.34643908799625933, 0.3471700489753857, 0.3419534619897604, 0.3430945680011064, 0.36317118583247066, 0.3646984670776874, 0.3441412370884791, 0.3436664171749726, 0.3436224309261888, 0.3403622079640627, 6.061206821119413, 1.5923514199675992, 0.3233986860141158, 0.32173564401455224, 0.3205642600078136, 0.32954564807005227, 0.33719104109331965, 0.4361002161167562, 0.33404941693879664, 0.33791214879602194, 0.3353727828944102, 0.3827096540480852, 0.3402253311360255, 0.32627502689138055, 0.3255485659465194, 0.34423125092871487, 3.15313569502905, 4.366754961083643, 1.8016857729526237, 0.3594835699768737, 0.3581790248863399, 0.3599916729144752, 0.36039965506643057, 0.3554761150153354, 0.356321377097629, 0.3567397288279608, 2.4010210600681603, 4.33055788197089, 0.4439016890246421, 0.3779781301273033, 0.37357520207297057, 0.3734155319398269, 0.3589896430494264, 0.3616075289901346, 0.3625420810421929, 0.35882300103548914, 0.35819062287919223, 0.3566190211568028, 0.35603526304475963, 0.3582183290272951, 7.7048700619488955, 4.108974049100652, 0.6073998269857839, 0.37087830307427794, 0.3701708111912012, 0.3668948949780315, 0.36799270287156105, 0.3661339631071314, 0.3662446920061484, 0.38942060514818877, 0.3931120560737327, 3.334202175028622, 2.6087780668167397, 0.36420102301053703, 0.3608195760753006, 0.3610902400687337, 0.35062747297342867, 0.33644134597852826, 0.3369904700666666, 0.3356727068312466, 0.33751439105253667, 0.3366888299351558, 0.37016356410458684, 0.36201099096797407, 0.34478228411171585, 0.3441518390318379, 2.9631890980526805, 4.241927940165624, 0.3751391270197928, 0.3819432408781722, 0.3654171519447118, 0.3645374459447339, 0.36142101406585425, 0.38073789002373815, 0.34637335187289864, 0.3380340279545635, 0.33545792009681463, 2.849907591124065, 5.436297988053411, 0.33644099987577647, 0.3347772470442578, 0.3288615520577878, 0.36177430604584515, 0.32476106099784374, 0.32777081697713584, 0.32440934085752815, 0.32718251703772694, 0.32897788390982896, 4.428870304953307, 1.0887283169431612, 0.3516640510642901, 0.349619589978829, 0.3440272267907858, 0.34179178380873054, 0.3337915779557079, 0.3336407449096441, 0.3360260840272531, 0.34555249521508813, 0.3650512979365885, 0.36459543101955205, 0.36419683403801173, 0.3661816450767219, 7.364501175004989, 7.51874090207275, 1.1129047040594742, 0.3443766199052334, 0.3426252199569717, 0.3417117960052565, 0.34478468098677695, 0.343149948050268, 0.34245666197966784, 0.3414590619504452, 0.3417076071491465, 5.414116673870012, 5.294785809004679, 0.3518875549780205, 0.3487880389438942, 0.34683936601504683, 0.34720263886265457, 0.3459662700770423, 0.34200937394052744, 2.895012697088532, 2.7407504779985175, 0.32338713807985187, 0.3605498820543289, 0.33306918188463897, 0.3304050149163231, 0.334542035125196, 0.33704199199564755, 0.40744128613732755, 0.3253040869021788, 0.9306352860294282, 6.556501811021008, 0.3551014281110838, 0.3525343640940264, 0.349847940960899, 0.34006018901709467, 0.32744138687849045, 0.3277115050004795, 0.327443263027817, 0.3491881008958444, 0.38470455701462924, 4.12980349897407, 2.9083500429987907, 0.3224008629331365, 0.32459921983536333, 0.33225439791567624, 0.33547518588602543, 0.3357807390857488, 0.33270941907539964, 6.334462692029774, 5.275078818900511, 0.3377383011393249, 0.3487271830672398, 0.3473516749218106, 0.3477320648962632, 0.34690067591145635, 0.34715981408953667, 1.121183184091933, 10.139684263966046, 0.38726955000311136, 0.3437836499651894, 0.34207004110794514, 0.34305531601421535, 0.3435706009622663, 0.34174654714297503, 0.34222467988729477, 0.3414522660896182, 0.34076833410654217, 0.34181723196525127, 0.34051654709037393, 0.3398730930639431, 0.34146700403653085, 0.3379866679897532, 0.33830534608568996, 0.33875095611438155, 0.33897834294475615, 0.33842427795752883, 0.3384116970701143, 0.33677690604235977, 0.33980265888385475, 0.3420995681080967, 0.3455652929842472, 0.34499681496527046, 0.3912027641199529, 3.461057608947158, 5.39230818010401, 0.34015338809695095, 0.3293878921540454, 0.3290559609886259, 0.3299276860198006, 0.32981252600438893, 0.32918683090247214, 0.32849559315945953, 4.35589574999176, 1.6964873319957405, 0.36334540508687496, 0.3562976310495287, 0.361099852132611, 0.35956929193343967, 0.3648675160948187, 0.35821378882974386, 0.3585993030574173, 0.38043103099334985, 2.696468200883828, 3.3247096120612696, 0.36731807608157396, 0.35572582797612995, 0.3526992261176929, 0.3513243329944089, 0.3581092129461467, 0.3511536280857399, 0.3543191631324589, 0.3591798839624971, 2.923498790129088, 5.722741577075794, 0.3813949559116736, 0.3765489789657295, 0.37447208003140986, 0.377972193993628, 0.3578819171525538, 0.34961994690820575, 0.35215169994626194, 0.3454073560424149, 0.3457385399378836, 0.350276364129968, 2.076760584837757, 8.956783503876068, 1.896469717961736, 0.3816635940456763, 0.3735977441538125, 0.3499118429608643, 0.3334439860191196, 0.3330630719428882, 7.32278688903898, 0.43420600495301187, 0.35609145904891193, 0.40101046196650714, 0.443050246918574, 0.3463883940130472, 4.9956769129494205, 0.37140874203760177, 0.35919431888032705, 0.35474381409585476, 0.34977425599936396, 0.3421172689413652, 0.33921320200897753, 3.967183263041079, 0.4455843560863286, 0.34073346911463886, 0.33882416086271405, 0.33950615383218974, 0.3444180120714009, 0.34013334894552827, 0.33714351803064346, 0.3396126660518348, 5.229480785899796, 5.30242631805595, 0.3475636028451845, 0.3452491380739957, 0.34518174186814576, 0.34383456595242023, 0.341959115001373, 0.34028134401887655, 0.3373318569501862, 0.3377450780244544, 0.3379881370346993, 0.3400914069497958, 2.9042289000935853, 4.646248901030049, 0.36101389001123607, 0.3593963279854506, 0.33785813907161355, 0.34238847403321415, 0.6202701010042801, 6.2077620279742405, 0.38065517507493496, 0.35905389906838536, 0.3509851590497419, 2.9789813570678234, 0.33158986596390605, 0.3333622639765963, 0.32906397606711835]
Total Epoch List: [846, 7, 4]
Total Time List: [0.07254559500142932, 0.07814370095729828, 0.07340241991914809]
T-times Epoch Time: 0.8136675806147978 ~ 0.08315325311946535
T-times Total Epoch: 357.1111111111111 ~ 60.37925406120593
T-times Total Time: 0.16571970243886527 ~ 0.1291704320606834
T-times Inference Elapsed: 0.16047369734022202 ~ 0.016368820767185474
T-times Time Per Graph: 0.0032802256085744164 ~ 0.00033071869348357376
T-times Speed: 654.5820957445666 ~ 7.584339003020455
T-times cross validation test micro f1 score:0.7270928462709284 ~ 0.06951852046675941
T-times cross validation test precision:0.6819481415754708 ~ 0.04453389694803598
T-times cross validation test recall:0.8413888888888889 ~ 0.15210914291397706
T-times cross validation test f1_score:0.7270928462709284 ~ 0.06543434876419664
