Namespace(seed=15, model='Ethident', dataset='ico_wallets/averVolume', num_heads=1, num_layers=2, dim_hidden=64, dropout=0.1, epochs=1000, lr=0.0003, weight_decay=0.0001, batch_size=64, abs_pe=None, abs_pe_dim=20, num_class=2, outdir='./outdir/ico_wallets/averVolume/seed15/edge_attr/khopgnn_gat_1_0.1_0.0003_0.0001_2_1_64_BN', warmup=10, layer_norm=False, use_edge_attr=True, edge_dim=128, gnn_type='gat', k_hop=1, global_pool='mean', se='khopgnn', ie='mamba', aggr='add', not_extract_node_feature=False, k_ford=3, early_stop=1, early_stop_mindelta=-0.0, patience=20, training_times=3, Lambda=0.01, order='sd', use_cuda=True, batch_norm=True, save_logs=True)
Data(edge_index=[2, 342], edge_attr=[342, 2], x=[99, 14887], y=[1, 1], num_nodes=99)
Data(edge_index=[2, 342], edge_attr=[342, 2], x=[99, 14887], y=[1, 1], num_nodes=99)
Data(edge_index=[2, 306], edge_attr=[306, 2], x=[90, 14887], y=[1, 1], num_nodes=99)
========================training times:0========================
========================k_idx:0========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
emb dim:  64
Model: Ethident
Ethident(
  (encoder): HGATE_encoder(
    (lin1): Linear(in_features=14887, out_features=64, bias=True)
    (account_convs): ModuleList(
      (0): GATEConv()
      (1): GATConv(64, 64, heads=1)
    )
    (subgraph_conv): GATConv(64, 64, heads=1)
    (lin2): Linear(in_features=64, out_features=64, bias=True)
    (lin3): Linear(in_features=64, out_features=2, bias=True)
  )
  (proj_head_g1): Project_Head(
    (block): Sequential(
      (0): Linear(in_features=64, out_features=64, bias=True)
      (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=64, bias=True)
      (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
      (5): ReLU()
    )
    (linear_shortcut): Linear(in_features=64, out_features=64, bias=True)
  )
  (proj_head_g2): Project_Head(
    (block): Sequential(
      (0): Linear(in_features=64, out_features=64, bias=True)
      (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=64, bias=True)
      (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
      (5): ReLU()
    )
    (linear_shortcut): Linear(in_features=64, out_features=64, bias=True)
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=64, bias=True)
    (1): ReLU()
    (2): Dropout(p=0.0, inplace=True)
    (3): Linear(in_features=64, out_features=2, bias=True)
  )
)
Total number of parameters: 1003972
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7a49927ee080>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.8953;  Loss pred: 0.8650; Loss self: 3.0331; time: 0.54s
Val loss: 0.8998 score: 0.2653 time: 0.21s
Test loss: 0.8294 score: 0.3265 time: 0.08s
Epoch 2/1000, LR 0.000000
Train loss: 0.8953;  Loss pred: 0.8650; Loss self: 3.0331; time: 0.43s
Val loss: 0.8330 score: 0.2857 time: 0.14s
Test loss: 0.7711 score: 0.3878 time: 0.07s
Epoch 3/1000, LR 0.000030
Train loss: 0.8313;  Loss pred: 0.8004; Loss self: 3.0909; time: 0.27s
Val loss: 0.7469 score: 0.3673 time: 0.11s
Test loss: 0.7214 score: 0.4898 time: 0.20s
Epoch 4/1000, LR 0.000060
Train loss: 0.7690;  Loss pred: 0.7371; Loss self: 3.1928; time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6918 score: 0.4898 time: 0.15s
Test loss: 0.7055 score: 0.4898 time: 0.11s
Epoch 5/1000, LR 0.000090
Train loss: 0.7246;  Loss pred: 0.6915; Loss self: 3.3052; time: 0.29s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6530 score: 0.4898 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6787 score: 0.5102 time: 0.16s
Epoch 6/1000, LR 0.000120
Train loss: 0.6887;  Loss pred: 0.6548; Loss self: 3.3984; time: 0.29s
Val loss: 0.6181 score: 0.5714 time: 0.14s
Test loss: 0.6460 score: 0.6531 time: 0.07s
Epoch 7/1000, LR 0.000150
Train loss: 0.6514;  Loss pred: 0.6168; Loss self: 3.4685; time: 0.30s
Val loss: 0.5946 score: 0.7755 time: 0.08s
Test loss: 0.6224 score: 0.6735 time: 0.13s
Epoch 8/1000, LR 0.000180
Train loss: 0.6245;  Loss pred: 0.5894; Loss self: 3.5187; time: 0.29s
Val loss: 0.5782 score: 0.8571 time: 0.13s
Test loss: 0.6070 score: 0.7959 time: 0.18s
Epoch 9/1000, LR 0.000210
Train loss: 0.6023;  Loss pred: 0.5669; Loss self: 3.5465; time: 0.26s
Val loss: 0.5543 score: 0.8571 time: 0.09s
Test loss: 0.5889 score: 0.8163 time: 0.17s
Epoch 10/1000, LR 0.000240
Train loss: 0.5776;  Loss pred: 0.5421; Loss self: 3.5500; time: 0.34s
Val loss: 0.5230 score: 0.8163 time: 0.12s
Test loss: 0.5620 score: 0.8571 time: 0.12s
Epoch 11/1000, LR 0.000270
Train loss: 0.5488;  Loss pred: 0.5136; Loss self: 3.5215; time: 0.26s
Val loss: 0.4934 score: 0.7551 time: 0.08s
Test loss: 0.5379 score: 0.7959 time: 0.07s
Epoch 12/1000, LR 0.000270
Train loss: 0.5190;  Loss pred: 0.4843; Loss self: 3.4731; time: 0.24s
Val loss: 0.4715 score: 0.7551 time: 0.07s
Test loss: 0.5175 score: 0.7755 time: 0.19s
Epoch 13/1000, LR 0.000270
Train loss: 0.4949;  Loss pred: 0.4607; Loss self: 3.4189; time: 0.26s
Val loss: 0.4527 score: 0.7551 time: 0.08s
Test loss: 0.5006 score: 0.7755 time: 0.09s
Epoch 14/1000, LR 0.000270
Train loss: 0.4698;  Loss pred: 0.4361; Loss self: 3.3731; time: 0.25s
Val loss: 0.4315 score: 0.7959 time: 0.12s
Test loss: 0.4835 score: 0.7959 time: 0.10s
Epoch 15/1000, LR 0.000270
Train loss: 0.4411;  Loss pred: 0.4077; Loss self: 3.3400; time: 0.23s
Val loss: 0.4110 score: 0.8980 time: 0.08s
Test loss: 0.4719 score: 0.8571 time: 0.07s
Epoch 16/1000, LR 0.000270
Train loss: 0.4115;  Loss pred: 0.3783; Loss self: 3.3162; time: 0.24s
Val loss: 0.3966 score: 0.8980 time: 0.09s
Test loss: 0.4690 score: 0.8367 time: 0.07s
Epoch 17/1000, LR 0.000270
Train loss: 0.3864;  Loss pred: 0.3535; Loss self: 3.2908; time: 0.24s
Val loss: 0.3748 score: 0.8776 time: 0.08s
Test loss: 0.4560 score: 0.8367 time: 0.08s
Epoch 18/1000, LR 0.000270
Train loss: 0.3605;  Loss pred: 0.3280; Loss self: 3.2526; time: 0.25s
Val loss: 0.3416 score: 0.8980 time: 0.07s
Test loss: 0.4302 score: 0.8367 time: 0.08s
Epoch 19/1000, LR 0.000270
Train loss: 0.3301;  Loss pred: 0.2981; Loss self: 3.2026; time: 0.30s
Val loss: 0.3140 score: 0.8980 time: 0.09s
Test loss: 0.4074 score: 0.8367 time: 0.11s
Epoch 20/1000, LR 0.000270
Train loss: 0.3046;  Loss pred: 0.2731; Loss self: 3.1515; time: 0.28s
Val loss: 0.2929 score: 0.8980 time: 0.08s
Test loss: 0.3940 score: 0.8367 time: 0.08s
Epoch 21/1000, LR 0.000270
Train loss: 0.2824;  Loss pred: 0.2513; Loss self: 3.1073; time: 0.28s
Val loss: 0.2747 score: 0.8980 time: 0.08s
Test loss: 0.3843 score: 0.8367 time: 0.08s
Epoch 22/1000, LR 0.000270
Train loss: 0.2574;  Loss pred: 0.2267; Loss self: 3.0653; time: 0.25s
Val loss: 0.2626 score: 0.8980 time: 0.07s
Test loss: 0.3799 score: 0.8367 time: 0.12s
Epoch 23/1000, LR 0.000270
Train loss: 0.2353;  Loss pred: 0.2050; Loss self: 3.0264; time: 0.28s
Val loss: 0.2554 score: 0.8980 time: 0.08s
Test loss: 0.3805 score: 0.8367 time: 0.12s
Epoch 24/1000, LR 0.000270
Train loss: 0.2170;  Loss pred: 0.1872; Loss self: 2.9871; time: 0.24s
Val loss: 0.2491 score: 0.8776 time: 0.08s
Test loss: 0.3810 score: 0.8367 time: 0.07s
Epoch 25/1000, LR 0.000270
Train loss: 0.1979;  Loss pred: 0.1684; Loss self: 2.9461; time: 0.25s
Val loss: 0.2418 score: 0.8776 time: 0.08s
Test loss: 0.3782 score: 0.8367 time: 0.07s
Epoch 26/1000, LR 0.000270
Train loss: 0.1799;  Loss pred: 0.1508; Loss self: 2.9068; time: 0.23s
Val loss: 0.2332 score: 0.8980 time: 0.07s
Test loss: 0.3773 score: 0.8367 time: 0.07s
Epoch 27/1000, LR 0.000270
Train loss: 0.1655;  Loss pred: 0.1368; Loss self: 2.8728; time: 0.23s
Val loss: 0.2225 score: 0.8980 time: 0.08s
Test loss: 0.3783 score: 0.8367 time: 0.07s
Epoch 28/1000, LR 0.000270
Train loss: 0.1529;  Loss pred: 0.1245; Loss self: 2.8466; time: 0.28s
Val loss: 0.2139 score: 0.8980 time: 0.08s
Test loss: 0.3789 score: 0.8367 time: 0.08s
Epoch 29/1000, LR 0.000270
Train loss: 0.1415;  Loss pred: 0.1132; Loss self: 2.8276; time: 0.30s
Val loss: 0.2072 score: 0.8980 time: 0.08s
Test loss: 0.3797 score: 0.8367 time: 0.11s
Epoch 30/1000, LR 0.000270
Train loss: 0.1323;  Loss pred: 0.1042; Loss self: 2.8138; time: 0.24s
Val loss: 0.2031 score: 0.8980 time: 0.08s
Test loss: 0.3846 score: 0.8571 time: 0.07s
Epoch 31/1000, LR 0.000270
Train loss: 0.1262;  Loss pred: 0.0981; Loss self: 2.8048; time: 0.30s
Val loss: 0.2011 score: 0.8980 time: 0.10s
Test loss: 0.3921 score: 0.8571 time: 0.07s
Epoch 32/1000, LR 0.000270
Train loss: 0.1219;  Loss pred: 0.0939; Loss self: 2.7993; time: 0.24s
Val loss: 0.1963 score: 0.8980 time: 0.08s
Test loss: 0.3991 score: 0.8571 time: 0.10s
Epoch 33/1000, LR 0.000270
Train loss: 0.1188;  Loss pred: 0.0908; Loss self: 2.7962; time: 0.27s
Val loss: 0.1907 score: 0.8980 time: 0.08s
Test loss: 0.4026 score: 0.8571 time: 0.07s
Epoch 34/1000, LR 0.000270
Train loss: 0.1159;  Loss pred: 0.0879; Loss self: 2.7960; time: 0.29s
Val loss: 0.1833 score: 0.8980 time: 0.09s
Test loss: 0.4020 score: 0.8571 time: 0.07s
Epoch 35/1000, LR 0.000270
Train loss: 0.1132;  Loss pred: 0.0852; Loss self: 2.7974; time: 0.25s
Val loss: 0.1760 score: 0.9184 time: 0.08s
Test loss: 0.3975 score: 0.8571 time: 0.09s
Epoch 36/1000, LR 0.000270
Train loss: 0.1102;  Loss pred: 0.0822; Loss self: 2.7998; time: 0.26s
Val loss: 0.1690 score: 0.9184 time: 0.08s
Test loss: 0.3893 score: 0.8571 time: 0.07s
Epoch 37/1000, LR 0.000270
Train loss: 0.1071;  Loss pred: 0.0791; Loss self: 2.8027; time: 0.24s
Val loss: 0.1630 score: 0.9184 time: 0.07s
Test loss: 0.3804 score: 0.8571 time: 0.07s
Epoch 38/1000, LR 0.000270
Train loss: 0.1041;  Loss pred: 0.0761; Loss self: 2.8060; time: 0.23s
Val loss: 0.1574 score: 0.9388 time: 0.07s
Test loss: 0.3718 score: 0.8571 time: 0.07s
Epoch 39/1000, LR 0.000269
Train loss: 0.1013;  Loss pred: 0.0732; Loss self: 2.8097; time: 0.23s
Val loss: 0.1527 score: 0.9388 time: 0.11s
Test loss: 0.3647 score: 0.8571 time: 0.08s
Epoch 40/1000, LR 0.000269
Train loss: 0.0985;  Loss pred: 0.0704; Loss self: 2.8132; time: 0.25s
Val loss: 0.1489 score: 0.9388 time: 0.08s
Test loss: 0.3588 score: 0.8571 time: 0.07s
Epoch 41/1000, LR 0.000269
Train loss: 0.0958;  Loss pred: 0.0676; Loss self: 2.8162; time: 0.23s
Val loss: 0.1461 score: 0.9388 time: 0.08s
Test loss: 0.3542 score: 0.8776 time: 0.07s
Epoch 42/1000, LR 0.000269
Train loss: 0.0931;  Loss pred: 0.0649; Loss self: 2.8183; time: 0.25s
Val loss: 0.1443 score: 0.9388 time: 0.07s
Test loss: 0.3507 score: 0.8776 time: 0.12s
Epoch 43/1000, LR 0.000269
Train loss: 0.0903;  Loss pred: 0.0621; Loss self: 2.8201; time: 0.25s
Val loss: 0.1430 score: 0.9592 time: 0.12s
Test loss: 0.3484 score: 0.8776 time: 0.08s
Epoch 44/1000, LR 0.000269
Train loss: 0.0875;  Loss pred: 0.0593; Loss self: 2.8216; time: 0.25s
Val loss: 0.1417 score: 0.9592 time: 0.09s
Test loss: 0.3465 score: 0.8776 time: 0.08s
Epoch 45/1000, LR 0.000269
Train loss: 0.0847;  Loss pred: 0.0564; Loss self: 2.8223; time: 0.25s
Val loss: 0.1406 score: 0.9592 time: 0.08s
Test loss: 0.3445 score: 0.8776 time: 0.11s
Epoch 46/1000, LR 0.000269
Train loss: 0.0818;  Loss pred: 0.0536; Loss self: 2.8225; time: 0.27s
Val loss: 0.1393 score: 0.9592 time: 0.14s
Test loss: 0.3425 score: 0.8776 time: 0.09s
Epoch 47/1000, LR 0.000269
Train loss: 0.0790;  Loss pred: 0.0508; Loss self: 2.8224; time: 0.24s
Val loss: 0.1380 score: 0.9592 time: 0.08s
Test loss: 0.3409 score: 0.8776 time: 0.07s
Epoch 48/1000, LR 0.000269
Train loss: 0.0760;  Loss pred: 0.0478; Loss self: 2.8218; time: 0.22s
Val loss: 0.1368 score: 0.9592 time: 0.08s
Test loss: 0.3391 score: 0.8776 time: 0.07s
Epoch 49/1000, LR 0.000269
Train loss: 0.0729;  Loss pred: 0.0447; Loss self: 2.8202; time: 0.24s
Val loss: 0.1357 score: 0.9592 time: 0.08s
Test loss: 0.3374 score: 0.8980 time: 0.08s
Epoch 50/1000, LR 0.000269
Train loss: 0.0698;  Loss pred: 0.0417; Loss self: 2.8182; time: 0.25s
Val loss: 0.1347 score: 0.9592 time: 0.13s
Test loss: 0.3360 score: 0.9184 time: 0.07s
Epoch 51/1000, LR 0.000269
Train loss: 0.0669;  Loss pred: 0.0388; Loss self: 2.8155; time: 0.25s
Val loss: 0.1334 score: 0.9592 time: 0.08s
Test loss: 0.3344 score: 0.9184 time: 0.07s
Epoch 52/1000, LR 0.000269
Train loss: 0.0642;  Loss pred: 0.0360; Loss self: 2.8122; time: 0.24s
Val loss: 0.1322 score: 0.9592 time: 0.08s
Test loss: 0.3339 score: 0.9184 time: 0.07s
Epoch 53/1000, LR 0.000269
Train loss: 0.0617;  Loss pred: 0.0336; Loss self: 2.8089; time: 0.24s
Val loss: 0.1309 score: 0.9592 time: 0.08s
Test loss: 0.3352 score: 0.9184 time: 0.07s
Epoch 54/1000, LR 0.000269
Train loss: 0.0594;  Loss pred: 0.0313; Loss self: 2.8060; time: 0.23s
Val loss: 0.1300 score: 0.9592 time: 0.09s
Test loss: 0.3390 score: 0.9184 time: 0.07s
Epoch 55/1000, LR 0.000269
Train loss: 0.0573;  Loss pred: 0.0293; Loss self: 2.8029; time: 0.28s
Val loss: 0.1292 score: 0.9592 time: 0.09s
Test loss: 0.3452 score: 0.9184 time: 0.14s
Epoch 56/1000, LR 0.000269
Train loss: 0.0553;  Loss pred: 0.0273; Loss self: 2.7998; time: 0.31s
Val loss: 0.1278 score: 0.9592 time: 0.13s
Test loss: 0.3527 score: 0.8980 time: 0.09s
Epoch 57/1000, LR 0.000269
Train loss: 0.0531;  Loss pred: 0.0252; Loss self: 2.7971; time: 0.26s
Val loss: 0.1267 score: 0.9592 time: 0.08s
Test loss: 0.3613 score: 0.8980 time: 0.07s
Epoch 58/1000, LR 0.000269
Train loss: 0.0510;  Loss pred: 0.0231; Loss self: 2.7947; time: 0.24s
Val loss: 0.1257 score: 0.9592 time: 0.12s
Test loss: 0.3708 score: 0.8980 time: 0.08s
Epoch 59/1000, LR 0.000268
Train loss: 0.0491;  Loss pred: 0.0211; Loss self: 2.7927; time: 0.25s
Val loss: 0.1251 score: 0.9592 time: 0.08s
Test loss: 0.3806 score: 0.8980 time: 0.08s
Epoch 60/1000, LR 0.000268
Train loss: 0.0473;  Loss pred: 0.0194; Loss self: 2.7909; time: 0.23s
Val loss: 0.1248 score: 0.9592 time: 0.08s
Test loss: 0.3889 score: 0.8776 time: 0.07s
Epoch 61/1000, LR 0.000268
Train loss: 0.0456;  Loss pred: 0.0177; Loss self: 2.7895; time: 0.23s
Val loss: 0.1249 score: 0.9592 time: 0.08s
Test loss: 0.3960 score: 0.8776 time: 0.08s
     INFO: Early stopping counter 1 of 20
Epoch 62/1000, LR 0.000268
Train loss: 0.0441;  Loss pred: 0.0162; Loss self: 2.7883; time: 0.23s
Val loss: 0.1253 score: 0.9592 time: 0.08s
Test loss: 0.4023 score: 0.8776 time: 0.14s
     INFO: Early stopping counter 2 of 20
Epoch 63/1000, LR 0.000268
Train loss: 0.0426;  Loss pred: 0.0147; Loss self: 2.7873; time: 0.27s
Val loss: 0.1261 score: 0.9592 time: 0.08s
Test loss: 0.4059 score: 0.8776 time: 0.09s
     INFO: Early stopping counter 3 of 20
Epoch 64/1000, LR 0.000268
Train loss: 0.0413;  Loss pred: 0.0135; Loss self: 2.7867; time: 0.23s
Val loss: 0.1268 score: 0.9592 time: 0.21s
Test loss: 0.4083 score: 0.8776 time: 0.07s
     INFO: Early stopping counter 4 of 20
Epoch 65/1000, LR 0.000268
Train loss: 0.0402;  Loss pred: 0.0123; Loss self: 2.7859; time: 0.24s
Val loss: 0.1276 score: 0.9592 time: 0.07s
Test loss: 0.4098 score: 0.8776 time: 0.08s
     INFO: Early stopping counter 5 of 20
Epoch 66/1000, LR 0.000268
Train loss: 0.0392;  Loss pred: 0.0113; Loss self: 2.7853; time: 0.24s
Val loss: 0.1283 score: 0.9592 time: 0.08s
Test loss: 0.4100 score: 0.8776 time: 0.07s
     INFO: Early stopping counter 6 of 20
Epoch 67/1000, LR 0.000268
Train loss: 0.0383;  Loss pred: 0.0104; Loss self: 2.7847; time: 0.23s
Val loss: 0.1292 score: 0.9592 time: 0.09s
Test loss: 0.4096 score: 0.8776 time: 0.10s
     INFO: Early stopping counter 7 of 20
Epoch 68/1000, LR 0.000268
Train loss: 0.0375;  Loss pred: 0.0097; Loss self: 2.7840; time: 0.35s
Val loss: 0.1303 score: 0.9592 time: 0.09s
Test loss: 0.4093 score: 0.8776 time: 0.07s
     INFO: Early stopping counter 8 of 20
Epoch 69/1000, LR 0.000268
Train loss: 0.0369;  Loss pred: 0.0091; Loss self: 2.7836; time: 0.26s
Val loss: 0.1315 score: 0.9592 time: 0.11s
Test loss: 0.4093 score: 0.8776 time: 0.08s
     INFO: Early stopping counter 9 of 20
Epoch 70/1000, LR 0.000268
Train loss: 0.0364;  Loss pred: 0.0086; Loss self: 2.7831; time: 0.28s
Val loss: 0.1330 score: 0.9592 time: 0.09s
Test loss: 0.4095 score: 0.8776 time: 0.07s
     INFO: Early stopping counter 10 of 20
Epoch 71/1000, LR 0.000268
Train loss: 0.0359;  Loss pred: 0.0081; Loss self: 2.7829; time: 0.25s
Val loss: 0.1343 score: 0.9592 time: 0.08s
Test loss: 0.4093 score: 0.8980 time: 0.09s
     INFO: Early stopping counter 11 of 20
Epoch 72/1000, LR 0.000267
Train loss: 0.0354;  Loss pred: 0.0076; Loss self: 2.7835; time: 0.30s
Val loss: 0.1357 score: 0.9592 time: 0.12s
Test loss: 0.4084 score: 0.8980 time: 0.10s
     INFO: Early stopping counter 12 of 20
Epoch 73/1000, LR 0.000267
Train loss: 0.0350;  Loss pred: 0.0072; Loss self: 2.7842; time: 0.25s
Val loss: 0.1373 score: 0.9592 time: 0.12s
Test loss: 0.4077 score: 0.8980 time: 0.07s
     INFO: Early stopping counter 13 of 20
Epoch 74/1000, LR 0.000267
Train loss: 0.0347;  Loss pred: 0.0068; Loss self: 2.7850; time: 0.26s
Val loss: 0.1386 score: 0.9592 time: 0.09s
Test loss: 0.4073 score: 0.8980 time: 0.09s
     INFO: Early stopping counter 14 of 20
Epoch 75/1000, LR 0.000267
Train loss: 0.0344;  Loss pred: 0.0065; Loss self: 2.7857; time: 0.26s
Val loss: 0.1397 score: 0.9592 time: 0.08s
Test loss: 0.4074 score: 0.8980 time: 0.08s
     INFO: Early stopping counter 15 of 20
Epoch 76/1000, LR 0.000267
Train loss: 0.0341;  Loss pred: 0.0062; Loss self: 2.7863; time: 0.25s
Val loss: 0.1408 score: 0.9592 time: 0.08s
Test loss: 0.4079 score: 0.8980 time: 0.07s
     INFO: Early stopping counter 16 of 20
Epoch 77/1000, LR 0.000267
Train loss: 0.0338;  Loss pred: 0.0059; Loss self: 2.7872; time: 0.23s
Val loss: 0.1420 score: 0.9592 time: 0.07s
Test loss: 0.4090 score: 0.8980 time: 0.07s
     INFO: Early stopping counter 17 of 20
Epoch 78/1000, LR 0.000267
Train loss: 0.0336;  Loss pred: 0.0057; Loss self: 2.7881; time: 0.23s
Val loss: 0.1429 score: 0.9592 time: 0.08s
Test loss: 0.4106 score: 0.8980 time: 0.07s
     INFO: Early stopping counter 18 of 20
Epoch 79/1000, LR 0.000267
Train loss: 0.0334;  Loss pred: 0.0055; Loss self: 2.7888; time: 0.25s
Val loss: 0.1439 score: 0.9592 time: 0.09s
Test loss: 0.4125 score: 0.8980 time: 0.07s
     INFO: Early stopping counter 19 of 20
Epoch 80/1000, LR 0.000267
Train loss: 0.0332;  Loss pred: 0.0053; Loss self: 2.7894; time: 0.27s
Val loss: 0.1449 score: 0.9592 time: 0.10s
Test loss: 0.4148 score: 0.8980 time: 0.09s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 059,   Train_Loss: 0.0473,   Val_Loss: 0.1248,   Val_Precision: 0.9583,   Val_Recall: 0.9583,   Val_accuracy: 0.9583,   Val_Score: 0.9592,   Val_Loss: 0.1248,   Test_Precision: 0.9524,   Test_Recall: 0.8000,   Test_accuracy: 0.8696,   Test_Score: 0.8776,   Test_loss: 0.3889


[0.08189915004186332, 0.07032897300086915, 0.2000812040641904, 0.11548979592043906, 0.16476947499904782, 0.07549699698574841, 0.12960480398032814, 0.1890471070073545, 0.17717552790418267, 0.12348108203150332, 0.07155381294433028, 0.19880801695398986, 0.09736896597314626, 0.10130585089791566, 0.07399243593681604, 0.07519998506177217, 0.0793704700190574, 0.08271386101841927, 0.11638969497289509, 0.08626393193844706, 0.08731207496020943, 0.12004659604281187, 0.12130114994943142, 0.07769500906579196, 0.07229159097187221, 0.07035359391011298, 0.07780947908759117, 0.08315639209467918, 0.1127208910183981, 0.07827799406368285, 0.0766982800560072, 0.10486559697892517, 0.07392151898238808, 0.0756055599777028, 0.09071328502614051, 0.07278257491998374, 0.07243742200080305, 0.07013582508079708, 0.08350524003617465, 0.07770106208045036, 0.07253592403139919, 0.12717502401210368, 0.08287368901073933, 0.08508335694205016, 0.11291082995012403, 0.09067827300168574, 0.07091463205870241, 0.07161686499603093, 0.08879717194940895, 0.07631555804982781, 0.07724104495719075, 0.07576603908091784, 0.07310851104557514, 0.07906504301354289, 0.1424025340238586, 0.09793070401065052, 0.07772832200862467, 0.08040315401740372, 0.08314119500573725, 0.07568398502189666, 0.08069918700493872, 0.1418736519990489, 0.09408001904375851, 0.07779310189653188, 0.08893538801930845, 0.07352509500924498, 0.1087808939628303, 0.07467471505515277, 0.081925316946581, 0.07907126809004694, 0.09015332104172558, 0.10613057401496917, 0.07906635396648198, 0.09367153700441122, 0.08064793492667377, 0.07870431093033403, 0.07251563505269587, 0.07551405299454927, 0.07852227601688355, 0.09858832997269928]
[0.0016714112253441494, 0.0014352851632830439, 0.004083289878861029, 0.0023569346106212053, 0.003362642346919343, 0.0015407550405254777, 0.0026449959995985335, 0.0038581042246398876, 0.003615827100085361, 0.002520022082275578, 0.0014602818968230669, 0.004057306468448773, 0.0019871217545540053, 0.0020674663448554216, 0.0015100497129962457, 0.0015346935726892278, 0.001619805510593008, 0.00168803797996774, 0.0023752998974060223, 0.0017604884069070828, 0.0017818790808206005, 0.0024499305314859567, 0.002475533672437376, 0.0015856124299141218, 0.0014753385912626982, 0.0014357876308186321, 0.001587948552807983, 0.001697069226422024, 0.002300426347314247, 0.001597510082932303, 0.0015652710215511676, 0.0021401142240596973, 0.0015086024282120016, 0.001542970611789853, 0.0018512915311457248, 0.001485358671836403, 0.0014783147347102665, 0.0014313433689958587, 0.0017041885721668297, 0.0015857359608255175, 0.0014803249802326365, 0.0025954086533082382, 0.001691299775729374, 0.0017363950396336767, 0.0023043026520433475, 0.0018505770000344028, 0.0014472373889531105, 0.0014615686733883862, 0.001812187182640999, 0.001557460368363833, 0.001576347856269199, 0.0015462456955289354, 0.0014920104295015335, 0.0016135723063988344, 0.0029061741637522163, 0.001998585796135725, 0.0015862922858902995, 0.001640880694232729, 0.0016967590817497397, 0.0015445711228958502, 0.0016469221837742596, 0.002895380653041814, 0.0019200003886481328, 0.001587614324419018, 0.0018150079187613968, 0.001500512143045816, 0.002220018244139394, 0.001523973776635771, 0.0016719452438077756, 0.0016136993487764681, 0.0018398636947290934, 0.0021659300819381463, 0.0016135990605404486, 0.0019116640204981882, 0.0016458762229933422, 0.0016062104271496742, 0.0014799109194427729, 0.0015411031223377402, 0.0016024954289159908, 0.00201200673413672]
[598.296807414403, 696.7256581351535, 244.9005653938375, 424.2799081033627, 297.3851801147219, 649.0324377968271, 378.07240545988856, 259.1946566952424, 276.56189644034487, 396.82191955119725, 684.799285792395, 246.46893395320205, 503.24042686777517, 483.6838106159983, 662.2298533574743, 651.5958741181864, 617.3580676570866, 592.4037325387139, 420.9994708845242, 568.0241892401051, 561.2053089143819, 408.1748388977666, 403.9533015179769, 630.6711407743952, 677.8105079893083, 696.4818323653043, 629.7433240086345, 589.2511539486969, 434.70202867720684, 625.9741398091548, 638.8669988977438, 467.2647790280308, 662.8651666597156, 648.1004838063607, 540.1634389701558, 673.2380663073543, 676.4459397720805, 698.6443795813562, 586.7895233733008, 630.6220106652627, 675.5273425453157, 385.2957794239261, 591.2612384571206, 575.9058147338221, 433.9707716402821, 540.3720028841867, 690.9716454488306, 684.1963831105372, 551.8193758233327, 642.0709125655218, 634.3777460177712, 646.7277502479467, 670.2366017200634, 619.7429120680664, 344.09500038665306, 500.3538011395382, 630.4008466124227, 609.4288289908835, 589.3588611111339, 647.4289109621205, 607.1932298029377, 345.37773088641217, 520.8332279058012, 629.8758990889974, 550.9617835069408, 666.4391252243712, 450.44674864267034, 656.1792698346408, 598.1057117172974, 619.6941213108969, 543.5185241519986, 461.69542052122324, 619.7326364735651, 523.1044730022148, 607.5791034767534, 622.583431844958, 675.7163467491189, 648.8858438513014, 624.0267410163227, 497.0162291375551]
Elapsed: 0.09382396371365757~0.029555048663475487
Time per graph: 0.0019147747696664809~0.0006031642584382752
Speed: 558.3193946016261~119.60877069963693
Total Time: 0.0991
best val loss: 0.12484066933393478 test_score: 0.8776

Testing...
Test loss: 0.3484 score: 0.8776 time: 0.07s
test Score 0.8776
Epoch Time List: [0.8212122227996588, 0.6402013768674806, 0.5769144799560308, 0.5279414800461382, 0.6287039531162009, 0.4979968130355701, 0.5052409779746085, 0.6093330360017717, 0.5224914151476696, 0.5774432679172605, 0.4158456870354712, 0.5035737900761887, 0.4342434382997453, 0.46042091597337276, 0.3831862809602171, 0.395123238209635, 0.3988121278816834, 0.39748076698742807, 0.5020002210512757, 0.4491215948946774, 0.44488459709100425, 0.4366635399637744, 0.4739067618502304, 0.3935432160506025, 0.4002528830897063, 0.3714926589746028, 0.3902100611012429, 0.4366112999850884, 0.49530278879683465, 0.3933899999829009, 0.4789617419010028, 0.4215195479337126, 0.41999341605696827, 0.4458163959207013, 0.42157638701610267, 0.4057810829253867, 0.3807230358943343, 0.36584602389484644, 0.427049447898753, 0.4000940350815654, 0.37094615097157657, 0.4408616260625422, 0.456954364082776, 0.42501894396264106, 0.4321541058598086, 0.5040275569772348, 0.38766335987020284, 0.36999851383734494, 0.40605602599680424, 0.4518683870555833, 0.404728485038504, 0.3945691171102226, 0.38803102483507246, 0.39753615495283157, 0.5034843608736992, 0.5399608191801235, 0.41708394698798656, 0.4414319481002167, 0.41151884698774666, 0.38057523604948074, 0.3906757839722559, 0.44140008103568107, 0.4366757929092273, 0.5091981129953638, 0.397135401959531, 0.3906647500116378, 0.42305495287291706, 0.514501724857837, 0.44269806996453553, 0.44388697296380997, 0.4151810199255124, 0.5196830501081422, 0.44646251713857055, 0.4408485230524093, 0.4219066429650411, 0.4007214339217171, 0.37524509709328413, 0.3749016427900642, 0.41649599792435765, 0.4609560790704563]
Total Epoch List: [80]
Total Time List: [0.09909544000402093]
========================k_idx:1========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
emb dim:  64
Model: Ethident
Ethident(
  (encoder): HGATE_encoder(
    (lin1): Linear(in_features=14887, out_features=64, bias=True)
    (account_convs): ModuleList(
      (0): GATEConv()
      (1): GATConv(64, 64, heads=1)
    )
    (subgraph_conv): GATConv(64, 64, heads=1)
    (lin2): Linear(in_features=64, out_features=64, bias=True)
    (lin3): Linear(in_features=64, out_features=2, bias=True)
  )
  (proj_head_g1): Project_Head(
    (block): Sequential(
      (0): Linear(in_features=64, out_features=64, bias=True)
      (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=64, bias=True)
      (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
      (5): ReLU()
    )
    (linear_shortcut): Linear(in_features=64, out_features=64, bias=True)
  )
  (proj_head_g2): Project_Head(
    (block): Sequential(
      (0): Linear(in_features=64, out_features=64, bias=True)
      (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=64, bias=True)
      (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
      (5): ReLU()
    )
    (linear_shortcut): Linear(in_features=64, out_features=64, bias=True)
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=64, bias=True)
    (1): ReLU()
    (2): Dropout(p=0.0, inplace=True)
    (3): Linear(in_features=64, out_features=2, bias=True)
  )
)
Total number of parameters: 1003972
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7a49927ee680>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.8435;  Loss pred: 0.8105; Loss self: 3.2983; time: 0.25s
Val loss: 0.7707 score: 0.3673 time: 0.08s
Test loss: 0.7784 score: 0.3469 time: 0.09s
Epoch 2/1000, LR 0.000000
Train loss: 0.8435;  Loss pred: 0.8105; Loss self: 3.2983; time: 0.42s
Val loss: 0.7446 score: 0.4898 time: 0.14s
Test loss: 0.7460 score: 0.4898 time: 0.10s
Epoch 3/1000, LR 0.000030
Train loss: 0.7924;  Loss pred: 0.7592; Loss self: 3.3200; time: 0.25s
Val loss: 0.7235 score: 0.5102 time: 0.08s
Test loss: 0.6953 score: 0.5510 time: 0.08s
Epoch 4/1000, LR 0.000060
Train loss: 0.7300;  Loss pred: 0.6966; Loss self: 3.3370; time: 0.23s
Val loss: 0.6950 score: 0.5306 time: 0.12s
Test loss: 0.6362 score: 0.6327 time: 0.11s
Epoch 5/1000, LR 0.000090
Train loss: 0.6730;  Loss pred: 0.6397; Loss self: 3.3267; time: 0.32s
Val loss: 0.6534 score: 0.6327 time: 0.08s
Test loss: 0.5827 score: 0.7755 time: 0.10s
Epoch 6/1000, LR 0.000120
Train loss: 0.6153;  Loss pred: 0.5821; Loss self: 3.3199; time: 0.25s
Val loss: 0.6465 score: 0.6122 time: 0.09s
Test loss: 0.5513 score: 0.7959 time: 0.09s
Epoch 7/1000, LR 0.000150
Train loss: 0.5971;  Loss pred: 0.5637; Loss self: 3.3462; time: 0.24s
Val loss: 0.6286 score: 0.6531 time: 0.13s
Test loss: 0.5273 score: 0.8163 time: 0.08s
Epoch 8/1000, LR 0.000180
Train loss: 0.5669;  Loss pred: 0.5333; Loss self: 3.3583; time: 0.22s
Val loss: 0.6069 score: 0.6531 time: 0.07s
Test loss: 0.5107 score: 0.7755 time: 0.09s
Epoch 9/1000, LR 0.000210
Train loss: 0.5233;  Loss pred: 0.4897; Loss self: 3.3546; time: 0.23s
Val loss: 0.6021 score: 0.6735 time: 0.09s
Test loss: 0.5022 score: 0.7143 time: 0.11s
Epoch 10/1000, LR 0.000240
Train loss: 0.4998;  Loss pred: 0.4665; Loss self: 3.3323; time: 0.34s
Val loss: 0.5953 score: 0.6531 time: 0.16s
Test loss: 0.4809 score: 0.7347 time: 0.14s
Epoch 11/1000, LR 0.000270
Train loss: 0.4724;  Loss pred: 0.4393; Loss self: 3.3181; time: 0.27s
Val loss: 0.5978 score: 0.6939 time: 0.11s
Test loss: 0.4517 score: 0.8367 time: 0.10s
     INFO: Early stopping counter 1 of 20
Epoch 12/1000, LR 0.000270
Train loss: 0.4434;  Loss pred: 0.4102; Loss self: 3.3151; time: 0.28s
Val loss: 0.6074 score: 0.7347 time: 0.08s
Test loss: 0.4364 score: 0.8367 time: 0.08s
     INFO: Early stopping counter 2 of 20
Epoch 13/1000, LR 0.000270
Train loss: 0.4272;  Loss pred: 0.3943; Loss self: 3.2927; time: 0.23s
Val loss: 0.6000 score: 0.7347 time: 0.07s
Test loss: 0.4206 score: 0.8571 time: 0.08s
     INFO: Early stopping counter 3 of 20
Epoch 14/1000, LR 0.000270
Train loss: 0.4026;  Loss pred: 0.3702; Loss self: 3.2455; time: 0.22s
Val loss: 0.5867 score: 0.7143 time: 0.08s
Test loss: 0.4066 score: 0.8571 time: 0.08s
Epoch 15/1000, LR 0.000270
Train loss: 0.3798;  Loss pred: 0.3479; Loss self: 3.1915; time: 0.24s
Val loss: 0.5811 score: 0.7143 time: 0.07s
Test loss: 0.3910 score: 0.8571 time: 0.08s
Epoch 16/1000, LR 0.000270
Train loss: 0.3544;  Loss pred: 0.3229; Loss self: 3.1488; time: 0.24s
Val loss: 0.5828 score: 0.7551 time: 0.08s
Test loss: 0.3698 score: 0.8776 time: 0.09s
     INFO: Early stopping counter 1 of 20
Epoch 17/1000, LR 0.000270
Train loss: 0.3237;  Loss pred: 0.2926; Loss self: 3.1135; time: 0.23s
Val loss: 0.5915 score: 0.7347 time: 0.08s
Test loss: 0.3525 score: 0.8980 time: 0.08s
     INFO: Early stopping counter 2 of 20
Epoch 18/1000, LR 0.000270
Train loss: 0.3001;  Loss pred: 0.2692; Loss self: 3.0916; time: 0.24s
Val loss: 0.6018 score: 0.7551 time: 0.08s
Test loss: 0.3414 score: 0.8776 time: 0.09s
     INFO: Early stopping counter 3 of 20
Epoch 19/1000, LR 0.000270
Train loss: 0.2836;  Loss pred: 0.2528; Loss self: 3.0796; time: 0.23s
Val loss: 0.5928 score: 0.7551 time: 0.08s
Test loss: 0.3258 score: 0.8776 time: 0.08s
     INFO: Early stopping counter 4 of 20
Epoch 20/1000, LR 0.000270
Train loss: 0.2598;  Loss pred: 0.2292; Loss self: 3.0543; time: 0.24s
Val loss: 0.5759 score: 0.7551 time: 0.07s
Test loss: 0.3103 score: 0.8980 time: 0.08s
Epoch 21/1000, LR 0.000270
Train loss: 0.2398;  Loss pred: 0.2095; Loss self: 3.0226; time: 0.26s
Val loss: 0.5676 score: 0.7755 time: 0.08s
Test loss: 0.2952 score: 0.8980 time: 0.13s
Epoch 22/1000, LR 0.000270
Train loss: 0.2228;  Loss pred: 0.1928; Loss self: 3.0018; time: 0.25s
Val loss: 0.5651 score: 0.7551 time: 0.14s
Test loss: 0.2808 score: 0.8980 time: 0.20s
Epoch 23/1000, LR 0.000270
Train loss: 0.2038;  Loss pred: 0.1739; Loss self: 2.9920; time: 0.28s
Val loss: 0.5646 score: 0.7755 time: 0.17s
Test loss: 0.2699 score: 0.8776 time: 0.09s
Epoch 24/1000, LR 0.000270
Train loss: 0.1865;  Loss pred: 0.1566; Loss self: 2.9861; time: 0.24s
Val loss: 0.5569 score: 0.7959 time: 0.07s
Test loss: 0.2574 score: 0.8776 time: 0.10s
Epoch 25/1000, LR 0.000270
Train loss: 0.1708;  Loss pred: 0.1410; Loss self: 2.9801; time: 0.23s
Val loss: 0.5383 score: 0.7959 time: 0.07s
Test loss: 0.2401 score: 0.8980 time: 0.08s
Epoch 26/1000, LR 0.000270
Train loss: 0.1535;  Loss pred: 0.1238; Loss self: 2.9727; time: 0.23s
Val loss: 0.5158 score: 0.7959 time: 0.08s
Test loss: 0.2226 score: 0.8980 time: 0.08s
Epoch 27/1000, LR 0.000270
Train loss: 0.1374;  Loss pred: 0.1078; Loss self: 2.9658; time: 0.24s
Val loss: 0.5027 score: 0.7959 time: 0.09s
Test loss: 0.2060 score: 0.9184 time: 0.08s
Epoch 28/1000, LR 0.000270
Train loss: 0.1237;  Loss pred: 0.0941; Loss self: 2.9571; time: 0.24s
Val loss: 0.5016 score: 0.7959 time: 0.14s
Test loss: 0.1898 score: 0.9184 time: 0.16s
Epoch 29/1000, LR 0.000270
Train loss: 0.1107;  Loss pred: 0.0812; Loss self: 2.9463; time: 0.22s
Val loss: 0.5003 score: 0.8163 time: 0.07s
Test loss: 0.1759 score: 0.9184 time: 0.14s
Epoch 30/1000, LR 0.000270
Train loss: 0.0999;  Loss pred: 0.0706; Loss self: 2.9340; time: 0.32s
Val loss: 0.4896 score: 0.8367 time: 0.12s
Test loss: 0.1622 score: 0.9388 time: 0.13s
Epoch 31/1000, LR 0.000270
Train loss: 0.0892;  Loss pred: 0.0600; Loss self: 2.9201; time: 0.27s
Val loss: 0.4752 score: 0.8571 time: 0.12s
Test loss: 0.1501 score: 0.9388 time: 0.13s
Epoch 32/1000, LR 0.000270
Train loss: 0.0813;  Loss pred: 0.0523; Loss self: 2.9056; time: 0.27s
Val loss: 0.4694 score: 0.8571 time: 0.08s
Test loss: 0.1412 score: 0.9388 time: 0.09s
Epoch 33/1000, LR 0.000270
Train loss: 0.0754;  Loss pred: 0.0465; Loss self: 2.8927; time: 0.23s
Val loss: 0.4716 score: 0.8571 time: 0.09s
Test loss: 0.1344 score: 0.9796 time: 0.09s
     INFO: Early stopping counter 1 of 20
Epoch 34/1000, LR 0.000270
Train loss: 0.0706;  Loss pred: 0.0418; Loss self: 2.8832; time: 0.23s
Val loss: 0.4767 score: 0.8571 time: 0.08s
Test loss: 0.1294 score: 0.9796 time: 0.09s
     INFO: Early stopping counter 2 of 20
Epoch 35/1000, LR 0.000270
Train loss: 0.0666;  Loss pred: 0.0379; Loss self: 2.8779; time: 0.31s
Val loss: 0.4811 score: 0.8571 time: 0.13s
Test loss: 0.1251 score: 0.9796 time: 0.11s
     INFO: Early stopping counter 3 of 20
Epoch 36/1000, LR 0.000270
Train loss: 0.0629;  Loss pred: 0.0342; Loss self: 2.8760; time: 0.26s
Val loss: 0.4818 score: 0.8571 time: 0.25s
Test loss: 0.1204 score: 0.9796 time: 0.09s
     INFO: Early stopping counter 4 of 20
Epoch 37/1000, LR 0.000270
Train loss: 0.0595;  Loss pred: 0.0307; Loss self: 2.8765; time: 0.22s
Val loss: 0.4786 score: 0.8571 time: 0.15s
Test loss: 0.1148 score: 0.9796 time: 0.14s
     INFO: Early stopping counter 5 of 20
Epoch 38/1000, LR 0.000270
Train loss: 0.0563;  Loss pred: 0.0275; Loss self: 2.8781; time: 0.29s
Val loss: 0.4766 score: 0.8571 time: 0.10s
Test loss: 0.1098 score: 0.9796 time: 0.11s
     INFO: Early stopping counter 6 of 20
Epoch 39/1000, LR 0.000269
Train loss: 0.0536;  Loss pred: 0.0248; Loss self: 2.8810; time: 0.32s
Val loss: 0.4782 score: 0.8571 time: 0.10s
Test loss: 0.1060 score: 0.9796 time: 0.12s
     INFO: Early stopping counter 7 of 20
Epoch 40/1000, LR 0.000269
Train loss: 0.0513;  Loss pred: 0.0224; Loss self: 2.8849; time: 0.50s
Val loss: 0.4842 score: 0.8571 time: 0.14s
Test loss: 0.1037 score: 0.9796 time: 0.18s
     INFO: Early stopping counter 8 of 20
Epoch 41/1000, LR 0.000269
Train loss: 0.0493;  Loss pred: 0.0204; Loss self: 2.8895; time: 0.35s
Val loss: 0.4941 score: 0.8571 time: 0.11s
Test loss: 0.1030 score: 0.9796 time: 0.12s
     INFO: Early stopping counter 9 of 20
Epoch 42/1000, LR 0.000269
Train loss: 0.0475;  Loss pred: 0.0186; Loss self: 2.8945; time: 0.44s
Val loss: 0.5063 score: 0.8571 time: 0.08s
Test loss: 0.1035 score: 0.9796 time: 0.20s
     INFO: Early stopping counter 10 of 20
Epoch 43/1000, LR 0.000269
Train loss: 0.0460;  Loss pred: 0.0170; Loss self: 2.8999; time: 0.36s
Val loss: 0.5199 score: 0.8571 time: 0.19s
Test loss: 0.1049 score: 0.9796 time: 0.13s
     INFO: Early stopping counter 11 of 20
Epoch 44/1000, LR 0.000269
Train loss: 0.0447;  Loss pred: 0.0156; Loss self: 2.9051; time: 0.36s
Val loss: 0.5329 score: 0.8571 time: 0.09s
Test loss: 0.1066 score: 0.9796 time: 0.14s
     INFO: Early stopping counter 12 of 20
Epoch 45/1000, LR 0.000269
Train loss: 0.0436;  Loss pred: 0.0145; Loss self: 2.9102; time: 0.34s
Val loss: 0.5450 score: 0.8571 time: 0.08s
Test loss: 0.1084 score: 0.9796 time: 0.20s
     INFO: Early stopping counter 13 of 20
Epoch 46/1000, LR 0.000269
Train loss: 0.0426;  Loss pred: 0.0135; Loss self: 2.9149; time: 0.34s
Val loss: 0.5558 score: 0.8571 time: 0.12s
Test loss: 0.1102 score: 0.9796 time: 0.10s
     INFO: Early stopping counter 14 of 20
Epoch 47/1000, LR 0.000269
Train loss: 0.0418;  Loss pred: 0.0126; Loss self: 2.9192; time: 0.62s
Val loss: 0.5656 score: 0.8571 time: 0.18s
Test loss: 0.1116 score: 0.9796 time: 0.08s
     INFO: Early stopping counter 15 of 20
Epoch 48/1000, LR 0.000269
Train loss: 0.0411;  Loss pred: 0.0119; Loss self: 2.9233; time: 0.42s
Val loss: 0.5739 score: 0.8571 time: 0.08s
Test loss: 0.1128 score: 0.9796 time: 0.11s
     INFO: Early stopping counter 16 of 20
Epoch 49/1000, LR 0.000269
Train loss: 0.0405;  Loss pred: 0.0112; Loss self: 2.9267; time: 0.25s
Val loss: 0.5809 score: 0.8571 time: 0.08s
Test loss: 0.1139 score: 0.9796 time: 0.11s
     INFO: Early stopping counter 17 of 20
Epoch 50/1000, LR 0.000269
Train loss: 0.0399;  Loss pred: 0.0106; Loss self: 2.9298; time: 0.38s
Val loss: 0.5869 score: 0.8571 time: 0.13s
Test loss: 0.1146 score: 0.9796 time: 0.38s
     INFO: Early stopping counter 18 of 20
Epoch 51/1000, LR 0.000269
Train loss: 0.0394;  Loss pred: 0.0101; Loss self: 2.9324; time: 0.35s
Val loss: 0.5917 score: 0.8571 time: 0.11s
Test loss: 0.1149 score: 0.9796 time: 0.11s
     INFO: Early stopping counter 19 of 20
Epoch 52/1000, LR 0.000269
Train loss: 0.0390;  Loss pred: 0.0096; Loss self: 2.9346; time: 0.24s
Val loss: 0.5956 score: 0.8571 time: 0.09s
Test loss: 0.1149 score: 0.9796 time: 0.08s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 031,   Train_Loss: 0.0813,   Val_Loss: 0.4694,   Val_Precision: 1.0000,   Val_Recall: 0.7200,   Val_accuracy: 0.8372,   Val_Score: 0.8571,   Val_Loss: 0.4694,   Test_Precision: 0.9565,   Test_Recall: 0.9167,   Test_accuracy: 0.9362,   Test_Score: 0.9388,   Test_loss: 0.1412


[0.08189915004186332, 0.07032897300086915, 0.2000812040641904, 0.11548979592043906, 0.16476947499904782, 0.07549699698574841, 0.12960480398032814, 0.1890471070073545, 0.17717552790418267, 0.12348108203150332, 0.07155381294433028, 0.19880801695398986, 0.09736896597314626, 0.10130585089791566, 0.07399243593681604, 0.07519998506177217, 0.0793704700190574, 0.08271386101841927, 0.11638969497289509, 0.08626393193844706, 0.08731207496020943, 0.12004659604281187, 0.12130114994943142, 0.07769500906579196, 0.07229159097187221, 0.07035359391011298, 0.07780947908759117, 0.08315639209467918, 0.1127208910183981, 0.07827799406368285, 0.0766982800560072, 0.10486559697892517, 0.07392151898238808, 0.0756055599777028, 0.09071328502614051, 0.07278257491998374, 0.07243742200080305, 0.07013582508079708, 0.08350524003617465, 0.07770106208045036, 0.07253592403139919, 0.12717502401210368, 0.08287368901073933, 0.08508335694205016, 0.11291082995012403, 0.09067827300168574, 0.07091463205870241, 0.07161686499603093, 0.08879717194940895, 0.07631555804982781, 0.07724104495719075, 0.07576603908091784, 0.07310851104557514, 0.07906504301354289, 0.1424025340238586, 0.09793070401065052, 0.07772832200862467, 0.08040315401740372, 0.08314119500573725, 0.07568398502189666, 0.08069918700493872, 0.1418736519990489, 0.09408001904375851, 0.07779310189653188, 0.08893538801930845, 0.07352509500924498, 0.1087808939628303, 0.07467471505515277, 0.081925316946581, 0.07907126809004694, 0.09015332104172558, 0.10613057401496917, 0.07906635396648198, 0.09367153700441122, 0.08064793492667377, 0.07870431093033403, 0.07251563505269587, 0.07551405299454927, 0.07852227601688355, 0.09858832997269928, 0.09539897798094898, 0.10540165496058762, 0.08741017896682024, 0.11768853093963116, 0.10341086599510163, 0.09931726905051619, 0.08756796410307288, 0.09921348898205906, 0.11845694691874087, 0.14558244601357728, 0.10824050800874829, 0.09250683290883899, 0.08323635591659695, 0.0846886180806905, 0.08243565598968416, 0.09035339299589396, 0.08644568093586713, 0.09437065094243735, 0.08827083895448595, 0.08446318004280329, 0.13515861495397985, 0.20419437007512897, 0.09868154907599092, 0.10113962402101606, 0.08621806604787707, 0.08704733499325812, 0.08743300009518862, 0.15951644198503345, 0.1432573579950258, 0.12992626905906945, 0.1376220779493451, 0.09172156697604805, 0.09198080096393824, 0.093180361087434, 0.1169048979645595, 0.09027662000153214, 0.1458456489490345, 0.1158561899792403, 0.12590665905736387, 0.18607952794991434, 0.12169235292822123, 0.2055104220053181, 0.13668426894582808, 0.14282872900366783, 0.20652080501895398, 0.1118160740006715, 0.08073895005509257, 0.11923877592198551, 0.11670298303943127, 0.3822485989658162, 0.11173733894247562, 0.08864705299492925]
[0.0016714112253441494, 0.0014352851632830439, 0.004083289878861029, 0.0023569346106212053, 0.003362642346919343, 0.0015407550405254777, 0.0026449959995985335, 0.0038581042246398876, 0.003615827100085361, 0.002520022082275578, 0.0014602818968230669, 0.004057306468448773, 0.0019871217545540053, 0.0020674663448554216, 0.0015100497129962457, 0.0015346935726892278, 0.001619805510593008, 0.00168803797996774, 0.0023752998974060223, 0.0017604884069070828, 0.0017818790808206005, 0.0024499305314859567, 0.002475533672437376, 0.0015856124299141218, 0.0014753385912626982, 0.0014357876308186321, 0.001587948552807983, 0.001697069226422024, 0.002300426347314247, 0.001597510082932303, 0.0015652710215511676, 0.0021401142240596973, 0.0015086024282120016, 0.001542970611789853, 0.0018512915311457248, 0.001485358671836403, 0.0014783147347102665, 0.0014313433689958587, 0.0017041885721668297, 0.0015857359608255175, 0.0014803249802326365, 0.0025954086533082382, 0.001691299775729374, 0.0017363950396336767, 0.0023043026520433475, 0.0018505770000344028, 0.0014472373889531105, 0.0014615686733883862, 0.001812187182640999, 0.001557460368363833, 0.001576347856269199, 0.0015462456955289354, 0.0014920104295015335, 0.0016135723063988344, 0.0029061741637522163, 0.001998585796135725, 0.0015862922858902995, 0.001640880694232729, 0.0016967590817497397, 0.0015445711228958502, 0.0016469221837742596, 0.002895380653041814, 0.0019200003886481328, 0.001587614324419018, 0.0018150079187613968, 0.001500512143045816, 0.002220018244139394, 0.001523973776635771, 0.0016719452438077756, 0.0016136993487764681, 0.0018398636947290934, 0.0021659300819381463, 0.0016135990605404486, 0.0019116640204981882, 0.0016458762229933422, 0.0016062104271496742, 0.0014799109194427729, 0.0015411031223377402, 0.0016024954289159908, 0.00201200673413672, 0.0019469179179785506, 0.002151054182869135, 0.0017838812034044946, 0.002401806753870024, 0.002110425836634727, 0.0020268830418472693, 0.0017871013082259772, 0.0020247650812665113, 0.002417488712627365, 0.0029710703268076995, 0.00220898995936221, 0.0018878945491599794, 0.0016987011411550399, 0.0017283391445038878, 0.001682360326320085, 0.0018439467958345705, 0.0017641975701197373, 0.0019259316518864765, 0.0018014456929486928, 0.0017237383682204752, 0.0027583390806934665, 0.004167232042349571, 0.002013909164816141, 0.0020640739596125726, 0.0017595523683240218, 0.0017764762243522064, 0.0017843469407181351, 0.0032554375915312948, 0.002923619550918894, 0.0026515565114095807, 0.0028086138357009205, 0.001871868713796899, 0.0018771592033456784, 0.0019016400221925304, 0.0023858142441746835, 0.0018423800000312682, 0.0029764418152864185, 0.002364412040392659, 0.002569523654231916, 0.0037975413867329458, 0.002483517406698392, 0.004194090245006492, 0.002789474876445471, 0.002914872020483017, 0.004214710306509265, 0.002281960693891255, 0.001647733674593726, 0.002433444406571133, 0.002381693531416965, 0.007800991815628902, 0.00228035385596889, 0.0018091235305087603]
[598.296807414403, 696.7256581351535, 244.9005653938375, 424.2799081033627, 297.3851801147219, 649.0324377968271, 378.07240545988856, 259.1946566952424, 276.56189644034487, 396.82191955119725, 684.799285792395, 246.46893395320205, 503.24042686777517, 483.6838106159983, 662.2298533574743, 651.5958741181864, 617.3580676570866, 592.4037325387139, 420.9994708845242, 568.0241892401051, 561.2053089143819, 408.1748388977666, 403.9533015179769, 630.6711407743952, 677.8105079893083, 696.4818323653043, 629.7433240086345, 589.2511539486969, 434.70202867720684, 625.9741398091548, 638.8669988977438, 467.2647790280308, 662.8651666597156, 648.1004838063607, 540.1634389701558, 673.2380663073543, 676.4459397720805, 698.6443795813562, 586.7895233733008, 630.6220106652627, 675.5273425453157, 385.2957794239261, 591.2612384571206, 575.9058147338221, 433.9707716402821, 540.3720028841867, 690.9716454488306, 684.1963831105372, 551.8193758233327, 642.0709125655218, 634.3777460177712, 646.7277502479467, 670.2366017200634, 619.7429120680664, 344.09500038665306, 500.3538011395382, 630.4008466124227, 609.4288289908835, 589.3588611111339, 647.4289109621205, 607.1932298029377, 345.37773088641217, 520.8332279058012, 629.8758990889974, 550.9617835069408, 666.4391252243712, 450.44674864267034, 656.1792698346408, 598.1057117172974, 619.6941213108969, 543.5185241519986, 461.69542052122324, 619.7326364735651, 523.1044730022148, 607.5791034767534, 622.583431844958, 675.7163467491189, 648.8858438513014, 624.0267410163227, 497.0162291375551, 513.6323369185907, 464.88833613022825, 560.5754453219888, 416.3532300792739, 473.83802010052824, 493.36837861577635, 559.5653673336974, 493.8844556596609, 413.6523967109589, 336.57904054881845, 452.69558413417354, 529.6906018638334, 588.6850698881884, 578.590147182627, 594.4029851127983, 542.3149964299267, 566.8299384020404, 519.2292255130062, 555.1097121130263, 580.1344440875697, 362.5370089556186, 239.96743877890214, 496.54672488234814, 484.47876363291766, 568.3263641380009, 562.9121213624184, 560.4291279797505, 307.17836600566477, 342.04176794675624, 377.1369743382897, 356.0475232617513, 534.2255002337208, 532.7198663904962, 525.8618814969154, 419.1441150297627, 542.776191655917, 335.97162721750414, 422.93812707616286, 389.1771917931306, 263.328269046281, 402.65471758034016, 238.4307302854548, 358.49041281714807, 343.06823523397514, 237.26423105654123, 438.219642729593, 606.894193775924, 410.9401461153819, 419.8693017422187, 128.18882824573015, 438.5284316214661, 552.7538518714533]
Elapsed: 0.10388401868774301~0.0402861860283174
Time per graph: 0.002120082014035572~0.0008221670618023958
Speed: 515.8986284437449~127.6635783292513
Total Time: 0.0892
best val loss: 0.4694494605064392 test_score: 0.9388

Testing...
Test loss: 0.1501 score: 0.9388 time: 0.08s
test Score 0.9388
Epoch Time List: [0.8212122227996588, 0.6402013768674806, 0.5769144799560308, 0.5279414800461382, 0.6287039531162009, 0.4979968130355701, 0.5052409779746085, 0.6093330360017717, 0.5224914151476696, 0.5774432679172605, 0.4158456870354712, 0.5035737900761887, 0.4342434382997453, 0.46042091597337276, 0.3831862809602171, 0.395123238209635, 0.3988121278816834, 0.39748076698742807, 0.5020002210512757, 0.4491215948946774, 0.44488459709100425, 0.4366635399637744, 0.4739067618502304, 0.3935432160506025, 0.4002528830897063, 0.3714926589746028, 0.3902100611012429, 0.4366112999850884, 0.49530278879683465, 0.3933899999829009, 0.4789617419010028, 0.4215195479337126, 0.41999341605696827, 0.4458163959207013, 0.42157638701610267, 0.4057810829253867, 0.3807230358943343, 0.36584602389484644, 0.427049447898753, 0.4000940350815654, 0.37094615097157657, 0.4408616260625422, 0.456954364082776, 0.42501894396264106, 0.4321541058598086, 0.5040275569772348, 0.38766335987020284, 0.36999851383734494, 0.40605602599680424, 0.4518683870555833, 0.404728485038504, 0.3945691171102226, 0.38803102483507246, 0.39753615495283157, 0.5034843608736992, 0.5399608191801235, 0.41708394698798656, 0.4414319481002167, 0.41151884698774666, 0.38057523604948074, 0.3906757839722559, 0.44140008103568107, 0.4366757929092273, 0.5091981129953638, 0.397135401959531, 0.3906647500116378, 0.42305495287291706, 0.514501724857837, 0.44269806996453553, 0.44388697296380997, 0.4151810199255124, 0.5196830501081422, 0.44646251713857055, 0.4408485230524093, 0.4219066429650411, 0.4007214339217171, 0.37524509709328413, 0.3749016427900642, 0.41649599792435765, 0.4609560790704563, 0.4226793219568208, 0.6605732408352196, 0.40931858017574996, 0.46815518697258085, 0.4900357600999996, 0.4301377780502662, 0.45198183285538107, 0.386617815005593, 0.42998324206564575, 0.6465792971430346, 0.48255506390705705, 0.43932721903547645, 0.37984814296942204, 0.3849160319659859, 0.38691891403868794, 0.4089338600169867, 0.3970113400137052, 0.4074379410594702, 0.39673523406963795, 0.3826596711296588, 0.47753493511117995, 0.586745992070064, 0.5468915000092238, 0.4079442509682849, 0.38308071403298527, 0.39244801620952785, 0.40523270692210644, 0.5335685000754893, 0.43721902393735945, 0.5744383689016104, 0.517961488920264, 0.44418566196691245, 0.39817315293475986, 0.3994299969635904, 0.5550249639200047, 0.5871592211769894, 0.5119607700034976, 0.4963397999526933, 0.5395306379068643, 0.8209771430119872, 0.5731303528882563, 0.7157921531470492, 0.6865744610549882, 0.5815315131330863, 0.61796892399434, 0.5596989720361307, 0.8739176699891686, 0.6184377359459177, 0.43494495400227606, 0.8848151119891554, 0.5685862569371238, 0.4170983979711309]
Total Epoch List: [80, 52]
Total Time List: [0.09909544000402093, 0.08924634999129921]
========================k_idx:2========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
emb dim:  64
Model: Ethident
Ethident(
  (encoder): HGATE_encoder(
    (lin1): Linear(in_features=14887, out_features=64, bias=True)
    (account_convs): ModuleList(
      (0): GATEConv()
      (1): GATConv(64, 64, heads=1)
    )
    (subgraph_conv): GATConv(64, 64, heads=1)
    (lin2): Linear(in_features=64, out_features=64, bias=True)
    (lin3): Linear(in_features=64, out_features=2, bias=True)
  )
  (proj_head_g1): Project_Head(
    (block): Sequential(
      (0): Linear(in_features=64, out_features=64, bias=True)
      (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=64, bias=True)
      (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
      (5): ReLU()
    )
    (linear_shortcut): Linear(in_features=64, out_features=64, bias=True)
  )
  (proj_head_g2): Project_Head(
    (block): Sequential(
      (0): Linear(in_features=64, out_features=64, bias=True)
      (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=64, bias=True)
      (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
      (5): ReLU()
    )
    (linear_shortcut): Linear(in_features=64, out_features=64, bias=True)
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=64, bias=True)
    (1): ReLU()
    (2): Dropout(p=0.0, inplace=True)
    (3): Linear(in_features=64, out_features=2, bias=True)
  )
)
Total number of parameters: 1003972
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7a49928018a0>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 1.2560;  Loss pred: 1.2234; Loss self: 3.2570; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 1.1040 score: 0.5102 time: 0.08s
Test loss: 1.1650 score: 0.4792 time: 0.08s
Epoch 2/1000, LR 0.000000
Train loss: 1.2560;  Loss pred: 1.2234; Loss self: 3.2570; time: 0.31s
Val loss: 1.0380 score: 0.4694 time: 0.09s
Test loss: 1.0984 score: 0.4792 time: 0.11s
Epoch 3/1000, LR 0.000030
Train loss: 1.1744;  Loss pred: 1.1417; Loss self: 3.2680; time: 0.24s
Val loss: 0.9344 score: 0.4694 time: 0.07s
Test loss: 0.9821 score: 0.4583 time: 0.09s
Epoch 4/1000, LR 0.000060
Train loss: 1.0359;  Loss pred: 1.0031; Loss self: 3.2801; time: 0.26s
Val loss: 0.8407 score: 0.4286 time: 0.12s
Test loss: 0.8666 score: 0.4167 time: 0.08s
Epoch 5/1000, LR 0.000090
Train loss: 0.9009;  Loss pred: 0.8680; Loss self: 3.2918; time: 0.28s
Val loss: 0.7952 score: 0.3878 time: 0.10s
Test loss: 0.7794 score: 0.4167 time: 0.08s
Epoch 6/1000, LR 0.000120
Train loss: 0.8080;  Loss pred: 0.7749; Loss self: 3.3116; time: 0.39s
Val loss: 0.7675 score: 0.3673 time: 0.11s
Test loss: 0.7426 score: 0.4792 time: 0.11s
Epoch 7/1000, LR 0.000150
Train loss: 0.7640;  Loss pred: 0.7306; Loss self: 3.3403; time: 0.25s
Val loss: 0.7335 score: 0.4490 time: 0.08s
Test loss: 0.7125 score: 0.5417 time: 0.08s
Epoch 8/1000, LR 0.000180
Train loss: 0.7362;  Loss pred: 0.7023; Loss self: 3.3836; time: 0.25s
Val loss: 0.6993 score: 0.5102 time: 0.08s
Test loss: 0.6685 score: 0.6250 time: 0.08s
Epoch 9/1000, LR 0.000210
Train loss: 0.6977;  Loss pred: 0.6634; Loss self: 3.4313; time: 0.30s
Val loss: 0.6731 score: 0.5510 time: 0.22s
Test loss: 0.6383 score: 0.6458 time: 0.08s
Epoch 10/1000, LR 0.000240
Train loss: 0.6640;  Loss pred: 0.6293; Loss self: 3.4719; time: 0.25s
Val loss: 0.6552 score: 0.6122 time: 0.09s
Test loss: 0.6191 score: 0.6042 time: 0.08s
Epoch 11/1000, LR 0.000270
Train loss: 0.6362;  Loss pred: 0.6011; Loss self: 3.5023; time: 0.24s
Val loss: 0.6385 score: 0.6531 time: 0.08s
Test loss: 0.6000 score: 0.6875 time: 0.09s
Epoch 12/1000, LR 0.000270
Train loss: 0.6108;  Loss pred: 0.5756; Loss self: 3.5211; time: 0.25s
Val loss: 0.6210 score: 0.6122 time: 0.09s
Test loss: 0.5798 score: 0.7083 time: 0.08s
Epoch 13/1000, LR 0.000270
Train loss: 0.5836;  Loss pred: 0.5483; Loss self: 3.5307; time: 0.27s
Val loss: 0.6073 score: 0.6122 time: 0.10s
Test loss: 0.5599 score: 0.7292 time: 0.12s
Epoch 14/1000, LR 0.000270
Train loss: 0.5568;  Loss pred: 0.5214; Loss self: 3.5350; time: 0.30s
Val loss: 0.5981 score: 0.7143 time: 0.08s
Test loss: 0.5444 score: 0.8125 time: 0.08s
Epoch 15/1000, LR 0.000270
Train loss: 0.5332;  Loss pred: 0.4979; Loss self: 3.5351; time: 0.24s
Val loss: 0.5902 score: 0.7551 time: 0.08s
Test loss: 0.5317 score: 0.8125 time: 0.08s
Epoch 16/1000, LR 0.000270
Train loss: 0.5125;  Loss pred: 0.4772; Loss self: 3.5308; time: 0.23s
Val loss: 0.5793 score: 0.7755 time: 0.07s
Test loss: 0.5173 score: 0.8333 time: 0.08s
Epoch 17/1000, LR 0.000270
Train loss: 0.4899;  Loss pred: 0.4547; Loss self: 3.5222; time: 0.23s
Val loss: 0.5659 score: 0.7755 time: 0.08s
Test loss: 0.5002 score: 0.8333 time: 0.07s
Epoch 18/1000, LR 0.000270
Train loss: 0.4651;  Loss pred: 0.4301; Loss self: 3.5088; time: 0.23s
Val loss: 0.5518 score: 0.7755 time: 0.07s
Test loss: 0.4815 score: 0.7917 time: 0.08s
Epoch 19/1000, LR 0.000270
Train loss: 0.4398;  Loss pred: 0.4049; Loss self: 3.4915; time: 0.26s
Val loss: 0.5393 score: 0.7755 time: 0.08s
Test loss: 0.4636 score: 0.8125 time: 0.08s
Epoch 20/1000, LR 0.000270
Train loss: 0.4164;  Loss pred: 0.3817; Loss self: 3.4703; time: 0.25s
Val loss: 0.5279 score: 0.7755 time: 0.10s
Test loss: 0.4466 score: 0.8125 time: 0.08s
Epoch 21/1000, LR 0.000270
Train loss: 0.3950;  Loss pred: 0.3605; Loss self: 3.4453; time: 0.24s
Val loss: 0.5179 score: 0.7755 time: 0.08s
Test loss: 0.4303 score: 0.8333 time: 0.10s
Epoch 22/1000, LR 0.000270
Train loss: 0.3737;  Loss pred: 0.3395; Loss self: 3.4166; time: 0.25s
Val loss: 0.5085 score: 0.7959 time: 0.08s
Test loss: 0.4146 score: 0.8333 time: 0.08s
Epoch 23/1000, LR 0.000270
Train loss: 0.3506;  Loss pred: 0.3168; Loss self: 3.3838; time: 0.23s
Val loss: 0.4988 score: 0.8163 time: 0.10s
Test loss: 0.3993 score: 0.8333 time: 0.07s
Epoch 24/1000, LR 0.000270
Train loss: 0.3267;  Loss pred: 0.2933; Loss self: 3.3470; time: 0.25s
Val loss: 0.4890 score: 0.8163 time: 0.11s
Test loss: 0.3848 score: 0.8542 time: 0.08s
Epoch 25/1000, LR 0.000270
Train loss: 0.3026;  Loss pred: 0.2695; Loss self: 3.3067; time: 0.30s
Val loss: 0.4796 score: 0.8163 time: 0.09s
Test loss: 0.3707 score: 0.8750 time: 0.11s
Epoch 26/1000, LR 0.000270
Train loss: 0.2779;  Loss pred: 0.2453; Loss self: 3.2640; time: 0.35s
Val loss: 0.4714 score: 0.8163 time: 0.08s
Test loss: 0.3575 score: 0.8750 time: 0.09s
Epoch 27/1000, LR 0.000270
Train loss: 0.2537;  Loss pred: 0.2215; Loss self: 3.2222; time: 0.31s
Val loss: 0.4633 score: 0.8163 time: 0.08s
Test loss: 0.3440 score: 0.8750 time: 0.09s
Epoch 28/1000, LR 0.000270
Train loss: 0.2300;  Loss pred: 0.1982; Loss self: 3.1808; time: 0.24s
Val loss: 0.4558 score: 0.8163 time: 0.08s
Test loss: 0.3311 score: 0.8750 time: 0.08s
Epoch 29/1000, LR 0.000270
Train loss: 0.2079;  Loss pred: 0.1765; Loss self: 3.1412; time: 0.32s
Val loss: 0.4494 score: 0.8163 time: 0.08s
Test loss: 0.3188 score: 0.8750 time: 0.08s
Epoch 30/1000, LR 0.000270
Train loss: 0.1868;  Loss pred: 0.1558; Loss self: 3.1048; time: 0.27s
Val loss: 0.4454 score: 0.8367 time: 0.09s
Test loss: 0.3084 score: 0.8750 time: 0.08s
Epoch 31/1000, LR 0.000270
Train loss: 0.1677;  Loss pred: 0.1370; Loss self: 3.0707; time: 0.24s
Val loss: 0.4429 score: 0.8367 time: 0.09s
Test loss: 0.3007 score: 0.8750 time: 0.08s
Epoch 32/1000, LR 0.000270
Train loss: 0.1508;  Loss pred: 0.1204; Loss self: 3.0413; time: 0.26s
Val loss: 0.4420 score: 0.8571 time: 0.08s
Test loss: 0.2944 score: 0.8958 time: 0.10s
Epoch 33/1000, LR 0.000270
Train loss: 0.1360;  Loss pred: 0.1059; Loss self: 3.0172; time: 0.26s
Val loss: 0.4419 score: 0.8571 time: 0.08s
Test loss: 0.2892 score: 0.8958 time: 0.12s
Epoch 34/1000, LR 0.000270
Train loss: 0.1232;  Loss pred: 0.0932; Loss self: 2.9996; time: 0.37s
Val loss: 0.4420 score: 0.8776 time: 0.08s
Test loss: 0.2848 score: 0.8958 time: 0.08s
     INFO: Early stopping counter 1 of 20
Epoch 35/1000, LR 0.000270
Train loss: 0.1120;  Loss pred: 0.0821; Loss self: 2.9871; time: 0.25s
Val loss: 0.4434 score: 0.8776 time: 0.09s
Test loss: 0.2803 score: 0.8958 time: 0.08s
     INFO: Early stopping counter 2 of 20
Epoch 36/1000, LR 0.000270
Train loss: 0.1021;  Loss pred: 0.0723; Loss self: 2.9777; time: 0.27s
Val loss: 0.4434 score: 0.8776 time: 0.13s
Test loss: 0.2755 score: 0.8958 time: 0.08s
     INFO: Early stopping counter 3 of 20
Epoch 37/1000, LR 0.000270
Train loss: 0.0935;  Loss pred: 0.0638; Loss self: 2.9719; time: 0.24s
Val loss: 0.4430 score: 0.8776 time: 0.08s
Test loss: 0.2710 score: 0.8958 time: 0.08s
     INFO: Early stopping counter 4 of 20
Epoch 38/1000, LR 0.000270
Train loss: 0.0866;  Loss pred: 0.0569; Loss self: 2.9688; time: 0.25s
Val loss: 0.4434 score: 0.8776 time: 0.07s
Test loss: 0.2671 score: 0.8958 time: 0.17s
     INFO: Early stopping counter 5 of 20
Epoch 39/1000, LR 0.000269
Train loss: 0.0810;  Loss pred: 0.0513; Loss self: 2.9676; time: 0.28s
Val loss: 0.4445 score: 0.8776 time: 0.08s
Test loss: 0.2645 score: 0.8958 time: 0.07s
     INFO: Early stopping counter 6 of 20
Epoch 40/1000, LR 0.000269
Train loss: 0.0762;  Loss pred: 0.0465; Loss self: 2.9683; time: 0.23s
Val loss: 0.4465 score: 0.8776 time: 0.13s
Test loss: 0.2633 score: 0.8958 time: 0.10s
     INFO: Early stopping counter 7 of 20
Epoch 41/1000, LR 0.000269
Train loss: 0.0720;  Loss pred: 0.0423; Loss self: 2.9697; time: 0.24s
Val loss: 0.4491 score: 0.8776 time: 0.09s
Test loss: 0.2629 score: 0.8958 time: 0.08s
     INFO: Early stopping counter 8 of 20
Epoch 42/1000, LR 0.000269
Train loss: 0.0683;  Loss pred: 0.0386; Loss self: 2.9716; time: 0.23s
Val loss: 0.4528 score: 0.8776 time: 0.08s
Test loss: 0.2634 score: 0.8958 time: 0.11s
     INFO: Early stopping counter 9 of 20
Epoch 43/1000, LR 0.000269
Train loss: 0.0653;  Loss pred: 0.0355; Loss self: 2.9737; time: 0.24s
Val loss: 0.4566 score: 0.8776 time: 0.09s
Test loss: 0.2647 score: 0.8958 time: 0.08s
     INFO: Early stopping counter 10 of 20
Epoch 44/1000, LR 0.000269
Train loss: 0.0627;  Loss pred: 0.0330; Loss self: 2.9761; time: 0.28s
Val loss: 0.4610 score: 0.8776 time: 0.16s
Test loss: 0.2670 score: 0.8958 time: 0.13s
     INFO: Early stopping counter 11 of 20
Epoch 45/1000, LR 0.000269
Train loss: 0.0606;  Loss pred: 0.0308; Loss self: 2.9795; time: 0.28s
Val loss: 0.4658 score: 0.8776 time: 0.09s
Test loss: 0.2699 score: 0.8958 time: 0.08s
     INFO: Early stopping counter 12 of 20
Epoch 46/1000, LR 0.000269
Train loss: 0.0588;  Loss pred: 0.0290; Loss self: 2.9834; time: 0.23s
Val loss: 0.4707 score: 0.8776 time: 0.08s
Test loss: 0.2727 score: 0.8958 time: 0.08s
     INFO: Early stopping counter 13 of 20
Epoch 47/1000, LR 0.000269
Train loss: 0.0573;  Loss pred: 0.0275; Loss self: 2.9871; time: 0.26s
Val loss: 0.4757 score: 0.8571 time: 0.08s
Test loss: 0.2752 score: 0.9167 time: 0.07s
     INFO: Early stopping counter 14 of 20
Epoch 48/1000, LR 0.000269
Train loss: 0.0560;  Loss pred: 0.0261; Loss self: 2.9907; time: 0.27s
Val loss: 0.4806 score: 0.8571 time: 0.09s
Test loss: 0.2772 score: 0.9167 time: 0.08s
     INFO: Early stopping counter 15 of 20
Epoch 49/1000, LR 0.000269
Train loss: 0.0548;  Loss pred: 0.0249; Loss self: 2.9942; time: 0.28s
Val loss: 0.4851 score: 0.8367 time: 0.13s
Test loss: 0.2792 score: 0.9167 time: 0.09s
     INFO: Early stopping counter 16 of 20
Epoch 50/1000, LR 0.000269
Train loss: 0.0537;  Loss pred: 0.0237; Loss self: 2.9972; time: 0.26s
Val loss: 0.4895 score: 0.8367 time: 0.09s
Test loss: 0.2812 score: 0.9167 time: 0.07s
     INFO: Early stopping counter 17 of 20
Epoch 51/1000, LR 0.000269
Train loss: 0.0526;  Loss pred: 0.0226; Loss self: 2.9998; time: 0.29s
Val loss: 0.4940 score: 0.8367 time: 0.08s
Test loss: 0.2833 score: 0.9167 time: 0.08s
     INFO: Early stopping counter 18 of 20
Epoch 52/1000, LR 0.000269
Train loss: 0.0516;  Loss pred: 0.0216; Loss self: 3.0021; time: 0.25s
Val loss: 0.4986 score: 0.8367 time: 0.08s
Test loss: 0.2855 score: 0.9167 time: 0.08s
     INFO: Early stopping counter 19 of 20
Epoch 53/1000, LR 0.000269
Train loss: 0.0507;  Loss pred: 0.0206; Loss self: 3.0042; time: 0.28s
Val loss: 0.5034 score: 0.8367 time: 0.09s
Test loss: 0.2876 score: 0.9167 time: 0.08s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 032,   Train_Loss: 0.1360,   Val_Loss: 0.4419,   Val_Precision: 0.9500,   Val_Recall: 0.7600,   Val_accuracy: 0.8444,   Val_Score: 0.8571,   Val_Loss: 0.4419,   Test_Precision: 1.0000,   Test_Recall: 0.7917,   Test_accuracy: 0.8837,   Test_Score: 0.8958,   Test_loss: 0.2892


[0.08189915004186332, 0.07032897300086915, 0.2000812040641904, 0.11548979592043906, 0.16476947499904782, 0.07549699698574841, 0.12960480398032814, 0.1890471070073545, 0.17717552790418267, 0.12348108203150332, 0.07155381294433028, 0.19880801695398986, 0.09736896597314626, 0.10130585089791566, 0.07399243593681604, 0.07519998506177217, 0.0793704700190574, 0.08271386101841927, 0.11638969497289509, 0.08626393193844706, 0.08731207496020943, 0.12004659604281187, 0.12130114994943142, 0.07769500906579196, 0.07229159097187221, 0.07035359391011298, 0.07780947908759117, 0.08315639209467918, 0.1127208910183981, 0.07827799406368285, 0.0766982800560072, 0.10486559697892517, 0.07392151898238808, 0.0756055599777028, 0.09071328502614051, 0.07278257491998374, 0.07243742200080305, 0.07013582508079708, 0.08350524003617465, 0.07770106208045036, 0.07253592403139919, 0.12717502401210368, 0.08287368901073933, 0.08508335694205016, 0.11291082995012403, 0.09067827300168574, 0.07091463205870241, 0.07161686499603093, 0.08879717194940895, 0.07631555804982781, 0.07724104495719075, 0.07576603908091784, 0.07310851104557514, 0.07906504301354289, 0.1424025340238586, 0.09793070401065052, 0.07772832200862467, 0.08040315401740372, 0.08314119500573725, 0.07568398502189666, 0.08069918700493872, 0.1418736519990489, 0.09408001904375851, 0.07779310189653188, 0.08893538801930845, 0.07352509500924498, 0.1087808939628303, 0.07467471505515277, 0.081925316946581, 0.07907126809004694, 0.09015332104172558, 0.10613057401496917, 0.07906635396648198, 0.09367153700441122, 0.08064793492667377, 0.07870431093033403, 0.07251563505269587, 0.07551405299454927, 0.07852227601688355, 0.09858832997269928, 0.09539897798094898, 0.10540165496058762, 0.08741017896682024, 0.11768853093963116, 0.10341086599510163, 0.09931726905051619, 0.08756796410307288, 0.09921348898205906, 0.11845694691874087, 0.14558244601357728, 0.10824050800874829, 0.09250683290883899, 0.08323635591659695, 0.0846886180806905, 0.08243565598968416, 0.09035339299589396, 0.08644568093586713, 0.09437065094243735, 0.08827083895448595, 0.08446318004280329, 0.13515861495397985, 0.20419437007512897, 0.09868154907599092, 0.10113962402101606, 0.08621806604787707, 0.08704733499325812, 0.08743300009518862, 0.15951644198503345, 0.1432573579950258, 0.12992626905906945, 0.1376220779493451, 0.09172156697604805, 0.09198080096393824, 0.093180361087434, 0.1169048979645595, 0.09027662000153214, 0.1458456489490345, 0.1158561899792403, 0.12590665905736387, 0.18607952794991434, 0.12169235292822123, 0.2055104220053181, 0.13668426894582808, 0.14282872900366783, 0.20652080501895398, 0.1118160740006715, 0.08073895005509257, 0.11923877592198551, 0.11670298303943127, 0.3822485989658162, 0.11173733894247562, 0.08864705299492925, 0.08309008390642703, 0.11329546303022653, 0.09424123994540423, 0.08396889397408813, 0.08173224504571408, 0.11682284693233669, 0.08254637592472136, 0.08106530096847564, 0.08661311899777502, 0.08174548309762031, 0.09403052099514753, 0.08215237292461097, 0.12270279799122363, 0.07937675795983523, 0.08253496896941215, 0.0820930419722572, 0.07744553999509662, 0.0821773570496589, 0.08052479801699519, 0.08368078898638487, 0.10303922602906823, 0.08106327301356941, 0.07767821091692895, 0.0805640909820795, 0.11460626195184886, 0.096484350040555, 0.09477403690107167, 0.07968544901814312, 0.08639855694491416, 0.08406974596437067, 0.08825347700621933, 0.10178256197832525, 0.12043844605796039, 0.08129156602080911, 0.08078630897216499, 0.07977625296916813, 0.07952336897142231, 0.17714988195803016, 0.07664783904328942, 0.10721253999508917, 0.08300445496570319, 0.11560714698862284, 0.0817234501009807, 0.13862701202742755, 0.08089977805502713, 0.08681198302656412, 0.07883885398041457, 0.08044615900143981, 0.10120266093872488, 0.07936661900021136, 0.08930793299805373, 0.08344171405769885, 0.08601610490586609]
[0.0016714112253441494, 0.0014352851632830439, 0.004083289878861029, 0.0023569346106212053, 0.003362642346919343, 0.0015407550405254777, 0.0026449959995985335, 0.0038581042246398876, 0.003615827100085361, 0.002520022082275578, 0.0014602818968230669, 0.004057306468448773, 0.0019871217545540053, 0.0020674663448554216, 0.0015100497129962457, 0.0015346935726892278, 0.001619805510593008, 0.00168803797996774, 0.0023752998974060223, 0.0017604884069070828, 0.0017818790808206005, 0.0024499305314859567, 0.002475533672437376, 0.0015856124299141218, 0.0014753385912626982, 0.0014357876308186321, 0.001587948552807983, 0.001697069226422024, 0.002300426347314247, 0.001597510082932303, 0.0015652710215511676, 0.0021401142240596973, 0.0015086024282120016, 0.001542970611789853, 0.0018512915311457248, 0.001485358671836403, 0.0014783147347102665, 0.0014313433689958587, 0.0017041885721668297, 0.0015857359608255175, 0.0014803249802326365, 0.0025954086533082382, 0.001691299775729374, 0.0017363950396336767, 0.0023043026520433475, 0.0018505770000344028, 0.0014472373889531105, 0.0014615686733883862, 0.001812187182640999, 0.001557460368363833, 0.001576347856269199, 0.0015462456955289354, 0.0014920104295015335, 0.0016135723063988344, 0.0029061741637522163, 0.001998585796135725, 0.0015862922858902995, 0.001640880694232729, 0.0016967590817497397, 0.0015445711228958502, 0.0016469221837742596, 0.002895380653041814, 0.0019200003886481328, 0.001587614324419018, 0.0018150079187613968, 0.001500512143045816, 0.002220018244139394, 0.001523973776635771, 0.0016719452438077756, 0.0016136993487764681, 0.0018398636947290934, 0.0021659300819381463, 0.0016135990605404486, 0.0019116640204981882, 0.0016458762229933422, 0.0016062104271496742, 0.0014799109194427729, 0.0015411031223377402, 0.0016024954289159908, 0.00201200673413672, 0.0019469179179785506, 0.002151054182869135, 0.0017838812034044946, 0.002401806753870024, 0.002110425836634727, 0.0020268830418472693, 0.0017871013082259772, 0.0020247650812665113, 0.002417488712627365, 0.0029710703268076995, 0.00220898995936221, 0.0018878945491599794, 0.0016987011411550399, 0.0017283391445038878, 0.001682360326320085, 0.0018439467958345705, 0.0017641975701197373, 0.0019259316518864765, 0.0018014456929486928, 0.0017237383682204752, 0.0027583390806934665, 0.004167232042349571, 0.002013909164816141, 0.0020640739596125726, 0.0017595523683240218, 0.0017764762243522064, 0.0017843469407181351, 0.0032554375915312948, 0.002923619550918894, 0.0026515565114095807, 0.0028086138357009205, 0.001871868713796899, 0.0018771592033456784, 0.0019016400221925304, 0.0023858142441746835, 0.0018423800000312682, 0.0029764418152864185, 0.002364412040392659, 0.002569523654231916, 0.0037975413867329458, 0.002483517406698392, 0.004194090245006492, 0.002789474876445471, 0.002914872020483017, 0.004214710306509265, 0.002281960693891255, 0.001647733674593726, 0.002433444406571133, 0.002381693531416965, 0.007800991815628902, 0.00228035385596889, 0.0018091235305087603, 0.0017310434147172298, 0.002360322146463053, 0.001963359165529255, 0.0017493519577935028, 0.0017027551051190433, 0.002433809311090348, 0.0017197161650983617, 0.0016888604368432425, 0.001804439979120313, 0.0017030308978670898, 0.001958969187398907, 0.0017115077692627285, 0.0025563082914838255, 0.0016536824574965674, 0.0017194785201960865, 0.0017102717077553582, 0.0016134487498978463, 0.0017120282718678936, 0.0016775999586873998, 0.0017433497705496848, 0.002146650542272255, 0.0016888181877826962, 0.001618296060769353, 0.0016784185621266563, 0.0023876304573301845, 0.002010090625844896, 0.00197445910210566, 0.001660113521211315, 0.0017999699363523785, 0.001751453040924389, 0.001838614104296236, 0.002120470041215109, 0.0025091342928741747, 0.00169357429210019, 0.0016830481035867706, 0.0016620052701910026, 0.0016567368535712983, 0.0036906225407922952, 0.0015968299800685297, 0.0022335945832310244, 0.0017292594784521498, 0.0024084822289296426, 0.0017025718771037646, 0.0028880627505714074, 0.0016854120428130652, 0.0018085829797200859, 0.0016424761245919701, 0.0016759616458633293, 0.0021083887695567682, 0.00165347122917107, 0.0018605819374594528, 0.001738369042868726, 0.0017920021855388768]
[598.296807414403, 696.7256581351535, 244.9005653938375, 424.2799081033627, 297.3851801147219, 649.0324377968271, 378.07240545988856, 259.1946566952424, 276.56189644034487, 396.82191955119725, 684.799285792395, 246.46893395320205, 503.24042686777517, 483.6838106159983, 662.2298533574743, 651.5958741181864, 617.3580676570866, 592.4037325387139, 420.9994708845242, 568.0241892401051, 561.2053089143819, 408.1748388977666, 403.9533015179769, 630.6711407743952, 677.8105079893083, 696.4818323653043, 629.7433240086345, 589.2511539486969, 434.70202867720684, 625.9741398091548, 638.8669988977438, 467.2647790280308, 662.8651666597156, 648.1004838063607, 540.1634389701558, 673.2380663073543, 676.4459397720805, 698.6443795813562, 586.7895233733008, 630.6220106652627, 675.5273425453157, 385.2957794239261, 591.2612384571206, 575.9058147338221, 433.9707716402821, 540.3720028841867, 690.9716454488306, 684.1963831105372, 551.8193758233327, 642.0709125655218, 634.3777460177712, 646.7277502479467, 670.2366017200634, 619.7429120680664, 344.09500038665306, 500.3538011395382, 630.4008466124227, 609.4288289908835, 589.3588611111339, 647.4289109621205, 607.1932298029377, 345.37773088641217, 520.8332279058012, 629.8758990889974, 550.9617835069408, 666.4391252243712, 450.44674864267034, 656.1792698346408, 598.1057117172974, 619.6941213108969, 543.5185241519986, 461.69542052122324, 619.7326364735651, 523.1044730022148, 607.5791034767534, 622.583431844958, 675.7163467491189, 648.8858438513014, 624.0267410163227, 497.0162291375551, 513.6323369185907, 464.88833613022825, 560.5754453219888, 416.3532300792739, 473.83802010052824, 493.36837861577635, 559.5653673336974, 493.8844556596609, 413.6523967109589, 336.57904054881845, 452.69558413417354, 529.6906018638334, 588.6850698881884, 578.590147182627, 594.4029851127983, 542.3149964299267, 566.8299384020404, 519.2292255130062, 555.1097121130263, 580.1344440875697, 362.5370089556186, 239.96743877890214, 496.54672488234814, 484.47876363291766, 568.3263641380009, 562.9121213624184, 560.4291279797505, 307.17836600566477, 342.04176794675624, 377.1369743382897, 356.0475232617513, 534.2255002337208, 532.7198663904962, 525.8618814969154, 419.1441150297627, 542.776191655917, 335.97162721750414, 422.93812707616286, 389.1771917931306, 263.328269046281, 402.65471758034016, 238.4307302854548, 358.49041281714807, 343.06823523397514, 237.26423105654123, 438.219642729593, 606.894193775924, 410.9401461153819, 419.8693017422187, 128.18882824573015, 438.5284316214661, 552.7538518714533, 577.6862622266193, 423.6709813101156, 509.331159350273, 571.6402554356887, 587.283513051096, 410.8785332701351, 581.4913067022334, 592.1152382900059, 554.1885635273458, 587.1884070056627, 510.47255180556806, 584.2801405632958, 391.18912352294706, 604.7110165961689, 581.5716731872648, 584.7024162683762, 619.790371440874, 584.1025036981187, 596.0896665629555, 573.6083583988406, 465.8420084255951, 592.1300512004387, 617.9339023568967, 595.7989398859725, 418.82528216623024, 497.49000723769495, 506.467821457305, 602.3684448219794, 555.5648346141215, 570.9545027094852, 543.8879195277189, 471.593552638434, 398.54383356042507, 590.4671585206379, 594.1600824533084, 601.682809275976, 603.5961582217345, 270.95699680664774, 626.2407472817388, 447.70882214150157, 578.2822141273409, 415.1992437346783, 587.3467155472428, 346.25286441651883, 593.3267204682679, 552.9190594034949, 608.8368561512111, 596.6723656643558, 474.2958293266867, 604.7882674688734, 537.4662517499544, 575.2518454595579, 558.0350337013053]
Elapsed: 0.1003299988013365~0.03585892500907944
Time per graph: 0.0020586935757201078~0.0007304290418470172
Speed: 522.8405304827635~116.38790206329115
Total Time: 0.0867
best val loss: 0.4419061243534088 test_score: 0.8958

Testing...
Test loss: 0.2848 score: 0.8958 time: 0.09s
test Score 0.8958
Epoch Time List: [0.8212122227996588, 0.6402013768674806, 0.5769144799560308, 0.5279414800461382, 0.6287039531162009, 0.4979968130355701, 0.5052409779746085, 0.6093330360017717, 0.5224914151476696, 0.5774432679172605, 0.4158456870354712, 0.5035737900761887, 0.4342434382997453, 0.46042091597337276, 0.3831862809602171, 0.395123238209635, 0.3988121278816834, 0.39748076698742807, 0.5020002210512757, 0.4491215948946774, 0.44488459709100425, 0.4366635399637744, 0.4739067618502304, 0.3935432160506025, 0.4002528830897063, 0.3714926589746028, 0.3902100611012429, 0.4366112999850884, 0.49530278879683465, 0.3933899999829009, 0.4789617419010028, 0.4215195479337126, 0.41999341605696827, 0.4458163959207013, 0.42157638701610267, 0.4057810829253867, 0.3807230358943343, 0.36584602389484644, 0.427049447898753, 0.4000940350815654, 0.37094615097157657, 0.4408616260625422, 0.456954364082776, 0.42501894396264106, 0.4321541058598086, 0.5040275569772348, 0.38766335987020284, 0.36999851383734494, 0.40605602599680424, 0.4518683870555833, 0.404728485038504, 0.3945691171102226, 0.38803102483507246, 0.39753615495283157, 0.5034843608736992, 0.5399608191801235, 0.41708394698798656, 0.4414319481002167, 0.41151884698774666, 0.38057523604948074, 0.3906757839722559, 0.44140008103568107, 0.4366757929092273, 0.5091981129953638, 0.397135401959531, 0.3906647500116378, 0.42305495287291706, 0.514501724857837, 0.44269806996453553, 0.44388697296380997, 0.4151810199255124, 0.5196830501081422, 0.44646251713857055, 0.4408485230524093, 0.4219066429650411, 0.4007214339217171, 0.37524509709328413, 0.3749016427900642, 0.41649599792435765, 0.4609560790704563, 0.4226793219568208, 0.6605732408352196, 0.40931858017574996, 0.46815518697258085, 0.4900357600999996, 0.4301377780502662, 0.45198183285538107, 0.386617815005593, 0.42998324206564575, 0.6465792971430346, 0.48255506390705705, 0.43932721903547645, 0.37984814296942204, 0.3849160319659859, 0.38691891403868794, 0.4089338600169867, 0.3970113400137052, 0.4074379410594702, 0.39673523406963795, 0.3826596711296588, 0.47753493511117995, 0.586745992070064, 0.5468915000092238, 0.4079442509682849, 0.38308071403298527, 0.39244801620952785, 0.40523270692210644, 0.5335685000754893, 0.43721902393735945, 0.5744383689016104, 0.517961488920264, 0.44418566196691245, 0.39817315293475986, 0.3994299969635904, 0.5550249639200047, 0.5871592211769894, 0.5119607700034976, 0.4963397999526933, 0.5395306379068643, 0.8209771430119872, 0.5731303528882563, 0.7157921531470492, 0.6865744610549882, 0.5815315131330863, 0.61796892399434, 0.5596989720361307, 0.8739176699891686, 0.6184377359459177, 0.43494495400227606, 0.8848151119891554, 0.5685862569371238, 0.4170983979711309, 0.4102488149655983, 0.5080274000065401, 0.40418034605681896, 0.4573141831206158, 0.4497869638726115, 0.61204860790167, 0.4137701520230621, 0.4089626909699291, 0.6024837921140715, 0.41847331798635423, 0.4086325269890949, 0.42726398597005755, 0.48942887887824327, 0.45376515411771834, 0.40113820403348655, 0.3866686269175261, 0.3837807161035016, 0.37592109094839543, 0.4098994090454653, 0.43276914791204035, 0.42497236107010394, 0.4030973599292338, 0.4017943739891052, 0.43392075796145946, 0.5047573640476912, 0.5274944039992988, 0.4804892030078918, 0.3951870119199157, 0.4749275471549481, 0.4395789420232177, 0.40710691700223833, 0.43582562101073563, 0.4506317119812593, 0.5231167250312865, 0.4113681969465688, 0.4706514070276171, 0.39662778202909976, 0.49435685807839036, 0.42487179895397276, 0.46849958202801645, 0.4017149329883978, 0.42454120016191155, 0.4037079999689013, 0.5780039608944207, 0.4455680060200393, 0.3908537410898134, 0.4089437599759549, 0.43153312720824033, 0.5010354400146753, 0.4210395060945302, 0.4500925140455365, 0.4132842308608815, 0.4495490171248093]
Total Epoch List: [80, 52, 53]
Total Time List: [0.09909544000402093, 0.08924634999129921, 0.08674364897888154]
========================training times:1========================
========================k_idx:0========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
emb dim:  64
Model: Ethident
Ethident(
  (encoder): HGATE_encoder(
    (lin1): Linear(in_features=14887, out_features=64, bias=True)
    (account_convs): ModuleList(
      (0): GATEConv()
      (1): GATConv(64, 64, heads=1)
    )
    (subgraph_conv): GATConv(64, 64, heads=1)
    (lin2): Linear(in_features=64, out_features=64, bias=True)
    (lin3): Linear(in_features=64, out_features=2, bias=True)
  )
  (proj_head_g1): Project_Head(
    (block): Sequential(
      (0): Linear(in_features=64, out_features=64, bias=True)
      (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=64, bias=True)
      (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
      (5): ReLU()
    )
    (linear_shortcut): Linear(in_features=64, out_features=64, bias=True)
  )
  (proj_head_g2): Project_Head(
    (block): Sequential(
      (0): Linear(in_features=64, out_features=64, bias=True)
      (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=64, bias=True)
      (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
      (5): ReLU()
    )
    (linear_shortcut): Linear(in_features=64, out_features=64, bias=True)
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=64, bias=True)
    (1): ReLU()
    (2): Dropout(p=0.0, inplace=True)
    (3): Linear(in_features=64, out_features=2, bias=True)
  )
)
Total number of parameters: 1003972
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7a4992801d20>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.7206;  Loss pred: 0.6860; Loss self: 3.4628; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6948 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7060 score: 0.5102 time: 0.07s
Epoch 2/1000, LR 0.000000
Train loss: 0.7206;  Loss pred: 0.6860; Loss self: 3.4628; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6727 score: 0.4898 time: 0.18s
Test loss: 0.6944 score: 0.4898 time: 0.12s
Epoch 3/1000, LR 0.000030
Train loss: 0.6964;  Loss pred: 0.6619; Loss self: 3.4546; time: 0.23s
Val loss: 0.6417 score: 0.4694 time: 0.08s
Test loss: 0.6801 score: 0.4694 time: 0.07s
Epoch 4/1000, LR 0.000060
Train loss: 0.6654;  Loss pred: 0.6311; Loss self: 3.4310; time: 0.23s
Val loss: 0.6155 score: 0.6122 time: 0.07s
Test loss: 0.6622 score: 0.5102 time: 0.07s
Epoch 5/1000, LR 0.000090
Train loss: 0.6411;  Loss pred: 0.6071; Loss self: 3.3945; time: 0.22s
Val loss: 0.5957 score: 0.7347 time: 0.09s
Test loss: 0.6411 score: 0.5918 time: 0.07s
Epoch 6/1000, LR 0.000120
Train loss: 0.6140;  Loss pred: 0.5803; Loss self: 3.3732; time: 0.23s
Val loss: 0.5701 score: 0.8367 time: 0.07s
Test loss: 0.6215 score: 0.7143 time: 0.07s
Epoch 7/1000, LR 0.000150
Train loss: 0.5839;  Loss pred: 0.5503; Loss self: 3.3567; time: 0.22s
Val loss: 0.5241 score: 0.8163 time: 0.07s
Test loss: 0.5885 score: 0.7143 time: 0.11s
Epoch 8/1000, LR 0.000180
Train loss: 0.5368;  Loss pred: 0.5037; Loss self: 3.3085; time: 0.24s
Val loss: 0.4826 score: 0.8367 time: 0.11s
Test loss: 0.5573 score: 0.8163 time: 0.07s
Epoch 9/1000, LR 0.000210
Train loss: 0.4855;  Loss pred: 0.4528; Loss self: 3.2624; time: 0.30s
Val loss: 0.4466 score: 0.8571 time: 0.08s
Test loss: 0.5313 score: 0.8367 time: 0.07s
Epoch 10/1000, LR 0.000240
Train loss: 0.4465;  Loss pred: 0.4146; Loss self: 3.1922; time: 0.26s
Val loss: 0.4078 score: 0.8571 time: 0.08s
Test loss: 0.4984 score: 0.8163 time: 0.08s
Epoch 11/1000, LR 0.000270
Train loss: 0.4108;  Loss pred: 0.3799; Loss self: 3.0890; time: 0.24s
Val loss: 0.3834 score: 0.8776 time: 0.08s
Test loss: 0.4736 score: 0.8367 time: 0.07s
Epoch 12/1000, LR 0.000270
Train loss: 0.3757;  Loss pred: 0.3454; Loss self: 3.0351; time: 0.24s
Val loss: 0.3693 score: 0.8776 time: 0.08s
Test loss: 0.4618 score: 0.8571 time: 0.24s
Epoch 13/1000, LR 0.000270
Train loss: 0.3460;  Loss pred: 0.3160; Loss self: 3.0039; time: 0.25s
Val loss: 0.3439 score: 0.8776 time: 0.10s
Test loss: 0.4403 score: 0.8571 time: 0.07s
Epoch 14/1000, LR 0.000270
Train loss: 0.3194;  Loss pred: 0.2900; Loss self: 2.9471; time: 0.29s
Val loss: 0.3265 score: 0.8776 time: 0.15s
Test loss: 0.4220 score: 0.8571 time: 0.12s
Epoch 15/1000, LR 0.000270
Train loss: 0.3001;  Loss pred: 0.2711; Loss self: 2.9051; time: 0.35s
Val loss: 0.3142 score: 0.8776 time: 0.13s
Test loss: 0.4114 score: 0.8571 time: 0.10s
Epoch 16/1000, LR 0.000270
Train loss: 0.2765;  Loss pred: 0.2477; Loss self: 2.8832; time: 0.33s
Val loss: 0.3038 score: 0.8776 time: 0.07s
Test loss: 0.4073 score: 0.8571 time: 0.10s
Epoch 17/1000, LR 0.000270
Train loss: 0.2562;  Loss pred: 0.2276; Loss self: 2.8624; time: 0.30s
Val loss: 0.2877 score: 0.8776 time: 0.13s
Test loss: 0.3957 score: 0.8776 time: 0.11s
Epoch 18/1000, LR 0.000270
Train loss: 0.2341;  Loss pred: 0.2057; Loss self: 2.8392; time: 0.27s
Val loss: 0.2651 score: 0.8776 time: 0.07s
Test loss: 0.3780 score: 0.8571 time: 0.08s
Epoch 19/1000, LR 0.000270
Train loss: 0.2114;  Loss pred: 0.1832; Loss self: 2.8168; time: 0.25s
Val loss: 0.2498 score: 0.8980 time: 0.09s
Test loss: 0.3706 score: 0.8571 time: 0.08s
Epoch 20/1000, LR 0.000270
Train loss: 0.1942;  Loss pred: 0.1663; Loss self: 2.7932; time: 0.27s
Val loss: 0.2411 score: 0.8980 time: 0.09s
Test loss: 0.3710 score: 0.8571 time: 0.08s
Epoch 21/1000, LR 0.000270
Train loss: 0.1793;  Loss pred: 0.1516; Loss self: 2.7721; time: 0.32s
Val loss: 0.2397 score: 0.8980 time: 0.08s
Test loss: 0.3720 score: 0.8571 time: 0.07s
Epoch 22/1000, LR 0.000270
Train loss: 0.1655;  Loss pred: 0.1379; Loss self: 2.7572; time: 0.26s
Val loss: 0.2405 score: 0.8980 time: 0.08s
Test loss: 0.3758 score: 0.8776 time: 0.07s
     INFO: Early stopping counter 1 of 20
Epoch 23/1000, LR 0.000270
Train loss: 0.1544;  Loss pred: 0.1269; Loss self: 2.7468; time: 0.25s
Val loss: 0.2378 score: 0.8980 time: 0.09s
Test loss: 0.3772 score: 0.8571 time: 0.10s
Epoch 24/1000, LR 0.000270
Train loss: 0.1442;  Loss pred: 0.1168; Loss self: 2.7381; time: 0.26s
Val loss: 0.2306 score: 0.9184 time: 0.10s
Test loss: 0.3752 score: 0.8571 time: 0.07s
Epoch 25/1000, LR 0.000270
Train loss: 0.1343;  Loss pred: 0.1070; Loss self: 2.7302; time: 0.25s
Val loss: 0.2215 score: 0.9184 time: 0.08s
Test loss: 0.3694 score: 0.8571 time: 0.08s
Epoch 26/1000, LR 0.000270
Train loss: 0.1258;  Loss pred: 0.0986; Loss self: 2.7249; time: 0.26s
Val loss: 0.2151 score: 0.9184 time: 0.09s
Test loss: 0.3651 score: 0.8571 time: 0.08s
Epoch 27/1000, LR 0.000270
Train loss: 0.1190;  Loss pred: 0.0918; Loss self: 2.7227; time: 0.26s
Val loss: 0.2104 score: 0.9184 time: 0.09s
Test loss: 0.3640 score: 0.8571 time: 0.08s
Epoch 28/1000, LR 0.000270
Train loss: 0.1136;  Loss pred: 0.0864; Loss self: 2.7213; time: 0.25s
Val loss: 0.2069 score: 0.9388 time: 0.14s
Test loss: 0.3664 score: 0.8776 time: 0.10s
Epoch 29/1000, LR 0.000270
Train loss: 0.1087;  Loss pred: 0.0815; Loss self: 2.7188; time: 0.27s
Val loss: 0.2036 score: 0.9388 time: 0.08s
Test loss: 0.3707 score: 0.8776 time: 0.11s
Epoch 30/1000, LR 0.000270
Train loss: 0.1039;  Loss pred: 0.0768; Loss self: 2.7162; time: 0.23s
Val loss: 0.2015 score: 0.9388 time: 0.08s
Test loss: 0.3761 score: 0.8776 time: 0.13s
Epoch 31/1000, LR 0.000270
Train loss: 0.0996;  Loss pred: 0.0724; Loss self: 2.7130; time: 0.23s
Val loss: 0.2005 score: 0.9388 time: 0.08s
Test loss: 0.3833 score: 0.8776 time: 0.10s
Epoch 32/1000, LR 0.000270
Train loss: 0.0959;  Loss pred: 0.0688; Loss self: 2.7100; time: 0.35s
Val loss: 0.2005 score: 0.9388 time: 0.14s
Test loss: 0.3917 score: 0.8571 time: 0.14s
     INFO: Early stopping counter 1 of 20
Epoch 33/1000, LR 0.000270
Train loss: 0.0925;  Loss pred: 0.0654; Loss self: 2.7079; time: 0.36s
Val loss: 0.2006 score: 0.9388 time: 0.11s
Test loss: 0.4007 score: 0.8571 time: 0.11s
     INFO: Early stopping counter 2 of 20
Epoch 34/1000, LR 0.000270
Train loss: 0.0889;  Loss pred: 0.0618; Loss self: 2.7079; time: 0.25s
Val loss: 0.2000 score: 0.9388 time: 0.08s
Test loss: 0.4073 score: 0.8571 time: 0.07s
Epoch 35/1000, LR 0.000270
Train loss: 0.0855;  Loss pred: 0.0584; Loss self: 2.7091; time: 0.33s
Val loss: 0.1983 score: 0.9388 time: 0.10s
Test loss: 0.4089 score: 0.8571 time: 0.08s
Epoch 36/1000, LR 0.000270
Train loss: 0.0822;  Loss pred: 0.0551; Loss self: 2.7114; time: 0.26s
Val loss: 0.1954 score: 0.9388 time: 0.08s
Test loss: 0.4057 score: 0.8571 time: 0.07s
Epoch 37/1000, LR 0.000270
Train loss: 0.0791;  Loss pred: 0.0519; Loss self: 2.7141; time: 0.23s
Val loss: 0.1918 score: 0.9388 time: 0.07s
Test loss: 0.4006 score: 0.8571 time: 0.07s
Epoch 38/1000, LR 0.000270
Train loss: 0.0761;  Loss pred: 0.0490; Loss self: 2.7175; time: 0.23s
Val loss: 0.1881 score: 0.9388 time: 0.08s
Test loss: 0.3952 score: 0.8571 time: 0.11s
Epoch 39/1000, LR 0.000269
Train loss: 0.0733;  Loss pred: 0.0461; Loss self: 2.7211; time: 0.28s
Val loss: 0.1848 score: 0.9388 time: 0.08s
Test loss: 0.3897 score: 0.8776 time: 0.07s
Epoch 40/1000, LR 0.000269
Train loss: 0.0707;  Loss pred: 0.0435; Loss self: 2.7242; time: 0.25s
Val loss: 0.1828 score: 0.9388 time: 0.09s
Test loss: 0.3860 score: 0.8776 time: 0.08s
Epoch 41/1000, LR 0.000269
Train loss: 0.0683;  Loss pred: 0.0410; Loss self: 2.7270; time: 0.25s
Val loss: 0.1817 score: 0.9388 time: 0.08s
Test loss: 0.3842 score: 0.8776 time: 0.11s
Epoch 42/1000, LR 0.000269
Train loss: 0.0660;  Loss pred: 0.0387; Loss self: 2.7292; time: 0.25s
Val loss: 0.1811 score: 0.9388 time: 0.08s
Test loss: 0.3841 score: 0.8980 time: 0.07s
Epoch 43/1000, LR 0.000269
Train loss: 0.0638;  Loss pred: 0.0365; Loss self: 2.7309; time: 0.23s
Val loss: 0.1810 score: 0.9388 time: 0.12s
Test loss: 0.3852 score: 0.8980 time: 0.12s
Epoch 44/1000, LR 0.000269
Train loss: 0.0616;  Loss pred: 0.0342; Loss self: 2.7319; time: 0.27s
Val loss: 0.1810 score: 0.9388 time: 0.11s
Test loss: 0.3869 score: 0.8980 time: 0.10s
     INFO: Early stopping counter 1 of 20
Epoch 45/1000, LR 0.000269
Train loss: 0.0594;  Loss pred: 0.0321; Loss self: 2.7325; time: 0.34s
Val loss: 0.1811 score: 0.9388 time: 0.13s
Test loss: 0.3881 score: 0.8980 time: 0.11s
     INFO: Early stopping counter 2 of 20
Epoch 46/1000, LR 0.000269
Train loss: 0.0574;  Loss pred: 0.0301; Loss self: 2.7328; time: 0.24s
Val loss: 0.1812 score: 0.9388 time: 0.19s
Test loss: 0.3884 score: 0.8980 time: 0.10s
     INFO: Early stopping counter 3 of 20
Epoch 47/1000, LR 0.000269
Train loss: 0.0556;  Loss pred: 0.0283; Loss self: 2.7333; time: 0.28s
Val loss: 0.1811 score: 0.9388 time: 0.09s
Test loss: 0.3877 score: 0.8980 time: 0.09s
     INFO: Early stopping counter 4 of 20
Epoch 48/1000, LR 0.000269
Train loss: 0.0538;  Loss pred: 0.0265; Loss self: 2.7340; time: 0.34s
Val loss: 0.1807 score: 0.9388 time: 0.18s
Test loss: 0.3857 score: 0.8776 time: 0.24s
Epoch 49/1000, LR 0.000269
Train loss: 0.0521;  Loss pred: 0.0248; Loss self: 2.7351; time: 0.29s
Val loss: 0.1801 score: 0.9388 time: 0.08s
Test loss: 0.3834 score: 0.8776 time: 0.12s
Epoch 50/1000, LR 0.000269
Train loss: 0.0505;  Loss pred: 0.0232; Loss self: 2.7366; time: 0.27s
Val loss: 0.1790 score: 0.9388 time: 0.09s
Test loss: 0.3809 score: 0.8776 time: 0.11s
Epoch 51/1000, LR 0.000269
Train loss: 0.0491;  Loss pred: 0.0217; Loss self: 2.7387; time: 0.29s
Val loss: 0.1777 score: 0.9388 time: 0.09s
Test loss: 0.3784 score: 0.8776 time: 0.09s
Epoch 52/1000, LR 0.000269
Train loss: 0.0478;  Loss pred: 0.0204; Loss self: 2.7410; time: 0.26s
Val loss: 0.1760 score: 0.9388 time: 0.11s
Test loss: 0.3757 score: 0.8776 time: 0.11s
Epoch 53/1000, LR 0.000269
Train loss: 0.0465;  Loss pred: 0.0191; Loss self: 2.7432; time: 0.82s
Val loss: 0.1743 score: 0.9388 time: 0.22s
Test loss: 0.3732 score: 0.8776 time: 0.22s
Epoch 54/1000, LR 0.000269
Train loss: 0.0453;  Loss pred: 0.0179; Loss self: 2.7457; time: 0.35s
Val loss: 0.1726 score: 0.9388 time: 0.13s
Test loss: 0.3706 score: 0.8776 time: 0.12s
Epoch 55/1000, LR 0.000269
Train loss: 0.0442;  Loss pred: 0.0167; Loss self: 2.7482; time: 0.52s
Val loss: 0.1710 score: 0.9388 time: 0.26s
Test loss: 0.3689 score: 0.8776 time: 0.22s
Epoch 56/1000, LR 0.000269
Train loss: 0.0433;  Loss pred: 0.0158; Loss self: 2.7506; time: 0.37s
Val loss: 0.1699 score: 0.9388 time: 0.12s
Test loss: 0.3675 score: 0.8776 time: 0.13s
Epoch 57/1000, LR 0.000269
Train loss: 0.0424;  Loss pred: 0.0149; Loss self: 2.7530; time: 0.25s
Val loss: 0.1692 score: 0.9388 time: 0.08s
Test loss: 0.3671 score: 0.8776 time: 0.10s
Epoch 58/1000, LR 0.000269
Train loss: 0.0417;  Loss pred: 0.0142; Loss self: 2.7556; time: 0.28s
Val loss: 0.1691 score: 0.9388 time: 0.15s
Test loss: 0.3672 score: 0.8776 time: 0.22s
Epoch 59/1000, LR 0.000268
Train loss: 0.0410;  Loss pred: 0.0135; Loss self: 2.7583; time: 0.28s
Val loss: 0.1693 score: 0.9388 time: 0.08s
Test loss: 0.3681 score: 0.8776 time: 0.12s
     INFO: Early stopping counter 1 of 20
Epoch 60/1000, LR 0.000268
Train loss: 0.0404;  Loss pred: 0.0128; Loss self: 2.7613; time: 0.28s
Val loss: 0.1696 score: 0.9388 time: 0.14s
Test loss: 0.3696 score: 0.8776 time: 0.08s
     INFO: Early stopping counter 2 of 20
Epoch 61/1000, LR 0.000268
Train loss: 0.0398;  Loss pred: 0.0121; Loss self: 2.7640; time: 0.26s
Val loss: 0.1699 score: 0.9388 time: 0.09s
Test loss: 0.3717 score: 0.8776 time: 0.08s
     INFO: Early stopping counter 3 of 20
Epoch 62/1000, LR 0.000268
Train loss: 0.0392;  Loss pred: 0.0116; Loss self: 2.7665; time: 0.25s
Val loss: 0.1703 score: 0.9388 time: 0.08s
Test loss: 0.3744 score: 0.8776 time: 0.07s
     INFO: Early stopping counter 4 of 20
Epoch 63/1000, LR 0.000268
Train loss: 0.0388;  Loss pred: 0.0111; Loss self: 2.7689; time: 0.23s
Val loss: 0.1707 score: 0.9388 time: 0.08s
Test loss: 0.3773 score: 0.8776 time: 0.07s
     INFO: Early stopping counter 5 of 20
Epoch 64/1000, LR 0.000268
Train loss: 0.0384;  Loss pred: 0.0107; Loss self: 2.7709; time: 0.24s
Val loss: 0.1711 score: 0.9388 time: 0.09s
Test loss: 0.3805 score: 0.8776 time: 0.08s
     INFO: Early stopping counter 6 of 20
Epoch 65/1000, LR 0.000268
Train loss: 0.0380;  Loss pred: 0.0103; Loss self: 2.7725; time: 0.25s
Val loss: 0.1716 score: 0.9388 time: 0.08s
Test loss: 0.3839 score: 0.8776 time: 0.08s
     INFO: Early stopping counter 7 of 20
Epoch 66/1000, LR 0.000268
Train loss: 0.0377;  Loss pred: 0.0099; Loss self: 2.7736; time: 0.25s
Val loss: 0.1720 score: 0.9388 time: 0.09s
Test loss: 0.3871 score: 0.8776 time: 0.08s
     INFO: Early stopping counter 8 of 20
Epoch 67/1000, LR 0.000268
Train loss: 0.0373;  Loss pred: 0.0096; Loss self: 2.7746; time: 0.25s
Val loss: 0.1725 score: 0.9388 time: 0.08s
Test loss: 0.3900 score: 0.8776 time: 0.07s
     INFO: Early stopping counter 9 of 20
Epoch 68/1000, LR 0.000268
Train loss: 0.0370;  Loss pred: 0.0092; Loss self: 2.7755; time: 0.24s
Val loss: 0.1731 score: 0.9388 time: 0.08s
Test loss: 0.3928 score: 0.8776 time: 0.08s
     INFO: Early stopping counter 10 of 20
Epoch 69/1000, LR 0.000268
Train loss: 0.0367;  Loss pred: 0.0089; Loss self: 2.7765; time: 0.27s
Val loss: 0.1738 score: 0.9388 time: 0.08s
Test loss: 0.3958 score: 0.8776 time: 0.12s
     INFO: Early stopping counter 11 of 20
Epoch 70/1000, LR 0.000268
Train loss: 0.0364;  Loss pred: 0.0086; Loss self: 2.7775; time: 0.26s
Val loss: 0.1746 score: 0.9388 time: 0.13s
Test loss: 0.3989 score: 0.8776 time: 0.08s
     INFO: Early stopping counter 12 of 20
Epoch 71/1000, LR 0.000268
Train loss: 0.0361;  Loss pred: 0.0083; Loss self: 2.7785; time: 0.29s
Val loss: 0.1753 score: 0.9388 time: 0.09s
Test loss: 0.4012 score: 0.8776 time: 0.07s
     INFO: Early stopping counter 13 of 20
Epoch 72/1000, LR 0.000267
Train loss: 0.0358;  Loss pred: 0.0080; Loss self: 2.7794; time: 0.26s
Val loss: 0.1758 score: 0.9388 time: 0.09s
Test loss: 0.4028 score: 0.8776 time: 0.08s
     INFO: Early stopping counter 14 of 20
Epoch 73/1000, LR 0.000267
Train loss: 0.0355;  Loss pred: 0.0077; Loss self: 2.7803; time: 0.27s
Val loss: 0.1761 score: 0.9388 time: 0.09s
Test loss: 0.4037 score: 0.8776 time: 0.10s
     INFO: Early stopping counter 15 of 20
Epoch 74/1000, LR 0.000267
Train loss: 0.0352;  Loss pred: 0.0074; Loss self: 2.7810; time: 0.28s
Val loss: 0.1763 score: 0.9388 time: 0.09s
Test loss: 0.4039 score: 0.8776 time: 0.10s
     INFO: Early stopping counter 16 of 20
Epoch 75/1000, LR 0.000267
Train loss: 0.0350;  Loss pred: 0.0072; Loss self: 2.7817; time: 0.27s
Val loss: 0.1765 score: 0.9388 time: 0.09s
Test loss: 0.4041 score: 0.8776 time: 0.09s
     INFO: Early stopping counter 17 of 20
Epoch 76/1000, LR 0.000267
Train loss: 0.0347;  Loss pred: 0.0069; Loss self: 2.7823; time: 0.26s
Val loss: 0.1765 score: 0.9388 time: 0.08s
Test loss: 0.4041 score: 0.8776 time: 0.11s
     INFO: Early stopping counter 18 of 20
Epoch 77/1000, LR 0.000267
Train loss: 0.0345;  Loss pred: 0.0067; Loss self: 2.7828; time: 0.26s
Val loss: 0.1765 score: 0.9388 time: 0.08s
Test loss: 0.4040 score: 0.8776 time: 0.08s
     INFO: Early stopping counter 19 of 20
Epoch 78/1000, LR 0.000267
Train loss: 0.0343;  Loss pred: 0.0065; Loss self: 2.7832; time: 0.25s
Val loss: 0.1765 score: 0.9388 time: 0.14s
Test loss: 0.4040 score: 0.8776 time: 0.09s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 057,   Train_Loss: 0.0417,   Val_Loss: 0.1691,   Val_Precision: 0.9565,   Val_Recall: 0.9167,   Val_accuracy: 0.9362,   Val_Score: 0.9388,   Val_Loss: 0.1691,   Test_Precision: 0.9524,   Test_Recall: 0.8000,   Test_accuracy: 0.8696,   Test_Score: 0.8776,   Test_loss: 0.3672


[0.07579778006765991, 0.12498035503085703, 0.0720478609437123, 0.0715817449381575, 0.071297777001746, 0.0820099200354889, 0.1168902280041948, 0.07478887203615159, 0.07661506801377982, 0.08310861501377076, 0.07792099402286112, 0.24939917400479317, 0.07102595898322761, 0.12269827898126096, 0.1075786710716784, 0.10904887795913965, 0.11286116298288107, 0.08846131409518421, 0.08958206907846034, 0.08914866694249213, 0.07871539401821792, 0.07914467202499509, 0.10827917396090925, 0.07817469409201294, 0.08069903799332678, 0.08812897000461817, 0.08295346202794462, 0.10964709694962949, 0.11277634196449071, 0.1327707499731332, 0.10746902297250926, 0.14313994394615293, 0.11869145603850484, 0.07738077302929014, 0.08374895399902016, 0.07599263498559594, 0.07585973804816604, 0.11612676992081106, 0.07693224598187953, 0.0798889109864831, 0.1137230769963935, 0.0728098260005936, 0.12323251995258033, 0.10328565898817033, 0.11735596403013915, 0.10683211300056428, 0.09227800799999386, 0.23965686291921884, 0.12261835904791951, 0.11761650198604912, 0.09900729602668434, 0.11279556294903159, 0.22775919002015144, 0.1292446319712326, 0.2282884429441765, 0.13714733603410423, 0.10645070997998118, 0.2210533400066197, 0.129340257961303, 0.08855005307123065, 0.08123239001724869, 0.07623362191952765, 0.07504030200652778, 0.08046858408488333, 0.0841838390333578, 0.08067582303192466, 0.07667400396894664, 0.08003474096767604, 0.12452217901591212, 0.07999534800183028, 0.07597287197131664, 0.08049040997866541, 0.10739162797108293, 0.10738607204984874, 0.09136744390707463, 0.11765888100489974, 0.0820591509109363, 0.09132201795000583]
[0.0015468934707685697, 0.0025506194904256537, 0.001470364509055353, 0.0014608519375134184, 0.0014550566735050203, 0.0016736718374589573, 0.0023855148572284653, 0.0015263035109418692, 0.0015635728166077513, 0.0016960941839545053, 0.0015902243678134922, 0.005089779061322309, 0.001449509367004645, 0.002504046509821652, 0.0021954830830954774, 0.0022254873052885643, 0.0023032890404669605, 0.0018053329407180451, 0.0018282054913971498, 0.001819360549846778, 0.0016064366126166923, 0.0016151973882652059, 0.0022097790604267194, 0.0015954019202451622, 0.0016469191427209548, 0.0017985504082575136, 0.0016929277964886657, 0.0022376958561148874, 0.0023015579992753206, 0.002709607142308841, 0.002193245366785903, 0.0029212233458398556, 0.002422274613030711, 0.0015791994495773498, 0.0017091623265106156, 0.001550870101746856, 0.0015481579193503273, 0.002369934080016552, 0.0015700458363648883, 0.001630385938499655, 0.0023208791223753777, 0.001485914816338645, 0.0025149493867873537, 0.0021078705915953126, 0.0023950196740844722, 0.0021802472040931484, 0.0018832246530610993, 0.004890956386106507, 0.0025024154907738677, 0.002400336775225492, 0.0020205570617690682, 0.002301950264265951, 0.004648146735105131, 0.0026376455504333184, 0.004658947815187275, 0.0027989252251858004, 0.002172463468979208, 0.00451129265319632, 0.0026395971012510813, 0.001807143940229197, 0.0016578038779030346, 0.0015557882024393398, 0.0015314347348270975, 0.0016422160017323128, 0.0017180375312930163, 0.0016464453679984625, 0.0015647755912029926, 0.0016333620605648172, 0.0025412689595084104, 0.0016325581224863322, 0.0015504667749248293, 0.0016426614281360287, 0.002191665876960876, 0.0021915524908132397, 0.0018646417123892782, 0.002401201653161219, 0.0016746765492027815, 0.0018637146520409354]
[646.4569273171428, 392.06161630683596, 680.1034667535996, 684.5320694868947, 687.2584540581124, 597.4886937921142, 419.196718465136, 655.1776844062348, 639.5608758212805, 589.5898998182186, 628.8420805517953, 196.47218237803096, 689.8886083547436, 399.3536046865295, 455.48062187300934, 449.33979071623446, 434.1617497547175, 553.9144483799562, 546.984463565844, 549.6436646843955, 622.4957724109139, 619.1193765327002, 452.5339288023188, 626.8013014841628, 607.194350991544, 556.0033210127417, 590.6926462393253, 446.88825662671206, 434.48829024289836, 369.05719075862254, 455.94533796528765, 342.3223360939212, 412.8351073905762, 633.2322369208244, 585.081934283899, 644.7993283729104, 645.9289375463986, 421.9526646045006, 636.9240800735411, 613.3517079521914, 430.87121184343204, 672.9860884381253, 397.62231608065076, 474.4124255005445, 417.5331045588438, 458.66358554324563, 531.0040936297981, 204.45898941987085, 399.6138945298615, 416.6082069488179, 494.9130212261687, 434.4142510476357, 215.1395076337628, 379.12599736371624, 214.6407385676637, 357.2799984085381, 460.30693462932095, 221.66595627341636, 378.84569562757633, 553.3593521461116, 603.2076612493546, 642.7610123486522, 652.9824466289791, 608.9332943687901, 582.0594613246823, 607.3690748789752, 639.0692733334397, 612.234129923527, 393.5041964993121, 612.5356189322271, 644.9670616440541, 608.7681751526402, 456.27392866410503, 456.29753528236085, 536.2960580339263, 416.458150727776, 597.1302341792767, 536.5628257013003]
Elapsed: 0.10455280066414091~0.03877830825432908
Time per graph: 0.0021337306257987938~0.0007913940460067159
Speed: 511.97475943291414~126.634217429824
Total Time: 0.0918
best val loss: 0.16907139122486115 test_score: 0.8776

Testing...
Test loss: 0.3664 score: 0.8776 time: 0.07s
test Score 0.8776
Epoch Time List: [0.4007803910644725, 0.5516984510468319, 0.3782102280529216, 0.3712599908467382, 0.3732258459785953, 0.37547759513836354, 0.4083424049895257, 0.42042544297873974, 0.4518230250105262, 0.41706275602336973, 0.3886409110855311, 0.5671575979795307, 0.4226319710724056, 0.5521726179867983, 0.5846743149450049, 0.5112970778718591, 0.5415604570880532, 0.43187812610995024, 0.41950779710896313, 0.4352545660221949, 0.47617799404542893, 0.42017064488027245, 0.44230440200772136, 0.43471012404188514, 0.41050912195350975, 0.4376249489141628, 0.4275085279950872, 0.4954313209746033, 0.45988705195486546, 0.4431313769891858, 0.41280964692123234, 0.6291287451749668, 0.5804483169922605, 0.4024466769769788, 0.507789001101628, 0.4067114948993549, 0.37432034406811, 0.42239004897419363, 0.43248670001048595, 0.41699008899740875, 0.4443285509478301, 0.40099149208981544, 0.4698884409153834, 0.4830544419819489, 0.5780074680224061, 0.5326850509736687, 0.45630134106613696, 0.7508634820114821, 0.48597013601101935, 0.47096476203296334, 0.46516098408028483, 0.474444248015061, 1.263447117060423, 0.6100300110410899, 1.0071825849590823, 0.6234098139684647, 0.4325231749098748, 0.6552417010534555, 0.4828730800654739, 0.504782555042766, 0.41913732804823667, 0.4021265748888254, 0.3849189068423584, 0.4032844949979335, 0.41202465898822993, 0.41455202805809677, 0.4018342880299315, 0.39765128411818296, 0.4706178050255403, 0.4683229000074789, 0.4474199420074001, 0.4226735479896888, 0.4622969808988273, 0.47642903693486005, 0.4510733550414443, 0.45104597485624254, 0.41766856308095157, 0.47271603194531053]
Total Epoch List: [78]
Total Time List: [0.09184962895233184]
========================k_idx:1========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
emb dim:  64
Model: Ethident
Ethident(
  (encoder): HGATE_encoder(
    (lin1): Linear(in_features=14887, out_features=64, bias=True)
    (account_convs): ModuleList(
      (0): GATEConv()
      (1): GATConv(64, 64, heads=1)
    )
    (subgraph_conv): GATConv(64, 64, heads=1)
    (lin2): Linear(in_features=64, out_features=64, bias=True)
    (lin3): Linear(in_features=64, out_features=2, bias=True)
  )
  (proj_head_g1): Project_Head(
    (block): Sequential(
      (0): Linear(in_features=64, out_features=64, bias=True)
      (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=64, bias=True)
      (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
      (5): ReLU()
    )
    (linear_shortcut): Linear(in_features=64, out_features=64, bias=True)
  )
  (proj_head_g2): Project_Head(
    (block): Sequential(
      (0): Linear(in_features=64, out_features=64, bias=True)
      (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=64, bias=True)
      (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
      (5): ReLU()
    )
    (linear_shortcut): Linear(in_features=64, out_features=64, bias=True)
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=64, bias=True)
    (1): ReLU()
    (2): Dropout(p=0.0, inplace=True)
    (3): Linear(in_features=64, out_features=2, bias=True)
  )
)
Total number of parameters: 1003972
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7a49928025f0>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.7202;  Loss pred: 0.6866; Loss self: 3.3621; time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6983 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6831 score: 0.4898 time: 0.09s
Epoch 2/1000, LR 0.000000
Train loss: 0.7202;  Loss pred: 0.6866; Loss self: 3.3621; time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6677 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6456 score: 0.4898 time: 0.08s
Epoch 3/1000, LR 0.000030
Train loss: 0.6645;  Loss pred: 0.6306; Loss self: 3.3858; time: 0.24s
Val loss: 0.6562 score: 0.5510 time: 0.13s
Test loss: 0.6096 score: 0.5918 time: 0.09s
Epoch 4/1000, LR 0.000060
Train loss: 0.6212;  Loss pred: 0.5869; Loss self: 3.4303; time: 0.23s
Val loss: 0.6723 score: 0.6327 time: 0.14s
Test loss: 0.6159 score: 0.6939 time: 0.12s
     INFO: Early stopping counter 1 of 20
Epoch 5/1000, LR 0.000090
Train loss: 0.6265;  Loss pred: 0.5924; Loss self: 3.4079; time: 0.25s
Val loss: 0.6269 score: 0.7143 time: 0.07s
Test loss: 0.5784 score: 0.7959 time: 0.08s
Epoch 6/1000, LR 0.000120
Train loss: 0.5793;  Loss pred: 0.5459; Loss self: 3.3437; time: 0.22s
Val loss: 0.6006 score: 0.6939 time: 0.07s
Test loss: 0.5631 score: 0.7755 time: 0.08s
Epoch 7/1000, LR 0.000150
Train loss: 0.5603;  Loss pred: 0.5270; Loss self: 3.3274; time: 0.28s
Val loss: 0.5943 score: 0.7143 time: 0.07s
Test loss: 0.5487 score: 0.7347 time: 0.08s
Epoch 8/1000, LR 0.000180
Train loss: 0.5393;  Loss pred: 0.5061; Loss self: 3.3195; time: 0.25s
Val loss: 0.5831 score: 0.6939 time: 0.07s
Test loss: 0.5108 score: 0.8367 time: 0.08s
Epoch 9/1000, LR 0.000210
Train loss: 0.4994;  Loss pred: 0.4662; Loss self: 3.3138; time: 0.24s
Val loss: 0.5883 score: 0.7143 time: 0.07s
Test loss: 0.4867 score: 0.8163 time: 0.11s
     INFO: Early stopping counter 1 of 20
Epoch 10/1000, LR 0.000240
Train loss: 0.4637;  Loss pred: 0.4307; Loss self: 3.3061; time: 0.30s
Val loss: 0.5831 score: 0.7347 time: 0.14s
Test loss: 0.4598 score: 0.8163 time: 0.12s
     INFO: Early stopping counter 2 of 20
Epoch 11/1000, LR 0.000270
Train loss: 0.4153;  Loss pred: 0.3825; Loss self: 3.2805; time: 0.23s
Val loss: 0.5694 score: 0.7143 time: 0.08s
Test loss: 0.4311 score: 0.8367 time: 0.08s
Epoch 12/1000, LR 0.000270
Train loss: 0.3763;  Loss pred: 0.3439; Loss self: 3.2429; time: 0.25s
Val loss: 0.5554 score: 0.7143 time: 0.07s
Test loss: 0.4007 score: 0.8571 time: 0.08s
Epoch 13/1000, LR 0.000270
Train loss: 0.3404;  Loss pred: 0.3084; Loss self: 3.2022; time: 0.25s
Val loss: 0.5538 score: 0.7551 time: 0.07s
Test loss: 0.3742 score: 0.8571 time: 0.08s
Epoch 14/1000, LR 0.000270
Train loss: 0.3015;  Loss pred: 0.2699; Loss self: 3.1649; time: 0.23s
Val loss: 0.5617 score: 0.7551 time: 0.10s
Test loss: 0.3562 score: 0.8776 time: 0.15s
     INFO: Early stopping counter 1 of 20
Epoch 15/1000, LR 0.000270
Train loss: 0.2735;  Loss pred: 0.2422; Loss self: 3.1331; time: 0.26s
Val loss: 0.5541 score: 0.7551 time: 0.07s
Test loss: 0.3403 score: 0.8776 time: 0.08s
     INFO: Early stopping counter 2 of 20
Epoch 16/1000, LR 0.000270
Train loss: 0.2547;  Loss pred: 0.2237; Loss self: 3.0965; time: 0.24s
Val loss: 0.5458 score: 0.7551 time: 0.15s
Test loss: 0.3260 score: 0.8776 time: 0.08s
Epoch 17/1000, LR 0.000270
Train loss: 0.2381;  Loss pred: 0.2075; Loss self: 3.0570; time: 0.23s
Val loss: 0.5418 score: 0.7551 time: 0.08s
Test loss: 0.3134 score: 0.8980 time: 0.08s
Epoch 18/1000, LR 0.000270
Train loss: 0.2180;  Loss pred: 0.1878; Loss self: 3.0184; time: 0.23s
Val loss: 0.5382 score: 0.7551 time: 0.07s
Test loss: 0.3047 score: 0.8980 time: 0.08s
Epoch 19/1000, LR 0.000270
Train loss: 0.1984;  Loss pred: 0.1686; Loss self: 2.9835; time: 0.23s
Val loss: 0.5342 score: 0.7755 time: 0.07s
Test loss: 0.2974 score: 0.8980 time: 0.16s
Epoch 20/1000, LR 0.000270
Train loss: 0.1832;  Loss pred: 0.1536; Loss self: 2.9545; time: 0.23s
Val loss: 0.5292 score: 0.7755 time: 0.07s
Test loss: 0.2893 score: 0.8980 time: 0.08s
Epoch 21/1000, LR 0.000270
Train loss: 0.1720;  Loss pred: 0.1426; Loss self: 2.9344; time: 0.22s
Val loss: 0.5255 score: 0.7755 time: 0.07s
Test loss: 0.2812 score: 0.8980 time: 0.08s
Epoch 22/1000, LR 0.000270
Train loss: 0.1625;  Loss pred: 0.1332; Loss self: 2.9244; time: 0.30s
Val loss: 0.5209 score: 0.7755 time: 0.08s
Test loss: 0.2722 score: 0.9184 time: 0.09s
Epoch 23/1000, LR 0.000270
Train loss: 0.1533;  Loss pred: 0.1241; Loss self: 2.9205; time: 0.36s
Val loss: 0.5186 score: 0.7755 time: 0.13s
Test loss: 0.2638 score: 0.9184 time: 0.22s
Epoch 24/1000, LR 0.000270
Train loss: 0.1440;  Loss pred: 0.1148; Loss self: 2.9190; time: 0.31s
Val loss: 0.5186 score: 0.7755 time: 0.12s
Test loss: 0.2549 score: 0.9184 time: 0.10s
Epoch 25/1000, LR 0.000270
Train loss: 0.1354;  Loss pred: 0.1062; Loss self: 2.9190; time: 0.26s
Val loss: 0.5186 score: 0.7755 time: 0.09s
Test loss: 0.2469 score: 0.9184 time: 0.08s
Epoch 26/1000, LR 0.000270
Train loss: 0.1279;  Loss pred: 0.0987; Loss self: 2.9188; time: 0.23s
Val loss: 0.5157 score: 0.7959 time: 0.08s
Test loss: 0.2391 score: 0.9184 time: 0.08s
Epoch 27/1000, LR 0.000270
Train loss: 0.1211;  Loss pred: 0.0919; Loss self: 2.9195; time: 0.24s
Val loss: 0.5104 score: 0.7755 time: 0.07s
Test loss: 0.2314 score: 0.9184 time: 0.10s
Epoch 28/1000, LR 0.000270
Train loss: 0.1150;  Loss pred: 0.0858; Loss self: 2.9212; time: 0.36s
Val loss: 0.5016 score: 0.7755 time: 0.10s
Test loss: 0.2223 score: 0.9184 time: 0.17s
Epoch 29/1000, LR 0.000270
Train loss: 0.1092;  Loss pred: 0.0800; Loss self: 2.9224; time: 0.26s
Val loss: 0.4926 score: 0.7755 time: 0.07s
Test loss: 0.2139 score: 0.9184 time: 0.09s
Epoch 30/1000, LR 0.000270
Train loss: 0.1034;  Loss pred: 0.0742; Loss self: 2.9236; time: 0.23s
Val loss: 0.4849 score: 0.7755 time: 0.07s
Test loss: 0.2060 score: 0.9184 time: 0.12s
Epoch 31/1000, LR 0.000270
Train loss: 0.0975;  Loss pred: 0.0682; Loss self: 2.9248; time: 0.23s
Val loss: 0.4779 score: 0.7959 time: 0.10s
Test loss: 0.1992 score: 0.9184 time: 0.11s
Epoch 32/1000, LR 0.000270
Train loss: 0.0917;  Loss pred: 0.0625; Loss self: 2.9254; time: 0.25s
Val loss: 0.4725 score: 0.7959 time: 0.08s
Test loss: 0.1929 score: 0.9184 time: 0.08s
Epoch 33/1000, LR 0.000270
Train loss: 0.0865;  Loss pred: 0.0572; Loss self: 2.9246; time: 0.23s
Val loss: 0.4694 score: 0.7959 time: 0.08s
Test loss: 0.1874 score: 0.9184 time: 0.10s
Epoch 34/1000, LR 0.000270
Train loss: 0.0816;  Loss pred: 0.0524; Loss self: 2.9232; time: 0.25s
Val loss: 0.4692 score: 0.7959 time: 0.15s
Test loss: 0.1824 score: 0.9184 time: 0.08s
Epoch 35/1000, LR 0.000270
Train loss: 0.0772;  Loss pred: 0.0480; Loss self: 2.9213; time: 0.25s
Val loss: 0.4690 score: 0.7959 time: 0.07s
Test loss: 0.1776 score: 0.9184 time: 0.08s
Epoch 36/1000, LR 0.000270
Train loss: 0.0737;  Loss pred: 0.0445; Loss self: 2.9200; time: 0.24s
Val loss: 0.4710 score: 0.7959 time: 0.07s
Test loss: 0.1733 score: 0.9184 time: 0.08s
     INFO: Early stopping counter 1 of 20
Epoch 37/1000, LR 0.000270
Train loss: 0.0704;  Loss pred: 0.0413; Loss self: 2.9189; time: 0.23s
Val loss: 0.4721 score: 0.8163 time: 0.07s
Test loss: 0.1688 score: 0.9184 time: 0.08s
     INFO: Early stopping counter 2 of 20
Epoch 38/1000, LR 0.000270
Train loss: 0.0673;  Loss pred: 0.0381; Loss self: 2.9184; time: 0.24s
Val loss: 0.4723 score: 0.8367 time: 0.07s
Test loss: 0.1642 score: 0.9184 time: 0.08s
     INFO: Early stopping counter 3 of 20
Epoch 39/1000, LR 0.000269
Train loss: 0.0643;  Loss pred: 0.0351; Loss self: 2.9176; time: 0.23s
Val loss: 0.4713 score: 0.8367 time: 0.07s
Test loss: 0.1594 score: 0.9184 time: 0.08s
     INFO: Early stopping counter 4 of 20
Epoch 40/1000, LR 0.000269
Train loss: 0.0615;  Loss pred: 0.0323; Loss self: 2.9170; time: 0.23s
Val loss: 0.4704 score: 0.8367 time: 0.07s
Test loss: 0.1550 score: 0.9184 time: 0.08s
     INFO: Early stopping counter 5 of 20
Epoch 41/1000, LR 0.000269
Train loss: 0.0589;  Loss pred: 0.0297; Loss self: 2.9167; time: 0.23s
Val loss: 0.4693 score: 0.8367 time: 0.07s
Test loss: 0.1509 score: 0.9184 time: 0.08s
     INFO: Early stopping counter 6 of 20
Epoch 42/1000, LR 0.000269
Train loss: 0.0566;  Loss pred: 0.0274; Loss self: 2.9170; time: 0.24s
Val loss: 0.4683 score: 0.8367 time: 0.07s
Test loss: 0.1471 score: 0.9184 time: 0.08s
Epoch 43/1000, LR 0.000269
Train loss: 0.0545;  Loss pred: 0.0253; Loss self: 2.9177; time: 0.24s
Val loss: 0.4688 score: 0.8367 time: 0.07s
Test loss: 0.1440 score: 0.9184 time: 0.08s
     INFO: Early stopping counter 1 of 20
Epoch 44/1000, LR 0.000269
Train loss: 0.0527;  Loss pred: 0.0235; Loss self: 2.9191; time: 0.25s
Val loss: 0.4706 score: 0.8367 time: 0.07s
Test loss: 0.1416 score: 0.9184 time: 0.09s
     INFO: Early stopping counter 2 of 20
Epoch 45/1000, LR 0.000269
Train loss: 0.0510;  Loss pred: 0.0218; Loss self: 2.9211; time: 0.24s
Val loss: 0.4747 score: 0.8367 time: 0.08s
Test loss: 0.1402 score: 0.9184 time: 0.10s
     INFO: Early stopping counter 3 of 20
Epoch 46/1000, LR 0.000269
Train loss: 0.0495;  Loss pred: 0.0203; Loss self: 2.9236; time: 0.24s
Val loss: 0.4807 score: 0.8367 time: 0.07s
Test loss: 0.1394 score: 0.9184 time: 0.08s
     INFO: Early stopping counter 4 of 20
Epoch 47/1000, LR 0.000269
Train loss: 0.0482;  Loss pred: 0.0189; Loss self: 2.9262; time: 0.22s
Val loss: 0.4871 score: 0.8367 time: 0.07s
Test loss: 0.1399 score: 0.9184 time: 0.08s
     INFO: Early stopping counter 5 of 20
Epoch 48/1000, LR 0.000269
Train loss: 0.0469;  Loss pred: 0.0177; Loss self: 2.9287; time: 0.22s
Val loss: 0.4939 score: 0.8367 time: 0.07s
Test loss: 0.1410 score: 0.9388 time: 0.12s
     INFO: Early stopping counter 6 of 20
Epoch 49/1000, LR 0.000269
Train loss: 0.0458;  Loss pred: 0.0165; Loss self: 2.9311; time: 0.22s
Val loss: 0.5013 score: 0.8367 time: 0.07s
Test loss: 0.1423 score: 0.9388 time: 0.13s
     INFO: Early stopping counter 7 of 20
Epoch 50/1000, LR 0.000269
Train loss: 0.0449;  Loss pred: 0.0156; Loss self: 2.9335; time: 0.22s
Val loss: 0.5089 score: 0.8367 time: 0.07s
Test loss: 0.1442 score: 0.9184 time: 0.08s
     INFO: Early stopping counter 8 of 20
Epoch 51/1000, LR 0.000269
Train loss: 0.0441;  Loss pred: 0.0148; Loss self: 2.9355; time: 0.23s
Val loss: 0.5168 score: 0.8367 time: 0.07s
Test loss: 0.1462 score: 0.9184 time: 0.08s
     INFO: Early stopping counter 9 of 20
Epoch 52/1000, LR 0.000269
Train loss: 0.0434;  Loss pred: 0.0140; Loss self: 2.9377; time: 0.31s
Val loss: 0.5248 score: 0.8367 time: 0.09s
Test loss: 0.1485 score: 0.9184 time: 0.09s
     INFO: Early stopping counter 10 of 20
Epoch 53/1000, LR 0.000269
Train loss: 0.0427;  Loss pred: 0.0133; Loss self: 2.9397; time: 0.24s
Val loss: 0.5327 score: 0.8367 time: 0.08s
Test loss: 0.1505 score: 0.9184 time: 0.08s
     INFO: Early stopping counter 11 of 20
Epoch 54/1000, LR 0.000269
Train loss: 0.0421;  Loss pred: 0.0127; Loss self: 2.9416; time: 0.23s
Val loss: 0.5402 score: 0.8367 time: 0.07s
Test loss: 0.1524 score: 0.9184 time: 0.09s
     INFO: Early stopping counter 12 of 20
Epoch 55/1000, LR 0.000269
Train loss: 0.0415;  Loss pred: 0.0121; Loss self: 2.9434; time: 0.23s
Val loss: 0.5471 score: 0.8367 time: 0.07s
Test loss: 0.1539 score: 0.9184 time: 0.08s
     INFO: Early stopping counter 13 of 20
Epoch 56/1000, LR 0.000269
Train loss: 0.0409;  Loss pred: 0.0115; Loss self: 2.9450; time: 0.23s
Val loss: 0.5534 score: 0.8367 time: 0.07s
Test loss: 0.1551 score: 0.9184 time: 0.10s
     INFO: Early stopping counter 14 of 20
Epoch 57/1000, LR 0.000269
Train loss: 0.0404;  Loss pred: 0.0110; Loss self: 2.9466; time: 0.22s
Val loss: 0.5592 score: 0.8367 time: 0.07s
Test loss: 0.1557 score: 0.9184 time: 0.08s
     INFO: Early stopping counter 15 of 20
Epoch 58/1000, LR 0.000269
Train loss: 0.0400;  Loss pred: 0.0105; Loss self: 2.9478; time: 0.23s
Val loss: 0.5644 score: 0.8367 time: 0.06s
Test loss: 0.1559 score: 0.9184 time: 0.09s
     INFO: Early stopping counter 16 of 20
Epoch 59/1000, LR 0.000268
Train loss: 0.0395;  Loss pred: 0.0100; Loss self: 2.9490; time: 0.24s
Val loss: 0.5691 score: 0.8367 time: 0.07s
Test loss: 0.1555 score: 0.9184 time: 0.09s
     INFO: Early stopping counter 17 of 20
Epoch 60/1000, LR 0.000268
Train loss: 0.0391;  Loss pred: 0.0096; Loss self: 2.9499; time: 0.23s
Val loss: 0.5732 score: 0.8367 time: 0.07s
Test loss: 0.1549 score: 0.9184 time: 0.08s
     INFO: Early stopping counter 18 of 20
Epoch 61/1000, LR 0.000268
Train loss: 0.0387;  Loss pred: 0.0092; Loss self: 2.9507; time: 0.24s
Val loss: 0.5772 score: 0.8367 time: 0.07s
Test loss: 0.1541 score: 0.9184 time: 0.08s
     INFO: Early stopping counter 19 of 20
Epoch 62/1000, LR 0.000268
Train loss: 0.0384;  Loss pred: 0.0089; Loss self: 2.9514; time: 0.24s
Val loss: 0.5811 score: 0.8367 time: 0.08s
Test loss: 0.1533 score: 0.9184 time: 0.08s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 041,   Train_Loss: 0.0566,   Val_Loss: 0.4683,   Val_Precision: 1.0000,   Val_Recall: 0.6800,   Val_accuracy: 0.8095,   Val_Score: 0.8367,   Val_Loss: 0.4683,   Test_Precision: 0.9545,   Test_Recall: 0.8750,   Test_accuracy: 0.9130,   Test_Score: 0.9184,   Test_loss: 0.1471


[0.07579778006765991, 0.12498035503085703, 0.0720478609437123, 0.0715817449381575, 0.071297777001746, 0.0820099200354889, 0.1168902280041948, 0.07478887203615159, 0.07661506801377982, 0.08310861501377076, 0.07792099402286112, 0.24939917400479317, 0.07102595898322761, 0.12269827898126096, 0.1075786710716784, 0.10904887795913965, 0.11286116298288107, 0.08846131409518421, 0.08958206907846034, 0.08914866694249213, 0.07871539401821792, 0.07914467202499509, 0.10827917396090925, 0.07817469409201294, 0.08069903799332678, 0.08812897000461817, 0.08295346202794462, 0.10964709694962949, 0.11277634196449071, 0.1327707499731332, 0.10746902297250926, 0.14313994394615293, 0.11869145603850484, 0.07738077302929014, 0.08374895399902016, 0.07599263498559594, 0.07585973804816604, 0.11612676992081106, 0.07693224598187953, 0.0798889109864831, 0.1137230769963935, 0.0728098260005936, 0.12323251995258033, 0.10328565898817033, 0.11735596403013915, 0.10683211300056428, 0.09227800799999386, 0.23965686291921884, 0.12261835904791951, 0.11761650198604912, 0.09900729602668434, 0.11279556294903159, 0.22775919002015144, 0.1292446319712326, 0.2282884429441765, 0.13714733603410423, 0.10645070997998118, 0.2210533400066197, 0.129340257961303, 0.08855005307123065, 0.08123239001724869, 0.07623362191952765, 0.07504030200652778, 0.08046858408488333, 0.0841838390333578, 0.08067582303192466, 0.07667400396894664, 0.08003474096767604, 0.12452217901591212, 0.07999534800183028, 0.07597287197131664, 0.08049040997866541, 0.10739162797108293, 0.10738607204984874, 0.09136744390707463, 0.11765888100489974, 0.0820591509109363, 0.09132201795000583, 0.09010711091104895, 0.08562336000613868, 0.0909555769758299, 0.12438340904191136, 0.08033566700760275, 0.08146073296666145, 0.08311955607496202, 0.0870211860165, 0.11252298590261489, 0.12015283794607967, 0.0842712779995054, 0.08035610697697848, 0.0838941199472174, 0.15594481804873794, 0.08569909504149109, 0.08063872100319713, 0.08162251301109791, 0.07978836796246469, 0.16379575501196086, 0.0804624070879072, 0.08032664703205228, 0.09411782294046134, 0.22900724003557116, 0.10128206305671483, 0.08373632503207773, 0.08102488494478166, 0.09945947304368019, 0.17827557597775012, 0.09523947292473167, 0.12932515901047736, 0.10959602298680693, 0.07980993296951056, 0.10312783892732114, 0.08158338407520205, 0.08136613806709647, 0.08048088196665049, 0.08517678896896541, 0.0860056810779497, 0.08743569790385664, 0.086290602106601, 0.08053429890424013, 0.08263932995032519, 0.08935279201250523, 0.0917826360091567, 0.10040106403175741, 0.08528852905146778, 0.08705247403122485, 0.12578868202399462, 0.13062772597186267, 0.08155760704539716, 0.08525058208033442, 0.09696453507058322, 0.08753229503054172, 0.09009422198869288, 0.08607525995466858, 0.10575992800295353, 0.08009394304826856, 0.09174251998774707, 0.09624936000909656, 0.08231136295944452, 0.08557626605033875, 0.08305476605892181]
[0.0015468934707685697, 0.0025506194904256537, 0.001470364509055353, 0.0014608519375134184, 0.0014550566735050203, 0.0016736718374589573, 0.0023855148572284653, 0.0015263035109418692, 0.0015635728166077513, 0.0016960941839545053, 0.0015902243678134922, 0.005089779061322309, 0.001449509367004645, 0.002504046509821652, 0.0021954830830954774, 0.0022254873052885643, 0.0023032890404669605, 0.0018053329407180451, 0.0018282054913971498, 0.001819360549846778, 0.0016064366126166923, 0.0016151973882652059, 0.0022097790604267194, 0.0015954019202451622, 0.0016469191427209548, 0.0017985504082575136, 0.0016929277964886657, 0.0022376958561148874, 0.0023015579992753206, 0.002709607142308841, 0.002193245366785903, 0.0029212233458398556, 0.002422274613030711, 0.0015791994495773498, 0.0017091623265106156, 0.001550870101746856, 0.0015481579193503273, 0.002369934080016552, 0.0015700458363648883, 0.001630385938499655, 0.0023208791223753777, 0.001485914816338645, 0.0025149493867873537, 0.0021078705915953126, 0.0023950196740844722, 0.0021802472040931484, 0.0018832246530610993, 0.004890956386106507, 0.0025024154907738677, 0.002400336775225492, 0.0020205570617690682, 0.002301950264265951, 0.004648146735105131, 0.0026376455504333184, 0.004658947815187275, 0.0027989252251858004, 0.002172463468979208, 0.00451129265319632, 0.0026395971012510813, 0.001807143940229197, 0.0016578038779030346, 0.0015557882024393398, 0.0015314347348270975, 0.0016422160017323128, 0.0017180375312930163, 0.0016464453679984625, 0.0015647755912029926, 0.0016333620605648172, 0.0025412689595084104, 0.0016325581224863322, 0.0015504667749248293, 0.0016426614281360287, 0.002191665876960876, 0.0021915524908132397, 0.0018646417123892782, 0.002401201653161219, 0.0016746765492027815, 0.0018637146520409354, 0.0018389206308377336, 0.0017474155103293608, 0.001856236264812855, 0.002538436919222681, 0.0016395034083184234, 0.0016624639380951316, 0.0016963174709175922, 0.001775942571765306, 0.002296387467400304, 0.0024520987335934627, 0.001719821999989906, 0.0016399205505505813, 0.001712124896881988, 0.003182547307117101, 0.0017489611232957365, 0.001645688183738717, 0.0016657655716550593, 0.0016283340400502998, 0.003342770510448181, 0.0016420899405695346, 0.0016393193271847404, 0.001920771896744109, 0.004673617143583085, 0.002066980878708466, 0.0017089045924913822, 0.0016535690805057483, 0.0020297851641567386, 0.0036382770607704105, 0.001943662712749626, 0.0026392889593974973, 0.0022366535303429986, 0.0016287741422349093, 0.002104649774026962, 0.001664967021942899, 0.0016605334299407443, 0.0016424669789112344, 0.0017383018156931717, 0.001755217981182647, 0.0017844019980378905, 0.0017610326960530815, 0.0016435571204946966, 0.0016865169377617386, 0.0018235263676021475, 0.0018731150205950349, 0.0020490013067705594, 0.0017405822255401587, 0.001776581102678058, 0.00256711595967336, 0.0026658719586094425, 0.0016644409601101462, 0.0017398077975578454, 0.001978868062664964, 0.001786373367970239, 0.0018386575916059771, 0.0017566379582585425, 0.0021583658776112963, 0.001634570266291195, 0.0018722963262805526, 0.0019642726532468685, 0.0016798237338662148, 0.0017464544091905867, 0.0016949952256922818]
[646.4569273171428, 392.06161630683596, 680.1034667535996, 684.5320694868947, 687.2584540581124, 597.4886937921142, 419.196718465136, 655.1776844062348, 639.5608758212805, 589.5898998182186, 628.8420805517953, 196.47218237803096, 689.8886083547436, 399.3536046865295, 455.48062187300934, 449.33979071623446, 434.1617497547175, 553.9144483799562, 546.984463565844, 549.6436646843955, 622.4957724109139, 619.1193765327002, 452.5339288023188, 626.8013014841628, 607.194350991544, 556.0033210127417, 590.6926462393253, 446.88825662671206, 434.48829024289836, 369.05719075862254, 455.94533796528765, 342.3223360939212, 412.8351073905762, 633.2322369208244, 585.081934283899, 644.7993283729104, 645.9289375463986, 421.9526646045006, 636.9240800735411, 613.3517079521914, 430.87121184343204, 672.9860884381253, 397.62231608065076, 474.4124255005445, 417.5331045588438, 458.66358554324563, 531.0040936297981, 204.45898941987085, 399.6138945298615, 416.6082069488179, 494.9130212261687, 434.4142510476357, 215.1395076337628, 379.12599736371624, 214.6407385676637, 357.2799984085381, 460.30693462932095, 221.66595627341636, 378.84569562757633, 553.3593521461116, 603.2076612493546, 642.7610123486522, 652.9824466289791, 608.9332943687901, 582.0594613246823, 607.3690748789752, 639.0692733334397, 612.234129923527, 393.5041964993121, 612.5356189322271, 644.9670616440541, 608.7681751526402, 456.27392866410503, 456.29753528236085, 536.2960580339263, 416.458150727776, 597.1302341792767, 536.5628257013003, 543.7972597786576, 572.2737346033488, 538.7245249735596, 393.9432145929471, 609.940787513008, 601.5168071229324, 589.512291858356, 563.0812707000932, 435.46658140060345, 407.8139213156951, 581.455522726113, 609.7856384958792, 584.0695394483988, 314.2137110621135, 571.7679979733359, 607.6485265441805, 600.3245696850531, 614.1246055195835, 299.15305189943336, 608.9800414057496, 610.0092784956879, 520.6240270878052, 213.96703437144146, 483.7974121099953, 585.1701753239, 604.7524786168277, 492.662976190115, 274.85537338056616, 514.4925575000294, 378.8899265612365, 447.0965155907033, 613.9586662567335, 475.13843506923985, 600.612496716644, 602.2161204160068, 608.8402463122177, 575.274092779588, 569.7298060530412, 560.4118360658581, 567.8486278200581, 608.43641363618, 592.938011833519, 548.3880122418814, 533.8700448210216, 488.0426365252566, 574.5204020394198, 562.8788905232515, 389.5422005507068, 375.11178913544467, 600.8023258054308, 574.7761341245235, 505.3394002697122, 559.7933880621198, 543.8750556739328, 569.2692653592424, 463.3134772806539, 611.7815921544802, 534.1034888353223, 509.0942941892704, 595.3005543614033, 572.5886657776889, 589.9721632499425]
Elapsed: 0.10118337049331916~0.03417003629129257
Time per graph: 0.002064966744761615~0.000697347679414134
Speed: 519.8979365968173~113.28958640911152
Total Time: 0.0836
best val loss: 0.46828943490982056 test_score: 0.9184

Testing...
Test loss: 0.1642 score: 0.9184 time: 0.08s
test Score 0.9184
Epoch Time List: [0.4007803910644725, 0.5516984510468319, 0.3782102280529216, 0.3712599908467382, 0.3732258459785953, 0.37547759513836354, 0.4083424049895257, 0.42042544297873974, 0.4518230250105262, 0.41706275602336973, 0.3886409110855311, 0.5671575979795307, 0.4226319710724056, 0.5521726179867983, 0.5846743149450049, 0.5112970778718591, 0.5415604570880532, 0.43187812610995024, 0.41950779710896313, 0.4352545660221949, 0.47617799404542893, 0.42017064488027245, 0.44230440200772136, 0.43471012404188514, 0.41050912195350975, 0.4376249489141628, 0.4275085279950872, 0.4954313209746033, 0.45988705195486546, 0.4431313769891858, 0.41280964692123234, 0.6291287451749668, 0.5804483169922605, 0.4024466769769788, 0.507789001101628, 0.4067114948993549, 0.37432034406811, 0.42239004897419363, 0.43248670001048595, 0.41699008899740875, 0.4443285509478301, 0.40099149208981544, 0.4698884409153834, 0.4830544419819489, 0.5780074680224061, 0.5326850509736687, 0.45630134106613696, 0.7508634820114821, 0.48597013601101935, 0.47096476203296334, 0.46516098408028483, 0.474444248015061, 1.263447117060423, 0.6100300110410899, 1.0071825849590823, 0.6234098139684647, 0.4325231749098748, 0.6552417010534555, 0.4828730800654739, 0.504782555042766, 0.41913732804823667, 0.4021265748888254, 0.3849189068423584, 0.4032844949979335, 0.41202465898822993, 0.41455202805809677, 0.4018342880299315, 0.39765128411818296, 0.4706178050255403, 0.4683229000074789, 0.4474199420074001, 0.4226735479896888, 0.4622969808988273, 0.47642903693486005, 0.4510733550414443, 0.45104597485624254, 0.41766856308095157, 0.47271603194531053, 0.3846845319494605, 0.42235302389599383, 0.4616062599234283, 0.4924150260630995, 0.39042309299111366, 0.36933031294029206, 0.42946929193567485, 0.4016224399674684, 0.4199186251498759, 0.5503297342220321, 0.3905405071564019, 0.3934044858906418, 0.403498413041234, 0.4865909649524838, 0.4124398339772597, 0.4632093511754647, 0.3913276109378785, 0.36985213903244585, 0.45425202103797346, 0.37531524698715657, 0.3669260579627007, 0.46680050692521036, 0.7094643471064046, 0.5303628069814295, 0.42763145198114216, 0.38175516203045845, 0.4068529000505805, 0.63313967094291, 0.41917055402882397, 0.4235042290529236, 0.43116408702917397, 0.4062675170134753, 0.39988122193608433, 0.4745244849473238, 0.39553645194973797, 0.3855419137980789, 0.37665966304484755, 0.39311047992669046, 0.38840423501096666, 0.3834078760119155, 0.3781516801100224, 0.3865536430384964, 0.3938638489926234, 0.4019572751130909, 0.41653916181530803, 0.3863452570512891, 0.37173136707860976, 0.4102867660112679, 0.4126237150048837, 0.37002675398252904, 0.3824094398878515, 0.49307656998280436, 0.40309709298890084, 0.3860527079086751, 0.38196090201381594, 0.399831713992171, 0.3561166441068053, 0.3796582259237766, 0.39911827305331826, 0.38169954798649997, 0.39143043803051114, 0.39497795898932964]
Total Epoch List: [78, 62]
Total Time List: [0.09184962895233184, 0.08363577304407954]
========================k_idx:2========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
emb dim:  64
Model: Ethident
Ethident(
  (encoder): HGATE_encoder(
    (lin1): Linear(in_features=14887, out_features=64, bias=True)
    (account_convs): ModuleList(
      (0): GATEConv()
      (1): GATConv(64, 64, heads=1)
    )
    (subgraph_conv): GATConv(64, 64, heads=1)
    (lin2): Linear(in_features=64, out_features=64, bias=True)
    (lin3): Linear(in_features=64, out_features=2, bias=True)
  )
  (proj_head_g1): Project_Head(
    (block): Sequential(
      (0): Linear(in_features=64, out_features=64, bias=True)
      (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=64, bias=True)
      (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
      (5): ReLU()
    )
    (linear_shortcut): Linear(in_features=64, out_features=64, bias=True)
  )
  (proj_head_g2): Project_Head(
    (block): Sequential(
      (0): Linear(in_features=64, out_features=64, bias=True)
      (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=64, bias=True)
      (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
      (5): ReLU()
    )
    (linear_shortcut): Linear(in_features=64, out_features=64, bias=True)
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=64, bias=True)
    (1): ReLU()
    (2): Dropout(p=0.0, inplace=True)
    (3): Linear(in_features=64, out_features=2, bias=True)
  )
)
Total number of parameters: 1003972
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7a49927ecf40>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.7370;  Loss pred: 0.7029; Loss self: 3.4135; time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7487 score: 0.5102 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7415 score: 0.5000 time: 0.08s
Epoch 2/1000, LR 0.000000
Train loss: 0.7370;  Loss pred: 0.7029; Loss self: 3.4135; time: 0.24s
Val loss: 0.7269 score: 0.5102 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7089 score: 0.5000 time: 0.11s
Epoch 3/1000, LR 0.000030
Train loss: 0.7046;  Loss pred: 0.6709; Loss self: 3.3678; time: 0.24s
Val loss: 0.6900 score: 0.4898 time: 0.10s
Test loss: 0.6689 score: 0.5208 time: 0.07s
Epoch 4/1000, LR 0.000060
Train loss: 0.6598;  Loss pred: 0.6274; Loss self: 3.2430; time: 0.23s
Val loss: 0.6498 score: 0.5306 time: 0.09s
Test loss: 0.6263 score: 0.6667 time: 0.15s
Epoch 5/1000, LR 0.000090
Train loss: 0.6102;  Loss pred: 0.5791; Loss self: 3.1115; time: 0.25s
Val loss: 0.6445 score: 0.6327 time: 0.07s
Test loss: 0.6089 score: 0.6875 time: 0.08s
Epoch 6/1000, LR 0.000120
Train loss: 0.6028;  Loss pred: 0.5719; Loss self: 3.0923; time: 0.25s
Val loss: 0.5924 score: 0.6939 time: 0.07s
Test loss: 0.5702 score: 0.7708 time: 0.08s
Epoch 7/1000, LR 0.000150
Train loss: 0.5504;  Loss pred: 0.5188; Loss self: 3.1576; time: 0.24s
Val loss: 0.5697 score: 0.7347 time: 0.07s
Test loss: 0.5684 score: 0.7917 time: 0.07s
Epoch 8/1000, LR 0.000180
Train loss: 0.5306;  Loss pred: 0.4982; Loss self: 3.2375; time: 0.23s
Val loss: 0.5495 score: 0.7959 time: 0.11s
Test loss: 0.5470 score: 0.8125 time: 0.07s
Epoch 9/1000, LR 0.000210
Train loss: 0.5009;  Loss pred: 0.4686; Loss self: 3.2369; time: 0.24s
Val loss: 0.5225 score: 0.7551 time: 0.07s
Test loss: 0.5113 score: 0.7708 time: 0.08s
Epoch 10/1000, LR 0.000240
Train loss: 0.4584;  Loss pred: 0.4265; Loss self: 3.1898; time: 0.23s
Val loss: 0.5152 score: 0.7143 time: 0.07s
Test loss: 0.4970 score: 0.7708 time: 0.07s
Epoch 11/1000, LR 0.000270
Train loss: 0.4368;  Loss pred: 0.4054; Loss self: 3.1342; time: 0.23s
Val loss: 0.4933 score: 0.7143 time: 0.07s
Test loss: 0.4693 score: 0.7708 time: 0.07s
Epoch 12/1000, LR 0.000270
Train loss: 0.4015;  Loss pred: 0.3703; Loss self: 3.1229; time: 0.22s
Val loss: 0.4754 score: 0.8163 time: 0.09s
Test loss: 0.4410 score: 0.8125 time: 0.12s
Epoch 13/1000, LR 0.000270
Train loss: 0.3690;  Loss pred: 0.3377; Loss self: 3.1348; time: 0.37s
Val loss: 0.4723 score: 0.8367 time: 0.13s
Test loss: 0.4280 score: 0.8333 time: 0.08s
Epoch 14/1000, LR 0.000270
Train loss: 0.3503;  Loss pred: 0.3190; Loss self: 3.1362; time: 0.26s
Val loss: 0.4496 score: 0.8571 time: 0.08s
Test loss: 0.4046 score: 0.8333 time: 0.07s
Epoch 15/1000, LR 0.000270
Train loss: 0.3208;  Loss pred: 0.2898; Loss self: 3.0976; time: 0.24s
Val loss: 0.4255 score: 0.8571 time: 0.07s
Test loss: 0.3806 score: 0.8125 time: 0.07s
Epoch 16/1000, LR 0.000270
Train loss: 0.2944;  Loss pred: 0.2638; Loss self: 3.0537; time: 0.24s
Val loss: 0.4067 score: 0.8571 time: 0.08s
Test loss: 0.3598 score: 0.8750 time: 0.07s
Epoch 17/1000, LR 0.000270
Train loss: 0.2686;  Loss pred: 0.2383; Loss self: 3.0289; time: 0.26s
Val loss: 0.3998 score: 0.8571 time: 0.12s
Test loss: 0.3460 score: 0.8542 time: 0.08s
Epoch 18/1000, LR 0.000270
Train loss: 0.2456;  Loss pred: 0.2155; Loss self: 3.0113; time: 0.29s
Val loss: 0.4023 score: 0.8571 time: 0.07s
Test loss: 0.3394 score: 0.8333 time: 0.07s
     INFO: Early stopping counter 1 of 20
Epoch 19/1000, LR 0.000270
Train loss: 0.2277;  Loss pred: 0.1978; Loss self: 2.9885; time: 0.22s
Val loss: 0.3967 score: 0.8571 time: 0.12s
Test loss: 0.3303 score: 0.8958 time: 0.13s
Epoch 20/1000, LR 0.000270
Train loss: 0.2093;  Loss pred: 0.1797; Loss self: 2.9590; time: 0.28s
Val loss: 0.3792 score: 0.8571 time: 0.08s
Test loss: 0.3131 score: 0.8542 time: 0.09s
Epoch 21/1000, LR 0.000270
Train loss: 0.1880;  Loss pred: 0.1587; Loss self: 2.9268; time: 0.36s
Val loss: 0.3561 score: 0.8571 time: 0.21s
Test loss: 0.2943 score: 0.8750 time: 0.10s
Epoch 22/1000, LR 0.000270
Train loss: 0.1685;  Loss pred: 0.1395; Loss self: 2.9013; time: 0.30s
Val loss: 0.3407 score: 0.8571 time: 0.08s
Test loss: 0.2810 score: 0.8750 time: 0.08s
Epoch 23/1000, LR 0.000270
Train loss: 0.1553;  Loss pred: 0.1265; Loss self: 2.8850; time: 0.32s
Val loss: 0.3324 score: 0.8776 time: 0.08s
Test loss: 0.2688 score: 0.8750 time: 0.19s
Epoch 24/1000, LR 0.000270
Train loss: 0.1452;  Loss pred: 0.1164; Loss self: 2.8777; time: 0.41s
Val loss: 0.3334 score: 0.8776 time: 0.07s
Test loss: 0.2582 score: 0.8750 time: 0.22s
     INFO: Early stopping counter 1 of 20
Epoch 25/1000, LR 0.000270
Train loss: 0.1347;  Loss pred: 0.1059; Loss self: 2.8776; time: 0.30s
Val loss: 0.3408 score: 0.8776 time: 0.07s
Test loss: 0.2549 score: 0.9167 time: 0.11s
     INFO: Early stopping counter 2 of 20
Epoch 26/1000, LR 0.000270
Train loss: 0.1258;  Loss pred: 0.0970; Loss self: 2.8805; time: 0.30s
Val loss: 0.3530 score: 0.8776 time: 0.07s
Test loss: 0.2616 score: 0.8750 time: 0.17s
     INFO: Early stopping counter 3 of 20
Epoch 27/1000, LR 0.000270
Train loss: 0.1193;  Loss pred: 0.0905; Loss self: 2.8849; time: 0.28s
Val loss: 0.3639 score: 0.8776 time: 0.12s
Test loss: 0.2691 score: 0.8750 time: 0.11s
     INFO: Early stopping counter 4 of 20
Epoch 28/1000, LR 0.000270
Train loss: 0.1143;  Loss pred: 0.0854; Loss self: 2.8909; time: 0.26s
Val loss: 0.3682 score: 0.8571 time: 0.16s
Test loss: 0.2702 score: 0.8750 time: 0.27s
     INFO: Early stopping counter 5 of 20
Epoch 29/1000, LR 0.000270
Train loss: 0.1087;  Loss pred: 0.0797; Loss self: 2.9000; time: 0.31s
Val loss: 0.3653 score: 0.8776 time: 0.10s
Test loss: 0.2654 score: 0.8750 time: 0.16s
     INFO: Early stopping counter 6 of 20
Epoch 30/1000, LR 0.000270
Train loss: 0.1029;  Loss pred: 0.0738; Loss self: 2.9108; time: 0.28s
Val loss: 0.3574 score: 0.8776 time: 0.13s
Test loss: 0.2549 score: 0.8958 time: 0.08s
     INFO: Early stopping counter 7 of 20
Epoch 31/1000, LR 0.000270
Train loss: 0.0972;  Loss pred: 0.0680; Loss self: 2.9207; time: 0.47s
Val loss: 0.3488 score: 0.8776 time: 0.12s
Test loss: 0.2432 score: 0.8958 time: 0.11s
     INFO: Early stopping counter 8 of 20
Epoch 32/1000, LR 0.000270
Train loss: 0.0923;  Loss pred: 0.0630; Loss self: 2.9297; time: 0.28s
Val loss: 0.3413 score: 0.8776 time: 0.22s
Test loss: 0.2327 score: 0.8958 time: 0.11s
     INFO: Early stopping counter 9 of 20
Epoch 33/1000, LR 0.000270
Train loss: 0.0882;  Loss pred: 0.0588; Loss self: 2.9375; time: 0.27s
Val loss: 0.3360 score: 0.8776 time: 0.13s
Test loss: 0.2254 score: 0.8958 time: 0.12s
     INFO: Early stopping counter 10 of 20
Epoch 34/1000, LR 0.000270
Train loss: 0.0849;  Loss pred: 0.0555; Loss self: 2.9435; time: 0.28s
Val loss: 0.3341 score: 0.8776 time: 0.20s
Test loss: 0.2210 score: 0.8958 time: 0.15s
     INFO: Early stopping counter 11 of 20
Epoch 35/1000, LR 0.000270
Train loss: 0.0820;  Loss pred: 0.0525; Loss self: 2.9486; time: 0.38s
Val loss: 0.3346 score: 0.8776 time: 0.07s
Test loss: 0.2192 score: 0.8958 time: 0.09s
     INFO: Early stopping counter 12 of 20
Epoch 36/1000, LR 0.000270
Train loss: 0.0794;  Loss pred: 0.0499; Loss self: 2.9520; time: 0.25s
Val loss: 0.3372 score: 0.8776 time: 0.07s
Test loss: 0.2198 score: 0.8958 time: 0.13s
     INFO: Early stopping counter 13 of 20
Epoch 37/1000, LR 0.000270
Train loss: 0.0770;  Loss pred: 0.0474; Loss self: 2.9547; time: 0.41s
Val loss: 0.3413 score: 0.8776 time: 0.13s
Test loss: 0.2223 score: 0.8958 time: 0.25s
     INFO: Early stopping counter 14 of 20
Epoch 38/1000, LR 0.000270
Train loss: 0.0747;  Loss pred: 0.0451; Loss self: 2.9571; time: 0.24s
Val loss: 0.3466 score: 0.8776 time: 0.08s
Test loss: 0.2261 score: 0.8958 time: 0.08s
     INFO: Early stopping counter 15 of 20
Epoch 39/1000, LR 0.000269
Train loss: 0.0726;  Loss pred: 0.0430; Loss self: 2.9588; time: 0.23s
Val loss: 0.3528 score: 0.8776 time: 0.08s
Test loss: 0.2311 score: 0.8958 time: 0.07s
     INFO: Early stopping counter 16 of 20
Epoch 40/1000, LR 0.000269
Train loss: 0.0707;  Loss pred: 0.0411; Loss self: 2.9597; time: 0.23s
Val loss: 0.3596 score: 0.8776 time: 0.07s
Test loss: 0.2366 score: 0.8958 time: 0.07s
     INFO: Early stopping counter 17 of 20
Epoch 41/1000, LR 0.000269
Train loss: 0.0688;  Loss pred: 0.0392; Loss self: 2.9601; time: 0.22s
Val loss: 0.3667 score: 0.8776 time: 0.07s
Test loss: 0.2422 score: 0.8958 time: 0.08s
     INFO: Early stopping counter 18 of 20
Epoch 42/1000, LR 0.000269
Train loss: 0.0672;  Loss pred: 0.0376; Loss self: 2.9600; time: 0.24s
Val loss: 0.3741 score: 0.8776 time: 0.07s
Test loss: 0.2479 score: 0.8958 time: 0.07s
     INFO: Early stopping counter 19 of 20
Epoch 43/1000, LR 0.000269
Train loss: 0.0656;  Loss pred: 0.0360; Loss self: 2.9595; time: 0.23s
Val loss: 0.3813 score: 0.8776 time: 0.08s
Test loss: 0.2534 score: 0.8958 time: 0.14s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 022,   Train_Loss: 0.1553,   Val_Loss: 0.3324,   Val_Precision: 1.0000,   Val_Recall: 0.7600,   Val_accuracy: 0.8636,   Val_Score: 0.8776,   Val_Loss: 0.3324,   Test_Precision: 0.9091,   Test_Recall: 0.8333,   Test_accuracy: 0.8696,   Test_Score: 0.8750,   Test_loss: 0.2688


[0.07579778006765991, 0.12498035503085703, 0.0720478609437123, 0.0715817449381575, 0.071297777001746, 0.0820099200354889, 0.1168902280041948, 0.07478887203615159, 0.07661506801377982, 0.08310861501377076, 0.07792099402286112, 0.24939917400479317, 0.07102595898322761, 0.12269827898126096, 0.1075786710716784, 0.10904887795913965, 0.11286116298288107, 0.08846131409518421, 0.08958206907846034, 0.08914866694249213, 0.07871539401821792, 0.07914467202499509, 0.10827917396090925, 0.07817469409201294, 0.08069903799332678, 0.08812897000461817, 0.08295346202794462, 0.10964709694962949, 0.11277634196449071, 0.1327707499731332, 0.10746902297250926, 0.14313994394615293, 0.11869145603850484, 0.07738077302929014, 0.08374895399902016, 0.07599263498559594, 0.07585973804816604, 0.11612676992081106, 0.07693224598187953, 0.0798889109864831, 0.1137230769963935, 0.0728098260005936, 0.12323251995258033, 0.10328565898817033, 0.11735596403013915, 0.10683211300056428, 0.09227800799999386, 0.23965686291921884, 0.12261835904791951, 0.11761650198604912, 0.09900729602668434, 0.11279556294903159, 0.22775919002015144, 0.1292446319712326, 0.2282884429441765, 0.13714733603410423, 0.10645070997998118, 0.2210533400066197, 0.129340257961303, 0.08855005307123065, 0.08123239001724869, 0.07623362191952765, 0.07504030200652778, 0.08046858408488333, 0.0841838390333578, 0.08067582303192466, 0.07667400396894664, 0.08003474096767604, 0.12452217901591212, 0.07999534800183028, 0.07597287197131664, 0.08049040997866541, 0.10739162797108293, 0.10738607204984874, 0.09136744390707463, 0.11765888100489974, 0.0820591509109363, 0.09132201795000583, 0.09010711091104895, 0.08562336000613868, 0.0909555769758299, 0.12438340904191136, 0.08033566700760275, 0.08146073296666145, 0.08311955607496202, 0.0870211860165, 0.11252298590261489, 0.12015283794607967, 0.0842712779995054, 0.08035610697697848, 0.0838941199472174, 0.15594481804873794, 0.08569909504149109, 0.08063872100319713, 0.08162251301109791, 0.07978836796246469, 0.16379575501196086, 0.0804624070879072, 0.08032664703205228, 0.09411782294046134, 0.22900724003557116, 0.10128206305671483, 0.08373632503207773, 0.08102488494478166, 0.09945947304368019, 0.17827557597775012, 0.09523947292473167, 0.12932515901047736, 0.10959602298680693, 0.07980993296951056, 0.10312783892732114, 0.08158338407520205, 0.08136613806709647, 0.08048088196665049, 0.08517678896896541, 0.0860056810779497, 0.08743569790385664, 0.086290602106601, 0.08053429890424013, 0.08263932995032519, 0.08935279201250523, 0.0917826360091567, 0.10040106403175741, 0.08528852905146778, 0.08705247403122485, 0.12578868202399462, 0.13062772597186267, 0.08155760704539716, 0.08525058208033442, 0.09696453507058322, 0.08753229503054172, 0.09009422198869288, 0.08607525995466858, 0.10575992800295353, 0.08009394304826856, 0.09174251998774707, 0.09624936000909656, 0.08231136295944452, 0.08557626605033875, 0.08305476605892181, 0.08238500193692744, 0.11283874907530844, 0.07910593401174992, 0.15545839094556868, 0.08469398296438158, 0.08245019498281181, 0.07689161191228777, 0.0756801349343732, 0.08560045796912163, 0.0768014460336417, 0.075953941908665, 0.12000638991594315, 0.08838918001856655, 0.07663059898186475, 0.07784610998351127, 0.07766706997063011, 0.08028492296580225, 0.07637395104393363, 0.13780505198519677, 0.09827036294154823, 0.10478276305366307, 0.08787462208420038, 0.19512152997776866, 0.2252871000673622, 0.1131227440200746, 0.173110993928276, 0.11178030609153211, 0.27668848005123436, 0.16869049589149654, 0.08444155903998762, 0.11444877891335636, 0.11381073598749936, 0.12303220201283693, 0.1524235709803179, 0.09839021600782871, 0.13892194803338498, 0.2570666739484295, 0.08164071699138731, 0.0759121379815042, 0.0756815728964284, 0.08572677697520703, 0.07659794599749148, 0.1517397720599547]
[0.0015468934707685697, 0.0025506194904256537, 0.001470364509055353, 0.0014608519375134184, 0.0014550566735050203, 0.0016736718374589573, 0.0023855148572284653, 0.0015263035109418692, 0.0015635728166077513, 0.0016960941839545053, 0.0015902243678134922, 0.005089779061322309, 0.001449509367004645, 0.002504046509821652, 0.0021954830830954774, 0.0022254873052885643, 0.0023032890404669605, 0.0018053329407180451, 0.0018282054913971498, 0.001819360549846778, 0.0016064366126166923, 0.0016151973882652059, 0.0022097790604267194, 0.0015954019202451622, 0.0016469191427209548, 0.0017985504082575136, 0.0016929277964886657, 0.0022376958561148874, 0.0023015579992753206, 0.002709607142308841, 0.002193245366785903, 0.0029212233458398556, 0.002422274613030711, 0.0015791994495773498, 0.0017091623265106156, 0.001550870101746856, 0.0015481579193503273, 0.002369934080016552, 0.0015700458363648883, 0.001630385938499655, 0.0023208791223753777, 0.001485914816338645, 0.0025149493867873537, 0.0021078705915953126, 0.0023950196740844722, 0.0021802472040931484, 0.0018832246530610993, 0.004890956386106507, 0.0025024154907738677, 0.002400336775225492, 0.0020205570617690682, 0.002301950264265951, 0.004648146735105131, 0.0026376455504333184, 0.004658947815187275, 0.0027989252251858004, 0.002172463468979208, 0.00451129265319632, 0.0026395971012510813, 0.001807143940229197, 0.0016578038779030346, 0.0015557882024393398, 0.0015314347348270975, 0.0016422160017323128, 0.0017180375312930163, 0.0016464453679984625, 0.0015647755912029926, 0.0016333620605648172, 0.0025412689595084104, 0.0016325581224863322, 0.0015504667749248293, 0.0016426614281360287, 0.002191665876960876, 0.0021915524908132397, 0.0018646417123892782, 0.002401201653161219, 0.0016746765492027815, 0.0018637146520409354, 0.0018389206308377336, 0.0017474155103293608, 0.001856236264812855, 0.002538436919222681, 0.0016395034083184234, 0.0016624639380951316, 0.0016963174709175922, 0.001775942571765306, 0.002296387467400304, 0.0024520987335934627, 0.001719821999989906, 0.0016399205505505813, 0.001712124896881988, 0.003182547307117101, 0.0017489611232957365, 0.001645688183738717, 0.0016657655716550593, 0.0016283340400502998, 0.003342770510448181, 0.0016420899405695346, 0.0016393193271847404, 0.001920771896744109, 0.004673617143583085, 0.002066980878708466, 0.0017089045924913822, 0.0016535690805057483, 0.0020297851641567386, 0.0036382770607704105, 0.001943662712749626, 0.0026392889593974973, 0.0022366535303429986, 0.0016287741422349093, 0.002104649774026962, 0.001664967021942899, 0.0016605334299407443, 0.0016424669789112344, 0.0017383018156931717, 0.001755217981182647, 0.0017844019980378905, 0.0017610326960530815, 0.0016435571204946966, 0.0016865169377617386, 0.0018235263676021475, 0.0018731150205950349, 0.0020490013067705594, 0.0017405822255401587, 0.001776581102678058, 0.00256711595967336, 0.0026658719586094425, 0.0016644409601101462, 0.0017398077975578454, 0.001978868062664964, 0.001786373367970239, 0.0018386575916059771, 0.0017566379582585425, 0.0021583658776112963, 0.001634570266291195, 0.0018722963262805526, 0.0019642726532468685, 0.0016798237338662148, 0.0017464544091905867, 0.0016949952256922818, 0.0017163542070193216, 0.002350807272402259, 0.0016480402919114567, 0.0032387164780326807, 0.0017644579784246162, 0.001717712395475246, 0.0016019085815059952, 0.0015766694777994417, 0.0017833428743567008, 0.0016000301257008687, 0.0015823737897638541, 0.0025001331232488155, 0.0018414412503868032, 0.0015964708121221822, 0.0016217939579898182, 0.0016180639577214606, 0.0016726025617875469, 0.0015911239800819506, 0.0028709385830249325, 0.0020472992279489213, 0.0021829742302846475, 0.0018307212934208412, 0.004065031874536847, 0.004693481251403379, 0.002356723833751554, 0.0036064790401724167, 0.002328756376906919, 0.005764343334400716, 0.0035143853310728446, 0.0017591991466664088, 0.002384349560694924, 0.00237105699973957, 0.0025631708752674363, 0.003175491062089956, 0.002049796166829765, 0.0028942072506955205, 0.005355555707258948, 0.0017008482706539023, 0.0015815028746146709, 0.0015766994353422585, 0.0017859745203168131, 0.0015957905416144058, 0.0031612452512490563]
[646.4569273171428, 392.06161630683596, 680.1034667535996, 684.5320694868947, 687.2584540581124, 597.4886937921142, 419.196718465136, 655.1776844062348, 639.5608758212805, 589.5898998182186, 628.8420805517953, 196.47218237803096, 689.8886083547436, 399.3536046865295, 455.48062187300934, 449.33979071623446, 434.1617497547175, 553.9144483799562, 546.984463565844, 549.6436646843955, 622.4957724109139, 619.1193765327002, 452.5339288023188, 626.8013014841628, 607.194350991544, 556.0033210127417, 590.6926462393253, 446.88825662671206, 434.48829024289836, 369.05719075862254, 455.94533796528765, 342.3223360939212, 412.8351073905762, 633.2322369208244, 585.081934283899, 644.7993283729104, 645.9289375463986, 421.9526646045006, 636.9240800735411, 613.3517079521914, 430.87121184343204, 672.9860884381253, 397.62231608065076, 474.4124255005445, 417.5331045588438, 458.66358554324563, 531.0040936297981, 204.45898941987085, 399.6138945298615, 416.6082069488179, 494.9130212261687, 434.4142510476357, 215.1395076337628, 379.12599736371624, 214.6407385676637, 357.2799984085381, 460.30693462932095, 221.66595627341636, 378.84569562757633, 553.3593521461116, 603.2076612493546, 642.7610123486522, 652.9824466289791, 608.9332943687901, 582.0594613246823, 607.3690748789752, 639.0692733334397, 612.234129923527, 393.5041964993121, 612.5356189322271, 644.9670616440541, 608.7681751526402, 456.27392866410503, 456.29753528236085, 536.2960580339263, 416.458150727776, 597.1302341792767, 536.5628257013003, 543.7972597786576, 572.2737346033488, 538.7245249735596, 393.9432145929471, 609.940787513008, 601.5168071229324, 589.512291858356, 563.0812707000932, 435.46658140060345, 407.8139213156951, 581.455522726113, 609.7856384958792, 584.0695394483988, 314.2137110621135, 571.7679979733359, 607.6485265441805, 600.3245696850531, 614.1246055195835, 299.15305189943336, 608.9800414057496, 610.0092784956879, 520.6240270878052, 213.96703437144146, 483.7974121099953, 585.1701753239, 604.7524786168277, 492.662976190115, 274.85537338056616, 514.4925575000294, 378.8899265612365, 447.0965155907033, 613.9586662567335, 475.13843506923985, 600.612496716644, 602.2161204160068, 608.8402463122177, 575.274092779588, 569.7298060530412, 560.4118360658581, 567.8486278200581, 608.43641363618, 592.938011833519, 548.3880122418814, 533.8700448210216, 488.0426365252566, 574.5204020394198, 562.8788905232515, 389.5422005507068, 375.11178913544467, 600.8023258054308, 574.7761341245235, 505.3394002697122, 559.7933880621198, 543.8750556739328, 569.2692653592424, 463.3134772806539, 611.7815921544802, 534.1034888353223, 509.0942941892704, 595.3005543614033, 572.5886657776889, 589.9721632499425, 582.6303194936863, 425.38578629549374, 606.781281324235, 308.7642918985727, 566.746282556892, 582.1696359845655, 624.2553486166323, 634.2483406196843, 560.7446635076985, 624.9882323696658, 631.9619336902914, 399.9787014143243, 543.0528939166239, 626.381636549123, 616.6011379395447, 618.0225418333825, 597.8706614745813, 628.4865368872734, 348.31814442591144, 488.4483842656679, 458.0906114817515, 546.232790099592, 246.0005310816747, 213.06146683786028, 424.317854166285, 277.2787499555779, 429.4137462881418, 173.4802980995517, 284.54477975376926, 568.4404758238703, 419.40159131220156, 421.75283011325206, 390.14176138204675, 314.91192399762195, 487.85338570839946, 345.5177578453255, 186.72198641209067, 587.9419212482389, 632.3099477410988, 634.2362897992206, 559.9184023199897, 626.648657152921, 316.3310406254892]
Elapsed: 0.10422458468053408~0.038650351447596154
Time per graph: 0.0021384339312694794~0.0007982560785474261
Speed: 510.0879599883293~122.06085753217147
Total Time: 0.1526
best val loss: 0.3324444890022278 test_score: 0.8750

Testing...
Test loss: 0.2688 score: 0.8750 time: 0.10s
test Score 0.8750
Epoch Time List: [0.4007803910644725, 0.5516984510468319, 0.3782102280529216, 0.3712599908467382, 0.3732258459785953, 0.37547759513836354, 0.4083424049895257, 0.42042544297873974, 0.4518230250105262, 0.41706275602336973, 0.3886409110855311, 0.5671575979795307, 0.4226319710724056, 0.5521726179867983, 0.5846743149450049, 0.5112970778718591, 0.5415604570880532, 0.43187812610995024, 0.41950779710896313, 0.4352545660221949, 0.47617799404542893, 0.42017064488027245, 0.44230440200772136, 0.43471012404188514, 0.41050912195350975, 0.4376249489141628, 0.4275085279950872, 0.4954313209746033, 0.45988705195486546, 0.4431313769891858, 0.41280964692123234, 0.6291287451749668, 0.5804483169922605, 0.4024466769769788, 0.507789001101628, 0.4067114948993549, 0.37432034406811, 0.42239004897419363, 0.43248670001048595, 0.41699008899740875, 0.4443285509478301, 0.40099149208981544, 0.4698884409153834, 0.4830544419819489, 0.5780074680224061, 0.5326850509736687, 0.45630134106613696, 0.7508634820114821, 0.48597013601101935, 0.47096476203296334, 0.46516098408028483, 0.474444248015061, 1.263447117060423, 0.6100300110410899, 1.0071825849590823, 0.6234098139684647, 0.4325231749098748, 0.6552417010534555, 0.4828730800654739, 0.504782555042766, 0.41913732804823667, 0.4021265748888254, 0.3849189068423584, 0.4032844949979335, 0.41202465898822993, 0.41455202805809677, 0.4018342880299315, 0.39765128411818296, 0.4706178050255403, 0.4683229000074789, 0.4474199420074001, 0.4226735479896888, 0.4622969808988273, 0.47642903693486005, 0.4510733550414443, 0.45104597485624254, 0.41766856308095157, 0.47271603194531053, 0.3846845319494605, 0.42235302389599383, 0.4616062599234283, 0.4924150260630995, 0.39042309299111366, 0.36933031294029206, 0.42946929193567485, 0.4016224399674684, 0.4199186251498759, 0.5503297342220321, 0.3905405071564019, 0.3934044858906418, 0.403498413041234, 0.4865909649524838, 0.4124398339772597, 0.4632093511754647, 0.3913276109378785, 0.36985213903244585, 0.45425202103797346, 0.37531524698715657, 0.3669260579627007, 0.46680050692521036, 0.7094643471064046, 0.5303628069814295, 0.42763145198114216, 0.38175516203045845, 0.4068529000505805, 0.63313967094291, 0.41917055402882397, 0.4235042290529236, 0.43116408702917397, 0.4062675170134753, 0.39988122193608433, 0.4745244849473238, 0.39553645194973797, 0.3855419137980789, 0.37665966304484755, 0.39311047992669046, 0.38840423501096666, 0.3834078760119155, 0.3781516801100224, 0.3865536430384964, 0.3938638489926234, 0.4019572751130909, 0.41653916181530803, 0.3863452570512891, 0.37173136707860976, 0.4102867660112679, 0.4126237150048837, 0.37002675398252904, 0.3824094398878515, 0.49307656998280436, 0.40309709298890084, 0.3860527079086751, 0.38196090201381594, 0.399831713992171, 0.3561166441068053, 0.3796582259237766, 0.39911827305331826, 0.38169954798649997, 0.39143043803051114, 0.39497795898932964, 0.4398786531528458, 0.43616595794446766, 0.41521567199379206, 0.4717170310905203, 0.3993391558760777, 0.3979068590560928, 0.3837680129799992, 0.41434021410532296, 0.3876301160780713, 0.3721681770402938, 0.36892546503804624, 0.4298773128539324, 0.5813995570642874, 0.4155642598634586, 0.38744322094134986, 0.3885291481856257, 0.4576258009765297, 0.43662983691319823, 0.4795220469823107, 0.453520096023567, 0.6756953570293263, 0.4629731649765745, 0.5917856581509113, 0.7034870979841799, 0.48315314506180584, 0.5408555610338226, 0.4994865688495338, 0.6954253369476646, 0.5721768940566108, 0.48778430302627385, 0.6991052919765934, 0.6110217869281769, 0.5181881519965827, 0.6229155479231849, 0.5493679728824645, 0.4605885558994487, 0.7893455011071637, 0.39675793098285794, 0.38267906615510583, 0.372801425983198, 0.37210579204838723, 0.38332928996533155, 0.45301440986804664]
Total Epoch List: [78, 62, 43]
Total Time List: [0.09184962895233184, 0.08363577304407954, 0.15262365399394184]
========================training times:2========================
========================k_idx:0========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
emb dim:  64
Model: Ethident
Ethident(
  (encoder): HGATE_encoder(
    (lin1): Linear(in_features=14887, out_features=64, bias=True)
    (account_convs): ModuleList(
      (0): GATEConv()
      (1): GATConv(64, 64, heads=1)
    )
    (subgraph_conv): GATConv(64, 64, heads=1)
    (lin2): Linear(in_features=64, out_features=64, bias=True)
    (lin3): Linear(in_features=64, out_features=2, bias=True)
  )
  (proj_head_g1): Project_Head(
    (block): Sequential(
      (0): Linear(in_features=64, out_features=64, bias=True)
      (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=64, bias=True)
      (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
      (5): ReLU()
    )
    (linear_shortcut): Linear(in_features=64, out_features=64, bias=True)
  )
  (proj_head_g2): Project_Head(
    (block): Sequential(
      (0): Linear(in_features=64, out_features=64, bias=True)
      (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=64, bias=True)
      (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
      (5): ReLU()
    )
    (linear_shortcut): Linear(in_features=64, out_features=64, bias=True)
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=64, bias=True)
    (1): ReLU()
    (2): Dropout(p=0.0, inplace=True)
    (3): Linear(in_features=64, out_features=2, bias=True)
  )
)
Total number of parameters: 1003972
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7a49927ef880>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.7843;  Loss pred: 0.7496; Loss self: 3.4667; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7491 score: 0.5102 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7411 score: 0.4898 time: 0.08s
Epoch 2/1000, LR 0.000000
Train loss: 0.7843;  Loss pred: 0.7496; Loss self: 3.4667; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6849 score: 0.5102 time: 0.10s
Test loss: 0.6806 score: 0.5102 time: 0.19s
Epoch 3/1000, LR 0.000030
Train loss: 0.7201;  Loss pred: 0.6852; Loss self: 3.4846; time: 0.25s
Val loss: 0.6250 score: 0.7143 time: 0.08s
Test loss: 0.6293 score: 0.7755 time: 0.07s
Epoch 4/1000, LR 0.000060
Train loss: 0.6650;  Loss pred: 0.6303; Loss self: 3.4686; time: 0.23s
Val loss: 0.6026 score: 0.6939 time: 0.08s
Test loss: 0.6179 score: 0.6735 time: 0.12s
Epoch 5/1000, LR 0.000090
Train loss: 0.6509;  Loss pred: 0.6166; Loss self: 3.4300; time: 0.27s
Val loss: 0.6113 score: 0.5918 time: 0.09s
Test loss: 0.6363 score: 0.5714 time: 0.12s
     INFO: Early stopping counter 1 of 20
Epoch 6/1000, LR 0.000120
Train loss: 0.6722;  Loss pred: 0.6379; Loss self: 3.4352; time: 0.29s
Val loss: 0.6125 score: 0.5918 time: 0.15s
Test loss: 0.6414 score: 0.5918 time: 0.21s
     INFO: Early stopping counter 2 of 20
Epoch 7/1000, LR 0.000150
Train loss: 0.6729;  Loss pred: 0.6383; Loss self: 3.4601; time: 0.26s
Val loss: 0.5997 score: 0.6122 time: 0.08s
Test loss: 0.6296 score: 0.5918 time: 0.08s
Epoch 8/1000, LR 0.000180
Train loss: 0.6519;  Loss pred: 0.6171; Loss self: 3.4828; time: 0.39s
Val loss: 0.5778 score: 0.6531 time: 0.08s
Test loss: 0.6113 score: 0.6531 time: 0.08s
Epoch 9/1000, LR 0.000210
Train loss: 0.6233;  Loss pred: 0.5883; Loss self: 3.5033; time: 0.26s
Val loss: 0.5609 score: 0.6735 time: 0.09s
Test loss: 0.5934 score: 0.6735 time: 0.07s
Epoch 10/1000, LR 0.000240
Train loss: 0.6031;  Loss pred: 0.5678; Loss self: 3.5258; time: 0.24s
Val loss: 0.5304 score: 0.6939 time: 0.08s
Test loss: 0.5727 score: 0.6531 time: 0.08s
Epoch 11/1000, LR 0.000270
Train loss: 0.5775;  Loss pred: 0.5422; Loss self: 3.5277; time: 0.25s
Val loss: 0.4984 score: 0.7347 time: 0.09s
Test loss: 0.5499 score: 0.6735 time: 0.08s
Epoch 12/1000, LR 0.000270
Train loss: 0.5480;  Loss pred: 0.5130; Loss self: 3.5007; time: 0.27s
Val loss: 0.4835 score: 0.7347 time: 0.08s
Test loss: 0.5383 score: 0.6735 time: 0.08s
Epoch 13/1000, LR 0.000270
Train loss: 0.5307;  Loss pred: 0.4961; Loss self: 3.4646; time: 0.24s
Val loss: 0.4653 score: 0.7551 time: 0.11s
Test loss: 0.5234 score: 0.6939 time: 0.07s
Epoch 14/1000, LR 0.000270
Train loss: 0.5086;  Loss pred: 0.4742; Loss self: 3.4404; time: 0.23s
Val loss: 0.4435 score: 0.8367 time: 0.09s
Test loss: 0.5069 score: 0.7551 time: 0.08s
Epoch 15/1000, LR 0.000270
Train loss: 0.4836;  Loss pred: 0.4493; Loss self: 3.4248; time: 0.23s
Val loss: 0.4270 score: 0.8776 time: 0.16s
Test loss: 0.4956 score: 0.8776 time: 0.16s
Epoch 16/1000, LR 0.000270
Train loss: 0.4636;  Loss pred: 0.4295; Loss self: 3.4118; time: 0.35s
Val loss: 0.4109 score: 0.8980 time: 0.08s
Test loss: 0.4843 score: 0.8571 time: 0.07s
Epoch 17/1000, LR 0.000270
Train loss: 0.4438;  Loss pred: 0.4099; Loss self: 3.3884; time: 0.34s
Val loss: 0.3921 score: 0.8980 time: 0.08s
Test loss: 0.4711 score: 0.8571 time: 0.07s
Epoch 18/1000, LR 0.000270
Train loss: 0.4213;  Loss pred: 0.3878; Loss self: 3.3492; time: 0.24s
Val loss: 0.3703 score: 0.8980 time: 0.08s
Test loss: 0.4565 score: 0.8776 time: 0.08s
Epoch 19/1000, LR 0.000270
Train loss: 0.3977;  Loss pred: 0.3647; Loss self: 3.2997; time: 0.28s
Val loss: 0.3474 score: 0.8980 time: 0.09s
Test loss: 0.4427 score: 0.8367 time: 0.09s
Epoch 20/1000, LR 0.000270
Train loss: 0.3747;  Loss pred: 0.3422; Loss self: 3.2530; time: 0.26s
Val loss: 0.3230 score: 0.8980 time: 0.08s
Test loss: 0.4268 score: 0.8776 time: 0.08s
Epoch 21/1000, LR 0.000270
Train loss: 0.3466;  Loss pred: 0.3144; Loss self: 3.2188; time: 0.25s
Val loss: 0.3024 score: 0.8980 time: 0.08s
Test loss: 0.4107 score: 0.8571 time: 0.10s
Epoch 22/1000, LR 0.000270
Train loss: 0.3157;  Loss pred: 0.2837; Loss self: 3.1953; time: 0.27s
Val loss: 0.2896 score: 0.8980 time: 0.08s
Test loss: 0.4017 score: 0.8571 time: 0.07s
Epoch 23/1000, LR 0.000270
Train loss: 0.2932;  Loss pred: 0.2615; Loss self: 3.1706; time: 0.24s
Val loss: 0.2720 score: 0.8980 time: 0.08s
Test loss: 0.3887 score: 0.8571 time: 0.12s
Epoch 24/1000, LR 0.000270
Train loss: 0.2703;  Loss pred: 0.2390; Loss self: 3.1275; time: 0.28s
Val loss: 0.2510 score: 0.8980 time: 0.08s
Test loss: 0.3759 score: 0.8571 time: 0.07s
Epoch 25/1000, LR 0.000270
Train loss: 0.2472;  Loss pred: 0.2164; Loss self: 3.0782; time: 0.25s
Val loss: 0.2340 score: 0.8980 time: 0.09s
Test loss: 0.3701 score: 0.8571 time: 0.08s
Epoch 26/1000, LR 0.000270
Train loss: 0.2299;  Loss pred: 0.1995; Loss self: 3.0383; time: 0.25s
Val loss: 0.2205 score: 0.9184 time: 0.09s
Test loss: 0.3651 score: 0.8571 time: 0.08s
Epoch 27/1000, LR 0.000270
Train loss: 0.2141;  Loss pred: 0.1840; Loss self: 3.0131; time: 0.24s
Val loss: 0.2088 score: 0.9184 time: 0.07s
Test loss: 0.3598 score: 0.8571 time: 0.07s
Epoch 28/1000, LR 0.000270
Train loss: 0.1986;  Loss pred: 0.1686; Loss self: 2.9984; time: 0.24s
Val loss: 0.2004 score: 0.9184 time: 0.08s
Test loss: 0.3581 score: 0.8571 time: 0.07s
Epoch 29/1000, LR 0.000270
Train loss: 0.1842;  Loss pred: 0.1543; Loss self: 2.9898; time: 0.24s
Val loss: 0.1946 score: 0.9184 time: 0.11s
Test loss: 0.3603 score: 0.8571 time: 0.08s
Epoch 30/1000, LR 0.000270
Train loss: 0.1722;  Loss pred: 0.1424; Loss self: 2.9827; time: 0.29s
Val loss: 0.1894 score: 0.8980 time: 0.09s
Test loss: 0.3647 score: 0.8571 time: 0.08s
Epoch 31/1000, LR 0.000270
Train loss: 0.1638;  Loss pred: 0.1340; Loss self: 2.9784; time: 0.26s
Val loss: 0.1843 score: 0.9184 time: 0.09s
Test loss: 0.3692 score: 0.8571 time: 0.07s
Epoch 32/1000, LR 0.000270
Train loss: 0.1556;  Loss pred: 0.1259; Loss self: 2.9762; time: 0.30s
Val loss: 0.1792 score: 0.9184 time: 0.09s
Test loss: 0.3705 score: 0.8571 time: 0.09s
Epoch 33/1000, LR 0.000270
Train loss: 0.1478;  Loss pred: 0.1180; Loss self: 2.9748; time: 0.24s
Val loss: 0.1732 score: 0.9184 time: 0.08s
Test loss: 0.3702 score: 0.8571 time: 0.07s
Epoch 34/1000, LR 0.000270
Train loss: 0.1406;  Loss pred: 0.1109; Loss self: 2.9739; time: 0.25s
Val loss: 0.1660 score: 0.9388 time: 0.09s
Test loss: 0.3690 score: 0.8571 time: 0.10s
Epoch 35/1000, LR 0.000270
Train loss: 0.1346;  Loss pred: 0.1048; Loss self: 2.9732; time: 0.25s
Val loss: 0.1602 score: 0.9388 time: 0.08s
Test loss: 0.3673 score: 0.8571 time: 0.08s
Epoch 36/1000, LR 0.000270
Train loss: 0.1296;  Loss pred: 0.0998; Loss self: 2.9723; time: 0.25s
Val loss: 0.1558 score: 0.9388 time: 0.09s
Test loss: 0.3638 score: 0.8571 time: 0.08s
Epoch 37/1000, LR 0.000270
Train loss: 0.1255;  Loss pred: 0.0957; Loss self: 2.9724; time: 0.25s
Val loss: 0.1520 score: 0.9592 time: 0.09s
Test loss: 0.3606 score: 0.8571 time: 0.08s
Epoch 38/1000, LR 0.000270
Train loss: 0.1221;  Loss pred: 0.0924; Loss self: 2.9728; time: 0.23s
Val loss: 0.1489 score: 0.9592 time: 0.08s
Test loss: 0.3582 score: 0.8571 time: 0.15s
Epoch 39/1000, LR 0.000269
Train loss: 0.1190;  Loss pred: 0.0893; Loss self: 2.9723; time: 0.23s
Val loss: 0.1467 score: 0.9592 time: 0.07s
Test loss: 0.3554 score: 0.8571 time: 0.07s
Epoch 40/1000, LR 0.000269
Train loss: 0.1157;  Loss pred: 0.0860; Loss self: 2.9719; time: 0.26s
Val loss: 0.1439 score: 0.9592 time: 0.13s
Test loss: 0.3535 score: 0.8571 time: 0.12s
Epoch 41/1000, LR 0.000269
Train loss: 0.1118;  Loss pred: 0.0821; Loss self: 2.9700; time: 0.22s
Val loss: 0.1404 score: 0.9592 time: 0.07s
Test loss: 0.3524 score: 0.8571 time: 0.07s
Epoch 42/1000, LR 0.000269
Train loss: 0.1075;  Loss pred: 0.0779; Loss self: 2.9661; time: 0.23s
Val loss: 0.1365 score: 0.9592 time: 0.09s
Test loss: 0.3518 score: 0.8571 time: 0.10s
Epoch 43/1000, LR 0.000269
Train loss: 0.1028;  Loss pred: 0.0732; Loss self: 2.9602; time: 0.23s
Val loss: 0.1325 score: 0.9592 time: 0.08s
Test loss: 0.3501 score: 0.8571 time: 0.08s
Epoch 44/1000, LR 0.000269
Train loss: 0.0980;  Loss pred: 0.0684; Loss self: 2.9524; time: 0.25s
Val loss: 0.1292 score: 0.9796 time: 0.08s
Test loss: 0.3473 score: 0.8571 time: 0.12s
Epoch 45/1000, LR 0.000269
Train loss: 0.0932;  Loss pred: 0.0637; Loss self: 2.9425; time: 0.28s
Val loss: 0.1255 score: 0.9796 time: 0.09s
Test loss: 0.3441 score: 0.8571 time: 0.08s
Epoch 46/1000, LR 0.000269
Train loss: 0.0876;  Loss pred: 0.0583; Loss self: 2.9319; time: 0.25s
Val loss: 0.1219 score: 0.9796 time: 0.08s
Test loss: 0.3403 score: 0.8776 time: 0.08s
Epoch 47/1000, LR 0.000269
Train loss: 0.0819;  Loss pred: 0.0527; Loss self: 2.9208; time: 0.24s
Val loss: 0.1193 score: 0.9796 time: 0.08s
Test loss: 0.3355 score: 0.8776 time: 0.08s
Epoch 48/1000, LR 0.000269
Train loss: 0.0761;  Loss pred: 0.0470; Loss self: 2.9091; time: 0.26s
Val loss: 0.1176 score: 0.9796 time: 0.10s
Test loss: 0.3297 score: 0.8776 time: 0.08s
Epoch 49/1000, LR 0.000269
Train loss: 0.0710;  Loss pred: 0.0420; Loss self: 2.8957; time: 0.25s
Val loss: 0.1170 score: 0.9796 time: 0.08s
Test loss: 0.3249 score: 0.8776 time: 0.08s
Epoch 50/1000, LR 0.000269
Train loss: 0.0674;  Loss pred: 0.0386; Loss self: 2.8817; time: 0.26s
Val loss: 0.1168 score: 0.9796 time: 0.08s
Test loss: 0.3223 score: 0.8776 time: 0.08s
Epoch 51/1000, LR 0.000269
Train loss: 0.0644;  Loss pred: 0.0358; Loss self: 2.8690; time: 0.25s
Val loss: 0.1158 score: 0.9796 time: 0.08s
Test loss: 0.3214 score: 0.8776 time: 0.08s
Epoch 52/1000, LR 0.000269
Train loss: 0.0618;  Loss pred: 0.0332; Loss self: 2.8575; time: 0.26s
Val loss: 0.1148 score: 0.9796 time: 0.09s
Test loss: 0.3242 score: 0.8776 time: 0.07s
Epoch 53/1000, LR 0.000269
Train loss: 0.0593;  Loss pred: 0.0308; Loss self: 2.8483; time: 0.24s
Val loss: 0.1138 score: 0.9796 time: 0.08s
Test loss: 0.3302 score: 0.8776 time: 0.11s
Epoch 54/1000, LR 0.000269
Train loss: 0.0566;  Loss pred: 0.0282; Loss self: 2.8422; time: 0.24s
Val loss: 0.1125 score: 0.9592 time: 0.09s
Test loss: 0.3385 score: 0.8571 time: 0.09s
Epoch 55/1000, LR 0.000269
Train loss: 0.0539;  Loss pred: 0.0256; Loss self: 2.8392; time: 0.26s
Val loss: 0.1120 score: 0.9592 time: 0.14s
Test loss: 0.3511 score: 0.8571 time: 0.10s
Epoch 56/1000, LR 0.000269
Train loss: 0.0514;  Loss pred: 0.0231; Loss self: 2.8398; time: 0.50s
Val loss: 0.1126 score: 0.9592 time: 0.09s
Test loss: 0.3656 score: 0.8571 time: 0.09s
     INFO: Early stopping counter 1 of 20
Epoch 57/1000, LR 0.000269
Train loss: 0.0493;  Loss pred: 0.0209; Loss self: 2.8431; time: 0.30s
Val loss: 0.1142 score: 0.9592 time: 0.08s
Test loss: 0.3791 score: 0.8571 time: 0.08s
     INFO: Early stopping counter 2 of 20
Epoch 58/1000, LR 0.000269
Train loss: 0.0475;  Loss pred: 0.0190; Loss self: 2.8477; time: 0.25s
Val loss: 0.1156 score: 0.9592 time: 0.08s
Test loss: 0.3916 score: 0.8571 time: 0.08s
     INFO: Early stopping counter 3 of 20
Epoch 59/1000, LR 0.000268
Train loss: 0.0459;  Loss pred: 0.0174; Loss self: 2.8526; time: 0.25s
Val loss: 0.1168 score: 0.9592 time: 0.08s
Test loss: 0.4023 score: 0.8571 time: 0.07s
     INFO: Early stopping counter 4 of 20
Epoch 60/1000, LR 0.000268
Train loss: 0.0445;  Loss pred: 0.0160; Loss self: 2.8577; time: 0.25s
Val loss: 0.1176 score: 0.9592 time: 0.13s
Test loss: 0.4101 score: 0.8571 time: 0.24s
     INFO: Early stopping counter 5 of 20
Epoch 61/1000, LR 0.000268
Train loss: 0.0433;  Loss pred: 0.0146; Loss self: 2.8631; time: 0.46s
Val loss: 0.1181 score: 0.9592 time: 0.09s
Test loss: 0.4151 score: 0.8571 time: 0.09s
     INFO: Early stopping counter 6 of 20
Epoch 62/1000, LR 0.000268
Train loss: 0.0421;  Loss pred: 0.0135; Loss self: 2.8686; time: 0.23s
Val loss: 0.1185 score: 0.9592 time: 0.08s
Test loss: 0.4171 score: 0.8571 time: 0.07s
     INFO: Early stopping counter 7 of 20
Epoch 63/1000, LR 0.000268
Train loss: 0.0411;  Loss pred: 0.0124; Loss self: 2.8742; time: 0.23s
Val loss: 0.1189 score: 0.9592 time: 0.08s
Test loss: 0.4168 score: 0.8571 time: 0.07s
     INFO: Early stopping counter 8 of 20
Epoch 64/1000, LR 0.000268
Train loss: 0.0402;  Loss pred: 0.0114; Loss self: 2.8799; time: 0.22s
Val loss: 0.1189 score: 0.9592 time: 0.32s
Test loss: 0.4137 score: 0.8571 time: 0.09s
     INFO: Early stopping counter 9 of 20
Epoch 65/1000, LR 0.000268
Train loss: 0.0395;  Loss pred: 0.0106; Loss self: 2.8854; time: 0.33s
Val loss: 0.1190 score: 0.9592 time: 0.09s
Test loss: 0.4097 score: 0.8571 time: 0.16s
     INFO: Early stopping counter 10 of 20
Epoch 66/1000, LR 0.000268
Train loss: 0.0388;  Loss pred: 0.0099; Loss self: 2.8907; time: 0.33s
Val loss: 0.1186 score: 0.9592 time: 0.12s
Test loss: 0.4053 score: 0.8571 time: 0.07s
     INFO: Early stopping counter 11 of 20
Epoch 67/1000, LR 0.000268
Train loss: 0.0383;  Loss pred: 0.0093; Loss self: 2.8955; time: 0.25s
Val loss: 0.1180 score: 0.9592 time: 0.09s
Test loss: 0.4004 score: 0.8571 time: 0.08s
     INFO: Early stopping counter 12 of 20
Epoch 68/1000, LR 0.000268
Train loss: 0.0378;  Loss pred: 0.0088; Loss self: 2.8998; time: 0.25s
Val loss: 0.1175 score: 0.9592 time: 0.09s
Test loss: 0.3960 score: 0.8571 time: 0.11s
     INFO: Early stopping counter 13 of 20
Epoch 69/1000, LR 0.000268
Train loss: 0.0373;  Loss pred: 0.0083; Loss self: 2.9037; time: 0.27s
Val loss: 0.1171 score: 0.9592 time: 0.25s
Test loss: 0.3921 score: 0.8776 time: 0.13s
     INFO: Early stopping counter 14 of 20
Epoch 70/1000, LR 0.000268
Train loss: 0.0369;  Loss pred: 0.0078; Loss self: 2.9071; time: 0.25s
Val loss: 0.1165 score: 0.9592 time: 0.09s
Test loss: 0.3888 score: 0.8776 time: 0.10s
     INFO: Early stopping counter 15 of 20
Epoch 71/1000, LR 0.000268
Train loss: 0.0365;  Loss pred: 0.0074; Loss self: 2.9100; time: 0.24s
Val loss: 0.1160 score: 0.9592 time: 0.09s
Test loss: 0.3856 score: 0.8776 time: 0.11s
     INFO: Early stopping counter 16 of 20
Epoch 72/1000, LR 0.000267
Train loss: 0.0362;  Loss pred: 0.0071; Loss self: 2.9126; time: 0.33s
Val loss: 0.1156 score: 0.9592 time: 0.13s
Test loss: 0.3830 score: 0.8776 time: 0.11s
     INFO: Early stopping counter 17 of 20
Epoch 73/1000, LR 0.000267
Train loss: 0.0359;  Loss pred: 0.0067; Loss self: 2.9148; time: 0.34s
Val loss: 0.1152 score: 0.9592 time: 0.14s
Test loss: 0.3809 score: 0.8571 time: 0.10s
     INFO: Early stopping counter 18 of 20
Epoch 74/1000, LR 0.000267
Train loss: 0.0356;  Loss pred: 0.0064; Loss self: 2.9166; time: 0.29s
Val loss: 0.1149 score: 0.9592 time: 0.08s
Test loss: 0.3795 score: 0.8571 time: 0.06s
     INFO: Early stopping counter 19 of 20
Epoch 75/1000, LR 0.000267
Train loss: 0.0353;  Loss pred: 0.0061; Loss self: 2.9180; time: 0.23s
Val loss: 0.1147 score: 0.9592 time: 0.08s
Test loss: 0.3785 score: 0.8571 time: 0.07s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 054,   Train_Loss: 0.0539,   Val_Loss: 0.1120,   Val_Precision: 0.9583,   Val_Recall: 0.9583,   Val_accuracy: 0.9583,   Val_Score: 0.9592,   Val_Loss: 0.1120,   Test_Precision: 0.9500,   Test_Recall: 0.7600,   Test_accuracy: 0.8444,   Test_Score: 0.8571,   Test_loss: 0.3511


[0.0822108689462766, 0.1919707260094583, 0.0775856050895527, 0.1258895000210032, 0.12200248008593917, 0.21538004698231816, 0.08034527895506471, 0.08553361601661891, 0.07836883806157857, 0.07981945900246501, 0.08099335501901805, 0.08076202403753996, 0.0733612289186567, 0.0806000700686127, 0.16822781995870173, 0.07689131493680179, 0.07497978897299618, 0.08618391107302159, 0.09108580090105534, 0.08469932596199214, 0.10165938502177596, 0.078079893020913, 0.12570375797804445, 0.07913009892217815, 0.08013161597773433, 0.08555564202833921, 0.07850829197559506, 0.07548691798001528, 0.08097247208934277, 0.08530214393977076, 0.07492151402402669, 0.09377067105378956, 0.07668030902277678, 0.10190235497429967, 0.08052870607934892, 0.08103544800542295, 0.08276152506005019, 0.15497123601380736, 0.07677015895023942, 0.12851901398971677, 0.0747830830514431, 0.1000933040631935, 0.08265943999867886, 0.12380159390158951, 0.08043315401300788, 0.08493194996844977, 0.08911309402901679, 0.07960248808376491, 0.08641995792277157, 0.0843153630848974, 0.08684581296984106, 0.07407773309387267, 0.11997971497476101, 0.09011715499218553, 0.10347434994764626, 0.09906219702679664, 0.08371546200942248, 0.07985387404914945, 0.07860535790678114, 0.2425458850339055, 0.09649909404106438, 0.07393329998012632, 0.07155625894665718, 0.09095546999014914, 0.1622127729933709, 0.07457207702100277, 0.08314530702773482, 0.11273038503713906, 0.1407508369302377, 0.10662928503006697, 0.11870143900159746, 0.11821022199001163, 0.10714370792265981, 0.06868412194307894, 0.07189519796520472]
[0.001677772835638298, 0.003917769918560373, 0.001583379695705157, 0.0025691734698163916, 0.0024898465323661056, 0.0043955111629044525, 0.0016396995705115248, 0.0017455840003391613, 0.0015993640420730322, 0.0016289685510707144, 0.0016529256126330215, 0.001648204572194693, 0.001497167937115443, 0.0016448993891553612, 0.0034332208154837086, 0.0015692105089143223, 0.0015301997749591057, 0.0017588553280208487, 0.0018588938959399049, 0.0017285576726937173, 0.0020746813269750196, 0.0015934672045084287, 0.0025653828158784583, 0.0016148999780036357, 0.0016353391015864148, 0.0017460335107824328, 0.0016022100403182665, 0.001540549346530924, 0.0016524994303947504, 0.0017408600804034848, 0.001529010490286259, 0.001913687164363052, 0.0015649042657709547, 0.002079639897434687, 0.0016434429812112025, 0.001653784653171897, 0.0016890107155112283, 0.0031626782859960683, 0.001566737937759988, 0.002622837020198301, 0.0015261853683967979, 0.0020427204910855815, 0.0016869273469118135, 0.0025265631408487657, 0.0016414929390409772, 0.0017333051013969341, 0.0018186345720207508, 0.0016245405731380594, 0.0017636726106688076, 0.001720721695610151, 0.0017723635299967564, 0.0015117904713035238, 0.0024485656117298163, 0.001839125612085419, 0.002111721427502985, 0.0020216774903427884, 0.0017084788165188261, 0.0016296708989622338, 0.001604190977689411, 0.0049499160211001125, 0.0019693692661441713, 0.0015088428567372719, 0.0014603318152379018, 0.0018562340814316151, 0.0033104647549667527, 0.0015218791228776075, 0.0016968430005660166, 0.0023006201027987562, 0.0028724660598007695, 0.0021761078577564688, 0.002422478346971377, 0.0024124535100002375, 0.0021866062841359147, 0.0014017167743485496, 0.0014672489380654023]
[596.0282457544715, 255.24725054999163, 631.5604543322445, 389.23023756409333, 401.63117967343084, 227.50482547727694, 609.8678184614255, 572.8741783871204, 625.2485198453254, 613.8853935164703, 604.987902877888, 606.7208020594398, 667.9277422455865, 607.9399181450785, 291.27168153298913, 637.2631296561112, 653.5094412928696, 568.5515937943853, 537.9543190626134, 578.5170005011397, 482.0017353981037, 627.5623352464864, 389.8053708828529, 619.2333974988442, 611.49397029027, 572.7266938604631, 624.137893806581, 649.1190965429595, 605.1439302227893, 574.4287040967857, 654.0177496184356, 522.5514486495697, 639.0167257339202, 480.85247894769526, 608.4786703479114, 604.6736484596332, 592.0625552084321, 316.1877085089151, 638.2688360950331, 381.26654164900964, 655.2284019407574, 489.5432362694716, 592.7937571411464, 395.7945811178355, 609.2015239396876, 576.9324738005232, 549.8630760597846, 615.5586487251226, 566.9986560718811, 581.151503204247, 564.218335051066, 661.4673256524508, 408.40237043659977, 543.7366504107792, 473.5473093070116, 494.6387367801397, 585.3160076269407, 613.6208240797544, 623.3671762948981, 202.0236294388185, 507.7767878229868, 662.7595415485508, 684.7758773488686, 538.7251586441905, 302.0723898358021, 657.0824088244109, 589.3297138665331, 434.6654185901781, 348.1329210446297, 459.5360457137375, 412.8003873595885, 414.515759932676, 457.3297018558473, 713.4108817844117, 681.5476052199571]
Elapsed: 0.0973511021475618~0.032641404950762176
Time per graph: 0.0019867571866849345~0.0006661511214441259
Speed: 540.5148793138123~117.48658643705556
Total Time: 0.0723
best val loss: 0.11202115565538406 test_score: 0.8571

Testing...
Test loss: 0.3473 score: 0.8571 time: 0.07s
test Score 0.8571
Epoch Time List: [0.39578646095469594, 0.5258555840700865, 0.4026856740238145, 0.4278206310700625, 0.467819846002385, 0.6515395969618112, 0.4175676649902016, 0.5503474089782685, 0.4265239519299939, 0.4028221048647538, 0.42095670895650983, 0.4266176460077986, 0.4204322400037199, 0.4044971000403166, 0.5581564218737185, 0.5026286209467798, 0.4859235550975427, 0.40031100797932595, 0.4655602319398895, 0.42038007196970284, 0.42827417713124305, 0.41743412415962666, 0.4425220830598846, 0.43604431697167456, 0.4070489809382707, 0.4130787869216874, 0.3940317799570039, 0.38615717797074467, 0.4308566309046, 0.4550861819880083, 0.41537020809482783, 0.47363303496968, 0.4004715859191492, 0.4427669399883598, 0.40427433303557336, 0.4103048350661993, 0.4119646449107677, 0.4574936638819054, 0.3814487080089748, 0.5115188440540805, 0.3700429010204971, 0.4096368111204356, 0.384090143837966, 0.4504503790521994, 0.4439002750441432, 0.41260244185104966, 0.4022644329816103, 0.430324112996459, 0.4194762709084898, 0.419574867002666, 0.4150240420131013, 0.42678257497027516, 0.4402795350179076, 0.4074930108617991, 0.49695852992590517, 0.688481007120572, 0.46313751698471606, 0.40912802203092724, 0.4011010261019692, 0.6116448498796672, 0.6473373001208529, 0.3841582011664286, 0.37279291392769665, 0.6227723601041362, 0.5853807928506285, 0.5209252788918093, 0.4153331269044429, 0.4494899120181799, 0.6537859330419451, 0.4434493030421436, 0.4465473419986665, 0.5683755681384355, 0.576937458012253, 0.43185365106910467, 0.37866739102173597]
Total Epoch List: [75]
Total Time List: [0.07230219605844468]
========================k_idx:1========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
emb dim:  64
Model: Ethident
Ethident(
  (encoder): HGATE_encoder(
    (lin1): Linear(in_features=14887, out_features=64, bias=True)
    (account_convs): ModuleList(
      (0): GATEConv()
      (1): GATConv(64, 64, heads=1)
    )
    (subgraph_conv): GATConv(64, 64, heads=1)
    (lin2): Linear(in_features=64, out_features=64, bias=True)
    (lin3): Linear(in_features=64, out_features=2, bias=True)
  )
  (proj_head_g1): Project_Head(
    (block): Sequential(
      (0): Linear(in_features=64, out_features=64, bias=True)
      (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=64, bias=True)
      (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
      (5): ReLU()
    )
    (linear_shortcut): Linear(in_features=64, out_features=64, bias=True)
  )
  (proj_head_g2): Project_Head(
    (block): Sequential(
      (0): Linear(in_features=64, out_features=64, bias=True)
      (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=64, bias=True)
      (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
      (5): ReLU()
    )
    (linear_shortcut): Linear(in_features=64, out_features=64, bias=True)
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=64, bias=True)
    (1): ReLU()
    (2): Dropout(p=0.0, inplace=True)
    (3): Linear(in_features=64, out_features=2, bias=True)
  )
)
Total number of parameters: 1003972
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7a49927ef460>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.8505;  Loss pred: 0.8188; Loss self: 3.1690; time: 0.30s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.8253 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.8364 score: 0.5102 time: 0.10s
Epoch 2/1000, LR 0.000000
Train loss: 0.8505;  Loss pred: 0.8188; Loss self: 3.1690; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7473 score: 0.4898 time: 0.07s
Test loss: 0.7382 score: 0.5306 time: 0.09s
Epoch 3/1000, LR 0.000030
Train loss: 0.7381;  Loss pred: 0.7065; Loss self: 3.1557; time: 0.24s
Val loss: 0.6715 score: 0.5714 time: 0.07s
Test loss: 0.6264 score: 0.6327 time: 0.09s
Epoch 4/1000, LR 0.000060
Train loss: 0.6200;  Loss pred: 0.5885; Loss self: 3.1467; time: 0.28s
Val loss: 0.6411 score: 0.6531 time: 0.07s
Test loss: 0.5736 score: 0.7347 time: 0.09s
Epoch 5/1000, LR 0.000090
Train loss: 0.5705;  Loss pred: 0.5385; Loss self: 3.1958; time: 0.24s
Val loss: 0.6268 score: 0.6735 time: 0.07s
Test loss: 0.5552 score: 0.7143 time: 0.08s
Epoch 6/1000, LR 0.000120
Train loss: 0.5600;  Loss pred: 0.5272; Loss self: 3.2793; time: 0.23s
Val loss: 0.6264 score: 0.6327 time: 0.08s
Test loss: 0.5619 score: 0.7143 time: 0.08s
Epoch 7/1000, LR 0.000150
Train loss: 0.5665;  Loss pred: 0.5330; Loss self: 3.3554; time: 0.23s
Val loss: 0.6161 score: 0.6327 time: 0.07s
Test loss: 0.5561 score: 0.7143 time: 0.08s
Epoch 8/1000, LR 0.000180
Train loss: 0.5680;  Loss pred: 0.5338; Loss self: 3.4225; time: 0.23s
Val loss: 0.6082 score: 0.6327 time: 0.07s
Test loss: 0.5454 score: 0.7143 time: 0.10s
Epoch 9/1000, LR 0.000210
Train loss: 0.5659;  Loss pred: 0.5312; Loss self: 3.4733; time: 0.23s
Val loss: 0.6029 score: 0.6735 time: 0.07s
Test loss: 0.5368 score: 0.7143 time: 0.08s
Epoch 10/1000, LR 0.000240
Train loss: 0.5611;  Loss pred: 0.5260; Loss self: 3.5097; time: 0.23s
Val loss: 0.5958 score: 0.6939 time: 0.07s
Test loss: 0.5259 score: 0.7143 time: 0.08s
Epoch 11/1000, LR 0.000270
Train loss: 0.5482;  Loss pred: 0.5128; Loss self: 3.5321; time: 0.23s
Val loss: 0.5876 score: 0.7143 time: 0.07s
Test loss: 0.5133 score: 0.7143 time: 0.08s
Epoch 12/1000, LR 0.000270
Train loss: 0.5321;  Loss pred: 0.4967; Loss self: 3.5414; time: 0.25s
Val loss: 0.5796 score: 0.7347 time: 0.09s
Test loss: 0.5019 score: 0.7755 time: 0.09s
Epoch 13/1000, LR 0.000270
Train loss: 0.5143;  Loss pred: 0.4789; Loss self: 3.5420; time: 0.24s
Val loss: 0.5728 score: 0.7347 time: 0.07s
Test loss: 0.4894 score: 0.8367 time: 0.09s
Epoch 14/1000, LR 0.000270
Train loss: 0.4977;  Loss pred: 0.4623; Loss self: 3.5366; time: 0.24s
Val loss: 0.5649 score: 0.7347 time: 0.07s
Test loss: 0.4760 score: 0.8367 time: 0.10s
Epoch 15/1000, LR 0.000270
Train loss: 0.4799;  Loss pred: 0.4446; Loss self: 3.5258; time: 0.23s
Val loss: 0.5558 score: 0.7347 time: 0.07s
Test loss: 0.4610 score: 0.8367 time: 0.08s
Epoch 16/1000, LR 0.000270
Train loss: 0.4607;  Loss pred: 0.4256; Loss self: 3.5107; time: 0.27s
Val loss: 0.5460 score: 0.7551 time: 0.09s
Test loss: 0.4451 score: 0.8571 time: 0.10s
Epoch 17/1000, LR 0.000270
Train loss: 0.4396;  Loss pred: 0.4047; Loss self: 3.4929; time: 0.23s
Val loss: 0.5358 score: 0.7551 time: 0.15s
Test loss: 0.4281 score: 0.8571 time: 0.35s
Epoch 18/1000, LR 0.000270
Train loss: 0.4176;  Loss pred: 0.3828; Loss self: 3.4723; time: 0.36s
Val loss: 0.5236 score: 0.7551 time: 0.15s
Test loss: 0.4086 score: 0.8571 time: 0.10s
Epoch 19/1000, LR 0.000270
Train loss: 0.3941;  Loss pred: 0.3596; Loss self: 3.4489; time: 0.29s
Val loss: 0.5118 score: 0.7551 time: 0.07s
Test loss: 0.3884 score: 0.8776 time: 0.13s
Epoch 20/1000, LR 0.000270
Train loss: 0.3708;  Loss pred: 0.3365; Loss self: 3.4222; time: 0.24s
Val loss: 0.4997 score: 0.7755 time: 0.08s
Test loss: 0.3691 score: 0.8776 time: 0.14s
Epoch 21/1000, LR 0.000270
Train loss: 0.3487;  Loss pred: 0.3148; Loss self: 3.3939; time: 1.02s
Val loss: 0.4879 score: 0.7755 time: 0.15s
Test loss: 0.3506 score: 0.8776 time: 0.16s
Epoch 22/1000, LR 0.000270
Train loss: 0.3269;  Loss pred: 0.2932; Loss self: 3.3641; time: 0.27s
Val loss: 0.4765 score: 0.7959 time: 0.11s
Test loss: 0.3322 score: 0.8776 time: 0.09s
Epoch 23/1000, LR 0.000270
Train loss: 0.3044;  Loss pred: 0.2711; Loss self: 3.3352; time: 0.33s
Val loss: 0.4669 score: 0.7959 time: 0.17s
Test loss: 0.3144 score: 0.8776 time: 0.23s
Epoch 24/1000, LR 0.000270
Train loss: 0.2821;  Loss pred: 0.2490; Loss self: 3.3083; time: 0.36s
Val loss: 0.4610 score: 0.8163 time: 0.07s
Test loss: 0.2995 score: 0.8776 time: 0.21s
Epoch 25/1000, LR 0.000270
Train loss: 0.2623;  Loss pred: 0.2295; Loss self: 3.2836; time: 0.26s
Val loss: 0.4566 score: 0.8163 time: 0.19s
Test loss: 0.2862 score: 0.8980 time: 0.14s
Epoch 26/1000, LR 0.000270
Train loss: 0.2447;  Loss pred: 0.2121; Loss self: 3.2567; time: 0.35s
Val loss: 0.4516 score: 0.8163 time: 0.40s
Test loss: 0.2726 score: 0.8980 time: 0.13s
Epoch 27/1000, LR 0.000270
Train loss: 0.2277;  Loss pred: 0.1954; Loss self: 3.2249; time: 0.23s
Val loss: 0.4450 score: 0.8163 time: 0.09s
Test loss: 0.2582 score: 0.8980 time: 0.13s
Epoch 28/1000, LR 0.000270
Train loss: 0.2111;  Loss pred: 0.1792; Loss self: 3.1896; time: 0.32s
Val loss: 0.4382 score: 0.8163 time: 0.09s
Test loss: 0.2434 score: 0.8980 time: 0.12s
Epoch 29/1000, LR 0.000270
Train loss: 0.1949;  Loss pred: 0.1634; Loss self: 3.1539; time: 0.34s
Val loss: 0.4327 score: 0.8367 time: 0.07s
Test loss: 0.2294 score: 0.8980 time: 0.09s
Epoch 30/1000, LR 0.000270
Train loss: 0.1802;  Loss pred: 0.1490; Loss self: 3.1202; time: 0.38s
Val loss: 0.4296 score: 0.8367 time: 0.08s
Test loss: 0.2172 score: 0.8980 time: 0.13s
Epoch 31/1000, LR 0.000270
Train loss: 0.1675;  Loss pred: 0.1366; Loss self: 3.0916; time: 0.50s
Val loss: 0.4303 score: 0.8367 time: 0.08s
Test loss: 0.2072 score: 0.8980 time: 0.17s
     INFO: Early stopping counter 1 of 20
Epoch 32/1000, LR 0.000270
Train loss: 0.1561;  Loss pred: 0.1254; Loss self: 3.0685; time: 0.27s
Val loss: 0.4343 score: 0.8367 time: 0.09s
Test loss: 0.1997 score: 0.8980 time: 0.14s
     INFO: Early stopping counter 2 of 20
Epoch 33/1000, LR 0.000270
Train loss: 0.1456;  Loss pred: 0.1151; Loss self: 3.0497; time: 0.28s
Val loss: 0.4409 score: 0.8367 time: 0.07s
Test loss: 0.1938 score: 0.8980 time: 0.09s
     INFO: Early stopping counter 3 of 20
Epoch 34/1000, LR 0.000270
Train loss: 0.1364;  Loss pred: 0.1060; Loss self: 3.0330; time: 0.22s
Val loss: 0.4485 score: 0.8367 time: 0.08s
Test loss: 0.1884 score: 0.9184 time: 0.08s
     INFO: Early stopping counter 4 of 20
Epoch 35/1000, LR 0.000270
Train loss: 0.1282;  Loss pred: 0.0981; Loss self: 3.0177; time: 0.23s
Val loss: 0.4551 score: 0.8367 time: 0.07s
Test loss: 0.1825 score: 0.9184 time: 0.08s
     INFO: Early stopping counter 5 of 20
Epoch 36/1000, LR 0.000270
Train loss: 0.1209;  Loss pred: 0.0908; Loss self: 3.0034; time: 0.22s
Val loss: 0.4601 score: 0.8367 time: 0.07s
Test loss: 0.1762 score: 0.9184 time: 0.08s
     INFO: Early stopping counter 6 of 20
Epoch 37/1000, LR 0.000270
Train loss: 0.1141;  Loss pred: 0.0842; Loss self: 2.9907; time: 0.22s
Val loss: 0.4639 score: 0.8367 time: 0.07s
Test loss: 0.1695 score: 0.9184 time: 0.08s
     INFO: Early stopping counter 7 of 20
Epoch 38/1000, LR 0.000270
Train loss: 0.1079;  Loss pred: 0.0781; Loss self: 2.9795; time: 0.22s
Val loss: 0.4675 score: 0.8367 time: 0.08s
Test loss: 0.1628 score: 0.9184 time: 0.08s
     INFO: Early stopping counter 8 of 20
Epoch 39/1000, LR 0.000269
Train loss: 0.1023;  Loss pred: 0.0726; Loss self: 2.9709; time: 0.24s
Val loss: 0.4704 score: 0.8367 time: 0.07s
Test loss: 0.1559 score: 0.9184 time: 0.08s
     INFO: Early stopping counter 9 of 20
Epoch 40/1000, LR 0.000269
Train loss: 0.0972;  Loss pred: 0.0676; Loss self: 2.9649; time: 0.23s
Val loss: 0.4737 score: 0.8367 time: 0.07s
Test loss: 0.1493 score: 0.9184 time: 0.08s
     INFO: Early stopping counter 10 of 20
Epoch 41/1000, LR 0.000269
Train loss: 0.0924;  Loss pred: 0.0628; Loss self: 2.9611; time: 0.27s
Val loss: 0.4770 score: 0.8367 time: 0.24s
Test loss: 0.1438 score: 0.9184 time: 0.09s
     INFO: Early stopping counter 11 of 20
Epoch 42/1000, LR 0.000269
Train loss: 0.0883;  Loss pred: 0.0587; Loss self: 2.9589; time: 0.30s
Val loss: 0.4796 score: 0.8367 time: 0.07s
Test loss: 0.1389 score: 0.9184 time: 0.08s
     INFO: Early stopping counter 12 of 20
Epoch 43/1000, LR 0.000269
Train loss: 0.0846;  Loss pred: 0.0550; Loss self: 2.9578; time: 0.23s
Val loss: 0.4828 score: 0.8367 time: 0.07s
Test loss: 0.1348 score: 0.9184 time: 0.09s
     INFO: Early stopping counter 13 of 20
Epoch 44/1000, LR 0.000269
Train loss: 0.0811;  Loss pred: 0.0515; Loss self: 2.9574; time: 0.23s
Val loss: 0.4872 score: 0.8367 time: 0.07s
Test loss: 0.1313 score: 0.9184 time: 0.17s
     INFO: Early stopping counter 14 of 20
Epoch 45/1000, LR 0.000269
Train loss: 0.0778;  Loss pred: 0.0482; Loss self: 2.9576; time: 0.27s
Val loss: 0.4927 score: 0.8367 time: 0.07s
Test loss: 0.1286 score: 0.9184 time: 0.10s
     INFO: Early stopping counter 15 of 20
Epoch 46/1000, LR 0.000269
Train loss: 0.0746;  Loss pred: 0.0450; Loss self: 2.9583; time: 0.25s
Val loss: 0.4988 score: 0.8367 time: 0.07s
Test loss: 0.1264 score: 0.9184 time: 0.09s
     INFO: Early stopping counter 16 of 20
Epoch 47/1000, LR 0.000269
Train loss: 0.0717;  Loss pred: 0.0421; Loss self: 2.9595; time: 0.25s
Val loss: 0.5045 score: 0.8367 time: 0.07s
Test loss: 0.1244 score: 0.9184 time: 0.17s
     INFO: Early stopping counter 17 of 20
Epoch 48/1000, LR 0.000269
Train loss: 0.0690;  Loss pred: 0.0394; Loss self: 2.9605; time: 0.23s
Val loss: 0.5100 score: 0.8367 time: 0.07s
Test loss: 0.1227 score: 0.9184 time: 0.16s
     INFO: Early stopping counter 18 of 20
Epoch 49/1000, LR 0.000269
Train loss: 0.0666;  Loss pred: 0.0370; Loss self: 2.9613; time: 0.31s
Val loss: 0.5156 score: 0.8367 time: 0.11s
Test loss: 0.1215 score: 0.9184 time: 0.18s
     INFO: Early stopping counter 19 of 20
Epoch 50/1000, LR 0.000269
Train loss: 0.0644;  Loss pred: 0.0348; Loss self: 2.9618; time: 0.26s
Val loss: 0.5213 score: 0.8367 time: 0.07s
Test loss: 0.1205 score: 0.9184 time: 0.13s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 029,   Train_Loss: 0.1802,   Val_Loss: 0.4296,   Val_Precision: 1.0000,   Val_Recall: 0.6800,   Val_accuracy: 0.8095,   Val_Score: 0.8367,   Val_Loss: 0.4296,   Test_Precision: 0.9524,   Test_Recall: 0.8333,   Test_accuracy: 0.8889,   Test_Score: 0.8980,   Test_loss: 0.2172


[0.0822108689462766, 0.1919707260094583, 0.0775856050895527, 0.1258895000210032, 0.12200248008593917, 0.21538004698231816, 0.08034527895506471, 0.08553361601661891, 0.07836883806157857, 0.07981945900246501, 0.08099335501901805, 0.08076202403753996, 0.0733612289186567, 0.0806000700686127, 0.16822781995870173, 0.07689131493680179, 0.07497978897299618, 0.08618391107302159, 0.09108580090105534, 0.08469932596199214, 0.10165938502177596, 0.078079893020913, 0.12570375797804445, 0.07913009892217815, 0.08013161597773433, 0.08555564202833921, 0.07850829197559506, 0.07548691798001528, 0.08097247208934277, 0.08530214393977076, 0.07492151402402669, 0.09377067105378956, 0.07668030902277678, 0.10190235497429967, 0.08052870607934892, 0.08103544800542295, 0.08276152506005019, 0.15497123601380736, 0.07677015895023942, 0.12851901398971677, 0.0747830830514431, 0.1000933040631935, 0.08265943999867886, 0.12380159390158951, 0.08043315401300788, 0.08493194996844977, 0.08911309402901679, 0.07960248808376491, 0.08641995792277157, 0.0843153630848974, 0.08684581296984106, 0.07407773309387267, 0.11997971497476101, 0.09011715499218553, 0.10347434994764626, 0.09906219702679664, 0.08371546200942248, 0.07985387404914945, 0.07860535790678114, 0.2425458850339055, 0.09649909404106438, 0.07393329998012632, 0.07155625894665718, 0.09095546999014914, 0.1622127729933709, 0.07457207702100277, 0.08314530702773482, 0.11273038503713906, 0.1407508369302377, 0.10662928503006697, 0.11870143900159746, 0.11821022199001163, 0.10714370792265981, 0.06868412194307894, 0.07189519796520472, 0.10658553300891072, 0.09153839107602835, 0.09735885704867542, 0.09385686507448554, 0.08907418698072433, 0.0867875040275976, 0.08527431497350335, 0.10364075202960521, 0.08169588691089302, 0.081921297009103, 0.0849083960056305, 0.09276940498966724, 0.0903816829668358, 0.10503086994867772, 0.08596796507481486, 0.10824610700365156, 0.3505287910811603, 0.10825031099375337, 0.13087714894209057, 0.14640033605974168, 0.16066691395826638, 0.09485826303716749, 0.23843410389963537, 0.21465766394976526, 0.149493922945112, 0.13354548905044794, 0.1365576219977811, 0.12851891899481416, 0.0947347660548985, 0.1363017539260909, 0.17539754195604473, 0.14385555894114077, 0.09066421608440578, 0.08384079998359084, 0.08596805296838284, 0.08480218600016087, 0.08404065307695419, 0.08315655100159347, 0.08862584095913917, 0.08863084204494953, 0.09634333394933492, 0.08237829699646682, 0.09682628698647022, 0.17034978896845132, 0.09977834194432944, 0.09859608393162489, 0.17043319600634277, 0.16811957606114447, 0.18877434090245515, 0.1303605770226568]
[0.001677772835638298, 0.003917769918560373, 0.001583379695705157, 0.0025691734698163916, 0.0024898465323661056, 0.0043955111629044525, 0.0016396995705115248, 0.0017455840003391613, 0.0015993640420730322, 0.0016289685510707144, 0.0016529256126330215, 0.001648204572194693, 0.001497167937115443, 0.0016448993891553612, 0.0034332208154837086, 0.0015692105089143223, 0.0015301997749591057, 0.0017588553280208487, 0.0018588938959399049, 0.0017285576726937173, 0.0020746813269750196, 0.0015934672045084287, 0.0025653828158784583, 0.0016148999780036357, 0.0016353391015864148, 0.0017460335107824328, 0.0016022100403182665, 0.001540549346530924, 0.0016524994303947504, 0.0017408600804034848, 0.001529010490286259, 0.001913687164363052, 0.0015649042657709547, 0.002079639897434687, 0.0016434429812112025, 0.001653784653171897, 0.0016890107155112283, 0.0031626782859960683, 0.001566737937759988, 0.002622837020198301, 0.0015261853683967979, 0.0020427204910855815, 0.0016869273469118135, 0.0025265631408487657, 0.0016414929390409772, 0.0017333051013969341, 0.0018186345720207508, 0.0016245405731380594, 0.0017636726106688076, 0.001720721695610151, 0.0017723635299967564, 0.0015117904713035238, 0.0024485656117298163, 0.001839125612085419, 0.002111721427502985, 0.0020216774903427884, 0.0017084788165188261, 0.0016296708989622338, 0.001604190977689411, 0.0049499160211001125, 0.0019693692661441713, 0.0015088428567372719, 0.0014603318152379018, 0.0018562340814316151, 0.0033104647549667527, 0.0015218791228776075, 0.0016968430005660166, 0.0023006201027987562, 0.0028724660598007695, 0.0021761078577564688, 0.002422478346971377, 0.0024124535100002375, 0.0021866062841359147, 0.0014017167743485496, 0.0014672489380654023, 0.002175214959365525, 0.0018681304301230274, 0.001986915449972968, 0.001915446226009909, 0.0018178405506270273, 0.0017711735515836247, 0.0017402921423163948, 0.00211511738835929, 0.0016672629981814902, 0.001671863204267408, 0.0017328244082781734, 0.0018932531630544334, 0.0018445241421803224, 0.002143487141809749, 0.0017544482668329562, 0.0022091042245643176, 0.0071536487975747, 0.002209190020280681, 0.002670962223307971, 0.002987761960402891, 0.0032789166113931915, 0.001935882919125867, 0.004866002120400722, 0.004380768652036026, 0.0030508963866349385, 0.0027254181438866928, 0.0027868902448526757, 0.0026228350815268196, 0.0019333625725489489, 0.002781668447471243, 0.0035795416725723415, 0.0029358277334926687, 0.0018502901241715466, 0.0017110367343589968, 0.0017544500605792416, 0.0017306568571461402, 0.0017151153689174323, 0.001697072469420275, 0.0018086906318191666, 0.0018087926947948883, 0.001966190488761937, 0.0016811897346217717, 0.0019760466731932698, 0.0034765263054785983, 0.002036292692741417, 0.0020121649781964265, 0.0034782284899253627, 0.003431011756349887, 0.00385253756943786, 0.002660419939237894]
[596.0282457544715, 255.24725054999163, 631.5604543322445, 389.23023756409333, 401.63117967343084, 227.50482547727694, 609.8678184614255, 572.8741783871204, 625.2485198453254, 613.8853935164703, 604.987902877888, 606.7208020594398, 667.9277422455865, 607.9399181450785, 291.27168153298913, 637.2631296561112, 653.5094412928696, 568.5515937943853, 537.9543190626134, 578.5170005011397, 482.0017353981037, 627.5623352464864, 389.8053708828529, 619.2333974988442, 611.49397029027, 572.7266938604631, 624.137893806581, 649.1190965429595, 605.1439302227893, 574.4287040967857, 654.0177496184356, 522.5514486495697, 639.0167257339202, 480.85247894769526, 608.4786703479114, 604.6736484596332, 592.0625552084321, 316.1877085089151, 638.2688360950331, 381.26654164900964, 655.2284019407574, 489.5432362694716, 592.7937571411464, 395.7945811178355, 609.2015239396876, 576.9324738005232, 549.8630760597846, 615.5586487251226, 566.9986560718811, 581.151503204247, 564.218335051066, 661.4673256524508, 408.40237043659977, 543.7366504107792, 473.5473093070116, 494.6387367801397, 585.3160076269407, 613.6208240797544, 623.3671762948981, 202.0236294388185, 507.7767878229868, 662.7595415485508, 684.7758773488686, 538.7251586441905, 302.0723898358021, 657.0824088244109, 589.3297138665331, 434.6654185901781, 348.1329210446297, 459.5360457137375, 412.8003873595885, 414.515759932676, 457.3297018558473, 713.4108817844117, 681.5476052199571, 459.7246794825666, 535.2945296941307, 503.2926791190864, 522.0715603606963, 550.103252815584, 564.5974100651456, 574.6161668402192, 472.78699778252326, 599.7853974392256, 598.1350611984961, 577.092517408416, 528.1913795335615, 542.145248810866, 466.5295072195761, 569.9797588247791, 452.6721686013802, 139.7888026511771, 452.6545887043924, 374.3969088269268, 334.69868525441456, 304.9787837010915, 516.5601649357711, 205.50751422969964, 228.27044279894477, 327.7725210140534, 366.9161747686575, 358.82288577635154, 381.26682346260003, 517.233556808539, 359.49647446627915, 279.36537452890633, 340.61944050454525, 540.455784169384, 584.4409882728955, 569.9791760785967, 577.8152935810764, 583.0511568625219, 589.2500279269765, 552.886150017932, 552.8549528520718, 508.59772016783376, 594.8168605877055, 506.06092131620096, 287.64344409651585, 491.08853730340775, 496.97714195201576, 287.50267640452176, 291.4592169931411, 259.56917537495013, 375.88050865626155]
Elapsed: 0.10656910996697844~0.04185182237604071
Time per graph: 0.002174879795244458~0.0008541188240008307
Speed: 505.5545051102284~126.90162279322598
Total Time: 0.1309
best val loss: 0.4296192526817322 test_score: 0.8980

Testing...
Test loss: 0.2294 score: 0.8980 time: 0.11s
test Score 0.8980
Epoch Time List: [0.39578646095469594, 0.5258555840700865, 0.4026856740238145, 0.4278206310700625, 0.467819846002385, 0.6515395969618112, 0.4175676649902016, 0.5503474089782685, 0.4265239519299939, 0.4028221048647538, 0.42095670895650983, 0.4266176460077986, 0.4204322400037199, 0.4044971000403166, 0.5581564218737185, 0.5026286209467798, 0.4859235550975427, 0.40031100797932595, 0.4655602319398895, 0.42038007196970284, 0.42827417713124305, 0.41743412415962666, 0.4425220830598846, 0.43604431697167456, 0.4070489809382707, 0.4130787869216874, 0.3940317799570039, 0.38615717797074467, 0.4308566309046, 0.4550861819880083, 0.41537020809482783, 0.47363303496968, 0.4004715859191492, 0.4427669399883598, 0.40427433303557336, 0.4103048350661993, 0.4119646449107677, 0.4574936638819054, 0.3814487080089748, 0.5115188440540805, 0.3700429010204971, 0.4096368111204356, 0.384090143837966, 0.4504503790521994, 0.4439002750441432, 0.41260244185104966, 0.4022644329816103, 0.430324112996459, 0.4194762709084898, 0.419574867002666, 0.4150240420131013, 0.42678257497027516, 0.4402795350179076, 0.4074930108617991, 0.49695852992590517, 0.688481007120572, 0.46313751698471606, 0.40912802203092724, 0.4011010261019692, 0.6116448498796672, 0.6473373001208529, 0.3841582011664286, 0.37279291392769665, 0.6227723601041362, 0.5853807928506285, 0.5209252788918093, 0.4153331269044429, 0.4494899120181799, 0.6537859330419451, 0.4434493030421436, 0.4465473419986665, 0.5683755681384355, 0.576937458012253, 0.43185365106910467, 0.37866739102173597, 0.4836575170047581, 0.403089300962165, 0.40939860395155847, 0.4365708150435239, 0.39821603207383305, 0.39493073406629264, 0.3760034618899226, 0.3953483069781214, 0.37555093003902584, 0.38050285808276385, 0.3771841659909114, 0.4196103358408436, 0.4034120700089261, 0.4185364911099896, 0.3849706038599834, 0.46540345111861825, 0.7235517560038716, 0.6148283157963306, 0.4895388700533658, 0.45728985604364425, 1.3246665600454435, 0.472923799068667, 0.7390244289999828, 0.6362323670182377, 0.5979024640982971, 0.8734237570315599, 0.45984042703639716, 0.5328644749242812, 0.4973394601838663, 0.5948441621148959, 0.7480770549736917, 0.5038687380729243, 0.4362689880654216, 0.380736356950365, 0.3801019919337705, 0.36680141591932625, 0.36774810194037855, 0.3868342989590019, 0.3947586138965562, 0.3858454469591379, 0.5944187319837511, 0.4499450210714713, 0.3869304789695889, 0.4718858830165118, 0.43928686098661274, 0.4141338460613042, 0.4792900369502604, 0.45949319610372186, 0.5996867481153458, 0.4536856801714748]
Total Epoch List: [75, 50]
Total Time List: [0.07230219605844468, 0.13093393703456968]
========================k_idx:2========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
emb dim:  64
Model: Ethident
Ethident(
  (encoder): HGATE_encoder(
    (lin1): Linear(in_features=14887, out_features=64, bias=True)
    (account_convs): ModuleList(
      (0): GATEConv()
      (1): GATConv(64, 64, heads=1)
    )
    (subgraph_conv): GATConv(64, 64, heads=1)
    (lin2): Linear(in_features=64, out_features=64, bias=True)
    (lin3): Linear(in_features=64, out_features=2, bias=True)
  )
  (proj_head_g1): Project_Head(
    (block): Sequential(
      (0): Linear(in_features=64, out_features=64, bias=True)
      (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=64, bias=True)
      (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
      (5): ReLU()
    )
    (linear_shortcut): Linear(in_features=64, out_features=64, bias=True)
  )
  (proj_head_g2): Project_Head(
    (block): Sequential(
      (0): Linear(in_features=64, out_features=64, bias=True)
      (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=64, bias=True)
      (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
      (5): ReLU()
    )
    (linear_shortcut): Linear(in_features=64, out_features=64, bias=True)
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=64, bias=True)
    (1): ReLU()
    (2): Dropout(p=0.0, inplace=True)
    (3): Linear(in_features=64, out_features=2, bias=True)
  )
)
Total number of parameters: 1003972
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7a49927ee1d0>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.8978;  Loss pred: 0.8608; Loss self: 3.6930; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.8834 score: 0.4898 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.8915 score: 0.5000 time: 0.08s
Epoch 2/1000, LR 0.000000
Train loss: 0.8978;  Loss pred: 0.8608; Loss self: 3.6930; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.8079 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.8119 score: 0.5000 time: 0.07s
Epoch 3/1000, LR 0.000030
Train loss: 0.8226;  Loss pred: 0.7859; Loss self: 3.6651; time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6977 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6855 score: 0.5000 time: 0.07s
Epoch 4/1000, LR 0.000060
Train loss: 0.6944;  Loss pred: 0.6585; Loss self: 3.5903; time: 0.23s
Val loss: 0.6228 score: 0.6939 time: 0.08s
Test loss: 0.6059 score: 0.7083 time: 0.12s
Epoch 5/1000, LR 0.000090
Train loss: 0.6129;  Loss pred: 0.5785; Loss self: 3.4361; time: 0.23s
Val loss: 0.5993 score: 0.6327 time: 0.07s
Test loss: 0.5769 score: 0.7292 time: 0.08s
Epoch 6/1000, LR 0.000120
Train loss: 0.5874;  Loss pred: 0.5547; Loss self: 3.2751; time: 0.32s
Val loss: 0.6408 score: 0.6327 time: 0.10s
Test loss: 0.6135 score: 0.6875 time: 0.10s
     INFO: Early stopping counter 1 of 20
Epoch 7/1000, LR 0.000150
Train loss: 0.6342;  Loss pred: 0.6020; Loss self: 3.2186; time: 0.29s
Val loss: 0.6678 score: 0.6531 time: 0.11s
Test loss: 0.6383 score: 0.6458 time: 0.12s
     INFO: Early stopping counter 2 of 20
Epoch 8/1000, LR 0.000180
Train loss: 0.6645;  Loss pred: 0.6321; Loss self: 3.2382; time: 0.26s
Val loss: 0.6664 score: 0.6327 time: 0.12s
Test loss: 0.6395 score: 0.6458 time: 0.08s
     INFO: Early stopping counter 3 of 20
Epoch 9/1000, LR 0.000210
Train loss: 0.6617;  Loss pred: 0.6289; Loss self: 3.2785; time: 0.24s
Val loss: 0.6499 score: 0.6531 time: 0.08s
Test loss: 0.6224 score: 0.6458 time: 0.08s
     INFO: Early stopping counter 4 of 20
Epoch 10/1000, LR 0.000240
Train loss: 0.6393;  Loss pred: 0.6061; Loss self: 3.3266; time: 0.24s
Val loss: 0.6272 score: 0.6531 time: 0.08s
Test loss: 0.6003 score: 0.6667 time: 0.07s
     INFO: Early stopping counter 5 of 20
Epoch 11/1000, LR 0.000270
Train loss: 0.6146;  Loss pred: 0.5808; Loss self: 3.3786; time: 0.23s
Val loss: 0.6052 score: 0.6531 time: 0.11s
Test loss: 0.5797 score: 0.7083 time: 0.14s
     INFO: Early stopping counter 6 of 20
Epoch 12/1000, LR 0.000270
Train loss: 0.5916;  Loss pred: 0.5573; Loss self: 3.4287; time: 0.22s
Val loss: 0.5884 score: 0.6939 time: 0.07s
Test loss: 0.5644 score: 0.7292 time: 0.07s
Epoch 13/1000, LR 0.000270
Train loss: 0.5727;  Loss pred: 0.5379; Loss self: 3.4722; time: 0.24s
Val loss: 0.5828 score: 0.7755 time: 0.07s
Test loss: 0.5556 score: 0.8125 time: 0.14s
Epoch 14/1000, LR 0.000270
Train loss: 0.5577;  Loss pred: 0.5226; Loss self: 3.5070; time: 0.27s
Val loss: 0.5813 score: 0.7959 time: 0.07s
Test loss: 0.5484 score: 0.8333 time: 0.08s
Epoch 15/1000, LR 0.000270
Train loss: 0.5469;  Loss pred: 0.5117; Loss self: 3.5205; time: 0.24s
Val loss: 0.5753 score: 0.7755 time: 0.08s
Test loss: 0.5364 score: 0.8125 time: 0.13s
Epoch 16/1000, LR 0.000270
Train loss: 0.5298;  Loss pred: 0.4947; Loss self: 3.5119; time: 0.28s
Val loss: 0.5648 score: 0.7959 time: 0.16s
Test loss: 0.5197 score: 0.8125 time: 0.12s
Epoch 17/1000, LR 0.000270
Train loss: 0.5073;  Loss pred: 0.4724; Loss self: 3.4888; time: 0.29s
Val loss: 0.5503 score: 0.8163 time: 0.08s
Test loss: 0.5010 score: 0.8542 time: 0.09s
Epoch 18/1000, LR 0.000270
Train loss: 0.4834;  Loss pred: 0.4488; Loss self: 3.4598; time: 0.23s
Val loss: 0.5364 score: 0.7959 time: 0.08s
Test loss: 0.4855 score: 0.8333 time: 0.08s
Epoch 19/1000, LR 0.000270
Train loss: 0.4646;  Loss pred: 0.4303; Loss self: 3.4310; time: 0.24s
Val loss: 0.5257 score: 0.8163 time: 0.08s
Test loss: 0.4739 score: 0.8542 time: 0.12s
Epoch 20/1000, LR 0.000270
Train loss: 0.4499;  Loss pred: 0.4159; Loss self: 3.4066; time: 0.23s
Val loss: 0.5151 score: 0.7959 time: 0.11s
Test loss: 0.4609 score: 0.8542 time: 0.11s
Epoch 21/1000, LR 0.000270
Train loss: 0.4328;  Loss pred: 0.3989; Loss self: 3.3843; time: 0.23s
Val loss: 0.5052 score: 0.7959 time: 0.07s
Test loss: 0.4463 score: 0.8542 time: 0.07s
Epoch 22/1000, LR 0.000270
Train loss: 0.4120;  Loss pred: 0.3784; Loss self: 3.3620; time: 0.33s
Val loss: 0.4975 score: 0.8163 time: 0.17s
Test loss: 0.4314 score: 0.8333 time: 0.14s
Epoch 23/1000, LR 0.000270
Train loss: 0.3890;  Loss pred: 0.3556; Loss self: 3.3397; time: 0.27s
Val loss: 0.4924 score: 0.8367 time: 0.08s
Test loss: 0.4201 score: 0.8542 time: 0.08s
Epoch 24/1000, LR 0.000270
Train loss: 0.3684;  Loss pred: 0.3352; Loss self: 3.3175; time: 0.24s
Val loss: 0.4869 score: 0.8367 time: 0.07s
Test loss: 0.4080 score: 0.8542 time: 0.08s
Epoch 25/1000, LR 0.000270
Train loss: 0.3479;  Loss pred: 0.3150; Loss self: 3.2921; time: 0.24s
Val loss: 0.4773 score: 0.8367 time: 0.13s
Test loss: 0.3931 score: 0.8542 time: 0.08s
Epoch 26/1000, LR 0.000270
Train loss: 0.3262;  Loss pred: 0.2936; Loss self: 3.2624; time: 0.26s
Val loss: 0.4646 score: 0.8367 time: 0.08s
Test loss: 0.3765 score: 0.8750 time: 0.08s
Epoch 27/1000, LR 0.000270
Train loss: 0.3050;  Loss pred: 0.2727; Loss self: 3.2307; time: 0.25s
Val loss: 0.4536 score: 0.8367 time: 0.17s
Test loss: 0.3621 score: 0.8750 time: 0.19s
Epoch 28/1000, LR 0.000270
Train loss: 0.2866;  Loss pred: 0.2545; Loss self: 3.2012; time: 0.24s
Val loss: 0.4443 score: 0.8163 time: 0.07s
Test loss: 0.3495 score: 0.8750 time: 0.07s
Epoch 29/1000, LR 0.000270
Train loss: 0.2693;  Loss pred: 0.2375; Loss self: 3.1734; time: 0.23s
Val loss: 0.4376 score: 0.8367 time: 0.10s
Test loss: 0.3377 score: 0.8958 time: 0.16s
Epoch 30/1000, LR 0.000270
Train loss: 0.2510;  Loss pred: 0.2195; Loss self: 3.1487; time: 0.23s
Val loss: 0.4351 score: 0.8367 time: 0.07s
Test loss: 0.3265 score: 0.8958 time: 0.07s
Epoch 31/1000, LR 0.000270
Train loss: 0.2327;  Loss pred: 0.2014; Loss self: 3.1263; time: 0.24s
Val loss: 0.4361 score: 0.8367 time: 0.07s
Test loss: 0.3179 score: 0.8750 time: 0.07s
     INFO: Early stopping counter 1 of 20
Epoch 32/1000, LR 0.000270
Train loss: 0.2155;  Loss pred: 0.1845; Loss self: 3.1067; time: 0.23s
Val loss: 0.4400 score: 0.8367 time: 0.07s
Test loss: 0.3109 score: 0.8958 time: 0.08s
     INFO: Early stopping counter 2 of 20
Epoch 33/1000, LR 0.000270
Train loss: 0.1999;  Loss pred: 0.1691; Loss self: 3.0887; time: 0.25s
Val loss: 0.4429 score: 0.8367 time: 0.08s
Test loss: 0.3046 score: 0.8958 time: 0.11s
     INFO: Early stopping counter 3 of 20
Epoch 34/1000, LR 0.000270
Train loss: 0.1854;  Loss pred: 0.1546; Loss self: 3.0726; time: 0.33s
Val loss: 0.4435 score: 0.8367 time: 0.07s
Test loss: 0.2971 score: 0.8958 time: 0.08s
     INFO: Early stopping counter 4 of 20
Epoch 35/1000, LR 0.000270
Train loss: 0.1712;  Loss pred: 0.1406; Loss self: 3.0595; time: 0.25s
Val loss: 0.4389 score: 0.8367 time: 0.09s
Test loss: 0.2877 score: 0.8958 time: 0.12s
     INFO: Early stopping counter 5 of 20
Epoch 36/1000, LR 0.000270
Train loss: 0.1579;  Loss pred: 0.1274; Loss self: 3.0480; time: 0.32s
Val loss: 0.4307 score: 0.8367 time: 0.15s
Test loss: 0.2773 score: 0.8958 time: 0.09s
Epoch 37/1000, LR 0.000270
Train loss: 0.1459;  Loss pred: 0.1155; Loss self: 3.0385; time: 0.26s
Val loss: 0.4216 score: 0.8367 time: 0.11s
Test loss: 0.2672 score: 0.8958 time: 0.08s
Epoch 38/1000, LR 0.000270
Train loss: 0.1358;  Loss pred: 0.1054; Loss self: 3.0313; time: 0.24s
Val loss: 0.4148 score: 0.8367 time: 0.08s
Test loss: 0.2583 score: 0.9167 time: 0.09s
Epoch 39/1000, LR 0.000269
Train loss: 0.1268;  Loss pred: 0.0965; Loss self: 3.0253; time: 0.24s
Val loss: 0.4112 score: 0.8571 time: 0.13s
Test loss: 0.2511 score: 0.9167 time: 0.11s
Epoch 40/1000, LR 0.000269
Train loss: 0.1187;  Loss pred: 0.0885; Loss self: 3.0201; time: 0.26s
Val loss: 0.4123 score: 0.8571 time: 0.11s
Test loss: 0.2466 score: 0.9167 time: 0.12s
     INFO: Early stopping counter 1 of 20
Epoch 41/1000, LR 0.000269
Train loss: 0.1109;  Loss pred: 0.0807; Loss self: 3.0152; time: 0.39s
Val loss: 0.4177 score: 0.8571 time: 0.16s
Test loss: 0.2452 score: 0.9167 time: 0.09s
     INFO: Early stopping counter 2 of 20
Epoch 42/1000, LR 0.000269
Train loss: 0.1038;  Loss pred: 0.0737; Loss self: 3.0117; time: 0.30s
Val loss: 0.4266 score: 0.8571 time: 0.09s
Test loss: 0.2469 score: 0.9375 time: 0.07s
     INFO: Early stopping counter 3 of 20
Epoch 43/1000, LR 0.000269
Train loss: 0.0975;  Loss pred: 0.0674; Loss self: 3.0096; time: 0.24s
Val loss: 0.4381 score: 0.8571 time: 0.09s
Test loss: 0.2514 score: 0.9375 time: 0.08s
     INFO: Early stopping counter 4 of 20
Epoch 44/1000, LR 0.000269
Train loss: 0.0922;  Loss pred: 0.0621; Loss self: 3.0084; time: 0.25s
Val loss: 0.4511 score: 0.8571 time: 0.13s
Test loss: 0.2572 score: 0.9375 time: 0.08s
     INFO: Early stopping counter 5 of 20
Epoch 45/1000, LR 0.000269
Train loss: 0.0877;  Loss pred: 0.0576; Loss self: 3.0082; time: 0.25s
Val loss: 0.4643 score: 0.8571 time: 0.07s
Test loss: 0.2636 score: 0.8958 time: 0.07s
     INFO: Early stopping counter 6 of 20
Epoch 46/1000, LR 0.000269
Train loss: 0.0838;  Loss pred: 0.0537; Loss self: 3.0090; time: 0.23s
Val loss: 0.4766 score: 0.8571 time: 0.08s
Test loss: 0.2702 score: 0.8958 time: 0.08s
     INFO: Early stopping counter 7 of 20
Epoch 47/1000, LR 0.000269
Train loss: 0.0803;  Loss pred: 0.0502; Loss self: 3.0104; time: 0.36s
Val loss: 0.4881 score: 0.8367 time: 0.08s
Test loss: 0.2760 score: 0.8958 time: 0.09s
     INFO: Early stopping counter 8 of 20
Epoch 48/1000, LR 0.000269
Train loss: 0.0771;  Loss pred: 0.0470; Loss self: 3.0127; time: 0.31s
Val loss: 0.4975 score: 0.8367 time: 0.15s
Test loss: 0.2806 score: 0.8958 time: 0.07s
     INFO: Early stopping counter 9 of 20
Epoch 49/1000, LR 0.000269
Train loss: 0.0742;  Loss pred: 0.0440; Loss self: 3.0153; time: 0.26s
Val loss: 0.5050 score: 0.8367 time: 0.08s
Test loss: 0.2832 score: 0.8958 time: 0.08s
     INFO: Early stopping counter 10 of 20
Epoch 50/1000, LR 0.000269
Train loss: 0.0714;  Loss pred: 0.0413; Loss self: 3.0180; time: 0.24s
Val loss: 0.5101 score: 0.8367 time: 0.08s
Test loss: 0.2846 score: 0.8958 time: 0.08s
     INFO: Early stopping counter 11 of 20
Epoch 51/1000, LR 0.000269
Train loss: 0.0689;  Loss pred: 0.0387; Loss self: 3.0208; time: 0.30s
Val loss: 0.5134 score: 0.8367 time: 0.09s
Test loss: 0.2851 score: 0.8958 time: 0.11s
     INFO: Early stopping counter 12 of 20
Epoch 52/1000, LR 0.000269
Train loss: 0.0666;  Loss pred: 0.0364; Loss self: 3.0235; time: 0.24s
Val loss: 0.5157 score: 0.8367 time: 0.08s
Test loss: 0.2850 score: 0.8958 time: 0.08s
     INFO: Early stopping counter 13 of 20
Epoch 53/1000, LR 0.000269
Train loss: 0.0645;  Loss pred: 0.0342; Loss self: 3.0261; time: 0.23s
Val loss: 0.5176 score: 0.8367 time: 0.16s
Test loss: 0.2846 score: 0.8958 time: 0.07s
     INFO: Early stopping counter 14 of 20
Epoch 54/1000, LR 0.000269
Train loss: 0.0625;  Loss pred: 0.0322; Loss self: 3.0286; time: 0.23s
Val loss: 0.5199 score: 0.8367 time: 0.10s
Test loss: 0.2849 score: 0.8958 time: 0.07s
     INFO: Early stopping counter 15 of 20
Epoch 55/1000, LR 0.000269
Train loss: 0.0608;  Loss pred: 0.0304; Loss self: 3.0308; time: 0.23s
Val loss: 0.5227 score: 0.8367 time: 0.07s
Test loss: 0.2857 score: 0.8958 time: 0.07s
     INFO: Early stopping counter 16 of 20
Epoch 56/1000, LR 0.000269
Train loss: 0.0591;  Loss pred: 0.0288; Loss self: 3.0328; time: 0.25s
Val loss: 0.5259 score: 0.8367 time: 0.07s
Test loss: 0.2871 score: 0.8958 time: 0.07s
     INFO: Early stopping counter 17 of 20
Epoch 57/1000, LR 0.000269
Train loss: 0.0576;  Loss pred: 0.0273; Loss self: 3.0346; time: 0.25s
Val loss: 0.5297 score: 0.8367 time: 0.09s
Test loss: 0.2890 score: 0.8958 time: 0.08s
     INFO: Early stopping counter 18 of 20
Epoch 58/1000, LR 0.000269
Train loss: 0.0562;  Loss pred: 0.0259; Loss self: 3.0363; time: 0.25s
Val loss: 0.5339 score: 0.8367 time: 0.07s
Test loss: 0.2911 score: 0.8958 time: 0.11s
     INFO: Early stopping counter 19 of 20
Epoch 59/1000, LR 0.000268
Train loss: 0.0550;  Loss pred: 0.0246; Loss self: 3.0378; time: 0.26s
Val loss: 0.5387 score: 0.8367 time: 0.08s
Test loss: 0.2937 score: 0.8958 time: 0.09s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 038,   Train_Loss: 0.1268,   Val_Loss: 0.4112,   Val_Precision: 1.0000,   Val_Recall: 0.7200,   Val_accuracy: 0.8372,   Val_Score: 0.8571,   Val_Loss: 0.4112,   Test_Precision: 1.0000,   Test_Recall: 0.8333,   Test_accuracy: 0.9091,   Test_Score: 0.9167,   Test_loss: 0.2511


[0.0822108689462766, 0.1919707260094583, 0.0775856050895527, 0.1258895000210032, 0.12200248008593917, 0.21538004698231816, 0.08034527895506471, 0.08553361601661891, 0.07836883806157857, 0.07981945900246501, 0.08099335501901805, 0.08076202403753996, 0.0733612289186567, 0.0806000700686127, 0.16822781995870173, 0.07689131493680179, 0.07497978897299618, 0.08618391107302159, 0.09108580090105534, 0.08469932596199214, 0.10165938502177596, 0.078079893020913, 0.12570375797804445, 0.07913009892217815, 0.08013161597773433, 0.08555564202833921, 0.07850829197559506, 0.07548691798001528, 0.08097247208934277, 0.08530214393977076, 0.07492151402402669, 0.09377067105378956, 0.07668030902277678, 0.10190235497429967, 0.08052870607934892, 0.08103544800542295, 0.08276152506005019, 0.15497123601380736, 0.07677015895023942, 0.12851901398971677, 0.0747830830514431, 0.1000933040631935, 0.08265943999867886, 0.12380159390158951, 0.08043315401300788, 0.08493194996844977, 0.08911309402901679, 0.07960248808376491, 0.08641995792277157, 0.0843153630848974, 0.08684581296984106, 0.07407773309387267, 0.11997971497476101, 0.09011715499218553, 0.10347434994764626, 0.09906219702679664, 0.08371546200942248, 0.07985387404914945, 0.07860535790678114, 0.2425458850339055, 0.09649909404106438, 0.07393329998012632, 0.07155625894665718, 0.09095546999014914, 0.1622127729933709, 0.07457207702100277, 0.08314530702773482, 0.11273038503713906, 0.1407508369302377, 0.10662928503006697, 0.11870143900159746, 0.11821022199001163, 0.10714370792265981, 0.06868412194307894, 0.07189519796520472, 0.10658553300891072, 0.09153839107602835, 0.09735885704867542, 0.09385686507448554, 0.08907418698072433, 0.0867875040275976, 0.08527431497350335, 0.10364075202960521, 0.08169588691089302, 0.081921297009103, 0.0849083960056305, 0.09276940498966724, 0.0903816829668358, 0.10503086994867772, 0.08596796507481486, 0.10824610700365156, 0.3505287910811603, 0.10825031099375337, 0.13087714894209057, 0.14640033605974168, 0.16066691395826638, 0.09485826303716749, 0.23843410389963537, 0.21465766394976526, 0.149493922945112, 0.13354548905044794, 0.1365576219977811, 0.12851891899481416, 0.0947347660548985, 0.1363017539260909, 0.17539754195604473, 0.14385555894114077, 0.09066421608440578, 0.08384079998359084, 0.08596805296838284, 0.08480218600016087, 0.08404065307695419, 0.08315655100159347, 0.08862584095913917, 0.08863084204494953, 0.09634333394933492, 0.08237829699646682, 0.09682628698647022, 0.17034978896845132, 0.09977834194432944, 0.09859608393162489, 0.17043319600634277, 0.16811957606114447, 0.18877434090245515, 0.1303605770226568, 0.0844785989029333, 0.08005169301759452, 0.07761330902576447, 0.12346391100436449, 0.07937691395636648, 0.10340658493805677, 0.12168489408213645, 0.08463289600331336, 0.08042062690947205, 0.07757937000133097, 0.14266671205405146, 0.07453603099565953, 0.14421290601603687, 0.08153468510136008, 0.13044143002480268, 0.12361093400977552, 0.09202370990533382, 0.08389631903264672, 0.12914494099095464, 0.11682213994208723, 0.07873679301701486, 0.14461946103256196, 0.08271316601894796, 0.08025635604280978, 0.08027858310379088, 0.08460094290785491, 0.19197668007109314, 0.07561112998519093, 0.16595610789954662, 0.07681445498019457, 0.07713540398981422, 0.08380519703496248, 0.11823903396725655, 0.08115709095727652, 0.12498167296871543, 0.0990186840062961, 0.08177821105346084, 0.09162899199873209, 0.11452706507407129, 0.1234195230063051, 0.09378800401464105, 0.0787178409518674, 0.08734465693123639, 0.08269363106228411, 0.07714610698167235, 0.0809755299706012, 0.09442938398569822, 0.07944499107543379, 0.08200828905683011, 0.08144665800500661, 0.11666900105774403, 0.07984241098165512, 0.07594746502581984, 0.0767751190578565, 0.07624331105034798, 0.07680629298556596, 0.08044622896704823, 0.11320147500373423, 0.09143431799020618]
[0.001677772835638298, 0.003917769918560373, 0.001583379695705157, 0.0025691734698163916, 0.0024898465323661056, 0.0043955111629044525, 0.0016396995705115248, 0.0017455840003391613, 0.0015993640420730322, 0.0016289685510707144, 0.0016529256126330215, 0.001648204572194693, 0.001497167937115443, 0.0016448993891553612, 0.0034332208154837086, 0.0015692105089143223, 0.0015301997749591057, 0.0017588553280208487, 0.0018588938959399049, 0.0017285576726937173, 0.0020746813269750196, 0.0015934672045084287, 0.0025653828158784583, 0.0016148999780036357, 0.0016353391015864148, 0.0017460335107824328, 0.0016022100403182665, 0.001540549346530924, 0.0016524994303947504, 0.0017408600804034848, 0.001529010490286259, 0.001913687164363052, 0.0015649042657709547, 0.002079639897434687, 0.0016434429812112025, 0.001653784653171897, 0.0016890107155112283, 0.0031626782859960683, 0.001566737937759988, 0.002622837020198301, 0.0015261853683967979, 0.0020427204910855815, 0.0016869273469118135, 0.0025265631408487657, 0.0016414929390409772, 0.0017333051013969341, 0.0018186345720207508, 0.0016245405731380594, 0.0017636726106688076, 0.001720721695610151, 0.0017723635299967564, 0.0015117904713035238, 0.0024485656117298163, 0.001839125612085419, 0.002111721427502985, 0.0020216774903427884, 0.0017084788165188261, 0.0016296708989622338, 0.001604190977689411, 0.0049499160211001125, 0.0019693692661441713, 0.0015088428567372719, 0.0014603318152379018, 0.0018562340814316151, 0.0033104647549667527, 0.0015218791228776075, 0.0016968430005660166, 0.0023006201027987562, 0.0028724660598007695, 0.0021761078577564688, 0.002422478346971377, 0.0024124535100002375, 0.0021866062841359147, 0.0014017167743485496, 0.0014672489380654023, 0.002175214959365525, 0.0018681304301230274, 0.001986915449972968, 0.001915446226009909, 0.0018178405506270273, 0.0017711735515836247, 0.0017402921423163948, 0.00211511738835929, 0.0016672629981814902, 0.001671863204267408, 0.0017328244082781734, 0.0018932531630544334, 0.0018445241421803224, 0.002143487141809749, 0.0017544482668329562, 0.0022091042245643176, 0.0071536487975747, 0.002209190020280681, 0.002670962223307971, 0.002987761960402891, 0.0032789166113931915, 0.001935882919125867, 0.004866002120400722, 0.004380768652036026, 0.0030508963866349385, 0.0027254181438866928, 0.0027868902448526757, 0.0026228350815268196, 0.0019333625725489489, 0.002781668447471243, 0.0035795416725723415, 0.0029358277334926687, 0.0018502901241715466, 0.0017110367343589968, 0.0017544500605792416, 0.0017306568571461402, 0.0017151153689174323, 0.001697072469420275, 0.0018086906318191666, 0.0018087926947948883, 0.001966190488761937, 0.0016811897346217717, 0.0019760466731932698, 0.0034765263054785983, 0.002036292692741417, 0.0020121649781964265, 0.0034782284899253627, 0.003431011756349887, 0.00385253756943786, 0.002660419939237894, 0.0017599708104777771, 0.001667743604533219, 0.0016169439380367596, 0.002572164812590927, 0.0016536857074243017, 0.0021543038528761826, 0.0025351019600445093, 0.0017631853334023617, 0.0016754297272806677, 0.0016162368750277285, 0.002972223167792739, 0.0015528339790762402, 0.0030044355420007682, 0.0016986392729450017, 0.0027175297921833894, 0.0025752277918703235, 0.0019171606230277878, 0.0017478399798468065, 0.0026905196039782218, 0.002433794582126817, 0.001640349854521143, 0.003012905438178374, 0.0017231909587280825, 0.001672007417558537, 0.0016724704813289766, 0.0017625196439136441, 0.0039995141681477735, 0.0015752318746914777, 0.003457418914573888, 0.00160030114542072, 0.0016069875831211295, 0.0017459416048950516, 0.002463313207651178, 0.0016907727282765943, 0.0026037848535149046, 0.002062889250131169, 0.001703712730280434, 0.0019089373333069186, 0.002385980522376485, 0.002571240062631356, 0.0019539167503050217, 0.0016399550198305708, 0.0018196803527340915, 0.0017227839804642524, 0.0016072105621181738, 0.0016869902077208583, 0.0019672788330353796, 0.001655103980738204, 0.0017085060220172938, 0.0016968053751043044, 0.0024306041887030005, 0.001663383562117815, 0.00158223885470458, 0.001599481647038677, 0.0015884023135489163, 0.0016001311038659576, 0.0016759631034801714, 0.0023583640625777966, 0.0019048816247959621]
[596.0282457544715, 255.24725054999163, 631.5604543322445, 389.23023756409333, 401.63117967343084, 227.50482547727694, 609.8678184614255, 572.8741783871204, 625.2485198453254, 613.8853935164703, 604.987902877888, 606.7208020594398, 667.9277422455865, 607.9399181450785, 291.27168153298913, 637.2631296561112, 653.5094412928696, 568.5515937943853, 537.9543190626134, 578.5170005011397, 482.0017353981037, 627.5623352464864, 389.8053708828529, 619.2333974988442, 611.49397029027, 572.7266938604631, 624.137893806581, 649.1190965429595, 605.1439302227893, 574.4287040967857, 654.0177496184356, 522.5514486495697, 639.0167257339202, 480.85247894769526, 608.4786703479114, 604.6736484596332, 592.0625552084321, 316.1877085089151, 638.2688360950331, 381.26654164900964, 655.2284019407574, 489.5432362694716, 592.7937571411464, 395.7945811178355, 609.2015239396876, 576.9324738005232, 549.8630760597846, 615.5586487251226, 566.9986560718811, 581.151503204247, 564.218335051066, 661.4673256524508, 408.40237043659977, 543.7366504107792, 473.5473093070116, 494.6387367801397, 585.3160076269407, 613.6208240797544, 623.3671762948981, 202.0236294388185, 507.7767878229868, 662.7595415485508, 684.7758773488686, 538.7251586441905, 302.0723898358021, 657.0824088244109, 589.3297138665331, 434.6654185901781, 348.1329210446297, 459.5360457137375, 412.8003873595885, 414.515759932676, 457.3297018558473, 713.4108817844117, 681.5476052199571, 459.7246794825666, 535.2945296941307, 503.2926791190864, 522.0715603606963, 550.103252815584, 564.5974100651456, 574.6161668402192, 472.78699778252326, 599.7853974392256, 598.1350611984961, 577.092517408416, 528.1913795335615, 542.145248810866, 466.5295072195761, 569.9797588247791, 452.6721686013802, 139.7888026511771, 452.6545887043924, 374.3969088269268, 334.69868525441456, 304.9787837010915, 516.5601649357711, 205.50751422969964, 228.27044279894477, 327.7725210140534, 366.9161747686575, 358.82288577635154, 381.26682346260003, 517.233556808539, 359.49647446627915, 279.36537452890633, 340.61944050454525, 540.455784169384, 584.4409882728955, 569.9791760785967, 577.8152935810764, 583.0511568625219, 589.2500279269765, 552.886150017932, 552.8549528520718, 508.59772016783376, 594.8168605877055, 506.06092131620096, 287.64344409651585, 491.08853730340775, 496.97714195201576, 287.50267640452176, 291.4592169931411, 259.56917537495013, 375.88050865626155, 568.191241608451, 599.6125527220281, 618.4506317604105, 388.7775756455924, 604.709828179836, 464.18707308391674, 394.46145194982324, 567.1553529034479, 596.8617983298324, 618.7211883671715, 336.4484910945061, 643.983847258988, 332.8412229253758, 588.7065111041842, 367.9812463791073, 388.3151630923201, 521.6047043678018, 572.1347557730355, 371.6754185776579, 410.8810198460262, 609.6260485187312, 331.90553786666726, 580.3187365479897, 598.0834711009827, 597.9178772742112, 567.3695629170506, 250.03036817922134, 634.8271743776505, 289.233102701194, 624.8823872066275, 622.2823439978149, 572.7568420365983, 405.9573085931372, 591.4455463327118, 384.0563088959054, 484.75699795149694, 586.9534119378202, 523.8516647729159, 419.11490501354956, 388.9174000255036, 511.79253151081906, 609.7728217590463, 549.5470666029273, 580.455826928761, 622.1960106347738, 592.7716683969438, 508.31635211418757, 604.191646952588, 585.3066873122661, 589.3427818370324, 411.42033929169355, 601.184250448407, 632.0158280948739, 625.2025472448694, 629.5634244990063, 624.9487917483602, 596.6718467271025, 424.02274350591813, 524.9670042394961]
Elapsed: 0.10345300334270376~0.03766427681116524
Time per graph: 0.0021244896558709164~0.0007689580543029705
Speed: 511.0760401078527~120.89716509791079
Total Time: 0.0921
best val loss: 0.4111703038215637 test_score: 0.9167

Testing...
Test loss: 0.2511 score: 0.9167 time: 0.09s
test Score 0.9167
Epoch Time List: [0.39578646095469594, 0.5258555840700865, 0.4026856740238145, 0.4278206310700625, 0.467819846002385, 0.6515395969618112, 0.4175676649902016, 0.5503474089782685, 0.4265239519299939, 0.4028221048647538, 0.42095670895650983, 0.4266176460077986, 0.4204322400037199, 0.4044971000403166, 0.5581564218737185, 0.5026286209467798, 0.4859235550975427, 0.40031100797932595, 0.4655602319398895, 0.42038007196970284, 0.42827417713124305, 0.41743412415962666, 0.4425220830598846, 0.43604431697167456, 0.4070489809382707, 0.4130787869216874, 0.3940317799570039, 0.38615717797074467, 0.4308566309046, 0.4550861819880083, 0.41537020809482783, 0.47363303496968, 0.4004715859191492, 0.4427669399883598, 0.40427433303557336, 0.4103048350661993, 0.4119646449107677, 0.4574936638819054, 0.3814487080089748, 0.5115188440540805, 0.3700429010204971, 0.4096368111204356, 0.384090143837966, 0.4504503790521994, 0.4439002750441432, 0.41260244185104966, 0.4022644329816103, 0.430324112996459, 0.4194762709084898, 0.419574867002666, 0.4150240420131013, 0.42678257497027516, 0.4402795350179076, 0.4074930108617991, 0.49695852992590517, 0.688481007120572, 0.46313751698471606, 0.40912802203092724, 0.4011010261019692, 0.6116448498796672, 0.6473373001208529, 0.3841582011664286, 0.37279291392769665, 0.6227723601041362, 0.5853807928506285, 0.5209252788918093, 0.4153331269044429, 0.4494899120181799, 0.6537859330419451, 0.4434493030421436, 0.4465473419986665, 0.5683755681384355, 0.576937458012253, 0.43185365106910467, 0.37866739102173597, 0.4836575170047581, 0.403089300962165, 0.40939860395155847, 0.4365708150435239, 0.39821603207383305, 0.39493073406629264, 0.3760034618899226, 0.3953483069781214, 0.37555093003902584, 0.38050285808276385, 0.3771841659909114, 0.4196103358408436, 0.4034120700089261, 0.4185364911099896, 0.3849706038599834, 0.46540345111861825, 0.7235517560038716, 0.6148283157963306, 0.4895388700533658, 0.45728985604364425, 1.3246665600454435, 0.472923799068667, 0.7390244289999828, 0.6362323670182377, 0.5979024640982971, 0.8734237570315599, 0.45984042703639716, 0.5328644749242812, 0.4973394601838663, 0.5948441621148959, 0.7480770549736917, 0.5038687380729243, 0.4362689880654216, 0.380736356950365, 0.3801019919337705, 0.36680141591932625, 0.36774810194037855, 0.3868342989590019, 0.3947586138965562, 0.3858454469591379, 0.5944187319837511, 0.4499450210714713, 0.3869304789695889, 0.4718858830165118, 0.43928686098661274, 0.4141338460613042, 0.4792900369502604, 0.45949319610372186, 0.5996867481153458, 0.4536856801714748, 0.5128526970511302, 0.40108747396152467, 0.366923519060947, 0.42310771986376494, 0.37861026101745665, 0.5221840490121394, 0.5192498759133741, 0.4570337460609153, 0.394440516945906, 0.38637423189356923, 0.480343385017477, 0.3651750379940495, 0.4513883840991184, 0.4227667178492993, 0.4438424468971789, 0.555133622023277, 0.4553433230612427, 0.3935741300228983, 0.45188346912618726, 0.44897724606562406, 0.3820705870166421, 0.6396437640069053, 0.43010727094952017, 0.3879808309720829, 0.43911729601677507, 0.4202875088667497, 0.6106051269453019, 0.3785720559535548, 0.4936294868821278, 0.3786773760803044, 0.3815894820727408, 0.38226463203318417, 0.4341673869639635, 0.4764248048886657, 0.457185243954882, 0.564407111145556, 0.4469841090030968, 0.40274875692557544, 0.47663255012594163, 0.48205850599333644, 0.6317523369798437, 0.4681798859965056, 0.4119101850083098, 0.456109459977597, 0.3972701180027798, 0.386748853023164, 0.528152768034488, 0.5329034150345251, 0.4195110759465024, 0.3877594619989395, 0.49829958414193243, 0.38906639302149415, 0.46299020096194, 0.41052017896436155, 0.3735275469953194, 0.391923512914218, 0.42388665908947587, 0.4281855250010267, 0.4246776840882376]
Total Epoch List: [75, 50, 59]
Total Time List: [0.07230219605844468, 0.13093393703456968, 0.0920888219261542]
T-times Epoch Time: 0.46199875122295103 ~ 0.0015237325680467326
T-times Total Epoch: 61.333333333333336 ~ 0.27216552697590773
T-times Total Time: 0.09983549444263595 ~ 0.007282601664832932
T-times Inference Elapsed: 0.10266919560819145 ~ 0.0016837885291057762
T-times Time Per Graph: 0.0021072057209535013 ~ 3.477242087686864e-05
T-times Speed: 514.6681768596485 ~ 5.7927885275043955
T-times cross validation test micro f1 score:0.8871171684286155 ~ 0.006414710755704696
T-times cross validation test precision:0.9585889955455174 ~ 0.01411106900602225
T-times cross validation test recall:0.8270370370370371 ~ 0.012832678621533656
T-times cross validation test f1_score:0.8871171684286155 ~ 0.006755936646455323
