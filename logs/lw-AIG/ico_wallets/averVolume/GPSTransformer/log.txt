Namespace(seed=60, model='GPSTransformer', dataset='ico_wallets/averVolume', num_heads=1, num_layers=2, dim_hidden=64, dropout=0.1, epochs=1000, lr=0.0003, weight_decay=0.0001, batch_size=64, abs_pe=None, abs_pe_dim=20, num_class=2, outdir='./outdir/ico_wallets/averVolume/seed60/edge_attr/khopgnn_gat_1_0.1_0.0003_0.0001_2_1_64_BN', warmup=10, layer_norm=False, use_edge_attr=True, edge_dim=128, gnn_type='gat', k_hop=1, global_pool='mean', se='khopgnn', ie='mamba', aggr='add', not_extract_node_feature=False, k_ford=3, early_stop=1, early_stop_mindelta=-0.0, patience=20, training_times=3, Lambda=0.01, order='sd', use_cuda=True, batch_norm=True, save_logs=True)
Data(edge_index=[2, 342], edge_attr=[342, 2], x=[99, 14887], y=[1, 1], num_nodes=99)
Data(edge_index=[2, 342], edge_attr=[342, 2], x=[99, 14887], y=[1, 1], num_nodes=99)
Data(edge_index=[2, 298], edge_attr=[298, 2], x=[87, 14887], y=[1, 1], num_nodes=99)
========================training times:0========================
========================k_idx:0========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: GPSTransformer
GPS(
  (x_lin): Linear(in_features=14887, out_features=64, bias=True)
  (pe_lin): Linear(in_features=20, out_features=8, bias=True)
  (pe_norm): BatchNorm1d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (convs): ModuleList(
    (0-1): 2 x GPSConv(64, conv=GINEConv(nn=Sequential(
      (0): Linear(in_features=64, out_features=64, bias=True)
      (1): ReLU()
      (2): Linear(in_features=64, out_features=64, bias=True)
    )), heads=1, attn_type=multihead)
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=32, bias=True)
    (1): ReLU()
    (2): Linear(in_features=32, out_features=16, bias=True)
    (3): ReLU()
    (4): Linear(in_features=16, out_features=2, bias=True)
  )
)
Total number of parameters: 1039906
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x74b5c4faf490>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.9805;  Loss pred: 0.9805; Loss self: 0.0000; time: 0.34s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6188 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6502 score: 0.5102 time: 0.06s
Epoch 2/1000, LR 0.000000
Train loss: 0.9731;  Loss pred: 0.9731; Loss self: 0.0000; time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6288 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6532 score: 0.5102 time: 0.06s
     INFO: Early stopping counter 1 of 20
Epoch 3/1000, LR 0.000030
Train loss: 0.9239;  Loss pred: 0.9239; Loss self: 0.0000; time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6326 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6542 score: 0.5102 time: 0.06s
     INFO: Early stopping counter 2 of 20
Epoch 4/1000, LR 0.000060
Train loss: 0.7912;  Loss pred: 0.7912; Loss self: 0.0000; time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6328 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6513 score: 0.5102 time: 0.06s
     INFO: Early stopping counter 3 of 20
Epoch 5/1000, LR 0.000090
Train loss: 0.6924;  Loss pred: 0.6924; Loss self: 0.0000; time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6302 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6534 score: 0.5102 time: 0.06s
     INFO: Early stopping counter 4 of 20
Epoch 6/1000, LR 0.000120
Train loss: 0.5888;  Loss pred: 0.5888; Loss self: 0.0000; time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6295 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6547 score: 0.5102 time: 0.06s
     INFO: Early stopping counter 5 of 20
Epoch 7/1000, LR 0.000150
Train loss: 0.4835;  Loss pred: 0.4835; Loss self: 0.0000; time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6255 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6485 score: 0.5102 time: 0.06s
     INFO: Early stopping counter 6 of 20
Epoch 8/1000, LR 0.000180
Train loss: 0.4123;  Loss pred: 0.4123; Loss self: 0.0000; time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6193 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6476 score: 0.5102 time: 0.06s
     INFO: Early stopping counter 7 of 20
Epoch 9/1000, LR 0.000210
Train loss: 0.3584;  Loss pred: 0.3584; Loss self: 0.0000; time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6161 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6487 score: 0.5102 time: 0.06s
Epoch 10/1000, LR 0.000240
Train loss: 0.3117;  Loss pred: 0.3117; Loss self: 0.0000; time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6154 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6531 score: 0.5102 time: 0.07s
Epoch 11/1000, LR 0.000270
Train loss: 0.2726;  Loss pred: 0.2726; Loss self: 0.0000; time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6182 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6536 score: 0.5102 time: 0.06s
     INFO: Early stopping counter 1 of 20
Epoch 12/1000, LR 0.000270
Train loss: 0.2409;  Loss pred: 0.2409; Loss self: 0.0000; time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6189 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6516 score: 0.5102 time: 0.06s
     INFO: Early stopping counter 2 of 20
Epoch 13/1000, LR 0.000270
Train loss: 0.2004;  Loss pred: 0.2004; Loss self: 0.0000; time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6197 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6490 score: 0.5102 time: 0.06s
     INFO: Early stopping counter 3 of 20
Epoch 14/1000, LR 0.000270
Train loss: 0.1822;  Loss pred: 0.1822; Loss self: 0.0000; time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6206 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6473 score: 0.5102 time: 0.06s
     INFO: Early stopping counter 4 of 20
Epoch 15/1000, LR 0.000270
Train loss: 0.1599;  Loss pred: 0.1599; Loss self: 0.0000; time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6229 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6461 score: 0.5102 time: 0.07s
     INFO: Early stopping counter 5 of 20
Epoch 16/1000, LR 0.000270
Train loss: 0.1396;  Loss pred: 0.1396; Loss self: 0.0000; time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6248 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6450 score: 0.5102 time: 0.07s
     INFO: Early stopping counter 6 of 20
Epoch 17/1000, LR 0.000270
Train loss: 0.1228;  Loss pred: 0.1228; Loss self: 0.0000; time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6269 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6436 score: 0.5102 time: 0.06s
     INFO: Early stopping counter 7 of 20
Epoch 18/1000, LR 0.000270
Train loss: 0.1151;  Loss pred: 0.1151; Loss self: 0.0000; time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6282 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6418 score: 0.5102 time: 0.07s
     INFO: Early stopping counter 8 of 20
Epoch 19/1000, LR 0.000270
Train loss: 0.0988;  Loss pred: 0.0988; Loss self: 0.0000; time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6286 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6400 score: 0.5102 time: 0.06s
     INFO: Early stopping counter 9 of 20
Epoch 20/1000, LR 0.000270
Train loss: 0.0912;  Loss pred: 0.0912; Loss self: 0.0000; time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6292 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6385 score: 0.5102 time: 0.06s
     INFO: Early stopping counter 10 of 20
Epoch 21/1000, LR 0.000270
Train loss: 0.0796;  Loss pred: 0.0796; Loss self: 0.0000; time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6299 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6371 score: 0.5102 time: 0.06s
     INFO: Early stopping counter 11 of 20
Epoch 22/1000, LR 0.000270
Train loss: 0.0749;  Loss pred: 0.0749; Loss self: 0.0000; time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6301 score: 0.4898 time: 0.08s
Test loss: 0.6358 score: 0.5306 time: 0.07s
     INFO: Early stopping counter 12 of 20
Epoch 23/1000, LR 0.000270
Train loss: 0.0650;  Loss pred: 0.0650; Loss self: 0.0000; time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6296 score: 0.4898 time: 0.07s
Test loss: 0.6342 score: 0.5510 time: 0.06s
     INFO: Early stopping counter 13 of 20
Epoch 24/1000, LR 0.000270
Train loss: 0.0560;  Loss pred: 0.0560; Loss self: 0.0000; time: 0.21s
Val loss: 0.6301 score: 0.5102 time: 0.08s
Test loss: 0.6330 score: 0.5510 time: 0.06s
     INFO: Early stopping counter 14 of 20
Epoch 25/1000, LR 0.000270
Train loss: 0.0532;  Loss pred: 0.0532; Loss self: 0.0000; time: 0.22s
Val loss: 0.6295 score: 0.5510 time: 0.08s
Test loss: 0.6307 score: 0.5714 time: 0.06s
     INFO: Early stopping counter 15 of 20
Epoch 26/1000, LR 0.000270
Train loss: 0.0476;  Loss pred: 0.0476; Loss self: 0.0000; time: 0.21s
Val loss: 0.6280 score: 0.6122 time: 0.07s
Test loss: 0.6283 score: 0.5510 time: 0.06s
     INFO: Early stopping counter 16 of 20
Epoch 27/1000, LR 0.000270
Train loss: 0.0448;  Loss pred: 0.0448; Loss self: 0.0000; time: 0.20s
Val loss: 0.6255 score: 0.6122 time: 0.07s
Test loss: 0.6257 score: 0.5918 time: 0.06s
     INFO: Early stopping counter 17 of 20
Epoch 28/1000, LR 0.000270
Train loss: 0.0434;  Loss pred: 0.0434; Loss self: 0.0000; time: 0.21s
Val loss: 0.6233 score: 0.5714 time: 0.07s
Test loss: 0.6235 score: 0.5918 time: 0.06s
     INFO: Early stopping counter 18 of 20
Epoch 29/1000, LR 0.000270
Train loss: 0.0395;  Loss pred: 0.0395; Loss self: 0.0000; time: 0.20s
Val loss: 0.6211 score: 0.6122 time: 0.07s
Test loss: 0.6214 score: 0.6122 time: 0.06s
     INFO: Early stopping counter 19 of 20
Epoch 30/1000, LR 0.000270
Train loss: 0.0351;  Loss pred: 0.0351; Loss self: 0.0000; time: 0.20s
Val loss: 0.6199 score: 0.6531 time: 0.07s
Test loss: 0.6200 score: 0.6531 time: 0.06s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 009,   Train_Loss: 0.3117,   Val_Loss: 0.6154,   Val_Precision: 0.4898,   Val_Recall: 1.0000,   Val_accuracy: 0.6575,   Val_Score: 0.4898,   Val_Loss: 0.6154,   Test_Precision: 0.5102,   Test_Recall: 1.0000,   Test_accuracy: 0.6757,   Test_Score: 0.5102,   Test_loss: 0.6531


[0.06482695194426924, 0.06583771295845509, 0.0650867949007079, 0.06547505501657724, 0.0653332929359749, 0.06579327001236379, 0.06679044698830694, 0.06654995097778738, 0.06616380310151726, 0.07342366594821215, 0.06916214502416551, 0.0675493850139901, 0.06781924597453326, 0.06805131293367594, 0.07393869699444622, 0.07485074806027114, 0.06921576103195548, 0.0723197340266779, 0.0689182699425146, 0.06944285496138036, 0.06899320601951331, 0.07221619307529181, 0.06842620193492621, 0.06858358602039516, 0.06858222698792815, 0.06734950805548579, 0.06743598508182913, 0.0675579069647938, 0.06718390190508217, 0.06666443101130426]
[0.0013229990192708007, 0.001343626795070512, 0.0013283019367491408, 0.0013362256125832091, 0.0013333325088974468, 0.0013427197961706895, 0.0013630703467001415, 0.0013581622648528035, 0.0013502816959493319, 0.0014984421622084112, 0.0014114723474319493, 0.0013785588778365328, 0.0013840662443782299, 0.001388802304768897, 0.0015089529998866574, 0.001527566286944309, 0.0014125665516725608, 0.0014759129393199573, 0.0014064953049492774, 0.0014172011216608237, 0.0014080246126431288, 0.0014737998586794247, 0.0013964531007127799, 0.001399665020824391, 0.0013996372854679215, 0.0013744797562344037, 0.0013762445935067169, 0.0013787327951998735, 0.0013711000388792278, 0.0013604985920674338]
[755.8584590268036, 744.2542852440816, 752.8408807769864, 748.3766143853408, 750.0004637454729, 744.7570244007021, 733.6378510624202, 736.2890472504617, 740.5862073076078, 667.3597588352661, 708.4800505085436, 725.3952051503007, 722.5087701269923, 720.0448880061476, 662.7111646784979, 654.6360760555704, 707.9312467196268, 677.5467396205366, 710.9870871812566, 705.6161505348626, 710.2148577664497, 678.5181814958473, 716.0999531524392, 714.4566629313979, 714.4708206781472, 727.5480016814886, 726.6150252056335, 725.3037016900951, 729.3413840301729, 735.0246489269681]
Elapsed: 0.0683180748601444~0.0026125884124585234
Time per graph: 0.0013942464257172325~5.331813086650049e-05
Speed: 718.2470402725371~26.531873492805996
Total Time: 0.0704
best val loss: 0.6153813600540161 test_score: 0.5102

Testing...
Test loss: 0.6200 score: 0.6531 time: 0.06s
test Score 0.6531
Epoch Time List: [0.4705711400602013, 0.321695368969813, 0.3223287450382486, 0.32363016181625426, 0.3219838390359655, 0.323124471004121, 0.32558802887797356, 0.33685650711413473, 0.32541257492266595, 0.35035461094230413, 0.34347808407619596, 0.3387586160097271, 0.3440129399532452, 0.3394425930455327, 0.3477822410641238, 0.3456348628969863, 0.34576276200823486, 0.34389086987357587, 0.34064382791984826, 0.3528313289862126, 0.35252513096202165, 0.35730106104165316, 0.34459661005530506, 0.34752377786207944, 0.3609588759718463, 0.3386324670864269, 0.33904182293917984, 0.3475183689733967, 0.34126539004500955, 0.33420956891495734]
Total Epoch List: [30]
Total Time List: [0.07038785191252828]
========================k_idx:1========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: GPSTransformer
GPS(
  (x_lin): Linear(in_features=14887, out_features=64, bias=True)
  (pe_lin): Linear(in_features=20, out_features=8, bias=True)
  (pe_norm): BatchNorm1d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (convs): ModuleList(
    (0-1): 2 x GPSConv(64, conv=GINEConv(nn=Sequential(
      (0): Linear(in_features=64, out_features=64, bias=True)
      (1): ReLU()
      (2): Linear(in_features=64, out_features=64, bias=True)
    )), heads=1, attn_type=multihead)
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=32, bias=True)
    (1): ReLU()
    (2): Linear(in_features=32, out_features=16, bias=True)
    (3): ReLU()
    (4): Linear(in_features=16, out_features=2, bias=True)
  )
)
Total number of parameters: 1039906
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x74b5c4faf310>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.9243;  Loss pred: 0.9243; Loss self: 0.0000; time: 0.21s
Val loss: 0.9938 score: 0.4286 time: 0.07s
Test loss: 1.3263 score: 0.3878 time: 0.07s
Epoch 2/1000, LR 0.000000
Train loss: 0.9202;  Loss pred: 0.9202; Loss self: 0.0000; time: 0.20s
Val loss: 0.9354 score: 0.4082 time: 0.06s
Test loss: 1.2433 score: 0.4082 time: 0.07s
Epoch 3/1000, LR 0.000030
Train loss: 0.8667;  Loss pred: 0.8667; Loss self: 0.0000; time: 0.20s
Val loss: 0.8920 score: 0.3878 time: 0.07s
Test loss: 1.1739 score: 0.3061 time: 0.08s
Epoch 4/1000, LR 0.000060
Train loss: 0.7861;  Loss pred: 0.7861; Loss self: 0.0000; time: 0.21s
Val loss: 0.8499 score: 0.3878 time: 0.07s
Test loss: 1.0977 score: 0.2857 time: 0.08s
Epoch 5/1000, LR 0.000090
Train loss: 0.6704;  Loss pred: 0.6704; Loss self: 0.0000; time: 0.19s
Val loss: 0.8124 score: 0.4082 time: 0.07s
Test loss: 1.0169 score: 0.3061 time: 0.08s
Epoch 6/1000, LR 0.000120
Train loss: 0.5640;  Loss pred: 0.5640; Loss self: 0.0000; time: 0.32s
Val loss: 0.7791 score: 0.3878 time: 0.08s
Test loss: 0.9443 score: 0.2653 time: 0.09s
Epoch 7/1000, LR 0.000150
Train loss: 0.4854;  Loss pred: 0.4854; Loss self: 0.0000; time: 0.20s
Val loss: 0.7490 score: 0.3265 time: 0.07s
Test loss: 0.8782 score: 0.3061 time: 0.08s
Epoch 8/1000, LR 0.000180
Train loss: 0.4300;  Loss pred: 0.4300; Loss self: 0.0000; time: 0.20s
Val loss: 0.7238 score: 0.4082 time: 0.06s
Test loss: 0.8184 score: 0.3061 time: 0.08s
Epoch 9/1000, LR 0.000210
Train loss: 0.3799;  Loss pred: 0.3799; Loss self: 0.0000; time: 0.20s
Val loss: 0.7057 score: 0.3878 time: 0.07s
Test loss: 0.7675 score: 0.2857 time: 0.07s
Epoch 10/1000, LR 0.000240
Train loss: 0.3309;  Loss pred: 0.3309; Loss self: 0.0000; time: 0.21s
Val loss: 0.6903 score: 0.4694 time: 0.07s
Test loss: 0.7245 score: 0.3469 time: 0.15s
Epoch 11/1000, LR 0.000270
Train loss: 0.2851;  Loss pred: 0.2851; Loss self: 0.0000; time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6793 score: 0.5102 time: 0.06s
Test loss: 0.6918 score: 0.4286 time: 0.07s
Epoch 12/1000, LR 0.000270
Train loss: 0.2434;  Loss pred: 0.2434; Loss self: 0.0000; time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6721 score: 0.5102 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6679 score: 0.4898 time: 0.07s
Epoch 13/1000, LR 0.000270
Train loss: 0.2115;  Loss pred: 0.2115; Loss self: 0.0000; time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6672 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6516 score: 0.4898 time: 0.08s
Epoch 14/1000, LR 0.000270
Train loss: 0.1879;  Loss pred: 0.1879; Loss self: 0.0000; time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6635 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6400 score: 0.4898 time: 0.07s
Epoch 15/1000, LR 0.000270
Train loss: 0.1602;  Loss pred: 0.1602; Loss self: 0.0000; time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6617 score: 0.5102 time: 0.10s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6312 score: 0.4898 time: 0.09s
Epoch 16/1000, LR 0.000270
Train loss: 0.1418;  Loss pred: 0.1418; Loss self: 0.0000; time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6616 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6276 score: 0.4898 time: 0.08s
Epoch 17/1000, LR 0.000270
Train loss: 0.1313;  Loss pred: 0.1313; Loss self: 0.0000; time: 0.21s
Val loss: 0.6631 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6249 score: 0.4898 time: 0.08s
     INFO: Early stopping counter 1 of 20
Epoch 18/1000, LR 0.000270
Train loss: 0.1174;  Loss pred: 0.1174; Loss self: 0.0000; time: 0.22s
Val loss: 0.6653 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6235 score: 0.4898 time: 0.08s
     INFO: Early stopping counter 2 of 20
Epoch 19/1000, LR 0.000270
Train loss: 0.1024;  Loss pred: 0.1024; Loss self: 0.0000; time: 0.21s
Val loss: 0.6670 score: 0.4694 time: 0.07s
Test loss: 0.6224 score: 0.4694 time: 0.08s
     INFO: Early stopping counter 3 of 20
Epoch 20/1000, LR 0.000270
Train loss: 0.0964;  Loss pred: 0.0964; Loss self: 0.0000; time: 0.33s
Val loss: 0.6684 score: 0.4490 time: 0.07s
Test loss: 0.6212 score: 0.4490 time: 0.07s
     INFO: Early stopping counter 4 of 20
Epoch 21/1000, LR 0.000270
Train loss: 0.0847;  Loss pred: 0.0847; Loss self: 0.0000; time: 0.21s
Val loss: 0.6691 score: 0.4490 time: 0.07s
Test loss: 0.6201 score: 0.4490 time: 0.08s
     INFO: Early stopping counter 5 of 20
Epoch 22/1000, LR 0.000270
Train loss: 0.0807;  Loss pred: 0.0807; Loss self: 0.0000; time: 0.21s
Val loss: 0.6694 score: 0.5102 time: 0.07s
Test loss: 0.6199 score: 0.4286 time: 0.08s
     INFO: Early stopping counter 6 of 20
Epoch 23/1000, LR 0.000270
Train loss: 0.0720;  Loss pred: 0.0720; Loss self: 0.0000; time: 0.21s
Val loss: 0.6704 score: 0.4898 time: 0.07s
Test loss: 0.6210 score: 0.4898 time: 0.07s
     INFO: Early stopping counter 7 of 20
Epoch 24/1000, LR 0.000270
Train loss: 0.0643;  Loss pred: 0.0643; Loss self: 0.0000; time: 0.21s
Val loss: 0.6721 score: 0.5510 time: 0.07s
Test loss: 0.6226 score: 0.5306 time: 0.07s
     INFO: Early stopping counter 8 of 20
Epoch 25/1000, LR 0.000270
Train loss: 0.0604;  Loss pred: 0.0604; Loss self: 0.0000; time: 0.21s
Val loss: 0.6712 score: 0.5510 time: 0.07s
Test loss: 0.6212 score: 0.5510 time: 0.07s
     INFO: Early stopping counter 9 of 20
Epoch 26/1000, LR 0.000270
Train loss: 0.0537;  Loss pred: 0.0537; Loss self: 0.0000; time: 0.21s
Val loss: 0.6680 score: 0.5306 time: 0.07s
Test loss: 0.6176 score: 0.5918 time: 0.07s
     INFO: Early stopping counter 10 of 20
Epoch 27/1000, LR 0.000270
Train loss: 0.0519;  Loss pred: 0.0519; Loss self: 0.0000; time: 0.21s
Val loss: 0.6641 score: 0.5510 time: 0.07s
Test loss: 0.6137 score: 0.6735 time: 0.08s
     INFO: Early stopping counter 11 of 20
Epoch 28/1000, LR 0.000270
Train loss: 0.0453;  Loss pred: 0.0453; Loss self: 0.0000; time: 0.20s
Val loss: 0.6595 score: 0.5510 time: 0.07s
Test loss: 0.6077 score: 0.7143 time: 0.07s
Epoch 29/1000, LR 0.000270
Train loss: 0.0429;  Loss pred: 0.0429; Loss self: 0.0000; time: 0.21s
Val loss: 0.6537 score: 0.5714 time: 0.07s
Test loss: 0.6010 score: 0.7347 time: 0.07s
Epoch 30/1000, LR 0.000270
Train loss: 0.0389;  Loss pred: 0.0389; Loss self: 0.0000; time: 0.21s
Val loss: 0.6482 score: 0.5918 time: 0.07s
Test loss: 0.5953 score: 0.7551 time: 0.08s
Epoch 31/1000, LR 0.000270
Train loss: 0.0354;  Loss pred: 0.0354; Loss self: 0.0000; time: 0.21s
Val loss: 0.6430 score: 0.5918 time: 0.07s
Test loss: 0.5908 score: 0.7551 time: 0.08s
Epoch 32/1000, LR 0.000270
Train loss: 0.0347;  Loss pred: 0.0347; Loss self: 0.0000; time: 0.20s
Val loss: 0.6401 score: 0.5510 time: 0.07s
Test loss: 0.5891 score: 0.7551 time: 0.08s
Epoch 33/1000, LR 0.000270
Train loss: 0.0332;  Loss pred: 0.0332; Loss self: 0.0000; time: 0.20s
Val loss: 0.6370 score: 0.6122 time: 0.07s
Test loss: 0.5884 score: 0.7551 time: 0.08s
Epoch 34/1000, LR 0.000270
Train loss: 0.0298;  Loss pred: 0.0298; Loss self: 0.0000; time: 0.20s
Val loss: 0.6326 score: 0.6122 time: 0.07s
Test loss: 0.5866 score: 0.7347 time: 0.08s
Epoch 35/1000, LR 0.000270
Train loss: 0.0272;  Loss pred: 0.0272; Loss self: 0.0000; time: 0.20s
Val loss: 0.6269 score: 0.6327 time: 0.07s
Test loss: 0.5833 score: 0.7347 time: 0.08s
Epoch 36/1000, LR 0.000270
Train loss: 0.0250;  Loss pred: 0.0250; Loss self: 0.0000; time: 0.20s
Val loss: 0.6202 score: 0.6531 time: 0.06s
Test loss: 0.5786 score: 0.7143 time: 0.08s
Epoch 37/1000, LR 0.000270
Train loss: 0.0231;  Loss pred: 0.0231; Loss self: 0.0000; time: 0.20s
Val loss: 0.6128 score: 0.6939 time: 0.07s
Test loss: 0.5720 score: 0.7143 time: 0.08s
Epoch 38/1000, LR 0.000270
Train loss: 0.0225;  Loss pred: 0.0225; Loss self: 0.0000; time: 0.22s
Val loss: 0.6068 score: 0.6939 time: 0.06s
Test loss: 0.5660 score: 0.7551 time: 0.08s
Epoch 39/1000, LR 0.000269
Train loss: 0.0199;  Loss pred: 0.0199; Loss self: 0.0000; time: 0.21s
Val loss: 0.6013 score: 0.6939 time: 0.06s
Test loss: 0.5595 score: 0.7551 time: 0.08s
Epoch 40/1000, LR 0.000269
Train loss: 0.0203;  Loss pred: 0.0203; Loss self: 0.0000; time: 0.20s
Val loss: 0.5972 score: 0.6939 time: 0.07s
Test loss: 0.5542 score: 0.7347 time: 0.08s
Epoch 41/1000, LR 0.000269
Train loss: 0.0189;  Loss pred: 0.0189; Loss self: 0.0000; time: 0.22s
Val loss: 0.5940 score: 0.6939 time: 0.07s
Test loss: 0.5501 score: 0.7347 time: 0.07s
Epoch 42/1000, LR 0.000269
Train loss: 0.0162;  Loss pred: 0.0162; Loss self: 0.0000; time: 0.21s
Val loss: 0.5902 score: 0.6939 time: 0.06s
Test loss: 0.5452 score: 0.7143 time: 0.08s
Epoch 43/1000, LR 0.000269
Train loss: 0.0160;  Loss pred: 0.0160; Loss self: 0.0000; time: 0.20s
Val loss: 0.5858 score: 0.6939 time: 0.06s
Test loss: 0.5386 score: 0.7551 time: 0.07s
Epoch 44/1000, LR 0.000269
Train loss: 0.0139;  Loss pred: 0.0139; Loss self: 0.0000; time: 0.20s
Val loss: 0.5814 score: 0.7143 time: 0.06s
Test loss: 0.5318 score: 0.7551 time: 0.08s
Epoch 45/1000, LR 0.000269
Train loss: 0.0138;  Loss pred: 0.0138; Loss self: 0.0000; time: 0.22s
Val loss: 0.5777 score: 0.6939 time: 0.07s
Test loss: 0.5262 score: 0.8163 time: 0.08s
Epoch 46/1000, LR 0.000269
Train loss: 0.0126;  Loss pred: 0.0126; Loss self: 0.0000; time: 0.21s
Val loss: 0.5744 score: 0.6939 time: 0.07s
Test loss: 0.5220 score: 0.8163 time: 0.07s
Epoch 47/1000, LR 0.000269
Train loss: 0.0120;  Loss pred: 0.0120; Loss self: 0.0000; time: 0.21s
Val loss: 0.5711 score: 0.6735 time: 0.07s
Test loss: 0.5181 score: 0.8367 time: 0.08s
Epoch 48/1000, LR 0.000269
Train loss: 0.0116;  Loss pred: 0.0116; Loss self: 0.0000; time: 0.20s
Val loss: 0.5673 score: 0.6735 time: 0.07s
Test loss: 0.5133 score: 0.8367 time: 0.08s
Epoch 49/1000, LR 0.000269
Train loss: 0.0117;  Loss pred: 0.0117; Loss self: 0.0000; time: 0.21s
Val loss: 0.5628 score: 0.6735 time: 0.07s
Test loss: 0.5068 score: 0.8367 time: 0.07s
Epoch 50/1000, LR 0.000269
Train loss: 0.0115;  Loss pred: 0.0115; Loss self: 0.0000; time: 0.21s
Val loss: 0.5583 score: 0.6735 time: 0.07s
Test loss: 0.4988 score: 0.8367 time: 0.07s
Epoch 51/1000, LR 0.000269
Train loss: 0.0097;  Loss pred: 0.0097; Loss self: 0.0000; time: 0.21s
Val loss: 0.5529 score: 0.6939 time: 0.07s
Test loss: 0.4899 score: 0.8367 time: 0.07s
Epoch 52/1000, LR 0.000269
Train loss: 0.0083;  Loss pred: 0.0083; Loss self: 0.0000; time: 0.21s
Val loss: 0.5472 score: 0.7143 time: 0.06s
Test loss: 0.4802 score: 0.8367 time: 0.07s
Epoch 53/1000, LR 0.000269
Train loss: 0.0087;  Loss pred: 0.0087; Loss self: 0.0000; time: 0.20s
Val loss: 0.5408 score: 0.7347 time: 0.06s
Test loss: 0.4698 score: 0.8571 time: 0.07s
Epoch 54/1000, LR 0.000269
Train loss: 0.0084;  Loss pred: 0.0084; Loss self: 0.0000; time: 0.21s
Val loss: 0.5342 score: 0.7347 time: 0.07s
Test loss: 0.4592 score: 0.8571 time: 0.09s
Epoch 55/1000, LR 0.000269
Train loss: 0.0074;  Loss pred: 0.0074; Loss self: 0.0000; time: 0.21s
Val loss: 0.5277 score: 0.7347 time: 0.07s
Test loss: 0.4483 score: 0.8571 time: 0.08s
Epoch 56/1000, LR 0.000269
Train loss: 0.0071;  Loss pred: 0.0071; Loss self: 0.0000; time: 0.20s
Val loss: 0.5214 score: 0.7347 time: 0.06s
Test loss: 0.4376 score: 0.8776 time: 0.07s
Epoch 57/1000, LR 0.000269
Train loss: 0.0074;  Loss pred: 0.0074; Loss self: 0.0000; time: 0.20s
Val loss: 0.5152 score: 0.7347 time: 0.07s
Test loss: 0.4272 score: 0.8980 time: 0.08s
Epoch 58/1000, LR 0.000269
Train loss: 0.0074;  Loss pred: 0.0074; Loss self: 0.0000; time: 0.20s
Val loss: 0.5093 score: 0.7551 time: 0.07s
Test loss: 0.4174 score: 0.8980 time: 0.08s
Epoch 59/1000, LR 0.000268
Train loss: 0.0063;  Loss pred: 0.0063; Loss self: 0.0000; time: 0.20s
Val loss: 0.5038 score: 0.7551 time: 0.06s
Test loss: 0.4081 score: 0.8980 time: 0.08s
Epoch 60/1000, LR 0.000268
Train loss: 0.0071;  Loss pred: 0.0071; Loss self: 0.0000; time: 0.21s
Val loss: 0.4990 score: 0.7551 time: 0.07s
Test loss: 0.3996 score: 0.8980 time: 0.07s
Epoch 61/1000, LR 0.000268
Train loss: 0.0056;  Loss pred: 0.0056; Loss self: 0.0000; time: 0.20s
Val loss: 0.4944 score: 0.7551 time: 0.07s
Test loss: 0.3914 score: 0.8980 time: 0.08s
Epoch 62/1000, LR 0.000268
Train loss: 0.0056;  Loss pred: 0.0056; Loss self: 0.0000; time: 0.20s
Val loss: 0.4899 score: 0.7551 time: 0.06s
Test loss: 0.3830 score: 0.8980 time: 0.07s
Epoch 63/1000, LR 0.000268
Train loss: 0.0051;  Loss pred: 0.0051; Loss self: 0.0000; time: 0.20s
Val loss: 0.4860 score: 0.7551 time: 0.07s
Test loss: 0.3749 score: 0.8980 time: 0.08s
Epoch 64/1000, LR 0.000268
Train loss: 0.0058;  Loss pred: 0.0058; Loss self: 0.0000; time: 0.20s
Val loss: 0.4812 score: 0.7551 time: 0.08s
Test loss: 0.3652 score: 0.8980 time: 0.08s
Epoch 65/1000, LR 0.000268
Train loss: 0.0052;  Loss pred: 0.0052; Loss self: 0.0000; time: 0.20s
Val loss: 0.4758 score: 0.7551 time: 0.07s
Test loss: 0.3550 score: 0.9184 time: 0.08s
Epoch 66/1000, LR 0.000268
Train loss: 0.0051;  Loss pred: 0.0051; Loss self: 0.0000; time: 0.21s
Val loss: 0.4701 score: 0.7551 time: 0.07s
Test loss: 0.3442 score: 0.9184 time: 0.08s
Epoch 67/1000, LR 0.000268
Train loss: 0.0051;  Loss pred: 0.0051; Loss self: 0.0000; time: 0.20s
Val loss: 0.4642 score: 0.7551 time: 0.07s
Test loss: 0.3333 score: 0.9184 time: 0.08s
Epoch 68/1000, LR 0.000268
Train loss: 0.0044;  Loss pred: 0.0044; Loss self: 0.0000; time: 0.20s
Val loss: 0.4586 score: 0.7551 time: 0.07s
Test loss: 0.3227 score: 0.8980 time: 0.08s
Epoch 69/1000, LR 0.000268
Train loss: 0.0043;  Loss pred: 0.0043; Loss self: 0.0000; time: 0.20s
Val loss: 0.4532 score: 0.7755 time: 0.08s
Test loss: 0.3121 score: 0.8980 time: 0.07s
Epoch 70/1000, LR 0.000268
Train loss: 0.0041;  Loss pred: 0.0041; Loss self: 0.0000; time: 0.22s
Val loss: 0.4477 score: 0.7755 time: 0.06s
Test loss: 0.3023 score: 0.8980 time: 0.08s
Epoch 71/1000, LR 0.000268
Train loss: 0.0037;  Loss pred: 0.0037; Loss self: 0.0000; time: 0.20s
Val loss: 0.4425 score: 0.7755 time: 0.06s
Test loss: 0.2932 score: 0.8980 time: 0.08s
Epoch 72/1000, LR 0.000267
Train loss: 0.0040;  Loss pred: 0.0040; Loss self: 0.0000; time: 0.21s
Val loss: 0.4375 score: 0.7755 time: 0.07s
Test loss: 0.2845 score: 0.9184 time: 0.08s
Epoch 73/1000, LR 0.000267
Train loss: 0.0038;  Loss pred: 0.0038; Loss self: 0.0000; time: 0.20s
Val loss: 0.4331 score: 0.7755 time: 0.08s
Test loss: 0.2766 score: 0.9184 time: 0.07s
Epoch 74/1000, LR 0.000267
Train loss: 0.0039;  Loss pred: 0.0039; Loss self: 0.0000; time: 0.21s
Val loss: 0.4289 score: 0.7959 time: 0.07s
Test loss: 0.2685 score: 0.9388 time: 0.09s
Epoch 75/1000, LR 0.000267
Train loss: 0.0036;  Loss pred: 0.0036; Loss self: 0.0000; time: 0.21s
Val loss: 0.4250 score: 0.7959 time: 0.08s
Test loss: 0.2604 score: 0.9388 time: 0.08s
Epoch 76/1000, LR 0.000267
Train loss: 0.0032;  Loss pred: 0.0032; Loss self: 0.0000; time: 0.21s
Val loss: 0.4216 score: 0.7755 time: 0.06s
Test loss: 0.2530 score: 0.9388 time: 0.08s
Epoch 77/1000, LR 0.000267
Train loss: 0.0031;  Loss pred: 0.0031; Loss self: 0.0000; time: 0.20s
Val loss: 0.4189 score: 0.7959 time: 0.07s
Test loss: 0.2462 score: 0.9388 time: 0.08s
Epoch 78/1000, LR 0.000267
Train loss: 0.0030;  Loss pred: 0.0030; Loss self: 0.0000; time: 0.21s
Val loss: 0.4173 score: 0.8163 time: 0.07s
Test loss: 0.2403 score: 0.9388 time: 0.08s
Epoch 79/1000, LR 0.000267
Train loss: 0.0030;  Loss pred: 0.0030; Loss self: 0.0000; time: 0.22s
Val loss: 0.4158 score: 0.8163 time: 0.07s
Test loss: 0.2347 score: 0.9388 time: 0.08s
Epoch 80/1000, LR 0.000267
Train loss: 0.0028;  Loss pred: 0.0028; Loss self: 0.0000; time: 0.21s
Val loss: 0.4146 score: 0.8163 time: 0.07s
Test loss: 0.2296 score: 0.9388 time: 0.08s
Epoch 81/1000, LR 0.000267
Train loss: 0.0028;  Loss pred: 0.0028; Loss self: 0.0000; time: 0.20s
Val loss: 0.4146 score: 0.8163 time: 0.06s
Test loss: 0.2253 score: 0.9388 time: 0.07s
Epoch 82/1000, LR 0.000267
Train loss: 0.0031;  Loss pred: 0.0031; Loss self: 0.0000; time: 0.20s
Val loss: 0.4147 score: 0.8163 time: 0.06s
Test loss: 0.2203 score: 0.9388 time: 0.07s
     INFO: Early stopping counter 1 of 20
Epoch 83/1000, LR 0.000266
Train loss: 0.0028;  Loss pred: 0.0028; Loss self: 0.0000; time: 0.20s
Val loss: 0.4153 score: 0.8163 time: 0.06s
Test loss: 0.2153 score: 0.9388 time: 0.07s
     INFO: Early stopping counter 2 of 20
Epoch 84/1000, LR 0.000266
Train loss: 0.0028;  Loss pred: 0.0028; Loss self: 0.0000; time: 0.20s
Val loss: 0.4159 score: 0.8163 time: 0.07s
Test loss: 0.2099 score: 0.9388 time: 0.07s
     INFO: Early stopping counter 3 of 20
Epoch 85/1000, LR 0.000266
Train loss: 0.0031;  Loss pred: 0.0031; Loss self: 0.0000; time: 0.20s
Val loss: 0.4171 score: 0.7959 time: 0.07s
Test loss: 0.2056 score: 0.9388 time: 0.07s
     INFO: Early stopping counter 4 of 20
Epoch 86/1000, LR 0.000266
Train loss: 0.0027;  Loss pred: 0.0027; Loss self: 0.0000; time: 0.20s
Val loss: 0.4204 score: 0.7959 time: 0.07s
Test loss: 0.2035 score: 0.9388 time: 0.07s
     INFO: Early stopping counter 5 of 20
Epoch 87/1000, LR 0.000266
Train loss: 0.0029;  Loss pred: 0.0029; Loss self: 0.0000; time: 0.20s
Val loss: 0.4254 score: 0.7959 time: 0.06s
Test loss: 0.2029 score: 0.9388 time: 0.08s
     INFO: Early stopping counter 6 of 20
Epoch 88/1000, LR 0.000266
Train loss: 0.0025;  Loss pred: 0.0025; Loss self: 0.0000; time: 0.21s
Val loss: 0.4304 score: 0.7959 time: 0.07s
Test loss: 0.2024 score: 0.9388 time: 0.08s
     INFO: Early stopping counter 7 of 20
Epoch 89/1000, LR 0.000266
Train loss: 0.0028;  Loss pred: 0.0028; Loss self: 0.0000; time: 0.21s
Val loss: 0.4348 score: 0.7959 time: 0.07s
Test loss: 0.2017 score: 0.9388 time: 0.08s
     INFO: Early stopping counter 8 of 20
Epoch 90/1000, LR 0.000266
Train loss: 0.0027;  Loss pred: 0.0027; Loss self: 0.0000; time: 0.20s
Val loss: 0.4394 score: 0.7959 time: 0.07s
Test loss: 0.2009 score: 0.9388 time: 0.07s
     INFO: Early stopping counter 9 of 20
Epoch 91/1000, LR 0.000266
Train loss: 0.0025;  Loss pred: 0.0025; Loss self: 0.0000; time: 0.20s
Val loss: 0.4443 score: 0.7959 time: 0.06s
Test loss: 0.2003 score: 0.9184 time: 0.07s
     INFO: Early stopping counter 10 of 20
Epoch 92/1000, LR 0.000266
Train loss: 0.0024;  Loss pred: 0.0024; Loss self: 0.0000; time: 0.21s
Val loss: 0.4494 score: 0.7959 time: 0.07s
Test loss: 0.1998 score: 0.9184 time: 0.07s
     INFO: Early stopping counter 11 of 20
Epoch 93/1000, LR 0.000265
Train loss: 0.0023;  Loss pred: 0.0023; Loss self: 0.0000; time: 0.21s
Val loss: 0.4551 score: 0.7959 time: 0.07s
Test loss: 0.1997 score: 0.9184 time: 0.07s
     INFO: Early stopping counter 12 of 20
Epoch 94/1000, LR 0.000265
Train loss: 0.0021;  Loss pred: 0.0021; Loss self: 0.0000; time: 0.21s
Val loss: 0.4609 score: 0.7959 time: 0.07s
Test loss: 0.1994 score: 0.9184 time: 0.07s
     INFO: Early stopping counter 13 of 20
Epoch 95/1000, LR 0.000265
Train loss: 0.0022;  Loss pred: 0.0022; Loss self: 0.0000; time: 0.20s
Val loss: 0.4652 score: 0.7959 time: 0.06s
Test loss: 0.1976 score: 0.9184 time: 0.08s
     INFO: Early stopping counter 14 of 20
Epoch 96/1000, LR 0.000265
Train loss: 0.0020;  Loss pred: 0.0020; Loss self: 0.0000; time: 0.20s
Val loss: 0.4687 score: 0.7959 time: 0.07s
Test loss: 0.1949 score: 0.9184 time: 0.07s
     INFO: Early stopping counter 15 of 20
Epoch 97/1000, LR 0.000265
Train loss: 0.0022;  Loss pred: 0.0022; Loss self: 0.0000; time: 0.21s
Val loss: 0.4724 score: 0.7959 time: 0.07s
Test loss: 0.1927 score: 0.9184 time: 0.08s
     INFO: Early stopping counter 16 of 20
Epoch 98/1000, LR 0.000265
Train loss: 0.0020;  Loss pred: 0.0020; Loss self: 0.0000; time: 0.20s
Val loss: 0.4773 score: 0.7959 time: 0.07s
Test loss: 0.1916 score: 0.9184 time: 0.07s
     INFO: Early stopping counter 17 of 20
Epoch 99/1000, LR 0.000265
Train loss: 0.0021;  Loss pred: 0.0021; Loss self: 0.0000; time: 0.20s
Val loss: 0.4828 score: 0.7959 time: 0.07s
Test loss: 0.1912 score: 0.9184 time: 0.08s
     INFO: Early stopping counter 18 of 20
Epoch 100/1000, LR 0.000265
Train loss: 0.0019;  Loss pred: 0.0019; Loss self: 0.0000; time: 0.20s
Val loss: 0.4898 score: 0.7959 time: 0.07s
Test loss: 0.1928 score: 0.9184 time: 0.08s
     INFO: Early stopping counter 19 of 20
Epoch 101/1000, LR 0.000265
Train loss: 0.0018;  Loss pred: 0.0018; Loss self: 0.0000; time: 0.20s
Val loss: 0.4978 score: 0.7959 time: 0.07s
Test loss: 0.1954 score: 0.9184 time: 0.08s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 080,   Train_Loss: 0.0028,   Val_Loss: 0.4146,   Val_Precision: 1.0000,   Val_Recall: 0.6400,   Val_accuracy: 0.7805,   Val_Score: 0.8163,   Val_Loss: 0.4146,   Test_Precision: 1.0000,   Test_Recall: 0.8750,   Test_accuracy: 0.9333,   Test_Score: 0.9388,   Test_loss: 0.2253


[0.06482695194426924, 0.06583771295845509, 0.0650867949007079, 0.06547505501657724, 0.0653332929359749, 0.06579327001236379, 0.06679044698830694, 0.06654995097778738, 0.06616380310151726, 0.07342366594821215, 0.06916214502416551, 0.0675493850139901, 0.06781924597453326, 0.06805131293367594, 0.07393869699444622, 0.07485074806027114, 0.06921576103195548, 0.0723197340266779, 0.0689182699425146, 0.06944285496138036, 0.06899320601951331, 0.07221619307529181, 0.06842620193492621, 0.06858358602039516, 0.06858222698792815, 0.06734950805548579, 0.06743598508182913, 0.0675579069647938, 0.06718390190508217, 0.06666443101130426, 0.07870153500698507, 0.07812334201298654, 0.08040917699690908, 0.08057387999724597, 0.08267879602499306, 0.09258812200278044, 0.08119916100986302, 0.08352010103408247, 0.0732206430984661, 0.15788360696751624, 0.07666597608476877, 0.07381408906076103, 0.08072974602691829, 0.0793804629938677, 0.08974163501989096, 0.0885987930232659, 0.08712012006435543, 0.0797891819383949, 0.09460132499225438, 0.07858771493192762, 0.08179382793605328, 0.08210221806075424, 0.07827191601973027, 0.07691160403192043, 0.07701224100310355, 0.07720945100300014, 0.08838088402990252, 0.0756651780102402, 0.07731703401077539, 0.08255784201901406, 0.08758679998572916, 0.08221058896742761, 0.08212011400610209, 0.08109443099237978, 0.0820831679739058, 0.08796141296625137, 0.08322881802450866, 0.07981455593835562, 0.08743783691897988, 0.0843059909529984, 0.07630958408117294, 0.08788257697597146, 0.07543317798990756, 0.08148276701103896, 0.08441786502953619, 0.07612562109716237, 0.08149443904403597, 0.08363239897880703, 0.07696750899776816, 0.07707213098183274, 0.07729208702221513, 0.07653446099720895, 0.07606860401574522, 0.08954264293424785, 0.08080121991224587, 0.07651120400987566, 0.08009913808200508, 0.08188897301442921, 0.0852567459223792, 0.07568250200711191, 0.08189619600307196, 0.07500909606460482, 0.0841889790026471, 0.08197807194665074, 0.08128126105293632, 0.08776152494829148, 0.08150951494462788, 0.08110297296661884, 0.07682529708836228, 0.0798028310528025, 0.08061097899917513, 0.08349049708340317, 0.07687000907026231, 0.08962043898645788, 0.081524264998734, 0.08876278495881706, 0.08875767001882195, 0.08295854600146413, 0.08094940904993564, 0.08594070910476148, 0.07272796693723649, 0.07246170507278293, 0.0730430850526318, 0.0784833460347727, 0.0783388769486919, 0.07796553091611713, 0.08462349709589034, 0.08056456199847162, 0.08009241498075426, 0.07874759996775538, 0.07818880898412317, 0.07925349497236311, 0.07387999701313674, 0.07923443405888975, 0.08463884796947241, 0.07417196698952466, 0.07981541205663234, 0.0733684339793399, 0.08512714901007712, 0.07973835803568363, 0.08606273995246738]
[0.0013229990192708007, 0.001343626795070512, 0.0013283019367491408, 0.0013362256125832091, 0.0013333325088974468, 0.0013427197961706895, 0.0013630703467001415, 0.0013581622648528035, 0.0013502816959493319, 0.0014984421622084112, 0.0014114723474319493, 0.0013785588778365328, 0.0013840662443782299, 0.001388802304768897, 0.0015089529998866574, 0.001527566286944309, 0.0014125665516725608, 0.0014759129393199573, 0.0014064953049492774, 0.0014172011216608237, 0.0014080246126431288, 0.0014737998586794247, 0.0013964531007127799, 0.001399665020824391, 0.0013996372854679215, 0.0013744797562344037, 0.0013762445935067169, 0.0013787327951998735, 0.0013711000388792278, 0.0013604985920674338, 0.0016061537756527566, 0.0015943539186323785, 0.001641003612181818, 0.001644364897902979, 0.0016873223678570014, 0.0018895535102608253, 0.0016571257348951636, 0.0017044918578384177, 0.001494298838744206, 0.003222114427908495, 0.0015646117568320157, 0.0015064099808318578, 0.0016475458372840468, 0.0016200094488544427, 0.0018314619391814483, 0.0018081386331278756, 0.0017779616339664375, 0.0016283506518039777, 0.0019306392855562118, 0.0016038309169781146, 0.0016692617946133322, 0.0016755554706276376, 0.0015973860412189852, 0.0015696245720800087, 0.0015716783878184399, 0.0015757030816938803, 0.001803691510814337, 0.0015441873063314327, 0.0015778986532811303, 0.001684853918755389, 0.0017874857139944726, 0.001677767121784237, 0.0016759206940020835, 0.0016549883875995874, 0.0016751666933450165, 0.001795130876862273, 0.0016985473066226256, 0.00162886848853787, 0.0017844456514077528, 0.0017205304276122122, 0.0015573384506361826, 0.0017935219791014583, 0.00153945261203893, 0.0016629136124701829, 0.0017228135720313508, 0.0015535841040237217, 0.001663151817225224, 0.0017067836526287149, 0.0015707654897503707, 0.0015729006322823009, 0.0015773895310656149, 0.001561927775453244, 0.0015524204901172494, 0.0018274008762091398, 0.0016490044880050178, 0.0015614531430586868, 0.0016346762873878588, 0.0016712035309067186, 0.0017399335902526366, 0.001544540857287998, 0.0016713509388382034, 0.001530797878869486, 0.001718142428625451, 0.00167302187646226, 0.001658801245978292, 0.001791051529556969, 0.001663459488665875, 0.001655162713604466, 0.0015678632058849444, 0.0016286292051592348, 0.0016451220203913292, 0.0017038876955796564, 0.0015687756953114758, 0.0018289885507440384, 0.001663760510178245, 0.0018114854073227973, 0.0018113810207922847, 0.0016930315510502883, 0.0016520287561211356, 0.0017538920225461526, 0.001484244223208908, 0.001478810307607815, 0.001490675205155751, 0.001601700939485157, 0.0015987525907896307, 0.0015911332840023904, 0.0017270101448140886, 0.0016441747346626861, 0.0016345390812398828, 0.0016070938768929669, 0.0015956899792678198, 0.0016174182647421043, 0.0015077550410844234, 0.001617029266507954, 0.0017273234279484165, 0.0015137136120311155, 0.0016288859603394354, 0.001497314979170202, 0.0017372887553076964, 0.001627313429299666, 0.0017563824480095384]
[755.8584590268036, 744.2542852440816, 752.8408807769864, 748.3766143853408, 750.0004637454729, 744.7570244007021, 733.6378510624202, 736.2890472504617, 740.5862073076078, 667.3597588352661, 708.4800505085436, 725.3952051503007, 722.5087701269923, 720.0448880061476, 662.7111646784979, 654.6360760555704, 707.9312467196268, 677.5467396205366, 710.9870871812566, 705.6161505348626, 710.2148577664497, 678.5181814958473, 716.0999531524392, 714.4566629313979, 714.4708206781472, 727.5480016814886, 726.6150252056335, 725.3037016900951, 729.3413840301729, 735.0246489269681, 622.6053913135374, 627.2133108675083, 609.3831802542085, 608.1375254818911, 592.6549775251655, 529.2255522639126, 603.4545109899365, 586.685114042239, 669.2101834465656, 310.3552100255823, 639.1361918593616, 663.8299086731939, 606.9633860072046, 617.2803502517407, 546.0118927980228, 553.0549381991319, 562.44183276841, 614.1183404766929, 517.9631469645055, 623.5071224865568, 599.0672063704902, 596.8170063778404, 626.0227485379098, 637.0950211838469, 636.2624871288361, 634.6373321330312, 554.4185322181377, 647.5898331114552, 633.7542642048459, 593.5232656482798, 559.445030620867, 596.0302756061525, 596.6869456167458, 604.233846891464, 596.9555173062652, 557.0624475848301, 588.7383860908708, 613.923104926436, 560.3981265616568, 581.2161086786595, 642.1211777000008, 557.5621663142332, 649.5815409839403, 601.3541488270971, 580.4458568438782, 643.6729092490322, 601.2680199384228, 585.8973388102487, 636.6322703963416, 635.7680704527316, 633.9588163263921, 640.2344690424738, 644.1553730873992, 547.2253039926601, 606.4264877834323, 640.4290800818576, 611.7419135001684, 598.3711627616331, 574.7345792978231, 647.4415974698545, 598.3183882943963, 653.2541061126325, 582.0239249897463, 597.7208152917766, 602.8449776152908, 558.3312280509082, 601.1568101379002, 604.170207424676, 637.8107453804128, 614.0133044600706, 607.8576467915295, 586.8931400785799, 637.439758270511, 546.7502787774132, 601.0480438034115, 552.0331524380892, 552.064965085373, 590.6564466442699, 605.3163398607783, 570.160527070694, 673.7435688568953, 676.2192519591249, 670.8369445881516, 624.3362761099679, 625.4876494092783, 628.4828619036654, 579.0353942058917, 608.2078619248197, 611.793264215774, 622.2411860179095, 626.6881493226201, 618.2692639244116, 663.237709542506, 618.4179969479118, 578.9303750645726, 660.6269455806705, 613.9165198474759, 667.8621491880023, 575.6095507697493, 614.509769289105, 569.3520799716904]
Elapsed: 0.07865954577418058~0.009727799284045624
Time per graph: 0.0016052968525342975~0.00019852651600093114
Speed: 630.314600150751~63.420013767383196
Total Time: 0.0866
best val loss: 0.41455328464508057 test_score: 0.9388

Testing...
Test loss: 0.2403 score: 0.9388 time: 0.08s
test Score 0.9388
Epoch Time List: [0.4705711400602013, 0.321695368969813, 0.3223287450382486, 0.32363016181625426, 0.3219838390359655, 0.323124471004121, 0.32558802887797356, 0.33685650711413473, 0.32541257492266595, 0.35035461094230413, 0.34347808407619596, 0.3387586160097271, 0.3440129399532452, 0.3394425930455327, 0.3477822410641238, 0.3456348628969863, 0.34576276200823486, 0.34389086987357587, 0.34064382791984826, 0.3528313289862126, 0.35252513096202165, 0.35730106104165316, 0.34459661005530506, 0.34752377786207944, 0.3609588759718463, 0.3386324670864269, 0.33904182293917984, 0.3475183689733967, 0.34126539004500955, 0.33420956891495734, 0.3541478900006041, 0.33571887004654855, 0.3455190029926598, 0.3503502479288727, 0.34005355404224247, 0.48442754393909127, 0.33923114312347025, 0.3373927141074091, 0.3354944980237633, 0.42491093499120325, 0.3407576309982687, 0.3312757540261373, 0.34736836794763803, 0.353230744949542, 0.4013506709598005, 0.3686686819419265, 0.3617798910709098, 0.36296388402115554, 0.3572379780234769, 0.46997331315651536, 0.35908258706331253, 0.3606451980303973, 0.35679218592122197, 0.361537927063182, 0.35119260696228594, 0.35505662695504725, 0.36366706201806664, 0.3446365117561072, 0.34945836605038494, 0.35837860696483403, 0.3608768411213532, 0.3505430289078504, 0.3509398539317772, 0.3464535919483751, 0.35296940605621785, 0.3496358689153567, 0.3507593311369419, 0.35750082915183157, 0.3522787431720644, 0.34975187212694436, 0.362454065005295, 0.3568229250377044, 0.3346811970695853, 0.34068619296886027, 0.3638943750411272, 0.34734036796726286, 0.3625305249588564, 0.3570648030145094, 0.3543350809486583, 0.3532214560545981, 0.3594633130123839, 0.3464380159275606, 0.33697051007766277, 0.3702546670101583, 0.35093897010665387, 0.3376767879817635, 0.3488336850423366, 0.35411836102139205, 0.34821154503151774, 0.35503679094836116, 0.34980498626828194, 0.33544474304653704, 0.3539080349728465, 0.35434849304147065, 0.3464109528576955, 0.3587363390251994, 0.34577967086806893, 0.34682817792054266, 0.3490842329338193, 0.3548150500282645, 0.3406273761065677, 0.3527990289730951, 0.35109699703752995, 0.36546506208833307, 0.35949207306839526, 0.35268740786705166, 0.35818508092779666, 0.3557611360447481, 0.36519876192323864, 0.3563628940610215, 0.32839242892805487, 0.32636107108555734, 0.32749608193989843, 0.3441248619928956, 0.33973498409613967, 0.3425032049417496, 0.3408758119912818, 0.3519981330027804, 0.3572448140475899, 0.3452857331139967, 0.3414366989163682, 0.35797479294706136, 0.3441938469186425, 0.3470138590782881, 0.345897707156837, 0.3362433600705117, 0.3556297670584172, 0.33170307392720133, 0.3518869540421292, 0.34545445907860994, 0.34832695592194796]
Total Epoch List: [30, 101]
Total Time List: [0.07038785191252828, 0.08662328799255192]
========================k_idx:2========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: GPSTransformer
GPS(
  (x_lin): Linear(in_features=14887, out_features=64, bias=True)
  (pe_lin): Linear(in_features=20, out_features=8, bias=True)
  (pe_norm): BatchNorm1d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (convs): ModuleList(
    (0-1): 2 x GPSConv(64, conv=GINEConv(nn=Sequential(
      (0): Linear(in_features=64, out_features=64, bias=True)
      (1): ReLU()
      (2): Linear(in_features=64, out_features=64, bias=True)
    )), heads=1, attn_type=multihead)
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=32, bias=True)
    (1): ReLU()
    (2): Linear(in_features=32, out_features=16, bias=True)
    (3): ReLU()
    (4): Linear(in_features=16, out_features=2, bias=True)
  )
)
Total number of parameters: 1039906
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x74b5c4f92680>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.9542;  Loss pred: 0.9542; Loss self: 0.0000; time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.9725 score: 0.4898 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.9942 score: 0.5000 time: 0.06s
Epoch 2/1000, LR 0.000000
Train loss: 0.9511;  Loss pred: 0.9511; Loss self: 0.0000; time: 0.20s
Val loss: 0.6626 score: 0.6531 time: 0.06s
Test loss: 0.6637 score: 0.6458 time: 0.06s
Epoch 3/1000, LR 0.000030
Train loss: 0.8780;  Loss pred: 0.8780; Loss self: 0.0000; time: 0.20s
Val loss: 0.5942 score: 0.6327 time: 0.06s
Test loss: 0.6004 score: 0.6667 time: 0.06s
Epoch 4/1000, LR 0.000060
Train loss: 0.7360;  Loss pred: 0.7360; Loss self: 0.0000; time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6046 score: 0.5102 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6345 score: 0.5000 time: 0.06s
     INFO: Early stopping counter 1 of 20
Epoch 5/1000, LR 0.000090
Train loss: 0.5803;  Loss pred: 0.5803; Loss self: 0.0000; time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6599 score: 0.5102 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7088 score: 0.5000 time: 0.06s
     INFO: Early stopping counter 2 of 20
Epoch 6/1000, LR 0.000120
Train loss: 0.4870;  Loss pred: 0.4870; Loss self: 0.0000; time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7134 score: 0.5102 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7725 score: 0.5000 time: 0.06s
     INFO: Early stopping counter 3 of 20
Epoch 7/1000, LR 0.000150
Train loss: 0.4321;  Loss pred: 0.4321; Loss self: 0.0000; time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7426 score: 0.5102 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.8214 score: 0.5000 time: 0.06s
     INFO: Early stopping counter 4 of 20
Epoch 8/1000, LR 0.000180
Train loss: 0.3811;  Loss pred: 0.3811; Loss self: 0.0000; time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7439 score: 0.5102 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.8458 score: 0.5000 time: 0.06s
     INFO: Early stopping counter 5 of 20
Epoch 9/1000, LR 0.000210
Train loss: 0.3356;  Loss pred: 0.3356; Loss self: 0.0000; time: 0.19s
Val loss: 0.7243 score: 0.5306 time: 0.06s
Test loss: 0.8263 score: 0.5208 time: 0.06s
     INFO: Early stopping counter 6 of 20
Epoch 10/1000, LR 0.000240
Train loss: 0.2884;  Loss pred: 0.2884; Loss self: 0.0000; time: 0.20s
Val loss: 0.7058 score: 0.5510 time: 0.07s
Test loss: 0.8074 score: 0.5625 time: 0.07s
     INFO: Early stopping counter 7 of 20
Epoch 11/1000, LR 0.000270
Train loss: 0.2507;  Loss pred: 0.2507; Loss self: 0.0000; time: 0.20s
Val loss: 0.6915 score: 0.5306 time: 0.06s
Test loss: 0.7906 score: 0.5625 time: 0.06s
     INFO: Early stopping counter 8 of 20
Epoch 12/1000, LR 0.000270
Train loss: 0.2123;  Loss pred: 0.2123; Loss self: 0.0000; time: 0.19s
Val loss: 0.6805 score: 0.5306 time: 0.06s
Test loss: 0.7753 score: 0.5625 time: 0.06s
     INFO: Early stopping counter 9 of 20
Epoch 13/1000, LR 0.000270
Train loss: 0.1807;  Loss pred: 0.1807; Loss self: 0.0000; time: 0.19s
Val loss: 0.6702 score: 0.5510 time: 0.06s
Test loss: 0.7603 score: 0.5625 time: 0.06s
     INFO: Early stopping counter 10 of 20
Epoch 14/1000, LR 0.000270
Train loss: 0.1526;  Loss pred: 0.1526; Loss self: 0.0000; time: 0.20s
Val loss: 0.6604 score: 0.5714 time: 0.07s
Test loss: 0.7443 score: 0.5625 time: 0.06s
     INFO: Early stopping counter 11 of 20
Epoch 15/1000, LR 0.000270
Train loss: 0.1304;  Loss pred: 0.1304; Loss self: 0.0000; time: 0.19s
Val loss: 0.6519 score: 0.5714 time: 0.06s
Test loss: 0.7292 score: 0.5625 time: 0.06s
     INFO: Early stopping counter 12 of 20
Epoch 16/1000, LR 0.000270
Train loss: 0.1058;  Loss pred: 0.1058; Loss self: 0.0000; time: 0.19s
Val loss: 0.6428 score: 0.5714 time: 0.06s
Test loss: 0.7137 score: 0.5625 time: 0.06s
     INFO: Early stopping counter 13 of 20
Epoch 17/1000, LR 0.000270
Train loss: 0.0881;  Loss pred: 0.0881; Loss self: 0.0000; time: 0.19s
Val loss: 0.6336 score: 0.5714 time: 0.06s
Test loss: 0.6956 score: 0.6042 time: 0.06s
     INFO: Early stopping counter 14 of 20
Epoch 18/1000, LR 0.000270
Train loss: 0.0761;  Loss pred: 0.0761; Loss self: 0.0000; time: 0.19s
Val loss: 0.6262 score: 0.5714 time: 0.06s
Test loss: 0.6810 score: 0.6042 time: 0.06s
     INFO: Early stopping counter 15 of 20
Epoch 19/1000, LR 0.000270
Train loss: 0.0663;  Loss pred: 0.0663; Loss self: 0.0000; time: 0.19s
Val loss: 0.6212 score: 0.5714 time: 0.06s
Test loss: 0.6722 score: 0.6042 time: 0.06s
     INFO: Early stopping counter 16 of 20
Epoch 20/1000, LR 0.000270
Train loss: 0.0566;  Loss pred: 0.0566; Loss self: 0.0000; time: 0.19s
Val loss: 0.6169 score: 0.5714 time: 0.06s
Test loss: 0.6652 score: 0.6042 time: 0.06s
     INFO: Early stopping counter 17 of 20
Epoch 21/1000, LR 0.000270
Train loss: 0.0467;  Loss pred: 0.0467; Loss self: 0.0000; time: 0.19s
Val loss: 0.6137 score: 0.5714 time: 0.06s
Test loss: 0.6550 score: 0.6250 time: 0.06s
     INFO: Early stopping counter 18 of 20
Epoch 22/1000, LR 0.000270
Train loss: 0.0379;  Loss pred: 0.0379; Loss self: 0.0000; time: 0.19s
Val loss: 0.6108 score: 0.5714 time: 0.06s
Test loss: 0.6452 score: 0.6250 time: 0.06s
     INFO: Early stopping counter 19 of 20
Epoch 23/1000, LR 0.000270
Train loss: 0.0338;  Loss pred: 0.0338; Loss self: 0.0000; time: 0.19s
Val loss: 0.6080 score: 0.5714 time: 0.06s
Test loss: 0.6351 score: 0.6250 time: 0.07s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 002,   Train_Loss: 0.8780,   Val_Loss: 0.5942,   Val_Precision: 0.6129,   Val_Recall: 0.7600,   Val_accuracy: 0.6786,   Val_Score: 0.6327,   Val_Loss: 0.5942,   Test_Precision: 0.6429,   Test_Recall: 0.7500,   Test_accuracy: 0.6923,   Test_Score: 0.6667,   Test_loss: 0.6004


[0.06482695194426924, 0.06583771295845509, 0.0650867949007079, 0.06547505501657724, 0.0653332929359749, 0.06579327001236379, 0.06679044698830694, 0.06654995097778738, 0.06616380310151726, 0.07342366594821215, 0.06916214502416551, 0.0675493850139901, 0.06781924597453326, 0.06805131293367594, 0.07393869699444622, 0.07485074806027114, 0.06921576103195548, 0.0723197340266779, 0.0689182699425146, 0.06944285496138036, 0.06899320601951331, 0.07221619307529181, 0.06842620193492621, 0.06858358602039516, 0.06858222698792815, 0.06734950805548579, 0.06743598508182913, 0.0675579069647938, 0.06718390190508217, 0.06666443101130426, 0.07870153500698507, 0.07812334201298654, 0.08040917699690908, 0.08057387999724597, 0.08267879602499306, 0.09258812200278044, 0.08119916100986302, 0.08352010103408247, 0.0732206430984661, 0.15788360696751624, 0.07666597608476877, 0.07381408906076103, 0.08072974602691829, 0.0793804629938677, 0.08974163501989096, 0.0885987930232659, 0.08712012006435543, 0.0797891819383949, 0.09460132499225438, 0.07858771493192762, 0.08179382793605328, 0.08210221806075424, 0.07827191601973027, 0.07691160403192043, 0.07701224100310355, 0.07720945100300014, 0.08838088402990252, 0.0756651780102402, 0.07731703401077539, 0.08255784201901406, 0.08758679998572916, 0.08221058896742761, 0.08212011400610209, 0.08109443099237978, 0.0820831679739058, 0.08796141296625137, 0.08322881802450866, 0.07981455593835562, 0.08743783691897988, 0.0843059909529984, 0.07630958408117294, 0.08788257697597146, 0.07543317798990756, 0.08148276701103896, 0.08441786502953619, 0.07612562109716237, 0.08149443904403597, 0.08363239897880703, 0.07696750899776816, 0.07707213098183274, 0.07729208702221513, 0.07653446099720895, 0.07606860401574522, 0.08954264293424785, 0.08080121991224587, 0.07651120400987566, 0.08009913808200508, 0.08188897301442921, 0.0852567459223792, 0.07568250200711191, 0.08189619600307196, 0.07500909606460482, 0.0841889790026471, 0.08197807194665074, 0.08128126105293632, 0.08776152494829148, 0.08150951494462788, 0.08110297296661884, 0.07682529708836228, 0.0798028310528025, 0.08061097899917513, 0.08349049708340317, 0.07687000907026231, 0.08962043898645788, 0.081524264998734, 0.08876278495881706, 0.08875767001882195, 0.08295854600146413, 0.08094940904993564, 0.08594070910476148, 0.07272796693723649, 0.07246170507278293, 0.0730430850526318, 0.0784833460347727, 0.0783388769486919, 0.07796553091611713, 0.08462349709589034, 0.08056456199847162, 0.08009241498075426, 0.07874759996775538, 0.07818880898412317, 0.07925349497236311, 0.07387999701313674, 0.07923443405888975, 0.08463884796947241, 0.07417196698952466, 0.07981541205663234, 0.0733684339793399, 0.08512714901007712, 0.07973835803568363, 0.08606273995246738, 0.06843497196678072, 0.06899049505591393, 0.06916745495982468, 0.06889547395985574, 0.0687725730240345, 0.06903586303815246, 0.06934646994341165, 0.06898195203393698, 0.06912833300884813, 0.06979195994790643, 0.06901952298358083, 0.06871587107889354, 0.0693255600053817, 0.06894633395131677, 0.06855807499960065, 0.0693860569735989, 0.06909842696040869, 0.06923749402631074, 0.06933900201693177, 0.06898883904796094, 0.06944712705444545, 0.06919049005955458, 0.06947609991766512]
[0.0013229990192708007, 0.001343626795070512, 0.0013283019367491408, 0.0013362256125832091, 0.0013333325088974468, 0.0013427197961706895, 0.0013630703467001415, 0.0013581622648528035, 0.0013502816959493319, 0.0014984421622084112, 0.0014114723474319493, 0.0013785588778365328, 0.0013840662443782299, 0.001388802304768897, 0.0015089529998866574, 0.001527566286944309, 0.0014125665516725608, 0.0014759129393199573, 0.0014064953049492774, 0.0014172011216608237, 0.0014080246126431288, 0.0014737998586794247, 0.0013964531007127799, 0.001399665020824391, 0.0013996372854679215, 0.0013744797562344037, 0.0013762445935067169, 0.0013787327951998735, 0.0013711000388792278, 0.0013604985920674338, 0.0016061537756527566, 0.0015943539186323785, 0.001641003612181818, 0.001644364897902979, 0.0016873223678570014, 0.0018895535102608253, 0.0016571257348951636, 0.0017044918578384177, 0.001494298838744206, 0.003222114427908495, 0.0015646117568320157, 0.0015064099808318578, 0.0016475458372840468, 0.0016200094488544427, 0.0018314619391814483, 0.0018081386331278756, 0.0017779616339664375, 0.0016283506518039777, 0.0019306392855562118, 0.0016038309169781146, 0.0016692617946133322, 0.0016755554706276376, 0.0015973860412189852, 0.0015696245720800087, 0.0015716783878184399, 0.0015757030816938803, 0.001803691510814337, 0.0015441873063314327, 0.0015778986532811303, 0.001684853918755389, 0.0017874857139944726, 0.001677767121784237, 0.0016759206940020835, 0.0016549883875995874, 0.0016751666933450165, 0.001795130876862273, 0.0016985473066226256, 0.00162886848853787, 0.0017844456514077528, 0.0017205304276122122, 0.0015573384506361826, 0.0017935219791014583, 0.00153945261203893, 0.0016629136124701829, 0.0017228135720313508, 0.0015535841040237217, 0.001663151817225224, 0.0017067836526287149, 0.0015707654897503707, 0.0015729006322823009, 0.0015773895310656149, 0.001561927775453244, 0.0015524204901172494, 0.0018274008762091398, 0.0016490044880050178, 0.0015614531430586868, 0.0016346762873878588, 0.0016712035309067186, 0.0017399335902526366, 0.001544540857287998, 0.0016713509388382034, 0.001530797878869486, 0.001718142428625451, 0.00167302187646226, 0.001658801245978292, 0.001791051529556969, 0.001663459488665875, 0.001655162713604466, 0.0015678632058849444, 0.0016286292051592348, 0.0016451220203913292, 0.0017038876955796564, 0.0015687756953114758, 0.0018289885507440384, 0.001663760510178245, 0.0018114854073227973, 0.0018113810207922847, 0.0016930315510502883, 0.0016520287561211356, 0.0017538920225461526, 0.001484244223208908, 0.001478810307607815, 0.001490675205155751, 0.001601700939485157, 0.0015987525907896307, 0.0015911332840023904, 0.0017270101448140886, 0.0016441747346626861, 0.0016345390812398828, 0.0016070938768929669, 0.0015956899792678198, 0.0016174182647421043, 0.0015077550410844234, 0.001617029266507954, 0.0017273234279484165, 0.0015137136120311155, 0.0016288859603394354, 0.001497314979170202, 0.0017372887553076964, 0.001627313429299666, 0.0017563824480095384, 0.0014257285826412651, 0.00143730198033154, 0.0014409886449963476, 0.0014353223741636612, 0.0014327619380007188, 0.001438247146628176, 0.001444718123821076, 0.0014371240007070203, 0.0014401736043510027, 0.001453999165581384, 0.0014379067288246006, 0.0014315806474769488, 0.001444282500112119, 0.0014363819573190995, 0.0014282932291583468, 0.0014455428536166437, 0.001439550561675181, 0.0014424477922148071, 0.001444562542019412, 0.001437267480165853, 0.0014468151469676134, 0.001441468542907387, 0.00144741874828469]
[755.8584590268036, 744.2542852440816, 752.8408807769864, 748.3766143853408, 750.0004637454729, 744.7570244007021, 733.6378510624202, 736.2890472504617, 740.5862073076078, 667.3597588352661, 708.4800505085436, 725.3952051503007, 722.5087701269923, 720.0448880061476, 662.7111646784979, 654.6360760555704, 707.9312467196268, 677.5467396205366, 710.9870871812566, 705.6161505348626, 710.2148577664497, 678.5181814958473, 716.0999531524392, 714.4566629313979, 714.4708206781472, 727.5480016814886, 726.6150252056335, 725.3037016900951, 729.3413840301729, 735.0246489269681, 622.6053913135374, 627.2133108675083, 609.3831802542085, 608.1375254818911, 592.6549775251655, 529.2255522639126, 603.4545109899365, 586.685114042239, 669.2101834465656, 310.3552100255823, 639.1361918593616, 663.8299086731939, 606.9633860072046, 617.2803502517407, 546.0118927980228, 553.0549381991319, 562.44183276841, 614.1183404766929, 517.9631469645055, 623.5071224865568, 599.0672063704902, 596.8170063778404, 626.0227485379098, 637.0950211838469, 636.2624871288361, 634.6373321330312, 554.4185322181377, 647.5898331114552, 633.7542642048459, 593.5232656482798, 559.445030620867, 596.0302756061525, 596.6869456167458, 604.233846891464, 596.9555173062652, 557.0624475848301, 588.7383860908708, 613.923104926436, 560.3981265616568, 581.2161086786595, 642.1211777000008, 557.5621663142332, 649.5815409839403, 601.3541488270971, 580.4458568438782, 643.6729092490322, 601.2680199384228, 585.8973388102487, 636.6322703963416, 635.7680704527316, 633.9588163263921, 640.2344690424738, 644.1553730873992, 547.2253039926601, 606.4264877834323, 640.4290800818576, 611.7419135001684, 598.3711627616331, 574.7345792978231, 647.4415974698545, 598.3183882943963, 653.2541061126325, 582.0239249897463, 597.7208152917766, 602.8449776152908, 558.3312280509082, 601.1568101379002, 604.170207424676, 637.8107453804128, 614.0133044600706, 607.8576467915295, 586.8931400785799, 637.439758270511, 546.7502787774132, 601.0480438034115, 552.0331524380892, 552.064965085373, 590.6564466442699, 605.3163398607783, 570.160527070694, 673.7435688568953, 676.2192519591249, 670.8369445881516, 624.3362761099679, 625.4876494092783, 628.4828619036654, 579.0353942058917, 608.2078619248197, 611.793264215774, 622.2411860179095, 626.6881493226201, 618.2692639244116, 663.237709542506, 618.4179969479118, 578.9303750645726, 660.6269455806705, 613.9165198474759, 667.8621491880023, 575.6095507697493, 614.509769289105, 569.3520799716904, 701.3957720812666, 695.7480151591607, 693.967994454623, 696.7075954505925, 697.9526559697724, 695.2907936194403, 692.1765453839125, 695.8341795892568, 694.3607333024537, 687.7583039053178, 695.4554005164423, 698.5285822090592, 692.3853193003242, 696.193651629004, 700.136344264036, 691.781635873383, 694.6612551325163, 693.2659922925526, 692.2510939554475, 695.7647158931095, 691.1733002629289, 693.7369566060999, 690.8850677698365]
Elapsed: 0.07723165547033747~0.00959810069030471
Time per graph: 0.0015805439738568693~0.0001924115586046704
Speed: 639.926133275123~62.84080593171394
Total Time: 0.0700
best val loss: 0.5941584706306458 test_score: 0.6667

Testing...
Test loss: 0.6637 score: 0.6458 time: 0.06s
test Score 0.6458
Epoch Time List: [0.4705711400602013, 0.321695368969813, 0.3223287450382486, 0.32363016181625426, 0.3219838390359655, 0.323124471004121, 0.32558802887797356, 0.33685650711413473, 0.32541257492266595, 0.35035461094230413, 0.34347808407619596, 0.3387586160097271, 0.3440129399532452, 0.3394425930455327, 0.3477822410641238, 0.3456348628969863, 0.34576276200823486, 0.34389086987357587, 0.34064382791984826, 0.3528313289862126, 0.35252513096202165, 0.35730106104165316, 0.34459661005530506, 0.34752377786207944, 0.3609588759718463, 0.3386324670864269, 0.33904182293917984, 0.3475183689733967, 0.34126539004500955, 0.33420956891495734, 0.3541478900006041, 0.33571887004654855, 0.3455190029926598, 0.3503502479288727, 0.34005355404224247, 0.48442754393909127, 0.33923114312347025, 0.3373927141074091, 0.3354944980237633, 0.42491093499120325, 0.3407576309982687, 0.3312757540261373, 0.34736836794763803, 0.353230744949542, 0.4013506709598005, 0.3686686819419265, 0.3617798910709098, 0.36296388402115554, 0.3572379780234769, 0.46997331315651536, 0.35908258706331253, 0.3606451980303973, 0.35679218592122197, 0.361537927063182, 0.35119260696228594, 0.35505662695504725, 0.36366706201806664, 0.3446365117561072, 0.34945836605038494, 0.35837860696483403, 0.3608768411213532, 0.3505430289078504, 0.3509398539317772, 0.3464535919483751, 0.35296940605621785, 0.3496358689153567, 0.3507593311369419, 0.35750082915183157, 0.3522787431720644, 0.34975187212694436, 0.362454065005295, 0.3568229250377044, 0.3346811970695853, 0.34068619296886027, 0.3638943750411272, 0.34734036796726286, 0.3625305249588564, 0.3570648030145094, 0.3543350809486583, 0.3532214560545981, 0.3594633130123839, 0.3464380159275606, 0.33697051007766277, 0.3702546670101583, 0.35093897010665387, 0.3376767879817635, 0.3488336850423366, 0.35411836102139205, 0.34821154503151774, 0.35503679094836116, 0.34980498626828194, 0.33544474304653704, 0.3539080349728465, 0.35434849304147065, 0.3464109528576955, 0.3587363390251994, 0.34577967086806893, 0.34682817792054266, 0.3490842329338193, 0.3548150500282645, 0.3406273761065677, 0.3527990289730951, 0.35109699703752995, 0.36546506208833307, 0.35949207306839526, 0.35268740786705166, 0.35818508092779666, 0.3557611360447481, 0.36519876192323864, 0.3563628940610215, 0.32839242892805487, 0.32636107108555734, 0.32749608193989843, 0.3441248619928956, 0.33973498409613967, 0.3425032049417496, 0.3408758119912818, 0.3519981330027804, 0.3572448140475899, 0.3452857331139967, 0.3414366989163682, 0.35797479294706136, 0.3441938469186425, 0.3470138590782881, 0.345897707156837, 0.3362433600705117, 0.3556297670584172, 0.33170307392720133, 0.3518869540421292, 0.34545445907860994, 0.34832695592194796, 0.33122365490999073, 0.32584337901789695, 0.32636784901842475, 0.32444785302504897, 0.32460397598333657, 0.3237223830074072, 0.32223294279538095, 0.32524060015566647, 0.32316299388185143, 0.33154462999664247, 0.3303005949128419, 0.3219534900272265, 0.3236190869938582, 0.327676368993707, 0.32193963089957833, 0.3223924250341952, 0.32248240313492715, 0.32318991201464087, 0.3226574051659554, 0.32390313979703933, 0.32333134894724935, 0.32309351686853915, 0.3234576629474759]
Total Epoch List: [30, 101, 23]
Total Time List: [0.07038785191252828, 0.08662328799255192, 0.07004597003106028]
========================training times:1========================
========================k_idx:0========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: GPSTransformer
GPS(
  (x_lin): Linear(in_features=14887, out_features=64, bias=True)
  (pe_lin): Linear(in_features=20, out_features=8, bias=True)
  (pe_norm): BatchNorm1d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (convs): ModuleList(
    (0-1): 2 x GPSConv(64, conv=GINEConv(nn=Sequential(
      (0): Linear(in_features=64, out_features=64, bias=True)
      (1): ReLU()
      (2): Linear(in_features=64, out_features=64, bias=True)
    )), heads=1, attn_type=multihead)
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=32, bias=True)
    (1): ReLU()
    (2): Linear(in_features=32, out_features=16, bias=True)
    (3): ReLU()
    (4): Linear(in_features=16, out_features=2, bias=True)
  )
)
Total number of parameters: 1039906
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x74b5c4f93610>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.8224;  Loss pred: 0.8224; Loss self: 0.0000; time: 0.19s
Val loss: 0.9318 score: 0.5306 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.8357 score: 0.4898 time: 0.06s
Epoch 2/1000, LR 0.000000
Train loss: 0.8368;  Loss pred: 0.8368; Loss self: 0.0000; time: 0.19s
Val loss: 0.7814 score: 0.5306 time: 0.07s
Test loss: 0.7494 score: 0.5306 time: 0.06s
Epoch 3/1000, LR 0.000030
Train loss: 0.7608;  Loss pred: 0.7608; Loss self: 0.0000; time: 0.19s
Val loss: 0.7352 score: 0.5510 time: 0.07s
Test loss: 0.7223 score: 0.5102 time: 0.06s
Epoch 4/1000, LR 0.000060
Train loss: 0.6432;  Loss pred: 0.6432; Loss self: 0.0000; time: 0.19s
Val loss: 0.7170 score: 0.5918 time: 0.07s
Test loss: 0.7107 score: 0.5306 time: 0.06s
Epoch 5/1000, LR 0.000090
Train loss: 0.5458;  Loss pred: 0.5458; Loss self: 0.0000; time: 0.19s
Val loss: 0.7018 score: 0.5918 time: 0.07s
Test loss: 0.7056 score: 0.5510 time: 0.06s
Epoch 6/1000, LR 0.000120
Train loss: 0.4426;  Loss pred: 0.4426; Loss self: 0.0000; time: 0.19s
Val loss: 0.6916 score: 0.5918 time: 0.07s
Test loss: 0.7101 score: 0.5714 time: 0.06s
Epoch 7/1000, LR 0.000150
Train loss: 0.3628;  Loss pred: 0.3628; Loss self: 0.0000; time: 0.19s
Val loss: 0.6940 score: 0.6327 time: 0.07s
Test loss: 0.7274 score: 0.5510 time: 0.06s
     INFO: Early stopping counter 1 of 20
Epoch 8/1000, LR 0.000180
Train loss: 0.3060;  Loss pred: 0.3060; Loss self: 0.0000; time: 0.20s
Val loss: 0.7100 score: 0.6122 time: 0.07s
Test loss: 0.7452 score: 0.5714 time: 0.06s
     INFO: Early stopping counter 2 of 20
Epoch 9/1000, LR 0.000210
Train loss: 0.2550;  Loss pred: 0.2550; Loss self: 0.0000; time: 0.20s
Val loss: 0.7305 score: 0.5918 time: 0.07s
Test loss: 0.7600 score: 0.5510 time: 0.06s
     INFO: Early stopping counter 3 of 20
Epoch 10/1000, LR 0.000240
Train loss: 0.2318;  Loss pred: 0.2318; Loss self: 0.0000; time: 0.19s
Val loss: 0.7559 score: 0.5918 time: 0.07s
Test loss: 0.7701 score: 0.5510 time: 0.06s
     INFO: Early stopping counter 4 of 20
Epoch 11/1000, LR 0.000270
Train loss: 0.1884;  Loss pred: 0.1884; Loss self: 0.0000; time: 0.19s
Val loss: 0.7824 score: 0.5714 time: 0.07s
Test loss: 0.7778 score: 0.5510 time: 0.06s
     INFO: Early stopping counter 5 of 20
Epoch 12/1000, LR 0.000270
Train loss: 0.1571;  Loss pred: 0.1571; Loss self: 0.0000; time: 0.20s
Val loss: 0.8046 score: 0.5306 time: 0.07s
Test loss: 0.7856 score: 0.5306 time: 0.06s
     INFO: Early stopping counter 6 of 20
Epoch 13/1000, LR 0.000270
Train loss: 0.1411;  Loss pred: 0.1411; Loss self: 0.0000; time: 0.21s
Val loss: 0.8228 score: 0.5306 time: 0.07s
Test loss: 0.7927 score: 0.5306 time: 0.06s
     INFO: Early stopping counter 7 of 20
Epoch 14/1000, LR 0.000270
Train loss: 0.1256;  Loss pred: 0.1256; Loss self: 0.0000; time: 0.19s
Val loss: 0.8359 score: 0.5102 time: 0.07s
Test loss: 0.7984 score: 0.5306 time: 0.06s
     INFO: Early stopping counter 8 of 20
Epoch 15/1000, LR 0.000270
Train loss: 0.1099;  Loss pred: 0.1099; Loss self: 0.0000; time: 0.19s
Val loss: 0.8456 score: 0.5306 time: 0.07s
Test loss: 0.8012 score: 0.5102 time: 0.06s
     INFO: Early stopping counter 9 of 20
Epoch 16/1000, LR 0.000270
Train loss: 0.0897;  Loss pred: 0.0897; Loss self: 0.0000; time: 0.19s
Val loss: 0.8514 score: 0.5306 time: 0.07s
Test loss: 0.8012 score: 0.5102 time: 0.06s
     INFO: Early stopping counter 10 of 20
Epoch 17/1000, LR 0.000270
Train loss: 0.0828;  Loss pred: 0.0828; Loss self: 0.0000; time: 0.20s
Val loss: 0.8565 score: 0.5306 time: 0.07s
Test loss: 0.8009 score: 0.5102 time: 0.06s
     INFO: Early stopping counter 11 of 20
Epoch 18/1000, LR 0.000270
Train loss: 0.0731;  Loss pred: 0.0731; Loss self: 0.0000; time: 0.19s
Val loss: 0.8613 score: 0.5306 time: 0.07s
Test loss: 0.7992 score: 0.4898 time: 0.06s
     INFO: Early stopping counter 12 of 20
Epoch 19/1000, LR 0.000270
Train loss: 0.0595;  Loss pred: 0.0595; Loss self: 0.0000; time: 0.19s
Val loss: 0.8635 score: 0.5306 time: 0.07s
Test loss: 0.7944 score: 0.4898 time: 0.06s
     INFO: Early stopping counter 13 of 20
Epoch 20/1000, LR 0.000270
Train loss: 0.0556;  Loss pred: 0.0556; Loss self: 0.0000; time: 0.19s
Val loss: 0.8592 score: 0.5306 time: 0.07s
Test loss: 0.7879 score: 0.5102 time: 0.06s
     INFO: Early stopping counter 14 of 20
Epoch 21/1000, LR 0.000270
Train loss: 0.0568;  Loss pred: 0.0568; Loss self: 0.0000; time: 0.20s
Val loss: 0.8469 score: 0.5306 time: 0.07s
Test loss: 0.7791 score: 0.4898 time: 0.06s
     INFO: Early stopping counter 15 of 20
Epoch 22/1000, LR 0.000270
Train loss: 0.0473;  Loss pred: 0.0473; Loss self: 0.0000; time: 0.19s
Val loss: 0.8295 score: 0.5306 time: 0.07s
Test loss: 0.7686 score: 0.4898 time: 0.06s
     INFO: Early stopping counter 16 of 20
Epoch 23/1000, LR 0.000270
Train loss: 0.0406;  Loss pred: 0.0406; Loss self: 0.0000; time: 0.20s
Val loss: 0.8096 score: 0.5510 time: 0.07s
Test loss: 0.7566 score: 0.5306 time: 0.06s
     INFO: Early stopping counter 17 of 20
Epoch 24/1000, LR 0.000270
Train loss: 0.0376;  Loss pred: 0.0376; Loss self: 0.0000; time: 0.20s
Val loss: 0.7927 score: 0.5918 time: 0.07s
Test loss: 0.7439 score: 0.5306 time: 0.06s
     INFO: Early stopping counter 18 of 20
Epoch 25/1000, LR 0.000270
Train loss: 0.0337;  Loss pred: 0.0337; Loss self: 0.0000; time: 0.19s
Val loss: 0.7778 score: 0.5918 time: 0.07s
Test loss: 0.7330 score: 0.5714 time: 0.06s
     INFO: Early stopping counter 19 of 20
Epoch 26/1000, LR 0.000270
Train loss: 0.0307;  Loss pred: 0.0307; Loss self: 0.0000; time: 0.20s
Val loss: 0.7648 score: 0.5714 time: 0.07s
Test loss: 0.7231 score: 0.6122 time: 0.06s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 005,   Train_Loss: 0.4426,   Val_Loss: 0.6916,   Val_Precision: 0.6429,   Val_Recall: 0.3750,   Val_accuracy: 0.4737,   Val_Score: 0.5918,   Val_Loss: 0.6916,   Test_Precision: 0.7000,   Test_Recall: 0.2800,   Test_accuracy: 0.4000,   Test_Score: 0.5714,   Test_loss: 0.7101


[0.06536245299503207, 0.06498479307629168, 0.06517278694082052, 0.0655033509247005, 0.06540117296390235, 0.06527594395447522, 0.0652049119817093, 0.06538243195973337, 0.06516556406859308, 0.06561034999322146, 0.06535083602648228, 0.06570164498407394, 0.06521763594355434, 0.0657589309848845, 0.06525927397888154, 0.0654908720171079, 0.06568012409843504, 0.06520061905030161, 0.0653998430352658, 0.06531489407643676, 0.06531110405921936, 0.06577758700586855, 0.06565138604491949, 0.06502032198477536, 0.06543521804269403, 0.06494482897687703]
[0.0013339276121435116, 0.0013262202668630956, 0.0013300568763432758, 0.0013368030800959285, 0.001334717815589844, 0.0013321621215199024, 0.0013307124894226389, 0.0013343353461170075, 0.001329909470787614, 0.00133898673455554, 0.0013336905311526997, 0.001340849897634162, 0.001330972162113354, 0.0013420189996915205, 0.001331821917936358, 0.001336548408512406, 0.0013404106958864294, 0.0013306248785775838, 0.0013346906741890981, 0.0013329570219680971, 0.001332879674677946, 0.001342399734813644, 0.001339824204998357, 0.0013269453466280686, 0.0013354126131162047, 0.0013254046729974905]
[749.6658670953534, 754.0225594390114, 751.8475471133979, 748.0533332764616, 749.2220365381696, 750.6593858554324, 751.4771281915854, 749.436791066097, 751.9308809852814, 746.8333884068968, 749.7991300393404, 745.7956343692398, 751.3305149915177, 745.14593327655, 750.8511359758128, 748.195870520703, 746.0400033130802, 751.5266068593153, 749.2372722298056, 750.2117349016328, 750.2552698476871, 744.9345929279575, 746.3665727708109, 753.6105405856564, 748.8322262184461, 754.4865506912947]
Elapsed: 0.06536841842954835~0.00022688658095022655
Time per graph: 0.0013340493557050686~4.63033838673931e-06
Speed: 749.6064810571744~2.6009285410829235
Total Time: 0.0652
best val loss: 0.6915860176086426 test_score: 0.5714

Testing...
Test loss: 0.7274 score: 0.5510 time: 0.06s
test Score 0.5510
Epoch Time List: [0.3231512511847541, 0.32157175603788346, 0.32225958607159555, 0.3230605380376801, 0.321877604117617, 0.32218650286085904, 0.3213745200773701, 0.32415115099865943, 0.32777362992055714, 0.32252413290552795, 0.32294920191634446, 0.3258672798983753, 0.3343243838753551, 0.3227181170368567, 0.3214159399503842, 0.32817996200174093, 0.32379537587985396, 0.3227470818674192, 0.3221816929290071, 0.3228515909286216, 0.32357651996426284, 0.3286313018761575, 0.32418785407207906, 0.3229531500255689, 0.3227580130333081, 0.3291280370904133]
Total Epoch List: [26]
Total Time List: [0.06523587100673467]
========================k_idx:1========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: GPSTransformer
GPS(
  (x_lin): Linear(in_features=14887, out_features=64, bias=True)
  (pe_lin): Linear(in_features=20, out_features=8, bias=True)
  (pe_norm): BatchNorm1d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (convs): ModuleList(
    (0-1): 2 x GPSConv(64, conv=GINEConv(nn=Sequential(
      (0): Linear(in_features=64, out_features=64, bias=True)
      (1): ReLU()
      (2): Linear(in_features=64, out_features=64, bias=True)
    )), heads=1, attn_type=multihead)
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=32, bias=True)
    (1): ReLU()
    (2): Linear(in_features=32, out_features=16, bias=True)
    (3): ReLU()
    (4): Linear(in_features=16, out_features=2, bias=True)
  )
)
Total number of parameters: 1039906
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x74b5c4f93280>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.7934;  Loss pred: 0.7934; Loss self: 0.0000; time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 1.2759 score: 0.5102 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 1.2259 score: 0.4898 time: 0.07s
Epoch 2/1000, LR 0.000000
Train loss: 0.8051;  Loss pred: 0.8051; Loss self: 0.0000; time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 1.1549 score: 0.5102 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 1.1237 score: 0.4898 time: 0.07s
Epoch 3/1000, LR 0.000030
Train loss: 0.7473;  Loss pred: 0.7473; Loss self: 0.0000; time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 1.1368 score: 0.5102 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 1.1051 score: 0.4898 time: 0.07s
Epoch 4/1000, LR 0.000060
Train loss: 0.6200;  Loss pred: 0.6200; Loss self: 0.0000; time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 1.1409 score: 0.5102 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 1.0942 score: 0.4898 time: 0.07s
     INFO: Early stopping counter 1 of 20
Epoch 5/1000, LR 0.000090
Train loss: 0.5334;  Loss pred: 0.5334; Loss self: 0.0000; time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 1.1358 score: 0.5102 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 1.0885 score: 0.4898 time: 0.07s
Epoch 6/1000, LR 0.000120
Train loss: 0.4595;  Loss pred: 0.4595; Loss self: 0.0000; time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 1.1257 score: 0.5102 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 1.0800 score: 0.4898 time: 0.07s
Epoch 7/1000, LR 0.000150
Train loss: 0.3874;  Loss pred: 0.3874; Loss self: 0.0000; time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 1.1037 score: 0.5102 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 1.0586 score: 0.4898 time: 0.07s
Epoch 8/1000, LR 0.000180
Train loss: 0.3181;  Loss pred: 0.3181; Loss self: 0.0000; time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 1.0724 score: 0.5102 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 1.0255 score: 0.4898 time: 0.07s
Epoch 9/1000, LR 0.000210
Train loss: 0.2656;  Loss pred: 0.2656; Loss self: 0.0000; time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 1.0356 score: 0.5102 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.9851 score: 0.4898 time: 0.17s
Epoch 10/1000, LR 0.000240
Train loss: 0.2175;  Loss pred: 0.2175; Loss self: 0.0000; time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.9927 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.9402 score: 0.4898 time: 0.08s
Epoch 11/1000, LR 0.000270
Train loss: 0.1735;  Loss pred: 0.1735; Loss self: 0.0000; time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.9455 score: 0.5102 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.8953 score: 0.4898 time: 0.08s
Epoch 12/1000, LR 0.000270
Train loss: 0.1347;  Loss pred: 0.1347; Loss self: 0.0000; time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.9036 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.8516 score: 0.4898 time: 0.08s
Epoch 13/1000, LR 0.000270
Train loss: 0.1040;  Loss pred: 0.1040; Loss self: 0.0000; time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.8655 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.8091 score: 0.4898 time: 0.08s
Epoch 14/1000, LR 0.000270
Train loss: 0.0815;  Loss pred: 0.0815; Loss self: 0.0000; time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.8275 score: 0.5102 time: 0.10s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7662 score: 0.4898 time: 0.07s
Epoch 15/1000, LR 0.000270
Train loss: 0.0609;  Loss pred: 0.0609; Loss self: 0.0000; time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7926 score: 0.5102 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7277 score: 0.4898 time: 0.08s
Epoch 16/1000, LR 0.000270
Train loss: 0.0517;  Loss pred: 0.0517; Loss self: 0.0000; time: 0.21s
Val loss: 0.7616 score: 0.5102 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6929 score: 0.4898 time: 0.07s
Epoch 17/1000, LR 0.000270
Train loss: 0.0376;  Loss pred: 0.0376; Loss self: 0.0000; time: 0.21s
Val loss: 0.7342 score: 0.5306 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6618 score: 0.4898 time: 0.07s
Epoch 18/1000, LR 0.000270
Train loss: 0.0306;  Loss pred: 0.0306; Loss self: 0.0000; time: 0.21s
Val loss: 0.7114 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6328 score: 0.4898 time: 0.08s
Epoch 19/1000, LR 0.000270
Train loss: 0.0261;  Loss pred: 0.0261; Loss self: 0.0000; time: 0.21s
Val loss: 0.6918 score: 0.5306 time: 0.07s
Test loss: 0.6084 score: 0.5102 time: 0.16s
Epoch 20/1000, LR 0.000270
Train loss: 0.0218;  Loss pred: 0.0218; Loss self: 0.0000; time: 0.21s
Val loss: 0.6735 score: 0.5510 time: 0.07s
Test loss: 0.5879 score: 0.5102 time: 0.07s
Epoch 21/1000, LR 0.000270
Train loss: 0.0172;  Loss pred: 0.0172; Loss self: 0.0000; time: 0.21s
Val loss: 0.6570 score: 0.5306 time: 0.07s
Test loss: 0.5725 score: 0.4898 time: 0.07s
Epoch 22/1000, LR 0.000270
Train loss: 0.0147;  Loss pred: 0.0147; Loss self: 0.0000; time: 0.22s
Val loss: 0.6437 score: 0.5510 time: 0.07s
Test loss: 0.5610 score: 0.5510 time: 0.07s
Epoch 23/1000, LR 0.000270
Train loss: 0.0140;  Loss pred: 0.0140; Loss self: 0.0000; time: 0.21s
Val loss: 0.6335 score: 0.5714 time: 0.07s
Test loss: 0.5539 score: 0.5510 time: 0.08s
Epoch 24/1000, LR 0.000270
Train loss: 0.0112;  Loss pred: 0.0112; Loss self: 0.0000; time: 0.22s
Val loss: 0.6245 score: 0.6122 time: 0.07s
Test loss: 0.5475 score: 0.5714 time: 0.09s
Epoch 25/1000, LR 0.000270
Train loss: 0.0093;  Loss pred: 0.0093; Loss self: 0.0000; time: 0.28s
Val loss: 0.6169 score: 0.6327 time: 0.07s
Test loss: 0.5413 score: 0.5918 time: 0.08s
Epoch 26/1000, LR 0.000270
Train loss: 0.0088;  Loss pred: 0.0088; Loss self: 0.0000; time: 0.21s
Val loss: 0.6114 score: 0.6122 time: 0.07s
Test loss: 0.5364 score: 0.5918 time: 0.08s
Epoch 27/1000, LR 0.000270
Train loss: 0.0072;  Loss pred: 0.0072; Loss self: 0.0000; time: 0.21s
Val loss: 0.6066 score: 0.6735 time: 0.07s
Test loss: 0.5327 score: 0.6122 time: 0.08s
Epoch 28/1000, LR 0.000270
Train loss: 0.0070;  Loss pred: 0.0070; Loss self: 0.0000; time: 0.21s
Val loss: 0.6037 score: 0.6531 time: 0.07s
Test loss: 0.5295 score: 0.6122 time: 0.07s
Epoch 29/1000, LR 0.000270
Train loss: 0.0068;  Loss pred: 0.0068; Loss self: 0.0000; time: 0.21s
Val loss: 0.6020 score: 0.6531 time: 0.08s
Test loss: 0.5264 score: 0.6327 time: 0.10s
Epoch 30/1000, LR 0.000270
Train loss: 0.0058;  Loss pred: 0.0058; Loss self: 0.0000; time: 0.20s
Val loss: 0.6007 score: 0.6531 time: 0.06s
Test loss: 0.5237 score: 0.6735 time: 0.07s
Epoch 31/1000, LR 0.000270
Train loss: 0.0046;  Loss pred: 0.0046; Loss self: 0.0000; time: 0.20s
Val loss: 0.5999 score: 0.6327 time: 0.06s
Test loss: 0.5209 score: 0.6939 time: 0.07s
Epoch 32/1000, LR 0.000270
Train loss: 0.0051;  Loss pred: 0.0051; Loss self: 0.0000; time: 0.20s
Val loss: 0.5997 score: 0.6327 time: 0.07s
Test loss: 0.5191 score: 0.7143 time: 0.07s
Epoch 33/1000, LR 0.000270
Train loss: 0.0043;  Loss pred: 0.0043; Loss self: 0.0000; time: 0.20s
Val loss: 0.5996 score: 0.6327 time: 0.06s
Test loss: 0.5179 score: 0.7143 time: 0.07s
Epoch 34/1000, LR 0.000270
Train loss: 0.0041;  Loss pred: 0.0041; Loss self: 0.0000; time: 0.20s
Val loss: 0.5998 score: 0.6327 time: 0.06s
Test loss: 0.5165 score: 0.7347 time: 0.07s
     INFO: Early stopping counter 1 of 20
Epoch 35/1000, LR 0.000270
Train loss: 0.0038;  Loss pred: 0.0038; Loss self: 0.0000; time: 0.20s
Val loss: 0.6009 score: 0.6939 time: 0.06s
Test loss: 0.5145 score: 0.7551 time: 0.07s
     INFO: Early stopping counter 2 of 20
Epoch 36/1000, LR 0.000270
Train loss: 0.0033;  Loss pred: 0.0033; Loss self: 0.0000; time: 0.20s
Val loss: 0.6029 score: 0.7551 time: 0.06s
Test loss: 0.5132 score: 0.7347 time: 0.07s
     INFO: Early stopping counter 3 of 20
Epoch 37/1000, LR 0.000270
Train loss: 0.0030;  Loss pred: 0.0030; Loss self: 0.0000; time: 0.20s
Val loss: 0.6045 score: 0.7347 time: 0.06s
Test loss: 0.5126 score: 0.7143 time: 0.07s
     INFO: Early stopping counter 4 of 20
Epoch 38/1000, LR 0.000270
Train loss: 0.0031;  Loss pred: 0.0031; Loss self: 0.0000; time: 0.20s
Val loss: 0.6056 score: 0.6939 time: 0.06s
Test loss: 0.5120 score: 0.7347 time: 0.07s
     INFO: Early stopping counter 5 of 20
Epoch 39/1000, LR 0.000269
Train loss: 0.0028;  Loss pred: 0.0028; Loss self: 0.0000; time: 0.21s
Val loss: 0.6067 score: 0.6735 time: 0.06s
Test loss: 0.5116 score: 0.7551 time: 0.07s
     INFO: Early stopping counter 6 of 20
Epoch 40/1000, LR 0.000269
Train loss: 0.0029;  Loss pred: 0.0029; Loss self: 0.0000; time: 0.20s
Val loss: 0.6092 score: 0.6939 time: 0.06s
Test loss: 0.5120 score: 0.7551 time: 0.07s
     INFO: Early stopping counter 7 of 20
Epoch 41/1000, LR 0.000269
Train loss: 0.0023;  Loss pred: 0.0023; Loss self: 0.0000; time: 0.20s
Val loss: 0.6123 score: 0.7143 time: 0.06s
Test loss: 0.5125 score: 0.7551 time: 0.07s
     INFO: Early stopping counter 8 of 20
Epoch 42/1000, LR 0.000269
Train loss: 0.0023;  Loss pred: 0.0023; Loss self: 0.0000; time: 0.20s
Val loss: 0.6169 score: 0.6939 time: 0.06s
Test loss: 0.5144 score: 0.7755 time: 0.07s
     INFO: Early stopping counter 9 of 20
Epoch 43/1000, LR 0.000269
Train loss: 0.0022;  Loss pred: 0.0022; Loss self: 0.0000; time: 0.19s
Val loss: 0.6218 score: 0.6735 time: 0.06s
Test loss: 0.5155 score: 0.7959 time: 0.07s
     INFO: Early stopping counter 10 of 20
Epoch 44/1000, LR 0.000269
Train loss: 0.0027;  Loss pred: 0.0027; Loss self: 0.0000; time: 0.20s
Val loss: 0.6225 score: 0.6939 time: 0.06s
Test loss: 0.5112 score: 0.8163 time: 0.07s
     INFO: Early stopping counter 11 of 20
Epoch 45/1000, LR 0.000269
Train loss: 0.0024;  Loss pred: 0.0024; Loss self: 0.0000; time: 0.20s
Val loss: 0.6215 score: 0.6939 time: 0.07s
Test loss: 0.5046 score: 0.8367 time: 0.07s
     INFO: Early stopping counter 12 of 20
Epoch 46/1000, LR 0.000269
Train loss: 0.0020;  Loss pred: 0.0020; Loss self: 0.0000; time: 0.21s
Val loss: 0.6191 score: 0.7143 time: 0.07s
Test loss: 0.4967 score: 0.8367 time: 0.07s
     INFO: Early stopping counter 13 of 20
Epoch 47/1000, LR 0.000269
Train loss: 0.0021;  Loss pred: 0.0021; Loss self: 0.0000; time: 0.21s
Val loss: 0.6139 score: 0.7143 time: 0.07s
Test loss: 0.4869 score: 0.8571 time: 0.07s
     INFO: Early stopping counter 14 of 20
Epoch 48/1000, LR 0.000269
Train loss: 0.0017;  Loss pred: 0.0017; Loss self: 0.0000; time: 0.21s
Val loss: 0.6067 score: 0.7143 time: 0.07s
Test loss: 0.4754 score: 0.8776 time: 0.07s
     INFO: Early stopping counter 15 of 20
Epoch 49/1000, LR 0.000269
Train loss: 0.0017;  Loss pred: 0.0017; Loss self: 0.0000; time: 0.22s
Val loss: 0.5993 score: 0.7347 time: 0.06s
Test loss: 0.4646 score: 0.8776 time: 0.07s
Epoch 50/1000, LR 0.000269
Train loss: 0.0016;  Loss pred: 0.0016; Loss self: 0.0000; time: 0.21s
Val loss: 0.5912 score: 0.7551 time: 0.07s
Test loss: 0.4542 score: 0.8776 time: 0.07s
Epoch 51/1000, LR 0.000269
Train loss: 0.0016;  Loss pred: 0.0016; Loss self: 0.0000; time: 0.21s
Val loss: 0.5832 score: 0.7551 time: 0.06s
Test loss: 0.4449 score: 0.8776 time: 0.07s
Epoch 52/1000, LR 0.000269
Train loss: 0.0019;  Loss pred: 0.0019; Loss self: 0.0000; time: 0.20s
Val loss: 0.5769 score: 0.7551 time: 0.06s
Test loss: 0.4375 score: 0.8776 time: 0.07s
Epoch 53/1000, LR 0.000269
Train loss: 0.0014;  Loss pred: 0.0014; Loss self: 0.0000; time: 0.20s
Val loss: 0.5698 score: 0.8163 time: 0.06s
Test loss: 0.4291 score: 0.8776 time: 0.07s
Epoch 54/1000, LR 0.000269
Train loss: 0.0016;  Loss pred: 0.0016; Loss self: 0.0000; time: 0.20s
Val loss: 0.5639 score: 0.7959 time: 0.06s
Test loss: 0.4217 score: 0.9184 time: 0.07s
Epoch 55/1000, LR 0.000269
Train loss: 0.0013;  Loss pred: 0.0013; Loss self: 0.0000; time: 0.20s
Val loss: 0.5586 score: 0.7959 time: 0.06s
Test loss: 0.4144 score: 0.9592 time: 0.07s
Epoch 56/1000, LR 0.000269
Train loss: 0.0015;  Loss pred: 0.0015; Loss self: 0.0000; time: 0.20s
Val loss: 0.5522 score: 0.7959 time: 0.06s
Test loss: 0.4054 score: 0.9592 time: 0.07s
Epoch 57/1000, LR 0.000269
Train loss: 0.0012;  Loss pred: 0.0012; Loss self: 0.0000; time: 0.19s
Val loss: 0.5444 score: 0.7959 time: 0.06s
Test loss: 0.3954 score: 0.9592 time: 0.07s
Epoch 58/1000, LR 0.000269
Train loss: 0.0012;  Loss pred: 0.0012; Loss self: 0.0000; time: 0.20s
Val loss: 0.5352 score: 0.7959 time: 0.06s
Test loss: 0.3845 score: 0.9592 time: 0.07s
Epoch 59/1000, LR 0.000268
Train loss: 0.0012;  Loss pred: 0.0012; Loss self: 0.0000; time: 0.20s
Val loss: 0.5247 score: 0.8163 time: 0.06s
Test loss: 0.3717 score: 0.9592 time: 0.07s
Epoch 60/1000, LR 0.000268
Train loss: 0.0010;  Loss pred: 0.0010; Loss self: 0.0000; time: 0.20s
Val loss: 0.5155 score: 0.8163 time: 0.06s
Test loss: 0.3585 score: 0.9592 time: 0.07s
Epoch 61/1000, LR 0.000268
Train loss: 0.0010;  Loss pred: 0.0010; Loss self: 0.0000; time: 0.19s
Val loss: 0.5046 score: 0.8163 time: 0.06s
Test loss: 0.3441 score: 0.9592 time: 0.07s
Epoch 62/1000, LR 0.000268
Train loss: 0.0014;  Loss pred: 0.0014; Loss self: 0.0000; time: 0.20s
Val loss: 0.4928 score: 0.8163 time: 0.06s
Test loss: 0.3302 score: 0.9796 time: 0.07s
Epoch 63/1000, LR 0.000268
Train loss: 0.0012;  Loss pred: 0.0012; Loss self: 0.0000; time: 0.19s
Val loss: 0.4803 score: 0.8163 time: 0.06s
Test loss: 0.3170 score: 0.9796 time: 0.07s
Epoch 64/1000, LR 0.000268
Train loss: 0.0011;  Loss pred: 0.0011; Loss self: 0.0000; time: 0.21s
Val loss: 0.4689 score: 0.7959 time: 0.06s
Test loss: 0.3047 score: 0.9796 time: 0.07s
Epoch 65/1000, LR 0.000268
Train loss: 0.0010;  Loss pred: 0.0010; Loss self: 0.0000; time: 0.20s
Val loss: 0.4594 score: 0.7755 time: 0.06s
Test loss: 0.2936 score: 0.9796 time: 0.07s
Epoch 66/1000, LR 0.000268
Train loss: 0.0010;  Loss pred: 0.0010; Loss self: 0.0000; time: 0.21s
Val loss: 0.4512 score: 0.7755 time: 0.07s
Test loss: 0.2831 score: 0.9796 time: 0.07s
Epoch 67/1000, LR 0.000268
Train loss: 0.0010;  Loss pred: 0.0010; Loss self: 0.0000; time: 0.20s
Val loss: 0.4453 score: 0.7755 time: 0.07s
Test loss: 0.2738 score: 0.9796 time: 0.07s
Epoch 68/1000, LR 0.000268
Train loss: 0.0010;  Loss pred: 0.0010; Loss self: 0.0000; time: 0.20s
Val loss: 0.4412 score: 0.7755 time: 0.06s
Test loss: 0.2653 score: 0.9796 time: 0.07s
Epoch 69/1000, LR 0.000268
Train loss: 0.0009;  Loss pred: 0.0009; Loss self: 0.0000; time: 0.21s
Val loss: 0.4366 score: 0.7755 time: 0.06s
Test loss: 0.2566 score: 0.9796 time: 0.07s
Epoch 70/1000, LR 0.000268
Train loss: 0.0008;  Loss pred: 0.0008; Loss self: 0.0000; time: 0.20s
Val loss: 0.4325 score: 0.7959 time: 0.06s
Test loss: 0.2484 score: 0.9796 time: 0.07s
Epoch 71/1000, LR 0.000268
Train loss: 0.0010;  Loss pred: 0.0010; Loss self: 0.0000; time: 0.20s
Val loss: 0.4279 score: 0.8163 time: 0.06s
Test loss: 0.2404 score: 0.9796 time: 0.07s
Epoch 72/1000, LR 0.000267
Train loss: 0.0009;  Loss pred: 0.0009; Loss self: 0.0000; time: 0.19s
Val loss: 0.4219 score: 0.7959 time: 0.06s
Test loss: 0.2323 score: 0.9796 time: 0.07s
Epoch 73/1000, LR 0.000267
Train loss: 0.0009;  Loss pred: 0.0009; Loss self: 0.0000; time: 0.20s
Val loss: 0.4148 score: 0.7959 time: 0.06s
Test loss: 0.2241 score: 0.9796 time: 0.07s
Epoch 74/1000, LR 0.000267
Train loss: 0.0010;  Loss pred: 0.0010; Loss self: 0.0000; time: 0.20s
Val loss: 0.4080 score: 0.7959 time: 0.06s
Test loss: 0.2161 score: 0.9796 time: 0.07s
Epoch 75/1000, LR 0.000267
Train loss: 0.0008;  Loss pred: 0.0008; Loss self: 0.0000; time: 0.19s
Val loss: 0.4026 score: 0.7959 time: 0.06s
Test loss: 0.2086 score: 0.9796 time: 0.07s
Epoch 76/1000, LR 0.000267
Train loss: 0.0008;  Loss pred: 0.0008; Loss self: 0.0000; time: 0.20s
Val loss: 0.3983 score: 0.7959 time: 0.06s
Test loss: 0.2017 score: 0.9796 time: 0.07s
Epoch 77/1000, LR 0.000267
Train loss: 0.0006;  Loss pred: 0.0006; Loss self: 0.0000; time: 0.20s
Val loss: 0.3964 score: 0.7959 time: 0.06s
Test loss: 0.1963 score: 0.9796 time: 0.07s
Epoch 78/1000, LR 0.000267
Train loss: 0.0009;  Loss pred: 0.0009; Loss self: 0.0000; time: 0.20s
Val loss: 0.3984 score: 0.7959 time: 0.06s
Test loss: 0.1924 score: 0.9796 time: 0.07s
     INFO: Early stopping counter 1 of 20
Epoch 79/1000, LR 0.000267
Train loss: 0.0007;  Loss pred: 0.0007; Loss self: 0.0000; time: 0.20s
Val loss: 0.4018 score: 0.7959 time: 0.06s
Test loss: 0.1899 score: 0.9796 time: 0.07s
     INFO: Early stopping counter 2 of 20
Epoch 80/1000, LR 0.000267
Train loss: 0.0010;  Loss pred: 0.0010; Loss self: 0.0000; time: 0.20s
Val loss: 0.4066 score: 0.7959 time: 0.06s
Test loss: 0.1891 score: 0.9796 time: 0.07s
     INFO: Early stopping counter 3 of 20
Epoch 81/1000, LR 0.000267
Train loss: 0.0006;  Loss pred: 0.0006; Loss self: 0.0000; time: 0.20s
Val loss: 0.4123 score: 0.7959 time: 0.06s
Test loss: 0.1892 score: 0.9796 time: 0.07s
     INFO: Early stopping counter 4 of 20
Epoch 82/1000, LR 0.000267
Train loss: 0.0006;  Loss pred: 0.0006; Loss self: 0.0000; time: 0.20s
Val loss: 0.4188 score: 0.7959 time: 0.06s
Test loss: 0.1900 score: 0.9796 time: 0.07s
     INFO: Early stopping counter 5 of 20
Epoch 83/1000, LR 0.000266
Train loss: 0.0007;  Loss pred: 0.0007; Loss self: 0.0000; time: 0.20s
Val loss: 0.4237 score: 0.7959 time: 0.06s
Test loss: 0.1906 score: 0.9592 time: 0.07s
     INFO: Early stopping counter 6 of 20
Epoch 84/1000, LR 0.000266
Train loss: 0.0007;  Loss pred: 0.0007; Loss self: 0.0000; time: 0.20s
Val loss: 0.4269 score: 0.7959 time: 0.06s
Test loss: 0.1910 score: 0.9592 time: 0.07s
     INFO: Early stopping counter 7 of 20
Epoch 85/1000, LR 0.000266
Train loss: 0.0006;  Loss pred: 0.0006; Loss self: 0.0000; time: 0.20s
Val loss: 0.4292 score: 0.7959 time: 0.06s
Test loss: 0.1904 score: 0.9592 time: 0.07s
     INFO: Early stopping counter 8 of 20
Epoch 86/1000, LR 0.000266
Train loss: 0.0006;  Loss pred: 0.0006; Loss self: 0.0000; time: 0.20s
Val loss: 0.4327 score: 0.7959 time: 0.06s
Test loss: 0.1901 score: 0.9592 time: 0.07s
     INFO: Early stopping counter 9 of 20
Epoch 87/1000, LR 0.000266
Train loss: 0.0006;  Loss pred: 0.0006; Loss self: 0.0000; time: 0.20s
Val loss: 0.4369 score: 0.7959 time: 0.06s
Test loss: 0.1891 score: 0.9592 time: 0.07s
     INFO: Early stopping counter 10 of 20
Epoch 88/1000, LR 0.000266
Train loss: 0.0007;  Loss pred: 0.0007; Loss self: 0.0000; time: 0.20s
Val loss: 0.4422 score: 0.7959 time: 0.06s
Test loss: 0.1885 score: 0.9592 time: 0.07s
     INFO: Early stopping counter 11 of 20
Epoch 89/1000, LR 0.000266
Train loss: 0.0006;  Loss pred: 0.0006; Loss self: 0.0000; time: 0.20s
Val loss: 0.4490 score: 0.7959 time: 0.06s
Test loss: 0.1909 score: 0.9592 time: 0.07s
     INFO: Early stopping counter 12 of 20
Epoch 90/1000, LR 0.000266
Train loss: 0.0006;  Loss pred: 0.0006; Loss self: 0.0000; time: 0.20s
Val loss: 0.4572 score: 0.7959 time: 0.06s
Test loss: 0.1938 score: 0.9592 time: 0.07s
     INFO: Early stopping counter 13 of 20
Epoch 91/1000, LR 0.000266
Train loss: 0.0009;  Loss pred: 0.0009; Loss self: 0.0000; time: 0.20s
Val loss: 0.4644 score: 0.7959 time: 0.06s
Test loss: 0.1951 score: 0.9592 time: 0.07s
     INFO: Early stopping counter 14 of 20
Epoch 92/1000, LR 0.000266
Train loss: 0.0006;  Loss pred: 0.0006; Loss self: 0.0000; time: 0.20s
Val loss: 0.4707 score: 0.7959 time: 0.06s
Test loss: 0.1957 score: 0.9592 time: 0.07s
     INFO: Early stopping counter 15 of 20
Epoch 93/1000, LR 0.000265
Train loss: 0.0005;  Loss pred: 0.0005; Loss self: 0.0000; time: 0.20s
Val loss: 0.4801 score: 0.7959 time: 0.06s
Test loss: 0.1989 score: 0.9592 time: 0.07s
     INFO: Early stopping counter 16 of 20
Epoch 94/1000, LR 0.000265
Train loss: 0.0005;  Loss pred: 0.0005; Loss self: 0.0000; time: 0.20s
Val loss: 0.4946 score: 0.7959 time: 0.06s
Test loss: 0.2057 score: 0.9592 time: 0.07s
     INFO: Early stopping counter 17 of 20
Epoch 95/1000, LR 0.000265
Train loss: 0.0006;  Loss pred: 0.0006; Loss self: 0.0000; time: 0.20s
Val loss: 0.5138 score: 0.7959 time: 0.06s
Test loss: 0.2156 score: 0.9592 time: 0.07s
     INFO: Early stopping counter 18 of 20
Epoch 96/1000, LR 0.000265
Train loss: 0.0005;  Loss pred: 0.0005; Loss self: 0.0000; time: 0.20s
Val loss: 0.5355 score: 0.7959 time: 0.06s
Test loss: 0.2265 score: 0.9592 time: 0.07s
     INFO: Early stopping counter 19 of 20
Epoch 97/1000, LR 0.000265
Train loss: 0.0005;  Loss pred: 0.0005; Loss self: 0.0000; time: 0.20s
Val loss: 0.5585 score: 0.7959 time: 0.06s
Test loss: 0.2365 score: 0.9592 time: 0.07s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 076,   Train_Loss: 0.0006,   Val_Loss: 0.3964,   Val_Precision: 1.0000,   Val_Recall: 0.6000,   Val_accuracy: 0.7500,   Val_Score: 0.7959,   Val_Loss: 0.3964,   Test_Precision: 1.0000,   Test_Recall: 0.9583,   Test_accuracy: 0.9787,   Test_Score: 0.9796,   Test_loss: 0.1963


[0.06536245299503207, 0.06498479307629168, 0.06517278694082052, 0.0655033509247005, 0.06540117296390235, 0.06527594395447522, 0.0652049119817093, 0.06538243195973337, 0.06516556406859308, 0.06561034999322146, 0.06535083602648228, 0.06570164498407394, 0.06521763594355434, 0.0657589309848845, 0.06525927397888154, 0.0654908720171079, 0.06568012409843504, 0.06520061905030161, 0.0653998430352658, 0.06531489407643676, 0.06531110405921936, 0.06577758700586855, 0.06565138604491949, 0.06502032198477536, 0.06543521804269403, 0.06494482897687703, 0.07327295001596212, 0.07253202295396477, 0.07728287391364574, 0.07345235301181674, 0.07374285091646016, 0.07727687095757574, 0.07325936097186059, 0.07916854904033244, 0.17911807808559388, 0.08587669197004288, 0.08059052296448499, 0.08037801808677614, 0.08469690498895943, 0.07757169601973146, 0.08111767307855189, 0.07848339108750224, 0.07853813108522445, 0.08225323399528861, 0.16367818601429462, 0.07856252300553024, 0.07798082998488098, 0.07787705992814153, 0.08467481599655002, 0.09126773197203875, 0.08311981498263776, 0.08289293700363487, 0.08202750002965331, 0.07809172593988478, 0.10024787893053144, 0.07367166399490088, 0.07408946205396205, 0.07794106402434409, 0.07658519805409014, 0.0766402690205723, 0.07651966204866767, 0.07325204007793218, 0.07700520392972976, 0.07818336097989231, 0.07300509593915194, 0.07793125894386321, 0.07791359710972756, 0.07275302207563072, 0.07754558604210615, 0.07298973191063851, 0.07687668001744896, 0.07694948697462678, 0.07675380108412355, 0.07727717398665845, 0.07714721607044339, 0.07711328892037272, 0.07606785104144365, 0.07264879404101521, 0.0780080029508099, 0.07657565199770033, 0.07645220297854394, 0.07632875198032707, 0.07593597797676921, 0.072225849959068, 0.07603982090950012, 0.07191821793094277, 0.07230732194148004, 0.07207576907239854, 0.07213277707342058, 0.0725681979674846, 0.077641248004511, 0.07733262702822685, 0.07811200909782201, 0.07434580894187093, 0.07275242393370718, 0.07307766296435148, 0.07293155300430954, 0.07317119103390723, 0.0732118779560551, 0.07679623609874398, 0.0726124100619927, 0.07683658401947469, 0.0768393169855699, 0.07298109796829522, 0.07280037901364267, 0.07660662394482642, 0.07292128494009376, 0.07222495297901332, 0.07329606101848185, 0.07758529996499419, 0.07629835396073759, 0.07727547292597592, 0.0731858640210703, 0.07317148894071579, 0.07930335192941129, 0.07882503792643547, 0.07295401603914797, 0.07804477901663631, 0.07404278009198606, 0.0783001130912453, 0.07348293205723166, 0.07800611096899956, 0.07337203889619559]
[0.0013339276121435116, 0.0013262202668630956, 0.0013300568763432758, 0.0013368030800959285, 0.001334717815589844, 0.0013321621215199024, 0.0013307124894226389, 0.0013343353461170075, 0.001329909470787614, 0.00133898673455554, 0.0013336905311526997, 0.001340849897634162, 0.001330972162113354, 0.0013420189996915205, 0.001331821917936358, 0.001336548408512406, 0.0013404106958864294, 0.0013306248785775838, 0.0013346906741890981, 0.0013329570219680971, 0.001332879674677946, 0.001342399734813644, 0.001339824204998357, 0.0013269453466280686, 0.0013354126131162047, 0.0013254046729974905, 0.0014953663268563698, 0.0014802453664074444, 0.00157720150844175, 0.001499027612486056, 0.001504956141152248, 0.0015770789991341988, 0.0014950889994257263, 0.0016156846742924986, 0.0036554709813386506, 0.0017525855504090385, 0.0016447045502956121, 0.0016403677160566558, 0.0017285082650808047, 0.0015830958371373769, 0.001655462715888814, 0.0016017018589286171, 0.0016028190017392744, 0.0016786374284752778, 0.00334037114314887, 0.0016033167960312292, 0.0015914455098955302, 0.0015893277536355415, 0.0017280574693173474, 0.0018626067749395662, 0.0016963227547477095, 0.0016916925919109157, 0.0016740306128500675, 0.00159370869265071, 0.0020458750802149275, 0.0015035033468347118, 0.0015120298378359601, 0.0015906339596804915, 0.0015629632255936764, 0.0015640871228688226, 0.0015616257560952585, 0.0014949395934271874, 0.0015715347740761175, 0.0015955787955080063, 0.0014898999171255498, 0.0015904338559972085, 0.0015900734104026034, 0.0014847555525638924, 0.0015825629804511459, 0.001489586365523235, 0.0015689118370907952, 0.0015703976933597302, 0.0015664041037576236, 0.0015770851834011929, 0.001574432981029457, 0.0015737405902116882, 0.0015524051232947682, 0.0014826284498166368, 0.00159200006022061, 0.0015627684081163332, 0.0015602490403784476, 0.0015577296322515728, 0.001549713836260596, 0.0014739969379401632, 0.0015518330797857167, 0.0014677187332845464, 0.0014756596314587764, 0.001470934062702011, 0.0014720974912942977, 0.0014809836319894815, 0.0015845152653981836, 0.0015782168781270786, 0.0015941226346494288, 0.0015172614069769578, 0.0014847433455858607, 0.0014913808768234995, 0.0014883990409042761, 0.0014932896129368823, 0.001494119958286839, 0.0015672701244641627, 0.001481885919632504, 0.0015680935514178506, 0.0015681493262361204, 0.0014894101626182698, 0.0014857220206865851, 0.001563400488669927, 0.001488189488573342, 0.0014739786322247616, 0.0014958379799690173, 0.001583373468673351, 0.0015571092645048487, 0.0015770504678770596, 0.001493589061654496, 0.0014932956926676693, 0.001618435753661455, 0.0016086742433966423, 0.0014888574701866933, 0.0015927505921762512, 0.0015110771447344093, 0.0015979614916580673, 0.001499651674637381, 0.00159196144834693, 0.001497388548901951]
[749.6658670953534, 754.0225594390114, 751.8475471133979, 748.0533332764616, 749.2220365381696, 750.6593858554324, 751.4771281915854, 749.436791066097, 751.9308809852814, 746.8333884068968, 749.7991300393404, 745.7956343692398, 751.3305149915177, 745.14593327655, 750.8511359758128, 748.195870520703, 746.0400033130802, 751.5266068593153, 749.2372722298056, 750.2117349016328, 750.2552698476871, 744.9345929279575, 746.3665727708109, 753.6105405856564, 748.8322262184461, 754.4865506912947, 668.7324584219089, 675.5636752486516, 634.0343923383539, 667.0991192360722, 664.4711913228011, 634.0836448579877, 668.8565031139328, 618.9326518417931, 273.5625600928161, 570.5855555904866, 608.0119373538939, 609.6194104599541, 578.533536808545, 631.6736969053269, 604.060719943851, 624.3359177149878, 623.9007641629313, 595.7212576323343, 299.3679316296959, 623.7070568183095, 628.3595597725772, 629.1968398038282, 578.6844579856713, 536.8819728643185, 589.5104556023762, 591.1239457934919, 597.3606410324132, 627.4672432995056, 488.7883965500699, 665.1132517298848, 661.3626100336849, 628.680152283979, 639.8103190304812, 639.3505741328634, 640.3582907728377, 668.9233494093727, 636.3206315863329, 626.731818456898, 671.1860229707851, 628.7592509611134, 628.9017811742428, 673.5115408547817, 631.8863845247581, 671.327304777483, 637.3844446570573, 636.7813734243243, 638.4048647479376, 634.081158408557, 635.1492963175486, 635.4287397934415, 644.1617494005923, 674.4778168283998, 628.1406797569001, 639.8900789179246, 640.9233232134814, 641.9599263542162, 645.2804231347409, 678.4274609128088, 644.399203126978, 681.3294518372344, 677.6630455164252, 679.8401270027457, 679.3028355212941, 675.226908927176, 631.1078358394377, 633.6264767277946, 627.3043103863304, 659.0822091708201, 673.5170782027736, 670.5195269299052, 671.8628355151657, 669.6624628850664, 669.2903032676186, 638.0521037124294, 674.8157781592203, 637.7170539957979, 637.6943721298563, 671.4067253590348, 673.0734189009852, 639.6313722856493, 671.9574406876462, 678.4358864759402, 668.5216001941017, 631.5629381095177, 642.215689544429, 634.095116401789, 669.5282026853279, 669.6597364541843, 617.8805663046297, 621.629894370998, 671.6559644051129, 627.8446888747674, 661.779581197863, 625.7973081456337, 666.8215138970869, 628.1559148548388, 667.8293357681341]
Elapsed: 0.07584048858540875~0.01368233853234089
Time per graph: 0.001547765073171607~0.0002792313986192019
Speed: 657.9366458050781~71.07886916571897
Total Time: 0.0739
best val loss: 0.39636486768722534 test_score: 0.9796

Testing...
Test loss: 0.4291 score: 0.8776 time: 0.07s
test Score 0.8776
Epoch Time List: [0.3231512511847541, 0.32157175603788346, 0.32225958607159555, 0.3230605380376801, 0.321877604117617, 0.32218650286085904, 0.3213745200773701, 0.32415115099865943, 0.32777362992055714, 0.32252413290552795, 0.32294920191634446, 0.3258672798983753, 0.3343243838753551, 0.3227181170368567, 0.3214159399503842, 0.32817996200174093, 0.32379537587985396, 0.3227470818674192, 0.3221816929290071, 0.3228515909286216, 0.32357651996426284, 0.3286313018761575, 0.32418785407207906, 0.3229531500255689, 0.3227580130333081, 0.3291280370904133, 0.3238214390585199, 0.3279879349283874, 0.32657654606737196, 0.3237398079363629, 0.32298798696137965, 0.32721057487651706, 0.323597565991804, 0.338247584993951, 0.4342440019827336, 0.36332764907274395, 0.37469503097236156, 0.3644465639954433, 0.3625387530773878, 0.4044134601717815, 0.35743730410467833, 0.34946094697806984, 0.3491337421583012, 0.354283231892623, 0.4390511499950662, 0.3609267509309575, 0.3514302971307188, 0.35706403595395386, 0.3610042390646413, 0.3675999710103497, 0.42623469210229814, 0.3567572260508314, 0.3565785310929641, 0.3534884660039097, 0.38192321301903576, 0.3272409290075302, 0.3285410129465163, 0.3381954290671274, 0.33186440891586244, 0.3302194429561496, 0.33187942195218056, 0.32751788292080164, 0.3302817781222984, 0.3348036460811272, 0.3415346989640966, 0.3312237940263003, 0.33321859303396195, 0.32683467911556363, 0.3286595968529582, 0.32878469803836197, 0.33553923503495753, 0.34774040698539466, 0.3481845089700073, 0.3498724380042404, 0.3542920059990138, 0.34743603703100234, 0.339397381991148, 0.3237173790112138, 0.33472505607642233, 0.33237642294261605, 0.3287894930690527, 0.32806656893808395, 0.32358424086123705, 0.32457298098597676, 0.3307465489488095, 0.3228722829371691, 0.31878745404537767, 0.32277932297438383, 0.32617373997345567, 0.33367397694382817, 0.33760932506993413, 0.3538501929724589, 0.3414208439644426, 0.3272322209086269, 0.3347716168500483, 0.3269505968783051, 0.32615990191698074, 0.32374683406669647, 0.3274386040866375, 0.33229257888160646, 0.3231795390602201, 0.3279637289233506, 0.3291340119903907, 0.3260237129870802, 0.3299458469264209, 0.32801481196656823, 0.32550369994714856, 0.3236278749536723, 0.3253855051007122, 0.3365278630517423, 0.3283712570555508, 0.32936998503282666, 0.32444986200425774, 0.32393405598122627, 0.33284801710397005, 0.33573603990953416, 0.32874650694429874, 0.33158406999427825, 0.32758944015949965, 0.3317527030594647, 0.32751128589734435, 0.33184226194862276, 0.3284247010014951]
Total Epoch List: [26, 97]
Total Time List: [0.06523587100673467, 0.07394551194738597]
========================k_idx:2========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: GPSTransformer
GPS(
  (x_lin): Linear(in_features=14887, out_features=64, bias=True)
  (pe_lin): Linear(in_features=20, out_features=8, bias=True)
  (pe_norm): BatchNorm1d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (convs): ModuleList(
    (0-1): 2 x GPSConv(64, conv=GINEConv(nn=Sequential(
      (0): Linear(in_features=64, out_features=64, bias=True)
      (1): ReLU()
      (2): Linear(in_features=64, out_features=64, bias=True)
    )), heads=1, attn_type=multihead)
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=32, bias=True)
    (1): ReLU()
    (2): Linear(in_features=32, out_features=16, bias=True)
    (3): ReLU()
    (4): Linear(in_features=16, out_features=2, bias=True)
  )
)
Total number of parameters: 1039906
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x74b5c4f93700>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.6079;  Loss pred: 0.6079; Loss self: 0.0000; time: 0.19s
Val loss: 0.7269 score: 0.5306 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7802 score: 0.5000 time: 0.07s
Epoch 2/1000, LR 0.000000
Train loss: 0.6132;  Loss pred: 0.6132; Loss self: 0.0000; time: 0.19s
Val loss: 0.6264 score: 0.5102 time: 0.06s
Test loss: 0.6655 score: 0.5208 time: 0.06s
Epoch 3/1000, LR 0.000030
Train loss: 0.5739;  Loss pred: 0.5739; Loss self: 0.0000; time: 0.19s
Val loss: 0.5954 score: 0.5918 time: 0.06s
Test loss: 0.6217 score: 0.5833 time: 0.07s
Epoch 4/1000, LR 0.000060
Train loss: 0.5115;  Loss pred: 0.5115; Loss self: 0.0000; time: 0.19s
Val loss: 0.5909 score: 0.5714 time: 0.06s
Test loss: 0.6074 score: 0.5417 time: 0.07s
Epoch 5/1000, LR 0.000090
Train loss: 0.4546;  Loss pred: 0.4546; Loss self: 0.0000; time: 0.20s
Val loss: 0.5902 score: 0.5102 time: 0.06s
Test loss: 0.5977 score: 0.5208 time: 0.06s
Epoch 6/1000, LR 0.000120
Train loss: 0.3965;  Loss pred: 0.3965; Loss self: 0.0000; time: 0.19s
Val loss: 0.5913 score: 0.5306 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.5992 score: 0.5000 time: 0.06s
     INFO: Early stopping counter 1 of 20
Epoch 7/1000, LR 0.000150
Train loss: 0.3445;  Loss pred: 0.3445; Loss self: 0.0000; time: 0.19s
Val loss: 0.5972 score: 0.5306 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6042 score: 0.5000 time: 0.06s
     INFO: Early stopping counter 2 of 20
Epoch 8/1000, LR 0.000180
Train loss: 0.2809;  Loss pred: 0.2809; Loss self: 0.0000; time: 0.20s
Val loss: 0.5987 score: 0.5306 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6087 score: 0.5000 time: 0.06s
     INFO: Early stopping counter 3 of 20
Epoch 9/1000, LR 0.000210
Train loss: 0.2218;  Loss pred: 0.2218; Loss self: 0.0000; time: 0.19s
Val loss: 0.6009 score: 0.5306 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6113 score: 0.5000 time: 0.06s
     INFO: Early stopping counter 4 of 20
Epoch 10/1000, LR 0.000240
Train loss: 0.1622;  Loss pred: 0.1622; Loss self: 0.0000; time: 0.20s
Val loss: 0.6046 score: 0.5306 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6095 score: 0.5000 time: 0.06s
     INFO: Early stopping counter 5 of 20
Epoch 11/1000, LR 0.000270
Train loss: 0.1266;  Loss pred: 0.1266; Loss self: 0.0000; time: 0.19s
Val loss: 0.6068 score: 0.5306 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6084 score: 0.5000 time: 0.07s
     INFO: Early stopping counter 6 of 20
Epoch 12/1000, LR 0.000270
Train loss: 0.1008;  Loss pred: 0.1008; Loss self: 0.0000; time: 0.20s
Val loss: 0.6076 score: 0.5306 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6067 score: 0.5000 time: 0.06s
     INFO: Early stopping counter 7 of 20
Epoch 13/1000, LR 0.000270
Train loss: 0.0759;  Loss pred: 0.0759; Loss self: 0.0000; time: 0.19s
Val loss: 0.6090 score: 0.5306 time: 0.06s
Test loss: 0.6053 score: 0.5208 time: 0.06s
     INFO: Early stopping counter 8 of 20
Epoch 14/1000, LR 0.000270
Train loss: 0.0586;  Loss pred: 0.0586; Loss self: 0.0000; time: 0.19s
Val loss: 0.6093 score: 0.5306 time: 0.06s
Test loss: 0.6020 score: 0.5208 time: 0.06s
     INFO: Early stopping counter 9 of 20
Epoch 15/1000, LR 0.000270
Train loss: 0.0448;  Loss pred: 0.0448; Loss self: 0.0000; time: 0.20s
Val loss: 0.6103 score: 0.5510 time: 0.06s
Test loss: 0.6009 score: 0.5208 time: 0.06s
     INFO: Early stopping counter 10 of 20
Epoch 16/1000, LR 0.000270
Train loss: 0.0357;  Loss pred: 0.0357; Loss self: 0.0000; time: 0.20s
Val loss: 0.6106 score: 0.5510 time: 0.06s
Test loss: 0.5983 score: 0.5625 time: 0.06s
     INFO: Early stopping counter 11 of 20
Epoch 17/1000, LR 0.000270
Train loss: 0.0300;  Loss pred: 0.0300; Loss self: 0.0000; time: 0.19s
Val loss: 0.6108 score: 0.5306 time: 0.06s
Test loss: 0.5929 score: 0.5625 time: 0.06s
     INFO: Early stopping counter 12 of 20
Epoch 18/1000, LR 0.000270
Train loss: 0.0261;  Loss pred: 0.0261; Loss self: 0.0000; time: 0.20s
Val loss: 0.6091 score: 0.5306 time: 0.06s
Test loss: 0.5883 score: 0.5625 time: 0.06s
     INFO: Early stopping counter 13 of 20
Epoch 19/1000, LR 0.000270
Train loss: 0.0228;  Loss pred: 0.0228; Loss self: 0.0000; time: 0.19s
Val loss: 0.6061 score: 0.5510 time: 0.06s
Test loss: 0.5848 score: 0.5625 time: 0.06s
     INFO: Early stopping counter 14 of 20
Epoch 20/1000, LR 0.000270
Train loss: 0.0206;  Loss pred: 0.0206; Loss self: 0.0000; time: 0.19s
Val loss: 0.6008 score: 0.5714 time: 0.06s
Test loss: 0.5802 score: 0.5625 time: 0.07s
     INFO: Early stopping counter 15 of 20
Epoch 21/1000, LR 0.000270
Train loss: 0.0183;  Loss pred: 0.0183; Loss self: 0.0000; time: 0.19s
Val loss: 0.5971 score: 0.5714 time: 0.06s
Test loss: 0.5780 score: 0.5833 time: 0.07s
     INFO: Early stopping counter 16 of 20
Epoch 22/1000, LR 0.000270
Train loss: 0.0164;  Loss pred: 0.0164; Loss self: 0.0000; time: 0.19s
Val loss: 0.5954 score: 0.5714 time: 0.06s
Test loss: 0.5724 score: 0.5833 time: 0.07s
     INFO: Early stopping counter 17 of 20
Epoch 23/1000, LR 0.000270
Train loss: 0.0156;  Loss pred: 0.0156; Loss self: 0.0000; time: 0.19s
Val loss: 0.5923 score: 0.5918 time: 0.06s
Test loss: 0.5667 score: 0.6042 time: 0.07s
     INFO: Early stopping counter 18 of 20
Epoch 24/1000, LR 0.000270
Train loss: 0.0145;  Loss pred: 0.0145; Loss self: 0.0000; time: 0.20s
Val loss: 0.5891 score: 0.6327 time: 0.06s
Test loss: 0.5602 score: 0.6042 time: 0.07s
Epoch 25/1000, LR 0.000270
Train loss: 0.0134;  Loss pred: 0.0134; Loss self: 0.0000; time: 0.20s
Val loss: 0.5853 score: 0.6531 time: 0.07s
Test loss: 0.5543 score: 0.6042 time: 0.07s
Epoch 26/1000, LR 0.000270
Train loss: 0.0124;  Loss pred: 0.0124; Loss self: 0.0000; time: 0.19s
Val loss: 0.5817 score: 0.6939 time: 0.07s
Test loss: 0.5493 score: 0.6458 time: 0.07s
Epoch 27/1000, LR 0.000270
Train loss: 0.0119;  Loss pred: 0.0119; Loss self: 0.0000; time: 0.20s
Val loss: 0.5785 score: 0.6939 time: 0.06s
Test loss: 0.5454 score: 0.6875 time: 0.07s
Epoch 28/1000, LR 0.000270
Train loss: 0.0113;  Loss pred: 0.0113; Loss self: 0.0000; time: 0.21s
Val loss: 0.5752 score: 0.7143 time: 0.06s
Test loss: 0.5416 score: 0.7500 time: 0.07s
Epoch 29/1000, LR 0.000270
Train loss: 0.0105;  Loss pred: 0.0105; Loss self: 0.0000; time: 0.19s
Val loss: 0.5714 score: 0.6939 time: 0.06s
Test loss: 0.5379 score: 0.7500 time: 0.07s
Epoch 30/1000, LR 0.000270
Train loss: 0.0099;  Loss pred: 0.0099; Loss self: 0.0000; time: 0.20s
Val loss: 0.5668 score: 0.6939 time: 0.06s
Test loss: 0.5337 score: 0.7708 time: 0.07s
Epoch 31/1000, LR 0.000270
Train loss: 0.0088;  Loss pred: 0.0088; Loss self: 0.0000; time: 0.19s
Val loss: 0.5620 score: 0.6735 time: 0.06s
Test loss: 0.5284 score: 0.7917 time: 0.07s
Epoch 32/1000, LR 0.000270
Train loss: 0.0089;  Loss pred: 0.0089; Loss self: 0.0000; time: 0.20s
Val loss: 0.5555 score: 0.7143 time: 0.07s
Test loss: 0.5228 score: 0.8542 time: 0.06s
Epoch 33/1000, LR 0.000270
Train loss: 0.0080;  Loss pred: 0.0080; Loss self: 0.0000; time: 0.19s
Val loss: 0.5485 score: 0.7347 time: 0.06s
Test loss: 0.5163 score: 0.8542 time: 0.06s
Epoch 34/1000, LR 0.000270
Train loss: 0.0078;  Loss pred: 0.0078; Loss self: 0.0000; time: 0.20s
Val loss: 0.5420 score: 0.7143 time: 0.07s
Test loss: 0.5095 score: 0.8542 time: 0.07s
Epoch 35/1000, LR 0.000270
Train loss: 0.0071;  Loss pred: 0.0071; Loss self: 0.0000; time: 0.19s
Val loss: 0.5352 score: 0.7143 time: 0.06s
Test loss: 0.5028 score: 0.8542 time: 0.07s
Epoch 36/1000, LR 0.000270
Train loss: 0.0070;  Loss pred: 0.0070; Loss self: 0.0000; time: 0.19s
Val loss: 0.5281 score: 0.7143 time: 0.06s
Test loss: 0.4969 score: 0.8542 time: 0.06s
Epoch 37/1000, LR 0.000270
Train loss: 0.0067;  Loss pred: 0.0067; Loss self: 0.0000; time: 0.19s
Val loss: 0.5208 score: 0.7143 time: 0.06s
Test loss: 0.4910 score: 0.8542 time: 0.07s
Epoch 38/1000, LR 0.000270
Train loss: 0.0062;  Loss pred: 0.0062; Loss self: 0.0000; time: 0.21s
Val loss: 0.5136 score: 0.7347 time: 0.06s
Test loss: 0.4844 score: 0.8333 time: 0.07s
Epoch 39/1000, LR 0.000269
Train loss: 0.0059;  Loss pred: 0.0059; Loss self: 0.0000; time: 0.20s
Val loss: 0.5064 score: 0.7551 time: 0.06s
Test loss: 0.4765 score: 0.8333 time: 0.06s
Epoch 40/1000, LR 0.000269
Train loss: 0.0056;  Loss pred: 0.0056; Loss self: 0.0000; time: 0.19s
Val loss: 0.4992 score: 0.7755 time: 0.06s
Test loss: 0.4689 score: 0.8750 time: 0.07s
Epoch 41/1000, LR 0.000269
Train loss: 0.0051;  Loss pred: 0.0051; Loss self: 0.0000; time: 0.19s
Val loss: 0.4923 score: 0.7959 time: 0.06s
Test loss: 0.4614 score: 0.8750 time: 0.07s
Epoch 42/1000, LR 0.000269
Train loss: 0.0047;  Loss pred: 0.0047; Loss self: 0.0000; time: 0.20s
Val loss: 0.4854 score: 0.8163 time: 0.07s
Test loss: 0.4541 score: 0.8958 time: 0.07s
Epoch 43/1000, LR 0.000269
Train loss: 0.0045;  Loss pred: 0.0045; Loss self: 0.0000; time: 0.20s
Val loss: 0.4786 score: 0.8163 time: 0.07s
Test loss: 0.4466 score: 0.8958 time: 0.07s
Epoch 44/1000, LR 0.000269
Train loss: 0.0043;  Loss pred: 0.0043; Loss self: 0.0000; time: 0.19s
Val loss: 0.4721 score: 0.8163 time: 0.07s
Test loss: 0.4392 score: 0.8958 time: 0.07s
Epoch 45/1000, LR 0.000269
Train loss: 0.0038;  Loss pred: 0.0038; Loss self: 0.0000; time: 0.19s
Val loss: 0.4653 score: 0.8571 time: 0.06s
Test loss: 0.4324 score: 0.8958 time: 0.06s
Epoch 46/1000, LR 0.000269
Train loss: 0.0037;  Loss pred: 0.0037; Loss self: 0.0000; time: 0.20s
Val loss: 0.4589 score: 0.8367 time: 0.06s
Test loss: 0.4257 score: 0.8958 time: 0.07s
Epoch 47/1000, LR 0.000269
Train loss: 0.0036;  Loss pred: 0.0036; Loss self: 0.0000; time: 0.19s
Val loss: 0.4532 score: 0.8367 time: 0.06s
Test loss: 0.4197 score: 0.8958 time: 0.07s
Epoch 48/1000, LR 0.000269
Train loss: 0.0027;  Loss pred: 0.0027; Loss self: 0.0000; time: 0.20s
Val loss: 0.4475 score: 0.8367 time: 0.06s
Test loss: 0.4142 score: 0.8958 time: 0.07s
Epoch 49/1000, LR 0.000269
Train loss: 0.0029;  Loss pred: 0.0029; Loss self: 0.0000; time: 0.20s
Val loss: 0.4427 score: 0.8367 time: 0.06s
Test loss: 0.4097 score: 0.8958 time: 0.06s
Epoch 50/1000, LR 0.000269
Train loss: 0.0025;  Loss pred: 0.0025; Loss self: 0.0000; time: 0.19s
Val loss: 0.4388 score: 0.8367 time: 0.06s
Test loss: 0.4063 score: 0.8958 time: 0.07s
Epoch 51/1000, LR 0.000269
Train loss: 0.0025;  Loss pred: 0.0025; Loss self: 0.0000; time: 0.20s
Val loss: 0.4355 score: 0.8367 time: 0.06s
Test loss: 0.4034 score: 0.8542 time: 0.06s
Epoch 52/1000, LR 0.000269
Train loss: 0.0021;  Loss pred: 0.0021; Loss self: 0.0000; time: 0.19s
Val loss: 0.4328 score: 0.8367 time: 0.06s
Test loss: 0.4019 score: 0.8542 time: 0.07s
Epoch 53/1000, LR 0.000269
Train loss: 0.0021;  Loss pred: 0.0021; Loss self: 0.0000; time: 0.20s
Val loss: 0.4312 score: 0.8367 time: 0.06s
Test loss: 0.4011 score: 0.8333 time: 0.07s
Epoch 54/1000, LR 0.000269
Train loss: 0.0016;  Loss pred: 0.0016; Loss self: 0.0000; time: 0.20s
Val loss: 0.4293 score: 0.8367 time: 0.07s
Test loss: 0.4000 score: 0.8333 time: 0.06s
Epoch 55/1000, LR 0.000269
Train loss: 0.0017;  Loss pred: 0.0017; Loss self: 0.0000; time: 0.19s
Val loss: 0.4275 score: 0.8367 time: 0.06s
Test loss: 0.3988 score: 0.8333 time: 0.06s
Epoch 56/1000, LR 0.000269
Train loss: 0.0015;  Loss pred: 0.0015; Loss self: 0.0000; time: 0.19s
Val loss: 0.4255 score: 0.8367 time: 0.06s
Test loss: 0.3974 score: 0.8333 time: 0.07s
Epoch 57/1000, LR 0.000269
Train loss: 0.0015;  Loss pred: 0.0015; Loss self: 0.0000; time: 0.20s
Val loss: 0.4224 score: 0.8367 time: 0.06s
Test loss: 0.3949 score: 0.8333 time: 0.07s
Epoch 58/1000, LR 0.000269
Train loss: 0.0016;  Loss pred: 0.0016; Loss self: 0.0000; time: 0.20s
Val loss: 0.4189 score: 0.8367 time: 0.06s
Test loss: 0.3912 score: 0.8542 time: 0.06s
Epoch 59/1000, LR 0.000268
Train loss: 0.0013;  Loss pred: 0.0013; Loss self: 0.0000; time: 0.19s
Val loss: 0.4156 score: 0.8367 time: 0.06s
Test loss: 0.3871 score: 0.8542 time: 0.06s
Epoch 60/1000, LR 0.000268
Train loss: 0.0013;  Loss pred: 0.0013; Loss self: 0.0000; time: 0.19s
Val loss: 0.4118 score: 0.8367 time: 0.06s
Test loss: 0.3819 score: 0.8542 time: 0.07s
Epoch 61/1000, LR 0.000268
Train loss: 0.0011;  Loss pred: 0.0011; Loss self: 0.0000; time: 0.19s
Val loss: 0.4076 score: 0.8367 time: 0.06s
Test loss: 0.3762 score: 0.8542 time: 0.07s
Epoch 62/1000, LR 0.000268
Train loss: 0.0011;  Loss pred: 0.0011; Loss self: 0.0000; time: 0.20s
Val loss: 0.4031 score: 0.8367 time: 0.07s
Test loss: 0.3700 score: 0.8542 time: 0.07s
Epoch 63/1000, LR 0.000268
Train loss: 0.0011;  Loss pred: 0.0011; Loss self: 0.0000; time: 0.21s
Val loss: 0.3983 score: 0.8367 time: 0.08s
Test loss: 0.3628 score: 0.8542 time: 0.09s
Epoch 64/1000, LR 0.000268
Train loss: 0.0010;  Loss pred: 0.0010; Loss self: 0.0000; time: 0.20s
Val loss: 0.3931 score: 0.8367 time: 0.07s
Test loss: 0.3554 score: 0.8542 time: 0.07s
Epoch 65/1000, LR 0.000268
Train loss: 0.0010;  Loss pred: 0.0010; Loss self: 0.0000; time: 0.19s
Val loss: 0.3876 score: 0.8571 time: 0.06s
Test loss: 0.3478 score: 0.8750 time: 0.07s
Epoch 66/1000, LR 0.000268
Train loss: 0.0010;  Loss pred: 0.0010; Loss self: 0.0000; time: 0.19s
Val loss: 0.3819 score: 0.8571 time: 0.06s
Test loss: 0.3397 score: 0.8750 time: 0.06s
Epoch 67/1000, LR 0.000268
Train loss: 0.0010;  Loss pred: 0.0010; Loss self: 0.0000; time: 0.19s
Val loss: 0.3766 score: 0.8571 time: 0.06s
Test loss: 0.3324 score: 0.8750 time: 0.07s
Epoch 68/1000, LR 0.000268
Train loss: 0.0009;  Loss pred: 0.0009; Loss self: 0.0000; time: 0.19s
Val loss: 0.3716 score: 0.8571 time: 0.06s
Test loss: 0.3254 score: 0.8542 time: 0.07s
Epoch 69/1000, LR 0.000268
Train loss: 0.0010;  Loss pred: 0.0010; Loss self: 0.0000; time: 0.19s
Val loss: 0.3676 score: 0.8571 time: 0.06s
Test loss: 0.3192 score: 0.8542 time: 0.07s
Epoch 70/1000, LR 0.000268
Train loss: 0.0008;  Loss pred: 0.0008; Loss self: 0.0000; time: 0.19s
Val loss: 0.3641 score: 0.8571 time: 0.06s
Test loss: 0.3140 score: 0.8542 time: 0.06s
Epoch 71/1000, LR 0.000268
Train loss: 0.0008;  Loss pred: 0.0008; Loss self: 0.0000; time: 0.19s
Val loss: 0.3613 score: 0.8571 time: 0.06s
Test loss: 0.3098 score: 0.8542 time: 0.06s
Epoch 72/1000, LR 0.000267
Train loss: 0.0008;  Loss pred: 0.0008; Loss self: 0.0000; time: 0.19s
Val loss: 0.3591 score: 0.8571 time: 0.07s
Test loss: 0.3055 score: 0.8542 time: 0.07s
Epoch 73/1000, LR 0.000267
Train loss: 0.0008;  Loss pred: 0.0008; Loss self: 0.0000; time: 0.21s
Val loss: 0.3574 score: 0.8571 time: 0.07s
Test loss: 0.3016 score: 0.8542 time: 0.06s
Epoch 74/1000, LR 0.000267
Train loss: 0.0007;  Loss pred: 0.0007; Loss self: 0.0000; time: 0.19s
Val loss: 0.3566 score: 0.8571 time: 0.06s
Test loss: 0.2980 score: 0.8542 time: 0.07s
Epoch 75/1000, LR 0.000267
Train loss: 0.0007;  Loss pred: 0.0007; Loss self: 0.0000; time: 0.19s
Val loss: 0.3564 score: 0.8571 time: 0.06s
Test loss: 0.2952 score: 0.8542 time: 0.06s
Epoch 76/1000, LR 0.000267
Train loss: 0.0008;  Loss pred: 0.0008; Loss self: 0.0000; time: 0.19s
Val loss: 0.3568 score: 0.8571 time: 0.06s
Test loss: 0.2929 score: 0.8542 time: 0.06s
     INFO: Early stopping counter 1 of 20
Epoch 77/1000, LR 0.000267
Train loss: 0.0006;  Loss pred: 0.0006; Loss self: 0.0000; time: 0.19s
Val loss: 0.3583 score: 0.8571 time: 0.06s
Test loss: 0.2914 score: 0.8542 time: 0.06s
     INFO: Early stopping counter 2 of 20
Epoch 78/1000, LR 0.000267
Train loss: 0.0007;  Loss pred: 0.0007; Loss self: 0.0000; time: 0.19s
Val loss: 0.3608 score: 0.8571 time: 0.06s
Test loss: 0.2902 score: 0.8542 time: 0.07s
     INFO: Early stopping counter 3 of 20
Epoch 79/1000, LR 0.000267
Train loss: 0.0007;  Loss pred: 0.0007; Loss self: 0.0000; time: 0.19s
Val loss: 0.3641 score: 0.8571 time: 0.06s
Test loss: 0.2893 score: 0.8542 time: 0.07s
     INFO: Early stopping counter 4 of 20
Epoch 80/1000, LR 0.000267
Train loss: 0.0006;  Loss pred: 0.0006; Loss self: 0.0000; time: 0.20s
Val loss: 0.3681 score: 0.8571 time: 0.07s
Test loss: 0.2888 score: 0.8542 time: 0.07s
     INFO: Early stopping counter 5 of 20
Epoch 81/1000, LR 0.000267
Train loss: 0.0007;  Loss pred: 0.0007; Loss self: 0.0000; time: 0.20s
Val loss: 0.3726 score: 0.8571 time: 0.08s
Test loss: 0.2891 score: 0.8542 time: 0.09s
     INFO: Early stopping counter 6 of 20
Epoch 82/1000, LR 0.000267
Train loss: 0.0006;  Loss pred: 0.0006; Loss self: 0.0000; time: 0.21s
Val loss: 0.3780 score: 0.8571 time: 0.07s
Test loss: 0.2902 score: 0.8542 time: 0.07s
     INFO: Early stopping counter 7 of 20
Epoch 83/1000, LR 0.000266
Train loss: 0.0006;  Loss pred: 0.0006; Loss self: 0.0000; time: 0.20s
Val loss: 0.3849 score: 0.8571 time: 0.07s
Test loss: 0.2925 score: 0.8542 time: 0.07s
     INFO: Early stopping counter 8 of 20
Epoch 84/1000, LR 0.000266
Train loss: 0.0006;  Loss pred: 0.0006; Loss self: 0.0000; time: 0.20s
Val loss: 0.3923 score: 0.8571 time: 0.07s
Test loss: 0.2958 score: 0.8542 time: 0.07s
     INFO: Early stopping counter 9 of 20
Epoch 85/1000, LR 0.000266
Train loss: 0.0006;  Loss pred: 0.0006; Loss self: 0.0000; time: 0.20s
Val loss: 0.4001 score: 0.8571 time: 0.07s
Test loss: 0.2995 score: 0.8542 time: 0.07s
     INFO: Early stopping counter 10 of 20
Epoch 86/1000, LR 0.000266
Train loss: 0.0005;  Loss pred: 0.0005; Loss self: 0.0000; time: 0.20s
Val loss: 0.4083 score: 0.8571 time: 0.07s
Test loss: 0.3038 score: 0.8542 time: 0.16s
     INFO: Early stopping counter 11 of 20
Epoch 87/1000, LR 0.000266
Train loss: 0.0005;  Loss pred: 0.0005; Loss self: 0.0000; time: 0.21s
Val loss: 0.4170 score: 0.8571 time: 0.07s
Test loss: 0.3086 score: 0.8542 time: 0.07s
     INFO: Early stopping counter 12 of 20
Epoch 88/1000, LR 0.000266
Train loss: 0.0005;  Loss pred: 0.0005; Loss self: 0.0000; time: 0.21s
Val loss: 0.4264 score: 0.8571 time: 0.07s
Test loss: 0.3140 score: 0.8542 time: 0.07s
     INFO: Early stopping counter 13 of 20
Epoch 89/1000, LR 0.000266
Train loss: 0.0005;  Loss pred: 0.0005; Loss self: 0.0000; time: 0.20s
Val loss: 0.4362 score: 0.8571 time: 0.07s
Test loss: 0.3197 score: 0.8542 time: 0.07s
     INFO: Early stopping counter 14 of 20
Epoch 90/1000, LR 0.000266
Train loss: 0.0005;  Loss pred: 0.0005; Loss self: 0.0000; time: 0.20s
Val loss: 0.4463 score: 0.8571 time: 0.07s
Test loss: 0.3257 score: 0.8542 time: 0.07s
     INFO: Early stopping counter 15 of 20
Epoch 91/1000, LR 0.000266
Train loss: 0.0005;  Loss pred: 0.0005; Loss self: 0.0000; time: 0.20s
Val loss: 0.4564 score: 0.8571 time: 0.07s
Test loss: 0.3322 score: 0.8542 time: 0.07s
     INFO: Early stopping counter 16 of 20
Epoch 92/1000, LR 0.000266
Train loss: 0.0006;  Loss pred: 0.0006; Loss self: 0.0000; time: 0.21s
Val loss: 0.4665 score: 0.8571 time: 0.08s
Test loss: 0.3383 score: 0.8542 time: 0.07s
     INFO: Early stopping counter 17 of 20
Epoch 93/1000, LR 0.000265
Train loss: 0.0005;  Loss pred: 0.0005; Loss self: 0.0000; time: 0.22s
Val loss: 0.4767 score: 0.8571 time: 0.07s
Test loss: 0.3444 score: 0.8542 time: 0.07s
     INFO: Early stopping counter 18 of 20
Epoch 94/1000, LR 0.000265
Train loss: 0.0005;  Loss pred: 0.0005; Loss self: 0.0000; time: 0.21s
Val loss: 0.4871 score: 0.8571 time: 0.07s
Test loss: 0.3509 score: 0.8542 time: 0.07s
     INFO: Early stopping counter 19 of 20
Epoch 95/1000, LR 0.000265
Train loss: 0.0005;  Loss pred: 0.0005; Loss self: 0.0000; time: 0.21s
Val loss: 0.4978 score: 0.8571 time: 0.07s
Test loss: 0.3578 score: 0.8542 time: 0.07s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 074,   Train_Loss: 0.0007,   Val_Loss: 0.3564,   Val_Precision: 1.0000,   Val_Recall: 0.7200,   Val_accuracy: 0.8372,   Val_Score: 0.8571,   Val_Loss: 0.3564,   Test_Precision: 1.0000,   Test_Recall: 0.7083,   Test_accuracy: 0.8293,   Test_Score: 0.8542,   Test_loss: 0.2952


[0.06536245299503207, 0.06498479307629168, 0.06517278694082052, 0.0655033509247005, 0.06540117296390235, 0.06527594395447522, 0.0652049119817093, 0.06538243195973337, 0.06516556406859308, 0.06561034999322146, 0.06535083602648228, 0.06570164498407394, 0.06521763594355434, 0.0657589309848845, 0.06525927397888154, 0.0654908720171079, 0.06568012409843504, 0.06520061905030161, 0.0653998430352658, 0.06531489407643676, 0.06531110405921936, 0.06577758700586855, 0.06565138604491949, 0.06502032198477536, 0.06543521804269403, 0.06494482897687703, 0.07327295001596212, 0.07253202295396477, 0.07728287391364574, 0.07345235301181674, 0.07374285091646016, 0.07727687095757574, 0.07325936097186059, 0.07916854904033244, 0.17911807808559388, 0.08587669197004288, 0.08059052296448499, 0.08037801808677614, 0.08469690498895943, 0.07757169601973146, 0.08111767307855189, 0.07848339108750224, 0.07853813108522445, 0.08225323399528861, 0.16367818601429462, 0.07856252300553024, 0.07798082998488098, 0.07787705992814153, 0.08467481599655002, 0.09126773197203875, 0.08311981498263776, 0.08289293700363487, 0.08202750002965331, 0.07809172593988478, 0.10024787893053144, 0.07367166399490088, 0.07408946205396205, 0.07794106402434409, 0.07658519805409014, 0.0766402690205723, 0.07651966204866767, 0.07325204007793218, 0.07700520392972976, 0.07818336097989231, 0.07300509593915194, 0.07793125894386321, 0.07791359710972756, 0.07275302207563072, 0.07754558604210615, 0.07298973191063851, 0.07687668001744896, 0.07694948697462678, 0.07675380108412355, 0.07727717398665845, 0.07714721607044339, 0.07711328892037272, 0.07606785104144365, 0.07264879404101521, 0.0780080029508099, 0.07657565199770033, 0.07645220297854394, 0.07632875198032707, 0.07593597797676921, 0.072225849959068, 0.07603982090950012, 0.07191821793094277, 0.07230732194148004, 0.07207576907239854, 0.07213277707342058, 0.0725681979674846, 0.077641248004511, 0.07733262702822685, 0.07811200909782201, 0.07434580894187093, 0.07275242393370718, 0.07307766296435148, 0.07293155300430954, 0.07317119103390723, 0.0732118779560551, 0.07679623609874398, 0.0726124100619927, 0.07683658401947469, 0.0768393169855699, 0.07298109796829522, 0.07280037901364267, 0.07660662394482642, 0.07292128494009376, 0.07222495297901332, 0.07329606101848185, 0.07758529996499419, 0.07629835396073759, 0.07727547292597592, 0.0731858640210703, 0.07317148894071579, 0.07930335192941129, 0.07882503792643547, 0.07295401603914797, 0.07804477901663631, 0.07404278009198606, 0.0783001130912453, 0.07348293205723166, 0.07800611096899956, 0.07337203889619559, 0.07021856494247913, 0.06944389699492604, 0.06957572302781045, 0.06987098208628595, 0.0690721549326554, 0.06847305002156645, 0.06917482102289796, 0.06906565802637488, 0.06861454504542053, 0.06879439402837306, 0.06971895205788314, 0.06869914499111474, 0.06868655700236559, 0.0688151529757306, 0.0690337719861418, 0.06891521601937711, 0.06934725807514042, 0.0694212120724842, 0.0690768079366535, 0.06948712700977921, 0.07024215301498771, 0.0700375159503892, 0.06957594200503081, 0.06976386497262865, 0.0696423560148105, 0.07058206398505718, 0.07022354891523719, 0.06961923697963357, 0.06987709598615766, 0.07013661100063473, 0.06954148702789098, 0.0693562370724976, 0.06901330803520977, 0.07011698989663273, 0.06981983000878245, 0.06883808702696115, 0.06969586201012135, 0.07014808699022979, 0.06902687402907759, 0.07025055098347366, 0.07044583803508431, 0.07020106096751988, 0.0701686559477821, 0.07047492603305727, 0.06856998894363642, 0.06952619703952223, 0.069479821017012, 0.07219814194831997, 0.069323402014561, 0.06990092701744288, 0.06940976204350591, 0.06949022703338414, 0.0697601210558787, 0.06937743199523538, 0.0691806849790737, 0.06977218703832477, 0.07079517201054841, 0.0692873599473387, 0.0691024730913341, 0.06959493597969413, 0.072208630037494, 0.07037967105861753, 0.09781405795365572, 0.07004890404641628, 0.06944259000010788, 0.06897446792572737, 0.0694254549453035, 0.06942463398445398, 0.06948695296887308, 0.0692996559664607, 0.06910144002176821, 0.06994810199830681, 0.06935661903116852, 0.06945687893312424, 0.06926592392846942, 0.06917573395185173, 0.0690895909210667, 0.06981693499255925, 0.06950246402993798, 0.06972222495824099, 0.09799219504930079, 0.0706814939621836, 0.07190187205560505, 0.07121359894517809, 0.07132433296646923, 0.161228229990229, 0.07510568096768111, 0.07056093006394804, 0.07118963706307113, 0.07183754094876349, 0.07090392406098545, 0.0746447240235284, 0.07403665909077972, 0.07505847397260368, 0.075544886989519]
[0.0013339276121435116, 0.0013262202668630956, 0.0013300568763432758, 0.0013368030800959285, 0.001334717815589844, 0.0013321621215199024, 0.0013307124894226389, 0.0013343353461170075, 0.001329909470787614, 0.00133898673455554, 0.0013336905311526997, 0.001340849897634162, 0.001330972162113354, 0.0013420189996915205, 0.001331821917936358, 0.001336548408512406, 0.0013404106958864294, 0.0013306248785775838, 0.0013346906741890981, 0.0013329570219680971, 0.001332879674677946, 0.001342399734813644, 0.001339824204998357, 0.0013269453466280686, 0.0013354126131162047, 0.0013254046729974905, 0.0014953663268563698, 0.0014802453664074444, 0.00157720150844175, 0.001499027612486056, 0.001504956141152248, 0.0015770789991341988, 0.0014950889994257263, 0.0016156846742924986, 0.0036554709813386506, 0.0017525855504090385, 0.0016447045502956121, 0.0016403677160566558, 0.0017285082650808047, 0.0015830958371373769, 0.001655462715888814, 0.0016017018589286171, 0.0016028190017392744, 0.0016786374284752778, 0.00334037114314887, 0.0016033167960312292, 0.0015914455098955302, 0.0015893277536355415, 0.0017280574693173474, 0.0018626067749395662, 0.0016963227547477095, 0.0016916925919109157, 0.0016740306128500675, 0.00159370869265071, 0.0020458750802149275, 0.0015035033468347118, 0.0015120298378359601, 0.0015906339596804915, 0.0015629632255936764, 0.0015640871228688226, 0.0015616257560952585, 0.0014949395934271874, 0.0015715347740761175, 0.0015955787955080063, 0.0014898999171255498, 0.0015904338559972085, 0.0015900734104026034, 0.0014847555525638924, 0.0015825629804511459, 0.001489586365523235, 0.0015689118370907952, 0.0015703976933597302, 0.0015664041037576236, 0.0015770851834011929, 0.001574432981029457, 0.0015737405902116882, 0.0015524051232947682, 0.0014826284498166368, 0.00159200006022061, 0.0015627684081163332, 0.0015602490403784476, 0.0015577296322515728, 0.001549713836260596, 0.0014739969379401632, 0.0015518330797857167, 0.0014677187332845464, 0.0014756596314587764, 0.001470934062702011, 0.0014720974912942977, 0.0014809836319894815, 0.0015845152653981836, 0.0015782168781270786, 0.0015941226346494288, 0.0015172614069769578, 0.0014847433455858607, 0.0014913808768234995, 0.0014883990409042761, 0.0014932896129368823, 0.001494119958286839, 0.0015672701244641627, 0.001481885919632504, 0.0015680935514178506, 0.0015681493262361204, 0.0014894101626182698, 0.0014857220206865851, 0.001563400488669927, 0.001488189488573342, 0.0014739786322247616, 0.0014958379799690173, 0.001583373468673351, 0.0015571092645048487, 0.0015770504678770596, 0.001493589061654496, 0.0014932956926676693, 0.001618435753661455, 0.0016086742433966423, 0.0014888574701866933, 0.0015927505921762512, 0.0015110771447344093, 0.0015979614916580673, 0.001499651674637381, 0.00159196144834693, 0.001497388548901951, 0.0014628867696349819, 0.001446747854060959, 0.0014494942297460511, 0.0014556454601309572, 0.001439003227763654, 0.001426521875449301, 0.0014411421046437074, 0.0014388678755494766, 0.001429469688446261, 0.001433216542257772, 0.0014524781678725656, 0.0014312321873148903, 0.001430969937549283, 0.0014336490203277208, 0.001438203583044621, 0.0014357336670703564, 0.001444734543232092, 0.0014462752515100874, 0.0014391001653469477, 0.0014476484793704003, 0.0014633781878122438, 0.0014591149156331085, 0.0014494987917714752, 0.0014534138535964303, 0.0014508824169752188, 0.0014704596663553577, 0.0014629906024007748, 0.0014504007704090327, 0.001455772833044951, 0.0014611793958465569, 0.0014487809797477287, 0.0014449216056770335, 0.001437777250733537, 0.0014607706228465152, 0.0014545797918496344, 0.0014341268130616907, 0.0014519971252108614, 0.0014614184789631206, 0.0014380598756057832, 0.0014635531454890345, 0.0014676216257309231, 0.0014625221034899976, 0.001461846998912127, 0.0014682276256886932, 0.0014285414363257587, 0.0014484624383233797, 0.00144749627118775, 0.0015041279572566661, 0.0014442375419700209, 0.0014562693128633934, 0.0014460367092397064, 0.001447713063195503, 0.0014533358553308062, 0.0014453631665674038, 0.0014412642703973688, 0.0014535872299650994, 0.0014748994168864253, 0.0014434866655695562, 0.0014396348560694605, 0.001449894499576961, 0.0015043464591144584, 0.0014662431470545318, 0.002037792874034494, 0.0014593521676336725, 0.0014467206250022475, 0.001436968081785987, 0.0014463636446938228, 0.0014463465413427912, 0.0014476448535181892, 0.001443742832634598, 0.0014396133337868378, 0.0014572521249647252, 0.0014449295631493442, 0.0014470183111067552, 0.0014430400818431128, 0.0014411611239969109, 0.001439366477522223, 0.0014545194790116511, 0.001447968000623708, 0.0014525463532966871, 0.0020415040635271, 0.0014725311242121582, 0.0014979556678251054, 0.0014836166446912102, 0.001485923603468109, 0.003358921458129771, 0.0015647016868266899, 0.0014700193763322507, 0.0014831174388139818, 0.0014966154364325728, 0.0014771650846038635, 0.0015550984171568416, 0.0015424303977245775, 0.0015637182077625766, 0.0015738518122816458]
[749.6658670953534, 754.0225594390114, 751.8475471133979, 748.0533332764616, 749.2220365381696, 750.6593858554324, 751.4771281915854, 749.436791066097, 751.9308809852814, 746.8333884068968, 749.7991300393404, 745.7956343692398, 751.3305149915177, 745.14593327655, 750.8511359758128, 748.195870520703, 746.0400033130802, 751.5266068593153, 749.2372722298056, 750.2117349016328, 750.2552698476871, 744.9345929279575, 746.3665727708109, 753.6105405856564, 748.8322262184461, 754.4865506912947, 668.7324584219089, 675.5636752486516, 634.0343923383539, 667.0991192360722, 664.4711913228011, 634.0836448579877, 668.8565031139328, 618.9326518417931, 273.5625600928161, 570.5855555904866, 608.0119373538939, 609.6194104599541, 578.533536808545, 631.6736969053269, 604.060719943851, 624.3359177149878, 623.9007641629313, 595.7212576323343, 299.3679316296959, 623.7070568183095, 628.3595597725772, 629.1968398038282, 578.6844579856713, 536.8819728643185, 589.5104556023762, 591.1239457934919, 597.3606410324132, 627.4672432995056, 488.7883965500699, 665.1132517298848, 661.3626100336849, 628.680152283979, 639.8103190304812, 639.3505741328634, 640.3582907728377, 668.9233494093727, 636.3206315863329, 626.731818456898, 671.1860229707851, 628.7592509611134, 628.9017811742428, 673.5115408547817, 631.8863845247581, 671.327304777483, 637.3844446570573, 636.7813734243243, 638.4048647479376, 634.081158408557, 635.1492963175486, 635.4287397934415, 644.1617494005923, 674.4778168283998, 628.1406797569001, 639.8900789179246, 640.9233232134814, 641.9599263542162, 645.2804231347409, 678.4274609128088, 644.399203126978, 681.3294518372344, 677.6630455164252, 679.8401270027457, 679.3028355212941, 675.226908927176, 631.1078358394377, 633.6264767277946, 627.3043103863304, 659.0822091708201, 673.5170782027736, 670.5195269299052, 671.8628355151657, 669.6624628850664, 669.2903032676186, 638.0521037124294, 674.8157781592203, 637.7170539957979, 637.6943721298563, 671.4067253590348, 673.0734189009852, 639.6313722856493, 671.9574406876462, 678.4358864759402, 668.5216001941017, 631.5629381095177, 642.215689544429, 634.095116401789, 669.5282026853279, 669.6597364541843, 617.8805663046297, 621.629894370998, 671.6559644051129, 627.8446888747674, 661.779581197863, 625.7973081456337, 666.8215138970869, 628.1559148548388, 667.8293357681341, 683.5799056748042, 691.2054489612982, 689.8958129520794, 686.9804683827577, 694.9254739018854, 701.0057239290757, 693.8940974507364, 694.9908445333236, 699.5601292441072, 697.7312712457824, 688.478506678481, 698.6986520168209, 698.8267005193878, 697.5207919239591, 695.3118541695183, 696.5080104588758, 692.1686787960695, 691.4313156889591, 694.8786638203974, 690.7754294294648, 683.3503521704146, 685.3469793817446, 689.8936416344787, 688.0352746917398, 689.2357287538072, 680.0594554752892, 683.5313899891053, 689.4646089563138, 686.9203609936593, 684.3786620879872, 690.2354558617457, 692.0790692526459, 695.5180292982184, 684.570174372319, 687.4837706416961, 697.2884063614421, 688.7065977178008, 684.2667000553477, 695.3813377059489, 683.268662352441, 681.3745331000881, 683.7503499015249, 684.0661168673446, 681.0933008639827, 700.0146965089251, 690.3872503297478, 690.8480663507653, 664.8370540388532, 692.4068727890456, 686.6861721021556, 691.5453761376345, 690.7446132956234, 688.0722004704008, 691.8676379272223, 693.8352809677933, 687.9532094018258, 678.0123366724506, 692.767050678248, 694.6205808952373, 689.7053546253002, 664.7404884302086, 682.0151228047365, 490.72700800065354, 685.2355601194547, 691.2184582966365, 695.9096814155499, 691.3890595001006, 691.3972353206569, 690.7771595842138, 692.6441312094074, 694.6309655034559, 686.2230515012674, 692.0752578557648, 691.0762582093021, 692.9814442317894, 693.8849399618855, 694.750097085376, 687.512277717657, 690.6229968958243, 688.4461881236412, 489.83492997427754, 679.1027935216144, 667.5764987437236, 674.028566326939, 672.982108680436, 297.7146124032309, 639.0994580111055, 680.2631421737002, 674.2554391374962, 668.1743189711201, 676.9724050634283, 643.046117832389, 648.3274716805497, 639.5014108269772, 635.3838348670699]
Elapsed: 0.07397989547755911~0.012457071595250896
Time per graph: 0.0015230544829995965~0.0002536630519956269
Speed: 666.4802679608364~63.47726390634043
Total Time: 0.0763
best val loss: 0.35641226172447205 test_score: 0.8542

Testing...
Test loss: 0.4324 score: 0.8958 time: 0.07s
test Score 0.8958
Epoch Time List: [0.3231512511847541, 0.32157175603788346, 0.32225958607159555, 0.3230605380376801, 0.321877604117617, 0.32218650286085904, 0.3213745200773701, 0.32415115099865943, 0.32777362992055714, 0.32252413290552795, 0.32294920191634446, 0.3258672798983753, 0.3343243838753551, 0.3227181170368567, 0.3214159399503842, 0.32817996200174093, 0.32379537587985396, 0.3227470818674192, 0.3221816929290071, 0.3228515909286216, 0.32357651996426284, 0.3286313018761575, 0.32418785407207906, 0.3229531500255689, 0.3227580130333081, 0.3291280370904133, 0.3238214390585199, 0.3279879349283874, 0.32657654606737196, 0.3237398079363629, 0.32298798696137965, 0.32721057487651706, 0.323597565991804, 0.338247584993951, 0.4342440019827336, 0.36332764907274395, 0.37469503097236156, 0.3644465639954433, 0.3625387530773878, 0.4044134601717815, 0.35743730410467833, 0.34946094697806984, 0.3491337421583012, 0.354283231892623, 0.4390511499950662, 0.3609267509309575, 0.3514302971307188, 0.35706403595395386, 0.3610042390646413, 0.3675999710103497, 0.42623469210229814, 0.3567572260508314, 0.3565785310929641, 0.3534884660039097, 0.38192321301903576, 0.3272409290075302, 0.3285410129465163, 0.3381954290671274, 0.33186440891586244, 0.3302194429561496, 0.33187942195218056, 0.32751788292080164, 0.3302817781222984, 0.3348036460811272, 0.3415346989640966, 0.3312237940263003, 0.33321859303396195, 0.32683467911556363, 0.3286595968529582, 0.32878469803836197, 0.33553923503495753, 0.34774040698539466, 0.3481845089700073, 0.3498724380042404, 0.3542920059990138, 0.34743603703100234, 0.339397381991148, 0.3237173790112138, 0.33472505607642233, 0.33237642294261605, 0.3287894930690527, 0.32806656893808395, 0.32358424086123705, 0.32457298098597676, 0.3307465489488095, 0.3228722829371691, 0.31878745404537767, 0.32277932297438383, 0.32617373997345567, 0.33367397694382817, 0.33760932506993413, 0.3538501929724589, 0.3414208439644426, 0.3272322209086269, 0.3347716168500483, 0.3269505968783051, 0.32615990191698074, 0.32374683406669647, 0.3274386040866375, 0.33229257888160646, 0.3231795390602201, 0.3279637289233506, 0.3291340119903907, 0.3260237129870802, 0.3299458469264209, 0.32801481196656823, 0.32550369994714856, 0.3236278749536723, 0.3253855051007122, 0.3365278630517423, 0.3283712570555508, 0.32936998503282666, 0.32444986200425774, 0.32393405598122627, 0.33284801710397005, 0.33573603990953416, 0.32874650694429874, 0.33158406999427825, 0.32758944015949965, 0.3317527030594647, 0.32751128589734435, 0.33184226194862276, 0.3284247010014951, 0.32048376998864114, 0.32238102797418833, 0.32374216604512185, 0.3222180249867961, 0.324051013099961, 0.31853605702053756, 0.32237157283816487, 0.32768115994986147, 0.3200342959025875, 0.32597863394767046, 0.32710145611781627, 0.33240428008139133, 0.319941324996762, 0.3209290271624923, 0.32480924401897937, 0.3260176038602367, 0.3235517730936408, 0.32571018498856574, 0.32250530179589987, 0.3227475950261578, 0.32458123192191124, 0.32405638811178505, 0.32421030302066356, 0.32487938494887203, 0.3303988759871572, 0.3263745199656114, 0.3294951500138268, 0.3388380748219788, 0.3238144429633394, 0.3250724798999727, 0.32389913697261363, 0.3298209039494395, 0.3222592781530693, 0.32789760490413755, 0.3232460079016164, 0.3222043509595096, 0.32289632817264646, 0.3389669479802251, 0.32405376399401575, 0.3238958709407598, 0.3255075808847323, 0.32961293403059244, 0.330698783043772, 0.33017679885961115, 0.3238108829827979, 0.3267523878021166, 0.32287994504440576, 0.3354570909868926, 0.32562833302654326, 0.3237870748853311, 0.32661624997854233, 0.32284492696635425, 0.3242165071424097, 0.3287300409283489, 0.32293271692469716, 0.3234850170556456, 0.32741203205659986, 0.32881257007829845, 0.3233775170519948, 0.32332329906057566, 0.3264885488897562, 0.3303544451482594, 0.3805333600612357, 0.33547100704163313, 0.3235381030244753, 0.32302223588339984, 0.3223135400330648, 0.3235971669200808, 0.32291813392657787, 0.32301342592108995, 0.32221207791008055, 0.32570366584695876, 0.338190607028082, 0.32313459494616836, 0.3237273389240727, 0.323748332913965, 0.32186309492681175, 0.32301410392392427, 0.32335891493130475, 0.32705669198185205, 0.36968896503094584, 0.34003999806009233, 0.3344175301026553, 0.33628263999707997, 0.33690997085068375, 0.42336560296826065, 0.3488276720745489, 0.34701344603672624, 0.3326348669361323, 0.3349708649329841, 0.3311465709703043, 0.3680784731404856, 0.354494625935331, 0.3505514890421182, 0.3510882309637964]
Total Epoch List: [26, 97, 95]
Total Time List: [0.06523587100673467, 0.07394551194738597, 0.0762690199771896]
========================training times:2========================
========================k_idx:0========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: GPSTransformer
GPS(
  (x_lin): Linear(in_features=14887, out_features=64, bias=True)
  (pe_lin): Linear(in_features=20, out_features=8, bias=True)
  (pe_norm): BatchNorm1d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (convs): ModuleList(
    (0-1): 2 x GPSConv(64, conv=GINEConv(nn=Sequential(
      (0): Linear(in_features=64, out_features=64, bias=True)
      (1): ReLU()
      (2): Linear(in_features=64, out_features=64, bias=True)
    )), heads=1, attn_type=multihead)
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=32, bias=True)
    (1): ReLU()
    (2): Linear(in_features=32, out_features=16, bias=True)
    (3): ReLU()
    (4): Linear(in_features=16, out_features=2, bias=True)
  )
)
Total number of parameters: 1039906
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x74b5c4fac2b0>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 1.1026;  Loss pred: 1.1026; Loss self: 0.0000; time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 2.1730 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 1.7712 score: 0.4898 time: 0.06s
Epoch 2/1000, LR 0.000000
Train loss: 1.1129;  Loss pred: 1.1129; Loss self: 0.0000; time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 1.9635 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 1.5860 score: 0.4898 time: 0.06s
Epoch 3/1000, LR 0.000030
Train loss: 1.0253;  Loss pred: 1.0253; Loss self: 0.0000; time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 1.7701 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 1.4199 score: 0.4898 time: 0.06s
Epoch 4/1000, LR 0.000060
Train loss: 0.9010;  Loss pred: 0.9010; Loss self: 0.0000; time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 1.5827 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 1.2760 score: 0.4898 time: 0.06s
Epoch 5/1000, LR 0.000090
Train loss: 0.7552;  Loss pred: 0.7552; Loss self: 0.0000; time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 1.4336 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 1.1751 score: 0.4898 time: 0.06s
Epoch 6/1000, LR 0.000120
Train loss: 0.6116;  Loss pred: 0.6116; Loss self: 0.0000; time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 1.3161 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 1.1110 score: 0.4898 time: 0.06s
Epoch 7/1000, LR 0.000150
Train loss: 0.4887;  Loss pred: 0.4887; Loss self: 0.0000; time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 1.2128 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 1.0658 score: 0.4898 time: 0.06s
Epoch 8/1000, LR 0.000180
Train loss: 0.4126;  Loss pred: 0.4126; Loss self: 0.0000; time: 0.20s
Val loss: 1.1152 score: 0.5306 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 1.0284 score: 0.4898 time: 0.06s
Epoch 9/1000, LR 0.000210
Train loss: 0.3478;  Loss pred: 0.3478; Loss self: 0.0000; time: 0.20s
Val loss: 1.0436 score: 0.5306 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 1.0050 score: 0.4898 time: 0.06s
Epoch 10/1000, LR 0.000240
Train loss: 0.3025;  Loss pred: 0.3025; Loss self: 0.0000; time: 0.20s
Val loss: 0.9969 score: 0.5510 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.9917 score: 0.4898 time: 0.06s
Epoch 11/1000, LR 0.000270
Train loss: 0.2602;  Loss pred: 0.2602; Loss self: 0.0000; time: 0.20s
Val loss: 0.9741 score: 0.5510 time: 0.07s
Test loss: 0.9910 score: 0.5102 time: 0.06s
Epoch 12/1000, LR 0.000270
Train loss: 0.2211;  Loss pred: 0.2211; Loss self: 0.0000; time: 0.20s
Val loss: 0.9678 score: 0.5510 time: 0.07s
Test loss: 0.9912 score: 0.5102 time: 0.06s
Epoch 13/1000, LR 0.000270
Train loss: 0.1955;  Loss pred: 0.1955; Loss self: 0.0000; time: 0.20s
Val loss: 0.9643 score: 0.5714 time: 0.07s
Test loss: 0.9942 score: 0.5306 time: 0.06s
Epoch 14/1000, LR 0.000270
Train loss: 0.1687;  Loss pred: 0.1687; Loss self: 0.0000; time: 0.20s
Val loss: 0.9819 score: 0.5714 time: 0.07s
Test loss: 0.9978 score: 0.5306 time: 0.06s
     INFO: Early stopping counter 1 of 20
Epoch 15/1000, LR 0.000270
Train loss: 0.1536;  Loss pred: 0.1536; Loss self: 0.0000; time: 0.20s
Val loss: 1.0132 score: 0.5714 time: 0.07s
Test loss: 1.0114 score: 0.5306 time: 0.06s
     INFO: Early stopping counter 2 of 20
Epoch 16/1000, LR 0.000270
Train loss: 0.1337;  Loss pred: 0.1337; Loss self: 0.0000; time: 0.21s
Val loss: 1.0470 score: 0.5714 time: 0.07s
Test loss: 1.0273 score: 0.5510 time: 0.06s
     INFO: Early stopping counter 3 of 20
Epoch 17/1000, LR 0.000270
Train loss: 0.1189;  Loss pred: 0.1189; Loss self: 0.0000; time: 0.20s
Val loss: 1.0780 score: 0.5714 time: 0.07s
Test loss: 1.0421 score: 0.5510 time: 0.06s
     INFO: Early stopping counter 4 of 20
Epoch 18/1000, LR 0.000270
Train loss: 0.1075;  Loss pred: 0.1075; Loss self: 0.0000; time: 0.20s
Val loss: 1.0958 score: 0.5714 time: 0.07s
Test loss: 1.0508 score: 0.5510 time: 0.06s
     INFO: Early stopping counter 5 of 20
Epoch 19/1000, LR 0.000270
Train loss: 0.1000;  Loss pred: 0.1000; Loss self: 0.0000; time: 0.20s
Val loss: 1.0929 score: 0.5714 time: 0.07s
Test loss: 1.0475 score: 0.5510 time: 0.06s
     INFO: Early stopping counter 6 of 20
Epoch 20/1000, LR 0.000270
Train loss: 0.0936;  Loss pred: 0.0936; Loss self: 0.0000; time: 0.20s
Val loss: 1.0814 score: 0.5714 time: 0.07s
Test loss: 1.0399 score: 0.5510 time: 0.06s
     INFO: Early stopping counter 7 of 20
Epoch 21/1000, LR 0.000270
Train loss: 0.0853;  Loss pred: 0.0853; Loss self: 0.0000; time: 0.20s
Val loss: 1.0664 score: 0.5714 time: 0.07s
Test loss: 1.0301 score: 0.5510 time: 0.06s
     INFO: Early stopping counter 8 of 20
Epoch 22/1000, LR 0.000270
Train loss: 0.0765;  Loss pred: 0.0765; Loss self: 0.0000; time: 0.20s
Val loss: 1.0628 score: 0.5714 time: 0.07s
Test loss: 1.0287 score: 0.5510 time: 0.06s
     INFO: Early stopping counter 9 of 20
Epoch 23/1000, LR 0.000270
Train loss: 0.0764;  Loss pred: 0.0764; Loss self: 0.0000; time: 0.20s
Val loss: 1.0618 score: 0.5714 time: 0.07s
Test loss: 1.0323 score: 0.5510 time: 0.06s
     INFO: Early stopping counter 10 of 20
Epoch 24/1000, LR 0.000270
Train loss: 0.0646;  Loss pred: 0.0646; Loss self: 0.0000; time: 0.20s
Val loss: 1.0643 score: 0.5714 time: 0.07s
Test loss: 1.0418 score: 0.5510 time: 0.06s
     INFO: Early stopping counter 11 of 20
Epoch 25/1000, LR 0.000270
Train loss: 0.0599;  Loss pred: 0.0599; Loss self: 0.0000; time: 0.20s
Val loss: 1.0639 score: 0.5714 time: 0.07s
Test loss: 1.0466 score: 0.5510 time: 0.06s
     INFO: Early stopping counter 12 of 20
Epoch 26/1000, LR 0.000270
Train loss: 0.0554;  Loss pred: 0.0554; Loss self: 0.0000; time: 0.20s
Val loss: 1.0512 score: 0.5714 time: 0.07s
Test loss: 1.0426 score: 0.5510 time: 0.06s
     INFO: Early stopping counter 13 of 20
Epoch 27/1000, LR 0.000270
Train loss: 0.0502;  Loss pred: 0.0502; Loss self: 0.0000; time: 0.20s
Val loss: 1.0265 score: 0.5714 time: 0.07s
Test loss: 1.0295 score: 0.5510 time: 0.06s
     INFO: Early stopping counter 14 of 20
Epoch 28/1000, LR 0.000270
Train loss: 0.0461;  Loss pred: 0.0461; Loss self: 0.0000; time: 0.20s
Val loss: 0.9981 score: 0.5714 time: 0.07s
Test loss: 1.0157 score: 0.5510 time: 0.06s
     INFO: Early stopping counter 15 of 20
Epoch 29/1000, LR 0.000270
Train loss: 0.0441;  Loss pred: 0.0441; Loss self: 0.0000; time: 0.20s
Val loss: 0.9660 score: 0.5714 time: 0.07s
Test loss: 1.0009 score: 0.5510 time: 0.06s
     INFO: Early stopping counter 16 of 20
Epoch 30/1000, LR 0.000270
Train loss: 0.0407;  Loss pred: 0.0407; Loss self: 0.0000; time: 0.20s
Val loss: 0.9362 score: 0.5714 time: 0.07s
Test loss: 0.9861 score: 0.5714 time: 0.06s
Epoch 31/1000, LR 0.000270
Train loss: 0.0365;  Loss pred: 0.0365; Loss self: 0.0000; time: 0.21s
Val loss: 0.9055 score: 0.5918 time: 0.07s
Test loss: 0.9726 score: 0.5714 time: 0.06s
Epoch 32/1000, LR 0.000270
Train loss: 0.0321;  Loss pred: 0.0321; Loss self: 0.0000; time: 0.20s
Val loss: 0.8829 score: 0.5918 time: 0.07s
Test loss: 0.9637 score: 0.5714 time: 0.06s
Epoch 33/1000, LR 0.000270
Train loss: 0.0320;  Loss pred: 0.0320; Loss self: 0.0000; time: 0.20s
Val loss: 0.8654 score: 0.5918 time: 0.07s
Test loss: 0.9609 score: 0.5714 time: 0.06s
Epoch 34/1000, LR 0.000270
Train loss: 0.0307;  Loss pred: 0.0307; Loss self: 0.0000; time: 0.20s
Val loss: 0.8486 score: 0.5918 time: 0.07s
Test loss: 0.9572 score: 0.5714 time: 0.06s
Epoch 35/1000, LR 0.000270
Train loss: 0.0285;  Loss pred: 0.0285; Loss self: 0.0000; time: 0.20s
Val loss: 0.8314 score: 0.5918 time: 0.07s
Test loss: 0.9522 score: 0.5714 time: 0.06s
Epoch 36/1000, LR 0.000270
Train loss: 0.0244;  Loss pred: 0.0244; Loss self: 0.0000; time: 0.20s
Val loss: 0.8079 score: 0.5918 time: 0.07s
Test loss: 0.9374 score: 0.5714 time: 0.06s
Epoch 37/1000, LR 0.000270
Train loss: 0.0236;  Loss pred: 0.0236; Loss self: 0.0000; time: 0.20s
Val loss: 0.7741 score: 0.5918 time: 0.07s
Test loss: 0.9091 score: 0.5714 time: 0.06s
Epoch 38/1000, LR 0.000270
Train loss: 0.0197;  Loss pred: 0.0197; Loss self: 0.0000; time: 0.20s
Val loss: 0.7392 score: 0.6122 time: 0.07s
Test loss: 0.8737 score: 0.5510 time: 0.06s
Epoch 39/1000, LR 0.000269
Train loss: 0.0186;  Loss pred: 0.0186; Loss self: 0.0000; time: 0.20s
Val loss: 0.7133 score: 0.6531 time: 0.07s
Test loss: 0.8505 score: 0.5510 time: 0.06s
Epoch 40/1000, LR 0.000269
Train loss: 0.0166;  Loss pred: 0.0166; Loss self: 0.0000; time: 0.20s
Val loss: 0.6982 score: 0.6531 time: 0.07s
Test loss: 0.8364 score: 0.5510 time: 0.06s
Epoch 41/1000, LR 0.000269
Train loss: 0.0152;  Loss pred: 0.0152; Loss self: 0.0000; time: 0.20s
Val loss: 0.6941 score: 0.6531 time: 0.07s
Test loss: 0.8329 score: 0.5714 time: 0.06s
Epoch 42/1000, LR 0.000269
Train loss: 0.0143;  Loss pred: 0.0143; Loss self: 0.0000; time: 0.20s
Val loss: 0.6980 score: 0.6531 time: 0.07s
Test loss: 0.8370 score: 0.5714 time: 0.06s
     INFO: Early stopping counter 1 of 20
Epoch 43/1000, LR 0.000269
Train loss: 0.0121;  Loss pred: 0.0121; Loss self: 0.0000; time: 0.20s
Val loss: 0.7115 score: 0.6327 time: 0.07s
Test loss: 0.8527 score: 0.5714 time: 0.06s
     INFO: Early stopping counter 2 of 20
Epoch 44/1000, LR 0.000269
Train loss: 0.0128;  Loss pred: 0.0128; Loss self: 0.0000; time: 0.20s
Val loss: 0.7242 score: 0.6122 time: 0.07s
Test loss: 0.8725 score: 0.5714 time: 0.06s
     INFO: Early stopping counter 3 of 20
Epoch 45/1000, LR 0.000269
Train loss: 0.0112;  Loss pred: 0.0112; Loss self: 0.0000; time: 0.20s
Val loss: 0.7350 score: 0.6122 time: 0.07s
Test loss: 0.8890 score: 0.5714 time: 0.06s
     INFO: Early stopping counter 4 of 20
Epoch 46/1000, LR 0.000269
Train loss: 0.0103;  Loss pred: 0.0103; Loss self: 0.0000; time: 0.20s
Val loss: 0.7373 score: 0.5918 time: 0.07s
Test loss: 0.8899 score: 0.5714 time: 0.06s
     INFO: Early stopping counter 5 of 20
Epoch 47/1000, LR 0.000269
Train loss: 0.0093;  Loss pred: 0.0093; Loss self: 0.0000; time: 0.20s
Val loss: 0.7307 score: 0.6327 time: 0.07s
Test loss: 0.8764 score: 0.5714 time: 0.06s
     INFO: Early stopping counter 6 of 20
Epoch 48/1000, LR 0.000269
Train loss: 0.0083;  Loss pred: 0.0083; Loss self: 0.0000; time: 0.20s
Val loss: 0.7201 score: 0.6327 time: 0.07s
Test loss: 0.8607 score: 0.5714 time: 0.06s
     INFO: Early stopping counter 7 of 20
Epoch 49/1000, LR 0.000269
Train loss: 0.0080;  Loss pred: 0.0080; Loss self: 0.0000; time: 0.20s
Val loss: 0.7053 score: 0.6327 time: 0.07s
Test loss: 0.8435 score: 0.5714 time: 0.06s
     INFO: Early stopping counter 8 of 20
Epoch 50/1000, LR 0.000269
Train loss: 0.0072;  Loss pred: 0.0072; Loss self: 0.0000; time: 0.20s
Val loss: 0.6926 score: 0.6327 time: 0.07s
Test loss: 0.8317 score: 0.5714 time: 0.06s
Epoch 51/1000, LR 0.000269
Train loss: 0.0085;  Loss pred: 0.0085; Loss self: 0.0000; time: 0.20s
Val loss: 0.6838 score: 0.6327 time: 0.07s
Test loss: 0.8236 score: 0.5714 time: 0.06s
Epoch 52/1000, LR 0.000269
Train loss: 0.0075;  Loss pred: 0.0075; Loss self: 0.0000; time: 0.20s
Val loss: 0.6727 score: 0.6327 time: 0.07s
Test loss: 0.8142 score: 0.5714 time: 0.06s
Epoch 53/1000, LR 0.000269
Train loss: 0.0059;  Loss pred: 0.0059; Loss self: 0.0000; time: 0.20s
Val loss: 0.6634 score: 0.6327 time: 0.07s
Test loss: 0.8039 score: 0.5714 time: 0.06s
Epoch 54/1000, LR 0.000269
Train loss: 0.0051;  Loss pred: 0.0051; Loss self: 0.0000; time: 0.20s
Val loss: 0.6541 score: 0.6122 time: 0.07s
Test loss: 0.7916 score: 0.5714 time: 0.06s
Epoch 55/1000, LR 0.000269
Train loss: 0.0055;  Loss pred: 0.0055; Loss self: 0.0000; time: 0.20s
Val loss: 0.6445 score: 0.6122 time: 0.07s
Test loss: 0.7749 score: 0.5918 time: 0.06s
Epoch 56/1000, LR 0.000269
Train loss: 0.0048;  Loss pred: 0.0048; Loss self: 0.0000; time: 0.20s
Val loss: 0.6363 score: 0.6122 time: 0.07s
Test loss: 0.7563 score: 0.5918 time: 0.06s
Epoch 57/1000, LR 0.000269
Train loss: 0.0044;  Loss pred: 0.0044; Loss self: 0.0000; time: 0.20s
Val loss: 0.6345 score: 0.6122 time: 0.07s
Test loss: 0.7449 score: 0.5918 time: 0.06s
Epoch 58/1000, LR 0.000269
Train loss: 0.0040;  Loss pred: 0.0040; Loss self: 0.0000; time: 0.20s
Val loss: 0.6447 score: 0.6122 time: 0.07s
Test loss: 0.7487 score: 0.5918 time: 0.06s
     INFO: Early stopping counter 1 of 20
Epoch 59/1000, LR 0.000268
Train loss: 0.0043;  Loss pred: 0.0043; Loss self: 0.0000; time: 0.20s
Val loss: 0.6538 score: 0.6122 time: 0.07s
Test loss: 0.7616 score: 0.5918 time: 0.06s
     INFO: Early stopping counter 2 of 20
Epoch 60/1000, LR 0.000268
Train loss: 0.0038;  Loss pred: 0.0038; Loss self: 0.0000; time: 0.20s
Val loss: 0.6606 score: 0.6122 time: 0.07s
Test loss: 0.7803 score: 0.5918 time: 0.06s
     INFO: Early stopping counter 3 of 20
Epoch 61/1000, LR 0.000268
Train loss: 0.0037;  Loss pred: 0.0037; Loss self: 0.0000; time: 0.21s
Val loss: 0.6649 score: 0.6122 time: 0.07s
Test loss: 0.8027 score: 0.5918 time: 0.06s
     INFO: Early stopping counter 4 of 20
Epoch 62/1000, LR 0.000268
Train loss: 0.0038;  Loss pred: 0.0038; Loss self: 0.0000; time: 0.20s
Val loss: 0.6640 score: 0.6122 time: 0.07s
Test loss: 0.8249 score: 0.5918 time: 0.06s
     INFO: Early stopping counter 5 of 20
Epoch 63/1000, LR 0.000268
Train loss: 0.0036;  Loss pred: 0.0036; Loss self: 0.0000; time: 0.20s
Val loss: 0.6572 score: 0.6122 time: 0.07s
Test loss: 0.8430 score: 0.5510 time: 0.06s
     INFO: Early stopping counter 6 of 20
Epoch 64/1000, LR 0.000268
Train loss: 0.0029;  Loss pred: 0.0029; Loss self: 0.0000; time: 0.20s
Val loss: 0.6547 score: 0.6122 time: 0.07s
Test loss: 0.8621 score: 0.5510 time: 0.06s
     INFO: Early stopping counter 7 of 20
Epoch 65/1000, LR 0.000268
Train loss: 0.0033;  Loss pred: 0.0033; Loss self: 0.0000; time: 0.20s
Val loss: 0.6505 score: 0.6122 time: 0.07s
Test loss: 0.8742 score: 0.5510 time: 0.06s
     INFO: Early stopping counter 8 of 20
Epoch 66/1000, LR 0.000268
Train loss: 0.0032;  Loss pred: 0.0032; Loss self: 0.0000; time: 0.21s
Val loss: 0.6386 score: 0.6122 time: 0.07s
Test loss: 0.8673 score: 0.5510 time: 0.06s
     INFO: Early stopping counter 9 of 20
Epoch 67/1000, LR 0.000268
Train loss: 0.0027;  Loss pred: 0.0027; Loss self: 0.0000; time: 0.20s
Val loss: 0.6185 score: 0.6327 time: 0.07s
Test loss: 0.8488 score: 0.5714 time: 0.06s
Epoch 68/1000, LR 0.000268
Train loss: 0.0027;  Loss pred: 0.0027; Loss self: 0.0000; time: 0.20s
Val loss: 0.5955 score: 0.6531 time: 0.07s
Test loss: 0.8264 score: 0.5918 time: 0.06s
Epoch 69/1000, LR 0.000268
Train loss: 0.0029;  Loss pred: 0.0029; Loss self: 0.0000; time: 0.20s
Val loss: 0.5767 score: 0.6735 time: 0.07s
Test loss: 0.8093 score: 0.5918 time: 0.06s
Epoch 70/1000, LR 0.000268
Train loss: 0.0027;  Loss pred: 0.0027; Loss self: 0.0000; time: 0.21s
Val loss: 0.5602 score: 0.6735 time: 0.07s
Test loss: 0.7978 score: 0.5918 time: 0.07s
Epoch 71/1000, LR 0.000268
Train loss: 0.0023;  Loss pred: 0.0023; Loss self: 0.0000; time: 0.20s
Val loss: 0.5432 score: 0.6735 time: 0.07s
Test loss: 0.7854 score: 0.5918 time: 0.06s
Epoch 72/1000, LR 0.000267
Train loss: 0.0021;  Loss pred: 0.0021; Loss self: 0.0000; time: 0.20s
Val loss: 0.5281 score: 0.6735 time: 0.07s
Test loss: 0.7765 score: 0.6122 time: 0.06s
Epoch 73/1000, LR 0.000267
Train loss: 0.0025;  Loss pred: 0.0025; Loss self: 0.0000; time: 0.20s
Val loss: 0.5125 score: 0.6939 time: 0.07s
Test loss: 0.7691 score: 0.6122 time: 0.06s
Epoch 74/1000, LR 0.000267
Train loss: 0.0021;  Loss pred: 0.0021; Loss self: 0.0000; time: 0.20s
Val loss: 0.4962 score: 0.6939 time: 0.07s
Test loss: 0.7596 score: 0.6122 time: 0.06s
Epoch 75/1000, LR 0.000267
Train loss: 0.0021;  Loss pred: 0.0021; Loss self: 0.0000; time: 0.20s
Val loss: 0.4803 score: 0.6939 time: 0.07s
Test loss: 0.7502 score: 0.6122 time: 0.06s
Epoch 76/1000, LR 0.000267
Train loss: 0.0020;  Loss pred: 0.0020; Loss self: 0.0000; time: 0.20s
Val loss: 0.4637 score: 0.6939 time: 0.07s
Test loss: 0.7378 score: 0.6327 time: 0.06s
Epoch 77/1000, LR 0.000267
Train loss: 0.0020;  Loss pred: 0.0020; Loss self: 0.0000; time: 0.20s
Val loss: 0.4467 score: 0.7347 time: 0.07s
Test loss: 0.7230 score: 0.6327 time: 0.06s
Epoch 78/1000, LR 0.000267
Train loss: 0.0021;  Loss pred: 0.0021; Loss self: 0.0000; time: 0.20s
Val loss: 0.4282 score: 0.7347 time: 0.07s
Test loss: 0.7078 score: 0.6531 time: 0.06s
Epoch 79/1000, LR 0.000267
Train loss: 0.0024;  Loss pred: 0.0024; Loss self: 0.0000; time: 0.20s
Val loss: 0.4089 score: 0.7551 time: 0.07s
Test loss: 0.6913 score: 0.6531 time: 0.06s
Epoch 80/1000, LR 0.000267
Train loss: 0.0018;  Loss pred: 0.0018; Loss self: 0.0000; time: 0.20s
Val loss: 0.3885 score: 0.7959 time: 0.07s
Test loss: 0.6723 score: 0.6531 time: 0.06s
Epoch 81/1000, LR 0.000267
Train loss: 0.0018;  Loss pred: 0.0018; Loss self: 0.0000; time: 0.20s
Val loss: 0.3717 score: 0.8163 time: 0.07s
Test loss: 0.6568 score: 0.6735 time: 0.06s
Epoch 82/1000, LR 0.000267
Train loss: 0.0015;  Loss pred: 0.0015; Loss self: 0.0000; time: 0.20s
Val loss: 0.3545 score: 0.8571 time: 0.07s
Test loss: 0.6410 score: 0.7143 time: 0.06s
Epoch 83/1000, LR 0.000266
Train loss: 0.0017;  Loss pred: 0.0017; Loss self: 0.0000; time: 0.20s
Val loss: 0.3407 score: 0.8571 time: 0.07s
Test loss: 0.6311 score: 0.7143 time: 0.06s
Epoch 84/1000, LR 0.000266
Train loss: 0.0015;  Loss pred: 0.0015; Loss self: 0.0000; time: 0.20s
Val loss: 0.3277 score: 0.8571 time: 0.07s
Test loss: 0.6239 score: 0.7551 time: 0.06s
Epoch 85/1000, LR 0.000266
Train loss: 0.0018;  Loss pred: 0.0018; Loss self: 0.0000; time: 0.20s
Val loss: 0.3176 score: 0.8571 time: 0.07s
Test loss: 0.6240 score: 0.7551 time: 0.06s
Epoch 86/1000, LR 0.000266
Train loss: 0.0019;  Loss pred: 0.0019; Loss self: 0.0000; time: 0.20s
Val loss: 0.3077 score: 0.8571 time: 0.07s
Test loss: 0.6271 score: 0.7551 time: 0.06s
Epoch 87/1000, LR 0.000266
Train loss: 0.0016;  Loss pred: 0.0016; Loss self: 0.0000; time: 0.20s
Val loss: 0.3011 score: 0.8571 time: 0.07s
Test loss: 0.6353 score: 0.7755 time: 0.06s
Epoch 88/1000, LR 0.000266
Train loss: 0.0017;  Loss pred: 0.0017; Loss self: 0.0000; time: 0.20s
Val loss: 0.2953 score: 0.8980 time: 0.07s
Test loss: 0.6455 score: 0.7755 time: 0.06s
Epoch 89/1000, LR 0.000266
Train loss: 0.0016;  Loss pred: 0.0016; Loss self: 0.0000; time: 0.20s
Val loss: 0.2912 score: 0.8980 time: 0.07s
Test loss: 0.6565 score: 0.7755 time: 0.06s
Epoch 90/1000, LR 0.000266
Train loss: 0.0016;  Loss pred: 0.0016; Loss self: 0.0000; time: 0.20s
Val loss: 0.2853 score: 0.8980 time: 0.07s
Test loss: 0.6582 score: 0.7755 time: 0.06s
Epoch 91/1000, LR 0.000266
Train loss: 0.0013;  Loss pred: 0.0013; Loss self: 0.0000; time: 0.20s
Val loss: 0.2788 score: 0.8980 time: 0.07s
Test loss: 0.6552 score: 0.7959 time: 0.06s
Epoch 92/1000, LR 0.000266
Train loss: 0.0014;  Loss pred: 0.0014; Loss self: 0.0000; time: 0.20s
Val loss: 0.2715 score: 0.8980 time: 0.07s
Test loss: 0.6494 score: 0.7959 time: 0.06s
Epoch 93/1000, LR 0.000265
Train loss: 0.0013;  Loss pred: 0.0013; Loss self: 0.0000; time: 0.20s
Val loss: 0.2636 score: 0.9184 time: 0.07s
Test loss: 0.6426 score: 0.7959 time: 0.06s
Epoch 94/1000, LR 0.000265
Train loss: 0.0013;  Loss pred: 0.0013; Loss self: 0.0000; time: 0.20s
Val loss: 0.2539 score: 0.9184 time: 0.07s
Test loss: 0.6292 score: 0.7959 time: 0.06s
Epoch 95/1000, LR 0.000265
Train loss: 0.0013;  Loss pred: 0.0013; Loss self: 0.0000; time: 0.20s
Val loss: 0.2445 score: 0.9184 time: 0.07s
Test loss: 0.6186 score: 0.7959 time: 0.06s
Epoch 96/1000, LR 0.000265
Train loss: 0.0012;  Loss pred: 0.0012; Loss self: 0.0000; time: 0.20s
Val loss: 0.2353 score: 0.9184 time: 0.07s
Test loss: 0.6077 score: 0.7959 time: 0.06s
Epoch 97/1000, LR 0.000265
Train loss: 0.0013;  Loss pred: 0.0013; Loss self: 0.0000; time: 0.20s
Val loss: 0.2267 score: 0.9184 time: 0.07s
Test loss: 0.5951 score: 0.7959 time: 0.06s
Epoch 98/1000, LR 0.000265
Train loss: 0.0013;  Loss pred: 0.0013; Loss self: 0.0000; time: 0.20s
Val loss: 0.2198 score: 0.9184 time: 0.07s
Test loss: 0.5852 score: 0.7959 time: 0.06s
Epoch 99/1000, LR 0.000265
Train loss: 0.0014;  Loss pred: 0.0014; Loss self: 0.0000; time: 0.20s
Val loss: 0.2130 score: 0.9184 time: 0.07s
Test loss: 0.5733 score: 0.7959 time: 0.06s
Epoch 100/1000, LR 0.000265
Train loss: 0.0013;  Loss pred: 0.0013; Loss self: 0.0000; time: 0.20s
Val loss: 0.2075 score: 0.9184 time: 0.07s
Test loss: 0.5647 score: 0.7959 time: 0.06s
Epoch 101/1000, LR 0.000265
Train loss: 0.0009;  Loss pred: 0.0009; Loss self: 0.0000; time: 0.20s
Val loss: 0.2039 score: 0.9184 time: 0.07s
Test loss: 0.5585 score: 0.7959 time: 0.06s
Epoch 102/1000, LR 0.000264
Train loss: 0.0012;  Loss pred: 0.0012; Loss self: 0.0000; time: 0.20s
Val loss: 0.2016 score: 0.9388 time: 0.07s
Test loss: 0.5559 score: 0.7959 time: 0.06s
Epoch 103/1000, LR 0.000264
Train loss: 0.0010;  Loss pred: 0.0010; Loss self: 0.0000; time: 0.20s
Val loss: 0.1992 score: 0.9388 time: 0.07s
Test loss: 0.5530 score: 0.8163 time: 0.06s
Epoch 104/1000, LR 0.000264
Train loss: 0.0011;  Loss pred: 0.0011; Loss self: 0.0000; time: 0.20s
Val loss: 0.1973 score: 0.9388 time: 0.07s
Test loss: 0.5535 score: 0.8163 time: 0.06s
Epoch 105/1000, LR 0.000264
Train loss: 0.0010;  Loss pred: 0.0010; Loss self: 0.0000; time: 0.20s
Val loss: 0.1956 score: 0.9388 time: 0.07s
Test loss: 0.5559 score: 0.8163 time: 0.06s
Epoch 106/1000, LR 0.000264
Train loss: 0.0012;  Loss pred: 0.0012; Loss self: 0.0000; time: 0.20s
Val loss: 0.1950 score: 0.9388 time: 0.07s
Test loss: 0.5602 score: 0.8163 time: 0.06s
Epoch 107/1000, LR 0.000264
Train loss: 0.0011;  Loss pred: 0.0011; Loss self: 0.0000; time: 0.20s
Val loss: 0.1927 score: 0.9388 time: 0.07s
Test loss: 0.5616 score: 0.8163 time: 0.06s
Epoch 108/1000, LR 0.000264
Train loss: 0.0010;  Loss pred: 0.0010; Loss self: 0.0000; time: 0.20s
Val loss: 0.1899 score: 0.9388 time: 0.07s
Test loss: 0.5608 score: 0.8163 time: 0.06s
Epoch 109/1000, LR 0.000264
Train loss: 0.0009;  Loss pred: 0.0009; Loss self: 0.0000; time: 0.20s
Val loss: 0.1882 score: 0.9388 time: 0.07s
Test loss: 0.5639 score: 0.8163 time: 0.06s
Epoch 110/1000, LR 0.000263
Train loss: 0.0008;  Loss pred: 0.0008; Loss self: 0.0000; time: 0.20s
Val loss: 0.1855 score: 0.9388 time: 0.07s
Test loss: 0.5651 score: 0.8367 time: 0.06s
Epoch 111/1000, LR 0.000263
Train loss: 0.0011;  Loss pred: 0.0011; Loss self: 0.0000; time: 0.20s
Val loss: 0.1836 score: 0.9388 time: 0.07s
Test loss: 0.5664 score: 0.8367 time: 0.06s
Epoch 112/1000, LR 0.000263
Train loss: 0.0012;  Loss pred: 0.0012; Loss self: 0.0000; time: 0.20s
Val loss: 0.1828 score: 0.9388 time: 0.07s
Test loss: 0.5677 score: 0.8367 time: 0.06s
Epoch 113/1000, LR 0.000263
Train loss: 0.0008;  Loss pred: 0.0008; Loss self: 0.0000; time: 0.20s
Val loss: 0.1811 score: 0.9388 time: 0.07s
Test loss: 0.5683 score: 0.8367 time: 0.06s
Epoch 114/1000, LR 0.000263
Train loss: 0.0011;  Loss pred: 0.0011; Loss self: 0.0000; time: 0.20s
Val loss: 0.1787 score: 0.9388 time: 0.07s
Test loss: 0.5653 score: 0.8367 time: 0.06s
Epoch 115/1000, LR 0.000263
Train loss: 0.0009;  Loss pred: 0.0009; Loss self: 0.0000; time: 0.20s
Val loss: 0.1764 score: 0.9388 time: 0.07s
Test loss: 0.5598 score: 0.8367 time: 0.06s
Epoch 116/1000, LR 0.000263
Train loss: 0.0013;  Loss pred: 0.0013; Loss self: 0.0000; time: 0.20s
Val loss: 0.1761 score: 0.9388 time: 0.07s
Test loss: 0.5554 score: 0.8367 time: 0.06s
Epoch 117/1000, LR 0.000262
Train loss: 0.0008;  Loss pred: 0.0008; Loss self: 0.0000; time: 0.20s
Val loss: 0.1755 score: 0.9388 time: 0.07s
Test loss: 0.5527 score: 0.8571 time: 0.06s
Epoch 118/1000, LR 0.000262
Train loss: 0.0010;  Loss pred: 0.0010; Loss self: 0.0000; time: 0.20s
Val loss: 0.1758 score: 0.9388 time: 0.07s
Test loss: 0.5511 score: 0.8571 time: 0.06s
     INFO: Early stopping counter 1 of 20
Epoch 119/1000, LR 0.000262
Train loss: 0.0008;  Loss pred: 0.0008; Loss self: 0.0000; time: 0.20s
Val loss: 0.1758 score: 0.9388 time: 0.07s
Test loss: 0.5497 score: 0.8571 time: 0.06s
     INFO: Early stopping counter 2 of 20
Epoch 120/1000, LR 0.000262
Train loss: 0.0008;  Loss pred: 0.0008; Loss self: 0.0000; time: 0.20s
Val loss: 0.1755 score: 0.9388 time: 0.07s
Test loss: 0.5483 score: 0.8571 time: 0.06s
Epoch 121/1000, LR 0.000262
Train loss: 0.0008;  Loss pred: 0.0008; Loss self: 0.0000; time: 0.20s
Val loss: 0.1750 score: 0.9388 time: 0.07s
Test loss: 0.5506 score: 0.8571 time: 0.06s
Epoch 122/1000, LR 0.000262
Train loss: 0.0008;  Loss pred: 0.0008; Loss self: 0.0000; time: 0.20s
Val loss: 0.1740 score: 0.9388 time: 0.07s
Test loss: 0.5522 score: 0.8571 time: 0.06s
Epoch 123/1000, LR 0.000262
Train loss: 0.0008;  Loss pred: 0.0008; Loss self: 0.0000; time: 0.20s
Val loss: 0.1736 score: 0.9592 time: 0.07s
Test loss: 0.5529 score: 0.8571 time: 0.06s
Epoch 124/1000, LR 0.000261
Train loss: 0.0008;  Loss pred: 0.0008; Loss self: 0.0000; time: 0.20s
Val loss: 0.1730 score: 0.9592 time: 0.07s
Test loss: 0.5517 score: 0.8571 time: 0.06s
Epoch 125/1000, LR 0.000261
Train loss: 0.0007;  Loss pred: 0.0007; Loss self: 0.0000; time: 0.20s
Val loss: 0.1723 score: 0.9592 time: 0.07s
Test loss: 0.5537 score: 0.8571 time: 0.06s
Epoch 126/1000, LR 0.000261
Train loss: 0.0007;  Loss pred: 0.0007; Loss self: 0.0000; time: 0.20s
Val loss: 0.1710 score: 0.9592 time: 0.07s
Test loss: 0.5561 score: 0.8571 time: 0.06s
Epoch 127/1000, LR 0.000261
Train loss: 0.0008;  Loss pred: 0.0008; Loss self: 0.0000; time: 0.20s
Val loss: 0.1696 score: 0.9592 time: 0.07s
Test loss: 0.5573 score: 0.8571 time: 0.06s
Epoch 128/1000, LR 0.000261
Train loss: 0.0008;  Loss pred: 0.0008; Loss self: 0.0000; time: 0.20s
Val loss: 0.1685 score: 0.9592 time: 0.07s
Test loss: 0.5583 score: 0.8571 time: 0.06s
Epoch 129/1000, LR 0.000261
Train loss: 0.0008;  Loss pred: 0.0008; Loss self: 0.0000; time: 0.20s
Val loss: 0.1668 score: 0.9592 time: 0.07s
Test loss: 0.5558 score: 0.8571 time: 0.06s
Epoch 130/1000, LR 0.000260
Train loss: 0.0007;  Loss pred: 0.0007; Loss self: 0.0000; time: 0.20s
Val loss: 0.1651 score: 0.9592 time: 0.07s
Test loss: 0.5516 score: 0.8571 time: 0.06s
Epoch 131/1000, LR 0.000260
Train loss: 0.0015;  Loss pred: 0.0015; Loss self: 0.0000; time: 0.20s
Val loss: 0.1673 score: 0.9592 time: 0.07s
Test loss: 0.5576 score: 0.8571 time: 0.06s
     INFO: Early stopping counter 1 of 20
Epoch 132/1000, LR 0.000260
Train loss: 0.0008;  Loss pred: 0.0008; Loss self: 0.0000; time: 0.20s
Val loss: 0.1684 score: 0.9592 time: 0.07s
Test loss: 0.5608 score: 0.8571 time: 0.06s
     INFO: Early stopping counter 2 of 20
Epoch 133/1000, LR 0.000260
Train loss: 0.0007;  Loss pred: 0.0007; Loss self: 0.0000; time: 0.20s
Val loss: 0.1698 score: 0.9592 time: 0.07s
Test loss: 0.5623 score: 0.8571 time: 0.06s
     INFO: Early stopping counter 3 of 20
Epoch 134/1000, LR 0.000260
Train loss: 0.0006;  Loss pred: 0.0006; Loss self: 0.0000; time: 0.20s
Val loss: 0.1706 score: 0.9592 time: 0.07s
Test loss: 0.5618 score: 0.8571 time: 0.06s
     INFO: Early stopping counter 4 of 20
Epoch 135/1000, LR 0.000260
Train loss: 0.0008;  Loss pred: 0.0008; Loss self: 0.0000; time: 0.20s
Val loss: 0.1706 score: 0.9592 time: 0.07s
Test loss: 0.5591 score: 0.8571 time: 0.06s
     INFO: Early stopping counter 5 of 20
Epoch 136/1000, LR 0.000260
Train loss: 0.0007;  Loss pred: 0.0007; Loss self: 0.0000; time: 0.20s
Val loss: 0.1709 score: 0.9592 time: 0.07s
Test loss: 0.5566 score: 0.8571 time: 0.06s
     INFO: Early stopping counter 6 of 20
Epoch 137/1000, LR 0.000259
Train loss: 0.0006;  Loss pred: 0.0006; Loss self: 0.0000; time: 0.20s
Val loss: 0.1706 score: 0.9592 time: 0.07s
Test loss: 0.5538 score: 0.8571 time: 0.06s
     INFO: Early stopping counter 7 of 20
Epoch 138/1000, LR 0.000259
Train loss: 0.0007;  Loss pred: 0.0007; Loss self: 0.0000; time: 0.20s
Val loss: 0.1705 score: 0.9592 time: 0.07s
Test loss: 0.5522 score: 0.8571 time: 0.06s
     INFO: Early stopping counter 8 of 20
Epoch 139/1000, LR 0.000259
Train loss: 0.0008;  Loss pred: 0.0008; Loss self: 0.0000; time: 0.20s
Val loss: 0.1709 score: 0.9592 time: 0.07s
Test loss: 0.5526 score: 0.8571 time: 0.06s
     INFO: Early stopping counter 9 of 20
Epoch 140/1000, LR 0.000259
Train loss: 0.0007;  Loss pred: 0.0007; Loss self: 0.0000; time: 0.20s
Val loss: 0.1707 score: 0.9592 time: 0.07s
Test loss: 0.5527 score: 0.8571 time: 0.06s
     INFO: Early stopping counter 10 of 20
Epoch 141/1000, LR 0.000259
Train loss: 0.0006;  Loss pred: 0.0006; Loss self: 0.0000; time: 0.20s
Val loss: 0.1703 score: 0.9592 time: 0.07s
Test loss: 0.5545 score: 0.8571 time: 0.06s
     INFO: Early stopping counter 11 of 20
Epoch 142/1000, LR 0.000259
Train loss: 0.0006;  Loss pred: 0.0006; Loss self: 0.0000; time: 0.20s
Val loss: 0.1689 score: 0.9592 time: 0.07s
Test loss: 0.5560 score: 0.8571 time: 0.06s
     INFO: Early stopping counter 12 of 20
Epoch 143/1000, LR 0.000258
Train loss: 0.0006;  Loss pred: 0.0006; Loss self: 0.0000; time: 0.20s
Val loss: 0.1677 score: 0.9592 time: 0.07s
Test loss: 0.5598 score: 0.8571 time: 0.06s
     INFO: Early stopping counter 13 of 20
Epoch 144/1000, LR 0.000258
Train loss: 0.0007;  Loss pred: 0.0007; Loss self: 0.0000; time: 0.20s
Val loss: 0.1668 score: 0.9592 time: 0.07s
Test loss: 0.5669 score: 0.8571 time: 0.06s
     INFO: Early stopping counter 14 of 20
Epoch 145/1000, LR 0.000258
Train loss: 0.0007;  Loss pred: 0.0007; Loss self: 0.0000; time: 0.20s
Val loss: 0.1659 score: 0.9592 time: 0.07s
Test loss: 0.5728 score: 0.8571 time: 0.06s
     INFO: Early stopping counter 15 of 20
Epoch 146/1000, LR 0.000258
Train loss: 0.0006;  Loss pred: 0.0006; Loss self: 0.0000; time: 0.20s
Val loss: 0.1654 score: 0.9592 time: 0.07s
Test loss: 0.5787 score: 0.8571 time: 0.06s
     INFO: Early stopping counter 16 of 20
Epoch 147/1000, LR 0.000258
Train loss: 0.0005;  Loss pred: 0.0005; Loss self: 0.0000; time: 0.20s
Val loss: 0.1653 score: 0.9592 time: 0.07s
Test loss: 0.5829 score: 0.8571 time: 0.06s
     INFO: Early stopping counter 17 of 20
Epoch 148/1000, LR 0.000257
Train loss: 0.0007;  Loss pred: 0.0007; Loss self: 0.0000; time: 0.20s
Val loss: 0.1655 score: 0.9592 time: 0.07s
Test loss: 0.5859 score: 0.8571 time: 0.06s
     INFO: Early stopping counter 18 of 20
Epoch 149/1000, LR 0.000257
Train loss: 0.0006;  Loss pred: 0.0006; Loss self: 0.0000; time: 0.20s
Val loss: 0.1666 score: 0.9592 time: 0.07s
Test loss: 0.5897 score: 0.8571 time: 0.06s
     INFO: Early stopping counter 19 of 20
Epoch 150/1000, LR 0.000257
Train loss: 0.0006;  Loss pred: 0.0006; Loss self: 0.0000; time: 0.20s
Val loss: 0.1675 score: 0.9592 time: 0.07s
Test loss: 0.5918 score: 0.8571 time: 0.06s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 129,   Train_Loss: 0.0007,   Val_Loss: 0.1651,   Val_Precision: 1.0000,   Val_Recall: 0.9167,   Val_accuracy: 0.9565,   Val_Score: 0.9592,   Val_Loss: 0.1651,   Test_Precision: 1.0000,   Test_Recall: 0.7200,   Test_accuracy: 0.8372,   Test_Score: 0.8571,   Test_loss: 0.5516


[0.06653004908002913, 0.06610951002221555, 0.06770800799131393, 0.0676920679397881, 0.06670175201725215, 0.06619879498612136, 0.06624210090376437, 0.06598499894607812, 0.06619761209003627, 0.06596349796745926, 0.06603561306837946, 0.06581066292710602, 0.06567679799627513, 0.06595130905043334, 0.06575967906974256, 0.06574518303386867, 0.06608675001189113, 0.06613295606803149, 0.06639591895509511, 0.06623150710947812, 0.06578854203689843, 0.06584402895532548, 0.06592013605404645, 0.06624365702737123, 0.06574834999628365, 0.06564155709929764, 0.06610373791772872, 0.06612707697786391, 0.06647406402043998, 0.06724994501564652, 0.06639685505069792, 0.06637395592406392, 0.066052061971277, 0.06553001899737865, 0.06588779110461473, 0.06585439096670598, 0.06607440405059606, 0.06621538801118731, 0.06610940396785736, 0.06649822799954563, 0.06600356101989746, 0.06571503798477352, 0.06575496098957956, 0.06629602995235473, 0.0667801279341802, 0.06611341494135559, 0.06602570903487504, 0.06633813609369099, 0.06571667699608952, 0.0659255530918017, 0.06623123097233474, 0.06624143000226468, 0.06656469404697418, 0.06673382001463324, 0.06570359808392823, 0.06581750302575529, 0.06646456802263856, 0.06641056004446, 0.06607569893822074, 0.06649216893129051, 0.06598602898884565, 0.06633992597926408, 0.06594560900703073, 0.06618394399993122, 0.066159290028736, 0.0667649069800973, 0.06609958095941693, 0.06554643099661916, 0.06612603005487472, 0.07081823702901602, 0.06618931202683598, 0.06595342606306076, 0.06575548299588263, 0.067293671076186, 0.06801166606601328, 0.06605721008963883, 0.06609580700751394, 0.06629381596576422, 0.06676653597969562, 0.06581298890523612, 0.06603154796175659, 0.0666289689252153, 0.06618497800081968, 0.06602604407817125, 0.06613494001794606, 0.06623689201660454, 0.06591854593716562, 0.06599544803611934, 0.06614058592822403, 0.06758303206879646, 0.06575622595846653, 0.06671825307421386, 0.0656401610467583, 0.06598355900496244, 0.06648111995309591, 0.06661088101100177, 0.06667307100724429, 0.06653862597886473, 0.06621336203534156, 0.06601031497120857, 0.06618479697499424, 0.06634152808692306, 0.06647824903484434, 0.06643100106157362, 0.06746940896846354, 0.06646313308738172, 0.06646295299287885, 0.06669786595739424, 0.06599911698140204, 0.06633187900297344, 0.06663203798234463, 0.06683707097545266, 0.06675081001594663, 0.06632602494210005, 0.06618673505727202, 0.06698055996093899, 0.06610000599175692, 0.0669174250215292, 0.0664925379678607, 0.06737054791301489, 0.06652807304635644, 0.06654881103895605, 0.06667103001382202, 0.06685043801553547, 0.06703193602152169, 0.06639367807656527, 0.06653823191300035, 0.06658501701895148, 0.06664203898981214, 0.06635120394639671, 0.06710969691630453, 0.0666060980875045, 0.06667353003285825, 0.06626723194494843, 0.06784916610922664, 0.06629418500233442, 0.06626583903562278, 0.06570346700027585, 0.06661877792794257, 0.06630181905347854, 0.06640631798654795, 0.06609042594209313, 0.06599961803294718, 0.06624913599807769, 0.06592124700546265, 0.06583352200686932, 0.06639320100657642, 0.06622853397857398, 0.06689190398901701, 0.06731797999236733]
[0.0013577561036740638, 0.0013491736739227663, 0.0013817960814553865, 0.00138147077428139, 0.0013612602452500438, 0.001350995816043293, 0.0013518796102809055, 0.0013466326315526146, 0.0013509716753068628, 0.0013461938360705972, 0.0013476655728240706, 0.0013430747536144086, 0.0013403428162505127, 0.0013459450826619047, 0.00134203426672944, 0.001341738429262626, 0.0013487091839161454, 0.0013496521646537039, 0.0013550187541856145, 0.0013516634103975125, 0.0013426233068754782, 0.0013437556929658263, 0.0013453088990621725, 0.0013519113679055352, 0.001341803061148646, 0.0013396236142713804, 0.0013490558758720147, 0.0013495321832217124, 0.0013566135514375506, 0.001372447857462174, 0.0013550378581775086, 0.001354570529062529, 0.0013480012647199388, 0.0013373473264771153, 0.0013446487980533618, 0.0013439671625858362, 0.0013484572255223685, 0.0013513344492079044, 0.0013491715095481094, 0.0013571066938682782, 0.0013470114493856625, 0.0013411232241790515, 0.0013419379793791746, 0.0013529802031092802, 0.0013628597537587797, 0.001349253366150114, 0.0013474634496913273, 0.0013538395121161426, 0.001341156673389582, 0.001345419450853096, 0.001351657774945607, 0.0013518659184135649, 0.0013584631438157996, 0.0013619146941761886, 0.0013408897568148617, 0.0013432143474643935, 0.0013564197555640523, 0.001355317551927755, 0.0013484836518004232, 0.001356983039414092, 0.0013466536528335847, 0.0013538760403931445, 0.001345828755245525, 0.0013506927346924739, 0.0013501895924231835, 0.0013625491220428019, 0.0013489710399881005, 0.0013376822652371259, 0.0013495108174464228, 0.0014452701434493065, 0.0013508022862619587, 0.00134598828700124, 0.0013419486325690333, 0.0013733402260446123, 0.001387993185020679, 0.001348106328359976, 0.0013488940205615089, 0.001352935019709474, 0.0013625823669325635, 0.0013431222225558392, 0.00134758261146442, 0.0013597748760248022, 0.0013507138367514222, 0.0013474702873096174, 0.0013496926534274708, 0.0013517733064613172, 0.0013452764476972576, 0.0013468458782881498, 0.0013498078760862046, 0.0013792455524244175, 0.0013419637950707454, 0.0013615970015145686, 0.0013395951234032304, 0.0013466032449992336, 0.001356757550063182, 0.0013594057349184034, 0.0013606749185151896, 0.0013579311424258109, 0.0013512931027620727, 0.0013471492851267056, 0.0013507101423468214, 0.0013539087364678175, 0.0013566989598947825, 0.001355734715542319, 0.001376926713642113, 0.0013563904711710556, 0.0013563867957730377, 0.0013611809379060048, 0.0013469207547224906, 0.001353711816387213, 0.001359837509843768, 0.001364021856641891, 0.0013622614288968699, 0.0013535923457571439, 0.0013507496950463677, 0.0013669502032844691, 0.0013489797141174882, 0.001365661735133249, 0.0013569905707726674, 0.0013749091410819366, 0.001357715776456254, 0.0013581390007950213, 0.0013606332655882044, 0.001364294653378275, 0.001367998694316769, 0.0013549730219707197, 0.0013579231002653132, 0.0013588778983459485, 0.0013600416120369823, 0.001354106202987688, 0.0013695856513531537, 0.001359308124234786, 0.0013606842863848622, 0.001352392488672417, 0.0013846768593719723, 0.0013529425510680493, 0.0013523640619514852, 0.0013408870816382827, 0.001359566896488624, 0.0013530983480301742, 0.001355230979317305, 0.00134878420289986, 0.0013469309802642282, 0.0013520231836342386, 0.001345331571540054, 0.0013435412654463127, 0.0013549632858484983, 0.0013516027342566118, 0.001365140897735041, 0.0013738363263748434]
[736.5093018503234, 741.1944209469101, 723.6957850877266, 723.8662001519197, 734.6133874763341, 740.1947423706564, 739.7108384467839, 742.593025424491, 740.2079690329982, 742.8350756076096, 742.0238523304207, 744.560194664411, 746.0777853813617, 742.972364089535, 745.1374564652635, 745.3017504682833, 741.449685317909, 740.9316460857023, 737.9971656562157, 739.8291559182686, 744.810547291315, 744.1828936872319, 743.3237085528159, 739.6934619680444, 745.2658508201296, 746.4783311870019, 741.259141215045, 740.9975193127437, 737.1295966639423, 728.6251310480559, 737.9867610082676, 738.2413676843241, 741.8390666033688, 747.7489057642439, 743.6886133001365, 744.0657985095169, 741.5882247303897, 740.0092557295184, 741.1956099895256, 736.861740140426, 742.3841871990579, 745.6436380871229, 745.1909219102909, 739.1091146063354, 733.7512148568412, 741.1506430799914, 742.1351578991452, 738.6399872736263, 745.6250413105299, 743.2626303759215, 739.8322404798372, 739.7183303308032, 736.1259704043871, 734.2603793587028, 745.773464908399, 744.482816080483, 737.2349126426288, 737.8344643863247, 741.5736918017905, 736.92888632696, 742.581433537742, 738.6200583840863, 743.0365832966364, 740.360834344519, 740.636726583932, 733.9184942563758, 741.3057585052538, 747.5616788735207, 741.0092509611923, 691.9121691764716, 740.3007902564889, 742.9485157169713, 745.1850061396127, 728.1516852383493, 720.4646325299511, 741.7812519406677, 741.3480857330262, 739.1337983214727, 733.9005877869926, 744.5338802429246, 742.0695336171624, 735.4158527501431, 740.3492677657632, 742.1313920002032, 740.9094192374498, 739.7690095078204, 743.3416393424001, 742.4754503247296, 740.8461735306512, 725.0340581067778, 745.1765864870312, 734.4316996054287, 746.4942074882358, 742.6092308284681, 737.0513618689143, 735.6155519382326, 734.9293989274314, 736.4143650270794, 740.0318983024321, 742.3082289695499, 740.3512927374098, 738.6022211577405, 737.0831920425104, 737.6074305215246, 726.2550650607228, 737.2508295023913, 737.2528272291796, 734.6561887197498, 742.4341755027989, 738.7096632345285, 735.3819796564446, 733.1260823502618, 734.0734889703062, 738.7748631517572, 740.3296137451082, 731.5555443038294, 741.3009918049116, 732.2457489097247, 736.9247963385643, 727.3207880581004, 736.5311778361149, 736.3016594138188, 734.9518972459475, 732.9794905549132, 730.9948497424833, 738.0220740820104, 736.4187263657408, 735.9012912177162, 735.2716204780417, 738.4945123163965, 730.1478363269927, 735.6683758238728, 734.9243391770568, 739.4303121142407, 722.1901581092035, 739.1296838218097, 739.445854954902, 745.7749527858899, 735.5283528767261, 739.0445797645006, 737.8815974998948, 741.4084461028083, 742.4285391400155, 739.6322874523496, 743.3111815366532, 744.3016643540225, 738.0273771578874, 739.8623683237848, 732.52512005108, 727.88874540733]
Elapsed: 0.06635343428545942~0.0005946335740168255
Time per graph: 0.0013541517201114167~1.213537906156787e-05
Speed: 738.5273494918813~6.436785163526353
Total Time: 0.0679
best val loss: 0.16509464383125305 test_score: 0.8571

Testing...
Test loss: 0.5529 score: 0.8571 time: 0.06s
test Score 0.8571
Epoch Time List: [0.34173178183846176, 0.3283941481495276, 0.3296439310070127, 0.33199621888343245, 0.328264641109854, 0.32853396818973124, 0.3268393420148641, 0.3262431720504537, 0.3267954319017008, 0.32771452597808093, 0.33072660001926124, 0.3300640902016312, 0.32886464009061456, 0.32925805693957955, 0.3326858729124069, 0.33657654316630214, 0.3296876910608262, 0.32936913101002574, 0.33049840782769024, 0.3280723198549822, 0.32896482187788934, 0.33021015592385083, 0.32883515302091837, 0.32709692197386175, 0.3297765851020813, 0.32832052803132683, 0.32941042305901647, 0.3292831960134208, 0.329916590009816, 0.3355257530929521, 0.33919497614260763, 0.3307043630629778, 0.32910826499573886, 0.32943040900863707, 0.32845854898914695, 0.3268160540610552, 0.32727101806085557, 0.3287673101294786, 0.3292882919777185, 0.32930946606211364, 0.3291163770481944, 0.32846418709959835, 0.32749698299448937, 0.3282003180356696, 0.3334445410873741, 0.33017892809584737, 0.3275304251583293, 0.3276485699461773, 0.3267573819030076, 0.3267537389183417, 0.32740929012652487, 0.33078396797645837, 0.3303196020424366, 0.33050227409694344, 0.32845917192753404, 0.3288658680394292, 0.3301896839402616, 0.32653154409490526, 0.32650251500308514, 0.33031935908365995, 0.33709537400864065, 0.3301983700366691, 0.3291788628557697, 0.3298754539573565, 0.3340656330110505, 0.3424608149798587, 0.3320079939439893, 0.3289565750164911, 0.3288702921709046, 0.35227956995368004, 0.33254632086027414, 0.3298281991155818, 0.3302711199503392, 0.33518447005189955, 0.3349147760309279, 0.33183614211156964, 0.32952570903580636, 0.3296131599927321, 0.33079052006360143, 0.32889569620601833, 0.33053318003658205, 0.33077916700858623, 0.3328121380181983, 0.3301131749758497, 0.32977318798657507, 0.3310423350194469, 0.3291421299800277, 0.32929914514534175, 0.32989127398468554, 0.3330668529961258, 0.332504601101391, 0.32982042396906763, 0.3287309060106054, 0.32932235195767134, 0.32867449009791017, 0.3306796009419486, 0.3293301409576088, 0.32956018985714763, 0.33213675394654274, 0.3293540918966755, 0.33137634897138923, 0.33110448403749615, 0.3313313939142972, 0.33208656986244023, 0.33538681408390403, 0.3340346359182149, 0.3299824499990791, 0.3327450258657336, 0.3274789290735498, 0.32726919604465365, 0.32722526602447033, 0.3286510822363198, 0.32671639579348266, 0.32626078301109374, 0.3277059089159593, 0.32773389806970954, 0.32708470604848117, 0.32848030992317945, 0.3277462701080367, 0.33073206897825, 0.33397366502322257, 0.3275739620439708, 0.3273940399521962, 0.32766594109125435, 0.32926953805144876, 0.3273344661574811, 0.3270749399671331, 0.32882631209213287, 0.32836127607151866, 0.32700107991695404, 0.3295759118627757, 0.3278880879515782, 0.32770410703960806, 0.3279180289246142, 0.3305149609223008, 0.33133166190236807, 0.32687180303037167, 0.3265653708949685, 0.32608731102664024, 0.3269410200882703, 0.3271347559057176, 0.3286738229217008, 0.326653953990899, 0.32648552500177175, 0.3258517680224031, 0.32633360696490854, 0.3267153979977593, 0.3280975391389802, 0.3272959579480812, 0.3279249439947307]
Total Epoch List: [150]
Total Time List: [0.06792019703425467]
========================k_idx:1========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: GPSTransformer
GPS(
  (x_lin): Linear(in_features=14887, out_features=64, bias=True)
  (pe_lin): Linear(in_features=20, out_features=8, bias=True)
  (pe_norm): BatchNorm1d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (convs): ModuleList(
    (0-1): 2 x GPSConv(64, conv=GINEConv(nn=Sequential(
      (0): Linear(in_features=64, out_features=64, bias=True)
      (1): ReLU()
      (2): Linear(in_features=64, out_features=64, bias=True)
    )), heads=1, attn_type=multihead)
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=32, bias=True)
    (1): ReLU()
    (2): Linear(in_features=32, out_features=16, bias=True)
    (3): ReLU()
    (4): Linear(in_features=16, out_features=2, bias=True)
  )
)
Total number of parameters: 1039906
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x74b5c4fafa00>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.5876;  Loss pred: 0.5876; Loss self: 0.0000; time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7774 score: 0.5102 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7170 score: 0.4898 time: 0.07s
Epoch 2/1000, LR 0.000000
Train loss: 0.5828;  Loss pred: 0.5828; Loss self: 0.0000; time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7184 score: 0.5102 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6629 score: 0.4898 time: 0.07s
Epoch 3/1000, LR 0.000030
Train loss: 0.5324;  Loss pred: 0.5324; Loss self: 0.0000; time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6898 score: 0.5102 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6360 score: 0.4898 time: 0.07s
Epoch 4/1000, LR 0.000060
Train loss: 0.4554;  Loss pred: 0.4554; Loss self: 0.0000; time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6718 score: 0.5102 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6216 score: 0.4898 time: 0.07s
Epoch 5/1000, LR 0.000090
Train loss: 0.3941;  Loss pred: 0.3941; Loss self: 0.0000; time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6713 score: 0.5102 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6288 score: 0.4898 time: 0.17s
Epoch 6/1000, LR 0.000120
Train loss: 0.3309;  Loss pred: 0.3309; Loss self: 0.0000; time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6709 score: 0.5102 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6266 score: 0.4898 time: 0.07s
Epoch 7/1000, LR 0.000150
Train loss: 0.2692;  Loss pred: 0.2692; Loss self: 0.0000; time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6682 score: 0.5102 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6245 score: 0.4898 time: 0.07s
Epoch 8/1000, LR 0.000180
Train loss: 0.2184;  Loss pred: 0.2184; Loss self: 0.0000; time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6657 score: 0.5102 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6265 score: 0.4898 time: 0.07s
Epoch 9/1000, LR 0.000210
Train loss: 0.1714;  Loss pred: 0.1714; Loss self: 0.0000; time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6662 score: 0.5102 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6265 score: 0.4898 time: 0.07s
     INFO: Early stopping counter 1 of 20
Epoch 10/1000, LR 0.000240
Train loss: 0.1343;  Loss pred: 0.1343; Loss self: 0.0000; time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6684 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6282 score: 0.4898 time: 0.19s
     INFO: Early stopping counter 2 of 20
Epoch 11/1000, LR 0.000270
Train loss: 0.1035;  Loss pred: 0.1035; Loss self: 0.0000; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6709 score: 0.5102 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6356 score: 0.4898 time: 0.07s
     INFO: Early stopping counter 3 of 20
Epoch 12/1000, LR 0.000270
Train loss: 0.0839;  Loss pred: 0.0839; Loss self: 0.0000; time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6717 score: 0.5102 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6438 score: 0.4898 time: 0.07s
     INFO: Early stopping counter 4 of 20
Epoch 13/1000, LR 0.000270
Train loss: 0.0618;  Loss pred: 0.0618; Loss self: 0.0000; time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6755 score: 0.5102 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6516 score: 0.4898 time: 0.07s
     INFO: Early stopping counter 5 of 20
Epoch 14/1000, LR 0.000270
Train loss: 0.0481;  Loss pred: 0.0481; Loss self: 0.0000; time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6775 score: 0.5102 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6577 score: 0.4898 time: 0.07s
     INFO: Early stopping counter 6 of 20
Epoch 15/1000, LR 0.000270
Train loss: 0.0372;  Loss pred: 0.0372; Loss self: 0.0000; time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6804 score: 0.5102 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6604 score: 0.4898 time: 0.14s
     INFO: Early stopping counter 7 of 20
Epoch 16/1000, LR 0.000270
Train loss: 0.0307;  Loss pred: 0.0307; Loss self: 0.0000; time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6823 score: 0.5102 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6617 score: 0.4898 time: 0.07s
     INFO: Early stopping counter 8 of 20
Epoch 17/1000, LR 0.000270
Train loss: 0.0249;  Loss pred: 0.0249; Loss self: 0.0000; time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6845 score: 0.5102 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6623 score: 0.4898 time: 0.07s
     INFO: Early stopping counter 9 of 20
Epoch 18/1000, LR 0.000270
Train loss: 0.0200;  Loss pred: 0.0200; Loss self: 0.0000; time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6843 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6601 score: 0.4898 time: 0.07s
     INFO: Early stopping counter 10 of 20
Epoch 19/1000, LR 0.000270
Train loss: 0.0157;  Loss pred: 0.0157; Loss self: 0.0000; time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6824 score: 0.5102 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6565 score: 0.4898 time: 0.07s
     INFO: Early stopping counter 11 of 20
Epoch 20/1000, LR 0.000270
Train loss: 0.0131;  Loss pred: 0.0131; Loss self: 0.0000; time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6811 score: 0.5102 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6521 score: 0.4898 time: 0.07s
     INFO: Early stopping counter 12 of 20
Epoch 21/1000, LR 0.000270
Train loss: 0.0115;  Loss pred: 0.0115; Loss self: 0.0000; time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6801 score: 0.5102 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6499 score: 0.4898 time: 0.07s
     INFO: Early stopping counter 13 of 20
Epoch 22/1000, LR 0.000270
Train loss: 0.0101;  Loss pred: 0.0101; Loss self: 0.0000; time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6797 score: 0.5102 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6491 score: 0.4898 time: 0.07s
     INFO: Early stopping counter 14 of 20
Epoch 23/1000, LR 0.000270
Train loss: 0.0097;  Loss pred: 0.0097; Loss self: 0.0000; time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6795 score: 0.5102 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6489 score: 0.4898 time: 0.08s
     INFO: Early stopping counter 15 of 20
Epoch 24/1000, LR 0.000270
Train loss: 0.0074;  Loss pred: 0.0074; Loss self: 0.0000; time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6795 score: 0.5102 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6488 score: 0.4898 time: 0.07s
     INFO: Early stopping counter 16 of 20
Epoch 25/1000, LR 0.000270
Train loss: 0.0061;  Loss pred: 0.0061; Loss self: 0.0000; time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6782 score: 0.5102 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6478 score: 0.4898 time: 0.07s
     INFO: Early stopping counter 17 of 20
Epoch 26/1000, LR 0.000270
Train loss: 0.0071;  Loss pred: 0.0071; Loss self: 0.0000; time: 0.20s
Val loss: 0.6764 score: 0.5306 time: 0.06s
Test loss: 0.6452 score: 0.5306 time: 0.07s
     INFO: Early stopping counter 18 of 20
Epoch 27/1000, LR 0.000270
Train loss: 0.0062;  Loss pred: 0.0062; Loss self: 0.0000; time: 0.20s
Val loss: 0.6737 score: 0.5714 time: 0.06s
Test loss: 0.6423 score: 0.5306 time: 0.07s
     INFO: Early stopping counter 19 of 20
Epoch 28/1000, LR 0.000270
Train loss: 0.0042;  Loss pred: 0.0042; Loss self: 0.0000; time: 0.20s
Val loss: 0.6703 score: 0.5714 time: 0.06s
Test loss: 0.6385 score: 0.5306 time: 0.07s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 007,   Train_Loss: 0.2184,   Val_Loss: 0.6657,   Val_Precision: 0.5102,   Val_Recall: 1.0000,   Val_accuracy: 0.6757,   Val_Score: 0.5102,   Val_Loss: 0.6657,   Test_Precision: 0.4898,   Test_Recall: 1.0000,   Test_accuracy: 0.6575,   Test_Score: 0.4898,   Test_loss: 0.6265


[0.06653004908002913, 0.06610951002221555, 0.06770800799131393, 0.0676920679397881, 0.06670175201725215, 0.06619879498612136, 0.06624210090376437, 0.06598499894607812, 0.06619761209003627, 0.06596349796745926, 0.06603561306837946, 0.06581066292710602, 0.06567679799627513, 0.06595130905043334, 0.06575967906974256, 0.06574518303386867, 0.06608675001189113, 0.06613295606803149, 0.06639591895509511, 0.06623150710947812, 0.06578854203689843, 0.06584402895532548, 0.06592013605404645, 0.06624365702737123, 0.06574834999628365, 0.06564155709929764, 0.06610373791772872, 0.06612707697786391, 0.06647406402043998, 0.06724994501564652, 0.06639685505069792, 0.06637395592406392, 0.066052061971277, 0.06553001899737865, 0.06588779110461473, 0.06585439096670598, 0.06607440405059606, 0.06621538801118731, 0.06610940396785736, 0.06649822799954563, 0.06600356101989746, 0.06571503798477352, 0.06575496098957956, 0.06629602995235473, 0.0667801279341802, 0.06611341494135559, 0.06602570903487504, 0.06633813609369099, 0.06571667699608952, 0.0659255530918017, 0.06623123097233474, 0.06624143000226468, 0.06656469404697418, 0.06673382001463324, 0.06570359808392823, 0.06581750302575529, 0.06646456802263856, 0.06641056004446, 0.06607569893822074, 0.06649216893129051, 0.06598602898884565, 0.06633992597926408, 0.06594560900703073, 0.06618394399993122, 0.066159290028736, 0.0667649069800973, 0.06609958095941693, 0.06554643099661916, 0.06612603005487472, 0.07081823702901602, 0.06618931202683598, 0.06595342606306076, 0.06575548299588263, 0.067293671076186, 0.06801166606601328, 0.06605721008963883, 0.06609580700751394, 0.06629381596576422, 0.06676653597969562, 0.06581298890523612, 0.06603154796175659, 0.0666289689252153, 0.06618497800081968, 0.06602604407817125, 0.06613494001794606, 0.06623689201660454, 0.06591854593716562, 0.06599544803611934, 0.06614058592822403, 0.06758303206879646, 0.06575622595846653, 0.06671825307421386, 0.0656401610467583, 0.06598355900496244, 0.06648111995309591, 0.06661088101100177, 0.06667307100724429, 0.06653862597886473, 0.06621336203534156, 0.06601031497120857, 0.06618479697499424, 0.06634152808692306, 0.06647824903484434, 0.06643100106157362, 0.06746940896846354, 0.06646313308738172, 0.06646295299287885, 0.06669786595739424, 0.06599911698140204, 0.06633187900297344, 0.06663203798234463, 0.06683707097545266, 0.06675081001594663, 0.06632602494210005, 0.06618673505727202, 0.06698055996093899, 0.06610000599175692, 0.0669174250215292, 0.0664925379678607, 0.06737054791301489, 0.06652807304635644, 0.06654881103895605, 0.06667103001382202, 0.06685043801553547, 0.06703193602152169, 0.06639367807656527, 0.06653823191300035, 0.06658501701895148, 0.06664203898981214, 0.06635120394639671, 0.06710969691630453, 0.0666060980875045, 0.06667353003285825, 0.06626723194494843, 0.06784916610922664, 0.06629418500233442, 0.06626583903562278, 0.06570346700027585, 0.06661877792794257, 0.06630181905347854, 0.06640631798654795, 0.06609042594209313, 0.06599961803294718, 0.06624913599807769, 0.06592124700546265, 0.06583352200686932, 0.06639320100657642, 0.06622853397857398, 0.06689190398901701, 0.06731797999236733, 0.07384751504287124, 0.07406147092115134, 0.07408939802553505, 0.07508392399176955, 0.17102760798297822, 0.07349557604175061, 0.07534607592970133, 0.07412420096807182, 0.0755501309176907, 0.1987082469277084, 0.07323490793351084, 0.07366963103413582, 0.07277535798493773, 0.0730594729539007, 0.14436886308249086, 0.07751418801490217, 0.07695669599343091, 0.07742410397622734, 0.07714990293607116, 0.0777791989967227, 0.07720438996329904, 0.07678910403046757, 0.08301036094781011, 0.07725844602100551, 0.07616445701569319, 0.0766218330245465, 0.07628140994347632, 0.0768672259291634]
[0.0013577561036740638, 0.0013491736739227663, 0.0013817960814553865, 0.00138147077428139, 0.0013612602452500438, 0.001350995816043293, 0.0013518796102809055, 0.0013466326315526146, 0.0013509716753068628, 0.0013461938360705972, 0.0013476655728240706, 0.0013430747536144086, 0.0013403428162505127, 0.0013459450826619047, 0.00134203426672944, 0.001341738429262626, 0.0013487091839161454, 0.0013496521646537039, 0.0013550187541856145, 0.0013516634103975125, 0.0013426233068754782, 0.0013437556929658263, 0.0013453088990621725, 0.0013519113679055352, 0.001341803061148646, 0.0013396236142713804, 0.0013490558758720147, 0.0013495321832217124, 0.0013566135514375506, 0.001372447857462174, 0.0013550378581775086, 0.001354570529062529, 0.0013480012647199388, 0.0013373473264771153, 0.0013446487980533618, 0.0013439671625858362, 0.0013484572255223685, 0.0013513344492079044, 0.0013491715095481094, 0.0013571066938682782, 0.0013470114493856625, 0.0013411232241790515, 0.0013419379793791746, 0.0013529802031092802, 0.0013628597537587797, 0.001349253366150114, 0.0013474634496913273, 0.0013538395121161426, 0.001341156673389582, 0.001345419450853096, 0.001351657774945607, 0.0013518659184135649, 0.0013584631438157996, 0.0013619146941761886, 0.0013408897568148617, 0.0013432143474643935, 0.0013564197555640523, 0.001355317551927755, 0.0013484836518004232, 0.001356983039414092, 0.0013466536528335847, 0.0013538760403931445, 0.001345828755245525, 0.0013506927346924739, 0.0013501895924231835, 0.0013625491220428019, 0.0013489710399881005, 0.0013376822652371259, 0.0013495108174464228, 0.0014452701434493065, 0.0013508022862619587, 0.00134598828700124, 0.0013419486325690333, 0.0013733402260446123, 0.001387993185020679, 0.001348106328359976, 0.0013488940205615089, 0.001352935019709474, 0.0013625823669325635, 0.0013431222225558392, 0.00134758261146442, 0.0013597748760248022, 0.0013507138367514222, 0.0013474702873096174, 0.0013496926534274708, 0.0013517733064613172, 0.0013452764476972576, 0.0013468458782881498, 0.0013498078760862046, 0.0013792455524244175, 0.0013419637950707454, 0.0013615970015145686, 0.0013395951234032304, 0.0013466032449992336, 0.001356757550063182, 0.0013594057349184034, 0.0013606749185151896, 0.0013579311424258109, 0.0013512931027620727, 0.0013471492851267056, 0.0013507101423468214, 0.0013539087364678175, 0.0013566989598947825, 0.001355734715542319, 0.001376926713642113, 0.0013563904711710556, 0.0013563867957730377, 0.0013611809379060048, 0.0013469207547224906, 0.001353711816387213, 0.001359837509843768, 0.001364021856641891, 0.0013622614288968699, 0.0013535923457571439, 0.0013507496950463677, 0.0013669502032844691, 0.0013489797141174882, 0.001365661735133249, 0.0013569905707726674, 0.0013749091410819366, 0.001357715776456254, 0.0013581390007950213, 0.0013606332655882044, 0.001364294653378275, 0.001367998694316769, 0.0013549730219707197, 0.0013579231002653132, 0.0013588778983459485, 0.0013600416120369823, 0.001354106202987688, 0.0013695856513531537, 0.001359308124234786, 0.0013606842863848622, 0.001352392488672417, 0.0013846768593719723, 0.0013529425510680493, 0.0013523640619514852, 0.0013408870816382827, 0.001359566896488624, 0.0013530983480301742, 0.001355230979317305, 0.00134878420289986, 0.0013469309802642282, 0.0013520231836342386, 0.001345331571540054, 0.0013435412654463127, 0.0013549632858484983, 0.0013516027342566118, 0.001365140897735041, 0.0013738363263748434, 0.001507092143732066, 0.0015114585902275784, 0.0015120285311333683, 0.0015323249794238685, 0.0034903593465913925, 0.0014999097151377676, 0.0015376750189734965, 0.0015127387952667717, 0.0015418394064834835, 0.004055270345463436, 0.0014945899578267519, 0.0015034618578395065, 0.0014852113874477086, 0.0014910096521204223, 0.002946303328214099, 0.0015819222043857587, 0.0015705448161924677, 0.0015800837546168845, 0.0015744878150218604, 0.001587330591769851, 0.001575599795169368, 0.0015671245720503585, 0.0016940889989349004, 0.0015767029800205206, 0.001554376673789657, 0.0015637108780519695, 0.0015567634682342106, 0.0015687188965135387]
[736.5093018503234, 741.1944209469101, 723.6957850877266, 723.8662001519197, 734.6133874763341, 740.1947423706564, 739.7108384467839, 742.593025424491, 740.2079690329982, 742.8350756076096, 742.0238523304207, 744.560194664411, 746.0777853813617, 742.972364089535, 745.1374564652635, 745.3017504682833, 741.449685317909, 740.9316460857023, 737.9971656562157, 739.8291559182686, 744.810547291315, 744.1828936872319, 743.3237085528159, 739.6934619680444, 745.2658508201296, 746.4783311870019, 741.259141215045, 740.9975193127437, 737.1295966639423, 728.6251310480559, 737.9867610082676, 738.2413676843241, 741.8390666033688, 747.7489057642439, 743.6886133001365, 744.0657985095169, 741.5882247303897, 740.0092557295184, 741.1956099895256, 736.861740140426, 742.3841871990579, 745.6436380871229, 745.1909219102909, 739.1091146063354, 733.7512148568412, 741.1506430799914, 742.1351578991452, 738.6399872736263, 745.6250413105299, 743.2626303759215, 739.8322404798372, 739.7183303308032, 736.1259704043871, 734.2603793587028, 745.773464908399, 744.482816080483, 737.2349126426288, 737.8344643863247, 741.5736918017905, 736.92888632696, 742.581433537742, 738.6200583840863, 743.0365832966364, 740.360834344519, 740.636726583932, 733.9184942563758, 741.3057585052538, 747.5616788735207, 741.0092509611923, 691.9121691764716, 740.3007902564889, 742.9485157169713, 745.1850061396127, 728.1516852383493, 720.4646325299511, 741.7812519406677, 741.3480857330262, 739.1337983214727, 733.9005877869926, 744.5338802429246, 742.0695336171624, 735.4158527501431, 740.3492677657632, 742.1313920002032, 740.9094192374498, 739.7690095078204, 743.3416393424001, 742.4754503247296, 740.8461735306512, 725.0340581067778, 745.1765864870312, 734.4316996054287, 746.4942074882358, 742.6092308284681, 737.0513618689143, 735.6155519382326, 734.9293989274314, 736.4143650270794, 740.0318983024321, 742.3082289695499, 740.3512927374098, 738.6022211577405, 737.0831920425104, 737.6074305215246, 726.2550650607228, 737.2508295023913, 737.2528272291796, 734.6561887197498, 742.4341755027989, 738.7096632345285, 735.3819796564446, 733.1260823502618, 734.0734889703062, 738.7748631517572, 740.3296137451082, 731.5555443038294, 741.3009918049116, 732.2457489097247, 736.9247963385643, 727.3207880581004, 736.5311778361149, 736.3016594138188, 734.9518972459475, 732.9794905549132, 730.9948497424833, 738.0220740820104, 736.4187263657408, 735.9012912177162, 735.2716204780417, 738.4945123163965, 730.1478363269927, 735.6683758238728, 734.9243391770568, 739.4303121142407, 722.1901581092035, 739.1296838218097, 739.445854954902, 745.7749527858899, 735.5283528767261, 739.0445797645006, 737.8815974998948, 741.4084461028083, 742.4285391400155, 739.6322874523496, 743.3111815366532, 744.3016643540225, 738.0273771578874, 739.8623683237848, 732.52512005108, 727.88874540733, 663.5294359133638, 661.6125684590745, 661.3631815865484, 652.6030792606313, 286.50345156483036, 666.7067956874654, 650.3324744571636, 661.0526570277123, 648.5759773650669, 246.59268428766615, 669.0798334106811, 665.131605957076, 673.3048295020617, 670.6864697876781, 339.40836655340223, 632.1423374851028, 636.7217220991748, 632.877844024455, 635.1271762532605, 629.9884883368966, 634.6789350099564, 638.111365130114, 590.287759750943, 634.2348639354922, 643.3447032899331, 639.5044084145364, 642.3583417808934, 637.4628381302026]
Elapsed: 0.06945212831095468~0.014074609634144798
Time per graph: 0.0014173903736929526~0.0002872369313090775
Speed: 718.1035203272112~65.16722234871979
Total Time: 0.0775
best val loss: 0.6657431721687317 test_score: 0.4898

Testing...
Test loss: 0.6423 score: 0.5306 time: 0.07s
test Score 0.5306
Epoch Time List: [0.34173178183846176, 0.3283941481495276, 0.3296439310070127, 0.33199621888343245, 0.328264641109854, 0.32853396818973124, 0.3268393420148641, 0.3262431720504537, 0.3267954319017008, 0.32771452597808093, 0.33072660001926124, 0.3300640902016312, 0.32886464009061456, 0.32925805693957955, 0.3326858729124069, 0.33657654316630214, 0.3296876910608262, 0.32936913101002574, 0.33049840782769024, 0.3280723198549822, 0.32896482187788934, 0.33021015592385083, 0.32883515302091837, 0.32709692197386175, 0.3297765851020813, 0.32832052803132683, 0.32941042305901647, 0.3292831960134208, 0.329916590009816, 0.3355257530929521, 0.33919497614260763, 0.3307043630629778, 0.32910826499573886, 0.32943040900863707, 0.32845854898914695, 0.3268160540610552, 0.32727101806085557, 0.3287673101294786, 0.3292882919777185, 0.32930946606211364, 0.3291163770481944, 0.32846418709959835, 0.32749698299448937, 0.3282003180356696, 0.3334445410873741, 0.33017892809584737, 0.3275304251583293, 0.3276485699461773, 0.3267573819030076, 0.3267537389183417, 0.32740929012652487, 0.33078396797645837, 0.3303196020424366, 0.33050227409694344, 0.32845917192753404, 0.3288658680394292, 0.3301896839402616, 0.32653154409490526, 0.32650251500308514, 0.33031935908365995, 0.33709537400864065, 0.3301983700366691, 0.3291788628557697, 0.3298754539573565, 0.3340656330110505, 0.3424608149798587, 0.3320079939439893, 0.3289565750164911, 0.3288702921709046, 0.35227956995368004, 0.33254632086027414, 0.3298281991155818, 0.3302711199503392, 0.33518447005189955, 0.3349147760309279, 0.33183614211156964, 0.32952570903580636, 0.3296131599927321, 0.33079052006360143, 0.32889569620601833, 0.33053318003658205, 0.33077916700858623, 0.3328121380181983, 0.3301131749758497, 0.32977318798657507, 0.3310423350194469, 0.3291421299800277, 0.32929914514534175, 0.32989127398468554, 0.3330668529961258, 0.332504601101391, 0.32982042396906763, 0.3287309060106054, 0.32932235195767134, 0.32867449009791017, 0.3306796009419486, 0.3293301409576088, 0.32956018985714763, 0.33213675394654274, 0.3293540918966755, 0.33137634897138923, 0.33110448403749615, 0.3313313939142972, 0.33208656986244023, 0.33538681408390403, 0.3340346359182149, 0.3299824499990791, 0.3327450258657336, 0.3274789290735498, 0.32726919604465365, 0.32722526602447033, 0.3286510822363198, 0.32671639579348266, 0.32626078301109374, 0.3277059089159593, 0.32773389806970954, 0.32708470604848117, 0.32848030992317945, 0.3277462701080367, 0.33073206897825, 0.33397366502322257, 0.3275739620439708, 0.3273940399521962, 0.32766594109125435, 0.32926953805144876, 0.3273344661574811, 0.3270749399671331, 0.32882631209213287, 0.32836127607151866, 0.32700107991695404, 0.3295759118627757, 0.3278880879515782, 0.32770410703960806, 0.3279180289246142, 0.3305149609223008, 0.33133166190236807, 0.32687180303037167, 0.3265653708949685, 0.32608731102664024, 0.3269410200882703, 0.3271347559057176, 0.3286738229217008, 0.326653953990899, 0.32648552500177175, 0.3258517680224031, 0.32633360696490854, 0.3267153979977593, 0.3280975391389802, 0.3272959579480812, 0.3279249439947307, 0.3250607111258432, 0.3259236590238288, 0.32572758907917887, 0.329522502142936, 0.4245877790963277, 0.32599681499414146, 0.3276583990082145, 0.32817863894160837, 0.33022663509473205, 0.46028115414083004, 0.3713264740072191, 0.3269664440304041, 0.3244339730590582, 0.3269934229319915, 0.3954720329493284, 0.34410258498974144, 0.340199331054464, 0.3415358540369198, 0.34133830294013023, 0.3394980590092018, 0.34017626103013754, 0.339146891143173, 0.3459040980087593, 0.3443237339379266, 0.3372790828580037, 0.33689443103503436, 0.3375165490433574, 0.3373852219665423]
Total Epoch List: [150, 28]
Total Time List: [0.06792019703425467, 0.07753906096331775]
========================k_idx:2========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: GPSTransformer
GPS(
  (x_lin): Linear(in_features=14887, out_features=64, bias=True)
  (pe_lin): Linear(in_features=20, out_features=8, bias=True)
  (pe_norm): BatchNorm1d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (convs): ModuleList(
    (0-1): 2 x GPSConv(64, conv=GINEConv(nn=Sequential(
      (0): Linear(in_features=64, out_features=64, bias=True)
      (1): ReLU()
      (2): Linear(in_features=64, out_features=64, bias=True)
    )), heads=1, attn_type=multihead)
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=32, bias=True)
    (1): ReLU()
    (2): Linear(in_features=32, out_features=16, bias=True)
    (3): ReLU()
    (4): Linear(in_features=16, out_features=2, bias=True)
  )
)
Total number of parameters: 1039906
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x74b5c4fad4b0>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.9305;  Loss pred: 0.9305; Loss self: 0.0000; time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 2.9493 score: 0.4898 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 3.2532 score: 0.5000 time: 0.06s
Epoch 2/1000, LR 0.000000
Train loss: 0.9294;  Loss pred: 0.9294; Loss self: 0.0000; time: 0.20s
Val loss: 2.5181 score: 0.4694 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 2.7699 score: 0.5000 time: 0.07s
Epoch 3/1000, LR 0.000030
Train loss: 0.8543;  Loss pred: 0.8543; Loss self: 0.0000; time: 0.20s
Val loss: 2.0733 score: 0.4694 time: 0.06s
Test loss: 2.2871 score: 0.4375 time: 0.06s
Epoch 4/1000, LR 0.000060
Train loss: 0.6952;  Loss pred: 0.6952; Loss self: 0.0000; time: 0.20s
Val loss: 1.7015 score: 0.4286 time: 0.06s
Test loss: 1.8909 score: 0.4375 time: 0.06s
Epoch 5/1000, LR 0.000090
Train loss: 0.5539;  Loss pred: 0.5539; Loss self: 0.0000; time: 0.20s
Val loss: 1.4077 score: 0.4490 time: 0.06s
Test loss: 1.5528 score: 0.3958 time: 0.07s
Epoch 6/1000, LR 0.000120
Train loss: 0.4588;  Loss pred: 0.4588; Loss self: 0.0000; time: 0.20s
Val loss: 1.1839 score: 0.4286 time: 0.06s
Test loss: 1.2994 score: 0.3958 time: 0.06s
Epoch 7/1000, LR 0.000150
Train loss: 0.3808;  Loss pred: 0.3808; Loss self: 0.0000; time: 0.19s
Val loss: 1.0118 score: 0.4082 time: 0.06s
Test loss: 1.1099 score: 0.4375 time: 0.06s
Epoch 8/1000, LR 0.000180
Train loss: 0.3234;  Loss pred: 0.3234; Loss self: 0.0000; time: 0.19s
Val loss: 0.8751 score: 0.3673 time: 0.06s
Test loss: 0.9685 score: 0.5625 time: 0.08s
Epoch 9/1000, LR 0.000210
Train loss: 0.2698;  Loss pred: 0.2698; Loss self: 0.0000; time: 0.20s
Val loss: 0.7790 score: 0.3673 time: 0.09s
Test loss: 0.8682 score: 0.4792 time: 0.07s
Epoch 10/1000, LR 0.000240
Train loss: 0.2228;  Loss pred: 0.2228; Loss self: 0.0000; time: 0.19s
Val loss: 0.7173 score: 0.4082 time: 0.07s
Test loss: 0.7963 score: 0.4792 time: 0.06s
Epoch 11/1000, LR 0.000270
Train loss: 0.1817;  Loss pred: 0.1817; Loss self: 0.0000; time: 0.20s
Val loss: 0.6795 score: 0.4898 time: 0.06s
Test loss: 0.7484 score: 0.5625 time: 0.07s
Epoch 12/1000, LR 0.000270
Train loss: 0.1517;  Loss pred: 0.1517; Loss self: 0.0000; time: 0.20s
Val loss: 0.6581 score: 0.6327 time: 0.06s
Test loss: 0.7058 score: 0.5833 time: 0.06s
Epoch 13/1000, LR 0.000270
Train loss: 0.1339;  Loss pred: 0.1339; Loss self: 0.0000; time: 0.19s
Val loss: 0.6450 score: 0.6327 time: 0.06s
Test loss: 0.6721 score: 0.6042 time: 0.06s
Epoch 14/1000, LR 0.000270
Train loss: 0.1160;  Loss pred: 0.1160; Loss self: 0.0000; time: 0.19s
Val loss: 0.6368 score: 0.6122 time: 0.06s
Test loss: 0.6441 score: 0.6042 time: 0.06s
Epoch 15/1000, LR 0.000270
Train loss: 0.0968;  Loss pred: 0.0968; Loss self: 0.0000; time: 0.20s
Val loss: 0.6324 score: 0.5918 time: 0.06s
Test loss: 0.6289 score: 0.6042 time: 0.06s
Epoch 16/1000, LR 0.000270
Train loss: 0.0859;  Loss pred: 0.0859; Loss self: 0.0000; time: 0.19s
Val loss: 0.6314 score: 0.5918 time: 0.06s
Test loss: 0.6235 score: 0.6042 time: 0.06s
Epoch 17/1000, LR 0.000270
Train loss: 0.0763;  Loss pred: 0.0763; Loss self: 0.0000; time: 0.19s
Val loss: 0.6330 score: 0.5918 time: 0.06s
Test loss: 0.6254 score: 0.6250 time: 0.06s
     INFO: Early stopping counter 1 of 20
Epoch 18/1000, LR 0.000270
Train loss: 0.0695;  Loss pred: 0.0695; Loss self: 0.0000; time: 0.20s
Val loss: 0.6377 score: 0.5714 time: 0.06s
Test loss: 0.6288 score: 0.6458 time: 0.06s
     INFO: Early stopping counter 2 of 20
Epoch 19/1000, LR 0.000270
Train loss: 0.0613;  Loss pred: 0.0613; Loss self: 0.0000; time: 0.19s
Val loss: 0.6432 score: 0.5714 time: 0.06s
Test loss: 0.6349 score: 0.6667 time: 0.06s
     INFO: Early stopping counter 3 of 20
Epoch 20/1000, LR 0.000270
Train loss: 0.0554;  Loss pred: 0.0554; Loss self: 0.0000; time: 0.20s
Val loss: 0.6495 score: 0.5714 time: 0.06s
Test loss: 0.6422 score: 0.6667 time: 0.06s
     INFO: Early stopping counter 4 of 20
Epoch 21/1000, LR 0.000270
Train loss: 0.0524;  Loss pred: 0.0524; Loss self: 0.0000; time: 0.20s
Val loss: 0.6573 score: 0.5714 time: 0.06s
Test loss: 0.6492 score: 0.6667 time: 0.06s
     INFO: Early stopping counter 5 of 20
Epoch 22/1000, LR 0.000270
Train loss: 0.0478;  Loss pred: 0.0478; Loss self: 0.0000; time: 0.19s
Val loss: 0.6640 score: 0.5714 time: 0.06s
Test loss: 0.6550 score: 0.6667 time: 0.06s
     INFO: Early stopping counter 6 of 20
Epoch 23/1000, LR 0.000270
Train loss: 0.0446;  Loss pred: 0.0446; Loss self: 0.0000; time: 0.20s
Val loss: 0.6725 score: 0.5714 time: 0.06s
Test loss: 0.6612 score: 0.6667 time: 0.06s
     INFO: Early stopping counter 7 of 20
Epoch 24/1000, LR 0.000270
Train loss: 0.0405;  Loss pred: 0.0405; Loss self: 0.0000; time: 0.20s
Val loss: 0.6816 score: 0.6327 time: 0.06s
Test loss: 0.6670 score: 0.6667 time: 0.06s
     INFO: Early stopping counter 8 of 20
Epoch 25/1000, LR 0.000270
Train loss: 0.0391;  Loss pred: 0.0391; Loss self: 0.0000; time: 0.20s
Val loss: 0.6912 score: 0.6327 time: 0.06s
Test loss: 0.6750 score: 0.6667 time: 0.06s
     INFO: Early stopping counter 9 of 20
Epoch 26/1000, LR 0.000270
Train loss: 0.0348;  Loss pred: 0.0348; Loss self: 0.0000; time: 0.19s
Val loss: 0.7029 score: 0.6327 time: 0.06s
Test loss: 0.6844 score: 0.6667 time: 0.06s
     INFO: Early stopping counter 10 of 20
Epoch 27/1000, LR 0.000270
Train loss: 0.0317;  Loss pred: 0.0317; Loss self: 0.0000; time: 0.20s
Val loss: 0.7190 score: 0.6327 time: 0.06s
Test loss: 0.6990 score: 0.6667 time: 0.06s
     INFO: Early stopping counter 11 of 20
Epoch 28/1000, LR 0.000270
Train loss: 0.0306;  Loss pred: 0.0306; Loss self: 0.0000; time: 0.20s
Val loss: 0.7364 score: 0.6122 time: 0.06s
Test loss: 0.7144 score: 0.6458 time: 0.06s
     INFO: Early stopping counter 12 of 20
Epoch 29/1000, LR 0.000270
Train loss: 0.0287;  Loss pred: 0.0287; Loss self: 0.0000; time: 0.20s
Val loss: 0.7544 score: 0.5918 time: 0.06s
Test loss: 0.7303 score: 0.6042 time: 0.06s
     INFO: Early stopping counter 13 of 20
Epoch 30/1000, LR 0.000270
Train loss: 0.0265;  Loss pred: 0.0265; Loss self: 0.0000; time: 0.19s
Val loss: 0.7716 score: 0.5918 time: 0.06s
Test loss: 0.7488 score: 0.6250 time: 0.06s
     INFO: Early stopping counter 14 of 20
Epoch 31/1000, LR 0.000270
Train loss: 0.0259;  Loss pred: 0.0259; Loss self: 0.0000; time: 0.20s
Val loss: 0.7882 score: 0.6122 time: 0.06s
Test loss: 0.7665 score: 0.6250 time: 0.06s
     INFO: Early stopping counter 15 of 20
Epoch 32/1000, LR 0.000270
Train loss: 0.0247;  Loss pred: 0.0247; Loss self: 0.0000; time: 0.19s
Val loss: 0.8015 score: 0.5918 time: 0.06s
Test loss: 0.7812 score: 0.6250 time: 0.06s
     INFO: Early stopping counter 16 of 20
Epoch 33/1000, LR 0.000270
Train loss: 0.0245;  Loss pred: 0.0245; Loss self: 0.0000; time: 0.20s
Val loss: 0.8124 score: 0.5918 time: 0.06s
Test loss: 0.7949 score: 0.6250 time: 0.06s
     INFO: Early stopping counter 17 of 20
Epoch 34/1000, LR 0.000270
Train loss: 0.0221;  Loss pred: 0.0221; Loss self: 0.0000; time: 0.19s
Val loss: 0.8228 score: 0.5918 time: 0.06s
Test loss: 0.8084 score: 0.6667 time: 0.06s
     INFO: Early stopping counter 18 of 20
Epoch 35/1000, LR 0.000270
Train loss: 0.0210;  Loss pred: 0.0210; Loss self: 0.0000; time: 0.20s
Val loss: 0.8237 score: 0.5918 time: 0.06s
Test loss: 0.8088 score: 0.6667 time: 0.06s
     INFO: Early stopping counter 19 of 20
Epoch 36/1000, LR 0.000270
Train loss: 0.0203;  Loss pred: 0.0203; Loss self: 0.0000; time: 0.20s
Val loss: 0.8175 score: 0.5918 time: 0.06s
Test loss: 0.8008 score: 0.6667 time: 0.07s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 015,   Train_Loss: 0.0859,   Val_Loss: 0.6314,   Val_Precision: 0.6000,   Val_Recall: 0.6000,   Val_accuracy: 0.6000,   Val_Score: 0.5918,   Val_Loss: 0.6314,   Test_Precision: 0.5926,   Test_Recall: 0.6667,   Test_accuracy: 0.6275,   Test_Score: 0.6042,   Test_loss: 0.6235


[0.06653004908002913, 0.06610951002221555, 0.06770800799131393, 0.0676920679397881, 0.06670175201725215, 0.06619879498612136, 0.06624210090376437, 0.06598499894607812, 0.06619761209003627, 0.06596349796745926, 0.06603561306837946, 0.06581066292710602, 0.06567679799627513, 0.06595130905043334, 0.06575967906974256, 0.06574518303386867, 0.06608675001189113, 0.06613295606803149, 0.06639591895509511, 0.06623150710947812, 0.06578854203689843, 0.06584402895532548, 0.06592013605404645, 0.06624365702737123, 0.06574834999628365, 0.06564155709929764, 0.06610373791772872, 0.06612707697786391, 0.06647406402043998, 0.06724994501564652, 0.06639685505069792, 0.06637395592406392, 0.066052061971277, 0.06553001899737865, 0.06588779110461473, 0.06585439096670598, 0.06607440405059606, 0.06621538801118731, 0.06610940396785736, 0.06649822799954563, 0.06600356101989746, 0.06571503798477352, 0.06575496098957956, 0.06629602995235473, 0.0667801279341802, 0.06611341494135559, 0.06602570903487504, 0.06633813609369099, 0.06571667699608952, 0.0659255530918017, 0.06623123097233474, 0.06624143000226468, 0.06656469404697418, 0.06673382001463324, 0.06570359808392823, 0.06581750302575529, 0.06646456802263856, 0.06641056004446, 0.06607569893822074, 0.06649216893129051, 0.06598602898884565, 0.06633992597926408, 0.06594560900703073, 0.06618394399993122, 0.066159290028736, 0.0667649069800973, 0.06609958095941693, 0.06554643099661916, 0.06612603005487472, 0.07081823702901602, 0.06618931202683598, 0.06595342606306076, 0.06575548299588263, 0.067293671076186, 0.06801166606601328, 0.06605721008963883, 0.06609580700751394, 0.06629381596576422, 0.06676653597969562, 0.06581298890523612, 0.06603154796175659, 0.0666289689252153, 0.06618497800081968, 0.06602604407817125, 0.06613494001794606, 0.06623689201660454, 0.06591854593716562, 0.06599544803611934, 0.06614058592822403, 0.06758303206879646, 0.06575622595846653, 0.06671825307421386, 0.0656401610467583, 0.06598355900496244, 0.06648111995309591, 0.06661088101100177, 0.06667307100724429, 0.06653862597886473, 0.06621336203534156, 0.06601031497120857, 0.06618479697499424, 0.06634152808692306, 0.06647824903484434, 0.06643100106157362, 0.06746940896846354, 0.06646313308738172, 0.06646295299287885, 0.06669786595739424, 0.06599911698140204, 0.06633187900297344, 0.06663203798234463, 0.06683707097545266, 0.06675081001594663, 0.06632602494210005, 0.06618673505727202, 0.06698055996093899, 0.06610000599175692, 0.0669174250215292, 0.0664925379678607, 0.06737054791301489, 0.06652807304635644, 0.06654881103895605, 0.06667103001382202, 0.06685043801553547, 0.06703193602152169, 0.06639367807656527, 0.06653823191300035, 0.06658501701895148, 0.06664203898981214, 0.06635120394639671, 0.06710969691630453, 0.0666060980875045, 0.06667353003285825, 0.06626723194494843, 0.06784916610922664, 0.06629418500233442, 0.06626583903562278, 0.06570346700027585, 0.06661877792794257, 0.06630181905347854, 0.06640631798654795, 0.06609042594209313, 0.06599961803294718, 0.06624913599807769, 0.06592124700546265, 0.06583352200686932, 0.06639320100657642, 0.06622853397857398, 0.06689190398901701, 0.06731797999236733, 0.07384751504287124, 0.07406147092115134, 0.07408939802553505, 0.07508392399176955, 0.17102760798297822, 0.07349557604175061, 0.07534607592970133, 0.07412420096807182, 0.0755501309176907, 0.1987082469277084, 0.07323490793351084, 0.07366963103413582, 0.07277535798493773, 0.0730594729539007, 0.14436886308249086, 0.07751418801490217, 0.07695669599343091, 0.07742410397622734, 0.07714990293607116, 0.0777791989967227, 0.07720438996329904, 0.07678910403046757, 0.08301036094781011, 0.07725844602100551, 0.07616445701569319, 0.0766218330245465, 0.07628140994347632, 0.0768672259291634, 0.06908606202341616, 0.0700108470628038, 0.06888714607339352, 0.06907860992942005, 0.07006531592924148, 0.06861189194023609, 0.0686988061061129, 0.0858816730324179, 0.07125767297111452, 0.06833705399185419, 0.07116313895676285, 0.06773099105339497, 0.06761382904369384, 0.06785415101330727, 0.0681671230122447, 0.06756831507664174, 0.06803135899826884, 0.06800365494564176, 0.06768001907039434, 0.06857441493775696, 0.06771152000874281, 0.0676789190620184, 0.06820437300484627, 0.06757284491322935, 0.06735583499539644, 0.0676405590493232, 0.0674956829752773, 0.06760693504475057, 0.06801140494644642, 0.06874795095063746, 0.06754785892553627, 0.06842448399402201, 0.0673812871100381, 0.06759126798715442, 0.06754188996274024, 0.07007101096678525]
[0.0013577561036740638, 0.0013491736739227663, 0.0013817960814553865, 0.00138147077428139, 0.0013612602452500438, 0.001350995816043293, 0.0013518796102809055, 0.0013466326315526146, 0.0013509716753068628, 0.0013461938360705972, 0.0013476655728240706, 0.0013430747536144086, 0.0013403428162505127, 0.0013459450826619047, 0.00134203426672944, 0.001341738429262626, 0.0013487091839161454, 0.0013496521646537039, 0.0013550187541856145, 0.0013516634103975125, 0.0013426233068754782, 0.0013437556929658263, 0.0013453088990621725, 0.0013519113679055352, 0.001341803061148646, 0.0013396236142713804, 0.0013490558758720147, 0.0013495321832217124, 0.0013566135514375506, 0.001372447857462174, 0.0013550378581775086, 0.001354570529062529, 0.0013480012647199388, 0.0013373473264771153, 0.0013446487980533618, 0.0013439671625858362, 0.0013484572255223685, 0.0013513344492079044, 0.0013491715095481094, 0.0013571066938682782, 0.0013470114493856625, 0.0013411232241790515, 0.0013419379793791746, 0.0013529802031092802, 0.0013628597537587797, 0.001349253366150114, 0.0013474634496913273, 0.0013538395121161426, 0.001341156673389582, 0.001345419450853096, 0.001351657774945607, 0.0013518659184135649, 0.0013584631438157996, 0.0013619146941761886, 0.0013408897568148617, 0.0013432143474643935, 0.0013564197555640523, 0.001355317551927755, 0.0013484836518004232, 0.001356983039414092, 0.0013466536528335847, 0.0013538760403931445, 0.001345828755245525, 0.0013506927346924739, 0.0013501895924231835, 0.0013625491220428019, 0.0013489710399881005, 0.0013376822652371259, 0.0013495108174464228, 0.0014452701434493065, 0.0013508022862619587, 0.00134598828700124, 0.0013419486325690333, 0.0013733402260446123, 0.001387993185020679, 0.001348106328359976, 0.0013488940205615089, 0.001352935019709474, 0.0013625823669325635, 0.0013431222225558392, 0.00134758261146442, 0.0013597748760248022, 0.0013507138367514222, 0.0013474702873096174, 0.0013496926534274708, 0.0013517733064613172, 0.0013452764476972576, 0.0013468458782881498, 0.0013498078760862046, 0.0013792455524244175, 0.0013419637950707454, 0.0013615970015145686, 0.0013395951234032304, 0.0013466032449992336, 0.001356757550063182, 0.0013594057349184034, 0.0013606749185151896, 0.0013579311424258109, 0.0013512931027620727, 0.0013471492851267056, 0.0013507101423468214, 0.0013539087364678175, 0.0013566989598947825, 0.001355734715542319, 0.001376926713642113, 0.0013563904711710556, 0.0013563867957730377, 0.0013611809379060048, 0.0013469207547224906, 0.001353711816387213, 0.001359837509843768, 0.001364021856641891, 0.0013622614288968699, 0.0013535923457571439, 0.0013507496950463677, 0.0013669502032844691, 0.0013489797141174882, 0.001365661735133249, 0.0013569905707726674, 0.0013749091410819366, 0.001357715776456254, 0.0013581390007950213, 0.0013606332655882044, 0.001364294653378275, 0.001367998694316769, 0.0013549730219707197, 0.0013579231002653132, 0.0013588778983459485, 0.0013600416120369823, 0.001354106202987688, 0.0013695856513531537, 0.001359308124234786, 0.0013606842863848622, 0.001352392488672417, 0.0013846768593719723, 0.0013529425510680493, 0.0013523640619514852, 0.0013408870816382827, 0.001359566896488624, 0.0013530983480301742, 0.001355230979317305, 0.00134878420289986, 0.0013469309802642282, 0.0013520231836342386, 0.001345331571540054, 0.0013435412654463127, 0.0013549632858484983, 0.0013516027342566118, 0.001365140897735041, 0.0013738363263748434, 0.001507092143732066, 0.0015114585902275784, 0.0015120285311333683, 0.0015323249794238685, 0.0034903593465913925, 0.0014999097151377676, 0.0015376750189734965, 0.0015127387952667717, 0.0015418394064834835, 0.004055270345463436, 0.0014945899578267519, 0.0015034618578395065, 0.0014852113874477086, 0.0014910096521204223, 0.002946303328214099, 0.0015819222043857587, 0.0015705448161924677, 0.0015800837546168845, 0.0015744878150218604, 0.001587330591769851, 0.001575599795169368, 0.0015671245720503585, 0.0016940889989349004, 0.0015767029800205206, 0.001554376673789657, 0.0015637108780519695, 0.0015567634682342106, 0.0015687188965135387, 0.00143929295882117, 0.0014585593138084125, 0.0014351488765290317, 0.0014391377068629179, 0.0014596940818591975, 0.0014294144154215853, 0.0014312251272106853, 0.001789201521508706, 0.0014845348535648857, 0.0014236886248302956, 0.0014825653949325595, 0.0014110623136123952, 0.0014086214384102884, 0.0014136281461105682, 0.0014201483960884314, 0.0014076732307633695, 0.0014173199791306008, 0.0014167428113675367, 0.001410000397299882, 0.0014286336445366032, 0.0014106566668488085, 0.0014099774804587166, 0.001420924437600964, 0.0014077676023589447, 0.0014032465624040924, 0.0014091783135275666, 0.001406160061984944, 0.0014084778134323035, 0.0014169042697176337, 0.0014322489781382803, 0.0014072470609486725, 0.001425510083208792, 0.0014037768147924605, 0.0014081514163990505, 0.0014071227075570885, 0.0014598127284746927]
[736.5093018503234, 741.1944209469101, 723.6957850877266, 723.8662001519197, 734.6133874763341, 740.1947423706564, 739.7108384467839, 742.593025424491, 740.2079690329982, 742.8350756076096, 742.0238523304207, 744.560194664411, 746.0777853813617, 742.972364089535, 745.1374564652635, 745.3017504682833, 741.449685317909, 740.9316460857023, 737.9971656562157, 739.8291559182686, 744.810547291315, 744.1828936872319, 743.3237085528159, 739.6934619680444, 745.2658508201296, 746.4783311870019, 741.259141215045, 740.9975193127437, 737.1295966639423, 728.6251310480559, 737.9867610082676, 738.2413676843241, 741.8390666033688, 747.7489057642439, 743.6886133001365, 744.0657985095169, 741.5882247303897, 740.0092557295184, 741.1956099895256, 736.861740140426, 742.3841871990579, 745.6436380871229, 745.1909219102909, 739.1091146063354, 733.7512148568412, 741.1506430799914, 742.1351578991452, 738.6399872736263, 745.6250413105299, 743.2626303759215, 739.8322404798372, 739.7183303308032, 736.1259704043871, 734.2603793587028, 745.773464908399, 744.482816080483, 737.2349126426288, 737.8344643863247, 741.5736918017905, 736.92888632696, 742.581433537742, 738.6200583840863, 743.0365832966364, 740.360834344519, 740.636726583932, 733.9184942563758, 741.3057585052538, 747.5616788735207, 741.0092509611923, 691.9121691764716, 740.3007902564889, 742.9485157169713, 745.1850061396127, 728.1516852383493, 720.4646325299511, 741.7812519406677, 741.3480857330262, 739.1337983214727, 733.9005877869926, 744.5338802429246, 742.0695336171624, 735.4158527501431, 740.3492677657632, 742.1313920002032, 740.9094192374498, 739.7690095078204, 743.3416393424001, 742.4754503247296, 740.8461735306512, 725.0340581067778, 745.1765864870312, 734.4316996054287, 746.4942074882358, 742.6092308284681, 737.0513618689143, 735.6155519382326, 734.9293989274314, 736.4143650270794, 740.0318983024321, 742.3082289695499, 740.3512927374098, 738.6022211577405, 737.0831920425104, 737.6074305215246, 726.2550650607228, 737.2508295023913, 737.2528272291796, 734.6561887197498, 742.4341755027989, 738.7096632345285, 735.3819796564446, 733.1260823502618, 734.0734889703062, 738.7748631517572, 740.3296137451082, 731.5555443038294, 741.3009918049116, 732.2457489097247, 736.9247963385643, 727.3207880581004, 736.5311778361149, 736.3016594138188, 734.9518972459475, 732.9794905549132, 730.9948497424833, 738.0220740820104, 736.4187263657408, 735.9012912177162, 735.2716204780417, 738.4945123163965, 730.1478363269927, 735.6683758238728, 734.9243391770568, 739.4303121142407, 722.1901581092035, 739.1296838218097, 739.445854954902, 745.7749527858899, 735.5283528767261, 739.0445797645006, 737.8815974998948, 741.4084461028083, 742.4285391400155, 739.6322874523496, 743.3111815366532, 744.3016643540225, 738.0273771578874, 739.8623683237848, 732.52512005108, 727.88874540733, 663.5294359133638, 661.6125684590745, 661.3631815865484, 652.6030792606313, 286.50345156483036, 666.7067956874654, 650.3324744571636, 661.0526570277123, 648.5759773650669, 246.59268428766615, 669.0798334106811, 665.131605957076, 673.3048295020617, 670.6864697876781, 339.40836655340223, 632.1423374851028, 636.7217220991748, 632.877844024455, 635.1271762532605, 629.9884883368966, 634.6789350099564, 638.111365130114, 590.287759750943, 634.2348639354922, 643.3447032899331, 639.5044084145364, 642.3583417808934, 637.4628381302026, 694.7855847353231, 685.6080452353506, 696.7918216391196, 694.8605371336108, 685.0750526619319, 699.5871800446789, 698.7020986341262, 558.9085343258434, 673.6116687315568, 702.4007796081116, 674.5065029967795, 708.6859243231763, 709.9139433292727, 707.3996105350495, 704.1517652340685, 710.3921408363419, 705.5569770584979, 705.8444143681463, 709.2196583171017, 699.9695155047, 708.8897132098396, 709.2311855042279, 703.7671909481427, 710.3445187432475, 712.6331371778058, 709.6334015364747, 711.1565937866235, 709.9863345118027, 705.7639823467283, 698.202627660351, 710.6072755453946, 701.5032806706088, 712.363952347969, 710.1509030592871, 710.6700749191264, 685.0193730293508]
Elapsed: 0.06935217167483643~0.012898951864462178
Time per graph: 0.0014202754334012508~0.00026333505311782775
Speed: 714.7585136378195~60.81284667079345
Total Time: 0.0708
best val loss: 0.6314107775688171 test_score: 0.6042

Testing...
Test loss: 0.7058 score: 0.5833 time: 0.07s
test Score 0.5833
Epoch Time List: [0.34173178183846176, 0.3283941481495276, 0.3296439310070127, 0.33199621888343245, 0.328264641109854, 0.32853396818973124, 0.3268393420148641, 0.3262431720504537, 0.3267954319017008, 0.32771452597808093, 0.33072660001926124, 0.3300640902016312, 0.32886464009061456, 0.32925805693957955, 0.3326858729124069, 0.33657654316630214, 0.3296876910608262, 0.32936913101002574, 0.33049840782769024, 0.3280723198549822, 0.32896482187788934, 0.33021015592385083, 0.32883515302091837, 0.32709692197386175, 0.3297765851020813, 0.32832052803132683, 0.32941042305901647, 0.3292831960134208, 0.329916590009816, 0.3355257530929521, 0.33919497614260763, 0.3307043630629778, 0.32910826499573886, 0.32943040900863707, 0.32845854898914695, 0.3268160540610552, 0.32727101806085557, 0.3287673101294786, 0.3292882919777185, 0.32930946606211364, 0.3291163770481944, 0.32846418709959835, 0.32749698299448937, 0.3282003180356696, 0.3334445410873741, 0.33017892809584737, 0.3275304251583293, 0.3276485699461773, 0.3267573819030076, 0.3267537389183417, 0.32740929012652487, 0.33078396797645837, 0.3303196020424366, 0.33050227409694344, 0.32845917192753404, 0.3288658680394292, 0.3301896839402616, 0.32653154409490526, 0.32650251500308514, 0.33031935908365995, 0.33709537400864065, 0.3301983700366691, 0.3291788628557697, 0.3298754539573565, 0.3340656330110505, 0.3424608149798587, 0.3320079939439893, 0.3289565750164911, 0.3288702921709046, 0.35227956995368004, 0.33254632086027414, 0.3298281991155818, 0.3302711199503392, 0.33518447005189955, 0.3349147760309279, 0.33183614211156964, 0.32952570903580636, 0.3296131599927321, 0.33079052006360143, 0.32889569620601833, 0.33053318003658205, 0.33077916700858623, 0.3328121380181983, 0.3301131749758497, 0.32977318798657507, 0.3310423350194469, 0.3291421299800277, 0.32929914514534175, 0.32989127398468554, 0.3330668529961258, 0.332504601101391, 0.32982042396906763, 0.3287309060106054, 0.32932235195767134, 0.32867449009791017, 0.3306796009419486, 0.3293301409576088, 0.32956018985714763, 0.33213675394654274, 0.3293540918966755, 0.33137634897138923, 0.33110448403749615, 0.3313313939142972, 0.33208656986244023, 0.33538681408390403, 0.3340346359182149, 0.3299824499990791, 0.3327450258657336, 0.3274789290735498, 0.32726919604465365, 0.32722526602447033, 0.3286510822363198, 0.32671639579348266, 0.32626078301109374, 0.3277059089159593, 0.32773389806970954, 0.32708470604848117, 0.32848030992317945, 0.3277462701080367, 0.33073206897825, 0.33397366502322257, 0.3275739620439708, 0.3273940399521962, 0.32766594109125435, 0.32926953805144876, 0.3273344661574811, 0.3270749399671331, 0.32882631209213287, 0.32836127607151866, 0.32700107991695404, 0.3295759118627757, 0.3278880879515782, 0.32770410703960806, 0.3279180289246142, 0.3305149609223008, 0.33133166190236807, 0.32687180303037167, 0.3265653708949685, 0.32608731102664024, 0.3269410200882703, 0.3271347559057176, 0.3286738229217008, 0.326653953990899, 0.32648552500177175, 0.3258517680224031, 0.32633360696490854, 0.3267153979977593, 0.3280975391389802, 0.3272959579480812, 0.3279249439947307, 0.3250607111258432, 0.3259236590238288, 0.32572758907917887, 0.329522502142936, 0.4245877790963277, 0.32599681499414146, 0.3276583990082145, 0.32817863894160837, 0.33022663509473205, 0.46028115414083004, 0.3713264740072191, 0.3269664440304041, 0.3244339730590582, 0.3269934229319915, 0.3954720329493284, 0.34410258498974144, 0.340199331054464, 0.3415358540369198, 0.34133830294013023, 0.3394980590092018, 0.34017626103013754, 0.339146891143173, 0.3459040980087593, 0.3443237339379266, 0.3372790828580037, 0.33689443103503436, 0.3375165490433574, 0.3373852219665423, 0.326130082947202, 0.3248403239995241, 0.3240373651497066, 0.32425716205034405, 0.32678704196587205, 0.32798190508037806, 0.32333140494301915, 0.3388162050396204, 0.3529071651864797, 0.3247597648296505, 0.32551564811728895, 0.3236036989837885, 0.3196611599996686, 0.31929305696394295, 0.3246669949730858, 0.32102388399653137, 0.3198814080096781, 0.3236561348894611, 0.32004986400716007, 0.32315204199403524, 0.3285961749497801, 0.3180471231462434, 0.3230291010113433, 0.3222017250955105, 0.32353313406929374, 0.32118398509919643, 0.32114493602421135, 0.32140808389522135, 0.32257377717178315, 0.3207885429728776, 0.32193420198746026, 0.321891934145242, 0.3224621880799532, 0.32024246093351394, 0.32143220014404505, 0.32621116400696337]
Total Epoch List: [150, 28, 36]
Total Time List: [0.06792019703425467, 0.07753906096331775, 0.07077228103298694]
T-times Epoch Time: 0.3373940198921399 ~ 0.007277485240746871
T-times Total Epoch: 65.1111111111111 ~ 9.757554844488954
T-times Total Time: 0.07319322798866779 ~ 0.0017656489260440807
T-times Inference Elapsed: 0.07352124087424433 ~ 0.0032330933850568217
T-times Time Per Graph: 0.001507957963419239 ~ 6.629444392266795e-05
T-times Speed: 673.7216382912596 ~ 30.97632828342285
T-times cross validation test micro f1 score:0.8368336586164476 ~ 0.06256825346448745
T-times cross validation test precision:0.7706055261610817 ~ 0.09199977240904188
T-times cross validation test recall:0.7731481481481483 ~ 0.09365940982096228
T-times cross validation test f1_score:0.8368336586164476 ~ 0.024382614462107804
