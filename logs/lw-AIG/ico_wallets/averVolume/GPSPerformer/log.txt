Namespace(seed=60, model='GPSPerformer', dataset='ico_wallets/averVolume', num_heads=1, num_layers=2, dim_hidden=64, dropout=0.1, epochs=1000, lr=0.0003, weight_decay=0.0001, batch_size=64, abs_pe=None, abs_pe_dim=20, num_class=2, outdir='./outdir/ico_wallets/averVolume/seed60/edge_attr/khopgnn_gat_1_0.1_0.0003_0.0001_2_1_64_BN', warmup=10, layer_norm=False, use_edge_attr=True, edge_dim=128, gnn_type='gat', k_hop=1, global_pool='mean', se='khopgnn', ie='mamba', aggr='add', not_extract_node_feature=False, k_ford=3, early_stop=1, early_stop_mindelta=-0.0, patience=20, training_times=3, Lambda=0.01, order='sd', use_cuda=True, batch_norm=True, save_logs=True)
Data(edge_index=[2, 342], edge_attr=[342, 2], x=[99, 14887], y=[1, 1], num_nodes=99)
Data(edge_index=[2, 342], edge_attr=[342, 2], x=[99, 14887], y=[1, 1], num_nodes=99)
Data(edge_index=[2, 298], edge_attr=[298, 2], x=[87, 14887], y=[1, 1], num_nodes=99)
========================training times:0========================
========================k_idx:0========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: GPSPerformer
GPS(
  (x_lin): Linear(in_features=14887, out_features=64, bias=True)
  (pe_lin): Linear(in_features=20, out_features=8, bias=True)
  (pe_norm): BatchNorm1d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (convs): ModuleList(
    (0-1): 2 x GPSConv(64, conv=GINEConv(nn=Sequential(
      (0): Linear(in_features=64, out_features=64, bias=True)
      (1): ReLU()
      (2): Linear(in_features=64, out_features=64, bias=True)
    )), heads=1, attn_type=performer)
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=32, bias=True)
    (1): ReLU()
    (2): Linear(in_features=32, out_features=16, bias=True)
    (3): ReLU()
    (4): Linear(in_features=16, out_features=2, bias=True)
  )
)
Total number of parameters: 1039522
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7c9d24ca7820>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.5524;  Loss pred: 0.5524; Loss self: 0.0000; time: 0.38s
Val loss: 0.8077 score: 0.5510 time: 0.08s
Test loss: 0.8901 score: 0.5306 time: 0.07s
Epoch 2/1000, LR 0.000000
Train loss: 0.5614;  Loss pred: 0.5614; Loss self: 0.0000; time: 0.21s
Val loss: 0.6973 score: 0.5714 time: 0.07s
Test loss: 0.7647 score: 0.5510 time: 0.06s
Epoch 3/1000, LR 0.000030
Train loss: 0.5873;  Loss pred: 0.5873; Loss self: 0.0000; time: 0.21s
Val loss: 0.6295 score: 0.5918 time: 0.07s
Test loss: 0.6883 score: 0.5714 time: 0.06s
Epoch 4/1000, LR 0.000060
Train loss: 0.5350;  Loss pred: 0.5350; Loss self: 0.0000; time: 0.20s
Val loss: 0.5848 score: 0.5918 time: 0.07s
Test loss: 0.6456 score: 0.5510 time: 0.06s
Epoch 5/1000, LR 0.000090
Train loss: 0.5895;  Loss pred: 0.5895; Loss self: 0.0000; time: 0.20s
Val loss: 0.5540 score: 0.6531 time: 0.07s
Test loss: 0.6211 score: 0.6122 time: 0.06s
Epoch 6/1000, LR 0.000120
Train loss: 0.5088;  Loss pred: 0.5088; Loss self: 0.0000; time: 0.20s
Val loss: 0.5310 score: 0.7143 time: 0.07s
Test loss: 0.6099 score: 0.6327 time: 0.06s
Epoch 7/1000, LR 0.000150
Train loss: 0.5226;  Loss pred: 0.5226; Loss self: 0.0000; time: 0.20s
Val loss: 0.5202 score: 0.7347 time: 0.07s
Test loss: 0.6097 score: 0.6327 time: 0.06s
Epoch 8/1000, LR 0.000180
Train loss: 0.5008;  Loss pred: 0.5008; Loss self: 0.0000; time: 0.20s
Val loss: 0.5216 score: 0.7143 time: 0.07s
Test loss: 0.6158 score: 0.6531 time: 0.06s
     INFO: Early stopping counter 1 of 20
Epoch 9/1000, LR 0.000210
Train loss: 0.4817;  Loss pred: 0.4817; Loss self: 0.0000; time: 0.20s
Val loss: 0.5265 score: 0.7551 time: 0.07s
Test loss: 0.6227 score: 0.6122 time: 0.06s
     INFO: Early stopping counter 2 of 20
Epoch 10/1000, LR 0.000240
Train loss: 0.4389;  Loss pred: 0.4389; Loss self: 0.0000; time: 0.19s
Val loss: 0.5341 score: 0.8163 time: 0.07s
Test loss: 0.6270 score: 0.6122 time: 0.06s
     INFO: Early stopping counter 3 of 20
Epoch 11/1000, LR 0.000270
Train loss: 0.3612;  Loss pred: 0.3612; Loss self: 0.0000; time: 0.20s
Val loss: 0.5450 score: 0.7755 time: 0.07s
Test loss: 0.6277 score: 0.6531 time: 0.06s
     INFO: Early stopping counter 4 of 20
Epoch 12/1000, LR 0.000270
Train loss: 0.3566;  Loss pred: 0.3566; Loss self: 0.0000; time: 0.20s
Val loss: 0.5576 score: 0.7347 time: 0.07s
Test loss: 0.6288 score: 0.6327 time: 0.06s
     INFO: Early stopping counter 5 of 20
Epoch 13/1000, LR 0.000270
Train loss: 0.3420;  Loss pred: 0.3420; Loss self: 0.0000; time: 0.20s
Val loss: 0.5685 score: 0.7551 time: 0.07s
Test loss: 0.6272 score: 0.6939 time: 0.06s
     INFO: Early stopping counter 6 of 20
Epoch 14/1000, LR 0.000270
Train loss: 0.3061;  Loss pred: 0.3061; Loss self: 0.0000; time: 0.19s
Val loss: 0.5764 score: 0.7551 time: 0.07s
Test loss: 0.6224 score: 0.6531 time: 0.06s
     INFO: Early stopping counter 7 of 20
Epoch 15/1000, LR 0.000270
Train loss: 0.2960;  Loss pred: 0.2960; Loss self: 0.0000; time: 0.19s
Val loss: 0.5823 score: 0.7347 time: 0.07s
Test loss: 0.6172 score: 0.6327 time: 0.06s
     INFO: Early stopping counter 8 of 20
Epoch 16/1000, LR 0.000270
Train loss: 0.2712;  Loss pred: 0.2712; Loss self: 0.0000; time: 0.19s
Val loss: 0.5843 score: 0.6327 time: 0.07s
Test loss: 0.6111 score: 0.6327 time: 0.06s
     INFO: Early stopping counter 9 of 20
Epoch 17/1000, LR 0.000270
Train loss: 0.2416;  Loss pred: 0.2416; Loss self: 0.0000; time: 0.20s
Val loss: 0.5844 score: 0.6735 time: 0.07s
Test loss: 0.6074 score: 0.5714 time: 0.06s
     INFO: Early stopping counter 10 of 20
Epoch 18/1000, LR 0.000270
Train loss: 0.2179;  Loss pred: 0.2179; Loss self: 0.0000; time: 0.20s
Val loss: 0.5840 score: 0.6327 time: 0.07s
Test loss: 0.6059 score: 0.5918 time: 0.06s
     INFO: Early stopping counter 11 of 20
Epoch 19/1000, LR 0.000270
Train loss: 0.1779;  Loss pred: 0.1779; Loss self: 0.0000; time: 0.22s
Val loss: 0.5814 score: 0.6327 time: 0.11s
Test loss: 0.6049 score: 0.6122 time: 0.08s
     INFO: Early stopping counter 12 of 20
Epoch 20/1000, LR 0.000270
Train loss: 0.1720;  Loss pred: 0.1720; Loss self: 0.0000; time: 0.20s
Val loss: 0.5769 score: 0.6327 time: 0.08s
Test loss: 0.6052 score: 0.6122 time: 0.08s
     INFO: Early stopping counter 13 of 20
Epoch 21/1000, LR 0.000270
Train loss: 0.1781;  Loss pred: 0.1781; Loss self: 0.0000; time: 0.21s
Val loss: 0.5712 score: 0.5918 time: 0.07s
Test loss: 0.6074 score: 0.5918 time: 0.07s
     INFO: Early stopping counter 14 of 20
Epoch 22/1000, LR 0.000270
Train loss: 0.1475;  Loss pred: 0.1475; Loss self: 0.0000; time: 0.21s
Val loss: 0.5667 score: 0.5714 time: 0.07s
Test loss: 0.6117 score: 0.5918 time: 0.07s
     INFO: Early stopping counter 15 of 20
Epoch 23/1000, LR 0.000270
Train loss: 0.1618;  Loss pred: 0.1618; Loss self: 0.0000; time: 0.20s
Val loss: 0.5672 score: 0.5714 time: 0.07s
Test loss: 0.6169 score: 0.5714 time: 0.06s
     INFO: Early stopping counter 16 of 20
Epoch 24/1000, LR 0.000270
Train loss: 0.1304;  Loss pred: 0.1304; Loss self: 0.0000; time: 0.21s
Val loss: 0.5697 score: 0.5714 time: 0.09s
Test loss: 0.6237 score: 0.5714 time: 0.06s
     INFO: Early stopping counter 17 of 20
Epoch 25/1000, LR 0.000270
Train loss: 0.1202;  Loss pred: 0.1202; Loss self: 0.0000; time: 0.19s
Val loss: 0.5740 score: 0.5510 time: 0.07s
Test loss: 0.6329 score: 0.5510 time: 0.06s
     INFO: Early stopping counter 18 of 20
Epoch 26/1000, LR 0.000270
Train loss: 0.1059;  Loss pred: 0.1059; Loss self: 0.0000; time: 0.19s
Val loss: 0.5811 score: 0.5102 time: 0.07s
Test loss: 0.6461 score: 0.5306 time: 0.06s
     INFO: Early stopping counter 19 of 20
Epoch 27/1000, LR 0.000270
Train loss: 0.1144;  Loss pred: 0.1144; Loss self: 0.0000; time: 0.20s
Val loss: 0.5908 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6629 score: 0.5102 time: 0.06s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 006,   Train_Loss: 0.5226,   Val_Loss: 0.5202,   Val_Precision: 0.6897,   Val_Recall: 0.8333,   Val_accuracy: 0.7547,   Val_Score: 0.7347,   Val_Loss: 0.5202,   Test_Precision: 0.6296,   Test_Recall: 0.6800,   Test_accuracy: 0.6538,   Test_Score: 0.6327,   Test_loss: 0.6097


[0.07357377500738949, 0.06916751794051379, 0.0693746639881283, 0.06666283600497991, 0.06562047998886555, 0.0653390750521794, 0.0654128990136087, 0.06629651796538383, 0.06573937309440225, 0.0653786800103262, 0.06556875200476497, 0.0654416719917208, 0.06496517092455178, 0.06530306197237223, 0.06536091805901378, 0.06565504393074661, 0.06594581296667457, 0.06659470498561859, 0.08836662000976503, 0.08160100900568068, 0.07080188591498882, 0.07003569602966309, 0.06555779196787626, 0.06617866596207023, 0.06514077901374549, 0.0656013049883768, 0.06661587092094123]
[0.0015015056123957038, 0.0014115819987859956, 0.0014158094691454756, 0.0013604660409179572, 0.0013391934691605214, 0.0013334505112689672, 0.001334957122726708, 0.0013529901625588536, 0.0013416198590694337, 0.001334258775720943, 0.0013381377960156118, 0.0013355443263616488, 0.001325819814786771, 0.001332715550456576, 0.0013338962869186486, 0.0013398988557295228, 0.001345832917687236, 0.0013590756119513999, 0.0018034004083625516, 0.0016653267144016465, 0.0014449364472446696, 0.001429299918972716, 0.001337914121793393, 0.0013505850196340863, 0.0013294036533417447, 0.0013388021426199346, 0.0013595075698151272]
[665.9981765932034, 708.4250159466691, 706.3097272569867, 735.0422354719436, 746.718097891303, 749.9340932033227, 749.0877294676375, 739.1036739755313, 745.3676190315296, 749.4797997185125, 747.3071928597802, 748.7583753391741, 754.2503052428946, 750.347663953804, 749.6834722510835, 746.3249899228691, 743.0342852056724, 735.79423485068, 554.5080257068247, 600.4827709494235, 692.0719606089851, 699.6432216400965, 747.4321286478092, 740.4198813569914, 752.2169790088082, 746.9363606209022, 735.5604501238527]
Elapsed: 0.0680481695820129~0.005287546002271691
Time per graph: 0.001388738154734957~0.00010790910208717736
Speed: 723.7125358091217~46.938753150163606
Total Time: 0.0670
best val loss: 0.520228922367096 test_score: 0.6327

Testing...
Test loss: 0.6270 score: 0.6122 time: 0.06s
test Score 0.6122
Epoch Time List: [0.5350541829830036, 0.3449331499869004, 0.3471056040143594, 0.33054709900170565, 0.33675724803470075, 0.3258219719864428, 0.32432751392479986, 0.32464444311335683, 0.329966633929871, 0.3223410228965804, 0.326826686039567, 0.3236594299087301, 0.324169036000967, 0.3215623099822551, 0.3216194718843326, 0.32134407095145434, 0.32738736900500953, 0.3254573850426823, 0.41249169094953686, 0.3492376741487533, 0.3478119099745527, 0.3479646068299189, 0.33112306287512183, 0.362885823007673, 0.3223589969566092, 0.3225015519419685, 0.3241286900592968]
Total Epoch List: [27]
Total Time List: [0.06697643792722374]
========================k_idx:1========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: GPSPerformer
GPS(
  (x_lin): Linear(in_features=14887, out_features=64, bias=True)
  (pe_lin): Linear(in_features=20, out_features=8, bias=True)
  (pe_norm): BatchNorm1d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (convs): ModuleList(
    (0-1): 2 x GPSConv(64, conv=GINEConv(nn=Sequential(
      (0): Linear(in_features=64, out_features=64, bias=True)
      (1): ReLU()
      (2): Linear(in_features=64, out_features=64, bias=True)
    )), heads=1, attn_type=performer)
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=32, bias=True)
    (1): ReLU()
    (2): Linear(in_features=32, out_features=16, bias=True)
    (3): ReLU()
    (4): Linear(in_features=16, out_features=2, bias=True)
  )
)
Total number of parameters: 1039522
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7c9d24ca74f0>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 1.2241;  Loss pred: 1.2241; Loss self: 0.0000; time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.9448 score: 0.4898 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 1.0432 score: 0.5102 time: 0.11s
Epoch 2/1000, LR 0.000000
Train loss: 1.3292;  Loss pred: 1.3292; Loss self: 0.0000; time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.9024 score: 0.4898 time: 0.06s
Test loss: 0.9765 score: 0.5306 time: 0.07s
Epoch 3/1000, LR 0.000030
Train loss: 1.2065;  Loss pred: 1.2065; Loss self: 0.0000; time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.8674 score: 0.4898 time: 0.06s
Test loss: 0.9185 score: 0.5306 time: 0.07s
Epoch 4/1000, LR 0.000060
Train loss: 1.0331;  Loss pred: 1.0331; Loss self: 0.0000; time: 0.20s
Val loss: 0.8316 score: 0.5306 time: 0.07s
Test loss: 0.8541 score: 0.5306 time: 0.07s
Epoch 5/1000, LR 0.000090
Train loss: 1.0735;  Loss pred: 1.0735; Loss self: 0.0000; time: 0.20s
Val loss: 0.7967 score: 0.5306 time: 0.06s
Test loss: 0.7948 score: 0.5510 time: 0.07s
Epoch 6/1000, LR 0.000120
Train loss: 1.0043;  Loss pred: 1.0043; Loss self: 0.0000; time: 0.20s
Val loss: 0.7727 score: 0.5306 time: 0.06s
Test loss: 0.7516 score: 0.5918 time: 0.07s
Epoch 7/1000, LR 0.000150
Train loss: 0.9204;  Loss pred: 0.9204; Loss self: 0.0000; time: 0.20s
Val loss: 0.7511 score: 0.5714 time: 0.07s
Test loss: 0.7203 score: 0.6327 time: 0.07s
Epoch 8/1000, LR 0.000180
Train loss: 0.8994;  Loss pred: 0.8994; Loss self: 0.0000; time: 0.21s
Val loss: 0.7420 score: 0.5306 time: 0.07s
Test loss: 0.7044 score: 0.6327 time: 0.08s
Epoch 9/1000, LR 0.000210
Train loss: 0.6990;  Loss pred: 0.6990; Loss self: 0.0000; time: 0.20s
Val loss: 0.7348 score: 0.5510 time: 0.06s
Test loss: 0.7004 score: 0.6327 time: 0.07s
Epoch 10/1000, LR 0.000240
Train loss: 0.7242;  Loss pred: 0.7242; Loss self: 0.0000; time: 0.19s
Val loss: 0.7342 score: 0.5510 time: 0.06s
Test loss: 0.6945 score: 0.6327 time: 0.07s
Epoch 11/1000, LR 0.000270
Train loss: 0.7610;  Loss pred: 0.7610; Loss self: 0.0000; time: 0.19s
Val loss: 0.7287 score: 0.5714 time: 0.06s
Test loss: 0.6715 score: 0.6531 time: 0.07s
Epoch 12/1000, LR 0.000270
Train loss: 0.6478;  Loss pred: 0.6478; Loss self: 0.0000; time: 0.19s
Val loss: 0.7309 score: 0.5510 time: 0.06s
Test loss: 0.6687 score: 0.6939 time: 0.07s
     INFO: Early stopping counter 1 of 20
Epoch 13/1000, LR 0.000270
Train loss: 0.6183;  Loss pred: 0.6183; Loss self: 0.0000; time: 0.19s
Val loss: 0.7327 score: 0.5510 time: 0.06s
Test loss: 0.6700 score: 0.6939 time: 0.08s
     INFO: Early stopping counter 2 of 20
Epoch 14/1000, LR 0.000270
Train loss: 0.5826;  Loss pred: 0.5826; Loss self: 0.0000; time: 0.19s
Val loss: 0.7346 score: 0.5510 time: 0.06s
Test loss: 0.6701 score: 0.6939 time: 0.07s
     INFO: Early stopping counter 3 of 20
Epoch 15/1000, LR 0.000270
Train loss: 0.5603;  Loss pred: 0.5603; Loss self: 0.0000; time: 0.19s
Val loss: 0.7312 score: 0.5510 time: 0.06s
Test loss: 0.6662 score: 0.6939 time: 0.07s
     INFO: Early stopping counter 4 of 20
Epoch 16/1000, LR 0.000270
Train loss: 0.5454;  Loss pred: 0.5454; Loss self: 0.0000; time: 0.20s
Val loss: 0.7270 score: 0.5510 time: 0.06s
Test loss: 0.6626 score: 0.7143 time: 0.07s
Epoch 17/1000, LR 0.000270
Train loss: 0.4595;  Loss pred: 0.4595; Loss self: 0.0000; time: 0.21s
Val loss: 0.7187 score: 0.5510 time: 0.08s
Test loss: 0.6545 score: 0.7143 time: 0.08s
Epoch 18/1000, LR 0.000270
Train loss: 0.4338;  Loss pred: 0.4338; Loss self: 0.0000; time: 0.20s
Val loss: 0.7133 score: 0.5510 time: 0.06s
Test loss: 0.6548 score: 0.7143 time: 0.07s
Epoch 19/1000, LR 0.000270
Train loss: 0.4266;  Loss pred: 0.4266; Loss self: 0.0000; time: 0.19s
Val loss: 0.7100 score: 0.5714 time: 0.06s
Test loss: 0.6548 score: 0.7143 time: 0.07s
Epoch 20/1000, LR 0.000270
Train loss: 0.4034;  Loss pred: 0.4034; Loss self: 0.0000; time: 0.19s
Val loss: 0.7009 score: 0.5714 time: 0.06s
Test loss: 0.6441 score: 0.7143 time: 0.07s
Epoch 21/1000, LR 0.000270
Train loss: 0.4584;  Loss pred: 0.4584; Loss self: 0.0000; time: 0.19s
Val loss: 0.6861 score: 0.5306 time: 0.06s
Test loss: 0.6251 score: 0.7143 time: 0.07s
Epoch 22/1000, LR 0.000270
Train loss: 0.3918;  Loss pred: 0.3918; Loss self: 0.0000; time: 0.19s
Val loss: 0.6711 score: 0.5306 time: 0.06s
Test loss: 0.6038 score: 0.7143 time: 0.07s
Epoch 23/1000, LR 0.000270
Train loss: 0.3645;  Loss pred: 0.3645; Loss self: 0.0000; time: 0.19s
Val loss: 0.6620 score: 0.5306 time: 0.06s
Test loss: 0.5870 score: 0.7143 time: 0.07s
Epoch 24/1000, LR 0.000270
Train loss: 0.3319;  Loss pred: 0.3319; Loss self: 0.0000; time: 0.19s
Val loss: 0.6567 score: 0.5510 time: 0.07s
Test loss: 0.5761 score: 0.7143 time: 0.07s
Epoch 25/1000, LR 0.000270
Train loss: 0.3105;  Loss pred: 0.3105; Loss self: 0.0000; time: 0.19s
Val loss: 0.6522 score: 0.5510 time: 0.07s
Test loss: 0.5674 score: 0.7347 time: 0.08s
Epoch 26/1000, LR 0.000270
Train loss: 0.3002;  Loss pred: 0.3002; Loss self: 0.0000; time: 0.20s
Val loss: 0.6469 score: 0.5306 time: 0.06s
Test loss: 0.5590 score: 0.7551 time: 0.07s
Epoch 27/1000, LR 0.000270
Train loss: 0.2815;  Loss pred: 0.2815; Loss self: 0.0000; time: 0.19s
Val loss: 0.6455 score: 0.5510 time: 0.06s
Test loss: 0.5550 score: 0.7551 time: 0.07s
Epoch 28/1000, LR 0.000270
Train loss: 0.2661;  Loss pred: 0.2661; Loss self: 0.0000; time: 0.20s
Val loss: 0.6452 score: 0.5510 time: 0.06s
Test loss: 0.5486 score: 0.7755 time: 0.07s
Epoch 29/1000, LR 0.000270
Train loss: 0.2567;  Loss pred: 0.2567; Loss self: 0.0000; time: 0.19s
Val loss: 0.6418 score: 0.5714 time: 0.06s
Test loss: 0.5401 score: 0.7755 time: 0.07s
Epoch 30/1000, LR 0.000270
Train loss: 0.2211;  Loss pred: 0.2211; Loss self: 0.0000; time: 0.19s
Val loss: 0.6359 score: 0.5714 time: 0.06s
Test loss: 0.5304 score: 0.7755 time: 0.07s
Epoch 31/1000, LR 0.000270
Train loss: 0.2228;  Loss pred: 0.2228; Loss self: 0.0000; time: 0.19s
Val loss: 0.6307 score: 0.5714 time: 0.06s
Test loss: 0.5221 score: 0.7347 time: 0.07s
Epoch 32/1000, LR 0.000270
Train loss: 0.1843;  Loss pred: 0.1843; Loss self: 0.0000; time: 0.20s
Val loss: 0.6278 score: 0.5918 time: 0.06s
Test loss: 0.5176 score: 0.7347 time: 0.07s
Epoch 33/1000, LR 0.000270
Train loss: 0.1786;  Loss pred: 0.1786; Loss self: 0.0000; time: 0.19s
Val loss: 0.6274 score: 0.5918 time: 0.06s
Test loss: 0.5169 score: 0.7347 time: 0.07s
Epoch 34/1000, LR 0.000270
Train loss: 0.1738;  Loss pred: 0.1738; Loss self: 0.0000; time: 0.19s
Val loss: 0.6281 score: 0.5714 time: 0.06s
Test loss: 0.5176 score: 0.7551 time: 0.07s
     INFO: Early stopping counter 1 of 20
Epoch 35/1000, LR 0.000270
Train loss: 0.1555;  Loss pred: 0.1555; Loss self: 0.0000; time: 0.19s
Val loss: 0.6286 score: 0.6122 time: 0.06s
Test loss: 0.5213 score: 0.7755 time: 0.07s
     INFO: Early stopping counter 2 of 20
Epoch 36/1000, LR 0.000270
Train loss: 0.1432;  Loss pred: 0.1432; Loss self: 0.0000; time: 0.19s
Val loss: 0.6299 score: 0.5918 time: 0.06s
Test loss: 0.5252 score: 0.7755 time: 0.07s
     INFO: Early stopping counter 3 of 20
Epoch 37/1000, LR 0.000270
Train loss: 0.1289;  Loss pred: 0.1289; Loss self: 0.0000; time: 0.19s
Val loss: 0.6278 score: 0.6122 time: 0.06s
Test loss: 0.5247 score: 0.8163 time: 0.07s
     INFO: Early stopping counter 4 of 20
Epoch 38/1000, LR 0.000270
Train loss: 0.1361;  Loss pred: 0.1361; Loss self: 0.0000; time: 0.19s
Val loss: 0.6226 score: 0.6327 time: 0.06s
Test loss: 0.5188 score: 0.8163 time: 0.07s
Epoch 39/1000, LR 0.000269
Train loss: 0.1225;  Loss pred: 0.1225; Loss self: 0.0000; time: 0.19s
Val loss: 0.6160 score: 0.6531 time: 0.06s
Test loss: 0.5120 score: 0.8367 time: 0.07s
Epoch 40/1000, LR 0.000269
Train loss: 0.1045;  Loss pred: 0.1045; Loss self: 0.0000; time: 0.19s
Val loss: 0.6093 score: 0.6939 time: 0.06s
Test loss: 0.5077 score: 0.8367 time: 0.07s
Epoch 41/1000, LR 0.000269
Train loss: 0.1053;  Loss pred: 0.1053; Loss self: 0.0000; time: 0.19s
Val loss: 0.6050 score: 0.6939 time: 0.06s
Test loss: 0.5034 score: 0.8367 time: 0.07s
Epoch 42/1000, LR 0.000269
Train loss: 0.0994;  Loss pred: 0.0994; Loss self: 0.0000; time: 0.19s
Val loss: 0.6009 score: 0.6939 time: 0.06s
Test loss: 0.4987 score: 0.8367 time: 0.07s
Epoch 43/1000, LR 0.000269
Train loss: 0.0834;  Loss pred: 0.0834; Loss self: 0.0000; time: 0.19s
Val loss: 0.5928 score: 0.6939 time: 0.06s
Test loss: 0.4912 score: 0.8367 time: 0.07s
Epoch 44/1000, LR 0.000269
Train loss: 0.0913;  Loss pred: 0.0913; Loss self: 0.0000; time: 0.19s
Val loss: 0.5836 score: 0.7347 time: 0.07s
Test loss: 0.4826 score: 0.8367 time: 0.07s
Epoch 45/1000, LR 0.000269
Train loss: 0.0811;  Loss pred: 0.0811; Loss self: 0.0000; time: 0.19s
Val loss: 0.5725 score: 0.7551 time: 0.06s
Test loss: 0.4734 score: 0.8367 time: 0.07s
Epoch 46/1000, LR 0.000269
Train loss: 0.0689;  Loss pred: 0.0689; Loss self: 0.0000; time: 0.19s
Val loss: 0.5596 score: 0.7551 time: 0.06s
Test loss: 0.4638 score: 0.8367 time: 0.07s
Epoch 47/1000, LR 0.000269
Train loss: 0.0615;  Loss pred: 0.0615; Loss self: 0.0000; time: 0.20s
Val loss: 0.5453 score: 0.7755 time: 0.06s
Test loss: 0.4528 score: 0.8571 time: 0.07s
Epoch 48/1000, LR 0.000269
Train loss: 0.0617;  Loss pred: 0.0617; Loss self: 0.0000; time: 0.19s
Val loss: 0.5328 score: 0.7755 time: 0.06s
Test loss: 0.4427 score: 0.8980 time: 0.07s
Epoch 49/1000, LR 0.000269
Train loss: 0.0573;  Loss pred: 0.0573; Loss self: 0.0000; time: 0.19s
Val loss: 0.5225 score: 0.8163 time: 0.06s
Test loss: 0.4294 score: 0.8980 time: 0.07s
Epoch 50/1000, LR 0.000269
Train loss: 0.0453;  Loss pred: 0.0453; Loss self: 0.0000; time: 0.19s
Val loss: 0.5125 score: 0.7959 time: 0.06s
Test loss: 0.4168 score: 0.8980 time: 0.07s
Epoch 51/1000, LR 0.000269
Train loss: 0.0429;  Loss pred: 0.0429; Loss self: 0.0000; time: 0.19s
Val loss: 0.5062 score: 0.7959 time: 0.06s
Test loss: 0.4066 score: 0.8776 time: 0.07s
Epoch 52/1000, LR 0.000269
Train loss: 0.0434;  Loss pred: 0.0434; Loss self: 0.0000; time: 0.19s
Val loss: 0.5037 score: 0.7959 time: 0.06s
Test loss: 0.3982 score: 0.8776 time: 0.07s
Epoch 53/1000, LR 0.000269
Train loss: 0.0332;  Loss pred: 0.0332; Loss self: 0.0000; time: 0.19s
Val loss: 0.5019 score: 0.7755 time: 0.06s
Test loss: 0.3912 score: 0.8776 time: 0.07s
Epoch 54/1000, LR 0.000269
Train loss: 0.0363;  Loss pred: 0.0363; Loss self: 0.0000; time: 0.20s
Val loss: 0.5004 score: 0.7551 time: 0.06s
Test loss: 0.3847 score: 0.8776 time: 0.07s
Epoch 55/1000, LR 0.000269
Train loss: 0.0365;  Loss pred: 0.0365; Loss self: 0.0000; time: 0.20s
Val loss: 0.5021 score: 0.7551 time: 0.06s
Test loss: 0.3798 score: 0.8776 time: 0.07s
     INFO: Early stopping counter 1 of 20
Epoch 56/1000, LR 0.000269
Train loss: 0.0352;  Loss pred: 0.0352; Loss self: 0.0000; time: 0.20s
Val loss: 0.5052 score: 0.7551 time: 0.06s
Test loss: 0.3760 score: 0.8776 time: 0.07s
     INFO: Early stopping counter 2 of 20
Epoch 57/1000, LR 0.000269
Train loss: 0.0356;  Loss pred: 0.0356; Loss self: 0.0000; time: 0.20s
Val loss: 0.5028 score: 0.7551 time: 0.06s
Test loss: 0.3678 score: 0.8776 time: 0.07s
     INFO: Early stopping counter 3 of 20
Epoch 58/1000, LR 0.000269
Train loss: 0.0363;  Loss pred: 0.0363; Loss self: 0.0000; time: 0.20s
Val loss: 0.5004 score: 0.7143 time: 0.06s
Test loss: 0.3585 score: 0.8776 time: 0.07s
     INFO: Early stopping counter 4 of 20
Epoch 59/1000, LR 0.000268
Train loss: 0.0289;  Loss pred: 0.0289; Loss self: 0.0000; time: 0.20s
Val loss: 0.4977 score: 0.6939 time: 0.06s
Test loss: 0.3481 score: 0.8571 time: 0.07s
Epoch 60/1000, LR 0.000268
Train loss: 0.0241;  Loss pred: 0.0241; Loss self: 0.0000; time: 0.20s
Val loss: 0.4938 score: 0.6939 time: 0.06s
Test loss: 0.3357 score: 0.8776 time: 0.07s
Epoch 61/1000, LR 0.000268
Train loss: 0.0257;  Loss pred: 0.0257; Loss self: 0.0000; time: 0.20s
Val loss: 0.4895 score: 0.6939 time: 0.06s
Test loss: 0.3231 score: 0.8571 time: 0.07s
Epoch 62/1000, LR 0.000268
Train loss: 0.0264;  Loss pred: 0.0264; Loss self: 0.0000; time: 0.19s
Val loss: 0.4875 score: 0.6939 time: 0.07s
Test loss: 0.3119 score: 0.8571 time: 0.07s
Epoch 63/1000, LR 0.000268
Train loss: 0.0254;  Loss pred: 0.0254; Loss self: 0.0000; time: 0.19s
Val loss: 0.4787 score: 0.7143 time: 0.06s
Test loss: 0.2986 score: 0.8571 time: 0.07s
Epoch 64/1000, LR 0.000268
Train loss: 0.0232;  Loss pred: 0.0232; Loss self: 0.0000; time: 0.19s
Val loss: 0.4712 score: 0.7143 time: 0.06s
Test loss: 0.2865 score: 0.8571 time: 0.07s
Epoch 65/1000, LR 0.000268
Train loss: 0.0200;  Loss pred: 0.0200; Loss self: 0.0000; time: 0.19s
Val loss: 0.4563 score: 0.7143 time: 0.06s
Test loss: 0.2715 score: 0.8571 time: 0.07s
Epoch 66/1000, LR 0.000268
Train loss: 0.0194;  Loss pred: 0.0194; Loss self: 0.0000; time: 0.19s
Val loss: 0.4390 score: 0.7551 time: 0.06s
Test loss: 0.2565 score: 0.8980 time: 0.07s
Epoch 67/1000, LR 0.000268
Train loss: 0.0206;  Loss pred: 0.0206; Loss self: 0.0000; time: 0.20s
Val loss: 0.4219 score: 0.7755 time: 0.06s
Test loss: 0.2438 score: 0.9184 time: 0.07s
Epoch 68/1000, LR 0.000268
Train loss: 0.0175;  Loss pred: 0.0175; Loss self: 0.0000; time: 0.19s
Val loss: 0.4065 score: 0.7959 time: 0.06s
Test loss: 0.2337 score: 0.9184 time: 0.07s
Epoch 69/1000, LR 0.000268
Train loss: 0.0179;  Loss pred: 0.0179; Loss self: 0.0000; time: 0.19s
Val loss: 0.3948 score: 0.8163 time: 0.06s
Test loss: 0.2273 score: 0.9592 time: 0.07s
Epoch 70/1000, LR 0.000268
Train loss: 0.0171;  Loss pred: 0.0171; Loss self: 0.0000; time: 0.19s
Val loss: 0.3919 score: 0.8163 time: 0.06s
Test loss: 0.2208 score: 0.9592 time: 0.07s
Epoch 71/1000, LR 0.000268
Train loss: 0.0176;  Loss pred: 0.0176; Loss self: 0.0000; time: 0.19s
Val loss: 0.3956 score: 0.8367 time: 0.06s
Test loss: 0.2136 score: 0.9592 time: 0.07s
     INFO: Early stopping counter 1 of 20
Epoch 72/1000, LR 0.000267
Train loss: 0.0152;  Loss pred: 0.0152; Loss self: 0.0000; time: 0.19s
Val loss: 0.4026 score: 0.8163 time: 0.06s
Test loss: 0.2072 score: 0.9388 time: 0.07s
     INFO: Early stopping counter 2 of 20
Epoch 73/1000, LR 0.000267
Train loss: 0.0167;  Loss pred: 0.0167; Loss self: 0.0000; time: 0.19s
Val loss: 0.4147 score: 0.7755 time: 0.06s
Test loss: 0.2034 score: 0.9388 time: 0.07s
     INFO: Early stopping counter 3 of 20
Epoch 74/1000, LR 0.000267
Train loss: 0.0157;  Loss pred: 0.0157; Loss self: 0.0000; time: 0.19s
Val loss: 0.4354 score: 0.7347 time: 0.06s
Test loss: 0.2047 score: 0.9184 time: 0.07s
     INFO: Early stopping counter 4 of 20
Epoch 75/1000, LR 0.000267
Train loss: 0.0148;  Loss pred: 0.0148; Loss self: 0.0000; time: 0.19s
Val loss: 0.4535 score: 0.7347 time: 0.06s
Test loss: 0.2062 score: 0.9184 time: 0.07s
     INFO: Early stopping counter 5 of 20
Epoch 76/1000, LR 0.000267
Train loss: 0.0157;  Loss pred: 0.0157; Loss self: 0.0000; time: 0.19s
Val loss: 0.4610 score: 0.7143 time: 0.06s
Test loss: 0.2039 score: 0.8980 time: 0.07s
     INFO: Early stopping counter 6 of 20
Epoch 77/1000, LR 0.000267
Train loss: 0.0144;  Loss pred: 0.0144; Loss self: 0.0000; time: 0.19s
Val loss: 0.4641 score: 0.7143 time: 0.06s
Test loss: 0.2006 score: 0.9184 time: 0.08s
     INFO: Early stopping counter 7 of 20
Epoch 78/1000, LR 0.000267
Train loss: 0.0122;  Loss pred: 0.0122; Loss self: 0.0000; time: 0.20s
Val loss: 0.4674 score: 0.7143 time: 0.06s
Test loss: 0.1976 score: 0.8980 time: 0.07s
     INFO: Early stopping counter 8 of 20
Epoch 79/1000, LR 0.000267
Train loss: 0.0109;  Loss pred: 0.0109; Loss self: 0.0000; time: 0.19s
Val loss: 0.4515 score: 0.7755 time: 0.06s
Test loss: 0.1851 score: 0.9184 time: 0.07s
     INFO: Early stopping counter 9 of 20
Epoch 80/1000, LR 0.000267
Train loss: 0.0107;  Loss pred: 0.0107; Loss self: 0.0000; time: 0.19s
Val loss: 0.4387 score: 0.7755 time: 0.06s
Test loss: 0.1725 score: 0.9184 time: 0.07s
     INFO: Early stopping counter 10 of 20
Epoch 81/1000, LR 0.000267
Train loss: 0.0108;  Loss pred: 0.0108; Loss self: 0.0000; time: 0.19s
Val loss: 0.4371 score: 0.7959 time: 0.06s
Test loss: 0.1665 score: 0.9184 time: 0.07s
     INFO: Early stopping counter 11 of 20
Epoch 82/1000, LR 0.000267
Train loss: 0.0094;  Loss pred: 0.0094; Loss self: 0.0000; time: 0.20s
Val loss: 0.4313 score: 0.7959 time: 0.06s
Test loss: 0.1587 score: 0.9388 time: 0.07s
     INFO: Early stopping counter 12 of 20
Epoch 83/1000, LR 0.000266
Train loss: 0.0092;  Loss pred: 0.0092; Loss self: 0.0000; time: 0.20s
Val loss: 0.4261 score: 0.7959 time: 0.06s
Test loss: 0.1518 score: 0.9388 time: 0.07s
     INFO: Early stopping counter 13 of 20
Epoch 84/1000, LR 0.000266
Train loss: 0.0094;  Loss pred: 0.0094; Loss self: 0.0000; time: 0.20s
Val loss: 0.4252 score: 0.8163 time: 0.06s
Test loss: 0.1483 score: 0.9388 time: 0.07s
     INFO: Early stopping counter 14 of 20
Epoch 85/1000, LR 0.000266
Train loss: 0.0090;  Loss pred: 0.0090; Loss self: 0.0000; time: 0.20s
Val loss: 0.4317 score: 0.8163 time: 0.06s
Test loss: 0.1476 score: 0.9184 time: 0.07s
     INFO: Early stopping counter 15 of 20
Epoch 86/1000, LR 0.000266
Train loss: 0.0092;  Loss pred: 0.0092; Loss self: 0.0000; time: 0.20s
Val loss: 0.4448 score: 0.7959 time: 0.06s
Test loss: 0.1489 score: 0.9184 time: 0.07s
     INFO: Early stopping counter 16 of 20
Epoch 87/1000, LR 0.000266
Train loss: 0.0092;  Loss pred: 0.0092; Loss self: 0.0000; time: 0.20s
Val loss: 0.4582 score: 0.7755 time: 0.06s
Test loss: 0.1507 score: 0.9184 time: 0.07s
     INFO: Early stopping counter 17 of 20
Epoch 88/1000, LR 0.000266
Train loss: 0.0088;  Loss pred: 0.0088; Loss self: 0.0000; time: 0.20s
Val loss: 0.4752 score: 0.7551 time: 0.06s
Test loss: 0.1540 score: 0.9184 time: 0.07s
     INFO: Early stopping counter 18 of 20
Epoch 89/1000, LR 0.000266
Train loss: 0.0080;  Loss pred: 0.0080; Loss self: 0.0000; time: 0.20s
Val loss: 0.4938 score: 0.7551 time: 0.06s
Test loss: 0.1596 score: 0.8980 time: 0.07s
     INFO: Early stopping counter 19 of 20
Epoch 90/1000, LR 0.000266
Train loss: 0.0073;  Loss pred: 0.0073; Loss self: 0.0000; time: 0.20s
Val loss: 0.5109 score: 0.7551 time: 0.06s
Test loss: 0.1634 score: 0.8980 time: 0.07s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 069,   Train_Loss: 0.0171,   Val_Loss: 0.3919,   Val_Precision: 1.0000,   Val_Recall: 0.6400,   Val_accuracy: 0.7805,   Val_Score: 0.8163,   Val_Loss: 0.3919,   Test_Precision: 1.0000,   Test_Recall: 0.9167,   Test_accuracy: 0.9565,   Test_Score: 0.9592,   Test_loss: 0.2208


[0.07357377500738949, 0.06916751794051379, 0.0693746639881283, 0.06666283600497991, 0.06562047998886555, 0.0653390750521794, 0.0654128990136087, 0.06629651796538383, 0.06573937309440225, 0.0653786800103262, 0.06556875200476497, 0.0654416719917208, 0.06496517092455178, 0.06530306197237223, 0.06536091805901378, 0.06565504393074661, 0.06594581296667457, 0.06659470498561859, 0.08836662000976503, 0.08160100900568068, 0.07080188591498882, 0.07003569602966309, 0.06555779196787626, 0.06617866596207023, 0.06514077901374549, 0.0656013049883768, 0.06661587092094123, 0.11635970196221024, 0.07901623891666532, 0.0743140410631895, 0.07818414003122598, 0.07832567603327334, 0.07384344295132905, 0.07381266995798796, 0.08079240703955293, 0.07796510402113199, 0.0734138279221952, 0.07705338497180492, 0.07732861000113189, 0.080471056047827, 0.07704853010363877, 0.07712277595419437, 0.07901579700410366, 0.08822646201588213, 0.07683931803330779, 0.0720868669450283, 0.07746960606891662, 0.07728940097149462, 0.07224983803462237, 0.07291183096822351, 0.07719401805661619, 0.08617742999922484, 0.07675874105188996, 0.0729828579351306, 0.07661131001077592, 0.07671273092273623, 0.07256224390584975, 0.07678078697063029, 0.07692550390493125, 0.07274998410139233, 0.07697516994085163, 0.07211634097620845, 0.07681199198123068, 0.07701342704240233, 0.07225965894758701, 0.07745361991692334, 0.07665395503863692, 0.07603495195508003, 0.0770227120956406, 0.07389223598875105, 0.07212469098158181, 0.07674506097100675, 0.07765200803987682, 0.07599195907823741, 0.07229709310922772, 0.07272716797888279, 0.07284077594522387, 0.07239058299455792, 0.07664669607765973, 0.07720375200733542, 0.07655105995945632, 0.0766135361045599, 0.07229266897775233, 0.07635718397796154, 0.07225054199807346, 0.07695829996373504, 0.07716981007251889, 0.07366591203026474, 0.07739822007715702, 0.07383964397013187, 0.07825817202683538, 0.07341461698524654, 0.07890102500095963, 0.07750446791760623, 0.07320921099744737, 0.07158203097060323, 0.0721808149246499, 0.07678149198181927, 0.07225836697034538, 0.07196000206749886, 0.07267429702915251, 0.07684060605242848, 0.07350250089075416, 0.07966141507495195, 0.0726492831017822, 0.07277916895691305, 0.07325093902181834, 0.07286444294732064, 0.0728177489945665, 0.07296733907423913, 0.07351534999907017, 0.07292572106234729, 0.0735834949882701, 0.07262995606288314, 0.07331368804443628, 0.07295519101899117, 0.07246066501829773]
[0.0015015056123957038, 0.0014115819987859956, 0.0014158094691454756, 0.0013604660409179572, 0.0013391934691605214, 0.0013334505112689672, 0.001334957122726708, 0.0013529901625588536, 0.0013416198590694337, 0.001334258775720943, 0.0013381377960156118, 0.0013355443263616488, 0.001325819814786771, 0.001332715550456576, 0.0013338962869186486, 0.0013398988557295228, 0.001345832917687236, 0.0013590756119513999, 0.0018034004083625516, 0.0016653267144016465, 0.0014449364472446696, 0.001429299918972716, 0.001337914121793393, 0.0013505850196340863, 0.0013294036533417447, 0.0013388021426199346, 0.0013595075698151272, 0.0023746877951471477, 0.0016125763044217412, 0.0015166130829222348, 0.001595594694514816, 0.0015984831843525171, 0.001507009039823042, 0.0015063810195507748, 0.0016488246334602638, 0.0015911245718598366, 0.001498241386167249, 0.00157251806064908, 0.0015781348979822835, 0.0016422664499556531, 0.0015724189817069136, 0.0015739342031468237, 0.001612567285798034, 0.0018005400411404517, 0.0015681493476185264, 0.0014711605498985369, 0.0015810123687534003, 0.0015773347137039717, 0.0014744864905024975, 0.0014879965503719083, 0.001575388123604412, 0.0017587230612086703, 0.0015665049194263257, 0.0014894460803087876, 0.0015634961226688965, 0.0015655659371986985, 0.001480862120527546, 0.001566954836135312, 0.0015699082429577805, 0.0014846935530896394, 0.0015709218355275843, 0.0014717620607389479, 0.0015675916730863403, 0.0015717025927020883, 0.0014746869172976942, 0.0015806861207535376, 0.0015643664293599371, 0.0015517337133689802, 0.001571892083584502, 0.0015080048160969603, 0.0014719324690118736, 0.0015662257341021787, 0.0015847348579566699, 0.001550856307719131, 0.0014754508797801575, 0.0014842279179363834, 0.0014865464478617115, 0.001477358836623631, 0.0015642182872991782, 0.0015755867756599067, 0.0015622665297848229, 0.0015635415531542836, 0.0014753605913827006, 0.001558309877101256, 0.00147450085710354, 0.001570577550280307, 0.0015748940831126304, 0.0015033859598013212, 0.0015795555117787147, 0.001506931509594528, 0.001597105551568069, 0.0014982574894948273, 0.0016102250000195844, 0.0015817238350531884, 0.0014940655305601504, 0.00146085777491027, 0.0014730778556051, 0.0015669692241187607, 0.0014746605504152117, 0.001468571470765283, 0.0014831489189622961, 0.0015681756337230302, 0.0015000510385868196, 0.0016257431647949377, 0.0014826384306486165, 0.0014852891623859806, 0.0014949171228942518, 0.001487029447904503, 0.0014860765100931938, 0.0014891293688620232, 0.0015003132652871463, 0.001488280021680557, 0.0015017039793524512, 0.0014822440012833293, 0.0014961977151925771, 0.0014888814493671668, 0.001478789082006076]
[665.9981765932034, 708.4250159466691, 706.3097272569867, 735.0422354719436, 746.718097891303, 749.9340932033227, 749.0877294676375, 739.1036739755313, 745.3676190315296, 749.4797997185125, 747.3071928597802, 748.7583753391741, 754.2503052428946, 750.347663953804, 749.6834722510835, 746.3249899228691, 743.0342852056724, 735.79423485068, 554.5080257068247, 600.4827709494235, 692.0719606089851, 699.6432216400965, 747.4321286478092, 740.4198813569914, 752.2169790088082, 746.9363606209022, 735.5604501238527, 421.1079881926268, 620.1256940573694, 659.3639546305269, 626.7255735041645, 625.5930683468908, 663.5660261981066, 663.8426712905708, 606.4926370619388, 628.4863031378607, 667.4491902524243, 635.9227439252655, 633.6593920320405, 608.9145887544639, 635.9628137498482, 635.3505743764026, 620.1291622415098, 555.3889261838385, 637.6943634346131, 679.735464677168, 632.5061206121252, 633.9808483969473, 678.2022123913832, 672.0445687525694, 634.7642114452711, 568.5943523778877, 638.3637788805751, 671.3905345218559, 639.5922481042003, 638.7466514437085, 675.282314361419, 638.180486724409, 636.9799027973464, 673.5396660940605, 636.5689096581666, 679.4576560139865, 637.921224747997, 636.2526884178444, 678.110036964633, 632.6366676284122, 639.2364226386214, 644.4404677068546, 636.1759884429381, 663.1278556445293, 679.3789939774291, 638.4775695013329, 631.0203848796403, 644.8050635140505, 677.7589235291928, 673.7509703970288, 672.7001375829373, 676.8836217783141, 639.2969626551466, 634.6841795375997, 640.0956436913066, 639.573664021019, 677.8004006890309, 641.7208892111912, 678.1956044192245, 636.7084515002307, 634.9633354540223, 665.165184948351, 633.0894941918909, 663.6001660547076, 626.132692994637, 667.4420164835442, 621.0312223371501, 632.2216165923637, 669.3146850292993, 684.5293341861584, 678.8507451897221, 638.1746269218431, 678.1221615499484, 680.9338325760154, 674.2411279237305, 637.6836742615905, 666.6439836221094, 615.1033088465326, 674.4732763756336, 673.2695729049768, 668.933404190286, 672.4816387524695, 672.9128636433992, 671.5333273993443, 666.5274667211645, 671.9165650499064, 665.9102018436479, 674.6527556422548, 668.3608655767056, 671.645147049847, 676.2289579819139]
Elapsed: 0.0740793987006172~0.006193695554118797
Time per graph: 0.0015118244632779019~0.00012640195008405708
Speed: 665.4768045371069~49.36144645832725
Total Time: 0.0730
best val loss: 0.39191165566444397 test_score: 0.9592

Testing...
Test loss: 0.2136 score: 0.9592 time: 0.07s
test Score 0.9592
Epoch Time List: [0.5350541829830036, 0.3449331499869004, 0.3471056040143594, 0.33054709900170565, 0.33675724803470075, 0.3258219719864428, 0.32432751392479986, 0.32464444311335683, 0.329966633929871, 0.3223410228965804, 0.326826686039567, 0.3236594299087301, 0.324169036000967, 0.3215623099822551, 0.3216194718843326, 0.32134407095145434, 0.32738736900500953, 0.3254573850426823, 0.41249169094953686, 0.3492376741487533, 0.3478119099745527, 0.3479646068299189, 0.33112306287512183, 0.362885823007673, 0.3223589969566092, 0.3225015519419685, 0.3241286900592968, 0.39166441606357694, 0.33757491398137063, 0.32886252005118877, 0.3377672149799764, 0.3364312469493598, 0.32749045186210424, 0.3358456139685586, 0.3478365500923246, 0.3370538739254698, 0.32314916781615466, 0.3290828359313309, 0.32522637699730694, 0.3288843221962452, 0.3254227130673826, 0.32409783406183124, 0.33196051395498216, 0.3679431109922007, 0.3289680400630459, 0.31855882494710386, 0.32308909005951136, 0.323608992039226, 0.3188415670301765, 0.31960992189124227, 0.3286554729565978, 0.345324904890731, 0.33342306001577526, 0.3216423951089382, 0.32865619112271816, 0.3234976631356403, 0.319817699957639, 0.3267756630666554, 0.3332959059625864, 0.319427679060027, 0.32745700306259096, 0.32050748309120536, 0.32776339701376855, 0.3249671689700335, 0.31829012103844434, 0.3246209550416097, 0.32523462106473744, 0.32301702396944165, 0.32976007903926075, 0.32187778898514807, 0.32528290210757405, 0.32336322811897844, 0.32559454604052007, 0.32941252598538995, 0.31867830792907625, 0.322853654040955, 0.3229810199700296, 0.3180531448451802, 0.32213915896136314, 0.32394598016981035, 0.3295872911112383, 0.33029984100721776, 0.3255713559919968, 0.32937487098388374, 0.3266047779470682, 0.3329679500311613, 0.3313328329240903, 0.3267416279995814, 0.3395354258827865, 0.3262829828308895, 0.32821286108810455, 0.32233813602942973, 0.3299815889913589, 0.33120358805172145, 0.32438231899868697, 0.32033817504998296, 0.3192249189596623, 0.32242422399576753, 0.32058768707793206, 0.31897014391142875, 0.3216236828593537, 0.3265884219435975, 0.3218879150226712, 0.3304641479626298, 0.32715815398842096, 0.32130543689709157, 0.3227135579800233, 0.32432118977885693, 0.32793715596199036, 0.3279292330844328, 0.33175860706251115, 0.32735856296494603, 0.32760068005882204, 0.3282601898536086, 0.32857381517533213, 0.32639772805850953, 0.3248999691568315]
Total Epoch List: [27, 90]
Total Time List: [0.06697643792722374, 0.07300460198894143]
========================k_idx:2========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: GPSPerformer
GPS(
  (x_lin): Linear(in_features=14887, out_features=64, bias=True)
  (pe_lin): Linear(in_features=20, out_features=8, bias=True)
  (pe_norm): BatchNorm1d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (convs): ModuleList(
    (0-1): 2 x GPSConv(64, conv=GINEConv(nn=Sequential(
      (0): Linear(in_features=64, out_features=64, bias=True)
      (1): ReLU()
      (2): Linear(in_features=64, out_features=64, bias=True)
    )), heads=1, attn_type=performer)
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=32, bias=True)
    (1): ReLU()
    (2): Linear(in_features=32, out_features=16, bias=True)
    (3): ReLU()
    (4): Linear(in_features=16, out_features=2, bias=True)
  )
)
Total number of parameters: 1039522
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7c9d24c93d90>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.7942;  Loss pred: 0.7942; Loss self: 0.0000; time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.8713 score: 0.5102 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.8646 score: 0.5000 time: 0.06s
Epoch 2/1000, LR 0.000000
Train loss: 0.7417;  Loss pred: 0.7417; Loss self: 0.0000; time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7927 score: 0.5102 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7801 score: 0.5000 time: 0.06s
Epoch 3/1000, LR 0.000030
Train loss: 0.6874;  Loss pred: 0.6874; Loss self: 0.0000; time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7610 score: 0.5102 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7465 score: 0.5000 time: 0.06s
Epoch 4/1000, LR 0.000060
Train loss: 0.7320;  Loss pred: 0.7320; Loss self: 0.0000; time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7544 score: 0.5102 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7387 score: 0.5000 time: 0.06s
Epoch 5/1000, LR 0.000090
Train loss: 0.7925;  Loss pred: 0.7925; Loss self: 0.0000; time: 0.19s
Val loss: 0.7368 score: 0.5306 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7335 score: 0.5000 time: 0.06s
Epoch 6/1000, LR 0.000120
Train loss: 0.6685;  Loss pred: 0.6685; Loss self: 0.0000; time: 0.20s
Val loss: 0.7141 score: 0.5306 time: 0.06s
Test loss: 0.7181 score: 0.5208 time: 0.06s
Epoch 7/1000, LR 0.000150
Train loss: 0.6156;  Loss pred: 0.6156; Loss self: 0.0000; time: 0.19s
Val loss: 0.6916 score: 0.5510 time: 0.06s
Test loss: 0.7014 score: 0.5625 time: 0.06s
Epoch 8/1000, LR 0.000180
Train loss: 0.5803;  Loss pred: 0.5803; Loss self: 0.0000; time: 0.19s
Val loss: 0.6689 score: 0.5306 time: 0.06s
Test loss: 0.6865 score: 0.5417 time: 0.06s
Epoch 9/1000, LR 0.000210
Train loss: 0.5524;  Loss pred: 0.5524; Loss self: 0.0000; time: 0.19s
Val loss: 0.6567 score: 0.5306 time: 0.06s
Test loss: 0.6744 score: 0.5208 time: 0.06s
Epoch 10/1000, LR 0.000240
Train loss: 0.5319;  Loss pred: 0.5319; Loss self: 0.0000; time: 0.19s
Val loss: 0.6475 score: 0.5102 time: 0.07s
Test loss: 0.6696 score: 0.5208 time: 0.06s
Epoch 11/1000, LR 0.000270
Train loss: 0.5358;  Loss pred: 0.5358; Loss self: 0.0000; time: 0.19s
Val loss: 0.6416 score: 0.5306 time: 0.07s
Test loss: 0.6629 score: 0.5625 time: 0.07s
Epoch 12/1000, LR 0.000270
Train loss: 0.5267;  Loss pred: 0.5267; Loss self: 0.0000; time: 0.21s
Val loss: 0.6372 score: 0.5918 time: 0.06s
Test loss: 0.6577 score: 0.6042 time: 0.06s
Epoch 13/1000, LR 0.000270
Train loss: 0.5267;  Loss pred: 0.5267; Loss self: 0.0000; time: 0.19s
Val loss: 0.6326 score: 0.5714 time: 0.06s
Test loss: 0.6522 score: 0.5833 time: 0.06s
Epoch 14/1000, LR 0.000270
Train loss: 0.4666;  Loss pred: 0.4666; Loss self: 0.0000; time: 0.19s
Val loss: 0.6282 score: 0.5714 time: 0.06s
Test loss: 0.6495 score: 0.5833 time: 0.06s
Epoch 15/1000, LR 0.000270
Train loss: 0.4881;  Loss pred: 0.4881; Loss self: 0.0000; time: 0.19s
Val loss: 0.6232 score: 0.5918 time: 0.06s
Test loss: 0.6461 score: 0.5625 time: 0.06s
Epoch 16/1000, LR 0.000270
Train loss: 0.4661;  Loss pred: 0.4661; Loss self: 0.0000; time: 0.19s
Val loss: 0.6187 score: 0.5918 time: 0.06s
Test loss: 0.6393 score: 0.5833 time: 0.06s
Epoch 17/1000, LR 0.000270
Train loss: 0.4233;  Loss pred: 0.4233; Loss self: 0.0000; time: 0.19s
Val loss: 0.6158 score: 0.5714 time: 0.06s
Test loss: 0.6351 score: 0.5833 time: 0.06s
Epoch 18/1000, LR 0.000270
Train loss: 0.4343;  Loss pred: 0.4343; Loss self: 0.0000; time: 0.19s
Val loss: 0.6124 score: 0.5918 time: 0.06s
Test loss: 0.6313 score: 0.6042 time: 0.06s
Epoch 19/1000, LR 0.000270
Train loss: 0.3719;  Loss pred: 0.3719; Loss self: 0.0000; time: 0.20s
Val loss: 0.6093 score: 0.6122 time: 0.06s
Test loss: 0.6286 score: 0.5833 time: 0.06s
Epoch 20/1000, LR 0.000270
Train loss: 0.3447;  Loss pred: 0.3447; Loss self: 0.0000; time: 0.19s
Val loss: 0.6068 score: 0.6122 time: 0.07s
Test loss: 0.6255 score: 0.5833 time: 0.06s
Epoch 21/1000, LR 0.000270
Train loss: 0.3285;  Loss pred: 0.3285; Loss self: 0.0000; time: 0.20s
Val loss: 0.6046 score: 0.5918 time: 0.06s
Test loss: 0.6226 score: 0.6042 time: 0.06s
Epoch 22/1000, LR 0.000270
Train loss: 0.2975;  Loss pred: 0.2975; Loss self: 0.0000; time: 0.20s
Val loss: 0.6020 score: 0.5714 time: 0.06s
Test loss: 0.6201 score: 0.6042 time: 0.06s
Epoch 23/1000, LR 0.000270
Train loss: 0.2699;  Loss pred: 0.2699; Loss self: 0.0000; time: 0.19s
Val loss: 0.5991 score: 0.5510 time: 0.06s
Test loss: 0.6170 score: 0.6042 time: 0.06s
Epoch 24/1000, LR 0.000270
Train loss: 0.2813;  Loss pred: 0.2813; Loss self: 0.0000; time: 0.19s
Val loss: 0.5954 score: 0.5714 time: 0.06s
Test loss: 0.6128 score: 0.6250 time: 0.06s
Epoch 25/1000, LR 0.000270
Train loss: 0.2186;  Loss pred: 0.2186; Loss self: 0.0000; time: 0.19s
Val loss: 0.5913 score: 0.5714 time: 0.06s
Test loss: 0.6073 score: 0.6250 time: 0.06s
Epoch 26/1000, LR 0.000270
Train loss: 0.1940;  Loss pred: 0.1940; Loss self: 0.0000; time: 0.19s
Val loss: 0.5870 score: 0.5714 time: 0.06s
Test loss: 0.6008 score: 0.6667 time: 0.06s
Epoch 27/1000, LR 0.000270
Train loss: 0.1723;  Loss pred: 0.1723; Loss self: 0.0000; time: 0.20s
Val loss: 0.5835 score: 0.5714 time: 0.07s
Test loss: 0.5937 score: 0.6875 time: 0.08s
Epoch 28/1000, LR 0.000270
Train loss: 0.1648;  Loss pred: 0.1648; Loss self: 0.0000; time: 0.19s
Val loss: 0.5810 score: 0.6531 time: 0.06s
Test loss: 0.5883 score: 0.6875 time: 0.06s
Epoch 29/1000, LR 0.000270
Train loss: 0.1343;  Loss pred: 0.1343; Loss self: 0.0000; time: 0.20s
Val loss: 0.5757 score: 0.6735 time: 0.06s
Test loss: 0.5828 score: 0.6875 time: 0.06s
Epoch 30/1000, LR 0.000270
Train loss: 0.1301;  Loss pred: 0.1301; Loss self: 0.0000; time: 0.19s
Val loss: 0.5712 score: 0.7347 time: 0.06s
Test loss: 0.5775 score: 0.7083 time: 0.06s
Epoch 31/1000, LR 0.000270
Train loss: 0.1038;  Loss pred: 0.1038; Loss self: 0.0000; time: 0.19s
Val loss: 0.5673 score: 0.7551 time: 0.06s
Test loss: 0.5708 score: 0.7083 time: 0.06s
Epoch 32/1000, LR 0.000270
Train loss: 0.0887;  Loss pred: 0.0887; Loss self: 0.0000; time: 0.19s
Val loss: 0.5620 score: 0.7143 time: 0.06s
Test loss: 0.5619 score: 0.7083 time: 0.06s
Epoch 33/1000, LR 0.000270
Train loss: 0.0648;  Loss pred: 0.0648; Loss self: 0.0000; time: 0.19s
Val loss: 0.5560 score: 0.7143 time: 0.06s
Test loss: 0.5524 score: 0.7083 time: 0.06s
Epoch 34/1000, LR 0.000270
Train loss: 0.0776;  Loss pred: 0.0776; Loss self: 0.0000; time: 0.19s
Val loss: 0.5494 score: 0.7143 time: 0.06s
Test loss: 0.5454 score: 0.7083 time: 0.06s
Epoch 35/1000, LR 0.000270
Train loss: 0.0717;  Loss pred: 0.0717; Loss self: 0.0000; time: 0.19s
Val loss: 0.5435 score: 0.6939 time: 0.06s
Test loss: 0.5381 score: 0.7083 time: 0.06s
Epoch 36/1000, LR 0.000270
Train loss: 0.0555;  Loss pred: 0.0555; Loss self: 0.0000; time: 0.19s
Val loss: 0.5385 score: 0.6939 time: 0.06s
Test loss: 0.5317 score: 0.7083 time: 0.07s
Epoch 37/1000, LR 0.000270
Train loss: 0.0497;  Loss pred: 0.0497; Loss self: 0.0000; time: 0.19s
Val loss: 0.5338 score: 0.6939 time: 0.06s
Test loss: 0.5245 score: 0.7083 time: 0.06s
Epoch 38/1000, LR 0.000270
Train loss: 0.0350;  Loss pred: 0.0350; Loss self: 0.0000; time: 0.20s
Val loss: 0.5274 score: 0.7347 time: 0.06s
Test loss: 0.5155 score: 0.7292 time: 0.07s
Epoch 39/1000, LR 0.000269
Train loss: 0.0338;  Loss pred: 0.0338; Loss self: 0.0000; time: 0.19s
Val loss: 0.5219 score: 0.7347 time: 0.06s
Test loss: 0.5071 score: 0.7500 time: 0.06s
Epoch 40/1000, LR 0.000269
Train loss: 0.0380;  Loss pred: 0.0380; Loss self: 0.0000; time: 0.21s
Val loss: 0.5146 score: 0.7347 time: 0.06s
Test loss: 0.4995 score: 0.7500 time: 0.06s
Epoch 41/1000, LR 0.000269
Train loss: 0.0277;  Loss pred: 0.0277; Loss self: 0.0000; time: 0.20s
Val loss: 0.5073 score: 0.7551 time: 0.07s
Test loss: 0.4915 score: 0.8125 time: 0.06s
Epoch 42/1000, LR 0.000269
Train loss: 0.0285;  Loss pred: 0.0285; Loss self: 0.0000; time: 0.19s
Val loss: 0.5000 score: 0.7755 time: 0.06s
Test loss: 0.4825 score: 0.8125 time: 0.07s
Epoch 43/1000, LR 0.000269
Train loss: 0.0226;  Loss pred: 0.0226; Loss self: 0.0000; time: 0.20s
Val loss: 0.4910 score: 0.7959 time: 0.06s
Test loss: 0.4709 score: 0.8125 time: 0.06s
Epoch 44/1000, LR 0.000269
Train loss: 0.0194;  Loss pred: 0.0194; Loss self: 0.0000; time: 0.20s
Val loss: 0.4812 score: 0.8163 time: 0.06s
Test loss: 0.4570 score: 0.8750 time: 0.06s
Epoch 45/1000, LR 0.000269
Train loss: 0.0197;  Loss pred: 0.0197; Loss self: 0.0000; time: 0.20s
Val loss: 0.4737 score: 0.8367 time: 0.06s
Test loss: 0.4452 score: 0.8958 time: 0.06s
Epoch 46/1000, LR 0.000269
Train loss: 0.0201;  Loss pred: 0.0201; Loss self: 0.0000; time: 0.20s
Val loss: 0.4675 score: 0.8571 time: 0.06s
Test loss: 0.4359 score: 0.8958 time: 0.06s
Epoch 47/1000, LR 0.000269
Train loss: 0.0150;  Loss pred: 0.0150; Loss self: 0.0000; time: 0.20s
Val loss: 0.4616 score: 0.8776 time: 0.06s
Test loss: 0.4268 score: 0.9375 time: 0.06s
Epoch 48/1000, LR 0.000269
Train loss: 0.0127;  Loss pred: 0.0127; Loss self: 0.0000; time: 0.20s
Val loss: 0.4573 score: 0.8571 time: 0.06s
Test loss: 0.4192 score: 0.9375 time: 0.06s
Epoch 49/1000, LR 0.000269
Train loss: 0.0116;  Loss pred: 0.0116; Loss self: 0.0000; time: 0.19s
Val loss: 0.4526 score: 0.8571 time: 0.06s
Test loss: 0.4112 score: 0.9375 time: 0.07s
Epoch 50/1000, LR 0.000269
Train loss: 0.0102;  Loss pred: 0.0102; Loss self: 0.0000; time: 0.19s
Val loss: 0.4481 score: 0.8571 time: 0.07s
Test loss: 0.4028 score: 0.9375 time: 0.07s
Epoch 51/1000, LR 0.000269
Train loss: 0.0103;  Loss pred: 0.0103; Loss self: 0.0000; time: 0.19s
Val loss: 0.4402 score: 0.8571 time: 0.07s
Test loss: 0.3914 score: 0.9375 time: 0.06s
Epoch 52/1000, LR 0.000269
Train loss: 0.0080;  Loss pred: 0.0080; Loss self: 0.0000; time: 0.19s
Val loss: 0.4295 score: 0.8571 time: 0.07s
Test loss: 0.3786 score: 0.9375 time: 0.06s
Epoch 53/1000, LR 0.000269
Train loss: 0.0072;  Loss pred: 0.0072; Loss self: 0.0000; time: 0.20s
Val loss: 0.4190 score: 0.8776 time: 0.06s
Test loss: 0.3661 score: 0.9375 time: 0.06s
Epoch 54/1000, LR 0.000269
Train loss: 0.0072;  Loss pred: 0.0072; Loss self: 0.0000; time: 0.19s
Val loss: 0.4083 score: 0.8776 time: 0.06s
Test loss: 0.3534 score: 0.9375 time: 0.06s
Epoch 55/1000, LR 0.000269
Train loss: 0.0052;  Loss pred: 0.0052; Loss self: 0.0000; time: 0.19s
Val loss: 0.3977 score: 0.8571 time: 0.06s
Test loss: 0.3394 score: 0.9375 time: 0.06s
Epoch 56/1000, LR 0.000269
Train loss: 0.0066;  Loss pred: 0.0066; Loss self: 0.0000; time: 0.19s
Val loss: 0.3875 score: 0.8571 time: 0.06s
Test loss: 0.3260 score: 0.9375 time: 0.06s
Epoch 57/1000, LR 0.000269
Train loss: 0.0056;  Loss pred: 0.0056; Loss self: 0.0000; time: 0.19s
Val loss: 0.3776 score: 0.8571 time: 0.06s
Test loss: 0.3122 score: 0.9375 time: 0.07s
Epoch 58/1000, LR 0.000269
Train loss: 0.0052;  Loss pred: 0.0052; Loss self: 0.0000; time: 0.20s
Val loss: 0.3677 score: 0.8571 time: 0.07s
Test loss: 0.2990 score: 0.9375 time: 0.06s
Epoch 59/1000, LR 0.000268
Train loss: 0.0054;  Loss pred: 0.0054; Loss self: 0.0000; time: 0.19s
Val loss: 0.3567 score: 0.8571 time: 0.06s
Test loss: 0.2853 score: 0.9375 time: 0.06s
Epoch 60/1000, LR 0.000268
Train loss: 0.0041;  Loss pred: 0.0041; Loss self: 0.0000; time: 0.19s
Val loss: 0.3466 score: 0.8571 time: 0.06s
Test loss: 0.2727 score: 0.9375 time: 0.06s
Epoch 61/1000, LR 0.000268
Train loss: 0.0045;  Loss pred: 0.0045; Loss self: 0.0000; time: 0.19s
Val loss: 0.3365 score: 0.8980 time: 0.06s
Test loss: 0.2605 score: 0.9375 time: 0.06s
Epoch 62/1000, LR 0.000268
Train loss: 0.0046;  Loss pred: 0.0046; Loss self: 0.0000; time: 0.19s
Val loss: 0.3259 score: 0.8980 time: 0.06s
Test loss: 0.2484 score: 0.9375 time: 0.06s
Epoch 63/1000, LR 0.000268
Train loss: 0.0039;  Loss pred: 0.0039; Loss self: 0.0000; time: 0.19s
Val loss: 0.3171 score: 0.8980 time: 0.06s
Test loss: 0.2382 score: 0.9375 time: 0.07s
Epoch 64/1000, LR 0.000268
Train loss: 0.0029;  Loss pred: 0.0029; Loss self: 0.0000; time: 0.19s
Val loss: 0.3101 score: 0.8980 time: 0.06s
Test loss: 0.2302 score: 0.9375 time: 0.06s
Epoch 65/1000, LR 0.000268
Train loss: 0.0032;  Loss pred: 0.0032; Loss self: 0.0000; time: 0.19s
Val loss: 0.3057 score: 0.8980 time: 0.07s
Test loss: 0.2244 score: 0.9375 time: 0.07s
Epoch 66/1000, LR 0.000268
Train loss: 0.0028;  Loss pred: 0.0028; Loss self: 0.0000; time: 0.19s
Val loss: 0.3029 score: 0.8980 time: 0.06s
Test loss: 0.2202 score: 0.9375 time: 0.06s
Epoch 67/1000, LR 0.000268
Train loss: 0.0027;  Loss pred: 0.0027; Loss self: 0.0000; time: 0.20s
Val loss: 0.3017 score: 0.8980 time: 0.15s
Test loss: 0.2176 score: 0.9375 time: 0.06s
Epoch 68/1000, LR 0.000268
Train loss: 0.0028;  Loss pred: 0.0028; Loss self: 0.0000; time: 0.20s
Val loss: 0.3013 score: 0.8980 time: 0.06s
Test loss: 0.2150 score: 0.9375 time: 0.06s
Epoch 69/1000, LR 0.000268
Train loss: 0.0028;  Loss pred: 0.0028; Loss self: 0.0000; time: 0.19s
Val loss: 0.3013 score: 0.8980 time: 0.07s
Test loss: 0.2125 score: 0.9167 time: 0.06s
     INFO: Early stopping counter 1 of 20
Epoch 70/1000, LR 0.000268
Train loss: 0.0030;  Loss pred: 0.0030; Loss self: 0.0000; time: 0.20s
Val loss: 0.3022 score: 0.8980 time: 0.06s
Test loss: 0.2104 score: 0.9167 time: 0.06s
     INFO: Early stopping counter 2 of 20
Epoch 71/1000, LR 0.000268
Train loss: 0.0026;  Loss pred: 0.0026; Loss self: 0.0000; time: 0.20s
Val loss: 0.3028 score: 0.8980 time: 0.06s
Test loss: 0.2095 score: 0.9167 time: 0.06s
     INFO: Early stopping counter 3 of 20
Epoch 72/1000, LR 0.000267
Train loss: 0.0024;  Loss pred: 0.0024; Loss self: 0.0000; time: 0.21s
Val loss: 0.3026 score: 0.8980 time: 0.07s
Test loss: 0.2077 score: 0.9167 time: 0.07s
     INFO: Early stopping counter 4 of 20
Epoch 73/1000, LR 0.000267
Train loss: 0.0022;  Loss pred: 0.0022; Loss self: 0.0000; time: 0.21s
Val loss: 0.3019 score: 0.8980 time: 0.10s
Test loss: 0.2045 score: 0.9167 time: 0.06s
     INFO: Early stopping counter 5 of 20
Epoch 74/1000, LR 0.000267
Train loss: 0.0019;  Loss pred: 0.0019; Loss self: 0.0000; time: 0.20s
Val loss: 0.2998 score: 0.8980 time: 0.06s
Test loss: 0.1974 score: 0.9375 time: 0.06s
Epoch 75/1000, LR 0.000267
Train loss: 0.0018;  Loss pred: 0.0018; Loss self: 0.0000; time: 0.20s
Val loss: 0.2979 score: 0.8980 time: 0.06s
Test loss: 0.1910 score: 0.9375 time: 0.06s
Epoch 76/1000, LR 0.000267
Train loss: 0.0018;  Loss pred: 0.0018; Loss self: 0.0000; time: 0.20s
Val loss: 0.2964 score: 0.8980 time: 0.06s
Test loss: 0.1836 score: 0.9375 time: 0.06s
Epoch 77/1000, LR 0.000267
Train loss: 0.0015;  Loss pred: 0.0015; Loss self: 0.0000; time: 0.20s
Val loss: 0.2963 score: 0.8980 time: 0.06s
Test loss: 0.1778 score: 0.9375 time: 0.06s
Epoch 78/1000, LR 0.000267
Train loss: 0.0019;  Loss pred: 0.0019; Loss self: 0.0000; time: 0.21s
Val loss: 0.2974 score: 0.8980 time: 0.10s
Test loss: 0.1727 score: 0.9375 time: 0.06s
     INFO: Early stopping counter 1 of 20
Epoch 79/1000, LR 0.000267
Train loss: 0.0013;  Loss pred: 0.0013; Loss self: 0.0000; time: 0.21s
Val loss: 0.3009 score: 0.8980 time: 0.06s
Test loss: 0.1696 score: 0.9375 time: 0.06s
     INFO: Early stopping counter 2 of 20
Epoch 80/1000, LR 0.000267
Train loss: 0.0014;  Loss pred: 0.0014; Loss self: 0.0000; time: 0.20s
Val loss: 0.3048 score: 0.8980 time: 0.06s
Test loss: 0.1674 score: 0.9375 time: 0.06s
     INFO: Early stopping counter 3 of 20
Epoch 81/1000, LR 0.000267
Train loss: 0.0017;  Loss pred: 0.0017; Loss self: 0.0000; time: 0.20s
Val loss: 0.3103 score: 0.8980 time: 0.06s
Test loss: 0.1672 score: 0.9583 time: 0.06s
     INFO: Early stopping counter 4 of 20
Epoch 82/1000, LR 0.000267
Train loss: 0.0018;  Loss pred: 0.0018; Loss self: 0.0000; time: 0.20s
Val loss: 0.3176 score: 0.8980 time: 0.07s
Test loss: 0.1686 score: 0.9583 time: 0.06s
     INFO: Early stopping counter 5 of 20
Epoch 83/1000, LR 0.000266
Train loss: 0.0014;  Loss pred: 0.0014; Loss self: 0.0000; time: 0.20s
Val loss: 0.3265 score: 0.8980 time: 0.06s
Test loss: 0.1716 score: 0.9375 time: 0.06s
     INFO: Early stopping counter 6 of 20
Epoch 84/1000, LR 0.000266
Train loss: 0.0011;  Loss pred: 0.0011; Loss self: 0.0000; time: 0.21s
Val loss: 0.3387 score: 0.8980 time: 0.09s
Test loss: 0.1764 score: 0.9375 time: 0.06s
     INFO: Early stopping counter 7 of 20
Epoch 85/1000, LR 0.000266
Train loss: 0.0013;  Loss pred: 0.0013; Loss self: 0.0000; time: 0.20s
Val loss: 0.3548 score: 0.8980 time: 0.06s
Test loss: 0.1838 score: 0.9375 time: 0.06s
     INFO: Early stopping counter 8 of 20
Epoch 86/1000, LR 0.000266
Train loss: 0.0014;  Loss pred: 0.0014; Loss self: 0.0000; time: 0.20s
Val loss: 0.3713 score: 0.8980 time: 0.06s
Test loss: 0.1915 score: 0.9375 time: 0.06s
     INFO: Early stopping counter 9 of 20
Epoch 87/1000, LR 0.000266
Train loss: 0.0010;  Loss pred: 0.0010; Loss self: 0.0000; time: 0.20s
Val loss: 0.3915 score: 0.8980 time: 0.08s
Test loss: 0.2023 score: 0.9375 time: 0.07s
     INFO: Early stopping counter 10 of 20
Epoch 88/1000, LR 0.000266
Train loss: 0.0014;  Loss pred: 0.0014; Loss self: 0.0000; time: 0.20s
Val loss: 0.4152 score: 0.8776 time: 0.06s
Test loss: 0.2181 score: 0.9167 time: 0.06s
     INFO: Early stopping counter 11 of 20
Epoch 89/1000, LR 0.000266
Train loss: 0.0012;  Loss pred: 0.0012; Loss self: 0.0000; time: 0.20s
Val loss: 0.4374 score: 0.8571 time: 0.06s
Test loss: 0.2323 score: 0.9167 time: 0.06s
     INFO: Early stopping counter 12 of 20
Epoch 90/1000, LR 0.000266
Train loss: 0.0017;  Loss pred: 0.0017; Loss self: 0.0000; time: 0.20s
Val loss: 0.4678 score: 0.8571 time: 0.06s
Test loss: 0.2546 score: 0.9167 time: 0.06s
     INFO: Early stopping counter 13 of 20
Epoch 91/1000, LR 0.000266
Train loss: 0.0018;  Loss pred: 0.0018; Loss self: 0.0000; time: 0.20s
Val loss: 0.4868 score: 0.8571 time: 0.06s
Test loss: 0.2636 score: 0.8958 time: 0.07s
     INFO: Early stopping counter 14 of 20
Epoch 92/1000, LR 0.000266
Train loss: 0.0010;  Loss pred: 0.0010; Loss self: 0.0000; time: 0.22s
Val loss: 0.5040 score: 0.8571 time: 0.06s
Test loss: 0.2718 score: 0.8958 time: 0.06s
     INFO: Early stopping counter 15 of 20
Epoch 93/1000, LR 0.000265
Train loss: 0.0012;  Loss pred: 0.0012; Loss self: 0.0000; time: 0.20s
Val loss: 0.5161 score: 0.8571 time: 0.06s
Test loss: 0.2748 score: 0.8958 time: 0.06s
     INFO: Early stopping counter 16 of 20
Epoch 94/1000, LR 0.000265
Train loss: 0.0011;  Loss pred: 0.0011; Loss self: 0.0000; time: 0.20s
Val loss: 0.5276 score: 0.8571 time: 0.07s
Test loss: 0.2768 score: 0.8958 time: 0.06s
     INFO: Early stopping counter 17 of 20
Epoch 95/1000, LR 0.000265
Train loss: 0.0011;  Loss pred: 0.0011; Loss self: 0.0000; time: 0.20s
Val loss: 0.5351 score: 0.8571 time: 0.06s
Test loss: 0.2780 score: 0.8958 time: 0.06s
     INFO: Early stopping counter 18 of 20
Epoch 96/1000, LR 0.000265
Train loss: 0.0012;  Loss pred: 0.0012; Loss self: 0.0000; time: 0.20s
Val loss: 0.5399 score: 0.8571 time: 0.06s
Test loss: 0.2777 score: 0.8958 time: 0.06s
     INFO: Early stopping counter 19 of 20
Epoch 97/1000, LR 0.000265
Train loss: 0.0011;  Loss pred: 0.0011; Loss self: 0.0000; time: 0.20s
Val loss: 0.5449 score: 0.8571 time: 0.06s
Test loss: 0.2765 score: 0.8958 time: 0.06s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 076,   Train_Loss: 0.0015,   Val_Loss: 0.2963,   Val_Precision: 1.0000,   Val_Recall: 0.8000,   Val_accuracy: 0.8889,   Val_Score: 0.8980,   Val_Loss: 0.2963,   Test_Precision: 1.0000,   Test_Recall: 0.8750,   Test_accuracy: 0.9333,   Test_Score: 0.9375,   Test_loss: 0.1778


[0.07357377500738949, 0.06916751794051379, 0.0693746639881283, 0.06666283600497991, 0.06562047998886555, 0.0653390750521794, 0.0654128990136087, 0.06629651796538383, 0.06573937309440225, 0.0653786800103262, 0.06556875200476497, 0.0654416719917208, 0.06496517092455178, 0.06530306197237223, 0.06536091805901378, 0.06565504393074661, 0.06594581296667457, 0.06659470498561859, 0.08836662000976503, 0.08160100900568068, 0.07080188591498882, 0.07003569602966309, 0.06555779196787626, 0.06617866596207023, 0.06514077901374549, 0.0656013049883768, 0.06661587092094123, 0.11635970196221024, 0.07901623891666532, 0.0743140410631895, 0.07818414003122598, 0.07832567603327334, 0.07384344295132905, 0.07381266995798796, 0.08079240703955293, 0.07796510402113199, 0.0734138279221952, 0.07705338497180492, 0.07732861000113189, 0.080471056047827, 0.07704853010363877, 0.07712277595419437, 0.07901579700410366, 0.08822646201588213, 0.07683931803330779, 0.0720868669450283, 0.07746960606891662, 0.07728940097149462, 0.07224983803462237, 0.07291183096822351, 0.07719401805661619, 0.08617742999922484, 0.07675874105188996, 0.0729828579351306, 0.07661131001077592, 0.07671273092273623, 0.07256224390584975, 0.07678078697063029, 0.07692550390493125, 0.07274998410139233, 0.07697516994085163, 0.07211634097620845, 0.07681199198123068, 0.07701342704240233, 0.07225965894758701, 0.07745361991692334, 0.07665395503863692, 0.07603495195508003, 0.0770227120956406, 0.07389223598875105, 0.07212469098158181, 0.07674506097100675, 0.07765200803987682, 0.07599195907823741, 0.07229709310922772, 0.07272716797888279, 0.07284077594522387, 0.07239058299455792, 0.07664669607765973, 0.07720375200733542, 0.07655105995945632, 0.0766135361045599, 0.07229266897775233, 0.07635718397796154, 0.07225054199807346, 0.07695829996373504, 0.07716981007251889, 0.07366591203026474, 0.07739822007715702, 0.07383964397013187, 0.07825817202683538, 0.07341461698524654, 0.07890102500095963, 0.07750446791760623, 0.07320921099744737, 0.07158203097060323, 0.0721808149246499, 0.07678149198181927, 0.07225836697034538, 0.07196000206749886, 0.07267429702915251, 0.07684060605242848, 0.07350250089075416, 0.07966141507495195, 0.0726492831017822, 0.07277916895691305, 0.07325093902181834, 0.07286444294732064, 0.0728177489945665, 0.07296733907423913, 0.07351534999907017, 0.07292572106234729, 0.0735834949882701, 0.07262995606288314, 0.07331368804443628, 0.07295519101899117, 0.07246066501829773, 0.06889351294375956, 0.06869614403694868, 0.0688501310069114, 0.0686441280413419, 0.06867326807696372, 0.06862356304191053, 0.06898536195512861, 0.06901147600729018, 0.06886521994601935, 0.06896240392234176, 0.07020025001838803, 0.06900392507668585, 0.06942711304873228, 0.06888721499126405, 0.06864787603262812, 0.06899997999425977, 0.06869702797848731, 0.06837535405065864, 0.06897693197242916, 0.06917882803827524, 0.06873027398250997, 0.06883611797820777, 0.06919917499180883, 0.06855644495226443, 0.06864362698979676, 0.06915999599732459, 0.08070242300163954, 0.06841350207105279, 0.06866445008199662, 0.06866451795212924, 0.06870683492161334, 0.06893198809120804, 0.06895757594611496, 0.06860856700222939, 0.06897187093272805, 0.07346119405701756, 0.06873961095698178, 0.074133814079687, 0.06868790404405445, 0.06880914699286222, 0.0685948469908908, 0.07098793098703027, 0.06865247501991689, 0.06880469992756844, 0.06873366702347994, 0.06878929305821657, 0.06871050701010972, 0.06905369297601283, 0.06964495393913239, 0.0694888710277155, 0.0692630319390446, 0.0692765770945698, 0.06898040499072522, 0.06879194092471153, 0.06874231400433928, 0.0688518100650981, 0.07642233499791473, 0.06860609399154782, 0.06899752700701356, 0.06793346104677767, 0.06896445492748171, 0.06801335397176445, 0.07010666304267943, 0.06809574505314231, 0.069771409034729, 0.06790208502206951, 0.06604213896207511, 0.06732370494864881, 0.06831209198571742, 0.06742617697454989, 0.06797097995877266, 0.07121557090431452, 0.06811429304070771, 0.06673648301512003, 0.06796481995843351, 0.06836916995234787, 0.06855753902345896, 0.06867864797823131, 0.06835051800590008, 0.06808964698575437, 0.06791908503510058, 0.06844071194063872, 0.06844068190548569, 0.06917963910382241, 0.06886533193755895, 0.06888529495336115, 0.06952634791377932, 0.06878883705940098, 0.06827914097812027, 0.06814479199238122, 0.07168322301004082, 0.0682260199682787, 0.06809700897429138, 0.06734379404224455, 0.06758772709872574, 0.0677638619672507, 0.0677105690119788]
[0.0015015056123957038, 0.0014115819987859956, 0.0014158094691454756, 0.0013604660409179572, 0.0013391934691605214, 0.0013334505112689672, 0.001334957122726708, 0.0013529901625588536, 0.0013416198590694337, 0.001334258775720943, 0.0013381377960156118, 0.0013355443263616488, 0.001325819814786771, 0.001332715550456576, 0.0013338962869186486, 0.0013398988557295228, 0.001345832917687236, 0.0013590756119513999, 0.0018034004083625516, 0.0016653267144016465, 0.0014449364472446696, 0.001429299918972716, 0.001337914121793393, 0.0013505850196340863, 0.0013294036533417447, 0.0013388021426199346, 0.0013595075698151272, 0.0023746877951471477, 0.0016125763044217412, 0.0015166130829222348, 0.001595594694514816, 0.0015984831843525171, 0.001507009039823042, 0.0015063810195507748, 0.0016488246334602638, 0.0015911245718598366, 0.001498241386167249, 0.00157251806064908, 0.0015781348979822835, 0.0016422664499556531, 0.0015724189817069136, 0.0015739342031468237, 0.001612567285798034, 0.0018005400411404517, 0.0015681493476185264, 0.0014711605498985369, 0.0015810123687534003, 0.0015773347137039717, 0.0014744864905024975, 0.0014879965503719083, 0.001575388123604412, 0.0017587230612086703, 0.0015665049194263257, 0.0014894460803087876, 0.0015634961226688965, 0.0015655659371986985, 0.001480862120527546, 0.001566954836135312, 0.0015699082429577805, 0.0014846935530896394, 0.0015709218355275843, 0.0014717620607389479, 0.0015675916730863403, 0.0015717025927020883, 0.0014746869172976942, 0.0015806861207535376, 0.0015643664293599371, 0.0015517337133689802, 0.001571892083584502, 0.0015080048160969603, 0.0014719324690118736, 0.0015662257341021787, 0.0015847348579566699, 0.001550856307719131, 0.0014754508797801575, 0.0014842279179363834, 0.0014865464478617115, 0.001477358836623631, 0.0015642182872991782, 0.0015755867756599067, 0.0015622665297848229, 0.0015635415531542836, 0.0014753605913827006, 0.001558309877101256, 0.00147450085710354, 0.001570577550280307, 0.0015748940831126304, 0.0015033859598013212, 0.0015795555117787147, 0.001506931509594528, 0.001597105551568069, 0.0014982574894948273, 0.0016102250000195844, 0.0015817238350531884, 0.0014940655305601504, 0.00146085777491027, 0.0014730778556051, 0.0015669692241187607, 0.0014746605504152117, 0.001468571470765283, 0.0014831489189622961, 0.0015681756337230302, 0.0015000510385868196, 0.0016257431647949377, 0.0014826384306486165, 0.0014852891623859806, 0.0014949171228942518, 0.001487029447904503, 0.0014860765100931938, 0.0014891293688620232, 0.0015003132652871463, 0.001488280021680557, 0.0015017039793524512, 0.0014822440012833293, 0.0014961977151925771, 0.0014888814493671668, 0.001478789082006076, 0.0014352815196616575, 0.001431169667436431, 0.001434377729310654, 0.0014300860008612897, 0.0014306930849367443, 0.001429657563373136, 0.0014371950407318461, 0.0014377390834852122, 0.0014346920822087366, 0.0014367167483821202, 0.0014625052087164174, 0.0014375817724309552, 0.001446398188515256, 0.001435150312318001, 0.0014301640840130858, 0.0014374995832137454, 0.0014311880828851524, 0.001424486542722055, 0.001437019416092274, 0.0014412255841307342, 0.0014318807079689577, 0.0014340857912126619, 0.0014416494789960173, 0.0014282592698388423, 0.0014300755622874324, 0.0014408332499442622, 0.001681300479200824, 0.0014252812931469332, 0.0014305093767082628, 0.0014305107906693593, 0.0014313923942002778, 0.001436083085233501, 0.0014366161655440617, 0.001429345145879779, 0.0014369139777651678, 0.0015304415428545326, 0.0014320752282704536, 0.001544454459993479, 0.001430998000917801, 0.0014335238956846297, 0.001429059312310225, 0.001478915228896464, 0.0014302598962482687, 0.001433431248491009, 0.0014319513963224988, 0.0014331102720461786, 0.0014314688960439526, 0.001438618603666934, 0.0014509365403985914, 0.0014476848130774063, 0.001442979832063429, 0.0014432620228035375, 0.0014370917706401087, 0.00143316543593149, 0.0014321315417570684, 0.0014344127096895438, 0.0015921319791232236, 0.001429293624823913, 0.0014374484793127824, 0.0014152804384745348, 0.001436759477655869, 0.0014169448744117592, 0.0014605554800558214, 0.001418661355273798, 0.0014535710215568542, 0.0014146267712931149, 0.0013758778950432315, 0.0014025771864301835, 0.0014231685830357794, 0.0014047120203031227, 0.0014160620824744303, 0.0014836577271732192, 0.0014190477716814105, 0.001390343396148334, 0.0014159337491340314, 0.0014243577073405806, 0.0014282820629887283, 0.0014308051662131522, 0.0014239691251229185, 0.0014185343122032161, 0.0014149809382312621, 0.0014258481654299733, 0.0014258475396976185, 0.0014412424813296336, 0.0014346944153658114, 0.0014351103115283574, 0.001448465581537069, 0.0014331007720708537, 0.0014224821037108388, 0.001419683166507942, 0.0014934004793758504, 0.0014213754160058063, 0.0014186876869644038, 0.0014029957092134282, 0.0014080776478901196, 0.001411747124317723, 0.001410636854416225]
[665.9981765932034, 708.4250159466691, 706.3097272569867, 735.0422354719436, 746.718097891303, 749.9340932033227, 749.0877294676375, 739.1036739755313, 745.3676190315296, 749.4797997185125, 747.3071928597802, 748.7583753391741, 754.2503052428946, 750.347663953804, 749.6834722510835, 746.3249899228691, 743.0342852056724, 735.79423485068, 554.5080257068247, 600.4827709494235, 692.0719606089851, 699.6432216400965, 747.4321286478092, 740.4198813569914, 752.2169790088082, 746.9363606209022, 735.5604501238527, 421.1079881926268, 620.1256940573694, 659.3639546305269, 626.7255735041645, 625.5930683468908, 663.5660261981066, 663.8426712905708, 606.4926370619388, 628.4863031378607, 667.4491902524243, 635.9227439252655, 633.6593920320405, 608.9145887544639, 635.9628137498482, 635.3505743764026, 620.1291622415098, 555.3889261838385, 637.6943634346131, 679.735464677168, 632.5061206121252, 633.9808483969473, 678.2022123913832, 672.0445687525694, 634.7642114452711, 568.5943523778877, 638.3637788805751, 671.3905345218559, 639.5922481042003, 638.7466514437085, 675.282314361419, 638.180486724409, 636.9799027973464, 673.5396660940605, 636.5689096581666, 679.4576560139865, 637.921224747997, 636.2526884178444, 678.110036964633, 632.6366676284122, 639.2364226386214, 644.4404677068546, 636.1759884429381, 663.1278556445293, 679.3789939774291, 638.4775695013329, 631.0203848796403, 644.8050635140505, 677.7589235291928, 673.7509703970288, 672.7001375829373, 676.8836217783141, 639.2969626551466, 634.6841795375997, 640.0956436913066, 639.573664021019, 677.8004006890309, 641.7208892111912, 678.1956044192245, 636.7084515002307, 634.9633354540223, 665.165184948351, 633.0894941918909, 663.6001660547076, 626.132692994637, 667.4420164835442, 621.0312223371501, 632.2216165923637, 669.3146850292993, 684.5293341861584, 678.8507451897221, 638.1746269218431, 678.1221615499484, 680.9338325760154, 674.2411279237305, 637.6836742615905, 666.6439836221094, 615.1033088465326, 674.4732763756336, 673.2695729049768, 668.933404190286, 672.4816387524695, 672.9128636433992, 671.5333273993443, 666.5274667211645, 671.9165650499064, 665.9102018436479, 674.6527556422548, 668.3608655767056, 671.645147049847, 676.2289579819139, 696.7274268505404, 698.72917429227, 697.1664294317989, 699.2586455623898, 698.9619300803522, 699.4681982730176, 695.7997847604468, 695.5364930164573, 697.0136745025319, 696.0314210341706, 683.7582485450839, 695.6126038722632, 691.372547297305, 696.7911245372184, 699.2204679018147, 695.6523756092857, 698.7201835722988, 702.0073338770175, 695.884821597837, 693.853905322631, 698.3822007200891, 697.3083522112026, 693.6498882491273, 700.1529912092467, 699.2637496724169, 694.0428394740921, 594.7776809504819, 701.6158879010204, 699.0516918533554, 699.0510008890487, 698.6204510040744, 696.3385407728024, 696.0801527813029, 699.621083740756, 695.9358844537792, 653.4062046792266, 698.2873387229247, 647.4778155674607, 698.8129957963804, 697.5816747877892, 699.7610185845923, 676.1712777453644, 699.1736275505673, 697.6267616969509, 698.347725047215, 697.7830104952156, 698.5831147038039, 695.1112667743021, 689.2100186030788, 690.7580924844108, 693.0103787868069, 692.8748794051265, 695.8497852608122, 697.7561521709788, 698.2598810533212, 697.1494279470197, 628.0886340532481, 699.6463026435164, 695.677107313148, 706.5737452556426, 696.0107210369966, 705.7437576145277, 684.6710129503473, 704.88985710547, 687.9608805966324, 706.9002370751805, 726.8086823711773, 712.9732393161075, 702.657444746909, 711.8896866734365, 706.1837276601584, 674.0099024761447, 704.6979107793626, 719.2467722508686, 706.247732714605, 702.0708315378871, 700.1418178615688, 698.9071773109787, 702.2624173214984, 704.9529866125242, 706.7233013400225, 701.3369475412859, 701.3372553225931, 693.845770544759, 697.0125409911942, 696.8105461767775, 690.3857521687394, 697.7876360745964, 702.9965420241795, 704.3825154733254, 669.6127487637733, 703.5438975088584, 704.8767739288139, 712.7605547422788, 710.1881075226299, 708.3421547490565, 708.8996695849393]
Elapsed: 0.07178356255530859~0.0053618357741798735
Time per graph: 0.0014782709233481818~0.0001034833486335617
Speed: 679.2770703937389~41.003943672016796
Total Time: 0.0684
best val loss: 0.2963242530822754 test_score: 0.9375

Testing...
Test loss: 0.2605 score: 0.9375 time: 0.07s
test Score 0.9375
Epoch Time List: [0.5350541829830036, 0.3449331499869004, 0.3471056040143594, 0.33054709900170565, 0.33675724803470075, 0.3258219719864428, 0.32432751392479986, 0.32464444311335683, 0.329966633929871, 0.3223410228965804, 0.326826686039567, 0.3236594299087301, 0.324169036000967, 0.3215623099822551, 0.3216194718843326, 0.32134407095145434, 0.32738736900500953, 0.3254573850426823, 0.41249169094953686, 0.3492376741487533, 0.3478119099745527, 0.3479646068299189, 0.33112306287512183, 0.362885823007673, 0.3223589969566092, 0.3225015519419685, 0.3241286900592968, 0.39166441606357694, 0.33757491398137063, 0.32886252005118877, 0.3377672149799764, 0.3364312469493598, 0.32749045186210424, 0.3358456139685586, 0.3478365500923246, 0.3370538739254698, 0.32314916781615466, 0.3290828359313309, 0.32522637699730694, 0.3288843221962452, 0.3254227130673826, 0.32409783406183124, 0.33196051395498216, 0.3679431109922007, 0.3289680400630459, 0.31855882494710386, 0.32308909005951136, 0.323608992039226, 0.3188415670301765, 0.31960992189124227, 0.3286554729565978, 0.345324904890731, 0.33342306001577526, 0.3216423951089382, 0.32865619112271816, 0.3234976631356403, 0.319817699957639, 0.3267756630666554, 0.3332959059625864, 0.319427679060027, 0.32745700306259096, 0.32050748309120536, 0.32776339701376855, 0.3249671689700335, 0.31829012103844434, 0.3246209550416097, 0.32523462106473744, 0.32301702396944165, 0.32976007903926075, 0.32187778898514807, 0.32528290210757405, 0.32336322811897844, 0.32559454604052007, 0.32941252598538995, 0.31867830792907625, 0.322853654040955, 0.3229810199700296, 0.3180531448451802, 0.32213915896136314, 0.32394598016981035, 0.3295872911112383, 0.33029984100721776, 0.3255713559919968, 0.32937487098388374, 0.3266047779470682, 0.3329679500311613, 0.3313328329240903, 0.3267416279995814, 0.3395354258827865, 0.3262829828308895, 0.32821286108810455, 0.32233813602942973, 0.3299815889913589, 0.33120358805172145, 0.32438231899868697, 0.32033817504998296, 0.3192249189596623, 0.32242422399576753, 0.32058768707793206, 0.31897014391142875, 0.3216236828593537, 0.3265884219435975, 0.3218879150226712, 0.3304641479626298, 0.32715815398842096, 0.32130543689709157, 0.3227135579800233, 0.32432118977885693, 0.32793715596199036, 0.3279292330844328, 0.33175860706251115, 0.32735856296494603, 0.32760068005882204, 0.3282601898536086, 0.32857381517533213, 0.32639772805850953, 0.3248999691568315, 0.3246318040182814, 0.3199499810580164, 0.32190967397764325, 0.322080904035829, 0.32090594712644815, 0.32335585390683264, 0.3229300760431215, 0.3219886899460107, 0.32314971496816725, 0.3232838480034843, 0.326069253962487, 0.3341001569060609, 0.3219146759947762, 0.3204942950978875, 0.32077276601921767, 0.31949526397511363, 0.3212751060491428, 0.3204300559591502, 0.3235214650630951, 0.32783572492189705, 0.32596973702311516, 0.3271705547813326, 0.32037985301576555, 0.3193237299565226, 0.32049424096476287, 0.3201804499840364, 0.339068096014671, 0.320976248010993, 0.33030644699465483, 0.32230843498837203, 0.3204240120248869, 0.32337003701832145, 0.32078230008482933, 0.3211931479163468, 0.3212242558365688, 0.3266663629328832, 0.3213946840260178, 0.3338279000017792, 0.3211428578943014, 0.33787307294551283, 0.3274185119662434, 0.3259311808506027, 0.32658390409778804, 0.33131503697950393, 0.32939201581757516, 0.3299684369703755, 0.32995874900370836, 0.33176961599383503, 0.32342942617833614, 0.32807058293838054, 0.32791071094106883, 0.32805802789516747, 0.3268232720438391, 0.32146796805318445, 0.3232408339390531, 0.32168490102048963, 0.33038951188791543, 0.3317715850425884, 0.3197644539177418, 0.3196220851968974, 0.3192816120572388, 0.31948544189799577, 0.32128076907247305, 0.3197059070225805, 0.3228525338927284, 0.321681737084873, 0.4110306790098548, 0.328000143985264, 0.3267910530557856, 0.3242045039078221, 0.3257049969397485, 0.34035697916988283, 0.36832424299791455, 0.3222715411102399, 0.32255920593161136, 0.32848765002563596, 0.3278986137593165, 0.3741958448663354, 0.3377835670253262, 0.3289136470993981, 0.3283769869012758, 0.3284601860214025, 0.32618264900520444, 0.36279967590235174, 0.3252641150029376, 0.3244933430105448, 0.339113941998221, 0.326189927989617, 0.32996009092312306, 0.3237941999686882, 0.3275205740937963, 0.35178072610870004, 0.32711135607678443, 0.3283376310719177, 0.32418036297895014, 0.32974958198610693, 0.3234911581967026]
Total Epoch List: [27, 90, 97]
Total Time List: [0.06697643792722374, 0.07300460198894143, 0.06840927107259631]
========================training times:1========================
========================k_idx:0========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: GPSPerformer
GPS(
  (x_lin): Linear(in_features=14887, out_features=64, bias=True)
  (pe_lin): Linear(in_features=20, out_features=8, bias=True)
  (pe_norm): BatchNorm1d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (convs): ModuleList(
    (0-1): 2 x GPSConv(64, conv=GINEConv(nn=Sequential(
      (0): Linear(in_features=64, out_features=64, bias=True)
      (1): ReLU()
      (2): Linear(in_features=64, out_features=64, bias=True)
    )), heads=1, attn_type=performer)
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=32, bias=True)
    (1): ReLU()
    (2): Linear(in_features=32, out_features=16, bias=True)
    (3): ReLU()
    (4): Linear(in_features=16, out_features=2, bias=True)
  )
)
Total number of parameters: 1039522
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7c9d24c926e0>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.8269;  Loss pred: 0.8269; Loss self: 0.0000; time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 1.7531 score: 0.5102 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 1.3019 score: 0.4898 time: 0.06s
Epoch 2/1000, LR 0.000000
Train loss: 0.8184;  Loss pred: 0.8184; Loss self: 0.0000; time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 1.6104 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 1.2024 score: 0.4898 time: 0.06s
Epoch 3/1000, LR 0.000030
Train loss: 0.8154;  Loss pred: 0.8154; Loss self: 0.0000; time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 1.4550 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 1.1091 score: 0.4898 time: 0.06s
Epoch 4/1000, LR 0.000060
Train loss: 0.7741;  Loss pred: 0.7741; Loss self: 0.0000; time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 1.3280 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 1.0312 score: 0.4898 time: 0.07s
Epoch 5/1000, LR 0.000090
Train loss: 0.7261;  Loss pred: 0.7261; Loss self: 0.0000; time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 1.2270 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.9616 score: 0.4898 time: 0.06s
Epoch 6/1000, LR 0.000120
Train loss: 0.6775;  Loss pred: 0.6775; Loss self: 0.0000; time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 1.1453 score: 0.5102 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.9162 score: 0.4898 time: 0.06s
Epoch 7/1000, LR 0.000150
Train loss: 0.7051;  Loss pred: 0.7051; Loss self: 0.0000; time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 1.0700 score: 0.5102 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.8832 score: 0.4898 time: 0.06s
Epoch 8/1000, LR 0.000180
Train loss: 0.6572;  Loss pred: 0.6572; Loss self: 0.0000; time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.9883 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.8455 score: 0.4898 time: 0.07s
Epoch 9/1000, LR 0.000210
Train loss: 0.6232;  Loss pred: 0.6232; Loss self: 0.0000; time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.9109 score: 0.5102 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.8134 score: 0.4898 time: 0.06s
Epoch 10/1000, LR 0.000240
Train loss: 0.6152;  Loss pred: 0.6152; Loss self: 0.0000; time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.8604 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7841 score: 0.4898 time: 0.06s
Epoch 11/1000, LR 0.000270
Train loss: 0.5132;  Loss pred: 0.5132; Loss self: 0.0000; time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.8231 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7662 score: 0.4898 time: 0.06s
Epoch 12/1000, LR 0.000270
Train loss: 0.4657;  Loss pred: 0.4657; Loss self: 0.0000; time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7971 score: 0.5102 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7495 score: 0.4898 time: 0.06s
Epoch 13/1000, LR 0.000270
Train loss: 0.4586;  Loss pred: 0.4586; Loss self: 0.0000; time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7786 score: 0.5102 time: 0.07s
Test loss: 0.7335 score: 0.5102 time: 0.06s
Epoch 14/1000, LR 0.000270
Train loss: 0.4204;  Loss pred: 0.4204; Loss self: 0.0000; time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7602 score: 0.5102 time: 0.08s
Test loss: 0.7183 score: 0.5714 time: 0.07s
Epoch 15/1000, LR 0.000270
Train loss: 0.4049;  Loss pred: 0.4049; Loss self: 0.0000; time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7431 score: 0.5102 time: 0.07s
Test loss: 0.7049 score: 0.5918 time: 0.06s
Epoch 16/1000, LR 0.000270
Train loss: 0.3616;  Loss pred: 0.3616; Loss self: 0.0000; time: 0.20s
Val loss: 0.7254 score: 0.5102 time: 0.07s
Test loss: 0.6934 score: 0.5714 time: 0.06s
Epoch 17/1000, LR 0.000270
Train loss: 0.3452;  Loss pred: 0.3452; Loss self: 0.0000; time: 0.19s
Val loss: 0.7093 score: 0.5102 time: 0.07s
Test loss: 0.6837 score: 0.5714 time: 0.06s
Epoch 18/1000, LR 0.000270
Train loss: 0.3369;  Loss pred: 0.3369; Loss self: 0.0000; time: 0.20s
Val loss: 0.6949 score: 0.5306 time: 0.07s
Test loss: 0.6744 score: 0.5918 time: 0.06s
Epoch 19/1000, LR 0.000270
Train loss: 0.2765;  Loss pred: 0.2765; Loss self: 0.0000; time: 0.19s
Val loss: 0.6815 score: 0.5306 time: 0.07s
Test loss: 0.6658 score: 0.5918 time: 0.07s
Epoch 20/1000, LR 0.000270
Train loss: 0.2493;  Loss pred: 0.2493; Loss self: 0.0000; time: 0.20s
Val loss: 0.6717 score: 0.5306 time: 0.07s
Test loss: 0.6586 score: 0.6531 time: 0.06s
Epoch 21/1000, LR 0.000270
Train loss: 0.2420;  Loss pred: 0.2420; Loss self: 0.0000; time: 0.20s
Val loss: 0.6608 score: 0.5306 time: 0.07s
Test loss: 0.6492 score: 0.6735 time: 0.06s
Epoch 22/1000, LR 0.000270
Train loss: 0.2249;  Loss pred: 0.2249; Loss self: 0.0000; time: 0.20s
Val loss: 0.6506 score: 0.5714 time: 0.07s
Test loss: 0.6386 score: 0.6939 time: 0.06s
Epoch 23/1000, LR 0.000270
Train loss: 0.2050;  Loss pred: 0.2050; Loss self: 0.0000; time: 0.20s
Val loss: 0.6410 score: 0.6735 time: 0.07s
Test loss: 0.6264 score: 0.6939 time: 0.06s
Epoch 24/1000, LR 0.000270
Train loss: 0.2009;  Loss pred: 0.2009; Loss self: 0.0000; time: 0.20s
Val loss: 0.6328 score: 0.7347 time: 0.07s
Test loss: 0.6163 score: 0.6735 time: 0.07s
Epoch 25/1000, LR 0.000270
Train loss: 0.1946;  Loss pred: 0.1946; Loss self: 0.0000; time: 0.20s
Val loss: 0.6264 score: 0.7551 time: 0.07s
Test loss: 0.6079 score: 0.7143 time: 0.07s
Epoch 26/1000, LR 0.000270
Train loss: 0.1627;  Loss pred: 0.1627; Loss self: 0.0000; time: 0.20s
Val loss: 0.6215 score: 0.7551 time: 0.08s
Test loss: 0.6016 score: 0.6939 time: 0.06s
Epoch 27/1000, LR 0.000270
Train loss: 0.1426;  Loss pred: 0.1426; Loss self: 0.0000; time: 0.20s
Val loss: 0.6175 score: 0.7347 time: 0.07s
Test loss: 0.5967 score: 0.7143 time: 0.06s
Epoch 28/1000, LR 0.000270
Train loss: 0.1437;  Loss pred: 0.1437; Loss self: 0.0000; time: 0.20s
Val loss: 0.6121 score: 0.7551 time: 0.07s
Test loss: 0.5924 score: 0.6939 time: 0.06s
Epoch 29/1000, LR 0.000270
Train loss: 0.1409;  Loss pred: 0.1409; Loss self: 0.0000; time: 0.20s
Val loss: 0.6079 score: 0.6939 time: 0.08s
Test loss: 0.5894 score: 0.6939 time: 0.07s
Epoch 30/1000, LR 0.000270
Train loss: 0.1275;  Loss pred: 0.1275; Loss self: 0.0000; time: 0.20s
Val loss: 0.6018 score: 0.6531 time: 0.08s
Test loss: 0.5867 score: 0.6531 time: 0.06s
Epoch 31/1000, LR 0.000270
Train loss: 0.1255;  Loss pred: 0.1255; Loss self: 0.0000; time: 0.20s
Val loss: 0.5979 score: 0.6735 time: 0.07s
Test loss: 0.5853 score: 0.6327 time: 0.07s
Epoch 32/1000, LR 0.000270
Train loss: 0.1104;  Loss pred: 0.1104; Loss self: 0.0000; time: 0.20s
Val loss: 0.5967 score: 0.6735 time: 0.07s
Test loss: 0.5851 score: 0.6531 time: 0.06s
Epoch 33/1000, LR 0.000270
Train loss: 0.1134;  Loss pred: 0.1134; Loss self: 0.0000; time: 0.20s
Val loss: 0.5970 score: 0.6735 time: 0.07s
Test loss: 0.5853 score: 0.6327 time: 0.07s
     INFO: Early stopping counter 1 of 20
Epoch 34/1000, LR 0.000270
Train loss: 0.1130;  Loss pred: 0.1130; Loss self: 0.0000; time: 0.20s
Val loss: 0.5978 score: 0.6939 time: 0.07s
Test loss: 0.5853 score: 0.6327 time: 0.06s
     INFO: Early stopping counter 2 of 20
Epoch 35/1000, LR 0.000270
Train loss: 0.0930;  Loss pred: 0.0930; Loss self: 0.0000; time: 0.20s
Val loss: 0.5996 score: 0.6939 time: 0.07s
Test loss: 0.5861 score: 0.6122 time: 0.06s
     INFO: Early stopping counter 3 of 20
Epoch 36/1000, LR 0.000270
Train loss: 0.0877;  Loss pred: 0.0877; Loss self: 0.0000; time: 0.20s
Val loss: 0.6011 score: 0.6939 time: 0.08s
Test loss: 0.5862 score: 0.6122 time: 0.06s
     INFO: Early stopping counter 4 of 20
Epoch 37/1000, LR 0.000270
Train loss: 0.0834;  Loss pred: 0.0834; Loss self: 0.0000; time: 0.20s
Val loss: 0.6009 score: 0.6735 time: 0.07s
Test loss: 0.5847 score: 0.6327 time: 0.07s
     INFO: Early stopping counter 5 of 20
Epoch 38/1000, LR 0.000270
Train loss: 0.0904;  Loss pred: 0.0904; Loss self: 0.0000; time: 0.20s
Val loss: 0.5972 score: 0.6735 time: 0.07s
Test loss: 0.5818 score: 0.6327 time: 0.06s
     INFO: Early stopping counter 6 of 20
Epoch 39/1000, LR 0.000269
Train loss: 0.0647;  Loss pred: 0.0647; Loss self: 0.0000; time: 0.20s
Val loss: 0.5905 score: 0.6735 time: 0.08s
Test loss: 0.5780 score: 0.6327 time: 0.06s
Epoch 40/1000, LR 0.000269
Train loss: 0.0697;  Loss pred: 0.0697; Loss self: 0.0000; time: 0.20s
Val loss: 0.5817 score: 0.6735 time: 0.07s
Test loss: 0.5740 score: 0.6327 time: 0.06s
Epoch 41/1000, LR 0.000269
Train loss: 0.0703;  Loss pred: 0.0703; Loss self: 0.0000; time: 0.20s
Val loss: 0.5718 score: 0.6735 time: 0.07s
Test loss: 0.5705 score: 0.6327 time: 0.06s
Epoch 42/1000, LR 0.000269
Train loss: 0.0502;  Loss pred: 0.0502; Loss self: 0.0000; time: 0.20s
Val loss: 0.5626 score: 0.6327 time: 0.07s
Test loss: 0.5692 score: 0.6327 time: 0.06s
Epoch 43/1000, LR 0.000269
Train loss: 0.0627;  Loss pred: 0.0627; Loss self: 0.0000; time: 0.20s
Val loss: 0.5571 score: 0.6531 time: 0.07s
Test loss: 0.5678 score: 0.6327 time: 0.06s
Epoch 44/1000, LR 0.000269
Train loss: 0.0550;  Loss pred: 0.0550; Loss self: 0.0000; time: 0.20s
Val loss: 0.5527 score: 0.6531 time: 0.07s
Test loss: 0.5665 score: 0.6327 time: 0.07s
Epoch 45/1000, LR 0.000269
Train loss: 0.0494;  Loss pred: 0.0494; Loss self: 0.0000; time: 0.20s
Val loss: 0.5490 score: 0.6735 time: 0.07s
Test loss: 0.5648 score: 0.6327 time: 0.06s
Epoch 46/1000, LR 0.000269
Train loss: 0.0496;  Loss pred: 0.0496; Loss self: 0.0000; time: 0.20s
Val loss: 0.5488 score: 0.6939 time: 0.07s
Test loss: 0.5630 score: 0.6327 time: 0.06s
Epoch 47/1000, LR 0.000269
Train loss: 0.0392;  Loss pred: 0.0392; Loss self: 0.0000; time: 0.20s
Val loss: 0.5490 score: 0.6939 time: 0.07s
Test loss: 0.5620 score: 0.6327 time: 0.07s
     INFO: Early stopping counter 1 of 20
Epoch 48/1000, LR 0.000269
Train loss: 0.0387;  Loss pred: 0.0387; Loss self: 0.0000; time: 0.20s
Val loss: 0.5495 score: 0.6735 time: 0.07s
Test loss: 0.5600 score: 0.6531 time: 0.06s
     INFO: Early stopping counter 2 of 20
Epoch 49/1000, LR 0.000269
Train loss: 0.0391;  Loss pred: 0.0391; Loss self: 0.0000; time: 0.20s
Val loss: 0.5501 score: 0.6735 time: 0.07s
Test loss: 0.5553 score: 0.6531 time: 0.06s
     INFO: Early stopping counter 3 of 20
Epoch 50/1000, LR 0.000269
Train loss: 0.0350;  Loss pred: 0.0350; Loss self: 0.0000; time: 0.20s
Val loss: 0.5461 score: 0.6939 time: 0.08s
Test loss: 0.5494 score: 0.6531 time: 0.06s
Epoch 51/1000, LR 0.000269
Train loss: 0.0278;  Loss pred: 0.0278; Loss self: 0.0000; time: 0.20s
Val loss: 0.5374 score: 0.6939 time: 0.07s
Test loss: 0.5445 score: 0.6531 time: 0.07s
Epoch 52/1000, LR 0.000269
Train loss: 0.0298;  Loss pred: 0.0298; Loss self: 0.0000; time: 0.20s
Val loss: 0.5292 score: 0.6939 time: 0.08s
Test loss: 0.5413 score: 0.6531 time: 0.06s
Epoch 53/1000, LR 0.000269
Train loss: 0.0238;  Loss pred: 0.0238; Loss self: 0.0000; time: 0.20s
Val loss: 0.5212 score: 0.6939 time: 0.08s
Test loss: 0.5395 score: 0.6531 time: 0.06s
Epoch 54/1000, LR 0.000269
Train loss: 0.0233;  Loss pred: 0.0233; Loss self: 0.0000; time: 0.20s
Val loss: 0.5131 score: 0.7143 time: 0.07s
Test loss: 0.5415 score: 0.6531 time: 0.06s
Epoch 55/1000, LR 0.000269
Train loss: 0.0223;  Loss pred: 0.0223; Loss self: 0.0000; time: 0.20s
Val loss: 0.5077 score: 0.7347 time: 0.08s
Test loss: 0.5430 score: 0.6531 time: 0.06s
Epoch 56/1000, LR 0.000269
Train loss: 0.0182;  Loss pred: 0.0182; Loss self: 0.0000; time: 0.20s
Val loss: 0.5040 score: 0.7347 time: 0.08s
Test loss: 0.5449 score: 0.6735 time: 0.06s
Epoch 57/1000, LR 0.000269
Train loss: 0.0194;  Loss pred: 0.0194; Loss self: 0.0000; time: 0.20s
Val loss: 0.4999 score: 0.7347 time: 0.08s
Test loss: 0.5421 score: 0.6735 time: 0.06s
Epoch 58/1000, LR 0.000269
Train loss: 0.0179;  Loss pred: 0.0179; Loss self: 0.0000; time: 0.20s
Val loss: 0.4941 score: 0.7347 time: 0.08s
Test loss: 0.5360 score: 0.6939 time: 0.06s
Epoch 59/1000, LR 0.000268
Train loss: 0.0176;  Loss pred: 0.0176; Loss self: 0.0000; time: 0.20s
Val loss: 0.4891 score: 0.7347 time: 0.08s
Test loss: 0.5297 score: 0.7143 time: 0.06s
Epoch 60/1000, LR 0.000268
Train loss: 0.0148;  Loss pred: 0.0148; Loss self: 0.0000; time: 0.20s
Val loss: 0.4833 score: 0.7347 time: 0.07s
Test loss: 0.5233 score: 0.7143 time: 0.06s
Epoch 61/1000, LR 0.000268
Train loss: 0.0156;  Loss pred: 0.0156; Loss self: 0.0000; time: 0.20s
Val loss: 0.4771 score: 0.7551 time: 0.07s
Test loss: 0.5178 score: 0.7143 time: 0.06s
Epoch 62/1000, LR 0.000268
Train loss: 0.0136;  Loss pred: 0.0136; Loss self: 0.0000; time: 0.20s
Val loss: 0.4700 score: 0.7551 time: 0.07s
Test loss: 0.5108 score: 0.7347 time: 0.06s
Epoch 63/1000, LR 0.000268
Train loss: 0.0133;  Loss pred: 0.0133; Loss self: 0.0000; time: 0.20s
Val loss: 0.4581 score: 0.7551 time: 0.07s
Test loss: 0.4992 score: 0.7551 time: 0.06s
Epoch 64/1000, LR 0.000268
Train loss: 0.0126;  Loss pred: 0.0126; Loss self: 0.0000; time: 0.20s
Val loss: 0.4433 score: 0.7755 time: 0.07s
Test loss: 0.4862 score: 0.7551 time: 0.06s
Epoch 65/1000, LR 0.000268
Train loss: 0.0121;  Loss pred: 0.0121; Loss self: 0.0000; time: 0.20s
Val loss: 0.4255 score: 0.7755 time: 0.07s
Test loss: 0.4709 score: 0.7551 time: 0.06s
Epoch 66/1000, LR 0.000268
Train loss: 0.0111;  Loss pred: 0.0111; Loss self: 0.0000; time: 0.20s
Val loss: 0.4019 score: 0.7959 time: 0.07s
Test loss: 0.4518 score: 0.7755 time: 0.06s
Epoch 67/1000, LR 0.000268
Train loss: 0.0121;  Loss pred: 0.0121; Loss self: 0.0000; time: 0.20s
Val loss: 0.3765 score: 0.7959 time: 0.07s
Test loss: 0.4318 score: 0.7551 time: 0.06s
Epoch 68/1000, LR 0.000268
Train loss: 0.0102;  Loss pred: 0.0102; Loss self: 0.0000; time: 0.20s
Val loss: 0.3528 score: 0.8163 time: 0.07s
Test loss: 0.4129 score: 0.7755 time: 0.06s
Epoch 69/1000, LR 0.000268
Train loss: 0.0103;  Loss pred: 0.0103; Loss self: 0.0000; time: 0.20s
Val loss: 0.3329 score: 0.8367 time: 0.07s
Test loss: 0.3968 score: 0.8163 time: 0.06s
Epoch 70/1000, LR 0.000268
Train loss: 0.0104;  Loss pred: 0.0104; Loss self: 0.0000; time: 0.19s
Val loss: 0.3175 score: 0.8776 time: 0.08s
Test loss: 0.3847 score: 0.8163 time: 0.06s
Epoch 71/1000, LR 0.000268
Train loss: 0.0089;  Loss pred: 0.0089; Loss self: 0.0000; time: 0.20s
Val loss: 0.3039 score: 0.8980 time: 0.08s
Test loss: 0.3740 score: 0.8163 time: 0.06s
Epoch 72/1000, LR 0.000267
Train loss: 0.0075;  Loss pred: 0.0075; Loss self: 0.0000; time: 0.20s
Val loss: 0.2910 score: 0.9184 time: 0.07s
Test loss: 0.3649 score: 0.8163 time: 0.06s
Epoch 73/1000, LR 0.000267
Train loss: 0.0067;  Loss pred: 0.0067; Loss self: 0.0000; time: 0.20s
Val loss: 0.2804 score: 0.9388 time: 0.07s
Test loss: 0.3571 score: 0.8163 time: 0.06s
Epoch 74/1000, LR 0.000267
Train loss: 0.0071;  Loss pred: 0.0071; Loss self: 0.0000; time: 0.20s
Val loss: 0.2695 score: 0.9592 time: 0.07s
Test loss: 0.3475 score: 0.8367 time: 0.06s
Epoch 75/1000, LR 0.000267
Train loss: 0.0079;  Loss pred: 0.0079; Loss self: 0.0000; time: 0.20s
Val loss: 0.2577 score: 0.9388 time: 0.07s
Test loss: 0.3368 score: 0.8367 time: 0.06s
Epoch 76/1000, LR 0.000267
Train loss: 0.0081;  Loss pred: 0.0081; Loss self: 0.0000; time: 0.20s
Val loss: 0.2453 score: 0.9184 time: 0.07s
Test loss: 0.3242 score: 0.8571 time: 0.06s
Epoch 77/1000, LR 0.000267
Train loss: 0.0070;  Loss pred: 0.0070; Loss self: 0.0000; time: 0.20s
Val loss: 0.2346 score: 0.9388 time: 0.06s
Test loss: 0.3127 score: 0.8571 time: 0.06s
Epoch 78/1000, LR 0.000267
Train loss: 0.0065;  Loss pred: 0.0065; Loss self: 0.0000; time: 0.20s
Val loss: 0.2231 score: 0.9184 time: 0.07s
Test loss: 0.3017 score: 0.8571 time: 0.06s
Epoch 79/1000, LR 0.000267
Train loss: 0.0063;  Loss pred: 0.0063; Loss self: 0.0000; time: 0.20s
Val loss: 0.2152 score: 0.9184 time: 0.07s
Test loss: 0.2940 score: 0.8776 time: 0.07s
Epoch 80/1000, LR 0.000267
Train loss: 0.0061;  Loss pred: 0.0061; Loss self: 0.0000; time: 0.20s
Val loss: 0.2091 score: 0.9184 time: 0.07s
Test loss: 0.2886 score: 0.8980 time: 0.06s
Epoch 81/1000, LR 0.000267
Train loss: 0.0063;  Loss pred: 0.0063; Loss self: 0.0000; time: 0.20s
Val loss: 0.2047 score: 0.8980 time: 0.07s
Test loss: 0.2863 score: 0.8980 time: 0.07s
Epoch 82/1000, LR 0.000267
Train loss: 0.0056;  Loss pred: 0.0056; Loss self: 0.0000; time: 0.22s
Val loss: 0.2030 score: 0.8980 time: 0.08s
Test loss: 0.2854 score: 0.8980 time: 0.07s
Epoch 83/1000, LR 0.000266
Train loss: 0.0058;  Loss pred: 0.0058; Loss self: 0.0000; time: 0.22s
Val loss: 0.2001 score: 0.8980 time: 0.08s
Test loss: 0.2819 score: 0.9184 time: 0.07s
Epoch 84/1000, LR 0.000266
Train loss: 0.0049;  Loss pred: 0.0049; Loss self: 0.0000; time: 0.22s
Val loss: 0.1973 score: 0.8980 time: 0.07s
Test loss: 0.2799 score: 0.9184 time: 0.07s
Epoch 85/1000, LR 0.000266
Train loss: 0.0047;  Loss pred: 0.0047; Loss self: 0.0000; time: 0.22s
Val loss: 0.1931 score: 0.8980 time: 0.07s
Test loss: 0.2771 score: 0.9184 time: 0.07s
Epoch 86/1000, LR 0.000266
Train loss: 0.0046;  Loss pred: 0.0046; Loss self: 0.0000; time: 0.22s
Val loss: 0.1918 score: 0.9184 time: 0.07s
Test loss: 0.2757 score: 0.9184 time: 0.07s
Epoch 87/1000, LR 0.000266
Train loss: 0.0047;  Loss pred: 0.0047; Loss self: 0.0000; time: 0.21s
Val loss: 0.1879 score: 0.9184 time: 0.07s
Test loss: 0.2728 score: 0.9184 time: 0.07s
Epoch 88/1000, LR 0.000266
Train loss: 0.0048;  Loss pred: 0.0048; Loss self: 0.0000; time: 0.21s
Val loss: 0.1861 score: 0.9184 time: 0.07s
Test loss: 0.2715 score: 0.9388 time: 0.06s
Epoch 89/1000, LR 0.000266
Train loss: 0.0045;  Loss pred: 0.0045; Loss self: 0.0000; time: 0.21s
Val loss: 0.1871 score: 0.8980 time: 0.07s
Test loss: 0.2707 score: 0.9388 time: 0.06s
     INFO: Early stopping counter 1 of 20
Epoch 90/1000, LR 0.000266
Train loss: 0.0039;  Loss pred: 0.0039; Loss self: 0.0000; time: 0.20s
Val loss: 0.1907 score: 0.8980 time: 0.07s
Test loss: 0.2696 score: 0.9388 time: 0.06s
     INFO: Early stopping counter 2 of 20
Epoch 91/1000, LR 0.000266
Train loss: 0.0048;  Loss pred: 0.0048; Loss self: 0.0000; time: 0.20s
Val loss: 0.1921 score: 0.8980 time: 0.07s
Test loss: 0.2688 score: 0.9388 time: 0.06s
     INFO: Early stopping counter 3 of 20
Epoch 92/1000, LR 0.000266
Train loss: 0.0043;  Loss pred: 0.0043; Loss self: 0.0000; time: 0.20s
Val loss: 0.1907 score: 0.8980 time: 0.07s
Test loss: 0.2683 score: 0.9388 time: 0.06s
     INFO: Early stopping counter 4 of 20
Epoch 93/1000, LR 0.000265
Train loss: 0.0034;  Loss pred: 0.0034; Loss self: 0.0000; time: 0.20s
Val loss: 0.1907 score: 0.9184 time: 0.07s
Test loss: 0.2687 score: 0.9388 time: 0.06s
     INFO: Early stopping counter 5 of 20
Epoch 94/1000, LR 0.000265
Train loss: 0.0030;  Loss pred: 0.0030; Loss self: 0.0000; time: 0.20s
Val loss: 0.1925 score: 0.9184 time: 0.07s
Test loss: 0.2704 score: 0.9388 time: 0.06s
     INFO: Early stopping counter 6 of 20
Epoch 95/1000, LR 0.000265
Train loss: 0.0034;  Loss pred: 0.0034; Loss self: 0.0000; time: 0.20s
Val loss: 0.1949 score: 0.9184 time: 0.07s
Test loss: 0.2732 score: 0.9388 time: 0.06s
     INFO: Early stopping counter 7 of 20
Epoch 96/1000, LR 0.000265
Train loss: 0.0043;  Loss pred: 0.0043; Loss self: 0.0000; time: 0.20s
Val loss: 0.1998 score: 0.9184 time: 0.07s
Test loss: 0.2774 score: 0.9388 time: 0.07s
     INFO: Early stopping counter 8 of 20
Epoch 97/1000, LR 0.000265
Train loss: 0.0034;  Loss pred: 0.0034; Loss self: 0.0000; time: 0.21s
Val loss: 0.2036 score: 0.9184 time: 0.07s
Test loss: 0.2807 score: 0.9388 time: 0.06s
     INFO: Early stopping counter 9 of 20
Epoch 98/1000, LR 0.000265
Train loss: 0.0027;  Loss pred: 0.0027; Loss self: 0.0000; time: 0.20s
Val loss: 0.2061 score: 0.9184 time: 0.07s
Test loss: 0.2825 score: 0.9388 time: 0.06s
     INFO: Early stopping counter 10 of 20
Epoch 99/1000, LR 0.000265
Train loss: 0.0035;  Loss pred: 0.0035; Loss self: 0.0000; time: 0.20s
Val loss: 0.2105 score: 0.9184 time: 0.07s
Test loss: 0.2858 score: 0.9388 time: 0.06s
     INFO: Early stopping counter 11 of 20
Epoch 100/1000, LR 0.000265
Train loss: 0.0032;  Loss pred: 0.0032; Loss self: 0.0000; time: 0.20s
Val loss: 0.2133 score: 0.9184 time: 0.07s
Test loss: 0.2897 score: 0.9388 time: 0.06s
     INFO: Early stopping counter 12 of 20
Epoch 101/1000, LR 0.000265
Train loss: 0.0027;  Loss pred: 0.0027; Loss self: 0.0000; time: 0.21s
Val loss: 0.2161 score: 0.9184 time: 0.07s
Test loss: 0.2941 score: 0.9388 time: 0.07s
     INFO: Early stopping counter 13 of 20
Epoch 102/1000, LR 0.000264
Train loss: 0.0028;  Loss pred: 0.0028; Loss self: 0.0000; time: 0.21s
Val loss: 0.2175 score: 0.9184 time: 0.07s
Test loss: 0.2969 score: 0.9184 time: 0.07s
     INFO: Early stopping counter 14 of 20
Epoch 103/1000, LR 0.000264
Train loss: 0.0032;  Loss pred: 0.0032; Loss self: 0.0000; time: 0.20s
Val loss: 0.2185 score: 0.9184 time: 0.07s
Test loss: 0.2995 score: 0.9388 time: 0.06s
     INFO: Early stopping counter 15 of 20
Epoch 104/1000, LR 0.000264
Train loss: 0.0031;  Loss pred: 0.0031; Loss self: 0.0000; time: 0.22s
Val loss: 0.2201 score: 0.9184 time: 0.07s
Test loss: 0.3022 score: 0.9388 time: 0.07s
     INFO: Early stopping counter 16 of 20
Epoch 105/1000, LR 0.000264
Train loss: 0.0026;  Loss pred: 0.0026; Loss self: 0.0000; time: 0.20s
Val loss: 0.2223 score: 0.9184 time: 0.07s
Test loss: 0.3052 score: 0.9388 time: 0.06s
     INFO: Early stopping counter 17 of 20
Epoch 106/1000, LR 0.000264
Train loss: 0.0029;  Loss pred: 0.0029; Loss self: 0.0000; time: 0.20s
Val loss: 0.2270 score: 0.9184 time: 0.07s
Test loss: 0.3112 score: 0.9184 time: 0.07s
     INFO: Early stopping counter 18 of 20
Epoch 107/1000, LR 0.000264
Train loss: 0.0021;  Loss pred: 0.0021; Loss self: 0.0000; time: 0.20s
Val loss: 0.2316 score: 0.9184 time: 0.07s
Test loss: 0.3169 score: 0.9184 time: 0.06s
     INFO: Early stopping counter 19 of 20
Epoch 108/1000, LR 0.000264
Train loss: 0.0034;  Loss pred: 0.0034; Loss self: 0.0000; time: 0.20s
Val loss: 0.2403 score: 0.9184 time: 0.07s
Test loss: 0.3284 score: 0.9184 time: 0.06s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 087,   Train_Loss: 0.0048,   Val_Loss: 0.1861,   Val_Precision: 0.9545,   Val_Recall: 0.8750,   Val_accuracy: 0.9130,   Val_Score: 0.9184,   Val_Loss: 0.1861,   Test_Precision: 0.9231,   Test_Recall: 0.9600,   Test_accuracy: 0.9412,   Test_Score: 0.9388,   Test_loss: 0.2715


[0.0644826750503853, 0.06443862803280354, 0.0645348159596324, 0.07019063096959144, 0.06461871904321015, 0.06437401799485087, 0.06433912296779454, 0.07102904794737697, 0.0648910899180919, 0.0648136610398069, 0.06393573095556349, 0.06425134197343141, 0.06443162995856255, 0.07163019699510187, 0.06446594896260649, 0.06361221603583544, 0.06384435202926397, 0.06382109900005162, 0.07124983798712492, 0.0643562659388408, 0.0647353840759024, 0.06404850899707526, 0.06429420795757324, 0.07065497699659318, 0.07036611798685044, 0.06428111298009753, 0.0647413789993152, 0.06481685803737491, 0.07463502895552665, 0.06443383800797164, 0.06996510305907577, 0.06387466401793063, 0.07087652490008622, 0.06435540597885847, 0.06477649207226932, 0.06457676296122372, 0.0703419370111078, 0.06440019397996366, 0.06466191494837403, 0.06438850006088614, 0.0641920460620895, 0.06377141596749425, 0.06392341898754239, 0.07059295708313584, 0.06391120993066579, 0.06392881297506392, 0.07209730101749301, 0.06454430799931288, 0.06395665893796831, 0.06404085201211274, 0.07002345798537135, 0.06408313394058496, 0.06472722592297941, 0.06459272396750748, 0.06417917599901557, 0.06432376289740205, 0.06449272600002587, 0.0650650889147073, 0.0667443519923836, 0.06476360093802214, 0.06418451108038425, 0.06468058994505554, 0.0641525499522686, 0.06411690602544695, 0.06386055506300181, 0.064334562048316, 0.06383313296828419, 0.06396952993236482, 0.06894486001692712, 0.06414954492356628, 0.06386131094768643, 0.06418418302200735, 0.06362403905950487, 0.066609677975066, 0.06407150195445865, 0.0642807160038501, 0.06394871894735843, 0.06920175999403, 0.06955765199381858, 0.06403393601067364, 0.07042789901606739, 0.07088424207177013, 0.07081668300088495, 0.0704386419383809, 0.07570642000064254, 0.07054653007071465, 0.07081792107783258, 0.06726047792471945, 0.06716600002255291, 0.06700066605117172, 0.06723369401879609, 0.06688065291382372, 0.06716732401400805, 0.06709835398942232, 0.06756854394916445, 0.0720388978952542, 0.06745747302193195, 0.06793466000817716, 0.06738519598729908, 0.06728586996905506, 0.07186640799045563, 0.0713276609312743, 0.06769404502119869, 0.07209854200482368, 0.06747251597698778, 0.07182020496111363, 0.06682751793414354, 0.06721933896187693]
[0.0013159729602119448, 0.0013150740414857864, 0.001317037060400661, 0.0014324618565222742, 0.0013187493682287786, 0.0013137554692826709, 0.0013130433258733579, 0.0014495724070893259, 0.0013243079575120794, 0.0013227277763225899, 0.0013048108358278262, 0.0013112518770088042, 0.0013149312236441337, 0.001461840755002079, 0.001315631611481765, 0.001298208490527254, 0.0013029459597808974, 0.0013024714081643187, 0.0014540783262678555, 0.0013133931824253226, 0.0013211302872633143, 0.00130711242851174, 0.0013121266930116986, 0.001441938306052922, 0.0014360432242214376, 0.001311859448573419, 0.0013212526326390858, 0.0013227930211709166, 0.0015231638562352378, 0.0013149762858769723, 0.0014278592461035872, 0.0013035645717945025, 0.0014464596918384945, 0.0013133756322216015, 0.00132196922596468, 0.001317893121657627, 0.0014355497349205675, 0.001314289673060483, 0.0013196309173137558, 0.0013140510216507377, 0.0013100417563691735, 0.0013014574687243725, 0.0013045595711743344, 0.0014406725935333846, 0.0013043104067482815, 0.001304669652552325, 0.0014713734901529185, 0.0013172307754961811, 0.0013052379375095575, 0.001306956163512505, 0.0014290501629667623, 0.001307819060011938, 0.0013209637943465188, 0.0013182188564797445, 0.001309779102020726, 0.0013127298550490215, 0.001316178081633181, 0.0013278589574430063, 0.0013621296324976245, 0.0013217061415922884, 0.0013098879812323317, 0.0013200120396950111, 0.0013092357133116042, 0.0013085082862336114, 0.0013032766339388124, 0.001312950245884, 0.0013027169993527386, 0.001305500610864588, 0.0014070379595291249, 0.0013091743861952303, 0.001303292060156866, 0.0013098812861634152, 0.001298449776724589, 0.0013593811831646124, 0.0013075816725399724, 0.001311851347017349, 0.001305075896884866, 0.0014122808162046938, 0.0014195439182411954, 0.0013068150206259927, 0.0014373040615523957, 0.001446617185138166, 0.0014452384285894887, 0.0014375233048649163, 0.0015450289796049498, 0.0014397251034839725, 0.001445263695465971, 0.001372662814790193, 0.0013707346943378144, 0.0013673605316565658, 0.0013721162044652263, 0.0013649112839555862, 0.0013707617145715927, 0.0013693541630494352, 0.0013789498765135603, 0.0014701815896990653, 0.0013766831228965704, 0.0013864216328199422, 0.0013752080813734507, 0.001373181019776634, 0.0014666613875603189, 0.0014556665496178428, 0.001381511122881606, 0.0014713988164249733, 0.0013769901219793425, 0.0014657184685941559, 0.0013638268966151743, 0.0013718232441199373]
[759.8940329586593, 760.4134584469389, 759.2800765194762, 698.0988676569696, 758.2942021372165, 761.1766598741654, 761.589492361084, 689.8586059650195, 755.1113729458042, 756.0134578712572, 766.3946164008964, 762.6299855380764, 760.4960487809018, 684.0690386953795, 760.0911921489355, 770.2922968820365, 767.4915390720881, 767.7711723510178, 687.7208620299524, 761.3866231233147, 756.9276169358534, 765.0451316866339, 762.1215278417358, 693.5109468985131, 696.3578694103431, 762.2767828424376, 756.857526938348, 755.97616860332, 656.5281836923787, 760.469987740569, 700.3491434669414, 767.1273227557788, 691.3431502048768, 761.3967972806681, 756.4472609188542, 758.786872445479, 696.5972516830512, 760.8672734005272, 757.7876411349945, 761.0054583297532, 763.3344472710053, 768.3693274895514, 766.5422278108948, 694.1202355681705, 766.6886615533919, 766.4775508832448, 679.6370919365081, 759.1684149827997, 766.1438357423453, 765.1366035969056, 699.7654987309628, 764.6317679380448, 757.0230193134857, 758.5993745154454, 763.4875212600361, 761.7713546727075, 759.7756063215617, 753.09203164593, 734.1445161620798, 756.5978310392642, 763.4240594063688, 757.5688478046383, 763.8044011727898, 764.2290159876508, 767.2968071082206, 761.6434843094204, 767.6264303734848, 765.9896837104767, 710.7128796544032, 763.8401809145045, 767.2877251163785, 763.4279614215698, 770.1491562673721, 735.6288378746128, 764.7705845077377, 762.2814904094275, 766.2389615706926, 708.0744767795965, 704.4516109364145, 765.2192423691139, 695.7470077138204, 691.2678836346676, 691.9273527593446, 695.6408961272247, 647.2370506964155, 694.5770394501789, 691.9152561135825, 728.5110292383398, 729.5357767850825, 731.3360133252448, 728.8012463855011, 732.6483499366686, 729.5213962935435, 730.2712672761626, 725.1895206868066, 680.1880849322105, 726.3835688607694, 721.2813016816742, 727.1626843563033, 728.2361069647345, 681.8206359570322, 686.9705155088785, 723.8450588180309, 679.6253937662387, 726.2216221003523, 682.2592615341405, 733.23088324615, 728.9568858716399]
Elapsed: 0.06653271100798587~0.0029895313763444284
Time per graph: 0.001357810428734406~6.101084441519242e-05
Speed: 737.910512612224~31.86295361582065
Total Time: 0.0678
best val loss: 0.18606330454349518 test_score: 0.9388

Testing...
Test loss: 0.3475 score: 0.8367 time: 0.06s
test Score 0.8367
Epoch Time List: [0.33245348010677844, 0.32852764893323183, 0.3370728971203789, 0.32968225597869605, 0.3295571719063446, 0.33190234599169344, 0.3346420080633834, 0.33518304605968297, 0.3329746489180252, 0.33122653095051646, 0.31995276489760727, 0.33186898205894977, 0.3241046400507912, 0.3463307870551944, 0.33102158398833126, 0.32088763802312315, 0.319689805037342, 0.3217936120927334, 0.33466355106793344, 0.3317127062473446, 0.3241832018829882, 0.32209669798612595, 0.32162193313706666, 0.3292630580253899, 0.3305535651743412, 0.33248257101513445, 0.32576106302440166, 0.33000474399887025, 0.348557694000192, 0.33419915684498847, 0.3353538030060008, 0.32373901398386806, 0.3349831009982154, 0.32823484402615577, 0.3268913999199867, 0.3338019169168547, 0.33388584095519036, 0.3294920389307663, 0.33230421401094645, 0.3235986439976841, 0.3236075059976429, 0.32239997293800116, 0.32127891294658184, 0.33667722798418254, 0.32756817096378654, 0.32575596193782985, 0.33604643191210926, 0.32994258299004287, 0.322193932835944, 0.33105809800326824, 0.33116249507293105, 0.33176720002666116, 0.33220769208855927, 0.3355562739307061, 0.3320715018780902, 0.3313729028450325, 0.333300246973522, 0.33325597806833684, 0.340387150994502, 0.3337919219629839, 0.3307352770352736, 0.32446340401656926, 0.32435808097943664, 0.32574275496881455, 0.3253884450532496, 0.32874444511253387, 0.3275584459770471, 0.32729491498321295, 0.3261925739934668, 0.32862572197336704, 0.3319753339746967, 0.3285865270299837, 0.32053236302454025, 0.33021121891215444, 0.3275437589036301, 0.3216882279375568, 0.3210726941470057, 0.3320434900233522, 0.3336182909552008, 0.330510267871432, 0.3387459389632568, 0.36026847397442907, 0.3601068068528548, 0.35521745099686086, 0.3603520850883797, 0.35541727906093, 0.3530896690208465, 0.3435584349790588, 0.34299308713525534, 0.334249958046712, 0.3384693389525637, 0.33899327693507075, 0.3382542791077867, 0.334890631143935, 0.3357042360585183, 0.33859612909145653, 0.33672224590554833, 0.33966553397476673, 0.33610803296323866, 0.339433571905829, 0.3417611379409209, 0.3418157041305676, 0.34223654493689537, 0.35153683507815003, 0.33687545091379434, 0.3338408200070262, 0.3285765520995483, 0.3295315249124542]
Total Epoch List: [108]
Total Time List: [0.06779721495695412]
========================k_idx:1========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: GPSPerformer
GPS(
  (x_lin): Linear(in_features=14887, out_features=64, bias=True)
  (pe_lin): Linear(in_features=20, out_features=8, bias=True)
  (pe_norm): BatchNorm1d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (convs): ModuleList(
    (0-1): 2 x GPSConv(64, conv=GINEConv(nn=Sequential(
      (0): Linear(in_features=64, out_features=64, bias=True)
      (1): ReLU()
      (2): Linear(in_features=64, out_features=64, bias=True)
    )), heads=1, attn_type=performer)
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=32, bias=True)
    (1): ReLU()
    (2): Linear(in_features=32, out_features=16, bias=True)
    (3): ReLU()
    (4): Linear(in_features=16, out_features=2, bias=True)
  )
)
Total number of parameters: 1039522
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7c9d24c92650>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.6419;  Loss pred: 0.6419; Loss self: 0.0000; time: 0.20s
Val loss: 0.9168 score: 0.5714 time: 0.06s
Test loss: 0.8253 score: 0.6122 time: 0.07s
Epoch 2/1000, LR 0.000000
Train loss: 0.5894;  Loss pred: 0.5894; Loss self: 0.0000; time: 0.20s
Val loss: 0.7913 score: 0.5714 time: 0.06s
Test loss: 0.6894 score: 0.6122 time: 0.08s
Epoch 3/1000, LR 0.000030
Train loss: 0.5775;  Loss pred: 0.5775; Loss self: 0.0000; time: 0.20s
Val loss: 0.7211 score: 0.6122 time: 0.07s
Test loss: 0.6162 score: 0.6531 time: 0.07s
Epoch 4/1000, LR 0.000060
Train loss: 0.5745;  Loss pred: 0.5745; Loss self: 0.0000; time: 0.21s
Val loss: 0.6824 score: 0.6122 time: 0.06s
Test loss: 0.5780 score: 0.6735 time: 0.08s
Epoch 5/1000, LR 0.000090
Train loss: 0.5927;  Loss pred: 0.5927; Loss self: 0.0000; time: 0.20s
Val loss: 0.6582 score: 0.5918 time: 0.07s
Test loss: 0.5549 score: 0.6735 time: 0.07s
Epoch 6/1000, LR 0.000120
Train loss: 0.4814;  Loss pred: 0.4814; Loss self: 0.0000; time: 0.20s
Val loss: 0.6462 score: 0.5918 time: 0.06s
Test loss: 0.5425 score: 0.6735 time: 0.07s
Epoch 7/1000, LR 0.000150
Train loss: 0.4772;  Loss pred: 0.4772; Loss self: 0.0000; time: 0.20s
Val loss: 0.6372 score: 0.5918 time: 0.06s
Test loss: 0.5339 score: 0.6939 time: 0.07s
Epoch 8/1000, LR 0.000180
Train loss: 0.4298;  Loss pred: 0.4298; Loss self: 0.0000; time: 0.20s
Val loss: 0.6317 score: 0.5918 time: 0.06s
Test loss: 0.5258 score: 0.6939 time: 0.07s
Epoch 9/1000, LR 0.000210
Train loss: 0.4306;  Loss pred: 0.4306; Loss self: 0.0000; time: 0.20s
Val loss: 0.6261 score: 0.5918 time: 0.06s
Test loss: 0.5183 score: 0.7143 time: 0.07s
Epoch 10/1000, LR 0.000240
Train loss: 0.3929;  Loss pred: 0.3929; Loss self: 0.0000; time: 0.20s
Val loss: 0.6225 score: 0.5918 time: 0.06s
Test loss: 0.5151 score: 0.7551 time: 0.07s
Epoch 11/1000, LR 0.000270
Train loss: 0.4097;  Loss pred: 0.4097; Loss self: 0.0000; time: 0.20s
Val loss: 0.6218 score: 0.5918 time: 0.06s
Test loss: 0.5171 score: 0.7347 time: 0.07s
Epoch 12/1000, LR 0.000270
Train loss: 0.4051;  Loss pred: 0.4051; Loss self: 0.0000; time: 0.20s
Val loss: 0.6266 score: 0.5918 time: 0.07s
Test loss: 0.5200 score: 0.6939 time: 0.07s
     INFO: Early stopping counter 1 of 20
Epoch 13/1000, LR 0.000270
Train loss: 0.3396;  Loss pred: 0.3396; Loss self: 0.0000; time: 0.20s
Val loss: 0.6292 score: 0.5918 time: 0.06s
Test loss: 0.5201 score: 0.6939 time: 0.07s
     INFO: Early stopping counter 2 of 20
Epoch 14/1000, LR 0.000270
Train loss: 0.3349;  Loss pred: 0.3349; Loss self: 0.0000; time: 0.20s
Val loss: 0.6301 score: 0.5918 time: 0.06s
Test loss: 0.5198 score: 0.6939 time: 0.07s
     INFO: Early stopping counter 3 of 20
Epoch 15/1000, LR 0.000270
Train loss: 0.2851;  Loss pred: 0.2851; Loss self: 0.0000; time: 0.20s
Val loss: 0.6282 score: 0.5918 time: 0.06s
Test loss: 0.5176 score: 0.6939 time: 0.08s
     INFO: Early stopping counter 4 of 20
Epoch 16/1000, LR 0.000270
Train loss: 0.2779;  Loss pred: 0.2779; Loss self: 0.0000; time: 0.20s
Val loss: 0.6245 score: 0.5918 time: 0.06s
Test loss: 0.5134 score: 0.7347 time: 0.07s
     INFO: Early stopping counter 5 of 20
Epoch 17/1000, LR 0.000270
Train loss: 0.2623;  Loss pred: 0.2623; Loss self: 0.0000; time: 0.20s
Val loss: 0.6209 score: 0.5918 time: 0.06s
Test loss: 0.5093 score: 0.7551 time: 0.07s
Epoch 18/1000, LR 0.000270
Train loss: 0.2765;  Loss pred: 0.2765; Loss self: 0.0000; time: 0.20s
Val loss: 0.6171 score: 0.5918 time: 0.06s
Test loss: 0.5045 score: 0.7551 time: 0.07s
Epoch 19/1000, LR 0.000270
Train loss: 0.2205;  Loss pred: 0.2205; Loss self: 0.0000; time: 0.20s
Val loss: 0.6132 score: 0.5918 time: 0.07s
Test loss: 0.4997 score: 0.7551 time: 0.08s
Epoch 20/1000, LR 0.000270
Train loss: 0.2014;  Loss pred: 0.2014; Loss self: 0.0000; time: 0.20s
Val loss: 0.6099 score: 0.5918 time: 0.06s
Test loss: 0.4960 score: 0.7551 time: 0.07s
Epoch 21/1000, LR 0.000270
Train loss: 0.1756;  Loss pred: 0.1756; Loss self: 0.0000; time: 0.20s
Val loss: 0.6064 score: 0.6122 time: 0.06s
Test loss: 0.4935 score: 0.7551 time: 0.07s
Epoch 22/1000, LR 0.000270
Train loss: 0.1727;  Loss pred: 0.1727; Loss self: 0.0000; time: 0.20s
Val loss: 0.6031 score: 0.6122 time: 0.06s
Test loss: 0.4911 score: 0.7755 time: 0.07s
Epoch 23/1000, LR 0.000270
Train loss: 0.1495;  Loss pred: 0.1495; Loss self: 0.0000; time: 0.20s
Val loss: 0.5992 score: 0.6531 time: 0.06s
Test loss: 0.4888 score: 0.7755 time: 0.07s
Epoch 24/1000, LR 0.000270
Train loss: 0.1334;  Loss pred: 0.1334; Loss self: 0.0000; time: 0.20s
Val loss: 0.5971 score: 0.6531 time: 0.06s
Test loss: 0.4875 score: 0.7755 time: 0.07s
Epoch 25/1000, LR 0.000270
Train loss: 0.1144;  Loss pred: 0.1144; Loss self: 0.0000; time: 0.21s
Val loss: 0.5959 score: 0.6531 time: 0.08s
Test loss: 0.4862 score: 0.7755 time: 0.07s
Epoch 26/1000, LR 0.000270
Train loss: 0.0911;  Loss pred: 0.0911; Loss self: 0.0000; time: 0.20s
Val loss: 0.5956 score: 0.6531 time: 0.07s
Test loss: 0.4861 score: 0.7755 time: 0.07s
Epoch 27/1000, LR 0.000270
Train loss: 0.0963;  Loss pred: 0.0963; Loss self: 0.0000; time: 0.20s
Val loss: 0.5965 score: 0.6531 time: 0.07s
Test loss: 0.4876 score: 0.7755 time: 0.07s
     INFO: Early stopping counter 1 of 20
Epoch 28/1000, LR 0.000270
Train loss: 0.0746;  Loss pred: 0.0746; Loss self: 0.0000; time: 0.20s
Val loss: 0.5984 score: 0.6531 time: 0.06s
Test loss: 0.4909 score: 0.7755 time: 0.08s
     INFO: Early stopping counter 2 of 20
Epoch 29/1000, LR 0.000270
Train loss: 0.0632;  Loss pred: 0.0632; Loss self: 0.0000; time: 0.20s
Val loss: 0.6020 score: 0.6122 time: 0.07s
Test loss: 0.4972 score: 0.7755 time: 0.08s
     INFO: Early stopping counter 3 of 20
Epoch 30/1000, LR 0.000270
Train loss: 0.0690;  Loss pred: 0.0690; Loss self: 0.0000; time: 0.22s
Val loss: 0.6060 score: 0.5918 time: 0.09s
Test loss: 0.5039 score: 0.7551 time: 0.07s
     INFO: Early stopping counter 4 of 20
Epoch 31/1000, LR 0.000270
Train loss: 0.0494;  Loss pred: 0.0494; Loss self: 0.0000; time: 0.20s
Val loss: 0.6089 score: 0.5510 time: 0.06s
Test loss: 0.5105 score: 0.7143 time: 0.08s
     INFO: Early stopping counter 5 of 20
Epoch 32/1000, LR 0.000270
Train loss: 0.0487;  Loss pred: 0.0487; Loss self: 0.0000; time: 0.20s
Val loss: 0.6077 score: 0.5714 time: 0.06s
Test loss: 0.5132 score: 0.7143 time: 0.08s
     INFO: Early stopping counter 6 of 20
Epoch 33/1000, LR 0.000270
Train loss: 0.0374;  Loss pred: 0.0374; Loss self: 0.0000; time: 0.20s
Val loss: 0.6066 score: 0.5918 time: 0.07s
Test loss: 0.5156 score: 0.6939 time: 0.08s
     INFO: Early stopping counter 7 of 20
Epoch 34/1000, LR 0.000270
Train loss: 0.0340;  Loss pred: 0.0340; Loss self: 0.0000; time: 0.21s
Val loss: 0.6033 score: 0.5918 time: 0.07s
Test loss: 0.5148 score: 0.6735 time: 0.07s
     INFO: Early stopping counter 8 of 20
Epoch 35/1000, LR 0.000270
Train loss: 0.0241;  Loss pred: 0.0241; Loss self: 0.0000; time: 0.21s
Val loss: 0.6010 score: 0.6122 time: 0.08s
Test loss: 0.5135 score: 0.6735 time: 0.07s
     INFO: Early stopping counter 9 of 20
Epoch 36/1000, LR 0.000270
Train loss: 0.0329;  Loss pred: 0.0329; Loss self: 0.0000; time: 0.20s
Val loss: 0.6042 score: 0.6327 time: 0.06s
Test loss: 0.5152 score: 0.6531 time: 0.07s
     INFO: Early stopping counter 10 of 20
Epoch 37/1000, LR 0.000270
Train loss: 0.0230;  Loss pred: 0.0230; Loss self: 0.0000; time: 0.21s
Val loss: 0.6102 score: 0.6327 time: 0.06s
Test loss: 0.5181 score: 0.6122 time: 0.08s
     INFO: Early stopping counter 11 of 20
Epoch 38/1000, LR 0.000270
Train loss: 0.0219;  Loss pred: 0.0219; Loss self: 0.0000; time: 0.20s
Val loss: 0.6209 score: 0.6327 time: 0.06s
Test loss: 0.5278 score: 0.5918 time: 0.08s
     INFO: Early stopping counter 12 of 20
Epoch 39/1000, LR 0.000269
Train loss: 0.0160;  Loss pred: 0.0160; Loss self: 0.0000; time: 0.21s
Val loss: 0.6336 score: 0.6327 time: 0.06s
Test loss: 0.5413 score: 0.5714 time: 0.07s
     INFO: Early stopping counter 13 of 20
Epoch 40/1000, LR 0.000269
Train loss: 0.0169;  Loss pred: 0.0169; Loss self: 0.0000; time: 0.20s
Val loss: 0.6494 score: 0.6122 time: 0.15s
Test loss: 0.5613 score: 0.5714 time: 0.07s
     INFO: Early stopping counter 14 of 20
Epoch 41/1000, LR 0.000269
Train loss: 0.0112;  Loss pred: 0.0112; Loss self: 0.0000; time: 0.20s
Val loss: 0.6626 score: 0.6122 time: 0.06s
Test loss: 0.5788 score: 0.5510 time: 0.07s
     INFO: Early stopping counter 15 of 20
Epoch 42/1000, LR 0.000269
Train loss: 0.0108;  Loss pred: 0.0108; Loss self: 0.0000; time: 0.20s
Val loss: 0.6658 score: 0.6122 time: 0.07s
Test loss: 0.5897 score: 0.5510 time: 0.08s
     INFO: Early stopping counter 16 of 20
Epoch 43/1000, LR 0.000269
Train loss: 0.0129;  Loss pred: 0.0129; Loss self: 0.0000; time: 0.20s
Val loss: 0.6517 score: 0.6327 time: 0.07s
Test loss: 0.5720 score: 0.5714 time: 0.07s
     INFO: Early stopping counter 17 of 20
Epoch 44/1000, LR 0.000269
Train loss: 0.0114;  Loss pred: 0.0114; Loss self: 0.0000; time: 0.20s
Val loss: 0.6342 score: 0.6531 time: 0.06s
Test loss: 0.5496 score: 0.6327 time: 0.07s
     INFO: Early stopping counter 18 of 20
Epoch 45/1000, LR 0.000269
Train loss: 0.0083;  Loss pred: 0.0083; Loss self: 0.0000; time: 0.21s
Val loss: 0.6096 score: 0.6735 time: 0.06s
Test loss: 0.5213 score: 0.6939 time: 0.07s
     INFO: Early stopping counter 19 of 20
Epoch 46/1000, LR 0.000269
Train loss: 0.0054;  Loss pred: 0.0054; Loss self: 0.0000; time: 0.21s
Val loss: 0.5873 score: 0.6735 time: 0.06s
Test loss: 0.4965 score: 0.7143 time: 0.07s
Epoch 47/1000, LR 0.000269
Train loss: 0.0092;  Loss pred: 0.0092; Loss self: 0.0000; time: 0.20s
Val loss: 0.5685 score: 0.6939 time: 0.07s
Test loss: 0.4759 score: 0.7347 time: 0.08s
Epoch 48/1000, LR 0.000269
Train loss: 0.0061;  Loss pred: 0.0061; Loss self: 0.0000; time: 0.21s
Val loss: 0.5496 score: 0.6735 time: 0.08s
Test loss: 0.4548 score: 0.7551 time: 0.08s
Epoch 49/1000, LR 0.000269
Train loss: 0.0054;  Loss pred: 0.0054; Loss self: 0.0000; time: 0.20s
Val loss: 0.5333 score: 0.6735 time: 0.06s
Test loss: 0.4366 score: 0.7755 time: 0.08s
Epoch 50/1000, LR 0.000269
Train loss: 0.0053;  Loss pred: 0.0053; Loss self: 0.0000; time: 0.20s
Val loss: 0.5174 score: 0.6735 time: 0.06s
Test loss: 0.4195 score: 0.7959 time: 0.08s
Epoch 51/1000, LR 0.000269
Train loss: 0.0042;  Loss pred: 0.0042; Loss self: 0.0000; time: 0.20s
Val loss: 0.5050 score: 0.6939 time: 0.07s
Test loss: 0.4054 score: 0.8163 time: 0.08s
Epoch 52/1000, LR 0.000269
Train loss: 0.0043;  Loss pred: 0.0043; Loss self: 0.0000; time: 0.22s
Val loss: 0.4966 score: 0.6939 time: 0.07s
Test loss: 0.3948 score: 0.8367 time: 0.08s
Epoch 53/1000, LR 0.000269
Train loss: 0.0055;  Loss pred: 0.0055; Loss self: 0.0000; time: 0.22s
Val loss: 0.4903 score: 0.6939 time: 0.07s
Test loss: 0.3865 score: 0.8367 time: 0.08s
Epoch 54/1000, LR 0.000269
Train loss: 0.0043;  Loss pred: 0.0043; Loss self: 0.0000; time: 0.22s
Val loss: 0.4858 score: 0.7143 time: 0.07s
Test loss: 0.3806 score: 0.8367 time: 0.08s
Epoch 55/1000, LR 0.000269
Train loss: 0.0033;  Loss pred: 0.0033; Loss self: 0.0000; time: 0.22s
Val loss: 0.4815 score: 0.7347 time: 0.07s
Test loss: 0.3751 score: 0.8367 time: 0.08s
Epoch 56/1000, LR 0.000269
Train loss: 0.0026;  Loss pred: 0.0026; Loss self: 0.0000; time: 0.22s
Val loss: 0.4773 score: 0.7347 time: 0.07s
Test loss: 0.3680 score: 0.8776 time: 0.08s
Epoch 57/1000, LR 0.000269
Train loss: 0.0028;  Loss pred: 0.0028; Loss self: 0.0000; time: 0.22s
Val loss: 0.4745 score: 0.7347 time: 0.07s
Test loss: 0.3631 score: 0.8776 time: 0.08s
Epoch 58/1000, LR 0.000269
Train loss: 0.0025;  Loss pred: 0.0025; Loss self: 0.0000; time: 0.22s
Val loss: 0.4691 score: 0.7347 time: 0.07s
Test loss: 0.3546 score: 0.8776 time: 0.08s
Epoch 59/1000, LR 0.000268
Train loss: 0.0022;  Loss pred: 0.0022; Loss self: 0.0000; time: 0.23s
Val loss: 0.4627 score: 0.7347 time: 0.07s
Test loss: 0.3442 score: 0.8776 time: 0.08s
Epoch 60/1000, LR 0.000268
Train loss: 0.0023;  Loss pred: 0.0023; Loss self: 0.0000; time: 0.22s
Val loss: 0.4562 score: 0.7551 time: 0.07s
Test loss: 0.3348 score: 0.8776 time: 0.08s
Epoch 61/1000, LR 0.000268
Train loss: 0.0020;  Loss pred: 0.0020; Loss self: 0.0000; time: 0.24s
Val loss: 0.4477 score: 0.7959 time: 0.07s
Test loss: 0.3225 score: 0.8980 time: 0.08s
Epoch 62/1000, LR 0.000268
Train loss: 0.0019;  Loss pred: 0.0019; Loss self: 0.0000; time: 0.22s
Val loss: 0.4354 score: 0.7959 time: 0.07s
Test loss: 0.3057 score: 0.8776 time: 0.08s
Epoch 63/1000, LR 0.000268
Train loss: 0.0018;  Loss pred: 0.0018; Loss self: 0.0000; time: 0.22s
Val loss: 0.4230 score: 0.8163 time: 0.07s
Test loss: 0.2883 score: 0.8980 time: 0.08s
Epoch 64/1000, LR 0.000268
Train loss: 0.0016;  Loss pred: 0.0016; Loss self: 0.0000; time: 0.22s
Val loss: 0.4118 score: 0.8163 time: 0.07s
Test loss: 0.2703 score: 0.8980 time: 0.08s
Epoch 65/1000, LR 0.000268
Train loss: 0.0017;  Loss pred: 0.0017; Loss self: 0.0000; time: 0.21s
Val loss: 0.3980 score: 0.8163 time: 0.07s
Test loss: 0.2481 score: 0.9184 time: 0.08s
Epoch 66/1000, LR 0.000268
Train loss: 0.0015;  Loss pred: 0.0015; Loss self: 0.0000; time: 0.22s
Val loss: 0.3851 score: 0.8163 time: 0.07s
Test loss: 0.2273 score: 0.9184 time: 0.08s
Epoch 67/1000, LR 0.000268
Train loss: 0.0016;  Loss pred: 0.0016; Loss self: 0.0000; time: 0.22s
Val loss: 0.3756 score: 0.8163 time: 0.07s
Test loss: 0.2103 score: 0.9184 time: 0.08s
Epoch 68/1000, LR 0.000268
Train loss: 0.0015;  Loss pred: 0.0015; Loss self: 0.0000; time: 0.22s
Val loss: 0.3673 score: 0.7959 time: 0.07s
Test loss: 0.1920 score: 0.9184 time: 0.08s
Epoch 69/1000, LR 0.000268
Train loss: 0.0011;  Loss pred: 0.0011; Loss self: 0.0000; time: 0.23s
Val loss: 0.3633 score: 0.8163 time: 0.07s
Test loss: 0.1753 score: 0.9184 time: 0.08s
Epoch 70/1000, LR 0.000268
Train loss: 0.0016;  Loss pred: 0.0016; Loss self: 0.0000; time: 0.22s
Val loss: 0.3607 score: 0.8571 time: 0.07s
Test loss: 0.1581 score: 0.9592 time: 0.08s
Epoch 71/1000, LR 0.000268
Train loss: 0.0013;  Loss pred: 0.0013; Loss self: 0.0000; time: 0.22s
Val loss: 0.3610 score: 0.8571 time: 0.07s
Test loss: 0.1453 score: 0.9592 time: 0.08s
     INFO: Early stopping counter 1 of 20
Epoch 72/1000, LR 0.000267
Train loss: 0.0013;  Loss pred: 0.0013; Loss self: 0.0000; time: 0.22s
Val loss: 0.3618 score: 0.8776 time: 0.07s
Test loss: 0.1362 score: 0.9592 time: 0.08s
     INFO: Early stopping counter 2 of 20
Epoch 73/1000, LR 0.000267
Train loss: 0.0010;  Loss pred: 0.0010; Loss self: 0.0000; time: 0.22s
Val loss: 0.3654 score: 0.8571 time: 0.07s
Test loss: 0.1291 score: 0.9592 time: 0.08s
     INFO: Early stopping counter 3 of 20
Epoch 74/1000, LR 0.000267
Train loss: 0.0010;  Loss pred: 0.0010; Loss self: 0.0000; time: 0.22s
Val loss: 0.3708 score: 0.8571 time: 0.07s
Test loss: 0.1237 score: 0.9592 time: 0.08s
     INFO: Early stopping counter 4 of 20
Epoch 75/1000, LR 0.000267
Train loss: 0.0009;  Loss pred: 0.0009; Loss self: 0.0000; time: 0.22s
Val loss: 0.3770 score: 0.8571 time: 0.07s
Test loss: 0.1200 score: 0.9592 time: 0.08s
     INFO: Early stopping counter 5 of 20
Epoch 76/1000, LR 0.000267
Train loss: 0.0013;  Loss pred: 0.0013; Loss self: 0.0000; time: 0.22s
Val loss: 0.3839 score: 0.8367 time: 0.07s
Test loss: 0.1171 score: 0.9592 time: 0.08s
     INFO: Early stopping counter 6 of 20
Epoch 77/1000, LR 0.000267
Train loss: 0.0009;  Loss pred: 0.0009; Loss self: 0.0000; time: 0.23s
Val loss: 0.3900 score: 0.8367 time: 0.07s
Test loss: 0.1149 score: 0.9592 time: 0.08s
     INFO: Early stopping counter 7 of 20
Epoch 78/1000, LR 0.000267
Train loss: 0.0009;  Loss pred: 0.0009; Loss self: 0.0000; time: 0.22s
Val loss: 0.3958 score: 0.8367 time: 0.07s
Test loss: 0.1133 score: 0.9592 time: 0.08s
     INFO: Early stopping counter 8 of 20
Epoch 79/1000, LR 0.000267
Train loss: 0.0011;  Loss pred: 0.0011; Loss self: 0.0000; time: 0.22s
Val loss: 0.4013 score: 0.8367 time: 0.07s
Test loss: 0.1120 score: 0.9592 time: 0.08s
     INFO: Early stopping counter 9 of 20
Epoch 80/1000, LR 0.000267
Train loss: 0.0012;  Loss pred: 0.0012; Loss self: 0.0000; time: 0.22s
Val loss: 0.4075 score: 0.8367 time: 0.07s
Test loss: 0.1111 score: 0.9592 time: 0.08s
     INFO: Early stopping counter 10 of 20
Epoch 81/1000, LR 0.000267
Train loss: 0.0009;  Loss pred: 0.0009; Loss self: 0.0000; time: 0.22s
Val loss: 0.4146 score: 0.8367 time: 0.07s
Test loss: 0.1102 score: 0.9592 time: 0.08s
     INFO: Early stopping counter 11 of 20
Epoch 82/1000, LR 0.000267
Train loss: 0.0008;  Loss pred: 0.0008; Loss self: 0.0000; time: 0.22s
Val loss: 0.4227 score: 0.8367 time: 0.07s
Test loss: 0.1095 score: 0.9592 time: 0.08s
     INFO: Early stopping counter 12 of 20
Epoch 83/1000, LR 0.000266
Train loss: 0.0008;  Loss pred: 0.0008; Loss self: 0.0000; time: 0.22s
Val loss: 0.4318 score: 0.8367 time: 0.07s
Test loss: 0.1086 score: 0.9592 time: 0.08s
     INFO: Early stopping counter 13 of 20
Epoch 84/1000, LR 0.000266
Train loss: 0.0008;  Loss pred: 0.0008; Loss self: 0.0000; time: 0.22s
Val loss: 0.4414 score: 0.8163 time: 0.07s
Test loss: 0.1084 score: 0.9592 time: 0.08s
     INFO: Early stopping counter 14 of 20
Epoch 85/1000, LR 0.000266
Train loss: 0.0007;  Loss pred: 0.0007; Loss self: 0.0000; time: 0.22s
Val loss: 0.4530 score: 0.8163 time: 0.07s
Test loss: 0.1088 score: 0.9592 time: 0.08s
     INFO: Early stopping counter 15 of 20
Epoch 86/1000, LR 0.000266
Train loss: 0.0006;  Loss pred: 0.0006; Loss self: 0.0000; time: 0.22s
Val loss: 0.4659 score: 0.8367 time: 0.07s
Test loss: 0.1095 score: 0.9592 time: 0.08s
     INFO: Early stopping counter 16 of 20
Epoch 87/1000, LR 0.000266
Train loss: 0.0006;  Loss pred: 0.0006; Loss self: 0.0000; time: 0.22s
Val loss: 0.4815 score: 0.8367 time: 0.07s
Test loss: 0.1078 score: 0.9592 time: 0.08s
     INFO: Early stopping counter 17 of 20
Epoch 88/1000, LR 0.000266
Train loss: 0.0006;  Loss pred: 0.0006; Loss self: 0.0000; time: 0.22s
Val loss: 0.4997 score: 0.8163 time: 0.07s
Test loss: 0.1076 score: 0.9592 time: 0.08s
     INFO: Early stopping counter 18 of 20
Epoch 89/1000, LR 0.000266
Train loss: 0.0006;  Loss pred: 0.0006; Loss self: 0.0000; time: 0.22s
Val loss: 0.5207 score: 0.8163 time: 0.07s
Test loss: 0.1111 score: 0.9592 time: 0.08s
     INFO: Early stopping counter 19 of 20
Epoch 90/1000, LR 0.000266
Train loss: 0.0006;  Loss pred: 0.0006; Loss self: 0.0000; time: 0.22s
Val loss: 0.5486 score: 0.8367 time: 0.07s
Test loss: 0.1188 score: 0.8980 time: 0.09s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 069,   Train_Loss: 0.0016,   Val_Loss: 0.3607,   Val_Precision: 0.9091,   Val_Recall: 0.8000,   Val_accuracy: 0.8511,   Val_Score: 0.8571,   Val_Loss: 0.3607,   Test_Precision: 0.9583,   Test_Recall: 0.9583,   Test_accuracy: 0.9583,   Test_Score: 0.9592,   Test_loss: 0.1581


[0.0644826750503853, 0.06443862803280354, 0.0645348159596324, 0.07019063096959144, 0.06461871904321015, 0.06437401799485087, 0.06433912296779454, 0.07102904794737697, 0.0648910899180919, 0.0648136610398069, 0.06393573095556349, 0.06425134197343141, 0.06443162995856255, 0.07163019699510187, 0.06446594896260649, 0.06361221603583544, 0.06384435202926397, 0.06382109900005162, 0.07124983798712492, 0.0643562659388408, 0.0647353840759024, 0.06404850899707526, 0.06429420795757324, 0.07065497699659318, 0.07036611798685044, 0.06428111298009753, 0.0647413789993152, 0.06481685803737491, 0.07463502895552665, 0.06443383800797164, 0.06996510305907577, 0.06387466401793063, 0.07087652490008622, 0.06435540597885847, 0.06477649207226932, 0.06457676296122372, 0.0703419370111078, 0.06440019397996366, 0.06466191494837403, 0.06438850006088614, 0.0641920460620895, 0.06377141596749425, 0.06392341898754239, 0.07059295708313584, 0.06391120993066579, 0.06392881297506392, 0.07209730101749301, 0.06454430799931288, 0.06395665893796831, 0.06404085201211274, 0.07002345798537135, 0.06408313394058496, 0.06472722592297941, 0.06459272396750748, 0.06417917599901557, 0.06432376289740205, 0.06449272600002587, 0.0650650889147073, 0.0667443519923836, 0.06476360093802214, 0.06418451108038425, 0.06468058994505554, 0.0641525499522686, 0.06411690602544695, 0.06386055506300181, 0.064334562048316, 0.06383313296828419, 0.06396952993236482, 0.06894486001692712, 0.06414954492356628, 0.06386131094768643, 0.06418418302200735, 0.06362403905950487, 0.066609677975066, 0.06407150195445865, 0.0642807160038501, 0.06394871894735843, 0.06920175999403, 0.06955765199381858, 0.06403393601067364, 0.07042789901606739, 0.07088424207177013, 0.07081668300088495, 0.0704386419383809, 0.07570642000064254, 0.07054653007071465, 0.07081792107783258, 0.06726047792471945, 0.06716600002255291, 0.06700066605117172, 0.06723369401879609, 0.06688065291382372, 0.06716732401400805, 0.06709835398942232, 0.06756854394916445, 0.0720388978952542, 0.06745747302193195, 0.06793466000817716, 0.06738519598729908, 0.06728586996905506, 0.07186640799045563, 0.0713276609312743, 0.06769404502119869, 0.07209854200482368, 0.06747251597698778, 0.07182020496111363, 0.06682751793414354, 0.06721933896187693, 0.07525063399225473, 0.08440234104637057, 0.07597948506008834, 0.08085864107124507, 0.0792280089808628, 0.07924570993054658, 0.07494487694930285, 0.07879593200050294, 0.0791053909342736, 0.07895075506530702, 0.0786748849786818, 0.07561631198041141, 0.07491056306753308, 0.07456229499075562, 0.0847675969125703, 0.07800059800501913, 0.07957207201980054, 0.07878959004301578, 0.08208985603414476, 0.0782595950877294, 0.07775312394369394, 0.07419542700517923, 0.07473143201787025, 0.07861641293857247, 0.07894071796908975, 0.07962719595525414, 0.07928102591540664, 0.0805154099361971, 0.08000009099487215, 0.0797373839886859, 0.08155115391127765, 0.08098197798244655, 0.08229287597350776, 0.07922104699537158, 0.07954884704668075, 0.07556423195637763, 0.08123782905749977, 0.08104944205842912, 0.07977142301388085, 0.07959295099135488, 0.0797366650076583, 0.08089875697623938, 0.07723208901006728, 0.07515979406889528, 0.0752171230269596, 0.07877082494087517, 0.08076238795183599, 0.08012393501121551, 0.08091989497188479, 0.08169057301711291, 0.08672336302697659, 0.08523238799534738, 0.08168366795871407, 0.08452993491664529, 0.08068763208575547, 0.08478506107348949, 0.08629848097916692, 0.08647885301616043, 0.08502239501103759, 0.08211799699347466, 0.08843058708589524, 0.08259632997214794, 0.08712021401152015, 0.08521754399407655, 0.08541707100812346, 0.08086216496303678, 0.08639722398947924, 0.08802028105128556, 0.08558343898039311, 0.08448985498398542, 0.08687086310237646, 0.08591762091964483, 0.08517835196107626, 0.08479266602080315, 0.08133790199644864, 0.08091255300678313, 0.08066782902460545, 0.08538968407083303, 0.08731523295864463, 0.08147480897605419, 0.08560381107963622, 0.08678683103062212, 0.09166244103107601, 0.08742820902261883, 0.08688609895762056, 0.08167140802834183, 0.0854045629967004, 0.0871123829856515, 0.08609581994824111, 0.09228071209508926]
[0.0013159729602119448, 0.0013150740414857864, 0.001317037060400661, 0.0014324618565222742, 0.0013187493682287786, 0.0013137554692826709, 0.0013130433258733579, 0.0014495724070893259, 0.0013243079575120794, 0.0013227277763225899, 0.0013048108358278262, 0.0013112518770088042, 0.0013149312236441337, 0.001461840755002079, 0.001315631611481765, 0.001298208490527254, 0.0013029459597808974, 0.0013024714081643187, 0.0014540783262678555, 0.0013133931824253226, 0.0013211302872633143, 0.00130711242851174, 0.0013121266930116986, 0.001441938306052922, 0.0014360432242214376, 0.001311859448573419, 0.0013212526326390858, 0.0013227930211709166, 0.0015231638562352378, 0.0013149762858769723, 0.0014278592461035872, 0.0013035645717945025, 0.0014464596918384945, 0.0013133756322216015, 0.00132196922596468, 0.001317893121657627, 0.0014355497349205675, 0.001314289673060483, 0.0013196309173137558, 0.0013140510216507377, 0.0013100417563691735, 0.0013014574687243725, 0.0013045595711743344, 0.0014406725935333846, 0.0013043104067482815, 0.001304669652552325, 0.0014713734901529185, 0.0013172307754961811, 0.0013052379375095575, 0.001306956163512505, 0.0014290501629667623, 0.001307819060011938, 0.0013209637943465188, 0.0013182188564797445, 0.001309779102020726, 0.0013127298550490215, 0.001316178081633181, 0.0013278589574430063, 0.0013621296324976245, 0.0013217061415922884, 0.0013098879812323317, 0.0013200120396950111, 0.0013092357133116042, 0.0013085082862336114, 0.0013032766339388124, 0.001312950245884, 0.0013027169993527386, 0.001305500610864588, 0.0014070379595291249, 0.0013091743861952303, 0.001303292060156866, 0.0013098812861634152, 0.001298449776724589, 0.0013593811831646124, 0.0013075816725399724, 0.001311851347017349, 0.001305075896884866, 0.0014122808162046938, 0.0014195439182411954, 0.0013068150206259927, 0.0014373040615523957, 0.001446617185138166, 0.0014452384285894887, 0.0014375233048649163, 0.0015450289796049498, 0.0014397251034839725, 0.001445263695465971, 0.001372662814790193, 0.0013707346943378144, 0.0013673605316565658, 0.0013721162044652263, 0.0013649112839555862, 0.0013707617145715927, 0.0013693541630494352, 0.0013789498765135603, 0.0014701815896990653, 0.0013766831228965704, 0.0013864216328199422, 0.0013752080813734507, 0.001373181019776634, 0.0014666613875603189, 0.0014556665496178428, 0.001381511122881606, 0.0014713988164249733, 0.0013769901219793425, 0.0014657184685941559, 0.0013638268966151743, 0.0013718232441199373, 0.0015357272243317292, 0.0017224967560483789, 0.00155060173592017, 0.0016501763483927566, 0.0016168981424665876, 0.0016172593863376854, 0.00152948728467965, 0.0016080802449082233, 0.0016143957333525224, 0.00161123989929198, 0.0016056098975241184, 0.0015431900404165595, 0.001528787001378226, 0.0015216794896072575, 0.0017299509573993938, 0.0015918489388779414, 0.0016239198371387866, 0.0016079508172044037, 0.0016753031843703013, 0.0015971345936271306, 0.0015867984478304886, 0.0015141923878608005, 0.0015251312656708214, 0.0016044165905831115, 0.0016110350605936683, 0.0016250448154133497, 0.0016179801207225845, 0.0016431716313509612, 0.001632654918262697, 0.0016272935507895083, 0.0016643092634954623, 0.001652693428213195, 0.001679446448438934, 0.0016167560611300323, 0.0016234458580955255, 0.0015421271827832169, 0.001657914878724485, 0.0016540702460903901, 0.0016279882247730785, 0.0016243459385990792, 0.0016272788777073122, 0.0016509950403314159, 0.001576165081838108, 0.0015338733483448016, 0.0015350433270808082, 0.001607567855936228, 0.001648211999017061, 0.0016351823471676633, 0.0016514264279976487, 0.0016671545513696512, 0.0017698645515709507, 0.0017394364897009669, 0.0016670136318104913, 0.0017251007125845977, 0.0016466863690970503, 0.0017303073688467242, 0.0017611934893707536, 0.001764874551350213, 0.0017351509185926039, 0.0016758774896627482, 0.0018047058588958212, 0.0016856393871866927, 0.0017779635512555133, 0.0017391335508995214, 0.0017432055307780297, 0.0016502482645517709, 0.0017632086528465152, 0.0017963322663527665, 0.0017466007955182269, 0.0017242827547752128, 0.0017728747571913563, 0.0017534208350947925, 0.0017383337134913523, 0.0017304625718531255, 0.0016599571836009926, 0.0016512765919751659, 0.001646282224991948, 0.00174264661369047, 0.0017819435297682577, 0.0016627512035929427, 0.001747016552645637, 0.001771159816951472, 0.001870662061858694, 0.001784249163726915, 0.0017731856930126644, 0.0016667634291498332, 0.001742950265238784, 0.001777803734401051, 0.0017570575499641044, 0.001883279838675291]
[759.8940329586593, 760.4134584469389, 759.2800765194762, 698.0988676569696, 758.2942021372165, 761.1766598741654, 761.589492361084, 689.8586059650195, 755.1113729458042, 756.0134578712572, 766.3946164008964, 762.6299855380764, 760.4960487809018, 684.0690386953795, 760.0911921489355, 770.2922968820365, 767.4915390720881, 767.7711723510178, 687.7208620299524, 761.3866231233147, 756.9276169358534, 765.0451316866339, 762.1215278417358, 693.5109468985131, 696.3578694103431, 762.2767828424376, 756.857526938348, 755.97616860332, 656.5281836923787, 760.469987740569, 700.3491434669414, 767.1273227557788, 691.3431502048768, 761.3967972806681, 756.4472609188542, 758.786872445479, 696.5972516830512, 760.8672734005272, 757.7876411349945, 761.0054583297532, 763.3344472710053, 768.3693274895514, 766.5422278108948, 694.1202355681705, 766.6886615533919, 766.4775508832448, 679.6370919365081, 759.1684149827997, 766.1438357423453, 765.1366035969056, 699.7654987309628, 764.6317679380448, 757.0230193134857, 758.5993745154454, 763.4875212600361, 761.7713546727075, 759.7756063215617, 753.09203164593, 734.1445161620798, 756.5978310392642, 763.4240594063688, 757.5688478046383, 763.8044011727898, 764.2290159876508, 767.2968071082206, 761.6434843094204, 767.6264303734848, 765.9896837104767, 710.7128796544032, 763.8401809145045, 767.2877251163785, 763.4279614215698, 770.1491562673721, 735.6288378746128, 764.7705845077377, 762.2814904094275, 766.2389615706926, 708.0744767795965, 704.4516109364145, 765.2192423691139, 695.7470077138204, 691.2678836346676, 691.9273527593446, 695.6408961272247, 647.2370506964155, 694.5770394501789, 691.9152561135825, 728.5110292383398, 729.5357767850825, 731.3360133252448, 728.8012463855011, 732.6483499366686, 729.5213962935435, 730.2712672761626, 725.1895206868066, 680.1880849322105, 726.3835688607694, 721.2813016816742, 727.1626843563033, 728.2361069647345, 681.8206359570322, 686.9705155088785, 723.8450588180309, 679.6253937662387, 726.2216221003523, 682.2592615341405, 733.23088324615, 728.9568858716399, 651.1573046021564, 580.5526172914972, 644.9109251168046, 605.9958385502148, 618.4681482004142, 618.3300022543193, 653.8138695343579, 621.8595142663868, 619.4268105028732, 620.640042764225, 622.816290272014, 648.0083293759892, 654.1133585636743, 657.1686132525175, 578.0510688599422, 628.2003119623132, 615.7939432293148, 621.9095691860825, 596.9068818882903, 626.1213074904202, 630.1997593753798, 660.418060490164, 655.6812665958657, 623.2795184675562, 620.7189554468782, 615.3676443351736, 618.054565190457, 608.5791532183605, 612.4993033213025, 614.5172759486655, 600.8498672294546, 605.0728967205656, 595.4342878449695, 618.5224994926258, 615.9737295909001, 648.4549466245769, 603.1672752520014, 604.5692450871599, 614.2550571208141, 615.6324070120507, 614.5228170164102, 605.6953386118367, 634.451309398258, 651.9443088825406, 651.4474102185115, 622.0577229802919, 606.7180681831991, 611.5525902858006, 605.537118121876, 599.8244129066796, 565.0149889223949, 574.8988283969561, 599.8751185459301, 579.6763010443431, 607.2801832618214, 577.93200098692, 567.7967844165062, 566.6125103537544, 576.3187451216686, 596.7023282836975, 554.1069172412579, 593.2466977228061, 562.4412262522748, 574.9989697356917, 573.6558210400347, 605.969429861278, 567.1478519491185, 556.6898834536764, 572.5406759037311, 579.9512853855374, 564.0556367242948, 570.3137432754063, 575.2635367069723, 577.880166994375, 602.4251769137029, 605.5920642609335, 607.4292638401603, 573.8398090260316, 561.185011362313, 601.4128859682422, 572.4044219762118, 564.6017882910219, 534.5700970737588, 560.4598395390103, 563.956727115809, 599.9651675283459, 573.7398363819636, 562.4917872820781, 569.1333217972452, 530.9885336548843]
Elapsed: 0.07343810223715587~0.008339745752398865
Time per graph: 0.0014987367803501197~0.00017019889290609925
Speed: 675.6976984032651~74.82150509635416
Total Time: 0.0931
best val loss: 0.36069995164871216 test_score: 0.9592

Testing...
Test loss: 0.1362 score: 0.9592 time: 0.08s
test Score 0.9592
Epoch Time List: [0.33245348010677844, 0.32852764893323183, 0.3370728971203789, 0.32968225597869605, 0.3295571719063446, 0.33190234599169344, 0.3346420080633834, 0.33518304605968297, 0.3329746489180252, 0.33122653095051646, 0.31995276489760727, 0.33186898205894977, 0.3241046400507912, 0.3463307870551944, 0.33102158398833126, 0.32088763802312315, 0.319689805037342, 0.3217936120927334, 0.33466355106793344, 0.3317127062473446, 0.3241832018829882, 0.32209669798612595, 0.32162193313706666, 0.3292630580253899, 0.3305535651743412, 0.33248257101513445, 0.32576106302440166, 0.33000474399887025, 0.348557694000192, 0.33419915684498847, 0.3353538030060008, 0.32373901398386806, 0.3349831009982154, 0.32823484402615577, 0.3268913999199867, 0.3338019169168547, 0.33388584095519036, 0.3294920389307663, 0.33230421401094645, 0.3235986439976841, 0.3236075059976429, 0.32239997293800116, 0.32127891294658184, 0.33667722798418254, 0.32756817096378654, 0.32575596193782985, 0.33604643191210926, 0.32994258299004287, 0.322193932835944, 0.33105809800326824, 0.33116249507293105, 0.33176720002666116, 0.33220769208855927, 0.3355562739307061, 0.3320715018780902, 0.3313729028450325, 0.333300246973522, 0.33325597806833684, 0.340387150994502, 0.3337919219629839, 0.3307352770352736, 0.32446340401656926, 0.32435808097943664, 0.32574275496881455, 0.3253884450532496, 0.32874444511253387, 0.3275584459770471, 0.32729491498321295, 0.3261925739934668, 0.32862572197336704, 0.3319753339746967, 0.3285865270299837, 0.32053236302454025, 0.33021121891215444, 0.3275437589036301, 0.3216882279375568, 0.3210726941470057, 0.3320434900233522, 0.3336182909552008, 0.330510267871432, 0.3387459389632568, 0.36026847397442907, 0.3601068068528548, 0.35521745099686086, 0.3603520850883797, 0.35541727906093, 0.3530896690208465, 0.3435584349790588, 0.34299308713525534, 0.334249958046712, 0.3384693389525637, 0.33899327693507075, 0.3382542791077867, 0.334890631143935, 0.3357042360585183, 0.33859612909145653, 0.33672224590554833, 0.33966553397476673, 0.33610803296323866, 0.339433571905829, 0.3417611379409209, 0.3418157041305676, 0.34223654493689537, 0.35153683507815003, 0.33687545091379434, 0.3338408200070262, 0.3285765520995483, 0.3295315249124542, 0.3320798239437863, 0.3388600249309093, 0.33441538515035063, 0.3477792809717357, 0.3415331330616027, 0.33324234501924366, 0.3297927640378475, 0.3381947900634259, 0.3331736109685153, 0.33495816320646554, 0.33501721802167594, 0.33564622385893017, 0.33385182404890656, 0.3319035819731653, 0.33753478596918285, 0.3306253079790622, 0.33245407207868993, 0.333030498935841, 0.34115803707391024, 0.3318158119218424, 0.3308232920244336, 0.32636691408697516, 0.33671783399768174, 0.3368170610629022, 0.3709442439721897, 0.3381758240284398, 0.33758380298968405, 0.3426944230450317, 0.34281821781769395, 0.38469121896196157, 0.33506384398788214, 0.33859989303164184, 0.3447478578891605, 0.35172095394227654, 0.37346269900444895, 0.3321258540963754, 0.3469656379893422, 0.3430334848817438, 0.347816274035722, 0.4289493740070611, 0.3360568400239572, 0.3462826340692118, 0.34273999894503504, 0.3372524189762771, 0.3387985909357667, 0.34345045790541917, 0.34815998596604913, 0.3641308869700879, 0.339033778058365, 0.3437891900539398, 0.35245168511755764, 0.3641586798476055, 0.3673847339814529, 0.3675946380244568, 0.3610219289548695, 0.370054395054467, 0.3702684890013188, 0.3674646660219878, 0.3727401659125462, 0.3723900739569217, 0.3881261700298637, 0.3704573850845918, 0.3758587701013312, 0.3680979630444199, 0.36091587506234646, 0.3611978539265692, 0.3675756100565195, 0.37878470902796835, 0.37420502689201385, 0.3639414310455322, 0.37597326398827136, 0.3674348898930475, 0.3730869251303375, 0.36499470996204764, 0.36220415495336056, 0.36253583896905184, 0.3701987948734313, 0.3718234240077436, 0.3716757100773975, 0.36584836198017, 0.3659184080315754, 0.3751159409293905, 0.36992260604165494, 0.37111922702752054, 0.3804808569839224, 0.3656145309796557, 0.3665418300079182, 0.3735178771894425, 0.37846310797613114, 0.3761044970015064]
Total Epoch List: [108, 90]
Total Time List: [0.06779721495695412, 0.09310114406980574]
========================k_idx:2========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: GPSPerformer
GPS(
  (x_lin): Linear(in_features=14887, out_features=64, bias=True)
  (pe_lin): Linear(in_features=20, out_features=8, bias=True)
  (pe_norm): BatchNorm1d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (convs): ModuleList(
    (0-1): 2 x GPSConv(64, conv=GINEConv(nn=Sequential(
      (0): Linear(in_features=64, out_features=64, bias=True)
      (1): ReLU()
      (2): Linear(in_features=64, out_features=64, bias=True)
    )), heads=1, attn_type=performer)
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=32, bias=True)
    (1): ReLU()
    (2): Linear(in_features=32, out_features=16, bias=True)
    (3): ReLU()
    (4): Linear(in_features=16, out_features=2, bias=True)
  )
)
Total number of parameters: 1039522
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7c9d24c93850>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.8823;  Loss pred: 0.8823; Loss self: 0.0000; time: 0.22s
Val loss: 0.6125 score: 0.6327 time: 0.07s
Test loss: 0.6031 score: 0.6875 time: 0.07s
Epoch 2/1000, LR 0.000000
Train loss: 0.9521;  Loss pred: 0.9521; Loss self: 0.0000; time: 0.22s
Val loss: 0.6122 score: 0.6122 time: 0.07s
Test loss: 0.6043 score: 0.7083 time: 0.07s
Epoch 3/1000, LR 0.000030
Train loss: 0.9177;  Loss pred: 0.9177; Loss self: 0.0000; time: 0.22s
Val loss: 0.6069 score: 0.6122 time: 0.07s
Test loss: 0.6026 score: 0.7083 time: 0.07s
Epoch 4/1000, LR 0.000060
Train loss: 0.8313;  Loss pred: 0.8313; Loss self: 0.0000; time: 0.22s
Val loss: 0.5996 score: 0.6122 time: 0.07s
Test loss: 0.5989 score: 0.7083 time: 0.07s
Epoch 5/1000, LR 0.000090
Train loss: 0.7808;  Loss pred: 0.7808; Loss self: 0.0000; time: 0.22s
Val loss: 0.5925 score: 0.6531 time: 0.07s
Test loss: 0.5905 score: 0.7292 time: 0.07s
Epoch 6/1000, LR 0.000120
Train loss: 0.7637;  Loss pred: 0.7637; Loss self: 0.0000; time: 0.22s
Val loss: 0.5875 score: 0.6735 time: 0.08s
Test loss: 0.5803 score: 0.7083 time: 0.07s
Epoch 7/1000, LR 0.000150
Train loss: 0.6824;  Loss pred: 0.6824; Loss self: 0.0000; time: 0.22s
Val loss: 0.5824 score: 0.6939 time: 0.07s
Test loss: 0.5739 score: 0.6667 time: 0.07s
Epoch 8/1000, LR 0.000180
Train loss: 0.6292;  Loss pred: 0.6292; Loss self: 0.0000; time: 0.22s
Val loss: 0.5794 score: 0.6327 time: 0.07s
Test loss: 0.5691 score: 0.6875 time: 0.07s
Epoch 9/1000, LR 0.000210
Train loss: 0.6044;  Loss pred: 0.6044; Loss self: 0.0000; time: 0.22s
Val loss: 0.5790 score: 0.6327 time: 0.07s
Test loss: 0.5676 score: 0.7083 time: 0.07s
Epoch 10/1000, LR 0.000240
Train loss: 0.5896;  Loss pred: 0.5896; Loss self: 0.0000; time: 0.22s
Val loss: 0.5781 score: 0.6531 time: 0.07s
Test loss: 0.5740 score: 0.7083 time: 0.07s
Epoch 11/1000, LR 0.000270
Train loss: 0.5829;  Loss pred: 0.5829; Loss self: 0.0000; time: 0.22s
Val loss: 0.5770 score: 0.6122 time: 0.07s
Test loss: 0.5787 score: 0.7083 time: 0.07s
Epoch 12/1000, LR 0.000270
Train loss: 0.5664;  Loss pred: 0.5664; Loss self: 0.0000; time: 0.22s
Val loss: 0.5795 score: 0.6122 time: 0.07s
Test loss: 0.5810 score: 0.7083 time: 0.07s
     INFO: Early stopping counter 1 of 20
Epoch 13/1000, LR 0.000270
Train loss: 0.5015;  Loss pred: 0.5015; Loss self: 0.0000; time: 0.22s
Val loss: 0.5817 score: 0.6122 time: 0.07s
Test loss: 0.5804 score: 0.7083 time: 0.07s
     INFO: Early stopping counter 2 of 20
Epoch 14/1000, LR 0.000270
Train loss: 0.4816;  Loss pred: 0.4816; Loss self: 0.0000; time: 0.22s
Val loss: 0.5827 score: 0.6327 time: 0.06s
Test loss: 0.5796 score: 0.7083 time: 0.06s
     INFO: Early stopping counter 3 of 20
Epoch 15/1000, LR 0.000270
Train loss: 0.4604;  Loss pred: 0.4604; Loss self: 0.0000; time: 0.20s
Val loss: 0.5837 score: 0.6327 time: 0.06s
Test loss: 0.5790 score: 0.7083 time: 0.06s
     INFO: Early stopping counter 4 of 20
Epoch 16/1000, LR 0.000270
Train loss: 0.4409;  Loss pred: 0.4409; Loss self: 0.0000; time: 0.20s
Val loss: 0.5844 score: 0.6122 time: 0.06s
Test loss: 0.5756 score: 0.7292 time: 0.06s
     INFO: Early stopping counter 5 of 20
Epoch 17/1000, LR 0.000270
Train loss: 0.4362;  Loss pred: 0.4362; Loss self: 0.0000; time: 0.20s
Val loss: 0.5855 score: 0.6122 time: 0.06s
Test loss: 0.5711 score: 0.7292 time: 0.06s
     INFO: Early stopping counter 6 of 20
Epoch 18/1000, LR 0.000270
Train loss: 0.3908;  Loss pred: 0.3908; Loss self: 0.0000; time: 0.20s
Val loss: 0.5858 score: 0.6735 time: 0.06s
Test loss: 0.5675 score: 0.7083 time: 0.06s
     INFO: Early stopping counter 7 of 20
Epoch 19/1000, LR 0.000270
Train loss: 0.3920;  Loss pred: 0.3920; Loss self: 0.0000; time: 0.20s
Val loss: 0.5858 score: 0.6939 time: 0.06s
Test loss: 0.5664 score: 0.6667 time: 0.06s
     INFO: Early stopping counter 8 of 20
Epoch 20/1000, LR 0.000270
Train loss: 0.3773;  Loss pred: 0.3773; Loss self: 0.0000; time: 0.20s
Val loss: 0.5862 score: 0.6735 time: 0.06s
Test loss: 0.5652 score: 0.6458 time: 0.07s
     INFO: Early stopping counter 9 of 20
Epoch 21/1000, LR 0.000270
Train loss: 0.3601;  Loss pred: 0.3601; Loss self: 0.0000; time: 0.20s
Val loss: 0.5868 score: 0.6735 time: 0.06s
Test loss: 0.5644 score: 0.6458 time: 0.06s
     INFO: Early stopping counter 10 of 20
Epoch 22/1000, LR 0.000270
Train loss: 0.3296;  Loss pred: 0.3296; Loss self: 0.0000; time: 0.20s
Val loss: 0.5887 score: 0.5918 time: 0.06s
Test loss: 0.5661 score: 0.6458 time: 0.06s
     INFO: Early stopping counter 11 of 20
Epoch 23/1000, LR 0.000270
Train loss: 0.3230;  Loss pred: 0.3230; Loss self: 0.0000; time: 0.20s
Val loss: 0.5919 score: 0.6122 time: 0.07s
Test loss: 0.5686 score: 0.6667 time: 0.06s
     INFO: Early stopping counter 12 of 20
Epoch 24/1000, LR 0.000270
Train loss: 0.3179;  Loss pred: 0.3179; Loss self: 0.0000; time: 0.21s
Val loss: 0.5968 score: 0.5918 time: 0.07s
Test loss: 0.5709 score: 0.6875 time: 0.07s
     INFO: Early stopping counter 13 of 20
Epoch 25/1000, LR 0.000270
Train loss: 0.2665;  Loss pred: 0.2665; Loss self: 0.0000; time: 0.21s
Val loss: 0.6024 score: 0.5918 time: 0.07s
Test loss: 0.5732 score: 0.7083 time: 0.07s
     INFO: Early stopping counter 14 of 20
Epoch 26/1000, LR 0.000270
Train loss: 0.2723;  Loss pred: 0.2723; Loss self: 0.0000; time: 0.20s
Val loss: 0.6068 score: 0.5510 time: 0.06s
Test loss: 0.5760 score: 0.7292 time: 0.06s
     INFO: Early stopping counter 15 of 20
Epoch 27/1000, LR 0.000270
Train loss: 0.2519;  Loss pred: 0.2519; Loss self: 0.0000; time: 0.19s
Val loss: 0.6100 score: 0.5714 time: 0.07s
Test loss: 0.5783 score: 0.7500 time: 0.07s
     INFO: Early stopping counter 16 of 20
Epoch 28/1000, LR 0.000270
Train loss: 0.2165;  Loss pred: 0.2165; Loss self: 0.0000; time: 0.19s
Val loss: 0.6130 score: 0.5510 time: 0.06s
Test loss: 0.5797 score: 0.7292 time: 0.06s
     INFO: Early stopping counter 17 of 20
Epoch 29/1000, LR 0.000270
Train loss: 0.2059;  Loss pred: 0.2059; Loss self: 0.0000; time: 0.19s
Val loss: 0.6163 score: 0.6122 time: 0.06s
Test loss: 0.5809 score: 0.7083 time: 0.06s
     INFO: Early stopping counter 18 of 20
Epoch 30/1000, LR 0.000270
Train loss: 0.2085;  Loss pred: 0.2085; Loss self: 0.0000; time: 0.19s
Val loss: 0.6217 score: 0.6122 time: 0.06s
Test loss: 0.5829 score: 0.7292 time: 0.06s
     INFO: Early stopping counter 19 of 20
Epoch 31/1000, LR 0.000270
Train loss: 0.1783;  Loss pred: 0.1783; Loss self: 0.0000; time: 0.20s
Val loss: 0.6297 score: 0.5918 time: 0.06s
Test loss: 0.5855 score: 0.7500 time: 0.06s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 010,   Train_Loss: 0.5829,   Val_Loss: 0.5770,   Val_Precision: 0.6000,   Val_Recall: 0.7200,   Val_accuracy: 0.6545,   Val_Score: 0.6122,   Val_Loss: 0.5770,   Test_Precision: 0.6786,   Test_Recall: 0.7917,   Test_accuracy: 0.7308,   Test_Score: 0.7083,   Test_loss: 0.5787


[0.0644826750503853, 0.06443862803280354, 0.0645348159596324, 0.07019063096959144, 0.06461871904321015, 0.06437401799485087, 0.06433912296779454, 0.07102904794737697, 0.0648910899180919, 0.0648136610398069, 0.06393573095556349, 0.06425134197343141, 0.06443162995856255, 0.07163019699510187, 0.06446594896260649, 0.06361221603583544, 0.06384435202926397, 0.06382109900005162, 0.07124983798712492, 0.0643562659388408, 0.0647353840759024, 0.06404850899707526, 0.06429420795757324, 0.07065497699659318, 0.07036611798685044, 0.06428111298009753, 0.0647413789993152, 0.06481685803737491, 0.07463502895552665, 0.06443383800797164, 0.06996510305907577, 0.06387466401793063, 0.07087652490008622, 0.06435540597885847, 0.06477649207226932, 0.06457676296122372, 0.0703419370111078, 0.06440019397996366, 0.06466191494837403, 0.06438850006088614, 0.0641920460620895, 0.06377141596749425, 0.06392341898754239, 0.07059295708313584, 0.06391120993066579, 0.06392881297506392, 0.07209730101749301, 0.06454430799931288, 0.06395665893796831, 0.06404085201211274, 0.07002345798537135, 0.06408313394058496, 0.06472722592297941, 0.06459272396750748, 0.06417917599901557, 0.06432376289740205, 0.06449272600002587, 0.0650650889147073, 0.0667443519923836, 0.06476360093802214, 0.06418451108038425, 0.06468058994505554, 0.0641525499522686, 0.06411690602544695, 0.06386055506300181, 0.064334562048316, 0.06383313296828419, 0.06396952993236482, 0.06894486001692712, 0.06414954492356628, 0.06386131094768643, 0.06418418302200735, 0.06362403905950487, 0.066609677975066, 0.06407150195445865, 0.0642807160038501, 0.06394871894735843, 0.06920175999403, 0.06955765199381858, 0.06403393601067364, 0.07042789901606739, 0.07088424207177013, 0.07081668300088495, 0.0704386419383809, 0.07570642000064254, 0.07054653007071465, 0.07081792107783258, 0.06726047792471945, 0.06716600002255291, 0.06700066605117172, 0.06723369401879609, 0.06688065291382372, 0.06716732401400805, 0.06709835398942232, 0.06756854394916445, 0.0720388978952542, 0.06745747302193195, 0.06793466000817716, 0.06738519598729908, 0.06728586996905506, 0.07186640799045563, 0.0713276609312743, 0.06769404502119869, 0.07209854200482368, 0.06747251597698778, 0.07182020496111363, 0.06682751793414354, 0.06721933896187693, 0.07525063399225473, 0.08440234104637057, 0.07597948506008834, 0.08085864107124507, 0.0792280089808628, 0.07924570993054658, 0.07494487694930285, 0.07879593200050294, 0.0791053909342736, 0.07895075506530702, 0.0786748849786818, 0.07561631198041141, 0.07491056306753308, 0.07456229499075562, 0.0847675969125703, 0.07800059800501913, 0.07957207201980054, 0.07878959004301578, 0.08208985603414476, 0.0782595950877294, 0.07775312394369394, 0.07419542700517923, 0.07473143201787025, 0.07861641293857247, 0.07894071796908975, 0.07962719595525414, 0.07928102591540664, 0.0805154099361971, 0.08000009099487215, 0.0797373839886859, 0.08155115391127765, 0.08098197798244655, 0.08229287597350776, 0.07922104699537158, 0.07954884704668075, 0.07556423195637763, 0.08123782905749977, 0.08104944205842912, 0.07977142301388085, 0.07959295099135488, 0.0797366650076583, 0.08089875697623938, 0.07723208901006728, 0.07515979406889528, 0.0752171230269596, 0.07877082494087517, 0.08076238795183599, 0.08012393501121551, 0.08091989497188479, 0.08169057301711291, 0.08672336302697659, 0.08523238799534738, 0.08168366795871407, 0.08452993491664529, 0.08068763208575547, 0.08478506107348949, 0.08629848097916692, 0.08647885301616043, 0.08502239501103759, 0.08211799699347466, 0.08843058708589524, 0.08259632997214794, 0.08712021401152015, 0.08521754399407655, 0.08541707100812346, 0.08086216496303678, 0.08639722398947924, 0.08802028105128556, 0.08558343898039311, 0.08448985498398542, 0.08687086310237646, 0.08591762091964483, 0.08517835196107626, 0.08479266602080315, 0.08133790199644864, 0.08091255300678313, 0.08066782902460545, 0.08538968407083303, 0.08731523295864463, 0.08147480897605419, 0.08560381107963622, 0.08678683103062212, 0.09166244103107601, 0.08742820902261883, 0.08688609895762056, 0.08167140802834183, 0.0854045629967004, 0.0871123829856515, 0.08609581994824111, 0.09228071209508926, 0.07640738703776151, 0.07553224207367748, 0.07617421797476709, 0.07642942399252206, 0.07659461791627109, 0.07640837703365833, 0.07465250696986914, 0.07446918205823749, 0.07441810506861657, 0.07467569201253355, 0.07435073994565755, 0.07450830703601241, 0.07447484298609197, 0.06900173495523632, 0.06884521595202386, 0.06930695800110698, 0.0686388339381665, 0.06909701297990978, 0.06889408198185265, 0.07105435396078974, 0.0681969600263983, 0.06899733806494623, 0.06880835606716573, 0.0725150549551472, 0.07244397304020822, 0.06885710696224123, 0.0696757270488888, 0.06890993902925402, 0.06870692397933453, 0.06863629992585629, 0.06881289696320891]
[0.0013159729602119448, 0.0013150740414857864, 0.001317037060400661, 0.0014324618565222742, 0.0013187493682287786, 0.0013137554692826709, 0.0013130433258733579, 0.0014495724070893259, 0.0013243079575120794, 0.0013227277763225899, 0.0013048108358278262, 0.0013112518770088042, 0.0013149312236441337, 0.001461840755002079, 0.001315631611481765, 0.001298208490527254, 0.0013029459597808974, 0.0013024714081643187, 0.0014540783262678555, 0.0013133931824253226, 0.0013211302872633143, 0.00130711242851174, 0.0013121266930116986, 0.001441938306052922, 0.0014360432242214376, 0.001311859448573419, 0.0013212526326390858, 0.0013227930211709166, 0.0015231638562352378, 0.0013149762858769723, 0.0014278592461035872, 0.0013035645717945025, 0.0014464596918384945, 0.0013133756322216015, 0.00132196922596468, 0.001317893121657627, 0.0014355497349205675, 0.001314289673060483, 0.0013196309173137558, 0.0013140510216507377, 0.0013100417563691735, 0.0013014574687243725, 0.0013045595711743344, 0.0014406725935333846, 0.0013043104067482815, 0.001304669652552325, 0.0014713734901529185, 0.0013172307754961811, 0.0013052379375095575, 0.001306956163512505, 0.0014290501629667623, 0.001307819060011938, 0.0013209637943465188, 0.0013182188564797445, 0.001309779102020726, 0.0013127298550490215, 0.001316178081633181, 0.0013278589574430063, 0.0013621296324976245, 0.0013217061415922884, 0.0013098879812323317, 0.0013200120396950111, 0.0013092357133116042, 0.0013085082862336114, 0.0013032766339388124, 0.001312950245884, 0.0013027169993527386, 0.001305500610864588, 0.0014070379595291249, 0.0013091743861952303, 0.001303292060156866, 0.0013098812861634152, 0.001298449776724589, 0.0013593811831646124, 0.0013075816725399724, 0.001311851347017349, 0.001305075896884866, 0.0014122808162046938, 0.0014195439182411954, 0.0013068150206259927, 0.0014373040615523957, 0.001446617185138166, 0.0014452384285894887, 0.0014375233048649163, 0.0015450289796049498, 0.0014397251034839725, 0.001445263695465971, 0.001372662814790193, 0.0013707346943378144, 0.0013673605316565658, 0.0013721162044652263, 0.0013649112839555862, 0.0013707617145715927, 0.0013693541630494352, 0.0013789498765135603, 0.0014701815896990653, 0.0013766831228965704, 0.0013864216328199422, 0.0013752080813734507, 0.001373181019776634, 0.0014666613875603189, 0.0014556665496178428, 0.001381511122881606, 0.0014713988164249733, 0.0013769901219793425, 0.0014657184685941559, 0.0013638268966151743, 0.0013718232441199373, 0.0015357272243317292, 0.0017224967560483789, 0.00155060173592017, 0.0016501763483927566, 0.0016168981424665876, 0.0016172593863376854, 0.00152948728467965, 0.0016080802449082233, 0.0016143957333525224, 0.00161123989929198, 0.0016056098975241184, 0.0015431900404165595, 0.001528787001378226, 0.0015216794896072575, 0.0017299509573993938, 0.0015918489388779414, 0.0016239198371387866, 0.0016079508172044037, 0.0016753031843703013, 0.0015971345936271306, 0.0015867984478304886, 0.0015141923878608005, 0.0015251312656708214, 0.0016044165905831115, 0.0016110350605936683, 0.0016250448154133497, 0.0016179801207225845, 0.0016431716313509612, 0.001632654918262697, 0.0016272935507895083, 0.0016643092634954623, 0.001652693428213195, 0.001679446448438934, 0.0016167560611300323, 0.0016234458580955255, 0.0015421271827832169, 0.001657914878724485, 0.0016540702460903901, 0.0016279882247730785, 0.0016243459385990792, 0.0016272788777073122, 0.0016509950403314159, 0.001576165081838108, 0.0015338733483448016, 0.0015350433270808082, 0.001607567855936228, 0.001648211999017061, 0.0016351823471676633, 0.0016514264279976487, 0.0016671545513696512, 0.0017698645515709507, 0.0017394364897009669, 0.0016670136318104913, 0.0017251007125845977, 0.0016466863690970503, 0.0017303073688467242, 0.0017611934893707536, 0.001764874551350213, 0.0017351509185926039, 0.0016758774896627482, 0.0018047058588958212, 0.0016856393871866927, 0.0017779635512555133, 0.0017391335508995214, 0.0017432055307780297, 0.0016502482645517709, 0.0017632086528465152, 0.0017963322663527665, 0.0017466007955182269, 0.0017242827547752128, 0.0017728747571913563, 0.0017534208350947925, 0.0017383337134913523, 0.0017304625718531255, 0.0016599571836009926, 0.0016512765919751659, 0.001646282224991948, 0.00174264661369047, 0.0017819435297682577, 0.0016627512035929427, 0.001747016552645637, 0.001771159816951472, 0.001870662061858694, 0.001784249163726915, 0.0017731856930126644, 0.0016667634291498332, 0.001742950265238784, 0.001777803734401051, 0.0017570575499641044, 0.001883279838675291, 0.0015918205632866982, 0.0015735883765349474, 0.0015869628744743143, 0.0015922796665108763, 0.001595721206588981, 0.0015918411882012151, 0.0015552605618722737, 0.0015514412928799477, 0.001550377188929512, 0.0015557435835944489, 0.0015489737488678657, 0.0015522563965835918, 0.001551559228876916, 0.0014375361449007566, 0.0014342753323338304, 0.0014438949583563954, 0.0014299757070451353, 0.0014395211037481204, 0.0014352933746219303, 0.0014802990408497863, 0.0014207700005499646, 0.0014374445430197131, 0.0014335074180659528, 0.0015107303115655668, 0.0015092494383376713, 0.001434523061713359, 0.00145157764685185, 0.0014356237297761254, 0.0014313942495694694, 0.001429922915122006, 0.0014336020200668524]
[759.8940329586593, 760.4134584469389, 759.2800765194762, 698.0988676569696, 758.2942021372165, 761.1766598741654, 761.589492361084, 689.8586059650195, 755.1113729458042, 756.0134578712572, 766.3946164008964, 762.6299855380764, 760.4960487809018, 684.0690386953795, 760.0911921489355, 770.2922968820365, 767.4915390720881, 767.7711723510178, 687.7208620299524, 761.3866231233147, 756.9276169358534, 765.0451316866339, 762.1215278417358, 693.5109468985131, 696.3578694103431, 762.2767828424376, 756.857526938348, 755.97616860332, 656.5281836923787, 760.469987740569, 700.3491434669414, 767.1273227557788, 691.3431502048768, 761.3967972806681, 756.4472609188542, 758.786872445479, 696.5972516830512, 760.8672734005272, 757.7876411349945, 761.0054583297532, 763.3344472710053, 768.3693274895514, 766.5422278108948, 694.1202355681705, 766.6886615533919, 766.4775508832448, 679.6370919365081, 759.1684149827997, 766.1438357423453, 765.1366035969056, 699.7654987309628, 764.6317679380448, 757.0230193134857, 758.5993745154454, 763.4875212600361, 761.7713546727075, 759.7756063215617, 753.09203164593, 734.1445161620798, 756.5978310392642, 763.4240594063688, 757.5688478046383, 763.8044011727898, 764.2290159876508, 767.2968071082206, 761.6434843094204, 767.6264303734848, 765.9896837104767, 710.7128796544032, 763.8401809145045, 767.2877251163785, 763.4279614215698, 770.1491562673721, 735.6288378746128, 764.7705845077377, 762.2814904094275, 766.2389615706926, 708.0744767795965, 704.4516109364145, 765.2192423691139, 695.7470077138204, 691.2678836346676, 691.9273527593446, 695.6408961272247, 647.2370506964155, 694.5770394501789, 691.9152561135825, 728.5110292383398, 729.5357767850825, 731.3360133252448, 728.8012463855011, 732.6483499366686, 729.5213962935435, 730.2712672761626, 725.1895206868066, 680.1880849322105, 726.3835688607694, 721.2813016816742, 727.1626843563033, 728.2361069647345, 681.8206359570322, 686.9705155088785, 723.8450588180309, 679.6253937662387, 726.2216221003523, 682.2592615341405, 733.23088324615, 728.9568858716399, 651.1573046021564, 580.5526172914972, 644.9109251168046, 605.9958385502148, 618.4681482004142, 618.3300022543193, 653.8138695343579, 621.8595142663868, 619.4268105028732, 620.640042764225, 622.816290272014, 648.0083293759892, 654.1133585636743, 657.1686132525175, 578.0510688599422, 628.2003119623132, 615.7939432293148, 621.9095691860825, 596.9068818882903, 626.1213074904202, 630.1997593753798, 660.418060490164, 655.6812665958657, 623.2795184675562, 620.7189554468782, 615.3676443351736, 618.054565190457, 608.5791532183605, 612.4993033213025, 614.5172759486655, 600.8498672294546, 605.0728967205656, 595.4342878449695, 618.5224994926258, 615.9737295909001, 648.4549466245769, 603.1672752520014, 604.5692450871599, 614.2550571208141, 615.6324070120507, 614.5228170164102, 605.6953386118367, 634.451309398258, 651.9443088825406, 651.4474102185115, 622.0577229802919, 606.7180681831991, 611.5525902858006, 605.537118121876, 599.8244129066796, 565.0149889223949, 574.8988283969561, 599.8751185459301, 579.6763010443431, 607.2801832618214, 577.93200098692, 567.7967844165062, 566.6125103537544, 576.3187451216686, 596.7023282836975, 554.1069172412579, 593.2466977228061, 562.4412262522748, 574.9989697356917, 573.6558210400347, 605.969429861278, 567.1478519491185, 556.6898834536764, 572.5406759037311, 579.9512853855374, 564.0556367242948, 570.3137432754063, 575.2635367069723, 577.880166994375, 602.4251769137029, 605.5920642609335, 607.4292638401603, 573.8398090260316, 561.185011362313, 601.4128859682422, 572.4044219762118, 564.6017882910219, 534.5700970737588, 560.4598395390103, 563.956727115809, 599.9651675283459, 573.7398363819636, 562.4917872820781, 569.1333217972452, 530.9885336548843, 628.2115101813099, 635.490205006475, 630.1344638142544, 628.0303774721157, 626.6758854058243, 628.2033706704138, 642.9790766353435, 644.5619338542261, 645.0043300046677, 642.779446783616, 645.5887330117074, 644.2234686234376, 644.5129398790932, 695.6346826806481, 697.2162021170758, 692.5711556873314, 699.3125792789683, 694.6754704715844, 696.7216721552898, 675.539179857832, 703.8436901207872, 695.6790123528863, 697.5896932219378, 661.9315124244127, 662.5809986064513, 697.0957990773774, 688.9056208386635, 696.5613476979393, 698.6195454542151, 699.33839749304, 697.5436599575714]
Elapsed: 0.0732281163881846~0.007856955807472444
Time per graph: 0.0014985888619345842~0.00016005228464307977
Speed: 674.7855905881353~70.40881649246388
Total Time: 0.0696
best val loss: 0.576978862285614 test_score: 0.7083

Testing...
Test loss: 0.5739 score: 0.6667 time: 0.06s
test Score 0.6667
Epoch Time List: [0.33245348010677844, 0.32852764893323183, 0.3370728971203789, 0.32968225597869605, 0.3295571719063446, 0.33190234599169344, 0.3346420080633834, 0.33518304605968297, 0.3329746489180252, 0.33122653095051646, 0.31995276489760727, 0.33186898205894977, 0.3241046400507912, 0.3463307870551944, 0.33102158398833126, 0.32088763802312315, 0.319689805037342, 0.3217936120927334, 0.33466355106793344, 0.3317127062473446, 0.3241832018829882, 0.32209669798612595, 0.32162193313706666, 0.3292630580253899, 0.3305535651743412, 0.33248257101513445, 0.32576106302440166, 0.33000474399887025, 0.348557694000192, 0.33419915684498847, 0.3353538030060008, 0.32373901398386806, 0.3349831009982154, 0.32823484402615577, 0.3268913999199867, 0.3338019169168547, 0.33388584095519036, 0.3294920389307663, 0.33230421401094645, 0.3235986439976841, 0.3236075059976429, 0.32239997293800116, 0.32127891294658184, 0.33667722798418254, 0.32756817096378654, 0.32575596193782985, 0.33604643191210926, 0.32994258299004287, 0.322193932835944, 0.33105809800326824, 0.33116249507293105, 0.33176720002666116, 0.33220769208855927, 0.3355562739307061, 0.3320715018780902, 0.3313729028450325, 0.333300246973522, 0.33325597806833684, 0.340387150994502, 0.3337919219629839, 0.3307352770352736, 0.32446340401656926, 0.32435808097943664, 0.32574275496881455, 0.3253884450532496, 0.32874444511253387, 0.3275584459770471, 0.32729491498321295, 0.3261925739934668, 0.32862572197336704, 0.3319753339746967, 0.3285865270299837, 0.32053236302454025, 0.33021121891215444, 0.3275437589036301, 0.3216882279375568, 0.3210726941470057, 0.3320434900233522, 0.3336182909552008, 0.330510267871432, 0.3387459389632568, 0.36026847397442907, 0.3601068068528548, 0.35521745099686086, 0.3603520850883797, 0.35541727906093, 0.3530896690208465, 0.3435584349790588, 0.34299308713525534, 0.334249958046712, 0.3384693389525637, 0.33899327693507075, 0.3382542791077867, 0.334890631143935, 0.3357042360585183, 0.33859612909145653, 0.33672224590554833, 0.33966553397476673, 0.33610803296323866, 0.339433571905829, 0.3417611379409209, 0.3418157041305676, 0.34223654493689537, 0.35153683507815003, 0.33687545091379434, 0.3338408200070262, 0.3285765520995483, 0.3295315249124542, 0.3320798239437863, 0.3388600249309093, 0.33441538515035063, 0.3477792809717357, 0.3415331330616027, 0.33324234501924366, 0.3297927640378475, 0.3381947900634259, 0.3331736109685153, 0.33495816320646554, 0.33501721802167594, 0.33564622385893017, 0.33385182404890656, 0.3319035819731653, 0.33753478596918285, 0.3306253079790622, 0.33245407207868993, 0.333030498935841, 0.34115803707391024, 0.3318158119218424, 0.3308232920244336, 0.32636691408697516, 0.33671783399768174, 0.3368170610629022, 0.3709442439721897, 0.3381758240284398, 0.33758380298968405, 0.3426944230450317, 0.34281821781769395, 0.38469121896196157, 0.33506384398788214, 0.33859989303164184, 0.3447478578891605, 0.35172095394227654, 0.37346269900444895, 0.3321258540963754, 0.3469656379893422, 0.3430334848817438, 0.347816274035722, 0.4289493740070611, 0.3360568400239572, 0.3462826340692118, 0.34273999894503504, 0.3372524189762771, 0.3387985909357667, 0.34345045790541917, 0.34815998596604913, 0.3641308869700879, 0.339033778058365, 0.3437891900539398, 0.35245168511755764, 0.3641586798476055, 0.3673847339814529, 0.3675946380244568, 0.3610219289548695, 0.370054395054467, 0.3702684890013188, 0.3674646660219878, 0.3727401659125462, 0.3723900739569217, 0.3881261700298637, 0.3704573850845918, 0.3758587701013312, 0.3680979630444199, 0.36091587506234646, 0.3611978539265692, 0.3675756100565195, 0.37878470902796835, 0.37420502689201385, 0.3639414310455322, 0.37597326398827136, 0.3674348898930475, 0.3730869251303375, 0.36499470996204764, 0.36220415495336056, 0.36253583896905184, 0.3701987948734313, 0.3718234240077436, 0.3716757100773975, 0.36584836198017, 0.3659184080315754, 0.3751159409293905, 0.36992260604165494, 0.37111922702752054, 0.3804808569839224, 0.3656145309796557, 0.3665418300079182, 0.3735178771894425, 0.37846310797613114, 0.3761044970015064, 0.3667502050520852, 0.35904524417128414, 0.360268960124813, 0.36190559598617256, 0.36097136593889445, 0.3657358310883865, 0.3565534548833966, 0.3557483609765768, 0.3551726279547438, 0.3643207380082458, 0.35897889896295965, 0.3554962428752333, 0.35617054405156523, 0.3446341659873724, 0.3268449679017067, 0.329107306082733, 0.32733100396580994, 0.32968720607459545, 0.3273261209251359, 0.3332119439728558, 0.32714272604789585, 0.3298844189848751, 0.3354334948817268, 0.3534813749138266, 0.35510595177765936, 0.32407557405531406, 0.32492521696258336, 0.3226027829805389, 0.3206464541144669, 0.3197319181635976, 0.327635754016228]
Total Epoch List: [108, 90, 31]
Total Time List: [0.06779721495695412, 0.09310114406980574, 0.06960719497874379]
========================training times:2========================
========================k_idx:0========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: GPSPerformer
GPS(
  (x_lin): Linear(in_features=14887, out_features=64, bias=True)
  (pe_lin): Linear(in_features=20, out_features=8, bias=True)
  (pe_norm): BatchNorm1d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (convs): ModuleList(
    (0-1): 2 x GPSConv(64, conv=GINEConv(nn=Sequential(
      (0): Linear(in_features=64, out_features=64, bias=True)
      (1): ReLU()
      (2): Linear(in_features=64, out_features=64, bias=True)
    )), heads=1, attn_type=performer)
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=32, bias=True)
    (1): ReLU()
    (2): Linear(in_features=32, out_features=16, bias=True)
    (3): ReLU()
    (4): Linear(in_features=16, out_features=2, bias=True)
  )
)
Total number of parameters: 1039522
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7c9d24ca7880>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.6578;  Loss pred: 0.6578; Loss self: 0.0000; time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.8374 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.8393 score: 0.5102 time: 0.06s
Epoch 2/1000, LR 0.000000
Train loss: 0.6702;  Loss pred: 0.6702; Loss self: 0.0000; time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7506 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7591 score: 0.5102 time: 0.06s
Epoch 3/1000, LR 0.000030
Train loss: 0.6697;  Loss pred: 0.6697; Loss self: 0.0000; time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6959 score: 0.4898 time: 0.07s
Test loss: 0.7149 score: 0.5306 time: 0.06s
Epoch 4/1000, LR 0.000060
Train loss: 0.6112;  Loss pred: 0.6112; Loss self: 0.0000; time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6579 score: 0.4898 time: 0.07s
Test loss: 0.6869 score: 0.5306 time: 0.06s
Epoch 5/1000, LR 0.000090
Train loss: 0.6451;  Loss pred: 0.6451; Loss self: 0.0000; time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6310 score: 0.4898 time: 0.07s
Test loss: 0.6710 score: 0.5102 time: 0.06s
Epoch 6/1000, LR 0.000120
Train loss: 0.6280;  Loss pred: 0.6280; Loss self: 0.0000; time: 0.19s
Val loss: 0.6124 score: 0.5102 time: 0.07s
Test loss: 0.6606 score: 0.5102 time: 0.06s
Epoch 7/1000, LR 0.000150
Train loss: 0.6113;  Loss pred: 0.6113; Loss self: 0.0000; time: 0.20s
Val loss: 0.6002 score: 0.5102 time: 0.07s
Test loss: 0.6555 score: 0.4898 time: 0.06s
Epoch 8/1000, LR 0.000180
Train loss: 0.5837;  Loss pred: 0.5837; Loss self: 0.0000; time: 0.21s
Val loss: 0.5917 score: 0.5102 time: 0.07s
Test loss: 0.6538 score: 0.4694 time: 0.08s
Epoch 9/1000, LR 0.000210
Train loss: 0.5508;  Loss pred: 0.5508; Loss self: 0.0000; time: 0.21s
Val loss: 0.5888 score: 0.5306 time: 0.07s
Test loss: 0.6571 score: 0.4694 time: 0.06s
Epoch 10/1000, LR 0.000240
Train loss: 0.5382;  Loss pred: 0.5382; Loss self: 0.0000; time: 0.20s
Val loss: 0.5912 score: 0.5102 time: 0.07s
Test loss: 0.6606 score: 0.5102 time: 0.06s
     INFO: Early stopping counter 1 of 20
Epoch 11/1000, LR 0.000270
Train loss: 0.4832;  Loss pred: 0.4832; Loss self: 0.0000; time: 0.20s
Val loss: 0.5992 score: 0.5306 time: 0.07s
Test loss: 0.6688 score: 0.4898 time: 0.06s
     INFO: Early stopping counter 2 of 20
Epoch 12/1000, LR 0.000270
Train loss: 0.4722;  Loss pred: 0.4722; Loss self: 0.0000; time: 0.20s
Val loss: 0.6010 score: 0.5102 time: 0.07s
Test loss: 0.6693 score: 0.4694 time: 0.06s
     INFO: Early stopping counter 3 of 20
Epoch 13/1000, LR 0.000270
Train loss: 0.4764;  Loss pred: 0.4764; Loss self: 0.0000; time: 0.20s
Val loss: 0.6030 score: 0.5102 time: 0.07s
Test loss: 0.6695 score: 0.4490 time: 0.06s
     INFO: Early stopping counter 4 of 20
Epoch 14/1000, LR 0.000270
Train loss: 0.4184;  Loss pred: 0.4184; Loss self: 0.0000; time: 0.20s
Val loss: 0.6143 score: 0.4694 time: 0.07s
Test loss: 0.6768 score: 0.4286 time: 0.06s
     INFO: Early stopping counter 5 of 20
Epoch 15/1000, LR 0.000270
Train loss: 0.3751;  Loss pred: 0.3751; Loss self: 0.0000; time: 0.20s
Val loss: 0.6321 score: 0.4490 time: 0.07s
Test loss: 0.6852 score: 0.4082 time: 0.06s
     INFO: Early stopping counter 6 of 20
Epoch 16/1000, LR 0.000270
Train loss: 0.3736;  Loss pred: 0.3736; Loss self: 0.0000; time: 0.20s
Val loss: 0.6527 score: 0.4694 time: 0.07s
Test loss: 0.6932 score: 0.4286 time: 0.06s
     INFO: Early stopping counter 7 of 20
Epoch 17/1000, LR 0.000270
Train loss: 0.3427;  Loss pred: 0.3427; Loss self: 0.0000; time: 0.20s
Val loss: 0.6566 score: 0.4694 time: 0.07s
Test loss: 0.6888 score: 0.4286 time: 0.07s
     INFO: Early stopping counter 8 of 20
Epoch 18/1000, LR 0.000270
Train loss: 0.3187;  Loss pred: 0.3187; Loss self: 0.0000; time: 0.20s
Val loss: 0.6605 score: 0.4694 time: 0.07s
Test loss: 0.6856 score: 0.3878 time: 0.06s
     INFO: Early stopping counter 9 of 20
Epoch 19/1000, LR 0.000270
Train loss: 0.2834;  Loss pred: 0.2834; Loss self: 0.0000; time: 0.20s
Val loss: 0.6612 score: 0.4694 time: 0.07s
Test loss: 0.6800 score: 0.4490 time: 0.06s
     INFO: Early stopping counter 10 of 20
Epoch 20/1000, LR 0.000270
Train loss: 0.2382;  Loss pred: 0.2382; Loss self: 0.0000; time: 0.20s
Val loss: 0.6625 score: 0.4898 time: 0.07s
Test loss: 0.6747 score: 0.4286 time: 0.06s
     INFO: Early stopping counter 11 of 20
Epoch 21/1000, LR 0.000270
Train loss: 0.2163;  Loss pred: 0.2163; Loss self: 0.0000; time: 0.20s
Val loss: 0.6702 score: 0.4898 time: 0.07s
Test loss: 0.6711 score: 0.4286 time: 0.06s
     INFO: Early stopping counter 12 of 20
Epoch 22/1000, LR 0.000270
Train loss: 0.1981;  Loss pred: 0.1981; Loss self: 0.0000; time: 0.20s
Val loss: 0.6741 score: 0.4694 time: 0.07s
Test loss: 0.6657 score: 0.4490 time: 0.06s
     INFO: Early stopping counter 13 of 20
Epoch 23/1000, LR 0.000270
Train loss: 0.1869;  Loss pred: 0.1869; Loss self: 0.0000; time: 0.20s
Val loss: 0.6752 score: 0.4490 time: 0.07s
Test loss: 0.6614 score: 0.5102 time: 0.06s
     INFO: Early stopping counter 14 of 20
Epoch 24/1000, LR 0.000270
Train loss: 0.1785;  Loss pred: 0.1785; Loss self: 0.0000; time: 0.20s
Val loss: 0.6792 score: 0.4490 time: 0.07s
Test loss: 0.6612 score: 0.5306 time: 0.06s
     INFO: Early stopping counter 15 of 20
Epoch 25/1000, LR 0.000270
Train loss: 0.1638;  Loss pred: 0.1638; Loss self: 0.0000; time: 0.20s
Val loss: 0.6823 score: 0.4490 time: 0.07s
Test loss: 0.6593 score: 0.5306 time: 0.07s
     INFO: Early stopping counter 16 of 20
Epoch 26/1000, LR 0.000270
Train loss: 0.1513;  Loss pred: 0.1513; Loss self: 0.0000; time: 0.21s
Val loss: 0.6834 score: 0.4490 time: 0.07s
Test loss: 0.6545 score: 0.5306 time: 0.06s
     INFO: Early stopping counter 17 of 20
Epoch 27/1000, LR 0.000270
Train loss: 0.1336;  Loss pred: 0.1336; Loss self: 0.0000; time: 0.21s
Val loss: 0.6823 score: 0.4490 time: 0.07s
Test loss: 0.6468 score: 0.5102 time: 0.06s
     INFO: Early stopping counter 18 of 20
Epoch 28/1000, LR 0.000270
Train loss: 0.1353;  Loss pred: 0.1353; Loss self: 0.0000; time: 0.21s
Val loss: 0.6792 score: 0.4490 time: 0.07s
Test loss: 0.6381 score: 0.5102 time: 0.07s
     INFO: Early stopping counter 19 of 20
Epoch 29/1000, LR 0.000270
Train loss: 0.1159;  Loss pred: 0.1159; Loss self: 0.0000; time: 0.22s
Val loss: 0.6598 score: 0.4490 time: 0.07s
Test loss: 0.6228 score: 0.5102 time: 0.06s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 008,   Train_Loss: 0.5508,   Val_Loss: 0.5888,   Val_Precision: 0.5106,   Val_Recall: 1.0000,   Val_accuracy: 0.6761,   Val_Score: 0.5306,   Val_Loss: 0.5888,   Test_Precision: 0.4865,   Test_Recall: 0.7200,   Test_accuracy: 0.5806,   Test_Score: 0.4694,   Test_loss: 0.6571


[0.06551805592607707, 0.06593708205036819, 0.06583644892089069, 0.06518786097876728, 0.06579508597496897, 0.06549061997793615, 0.06530202808789909, 0.0873524839989841, 0.06478835002053529, 0.06467129394877702, 0.06468775100074708, 0.06497310905251652, 0.06481277896091342, 0.06498708995059133, 0.06475997995585203, 0.06488925707526505, 0.06966209306847304, 0.06508939201012254, 0.06421613995917141, 0.06492818903643638, 0.06454007094725966, 0.06445524003356695, 0.06600539106875658, 0.06461790599860251, 0.06983065302483737, 0.06938169989734888, 0.06945548497606069, 0.06967175402678549, 0.06268204399384558]
[0.0013371031821648382, 0.0013456547357217998, 0.0013436009983855243, 0.0013303645097707607, 0.0013427568566320197, 0.0013365432648558397, 0.0013326944507734508, 0.001782703755081308, 0.0013222112249088834, 0.0013198223254852453, 0.001320158183688716, 0.0013259818173982963, 0.0013227097747125188, 0.0013262671418488026, 0.0013216322439969803, 0.0013242705525564297, 0.0014216753687443478, 0.0013283549389820925, 0.0013105334685545188, 0.0013250650823762526, 0.0013171443050461157, 0.0013154130619095297, 0.001347048797321563, 0.0013187327754816838, 0.001425115367853824, 0.001415953059129569, 0.001417458877062463, 0.0014218725311588875, 0.0012792253876295016]
[747.8854387145718, 743.1326724857153, 744.2685746747759, 751.6736899215036, 744.7364689004513, 748.1987499356113, 750.3595437195929, 560.9456967539683, 756.3088114525062, 757.6777424433553, 757.4849835084558, 754.158154266471, 756.023746945805, 753.9959096068764, 756.640135364528, 755.1327015990474, 703.3954600220866, 752.8108419322713, 763.0480441701131, 754.6799121796266, 759.2182543468449, 760.2174776555299, 742.3636040419428, 758.303743254381, 701.6975766010905, 706.2381012932248, 705.4878389646101, 703.297924452452, 781.7230721578105]
Elapsed: 0.06653535634215021~0.004347352880467123
Time per graph: 0.0013578644151459223~8.872148735647189e-05
Speed: 739.0036162539731~39.28394606502192
Total Time: 0.0631
best val loss: 0.5887770652770996 test_score: 0.4694

Testing...
Test loss: 0.6571 score: 0.4694 time: 0.06s
test Score 0.4694
Epoch Time List: [0.3243853859603405, 0.3222107549663633, 0.32136415294371545, 0.32078198599629104, 0.3252676229458302, 0.32210884196683764, 0.32793034706264734, 0.3565483510028571, 0.3384747669333592, 0.3355271320324391, 0.3299674120498821, 0.3265936269890517, 0.3275762120028958, 0.3284368640743196, 0.3244710109429434, 0.3246677750721574, 0.3332553638610989, 0.3281951319659129, 0.3252182549331337, 0.3259224990615621, 0.3254728780593723, 0.32518224185332656, 0.3292822520015761, 0.328014521044679, 0.33810002205427736, 0.3494074238697067, 0.34760013804771006, 0.3500692439265549, 0.34435553196817636]
Total Epoch List: [29]
Total Time List: [0.06305443798191845]
========================k_idx:1========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: GPSPerformer
GPS(
  (x_lin): Linear(in_features=14887, out_features=64, bias=True)
  (pe_lin): Linear(in_features=20, out_features=8, bias=True)
  (pe_norm): BatchNorm1d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (convs): ModuleList(
    (0-1): 2 x GPSConv(64, conv=GINEConv(nn=Sequential(
      (0): Linear(in_features=64, out_features=64, bias=True)
      (1): ReLU()
      (2): Linear(in_features=64, out_features=64, bias=True)
    )), heads=1, attn_type=performer)
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=32, bias=True)
    (1): ReLU()
    (2): Linear(in_features=32, out_features=16, bias=True)
    (3): ReLU()
    (4): Linear(in_features=16, out_features=2, bias=True)
  )
)
Total number of parameters: 1039522
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7c9d24ca76a0>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.6959;  Loss pred: 0.6959; Loss self: 0.0000; time: 0.20s
Val loss: 0.6786 score: 0.4898 time: 0.06s
Test loss: 0.6690 score: 0.5918 time: 0.07s
Epoch 2/1000, LR 0.000000
Train loss: 0.7144;  Loss pred: 0.7144; Loss self: 0.0000; time: 0.20s
Val loss: 0.7037 score: 0.4898 time: 0.06s
Test loss: 0.7061 score: 0.5918 time: 0.07s
     INFO: Early stopping counter 1 of 20
Epoch 3/1000, LR 0.000030
Train loss: 0.8021;  Loss pred: 0.8021; Loss self: 0.0000; time: 0.19s
Val loss: 0.7319 score: 0.5102 time: 0.06s
Test loss: 0.7470 score: 0.5510 time: 0.07s
     INFO: Early stopping counter 2 of 20
Epoch 4/1000, LR 0.000060
Train loss: 0.6915;  Loss pred: 0.6915; Loss self: 0.0000; time: 0.20s
Val loss: 0.7592 score: 0.4490 time: 0.06s
Test loss: 0.7827 score: 0.4898 time: 0.07s
     INFO: Early stopping counter 3 of 20
Epoch 5/1000, LR 0.000090
Train loss: 0.7347;  Loss pred: 0.7347; Loss self: 0.0000; time: 0.20s
Val loss: 0.7707 score: 0.4898 time: 0.06s
Test loss: 0.8143 score: 0.4082 time: 0.07s
     INFO: Early stopping counter 4 of 20
Epoch 6/1000, LR 0.000120
Train loss: 0.6205;  Loss pred: 0.6205; Loss self: 0.0000; time: 0.20s
Val loss: 0.7678 score: 0.4898 time: 0.06s
Test loss: 0.8348 score: 0.4286 time: 0.07s
     INFO: Early stopping counter 5 of 20
Epoch 7/1000, LR 0.000150
Train loss: 0.6462;  Loss pred: 0.6462; Loss self: 0.0000; time: 0.19s
Val loss: 0.7571 score: 0.4490 time: 0.06s
Test loss: 0.8354 score: 0.3878 time: 0.07s
     INFO: Early stopping counter 6 of 20
Epoch 8/1000, LR 0.000180
Train loss: 0.5921;  Loss pred: 0.5921; Loss self: 0.0000; time: 0.20s
Val loss: 0.7491 score: 0.4694 time: 0.07s
Test loss: 0.8274 score: 0.4082 time: 0.07s
     INFO: Early stopping counter 7 of 20
Epoch 9/1000, LR 0.000210
Train loss: 0.5497;  Loss pred: 0.5497; Loss self: 0.0000; time: 0.21s
Val loss: 0.7425 score: 0.4694 time: 0.06s
Test loss: 0.8110 score: 0.3673 time: 0.07s
     INFO: Early stopping counter 8 of 20
Epoch 10/1000, LR 0.000240
Train loss: 0.5628;  Loss pred: 0.5628; Loss self: 0.0000; time: 0.21s
Val loss: 0.7346 score: 0.4490 time: 0.06s
Test loss: 0.7902 score: 0.4082 time: 0.07s
     INFO: Early stopping counter 9 of 20
Epoch 11/1000, LR 0.000270
Train loss: 0.4951;  Loss pred: 0.4951; Loss self: 0.0000; time: 0.21s
Val loss: 0.7273 score: 0.4286 time: 0.06s
Test loss: 0.7730 score: 0.3878 time: 0.07s
     INFO: Early stopping counter 10 of 20
Epoch 12/1000, LR 0.000270
Train loss: 0.5091;  Loss pred: 0.5091; Loss self: 0.0000; time: 0.21s
Val loss: 0.7213 score: 0.4490 time: 0.06s
Test loss: 0.7642 score: 0.3673 time: 0.07s
     INFO: Early stopping counter 11 of 20
Epoch 13/1000, LR 0.000270
Train loss: 0.4985;  Loss pred: 0.4985; Loss self: 0.0000; time: 0.19s
Val loss: 0.7157 score: 0.4286 time: 0.06s
Test loss: 0.7603 score: 0.3878 time: 0.07s
     INFO: Early stopping counter 12 of 20
Epoch 14/1000, LR 0.000270
Train loss: 0.4745;  Loss pred: 0.4745; Loss self: 0.0000; time: 0.20s
Val loss: 0.7089 score: 0.4694 time: 0.06s
Test loss: 0.7560 score: 0.4082 time: 0.07s
     INFO: Early stopping counter 13 of 20
Epoch 15/1000, LR 0.000270
Train loss: 0.4914;  Loss pred: 0.4914; Loss self: 0.0000; time: 0.19s
Val loss: 0.7020 score: 0.4694 time: 0.06s
Test loss: 0.7482 score: 0.4286 time: 0.07s
     INFO: Early stopping counter 14 of 20
Epoch 16/1000, LR 0.000270
Train loss: 0.4009;  Loss pred: 0.4009; Loss self: 0.0000; time: 0.19s
Val loss: 0.6951 score: 0.4898 time: 0.06s
Test loss: 0.7388 score: 0.4490 time: 0.07s
     INFO: Early stopping counter 15 of 20
Epoch 17/1000, LR 0.000270
Train loss: 0.3952;  Loss pred: 0.3952; Loss self: 0.0000; time: 0.19s
Val loss: 0.6877 score: 0.4898 time: 0.06s
Test loss: 0.7269 score: 0.4490 time: 0.08s
     INFO: Early stopping counter 16 of 20
Epoch 18/1000, LR 0.000270
Train loss: 0.3964;  Loss pred: 0.3964; Loss self: 0.0000; time: 0.21s
Val loss: 0.6807 score: 0.5306 time: 0.06s
Test loss: 0.7154 score: 0.4082 time: 0.07s
     INFO: Early stopping counter 17 of 20
Epoch 19/1000, LR 0.000270
Train loss: 0.3210;  Loss pred: 0.3210; Loss self: 0.0000; time: 0.21s
Val loss: 0.6750 score: 0.5306 time: 0.06s
Test loss: 0.7049 score: 0.4082 time: 0.07s
Epoch 20/1000, LR 0.000270
Train loss: 0.3195;  Loss pred: 0.3195; Loss self: 0.0000; time: 0.21s
Val loss: 0.6698 score: 0.5306 time: 0.06s
Test loss: 0.6977 score: 0.4082 time: 0.07s
Epoch 21/1000, LR 0.000270
Train loss: 0.2913;  Loss pred: 0.2913; Loss self: 0.0000; time: 0.21s
Val loss: 0.6645 score: 0.5306 time: 0.06s
Test loss: 0.6864 score: 0.4490 time: 0.07s
Epoch 22/1000, LR 0.000270
Train loss: 0.3006;  Loss pred: 0.3006; Loss self: 0.0000; time: 0.21s
Val loss: 0.6597 score: 0.5714 time: 0.07s
Test loss: 0.6715 score: 0.5102 time: 0.07s
Epoch 23/1000, LR 0.000270
Train loss: 0.2125;  Loss pred: 0.2125; Loss self: 0.0000; time: 0.21s
Val loss: 0.6523 score: 0.6122 time: 0.06s
Test loss: 0.6538 score: 0.5510 time: 0.08s
Epoch 24/1000, LR 0.000270
Train loss: 0.2174;  Loss pred: 0.2174; Loss self: 0.0000; time: 0.21s
Val loss: 0.6464 score: 0.6531 time: 0.06s
Test loss: 0.6415 score: 0.5510 time: 0.07s
Epoch 25/1000, LR 0.000270
Train loss: 0.1896;  Loss pred: 0.1896; Loss self: 0.0000; time: 0.22s
Val loss: 0.6386 score: 0.6735 time: 0.08s
Test loss: 0.6283 score: 0.6122 time: 0.07s
Epoch 26/1000, LR 0.000270
Train loss: 0.1926;  Loss pred: 0.1926; Loss self: 0.0000; time: 0.19s
Val loss: 0.6307 score: 0.7143 time: 0.06s
Test loss: 0.6131 score: 0.6122 time: 0.07s
Epoch 27/1000, LR 0.000270
Train loss: 0.1960;  Loss pred: 0.1960; Loss self: 0.0000; time: 0.19s
Val loss: 0.6246 score: 0.7551 time: 0.06s
Test loss: 0.5976 score: 0.6327 time: 0.07s
Epoch 28/1000, LR 0.000270
Train loss: 0.1642;  Loss pred: 0.1642; Loss self: 0.0000; time: 0.19s
Val loss: 0.6208 score: 0.7755 time: 0.06s
Test loss: 0.5829 score: 0.6327 time: 0.07s
Epoch 29/1000, LR 0.000270
Train loss: 0.1312;  Loss pred: 0.1312; Loss self: 0.0000; time: 0.19s
Val loss: 0.6167 score: 0.7959 time: 0.06s
Test loss: 0.5705 score: 0.7347 time: 0.07s
Epoch 30/1000, LR 0.000270
Train loss: 0.1249;  Loss pred: 0.1249; Loss self: 0.0000; time: 0.21s
Val loss: 0.6144 score: 0.7347 time: 0.09s
Test loss: 0.5623 score: 0.7347 time: 0.07s
Epoch 31/1000, LR 0.000270
Train loss: 0.1072;  Loss pred: 0.1072; Loss self: 0.0000; time: 0.19s
Val loss: 0.6132 score: 0.7143 time: 0.06s
Test loss: 0.5570 score: 0.7347 time: 0.07s
Epoch 32/1000, LR 0.000270
Train loss: 0.1042;  Loss pred: 0.1042; Loss self: 0.0000; time: 0.19s
Val loss: 0.6142 score: 0.6939 time: 0.06s
Test loss: 0.5549 score: 0.6939 time: 0.07s
     INFO: Early stopping counter 1 of 20
Epoch 33/1000, LR 0.000270
Train loss: 0.0789;  Loss pred: 0.0789; Loss self: 0.0000; time: 0.19s
Val loss: 0.6137 score: 0.6735 time: 0.06s
Test loss: 0.5519 score: 0.6939 time: 0.07s
     INFO: Early stopping counter 2 of 20
Epoch 34/1000, LR 0.000270
Train loss: 0.0794;  Loss pred: 0.0794; Loss self: 0.0000; time: 0.19s
Val loss: 0.6127 score: 0.6735 time: 0.06s
Test loss: 0.5485 score: 0.6939 time: 0.07s
Epoch 35/1000, LR 0.000270
Train loss: 0.0832;  Loss pred: 0.0832; Loss self: 0.0000; time: 0.19s
Val loss: 0.6107 score: 0.6735 time: 0.06s
Test loss: 0.5444 score: 0.6939 time: 0.16s
Epoch 36/1000, LR 0.000270
Train loss: 0.0731;  Loss pred: 0.0731; Loss self: 0.0000; time: 0.19s
Val loss: 0.6066 score: 0.6735 time: 0.06s
Test loss: 0.5374 score: 0.7143 time: 0.07s
Epoch 37/1000, LR 0.000270
Train loss: 0.0728;  Loss pred: 0.0728; Loss self: 0.0000; time: 0.19s
Val loss: 0.6007 score: 0.6735 time: 0.06s
Test loss: 0.5279 score: 0.7143 time: 0.07s
Epoch 38/1000, LR 0.000270
Train loss: 0.0680;  Loss pred: 0.0680; Loss self: 0.0000; time: 0.19s
Val loss: 0.5946 score: 0.6735 time: 0.07s
Test loss: 0.5185 score: 0.7143 time: 0.07s
Epoch 39/1000, LR 0.000269
Train loss: 0.0477;  Loss pred: 0.0477; Loss self: 0.0000; time: 0.19s
Val loss: 0.5858 score: 0.6735 time: 0.06s
Test loss: 0.5076 score: 0.7347 time: 0.07s
Epoch 40/1000, LR 0.000269
Train loss: 0.0429;  Loss pred: 0.0429; Loss self: 0.0000; time: 0.19s
Val loss: 0.5795 score: 0.6939 time: 0.06s
Test loss: 0.4989 score: 0.7347 time: 0.07s
Epoch 41/1000, LR 0.000269
Train loss: 0.0516;  Loss pred: 0.0516; Loss self: 0.0000; time: 0.27s
Val loss: 0.5729 score: 0.6939 time: 0.06s
Test loss: 0.4908 score: 0.7347 time: 0.07s
Epoch 42/1000, LR 0.000269
Train loss: 0.0330;  Loss pred: 0.0330; Loss self: 0.0000; time: 0.20s
Val loss: 0.5675 score: 0.6939 time: 0.06s
Test loss: 0.4834 score: 0.7347 time: 0.07s
Epoch 43/1000, LR 0.000269
Train loss: 0.0269;  Loss pred: 0.0269; Loss self: 0.0000; time: 0.20s
Val loss: 0.5603 score: 0.6939 time: 0.06s
Test loss: 0.4735 score: 0.7347 time: 0.07s
Epoch 44/1000, LR 0.000269
Train loss: 0.0212;  Loss pred: 0.0212; Loss self: 0.0000; time: 0.20s
Val loss: 0.5521 score: 0.6939 time: 0.06s
Test loss: 0.4627 score: 0.7347 time: 0.07s
Epoch 45/1000, LR 0.000269
Train loss: 0.0230;  Loss pred: 0.0230; Loss self: 0.0000; time: 0.20s
Val loss: 0.5422 score: 0.6939 time: 0.06s
Test loss: 0.4496 score: 0.7551 time: 0.07s
Epoch 46/1000, LR 0.000269
Train loss: 0.0192;  Loss pred: 0.0192; Loss self: 0.0000; time: 0.20s
Val loss: 0.5322 score: 0.7143 time: 0.06s
Test loss: 0.4367 score: 0.7755 time: 0.07s
Epoch 47/1000, LR 0.000269
Train loss: 0.0171;  Loss pred: 0.0171; Loss self: 0.0000; time: 0.20s
Val loss: 0.5231 score: 0.7143 time: 0.06s
Test loss: 0.4230 score: 0.7959 time: 0.07s
Epoch 48/1000, LR 0.000269
Train loss: 0.0135;  Loss pred: 0.0135; Loss self: 0.0000; time: 0.20s
Val loss: 0.5126 score: 0.7347 time: 0.06s
Test loss: 0.4087 score: 0.7755 time: 0.07s
Epoch 49/1000, LR 0.000269
Train loss: 0.0168;  Loss pred: 0.0168; Loss self: 0.0000; time: 0.21s
Val loss: 0.5030 score: 0.7551 time: 0.06s
Test loss: 0.3937 score: 0.8163 time: 0.07s
Epoch 50/1000, LR 0.000269
Train loss: 0.0163;  Loss pred: 0.0163; Loss self: 0.0000; time: 0.20s
Val loss: 0.4937 score: 0.7551 time: 0.06s
Test loss: 0.3779 score: 0.8367 time: 0.07s
Epoch 51/1000, LR 0.000269
Train loss: 0.0191;  Loss pred: 0.0191; Loss self: 0.0000; time: 0.20s
Val loss: 0.4863 score: 0.7551 time: 0.06s
Test loss: 0.3635 score: 0.8571 time: 0.07s
Epoch 52/1000, LR 0.000269
Train loss: 0.0135;  Loss pred: 0.0135; Loss self: 0.0000; time: 0.20s
Val loss: 0.4799 score: 0.7551 time: 0.06s
Test loss: 0.3503 score: 0.8980 time: 0.07s
Epoch 53/1000, LR 0.000269
Train loss: 0.0094;  Loss pred: 0.0094; Loss self: 0.0000; time: 0.20s
Val loss: 0.4752 score: 0.7755 time: 0.06s
Test loss: 0.3370 score: 0.9184 time: 0.07s
Epoch 54/1000, LR 0.000269
Train loss: 0.0101;  Loss pred: 0.0101; Loss self: 0.0000; time: 0.20s
Val loss: 0.4718 score: 0.7755 time: 0.06s
Test loss: 0.3256 score: 0.9184 time: 0.07s
Epoch 55/1000, LR 0.000269
Train loss: 0.0116;  Loss pred: 0.0116; Loss self: 0.0000; time: 0.20s
Val loss: 0.4700 score: 0.7959 time: 0.06s
Test loss: 0.3143 score: 0.9388 time: 0.07s
Epoch 56/1000, LR 0.000269
Train loss: 0.0085;  Loss pred: 0.0085; Loss self: 0.0000; time: 0.20s
Val loss: 0.4675 score: 0.7755 time: 0.06s
Test loss: 0.3028 score: 0.9184 time: 0.07s
Epoch 57/1000, LR 0.000269
Train loss: 0.0110;  Loss pred: 0.0110; Loss self: 0.0000; time: 0.20s
Val loss: 0.4630 score: 0.7755 time: 0.06s
Test loss: 0.2917 score: 0.9184 time: 0.07s
Epoch 58/1000, LR 0.000269
Train loss: 0.0073;  Loss pred: 0.0073; Loss self: 0.0000; time: 0.20s
Val loss: 0.4593 score: 0.7755 time: 0.06s
Test loss: 0.2819 score: 0.9184 time: 0.07s
Epoch 59/1000, LR 0.000268
Train loss: 0.0068;  Loss pred: 0.0068; Loss self: 0.0000; time: 0.20s
Val loss: 0.4572 score: 0.7959 time: 0.06s
Test loss: 0.2730 score: 0.9184 time: 0.07s
Epoch 60/1000, LR 0.000268
Train loss: 0.0064;  Loss pred: 0.0064; Loss self: 0.0000; time: 0.20s
Val loss: 0.4530 score: 0.7959 time: 0.06s
Test loss: 0.2636 score: 0.9184 time: 0.07s
Epoch 61/1000, LR 0.000268
Train loss: 0.0088;  Loss pred: 0.0088; Loss self: 0.0000; time: 0.20s
Val loss: 0.4498 score: 0.8163 time: 0.06s
Test loss: 0.2542 score: 0.8980 time: 0.07s
Epoch 62/1000, LR 0.000268
Train loss: 0.0059;  Loss pred: 0.0059; Loss self: 0.0000; time: 0.20s
Val loss: 0.4470 score: 0.8163 time: 0.06s
Test loss: 0.2450 score: 0.8980 time: 0.07s
Epoch 63/1000, LR 0.000268
Train loss: 0.0050;  Loss pred: 0.0050; Loss self: 0.0000; time: 0.20s
Val loss: 0.4416 score: 0.8163 time: 0.06s
Test loss: 0.2350 score: 0.8980 time: 0.07s
Epoch 64/1000, LR 0.000268
Train loss: 0.0050;  Loss pred: 0.0050; Loss self: 0.0000; time: 0.20s
Val loss: 0.4387 score: 0.8163 time: 0.06s
Test loss: 0.2264 score: 0.8980 time: 0.07s
Epoch 65/1000, LR 0.000268
Train loss: 0.0046;  Loss pred: 0.0046; Loss self: 0.0000; time: 0.20s
Val loss: 0.4353 score: 0.8163 time: 0.06s
Test loss: 0.2187 score: 0.8980 time: 0.07s
Epoch 66/1000, LR 0.000268
Train loss: 0.0043;  Loss pred: 0.0043; Loss self: 0.0000; time: 0.20s
Val loss: 0.4325 score: 0.8163 time: 0.06s
Test loss: 0.2118 score: 0.8980 time: 0.07s
Epoch 67/1000, LR 0.000268
Train loss: 0.0051;  Loss pred: 0.0051; Loss self: 0.0000; time: 0.20s
Val loss: 0.4254 score: 0.8163 time: 0.06s
Test loss: 0.2028 score: 0.8980 time: 0.07s
Epoch 68/1000, LR 0.000268
Train loss: 0.0050;  Loss pred: 0.0050; Loss self: 0.0000; time: 0.20s
Val loss: 0.4217 score: 0.8163 time: 0.06s
Test loss: 0.1952 score: 0.8980 time: 0.07s
Epoch 69/1000, LR 0.000268
Train loss: 0.0038;  Loss pred: 0.0038; Loss self: 0.0000; time: 0.20s
Val loss: 0.4178 score: 0.8163 time: 0.06s
Test loss: 0.1887 score: 0.8980 time: 0.07s
Epoch 70/1000, LR 0.000268
Train loss: 0.0034;  Loss pred: 0.0034; Loss self: 0.0000; time: 0.20s
Val loss: 0.4152 score: 0.8163 time: 0.06s
Test loss: 0.1825 score: 0.8980 time: 0.07s
Epoch 71/1000, LR 0.000268
Train loss: 0.0028;  Loss pred: 0.0028; Loss self: 0.0000; time: 0.20s
Val loss: 0.4163 score: 0.8367 time: 0.06s
Test loss: 0.1781 score: 0.8980 time: 0.07s
     INFO: Early stopping counter 1 of 20
Epoch 72/1000, LR 0.000267
Train loss: 0.0026;  Loss pred: 0.0026; Loss self: 0.0000; time: 0.20s
Val loss: 0.4189 score: 0.8367 time: 0.06s
Test loss: 0.1738 score: 0.8776 time: 0.07s
     INFO: Early stopping counter 2 of 20
Epoch 73/1000, LR 0.000267
Train loss: 0.0024;  Loss pred: 0.0024; Loss self: 0.0000; time: 0.20s
Val loss: 0.4235 score: 0.8367 time: 0.06s
Test loss: 0.1701 score: 0.8776 time: 0.07s
     INFO: Early stopping counter 3 of 20
Epoch 74/1000, LR 0.000267
Train loss: 0.0025;  Loss pred: 0.0025; Loss self: 0.0000; time: 0.20s
Val loss: 0.4305 score: 0.8367 time: 0.06s
Test loss: 0.1671 score: 0.8776 time: 0.07s
     INFO: Early stopping counter 4 of 20
Epoch 75/1000, LR 0.000267
Train loss: 0.0023;  Loss pred: 0.0023; Loss self: 0.0000; time: 0.20s
Val loss: 0.4380 score: 0.8163 time: 0.06s
Test loss: 0.1638 score: 0.8980 time: 0.07s
     INFO: Early stopping counter 5 of 20
Epoch 76/1000, LR 0.000267
Train loss: 0.0021;  Loss pred: 0.0021; Loss self: 0.0000; time: 0.20s
Val loss: 0.4486 score: 0.8163 time: 0.06s
Test loss: 0.1622 score: 0.8980 time: 0.07s
     INFO: Early stopping counter 6 of 20
Epoch 77/1000, LR 0.000267
Train loss: 0.0026;  Loss pred: 0.0026; Loss self: 0.0000; time: 0.20s
Val loss: 0.4579 score: 0.8163 time: 0.06s
Test loss: 0.1589 score: 0.9184 time: 0.07s
     INFO: Early stopping counter 7 of 20
Epoch 78/1000, LR 0.000267
Train loss: 0.0023;  Loss pred: 0.0023; Loss self: 0.0000; time: 0.20s
Val loss: 0.4732 score: 0.8163 time: 0.06s
Test loss: 0.1592 score: 0.9184 time: 0.07s
     INFO: Early stopping counter 8 of 20
Epoch 79/1000, LR 0.000267
Train loss: 0.0025;  Loss pred: 0.0025; Loss self: 0.0000; time: 0.20s
Val loss: 0.4891 score: 0.8163 time: 0.06s
Test loss: 0.1595 score: 0.9184 time: 0.07s
     INFO: Early stopping counter 9 of 20
Epoch 80/1000, LR 0.000267
Train loss: 0.0020;  Loss pred: 0.0020; Loss self: 0.0000; time: 0.20s
Val loss: 0.5047 score: 0.8163 time: 0.06s
Test loss: 0.1601 score: 0.9184 time: 0.07s
     INFO: Early stopping counter 10 of 20
Epoch 81/1000, LR 0.000267
Train loss: 0.0018;  Loss pred: 0.0018; Loss self: 0.0000; time: 0.20s
Val loss: 0.5219 score: 0.8163 time: 0.06s
Test loss: 0.1615 score: 0.8980 time: 0.07s
     INFO: Early stopping counter 11 of 20
Epoch 82/1000, LR 0.000267
Train loss: 0.0022;  Loss pred: 0.0022; Loss self: 0.0000; time: 0.20s
Val loss: 0.5380 score: 0.8163 time: 0.06s
Test loss: 0.1635 score: 0.8980 time: 0.07s
     INFO: Early stopping counter 12 of 20
Epoch 83/1000, LR 0.000266
Train loss: 0.0019;  Loss pred: 0.0019; Loss self: 0.0000; time: 0.20s
Val loss: 0.5530 score: 0.8163 time: 0.06s
Test loss: 0.1648 score: 0.8980 time: 0.07s
     INFO: Early stopping counter 13 of 20
Epoch 84/1000, LR 0.000266
Train loss: 0.0019;  Loss pred: 0.0019; Loss self: 0.0000; time: 0.20s
Val loss: 0.5658 score: 0.8163 time: 0.06s
Test loss: 0.1648 score: 0.9184 time: 0.07s
     INFO: Early stopping counter 14 of 20
Epoch 85/1000, LR 0.000266
Train loss: 0.0017;  Loss pred: 0.0017; Loss self: 0.0000; time: 0.20s
Val loss: 0.5780 score: 0.8163 time: 0.06s
Test loss: 0.1651 score: 0.9184 time: 0.07s
     INFO: Early stopping counter 15 of 20
Epoch 86/1000, LR 0.000266
Train loss: 0.0018;  Loss pred: 0.0018; Loss self: 0.0000; time: 0.20s
Val loss: 0.5941 score: 0.7959 time: 0.06s
Test loss: 0.1688 score: 0.9184 time: 0.07s
     INFO: Early stopping counter 16 of 20
Epoch 87/1000, LR 0.000266
Train loss: 0.0016;  Loss pred: 0.0016; Loss self: 0.0000; time: 0.20s
Val loss: 0.6102 score: 0.7959 time: 0.06s
Test loss: 0.1727 score: 0.9184 time: 0.07s
     INFO: Early stopping counter 17 of 20
Epoch 88/1000, LR 0.000266
Train loss: 0.0018;  Loss pred: 0.0018; Loss self: 0.0000; time: 0.20s
Val loss: 0.6259 score: 0.7959 time: 0.06s
Test loss: 0.1763 score: 0.9184 time: 0.07s
     INFO: Early stopping counter 18 of 20
Epoch 89/1000, LR 0.000266
Train loss: 0.0016;  Loss pred: 0.0016; Loss self: 0.0000; time: 0.20s
Val loss: 0.6434 score: 0.7959 time: 0.06s
Test loss: 0.1814 score: 0.9184 time: 0.07s
     INFO: Early stopping counter 19 of 20
Epoch 90/1000, LR 0.000266
Train loss: 0.0017;  Loss pred: 0.0017; Loss self: 0.0000; time: 0.20s
Val loss: 0.6617 score: 0.7959 time: 0.06s
Test loss: 0.1864 score: 0.9184 time: 0.07s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 069,   Train_Loss: 0.0034,   Val_Loss: 0.4152,   Val_Precision: 1.0000,   Val_Recall: 0.6400,   Val_accuracy: 0.7805,   Val_Score: 0.8163,   Val_Loss: 0.4152,   Test_Precision: 1.0000,   Test_Recall: 0.7917,   Test_accuracy: 0.8837,   Test_Score: 0.8980,   Test_loss: 0.1825


[0.06551805592607707, 0.06593708205036819, 0.06583644892089069, 0.06518786097876728, 0.06579508597496897, 0.06549061997793615, 0.06530202808789909, 0.0873524839989841, 0.06478835002053529, 0.06467129394877702, 0.06468775100074708, 0.06497310905251652, 0.06481277896091342, 0.06498708995059133, 0.06475997995585203, 0.06488925707526505, 0.06966209306847304, 0.06508939201012254, 0.06421613995917141, 0.06492818903643638, 0.06454007094725966, 0.06445524003356695, 0.06600539106875658, 0.06461790599860251, 0.06983065302483737, 0.06938169989734888, 0.06945548497606069, 0.06967175402678549, 0.06268204399384558, 0.07595257204957306, 0.0709757930599153, 0.07151052099652588, 0.07178896793629974, 0.07151582406368107, 0.07208174595143646, 0.07142971199937165, 0.07761065603699535, 0.07792094198521227, 0.07736591505818069, 0.0774561739526689, 0.07157329306937754, 0.07288609398528934, 0.07193993194960058, 0.07115531095769256, 0.07131258305162191, 0.0804967270232737, 0.0776639599353075, 0.07767517201136798, 0.07757513597607613, 0.07772865204606205, 0.07740414701402187, 0.08280242700129747, 0.07501279201824218, 0.07514620700385422, 0.07165675004944205, 0.0722780890064314, 0.0734831930603832, 0.07157503301277757, 0.07149815303273499, 0.07151708204764873, 0.07175754196941853, 0.07188682805281132, 0.07145667297299951, 0.16509054391644895, 0.0720897139981389, 0.0731007499853149, 0.07279925199691206, 0.07301875401753932, 0.07934323907829821, 0.0752874530153349, 0.07596274395473301, 0.07590584398712963, 0.07725674705579877, 0.07552563399076462, 0.07562406605575234, 0.07607527100481093, 0.0768202330218628, 0.07547867693938315, 0.0751662690890953, 0.07496168301440775, 0.07687002595048398, 0.07534361607395113, 0.07578815298620611, 0.07553746306803077, 0.07549895194824785, 0.07534386496990919, 0.07576936797704548, 0.07567510195076466, 0.07574153901077807, 0.07580443599727005, 0.07574654906056821, 0.07589410000946373, 0.07619404199067503, 0.07576866797171533, 0.0754954069852829, 0.07552675902843475, 0.07548474194481969, 0.0757932469714433, 0.07561410998459905, 0.0755823600338772, 0.07542227010708302, 0.07558933808468282, 0.0759076529648155, 0.07560227799694985, 0.0758562230039388, 0.07501816807780415, 0.07618317904416472, 0.07542742101941258, 0.07570521091111004, 0.07538525701966137, 0.07587351696565747, 0.07609854091424495, 0.07591776200570166, 0.07544106594286859, 0.07542553602252156, 0.07526099402457476, 0.07596257200930268, 0.07573553209658712, 0.07614157698117197]
[0.0013371031821648382, 0.0013456547357217998, 0.0013436009983855243, 0.0013303645097707607, 0.0013427568566320197, 0.0013365432648558397, 0.0013326944507734508, 0.001782703755081308, 0.0013222112249088834, 0.0013198223254852453, 0.001320158183688716, 0.0013259818173982963, 0.0013227097747125188, 0.0013262671418488026, 0.0013216322439969803, 0.0013242705525564297, 0.0014216753687443478, 0.0013283549389820925, 0.0013105334685545188, 0.0013250650823762526, 0.0013171443050461157, 0.0013154130619095297, 0.001347048797321563, 0.0013187327754816838, 0.001425115367853824, 0.001415953059129569, 0.001417458877062463, 0.0014218725311588875, 0.0012792253876295016, 0.0015500524908076136, 0.0014484855726513328, 0.0014593983876842018, 0.0014650809782918316, 0.0014595066135445116, 0.001471056039825234, 0.0014577492244769723, 0.0015838909395305173, 0.0015902233058206585, 0.0015788962256771569, 0.0015807382439320184, 0.00146067945039546, 0.0014874713058222312, 0.0014681618765224607, 0.0014521492032182155, 0.0014553588377882022, 0.0016427903474137491, 0.001584978774189949, 0.0015852075920687343, 0.0015831660403280842, 0.001586299021348205, 0.0015796764696739158, 0.001689845449006071, 0.0015308733064947383, 0.0015335960613031472, 0.001462382654070246, 0.0014750630409475795, 0.0014996570012323102, 0.0014607149594444402, 0.0014591459802598978, 0.0014595322866867088, 0.0014644396320289495, 0.0014670781235267617, 0.0014582994484285616, 0.0033691947738050806, 0.001471218653023243, 0.0014918520405166307, 0.001485699020345144, 0.0014901786534191699, 0.0016192497771081267, 0.0015364786329660186, 0.001550260080708837, 0.0015490988568801964, 0.0015766683072611994, 0.0015413394691992778, 0.0015433482868520885, 0.0015525565511185903, 0.0015677598575890368, 0.0015403811620282276, 0.0015340054916141896, 0.0015298302656001582, 0.0015687760398057954, 0.0015376248178357373, 0.0015466969997184922, 0.0015415808789394035, 0.0015407949377193439, 0.0015376298973450856, 0.0015463136321846017, 0.0015443898357298909, 0.0015457456940975115, 0.0015470293060667357, 0.0015458479400115963, 0.0015488591838666067, 0.0015549804487892864, 0.0015462993463615374, 0.0015407225915363856, 0.0015413624291517297, 0.0015405049376493814, 0.0015468009586008837, 0.0015431451017265112, 0.001542497143548514, 0.0015392300021853677, 0.001542639552748629, 0.001549135774792153, 0.0015429036325908133, 0.001548086183753853, 0.001530983021996003, 0.0015547587560033615, 0.0015393351228451546, 0.0015450043043083682, 0.0015384746330543136, 0.0015484391217481118, 0.0015530314472294888, 0.0015493420817490136, 0.0015396135906707875, 0.001539296653520848, 0.00153593865356275, 0.001550256571618422, 0.001545623104011982, 0.001553909734309632]
[747.8854387145718, 743.1326724857153, 744.2685746747759, 751.6736899215036, 744.7364689004513, 748.1987499356113, 750.3595437195929, 560.9456967539683, 756.3088114525062, 757.6777424433553, 757.4849835084558, 754.158154266471, 756.023746945805, 753.9959096068764, 756.640135364528, 755.1327015990474, 703.3954600220866, 752.8108419322713, 763.0480441701131, 754.6799121796266, 759.2182543468449, 760.2174776555299, 742.3636040419428, 758.303743254381, 701.6975766010905, 706.2381012932248, 705.4878389646101, 703.297924452452, 781.7230721578105, 645.1394426513754, 690.3762238857395, 685.2138582849999, 682.5561281711001, 685.1630480600782, 679.783755973568, 685.989046132946, 631.356601039975, 628.8425005090307, 633.3538479206384, 632.6158071007019, 684.6129037615084, 672.2818760172512, 681.1238024846652, 688.6344721216152, 687.1157641917104, 608.720401586425, 630.9232756199401, 630.8322045663281, 631.6456862558566, 630.3981699176073, 633.0410177005579, 591.7700938794003, 653.2219196438364, 652.0621858863324, 683.8155507498003, 677.9371269160134, 666.8191454301029, 684.5962612584827, 685.332388622202, 685.1509960564865, 682.855051262524, 681.6269590306916, 685.7302189050287, 296.80682392565456, 679.7086197521189, 670.3077603149562, 673.0838388570043, 671.060478356424, 617.5699476000138, 650.838858767336, 645.053054286712, 645.5365941034565, 634.2488114935735, 648.7863445938341, 647.9418861698831, 644.0989214077368, 637.8527905018818, 649.1899697626106, 651.8881486843499, 653.6672874671467, 637.4396182923559, 650.3537068343735, 646.5390442872817, 648.6847454205534, 649.0156318141735, 650.3515584124813, 646.699336529304, 647.5049089709867, 646.9369468849487, 646.4001658394325, 646.8941569974202, 645.6364854961041, 643.094902433406, 646.7053112018789, 649.046106997636, 648.7766803491753, 649.1378090133716, 646.4955910710855, 648.0272003463406, 648.2994177217731, 649.6754861718002, 648.2395697804001, 645.521210130319, 648.1286185844411, 645.9588687595967, 653.1751075176918, 643.1866012258932, 649.6311200589635, 647.2473877331086, 649.9944675816416, 645.8116344096558, 643.9019646279138, 645.4352539570377, 649.5136221578261, 649.64735531172, 651.0676697148083, 645.0545143995291, 646.9882582657407, 643.5380240695114]
Elapsed: 0.07373570929821424~0.009612793883869057
Time per graph: 0.0015048103938411067~0.00019617946701773584
Speed: 671.286880658623~56.51514071814592
Total Time: 0.0767
best val loss: 0.41523098945617676 test_score: 0.8980

Testing...
Test loss: 0.1781 score: 0.8980 time: 0.07s
test Score 0.8980
Epoch Time List: [0.3243853859603405, 0.3222107549663633, 0.32136415294371545, 0.32078198599629104, 0.3252676229458302, 0.32210884196683764, 0.32793034706264734, 0.3565483510028571, 0.3384747669333592, 0.3355271320324391, 0.3299674120498821, 0.3265936269890517, 0.3275762120028958, 0.3284368640743196, 0.3244710109429434, 0.3246677750721574, 0.3332553638610989, 0.3281951319659129, 0.3252182549331337, 0.3259224990615621, 0.3254728780593723, 0.32518224185332656, 0.3292822520015761, 0.328014521044679, 0.33810002205427736, 0.3494074238697067, 0.34760013804771006, 0.3500692439265549, 0.34435553196817636, 0.33982584497425705, 0.32202103384770453, 0.3213520720601082, 0.32234160602092743, 0.32199360302183777, 0.32245190313551575, 0.3210727469995618, 0.339111007982865, 0.35154087899718434, 0.34458953607827425, 0.3478604359552264, 0.3369269350077957, 0.3220441520679742, 0.3223309140885249, 0.31824981700628996, 0.32023131602909416, 0.32917697285301983, 0.34899631701409817, 0.3494256160920486, 0.3485374728916213, 0.34865538985468447, 0.3493479179451242, 0.35019224393181503, 0.346629866049625, 0.3714330110233277, 0.3165031939279288, 0.31701087206602097, 0.3211045399075374, 0.31913305493071675, 0.3633704581297934, 0.3149131149984896, 0.31515628716442734, 0.31793745001778007, 0.32036112691275775, 0.41144709498621523, 0.3186425919411704, 0.31854435394052416, 0.3333812780911103, 0.32193158892914653, 0.3246602601138875, 0.40307455882430077, 0.3381644629407674, 0.3364517339505255, 0.33751869900152087, 0.33324358600657433, 0.33475844690110534, 0.3350575868971646, 0.33842115791048855, 0.34008187195286155, 0.33370154502335936, 0.3339357660152018, 0.33553789800498635, 0.33707583707291633, 0.33497123897541314, 0.33591059199534357, 0.334528508130461, 0.3344549030298367, 0.33531493495684117, 0.3338740919716656, 0.3340314090019092, 0.33531780808698386, 0.33420963503886014, 0.3356986490543932, 0.335843920125626, 0.33473677502479404, 0.3344458091305569, 0.3364695670315996, 0.33597618096973747, 0.3337009579408914, 0.3335538530955091, 0.3332582488656044, 0.33324936311692, 0.33501232496928424, 0.33435696305241436, 0.33450359699781984, 0.33454564795829356, 0.3332870260346681, 0.33499178290367126, 0.33749218890443444, 0.33408150309696794, 0.33383169304579496, 0.3376185899833217, 0.33743751503061503, 0.33468605601228774, 0.33316005603410304, 0.3337661400437355, 0.33360917889513075, 0.33496450015809387, 0.33410874602850527, 0.3350568929454312]
Total Epoch List: [29, 90]
Total Time List: [0.06305443798191845, 0.0767055778997019]
========================k_idx:2========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: GPSPerformer
GPS(
  (x_lin): Linear(in_features=14887, out_features=64, bias=True)
  (pe_lin): Linear(in_features=20, out_features=8, bias=True)
  (pe_norm): BatchNorm1d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (convs): ModuleList(
    (0-1): 2 x GPSConv(64, conv=GINEConv(nn=Sequential(
      (0): Linear(in_features=64, out_features=64, bias=True)
      (1): ReLU()
      (2): Linear(in_features=64, out_features=64, bias=True)
    )), heads=1, attn_type=performer)
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=32, bias=True)
    (1): ReLU()
    (2): Linear(in_features=32, out_features=16, bias=True)
    (3): ReLU()
    (4): Linear(in_features=16, out_features=2, bias=True)
  )
)
Total number of parameters: 1039522
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7c9d24ca74f0>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.8386;  Loss pred: 0.8386; Loss self: 0.0000; time: 0.20s
Val loss: 1.1370 score: 0.4694 time: 0.06s
Test loss: 1.1427 score: 0.4375 time: 0.07s
Epoch 2/1000, LR 0.000000
Train loss: 0.8177;  Loss pred: 0.8177; Loss self: 0.0000; time: 0.20s
Val loss: 1.1112 score: 0.4898 time: 0.06s
Test loss: 1.1149 score: 0.4792 time: 0.06s
Epoch 3/1000, LR 0.000030
Train loss: 0.8576;  Loss pred: 0.8576; Loss self: 0.0000; time: 0.20s
Val loss: 1.0781 score: 0.4898 time: 0.06s
Test loss: 1.0765 score: 0.4792 time: 0.06s
Epoch 4/1000, LR 0.000060
Train loss: 0.7355;  Loss pred: 0.7355; Loss self: 0.0000; time: 0.21s
Val loss: 1.0508 score: 0.4898 time: 0.07s
Test loss: 1.0471 score: 0.4792 time: 0.07s
Epoch 5/1000, LR 0.000090
Train loss: 0.6720;  Loss pred: 0.6720; Loss self: 0.0000; time: 0.22s
Val loss: 1.0361 score: 0.5102 time: 0.07s
Test loss: 1.0416 score: 0.4792 time: 0.07s
Epoch 6/1000, LR 0.000120
Train loss: 0.6939;  Loss pred: 0.6939; Loss self: 0.0000; time: 0.22s
Val loss: 0.9958 score: 0.5102 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 1.0136 score: 0.5000 time: 0.07s
Epoch 7/1000, LR 0.000150
Train loss: 0.6513;  Loss pred: 0.6513; Loss self: 0.0000; time: 0.22s
Val loss: 0.9704 score: 0.4898 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.9835 score: 0.5000 time: 0.07s
Epoch 8/1000, LR 0.000180
Train loss: 0.5910;  Loss pred: 0.5910; Loss self: 0.0000; time: 0.22s
Val loss: 0.9319 score: 0.5714 time: 0.07s
Test loss: 0.9439 score: 0.5625 time: 0.07s
Epoch 9/1000, LR 0.000210
Train loss: 0.5693;  Loss pred: 0.5693; Loss self: 0.0000; time: 0.22s
Val loss: 0.8741 score: 0.5714 time: 0.07s
Test loss: 0.8935 score: 0.6042 time: 0.07s
Epoch 10/1000, LR 0.000240
Train loss: 0.5516;  Loss pred: 0.5516; Loss self: 0.0000; time: 0.22s
Val loss: 0.8324 score: 0.5510 time: 0.07s
Test loss: 0.8610 score: 0.6042 time: 0.07s
Epoch 11/1000, LR 0.000270
Train loss: 0.5308;  Loss pred: 0.5308; Loss self: 0.0000; time: 0.22s
Val loss: 0.8132 score: 0.5510 time: 0.07s
Test loss: 0.8459 score: 0.6042 time: 0.07s
Epoch 12/1000, LR 0.000270
Train loss: 0.5009;  Loss pred: 0.5009; Loss self: 0.0000; time: 0.22s
Val loss: 0.8077 score: 0.5714 time: 0.07s
Test loss: 0.8407 score: 0.6042 time: 0.07s
Epoch 13/1000, LR 0.000270
Train loss: 0.5294;  Loss pred: 0.5294; Loss self: 0.0000; time: 0.22s
Val loss: 0.8328 score: 0.5714 time: 0.07s
Test loss: 0.8637 score: 0.6042 time: 0.07s
     INFO: Early stopping counter 1 of 20
Epoch 14/1000, LR 0.000270
Train loss: 0.4764;  Loss pred: 0.4764; Loss self: 0.0000; time: 0.22s
Val loss: 0.8657 score: 0.5714 time: 0.07s
Test loss: 0.8933 score: 0.6042 time: 0.07s
     INFO: Early stopping counter 2 of 20
Epoch 15/1000, LR 0.000270
Train loss: 0.4167;  Loss pred: 0.4167; Loss self: 0.0000; time: 0.22s
Val loss: 0.8958 score: 0.5918 time: 0.08s
Test loss: 0.9199 score: 0.6042 time: 0.07s
     INFO: Early stopping counter 3 of 20
Epoch 16/1000, LR 0.000270
Train loss: 0.3535;  Loss pred: 0.3535; Loss self: 0.0000; time: 0.21s
Val loss: 0.9003 score: 0.5918 time: 0.07s
Test loss: 0.9237 score: 0.6042 time: 0.07s
     INFO: Early stopping counter 4 of 20
Epoch 17/1000, LR 0.000270
Train loss: 0.3715;  Loss pred: 0.3715; Loss self: 0.0000; time: 0.23s
Val loss: 0.8884 score: 0.5918 time: 0.07s
Test loss: 0.9123 score: 0.6042 time: 0.07s
     INFO: Early stopping counter 5 of 20
Epoch 18/1000, LR 0.000270
Train loss: 0.3442;  Loss pred: 0.3442; Loss self: 0.0000; time: 0.21s
Val loss: 0.8716 score: 0.5918 time: 0.07s
Test loss: 0.8962 score: 0.6042 time: 0.07s
     INFO: Early stopping counter 6 of 20
Epoch 19/1000, LR 0.000270
Train loss: 0.3205;  Loss pred: 0.3205; Loss self: 0.0000; time: 0.22s
Val loss: 0.8593 score: 0.5918 time: 0.07s
Test loss: 0.8852 score: 0.6042 time: 0.07s
     INFO: Early stopping counter 7 of 20
Epoch 20/1000, LR 0.000270
Train loss: 0.2998;  Loss pred: 0.2998; Loss self: 0.0000; time: 0.20s
Val loss: 0.8593 score: 0.5918 time: 0.07s
Test loss: 0.8852 score: 0.6042 time: 0.07s
     INFO: Early stopping counter 8 of 20
Epoch 21/1000, LR 0.000270
Train loss: 0.2426;  Loss pred: 0.2426; Loss self: 0.0000; time: 0.20s
Val loss: 0.8524 score: 0.5918 time: 0.07s
Test loss: 0.8784 score: 0.6042 time: 0.07s
     INFO: Early stopping counter 9 of 20
Epoch 22/1000, LR 0.000270
Train loss: 0.2509;  Loss pred: 0.2509; Loss self: 0.0000; time: 0.20s
Val loss: 0.8196 score: 0.6122 time: 0.07s
Test loss: 0.8444 score: 0.6250 time: 0.07s
     INFO: Early stopping counter 10 of 20
Epoch 23/1000, LR 0.000270
Train loss: 0.2197;  Loss pred: 0.2197; Loss self: 0.0000; time: 0.20s
Val loss: 0.7680 score: 0.5918 time: 0.07s
Test loss: 0.7882 score: 0.6250 time: 0.07s
Epoch 24/1000, LR 0.000270
Train loss: 0.1839;  Loss pred: 0.1839; Loss self: 0.0000; time: 0.20s
Val loss: 0.7156 score: 0.5918 time: 0.07s
Test loss: 0.7285 score: 0.6458 time: 0.07s
Epoch 25/1000, LR 0.000270
Train loss: 0.1663;  Loss pred: 0.1663; Loss self: 0.0000; time: 0.20s
Val loss: 0.6782 score: 0.6122 time: 0.07s
Test loss: 0.6829 score: 0.6458 time: 0.07s
Epoch 26/1000, LR 0.000270
Train loss: 0.1550;  Loss pred: 0.1550; Loss self: 0.0000; time: 0.20s
Val loss: 0.6501 score: 0.6327 time: 0.07s
Test loss: 0.6486 score: 0.6458 time: 0.07s
Epoch 27/1000, LR 0.000270
Train loss: 0.1414;  Loss pred: 0.1414; Loss self: 0.0000; time: 0.20s
Val loss: 0.6429 score: 0.6122 time: 0.07s
Test loss: 0.6367 score: 0.6458 time: 0.07s
Epoch 28/1000, LR 0.000270
Train loss: 0.1085;  Loss pred: 0.1085; Loss self: 0.0000; time: 0.21s
Val loss: 0.6411 score: 0.6122 time: 0.07s
Test loss: 0.6303 score: 0.6458 time: 0.07s
Epoch 29/1000, LR 0.000270
Train loss: 0.1006;  Loss pred: 0.1006; Loss self: 0.0000; time: 0.21s
Val loss: 0.6306 score: 0.6122 time: 0.07s
Test loss: 0.6148 score: 0.6667 time: 0.07s
Epoch 30/1000, LR 0.000270
Train loss: 0.0938;  Loss pred: 0.0938; Loss self: 0.0000; time: 0.22s
Val loss: 0.6248 score: 0.6327 time: 0.07s
Test loss: 0.6053 score: 0.6667 time: 0.07s
Epoch 31/1000, LR 0.000270
Train loss: 0.0990;  Loss pred: 0.0990; Loss self: 0.0000; time: 0.21s
Val loss: 0.6203 score: 0.6327 time: 0.07s
Test loss: 0.5975 score: 0.7083 time: 0.07s
Epoch 32/1000, LR 0.000270
Train loss: 0.0742;  Loss pred: 0.0742; Loss self: 0.0000; time: 0.22s
Val loss: 0.6154 score: 0.6939 time: 0.07s
Test loss: 0.5902 score: 0.7292 time: 0.07s
Epoch 33/1000, LR 0.000270
Train loss: 0.0663;  Loss pred: 0.0663; Loss self: 0.0000; time: 0.21s
Val loss: 0.6067 score: 0.6939 time: 0.07s
Test loss: 0.5812 score: 0.7500 time: 0.07s
Epoch 34/1000, LR 0.000270
Train loss: 0.0568;  Loss pred: 0.0568; Loss self: 0.0000; time: 0.21s
Val loss: 0.5991 score: 0.7143 time: 0.07s
Test loss: 0.5770 score: 0.7500 time: 0.07s
Epoch 35/1000, LR 0.000270
Train loss: 0.0506;  Loss pred: 0.0506; Loss self: 0.0000; time: 0.21s
Val loss: 0.5914 score: 0.7143 time: 0.07s
Test loss: 0.5747 score: 0.7500 time: 0.07s
Epoch 36/1000, LR 0.000270
Train loss: 0.0435;  Loss pred: 0.0435; Loss self: 0.0000; time: 0.20s
Val loss: 0.5854 score: 0.6939 time: 0.07s
Test loss: 0.5735 score: 0.7500 time: 0.07s
Epoch 37/1000, LR 0.000270
Train loss: 0.0354;  Loss pred: 0.0354; Loss self: 0.0000; time: 0.20s
Val loss: 0.5805 score: 0.6939 time: 0.07s
Test loss: 0.5722 score: 0.7708 time: 0.07s
Epoch 38/1000, LR 0.000270
Train loss: 0.0394;  Loss pred: 0.0394; Loss self: 0.0000; time: 0.21s
Val loss: 0.5728 score: 0.7143 time: 0.07s
Test loss: 0.5658 score: 0.7708 time: 0.07s
Epoch 39/1000, LR 0.000269
Train loss: 0.0293;  Loss pred: 0.0293; Loss self: 0.0000; time: 0.21s
Val loss: 0.5627 score: 0.7143 time: 0.07s
Test loss: 0.5570 score: 0.7917 time: 0.07s
Epoch 40/1000, LR 0.000269
Train loss: 0.0258;  Loss pred: 0.0258; Loss self: 0.0000; time: 0.21s
Val loss: 0.5626 score: 0.7143 time: 0.07s
Test loss: 0.5593 score: 0.7708 time: 0.07s
Epoch 41/1000, LR 0.000269
Train loss: 0.0258;  Loss pred: 0.0258; Loss self: 0.0000; time: 0.21s
Val loss: 0.5618 score: 0.7347 time: 0.07s
Test loss: 0.5609 score: 0.7708 time: 0.07s
Epoch 42/1000, LR 0.000269
Train loss: 0.0211;  Loss pred: 0.0211; Loss self: 0.0000; time: 0.20s
Val loss: 0.5580 score: 0.7347 time: 0.07s
Test loss: 0.5594 score: 0.7500 time: 0.07s
Epoch 43/1000, LR 0.000269
Train loss: 0.0184;  Loss pred: 0.0184; Loss self: 0.0000; time: 0.21s
Val loss: 0.5543 score: 0.7347 time: 0.07s
Test loss: 0.5599 score: 0.7708 time: 0.07s
Epoch 44/1000, LR 0.000269
Train loss: 0.0175;  Loss pred: 0.0175; Loss self: 0.0000; time: 0.20s
Val loss: 0.5411 score: 0.7347 time: 0.07s
Test loss: 0.5450 score: 0.7708 time: 0.07s
Epoch 45/1000, LR 0.000269
Train loss: 0.0167;  Loss pred: 0.0167; Loss self: 0.0000; time: 0.22s
Val loss: 0.5210 score: 0.7551 time: 0.07s
Test loss: 0.5228 score: 0.7708 time: 0.07s
Epoch 46/1000, LR 0.000269
Train loss: 0.0138;  Loss pred: 0.0138; Loss self: 0.0000; time: 0.22s
Val loss: 0.5006 score: 0.7755 time: 0.07s
Test loss: 0.5016 score: 0.7708 time: 0.07s
Epoch 47/1000, LR 0.000269
Train loss: 0.0122;  Loss pred: 0.0122; Loss self: 0.0000; time: 0.22s
Val loss: 0.4871 score: 0.8163 time: 0.07s
Test loss: 0.4874 score: 0.7708 time: 0.07s
Epoch 48/1000, LR 0.000269
Train loss: 0.0123;  Loss pred: 0.0123; Loss self: 0.0000; time: 0.22s
Val loss: 0.4710 score: 0.8163 time: 0.07s
Test loss: 0.4689 score: 0.8125 time: 0.07s
Epoch 49/1000, LR 0.000269
Train loss: 0.0101;  Loss pred: 0.0101; Loss self: 0.0000; time: 0.22s
Val loss: 0.4559 score: 0.8163 time: 0.07s
Test loss: 0.4517 score: 0.8333 time: 0.07s
Epoch 50/1000, LR 0.000269
Train loss: 0.0096;  Loss pred: 0.0096; Loss self: 0.0000; time: 0.21s
Val loss: 0.4437 score: 0.8163 time: 0.07s
Test loss: 0.4373 score: 0.8542 time: 0.07s
Epoch 51/1000, LR 0.000269
Train loss: 0.0095;  Loss pred: 0.0095; Loss self: 0.0000; time: 0.21s
Val loss: 0.4298 score: 0.8163 time: 0.07s
Test loss: 0.4202 score: 0.8750 time: 0.07s
Epoch 52/1000, LR 0.000269
Train loss: 0.0078;  Loss pred: 0.0078; Loss self: 0.0000; time: 0.22s
Val loss: 0.4112 score: 0.8367 time: 0.07s
Test loss: 0.3965 score: 0.8750 time: 0.07s
Epoch 53/1000, LR 0.000269
Train loss: 0.0077;  Loss pred: 0.0077; Loss self: 0.0000; time: 0.21s
Val loss: 0.3884 score: 0.8367 time: 0.07s
Test loss: 0.3673 score: 0.8750 time: 0.07s
Epoch 54/1000, LR 0.000269
Train loss: 0.0071;  Loss pred: 0.0071; Loss self: 0.0000; time: 0.22s
Val loss: 0.3681 score: 0.8367 time: 0.07s
Test loss: 0.3400 score: 0.8750 time: 0.07s
Epoch 55/1000, LR 0.000269
Train loss: 0.0054;  Loss pred: 0.0054; Loss self: 0.0000; time: 0.22s
Val loss: 0.3523 score: 0.8571 time: 0.07s
Test loss: 0.3185 score: 0.8958 time: 0.07s
Epoch 56/1000, LR 0.000269
Train loss: 0.0065;  Loss pred: 0.0065; Loss self: 0.0000; time: 0.21s
Val loss: 0.3416 score: 0.8571 time: 0.07s
Test loss: 0.3006 score: 0.8958 time: 0.07s
Epoch 57/1000, LR 0.000269
Train loss: 0.0058;  Loss pred: 0.0058; Loss self: 0.0000; time: 0.22s
Val loss: 0.3338 score: 0.8571 time: 0.07s
Test loss: 0.2858 score: 0.9167 time: 0.07s
Epoch 58/1000, LR 0.000269
Train loss: 0.0050;  Loss pred: 0.0050; Loss self: 0.0000; time: 0.21s
Val loss: 0.3273 score: 0.8571 time: 0.07s
Test loss: 0.2740 score: 0.9167 time: 0.07s
Epoch 59/1000, LR 0.000268
Train loss: 0.0049;  Loss pred: 0.0049; Loss self: 0.0000; time: 0.20s
Val loss: 0.3209 score: 0.8571 time: 0.07s
Test loss: 0.2634 score: 0.9167 time: 0.07s
Epoch 60/1000, LR 0.000268
Train loss: 0.0043;  Loss pred: 0.0043; Loss self: 0.0000; time: 0.20s
Val loss: 0.3180 score: 0.8571 time: 0.07s
Test loss: 0.2574 score: 0.9167 time: 0.07s
Epoch 61/1000, LR 0.000268
Train loss: 0.0046;  Loss pred: 0.0046; Loss self: 0.0000; time: 0.20s
Val loss: 0.3141 score: 0.8571 time: 0.07s
Test loss: 0.2497 score: 0.9167 time: 0.07s
Epoch 62/1000, LR 0.000268
Train loss: 0.0041;  Loss pred: 0.0041; Loss self: 0.0000; time: 0.20s
Val loss: 0.3121 score: 0.8571 time: 0.07s
Test loss: 0.2442 score: 0.9167 time: 0.07s
Epoch 63/1000, LR 0.000268
Train loss: 0.0043;  Loss pred: 0.0043; Loss self: 0.0000; time: 0.20s
Val loss: 0.3121 score: 0.8571 time: 0.07s
Test loss: 0.2416 score: 0.9167 time: 0.07s
     INFO: Early stopping counter 1 of 20
Epoch 64/1000, LR 0.000268
Train loss: 0.0037;  Loss pred: 0.0037; Loss self: 0.0000; time: 0.20s
Val loss: 0.3136 score: 0.8571 time: 0.07s
Test loss: 0.2409 score: 0.9167 time: 0.07s
     INFO: Early stopping counter 2 of 20
Epoch 65/1000, LR 0.000268
Train loss: 0.0036;  Loss pred: 0.0036; Loss self: 0.0000; time: 0.20s
Val loss: 0.3156 score: 0.8571 time: 0.07s
Test loss: 0.2424 score: 0.8958 time: 0.07s
     INFO: Early stopping counter 3 of 20
Epoch 66/1000, LR 0.000268
Train loss: 0.0037;  Loss pred: 0.0037; Loss self: 0.0000; time: 0.20s
Val loss: 0.3205 score: 0.8571 time: 0.07s
Test loss: 0.2471 score: 0.8958 time: 0.07s
     INFO: Early stopping counter 4 of 20
Epoch 67/1000, LR 0.000268
Train loss: 0.0032;  Loss pred: 0.0032; Loss self: 0.0000; time: 0.20s
Val loss: 0.3276 score: 0.8571 time: 0.07s
Test loss: 0.2537 score: 0.8958 time: 0.07s
     INFO: Early stopping counter 5 of 20
Epoch 68/1000, LR 0.000268
Train loss: 0.0024;  Loss pred: 0.0024; Loss self: 0.0000; time: 0.20s
Val loss: 0.3373 score: 0.8571 time: 0.07s
Test loss: 0.2649 score: 0.8958 time: 0.07s
     INFO: Early stopping counter 6 of 20
Epoch 69/1000, LR 0.000268
Train loss: 0.0033;  Loss pred: 0.0033; Loss self: 0.0000; time: 0.20s
Val loss: 0.3491 score: 0.8571 time: 0.07s
Test loss: 0.2793 score: 0.8750 time: 0.07s
     INFO: Early stopping counter 7 of 20
Epoch 70/1000, LR 0.000268
Train loss: 0.0025;  Loss pred: 0.0025; Loss self: 0.0000; time: 0.20s
Val loss: 0.3623 score: 0.8571 time: 0.07s
Test loss: 0.2968 score: 0.8542 time: 0.07s
     INFO: Early stopping counter 8 of 20
Epoch 71/1000, LR 0.000268
Train loss: 0.0025;  Loss pred: 0.0025; Loss self: 0.0000; time: 0.20s
Val loss: 0.3708 score: 0.8571 time: 0.07s
Test loss: 0.3049 score: 0.8542 time: 0.07s
     INFO: Early stopping counter 9 of 20
Epoch 72/1000, LR 0.000267
Train loss: 0.0027;  Loss pred: 0.0027; Loss self: 0.0000; time: 0.20s
Val loss: 0.3822 score: 0.8571 time: 0.07s
Test loss: 0.3156 score: 0.8542 time: 0.07s
     INFO: Early stopping counter 10 of 20
Epoch 73/1000, LR 0.000267
Train loss: 0.0021;  Loss pred: 0.0021; Loss self: 0.0000; time: 0.21s
Val loss: 0.3926 score: 0.8571 time: 0.07s
Test loss: 0.3237 score: 0.8542 time: 0.07s
     INFO: Early stopping counter 11 of 20
Epoch 74/1000, LR 0.000267
Train loss: 0.0025;  Loss pred: 0.0025; Loss self: 0.0000; time: 0.20s
Val loss: 0.4009 score: 0.8571 time: 0.07s
Test loss: 0.3300 score: 0.8542 time: 0.07s
     INFO: Early stopping counter 12 of 20
Epoch 75/1000, LR 0.000267
Train loss: 0.0025;  Loss pred: 0.0025; Loss self: 0.0000; time: 0.20s
Val loss: 0.4083 score: 0.8571 time: 0.07s
Test loss: 0.3355 score: 0.8542 time: 0.07s
     INFO: Early stopping counter 13 of 20
Epoch 76/1000, LR 0.000267
Train loss: 0.0024;  Loss pred: 0.0024; Loss self: 0.0000; time: 0.20s
Val loss: 0.4148 score: 0.8571 time: 0.07s
Test loss: 0.3416 score: 0.8542 time: 0.07s
     INFO: Early stopping counter 14 of 20
Epoch 77/1000, LR 0.000267
Train loss: 0.0023;  Loss pred: 0.0023; Loss self: 0.0000; time: 0.20s
Val loss: 0.4226 score: 0.8571 time: 0.07s
Test loss: 0.3500 score: 0.8542 time: 0.07s
     INFO: Early stopping counter 15 of 20
Epoch 78/1000, LR 0.000267
Train loss: 0.0020;  Loss pred: 0.0020; Loss self: 0.0000; time: 0.20s
Val loss: 0.4271 score: 0.8571 time: 0.07s
Test loss: 0.3526 score: 0.8542 time: 0.06s
     INFO: Early stopping counter 16 of 20
Epoch 79/1000, LR 0.000267
Train loss: 0.0024;  Loss pred: 0.0024; Loss self: 0.0000; time: 0.19s
Val loss: 0.4311 score: 0.8571 time: 0.06s
Test loss: 0.3520 score: 0.8542 time: 0.07s
     INFO: Early stopping counter 17 of 20
Epoch 80/1000, LR 0.000267
Train loss: 0.0021;  Loss pred: 0.0021; Loss self: 0.0000; time: 0.20s
Val loss: 0.4404 score: 0.8571 time: 0.06s
Test loss: 0.3585 score: 0.8542 time: 0.07s
     INFO: Early stopping counter 18 of 20
Epoch 81/1000, LR 0.000267
Train loss: 0.0018;  Loss pred: 0.0018; Loss self: 0.0000; time: 0.20s
Val loss: 0.4467 score: 0.8571 time: 0.07s
Test loss: 0.3588 score: 0.8542 time: 0.07s
     INFO: Early stopping counter 19 of 20
Epoch 82/1000, LR 0.000267
Train loss: 0.0017;  Loss pred: 0.0017; Loss self: 0.0000; time: 0.20s
Val loss: 0.4546 score: 0.8571 time: 0.07s
Test loss: 0.3609 score: 0.8542 time: 0.07s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 061,   Train_Loss: 0.0041,   Val_Loss: 0.3121,   Val_Precision: 1.0000,   Val_Recall: 0.7200,   Val_accuracy: 0.8372,   Val_Score: 0.8571,   Val_Loss: 0.3121,   Test_Precision: 1.0000,   Test_Recall: 0.8333,   Test_accuracy: 0.9091,   Test_Score: 0.9167,   Test_loss: 0.2442


[0.06551805592607707, 0.06593708205036819, 0.06583644892089069, 0.06518786097876728, 0.06579508597496897, 0.06549061997793615, 0.06530202808789909, 0.0873524839989841, 0.06478835002053529, 0.06467129394877702, 0.06468775100074708, 0.06497310905251652, 0.06481277896091342, 0.06498708995059133, 0.06475997995585203, 0.06488925707526505, 0.06966209306847304, 0.06508939201012254, 0.06421613995917141, 0.06492818903643638, 0.06454007094725966, 0.06445524003356695, 0.06600539106875658, 0.06461790599860251, 0.06983065302483737, 0.06938169989734888, 0.06945548497606069, 0.06967175402678549, 0.06268204399384558, 0.07595257204957306, 0.0709757930599153, 0.07151052099652588, 0.07178896793629974, 0.07151582406368107, 0.07208174595143646, 0.07142971199937165, 0.07761065603699535, 0.07792094198521227, 0.07736591505818069, 0.0774561739526689, 0.07157329306937754, 0.07288609398528934, 0.07193993194960058, 0.07115531095769256, 0.07131258305162191, 0.0804967270232737, 0.0776639599353075, 0.07767517201136798, 0.07757513597607613, 0.07772865204606205, 0.07740414701402187, 0.08280242700129747, 0.07501279201824218, 0.07514620700385422, 0.07165675004944205, 0.0722780890064314, 0.0734831930603832, 0.07157503301277757, 0.07149815303273499, 0.07151708204764873, 0.07175754196941853, 0.07188682805281132, 0.07145667297299951, 0.16509054391644895, 0.0720897139981389, 0.0731007499853149, 0.07279925199691206, 0.07301875401753932, 0.07934323907829821, 0.0752874530153349, 0.07596274395473301, 0.07590584398712963, 0.07725674705579877, 0.07552563399076462, 0.07562406605575234, 0.07607527100481093, 0.0768202330218628, 0.07547867693938315, 0.0751662690890953, 0.07496168301440775, 0.07687002595048398, 0.07534361607395113, 0.07578815298620611, 0.07553746306803077, 0.07549895194824785, 0.07534386496990919, 0.07576936797704548, 0.07567510195076466, 0.07574153901077807, 0.07580443599727005, 0.07574654906056821, 0.07589410000946373, 0.07619404199067503, 0.07576866797171533, 0.0754954069852829, 0.07552675902843475, 0.07548474194481969, 0.0757932469714433, 0.07561410998459905, 0.0755823600338772, 0.07542227010708302, 0.07558933808468282, 0.0759076529648155, 0.07560227799694985, 0.0758562230039388, 0.07501816807780415, 0.07618317904416472, 0.07542742101941258, 0.07570521091111004, 0.07538525701966137, 0.07587351696565747, 0.07609854091424495, 0.07591776200570166, 0.07544106594286859, 0.07542553602252156, 0.07526099402457476, 0.07596257200930268, 0.07573553209658712, 0.07614157698117197, 0.07624018797650933, 0.06896251102443784, 0.06869008601643145, 0.0769344869768247, 0.07716511900071055, 0.07671657903119922, 0.07671674899756908, 0.0766690099844709, 0.07700259401462972, 0.0741141039179638, 0.07456614694092423, 0.07519358501303941, 0.07512898196000606, 0.07474818604532629, 0.07760092802345753, 0.07143709610681981, 0.07135405903682113, 0.07287278608419001, 0.0736090550199151, 0.07136694190558046, 0.07087202498223633, 0.07089742401149124, 0.07133955997414887, 0.07074344402644783, 0.0711857209680602, 0.07030077592935413, 0.07250930008012801, 0.07267315697390586, 0.07318323291838169, 0.07322455698158592, 0.07851462694816291, 0.0740433291066438, 0.07285781810060143, 0.07346577302087098, 0.07320418301969767, 0.07121136097703129, 0.07095623505301774, 0.07256601192057133, 0.07299809507094324, 0.07218990195542574, 0.07135772902984172, 0.07139268994797021, 0.07208714599255472, 0.07668335107155144, 0.07660607097204775, 0.07640889997128397, 0.07612437708303332, 0.07589084305800498, 0.07377147697843611, 0.07519955700263381, 0.07522241096012294, 0.07572504295967519, 0.07528978597838432, 0.0749811630230397, 0.07537188695278019, 0.07522837002761662, 0.07090960897039622, 0.07052377995569259, 0.07028600596822798, 0.07063632702920586, 0.07117142889183015, 0.07009661698248237, 0.07048415101598948, 0.07198232295922935, 0.0710055329836905, 0.07005047309212387, 0.07144974393304437, 0.070233438978903, 0.07039664289914072, 0.07014146901201457, 0.07001312996726483, 0.0713523110607639, 0.06980497308541089, 0.07035328890196979, 0.0703264030162245, 0.07027154497336596, 0.0701490530045703, 0.06933494005352259, 0.07004957902245224, 0.07029859302565455, 0.07040975708514452, 0.0700430020224303]
[0.0013371031821648382, 0.0013456547357217998, 0.0013436009983855243, 0.0013303645097707607, 0.0013427568566320197, 0.0013365432648558397, 0.0013326944507734508, 0.001782703755081308, 0.0013222112249088834, 0.0013198223254852453, 0.001320158183688716, 0.0013259818173982963, 0.0013227097747125188, 0.0013262671418488026, 0.0013216322439969803, 0.0013242705525564297, 0.0014216753687443478, 0.0013283549389820925, 0.0013105334685545188, 0.0013250650823762526, 0.0013171443050461157, 0.0013154130619095297, 0.001347048797321563, 0.0013187327754816838, 0.001425115367853824, 0.001415953059129569, 0.001417458877062463, 0.0014218725311588875, 0.0012792253876295016, 0.0015500524908076136, 0.0014484855726513328, 0.0014593983876842018, 0.0014650809782918316, 0.0014595066135445116, 0.001471056039825234, 0.0014577492244769723, 0.0015838909395305173, 0.0015902233058206585, 0.0015788962256771569, 0.0015807382439320184, 0.00146067945039546, 0.0014874713058222312, 0.0014681618765224607, 0.0014521492032182155, 0.0014553588377882022, 0.0016427903474137491, 0.001584978774189949, 0.0015852075920687343, 0.0015831660403280842, 0.001586299021348205, 0.0015796764696739158, 0.001689845449006071, 0.0015308733064947383, 0.0015335960613031472, 0.001462382654070246, 0.0014750630409475795, 0.0014996570012323102, 0.0014607149594444402, 0.0014591459802598978, 0.0014595322866867088, 0.0014644396320289495, 0.0014670781235267617, 0.0014582994484285616, 0.0033691947738050806, 0.001471218653023243, 0.0014918520405166307, 0.001485699020345144, 0.0014901786534191699, 0.0016192497771081267, 0.0015364786329660186, 0.001550260080708837, 0.0015490988568801964, 0.0015766683072611994, 0.0015413394691992778, 0.0015433482868520885, 0.0015525565511185903, 0.0015677598575890368, 0.0015403811620282276, 0.0015340054916141896, 0.0015298302656001582, 0.0015687760398057954, 0.0015376248178357373, 0.0015466969997184922, 0.0015415808789394035, 0.0015407949377193439, 0.0015376298973450856, 0.0015463136321846017, 0.0015443898357298909, 0.0015457456940975115, 0.0015470293060667357, 0.0015458479400115963, 0.0015488591838666067, 0.0015549804487892864, 0.0015462993463615374, 0.0015407225915363856, 0.0015413624291517297, 0.0015405049376493814, 0.0015468009586008837, 0.0015431451017265112, 0.001542497143548514, 0.0015392300021853677, 0.001542639552748629, 0.001549135774792153, 0.0015429036325908133, 0.001548086183753853, 0.001530983021996003, 0.0015547587560033615, 0.0015393351228451546, 0.0015450043043083682, 0.0015384746330543136, 0.0015484391217481118, 0.0015530314472294888, 0.0015493420817490136, 0.0015396135906707875, 0.001539296653520848, 0.00153593865356275, 0.001550256571618422, 0.001545623104011982, 0.001553909734309632, 0.0015883372495106112, 0.0014367189796757884, 0.0014310434586756553, 0.0016028018120171812, 0.0016076066458481364, 0.0015982620631499838, 0.0015982656041160226, 0.001597271041343144, 0.0016042207086381193, 0.001544043831624246, 0.0015534613946025881, 0.0015665330211049877, 0.001565187124166793, 0.0015572538759442978, 0.0016166860004886985, 0.001488272835558746, 0.0014865428966004401, 0.0015181830434206252, 0.0015335219795815647, 0.0014868112896995929, 0.0014765005204632569, 0.0014770296669060674, 0.001486240832794768, 0.0014738217505509965, 0.001483035853501254, 0.0014645994985282111, 0.0015106104183360003, 0.0015140241036230389, 0.0015246506857996185, 0.00152551160378304, 0.001635721394753394, 0.0015425693563884124, 0.0015178712104291965, 0.0015305369379348122, 0.0015250871462437015, 0.0014835700203548186, 0.0014782548969378695, 0.0015117919150119026, 0.0015207936473113175, 0.0015039562907380362, 0.0014866193547883693, 0.0014873477072493795, 0.0015018155415115568, 0.0015975698139906551, 0.0015959598119176615, 0.0015918520827350828, 0.0015859245225631942, 0.0015810592303751037, 0.0015369057703840856, 0.001566657437554871, 0.001567133561669228, 0.0015776050616598998, 0.0015685372078830067, 0.0015621075629799936, 0.0015702476448495872, 0.0015672577089086797, 0.001477283520216588, 0.0014692454157435957, 0.0014642917910047497, 0.0014715901464417887, 0.001482738101913128, 0.0014603461871350494, 0.0014684198128331143, 0.0014996317283172782, 0.0014792819371602188, 0.001459384856085914, 0.0014885363319384244, 0.0014631966453938123, 0.0014665967270654316, 0.0014612806044169702, 0.0014586068743180174, 0.0014865064804325812, 0.001454270272612727, 0.0014656935187910374, 0.0014651333961713438, 0.0014639905202784576, 0.0014614386042618814, 0.0014444779177817206, 0.0014593662296344216, 0.0014645540213678032, 0.0014668699392738442, 0.0014592292088006313]
[747.8854387145718, 743.1326724857153, 744.2685746747759, 751.6736899215036, 744.7364689004513, 748.1987499356113, 750.3595437195929, 560.9456967539683, 756.3088114525062, 757.6777424433553, 757.4849835084558, 754.158154266471, 756.023746945805, 753.9959096068764, 756.640135364528, 755.1327015990474, 703.3954600220866, 752.8108419322713, 763.0480441701131, 754.6799121796266, 759.2182543468449, 760.2174776555299, 742.3636040419428, 758.303743254381, 701.6975766010905, 706.2381012932248, 705.4878389646101, 703.297924452452, 781.7230721578105, 645.1394426513754, 690.3762238857395, 685.2138582849999, 682.5561281711001, 685.1630480600782, 679.783755973568, 685.989046132946, 631.356601039975, 628.8425005090307, 633.3538479206384, 632.6158071007019, 684.6129037615084, 672.2818760172512, 681.1238024846652, 688.6344721216152, 687.1157641917104, 608.720401586425, 630.9232756199401, 630.8322045663281, 631.6456862558566, 630.3981699176073, 633.0410177005579, 591.7700938794003, 653.2219196438364, 652.0621858863324, 683.8155507498003, 677.9371269160134, 666.8191454301029, 684.5962612584827, 685.332388622202, 685.1509960564865, 682.855051262524, 681.6269590306916, 685.7302189050287, 296.80682392565456, 679.7086197521189, 670.3077603149562, 673.0838388570043, 671.060478356424, 617.5699476000138, 650.838858767336, 645.053054286712, 645.5365941034565, 634.2488114935735, 648.7863445938341, 647.9418861698831, 644.0989214077368, 637.8527905018818, 649.1899697626106, 651.8881486843499, 653.6672874671467, 637.4396182923559, 650.3537068343735, 646.5390442872817, 648.6847454205534, 649.0156318141735, 650.3515584124813, 646.699336529304, 647.5049089709867, 646.9369468849487, 646.4001658394325, 646.8941569974202, 645.6364854961041, 643.094902433406, 646.7053112018789, 649.046106997636, 648.7766803491753, 649.1378090133716, 646.4955910710855, 648.0272003463406, 648.2994177217731, 649.6754861718002, 648.2395697804001, 645.521210130319, 648.1286185844411, 645.9588687595967, 653.1751075176918, 643.1866012258932, 649.6311200589635, 647.2473877331086, 649.9944675816416, 645.8116344096558, 643.9019646279138, 645.4352539570377, 649.5136221578261, 649.64735531172, 651.0676697148083, 645.0545143995291, 646.9882582657407, 643.5380240695114, 629.5892136938261, 696.0303400639011, 698.7907976781082, 623.9074553712074, 622.0427133606573, 625.6796197922132, 625.6782335956516, 626.0678207495084, 623.3556234596523, 647.6500080623082, 643.7237535959647, 638.3523274183066, 638.9012435381086, 642.1560513976011, 618.5493037594906, 671.9198093974264, 672.7017446229704, 658.6821031454122, 652.0936858517405, 672.5803112525786, 677.2771063340003, 677.0344715517455, 672.8384646245875, 678.5081029141716, 674.2925315252025, 682.7805150861439, 661.9840482111472, 660.4914661576482, 655.8879416208965, 655.5177931915758, 611.3510547746811, 648.2690686539246, 658.8174234606096, 653.3654792737785, 655.7002348770731, 674.0497491050909, 676.4733213950109, 661.4666939743006, 657.5514053257305, 664.9129407273333, 672.6671469593217, 672.3377426313757, 665.8607347967072, 625.9507354498935, 626.5821936947319, 628.1990712867138, 630.5470315723384, 632.487373520315, 650.657977391868, 638.3016325258251, 638.107704703135, 633.8722055999463, 637.5366774688507, 640.1607825854993, 636.8422224863705, 638.05715825531, 676.9181313641052, 680.6214872509184, 682.9239951648109, 679.5370317054218, 674.427937549951, 684.769138173894, 681.0041592061043, 666.8303831648655, 676.0036575040607, 685.2202116733012, 671.8008681036121, 683.4351371348687, 681.8506966130611, 684.3312618927051, 685.5856897476625, 672.7182243490756, 687.6300910720057, 682.2708753088026, 682.5317084527452, 683.0645322824874, 684.2572770992751, 692.2916492456294, 685.2289574019435, 682.8017167069478, 681.7236983498602, 685.2933000305821]
Elapsed: 0.07333179129593408~0.007584968062759649
Time per graph: 0.0015091850760381071~0.00015471295839053655
Speed: 666.9323431763397~46.10253526092607
Total Time: 0.0708
best val loss: 0.31209126114845276 test_score: 0.9167

Testing...
Test loss: 0.3185 score: 0.8958 time: 0.07s
test Score 0.8958
Epoch Time List: [0.3243853859603405, 0.3222107549663633, 0.32136415294371545, 0.32078198599629104, 0.3252676229458302, 0.32210884196683764, 0.32793034706264734, 0.3565483510028571, 0.3384747669333592, 0.3355271320324391, 0.3299674120498821, 0.3265936269890517, 0.3275762120028958, 0.3284368640743196, 0.3244710109429434, 0.3246677750721574, 0.3332553638610989, 0.3281951319659129, 0.3252182549331337, 0.3259224990615621, 0.3254728780593723, 0.32518224185332656, 0.3292822520015761, 0.328014521044679, 0.33810002205427736, 0.3494074238697067, 0.34760013804771006, 0.3500692439265549, 0.34435553196817636, 0.33982584497425705, 0.32202103384770453, 0.3213520720601082, 0.32234160602092743, 0.32199360302183777, 0.32245190313551575, 0.3210727469995618, 0.339111007982865, 0.35154087899718434, 0.34458953607827425, 0.3478604359552264, 0.3369269350077957, 0.3220441520679742, 0.3223309140885249, 0.31824981700628996, 0.32023131602909416, 0.32917697285301983, 0.34899631701409817, 0.3494256160920486, 0.3485374728916213, 0.34865538985468447, 0.3493479179451242, 0.35019224393181503, 0.346629866049625, 0.3714330110233277, 0.3165031939279288, 0.31701087206602097, 0.3211045399075374, 0.31913305493071675, 0.3633704581297934, 0.3149131149984896, 0.31515628716442734, 0.31793745001778007, 0.32036112691275775, 0.41144709498621523, 0.3186425919411704, 0.31854435394052416, 0.3333812780911103, 0.32193158892914653, 0.3246602601138875, 0.40307455882430077, 0.3381644629407674, 0.3364517339505255, 0.33751869900152087, 0.33324358600657433, 0.33475844690110534, 0.3350575868971646, 0.33842115791048855, 0.34008187195286155, 0.33370154502335936, 0.3339357660152018, 0.33553789800498635, 0.33707583707291633, 0.33497123897541314, 0.33591059199534357, 0.334528508130461, 0.3344549030298367, 0.33531493495684117, 0.3338740919716656, 0.3340314090019092, 0.33531780808698386, 0.33420963503886014, 0.3356986490543932, 0.335843920125626, 0.33473677502479404, 0.3344458091305569, 0.3364695670315996, 0.33597618096973747, 0.3337009579408914, 0.3335538530955091, 0.3332582488656044, 0.33324936311692, 0.33501232496928424, 0.33435696305241436, 0.33450359699781984, 0.33454564795829356, 0.3332870260346681, 0.33499178290367126, 0.33749218890443444, 0.33408150309696794, 0.33383169304579496, 0.3376185899833217, 0.33743751503061503, 0.33468605601228774, 0.33316005603410304, 0.3337661400437355, 0.33360917889513075, 0.33496450015809387, 0.33410874602850527, 0.3350568929454312, 0.34073125396389514, 0.33199295005761087, 0.32495970104355365, 0.352999820956029, 0.3687156660016626, 0.36616832099389285, 0.36567856394685805, 0.36594795307610184, 0.36524531396571547, 0.35614260006695986, 0.3564833311829716, 0.3609828109620139, 0.3578224489465356, 0.3562140610301867, 0.3671145758125931, 0.3473908130545169, 0.3650625409791246, 0.35080116998869926, 0.3544639068422839, 0.33603479200974107, 0.33606565999798477, 0.33488071500323713, 0.3341427040286362, 0.3302480529528111, 0.3368361098691821, 0.3332365920068696, 0.3364468249492347, 0.3410822390578687, 0.34449175104964525, 0.35571645188611, 0.35887392493896186, 0.35376651701517403, 0.34574034810066223, 0.3459759580437094, 0.343852901016362, 0.33686297095846385, 0.3366073169745505, 0.3417927889386192, 0.3435413188999519, 0.34389193693641573, 0.34304833598434925, 0.33759937493596226, 0.3407001890009269, 0.34921678190585226, 0.3620883789844811, 0.3627057180274278, 0.3590731860604137, 0.36197129206266254, 0.3558568609878421, 0.3552822021301836, 0.35550912807229906, 0.3589190160855651, 0.35509958991315216, 0.3564719180576503, 0.356264705886133, 0.35256187606137246, 0.34764820500276983, 0.3437279260251671, 0.3330564899370074, 0.3341170981293544, 0.331119300913997, 0.33183753304183483, 0.33215611800551414, 0.3333920082077384, 0.3350427779369056, 0.33061789185740054, 0.3330735658528283, 0.33372968703042716, 0.332050597993657, 0.33110867091454566, 0.33393711294047534, 0.3340326640754938, 0.3371871398994699, 0.3329504589783028, 0.3300528699764982, 0.33049151790328324, 0.3318457311252132, 0.32988518418278545, 0.32497621898073703, 0.32630281103774905, 0.33383639599196613, 0.3307891479926184]
Total Epoch List: [29, 90, 82]
Total Time List: [0.06305443798191845, 0.0767055778997019, 0.07080297195352614]
T-times Epoch Time: 0.3369775284505239 ~ 0.005472068441639547
T-times Total Epoch: 71.55555555555556 ~ 3.8135560670512234
T-times Total Time: 0.07216209475882351 ~ 0.003317574682753588
T-times Inference Elapsed: 0.07278115674647577 ~ 0.0007066742527794426
T-times Time Per Graph: 0.0014953482871069576 ~ 1.2826982057212803e-05
T-times Speed: 673.6650013860713 ~ 5.101624750203655
T-times cross validation test micro f1 score:0.8386041401793901 ~ 0.0458081516030759
T-times cross validation test precision:0.8528997556775334 ~ 0.01948165956896041
T-times cross validation test recall:0.8362962962962963 ~ 0.05043908711422443
T-times cross validation test f1_score:0.8386041401793901 ~ 0.03556186931344052
